Epoch: 1| Step: 0
Training loss: 6.611223220825195
Validation loss: 5.256025078476116

Epoch: 6| Step: 1
Training loss: 4.850004196166992
Validation loss: 5.249504802047565

Epoch: 6| Step: 2
Training loss: 4.911543846130371
Validation loss: 5.2427843206672256

Epoch: 6| Step: 3
Training loss: 4.884400844573975
Validation loss: 5.236526258530155

Epoch: 6| Step: 4
Training loss: 4.630511283874512
Validation loss: 5.230346043904622

Epoch: 6| Step: 5
Training loss: 4.295388221740723
Validation loss: 5.224951580006589

Epoch: 6| Step: 6
Training loss: 5.6388092041015625
Validation loss: 5.219180742899577

Epoch: 6| Step: 7
Training loss: 4.669382095336914
Validation loss: 5.213296362148818

Epoch: 6| Step: 8
Training loss: 3.8724217414855957
Validation loss: 5.207387308920583

Epoch: 6| Step: 9
Training loss: 4.798789978027344
Validation loss: 5.201474323067614

Epoch: 6| Step: 10
Training loss: 5.923964023590088
Validation loss: 5.195770827672815

Epoch: 6| Step: 11
Training loss: 5.496102333068848
Validation loss: 5.189478310205603

Epoch: 6| Step: 12
Training loss: 4.363236904144287
Validation loss: 5.1826618051016204

Epoch: 6| Step: 13
Training loss: 5.37039041519165
Validation loss: 5.175947194458336

Epoch: 2| Step: 0
Training loss: 4.852326393127441
Validation loss: 5.1685385960404595

Epoch: 6| Step: 1
Training loss: 5.2814836502075195
Validation loss: 5.160999590350736

Epoch: 6| Step: 2
Training loss: 5.42732048034668
Validation loss: 5.153078499660697

Epoch: 6| Step: 3
Training loss: 5.220109939575195
Validation loss: 5.1447565017207975

Epoch: 6| Step: 4
Training loss: 3.7518622875213623
Validation loss: 5.136042615418793

Epoch: 6| Step: 5
Training loss: 3.353580951690674
Validation loss: 5.12707338025493

Epoch: 6| Step: 6
Training loss: 6.150394439697266
Validation loss: 5.117024134564144

Epoch: 6| Step: 7
Training loss: 5.632672309875488
Validation loss: 5.10718390762165

Epoch: 6| Step: 8
Training loss: 3.574937343597412
Validation loss: 5.0968962074607935

Epoch: 6| Step: 9
Training loss: 6.5773725509643555
Validation loss: 5.08560057609312

Epoch: 6| Step: 10
Training loss: 5.0198516845703125
Validation loss: 5.0742568046815935

Epoch: 6| Step: 11
Training loss: 4.751543045043945
Validation loss: 5.061883480318131

Epoch: 6| Step: 12
Training loss: 4.559866905212402
Validation loss: 5.050396221940235

Epoch: 6| Step: 13
Training loss: 4.14824914932251
Validation loss: 5.03662916409072

Epoch: 3| Step: 0
Training loss: 4.489612579345703
Validation loss: 5.022639448924731

Epoch: 6| Step: 1
Training loss: 4.639498233795166
Validation loss: 5.008984478571081

Epoch: 6| Step: 2
Training loss: 4.203922271728516
Validation loss: 4.994070899101995

Epoch: 6| Step: 3
Training loss: 4.459630966186523
Validation loss: 4.978954817659112

Epoch: 6| Step: 4
Training loss: 4.710973262786865
Validation loss: 4.962612008535734

Epoch: 6| Step: 5
Training loss: 5.725873947143555
Validation loss: 4.94656862238402

Epoch: 6| Step: 6
Training loss: 5.31526517868042
Validation loss: 4.92901268313008

Epoch: 6| Step: 7
Training loss: 3.7512688636779785
Validation loss: 4.911402138330603

Epoch: 6| Step: 8
Training loss: 5.2782440185546875
Validation loss: 4.89213429727862

Epoch: 6| Step: 9
Training loss: 5.783625602722168
Validation loss: 4.874547512300553

Epoch: 6| Step: 10
Training loss: 4.910625457763672
Validation loss: 4.8535807414721415

Epoch: 6| Step: 11
Training loss: 4.229805946350098
Validation loss: 4.832501960057084

Epoch: 6| Step: 12
Training loss: 4.313422203063965
Validation loss: 4.811919827615061

Epoch: 6| Step: 13
Training loss: 3.686551809310913
Validation loss: 4.789809849954421

Epoch: 4| Step: 0
Training loss: 4.508617401123047
Validation loss: 4.768524790322909

Epoch: 6| Step: 1
Training loss: 4.680417060852051
Validation loss: 4.746856312597951

Epoch: 6| Step: 2
Training loss: 4.39690637588501
Validation loss: 4.723671836237753

Epoch: 6| Step: 3
Training loss: 3.631518840789795
Validation loss: 4.699336031431793

Epoch: 6| Step: 4
Training loss: 5.038854598999023
Validation loss: 4.675322527526527

Epoch: 6| Step: 5
Training loss: 4.517186164855957
Validation loss: 4.65154992893178

Epoch: 6| Step: 6
Training loss: 4.3671793937683105
Validation loss: 4.625813720046833

Epoch: 6| Step: 7
Training loss: 4.931180953979492
Validation loss: 4.599993090475759

Epoch: 6| Step: 8
Training loss: 4.5510382652282715
Validation loss: 4.5748032318648475

Epoch: 6| Step: 9
Training loss: 5.1651411056518555
Validation loss: 4.549620489920339

Epoch: 6| Step: 10
Training loss: 3.259248733520508
Validation loss: 4.521718932736304

Epoch: 6| Step: 11
Training loss: 3.6251981258392334
Validation loss: 4.495232284709972

Epoch: 6| Step: 12
Training loss: 4.046657085418701
Validation loss: 4.468572693486368

Epoch: 6| Step: 13
Training loss: 5.20041561126709
Validation loss: 4.441758858260288

Epoch: 5| Step: 0
Training loss: 4.416008472442627
Validation loss: 4.414790476522138

Epoch: 6| Step: 1
Training loss: 4.125161647796631
Validation loss: 4.388126609145954

Epoch: 6| Step: 2
Training loss: 2.9739480018615723
Validation loss: 4.360309508539015

Epoch: 6| Step: 3
Training loss: 4.830582618713379
Validation loss: 4.33385572894927

Epoch: 6| Step: 4
Training loss: 4.080102443695068
Validation loss: 4.307256273044053

Epoch: 6| Step: 5
Training loss: 3.0363869667053223
Validation loss: 4.28211582860639

Epoch: 6| Step: 6
Training loss: 4.707036018371582
Validation loss: 4.256566683451335

Epoch: 6| Step: 7
Training loss: 3.4244794845581055
Validation loss: 4.232613876301755

Epoch: 6| Step: 8
Training loss: 4.477222919464111
Validation loss: 4.20792942918757

Epoch: 6| Step: 9
Training loss: 4.690040588378906
Validation loss: 4.184738138670562

Epoch: 6| Step: 10
Training loss: 3.817490577697754
Validation loss: 4.162031665925057

Epoch: 6| Step: 11
Training loss: 5.3983154296875
Validation loss: 4.139044587330152

Epoch: 6| Step: 12
Training loss: 3.9404191970825195
Validation loss: 4.116677094531315

Epoch: 6| Step: 13
Training loss: 2.143829107284546
Validation loss: 4.093340807063605

Epoch: 6| Step: 0
Training loss: 3.2290735244750977
Validation loss: 4.0718308853846725

Epoch: 6| Step: 1
Training loss: 4.714592933654785
Validation loss: 4.052182976917554

Epoch: 6| Step: 2
Training loss: 3.496354103088379
Validation loss: 4.029238429120792

Epoch: 6| Step: 3
Training loss: 3.302600622177124
Validation loss: 4.011656807314965

Epoch: 6| Step: 4
Training loss: 2.945794105529785
Validation loss: 3.9906943741665093

Epoch: 6| Step: 5
Training loss: 2.8213882446289062
Validation loss: 3.9707932549138225

Epoch: 6| Step: 6
Training loss: 4.864553928375244
Validation loss: 3.953450854106616

Epoch: 6| Step: 7
Training loss: 3.7872934341430664
Validation loss: 3.934058130428355

Epoch: 6| Step: 8
Training loss: 4.775304317474365
Validation loss: 3.9177088737487793

Epoch: 6| Step: 9
Training loss: 5.48374080657959
Validation loss: 3.8995001495525403

Epoch: 6| Step: 10
Training loss: 3.15618896484375
Validation loss: 3.8845562396510953

Epoch: 6| Step: 11
Training loss: 2.9852147102355957
Validation loss: 3.8677116696552565

Epoch: 6| Step: 12
Training loss: 3.6772074699401855
Validation loss: 3.851116257329141

Epoch: 6| Step: 13
Training loss: 4.2381157875061035
Validation loss: 3.837676868643812

Epoch: 7| Step: 0
Training loss: 3.6516261100769043
Validation loss: 3.8242788699365433

Epoch: 6| Step: 1
Training loss: 3.631436824798584
Validation loss: 3.8102499028687835

Epoch: 6| Step: 2
Training loss: 4.108004570007324
Validation loss: 3.7990495363871255

Epoch: 6| Step: 3
Training loss: 3.2078604698181152
Validation loss: 3.785220464070638

Epoch: 6| Step: 4
Training loss: 3.6458067893981934
Validation loss: 3.775166416680941

Epoch: 6| Step: 5
Training loss: 4.392737865447998
Validation loss: 3.764305360855595

Epoch: 6| Step: 6
Training loss: 3.976807117462158
Validation loss: 3.7522961708807174

Epoch: 6| Step: 7
Training loss: 4.366090774536133
Validation loss: 3.7412718572924213

Epoch: 6| Step: 8
Training loss: 2.932910919189453
Validation loss: 3.730750386432935

Epoch: 6| Step: 9
Training loss: 3.4403085708618164
Validation loss: 3.719655462490615

Epoch: 6| Step: 10
Training loss: 4.384021759033203
Validation loss: 3.70890220262671

Epoch: 6| Step: 11
Training loss: 3.0360805988311768
Validation loss: 3.6994613062950874

Epoch: 6| Step: 12
Training loss: 2.2468228340148926
Validation loss: 3.68762743601235

Epoch: 6| Step: 13
Training loss: 4.195643901824951
Validation loss: 3.6773804131374566

Epoch: 8| Step: 0
Training loss: 3.3783340454101562
Validation loss: 3.6650275722626717

Epoch: 6| Step: 1
Training loss: 2.9540352821350098
Validation loss: 3.6564032441826275

Epoch: 6| Step: 2
Training loss: 2.867396831512451
Validation loss: 3.6479531462474535

Epoch: 6| Step: 3
Training loss: 3.0566296577453613
Validation loss: 3.6405837740949405

Epoch: 6| Step: 4
Training loss: 2.4918267726898193
Validation loss: 3.632903406696935

Epoch: 6| Step: 5
Training loss: 4.612195014953613
Validation loss: 3.62646568975141

Epoch: 6| Step: 6
Training loss: 4.490178108215332
Validation loss: 3.6178595071197837

Epoch: 6| Step: 7
Training loss: 3.7933249473571777
Validation loss: 3.6104434433803765

Epoch: 6| Step: 8
Training loss: 4.026980400085449
Validation loss: 3.6023726642772718

Epoch: 6| Step: 9
Training loss: 3.8698277473449707
Validation loss: 3.595213600384292

Epoch: 6| Step: 10
Training loss: 3.746119499206543
Validation loss: 3.5882952264560166

Epoch: 6| Step: 11
Training loss: 3.3747949600219727
Validation loss: 3.5803063941258255

Epoch: 6| Step: 12
Training loss: 3.353632926940918
Validation loss: 3.571949548618768

Epoch: 6| Step: 13
Training loss: 3.242405652999878
Validation loss: 3.5630395925173195

Epoch: 9| Step: 0
Training loss: 3.154686212539673
Validation loss: 3.5553134051702355

Epoch: 6| Step: 1
Training loss: 3.3917293548583984
Validation loss: 3.547842420557494

Epoch: 6| Step: 2
Training loss: 3.8276240825653076
Validation loss: 3.5405097597388813

Epoch: 6| Step: 3
Training loss: 2.512883424758911
Validation loss: 3.5333482168054067

Epoch: 6| Step: 4
Training loss: 3.9488282203674316
Validation loss: 3.5276179826387795

Epoch: 6| Step: 5
Training loss: 4.545959949493408
Validation loss: 3.520362195148263

Epoch: 6| Step: 6
Training loss: 3.7571451663970947
Validation loss: 3.514190791755594

Epoch: 6| Step: 7
Training loss: 3.347200393676758
Validation loss: 3.504749180168234

Epoch: 6| Step: 8
Training loss: 3.8437633514404297
Validation loss: 3.4959530240745953

Epoch: 6| Step: 9
Training loss: 2.6631264686584473
Validation loss: 3.490907884413196

Epoch: 6| Step: 10
Training loss: 2.6138477325439453
Validation loss: 3.485648796122561

Epoch: 6| Step: 11
Training loss: 3.834280490875244
Validation loss: 3.478572368621826

Epoch: 6| Step: 12
Training loss: 3.3869524002075195
Validation loss: 3.4716122278603176

Epoch: 6| Step: 13
Training loss: 3.2431459426879883
Validation loss: 3.464333841877599

Epoch: 10| Step: 0
Training loss: 2.509589672088623
Validation loss: 3.45856382513559

Epoch: 6| Step: 1
Training loss: 4.52475643157959
Validation loss: 3.452231732747888

Epoch: 6| Step: 2
Training loss: 3.8495407104492188
Validation loss: 3.445707913367979

Epoch: 6| Step: 3
Training loss: 3.8283157348632812
Validation loss: 3.439921058634276

Epoch: 6| Step: 4
Training loss: 3.7794837951660156
Validation loss: 3.434207864986953

Epoch: 6| Step: 5
Training loss: 2.9792394638061523
Validation loss: 3.4279210285473893

Epoch: 6| Step: 6
Training loss: 3.1552224159240723
Validation loss: 3.4215153596734487

Epoch: 6| Step: 7
Training loss: 3.9591267108917236
Validation loss: 3.418566155177291

Epoch: 6| Step: 8
Training loss: 1.7670960426330566
Validation loss: 3.4119668775989163

Epoch: 6| Step: 9
Training loss: 2.9030556678771973
Validation loss: 3.4096932513739473

Epoch: 6| Step: 10
Training loss: 3.239265203475952
Validation loss: 3.419240613137522

Epoch: 6| Step: 11
Training loss: 3.4877583980560303
Validation loss: 3.401134398675734

Epoch: 6| Step: 12
Training loss: 3.4680776596069336
Validation loss: 3.3949888624170774

Epoch: 6| Step: 13
Training loss: 4.060882091522217
Validation loss: 3.3910904751029065

Epoch: 11| Step: 0
Training loss: 3.396488904953003
Validation loss: 3.388805353513328

Epoch: 6| Step: 1
Training loss: 4.210420608520508
Validation loss: 3.3843503869989866

Epoch: 6| Step: 2
Training loss: 3.2257213592529297
Validation loss: 3.382587214951874

Epoch: 6| Step: 3
Training loss: 2.852099895477295
Validation loss: 3.377979552874001

Epoch: 6| Step: 4
Training loss: 2.307445526123047
Validation loss: 3.3760197598447084

Epoch: 6| Step: 5
Training loss: 2.494997978210449
Validation loss: 3.3686565122296734

Epoch: 6| Step: 6
Training loss: 4.2885637283325195
Validation loss: 3.3668359735960602

Epoch: 6| Step: 7
Training loss: 2.5193514823913574
Validation loss: 3.359719322573754

Epoch: 6| Step: 8
Training loss: 3.0580971240997314
Validation loss: 3.358365297317505

Epoch: 6| Step: 9
Training loss: 4.799936771392822
Validation loss: 3.354509758692916

Epoch: 6| Step: 10
Training loss: 3.619900941848755
Validation loss: 3.350417565273982

Epoch: 6| Step: 11
Training loss: 3.2949862480163574
Validation loss: 3.346671171085809

Epoch: 6| Step: 12
Training loss: 3.272871494293213
Validation loss: 3.34395827785615

Epoch: 6| Step: 13
Training loss: 3.0074357986450195
Validation loss: 3.337844171831685

Epoch: 12| Step: 0
Training loss: 3.7186808586120605
Validation loss: 3.334715509927401

Epoch: 6| Step: 1
Training loss: 2.9559168815612793
Validation loss: 3.331415284064508

Epoch: 6| Step: 2
Training loss: 2.7905917167663574
Validation loss: 3.328813898947931

Epoch: 6| Step: 3
Training loss: 2.530101776123047
Validation loss: 3.3275863124478247

Epoch: 6| Step: 4
Training loss: 3.0038914680480957
Validation loss: 3.326094837598903

Epoch: 6| Step: 5
Training loss: 3.786214828491211
Validation loss: 3.319153565232472

Epoch: 6| Step: 6
Training loss: 3.374366044998169
Validation loss: 3.317079056975662

Epoch: 6| Step: 7
Training loss: 4.028831958770752
Validation loss: 3.3122680161588933

Epoch: 6| Step: 8
Training loss: 4.27249002456665
Validation loss: 3.30765789042237

Epoch: 6| Step: 9
Training loss: 3.247511386871338
Validation loss: 3.3062887858319026

Epoch: 6| Step: 10
Training loss: 2.3643250465393066
Validation loss: 3.30694511372556

Epoch: 6| Step: 11
Training loss: 3.103545665740967
Validation loss: 3.336870862591651

Epoch: 6| Step: 12
Training loss: 3.801352024078369
Validation loss: 3.3128631063686904

Epoch: 6| Step: 13
Training loss: 2.767592191696167
Validation loss: 3.2956476083365818

Epoch: 13| Step: 0
Training loss: 2.744490146636963
Validation loss: 3.2917332315957673

Epoch: 6| Step: 1
Training loss: 2.3267979621887207
Validation loss: 3.2930635995762323

Epoch: 6| Step: 2
Training loss: 3.9955315589904785
Validation loss: 3.2961473285510974

Epoch: 6| Step: 3
Training loss: 2.3420841693878174
Validation loss: 3.2931805528620237

Epoch: 6| Step: 4
Training loss: 2.9949851036071777
Validation loss: 3.2920321905484764

Epoch: 6| Step: 5
Training loss: 3.5385756492614746
Validation loss: 3.2890466105553413

Epoch: 6| Step: 6
Training loss: 3.544218063354492
Validation loss: 3.286597272401215

Epoch: 6| Step: 7
Training loss: 4.614770412445068
Validation loss: 3.2840393973935034

Epoch: 6| Step: 8
Training loss: 2.7839467525482178
Validation loss: 3.2836585762680217

Epoch: 6| Step: 9
Training loss: 4.476806640625
Validation loss: 3.276194444266699

Epoch: 6| Step: 10
Training loss: 3.279024600982666
Validation loss: 3.274263922886182

Epoch: 6| Step: 11
Training loss: 3.365628242492676
Validation loss: 3.268754236159786

Epoch: 6| Step: 12
Training loss: 2.908243179321289
Validation loss: 3.266212653088313

Epoch: 6| Step: 13
Training loss: 2.24605655670166
Validation loss: 3.263259482640092

Epoch: 14| Step: 0
Training loss: 3.543701648712158
Validation loss: 3.2588116994468113

Epoch: 6| Step: 1
Training loss: 3.2680208683013916
Validation loss: 3.258906833587154

Epoch: 6| Step: 2
Training loss: 2.9414162635803223
Validation loss: 3.256884518490043

Epoch: 6| Step: 3
Training loss: 3.1450812816619873
Validation loss: 3.254859842279906

Epoch: 6| Step: 4
Training loss: 3.1986141204833984
Validation loss: 3.2503034889057116

Epoch: 6| Step: 5
Training loss: 3.593794822692871
Validation loss: 3.247295697530111

Epoch: 6| Step: 6
Training loss: 2.8188796043395996
Validation loss: 3.2439144990777455

Epoch: 6| Step: 7
Training loss: 3.778170108795166
Validation loss: 3.245329039071196

Epoch: 6| Step: 8
Training loss: 4.158473014831543
Validation loss: 3.2415830653200866

Epoch: 6| Step: 9
Training loss: 2.6451573371887207
Validation loss: 3.2352863050276235

Epoch: 6| Step: 10
Training loss: 3.064443826675415
Validation loss: 3.2325677410248788

Epoch: 6| Step: 11
Training loss: 3.34222412109375
Validation loss: 3.2303094351163475

Epoch: 6| Step: 12
Training loss: 3.233203887939453
Validation loss: 3.2287964205588064

Epoch: 6| Step: 13
Training loss: 1.8714402914047241
Validation loss: 3.227727864378242

Epoch: 15| Step: 0
Training loss: 3.112710475921631
Validation loss: 3.2237590769285798

Epoch: 6| Step: 1
Training loss: 3.2293741703033447
Validation loss: 3.2189029544912358

Epoch: 6| Step: 2
Training loss: 3.142895221710205
Validation loss: 3.2189936996788107

Epoch: 6| Step: 3
Training loss: 3.3892314434051514
Validation loss: 3.214691480000814

Epoch: 6| Step: 4
Training loss: 3.5367157459259033
Validation loss: 3.2148737702318417

Epoch: 6| Step: 5
Training loss: 3.0395984649658203
Validation loss: 3.2131718409958707

Epoch: 6| Step: 6
Training loss: 3.5026960372924805
Validation loss: 3.211002157580468

Epoch: 6| Step: 7
Training loss: 3.1845157146453857
Validation loss: 3.2062366777850735

Epoch: 6| Step: 8
Training loss: 2.9565582275390625
Validation loss: 3.2024279896930983

Epoch: 6| Step: 9
Training loss: 2.5342540740966797
Validation loss: 3.198360768697595

Epoch: 6| Step: 10
Training loss: 2.6098830699920654
Validation loss: 3.1973058664670555

Epoch: 6| Step: 11
Training loss: 3.830942392349243
Validation loss: 3.200381214900683

Epoch: 6| Step: 12
Training loss: 3.3618156909942627
Validation loss: 3.196884685947049

Epoch: 6| Step: 13
Training loss: 3.5615899562835693
Validation loss: 3.193582791154103

Epoch: 16| Step: 0
Training loss: 3.4579579830169678
Validation loss: 3.1886366669849684

Epoch: 6| Step: 1
Training loss: 3.5425171852111816
Validation loss: 3.185895473726334

Epoch: 6| Step: 2
Training loss: 2.3905348777770996
Validation loss: 3.188217801432456

Epoch: 6| Step: 3
Training loss: 3.098902702331543
Validation loss: 3.192376628998787

Epoch: 6| Step: 4
Training loss: 3.4350481033325195
Validation loss: 3.1868320280505764

Epoch: 6| Step: 5
Training loss: 2.9955363273620605
Validation loss: 3.1788851266266196

Epoch: 6| Step: 6
Training loss: 3.822441339492798
Validation loss: 3.178477748747795

Epoch: 6| Step: 7
Training loss: 2.492918014526367
Validation loss: 3.1747142832766295

Epoch: 6| Step: 8
Training loss: 3.6587331295013428
Validation loss: 3.1709813635836364

Epoch: 6| Step: 9
Training loss: 2.7088658809661865
Validation loss: 3.1692607864256828

Epoch: 6| Step: 10
Training loss: 3.274827480316162
Validation loss: 3.1648660936663227

Epoch: 6| Step: 11
Training loss: 3.839308023452759
Validation loss: 3.165530058645433

Epoch: 6| Step: 12
Training loss: 2.6265082359313965
Validation loss: 3.1589787903652398

Epoch: 6| Step: 13
Training loss: 3.1827380657196045
Validation loss: 3.1587942005485616

Epoch: 17| Step: 0
Training loss: 3.6555233001708984
Validation loss: 3.158937931060791

Epoch: 6| Step: 1
Training loss: 3.189422845840454
Validation loss: 3.1605052691633984

Epoch: 6| Step: 2
Training loss: 2.6135196685791016
Validation loss: 3.1569497226386942

Epoch: 6| Step: 3
Training loss: 2.431364059448242
Validation loss: 3.1573047125211327

Epoch: 6| Step: 4
Training loss: 3.405568838119507
Validation loss: 3.1532926200538554

Epoch: 6| Step: 5
Training loss: 3.252927780151367
Validation loss: 3.1491308443007933

Epoch: 6| Step: 6
Training loss: 3.7235798835754395
Validation loss: 3.1461764663778324

Epoch: 6| Step: 7
Training loss: 2.7995007038116455
Validation loss: 3.1409196238363943

Epoch: 6| Step: 8
Training loss: 2.2620534896850586
Validation loss: 3.14029118322557

Epoch: 6| Step: 9
Training loss: 3.1585447788238525
Validation loss: 3.1426470356602825

Epoch: 6| Step: 10
Training loss: 2.774001121520996
Validation loss: 3.1409354261172715

Epoch: 6| Step: 11
Training loss: 3.9755399227142334
Validation loss: 3.1551463988519486

Epoch: 6| Step: 12
Training loss: 3.8357393741607666
Validation loss: 3.136236265141477

Epoch: 6| Step: 13
Training loss: 3.110413074493408
Validation loss: 3.131588233414517

Epoch: 18| Step: 0
Training loss: 3.068861961364746
Validation loss: 3.1303582601649786

Epoch: 6| Step: 1
Training loss: 2.557635545730591
Validation loss: 3.1387827934757357

Epoch: 6| Step: 2
Training loss: 3.7169246673583984
Validation loss: 3.135702702306932

Epoch: 6| Step: 3
Training loss: 3.4252758026123047
Validation loss: 3.1256658543822584

Epoch: 6| Step: 4
Training loss: 3.4457168579101562
Validation loss: 3.1238672143669537

Epoch: 6| Step: 5
Training loss: 2.5982539653778076
Validation loss: 3.1238529271976923

Epoch: 6| Step: 6
Training loss: 3.1432414054870605
Validation loss: 3.1215366496834704

Epoch: 6| Step: 7
Training loss: 3.5181686878204346
Validation loss: 3.1223098898446686

Epoch: 6| Step: 8
Training loss: 3.6179327964782715
Validation loss: 3.1209323867674796

Epoch: 6| Step: 9
Training loss: 3.3694241046905518
Validation loss: 3.1168745563876246

Epoch: 6| Step: 10
Training loss: 2.7899627685546875
Validation loss: 3.112129467789845

Epoch: 6| Step: 11
Training loss: 2.5943830013275146
Validation loss: 3.1056239476767917

Epoch: 6| Step: 12
Training loss: 2.970576763153076
Validation loss: 3.102437268021286

Epoch: 6| Step: 13
Training loss: 3.216088056564331
Validation loss: 3.102902804651568

Epoch: 19| Step: 0
Training loss: 3.633190155029297
Validation loss: 3.0968986993194907

Epoch: 6| Step: 1
Training loss: 2.925938844680786
Validation loss: 3.099844947937996

Epoch: 6| Step: 2
Training loss: 2.4895737171173096
Validation loss: 3.096446737166374

Epoch: 6| Step: 3
Training loss: 2.959865093231201
Validation loss: 3.0967575298842562

Epoch: 6| Step: 4
Training loss: 2.291476249694824
Validation loss: 3.093072670762257

Epoch: 6| Step: 5
Training loss: 2.718973159790039
Validation loss: 3.0880068399572886

Epoch: 6| Step: 6
Training loss: 3.50875186920166
Validation loss: 3.0887062165044967

Epoch: 6| Step: 7
Training loss: 2.718567371368408
Validation loss: 3.083499898192703

Epoch: 6| Step: 8
Training loss: 3.522289276123047
Validation loss: 3.078558224503712

Epoch: 6| Step: 9
Training loss: 3.2993130683898926
Validation loss: 3.078143658176545

Epoch: 6| Step: 10
Training loss: 3.988053560256958
Validation loss: 3.0875060814683155

Epoch: 6| Step: 11
Training loss: 3.4349489212036133
Validation loss: 3.103421211242676

Epoch: 6| Step: 12
Training loss: 3.597334384918213
Validation loss: 3.0749179214559574

Epoch: 6| Step: 13
Training loss: 2.218062400817871
Validation loss: 3.0752746520503873

Epoch: 20| Step: 0
Training loss: 3.038475513458252
Validation loss: 3.0730589999947497

Epoch: 6| Step: 1
Training loss: 3.370405673980713
Validation loss: 3.078717634242068

Epoch: 6| Step: 2
Training loss: 3.3845162391662598
Validation loss: 3.077268954246275

Epoch: 6| Step: 3
Training loss: 3.515878200531006
Validation loss: 3.0713235485938286

Epoch: 6| Step: 4
Training loss: 2.6810779571533203
Validation loss: 3.0676188622751543

Epoch: 6| Step: 5
Training loss: 3.1733579635620117
Validation loss: 3.065799800298547

Epoch: 6| Step: 6
Training loss: 3.9522910118103027
Validation loss: 3.0633536231133247

Epoch: 6| Step: 7
Training loss: 2.2995219230651855
Validation loss: 3.063700427291214

Epoch: 6| Step: 8
Training loss: 2.760136842727661
Validation loss: 3.0624888430359545

Epoch: 6| Step: 9
Training loss: 2.5749168395996094
Validation loss: 3.0630696204400834

Epoch: 6| Step: 10
Training loss: 3.2890472412109375
Validation loss: 3.064319851577923

Epoch: 6| Step: 11
Training loss: 2.8638157844543457
Validation loss: 3.058996372325446

Epoch: 6| Step: 12
Training loss: 3.0179600715637207
Validation loss: 3.069051532335179

Epoch: 6| Step: 13
Training loss: 3.979278564453125
Validation loss: 3.0785474879767305

Epoch: 21| Step: 0
Training loss: 2.180838108062744
Validation loss: 3.054264481349658

Epoch: 6| Step: 1
Training loss: 3.693216323852539
Validation loss: 3.048131791494226

Epoch: 6| Step: 2
Training loss: 3.8841168880462646
Validation loss: 3.0478031763466458

Epoch: 6| Step: 3
Training loss: 3.0079870223999023
Validation loss: 3.048723897626323

Epoch: 6| Step: 4
Training loss: 3.030184507369995
Validation loss: 3.0503799428222

Epoch: 6| Step: 5
Training loss: 2.704002857208252
Validation loss: 3.0511744817097983

Epoch: 6| Step: 6
Training loss: 2.133772611618042
Validation loss: 3.0554326272779897

Epoch: 6| Step: 7
Training loss: 2.957163095474243
Validation loss: 3.0665829027852705

Epoch: 6| Step: 8
Training loss: 2.911044120788574
Validation loss: 3.0727864260314615

Epoch: 6| Step: 9
Training loss: 2.9388668537139893
Validation loss: 3.0501408038600797

Epoch: 6| Step: 10
Training loss: 3.5977869033813477
Validation loss: 3.049477882282708

Epoch: 6| Step: 11
Training loss: 2.9184389114379883
Validation loss: 3.059815358090144

Epoch: 6| Step: 12
Training loss: 3.298013210296631
Validation loss: 3.0592209754451627

Epoch: 6| Step: 13
Training loss: 4.889618873596191
Validation loss: 3.0473371833883305

Epoch: 22| Step: 0
Training loss: 3.6979408264160156
Validation loss: 3.042230490715273

Epoch: 6| Step: 1
Training loss: 2.417780637741089
Validation loss: 3.041459842394757

Epoch: 6| Step: 2
Training loss: 3.7451670169830322
Validation loss: 3.039814859308222

Epoch: 6| Step: 3
Training loss: 3.124380111694336
Validation loss: 3.0375564585449877

Epoch: 6| Step: 4
Training loss: 3.3837244510650635
Validation loss: 3.03546554042447

Epoch: 6| Step: 5
Training loss: 2.7050564289093018
Validation loss: 3.0333023148198284

Epoch: 6| Step: 6
Training loss: 2.803560256958008
Validation loss: 3.034941070823259

Epoch: 6| Step: 7
Training loss: 2.5077195167541504
Validation loss: 3.0313130091595393

Epoch: 6| Step: 8
Training loss: 3.968214750289917
Validation loss: 3.0283210918467534

Epoch: 6| Step: 9
Training loss: 3.609097719192505
Validation loss: 3.0270039112337175

Epoch: 6| Step: 10
Training loss: 2.4898149967193604
Validation loss: 3.024945825658819

Epoch: 6| Step: 11
Training loss: 3.1970176696777344
Validation loss: 3.023426668618315

Epoch: 6| Step: 12
Training loss: 2.791073799133301
Validation loss: 3.0222752671087942

Epoch: 6| Step: 13
Training loss: 2.474752426147461
Validation loss: 3.0205706934775076

Epoch: 23| Step: 0
Training loss: 2.001883029937744
Validation loss: 3.018996669400123

Epoch: 6| Step: 1
Training loss: 3.3794617652893066
Validation loss: 3.017356580303561

Epoch: 6| Step: 2
Training loss: 4.218160629272461
Validation loss: 3.0159078208349084

Epoch: 6| Step: 3
Training loss: 2.978663444519043
Validation loss: 3.0119383283840713

Epoch: 6| Step: 4
Training loss: 3.039278984069824
Validation loss: 3.0132647329761135

Epoch: 6| Step: 5
Training loss: 3.9860446453094482
Validation loss: 3.018136698712585

Epoch: 6| Step: 6
Training loss: 2.6818909645080566
Validation loss: 3.0113083649707097

Epoch: 6| Step: 7
Training loss: 3.073721408843994
Validation loss: 3.008537869299612

Epoch: 6| Step: 8
Training loss: 3.3602209091186523
Validation loss: 3.0083260561830256

Epoch: 6| Step: 9
Training loss: 3.115662097930908
Validation loss: 3.01113824946906

Epoch: 6| Step: 10
Training loss: 2.889976739883423
Validation loss: 3.0131760925375004

Epoch: 6| Step: 11
Training loss: 2.807335376739502
Validation loss: 3.017587995016447

Epoch: 6| Step: 12
Training loss: 3.0453476905822754
Validation loss: 3.0192603090757966

Epoch: 6| Step: 13
Training loss: 1.9487890005111694
Validation loss: 3.0134420035987772

Epoch: 24| Step: 0
Training loss: 2.953263759613037
Validation loss: 3.0153001713496383

Epoch: 6| Step: 1
Training loss: 4.444727420806885
Validation loss: 3.007454651658253

Epoch: 6| Step: 2
Training loss: 3.254988670349121
Validation loss: 2.999738557364351

Epoch: 6| Step: 3
Training loss: 3.981398344039917
Validation loss: 2.999364201740552

Epoch: 6| Step: 4
Training loss: 3.305572509765625
Validation loss: 3.002821227555634

Epoch: 6| Step: 5
Training loss: 4.190874099731445
Validation loss: 3.009278440988192

Epoch: 6| Step: 6
Training loss: 2.4992311000823975
Validation loss: 3.0024837729751424

Epoch: 6| Step: 7
Training loss: 2.4613771438598633
Validation loss: 3.0070341325575307

Epoch: 6| Step: 8
Training loss: 2.645204782485962
Validation loss: 2.9967822438927105

Epoch: 6| Step: 9
Training loss: 2.8299448490142822
Validation loss: 2.9937763803748676

Epoch: 6| Step: 10
Training loss: 2.427032709121704
Validation loss: 2.993612899575182

Epoch: 6| Step: 11
Training loss: 2.8690414428710938
Validation loss: 2.9911558986991964

Epoch: 6| Step: 12
Training loss: 2.5368480682373047
Validation loss: 2.9914930482064523

Epoch: 6| Step: 13
Training loss: 2.045017957687378
Validation loss: 2.9919214607566915

Epoch: 25| Step: 0
Training loss: 2.529745578765869
Validation loss: 2.9911843961285007

Epoch: 6| Step: 1
Training loss: 3.5095930099487305
Validation loss: 2.990912696366669

Epoch: 6| Step: 2
Training loss: 2.113513708114624
Validation loss: 2.990829788228517

Epoch: 6| Step: 3
Training loss: 3.637627363204956
Validation loss: 2.993231442666823

Epoch: 6| Step: 4
Training loss: 2.9102377891540527
Validation loss: 2.992785538396528

Epoch: 6| Step: 5
Training loss: 3.4895718097686768
Validation loss: 2.9912290521847305

Epoch: 6| Step: 6
Training loss: 2.9342989921569824
Validation loss: 2.989249239685715

Epoch: 6| Step: 7
Training loss: 3.146536350250244
Validation loss: 2.9874933329961633

Epoch: 6| Step: 8
Training loss: 3.2666497230529785
Validation loss: 2.986821702731553

Epoch: 6| Step: 9
Training loss: 2.33516788482666
Validation loss: 2.9839711368724866

Epoch: 6| Step: 10
Training loss: 3.795879364013672
Validation loss: 2.9826866503684752

Epoch: 6| Step: 11
Training loss: 2.6854374408721924
Validation loss: 2.978746468021024

Epoch: 6| Step: 12
Training loss: 3.426724672317505
Validation loss: 2.9786313118473178

Epoch: 6| Step: 13
Training loss: 2.95387601852417
Validation loss: 2.9763288523561213

Epoch: 26| Step: 0
Training loss: 2.4693527221679688
Validation loss: 2.976847046165056

Epoch: 6| Step: 1
Training loss: 2.9772989749908447
Validation loss: 2.9762990346518894

Epoch: 6| Step: 2
Training loss: 3.6194005012512207
Validation loss: 2.974965085265457

Epoch: 6| Step: 3
Training loss: 3.25223970413208
Validation loss: 2.9739038098242974

Epoch: 6| Step: 4
Training loss: 3.047323703765869
Validation loss: 2.974280990580077

Epoch: 6| Step: 5
Training loss: 2.729933738708496
Validation loss: 2.9704944369613484

Epoch: 6| Step: 6
Training loss: 2.7156224250793457
Validation loss: 2.970347373716293

Epoch: 6| Step: 7
Training loss: 3.3984322547912598
Validation loss: 2.9665006283790833

Epoch: 6| Step: 8
Training loss: 2.5498244762420654
Validation loss: 2.966930022803686

Epoch: 6| Step: 9
Training loss: 3.727391242980957
Validation loss: 2.9663914865063084

Epoch: 6| Step: 10
Training loss: 3.312386989593506
Validation loss: 2.9676499443669475

Epoch: 6| Step: 11
Training loss: 2.8862829208374023
Validation loss: 2.9659177410987114

Epoch: 6| Step: 12
Training loss: 3.1846437454223633
Validation loss: 2.9649694401730775

Epoch: 6| Step: 13
Training loss: 2.5239744186401367
Validation loss: 2.9676011890493412

Epoch: 27| Step: 0
Training loss: 2.6995701789855957
Validation loss: 2.964713560637607

Epoch: 6| Step: 1
Training loss: 3.0415804386138916
Validation loss: 2.9631927244124876

Epoch: 6| Step: 2
Training loss: 3.535238265991211
Validation loss: 2.9647761826874106

Epoch: 6| Step: 3
Training loss: 3.43764066696167
Validation loss: 2.960822182316934

Epoch: 6| Step: 4
Training loss: 2.5278611183166504
Validation loss: 2.961938355558662

Epoch: 6| Step: 5
Training loss: 3.75795316696167
Validation loss: 2.9598420153382006

Epoch: 6| Step: 6
Training loss: 2.2263760566711426
Validation loss: 2.958416533726518

Epoch: 6| Step: 7
Training loss: 3.2852888107299805
Validation loss: 2.957074765236147

Epoch: 6| Step: 8
Training loss: 2.934459686279297
Validation loss: 2.9561882736862346

Epoch: 6| Step: 9
Training loss: 2.0955681800842285
Validation loss: 2.9580192873554845

Epoch: 6| Step: 10
Training loss: 4.413595676422119
Validation loss: 2.9589069709982923

Epoch: 6| Step: 11
Training loss: 2.4268274307250977
Validation loss: 2.9564274716120895

Epoch: 6| Step: 12
Training loss: 3.013075828552246
Validation loss: 2.9541264913415395

Epoch: 6| Step: 13
Training loss: 3.226047992706299
Validation loss: 2.9523782627556914

Epoch: 28| Step: 0
Training loss: 3.207156181335449
Validation loss: 2.953169943183981

Epoch: 6| Step: 1
Training loss: 2.648366689682007
Validation loss: 2.953087140155095

Epoch: 6| Step: 2
Training loss: 2.9987454414367676
Validation loss: 2.9501019139443674

Epoch: 6| Step: 3
Training loss: 3.22153902053833
Validation loss: 2.948746186430736

Epoch: 6| Step: 4
Training loss: 3.399143934249878
Validation loss: 2.9496083413400958

Epoch: 6| Step: 5
Training loss: 2.9087672233581543
Validation loss: 2.9479419262178483

Epoch: 6| Step: 6
Training loss: 3.135499954223633
Validation loss: 2.9496054444261777

Epoch: 6| Step: 7
Training loss: 2.7098941802978516
Validation loss: 2.951802974106163

Epoch: 6| Step: 8
Training loss: 3.428842306137085
Validation loss: 2.9594082473426737

Epoch: 6| Step: 9
Training loss: 3.108785629272461
Validation loss: 2.9592667830887662

Epoch: 6| Step: 10
Training loss: 2.1312649250030518
Validation loss: 2.9533217953097437

Epoch: 6| Step: 11
Training loss: 3.6026673316955566
Validation loss: 2.949475642173521

Epoch: 6| Step: 12
Training loss: 2.5436339378356934
Validation loss: 2.9496900189307427

Epoch: 6| Step: 13
Training loss: 3.6963915824890137
Validation loss: 2.946438540694534

Epoch: 29| Step: 0
Training loss: 2.9020726680755615
Validation loss: 2.9465742136842463

Epoch: 6| Step: 1
Training loss: 2.6208066940307617
Validation loss: 2.9422739603186168

Epoch: 6| Step: 2
Training loss: 2.7165231704711914
Validation loss: 2.9493282354006203

Epoch: 6| Step: 3
Training loss: 3.038379669189453
Validation loss: 2.943183232379216

Epoch: 6| Step: 4
Training loss: 3.0844855308532715
Validation loss: 2.938676962288477

Epoch: 6| Step: 5
Training loss: 2.9234910011291504
Validation loss: 2.9385961409538024

Epoch: 6| Step: 6
Training loss: 2.3061141967773438
Validation loss: 2.938937307685934

Epoch: 6| Step: 7
Training loss: 3.2772130966186523
Validation loss: 2.9427949741322506

Epoch: 6| Step: 8
Training loss: 3.5408735275268555
Validation loss: 2.9426485723064792

Epoch: 6| Step: 9
Training loss: 3.4950432777404785
Validation loss: 2.9423481648968113

Epoch: 6| Step: 10
Training loss: 3.3258774280548096
Validation loss: 2.9418431353825394

Epoch: 6| Step: 11
Training loss: 2.6388254165649414
Validation loss: 2.9376816185571815

Epoch: 6| Step: 12
Training loss: 2.735118865966797
Validation loss: 2.938650285044024

Epoch: 6| Step: 13
Training loss: 4.331801414489746
Validation loss: 2.9365963628215175

Epoch: 30| Step: 0
Training loss: 2.7089672088623047
Validation loss: 2.9350626904477357

Epoch: 6| Step: 1
Training loss: 3.080825090408325
Validation loss: 2.9342849075153308

Epoch: 6| Step: 2
Training loss: 3.567958354949951
Validation loss: 2.932725234698224

Epoch: 6| Step: 3
Training loss: 3.362565279006958
Validation loss: 2.931126238197409

Epoch: 6| Step: 4
Training loss: 3.278665065765381
Validation loss: 2.93049709258541

Epoch: 6| Step: 5
Training loss: 2.4088778495788574
Validation loss: 2.929346158940305

Epoch: 6| Step: 6
Training loss: 3.0539498329162598
Validation loss: 2.927260321955527

Epoch: 6| Step: 7
Training loss: 3.7652063369750977
Validation loss: 2.9257624969687512

Epoch: 6| Step: 8
Training loss: 2.812420129776001
Validation loss: 2.9246584523108696

Epoch: 6| Step: 9
Training loss: 2.561408042907715
Validation loss: 2.92502853690937

Epoch: 6| Step: 10
Training loss: 2.663569927215576
Validation loss: 2.9230120669129076

Epoch: 6| Step: 11
Training loss: 3.0388455390930176
Validation loss: 2.925744566866147

Epoch: 6| Step: 12
Training loss: 2.9765400886535645
Validation loss: 2.921195589086061

Epoch: 6| Step: 13
Training loss: 2.8748350143432617
Validation loss: 2.9367025103620303

Epoch: 31| Step: 0
Training loss: 3.4087884426116943
Validation loss: 2.9836556706377255

Epoch: 6| Step: 1
Training loss: 3.3942136764526367
Validation loss: 2.935013763366207

Epoch: 6| Step: 2
Training loss: 2.798412799835205
Validation loss: 2.91611990621013

Epoch: 6| Step: 3
Training loss: 3.5556650161743164
Validation loss: 2.9266264336083525

Epoch: 6| Step: 4
Training loss: 2.4322333335876465
Validation loss: 2.9476780558145173

Epoch: 6| Step: 5
Training loss: 3.7770771980285645
Validation loss: 2.950413696227535

Epoch: 6| Step: 6
Training loss: 3.6823244094848633
Validation loss: 2.943600164946689

Epoch: 6| Step: 7
Training loss: 3.234779119491577
Validation loss: 2.9416179938982894

Epoch: 6| Step: 8
Training loss: 2.5728399753570557
Validation loss: 2.9262668804455827

Epoch: 6| Step: 9
Training loss: 1.7436375617980957
Validation loss: 2.92215992558387

Epoch: 6| Step: 10
Training loss: 2.559420347213745
Validation loss: 2.9205631491958455

Epoch: 6| Step: 11
Training loss: 3.3412609100341797
Validation loss: 2.916978782223117

Epoch: 6| Step: 12
Training loss: 2.7950639724731445
Validation loss: 2.919624584977345

Epoch: 6| Step: 13
Training loss: 2.8819491863250732
Validation loss: 2.914833435448267

Epoch: 32| Step: 0
Training loss: 3.2647595405578613
Validation loss: 2.9148026486878753

Epoch: 6| Step: 1
Training loss: 3.0031051635742188
Validation loss: 2.9144990598001788

Epoch: 6| Step: 2
Training loss: 3.119993209838867
Validation loss: 2.912516245277979

Epoch: 6| Step: 3
Training loss: 3.3985018730163574
Validation loss: 2.9147339174824376

Epoch: 6| Step: 4
Training loss: 3.3594086170196533
Validation loss: 2.9129294964574997

Epoch: 6| Step: 5
Training loss: 2.563624143600464
Validation loss: 2.909586614178073

Epoch: 6| Step: 6
Training loss: 2.5604248046875
Validation loss: 2.91055146596765

Epoch: 6| Step: 7
Training loss: 3.3035387992858887
Validation loss: 2.9109746179273053

Epoch: 6| Step: 8
Training loss: 2.176142454147339
Validation loss: 2.9096181418306086

Epoch: 6| Step: 9
Training loss: 3.8296945095062256
Validation loss: 2.911374356157036

Epoch: 6| Step: 10
Training loss: 2.3814268112182617
Validation loss: 2.91021974625126

Epoch: 6| Step: 11
Training loss: 3.61824369430542
Validation loss: 2.9098080409470426

Epoch: 6| Step: 12
Training loss: 2.452259063720703
Validation loss: 2.911125962452222

Epoch: 6| Step: 13
Training loss: 2.986793041229248
Validation loss: 2.910497793587305

Epoch: 33| Step: 0
Training loss: 3.5902562141418457
Validation loss: 2.905722218175088

Epoch: 6| Step: 1
Training loss: 2.387716770172119
Validation loss: 2.903815043869839

Epoch: 6| Step: 2
Training loss: 2.123481273651123
Validation loss: 2.9019044522316224

Epoch: 6| Step: 3
Training loss: 2.8336129188537598
Validation loss: 2.896137273439797

Epoch: 6| Step: 4
Training loss: 2.6243603229522705
Validation loss: 2.895496504281157

Epoch: 6| Step: 5
Training loss: 3.2186107635498047
Validation loss: 2.895820909930814

Epoch: 6| Step: 6
Training loss: 2.9121344089508057
Validation loss: 2.893057348907635

Epoch: 6| Step: 7
Training loss: 2.9180045127868652
Validation loss: 2.8921803223189486

Epoch: 6| Step: 8
Training loss: 3.064384937286377
Validation loss: 2.891593046085809

Epoch: 6| Step: 9
Training loss: 2.5358333587646484
Validation loss: 2.8889382449529504

Epoch: 6| Step: 10
Training loss: 2.4915285110473633
Validation loss: 2.8898992640997774

Epoch: 6| Step: 11
Training loss: 4.762946605682373
Validation loss: 2.888440232123098

Epoch: 6| Step: 12
Training loss: 3.4759809970855713
Validation loss: 2.886594787720711

Epoch: 6| Step: 13
Training loss: 2.8933470249176025
Validation loss: 2.8828331603798816

Epoch: 34| Step: 0
Training loss: 2.725857973098755
Validation loss: 2.882578275536978

Epoch: 6| Step: 1
Training loss: 2.6746020317077637
Validation loss: 2.884490313068513

Epoch: 6| Step: 2
Training loss: 3.4759023189544678
Validation loss: 2.879277349800192

Epoch: 6| Step: 3
Training loss: 2.788975477218628
Validation loss: 2.880431264959356

Epoch: 6| Step: 4
Training loss: 3.2334606647491455
Validation loss: 2.881016477461784

Epoch: 6| Step: 5
Training loss: 3.464791774749756
Validation loss: 2.877652778420397

Epoch: 6| Step: 6
Training loss: 3.2357969284057617
Validation loss: 2.8803032598187848

Epoch: 6| Step: 7
Training loss: 1.8989967107772827
Validation loss: 2.88469333546136

Epoch: 6| Step: 8
Training loss: 1.9758275747299194
Validation loss: 2.88261442799722

Epoch: 6| Step: 9
Training loss: 2.95310115814209
Validation loss: 2.874924052146173

Epoch: 6| Step: 10
Training loss: 3.268899440765381
Validation loss: 2.8746443563891995

Epoch: 6| Step: 11
Training loss: 3.4108965396881104
Validation loss: 2.8823114492559947

Epoch: 6| Step: 12
Training loss: 3.7358310222625732
Validation loss: 2.8937490652966242

Epoch: 6| Step: 13
Training loss: 2.8193750381469727
Validation loss: 2.8915539531297583

Epoch: 35| Step: 0
Training loss: 2.360945224761963
Validation loss: 2.8740904613207747

Epoch: 6| Step: 1
Training loss: 2.257025718688965
Validation loss: 2.8729730498406196

Epoch: 6| Step: 2
Training loss: 2.7346556186676025
Validation loss: 2.8660766104216218

Epoch: 6| Step: 3
Training loss: 3.0077314376831055
Validation loss: 2.8706707646769862

Epoch: 6| Step: 4
Training loss: 3.9140090942382812
Validation loss: 2.869261667292605

Epoch: 6| Step: 5
Training loss: 3.152167320251465
Validation loss: 2.874337601405318

Epoch: 6| Step: 6
Training loss: 3.897299289703369
Validation loss: 2.873341780836864

Epoch: 6| Step: 7
Training loss: 3.553854465484619
Validation loss: 2.8697983834051315

Epoch: 6| Step: 8
Training loss: 2.791506767272949
Validation loss: 2.870610296085317

Epoch: 6| Step: 9
Training loss: 2.796670913696289
Validation loss: 2.867628597444104

Epoch: 6| Step: 10
Training loss: 2.823591470718384
Validation loss: 2.8742341379965506

Epoch: 6| Step: 11
Training loss: 2.68697452545166
Validation loss: 2.8740986111343547

Epoch: 6| Step: 12
Training loss: 2.687365770339966
Validation loss: 2.870900538659865

Epoch: 6| Step: 13
Training loss: 3.0682592391967773
Validation loss: 2.8694262504577637

Epoch: 36| Step: 0
Training loss: 3.044206142425537
Validation loss: 2.8706549931597967

Epoch: 6| Step: 1
Training loss: 3.275350570678711
Validation loss: 2.8701194435037594

Epoch: 6| Step: 2
Training loss: 2.5130538940429688
Validation loss: 2.8720381388100247

Epoch: 6| Step: 3
Training loss: 3.475712299346924
Validation loss: 2.8701750668146278

Epoch: 6| Step: 4
Training loss: 2.8588356971740723
Validation loss: 2.8689926978080504

Epoch: 6| Step: 5
Training loss: 1.9811816215515137
Validation loss: 2.8663331744491414

Epoch: 6| Step: 6
Training loss: 3.559957504272461
Validation loss: 2.8671519499953075

Epoch: 6| Step: 7
Training loss: 3.3256192207336426
Validation loss: 2.8675118338677192

Epoch: 6| Step: 8
Training loss: 3.1169369220733643
Validation loss: 2.8654341877147718

Epoch: 6| Step: 9
Training loss: 2.2954368591308594
Validation loss: 2.8632236296130764

Epoch: 6| Step: 10
Training loss: 3.1989855766296387
Validation loss: 2.8597665550888225

Epoch: 6| Step: 11
Training loss: 3.7936666011810303
Validation loss: 2.858701149622599

Epoch: 6| Step: 12
Training loss: 2.1982226371765137
Validation loss: 2.8622213332883772

Epoch: 6| Step: 13
Training loss: 2.943694591522217
Validation loss: 2.861109254180744

Epoch: 37| Step: 0
Training loss: 3.0827717781066895
Validation loss: 2.8611751756360455

Epoch: 6| Step: 1
Training loss: 1.797067642211914
Validation loss: 2.8607472245411207

Epoch: 6| Step: 2
Training loss: 3.3397464752197266
Validation loss: 2.857725481833181

Epoch: 6| Step: 3
Training loss: 2.545889377593994
Validation loss: 2.857872878351519

Epoch: 6| Step: 4
Training loss: 2.6499953269958496
Validation loss: 2.861458468180831

Epoch: 6| Step: 5
Training loss: 2.67964243888855
Validation loss: 2.8591128421086136

Epoch: 6| Step: 6
Training loss: 3.6350200176239014
Validation loss: 2.854266874251827

Epoch: 6| Step: 7
Training loss: 3.2298219203948975
Validation loss: 2.8542613419153358

Epoch: 6| Step: 8
Training loss: 2.902860403060913
Validation loss: 2.8509769644788516

Epoch: 6| Step: 9
Training loss: 4.232941627502441
Validation loss: 2.8522378193434847

Epoch: 6| Step: 10
Training loss: 3.5776641368865967
Validation loss: 2.853394323779691

Epoch: 6| Step: 11
Training loss: 1.9669957160949707
Validation loss: 2.8532173249029342

Epoch: 6| Step: 12
Training loss: 2.5621023178100586
Validation loss: 2.858696347923689

Epoch: 6| Step: 13
Training loss: 3.619128704071045
Validation loss: 2.8637624658564085

Epoch: 38| Step: 0
Training loss: 3.6800456047058105
Validation loss: 2.86089148059968

Epoch: 6| Step: 1
Training loss: 2.7803702354431152
Validation loss: 2.8615771442331295

Epoch: 6| Step: 2
Training loss: 3.006260395050049
Validation loss: 2.8576869349325857

Epoch: 6| Step: 3
Training loss: 2.526498317718506
Validation loss: 2.8548479669837543

Epoch: 6| Step: 4
Training loss: 2.283257007598877
Validation loss: 2.8563279721044723

Epoch: 6| Step: 5
Training loss: 2.9758095741271973
Validation loss: 2.850100612127653

Epoch: 6| Step: 6
Training loss: 4.09846830368042
Validation loss: 2.852322486139113

Epoch: 6| Step: 7
Training loss: 3.4020252227783203
Validation loss: 2.8501477497880177

Epoch: 6| Step: 8
Training loss: 1.9751744270324707
Validation loss: 2.8474176981115855

Epoch: 6| Step: 9
Training loss: 2.8293814659118652
Validation loss: 2.8494848948653027

Epoch: 6| Step: 10
Training loss: 3.0287201404571533
Validation loss: 2.8513478027876986

Epoch: 6| Step: 11
Training loss: 3.376465320587158
Validation loss: 2.8551012290421354

Epoch: 6| Step: 12
Training loss: 2.210228443145752
Validation loss: 2.8514704165920133

Epoch: 6| Step: 13
Training loss: 3.571803092956543
Validation loss: 2.8517761538105626

Epoch: 39| Step: 0
Training loss: 3.2913336753845215
Validation loss: 2.8547616927854476

Epoch: 6| Step: 1
Training loss: 3.1474204063415527
Validation loss: 2.858928911147579

Epoch: 6| Step: 2
Training loss: 2.7738397121429443
Validation loss: 2.868725166525892

Epoch: 6| Step: 3
Training loss: 2.6939644813537598
Validation loss: 2.8604251902590514

Epoch: 6| Step: 4
Training loss: 3.169236660003662
Validation loss: 2.858172957615186

Epoch: 6| Step: 5
Training loss: 3.8466527462005615
Validation loss: 2.8678141511896604

Epoch: 6| Step: 6
Training loss: 2.666109561920166
Validation loss: 2.8564928116336947

Epoch: 6| Step: 7
Training loss: 2.678928852081299
Validation loss: 2.8547285244029057

Epoch: 6| Step: 8
Training loss: 2.825312614440918
Validation loss: 2.855733533059397

Epoch: 6| Step: 9
Training loss: 2.788364887237549
Validation loss: 2.849718721964026

Epoch: 6| Step: 10
Training loss: 2.999020576477051
Validation loss: 2.8479699883409726

Epoch: 6| Step: 11
Training loss: 2.6729698181152344
Validation loss: 2.8461468732485207

Epoch: 6| Step: 12
Training loss: 2.949427366256714
Validation loss: 2.843823130412768

Epoch: 6| Step: 13
Training loss: 2.9298784732818604
Validation loss: 2.843346380418347

Epoch: 40| Step: 0
Training loss: 3.042923927307129
Validation loss: 2.8435629516519527

Epoch: 6| Step: 1
Training loss: 2.7049789428710938
Validation loss: 2.841586648776967

Epoch: 6| Step: 2
Training loss: 2.5913076400756836
Validation loss: 2.842004358127553

Epoch: 6| Step: 3
Training loss: 3.041841983795166
Validation loss: 2.8409933582428963

Epoch: 6| Step: 4
Training loss: 2.554636240005493
Validation loss: 2.8427767984328733

Epoch: 6| Step: 5
Training loss: 4.129371643066406
Validation loss: 2.8433784618172595

Epoch: 6| Step: 6
Training loss: 2.7310495376586914
Validation loss: 2.840141962933284

Epoch: 6| Step: 7
Training loss: 2.9868102073669434
Validation loss: 2.8370447338268323

Epoch: 6| Step: 8
Training loss: 3.359163284301758
Validation loss: 2.8345049350492415

Epoch: 6| Step: 9
Training loss: 2.214784860610962
Validation loss: 2.8387499932319886

Epoch: 6| Step: 10
Training loss: 2.806119441986084
Validation loss: 2.8356441374747985

Epoch: 6| Step: 11
Training loss: 3.4219536781311035
Validation loss: 2.835614360788817

Epoch: 6| Step: 12
Training loss: 3.1075363159179688
Validation loss: 2.8351281945423414

Epoch: 6| Step: 13
Training loss: 2.471895456314087
Validation loss: 2.834462978506601

Epoch: 41| Step: 0
Training loss: 2.349630832672119
Validation loss: 2.831327376827117

Epoch: 6| Step: 1
Training loss: 3.2711868286132812
Validation loss: 2.8315135099554576

Epoch: 6| Step: 2
Training loss: 3.6346964836120605
Validation loss: 2.8315433738052205

Epoch: 6| Step: 3
Training loss: 1.9052178859710693
Validation loss: 2.832683660650766

Epoch: 6| Step: 4
Training loss: 2.7062437534332275
Validation loss: 2.828749777168356

Epoch: 6| Step: 5
Training loss: 2.843538284301758
Validation loss: 2.8331835782656105

Epoch: 6| Step: 6
Training loss: 3.0237550735473633
Validation loss: 2.831361424538397

Epoch: 6| Step: 7
Training loss: 2.0802576541900635
Validation loss: 2.831272630281346

Epoch: 6| Step: 8
Training loss: 3.335739850997925
Validation loss: 2.831284523010254

Epoch: 6| Step: 9
Training loss: 3.431964874267578
Validation loss: 2.8345599379590762

Epoch: 6| Step: 10
Training loss: 3.426259994506836
Validation loss: 2.8302717208862305

Epoch: 6| Step: 11
Training loss: 3.311807870864868
Validation loss: 2.833233617967175

Epoch: 6| Step: 12
Training loss: 2.9892306327819824
Validation loss: 2.8299528450094242

Epoch: 6| Step: 13
Training loss: 2.980698585510254
Validation loss: 2.8301634403967086

Epoch: 42| Step: 0
Training loss: 3.0531415939331055
Validation loss: 2.8308546773848997

Epoch: 6| Step: 1
Training loss: 3.1649138927459717
Validation loss: 2.8221956196651665

Epoch: 6| Step: 2
Training loss: 2.779843807220459
Validation loss: 2.823843768847886

Epoch: 6| Step: 3
Training loss: 2.463397979736328
Validation loss: 2.824068292494743

Epoch: 6| Step: 4
Training loss: 2.84778094291687
Validation loss: 2.825266348418369

Epoch: 6| Step: 5
Training loss: 2.3480563163757324
Validation loss: 2.8246488699349026

Epoch: 6| Step: 6
Training loss: 3.7307915687561035
Validation loss: 2.8239309710841023

Epoch: 6| Step: 7
Training loss: 3.0543711185455322
Validation loss: 2.8256048746006464

Epoch: 6| Step: 8
Training loss: 3.310727834701538
Validation loss: 2.82479089818975

Epoch: 6| Step: 9
Training loss: 2.8357787132263184
Validation loss: 2.823532199346891

Epoch: 6| Step: 10
Training loss: 2.6099348068237305
Validation loss: 2.826234050976333

Epoch: 6| Step: 11
Training loss: 3.5328123569488525
Validation loss: 2.8189605769290718

Epoch: 6| Step: 12
Training loss: 2.477935552597046
Validation loss: 2.827817009341332

Epoch: 6| Step: 13
Training loss: 2.95766544342041
Validation loss: 2.822376984421925

Epoch: 43| Step: 0
Training loss: 3.672407627105713
Validation loss: 2.8214695325461765

Epoch: 6| Step: 1
Training loss: 2.787966012954712
Validation loss: 2.8180369766809608

Epoch: 6| Step: 2
Training loss: 3.6392927169799805
Validation loss: 2.8169791570273777

Epoch: 6| Step: 3
Training loss: 3.4349544048309326
Validation loss: 2.8136637851756108

Epoch: 6| Step: 4
Training loss: 2.457451820373535
Validation loss: 2.8126231034596763

Epoch: 6| Step: 5
Training loss: 2.8828330039978027
Validation loss: 2.8131995149838027

Epoch: 6| Step: 6
Training loss: 2.8689920902252197
Validation loss: 2.8111329488856818

Epoch: 6| Step: 7
Training loss: 3.0692925453186035
Validation loss: 2.8157123134982203

Epoch: 6| Step: 8
Training loss: 1.7827059030532837
Validation loss: 2.8176426220965642

Epoch: 6| Step: 9
Training loss: 3.5055880546569824
Validation loss: 2.8315529643848376

Epoch: 6| Step: 10
Training loss: 3.922543525695801
Validation loss: 2.823941292301301

Epoch: 6| Step: 11
Training loss: 2.0538136959075928
Validation loss: 2.811287920962098

Epoch: 6| Step: 12
Training loss: 2.6485214233398438
Validation loss: 2.809453502778084

Epoch: 6| Step: 13
Training loss: 2.080904722213745
Validation loss: 2.811683488148515

Epoch: 44| Step: 0
Training loss: 3.1910438537597656
Validation loss: 2.810730885433894

Epoch: 6| Step: 1
Training loss: 3.3707168102264404
Validation loss: 2.815558143841323

Epoch: 6| Step: 2
Training loss: 3.192656993865967
Validation loss: 2.821464841083814

Epoch: 6| Step: 3
Training loss: 2.866955280303955
Validation loss: 2.8255908284136044

Epoch: 6| Step: 4
Training loss: 2.3516862392425537
Validation loss: 2.823043100295528

Epoch: 6| Step: 5
Training loss: 2.0955233573913574
Validation loss: 2.815496716448056

Epoch: 6| Step: 6
Training loss: 3.2698616981506348
Validation loss: 2.810366999718451

Epoch: 6| Step: 7
Training loss: 2.11904239654541
Validation loss: 2.812439754445066

Epoch: 6| Step: 8
Training loss: 3.340885639190674
Validation loss: 2.808092112182289

Epoch: 6| Step: 9
Training loss: 2.2131686210632324
Validation loss: 2.8081175742610807

Epoch: 6| Step: 10
Training loss: 3.32684326171875
Validation loss: 2.808218591956682

Epoch: 6| Step: 11
Training loss: 3.205801010131836
Validation loss: 2.805987560620872

Epoch: 6| Step: 12
Training loss: 3.9377598762512207
Validation loss: 2.806659460067749

Epoch: 6| Step: 13
Training loss: 2.294379234313965
Validation loss: 2.8086185173321794

Epoch: 45| Step: 0
Training loss: 4.068207740783691
Validation loss: 2.802850710448398

Epoch: 6| Step: 1
Training loss: 2.573336601257324
Validation loss: 2.8077402345595823

Epoch: 6| Step: 2
Training loss: 2.3637866973876953
Validation loss: 2.8079654324439263

Epoch: 6| Step: 3
Training loss: 3.158623695373535
Validation loss: 2.80138397729525

Epoch: 6| Step: 4
Training loss: 2.6940786838531494
Validation loss: 2.800505035667009

Epoch: 6| Step: 5
Training loss: 2.461381196975708
Validation loss: 2.8016783601494244

Epoch: 6| Step: 6
Training loss: 2.726710557937622
Validation loss: 2.8075704318220898

Epoch: 6| Step: 7
Training loss: 3.5597076416015625
Validation loss: 2.8079033282495316

Epoch: 6| Step: 8
Training loss: 2.588242530822754
Validation loss: 2.8079712570354505

Epoch: 6| Step: 9
Training loss: 3.9728307723999023
Validation loss: 2.798973821824597

Epoch: 6| Step: 10
Training loss: 2.4086761474609375
Validation loss: 2.803028896290769

Epoch: 6| Step: 11
Training loss: 3.0012478828430176
Validation loss: 2.7984035194561048

Epoch: 6| Step: 12
Training loss: 2.541469097137451
Validation loss: 2.799184645375898

Epoch: 6| Step: 13
Training loss: 2.899184226989746
Validation loss: 2.7977289281865603

Epoch: 46| Step: 0
Training loss: 2.7345805168151855
Validation loss: 2.7995991271029235

Epoch: 6| Step: 1
Training loss: 2.2115252017974854
Validation loss: 2.7972571208912838

Epoch: 6| Step: 2
Training loss: 3.060283899307251
Validation loss: 2.804182626867807

Epoch: 6| Step: 3
Training loss: 3.321103096008301
Validation loss: 2.8010242344230734

Epoch: 6| Step: 4
Training loss: 2.0018434524536133
Validation loss: 2.8069215743772444

Epoch: 6| Step: 5
Training loss: 2.8757894039154053
Validation loss: 2.8069473440929125

Epoch: 6| Step: 6
Training loss: 3.2792258262634277
Validation loss: 2.8068840067873717

Epoch: 6| Step: 7
Training loss: 3.628084182739258
Validation loss: 2.8039451235084125

Epoch: 6| Step: 8
Training loss: 3.4787540435791016
Validation loss: 2.8035764027667303

Epoch: 6| Step: 9
Training loss: 3.213087320327759
Validation loss: 2.7990528101562173

Epoch: 6| Step: 10
Training loss: 2.5753214359283447
Validation loss: 2.7985995328554543

Epoch: 6| Step: 11
Training loss: 2.8177566528320312
Validation loss: 2.7997075127017115

Epoch: 6| Step: 12
Training loss: 2.8223650455474854
Validation loss: 2.801600215255573

Epoch: 6| Step: 13
Training loss: 2.8515806198120117
Validation loss: 2.803312563127087

Epoch: 47| Step: 0
Training loss: 2.9052422046661377
Validation loss: 2.80506048920334

Epoch: 6| Step: 1
Training loss: 3.611273765563965
Validation loss: 2.8060031501195764

Epoch: 6| Step: 2
Training loss: 2.373605728149414
Validation loss: 2.803916262042138

Epoch: 6| Step: 3
Training loss: 3.046278238296509
Validation loss: 2.79735807705951

Epoch: 6| Step: 4
Training loss: 3.007258653640747
Validation loss: 2.791772988534743

Epoch: 6| Step: 5
Training loss: 3.361440420150757
Validation loss: 2.7951808744861233

Epoch: 6| Step: 6
Training loss: 3.70711088180542
Validation loss: 2.796031974977063

Epoch: 6| Step: 7
Training loss: 1.6875767707824707
Validation loss: 2.792615939212102

Epoch: 6| Step: 8
Training loss: 3.1219043731689453
Validation loss: 2.789750814437866

Epoch: 6| Step: 9
Training loss: 2.554736614227295
Validation loss: 2.797981431407313

Epoch: 6| Step: 10
Training loss: 2.7390666007995605
Validation loss: 2.804494744987898

Epoch: 6| Step: 11
Training loss: 3.3263626098632812
Validation loss: 2.7990884524519726

Epoch: 6| Step: 12
Training loss: 3.3906607627868652
Validation loss: 2.795675377691946

Epoch: 6| Step: 13
Training loss: 1.3916304111480713
Validation loss: 2.789721337697839

Epoch: 48| Step: 0
Training loss: 3.090407609939575
Validation loss: 2.7914109306950725

Epoch: 6| Step: 1
Training loss: 2.386918067932129
Validation loss: 2.790743561201198

Epoch: 6| Step: 2
Training loss: 3.1290717124938965
Validation loss: 2.7945773242622294

Epoch: 6| Step: 3
Training loss: 3.3673481941223145
Validation loss: 2.7930846701386156

Epoch: 6| Step: 4
Training loss: 3.120598793029785
Validation loss: 2.7929474769100064

Epoch: 6| Step: 5
Training loss: 2.702157974243164
Validation loss: 2.794126336292554

Epoch: 6| Step: 6
Training loss: 3.733980655670166
Validation loss: 2.7919353772235174

Epoch: 6| Step: 7
Training loss: 3.539287567138672
Validation loss: 2.793762565940939

Epoch: 6| Step: 8
Training loss: 2.6241745948791504
Validation loss: 2.796084646255739

Epoch: 6| Step: 9
Training loss: 3.256882429122925
Validation loss: 2.7955410070316766

Epoch: 6| Step: 10
Training loss: 2.0335817337036133
Validation loss: 2.8060280917793192

Epoch: 6| Step: 11
Training loss: 1.8969347476959229
Validation loss: 2.8086463610331216

Epoch: 6| Step: 12
Training loss: 2.71644926071167
Validation loss: 2.8105972172111593

Epoch: 6| Step: 13
Training loss: 3.624680519104004
Validation loss: 2.792731113331292

Epoch: 49| Step: 0
Training loss: 2.862474203109741
Validation loss: 2.788288706092424

Epoch: 6| Step: 1
Training loss: 2.4394869804382324
Validation loss: 2.7860841597280195

Epoch: 6| Step: 2
Training loss: 2.509675979614258
Validation loss: 2.788751845718712

Epoch: 6| Step: 3
Training loss: 3.3364524841308594
Validation loss: 2.7861724233114593

Epoch: 6| Step: 4
Training loss: 3.038574695587158
Validation loss: 2.783700627665366

Epoch: 6| Step: 5
Training loss: 3.4299464225769043
Validation loss: 2.782586759136569

Epoch: 6| Step: 6
Training loss: 3.202004909515381
Validation loss: 2.786374997067195

Epoch: 6| Step: 7
Training loss: 2.8665144443511963
Validation loss: 2.787598389451222

Epoch: 6| Step: 8
Training loss: 2.149003505706787
Validation loss: 2.78671452306932

Epoch: 6| Step: 9
Training loss: 2.830470085144043
Validation loss: 2.7816082431424047

Epoch: 6| Step: 10
Training loss: 2.762064218521118
Validation loss: 2.7803129278203493

Epoch: 6| Step: 11
Training loss: 3.0240378379821777
Validation loss: 2.7883807074639106

Epoch: 6| Step: 12
Training loss: 3.4018325805664062
Validation loss: 2.8057347087449926

Epoch: 6| Step: 13
Training loss: 3.0100204944610596
Validation loss: 2.7987856172746226

Epoch: 50| Step: 0
Training loss: 2.923248291015625
Validation loss: 2.7921651588973178

Epoch: 6| Step: 1
Training loss: 2.4512624740600586
Validation loss: 2.7976242547394126

Epoch: 6| Step: 2
Training loss: 3.074374198913574
Validation loss: 2.8090905707369567

Epoch: 6| Step: 3
Training loss: 4.041624069213867
Validation loss: 2.8037302545321885

Epoch: 6| Step: 4
Training loss: 2.234166145324707
Validation loss: 2.7961580522598757

Epoch: 6| Step: 5
Training loss: 3.205230712890625
Validation loss: 2.793927172178863

Epoch: 6| Step: 6
Training loss: 2.3073854446411133
Validation loss: 2.7989232053038893

Epoch: 6| Step: 7
Training loss: 3.266964912414551
Validation loss: 2.797691640033517

Epoch: 6| Step: 8
Training loss: 2.0745456218719482
Validation loss: 2.799000446514417

Epoch: 6| Step: 9
Training loss: 2.4954867362976074
Validation loss: 2.797082867673648

Epoch: 6| Step: 10
Training loss: 2.603393793106079
Validation loss: 2.795627501703078

Epoch: 6| Step: 11
Training loss: 3.452518939971924
Validation loss: 2.7873412486045592

Epoch: 6| Step: 12
Training loss: 3.3291869163513184
Validation loss: 2.785463692039572

Epoch: 6| Step: 13
Training loss: 3.6813888549804688
Validation loss: 2.781742808639362

Epoch: 51| Step: 0
Training loss: 2.6685614585876465
Validation loss: 2.7836044988324566

Epoch: 6| Step: 1
Training loss: 2.6615986824035645
Validation loss: 2.7829613301061813

Epoch: 6| Step: 2
Training loss: 3.2707855701446533
Validation loss: 2.7838361622184835

Epoch: 6| Step: 3
Training loss: 3.1359710693359375
Validation loss: 2.7920223230956704

Epoch: 6| Step: 4
Training loss: 3.061511278152466
Validation loss: 2.794331878744146

Epoch: 6| Step: 5
Training loss: 2.987022876739502
Validation loss: 2.787101214931857

Epoch: 6| Step: 6
Training loss: 3.4419264793395996
Validation loss: 2.784809386858376

Epoch: 6| Step: 7
Training loss: 2.9080076217651367
Validation loss: 2.7871752605643323

Epoch: 6| Step: 8
Training loss: 2.8523664474487305
Validation loss: 2.781584355138963

Epoch: 6| Step: 9
Training loss: 2.243455171585083
Validation loss: 2.7847515357437955

Epoch: 6| Step: 10
Training loss: 2.6980745792388916
Validation loss: 2.7811638924383346

Epoch: 6| Step: 11
Training loss: 2.9898769855499268
Validation loss: 2.7801276124933714

Epoch: 6| Step: 12
Training loss: 3.3418428897857666
Validation loss: 2.781201611283005

Epoch: 6| Step: 13
Training loss: 2.209886074066162
Validation loss: 2.7829596047760337

Epoch: 52| Step: 0
Training loss: 2.295635223388672
Validation loss: 2.785268832278508

Epoch: 6| Step: 1
Training loss: 2.388537883758545
Validation loss: 2.7810574244427424

Epoch: 6| Step: 2
Training loss: 2.656064033508301
Validation loss: 2.7815030390216458

Epoch: 6| Step: 3
Training loss: 2.7136032581329346
Validation loss: 2.783821157229844

Epoch: 6| Step: 4
Training loss: 2.6194677352905273
Validation loss: 2.782174635958928

Epoch: 6| Step: 5
Training loss: 2.7229275703430176
Validation loss: 2.7814351615085395

Epoch: 6| Step: 6
Training loss: 3.506695508956909
Validation loss: 2.7790872640507196

Epoch: 6| Step: 7
Training loss: 3.1221883296966553
Validation loss: 2.7793012203708773

Epoch: 6| Step: 8
Training loss: 2.8473615646362305
Validation loss: 2.7815891004377797

Epoch: 6| Step: 9
Training loss: 3.609234094619751
Validation loss: 2.7847743803454983

Epoch: 6| Step: 10
Training loss: 3.3006789684295654
Validation loss: 2.782059154202861

Epoch: 6| Step: 11
Training loss: 3.3853864669799805
Validation loss: 2.7803677435844176

Epoch: 6| Step: 12
Training loss: 2.4433436393737793
Validation loss: 2.781420789739137

Epoch: 6| Step: 13
Training loss: 3.2511513233184814
Validation loss: 2.785319838472592

Epoch: 53| Step: 0
Training loss: 2.184218645095825
Validation loss: 2.790968551430651

Epoch: 6| Step: 1
Training loss: 2.8989109992980957
Validation loss: 2.7944952441800024

Epoch: 6| Step: 2
Training loss: 2.2255897521972656
Validation loss: 2.7980451558225896

Epoch: 6| Step: 3
Training loss: 3.3187499046325684
Validation loss: 2.797795764861568

Epoch: 6| Step: 4
Training loss: 2.6676430702209473
Validation loss: 2.79089032706394

Epoch: 6| Step: 5
Training loss: 2.687474012374878
Validation loss: 2.7843531690618044

Epoch: 6| Step: 6
Training loss: 3.927243947982788
Validation loss: 2.7824435644252326

Epoch: 6| Step: 7
Training loss: 2.8872225284576416
Validation loss: 2.785029413879559

Epoch: 6| Step: 8
Training loss: 3.249668598175049
Validation loss: 2.7815192822487123

Epoch: 6| Step: 9
Training loss: 3.343723773956299
Validation loss: 2.7782756128618793

Epoch: 6| Step: 10
Training loss: 2.9907784461975098
Validation loss: 2.780014286759079

Epoch: 6| Step: 11
Training loss: 2.2976250648498535
Validation loss: 2.7721954750758346

Epoch: 6| Step: 12
Training loss: 2.8287954330444336
Validation loss: 2.7716767044477564

Epoch: 6| Step: 13
Training loss: 3.3459420204162598
Validation loss: 2.771866247218142

Epoch: 54| Step: 0
Training loss: 3.1009750366210938
Validation loss: 2.7712959422860095

Epoch: 6| Step: 1
Training loss: 2.1658849716186523
Validation loss: 2.7711012901798373

Epoch: 6| Step: 2
Training loss: 2.7856297492980957
Validation loss: 2.7736065695362706

Epoch: 6| Step: 3
Training loss: 3.0261993408203125
Validation loss: 2.7715171178181968

Epoch: 6| Step: 4
Training loss: 3.109386444091797
Validation loss: 2.7717263160213346

Epoch: 6| Step: 5
Training loss: 3.087834119796753
Validation loss: 2.772920085537818

Epoch: 6| Step: 6
Training loss: 3.460545063018799
Validation loss: 2.773168379260648

Epoch: 6| Step: 7
Training loss: 2.7603089809417725
Validation loss: 2.771282519063642

Epoch: 6| Step: 8
Training loss: 2.38986873626709
Validation loss: 2.7717933090784217

Epoch: 6| Step: 9
Training loss: 3.21445369720459
Validation loss: 2.771404168939078

Epoch: 6| Step: 10
Training loss: 3.5653889179229736
Validation loss: 2.7743872391280306

Epoch: 6| Step: 11
Training loss: 2.118215560913086
Validation loss: 2.774688107993013

Epoch: 6| Step: 12
Training loss: 3.004404067993164
Validation loss: 2.7778785946548625

Epoch: 6| Step: 13
Training loss: 2.835239887237549
Validation loss: 2.7789463381613455

Epoch: 55| Step: 0
Training loss: 3.6016037464141846
Validation loss: 2.777921317726053

Epoch: 6| Step: 1
Training loss: 2.4722092151641846
Validation loss: 2.775997838666362

Epoch: 6| Step: 2
Training loss: 2.809441089630127
Validation loss: 2.780093095635855

Epoch: 6| Step: 3
Training loss: 2.4626259803771973
Validation loss: 2.7820044948208715

Epoch: 6| Step: 4
Training loss: 3.0966362953186035
Validation loss: 2.782834570894959

Epoch: 6| Step: 5
Training loss: 3.3045127391815186
Validation loss: 2.7831235547219553

Epoch: 6| Step: 6
Training loss: 2.504207134246826
Validation loss: 2.7776963531330066

Epoch: 6| Step: 7
Training loss: 3.5559825897216797
Validation loss: 2.7842489211790022

Epoch: 6| Step: 8
Training loss: 3.411487102508545
Validation loss: 2.779485530750726

Epoch: 6| Step: 9
Training loss: 2.3621933460235596
Validation loss: 2.778477273961549

Epoch: 6| Step: 10
Training loss: 2.668156623840332
Validation loss: 2.772532550237512

Epoch: 6| Step: 11
Training loss: 2.3812947273254395
Validation loss: 2.7707368353361725

Epoch: 6| Step: 12
Training loss: 2.994781970977783
Validation loss: 2.769372496553647

Epoch: 6| Step: 13
Training loss: 3.0824553966522217
Validation loss: 2.769990300619474

Epoch: 56| Step: 0
Training loss: 2.7823586463928223
Validation loss: 2.76694397259784

Epoch: 6| Step: 1
Training loss: 2.986969470977783
Validation loss: 2.766352914994763

Epoch: 6| Step: 2
Training loss: 2.8416149616241455
Validation loss: 2.7711531974936046

Epoch: 6| Step: 3
Training loss: 2.6849231719970703
Validation loss: 2.770222520315519

Epoch: 6| Step: 4
Training loss: 2.364828586578369
Validation loss: 2.7673776431750228

Epoch: 6| Step: 5
Training loss: 2.3479576110839844
Validation loss: 2.7655993764118483

Epoch: 6| Step: 6
Training loss: 2.8965158462524414
Validation loss: 2.7696277633790047

Epoch: 6| Step: 7
Training loss: 2.6829886436462402
Validation loss: 2.774269070676578

Epoch: 6| Step: 8
Training loss: 2.935238838195801
Validation loss: 2.7946930752005628

Epoch: 6| Step: 9
Training loss: 3.3127846717834473
Validation loss: 2.8036379327056227

Epoch: 6| Step: 10
Training loss: 2.4818692207336426
Validation loss: 2.7694329010543

Epoch: 6| Step: 11
Training loss: 4.142889022827148
Validation loss: 2.7633506149374027

Epoch: 6| Step: 12
Training loss: 3.373567581176758
Validation loss: 2.7647519009087675

Epoch: 6| Step: 13
Training loss: 2.6978988647460938
Validation loss: 2.7695236923874065

Epoch: 57| Step: 0
Training loss: 2.7962117195129395
Validation loss: 2.7748415649578138

Epoch: 6| Step: 1
Training loss: 4.039809703826904
Validation loss: 2.7867736944588284

Epoch: 6| Step: 2
Training loss: 2.7772316932678223
Validation loss: 2.7927968450771865

Epoch: 6| Step: 3
Training loss: 3.1887965202331543
Validation loss: 2.7967461270670735

Epoch: 6| Step: 4
Training loss: 1.860551118850708
Validation loss: 2.7817624025447394

Epoch: 6| Step: 5
Training loss: 2.425421953201294
Validation loss: 2.772723205627934

Epoch: 6| Step: 6
Training loss: 2.9907212257385254
Validation loss: 2.7668372226017777

Epoch: 6| Step: 7
Training loss: 2.74692964553833
Validation loss: 2.766144198756064

Epoch: 6| Step: 8
Training loss: 2.949134349822998
Validation loss: 2.7638362402557046

Epoch: 6| Step: 9
Training loss: 3.1046528816223145
Validation loss: 2.765063531937138

Epoch: 6| Step: 10
Training loss: 2.5105719566345215
Validation loss: 2.7611140563923824

Epoch: 6| Step: 11
Training loss: 3.2817482948303223
Validation loss: 2.766913588329028

Epoch: 6| Step: 12
Training loss: 3.144049882888794
Validation loss: 2.7667185850040887

Epoch: 6| Step: 13
Training loss: 2.694005250930786
Validation loss: 2.7676910431154313

Epoch: 58| Step: 0
Training loss: 3.2844865322113037
Validation loss: 2.7698996759230092

Epoch: 6| Step: 1
Training loss: 2.5443785190582275
Validation loss: 2.7686758631019184

Epoch: 6| Step: 2
Training loss: 1.8339406251907349
Validation loss: 2.7693349366546958

Epoch: 6| Step: 3
Training loss: 2.707129955291748
Validation loss: 2.768049475967243

Epoch: 6| Step: 4
Training loss: 3.0702900886535645
Validation loss: 2.7679211862625612

Epoch: 6| Step: 5
Training loss: 2.9179527759552
Validation loss: 2.7646559464034213

Epoch: 6| Step: 6
Training loss: 2.077996253967285
Validation loss: 2.7626456881082184

Epoch: 6| Step: 7
Training loss: 3.5652341842651367
Validation loss: 2.765055061668478

Epoch: 6| Step: 8
Training loss: 2.387399196624756
Validation loss: 2.7676656323094524

Epoch: 6| Step: 9
Training loss: 4.252153396606445
Validation loss: 2.765194550637276

Epoch: 6| Step: 10
Training loss: 3.959280490875244
Validation loss: 2.764485131027878

Epoch: 6| Step: 11
Training loss: 2.428830146789551
Validation loss: 2.7678419902760494

Epoch: 6| Step: 12
Training loss: 2.2451717853546143
Validation loss: 2.7714466792280956

Epoch: 6| Step: 13
Training loss: 3.5995187759399414
Validation loss: 2.7660151399591917

Epoch: 59| Step: 0
Training loss: 2.7494874000549316
Validation loss: 2.7671961143452632

Epoch: 6| Step: 1
Training loss: 1.4797797203063965
Validation loss: 2.7626445793336436

Epoch: 6| Step: 2
Training loss: 2.7357754707336426
Validation loss: 2.764300197683355

Epoch: 6| Step: 3
Training loss: 2.917222261428833
Validation loss: 2.7585658924553984

Epoch: 6| Step: 4
Training loss: 3.9120230674743652
Validation loss: 2.7600466384682605

Epoch: 6| Step: 5
Training loss: 2.6555910110473633
Validation loss: 2.757304576135451

Epoch: 6| Step: 6
Training loss: 3.4048879146575928
Validation loss: 2.7590515844283567

Epoch: 6| Step: 7
Training loss: 3.0288748741149902
Validation loss: 2.7569162127792195

Epoch: 6| Step: 8
Training loss: 3.0174331665039062
Validation loss: 2.757480331646499

Epoch: 6| Step: 9
Training loss: 3.139005661010742
Validation loss: 2.7570164101098174

Epoch: 6| Step: 10
Training loss: 3.026158571243286
Validation loss: 2.760541472383725

Epoch: 6| Step: 11
Training loss: 3.1753056049346924
Validation loss: 2.7600238835939797

Epoch: 6| Step: 12
Training loss: 2.943673610687256
Validation loss: 2.762153733161188

Epoch: 6| Step: 13
Training loss: 1.8678431510925293
Validation loss: 2.757432306966474

Epoch: 60| Step: 0
Training loss: 2.2681524753570557
Validation loss: 2.75626370727375

Epoch: 6| Step: 1
Training loss: 2.4064197540283203
Validation loss: 2.7624323316799697

Epoch: 6| Step: 2
Training loss: 3.2641592025756836
Validation loss: 2.761204191433486

Epoch: 6| Step: 3
Training loss: 3.2836356163024902
Validation loss: 2.7564085401514524

Epoch: 6| Step: 4
Training loss: 2.5439867973327637
Validation loss: 2.7597291085027877

Epoch: 6| Step: 5
Training loss: 3.590400218963623
Validation loss: 2.7506954080315045

Epoch: 6| Step: 6
Training loss: 2.3128814697265625
Validation loss: 2.753019094467163

Epoch: 6| Step: 7
Training loss: 3.7208633422851562
Validation loss: 2.7544448632065968

Epoch: 6| Step: 8
Training loss: 3.479952812194824
Validation loss: 2.74933264845161

Epoch: 6| Step: 9
Training loss: 2.673729419708252
Validation loss: 2.753611026271697

Epoch: 6| Step: 10
Training loss: 3.383446455001831
Validation loss: 2.7524472052051174

Epoch: 6| Step: 11
Training loss: 1.9702587127685547
Validation loss: 2.7506473807878393

Epoch: 6| Step: 12
Training loss: 2.552351951599121
Validation loss: 2.7512387203913864

Epoch: 6| Step: 13
Training loss: 3.0493152141571045
Validation loss: 2.7535924757680585

Epoch: 61| Step: 0
Training loss: 2.7729947566986084
Validation loss: 2.7514717296887468

Epoch: 6| Step: 1
Training loss: 2.728550434112549
Validation loss: 2.7547261561116865

Epoch: 6| Step: 2
Training loss: 2.7372071743011475
Validation loss: 2.762741252940188

Epoch: 6| Step: 3
Training loss: 2.50626540184021
Validation loss: 2.7604545367661344

Epoch: 6| Step: 4
Training loss: 3.2040812969207764
Validation loss: 2.7640061301569783

Epoch: 6| Step: 5
Training loss: 3.3377819061279297
Validation loss: 2.766663917931177

Epoch: 6| Step: 6
Training loss: 2.967438220977783
Validation loss: 2.7545465320669194

Epoch: 6| Step: 7
Training loss: 3.388990879058838
Validation loss: 2.7540837257139144

Epoch: 6| Step: 8
Training loss: 3.049196720123291
Validation loss: 2.75049489800648

Epoch: 6| Step: 9
Training loss: 2.4381046295166016
Validation loss: 2.7512269994264007

Epoch: 6| Step: 10
Training loss: 3.1791653633117676
Validation loss: 2.7482494231193297

Epoch: 6| Step: 11
Training loss: 2.491457939147949
Validation loss: 2.7532428131308606

Epoch: 6| Step: 12
Training loss: 2.418656826019287
Validation loss: 2.747839594400057

Epoch: 6| Step: 13
Training loss: 3.436586380004883
Validation loss: 2.748720040885351

Epoch: 62| Step: 0
Training loss: 2.5460872650146484
Validation loss: 2.7493030281477076

Epoch: 6| Step: 1
Training loss: 3.671802520751953
Validation loss: 2.7505463323285504

Epoch: 6| Step: 2
Training loss: 3.0282294750213623
Validation loss: 2.74683819278594

Epoch: 6| Step: 3
Training loss: 3.228118658065796
Validation loss: 2.74705324890793

Epoch: 6| Step: 4
Training loss: 2.1917333602905273
Validation loss: 2.747108777364095

Epoch: 6| Step: 5
Training loss: 3.451833724975586
Validation loss: 2.7476427478175007

Epoch: 6| Step: 6
Training loss: 2.203991651535034
Validation loss: 2.7455157567096014

Epoch: 6| Step: 7
Training loss: 2.6874489784240723
Validation loss: 2.7482119068022697

Epoch: 6| Step: 8
Training loss: 2.5358986854553223
Validation loss: 2.74650824967251

Epoch: 6| Step: 9
Training loss: 2.6072282791137695
Validation loss: 2.7476400688130367

Epoch: 6| Step: 10
Training loss: 2.671511173248291
Validation loss: 2.749496900907127

Epoch: 6| Step: 11
Training loss: 2.895528554916382
Validation loss: 2.748406820399787

Epoch: 6| Step: 12
Training loss: 3.484189510345459
Validation loss: 2.7488067098843154

Epoch: 6| Step: 13
Training loss: 3.5305092334747314
Validation loss: 2.749057367283811

Epoch: 63| Step: 0
Training loss: 2.99745512008667
Validation loss: 2.7466549540078766

Epoch: 6| Step: 1
Training loss: 3.4943227767944336
Validation loss: 2.7477883267146286

Epoch: 6| Step: 2
Training loss: 2.886497974395752
Validation loss: 2.7447226201334307

Epoch: 6| Step: 3
Training loss: 2.7953391075134277
Validation loss: 2.745779437403525

Epoch: 6| Step: 4
Training loss: 2.896167039871216
Validation loss: 2.748479440648069

Epoch: 6| Step: 5
Training loss: 2.190075159072876
Validation loss: 2.7449072996775308

Epoch: 6| Step: 6
Training loss: 3.281306743621826
Validation loss: 2.7438769109787478

Epoch: 6| Step: 7
Training loss: 2.610928535461426
Validation loss: 2.739173371304748

Epoch: 6| Step: 8
Training loss: 3.0089218616485596
Validation loss: 2.745693970752019

Epoch: 6| Step: 9
Training loss: 3.0597949028015137
Validation loss: 2.7424331659911783

Epoch: 6| Step: 10
Training loss: 2.893042802810669
Validation loss: 2.745027224222819

Epoch: 6| Step: 11
Training loss: 2.7801012992858887
Validation loss: 2.7474629135542017

Epoch: 6| Step: 12
Training loss: 2.6716837882995605
Validation loss: 2.745647822656939

Epoch: 6| Step: 13
Training loss: 2.6820714473724365
Validation loss: 2.7440282837037118

Epoch: 64| Step: 0
Training loss: 2.7251226902008057
Validation loss: 2.746311233889672

Epoch: 6| Step: 1
Training loss: 3.219788074493408
Validation loss: 2.746508252236151

Epoch: 6| Step: 2
Training loss: 2.5486807823181152
Validation loss: 2.752074131401636

Epoch: 6| Step: 3
Training loss: 3.106232166290283
Validation loss: 2.7523857009026313

Epoch: 6| Step: 4
Training loss: 2.8597607612609863
Validation loss: 2.752315082857686

Epoch: 6| Step: 5
Training loss: 2.8040919303894043
Validation loss: 2.752724914140599

Epoch: 6| Step: 6
Training loss: 2.2975926399230957
Validation loss: 2.754338184992472

Epoch: 6| Step: 7
Training loss: 3.3716588020324707
Validation loss: 2.7525268036832093

Epoch: 6| Step: 8
Training loss: 2.271101236343384
Validation loss: 2.7564688523610434

Epoch: 6| Step: 9
Training loss: 2.8786911964416504
Validation loss: 2.7551903006851033

Epoch: 6| Step: 10
Training loss: 2.835279941558838
Validation loss: 2.747323307939755

Epoch: 6| Step: 11
Training loss: 2.7049202919006348
Validation loss: 2.7433118640735583

Epoch: 6| Step: 12
Training loss: 3.3892340660095215
Validation loss: 2.741856328902706

Epoch: 6| Step: 13
Training loss: 3.5716917514801025
Validation loss: 2.737462156562395

Epoch: 65| Step: 0
Training loss: 3.159949779510498
Validation loss: 2.7491631918056036

Epoch: 6| Step: 1
Training loss: 1.9627635478973389
Validation loss: 2.7652514519230014

Epoch: 6| Step: 2
Training loss: 4.040231704711914
Validation loss: 2.7788197173867175

Epoch: 6| Step: 3
Training loss: 3.2785472869873047
Validation loss: 2.8011549339499524

Epoch: 6| Step: 4
Training loss: 2.4876365661621094
Validation loss: 2.773047383113574

Epoch: 6| Step: 5
Training loss: 2.9545650482177734
Validation loss: 2.7391351192228255

Epoch: 6| Step: 6
Training loss: 2.6018412113189697
Validation loss: 2.7339332283184095

Epoch: 6| Step: 7
Training loss: 2.983051061630249
Validation loss: 2.7367344210224767

Epoch: 6| Step: 8
Training loss: 2.5294992923736572
Validation loss: 2.7415829679017425

Epoch: 6| Step: 9
Training loss: 2.550410270690918
Validation loss: 2.745592153200539

Epoch: 6| Step: 10
Training loss: 3.3541173934936523
Validation loss: 2.7539386492903515

Epoch: 6| Step: 11
Training loss: 2.8418047428131104
Validation loss: 2.7595642997372534

Epoch: 6| Step: 12
Training loss: 2.5287981033325195
Validation loss: 2.7644182764073855

Epoch: 6| Step: 13
Training loss: 3.0671253204345703
Validation loss: 2.776082682353194

Epoch: 66| Step: 0
Training loss: 2.1996047496795654
Validation loss: 2.7793579409199376

Epoch: 6| Step: 1
Training loss: 2.7241744995117188
Validation loss: 2.7798800468444824

Epoch: 6| Step: 2
Training loss: 2.604139804840088
Validation loss: 2.773499899013068

Epoch: 6| Step: 3
Training loss: 3.509484052658081
Validation loss: 2.762289403587259

Epoch: 6| Step: 4
Training loss: 2.604102849960327
Validation loss: 2.752945899963379

Epoch: 6| Step: 5
Training loss: 3.09826397895813
Validation loss: 2.7465647215484292

Epoch: 6| Step: 6
Training loss: 3.5311436653137207
Validation loss: 2.7425683365073255

Epoch: 6| Step: 7
Training loss: 2.845851421356201
Validation loss: 2.744617931304439

Epoch: 6| Step: 8
Training loss: 3.5322654247283936
Validation loss: 2.7461347990138556

Epoch: 6| Step: 9
Training loss: 2.749060869216919
Validation loss: 2.743509315675305

Epoch: 6| Step: 10
Training loss: 2.564549446105957
Validation loss: 2.741418820555492

Epoch: 6| Step: 11
Training loss: 3.072741985321045
Validation loss: 2.739194985358946

Epoch: 6| Step: 12
Training loss: 2.6386451721191406
Validation loss: 2.738794767728416

Epoch: 6| Step: 13
Training loss: 2.3666229248046875
Validation loss: 2.738715402541622

Epoch: 67| Step: 0
Training loss: 4.274263858795166
Validation loss: 2.7361667694584018

Epoch: 6| Step: 1
Training loss: 3.4667725563049316
Validation loss: 2.739406552366031

Epoch: 6| Step: 2
Training loss: 2.168170928955078
Validation loss: 2.736744137220485

Epoch: 6| Step: 3
Training loss: 2.2468111515045166
Validation loss: 2.7376417908617245

Epoch: 6| Step: 4
Training loss: 3.1327571868896484
Validation loss: 2.7336270014444985

Epoch: 6| Step: 5
Training loss: 2.0804829597473145
Validation loss: 2.730874838367585

Epoch: 6| Step: 6
Training loss: 3.1980810165405273
Validation loss: 2.7283626935815297

Epoch: 6| Step: 7
Training loss: 2.711287021636963
Validation loss: 2.730751827198972

Epoch: 6| Step: 8
Training loss: 2.937699317932129
Validation loss: 2.7286952157174387

Epoch: 6| Step: 9
Training loss: 2.496363878250122
Validation loss: 2.730249471561883

Epoch: 6| Step: 10
Training loss: 3.431920051574707
Validation loss: 2.7291519129148094

Epoch: 6| Step: 11
Training loss: 2.1064629554748535
Validation loss: 2.7311767711434314

Epoch: 6| Step: 12
Training loss: 2.9567513465881348
Validation loss: 2.7334876239940686

Epoch: 6| Step: 13
Training loss: 3.1089231967926025
Validation loss: 2.729954088887861

Epoch: 68| Step: 0
Training loss: 3.6159234046936035
Validation loss: 2.729451976796632

Epoch: 6| Step: 1
Training loss: 2.6460649967193604
Validation loss: 2.730785708273611

Epoch: 6| Step: 2
Training loss: 2.357774257659912
Validation loss: 2.726037122870004

Epoch: 6| Step: 3
Training loss: 3.2967636585235596
Validation loss: 2.722165746073569

Epoch: 6| Step: 4
Training loss: 2.6220107078552246
Validation loss: 2.7289100000935216

Epoch: 6| Step: 5
Training loss: 2.2765114307403564
Validation loss: 2.7244903656744186

Epoch: 6| Step: 6
Training loss: 2.2931056022644043
Validation loss: 2.7234782544515466

Epoch: 6| Step: 7
Training loss: 2.2907321453094482
Validation loss: 2.7232317206680134

Epoch: 6| Step: 8
Training loss: 3.0586583614349365
Validation loss: 2.7229632664752264

Epoch: 6| Step: 9
Training loss: 2.5402684211730957
Validation loss: 2.728107495974469

Epoch: 6| Step: 10
Training loss: 3.6138968467712402
Validation loss: 2.7224986143009637

Epoch: 6| Step: 11
Training loss: 2.2950382232666016
Validation loss: 2.723968508423016

Epoch: 6| Step: 12
Training loss: 4.079635143280029
Validation loss: 2.722180735680365

Epoch: 6| Step: 13
Training loss: 3.3282196521759033
Validation loss: 2.7249672028326217

Epoch: 69| Step: 0
Training loss: 2.418236255645752
Validation loss: 2.7224022291039907

Epoch: 6| Step: 1
Training loss: 2.546389102935791
Validation loss: 2.724738323560325

Epoch: 6| Step: 2
Training loss: 2.8866381645202637
Validation loss: 2.7231304337901454

Epoch: 6| Step: 3
Training loss: 3.023972272872925
Validation loss: 2.7269783148201565

Epoch: 6| Step: 4
Training loss: 2.8990983963012695
Validation loss: 2.7255683355433966

Epoch: 6| Step: 5
Training loss: 3.7083091735839844
Validation loss: 2.7222644898199264

Epoch: 6| Step: 6
Training loss: 3.3611974716186523
Validation loss: 2.720821272942328

Epoch: 6| Step: 7
Training loss: 2.6739773750305176
Validation loss: 2.721751597619826

Epoch: 6| Step: 8
Training loss: 3.5105419158935547
Validation loss: 2.7257735190852994

Epoch: 6| Step: 9
Training loss: 1.8444675207138062
Validation loss: 2.72271268726677

Epoch: 6| Step: 10
Training loss: 2.126267910003662
Validation loss: 2.721017858033539

Epoch: 6| Step: 11
Training loss: 4.172553539276123
Validation loss: 2.722969478176486

Epoch: 6| Step: 12
Training loss: 1.6924753189086914
Validation loss: 2.7221994425660823

Epoch: 6| Step: 13
Training loss: 3.435715913772583
Validation loss: 2.7228442263859574

Epoch: 70| Step: 0
Training loss: 3.1461181640625
Validation loss: 2.7226078561557236

Epoch: 6| Step: 1
Training loss: 3.165313243865967
Validation loss: 2.7220375178962626

Epoch: 6| Step: 2
Training loss: 3.266432762145996
Validation loss: 2.727101454170801

Epoch: 6| Step: 3
Training loss: 2.739828109741211
Validation loss: 2.7238552954889115

Epoch: 6| Step: 4
Training loss: 3.1128764152526855
Validation loss: 2.7262790254367295

Epoch: 6| Step: 5
Training loss: 2.99735164642334
Validation loss: 2.723751155279016

Epoch: 6| Step: 6
Training loss: 2.9006505012512207
Validation loss: 2.725261649777812

Epoch: 6| Step: 7
Training loss: 3.151634454727173
Validation loss: 2.7228078483253397

Epoch: 6| Step: 8
Training loss: 2.457125186920166
Validation loss: 2.7198992185695197

Epoch: 6| Step: 9
Training loss: 2.454676389694214
Validation loss: 2.7225375944568264

Epoch: 6| Step: 10
Training loss: 2.455803871154785
Validation loss: 2.7230296442585606

Epoch: 6| Step: 11
Training loss: 2.9421863555908203
Validation loss: 2.7246803442637124

Epoch: 6| Step: 12
Training loss: 2.5541155338287354
Validation loss: 2.725745221619965

Epoch: 6| Step: 13
Training loss: 2.4489216804504395
Validation loss: 2.7248080571492515

Epoch: 71| Step: 0
Training loss: 2.4824604988098145
Validation loss: 2.7255013706863567

Epoch: 6| Step: 1
Training loss: 2.5019073486328125
Validation loss: 2.7300923229545675

Epoch: 6| Step: 2
Training loss: 3.0421054363250732
Validation loss: 2.731228828430176

Epoch: 6| Step: 3
Training loss: 3.4143733978271484
Validation loss: 2.7289021784259426

Epoch: 6| Step: 4
Training loss: 3.3560783863067627
Validation loss: 2.7258526381625923

Epoch: 6| Step: 5
Training loss: 2.6441402435302734
Validation loss: 2.723526531650174

Epoch: 6| Step: 6
Training loss: 2.9769351482391357
Validation loss: 2.724612730805592

Epoch: 6| Step: 7
Training loss: 2.959895133972168
Validation loss: 2.7202142643672165

Epoch: 6| Step: 8
Training loss: 1.7335041761398315
Validation loss: 2.7182407430423203

Epoch: 6| Step: 9
Training loss: 2.5226922035217285
Validation loss: 2.7187516279118036

Epoch: 6| Step: 10
Training loss: 3.4381425380706787
Validation loss: 2.7197750793990267

Epoch: 6| Step: 11
Training loss: 2.5923073291778564
Validation loss: 2.720889478601435

Epoch: 6| Step: 12
Training loss: 2.7266931533813477
Validation loss: 2.7192100324938373

Epoch: 6| Step: 13
Training loss: 4.1022233963012695
Validation loss: 2.719408542879166

Epoch: 72| Step: 0
Training loss: 3.8402318954467773
Validation loss: 2.7149298755071496

Epoch: 6| Step: 1
Training loss: 1.4429813623428345
Validation loss: 2.716902725158199

Epoch: 6| Step: 2
Training loss: 2.474181652069092
Validation loss: 2.719563120154924

Epoch: 6| Step: 3
Training loss: 2.4726223945617676
Validation loss: 2.717091888509771

Epoch: 6| Step: 4
Training loss: 3.530695915222168
Validation loss: 2.718820766736102

Epoch: 6| Step: 5
Training loss: 2.7058820724487305
Validation loss: 2.716963368077432

Epoch: 6| Step: 6
Training loss: 2.5139012336730957
Validation loss: 2.7208638806496896

Epoch: 6| Step: 7
Training loss: 3.203824043273926
Validation loss: 2.7233088836875012

Epoch: 6| Step: 8
Training loss: 2.5464718341827393
Validation loss: 2.7222121325872277

Epoch: 6| Step: 9
Training loss: 2.4370410442352295
Validation loss: 2.718575882655318

Epoch: 6| Step: 10
Training loss: 2.653961181640625
Validation loss: 2.718552989344443

Epoch: 6| Step: 11
Training loss: 3.2832117080688477
Validation loss: 2.716641195358769

Epoch: 6| Step: 12
Training loss: 3.2853989601135254
Validation loss: 2.71803238314967

Epoch: 6| Step: 13
Training loss: 4.122106552124023
Validation loss: 2.7195470256190144

Epoch: 73| Step: 0
Training loss: 3.7439680099487305
Validation loss: 2.720972627721807

Epoch: 6| Step: 1
Training loss: 2.7818679809570312
Validation loss: 2.713936254542361

Epoch: 6| Step: 2
Training loss: 2.868636131286621
Validation loss: 2.715338004532681

Epoch: 6| Step: 3
Training loss: 2.161309003829956
Validation loss: 2.714751043627339

Epoch: 6| Step: 4
Training loss: 3.15103816986084
Validation loss: 2.719215544321204

Epoch: 6| Step: 5
Training loss: 3.5397567749023438
Validation loss: 2.721156894519765

Epoch: 6| Step: 6
Training loss: 1.7254235744476318
Validation loss: 2.7191108247285247

Epoch: 6| Step: 7
Training loss: 2.7887790203094482
Validation loss: 2.717451003289992

Epoch: 6| Step: 8
Training loss: 3.585540771484375
Validation loss: 2.716672048773817

Epoch: 6| Step: 9
Training loss: 2.7818663120269775
Validation loss: 2.7161324331837315

Epoch: 6| Step: 10
Training loss: 2.4940428733825684
Validation loss: 2.716439208676738

Epoch: 6| Step: 11
Training loss: 2.2007391452789307
Validation loss: 2.7172553513639714

Epoch: 6| Step: 12
Training loss: 3.290985345840454
Validation loss: 2.7128351708894134

Epoch: 6| Step: 13
Training loss: 2.8148975372314453
Validation loss: 2.711737778878981

Epoch: 74| Step: 0
Training loss: 3.202579975128174
Validation loss: 2.714703590639176

Epoch: 6| Step: 1
Training loss: 3.149202823638916
Validation loss: 2.713780769737818

Epoch: 6| Step: 2
Training loss: 2.911320209503174
Validation loss: 2.7126224143530733

Epoch: 6| Step: 3
Training loss: 3.076376438140869
Validation loss: 2.7121040128892466

Epoch: 6| Step: 4
Training loss: 2.8945722579956055
Validation loss: 2.713515302186371

Epoch: 6| Step: 5
Training loss: 1.7992883920669556
Validation loss: 2.7128939346600602

Epoch: 6| Step: 6
Training loss: 2.614715576171875
Validation loss: 2.716806827052947

Epoch: 6| Step: 7
Training loss: 2.4859519004821777
Validation loss: 2.7134114465405865

Epoch: 6| Step: 8
Training loss: 2.9742908477783203
Validation loss: 2.7148935000101724

Epoch: 6| Step: 9
Training loss: 3.0598862171173096
Validation loss: 2.7177306708469184

Epoch: 6| Step: 10
Training loss: 2.548564910888672
Validation loss: 2.7228720931596655

Epoch: 6| Step: 11
Training loss: 3.439344882965088
Validation loss: 2.7184598932984056

Epoch: 6| Step: 12
Training loss: 2.839881420135498
Validation loss: 2.7166617275566183

Epoch: 6| Step: 13
Training loss: 2.7725753784179688
Validation loss: 2.7173619065233456

Epoch: 75| Step: 0
Training loss: 2.373526096343994
Validation loss: 2.717288758165093

Epoch: 6| Step: 1
Training loss: 3.140012264251709
Validation loss: 2.720221645088606

Epoch: 6| Step: 2
Training loss: 2.904144763946533
Validation loss: 2.7156566855727986

Epoch: 6| Step: 3
Training loss: 2.575096607208252
Validation loss: 2.716162061178556

Epoch: 6| Step: 4
Training loss: 3.0089144706726074
Validation loss: 2.714180879695441

Epoch: 6| Step: 5
Training loss: 2.200153350830078
Validation loss: 2.713193888305336

Epoch: 6| Step: 6
Training loss: 2.3864426612854004
Validation loss: 2.71160231098052

Epoch: 6| Step: 7
Training loss: 2.3308634757995605
Validation loss: 2.7117711420982116

Epoch: 6| Step: 8
Training loss: 3.660938262939453
Validation loss: 2.7171278307514806

Epoch: 6| Step: 9
Training loss: 2.6868996620178223
Validation loss: 2.7205649498970277

Epoch: 6| Step: 10
Training loss: 2.859567642211914
Validation loss: 2.722644326507404

Epoch: 6| Step: 11
Training loss: 3.780860662460327
Validation loss: 2.7394636266974994

Epoch: 6| Step: 12
Training loss: 3.0662479400634766
Validation loss: 2.728003345510011

Epoch: 6| Step: 13
Training loss: 2.80161714553833
Validation loss: 2.718942819103118

Epoch: 76| Step: 0
Training loss: 3.2297422885894775
Validation loss: 2.7143256997549408

Epoch: 6| Step: 1
Training loss: 3.053912401199341
Validation loss: 2.7098048733126734

Epoch: 6| Step: 2
Training loss: 2.624269485473633
Validation loss: 2.7063487806627826

Epoch: 6| Step: 3
Training loss: 2.841649055480957
Validation loss: 2.704638619576731

Epoch: 6| Step: 4
Training loss: 2.8126220703125
Validation loss: 2.7092797628013034

Epoch: 6| Step: 5
Training loss: 2.475493907928467
Validation loss: 2.7069795721320697

Epoch: 6| Step: 6
Training loss: 3.152981996536255
Validation loss: 2.721101217372443

Epoch: 6| Step: 7
Training loss: 3.4390928745269775
Validation loss: 2.716565409014302

Epoch: 6| Step: 8
Training loss: 3.0781469345092773
Validation loss: 2.7190323286159064

Epoch: 6| Step: 9
Training loss: 3.340923309326172
Validation loss: 2.727621057982086

Epoch: 6| Step: 10
Training loss: 2.684091567993164
Validation loss: 2.7214563687642417

Epoch: 6| Step: 11
Training loss: 2.8446428775787354
Validation loss: 2.7187042646510626

Epoch: 6| Step: 12
Training loss: 1.5595065355300903
Validation loss: 2.712213072725522

Epoch: 6| Step: 13
Training loss: 2.6107168197631836
Validation loss: 2.7062699999860538

Epoch: 77| Step: 0
Training loss: 2.520829677581787
Validation loss: 2.7106625290327173

Epoch: 6| Step: 1
Training loss: 2.93356990814209
Validation loss: 2.7078742955320623

Epoch: 6| Step: 2
Training loss: 2.9378585815429688
Validation loss: 2.7068272944419616

Epoch: 6| Step: 3
Training loss: 1.7481892108917236
Validation loss: 2.7041516919289865

Epoch: 6| Step: 4
Training loss: 3.1198906898498535
Validation loss: 2.706942237833495

Epoch: 6| Step: 5
Training loss: 2.205226182937622
Validation loss: 2.701687366731705

Epoch: 6| Step: 6
Training loss: 3.1399378776550293
Validation loss: 2.703189701162359

Epoch: 6| Step: 7
Training loss: 2.89152455329895
Validation loss: 2.7049173308957006

Epoch: 6| Step: 8
Training loss: 2.791994094848633
Validation loss: 2.7070944975781184

Epoch: 6| Step: 9
Training loss: 2.4903721809387207
Validation loss: 2.705914520448254

Epoch: 6| Step: 10
Training loss: 3.5302839279174805
Validation loss: 2.70581563057438

Epoch: 6| Step: 11
Training loss: 3.528174877166748
Validation loss: 2.7088968497450634

Epoch: 6| Step: 12
Training loss: 3.058957576751709
Validation loss: 2.708443946735833

Epoch: 6| Step: 13
Training loss: 2.856081485748291
Validation loss: 2.7092934449513755

Epoch: 78| Step: 0
Training loss: 3.0480408668518066
Validation loss: 2.7138916830862723

Epoch: 6| Step: 1
Training loss: 2.1707115173339844
Validation loss: 2.7123250781848864

Epoch: 6| Step: 2
Training loss: 2.9786770343780518
Validation loss: 2.711930142935886

Epoch: 6| Step: 3
Training loss: 2.9796640872955322
Validation loss: 2.7170718062308525

Epoch: 6| Step: 4
Training loss: 3.4623563289642334
Validation loss: 2.7158081249524186

Epoch: 6| Step: 5
Training loss: 2.8587939739227295
Validation loss: 2.71439084699077

Epoch: 6| Step: 6
Training loss: 3.4100124835968018
Validation loss: 2.707017155103786

Epoch: 6| Step: 7
Training loss: 3.017690658569336
Validation loss: 2.704261561875702

Epoch: 6| Step: 8
Training loss: 3.037086009979248
Validation loss: 2.702874755346647

Epoch: 6| Step: 9
Training loss: 2.978614330291748
Validation loss: 2.7012893845958095

Epoch: 6| Step: 10
Training loss: 2.7018539905548096
Validation loss: 2.698713412848852

Epoch: 6| Step: 11
Training loss: 2.243481397628784
Validation loss: 2.6981610867284958

Epoch: 6| Step: 12
Training loss: 2.358677864074707
Validation loss: 2.693570747170397

Epoch: 6| Step: 13
Training loss: 2.125088930130005
Validation loss: 2.6937671271703576

Epoch: 79| Step: 0
Training loss: 2.38830828666687
Validation loss: 2.6941368426046064

Epoch: 6| Step: 1
Training loss: 2.806260585784912
Validation loss: 2.695952292411558

Epoch: 6| Step: 2
Training loss: 2.196643352508545
Validation loss: 2.6973547243302867

Epoch: 6| Step: 3
Training loss: 2.8058180809020996
Validation loss: 2.6961036882092877

Epoch: 6| Step: 4
Training loss: 2.2195653915405273
Validation loss: 2.698625164647256

Epoch: 6| Step: 5
Training loss: 3.290125846862793
Validation loss: 2.7007019237805436

Epoch: 6| Step: 6
Training loss: 3.605851650238037
Validation loss: 2.7052928786123953

Epoch: 6| Step: 7
Training loss: 2.0044376850128174
Validation loss: 2.709595029072095

Epoch: 6| Step: 8
Training loss: 3.0084519386291504
Validation loss: 2.712283524133826

Epoch: 6| Step: 9
Training loss: 4.035277366638184
Validation loss: 2.733793922649917

Epoch: 6| Step: 10
Training loss: 3.654444932937622
Validation loss: 2.7515986709184546

Epoch: 6| Step: 11
Training loss: 2.5755298137664795
Validation loss: 2.7113740623638196

Epoch: 6| Step: 12
Training loss: 2.8934102058410645
Validation loss: 2.7020140130032777

Epoch: 6| Step: 13
Training loss: 1.9257978200912476
Validation loss: 2.7044833475543606

Epoch: 80| Step: 0
Training loss: 2.9306259155273438
Validation loss: 2.6991698998276905

Epoch: 6| Step: 1
Training loss: 2.9424057006835938
Validation loss: 2.7074459688637846

Epoch: 6| Step: 2
Training loss: 3.073960304260254
Validation loss: 2.708383167943647

Epoch: 6| Step: 3
Training loss: 1.7662909030914307
Validation loss: 2.7112232023669827

Epoch: 6| Step: 4
Training loss: 2.985985040664673
Validation loss: 2.709761481131277

Epoch: 6| Step: 5
Training loss: 3.427745819091797
Validation loss: 2.7199952448568037

Epoch: 6| Step: 6
Training loss: 2.9308149814605713
Validation loss: 2.728590073124055

Epoch: 6| Step: 7
Training loss: 3.5045838356018066
Validation loss: 2.720088889521937

Epoch: 6| Step: 8
Training loss: 2.769376277923584
Validation loss: 2.7160350481669107

Epoch: 6| Step: 9
Training loss: 2.3325982093811035
Validation loss: 2.706993461937033

Epoch: 6| Step: 10
Training loss: 2.733022928237915
Validation loss: 2.696352543369416

Epoch: 6| Step: 11
Training loss: 3.0420920848846436
Validation loss: 2.6982124082503782

Epoch: 6| Step: 12
Training loss: 2.4632863998413086
Validation loss: 2.694209716653311

Epoch: 6| Step: 13
Training loss: 2.767871379852295
Validation loss: 2.6916191001092233

Epoch: 81| Step: 0
Training loss: 2.6830782890319824
Validation loss: 2.6894426345825195

Epoch: 6| Step: 1
Training loss: 2.7555553913116455
Validation loss: 2.6928687813461467

Epoch: 6| Step: 2
Training loss: 2.6830039024353027
Validation loss: 2.689863904829948

Epoch: 6| Step: 3
Training loss: 3.2235488891601562
Validation loss: 2.6915278537299043

Epoch: 6| Step: 4
Training loss: 2.437945604324341
Validation loss: 2.6895366330300607

Epoch: 6| Step: 5
Training loss: 3.603410243988037
Validation loss: 2.6860937354385213

Epoch: 6| Step: 6
Training loss: 2.230792999267578
Validation loss: 2.68617836377954

Epoch: 6| Step: 7
Training loss: 2.2460663318634033
Validation loss: 2.685870219302434

Epoch: 6| Step: 8
Training loss: 3.1581544876098633
Validation loss: 2.687693157503682

Epoch: 6| Step: 9
Training loss: 3.444765090942383
Validation loss: 2.685835071789321

Epoch: 6| Step: 10
Training loss: 2.643313407897949
Validation loss: 2.6868998978727605

Epoch: 6| Step: 11
Training loss: 2.7950010299682617
Validation loss: 2.688476526609031

Epoch: 6| Step: 12
Training loss: 2.7423095703125
Validation loss: 2.6867612049143803

Epoch: 6| Step: 13
Training loss: 3.0387563705444336
Validation loss: 2.6883374619227585

Epoch: 82| Step: 0
Training loss: 2.787828207015991
Validation loss: 2.686762161152337

Epoch: 6| Step: 1
Training loss: 2.331969738006592
Validation loss: 2.6877877353340067

Epoch: 6| Step: 2
Training loss: 3.4623444080352783
Validation loss: 2.688611733016147

Epoch: 6| Step: 3
Training loss: 3.2188773155212402
Validation loss: 2.6912129361142396

Epoch: 6| Step: 4
Training loss: 3.304532527923584
Validation loss: 2.6867092399186987

Epoch: 6| Step: 5
Training loss: 2.729294776916504
Validation loss: 2.688034434472361

Epoch: 6| Step: 6
Training loss: 2.9300670623779297
Validation loss: 2.6870369834284626

Epoch: 6| Step: 7
Training loss: 2.2250375747680664
Validation loss: 2.6908328635718233

Epoch: 6| Step: 8
Training loss: 2.5832161903381348
Validation loss: 2.6893354205675024

Epoch: 6| Step: 9
Training loss: 2.1558492183685303
Validation loss: 2.684236631598524

Epoch: 6| Step: 10
Training loss: 3.207132339477539
Validation loss: 2.688516280984366

Epoch: 6| Step: 11
Training loss: 2.5739150047302246
Validation loss: 2.691132324998097

Epoch: 6| Step: 12
Training loss: 3.141545534133911
Validation loss: 2.695073027764597

Epoch: 6| Step: 13
Training loss: 2.8553240299224854
Validation loss: 2.6990610476463073

Epoch: 83| Step: 0
Training loss: 2.3909270763397217
Validation loss: 2.6948881200564805

Epoch: 6| Step: 1
Training loss: 2.3982439041137695
Validation loss: 2.686405304939516

Epoch: 6| Step: 2
Training loss: 3.264766216278076
Validation loss: 2.6870384139399373

Epoch: 6| Step: 3
Training loss: 2.693655490875244
Validation loss: 2.682234687189902

Epoch: 6| Step: 4
Training loss: 2.3974719047546387
Validation loss: 2.6835549287898566

Epoch: 6| Step: 5
Training loss: 3.5112719535827637
Validation loss: 2.6853983427888606

Epoch: 6| Step: 6
Training loss: 2.136392116546631
Validation loss: 2.6838859588869157

Epoch: 6| Step: 7
Training loss: 3.155487060546875
Validation loss: 2.684423364618773

Epoch: 6| Step: 8
Training loss: 3.126096725463867
Validation loss: 2.6885221824851087

Epoch: 6| Step: 9
Training loss: 2.7954092025756836
Validation loss: 2.6944183124009

Epoch: 6| Step: 10
Training loss: 2.6741223335266113
Validation loss: 2.693782752560031

Epoch: 6| Step: 11
Training loss: 2.7699971199035645
Validation loss: 2.6971949274821947

Epoch: 6| Step: 12
Training loss: 3.2962050437927246
Validation loss: 2.6970407193706882

Epoch: 6| Step: 13
Training loss: 2.837155342102051
Validation loss: 2.6923248537125124

Epoch: 84| Step: 0
Training loss: 3.363502025604248
Validation loss: 2.6847300196206696

Epoch: 6| Step: 1
Training loss: 2.223186492919922
Validation loss: 2.684623731079922

Epoch: 6| Step: 2
Training loss: 2.728536605834961
Validation loss: 2.6831952448814147

Epoch: 6| Step: 3
Training loss: 3.2064125537872314
Validation loss: 2.679163086798883

Epoch: 6| Step: 4
Training loss: 3.65391206741333
Validation loss: 2.677404398559242

Epoch: 6| Step: 5
Training loss: 1.9736073017120361
Validation loss: 2.679952474050624

Epoch: 6| Step: 6
Training loss: 2.257103681564331
Validation loss: 2.6812852146804973

Epoch: 6| Step: 7
Training loss: 2.836683750152588
Validation loss: 2.6785949968522593

Epoch: 6| Step: 8
Training loss: 2.883793592453003
Validation loss: 2.679474030771563

Epoch: 6| Step: 9
Training loss: 2.804934501647949
Validation loss: 2.6789122268717778

Epoch: 6| Step: 10
Training loss: 3.204287052154541
Validation loss: 2.6760879767838346

Epoch: 6| Step: 11
Training loss: 2.4684348106384277
Validation loss: 2.683580792078408

Epoch: 6| Step: 12
Training loss: 3.380673408508301
Validation loss: 2.6768295482922624

Epoch: 6| Step: 13
Training loss: 2.2002344131469727
Validation loss: 2.689632454226094

Epoch: 85| Step: 0
Training loss: 3.0738303661346436
Validation loss: 2.6841590558328936

Epoch: 6| Step: 1
Training loss: 2.6954569816589355
Validation loss: 2.6815947922327186

Epoch: 6| Step: 2
Training loss: 3.413545608520508
Validation loss: 2.684938987096151

Epoch: 6| Step: 3
Training loss: 2.4554803371429443
Validation loss: 2.683112188052106

Epoch: 6| Step: 4
Training loss: 3.1356754302978516
Validation loss: 2.684443809652841

Epoch: 6| Step: 5
Training loss: 2.6448171138763428
Validation loss: 2.6839160329552105

Epoch: 6| Step: 6
Training loss: 1.8179593086242676
Validation loss: 2.6807930136239655

Epoch: 6| Step: 7
Training loss: 3.082883358001709
Validation loss: 2.6787120244836293

Epoch: 6| Step: 8
Training loss: 2.8769659996032715
Validation loss: 2.6789176156443935

Epoch: 6| Step: 9
Training loss: 2.510491371154785
Validation loss: 2.6797315459097586

Epoch: 6| Step: 10
Training loss: 3.1323111057281494
Validation loss: 2.6764889814520396

Epoch: 6| Step: 11
Training loss: 2.221557855606079
Validation loss: 2.6787587827251804

Epoch: 6| Step: 12
Training loss: 2.674189329147339
Validation loss: 2.67656720325511

Epoch: 6| Step: 13
Training loss: 4.269839286804199
Validation loss: 2.679973135712326

Epoch: 86| Step: 0
Training loss: 2.253370523452759
Validation loss: 2.6772199138518302

Epoch: 6| Step: 1
Training loss: 2.6604321002960205
Validation loss: 2.6734279970968924

Epoch: 6| Step: 2
Training loss: 3.5847666263580322
Validation loss: 2.674911960478752

Epoch: 6| Step: 3
Training loss: 2.987675189971924
Validation loss: 2.67131882585505

Epoch: 6| Step: 4
Training loss: 2.869770050048828
Validation loss: 2.67484200128945

Epoch: 6| Step: 5
Training loss: 2.75886607170105
Validation loss: 2.6744342465554514

Epoch: 6| Step: 6
Training loss: 2.840439558029175
Validation loss: 2.671767868021483

Epoch: 6| Step: 7
Training loss: 2.7179911136627197
Validation loss: 2.671580655600435

Epoch: 6| Step: 8
Training loss: 3.091299057006836
Validation loss: 2.673288991374354

Epoch: 6| Step: 9
Training loss: 2.6702098846435547
Validation loss: 2.6711444367644606

Epoch: 6| Step: 10
Training loss: 2.237180471420288
Validation loss: 2.6741839429383636

Epoch: 6| Step: 11
Training loss: 2.3309831619262695
Validation loss: 2.6726111878630934

Epoch: 6| Step: 12
Training loss: 3.107757329940796
Validation loss: 2.6730751401634625

Epoch: 6| Step: 13
Training loss: 3.5601625442504883
Validation loss: 2.672693124381445

Epoch: 87| Step: 0
Training loss: 2.9073550701141357
Validation loss: 2.6732991895368023

Epoch: 6| Step: 1
Training loss: 2.8287434577941895
Validation loss: 2.6782623772980063

Epoch: 6| Step: 2
Training loss: 2.5042524337768555
Validation loss: 2.6776876500857774

Epoch: 6| Step: 3
Training loss: 3.531893014907837
Validation loss: 2.6717603514271397

Epoch: 6| Step: 4
Training loss: 2.230146884918213
Validation loss: 2.673781728231779

Epoch: 6| Step: 5
Training loss: 2.028057336807251
Validation loss: 2.6755664399875108

Epoch: 6| Step: 6
Training loss: 3.3677871227264404
Validation loss: 2.672526785122451

Epoch: 6| Step: 7
Training loss: 3.3803868293762207
Validation loss: 2.674864435708651

Epoch: 6| Step: 8
Training loss: 2.8835248947143555
Validation loss: 2.6696662979741252

Epoch: 6| Step: 9
Training loss: 3.1366772651672363
Validation loss: 2.670262436712942

Epoch: 6| Step: 10
Training loss: 2.7801103591918945
Validation loss: 2.670692495120469

Epoch: 6| Step: 11
Training loss: 2.8622546195983887
Validation loss: 2.6691447329777542

Epoch: 6| Step: 12
Training loss: 2.58676815032959
Validation loss: 2.6685769173406784

Epoch: 6| Step: 13
Training loss: 1.7696185111999512
Validation loss: 2.668073279883272

Epoch: 88| Step: 0
Training loss: 2.691986560821533
Validation loss: 2.6679074148977957

Epoch: 6| Step: 1
Training loss: 3.0536651611328125
Validation loss: 2.6683263394140426

Epoch: 6| Step: 2
Training loss: 3.3246474266052246
Validation loss: 2.6661376491669686

Epoch: 6| Step: 3
Training loss: 2.5201923847198486
Validation loss: 2.667252820025208

Epoch: 6| Step: 4
Training loss: 3.2968735694885254
Validation loss: 2.666617780603388

Epoch: 6| Step: 5
Training loss: 1.8373034000396729
Validation loss: 2.6653113877901466

Epoch: 6| Step: 6
Training loss: 2.603776454925537
Validation loss: 2.66453359716682

Epoch: 6| Step: 7
Training loss: 2.9386799335479736
Validation loss: 2.6659106567341793

Epoch: 6| Step: 8
Training loss: 2.687253475189209
Validation loss: 2.667063013199837

Epoch: 6| Step: 9
Training loss: 3.4768826961517334
Validation loss: 2.666252697667768

Epoch: 6| Step: 10
Training loss: 2.8008604049682617
Validation loss: 2.6665674653104556

Epoch: 6| Step: 11
Training loss: 2.844967842102051
Validation loss: 2.671944131133377

Epoch: 6| Step: 12
Training loss: 2.297637939453125
Validation loss: 2.6718441183849047

Epoch: 6| Step: 13
Training loss: 2.9416160583496094
Validation loss: 2.6709436037207164

Epoch: 89| Step: 0
Training loss: 2.362945556640625
Validation loss: 2.6694036171000493

Epoch: 6| Step: 1
Training loss: 3.1050753593444824
Validation loss: 2.673135167808943

Epoch: 6| Step: 2
Training loss: 2.6196415424346924
Validation loss: 2.6725071655806674

Epoch: 6| Step: 3
Training loss: 2.2368099689483643
Validation loss: 2.6784432805994505

Epoch: 6| Step: 4
Training loss: 2.517636299133301
Validation loss: 2.6935270858067337

Epoch: 6| Step: 5
Training loss: 2.8480424880981445
Validation loss: 2.691177821928455

Epoch: 6| Step: 6
Training loss: 2.784073829650879
Validation loss: 2.683482462360013

Epoch: 6| Step: 7
Training loss: 3.024749279022217
Validation loss: 2.6834899481906684

Epoch: 6| Step: 8
Training loss: 2.2156620025634766
Validation loss: 2.682545474780503

Epoch: 6| Step: 9
Training loss: 3.0249407291412354
Validation loss: 2.675643951662125

Epoch: 6| Step: 10
Training loss: 2.5954365730285645
Validation loss: 2.6768061704533075

Epoch: 6| Step: 11
Training loss: 3.340985059738159
Validation loss: 2.67950027988803

Epoch: 6| Step: 12
Training loss: 3.181972026824951
Validation loss: 2.6752719366422264

Epoch: 6| Step: 13
Training loss: 3.876039743423462
Validation loss: 2.670960364803191

Epoch: 90| Step: 0
Training loss: 3.0187904834747314
Validation loss: 2.668356790337511

Epoch: 6| Step: 1
Training loss: 2.2700624465942383
Validation loss: 2.667102531720233

Epoch: 6| Step: 2
Training loss: 3.3083181381225586
Validation loss: 2.6735966461960987

Epoch: 6| Step: 3
Training loss: 2.958181858062744
Validation loss: 2.6691499602410103

Epoch: 6| Step: 4
Training loss: 2.3453121185302734
Validation loss: 2.6706465803166872

Epoch: 6| Step: 5
Training loss: 2.8771719932556152
Validation loss: 2.681137510525283

Epoch: 6| Step: 6
Training loss: 3.326791286468506
Validation loss: 2.6863706906636557

Epoch: 6| Step: 7
Training loss: 2.2826972007751465
Validation loss: 2.674656250143564

Epoch: 6| Step: 8
Training loss: 3.085757255554199
Validation loss: 2.672563914329775

Epoch: 6| Step: 9
Training loss: 2.750253677368164
Validation loss: 2.671508043043075

Epoch: 6| Step: 10
Training loss: 3.282536029815674
Validation loss: 2.664209927282026

Epoch: 6| Step: 11
Training loss: 3.121100425720215
Validation loss: 2.6698506391176613

Epoch: 6| Step: 12
Training loss: 1.8376984596252441
Validation loss: 2.665963698458928

Epoch: 6| Step: 13
Training loss: 2.602842092514038
Validation loss: 2.6617562206842567

Epoch: 91| Step: 0
Training loss: 2.066527843475342
Validation loss: 2.6734913395297144

Epoch: 6| Step: 1
Training loss: 2.9983162879943848
Validation loss: 2.6693678568768244

Epoch: 6| Step: 2
Training loss: 2.656977653503418
Validation loss: 2.6783582907851025

Epoch: 6| Step: 3
Training loss: 2.355225086212158
Validation loss: 2.6749162058676443

Epoch: 6| Step: 4
Training loss: 2.604434013366699
Validation loss: 2.6736233336951143

Epoch: 6| Step: 5
Training loss: 3.3264527320861816
Validation loss: 2.671508337861748

Epoch: 6| Step: 6
Training loss: 2.338778495788574
Validation loss: 2.672184275042626

Epoch: 6| Step: 7
Training loss: 2.6126227378845215
Validation loss: 2.6687298359409457

Epoch: 6| Step: 8
Training loss: 2.410297155380249
Validation loss: 2.671995332164149

Epoch: 6| Step: 9
Training loss: 3.078866720199585
Validation loss: 2.6664789338265695

Epoch: 6| Step: 10
Training loss: 3.4900357723236084
Validation loss: 2.664991637711884

Epoch: 6| Step: 11
Training loss: 3.3446555137634277
Validation loss: 2.665619788631316

Epoch: 6| Step: 12
Training loss: 2.6205124855041504
Validation loss: 2.661473379340223

Epoch: 6| Step: 13
Training loss: 3.72318434715271
Validation loss: 2.6615671086054977

Epoch: 92| Step: 0
Training loss: 2.757963180541992
Validation loss: 2.660408689129737

Epoch: 6| Step: 1
Training loss: 2.804931640625
Validation loss: 2.663567089265393

Epoch: 6| Step: 2
Training loss: 2.2565150260925293
Validation loss: 2.667457119111092

Epoch: 6| Step: 3
Training loss: 2.872389316558838
Validation loss: 2.67017985159351

Epoch: 6| Step: 4
Training loss: 2.7364282608032227
Validation loss: 2.6676846652902584

Epoch: 6| Step: 5
Training loss: 3.029602527618408
Validation loss: 2.6644479100422194

Epoch: 6| Step: 6
Training loss: 2.3237948417663574
Validation loss: 2.6623551999368975

Epoch: 6| Step: 7
Training loss: 3.0285258293151855
Validation loss: 2.6606248271080757

Epoch: 6| Step: 8
Training loss: 2.2856502532958984
Validation loss: 2.6594958664268575

Epoch: 6| Step: 9
Training loss: 2.9752907752990723
Validation loss: 2.659128540305681

Epoch: 6| Step: 10
Training loss: 3.1983065605163574
Validation loss: 2.6640525453834125

Epoch: 6| Step: 11
Training loss: 3.026994228363037
Validation loss: 2.6653558105550785

Epoch: 6| Step: 12
Training loss: 3.3327598571777344
Validation loss: 2.6606368582735778

Epoch: 6| Step: 13
Training loss: 2.5243101119995117
Validation loss: 2.6660265512363885

Epoch: 93| Step: 0
Training loss: 1.6460750102996826
Validation loss: 2.663088834413918

Epoch: 6| Step: 1
Training loss: 3.672018051147461
Validation loss: 2.6628470061927714

Epoch: 6| Step: 2
Training loss: 2.9792580604553223
Validation loss: 2.6615815239567913

Epoch: 6| Step: 3
Training loss: 3.390981435775757
Validation loss: 2.66268834247384

Epoch: 6| Step: 4
Training loss: 2.081346035003662
Validation loss: 2.662689091056906

Epoch: 6| Step: 5
Training loss: 3.077868938446045
Validation loss: 2.656607304849932

Epoch: 6| Step: 6
Training loss: 2.971221923828125
Validation loss: 2.65224168890266

Epoch: 6| Step: 7
Training loss: 3.353661060333252
Validation loss: 2.655567043571062

Epoch: 6| Step: 8
Training loss: 2.450134038925171
Validation loss: 2.6576312793198453

Epoch: 6| Step: 9
Training loss: 2.4774224758148193
Validation loss: 2.6529593518985215

Epoch: 6| Step: 10
Training loss: 3.3350415229797363
Validation loss: 2.655561416379867

Epoch: 6| Step: 11
Training loss: 2.518725872039795
Validation loss: 2.653050499577676

Epoch: 6| Step: 12
Training loss: 2.340017318725586
Validation loss: 2.6529928253542994

Epoch: 6| Step: 13
Training loss: 2.877527952194214
Validation loss: 2.65784885550058

Epoch: 94| Step: 0
Training loss: 1.851924180984497
Validation loss: 2.6543817481686993

Epoch: 6| Step: 1
Training loss: 2.4864425659179688
Validation loss: 2.660222976438461

Epoch: 6| Step: 2
Training loss: 3.2761569023132324
Validation loss: 2.659570268405381

Epoch: 6| Step: 3
Training loss: 3.3884663581848145
Validation loss: 2.65979459721555

Epoch: 6| Step: 4
Training loss: 2.9998812675476074
Validation loss: 2.655191416381508

Epoch: 6| Step: 5
Training loss: 2.469165802001953
Validation loss: 2.6544125567200365

Epoch: 6| Step: 6
Training loss: 1.8225079774856567
Validation loss: 2.654908000781972

Epoch: 6| Step: 7
Training loss: 2.992912530899048
Validation loss: 2.655160193802208

Epoch: 6| Step: 8
Training loss: 3.119349241256714
Validation loss: 2.658904124331731

Epoch: 6| Step: 9
Training loss: 2.7886061668395996
Validation loss: 2.6631126249990156

Epoch: 6| Step: 10
Training loss: 3.469141721725464
Validation loss: 2.659643678254979

Epoch: 6| Step: 11
Training loss: 2.268653392791748
Validation loss: 2.659623989494898

Epoch: 6| Step: 12
Training loss: 2.793287754058838
Validation loss: 2.653032910439276

Epoch: 6| Step: 13
Training loss: 3.8338961601257324
Validation loss: 2.6570345586346042

Epoch: 95| Step: 0
Training loss: 2.4497928619384766
Validation loss: 2.6583724893549436

Epoch: 6| Step: 1
Training loss: 2.296386957168579
Validation loss: 2.6598384380340576

Epoch: 6| Step: 2
Training loss: 3.1700997352600098
Validation loss: 2.6576718258601364

Epoch: 6| Step: 3
Training loss: 3.332303285598755
Validation loss: 2.6575675549045688

Epoch: 6| Step: 4
Training loss: 3.175930976867676
Validation loss: 2.659033703547652

Epoch: 6| Step: 5
Training loss: 3.0814735889434814
Validation loss: 2.6633519690523864

Epoch: 6| Step: 6
Training loss: 2.1969590187072754
Validation loss: 2.654813351169709

Epoch: 6| Step: 7
Training loss: 2.5482735633850098
Validation loss: 2.657890799225018

Epoch: 6| Step: 8
Training loss: 3.5829100608825684
Validation loss: 2.6568297955297653

Epoch: 6| Step: 9
Training loss: 2.402601480484009
Validation loss: 2.651528755823771

Epoch: 6| Step: 10
Training loss: 2.1015422344207764
Validation loss: 2.654172661483929

Epoch: 6| Step: 11
Training loss: 2.3688838481903076
Validation loss: 2.6542269440107447

Epoch: 6| Step: 12
Training loss: 2.952597141265869
Validation loss: 2.6548075188872633

Epoch: 6| Step: 13
Training loss: 3.9016103744506836
Validation loss: 2.6531822758336223

Epoch: 96| Step: 0
Training loss: 3.134638547897339
Validation loss: 2.653858851361018

Epoch: 6| Step: 1
Training loss: 2.8201522827148438
Validation loss: 2.6514861506800496

Epoch: 6| Step: 2
Training loss: 3.2940595149993896
Validation loss: 2.6510900143654115

Epoch: 6| Step: 3
Training loss: 2.431380271911621
Validation loss: 2.6505763607640422

Epoch: 6| Step: 4
Training loss: 3.0575191974639893
Validation loss: 2.655582986852174

Epoch: 6| Step: 5
Training loss: 2.695286989212036
Validation loss: 2.658068431321011

Epoch: 6| Step: 6
Training loss: 2.4709696769714355
Validation loss: 2.6580729407648884

Epoch: 6| Step: 7
Training loss: 2.6440305709838867
Validation loss: 2.662008936687182

Epoch: 6| Step: 8
Training loss: 3.1234607696533203
Validation loss: 2.6760848952877905

Epoch: 6| Step: 9
Training loss: 2.5407509803771973
Validation loss: 2.684524110568467

Epoch: 6| Step: 10
Training loss: 2.9585139751434326
Validation loss: 2.681470835080711

Epoch: 6| Step: 11
Training loss: 3.2928967475891113
Validation loss: 2.6872501886019142

Epoch: 6| Step: 12
Training loss: 2.4066126346588135
Validation loss: 2.6837054785861763

Epoch: 6| Step: 13
Training loss: 1.9217675924301147
Validation loss: 2.67777011727774

Epoch: 97| Step: 0
Training loss: 2.1595301628112793
Validation loss: 2.667611675877725

Epoch: 6| Step: 1
Training loss: 2.292424440383911
Validation loss: 2.661290127743957

Epoch: 6| Step: 2
Training loss: 3.2127392292022705
Validation loss: 2.661728920475129

Epoch: 6| Step: 3
Training loss: 2.465935230255127
Validation loss: 2.66133532216472

Epoch: 6| Step: 4
Training loss: 2.1626687049865723
Validation loss: 2.655552428255799

Epoch: 6| Step: 5
Training loss: 3.175575017929077
Validation loss: 2.655360832009264

Epoch: 6| Step: 6
Training loss: 2.6039934158325195
Validation loss: 2.6546651368500083

Epoch: 6| Step: 7
Training loss: 3.3133177757263184
Validation loss: 2.650660414849558

Epoch: 6| Step: 8
Training loss: 3.3347320556640625
Validation loss: 2.653360389894055

Epoch: 6| Step: 9
Training loss: 2.40261173248291
Validation loss: 2.6486187186292423

Epoch: 6| Step: 10
Training loss: 2.5880517959594727
Validation loss: 2.65080016146424

Epoch: 6| Step: 11
Training loss: 3.7420363426208496
Validation loss: 2.653876471263106

Epoch: 6| Step: 12
Training loss: 2.845938205718994
Validation loss: 2.656292733325753

Epoch: 6| Step: 13
Training loss: 2.557429313659668
Validation loss: 2.6620805366064912

Epoch: 98| Step: 0
Training loss: 2.682908296585083
Validation loss: 2.6687617532668577

Epoch: 6| Step: 1
Training loss: 3.1393542289733887
Validation loss: 2.67602829522984

Epoch: 6| Step: 2
Training loss: 3.2793688774108887
Validation loss: 2.6702449270474014

Epoch: 6| Step: 3
Training loss: 1.8022264242172241
Validation loss: 2.681844521594304

Epoch: 6| Step: 4
Training loss: 2.7662830352783203
Validation loss: 2.6888989325492614

Epoch: 6| Step: 5
Training loss: 3.741269111633301
Validation loss: 2.6882382208301174

Epoch: 6| Step: 6
Training loss: 2.8423123359680176
Validation loss: 2.680624715743526

Epoch: 6| Step: 7
Training loss: 2.4893126487731934
Validation loss: 2.659346031886275

Epoch: 6| Step: 8
Training loss: 2.600928783416748
Validation loss: 2.6544834798382175

Epoch: 6| Step: 9
Training loss: 2.376763343811035
Validation loss: 2.6632652744170158

Epoch: 6| Step: 10
Training loss: 3.410869598388672
Validation loss: 2.6672461237958682

Epoch: 6| Step: 11
Training loss: 2.502943515777588
Validation loss: 2.659688154856364

Epoch: 6| Step: 12
Training loss: 2.6882572174072266
Validation loss: 2.6723328508356565

Epoch: 6| Step: 13
Training loss: 2.7103796005249023
Validation loss: 2.6729147588053057

Epoch: 99| Step: 0
Training loss: 2.42565655708313
Validation loss: 2.6720029820678053

Epoch: 6| Step: 1
Training loss: 2.167576789855957
Validation loss: 2.6681912868253645

Epoch: 6| Step: 2
Training loss: 3.101496696472168
Validation loss: 2.6644547241990284

Epoch: 6| Step: 3
Training loss: 3.0800139904022217
Validation loss: 2.6634233946441324

Epoch: 6| Step: 4
Training loss: 2.6499266624450684
Validation loss: 2.6529447776015087

Epoch: 6| Step: 5
Training loss: 3.7461318969726562
Validation loss: 2.6501656450251097

Epoch: 6| Step: 6
Training loss: 3.7163782119750977
Validation loss: 2.651481387435749

Epoch: 6| Step: 7
Training loss: 2.306931734085083
Validation loss: 2.6474057653898835

Epoch: 6| Step: 8
Training loss: 3.0339419841766357
Validation loss: 2.647421060069915

Epoch: 6| Step: 9
Training loss: 3.0766801834106445
Validation loss: 2.641752350714899

Epoch: 6| Step: 10
Training loss: 2.6773078441619873
Validation loss: 2.647891062562184

Epoch: 6| Step: 11
Training loss: 2.122753620147705
Validation loss: 2.6496883028297016

Epoch: 6| Step: 12
Training loss: 2.464627265930176
Validation loss: 2.6476771164965887

Epoch: 6| Step: 13
Training loss: 2.2379250526428223
Validation loss: 2.6523116660374466

Epoch: 100| Step: 0
Training loss: 2.307910442352295
Validation loss: 2.651827550703479

Epoch: 6| Step: 1
Training loss: 1.820788860321045
Validation loss: 2.656707191980013

Epoch: 6| Step: 2
Training loss: 3.3311350345611572
Validation loss: 2.6556172781093146

Epoch: 6| Step: 3
Training loss: 2.1202828884124756
Validation loss: 2.6719389218156055

Epoch: 6| Step: 4
Training loss: 2.74003267288208
Validation loss: 2.678050733381702

Epoch: 6| Step: 5
Training loss: 2.787778615951538
Validation loss: 2.6780203132219214

Epoch: 6| Step: 6
Training loss: 3.0414228439331055
Validation loss: 2.6738110511533675

Epoch: 6| Step: 7
Training loss: 2.7181906700134277
Validation loss: 2.6714162903447307

Epoch: 6| Step: 8
Training loss: 3.6119465827941895
Validation loss: 2.659395433241321

Epoch: 6| Step: 9
Training loss: 3.709503650665283
Validation loss: 2.6664610344876527

Epoch: 6| Step: 10
Training loss: 2.737931251525879
Validation loss: 2.673706531524658

Epoch: 6| Step: 11
Training loss: 2.4299516677856445
Validation loss: 2.6743450703159457

Epoch: 6| Step: 12
Training loss: 3.169888973236084
Validation loss: 2.6659328399165982

Epoch: 6| Step: 13
Training loss: 2.4624884128570557
Validation loss: 2.6615528009271108

Epoch: 101| Step: 0
Training loss: 2.5652341842651367
Validation loss: 2.6537748434210338

Epoch: 6| Step: 1
Training loss: 3.2810235023498535
Validation loss: 2.6448401610056558

Epoch: 6| Step: 2
Training loss: 2.7800042629241943
Validation loss: 2.64469821991459

Epoch: 6| Step: 3
Training loss: 2.8313822746276855
Validation loss: 2.6521401379698064

Epoch: 6| Step: 4
Training loss: 2.402939558029175
Validation loss: 2.6541848951770413

Epoch: 6| Step: 5
Training loss: 3.163594961166382
Validation loss: 2.658689593756071

Epoch: 6| Step: 6
Training loss: 3.1470279693603516
Validation loss: 2.65681347539348

Epoch: 6| Step: 7
Training loss: 2.3914384841918945
Validation loss: 2.6578157486454135

Epoch: 6| Step: 8
Training loss: 3.1902146339416504
Validation loss: 2.652290521129485

Epoch: 6| Step: 9
Training loss: 3.383265733718872
Validation loss: 2.6485571604903027

Epoch: 6| Step: 10
Training loss: 2.5250511169433594
Validation loss: 2.644785224750478

Epoch: 6| Step: 11
Training loss: 1.9707542657852173
Validation loss: 2.6403780547521447

Epoch: 6| Step: 12
Training loss: 2.7918245792388916
Validation loss: 2.642341854751751

Epoch: 6| Step: 13
Training loss: 2.642319917678833
Validation loss: 2.64140740261283

Epoch: 102| Step: 0
Training loss: 2.357881546020508
Validation loss: 2.639133481569188

Epoch: 6| Step: 1
Training loss: 3.0032002925872803
Validation loss: 2.6387779174312467

Epoch: 6| Step: 2
Training loss: 2.6345314979553223
Validation loss: 2.642538898734636

Epoch: 6| Step: 3
Training loss: 2.634103298187256
Validation loss: 2.637466722919095

Epoch: 6| Step: 4
Training loss: 2.5065579414367676
Validation loss: 2.644736164359636

Epoch: 6| Step: 5
Training loss: 2.668292760848999
Validation loss: 2.6380777307735976

Epoch: 6| Step: 6
Training loss: 2.6202406883239746
Validation loss: 2.6381184106232016

Epoch: 6| Step: 7
Training loss: 2.7360284328460693
Validation loss: 2.6398017303917998

Epoch: 6| Step: 8
Training loss: 2.5098540782928467
Validation loss: 2.6364317529944965

Epoch: 6| Step: 9
Training loss: 3.414987802505493
Validation loss: 2.637022331196775

Epoch: 6| Step: 10
Training loss: 3.270111560821533
Validation loss: 2.6390980418010423

Epoch: 6| Step: 11
Training loss: 2.9861674308776855
Validation loss: 2.642311126955094

Epoch: 6| Step: 12
Training loss: 3.072890043258667
Validation loss: 2.6426120368383264

Epoch: 6| Step: 13
Training loss: 2.4110023975372314
Validation loss: 2.644578920897617

Epoch: 103| Step: 0
Training loss: 2.7733473777770996
Validation loss: 2.639497874885477

Epoch: 6| Step: 1
Training loss: 3.324643611907959
Validation loss: 2.637829895942442

Epoch: 6| Step: 2
Training loss: 2.229306936264038
Validation loss: 2.6380592751246628

Epoch: 6| Step: 3
Training loss: 2.2782459259033203
Validation loss: 2.635119997045045

Epoch: 6| Step: 4
Training loss: 3.270054817199707
Validation loss: 2.6335141966419835

Epoch: 6| Step: 5
Training loss: 2.4530141353607178
Validation loss: 2.6335183189761255

Epoch: 6| Step: 6
Training loss: 2.9489593505859375
Validation loss: 2.631167833523084

Epoch: 6| Step: 7
Training loss: 2.5885159969329834
Validation loss: 2.6361840412180912

Epoch: 6| Step: 8
Training loss: 2.7074949741363525
Validation loss: 2.6345913282004734

Epoch: 6| Step: 9
Training loss: 3.3680660724639893
Validation loss: 2.6349074994364092

Epoch: 6| Step: 10
Training loss: 3.2269060611724854
Validation loss: 2.6394973724119124

Epoch: 6| Step: 11
Training loss: 2.374765157699585
Validation loss: 2.639063266015822

Epoch: 6| Step: 12
Training loss: 2.6533238887786865
Validation loss: 2.639259056378436

Epoch: 6| Step: 13
Training loss: 2.8636889457702637
Validation loss: 2.635559833177956

Epoch: 104| Step: 0
Training loss: 2.9572033882141113
Validation loss: 2.6335195367054274

Epoch: 6| Step: 1
Training loss: 2.3395090103149414
Validation loss: 2.6373682175913165

Epoch: 6| Step: 2
Training loss: 2.594209909439087
Validation loss: 2.63563528624914

Epoch: 6| Step: 3
Training loss: 2.7832393646240234
Validation loss: 2.636922738885367

Epoch: 6| Step: 4
Training loss: 2.28328537940979
Validation loss: 2.641335710402458

Epoch: 6| Step: 5
Training loss: 1.7016032934188843
Validation loss: 2.6390713158474175

Epoch: 6| Step: 6
Training loss: 2.704503297805786
Validation loss: 2.644349213569395

Epoch: 6| Step: 7
Training loss: 3.241582155227661
Validation loss: 2.6485248842547016

Epoch: 6| Step: 8
Training loss: 3.8997881412506104
Validation loss: 2.662149488285024

Epoch: 6| Step: 9
Training loss: 3.243450880050659
Validation loss: 2.6614715283916843

Epoch: 6| Step: 10
Training loss: 2.6882927417755127
Validation loss: 2.6660400436770533

Epoch: 6| Step: 11
Training loss: 2.478325366973877
Validation loss: 2.6602641587616294

Epoch: 6| Step: 12
Training loss: 2.1039140224456787
Validation loss: 2.6559898545665126

Epoch: 6| Step: 13
Training loss: 4.942397117614746
Validation loss: 2.6485774260695263

Epoch: 105| Step: 0
Training loss: 3.7444915771484375
Validation loss: 2.6404927622887397

Epoch: 6| Step: 1
Training loss: 2.319154739379883
Validation loss: 2.6360846027251212

Epoch: 6| Step: 2
Training loss: 2.743288516998291
Validation loss: 2.6327924523302304

Epoch: 6| Step: 3
Training loss: 3.492377281188965
Validation loss: 2.629888924219275

Epoch: 6| Step: 4
Training loss: 3.11080265045166
Validation loss: 2.6270465799557265

Epoch: 6| Step: 5
Training loss: 2.8179736137390137
Validation loss: 2.626293497700845

Epoch: 6| Step: 6
Training loss: 2.2590126991271973
Validation loss: 2.624778151512146

Epoch: 6| Step: 7
Training loss: 2.0322751998901367
Validation loss: 2.626212648166123

Epoch: 6| Step: 8
Training loss: 2.5494844913482666
Validation loss: 2.627269406472483

Epoch: 6| Step: 9
Training loss: 2.6408143043518066
Validation loss: 2.6279096808484805

Epoch: 6| Step: 10
Training loss: 3.0339980125427246
Validation loss: 2.6295219339350218

Epoch: 6| Step: 11
Training loss: 2.674968719482422
Validation loss: 2.6279922967316

Epoch: 6| Step: 12
Training loss: 2.9209883213043213
Validation loss: 2.6269897850610877

Epoch: 6| Step: 13
Training loss: 2.5453782081604004
Validation loss: 2.6259930262001614

Epoch: 106| Step: 0
Training loss: 2.8902804851531982
Validation loss: 2.628272556489514

Epoch: 6| Step: 1
Training loss: 3.6957077980041504
Validation loss: 2.6263938744862876

Epoch: 6| Step: 2
Training loss: 2.5225586891174316
Validation loss: 2.623545467212636

Epoch: 6| Step: 3
Training loss: 2.3696329593658447
Validation loss: 2.625225326066376

Epoch: 6| Step: 4
Training loss: 3.4050116539001465
Validation loss: 2.62489684935539

Epoch: 6| Step: 5
Training loss: 2.507737636566162
Validation loss: 2.627738783436437

Epoch: 6| Step: 6
Training loss: 2.3294169902801514
Validation loss: 2.624138221945814

Epoch: 6| Step: 7
Training loss: 2.2241344451904297
Validation loss: 2.6250750915978545

Epoch: 6| Step: 8
Training loss: 3.6031012535095215
Validation loss: 2.6246274696883334

Epoch: 6| Step: 9
Training loss: 2.5227274894714355
Validation loss: 2.6232654176732546

Epoch: 6| Step: 10
Training loss: 2.6170244216918945
Validation loss: 2.622062334450342

Epoch: 6| Step: 11
Training loss: 2.8358101844787598
Validation loss: 2.622700665586738

Epoch: 6| Step: 12
Training loss: 2.9058377742767334
Validation loss: 2.622353228189612

Epoch: 6| Step: 13
Training loss: 2.2264270782470703
Validation loss: 2.627247174580892

Epoch: 107| Step: 0
Training loss: 1.8928391933441162
Validation loss: 2.6232464005870204

Epoch: 6| Step: 1
Training loss: 2.8150949478149414
Validation loss: 2.6248590433469383

Epoch: 6| Step: 2
Training loss: 3.010312795639038
Validation loss: 2.624576581421719

Epoch: 6| Step: 3
Training loss: 2.2738699913024902
Validation loss: 2.6195094713600735

Epoch: 6| Step: 4
Training loss: 2.8254940509796143
Validation loss: 2.621871727769093

Epoch: 6| Step: 5
Training loss: 3.144679546356201
Validation loss: 2.6233646305658485

Epoch: 6| Step: 6
Training loss: 3.0171046257019043
Validation loss: 2.622161216633294

Epoch: 6| Step: 7
Training loss: 2.512486457824707
Validation loss: 2.618756407050676

Epoch: 6| Step: 8
Training loss: 2.4192557334899902
Validation loss: 2.6234371303230204

Epoch: 6| Step: 9
Training loss: 3.6920559406280518
Validation loss: 2.621568270908889

Epoch: 6| Step: 10
Training loss: 2.934597969055176
Validation loss: 2.624440808450022

Epoch: 6| Step: 11
Training loss: 2.0565133094787598
Validation loss: 2.6267766619241364

Epoch: 6| Step: 12
Training loss: 3.093207836151123
Validation loss: 2.6282843671819216

Epoch: 6| Step: 13
Training loss: 3.4384946823120117
Validation loss: 2.6353687727323143

Epoch: 108| Step: 0
Training loss: 2.0133707523345947
Validation loss: 2.631459543781896

Epoch: 6| Step: 1
Training loss: 3.0358095169067383
Validation loss: 2.632147296782463

Epoch: 6| Step: 2
Training loss: 1.9071776866912842
Validation loss: 2.6286255749323035

Epoch: 6| Step: 3
Training loss: 2.640598773956299
Validation loss: 2.6245859592191634

Epoch: 6| Step: 4
Training loss: 2.772815704345703
Validation loss: 2.620266301657564

Epoch: 6| Step: 5
Training loss: 3.7207565307617188
Validation loss: 2.620644146396268

Epoch: 6| Step: 6
Training loss: 3.8216800689697266
Validation loss: 2.615931959562404

Epoch: 6| Step: 7
Training loss: 2.6156296730041504
Validation loss: 2.6199751823179183

Epoch: 6| Step: 8
Training loss: 2.8411662578582764
Validation loss: 2.617241741508566

Epoch: 6| Step: 9
Training loss: 2.575408697128296
Validation loss: 2.6190333340757634

Epoch: 6| Step: 10
Training loss: 2.8850252628326416
Validation loss: 2.6198342820649505

Epoch: 6| Step: 11
Training loss: 2.7600808143615723
Validation loss: 2.6175645884647163

Epoch: 6| Step: 12
Training loss: 2.440800666809082
Validation loss: 2.620421568552653

Epoch: 6| Step: 13
Training loss: 2.7299389839172363
Validation loss: 2.6226288810853036

Epoch: 109| Step: 0
Training loss: 4.072366714477539
Validation loss: 2.6226709760645384

Epoch: 6| Step: 1
Training loss: 2.120218515396118
Validation loss: 2.622257596702986

Epoch: 6| Step: 2
Training loss: 2.916761636734009
Validation loss: 2.6194889801804737

Epoch: 6| Step: 3
Training loss: 3.2361936569213867
Validation loss: 2.6210799063405683

Epoch: 6| Step: 4
Training loss: 2.8850255012512207
Validation loss: 2.623320561583324

Epoch: 6| Step: 5
Training loss: 2.6899495124816895
Validation loss: 2.6283500579095658

Epoch: 6| Step: 6
Training loss: 2.993434429168701
Validation loss: 2.6232330209465435

Epoch: 6| Step: 7
Training loss: 2.4706664085388184
Validation loss: 2.6245751637284473

Epoch: 6| Step: 8
Training loss: 2.791750431060791
Validation loss: 2.625471612458588

Epoch: 6| Step: 9
Training loss: 2.323610544204712
Validation loss: 2.6210187378750054

Epoch: 6| Step: 10
Training loss: 2.431112289428711
Validation loss: 2.6187362235079528

Epoch: 6| Step: 11
Training loss: 2.145899772644043
Validation loss: 2.6209951472538773

Epoch: 6| Step: 12
Training loss: 3.347155809402466
Validation loss: 2.622029128895011

Epoch: 6| Step: 13
Training loss: 2.010629653930664
Validation loss: 2.6232320570176646

Epoch: 110| Step: 0
Training loss: 3.4242467880249023
Validation loss: 2.62374444418056

Epoch: 6| Step: 1
Training loss: 2.511641025543213
Validation loss: 2.61932356895939

Epoch: 6| Step: 2
Training loss: 2.281615734100342
Validation loss: 2.626639099531276

Epoch: 6| Step: 3
Training loss: 2.6142847537994385
Validation loss: 2.628563993720598

Epoch: 6| Step: 4
Training loss: 2.4229962825775146
Validation loss: 2.62874517133159

Epoch: 6| Step: 5
Training loss: 2.2404632568359375
Validation loss: 2.641040599474343

Epoch: 6| Step: 6
Training loss: 2.3456923961639404
Validation loss: 2.6605156890807615

Epoch: 6| Step: 7
Training loss: 3.0499825477600098
Validation loss: 2.666519252202844

Epoch: 6| Step: 8
Training loss: 2.766744613647461
Validation loss: 2.637809955945579

Epoch: 6| Step: 9
Training loss: 2.989197015762329
Validation loss: 2.6353612381924867

Epoch: 6| Step: 10
Training loss: 3.123974323272705
Validation loss: 2.633059363211355

Epoch: 6| Step: 11
Training loss: 2.7040722370147705
Validation loss: 2.626685001516855

Epoch: 6| Step: 12
Training loss: 3.477872848510742
Validation loss: 2.628649719299809

Epoch: 6| Step: 13
Training loss: 2.7313950061798096
Validation loss: 2.631977432517595

Epoch: 111| Step: 0
Training loss: 2.217212200164795
Validation loss: 2.6329990817654516

Epoch: 6| Step: 1
Training loss: 2.890516519546509
Validation loss: 2.644464990144135

Epoch: 6| Step: 2
Training loss: 2.585989475250244
Validation loss: 2.638030787949921

Epoch: 6| Step: 3
Training loss: 3.3016083240509033
Validation loss: 2.6390463254785024

Epoch: 6| Step: 4
Training loss: 2.211695671081543
Validation loss: 2.6384742823980187

Epoch: 6| Step: 5
Training loss: 2.623530387878418
Validation loss: 2.6290873032744213

Epoch: 6| Step: 6
Training loss: 3.3640987873077393
Validation loss: 2.6277949553664013

Epoch: 6| Step: 7
Training loss: 2.6073737144470215
Validation loss: 2.6288043606665825

Epoch: 6| Step: 8
Training loss: 2.703453302383423
Validation loss: 2.620436732487012

Epoch: 6| Step: 9
Training loss: 1.9139564037322998
Validation loss: 2.6148486291208575

Epoch: 6| Step: 10
Training loss: 3.4660110473632812
Validation loss: 2.6143037067946566

Epoch: 6| Step: 11
Training loss: 2.7446141242980957
Validation loss: 2.6147171246108187

Epoch: 6| Step: 12
Training loss: 3.4481117725372314
Validation loss: 2.6135316202717442

Epoch: 6| Step: 13
Training loss: 2.7276248931884766
Validation loss: 2.614417568329842

Epoch: 112| Step: 0
Training loss: 2.5204086303710938
Validation loss: 2.61060486044935

Epoch: 6| Step: 1
Training loss: 3.492826461791992
Validation loss: 2.614784866250971

Epoch: 6| Step: 2
Training loss: 3.080770969390869
Validation loss: 2.6128609923906225

Epoch: 6| Step: 3
Training loss: 3.0066769123077393
Validation loss: 2.6114336162485103

Epoch: 6| Step: 4
Training loss: 2.9558777809143066
Validation loss: 2.613743164206064

Epoch: 6| Step: 5
Training loss: 2.3784399032592773
Validation loss: 2.6129711417741674

Epoch: 6| Step: 6
Training loss: 3.0121428966522217
Validation loss: 2.617652041937715

Epoch: 6| Step: 7
Training loss: 2.387730836868286
Validation loss: 2.6135844056324293

Epoch: 6| Step: 8
Training loss: 1.670461893081665
Validation loss: 2.6196515483240925

Epoch: 6| Step: 9
Training loss: 3.4508209228515625
Validation loss: 2.624537298756261

Epoch: 6| Step: 10
Training loss: 2.8963704109191895
Validation loss: 2.6302612135487218

Epoch: 6| Step: 11
Training loss: 2.5879342555999756
Validation loss: 2.641571457668017

Epoch: 6| Step: 12
Training loss: 2.893240213394165
Validation loss: 2.642487067048268

Epoch: 6| Step: 13
Training loss: 2.0748727321624756
Validation loss: 2.6386043999784734

Epoch: 113| Step: 0
Training loss: 2.922311305999756
Validation loss: 2.6388849109731694

Epoch: 6| Step: 1
Training loss: 3.0414392948150635
Validation loss: 2.623525773325274

Epoch: 6| Step: 2
Training loss: 3.5973522663116455
Validation loss: 2.621575681112146

Epoch: 6| Step: 3
Training loss: 2.182000160217285
Validation loss: 2.621661924546765

Epoch: 6| Step: 4
Training loss: 2.4368157386779785
Validation loss: 2.6180606683095298

Epoch: 6| Step: 5
Training loss: 3.327726364135742
Validation loss: 2.616661289686798

Epoch: 6| Step: 6
Training loss: 3.114609718322754
Validation loss: 2.6126777100306686

Epoch: 6| Step: 7
Training loss: 2.0036821365356445
Validation loss: 2.6137560721366637

Epoch: 6| Step: 8
Training loss: 2.3073039054870605
Validation loss: 2.61499426954536

Epoch: 6| Step: 9
Training loss: 2.633274793624878
Validation loss: 2.6171584103697088

Epoch: 6| Step: 10
Training loss: 2.4008677005767822
Validation loss: 2.6137830749634774

Epoch: 6| Step: 11
Training loss: 3.1660475730895996
Validation loss: 2.61922090284286

Epoch: 6| Step: 12
Training loss: 2.8128297328948975
Validation loss: 2.6193496463119343

Epoch: 6| Step: 13
Training loss: 2.5609588623046875
Validation loss: 2.6162626076770086

Epoch: 114| Step: 0
Training loss: 2.0530295372009277
Validation loss: 2.6148144250274985

Epoch: 6| Step: 1
Training loss: 1.8421587944030762
Validation loss: 2.6183150224788214

Epoch: 6| Step: 2
Training loss: 3.9994213581085205
Validation loss: 2.613635652808733

Epoch: 6| Step: 3
Training loss: 2.734005928039551
Validation loss: 2.6162374506714525

Epoch: 6| Step: 4
Training loss: 3.4109089374542236
Validation loss: 2.6115057711960166

Epoch: 6| Step: 5
Training loss: 2.895314931869507
Validation loss: 2.609427446960121

Epoch: 6| Step: 6
Training loss: 2.2651586532592773
Validation loss: 2.6077075799306235

Epoch: 6| Step: 7
Training loss: 2.0989627838134766
Validation loss: 2.606759250804942

Epoch: 6| Step: 8
Training loss: 2.5783438682556152
Validation loss: 2.606790816912087

Epoch: 6| Step: 9
Training loss: 2.821563243865967
Validation loss: 2.6047687427971953

Epoch: 6| Step: 10
Training loss: 3.0457334518432617
Validation loss: 2.6039265355756207

Epoch: 6| Step: 11
Training loss: 2.6989951133728027
Validation loss: 2.6045888341883177

Epoch: 6| Step: 12
Training loss: 3.258479595184326
Validation loss: 2.6062800858610418

Epoch: 6| Step: 13
Training loss: 3.028271436691284
Validation loss: 2.6061667767904138

Epoch: 115| Step: 0
Training loss: 1.3558201789855957
Validation loss: 2.6100383086871077

Epoch: 6| Step: 1
Training loss: 2.5256266593933105
Validation loss: 2.608910975917693

Epoch: 6| Step: 2
Training loss: 3.470810890197754
Validation loss: 2.60831287599379

Epoch: 6| Step: 3
Training loss: 1.999998688697815
Validation loss: 2.6072154762924358

Epoch: 6| Step: 4
Training loss: 2.6288235187530518
Validation loss: 2.6066254697820193

Epoch: 6| Step: 5
Training loss: 3.3742117881774902
Validation loss: 2.610155649082635

Epoch: 6| Step: 6
Training loss: 3.0034515857696533
Validation loss: 2.6082629644742577

Epoch: 6| Step: 7
Training loss: 3.819159984588623
Validation loss: 2.6133108292856524

Epoch: 6| Step: 8
Training loss: 2.197275161743164
Validation loss: 2.6067578843844834

Epoch: 6| Step: 9
Training loss: 3.594292163848877
Validation loss: 2.610612528298491

Epoch: 6| Step: 10
Training loss: 2.392756223678589
Validation loss: 2.614511607795633

Epoch: 6| Step: 11
Training loss: 2.8351082801818848
Validation loss: 2.6132856492073304

Epoch: 6| Step: 12
Training loss: 2.835892677307129
Validation loss: 2.608031234433574

Epoch: 6| Step: 13
Training loss: 2.3253328800201416
Validation loss: 2.608589538963892

Epoch: 116| Step: 0
Training loss: 2.9693853855133057
Validation loss: 2.6042704146395446

Epoch: 6| Step: 1
Training loss: 2.5178070068359375
Validation loss: 2.604542273347096

Epoch: 6| Step: 2
Training loss: 2.705090045928955
Validation loss: 2.6096959549893617

Epoch: 6| Step: 3
Training loss: 3.2481260299682617
Validation loss: 2.6172965265089467

Epoch: 6| Step: 4
Training loss: 3.3384132385253906
Validation loss: 2.6122658303988877

Epoch: 6| Step: 5
Training loss: 2.7545390129089355
Validation loss: 2.6270454750266126

Epoch: 6| Step: 6
Training loss: 2.9823381900787354
Validation loss: 2.6248902761808006

Epoch: 6| Step: 7
Training loss: 2.273970127105713
Validation loss: 2.6211594176548783

Epoch: 6| Step: 8
Training loss: 2.577845335006714
Validation loss: 2.617329966637396

Epoch: 6| Step: 9
Training loss: 2.609788656234741
Validation loss: 2.6140338605450046

Epoch: 6| Step: 10
Training loss: 2.9635233879089355
Validation loss: 2.608584137373073

Epoch: 6| Step: 11
Training loss: 2.4686520099639893
Validation loss: 2.6055773740173667

Epoch: 6| Step: 12
Training loss: 3.120945453643799
Validation loss: 2.6020303054522445

Epoch: 6| Step: 13
Training loss: 1.6191343069076538
Validation loss: 2.602928940967847

Epoch: 117| Step: 0
Training loss: 2.2473573684692383
Validation loss: 2.605842759532313

Epoch: 6| Step: 1
Training loss: 2.598874568939209
Validation loss: 2.6083014447201966

Epoch: 6| Step: 2
Training loss: 3.195218324661255
Validation loss: 2.6127934532780803

Epoch: 6| Step: 3
Training loss: 3.20747709274292
Validation loss: 2.6151788798711633

Epoch: 6| Step: 4
Training loss: 3.224588632583618
Validation loss: 2.6137165049070954

Epoch: 6| Step: 5
Training loss: 3.1102757453918457
Validation loss: 2.614172604776198

Epoch: 6| Step: 6
Training loss: 2.9572067260742188
Validation loss: 2.6091781175264748

Epoch: 6| Step: 7
Training loss: 3.9102749824523926
Validation loss: 2.6163808786740868

Epoch: 6| Step: 8
Training loss: 2.428133964538574
Validation loss: 2.609697026591147

Epoch: 6| Step: 9
Training loss: 2.856222629547119
Validation loss: 2.609856682439004

Epoch: 6| Step: 10
Training loss: 1.7988572120666504
Validation loss: 2.601066848283173

Epoch: 6| Step: 11
Training loss: 1.676696538925171
Validation loss: 2.611562687863586

Epoch: 6| Step: 12
Training loss: 2.1515514850616455
Validation loss: 2.614430539069637

Epoch: 6| Step: 13
Training loss: 3.535090208053589
Validation loss: 2.614594664624942

Epoch: 118| Step: 0
Training loss: 2.802295684814453
Validation loss: 2.6062093921886977

Epoch: 6| Step: 1
Training loss: 1.9730234146118164
Validation loss: 2.604392138860559

Epoch: 6| Step: 2
Training loss: 2.755980968475342
Validation loss: 2.614998194479173

Epoch: 6| Step: 3
Training loss: 3.3497986793518066
Validation loss: 2.613677737533405

Epoch: 6| Step: 4
Training loss: 2.2779123783111572
Validation loss: 2.6186656310994136

Epoch: 6| Step: 5
Training loss: 3.3851637840270996
Validation loss: 2.6184468038620485

Epoch: 6| Step: 6
Training loss: 3.2285659313201904
Validation loss: 2.6181091236811813

Epoch: 6| Step: 7
Training loss: 1.6669672727584839
Validation loss: 2.6133555596874607

Epoch: 6| Step: 8
Training loss: 2.5103583335876465
Validation loss: 2.6101903402677147

Epoch: 6| Step: 9
Training loss: 2.8661653995513916
Validation loss: 2.6065059067100607

Epoch: 6| Step: 10
Training loss: 4.328412055969238
Validation loss: 2.6009390174701648

Epoch: 6| Step: 11
Training loss: 2.735689640045166
Validation loss: 2.5969044546927176

Epoch: 6| Step: 12
Training loss: 2.066892385482788
Validation loss: 2.5918951137091524

Epoch: 6| Step: 13
Training loss: 2.5092196464538574
Validation loss: 2.5947414418702484

Epoch: 119| Step: 0
Training loss: 3.107619285583496
Validation loss: 2.592029079314201

Epoch: 6| Step: 1
Training loss: 2.630504846572876
Validation loss: 2.5977063666107836

Epoch: 6| Step: 2
Training loss: 3.650554895401001
Validation loss: 2.597719774451307

Epoch: 6| Step: 3
Training loss: 2.9798583984375
Validation loss: 2.598684831332135

Epoch: 6| Step: 4
Training loss: 2.3053131103515625
Validation loss: 2.605402474762291

Epoch: 6| Step: 5
Training loss: 3.270752429962158
Validation loss: 2.6067572998744186

Epoch: 6| Step: 6
Training loss: 3.1522631645202637
Validation loss: 2.613643959004392

Epoch: 6| Step: 7
Training loss: 2.7996022701263428
Validation loss: 2.61643624049361

Epoch: 6| Step: 8
Training loss: 1.8650648593902588
Validation loss: 2.6129875362560315

Epoch: 6| Step: 9
Training loss: 2.503048896789551
Validation loss: 2.612570321688088

Epoch: 6| Step: 10
Training loss: 2.2326788902282715
Validation loss: 2.6044290193947415

Epoch: 6| Step: 11
Training loss: 2.7120912075042725
Validation loss: 2.608666971165647

Epoch: 6| Step: 12
Training loss: 2.7513270378112793
Validation loss: 2.6031063115724953

Epoch: 6| Step: 13
Training loss: 2.476808786392212
Validation loss: 2.6087220714938257

Epoch: 120| Step: 0
Training loss: 2.960954427719116
Validation loss: 2.608595004645727

Epoch: 6| Step: 1
Training loss: 2.7575876712799072
Validation loss: 2.617859084119079

Epoch: 6| Step: 2
Training loss: 3.466822385787964
Validation loss: 2.6169607434221493

Epoch: 6| Step: 3
Training loss: 2.8367180824279785
Validation loss: 2.613859309945055

Epoch: 6| Step: 4
Training loss: 2.6220550537109375
Validation loss: 2.6138965186252388

Epoch: 6| Step: 5
Training loss: 2.084493398666382
Validation loss: 2.609572310601511

Epoch: 6| Step: 6
Training loss: 3.5370044708251953
Validation loss: 2.606385615564162

Epoch: 6| Step: 7
Training loss: 2.6567065715789795
Validation loss: 2.6003451757533576

Epoch: 6| Step: 8
Training loss: 2.739819049835205
Validation loss: 2.598385536542503

Epoch: 6| Step: 9
Training loss: 3.2325451374053955
Validation loss: 2.5948755100209224

Epoch: 6| Step: 10
Training loss: 1.8546403646469116
Validation loss: 2.595462743953992

Epoch: 6| Step: 11
Training loss: 2.606142282485962
Validation loss: 2.593526065990489

Epoch: 6| Step: 12
Training loss: 2.4316513538360596
Validation loss: 2.593384360754362

Epoch: 6| Step: 13
Training loss: 2.6209867000579834
Validation loss: 2.594702805242231

Epoch: 121| Step: 0
Training loss: 2.345417022705078
Validation loss: 2.5949025025931736

Epoch: 6| Step: 1
Training loss: 3.1378140449523926
Validation loss: 2.5944556907940934

Epoch: 6| Step: 2
Training loss: 2.7415387630462646
Validation loss: 2.599875619334559

Epoch: 6| Step: 3
Training loss: 2.7949140071868896
Validation loss: 2.6003479598670878

Epoch: 6| Step: 4
Training loss: 2.981802225112915
Validation loss: 2.5968861297894548

Epoch: 6| Step: 5
Training loss: 2.933108329772949
Validation loss: 2.594494414585893

Epoch: 6| Step: 6
Training loss: 2.8165628910064697
Validation loss: 2.592970948065481

Epoch: 6| Step: 7
Training loss: 2.5086123943328857
Validation loss: 2.594019410430744

Epoch: 6| Step: 8
Training loss: 2.6805899143218994
Validation loss: 2.5939720856246127

Epoch: 6| Step: 9
Training loss: 2.1152195930480957
Validation loss: 2.5936774899882655

Epoch: 6| Step: 10
Training loss: 3.2493083477020264
Validation loss: 2.5960413666181665

Epoch: 6| Step: 11
Training loss: 3.0108962059020996
Validation loss: 2.5998476397606636

Epoch: 6| Step: 12
Training loss: 2.4185590744018555
Validation loss: 2.6014441956755934

Epoch: 6| Step: 13
Training loss: 2.6467208862304688
Validation loss: 2.6037701188877063

Epoch: 122| Step: 0
Training loss: 2.814568042755127
Validation loss: 2.596681466666601

Epoch: 6| Step: 1
Training loss: 3.155351161956787
Validation loss: 2.6012762054320304

Epoch: 6| Step: 2
Training loss: 3.296290874481201
Validation loss: 2.602269341868739

Epoch: 6| Step: 3
Training loss: 3.0314433574676514
Validation loss: 2.6072634650814916

Epoch: 6| Step: 4
Training loss: 2.308018684387207
Validation loss: 2.6093926275930097

Epoch: 6| Step: 5
Training loss: 2.7799487113952637
Validation loss: 2.6110646391427643

Epoch: 6| Step: 6
Training loss: 2.8644251823425293
Validation loss: 2.607134239647978

Epoch: 6| Step: 7
Training loss: 2.452373504638672
Validation loss: 2.6086253555872108

Epoch: 6| Step: 8
Training loss: 3.6104142665863037
Validation loss: 2.606943835494339

Epoch: 6| Step: 9
Training loss: 2.5211734771728516
Validation loss: 2.6019396781921387

Epoch: 6| Step: 10
Training loss: 1.8823128938674927
Validation loss: 2.6019716083362536

Epoch: 6| Step: 11
Training loss: 2.9314496517181396
Validation loss: 2.6028192043304443

Epoch: 6| Step: 12
Training loss: 2.0388760566711426
Validation loss: 2.601184198933263

Epoch: 6| Step: 13
Training loss: 2.6687517166137695
Validation loss: 2.599666267312983

Epoch: 123| Step: 0
Training loss: 2.785855770111084
Validation loss: 2.5995880737099597

Epoch: 6| Step: 1
Training loss: 2.1377530097961426
Validation loss: 2.6010885213011052

Epoch: 6| Step: 2
Training loss: 2.62766170501709
Validation loss: 2.5925347215385846

Epoch: 6| Step: 3
Training loss: 3.2273502349853516
Validation loss: 2.5919648549890004

Epoch: 6| Step: 4
Training loss: 2.481545925140381
Validation loss: 2.599323107350257

Epoch: 6| Step: 5
Training loss: 3.1548080444335938
Validation loss: 2.597332318623861

Epoch: 6| Step: 6
Training loss: 2.807894229888916
Validation loss: 2.5977416115422405

Epoch: 6| Step: 7
Training loss: 2.868302583694458
Validation loss: 2.5920225702306277

Epoch: 6| Step: 8
Training loss: 2.2105190753936768
Validation loss: 2.5870435084066083

Epoch: 6| Step: 9
Training loss: 2.133744716644287
Validation loss: 2.5881776860965195

Epoch: 6| Step: 10
Training loss: 3.3407540321350098
Validation loss: 2.5890056625489266

Epoch: 6| Step: 11
Training loss: 3.6582794189453125
Validation loss: 2.5957416283187045

Epoch: 6| Step: 12
Training loss: 2.821871757507324
Validation loss: 2.59449311225645

Epoch: 6| Step: 13
Training loss: 1.6424702405929565
Validation loss: 2.5905874518937964

Epoch: 124| Step: 0
Training loss: 2.9319920539855957
Validation loss: 2.5901856012241815

Epoch: 6| Step: 1
Training loss: 3.5308148860931396
Validation loss: 2.589130115765397

Epoch: 6| Step: 2
Training loss: 1.8946692943572998
Validation loss: 2.5830675068721978

Epoch: 6| Step: 3
Training loss: 2.56176495552063
Validation loss: 2.5823297833883636

Epoch: 6| Step: 4
Training loss: 3.234138011932373
Validation loss: 2.58563813342843

Epoch: 6| Step: 5
Training loss: 2.8490755558013916
Validation loss: 2.5845546824957735

Epoch: 6| Step: 6
Training loss: 2.9056787490844727
Validation loss: 2.5925476269055436

Epoch: 6| Step: 7
Training loss: 2.406643867492676
Validation loss: 2.596951938444568

Epoch: 6| Step: 8
Training loss: 2.5472912788391113
Validation loss: 2.601647651323708

Epoch: 6| Step: 9
Training loss: 1.9468610286712646
Validation loss: 2.6055121985814904

Epoch: 6| Step: 10
Training loss: 2.3494534492492676
Validation loss: 2.59995594075931

Epoch: 6| Step: 11
Training loss: 3.772303581237793
Validation loss: 2.599745470990417

Epoch: 6| Step: 12
Training loss: 2.825089454650879
Validation loss: 2.5871912535800727

Epoch: 6| Step: 13
Training loss: 2.4949984550476074
Validation loss: 2.5849682541303736

Epoch: 125| Step: 0
Training loss: 2.1408371925354004
Validation loss: 2.596114689303983

Epoch: 6| Step: 1
Training loss: 3.3882548809051514
Validation loss: 2.606148160913939

Epoch: 6| Step: 2
Training loss: 3.7959847450256348
Validation loss: 2.6165935480466453

Epoch: 6| Step: 3
Training loss: 2.471125602722168
Validation loss: 2.6082851117657078

Epoch: 6| Step: 4
Training loss: 2.5610227584838867
Validation loss: 2.604680925287226

Epoch: 6| Step: 5
Training loss: 2.615990161895752
Validation loss: 2.6060259598557667

Epoch: 6| Step: 6
Training loss: 2.727173328399658
Validation loss: 2.605013229513681

Epoch: 6| Step: 7
Training loss: 2.477205276489258
Validation loss: 2.599823077519735

Epoch: 6| Step: 8
Training loss: 3.4572954177856445
Validation loss: 2.5936578678828415

Epoch: 6| Step: 9
Training loss: 2.4067790508270264
Validation loss: 2.5982059868433143

Epoch: 6| Step: 10
Training loss: 2.2424697875976562
Validation loss: 2.5950604946382585

Epoch: 6| Step: 11
Training loss: 2.549501895904541
Validation loss: 2.589709912576983

Epoch: 6| Step: 12
Training loss: 2.6530284881591797
Validation loss: 2.5874082990871963

Epoch: 6| Step: 13
Training loss: 3.1293694972991943
Validation loss: 2.58750807598073

Epoch: 126| Step: 0
Training loss: 3.0900938510894775
Validation loss: 2.5896557607958393

Epoch: 6| Step: 1
Training loss: 2.5330934524536133
Validation loss: 2.593895681442753

Epoch: 6| Step: 2
Training loss: 2.3300395011901855
Validation loss: 2.5916709669174685

Epoch: 6| Step: 3
Training loss: 2.1282501220703125
Validation loss: 2.591170592974591

Epoch: 6| Step: 4
Training loss: 2.359785318374634
Validation loss: 2.5963767292678996

Epoch: 6| Step: 5
Training loss: 2.676210403442383
Validation loss: 2.598760829176954

Epoch: 6| Step: 6
Training loss: 3.991687536239624
Validation loss: 2.602605512065272

Epoch: 6| Step: 7
Training loss: 3.1436572074890137
Validation loss: 2.5991087318748556

Epoch: 6| Step: 8
Training loss: 3.0707054138183594
Validation loss: 2.5960677926258375

Epoch: 6| Step: 9
Training loss: 2.4654479026794434
Validation loss: 2.5930902214460474

Epoch: 6| Step: 10
Training loss: 3.229708671569824
Validation loss: 2.5927688152559343

Epoch: 6| Step: 11
Training loss: 2.658768653869629
Validation loss: 2.596949715768137

Epoch: 6| Step: 12
Training loss: 2.1297836303710938
Validation loss: 2.590347654076033

Epoch: 6| Step: 13
Training loss: 2.2229416370391846
Validation loss: 2.5872580748732372

Epoch: 127| Step: 0
Training loss: 2.746802806854248
Validation loss: 2.5928775341280046

Epoch: 6| Step: 1
Training loss: 2.970180034637451
Validation loss: 2.5903491230421167

Epoch: 6| Step: 2
Training loss: 2.15621280670166
Validation loss: 2.5856588014992337

Epoch: 6| Step: 3
Training loss: 2.432816982269287
Validation loss: 2.5910439388726347

Epoch: 6| Step: 4
Training loss: 2.909705877304077
Validation loss: 2.5910290210477767

Epoch: 6| Step: 5
Training loss: 2.1184654235839844
Validation loss: 2.5905589442099295

Epoch: 6| Step: 6
Training loss: 2.2827205657958984
Validation loss: 2.5955979285701627

Epoch: 6| Step: 7
Training loss: 3.0648677349090576
Validation loss: 2.6057343098425094

Epoch: 6| Step: 8
Training loss: 3.184749126434326
Validation loss: 2.607566128494919

Epoch: 6| Step: 9
Training loss: 2.382599353790283
Validation loss: 2.615665912628174

Epoch: 6| Step: 10
Training loss: 2.952232837677002
Validation loss: 2.613891065761607

Epoch: 6| Step: 11
Training loss: 3.735710620880127
Validation loss: 2.622372386276081

Epoch: 6| Step: 12
Training loss: 2.60007643699646
Validation loss: 2.6263999221145466

Epoch: 6| Step: 13
Training loss: 2.8859622478485107
Validation loss: 2.6252212485959454

Epoch: 128| Step: 0
Training loss: 2.33642315864563
Validation loss: 2.6308013803215435

Epoch: 6| Step: 1
Training loss: 3.2600150108337402
Validation loss: 2.6243153079863517

Epoch: 6| Step: 2
Training loss: 3.0625481605529785
Validation loss: 2.615784811717208

Epoch: 6| Step: 3
Training loss: 3.111968517303467
Validation loss: 2.6106047220127557

Epoch: 6| Step: 4
Training loss: 1.9726061820983887
Validation loss: 2.5947041588444866

Epoch: 6| Step: 5
Training loss: 2.8150534629821777
Validation loss: 2.588463406408987

Epoch: 6| Step: 6
Training loss: 3.067312240600586
Validation loss: 2.590705697254468

Epoch: 6| Step: 7
Training loss: 2.7293894290924072
Validation loss: 2.5911199815811647

Epoch: 6| Step: 8
Training loss: 2.0640501976013184
Validation loss: 2.5918106981503066

Epoch: 6| Step: 9
Training loss: 1.976491928100586
Validation loss: 2.595028305566439

Epoch: 6| Step: 10
Training loss: 3.254408836364746
Validation loss: 2.5834559702104136

Epoch: 6| Step: 11
Training loss: 3.1594204902648926
Validation loss: 2.5832441660665695

Epoch: 6| Step: 12
Training loss: 2.799185037612915
Validation loss: 2.580389750901089

Epoch: 6| Step: 13
Training loss: 2.7257080078125
Validation loss: 2.5800595565508773

Epoch: 129| Step: 0
Training loss: 1.4665944576263428
Validation loss: 2.5837584490417154

Epoch: 6| Step: 1
Training loss: 3.3075337409973145
Validation loss: 2.589096943537394

Epoch: 6| Step: 2
Training loss: 3.042266368865967
Validation loss: 2.5851554562968593

Epoch: 6| Step: 3
Training loss: 1.8874359130859375
Validation loss: 2.5845125388073664

Epoch: 6| Step: 4
Training loss: 2.6106669902801514
Validation loss: 2.587421881255283

Epoch: 6| Step: 5
Training loss: 4.034423828125
Validation loss: 2.5884116413772746

Epoch: 6| Step: 6
Training loss: 2.340259552001953
Validation loss: 2.588229384473575

Epoch: 6| Step: 7
Training loss: 2.620664596557617
Validation loss: 2.5807831748839347

Epoch: 6| Step: 8
Training loss: 2.7860946655273438
Validation loss: 2.5806816188238

Epoch: 6| Step: 9
Training loss: 2.920154571533203
Validation loss: 2.581562357564126

Epoch: 6| Step: 10
Training loss: 2.914029598236084
Validation loss: 2.5747411609977804

Epoch: 6| Step: 11
Training loss: 2.6952033042907715
Validation loss: 2.5765384422835482

Epoch: 6| Step: 12
Training loss: 2.9201838970184326
Validation loss: 2.5752035956228934

Epoch: 6| Step: 13
Training loss: 2.5482418537139893
Validation loss: 2.5739333116880028

Epoch: 130| Step: 0
Training loss: 2.963536500930786
Validation loss: 2.5765748408532914

Epoch: 6| Step: 1
Training loss: 2.3358206748962402
Validation loss: 2.579050697306151

Epoch: 6| Step: 2
Training loss: 2.666456460952759
Validation loss: 2.5841024088603195

Epoch: 6| Step: 3
Training loss: 2.8373279571533203
Validation loss: 2.593708607458299

Epoch: 6| Step: 4
Training loss: 2.0896096229553223
Validation loss: 2.5901868112625612

Epoch: 6| Step: 5
Training loss: 3.7736668586730957
Validation loss: 2.580764450052733

Epoch: 6| Step: 6
Training loss: 2.8879594802856445
Validation loss: 2.584799128194009

Epoch: 6| Step: 7
Training loss: 3.1070871353149414
Validation loss: 2.5783931286104265

Epoch: 6| Step: 8
Training loss: 2.992720603942871
Validation loss: 2.583589082123131

Epoch: 6| Step: 9
Training loss: 2.3491852283477783
Validation loss: 2.5768004668656217

Epoch: 6| Step: 10
Training loss: 2.473568916320801
Validation loss: 2.577758696771437

Epoch: 6| Step: 11
Training loss: 2.073111057281494
Validation loss: 2.575120105538317

Epoch: 6| Step: 12
Training loss: 2.926584243774414
Validation loss: 2.5823938205677974

Epoch: 6| Step: 13
Training loss: 2.697380304336548
Validation loss: 2.583803710117135

Epoch: 131| Step: 0
Training loss: 3.399796485900879
Validation loss: 2.58318692381664

Epoch: 6| Step: 1
Training loss: 3.465050220489502
Validation loss: 2.5856981533829884

Epoch: 6| Step: 2
Training loss: 2.273489236831665
Validation loss: 2.5792418320973716

Epoch: 6| Step: 3
Training loss: 2.2207894325256348
Validation loss: 2.578369112424953

Epoch: 6| Step: 4
Training loss: 2.4879932403564453
Validation loss: 2.578891223476779

Epoch: 6| Step: 5
Training loss: 2.6428518295288086
Validation loss: 2.5749442910635345

Epoch: 6| Step: 6
Training loss: 2.4363291263580322
Validation loss: 2.576469518805063

Epoch: 6| Step: 7
Training loss: 2.49405574798584
Validation loss: 2.5754788434633644

Epoch: 6| Step: 8
Training loss: 2.5850648880004883
Validation loss: 2.5693326919309554

Epoch: 6| Step: 9
Training loss: 2.2888381481170654
Validation loss: 2.5723071329055296

Epoch: 6| Step: 10
Training loss: 2.4900569915771484
Validation loss: 2.5724853059296966

Epoch: 6| Step: 11
Training loss: 3.478050947189331
Validation loss: 2.5740417818869314

Epoch: 6| Step: 12
Training loss: 3.1922008991241455
Validation loss: 2.579085034708823

Epoch: 6| Step: 13
Training loss: 2.50429105758667
Validation loss: 2.5785307217669744

Epoch: 132| Step: 0
Training loss: 3.0674471855163574
Validation loss: 2.577567995235484

Epoch: 6| Step: 1
Training loss: 2.2468042373657227
Validation loss: 2.5745288941168014

Epoch: 6| Step: 2
Training loss: 2.8286070823669434
Validation loss: 2.5695403314405874

Epoch: 6| Step: 3
Training loss: 2.9209580421447754
Validation loss: 2.573291619618734

Epoch: 6| Step: 4
Training loss: 3.114710807800293
Validation loss: 2.5709865400868077

Epoch: 6| Step: 5
Training loss: 2.1654975414276123
Validation loss: 2.5799817346757457

Epoch: 6| Step: 6
Training loss: 3.1637511253356934
Validation loss: 2.5784884601510982

Epoch: 6| Step: 7
Training loss: 2.229567050933838
Validation loss: 2.5780930134557907

Epoch: 6| Step: 8
Training loss: 1.8456023931503296
Validation loss: 2.575637532818702

Epoch: 6| Step: 9
Training loss: 2.899139881134033
Validation loss: 2.574856609426519

Epoch: 6| Step: 10
Training loss: 2.5983331203460693
Validation loss: 2.5716767208550566

Epoch: 6| Step: 11
Training loss: 3.35184907913208
Validation loss: 2.571847733630929

Epoch: 6| Step: 12
Training loss: 2.9150338172912598
Validation loss: 2.5671028757608063

Epoch: 6| Step: 13
Training loss: 2.8211379051208496
Validation loss: 2.569562026249465

Epoch: 133| Step: 0
Training loss: 2.259230136871338
Validation loss: 2.565943964066044

Epoch: 6| Step: 1
Training loss: 3.1616227626800537
Validation loss: 2.5683248094333115

Epoch: 6| Step: 2
Training loss: 2.127075672149658
Validation loss: 2.563295543834727

Epoch: 6| Step: 3
Training loss: 3.1699371337890625
Validation loss: 2.5671633981889292

Epoch: 6| Step: 4
Training loss: 2.156834125518799
Validation loss: 2.566758076349894

Epoch: 6| Step: 5
Training loss: 2.8215765953063965
Validation loss: 2.568307271567724

Epoch: 6| Step: 6
Training loss: 2.356142520904541
Validation loss: 2.5694509090915805

Epoch: 6| Step: 7
Training loss: 2.8128130435943604
Validation loss: 2.574744619348998

Epoch: 6| Step: 8
Training loss: 2.7030274868011475
Validation loss: 2.569873804687172

Epoch: 6| Step: 9
Training loss: 3.814760684967041
Validation loss: 2.5701786600133425

Epoch: 6| Step: 10
Training loss: 2.4597373008728027
Validation loss: 2.5729326535296697

Epoch: 6| Step: 11
Training loss: 2.731006383895874
Validation loss: 2.58517316849001

Epoch: 6| Step: 12
Training loss: 2.7595252990722656
Validation loss: 2.5792656073006253

Epoch: 6| Step: 13
Training loss: 2.719557762145996
Validation loss: 2.587090669139739

Epoch: 134| Step: 0
Training loss: 1.9619183540344238
Validation loss: 2.5903656046877623

Epoch: 6| Step: 1
Training loss: 2.701780319213867
Validation loss: 2.588167882734729

Epoch: 6| Step: 2
Training loss: 2.626067876815796
Validation loss: 2.5766478353931057

Epoch: 6| Step: 3
Training loss: 2.203834056854248
Validation loss: 2.581823851472588

Epoch: 6| Step: 4
Training loss: 3.148371696472168
Validation loss: 2.5774375597635903

Epoch: 6| Step: 5
Training loss: 2.814065456390381
Validation loss: 2.575331370035807

Epoch: 6| Step: 6
Training loss: 3.1059978008270264
Validation loss: 2.5746935875185075

Epoch: 6| Step: 7
Training loss: 2.4833011627197266
Validation loss: 2.5621257981946393

Epoch: 6| Step: 8
Training loss: 2.666900157928467
Validation loss: 2.5641660228852303

Epoch: 6| Step: 9
Training loss: 3.097916603088379
Validation loss: 2.561713869853686

Epoch: 6| Step: 10
Training loss: 2.7710633277893066
Validation loss: 2.568486213684082

Epoch: 6| Step: 11
Training loss: 3.069047689437866
Validation loss: 2.578453222910563

Epoch: 6| Step: 12
Training loss: 2.63925838470459
Validation loss: 2.587477443038776

Epoch: 6| Step: 13
Training loss: 2.739100217819214
Validation loss: 2.5873597591154036

Epoch: 135| Step: 0
Training loss: 3.36340069770813
Validation loss: 2.5862610570846067

Epoch: 6| Step: 1
Training loss: 2.625244140625
Validation loss: 2.5831554038550264

Epoch: 6| Step: 2
Training loss: 2.3244166374206543
Validation loss: 2.5779226133900304

Epoch: 6| Step: 3
Training loss: 2.425835132598877
Validation loss: 2.5697993309267106

Epoch: 6| Step: 4
Training loss: 1.9048912525177002
Validation loss: 2.561485982710315

Epoch: 6| Step: 5
Training loss: 1.6082139015197754
Validation loss: 2.562617814669045

Epoch: 6| Step: 6
Training loss: 3.0117416381835938
Validation loss: 2.556522015602358

Epoch: 6| Step: 7
Training loss: 3.5568606853485107
Validation loss: 2.5623316893013577

Epoch: 6| Step: 8
Training loss: 2.984187126159668
Validation loss: 2.576625536846858

Epoch: 6| Step: 9
Training loss: 3.1301605701446533
Validation loss: 2.5806221423610562

Epoch: 6| Step: 10
Training loss: 2.873969793319702
Validation loss: 2.5917266363738687

Epoch: 6| Step: 11
Training loss: 2.5354456901550293
Validation loss: 2.5793790381441832

Epoch: 6| Step: 12
Training loss: 3.300189256668091
Validation loss: 2.5884531595373668

Epoch: 6| Step: 13
Training loss: 2.565237045288086
Validation loss: 2.586959474830217

Epoch: 136| Step: 0
Training loss: 2.155916690826416
Validation loss: 2.5801888178753596

Epoch: 6| Step: 1
Training loss: 2.758523941040039
Validation loss: 2.5671273380197506

Epoch: 6| Step: 2
Training loss: 2.498724937438965
Validation loss: 2.569627220912646

Epoch: 6| Step: 3
Training loss: 3.0489699840545654
Validation loss: 2.5663856716566187

Epoch: 6| Step: 4
Training loss: 2.62528657913208
Validation loss: 2.567366502618277

Epoch: 6| Step: 5
Training loss: 2.1490981578826904
Validation loss: 2.566173509884906

Epoch: 6| Step: 6
Training loss: 3.284214496612549
Validation loss: 2.5652550394817064

Epoch: 6| Step: 7
Training loss: 2.317321300506592
Validation loss: 2.563386468477147

Epoch: 6| Step: 8
Training loss: 2.769878625869751
Validation loss: 2.5566941474073674

Epoch: 6| Step: 9
Training loss: 3.390641689300537
Validation loss: 2.55713076232582

Epoch: 6| Step: 10
Training loss: 2.916212797164917
Validation loss: 2.5519736556596655

Epoch: 6| Step: 11
Training loss: 2.465120792388916
Validation loss: 2.5547345402420207

Epoch: 6| Step: 12
Training loss: 3.033036708831787
Validation loss: 2.550306717554728

Epoch: 6| Step: 13
Training loss: 2.4433250427246094
Validation loss: 2.5530911773763676

Epoch: 137| Step: 0
Training loss: 2.2092535495758057
Validation loss: 2.5578843880725164

Epoch: 6| Step: 1
Training loss: 3.649359703063965
Validation loss: 2.5569346156171573

Epoch: 6| Step: 2
Training loss: 3.207764148712158
Validation loss: 2.5538442647585304

Epoch: 6| Step: 3
Training loss: 2.9891839027404785
Validation loss: 2.5568897416514735

Epoch: 6| Step: 4
Training loss: 2.842832565307617
Validation loss: 2.549928275487756

Epoch: 6| Step: 5
Training loss: 2.8304049968719482
Validation loss: 2.55127852706499

Epoch: 6| Step: 6
Training loss: 2.9572129249572754
Validation loss: 2.5466818681327243

Epoch: 6| Step: 7
Training loss: 1.486626148223877
Validation loss: 2.5501729185863207

Epoch: 6| Step: 8
Training loss: 3.3853583335876465
Validation loss: 2.5511684750997894

Epoch: 6| Step: 9
Training loss: 3.1404829025268555
Validation loss: 2.5500521429123415

Epoch: 6| Step: 10
Training loss: 2.266280174255371
Validation loss: 2.54914032002931

Epoch: 6| Step: 11
Training loss: 2.277561902999878
Validation loss: 2.5476856872599614

Epoch: 6| Step: 12
Training loss: 2.34651517868042
Validation loss: 2.5523736707625853

Epoch: 6| Step: 13
Training loss: 2.3273096084594727
Validation loss: 2.555541633277811

Epoch: 138| Step: 0
Training loss: 2.7914891242980957
Validation loss: 2.5517938829237417

Epoch: 6| Step: 1
Training loss: 3.3076400756835938
Validation loss: 2.556429470739057

Epoch: 6| Step: 2
Training loss: 2.5023021697998047
Validation loss: 2.5522751295438377

Epoch: 6| Step: 3
Training loss: 2.9913668632507324
Validation loss: 2.551559825097361

Epoch: 6| Step: 4
Training loss: 3.328587532043457
Validation loss: 2.5548229935348674

Epoch: 6| Step: 5
Training loss: 2.555882215499878
Validation loss: 2.5594199780494935

Epoch: 6| Step: 6
Training loss: 2.9733994007110596
Validation loss: 2.5600172140265025

Epoch: 6| Step: 7
Training loss: 3.245131015777588
Validation loss: 2.5624050119871735

Epoch: 6| Step: 8
Training loss: 2.409956455230713
Validation loss: 2.5621837236547984

Epoch: 6| Step: 9
Training loss: 2.537710666656494
Validation loss: 2.5687704957941526

Epoch: 6| Step: 10
Training loss: 2.078202247619629
Validation loss: 2.569555139028898

Epoch: 6| Step: 11
Training loss: 1.9771002531051636
Validation loss: 2.5712526639302573

Epoch: 6| Step: 12
Training loss: 1.9592952728271484
Validation loss: 2.5723430725835983

Epoch: 6| Step: 13
Training loss: 3.7649316787719727
Validation loss: 2.572237519807713

Epoch: 139| Step: 0
Training loss: 2.9349074363708496
Validation loss: 2.5665452608498196

Epoch: 6| Step: 1
Training loss: 2.5988121032714844
Validation loss: 2.5709587450950377

Epoch: 6| Step: 2
Training loss: 2.4657340049743652
Validation loss: 2.564652273731847

Epoch: 6| Step: 3
Training loss: 2.752103567123413
Validation loss: 2.5646876212089293

Epoch: 6| Step: 4
Training loss: 2.865025043487549
Validation loss: 2.5674290759589082

Epoch: 6| Step: 5
Training loss: 2.7159171104431152
Validation loss: 2.566254069728236

Epoch: 6| Step: 6
Training loss: 2.274366855621338
Validation loss: 2.5680446932392735

Epoch: 6| Step: 7
Training loss: 3.1058318614959717
Validation loss: 2.563521026283182

Epoch: 6| Step: 8
Training loss: 2.7364914417266846
Validation loss: 2.564139509713778

Epoch: 6| Step: 9
Training loss: 2.842686176300049
Validation loss: 2.5665064447669574

Epoch: 6| Step: 10
Training loss: 2.327287197113037
Validation loss: 2.5572929100323747

Epoch: 6| Step: 11
Training loss: 3.5660200119018555
Validation loss: 2.5668193755611295

Epoch: 6| Step: 12
Training loss: 2.124018430709839
Validation loss: 2.560924647956766

Epoch: 6| Step: 13
Training loss: 2.5283896923065186
Validation loss: 2.5614870850757887

Epoch: 140| Step: 0
Training loss: 3.4350671768188477
Validation loss: 2.5580590053271224

Epoch: 6| Step: 1
Training loss: 2.731996536254883
Validation loss: 2.56057834112516

Epoch: 6| Step: 2
Training loss: 2.336392402648926
Validation loss: 2.5545735795010804

Epoch: 6| Step: 3
Training loss: 2.2724320888519287
Validation loss: 2.5646748132603143

Epoch: 6| Step: 4
Training loss: 2.589162588119507
Validation loss: 2.557337471233901

Epoch: 6| Step: 5
Training loss: 2.554457902908325
Validation loss: 2.5522977639270086

Epoch: 6| Step: 6
Training loss: 2.5330262184143066
Validation loss: 2.559822451683783

Epoch: 6| Step: 7
Training loss: 3.1012494564056396
Validation loss: 2.559702545083979

Epoch: 6| Step: 8
Training loss: 2.3260841369628906
Validation loss: 2.5556441148122153

Epoch: 6| Step: 9
Training loss: 2.67630672454834
Validation loss: 2.560259965158278

Epoch: 6| Step: 10
Training loss: 3.004753351211548
Validation loss: 2.5672300041362806

Epoch: 6| Step: 11
Training loss: 3.375594139099121
Validation loss: 2.5648575290556876

Epoch: 6| Step: 12
Training loss: 2.654291868209839
Validation loss: 2.5639902853196666

Epoch: 6| Step: 13
Training loss: 1.9746848344802856
Validation loss: 2.5624811162230787

Epoch: 141| Step: 0
Training loss: 2.8135457038879395
Validation loss: 2.558639867331392

Epoch: 6| Step: 1
Training loss: 3.2866578102111816
Validation loss: 2.5642905824927875

Epoch: 6| Step: 2
Training loss: 4.055852890014648
Validation loss: 2.5629893195244575

Epoch: 6| Step: 3
Training loss: 2.6400394439697266
Validation loss: 2.565320594336397

Epoch: 6| Step: 4
Training loss: 2.8422646522521973
Validation loss: 2.561316961883217

Epoch: 6| Step: 5
Training loss: 2.350571632385254
Validation loss: 2.5543940169836885

Epoch: 6| Step: 6
Training loss: 2.4511234760284424
Validation loss: 2.552882368846606

Epoch: 6| Step: 7
Training loss: 2.4704489707946777
Validation loss: 2.5535248530808317

Epoch: 6| Step: 8
Training loss: 1.7115058898925781
Validation loss: 2.549399524606684

Epoch: 6| Step: 9
Training loss: 2.3785486221313477
Validation loss: 2.549937068775136

Epoch: 6| Step: 10
Training loss: 2.7669806480407715
Validation loss: 2.5528029600779214

Epoch: 6| Step: 11
Training loss: 3.1163387298583984
Validation loss: 2.5532564886154665

Epoch: 6| Step: 12
Training loss: 2.701801300048828
Validation loss: 2.557741044670023

Epoch: 6| Step: 13
Training loss: 2.0301852226257324
Validation loss: 2.5575108194863923

Epoch: 142| Step: 0
Training loss: 2.861957550048828
Validation loss: 2.5600483725147862

Epoch: 6| Step: 1
Training loss: 3.034987449645996
Validation loss: 2.5623892891791558

Epoch: 6| Step: 2
Training loss: 2.951199531555176
Validation loss: 2.5746668051647883

Epoch: 6| Step: 3
Training loss: 3.002373695373535
Validation loss: 2.5717676275519916

Epoch: 6| Step: 4
Training loss: 2.8042922019958496
Validation loss: 2.570453756599016

Epoch: 6| Step: 5
Training loss: 2.669832468032837
Validation loss: 2.5656493069023214

Epoch: 6| Step: 6
Training loss: 2.5389721393585205
Validation loss: 2.561933858420259

Epoch: 6| Step: 7
Training loss: 2.8994827270507812
Validation loss: 2.5563746472840667

Epoch: 6| Step: 8
Training loss: 2.1725869178771973
Validation loss: 2.556766302354874

Epoch: 6| Step: 9
Training loss: 2.7541050910949707
Validation loss: 2.561512179272149

Epoch: 6| Step: 10
Training loss: 2.3301210403442383
Validation loss: 2.575522061317198

Epoch: 6| Step: 11
Training loss: 2.660327434539795
Validation loss: 2.579819312659643

Epoch: 6| Step: 12
Training loss: 2.4994547367095947
Validation loss: 2.576168626867315

Epoch: 6| Step: 13
Training loss: 2.847137928009033
Validation loss: 2.5803854004029305

Epoch: 143| Step: 0
Training loss: 3.161001443862915
Validation loss: 2.566302425117903

Epoch: 6| Step: 1
Training loss: 1.7295751571655273
Validation loss: 2.559267141485727

Epoch: 6| Step: 2
Training loss: 2.666322946548462
Validation loss: 2.5534014599297636

Epoch: 6| Step: 3
Training loss: 2.6297645568847656
Validation loss: 2.5428813042179232

Epoch: 6| Step: 4
Training loss: 2.8735127449035645
Validation loss: 2.5470722106195267

Epoch: 6| Step: 5
Training loss: 2.796365737915039
Validation loss: 2.545775323785761

Epoch: 6| Step: 6
Training loss: 2.499915361404419
Validation loss: 2.5425165750647105

Epoch: 6| Step: 7
Training loss: 2.6376023292541504
Validation loss: 2.543838447140109

Epoch: 6| Step: 8
Training loss: 2.1688661575317383
Validation loss: 2.5544881282314176

Epoch: 6| Step: 9
Training loss: 2.947145938873291
Validation loss: 2.558912836095338

Epoch: 6| Step: 10
Training loss: 3.406789779663086
Validation loss: 2.5585067426004717

Epoch: 6| Step: 11
Training loss: 2.9232735633850098
Validation loss: 2.555274148141184

Epoch: 6| Step: 12
Training loss: 3.0374364852905273
Validation loss: 2.55831003445451

Epoch: 6| Step: 13
Training loss: 2.212728500366211
Validation loss: 2.550012224464006

Epoch: 144| Step: 0
Training loss: 2.6954588890075684
Validation loss: 2.547137568073888

Epoch: 6| Step: 1
Training loss: 1.9960578680038452
Validation loss: 2.5412968076685423

Epoch: 6| Step: 2
Training loss: 2.2219250202178955
Validation loss: 2.5404196323886996

Epoch: 6| Step: 3
Training loss: 2.737548589706421
Validation loss: 2.5424177774818997

Epoch: 6| Step: 4
Training loss: 3.339419364929199
Validation loss: 2.5414253204099593

Epoch: 6| Step: 5
Training loss: 3.4398159980773926
Validation loss: 2.541031329862533

Epoch: 6| Step: 6
Training loss: 2.229823112487793
Validation loss: 2.5443717587378716

Epoch: 6| Step: 7
Training loss: 2.436715602874756
Validation loss: 2.546797360143354

Epoch: 6| Step: 8
Training loss: 2.666477918624878
Validation loss: 2.5458335363736717

Epoch: 6| Step: 9
Training loss: 2.2461936473846436
Validation loss: 2.5490729449897684

Epoch: 6| Step: 10
Training loss: 2.592437744140625
Validation loss: 2.55131592289094

Epoch: 6| Step: 11
Training loss: 2.9389002323150635
Validation loss: 2.549493989636821

Epoch: 6| Step: 12
Training loss: 3.257249355316162
Validation loss: 2.5494665279183337

Epoch: 6| Step: 13
Training loss: 3.3477697372436523
Validation loss: 2.5458305061504407

Epoch: 145| Step: 0
Training loss: 2.4692697525024414
Validation loss: 2.546343177877447

Epoch: 6| Step: 1
Training loss: 2.7850983142852783
Validation loss: 2.5475536495126705

Epoch: 6| Step: 2
Training loss: 1.97226881980896
Validation loss: 2.543737746054126

Epoch: 6| Step: 3
Training loss: 2.2417609691619873
Validation loss: 2.546941339328725

Epoch: 6| Step: 4
Training loss: 2.7637014389038086
Validation loss: 2.5418109765616794

Epoch: 6| Step: 5
Training loss: 3.5596399307250977
Validation loss: 2.5404250698704876

Epoch: 6| Step: 6
Training loss: 3.0669808387756348
Validation loss: 2.544834083126437

Epoch: 6| Step: 7
Training loss: 3.8367371559143066
Validation loss: 2.5506325690977034

Epoch: 6| Step: 8
Training loss: 2.0744946002960205
Validation loss: 2.5452240590126283

Epoch: 6| Step: 9
Training loss: 2.8805952072143555
Validation loss: 2.547682757018715

Epoch: 6| Step: 10
Training loss: 2.0357823371887207
Validation loss: 2.5441682530987646

Epoch: 6| Step: 11
Training loss: 2.0523386001586914
Validation loss: 2.5423881725598405

Epoch: 6| Step: 12
Training loss: 3.2421724796295166
Validation loss: 2.5390533247301654

Epoch: 6| Step: 13
Training loss: 3.1520841121673584
Validation loss: 2.538400537224226

Epoch: 146| Step: 0
Training loss: 2.442558526992798
Validation loss: 2.540442551335981

Epoch: 6| Step: 1
Training loss: 2.1349244117736816
Validation loss: 2.5363588743312384

Epoch: 6| Step: 2
Training loss: 3.3592405319213867
Validation loss: 2.5380862169368292

Epoch: 6| Step: 3
Training loss: 2.2389965057373047
Validation loss: 2.5419801153162473

Epoch: 6| Step: 4
Training loss: 2.436692476272583
Validation loss: 2.545034567515055

Epoch: 6| Step: 5
Training loss: 3.155512809753418
Validation loss: 2.5524311245128675

Epoch: 6| Step: 6
Training loss: 2.78593373298645
Validation loss: 2.5477117646125054

Epoch: 6| Step: 7
Training loss: 2.8403372764587402
Validation loss: 2.551230594676028

Epoch: 6| Step: 8
Training loss: 2.5474014282226562
Validation loss: 2.5458484900894987

Epoch: 6| Step: 9
Training loss: 3.15328049659729
Validation loss: 2.539201421122397

Epoch: 6| Step: 10
Training loss: 3.0213701725006104
Validation loss: 2.5412887937279156

Epoch: 6| Step: 11
Training loss: 2.696903705596924
Validation loss: 2.5484316041392665

Epoch: 6| Step: 12
Training loss: 2.2021796703338623
Validation loss: 2.5443787805495726

Epoch: 6| Step: 13
Training loss: 2.8876240253448486
Validation loss: 2.549629347298735

Epoch: 147| Step: 0
Training loss: 3.0588645935058594
Validation loss: 2.5466848137558147

Epoch: 6| Step: 1
Training loss: 1.9313164949417114
Validation loss: 2.5437185379766647

Epoch: 6| Step: 2
Training loss: 2.4526283740997314
Validation loss: 2.53603994846344

Epoch: 6| Step: 3
Training loss: 2.5744664669036865
Validation loss: 2.5394405241935485

Epoch: 6| Step: 4
Training loss: 2.6757102012634277
Validation loss: 2.538195002463556

Epoch: 6| Step: 5
Training loss: 3.2764158248901367
Validation loss: 2.5416778569580405

Epoch: 6| Step: 6
Training loss: 2.4758291244506836
Validation loss: 2.5460427422677316

Epoch: 6| Step: 7
Training loss: 2.97076416015625
Validation loss: 2.5512177892910537

Epoch: 6| Step: 8
Training loss: 2.770162343978882
Validation loss: 2.545617126649426

Epoch: 6| Step: 9
Training loss: 2.2037415504455566
Validation loss: 2.542004972375849

Epoch: 6| Step: 10
Training loss: 3.0204708576202393
Validation loss: 2.5401779118404595

Epoch: 6| Step: 11
Training loss: 3.449493408203125
Validation loss: 2.529332117367816

Epoch: 6| Step: 12
Training loss: 2.519038438796997
Validation loss: 2.5322497352477042

Epoch: 6| Step: 13
Training loss: 2.20119047164917
Validation loss: 2.529190383931642

Epoch: 148| Step: 0
Training loss: 2.6830737590789795
Validation loss: 2.540859563376314

Epoch: 6| Step: 1
Training loss: 2.8433406352996826
Validation loss: 2.545158037575342

Epoch: 6| Step: 2
Training loss: 2.4047493934631348
Validation loss: 2.5489493980202624

Epoch: 6| Step: 3
Training loss: 2.2880642414093018
Validation loss: 2.5522448196206042

Epoch: 6| Step: 4
Training loss: 3.334399700164795
Validation loss: 2.5491830866823912

Epoch: 6| Step: 5
Training loss: 3.7997663021087646
Validation loss: 2.547549888651858

Epoch: 6| Step: 6
Training loss: 2.463655948638916
Validation loss: 2.545681584265924

Epoch: 6| Step: 7
Training loss: 3.029620885848999
Validation loss: 2.541182582096387

Epoch: 6| Step: 8
Training loss: 2.9021806716918945
Validation loss: 2.538013573615782

Epoch: 6| Step: 9
Training loss: 1.755226731300354
Validation loss: 2.536749924382856

Epoch: 6| Step: 10
Training loss: 3.0842766761779785
Validation loss: 2.533720777880761

Epoch: 6| Step: 11
Training loss: 2.7959108352661133
Validation loss: 2.5275659061247304

Epoch: 6| Step: 12
Training loss: 2.4719924926757812
Validation loss: 2.5299990279700166

Epoch: 6| Step: 13
Training loss: 1.5361027717590332
Validation loss: 2.533048819470149

Epoch: 149| Step: 0
Training loss: 3.3064992427825928
Validation loss: 2.5395243116604385

Epoch: 6| Step: 1
Training loss: 2.1626811027526855
Validation loss: 2.5459047440559632

Epoch: 6| Step: 2
Training loss: 3.0497608184814453
Validation loss: 2.5489931080930974

Epoch: 6| Step: 3
Training loss: 2.673351287841797
Validation loss: 2.553944772289645

Epoch: 6| Step: 4
Training loss: 2.859501838684082
Validation loss: 2.553242178373439

Epoch: 6| Step: 5
Training loss: 3.1506214141845703
Validation loss: 2.5458023368671374

Epoch: 6| Step: 6
Training loss: 2.866894245147705
Validation loss: 2.54021050853114

Epoch: 6| Step: 7
Training loss: 2.9789416790008545
Validation loss: 2.5407996562219437

Epoch: 6| Step: 8
Training loss: 2.470693588256836
Validation loss: 2.5315378994070072

Epoch: 6| Step: 9
Training loss: 2.532313823699951
Validation loss: 2.5351163469335085

Epoch: 6| Step: 10
Training loss: 2.9925599098205566
Validation loss: 2.530023628665555

Epoch: 6| Step: 11
Training loss: 2.1907715797424316
Validation loss: 2.530541560983145

Epoch: 6| Step: 12
Training loss: 2.2940516471862793
Validation loss: 2.5276863241708405

Epoch: 6| Step: 13
Training loss: 2.091946601867676
Validation loss: 2.5276212461533083

Epoch: 150| Step: 0
Training loss: 3.2780637741088867
Validation loss: 2.532743469361336

Epoch: 6| Step: 1
Training loss: 2.779655694961548
Validation loss: 2.534221551751578

Epoch: 6| Step: 2
Training loss: 2.232532501220703
Validation loss: 2.5324364939043598

Epoch: 6| Step: 3
Training loss: 3.0578083992004395
Validation loss: 2.5357728363365255

Epoch: 6| Step: 4
Training loss: 2.669118642807007
Validation loss: 2.535773179864371

Epoch: 6| Step: 5
Training loss: 1.6199884414672852
Validation loss: 2.537405747239308

Epoch: 6| Step: 6
Training loss: 2.254607677459717
Validation loss: 2.535579886487735

Epoch: 6| Step: 7
Training loss: 3.0058116912841797
Validation loss: 2.538434895136023

Epoch: 6| Step: 8
Training loss: 2.7539212703704834
Validation loss: 2.5329525880916144

Epoch: 6| Step: 9
Training loss: 3.2299439907073975
Validation loss: 2.5381119353796846

Epoch: 6| Step: 10
Training loss: 2.4131720066070557
Validation loss: 2.5384950483998945

Epoch: 6| Step: 11
Training loss: 2.919105052947998
Validation loss: 2.536533424931188

Epoch: 6| Step: 12
Training loss: 2.736629009246826
Validation loss: 2.5360186946007515

Epoch: 6| Step: 13
Training loss: 2.8239259719848633
Validation loss: 2.5381477981485348

Epoch: 151| Step: 0
Training loss: 3.8002941608428955
Validation loss: 2.540206209305794

Epoch: 6| Step: 1
Training loss: 3.247504234313965
Validation loss: 2.538911268275271

Epoch: 6| Step: 2
Training loss: 2.17932391166687
Validation loss: 2.5407825926298737

Epoch: 6| Step: 3
Training loss: 2.9883124828338623
Validation loss: 2.543861214832593

Epoch: 6| Step: 4
Training loss: 2.2985177040100098
Validation loss: 2.5421161549065703

Epoch: 6| Step: 5
Training loss: 2.9051074981689453
Validation loss: 2.536013598083168

Epoch: 6| Step: 6
Training loss: 1.9366830587387085
Validation loss: 2.5373773677374727

Epoch: 6| Step: 7
Training loss: 2.381352424621582
Validation loss: 2.53683042013517

Epoch: 6| Step: 8
Training loss: 2.873516082763672
Validation loss: 2.542538453173894

Epoch: 6| Step: 9
Training loss: 1.5063657760620117
Validation loss: 2.5392190000062347

Epoch: 6| Step: 10
Training loss: 3.0069611072540283
Validation loss: 2.536119446959547

Epoch: 6| Step: 11
Training loss: 3.631349563598633
Validation loss: 2.540289176407681

Epoch: 6| Step: 12
Training loss: 1.9492379426956177
Validation loss: 2.5338270023304927

Epoch: 6| Step: 13
Training loss: 3.1464693546295166
Validation loss: 2.539823794877657

Epoch: 152| Step: 0
Training loss: 3.135928153991699
Validation loss: 2.5356137649987334

Epoch: 6| Step: 1
Training loss: 2.9850099086761475
Validation loss: 2.538485211710776

Epoch: 6| Step: 2
Training loss: 2.6250052452087402
Validation loss: 2.536561776232976

Epoch: 6| Step: 3
Training loss: 2.5550174713134766
Validation loss: 2.5378726836173766

Epoch: 6| Step: 4
Training loss: 2.3207404613494873
Validation loss: 2.536299390177573

Epoch: 6| Step: 5
Training loss: 2.9557719230651855
Validation loss: 2.5427706318516887

Epoch: 6| Step: 6
Training loss: 3.52610182762146
Validation loss: 2.5384852860563543

Epoch: 6| Step: 7
Training loss: 2.4000492095947266
Validation loss: 2.5482699717244794

Epoch: 6| Step: 8
Training loss: 2.484768867492676
Validation loss: 2.554794652487642

Epoch: 6| Step: 9
Training loss: 2.762009620666504
Validation loss: 2.5523600962854203

Epoch: 6| Step: 10
Training loss: 3.284745216369629
Validation loss: 2.5555538413345174

Epoch: 6| Step: 11
Training loss: 1.700544834136963
Validation loss: 2.5505002160226145

Epoch: 6| Step: 12
Training loss: 2.547025203704834
Validation loss: 2.5387556552886963

Epoch: 6| Step: 13
Training loss: 2.016709566116333
Validation loss: 2.535904542092354

Epoch: 153| Step: 0
Training loss: 2.672452926635742
Validation loss: 2.5316314748538438

Epoch: 6| Step: 1
Training loss: 3.0399816036224365
Validation loss: 2.5299994381525184

Epoch: 6| Step: 2
Training loss: 3.2006514072418213
Validation loss: 2.5350485360750588

Epoch: 6| Step: 3
Training loss: 2.337996244430542
Validation loss: 2.5285235989478325

Epoch: 6| Step: 4
Training loss: 2.4975790977478027
Validation loss: 2.529209795818534

Epoch: 6| Step: 5
Training loss: 2.035257339477539
Validation loss: 2.529350329470891

Epoch: 6| Step: 6
Training loss: 3.3247556686401367
Validation loss: 2.53339841288905

Epoch: 6| Step: 7
Training loss: 2.536125659942627
Validation loss: 2.5264894705946728

Epoch: 6| Step: 8
Training loss: 2.8864951133728027
Validation loss: 2.523385211985598

Epoch: 6| Step: 9
Training loss: 2.8822035789489746
Validation loss: 2.523856511680029

Epoch: 6| Step: 10
Training loss: 3.100980758666992
Validation loss: 2.52361540127826

Epoch: 6| Step: 11
Training loss: 2.2104904651641846
Validation loss: 2.522139521055324

Epoch: 6| Step: 12
Training loss: 2.427945137023926
Validation loss: 2.524730428572624

Epoch: 6| Step: 13
Training loss: 2.4282913208007812
Validation loss: 2.528700831115887

Epoch: 154| Step: 0
Training loss: 2.029726505279541
Validation loss: 2.5419594216090378

Epoch: 6| Step: 1
Training loss: 3.3379650115966797
Validation loss: 2.5395489918288363

Epoch: 6| Step: 2
Training loss: 2.59458065032959
Validation loss: 2.5420753930204656

Epoch: 6| Step: 3
Training loss: 3.072009563446045
Validation loss: 2.555306129558112

Epoch: 6| Step: 4
Training loss: 2.844174861907959
Validation loss: 2.562719160510648

Epoch: 6| Step: 5
Training loss: 2.4475338459014893
Validation loss: 2.5606514587197253

Epoch: 6| Step: 6
Training loss: 2.480347156524658
Validation loss: 2.561352816961145

Epoch: 6| Step: 7
Training loss: 3.8226003646850586
Validation loss: 2.5569647512128277

Epoch: 6| Step: 8
Training loss: 2.6352710723876953
Validation loss: 2.543196142360728

Epoch: 6| Step: 9
Training loss: 2.588446855545044
Validation loss: 2.533364419014223

Epoch: 6| Step: 10
Training loss: 2.4878547191619873
Validation loss: 2.5283479190641835

Epoch: 6| Step: 11
Training loss: 2.572636604309082
Validation loss: 2.5335926676309235

Epoch: 6| Step: 12
Training loss: 2.8342387676239014
Validation loss: 2.538501290864842

Epoch: 6| Step: 13
Training loss: 1.4655824899673462
Validation loss: 2.5373227929556244

Epoch: 155| Step: 0
Training loss: 2.6922693252563477
Validation loss: 2.533952882212977

Epoch: 6| Step: 1
Training loss: 2.4199936389923096
Validation loss: 2.5267218466727965

Epoch: 6| Step: 2
Training loss: 2.664719581604004
Validation loss: 2.524316082718552

Epoch: 6| Step: 3
Training loss: 3.1935184001922607
Validation loss: 2.5208130626268286

Epoch: 6| Step: 4
Training loss: 2.9969394207000732
Validation loss: 2.5205955428461873

Epoch: 6| Step: 5
Training loss: 2.328749179840088
Validation loss: 2.521657141306067

Epoch: 6| Step: 6
Training loss: 2.0744004249572754
Validation loss: 2.5274650973658406

Epoch: 6| Step: 7
Training loss: 2.6952173709869385
Validation loss: 2.5215173229094474

Epoch: 6| Step: 8
Training loss: 2.496236562728882
Validation loss: 2.524422440477597

Epoch: 6| Step: 9
Training loss: 2.777230739593506
Validation loss: 2.5240990987388034

Epoch: 6| Step: 10
Training loss: 3.382272243499756
Validation loss: 2.5268060853404384

Epoch: 6| Step: 11
Training loss: 3.2307491302490234
Validation loss: 2.5235049442578386

Epoch: 6| Step: 12
Training loss: 1.9993187189102173
Validation loss: 2.5237105918186966

Epoch: 6| Step: 13
Training loss: 2.7604289054870605
Validation loss: 2.5187241403005456

Epoch: 156| Step: 0
Training loss: 2.5114970207214355
Validation loss: 2.521459238503569

Epoch: 6| Step: 1
Training loss: 2.9458296298980713
Validation loss: 2.523069374022945

Epoch: 6| Step: 2
Training loss: 2.9555351734161377
Validation loss: 2.5248961858851935

Epoch: 6| Step: 3
Training loss: 2.5266261100769043
Validation loss: 2.523911735062958

Epoch: 6| Step: 4
Training loss: 3.117764472961426
Validation loss: 2.529869197517313

Epoch: 6| Step: 5
Training loss: 2.0447566509246826
Validation loss: 2.5398886383220716

Epoch: 6| Step: 6
Training loss: 2.321290969848633
Validation loss: 2.5353680759347896

Epoch: 6| Step: 7
Training loss: 3.199000358581543
Validation loss: 2.5449940671202955

Epoch: 6| Step: 8
Training loss: 2.583927869796753
Validation loss: 2.5465540450106383

Epoch: 6| Step: 9
Training loss: 2.27304744720459
Validation loss: 2.5473207594245992

Epoch: 6| Step: 10
Training loss: 2.1321825981140137
Validation loss: 2.5420234690430346

Epoch: 6| Step: 11
Training loss: 2.9410009384155273
Validation loss: 2.5439598380878405

Epoch: 6| Step: 12
Training loss: 3.19374942779541
Validation loss: 2.5495561835586384

Epoch: 6| Step: 13
Training loss: 3.046004295349121
Validation loss: 2.5481016712803997

Epoch: 157| Step: 0
Training loss: 3.007071018218994
Validation loss: 2.5423092457555954

Epoch: 6| Step: 1
Training loss: 2.9377617835998535
Validation loss: 2.5349181057304464

Epoch: 6| Step: 2
Training loss: 3.2011077404022217
Validation loss: 2.532629736008183

Epoch: 6| Step: 3
Training loss: 2.7872021198272705
Validation loss: 2.522404114405314

Epoch: 6| Step: 4
Training loss: 2.4886226654052734
Validation loss: 2.52587733217465

Epoch: 6| Step: 5
Training loss: 2.8177456855773926
Validation loss: 2.522394685335057

Epoch: 6| Step: 6
Training loss: 2.3012917041778564
Validation loss: 2.529168769877444

Epoch: 6| Step: 7
Training loss: 2.719409942626953
Validation loss: 2.5186109081391366

Epoch: 6| Step: 8
Training loss: 2.5612969398498535
Validation loss: 2.5119764266475553

Epoch: 6| Step: 9
Training loss: 2.5517804622650146
Validation loss: 2.5115060165364254

Epoch: 6| Step: 10
Training loss: 2.099123954772949
Validation loss: 2.5133439417808288

Epoch: 6| Step: 11
Training loss: 3.3001885414123535
Validation loss: 2.5168921819297214

Epoch: 6| Step: 12
Training loss: 2.043083667755127
Validation loss: 2.5150271692583637

Epoch: 6| Step: 13
Training loss: 3.125030517578125
Validation loss: 2.511966179775935

Epoch: 158| Step: 0
Training loss: 2.3329246044158936
Validation loss: 2.5170440955828597

Epoch: 6| Step: 1
Training loss: 2.464038372039795
Validation loss: 2.514404571184548

Epoch: 6| Step: 2
Training loss: 2.3661913871765137
Validation loss: 2.513182965658044

Epoch: 6| Step: 3
Training loss: 3.0007734298706055
Validation loss: 2.5176738257049234

Epoch: 6| Step: 4
Training loss: 2.3821675777435303
Validation loss: 2.5221537146517026

Epoch: 6| Step: 5
Training loss: 3.470583438873291
Validation loss: 2.517840426455262

Epoch: 6| Step: 6
Training loss: 2.4201202392578125
Validation loss: 2.52111901006391

Epoch: 6| Step: 7
Training loss: 1.9860279560089111
Validation loss: 2.521727003077025

Epoch: 6| Step: 8
Training loss: 2.6732606887817383
Validation loss: 2.520375026169644

Epoch: 6| Step: 9
Training loss: 2.2147598266601562
Validation loss: 2.5206629922313075

Epoch: 6| Step: 10
Training loss: 1.9621214866638184
Validation loss: 2.517265007060061

Epoch: 6| Step: 11
Training loss: 3.686873197555542
Validation loss: 2.5175027744744414

Epoch: 6| Step: 12
Training loss: 3.607055187225342
Validation loss: 2.5181259929492907

Epoch: 6| Step: 13
Training loss: 3.3458666801452637
Validation loss: 2.518271194991245

Epoch: 159| Step: 0
Training loss: 2.499115467071533
Validation loss: 2.5229451143613426

Epoch: 6| Step: 1
Training loss: 3.827195167541504
Validation loss: 2.520237056157922

Epoch: 6| Step: 2
Training loss: 2.4295575618743896
Validation loss: 2.5192709148571057

Epoch: 6| Step: 3
Training loss: 2.8972461223602295
Validation loss: 2.5138987443780385

Epoch: 6| Step: 4
Training loss: 2.7195451259613037
Validation loss: 2.5156013632333405

Epoch: 6| Step: 5
Training loss: 3.1620049476623535
Validation loss: 2.5133398373921714

Epoch: 6| Step: 6
Training loss: 3.1135432720184326
Validation loss: 2.5140036382982807

Epoch: 6| Step: 7
Training loss: 2.3265514373779297
Validation loss: 2.5164998218577397

Epoch: 6| Step: 8
Training loss: 2.0191335678100586
Validation loss: 2.5150047707301315

Epoch: 6| Step: 9
Training loss: 2.0274016857147217
Validation loss: 2.513096373568299

Epoch: 6| Step: 10
Training loss: 3.570157051086426
Validation loss: 2.5109930833180747

Epoch: 6| Step: 11
Training loss: 2.5486254692077637
Validation loss: 2.510159118201143

Epoch: 6| Step: 12
Training loss: 2.2245984077453613
Validation loss: 2.51024321586855

Epoch: 6| Step: 13
Training loss: 2.0388598442077637
Validation loss: 2.508685963128203

Epoch: 160| Step: 0
Training loss: 3.262807846069336
Validation loss: 2.512246924061929

Epoch: 6| Step: 1
Training loss: 2.7156901359558105
Validation loss: 2.5071467789270545

Epoch: 6| Step: 2
Training loss: 2.1842870712280273
Validation loss: 2.5074250851908038

Epoch: 6| Step: 3
Training loss: 3.1744298934936523
Validation loss: 2.507616937801402

Epoch: 6| Step: 4
Training loss: 2.53511643409729
Validation loss: 2.5120729489993026

Epoch: 6| Step: 5
Training loss: 2.8148138523101807
Validation loss: 2.512965556113951

Epoch: 6| Step: 6
Training loss: 2.253788471221924
Validation loss: 2.5142501836181967

Epoch: 6| Step: 7
Training loss: 2.3009791374206543
Validation loss: 2.5181108443967757

Epoch: 6| Step: 8
Training loss: 2.304356575012207
Validation loss: 2.526019670630014

Epoch: 6| Step: 9
Training loss: 2.4165403842926025
Validation loss: 2.5349196054602183

Epoch: 6| Step: 10
Training loss: 2.748781442642212
Validation loss: 2.543831815001785

Epoch: 6| Step: 11
Training loss: 3.7751152515411377
Validation loss: 2.5547481531737954

Epoch: 6| Step: 12
Training loss: 3.004971981048584
Validation loss: 2.5584783861714024

Epoch: 6| Step: 13
Training loss: 1.6948403120040894
Validation loss: 2.5581680754179597

Epoch: 161| Step: 0
Training loss: 2.053074836730957
Validation loss: 2.554577606980519

Epoch: 6| Step: 1
Training loss: 2.6432294845581055
Validation loss: 2.53943286659897

Epoch: 6| Step: 2
Training loss: 2.763401508331299
Validation loss: 2.5369239520001154

Epoch: 6| Step: 3
Training loss: 3.3280465602874756
Validation loss: 2.5320736310815297

Epoch: 6| Step: 4
Training loss: 3.1216440200805664
Validation loss: 2.525533535147226

Epoch: 6| Step: 5
Training loss: 1.9503047466278076
Validation loss: 2.5219801779716247

Epoch: 6| Step: 6
Training loss: 3.1646833419799805
Validation loss: 2.518506460292365

Epoch: 6| Step: 7
Training loss: 2.7363271713256836
Validation loss: 2.5182903658959175

Epoch: 6| Step: 8
Training loss: 2.4805312156677246
Validation loss: 2.5184642473856607

Epoch: 6| Step: 9
Training loss: 2.9690725803375244
Validation loss: 2.5138691189468547

Epoch: 6| Step: 10
Training loss: 3.143803119659424
Validation loss: 2.513175238845169

Epoch: 6| Step: 11
Training loss: 1.429136037826538
Validation loss: 2.5119248179979223

Epoch: 6| Step: 12
Training loss: 2.870729446411133
Validation loss: 2.518536321578487

Epoch: 6| Step: 13
Training loss: 3.0379714965820312
Validation loss: 2.51721727976235

Epoch: 162| Step: 0
Training loss: 2.781534194946289
Validation loss: 2.51977099398131

Epoch: 6| Step: 1
Training loss: 2.1369762420654297
Validation loss: 2.517125565518615

Epoch: 6| Step: 2
Training loss: 3.6229074001312256
Validation loss: 2.5202197028744604

Epoch: 6| Step: 3
Training loss: 2.6488828659057617
Validation loss: 2.515401453100225

Epoch: 6| Step: 4
Training loss: 2.7693121433258057
Validation loss: 2.520398001517019

Epoch: 6| Step: 5
Training loss: 2.3609657287597656
Validation loss: 2.5216446820125786

Epoch: 6| Step: 6
Training loss: 2.0068295001983643
Validation loss: 2.5219600200653076

Epoch: 6| Step: 7
Training loss: 3.222342014312744
Validation loss: 2.522531517090336

Epoch: 6| Step: 8
Training loss: 2.880514144897461
Validation loss: 2.527725019762593

Epoch: 6| Step: 9
Training loss: 2.6525750160217285
Validation loss: 2.533118191585746

Epoch: 6| Step: 10
Training loss: 2.853321075439453
Validation loss: 2.5289163897114415

Epoch: 6| Step: 11
Training loss: 2.8515982627868652
Validation loss: 2.531961541022024

Epoch: 6| Step: 12
Training loss: 2.7724528312683105
Validation loss: 2.530167820633099

Epoch: 6| Step: 13
Training loss: 1.5421726703643799
Validation loss: 2.525178829828898

Epoch: 163| Step: 0
Training loss: 2.432835817337036
Validation loss: 2.524662107549688

Epoch: 6| Step: 1
Training loss: 2.617955446243286
Validation loss: 2.5168656251763784

Epoch: 6| Step: 2
Training loss: 2.1378703117370605
Validation loss: 2.5175714108251754

Epoch: 6| Step: 3
Training loss: 3.3966832160949707
Validation loss: 2.5151452813097226

Epoch: 6| Step: 4
Training loss: 1.848350167274475
Validation loss: 2.514262053274339

Epoch: 6| Step: 5
Training loss: 3.2076873779296875
Validation loss: 2.5076112183191444

Epoch: 6| Step: 6
Training loss: 2.7501745223999023
Validation loss: 2.513428831613192

Epoch: 6| Step: 7
Training loss: 2.433614730834961
Validation loss: 2.5105177381987214

Epoch: 6| Step: 8
Training loss: 2.375002384185791
Validation loss: 2.5125662690849713

Epoch: 6| Step: 9
Training loss: 2.4165902137756348
Validation loss: 2.5095791970529864

Epoch: 6| Step: 10
Training loss: 3.3585824966430664
Validation loss: 2.511102117517943

Epoch: 6| Step: 11
Training loss: 2.975372314453125
Validation loss: 2.5113660417577273

Epoch: 6| Step: 12
Training loss: 2.595533847808838
Validation loss: 2.506979037356633

Epoch: 6| Step: 13
Training loss: 3.124838352203369
Validation loss: 2.5101510170967347

Epoch: 164| Step: 0
Training loss: 2.351163387298584
Validation loss: 2.514373181968607

Epoch: 6| Step: 1
Training loss: 3.0392417907714844
Validation loss: 2.5121480546971804

Epoch: 6| Step: 2
Training loss: 2.8693928718566895
Validation loss: 2.513038453235421

Epoch: 6| Step: 3
Training loss: 1.7457777261734009
Validation loss: 2.516361936446159

Epoch: 6| Step: 4
Training loss: 3.094679117202759
Validation loss: 2.519679392537763

Epoch: 6| Step: 5
Training loss: 2.9392740726470947
Validation loss: 2.51711856677968

Epoch: 6| Step: 6
Training loss: 2.5212864875793457
Validation loss: 2.5126127504533335

Epoch: 6| Step: 7
Training loss: 2.3388285636901855
Validation loss: 2.5181934115707234

Epoch: 6| Step: 8
Training loss: 2.6250925064086914
Validation loss: 2.5170926611910582

Epoch: 6| Step: 9
Training loss: 3.2150721549987793
Validation loss: 2.518477552680559

Epoch: 6| Step: 10
Training loss: 2.500347137451172
Validation loss: 2.5186704666383806

Epoch: 6| Step: 11
Training loss: 2.882720947265625
Validation loss: 2.5171290238698325

Epoch: 6| Step: 12
Training loss: 3.0013327598571777
Validation loss: 2.5164431551451325

Epoch: 6| Step: 13
Training loss: 1.8948156833648682
Validation loss: 2.520181809702227

Epoch: 165| Step: 0
Training loss: 2.483234405517578
Validation loss: 2.52011215302252

Epoch: 6| Step: 1
Training loss: 3.5352120399475098
Validation loss: 2.5227238978109052

Epoch: 6| Step: 2
Training loss: 2.0804967880249023
Validation loss: 2.515672322242491

Epoch: 6| Step: 3
Training loss: 1.9632874727249146
Validation loss: 2.5233795963307863

Epoch: 6| Step: 4
Training loss: 1.8425476551055908
Validation loss: 2.523760152119462

Epoch: 6| Step: 5
Training loss: 3.0909781455993652
Validation loss: 2.5199953099732757

Epoch: 6| Step: 6
Training loss: 2.509636640548706
Validation loss: 2.5259975617931736

Epoch: 6| Step: 7
Training loss: 2.416322946548462
Validation loss: 2.5280093326363513

Epoch: 6| Step: 8
Training loss: 2.3773488998413086
Validation loss: 2.527757167816162

Epoch: 6| Step: 9
Training loss: 2.42808198928833
Validation loss: 2.534916026617891

Epoch: 6| Step: 10
Training loss: 3.2173912525177
Validation loss: 2.528805655817832

Epoch: 6| Step: 11
Training loss: 3.5323352813720703
Validation loss: 2.530465292674239

Epoch: 6| Step: 12
Training loss: 2.589026927947998
Validation loss: 2.5297608811368226

Epoch: 6| Step: 13
Training loss: 3.9060354232788086
Validation loss: 2.5287594103044078

Epoch: 166| Step: 0
Training loss: 2.81978702545166
Validation loss: 2.512250354213099

Epoch: 6| Step: 1
Training loss: 2.368617057800293
Validation loss: 2.5105048661590903

Epoch: 6| Step: 2
Training loss: 2.8911869525909424
Validation loss: 2.509758992861676

Epoch: 6| Step: 3
Training loss: 3.1868739128112793
Validation loss: 2.5050685533913235

Epoch: 6| Step: 4
Training loss: 3.022003173828125
Validation loss: 2.5010056931485414

Epoch: 6| Step: 5
Training loss: 3.224128246307373
Validation loss: 2.4980660715410785

Epoch: 6| Step: 6
Training loss: 2.347808837890625
Validation loss: 2.5009087849688787

Epoch: 6| Step: 7
Training loss: 2.60616397857666
Validation loss: 2.4989975857478317

Epoch: 6| Step: 8
Training loss: 2.459150791168213
Validation loss: 2.4955050714554323

Epoch: 6| Step: 9
Training loss: 2.272167205810547
Validation loss: 2.4980515203168316

Epoch: 6| Step: 10
Training loss: 2.4014320373535156
Validation loss: 2.4953872644773094

Epoch: 6| Step: 11
Training loss: 2.5266478061676025
Validation loss: 2.4967857201894126

Epoch: 6| Step: 12
Training loss: 2.924370050430298
Validation loss: 2.503212216079876

Epoch: 6| Step: 13
Training loss: 2.1931896209716797
Validation loss: 2.502181508207834

Epoch: 167| Step: 0
Training loss: 3.1056785583496094
Validation loss: 2.502700818482266

Epoch: 6| Step: 1
Training loss: 2.909252166748047
Validation loss: 2.5085149016431583

Epoch: 6| Step: 2
Training loss: 2.1902031898498535
Validation loss: 2.5082745577699397

Epoch: 6| Step: 3
Training loss: 2.7623450756073
Validation loss: 2.5165869523120183

Epoch: 6| Step: 4
Training loss: 2.8603010177612305
Validation loss: 2.5216383523838495

Epoch: 6| Step: 5
Training loss: 3.1903626918792725
Validation loss: 2.5245647597056564

Epoch: 6| Step: 6
Training loss: 2.7783432006835938
Validation loss: 2.54029526505419

Epoch: 6| Step: 7
Training loss: 2.5746302604675293
Validation loss: 2.529306520697891

Epoch: 6| Step: 8
Training loss: 2.646902561187744
Validation loss: 2.533595733745124

Epoch: 6| Step: 9
Training loss: 2.8554344177246094
Validation loss: 2.5226502162154003

Epoch: 6| Step: 10
Training loss: 2.6404471397399902
Validation loss: 2.5128324083102647

Epoch: 6| Step: 11
Training loss: 2.5637059211730957
Validation loss: 2.507703770873367

Epoch: 6| Step: 12
Training loss: 2.0846171379089355
Validation loss: 2.5051620544925814

Epoch: 6| Step: 13
Training loss: 2.0265755653381348
Validation loss: 2.5038405285086682

Epoch: 168| Step: 0
Training loss: 2.2621240615844727
Validation loss: 2.5037948290506997

Epoch: 6| Step: 1
Training loss: 3.038881301879883
Validation loss: 2.501851611239936

Epoch: 6| Step: 2
Training loss: 2.597634792327881
Validation loss: 2.5031905917711157

Epoch: 6| Step: 3
Training loss: 2.584336996078491
Validation loss: 2.508990877418108

Epoch: 6| Step: 4
Training loss: 2.2416491508483887
Validation loss: 2.5088264839623564

Epoch: 6| Step: 5
Training loss: 2.777585029602051
Validation loss: 2.516180496062002

Epoch: 6| Step: 6
Training loss: 2.7730870246887207
Validation loss: 2.5279081277949835

Epoch: 6| Step: 7
Training loss: 2.793457508087158
Validation loss: 2.522941871355939

Epoch: 6| Step: 8
Training loss: 2.5898375511169434
Validation loss: 2.5223813338946273

Epoch: 6| Step: 9
Training loss: 3.24605131149292
Validation loss: 2.527602505940263

Epoch: 6| Step: 10
Training loss: 3.250892162322998
Validation loss: 2.5234972251358854

Epoch: 6| Step: 11
Training loss: 2.559406042098999
Validation loss: 2.5180086320446384

Epoch: 6| Step: 12
Training loss: 1.9838759899139404
Validation loss: 2.511300738139819

Epoch: 6| Step: 13
Training loss: 2.8774986267089844
Validation loss: 2.5160326829520603

Epoch: 169| Step: 0
Training loss: 2.8460662364959717
Validation loss: 2.518345917424848

Epoch: 6| Step: 1
Training loss: 2.4225854873657227
Validation loss: 2.516513603989796

Epoch: 6| Step: 2
Training loss: 2.9884281158447266
Validation loss: 2.512483427601476

Epoch: 6| Step: 3
Training loss: 3.081481456756592
Validation loss: 2.5205232815075944

Epoch: 6| Step: 4
Training loss: 2.5770390033721924
Validation loss: 2.509076923452398

Epoch: 6| Step: 5
Training loss: 2.81037974357605
Validation loss: 2.506727228882492

Epoch: 6| Step: 6
Training loss: 2.9255735874176025
Validation loss: 2.500551446791618

Epoch: 6| Step: 7
Training loss: 3.017983913421631
Validation loss: 2.5035716282424105

Epoch: 6| Step: 8
Training loss: 1.9247729778289795
Validation loss: 2.4975960126487156

Epoch: 6| Step: 9
Training loss: 1.9839606285095215
Validation loss: 2.4946003267841954

Epoch: 6| Step: 10
Training loss: 2.7153735160827637
Validation loss: 2.4967919370179534

Epoch: 6| Step: 11
Training loss: 3.1938083171844482
Validation loss: 2.4916325769116803

Epoch: 6| Step: 12
Training loss: 2.6965219974517822
Validation loss: 2.496348798915904

Epoch: 6| Step: 13
Training loss: 2.012749671936035
Validation loss: 2.498950425014701

Epoch: 170| Step: 0
Training loss: 2.7592687606811523
Validation loss: 2.5000878713464223

Epoch: 6| Step: 1
Training loss: 1.6608325242996216
Validation loss: 2.4985898758775447

Epoch: 6| Step: 2
Training loss: 2.649043083190918
Validation loss: 2.5078413255753054

Epoch: 6| Step: 3
Training loss: 2.525970458984375
Validation loss: 2.507518970838157

Epoch: 6| Step: 4
Training loss: 2.7103652954101562
Validation loss: 2.5166651484786824

Epoch: 6| Step: 5
Training loss: 3.072681188583374
Validation loss: 2.521897198051535

Epoch: 6| Step: 6
Training loss: 2.7056891918182373
Validation loss: 2.531071275793096

Epoch: 6| Step: 7
Training loss: 2.7178826332092285
Validation loss: 2.529399953862672

Epoch: 6| Step: 8
Training loss: 2.3727331161499023
Validation loss: 2.534732036693122

Epoch: 6| Step: 9
Training loss: 2.5673069953918457
Validation loss: 2.5645352871187272

Epoch: 6| Step: 10
Training loss: 3.6496236324310303
Validation loss: 2.6023880230483187

Epoch: 6| Step: 11
Training loss: 2.566115617752075
Validation loss: 2.586392161666706

Epoch: 6| Step: 12
Training loss: 2.359856367111206
Validation loss: 2.54223556159645

Epoch: 6| Step: 13
Training loss: 3.6151890754699707
Validation loss: 2.516366086980348

Epoch: 171| Step: 0
Training loss: 2.442809581756592
Validation loss: 2.5025133086789038

Epoch: 6| Step: 1
Training loss: 3.0055124759674072
Validation loss: 2.498238061064033

Epoch: 6| Step: 2
Training loss: 2.798363208770752
Validation loss: 2.495225093697989

Epoch: 6| Step: 3
Training loss: 2.867161989212036
Validation loss: 2.5030385242995394

Epoch: 6| Step: 4
Training loss: 3.7741618156433105
Validation loss: 2.515913978699715

Epoch: 6| Step: 5
Training loss: 3.0511674880981445
Validation loss: 2.521293890091681

Epoch: 6| Step: 6
Training loss: 2.4176535606384277
Validation loss: 2.5302624933181272

Epoch: 6| Step: 7
Training loss: 2.887791872024536
Validation loss: 2.5267332753827496

Epoch: 6| Step: 8
Training loss: 2.4471936225891113
Validation loss: 2.5226355047636133

Epoch: 6| Step: 9
Training loss: 2.3816280364990234
Validation loss: 2.5200749827969458

Epoch: 6| Step: 10
Training loss: 2.400381088256836
Validation loss: 2.5112150715243433

Epoch: 6| Step: 11
Training loss: 2.232670545578003
Validation loss: 2.5035844592637915

Epoch: 6| Step: 12
Training loss: 2.1967177391052246
Validation loss: 2.4956849211005756

Epoch: 6| Step: 13
Training loss: 2.7930989265441895
Validation loss: 2.4898298991623746

Epoch: 172| Step: 0
Training loss: 3.0664587020874023
Validation loss: 2.4865040292022047

Epoch: 6| Step: 1
Training loss: 2.5637991428375244
Validation loss: 2.4855149228085756

Epoch: 6| Step: 2
Training loss: 2.214935779571533
Validation loss: 2.485845342759163

Epoch: 6| Step: 3
Training loss: 3.198668956756592
Validation loss: 2.488790573612336

Epoch: 6| Step: 4
Training loss: 2.9492392539978027
Validation loss: 2.4891896119681736

Epoch: 6| Step: 5
Training loss: 3.030799388885498
Validation loss: 2.48656524637694

Epoch: 6| Step: 6
Training loss: 2.3163444995880127
Validation loss: 2.486972406346311

Epoch: 6| Step: 7
Training loss: 3.0120491981506348
Validation loss: 2.486291157302036

Epoch: 6| Step: 8
Training loss: 2.468574047088623
Validation loss: 2.4839844293491815

Epoch: 6| Step: 9
Training loss: 2.89321231842041
Validation loss: 2.486205736796061

Epoch: 6| Step: 10
Training loss: 2.5776917934417725
Validation loss: 2.483459641856532

Epoch: 6| Step: 11
Training loss: 1.812084436416626
Validation loss: 2.485847229598671

Epoch: 6| Step: 12
Training loss: 2.1555304527282715
Validation loss: 2.486286811931159

Epoch: 6| Step: 13
Training loss: 3.676363229751587
Validation loss: 2.489151693159534

Epoch: 173| Step: 0
Training loss: 2.7345118522644043
Validation loss: 2.492867551824098

Epoch: 6| Step: 1
Training loss: 2.7506632804870605
Validation loss: 2.492925759284727

Epoch: 6| Step: 2
Training loss: 2.9747769832611084
Validation loss: 2.4950425086482877

Epoch: 6| Step: 3
Training loss: 2.556142807006836
Validation loss: 2.491577338146907

Epoch: 6| Step: 4
Training loss: 3.092576503753662
Validation loss: 2.490572098762758

Epoch: 6| Step: 5
Training loss: 2.7049789428710938
Validation loss: 2.488672682034072

Epoch: 6| Step: 6
Training loss: 2.3892078399658203
Validation loss: 2.4909488206268637

Epoch: 6| Step: 7
Training loss: 3.557328224182129
Validation loss: 2.48909249613362

Epoch: 6| Step: 8
Training loss: 1.857184886932373
Validation loss: 2.492986015094224

Epoch: 6| Step: 9
Training loss: 2.606320858001709
Validation loss: 2.4920897483825684

Epoch: 6| Step: 10
Training loss: 2.6318535804748535
Validation loss: 2.4888798165064987

Epoch: 6| Step: 11
Training loss: 3.014012098312378
Validation loss: 2.4913552397040912

Epoch: 6| Step: 12
Training loss: 1.6650686264038086
Validation loss: 2.497652066651211

Epoch: 6| Step: 13
Training loss: 2.9651544094085693
Validation loss: 2.4959619788713354

Epoch: 174| Step: 0
Training loss: 3.472696542739868
Validation loss: 2.501267066565893

Epoch: 6| Step: 1
Training loss: 2.2004101276397705
Validation loss: 2.497097820364019

Epoch: 6| Step: 2
Training loss: 2.734619617462158
Validation loss: 2.494297532625096

Epoch: 6| Step: 3
Training loss: 2.6671152114868164
Validation loss: 2.492234932479038

Epoch: 6| Step: 4
Training loss: 3.0623128414154053
Validation loss: 2.492169944188928

Epoch: 6| Step: 5
Training loss: 2.8934249877929688
Validation loss: 2.489428802203107

Epoch: 6| Step: 6
Training loss: 2.650729179382324
Validation loss: 2.4884970213777278

Epoch: 6| Step: 7
Training loss: 2.7047762870788574
Validation loss: 2.49149328149775

Epoch: 6| Step: 8
Training loss: 1.3429778814315796
Validation loss: 2.4889089856096493

Epoch: 6| Step: 9
Training loss: 2.7862772941589355
Validation loss: 2.4922172254131687

Epoch: 6| Step: 10
Training loss: 3.0497846603393555
Validation loss: 2.493327012626074

Epoch: 6| Step: 11
Training loss: 2.2247650623321533
Validation loss: 2.4922090268904165

Epoch: 6| Step: 12
Training loss: 3.0158047676086426
Validation loss: 2.4967949813412083

Epoch: 6| Step: 13
Training loss: 2.633315086364746
Validation loss: 2.4948455313200593

Epoch: 175| Step: 0
Training loss: 2.646294593811035
Validation loss: 2.4947943687438965

Epoch: 6| Step: 1
Training loss: 1.8490655422210693
Validation loss: 2.494805271907519

Epoch: 6| Step: 2
Training loss: 2.075852394104004
Validation loss: 2.4891314147621073

Epoch: 6| Step: 3
Training loss: 2.9049620628356934
Validation loss: 2.489807503197783

Epoch: 6| Step: 4
Training loss: 2.436131238937378
Validation loss: 2.489755900957251

Epoch: 6| Step: 5
Training loss: 2.507617712020874
Validation loss: 2.490882060861075

Epoch: 6| Step: 6
Training loss: 2.6662681102752686
Validation loss: 2.4956668679432203

Epoch: 6| Step: 7
Training loss: 2.7075307369232178
Validation loss: 2.5012656245180356

Epoch: 6| Step: 8
Training loss: 3.5219175815582275
Validation loss: 2.504534116355322

Epoch: 6| Step: 9
Training loss: 3.014340400695801
Validation loss: 2.507419055508029

Epoch: 6| Step: 10
Training loss: 2.9673218727111816
Validation loss: 2.5035571603364843

Epoch: 6| Step: 11
Training loss: 3.1010217666625977
Validation loss: 2.5110046376464186

Epoch: 6| Step: 12
Training loss: 2.5339839458465576
Validation loss: 2.5112113004089682

Epoch: 6| Step: 13
Training loss: 2.2542669773101807
Validation loss: 2.513278212598575

Epoch: 176| Step: 0
Training loss: 2.1312994956970215
Validation loss: 2.516789969577584

Epoch: 6| Step: 1
Training loss: 2.724785566329956
Validation loss: 2.511122247224213

Epoch: 6| Step: 2
Training loss: 2.622063159942627
Validation loss: 2.5174899754985685

Epoch: 6| Step: 3
Training loss: 2.781160593032837
Validation loss: 2.525663960364557

Epoch: 6| Step: 4
Training loss: 3.0651350021362305
Validation loss: 2.5283347534877

Epoch: 6| Step: 5
Training loss: 3.0464119911193848
Validation loss: 2.5323060635597474

Epoch: 6| Step: 6
Training loss: 2.4240517616271973
Validation loss: 2.523636841004895

Epoch: 6| Step: 7
Training loss: 2.1241259574890137
Validation loss: 2.526278172769854

Epoch: 6| Step: 8
Training loss: 2.2293429374694824
Validation loss: 2.5053418297921457

Epoch: 6| Step: 9
Training loss: 3.0057034492492676
Validation loss: 2.5066261035139843

Epoch: 6| Step: 10
Training loss: 2.632344961166382
Validation loss: 2.504936577171408

Epoch: 6| Step: 11
Training loss: 2.4641666412353516
Validation loss: 2.4996411697838896

Epoch: 6| Step: 12
Training loss: 2.887176036834717
Validation loss: 2.501463400420322

Epoch: 6| Step: 13
Training loss: 3.4146888256073
Validation loss: 2.503503196982927

Epoch: 177| Step: 0
Training loss: 3.6328306198120117
Validation loss: 2.4952655812745452

Epoch: 6| Step: 1
Training loss: 2.239809513092041
Validation loss: 2.495698716050835

Epoch: 6| Step: 2
Training loss: 2.329044818878174
Validation loss: 2.4925710308936333

Epoch: 6| Step: 3
Training loss: 1.693230152130127
Validation loss: 2.49631247469174

Epoch: 6| Step: 4
Training loss: 2.820854902267456
Validation loss: 2.4896865365325764

Epoch: 6| Step: 5
Training loss: 3.2119221687316895
Validation loss: 2.4871947996078

Epoch: 6| Step: 6
Training loss: 2.6640853881835938
Validation loss: 2.4835615516990743

Epoch: 6| Step: 7
Training loss: 2.9678330421447754
Validation loss: 2.486122626130299

Epoch: 6| Step: 8
Training loss: 2.5341763496398926
Validation loss: 2.4849265006280716

Epoch: 6| Step: 9
Training loss: 2.945305347442627
Validation loss: 2.4827602396729174

Epoch: 6| Step: 10
Training loss: 2.4947335720062256
Validation loss: 2.4837112913849535

Epoch: 6| Step: 11
Training loss: 2.589810371398926
Validation loss: 2.485687750642018

Epoch: 6| Step: 12
Training loss: 2.5280888080596924
Validation loss: 2.48712985233594

Epoch: 6| Step: 13
Training loss: 2.683913230895996
Validation loss: 2.4906331441735707

Epoch: 178| Step: 0
Training loss: 3.0684030055999756
Validation loss: 2.4946609107396935

Epoch: 6| Step: 1
Training loss: 2.732724666595459
Validation loss: 2.497682535520164

Epoch: 6| Step: 2
Training loss: 2.1097524166107178
Validation loss: 2.5027069686561503

Epoch: 6| Step: 3
Training loss: 3.0216150283813477
Validation loss: 2.5089266274565007

Epoch: 6| Step: 4
Training loss: 2.9975767135620117
Validation loss: 2.511663585580805

Epoch: 6| Step: 5
Training loss: 1.8175593614578247
Validation loss: 2.5073462352957776

Epoch: 6| Step: 6
Training loss: 3.38155198097229
Validation loss: 2.494427643796449

Epoch: 6| Step: 7
Training loss: 2.7740883827209473
Validation loss: 2.497676472510061

Epoch: 6| Step: 8
Training loss: 2.5101852416992188
Validation loss: 2.4926741943564465

Epoch: 6| Step: 9
Training loss: 2.9410252571105957
Validation loss: 2.497280610504971

Epoch: 6| Step: 10
Training loss: 2.289409637451172
Validation loss: 2.4922313485094296

Epoch: 6| Step: 11
Training loss: 2.9568984508514404
Validation loss: 2.4949067690039195

Epoch: 6| Step: 12
Training loss: 2.5392818450927734
Validation loss: 2.4882687881428707

Epoch: 6| Step: 13
Training loss: 1.8455954790115356
Validation loss: 2.487183863116849

Epoch: 179| Step: 0
Training loss: 2.020268440246582
Validation loss: 2.4868693736291703

Epoch: 6| Step: 1
Training loss: 3.3395800590515137
Validation loss: 2.4862332138963925

Epoch: 6| Step: 2
Training loss: 3.0329010486602783
Validation loss: 2.4881864875875492

Epoch: 6| Step: 3
Training loss: 2.0237295627593994
Validation loss: 2.491662722761913

Epoch: 6| Step: 4
Training loss: 2.126117706298828
Validation loss: 2.4908242430738223

Epoch: 6| Step: 5
Training loss: 2.802562952041626
Validation loss: 2.4957355478758454

Epoch: 6| Step: 6
Training loss: 2.9102377891540527
Validation loss: 2.49505599083439

Epoch: 6| Step: 7
Training loss: 2.604884624481201
Validation loss: 2.497084476614511

Epoch: 6| Step: 8
Training loss: 2.5341877937316895
Validation loss: 2.4977561427700903

Epoch: 6| Step: 9
Training loss: 2.695985794067383
Validation loss: 2.5034623889512915

Epoch: 6| Step: 10
Training loss: 3.5468292236328125
Validation loss: 2.5044234465527278

Epoch: 6| Step: 11
Training loss: 2.9523260593414307
Validation loss: 2.5005292123363865

Epoch: 6| Step: 12
Training loss: 2.266923189163208
Validation loss: 2.506233658841861

Epoch: 6| Step: 13
Training loss: 2.059027910232544
Validation loss: 2.500156348751437

Epoch: 180| Step: 0
Training loss: 3.0544519424438477
Validation loss: 2.4989005186224498

Epoch: 6| Step: 1
Training loss: 2.8733835220336914
Validation loss: 2.5115442045273317

Epoch: 6| Step: 2
Training loss: 2.8640103340148926
Validation loss: 2.511687422311434

Epoch: 6| Step: 3
Training loss: 2.4521560668945312
Validation loss: 2.515128107481105

Epoch: 6| Step: 4
Training loss: 3.0956380367279053
Validation loss: 2.5108170176065094

Epoch: 6| Step: 5
Training loss: 2.9065358638763428
Validation loss: 2.5146616453765542

Epoch: 6| Step: 6
Training loss: 2.129930257797241
Validation loss: 2.5055288499401462

Epoch: 6| Step: 7
Training loss: 2.462556838989258
Validation loss: 2.5078908064032115

Epoch: 6| Step: 8
Training loss: 2.7789206504821777
Validation loss: 2.495527016219272

Epoch: 6| Step: 9
Training loss: 2.9146792888641357
Validation loss: 2.4926946111904678

Epoch: 6| Step: 10
Training loss: 2.3402814865112305
Validation loss: 2.495338157940936

Epoch: 6| Step: 11
Training loss: 2.1780130863189697
Validation loss: 2.4978930898891982

Epoch: 6| Step: 12
Training loss: 1.7911850214004517
Validation loss: 2.4894926881277435

Epoch: 6| Step: 13
Training loss: 3.849260091781616
Validation loss: 2.5039609375820366

Epoch: 181| Step: 0
Training loss: 3.145205020904541
Validation loss: 2.505036125900925

Epoch: 6| Step: 1
Training loss: 1.923264980316162
Validation loss: 2.5043308452893327

Epoch: 6| Step: 2
Training loss: 2.910762310028076
Validation loss: 2.508474539684993

Epoch: 6| Step: 3
Training loss: 3.080009937286377
Validation loss: 2.5069194557846233

Epoch: 6| Step: 4
Training loss: 2.5300965309143066
Validation loss: 2.5054512280289845

Epoch: 6| Step: 5
Training loss: 2.3693995475769043
Validation loss: 2.499879639635804

Epoch: 6| Step: 6
Training loss: 3.427861213684082
Validation loss: 2.49368873206518

Epoch: 6| Step: 7
Training loss: 2.282658576965332
Validation loss: 2.4985421011524815

Epoch: 6| Step: 8
Training loss: 3.4651458263397217
Validation loss: 2.4997671445210776

Epoch: 6| Step: 9
Training loss: 2.8603084087371826
Validation loss: 2.4999036968395276

Epoch: 6| Step: 10
Training loss: 2.118164539337158
Validation loss: 2.4956617868074806

Epoch: 6| Step: 11
Training loss: 2.424403667449951
Validation loss: 2.509548541038267

Epoch: 6| Step: 12
Training loss: 2.227605104446411
Validation loss: 2.502424778476838

Epoch: 6| Step: 13
Training loss: 2.2780957221984863
Validation loss: 2.493446143724585

Epoch: 182| Step: 0
Training loss: 3.178546905517578
Validation loss: 2.489857186553299

Epoch: 6| Step: 1
Training loss: 2.9973082542419434
Validation loss: 2.492700899800947

Epoch: 6| Step: 2
Training loss: 2.398496150970459
Validation loss: 2.4957611612094346

Epoch: 6| Step: 3
Training loss: 2.8551511764526367
Validation loss: 2.487727539513701

Epoch: 6| Step: 4
Training loss: 3.1032519340515137
Validation loss: 2.4854070678833993

Epoch: 6| Step: 5
Training loss: 2.023949146270752
Validation loss: 2.4950901692913425

Epoch: 6| Step: 6
Training loss: 2.648822784423828
Validation loss: 2.4954736745485695

Epoch: 6| Step: 7
Training loss: 1.7807906866073608
Validation loss: 2.497868081574799

Epoch: 6| Step: 8
Training loss: 2.609109401702881
Validation loss: 2.4893792957387944

Epoch: 6| Step: 9
Training loss: 2.2093424797058105
Validation loss: 2.485806526676301

Epoch: 6| Step: 10
Training loss: 2.9362611770629883
Validation loss: 2.4886188289170623

Epoch: 6| Step: 11
Training loss: 3.2093820571899414
Validation loss: 2.481764934396231

Epoch: 6| Step: 12
Training loss: 2.3263285160064697
Validation loss: 2.4810657296129452

Epoch: 6| Step: 13
Training loss: 3.171276807785034
Validation loss: 2.489535076643831

Epoch: 183| Step: 0
Training loss: 2.4604053497314453
Validation loss: 2.494124670182505

Epoch: 6| Step: 1
Training loss: 2.361123561859131
Validation loss: 2.499945807200606

Epoch: 6| Step: 2
Training loss: 2.7780699729919434
Validation loss: 2.505597053035613

Epoch: 6| Step: 3
Training loss: 3.247555732727051
Validation loss: 2.4975920274693477

Epoch: 6| Step: 4
Training loss: 3.35730242729187
Validation loss: 2.4967795315609185

Epoch: 6| Step: 5
Training loss: 2.915797710418701
Validation loss: 2.493718725378795

Epoch: 6| Step: 6
Training loss: 2.4967684745788574
Validation loss: 2.49793856118315

Epoch: 6| Step: 7
Training loss: 1.7915492057800293
Validation loss: 2.4938210825766287

Epoch: 6| Step: 8
Training loss: 2.4534387588500977
Validation loss: 2.501328699050411

Epoch: 6| Step: 9
Training loss: 2.277070999145508
Validation loss: 2.5008295223277104

Epoch: 6| Step: 10
Training loss: 2.2842535972595215
Validation loss: 2.5064779686671432

Epoch: 6| Step: 11
Training loss: 2.9299490451812744
Validation loss: 2.508608223289572

Epoch: 6| Step: 12
Training loss: 3.066071033477783
Validation loss: 2.5079525670697613

Epoch: 6| Step: 13
Training loss: 2.9478917121887207
Validation loss: 2.516012981373777

Epoch: 184| Step: 0
Training loss: 2.8979437351226807
Validation loss: 2.5190779650083153

Epoch: 6| Step: 1
Training loss: 2.1465039253234863
Validation loss: 2.5185605069642425

Epoch: 6| Step: 2
Training loss: 2.5812416076660156
Validation loss: 2.5166298830381004

Epoch: 6| Step: 3
Training loss: 2.7004880905151367
Validation loss: 2.5141861695115284

Epoch: 6| Step: 4
Training loss: 3.2008979320526123
Validation loss: 2.5157200649220455

Epoch: 6| Step: 5
Training loss: 2.791398525238037
Validation loss: 2.496666846736785

Epoch: 6| Step: 6
Training loss: 2.064685821533203
Validation loss: 2.492523962451566

Epoch: 6| Step: 7
Training loss: 2.7844691276550293
Validation loss: 2.493162214115102

Epoch: 6| Step: 8
Training loss: 1.7787493467330933
Validation loss: 2.490269484058503

Epoch: 6| Step: 9
Training loss: 3.164038896560669
Validation loss: 2.487059265054682

Epoch: 6| Step: 10
Training loss: 2.8251953125
Validation loss: 2.4887634144034436

Epoch: 6| Step: 11
Training loss: 2.4281580448150635
Validation loss: 2.4834255403087986

Epoch: 6| Step: 12
Training loss: 2.9807753562927246
Validation loss: 2.485603242792109

Epoch: 6| Step: 13
Training loss: 2.9714996814727783
Validation loss: 2.4844096091485794

Epoch: 185| Step: 0
Training loss: 2.691593647003174
Validation loss: 2.4800983552009828

Epoch: 6| Step: 1
Training loss: 2.35184645652771
Validation loss: 2.4858365494717836

Epoch: 6| Step: 2
Training loss: 3.289006233215332
Validation loss: 2.480377392102313

Epoch: 6| Step: 3
Training loss: 2.3517568111419678
Validation loss: 2.4821267025445097

Epoch: 6| Step: 4
Training loss: 1.9152029752731323
Validation loss: 2.480704028119323

Epoch: 6| Step: 5
Training loss: 2.6297523975372314
Validation loss: 2.4865194930825183

Epoch: 6| Step: 6
Training loss: 2.4307971000671387
Validation loss: 2.480667798749862

Epoch: 6| Step: 7
Training loss: 2.893578052520752
Validation loss: 2.480194373797345

Epoch: 6| Step: 8
Training loss: 2.9940614700317383
Validation loss: 2.4809363144700245

Epoch: 6| Step: 9
Training loss: 2.965008497238159
Validation loss: 2.479809291901127

Epoch: 6| Step: 10
Training loss: 2.5030555725097656
Validation loss: 2.4738963547573296

Epoch: 6| Step: 11
Training loss: 2.6487207412719727
Validation loss: 2.474422429197578

Epoch: 6| Step: 12
Training loss: 2.8484435081481934
Validation loss: 2.472453522425826

Epoch: 6| Step: 13
Training loss: 2.5426394939422607
Validation loss: 2.479631677750618

Epoch: 186| Step: 0
Training loss: 2.868875503540039
Validation loss: 2.475451574530653

Epoch: 6| Step: 1
Training loss: 2.775075912475586
Validation loss: 2.4769750359237834

Epoch: 6| Step: 2
Training loss: 3.3405203819274902
Validation loss: 2.4804876517224055

Epoch: 6| Step: 3
Training loss: 1.9975944757461548
Validation loss: 2.479366135853593

Epoch: 6| Step: 4
Training loss: 2.8480587005615234
Validation loss: 2.483935043375979

Epoch: 6| Step: 5
Training loss: 2.2477242946624756
Validation loss: 2.4860285238553117

Epoch: 6| Step: 6
Training loss: 2.3451151847839355
Validation loss: 2.4808554316079743

Epoch: 6| Step: 7
Training loss: 2.578523635864258
Validation loss: 2.478328184414935

Epoch: 6| Step: 8
Training loss: 2.336960792541504
Validation loss: 2.482336898003855

Epoch: 6| Step: 9
Training loss: 2.942095994949341
Validation loss: 2.484958971700361

Epoch: 6| Step: 10
Training loss: 2.001687526702881
Validation loss: 2.482226420474309

Epoch: 6| Step: 11
Training loss: 2.8032617568969727
Validation loss: 2.4879280956842567

Epoch: 6| Step: 12
Training loss: 3.1014881134033203
Validation loss: 2.4882113625926356

Epoch: 6| Step: 13
Training loss: 2.9472639560699463
Validation loss: 2.4778400851834204

Epoch: 187| Step: 0
Training loss: 2.479541778564453
Validation loss: 2.479343327142859

Epoch: 6| Step: 1
Training loss: 1.884873390197754
Validation loss: 2.4795845118902062

Epoch: 6| Step: 2
Training loss: 2.9390265941619873
Validation loss: 2.4784142150673816

Epoch: 6| Step: 3
Training loss: 2.8538479804992676
Validation loss: 2.481290166096021

Epoch: 6| Step: 4
Training loss: 3.0628838539123535
Validation loss: 2.4849563593505533

Epoch: 6| Step: 5
Training loss: 2.5978379249572754
Validation loss: 2.480199867679227

Epoch: 6| Step: 6
Training loss: 3.2207393646240234
Validation loss: 2.486940999184885

Epoch: 6| Step: 7
Training loss: 2.3618814945220947
Validation loss: 2.48887038230896

Epoch: 6| Step: 8
Training loss: 2.913860321044922
Validation loss: 2.4885593280997327

Epoch: 6| Step: 9
Training loss: 2.374338150024414
Validation loss: 2.4897544460911907

Epoch: 6| Step: 10
Training loss: 2.5694448947906494
Validation loss: 2.4930205473335842

Epoch: 6| Step: 11
Training loss: 2.354574203491211
Validation loss: 2.4937183728782077

Epoch: 6| Step: 12
Training loss: 2.7142293453216553
Validation loss: 2.5051198595313617

Epoch: 6| Step: 13
Training loss: 2.8529999256134033
Validation loss: 2.502701026137157

Epoch: 188| Step: 0
Training loss: 2.192959785461426
Validation loss: 2.506406440529772

Epoch: 6| Step: 1
Training loss: 3.4034581184387207
Validation loss: 2.498502577504804

Epoch: 6| Step: 2
Training loss: 3.440110206604004
Validation loss: 2.494373183096609

Epoch: 6| Step: 3
Training loss: 3.107372999191284
Validation loss: 2.4920071273721676

Epoch: 6| Step: 4
Training loss: 1.8083477020263672
Validation loss: 2.4824812232807116

Epoch: 6| Step: 5
Training loss: 2.170267105102539
Validation loss: 2.4831924592295

Epoch: 6| Step: 6
Training loss: 2.6937901973724365
Validation loss: 2.49217918867706

Epoch: 6| Step: 7
Training loss: 2.551748514175415
Validation loss: 2.4898649954026744

Epoch: 6| Step: 8
Training loss: 3.3233721256256104
Validation loss: 2.4860921405976817

Epoch: 6| Step: 9
Training loss: 2.2498271465301514
Validation loss: 2.4849586486816406

Epoch: 6| Step: 10
Training loss: 1.9615877866744995
Validation loss: 2.485296928754417

Epoch: 6| Step: 11
Training loss: 2.329880714416504
Validation loss: 2.4871059130596858

Epoch: 6| Step: 12
Training loss: 3.493246078491211
Validation loss: 2.4909805546524706

Epoch: 6| Step: 13
Training loss: 2.10770583152771
Validation loss: 2.4927389314097743

Epoch: 189| Step: 0
Training loss: 2.948885440826416
Validation loss: 2.4909884980929795

Epoch: 6| Step: 1
Training loss: 2.3357839584350586
Validation loss: 2.495902742108991

Epoch: 6| Step: 2
Training loss: 1.9854785203933716
Validation loss: 2.48714280128479

Epoch: 6| Step: 3
Training loss: 2.5308151245117188
Validation loss: 2.4931953619885188

Epoch: 6| Step: 4
Training loss: 3.0555498600006104
Validation loss: 2.4913513891158567

Epoch: 6| Step: 5
Training loss: 2.324619770050049
Validation loss: 2.4946328183656097

Epoch: 6| Step: 6
Training loss: 3.2576940059661865
Validation loss: 2.4950567547992994

Epoch: 6| Step: 7
Training loss: 2.615964412689209
Validation loss: 2.500455960150688

Epoch: 6| Step: 8
Training loss: 2.8073902130126953
Validation loss: 2.500013182240148

Epoch: 6| Step: 9
Training loss: 2.5555105209350586
Validation loss: 2.495035522727556

Epoch: 6| Step: 10
Training loss: 2.792421340942383
Validation loss: 2.4927317019431823

Epoch: 6| Step: 11
Training loss: 2.676999568939209
Validation loss: 2.4959400059074484

Epoch: 6| Step: 12
Training loss: 2.638965606689453
Validation loss: 2.506422999084637

Epoch: 6| Step: 13
Training loss: 2.1442437171936035
Validation loss: 2.5071110417765956

Epoch: 190| Step: 0
Training loss: 2.5054006576538086
Validation loss: 2.5292339145496325

Epoch: 6| Step: 1
Training loss: 2.896665096282959
Validation loss: 2.5211997416711625

Epoch: 6| Step: 2
Training loss: 2.1655397415161133
Validation loss: 2.508440584264776

Epoch: 6| Step: 3
Training loss: 3.400127410888672
Validation loss: 2.519072424980902

Epoch: 6| Step: 4
Training loss: 2.4708328247070312
Validation loss: 2.5160958331118346

Epoch: 6| Step: 5
Training loss: 2.7507104873657227
Validation loss: 2.5092162701391403

Epoch: 6| Step: 6
Training loss: 1.8214161396026611
Validation loss: 2.5112219266994025

Epoch: 6| Step: 7
Training loss: 3.362844705581665
Validation loss: 2.5120851891015166

Epoch: 6| Step: 8
Training loss: 3.491208791732788
Validation loss: 2.5213396164678756

Epoch: 6| Step: 9
Training loss: 2.9010753631591797
Validation loss: 2.515287970983854

Epoch: 6| Step: 10
Training loss: 2.493403196334839
Validation loss: 2.500167627488413

Epoch: 6| Step: 11
Training loss: 1.6821069717407227
Validation loss: 2.482609454021659

Epoch: 6| Step: 12
Training loss: 3.0066709518432617
Validation loss: 2.479152061605966

Epoch: 6| Step: 13
Training loss: 1.8633302450180054
Validation loss: 2.4751241258395615

Epoch: 191| Step: 0
Training loss: 2.2760391235351562
Validation loss: 2.480607899286414

Epoch: 6| Step: 1
Training loss: 2.8316874504089355
Validation loss: 2.4789065135422574

Epoch: 6| Step: 2
Training loss: 3.220440149307251
Validation loss: 2.4766181925291657

Epoch: 6| Step: 3
Training loss: 2.2990312576293945
Validation loss: 2.4820676362642677

Epoch: 6| Step: 4
Training loss: 2.736315965652466
Validation loss: 2.4814457149915796

Epoch: 6| Step: 5
Training loss: 2.4927592277526855
Validation loss: 2.4800119425660823

Epoch: 6| Step: 6
Training loss: 2.3075709342956543
Validation loss: 2.4790893216286936

Epoch: 6| Step: 7
Training loss: 3.301034450531006
Validation loss: 2.477687055064786

Epoch: 6| Step: 8
Training loss: 2.1617584228515625
Validation loss: 2.4829224258340816

Epoch: 6| Step: 9
Training loss: 1.8149045705795288
Validation loss: 2.4725820454218055

Epoch: 6| Step: 10
Training loss: 2.6653811931610107
Validation loss: 2.4779532596629155

Epoch: 6| Step: 11
Training loss: 2.5449392795562744
Validation loss: 2.481713366764848

Epoch: 6| Step: 12
Training loss: 3.468555450439453
Validation loss: 2.4870822198929323

Epoch: 6| Step: 13
Training loss: 3.304507255554199
Validation loss: 2.486008961995443

Epoch: 192| Step: 0
Training loss: 2.7291316986083984
Validation loss: 2.482914327293314

Epoch: 6| Step: 1
Training loss: 2.3146257400512695
Validation loss: 2.4858247221157117

Epoch: 6| Step: 2
Training loss: 2.691316604614258
Validation loss: 2.485688009569722

Epoch: 6| Step: 3
Training loss: 2.952028274536133
Validation loss: 2.481160730443975

Epoch: 6| Step: 4
Training loss: 2.6062021255493164
Validation loss: 2.4743575588349374

Epoch: 6| Step: 5
Training loss: 2.826930046081543
Validation loss: 2.478140092665149

Epoch: 6| Step: 6
Training loss: 2.982853412628174
Validation loss: 2.4769031693858485

Epoch: 6| Step: 7
Training loss: 2.5752975940704346
Validation loss: 2.489694139008881

Epoch: 6| Step: 8
Training loss: 3.1880455017089844
Validation loss: 2.487110604522049

Epoch: 6| Step: 9
Training loss: 2.489198684692383
Validation loss: 2.4859709842230684

Epoch: 6| Step: 10
Training loss: 2.7184078693389893
Validation loss: 2.504760919078704

Epoch: 6| Step: 11
Training loss: 2.5453314781188965
Validation loss: 2.524447956392842

Epoch: 6| Step: 12
Training loss: 2.2459726333618164
Validation loss: 2.5648331949787755

Epoch: 6| Step: 13
Training loss: 1.8037325143814087
Validation loss: 2.5345272223154702

Epoch: 193| Step: 0
Training loss: 2.4722015857696533
Validation loss: 2.516205328767018

Epoch: 6| Step: 1
Training loss: 3.0318503379821777
Validation loss: 2.494835258812033

Epoch: 6| Step: 2
Training loss: 2.315917491912842
Validation loss: 2.4833397532022126

Epoch: 6| Step: 3
Training loss: 2.429871082305908
Validation loss: 2.4774136312546267

Epoch: 6| Step: 4
Training loss: 2.820903778076172
Validation loss: 2.4723353667925765

Epoch: 6| Step: 5
Training loss: 2.507756471633911
Validation loss: 2.4728984089307886

Epoch: 6| Step: 6
Training loss: 3.8006317615509033
Validation loss: 2.465293549722241

Epoch: 6| Step: 7
Training loss: 2.5129008293151855
Validation loss: 2.4616399657341743

Epoch: 6| Step: 8
Training loss: 2.549489974975586
Validation loss: 2.4739801499151413

Epoch: 6| Step: 9
Training loss: 2.4814600944519043
Validation loss: 2.4677775470159387

Epoch: 6| Step: 10
Training loss: 2.3787388801574707
Validation loss: 2.4701647630301853

Epoch: 6| Step: 11
Training loss: 2.492906332015991
Validation loss: 2.4761656612478276

Epoch: 6| Step: 12
Training loss: 2.1167550086975098
Validation loss: 2.466153214054723

Epoch: 6| Step: 13
Training loss: 3.509610414505005
Validation loss: 2.4658675603969122

Epoch: 194| Step: 0
Training loss: 2.1585874557495117
Validation loss: 2.4666595817894064

Epoch: 6| Step: 1
Training loss: 2.2814130783081055
Validation loss: 2.477910521209881

Epoch: 6| Step: 2
Training loss: 2.8675854206085205
Validation loss: 2.4937506644956526

Epoch: 6| Step: 3
Training loss: 2.9991021156311035
Validation loss: 2.4879786609321513

Epoch: 6| Step: 4
Training loss: 2.8374855518341064
Validation loss: 2.5021880826642438

Epoch: 6| Step: 5
Training loss: 1.8452577590942383
Validation loss: 2.4991804886889715

Epoch: 6| Step: 6
Training loss: 3.3200178146362305
Validation loss: 2.515266751730314

Epoch: 6| Step: 7
Training loss: 2.0073330402374268
Validation loss: 2.5166961377666843

Epoch: 6| Step: 8
Training loss: 2.8227639198303223
Validation loss: 2.5115160967714045

Epoch: 6| Step: 9
Training loss: 2.628366470336914
Validation loss: 2.518609728864444

Epoch: 6| Step: 10
Training loss: 2.3960177898406982
Validation loss: 2.514567195728261

Epoch: 6| Step: 11
Training loss: 2.6581430435180664
Validation loss: 2.4967199525525494

Epoch: 6| Step: 12
Training loss: 2.7659857273101807
Validation loss: 2.500397256625596

Epoch: 6| Step: 13
Training loss: 3.9809930324554443
Validation loss: 2.4955073953956686

Epoch: 195| Step: 0
Training loss: 2.1681301593780518
Validation loss: 2.4956409213363484

Epoch: 6| Step: 1
Training loss: 2.614226818084717
Validation loss: 2.5043657902748353

Epoch: 6| Step: 2
Training loss: 2.627668857574463
Validation loss: 2.498587426318917

Epoch: 6| Step: 3
Training loss: 2.5706233978271484
Validation loss: 2.4945455648565806

Epoch: 6| Step: 4
Training loss: 2.4911608695983887
Validation loss: 2.48405102504197

Epoch: 6| Step: 5
Training loss: 2.7990400791168213
Validation loss: 2.4773378192737536

Epoch: 6| Step: 6
Training loss: 1.6213221549987793
Validation loss: 2.474890201322494

Epoch: 6| Step: 7
Training loss: 2.7730348110198975
Validation loss: 2.469248289703041

Epoch: 6| Step: 8
Training loss: 2.4295248985290527
Validation loss: 2.4771619099442677

Epoch: 6| Step: 9
Training loss: 2.993584632873535
Validation loss: 2.4840253476173646

Epoch: 6| Step: 10
Training loss: 3.150801181793213
Validation loss: 2.4880618433798514

Epoch: 6| Step: 11
Training loss: 2.590540885925293
Validation loss: 2.4834108121933474

Epoch: 6| Step: 12
Training loss: 3.449267625808716
Validation loss: 2.509511911740867

Epoch: 6| Step: 13
Training loss: 2.7538137435913086
Validation loss: 2.5140769712386595

Epoch: 196| Step: 0
Training loss: 2.5776288509368896
Validation loss: 2.514429683326393

Epoch: 6| Step: 1
Training loss: 3.1679537296295166
Validation loss: 2.5230040601504746

Epoch: 6| Step: 2
Training loss: 2.2711939811706543
Validation loss: 2.5288225015004477

Epoch: 6| Step: 3
Training loss: 2.4488487243652344
Validation loss: 2.5327979544157624

Epoch: 6| Step: 4
Training loss: 2.7937703132629395
Validation loss: 2.5288539496801232

Epoch: 6| Step: 5
Training loss: 2.459657669067383
Validation loss: 2.5269584040487967

Epoch: 6| Step: 6
Training loss: 3.2227256298065186
Validation loss: 2.524424086334885

Epoch: 6| Step: 7
Training loss: 2.2230401039123535
Validation loss: 2.5067639607255177

Epoch: 6| Step: 8
Training loss: 2.7458581924438477
Validation loss: 2.4988741669603574

Epoch: 6| Step: 9
Training loss: 2.6300876140594482
Validation loss: 2.4956453231073197

Epoch: 6| Step: 10
Training loss: 2.1073338985443115
Validation loss: 2.488004215302006

Epoch: 6| Step: 11
Training loss: 2.478283643722534
Validation loss: 2.4981483644054783

Epoch: 6| Step: 12
Training loss: 2.828171730041504
Validation loss: 2.4997353374317126

Epoch: 6| Step: 13
Training loss: 3.694620370864868
Validation loss: 2.4970756833271315

Epoch: 197| Step: 0
Training loss: 2.9341301918029785
Validation loss: 2.4940079514698317

Epoch: 6| Step: 1
Training loss: 2.664625406265259
Validation loss: 2.4892925344487673

Epoch: 6| Step: 2
Training loss: 2.6932969093322754
Validation loss: 2.485323590617026

Epoch: 6| Step: 3
Training loss: 2.3680343627929688
Validation loss: 2.4721943819394676

Epoch: 6| Step: 4
Training loss: 2.293111801147461
Validation loss: 2.463837359541206

Epoch: 6| Step: 5
Training loss: 1.8627266883850098
Validation loss: 2.4622207969747563

Epoch: 6| Step: 6
Training loss: 3.061150550842285
Validation loss: 2.457374470208281

Epoch: 6| Step: 7
Training loss: 2.5456089973449707
Validation loss: 2.455934057953537

Epoch: 6| Step: 8
Training loss: 3.2780234813690186
Validation loss: 2.4624622714134956

Epoch: 6| Step: 9
Training loss: 2.1703133583068848
Validation loss: 2.4700126237766717

Epoch: 6| Step: 10
Training loss: 2.8239312171936035
Validation loss: 2.464943496129846

Epoch: 6| Step: 11
Training loss: 2.9414892196655273
Validation loss: 2.4625992467326503

Epoch: 6| Step: 12
Training loss: 2.8104259967803955
Validation loss: 2.4612309753253894

Epoch: 6| Step: 13
Training loss: 2.545881509780884
Validation loss: 2.453885139957551

Epoch: 198| Step: 0
Training loss: 2.9141390323638916
Validation loss: 2.4505560193010556

Epoch: 6| Step: 1
Training loss: 2.5509016513824463
Validation loss: 2.454817707820605

Epoch: 6| Step: 2
Training loss: 2.6732659339904785
Validation loss: 2.4500196723527807

Epoch: 6| Step: 3
Training loss: 2.540027379989624
Validation loss: 2.4482114494487806

Epoch: 6| Step: 4
Training loss: 1.9414340257644653
Validation loss: 2.453451705235307

Epoch: 6| Step: 5
Training loss: 3.3052988052368164
Validation loss: 2.454102626410864

Epoch: 6| Step: 6
Training loss: 2.2041523456573486
Validation loss: 2.4548310387519097

Epoch: 6| Step: 7
Training loss: 2.6084070205688477
Validation loss: 2.4522994743880404

Epoch: 6| Step: 8
Training loss: 2.6841208934783936
Validation loss: 2.4595799651197208

Epoch: 6| Step: 9
Training loss: 2.986052989959717
Validation loss: 2.4653731597367154

Epoch: 6| Step: 10
Training loss: 2.4833950996398926
Validation loss: 2.4647027010558755

Epoch: 6| Step: 11
Training loss: 2.328252077102661
Validation loss: 2.475931224002633

Epoch: 6| Step: 12
Training loss: 2.8235433101654053
Validation loss: 2.476862097299227

Epoch: 6| Step: 13
Training loss: 3.18422269821167
Validation loss: 2.4854454609655563

Epoch: 199| Step: 0
Training loss: 2.716327428817749
Validation loss: 2.485175068660449

Epoch: 6| Step: 1
Training loss: 2.150937795639038
Validation loss: 2.487406256378338

Epoch: 6| Step: 2
Training loss: 2.565974235534668
Validation loss: 2.507153795611474

Epoch: 6| Step: 3
Training loss: 3.1634271144866943
Validation loss: 2.481966810841714

Epoch: 6| Step: 4
Training loss: 3.2863011360168457
Validation loss: 2.477739921180151

Epoch: 6| Step: 5
Training loss: 2.295943260192871
Validation loss: 2.4503038621717885

Epoch: 6| Step: 6
Training loss: 2.6553120613098145
Validation loss: 2.4612364999709593

Epoch: 6| Step: 7
Training loss: 2.492204189300537
Validation loss: 2.4598432458857054

Epoch: 6| Step: 8
Training loss: 3.1587796211242676
Validation loss: 2.458390712738037

Epoch: 6| Step: 9
Training loss: 1.9651895761489868
Validation loss: 2.4566787776126655

Epoch: 6| Step: 10
Training loss: 2.4207918643951416
Validation loss: 2.462962637665451

Epoch: 6| Step: 11
Training loss: 3.310606002807617
Validation loss: 2.4586272367867092

Epoch: 6| Step: 12
Training loss: 2.49775767326355
Validation loss: 2.4609385587835826

Epoch: 6| Step: 13
Training loss: 2.1999120712280273
Validation loss: 2.454897619062854

Epoch: 200| Step: 0
Training loss: 1.6197619438171387
Validation loss: 2.477595485666747

Epoch: 6| Step: 1
Training loss: 2.106353759765625
Validation loss: 2.4738662524889876

Epoch: 6| Step: 2
Training loss: 3.040306568145752
Validation loss: 2.500556151072184

Epoch: 6| Step: 3
Training loss: 2.128089427947998
Validation loss: 2.5048232642553185

Epoch: 6| Step: 4
Training loss: 2.7789626121520996
Validation loss: 2.5213963985443115

Epoch: 6| Step: 5
Training loss: 2.0716586112976074
Validation loss: 2.5053318315936672

Epoch: 6| Step: 6
Training loss: 2.7133960723876953
Validation loss: 2.5018258017878376

Epoch: 6| Step: 7
Training loss: 2.9835667610168457
Validation loss: 2.508759537050801

Epoch: 6| Step: 8
Training loss: 3.662395477294922
Validation loss: 2.496560478723177

Epoch: 6| Step: 9
Training loss: 2.7361080646514893
Validation loss: 2.4883961780096895

Epoch: 6| Step: 10
Training loss: 3.4377901554107666
Validation loss: 2.4890413309938166

Epoch: 6| Step: 11
Training loss: 2.580230712890625
Validation loss: 2.496637475106024

Epoch: 6| Step: 12
Training loss: 2.952357292175293
Validation loss: 2.478874270633985

Epoch: 6| Step: 13
Training loss: 1.70821213722229
Validation loss: 2.482233198740149

Epoch: 201| Step: 0
Training loss: 2.850193977355957
Validation loss: 2.473455681595751

Epoch: 6| Step: 1
Training loss: 2.2189114093780518
Validation loss: 2.4841097862489763

Epoch: 6| Step: 2
Training loss: 2.46305513381958
Validation loss: 2.4763558936375443

Epoch: 6| Step: 3
Training loss: 2.5238959789276123
Validation loss: 2.4787889783100416

Epoch: 6| Step: 4
Training loss: 2.7083210945129395
Validation loss: 2.4800024596593713

Epoch: 6| Step: 5
Training loss: 2.343898296356201
Validation loss: 2.4983258170466267

Epoch: 6| Step: 6
Training loss: 3.265105724334717
Validation loss: 2.4870693068350516

Epoch: 6| Step: 7
Training loss: 2.8234920501708984
Validation loss: 2.4971063598509757

Epoch: 6| Step: 8
Training loss: 2.5757951736450195
Validation loss: 2.5052631388428392

Epoch: 6| Step: 9
Training loss: 2.862942695617676
Validation loss: 2.496403630061816

Epoch: 6| Step: 10
Training loss: 2.7293317317962646
Validation loss: 2.4974735090809483

Epoch: 6| Step: 11
Training loss: 1.9817917346954346
Validation loss: 2.490759011237852

Epoch: 6| Step: 12
Training loss: 2.4755098819732666
Validation loss: 2.4878860647960375

Epoch: 6| Step: 13
Training loss: 3.4192075729370117
Validation loss: 2.49748396360746

Epoch: 202| Step: 0
Training loss: 2.1830170154571533
Validation loss: 2.4960853284405125

Epoch: 6| Step: 1
Training loss: 3.162067413330078
Validation loss: 2.508622689913678

Epoch: 6| Step: 2
Training loss: 2.792954921722412
Validation loss: 2.4958189123420307

Epoch: 6| Step: 3
Training loss: 2.6544370651245117
Validation loss: 2.4833464571224746

Epoch: 6| Step: 4
Training loss: 3.260991334915161
Validation loss: 2.479907792101624

Epoch: 6| Step: 5
Training loss: 1.5757850408554077
Validation loss: 2.493342753379576

Epoch: 6| Step: 6
Training loss: 2.7273848056793213
Validation loss: 2.467251080338673

Epoch: 6| Step: 7
Training loss: 2.5786843299865723
Validation loss: 2.4778367037414224

Epoch: 6| Step: 8
Training loss: 2.9349913597106934
Validation loss: 2.482872268205048

Epoch: 6| Step: 9
Training loss: 2.6773438453674316
Validation loss: 2.4869575346669843

Epoch: 6| Step: 10
Training loss: 2.8306009769439697
Validation loss: 2.4964040581898024

Epoch: 6| Step: 11
Training loss: 2.531951904296875
Validation loss: 2.494215024414883

Epoch: 6| Step: 12
Training loss: 2.3515422344207764
Validation loss: 2.492767944130846

Epoch: 6| Step: 13
Training loss: 2.3051822185516357
Validation loss: 2.4868378254675094

Epoch: 203| Step: 0
Training loss: 2.596738576889038
Validation loss: 2.4746102312559723

Epoch: 6| Step: 1
Training loss: 2.8389787673950195
Validation loss: 2.471567471822103

Epoch: 6| Step: 2
Training loss: 2.381514549255371
Validation loss: 2.4682538945187806

Epoch: 6| Step: 3
Training loss: 3.1455588340759277
Validation loss: 2.46966468134234

Epoch: 6| Step: 4
Training loss: 2.3771204948425293
Validation loss: 2.4804850547544417

Epoch: 6| Step: 5
Training loss: 1.8237271308898926
Validation loss: 2.475971575706236

Epoch: 6| Step: 6
Training loss: 3.024895668029785
Validation loss: 2.4681690226319017

Epoch: 6| Step: 7
Training loss: 2.202707529067993
Validation loss: 2.4799320313238327

Epoch: 6| Step: 8
Training loss: 3.7843177318573
Validation loss: 2.5233409430391047

Epoch: 6| Step: 9
Training loss: 3.3522417545318604
Validation loss: 2.539909370483891

Epoch: 6| Step: 10
Training loss: 2.772202491760254
Validation loss: 2.541531562805176

Epoch: 6| Step: 11
Training loss: 2.0613696575164795
Validation loss: 2.546546482270764

Epoch: 6| Step: 12
Training loss: 2.083864212036133
Validation loss: 2.5338539513208533

Epoch: 6| Step: 13
Training loss: 2.301744222640991
Validation loss: 2.5138673859257854

Epoch: 204| Step: 0
Training loss: 2.082463264465332
Validation loss: 2.484583080455821

Epoch: 6| Step: 1
Training loss: 2.501319646835327
Validation loss: 2.4636239800401913

Epoch: 6| Step: 2
Training loss: 2.9148073196411133
Validation loss: 2.4664127237053326

Epoch: 6| Step: 3
Training loss: 2.8268826007843018
Validation loss: 2.4715604397558395

Epoch: 6| Step: 4
Training loss: 2.538978099822998
Validation loss: 2.466774641826589

Epoch: 6| Step: 5
Training loss: 3.2369203567504883
Validation loss: 2.4709033017517417

Epoch: 6| Step: 6
Training loss: 3.3553805351257324
Validation loss: 2.4726284088626986

Epoch: 6| Step: 7
Training loss: 2.8657619953155518
Validation loss: 2.472100386055567

Epoch: 6| Step: 8
Training loss: 2.66276216506958
Validation loss: 2.4731946222243772

Epoch: 6| Step: 9
Training loss: 1.9323885440826416
Validation loss: 2.477089471714471

Epoch: 6| Step: 10
Training loss: 3.1477386951446533
Validation loss: 2.482249116384855

Epoch: 6| Step: 11
Training loss: 2.6608569622039795
Validation loss: 2.4747880402431695

Epoch: 6| Step: 12
Training loss: 1.6168394088745117
Validation loss: 2.4625682061718357

Epoch: 6| Step: 13
Training loss: 2.3782379627227783
Validation loss: 2.4656589492674796

Epoch: 205| Step: 0
Training loss: 2.545356035232544
Validation loss: 2.465362779555782

Epoch: 6| Step: 1
Training loss: 2.1902146339416504
Validation loss: 2.4924942242201937

Epoch: 6| Step: 2
Training loss: 2.51550555229187
Validation loss: 2.504470420140092

Epoch: 6| Step: 3
Training loss: 2.0777573585510254
Validation loss: 2.5293286128710677

Epoch: 6| Step: 4
Training loss: 2.5595505237579346
Validation loss: 2.546674141319849

Epoch: 6| Step: 5
Training loss: 2.333158493041992
Validation loss: 2.5528305089601906

Epoch: 6| Step: 6
Training loss: 2.825634717941284
Validation loss: 2.532055567669612

Epoch: 6| Step: 7
Training loss: 3.1439595222473145
Validation loss: 2.5284711519877114

Epoch: 6| Step: 8
Training loss: 2.4118096828460693
Validation loss: 2.5049447039122223

Epoch: 6| Step: 9
Training loss: 2.992213249206543
Validation loss: 2.4786155377664874

Epoch: 6| Step: 10
Training loss: 2.8051395416259766
Validation loss: 2.470842415286649

Epoch: 6| Step: 11
Training loss: 3.086364984512329
Validation loss: 2.4614354974480084

Epoch: 6| Step: 12
Training loss: 2.942355155944824
Validation loss: 2.4595051401404926

Epoch: 6| Step: 13
Training loss: 2.2189886569976807
Validation loss: 2.4591391983852593

Epoch: 206| Step: 0
Training loss: 2.341728448867798
Validation loss: 2.4561497575493267

Epoch: 6| Step: 1
Training loss: 3.525705337524414
Validation loss: 2.467139902935233

Epoch: 6| Step: 2
Training loss: 2.712449789047241
Validation loss: 2.463571028042865

Epoch: 6| Step: 3
Training loss: 1.4688365459442139
Validation loss: 2.4680075209627867

Epoch: 6| Step: 4
Training loss: 3.124523162841797
Validation loss: 2.4658300081888833

Epoch: 6| Step: 5
Training loss: 3.175370931625366
Validation loss: 2.4625994082420104

Epoch: 6| Step: 6
Training loss: 2.243643045425415
Validation loss: 2.4539174033749487

Epoch: 6| Step: 7
Training loss: 2.1950600147247314
Validation loss: 2.444597316044633

Epoch: 6| Step: 8
Training loss: 2.9221088886260986
Validation loss: 2.4535426606414137

Epoch: 6| Step: 9
Training loss: 2.01934814453125
Validation loss: 2.4449416232365433

Epoch: 6| Step: 10
Training loss: 2.9633474349975586
Validation loss: 2.445481777191162

Epoch: 6| Step: 11
Training loss: 3.08821439743042
Validation loss: 2.4637321451658845

Epoch: 6| Step: 12
Training loss: 2.471615791320801
Validation loss: 2.4701641451927925

Epoch: 6| Step: 13
Training loss: 2.989732503890991
Validation loss: 2.468135328703029

Epoch: 207| Step: 0
Training loss: 2.0416293144226074
Validation loss: 2.4869722756006385

Epoch: 6| Step: 1
Training loss: 3.219749927520752
Validation loss: 2.501523833121023

Epoch: 6| Step: 2
Training loss: 2.661963939666748
Validation loss: 2.51131300516026

Epoch: 6| Step: 3
Training loss: 2.432382583618164
Validation loss: 2.510188910268968

Epoch: 6| Step: 4
Training loss: 3.3645195960998535
Validation loss: 2.5146038916803177

Epoch: 6| Step: 5
Training loss: 2.7288713455200195
Validation loss: 2.5069092217312066

Epoch: 6| Step: 6
Training loss: 2.237400531768799
Validation loss: 2.4833758723351265

Epoch: 6| Step: 7
Training loss: 2.0863733291625977
Validation loss: 2.4703870832279162

Epoch: 6| Step: 8
Training loss: 1.717414379119873
Validation loss: 2.454542789407956

Epoch: 6| Step: 9
Training loss: 3.22475528717041
Validation loss: 2.4450184401645454

Epoch: 6| Step: 10
Training loss: 2.7832021713256836
Validation loss: 2.4388826918858353

Epoch: 6| Step: 11
Training loss: 2.8733513355255127
Validation loss: 2.4289723621901644

Epoch: 6| Step: 12
Training loss: 2.7395668029785156
Validation loss: 2.435940040055142

Epoch: 6| Step: 13
Training loss: 3.303466320037842
Validation loss: 2.433285100485689

Epoch: 208| Step: 0
Training loss: 2.328498363494873
Validation loss: 2.430174228965595

Epoch: 6| Step: 1
Training loss: 2.248832941055298
Validation loss: 2.4328145519379647

Epoch: 6| Step: 2
Training loss: 2.881845474243164
Validation loss: 2.433574209931076

Epoch: 6| Step: 3
Training loss: 2.562894344329834
Validation loss: 2.4443926913763887

Epoch: 6| Step: 4
Training loss: 2.261106252670288
Validation loss: 2.445931432067707

Epoch: 6| Step: 5
Training loss: 2.698608875274658
Validation loss: 2.4493451131287443

Epoch: 6| Step: 6
Training loss: 2.9911794662475586
Validation loss: 2.4570181395417903

Epoch: 6| Step: 7
Training loss: 2.5291740894317627
Validation loss: 2.4460126405121176

Epoch: 6| Step: 8
Training loss: 2.522963523864746
Validation loss: 2.4348054675645727

Epoch: 6| Step: 9
Training loss: 2.9511666297912598
Validation loss: 2.4373450458690686

Epoch: 6| Step: 10
Training loss: 3.3917951583862305
Validation loss: 2.4329997954830045

Epoch: 6| Step: 11
Training loss: 2.1729345321655273
Validation loss: 2.4364396115785003

Epoch: 6| Step: 12
Training loss: 2.580183982849121
Validation loss: 2.437062730071365

Epoch: 6| Step: 13
Training loss: 3.0809288024902344
Validation loss: 2.4506002446656585

Epoch: 209| Step: 0
Training loss: 2.5140514373779297
Validation loss: 2.4533923261909076

Epoch: 6| Step: 1
Training loss: 2.388669967651367
Validation loss: 2.460554956108011

Epoch: 6| Step: 2
Training loss: 2.7382688522338867
Validation loss: 2.4575255481145715

Epoch: 6| Step: 3
Training loss: 3.015124797821045
Validation loss: 2.467086261318576

Epoch: 6| Step: 4
Training loss: 2.8916165828704834
Validation loss: 2.480975409989716

Epoch: 6| Step: 5
Training loss: 2.742363929748535
Validation loss: 2.477062955979378

Epoch: 6| Step: 6
Training loss: 3.0214805603027344
Validation loss: 2.484109042793192

Epoch: 6| Step: 7
Training loss: 2.1718387603759766
Validation loss: 2.4946649536009757

Epoch: 6| Step: 8
Training loss: 2.3621468544006348
Validation loss: 2.488193109471311

Epoch: 6| Step: 9
Training loss: 2.341231346130371
Validation loss: 2.4716642543833744

Epoch: 6| Step: 10
Training loss: 2.391234874725342
Validation loss: 2.4717845224565074

Epoch: 6| Step: 11
Training loss: 2.674241065979004
Validation loss: 2.4774803166748374

Epoch: 6| Step: 12
Training loss: 2.617739200592041
Validation loss: 2.473849250424293

Epoch: 6| Step: 13
Training loss: 3.1526055335998535
Validation loss: 2.4720501592082362

Epoch: 210| Step: 0
Training loss: 2.324962615966797
Validation loss: 2.4722235074607273

Epoch: 6| Step: 1
Training loss: 2.171722650527954
Validation loss: 2.473434966097596

Epoch: 6| Step: 2
Training loss: 2.8758816719055176
Validation loss: 2.4671761630683817

Epoch: 6| Step: 3
Training loss: 2.1525752544403076
Validation loss: 2.4757504001740487

Epoch: 6| Step: 4
Training loss: 2.0519466400146484
Validation loss: 2.470860073643346

Epoch: 6| Step: 5
Training loss: 3.40427565574646
Validation loss: 2.466068411386141

Epoch: 6| Step: 6
Training loss: 2.9388861656188965
Validation loss: 2.4643242564252628

Epoch: 6| Step: 7
Training loss: 2.4474735260009766
Validation loss: 2.469366837573308

Epoch: 6| Step: 8
Training loss: 1.3192323446273804
Validation loss: 2.472475867117605

Epoch: 6| Step: 9
Training loss: 2.7743091583251953
Validation loss: 2.465869249836091

Epoch: 6| Step: 10
Training loss: 2.6417133808135986
Validation loss: 2.4745300303223314

Epoch: 6| Step: 11
Training loss: 2.7744431495666504
Validation loss: 2.455526687765634

Epoch: 6| Step: 12
Training loss: 3.202942371368408
Validation loss: 2.45469940862348

Epoch: 6| Step: 13
Training loss: 4.0935444831848145
Validation loss: 2.4608168601989746

Epoch: 211| Step: 0
Training loss: 2.251603126525879
Validation loss: 2.463808241710868

Epoch: 6| Step: 1
Training loss: 2.6043548583984375
Validation loss: 2.4638330526249383

Epoch: 6| Step: 2
Training loss: 2.8583030700683594
Validation loss: 2.4731225890498005

Epoch: 6| Step: 3
Training loss: 2.0804967880249023
Validation loss: 2.471049185722105

Epoch: 6| Step: 4
Training loss: 2.616116523742676
Validation loss: 2.469211075895576

Epoch: 6| Step: 5
Training loss: 2.8433003425598145
Validation loss: 2.4733035102967293

Epoch: 6| Step: 6
Training loss: 2.4285597801208496
Validation loss: 2.4676283072399836

Epoch: 6| Step: 7
Training loss: 2.4096639156341553
Validation loss: 2.483087016690162

Epoch: 6| Step: 8
Training loss: 3.039030075073242
Validation loss: 2.4918233451022895

Epoch: 6| Step: 9
Training loss: 2.774104595184326
Validation loss: 2.495922662878549

Epoch: 6| Step: 10
Training loss: 2.439460277557373
Validation loss: 2.4771384962143435

Epoch: 6| Step: 11
Training loss: 2.547297239303589
Validation loss: 2.475893066775414

Epoch: 6| Step: 12
Training loss: 2.889972686767578
Validation loss: 2.4754802488511607

Epoch: 6| Step: 13
Training loss: 2.8782708644866943
Validation loss: 2.490511789116808

Epoch: 212| Step: 0
Training loss: 3.765303611755371
Validation loss: 2.504358935099776

Epoch: 6| Step: 1
Training loss: 3.0989878177642822
Validation loss: 2.5188891041663384

Epoch: 6| Step: 2
Training loss: 2.536991596221924
Validation loss: 2.5143601971287883

Epoch: 6| Step: 3
Training loss: 1.9162911176681519
Validation loss: 2.521931891800255

Epoch: 6| Step: 4
Training loss: 2.901275873184204
Validation loss: 2.5122225258940007

Epoch: 6| Step: 5
Training loss: 2.7389745712280273
Validation loss: 2.483593674116237

Epoch: 6| Step: 6
Training loss: 1.961432933807373
Validation loss: 2.4593913683327298

Epoch: 6| Step: 7
Training loss: 3.213762044906616
Validation loss: 2.4527620346315446

Epoch: 6| Step: 8
Training loss: 2.795013904571533
Validation loss: 2.439870321622459

Epoch: 6| Step: 9
Training loss: 2.836881160736084
Validation loss: 2.4447798062396306

Epoch: 6| Step: 10
Training loss: 2.4464924335479736
Validation loss: 2.443540667974821

Epoch: 6| Step: 11
Training loss: 2.009890079498291
Validation loss: 2.4439559059758342

Epoch: 6| Step: 12
Training loss: 2.2462430000305176
Validation loss: 2.4460760547268774

Epoch: 6| Step: 13
Training loss: 2.4212632179260254
Validation loss: 2.449875536785331

Epoch: 213| Step: 0
Training loss: 2.2843985557556152
Validation loss: 2.444716212570026

Epoch: 6| Step: 1
Training loss: 2.135758876800537
Validation loss: 2.4539607596653763

Epoch: 6| Step: 2
Training loss: 2.6672329902648926
Validation loss: 2.450260011098718

Epoch: 6| Step: 3
Training loss: 2.117922306060791
Validation loss: 2.454686634002193

Epoch: 6| Step: 4
Training loss: 2.927227020263672
Validation loss: 2.4556305011113486

Epoch: 6| Step: 5
Training loss: 3.2056872844696045
Validation loss: 2.4557850771052863

Epoch: 6| Step: 6
Training loss: 2.704392910003662
Validation loss: 2.4599752733784337

Epoch: 6| Step: 7
Training loss: 2.3830814361572266
Validation loss: 2.4699803834320395

Epoch: 6| Step: 8
Training loss: 2.616716146469116
Validation loss: 2.475552740917411

Epoch: 6| Step: 9
Training loss: 2.888510227203369
Validation loss: 2.4728004778585126

Epoch: 6| Step: 10
Training loss: 2.534179210662842
Validation loss: 2.4731464283440703

Epoch: 6| Step: 11
Training loss: 2.355247974395752
Validation loss: 2.4775452972740255

Epoch: 6| Step: 12
Training loss: 3.0485739707946777
Validation loss: 2.476879022454703

Epoch: 6| Step: 13
Training loss: 3.0798444747924805
Validation loss: 2.4770732977057017

Epoch: 214| Step: 0
Training loss: 2.7445571422576904
Validation loss: 2.468940160607779

Epoch: 6| Step: 1
Training loss: 2.592115640640259
Validation loss: 2.4745851716687604

Epoch: 6| Step: 2
Training loss: 2.621424913406372
Validation loss: 2.4647415735388316

Epoch: 6| Step: 3
Training loss: 2.641512870788574
Validation loss: 2.4755810204372612

Epoch: 6| Step: 4
Training loss: 2.810436487197876
Validation loss: 2.452364998479043

Epoch: 6| Step: 5
Training loss: 2.8187427520751953
Validation loss: 2.4636466887689408

Epoch: 6| Step: 6
Training loss: 3.6705522537231445
Validation loss: 2.4614050490881807

Epoch: 6| Step: 7
Training loss: 2.0836377143859863
Validation loss: 2.440975078972437

Epoch: 6| Step: 8
Training loss: 1.7674291133880615
Validation loss: 2.4380900782923542

Epoch: 6| Step: 9
Training loss: 2.62174391746521
Validation loss: 2.435627898862285

Epoch: 6| Step: 10
Training loss: 2.5148541927337646
Validation loss: 2.43879682530639

Epoch: 6| Step: 11
Training loss: 2.6368660926818848
Validation loss: 2.444084072625765

Epoch: 6| Step: 12
Training loss: 2.8752872943878174
Validation loss: 2.432888320697251

Epoch: 6| Step: 13
Training loss: 2.2321271896362305
Validation loss: 2.424651858627155

Epoch: 215| Step: 0
Training loss: 3.657649278640747
Validation loss: 2.4278205979254937

Epoch: 6| Step: 1
Training loss: 1.7296881675720215
Validation loss: 2.425665981026106

Epoch: 6| Step: 2
Training loss: 2.8082313537597656
Validation loss: 2.4236280302847586

Epoch: 6| Step: 3
Training loss: 1.9538156986236572
Validation loss: 2.4204845813012894

Epoch: 6| Step: 4
Training loss: 2.23593807220459
Validation loss: 2.420380127045416

Epoch: 6| Step: 5
Training loss: 3.211799383163452
Validation loss: 2.4220976932074434

Epoch: 6| Step: 6
Training loss: 2.8220648765563965
Validation loss: 2.426110008711456

Epoch: 6| Step: 7
Training loss: 1.9254260063171387
Validation loss: 2.431843034682735

Epoch: 6| Step: 8
Training loss: 2.8192434310913086
Validation loss: 2.4351135761507097

Epoch: 6| Step: 9
Training loss: 2.9460103511810303
Validation loss: 2.439395340540076

Epoch: 6| Step: 10
Training loss: 3.425373077392578
Validation loss: 2.455575942993164

Epoch: 6| Step: 11
Training loss: 2.211195945739746
Validation loss: 2.455834683551583

Epoch: 6| Step: 12
Training loss: 2.3413643836975098
Validation loss: 2.4593733510663434

Epoch: 6| Step: 13
Training loss: 2.676990032196045
Validation loss: 2.468525968572145

Epoch: 216| Step: 0
Training loss: 2.7121496200561523
Validation loss: 2.486339763928485

Epoch: 6| Step: 1
Training loss: 2.5636093616485596
Validation loss: 2.4737209479014077

Epoch: 6| Step: 2
Training loss: 3.16054105758667
Validation loss: 2.495235881497783

Epoch: 6| Step: 3
Training loss: 2.0894038677215576
Validation loss: 2.4899347033551944

Epoch: 6| Step: 4
Training loss: 2.378105401992798
Validation loss: 2.485468041512274

Epoch: 6| Step: 5
Training loss: 2.779120922088623
Validation loss: 2.47826494965502

Epoch: 6| Step: 6
Training loss: 2.3255114555358887
Validation loss: 2.4813385368675314

Epoch: 6| Step: 7
Training loss: 2.3476037979125977
Validation loss: 2.4782019892046527

Epoch: 6| Step: 8
Training loss: 2.8697025775909424
Validation loss: 2.4626923966151413

Epoch: 6| Step: 9
Training loss: 2.348820209503174
Validation loss: 2.4739861872888382

Epoch: 6| Step: 10
Training loss: 2.1812386512756348
Validation loss: 2.4657157928712907

Epoch: 6| Step: 11
Training loss: 2.240722417831421
Validation loss: 2.4860578531860025

Epoch: 6| Step: 12
Training loss: 4.279570579528809
Validation loss: 2.4780889582890335

Epoch: 6| Step: 13
Training loss: 2.318166971206665
Validation loss: 2.466386984753352

Epoch: 217| Step: 0
Training loss: 2.771813154220581
Validation loss: 2.4803230224117154

Epoch: 6| Step: 1
Training loss: 1.7455155849456787
Validation loss: 2.459449568102437

Epoch: 6| Step: 2
Training loss: 2.222766160964966
Validation loss: 2.4517717322995587

Epoch: 6| Step: 3
Training loss: 2.3732151985168457
Validation loss: 2.4671431292769728

Epoch: 6| Step: 4
Training loss: 2.4276676177978516
Validation loss: 2.462877640160181

Epoch: 6| Step: 5
Training loss: 2.3202524185180664
Validation loss: 2.4569445040918167

Epoch: 6| Step: 6
Training loss: 3.043790340423584
Validation loss: 2.4477860107216785

Epoch: 6| Step: 7
Training loss: 2.6260461807250977
Validation loss: 2.4451051681272444

Epoch: 6| Step: 8
Training loss: 2.397005558013916
Validation loss: 2.443268927194739

Epoch: 6| Step: 9
Training loss: 3.2625768184661865
Validation loss: 2.4457466269052155

Epoch: 6| Step: 10
Training loss: 2.2894275188446045
Validation loss: 2.4369975777082544

Epoch: 6| Step: 11
Training loss: 2.7073769569396973
Validation loss: 2.4447946112643004

Epoch: 6| Step: 12
Training loss: 3.264307975769043
Validation loss: 2.446982140182167

Epoch: 6| Step: 13
Training loss: 3.2802207469940186
Validation loss: 2.433896480068084

Epoch: 218| Step: 0
Training loss: 2.8235301971435547
Validation loss: 2.439176356920632

Epoch: 6| Step: 1
Training loss: 2.701740264892578
Validation loss: 2.4371482967048563

Epoch: 6| Step: 2
Training loss: 2.689784526824951
Validation loss: 2.455241495563138

Epoch: 6| Step: 3
Training loss: 3.0198071002960205
Validation loss: 2.456350918739073

Epoch: 6| Step: 4
Training loss: 2.3233180046081543
Validation loss: 2.4532145787310857

Epoch: 6| Step: 5
Training loss: 2.310142993927002
Validation loss: 2.4774421620112594

Epoch: 6| Step: 6
Training loss: 2.012721538543701
Validation loss: 2.463683469321138

Epoch: 6| Step: 7
Training loss: 3.0317533016204834
Validation loss: 2.4509883798578733

Epoch: 6| Step: 8
Training loss: 2.030165910720825
Validation loss: 2.441579596970671

Epoch: 6| Step: 9
Training loss: 3.2550830841064453
Validation loss: 2.440144033842189

Epoch: 6| Step: 10
Training loss: 3.030824661254883
Validation loss: 2.430155013197212

Epoch: 6| Step: 11
Training loss: 2.370373010635376
Validation loss: 2.431648995286675

Epoch: 6| Step: 12
Training loss: 3.152778387069702
Validation loss: 2.4448384136281986

Epoch: 6| Step: 13
Training loss: 1.002232551574707
Validation loss: 2.4436789046051683

Epoch: 219| Step: 0
Training loss: 2.2152678966522217
Validation loss: 2.447018018332861

Epoch: 6| Step: 1
Training loss: 1.857038140296936
Validation loss: 2.4588339995312434

Epoch: 6| Step: 2
Training loss: 3.1849398612976074
Validation loss: 2.4777066297428583

Epoch: 6| Step: 3
Training loss: 2.0066781044006348
Validation loss: 2.4760172790096653

Epoch: 6| Step: 4
Training loss: 2.7030744552612305
Validation loss: 2.476243745896124

Epoch: 6| Step: 5
Training loss: 2.8754680156707764
Validation loss: 2.48699959119161

Epoch: 6| Step: 6
Training loss: 2.5015344619750977
Validation loss: 2.4823072084816555

Epoch: 6| Step: 7
Training loss: 2.445615291595459
Validation loss: 2.4823184705549672

Epoch: 6| Step: 8
Training loss: 3.2030458450317383
Validation loss: 2.4847529754843762

Epoch: 6| Step: 9
Training loss: 2.6563103199005127
Validation loss: 2.479760367383239

Epoch: 6| Step: 10
Training loss: 3.275519847869873
Validation loss: 2.4609105176823114

Epoch: 6| Step: 11
Training loss: 2.9436540603637695
Validation loss: 2.454752006838399

Epoch: 6| Step: 12
Training loss: 2.6606857776641846
Validation loss: 2.4484629451587634

Epoch: 6| Step: 13
Training loss: 1.407369613647461
Validation loss: 2.4482102471013225

Epoch: 220| Step: 0
Training loss: 2.4245972633361816
Validation loss: 2.4462175933263635

Epoch: 6| Step: 1
Training loss: 2.632375717163086
Validation loss: 2.4408644142971245

Epoch: 6| Step: 2
Training loss: 3.083333730697632
Validation loss: 2.450855347418016

Epoch: 6| Step: 3
Training loss: 2.523981809616089
Validation loss: 2.442208415718489

Epoch: 6| Step: 4
Training loss: 2.7791054248809814
Validation loss: 2.4495094437753

Epoch: 6| Step: 5
Training loss: 2.2040445804595947
Validation loss: 2.440585667087186

Epoch: 6| Step: 6
Training loss: 1.9640294313430786
Validation loss: 2.4416291008713427

Epoch: 6| Step: 7
Training loss: 2.5147857666015625
Validation loss: 2.444159748733685

Epoch: 6| Step: 8
Training loss: 1.912498950958252
Validation loss: 2.4423981302527973

Epoch: 6| Step: 9
Training loss: 2.4501020908355713
Validation loss: 2.437020145436769

Epoch: 6| Step: 10
Training loss: 3.106285333633423
Validation loss: 2.4438301106934905

Epoch: 6| Step: 11
Training loss: 3.0044407844543457
Validation loss: 2.4410041352753997

Epoch: 6| Step: 12
Training loss: 3.3326103687286377
Validation loss: 2.440277212409563

Epoch: 6| Step: 13
Training loss: 2.5565779209136963
Validation loss: 2.4387132070397817

Epoch: 221| Step: 0
Training loss: 1.401247501373291
Validation loss: 2.444028854370117

Epoch: 6| Step: 1
Training loss: 3.3507633209228516
Validation loss: 2.446489585343228

Epoch: 6| Step: 2
Training loss: 2.8338403701782227
Validation loss: 2.4392154639767063

Epoch: 6| Step: 3
Training loss: 3.0567474365234375
Validation loss: 2.4498588705575592

Epoch: 6| Step: 4
Training loss: 2.605311393737793
Validation loss: 2.465795632331602

Epoch: 6| Step: 5
Training loss: 3.137169361114502
Validation loss: 2.458148676861999

Epoch: 6| Step: 6
Training loss: 2.242753744125366
Validation loss: 2.447662491952219

Epoch: 6| Step: 7
Training loss: 3.1159627437591553
Validation loss: 2.454931943647323

Epoch: 6| Step: 8
Training loss: 2.5902295112609863
Validation loss: 2.46555337854611

Epoch: 6| Step: 9
Training loss: 1.863960862159729
Validation loss: 2.450716016113117

Epoch: 6| Step: 10
Training loss: 2.326584815979004
Validation loss: 2.467287140507852

Epoch: 6| Step: 11
Training loss: 2.2231714725494385
Validation loss: 2.4585611474129463

Epoch: 6| Step: 12
Training loss: 2.833977222442627
Validation loss: 2.4429033187127884

Epoch: 6| Step: 13
Training loss: 3.0712270736694336
Validation loss: 2.4409232242133028

Epoch: 222| Step: 0
Training loss: 2.1445493698120117
Validation loss: 2.442869737584104

Epoch: 6| Step: 1
Training loss: 2.7605831623077393
Validation loss: 2.4296187136762883

Epoch: 6| Step: 2
Training loss: 2.8641467094421387
Validation loss: 2.4239108729106125

Epoch: 6| Step: 3
Training loss: 2.362868309020996
Validation loss: 2.4304852024201424

Epoch: 6| Step: 4
Training loss: 2.4058048725128174
Validation loss: 2.4262319841692523

Epoch: 6| Step: 5
Training loss: 2.253091335296631
Validation loss: 2.4223116585003432

Epoch: 6| Step: 6
Training loss: 2.8027143478393555
Validation loss: 2.4211907771325882

Epoch: 6| Step: 7
Training loss: 2.5537853240966797
Validation loss: 2.4232916626878964

Epoch: 6| Step: 8
Training loss: 2.9394168853759766
Validation loss: 2.425515777321272

Epoch: 6| Step: 9
Training loss: 3.006808042526245
Validation loss: 2.4289498354799006

Epoch: 6| Step: 10
Training loss: 2.3315188884735107
Validation loss: 2.4282213705842213

Epoch: 6| Step: 11
Training loss: 3.0746874809265137
Validation loss: 2.4423463908574914

Epoch: 6| Step: 12
Training loss: 2.434379816055298
Validation loss: 2.4417871787983882

Epoch: 6| Step: 13
Training loss: 2.414543628692627
Validation loss: 2.441280475226782

Epoch: 223| Step: 0
Training loss: 3.348390579223633
Validation loss: 2.4407751611483994

Epoch: 6| Step: 1
Training loss: 2.5438425540924072
Validation loss: 2.4379697153645177

Epoch: 6| Step: 2
Training loss: 2.7241370677948
Validation loss: 2.440429118371779

Epoch: 6| Step: 3
Training loss: 2.313939094543457
Validation loss: 2.447417038743214

Epoch: 6| Step: 4
Training loss: 3.159740447998047
Validation loss: 2.4412075114506546

Epoch: 6| Step: 5
Training loss: 2.5829317569732666
Validation loss: 2.45834667195556

Epoch: 6| Step: 6
Training loss: 2.895007610321045
Validation loss: 2.451311247323149

Epoch: 6| Step: 7
Training loss: 2.7564096450805664
Validation loss: 2.468652932874618

Epoch: 6| Step: 8
Training loss: 2.5314528942108154
Validation loss: 2.4755356965526456

Epoch: 6| Step: 9
Training loss: 2.4009876251220703
Validation loss: 2.46403302556725

Epoch: 6| Step: 10
Training loss: 2.552650213241577
Validation loss: 2.45717893620973

Epoch: 6| Step: 11
Training loss: 3.0194003582000732
Validation loss: 2.4743978105565554

Epoch: 6| Step: 12
Training loss: 1.3861685991287231
Validation loss: 2.4541564192823184

Epoch: 6| Step: 13
Training loss: 1.7366681098937988
Validation loss: 2.4522214833126275

Epoch: 224| Step: 0
Training loss: 2.2041332721710205
Validation loss: 2.4533730758133756

Epoch: 6| Step: 1
Training loss: 2.5689492225646973
Validation loss: 2.4653565242726314

Epoch: 6| Step: 2
Training loss: 3.158611297607422
Validation loss: 2.46317381499916

Epoch: 6| Step: 3
Training loss: 2.4146199226379395
Validation loss: 2.456839449944035

Epoch: 6| Step: 4
Training loss: 3.1364731788635254
Validation loss: 2.44875782023194

Epoch: 6| Step: 5
Training loss: 2.3486132621765137
Validation loss: 2.437471605116321

Epoch: 6| Step: 6
Training loss: 2.749068260192871
Validation loss: 2.4358458929164435

Epoch: 6| Step: 7
Training loss: 2.44100022315979
Validation loss: 2.436094707058322

Epoch: 6| Step: 8
Training loss: 1.9775924682617188
Validation loss: 2.4340159431580575

Epoch: 6| Step: 9
Training loss: 2.4539456367492676
Validation loss: 2.4317066772009737

Epoch: 6| Step: 10
Training loss: 2.5297365188598633
Validation loss: 2.429728010649322

Epoch: 6| Step: 11
Training loss: 2.841447591781616
Validation loss: 2.4325290520985923

Epoch: 6| Step: 12
Training loss: 3.0871710777282715
Validation loss: 2.439476316975009

Epoch: 6| Step: 13
Training loss: 2.4643640518188477
Validation loss: 2.4415101082094255

Epoch: 225| Step: 0
Training loss: 2.6732654571533203
Validation loss: 2.4421913008536063

Epoch: 6| Step: 1
Training loss: 3.409822463989258
Validation loss: 2.4467678762251333

Epoch: 6| Step: 2
Training loss: 3.3831238746643066
Validation loss: 2.4438486509425665

Epoch: 6| Step: 3
Training loss: 2.588593006134033
Validation loss: 2.463195755917539

Epoch: 6| Step: 4
Training loss: 2.594784736633301
Validation loss: 2.446974115986978

Epoch: 6| Step: 5
Training loss: 1.8565127849578857
Validation loss: 2.441889347568635

Epoch: 6| Step: 6
Training loss: 2.9789702892303467
Validation loss: 2.4531669129607496

Epoch: 6| Step: 7
Training loss: 1.8838963508605957
Validation loss: 2.4429476286775325

Epoch: 6| Step: 8
Training loss: 2.354715347290039
Validation loss: 2.4454829526203934

Epoch: 6| Step: 9
Training loss: 2.3646113872528076
Validation loss: 2.4445155307810795

Epoch: 6| Step: 10
Training loss: 2.8331801891326904
Validation loss: 2.451330748937463

Epoch: 6| Step: 11
Training loss: 2.9799609184265137
Validation loss: 2.446946305613364

Epoch: 6| Step: 12
Training loss: 2.3001928329467773
Validation loss: 2.438036916076496

Epoch: 6| Step: 13
Training loss: 1.81764554977417
Validation loss: 2.426922454628893

Epoch: 226| Step: 0
Training loss: 2.432772159576416
Validation loss: 2.4324713830024964

Epoch: 6| Step: 1
Training loss: 3.045419692993164
Validation loss: 2.43606242569544

Epoch: 6| Step: 2
Training loss: 3.1528491973876953
Validation loss: 2.4456966692401516

Epoch: 6| Step: 3
Training loss: 2.592989444732666
Validation loss: 2.4263255955070577

Epoch: 6| Step: 4
Training loss: 2.694082260131836
Validation loss: 2.4384645723527476

Epoch: 6| Step: 5
Training loss: 2.7842156887054443
Validation loss: 2.431581179300944

Epoch: 6| Step: 6
Training loss: 3.822502613067627
Validation loss: 2.437383267187303

Epoch: 6| Step: 7
Training loss: 1.582076072692871
Validation loss: 2.445598404894593

Epoch: 6| Step: 8
Training loss: 2.3741135597229004
Validation loss: 2.466462624970303

Epoch: 6| Step: 9
Training loss: 2.5832931995391846
Validation loss: 2.464186132595103

Epoch: 6| Step: 10
Training loss: 2.086226463317871
Validation loss: 2.473212621545279

Epoch: 6| Step: 11
Training loss: 1.9192107915878296
Validation loss: 2.499843346175327

Epoch: 6| Step: 12
Training loss: 3.0501785278320312
Validation loss: 2.492046115218952

Epoch: 6| Step: 13
Training loss: 1.9431558847427368
Validation loss: 2.5178424619859263

Epoch: 227| Step: 0
Training loss: 3.1810836791992188
Validation loss: 2.518810923381518

Epoch: 6| Step: 1
Training loss: 3.258878231048584
Validation loss: 2.4968424022838636

Epoch: 6| Step: 2
Training loss: 2.4096148014068604
Validation loss: 2.489006439844767

Epoch: 6| Step: 3
Training loss: 2.6753814220428467
Validation loss: 2.465699980335851

Epoch: 6| Step: 4
Training loss: 2.4965591430664062
Validation loss: 2.4507347601716236

Epoch: 6| Step: 5
Training loss: 1.6575387716293335
Validation loss: 2.436853144758491

Epoch: 6| Step: 6
Training loss: 1.816468596458435
Validation loss: 2.4419052523951374

Epoch: 6| Step: 7
Training loss: 2.294725179672241
Validation loss: 2.444347168809624

Epoch: 6| Step: 8
Training loss: 2.4856433868408203
Validation loss: 2.4349613266606487

Epoch: 6| Step: 9
Training loss: 2.252793550491333
Validation loss: 2.4346725530521844

Epoch: 6| Step: 10
Training loss: 3.892037868499756
Validation loss: 2.428846936072073

Epoch: 6| Step: 11
Training loss: 2.7482717037200928
Validation loss: 2.431688198479273

Epoch: 6| Step: 12
Training loss: 2.2959256172180176
Validation loss: 2.424404090450656

Epoch: 6| Step: 13
Training loss: 3.318589925765991
Validation loss: 2.4231753682577484

Epoch: 228| Step: 0
Training loss: 2.896892547607422
Validation loss: 2.428391638622489

Epoch: 6| Step: 1
Training loss: 2.2257213592529297
Validation loss: 2.422880013783773

Epoch: 6| Step: 2
Training loss: 3.0570082664489746
Validation loss: 2.4288952837708178

Epoch: 6| Step: 3
Training loss: 2.130204677581787
Validation loss: 2.42765107206119

Epoch: 6| Step: 4
Training loss: 2.931755542755127
Validation loss: 2.418003479639689

Epoch: 6| Step: 5
Training loss: 2.440312623977661
Validation loss: 2.421786485179778

Epoch: 6| Step: 6
Training loss: 3.132187604904175
Validation loss: 2.4142141495981524

Epoch: 6| Step: 7
Training loss: 2.3639936447143555
Validation loss: 2.4223621942663707

Epoch: 6| Step: 8
Training loss: 3.3158485889434814
Validation loss: 2.436345047848199

Epoch: 6| Step: 9
Training loss: 2.893627166748047
Validation loss: 2.4321798137439194

Epoch: 6| Step: 10
Training loss: 1.5191932916641235
Validation loss: 2.4229552002363306

Epoch: 6| Step: 11
Training loss: 2.5085363388061523
Validation loss: 2.4242886292037142

Epoch: 6| Step: 12
Training loss: 2.367253303527832
Validation loss: 2.439609748060985

Epoch: 6| Step: 13
Training loss: 2.459620475769043
Validation loss: 2.433091157226152

Epoch: 229| Step: 0
Training loss: 2.565168619155884
Validation loss: 2.441212997641615

Epoch: 6| Step: 1
Training loss: 2.7961034774780273
Validation loss: 2.4357301163417038

Epoch: 6| Step: 2
Training loss: 2.826347589492798
Validation loss: 2.42618122664831

Epoch: 6| Step: 3
Training loss: 2.579641819000244
Validation loss: 2.4315431605103197

Epoch: 6| Step: 4
Training loss: 2.310227155685425
Validation loss: 2.42643488607099

Epoch: 6| Step: 5
Training loss: 2.4785547256469727
Validation loss: 2.422515833249656

Epoch: 6| Step: 6
Training loss: 2.0165040493011475
Validation loss: 2.4309022195877565

Epoch: 6| Step: 7
Training loss: 2.0468060970306396
Validation loss: 2.4236522797615296

Epoch: 6| Step: 8
Training loss: 3.205549716949463
Validation loss: 2.4277891728185836

Epoch: 6| Step: 9
Training loss: 2.2641847133636475
Validation loss: 2.439112183868244

Epoch: 6| Step: 10
Training loss: 3.092113733291626
Validation loss: 2.434096823456467

Epoch: 6| Step: 11
Training loss: 2.576784133911133
Validation loss: 2.434857383851082

Epoch: 6| Step: 12
Training loss: 2.7440924644470215
Validation loss: 2.445385338157736

Epoch: 6| Step: 13
Training loss: 2.7960948944091797
Validation loss: 2.452025700640935

Epoch: 230| Step: 0
Training loss: 3.404036283493042
Validation loss: 2.465689548882105

Epoch: 6| Step: 1
Training loss: 1.8371779918670654
Validation loss: 2.4731292929700626

Epoch: 6| Step: 2
Training loss: 2.2692015171051025
Validation loss: 2.4629053838791384

Epoch: 6| Step: 3
Training loss: 2.8656842708587646
Validation loss: 2.468607635908229

Epoch: 6| Step: 4
Training loss: 2.9059581756591797
Validation loss: 2.458472046800839

Epoch: 6| Step: 5
Training loss: 3.5352747440338135
Validation loss: 2.4468602339426675

Epoch: 6| Step: 6
Training loss: 3.091907024383545
Validation loss: 2.445875665192963

Epoch: 6| Step: 7
Training loss: 2.1728529930114746
Validation loss: 2.4408671548289638

Epoch: 6| Step: 8
Training loss: 1.8674651384353638
Validation loss: 2.431912704180646

Epoch: 6| Step: 9
Training loss: 3.013293504714966
Validation loss: 2.434299715103642

Epoch: 6| Step: 10
Training loss: 2.268881320953369
Validation loss: 2.4325378018040813

Epoch: 6| Step: 11
Training loss: 2.09599232673645
Validation loss: 2.4301367088030745

Epoch: 6| Step: 12
Training loss: 2.235459327697754
Validation loss: 2.4352412736544045

Epoch: 6| Step: 13
Training loss: 2.750631809234619
Validation loss: 2.4630512140130483

Epoch: 231| Step: 0
Training loss: 2.2185215950012207
Validation loss: 2.4673017378776305

Epoch: 6| Step: 1
Training loss: 2.57444429397583
Validation loss: 2.462924800893312

Epoch: 6| Step: 2
Training loss: 2.5654850006103516
Validation loss: 2.4764101402733916

Epoch: 6| Step: 3
Training loss: 2.5029265880584717
Validation loss: 2.4657605143003565

Epoch: 6| Step: 4
Training loss: 2.2336525917053223
Validation loss: 2.4663156924709195

Epoch: 6| Step: 5
Training loss: 3.076590061187744
Validation loss: 2.4521102854000625

Epoch: 6| Step: 6
Training loss: 2.6384780406951904
Validation loss: 2.4580714753879014

Epoch: 6| Step: 7
Training loss: 2.201796531677246
Validation loss: 2.458010568413683

Epoch: 6| Step: 8
Training loss: 3.004476547241211
Validation loss: 2.4437462206809752

Epoch: 6| Step: 9
Training loss: 2.8194661140441895
Validation loss: 2.4280210028412523

Epoch: 6| Step: 10
Training loss: 2.8467283248901367
Validation loss: 2.4254530783622497

Epoch: 6| Step: 11
Training loss: 2.4842000007629395
Validation loss: 2.4310248821012435

Epoch: 6| Step: 12
Training loss: 2.7416882514953613
Validation loss: 2.4161566790714057

Epoch: 6| Step: 13
Training loss: 2.3478493690490723
Validation loss: 2.417237648399927

Epoch: 232| Step: 0
Training loss: 2.6980161666870117
Validation loss: 2.4225837927992626

Epoch: 6| Step: 1
Training loss: 2.381330966949463
Validation loss: 2.42013652606677

Epoch: 6| Step: 2
Training loss: 2.477025032043457
Validation loss: 2.4386451269990657

Epoch: 6| Step: 3
Training loss: 2.401752233505249
Validation loss: 2.445167226176108

Epoch: 6| Step: 4
Training loss: 3.141803026199341
Validation loss: 2.425950704082366

Epoch: 6| Step: 5
Training loss: 2.803934097290039
Validation loss: 2.436086698244977

Epoch: 6| Step: 6
Training loss: 2.4456090927124023
Validation loss: 2.4462490030514297

Epoch: 6| Step: 7
Training loss: 2.3133018016815186
Validation loss: 2.4351035420612623

Epoch: 6| Step: 8
Training loss: 2.298733711242676
Validation loss: 2.440164112275647

Epoch: 6| Step: 9
Training loss: 2.6380960941314697
Validation loss: 2.4330765867746003

Epoch: 6| Step: 10
Training loss: 2.3667588233947754
Validation loss: 2.4346044448114212

Epoch: 6| Step: 11
Training loss: 3.2311110496520996
Validation loss: 2.451957997455392

Epoch: 6| Step: 12
Training loss: 2.7254035472869873
Validation loss: 2.4561513521338023

Epoch: 6| Step: 13
Training loss: 1.7965706586837769
Validation loss: 2.4657626434039046

Epoch: 233| Step: 0
Training loss: 2.3360722064971924
Validation loss: 2.4665115853791595

Epoch: 6| Step: 1
Training loss: 2.9578607082366943
Validation loss: 2.465960887170607

Epoch: 6| Step: 2
Training loss: 2.2401516437530518
Validation loss: 2.466737921519946

Epoch: 6| Step: 3
Training loss: 2.365048885345459
Validation loss: 2.475841247907249

Epoch: 6| Step: 4
Training loss: 2.2760791778564453
Validation loss: 2.461850468830396

Epoch: 6| Step: 5
Training loss: 2.271500587463379
Validation loss: 2.4613318930390062

Epoch: 6| Step: 6
Training loss: 1.7604767084121704
Validation loss: 2.462453116652786

Epoch: 6| Step: 7
Training loss: 2.7476961612701416
Validation loss: 2.450023120449435

Epoch: 6| Step: 8
Training loss: 2.5305979251861572
Validation loss: 2.4394888570231776

Epoch: 6| Step: 9
Training loss: 3.6715574264526367
Validation loss: 2.434901137505808

Epoch: 6| Step: 10
Training loss: 3.0223944187164307
Validation loss: 2.4281752955529

Epoch: 6| Step: 11
Training loss: 2.952810287475586
Validation loss: 2.4273336138776553

Epoch: 6| Step: 12
Training loss: 2.351588010787964
Validation loss: 2.431670570886263

Epoch: 6| Step: 13
Training loss: 2.79156231880188
Validation loss: 2.435175049689508

Epoch: 234| Step: 0
Training loss: 2.9587907791137695
Validation loss: 2.436399475220711

Epoch: 6| Step: 1
Training loss: 2.2060489654541016
Validation loss: 2.438441571368966

Epoch: 6| Step: 2
Training loss: 1.8720089197158813
Validation loss: 2.4445388137653308

Epoch: 6| Step: 3
Training loss: 2.4680488109588623
Validation loss: 2.4657286110744683

Epoch: 6| Step: 4
Training loss: 2.658041477203369
Validation loss: 2.4684780874559955

Epoch: 6| Step: 5
Training loss: 2.8913965225219727
Validation loss: 2.4804321655663113

Epoch: 6| Step: 6
Training loss: 2.99796986579895
Validation loss: 2.473068198850078

Epoch: 6| Step: 7
Training loss: 2.353790283203125
Validation loss: 2.4815094906796693

Epoch: 6| Step: 8
Training loss: 3.178088426589966
Validation loss: 2.4720192904113443

Epoch: 6| Step: 9
Training loss: 2.79658579826355
Validation loss: 2.4652466312531502

Epoch: 6| Step: 10
Training loss: 1.7625503540039062
Validation loss: 2.453700206613028

Epoch: 6| Step: 11
Training loss: 3.1281232833862305
Validation loss: 2.4512609333120365

Epoch: 6| Step: 12
Training loss: 2.7447760105133057
Validation loss: 2.4463337185562297

Epoch: 6| Step: 13
Training loss: 1.8106863498687744
Validation loss: 2.4301736406100694

Epoch: 235| Step: 0
Training loss: 2.867699146270752
Validation loss: 2.436987566691573

Epoch: 6| Step: 1
Training loss: 2.844257354736328
Validation loss: 2.4407074912901847

Epoch: 6| Step: 2
Training loss: 2.047281265258789
Validation loss: 2.4427174829667613

Epoch: 6| Step: 3
Training loss: 3.3162808418273926
Validation loss: 2.4526997356004614

Epoch: 6| Step: 4
Training loss: 3.2817134857177734
Validation loss: 2.451303558964883

Epoch: 6| Step: 5
Training loss: 2.5337045192718506
Validation loss: 2.455379045137795

Epoch: 6| Step: 6
Training loss: 2.5232150554656982
Validation loss: 2.455100938838015

Epoch: 6| Step: 7
Training loss: 2.6358017921447754
Validation loss: 2.4453689359849498

Epoch: 6| Step: 8
Training loss: 2.628349781036377
Validation loss: 2.4825784519154537

Epoch: 6| Step: 9
Training loss: 2.193922519683838
Validation loss: 2.532263289215744

Epoch: 6| Step: 10
Training loss: 1.6011056900024414
Validation loss: 2.572267596439649

Epoch: 6| Step: 11
Training loss: 2.609997510910034
Validation loss: 2.5851329552230013

Epoch: 6| Step: 12
Training loss: 2.8672263622283936
Validation loss: 2.583065471341533

Epoch: 6| Step: 13
Training loss: 3.1305510997772217
Validation loss: 2.588121860258041

Epoch: 236| Step: 0
Training loss: 2.726243734359741
Validation loss: 2.588787442894392

Epoch: 6| Step: 1
Training loss: 2.082005500793457
Validation loss: 2.583703943478164

Epoch: 6| Step: 2
Training loss: 3.1848154067993164
Validation loss: 2.5919262439973894

Epoch: 6| Step: 3
Training loss: 2.459730625152588
Validation loss: 2.5861608033539145

Epoch: 6| Step: 4
Training loss: 3.180649995803833
Validation loss: 2.5815155557406846

Epoch: 6| Step: 5
Training loss: 2.759293556213379
Validation loss: 2.5803440155521518

Epoch: 6| Step: 6
Training loss: 2.519731283187866
Validation loss: 2.566465154770882

Epoch: 6| Step: 7
Training loss: 2.769819736480713
Validation loss: 2.568617938667215

Epoch: 6| Step: 8
Training loss: 2.084838628768921
Validation loss: 2.5541552420585387

Epoch: 6| Step: 9
Training loss: 3.0800235271453857
Validation loss: 2.543797900599818

Epoch: 6| Step: 10
Training loss: 2.3400683403015137
Validation loss: 2.547674958423902

Epoch: 6| Step: 11
Training loss: 3.3056015968322754
Validation loss: 2.5492769954025105

Epoch: 6| Step: 12
Training loss: 2.8342411518096924
Validation loss: 2.536653341785554

Epoch: 6| Step: 13
Training loss: 1.4231369495391846
Validation loss: 2.538395368924705

Epoch: 237| Step: 0
Training loss: 2.61128830909729
Validation loss: 2.534324058922388

Epoch: 6| Step: 1
Training loss: 2.1185929775238037
Validation loss: 2.5349554605381464

Epoch: 6| Step: 2
Training loss: 3.089653253555298
Validation loss: 2.5386036365262923

Epoch: 6| Step: 3
Training loss: 3.451968193054199
Validation loss: 2.5436433822877946

Epoch: 6| Step: 4
Training loss: 2.925236225128174
Validation loss: 2.548705424031904

Epoch: 6| Step: 5
Training loss: 2.9557790756225586
Validation loss: 2.559420783032653

Epoch: 6| Step: 6
Training loss: 3.29349946975708
Validation loss: 2.5564661884820588

Epoch: 6| Step: 7
Training loss: 2.169508934020996
Validation loss: 2.5539346177090883

Epoch: 6| Step: 8
Training loss: 1.6738961935043335
Validation loss: 2.554474215353689

Epoch: 6| Step: 9
Training loss: 2.2536544799804688
Validation loss: 2.5536811479958157

Epoch: 6| Step: 10
Training loss: 2.974175214767456
Validation loss: 2.5460528712118826

Epoch: 6| Step: 11
Training loss: 2.163210153579712
Validation loss: 2.556231749955044

Epoch: 6| Step: 12
Training loss: 3.2046289443969727
Validation loss: 2.563105190953901

Epoch: 6| Step: 13
Training loss: 2.131401300430298
Validation loss: 2.573097221312984

Epoch: 238| Step: 0
Training loss: 2.7147631645202637
Validation loss: 2.5553492987027733

Epoch: 6| Step: 1
Training loss: 3.0979530811309814
Validation loss: 2.5520722917331162

Epoch: 6| Step: 2
Training loss: 3.074697256088257
Validation loss: 2.5595324577823764

Epoch: 6| Step: 3
Training loss: 2.7558817863464355
Validation loss: 2.5600059750259563

Epoch: 6| Step: 4
Training loss: 1.7572628259658813
Validation loss: 2.5680214128186627

Epoch: 6| Step: 5
Training loss: 2.718404769897461
Validation loss: 2.5551183454452024

Epoch: 6| Step: 6
Training loss: 2.859971761703491
Validation loss: 2.549172342464488

Epoch: 6| Step: 7
Training loss: 2.112499713897705
Validation loss: 2.5602795077908422

Epoch: 6| Step: 8
Training loss: 2.880171060562134
Validation loss: 2.551473461171632

Epoch: 6| Step: 9
Training loss: 2.5666515827178955
Validation loss: 2.542847156524658

Epoch: 6| Step: 10
Training loss: 2.3445322513580322
Validation loss: 2.542275159589706

Epoch: 6| Step: 11
Training loss: 2.2569520473480225
Validation loss: 2.5539957195199947

Epoch: 6| Step: 12
Training loss: 3.1275901794433594
Validation loss: 2.5453942873144664

Epoch: 6| Step: 13
Training loss: 3.01025128364563
Validation loss: 2.5383704452104467

Epoch: 239| Step: 0
Training loss: 2.001131296157837
Validation loss: 2.5411801876560336

Epoch: 6| Step: 1
Training loss: 2.78285551071167
Validation loss: 2.5406596788796048

Epoch: 6| Step: 2
Training loss: 2.8986265659332275
Validation loss: 2.541874042121313

Epoch: 6| Step: 3
Training loss: 2.9122469425201416
Validation loss: 2.554074207941691

Epoch: 6| Step: 4
Training loss: 3.14487886428833
Validation loss: 2.5436830136083786

Epoch: 6| Step: 5
Training loss: 2.50441312789917
Validation loss: 2.553288149577315

Epoch: 6| Step: 6
Training loss: 2.626265525817871
Validation loss: 2.5552754402160645

Epoch: 6| Step: 7
Training loss: 2.92842173576355
Validation loss: 2.56911071654289

Epoch: 6| Step: 8
Training loss: 2.459256649017334
Validation loss: 2.5620577335357666

Epoch: 6| Step: 9
Training loss: 3.0675854682922363
Validation loss: 2.5560343496261106

Epoch: 6| Step: 10
Training loss: 1.5153694152832031
Validation loss: 2.5669081236726496

Epoch: 6| Step: 11
Training loss: 2.8016114234924316
Validation loss: 2.551733063113305

Epoch: 6| Step: 12
Training loss: 3.1511106491088867
Validation loss: 2.5509144388219362

Epoch: 6| Step: 13
Training loss: 2.2042243480682373
Validation loss: 2.5430361455486667

Epoch: 240| Step: 0
Training loss: 2.405574321746826
Validation loss: 2.5395902151702554

Epoch: 6| Step: 1
Training loss: 3.6373705863952637
Validation loss: 2.5331646216812955

Epoch: 6| Step: 2
Training loss: 3.2742342948913574
Validation loss: 2.531192341158467

Epoch: 6| Step: 3
Training loss: 2.1948537826538086
Validation loss: 2.532588833121843

Epoch: 6| Step: 4
Training loss: 2.9827589988708496
Validation loss: 2.5294956135493454

Epoch: 6| Step: 5
Training loss: 1.5226469039916992
Validation loss: 2.5238798485007337

Epoch: 6| Step: 6
Training loss: 2.4878180027008057
Validation loss: 2.528341995772495

Epoch: 6| Step: 7
Training loss: 2.2703843116760254
Validation loss: 2.5319691370892268

Epoch: 6| Step: 8
Training loss: 2.470686435699463
Validation loss: 2.5371477168093444

Epoch: 6| Step: 9
Training loss: 2.9057648181915283
Validation loss: 2.537515922259259

Epoch: 6| Step: 10
Training loss: 2.782121419906616
Validation loss: 2.5471868181741364

Epoch: 6| Step: 11
Training loss: 3.1014914512634277
Validation loss: 2.5440730843492734

Epoch: 6| Step: 12
Training loss: 2.8615190982818604
Validation loss: 2.5491025165844987

Epoch: 6| Step: 13
Training loss: 2.220700263977051
Validation loss: 2.5506174461815947

Epoch: 241| Step: 0
Training loss: 2.6898937225341797
Validation loss: 2.5563642183939614

Epoch: 6| Step: 1
Training loss: 2.886559009552002
Validation loss: 2.5482778754285587

Epoch: 6| Step: 2
Training loss: 2.4774749279022217
Validation loss: 2.5422052696187007

Epoch: 6| Step: 3
Training loss: 2.459056854248047
Validation loss: 2.5457310471483456

Epoch: 6| Step: 4
Training loss: 2.452836513519287
Validation loss: 2.5429983113401677

Epoch: 6| Step: 5
Training loss: 3.8706700801849365
Validation loss: 2.5388969580332437

Epoch: 6| Step: 6
Training loss: 1.617077350616455
Validation loss: 2.5288818933630504

Epoch: 6| Step: 7
Training loss: 3.149155378341675
Validation loss: 2.5296094084298737

Epoch: 6| Step: 8
Training loss: 2.7679834365844727
Validation loss: 2.5280154097464775

Epoch: 6| Step: 9
Training loss: 3.098665475845337
Validation loss: 2.524548284469112

Epoch: 6| Step: 10
Training loss: 1.921542763710022
Validation loss: 2.5241127039796565

Epoch: 6| Step: 11
Training loss: 2.4196383953094482
Validation loss: 2.519250085276942

Epoch: 6| Step: 12
Training loss: 3.095320224761963
Validation loss: 2.523171529974989

Epoch: 6| Step: 13
Training loss: 1.8441096544265747
Validation loss: 2.527853847831808

Epoch: 242| Step: 0
Training loss: 1.926934003829956
Validation loss: 2.534928027019706

Epoch: 6| Step: 1
Training loss: 3.104661464691162
Validation loss: 2.525569264606763

Epoch: 6| Step: 2
Training loss: 2.3890492916107178
Validation loss: 2.5353757642930552

Epoch: 6| Step: 3
Training loss: 2.666320562362671
Validation loss: 2.5407391081574144

Epoch: 6| Step: 4
Training loss: 2.5490925312042236
Validation loss: 2.545601455114221

Epoch: 6| Step: 5
Training loss: 3.223033905029297
Validation loss: 2.5419472571342223

Epoch: 6| Step: 6
Training loss: 1.8268797397613525
Validation loss: 2.5595184167226157

Epoch: 6| Step: 7
Training loss: 2.8771181106567383
Validation loss: 2.5446266410171345

Epoch: 6| Step: 8
Training loss: 2.068408727645874
Validation loss: 2.524960369192144

Epoch: 6| Step: 9
Training loss: 2.916572093963623
Validation loss: 2.507756845925444

Epoch: 6| Step: 10
Training loss: 3.594892978668213
Validation loss: 2.502496765505883

Epoch: 6| Step: 11
Training loss: 2.4863405227661133
Validation loss: 2.487820904742005

Epoch: 6| Step: 12
Training loss: 2.349583625793457
Validation loss: 2.485610031312512

Epoch: 6| Step: 13
Training loss: 3.1929004192352295
Validation loss: 2.4825193317987586

Epoch: 243| Step: 0
Training loss: 2.6271426677703857
Validation loss: 2.4631927577398156

Epoch: 6| Step: 1
Training loss: 2.4933981895446777
Validation loss: 2.4444317535687516

Epoch: 6| Step: 2
Training loss: 2.622302770614624
Validation loss: 2.4009997921605266

Epoch: 6| Step: 3
Training loss: 2.418721914291382
Validation loss: 2.393337652247439

Epoch: 6| Step: 4
Training loss: 3.1609368324279785
Validation loss: 2.39089935569353

Epoch: 6| Step: 5
Training loss: 3.3195767402648926
Validation loss: 2.386015885619707

Epoch: 6| Step: 6
Training loss: 1.936848759651184
Validation loss: 2.3990451212852233

Epoch: 6| Step: 7
Training loss: 2.457925319671631
Validation loss: 2.400050206850934

Epoch: 6| Step: 8
Training loss: 3.1267685890197754
Validation loss: 2.4062378303979033

Epoch: 6| Step: 9
Training loss: 2.051328420639038
Validation loss: 2.4032649545259375

Epoch: 6| Step: 10
Training loss: 3.294342517852783
Validation loss: 2.4095820970432733

Epoch: 6| Step: 11
Training loss: 1.5714776515960693
Validation loss: 2.405569668739073

Epoch: 6| Step: 12
Training loss: 2.493335008621216
Validation loss: 2.4098073846550396

Epoch: 6| Step: 13
Training loss: 2.890493631362915
Validation loss: 2.4086264910236483

Epoch: 244| Step: 0
Training loss: 1.6989717483520508
Validation loss: 2.426142120874056

Epoch: 6| Step: 1
Training loss: 2.9272916316986084
Validation loss: 2.415489573632517

Epoch: 6| Step: 2
Training loss: 2.577491283416748
Validation loss: 2.4241974994700444

Epoch: 6| Step: 3
Training loss: 2.781850814819336
Validation loss: 2.4264145153825

Epoch: 6| Step: 4
Training loss: 3.296067237854004
Validation loss: 2.443956244376398

Epoch: 6| Step: 5
Training loss: 2.170706272125244
Validation loss: 2.4382770548584642

Epoch: 6| Step: 6
Training loss: 2.737499237060547
Validation loss: 2.426241341457572

Epoch: 6| Step: 7
Training loss: 2.400181770324707
Validation loss: 2.4335231499005388

Epoch: 6| Step: 8
Training loss: 3.400320053100586
Validation loss: 2.422068959923201

Epoch: 6| Step: 9
Training loss: 2.218470811843872
Validation loss: 2.426146235517276

Epoch: 6| Step: 10
Training loss: 2.701646327972412
Validation loss: 2.4164445143873974

Epoch: 6| Step: 11
Training loss: 2.5765748023986816
Validation loss: 2.414708988640898

Epoch: 6| Step: 12
Training loss: 2.142026901245117
Validation loss: 2.4100359229631323

Epoch: 6| Step: 13
Training loss: 2.4213707447052
Validation loss: 2.4110620047456477

Epoch: 245| Step: 0
Training loss: 2.558021068572998
Validation loss: 2.4046784626540316

Epoch: 6| Step: 1
Training loss: 2.56178617477417
Validation loss: 2.407375386966172

Epoch: 6| Step: 2
Training loss: 2.306884765625
Validation loss: 2.4027144332085886

Epoch: 6| Step: 3
Training loss: 2.5223701000213623
Validation loss: 2.4047580085774904

Epoch: 6| Step: 4
Training loss: 2.8275070190429688
Validation loss: 2.403810470334945

Epoch: 6| Step: 5
Training loss: 2.2311887741088867
Validation loss: 2.4014339523930706

Epoch: 6| Step: 6
Training loss: 2.703726053237915
Validation loss: 2.4085947595616823

Epoch: 6| Step: 7
Training loss: 3.189060688018799
Validation loss: 2.4151709169469853

Epoch: 6| Step: 8
Training loss: 2.709559440612793
Validation loss: 2.427635577417189

Epoch: 6| Step: 9
Training loss: 2.551887273788452
Validation loss: 2.421921691586894

Epoch: 6| Step: 10
Training loss: 1.7997915744781494
Validation loss: 2.431476108489498

Epoch: 6| Step: 11
Training loss: 2.659269094467163
Validation loss: 2.42932036102459

Epoch: 6| Step: 12
Training loss: 2.6057677268981934
Validation loss: 2.4281552171194427

Epoch: 6| Step: 13
Training loss: 2.892240524291992
Validation loss: 2.422538008741153

Epoch: 246| Step: 0
Training loss: 2.8745298385620117
Validation loss: 2.4146958679281254

Epoch: 6| Step: 1
Training loss: 2.386091709136963
Validation loss: 2.421864360891363

Epoch: 6| Step: 2
Training loss: 2.539201259613037
Validation loss: 2.408236845847099

Epoch: 6| Step: 3
Training loss: 2.0393118858337402
Validation loss: 2.4087487933456257

Epoch: 6| Step: 4
Training loss: 2.2823314666748047
Validation loss: 2.4041502450102117

Epoch: 6| Step: 5
Training loss: 2.812011957168579
Validation loss: 2.3959388322727655

Epoch: 6| Step: 6
Training loss: 1.9750661849975586
Validation loss: 2.3963502760856383

Epoch: 6| Step: 7
Training loss: 2.5724284648895264
Validation loss: 2.406393625402963

Epoch: 6| Step: 8
Training loss: 3.0838871002197266
Validation loss: 2.419515235449678

Epoch: 6| Step: 9
Training loss: 2.6770739555358887
Validation loss: 2.416813988839426

Epoch: 6| Step: 10
Training loss: 2.628528118133545
Validation loss: 2.405811261105281

Epoch: 6| Step: 11
Training loss: 2.4887630939483643
Validation loss: 2.4045496627848637

Epoch: 6| Step: 12
Training loss: 3.246767997741699
Validation loss: 2.408841489463724

Epoch: 6| Step: 13
Training loss: 2.28963565826416
Validation loss: 2.4110774276077107

Epoch: 247| Step: 0
Training loss: 2.565542221069336
Validation loss: 2.4122396848535024

Epoch: 6| Step: 1
Training loss: 2.428361415863037
Validation loss: 2.4362407294652795

Epoch: 6| Step: 2
Training loss: 2.545323610305786
Validation loss: 2.445748357362645

Epoch: 6| Step: 3
Training loss: 3.1601076126098633
Validation loss: 2.455248753229777

Epoch: 6| Step: 4
Training loss: 2.833055257797241
Validation loss: 2.442611494371968

Epoch: 6| Step: 5
Training loss: 2.6058123111724854
Validation loss: 2.429605189190116

Epoch: 6| Step: 6
Training loss: 2.4354424476623535
Validation loss: 2.425022817427112

Epoch: 6| Step: 7
Training loss: 2.035473108291626
Validation loss: 2.4041519498312347

Epoch: 6| Step: 8
Training loss: 2.2805323600769043
Validation loss: 2.3967441538328766

Epoch: 6| Step: 9
Training loss: 2.4776721000671387
Validation loss: 2.392746017825219

Epoch: 6| Step: 10
Training loss: 2.512636423110962
Validation loss: 2.3903290879341865

Epoch: 6| Step: 11
Training loss: 2.938264846801758
Validation loss: 2.3941859942610546

Epoch: 6| Step: 12
Training loss: 2.9528439044952393
Validation loss: 2.397169554105369

Epoch: 6| Step: 13
Training loss: 1.9582059383392334
Validation loss: 2.40584703927399

Epoch: 248| Step: 0
Training loss: 2.7143383026123047
Validation loss: 2.4183182870188067

Epoch: 6| Step: 1
Training loss: 3.0536391735076904
Validation loss: 2.4053148813145135

Epoch: 6| Step: 2
Training loss: 2.9540796279907227
Validation loss: 2.4019217619331936

Epoch: 6| Step: 3
Training loss: 2.7674078941345215
Validation loss: 2.3957142291530484

Epoch: 6| Step: 4
Training loss: 3.26945161819458
Validation loss: 2.3893455433589157

Epoch: 6| Step: 5
Training loss: 2.4531407356262207
Validation loss: 2.383889772558725

Epoch: 6| Step: 6
Training loss: 2.466700792312622
Validation loss: 2.385023911794027

Epoch: 6| Step: 7
Training loss: 2.2643814086914062
Validation loss: 2.398051807957311

Epoch: 6| Step: 8
Training loss: 2.66697359085083
Validation loss: 2.399565767216426

Epoch: 6| Step: 9
Training loss: 2.441984176635742
Validation loss: 2.4119289741721204

Epoch: 6| Step: 10
Training loss: 1.8470795154571533
Validation loss: 2.4310311835299254

Epoch: 6| Step: 11
Training loss: 2.3626372814178467
Validation loss: 2.4316841235724826

Epoch: 6| Step: 12
Training loss: 2.4846160411834717
Validation loss: 2.440592517134964

Epoch: 6| Step: 13
Training loss: 2.1894710063934326
Validation loss: 2.4521959930337887

Epoch: 249| Step: 0
Training loss: 2.7229700088500977
Validation loss: 2.4601807786572363

Epoch: 6| Step: 1
Training loss: 3.238635301589966
Validation loss: 2.469011475962977

Epoch: 6| Step: 2
Training loss: 3.2747550010681152
Validation loss: 2.458190882077781

Epoch: 6| Step: 3
Training loss: 2.6358048915863037
Validation loss: 2.433612446631155

Epoch: 6| Step: 4
Training loss: 1.8034844398498535
Validation loss: 2.4126276803273026

Epoch: 6| Step: 5
Training loss: 3.028792381286621
Validation loss: 2.420534823530464

Epoch: 6| Step: 6
Training loss: 2.885333776473999
Validation loss: 2.415081198497485

Epoch: 6| Step: 7
Training loss: 1.9887378215789795
Validation loss: 2.4084802904436664

Epoch: 6| Step: 8
Training loss: 2.9364819526672363
Validation loss: 2.4222327893780125

Epoch: 6| Step: 9
Training loss: 2.357055187225342
Validation loss: 2.3973345730894353

Epoch: 6| Step: 10
Training loss: 2.2002525329589844
Validation loss: 2.3875205670633624

Epoch: 6| Step: 11
Training loss: 2.3570284843444824
Validation loss: 2.373445587773477

Epoch: 6| Step: 12
Training loss: 2.607274293899536
Validation loss: 2.3787259670995895

Epoch: 6| Step: 13
Training loss: 2.044050455093384
Validation loss: 2.3736010059233634

Epoch: 250| Step: 0
Training loss: 2.402134418487549
Validation loss: 2.3713525854131228

Epoch: 6| Step: 1
Training loss: 2.6097536087036133
Validation loss: 2.3808592468179683

Epoch: 6| Step: 2
Training loss: 2.429905414581299
Validation loss: 2.3863428305554133

Epoch: 6| Step: 3
Training loss: 2.921274185180664
Validation loss: 2.394625699648293

Epoch: 6| Step: 4
Training loss: 2.5542218685150146
Validation loss: 2.4067832064884964

Epoch: 6| Step: 5
Training loss: 2.972475290298462
Validation loss: 2.4011671004756803

Epoch: 6| Step: 6
Training loss: 2.3525588512420654
Validation loss: 2.4108660785100793

Epoch: 6| Step: 7
Training loss: 2.287492275238037
Validation loss: 2.42749806116986

Epoch: 6| Step: 8
Training loss: 2.339876651763916
Validation loss: 2.4500693557082966

Epoch: 6| Step: 9
Training loss: 2.3750712871551514
Validation loss: 2.4542197540242183

Epoch: 6| Step: 10
Training loss: 2.711334228515625
Validation loss: 2.465410960617886

Epoch: 6| Step: 11
Training loss: 2.724177837371826
Validation loss: 2.455264788801952

Epoch: 6| Step: 12
Training loss: 2.631164789199829
Validation loss: 2.464163693048621

Epoch: 6| Step: 13
Training loss: 2.565842866897583
Validation loss: 2.4430299112873692

Epoch: 251| Step: 0
Training loss: 1.6781364679336548
Validation loss: 2.428296591645928

Epoch: 6| Step: 1
Training loss: 2.1870839595794678
Validation loss: 2.417969067891439

Epoch: 6| Step: 2
Training loss: 2.579814910888672
Validation loss: 2.4076637991013063

Epoch: 6| Step: 3
Training loss: 2.4157357215881348
Validation loss: 2.404883348813621

Epoch: 6| Step: 4
Training loss: 2.7987422943115234
Validation loss: 2.4074319870241228

Epoch: 6| Step: 5
Training loss: 2.871847152709961
Validation loss: 2.399626439617526

Epoch: 6| Step: 6
Training loss: 2.226188898086548
Validation loss: 2.394178395630211

Epoch: 6| Step: 7
Training loss: 1.9104411602020264
Validation loss: 2.402301252529185

Epoch: 6| Step: 8
Training loss: 2.495291233062744
Validation loss: 2.4013981024424234

Epoch: 6| Step: 9
Training loss: 2.915480852127075
Validation loss: 2.4062164393804406

Epoch: 6| Step: 10
Training loss: 2.388002395629883
Validation loss: 2.4199711815003426

Epoch: 6| Step: 11
Training loss: 3.1386375427246094
Validation loss: 2.398503001018237

Epoch: 6| Step: 12
Training loss: 3.018083095550537
Validation loss: 2.39851729331478

Epoch: 6| Step: 13
Training loss: 3.41648006439209
Validation loss: 2.398758831844535

Epoch: 252| Step: 0
Training loss: 2.9108362197875977
Validation loss: 2.402659708453763

Epoch: 6| Step: 1
Training loss: 3.050809860229492
Validation loss: 2.4039612482952815

Epoch: 6| Step: 2
Training loss: 1.984752893447876
Validation loss: 2.4026832606202815

Epoch: 6| Step: 3
Training loss: 3.3989381790161133
Validation loss: 2.399705163894161

Epoch: 6| Step: 4
Training loss: 2.6046853065490723
Validation loss: 2.4030542758203324

Epoch: 6| Step: 5
Training loss: 1.8309590816497803
Validation loss: 2.4209348219697193

Epoch: 6| Step: 6
Training loss: 2.3769869804382324
Validation loss: 2.42061072908422

Epoch: 6| Step: 7
Training loss: 2.5730199813842773
Validation loss: 2.4333708235012588

Epoch: 6| Step: 8
Training loss: 2.241360664367676
Validation loss: 2.4301608377887356

Epoch: 6| Step: 9
Training loss: 3.0127100944519043
Validation loss: 2.44555341300144

Epoch: 6| Step: 10
Training loss: 2.238677740097046
Validation loss: 2.442892480922002

Epoch: 6| Step: 11
Training loss: 2.3950936794281006
Validation loss: 2.4497720503037974

Epoch: 6| Step: 12
Training loss: 2.3722081184387207
Validation loss: 2.456760762840189

Epoch: 6| Step: 13
Training loss: 2.79030179977417
Validation loss: 2.4421061931117887

Epoch: 253| Step: 0
Training loss: 1.7744872570037842
Validation loss: 2.4460783671307307

Epoch: 6| Step: 1
Training loss: 2.943229913711548
Validation loss: 2.4206432527111423

Epoch: 6| Step: 2
Training loss: 2.472578525543213
Validation loss: 2.405682727854739

Epoch: 6| Step: 3
Training loss: 2.396944761276245
Validation loss: 2.4055716863242527

Epoch: 6| Step: 4
Training loss: 2.4119880199432373
Validation loss: 2.3990840270955074

Epoch: 6| Step: 5
Training loss: 3.286583423614502
Validation loss: 2.3912765800312

Epoch: 6| Step: 6
Training loss: 2.232126235961914
Validation loss: 2.398646236747824

Epoch: 6| Step: 7
Training loss: 3.143174648284912
Validation loss: 2.388413651015169

Epoch: 6| Step: 8
Training loss: 3.1116435527801514
Validation loss: 2.379446403954619

Epoch: 6| Step: 9
Training loss: 3.0451250076293945
Validation loss: 2.3897875790954917

Epoch: 6| Step: 10
Training loss: 1.8812830448150635
Validation loss: 2.384970949542138

Epoch: 6| Step: 11
Training loss: 2.2789433002471924
Validation loss: 2.4058085000643166

Epoch: 6| Step: 12
Training loss: 2.198131561279297
Validation loss: 2.4117919142528246

Epoch: 6| Step: 13
Training loss: 2.333160638809204
Validation loss: 2.4231249568282918

Epoch: 254| Step: 0
Training loss: 1.8999919891357422
Validation loss: 2.434341848537486

Epoch: 6| Step: 1
Training loss: 3.0369739532470703
Validation loss: 2.437497638886975

Epoch: 6| Step: 2
Training loss: 3.07525634765625
Validation loss: 2.4423001171440206

Epoch: 6| Step: 3
Training loss: 2.4424595832824707
Validation loss: 2.4248809737543904

Epoch: 6| Step: 4
Training loss: 2.9401350021362305
Validation loss: 2.3982753240933983

Epoch: 6| Step: 5
Training loss: 2.7894814014434814
Validation loss: 2.3851733733248968

Epoch: 6| Step: 6
Training loss: 1.8457921743392944
Validation loss: 2.377384055045343

Epoch: 6| Step: 7
Training loss: 1.7287273406982422
Validation loss: 2.385143724820947

Epoch: 6| Step: 8
Training loss: 2.6381964683532715
Validation loss: 2.3746343351179555

Epoch: 6| Step: 9
Training loss: 2.488201141357422
Validation loss: 2.370226780573527

Epoch: 6| Step: 10
Training loss: 3.1576504707336426
Validation loss: 2.370039565588838

Epoch: 6| Step: 11
Training loss: 2.3875787258148193
Validation loss: 2.3720819411739225

Epoch: 6| Step: 12
Training loss: 2.422316074371338
Validation loss: 2.366442152248916

Epoch: 6| Step: 13
Training loss: 3.3173580169677734
Validation loss: 2.3635006309837423

Epoch: 255| Step: 0
Training loss: 2.0163726806640625
Validation loss: 2.3688417429565103

Epoch: 6| Step: 1
Training loss: 2.3575332164764404
Validation loss: 2.3682182014629407

Epoch: 6| Step: 2
Training loss: 2.186201572418213
Validation loss: 2.3741411624416227

Epoch: 6| Step: 3
Training loss: 2.8943490982055664
Validation loss: 2.3846191103740404

Epoch: 6| Step: 4
Training loss: 2.4659271240234375
Validation loss: 2.3987361141430434

Epoch: 6| Step: 5
Training loss: 2.6280031204223633
Validation loss: 2.395196642926944

Epoch: 6| Step: 6
Training loss: 2.6208479404449463
Validation loss: 2.403506126455081

Epoch: 6| Step: 7
Training loss: 2.350529432296753
Validation loss: 2.4014645391894924

Epoch: 6| Step: 8
Training loss: 2.653475761413574
Validation loss: 2.4271279509349535

Epoch: 6| Step: 9
Training loss: 2.0433740615844727
Validation loss: 2.4333719515031382

Epoch: 6| Step: 10
Training loss: 3.4167098999023438
Validation loss: 2.4361207639017413

Epoch: 6| Step: 11
Training loss: 3.1131210327148438
Validation loss: 2.4263680904142317

Epoch: 6| Step: 12
Training loss: 2.6765644550323486
Validation loss: 2.4267884608237975

Epoch: 6| Step: 13
Training loss: 2.61630916595459
Validation loss: 2.413799975508003

Epoch: 256| Step: 0
Training loss: 2.5145232677459717
Validation loss: 2.405338412971907

Epoch: 6| Step: 1
Training loss: 3.0737290382385254
Validation loss: 2.40103538702893

Epoch: 6| Step: 2
Training loss: 2.6324262619018555
Validation loss: 2.3970788166087162

Epoch: 6| Step: 3
Training loss: 2.7417938709259033
Validation loss: 2.4051153531638523

Epoch: 6| Step: 4
Training loss: 2.056075096130371
Validation loss: 2.4199028233046174

Epoch: 6| Step: 5
Training loss: 1.961342692375183
Validation loss: 2.411365973052158

Epoch: 6| Step: 6
Training loss: 3.0254461765289307
Validation loss: 2.4173810789662022

Epoch: 6| Step: 7
Training loss: 2.001593828201294
Validation loss: 2.4034059842427573

Epoch: 6| Step: 8
Training loss: 2.140566349029541
Validation loss: 2.396021725029074

Epoch: 6| Step: 9
Training loss: 3.243011713027954
Validation loss: 2.400417662435962

Epoch: 6| Step: 10
Training loss: 3.219614267349243
Validation loss: 2.3954361664351596

Epoch: 6| Step: 11
Training loss: 2.1582624912261963
Validation loss: 2.3892589128145607

Epoch: 6| Step: 12
Training loss: 2.203098773956299
Validation loss: 2.386813079157183

Epoch: 6| Step: 13
Training loss: 2.8766071796417236
Validation loss: 2.379363126652215

Epoch: 257| Step: 0
Training loss: 2.115671157836914
Validation loss: 2.378962151465877

Epoch: 6| Step: 1
Training loss: 2.5334107875823975
Validation loss: 2.403381798857002

Epoch: 6| Step: 2
Training loss: 2.3048195838928223
Validation loss: 2.397815171108451

Epoch: 6| Step: 3
Training loss: 2.9294662475585938
Validation loss: 2.404748132151942

Epoch: 6| Step: 4
Training loss: 2.3586671352386475
Validation loss: 2.4027070178780505

Epoch: 6| Step: 5
Training loss: 2.2693326473236084
Validation loss: 2.3999992262932563

Epoch: 6| Step: 6
Training loss: 2.9559764862060547
Validation loss: 2.40910981803812

Epoch: 6| Step: 7
Training loss: 2.1459178924560547
Validation loss: 2.42762416408908

Epoch: 6| Step: 8
Training loss: 2.1114561557769775
Validation loss: 2.4225330583510862

Epoch: 6| Step: 9
Training loss: 2.6533102989196777
Validation loss: 2.4279786617525163

Epoch: 6| Step: 10
Training loss: 2.72359561920166
Validation loss: 2.441683042433954

Epoch: 6| Step: 11
Training loss: 3.3726232051849365
Validation loss: 2.438030619775095

Epoch: 6| Step: 12
Training loss: 2.609135866165161
Validation loss: 2.4446355450537895

Epoch: 6| Step: 13
Training loss: 2.6084420680999756
Validation loss: 2.426311674938407

Epoch: 258| Step: 0
Training loss: 2.858375310897827
Validation loss: 2.409224946011779

Epoch: 6| Step: 1
Training loss: 2.4655613899230957
Validation loss: 2.3960402678417903

Epoch: 6| Step: 2
Training loss: 2.7983574867248535
Validation loss: 2.389695777687975

Epoch: 6| Step: 3
Training loss: 2.2915420532226562
Validation loss: 2.3796750294264926

Epoch: 6| Step: 4
Training loss: 3.1452083587646484
Validation loss: 2.373759064623105

Epoch: 6| Step: 5
Training loss: 2.6669869422912598
Validation loss: 2.37375109939165

Epoch: 6| Step: 6
Training loss: 2.438965320587158
Validation loss: 2.3664289238632366

Epoch: 6| Step: 7
Training loss: 2.5496339797973633
Validation loss: 2.3672374115195325

Epoch: 6| Step: 8
Training loss: 2.7470550537109375
Validation loss: 2.3688153041306363

Epoch: 6| Step: 9
Training loss: 2.136810541152954
Validation loss: 2.3659463633773146

Epoch: 6| Step: 10
Training loss: 2.288790225982666
Validation loss: 2.3688640876482894

Epoch: 6| Step: 11
Training loss: 2.539679765701294
Validation loss: 2.3744782581124255

Epoch: 6| Step: 12
Training loss: 2.030226945877075
Validation loss: 2.3624911923562326

Epoch: 6| Step: 13
Training loss: 3.152373790740967
Validation loss: 2.368551500381962

Epoch: 259| Step: 0
Training loss: 2.395867347717285
Validation loss: 2.372524207638156

Epoch: 6| Step: 1
Training loss: 2.5753374099731445
Validation loss: 2.3742364375822005

Epoch: 6| Step: 2
Training loss: 2.65281081199646
Validation loss: 2.381209909275014

Epoch: 6| Step: 3
Training loss: 2.5984079837799072
Validation loss: 2.3850497456007105

Epoch: 6| Step: 4
Training loss: 2.346467971801758
Validation loss: 2.3872356491704143

Epoch: 6| Step: 5
Training loss: 2.417349338531494
Validation loss: 2.393344224140208

Epoch: 6| Step: 6
Training loss: 2.103759527206421
Validation loss: 2.3904336960084978

Epoch: 6| Step: 7
Training loss: 3.0795538425445557
Validation loss: 2.3986870806704284

Epoch: 6| Step: 8
Training loss: 2.8934214115142822
Validation loss: 2.3970255364653883

Epoch: 6| Step: 9
Training loss: 2.3711202144622803
Validation loss: 2.3889685138579337

Epoch: 6| Step: 10
Training loss: 2.3461954593658447
Validation loss: 2.4007752864591536

Epoch: 6| Step: 11
Training loss: 2.9974591732025146
Validation loss: 2.406474159609887

Epoch: 6| Step: 12
Training loss: 2.516237735748291
Validation loss: 2.397621529076689

Epoch: 6| Step: 13
Training loss: 2.0520169734954834
Validation loss: 2.3960431339920207

Epoch: 260| Step: 0
Training loss: 2.2048749923706055
Validation loss: 2.3914269426817536

Epoch: 6| Step: 1
Training loss: 2.4718222618103027
Validation loss: 2.3868213058799825

Epoch: 6| Step: 2
Training loss: 2.5137274265289307
Validation loss: 2.3997329717041342

Epoch: 6| Step: 3
Training loss: 2.65472149848938
Validation loss: 2.4101226868168

Epoch: 6| Step: 4
Training loss: 1.7579550743103027
Validation loss: 2.410163264120779

Epoch: 6| Step: 5
Training loss: 1.8645215034484863
Validation loss: 2.4214923894533547

Epoch: 6| Step: 6
Training loss: 1.9234206676483154
Validation loss: 2.413445762408677

Epoch: 6| Step: 7
Training loss: 3.7824478149414062
Validation loss: 2.425341690740278

Epoch: 6| Step: 8
Training loss: 2.373030662536621
Validation loss: 2.422417881668255

Epoch: 6| Step: 9
Training loss: 2.829367160797119
Validation loss: 2.409808140929027

Epoch: 6| Step: 10
Training loss: 2.867220401763916
Validation loss: 2.418024537383869

Epoch: 6| Step: 11
Training loss: 3.0243747234344482
Validation loss: 2.4098382790883384

Epoch: 6| Step: 12
Training loss: 2.5172364711761475
Validation loss: 2.4196216598633797

Epoch: 6| Step: 13
Training loss: 3.0927016735076904
Validation loss: 2.4105592209805726

Epoch: 261| Step: 0
Training loss: 2.544375419616699
Validation loss: 2.404865490492954

Epoch: 6| Step: 1
Training loss: 2.937849521636963
Validation loss: 2.398834010606171

Epoch: 6| Step: 2
Training loss: 2.8968329429626465
Validation loss: 2.4128449962985132

Epoch: 6| Step: 3
Training loss: 2.872124195098877
Validation loss: 2.40730643528764

Epoch: 6| Step: 4
Training loss: 2.232952117919922
Validation loss: 2.420576410908853

Epoch: 6| Step: 5
Training loss: 2.951582908630371
Validation loss: 2.4198426328679568

Epoch: 6| Step: 6
Training loss: 2.174232006072998
Validation loss: 2.4110985058610157

Epoch: 6| Step: 7
Training loss: 2.735544204711914
Validation loss: 2.4197070649875108

Epoch: 6| Step: 8
Training loss: 1.7246057987213135
Validation loss: 2.4235989098907798

Epoch: 6| Step: 9
Training loss: 3.040776252746582
Validation loss: 2.4261378908670075

Epoch: 6| Step: 10
Training loss: 1.792371392250061
Validation loss: 2.4190288743665143

Epoch: 6| Step: 11
Training loss: 2.5382471084594727
Validation loss: 2.4180687242938625

Epoch: 6| Step: 12
Training loss: 2.7990102767944336
Validation loss: 2.4035922199167232

Epoch: 6| Step: 13
Training loss: 1.8212637901306152
Validation loss: 2.3978578711068756

Epoch: 262| Step: 0
Training loss: 1.9472609758377075
Validation loss: 2.396059625892229

Epoch: 6| Step: 1
Training loss: 3.0218305587768555
Validation loss: 2.3840705271690124

Epoch: 6| Step: 2
Training loss: 2.5070934295654297
Validation loss: 2.3905972473083006

Epoch: 6| Step: 3
Training loss: 2.4655208587646484
Validation loss: 2.3846592928773616

Epoch: 6| Step: 4
Training loss: 3.4791269302368164
Validation loss: 2.3755283612076954

Epoch: 6| Step: 5
Training loss: 1.9840409755706787
Validation loss: 2.3812835959978003

Epoch: 6| Step: 6
Training loss: 1.9709861278533936
Validation loss: 2.4113957292290142

Epoch: 6| Step: 7
Training loss: 2.5462000370025635
Validation loss: 2.429298439333516

Epoch: 6| Step: 8
Training loss: 2.656156301498413
Validation loss: 2.436753334537629

Epoch: 6| Step: 9
Training loss: 2.567415237426758
Validation loss: 2.4387907084598335

Epoch: 6| Step: 10
Training loss: 3.556331157684326
Validation loss: 2.4343458683260026

Epoch: 6| Step: 11
Training loss: 2.2042236328125
Validation loss: 2.430038518803094

Epoch: 6| Step: 12
Training loss: 2.4472060203552246
Validation loss: 2.4207130221910376

Epoch: 6| Step: 13
Training loss: 2.4038217067718506
Validation loss: 2.398953125041018

Epoch: 263| Step: 0
Training loss: 2.098592758178711
Validation loss: 2.383122049352174

Epoch: 6| Step: 1
Training loss: 2.419773578643799
Validation loss: 2.369980855654645

Epoch: 6| Step: 2
Training loss: 2.791508674621582
Validation loss: 2.3553100132173106

Epoch: 6| Step: 3
Training loss: 3.293468952178955
Validation loss: 2.3660650817296838

Epoch: 6| Step: 4
Training loss: 2.0439085960388184
Validation loss: 2.36038484368273

Epoch: 6| Step: 5
Training loss: 2.672849655151367
Validation loss: 2.3618286143067064

Epoch: 6| Step: 6
Training loss: 2.768394947052002
Validation loss: 2.362475441348168

Epoch: 6| Step: 7
Training loss: 2.4242868423461914
Validation loss: 2.36431949369369

Epoch: 6| Step: 8
Training loss: 2.344484567642212
Validation loss: 2.3687791773068008

Epoch: 6| Step: 9
Training loss: 2.644289970397949
Validation loss: 2.367700843400853

Epoch: 6| Step: 10
Training loss: 2.3181543350219727
Validation loss: 2.355884475092734

Epoch: 6| Step: 11
Training loss: 1.9211888313293457
Validation loss: 2.3589344793750393

Epoch: 6| Step: 12
Training loss: 2.7337496280670166
Validation loss: 2.3603928524960756

Epoch: 6| Step: 13
Training loss: 3.44718599319458
Validation loss: 2.3694091663565686

Epoch: 264| Step: 0
Training loss: 2.304013967514038
Validation loss: 2.3885114500599522

Epoch: 6| Step: 1
Training loss: 2.265702247619629
Validation loss: 2.39651116119918

Epoch: 6| Step: 2
Training loss: 2.5570027828216553
Validation loss: 2.4001690110852643

Epoch: 6| Step: 3
Training loss: 2.6517333984375
Validation loss: 2.3920686501328663

Epoch: 6| Step: 4
Training loss: 2.4669387340545654
Validation loss: 2.413887034180344

Epoch: 6| Step: 5
Training loss: 2.385127067565918
Validation loss: 2.4081197707883772

Epoch: 6| Step: 6
Training loss: 2.3670339584350586
Validation loss: 2.4128390999250513

Epoch: 6| Step: 7
Training loss: 2.744889259338379
Validation loss: 2.4226935499457904

Epoch: 6| Step: 8
Training loss: 2.8732943534851074
Validation loss: 2.4223834135199107

Epoch: 6| Step: 9
Training loss: 2.4930381774902344
Validation loss: 2.41742012064944

Epoch: 6| Step: 10
Training loss: 2.3008246421813965
Validation loss: 2.4077865436512935

Epoch: 6| Step: 11
Training loss: 2.684875965118408
Validation loss: 2.414216797838929

Epoch: 6| Step: 12
Training loss: 3.022271156311035
Validation loss: 2.414819335424772

Epoch: 6| Step: 13
Training loss: 2.212801694869995
Validation loss: 2.3999772815294165

Epoch: 265| Step: 0
Training loss: 2.121347188949585
Validation loss: 2.3757276586306992

Epoch: 6| Step: 1
Training loss: 2.2422118186950684
Validation loss: 2.3746238677732405

Epoch: 6| Step: 2
Training loss: 2.416609048843384
Validation loss: 2.36857521405784

Epoch: 6| Step: 3
Training loss: 2.414196252822876
Validation loss: 2.364088258435649

Epoch: 6| Step: 4
Training loss: 2.8257668018341064
Validation loss: 2.3630292723255772

Epoch: 6| Step: 5
Training loss: 2.708103656768799
Validation loss: 2.3616585039323374

Epoch: 6| Step: 6
Training loss: 1.8020241260528564
Validation loss: 2.359765004086238

Epoch: 6| Step: 7
Training loss: 3.1139256954193115
Validation loss: 2.369076221219955

Epoch: 6| Step: 8
Training loss: 2.439265727996826
Validation loss: 2.358877041006601

Epoch: 6| Step: 9
Training loss: 3.0779545307159424
Validation loss: 2.3603320890857327

Epoch: 6| Step: 10
Training loss: 3.5268006324768066
Validation loss: 2.3668852877873245

Epoch: 6| Step: 11
Training loss: 2.2634310722351074
Validation loss: 2.363183929074195

Epoch: 6| Step: 12
Training loss: 2.2555112838745117
Validation loss: 2.3703258986114175

Epoch: 6| Step: 13
Training loss: 2.183342456817627
Validation loss: 2.3703869106949016

Epoch: 266| Step: 0
Training loss: 2.261594533920288
Validation loss: 2.3821929193312124

Epoch: 6| Step: 1
Training loss: 2.78201961517334
Validation loss: 2.37612090059506

Epoch: 6| Step: 2
Training loss: 2.6501102447509766
Validation loss: 2.3952006704063824

Epoch: 6| Step: 3
Training loss: 3.1289360523223877
Validation loss: 2.389838041797761

Epoch: 6| Step: 4
Training loss: 2.5360395908355713
Validation loss: 2.387861615868025

Epoch: 6| Step: 5
Training loss: 2.1884472370147705
Validation loss: 2.3746727922911286

Epoch: 6| Step: 6
Training loss: 2.781205654144287
Validation loss: 2.386444866016347

Epoch: 6| Step: 7
Training loss: 2.449627161026001
Validation loss: 2.393458956031389

Epoch: 6| Step: 8
Training loss: 2.2080321311950684
Validation loss: 2.390783043317897

Epoch: 6| Step: 9
Training loss: 2.0289759635925293
Validation loss: 2.3920967476342314

Epoch: 6| Step: 10
Training loss: 2.474790334701538
Validation loss: 2.3851904997261624

Epoch: 6| Step: 11
Training loss: 2.910804271697998
Validation loss: 2.3933381983028945

Epoch: 6| Step: 12
Training loss: 2.5328569412231445
Validation loss: 2.3875713040751796

Epoch: 6| Step: 13
Training loss: 2.2159318923950195
Validation loss: 2.391993434198441

Epoch: 267| Step: 0
Training loss: 2.4771981239318848
Validation loss: 2.3780792067127843

Epoch: 6| Step: 1
Training loss: 1.9158804416656494
Validation loss: 2.373428313962875

Epoch: 6| Step: 2
Training loss: 2.128706932067871
Validation loss: 2.373806699629753

Epoch: 6| Step: 3
Training loss: 2.8657379150390625
Validation loss: 2.3826986820467058

Epoch: 6| Step: 4
Training loss: 2.686519145965576
Validation loss: 2.3703638340837214

Epoch: 6| Step: 5
Training loss: 2.7645936012268066
Validation loss: 2.3766859423729683

Epoch: 6| Step: 6
Training loss: 2.5411033630371094
Validation loss: 2.3864635293201735

Epoch: 6| Step: 7
Training loss: 3.0625741481781006
Validation loss: 2.389068353560663

Epoch: 6| Step: 8
Training loss: 2.5410614013671875
Validation loss: 2.3853861311430573

Epoch: 6| Step: 9
Training loss: 2.7816832065582275
Validation loss: 2.369278618084487

Epoch: 6| Step: 10
Training loss: 2.4681484699249268
Validation loss: 2.3738199895428074

Epoch: 6| Step: 11
Training loss: 2.7615585327148438
Validation loss: 2.376741237537835

Epoch: 6| Step: 12
Training loss: 2.280280113220215
Validation loss: 2.3667350405006

Epoch: 6| Step: 13
Training loss: 1.7175097465515137
Validation loss: 2.3688140453830844

Epoch: 268| Step: 0
Training loss: 2.272397518157959
Validation loss: 2.3658624336283696

Epoch: 6| Step: 1
Training loss: 2.0339369773864746
Validation loss: 2.383931090754847

Epoch: 6| Step: 2
Training loss: 2.546503782272339
Validation loss: 2.370704471424062

Epoch: 6| Step: 3
Training loss: 2.237839460372925
Validation loss: 2.3820237933948474

Epoch: 6| Step: 4
Training loss: 2.6836538314819336
Validation loss: 2.3940799236297607

Epoch: 6| Step: 5
Training loss: 2.2230844497680664
Validation loss: 2.389765508713261

Epoch: 6| Step: 6
Training loss: 2.477550745010376
Validation loss: 2.400752612339553

Epoch: 6| Step: 7
Training loss: 2.450026035308838
Validation loss: 2.4101432907965874

Epoch: 6| Step: 8
Training loss: 2.985773801803589
Validation loss: 2.4110885076625372

Epoch: 6| Step: 9
Training loss: 2.3919944763183594
Validation loss: 2.4092332624620005

Epoch: 6| Step: 10
Training loss: 2.6190290451049805
Validation loss: 2.4081501114752983

Epoch: 6| Step: 11
Training loss: 2.5575437545776367
Validation loss: 2.403892401726015

Epoch: 6| Step: 12
Training loss: 3.033190965652466
Validation loss: 2.4093169448196248

Epoch: 6| Step: 13
Training loss: 2.7581710815429688
Validation loss: 2.4054990763305337

Epoch: 269| Step: 0
Training loss: 2.765031576156616
Validation loss: 2.3777413586134553

Epoch: 6| Step: 1
Training loss: 1.9517438411712646
Validation loss: 2.3827535285744617

Epoch: 6| Step: 2
Training loss: 3.1815686225891113
Validation loss: 2.3801217258617444

Epoch: 6| Step: 3
Training loss: 3.2665605545043945
Validation loss: 2.3864853907656927

Epoch: 6| Step: 4
Training loss: 1.9617152214050293
Validation loss: 2.3857541366290023

Epoch: 6| Step: 5
Training loss: 2.519139051437378
Validation loss: 2.390667857662324

Epoch: 6| Step: 6
Training loss: 2.40907621383667
Validation loss: 2.378356559302217

Epoch: 6| Step: 7
Training loss: 2.5572690963745117
Validation loss: 2.373820299743324

Epoch: 6| Step: 8
Training loss: 2.224250555038452
Validation loss: 2.3791352882180163

Epoch: 6| Step: 9
Training loss: 2.918184280395508
Validation loss: 2.373966628505338

Epoch: 6| Step: 10
Training loss: 2.273716926574707
Validation loss: 2.361396469095702

Epoch: 6| Step: 11
Training loss: 3.1845874786376953
Validation loss: 2.354610026523631

Epoch: 6| Step: 12
Training loss: 2.2911462783813477
Validation loss: 2.3462976460815756

Epoch: 6| Step: 13
Training loss: 1.175472617149353
Validation loss: 2.351949240571709

Epoch: 270| Step: 0
Training loss: 2.8598546981811523
Validation loss: 2.359844230836438

Epoch: 6| Step: 1
Training loss: 2.2172048091888428
Validation loss: 2.3661324952238347

Epoch: 6| Step: 2
Training loss: 1.9185541868209839
Validation loss: 2.3805637205800703

Epoch: 6| Step: 3
Training loss: 2.7988686561584473
Validation loss: 2.403599690365535

Epoch: 6| Step: 4
Training loss: 2.858750581741333
Validation loss: 2.4064979066130934

Epoch: 6| Step: 5
Training loss: 2.2834393978118896
Validation loss: 2.398971901145033

Epoch: 6| Step: 6
Training loss: 2.6557140350341797
Validation loss: 2.412296369511594

Epoch: 6| Step: 7
Training loss: 2.1447129249572754
Validation loss: 2.4109569134250766

Epoch: 6| Step: 8
Training loss: 1.9610618352890015
Validation loss: 2.4023191364862586

Epoch: 6| Step: 9
Training loss: 3.4738783836364746
Validation loss: 2.3957310620174614

Epoch: 6| Step: 10
Training loss: 2.711090326309204
Validation loss: 2.39201477009763

Epoch: 6| Step: 11
Training loss: 2.922344207763672
Validation loss: 2.395242057820802

Epoch: 6| Step: 12
Training loss: 2.1073145866394043
Validation loss: 2.3744761892544326

Epoch: 6| Step: 13
Training loss: 2.003420114517212
Validation loss: 2.3758422149124967

Epoch: 271| Step: 0
Training loss: 2.4040699005126953
Validation loss: 2.3724784863892423

Epoch: 6| Step: 1
Training loss: 1.9808428287506104
Validation loss: 2.374159542463159

Epoch: 6| Step: 2
Training loss: 2.5897345542907715
Validation loss: 2.3673509731087634

Epoch: 6| Step: 3
Training loss: 2.9918594360351562
Validation loss: 2.3714649651640203

Epoch: 6| Step: 4
Training loss: 2.381086826324463
Validation loss: 2.360426361842822

Epoch: 6| Step: 5
Training loss: 2.158794403076172
Validation loss: 2.3579302680107856

Epoch: 6| Step: 6
Training loss: 2.5961294174194336
Validation loss: 2.361065577435237

Epoch: 6| Step: 7
Training loss: 2.0550382137298584
Validation loss: 2.3629174540119786

Epoch: 6| Step: 8
Training loss: 2.8195433616638184
Validation loss: 2.3798319344879477

Epoch: 6| Step: 9
Training loss: 2.9180846214294434
Validation loss: 2.398696977605102

Epoch: 6| Step: 10
Training loss: 2.461829662322998
Validation loss: 2.430449131996401

Epoch: 6| Step: 11
Training loss: 2.77651309967041
Validation loss: 2.4275798874516643

Epoch: 6| Step: 12
Training loss: 2.112884521484375
Validation loss: 2.4266194605058238

Epoch: 6| Step: 13
Training loss: 3.173339605331421
Validation loss: 2.4135025752488004

Epoch: 272| Step: 0
Training loss: 1.8819478750228882
Validation loss: 2.3980788825660624

Epoch: 6| Step: 1
Training loss: 2.394491195678711
Validation loss: 2.3848677322428715

Epoch: 6| Step: 2
Training loss: 2.518085479736328
Validation loss: 2.3893477865444717

Epoch: 6| Step: 3
Training loss: 2.828031063079834
Validation loss: 2.3683377619712584

Epoch: 6| Step: 4
Training loss: 3.004389762878418
Validation loss: 2.364726584444764

Epoch: 6| Step: 5
Training loss: 2.2496085166931152
Validation loss: 2.3716526569858676

Epoch: 6| Step: 6
Training loss: 3.1324210166931152
Validation loss: 2.3622338874365694

Epoch: 6| Step: 7
Training loss: 2.4679040908813477
Validation loss: 2.3716965285680627

Epoch: 6| Step: 8
Training loss: 2.3280954360961914
Validation loss: 2.3695239995115545

Epoch: 6| Step: 9
Training loss: 2.809053897857666
Validation loss: 2.3745061415497974

Epoch: 6| Step: 10
Training loss: 3.144329071044922
Validation loss: 2.3672080809070217

Epoch: 6| Step: 11
Training loss: 2.1407084465026855
Validation loss: 2.3533721559791156

Epoch: 6| Step: 12
Training loss: 2.217589855194092
Validation loss: 2.3576522334929435

Epoch: 6| Step: 13
Training loss: 2.1795735359191895
Validation loss: 2.3493829183681036

Epoch: 273| Step: 0
Training loss: 2.518221616744995
Validation loss: 2.3448046638119604

Epoch: 6| Step: 1
Training loss: 2.1042985916137695
Validation loss: 2.3550431753999446

Epoch: 6| Step: 2
Training loss: 2.3178188800811768
Validation loss: 2.354259488403156

Epoch: 6| Step: 3
Training loss: 2.7474937438964844
Validation loss: 2.3654906211360807

Epoch: 6| Step: 4
Training loss: 2.3981270790100098
Validation loss: 2.369940503951042

Epoch: 6| Step: 5
Training loss: 2.1435160636901855
Validation loss: 2.3755824694069485

Epoch: 6| Step: 6
Training loss: 1.982163429260254
Validation loss: 2.37590394994264

Epoch: 6| Step: 7
Training loss: 2.2413275241851807
Validation loss: 2.3844044054708173

Epoch: 6| Step: 8
Training loss: 3.0108108520507812
Validation loss: 2.384991991904474

Epoch: 6| Step: 9
Training loss: 2.4686179161071777
Validation loss: 2.3871801284051712

Epoch: 6| Step: 10
Training loss: 3.365748167037964
Validation loss: 2.354953767150961

Epoch: 6| Step: 11
Training loss: 2.923497200012207
Validation loss: 2.3499942543686076

Epoch: 6| Step: 12
Training loss: 2.6659979820251465
Validation loss: 2.3527590741393385

Epoch: 6| Step: 13
Training loss: 2.2663214206695557
Validation loss: 2.352923817532037

Epoch: 274| Step: 0
Training loss: 1.9245004653930664
Validation loss: 2.344567024579612

Epoch: 6| Step: 1
Training loss: 2.9725866317749023
Validation loss: 2.3419598943443707

Epoch: 6| Step: 2
Training loss: 2.63970685005188
Validation loss: 2.335646870315716

Epoch: 6| Step: 3
Training loss: 2.678555727005005
Validation loss: 2.3462760704819874

Epoch: 6| Step: 4
Training loss: 1.8528496026992798
Validation loss: 2.342612022994667

Epoch: 6| Step: 5
Training loss: 2.809150218963623
Validation loss: 2.341535614382836

Epoch: 6| Step: 6
Training loss: 2.443268299102783
Validation loss: 2.3412662821431316

Epoch: 6| Step: 7
Training loss: 2.653597354888916
Validation loss: 2.3435243124602945

Epoch: 6| Step: 8
Training loss: 2.479609727859497
Validation loss: 2.346262442168369

Epoch: 6| Step: 9
Training loss: 3.1627800464630127
Validation loss: 2.3453093421074653

Epoch: 6| Step: 10
Training loss: 3.0883448123931885
Validation loss: 2.349102880365105

Epoch: 6| Step: 11
Training loss: 2.2513675689697266
Validation loss: 2.354813573180988

Epoch: 6| Step: 12
Training loss: 1.8989475965499878
Validation loss: 2.3629538000270887

Epoch: 6| Step: 13
Training loss: 2.3605265617370605
Validation loss: 2.3620008448118806

Epoch: 275| Step: 0
Training loss: 1.7379567623138428
Validation loss: 2.3636932526865313

Epoch: 6| Step: 1
Training loss: 3.1643307209014893
Validation loss: 2.398214676046884

Epoch: 6| Step: 2
Training loss: 2.5108845233917236
Validation loss: 2.41817565630841

Epoch: 6| Step: 3
Training loss: 2.6037540435791016
Validation loss: 2.4311936337460756

Epoch: 6| Step: 4
Training loss: 2.903146266937256
Validation loss: 2.4314299168125277

Epoch: 6| Step: 5
Training loss: 2.658038854598999
Validation loss: 2.406037590836966

Epoch: 6| Step: 6
Training loss: 2.40712833404541
Validation loss: 2.3835172627561834

Epoch: 6| Step: 7
Training loss: 2.708415985107422
Validation loss: 2.367984402564264

Epoch: 6| Step: 8
Training loss: 2.459752082824707
Validation loss: 2.350744970383183

Epoch: 6| Step: 9
Training loss: 2.5143580436706543
Validation loss: 2.3525461522481774

Epoch: 6| Step: 10
Training loss: 2.152127265930176
Validation loss: 2.3431331521721295

Epoch: 6| Step: 11
Training loss: 2.675650119781494
Validation loss: 2.344284460108767

Epoch: 6| Step: 12
Training loss: 2.7049403190612793
Validation loss: 2.3395092897517706

Epoch: 6| Step: 13
Training loss: 1.6836299896240234
Validation loss: 2.3390487829844155

Epoch: 276| Step: 0
Training loss: 2.231658458709717
Validation loss: 2.3379836800277873

Epoch: 6| Step: 1
Training loss: 2.594003200531006
Validation loss: 2.343614024500693

Epoch: 6| Step: 2
Training loss: 2.247138023376465
Validation loss: 2.344471464874924

Epoch: 6| Step: 3
Training loss: 2.729823589324951
Validation loss: 2.3344062015574467

Epoch: 6| Step: 4
Training loss: 2.005523204803467
Validation loss: 2.3437663842273015

Epoch: 6| Step: 5
Training loss: 2.480288028717041
Validation loss: 2.3380398391395487

Epoch: 6| Step: 6
Training loss: 2.7607955932617188
Validation loss: 2.337123506812639

Epoch: 6| Step: 7
Training loss: 2.3257341384887695
Validation loss: 2.333321281658706

Epoch: 6| Step: 8
Training loss: 2.404439926147461
Validation loss: 2.326495147520496

Epoch: 6| Step: 9
Training loss: 3.4520928859710693
Validation loss: 2.3240291328840357

Epoch: 6| Step: 10
Training loss: 2.8527145385742188
Validation loss: 2.327957889085175

Epoch: 6| Step: 11
Training loss: 2.007246255874634
Validation loss: 2.3319652208717923

Epoch: 6| Step: 12
Training loss: 2.188476324081421
Validation loss: 2.3582470263204267

Epoch: 6| Step: 13
Training loss: 3.2425169944763184
Validation loss: 2.370012411507227

Epoch: 277| Step: 0
Training loss: 2.5482687950134277
Validation loss: 2.3917984988099787

Epoch: 6| Step: 1
Training loss: 3.0571186542510986
Validation loss: 2.393740487355058

Epoch: 6| Step: 2
Training loss: 2.4299979209899902
Validation loss: 2.387143627289803

Epoch: 6| Step: 3
Training loss: 2.396087169647217
Validation loss: 2.382815825041904

Epoch: 6| Step: 4
Training loss: 3.077130079269409
Validation loss: 2.368426461373606

Epoch: 6| Step: 5
Training loss: 2.8246774673461914
Validation loss: 2.362274992850519

Epoch: 6| Step: 6
Training loss: 2.0044713020324707
Validation loss: 2.3530787883266324

Epoch: 6| Step: 7
Training loss: 1.8056509494781494
Validation loss: 2.3459221880923034

Epoch: 6| Step: 8
Training loss: 2.4612278938293457
Validation loss: 2.3406388477612565

Epoch: 6| Step: 9
Training loss: 2.98852276802063
Validation loss: 2.3452237652194117

Epoch: 6| Step: 10
Training loss: 2.753742218017578
Validation loss: 2.342231223660131

Epoch: 6| Step: 11
Training loss: 2.357332706451416
Validation loss: 2.3456043145989858

Epoch: 6| Step: 12
Training loss: 1.9189047813415527
Validation loss: 2.3509038135569584

Epoch: 6| Step: 13
Training loss: 2.5271894931793213
Validation loss: 2.366780978377147

Epoch: 278| Step: 0
Training loss: 2.6030209064483643
Validation loss: 2.36388220325593

Epoch: 6| Step: 1
Training loss: 3.085406541824341
Validation loss: 2.3662005650099887

Epoch: 6| Step: 2
Training loss: 1.5719447135925293
Validation loss: 2.39466231612749

Epoch: 6| Step: 3
Training loss: 3.1388912200927734
Validation loss: 2.395026381297778

Epoch: 6| Step: 4
Training loss: 2.1905570030212402
Validation loss: 2.4097604367040817

Epoch: 6| Step: 5
Training loss: 2.1340363025665283
Validation loss: 2.411813243742912

Epoch: 6| Step: 6
Training loss: 2.2051188945770264
Validation loss: 2.403816597436064

Epoch: 6| Step: 7
Training loss: 3.152416229248047
Validation loss: 2.3797948847534838

Epoch: 6| Step: 8
Training loss: 2.394603729248047
Validation loss: 2.354161704740217

Epoch: 6| Step: 9
Training loss: 2.544882297515869
Validation loss: 2.345106919606527

Epoch: 6| Step: 10
Training loss: 1.6238552331924438
Validation loss: 2.3448863132025606

Epoch: 6| Step: 11
Training loss: 2.874943256378174
Validation loss: 2.3359408096600602

Epoch: 6| Step: 12
Training loss: 2.3946382999420166
Validation loss: 2.329926088292112

Epoch: 6| Step: 13
Training loss: 3.5095415115356445
Validation loss: 2.34067089070556

Epoch: 279| Step: 0
Training loss: 2.1977310180664062
Validation loss: 2.3491506807265745

Epoch: 6| Step: 1
Training loss: 3.6786653995513916
Validation loss: 2.349050127049928

Epoch: 6| Step: 2
Training loss: 2.143907070159912
Validation loss: 2.349271883246719

Epoch: 6| Step: 3
Training loss: 3.2556562423706055
Validation loss: 2.349185007874684

Epoch: 6| Step: 4
Training loss: 2.0708038806915283
Validation loss: 2.3548825402413645

Epoch: 6| Step: 5
Training loss: 2.8960490226745605
Validation loss: 2.3604607992274786

Epoch: 6| Step: 6
Training loss: 2.267609119415283
Validation loss: 2.3472952919621624

Epoch: 6| Step: 7
Training loss: 2.2357594966888428
Validation loss: 2.351027855309107

Epoch: 6| Step: 8
Training loss: 2.285275936126709
Validation loss: 2.3335712007296983

Epoch: 6| Step: 9
Training loss: 2.0836715698242188
Validation loss: 2.3376923248332035

Epoch: 6| Step: 10
Training loss: 2.005208969116211
Validation loss: 2.3415417440475954

Epoch: 6| Step: 11
Training loss: 2.3378853797912598
Validation loss: 2.334073535857662

Epoch: 6| Step: 12
Training loss: 2.612926721572876
Validation loss: 2.3372068469242384

Epoch: 6| Step: 13
Training loss: 3.151698112487793
Validation loss: 2.3534152277054323

Epoch: 280| Step: 0
Training loss: 2.6284570693969727
Validation loss: 2.365707130842311

Epoch: 6| Step: 1
Training loss: 1.367446780204773
Validation loss: 2.3719075854106615

Epoch: 6| Step: 2
Training loss: 3.3737361431121826
Validation loss: 2.3770505817987586

Epoch: 6| Step: 3
Training loss: 2.6396241188049316
Validation loss: 2.3499789058521228

Epoch: 6| Step: 4
Training loss: 2.1831631660461426
Validation loss: 2.329724365665067

Epoch: 6| Step: 5
Training loss: 2.541667938232422
Validation loss: 2.3241042552455777

Epoch: 6| Step: 6
Training loss: 2.8136074542999268
Validation loss: 2.3203076419009956

Epoch: 6| Step: 7
Training loss: 1.7195348739624023
Validation loss: 2.3259408191968034

Epoch: 6| Step: 8
Training loss: 2.6290059089660645
Validation loss: 2.3221638446213095

Epoch: 6| Step: 9
Training loss: 3.1049728393554688
Validation loss: 2.3286909223884664

Epoch: 6| Step: 10
Training loss: 2.4321792125701904
Validation loss: 2.3166396976799093

Epoch: 6| Step: 11
Training loss: 2.1385951042175293
Validation loss: 2.318483470588602

Epoch: 6| Step: 12
Training loss: 2.421245574951172
Validation loss: 2.3482221326520367

Epoch: 6| Step: 13
Training loss: 3.0857441425323486
Validation loss: 2.3555462488564114

Epoch: 281| Step: 0
Training loss: 3.054335117340088
Validation loss: 2.3667216570146623

Epoch: 6| Step: 1
Training loss: 2.2729039192199707
Validation loss: 2.3623369329719135

Epoch: 6| Step: 2
Training loss: 2.926240921020508
Validation loss: 2.3547343643762733

Epoch: 6| Step: 3
Training loss: 1.9033946990966797
Validation loss: 2.3603826210062993

Epoch: 6| Step: 4
Training loss: 2.6685519218444824
Validation loss: 2.3510223998818347

Epoch: 6| Step: 5
Training loss: 2.877272605895996
Validation loss: 2.3378200902733752

Epoch: 6| Step: 6
Training loss: 2.058173179626465
Validation loss: 2.336689461943924

Epoch: 6| Step: 7
Training loss: 3.0276148319244385
Validation loss: 2.3208131841433945

Epoch: 6| Step: 8
Training loss: 2.3277125358581543
Validation loss: 2.3244386039754397

Epoch: 6| Step: 9
Training loss: 3.232408046722412
Validation loss: 2.320462258913184

Epoch: 6| Step: 10
Training loss: 2.3526320457458496
Validation loss: 2.3236565000267437

Epoch: 6| Step: 11
Training loss: 2.6953492164611816
Validation loss: 2.306521656692669

Epoch: 6| Step: 12
Training loss: 1.6935288906097412
Validation loss: 2.3150142469713764

Epoch: 6| Step: 13
Training loss: 1.7366071939468384
Validation loss: 2.3176526408041678

Epoch: 282| Step: 0
Training loss: 2.686894416809082
Validation loss: 2.3231698159248597

Epoch: 6| Step: 1
Training loss: 2.4738409519195557
Validation loss: 2.324441453462006

Epoch: 6| Step: 2
Training loss: 2.16957950592041
Validation loss: 2.330961222289711

Epoch: 6| Step: 3
Training loss: 2.973954439163208
Validation loss: 2.338039364866031

Epoch: 6| Step: 4
Training loss: 2.6159420013427734
Validation loss: 2.3440914410416798

Epoch: 6| Step: 5
Training loss: 3.051722764968872
Validation loss: 2.35509818343706

Epoch: 6| Step: 6
Training loss: 2.7899744510650635
Validation loss: 2.352081186027937

Epoch: 6| Step: 7
Training loss: 2.1719911098480225
Validation loss: 2.3747708823091243

Epoch: 6| Step: 8
Training loss: 2.7082443237304688
Validation loss: 2.3819495631802465

Epoch: 6| Step: 9
Training loss: 2.431269645690918
Validation loss: 2.393007432260821

Epoch: 6| Step: 10
Training loss: 1.7548828125
Validation loss: 2.397201527831375

Epoch: 6| Step: 11
Training loss: 2.43361759185791
Validation loss: 2.3768469313139557

Epoch: 6| Step: 12
Training loss: 1.7623417377471924
Validation loss: 2.3985974455392487

Epoch: 6| Step: 13
Training loss: 2.8539772033691406
Validation loss: 2.387498983772852

Epoch: 283| Step: 0
Training loss: 2.5929675102233887
Validation loss: 2.350772606429233

Epoch: 6| Step: 1
Training loss: 2.324599266052246
Validation loss: 2.342128212733935

Epoch: 6| Step: 2
Training loss: 2.2821874618530273
Validation loss: 2.3197128131825435

Epoch: 6| Step: 3
Training loss: 2.3817684650421143
Validation loss: 2.3121718001622025

Epoch: 6| Step: 4
Training loss: 2.462773561477661
Validation loss: 2.304129126251385

Epoch: 6| Step: 5
Training loss: 2.9710357189178467
Validation loss: 2.3117045074380855

Epoch: 6| Step: 6
Training loss: 3.0845117568969727
Validation loss: 2.2991536509606147

Epoch: 6| Step: 7
Training loss: 2.7442078590393066
Validation loss: 2.3142365153117845

Epoch: 6| Step: 8
Training loss: 2.5052924156188965
Validation loss: 2.304635586277131

Epoch: 6| Step: 9
Training loss: 2.27651309967041
Validation loss: 2.314000011772238

Epoch: 6| Step: 10
Training loss: 2.1498963832855225
Validation loss: 2.305548224397885

Epoch: 6| Step: 11
Training loss: 2.6607351303100586
Validation loss: 2.309092201212401

Epoch: 6| Step: 12
Training loss: 1.7112083435058594
Validation loss: 2.3087306996827484

Epoch: 6| Step: 13
Training loss: 2.8571133613586426
Validation loss: 2.330736797343018

Epoch: 284| Step: 0
Training loss: 2.199986219406128
Validation loss: 2.3283772058384393

Epoch: 6| Step: 1
Training loss: 2.619529962539673
Validation loss: 2.3375168795226724

Epoch: 6| Step: 2
Training loss: 3.077042579650879
Validation loss: 2.3404661276007213

Epoch: 6| Step: 3
Training loss: 2.2739439010620117
Validation loss: 2.368230647938226

Epoch: 6| Step: 4
Training loss: 2.4893147945404053
Validation loss: 2.3826888953485796

Epoch: 6| Step: 5
Training loss: 3.236222743988037
Validation loss: 2.3936256939365017

Epoch: 6| Step: 6
Training loss: 2.541255235671997
Validation loss: 2.3998640301407024

Epoch: 6| Step: 7
Training loss: 2.121962070465088
Validation loss: 2.3987809099176878

Epoch: 6| Step: 8
Training loss: 2.6741063594818115
Validation loss: 2.3897435870221866

Epoch: 6| Step: 9
Training loss: 2.6037826538085938
Validation loss: 2.3774143649685766

Epoch: 6| Step: 10
Training loss: 2.770639419555664
Validation loss: 2.348298539397537

Epoch: 6| Step: 11
Training loss: 1.9942091703414917
Validation loss: 2.3302798219906387

Epoch: 6| Step: 12
Training loss: 2.378514528274536
Validation loss: 2.324571978661322

Epoch: 6| Step: 13
Training loss: 1.6423901319503784
Validation loss: 2.3200861638592136

Epoch: 285| Step: 0
Training loss: 2.283384323120117
Validation loss: 2.311819509793353

Epoch: 6| Step: 1
Training loss: 2.9097237586975098
Validation loss: 2.3326981400930755

Epoch: 6| Step: 2
Training loss: 2.1033437252044678
Validation loss: 2.318741939401114

Epoch: 6| Step: 3
Training loss: 2.3718655109405518
Validation loss: 2.323053075421241

Epoch: 6| Step: 4
Training loss: 2.412628173828125
Validation loss: 2.330579191125849

Epoch: 6| Step: 5
Training loss: 1.9729712009429932
Validation loss: 2.3328844501126196

Epoch: 6| Step: 6
Training loss: 2.725529193878174
Validation loss: 2.359916055074302

Epoch: 6| Step: 7
Training loss: 2.56673002243042
Validation loss: 2.356399659187563

Epoch: 6| Step: 8
Training loss: 2.2835798263549805
Validation loss: 2.3694315289938324

Epoch: 6| Step: 9
Training loss: 2.736056327819824
Validation loss: 2.3619588626328336

Epoch: 6| Step: 10
Training loss: 2.333237648010254
Validation loss: 2.348557528629098

Epoch: 6| Step: 11
Training loss: 2.885523557662964
Validation loss: 2.3359245638693533

Epoch: 6| Step: 12
Training loss: 2.0545082092285156
Validation loss: 2.3286803178889777

Epoch: 6| Step: 13
Training loss: 3.2338807582855225
Validation loss: 2.323637495758713

Epoch: 286| Step: 0
Training loss: 2.1555111408233643
Validation loss: 2.31568750258415

Epoch: 6| Step: 1
Training loss: 2.6472439765930176
Validation loss: 2.318834650901056

Epoch: 6| Step: 2
Training loss: 2.885328531265259
Validation loss: 2.310615690805579

Epoch: 6| Step: 3
Training loss: 3.421114921569824
Validation loss: 2.3077892282957673

Epoch: 6| Step: 4
Training loss: 2.283994197845459
Validation loss: 2.3068236022867183

Epoch: 6| Step: 5
Training loss: 2.262754201889038
Validation loss: 2.323977247361214

Epoch: 6| Step: 6
Training loss: 2.223630905151367
Validation loss: 2.3165241877237954

Epoch: 6| Step: 7
Training loss: 2.626746654510498
Validation loss: 2.3255433882436445

Epoch: 6| Step: 8
Training loss: 2.3719897270202637
Validation loss: 2.318779940246254

Epoch: 6| Step: 9
Training loss: 1.8468348979949951
Validation loss: 2.3218646741682485

Epoch: 6| Step: 10
Training loss: 2.3033552169799805
Validation loss: 2.328510151114515

Epoch: 6| Step: 11
Training loss: 3.0778326988220215
Validation loss: 2.3280725658580823

Epoch: 6| Step: 12
Training loss: 2.140598773956299
Validation loss: 2.337173513186875

Epoch: 6| Step: 13
Training loss: 2.48323917388916
Validation loss: 2.356203202278383

Epoch: 287| Step: 0
Training loss: 2.373650312423706
Validation loss: 2.3711346451954176

Epoch: 6| Step: 1
Training loss: 2.335416793823242
Validation loss: 2.3849469538657897

Epoch: 6| Step: 2
Training loss: 2.291830062866211
Validation loss: 2.394134498411609

Epoch: 6| Step: 3
Training loss: 2.9688920974731445
Validation loss: 2.4087372877264537

Epoch: 6| Step: 4
Training loss: 2.53476619720459
Validation loss: 2.388041624458887

Epoch: 6| Step: 5
Training loss: 2.817594051361084
Validation loss: 2.3789895144842004

Epoch: 6| Step: 6
Training loss: 2.845564603805542
Validation loss: 2.358057946287176

Epoch: 6| Step: 7
Training loss: 1.9672038555145264
Validation loss: 2.3392920545352403

Epoch: 6| Step: 8
Training loss: 2.9839305877685547
Validation loss: 2.320295169789304

Epoch: 6| Step: 9
Training loss: 2.6490015983581543
Validation loss: 2.3087781449799896

Epoch: 6| Step: 10
Training loss: 2.6560826301574707
Validation loss: 2.3179605289172103

Epoch: 6| Step: 11
Training loss: 1.5311124324798584
Validation loss: 2.3051628476829937

Epoch: 6| Step: 12
Training loss: 2.873598098754883
Validation loss: 2.304414851691133

Epoch: 6| Step: 13
Training loss: 1.387279987335205
Validation loss: 2.3081354069453415

Epoch: 288| Step: 0
Training loss: 2.3206005096435547
Validation loss: 2.30606666175268

Epoch: 6| Step: 1
Training loss: 2.242074728012085
Validation loss: 2.341795495761338

Epoch: 6| Step: 2
Training loss: 1.865546703338623
Validation loss: 2.3600853079108783

Epoch: 6| Step: 3
Training loss: 2.6728062629699707
Validation loss: 2.3654009398593696

Epoch: 6| Step: 4
Training loss: 3.0946621894836426
Validation loss: 2.38042192305288

Epoch: 6| Step: 5
Training loss: 2.1063694953918457
Validation loss: 2.3640273540250716

Epoch: 6| Step: 6
Training loss: 2.720268726348877
Validation loss: 2.3443720879093295

Epoch: 6| Step: 7
Training loss: 2.6373867988586426
Validation loss: 2.339194051681026

Epoch: 6| Step: 8
Training loss: 1.9518451690673828
Validation loss: 2.332877750037819

Epoch: 6| Step: 9
Training loss: 1.9932860136032104
Validation loss: 2.32919563529312

Epoch: 6| Step: 10
Training loss: 2.3654866218566895
Validation loss: 2.3244817744019213

Epoch: 6| Step: 11
Training loss: 3.379945755004883
Validation loss: 2.311593481289443

Epoch: 6| Step: 12
Training loss: 2.731029987335205
Validation loss: 2.3196173226961525

Epoch: 6| Step: 13
Training loss: 2.828598976135254
Validation loss: 2.3064119841462825

Epoch: 289| Step: 0
Training loss: 3.2240843772888184
Validation loss: 2.3267177625369

Epoch: 6| Step: 1
Training loss: 2.3527565002441406
Validation loss: 2.3407833704384426

Epoch: 6| Step: 2
Training loss: 2.655071258544922
Validation loss: 2.367075485567893

Epoch: 6| Step: 3
Training loss: 2.3331074714660645
Validation loss: 2.3742267598388014

Epoch: 6| Step: 4
Training loss: 2.3243985176086426
Validation loss: 2.384416469963648

Epoch: 6| Step: 5
Training loss: 3.048529624938965
Validation loss: 2.4174971759960218

Epoch: 6| Step: 6
Training loss: 2.489410877227783
Validation loss: 2.410351992935263

Epoch: 6| Step: 7
Training loss: 2.300858497619629
Validation loss: 2.3705258625809864

Epoch: 6| Step: 8
Training loss: 2.6198549270629883
Validation loss: 2.3512968940119587

Epoch: 6| Step: 9
Training loss: 1.7953991889953613
Validation loss: 2.339390183007845

Epoch: 6| Step: 10
Training loss: 1.8922913074493408
Validation loss: 2.311654131899598

Epoch: 6| Step: 11
Training loss: 2.9731640815734863
Validation loss: 2.287900778555101

Epoch: 6| Step: 12
Training loss: 2.155198574066162
Validation loss: 2.2978283820613736

Epoch: 6| Step: 13
Training loss: 2.65740966796875
Validation loss: 2.303336274239325

Epoch: 290| Step: 0
Training loss: 2.840423107147217
Validation loss: 2.3271156075180217

Epoch: 6| Step: 1
Training loss: 2.1639113426208496
Validation loss: 2.3174287785765944

Epoch: 6| Step: 2
Training loss: 3.059023141860962
Validation loss: 2.3199859614013345

Epoch: 6| Step: 3
Training loss: 2.224972724914551
Validation loss: 2.321566104888916

Epoch: 6| Step: 4
Training loss: 2.42040753364563
Validation loss: 2.3360072258980042

Epoch: 6| Step: 5
Training loss: 2.6545467376708984
Validation loss: 2.3412509656721547

Epoch: 6| Step: 6
Training loss: 2.283154010772705
Validation loss: 2.325066153721143

Epoch: 6| Step: 7
Training loss: 2.0014777183532715
Validation loss: 2.3224887309535855

Epoch: 6| Step: 8
Training loss: 2.180306911468506
Validation loss: 2.3192148003526913

Epoch: 6| Step: 9
Training loss: 2.677170991897583
Validation loss: 2.309395533736034

Epoch: 6| Step: 10
Training loss: 2.534533739089966
Validation loss: 2.3027769083617837

Epoch: 6| Step: 11
Training loss: 2.731113910675049
Validation loss: 2.298641453507126

Epoch: 6| Step: 12
Training loss: 2.481186866760254
Validation loss: 2.301605196409328

Epoch: 6| Step: 13
Training loss: 3.1198387145996094
Validation loss: 2.3174318985272477

Epoch: 291| Step: 0
Training loss: 3.045569896697998
Validation loss: 2.327493690675305

Epoch: 6| Step: 1
Training loss: 2.0726325511932373
Validation loss: 2.354166457729955

Epoch: 6| Step: 2
Training loss: 1.917999029159546
Validation loss: 2.3492719486195552

Epoch: 6| Step: 3
Training loss: 2.6862998008728027
Validation loss: 2.3526068246492775

Epoch: 6| Step: 4
Training loss: 2.4583353996276855
Validation loss: 2.344593696696784

Epoch: 6| Step: 5
Training loss: 2.0167505741119385
Validation loss: 2.340206092403781

Epoch: 6| Step: 6
Training loss: 2.633113384246826
Validation loss: 2.3389487420358965

Epoch: 6| Step: 7
Training loss: 2.412773847579956
Validation loss: 2.314456775624265

Epoch: 6| Step: 8
Training loss: 2.6266839504241943
Validation loss: 2.318465212339996

Epoch: 6| Step: 9
Training loss: 2.489293098449707
Validation loss: 2.314943295653148

Epoch: 6| Step: 10
Training loss: 2.382748603820801
Validation loss: 2.304213285446167

Epoch: 6| Step: 11
Training loss: 2.7410519123077393
Validation loss: 2.302804034243348

Epoch: 6| Step: 12
Training loss: 2.601840019226074
Validation loss: 2.309355471723823

Epoch: 6| Step: 13
Training loss: 2.555039405822754
Validation loss: 2.3078322449038104

Epoch: 292| Step: 0
Training loss: 2.581472396850586
Validation loss: 2.302590529123942

Epoch: 6| Step: 1
Training loss: 2.5307860374450684
Validation loss: 2.3100222003075386

Epoch: 6| Step: 2
Training loss: 2.376213550567627
Validation loss: 2.3092917678176716

Epoch: 6| Step: 3
Training loss: 2.3036670684814453
Validation loss: 2.314531280148414

Epoch: 6| Step: 4
Training loss: 2.8364157676696777
Validation loss: 2.336909355655793

Epoch: 6| Step: 5
Training loss: 1.8265364170074463
Validation loss: 2.3500853584658716

Epoch: 6| Step: 6
Training loss: 2.7772700786590576
Validation loss: 2.352184103381249

Epoch: 6| Step: 7
Training loss: 2.258820056915283
Validation loss: 2.3477317363985124

Epoch: 6| Step: 8
Training loss: 2.1967532634735107
Validation loss: 2.346951566716676

Epoch: 6| Step: 9
Training loss: 1.9928772449493408
Validation loss: 2.3527140976280294

Epoch: 6| Step: 10
Training loss: 2.0513532161712646
Validation loss: 2.352938905838997

Epoch: 6| Step: 11
Training loss: 2.793257236480713
Validation loss: 2.3377845594959874

Epoch: 6| Step: 12
Training loss: 3.378371477127075
Validation loss: 2.3383569948134886

Epoch: 6| Step: 13
Training loss: 2.7063608169555664
Validation loss: 2.335061216867098

Epoch: 293| Step: 0
Training loss: 2.57106351852417
Validation loss: 2.3186671810765422

Epoch: 6| Step: 1
Training loss: 3.0685062408447266
Validation loss: 2.3212278555798274

Epoch: 6| Step: 2
Training loss: 2.1624386310577393
Validation loss: 2.3098578427427556

Epoch: 6| Step: 3
Training loss: 2.5050923824310303
Validation loss: 2.3033581702939925

Epoch: 6| Step: 4
Training loss: 1.8774642944335938
Validation loss: 2.2920327878767446

Epoch: 6| Step: 5
Training loss: 2.5320775508880615
Validation loss: 2.31013245736399

Epoch: 6| Step: 6
Training loss: 2.631289482116699
Validation loss: 2.30144001078862

Epoch: 6| Step: 7
Training loss: 2.469324827194214
Validation loss: 2.30931923466344

Epoch: 6| Step: 8
Training loss: 2.0477843284606934
Validation loss: 2.3074925907196535

Epoch: 6| Step: 9
Training loss: 3.150758743286133
Validation loss: 2.3171237578956028

Epoch: 6| Step: 10
Training loss: 2.066032886505127
Validation loss: 2.322015490583194

Epoch: 6| Step: 11
Training loss: 2.182792901992798
Validation loss: 2.304890424974503

Epoch: 6| Step: 12
Training loss: 3.1730456352233887
Validation loss: 2.332200490018373

Epoch: 6| Step: 13
Training loss: 1.8486099243164062
Validation loss: 2.3428039012416715

Epoch: 294| Step: 0
Training loss: 2.125847339630127
Validation loss: 2.359087395411666

Epoch: 6| Step: 1
Training loss: 2.365342855453491
Validation loss: 2.3809931226955947

Epoch: 6| Step: 2
Training loss: 2.655367851257324
Validation loss: 2.376493056615194

Epoch: 6| Step: 3
Training loss: 2.7527976036071777
Validation loss: 2.3822493886434906

Epoch: 6| Step: 4
Training loss: 2.7099952697753906
Validation loss: 2.3826470451970256

Epoch: 6| Step: 5
Training loss: 2.38169002532959
Validation loss: 2.3692791410671767

Epoch: 6| Step: 6
Training loss: 2.3104677200317383
Validation loss: 2.3439011778882755

Epoch: 6| Step: 7
Training loss: 2.550173282623291
Validation loss: 2.344068434930617

Epoch: 6| Step: 8
Training loss: 2.4323506355285645
Validation loss: 2.350382959970864

Epoch: 6| Step: 9
Training loss: 2.421855926513672
Validation loss: 2.3404431958352365

Epoch: 6| Step: 10
Training loss: 2.6306328773498535
Validation loss: 2.3470592062960387

Epoch: 6| Step: 11
Training loss: 2.75522780418396
Validation loss: 2.32433763370719

Epoch: 6| Step: 12
Training loss: 1.9739470481872559
Validation loss: 2.3193016411155782

Epoch: 6| Step: 13
Training loss: 2.7084624767303467
Validation loss: 2.3044650657202608

Epoch: 295| Step: 0
Training loss: 2.1796631813049316
Validation loss: 2.3012739663482993

Epoch: 6| Step: 1
Training loss: 2.1849136352539062
Validation loss: 2.302045460670225

Epoch: 6| Step: 2
Training loss: 3.1802330017089844
Validation loss: 2.306134157283332

Epoch: 6| Step: 3
Training loss: 2.472184419631958
Validation loss: 2.2988718555819605

Epoch: 6| Step: 4
Training loss: 3.0037660598754883
Validation loss: 2.304846194482619

Epoch: 6| Step: 5
Training loss: 2.5571422576904297
Validation loss: 2.307329129147273

Epoch: 6| Step: 6
Training loss: 2.938261032104492
Validation loss: 2.332495574028261

Epoch: 6| Step: 7
Training loss: 2.7196836471557617
Validation loss: 2.347365956152639

Epoch: 6| Step: 8
Training loss: 1.729475736618042
Validation loss: 2.3400317674042075

Epoch: 6| Step: 9
Training loss: 1.8482434749603271
Validation loss: 2.3438717037118892

Epoch: 6| Step: 10
Training loss: 2.23358416557312
Validation loss: 2.3361194723395893

Epoch: 6| Step: 11
Training loss: 1.7194695472717285
Validation loss: 2.3279218032795894

Epoch: 6| Step: 12
Training loss: 2.8476674556732178
Validation loss: 2.3142769003427155

Epoch: 6| Step: 13
Training loss: 2.8900532722473145
Validation loss: 2.3205437173125563

Epoch: 296| Step: 0
Training loss: 2.0830588340759277
Validation loss: 2.3175944923072733

Epoch: 6| Step: 1
Training loss: 2.102675437927246
Validation loss: 2.3316458809760308

Epoch: 6| Step: 2
Training loss: 2.6528453826904297
Validation loss: 2.346950879660986

Epoch: 6| Step: 3
Training loss: 2.270693302154541
Validation loss: 2.335570549452177

Epoch: 6| Step: 4
Training loss: 3.43574857711792
Validation loss: 2.3520738617066415

Epoch: 6| Step: 5
Training loss: 2.466196060180664
Validation loss: 2.3253403837962816

Epoch: 6| Step: 6
Training loss: 2.6210780143737793
Validation loss: 2.29748603861819

Epoch: 6| Step: 7
Training loss: 2.3638484477996826
Validation loss: 2.298209974842687

Epoch: 6| Step: 8
Training loss: 2.2983579635620117
Validation loss: 2.308232940653319

Epoch: 6| Step: 9
Training loss: 2.1055939197540283
Validation loss: 2.3104809048355266

Epoch: 6| Step: 10
Training loss: 2.5507848262786865
Validation loss: 2.310121259381694

Epoch: 6| Step: 11
Training loss: 2.1108856201171875
Validation loss: 2.3174312499261673

Epoch: 6| Step: 12
Training loss: 2.5309009552001953
Validation loss: 2.3293157521114556

Epoch: 6| Step: 13
Training loss: 2.583160638809204
Validation loss: 2.337405389355075

Epoch: 297| Step: 0
Training loss: 2.5200603008270264
Validation loss: 2.3374647043084584

Epoch: 6| Step: 1
Training loss: 1.5751700401306152
Validation loss: 2.3343455688927763

Epoch: 6| Step: 2
Training loss: 2.7077412605285645
Validation loss: 2.327751839032737

Epoch: 6| Step: 3
Training loss: 2.224569797515869
Validation loss: 2.3188815552701234

Epoch: 6| Step: 4
Training loss: 2.5115442276000977
Validation loss: 2.3300004800160727

Epoch: 6| Step: 5
Training loss: 2.9805655479431152
Validation loss: 2.33442267551217

Epoch: 6| Step: 6
Training loss: 2.3719406127929688
Validation loss: 2.3258685399127264

Epoch: 6| Step: 7
Training loss: 2.669783592224121
Validation loss: 2.319401269317955

Epoch: 6| Step: 8
Training loss: 2.9687509536743164
Validation loss: 2.3037600645454983

Epoch: 6| Step: 9
Training loss: 2.5740318298339844
Validation loss: 2.313008536574661

Epoch: 6| Step: 10
Training loss: 1.9974455833435059
Validation loss: 2.301666053392554

Epoch: 6| Step: 11
Training loss: 1.8248707056045532
Validation loss: 2.310018403555757

Epoch: 6| Step: 12
Training loss: 2.641130208969116
Validation loss: 2.3282738103661487

Epoch: 6| Step: 13
Training loss: 2.9711930751800537
Validation loss: 2.3230350555912143

Epoch: 298| Step: 0
Training loss: 2.691970109939575
Validation loss: 2.3307423976159867

Epoch: 6| Step: 1
Training loss: 2.856203556060791
Validation loss: 2.3355804028049594

Epoch: 6| Step: 2
Training loss: 2.5328478813171387
Validation loss: 2.3137293989940355

Epoch: 6| Step: 3
Training loss: 1.9955590963363647
Validation loss: 2.3022777060026764

Epoch: 6| Step: 4
Training loss: 3.0681114196777344
Validation loss: 2.291855248071814

Epoch: 6| Step: 5
Training loss: 1.5783865451812744
Validation loss: 2.28602074679508

Epoch: 6| Step: 6
Training loss: 2.994861364364624
Validation loss: 2.2708225788608676

Epoch: 6| Step: 7
Training loss: 1.720318078994751
Validation loss: 2.2673830037475913

Epoch: 6| Step: 8
Training loss: 2.8056893348693848
Validation loss: 2.2783157492196686

Epoch: 6| Step: 9
Training loss: 2.1785123348236084
Validation loss: 2.2772219155424382

Epoch: 6| Step: 10
Training loss: 2.647411584854126
Validation loss: 2.3023476241737284

Epoch: 6| Step: 11
Training loss: 2.474368095397949
Validation loss: 2.3116426801168792

Epoch: 6| Step: 12
Training loss: 3.067824125289917
Validation loss: 2.29950136779457

Epoch: 6| Step: 13
Training loss: 1.0320430994033813
Validation loss: 2.332577028582173

Epoch: 299| Step: 0
Training loss: 1.9979655742645264
Validation loss: 2.340060310979043

Epoch: 6| Step: 1
Training loss: 2.1121158599853516
Validation loss: 2.351029460148145

Epoch: 6| Step: 2
Training loss: 2.9638099670410156
Validation loss: 2.3847899026768182

Epoch: 6| Step: 3
Training loss: 2.0541791915893555
Validation loss: 2.3852170616067867

Epoch: 6| Step: 4
Training loss: 2.513659715652466
Validation loss: 2.4044406747305267

Epoch: 6| Step: 5
Training loss: 2.688507318496704
Validation loss: 2.406385034643194

Epoch: 6| Step: 6
Training loss: 3.0803420543670654
Validation loss: 2.3931350272188903

Epoch: 6| Step: 7
Training loss: 2.5064234733581543
Validation loss: 2.3803555170694985

Epoch: 6| Step: 8
Training loss: 2.7353594303131104
Validation loss: 2.372329222258701

Epoch: 6| Step: 9
Training loss: 2.8204727172851562
Validation loss: 2.3307877484188286

Epoch: 6| Step: 10
Training loss: 2.6851096153259277
Validation loss: 2.3255340053189184

Epoch: 6| Step: 11
Training loss: 2.1601858139038086
Validation loss: 2.314799818941342

Epoch: 6| Step: 12
Training loss: 1.9054934978485107
Validation loss: 2.3102673343432847

Epoch: 6| Step: 13
Training loss: 2.484719753265381
Validation loss: 2.2968701034463863

Epoch: 300| Step: 0
Training loss: 2.147257089614868
Validation loss: 2.3042128444999777

Epoch: 6| Step: 1
Training loss: 2.7297956943511963
Validation loss: 2.2956480672282558

Epoch: 6| Step: 2
Training loss: 2.382986545562744
Validation loss: 2.3077969345995175

Epoch: 6| Step: 3
Training loss: 3.0387778282165527
Validation loss: 2.308380606353924

Epoch: 6| Step: 4
Training loss: 2.2628040313720703
Validation loss: 2.303197714590257

Epoch: 6| Step: 5
Training loss: 2.226627826690674
Validation loss: 2.303578381897301

Epoch: 6| Step: 6
Training loss: 1.971794605255127
Validation loss: 2.3014247904541674

Epoch: 6| Step: 7
Training loss: 2.697848320007324
Validation loss: 2.3156360426256732

Epoch: 6| Step: 8
Training loss: 2.106038808822632
Validation loss: 2.3064065325644707

Epoch: 6| Step: 9
Training loss: 2.60018253326416
Validation loss: 2.298613425224058

Epoch: 6| Step: 10
Training loss: 2.787430763244629
Validation loss: 2.3220924844024

Epoch: 6| Step: 11
Training loss: 1.9466333389282227
Validation loss: 2.3132460988977903

Epoch: 6| Step: 12
Training loss: 2.762418746948242
Validation loss: 2.3281633700093916

Epoch: 6| Step: 13
Training loss: 2.8506784439086914
Validation loss: 2.348844579471055

Epoch: 301| Step: 0
Training loss: 3.2583765983581543
Validation loss: 2.367766039345854

Epoch: 6| Step: 1
Training loss: 2.1949005126953125
Validation loss: 2.384187961137423

Epoch: 6| Step: 2
Training loss: 1.764580249786377
Validation loss: 2.381897864803191

Epoch: 6| Step: 3
Training loss: 2.995147466659546
Validation loss: 2.377736691505678

Epoch: 6| Step: 4
Training loss: 2.164477586746216
Validation loss: 2.368996653505551

Epoch: 6| Step: 5
Training loss: 2.0154495239257812
Validation loss: 2.367386956368723

Epoch: 6| Step: 6
Training loss: 1.9910199642181396
Validation loss: 2.3569273512850524

Epoch: 6| Step: 7
Training loss: 2.8101139068603516
Validation loss: 2.3471787693679973

Epoch: 6| Step: 8
Training loss: 2.716681480407715
Validation loss: 2.333803535789572

Epoch: 6| Step: 9
Training loss: 2.5002260208129883
Validation loss: 2.3110279575470956

Epoch: 6| Step: 10
Training loss: 2.1548244953155518
Validation loss: 2.312454946579472

Epoch: 6| Step: 11
Training loss: 2.6571619510650635
Validation loss: 2.3153428569916756

Epoch: 6| Step: 12
Training loss: 2.9821057319641113
Validation loss: 2.308970438536777

Epoch: 6| Step: 13
Training loss: 2.026677131652832
Validation loss: 2.3037424113160823

Epoch: 302| Step: 0
Training loss: 1.8264307975769043
Validation loss: 2.305015153782342

Epoch: 6| Step: 1
Training loss: 2.947866439819336
Validation loss: 2.308422355241673

Epoch: 6| Step: 2
Training loss: 2.502119779586792
Validation loss: 2.304194946442881

Epoch: 6| Step: 3
Training loss: 2.0048954486846924
Validation loss: 2.3030579423391693

Epoch: 6| Step: 4
Training loss: 2.720177173614502
Validation loss: 2.3020277946226058

Epoch: 6| Step: 5
Training loss: 2.6486477851867676
Validation loss: 2.298681382210024

Epoch: 6| Step: 6
Training loss: 2.583655834197998
Validation loss: 2.2971018514325543

Epoch: 6| Step: 7
Training loss: 2.814458131790161
Validation loss: 2.2947940980234454

Epoch: 6| Step: 8
Training loss: 1.8937366008758545
Validation loss: 2.281278473074718

Epoch: 6| Step: 9
Training loss: 3.0351152420043945
Validation loss: 2.291217445045389

Epoch: 6| Step: 10
Training loss: 1.8231348991394043
Validation loss: 2.2735803716926166

Epoch: 6| Step: 11
Training loss: 2.023087978363037
Validation loss: 2.2807933515118015

Epoch: 6| Step: 12
Training loss: 2.9186575412750244
Validation loss: 2.2868079088067494

Epoch: 6| Step: 13
Training loss: 2.8569576740264893
Validation loss: 2.2990511745534916

Epoch: 303| Step: 0
Training loss: 1.8474211692810059
Validation loss: 2.3096702867938625

Epoch: 6| Step: 1
Training loss: 1.9797139167785645
Validation loss: 2.33968476710781

Epoch: 6| Step: 2
Training loss: 2.756499767303467
Validation loss: 2.329713116409958

Epoch: 6| Step: 3
Training loss: 1.7751922607421875
Validation loss: 2.3239931483422556

Epoch: 6| Step: 4
Training loss: 2.4272942543029785
Validation loss: 2.3239761937049126

Epoch: 6| Step: 5
Training loss: 2.8161253929138184
Validation loss: 2.3365279987294185

Epoch: 6| Step: 6
Training loss: 2.6874327659606934
Validation loss: 2.3389987689192577

Epoch: 6| Step: 7
Training loss: 3.052455186843872
Validation loss: 2.327652987613473

Epoch: 6| Step: 8
Training loss: 2.5521938800811768
Validation loss: 2.333803694735291

Epoch: 6| Step: 9
Training loss: 2.1913909912109375
Validation loss: 2.3431681484304447

Epoch: 6| Step: 10
Training loss: 2.2397282123565674
Validation loss: 2.3318483701316257

Epoch: 6| Step: 11
Training loss: 2.706516742706299
Validation loss: 2.342029725351641

Epoch: 6| Step: 12
Training loss: 3.167691469192505
Validation loss: 2.3347057296383764

Epoch: 6| Step: 13
Training loss: 1.9963141679763794
Validation loss: 2.330647627512614

Epoch: 304| Step: 0
Training loss: 2.171858549118042
Validation loss: 2.320141084732548

Epoch: 6| Step: 1
Training loss: 1.447249412536621
Validation loss: 2.3136809590042278

Epoch: 6| Step: 2
Training loss: 3.0504603385925293
Validation loss: 2.3133895781732376

Epoch: 6| Step: 3
Training loss: 2.4151296615600586
Validation loss: 2.3024021784464517

Epoch: 6| Step: 4
Training loss: 2.4477477073669434
Validation loss: 2.3131035758603002

Epoch: 6| Step: 5
Training loss: 1.9174587726593018
Validation loss: 2.298278926521219

Epoch: 6| Step: 6
Training loss: 1.826706886291504
Validation loss: 2.2937366424068326

Epoch: 6| Step: 7
Training loss: 2.0807881355285645
Validation loss: 2.2841851070363033

Epoch: 6| Step: 8
Training loss: 2.5872268676757812
Validation loss: 2.2879318344977593

Epoch: 6| Step: 9
Training loss: 2.743067502975464
Validation loss: 2.2891999829200005

Epoch: 6| Step: 10
Training loss: 2.186941623687744
Validation loss: 2.2939201016579904

Epoch: 6| Step: 11
Training loss: 2.8894102573394775
Validation loss: 2.312190653175436

Epoch: 6| Step: 12
Training loss: 3.4008283615112305
Validation loss: 2.3114661708954842

Epoch: 6| Step: 13
Training loss: 3.2108547687530518
Validation loss: 2.3343263082606818

Epoch: 305| Step: 0
Training loss: 2.4926950931549072
Validation loss: 2.3319158656622774

Epoch: 6| Step: 1
Training loss: 2.2665767669677734
Validation loss: 2.341132599820373

Epoch: 6| Step: 2
Training loss: 2.391737937927246
Validation loss: 2.3458404617924846

Epoch: 6| Step: 3
Training loss: 3.218235969543457
Validation loss: 2.335122505823771

Epoch: 6| Step: 4
Training loss: 2.7424821853637695
Validation loss: 2.3283217286550872

Epoch: 6| Step: 5
Training loss: 2.0992813110351562
Validation loss: 2.3127909809030514

Epoch: 6| Step: 6
Training loss: 1.2961256504058838
Validation loss: 2.3093741786095405

Epoch: 6| Step: 7
Training loss: 2.768071413040161
Validation loss: 2.2940038481066303

Epoch: 6| Step: 8
Training loss: 2.570441246032715
Validation loss: 2.2706221713814685

Epoch: 6| Step: 9
Training loss: 2.2696189880371094
Validation loss: 2.262969522066014

Epoch: 6| Step: 10
Training loss: 2.4584879875183105
Validation loss: 2.270004782625424

Epoch: 6| Step: 11
Training loss: 3.1056976318359375
Validation loss: 2.2504374147743307

Epoch: 6| Step: 12
Training loss: 2.1696345806121826
Validation loss: 2.2725520492881857

Epoch: 6| Step: 13
Training loss: 2.5859479904174805
Validation loss: 2.2549571350056636

Epoch: 306| Step: 0
Training loss: 2.4170196056365967
Validation loss: 2.2682939037199943

Epoch: 6| Step: 1
Training loss: 1.8760753870010376
Validation loss: 2.2855896949768066

Epoch: 6| Step: 2
Training loss: 2.274531364440918
Validation loss: 2.2792572949522283

Epoch: 6| Step: 3
Training loss: 2.548156261444092
Validation loss: 2.2919123300942044

Epoch: 6| Step: 4
Training loss: 2.4118847846984863
Validation loss: 2.286213101879243

Epoch: 6| Step: 5
Training loss: 1.9932191371917725
Validation loss: 2.2961807097158125

Epoch: 6| Step: 6
Training loss: 2.092745304107666
Validation loss: 2.301903206815002

Epoch: 6| Step: 7
Training loss: 2.431696653366089
Validation loss: 2.2928734684503205

Epoch: 6| Step: 8
Training loss: 2.8398733139038086
Validation loss: 2.293017707845216

Epoch: 6| Step: 9
Training loss: 3.100421905517578
Validation loss: 2.2981475219931653

Epoch: 6| Step: 10
Training loss: 2.643137216567993
Validation loss: 2.2991680791301112

Epoch: 6| Step: 11
Training loss: 3.414278030395508
Validation loss: 2.302001746751929

Epoch: 6| Step: 12
Training loss: 1.7002540826797485
Validation loss: 2.298922638739309

Epoch: 6| Step: 13
Training loss: 2.434880256652832
Validation loss: 2.305353767128401

Epoch: 307| Step: 0
Training loss: 2.475611925125122
Validation loss: 2.293679416820567

Epoch: 6| Step: 1
Training loss: 2.289743661880493
Validation loss: 2.3037260219614994

Epoch: 6| Step: 2
Training loss: 2.780822515487671
Validation loss: 2.317080310595933

Epoch: 6| Step: 3
Training loss: 1.8467069864273071
Validation loss: 2.325886518724503

Epoch: 6| Step: 4
Training loss: 2.694288730621338
Validation loss: 2.2996603109503306

Epoch: 6| Step: 5
Training loss: 2.750674247741699
Validation loss: 2.3069658561419417

Epoch: 6| Step: 6
Training loss: 2.4264492988586426
Validation loss: 2.295331706282913

Epoch: 6| Step: 7
Training loss: 2.7578182220458984
Validation loss: 2.2822648684183755

Epoch: 6| Step: 8
Training loss: 2.827423572540283
Validation loss: 2.2763623524737615

Epoch: 6| Step: 9
Training loss: 2.2164363861083984
Validation loss: 2.259088827717689

Epoch: 6| Step: 10
Training loss: 2.6006267070770264
Validation loss: 2.263555424187773

Epoch: 6| Step: 11
Training loss: 2.161536693572998
Validation loss: 2.26477651954979

Epoch: 6| Step: 12
Training loss: 2.162672996520996
Validation loss: 2.2606669702837543

Epoch: 6| Step: 13
Training loss: 1.9712401628494263
Validation loss: 2.258006613741639

Epoch: 308| Step: 0
Training loss: 2.2585206031799316
Validation loss: 2.2586901854443293

Epoch: 6| Step: 1
Training loss: 2.860764741897583
Validation loss: 2.265687902768453

Epoch: 6| Step: 2
Training loss: 1.4870966672897339
Validation loss: 2.26652939345247

Epoch: 6| Step: 3
Training loss: 2.4643187522888184
Validation loss: 2.273181738391999

Epoch: 6| Step: 4
Training loss: 2.7538299560546875
Validation loss: 2.2732923774309057

Epoch: 6| Step: 5
Training loss: 1.870213508605957
Validation loss: 2.2818688243948

Epoch: 6| Step: 6
Training loss: 2.5516436100006104
Validation loss: 2.2798044502094226

Epoch: 6| Step: 7
Training loss: 2.9080235958099365
Validation loss: 2.2905649959400134

Epoch: 6| Step: 8
Training loss: 2.696118116378784
Validation loss: 2.3031142552693686

Epoch: 6| Step: 9
Training loss: 2.2520761489868164
Validation loss: 2.295712717117802

Epoch: 6| Step: 10
Training loss: 1.8406438827514648
Validation loss: 2.308291435241699

Epoch: 6| Step: 11
Training loss: 3.11218523979187
Validation loss: 2.3075900770002797

Epoch: 6| Step: 12
Training loss: 2.2379257678985596
Validation loss: 2.315099395731444

Epoch: 6| Step: 13
Training loss: 2.5025243759155273
Validation loss: 2.3216498282647904

Epoch: 309| Step: 0
Training loss: 2.163914680480957
Validation loss: 2.341365452735655

Epoch: 6| Step: 1
Training loss: 2.118985891342163
Validation loss: 2.3491462840828845

Epoch: 6| Step: 2
Training loss: 2.584463119506836
Validation loss: 2.350175044869864

Epoch: 6| Step: 3
Training loss: 2.4313976764678955
Validation loss: 2.3580481339526433

Epoch: 6| Step: 4
Training loss: 2.9800467491149902
Validation loss: 2.3643089109851467

Epoch: 6| Step: 5
Training loss: 2.1870062351226807
Validation loss: 2.327656299837174

Epoch: 6| Step: 6
Training loss: 2.371180534362793
Validation loss: 2.306481510080317

Epoch: 6| Step: 7
Training loss: 2.0891244411468506
Validation loss: 2.295222067063855

Epoch: 6| Step: 8
Training loss: 3.600442409515381
Validation loss: 2.2768306809086956

Epoch: 6| Step: 9
Training loss: 2.2150325775146484
Validation loss: 2.272918007707083

Epoch: 6| Step: 10
Training loss: 3.1928677558898926
Validation loss: 2.260899137425166

Epoch: 6| Step: 11
Training loss: 2.12353777885437
Validation loss: 2.2608627068099154

Epoch: 6| Step: 12
Training loss: 2.1158671379089355
Validation loss: 2.2631688912709556

Epoch: 6| Step: 13
Training loss: 1.4614031314849854
Validation loss: 2.2562917458113803

Epoch: 310| Step: 0
Training loss: 2.248093843460083
Validation loss: 2.257894162208803

Epoch: 6| Step: 1
Training loss: 2.117581605911255
Validation loss: 2.2614108849597234

Epoch: 6| Step: 2
Training loss: 2.4959218502044678
Validation loss: 2.260275243431009

Epoch: 6| Step: 3
Training loss: 2.2271387577056885
Validation loss: 2.2521357433770293

Epoch: 6| Step: 4
Training loss: 2.8619234561920166
Validation loss: 2.2532343172257945

Epoch: 6| Step: 5
Training loss: 1.8477600812911987
Validation loss: 2.2603696238610054

Epoch: 6| Step: 6
Training loss: 2.8449130058288574
Validation loss: 2.2631833707132647

Epoch: 6| Step: 7
Training loss: 3.1859006881713867
Validation loss: 2.256989550846879

Epoch: 6| Step: 8
Training loss: 2.6082963943481445
Validation loss: 2.265455675381486

Epoch: 6| Step: 9
Training loss: 2.059065341949463
Validation loss: 2.263203274819159

Epoch: 6| Step: 10
Training loss: 2.5344772338867188
Validation loss: 2.2925686720878846

Epoch: 6| Step: 11
Training loss: 2.444624423980713
Validation loss: 2.2755159408815446

Epoch: 6| Step: 12
Training loss: 2.3175837993621826
Validation loss: 2.3034195707690333

Epoch: 6| Step: 13
Training loss: 1.9395735263824463
Validation loss: 2.313366477207471

Epoch: 311| Step: 0
Training loss: 2.4406847953796387
Validation loss: 2.3137168730458906

Epoch: 6| Step: 1
Training loss: 2.4258227348327637
Validation loss: 2.2920242765898347

Epoch: 6| Step: 2
Training loss: 2.440183162689209
Validation loss: 2.2886257389540314

Epoch: 6| Step: 3
Training loss: 1.8690171241760254
Validation loss: 2.2869873918512815

Epoch: 6| Step: 4
Training loss: 2.5966219902038574
Validation loss: 2.275405042914934

Epoch: 6| Step: 5
Training loss: 2.0359508991241455
Validation loss: 2.27191008803665

Epoch: 6| Step: 6
Training loss: 2.3121039867401123
Validation loss: 2.2691425546523063

Epoch: 6| Step: 7
Training loss: 2.437592029571533
Validation loss: 2.2731520475879794

Epoch: 6| Step: 8
Training loss: 2.403125762939453
Validation loss: 2.2839645493415093

Epoch: 6| Step: 9
Training loss: 2.147691488265991
Validation loss: 2.2886268323467625

Epoch: 6| Step: 10
Training loss: 2.7073583602905273
Validation loss: 2.2963488012231807

Epoch: 6| Step: 11
Training loss: 2.9240286350250244
Validation loss: 2.3129909576908236

Epoch: 6| Step: 12
Training loss: 2.344675302505493
Validation loss: 2.330758980525437

Epoch: 6| Step: 13
Training loss: 3.021338939666748
Validation loss: 2.329295548059607

Epoch: 312| Step: 0
Training loss: 2.203035831451416
Validation loss: 2.3431213876252532

Epoch: 6| Step: 1
Training loss: 3.066842794418335
Validation loss: 2.3300889051088722

Epoch: 6| Step: 2
Training loss: 3.0390117168426514
Validation loss: 2.3020742375363588

Epoch: 6| Step: 3
Training loss: 2.2633097171783447
Validation loss: 2.283814517400598

Epoch: 6| Step: 4
Training loss: 1.7897478342056274
Validation loss: 2.2760106196967502

Epoch: 6| Step: 5
Training loss: 2.337827205657959
Validation loss: 2.2718939063369588

Epoch: 6| Step: 6
Training loss: 2.7522854804992676
Validation loss: 2.2684223882613646

Epoch: 6| Step: 7
Training loss: 2.590487003326416
Validation loss: 2.2547049099399197

Epoch: 6| Step: 8
Training loss: 2.462963342666626
Validation loss: 2.257628248583886

Epoch: 6| Step: 9
Training loss: 1.7718303203582764
Validation loss: 2.2608094958848852

Epoch: 6| Step: 10
Training loss: 2.961386203765869
Validation loss: 2.2614229635525773

Epoch: 6| Step: 11
Training loss: 2.047412872314453
Validation loss: 2.262404667433872

Epoch: 6| Step: 12
Training loss: 2.348196029663086
Validation loss: 2.258363705809398

Epoch: 6| Step: 13
Training loss: 2.1217360496520996
Validation loss: 2.2864018486392115

Epoch: 313| Step: 0
Training loss: 2.6371278762817383
Validation loss: 2.3022736964687223

Epoch: 6| Step: 1
Training loss: 2.4740655422210693
Validation loss: 2.311207189354845

Epoch: 6| Step: 2
Training loss: 2.700277090072632
Validation loss: 2.2839400101733465

Epoch: 6| Step: 3
Training loss: 2.839205503463745
Validation loss: 2.2886172110034573

Epoch: 6| Step: 4
Training loss: 1.583981990814209
Validation loss: 2.2690403846002396

Epoch: 6| Step: 5
Training loss: 2.7892680168151855
Validation loss: 2.2744600490857194

Epoch: 6| Step: 6
Training loss: 3.2002921104431152
Validation loss: 2.277692792236164

Epoch: 6| Step: 7
Training loss: 1.8843187093734741
Validation loss: 2.268748503859325

Epoch: 6| Step: 8
Training loss: 2.528136968612671
Validation loss: 2.272209543053822

Epoch: 6| Step: 9
Training loss: 2.5106282234191895
Validation loss: 2.27696120098073

Epoch: 6| Step: 10
Training loss: 2.9067931175231934
Validation loss: 2.2826270082945466

Epoch: 6| Step: 11
Training loss: 1.6476621627807617
Validation loss: 2.2686796880537465

Epoch: 6| Step: 12
Training loss: 1.857926368713379
Validation loss: 2.2882717117186515

Epoch: 6| Step: 13
Training loss: 2.512702703475952
Validation loss: 2.2800260102877052

Epoch: 314| Step: 0
Training loss: 2.2876710891723633
Validation loss: 2.2753865334295456

Epoch: 6| Step: 1
Training loss: 2.018178701400757
Validation loss: 2.3138717759040093

Epoch: 6| Step: 2
Training loss: 2.3895158767700195
Validation loss: 2.3214772734590756

Epoch: 6| Step: 3
Training loss: 1.887751579284668
Validation loss: 2.321071242773405

Epoch: 6| Step: 4
Training loss: 2.3401010036468506
Validation loss: 2.3435502975217757

Epoch: 6| Step: 5
Training loss: 3.3501715660095215
Validation loss: 2.3737026363290767

Epoch: 6| Step: 6
Training loss: 1.9786418676376343
Validation loss: 2.348446540935065

Epoch: 6| Step: 7
Training loss: 2.6471140384674072
Validation loss: 2.323707421620687

Epoch: 6| Step: 8
Training loss: 2.9376964569091797
Validation loss: 2.296413860013408

Epoch: 6| Step: 9
Training loss: 2.2337658405303955
Validation loss: 2.270207779381865

Epoch: 6| Step: 10
Training loss: 2.7363083362579346
Validation loss: 2.2500437998002574

Epoch: 6| Step: 11
Training loss: 2.0021305084228516
Validation loss: 2.2666782076640795

Epoch: 6| Step: 12
Training loss: 2.695167064666748
Validation loss: 2.2637698188904793

Epoch: 6| Step: 13
Training loss: 2.6519651412963867
Validation loss: 2.2713125085317962

Epoch: 315| Step: 0
Training loss: 1.5785634517669678
Validation loss: 2.27661822688195

Epoch: 6| Step: 1
Training loss: 2.3280394077301025
Validation loss: 2.2748387013712237

Epoch: 6| Step: 2
Training loss: 2.780212879180908
Validation loss: 2.283871140531314

Epoch: 6| Step: 3
Training loss: 2.5239510536193848
Validation loss: 2.268697589956304

Epoch: 6| Step: 4
Training loss: 2.961087942123413
Validation loss: 2.2715289208196823

Epoch: 6| Step: 5
Training loss: 1.878087043762207
Validation loss: 2.2777978656112507

Epoch: 6| Step: 6
Training loss: 1.8944408893585205
Validation loss: 2.290568615800591

Epoch: 6| Step: 7
Training loss: 2.6539440155029297
Validation loss: 2.28906245898175

Epoch: 6| Step: 8
Training loss: 2.802123785018921
Validation loss: 2.2955612213380876

Epoch: 6| Step: 9
Training loss: 2.484164237976074
Validation loss: 2.293111857547555

Epoch: 6| Step: 10
Training loss: 1.9892948865890503
Validation loss: 2.29882247729968

Epoch: 6| Step: 11
Training loss: 2.8561763763427734
Validation loss: 2.2977033456166587

Epoch: 6| Step: 12
Training loss: 2.5084481239318848
Validation loss: 2.301944440410983

Epoch: 6| Step: 13
Training loss: 2.629392147064209
Validation loss: 2.298398833121023

Epoch: 316| Step: 0
Training loss: 2.9076123237609863
Validation loss: 2.2871250080805954

Epoch: 6| Step: 1
Training loss: 2.6836798191070557
Validation loss: 2.290920408823157

Epoch: 6| Step: 2
Training loss: 2.431380271911621
Validation loss: 2.2836524850578717

Epoch: 6| Step: 3
Training loss: 2.7166671752929688
Validation loss: 2.2729670552797216

Epoch: 6| Step: 4
Training loss: 1.8615175485610962
Validation loss: 2.2672300518199964

Epoch: 6| Step: 5
Training loss: 1.8411688804626465
Validation loss: 2.2563856058223273

Epoch: 6| Step: 6
Training loss: 2.778529167175293
Validation loss: 2.2617866941677627

Epoch: 6| Step: 7
Training loss: 2.725334882736206
Validation loss: 2.2653813118575723

Epoch: 6| Step: 8
Training loss: 1.6106164455413818
Validation loss: 2.268848866544744

Epoch: 6| Step: 9
Training loss: 1.7781065702438354
Validation loss: 2.2582864351170038

Epoch: 6| Step: 10
Training loss: 2.873579502105713
Validation loss: 2.2596867648504113

Epoch: 6| Step: 11
Training loss: 2.22446870803833
Validation loss: 2.276568079507479

Epoch: 6| Step: 12
Training loss: 2.8215701580047607
Validation loss: 2.2689861277098298

Epoch: 6| Step: 13
Training loss: 2.468076467514038
Validation loss: 2.2800516351576774

Epoch: 317| Step: 0
Training loss: 1.743224859237671
Validation loss: 2.293715100134573

Epoch: 6| Step: 1
Training loss: 2.2500295639038086
Validation loss: 2.300335919985207

Epoch: 6| Step: 2
Training loss: 2.397423267364502
Validation loss: 2.3098393050573205

Epoch: 6| Step: 3
Training loss: 2.0876615047454834
Validation loss: 2.3079818961440877

Epoch: 6| Step: 4
Training loss: 2.2577133178710938
Validation loss: 2.318845979629024

Epoch: 6| Step: 5
Training loss: 2.266873598098755
Validation loss: 2.306513338960627

Epoch: 6| Step: 6
Training loss: 2.762434244155884
Validation loss: 2.3004029925151537

Epoch: 6| Step: 7
Training loss: 3.1107120513916016
Validation loss: 2.295502124294158

Epoch: 6| Step: 8
Training loss: 1.8886237144470215
Validation loss: 2.273034364946427

Epoch: 6| Step: 9
Training loss: 2.2526192665100098
Validation loss: 2.262409228150563

Epoch: 6| Step: 10
Training loss: 2.625396728515625
Validation loss: 2.2801680744335218

Epoch: 6| Step: 11
Training loss: 2.2788076400756836
Validation loss: 2.2673982522820912

Epoch: 6| Step: 12
Training loss: 3.106356143951416
Validation loss: 2.277649574382331

Epoch: 6| Step: 13
Training loss: 2.791145086288452
Validation loss: 2.2762174452504804

Epoch: 318| Step: 0
Training loss: 2.5233662128448486
Validation loss: 2.268765349541941

Epoch: 6| Step: 1
Training loss: 1.5733938217163086
Validation loss: 2.269669863485521

Epoch: 6| Step: 2
Training loss: 2.3737692832946777
Validation loss: 2.2795189836973786

Epoch: 6| Step: 3
Training loss: 2.697948455810547
Validation loss: 2.275287194918561

Epoch: 6| Step: 4
Training loss: 2.6658108234405518
Validation loss: 2.287494892715126

Epoch: 6| Step: 5
Training loss: 1.691636323928833
Validation loss: 2.2823260266293763

Epoch: 6| Step: 6
Training loss: 3.547757625579834
Validation loss: 2.2969886256802465

Epoch: 6| Step: 7
Training loss: 2.3283252716064453
Validation loss: 2.2869928780422417

Epoch: 6| Step: 8
Training loss: 1.812147855758667
Validation loss: 2.2941846104078394

Epoch: 6| Step: 9
Training loss: 2.2006020545959473
Validation loss: 2.3078161926679712

Epoch: 6| Step: 10
Training loss: 3.1938581466674805
Validation loss: 2.3176705016884753

Epoch: 6| Step: 11
Training loss: 2.287876605987549
Validation loss: 2.316261878577612

Epoch: 6| Step: 12
Training loss: 2.546208381652832
Validation loss: 2.331984572513129

Epoch: 6| Step: 13
Training loss: 1.942930817604065
Validation loss: 2.3230562979175198

Epoch: 319| Step: 0
Training loss: 2.544066905975342
Validation loss: 2.2979452943289154

Epoch: 6| Step: 1
Training loss: 2.1275136470794678
Validation loss: 2.3002242170354372

Epoch: 6| Step: 2
Training loss: 2.4429171085357666
Validation loss: 2.28410162976993

Epoch: 6| Step: 3
Training loss: 1.6931051015853882
Validation loss: 2.295075688310849

Epoch: 6| Step: 4
Training loss: 3.0948944091796875
Validation loss: 2.3069138783280567

Epoch: 6| Step: 5
Training loss: 2.3770761489868164
Validation loss: 2.3163822850873395

Epoch: 6| Step: 6
Training loss: 2.29681134223938
Validation loss: 2.3035652945118565

Epoch: 6| Step: 7
Training loss: 2.4316883087158203
Validation loss: 2.3350098645815285

Epoch: 6| Step: 8
Training loss: 2.8317978382110596
Validation loss: 2.32750097397835

Epoch: 6| Step: 9
Training loss: 2.138268232345581
Validation loss: 2.310465287136775

Epoch: 6| Step: 10
Training loss: 2.7344696521759033
Validation loss: 2.301867687573997

Epoch: 6| Step: 11
Training loss: 2.5914928913116455
Validation loss: 2.2851441649980444

Epoch: 6| Step: 12
Training loss: 2.5833230018615723
Validation loss: 2.245723093709638

Epoch: 6| Step: 13
Training loss: 1.0500692129135132
Validation loss: 2.2422459433155675

Epoch: 320| Step: 0
Training loss: 2.8904647827148438
Validation loss: 2.2600011082105738

Epoch: 6| Step: 1
Training loss: 1.987415075302124
Validation loss: 2.254371396956905

Epoch: 6| Step: 2
Training loss: 2.2948410511016846
Validation loss: 2.2556142012278237

Epoch: 6| Step: 3
Training loss: 2.464313507080078
Validation loss: 2.2924745159764446

Epoch: 6| Step: 4
Training loss: 2.197863817214966
Validation loss: 2.2899598716407694

Epoch: 6| Step: 5
Training loss: 2.419917583465576
Validation loss: 2.298292462543775

Epoch: 6| Step: 6
Training loss: 2.7436532974243164
Validation loss: 2.3035185465248684

Epoch: 6| Step: 7
Training loss: 2.209181785583496
Validation loss: 2.2849750749526487

Epoch: 6| Step: 8
Training loss: 1.3721987009048462
Validation loss: 2.280428704395089

Epoch: 6| Step: 9
Training loss: 2.823172092437744
Validation loss: 2.2692186370972665

Epoch: 6| Step: 10
Training loss: 2.629206657409668
Validation loss: 2.260572107889319

Epoch: 6| Step: 11
Training loss: 2.5705368518829346
Validation loss: 2.2357545206623692

Epoch: 6| Step: 12
Training loss: 2.9072265625
Validation loss: 2.2268400487079414

Epoch: 6| Step: 13
Training loss: 1.9719170331954956
Validation loss: 2.245571990166941

Epoch: 321| Step: 0
Training loss: 2.1863081455230713
Validation loss: 2.251252661469162

Epoch: 6| Step: 1
Training loss: 2.4401912689208984
Validation loss: 2.25082883527202

Epoch: 6| Step: 2
Training loss: 2.5794949531555176
Validation loss: 2.258950730805756

Epoch: 6| Step: 3
Training loss: 1.7347114086151123
Validation loss: 2.271218552384325

Epoch: 6| Step: 4
Training loss: 2.6533992290496826
Validation loss: 2.26692134077831

Epoch: 6| Step: 5
Training loss: 2.8272337913513184
Validation loss: 2.2914055701225036

Epoch: 6| Step: 6
Training loss: 2.5507287979125977
Validation loss: 2.307859375912656

Epoch: 6| Step: 7
Training loss: 2.146810293197632
Validation loss: 2.3141282937859975

Epoch: 6| Step: 8
Training loss: 2.1825599670410156
Validation loss: 2.320443917346257

Epoch: 6| Step: 9
Training loss: 2.462416172027588
Validation loss: 2.3154175178979033

Epoch: 6| Step: 10
Training loss: 2.9199514389038086
Validation loss: 2.303657885520689

Epoch: 6| Step: 11
Training loss: 2.034794330596924
Validation loss: 2.3007392934573594

Epoch: 6| Step: 12
Training loss: 2.8730039596557617
Validation loss: 2.293892211811517

Epoch: 6| Step: 13
Training loss: 1.935813069343567
Validation loss: 2.2874290712418093

Epoch: 322| Step: 0
Training loss: 1.5964319705963135
Validation loss: 2.274284703757173

Epoch: 6| Step: 1
Training loss: 3.106522560119629
Validation loss: 2.2610116774036038

Epoch: 6| Step: 2
Training loss: 2.1339035034179688
Validation loss: 2.2570398212761007

Epoch: 6| Step: 3
Training loss: 2.532193422317505
Validation loss: 2.255852432661159

Epoch: 6| Step: 4
Training loss: 3.098072052001953
Validation loss: 2.250477344759049

Epoch: 6| Step: 5
Training loss: 2.6283559799194336
Validation loss: 2.2396838870099796

Epoch: 6| Step: 6
Training loss: 2.620999813079834
Validation loss: 2.231299302911246

Epoch: 6| Step: 7
Training loss: 2.030229091644287
Validation loss: 2.2226142985846407

Epoch: 6| Step: 8
Training loss: 1.7475095987319946
Validation loss: 2.2251503698287474

Epoch: 6| Step: 9
Training loss: 2.786482572555542
Validation loss: 2.2239607457191712

Epoch: 6| Step: 10
Training loss: 2.006267547607422
Validation loss: 2.245273551633281

Epoch: 6| Step: 11
Training loss: 2.7606654167175293
Validation loss: 2.244685108943652

Epoch: 6| Step: 12
Training loss: 1.7073884010314941
Validation loss: 2.2455587566539807

Epoch: 6| Step: 13
Training loss: 2.8526437282562256
Validation loss: 2.2542347420928297

Epoch: 323| Step: 0
Training loss: 2.05206036567688
Validation loss: 2.2587384703338786

Epoch: 6| Step: 1
Training loss: 2.9472198486328125
Validation loss: 2.2495503310234315

Epoch: 6| Step: 2
Training loss: 2.5256237983703613
Validation loss: 2.2535801626020864

Epoch: 6| Step: 3
Training loss: 1.7579296827316284
Validation loss: 2.240910768508911

Epoch: 6| Step: 4
Training loss: 2.5227890014648438
Validation loss: 2.2449783048322125

Epoch: 6| Step: 5
Training loss: 2.738779067993164
Validation loss: 2.246705852529054

Epoch: 6| Step: 6
Training loss: 2.249758243560791
Validation loss: 2.2468968501655002

Epoch: 6| Step: 7
Training loss: 2.582601547241211
Validation loss: 2.2491799759608444

Epoch: 6| Step: 8
Training loss: 2.6227593421936035
Validation loss: 2.262997293985018

Epoch: 6| Step: 9
Training loss: 2.234755516052246
Validation loss: 2.255248719646085

Epoch: 6| Step: 10
Training loss: 3.2205405235290527
Validation loss: 2.2716221399204706

Epoch: 6| Step: 11
Training loss: 2.0344398021698
Validation loss: 2.240095974296652

Epoch: 6| Step: 12
Training loss: 1.2359647750854492
Validation loss: 2.2595588622554654

Epoch: 6| Step: 13
Training loss: 2.639219045639038
Validation loss: 2.264032716392189

Epoch: 324| Step: 0
Training loss: 2.337768077850342
Validation loss: 2.2812048773611746

Epoch: 6| Step: 1
Training loss: 2.513113498687744
Validation loss: 2.278001913460352

Epoch: 6| Step: 2
Training loss: 2.3779122829437256
Validation loss: 2.296957846610777

Epoch: 6| Step: 3
Training loss: 2.696730613708496
Validation loss: 2.2949748833974204

Epoch: 6| Step: 4
Training loss: 2.8333773612976074
Validation loss: 2.2895633943619265

Epoch: 6| Step: 5
Training loss: 2.410055637359619
Validation loss: 2.272440428374916

Epoch: 6| Step: 6
Training loss: 2.2069365978240967
Validation loss: 2.2802725363803167

Epoch: 6| Step: 7
Training loss: 2.5712101459503174
Validation loss: 2.2479191877508677

Epoch: 6| Step: 8
Training loss: 2.150454521179199
Validation loss: 2.2354905746316396

Epoch: 6| Step: 9
Training loss: 2.1798181533813477
Validation loss: 2.2350559196164532

Epoch: 6| Step: 10
Training loss: 2.8944950103759766
Validation loss: 2.2300935099201817

Epoch: 6| Step: 11
Training loss: 1.5247001647949219
Validation loss: 2.2146744625542754

Epoch: 6| Step: 12
Training loss: 2.6289968490600586
Validation loss: 2.2198161899402575

Epoch: 6| Step: 13
Training loss: 1.7934578657150269
Validation loss: 2.210230658131261

Epoch: 325| Step: 0
Training loss: 1.3858250379562378
Validation loss: 2.2118906077518257

Epoch: 6| Step: 1
Training loss: 2.8479652404785156
Validation loss: 2.2283991382968042

Epoch: 6| Step: 2
Training loss: 2.2393739223480225
Validation loss: 2.216574607356902

Epoch: 6| Step: 3
Training loss: 1.9486814737319946
Validation loss: 2.2216249665906354

Epoch: 6| Step: 4
Training loss: 1.9402129650115967
Validation loss: 2.219909288549936

Epoch: 6| Step: 5
Training loss: 2.7768774032592773
Validation loss: 2.222401447193597

Epoch: 6| Step: 6
Training loss: 2.4272875785827637
Validation loss: 2.235215239627387

Epoch: 6| Step: 7
Training loss: 2.710604190826416
Validation loss: 2.241433822980491

Epoch: 6| Step: 8
Training loss: 2.2631490230560303
Validation loss: 2.2403576604781614

Epoch: 6| Step: 9
Training loss: 2.026381492614746
Validation loss: 2.240251677010649

Epoch: 6| Step: 10
Training loss: 2.41223406791687
Validation loss: 2.242229038669217

Epoch: 6| Step: 11
Training loss: 2.667515277862549
Validation loss: 2.2473149837986117

Epoch: 6| Step: 12
Training loss: 3.0630640983581543
Validation loss: 2.260527677433465

Epoch: 6| Step: 13
Training loss: 3.2376999855041504
Validation loss: 2.264650524303477

Epoch: 326| Step: 0
Training loss: 2.15474796295166
Validation loss: 2.2776910566514537

Epoch: 6| Step: 1
Training loss: 2.06988525390625
Validation loss: 2.2759336245957242

Epoch: 6| Step: 2
Training loss: 2.628695011138916
Validation loss: 2.279055531306933

Epoch: 6| Step: 3
Training loss: 3.3487133979797363
Validation loss: 2.2834983743647093

Epoch: 6| Step: 4
Training loss: 1.8394691944122314
Validation loss: 2.284140320234401

Epoch: 6| Step: 5
Training loss: 2.5762124061584473
Validation loss: 2.27190198052314

Epoch: 6| Step: 6
Training loss: 2.2974255084991455
Validation loss: 2.289901130942888

Epoch: 6| Step: 7
Training loss: 1.7089173793792725
Validation loss: 2.270936071231801

Epoch: 6| Step: 8
Training loss: 2.6032090187072754
Validation loss: 2.277690259359216

Epoch: 6| Step: 9
Training loss: 2.6411638259887695
Validation loss: 2.253717040502897

Epoch: 6| Step: 10
Training loss: 1.6530827283859253
Validation loss: 2.250303691433322

Epoch: 6| Step: 11
Training loss: 2.2958056926727295
Validation loss: 2.2393338705903743

Epoch: 6| Step: 12
Training loss: 2.113525390625
Validation loss: 2.248376443821897

Epoch: 6| Step: 13
Training loss: 4.287628173828125
Validation loss: 2.2324288429752475

Epoch: 327| Step: 0
Training loss: 1.5288047790527344
Validation loss: 2.24028411475561

Epoch: 6| Step: 1
Training loss: 2.5656087398529053
Validation loss: 2.2298361255276586

Epoch: 6| Step: 2
Training loss: 2.4221010208129883
Validation loss: 2.233163772090789

Epoch: 6| Step: 3
Training loss: 2.8203322887420654
Validation loss: 2.2378128344012844

Epoch: 6| Step: 4
Training loss: 2.7701354026794434
Validation loss: 2.240443855203608

Epoch: 6| Step: 5
Training loss: 2.0992648601531982
Validation loss: 2.2390894864195134

Epoch: 6| Step: 6
Training loss: 2.0303916931152344
Validation loss: 2.2349716104486936

Epoch: 6| Step: 7
Training loss: 2.045323371887207
Validation loss: 2.232695864092919

Epoch: 6| Step: 8
Training loss: 2.4209165573120117
Validation loss: 2.2192701652485836

Epoch: 6| Step: 9
Training loss: 2.3234877586364746
Validation loss: 2.222492603845494

Epoch: 6| Step: 10
Training loss: 2.398494005203247
Validation loss: 2.2324276329368673

Epoch: 6| Step: 11
Training loss: 2.9237334728240967
Validation loss: 2.2435795184104674

Epoch: 6| Step: 12
Training loss: 2.183562994003296
Validation loss: 2.2517356872558594

Epoch: 6| Step: 13
Training loss: 2.7424073219299316
Validation loss: 2.2539270103618665

Epoch: 328| Step: 0
Training loss: 2.329054832458496
Validation loss: 2.2643384472016366

Epoch: 6| Step: 1
Training loss: 2.4262161254882812
Validation loss: 2.2783431186470935

Epoch: 6| Step: 2
Training loss: 2.006535768508911
Validation loss: 2.283460109464584

Epoch: 6| Step: 3
Training loss: 1.8397746086120605
Validation loss: 2.3043592796530774

Epoch: 6| Step: 4
Training loss: 2.2926101684570312
Validation loss: 2.297004827889063

Epoch: 6| Step: 5
Training loss: 2.378727912902832
Validation loss: 2.29879714340292

Epoch: 6| Step: 6
Training loss: 2.5281317234039307
Validation loss: 2.2775027111012447

Epoch: 6| Step: 7
Training loss: 3.320852756500244
Validation loss: 2.2838987176136305

Epoch: 6| Step: 8
Training loss: 2.5103931427001953
Validation loss: 2.268380936755929

Epoch: 6| Step: 9
Training loss: 2.560990333557129
Validation loss: 2.249873266425184

Epoch: 6| Step: 10
Training loss: 2.077390670776367
Validation loss: 2.2522027953978507

Epoch: 6| Step: 11
Training loss: 1.9757721424102783
Validation loss: 2.2672304825116227

Epoch: 6| Step: 12
Training loss: 2.441333055496216
Validation loss: 2.253937090596845

Epoch: 6| Step: 13
Training loss: 2.609995126724243
Validation loss: 2.2517251148018786

Epoch: 329| Step: 0
Training loss: 2.1843905448913574
Validation loss: 2.2521260861427552

Epoch: 6| Step: 1
Training loss: 2.080045700073242
Validation loss: 2.2534215411832257

Epoch: 6| Step: 2
Training loss: 2.2476727962493896
Validation loss: 2.2481773514901437

Epoch: 6| Step: 3
Training loss: 1.4494872093200684
Validation loss: 2.2232464410925425

Epoch: 6| Step: 4
Training loss: 2.329763412475586
Validation loss: 2.230078169094619

Epoch: 6| Step: 5
Training loss: 1.6335506439208984
Validation loss: 2.237666453084638

Epoch: 6| Step: 6
Training loss: 3.182584285736084
Validation loss: 2.241531777125533

Epoch: 6| Step: 7
Training loss: 2.6476316452026367
Validation loss: 2.2471350123805385

Epoch: 6| Step: 8
Training loss: 2.417776107788086
Validation loss: 2.2565058328772105

Epoch: 6| Step: 9
Training loss: 2.8417255878448486
Validation loss: 2.2766391000440045

Epoch: 6| Step: 10
Training loss: 2.9912166595458984
Validation loss: 2.279638749296947

Epoch: 6| Step: 11
Training loss: 2.050351142883301
Validation loss: 2.284313601832236

Epoch: 6| Step: 12
Training loss: 1.9611868858337402
Validation loss: 2.2735061850599063

Epoch: 6| Step: 13
Training loss: 3.449876308441162
Validation loss: 2.2556684658091557

Epoch: 330| Step: 0
Training loss: 2.3378758430480957
Validation loss: 2.235761668092461

Epoch: 6| Step: 1
Training loss: 2.288771152496338
Validation loss: 2.221895258913758

Epoch: 6| Step: 2
Training loss: 2.089433431625366
Validation loss: 2.22334457981971

Epoch: 6| Step: 3
Training loss: 1.7689194679260254
Validation loss: 2.214146206455846

Epoch: 6| Step: 4
Training loss: 3.0251078605651855
Validation loss: 2.2097644036816013

Epoch: 6| Step: 5
Training loss: 2.657944917678833
Validation loss: 2.2075499385915776

Epoch: 6| Step: 6
Training loss: 2.1294217109680176
Validation loss: 2.2166583435509795

Epoch: 6| Step: 7
Training loss: 2.1335408687591553
Validation loss: 2.2165319445312663

Epoch: 6| Step: 8
Training loss: 2.192028522491455
Validation loss: 2.2356287766528387

Epoch: 6| Step: 9
Training loss: 2.0740559101104736
Validation loss: 2.247500765708185

Epoch: 6| Step: 10
Training loss: 2.6858959197998047
Validation loss: 2.258684855635448

Epoch: 6| Step: 11
Training loss: 2.4023592472076416
Validation loss: 2.259966161943251

Epoch: 6| Step: 12
Training loss: 2.4105844497680664
Validation loss: 2.282890227533156

Epoch: 6| Step: 13
Training loss: 3.5462770462036133
Validation loss: 2.287522486461106

Epoch: 331| Step: 0
Training loss: 2.155794620513916
Validation loss: 2.2707736569066204

Epoch: 6| Step: 1
Training loss: 2.691103458404541
Validation loss: 2.253915576524632

Epoch: 6| Step: 2
Training loss: 1.761303424835205
Validation loss: 2.2515074668392057

Epoch: 6| Step: 3
Training loss: 2.4700663089752197
Validation loss: 2.2439598293714624

Epoch: 6| Step: 4
Training loss: 2.2338712215423584
Validation loss: 2.224635990717078

Epoch: 6| Step: 5
Training loss: 2.8021748065948486
Validation loss: 2.219027849935716

Epoch: 6| Step: 6
Training loss: 2.6123838424682617
Validation loss: 2.225836019362173

Epoch: 6| Step: 7
Training loss: 2.5606141090393066
Validation loss: 2.225518754733506

Epoch: 6| Step: 8
Training loss: 2.7473793029785156
Validation loss: 2.2317255914852185

Epoch: 6| Step: 9
Training loss: 2.6622231006622314
Validation loss: 2.247823397318522

Epoch: 6| Step: 10
Training loss: 2.5412235260009766
Validation loss: 2.2482130796678605

Epoch: 6| Step: 11
Training loss: 2.258904218673706
Validation loss: 2.245061198870341

Epoch: 6| Step: 12
Training loss: 1.8729610443115234
Validation loss: 2.240473649835074

Epoch: 6| Step: 13
Training loss: 1.8883171081542969
Validation loss: 2.2204957085271038

Epoch: 332| Step: 0
Training loss: 1.9454094171524048
Validation loss: 2.22996925538586

Epoch: 6| Step: 1
Training loss: 2.6914286613464355
Validation loss: 2.2228284023141347

Epoch: 6| Step: 2
Training loss: 2.285163402557373
Validation loss: 2.236475762500558

Epoch: 6| Step: 3
Training loss: 2.351931095123291
Validation loss: 2.23834054957154

Epoch: 6| Step: 4
Training loss: 2.332261800765991
Validation loss: 2.2489516555622058

Epoch: 6| Step: 5
Training loss: 2.6355602741241455
Validation loss: 2.2660074259645198

Epoch: 6| Step: 6
Training loss: 2.4894368648529053
Validation loss: 2.2677560160236974

Epoch: 6| Step: 7
Training loss: 2.1233410835266113
Validation loss: 2.2590850091749624

Epoch: 6| Step: 8
Training loss: 2.469273567199707
Validation loss: 2.2565181127158542

Epoch: 6| Step: 9
Training loss: 1.9651210308074951
Validation loss: 2.260543425877889

Epoch: 6| Step: 10
Training loss: 3.0462441444396973
Validation loss: 2.254969419971589

Epoch: 6| Step: 11
Training loss: 2.6708967685699463
Validation loss: 2.25662047888643

Epoch: 6| Step: 12
Training loss: 1.8152912855148315
Validation loss: 2.2570635375156196

Epoch: 6| Step: 13
Training loss: 2.436131000518799
Validation loss: 2.2592302881261355

Epoch: 333| Step: 0
Training loss: 2.96207332611084
Validation loss: 2.2630958634038127

Epoch: 6| Step: 1
Training loss: 2.9303836822509766
Validation loss: 2.2472148915772796

Epoch: 6| Step: 2
Training loss: 2.330561637878418
Validation loss: 2.235345935308805

Epoch: 6| Step: 3
Training loss: 2.231611728668213
Validation loss: 2.238805970837993

Epoch: 6| Step: 4
Training loss: 2.261367082595825
Validation loss: 2.2389228959237375

Epoch: 6| Step: 5
Training loss: 2.824871301651001
Validation loss: 2.2366634773951706

Epoch: 6| Step: 6
Training loss: 1.7233123779296875
Validation loss: 2.2417697188674763

Epoch: 6| Step: 7
Training loss: 1.557293176651001
Validation loss: 2.249960478915963

Epoch: 6| Step: 8
Training loss: 2.1266393661499023
Validation loss: 2.26127895744898

Epoch: 6| Step: 9
Training loss: 2.969144344329834
Validation loss: 2.2593569499190136

Epoch: 6| Step: 10
Training loss: 2.525954008102417
Validation loss: 2.2709941325649137

Epoch: 6| Step: 11
Training loss: 2.010434627532959
Validation loss: 2.299033985343031

Epoch: 6| Step: 12
Training loss: 2.4105849266052246
Validation loss: 2.2952553200465378

Epoch: 6| Step: 13
Training loss: 2.3300082683563232
Validation loss: 2.2867398415842364

Epoch: 334| Step: 0
Training loss: 2.3746206760406494
Validation loss: 2.2758799368335354

Epoch: 6| Step: 1
Training loss: 2.742898941040039
Validation loss: 2.267844561607607

Epoch: 6| Step: 2
Training loss: 2.258044958114624
Validation loss: 2.266797927118117

Epoch: 6| Step: 3
Training loss: 1.399782657623291
Validation loss: 2.22396408870656

Epoch: 6| Step: 4
Training loss: 2.577638626098633
Validation loss: 2.242837941774758

Epoch: 6| Step: 5
Training loss: 2.395348072052002
Validation loss: 2.232388142616518

Epoch: 6| Step: 6
Training loss: 2.413907051086426
Validation loss: 2.2260909413778656

Epoch: 6| Step: 7
Training loss: 2.7636263370513916
Validation loss: 2.234532722862818

Epoch: 6| Step: 8
Training loss: 3.011610507965088
Validation loss: 2.2413611309502715

Epoch: 6| Step: 9
Training loss: 2.0081095695495605
Validation loss: 2.2382961139884046

Epoch: 6| Step: 10
Training loss: 2.0129640102386475
Validation loss: 2.2276252969618766

Epoch: 6| Step: 11
Training loss: 2.2227377891540527
Validation loss: 2.2228237941700923

Epoch: 6| Step: 12
Training loss: 1.9853515625
Validation loss: 2.2259775515525573

Epoch: 6| Step: 13
Training loss: 3.0699031352996826
Validation loss: 2.2212065676207184

Epoch: 335| Step: 0
Training loss: 3.132650852203369
Validation loss: 2.2233467076414373

Epoch: 6| Step: 1
Training loss: 2.3848958015441895
Validation loss: 2.240539291853546

Epoch: 6| Step: 2
Training loss: 2.0804786682128906
Validation loss: 2.2082765153659287

Epoch: 6| Step: 3
Training loss: 2.5394811630249023
Validation loss: 2.222613278255668

Epoch: 6| Step: 4
Training loss: 2.3716259002685547
Validation loss: 2.2227939149384857

Epoch: 6| Step: 5
Training loss: 2.3037657737731934
Validation loss: 2.244888436409735

Epoch: 6| Step: 6
Training loss: 2.260082960128784
Validation loss: 2.2653996521426785

Epoch: 6| Step: 7
Training loss: 1.497434139251709
Validation loss: 2.2622939617403093

Epoch: 6| Step: 8
Training loss: 2.510166883468628
Validation loss: 2.2633349049475884

Epoch: 6| Step: 9
Training loss: 1.9619550704956055
Validation loss: 2.2520264297403316

Epoch: 6| Step: 10
Training loss: 1.8865480422973633
Validation loss: 2.238424003765147

Epoch: 6| Step: 11
Training loss: 2.869838237762451
Validation loss: 2.234938952230638

Epoch: 6| Step: 12
Training loss: 1.930999517440796
Validation loss: 2.255086265584474

Epoch: 6| Step: 13
Training loss: 3.712520122528076
Validation loss: 2.2201282734512002

Epoch: 336| Step: 0
Training loss: 2.7171144485473633
Validation loss: 2.2342593234072448

Epoch: 6| Step: 1
Training loss: 2.7332522869110107
Validation loss: 2.2280621836262364

Epoch: 6| Step: 2
Training loss: 1.968846321105957
Validation loss: 2.2138190102833573

Epoch: 6| Step: 3
Training loss: 2.2793660163879395
Validation loss: 2.234320363690776

Epoch: 6| Step: 4
Training loss: 1.7339277267456055
Validation loss: 2.208725485750424

Epoch: 6| Step: 5
Training loss: 2.539607524871826
Validation loss: 2.2133560667755785

Epoch: 6| Step: 6
Training loss: 1.9704599380493164
Validation loss: 2.219634694437827

Epoch: 6| Step: 7
Training loss: 1.6507415771484375
Validation loss: 2.203278781265341

Epoch: 6| Step: 8
Training loss: 2.561577558517456
Validation loss: 2.232545442478631

Epoch: 6| Step: 9
Training loss: 2.3906044960021973
Validation loss: 2.230789783180401

Epoch: 6| Step: 10
Training loss: 3.08113694190979
Validation loss: 2.234784758219155

Epoch: 6| Step: 11
Training loss: 2.852591037750244
Validation loss: 2.2382131161228305

Epoch: 6| Step: 12
Training loss: 2.453859329223633
Validation loss: 2.2254788029578423

Epoch: 6| Step: 13
Training loss: 1.581889033317566
Validation loss: 2.224210189234826

Epoch: 337| Step: 0
Training loss: 2.7368431091308594
Validation loss: 2.2180370771756737

Epoch: 6| Step: 1
Training loss: 2.6989736557006836
Validation loss: 2.2169494757088284

Epoch: 6| Step: 2
Training loss: 2.317903757095337
Validation loss: 2.217837160633456

Epoch: 6| Step: 3
Training loss: 2.976935386657715
Validation loss: 2.2062013713262414

Epoch: 6| Step: 4
Training loss: 1.4825360774993896
Validation loss: 2.224984202333676

Epoch: 6| Step: 5
Training loss: 2.2151105403900146
Validation loss: 2.210473878409273

Epoch: 6| Step: 6
Training loss: 1.6029887199401855
Validation loss: 2.2216168962499148

Epoch: 6| Step: 7
Training loss: 2.962599277496338
Validation loss: 2.213426043910365

Epoch: 6| Step: 8
Training loss: 2.4388248920440674
Validation loss: 2.207855596337267

Epoch: 6| Step: 9
Training loss: 2.8416690826416016
Validation loss: 2.2377557164879254

Epoch: 6| Step: 10
Training loss: 1.6436618566513062
Validation loss: 2.235360604460521

Epoch: 6| Step: 11
Training loss: 2.032402515411377
Validation loss: 2.2380454488979873

Epoch: 6| Step: 12
Training loss: 1.8940558433532715
Validation loss: 2.2383077785532963

Epoch: 6| Step: 13
Training loss: 3.8503592014312744
Validation loss: 2.2626238728082306

Epoch: 338| Step: 0
Training loss: 2.358743667602539
Validation loss: 2.2797277204452024

Epoch: 6| Step: 1
Training loss: 2.2951674461364746
Validation loss: 2.3218426242951424

Epoch: 6| Step: 2
Training loss: 3.021850109100342
Validation loss: 2.3423587686272076

Epoch: 6| Step: 3
Training loss: 2.969867706298828
Validation loss: 2.3321817459598666

Epoch: 6| Step: 4
Training loss: 2.0299878120422363
Validation loss: 2.317828142514793

Epoch: 6| Step: 5
Training loss: 1.955316424369812
Validation loss: 2.3349356856397403

Epoch: 6| Step: 6
Training loss: 2.6893436908721924
Validation loss: 2.315390561216621

Epoch: 6| Step: 7
Training loss: 1.5103299617767334
Validation loss: 2.24051018299595

Epoch: 6| Step: 8
Training loss: 2.7749457359313965
Validation loss: 2.2430216855900262

Epoch: 6| Step: 9
Training loss: 2.5715491771698
Validation loss: 2.215774050322912

Epoch: 6| Step: 10
Training loss: 2.3884620666503906
Validation loss: 2.2019117288692023

Epoch: 6| Step: 11
Training loss: 2.016810178756714
Validation loss: 2.1950881775989326

Epoch: 6| Step: 12
Training loss: 2.239527702331543
Validation loss: 2.2110370256567515

Epoch: 6| Step: 13
Training loss: 2.171936273574829
Validation loss: 2.1985243469156246

Epoch: 339| Step: 0
Training loss: 3.2912216186523438
Validation loss: 2.192092962162469

Epoch: 6| Step: 1
Training loss: 1.8065158128738403
Validation loss: 2.1937959014728503

Epoch: 6| Step: 2
Training loss: 2.4911627769470215
Validation loss: 2.198832783647763

Epoch: 6| Step: 3
Training loss: 1.9866260290145874
Validation loss: 2.1989086212650424

Epoch: 6| Step: 4
Training loss: 2.500762462615967
Validation loss: 2.191315815012942

Epoch: 6| Step: 5
Training loss: 3.3672938346862793
Validation loss: 2.2043934035044845

Epoch: 6| Step: 6
Training loss: 1.8852968215942383
Validation loss: 2.2069265842437744

Epoch: 6| Step: 7
Training loss: 2.487659454345703
Validation loss: 2.212584639108309

Epoch: 6| Step: 8
Training loss: 2.3062477111816406
Validation loss: 2.2202705926792596

Epoch: 6| Step: 9
Training loss: 1.984798550605774
Validation loss: 2.2073907903445664

Epoch: 6| Step: 10
Training loss: 2.005880832672119
Validation loss: 2.219851663035731

Epoch: 6| Step: 11
Training loss: 1.5679006576538086
Validation loss: 2.2142113998372066

Epoch: 6| Step: 12
Training loss: 2.6316936016082764
Validation loss: 2.2115798791249595

Epoch: 6| Step: 13
Training loss: 2.574800491333008
Validation loss: 2.213115674193187

Epoch: 340| Step: 0
Training loss: 2.3263306617736816
Validation loss: 2.1946630118995585

Epoch: 6| Step: 1
Training loss: 2.2218542098999023
Validation loss: 2.188750365728973

Epoch: 6| Step: 2
Training loss: 2.06889009475708
Validation loss: 2.192742340026363

Epoch: 6| Step: 3
Training loss: 2.7878623008728027
Validation loss: 2.202007697474572

Epoch: 6| Step: 4
Training loss: 2.928050994873047
Validation loss: 2.19676366929085

Epoch: 6| Step: 5
Training loss: 2.225977897644043
Validation loss: 2.1910861692120953

Epoch: 6| Step: 6
Training loss: 2.5689783096313477
Validation loss: 2.2021905273519535

Epoch: 6| Step: 7
Training loss: 1.878770112991333
Validation loss: 2.2070988749945037

Epoch: 6| Step: 8
Training loss: 2.6988842487335205
Validation loss: 2.2269651889801025

Epoch: 6| Step: 9
Training loss: 2.2402782440185547
Validation loss: 2.231417609799293

Epoch: 6| Step: 10
Training loss: 2.5619163513183594
Validation loss: 2.217204172124145

Epoch: 6| Step: 11
Training loss: 1.9122895002365112
Validation loss: 2.209871586932931

Epoch: 6| Step: 12
Training loss: 1.6351754665374756
Validation loss: 2.223144926050658

Epoch: 6| Step: 13
Training loss: 2.5035104751586914
Validation loss: 2.2190368137051983

Epoch: 341| Step: 0
Training loss: 2.5342206954956055
Validation loss: 2.205083977791571

Epoch: 6| Step: 1
Training loss: 2.6645073890686035
Validation loss: 2.1878286433476273

Epoch: 6| Step: 2
Training loss: 2.751892328262329
Validation loss: 2.173225207995343

Epoch: 6| Step: 3
Training loss: 2.517362594604492
Validation loss: 2.18590759974654

Epoch: 6| Step: 4
Training loss: 2.1312508583068848
Validation loss: 2.1958528180276193

Epoch: 6| Step: 5
Training loss: 1.836929202079773
Validation loss: 2.188518108860139

Epoch: 6| Step: 6
Training loss: 2.4117889404296875
Validation loss: 2.189610235152706

Epoch: 6| Step: 7
Training loss: 2.0509166717529297
Validation loss: 2.1974136239738873

Epoch: 6| Step: 8
Training loss: 1.9367015361785889
Validation loss: 2.224169869576731

Epoch: 6| Step: 9
Training loss: 2.009573221206665
Validation loss: 2.2318288767209618

Epoch: 6| Step: 10
Training loss: 2.3101911544799805
Validation loss: 2.237712683216218

Epoch: 6| Step: 11
Training loss: 2.4725518226623535
Validation loss: 2.2238886305080947

Epoch: 6| Step: 12
Training loss: 2.743311643600464
Validation loss: 2.2284867507155224

Epoch: 6| Step: 13
Training loss: 2.1222987174987793
Validation loss: 2.2190231482187905

Epoch: 342| Step: 0
Training loss: 3.046645164489746
Validation loss: 2.2082229660403345

Epoch: 6| Step: 1
Training loss: 2.064851999282837
Validation loss: 2.2158847585801156

Epoch: 6| Step: 2
Training loss: 2.5196423530578613
Validation loss: 2.2159043230036253

Epoch: 6| Step: 3
Training loss: 2.162219285964966
Validation loss: 2.2041890057184363

Epoch: 6| Step: 4
Training loss: 2.7104649543762207
Validation loss: 2.20957685542363

Epoch: 6| Step: 5
Training loss: 1.8635238409042358
Validation loss: 2.2112134707871305

Epoch: 6| Step: 6
Training loss: 1.69453763961792
Validation loss: 2.224154895351779

Epoch: 6| Step: 7
Training loss: 2.650949478149414
Validation loss: 2.217242012741745

Epoch: 6| Step: 8
Training loss: 2.1223952770233154
Validation loss: 2.2440955972158783

Epoch: 6| Step: 9
Training loss: 1.8530397415161133
Validation loss: 2.229440007158505

Epoch: 6| Step: 10
Training loss: 2.5120911598205566
Validation loss: 2.245110043915369

Epoch: 6| Step: 11
Training loss: 2.124258518218994
Validation loss: 2.226407486905334

Epoch: 6| Step: 12
Training loss: 2.036634683609009
Validation loss: 2.2274722796614452

Epoch: 6| Step: 13
Training loss: 3.677577495574951
Validation loss: 2.228212228385351

Epoch: 343| Step: 0
Training loss: 1.9543981552124023
Validation loss: 2.2266194256403113

Epoch: 6| Step: 1
Training loss: 2.197352409362793
Validation loss: 2.2172154175337924

Epoch: 6| Step: 2
Training loss: 2.432811975479126
Validation loss: 2.204958592691729

Epoch: 6| Step: 3
Training loss: 1.8612257242202759
Validation loss: 2.2172062422639582

Epoch: 6| Step: 4
Training loss: 3.031172513961792
Validation loss: 2.2094856641625844

Epoch: 6| Step: 5
Training loss: 2.2712416648864746
Validation loss: 2.197932597129576

Epoch: 6| Step: 6
Training loss: 2.281247615814209
Validation loss: 2.1925139324639433

Epoch: 6| Step: 7
Training loss: 2.4539387226104736
Validation loss: 2.177182077079691

Epoch: 6| Step: 8
Training loss: 2.1799674034118652
Validation loss: 2.1845187192322104

Epoch: 6| Step: 9
Training loss: 1.9684956073760986
Validation loss: 2.1995519899552867

Epoch: 6| Step: 10
Training loss: 2.6549737453460693
Validation loss: 2.1907944730533067

Epoch: 6| Step: 11
Training loss: 2.4127907752990723
Validation loss: 2.2005056168443415

Epoch: 6| Step: 12
Training loss: 1.807901382446289
Validation loss: 2.214533636646886

Epoch: 6| Step: 13
Training loss: 3.253995180130005
Validation loss: 2.2143584912823093

Epoch: 344| Step: 0
Training loss: 1.8066749572753906
Validation loss: 2.2488181424397293

Epoch: 6| Step: 1
Training loss: 2.3481285572052
Validation loss: 2.2770361438874276

Epoch: 6| Step: 2
Training loss: 2.192085027694702
Validation loss: 2.2730132020929807

Epoch: 6| Step: 3
Training loss: 3.1194305419921875
Validation loss: 2.2817970424570064

Epoch: 6| Step: 4
Training loss: 2.0507731437683105
Validation loss: 2.267630962915318

Epoch: 6| Step: 5
Training loss: 2.338839530944824
Validation loss: 2.2737165420286116

Epoch: 6| Step: 6
Training loss: 2.536726951599121
Validation loss: 2.252052071273968

Epoch: 6| Step: 7
Training loss: 2.510125160217285
Validation loss: 2.2417461692645984

Epoch: 6| Step: 8
Training loss: 2.6369521617889404
Validation loss: 2.2159526348114014

Epoch: 6| Step: 9
Training loss: 2.0742530822753906
Validation loss: 2.2001865012671358

Epoch: 6| Step: 10
Training loss: 1.6512682437896729
Validation loss: 2.2086578543468187

Epoch: 6| Step: 11
Training loss: 2.2968573570251465
Validation loss: 2.195684693192923

Epoch: 6| Step: 12
Training loss: 2.691159725189209
Validation loss: 2.1987521084406043

Epoch: 6| Step: 13
Training loss: 2.0208613872528076
Validation loss: 2.19327228043669

Epoch: 345| Step: 0
Training loss: 2.5043835639953613
Validation loss: 2.207584786158736

Epoch: 6| Step: 1
Training loss: 1.691033124923706
Validation loss: 2.207348512065026

Epoch: 6| Step: 2
Training loss: 3.0940709114074707
Validation loss: 2.2091492965657222

Epoch: 6| Step: 3
Training loss: 1.479715347290039
Validation loss: 2.1864881900049027

Epoch: 6| Step: 4
Training loss: 2.837033271789551
Validation loss: 2.204699444514449

Epoch: 6| Step: 5
Training loss: 2.176478147506714
Validation loss: 2.197690586889944

Epoch: 6| Step: 6
Training loss: 1.9817441701889038
Validation loss: 2.202067075237151

Epoch: 6| Step: 7
Training loss: 1.9339815378189087
Validation loss: 2.194423710146258

Epoch: 6| Step: 8
Training loss: 1.9133893251419067
Validation loss: 2.2006566114323114

Epoch: 6| Step: 9
Training loss: 2.5332629680633545
Validation loss: 2.1956401871096705

Epoch: 6| Step: 10
Training loss: 2.4908223152160645
Validation loss: 2.2146512718610865

Epoch: 6| Step: 11
Training loss: 2.474802017211914
Validation loss: 2.213047901789347

Epoch: 6| Step: 12
Training loss: 2.3649415969848633
Validation loss: 2.1989245401915682

Epoch: 6| Step: 13
Training loss: 3.16082763671875
Validation loss: 2.2108713324351976

Epoch: 346| Step: 0
Training loss: 2.3426151275634766
Validation loss: 2.216785853908908

Epoch: 6| Step: 1
Training loss: 3.083061933517456
Validation loss: 2.2304400654249292

Epoch: 6| Step: 2
Training loss: 1.965506672859192
Validation loss: 2.2175948517296904

Epoch: 6| Step: 3
Training loss: 3.0682356357574463
Validation loss: 2.22915659540443

Epoch: 6| Step: 4
Training loss: 1.816850185394287
Validation loss: 2.2455873181743007

Epoch: 6| Step: 5
Training loss: 2.0522265434265137
Validation loss: 2.23783455869203

Epoch: 6| Step: 6
Training loss: 2.3334314823150635
Validation loss: 2.2271876668417327

Epoch: 6| Step: 7
Training loss: 2.0835444927215576
Validation loss: 2.2258586986090547

Epoch: 6| Step: 8
Training loss: 1.9041852951049805
Validation loss: 2.2105996890734603

Epoch: 6| Step: 9
Training loss: 2.4068048000335693
Validation loss: 2.223204670413848

Epoch: 6| Step: 10
Training loss: 2.6620469093322754
Validation loss: 2.2196747103045062

Epoch: 6| Step: 11
Training loss: 2.6775946617126465
Validation loss: 2.2107416404190885

Epoch: 6| Step: 12
Training loss: 1.4401143789291382
Validation loss: 2.18835606626285

Epoch: 6| Step: 13
Training loss: 2.5936083793640137
Validation loss: 2.197060218421362

Epoch: 347| Step: 0
Training loss: 1.9000828266143799
Validation loss: 2.2007786791811705

Epoch: 6| Step: 1
Training loss: 2.493478298187256
Validation loss: 2.205273146270424

Epoch: 6| Step: 2
Training loss: 2.2231998443603516
Validation loss: 2.200315329336351

Epoch: 6| Step: 3
Training loss: 3.3892717361450195
Validation loss: 2.214357988808745

Epoch: 6| Step: 4
Training loss: 1.7289550304412842
Validation loss: 2.223324457804362

Epoch: 6| Step: 5
Training loss: 2.0018727779388428
Validation loss: 2.2206684453513033

Epoch: 6| Step: 6
Training loss: 2.58978271484375
Validation loss: 2.230154827076902

Epoch: 6| Step: 7
Training loss: 2.538684844970703
Validation loss: 2.2019303037274267

Epoch: 6| Step: 8
Training loss: 1.7942373752593994
Validation loss: 2.222517264786587

Epoch: 6| Step: 9
Training loss: 2.3940305709838867
Validation loss: 2.2317861023769585

Epoch: 6| Step: 10
Training loss: 2.3848023414611816
Validation loss: 2.2621528794688563

Epoch: 6| Step: 11
Training loss: 2.1533613204956055
Validation loss: 2.294767766870478

Epoch: 6| Step: 12
Training loss: 2.6824207305908203
Validation loss: 2.2731594219002673

Epoch: 6| Step: 13
Training loss: 2.389620780944824
Validation loss: 2.256294952925815

Epoch: 348| Step: 0
Training loss: 2.0360822677612305
Validation loss: 2.2373628847060667

Epoch: 6| Step: 1
Training loss: 2.1411032676696777
Validation loss: 2.2137437071851505

Epoch: 6| Step: 2
Training loss: 1.665354609489441
Validation loss: 2.1933402604954217

Epoch: 6| Step: 3
Training loss: 2.1674671173095703
Validation loss: 2.178879799381379

Epoch: 6| Step: 4
Training loss: 2.9648163318634033
Validation loss: 2.1843529567923596

Epoch: 6| Step: 5
Training loss: 1.865286111831665
Validation loss: 2.189911696218675

Epoch: 6| Step: 6
Training loss: 2.231398582458496
Validation loss: 2.188531726919195

Epoch: 6| Step: 7
Training loss: 2.1846461296081543
Validation loss: 2.1828656068412204

Epoch: 6| Step: 8
Training loss: 1.9711856842041016
Validation loss: 2.1944379088699177

Epoch: 6| Step: 9
Training loss: 2.8855762481689453
Validation loss: 2.1865232862452024

Epoch: 6| Step: 10
Training loss: 2.1991164684295654
Validation loss: 2.1839076985595045

Epoch: 6| Step: 11
Training loss: 2.651982545852661
Validation loss: 2.189179887053787

Epoch: 6| Step: 12
Training loss: 2.8094687461853027
Validation loss: 2.168981306014522

Epoch: 6| Step: 13
Training loss: 2.4760303497314453
Validation loss: 2.1800072398237003

Epoch: 349| Step: 0
Training loss: 2.8552589416503906
Validation loss: 2.1704354722012758

Epoch: 6| Step: 1
Training loss: 2.49765944480896
Validation loss: 2.1745741521158526

Epoch: 6| Step: 2
Training loss: 2.2537081241607666
Validation loss: 2.1740913288567656

Epoch: 6| Step: 3
Training loss: 2.531111717224121
Validation loss: 2.174873040568444

Epoch: 6| Step: 4
Training loss: 2.0835139751434326
Validation loss: 2.190112334425731

Epoch: 6| Step: 5
Training loss: 2.7326571941375732
Validation loss: 2.169267669800789

Epoch: 6| Step: 6
Training loss: 2.14418888092041
Validation loss: 2.2009475282443467

Epoch: 6| Step: 7
Training loss: 2.180375337600708
Validation loss: 2.2201741305730676

Epoch: 6| Step: 8
Training loss: 2.3339273929595947
Validation loss: 2.223645435866489

Epoch: 6| Step: 9
Training loss: 1.8914613723754883
Validation loss: 2.2438979123228338

Epoch: 6| Step: 10
Training loss: 1.9319697618484497
Validation loss: 2.226064187224193

Epoch: 6| Step: 11
Training loss: 2.027557849884033
Validation loss: 2.25238376022667

Epoch: 6| Step: 12
Training loss: 2.013568639755249
Validation loss: 2.231983733433549

Epoch: 6| Step: 13
Training loss: 3.06500244140625
Validation loss: 2.227440167498845

Epoch: 350| Step: 0
Training loss: 2.246000289916992
Validation loss: 2.210692649246544

Epoch: 6| Step: 1
Training loss: 2.6514077186584473
Validation loss: 2.1954434789637083

Epoch: 6| Step: 2
Training loss: 2.4711594581604004
Validation loss: 2.203904946645101

Epoch: 6| Step: 3
Training loss: 1.9217615127563477
Validation loss: 2.216653326506256

Epoch: 6| Step: 4
Training loss: 2.1556129455566406
Validation loss: 2.221422541526056

Epoch: 6| Step: 5
Training loss: 2.1749610900878906
Validation loss: 2.2063666287288872

Epoch: 6| Step: 6
Training loss: 3.0534181594848633
Validation loss: 2.221561752339845

Epoch: 6| Step: 7
Training loss: 2.201681613922119
Validation loss: 2.214401557881345

Epoch: 6| Step: 8
Training loss: 1.9630199670791626
Validation loss: 2.214743032250353

Epoch: 6| Step: 9
Training loss: 2.6231863498687744
Validation loss: 2.213034547785277

Epoch: 6| Step: 10
Training loss: 2.6338768005371094
Validation loss: 2.2023569319837835

Epoch: 6| Step: 11
Training loss: 1.967142939567566
Validation loss: 2.205454594345503

Epoch: 6| Step: 12
Training loss: 2.0993833541870117
Validation loss: 2.1984398531657394

Epoch: 6| Step: 13
Training loss: 1.8663618564605713
Validation loss: 2.214065954249392

Testing loss: 2.3955865754021537

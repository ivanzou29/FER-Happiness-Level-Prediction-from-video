Epoch: 1| Step: 0
Training loss: 4.57651424407959
Validation loss: 5.120296524417016

Epoch: 6| Step: 1
Training loss: 4.546321392059326
Validation loss: 5.112616708201747

Epoch: 6| Step: 2
Training loss: 5.836612701416016
Validation loss: 5.104948633460588

Epoch: 6| Step: 3
Training loss: 3.9765419960021973
Validation loss: 5.097663453830186

Epoch: 6| Step: 4
Training loss: 5.907284736633301
Validation loss: 5.0899362769178165

Epoch: 6| Step: 5
Training loss: 5.578943252563477
Validation loss: 5.08195359219787

Epoch: 6| Step: 6
Training loss: 5.676258563995361
Validation loss: 5.073195134439776

Epoch: 6| Step: 7
Training loss: 6.230273246765137
Validation loss: 5.063634077707927

Epoch: 6| Step: 8
Training loss: 5.018398761749268
Validation loss: 5.052875898217642

Epoch: 6| Step: 9
Training loss: 4.385053634643555
Validation loss: 5.041582927908949

Epoch: 6| Step: 10
Training loss: 4.0832343101501465
Validation loss: 5.0292357424254055

Epoch: 6| Step: 11
Training loss: 4.166417121887207
Validation loss: 5.015359053047755

Epoch: 6| Step: 12
Training loss: 3.73760986328125
Validation loss: 5.000786099382626

Epoch: 6| Step: 13
Training loss: 3.923112630844116
Validation loss: 4.985348747622583

Epoch: 2| Step: 0
Training loss: 4.246786117553711
Validation loss: 4.968210620264853

Epoch: 6| Step: 1
Training loss: 4.375858306884766
Validation loss: 4.95033113930815

Epoch: 6| Step: 2
Training loss: 3.9436159133911133
Validation loss: 4.930900968531127

Epoch: 6| Step: 3
Training loss: 5.575397491455078
Validation loss: 4.909658226915585

Epoch: 6| Step: 4
Training loss: 5.072895050048828
Validation loss: 4.88628020850561

Epoch: 6| Step: 5
Training loss: 4.222820281982422
Validation loss: 4.860765123880038

Epoch: 6| Step: 6
Training loss: 3.525916814804077
Validation loss: 4.834627659090104

Epoch: 6| Step: 7
Training loss: 4.559685707092285
Validation loss: 4.80623879483951

Epoch: 6| Step: 8
Training loss: 4.7965593338012695
Validation loss: 4.774754734449489

Epoch: 6| Step: 9
Training loss: 5.344853401184082
Validation loss: 4.743014453559794

Epoch: 6| Step: 10
Training loss: 5.043935298919678
Validation loss: 4.707735256482196

Epoch: 6| Step: 11
Training loss: 4.939360618591309
Validation loss: 4.672158671963599

Epoch: 6| Step: 12
Training loss: 4.935250282287598
Validation loss: 4.6335000222729095

Epoch: 6| Step: 13
Training loss: 3.060608148574829
Validation loss: 4.5927244668365805

Epoch: 3| Step: 0
Training loss: 3.1048784255981445
Validation loss: 4.551766175095753

Epoch: 6| Step: 1
Training loss: 4.567133903503418
Validation loss: 4.5087041290857455

Epoch: 6| Step: 2
Training loss: 3.4253604412078857
Validation loss: 4.46395363858951

Epoch: 6| Step: 3
Training loss: 4.405081748962402
Validation loss: 4.42049446926322

Epoch: 6| Step: 4
Training loss: 4.914942741394043
Validation loss: 4.378922288135816

Epoch: 6| Step: 5
Training loss: 4.092811584472656
Validation loss: 4.334796490207795

Epoch: 6| Step: 6
Training loss: 3.809365749359131
Validation loss: 4.291253464196318

Epoch: 6| Step: 7
Training loss: 4.5590057373046875
Validation loss: 4.246478662695936

Epoch: 6| Step: 8
Training loss: 4.479700565338135
Validation loss: 4.204723660663892

Epoch: 6| Step: 9
Training loss: 3.3233466148376465
Validation loss: 4.161818576115434

Epoch: 6| Step: 10
Training loss: 4.126206398010254
Validation loss: 4.117327756779169

Epoch: 6| Step: 11
Training loss: 4.573032379150391
Validation loss: 4.075497114530173

Epoch: 6| Step: 12
Training loss: 3.6322426795959473
Validation loss: 4.029775886125462

Epoch: 6| Step: 13
Training loss: 3.856804847717285
Validation loss: 3.986018357738372

Epoch: 4| Step: 0
Training loss: 4.943533897399902
Validation loss: 3.938766294910062

Epoch: 6| Step: 1
Training loss: 2.9560091495513916
Validation loss: 3.8927394164505826

Epoch: 6| Step: 2
Training loss: 3.1967670917510986
Validation loss: 3.850336697793776

Epoch: 6| Step: 3
Training loss: 4.21893310546875
Validation loss: 3.811079509796635

Epoch: 6| Step: 4
Training loss: 4.0181074142456055
Validation loss: 3.7741324029942995

Epoch: 6| Step: 5
Training loss: 2.6051177978515625
Validation loss: 3.741441306247506

Epoch: 6| Step: 6
Training loss: 5.000123023986816
Validation loss: 3.712908870430403

Epoch: 6| Step: 7
Training loss: 3.371171712875366
Validation loss: 3.6858086432180097

Epoch: 6| Step: 8
Training loss: 3.1797890663146973
Validation loss: 3.660646169416366

Epoch: 6| Step: 9
Training loss: 3.8475117683410645
Validation loss: 3.6378362870985463

Epoch: 6| Step: 10
Training loss: 4.597655296325684
Validation loss: 3.618311010381227

Epoch: 6| Step: 11
Training loss: 2.355228900909424
Validation loss: 3.6017372403093564

Epoch: 6| Step: 12
Training loss: 3.081815004348755
Validation loss: 3.5878559594513266

Epoch: 6| Step: 13
Training loss: 3.270815849304199
Validation loss: 3.5746563814019643

Epoch: 5| Step: 0
Training loss: 3.7284679412841797
Validation loss: 3.5630905346203874

Epoch: 6| Step: 1
Training loss: 3.9844181537628174
Validation loss: 3.5515488911700506

Epoch: 6| Step: 2
Training loss: 2.8821592330932617
Validation loss: 3.5342271379245225

Epoch: 6| Step: 3
Training loss: 3.8185923099517822
Validation loss: 3.518441656584381

Epoch: 6| Step: 4
Training loss: 4.652256965637207
Validation loss: 3.501755706725582

Epoch: 6| Step: 5
Training loss: 4.056192874908447
Validation loss: 3.484334586769022

Epoch: 6| Step: 6
Training loss: 2.683194160461426
Validation loss: 3.4691715086660078

Epoch: 6| Step: 7
Training loss: 2.8255317211151123
Validation loss: 3.4578535813157276

Epoch: 6| Step: 8
Training loss: 3.1403565406799316
Validation loss: 3.4438156107420563

Epoch: 6| Step: 9
Training loss: 3.3069448471069336
Validation loss: 3.434528858430924

Epoch: 6| Step: 10
Training loss: 3.400362968444824
Validation loss: 3.4234810977853756

Epoch: 6| Step: 11
Training loss: 3.4898571968078613
Validation loss: 3.4126315732156076

Epoch: 6| Step: 12
Training loss: 2.4705119132995605
Validation loss: 3.4036729976695073

Epoch: 6| Step: 13
Training loss: 3.3934695720672607
Validation loss: 3.3962151414604596

Epoch: 6| Step: 0
Training loss: 3.1114799976348877
Validation loss: 3.3906615780245875

Epoch: 6| Step: 1
Training loss: 3.6700892448425293
Validation loss: 3.3823159176816224

Epoch: 6| Step: 2
Training loss: 3.2588000297546387
Validation loss: 3.3712749122291483

Epoch: 6| Step: 3
Training loss: 4.251229286193848
Validation loss: 3.3615909725107174

Epoch: 6| Step: 4
Training loss: 3.264458656311035
Validation loss: 3.3536710713499334

Epoch: 6| Step: 5
Training loss: 2.5854015350341797
Validation loss: 3.3449590052327802

Epoch: 6| Step: 6
Training loss: 3.2896273136138916
Validation loss: 3.3384271078212286

Epoch: 6| Step: 7
Training loss: 3.0470619201660156
Validation loss: 3.3299866825021724

Epoch: 6| Step: 8
Training loss: 3.355074405670166
Validation loss: 3.3197412516481135

Epoch: 6| Step: 9
Training loss: 3.445798397064209
Validation loss: 3.3106670661639144

Epoch: 6| Step: 10
Training loss: 3.4216365814208984
Validation loss: 3.3026347391067015

Epoch: 6| Step: 11
Training loss: 3.0543859004974365
Validation loss: 3.2921682634661273

Epoch: 6| Step: 12
Training loss: 3.0424726009368896
Validation loss: 3.2861346993395077

Epoch: 6| Step: 13
Training loss: 3.40341854095459
Validation loss: 3.2783092862816265

Epoch: 7| Step: 0
Training loss: 3.733776330947876
Validation loss: 3.2691010582831597

Epoch: 6| Step: 1
Training loss: 3.2380106449127197
Validation loss: 3.2631455390684065

Epoch: 6| Step: 2
Training loss: 2.8273487091064453
Validation loss: 3.255324773890998

Epoch: 6| Step: 3
Training loss: 4.214611530303955
Validation loss: 3.249858328091201

Epoch: 6| Step: 4
Training loss: 3.6174299716949463
Validation loss: 3.2433534411973852

Epoch: 6| Step: 5
Training loss: 3.2578930854797363
Validation loss: 3.2355695283541115

Epoch: 6| Step: 6
Training loss: 2.8730125427246094
Validation loss: 3.2293830712636313

Epoch: 6| Step: 7
Training loss: 3.4155986309051514
Validation loss: 3.220822967508788

Epoch: 6| Step: 8
Training loss: 3.3893356323242188
Validation loss: 3.2136363008970856

Epoch: 6| Step: 9
Training loss: 3.185753107070923
Validation loss: 3.2056351682191253

Epoch: 6| Step: 10
Training loss: 3.0394539833068848
Validation loss: 3.200703687565301

Epoch: 6| Step: 11
Training loss: 2.547502279281616
Validation loss: 3.1965599700968754

Epoch: 6| Step: 12
Training loss: 2.260692596435547
Validation loss: 3.1931104711306992

Epoch: 6| Step: 13
Training loss: 3.6001832485198975
Validation loss: 3.1915754143909743

Epoch: 8| Step: 0
Training loss: 3.6232094764709473
Validation loss: 3.1819378278588735

Epoch: 6| Step: 1
Training loss: 3.309603214263916
Validation loss: 3.1767073497977307

Epoch: 6| Step: 2
Training loss: 3.1209425926208496
Validation loss: 3.170210358917072

Epoch: 6| Step: 3
Training loss: 3.2842297554016113
Validation loss: 3.1651313920174875

Epoch: 6| Step: 4
Training loss: 3.1159749031066895
Validation loss: 3.159592133696361

Epoch: 6| Step: 5
Training loss: 2.488025188446045
Validation loss: 3.1534417598478255

Epoch: 6| Step: 6
Training loss: 2.8072071075439453
Validation loss: 3.1484120712485364

Epoch: 6| Step: 7
Training loss: 3.3605024814605713
Validation loss: 3.145717285012686

Epoch: 6| Step: 8
Training loss: 3.457263469696045
Validation loss: 3.1487510768316125

Epoch: 6| Step: 9
Training loss: 3.1123149394989014
Validation loss: 3.13549893389466

Epoch: 6| Step: 10
Training loss: 3.2524023056030273
Validation loss: 3.1295164015985306

Epoch: 6| Step: 11
Training loss: 3.916475296020508
Validation loss: 3.123591597362231

Epoch: 6| Step: 12
Training loss: 2.0509090423583984
Validation loss: 3.1178860843822522

Epoch: 6| Step: 13
Training loss: 3.526669502258301
Validation loss: 3.1153103382356706

Epoch: 9| Step: 0
Training loss: 2.8962044715881348
Validation loss: 3.10814860815643

Epoch: 6| Step: 1
Training loss: 2.893979072570801
Validation loss: 3.1022716619635142

Epoch: 6| Step: 2
Training loss: 3.6803908348083496
Validation loss: 3.098315287661809

Epoch: 6| Step: 3
Training loss: 3.3240325450897217
Validation loss: 3.093477790073682

Epoch: 6| Step: 4
Training loss: 2.7085375785827637
Validation loss: 3.0855687972038024

Epoch: 6| Step: 5
Training loss: 3.046182632446289
Validation loss: 3.081749951967629

Epoch: 6| Step: 6
Training loss: 2.696789264678955
Validation loss: 3.07766039909855

Epoch: 6| Step: 7
Training loss: 3.564687967300415
Validation loss: 3.072410709114485

Epoch: 6| Step: 8
Training loss: 2.6783447265625
Validation loss: 3.065039591122699

Epoch: 6| Step: 9
Training loss: 2.6060104370117188
Validation loss: 3.0618222990343646

Epoch: 6| Step: 10
Training loss: 3.479679822921753
Validation loss: 3.056623897244853

Epoch: 6| Step: 11
Training loss: 2.666531562805176
Validation loss: 3.0524389231076805

Epoch: 6| Step: 12
Training loss: 3.8840458393096924
Validation loss: 3.0520074649523665

Epoch: 6| Step: 13
Training loss: 3.6645097732543945
Validation loss: 3.043561920042961

Epoch: 10| Step: 0
Training loss: 2.8695125579833984
Validation loss: 3.034789252024825

Epoch: 6| Step: 1
Training loss: 3.736126661300659
Validation loss: 3.0297248696768158

Epoch: 6| Step: 2
Training loss: 2.8468141555786133
Validation loss: 3.0255971108713458

Epoch: 6| Step: 3
Training loss: 3.0956757068634033
Validation loss: 3.024103638946369

Epoch: 6| Step: 4
Training loss: 3.165030002593994
Validation loss: 3.0213064019398024

Epoch: 6| Step: 5
Training loss: 3.1338448524475098
Validation loss: 3.0159847197994107

Epoch: 6| Step: 6
Training loss: 2.756624698638916
Validation loss: 3.0164042647166918

Epoch: 6| Step: 7
Training loss: 2.1678223609924316
Validation loss: 3.013791007380332

Epoch: 6| Step: 8
Training loss: 3.0032308101654053
Validation loss: 3.003211518769623

Epoch: 6| Step: 9
Training loss: 2.3893332481384277
Validation loss: 2.998927293285247

Epoch: 6| Step: 10
Training loss: 4.5003862380981445
Validation loss: 2.9964332939476095

Epoch: 6| Step: 11
Training loss: 2.761835813522339
Validation loss: 2.993186555882936

Epoch: 6| Step: 12
Training loss: 3.6697044372558594
Validation loss: 2.9857295713117047

Epoch: 6| Step: 13
Training loss: 2.7542965412139893
Validation loss: 2.9805539961784118

Epoch: 11| Step: 0
Training loss: 3.0356855392456055
Validation loss: 2.9795690557008148

Epoch: 6| Step: 1
Training loss: 2.9324944019317627
Validation loss: 2.9752437530025357

Epoch: 6| Step: 2
Training loss: 3.326087236404419
Validation loss: 2.972379538320726

Epoch: 6| Step: 3
Training loss: 2.397494316101074
Validation loss: 2.967489578390634

Epoch: 6| Step: 4
Training loss: 3.3839454650878906
Validation loss: 2.964597161098193

Epoch: 6| Step: 5
Training loss: 2.5890820026397705
Validation loss: 2.9629570643107095

Epoch: 6| Step: 6
Training loss: 3.630993127822876
Validation loss: 2.95987283286228

Epoch: 6| Step: 7
Training loss: 3.243774890899658
Validation loss: 2.9574482748585362

Epoch: 6| Step: 8
Training loss: 2.8778040409088135
Validation loss: 2.952473768623926

Epoch: 6| Step: 9
Training loss: 3.174612045288086
Validation loss: 2.9507881569606003

Epoch: 6| Step: 10
Training loss: 2.55234432220459
Validation loss: 2.9466078640312277

Epoch: 6| Step: 11
Training loss: 2.75803804397583
Validation loss: 2.944392675994545

Epoch: 6| Step: 12
Training loss: 3.1931304931640625
Validation loss: 2.9421962127890637

Epoch: 6| Step: 13
Training loss: 3.6963086128234863
Validation loss: 2.940682772667177

Epoch: 12| Step: 0
Training loss: 2.8670618534088135
Validation loss: 2.9363602258825816

Epoch: 6| Step: 1
Training loss: 3.3124279975891113
Validation loss: 2.9346163247221257

Epoch: 6| Step: 2
Training loss: 3.612189531326294
Validation loss: 2.932887451623076

Epoch: 6| Step: 3
Training loss: 3.6266069412231445
Validation loss: 2.93093442404142

Epoch: 6| Step: 4
Training loss: 3.0409650802612305
Validation loss: 2.928416023972214

Epoch: 6| Step: 5
Training loss: 2.147576332092285
Validation loss: 2.9271013557270007

Epoch: 6| Step: 6
Training loss: 3.12302827835083
Validation loss: 2.922972689392746

Epoch: 6| Step: 7
Training loss: 3.6991400718688965
Validation loss: 2.921140511830648

Epoch: 6| Step: 8
Training loss: 1.9753626585006714
Validation loss: 2.921749968682566

Epoch: 6| Step: 9
Training loss: 3.7634575366973877
Validation loss: 2.9170054902312574

Epoch: 6| Step: 10
Training loss: 2.393059015274048
Validation loss: 2.9178699626717517

Epoch: 6| Step: 11
Training loss: 2.7824976444244385
Validation loss: 2.915646340257378

Epoch: 6| Step: 12
Training loss: 2.694180488586426
Validation loss: 2.916621600427935

Epoch: 6| Step: 13
Training loss: 3.2865824699401855
Validation loss: 2.9113229756714194

Epoch: 13| Step: 0
Training loss: 3.126755714416504
Validation loss: 2.9083136384205153

Epoch: 6| Step: 1
Training loss: 3.0511603355407715
Validation loss: 2.907482957327238

Epoch: 6| Step: 2
Training loss: 2.6430084705352783
Validation loss: 2.9047932009543143

Epoch: 6| Step: 3
Training loss: 2.6340911388397217
Validation loss: 2.901272625051519

Epoch: 6| Step: 4
Training loss: 3.5205917358398438
Validation loss: 2.8984466752698346

Epoch: 6| Step: 5
Training loss: 2.6439013481140137
Validation loss: 2.8973437996320826

Epoch: 6| Step: 6
Training loss: 3.0079259872436523
Validation loss: 2.8930556389593307

Epoch: 6| Step: 7
Training loss: 3.0503175258636475
Validation loss: 2.893300856313398

Epoch: 6| Step: 8
Training loss: 2.637901782989502
Validation loss: 2.8915342515514744

Epoch: 6| Step: 9
Training loss: 3.021287202835083
Validation loss: 2.8869771777942614

Epoch: 6| Step: 10
Training loss: 2.50140380859375
Validation loss: 2.8829778496937086

Epoch: 6| Step: 11
Training loss: 3.291940212249756
Validation loss: 2.882896625867454

Epoch: 6| Step: 12
Training loss: 3.798485279083252
Validation loss: 2.879182900151899

Epoch: 6| Step: 13
Training loss: 3.0030717849731445
Validation loss: 2.874335781220467

Epoch: 14| Step: 0
Training loss: 3.3142290115356445
Validation loss: 2.873603154254216

Epoch: 6| Step: 1
Training loss: 2.0766634941101074
Validation loss: 2.872392572382445

Epoch: 6| Step: 2
Training loss: 3.0786819458007812
Validation loss: 2.869603544153193

Epoch: 6| Step: 3
Training loss: 3.801356792449951
Validation loss: 2.869214116886098

Epoch: 6| Step: 4
Training loss: 2.966625690460205
Validation loss: 2.8708594101731495

Epoch: 6| Step: 5
Training loss: 3.923429250717163
Validation loss: 2.8615607343694216

Epoch: 6| Step: 6
Training loss: 3.007150650024414
Validation loss: 2.8623368945173038

Epoch: 6| Step: 7
Training loss: 3.133352756500244
Validation loss: 2.867930555856356

Epoch: 6| Step: 8
Training loss: 2.627852201461792
Validation loss: 2.8712720973517305

Epoch: 6| Step: 9
Training loss: 2.60508394241333
Validation loss: 2.867794147101782

Epoch: 6| Step: 10
Training loss: 2.6993443965911865
Validation loss: 2.8577020296486477

Epoch: 6| Step: 11
Training loss: 2.557689666748047
Validation loss: 2.85113198526444

Epoch: 6| Step: 12
Training loss: 2.61604380607605
Validation loss: 2.8532385364655526

Epoch: 6| Step: 13
Training loss: 3.4834554195404053
Validation loss: 2.853688883525069

Epoch: 15| Step: 0
Training loss: 2.3606879711151123
Validation loss: 2.858810387631898

Epoch: 6| Step: 1
Training loss: 2.4527077674865723
Validation loss: 2.858114557881509

Epoch: 6| Step: 2
Training loss: 3.4906435012817383
Validation loss: 2.8534958157488095

Epoch: 6| Step: 3
Training loss: 2.9068233966827393
Validation loss: 2.852144310551305

Epoch: 6| Step: 4
Training loss: 3.1577470302581787
Validation loss: 2.849553136415379

Epoch: 6| Step: 5
Training loss: 3.125410795211792
Validation loss: 2.8462809413991947

Epoch: 6| Step: 6
Training loss: 3.355175495147705
Validation loss: 2.8439569114356913

Epoch: 6| Step: 7
Training loss: 1.7091401815414429
Validation loss: 2.8426294711328324

Epoch: 6| Step: 8
Training loss: 3.3365612030029297
Validation loss: 2.838942168861307

Epoch: 6| Step: 9
Training loss: 2.3568878173828125
Validation loss: 2.837389592201479

Epoch: 6| Step: 10
Training loss: 3.583508014678955
Validation loss: 2.835367992360105

Epoch: 6| Step: 11
Training loss: 3.4123146533966064
Validation loss: 2.8345722434341267

Epoch: 6| Step: 12
Training loss: 2.883632183074951
Validation loss: 2.834339810955909

Epoch: 6| Step: 13
Training loss: 3.616515636444092
Validation loss: 2.8324959457561536

Epoch: 16| Step: 0
Training loss: 3.3935470581054688
Validation loss: 2.8295022595313286

Epoch: 6| Step: 1
Training loss: 2.0624046325683594
Validation loss: 2.8280098104989655

Epoch: 6| Step: 2
Training loss: 3.1195762157440186
Validation loss: 2.8264588925146286

Epoch: 6| Step: 3
Training loss: 1.9743266105651855
Validation loss: 2.82437180190958

Epoch: 6| Step: 4
Training loss: 3.3325448036193848
Validation loss: 2.823188712519984

Epoch: 6| Step: 5
Training loss: 3.002638339996338
Validation loss: 2.821274242093486

Epoch: 6| Step: 6
Training loss: 2.7256479263305664
Validation loss: 2.8200884429357385

Epoch: 6| Step: 7
Training loss: 2.552405834197998
Validation loss: 2.8188127548463884

Epoch: 6| Step: 8
Training loss: 4.039563179016113
Validation loss: 2.8173825561359362

Epoch: 6| Step: 9
Training loss: 3.1931896209716797
Validation loss: 2.814036830779045

Epoch: 6| Step: 10
Training loss: 3.245640277862549
Validation loss: 2.8128163724817257

Epoch: 6| Step: 11
Training loss: 3.0097055435180664
Validation loss: 2.807607135465068

Epoch: 6| Step: 12
Training loss: 2.6332855224609375
Validation loss: 2.802549223746023

Epoch: 6| Step: 13
Training loss: 2.9774014949798584
Validation loss: 2.803644767371557

Epoch: 17| Step: 0
Training loss: 2.717909812927246
Validation loss: 2.7992070900496615

Epoch: 6| Step: 1
Training loss: 2.844374656677246
Validation loss: 2.7997278936447634

Epoch: 6| Step: 2
Training loss: 2.4139533042907715
Validation loss: 2.7967389860460834

Epoch: 6| Step: 3
Training loss: 3.5926413536071777
Validation loss: 2.795840181330199

Epoch: 6| Step: 4
Training loss: 2.806518316268921
Validation loss: 2.7952617086390013

Epoch: 6| Step: 5
Training loss: 2.77553653717041
Validation loss: 2.7939284437446186

Epoch: 6| Step: 6
Training loss: 2.8838109970092773
Validation loss: 2.793283639415618

Epoch: 6| Step: 7
Training loss: 2.9441685676574707
Validation loss: 2.7911258897473736

Epoch: 6| Step: 8
Training loss: 3.2543387413024902
Validation loss: 2.7920345362796577

Epoch: 6| Step: 9
Training loss: 3.3585996627807617
Validation loss: 2.790883764143913

Epoch: 6| Step: 10
Training loss: 3.164077043533325
Validation loss: 2.789359910513765

Epoch: 6| Step: 11
Training loss: 1.9887889623641968
Validation loss: 2.7888084406493814

Epoch: 6| Step: 12
Training loss: 3.006263494491577
Validation loss: 2.786886276737336

Epoch: 6| Step: 13
Training loss: 3.54312801361084
Validation loss: 2.7864984363637944

Epoch: 18| Step: 0
Training loss: 2.9053428173065186
Validation loss: 2.7847828352323143

Epoch: 6| Step: 1
Training loss: 2.6557750701904297
Validation loss: 2.783545476134105

Epoch: 6| Step: 2
Training loss: 2.7335238456726074
Validation loss: 2.783061981201172

Epoch: 6| Step: 3
Training loss: 2.859304666519165
Validation loss: 2.787337831271592

Epoch: 6| Step: 4
Training loss: 2.769120931625366
Validation loss: 2.7948034194207962

Epoch: 6| Step: 5
Training loss: 2.708705425262451
Validation loss: 2.7848007473894345

Epoch: 6| Step: 6
Training loss: 2.9582033157348633
Validation loss: 2.792627634540681

Epoch: 6| Step: 7
Training loss: 3.5844812393188477
Validation loss: 2.7980378315012944

Epoch: 6| Step: 8
Training loss: 3.4021952152252197
Validation loss: 2.7982050039434947

Epoch: 6| Step: 9
Training loss: 2.6979618072509766
Validation loss: 2.7947303043898715

Epoch: 6| Step: 10
Training loss: 2.9192824363708496
Validation loss: 2.7919539200362338

Epoch: 6| Step: 11
Training loss: 2.642331600189209
Validation loss: 2.7932475895010014

Epoch: 6| Step: 12
Training loss: 3.4043478965759277
Validation loss: 2.798396461753435

Epoch: 6| Step: 13
Training loss: 2.6811704635620117
Validation loss: 2.8000902078484975

Epoch: 19| Step: 0
Training loss: 2.6308717727661133
Validation loss: 2.7991185060111423

Epoch: 6| Step: 1
Training loss: 2.5813939571380615
Validation loss: 2.7960973913951586

Epoch: 6| Step: 2
Training loss: 2.3335485458374023
Validation loss: 2.7933122880997194

Epoch: 6| Step: 3
Training loss: 3.339390277862549
Validation loss: 2.787738520611999

Epoch: 6| Step: 4
Training loss: 1.6714421510696411
Validation loss: 2.7823887153338362

Epoch: 6| Step: 5
Training loss: 3.2642831802368164
Validation loss: 2.7810811611913864

Epoch: 6| Step: 6
Training loss: 3.337038516998291
Validation loss: 2.7808976968129477

Epoch: 6| Step: 7
Training loss: 3.030381202697754
Validation loss: 2.779392255249844

Epoch: 6| Step: 8
Training loss: 2.3781471252441406
Validation loss: 2.7780817247206167

Epoch: 6| Step: 9
Training loss: 3.8681859970092773
Validation loss: 2.77775897518281

Epoch: 6| Step: 10
Training loss: 2.9782185554504395
Validation loss: 2.7782190589494604

Epoch: 6| Step: 11
Training loss: 2.9923553466796875
Validation loss: 2.7759279717681227

Epoch: 6| Step: 12
Training loss: 3.47819185256958
Validation loss: 2.773138407737978

Epoch: 6| Step: 13
Training loss: 3.1140458583831787
Validation loss: 2.7728778111037387

Epoch: 20| Step: 0
Training loss: 2.9239134788513184
Validation loss: 2.7717447024519726

Epoch: 6| Step: 1
Training loss: 2.7587757110595703
Validation loss: 2.7717635093196744

Epoch: 6| Step: 2
Training loss: 3.17547607421875
Validation loss: 2.7706722700467674

Epoch: 6| Step: 3
Training loss: 2.7207324504852295
Validation loss: 2.770527801206035

Epoch: 6| Step: 4
Training loss: 2.0926458835601807
Validation loss: 2.7712816730622323

Epoch: 6| Step: 5
Training loss: 3.323777437210083
Validation loss: 2.7707691038808515

Epoch: 6| Step: 6
Training loss: 2.6275901794433594
Validation loss: 2.7706367020965903

Epoch: 6| Step: 7
Training loss: 3.6087498664855957
Validation loss: 2.7699234331807783

Epoch: 6| Step: 8
Training loss: 3.053981304168701
Validation loss: 2.7678633710389495

Epoch: 6| Step: 9
Training loss: 3.2565815448760986
Validation loss: 2.7668083483173

Epoch: 6| Step: 10
Training loss: 2.8149125576019287
Validation loss: 2.7660126327186503

Epoch: 6| Step: 11
Training loss: 2.598179578781128
Validation loss: 2.765773909066313

Epoch: 6| Step: 12
Training loss: 2.325730800628662
Validation loss: 2.7653869608397126

Epoch: 6| Step: 13
Training loss: 4.037116050720215
Validation loss: 2.7652078341412287

Epoch: 21| Step: 0
Training loss: 3.1835720539093018
Validation loss: 2.7654162940158638

Epoch: 6| Step: 1
Training loss: 3.901038408279419
Validation loss: 2.764505155624882

Epoch: 6| Step: 2
Training loss: 3.6202712059020996
Validation loss: 2.764479798655356

Epoch: 6| Step: 3
Training loss: 2.213244915008545
Validation loss: 2.7641529088379233

Epoch: 6| Step: 4
Training loss: 2.3242831230163574
Validation loss: 2.7631600979835755

Epoch: 6| Step: 5
Training loss: 2.0624680519104004
Validation loss: 2.7622246383338847

Epoch: 6| Step: 6
Training loss: 3.3567771911621094
Validation loss: 2.762559631819366

Epoch: 6| Step: 7
Training loss: 2.630807638168335
Validation loss: 2.762081256476782

Epoch: 6| Step: 8
Training loss: 3.3193912506103516
Validation loss: 2.7632969656298236

Epoch: 6| Step: 9
Training loss: 2.4874231815338135
Validation loss: 2.761720936785462

Epoch: 6| Step: 10
Training loss: 3.4228434562683105
Validation loss: 2.761054285110966

Epoch: 6| Step: 11
Training loss: 3.4385714530944824
Validation loss: 2.760774714972383

Epoch: 6| Step: 12
Training loss: 2.2528438568115234
Validation loss: 2.759904505104147

Epoch: 6| Step: 13
Training loss: 2.2560744285583496
Validation loss: 2.759631438921857

Epoch: 22| Step: 0
Training loss: 3.0160369873046875
Validation loss: 2.7598388297583467

Epoch: 6| Step: 1
Training loss: 2.070065975189209
Validation loss: 2.7589622876977407

Epoch: 6| Step: 2
Training loss: 3.300769567489624
Validation loss: 2.7594860830614643

Epoch: 6| Step: 3
Training loss: 2.9261746406555176
Validation loss: 2.7598422778550016

Epoch: 6| Step: 4
Training loss: 3.0178472995758057
Validation loss: 2.7594975579169487

Epoch: 6| Step: 5
Training loss: 2.3907527923583984
Validation loss: 2.7604854029993855

Epoch: 6| Step: 6
Training loss: 2.8338465690612793
Validation loss: 2.758108131347164

Epoch: 6| Step: 7
Training loss: 3.215528964996338
Validation loss: 2.757991270352435

Epoch: 6| Step: 8
Training loss: 2.577000141143799
Validation loss: 2.757677847339261

Epoch: 6| Step: 9
Training loss: 3.526472806930542
Validation loss: 2.7575073703642814

Epoch: 6| Step: 10
Training loss: 2.9347124099731445
Validation loss: 2.7565783377616637

Epoch: 6| Step: 11
Training loss: 3.5144619941711426
Validation loss: 2.7564223376653527

Epoch: 6| Step: 12
Training loss: 2.7145872116088867
Validation loss: 2.7562444107506865

Epoch: 6| Step: 13
Training loss: 2.4810714721679688
Validation loss: 2.7558311493166032

Epoch: 23| Step: 0
Training loss: 2.8897581100463867
Validation loss: 2.7567248575149046

Epoch: 6| Step: 1
Training loss: 2.1674766540527344
Validation loss: 2.7557051156156804

Epoch: 6| Step: 2
Training loss: 2.8503904342651367
Validation loss: 2.7556683248089207

Epoch: 6| Step: 3
Training loss: 2.6065335273742676
Validation loss: 2.7551627774392404

Epoch: 6| Step: 4
Training loss: 2.6631603240966797
Validation loss: 2.755010202366819

Epoch: 6| Step: 5
Training loss: 2.1633400917053223
Validation loss: 2.7540061217482372

Epoch: 6| Step: 6
Training loss: 3.9950342178344727
Validation loss: 2.7539785344113588

Epoch: 6| Step: 7
Training loss: 2.9415202140808105
Validation loss: 2.753279665464996

Epoch: 6| Step: 8
Training loss: 2.84706974029541
Validation loss: 2.7538589944121656

Epoch: 6| Step: 9
Training loss: 3.0717055797576904
Validation loss: 2.7517615595171527

Epoch: 6| Step: 10
Training loss: 3.1869726181030273
Validation loss: 2.7502925319056355

Epoch: 6| Step: 11
Training loss: 2.7257027626037598
Validation loss: 2.753279209136963

Epoch: 6| Step: 12
Training loss: 3.342301368713379
Validation loss: 2.751684986135011

Epoch: 6| Step: 13
Training loss: 3.445908546447754
Validation loss: 2.7494783837308168

Epoch: 24| Step: 0
Training loss: 3.2033329010009766
Validation loss: 2.7509019144119753

Epoch: 6| Step: 1
Training loss: 3.012104034423828
Validation loss: 2.7488966039431992

Epoch: 6| Step: 2
Training loss: 2.4445688724517822
Validation loss: 2.7502933240705922

Epoch: 6| Step: 3
Training loss: 3.2819879055023193
Validation loss: 2.7492340328872844

Epoch: 6| Step: 4
Training loss: 3.0964479446411133
Validation loss: 2.7553859423565608

Epoch: 6| Step: 5
Training loss: 3.04543399810791
Validation loss: 2.7562837113616285

Epoch: 6| Step: 6
Training loss: 3.236924648284912
Validation loss: 2.779883864105389

Epoch: 6| Step: 7
Training loss: 3.081233501434326
Validation loss: 2.820495390122937

Epoch: 6| Step: 8
Training loss: 3.020421028137207
Validation loss: 2.829206807639009

Epoch: 6| Step: 9
Training loss: 3.033608913421631
Validation loss: 2.8201846845688356

Epoch: 6| Step: 10
Training loss: 2.703984260559082
Validation loss: 2.8196453509792203

Epoch: 6| Step: 11
Training loss: 3.0875234603881836
Validation loss: 2.8139458087182816

Epoch: 6| Step: 12
Training loss: 2.228757619857788
Validation loss: 2.8122540263719458

Epoch: 6| Step: 13
Training loss: 2.0822415351867676
Validation loss: 2.8036375968686995

Epoch: 25| Step: 0
Training loss: 2.7082109451293945
Validation loss: 2.777863751175583

Epoch: 6| Step: 1
Training loss: 3.160273313522339
Validation loss: 2.755904571984404

Epoch: 6| Step: 2
Training loss: 3.1026904582977295
Validation loss: 2.76607713648068

Epoch: 6| Step: 3
Training loss: 2.4358749389648438
Validation loss: 2.762406544018817

Epoch: 6| Step: 4
Training loss: 2.7621426582336426
Validation loss: 2.7559229084240493

Epoch: 6| Step: 5
Training loss: 2.941624641418457
Validation loss: 2.7502813813506917

Epoch: 6| Step: 6
Training loss: 2.6947450637817383
Validation loss: 2.749395047464678

Epoch: 6| Step: 7
Training loss: 3.2804903984069824
Validation loss: 2.7486580469275035

Epoch: 6| Step: 8
Training loss: 3.3773465156555176
Validation loss: 2.751816318881127

Epoch: 6| Step: 9
Training loss: 2.3147382736206055
Validation loss: 2.7489599848306305

Epoch: 6| Step: 10
Training loss: 2.337831974029541
Validation loss: 2.7522267244195424

Epoch: 6| Step: 11
Training loss: 3.5251615047454834
Validation loss: 2.7518632617048038

Epoch: 6| Step: 12
Training loss: 2.976461172103882
Validation loss: 2.7551652590433755

Epoch: 6| Step: 13
Training loss: 3.1901748180389404
Validation loss: 2.7578341858361357

Epoch: 26| Step: 0
Training loss: 3.329347848892212
Validation loss: 2.7543274151381625

Epoch: 6| Step: 1
Training loss: 3.6403980255126953
Validation loss: 2.748960864159369

Epoch: 6| Step: 2
Training loss: 2.638371229171753
Validation loss: 2.747975169971425

Epoch: 6| Step: 3
Training loss: 2.374976396560669
Validation loss: 2.747887260170393

Epoch: 6| Step: 4
Training loss: 3.2796576023101807
Validation loss: 2.745683839244227

Epoch: 6| Step: 5
Training loss: 2.2886064052581787
Validation loss: 2.7452512735961587

Epoch: 6| Step: 6
Training loss: 3.337129831314087
Validation loss: 2.743039846420288

Epoch: 6| Step: 7
Training loss: 2.9064033031463623
Validation loss: 2.74231300046367

Epoch: 6| Step: 8
Training loss: 2.6545674800872803
Validation loss: 2.73979583350561

Epoch: 6| Step: 9
Training loss: 2.7691845893859863
Validation loss: 2.7405701196321877

Epoch: 6| Step: 10
Training loss: 3.508798599243164
Validation loss: 2.7398020477705103

Epoch: 6| Step: 11
Training loss: 3.1042675971984863
Validation loss: 2.739530024989959

Epoch: 6| Step: 12
Training loss: 2.069554567337036
Validation loss: 2.7390354500021985

Epoch: 6| Step: 13
Training loss: 2.515699863433838
Validation loss: 2.739332929734261

Epoch: 27| Step: 0
Training loss: 2.3159472942352295
Validation loss: 2.7384818625706497

Epoch: 6| Step: 1
Training loss: 3.680166721343994
Validation loss: 2.7381726054735083

Epoch: 6| Step: 2
Training loss: 2.9698028564453125
Validation loss: 2.738380721820298

Epoch: 6| Step: 3
Training loss: 3.0260021686553955
Validation loss: 2.737766278687344

Epoch: 6| Step: 4
Training loss: 3.315296173095703
Validation loss: 2.7372805405688543

Epoch: 6| Step: 5
Training loss: 2.602888345718384
Validation loss: 2.737216672589702

Epoch: 6| Step: 6
Training loss: 2.0085432529449463
Validation loss: 2.737094840695781

Epoch: 6| Step: 7
Training loss: 3.470221519470215
Validation loss: 2.7359499623698573

Epoch: 6| Step: 8
Training loss: 3.024196147918701
Validation loss: 2.7362944592711744

Epoch: 6| Step: 9
Training loss: 3.1091716289520264
Validation loss: 2.7355313095995175

Epoch: 6| Step: 10
Training loss: 2.574578285217285
Validation loss: 2.7360334242543867

Epoch: 6| Step: 11
Training loss: 2.4480090141296387
Validation loss: 2.7344696957577943

Epoch: 6| Step: 12
Training loss: 3.2504048347473145
Validation loss: 2.734009147972189

Epoch: 6| Step: 13
Training loss: 2.581880569458008
Validation loss: 2.734127385641939

Epoch: 28| Step: 0
Training loss: 3.074174404144287
Validation loss: 2.7343797529897382

Epoch: 6| Step: 1
Training loss: 2.08333420753479
Validation loss: 2.7336149190061834

Epoch: 6| Step: 2
Training loss: 3.5258350372314453
Validation loss: 2.7329885985261653

Epoch: 6| Step: 3
Training loss: 2.9376888275146484
Validation loss: 2.7324276252459456

Epoch: 6| Step: 4
Training loss: 2.2222845554351807
Validation loss: 2.7328150272369385

Epoch: 6| Step: 5
Training loss: 2.797879934310913
Validation loss: 2.7327813102353002

Epoch: 6| Step: 6
Training loss: 2.6223297119140625
Validation loss: 2.731562165803807

Epoch: 6| Step: 7
Training loss: 3.602879762649536
Validation loss: 2.7327681792679654

Epoch: 6| Step: 8
Training loss: 2.6393754482269287
Validation loss: 2.7324513389218237

Epoch: 6| Step: 9
Training loss: 3.0043773651123047
Validation loss: 2.7317272616970922

Epoch: 6| Step: 10
Training loss: 3.077549457550049
Validation loss: 2.7318391671744724

Epoch: 6| Step: 11
Training loss: 3.212571620941162
Validation loss: 2.7314059298525573

Epoch: 6| Step: 12
Training loss: 2.615809679031372
Validation loss: 2.730268447629867

Epoch: 6| Step: 13
Training loss: 3.1803674697875977
Validation loss: 2.730760846086728

Epoch: 29| Step: 0
Training loss: 2.1731114387512207
Validation loss: 2.729263851719518

Epoch: 6| Step: 1
Training loss: 2.425037145614624
Validation loss: 2.7294683917876212

Epoch: 6| Step: 2
Training loss: 2.6700990200042725
Validation loss: 2.729873154752998

Epoch: 6| Step: 3
Training loss: 2.68678617477417
Validation loss: 2.728032048030566

Epoch: 6| Step: 4
Training loss: 2.736908197402954
Validation loss: 2.7298240277074997

Epoch: 6| Step: 5
Training loss: 2.8095617294311523
Validation loss: 2.7292889907795894

Epoch: 6| Step: 6
Training loss: 3.159553050994873
Validation loss: 2.729425739216548

Epoch: 6| Step: 7
Training loss: 3.1840932369232178
Validation loss: 2.727523062818794

Epoch: 6| Step: 8
Training loss: 3.551710367202759
Validation loss: 2.727917153348205

Epoch: 6| Step: 9
Training loss: 3.110389232635498
Validation loss: 2.727768108408938

Epoch: 6| Step: 10
Training loss: 2.7064313888549805
Validation loss: 2.7265417037471646

Epoch: 6| Step: 11
Training loss: 3.2107672691345215
Validation loss: 2.726526657740275

Epoch: 6| Step: 12
Training loss: 3.0959911346435547
Validation loss: 2.7263977194345124

Epoch: 6| Step: 13
Training loss: 2.9354848861694336
Validation loss: 2.726340724575904

Epoch: 30| Step: 0
Training loss: 2.7979350090026855
Validation loss: 2.725934213207614

Epoch: 6| Step: 1
Training loss: 2.463675022125244
Validation loss: 2.726616341580627

Epoch: 6| Step: 2
Training loss: 3.013138771057129
Validation loss: 2.7254155476888022

Epoch: 6| Step: 3
Training loss: 2.86891770362854
Validation loss: 2.7296968352410103

Epoch: 6| Step: 4
Training loss: 3.8128881454467773
Validation loss: 2.726085196259201

Epoch: 6| Step: 5
Training loss: 2.572202682495117
Validation loss: 2.7252724196321223

Epoch: 6| Step: 6
Training loss: 3.2430496215820312
Validation loss: 2.7243014920142388

Epoch: 6| Step: 7
Training loss: 3.3844995498657227
Validation loss: 2.7240295076882965

Epoch: 6| Step: 8
Training loss: 2.668408155441284
Validation loss: 2.722773690377512

Epoch: 6| Step: 9
Training loss: 3.1147780418395996
Validation loss: 2.7220696198043

Epoch: 6| Step: 10
Training loss: 2.7267982959747314
Validation loss: 2.7217158463693436

Epoch: 6| Step: 11
Training loss: 3.0465917587280273
Validation loss: 2.721509641216647

Epoch: 6| Step: 12
Training loss: 1.876607894897461
Validation loss: 2.7220679713833715

Epoch: 6| Step: 13
Training loss: 2.74939227104187
Validation loss: 2.7213529309918805

Epoch: 31| Step: 0
Training loss: 3.1590263843536377
Validation loss: 2.7172224803637435

Epoch: 6| Step: 1
Training loss: 2.23199725151062
Validation loss: 2.7199020744651876

Epoch: 6| Step: 2
Training loss: 3.1153321266174316
Validation loss: 2.7207784498891523

Epoch: 6| Step: 3
Training loss: 2.3202781677246094
Validation loss: 2.7233019567305043

Epoch: 6| Step: 4
Training loss: 3.152916431427002
Validation loss: 2.7198363555374967

Epoch: 6| Step: 5
Training loss: 2.5220160484313965
Validation loss: 2.7236616175661803

Epoch: 6| Step: 6
Training loss: 2.819844961166382
Validation loss: 2.7257032932773715

Epoch: 6| Step: 7
Training loss: 3.4089818000793457
Validation loss: 2.722511245358375

Epoch: 6| Step: 8
Training loss: 2.583303689956665
Validation loss: 2.722700580473869

Epoch: 6| Step: 9
Training loss: 3.157008171081543
Validation loss: 2.7182019602867866

Epoch: 6| Step: 10
Training loss: 2.6096277236938477
Validation loss: 2.7170985744845484

Epoch: 6| Step: 11
Training loss: 3.377851724624634
Validation loss: 2.7161411469982517

Epoch: 6| Step: 12
Training loss: 2.9228458404541016
Validation loss: 2.71846467192455

Epoch: 6| Step: 13
Training loss: 3.090953826904297
Validation loss: 2.717229543193694

Epoch: 32| Step: 0
Training loss: 2.3559494018554688
Validation loss: 2.716309301314815

Epoch: 6| Step: 1
Training loss: 3.4574098587036133
Validation loss: 2.7183410044639342

Epoch: 6| Step: 2
Training loss: 2.4754977226257324
Validation loss: 2.71950868380967

Epoch: 6| Step: 3
Training loss: 2.770390033721924
Validation loss: 2.721862716059531

Epoch: 6| Step: 4
Training loss: 3.540492534637451
Validation loss: 2.7250584992029334

Epoch: 6| Step: 5
Training loss: 1.859480619430542
Validation loss: 2.726134859105592

Epoch: 6| Step: 6
Training loss: 1.9158124923706055
Validation loss: 2.726584367854621

Epoch: 6| Step: 7
Training loss: 2.7606160640716553
Validation loss: 2.7284922497246855

Epoch: 6| Step: 8
Training loss: 3.10359525680542
Validation loss: 2.728952423218758

Epoch: 6| Step: 9
Training loss: 3.455873727798462
Validation loss: 2.726459264755249

Epoch: 6| Step: 10
Training loss: 2.8984427452087402
Validation loss: 2.7236007029010403

Epoch: 6| Step: 11
Training loss: 3.5499184131622314
Validation loss: 2.720444561332785

Epoch: 6| Step: 12
Training loss: 2.721625328063965
Validation loss: 2.719362202511039

Epoch: 6| Step: 13
Training loss: 4.000004291534424
Validation loss: 2.7191643484177126

Epoch: 33| Step: 0
Training loss: 2.6434853076934814
Validation loss: 2.718286642464258

Epoch: 6| Step: 1
Training loss: 3.0965168476104736
Validation loss: 2.717471353469356

Epoch: 6| Step: 2
Training loss: 2.4373159408569336
Validation loss: 2.7167958649255897

Epoch: 6| Step: 3
Training loss: 2.8359718322753906
Validation loss: 2.7164777478864117

Epoch: 6| Step: 4
Training loss: 4.226190567016602
Validation loss: 2.7151235226661927

Epoch: 6| Step: 5
Training loss: 2.4712162017822266
Validation loss: 2.715802292669973

Epoch: 6| Step: 6
Training loss: 2.168938159942627
Validation loss: 2.7140223672313075

Epoch: 6| Step: 7
Training loss: 2.9231834411621094
Validation loss: 2.7141898934559157

Epoch: 6| Step: 8
Training loss: 3.437729835510254
Validation loss: 2.7124500889931955

Epoch: 6| Step: 9
Training loss: 3.406972646713257
Validation loss: 2.7108879960993284

Epoch: 6| Step: 10
Training loss: 2.385077953338623
Validation loss: 2.7767239514217583

Epoch: 6| Step: 11
Training loss: 2.5999536514282227
Validation loss: 2.7766005198160806

Epoch: 6| Step: 12
Training loss: 3.054431915283203
Validation loss: 2.776686345377276

Epoch: 6| Step: 13
Training loss: 2.7083194255828857
Validation loss: 2.7845241741467546

Epoch: 34| Step: 0
Training loss: 3.2984378337860107
Validation loss: 2.777926362970824

Epoch: 6| Step: 1
Training loss: 2.9023098945617676
Validation loss: 2.7847207618016068

Epoch: 6| Step: 2
Training loss: 2.745028495788574
Validation loss: 2.781870339506416

Epoch: 6| Step: 3
Training loss: 3.2289953231811523
Validation loss: 2.7906522161217144

Epoch: 6| Step: 4
Training loss: 2.0417168140411377
Validation loss: 2.782768170038859

Epoch: 6| Step: 5
Training loss: 3.197535514831543
Validation loss: 2.780278959581929

Epoch: 6| Step: 6
Training loss: 2.3393449783325195
Validation loss: 2.7818153289056595

Epoch: 6| Step: 7
Training loss: 2.0608811378479004
Validation loss: 2.7879779646473546

Epoch: 6| Step: 8
Training loss: 2.758159637451172
Validation loss: 2.7812043210511566

Epoch: 6| Step: 9
Training loss: 3.2907230854034424
Validation loss: 2.7809576783128964

Epoch: 6| Step: 10
Training loss: 3.793200969696045
Validation loss: 2.7826612662243586

Epoch: 6| Step: 11
Training loss: 3.088399887084961
Validation loss: 2.785194073953936

Epoch: 6| Step: 12
Training loss: 3.2256765365600586
Validation loss: 2.775133422625962

Epoch: 6| Step: 13
Training loss: 2.966397285461426
Validation loss: 2.778376840776013

Epoch: 35| Step: 0
Training loss: 3.0200538635253906
Validation loss: 2.776049485770605

Epoch: 6| Step: 1
Training loss: 2.272677183151245
Validation loss: 2.7709780021380355

Epoch: 6| Step: 2
Training loss: 2.62099289894104
Validation loss: 2.7722561385041926

Epoch: 6| Step: 3
Training loss: 3.8458995819091797
Validation loss: 2.767208053219703

Epoch: 6| Step: 4
Training loss: 2.8278183937072754
Validation loss: 2.769431698706842

Epoch: 6| Step: 5
Training loss: 2.960017442703247
Validation loss: 2.7691993431378434

Epoch: 6| Step: 6
Training loss: 2.3743577003479004
Validation loss: 2.7690989509705575

Epoch: 6| Step: 7
Training loss: 3.090961456298828
Validation loss: 2.77136428125443

Epoch: 6| Step: 8
Training loss: 3.4876022338867188
Validation loss: 2.768808208486085

Epoch: 6| Step: 9
Training loss: 3.164029836654663
Validation loss: 2.7661489158548336

Epoch: 6| Step: 10
Training loss: 2.9239394664764404
Validation loss: 2.7684649677686792

Epoch: 6| Step: 11
Training loss: 1.9144483804702759
Validation loss: 2.7676356607867825

Epoch: 6| Step: 12
Training loss: 3.6432902812957764
Validation loss: 2.788041355789349

Epoch: 6| Step: 13
Training loss: 2.4214351177215576
Validation loss: 2.805606529276858

Epoch: 36| Step: 0
Training loss: 2.8524680137634277
Validation loss: 2.8311956236439366

Epoch: 6| Step: 1
Training loss: 2.657382011413574
Validation loss: 2.8347486244734896

Epoch: 6| Step: 2
Training loss: 2.913578987121582
Validation loss: 2.832714219247141

Epoch: 6| Step: 3
Training loss: 3.0235118865966797
Validation loss: 2.8332809248278217

Epoch: 6| Step: 4
Training loss: 3.39811110496521
Validation loss: 2.8308919424651773

Epoch: 6| Step: 5
Training loss: 2.9587717056274414
Validation loss: 2.8292106992454937

Epoch: 6| Step: 6
Training loss: 2.9575352668762207
Validation loss: 2.8237313455150974

Epoch: 6| Step: 7
Training loss: 3.499699354171753
Validation loss: 2.827822423750354

Epoch: 6| Step: 8
Training loss: 2.4243903160095215
Validation loss: 2.8252026034939672

Epoch: 6| Step: 9
Training loss: 3.108689308166504
Validation loss: 2.828186624793596

Epoch: 6| Step: 10
Training loss: 2.019596815109253
Validation loss: 2.8262754922272055

Epoch: 6| Step: 11
Training loss: 3.8594868183135986
Validation loss: 2.8235664213857343

Epoch: 6| Step: 12
Training loss: 2.9158761501312256
Validation loss: 2.8215931871885895

Epoch: 6| Step: 13
Training loss: 2.5516035556793213
Validation loss: 2.820868415217246

Epoch: 37| Step: 0
Training loss: 3.041289806365967
Validation loss: 2.8245837637173232

Epoch: 6| Step: 1
Training loss: 3.41755747795105
Validation loss: 2.827624874730264

Epoch: 6| Step: 2
Training loss: 3.008429527282715
Validation loss: 2.8385058167160198

Epoch: 6| Step: 3
Training loss: 2.9914917945861816
Validation loss: 2.8297744976576937

Epoch: 6| Step: 4
Training loss: 2.6498496532440186
Validation loss: 2.8238560230501237

Epoch: 6| Step: 5
Training loss: 2.992915630340576
Validation loss: 2.819731463668167

Epoch: 6| Step: 6
Training loss: 2.630178451538086
Validation loss: 2.817029683820663

Epoch: 6| Step: 7
Training loss: 2.9738729000091553
Validation loss: 2.8173081490301315

Epoch: 6| Step: 8
Training loss: 2.6210861206054688
Validation loss: 2.807068658131425

Epoch: 6| Step: 9
Training loss: 1.996593713760376
Validation loss: 2.776707746649301

Epoch: 6| Step: 10
Training loss: 3.427471876144409
Validation loss: 2.7885599597807853

Epoch: 6| Step: 11
Training loss: 2.8663954734802246
Validation loss: 2.775444035889

Epoch: 6| Step: 12
Training loss: 3.431912660598755
Validation loss: 2.76246972750592

Epoch: 6| Step: 13
Training loss: 3.111523151397705
Validation loss: 2.7552863602997153

Epoch: 38| Step: 0
Training loss: 2.0973825454711914
Validation loss: 2.7542000483441096

Epoch: 6| Step: 1
Training loss: 3.3023900985717773
Validation loss: 2.753784615506408

Epoch: 6| Step: 2
Training loss: 2.6337528228759766
Validation loss: 2.7696373795950286

Epoch: 6| Step: 3
Training loss: 3.9218719005584717
Validation loss: 2.7928824552925686

Epoch: 6| Step: 4
Training loss: 3.19781231880188
Validation loss: 2.767753187046256

Epoch: 6| Step: 5
Training loss: 3.053083896636963
Validation loss: 2.765724415420204

Epoch: 6| Step: 6
Training loss: 2.6801252365112305
Validation loss: 2.7520183081267984

Epoch: 6| Step: 7
Training loss: 2.2263662815093994
Validation loss: 2.750294026508126

Epoch: 6| Step: 8
Training loss: 2.486873149871826
Validation loss: 2.7528141672893236

Epoch: 6| Step: 9
Training loss: 3.344492197036743
Validation loss: 2.7488459156405542

Epoch: 6| Step: 10
Training loss: 2.686631679534912
Validation loss: 2.7505926880785214

Epoch: 6| Step: 11
Training loss: 2.8621909618377686
Validation loss: 2.7521822632000013

Epoch: 6| Step: 12
Training loss: 2.981088638305664
Validation loss: 2.7518583266965804

Epoch: 6| Step: 13
Training loss: 3.494906187057495
Validation loss: 2.7503094262974237

Epoch: 39| Step: 0
Training loss: 2.1682374477386475
Validation loss: 2.751294707739225

Epoch: 6| Step: 1
Training loss: 3.3519389629364014
Validation loss: 2.756404217853341

Epoch: 6| Step: 2
Training loss: 2.34121036529541
Validation loss: 2.7551414376945904

Epoch: 6| Step: 3
Training loss: 2.762014389038086
Validation loss: 2.7558395093487156

Epoch: 6| Step: 4
Training loss: 3.551018238067627
Validation loss: 2.7565396703699583

Epoch: 6| Step: 5
Training loss: 2.643885612487793
Validation loss: 2.7533093498599146

Epoch: 6| Step: 6
Training loss: 2.9971120357513428
Validation loss: 2.7501264567016275

Epoch: 6| Step: 7
Training loss: 2.4862821102142334
Validation loss: 2.7472742911308043

Epoch: 6| Step: 8
Training loss: 2.792004108428955
Validation loss: 2.735836693035659

Epoch: 6| Step: 9
Training loss: 2.8857531547546387
Validation loss: 2.715050825508692

Epoch: 6| Step: 10
Training loss: 2.87353515625
Validation loss: 2.7018438808379637

Epoch: 6| Step: 11
Training loss: 3.683753728866577
Validation loss: 2.689314308986869

Epoch: 6| Step: 12
Training loss: 2.840651273727417
Validation loss: 2.6857227612567205

Epoch: 6| Step: 13
Training loss: 3.246152877807617
Validation loss: 2.6843864148662937

Epoch: 40| Step: 0
Training loss: 3.0389163494110107
Validation loss: 2.684059064875367

Epoch: 6| Step: 1
Training loss: 2.7229576110839844
Validation loss: 2.684910023084251

Epoch: 6| Step: 2
Training loss: 2.7751002311706543
Validation loss: 2.683998066891906

Epoch: 6| Step: 3
Training loss: 3.397091865539551
Validation loss: 2.683011534393475

Epoch: 6| Step: 4
Training loss: 3.3466432094573975
Validation loss: 2.6857374893721713

Epoch: 6| Step: 5
Training loss: 2.3142027854919434
Validation loss: 2.682662397302607

Epoch: 6| Step: 6
Training loss: 3.0688138008117676
Validation loss: 2.681175021715062

Epoch: 6| Step: 7
Training loss: 2.674593210220337
Validation loss: 2.6808400461750646

Epoch: 6| Step: 8
Training loss: 3.2151269912719727
Validation loss: 2.6807132074909825

Epoch: 6| Step: 9
Training loss: 3.1572465896606445
Validation loss: 2.680868879441292

Epoch: 6| Step: 10
Training loss: 2.641810894012451
Validation loss: 2.681890890162478

Epoch: 6| Step: 11
Training loss: 2.1817238330841064
Validation loss: 2.6808943287018807

Epoch: 6| Step: 12
Training loss: 2.5749292373657227
Validation loss: 2.679412154741185

Epoch: 6| Step: 13
Training loss: 2.9725821018218994
Validation loss: 2.679898162041941

Epoch: 41| Step: 0
Training loss: 3.217111587524414
Validation loss: 2.677131147794826

Epoch: 6| Step: 1
Training loss: 2.9486539363861084
Validation loss: 2.67710865441189

Epoch: 6| Step: 2
Training loss: 2.2829039096832275
Validation loss: 2.6793744948602494

Epoch: 6| Step: 3
Training loss: 2.526508331298828
Validation loss: 2.6781926257635957

Epoch: 6| Step: 4
Training loss: 2.2801246643066406
Validation loss: 2.6780798563393216

Epoch: 6| Step: 5
Training loss: 3.1712260246276855
Validation loss: 2.680190678565733

Epoch: 6| Step: 6
Training loss: 2.6697473526000977
Validation loss: 2.676875073422668

Epoch: 6| Step: 7
Training loss: 3.014866590499878
Validation loss: 2.676833991081484

Epoch: 6| Step: 8
Training loss: 3.3606743812561035
Validation loss: 2.6770881581050094

Epoch: 6| Step: 9
Training loss: 3.0037193298339844
Validation loss: 2.6774822665799047

Epoch: 6| Step: 10
Training loss: 2.7689549922943115
Validation loss: 2.6745669790493545

Epoch: 6| Step: 11
Training loss: 3.3518314361572266
Validation loss: 2.6757394318939536

Epoch: 6| Step: 12
Training loss: 2.851337194442749
Validation loss: 2.675657803012479

Epoch: 6| Step: 13
Training loss: 2.3073675632476807
Validation loss: 2.6761220680770053

Epoch: 42| Step: 0
Training loss: 2.879392623901367
Validation loss: 2.674330593437277

Epoch: 6| Step: 1
Training loss: 2.7449090480804443
Validation loss: 2.6752537835028862

Epoch: 6| Step: 2
Training loss: 2.586315393447876
Validation loss: 2.67423915350309

Epoch: 6| Step: 3
Training loss: 2.9037675857543945
Validation loss: 2.672757617888912

Epoch: 6| Step: 4
Training loss: 3.1680893898010254
Validation loss: 2.670243799045522

Epoch: 6| Step: 5
Training loss: 2.8940176963806152
Validation loss: 2.6722254727476384

Epoch: 6| Step: 6
Training loss: 2.7785491943359375
Validation loss: 2.6711504895200013

Epoch: 6| Step: 7
Training loss: 3.5401415824890137
Validation loss: 2.670211956065188

Epoch: 6| Step: 8
Training loss: 3.0513010025024414
Validation loss: 2.6755809527571484

Epoch: 6| Step: 9
Training loss: 2.019969940185547
Validation loss: 2.6683074915280907

Epoch: 6| Step: 10
Training loss: 3.668853282928467
Validation loss: 2.67236320177714

Epoch: 6| Step: 11
Training loss: 2.581631660461426
Validation loss: 2.66970464234711

Epoch: 6| Step: 12
Training loss: 2.2992517948150635
Validation loss: 2.6698445479075112

Epoch: 6| Step: 13
Training loss: 2.7589056491851807
Validation loss: 2.669721882830384

Epoch: 43| Step: 0
Training loss: 3.131284475326538
Validation loss: 2.6704792361105643

Epoch: 6| Step: 1
Training loss: 3.313998222351074
Validation loss: 2.6652749558930755

Epoch: 6| Step: 2
Training loss: 3.0677742958068848
Validation loss: 2.6643811041308987

Epoch: 6| Step: 3
Training loss: 3.3419265747070312
Validation loss: 2.6660384234561714

Epoch: 6| Step: 4
Training loss: 2.1219265460968018
Validation loss: 2.6683334586440877

Epoch: 6| Step: 5
Training loss: 2.8712801933288574
Validation loss: 2.6656804136050645

Epoch: 6| Step: 6
Training loss: 2.403968095779419
Validation loss: 2.66601308443213

Epoch: 6| Step: 7
Training loss: 3.2520570755004883
Validation loss: 2.664715546433644

Epoch: 6| Step: 8
Training loss: 3.177957057952881
Validation loss: 2.664820750554403

Epoch: 6| Step: 9
Training loss: 2.765947103500366
Validation loss: 2.6711376943895893

Epoch: 6| Step: 10
Training loss: 2.9607534408569336
Validation loss: 2.6733711458021596

Epoch: 6| Step: 11
Training loss: 2.45719051361084
Validation loss: 2.6677942045273317

Epoch: 6| Step: 12
Training loss: 2.5710608959198
Validation loss: 2.659154120311942

Epoch: 6| Step: 13
Training loss: 2.070260763168335
Validation loss: 2.6652900890637468

Epoch: 44| Step: 0
Training loss: 2.586604356765747
Validation loss: 2.6677515993836107

Epoch: 6| Step: 1
Training loss: 2.550990581512451
Validation loss: 2.6639556038764214

Epoch: 6| Step: 2
Training loss: 3.205838203430176
Validation loss: 2.6636156753827165

Epoch: 6| Step: 3
Training loss: 3.223911762237549
Validation loss: 2.6650701389517835

Epoch: 6| Step: 4
Training loss: 3.619173526763916
Validation loss: 2.6659328886257705

Epoch: 6| Step: 5
Training loss: 1.9938558340072632
Validation loss: 2.662036044623262

Epoch: 6| Step: 6
Training loss: 3.0762171745300293
Validation loss: 2.665074145922097

Epoch: 6| Step: 7
Training loss: 2.3205811977386475
Validation loss: 2.6644252602772047

Epoch: 6| Step: 8
Training loss: 3.2340030670166016
Validation loss: 2.6618042453642814

Epoch: 6| Step: 9
Training loss: 3.300402879714966
Validation loss: 2.666352379706598

Epoch: 6| Step: 10
Training loss: 3.25936222076416
Validation loss: 2.6701646261317755

Epoch: 6| Step: 11
Training loss: 2.3940510749816895
Validation loss: 2.6693097263254146

Epoch: 6| Step: 12
Training loss: 2.1725809574127197
Validation loss: 2.66656058578081

Epoch: 6| Step: 13
Training loss: 2.952029228210449
Validation loss: 2.6610817165784937

Epoch: 45| Step: 0
Training loss: 3.0049970149993896
Validation loss: 2.6602024903861423

Epoch: 6| Step: 1
Training loss: 3.0301055908203125
Validation loss: 2.6575502246938725

Epoch: 6| Step: 2
Training loss: 2.196721315383911
Validation loss: 2.6631735806824057

Epoch: 6| Step: 3
Training loss: 3.7190470695495605
Validation loss: 2.657979867791617

Epoch: 6| Step: 4
Training loss: 2.7826731204986572
Validation loss: 2.6569929276743243

Epoch: 6| Step: 5
Training loss: 2.7913174629211426
Validation loss: 2.659396089533324

Epoch: 6| Step: 6
Training loss: 3.019615650177002
Validation loss: 2.664260769403109

Epoch: 6| Step: 7
Training loss: 2.8478684425354004
Validation loss: 2.661393619352771

Epoch: 6| Step: 8
Training loss: 2.2885384559631348
Validation loss: 2.663917943995486

Epoch: 6| Step: 9
Training loss: 2.424004316329956
Validation loss: 2.661355264725224

Epoch: 6| Step: 10
Training loss: 3.1891963481903076
Validation loss: 2.662116337847966

Epoch: 6| Step: 11
Training loss: 3.2935829162597656
Validation loss: 2.6619354935102564

Epoch: 6| Step: 12
Training loss: 2.6476211547851562
Validation loss: 2.660756749491538

Epoch: 6| Step: 13
Training loss: 2.2605433464050293
Validation loss: 2.6572584208621772

Epoch: 46| Step: 0
Training loss: 2.226454257965088
Validation loss: 2.657409075767763

Epoch: 6| Step: 1
Training loss: 2.344325542449951
Validation loss: 2.651610123213901

Epoch: 6| Step: 2
Training loss: 3.040590286254883
Validation loss: 2.6569776022306053

Epoch: 6| Step: 3
Training loss: 2.2875704765319824
Validation loss: 2.6532315413157144

Epoch: 6| Step: 4
Training loss: 2.954242467880249
Validation loss: 2.6491896542169715

Epoch: 6| Step: 5
Training loss: 3.606339454650879
Validation loss: 2.649537073668613

Epoch: 6| Step: 6
Training loss: 2.812589645385742
Validation loss: 2.648578861708282

Epoch: 6| Step: 7
Training loss: 3.537339687347412
Validation loss: 2.6487603956653225

Epoch: 6| Step: 8
Training loss: 2.7552831172943115
Validation loss: 2.6453265656707106

Epoch: 6| Step: 9
Training loss: 2.618116617202759
Validation loss: 2.6471023046842186

Epoch: 6| Step: 10
Training loss: 2.89028000831604
Validation loss: 2.6442147480544222

Epoch: 6| Step: 11
Training loss: 2.730905055999756
Validation loss: 2.6468686390948553

Epoch: 6| Step: 12
Training loss: 3.1910481452941895
Validation loss: 2.654717381282519

Epoch: 6| Step: 13
Training loss: 2.345240592956543
Validation loss: 2.6609841572341097

Epoch: 47| Step: 0
Training loss: 2.34505295753479
Validation loss: 2.6697047166926886

Epoch: 6| Step: 1
Training loss: 2.543832302093506
Validation loss: 2.6530763205661567

Epoch: 6| Step: 2
Training loss: 2.48858642578125
Validation loss: 2.6413396635363178

Epoch: 6| Step: 3
Training loss: 3.4941930770874023
Validation loss: 2.6452858858211066

Epoch: 6| Step: 4
Training loss: 2.344466209411621
Validation loss: 2.654714343368366

Epoch: 6| Step: 5
Training loss: 2.584805488586426
Validation loss: 2.6560078743965394

Epoch: 6| Step: 6
Training loss: 3.1404976844787598
Validation loss: 2.654886717437416

Epoch: 6| Step: 7
Training loss: 3.785799503326416
Validation loss: 2.6726123415013796

Epoch: 6| Step: 8
Training loss: 2.5367560386657715
Validation loss: 2.6532985318091606

Epoch: 6| Step: 9
Training loss: 2.4192566871643066
Validation loss: 2.648429573223155

Epoch: 6| Step: 10
Training loss: 3.1593079566955566
Validation loss: 2.6448241023607153

Epoch: 6| Step: 11
Training loss: 3.115858554840088
Validation loss: 2.643334286187285

Epoch: 6| Step: 12
Training loss: 2.7222795486450195
Validation loss: 2.655678710629863

Epoch: 6| Step: 13
Training loss: 2.864842653274536
Validation loss: 2.6729297073938514

Epoch: 48| Step: 0
Training loss: 3.097933292388916
Validation loss: 2.786045923027941

Epoch: 6| Step: 1
Training loss: 2.156221389770508
Validation loss: 2.9145281545577513

Epoch: 6| Step: 2
Training loss: 2.359886646270752
Validation loss: 2.884090256947343

Epoch: 6| Step: 3
Training loss: 3.055121421813965
Validation loss: 2.746587320040631

Epoch: 6| Step: 4
Training loss: 3.1348717212677
Validation loss: 2.697608914426578

Epoch: 6| Step: 5
Training loss: 3.5326690673828125
Validation loss: 2.705614930839949

Epoch: 6| Step: 6
Training loss: 2.178098440170288
Validation loss: 2.6827208688182216

Epoch: 6| Step: 7
Training loss: 2.7050607204437256
Validation loss: 2.6703077388066117

Epoch: 6| Step: 8
Training loss: 3.444896697998047
Validation loss: 2.673763736601799

Epoch: 6| Step: 9
Training loss: 2.4618899822235107
Validation loss: 2.680379142043411

Epoch: 6| Step: 10
Training loss: 3.01613187789917
Validation loss: 2.697388715641473

Epoch: 6| Step: 11
Training loss: 2.739835023880005
Validation loss: 2.716646748204385

Epoch: 6| Step: 12
Training loss: 3.6354806423187256
Validation loss: 2.7387203016588764

Epoch: 6| Step: 13
Training loss: 2.709182024002075
Validation loss: 2.7391098289079565

Epoch: 49| Step: 0
Training loss: 2.910895347595215
Validation loss: 2.7251815334443124

Epoch: 6| Step: 1
Training loss: 2.6016616821289062
Validation loss: 2.6782864370653705

Epoch: 6| Step: 2
Training loss: 2.691349506378174
Validation loss: 2.662826135594358

Epoch: 6| Step: 3
Training loss: 1.962134838104248
Validation loss: 2.653283590911537

Epoch: 6| Step: 4
Training loss: 3.5671238899230957
Validation loss: 2.662445537505611

Epoch: 6| Step: 5
Training loss: 3.062894821166992
Validation loss: 2.6682186741982736

Epoch: 6| Step: 6
Training loss: 2.3227896690368652
Validation loss: 2.729221236321234

Epoch: 6| Step: 7
Training loss: 3.6074228286743164
Validation loss: 2.7599209226587766

Epoch: 6| Step: 8
Training loss: 2.74544620513916
Validation loss: 2.7789727539144535

Epoch: 6| Step: 9
Training loss: 3.5953662395477295
Validation loss: 2.792288875067106

Epoch: 6| Step: 10
Training loss: 3.048771858215332
Validation loss: 2.7738494847410466

Epoch: 6| Step: 11
Training loss: 1.8966493606567383
Validation loss: 2.741879806723646

Epoch: 6| Step: 12
Training loss: 3.193364143371582
Validation loss: 2.733383294074766

Epoch: 6| Step: 13
Training loss: 3.0395097732543945
Validation loss: 2.7235354121013353

Epoch: 50| Step: 0
Training loss: 2.880157232284546
Validation loss: 2.730709973201957

Epoch: 6| Step: 1
Training loss: 3.0231215953826904
Validation loss: 2.7434910522994174

Epoch: 6| Step: 2
Training loss: 2.041557550430298
Validation loss: 2.746818955226611

Epoch: 6| Step: 3
Training loss: 3.1823792457580566
Validation loss: 2.7421128160210064

Epoch: 6| Step: 4
Training loss: 3.483755350112915
Validation loss: 2.733680678952125

Epoch: 6| Step: 5
Training loss: 3.7765626907348633
Validation loss: 2.735414530641289

Epoch: 6| Step: 6
Training loss: 2.638967514038086
Validation loss: 2.7247425048582015

Epoch: 6| Step: 7
Training loss: 2.5154244899749756
Validation loss: 2.7218280505108576

Epoch: 6| Step: 8
Training loss: 1.8453997373580933
Validation loss: 2.7216160963940363

Epoch: 6| Step: 9
Training loss: 2.435643196105957
Validation loss: 2.712552068054035

Epoch: 6| Step: 10
Training loss: 3.2366933822631836
Validation loss: 2.708591473999844

Epoch: 6| Step: 11
Training loss: 3.460862398147583
Validation loss: 2.707221459316951

Epoch: 6| Step: 12
Training loss: 2.7252936363220215
Validation loss: 2.7032830920270694

Epoch: 6| Step: 13
Training loss: 3.030165910720825
Validation loss: 2.7029120896452214

Epoch: 51| Step: 0
Training loss: 2.5938241481781006
Validation loss: 2.7003802791718514

Epoch: 6| Step: 1
Training loss: 2.8370437622070312
Validation loss: 2.6986038582299345

Epoch: 6| Step: 2
Training loss: 2.9184699058532715
Validation loss: 2.6989685361103346

Epoch: 6| Step: 3
Training loss: 2.437685012817383
Validation loss: 2.699268510264735

Epoch: 6| Step: 4
Training loss: 2.917027473449707
Validation loss: 2.705457951432915

Epoch: 6| Step: 5
Training loss: 3.4648871421813965
Validation loss: 2.7242482118709113

Epoch: 6| Step: 6
Training loss: 2.4986398220062256
Validation loss: 2.7293509591010308

Epoch: 6| Step: 7
Training loss: 2.801999092102051
Validation loss: 2.7336935997009277

Epoch: 6| Step: 8
Training loss: 2.797299385070801
Validation loss: 2.719289815554055

Epoch: 6| Step: 9
Training loss: 3.094331979751587
Validation loss: 2.7145365950881795

Epoch: 6| Step: 10
Training loss: 2.5286221504211426
Validation loss: 2.6849347468345397

Epoch: 6| Step: 11
Training loss: 3.6739795207977295
Validation loss: 2.684516345300982

Epoch: 6| Step: 12
Training loss: 2.696226119995117
Validation loss: 2.6788867955566733

Epoch: 6| Step: 13
Training loss: 2.4937169551849365
Validation loss: 2.6762849464211413

Epoch: 52| Step: 0
Training loss: 2.520270586013794
Validation loss: 2.6675185849589687

Epoch: 6| Step: 1
Training loss: 3.4212379455566406
Validation loss: 2.6613176304806947

Epoch: 6| Step: 2
Training loss: 2.7535698413848877
Validation loss: 2.610479308712867

Epoch: 6| Step: 3
Training loss: 3.388155460357666
Validation loss: 2.603130678976736

Epoch: 6| Step: 4
Training loss: 2.950577735900879
Validation loss: 2.6101111673539683

Epoch: 6| Step: 5
Training loss: 2.312572956085205
Validation loss: 2.619085406744352

Epoch: 6| Step: 6
Training loss: 2.668619394302368
Validation loss: 2.6269428960738646

Epoch: 6| Step: 7
Training loss: 2.864603042602539
Validation loss: 2.6218258591108423

Epoch: 6| Step: 8
Training loss: 2.833028793334961
Validation loss: 2.616613121442897

Epoch: 6| Step: 9
Training loss: 2.5768375396728516
Validation loss: 2.630937689094133

Epoch: 6| Step: 10
Training loss: 2.8712713718414307
Validation loss: 2.65302643211939

Epoch: 6| Step: 11
Training loss: 2.9167869091033936
Validation loss: 2.683525175176641

Epoch: 6| Step: 12
Training loss: 2.4756827354431152
Validation loss: 2.754973501287481

Epoch: 6| Step: 13
Training loss: 2.8550829887390137
Validation loss: 2.7348501156735163

Epoch: 53| Step: 0
Training loss: 2.1942930221557617
Validation loss: 2.70808825185222

Epoch: 6| Step: 1
Training loss: 3.1346006393432617
Validation loss: 2.6909891354140414

Epoch: 6| Step: 2
Training loss: 2.388103485107422
Validation loss: 2.68016315531987

Epoch: 6| Step: 3
Training loss: 2.782966136932373
Validation loss: 2.657838075391708

Epoch: 6| Step: 4
Training loss: 3.2422866821289062
Validation loss: 2.614579698090912

Epoch: 6| Step: 5
Training loss: 2.7569756507873535
Validation loss: 2.6141685926786034

Epoch: 6| Step: 6
Training loss: 2.65974760055542
Validation loss: 2.612383137467087

Epoch: 6| Step: 7
Training loss: 3.4587690830230713
Validation loss: 2.6030034557465584

Epoch: 6| Step: 8
Training loss: 3.028402090072632
Validation loss: 2.6013767232177076

Epoch: 6| Step: 9
Training loss: 2.9182064533233643
Validation loss: 2.597227883595292

Epoch: 6| Step: 10
Training loss: 2.693387746810913
Validation loss: 2.591913643703666

Epoch: 6| Step: 11
Training loss: 2.1496644020080566
Validation loss: 2.5907626075129353

Epoch: 6| Step: 12
Training loss: 3.022441864013672
Validation loss: 2.5902414552627073

Epoch: 6| Step: 13
Training loss: 2.900280237197876
Validation loss: 2.6024074041715233

Epoch: 54| Step: 0
Training loss: 3.0987329483032227
Validation loss: 2.6090098042641916

Epoch: 6| Step: 1
Training loss: 2.7460591793060303
Validation loss: 2.621965398070633

Epoch: 6| Step: 2
Training loss: 2.618928909301758
Validation loss: 2.629308844125399

Epoch: 6| Step: 3
Training loss: 3.1011524200439453
Validation loss: 2.627929590081656

Epoch: 6| Step: 4
Training loss: 3.283233642578125
Validation loss: 2.6304456982561337

Epoch: 6| Step: 5
Training loss: 2.6790249347686768
Validation loss: 2.6309891618708128

Epoch: 6| Step: 6
Training loss: 2.1094441413879395
Validation loss: 2.6173783604816725

Epoch: 6| Step: 7
Training loss: 3.106196165084839
Validation loss: 2.606730807212091

Epoch: 6| Step: 8
Training loss: 2.375016212463379
Validation loss: 2.5909392577345653

Epoch: 6| Step: 9
Training loss: 2.327101707458496
Validation loss: 2.58193778222607

Epoch: 6| Step: 10
Training loss: 2.6497912406921387
Validation loss: 2.583315432712596

Epoch: 6| Step: 11
Training loss: 3.109001874923706
Validation loss: 2.586284598996562

Epoch: 6| Step: 12
Training loss: 2.9490065574645996
Validation loss: 2.588004458335138

Epoch: 6| Step: 13
Training loss: 2.8301568031311035
Validation loss: 2.589862694022476

Epoch: 55| Step: 0
Training loss: 2.8854727745056152
Validation loss: 2.591253719022197

Epoch: 6| Step: 1
Training loss: 4.187593460083008
Validation loss: 2.6027605764327513

Epoch: 6| Step: 2
Training loss: 2.55057954788208
Validation loss: 2.6007870166532454

Epoch: 6| Step: 3
Training loss: 3.464273691177368
Validation loss: 2.597490031232116

Epoch: 6| Step: 4
Training loss: 2.2829809188842773
Validation loss: 2.5956768861380954

Epoch: 6| Step: 5
Training loss: 2.369621992111206
Validation loss: 2.586926867884974

Epoch: 6| Step: 6
Training loss: 2.12027645111084
Validation loss: 2.5856624239234516

Epoch: 6| Step: 7
Training loss: 2.8595147132873535
Validation loss: 2.587074431039954

Epoch: 6| Step: 8
Training loss: 2.3920884132385254
Validation loss: 2.5803225501891105

Epoch: 6| Step: 9
Training loss: 2.101304531097412
Validation loss: 2.5805703183656097

Epoch: 6| Step: 10
Training loss: 3.784614086151123
Validation loss: 2.5850433200918217

Epoch: 6| Step: 11
Training loss: 2.626065731048584
Validation loss: 2.58341149873631

Epoch: 6| Step: 12
Training loss: 2.931558609008789
Validation loss: 2.5890853071725495

Epoch: 6| Step: 13
Training loss: 1.9461849927902222
Validation loss: 2.5893252946997203

Epoch: 56| Step: 0
Training loss: 2.9720191955566406
Validation loss: 2.5927958001372633

Epoch: 6| Step: 1
Training loss: 2.3933029174804688
Validation loss: 2.595587932935325

Epoch: 6| Step: 2
Training loss: 2.7180235385894775
Validation loss: 2.621734321758311

Epoch: 6| Step: 3
Training loss: 3.554264783859253
Validation loss: 2.6161608439619823

Epoch: 6| Step: 4
Training loss: 2.5642356872558594
Validation loss: 2.6272728340600127

Epoch: 6| Step: 5
Training loss: 3.234679937362671
Validation loss: 2.615950128083588

Epoch: 6| Step: 6
Training loss: 2.2180299758911133
Validation loss: 2.591137232319001

Epoch: 6| Step: 7
Training loss: 3.0684690475463867
Validation loss: 2.584800922742454

Epoch: 6| Step: 8
Training loss: 2.2360520362854004
Validation loss: 2.5658489042712795

Epoch: 6| Step: 9
Training loss: 2.7527389526367188
Validation loss: 2.5677938512576524

Epoch: 6| Step: 10
Training loss: 2.2904772758483887
Validation loss: 2.573360473878922

Epoch: 6| Step: 11
Training loss: 3.2168073654174805
Validation loss: 2.580770261826054

Epoch: 6| Step: 12
Training loss: 3.1409614086151123
Validation loss: 2.590173649531539

Epoch: 6| Step: 13
Training loss: 2.1811437606811523
Validation loss: 2.599319019625264

Epoch: 57| Step: 0
Training loss: 2.9195168018341064
Validation loss: 2.6141318480173745

Epoch: 6| Step: 1
Training loss: 2.964994192123413
Validation loss: 2.6148470806819137

Epoch: 6| Step: 2
Training loss: 2.837634563446045
Validation loss: 2.6227921183391283

Epoch: 6| Step: 3
Training loss: 2.0334272384643555
Validation loss: 2.62871991690769

Epoch: 6| Step: 4
Training loss: 2.866356134414673
Validation loss: 2.62956589780828

Epoch: 6| Step: 5
Training loss: 2.7089807987213135
Validation loss: 2.61813045829855

Epoch: 6| Step: 6
Training loss: 2.4699454307556152
Validation loss: 2.5883905400512037

Epoch: 6| Step: 7
Training loss: 2.68811297416687
Validation loss: 2.5777371724446616

Epoch: 6| Step: 8
Training loss: 3.328806161880493
Validation loss: 2.56917401026654

Epoch: 6| Step: 9
Training loss: 3.0892767906188965
Validation loss: 2.5681049593033327

Epoch: 6| Step: 10
Training loss: 2.4434971809387207
Validation loss: 2.568805261324811

Epoch: 6| Step: 11
Training loss: 2.777744770050049
Validation loss: 2.5718917949225313

Epoch: 6| Step: 12
Training loss: 2.8381123542785645
Validation loss: 2.573937844204646

Epoch: 6| Step: 13
Training loss: 2.9009475708007812
Validation loss: 2.5727460307459675

Epoch: 58| Step: 0
Training loss: 2.941312551498413
Validation loss: 2.5797034925030125

Epoch: 6| Step: 1
Training loss: 2.7042136192321777
Validation loss: 2.5694731691832184

Epoch: 6| Step: 2
Training loss: 2.428530693054199
Validation loss: 2.5706182628549556

Epoch: 6| Step: 3
Training loss: 2.6805686950683594
Validation loss: 2.567697699351977

Epoch: 6| Step: 4
Training loss: 2.5568995475769043
Validation loss: 2.571599475799068

Epoch: 6| Step: 5
Training loss: 3.5092148780822754
Validation loss: 2.5721440802338305

Epoch: 6| Step: 6
Training loss: 2.5982871055603027
Validation loss: 2.5694364655402397

Epoch: 6| Step: 7
Training loss: 2.857605457305908
Validation loss: 2.5712137581199728

Epoch: 6| Step: 8
Training loss: 2.829415798187256
Validation loss: 2.5684575367999334

Epoch: 6| Step: 9
Training loss: 3.235257625579834
Validation loss: 2.569035332690003

Epoch: 6| Step: 10
Training loss: 2.214447021484375
Validation loss: 2.5678964981468777

Epoch: 6| Step: 11
Training loss: 2.690605401992798
Validation loss: 2.5643374842982136

Epoch: 6| Step: 12
Training loss: 2.7865970134735107
Validation loss: 2.5674302424153974

Epoch: 6| Step: 13
Training loss: 2.413947820663452
Validation loss: 2.5671006966662664

Epoch: 59| Step: 0
Training loss: 2.506883382797241
Validation loss: 2.569197077904978

Epoch: 6| Step: 1
Training loss: 2.1684048175811768
Validation loss: 2.5718356665744575

Epoch: 6| Step: 2
Training loss: 2.4071261882781982
Validation loss: 2.571528245044011

Epoch: 6| Step: 3
Training loss: 1.7504794597625732
Validation loss: 2.576436335040677

Epoch: 6| Step: 4
Training loss: 2.766843795776367
Validation loss: 2.576722788554366

Epoch: 6| Step: 5
Training loss: 2.495662212371826
Validation loss: 2.570306718990367

Epoch: 6| Step: 6
Training loss: 2.4506211280822754
Validation loss: 2.5717453777149157

Epoch: 6| Step: 7
Training loss: 2.454883098602295
Validation loss: 2.5756009189031457

Epoch: 6| Step: 8
Training loss: 3.539421796798706
Validation loss: 2.591368306067682

Epoch: 6| Step: 9
Training loss: 3.1823246479034424
Validation loss: 2.5822042393428024

Epoch: 6| Step: 10
Training loss: 3.307039737701416
Validation loss: 2.5844179891770884

Epoch: 6| Step: 11
Training loss: 3.4756360054016113
Validation loss: 2.59030238018241

Epoch: 6| Step: 12
Training loss: 2.9090609550476074
Validation loss: 2.58260428777305

Epoch: 6| Step: 13
Training loss: 3.368438720703125
Validation loss: 2.5632583069544967

Epoch: 60| Step: 0
Training loss: 3.1718382835388184
Validation loss: 2.556217029530515

Epoch: 6| Step: 1
Training loss: 2.1265416145324707
Validation loss: 2.559893674747918

Epoch: 6| Step: 2
Training loss: 2.608489990234375
Validation loss: 2.558134207161524

Epoch: 6| Step: 3
Training loss: 2.5134294033050537
Validation loss: 2.5576305543222735

Epoch: 6| Step: 4
Training loss: 2.6583001613616943
Validation loss: 2.554795424143473

Epoch: 6| Step: 5
Training loss: 2.511812210083008
Validation loss: 2.555650157313193

Epoch: 6| Step: 6
Training loss: 3.5072035789489746
Validation loss: 2.5529146553367696

Epoch: 6| Step: 7
Training loss: 2.6954894065856934
Validation loss: 2.5567568271390853

Epoch: 6| Step: 8
Training loss: 2.9251770973205566
Validation loss: 2.5575920176762406

Epoch: 6| Step: 9
Training loss: 2.411414861679077
Validation loss: 2.556698501750987

Epoch: 6| Step: 10
Training loss: 3.2406578063964844
Validation loss: 2.5549966648060787

Epoch: 6| Step: 11
Training loss: 2.4461441040039062
Validation loss: 2.555914637862995

Epoch: 6| Step: 12
Training loss: 3.354015827178955
Validation loss: 2.5576749899054088

Epoch: 6| Step: 13
Training loss: 1.953930139541626
Validation loss: 2.5593505879884124

Epoch: 61| Step: 0
Training loss: 2.0138044357299805
Validation loss: 2.5630509032998035

Epoch: 6| Step: 1
Training loss: 2.9082283973693848
Validation loss: 2.5654419929750505

Epoch: 6| Step: 2
Training loss: 3.293227195739746
Validation loss: 2.5728985648001395

Epoch: 6| Step: 3
Training loss: 3.3886406421661377
Validation loss: 2.561700513285975

Epoch: 6| Step: 4
Training loss: 2.2290215492248535
Validation loss: 2.557672349355554

Epoch: 6| Step: 5
Training loss: 2.6358039379119873
Validation loss: 2.5568734445879535

Epoch: 6| Step: 6
Training loss: 2.7074618339538574
Validation loss: 2.556829032077584

Epoch: 6| Step: 7
Training loss: 2.994492292404175
Validation loss: 2.551990524415047

Epoch: 6| Step: 8
Training loss: 2.846508026123047
Validation loss: 2.553755549974339

Epoch: 6| Step: 9
Training loss: 2.4208178520202637
Validation loss: 2.5524403074736237

Epoch: 6| Step: 10
Training loss: 2.3860394954681396
Validation loss: 2.5511376165574595

Epoch: 6| Step: 11
Training loss: 3.186685562133789
Validation loss: 2.5509704543698217

Epoch: 6| Step: 12
Training loss: 2.427776336669922
Validation loss: 2.5504131009501796

Epoch: 6| Step: 13
Training loss: 3.065403461456299
Validation loss: 2.5543755049346597

Epoch: 62| Step: 0
Training loss: 2.484117269515991
Validation loss: 2.5525430607539352

Epoch: 6| Step: 1
Training loss: 2.1985745429992676
Validation loss: 2.5518497677259546

Epoch: 6| Step: 2
Training loss: 2.6800739765167236
Validation loss: 2.5509859208137757

Epoch: 6| Step: 3
Training loss: 2.930872917175293
Validation loss: 2.549918105525355

Epoch: 6| Step: 4
Training loss: 2.4364733695983887
Validation loss: 2.543994959964547

Epoch: 6| Step: 5
Training loss: 2.658722162246704
Validation loss: 2.549793097280687

Epoch: 6| Step: 6
Training loss: 2.834549903869629
Validation loss: 2.5484083749914683

Epoch: 6| Step: 7
Training loss: 2.8271374702453613
Validation loss: 2.5571533787635063

Epoch: 6| Step: 8
Training loss: 3.298002243041992
Validation loss: 2.554171562194824

Epoch: 6| Step: 9
Training loss: 2.8398942947387695
Validation loss: 2.557901151718632

Epoch: 6| Step: 10
Training loss: 2.6808128356933594
Validation loss: 2.566377834607196

Epoch: 6| Step: 11
Training loss: 3.337906837463379
Validation loss: 2.555772873663133

Epoch: 6| Step: 12
Training loss: 2.6979002952575684
Validation loss: 2.550006156326622

Epoch: 6| Step: 13
Training loss: 2.2384376525878906
Validation loss: 2.547940905376147

Epoch: 63| Step: 0
Training loss: 3.4200334548950195
Validation loss: 2.5446532746796966

Epoch: 6| Step: 1
Training loss: 2.6561617851257324
Validation loss: 2.540924572175549

Epoch: 6| Step: 2
Training loss: 2.480766773223877
Validation loss: 2.5440693260521017

Epoch: 6| Step: 3
Training loss: 2.7731542587280273
Validation loss: 2.5385527123687086

Epoch: 6| Step: 4
Training loss: 2.687997341156006
Validation loss: 2.5393156338763494

Epoch: 6| Step: 5
Training loss: 2.68574595451355
Validation loss: 2.535252764660825

Epoch: 6| Step: 6
Training loss: 2.8878228664398193
Validation loss: 2.540968648848995

Epoch: 6| Step: 7
Training loss: 3.1708083152770996
Validation loss: 2.539036020155876

Epoch: 6| Step: 8
Training loss: 2.6771373748779297
Validation loss: 2.5366710206513763

Epoch: 6| Step: 9
Training loss: 3.0454349517822266
Validation loss: 2.5368447431954007

Epoch: 6| Step: 10
Training loss: 2.672441005706787
Validation loss: 2.538249302935857

Epoch: 6| Step: 11
Training loss: 2.2918407917022705
Validation loss: 2.5385857730783443

Epoch: 6| Step: 12
Training loss: 2.5534214973449707
Validation loss: 2.535303300426852

Epoch: 6| Step: 13
Training loss: 1.8126397132873535
Validation loss: 2.5350103070659022

Epoch: 64| Step: 0
Training loss: 2.570197582244873
Validation loss: 2.5406997998555503

Epoch: 6| Step: 1
Training loss: 2.6340694427490234
Validation loss: 2.5422947945133334

Epoch: 6| Step: 2
Training loss: 2.892627477645874
Validation loss: 2.5428513608953005

Epoch: 6| Step: 3
Training loss: 2.8707451820373535
Validation loss: 2.5385585126056465

Epoch: 6| Step: 4
Training loss: 2.970200300216675
Validation loss: 2.5389566600963636

Epoch: 6| Step: 5
Training loss: 2.358201742172241
Validation loss: 2.5440232728117254

Epoch: 6| Step: 6
Training loss: 2.6346230506896973
Validation loss: 2.5404405004234722

Epoch: 6| Step: 7
Training loss: 2.7365565299987793
Validation loss: 2.5439836825093916

Epoch: 6| Step: 8
Training loss: 2.627976894378662
Validation loss: 2.543154575491464

Epoch: 6| Step: 9
Training loss: 3.6652445793151855
Validation loss: 2.541948433845274

Epoch: 6| Step: 10
Training loss: 1.9732242822647095
Validation loss: 2.539293719876197

Epoch: 6| Step: 11
Training loss: 2.163524627685547
Validation loss: 2.5382237101113923

Epoch: 6| Step: 12
Training loss: 3.3931522369384766
Validation loss: 2.5358313950159217

Epoch: 6| Step: 13
Training loss: 2.796229362487793
Validation loss: 2.5367546235361407

Epoch: 65| Step: 0
Training loss: 2.38809871673584
Validation loss: 2.5335546411493772

Epoch: 6| Step: 1
Training loss: 2.176938533782959
Validation loss: 2.532558712908017

Epoch: 6| Step: 2
Training loss: 3.634915351867676
Validation loss: 2.531895019674814

Epoch: 6| Step: 3
Training loss: 2.620131731033325
Validation loss: 2.5307350030509372

Epoch: 6| Step: 4
Training loss: 1.8580069541931152
Validation loss: 2.5367028738862727

Epoch: 6| Step: 5
Training loss: 2.7790651321411133
Validation loss: 2.5350748133915726

Epoch: 6| Step: 6
Training loss: 2.7885944843292236
Validation loss: 2.540097023851128

Epoch: 6| Step: 7
Training loss: 3.50333833694458
Validation loss: 2.5348168547435472

Epoch: 6| Step: 8
Training loss: 2.179765224456787
Validation loss: 2.537264129166962

Epoch: 6| Step: 9
Training loss: 2.467738389968872
Validation loss: 2.5430363301307923

Epoch: 6| Step: 10
Training loss: 3.358961343765259
Validation loss: 2.544739031022595

Epoch: 6| Step: 11
Training loss: 2.4411416053771973
Validation loss: 2.559062204053325

Epoch: 6| Step: 12
Training loss: 3.169236660003662
Validation loss: 2.567290247127574

Epoch: 6| Step: 13
Training loss: 2.9341952800750732
Validation loss: 2.5595349804047616

Epoch: 66| Step: 0
Training loss: 2.917034864425659
Validation loss: 2.548360158038396

Epoch: 6| Step: 1
Training loss: 2.886241912841797
Validation loss: 2.543713972132693

Epoch: 6| Step: 2
Training loss: 2.7773869037628174
Validation loss: 2.5362271160207768

Epoch: 6| Step: 3
Training loss: 3.203461170196533
Validation loss: 2.5380926080929336

Epoch: 6| Step: 4
Training loss: 2.509101390838623
Validation loss: 2.543568185580674

Epoch: 6| Step: 5
Training loss: 2.4425196647644043
Validation loss: 2.5380321369376233

Epoch: 6| Step: 6
Training loss: 2.6122188568115234
Validation loss: 2.5345826994988228

Epoch: 6| Step: 7
Training loss: 2.5934066772460938
Validation loss: 2.5331971363354753

Epoch: 6| Step: 8
Training loss: 1.7586475610733032
Validation loss: 2.5285218325994347

Epoch: 6| Step: 9
Training loss: 3.535205841064453
Validation loss: 2.5255593356265815

Epoch: 6| Step: 10
Training loss: 2.9231839179992676
Validation loss: 2.525715699759863

Epoch: 6| Step: 11
Training loss: 1.697554349899292
Validation loss: 2.5231585400078886

Epoch: 6| Step: 12
Training loss: 2.7316136360168457
Validation loss: 2.5208475589752197

Epoch: 6| Step: 13
Training loss: 4.045393466949463
Validation loss: 2.5205635486110562

Epoch: 67| Step: 0
Training loss: 2.486766815185547
Validation loss: 2.520001167892128

Epoch: 6| Step: 1
Training loss: 3.14438533782959
Validation loss: 2.5238696990474576

Epoch: 6| Step: 2
Training loss: 2.4044761657714844
Validation loss: 2.522175383824174

Epoch: 6| Step: 3
Training loss: 2.0887539386749268
Validation loss: 2.5231014451672955

Epoch: 6| Step: 4
Training loss: 2.8124403953552246
Validation loss: 2.5218541006888113

Epoch: 6| Step: 5
Training loss: 3.117379665374756
Validation loss: 2.5196457703908286

Epoch: 6| Step: 6
Training loss: 3.163315773010254
Validation loss: 2.5213517168516755

Epoch: 6| Step: 7
Training loss: 2.533794641494751
Validation loss: 2.521958056316581

Epoch: 6| Step: 8
Training loss: 2.652611255645752
Validation loss: 2.5208314952029975

Epoch: 6| Step: 9
Training loss: 3.525017499923706
Validation loss: 2.5206052462259927

Epoch: 6| Step: 10
Training loss: 3.2572011947631836
Validation loss: 2.520291336121098

Epoch: 6| Step: 11
Training loss: 1.8726319074630737
Validation loss: 2.5233717067267305

Epoch: 6| Step: 12
Training loss: 2.656731128692627
Validation loss: 2.521059695110526

Epoch: 6| Step: 13
Training loss: 2.1989495754241943
Validation loss: 2.5224578893312843

Epoch: 68| Step: 0
Training loss: 3.054229736328125
Validation loss: 2.5169479231680594

Epoch: 6| Step: 1
Training loss: 2.564725399017334
Validation loss: 2.5280752361461682

Epoch: 6| Step: 2
Training loss: 2.1761951446533203
Validation loss: 2.534627576028147

Epoch: 6| Step: 3
Training loss: 2.4118404388427734
Validation loss: 2.5420288014155563

Epoch: 6| Step: 4
Training loss: 3.2189650535583496
Validation loss: 2.5359652555117043

Epoch: 6| Step: 5
Training loss: 2.9356155395507812
Validation loss: 2.545966268867575

Epoch: 6| Step: 6
Training loss: 2.4581692218780518
Validation loss: 2.5527311858310493

Epoch: 6| Step: 7
Training loss: 2.6100916862487793
Validation loss: 2.5619724283936205

Epoch: 6| Step: 8
Training loss: 2.4577102661132812
Validation loss: 2.5557894270907164

Epoch: 6| Step: 9
Training loss: 2.7253270149230957
Validation loss: 2.5633168758884555

Epoch: 6| Step: 10
Training loss: 3.8648643493652344
Validation loss: 2.575464064075101

Epoch: 6| Step: 11
Training loss: 2.147282600402832
Validation loss: 2.5645031365015174

Epoch: 6| Step: 12
Training loss: 2.8047876358032227
Validation loss: 2.5465255091267247

Epoch: 6| Step: 13
Training loss: 2.675513744354248
Validation loss: 2.535802379731209

Epoch: 69| Step: 0
Training loss: 2.844614028930664
Validation loss: 2.5344857938828005

Epoch: 6| Step: 1
Training loss: 2.457026481628418
Validation loss: 2.528643987512076

Epoch: 6| Step: 2
Training loss: 2.9358279705047607
Validation loss: 2.524251796865976

Epoch: 6| Step: 3
Training loss: 1.9066345691680908
Validation loss: 2.5219682903699976

Epoch: 6| Step: 4
Training loss: 2.582447052001953
Validation loss: 2.523416514037758

Epoch: 6| Step: 5
Training loss: 2.3567452430725098
Validation loss: 2.5201380663020636

Epoch: 6| Step: 6
Training loss: 2.504162311553955
Validation loss: 2.525116889707504

Epoch: 6| Step: 7
Training loss: 2.8777694702148438
Validation loss: 2.5317899283542427

Epoch: 6| Step: 8
Training loss: 2.7733030319213867
Validation loss: 2.5270446756834626

Epoch: 6| Step: 9
Training loss: 3.398921489715576
Validation loss: 2.5333151407139276

Epoch: 6| Step: 10
Training loss: 3.8051187992095947
Validation loss: 2.5316972245452223

Epoch: 6| Step: 11
Training loss: 2.5597896575927734
Validation loss: 2.5290168459697435

Epoch: 6| Step: 12
Training loss: 2.7278668880462646
Validation loss: 2.534951017748925

Epoch: 6| Step: 13
Training loss: 2.027888774871826
Validation loss: 2.5388662584366335

Epoch: 70| Step: 0
Training loss: 2.584516763687134
Validation loss: 2.5386225049213698

Epoch: 6| Step: 1
Training loss: 2.310668468475342
Validation loss: 2.5285481252977924

Epoch: 6| Step: 2
Training loss: 1.9511263370513916
Validation loss: 2.5329822673592517

Epoch: 6| Step: 3
Training loss: 3.1774256229400635
Validation loss: 2.5335961516185472

Epoch: 6| Step: 4
Training loss: 2.783989667892456
Validation loss: 2.5335620372526106

Epoch: 6| Step: 5
Training loss: 2.199282169342041
Validation loss: 2.530133890849288

Epoch: 6| Step: 6
Training loss: 3.075164318084717
Validation loss: 2.5311396506524857

Epoch: 6| Step: 7
Training loss: 2.616065263748169
Validation loss: 2.527495543162028

Epoch: 6| Step: 8
Training loss: 3.054621696472168
Validation loss: 2.528706324997769

Epoch: 6| Step: 9
Training loss: 2.509674072265625
Validation loss: 2.5292326763112056

Epoch: 6| Step: 10
Training loss: 3.1382529735565186
Validation loss: 2.5324500299269155

Epoch: 6| Step: 11
Training loss: 2.2509243488311768
Validation loss: 2.5331754453720583

Epoch: 6| Step: 12
Training loss: 3.1379575729370117
Validation loss: 2.5439649653691117

Epoch: 6| Step: 13
Training loss: 3.4634246826171875
Validation loss: 2.5503175015090616

Epoch: 71| Step: 0
Training loss: 2.6597275733947754
Validation loss: 2.545034106059741

Epoch: 6| Step: 1
Training loss: 2.296612024307251
Validation loss: 2.5451213159868793

Epoch: 6| Step: 2
Training loss: 2.528597593307495
Validation loss: 2.5364297974494194

Epoch: 6| Step: 3
Training loss: 3.1791152954101562
Validation loss: 2.531886649388139

Epoch: 6| Step: 4
Training loss: 2.919816493988037
Validation loss: 2.5293239496087514

Epoch: 6| Step: 5
Training loss: 2.5703186988830566
Validation loss: 2.5262619346700688

Epoch: 6| Step: 6
Training loss: 2.021512508392334
Validation loss: 2.5221557565914687

Epoch: 6| Step: 7
Training loss: 3.31196928024292
Validation loss: 2.5182891045847247

Epoch: 6| Step: 8
Training loss: 2.8529293537139893
Validation loss: 2.5196113201879684

Epoch: 6| Step: 9
Training loss: 3.08510422706604
Validation loss: 2.517033225746565

Epoch: 6| Step: 10
Training loss: 3.4175615310668945
Validation loss: 2.5194394050105924

Epoch: 6| Step: 11
Training loss: 2.7370266914367676
Validation loss: 2.519840963425175

Epoch: 6| Step: 12
Training loss: 1.7491446733474731
Validation loss: 2.516123822940293

Epoch: 6| Step: 13
Training loss: 2.504295587539673
Validation loss: 2.5169326592517156

Epoch: 72| Step: 0
Training loss: 3.0880064964294434
Validation loss: 2.522766169681344

Epoch: 6| Step: 1
Training loss: 3.047518253326416
Validation loss: 2.529234614423526

Epoch: 6| Step: 2
Training loss: 2.060502290725708
Validation loss: 2.529076845415177

Epoch: 6| Step: 3
Training loss: 2.7485146522521973
Validation loss: 2.541495205253683

Epoch: 6| Step: 4
Training loss: 2.620311737060547
Validation loss: 2.5411734914266937

Epoch: 6| Step: 5
Training loss: 3.488041877746582
Validation loss: 2.535656154796641

Epoch: 6| Step: 6
Training loss: 2.1414549350738525
Validation loss: 2.5323690522101616

Epoch: 6| Step: 7
Training loss: 1.977644681930542
Validation loss: 2.5291091370326217

Epoch: 6| Step: 8
Training loss: 2.59568452835083
Validation loss: 2.5285885346833097

Epoch: 6| Step: 9
Training loss: 2.7738852500915527
Validation loss: 2.5210309336262364

Epoch: 6| Step: 10
Training loss: 2.6518120765686035
Validation loss: 2.516490149241622

Epoch: 6| Step: 11
Training loss: 2.781186580657959
Validation loss: 2.5154197626216437

Epoch: 6| Step: 12
Training loss: 2.6366336345672607
Validation loss: 2.515518073112734

Epoch: 6| Step: 13
Training loss: 3.8107218742370605
Validation loss: 2.5102144877115884

Epoch: 73| Step: 0
Training loss: 3.822528123855591
Validation loss: 2.509068847984396

Epoch: 6| Step: 1
Training loss: 1.7137596607208252
Validation loss: 2.5073660291651243

Epoch: 6| Step: 2
Training loss: 3.3528478145599365
Validation loss: 2.505970142220938

Epoch: 6| Step: 3
Training loss: 2.4247138500213623
Validation loss: 2.5095031389626126

Epoch: 6| Step: 4
Training loss: 2.3678383827209473
Validation loss: 2.510150659468866

Epoch: 6| Step: 5
Training loss: 2.5379738807678223
Validation loss: 2.513535379081644

Epoch: 6| Step: 6
Training loss: 3.39497709274292
Validation loss: 2.5085238859217656

Epoch: 6| Step: 7
Training loss: 2.5550572872161865
Validation loss: 2.510643115607641

Epoch: 6| Step: 8
Training loss: 2.6740124225616455
Validation loss: 2.5083900395260064

Epoch: 6| Step: 9
Training loss: 2.098532199859619
Validation loss: 2.5044008019149944

Epoch: 6| Step: 10
Training loss: 2.566868305206299
Validation loss: 2.5081521208568285

Epoch: 6| Step: 11
Training loss: 3.0604705810546875
Validation loss: 2.5136786071203088

Epoch: 6| Step: 12
Training loss: 2.495809316635132
Validation loss: 2.5139274904804845

Epoch: 6| Step: 13
Training loss: 2.971707820892334
Validation loss: 2.5134764986653484

Epoch: 74| Step: 0
Training loss: 2.6904327869415283
Validation loss: 2.5145398596281647

Epoch: 6| Step: 1
Training loss: 2.867072582244873
Validation loss: 2.509931231057772

Epoch: 6| Step: 2
Training loss: 2.901082992553711
Validation loss: 2.511928850604642

Epoch: 6| Step: 3
Training loss: 2.310206413269043
Validation loss: 2.508211099973289

Epoch: 6| Step: 4
Training loss: 2.766871213912964
Validation loss: 2.508236136487735

Epoch: 6| Step: 5
Training loss: 3.0054280757904053
Validation loss: 2.507590422066309

Epoch: 6| Step: 6
Training loss: 2.98861026763916
Validation loss: 2.516607279418617

Epoch: 6| Step: 7
Training loss: 2.570327043533325
Validation loss: 2.521763547774284

Epoch: 6| Step: 8
Training loss: 2.780698299407959
Validation loss: 2.5231028577332855

Epoch: 6| Step: 9
Training loss: 2.5059080123901367
Validation loss: 2.5296117387792116

Epoch: 6| Step: 10
Training loss: 2.7482805252075195
Validation loss: 2.5263673105547504

Epoch: 6| Step: 11
Training loss: 2.518441677093506
Validation loss: 2.53063771288882

Epoch: 6| Step: 12
Training loss: 2.652083396911621
Validation loss: 2.532739729009649

Epoch: 6| Step: 13
Training loss: 2.324716329574585
Validation loss: 2.522291193726242

Epoch: 75| Step: 0
Training loss: 3.126309394836426
Validation loss: 2.5300165837810886

Epoch: 6| Step: 1
Training loss: 2.300332546234131
Validation loss: 2.5254668830543436

Epoch: 6| Step: 2
Training loss: 2.812833547592163
Validation loss: 2.5179237781032437

Epoch: 6| Step: 3
Training loss: 2.490159273147583
Validation loss: 2.527286083467545

Epoch: 6| Step: 4
Training loss: 2.390392780303955
Validation loss: 2.528898516008931

Epoch: 6| Step: 5
Training loss: 2.9018497467041016
Validation loss: 2.534225994540799

Epoch: 6| Step: 6
Training loss: 3.4434268474578857
Validation loss: 2.532724529184321

Epoch: 6| Step: 7
Training loss: 3.083439826965332
Validation loss: 2.527488044513169

Epoch: 6| Step: 8
Training loss: 3.126081943511963
Validation loss: 2.5186690258723434

Epoch: 6| Step: 9
Training loss: 2.635880708694458
Validation loss: 2.501219444377448

Epoch: 6| Step: 10
Training loss: 2.428739070892334
Validation loss: 2.506523706579721

Epoch: 6| Step: 11
Training loss: 1.9848122596740723
Validation loss: 2.507171300149733

Epoch: 6| Step: 12
Training loss: 2.598587989807129
Validation loss: 2.502727359853765

Epoch: 6| Step: 13
Training loss: 2.3799452781677246
Validation loss: 2.507499692260578

Epoch: 76| Step: 0
Training loss: 3.3418140411376953
Validation loss: 2.5129043671392624

Epoch: 6| Step: 1
Training loss: 3.37617826461792
Validation loss: 2.5150236211797243

Epoch: 6| Step: 2
Training loss: 2.6311845779418945
Validation loss: 2.5173622523584673

Epoch: 6| Step: 3
Training loss: 3.19248366355896
Validation loss: 2.520827821505967

Epoch: 6| Step: 4
Training loss: 2.5699710845947266
Validation loss: 2.530088742574056

Epoch: 6| Step: 5
Training loss: 2.7551064491271973
Validation loss: 2.529318383944932

Epoch: 6| Step: 6
Training loss: 2.049128293991089
Validation loss: 2.5375699202219644

Epoch: 6| Step: 7
Training loss: 1.8142139911651611
Validation loss: 2.524713441889773

Epoch: 6| Step: 8
Training loss: 2.598259210586548
Validation loss: 2.5154511979831162

Epoch: 6| Step: 9
Training loss: 1.7703205347061157
Validation loss: 2.5074869586575415

Epoch: 6| Step: 10
Training loss: 2.9452178478240967
Validation loss: 2.501584058166832

Epoch: 6| Step: 11
Training loss: 2.647364854812622
Validation loss: 2.4999227498167302

Epoch: 6| Step: 12
Training loss: 3.503309726715088
Validation loss: 2.5033799986685477

Epoch: 6| Step: 13
Training loss: 2.413703441619873
Validation loss: 2.503232632913897

Epoch: 77| Step: 0
Training loss: 2.352560043334961
Validation loss: 2.509534623033257

Epoch: 6| Step: 1
Training loss: 3.18741774559021
Validation loss: 2.5061673451495428

Epoch: 6| Step: 2
Training loss: 2.351001262664795
Validation loss: 2.509546305543633

Epoch: 6| Step: 3
Training loss: 2.5980329513549805
Validation loss: 2.519086273767615

Epoch: 6| Step: 4
Training loss: 3.108478546142578
Validation loss: 2.537870189195038

Epoch: 6| Step: 5
Training loss: 3.022035598754883
Validation loss: 2.5721366661851124

Epoch: 6| Step: 6
Training loss: 1.9277136325836182
Validation loss: 2.5860867910487677

Epoch: 6| Step: 7
Training loss: 2.7758562564849854
Validation loss: 2.581107783061202

Epoch: 6| Step: 8
Training loss: 2.799415111541748
Validation loss: 2.583392484213716

Epoch: 6| Step: 9
Training loss: 2.48004412651062
Validation loss: 2.563825253517397

Epoch: 6| Step: 10
Training loss: 2.637723922729492
Validation loss: 2.53860209321463

Epoch: 6| Step: 11
Training loss: 2.8343100547790527
Validation loss: 2.5055744365979264

Epoch: 6| Step: 12
Training loss: 1.934183120727539
Validation loss: 2.4951716853726293

Epoch: 6| Step: 13
Training loss: 5.034189224243164
Validation loss: 2.4935029040100756

Epoch: 78| Step: 0
Training loss: 3.3745477199554443
Validation loss: 2.4926972876312914

Epoch: 6| Step: 1
Training loss: 2.5163607597351074
Validation loss: 2.498748076859341

Epoch: 6| Step: 2
Training loss: 2.7585606575012207
Validation loss: 2.5026664759523127

Epoch: 6| Step: 3
Training loss: 2.9181160926818848
Validation loss: 2.5188461606220534

Epoch: 6| Step: 4
Training loss: 2.369471549987793
Validation loss: 2.5279354895314863

Epoch: 6| Step: 5
Training loss: 2.932871103286743
Validation loss: 2.5278593647864556

Epoch: 6| Step: 6
Training loss: 2.849632740020752
Validation loss: 2.5070738151509273

Epoch: 6| Step: 7
Training loss: 2.4091720581054688
Validation loss: 2.499125370415308

Epoch: 6| Step: 8
Training loss: 2.9172489643096924
Validation loss: 2.497793810341948

Epoch: 6| Step: 9
Training loss: 2.4868931770324707
Validation loss: 2.484513149466566

Epoch: 6| Step: 10
Training loss: 2.066936731338501
Validation loss: 2.486836387265113

Epoch: 6| Step: 11
Training loss: 3.0570735931396484
Validation loss: 2.486435318505892

Epoch: 6| Step: 12
Training loss: 3.015939950942993
Validation loss: 2.486881163812453

Epoch: 6| Step: 13
Training loss: 1.6116912364959717
Validation loss: 2.490870721878544

Epoch: 79| Step: 0
Training loss: 2.952160358428955
Validation loss: 2.48644700870719

Epoch: 6| Step: 1
Training loss: 3.056502103805542
Validation loss: 2.4912776331747732

Epoch: 6| Step: 2
Training loss: 2.980848789215088
Validation loss: 2.484543190207533

Epoch: 6| Step: 3
Training loss: 2.4413695335388184
Validation loss: 2.481905283466462

Epoch: 6| Step: 4
Training loss: 2.8662805557250977
Validation loss: 2.4831053467207056

Epoch: 6| Step: 5
Training loss: 3.018789052963257
Validation loss: 2.484908808944046

Epoch: 6| Step: 6
Training loss: 2.4523680210113525
Validation loss: 2.4861898037695114

Epoch: 6| Step: 7
Training loss: 1.9862886667251587
Validation loss: 2.4860580352044876

Epoch: 6| Step: 8
Training loss: 3.2104270458221436
Validation loss: 2.481015472001927

Epoch: 6| Step: 9
Training loss: 1.9968180656433105
Validation loss: 2.4813401673429754

Epoch: 6| Step: 10
Training loss: 2.3947722911834717
Validation loss: 2.482248621602212

Epoch: 6| Step: 11
Training loss: 2.974152088165283
Validation loss: 2.4821441096644246

Epoch: 6| Step: 12
Training loss: 3.197014093399048
Validation loss: 2.48218217203694

Epoch: 6| Step: 13
Training loss: 1.7535263299942017
Validation loss: 2.4822662440679406

Epoch: 80| Step: 0
Training loss: 2.4787487983703613
Validation loss: 2.483390138995263

Epoch: 6| Step: 1
Training loss: 2.98507022857666
Validation loss: 2.4902545405972387

Epoch: 6| Step: 2
Training loss: 2.9172780513763428
Validation loss: 2.4947637793838338

Epoch: 6| Step: 3
Training loss: 2.4994657039642334
Validation loss: 2.4961050838552494

Epoch: 6| Step: 4
Training loss: 3.5998568534851074
Validation loss: 2.503510254685597

Epoch: 6| Step: 5
Training loss: 2.9256110191345215
Validation loss: 2.502254045137795

Epoch: 6| Step: 6
Training loss: 3.0084176063537598
Validation loss: 2.4886391214145127

Epoch: 6| Step: 7
Training loss: 1.7859755754470825
Validation loss: 2.492750293465071

Epoch: 6| Step: 8
Training loss: 1.9971880912780762
Validation loss: 2.4946184594144105

Epoch: 6| Step: 9
Training loss: 3.2153334617614746
Validation loss: 2.4796968685683383

Epoch: 6| Step: 10
Training loss: 3.2658560276031494
Validation loss: 2.4773290234227336

Epoch: 6| Step: 11
Training loss: 2.3259592056274414
Validation loss: 2.4780459468082716

Epoch: 6| Step: 12
Training loss: 2.410971164703369
Validation loss: 2.4765705267588296

Epoch: 6| Step: 13
Training loss: 1.8381396532058716
Validation loss: 2.4739001668909544

Epoch: 81| Step: 0
Training loss: 2.4840879440307617
Validation loss: 2.4768147673658145

Epoch: 6| Step: 1
Training loss: 2.741455316543579
Validation loss: 2.4779266516367593

Epoch: 6| Step: 2
Training loss: 2.589261054992676
Validation loss: 2.476826178130283

Epoch: 6| Step: 3
Training loss: 2.2446112632751465
Validation loss: 2.4779161022555445

Epoch: 6| Step: 4
Training loss: 2.4181673526763916
Validation loss: 2.4785089902980353

Epoch: 6| Step: 5
Training loss: 2.7576956748962402
Validation loss: 2.4756661204881567

Epoch: 6| Step: 6
Training loss: 3.530374050140381
Validation loss: 2.4749425329187864

Epoch: 6| Step: 7
Training loss: 2.636453151702881
Validation loss: 2.473468295989498

Epoch: 6| Step: 8
Training loss: 3.3737740516662598
Validation loss: 2.4757708452081166

Epoch: 6| Step: 9
Training loss: 2.3397507667541504
Validation loss: 2.4752159477562032

Epoch: 6| Step: 10
Training loss: 2.9725518226623535
Validation loss: 2.4784381697254796

Epoch: 6| Step: 11
Training loss: 2.430912971496582
Validation loss: 2.476994332446847

Epoch: 6| Step: 12
Training loss: 2.8999457359313965
Validation loss: 2.48291435292972

Epoch: 6| Step: 13
Training loss: 1.8481584787368774
Validation loss: 2.484204608906982

Epoch: 82| Step: 0
Training loss: 2.34372615814209
Validation loss: 2.4767657300477386

Epoch: 6| Step: 1
Training loss: 2.0587096214294434
Validation loss: 2.477907375622821

Epoch: 6| Step: 2
Training loss: 2.4667739868164062
Validation loss: 2.4755777184681227

Epoch: 6| Step: 3
Training loss: 2.382092237472534
Validation loss: 2.4793839236741424

Epoch: 6| Step: 4
Training loss: 2.8354949951171875
Validation loss: 2.475642955431374

Epoch: 6| Step: 5
Training loss: 2.7395262718200684
Validation loss: 2.4830152296250865

Epoch: 6| Step: 6
Training loss: 2.4073562622070312
Validation loss: 2.4876614334762737

Epoch: 6| Step: 7
Training loss: 3.0327401161193848
Validation loss: 2.4942898340122674

Epoch: 6| Step: 8
Training loss: 2.76298189163208
Validation loss: 2.5003200602787796

Epoch: 6| Step: 9
Training loss: 3.5141587257385254
Validation loss: 2.512150349155549

Epoch: 6| Step: 10
Training loss: 2.329690933227539
Validation loss: 2.504583940711073

Epoch: 6| Step: 11
Training loss: 2.987989902496338
Validation loss: 2.4936769213727725

Epoch: 6| Step: 12
Training loss: 2.881100654602051
Validation loss: 2.4842547678178355

Epoch: 6| Step: 13
Training loss: 2.88071346282959
Validation loss: 2.4851389700366604

Epoch: 83| Step: 0
Training loss: 2.219468593597412
Validation loss: 2.4846787478334162

Epoch: 6| Step: 1
Training loss: 2.4394612312316895
Validation loss: 2.483490664471862

Epoch: 6| Step: 2
Training loss: 2.8024227619171143
Validation loss: 2.4824529591427056

Epoch: 6| Step: 3
Training loss: 2.6655733585357666
Validation loss: 2.489694397936585

Epoch: 6| Step: 4
Training loss: 2.878518581390381
Validation loss: 2.4821539873717935

Epoch: 6| Step: 5
Training loss: 3.279352903366089
Validation loss: 2.4777644834210797

Epoch: 6| Step: 6
Training loss: 2.485457181930542
Validation loss: 2.4713107834580126

Epoch: 6| Step: 7
Training loss: 2.9540212154388428
Validation loss: 2.470507615356035

Epoch: 6| Step: 8
Training loss: 2.097221851348877
Validation loss: 2.4711102824057303

Epoch: 6| Step: 9
Training loss: 3.1136207580566406
Validation loss: 2.4681457575931343

Epoch: 6| Step: 10
Training loss: 3.466677188873291
Validation loss: 2.4770286724131596

Epoch: 6| Step: 11
Training loss: 2.2588443756103516
Validation loss: 2.478233673239267

Epoch: 6| Step: 12
Training loss: 2.3835644721984863
Validation loss: 2.4814437076609623

Epoch: 6| Step: 13
Training loss: 2.403384208679199
Validation loss: 2.483335474485992

Epoch: 84| Step: 0
Training loss: 2.187643051147461
Validation loss: 2.4882563416675856

Epoch: 6| Step: 1
Training loss: 2.3675613403320312
Validation loss: 2.4869856090955835

Epoch: 6| Step: 2
Training loss: 3.1160948276519775
Validation loss: 2.497684157022866

Epoch: 6| Step: 3
Training loss: 2.4365410804748535
Validation loss: 2.488027516231742

Epoch: 6| Step: 4
Training loss: 2.892991065979004
Validation loss: 2.4929457377361994

Epoch: 6| Step: 5
Training loss: 2.522068738937378
Validation loss: 2.489957078810661

Epoch: 6| Step: 6
Training loss: 2.72688889503479
Validation loss: 2.4861148916265017

Epoch: 6| Step: 7
Training loss: 2.640559196472168
Validation loss: 2.4798224920867593

Epoch: 6| Step: 8
Training loss: 2.8396668434143066
Validation loss: 2.471306816224129

Epoch: 6| Step: 9
Training loss: 2.5295839309692383
Validation loss: 2.4644834072359147

Epoch: 6| Step: 10
Training loss: 2.9528985023498535
Validation loss: 2.4661377258198236

Epoch: 6| Step: 11
Training loss: 2.3836679458618164
Validation loss: 2.468755360572569

Epoch: 6| Step: 12
Training loss: 3.0529263019561768
Validation loss: 2.4692027030452603

Epoch: 6| Step: 13
Training loss: 2.863182544708252
Validation loss: 2.4755671178140948

Epoch: 85| Step: 0
Training loss: 2.550264358520508
Validation loss: 2.4772455794836885

Epoch: 6| Step: 1
Training loss: 3.4153714179992676
Validation loss: 2.481011224049394

Epoch: 6| Step: 2
Training loss: 2.206404685974121
Validation loss: 2.4815128234124955

Epoch: 6| Step: 3
Training loss: 3.1142563819885254
Validation loss: 2.4820244414832002

Epoch: 6| Step: 4
Training loss: 2.756579875946045
Validation loss: 2.483884803710445

Epoch: 6| Step: 5
Training loss: 2.535743236541748
Validation loss: 2.483802903083063

Epoch: 6| Step: 6
Training loss: 2.7434067726135254
Validation loss: 2.4845005645546863

Epoch: 6| Step: 7
Training loss: 3.0861358642578125
Validation loss: 2.4845815653442056

Epoch: 6| Step: 8
Training loss: 1.6744012832641602
Validation loss: 2.4824842150493334

Epoch: 6| Step: 9
Training loss: 3.2606401443481445
Validation loss: 2.4766849933132047

Epoch: 6| Step: 10
Training loss: 2.611863136291504
Validation loss: 2.4773052918013705

Epoch: 6| Step: 11
Training loss: 2.8890271186828613
Validation loss: 2.469567979535749

Epoch: 6| Step: 12
Training loss: 2.394056797027588
Validation loss: 2.4712953182958786

Epoch: 6| Step: 13
Training loss: 2.3532304763793945
Validation loss: 2.468460803390831

Epoch: 86| Step: 0
Training loss: 4.060802459716797
Validation loss: 2.467497535931167

Epoch: 6| Step: 1
Training loss: 2.192594051361084
Validation loss: 2.46489405503837

Epoch: 6| Step: 2
Training loss: 2.3650519847869873
Validation loss: 2.465044036988289

Epoch: 6| Step: 3
Training loss: 2.112605571746826
Validation loss: 2.4696707289705992

Epoch: 6| Step: 4
Training loss: 2.3129630088806152
Validation loss: 2.470345661204348

Epoch: 6| Step: 5
Training loss: 2.9754903316497803
Validation loss: 2.468379738510296

Epoch: 6| Step: 6
Training loss: 1.7190930843353271
Validation loss: 2.4683276684053483

Epoch: 6| Step: 7
Training loss: 2.587977886199951
Validation loss: 2.469971331216956

Epoch: 6| Step: 8
Training loss: 2.5616180896759033
Validation loss: 2.470503290494283

Epoch: 6| Step: 9
Training loss: 1.9204163551330566
Validation loss: 2.4697198278160504

Epoch: 6| Step: 10
Training loss: 2.8972392082214355
Validation loss: 2.4671540157769316

Epoch: 6| Step: 11
Training loss: 4.306262016296387
Validation loss: 2.467938264211019

Epoch: 6| Step: 12
Training loss: 3.02183198928833
Validation loss: 2.461657057526291

Epoch: 6| Step: 13
Training loss: 2.251192092895508
Validation loss: 2.4661257754090014

Epoch: 87| Step: 0
Training loss: 2.395751476287842
Validation loss: 2.4694511223864812

Epoch: 6| Step: 1
Training loss: 2.7740230560302734
Validation loss: 2.4578421551694154

Epoch: 6| Step: 2
Training loss: 2.768911838531494
Validation loss: 2.4639433994088122

Epoch: 6| Step: 3
Training loss: 2.6377577781677246
Validation loss: 2.460618362631849

Epoch: 6| Step: 4
Training loss: 2.6351494789123535
Validation loss: 2.462169425461882

Epoch: 6| Step: 5
Training loss: 2.319819927215576
Validation loss: 2.458325121992378

Epoch: 6| Step: 6
Training loss: 2.146505832672119
Validation loss: 2.456981974263345

Epoch: 6| Step: 7
Training loss: 2.9455223083496094
Validation loss: 2.457964522864229

Epoch: 6| Step: 8
Training loss: 2.8805391788482666
Validation loss: 2.4604357263093353

Epoch: 6| Step: 9
Training loss: 2.727365493774414
Validation loss: 2.459362401757189

Epoch: 6| Step: 10
Training loss: 2.9632866382598877
Validation loss: 2.4618653302551596

Epoch: 6| Step: 11
Training loss: 2.774937152862549
Validation loss: 2.4652421859002884

Epoch: 6| Step: 12
Training loss: 2.2000210285186768
Validation loss: 2.4692824553417903

Epoch: 6| Step: 13
Training loss: 3.5994632244110107
Validation loss: 2.467168866947133

Epoch: 88| Step: 0
Training loss: 2.514105796813965
Validation loss: 2.4668353449913765

Epoch: 6| Step: 1
Training loss: 3.216400146484375
Validation loss: 2.4686526765105543

Epoch: 6| Step: 2
Training loss: 3.233827590942383
Validation loss: 2.46135530676893

Epoch: 6| Step: 3
Training loss: 2.0677804946899414
Validation loss: 2.4684516999029342

Epoch: 6| Step: 4
Training loss: 2.1227211952209473
Validation loss: 2.4647280631526822

Epoch: 6| Step: 5
Training loss: 2.3065693378448486
Validation loss: 2.4710508290157525

Epoch: 6| Step: 6
Training loss: 2.9130430221557617
Validation loss: 2.4721122326389438

Epoch: 6| Step: 7
Training loss: 2.8643155097961426
Validation loss: 2.4743741558444117

Epoch: 6| Step: 8
Training loss: 2.7227983474731445
Validation loss: 2.4767003623388146

Epoch: 6| Step: 9
Training loss: 2.5084753036499023
Validation loss: 2.475810814929265

Epoch: 6| Step: 10
Training loss: 2.8626036643981934
Validation loss: 2.47302254041036

Epoch: 6| Step: 11
Training loss: 2.7293481826782227
Validation loss: 2.4711812260330364

Epoch: 6| Step: 12
Training loss: 3.1726861000061035
Validation loss: 2.4672876275995725

Epoch: 6| Step: 13
Training loss: 1.7522971630096436
Validation loss: 2.4675887810286654

Epoch: 89| Step: 0
Training loss: 2.636845827102661
Validation loss: 2.4617898284748034

Epoch: 6| Step: 1
Training loss: 2.1069445610046387
Validation loss: 2.4596134590846237

Epoch: 6| Step: 2
Training loss: 3.2084457874298096
Validation loss: 2.457815440752173

Epoch: 6| Step: 3
Training loss: 3.5970191955566406
Validation loss: 2.451796570131856

Epoch: 6| Step: 4
Training loss: 2.797451972961426
Validation loss: 2.4586709391686226

Epoch: 6| Step: 5
Training loss: 2.5119268894195557
Validation loss: 2.452734398585494

Epoch: 6| Step: 6
Training loss: 3.118961811065674
Validation loss: 2.450411022350352

Epoch: 6| Step: 7
Training loss: 1.6489583253860474
Validation loss: 2.452980487577377

Epoch: 6| Step: 8
Training loss: 2.026094436645508
Validation loss: 2.4553236730637087

Epoch: 6| Step: 9
Training loss: 2.89024019241333
Validation loss: 2.4581166774995866

Epoch: 6| Step: 10
Training loss: 2.8095571994781494
Validation loss: 2.455324452410462

Epoch: 6| Step: 11
Training loss: 2.8149211406707764
Validation loss: 2.456539592435283

Epoch: 6| Step: 12
Training loss: 2.1754183769226074
Validation loss: 2.457695386743033

Epoch: 6| Step: 13
Training loss: 3.1873281002044678
Validation loss: 2.4746770961310274

Epoch: 90| Step: 0
Training loss: 2.5584466457366943
Validation loss: 2.4985642715166976

Epoch: 6| Step: 1
Training loss: 2.5177435874938965
Validation loss: 2.5218330224355063

Epoch: 6| Step: 2
Training loss: 2.829641819000244
Validation loss: 2.512890100479126

Epoch: 6| Step: 3
Training loss: 2.7900843620300293
Validation loss: 2.4946787203511884

Epoch: 6| Step: 4
Training loss: 2.1631832122802734
Validation loss: 2.469197073290425

Epoch: 6| Step: 5
Training loss: 2.4657983779907227
Validation loss: 2.4578625156033422

Epoch: 6| Step: 6
Training loss: 2.91843843460083
Validation loss: 2.4548192229322208

Epoch: 6| Step: 7
Training loss: 3.335767984390259
Validation loss: 2.455908457438151

Epoch: 6| Step: 8
Training loss: 2.391852378845215
Validation loss: 2.4527735556325605

Epoch: 6| Step: 9
Training loss: 3.0315144062042236
Validation loss: 2.4500744317167547

Epoch: 6| Step: 10
Training loss: 3.2176287174224854
Validation loss: 2.451539733076608

Epoch: 6| Step: 11
Training loss: 2.1636710166931152
Validation loss: 2.452224108480638

Epoch: 6| Step: 12
Training loss: 2.553046703338623
Validation loss: 2.4556160665327504

Epoch: 6| Step: 13
Training loss: 2.4108831882476807
Validation loss: 2.452264711421023

Epoch: 91| Step: 0
Training loss: 2.179408073425293
Validation loss: 2.44645490313089

Epoch: 6| Step: 1
Training loss: 2.9762110710144043
Validation loss: 2.4457632418601745

Epoch: 6| Step: 2
Training loss: 3.196105480194092
Validation loss: 2.454495617138442

Epoch: 6| Step: 3
Training loss: 3.487267255783081
Validation loss: 2.4542271039819203

Epoch: 6| Step: 4
Training loss: 2.6012985706329346
Validation loss: 2.463518568264541

Epoch: 6| Step: 5
Training loss: 2.785890579223633
Validation loss: 2.4684609315728627

Epoch: 6| Step: 6
Training loss: 2.3521907329559326
Validation loss: 2.4653077561368226

Epoch: 6| Step: 7
Training loss: 2.2301442623138428
Validation loss: 2.4749004097395044

Epoch: 6| Step: 8
Training loss: 1.9716722965240479
Validation loss: 2.47605497734521

Epoch: 6| Step: 9
Training loss: 2.8984153270721436
Validation loss: 2.4708057988074517

Epoch: 6| Step: 10
Training loss: 2.3598761558532715
Validation loss: 2.4517989645722094

Epoch: 6| Step: 11
Training loss: 3.388366222381592
Validation loss: 2.4510528272198093

Epoch: 6| Step: 12
Training loss: 2.8426873683929443
Validation loss: 2.443221758770686

Epoch: 6| Step: 13
Training loss: 1.6768752336502075
Validation loss: 2.4484480170793432

Epoch: 92| Step: 0
Training loss: 2.520458221435547
Validation loss: 2.4453734838834373

Epoch: 6| Step: 1
Training loss: 2.4668970108032227
Validation loss: 2.4469933561099473

Epoch: 6| Step: 2
Training loss: 3.0172643661499023
Validation loss: 2.450307881960305

Epoch: 6| Step: 3
Training loss: 3.0918381214141846
Validation loss: 2.4517140978126117

Epoch: 6| Step: 4
Training loss: 1.9326170682907104
Validation loss: 2.4537430476116877

Epoch: 6| Step: 5
Training loss: 2.666511297225952
Validation loss: 2.4543832860967165

Epoch: 6| Step: 6
Training loss: 2.721701145172119
Validation loss: 2.455742633470925

Epoch: 6| Step: 7
Training loss: 2.789724826812744
Validation loss: 2.4609287759309173

Epoch: 6| Step: 8
Training loss: 2.6409120559692383
Validation loss: 2.4603973870636313

Epoch: 6| Step: 9
Training loss: 3.4830870628356934
Validation loss: 2.4669220780813568

Epoch: 6| Step: 10
Training loss: 2.2574331760406494
Validation loss: 2.473607429894068

Epoch: 6| Step: 11
Training loss: 2.3441720008850098
Validation loss: 2.48814461692687

Epoch: 6| Step: 12
Training loss: 3.0335373878479004
Validation loss: 2.488084211144396

Epoch: 6| Step: 13
Training loss: 2.08292555809021
Validation loss: 2.4705246007570656

Epoch: 93| Step: 0
Training loss: 2.6040947437286377
Validation loss: 2.4531835432975524

Epoch: 6| Step: 1
Training loss: 2.5434675216674805
Validation loss: 2.441213346296741

Epoch: 6| Step: 2
Training loss: 2.7192490100860596
Validation loss: 2.442487075764646

Epoch: 6| Step: 3
Training loss: 2.8768200874328613
Validation loss: 2.450770362730949

Epoch: 6| Step: 4
Training loss: 3.156562328338623
Validation loss: 2.4500517793880996

Epoch: 6| Step: 5
Training loss: 2.390749454498291
Validation loss: 2.4514509554832213

Epoch: 6| Step: 6
Training loss: 2.6486454010009766
Validation loss: 2.449178716187836

Epoch: 6| Step: 7
Training loss: 3.077204704284668
Validation loss: 2.451249578947662

Epoch: 6| Step: 8
Training loss: 1.9194873571395874
Validation loss: 2.444062363716864

Epoch: 6| Step: 9
Training loss: 2.6573057174682617
Validation loss: 2.4397592544555664

Epoch: 6| Step: 10
Training loss: 2.799185276031494
Validation loss: 2.4355878624864804

Epoch: 6| Step: 11
Training loss: 2.3596839904785156
Validation loss: 2.4378298303132415

Epoch: 6| Step: 12
Training loss: 2.898861885070801
Validation loss: 2.441948454867127

Epoch: 6| Step: 13
Training loss: 2.773338794708252
Validation loss: 2.443120784656976

Epoch: 94| Step: 0
Training loss: 2.702479600906372
Validation loss: 2.449583161261774

Epoch: 6| Step: 1
Training loss: 3.2158451080322266
Validation loss: 2.4719440475586922

Epoch: 6| Step: 2
Training loss: 2.83965802192688
Validation loss: 2.483039944402633

Epoch: 6| Step: 3
Training loss: 2.7427713871002197
Validation loss: 2.486808628164312

Epoch: 6| Step: 4
Training loss: 2.895030975341797
Validation loss: 2.4848235755838375

Epoch: 6| Step: 5
Training loss: 2.162871837615967
Validation loss: 2.473589197281868

Epoch: 6| Step: 6
Training loss: 2.926478385925293
Validation loss: 2.455873925198791

Epoch: 6| Step: 7
Training loss: 2.8785715103149414
Validation loss: 2.4539075538676274

Epoch: 6| Step: 8
Training loss: 3.088613748550415
Validation loss: 2.449331707851861

Epoch: 6| Step: 9
Training loss: 2.3829798698425293
Validation loss: 2.4439863645902244

Epoch: 6| Step: 10
Training loss: 2.448490619659424
Validation loss: 2.4426573989211873

Epoch: 6| Step: 11
Training loss: 2.497319221496582
Validation loss: 2.441695874737155

Epoch: 6| Step: 12
Training loss: 2.4200236797332764
Validation loss: 2.4455313246737242

Epoch: 6| Step: 13
Training loss: 1.701124668121338
Validation loss: 2.4501411402097313

Epoch: 95| Step: 0
Training loss: 2.2116756439208984
Validation loss: 2.4535394971088698

Epoch: 6| Step: 1
Training loss: 2.535799503326416
Validation loss: 2.4554873948456137

Epoch: 6| Step: 2
Training loss: 2.561997413635254
Validation loss: 2.453931543134874

Epoch: 6| Step: 3
Training loss: 2.7030773162841797
Validation loss: 2.4635670928544897

Epoch: 6| Step: 4
Training loss: 3.05757999420166
Validation loss: 2.458481219507033

Epoch: 6| Step: 5
Training loss: 1.9508099555969238
Validation loss: 2.456164562573997

Epoch: 6| Step: 6
Training loss: 2.47987699508667
Validation loss: 2.4537337646689465

Epoch: 6| Step: 7
Training loss: 3.454921245574951
Validation loss: 2.4448589227532826

Epoch: 6| Step: 8
Training loss: 2.096095561981201
Validation loss: 2.4479546393117597

Epoch: 6| Step: 9
Training loss: 2.8511292934417725
Validation loss: 2.445160599165065

Epoch: 6| Step: 10
Training loss: 2.906954288482666
Validation loss: 2.4439974190086446

Epoch: 6| Step: 11
Training loss: 2.8197968006134033
Validation loss: 2.4422178909342778

Epoch: 6| Step: 12
Training loss: 3.2179818153381348
Validation loss: 2.442032806334957

Epoch: 6| Step: 13
Training loss: 2.2337465286254883
Validation loss: 2.437544999584075

Epoch: 96| Step: 0
Training loss: 2.5156354904174805
Validation loss: 2.4385419968635804

Epoch: 6| Step: 1
Training loss: 2.314409017562866
Validation loss: 2.441792083042924

Epoch: 6| Step: 2
Training loss: 2.5432827472686768
Validation loss: 2.4435060254989134

Epoch: 6| Step: 3
Training loss: 2.1601572036743164
Validation loss: 2.4393769900004068

Epoch: 6| Step: 4
Training loss: 3.458611488342285
Validation loss: 2.4468227970984673

Epoch: 6| Step: 5
Training loss: 2.3766112327575684
Validation loss: 2.449080149332682

Epoch: 6| Step: 6
Training loss: 2.8590493202209473
Validation loss: 2.4562452582902807

Epoch: 6| Step: 7
Training loss: 2.766282081604004
Validation loss: 2.4611881497085735

Epoch: 6| Step: 8
Training loss: 2.9880080223083496
Validation loss: 2.4660653427083004

Epoch: 6| Step: 9
Training loss: 2.86505126953125
Validation loss: 2.465764727643741

Epoch: 6| Step: 10
Training loss: 2.8366312980651855
Validation loss: 2.4489655827963226

Epoch: 6| Step: 11
Training loss: 2.265836477279663
Validation loss: 2.443935486578172

Epoch: 6| Step: 12
Training loss: 2.765958309173584
Validation loss: 2.4357136936597925

Epoch: 6| Step: 13
Training loss: 2.4851438999176025
Validation loss: 2.4332263956787767

Epoch: 97| Step: 0
Training loss: 2.2198400497436523
Validation loss: 2.432500195759599

Epoch: 6| Step: 1
Training loss: 2.6848185062408447
Validation loss: 2.4345267511183217

Epoch: 6| Step: 2
Training loss: 2.320643663406372
Validation loss: 2.428835433016541

Epoch: 6| Step: 3
Training loss: 2.6965744495391846
Validation loss: 2.429295532165035

Epoch: 6| Step: 4
Training loss: 2.29799747467041
Validation loss: 2.4320064719005297

Epoch: 6| Step: 5
Training loss: 2.981841564178467
Validation loss: 2.4298895225729993

Epoch: 6| Step: 6
Training loss: 3.261906147003174
Validation loss: 2.429671223445605

Epoch: 6| Step: 7
Training loss: 2.7746753692626953
Validation loss: 2.4279120122232745

Epoch: 6| Step: 8
Training loss: 3.2606582641601562
Validation loss: 2.4292133341553392

Epoch: 6| Step: 9
Training loss: 2.95973801612854
Validation loss: 2.427135206037952

Epoch: 6| Step: 10
Training loss: 2.04073166847229
Validation loss: 2.4254262831903275

Epoch: 6| Step: 11
Training loss: 2.654615640640259
Validation loss: 2.4262932449258785

Epoch: 6| Step: 12
Training loss: 2.589289665222168
Validation loss: 2.4240297335450367

Epoch: 6| Step: 13
Training loss: 2.31227707862854
Validation loss: 2.4245354693423034

Epoch: 98| Step: 0
Training loss: 3.1372289657592773
Validation loss: 2.4338121670548634

Epoch: 6| Step: 1
Training loss: 2.6078262329101562
Validation loss: 2.4354127760856383

Epoch: 6| Step: 2
Training loss: 2.8867483139038086
Validation loss: 2.4425791360998668

Epoch: 6| Step: 3
Training loss: 2.919574499130249
Validation loss: 2.4581541707438808

Epoch: 6| Step: 4
Training loss: 3.1421074867248535
Validation loss: 2.478115315078407

Epoch: 6| Step: 5
Training loss: 2.032240867614746
Validation loss: 2.486897181439143

Epoch: 6| Step: 6
Training loss: 3.1750571727752686
Validation loss: 2.4938005273060133

Epoch: 6| Step: 7
Training loss: 2.1850903034210205
Validation loss: 2.4890934882625455

Epoch: 6| Step: 8
Training loss: 2.34757137298584
Validation loss: 2.4614044363780687

Epoch: 6| Step: 9
Training loss: 2.6328842639923096
Validation loss: 2.4435802211043653

Epoch: 6| Step: 10
Training loss: 2.0152201652526855
Validation loss: 2.442198891793528

Epoch: 6| Step: 11
Training loss: 2.7927370071411133
Validation loss: 2.434329209789153

Epoch: 6| Step: 12
Training loss: 2.9252052307128906
Validation loss: 2.4397968963910173

Epoch: 6| Step: 13
Training loss: 2.412567138671875
Validation loss: 2.4343803621107534

Epoch: 99| Step: 0
Training loss: 3.2520036697387695
Validation loss: 2.437591193824686

Epoch: 6| Step: 1
Training loss: 2.3349599838256836
Validation loss: 2.431940209481024

Epoch: 6| Step: 2
Training loss: 3.0659732818603516
Validation loss: 2.4369642247435865

Epoch: 6| Step: 3
Training loss: 2.7017648220062256
Validation loss: 2.4372073758033013

Epoch: 6| Step: 4
Training loss: 3.0475616455078125
Validation loss: 2.4383027374103503

Epoch: 6| Step: 5
Training loss: 2.8426361083984375
Validation loss: 2.451125478231779

Epoch: 6| Step: 6
Training loss: 2.2739012241363525
Validation loss: 2.4556453817634174

Epoch: 6| Step: 7
Training loss: 1.994692087173462
Validation loss: 2.4568798054930983

Epoch: 6| Step: 8
Training loss: 2.475943088531494
Validation loss: 2.4532768085438716

Epoch: 6| Step: 9
Training loss: 2.712047576904297
Validation loss: 2.4578166571996545

Epoch: 6| Step: 10
Training loss: 2.4608829021453857
Validation loss: 2.4408358450858825

Epoch: 6| Step: 11
Training loss: 2.5846335887908936
Validation loss: 2.430481485141221

Epoch: 6| Step: 12
Training loss: 2.4934957027435303
Validation loss: 2.4254776226576937

Epoch: 6| Step: 13
Training loss: 3.198007345199585
Validation loss: 2.4253025875296643

Epoch: 100| Step: 0
Training loss: 2.2618188858032227
Validation loss: 2.4229931523722987

Epoch: 6| Step: 1
Training loss: 2.372065544128418
Validation loss: 2.425147674416983

Epoch: 6| Step: 2
Training loss: 3.06428599357605
Validation loss: 2.427587245100288

Epoch: 6| Step: 3
Training loss: 2.738546371459961
Validation loss: 2.4260615379579606

Epoch: 6| Step: 4
Training loss: 2.445835590362549
Validation loss: 2.421357126646144

Epoch: 6| Step: 5
Training loss: 2.168368339538574
Validation loss: 2.4242977788371425

Epoch: 6| Step: 6
Training loss: 1.8475581407546997
Validation loss: 2.42570762865005

Epoch: 6| Step: 7
Training loss: 2.8590760231018066
Validation loss: 2.423724707736764

Epoch: 6| Step: 8
Training loss: 2.82420015335083
Validation loss: 2.4218828370494228

Epoch: 6| Step: 9
Training loss: 2.998046636581421
Validation loss: 2.4309244181520198

Epoch: 6| Step: 10
Training loss: 2.8194515705108643
Validation loss: 2.4364637636369273

Epoch: 6| Step: 11
Training loss: 2.4963326454162598
Validation loss: 2.4422599166952152

Epoch: 6| Step: 12
Training loss: 3.476245880126953
Validation loss: 2.452095103520219

Epoch: 6| Step: 13
Training loss: 2.602315664291382
Validation loss: 2.4523681338115404

Epoch: 101| Step: 0
Training loss: 3.022326946258545
Validation loss: 2.456003814615229

Epoch: 6| Step: 1
Training loss: 3.099493980407715
Validation loss: 2.4551353710953907

Epoch: 6| Step: 2
Training loss: 2.804388999938965
Validation loss: 2.448244761395198

Epoch: 6| Step: 3
Training loss: 2.819511890411377
Validation loss: 2.4389442884793846

Epoch: 6| Step: 4
Training loss: 2.5452494621276855
Validation loss: 2.429916948400518

Epoch: 6| Step: 5
Training loss: 3.2075862884521484
Validation loss: 2.42041439138433

Epoch: 6| Step: 6
Training loss: 2.193099021911621
Validation loss: 2.4247501255363546

Epoch: 6| Step: 7
Training loss: 2.741138458251953
Validation loss: 2.413514970451273

Epoch: 6| Step: 8
Training loss: 1.8173770904541016
Validation loss: 2.417155037644089

Epoch: 6| Step: 9
Training loss: 2.7519094944000244
Validation loss: 2.412864967059064

Epoch: 6| Step: 10
Training loss: 2.097428798675537
Validation loss: 2.4140271038137455

Epoch: 6| Step: 11
Training loss: 2.7049927711486816
Validation loss: 2.415120881090882

Epoch: 6| Step: 12
Training loss: 2.8906054496765137
Validation loss: 2.42334456341241

Epoch: 6| Step: 13
Training loss: 2.10443115234375
Validation loss: 2.4203167448761644

Epoch: 102| Step: 0
Training loss: 2.364262819290161
Validation loss: 2.426834196172735

Epoch: 6| Step: 1
Training loss: 2.9477837085723877
Validation loss: 2.4250587673597437

Epoch: 6| Step: 2
Training loss: 3.3089113235473633
Validation loss: 2.4247340668914137

Epoch: 6| Step: 3
Training loss: 2.734919548034668
Validation loss: 2.4273495725406113

Epoch: 6| Step: 4
Training loss: 3.140748977661133
Validation loss: 2.425208845446187

Epoch: 6| Step: 5
Training loss: 2.561269760131836
Validation loss: 2.419853730868268

Epoch: 6| Step: 6
Training loss: 2.0011849403381348
Validation loss: 2.41791965628183

Epoch: 6| Step: 7
Training loss: 3.1348018646240234
Validation loss: 2.4247149523868354

Epoch: 6| Step: 8
Training loss: 2.97591495513916
Validation loss: 2.431475413742886

Epoch: 6| Step: 9
Training loss: 2.954674482345581
Validation loss: 2.426731477501572

Epoch: 6| Step: 10
Training loss: 1.919521450996399
Validation loss: 2.4251395886944187

Epoch: 6| Step: 11
Training loss: 2.298159599304199
Validation loss: 2.425243572522235

Epoch: 6| Step: 12
Training loss: 2.1471176147460938
Validation loss: 2.4234026273091636

Epoch: 6| Step: 13
Training loss: 2.464588165283203
Validation loss: 2.4246184569533153

Epoch: 103| Step: 0
Training loss: 2.497490167617798
Validation loss: 2.428113954041594

Epoch: 6| Step: 1
Training loss: 2.487534999847412
Validation loss: 2.4291728183787358

Epoch: 6| Step: 2
Training loss: 2.945211887359619
Validation loss: 2.4368752817953787

Epoch: 6| Step: 3
Training loss: 2.7482457160949707
Validation loss: 2.4358287190878265

Epoch: 6| Step: 4
Training loss: 2.950312614440918
Validation loss: 2.438738671682214

Epoch: 6| Step: 5
Training loss: 3.2068724632263184
Validation loss: 2.4313015322531424

Epoch: 6| Step: 6
Training loss: 3.0371310710906982
Validation loss: 2.440723975499471

Epoch: 6| Step: 7
Training loss: 1.9970483779907227
Validation loss: 2.427896304797101

Epoch: 6| Step: 8
Training loss: 2.4676456451416016
Validation loss: 2.420245034720308

Epoch: 6| Step: 9
Training loss: 2.576155662536621
Validation loss: 2.417527324409895

Epoch: 6| Step: 10
Training loss: 2.826209306716919
Validation loss: 2.4163147839166785

Epoch: 6| Step: 11
Training loss: 1.7825238704681396
Validation loss: 2.4166962562068814

Epoch: 6| Step: 12
Training loss: 2.5052876472473145
Validation loss: 2.417318090315788

Epoch: 6| Step: 13
Training loss: 2.9526498317718506
Validation loss: 2.4316970558576685

Epoch: 104| Step: 0
Training loss: 2.2957892417907715
Validation loss: 2.428913431782876

Epoch: 6| Step: 1
Training loss: 2.470844268798828
Validation loss: 2.4307191666736396

Epoch: 6| Step: 2
Training loss: 2.9092607498168945
Validation loss: 2.4379364418727096

Epoch: 6| Step: 3
Training loss: 3.001641273498535
Validation loss: 2.4313566146358365

Epoch: 6| Step: 4
Training loss: 2.8125455379486084
Validation loss: 2.4280770427437237

Epoch: 6| Step: 5
Training loss: 2.627887725830078
Validation loss: 2.4338493295895156

Epoch: 6| Step: 6
Training loss: 2.4172868728637695
Validation loss: 2.427339164159631

Epoch: 6| Step: 7
Training loss: 3.105109691619873
Validation loss: 2.4298573104284142

Epoch: 6| Step: 8
Training loss: 2.486682891845703
Validation loss: 2.432725050116098

Epoch: 6| Step: 9
Training loss: 1.874590516090393
Validation loss: 2.424439744282794

Epoch: 6| Step: 10
Training loss: 2.855473756790161
Validation loss: 2.4243513076536116

Epoch: 6| Step: 11
Training loss: 2.7258729934692383
Validation loss: 2.4213752797854844

Epoch: 6| Step: 12
Training loss: 2.990445375442505
Validation loss: 2.4222919979403095

Epoch: 6| Step: 13
Training loss: 2.139263868331909
Validation loss: 2.4239084733429777

Epoch: 105| Step: 0
Training loss: 3.0014142990112305
Validation loss: 2.418370008468628

Epoch: 6| Step: 1
Training loss: 2.771493434906006
Validation loss: 2.420431144775883

Epoch: 6| Step: 2
Training loss: 2.1445484161376953
Validation loss: 2.4193126155484106

Epoch: 6| Step: 3
Training loss: 3.121328830718994
Validation loss: 2.418603079293364

Epoch: 6| Step: 4
Training loss: 1.9679352045059204
Validation loss: 2.4264324121577765

Epoch: 6| Step: 5
Training loss: 2.7604706287384033
Validation loss: 2.4180198587397093

Epoch: 6| Step: 6
Training loss: 1.7446953058242798
Validation loss: 2.426443774213073

Epoch: 6| Step: 7
Training loss: 2.3982365131378174
Validation loss: 2.429804032848727

Epoch: 6| Step: 8
Training loss: 3.1655120849609375
Validation loss: 2.4233905423072075

Epoch: 6| Step: 9
Training loss: 2.636044979095459
Validation loss: 2.4280506949270926

Epoch: 6| Step: 10
Training loss: 3.0340657234191895
Validation loss: 2.4185758252297678

Epoch: 6| Step: 11
Training loss: 2.8894705772399902
Validation loss: 2.4231511290355394

Epoch: 6| Step: 12
Training loss: 2.5712287425994873
Validation loss: 2.409835917975313

Epoch: 6| Step: 13
Training loss: 2.5859200954437256
Validation loss: 2.400246799633067

Epoch: 106| Step: 0
Training loss: 2.1713948249816895
Validation loss: 2.3951597598291214

Epoch: 6| Step: 1
Training loss: 3.241328239440918
Validation loss: 2.3899757631363405

Epoch: 6| Step: 2
Training loss: 2.7079427242279053
Validation loss: 2.3823192427235265

Epoch: 6| Step: 3
Training loss: 2.3053343296051025
Validation loss: 2.3754904013808056

Epoch: 6| Step: 4
Training loss: 3.01523756980896
Validation loss: 2.370003451583206

Epoch: 6| Step: 5
Training loss: 2.6635966300964355
Validation loss: 2.367748975753784

Epoch: 6| Step: 6
Training loss: 2.7328107357025146
Validation loss: 2.371377516818303

Epoch: 6| Step: 7
Training loss: 2.440751552581787
Validation loss: 2.37065113488064

Epoch: 6| Step: 8
Training loss: 2.5352883338928223
Validation loss: 2.3722204175046695

Epoch: 6| Step: 9
Training loss: 1.5025684833526611
Validation loss: 2.3725031755303823

Epoch: 6| Step: 10
Training loss: 2.4458112716674805
Validation loss: 2.3703722876887166

Epoch: 6| Step: 11
Training loss: 3.218637466430664
Validation loss: 2.3775671528231714

Epoch: 6| Step: 12
Training loss: 3.319110155105591
Validation loss: 2.379057207415181

Epoch: 6| Step: 13
Training loss: 2.27809739112854
Validation loss: 2.3810933148989113

Epoch: 107| Step: 0
Training loss: 3.187875747680664
Validation loss: 2.400531771362469

Epoch: 6| Step: 1
Training loss: 2.778918504714966
Validation loss: 2.3961474254567134

Epoch: 6| Step: 2
Training loss: 2.7030556201934814
Validation loss: 2.4007495731435795

Epoch: 6| Step: 3
Training loss: 2.0585670471191406
Validation loss: 2.4000040356830885

Epoch: 6| Step: 4
Training loss: 2.6557345390319824
Validation loss: 2.400922526595413

Epoch: 6| Step: 5
Training loss: 2.808873176574707
Validation loss: 2.4025352411372687

Epoch: 6| Step: 6
Training loss: 2.055844783782959
Validation loss: 2.387417133136462

Epoch: 6| Step: 7
Training loss: 1.934596061706543
Validation loss: 2.3823047760994203

Epoch: 6| Step: 8
Training loss: 2.7970893383026123
Validation loss: 2.3766426745281426

Epoch: 6| Step: 9
Training loss: 2.4575841426849365
Validation loss: 2.376754588978265

Epoch: 6| Step: 10
Training loss: 3.134249687194824
Validation loss: 2.3780647529068815

Epoch: 6| Step: 11
Training loss: 2.968647003173828
Validation loss: 2.3801633696402273

Epoch: 6| Step: 12
Training loss: 2.5839109420776367
Validation loss: 2.378313320939259

Epoch: 6| Step: 13
Training loss: 2.717034339904785
Validation loss: 2.38568433894906

Epoch: 108| Step: 0
Training loss: 2.705648899078369
Validation loss: 2.389454674977128

Epoch: 6| Step: 1
Training loss: 2.3188350200653076
Validation loss: 2.3880244352484263

Epoch: 6| Step: 2
Training loss: 2.6588525772094727
Validation loss: 2.3962924121528544

Epoch: 6| Step: 3
Training loss: 2.81365704536438
Validation loss: 2.400106632581321

Epoch: 6| Step: 4
Training loss: 2.628326892852783
Validation loss: 2.4023037084969143

Epoch: 6| Step: 5
Training loss: 3.2746100425720215
Validation loss: 2.403644061857654

Epoch: 6| Step: 6
Training loss: 2.336455821990967
Validation loss: 2.40174929300944

Epoch: 6| Step: 7
Training loss: 2.5917980670928955
Validation loss: 2.4082171660597607

Epoch: 6| Step: 8
Training loss: 2.636380672454834
Validation loss: 2.398585514355731

Epoch: 6| Step: 9
Training loss: 2.7116665840148926
Validation loss: 2.386490898747598

Epoch: 6| Step: 10
Training loss: 2.8677873611450195
Validation loss: 2.3819780990641606

Epoch: 6| Step: 11
Training loss: 2.4934089183807373
Validation loss: 2.3759333190097602

Epoch: 6| Step: 12
Training loss: 2.6006693840026855
Validation loss: 2.3748964648092947

Epoch: 6| Step: 13
Training loss: 1.5182636976242065
Validation loss: 2.380509030434393

Epoch: 109| Step: 0
Training loss: 2.710937738418579
Validation loss: 2.3711296871144283

Epoch: 6| Step: 1
Training loss: 1.775606393814087
Validation loss: 2.377074603111513

Epoch: 6| Step: 2
Training loss: 2.6634016036987305
Validation loss: 2.368383679338681

Epoch: 6| Step: 3
Training loss: 2.950648307800293
Validation loss: 2.374584082634218

Epoch: 6| Step: 4
Training loss: 1.8211227655410767
Validation loss: 2.3723031192697506

Epoch: 6| Step: 5
Training loss: 2.848865032196045
Validation loss: 2.372273434874832

Epoch: 6| Step: 6
Training loss: 2.810560703277588
Validation loss: 2.371398961672219

Epoch: 6| Step: 7
Training loss: 2.495619773864746
Validation loss: 2.365330643551324

Epoch: 6| Step: 8
Training loss: 3.1503405570983887
Validation loss: 2.374372095190069

Epoch: 6| Step: 9
Training loss: 2.8900060653686523
Validation loss: 2.3631522194031747

Epoch: 6| Step: 10
Training loss: 2.3987984657287598
Validation loss: 2.3770268399228334

Epoch: 6| Step: 11
Training loss: 2.7801926136016846
Validation loss: 2.3767677173819592

Epoch: 6| Step: 12
Training loss: 2.6330454349517822
Validation loss: 2.3739228774142522

Epoch: 6| Step: 13
Training loss: 2.7103474140167236
Validation loss: 2.3625292701105916

Epoch: 110| Step: 0
Training loss: 3.1845736503601074
Validation loss: 2.3617584833534817

Epoch: 6| Step: 1
Training loss: 2.8534586429595947
Validation loss: 2.362267648020098

Epoch: 6| Step: 2
Training loss: 2.738023281097412
Validation loss: 2.3601504397648636

Epoch: 6| Step: 3
Training loss: 2.497817039489746
Validation loss: 2.362045157340265

Epoch: 6| Step: 4
Training loss: 2.8396897315979004
Validation loss: 2.354842519247404

Epoch: 6| Step: 5
Training loss: 2.305875778198242
Validation loss: 2.360719060385099

Epoch: 6| Step: 6
Training loss: 2.734241008758545
Validation loss: 2.3667587849401657

Epoch: 6| Step: 7
Training loss: 2.11506986618042
Validation loss: 2.360610874750281

Epoch: 6| Step: 8
Training loss: 2.9938266277313232
Validation loss: 2.3605019097687094

Epoch: 6| Step: 9
Training loss: 2.789425849914551
Validation loss: 2.3578816126751643

Epoch: 6| Step: 10
Training loss: 2.633610248565674
Validation loss: 2.35283649352289

Epoch: 6| Step: 11
Training loss: 2.8486952781677246
Validation loss: 2.3553057973102858

Epoch: 6| Step: 12
Training loss: 1.9990147352218628
Validation loss: 2.355596734631446

Epoch: 6| Step: 13
Training loss: 1.554874062538147
Validation loss: 2.355649645610522

Epoch: 111| Step: 0
Training loss: 2.4316868782043457
Validation loss: 2.3636848644543718

Epoch: 6| Step: 1
Training loss: 2.664565086364746
Validation loss: 2.3619824429993987

Epoch: 6| Step: 2
Training loss: 2.343212127685547
Validation loss: 2.380294458840483

Epoch: 6| Step: 3
Training loss: 2.4392542839050293
Validation loss: 2.3858970006306968

Epoch: 6| Step: 4
Training loss: 2.7877440452575684
Validation loss: 2.386877149663946

Epoch: 6| Step: 5
Training loss: 2.7881176471710205
Validation loss: 2.395464909973965

Epoch: 6| Step: 6
Training loss: 3.396604537963867
Validation loss: 2.3919178798634517

Epoch: 6| Step: 7
Training loss: 2.7645456790924072
Validation loss: 2.3847143496236494

Epoch: 6| Step: 8
Training loss: 2.0052714347839355
Validation loss: 2.3822905684030182

Epoch: 6| Step: 9
Training loss: 2.6759700775146484
Validation loss: 2.3851615690415904

Epoch: 6| Step: 10
Training loss: 2.3059537410736084
Validation loss: 2.3779707544593403

Epoch: 6| Step: 11
Training loss: 2.1355152130126953
Validation loss: 2.380334097851989

Epoch: 6| Step: 12
Training loss: 2.6783769130706787
Validation loss: 2.374976586270076

Epoch: 6| Step: 13
Training loss: 3.4382596015930176
Validation loss: 2.3667973036407144

Epoch: 112| Step: 0
Training loss: 3.1918959617614746
Validation loss: 2.366201062356272

Epoch: 6| Step: 1
Training loss: 2.82513165473938
Validation loss: 2.365833409370915

Epoch: 6| Step: 2
Training loss: 2.7400827407836914
Validation loss: 2.3673169574429913

Epoch: 6| Step: 3
Training loss: 2.138726234436035
Validation loss: 2.3557503941238567

Epoch: 6| Step: 4
Training loss: 2.5610456466674805
Validation loss: 2.365394489739531

Epoch: 6| Step: 5
Training loss: 2.33669376373291
Validation loss: 2.3604307930956603

Epoch: 6| Step: 6
Training loss: 2.9826714992523193
Validation loss: 2.3628539910880466

Epoch: 6| Step: 7
Training loss: 3.1954140663146973
Validation loss: 2.364812348478584

Epoch: 6| Step: 8
Training loss: 3.150683879852295
Validation loss: 2.3577256048879316

Epoch: 6| Step: 9
Training loss: 2.0616278648376465
Validation loss: 2.3532583944259153

Epoch: 6| Step: 10
Training loss: 3.276665449142456
Validation loss: 2.3577979713357906

Epoch: 6| Step: 11
Training loss: 2.331691265106201
Validation loss: 2.3553384709101852

Epoch: 6| Step: 12
Training loss: 1.5743484497070312
Validation loss: 2.3639788935261388

Epoch: 6| Step: 13
Training loss: 1.6535230875015259
Validation loss: 2.3719776881638395

Epoch: 113| Step: 0
Training loss: 2.5774121284484863
Validation loss: 2.3746296000737015

Epoch: 6| Step: 1
Training loss: 2.1546432971954346
Validation loss: 2.3776873926962576

Epoch: 6| Step: 2
Training loss: 3.114954948425293
Validation loss: 2.371993967281875

Epoch: 6| Step: 3
Training loss: 3.7424073219299316
Validation loss: 2.361003306604201

Epoch: 6| Step: 4
Training loss: 2.3215179443359375
Validation loss: 2.34965274154499

Epoch: 6| Step: 5
Training loss: 2.136770248413086
Validation loss: 2.354300686108169

Epoch: 6| Step: 6
Training loss: 3.0775599479675293
Validation loss: 2.355380447961951

Epoch: 6| Step: 7
Training loss: 1.9145715236663818
Validation loss: 2.3591163517326437

Epoch: 6| Step: 8
Training loss: 1.9656245708465576
Validation loss: 2.3552416921943746

Epoch: 6| Step: 9
Training loss: 2.8802149295806885
Validation loss: 2.3553858444254887

Epoch: 6| Step: 10
Training loss: 3.0103647708892822
Validation loss: 2.3557997339515278

Epoch: 6| Step: 11
Training loss: 2.534574508666992
Validation loss: 2.3561628659566245

Epoch: 6| Step: 12
Training loss: 2.3913257122039795
Validation loss: 2.359107143135481

Epoch: 6| Step: 13
Training loss: 2.637493133544922
Validation loss: 2.3647734554865028

Epoch: 114| Step: 0
Training loss: 2.2575554847717285
Validation loss: 2.374698290260889

Epoch: 6| Step: 1
Training loss: 2.4009289741516113
Validation loss: 2.3794679923724105

Epoch: 6| Step: 2
Training loss: 2.582667827606201
Validation loss: 2.3856070092929307

Epoch: 6| Step: 3
Training loss: 2.803269386291504
Validation loss: 2.397083564471173

Epoch: 6| Step: 4
Training loss: 3.0336225032806396
Validation loss: 2.3903681180810414

Epoch: 6| Step: 5
Training loss: 3.431281566619873
Validation loss: 2.389137501357704

Epoch: 6| Step: 6
Training loss: 1.9094144105911255
Validation loss: 2.3711981132466304

Epoch: 6| Step: 7
Training loss: 2.504826545715332
Validation loss: 2.360606890852733

Epoch: 6| Step: 8
Training loss: 2.029707908630371
Validation loss: 2.3538351840870355

Epoch: 6| Step: 9
Training loss: 2.9145991802215576
Validation loss: 2.355836163284958

Epoch: 6| Step: 10
Training loss: 2.3027515411376953
Validation loss: 2.3493712794396187

Epoch: 6| Step: 11
Training loss: 2.5966854095458984
Validation loss: 2.3525327431258334

Epoch: 6| Step: 12
Training loss: 2.6233460903167725
Validation loss: 2.3515931124328286

Epoch: 6| Step: 13
Training loss: 3.4351720809936523
Validation loss: 2.350777890092583

Epoch: 115| Step: 0
Training loss: 2.579692840576172
Validation loss: 2.3471908723154375

Epoch: 6| Step: 1
Training loss: 2.680431365966797
Validation loss: 2.344850714488696

Epoch: 6| Step: 2
Training loss: 2.107621192932129
Validation loss: 2.344324440084478

Epoch: 6| Step: 3
Training loss: 2.3771607875823975
Validation loss: 2.3554167055314585

Epoch: 6| Step: 4
Training loss: 2.5921669006347656
Validation loss: 2.3593055138023953

Epoch: 6| Step: 5
Training loss: 2.7902116775512695
Validation loss: 2.370566914158483

Epoch: 6| Step: 6
Training loss: 2.748102903366089
Validation loss: 2.3733449879512993

Epoch: 6| Step: 7
Training loss: 2.9538216590881348
Validation loss: 2.381618651010657

Epoch: 6| Step: 8
Training loss: 1.7699004411697388
Validation loss: 2.3721489906311035

Epoch: 6| Step: 9
Training loss: 2.186004400253296
Validation loss: 2.3731236662915958

Epoch: 6| Step: 10
Training loss: 2.8404006958007812
Validation loss: 2.3734410552568335

Epoch: 6| Step: 11
Training loss: 2.7665791511535645
Validation loss: 2.3689326650352887

Epoch: 6| Step: 12
Training loss: 2.73344087600708
Validation loss: 2.3663818708030124

Epoch: 6| Step: 13
Training loss: 3.7335848808288574
Validation loss: 2.3602414464437835

Epoch: 116| Step: 0
Training loss: 2.774808883666992
Validation loss: 2.35747544739836

Epoch: 6| Step: 1
Training loss: 2.3541924953460693
Validation loss: 2.3641919474447928

Epoch: 6| Step: 2
Training loss: 2.2458455562591553
Validation loss: 2.3557810962841077

Epoch: 6| Step: 3
Training loss: 2.400578022003174
Validation loss: 2.355438279849227

Epoch: 6| Step: 4
Training loss: 2.604961633682251
Validation loss: 2.3490207272191204

Epoch: 6| Step: 5
Training loss: 2.666990041732788
Validation loss: 2.3517863314638854

Epoch: 6| Step: 6
Training loss: 2.2144126892089844
Validation loss: 2.3565152434892553

Epoch: 6| Step: 7
Training loss: 2.398763418197632
Validation loss: 2.3619632310764764

Epoch: 6| Step: 8
Training loss: 3.0473217964172363
Validation loss: 2.369728822861948

Epoch: 6| Step: 9
Training loss: 3.01114559173584
Validation loss: 2.366569488279281

Epoch: 6| Step: 10
Training loss: 3.2006924152374268
Validation loss: 2.365483368596723

Epoch: 6| Step: 11
Training loss: 2.6180315017700195
Validation loss: 2.356183280227005

Epoch: 6| Step: 12
Training loss: 2.177971363067627
Validation loss: 2.3657653613757064

Epoch: 6| Step: 13
Training loss: 2.5175774097442627
Validation loss: 2.3578556147954797

Epoch: 117| Step: 0
Training loss: 2.259690046310425
Validation loss: 2.35050626467633

Epoch: 6| Step: 1
Training loss: 3.5543336868286133
Validation loss: 2.3445476101290796

Epoch: 6| Step: 2
Training loss: 2.987656831741333
Validation loss: 2.3432557787946475

Epoch: 6| Step: 3
Training loss: 2.3517746925354004
Validation loss: 2.344093963664065

Epoch: 6| Step: 4
Training loss: 3.875230073928833
Validation loss: 2.3467476701223724

Epoch: 6| Step: 5
Training loss: 2.880164623260498
Validation loss: 2.3460206113835818

Epoch: 6| Step: 6
Training loss: 1.935886263847351
Validation loss: 2.3436343451981902

Epoch: 6| Step: 7
Training loss: 2.2119290828704834
Validation loss: 2.3448920865212717

Epoch: 6| Step: 8
Training loss: 2.0551257133483887
Validation loss: 2.3367758463787776

Epoch: 6| Step: 9
Training loss: 2.2317681312561035
Validation loss: 2.33696218972565

Epoch: 6| Step: 10
Training loss: 2.680967330932617
Validation loss: 2.3350438482017926

Epoch: 6| Step: 11
Training loss: 2.7085390090942383
Validation loss: 2.3389262589075233

Epoch: 6| Step: 12
Training loss: 2.4289934635162354
Validation loss: 2.3482511274276243

Epoch: 6| Step: 13
Training loss: 1.7679568529129028
Validation loss: 2.350940404399749

Epoch: 118| Step: 0
Training loss: 2.4151926040649414
Validation loss: 2.352313018614246

Epoch: 6| Step: 1
Training loss: 2.8017053604125977
Validation loss: 2.357377293289349

Epoch: 6| Step: 2
Training loss: 2.4483513832092285
Validation loss: 2.356187753779914

Epoch: 6| Step: 3
Training loss: 1.9073989391326904
Validation loss: 2.362873269665626

Epoch: 6| Step: 4
Training loss: 2.75193190574646
Validation loss: 2.3664764768333844

Epoch: 6| Step: 5
Training loss: 2.031782865524292
Validation loss: 2.369958859617992

Epoch: 6| Step: 6
Training loss: 3.1269006729125977
Validation loss: 2.3623437420014413

Epoch: 6| Step: 7
Training loss: 3.168618679046631
Validation loss: 2.3561871872153333

Epoch: 6| Step: 8
Training loss: 2.1586849689483643
Validation loss: 2.356701691945394

Epoch: 6| Step: 9
Training loss: 2.9279286861419678
Validation loss: 2.3555121690996232

Epoch: 6| Step: 10
Training loss: 2.557723045349121
Validation loss: 2.3530656406956334

Epoch: 6| Step: 11
Training loss: 2.7745683193206787
Validation loss: 2.353520603590114

Epoch: 6| Step: 12
Training loss: 2.4469127655029297
Validation loss: 2.3609276266508203

Epoch: 6| Step: 13
Training loss: 2.707631826400757
Validation loss: 2.3542476905289518

Epoch: 119| Step: 0
Training loss: 2.610757350921631
Validation loss: 2.3534454427739626

Epoch: 6| Step: 1
Training loss: 2.378474712371826
Validation loss: 2.366130059765231

Epoch: 6| Step: 2
Training loss: 2.6886913776397705
Validation loss: 2.376139243443807

Epoch: 6| Step: 3
Training loss: 3.462857723236084
Validation loss: 2.3926992775291525

Epoch: 6| Step: 4
Training loss: 2.3912782669067383
Validation loss: 2.3935536774255897

Epoch: 6| Step: 5
Training loss: 2.713991641998291
Validation loss: 2.3914043185531453

Epoch: 6| Step: 6
Training loss: 2.079073905944824
Validation loss: 2.3752649368778354

Epoch: 6| Step: 7
Training loss: 2.661529064178467
Validation loss: 2.364945129681659

Epoch: 6| Step: 8
Training loss: 2.834472417831421
Validation loss: 2.360145268901702

Epoch: 6| Step: 9
Training loss: 2.8404247760772705
Validation loss: 2.3526194198157198

Epoch: 6| Step: 10
Training loss: 2.9359424114227295
Validation loss: 2.3497868173865863

Epoch: 6| Step: 11
Training loss: 2.766331672668457
Validation loss: 2.3382478939589633

Epoch: 6| Step: 12
Training loss: 1.9325520992279053
Validation loss: 2.329697614075035

Epoch: 6| Step: 13
Training loss: 1.4247190952301025
Validation loss: 2.3331405860121532

Epoch: 120| Step: 0
Training loss: 2.4782841205596924
Validation loss: 2.3272919731755413

Epoch: 6| Step: 1
Training loss: 2.3072726726531982
Validation loss: 2.326234059949075

Epoch: 6| Step: 2
Training loss: 3.575744390487671
Validation loss: 2.3266631992914344

Epoch: 6| Step: 3
Training loss: 2.928159713745117
Validation loss: 2.3272614453428533

Epoch: 6| Step: 4
Training loss: 2.8859000205993652
Validation loss: 2.333244721094767

Epoch: 6| Step: 5
Training loss: 2.6991372108459473
Validation loss: 2.33072708242683

Epoch: 6| Step: 6
Training loss: 2.63676381111145
Validation loss: 2.3386888683483167

Epoch: 6| Step: 7
Training loss: 2.512110948562622
Validation loss: 2.3403143934024278

Epoch: 6| Step: 8
Training loss: 2.422027111053467
Validation loss: 2.33769331952577

Epoch: 6| Step: 9
Training loss: 1.8161330223083496
Validation loss: 2.335367871868995

Epoch: 6| Step: 10
Training loss: 2.5528793334960938
Validation loss: 2.328987508691767

Epoch: 6| Step: 11
Training loss: 2.6300206184387207
Validation loss: 2.3237326939900718

Epoch: 6| Step: 12
Training loss: 2.316589832305908
Validation loss: 2.324220226657006

Epoch: 6| Step: 13
Training loss: 2.369422435760498
Validation loss: 2.326903881565217

Epoch: 121| Step: 0
Training loss: 2.569406509399414
Validation loss: 2.328564323404784

Epoch: 6| Step: 1
Training loss: 2.6124167442321777
Validation loss: 2.3311184349880425

Epoch: 6| Step: 2
Training loss: 2.0980403423309326
Validation loss: 2.3321567145727014

Epoch: 6| Step: 3
Training loss: 2.89643931388855
Validation loss: 2.3293862394107285

Epoch: 6| Step: 4
Training loss: 2.6326749324798584
Validation loss: 2.3378546417400403

Epoch: 6| Step: 5
Training loss: 3.1356570720672607
Validation loss: 2.3379315125044955

Epoch: 6| Step: 6
Training loss: 2.820465564727783
Validation loss: 2.3413992722829184

Epoch: 6| Step: 7
Training loss: 2.498854160308838
Validation loss: 2.34123900628859

Epoch: 6| Step: 8
Training loss: 2.723635196685791
Validation loss: 2.3581844491343342

Epoch: 6| Step: 9
Training loss: 1.9613269567489624
Validation loss: 2.3574582274242113

Epoch: 6| Step: 10
Training loss: 2.8569791316986084
Validation loss: 2.3552738799843738

Epoch: 6| Step: 11
Training loss: 2.4807426929473877
Validation loss: 2.3566515996891964

Epoch: 6| Step: 12
Training loss: 2.445563793182373
Validation loss: 2.3564351374103176

Epoch: 6| Step: 13
Training loss: 2.253018856048584
Validation loss: 2.3512429729584725

Epoch: 122| Step: 0
Training loss: 2.7297511100769043
Validation loss: 2.3595025770125853

Epoch: 6| Step: 1
Training loss: 2.463975429534912
Validation loss: 2.354383901883197

Epoch: 6| Step: 2
Training loss: 3.066129207611084
Validation loss: 2.348919448032174

Epoch: 6| Step: 3
Training loss: 2.0224251747131348
Validation loss: 2.347778526685571

Epoch: 6| Step: 4
Training loss: 2.6111950874328613
Validation loss: 2.3452191583571897

Epoch: 6| Step: 5
Training loss: 2.7782034873962402
Validation loss: 2.3420821415480746

Epoch: 6| Step: 6
Training loss: 2.9002976417541504
Validation loss: 2.347356573227913

Epoch: 6| Step: 7
Training loss: 2.0381059646606445
Validation loss: 2.340635702174197

Epoch: 6| Step: 8
Training loss: 2.466843366622925
Validation loss: 2.330425544451642

Epoch: 6| Step: 9
Training loss: 2.616515874862671
Validation loss: 2.333484531730734

Epoch: 6| Step: 10
Training loss: 2.3873562812805176
Validation loss: 2.3312809313497236

Epoch: 6| Step: 11
Training loss: 2.6202316284179688
Validation loss: 2.329245872395013

Epoch: 6| Step: 12
Training loss: 3.0944766998291016
Validation loss: 2.33281700329114

Epoch: 6| Step: 13
Training loss: 1.9047560691833496
Validation loss: 2.3350097466540594

Epoch: 123| Step: 0
Training loss: 2.3020570278167725
Validation loss: 2.332198022514261

Epoch: 6| Step: 1
Training loss: 2.271605968475342
Validation loss: 2.343013655754828

Epoch: 6| Step: 2
Training loss: 2.6704068183898926
Validation loss: 2.350789552093834

Epoch: 6| Step: 3
Training loss: 2.9220261573791504
Validation loss: 2.3564833851270777

Epoch: 6| Step: 4
Training loss: 2.9461264610290527
Validation loss: 2.3612024168814383

Epoch: 6| Step: 5
Training loss: 2.695733070373535
Validation loss: 2.3752637909304712

Epoch: 6| Step: 6
Training loss: 2.5911600589752197
Validation loss: 2.378255731316023

Epoch: 6| Step: 7
Training loss: 2.740983009338379
Validation loss: 2.3675463071433445

Epoch: 6| Step: 8
Training loss: 2.0134291648864746
Validation loss: 2.363049145667784

Epoch: 6| Step: 9
Training loss: 1.7898536920547485
Validation loss: 2.3569485551567486

Epoch: 6| Step: 10
Training loss: 2.032895088195801
Validation loss: 2.3569445558773574

Epoch: 6| Step: 11
Training loss: 3.0916895866394043
Validation loss: 2.3590952170792447

Epoch: 6| Step: 12
Training loss: 3.1042749881744385
Validation loss: 2.349069268472733

Epoch: 6| Step: 13
Training loss: 2.9499502182006836
Validation loss: 2.351446656770604

Epoch: 124| Step: 0
Training loss: 2.602449893951416
Validation loss: 2.350148816262522

Epoch: 6| Step: 1
Training loss: 1.9829002618789673
Validation loss: 2.3371285187300814

Epoch: 6| Step: 2
Training loss: 2.1699020862579346
Validation loss: 2.335318460259386

Epoch: 6| Step: 3
Training loss: 2.116973876953125
Validation loss: 2.334838608259796

Epoch: 6| Step: 4
Training loss: 1.8773155212402344
Validation loss: 2.3216656382365892

Epoch: 6| Step: 5
Training loss: 3.186178207397461
Validation loss: 2.3215106174510014

Epoch: 6| Step: 6
Training loss: 2.7553892135620117
Validation loss: 2.317705851729198

Epoch: 6| Step: 7
Training loss: 3.2476072311401367
Validation loss: 2.3222337384377756

Epoch: 6| Step: 8
Training loss: 2.746830940246582
Validation loss: 2.319156498037359

Epoch: 6| Step: 9
Training loss: 2.7250020503997803
Validation loss: 2.3290982656581427

Epoch: 6| Step: 10
Training loss: 2.3686423301696777
Validation loss: 2.322184275555354

Epoch: 6| Step: 11
Training loss: 2.82047176361084
Validation loss: 2.320169198897577

Epoch: 6| Step: 12
Training loss: 2.6979053020477295
Validation loss: 2.3168299557060323

Epoch: 6| Step: 13
Training loss: 2.6783714294433594
Validation loss: 2.324840990445947

Epoch: 125| Step: 0
Training loss: 2.5240750312805176
Validation loss: 2.320857455653529

Epoch: 6| Step: 1
Training loss: 2.5666747093200684
Validation loss: 2.3533221085866294

Epoch: 6| Step: 2
Training loss: 2.88523530960083
Validation loss: 2.372154284549016

Epoch: 6| Step: 3
Training loss: 1.943026065826416
Validation loss: 2.365879971493957

Epoch: 6| Step: 4
Training loss: 3.0456972122192383
Validation loss: 2.3780563416019564

Epoch: 6| Step: 5
Training loss: 2.7110748291015625
Validation loss: 2.3484302900170766

Epoch: 6| Step: 6
Training loss: 2.556527853012085
Validation loss: 2.339780353730725

Epoch: 6| Step: 7
Training loss: 3.114408254623413
Validation loss: 2.3185552627809587

Epoch: 6| Step: 8
Training loss: 2.666555881500244
Validation loss: 2.318199642242924

Epoch: 6| Step: 9
Training loss: 2.4674813747406006
Validation loss: 2.30974216358636

Epoch: 6| Step: 10
Training loss: 2.0921499729156494
Validation loss: 2.30441104724843

Epoch: 6| Step: 11
Training loss: 2.54722261428833
Validation loss: 2.304068834550919

Epoch: 6| Step: 12
Training loss: 2.673128128051758
Validation loss: 2.305328563977313

Epoch: 6| Step: 13
Training loss: 2.16572904586792
Validation loss: 2.3034229688746954

Epoch: 126| Step: 0
Training loss: 2.5789310932159424
Validation loss: 2.300025222122028

Epoch: 6| Step: 1
Training loss: 2.1496129035949707
Validation loss: 2.3070015215104624

Epoch: 6| Step: 2
Training loss: 2.478024959564209
Validation loss: 2.3035045618652017

Epoch: 6| Step: 3
Training loss: 3.122467041015625
Validation loss: 2.3086683545061337

Epoch: 6| Step: 4
Training loss: 2.7761754989624023
Validation loss: 2.2999918563391573

Epoch: 6| Step: 5
Training loss: 2.6004183292388916
Validation loss: 2.303961402626448

Epoch: 6| Step: 6
Training loss: 2.7657551765441895
Validation loss: 2.3068681352881977

Epoch: 6| Step: 7
Training loss: 2.4031307697296143
Validation loss: 2.30817199009721

Epoch: 6| Step: 8
Training loss: 2.947883129119873
Validation loss: 2.302583325293756

Epoch: 6| Step: 9
Training loss: 2.1915740966796875
Validation loss: 2.30906771075341

Epoch: 6| Step: 10
Training loss: 2.5338306427001953
Validation loss: 2.3102106150760444

Epoch: 6| Step: 11
Training loss: 2.5385849475860596
Validation loss: 2.3207895448130946

Epoch: 6| Step: 12
Training loss: 1.9295971393585205
Validation loss: 2.320177173101774

Epoch: 6| Step: 13
Training loss: 3.155033826828003
Validation loss: 2.3297888130270024

Epoch: 127| Step: 0
Training loss: 2.136549711227417
Validation loss: 2.3274514649503972

Epoch: 6| Step: 1
Training loss: 2.9535341262817383
Validation loss: 2.33398756160531

Epoch: 6| Step: 2
Training loss: 2.5141282081604004
Validation loss: 2.340787644027382

Epoch: 6| Step: 3
Training loss: 2.240781784057617
Validation loss: 2.3521352916635494

Epoch: 6| Step: 4
Training loss: 2.356743812561035
Validation loss: 2.3595296311122116

Epoch: 6| Step: 5
Training loss: 2.4274168014526367
Validation loss: 2.3483400626849105

Epoch: 6| Step: 6
Training loss: 2.872220039367676
Validation loss: 2.350823897187428

Epoch: 6| Step: 7
Training loss: 3.4556450843811035
Validation loss: 2.3250468213071107

Epoch: 6| Step: 8
Training loss: 2.6359002590179443
Validation loss: 2.3173361875677623

Epoch: 6| Step: 9
Training loss: 2.8169028759002686
Validation loss: 2.3171782442318496

Epoch: 6| Step: 10
Training loss: 2.0089573860168457
Validation loss: 2.3166750861752416

Epoch: 6| Step: 11
Training loss: 2.5014963150024414
Validation loss: 2.3119770647377096

Epoch: 6| Step: 12
Training loss: 2.4190266132354736
Validation loss: 2.3183988268657396

Epoch: 6| Step: 13
Training loss: 2.4234039783477783
Validation loss: 2.3201275743463987

Epoch: 128| Step: 0
Training loss: 1.6738536357879639
Validation loss: 2.324858042501634

Epoch: 6| Step: 1
Training loss: 2.681328296661377
Validation loss: 2.324484791806949

Epoch: 6| Step: 2
Training loss: 1.862182855606079
Validation loss: 2.323575960692539

Epoch: 6| Step: 3
Training loss: 2.239975929260254
Validation loss: 2.339820482397592

Epoch: 6| Step: 4
Training loss: 2.805101156234741
Validation loss: 2.3371612077118247

Epoch: 6| Step: 5
Training loss: 2.463991641998291
Validation loss: 2.335565890035322

Epoch: 6| Step: 6
Training loss: 3.607556104660034
Validation loss: 2.3434197107950845

Epoch: 6| Step: 7
Training loss: 3.006194829940796
Validation loss: 2.332914290889617

Epoch: 6| Step: 8
Training loss: 3.710301399230957
Validation loss: 2.3328217024444253

Epoch: 6| Step: 9
Training loss: 1.707220196723938
Validation loss: 2.3284824586683706

Epoch: 6| Step: 10
Training loss: 3.055346965789795
Validation loss: 2.324940409711612

Epoch: 6| Step: 11
Training loss: 2.2391161918640137
Validation loss: 2.3114920098294496

Epoch: 6| Step: 12
Training loss: 2.2390553951263428
Validation loss: 2.3067750930786133

Epoch: 6| Step: 13
Training loss: 2.6117773056030273
Validation loss: 2.3053992050950245

Epoch: 129| Step: 0
Training loss: 3.6346378326416016
Validation loss: 2.3080704340370755

Epoch: 6| Step: 1
Training loss: 2.624586820602417
Validation loss: 2.3154301899735645

Epoch: 6| Step: 2
Training loss: 2.6890382766723633
Validation loss: 2.330404637962259

Epoch: 6| Step: 3
Training loss: 2.9036307334899902
Validation loss: 2.330518155969599

Epoch: 6| Step: 4
Training loss: 2.7619423866271973
Validation loss: 2.3401604032003753

Epoch: 6| Step: 5
Training loss: 1.8641339540481567
Validation loss: 2.325162818354945

Epoch: 6| Step: 6
Training loss: 2.2582969665527344
Validation loss: 2.3152568032664638

Epoch: 6| Step: 7
Training loss: 1.9521586894989014
Validation loss: 2.3043124650114324

Epoch: 6| Step: 8
Training loss: 2.190385580062866
Validation loss: 2.3010001515829437

Epoch: 6| Step: 9
Training loss: 2.7064528465270996
Validation loss: 2.3072463953366844

Epoch: 6| Step: 10
Training loss: 2.0500025749206543
Validation loss: 2.303921171413955

Epoch: 6| Step: 11
Training loss: 2.5215559005737305
Validation loss: 2.303318915828582

Epoch: 6| Step: 12
Training loss: 2.9219982624053955
Validation loss: 2.3131697946979153

Epoch: 6| Step: 13
Training loss: 2.7720015048980713
Validation loss: 2.31493640458712

Epoch: 130| Step: 0
Training loss: 2.623044013977051
Validation loss: 2.3155407956851426

Epoch: 6| Step: 1
Training loss: 2.567095994949341
Validation loss: 2.3060184447996077

Epoch: 6| Step: 2
Training loss: 2.70377254486084
Validation loss: 2.3037349229217856

Epoch: 6| Step: 3
Training loss: 3.48209285736084
Validation loss: 2.304280945049819

Epoch: 6| Step: 4
Training loss: 2.58242130279541
Validation loss: 2.299524296996414

Epoch: 6| Step: 5
Training loss: 2.8028907775878906
Validation loss: 2.3007438464831282

Epoch: 6| Step: 6
Training loss: 2.8015060424804688
Validation loss: 2.3041266241381244

Epoch: 6| Step: 7
Training loss: 2.424732208251953
Validation loss: 2.3070719806096887

Epoch: 6| Step: 8
Training loss: 2.172239065170288
Validation loss: 2.3095834819219445

Epoch: 6| Step: 9
Training loss: 2.4497809410095215
Validation loss: 2.3224159543232252

Epoch: 6| Step: 10
Training loss: 2.9243128299713135
Validation loss: 2.349842043333156

Epoch: 6| Step: 11
Training loss: 1.8666709661483765
Validation loss: 2.350330888584096

Epoch: 6| Step: 12
Training loss: 2.1226720809936523
Validation loss: 2.350279459389307

Epoch: 6| Step: 13
Training loss: 2.225266456604004
Validation loss: 2.3442081815452984

Epoch: 131| Step: 0
Training loss: 2.8945796489715576
Validation loss: 2.321816841761271

Epoch: 6| Step: 1
Training loss: 3.354203701019287
Validation loss: 2.303993296879594

Epoch: 6| Step: 2
Training loss: 2.7160487174987793
Validation loss: 2.3119234526029198

Epoch: 6| Step: 3
Training loss: 2.022692918777466
Validation loss: 2.30035598047318

Epoch: 6| Step: 4
Training loss: 2.01077938079834
Validation loss: 2.308500833408807

Epoch: 6| Step: 5
Training loss: 2.3144233226776123
Validation loss: 2.310314155394031

Epoch: 6| Step: 6
Training loss: 2.2349720001220703
Validation loss: 2.304582254860991

Epoch: 6| Step: 7
Training loss: 2.9766499996185303
Validation loss: 2.2999036158284833

Epoch: 6| Step: 8
Training loss: 2.7376294136047363
Validation loss: 2.3026348775432957

Epoch: 6| Step: 9
Training loss: 2.8339998722076416
Validation loss: 2.305863162522675

Epoch: 6| Step: 10
Training loss: 1.497441291809082
Validation loss: 2.3154421621753323

Epoch: 6| Step: 11
Training loss: 2.8056087493896484
Validation loss: 2.328363806970658

Epoch: 6| Step: 12
Training loss: 2.8250350952148438
Validation loss: 2.338922013518631

Epoch: 6| Step: 13
Training loss: 2.4433982372283936
Validation loss: 2.3669943348053963

Epoch: 132| Step: 0
Training loss: 2.511500835418701
Validation loss: 2.3524768506326983

Epoch: 6| Step: 1
Training loss: 2.7702364921569824
Validation loss: 2.3587590955918833

Epoch: 6| Step: 2
Training loss: 2.177452802658081
Validation loss: 2.355294530109693

Epoch: 6| Step: 3
Training loss: 1.8767471313476562
Validation loss: 2.3713618581013014

Epoch: 6| Step: 4
Training loss: 2.9200797080993652
Validation loss: 2.3692955381126812

Epoch: 6| Step: 5
Training loss: 2.16353178024292
Validation loss: 2.3719480576053744

Epoch: 6| Step: 6
Training loss: 2.800679922103882
Validation loss: 2.359290658786733

Epoch: 6| Step: 7
Training loss: 2.3340909481048584
Validation loss: 2.343977625652026

Epoch: 6| Step: 8
Training loss: 2.6885054111480713
Validation loss: 2.330999012916319

Epoch: 6| Step: 9
Training loss: 2.5691585540771484
Validation loss: 2.318657157241657

Epoch: 6| Step: 10
Training loss: 2.607882499694824
Validation loss: 2.306952871302123

Epoch: 6| Step: 11
Training loss: 3.727135181427002
Validation loss: 2.3013403236225085

Epoch: 6| Step: 12
Training loss: 2.54936146736145
Validation loss: 2.2907444584754204

Epoch: 6| Step: 13
Training loss: 1.7294648885726929
Validation loss: 2.295977146394791

Epoch: 133| Step: 0
Training loss: 2.4740352630615234
Validation loss: 2.2889081355064147

Epoch: 6| Step: 1
Training loss: 2.4758710861206055
Validation loss: 2.29116238317182

Epoch: 6| Step: 2
Training loss: 2.4407730102539062
Validation loss: 2.2989543996831423

Epoch: 6| Step: 3
Training loss: 1.735014796257019
Validation loss: 2.298438705423827

Epoch: 6| Step: 4
Training loss: 2.558835506439209
Validation loss: 2.2941097674831266

Epoch: 6| Step: 5
Training loss: 2.7464306354522705
Validation loss: 2.3020584775555517

Epoch: 6| Step: 6
Training loss: 3.184880018234253
Validation loss: 2.311340175649171

Epoch: 6| Step: 7
Training loss: 1.912666916847229
Validation loss: 2.3094640495956584

Epoch: 6| Step: 8
Training loss: 3.0231099128723145
Validation loss: 2.3131977922172955

Epoch: 6| Step: 9
Training loss: 2.941495895385742
Validation loss: 2.319104153622863

Epoch: 6| Step: 10
Training loss: 2.189629316329956
Validation loss: 2.3087550927233953

Epoch: 6| Step: 11
Training loss: 2.7073655128479004
Validation loss: 2.3087526572647916

Epoch: 6| Step: 12
Training loss: 2.982056140899658
Validation loss: 2.2951225055161344

Epoch: 6| Step: 13
Training loss: 2.1039936542510986
Validation loss: 2.2980047143915647

Epoch: 134| Step: 0
Training loss: 3.1349573135375977
Validation loss: 2.302802598604592

Epoch: 6| Step: 1
Training loss: 2.5115394592285156
Validation loss: 2.3019186604407524

Epoch: 6| Step: 2
Training loss: 2.421299457550049
Validation loss: 2.3018432227514123

Epoch: 6| Step: 3
Training loss: 2.3347511291503906
Validation loss: 2.3032223357949206

Epoch: 6| Step: 4
Training loss: 2.861138105392456
Validation loss: 2.306319316228231

Epoch: 6| Step: 5
Training loss: 2.6874423027038574
Validation loss: 2.297773758570353

Epoch: 6| Step: 6
Training loss: 2.265040874481201
Validation loss: 2.2983580173984652

Epoch: 6| Step: 7
Training loss: 2.7412302494049072
Validation loss: 2.2971667499952417

Epoch: 6| Step: 8
Training loss: 3.0478596687316895
Validation loss: 2.295342565864645

Epoch: 6| Step: 9
Training loss: 2.897359848022461
Validation loss: 2.29647978403235

Epoch: 6| Step: 10
Training loss: 2.6146867275238037
Validation loss: 2.301879975103563

Epoch: 6| Step: 11
Training loss: 1.9763953685760498
Validation loss: 2.306336033728815

Epoch: 6| Step: 12
Training loss: 1.8594781160354614
Validation loss: 2.3262343611768497

Epoch: 6| Step: 13
Training loss: 1.8983200788497925
Validation loss: 2.3311193578986713

Epoch: 135| Step: 0
Training loss: 2.248281955718994
Validation loss: 2.3492073525664625

Epoch: 6| Step: 1
Training loss: 2.5143375396728516
Validation loss: 2.3300515426102506

Epoch: 6| Step: 2
Training loss: 2.942060947418213
Validation loss: 2.314952927251016

Epoch: 6| Step: 3
Training loss: 2.9972989559173584
Validation loss: 2.304028462338191

Epoch: 6| Step: 4
Training loss: 2.7941675186157227
Validation loss: 2.291835654166437

Epoch: 6| Step: 5
Training loss: 2.84818172454834
Validation loss: 2.287253584913028

Epoch: 6| Step: 6
Training loss: 1.3617316484451294
Validation loss: 2.274551991493471

Epoch: 6| Step: 7
Training loss: 2.3462295532226562
Validation loss: 2.281873127465607

Epoch: 6| Step: 8
Training loss: 3.1137561798095703
Validation loss: 2.2708510019445933

Epoch: 6| Step: 9
Training loss: 2.8399763107299805
Validation loss: 2.27625782515413

Epoch: 6| Step: 10
Training loss: 1.883275032043457
Validation loss: 2.2841322165663525

Epoch: 6| Step: 11
Training loss: 2.4977641105651855
Validation loss: 2.288027173729353

Epoch: 6| Step: 12
Training loss: 2.2277626991271973
Validation loss: 2.2912688204037246

Epoch: 6| Step: 13
Training loss: 3.3559532165527344
Validation loss: 2.2923206052472516

Epoch: 136| Step: 0
Training loss: 2.1505470275878906
Validation loss: 2.2889942379407984

Epoch: 6| Step: 1
Training loss: 2.4725494384765625
Validation loss: 2.294160169939841

Epoch: 6| Step: 2
Training loss: 2.5550756454467773
Validation loss: 2.290389791611702

Epoch: 6| Step: 3
Training loss: 1.7970882654190063
Validation loss: 2.293694325672683

Epoch: 6| Step: 4
Training loss: 2.688957452774048
Validation loss: 2.2973853747049966

Epoch: 6| Step: 5
Training loss: 2.9431533813476562
Validation loss: 2.298436200746926

Epoch: 6| Step: 6
Training loss: 3.06862211227417
Validation loss: 2.290527541150329

Epoch: 6| Step: 7
Training loss: 3.031439781188965
Validation loss: 2.291154771722773

Epoch: 6| Step: 8
Training loss: 3.1014974117279053
Validation loss: 2.290939438727594

Epoch: 6| Step: 9
Training loss: 2.059311866760254
Validation loss: 2.283196564643614

Epoch: 6| Step: 10
Training loss: 2.3481483459472656
Validation loss: 2.280091654869818

Epoch: 6| Step: 11
Training loss: 2.244605302810669
Validation loss: 2.2806504926373883

Epoch: 6| Step: 12
Training loss: 2.5442800521850586
Validation loss: 2.28088544261071

Epoch: 6| Step: 13
Training loss: 2.584047317504883
Validation loss: 2.2803614113920476

Epoch: 137| Step: 0
Training loss: 2.1358819007873535
Validation loss: 2.2824392344361994

Epoch: 6| Step: 1
Training loss: 2.5672006607055664
Validation loss: 2.2822254062980734

Epoch: 6| Step: 2
Training loss: 2.3612494468688965
Validation loss: 2.286556702788158

Epoch: 6| Step: 3
Training loss: 2.623720645904541
Validation loss: 2.289637542540027

Epoch: 6| Step: 4
Training loss: 3.1011900901794434
Validation loss: 2.291799849079501

Epoch: 6| Step: 5
Training loss: 2.811143398284912
Validation loss: 2.2872307326204036

Epoch: 6| Step: 6
Training loss: 2.6871886253356934
Validation loss: 2.288849684499925

Epoch: 6| Step: 7
Training loss: 2.257988929748535
Validation loss: 2.2943840039673673

Epoch: 6| Step: 8
Training loss: 2.526416301727295
Validation loss: 2.293350040271718

Epoch: 6| Step: 9
Training loss: 2.1943066120147705
Validation loss: 2.286715510070965

Epoch: 6| Step: 10
Training loss: 2.6892974376678467
Validation loss: 2.2847779925151537

Epoch: 6| Step: 11
Training loss: 2.533784866333008
Validation loss: 2.281181494394938

Epoch: 6| Step: 12
Training loss: 2.0588021278381348
Validation loss: 2.2816560729857414

Epoch: 6| Step: 13
Training loss: 2.9997637271881104
Validation loss: 2.2886859140088482

Epoch: 138| Step: 0
Training loss: 2.758561849594116
Validation loss: 2.292607973980647

Epoch: 6| Step: 1
Training loss: 2.5768320560455322
Validation loss: 2.288396038034911

Epoch: 6| Step: 2
Training loss: 1.9420725107192993
Validation loss: 2.301514192294049

Epoch: 6| Step: 3
Training loss: 2.158158540725708
Validation loss: 2.2984709329502557

Epoch: 6| Step: 4
Training loss: 1.958210825920105
Validation loss: 2.302273791323426

Epoch: 6| Step: 5
Training loss: 2.899752140045166
Validation loss: 2.300421352027565

Epoch: 6| Step: 6
Training loss: 2.3086605072021484
Validation loss: 2.304561891863423

Epoch: 6| Step: 7
Training loss: 2.3690948486328125
Validation loss: 2.302955919696439

Epoch: 6| Step: 8
Training loss: 2.4864935874938965
Validation loss: 2.3017859561468965

Epoch: 6| Step: 9
Training loss: 3.236562967300415
Validation loss: 2.304050135356124

Epoch: 6| Step: 10
Training loss: 2.6142373085021973
Validation loss: 2.2985383464444067

Epoch: 6| Step: 11
Training loss: 3.2086985111236572
Validation loss: 2.306254235647058

Epoch: 6| Step: 12
Training loss: 2.5640296936035156
Validation loss: 2.3059304324529504

Epoch: 6| Step: 13
Training loss: 2.1846580505371094
Validation loss: 2.302459539905671

Epoch: 139| Step: 0
Training loss: 2.951617956161499
Validation loss: 2.2904022509051907

Epoch: 6| Step: 1
Training loss: 1.9572792053222656
Validation loss: 2.295135482665031

Epoch: 6| Step: 2
Training loss: 2.281257152557373
Validation loss: 2.2958622004396174

Epoch: 6| Step: 3
Training loss: 2.4127163887023926
Validation loss: 2.289781978053431

Epoch: 6| Step: 4
Training loss: 2.213106155395508
Validation loss: 2.294143292211717

Epoch: 6| Step: 5
Training loss: 2.9303770065307617
Validation loss: 2.2908037093377884

Epoch: 6| Step: 6
Training loss: 2.22843074798584
Validation loss: 2.291785337591684

Epoch: 6| Step: 7
Training loss: 2.4815421104431152
Validation loss: 2.2908828540514876

Epoch: 6| Step: 8
Training loss: 2.547337532043457
Validation loss: 2.2866885290350965

Epoch: 6| Step: 9
Training loss: 2.796724319458008
Validation loss: 2.2913455681134294

Epoch: 6| Step: 10
Training loss: 2.648874282836914
Validation loss: 2.2913189075326406

Epoch: 6| Step: 11
Training loss: 2.998805522918701
Validation loss: 2.282861832649477

Epoch: 6| Step: 12
Training loss: 2.3441171646118164
Validation loss: 2.272335085817563

Epoch: 6| Step: 13
Training loss: 2.84757399559021
Validation loss: 2.2761926151091054

Epoch: 140| Step: 0
Training loss: 3.296567916870117
Validation loss: 2.275589386622111

Epoch: 6| Step: 1
Training loss: 2.021444797515869
Validation loss: 2.2779828617649693

Epoch: 6| Step: 2
Training loss: 2.8350272178649902
Validation loss: 2.28210372565895

Epoch: 6| Step: 3
Training loss: 2.737730026245117
Validation loss: 2.314105115911012

Epoch: 6| Step: 4
Training loss: 3.213367462158203
Validation loss: 2.3172986917598273

Epoch: 6| Step: 5
Training loss: 1.8679344654083252
Validation loss: 2.338101176805394

Epoch: 6| Step: 6
Training loss: 2.390831470489502
Validation loss: 2.3375082195446057

Epoch: 6| Step: 7
Training loss: 2.362036943435669
Validation loss: 2.329792396996611

Epoch: 6| Step: 8
Training loss: 2.315702199935913
Validation loss: 2.3291837092368834

Epoch: 6| Step: 9
Training loss: 2.5403823852539062
Validation loss: 2.307790779298352

Epoch: 6| Step: 10
Training loss: 2.251009941101074
Validation loss: 2.301602735314318

Epoch: 6| Step: 11
Training loss: 3.119400978088379
Validation loss: 2.2938252366999143

Epoch: 6| Step: 12
Training loss: 2.210120916366577
Validation loss: 2.2863317945952057

Epoch: 6| Step: 13
Training loss: 2.3723912239074707
Validation loss: 2.2735638721014864

Epoch: 141| Step: 0
Training loss: 2.5731515884399414
Validation loss: 2.2671856982733614

Epoch: 6| Step: 1
Training loss: 3.474447250366211
Validation loss: 2.2678661577163206

Epoch: 6| Step: 2
Training loss: 2.460299491882324
Validation loss: 2.2745781893371255

Epoch: 6| Step: 3
Training loss: 2.762070655822754
Validation loss: 2.264306519621162

Epoch: 6| Step: 4
Training loss: 2.636281967163086
Validation loss: 2.2619436594747726

Epoch: 6| Step: 5
Training loss: 2.2468950748443604
Validation loss: 2.2634109015105874

Epoch: 6| Step: 6
Training loss: 2.3702380657196045
Validation loss: 2.267340898513794

Epoch: 6| Step: 7
Training loss: 2.1547744274139404
Validation loss: 2.2730089361949632

Epoch: 6| Step: 8
Training loss: 2.030395030975342
Validation loss: 2.268601748251146

Epoch: 6| Step: 9
Training loss: 2.9247093200683594
Validation loss: 2.2765898089255057

Epoch: 6| Step: 10
Training loss: 2.5358965396881104
Validation loss: 2.2718785706386773

Epoch: 6| Step: 11
Training loss: 2.8009815216064453
Validation loss: 2.270890353828348

Epoch: 6| Step: 12
Training loss: 2.3118042945861816
Validation loss: 2.266480435607254

Epoch: 6| Step: 13
Training loss: 1.8048701286315918
Validation loss: 2.265919977618802

Epoch: 142| Step: 0
Training loss: 3.2364754676818848
Validation loss: 2.266685470458

Epoch: 6| Step: 1
Training loss: 3.2995340824127197
Validation loss: 2.2716591383821223

Epoch: 6| Step: 2
Training loss: 2.0391440391540527
Validation loss: 2.2723935957877868

Epoch: 6| Step: 3
Training loss: 2.1838948726654053
Validation loss: 2.2640872386194046

Epoch: 6| Step: 4
Training loss: 2.583146572113037
Validation loss: 2.2717829647884575

Epoch: 6| Step: 5
Training loss: 2.0613396167755127
Validation loss: 2.2830714397532965

Epoch: 6| Step: 6
Training loss: 2.1236612796783447
Validation loss: 2.2768505901418705

Epoch: 6| Step: 7
Training loss: 2.652857780456543
Validation loss: 2.280933949255174

Epoch: 6| Step: 8
Training loss: 3.3644843101501465
Validation loss: 2.276489468031032

Epoch: 6| Step: 9
Training loss: 2.543828010559082
Validation loss: 2.2681276016337897

Epoch: 6| Step: 10
Training loss: 2.2976644039154053
Validation loss: 2.272209984000011

Epoch: 6| Step: 11
Training loss: 2.0000407695770264
Validation loss: 2.2735548711592153

Epoch: 6| Step: 12
Training loss: 2.6936514377593994
Validation loss: 2.2642083578212286

Epoch: 6| Step: 13
Training loss: 1.94074547290802
Validation loss: 2.263204118256928

Epoch: 143| Step: 0
Training loss: 3.054398536682129
Validation loss: 2.2644074014438096

Epoch: 6| Step: 1
Training loss: 3.0823702812194824
Validation loss: 2.2722002793383855

Epoch: 6| Step: 2
Training loss: 2.8086490631103516
Validation loss: 2.2671687872179094

Epoch: 6| Step: 3
Training loss: 1.5933393239974976
Validation loss: 2.266600119170322

Epoch: 6| Step: 4
Training loss: 2.3147363662719727
Validation loss: 2.2700603495361986

Epoch: 6| Step: 5
Training loss: 2.588357448577881
Validation loss: 2.265968832918393

Epoch: 6| Step: 6
Training loss: 2.7264914512634277
Validation loss: 2.271082757621683

Epoch: 6| Step: 7
Training loss: 2.4187259674072266
Validation loss: 2.2772137477833736

Epoch: 6| Step: 8
Training loss: 2.4166574478149414
Validation loss: 2.2937087064148276

Epoch: 6| Step: 9
Training loss: 2.7570464611053467
Validation loss: 2.320625892249487

Epoch: 6| Step: 10
Training loss: 2.8766703605651855
Validation loss: 2.3365138628149547

Epoch: 6| Step: 11
Training loss: 2.1578924655914307
Validation loss: 2.3296534861287763

Epoch: 6| Step: 12
Training loss: 2.46614933013916
Validation loss: 2.3166951107722458

Epoch: 6| Step: 13
Training loss: 1.8902047872543335
Validation loss: 2.3155770199273222

Epoch: 144| Step: 0
Training loss: 2.1368165016174316
Validation loss: 2.304253767895442

Epoch: 6| Step: 1
Training loss: 3.0673840045928955
Validation loss: 2.2913442324566584

Epoch: 6| Step: 2
Training loss: 3.0465054512023926
Validation loss: 2.2776543299357095

Epoch: 6| Step: 3
Training loss: 2.672774314880371
Validation loss: 2.2824569004838184

Epoch: 6| Step: 4
Training loss: 2.673649787902832
Validation loss: 2.286516176756992

Epoch: 6| Step: 5
Training loss: 2.7049713134765625
Validation loss: 2.2845918696413756

Epoch: 6| Step: 6
Training loss: 2.201148748397827
Validation loss: 2.2834829540662867

Epoch: 6| Step: 7
Training loss: 2.68308424949646
Validation loss: 2.272747242322532

Epoch: 6| Step: 8
Training loss: 2.1022086143493652
Validation loss: 2.2699208080127673

Epoch: 6| Step: 9
Training loss: 2.282468795776367
Validation loss: 2.2656388308412287

Epoch: 6| Step: 10
Training loss: 2.4643449783325195
Validation loss: 2.258950315495973

Epoch: 6| Step: 11
Training loss: 3.02079701423645
Validation loss: 2.2519023700426986

Epoch: 6| Step: 12
Training loss: 1.8814282417297363
Validation loss: 2.245257321224418

Epoch: 6| Step: 13
Training loss: 2.1485331058502197
Validation loss: 2.2387283540541127

Epoch: 145| Step: 0
Training loss: 2.7856459617614746
Validation loss: 2.246723090448687

Epoch: 6| Step: 1
Training loss: 2.442347764968872
Validation loss: 2.2489482138746526

Epoch: 6| Step: 2
Training loss: 1.9616587162017822
Validation loss: 2.250667912985689

Epoch: 6| Step: 3
Training loss: 2.8391361236572266
Validation loss: 2.2517411708831787

Epoch: 6| Step: 4
Training loss: 2.8433032035827637
Validation loss: 2.2478217412066717

Epoch: 6| Step: 5
Training loss: 2.066129684448242
Validation loss: 2.2584011298353954

Epoch: 6| Step: 6
Training loss: 3.1483232975006104
Validation loss: 2.258436825967604

Epoch: 6| Step: 7
Training loss: 2.588996171951294
Validation loss: 2.26388878719781

Epoch: 6| Step: 8
Training loss: 2.3401684761047363
Validation loss: 2.276661196062642

Epoch: 6| Step: 9
Training loss: 1.472890853881836
Validation loss: 2.286798087499475

Epoch: 6| Step: 10
Training loss: 2.736532211303711
Validation loss: 2.284558250058082

Epoch: 6| Step: 11
Training loss: 3.2453761100769043
Validation loss: 2.2966815733140513

Epoch: 6| Step: 12
Training loss: 1.99105966091156
Validation loss: 2.3037600696727796

Epoch: 6| Step: 13
Training loss: 2.830980062484741
Validation loss: 2.3009343121641423

Epoch: 146| Step: 0
Training loss: 2.685238838195801
Validation loss: 2.29859100362306

Epoch: 6| Step: 1
Training loss: 2.7561702728271484
Validation loss: 2.3060426327490036

Epoch: 6| Step: 2
Training loss: 2.447538375854492
Validation loss: 2.304997576180325

Epoch: 6| Step: 3
Training loss: 2.3819684982299805
Validation loss: 2.3014997359245055

Epoch: 6| Step: 4
Training loss: 3.1478567123413086
Validation loss: 2.3137283094467653

Epoch: 6| Step: 5
Training loss: 3.258542776107788
Validation loss: 2.2956779695326284

Epoch: 6| Step: 6
Training loss: 2.148514986038208
Validation loss: 2.287361591093002

Epoch: 6| Step: 7
Training loss: 2.212895393371582
Validation loss: 2.2673123716026224

Epoch: 6| Step: 8
Training loss: 2.847813129425049
Validation loss: 2.2532832776346514

Epoch: 6| Step: 9
Training loss: 2.3494491577148438
Validation loss: 2.2466689437948246

Epoch: 6| Step: 10
Training loss: 2.2663772106170654
Validation loss: 2.24421517310604

Epoch: 6| Step: 11
Training loss: 2.2661991119384766
Validation loss: 2.2503415948601178

Epoch: 6| Step: 12
Training loss: 2.3514862060546875
Validation loss: 2.255201465340071

Epoch: 6| Step: 13
Training loss: 1.9709243774414062
Validation loss: 2.252563379144156

Epoch: 147| Step: 0
Training loss: 2.085864543914795
Validation loss: 2.250889455118487

Epoch: 6| Step: 1
Training loss: 3.022782564163208
Validation loss: 2.2499845668833744

Epoch: 6| Step: 2
Training loss: 2.7632195949554443
Validation loss: 2.24929557564438

Epoch: 6| Step: 3
Training loss: 2.547863721847534
Validation loss: 2.256325685849754

Epoch: 6| Step: 4
Training loss: 2.357530117034912
Validation loss: 2.2624100664610505

Epoch: 6| Step: 5
Training loss: 2.5027101039886475
Validation loss: 2.264035983752179

Epoch: 6| Step: 6
Training loss: 2.698922634124756
Validation loss: 2.2673754615168416

Epoch: 6| Step: 7
Training loss: 2.455080509185791
Validation loss: 2.2685820825638308

Epoch: 6| Step: 8
Training loss: 2.725998878479004
Validation loss: 2.2722386570386988

Epoch: 6| Step: 9
Training loss: 2.817699909210205
Validation loss: 2.274675812772525

Epoch: 6| Step: 10
Training loss: 2.2123260498046875
Validation loss: 2.276500940322876

Epoch: 6| Step: 11
Training loss: 2.2547128200531006
Validation loss: 2.282338244940645

Epoch: 6| Step: 12
Training loss: 2.3786704540252686
Validation loss: 2.2900042841511388

Epoch: 6| Step: 13
Training loss: 2.004732608795166
Validation loss: 2.3003345304919827

Epoch: 148| Step: 0
Training loss: 1.420891284942627
Validation loss: 2.317650838564801

Epoch: 6| Step: 1
Training loss: 2.878791332244873
Validation loss: 2.311684649477723

Epoch: 6| Step: 2
Training loss: 2.2391438484191895
Validation loss: 2.303991307494461

Epoch: 6| Step: 3
Training loss: 3.0147604942321777
Validation loss: 2.2944138229534192

Epoch: 6| Step: 4
Training loss: 2.3274011611938477
Validation loss: 2.2851801174943165

Epoch: 6| Step: 5
Training loss: 1.898698091506958
Validation loss: 2.272601681370889

Epoch: 6| Step: 6
Training loss: 2.4414923191070557
Validation loss: 2.256284757327008

Epoch: 6| Step: 7
Training loss: 3.1193923950195312
Validation loss: 2.268030030753023

Epoch: 6| Step: 8
Training loss: 2.3767127990722656
Validation loss: 2.26308205563535

Epoch: 6| Step: 9
Training loss: 2.3146677017211914
Validation loss: 2.2557834643189625

Epoch: 6| Step: 10
Training loss: 3.050285816192627
Validation loss: 2.254374606634981

Epoch: 6| Step: 11
Training loss: 2.0616607666015625
Validation loss: 2.251989059550788

Epoch: 6| Step: 12
Training loss: 3.0200724601745605
Validation loss: 2.2457745946863645

Epoch: 6| Step: 13
Training loss: 3.105116605758667
Validation loss: 2.2444761158317648

Epoch: 149| Step: 0
Training loss: 1.5378836393356323
Validation loss: 2.242379343637856

Epoch: 6| Step: 1
Training loss: 2.7537693977355957
Validation loss: 2.2383864643753215

Epoch: 6| Step: 2
Training loss: 2.517883777618408
Validation loss: 2.229136436216293

Epoch: 6| Step: 3
Training loss: 2.8000853061676025
Validation loss: 2.2367984402564263

Epoch: 6| Step: 4
Training loss: 2.997142791748047
Validation loss: 2.23744261136619

Epoch: 6| Step: 5
Training loss: 2.0999767780303955
Validation loss: 2.241735669874376

Epoch: 6| Step: 6
Training loss: 2.164762496948242
Validation loss: 2.2393616707094255

Epoch: 6| Step: 7
Training loss: 2.5430219173431396
Validation loss: 2.238112275318433

Epoch: 6| Step: 8
Training loss: 2.8432881832122803
Validation loss: 2.2418809629255727

Epoch: 6| Step: 9
Training loss: 2.768139123916626
Validation loss: 2.2459945217255624

Epoch: 6| Step: 10
Training loss: 2.2011702060699463
Validation loss: 2.247733898060296

Epoch: 6| Step: 11
Training loss: 2.7816274166107178
Validation loss: 2.2468006418597315

Epoch: 6| Step: 12
Training loss: 3.001121997833252
Validation loss: 2.2548177242279053

Epoch: 6| Step: 13
Training loss: 1.4256036281585693
Validation loss: 2.2503840474672216

Epoch: 150| Step: 0
Training loss: 2.9030470848083496
Validation loss: 2.256272556961224

Epoch: 6| Step: 1
Training loss: 2.594223976135254
Validation loss: 2.264592234806348

Epoch: 6| Step: 2
Training loss: 2.3573288917541504
Validation loss: 2.2427558437470467

Epoch: 6| Step: 3
Training loss: 2.2287888526916504
Validation loss: 2.240258668058662

Epoch: 6| Step: 4
Training loss: 2.454185724258423
Validation loss: 2.260120076517905

Epoch: 6| Step: 5
Training loss: 3.0695881843566895
Validation loss: 2.2728885501943608

Epoch: 6| Step: 6
Training loss: 2.4209792613983154
Validation loss: 2.2543243387694

Epoch: 6| Step: 7
Training loss: 2.760289192199707
Validation loss: 2.2500357525323027

Epoch: 6| Step: 8
Training loss: 2.727627754211426
Validation loss: 2.2536864075609433

Epoch: 6| Step: 9
Training loss: 1.5241036415100098
Validation loss: 2.2492458384524108

Epoch: 6| Step: 10
Training loss: 2.4496240615844727
Validation loss: 2.2532763045321227

Epoch: 6| Step: 11
Training loss: 2.779787540435791
Validation loss: 2.2576475463887697

Epoch: 6| Step: 12
Training loss: 2.5239017009735107
Validation loss: 2.260203129501753

Epoch: 6| Step: 13
Training loss: 1.6784155368804932
Validation loss: 2.2658205673258793

Epoch: 151| Step: 0
Training loss: 2.464982271194458
Validation loss: 2.2771458010519705

Epoch: 6| Step: 1
Training loss: 2.8788886070251465
Validation loss: 2.274822647853564

Epoch: 6| Step: 2
Training loss: 2.715407371520996
Validation loss: 2.282966036950388

Epoch: 6| Step: 3
Training loss: 2.6261045932769775
Validation loss: 2.2815660866357947

Epoch: 6| Step: 4
Training loss: 2.471153736114502
Validation loss: 2.293383406054589

Epoch: 6| Step: 5
Training loss: 2.641371488571167
Validation loss: 2.2764668541569866

Epoch: 6| Step: 6
Training loss: 2.9483530521392822
Validation loss: 2.2739776129363687

Epoch: 6| Step: 7
Training loss: 2.2819292545318604
Validation loss: 2.2592257863731793

Epoch: 6| Step: 8
Training loss: 2.241424083709717
Validation loss: 2.250608674941524

Epoch: 6| Step: 9
Training loss: 1.2874006032943726
Validation loss: 2.245358685011505

Epoch: 6| Step: 10
Training loss: 3.011422634124756
Validation loss: 2.245466157954226

Epoch: 6| Step: 11
Training loss: 2.073791980743408
Validation loss: 2.2340774087495703

Epoch: 6| Step: 12
Training loss: 2.8046975135803223
Validation loss: 2.2304483998206353

Epoch: 6| Step: 13
Training loss: 2.416375160217285
Validation loss: 2.2309578798150502

Epoch: 152| Step: 0
Training loss: 2.1599199771881104
Validation loss: 2.233743739384477

Epoch: 6| Step: 1
Training loss: 2.2206947803497314
Validation loss: 2.2313120775325324

Epoch: 6| Step: 2
Training loss: 2.9514060020446777
Validation loss: 2.229374183121548

Epoch: 6| Step: 3
Training loss: 2.8049747943878174
Validation loss: 2.227479996219758

Epoch: 6| Step: 4
Training loss: 2.892366886138916
Validation loss: 2.2223087305663736

Epoch: 6| Step: 5
Training loss: 3.4211831092834473
Validation loss: 2.230343803282707

Epoch: 6| Step: 6
Training loss: 2.1434149742126465
Validation loss: 2.2421855977786485

Epoch: 6| Step: 7
Training loss: 2.2068979740142822
Validation loss: 2.227387733356927

Epoch: 6| Step: 8
Training loss: 2.2244746685028076
Validation loss: 2.234021641874826

Epoch: 6| Step: 9
Training loss: 2.682910919189453
Validation loss: 2.25226842716176

Epoch: 6| Step: 10
Training loss: 2.436253786087036
Validation loss: 2.2530452256561606

Epoch: 6| Step: 11
Training loss: 1.417456030845642
Validation loss: 2.2602063763526177

Epoch: 6| Step: 12
Training loss: 2.81795072555542
Validation loss: 2.2515979120808263

Epoch: 6| Step: 13
Training loss: 2.2659738063812256
Validation loss: 2.242600622997489

Epoch: 153| Step: 0
Training loss: 3.0899524688720703
Validation loss: 2.253198398056851

Epoch: 6| Step: 1
Training loss: 2.6184558868408203
Validation loss: 2.245471126289778

Epoch: 6| Step: 2
Training loss: 2.2772350311279297
Validation loss: 2.2473325447369645

Epoch: 6| Step: 3
Training loss: 2.1122946739196777
Validation loss: 2.251510235571092

Epoch: 6| Step: 4
Training loss: 2.7412123680114746
Validation loss: 2.2561961130429338

Epoch: 6| Step: 5
Training loss: 2.442340850830078
Validation loss: 2.258141271529659

Epoch: 6| Step: 6
Training loss: 2.4592881202697754
Validation loss: 2.2726196242916967

Epoch: 6| Step: 7
Training loss: 2.392254114151001
Validation loss: 2.267929930840769

Epoch: 6| Step: 8
Training loss: 2.5327248573303223
Validation loss: 2.265191373004708

Epoch: 6| Step: 9
Training loss: 1.763839840888977
Validation loss: 2.2458865193910498

Epoch: 6| Step: 10
Training loss: 2.6020123958587646
Validation loss: 2.2485212523450135

Epoch: 6| Step: 11
Training loss: 2.0749752521514893
Validation loss: 2.2441170907789663

Epoch: 6| Step: 12
Training loss: 2.8252480030059814
Validation loss: 2.2389794523997972

Epoch: 6| Step: 13
Training loss: 3.052682399749756
Validation loss: 2.2483006344046643

Epoch: 154| Step: 0
Training loss: 2.9267163276672363
Validation loss: 2.2604439720030753

Epoch: 6| Step: 1
Training loss: 2.1824305057525635
Validation loss: 2.257448181029289

Epoch: 6| Step: 2
Training loss: 2.8962063789367676
Validation loss: 2.2739284781999487

Epoch: 6| Step: 3
Training loss: 3.3686933517456055
Validation loss: 2.275220796626101

Epoch: 6| Step: 4
Training loss: 2.7664170265197754
Validation loss: 2.2565368426743375

Epoch: 6| Step: 5
Training loss: 2.6526453495025635
Validation loss: 2.249366670526484

Epoch: 6| Step: 6
Training loss: 2.5385637283325195
Validation loss: 2.230669703534854

Epoch: 6| Step: 7
Training loss: 2.146454095840454
Validation loss: 2.219327806144632

Epoch: 6| Step: 8
Training loss: 3.0717930793762207
Validation loss: 2.221252595224688

Epoch: 6| Step: 9
Training loss: 1.7106233835220337
Validation loss: 2.2185522522977603

Epoch: 6| Step: 10
Training loss: 1.8575639724731445
Validation loss: 2.21664612267607

Epoch: 6| Step: 11
Training loss: 2.1102190017700195
Validation loss: 2.2317194323385916

Epoch: 6| Step: 12
Training loss: 1.9217356443405151
Validation loss: 2.239827168885098

Epoch: 6| Step: 13
Training loss: 2.778017520904541
Validation loss: 2.2389483990207797

Epoch: 155| Step: 0
Training loss: 2.4246156215667725
Validation loss: 2.2281284870639926

Epoch: 6| Step: 1
Training loss: 2.0573387145996094
Validation loss: 2.233195371525262

Epoch: 6| Step: 2
Training loss: 2.4183459281921387
Validation loss: 2.2408196003206315

Epoch: 6| Step: 3
Training loss: 2.672226905822754
Validation loss: 2.2454938452730895

Epoch: 6| Step: 4
Training loss: 1.8426828384399414
Validation loss: 2.2543372082453903

Epoch: 6| Step: 5
Training loss: 2.578061580657959
Validation loss: 2.2681725230268253

Epoch: 6| Step: 6
Training loss: 2.726541042327881
Validation loss: 2.263515385248328

Epoch: 6| Step: 7
Training loss: 2.752439022064209
Validation loss: 2.269102370867165

Epoch: 6| Step: 8
Training loss: 2.3355746269226074
Validation loss: 2.2517715474610687

Epoch: 6| Step: 9
Training loss: 2.152890920639038
Validation loss: 2.251359049991895

Epoch: 6| Step: 10
Training loss: 2.203496217727661
Validation loss: 2.25076860766257

Epoch: 6| Step: 11
Training loss: 2.3392460346221924
Validation loss: 2.2589626889075003

Epoch: 6| Step: 12
Training loss: 3.1177053451538086
Validation loss: 2.2528275559025426

Epoch: 6| Step: 13
Training loss: 3.638904571533203
Validation loss: 2.2539586379963863

Epoch: 156| Step: 0
Training loss: 2.125816583633423
Validation loss: 2.2451926033983947

Epoch: 6| Step: 1
Training loss: 2.832813262939453
Validation loss: 2.2486784996524936

Epoch: 6| Step: 2
Training loss: 2.903096914291382
Validation loss: 2.2491285749661025

Epoch: 6| Step: 3
Training loss: 2.072309970855713
Validation loss: 2.252633930534445

Epoch: 6| Step: 4
Training loss: 2.612586259841919
Validation loss: 2.2604616380506948

Epoch: 6| Step: 5
Training loss: 2.4404187202453613
Validation loss: 2.28214858424279

Epoch: 6| Step: 6
Training loss: 2.8868513107299805
Validation loss: 2.2842176985997025

Epoch: 6| Step: 7
Training loss: 2.3387649059295654
Validation loss: 2.2797354626399216

Epoch: 6| Step: 8
Training loss: 2.2355189323425293
Validation loss: 2.2716907583257204

Epoch: 6| Step: 9
Training loss: 2.3425960540771484
Validation loss: 2.2598031554170834

Epoch: 6| Step: 10
Training loss: 2.875720500946045
Validation loss: 2.2522463516522477

Epoch: 6| Step: 11
Training loss: 2.4734716415405273
Validation loss: 2.24578183697116

Epoch: 6| Step: 12
Training loss: 1.9547847509384155
Validation loss: 2.2404889701515116

Epoch: 6| Step: 13
Training loss: 2.5809943675994873
Validation loss: 2.221941909482402

Epoch: 157| Step: 0
Training loss: 2.4867100715637207
Validation loss: 2.219751173450101

Epoch: 6| Step: 1
Training loss: 2.4812209606170654
Validation loss: 2.2164477174000075

Epoch: 6| Step: 2
Training loss: 1.9350671768188477
Validation loss: 2.2200749689532864

Epoch: 6| Step: 3
Training loss: 2.6001358032226562
Validation loss: 2.2132960724574264

Epoch: 6| Step: 4
Training loss: 2.5263397693634033
Validation loss: 2.2202022844745266

Epoch: 6| Step: 5
Training loss: 2.102719783782959
Validation loss: 2.2144651566782305

Epoch: 6| Step: 6
Training loss: 2.363467216491699
Validation loss: 2.2155564420966694

Epoch: 6| Step: 7
Training loss: 1.7244362831115723
Validation loss: 2.2158609372313305

Epoch: 6| Step: 8
Training loss: 2.298490524291992
Validation loss: 2.2166653961263676

Epoch: 6| Step: 9
Training loss: 3.048511505126953
Validation loss: 2.22398111128038

Epoch: 6| Step: 10
Training loss: 3.304394483566284
Validation loss: 2.2232264780229136

Epoch: 6| Step: 11
Training loss: 2.654182195663452
Validation loss: 2.2150309239664385

Epoch: 6| Step: 12
Training loss: 2.6367554664611816
Validation loss: 2.2053660500434136

Epoch: 6| Step: 13
Training loss: 2.4643301963806152
Validation loss: 2.2141064648987143

Epoch: 158| Step: 0
Training loss: 2.540989875793457
Validation loss: 2.214258699006932

Epoch: 6| Step: 1
Training loss: 2.1721878051757812
Validation loss: 2.2188148011443434

Epoch: 6| Step: 2
Training loss: 2.2303032875061035
Validation loss: 2.215516415975427

Epoch: 6| Step: 3
Training loss: 2.3330836296081543
Validation loss: 2.2157028618679253

Epoch: 6| Step: 4
Training loss: 2.358748197555542
Validation loss: 2.2141283045532885

Epoch: 6| Step: 5
Training loss: 2.9898898601531982
Validation loss: 2.2119204177651355

Epoch: 6| Step: 6
Training loss: 2.4545817375183105
Validation loss: 2.2136667338750695

Epoch: 6| Step: 7
Training loss: 2.7118451595306396
Validation loss: 2.220412115896902

Epoch: 6| Step: 8
Training loss: 3.0664172172546387
Validation loss: 2.227245374392438

Epoch: 6| Step: 9
Training loss: 2.355813980102539
Validation loss: 2.259097340286419

Epoch: 6| Step: 10
Training loss: 2.166485071182251
Validation loss: 2.3190501351510324

Epoch: 6| Step: 11
Training loss: 2.193108558654785
Validation loss: 2.377443903235979

Epoch: 6| Step: 12
Training loss: 2.6222891807556152
Validation loss: 2.46406570301261

Epoch: 6| Step: 13
Training loss: 3.0008578300476074
Validation loss: 2.436370834227531

Epoch: 159| Step: 0
Training loss: 2.2345948219299316
Validation loss: 2.3433212285400717

Epoch: 6| Step: 1
Training loss: 2.1374030113220215
Validation loss: 2.2836139381572766

Epoch: 6| Step: 2
Training loss: 2.4173192977905273
Validation loss: 2.2512047342074815

Epoch: 6| Step: 3
Training loss: 2.619476795196533
Validation loss: 2.240049155809546

Epoch: 6| Step: 4
Training loss: 2.9963228702545166
Validation loss: 2.239752964306903

Epoch: 6| Step: 5
Training loss: 2.519932270050049
Validation loss: 2.25014074899817

Epoch: 6| Step: 6
Training loss: 2.510530471801758
Validation loss: 2.2809388868270384

Epoch: 6| Step: 7
Training loss: 2.423079013824463
Validation loss: 2.2965648635741203

Epoch: 6| Step: 8
Training loss: 2.9403254985809326
Validation loss: 2.2952573555772022

Epoch: 6| Step: 9
Training loss: 2.6704728603363037
Validation loss: 2.313173037703319

Epoch: 6| Step: 10
Training loss: 2.634763717651367
Validation loss: 2.2611525609929073

Epoch: 6| Step: 11
Training loss: 2.4239025115966797
Validation loss: 2.246818032315982

Epoch: 6| Step: 12
Training loss: 2.311039924621582
Validation loss: 2.223682480473672

Epoch: 6| Step: 13
Training loss: 2.4200503826141357
Validation loss: 2.2171031018739105

Epoch: 160| Step: 0
Training loss: 1.8142883777618408
Validation loss: 2.2312410416141635

Epoch: 6| Step: 1
Training loss: 3.488896131515503
Validation loss: 2.251540760840139

Epoch: 6| Step: 2
Training loss: 2.0559592247009277
Validation loss: 2.270021592417071

Epoch: 6| Step: 3
Training loss: 2.6367835998535156
Validation loss: 2.2967359737683366

Epoch: 6| Step: 4
Training loss: 2.138549327850342
Validation loss: 2.328869230003767

Epoch: 6| Step: 5
Training loss: 2.3748230934143066
Validation loss: 2.3569518340531217

Epoch: 6| Step: 6
Training loss: 2.828763484954834
Validation loss: 2.345758602183352

Epoch: 6| Step: 7
Training loss: 3.055650234222412
Validation loss: 2.335370825183007

Epoch: 6| Step: 8
Training loss: 1.934693455696106
Validation loss: 2.31928950484081

Epoch: 6| Step: 9
Training loss: 2.1833038330078125
Validation loss: 2.268376450384817

Epoch: 6| Step: 10
Training loss: 2.3126378059387207
Validation loss: 2.2590409094287502

Epoch: 6| Step: 11
Training loss: 2.3463518619537354
Validation loss: 2.2374951608719362

Epoch: 6| Step: 12
Training loss: 3.103456497192383
Validation loss: 2.2206076601500153

Epoch: 6| Step: 13
Training loss: 2.499249219894409
Validation loss: 2.231225646952147

Epoch: 161| Step: 0
Training loss: 2.2181544303894043
Validation loss: 2.2748686805848153

Epoch: 6| Step: 1
Training loss: 3.5119071006774902
Validation loss: 2.283620334440662

Epoch: 6| Step: 2
Training loss: 2.388089179992676
Validation loss: 2.281488889007158

Epoch: 6| Step: 3
Training loss: 2.6976542472839355
Validation loss: 2.2741916641112296

Epoch: 6| Step: 4
Training loss: 2.6177868843078613
Validation loss: 2.2663615262636574

Epoch: 6| Step: 5
Training loss: 2.8208255767822266
Validation loss: 2.2549929080470914

Epoch: 6| Step: 6
Training loss: 2.128471851348877
Validation loss: 2.2464204321625414

Epoch: 6| Step: 7
Training loss: 2.70137882232666
Validation loss: 2.229952104630009

Epoch: 6| Step: 8
Training loss: 2.5679516792297363
Validation loss: 2.2343679422973306

Epoch: 6| Step: 9
Training loss: 2.4202628135681152
Validation loss: 2.2303857675162693

Epoch: 6| Step: 10
Training loss: 1.498522162437439
Validation loss: 2.2369504308187835

Epoch: 6| Step: 11
Training loss: 2.2980666160583496
Validation loss: 2.2350108251776746

Epoch: 6| Step: 12
Training loss: 2.4260425567626953
Validation loss: 2.2376275857289634

Epoch: 6| Step: 13
Training loss: 2.5639991760253906
Validation loss: 2.2565167642408803

Epoch: 162| Step: 0
Training loss: 2.507969856262207
Validation loss: 2.2717602637506302

Epoch: 6| Step: 1
Training loss: 2.3142716884613037
Validation loss: 2.2849978400814916

Epoch: 6| Step: 2
Training loss: 2.079089403152466
Validation loss: 2.2958278399641796

Epoch: 6| Step: 3
Training loss: 2.7153899669647217
Validation loss: 2.298737002957252

Epoch: 6| Step: 4
Training loss: 2.8167884349823
Validation loss: 2.2972198481200845

Epoch: 6| Step: 5
Training loss: 1.7810735702514648
Validation loss: 2.294243258814658

Epoch: 6| Step: 6
Training loss: 3.011133909225464
Validation loss: 2.2974981441292712

Epoch: 6| Step: 7
Training loss: 2.9867238998413086
Validation loss: 2.292892389400031

Epoch: 6| Step: 8
Training loss: 2.357301712036133
Validation loss: 2.2830666598453315

Epoch: 6| Step: 9
Training loss: 2.3980345726013184
Validation loss: 2.2771181098876463

Epoch: 6| Step: 10
Training loss: 2.70487642288208
Validation loss: 2.261084827043677

Epoch: 6| Step: 11
Training loss: 2.1926517486572266
Validation loss: 2.2600184768758793

Epoch: 6| Step: 12
Training loss: 2.383554458618164
Validation loss: 2.2361049100916874

Epoch: 6| Step: 13
Training loss: 2.236149787902832
Validation loss: 2.2217067133995796

Epoch: 163| Step: 0
Training loss: 2.1105945110321045
Validation loss: 2.228339607997607

Epoch: 6| Step: 1
Training loss: 2.8609108924865723
Validation loss: 2.2413830577686267

Epoch: 6| Step: 2
Training loss: 2.559699058532715
Validation loss: 2.250559514568698

Epoch: 6| Step: 3
Training loss: 3.0695366859436035
Validation loss: 2.2530443373546807

Epoch: 6| Step: 4
Training loss: 2.1001293659210205
Validation loss: 2.2335640127940843

Epoch: 6| Step: 5
Training loss: 2.5920166969299316
Validation loss: 2.2203391675026185

Epoch: 6| Step: 6
Training loss: 2.264834403991699
Validation loss: 2.222864261237524

Epoch: 6| Step: 7
Training loss: 2.1745553016662598
Validation loss: 2.229554614713115

Epoch: 6| Step: 8
Training loss: 2.5133860111236572
Validation loss: 2.2226115913801294

Epoch: 6| Step: 9
Training loss: 2.894928455352783
Validation loss: 2.21685593358932

Epoch: 6| Step: 10
Training loss: 2.1863396167755127
Validation loss: 2.2121532629894953

Epoch: 6| Step: 11
Training loss: 2.1449742317199707
Validation loss: 2.18866628472523

Epoch: 6| Step: 12
Training loss: 2.0128579139709473
Validation loss: 2.2009952837421047

Epoch: 6| Step: 13
Training loss: 3.5375826358795166
Validation loss: 2.194457414329693

Epoch: 164| Step: 0
Training loss: 3.107088327407837
Validation loss: 2.2040209039565055

Epoch: 6| Step: 1
Training loss: 2.2549617290496826
Validation loss: 2.199350533946868

Epoch: 6| Step: 2
Training loss: 2.260037422180176
Validation loss: 2.2135485320962887

Epoch: 6| Step: 3
Training loss: 2.7113723754882812
Validation loss: 2.2170759811196277

Epoch: 6| Step: 4
Training loss: 2.5722575187683105
Validation loss: 2.221610689675936

Epoch: 6| Step: 5
Training loss: 2.051431655883789
Validation loss: 2.2168162689414075

Epoch: 6| Step: 6
Training loss: 2.0018110275268555
Validation loss: 2.2194699036177767

Epoch: 6| Step: 7
Training loss: 3.1574997901916504
Validation loss: 2.2232970435132264

Epoch: 6| Step: 8
Training loss: 1.9076323509216309
Validation loss: 2.2091407340059996

Epoch: 6| Step: 9
Training loss: 2.0760931968688965
Validation loss: 2.208520870054922

Epoch: 6| Step: 10
Training loss: 3.1743593215942383
Validation loss: 2.2138817284696843

Epoch: 6| Step: 11
Training loss: 2.065530300140381
Validation loss: 2.21655781807438

Epoch: 6| Step: 12
Training loss: 2.6356823444366455
Validation loss: 2.2180219029867523

Epoch: 6| Step: 13
Training loss: 2.8636436462402344
Validation loss: 2.2115078690231487

Epoch: 165| Step: 0
Training loss: 2.8787903785705566
Validation loss: 2.2087630661585

Epoch: 6| Step: 1
Training loss: 2.2205891609191895
Validation loss: 2.1947236189278225

Epoch: 6| Step: 2
Training loss: 2.448244571685791
Validation loss: 2.195452283787471

Epoch: 6| Step: 3
Training loss: 2.16784930229187
Validation loss: 2.19845671807566

Epoch: 6| Step: 4
Training loss: 2.7544424533843994
Validation loss: 2.1917417075044368

Epoch: 6| Step: 5
Training loss: 1.8667194843292236
Validation loss: 2.187006242813603

Epoch: 6| Step: 6
Training loss: 2.1669416427612305
Validation loss: 2.17933597231424

Epoch: 6| Step: 7
Training loss: 2.1272521018981934
Validation loss: 2.1737276379780104

Epoch: 6| Step: 8
Training loss: 2.8082525730133057
Validation loss: 2.1802287947747017

Epoch: 6| Step: 9
Training loss: 2.4201254844665527
Validation loss: 2.1769848908147504

Epoch: 6| Step: 10
Training loss: 1.951840877532959
Validation loss: 2.184217632457774

Epoch: 6| Step: 11
Training loss: 2.698324680328369
Validation loss: 2.184344140432214

Epoch: 6| Step: 12
Training loss: 3.257429599761963
Validation loss: 2.185816213648806

Epoch: 6| Step: 13
Training loss: 2.737896680831909
Validation loss: 2.191804080881098

Epoch: 166| Step: 0
Training loss: 2.6368932723999023
Validation loss: 2.1892594522045505

Epoch: 6| Step: 1
Training loss: 1.8814380168914795
Validation loss: 2.208658372202227

Epoch: 6| Step: 2
Training loss: 2.8562963008880615
Validation loss: 2.210896525331723

Epoch: 6| Step: 3
Training loss: 2.487367630004883
Validation loss: 2.221122985245079

Epoch: 6| Step: 4
Training loss: 2.8216099739074707
Validation loss: 2.227642874563894

Epoch: 6| Step: 5
Training loss: 2.1936726570129395
Validation loss: 2.224847901252008

Epoch: 6| Step: 6
Training loss: 2.0156145095825195
Validation loss: 2.2150693478122836

Epoch: 6| Step: 7
Training loss: 3.200916051864624
Validation loss: 2.226894963172174

Epoch: 6| Step: 8
Training loss: 2.3371834754943848
Validation loss: 2.225290339480164

Epoch: 6| Step: 9
Training loss: 2.8475606441497803
Validation loss: 2.2350566156448854

Epoch: 6| Step: 10
Training loss: 2.346950054168701
Validation loss: 2.232166992720737

Epoch: 6| Step: 11
Training loss: 1.9560949802398682
Validation loss: 2.2353481118397047

Epoch: 6| Step: 12
Training loss: 1.7262310981750488
Validation loss: 2.2248579558505805

Epoch: 6| Step: 13
Training loss: 3.380434989929199
Validation loss: 2.2166677034029396

Epoch: 167| Step: 0
Training loss: 2.9754810333251953
Validation loss: 2.2071927952510055

Epoch: 6| Step: 1
Training loss: 2.758296012878418
Validation loss: 2.200686113808745

Epoch: 6| Step: 2
Training loss: 2.416743278503418
Validation loss: 2.190042416254679

Epoch: 6| Step: 3
Training loss: 2.4959983825683594
Validation loss: 2.1886102448227587

Epoch: 6| Step: 4
Training loss: 2.0696353912353516
Validation loss: 2.198418955649099

Epoch: 6| Step: 5
Training loss: 2.1242480278015137
Validation loss: 2.2059070961449736

Epoch: 6| Step: 6
Training loss: 2.761531352996826
Validation loss: 2.2213142354001283

Epoch: 6| Step: 7
Training loss: 2.151721954345703
Validation loss: 2.231435365574334

Epoch: 6| Step: 8
Training loss: 3.1377739906311035
Validation loss: 2.222529968907756

Epoch: 6| Step: 9
Training loss: 2.344986915588379
Validation loss: 2.2154760488899807

Epoch: 6| Step: 10
Training loss: 1.859671950340271
Validation loss: 2.2104962769375054

Epoch: 6| Step: 11
Training loss: 2.185965061187744
Validation loss: 2.2000008167759066

Epoch: 6| Step: 12
Training loss: 2.8784453868865967
Validation loss: 2.1891414491079186

Epoch: 6| Step: 13
Training loss: 2.0798730850219727
Validation loss: 2.198735196103332

Epoch: 168| Step: 0
Training loss: 2.06112003326416
Validation loss: 2.199737410391531

Epoch: 6| Step: 1
Training loss: 2.147580862045288
Validation loss: 2.2127956292962514

Epoch: 6| Step: 2
Training loss: 2.1996853351593018
Validation loss: 2.2225895773979927

Epoch: 6| Step: 3
Training loss: 3.3920037746429443
Validation loss: 2.252911839433896

Epoch: 6| Step: 4
Training loss: 2.8959391117095947
Validation loss: 2.274693053255799

Epoch: 6| Step: 5
Training loss: 2.2375950813293457
Validation loss: 2.2705707473139607

Epoch: 6| Step: 6
Training loss: 2.0272350311279297
Validation loss: 2.258082002721807

Epoch: 6| Step: 7
Training loss: 2.317582845687866
Validation loss: 2.2366934873724498

Epoch: 6| Step: 8
Training loss: 2.715573310852051
Validation loss: 2.2058022611884662

Epoch: 6| Step: 9
Training loss: 2.7393927574157715
Validation loss: 2.213091129897743

Epoch: 6| Step: 10
Training loss: 2.2990481853485107
Validation loss: 2.204836066051196

Epoch: 6| Step: 11
Training loss: 2.4491467475891113
Validation loss: 2.2059492475243023

Epoch: 6| Step: 12
Training loss: 2.7238657474517822
Validation loss: 2.208015395749

Epoch: 6| Step: 13
Training loss: 1.9817243814468384
Validation loss: 2.20147616376159

Epoch: 169| Step: 0
Training loss: 2.192319869995117
Validation loss: 2.1964591959471345

Epoch: 6| Step: 1
Training loss: 2.1154980659484863
Validation loss: 2.1980761763870076

Epoch: 6| Step: 2
Training loss: 2.424281358718872
Validation loss: 2.198320456730422

Epoch: 6| Step: 3
Training loss: 2.5646543502807617
Validation loss: 2.2048500866018315

Epoch: 6| Step: 4
Training loss: 2.1730620861053467
Validation loss: 2.221224977124122

Epoch: 6| Step: 5
Training loss: 2.3279531002044678
Validation loss: 2.2239570258766093

Epoch: 6| Step: 6
Training loss: 3.015740156173706
Validation loss: 2.221351877335579

Epoch: 6| Step: 7
Training loss: 3.5693931579589844
Validation loss: 2.230722876005275

Epoch: 6| Step: 8
Training loss: 2.324878692626953
Validation loss: 2.226353581233691

Epoch: 6| Step: 9
Training loss: 2.6053595542907715
Validation loss: 2.2203084935424147

Epoch: 6| Step: 10
Training loss: 2.385237693786621
Validation loss: 2.205308796257101

Epoch: 6| Step: 11
Training loss: 2.991138458251953
Validation loss: 2.1974916752948555

Epoch: 6| Step: 12
Training loss: 1.4619789123535156
Validation loss: 2.1896239096118557

Epoch: 6| Step: 13
Training loss: 1.7266851663589478
Validation loss: 2.1864288596696753

Epoch: 170| Step: 0
Training loss: 2.1050989627838135
Validation loss: 2.1881837665393786

Epoch: 6| Step: 1
Training loss: 2.171790361404419
Validation loss: 2.1968867291686354

Epoch: 6| Step: 2
Training loss: 2.7701919078826904
Validation loss: 2.1977399549176617

Epoch: 6| Step: 3
Training loss: 2.4135913848876953
Validation loss: 2.1998075413447555

Epoch: 6| Step: 4
Training loss: 2.3040316104888916
Validation loss: 2.204180125267275

Epoch: 6| Step: 5
Training loss: 2.099931240081787
Validation loss: 2.1999779234650316

Epoch: 6| Step: 6
Training loss: 1.8411359786987305
Validation loss: 2.2118122769940283

Epoch: 6| Step: 7
Training loss: 3.0367534160614014
Validation loss: 2.204929321042953

Epoch: 6| Step: 8
Training loss: 2.3220863342285156
Validation loss: 2.2218360926515315

Epoch: 6| Step: 9
Training loss: 2.645156145095825
Validation loss: 2.223010147771528

Epoch: 6| Step: 10
Training loss: 3.091207981109619
Validation loss: 2.2158339587591027

Epoch: 6| Step: 11
Training loss: 2.3202896118164062
Validation loss: 2.2094466314520886

Epoch: 6| Step: 12
Training loss: 2.325395107269287
Validation loss: 2.2236152233616

Epoch: 6| Step: 13
Training loss: 2.6472015380859375
Validation loss: 2.219483032021471

Epoch: 171| Step: 0
Training loss: 2.6467337608337402
Validation loss: 2.2267270229196034

Epoch: 6| Step: 1
Training loss: 2.2928109169006348
Validation loss: 2.2280763887589976

Epoch: 6| Step: 2
Training loss: 2.1948325634002686
Validation loss: 2.229074772968087

Epoch: 6| Step: 3
Training loss: 2.7983601093292236
Validation loss: 2.2182006976937734

Epoch: 6| Step: 4
Training loss: 2.8270926475524902
Validation loss: 2.2287224441446285

Epoch: 6| Step: 5
Training loss: 2.936636447906494
Validation loss: 2.2266994419918267

Epoch: 6| Step: 6
Training loss: 2.504854679107666
Validation loss: 2.2087334509818786

Epoch: 6| Step: 7
Training loss: 1.8167624473571777
Validation loss: 2.198724865913391

Epoch: 6| Step: 8
Training loss: 2.510904312133789
Validation loss: 2.2056899993650374

Epoch: 6| Step: 9
Training loss: 2.402194023132324
Validation loss: 2.2051175384111303

Epoch: 6| Step: 10
Training loss: 2.286198377609253
Validation loss: 2.2137449249144523

Epoch: 6| Step: 11
Training loss: 2.2200989723205566
Validation loss: 2.221722556698707

Epoch: 6| Step: 12
Training loss: 2.717665195465088
Validation loss: 2.2158703804016113

Epoch: 6| Step: 13
Training loss: 1.4938511848449707
Validation loss: 2.223961273829142

Epoch: 172| Step: 0
Training loss: 2.3155131340026855
Validation loss: 2.194798670789247

Epoch: 6| Step: 1
Training loss: 2.1467080116271973
Validation loss: 2.1848692688890683

Epoch: 6| Step: 2
Training loss: 1.9743531942367554
Validation loss: 2.1746485284579697

Epoch: 6| Step: 3
Training loss: 2.5585179328918457
Validation loss: 2.1947776886724655

Epoch: 6| Step: 4
Training loss: 2.5633647441864014
Validation loss: 2.199782298457238

Epoch: 6| Step: 5
Training loss: 2.175363779067993
Validation loss: 2.2048266203172746

Epoch: 6| Step: 6
Training loss: 2.547177791595459
Validation loss: 2.2071412532560286

Epoch: 6| Step: 7
Training loss: 2.431760787963867
Validation loss: 2.2102028451940066

Epoch: 6| Step: 8
Training loss: 3.3794565200805664
Validation loss: 2.203623266630275

Epoch: 6| Step: 9
Training loss: 2.6384849548339844
Validation loss: 2.210732803549818

Epoch: 6| Step: 10
Training loss: 2.397850275039673
Validation loss: 2.2304199613550657

Epoch: 6| Step: 11
Training loss: 2.400369644165039
Validation loss: 2.231702155964349

Epoch: 6| Step: 12
Training loss: 2.0813276767730713
Validation loss: 2.2250678590548936

Epoch: 6| Step: 13
Training loss: 2.5346710681915283
Validation loss: 2.225673311500139

Epoch: 173| Step: 0
Training loss: 2.4688892364501953
Validation loss: 2.2229356304291756

Epoch: 6| Step: 1
Training loss: 2.6794979572296143
Validation loss: 2.2085943145136677

Epoch: 6| Step: 2
Training loss: 1.8573131561279297
Validation loss: 2.207514842351278

Epoch: 6| Step: 3
Training loss: 3.315051555633545
Validation loss: 2.1999072644018356

Epoch: 6| Step: 4
Training loss: 2.0399017333984375
Validation loss: 2.2069959589230117

Epoch: 6| Step: 5
Training loss: 2.3957200050354004
Validation loss: 2.2143425864558064

Epoch: 6| Step: 6
Training loss: 2.3358917236328125
Validation loss: 2.2148743573055474

Epoch: 6| Step: 7
Training loss: 1.9171507358551025
Validation loss: 2.2222154781382573

Epoch: 6| Step: 8
Training loss: 2.8304429054260254
Validation loss: 2.214767571418516

Epoch: 6| Step: 9
Training loss: 2.145629644393921
Validation loss: 2.2195507326433734

Epoch: 6| Step: 10
Training loss: 2.4027814865112305
Validation loss: 2.220343462882503

Epoch: 6| Step: 11
Training loss: 2.5755748748779297
Validation loss: 2.213711157921822

Epoch: 6| Step: 12
Training loss: 2.736438751220703
Validation loss: 2.2208958774484615

Epoch: 6| Step: 13
Training loss: 2.076517343521118
Validation loss: 2.2210583533010175

Epoch: 174| Step: 0
Training loss: 2.5923657417297363
Validation loss: 2.2290177217093845

Epoch: 6| Step: 1
Training loss: 2.700838565826416
Validation loss: 2.244732772150347

Epoch: 6| Step: 2
Training loss: 2.3446033000946045
Validation loss: 2.23826604248375

Epoch: 6| Step: 3
Training loss: 2.149672031402588
Validation loss: 2.2301270320851314

Epoch: 6| Step: 4
Training loss: 2.1508147716522217
Validation loss: 2.2505627691104846

Epoch: 6| Step: 5
Training loss: 2.092500686645508
Validation loss: 2.236916706126223

Epoch: 6| Step: 6
Training loss: 2.5088443756103516
Validation loss: 2.2420281569163003

Epoch: 6| Step: 7
Training loss: 2.082895040512085
Validation loss: 2.2363365696322535

Epoch: 6| Step: 8
Training loss: 2.700894355773926
Validation loss: 2.227132725459273

Epoch: 6| Step: 9
Training loss: 2.6312766075134277
Validation loss: 2.195181103162868

Epoch: 6| Step: 10
Training loss: 2.1243715286254883
Validation loss: 2.185406026019845

Epoch: 6| Step: 11
Training loss: 2.3305721282958984
Validation loss: 2.186193637950446

Epoch: 6| Step: 12
Training loss: 2.824315309524536
Validation loss: 2.192712927377352

Epoch: 6| Step: 13
Training loss: 2.595654010772705
Validation loss: 2.2162492198328816

Epoch: 175| Step: 0
Training loss: 2.4999582767486572
Validation loss: 2.2385033792065037

Epoch: 6| Step: 1
Training loss: 2.5317487716674805
Validation loss: 2.256862658326344

Epoch: 6| Step: 2
Training loss: 2.8786187171936035
Validation loss: 2.2858340419748777

Epoch: 6| Step: 3
Training loss: 3.0298104286193848
Validation loss: 2.3181146267921693

Epoch: 6| Step: 4
Training loss: 1.8916963338851929
Validation loss: 2.3253459263873357

Epoch: 6| Step: 5
Training loss: 2.4912986755371094
Validation loss: 2.328649854147306

Epoch: 6| Step: 6
Training loss: 2.491140127182007
Validation loss: 2.330776842691565

Epoch: 6| Step: 7
Training loss: 2.172854423522949
Validation loss: 2.3361666407636417

Epoch: 6| Step: 8
Training loss: 2.7802248001098633
Validation loss: 2.3253456520777878

Epoch: 6| Step: 9
Training loss: 2.833815097808838
Validation loss: 2.3190371938931045

Epoch: 6| Step: 10
Training loss: 1.8615177869796753
Validation loss: 2.31233040748104

Epoch: 6| Step: 11
Training loss: 2.3320693969726562
Validation loss: 2.30535674864246

Epoch: 6| Step: 12
Training loss: 2.813138484954834
Validation loss: 2.2872906192656486

Epoch: 6| Step: 13
Training loss: 2.301147222518921
Validation loss: 2.2445929370900637

Epoch: 176| Step: 0
Training loss: 2.3600311279296875
Validation loss: 2.213129010251773

Epoch: 6| Step: 1
Training loss: 1.7310422658920288
Validation loss: 2.1906525063258346

Epoch: 6| Step: 2
Training loss: 2.674490451812744
Validation loss: 2.1792894973549792

Epoch: 6| Step: 3
Training loss: 3.146843433380127
Validation loss: 2.177748287877729

Epoch: 6| Step: 4
Training loss: 2.532421112060547
Validation loss: 2.1868952666559527

Epoch: 6| Step: 5
Training loss: 2.4875035285949707
Validation loss: 2.1771314015952488

Epoch: 6| Step: 6
Training loss: 1.7487412691116333
Validation loss: 2.172673422803161

Epoch: 6| Step: 7
Training loss: 2.529649257659912
Validation loss: 2.1719558085164716

Epoch: 6| Step: 8
Training loss: 2.0529446601867676
Validation loss: 2.176835856130046

Epoch: 6| Step: 9
Training loss: 2.148677110671997
Validation loss: 2.165789641359801

Epoch: 6| Step: 10
Training loss: 2.6574039459228516
Validation loss: 2.1778771133833033

Epoch: 6| Step: 11
Training loss: 2.2216711044311523
Validation loss: 2.1700435351299983

Epoch: 6| Step: 12
Training loss: 2.8767147064208984
Validation loss: 2.1908330378993863

Epoch: 6| Step: 13
Training loss: 2.739029884338379
Validation loss: 2.180728825189734

Epoch: 177| Step: 0
Training loss: 1.886127233505249
Validation loss: 2.1759859246592366

Epoch: 6| Step: 1
Training loss: 2.6645641326904297
Validation loss: 2.1823324875165055

Epoch: 6| Step: 2
Training loss: 2.827197551727295
Validation loss: 2.193326650127288

Epoch: 6| Step: 3
Training loss: 2.839155435562134
Validation loss: 2.1886157387046405

Epoch: 6| Step: 4
Training loss: 2.1005921363830566
Validation loss: 2.1983058426969793

Epoch: 6| Step: 5
Training loss: 2.485189914703369
Validation loss: 2.191786358433385

Epoch: 6| Step: 6
Training loss: 2.076279640197754
Validation loss: 2.1956391180715253

Epoch: 6| Step: 7
Training loss: 2.4249167442321777
Validation loss: 2.210065867311211

Epoch: 6| Step: 8
Training loss: 2.409135103225708
Validation loss: 2.209040923785138

Epoch: 6| Step: 9
Training loss: 2.423797130584717
Validation loss: 2.2208597249882196

Epoch: 6| Step: 10
Training loss: 2.109389066696167
Validation loss: 2.2100098030541533

Epoch: 6| Step: 11
Training loss: 2.6024255752563477
Validation loss: 2.2048438646460093

Epoch: 6| Step: 12
Training loss: 3.0715274810791016
Validation loss: 2.1939555445025043

Epoch: 6| Step: 13
Training loss: 1.3305315971374512
Validation loss: 2.200277884801229

Epoch: 178| Step: 0
Training loss: 2.4683942794799805
Validation loss: 2.1999119917551675

Epoch: 6| Step: 1
Training loss: 2.258953094482422
Validation loss: 2.232141379387148

Epoch: 6| Step: 2
Training loss: 3.016151189804077
Validation loss: 2.23358238897016

Epoch: 6| Step: 3
Training loss: 3.1896066665649414
Validation loss: 2.238434978710708

Epoch: 6| Step: 4
Training loss: 1.746873140335083
Validation loss: 2.233461041604319

Epoch: 6| Step: 5
Training loss: 2.437012195587158
Validation loss: 2.2137797391542824

Epoch: 6| Step: 6
Training loss: 2.6677095890045166
Validation loss: 2.1909870127195954

Epoch: 6| Step: 7
Training loss: 2.5214004516601562
Validation loss: 2.1846938927968345

Epoch: 6| Step: 8
Training loss: 2.403878688812256
Validation loss: 2.1903139288707445

Epoch: 6| Step: 9
Training loss: 1.985295057296753
Validation loss: 2.2034759034392652

Epoch: 6| Step: 10
Training loss: 2.1838057041168213
Validation loss: 2.1983828698435137

Epoch: 6| Step: 11
Training loss: 1.9736731052398682
Validation loss: 2.2032298400837886

Epoch: 6| Step: 12
Training loss: 2.3513338565826416
Validation loss: 2.2113082998542377

Epoch: 6| Step: 13
Training loss: 2.841525077819824
Validation loss: 2.228972306815527

Epoch: 179| Step: 0
Training loss: 2.3345510959625244
Validation loss: 2.2155141343352613

Epoch: 6| Step: 1
Training loss: 2.347139358520508
Validation loss: 2.206777398304273

Epoch: 6| Step: 2
Training loss: 2.999570369720459
Validation loss: 2.1968268771325388

Epoch: 6| Step: 3
Training loss: 2.1880483627319336
Validation loss: 2.20277310955909

Epoch: 6| Step: 4
Training loss: 1.9020936489105225
Validation loss: 2.2135822311524422

Epoch: 6| Step: 5
Training loss: 2.350381851196289
Validation loss: 2.223209596449329

Epoch: 6| Step: 6
Training loss: 2.519984245300293
Validation loss: 2.2379671360856745

Epoch: 6| Step: 7
Training loss: 3.036309242248535
Validation loss: 2.2469345856738347

Epoch: 6| Step: 8
Training loss: 2.248483657836914
Validation loss: 2.2415911843699794

Epoch: 6| Step: 9
Training loss: 2.3762049674987793
Validation loss: 2.227292573580178

Epoch: 6| Step: 10
Training loss: 1.9053088426589966
Validation loss: 2.227927787329561

Epoch: 6| Step: 11
Training loss: 2.281017780303955
Validation loss: 2.2032979867791616

Epoch: 6| Step: 12
Training loss: 2.3794775009155273
Validation loss: 2.2078050362166537

Epoch: 6| Step: 13
Training loss: 3.223271369934082
Validation loss: 2.1848115062200897

Epoch: 180| Step: 0
Training loss: 3.484990119934082
Validation loss: 2.179935178449077

Epoch: 6| Step: 1
Training loss: 1.4417383670806885
Validation loss: 2.163885206304571

Epoch: 6| Step: 2
Training loss: 2.4585649967193604
Validation loss: 2.1648587180722143

Epoch: 6| Step: 3
Training loss: 2.2618236541748047
Validation loss: 2.148291982630248

Epoch: 6| Step: 4
Training loss: 2.545146942138672
Validation loss: 2.1677348459920576

Epoch: 6| Step: 5
Training loss: 2.670963764190674
Validation loss: 2.169952510505594

Epoch: 6| Step: 6
Training loss: 2.0634615421295166
Validation loss: 2.179403958782073

Epoch: 6| Step: 7
Training loss: 2.394186496734619
Validation loss: 2.179098358718298

Epoch: 6| Step: 8
Training loss: 2.4416496753692627
Validation loss: 2.198140395584927

Epoch: 6| Step: 9
Training loss: 2.3021421432495117
Validation loss: 2.1723066478647213

Epoch: 6| Step: 10
Training loss: 2.4674158096313477
Validation loss: 2.176567218636954

Epoch: 6| Step: 11
Training loss: 2.782884120941162
Validation loss: 2.1734325475590204

Epoch: 6| Step: 12
Training loss: 2.474146604537964
Validation loss: 2.1661405691536526

Epoch: 6| Step: 13
Training loss: 2.1907429695129395
Validation loss: 2.1794794926079373

Epoch: 181| Step: 0
Training loss: 2.2138891220092773
Validation loss: 2.196404513492379

Epoch: 6| Step: 1
Training loss: 2.1447319984436035
Validation loss: 2.194726505587178

Epoch: 6| Step: 2
Training loss: 2.3230817317962646
Validation loss: 2.2046182617064445

Epoch: 6| Step: 3
Training loss: 2.214507818222046
Validation loss: 2.208011169587412

Epoch: 6| Step: 4
Training loss: 2.6541378498077393
Validation loss: 2.2196868081246652

Epoch: 6| Step: 5
Training loss: 3.0324015617370605
Validation loss: 2.2070587040275655

Epoch: 6| Step: 6
Training loss: 1.7650030851364136
Validation loss: 2.2056436436150664

Epoch: 6| Step: 7
Training loss: 2.32572603225708
Validation loss: 2.195009385385821

Epoch: 6| Step: 8
Training loss: 1.932093620300293
Validation loss: 2.190436758020873

Epoch: 6| Step: 9
Training loss: 2.2820069789886475
Validation loss: 2.180719885774838

Epoch: 6| Step: 10
Training loss: 2.907776355743408
Validation loss: 2.1964879266677366

Epoch: 6| Step: 11
Training loss: 2.6404290199279785
Validation loss: 2.22326438273153

Epoch: 6| Step: 12
Training loss: 2.8442773818969727
Validation loss: 2.2481292960464314

Epoch: 6| Step: 13
Training loss: 2.6529276371002197
Validation loss: 2.2476890471673783

Epoch: 182| Step: 0
Training loss: 2.073160409927368
Validation loss: 2.23845467387989

Epoch: 6| Step: 1
Training loss: 2.1082816123962402
Validation loss: 2.2133833605756044

Epoch: 6| Step: 2
Training loss: 3.031681537628174
Validation loss: 2.1883562085449055

Epoch: 6| Step: 3
Training loss: 2.8591864109039307
Validation loss: 2.1756273443980882

Epoch: 6| Step: 4
Training loss: 2.334641456604004
Validation loss: 2.164059099330697

Epoch: 6| Step: 5
Training loss: 1.791530728340149
Validation loss: 2.155851899936635

Epoch: 6| Step: 6
Training loss: 1.9727003574371338
Validation loss: 2.161523198568693

Epoch: 6| Step: 7
Training loss: 2.6148197650909424
Validation loss: 2.1646889448165894

Epoch: 6| Step: 8
Training loss: 2.3984224796295166
Validation loss: 2.158551476335013

Epoch: 6| Step: 9
Training loss: 2.5236239433288574
Validation loss: 2.1624345420509257

Epoch: 6| Step: 10
Training loss: 2.917985439300537
Validation loss: 2.1600826760774017

Epoch: 6| Step: 11
Training loss: 2.060880661010742
Validation loss: 2.168232105111563

Epoch: 6| Step: 12
Training loss: 2.4110679626464844
Validation loss: 2.1701881731710126

Epoch: 6| Step: 13
Training loss: 2.2927868366241455
Validation loss: 2.168624931766141

Epoch: 183| Step: 0
Training loss: 2.316467761993408
Validation loss: 2.1795755458134476

Epoch: 6| Step: 1
Training loss: 2.2092738151550293
Validation loss: 2.1838767772079795

Epoch: 6| Step: 2
Training loss: 2.0696516036987305
Validation loss: 2.1864301978900866

Epoch: 6| Step: 3
Training loss: 1.9870860576629639
Validation loss: 2.184193972618349

Epoch: 6| Step: 4
Training loss: 2.4157586097717285
Validation loss: 2.2169826979278238

Epoch: 6| Step: 5
Training loss: 2.35014009475708
Validation loss: 2.2352265491280505

Epoch: 6| Step: 6
Training loss: 1.9260424375534058
Validation loss: 2.2396082878112793

Epoch: 6| Step: 7
Training loss: 3.2892279624938965
Validation loss: 2.259479234295507

Epoch: 6| Step: 8
Training loss: 2.3016138076782227
Validation loss: 2.2401817844760035

Epoch: 6| Step: 9
Training loss: 2.9255869388580322
Validation loss: 2.2457346403470604

Epoch: 6| Step: 10
Training loss: 2.7918648719787598
Validation loss: 2.248355645005421

Epoch: 6| Step: 11
Training loss: 2.369870185852051
Validation loss: 2.2238226500890588

Epoch: 6| Step: 12
Training loss: 2.212860584259033
Validation loss: 2.2194071162131523

Epoch: 6| Step: 13
Training loss: 2.4080848693847656
Validation loss: 2.20590288664705

Epoch: 184| Step: 0
Training loss: 2.066493511199951
Validation loss: 2.2024496486110072

Epoch: 6| Step: 1
Training loss: 2.2033557891845703
Validation loss: 2.1828380938499206

Epoch: 6| Step: 2
Training loss: 2.5104708671569824
Validation loss: 2.1681658144920104

Epoch: 6| Step: 3
Training loss: 1.9814379215240479
Validation loss: 2.155720623590613

Epoch: 6| Step: 4
Training loss: 2.272759437561035
Validation loss: 2.152319977360387

Epoch: 6| Step: 5
Training loss: 1.4169694185256958
Validation loss: 2.136589591221143

Epoch: 6| Step: 6
Training loss: 2.6255440711975098
Validation loss: 2.1390048393639187

Epoch: 6| Step: 7
Training loss: 1.9549287557601929
Validation loss: 2.1351079043521675

Epoch: 6| Step: 8
Training loss: 2.633131504058838
Validation loss: 2.133896637988347

Epoch: 6| Step: 9
Training loss: 2.753326892852783
Validation loss: 2.131659360342128

Epoch: 6| Step: 10
Training loss: 2.371450424194336
Validation loss: 2.1334640800312

Epoch: 6| Step: 11
Training loss: 3.1605401039123535
Validation loss: 2.1418054257669756

Epoch: 6| Step: 12
Training loss: 3.0677905082702637
Validation loss: 2.1482148721653926

Epoch: 6| Step: 13
Training loss: 2.3471076488494873
Validation loss: 2.1472610581305718

Epoch: 185| Step: 0
Training loss: 2.412066698074341
Validation loss: 2.15567171958185

Epoch: 6| Step: 1
Training loss: 2.1671745777130127
Validation loss: 2.161825356944915

Epoch: 6| Step: 2
Training loss: 3.009744882583618
Validation loss: 2.170226699562483

Epoch: 6| Step: 3
Training loss: 2.532008171081543
Validation loss: 2.167769996068811

Epoch: 6| Step: 4
Training loss: 2.5231900215148926
Validation loss: 2.1726717000366538

Epoch: 6| Step: 5
Training loss: 3.0356686115264893
Validation loss: 2.1818020856508644

Epoch: 6| Step: 6
Training loss: 2.5941526889801025
Validation loss: 2.1993300299490652

Epoch: 6| Step: 7
Training loss: 2.4672577381134033
Validation loss: 2.1972103400896956

Epoch: 6| Step: 8
Training loss: 2.20448637008667
Validation loss: 2.202328388408948

Epoch: 6| Step: 9
Training loss: 2.1693174839019775
Validation loss: 2.191898630511376

Epoch: 6| Step: 10
Training loss: 2.542320728302002
Validation loss: 2.2093966417415167

Epoch: 6| Step: 11
Training loss: 1.9517875909805298
Validation loss: 2.195478816186228

Epoch: 6| Step: 12
Training loss: 1.5070726871490479
Validation loss: 2.1966980477815032

Epoch: 6| Step: 13
Training loss: 1.6743475198745728
Validation loss: 2.185574321336644

Epoch: 186| Step: 0
Training loss: 2.6661276817321777
Validation loss: 2.199494484932192

Epoch: 6| Step: 1
Training loss: 2.6301448345184326
Validation loss: 2.199193243057497

Epoch: 6| Step: 2
Training loss: 2.844376564025879
Validation loss: 2.2038226819807485

Epoch: 6| Step: 3
Training loss: 1.9541863203048706
Validation loss: 2.193775212892922

Epoch: 6| Step: 4
Training loss: 2.4260661602020264
Validation loss: 2.2156443929159515

Epoch: 6| Step: 5
Training loss: 1.3457523584365845
Validation loss: 2.2039597060090754

Epoch: 6| Step: 6
Training loss: 2.6313326358795166
Validation loss: 2.2011253244133404

Epoch: 6| Step: 7
Training loss: 2.389054536819458
Validation loss: 2.199836436138358

Epoch: 6| Step: 8
Training loss: 2.327859401702881
Validation loss: 2.1953111489613852

Epoch: 6| Step: 9
Training loss: 2.592209815979004
Validation loss: 2.1881544923269622

Epoch: 6| Step: 10
Training loss: 2.643314838409424
Validation loss: 2.1723268621711322

Epoch: 6| Step: 11
Training loss: 2.2965564727783203
Validation loss: 2.1815400943961194

Epoch: 6| Step: 12
Training loss: 2.5134167671203613
Validation loss: 2.175398470253073

Epoch: 6| Step: 13
Training loss: 1.3971657752990723
Validation loss: 2.175666909064016

Epoch: 187| Step: 0
Training loss: 1.7857162952423096
Validation loss: 2.1788433033932924

Epoch: 6| Step: 1
Training loss: 1.7391343116760254
Validation loss: 2.183130773164893

Epoch: 6| Step: 2
Training loss: 2.517916202545166
Validation loss: 2.186584859765986

Epoch: 6| Step: 3
Training loss: 2.634742259979248
Validation loss: 2.207173737146521

Epoch: 6| Step: 4
Training loss: 2.9484550952911377
Validation loss: 2.212110152808569

Epoch: 6| Step: 5
Training loss: 1.9713943004608154
Validation loss: 2.2118059768471667

Epoch: 6| Step: 6
Training loss: 2.2329628467559814
Validation loss: 2.2214212776512228

Epoch: 6| Step: 7
Training loss: 2.9124531745910645
Validation loss: 2.196286186095207

Epoch: 6| Step: 8
Training loss: 2.698662757873535
Validation loss: 2.1792457206274873

Epoch: 6| Step: 9
Training loss: 2.674577236175537
Validation loss: 2.1714769101911977

Epoch: 6| Step: 10
Training loss: 2.0503344535827637
Validation loss: 2.1578872896009877

Epoch: 6| Step: 11
Training loss: 2.606800079345703
Validation loss: 2.1480362543495755

Epoch: 6| Step: 12
Training loss: 1.6300402879714966
Validation loss: 2.1541444037550237

Epoch: 6| Step: 13
Training loss: 2.8998842239379883
Validation loss: 2.1521044277375743

Epoch: 188| Step: 0
Training loss: 2.0760302543640137
Validation loss: 2.158445683858728

Epoch: 6| Step: 1
Training loss: 2.6483023166656494
Validation loss: 2.1500980469488327

Epoch: 6| Step: 2
Training loss: 1.8013476133346558
Validation loss: 2.1524039788912703

Epoch: 6| Step: 3
Training loss: 2.9285998344421387
Validation loss: 2.1527722804777083

Epoch: 6| Step: 4
Training loss: 1.922534704208374
Validation loss: 2.157874253488356

Epoch: 6| Step: 5
Training loss: 2.7871644496917725
Validation loss: 2.1648785209143036

Epoch: 6| Step: 6
Training loss: 2.501739501953125
Validation loss: 2.1783209949411373

Epoch: 6| Step: 7
Training loss: 2.2925779819488525
Validation loss: 2.1963811535989084

Epoch: 6| Step: 8
Training loss: 2.3614375591278076
Validation loss: 2.197566250319122

Epoch: 6| Step: 9
Training loss: 2.032902479171753
Validation loss: 2.202982743581136

Epoch: 6| Step: 10
Training loss: 2.377349853515625
Validation loss: 2.1947604507528324

Epoch: 6| Step: 11
Training loss: 1.8570305109024048
Validation loss: 2.1899642098334526

Epoch: 6| Step: 12
Training loss: 2.6490249633789062
Validation loss: 2.1970252298539683

Epoch: 6| Step: 13
Training loss: 2.89528226852417
Validation loss: 2.180480126411684

Epoch: 189| Step: 0
Training loss: 2.1057257652282715
Validation loss: 2.1803078548882597

Epoch: 6| Step: 1
Training loss: 2.257894515991211
Validation loss: 2.1736047742187337

Epoch: 6| Step: 2
Training loss: 2.257585048675537
Validation loss: 2.1831952166813675

Epoch: 6| Step: 3
Training loss: 2.8703150749206543
Validation loss: 2.1785055745032524

Epoch: 6| Step: 4
Training loss: 2.350834846496582
Validation loss: 2.1809112346300514

Epoch: 6| Step: 5
Training loss: 2.223613739013672
Validation loss: 2.1908498797365414

Epoch: 6| Step: 6
Training loss: 2.5870485305786133
Validation loss: 2.1705978198718

Epoch: 6| Step: 7
Training loss: 2.382017135620117
Validation loss: 2.1862112040160806

Epoch: 6| Step: 8
Training loss: 1.8921763896942139
Validation loss: 2.1802072730115665

Epoch: 6| Step: 9
Training loss: 2.262613534927368
Validation loss: 2.1692840335189656

Epoch: 6| Step: 10
Training loss: 2.902782917022705
Validation loss: 2.1700116639496176

Epoch: 6| Step: 11
Training loss: 1.8901095390319824
Validation loss: 2.1762480325596307

Epoch: 6| Step: 12
Training loss: 2.5607709884643555
Validation loss: 2.1713562755174536

Epoch: 6| Step: 13
Training loss: 2.268989324569702
Validation loss: 2.18153473125991

Epoch: 190| Step: 0
Training loss: 2.4459309577941895
Validation loss: 2.1747668212459934

Epoch: 6| Step: 1
Training loss: 3.0465288162231445
Validation loss: 2.1481478842355872

Epoch: 6| Step: 2
Training loss: 2.408787250518799
Validation loss: 2.1480624278386435

Epoch: 6| Step: 3
Training loss: 2.4225003719329834
Validation loss: 2.1474805314053773

Epoch: 6| Step: 4
Training loss: 2.0431628227233887
Validation loss: 2.155205480514034

Epoch: 6| Step: 5
Training loss: 2.4064078330993652
Validation loss: 2.1567240684263167

Epoch: 6| Step: 6
Training loss: 2.3816733360290527
Validation loss: 2.163136556584348

Epoch: 6| Step: 7
Training loss: 2.157813787460327
Validation loss: 2.181596968763618

Epoch: 6| Step: 8
Training loss: 2.7171404361724854
Validation loss: 2.1958700251835648

Epoch: 6| Step: 9
Training loss: 2.209920883178711
Validation loss: 2.197754613814815

Epoch: 6| Step: 10
Training loss: 1.8873238563537598
Validation loss: 2.2301658353497906

Epoch: 6| Step: 11
Training loss: 2.156552314758301
Validation loss: 2.2606319663345174

Epoch: 6| Step: 12
Training loss: 2.396165609359741
Validation loss: 2.266281471457533

Epoch: 6| Step: 13
Training loss: 2.040376663208008
Validation loss: 2.2921814559608378

Epoch: 191| Step: 0
Training loss: 2.327509880065918
Validation loss: 2.2727175656185357

Epoch: 6| Step: 1
Training loss: 2.232193946838379
Validation loss: 2.2513912159909486

Epoch: 6| Step: 2
Training loss: 2.5523834228515625
Validation loss: 2.2273125622862127

Epoch: 6| Step: 3
Training loss: 3.3787851333618164
Validation loss: 2.204066902078608

Epoch: 6| Step: 4
Training loss: 2.453443765640259
Validation loss: 2.171853778182819

Epoch: 6| Step: 5
Training loss: 2.1780283451080322
Validation loss: 2.152761818260275

Epoch: 6| Step: 6
Training loss: 2.7414042949676514
Validation loss: 2.1568689910314416

Epoch: 6| Step: 7
Training loss: 2.72267484664917
Validation loss: 2.1693437099456787

Epoch: 6| Step: 8
Training loss: 2.2024917602539062
Validation loss: 2.1733742721619143

Epoch: 6| Step: 9
Training loss: 2.4040215015411377
Validation loss: 2.1649365425109863

Epoch: 6| Step: 10
Training loss: 2.127290725708008
Validation loss: 2.174384834945843

Epoch: 6| Step: 11
Training loss: 1.154736042022705
Validation loss: 2.1609841239067817

Epoch: 6| Step: 12
Training loss: 2.387723207473755
Validation loss: 2.1563003563111827

Epoch: 6| Step: 13
Training loss: 1.9980852603912354
Validation loss: 2.157117589827507

Epoch: 192| Step: 0
Training loss: 2.505645275115967
Validation loss: 2.1423380887636574

Epoch: 6| Step: 1
Training loss: 2.8152475357055664
Validation loss: 2.1427417955090924

Epoch: 6| Step: 2
Training loss: 2.443507671356201
Validation loss: 2.128788712204144

Epoch: 6| Step: 3
Training loss: 2.058627128601074
Validation loss: 2.12794320044979

Epoch: 6| Step: 4
Training loss: 2.259495973587036
Validation loss: 2.1253578393690047

Epoch: 6| Step: 5
Training loss: 1.5136011838912964
Validation loss: 2.12829997206247

Epoch: 6| Step: 6
Training loss: 2.176379680633545
Validation loss: 2.1303887213430097

Epoch: 6| Step: 7
Training loss: 3.467921733856201
Validation loss: 2.1268096739246

Epoch: 6| Step: 8
Training loss: 2.1511435508728027
Validation loss: 2.1300039675927933

Epoch: 6| Step: 9
Training loss: 1.475760817527771
Validation loss: 2.130607174288842

Epoch: 6| Step: 10
Training loss: 2.9249346256256104
Validation loss: 2.1378188222967167

Epoch: 6| Step: 11
Training loss: 1.8997876644134521
Validation loss: 2.1405843868050525

Epoch: 6| Step: 12
Training loss: 2.5100150108337402
Validation loss: 2.1438359022140503

Epoch: 6| Step: 13
Training loss: 2.7696783542633057
Validation loss: 2.150043891322228

Epoch: 193| Step: 0
Training loss: 2.317943572998047
Validation loss: 2.152234702981928

Epoch: 6| Step: 1
Training loss: 2.358804941177368
Validation loss: 2.1624874530300016

Epoch: 6| Step: 2
Training loss: 2.0208115577697754
Validation loss: 2.1788082161257343

Epoch: 6| Step: 3
Training loss: 1.9718269109725952
Validation loss: 2.1945869538091842

Epoch: 6| Step: 4
Training loss: 2.063108444213867
Validation loss: 2.2100276947021484

Epoch: 6| Step: 5
Training loss: 2.6382594108581543
Validation loss: 2.2300306622700026

Epoch: 6| Step: 6
Training loss: 2.4349803924560547
Validation loss: 2.224982870522366

Epoch: 6| Step: 7
Training loss: 3.1205763816833496
Validation loss: 2.204426657768988

Epoch: 6| Step: 8
Training loss: 2.8189101219177246
Validation loss: 2.1884895870762486

Epoch: 6| Step: 9
Training loss: 1.894898533821106
Validation loss: 2.184941132863363

Epoch: 6| Step: 10
Training loss: 2.815066337585449
Validation loss: 2.162126620610555

Epoch: 6| Step: 11
Training loss: 1.4331806898117065
Validation loss: 2.1777153348410003

Epoch: 6| Step: 12
Training loss: 2.4208712577819824
Validation loss: 2.1738042421238397

Epoch: 6| Step: 13
Training loss: 2.8174021244049072
Validation loss: 2.1651729101775796

Epoch: 194| Step: 0
Training loss: 2.514418125152588
Validation loss: 2.1539467098892375

Epoch: 6| Step: 1
Training loss: 2.781796455383301
Validation loss: 2.1297442451600106

Epoch: 6| Step: 2
Training loss: 2.1472578048706055
Validation loss: 2.122722077113326

Epoch: 6| Step: 3
Training loss: 2.269421100616455
Validation loss: 2.1287789267878376

Epoch: 6| Step: 4
Training loss: 2.3731274604797363
Validation loss: 2.1298703044973393

Epoch: 6| Step: 5
Training loss: 1.7933459281921387
Validation loss: 2.1357027856252526

Epoch: 6| Step: 6
Training loss: 2.1890358924865723
Validation loss: 2.1439536361284155

Epoch: 6| Step: 7
Training loss: 1.6339857578277588
Validation loss: 2.1431661459707443

Epoch: 6| Step: 8
Training loss: 2.0409862995147705
Validation loss: 2.1460119575582524

Epoch: 6| Step: 9
Training loss: 2.4660298824310303
Validation loss: 2.1584518353144326

Epoch: 6| Step: 10
Training loss: 1.7751853466033936
Validation loss: 2.168161587048602

Epoch: 6| Step: 11
Training loss: 2.585524082183838
Validation loss: 2.174980971121019

Epoch: 6| Step: 12
Training loss: 3.4134469032287598
Validation loss: 2.1717049306438816

Epoch: 6| Step: 13
Training loss: 2.82894229888916
Validation loss: 2.173259342870405

Epoch: 195| Step: 0
Training loss: 2.650052785873413
Validation loss: 2.1748487859643917

Epoch: 6| Step: 1
Training loss: 1.8836541175842285
Validation loss: 2.173540087156398

Epoch: 6| Step: 2
Training loss: 2.8334808349609375
Validation loss: 2.1558123352707073

Epoch: 6| Step: 3
Training loss: 1.824392557144165
Validation loss: 2.164906167214917

Epoch: 6| Step: 4
Training loss: 3.2060208320617676
Validation loss: 2.1689171047620874

Epoch: 6| Step: 5
Training loss: 2.517211675643921
Validation loss: 2.1636751390272573

Epoch: 6| Step: 6
Training loss: 1.748622179031372
Validation loss: 2.173529453175042

Epoch: 6| Step: 7
Training loss: 2.267056465148926
Validation loss: 2.1546522289194088

Epoch: 6| Step: 8
Training loss: 2.9709715843200684
Validation loss: 2.1549205139119136

Epoch: 6| Step: 9
Training loss: 2.0719590187072754
Validation loss: 2.1565209050332346

Epoch: 6| Step: 10
Training loss: 2.139507293701172
Validation loss: 2.1570758127397105

Epoch: 6| Step: 11
Training loss: 2.3067116737365723
Validation loss: 2.15224818260439

Epoch: 6| Step: 12
Training loss: 1.9828617572784424
Validation loss: 2.1476471885558097

Epoch: 6| Step: 13
Training loss: 1.8902969360351562
Validation loss: 2.128524418800108

Epoch: 196| Step: 0
Training loss: 2.418382406234741
Validation loss: 2.124655480025917

Epoch: 6| Step: 1
Training loss: 2.3762564659118652
Validation loss: 2.136084048978744

Epoch: 6| Step: 2
Training loss: 1.867139220237732
Validation loss: 2.1429415056782384

Epoch: 6| Step: 3
Training loss: 2.226548671722412
Validation loss: 2.15503998725645

Epoch: 6| Step: 4
Training loss: 2.882917881011963
Validation loss: 2.166865171924714

Epoch: 6| Step: 5
Training loss: 2.4282355308532715
Validation loss: 2.1712036184085313

Epoch: 6| Step: 6
Training loss: 2.3651390075683594
Validation loss: 2.161272128423055

Epoch: 6| Step: 7
Training loss: 2.278252601623535
Validation loss: 2.15316318696545

Epoch: 6| Step: 8
Training loss: 2.341398000717163
Validation loss: 2.1404085633575276

Epoch: 6| Step: 9
Training loss: 2.3630619049072266
Validation loss: 2.1434484476684244

Epoch: 6| Step: 10
Training loss: 2.6166696548461914
Validation loss: 2.1684073555854058

Epoch: 6| Step: 11
Training loss: 1.8509231805801392
Validation loss: 2.192204454893707

Epoch: 6| Step: 12
Training loss: 2.3349499702453613
Validation loss: 2.2200035792525097

Epoch: 6| Step: 13
Training loss: 2.2656052112579346
Validation loss: 2.215954129413892

Epoch: 197| Step: 0
Training loss: 2.8906843662261963
Validation loss: 2.2076619184145363

Epoch: 6| Step: 1
Training loss: 2.3158822059631348
Validation loss: 2.1555528538201445

Epoch: 6| Step: 2
Training loss: 2.2435708045959473
Validation loss: 2.1513545666971514

Epoch: 6| Step: 3
Training loss: 1.9259337186813354
Validation loss: 2.12721686978494

Epoch: 6| Step: 4
Training loss: 3.014496088027954
Validation loss: 2.1292634548679477

Epoch: 6| Step: 5
Training loss: 2.8758554458618164
Validation loss: 2.1172264263194096

Epoch: 6| Step: 6
Training loss: 2.380059242248535
Validation loss: 2.1255227058164534

Epoch: 6| Step: 7
Training loss: 2.109525203704834
Validation loss: 2.1593073260399605

Epoch: 6| Step: 8
Training loss: 2.5218029022216797
Validation loss: 2.1692051118420017

Epoch: 6| Step: 9
Training loss: 2.1138978004455566
Validation loss: 2.185840561825742

Epoch: 6| Step: 10
Training loss: 2.322328567504883
Validation loss: 2.2029777803728656

Epoch: 6| Step: 11
Training loss: 1.3538869619369507
Validation loss: 2.2070474342633317

Epoch: 6| Step: 12
Training loss: 1.7393579483032227
Validation loss: 2.206448019191783

Epoch: 6| Step: 13
Training loss: 3.2258639335632324
Validation loss: 2.2035314318954304

Epoch: 198| Step: 0
Training loss: 2.748448371887207
Validation loss: 2.207533280054728

Epoch: 6| Step: 1
Training loss: 2.4176902770996094
Validation loss: 2.201474585840779

Epoch: 6| Step: 2
Training loss: 2.4197258949279785
Validation loss: 2.1786945173817296

Epoch: 6| Step: 3
Training loss: 2.2462270259857178
Validation loss: 2.159601552512056

Epoch: 6| Step: 4
Training loss: 2.680626153945923
Validation loss: 2.150818045421313

Epoch: 6| Step: 5
Training loss: 2.1901121139526367
Validation loss: 2.149950304339009

Epoch: 6| Step: 6
Training loss: 2.622493267059326
Validation loss: 2.1415053400942075

Epoch: 6| Step: 7
Training loss: 2.6202564239501953
Validation loss: 2.142298247224541

Epoch: 6| Step: 8
Training loss: 2.0192434787750244
Validation loss: 2.1206157233125422

Epoch: 6| Step: 9
Training loss: 1.7137514352798462
Validation loss: 2.1063265364657164

Epoch: 6| Step: 10
Training loss: 1.8531193733215332
Validation loss: 2.101954822899193

Epoch: 6| Step: 11
Training loss: 2.3658387660980225
Validation loss: 2.1038454501859603

Epoch: 6| Step: 12
Training loss: 2.4868974685668945
Validation loss: 2.1096855184083343

Epoch: 6| Step: 13
Training loss: 1.85856294631958
Validation loss: 2.1138779565852177

Epoch: 199| Step: 0
Training loss: 2.6260461807250977
Validation loss: 2.1294775983338714

Epoch: 6| Step: 1
Training loss: 2.47965669631958
Validation loss: 2.1345088161447996

Epoch: 6| Step: 2
Training loss: 1.3051533699035645
Validation loss: 2.1405542089093115

Epoch: 6| Step: 3
Training loss: 2.6340341567993164
Validation loss: 2.148056519928799

Epoch: 6| Step: 4
Training loss: 3.3472962379455566
Validation loss: 2.1417506305120324

Epoch: 6| Step: 5
Training loss: 2.3799915313720703
Validation loss: 2.1527200565543225

Epoch: 6| Step: 6
Training loss: 2.0732789039611816
Validation loss: 2.1556511438021095

Epoch: 6| Step: 7
Training loss: 2.2955470085144043
Validation loss: 2.1729446021459435

Epoch: 6| Step: 8
Training loss: 2.3888943195343018
Validation loss: 2.207314819417974

Epoch: 6| Step: 9
Training loss: 2.6248228549957275
Validation loss: 2.2441153013578026

Epoch: 6| Step: 10
Training loss: 2.416158676147461
Validation loss: 2.2717734203543714

Epoch: 6| Step: 11
Training loss: 2.18131947517395
Validation loss: 2.29418779957679

Epoch: 6| Step: 12
Training loss: 2.1404824256896973
Validation loss: 2.277589979992118

Epoch: 6| Step: 13
Training loss: 1.0524839162826538
Validation loss: 2.2323031399839666

Epoch: 200| Step: 0
Training loss: 2.2159934043884277
Validation loss: 2.197650299277357

Epoch: 6| Step: 1
Training loss: 2.790009021759033
Validation loss: 2.1810766907148462

Epoch: 6| Step: 2
Training loss: 1.952876329421997
Validation loss: 2.1916825284240065

Epoch: 6| Step: 3
Training loss: 3.2604777812957764
Validation loss: 2.2063182092482045

Epoch: 6| Step: 4
Training loss: 1.5664541721343994
Validation loss: 2.2135374264050554

Epoch: 6| Step: 5
Training loss: 1.748291015625
Validation loss: 2.210766271878314

Epoch: 6| Step: 6
Training loss: 2.1740188598632812
Validation loss: 2.1697121256141254

Epoch: 6| Step: 7
Training loss: 2.9530954360961914
Validation loss: 2.148679535876038

Epoch: 6| Step: 8
Training loss: 2.256937026977539
Validation loss: 2.1260992198862056

Epoch: 6| Step: 9
Training loss: 2.1130895614624023
Validation loss: 2.1179369957216325

Epoch: 6| Step: 10
Training loss: 2.0996716022491455
Validation loss: 2.1287411989704257

Epoch: 6| Step: 11
Training loss: 2.958928108215332
Validation loss: 2.1317864246265863

Epoch: 6| Step: 12
Training loss: 2.469637155532837
Validation loss: 2.151371035524594

Epoch: 6| Step: 13
Training loss: 2.2811434268951416
Validation loss: 2.171077392434561

Epoch: 201| Step: 0
Training loss: 2.335444688796997
Validation loss: 2.204586354635095

Epoch: 6| Step: 1
Training loss: 2.3442790508270264
Validation loss: 2.215365945651967

Epoch: 6| Step: 2
Training loss: 2.572817802429199
Validation loss: 2.2271794401189333

Epoch: 6| Step: 3
Training loss: 2.977585554122925
Validation loss: 2.2303302775147142

Epoch: 6| Step: 4
Training loss: 2.50787091255188
Validation loss: 2.2151622631216563

Epoch: 6| Step: 5
Training loss: 2.073948383331299
Validation loss: 2.1919609756879908

Epoch: 6| Step: 6
Training loss: 1.8654239177703857
Validation loss: 2.145988882228892

Epoch: 6| Step: 7
Training loss: 2.212207794189453
Validation loss: 2.1314273008736233

Epoch: 6| Step: 8
Training loss: 2.072598457336426
Validation loss: 2.1210580615587133

Epoch: 6| Step: 9
Training loss: 2.0664725303649902
Validation loss: 2.1294676257717993

Epoch: 6| Step: 10
Training loss: 2.4831128120422363
Validation loss: 2.159108305490145

Epoch: 6| Step: 11
Training loss: 2.5320568084716797
Validation loss: 2.1637079254273446

Epoch: 6| Step: 12
Training loss: 2.565413475036621
Validation loss: 2.181888900777345

Epoch: 6| Step: 13
Training loss: 2.4361071586608887
Validation loss: 2.182117221175983

Epoch: 202| Step: 0
Training loss: 2.3566665649414062
Validation loss: 2.1755589157022457

Epoch: 6| Step: 1
Training loss: 2.3225650787353516
Validation loss: 2.1467898507272043

Epoch: 6| Step: 2
Training loss: 1.9737868309020996
Validation loss: 2.1532744515326714

Epoch: 6| Step: 3
Training loss: 2.1975553035736084
Validation loss: 2.1731168531602427

Epoch: 6| Step: 4
Training loss: 1.9078044891357422
Validation loss: 2.214008213371359

Epoch: 6| Step: 5
Training loss: 2.6098458766937256
Validation loss: 2.256071135561953

Epoch: 6| Step: 6
Training loss: 2.291659116744995
Validation loss: 2.3030144399212253

Epoch: 6| Step: 7
Training loss: 1.6925617456436157
Validation loss: 2.3523866027914067

Epoch: 6| Step: 8
Training loss: 2.2684037685394287
Validation loss: 2.3337989750728814

Epoch: 6| Step: 9
Training loss: 2.325988292694092
Validation loss: 2.331275619486327

Epoch: 6| Step: 10
Training loss: 2.3302836418151855
Validation loss: 2.2758785934858423

Epoch: 6| Step: 11
Training loss: 3.0838887691497803
Validation loss: 2.2052782222788823

Epoch: 6| Step: 12
Training loss: 2.968480110168457
Validation loss: 2.1752733658718806

Epoch: 6| Step: 13
Training loss: 3.130654811859131
Validation loss: 2.1504559106724237

Epoch: 203| Step: 0
Training loss: 2.28407621383667
Validation loss: 2.15212482278065

Epoch: 6| Step: 1
Training loss: 2.1758522987365723
Validation loss: 2.1579821532772434

Epoch: 6| Step: 2
Training loss: 2.710331439971924
Validation loss: 2.176206365708382

Epoch: 6| Step: 3
Training loss: 2.534762144088745
Validation loss: 2.182567399035218

Epoch: 6| Step: 4
Training loss: 3.5811917781829834
Validation loss: 2.202603219657816

Epoch: 6| Step: 5
Training loss: 2.1601309776306152
Validation loss: 2.210486335139121

Epoch: 6| Step: 6
Training loss: 1.8940274715423584
Validation loss: 2.222674844085529

Epoch: 6| Step: 7
Training loss: 2.2271766662597656
Validation loss: 2.2188895312688683

Epoch: 6| Step: 8
Training loss: 2.534273386001587
Validation loss: 2.1880865481591996

Epoch: 6| Step: 9
Training loss: 3.099001407623291
Validation loss: 2.1489150126775107

Epoch: 6| Step: 10
Training loss: 2.1233577728271484
Validation loss: 2.140765136288058

Epoch: 6| Step: 11
Training loss: 1.7164214849472046
Validation loss: 2.1349432622232745

Epoch: 6| Step: 12
Training loss: 2.201685905456543
Validation loss: 2.1355694673394643

Epoch: 6| Step: 13
Training loss: 0.8708299994468689
Validation loss: 2.126740009553971

Epoch: 204| Step: 0
Training loss: 2.7755188941955566
Validation loss: 2.1597695991557133

Epoch: 6| Step: 1
Training loss: 2.009108066558838
Validation loss: 2.1740659590690368

Epoch: 6| Step: 2
Training loss: 2.1334147453308105
Validation loss: 2.183589886593562

Epoch: 6| Step: 3
Training loss: 2.7758569717407227
Validation loss: 2.195255530777798

Epoch: 6| Step: 4
Training loss: 2.3573997020721436
Validation loss: 2.1764059797410042

Epoch: 6| Step: 5
Training loss: 2.0250370502471924
Validation loss: 2.1564847474457114

Epoch: 6| Step: 6
Training loss: 3.040152072906494
Validation loss: 2.1480173398089666

Epoch: 6| Step: 7
Training loss: 2.619861602783203
Validation loss: 2.133749915707496

Epoch: 6| Step: 8
Training loss: 1.867512822151184
Validation loss: 2.146879983204667

Epoch: 6| Step: 9
Training loss: 2.8758039474487305
Validation loss: 2.1485936141783193

Epoch: 6| Step: 10
Training loss: 1.6061995029449463
Validation loss: 2.1402172119386735

Epoch: 6| Step: 11
Training loss: 2.3522720336914062
Validation loss: 2.1371257587145736

Epoch: 6| Step: 12
Training loss: 1.836550235748291
Validation loss: 2.137520400426721

Epoch: 6| Step: 13
Training loss: 1.7676656246185303
Validation loss: 2.1248430334111696

Epoch: 205| Step: 0
Training loss: 2.7264788150787354
Validation loss: 2.1301339390457317

Epoch: 6| Step: 1
Training loss: 1.4630801677703857
Validation loss: 2.1513146674761208

Epoch: 6| Step: 2
Training loss: 2.633981704711914
Validation loss: 2.148098825126566

Epoch: 6| Step: 3
Training loss: 3.0676121711730957
Validation loss: 2.1571143365675405

Epoch: 6| Step: 4
Training loss: 2.4525625705718994
Validation loss: 2.1438602247545795

Epoch: 6| Step: 5
Training loss: 2.4710607528686523
Validation loss: 2.1394507910615657

Epoch: 6| Step: 6
Training loss: 1.6585276126861572
Validation loss: 2.134687880034088

Epoch: 6| Step: 7
Training loss: 2.2669177055358887
Validation loss: 2.130001773116409

Epoch: 6| Step: 8
Training loss: 1.369410514831543
Validation loss: 2.135809236957181

Epoch: 6| Step: 9
Training loss: 2.2371087074279785
Validation loss: 2.1441622498214885

Epoch: 6| Step: 10
Training loss: 3.0751585960388184
Validation loss: 2.142252104256743

Epoch: 6| Step: 11
Training loss: 2.0901732444763184
Validation loss: 2.1380356460489254

Epoch: 6| Step: 12
Training loss: 2.4505574703216553
Validation loss: 2.135032623044906

Epoch: 6| Step: 13
Training loss: 1.9735798835754395
Validation loss: 2.1463892382960164

Epoch: 206| Step: 0
Training loss: 1.7198801040649414
Validation loss: 2.142246215574203

Epoch: 6| Step: 1
Training loss: 1.667392373085022
Validation loss: 2.1397113107865855

Epoch: 6| Step: 2
Training loss: 2.349954128265381
Validation loss: 2.141791307797996

Epoch: 6| Step: 3
Training loss: 2.3304691314697266
Validation loss: 2.1404812207785984

Epoch: 6| Step: 4
Training loss: 2.48114013671875
Validation loss: 2.1320535521353445

Epoch: 6| Step: 5
Training loss: 2.3600707054138184
Validation loss: 2.122514094075849

Epoch: 6| Step: 6
Training loss: 2.3791117668151855
Validation loss: 2.1168984879729567

Epoch: 6| Step: 7
Training loss: 2.451024055480957
Validation loss: 2.1149347289916007

Epoch: 6| Step: 8
Training loss: 2.5730464458465576
Validation loss: 2.129033496302943

Epoch: 6| Step: 9
Training loss: 2.1830623149871826
Validation loss: 2.1111814539919616

Epoch: 6| Step: 10
Training loss: 2.5014283657073975
Validation loss: 2.1143836116278045

Epoch: 6| Step: 11
Training loss: 2.956378936767578
Validation loss: 2.1322281052989345

Epoch: 6| Step: 12
Training loss: 1.6658833026885986
Validation loss: 2.1501486891059467

Epoch: 6| Step: 13
Training loss: 2.446613073348999
Validation loss: 2.1468269466072

Epoch: 207| Step: 0
Training loss: 2.60133695602417
Validation loss: 2.157228736467259

Epoch: 6| Step: 1
Training loss: 1.7400121688842773
Validation loss: 2.16631152296579

Epoch: 6| Step: 2
Training loss: 2.3226561546325684
Validation loss: 2.176939046511086

Epoch: 6| Step: 3
Training loss: 2.3801138401031494
Validation loss: 2.1671432846335956

Epoch: 6| Step: 4
Training loss: 1.9783204793930054
Validation loss: 2.179306132819063

Epoch: 6| Step: 5
Training loss: 1.842918038368225
Validation loss: 2.1760901430601716

Epoch: 6| Step: 6
Training loss: 1.8612436056137085
Validation loss: 2.1816266121402865

Epoch: 6| Step: 7
Training loss: 2.2000696659088135
Validation loss: 2.1856898312927573

Epoch: 6| Step: 8
Training loss: 2.7288496494293213
Validation loss: 2.187917342749975

Epoch: 6| Step: 9
Training loss: 2.1109628677368164
Validation loss: 2.1826978703980804

Epoch: 6| Step: 10
Training loss: 2.815781593322754
Validation loss: 2.180013084924349

Epoch: 6| Step: 11
Training loss: 2.977475643157959
Validation loss: 2.1930693810985935

Epoch: 6| Step: 12
Training loss: 2.3022360801696777
Validation loss: 2.1531968193669475

Epoch: 6| Step: 13
Training loss: 1.92718505859375
Validation loss: 2.12945205678222

Epoch: 208| Step: 0
Training loss: 1.9607856273651123
Validation loss: 2.114583089787473

Epoch: 6| Step: 1
Training loss: 2.4125823974609375
Validation loss: 2.120235502078969

Epoch: 6| Step: 2
Training loss: 2.655806541442871
Validation loss: 2.1239106039847098

Epoch: 6| Step: 3
Training loss: 2.9708847999572754
Validation loss: 2.109971384848318

Epoch: 6| Step: 4
Training loss: 2.3381571769714355
Validation loss: 2.121176160791869

Epoch: 6| Step: 5
Training loss: 2.357055902481079
Validation loss: 2.119514911405502

Epoch: 6| Step: 6
Training loss: 1.7338192462921143
Validation loss: 2.106882890065511

Epoch: 6| Step: 7
Training loss: 2.380528688430786
Validation loss: 2.117215261664442

Epoch: 6| Step: 8
Training loss: 2.9751195907592773
Validation loss: 2.1036749065563245

Epoch: 6| Step: 9
Training loss: 1.6854302883148193
Validation loss: 2.1154582577366985

Epoch: 6| Step: 10
Training loss: 1.617322325706482
Validation loss: 2.1316721529088993

Epoch: 6| Step: 11
Training loss: 2.060030460357666
Validation loss: 2.129222200762841

Epoch: 6| Step: 12
Training loss: 2.382561445236206
Validation loss: 2.1306954417177426

Epoch: 6| Step: 13
Training loss: 2.2449004650115967
Validation loss: 2.123663310081728

Epoch: 209| Step: 0
Training loss: 2.470282554626465
Validation loss: 2.1361925473777195

Epoch: 6| Step: 1
Training loss: 2.492478847503662
Validation loss: 2.145904674324938

Epoch: 6| Step: 2
Training loss: 2.254413604736328
Validation loss: 2.1643507390893917

Epoch: 6| Step: 3
Training loss: 1.7671529054641724
Validation loss: 2.1754767971654094

Epoch: 6| Step: 4
Training loss: 2.0306174755096436
Validation loss: 2.1882683794985534

Epoch: 6| Step: 5
Training loss: 2.611455202102661
Validation loss: 2.1895299547462055

Epoch: 6| Step: 6
Training loss: 1.693482518196106
Validation loss: 2.1809305785804667

Epoch: 6| Step: 7
Training loss: 2.095018148422241
Validation loss: 2.1501082169112338

Epoch: 6| Step: 8
Training loss: 2.4217090606689453
Validation loss: 2.1469933858481784

Epoch: 6| Step: 9
Training loss: 2.4757397174835205
Validation loss: 2.1309972847661665

Epoch: 6| Step: 10
Training loss: 2.5125818252563477
Validation loss: 2.1313314771139495

Epoch: 6| Step: 11
Training loss: 2.615471839904785
Validation loss: 2.118531216857254

Epoch: 6| Step: 12
Training loss: 2.1062684059143066
Validation loss: 2.1183761345442904

Epoch: 6| Step: 13
Training loss: 2.250122308731079
Validation loss: 2.1123541555097027

Epoch: 210| Step: 0
Training loss: 2.5014870166778564
Validation loss: 2.1029880533936205

Epoch: 6| Step: 1
Training loss: 2.162848711013794
Validation loss: 2.115041322605584

Epoch: 6| Step: 2
Training loss: 2.208685874938965
Validation loss: 2.1110855148684595

Epoch: 6| Step: 3
Training loss: 1.7795376777648926
Validation loss: 2.0944336204118628

Epoch: 6| Step: 4
Training loss: 1.8090307712554932
Validation loss: 2.0894526435482885

Epoch: 6| Step: 5
Training loss: 2.030815362930298
Validation loss: 2.1076419943122455

Epoch: 6| Step: 6
Training loss: 3.150106906890869
Validation loss: 2.103796646159182

Epoch: 6| Step: 7
Training loss: 1.9837795495986938
Validation loss: 2.0969783875250045

Epoch: 6| Step: 8
Training loss: 2.3966331481933594
Validation loss: 2.094562506162992

Epoch: 6| Step: 9
Training loss: 2.355441093444824
Validation loss: 2.108269263339299

Epoch: 6| Step: 10
Training loss: 2.274209976196289
Validation loss: 2.1090899154704106

Epoch: 6| Step: 11
Training loss: 2.550255298614502
Validation loss: 2.114467146576092

Epoch: 6| Step: 12
Training loss: 2.3054356575012207
Validation loss: 2.112484908873035

Epoch: 6| Step: 13
Training loss: 2.2757608890533447
Validation loss: 2.103912135606171

Epoch: 211| Step: 0
Training loss: 2.368887186050415
Validation loss: 2.1091690678750314

Epoch: 6| Step: 1
Training loss: 2.508955955505371
Validation loss: 2.1156961866604385

Epoch: 6| Step: 2
Training loss: 2.2201547622680664
Validation loss: 2.1168623406399965

Epoch: 6| Step: 3
Training loss: 2.495563268661499
Validation loss: 2.1240978189693984

Epoch: 6| Step: 4
Training loss: 2.3012592792510986
Validation loss: 2.122686186144429

Epoch: 6| Step: 5
Training loss: 1.490300178527832
Validation loss: 2.132021209245087

Epoch: 6| Step: 6
Training loss: 2.8864426612854004
Validation loss: 2.14108931890098

Epoch: 6| Step: 7
Training loss: 1.7985451221466064
Validation loss: 2.1435462428677465

Epoch: 6| Step: 8
Training loss: 2.4292893409729004
Validation loss: 2.151029682928516

Epoch: 6| Step: 9
Training loss: 2.15925931930542
Validation loss: 2.158680820977816

Epoch: 6| Step: 10
Training loss: 2.390659809112549
Validation loss: 2.1459977985710226

Epoch: 6| Step: 11
Training loss: 1.8032135963439941
Validation loss: 2.137655838843315

Epoch: 6| Step: 12
Training loss: 2.3716745376586914
Validation loss: 2.1337046443775134

Epoch: 6| Step: 13
Training loss: 2.584470272064209
Validation loss: 2.13793602297383

Epoch: 212| Step: 0
Training loss: 2.812633991241455
Validation loss: 2.1477281149997505

Epoch: 6| Step: 1
Training loss: 2.6901211738586426
Validation loss: 2.1641896514482397

Epoch: 6| Step: 2
Training loss: 2.387193441390991
Validation loss: 2.171181389080581

Epoch: 6| Step: 3
Training loss: 2.431640148162842
Validation loss: 2.1454053566020024

Epoch: 6| Step: 4
Training loss: 2.1855154037475586
Validation loss: 2.1202257704991165

Epoch: 6| Step: 5
Training loss: 2.4304213523864746
Validation loss: 2.1119600598530104

Epoch: 6| Step: 6
Training loss: 2.1481361389160156
Validation loss: 2.102380553881327

Epoch: 6| Step: 7
Training loss: 1.8276376724243164
Validation loss: 2.1021211531854447

Epoch: 6| Step: 8
Training loss: 2.3784162998199463
Validation loss: 2.1105652073378205

Epoch: 6| Step: 9
Training loss: 1.6500192880630493
Validation loss: 2.104186639990858

Epoch: 6| Step: 10
Training loss: 2.8519911766052246
Validation loss: 2.1003321165679605

Epoch: 6| Step: 11
Training loss: 1.8290077447891235
Validation loss: 2.1045602470315914

Epoch: 6| Step: 12
Training loss: 2.4224395751953125
Validation loss: 2.1029355192697174

Epoch: 6| Step: 13
Training loss: 1.3870859146118164
Validation loss: 2.088211509489244

Epoch: 213| Step: 0
Training loss: 1.6023190021514893
Validation loss: 2.100710456089307

Epoch: 6| Step: 1
Training loss: 2.221391201019287
Validation loss: 2.1079294809731106

Epoch: 6| Step: 2
Training loss: 2.4145450592041016
Validation loss: 2.1075898344798754

Epoch: 6| Step: 3
Training loss: 1.8082072734832764
Validation loss: 2.12477639285467

Epoch: 6| Step: 4
Training loss: 2.6043455600738525
Validation loss: 2.132290078747657

Epoch: 6| Step: 5
Training loss: 2.3680994510650635
Validation loss: 2.1393801025165025

Epoch: 6| Step: 6
Training loss: 2.038909435272217
Validation loss: 2.1365582032870223

Epoch: 6| Step: 7
Training loss: 2.3409295082092285
Validation loss: 2.1408701635176137

Epoch: 6| Step: 8
Training loss: 2.2919809818267822
Validation loss: 2.13473859653678

Epoch: 6| Step: 9
Training loss: 2.44803524017334
Validation loss: 2.1436306994448424

Epoch: 6| Step: 10
Training loss: 2.0673155784606934
Validation loss: 2.1375998091954056

Epoch: 6| Step: 11
Training loss: 2.0121750831604004
Validation loss: 2.116239245219897

Epoch: 6| Step: 12
Training loss: 2.519412040710449
Validation loss: 2.126452047337768

Epoch: 6| Step: 13
Training loss: 3.012752056121826
Validation loss: 2.1295644557604225

Epoch: 214| Step: 0
Training loss: 2.3901920318603516
Validation loss: 2.1187903137617212

Epoch: 6| Step: 1
Training loss: 2.716695785522461
Validation loss: 2.1221509800162366

Epoch: 6| Step: 2
Training loss: 2.6128716468811035
Validation loss: 2.1019861544332197

Epoch: 6| Step: 3
Training loss: 2.0925750732421875
Validation loss: 2.092313171714865

Epoch: 6| Step: 4
Training loss: 2.535682201385498
Validation loss: 2.097896595155039

Epoch: 6| Step: 5
Training loss: 1.7630805969238281
Validation loss: 2.1015103734949583

Epoch: 6| Step: 6
Training loss: 2.5248396396636963
Validation loss: 2.103406903564289

Epoch: 6| Step: 7
Training loss: 2.5476465225219727
Validation loss: 2.1113930902173443

Epoch: 6| Step: 8
Training loss: 2.0612900257110596
Validation loss: 2.093077729466141

Epoch: 6| Step: 9
Training loss: 1.8580206632614136
Validation loss: 2.0774399080584125

Epoch: 6| Step: 10
Training loss: 2.4032397270202637
Validation loss: 2.083555790685838

Epoch: 6| Step: 11
Training loss: 2.1499152183532715
Validation loss: 2.096116030088035

Epoch: 6| Step: 12
Training loss: 1.7409852743148804
Validation loss: 2.1054905358181206

Epoch: 6| Step: 13
Training loss: 1.7434269189834595
Validation loss: 2.1088113272061912

Epoch: 215| Step: 0
Training loss: 1.485966444015503
Validation loss: 2.1363604350756575

Epoch: 6| Step: 1
Training loss: 2.3831281661987305
Validation loss: 2.1485010475240727

Epoch: 6| Step: 2
Training loss: 2.2995216846466064
Validation loss: 2.1435415821690715

Epoch: 6| Step: 3
Training loss: 2.117380142211914
Validation loss: 2.1314523194425847

Epoch: 6| Step: 4
Training loss: 2.464776039123535
Validation loss: 2.130185615631842

Epoch: 6| Step: 5
Training loss: 2.285308361053467
Validation loss: 2.133107231509301

Epoch: 6| Step: 6
Training loss: 3.0627121925354004
Validation loss: 2.1264704273593042

Epoch: 6| Step: 7
Training loss: 2.1886215209960938
Validation loss: 2.1473384570049983

Epoch: 6| Step: 8
Training loss: 1.8767030239105225
Validation loss: 2.151245609406502

Epoch: 6| Step: 9
Training loss: 2.8378119468688965
Validation loss: 2.165145984259985

Epoch: 6| Step: 10
Training loss: 1.9990618228912354
Validation loss: 2.1899302544132357

Epoch: 6| Step: 11
Training loss: 2.168929100036621
Validation loss: 2.1944029767026185

Epoch: 6| Step: 12
Training loss: 1.9509882926940918
Validation loss: 2.2103908190163235

Epoch: 6| Step: 13
Training loss: 2.726501941680908
Validation loss: 2.205124693532144

Epoch: 216| Step: 0
Training loss: 1.7426114082336426
Validation loss: 2.1805744030142344

Epoch: 6| Step: 1
Training loss: 2.936084747314453
Validation loss: 2.133153592386553

Epoch: 6| Step: 2
Training loss: 2.8637290000915527
Validation loss: 2.1158947303730953

Epoch: 6| Step: 3
Training loss: 2.4519171714782715
Validation loss: 2.117084377555437

Epoch: 6| Step: 4
Training loss: 1.1633658409118652
Validation loss: 2.1127785046895347

Epoch: 6| Step: 5
Training loss: 1.6893259286880493
Validation loss: 2.117751980340609

Epoch: 6| Step: 6
Training loss: 2.1954195499420166
Validation loss: 2.1138860487168833

Epoch: 6| Step: 7
Training loss: 2.0606768131256104
Validation loss: 2.121967374637563

Epoch: 6| Step: 8
Training loss: 1.6945993900299072
Validation loss: 2.10868731621773

Epoch: 6| Step: 9
Training loss: 1.8168749809265137
Validation loss: 2.1074524182145313

Epoch: 6| Step: 10
Training loss: 2.53283429145813
Validation loss: 2.1079569965280514

Epoch: 6| Step: 11
Training loss: 2.6072802543640137
Validation loss: 2.097128654039034

Epoch: 6| Step: 12
Training loss: 3.0167975425720215
Validation loss: 2.110333114541987

Epoch: 6| Step: 13
Training loss: 2.903423547744751
Validation loss: 2.112705333258516

Epoch: 217| Step: 0
Training loss: 1.799370527267456
Validation loss: 2.1199993471945486

Epoch: 6| Step: 1
Training loss: 2.5215959548950195
Validation loss: 2.126359539647256

Epoch: 6| Step: 2
Training loss: 2.8708748817443848
Validation loss: 2.1332475536613056

Epoch: 6| Step: 3
Training loss: 2.292341709136963
Validation loss: 2.1323225985291185

Epoch: 6| Step: 4
Training loss: 2.4420838356018066
Validation loss: 2.1228477852318877

Epoch: 6| Step: 5
Training loss: 2.063075304031372
Validation loss: 2.1161235737544235

Epoch: 6| Step: 6
Training loss: 1.7723387479782104
Validation loss: 2.110669159120129

Epoch: 6| Step: 7
Training loss: 1.642358422279358
Validation loss: 2.0971993989841913

Epoch: 6| Step: 8
Training loss: 2.2804641723632812
Validation loss: 2.095238535634933

Epoch: 6| Step: 9
Training loss: 2.6948142051696777
Validation loss: 2.0963853572004583

Epoch: 6| Step: 10
Training loss: 1.88473641872406
Validation loss: 2.0851225506874824

Epoch: 6| Step: 11
Training loss: 2.652390480041504
Validation loss: 2.1034673708741383

Epoch: 6| Step: 12
Training loss: 2.509930372238159
Validation loss: 2.100050623698901

Epoch: 6| Step: 13
Training loss: 1.6863775253295898
Validation loss: 2.0997587737216743

Epoch: 218| Step: 0
Training loss: 2.1405842304229736
Validation loss: 2.111077839328397

Epoch: 6| Step: 1
Training loss: 2.342989206314087
Validation loss: 2.1280204275602936

Epoch: 6| Step: 2
Training loss: 1.8693552017211914
Validation loss: 2.109941667126071

Epoch: 6| Step: 3
Training loss: 2.67911696434021
Validation loss: 2.1216872866435716

Epoch: 6| Step: 4
Training loss: 2.386819362640381
Validation loss: 2.1125788406659196

Epoch: 6| Step: 5
Training loss: 1.7428406476974487
Validation loss: 2.1014337872946136

Epoch: 6| Step: 6
Training loss: 2.211714029312134
Validation loss: 2.103992774922361

Epoch: 6| Step: 7
Training loss: 2.0424413681030273
Validation loss: 2.0952756738149994

Epoch: 6| Step: 8
Training loss: 2.1135125160217285
Validation loss: 2.1016324117619503

Epoch: 6| Step: 9
Training loss: 2.290761709213257
Validation loss: 2.1057210558204242

Epoch: 6| Step: 10
Training loss: 2.6430249214172363
Validation loss: 2.115295925448018

Epoch: 6| Step: 11
Training loss: 2.078251600265503
Validation loss: 2.117671246169716

Epoch: 6| Step: 12
Training loss: 2.60017466545105
Validation loss: 2.1396951854869886

Epoch: 6| Step: 13
Training loss: 2.188062906265259
Validation loss: 2.124378499164376

Epoch: 219| Step: 0
Training loss: 2.4996750354766846
Validation loss: 2.119498319523309

Epoch: 6| Step: 1
Training loss: 1.6677286624908447
Validation loss: 2.111027499680878

Epoch: 6| Step: 2
Training loss: 3.0151703357696533
Validation loss: 2.113427610807521

Epoch: 6| Step: 3
Training loss: 2.264862060546875
Validation loss: 2.107933187997469

Epoch: 6| Step: 4
Training loss: 1.6515815258026123
Validation loss: 2.1062486017903974

Epoch: 6| Step: 5
Training loss: 2.471144676208496
Validation loss: 2.1100410210189

Epoch: 6| Step: 6
Training loss: 2.352644920349121
Validation loss: 2.127689584609001

Epoch: 6| Step: 7
Training loss: 1.8753583431243896
Validation loss: 2.1193670816318964

Epoch: 6| Step: 8
Training loss: 2.4732506275177
Validation loss: 2.1213413489762174

Epoch: 6| Step: 9
Training loss: 2.3125832080841064
Validation loss: 2.1157003269400647

Epoch: 6| Step: 10
Training loss: 1.7253048419952393
Validation loss: 2.1157010793685913

Epoch: 6| Step: 11
Training loss: 2.7878329753875732
Validation loss: 2.100944483152

Epoch: 6| Step: 12
Training loss: 1.919624924659729
Validation loss: 2.0923073291778564

Epoch: 6| Step: 13
Training loss: 1.8787188529968262
Validation loss: 2.0868311723073325

Epoch: 220| Step: 0
Training loss: 3.199315071105957
Validation loss: 2.081316183972102

Epoch: 6| Step: 1
Training loss: 2.316504955291748
Validation loss: 2.079364558701874

Epoch: 6| Step: 2
Training loss: 1.984932541847229
Validation loss: 2.078730606263684

Epoch: 6| Step: 3
Training loss: 2.1941025257110596
Validation loss: 2.0744233836409864

Epoch: 6| Step: 4
Training loss: 1.3905409574508667
Validation loss: 2.0877758508087485

Epoch: 6| Step: 5
Training loss: 1.7797781229019165
Validation loss: 2.099390665690104

Epoch: 6| Step: 6
Training loss: 2.9375662803649902
Validation loss: 2.1052688449941654

Epoch: 6| Step: 7
Training loss: 2.1507749557495117
Validation loss: 2.123833369183284

Epoch: 6| Step: 8
Training loss: 2.1509623527526855
Validation loss: 2.126944239421557

Epoch: 6| Step: 9
Training loss: 2.9230334758758545
Validation loss: 2.138779445361066

Epoch: 6| Step: 10
Training loss: 2.216998338699341
Validation loss: 2.1387008005572903

Epoch: 6| Step: 11
Training loss: 2.2802140712738037
Validation loss: 2.1425704289508123

Epoch: 6| Step: 12
Training loss: 2.0206358432769775
Validation loss: 2.1407385487710275

Epoch: 6| Step: 13
Training loss: 1.2018681764602661
Validation loss: 2.130623354706713

Epoch: 221| Step: 0
Training loss: 2.209469795227051
Validation loss: 2.108187021747712

Epoch: 6| Step: 1
Training loss: 2.9005017280578613
Validation loss: 2.1105827311033845

Epoch: 6| Step: 2
Training loss: 1.7617523670196533
Validation loss: 2.1142527185460573

Epoch: 6| Step: 3
Training loss: 2.142993927001953
Validation loss: 2.1177199194508214

Epoch: 6| Step: 4
Training loss: 2.05936861038208
Validation loss: 2.112645851668491

Epoch: 6| Step: 5
Training loss: 2.466644763946533
Validation loss: 2.120394975908341

Epoch: 6| Step: 6
Training loss: 2.442718982696533
Validation loss: 2.1094996211349324

Epoch: 6| Step: 7
Training loss: 2.526401996612549
Validation loss: 2.09627434771548

Epoch: 6| Step: 8
Training loss: 1.8566222190856934
Validation loss: 2.0932134992332867

Epoch: 6| Step: 9
Training loss: 1.7099723815917969
Validation loss: 2.088055308147143

Epoch: 6| Step: 10
Training loss: 1.9276854991912842
Validation loss: 2.10253442359227

Epoch: 6| Step: 11
Training loss: 2.411313533782959
Validation loss: 2.1019154620426956

Epoch: 6| Step: 12
Training loss: 2.296067476272583
Validation loss: 2.1127665811969387

Epoch: 6| Step: 13
Training loss: 2.4913814067840576
Validation loss: 2.120435731385344

Epoch: 222| Step: 0
Training loss: 2.842650890350342
Validation loss: 2.1599483489990234

Epoch: 6| Step: 1
Training loss: 2.537663459777832
Validation loss: 2.18405334923857

Epoch: 6| Step: 2
Training loss: 2.609480381011963
Validation loss: 2.2138289713090464

Epoch: 6| Step: 3
Training loss: 2.525707244873047
Validation loss: 2.2183121378703783

Epoch: 6| Step: 4
Training loss: 2.02380108833313
Validation loss: 2.1936600541555755

Epoch: 6| Step: 5
Training loss: 1.8173362016677856
Validation loss: 2.1634601751963296

Epoch: 6| Step: 6
Training loss: 2.7331738471984863
Validation loss: 2.1222031962487007

Epoch: 6| Step: 7
Training loss: 1.6275110244750977
Validation loss: 2.111961446782594

Epoch: 6| Step: 8
Training loss: 2.7850797176361084
Validation loss: 2.106989727225355

Epoch: 6| Step: 9
Training loss: 1.6809771060943604
Validation loss: 2.127727770036267

Epoch: 6| Step: 10
Training loss: 2.2248268127441406
Validation loss: 2.1526177365292787

Epoch: 6| Step: 11
Training loss: 1.5073962211608887
Validation loss: 2.160229298376268

Epoch: 6| Step: 12
Training loss: 2.4249682426452637
Validation loss: 2.1488874394406556

Epoch: 6| Step: 13
Training loss: 2.1567015647888184
Validation loss: 2.1334173717806415

Epoch: 223| Step: 0
Training loss: 2.8709492683410645
Validation loss: 2.1344310673334266

Epoch: 6| Step: 1
Training loss: 2.066896677017212
Validation loss: 2.109977226103506

Epoch: 6| Step: 2
Training loss: 2.2678513526916504
Validation loss: 2.085887498753045

Epoch: 6| Step: 3
Training loss: 2.6111316680908203
Validation loss: 2.0870667375544065

Epoch: 6| Step: 4
Training loss: 2.395854949951172
Validation loss: 2.092464812340275

Epoch: 6| Step: 5
Training loss: 2.6651196479797363
Validation loss: 2.0975829657687934

Epoch: 6| Step: 6
Training loss: 1.9723893404006958
Validation loss: 2.086755742308914

Epoch: 6| Step: 7
Training loss: 2.0359325408935547
Validation loss: 2.083595957807315

Epoch: 6| Step: 8
Training loss: 2.316636085510254
Validation loss: 2.088332168517574

Epoch: 6| Step: 9
Training loss: 1.4441934823989868
Validation loss: 2.0859158962003645

Epoch: 6| Step: 10
Training loss: 2.0565099716186523
Validation loss: 2.087180170961606

Epoch: 6| Step: 11
Training loss: 2.564657688140869
Validation loss: 2.0936997346980597

Epoch: 6| Step: 12
Training loss: 1.6125357151031494
Validation loss: 2.0896716912587485

Epoch: 6| Step: 13
Training loss: 1.7053320407867432
Validation loss: 2.098258505585373

Epoch: 224| Step: 0
Training loss: 2.806102752685547
Validation loss: 2.104467873932213

Epoch: 6| Step: 1
Training loss: 1.9414153099060059
Validation loss: 2.116877558410809

Epoch: 6| Step: 2
Training loss: 1.27580988407135
Validation loss: 2.1065179519755866

Epoch: 6| Step: 3
Training loss: 2.1644701957702637
Validation loss: 2.118693838837326

Epoch: 6| Step: 4
Training loss: 2.726302146911621
Validation loss: 2.1152966227582706

Epoch: 6| Step: 5
Training loss: 2.5139029026031494
Validation loss: 2.1246496810707995

Epoch: 6| Step: 6
Training loss: 1.5208649635314941
Validation loss: 2.113168047320458

Epoch: 6| Step: 7
Training loss: 2.105576515197754
Validation loss: 2.1177914988610054

Epoch: 6| Step: 8
Training loss: 1.6539335250854492
Validation loss: 2.1209622236990158

Epoch: 6| Step: 9
Training loss: 2.1965389251708984
Validation loss: 2.1187842866425872

Epoch: 6| Step: 10
Training loss: 2.1502294540405273
Validation loss: 2.1204918071787846

Epoch: 6| Step: 11
Training loss: 3.217280387878418
Validation loss: 2.1191704298860286

Epoch: 6| Step: 12
Training loss: 2.245680332183838
Validation loss: 2.1153099998351066

Epoch: 6| Step: 13
Training loss: 2.322674512863159
Validation loss: 2.1076248691928003

Epoch: 225| Step: 0
Training loss: 1.8817291259765625
Validation loss: 2.1184619421600015

Epoch: 6| Step: 1
Training loss: 1.687132716178894
Validation loss: 2.121942704723727

Epoch: 6| Step: 2
Training loss: 2.7205898761749268
Validation loss: 2.113392158221173

Epoch: 6| Step: 3
Training loss: 2.3819236755371094
Validation loss: 2.110079260282619

Epoch: 6| Step: 4
Training loss: 1.9499341249465942
Validation loss: 2.0998909601601223

Epoch: 6| Step: 5
Training loss: 2.6845786571502686
Validation loss: 2.093645231698149

Epoch: 6| Step: 6
Training loss: 2.46083402633667
Validation loss: 2.0787897571440666

Epoch: 6| Step: 7
Training loss: 2.159454822540283
Validation loss: 2.0734692504329066

Epoch: 6| Step: 8
Training loss: 2.915391683578491
Validation loss: 2.0741109386567147

Epoch: 6| Step: 9
Training loss: 2.323129177093506
Validation loss: 2.0750113700025823

Epoch: 6| Step: 10
Training loss: 2.155747890472412
Validation loss: 2.077365765007593

Epoch: 6| Step: 11
Training loss: 1.8060686588287354
Validation loss: 2.086405018324493

Epoch: 6| Step: 12
Training loss: 1.7058682441711426
Validation loss: 2.083443823681083

Epoch: 6| Step: 13
Training loss: 1.675496220588684
Validation loss: 2.1051814748394873

Epoch: 226| Step: 0
Training loss: 2.366636276245117
Validation loss: 2.104683541482495

Epoch: 6| Step: 1
Training loss: 2.4125661849975586
Validation loss: 2.1103367984935804

Epoch: 6| Step: 2
Training loss: 1.9033582210540771
Validation loss: 2.1128144674403693

Epoch: 6| Step: 3
Training loss: 2.5282492637634277
Validation loss: 2.121661093927199

Epoch: 6| Step: 4
Training loss: 2.3165292739868164
Validation loss: 2.1229955970600085

Epoch: 6| Step: 5
Training loss: 1.871596336364746
Validation loss: 2.1526532198793147

Epoch: 6| Step: 6
Training loss: 1.9058406352996826
Validation loss: 2.177149836735059

Epoch: 6| Step: 7
Training loss: 2.694274425506592
Validation loss: 2.200574487768194

Epoch: 6| Step: 8
Training loss: 2.744166135787964
Validation loss: 2.1902118062460296

Epoch: 6| Step: 9
Training loss: 2.0457260608673096
Validation loss: 2.181933126141948

Epoch: 6| Step: 10
Training loss: 1.7442140579223633
Validation loss: 2.157520664635525

Epoch: 6| Step: 11
Training loss: 1.9083631038665771
Validation loss: 2.1415291114520003

Epoch: 6| Step: 12
Training loss: 2.065859317779541
Validation loss: 2.136371934285728

Epoch: 6| Step: 13
Training loss: 2.5897724628448486
Validation loss: 2.141192546454809

Epoch: 227| Step: 0
Training loss: 2.169053554534912
Validation loss: 2.1311814028729676

Epoch: 6| Step: 1
Training loss: 2.5814805030822754
Validation loss: 2.133690584090448

Epoch: 6| Step: 2
Training loss: 1.9600634574890137
Validation loss: 2.1479406792630433

Epoch: 6| Step: 3
Training loss: 2.5683093070983887
Validation loss: 2.1399440021925074

Epoch: 6| Step: 4
Training loss: 2.314758777618408
Validation loss: 2.1133022846714145

Epoch: 6| Step: 5
Training loss: 2.71885085105896
Validation loss: 2.1078669819780576

Epoch: 6| Step: 6
Training loss: 2.181666612625122
Validation loss: 2.1045625402081396

Epoch: 6| Step: 7
Training loss: 2.4367709159851074
Validation loss: 2.101088754592403

Epoch: 6| Step: 8
Training loss: 2.261111259460449
Validation loss: 2.0973133143558296

Epoch: 6| Step: 9
Training loss: 2.7178092002868652
Validation loss: 2.0796037873914166

Epoch: 6| Step: 10
Training loss: 1.4563149213790894
Validation loss: 2.084271570687653

Epoch: 6| Step: 11
Training loss: 1.518876075744629
Validation loss: 2.0823830942953787

Epoch: 6| Step: 12
Training loss: 1.676673412322998
Validation loss: 2.069474744540389

Epoch: 6| Step: 13
Training loss: 2.2976510524749756
Validation loss: 2.0701111773008942

Epoch: 228| Step: 0
Training loss: 1.9114136695861816
Validation loss: 2.0671202418624715

Epoch: 6| Step: 1
Training loss: 2.1086232662200928
Validation loss: 2.065071726358065

Epoch: 6| Step: 2
Training loss: 2.3464338779449463
Validation loss: 2.0660055773232573

Epoch: 6| Step: 3
Training loss: 1.7216885089874268
Validation loss: 2.0718387314068374

Epoch: 6| Step: 4
Training loss: 2.0913281440734863
Validation loss: 2.078923253602879

Epoch: 6| Step: 5
Training loss: 2.481555223464966
Validation loss: 2.0853345471043743

Epoch: 6| Step: 6
Training loss: 2.520480155944824
Validation loss: 2.105130728854928

Epoch: 6| Step: 7
Training loss: 1.6366360187530518
Validation loss: 2.1044496592654975

Epoch: 6| Step: 8
Training loss: 2.662964105606079
Validation loss: 2.137504716073313

Epoch: 6| Step: 9
Training loss: 2.365720272064209
Validation loss: 2.155448562355452

Epoch: 6| Step: 10
Training loss: 2.655524969100952
Validation loss: 2.1691589099104687

Epoch: 6| Step: 11
Training loss: 2.6509060859680176
Validation loss: 2.1826031951494116

Epoch: 6| Step: 12
Training loss: 1.8101286888122559
Validation loss: 2.1903796285711308

Epoch: 6| Step: 13
Training loss: 1.5311551094055176
Validation loss: 2.1645647992369947

Epoch: 229| Step: 0
Training loss: 2.3439137935638428
Validation loss: 2.148128258284702

Epoch: 6| Step: 1
Training loss: 1.8182270526885986
Validation loss: 2.113782495580694

Epoch: 6| Step: 2
Training loss: 2.441619396209717
Validation loss: 2.099154838951685

Epoch: 6| Step: 3
Training loss: 1.8180710077285767
Validation loss: 2.1108793597067557

Epoch: 6| Step: 4
Training loss: 2.4979140758514404
Validation loss: 2.1053048231268443

Epoch: 6| Step: 5
Training loss: 2.6934921741485596
Validation loss: 2.1015236223897626

Epoch: 6| Step: 6
Training loss: 2.3588685989379883
Validation loss: 2.0996035273357103

Epoch: 6| Step: 7
Training loss: 1.9050743579864502
Validation loss: 2.0944291955681256

Epoch: 6| Step: 8
Training loss: 1.9042606353759766
Validation loss: 2.095344517820625

Epoch: 6| Step: 9
Training loss: 2.027160167694092
Validation loss: 2.104825991456227

Epoch: 6| Step: 10
Training loss: 2.0050477981567383
Validation loss: 2.11937044512841

Epoch: 6| Step: 11
Training loss: 2.4240641593933105
Validation loss: 2.1107293328931256

Epoch: 6| Step: 12
Training loss: 2.000441551208496
Validation loss: 2.1134406059019026

Epoch: 6| Step: 13
Training loss: 2.985111951828003
Validation loss: 2.094727926356818

Epoch: 230| Step: 0
Training loss: 1.8489629030227661
Validation loss: 2.085088381203272

Epoch: 6| Step: 1
Training loss: 2.4832348823547363
Validation loss: 2.0901108352086877

Epoch: 6| Step: 2
Training loss: 2.0564827919006348
Validation loss: 2.063288806587137

Epoch: 6| Step: 3
Training loss: 1.7290337085723877
Validation loss: 2.070924364110475

Epoch: 6| Step: 4
Training loss: 2.32315731048584
Validation loss: 2.0684243094536567

Epoch: 6| Step: 5
Training loss: 2.922828197479248
Validation loss: 2.0771005153656006

Epoch: 6| Step: 6
Training loss: 2.3810997009277344
Validation loss: 2.0685112989077004

Epoch: 6| Step: 7
Training loss: 2.364874839782715
Validation loss: 2.0731421850060903

Epoch: 6| Step: 8
Training loss: 2.1413047313690186
Validation loss: 2.069253936890633

Epoch: 6| Step: 9
Training loss: 2.032160758972168
Validation loss: 2.0736245237370974

Epoch: 6| Step: 10
Training loss: 1.840481162071228
Validation loss: 2.0852407050389115

Epoch: 6| Step: 11
Training loss: 1.941704273223877
Validation loss: 2.0795637830611198

Epoch: 6| Step: 12
Training loss: 2.1600394248962402
Validation loss: 2.0840593871249946

Epoch: 6| Step: 13
Training loss: 2.2243738174438477
Validation loss: 2.0871808926264444

Epoch: 231| Step: 0
Training loss: 2.989934206008911
Validation loss: 2.0943942710917485

Epoch: 6| Step: 1
Training loss: 2.4160430431365967
Validation loss: 2.0969643285197597

Epoch: 6| Step: 2
Training loss: 1.8247463703155518
Validation loss: 2.102245053937358

Epoch: 6| Step: 3
Training loss: 1.749009132385254
Validation loss: 2.0886766449097665

Epoch: 6| Step: 4
Training loss: 2.1120035648345947
Validation loss: 2.1196391556852605

Epoch: 6| Step: 5
Training loss: 2.0970423221588135
Validation loss: 2.1208729359411422

Epoch: 6| Step: 6
Training loss: 2.5316123962402344
Validation loss: 2.1036986471504293

Epoch: 6| Step: 7
Training loss: 2.716611862182617
Validation loss: 2.1037202778682915

Epoch: 6| Step: 8
Training loss: 1.8942915201187134
Validation loss: 2.088903352778445

Epoch: 6| Step: 9
Training loss: 1.6807749271392822
Validation loss: 2.0781489674763014

Epoch: 6| Step: 10
Training loss: 1.6479899883270264
Validation loss: 2.067316145025274

Epoch: 6| Step: 11
Training loss: 2.4355454444885254
Validation loss: 2.063175037343015

Epoch: 6| Step: 12
Training loss: 2.385011911392212
Validation loss: 2.054332552417632

Epoch: 6| Step: 13
Training loss: 1.9254019260406494
Validation loss: 2.0729526704357517

Epoch: 232| Step: 0
Training loss: 1.9128401279449463
Validation loss: 2.058950113993819

Epoch: 6| Step: 1
Training loss: 2.3909573554992676
Validation loss: 2.084262576154483

Epoch: 6| Step: 2
Training loss: 2.509026527404785
Validation loss: 2.0829449071679065

Epoch: 6| Step: 3
Training loss: 2.629647731781006
Validation loss: 2.0747135890427457

Epoch: 6| Step: 4
Training loss: 2.02213191986084
Validation loss: 2.0711325471119215

Epoch: 6| Step: 5
Training loss: 2.5002636909484863
Validation loss: 2.0803853927120084

Epoch: 6| Step: 6
Training loss: 2.1529064178466797
Validation loss: 2.1038119049482447

Epoch: 6| Step: 7
Training loss: 1.6680738925933838
Validation loss: 2.1596934385197137

Epoch: 6| Step: 8
Training loss: 1.9865272045135498
Validation loss: 2.191412519383174

Epoch: 6| Step: 9
Training loss: 2.351478338241577
Validation loss: 2.236962031292659

Epoch: 6| Step: 10
Training loss: 2.30425763130188
Validation loss: 2.2271041536843903

Epoch: 6| Step: 11
Training loss: 2.0300347805023193
Validation loss: 2.202463746070862

Epoch: 6| Step: 12
Training loss: 2.0165843963623047
Validation loss: 2.161441492777999

Epoch: 6| Step: 13
Training loss: 2.09006929397583
Validation loss: 2.1432163894817395

Epoch: 233| Step: 0
Training loss: 2.0519208908081055
Validation loss: 2.090768127031224

Epoch: 6| Step: 1
Training loss: 2.3789312839508057
Validation loss: 2.0566930463237147

Epoch: 6| Step: 2
Training loss: 2.5132246017456055
Validation loss: 2.0514807803656465

Epoch: 6| Step: 3
Training loss: 1.5214855670928955
Validation loss: 2.0686674899952386

Epoch: 6| Step: 4
Training loss: 0.9345504641532898
Validation loss: 2.0515444893990793

Epoch: 6| Step: 5
Training loss: 2.0214123725891113
Validation loss: 2.051564285832067

Epoch: 6| Step: 6
Training loss: 1.908989429473877
Validation loss: 2.04908574012018

Epoch: 6| Step: 7
Training loss: 2.1919405460357666
Validation loss: 2.0705221340220463

Epoch: 6| Step: 8
Training loss: 2.085437059402466
Validation loss: 2.063382705052694

Epoch: 6| Step: 9
Training loss: 3.4363508224487305
Validation loss: 2.0583899713331655

Epoch: 6| Step: 10
Training loss: 2.8383290767669678
Validation loss: 2.0771484387818204

Epoch: 6| Step: 11
Training loss: 2.187387466430664
Validation loss: 2.087679538675534

Epoch: 6| Step: 12
Training loss: 2.146108627319336
Validation loss: 2.0934707990256687

Epoch: 6| Step: 13
Training loss: 1.700724482536316
Validation loss: 2.1096279159668954

Epoch: 234| Step: 0
Training loss: 1.5281589031219482
Validation loss: 2.1030669545614593

Epoch: 6| Step: 1
Training loss: 2.4936931133270264
Validation loss: 2.096971365713304

Epoch: 6| Step: 2
Training loss: 2.5081000328063965
Validation loss: 2.0811964337543776

Epoch: 6| Step: 3
Training loss: 2.5338492393493652
Validation loss: 2.08004714340292

Epoch: 6| Step: 4
Training loss: 1.8257548809051514
Validation loss: 2.08087727074982

Epoch: 6| Step: 5
Training loss: 1.7384518384933472
Validation loss: 2.080089730601157

Epoch: 6| Step: 6
Training loss: 2.36387300491333
Validation loss: 2.069302233316565

Epoch: 6| Step: 7
Training loss: 2.1923203468322754
Validation loss: 2.081215021430805

Epoch: 6| Step: 8
Training loss: 2.0516254901885986
Validation loss: 2.0759907717345865

Epoch: 6| Step: 9
Training loss: 2.082909107208252
Validation loss: 2.0806888816177205

Epoch: 6| Step: 10
Training loss: 1.9591898918151855
Validation loss: 2.0819805040154407

Epoch: 6| Step: 11
Training loss: 2.1155683994293213
Validation loss: 2.079196153148528

Epoch: 6| Step: 12
Training loss: 2.7252748012542725
Validation loss: 2.0794976347236225

Epoch: 6| Step: 13
Training loss: 1.4996647834777832
Validation loss: 2.0682582842406405

Epoch: 235| Step: 0
Training loss: 2.771852970123291
Validation loss: 2.058330007778701

Epoch: 6| Step: 1
Training loss: 2.885251522064209
Validation loss: 2.0654540113223496

Epoch: 6| Step: 2
Training loss: 2.3245949745178223
Validation loss: 2.0569214410679315

Epoch: 6| Step: 3
Training loss: 2.1814818382263184
Validation loss: 2.051618827286587

Epoch: 6| Step: 4
Training loss: 1.7556780576705933
Validation loss: 2.045827870727867

Epoch: 6| Step: 5
Training loss: 1.7821468114852905
Validation loss: 2.0498011522395636

Epoch: 6| Step: 6
Training loss: 2.0844764709472656
Validation loss: 2.078557106756395

Epoch: 6| Step: 7
Training loss: 2.0127601623535156
Validation loss: 2.096896529197693

Epoch: 6| Step: 8
Training loss: 1.692995548248291
Validation loss: 2.14800258862075

Epoch: 6| Step: 9
Training loss: 2.040332794189453
Validation loss: 2.1674637666312595

Epoch: 6| Step: 10
Training loss: 2.7767155170440674
Validation loss: 2.2082683822160125

Epoch: 6| Step: 11
Training loss: 1.9764442443847656
Validation loss: 2.1797625967251357

Epoch: 6| Step: 12
Training loss: 2.1456689834594727
Validation loss: 2.1285400211170153

Epoch: 6| Step: 13
Training loss: 1.8127528429031372
Validation loss: 2.078992871827977

Epoch: 236| Step: 0
Training loss: 2.480297565460205
Validation loss: 2.0530621185097644

Epoch: 6| Step: 1
Training loss: 2.6828436851501465
Validation loss: 2.0566259430300806

Epoch: 6| Step: 2
Training loss: 2.2833869457244873
Validation loss: 2.067745977832425

Epoch: 6| Step: 3
Training loss: 1.451322317123413
Validation loss: 2.0831741876499628

Epoch: 6| Step: 4
Training loss: 2.254178524017334
Validation loss: 2.1118637028560845

Epoch: 6| Step: 5
Training loss: 2.2352442741394043
Validation loss: 2.1379223267237344

Epoch: 6| Step: 6
Training loss: 1.8916988372802734
Validation loss: 2.1314692035798104

Epoch: 6| Step: 7
Training loss: 2.6360268592834473
Validation loss: 2.10809729688911

Epoch: 6| Step: 8
Training loss: 2.4595489501953125
Validation loss: 2.0752710898717246

Epoch: 6| Step: 9
Training loss: 1.5715479850769043
Validation loss: 2.0568152114909184

Epoch: 6| Step: 10
Training loss: 1.8421075344085693
Validation loss: 2.074266323479273

Epoch: 6| Step: 11
Training loss: 2.09269380569458
Validation loss: 2.090480953134516

Epoch: 6| Step: 12
Training loss: 2.0643298625946045
Validation loss: 2.162173698025365

Epoch: 6| Step: 13
Training loss: 2.7481532096862793
Validation loss: 2.2241661240977626

Epoch: 237| Step: 0
Training loss: 2.7362139225006104
Validation loss: 2.23677465479861

Epoch: 6| Step: 1
Training loss: 2.7635130882263184
Validation loss: 2.2370099995725896

Epoch: 6| Step: 2
Training loss: 1.6647067070007324
Validation loss: 2.221794036126906

Epoch: 6| Step: 3
Training loss: 2.189512252807617
Validation loss: 2.1849897420534523

Epoch: 6| Step: 4
Training loss: 2.764509916305542
Validation loss: 2.187700968916698

Epoch: 6| Step: 5
Training loss: 2.17329740524292
Validation loss: 2.1624527669722036

Epoch: 6| Step: 6
Training loss: 1.0693299770355225
Validation loss: 2.132642803653594

Epoch: 6| Step: 7
Training loss: 2.246584415435791
Validation loss: 2.100067371963173

Epoch: 6| Step: 8
Training loss: 2.439483642578125
Validation loss: 2.0792144421608216

Epoch: 6| Step: 9
Training loss: 2.1466455459594727
Validation loss: 2.056939632661881

Epoch: 6| Step: 10
Training loss: 1.881032109260559
Validation loss: 2.0639302666469286

Epoch: 6| Step: 11
Training loss: 1.490990400314331
Validation loss: 2.0841306306982554

Epoch: 6| Step: 12
Training loss: 1.9359041452407837
Validation loss: 2.0961719277084514

Epoch: 6| Step: 13
Training loss: 3.391955852508545
Validation loss: 2.0822677650759296

Epoch: 238| Step: 0
Training loss: 2.069483518600464
Validation loss: 2.052747699522203

Epoch: 6| Step: 1
Training loss: 1.6949270963668823
Validation loss: 2.0275351514098463

Epoch: 6| Step: 2
Training loss: 2.748767137527466
Validation loss: 2.02632535401211

Epoch: 6| Step: 3
Training loss: 1.778414249420166
Validation loss: 2.071146612526268

Epoch: 6| Step: 4
Training loss: 1.9260592460632324
Validation loss: 2.095312861986058

Epoch: 6| Step: 5
Training loss: 2.4878296852111816
Validation loss: 2.1036258769291702

Epoch: 6| Step: 6
Training loss: 1.5319266319274902
Validation loss: 2.1184583351176274

Epoch: 6| Step: 7
Training loss: 1.4702467918395996
Validation loss: 2.1397149973018195

Epoch: 6| Step: 8
Training loss: 2.5289900302886963
Validation loss: 2.144863792645034

Epoch: 6| Step: 9
Training loss: 2.3856921195983887
Validation loss: 2.13863234109776

Epoch: 6| Step: 10
Training loss: 2.362659454345703
Validation loss: 2.0947127880588656

Epoch: 6| Step: 11
Training loss: 2.506106376647949
Validation loss: 2.0904662378372683

Epoch: 6| Step: 12
Training loss: 2.372788667678833
Validation loss: 2.066278798605806

Epoch: 6| Step: 13
Training loss: 2.647688865661621
Validation loss: 2.0619674433944044

Epoch: 239| Step: 0
Training loss: 2.5259952545166016
Validation loss: 2.055028091194809

Epoch: 6| Step: 1
Training loss: 1.6959116458892822
Validation loss: 2.0604966250799035

Epoch: 6| Step: 2
Training loss: 1.8402326107025146
Validation loss: 2.0727117433342883

Epoch: 6| Step: 3
Training loss: 2.5592238903045654
Validation loss: 2.078915548580949

Epoch: 6| Step: 4
Training loss: 2.0257604122161865
Validation loss: 2.091832219913442

Epoch: 6| Step: 5
Training loss: 2.9410953521728516
Validation loss: 2.1069513367068384

Epoch: 6| Step: 6
Training loss: 2.427361488342285
Validation loss: 2.1108950517510854

Epoch: 6| Step: 7
Training loss: 1.3885400295257568
Validation loss: 2.109375233291298

Epoch: 6| Step: 8
Training loss: 1.18098783493042
Validation loss: 2.093537166554441

Epoch: 6| Step: 9
Training loss: 1.940108060836792
Validation loss: 2.094967165300923

Epoch: 6| Step: 10
Training loss: 2.6503446102142334
Validation loss: 2.095067124212942

Epoch: 6| Step: 11
Training loss: 2.2103543281555176
Validation loss: 2.0924392336158344

Epoch: 6| Step: 12
Training loss: 2.363158702850342
Validation loss: 2.085466279778429

Epoch: 6| Step: 13
Training loss: 2.0039162635803223
Validation loss: 2.06948096265075

Epoch: 240| Step: 0
Training loss: 2.213766098022461
Validation loss: 2.078289037109703

Epoch: 6| Step: 1
Training loss: 1.6885535717010498
Validation loss: 2.103675916630735

Epoch: 6| Step: 2
Training loss: 1.758246660232544
Validation loss: 2.0985028692471084

Epoch: 6| Step: 3
Training loss: 1.503519058227539
Validation loss: 2.1078214619749334

Epoch: 6| Step: 4
Training loss: 2.099470615386963
Validation loss: 2.120708127175608

Epoch: 6| Step: 5
Training loss: 2.6362037658691406
Validation loss: 2.1228819893252466

Epoch: 6| Step: 6
Training loss: 2.2870426177978516
Validation loss: 2.104565367903761

Epoch: 6| Step: 7
Training loss: 2.3953211307525635
Validation loss: 2.080796598106302

Epoch: 6| Step: 8
Training loss: 2.7829465866088867
Validation loss: 2.066259999429026

Epoch: 6| Step: 9
Training loss: 1.8071517944335938
Validation loss: 2.040868930919196

Epoch: 6| Step: 10
Training loss: 2.660745143890381
Validation loss: 2.0353177593600367

Epoch: 6| Step: 11
Training loss: 2.105132579803467
Validation loss: 2.044940084539434

Epoch: 6| Step: 12
Training loss: 2.0486879348754883
Validation loss: 2.0455494696094143

Epoch: 6| Step: 13
Training loss: 2.2161808013916016
Validation loss: 2.04242423401084

Epoch: 241| Step: 0
Training loss: 2.471921443939209
Validation loss: 2.0494714936902447

Epoch: 6| Step: 1
Training loss: 1.6865761280059814
Validation loss: 2.056129863185267

Epoch: 6| Step: 2
Training loss: 1.6817547082901
Validation loss: 2.066165542089811

Epoch: 6| Step: 3
Training loss: 1.6789624691009521
Validation loss: 2.0743392449553295

Epoch: 6| Step: 4
Training loss: 1.5205137729644775
Validation loss: 2.0994446380164034

Epoch: 6| Step: 5
Training loss: 1.9645037651062012
Validation loss: 2.1281434900017193

Epoch: 6| Step: 6
Training loss: 2.402202606201172
Validation loss: 2.149970664772936

Epoch: 6| Step: 7
Training loss: 2.4780759811401367
Validation loss: 2.1669221257650726

Epoch: 6| Step: 8
Training loss: 2.823439598083496
Validation loss: 2.1688899840078046

Epoch: 6| Step: 9
Training loss: 2.116832733154297
Validation loss: 2.144794528202344

Epoch: 6| Step: 10
Training loss: 2.208615779876709
Validation loss: 2.1183155582797144

Epoch: 6| Step: 11
Training loss: 1.7823487520217896
Validation loss: 2.082955760340537

Epoch: 6| Step: 12
Training loss: 2.3027501106262207
Validation loss: 2.070652555393916

Epoch: 6| Step: 13
Training loss: 3.196654796600342
Validation loss: 2.0518443892079015

Epoch: 242| Step: 0
Training loss: 3.16166615486145
Validation loss: 2.0531604854009484

Epoch: 6| Step: 1
Training loss: 1.4030184745788574
Validation loss: 2.049144350072389

Epoch: 6| Step: 2
Training loss: 2.674809455871582
Validation loss: 2.049750884373983

Epoch: 6| Step: 3
Training loss: 2.5109786987304688
Validation loss: 2.038628666631637

Epoch: 6| Step: 4
Training loss: 2.5564942359924316
Validation loss: 2.036469426206363

Epoch: 6| Step: 5
Training loss: 1.8410917520523071
Validation loss: 2.03805761824372

Epoch: 6| Step: 6
Training loss: 1.5529063940048218
Validation loss: 2.0443079625406573

Epoch: 6| Step: 7
Training loss: 1.4311821460723877
Validation loss: 2.051904023334544

Epoch: 6| Step: 8
Training loss: 2.2423224449157715
Validation loss: 2.057646643730902

Epoch: 6| Step: 9
Training loss: 2.0320382118225098
Validation loss: 2.0749528318323116

Epoch: 6| Step: 10
Training loss: 2.328108549118042
Validation loss: 2.070264590683804

Epoch: 6| Step: 11
Training loss: 2.119472026824951
Validation loss: 2.075085550226191

Epoch: 6| Step: 12
Training loss: 1.8841947317123413
Validation loss: 2.0693113573135866

Epoch: 6| Step: 13
Training loss: 2.012418270111084
Validation loss: 2.0796679168619137

Epoch: 243| Step: 0
Training loss: 1.7804286479949951
Validation loss: 2.0893815948117163

Epoch: 6| Step: 1
Training loss: 1.9174526929855347
Validation loss: 2.0787560516788113

Epoch: 6| Step: 2
Training loss: 1.8452684879302979
Validation loss: 2.0825950714849655

Epoch: 6| Step: 3
Training loss: 2.170314311981201
Validation loss: 2.068301477739888

Epoch: 6| Step: 4
Training loss: 1.8600780963897705
Validation loss: 2.0735859281273297

Epoch: 6| Step: 5
Training loss: 1.5093052387237549
Validation loss: 2.078462759653727

Epoch: 6| Step: 6
Training loss: 3.1443839073181152
Validation loss: 2.0885830066537343

Epoch: 6| Step: 7
Training loss: 1.4106497764587402
Validation loss: 2.07673410061867

Epoch: 6| Step: 8
Training loss: 2.2715904712677
Validation loss: 2.070260429895052

Epoch: 6| Step: 9
Training loss: 1.749833583831787
Validation loss: 2.06147845842505

Epoch: 6| Step: 10
Training loss: 2.271023988723755
Validation loss: 2.0490690456923617

Epoch: 6| Step: 11
Training loss: 2.960904598236084
Validation loss: 2.0545674318908365

Epoch: 6| Step: 12
Training loss: 2.661797285079956
Validation loss: 2.0463880697886148

Epoch: 6| Step: 13
Training loss: 1.554157018661499
Validation loss: 2.0453893676880868

Epoch: 244| Step: 0
Training loss: 2.2925007343292236
Validation loss: 2.039349043241111

Epoch: 6| Step: 1
Training loss: 2.0392584800720215
Validation loss: 2.0317286829794607

Epoch: 6| Step: 2
Training loss: 1.5266458988189697
Validation loss: 2.0369229021892754

Epoch: 6| Step: 3
Training loss: 1.3971304893493652
Validation loss: 2.045367702361076

Epoch: 6| Step: 4
Training loss: 2.166767120361328
Validation loss: 2.04973877117198

Epoch: 6| Step: 5
Training loss: 2.3024191856384277
Validation loss: 2.044833824198733

Epoch: 6| Step: 6
Training loss: 2.4378886222839355
Validation loss: 2.044356588394411

Epoch: 6| Step: 7
Training loss: 2.2444396018981934
Validation loss: 2.0515242468926216

Epoch: 6| Step: 8
Training loss: 1.797886610031128
Validation loss: 2.0703129614553144

Epoch: 6| Step: 9
Training loss: 2.5156664848327637
Validation loss: 2.0665156020913074

Epoch: 6| Step: 10
Training loss: 1.7567660808563232
Validation loss: 2.091832806987147

Epoch: 6| Step: 11
Training loss: 2.3961403369903564
Validation loss: 2.111144019711402

Epoch: 6| Step: 12
Training loss: 1.866745114326477
Validation loss: 2.129611897212203

Epoch: 6| Step: 13
Training loss: 3.0477261543273926
Validation loss: 2.1245522217084

Epoch: 245| Step: 0
Training loss: 1.9289041757583618
Validation loss: 2.115162311061736

Epoch: 6| Step: 1
Training loss: 1.5628749132156372
Validation loss: 2.1114256099988054

Epoch: 6| Step: 2
Training loss: 2.085052490234375
Validation loss: 2.0924756655129055

Epoch: 6| Step: 3
Training loss: 1.90445876121521
Validation loss: 2.088983405020929

Epoch: 6| Step: 4
Training loss: 2.559443473815918
Validation loss: 2.0870581198764104

Epoch: 6| Step: 5
Training loss: 1.5193277597427368
Validation loss: 2.084450908886489

Epoch: 6| Step: 6
Training loss: 1.9637874364852905
Validation loss: 2.072570167562013

Epoch: 6| Step: 7
Training loss: 1.8078888654708862
Validation loss: 2.060021990089006

Epoch: 6| Step: 8
Training loss: 2.7407541275024414
Validation loss: 2.063755040527672

Epoch: 6| Step: 9
Training loss: 2.08658504486084
Validation loss: 2.051064416926394

Epoch: 6| Step: 10
Training loss: 2.6402549743652344
Validation loss: 2.053060534179852

Epoch: 6| Step: 11
Training loss: 1.87555992603302
Validation loss: 2.046955144533547

Epoch: 6| Step: 12
Training loss: 2.546522378921509
Validation loss: 2.0449299786680486

Epoch: 6| Step: 13
Training loss: 1.8788632154464722
Validation loss: 2.0568002731569353

Epoch: 246| Step: 0
Training loss: 2.343010187149048
Validation loss: 2.073863822926757

Epoch: 6| Step: 1
Training loss: 2.174956798553467
Validation loss: 2.0924198986381612

Epoch: 6| Step: 2
Training loss: 2.341437816619873
Validation loss: 2.0949174511817192

Epoch: 6| Step: 3
Training loss: 1.5955103635787964
Validation loss: 2.119113720873351

Epoch: 6| Step: 4
Training loss: 1.884517788887024
Validation loss: 2.1383558165642524

Epoch: 6| Step: 5
Training loss: 1.906693458557129
Validation loss: 2.1190541328922397

Epoch: 6| Step: 6
Training loss: 2.574510097503662
Validation loss: 2.1335533793254564

Epoch: 6| Step: 7
Training loss: 2.418365240097046
Validation loss: 2.1300909032103834

Epoch: 6| Step: 8
Training loss: 1.8823764324188232
Validation loss: 2.106224108767766

Epoch: 6| Step: 9
Training loss: 2.448291778564453
Validation loss: 2.0803710004334808

Epoch: 6| Step: 10
Training loss: 2.108323574066162
Validation loss: 2.0618245165835143

Epoch: 6| Step: 11
Training loss: 1.9423882961273193
Validation loss: 2.0600947500557028

Epoch: 6| Step: 12
Training loss: 1.8532811403274536
Validation loss: 2.062429153791038

Epoch: 6| Step: 13
Training loss: 1.595266580581665
Validation loss: 2.0584790604088896

Epoch: 247| Step: 0
Training loss: 1.9466958045959473
Validation loss: 2.044005875946373

Epoch: 6| Step: 1
Training loss: 2.474620819091797
Validation loss: 2.0488796695586173

Epoch: 6| Step: 2
Training loss: 2.448960542678833
Validation loss: 2.0508430901394097

Epoch: 6| Step: 3
Training loss: 2.222203254699707
Validation loss: 2.029934425507822

Epoch: 6| Step: 4
Training loss: 1.9080144166946411
Validation loss: 2.0462471464628815

Epoch: 6| Step: 5
Training loss: 1.7032990455627441
Validation loss: 2.0460643101763982

Epoch: 6| Step: 6
Training loss: 2.36928391456604
Validation loss: 2.0435139543266705

Epoch: 6| Step: 7
Training loss: 2.545053482055664
Validation loss: 2.045185553130283

Epoch: 6| Step: 8
Training loss: 2.8126277923583984
Validation loss: 2.0558580147322787

Epoch: 6| Step: 9
Training loss: 1.6085292100906372
Validation loss: 2.0417980404310327

Epoch: 6| Step: 10
Training loss: 1.3156156539916992
Validation loss: 2.042351997026833

Epoch: 6| Step: 11
Training loss: 1.226983666419983
Validation loss: 2.0335116155685915

Epoch: 6| Step: 12
Training loss: 2.4999215602874756
Validation loss: 2.047075092151601

Epoch: 6| Step: 13
Training loss: 2.230393171310425
Validation loss: 2.0455808883072226

Epoch: 248| Step: 0
Training loss: 2.5772080421447754
Validation loss: 2.0443771552014094

Epoch: 6| Step: 1
Training loss: 1.9969398975372314
Validation loss: 2.0509566927468903

Epoch: 6| Step: 2
Training loss: 1.5925090312957764
Validation loss: 2.0447104233567432

Epoch: 6| Step: 3
Training loss: 1.9255777597427368
Validation loss: 2.036745996885402

Epoch: 6| Step: 4
Training loss: 1.6648175716400146
Validation loss: 2.058128115951374

Epoch: 6| Step: 5
Training loss: 2.1947710514068604
Validation loss: 2.054823637008667

Epoch: 6| Step: 6
Training loss: 1.7324804067611694
Validation loss: 2.054349781364523

Epoch: 6| Step: 7
Training loss: 2.7128381729125977
Validation loss: 2.059048464221339

Epoch: 6| Step: 8
Training loss: 1.8283262252807617
Validation loss: 2.0500379993069555

Epoch: 6| Step: 9
Training loss: 2.8825621604919434
Validation loss: 2.065313926307104

Epoch: 6| Step: 10
Training loss: 1.7035530805587769
Validation loss: 2.0667523081584642

Epoch: 6| Step: 11
Training loss: 1.9671666622161865
Validation loss: 2.099155769553236

Epoch: 6| Step: 12
Training loss: 1.8943777084350586
Validation loss: 2.0802093654550533

Epoch: 6| Step: 13
Training loss: 2.6236259937286377
Validation loss: 2.0591411923849456

Epoch: 249| Step: 0
Training loss: 2.2494430541992188
Validation loss: 2.0588983028165755

Epoch: 6| Step: 1
Training loss: 2.183927536010742
Validation loss: 2.043335965884629

Epoch: 6| Step: 2
Training loss: 1.6032371520996094
Validation loss: 2.031179012790803

Epoch: 6| Step: 3
Training loss: 1.2726083993911743
Validation loss: 2.029258233244701

Epoch: 6| Step: 4
Training loss: 2.159745216369629
Validation loss: 2.038116498660016

Epoch: 6| Step: 5
Training loss: 1.9895023107528687
Validation loss: 2.048770084176012

Epoch: 6| Step: 6
Training loss: 2.0515949726104736
Validation loss: 2.0628541797719975

Epoch: 6| Step: 7
Training loss: 2.056796073913574
Validation loss: 2.0618194533932592

Epoch: 6| Step: 8
Training loss: 3.1135945320129395
Validation loss: 2.0600963869402484

Epoch: 6| Step: 9
Training loss: 1.8641983270645142
Validation loss: 2.055797985804978

Epoch: 6| Step: 10
Training loss: 2.5431408882141113
Validation loss: 2.054050135356124

Epoch: 6| Step: 11
Training loss: 1.934570074081421
Validation loss: 2.048218064410712

Epoch: 6| Step: 12
Training loss: 1.7537257671356201
Validation loss: 2.042602946681361

Epoch: 6| Step: 13
Training loss: 2.0908327102661133
Validation loss: 2.034132203748149

Epoch: 250| Step: 0
Training loss: 1.9360430240631104
Validation loss: 2.0416617854948966

Epoch: 6| Step: 1
Training loss: 2.2912542819976807
Validation loss: 2.0296300534279115

Epoch: 6| Step: 2
Training loss: 2.2970054149627686
Validation loss: 2.0407693719351165

Epoch: 6| Step: 3
Training loss: 2.921696186065674
Validation loss: 2.0445461375738985

Epoch: 6| Step: 4
Training loss: 2.0985612869262695
Validation loss: 2.040349709090366

Epoch: 6| Step: 5
Training loss: 1.3336451053619385
Validation loss: 2.0441647703929613

Epoch: 6| Step: 6
Training loss: 1.4949926137924194
Validation loss: 2.052039489951185

Epoch: 6| Step: 7
Training loss: 1.9111469984054565
Validation loss: 2.0678705758945917

Epoch: 6| Step: 8
Training loss: 1.7875442504882812
Validation loss: 2.0664627539214266

Epoch: 6| Step: 9
Training loss: 2.5518641471862793
Validation loss: 2.088261127471924

Epoch: 6| Step: 10
Training loss: 1.8866482973098755
Validation loss: 2.0660205118117796

Epoch: 6| Step: 11
Training loss: 2.429395914077759
Validation loss: 2.0585362424132643

Epoch: 6| Step: 12
Training loss: 1.6798853874206543
Validation loss: 2.0643957455952964

Epoch: 6| Step: 13
Training loss: 2.0987114906311035
Validation loss: 2.0630583865668184

Testing loss: 2.2624620146221583

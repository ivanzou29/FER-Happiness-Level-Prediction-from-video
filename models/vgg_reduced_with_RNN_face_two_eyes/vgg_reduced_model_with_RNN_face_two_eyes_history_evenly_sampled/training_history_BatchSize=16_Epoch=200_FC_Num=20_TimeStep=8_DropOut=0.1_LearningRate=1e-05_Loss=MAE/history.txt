Epoch: 1| Step: 0
Training loss: 4.370118141174316
Validation loss: 5.232641015001523

Epoch: 6| Step: 1
Training loss: 5.192673206329346
Validation loss: 5.2242257825789915

Epoch: 6| Step: 2
Training loss: 6.316327095031738
Validation loss: 5.216782200721003

Epoch: 6| Step: 3
Training loss: 5.0262956619262695
Validation loss: 5.209462150450675

Epoch: 6| Step: 4
Training loss: 5.720680236816406
Validation loss: 5.201123032518613

Epoch: 6| Step: 5
Training loss: 4.50466251373291
Validation loss: 5.191754602616833

Epoch: 6| Step: 6
Training loss: 5.363842964172363
Validation loss: 5.181660744451707

Epoch: 6| Step: 7
Training loss: 3.7298405170440674
Validation loss: 5.170980227890835

Epoch: 6| Step: 8
Training loss: 5.175422191619873
Validation loss: 5.158817378423548

Epoch: 6| Step: 9
Training loss: 4.366117000579834
Validation loss: 5.145925814105619

Epoch: 6| Step: 10
Training loss: 4.668087482452393
Validation loss: 5.131845320424726

Epoch: 6| Step: 11
Training loss: 5.602521896362305
Validation loss: 5.116810649953862

Epoch: 6| Step: 12
Training loss: 4.391037464141846
Validation loss: 5.1003813179590365

Epoch: 6| Step: 13
Training loss: 5.182255744934082
Validation loss: 5.083170326807165

Epoch: 2| Step: 0
Training loss: 5.828525543212891
Validation loss: 5.064038415108958

Epoch: 6| Step: 1
Training loss: 4.7009782791137695
Validation loss: 5.043451211785757

Epoch: 6| Step: 2
Training loss: 5.14789342880249
Validation loss: 5.022363662719727

Epoch: 6| Step: 3
Training loss: 3.5888843536376953
Validation loss: 4.9985946942401185

Epoch: 6| Step: 4
Training loss: 4.728664398193359
Validation loss: 4.974617553013627

Epoch: 6| Step: 5
Training loss: 4.634444236755371
Validation loss: 4.94875616668373

Epoch: 6| Step: 6
Training loss: 5.3305816650390625
Validation loss: 4.921587964539887

Epoch: 6| Step: 7
Training loss: 4.771546363830566
Validation loss: 4.892760246030746

Epoch: 6| Step: 8
Training loss: 4.43492317199707
Validation loss: 4.863148796942927

Epoch: 6| Step: 9
Training loss: 4.1748809814453125
Validation loss: 4.831611728155485

Epoch: 6| Step: 10
Training loss: 5.044811725616455
Validation loss: 4.797915397151824

Epoch: 6| Step: 11
Training loss: 5.525473594665527
Validation loss: 4.763243275303995

Epoch: 6| Step: 12
Training loss: 3.815260887145996
Validation loss: 4.728463270330942

Epoch: 6| Step: 13
Training loss: 3.392963171005249
Validation loss: 4.693306076911188

Epoch: 3| Step: 0
Training loss: 4.672036647796631
Validation loss: 4.658008057584045

Epoch: 6| Step: 1
Training loss: 4.046969413757324
Validation loss: 4.622798540258921

Epoch: 6| Step: 2
Training loss: 4.808512210845947
Validation loss: 4.586147487804454

Epoch: 6| Step: 3
Training loss: 3.841485023498535
Validation loss: 4.5504640763805755

Epoch: 6| Step: 4
Training loss: 4.722109317779541
Validation loss: 4.51399254029797

Epoch: 6| Step: 5
Training loss: 4.691913604736328
Validation loss: 4.479126361108595

Epoch: 6| Step: 6
Training loss: 3.1439943313598633
Validation loss: 4.445039456890475

Epoch: 6| Step: 7
Training loss: 4.690560340881348
Validation loss: 4.410446054192

Epoch: 6| Step: 8
Training loss: 4.226022243499756
Validation loss: 4.376195876829086

Epoch: 6| Step: 9
Training loss: 4.106201171875
Validation loss: 4.342014076889202

Epoch: 6| Step: 10
Training loss: 3.9732134342193604
Validation loss: 4.306267025650189

Epoch: 6| Step: 11
Training loss: 4.333028793334961
Validation loss: 4.264716666231873

Epoch: 6| Step: 12
Training loss: 3.980736255645752
Validation loss: 4.213949090691023

Epoch: 6| Step: 13
Training loss: 3.82218074798584
Validation loss: 4.170936056362685

Epoch: 4| Step: 0
Training loss: 4.71993350982666
Validation loss: 4.139870212924096

Epoch: 6| Step: 1
Training loss: 3.929882526397705
Validation loss: 4.109996088089481

Epoch: 6| Step: 2
Training loss: 4.567610740661621
Validation loss: 4.0800704340780936

Epoch: 6| Step: 3
Training loss: 3.6799299716949463
Validation loss: 4.052169363985779

Epoch: 6| Step: 4
Training loss: 4.184690475463867
Validation loss: 4.028089013150943

Epoch: 6| Step: 5
Training loss: 2.7537784576416016
Validation loss: 4.00641094997365

Epoch: 6| Step: 6
Training loss: 3.631974697113037
Validation loss: 3.9826398049631426

Epoch: 6| Step: 7
Training loss: 3.980628728866577
Validation loss: 3.960384358641922

Epoch: 6| Step: 8
Training loss: 3.798882007598877
Validation loss: 3.9364047306840138

Epoch: 6| Step: 9
Training loss: 3.3468968868255615
Validation loss: 3.9100029237808718

Epoch: 6| Step: 10
Training loss: 4.297060012817383
Validation loss: 3.885568752083727

Epoch: 6| Step: 11
Training loss: 3.004929542541504
Validation loss: 3.8600597330318984

Epoch: 6| Step: 12
Training loss: 4.25078010559082
Validation loss: 3.8370831499817553

Epoch: 6| Step: 13
Training loss: 3.23720383644104
Validation loss: 3.8178273657316804

Epoch: 5| Step: 0
Training loss: 3.1981496810913086
Validation loss: 3.792556201258013

Epoch: 6| Step: 1
Training loss: 3.5365514755249023
Validation loss: 3.774644272301787

Epoch: 6| Step: 2
Training loss: 4.380470275878906
Validation loss: 3.7557960992218344

Epoch: 6| Step: 3
Training loss: 3.1759252548217773
Validation loss: 3.7345158130891862

Epoch: 6| Step: 4
Training loss: 3.559433937072754
Validation loss: 3.714402065482191

Epoch: 6| Step: 5
Training loss: 3.518199920654297
Validation loss: 3.6971006701069493

Epoch: 6| Step: 6
Training loss: 4.144559860229492
Validation loss: 3.68234767195999

Epoch: 6| Step: 7
Training loss: 3.9975593090057373
Validation loss: 3.6627426455097813

Epoch: 6| Step: 8
Training loss: 1.9343416690826416
Validation loss: 3.646762465917936

Epoch: 6| Step: 9
Training loss: 3.8826637268066406
Validation loss: 3.6332061726559877

Epoch: 6| Step: 10
Training loss: 3.7379674911499023
Validation loss: 3.617097585431991

Epoch: 6| Step: 11
Training loss: 3.471677780151367
Validation loss: 3.599590070786015

Epoch: 6| Step: 12
Training loss: 4.180100917816162
Validation loss: 3.5854680025449364

Epoch: 6| Step: 13
Training loss: 3.1940488815307617
Validation loss: 3.570797694626675

Epoch: 6| Step: 0
Training loss: 2.9799013137817383
Validation loss: 3.556729176993011

Epoch: 6| Step: 1
Training loss: 5.240938663482666
Validation loss: 3.543583593060893

Epoch: 6| Step: 2
Training loss: 3.896364688873291
Validation loss: 3.5305578042102117

Epoch: 6| Step: 3
Training loss: 3.413095235824585
Validation loss: 3.515540502404654

Epoch: 6| Step: 4
Training loss: 2.3510007858276367
Validation loss: 3.5069030331027125

Epoch: 6| Step: 5
Training loss: 4.400350570678711
Validation loss: 3.4998682057985695

Epoch: 6| Step: 6
Training loss: 4.187100887298584
Validation loss: 3.4882188971324632

Epoch: 6| Step: 7
Training loss: 3.158590316772461
Validation loss: 3.480030011105281

Epoch: 6| Step: 8
Training loss: 3.6589794158935547
Validation loss: 3.4705209732055664

Epoch: 6| Step: 9
Training loss: 2.6757988929748535
Validation loss: 3.4620925431610434

Epoch: 6| Step: 10
Training loss: 2.835517406463623
Validation loss: 3.4561866765381186

Epoch: 6| Step: 11
Training loss: 2.9644417762756348
Validation loss: 3.4476653170841995

Epoch: 6| Step: 12
Training loss: 2.2299704551696777
Validation loss: 3.43675704925291

Epoch: 6| Step: 13
Training loss: 4.484947204589844
Validation loss: 3.427585042932982

Epoch: 7| Step: 0
Training loss: 2.578400135040283
Validation loss: 3.4202993300653275

Epoch: 6| Step: 1
Training loss: 3.321770429611206
Validation loss: 3.4093062185472056

Epoch: 6| Step: 2
Training loss: 3.055128574371338
Validation loss: 3.397838225928686

Epoch: 6| Step: 3
Training loss: 3.179333209991455
Validation loss: 3.386822064717611

Epoch: 6| Step: 4
Training loss: 4.254933834075928
Validation loss: 3.37361002737476

Epoch: 6| Step: 5
Training loss: 2.7503323554992676
Validation loss: 3.3603901273460797

Epoch: 6| Step: 6
Training loss: 4.2173566818237305
Validation loss: 3.351047441523562

Epoch: 6| Step: 7
Training loss: 3.8105573654174805
Validation loss: 3.3441900719878492

Epoch: 6| Step: 8
Training loss: 4.337977886199951
Validation loss: 3.329919502299319

Epoch: 6| Step: 9
Training loss: 2.0775206089019775
Validation loss: 3.3134726298752653

Epoch: 6| Step: 10
Training loss: 3.6760873794555664
Validation loss: 3.3028541508541314

Epoch: 6| Step: 11
Training loss: 3.155961275100708
Validation loss: 3.2949370132979525

Epoch: 6| Step: 12
Training loss: 3.1220059394836426
Validation loss: 3.2814710217137493

Epoch: 6| Step: 13
Training loss: 2.3848958015441895
Validation loss: 3.270403590253604

Epoch: 8| Step: 0
Training loss: 4.0765061378479
Validation loss: 3.2577441251406105

Epoch: 6| Step: 1
Training loss: 3.7286624908447266
Validation loss: 3.2517085793197795

Epoch: 6| Step: 2
Training loss: 3.330716371536255
Validation loss: 3.2437223516484743

Epoch: 6| Step: 3
Training loss: 3.397151470184326
Validation loss: 3.2339411192042853

Epoch: 6| Step: 4
Training loss: 3.1556386947631836
Validation loss: 3.2315634912060154

Epoch: 6| Step: 5
Training loss: 2.3065969944000244
Validation loss: 3.2194817578920754

Epoch: 6| Step: 6
Training loss: 2.7269647121429443
Validation loss: 3.211578699850267

Epoch: 6| Step: 7
Training loss: 2.772260904312134
Validation loss: 3.2009901359517086

Epoch: 6| Step: 8
Training loss: 2.64599609375
Validation loss: 3.19254461924235

Epoch: 6| Step: 9
Training loss: 3.4888105392456055
Validation loss: 3.191834972750756

Epoch: 6| Step: 10
Training loss: 3.8776981830596924
Validation loss: 3.1814606241000596

Epoch: 6| Step: 11
Training loss: 2.1845669746398926
Validation loss: 3.173964021026447

Epoch: 6| Step: 12
Training loss: 3.738478183746338
Validation loss: 3.170190267665412

Epoch: 6| Step: 13
Training loss: 3.6524953842163086
Validation loss: 3.162335270194597

Epoch: 9| Step: 0
Training loss: 4.247076988220215
Validation loss: 3.1570190486087593

Epoch: 6| Step: 1
Training loss: 2.759603261947632
Validation loss: 3.1506078012527956

Epoch: 6| Step: 2
Training loss: 2.5034725666046143
Validation loss: 3.144068238555744

Epoch: 6| Step: 3
Training loss: 2.602076292037964
Validation loss: 3.1381211973005727

Epoch: 6| Step: 4
Training loss: 2.2663495540618896
Validation loss: 3.133241594478648

Epoch: 6| Step: 5
Training loss: 2.2169618606567383
Validation loss: 3.1252108004785355

Epoch: 6| Step: 6
Training loss: 3.795065402984619
Validation loss: 3.1197558167160198

Epoch: 6| Step: 7
Training loss: 3.5160913467407227
Validation loss: 3.114006145026094

Epoch: 6| Step: 8
Training loss: 3.6566364765167236
Validation loss: 3.1121261017296904

Epoch: 6| Step: 9
Training loss: 4.278482437133789
Validation loss: 3.1064238266278337

Epoch: 6| Step: 10
Training loss: 3.2233684062957764
Validation loss: 3.1026559183674474

Epoch: 6| Step: 11
Training loss: 3.584174156188965
Validation loss: 3.0956872970827165

Epoch: 6| Step: 12
Training loss: 2.874656915664673
Validation loss: 3.089696894409836

Epoch: 6| Step: 13
Training loss: 1.9766794443130493
Validation loss: 3.08052731585759

Epoch: 10| Step: 0
Training loss: 2.9295918941497803
Validation loss: 3.0771289461402485

Epoch: 6| Step: 1
Training loss: 1.8756029605865479
Validation loss: 3.072592386635401

Epoch: 6| Step: 2
Training loss: 2.4175407886505127
Validation loss: 3.068613067750008

Epoch: 6| Step: 3
Training loss: 2.6828646659851074
Validation loss: 3.0650643456366753

Epoch: 6| Step: 4
Training loss: 3.1594858169555664
Validation loss: 3.0608342950062086

Epoch: 6| Step: 5
Training loss: 2.8414523601531982
Validation loss: 3.0562643748457714

Epoch: 6| Step: 6
Training loss: 3.1132919788360596
Validation loss: 3.0552165944089174

Epoch: 6| Step: 7
Training loss: 3.7886664867401123
Validation loss: 3.059581659173453

Epoch: 6| Step: 8
Training loss: 3.1877248287200928
Validation loss: 3.0500586776323217

Epoch: 6| Step: 9
Training loss: 2.9365286827087402
Validation loss: 3.0436400469913276

Epoch: 6| Step: 10
Training loss: 4.171246528625488
Validation loss: 3.0362907071267404

Epoch: 6| Step: 11
Training loss: 3.0221927165985107
Validation loss: 3.0276409579861547

Epoch: 6| Step: 12
Training loss: 3.8993067741394043
Validation loss: 3.025351852499029

Epoch: 6| Step: 13
Training loss: 3.374211311340332
Validation loss: 3.0164538660357074

Epoch: 11| Step: 0
Training loss: 2.4369125366210938
Validation loss: 3.0142060710537817

Epoch: 6| Step: 1
Training loss: 3.6829848289489746
Validation loss: 3.0076769244286323

Epoch: 6| Step: 2
Training loss: 3.280851364135742
Validation loss: 3.007020919553695

Epoch: 6| Step: 3
Training loss: 2.6107940673828125
Validation loss: 2.999498790310275

Epoch: 6| Step: 4
Training loss: 3.5125203132629395
Validation loss: 2.9914897129099858

Epoch: 6| Step: 5
Training loss: 3.087909698486328
Validation loss: 2.988676824877339

Epoch: 6| Step: 6
Training loss: 3.378445625305176
Validation loss: 2.984831343414963

Epoch: 6| Step: 7
Training loss: 2.8459854125976562
Validation loss: 2.98351578815009

Epoch: 6| Step: 8
Training loss: 2.555604934692383
Validation loss: 2.9783541438400105

Epoch: 6| Step: 9
Training loss: 3.6604063510894775
Validation loss: 2.975927955360823

Epoch: 6| Step: 10
Training loss: 2.4806137084960938
Validation loss: 2.972172993485646

Epoch: 6| Step: 11
Training loss: 3.5069289207458496
Validation loss: 2.970956048657817

Epoch: 6| Step: 12
Training loss: 2.733748435974121
Validation loss: 2.9610270146400697

Epoch: 6| Step: 13
Training loss: 2.9815382957458496
Validation loss: 2.9640340446144022

Epoch: 12| Step: 0
Training loss: 2.628915309906006
Validation loss: 2.9634296483890985

Epoch: 6| Step: 1
Training loss: 3.1057288646698
Validation loss: 2.955044541307675

Epoch: 6| Step: 2
Training loss: 2.68717098236084
Validation loss: 2.9501980991773706

Epoch: 6| Step: 3
Training loss: 3.135456085205078
Validation loss: 2.949111118111559

Epoch: 6| Step: 4
Training loss: 3.1443421840667725
Validation loss: 2.9463660947738157

Epoch: 6| Step: 5
Training loss: 3.557047128677368
Validation loss: 2.9426845248027513

Epoch: 6| Step: 6
Training loss: 3.091212749481201
Validation loss: 2.941308775255757

Epoch: 6| Step: 7
Training loss: 2.5447542667388916
Validation loss: 2.941991372775006

Epoch: 6| Step: 8
Training loss: 1.872426152229309
Validation loss: 2.940144226115237

Epoch: 6| Step: 9
Training loss: 3.657015323638916
Validation loss: 2.9324381274561726

Epoch: 6| Step: 10
Training loss: 2.657831907272339
Validation loss: 2.9303914834094305

Epoch: 6| Step: 11
Training loss: 3.3150997161865234
Validation loss: 2.9265569999653804

Epoch: 6| Step: 12
Training loss: 3.2346019744873047
Validation loss: 2.9259652527429725

Epoch: 6| Step: 13
Training loss: 4.223115921020508
Validation loss: 2.923275888607066

Epoch: 13| Step: 0
Training loss: 3.472041368484497
Validation loss: 2.923352610680365

Epoch: 6| Step: 1
Training loss: 3.2574868202209473
Validation loss: 2.918268213989914

Epoch: 6| Step: 2
Training loss: 2.175398349761963
Validation loss: 2.91816129479357

Epoch: 6| Step: 3
Training loss: 3.361872673034668
Validation loss: 2.9141977346071632

Epoch: 6| Step: 4
Training loss: 2.9668521881103516
Validation loss: 2.909681817536713

Epoch: 6| Step: 5
Training loss: 2.5875446796417236
Validation loss: 2.907628979734195

Epoch: 6| Step: 6
Training loss: 3.058493137359619
Validation loss: 2.9078024048959055

Epoch: 6| Step: 7
Training loss: 3.0114688873291016
Validation loss: 2.9010895657283005

Epoch: 6| Step: 8
Training loss: 2.9420926570892334
Validation loss: 2.9035362376961658

Epoch: 6| Step: 9
Training loss: 3.198204755783081
Validation loss: 2.9037304668016333

Epoch: 6| Step: 10
Training loss: 2.779101610183716
Validation loss: 2.8970857974021667

Epoch: 6| Step: 11
Training loss: 3.4330546855926514
Validation loss: 2.8989774155360397

Epoch: 6| Step: 12
Training loss: 2.7744100093841553
Validation loss: 2.8959997469379055

Epoch: 6| Step: 13
Training loss: 3.091907024383545
Validation loss: 2.892830782039191

Epoch: 14| Step: 0
Training loss: 1.988518476486206
Validation loss: 2.8984231128487536

Epoch: 6| Step: 1
Training loss: 3.1249608993530273
Validation loss: 2.9005372088442565

Epoch: 6| Step: 2
Training loss: 2.517625093460083
Validation loss: 2.898445483176939

Epoch: 6| Step: 3
Training loss: 2.8889167308807373
Validation loss: 2.9018310936548377

Epoch: 6| Step: 4
Training loss: 3.1341564655303955
Validation loss: 2.8860539133830736

Epoch: 6| Step: 5
Training loss: 3.257192850112915
Validation loss: 2.887226814864784

Epoch: 6| Step: 6
Training loss: 3.6854195594787598
Validation loss: 2.897126308051489

Epoch: 6| Step: 7
Training loss: 3.4511656761169434
Validation loss: 2.8917664481747534

Epoch: 6| Step: 8
Training loss: 2.810093879699707
Validation loss: 2.883115060867802

Epoch: 6| Step: 9
Training loss: 3.6696696281433105
Validation loss: 2.879334072912893

Epoch: 6| Step: 10
Training loss: 2.7537057399749756
Validation loss: 2.8761326625782955

Epoch: 6| Step: 11
Training loss: 2.7783446311950684
Validation loss: 2.890405701052758

Epoch: 6| Step: 12
Training loss: 3.1010289192199707
Validation loss: 2.8825779473909767

Epoch: 6| Step: 13
Training loss: 2.4807348251342773
Validation loss: 2.8748452765967256

Epoch: 15| Step: 0
Training loss: 3.2314934730529785
Validation loss: 2.8664185385550223

Epoch: 6| Step: 1
Training loss: 3.378075122833252
Validation loss: 2.8704239988839753

Epoch: 6| Step: 2
Training loss: 3.102799892425537
Validation loss: 2.8703383681594685

Epoch: 6| Step: 3
Training loss: 3.3451285362243652
Validation loss: 2.8666404088338218

Epoch: 6| Step: 4
Training loss: 2.424077033996582
Validation loss: 2.8599563567869124

Epoch: 6| Step: 5
Training loss: 3.6768877506256104
Validation loss: 2.8611450323494534

Epoch: 6| Step: 6
Training loss: 3.2997355461120605
Validation loss: 2.8589299776220836

Epoch: 6| Step: 7
Training loss: 2.241539478302002
Validation loss: 2.856101405236029

Epoch: 6| Step: 8
Training loss: 1.9714323282241821
Validation loss: 2.851555091078563

Epoch: 6| Step: 9
Training loss: 1.9871952533721924
Validation loss: 2.86037552484902

Epoch: 6| Step: 10
Training loss: 3.9273130893707275
Validation loss: 2.8760021271244174

Epoch: 6| Step: 11
Training loss: 3.000546932220459
Validation loss: 2.851916361880559

Epoch: 6| Step: 12
Training loss: 2.8901612758636475
Validation loss: 2.8529689388890422

Epoch: 6| Step: 13
Training loss: 3.2305915355682373
Validation loss: 2.8581167421033307

Epoch: 16| Step: 0
Training loss: 3.020221710205078
Validation loss: 2.877490479459045

Epoch: 6| Step: 1
Training loss: 3.061223030090332
Validation loss: 2.8696692810263684

Epoch: 6| Step: 2
Training loss: 2.5200743675231934
Validation loss: 2.8729534251715547

Epoch: 6| Step: 3
Training loss: 3.308178663253784
Validation loss: 2.8654682636260986

Epoch: 6| Step: 4
Training loss: 2.7819762229919434
Validation loss: 2.8551805147560696

Epoch: 6| Step: 5
Training loss: 3.406663417816162
Validation loss: 2.8478889952423754

Epoch: 6| Step: 6
Training loss: 3.4878053665161133
Validation loss: 2.851034459247384

Epoch: 6| Step: 7
Training loss: 3.1370673179626465
Validation loss: 2.857140464167441

Epoch: 6| Step: 8
Training loss: 2.528435707092285
Validation loss: 2.861805551795549

Epoch: 6| Step: 9
Training loss: 1.8103398084640503
Validation loss: 2.860159884216965

Epoch: 6| Step: 10
Training loss: 3.744760036468506
Validation loss: 2.8700408217727498

Epoch: 6| Step: 11
Training loss: 3.000654935836792
Validation loss: 2.858309879097887

Epoch: 6| Step: 12
Training loss: 3.0899930000305176
Validation loss: 2.8432010809580484

Epoch: 6| Step: 13
Training loss: 2.425787925720215
Validation loss: 2.8360021396349837

Epoch: 17| Step: 0
Training loss: 3.8896846771240234
Validation loss: 2.842444889007076

Epoch: 6| Step: 1
Training loss: 2.7539281845092773
Validation loss: 2.843055504624562

Epoch: 6| Step: 2
Training loss: 3.6302919387817383
Validation loss: 2.858701116295271

Epoch: 6| Step: 3
Training loss: 3.49741530418396
Validation loss: 2.8742181588244695

Epoch: 6| Step: 4
Training loss: 2.554140567779541
Validation loss: 2.862917577066729

Epoch: 6| Step: 5
Training loss: 2.0586464405059814
Validation loss: 2.8483351943313435

Epoch: 6| Step: 6
Training loss: 2.983529806137085
Validation loss: 2.833890684189335

Epoch: 6| Step: 7
Training loss: 2.8600001335144043
Validation loss: 2.8281130739437637

Epoch: 6| Step: 8
Training loss: 2.891153573989868
Validation loss: 2.8244125919957317

Epoch: 6| Step: 9
Training loss: 2.523988723754883
Validation loss: 2.8292846782233125

Epoch: 6| Step: 10
Training loss: 3.099715232849121
Validation loss: 2.8279307375672045

Epoch: 6| Step: 11
Training loss: 2.9528684616088867
Validation loss: 2.827902151692298

Epoch: 6| Step: 12
Training loss: 2.4900338649749756
Validation loss: 2.8311390492223922

Epoch: 6| Step: 13
Training loss: 3.242419719696045
Validation loss: 2.8230508835084978

Epoch: 18| Step: 0
Training loss: 3.29516339302063
Validation loss: 2.8206490547426286

Epoch: 6| Step: 1
Training loss: 2.988325834274292
Validation loss: 2.8123989694861957

Epoch: 6| Step: 2
Training loss: 3.015653133392334
Validation loss: 2.8179902620213007

Epoch: 6| Step: 3
Training loss: 3.7889699935913086
Validation loss: 2.8175334212600545

Epoch: 6| Step: 4
Training loss: 2.236429452896118
Validation loss: 2.8129563562331663

Epoch: 6| Step: 5
Training loss: 3.257805347442627
Validation loss: 2.8132359135535454

Epoch: 6| Step: 6
Training loss: 3.0990965366363525
Validation loss: 2.8154483020946546

Epoch: 6| Step: 7
Training loss: 2.3635854721069336
Validation loss: 2.8142459341274795

Epoch: 6| Step: 8
Training loss: 3.2089152336120605
Validation loss: 2.8624594929397746

Epoch: 6| Step: 9
Training loss: 2.5620696544647217
Validation loss: 2.8156915556999946

Epoch: 6| Step: 10
Training loss: 3.3256256580352783
Validation loss: 2.8052678236397366

Epoch: 6| Step: 11
Training loss: 2.3722686767578125
Validation loss: 2.8102079796534714

Epoch: 6| Step: 12
Training loss: 2.8639767169952393
Validation loss: 2.8080465614154773

Epoch: 6| Step: 13
Training loss: 2.7639224529266357
Validation loss: 2.8151106372956307

Epoch: 19| Step: 0
Training loss: 3.3076534271240234
Validation loss: 2.8138107074204313

Epoch: 6| Step: 1
Training loss: 2.5894711017608643
Validation loss: 2.8130287354992283

Epoch: 6| Step: 2
Training loss: 1.9856247901916504
Validation loss: 2.8183412372425036

Epoch: 6| Step: 3
Training loss: 3.5498876571655273
Validation loss: 2.8280571532505814

Epoch: 6| Step: 4
Training loss: 3.412994384765625
Validation loss: 2.8347469581070768

Epoch: 6| Step: 5
Training loss: 2.7727203369140625
Validation loss: 2.8285931489800893

Epoch: 6| Step: 6
Training loss: 3.844083786010742
Validation loss: 2.8169541640948226

Epoch: 6| Step: 7
Training loss: 3.0262773036956787
Validation loss: 2.8156314229452484

Epoch: 6| Step: 8
Training loss: 2.6352224349975586
Validation loss: 2.8145276269605084

Epoch: 6| Step: 9
Training loss: 2.5206592082977295
Validation loss: 2.8094427380510556

Epoch: 6| Step: 10
Training loss: 2.7895936965942383
Validation loss: 2.8091866149697253

Epoch: 6| Step: 11
Training loss: 2.509054660797119
Validation loss: 2.805179157564717

Epoch: 6| Step: 12
Training loss: 3.81296968460083
Validation loss: 2.804522222088229

Epoch: 6| Step: 13
Training loss: 1.8630226850509644
Validation loss: 2.7993761724041355

Epoch: 20| Step: 0
Training loss: 3.528269052505493
Validation loss: 2.7998379891918552

Epoch: 6| Step: 1
Training loss: 2.449331760406494
Validation loss: 2.7988603320173038

Epoch: 6| Step: 2
Training loss: 2.2990658283233643
Validation loss: 2.799092984968616

Epoch: 6| Step: 3
Training loss: 3.1508326530456543
Validation loss: 2.797644099881572

Epoch: 6| Step: 4
Training loss: 4.181358337402344
Validation loss: 2.7932637378733647

Epoch: 6| Step: 5
Training loss: 2.9923641681671143
Validation loss: 2.790033619890931

Epoch: 6| Step: 6
Training loss: 2.546772003173828
Validation loss: 2.7905811212396108

Epoch: 6| Step: 7
Training loss: 2.9409220218658447
Validation loss: 2.7858215762722875

Epoch: 6| Step: 8
Training loss: 3.0313222408294678
Validation loss: 2.7868774270498626

Epoch: 6| Step: 9
Training loss: 2.096210479736328
Validation loss: 2.783167782650199

Epoch: 6| Step: 10
Training loss: 1.8094269037246704
Validation loss: 2.78703321180036

Epoch: 6| Step: 11
Training loss: 3.90440034866333
Validation loss: 2.7812821224171627

Epoch: 6| Step: 12
Training loss: 2.5352163314819336
Validation loss: 2.783088304663217

Epoch: 6| Step: 13
Training loss: 3.8685176372528076
Validation loss: 2.7801121383584957

Epoch: 21| Step: 0
Training loss: 2.074265480041504
Validation loss: 2.7848596957422074

Epoch: 6| Step: 1
Training loss: 3.7068796157836914
Validation loss: 2.7900203966325328

Epoch: 6| Step: 2
Training loss: 4.183085918426514
Validation loss: 2.784695686832551

Epoch: 6| Step: 3
Training loss: 1.8858416080474854
Validation loss: 2.779897592400992

Epoch: 6| Step: 4
Training loss: 2.6480321884155273
Validation loss: 2.7765829870777745

Epoch: 6| Step: 5
Training loss: 3.5744199752807617
Validation loss: 2.771999412967313

Epoch: 6| Step: 6
Training loss: 2.2497024536132812
Validation loss: 2.7730125483646186

Epoch: 6| Step: 7
Training loss: 2.553560972213745
Validation loss: 2.7775800381937334

Epoch: 6| Step: 8
Training loss: 3.3684988021850586
Validation loss: 2.7765858250279583

Epoch: 6| Step: 9
Training loss: 3.049281358718872
Validation loss: 2.7701537378372683

Epoch: 6| Step: 10
Training loss: 2.722160816192627
Validation loss: 2.7692847380074124

Epoch: 6| Step: 11
Training loss: 2.462515354156494
Validation loss: 2.772575616836548

Epoch: 6| Step: 12
Training loss: 3.0993661880493164
Validation loss: 2.770213216863653

Epoch: 6| Step: 13
Training loss: 3.4783291816711426
Validation loss: 2.7681620274820635

Epoch: 22| Step: 0
Training loss: 2.9201440811157227
Validation loss: 2.773217257633004

Epoch: 6| Step: 1
Training loss: 3.374847173690796
Validation loss: 2.7804802899719565

Epoch: 6| Step: 2
Training loss: 2.9823622703552246
Validation loss: 2.80099533706583

Epoch: 6| Step: 3
Training loss: 3.020656108856201
Validation loss: 2.797789578796715

Epoch: 6| Step: 4
Training loss: 2.454835891723633
Validation loss: 2.77657566788376

Epoch: 6| Step: 5
Training loss: 3.5904746055603027
Validation loss: 2.776742499361756

Epoch: 6| Step: 6
Training loss: 3.2560739517211914
Validation loss: 2.779173940740606

Epoch: 6| Step: 7
Training loss: 2.3150408267974854
Validation loss: 2.7830902222664125

Epoch: 6| Step: 8
Training loss: 2.5521905422210693
Validation loss: 2.781019200560867

Epoch: 6| Step: 9
Training loss: 2.554490566253662
Validation loss: 2.7753246625264487

Epoch: 6| Step: 10
Training loss: 3.126297950744629
Validation loss: 2.7746132163591284

Epoch: 6| Step: 11
Training loss: 2.480079174041748
Validation loss: 2.7739168392714633

Epoch: 6| Step: 12
Training loss: 3.226902484893799
Validation loss: 2.7685302354956187

Epoch: 6| Step: 13
Training loss: 2.8361289501190186
Validation loss: 2.7661815715092484

Epoch: 23| Step: 0
Training loss: 2.047527313232422
Validation loss: 2.7725808030815533

Epoch: 6| Step: 1
Training loss: 2.929327964782715
Validation loss: 2.789886995028424

Epoch: 6| Step: 2
Training loss: 3.363135814666748
Validation loss: 2.7739832324366414

Epoch: 6| Step: 3
Training loss: 3.077469825744629
Validation loss: 2.7661952459683983

Epoch: 6| Step: 4
Training loss: 3.596583127975464
Validation loss: 2.767493099294683

Epoch: 6| Step: 5
Training loss: 2.9208333492279053
Validation loss: 2.78122793474505

Epoch: 6| Step: 6
Training loss: 2.7392807006835938
Validation loss: 2.8082610022637153

Epoch: 6| Step: 7
Training loss: 3.177812099456787
Validation loss: 2.831474091417046

Epoch: 6| Step: 8
Training loss: 2.2166459560394287
Validation loss: 2.800235189417357

Epoch: 6| Step: 9
Training loss: 2.978238582611084
Validation loss: 2.769314396765924

Epoch: 6| Step: 10
Training loss: 2.6353907585144043
Validation loss: 2.7610071038687103

Epoch: 6| Step: 11
Training loss: 3.0847411155700684
Validation loss: 2.758401401581303

Epoch: 6| Step: 12
Training loss: 3.511706829071045
Validation loss: 2.7576940469844367

Epoch: 6| Step: 13
Training loss: 2.058950662612915
Validation loss: 2.7510643697554067

Epoch: 24| Step: 0
Training loss: 2.6858859062194824
Validation loss: 2.751142530031102

Epoch: 6| Step: 1
Training loss: 3.0488178730010986
Validation loss: 2.750890572865804

Epoch: 6| Step: 2
Training loss: 2.762495756149292
Validation loss: 2.747823005081505

Epoch: 6| Step: 3
Training loss: 2.449471950531006
Validation loss: 2.7535529700658654

Epoch: 6| Step: 4
Training loss: 2.360445499420166
Validation loss: 2.784181192357053

Epoch: 6| Step: 5
Training loss: 2.451571464538574
Validation loss: 2.851308681631601

Epoch: 6| Step: 6
Training loss: 3.416071891784668
Validation loss: 2.926901532757667

Epoch: 6| Step: 7
Training loss: 3.0083913803100586
Validation loss: 2.929343226135418

Epoch: 6| Step: 8
Training loss: 3.3106422424316406
Validation loss: 2.931621648932016

Epoch: 6| Step: 9
Training loss: 3.5141382217407227
Validation loss: 2.915935234356952

Epoch: 6| Step: 10
Training loss: 3.179161310195923
Validation loss: 2.9095563298912457

Epoch: 6| Step: 11
Training loss: 2.2500596046447754
Validation loss: 2.9034996853079846

Epoch: 6| Step: 12
Training loss: 4.022468090057373
Validation loss: 2.9010971335954565

Epoch: 6| Step: 13
Training loss: 3.0195956230163574
Validation loss: 2.8919140933662333

Epoch: 25| Step: 0
Training loss: 2.149412155151367
Validation loss: 2.8822448586904876

Epoch: 6| Step: 1
Training loss: 3.2147374153137207
Validation loss: 2.8731024034561647

Epoch: 6| Step: 2
Training loss: 3.539395809173584
Validation loss: 2.864609436322284

Epoch: 6| Step: 3
Training loss: 2.6388959884643555
Validation loss: 2.8643404488922446

Epoch: 6| Step: 4
Training loss: 3.1110517978668213
Validation loss: 2.879969837845013

Epoch: 6| Step: 5
Training loss: 2.1842873096466064
Validation loss: 2.9103948608521493

Epoch: 6| Step: 6
Training loss: 2.954190969467163
Validation loss: 2.935213683753885

Epoch: 6| Step: 7
Training loss: 3.360933780670166
Validation loss: 2.910888848766204

Epoch: 6| Step: 8
Training loss: 2.594167470932007
Validation loss: 2.8863239006329606

Epoch: 6| Step: 9
Training loss: 3.721217155456543
Validation loss: 2.8726395201939408

Epoch: 6| Step: 10
Training loss: 2.7896265983581543
Validation loss: 2.871420321925994

Epoch: 6| Step: 11
Training loss: 2.5534539222717285
Validation loss: 2.8750400004848355

Epoch: 6| Step: 12
Training loss: 3.5049471855163574
Validation loss: 2.87566606203715

Epoch: 6| Step: 13
Training loss: 3.684282064437866
Validation loss: 2.870891127535092

Epoch: 26| Step: 0
Training loss: 3.2129392623901367
Validation loss: 2.855088964585335

Epoch: 6| Step: 1
Training loss: 3.2392849922180176
Validation loss: 2.8484861620010866

Epoch: 6| Step: 2
Training loss: 2.2226595878601074
Validation loss: 2.849048160737561

Epoch: 6| Step: 3
Training loss: 3.743535280227661
Validation loss: 2.852014846699212

Epoch: 6| Step: 4
Training loss: 3.663971424102783
Validation loss: 2.853304601484729

Epoch: 6| Step: 5
Training loss: 2.524181365966797
Validation loss: 2.8525495606084026

Epoch: 6| Step: 6
Training loss: 2.4783596992492676
Validation loss: 2.844888784552133

Epoch: 6| Step: 7
Training loss: 3.440520763397217
Validation loss: 2.8573378234781246

Epoch: 6| Step: 8
Training loss: 3.3610949516296387
Validation loss: 2.8560612227327082

Epoch: 6| Step: 9
Training loss: 2.3964719772338867
Validation loss: 2.84629790500928

Epoch: 6| Step: 10
Training loss: 2.6098673343658447
Validation loss: 2.8385491755700882

Epoch: 6| Step: 11
Training loss: 3.534418821334839
Validation loss: 2.8325411760678856

Epoch: 6| Step: 12
Training loss: 2.2727999687194824
Validation loss: 2.838564424104588

Epoch: 6| Step: 13
Training loss: 2.422773599624634
Validation loss: 2.846864346534975

Epoch: 27| Step: 0
Training loss: 3.0799989700317383
Validation loss: 2.850019101173647

Epoch: 6| Step: 1
Training loss: 3.36631441116333
Validation loss: 2.864732147544943

Epoch: 6| Step: 2
Training loss: 3.0617928504943848
Validation loss: 2.844930984640634

Epoch: 6| Step: 3
Training loss: 2.237684726715088
Validation loss: 2.8323185084968485

Epoch: 6| Step: 4
Training loss: 2.110454559326172
Validation loss: 2.815775425203385

Epoch: 6| Step: 5
Training loss: 3.1661365032196045
Validation loss: 2.803360846734816

Epoch: 6| Step: 6
Training loss: 3.025911331176758
Validation loss: 2.8011137054812525

Epoch: 6| Step: 7
Training loss: 2.761715888977051
Validation loss: 2.7967730414482856

Epoch: 6| Step: 8
Training loss: 1.88667631149292
Validation loss: 2.794583979473319

Epoch: 6| Step: 9
Training loss: 3.700439453125
Validation loss: 2.7940796165056128

Epoch: 6| Step: 10
Training loss: 2.9941751956939697
Validation loss: 2.7961962684508292

Epoch: 6| Step: 11
Training loss: 3.2094507217407227
Validation loss: 2.800049279325752

Epoch: 6| Step: 12
Training loss: 3.135385513305664
Validation loss: 2.8160598098590808

Epoch: 6| Step: 13
Training loss: 3.401970863342285
Validation loss: 2.8237953339853594

Epoch: 28| Step: 0
Training loss: 2.814607620239258
Validation loss: 2.8107486463362172

Epoch: 6| Step: 1
Training loss: 2.585639715194702
Validation loss: 2.8040310695607173

Epoch: 6| Step: 2
Training loss: 3.2164106369018555
Validation loss: 2.8044543471387637

Epoch: 6| Step: 3
Training loss: 3.023695945739746
Validation loss: 2.8126325402208554

Epoch: 6| Step: 4
Training loss: 2.941983699798584
Validation loss: 2.8075760205586753

Epoch: 6| Step: 5
Training loss: 3.0865554809570312
Validation loss: 2.81241766611735

Epoch: 6| Step: 6
Training loss: 2.7792015075683594
Validation loss: 2.813741591668898

Epoch: 6| Step: 7
Training loss: 2.712911367416382
Validation loss: 2.808166673106532

Epoch: 6| Step: 8
Training loss: 2.7811241149902344
Validation loss: 2.8075147085292365

Epoch: 6| Step: 9
Training loss: 3.104111671447754
Validation loss: 2.8128513264399704

Epoch: 6| Step: 10
Training loss: 3.1993026733398438
Validation loss: 2.8107178454758017

Epoch: 6| Step: 11
Training loss: 2.8369431495666504
Validation loss: 2.8153852954987557

Epoch: 6| Step: 12
Training loss: 2.657881259918213
Validation loss: 2.8127939265261412

Epoch: 6| Step: 13
Training loss: 3.3990437984466553
Validation loss: 2.805155174706572

Epoch: 29| Step: 0
Training loss: 3.2608261108398438
Validation loss: 2.797080542451592

Epoch: 6| Step: 1
Training loss: 2.848609685897827
Validation loss: 2.795478302945373

Epoch: 6| Step: 2
Training loss: 3.1340460777282715
Validation loss: 2.7909466938305925

Epoch: 6| Step: 3
Training loss: 2.8970048427581787
Validation loss: 2.7874576865985827

Epoch: 6| Step: 4
Training loss: 3.7994346618652344
Validation loss: 2.7868011382318314

Epoch: 6| Step: 5
Training loss: 2.2391045093536377
Validation loss: 2.7852213408357356

Epoch: 6| Step: 6
Training loss: 2.1485812664031982
Validation loss: 2.7850796945633425

Epoch: 6| Step: 7
Training loss: 2.4078269004821777
Validation loss: 2.7835889324065177

Epoch: 6| Step: 8
Training loss: 2.938157081604004
Validation loss: 2.7838043782018844

Epoch: 6| Step: 9
Training loss: 3.197958469390869
Validation loss: 2.781650527831047

Epoch: 6| Step: 10
Training loss: 3.488345146179199
Validation loss: 2.7825134543962378

Epoch: 6| Step: 11
Training loss: 2.2566423416137695
Validation loss: 2.7803498929546726

Epoch: 6| Step: 12
Training loss: 3.1691644191741943
Validation loss: 2.7757644089319373

Epoch: 6| Step: 13
Training loss: 2.8956336975097656
Validation loss: 2.7706718111550934

Epoch: 30| Step: 0
Training loss: 3.796867847442627
Validation loss: 2.7712965883234495

Epoch: 6| Step: 1
Training loss: 2.9070582389831543
Validation loss: 2.7698107791203324

Epoch: 6| Step: 2
Training loss: 2.7876994609832764
Validation loss: 2.771238834627213

Epoch: 6| Step: 3
Training loss: 2.5463757514953613
Validation loss: 2.769351400354857

Epoch: 6| Step: 4
Training loss: 2.7620186805725098
Validation loss: 2.7671668837147374

Epoch: 6| Step: 5
Training loss: 3.0061330795288086
Validation loss: 2.769729819349063

Epoch: 6| Step: 6
Training loss: 2.2127017974853516
Validation loss: 2.7674195445993894

Epoch: 6| Step: 7
Training loss: 3.246589183807373
Validation loss: 2.767201728718255

Epoch: 6| Step: 8
Training loss: 2.3112921714782715
Validation loss: 2.7667063641291794

Epoch: 6| Step: 9
Training loss: 2.6821694374084473
Validation loss: 2.7675144210938485

Epoch: 6| Step: 10
Training loss: 2.863199472427368
Validation loss: 2.7651775293452765

Epoch: 6| Step: 11
Training loss: 3.287639617919922
Validation loss: 2.764848268160256

Epoch: 6| Step: 12
Training loss: 3.3954498767852783
Validation loss: 2.768727574297177

Epoch: 6| Step: 13
Training loss: 2.6217615604400635
Validation loss: 2.770526093821372

Epoch: 31| Step: 0
Training loss: 2.856475830078125
Validation loss: 2.7765867274294616

Epoch: 6| Step: 1
Training loss: 2.902167797088623
Validation loss: 2.7723929215502996

Epoch: 6| Step: 2
Training loss: 3.8593719005584717
Validation loss: 2.774827065006379

Epoch: 6| Step: 3
Training loss: 3.247969150543213
Validation loss: 2.766491598980401

Epoch: 6| Step: 4
Training loss: 2.215831995010376
Validation loss: 2.765939163905318

Epoch: 6| Step: 5
Training loss: 2.8946163654327393
Validation loss: 2.7656588144199823

Epoch: 6| Step: 6
Training loss: 1.873934268951416
Validation loss: 2.759758295551423

Epoch: 6| Step: 7
Training loss: 2.5140771865844727
Validation loss: 2.7588140759416806

Epoch: 6| Step: 8
Training loss: 3.783259868621826
Validation loss: 2.764418871172013

Epoch: 6| Step: 9
Training loss: 1.8055963516235352
Validation loss: 2.7668920486204085

Epoch: 6| Step: 10
Training loss: 3.6547975540161133
Validation loss: 2.7654035091400146

Epoch: 6| Step: 11
Training loss: 3.06583833694458
Validation loss: 2.7701284885406494

Epoch: 6| Step: 12
Training loss: 3.3199851512908936
Validation loss: 2.766737038089383

Epoch: 6| Step: 13
Training loss: 2.2875425815582275
Validation loss: 2.766782204310099

Epoch: 32| Step: 0
Training loss: 2.966231346130371
Validation loss: 2.7655657747740388

Epoch: 6| Step: 1
Training loss: 2.5803794860839844
Validation loss: 2.7634828782850698

Epoch: 6| Step: 2
Training loss: 3.0688998699188232
Validation loss: 2.7593027801923853

Epoch: 6| Step: 3
Training loss: 2.386596918106079
Validation loss: 2.758806577292822

Epoch: 6| Step: 4
Training loss: 3.3113245964050293
Validation loss: 2.7539496447450373

Epoch: 6| Step: 5
Training loss: 2.9179024696350098
Validation loss: 2.754828373591105

Epoch: 6| Step: 6
Training loss: 3.140803337097168
Validation loss: 2.747484632717666

Epoch: 6| Step: 7
Training loss: 2.914994478225708
Validation loss: 2.7538428255306777

Epoch: 6| Step: 8
Training loss: 3.3000411987304688
Validation loss: 2.756421540373115

Epoch: 6| Step: 9
Training loss: 2.4030256271362305
Validation loss: 2.7634824552843646

Epoch: 6| Step: 10
Training loss: 2.8719727993011475
Validation loss: 2.7582447195565827

Epoch: 6| Step: 11
Training loss: 2.715338706970215
Validation loss: 2.757569187430925

Epoch: 6| Step: 12
Training loss: 3.1457231044769287
Validation loss: 2.7572222807074107

Epoch: 6| Step: 13
Training loss: 2.5199153423309326
Validation loss: 2.75533563475455

Epoch: 33| Step: 0
Training loss: 2.4169273376464844
Validation loss: 2.7500523726145425

Epoch: 6| Step: 1
Training loss: 3.2215652465820312
Validation loss: 2.74765528914749

Epoch: 6| Step: 2
Training loss: 3.257789134979248
Validation loss: 2.7509599629268853

Epoch: 6| Step: 3
Training loss: 2.4205288887023926
Validation loss: 2.7478231691545054

Epoch: 6| Step: 4
Training loss: 2.6442008018493652
Validation loss: 2.7462037737651537

Epoch: 6| Step: 5
Training loss: 3.610031843185425
Validation loss: 2.746123984295835

Epoch: 6| Step: 6
Training loss: 3.0320193767547607
Validation loss: 2.747713388935212

Epoch: 6| Step: 7
Training loss: 2.9861667156219482
Validation loss: 2.7488489381728636

Epoch: 6| Step: 8
Training loss: 3.2012438774108887
Validation loss: 2.7494037971701673

Epoch: 6| Step: 9
Training loss: 2.4140048027038574
Validation loss: 2.7477783310797905

Epoch: 6| Step: 10
Training loss: 2.628643035888672
Validation loss: 2.7501213986386537

Epoch: 6| Step: 11
Training loss: 2.5393617153167725
Validation loss: 2.7507562073328162

Epoch: 6| Step: 12
Training loss: 3.348456382751465
Validation loss: 2.7488959079147666

Epoch: 6| Step: 13
Training loss: 2.394247055053711
Validation loss: 2.7470379542278986

Epoch: 34| Step: 0
Training loss: 2.6368789672851562
Validation loss: 2.746302625184418

Epoch: 6| Step: 1
Training loss: 2.828871011734009
Validation loss: 2.744328668040614

Epoch: 6| Step: 2
Training loss: 2.670222043991089
Validation loss: 2.7477767544408

Epoch: 6| Step: 3
Training loss: 2.4750053882598877
Validation loss: 2.7527390833823913

Epoch: 6| Step: 4
Training loss: 3.303563117980957
Validation loss: 2.7643245548330326

Epoch: 6| Step: 5
Training loss: 3.301548480987549
Validation loss: 2.7560567163651988

Epoch: 6| Step: 6
Training loss: 2.422276496887207
Validation loss: 2.7568244421353905

Epoch: 6| Step: 7
Training loss: 3.086940050125122
Validation loss: 2.755562948924239

Epoch: 6| Step: 8
Training loss: 2.723503828048706
Validation loss: 2.7552053774556806

Epoch: 6| Step: 9
Training loss: 2.8322620391845703
Validation loss: 2.75293924731593

Epoch: 6| Step: 10
Training loss: 3.0927834510803223
Validation loss: 2.7496601086790844

Epoch: 6| Step: 11
Training loss: 3.741994857788086
Validation loss: 2.7441603624692528

Epoch: 6| Step: 12
Training loss: 2.014753818511963
Validation loss: 2.7417784890820904

Epoch: 6| Step: 13
Training loss: 3.3043901920318604
Validation loss: 2.741191258994482

Epoch: 35| Step: 0
Training loss: 2.812196969985962
Validation loss: 2.7379499968662055

Epoch: 6| Step: 1
Training loss: 3.4871721267700195
Validation loss: 2.7369647564426547

Epoch: 6| Step: 2
Training loss: 2.3750548362731934
Validation loss: 2.7400189189500708

Epoch: 6| Step: 3
Training loss: 2.23647403717041
Validation loss: 2.7373560910583823

Epoch: 6| Step: 4
Training loss: 2.9317808151245117
Validation loss: 2.73722259459957

Epoch: 6| Step: 5
Training loss: 2.458036422729492
Validation loss: 2.7370432064097416

Epoch: 6| Step: 6
Training loss: 3.5550920963287354
Validation loss: 2.7397916675895773

Epoch: 6| Step: 7
Training loss: 3.2223870754241943
Validation loss: 2.7341662940158638

Epoch: 6| Step: 8
Training loss: 2.9608864784240723
Validation loss: 2.7371504204247588

Epoch: 6| Step: 9
Training loss: 2.7169783115386963
Validation loss: 2.735287368938487

Epoch: 6| Step: 10
Training loss: 3.152056932449341
Validation loss: 2.7338965477481967

Epoch: 6| Step: 11
Training loss: 2.1453049182891846
Validation loss: 2.7352339144675963

Epoch: 6| Step: 12
Training loss: 3.2163190841674805
Validation loss: 2.7328600447664977

Epoch: 6| Step: 13
Training loss: 2.852023124694824
Validation loss: 2.7323734580829577

Epoch: 36| Step: 0
Training loss: 3.339329957962036
Validation loss: 2.7324103924535934

Epoch: 6| Step: 1
Training loss: 2.3939409255981445
Validation loss: 2.734874063922513

Epoch: 6| Step: 2
Training loss: 2.4501090049743652
Validation loss: 2.7376879902296167

Epoch: 6| Step: 3
Training loss: 3.0896100997924805
Validation loss: 2.744969957618303

Epoch: 6| Step: 4
Training loss: 3.1074628829956055
Validation loss: 2.746659732634021

Epoch: 6| Step: 5
Training loss: 3.0422134399414062
Validation loss: 2.756841733891477

Epoch: 6| Step: 6
Training loss: 3.114656448364258
Validation loss: 2.7495855849276305

Epoch: 6| Step: 7
Training loss: 3.567261219024658
Validation loss: 2.7563396218002483

Epoch: 6| Step: 8
Training loss: 2.308026075363159
Validation loss: 2.759279756135838

Epoch: 6| Step: 9
Training loss: 2.7136874198913574
Validation loss: 2.7405124351542485

Epoch: 6| Step: 10
Training loss: 3.3061161041259766
Validation loss: 2.7385523293607976

Epoch: 6| Step: 11
Training loss: 2.323411464691162
Validation loss: 2.7386544212218253

Epoch: 6| Step: 12
Training loss: 2.626934289932251
Validation loss: 2.7337418781813754

Epoch: 6| Step: 13
Training loss: 2.670461416244507
Validation loss: 2.7290945232555432

Epoch: 37| Step: 0
Training loss: 3.7210693359375
Validation loss: 2.728430178857619

Epoch: 6| Step: 1
Training loss: 3.373790979385376
Validation loss: 2.723274246338875

Epoch: 6| Step: 2
Training loss: 2.5512332916259766
Validation loss: 2.728912650897939

Epoch: 6| Step: 3
Training loss: 2.5735363960266113
Validation loss: 2.726772541640907

Epoch: 6| Step: 4
Training loss: 2.9489195346832275
Validation loss: 2.7272372168879353

Epoch: 6| Step: 5
Training loss: 3.3677639961242676
Validation loss: 2.7299952353200605

Epoch: 6| Step: 6
Training loss: 2.4941861629486084
Validation loss: 2.728185425522507

Epoch: 6| Step: 7
Training loss: 3.4370598793029785
Validation loss: 2.72627648615068

Epoch: 6| Step: 8
Training loss: 3.0330169200897217
Validation loss: 2.726399157636909

Epoch: 6| Step: 9
Training loss: 2.464761257171631
Validation loss: 2.7294347363133586

Epoch: 6| Step: 10
Training loss: 2.6678314208984375
Validation loss: 2.726353668397473

Epoch: 6| Step: 11
Training loss: 2.7118797302246094
Validation loss: 2.7276540520370647

Epoch: 6| Step: 12
Training loss: 2.2998428344726562
Validation loss: 2.730832879261304

Epoch: 6| Step: 13
Training loss: 2.0562989711761475
Validation loss: 2.7337642869641705

Epoch: 38| Step: 0
Training loss: 2.928741455078125
Validation loss: 2.734114528984152

Epoch: 6| Step: 1
Training loss: 2.2824044227600098
Validation loss: 2.7304717007503716

Epoch: 6| Step: 2
Training loss: 2.621285915374756
Validation loss: 2.738544500002297

Epoch: 6| Step: 3
Training loss: 2.931300640106201
Validation loss: 2.7430138254678376

Epoch: 6| Step: 4
Training loss: 2.480203151702881
Validation loss: 2.7360974845065864

Epoch: 6| Step: 5
Training loss: 3.635476589202881
Validation loss: 2.742721237162108

Epoch: 6| Step: 6
Training loss: 3.009178400039673
Validation loss: 2.7408457776551605

Epoch: 6| Step: 7
Training loss: 3.0158891677856445
Validation loss: 2.7347586411301807

Epoch: 6| Step: 8
Training loss: 3.423600196838379
Validation loss: 2.726045475211195

Epoch: 6| Step: 9
Training loss: 3.634488105773926
Validation loss: 2.7217630212024977

Epoch: 6| Step: 10
Training loss: 3.2081236839294434
Validation loss: 2.7127396778393815

Epoch: 6| Step: 11
Training loss: 1.4668054580688477
Validation loss: 2.7167870254926783

Epoch: 6| Step: 12
Training loss: 2.251432418823242
Validation loss: 2.715146090394707

Epoch: 6| Step: 13
Training loss: 3.1448018550872803
Validation loss: 2.720397041689965

Epoch: 39| Step: 0
Training loss: 2.3668532371520996
Validation loss: 2.7239796910234677

Epoch: 6| Step: 1
Training loss: 2.7721214294433594
Validation loss: 2.7296575013027398

Epoch: 6| Step: 2
Training loss: 2.0244972705841064
Validation loss: 2.731909854437715

Epoch: 6| Step: 3
Training loss: 3.060142993927002
Validation loss: 2.7427902811317035

Epoch: 6| Step: 4
Training loss: 3.1828091144561768
Validation loss: 2.7517083409012004

Epoch: 6| Step: 5
Training loss: 3.4986588954925537
Validation loss: 2.7391822927741596

Epoch: 6| Step: 6
Training loss: 2.729776382446289
Validation loss: 2.720280649841473

Epoch: 6| Step: 7
Training loss: 2.4432473182678223
Validation loss: 2.710045527386409

Epoch: 6| Step: 8
Training loss: 2.7633442878723145
Validation loss: 2.7102726300557456

Epoch: 6| Step: 9
Training loss: 2.898717164993286
Validation loss: 2.7206919834177983

Epoch: 6| Step: 10
Training loss: 3.370431900024414
Validation loss: 2.7142421507066294

Epoch: 6| Step: 11
Training loss: 3.1943845748901367
Validation loss: 2.7150861576039302

Epoch: 6| Step: 12
Training loss: 2.580409526824951
Validation loss: 2.7141211314867904

Epoch: 6| Step: 13
Training loss: 3.5139260292053223
Validation loss: 2.7185650512736332

Epoch: 40| Step: 0
Training loss: 2.2129335403442383
Validation loss: 2.7103867171913065

Epoch: 6| Step: 1
Training loss: 3.02946400642395
Validation loss: 2.711152691994944

Epoch: 6| Step: 2
Training loss: 3.3611018657684326
Validation loss: 2.712645579409856

Epoch: 6| Step: 3
Training loss: 2.741624355316162
Validation loss: 2.7200445077752553

Epoch: 6| Step: 4
Training loss: 3.190614938735962
Validation loss: 2.716342892698062

Epoch: 6| Step: 5
Training loss: 3.1159019470214844
Validation loss: 2.7112797614066833

Epoch: 6| Step: 6
Training loss: 1.9971638917922974
Validation loss: 2.70829673223598

Epoch: 6| Step: 7
Training loss: 2.8809382915496826
Validation loss: 2.707182394560947

Epoch: 6| Step: 8
Training loss: 2.4406986236572266
Validation loss: 2.7066000687178744

Epoch: 6| Step: 9
Training loss: 3.450979232788086
Validation loss: 2.714615511637862

Epoch: 6| Step: 10
Training loss: 2.9238460063934326
Validation loss: 2.7202794526212957

Epoch: 6| Step: 11
Training loss: 2.6489272117614746
Validation loss: 2.7172214805438952

Epoch: 6| Step: 12
Training loss: 3.3547511100769043
Validation loss: 2.722517531405213

Epoch: 6| Step: 13
Training loss: 2.1590816974639893
Validation loss: 2.729726855472852

Epoch: 41| Step: 0
Training loss: 2.9138898849487305
Validation loss: 2.7383466484726116

Epoch: 6| Step: 1
Training loss: 2.6895699501037598
Validation loss: 2.729457239950857

Epoch: 6| Step: 2
Training loss: 1.9628814458847046
Validation loss: 2.736623705074351

Epoch: 6| Step: 3
Training loss: 2.9549083709716797
Validation loss: 2.723102851580548

Epoch: 6| Step: 4
Training loss: 2.557938575744629
Validation loss: 2.712835499035415

Epoch: 6| Step: 5
Training loss: 3.2763876914978027
Validation loss: 2.709031307569114

Epoch: 6| Step: 6
Training loss: 3.488556146621704
Validation loss: 2.7016260239385788

Epoch: 6| Step: 7
Training loss: 2.7468185424804688
Validation loss: 2.7046694576099353

Epoch: 6| Step: 8
Training loss: 3.171783447265625
Validation loss: 2.7046734286892797

Epoch: 6| Step: 9
Training loss: 2.5139260292053223
Validation loss: 2.7013494994050715

Epoch: 6| Step: 10
Training loss: 2.7311649322509766
Validation loss: 2.7052207377649125

Epoch: 6| Step: 11
Training loss: 3.5971508026123047
Validation loss: 2.7044946173185944

Epoch: 6| Step: 12
Training loss: 2.5084171295166016
Validation loss: 2.7027519056873937

Epoch: 6| Step: 13
Training loss: 2.6410911083221436
Validation loss: 2.7045229070930072

Epoch: 42| Step: 0
Training loss: 3.539022207260132
Validation loss: 2.7007270346405687

Epoch: 6| Step: 1
Training loss: 3.1499922275543213
Validation loss: 2.6997373052822646

Epoch: 6| Step: 2
Training loss: 2.1899561882019043
Validation loss: 2.7001483337853545

Epoch: 6| Step: 3
Training loss: 2.5351219177246094
Validation loss: 2.700706056369248

Epoch: 6| Step: 4
Training loss: 2.328305959701538
Validation loss: 2.700695022459953

Epoch: 6| Step: 5
Training loss: 1.958883285522461
Validation loss: 2.706544445407006

Epoch: 6| Step: 6
Training loss: 2.404102087020874
Validation loss: 2.7022083805453394

Epoch: 6| Step: 7
Training loss: 3.0345964431762695
Validation loss: 2.7050933760981404

Epoch: 6| Step: 8
Training loss: 2.946573495864868
Validation loss: 2.6993434967533236

Epoch: 6| Step: 9
Training loss: 3.5095863342285156
Validation loss: 2.702579247054233

Epoch: 6| Step: 10
Training loss: 3.078768014907837
Validation loss: 2.7026667492364043

Epoch: 6| Step: 11
Training loss: 2.6628849506378174
Validation loss: 2.702731260689356

Epoch: 6| Step: 12
Training loss: 3.148080348968506
Validation loss: 2.708883426522696

Epoch: 6| Step: 13
Training loss: 3.411342144012451
Validation loss: 2.7113169188140542

Epoch: 43| Step: 0
Training loss: 2.9947917461395264
Validation loss: 2.7088131955874863

Epoch: 6| Step: 1
Training loss: 3.7714858055114746
Validation loss: 2.709940828302855

Epoch: 6| Step: 2
Training loss: 2.6820545196533203
Validation loss: 2.7157429905347925

Epoch: 6| Step: 3
Training loss: 2.6696600914001465
Validation loss: 2.7122248244541947

Epoch: 6| Step: 4
Training loss: 2.092916488647461
Validation loss: 2.699676339344312

Epoch: 6| Step: 5
Training loss: 2.2939705848693848
Validation loss: 2.6991399385595836

Epoch: 6| Step: 6
Training loss: 3.0232081413269043
Validation loss: 2.6954709893913678

Epoch: 6| Step: 7
Training loss: 3.4819328784942627
Validation loss: 2.689539255634431

Epoch: 6| Step: 8
Training loss: 2.6675302982330322
Validation loss: 2.6922885089792232

Epoch: 6| Step: 9
Training loss: 3.2200655937194824
Validation loss: 2.699591411057339

Epoch: 6| Step: 10
Training loss: 2.156189441680908
Validation loss: 2.703182843423659

Epoch: 6| Step: 11
Training loss: 3.1464953422546387
Validation loss: 2.7061696770370647

Epoch: 6| Step: 12
Training loss: 3.016556739807129
Validation loss: 2.6958725375513874

Epoch: 6| Step: 13
Training loss: 2.198915719985962
Validation loss: 2.692559160212035

Epoch: 44| Step: 0
Training loss: 2.545314311981201
Validation loss: 2.694189574128838

Epoch: 6| Step: 1
Training loss: 1.69654381275177
Validation loss: 2.690012449859291

Epoch: 6| Step: 2
Training loss: 2.2508039474487305
Validation loss: 2.6932507356007895

Epoch: 6| Step: 3
Training loss: 3.1098809242248535
Validation loss: 2.70398739845522

Epoch: 6| Step: 4
Training loss: 2.7545645236968994
Validation loss: 2.7112686454608874

Epoch: 6| Step: 5
Training loss: 2.2908363342285156
Validation loss: 2.712300920999178

Epoch: 6| Step: 6
Training loss: 3.5916013717651367
Validation loss: 2.7145518615681636

Epoch: 6| Step: 7
Training loss: 3.869683265686035
Validation loss: 2.7226139550567954

Epoch: 6| Step: 8
Training loss: 2.68635630607605
Validation loss: 2.7186982042046

Epoch: 6| Step: 9
Training loss: 3.450315475463867
Validation loss: 2.7270737489064536

Epoch: 6| Step: 10
Training loss: 3.046677589416504
Validation loss: 2.7284895630292993

Epoch: 6| Step: 11
Training loss: 2.98146915435791
Validation loss: 2.707738553324053

Epoch: 6| Step: 12
Training loss: 2.638716697692871
Validation loss: 2.6899828090462634

Epoch: 6| Step: 13
Training loss: 2.7403173446655273
Validation loss: 2.687603663372737

Epoch: 45| Step: 0
Training loss: 2.5488064289093018
Validation loss: 2.685806307741391

Epoch: 6| Step: 1
Training loss: 3.290590763092041
Validation loss: 2.6902075634207776

Epoch: 6| Step: 2
Training loss: 2.7419517040252686
Validation loss: 2.7023010561543126

Epoch: 6| Step: 3
Training loss: 2.5147347450256348
Validation loss: 2.715319061792025

Epoch: 6| Step: 4
Training loss: 2.7030184268951416
Validation loss: 2.7256219874146166

Epoch: 6| Step: 5
Training loss: 2.819131851196289
Validation loss: 2.739156446149272

Epoch: 6| Step: 6
Training loss: 2.3606204986572266
Validation loss: 2.728005805323201

Epoch: 6| Step: 7
Training loss: 1.9827511310577393
Validation loss: 2.7130796319694928

Epoch: 6| Step: 8
Training loss: 2.873830556869507
Validation loss: 2.6999824329089095

Epoch: 6| Step: 9
Training loss: 3.6114161014556885
Validation loss: 2.690044992713518

Epoch: 6| Step: 10
Training loss: 3.1292381286621094
Validation loss: 2.6838404004291823

Epoch: 6| Step: 11
Training loss: 3.2716445922851562
Validation loss: 2.6850234000913558

Epoch: 6| Step: 12
Training loss: 3.0231027603149414
Validation loss: 2.692379282366845

Epoch: 6| Step: 13
Training loss: 3.135629415512085
Validation loss: 2.6946623453529934

Epoch: 46| Step: 0
Training loss: 3.1897974014282227
Validation loss: 2.688155151182605

Epoch: 6| Step: 1
Training loss: 2.8978333473205566
Validation loss: 2.6858217254761727

Epoch: 6| Step: 2
Training loss: 2.30087947845459
Validation loss: 2.6768214600060576

Epoch: 6| Step: 3
Training loss: 2.513223648071289
Validation loss: 2.6761708797947055

Epoch: 6| Step: 4
Training loss: 2.680997848510742
Validation loss: 2.6768199192580355

Epoch: 6| Step: 5
Training loss: 3.0009586811065674
Validation loss: 2.680311502948884

Epoch: 6| Step: 6
Training loss: 2.9983227252960205
Validation loss: 2.6810312322390977

Epoch: 6| Step: 7
Training loss: 2.6623759269714355
Validation loss: 2.6830327408288115

Epoch: 6| Step: 8
Training loss: 2.4120683670043945
Validation loss: 2.680381005810153

Epoch: 6| Step: 9
Training loss: 3.971102476119995
Validation loss: 2.677501704103203

Epoch: 6| Step: 10
Training loss: 2.9425323009490967
Validation loss: 2.6738582221410607

Epoch: 6| Step: 11
Training loss: 2.4047060012817383
Validation loss: 2.6740805461842525

Epoch: 6| Step: 12
Training loss: 2.4839673042297363
Validation loss: 2.6766157304086993

Epoch: 6| Step: 13
Training loss: 3.1436636447906494
Validation loss: 2.6735178680830103

Epoch: 47| Step: 0
Training loss: 2.477116584777832
Validation loss: 2.6763144487975747

Epoch: 6| Step: 1
Training loss: 3.4368348121643066
Validation loss: 2.6772463731868292

Epoch: 6| Step: 2
Training loss: 3.437350273132324
Validation loss: 2.6758718670055432

Epoch: 6| Step: 3
Training loss: 2.947068691253662
Validation loss: 2.677510343572145

Epoch: 6| Step: 4
Training loss: 2.8963687419891357
Validation loss: 2.6765786422196256

Epoch: 6| Step: 5
Training loss: 2.843179702758789
Validation loss: 2.675964832305908

Epoch: 6| Step: 6
Training loss: 2.724133253097534
Validation loss: 2.671169355351438

Epoch: 6| Step: 7
Training loss: 2.4394354820251465
Validation loss: 2.6722008156520065

Epoch: 6| Step: 8
Training loss: 2.865630626678467
Validation loss: 2.6737163887229016

Epoch: 6| Step: 9
Training loss: 2.448915481567383
Validation loss: 2.671073144482028

Epoch: 6| Step: 10
Training loss: 2.9705417156219482
Validation loss: 2.664883759713942

Epoch: 6| Step: 11
Training loss: 2.2757062911987305
Validation loss: 2.6693387749374553

Epoch: 6| Step: 12
Training loss: 3.1301801204681396
Validation loss: 2.6699684666049097

Epoch: 6| Step: 13
Training loss: 2.3386378288269043
Validation loss: 2.669302386622275

Epoch: 48| Step: 0
Training loss: 2.9372432231903076
Validation loss: 2.6663743270340787

Epoch: 6| Step: 1
Training loss: 2.0443601608276367
Validation loss: 2.6708388687461935

Epoch: 6| Step: 2
Training loss: 3.5557379722595215
Validation loss: 2.676968023341189

Epoch: 6| Step: 3
Training loss: 3.145792007446289
Validation loss: 2.6702789632222985

Epoch: 6| Step: 4
Training loss: 2.023575782775879
Validation loss: 2.6742333340388473

Epoch: 6| Step: 5
Training loss: 2.7907018661499023
Validation loss: 2.6757090142978135

Epoch: 6| Step: 6
Training loss: 3.1913585662841797
Validation loss: 2.675529054416123

Epoch: 6| Step: 7
Training loss: 2.6744279861450195
Validation loss: 2.6656753324693248

Epoch: 6| Step: 8
Training loss: 2.8234877586364746
Validation loss: 2.668221260911675

Epoch: 6| Step: 9
Training loss: 2.806145191192627
Validation loss: 2.665193521848289

Epoch: 6| Step: 10
Training loss: 2.8612210750579834
Validation loss: 2.6685564492338445

Epoch: 6| Step: 11
Training loss: 3.014389753341675
Validation loss: 2.6667526845009095

Epoch: 6| Step: 12
Training loss: 2.5151376724243164
Validation loss: 2.6675578266061764

Epoch: 6| Step: 13
Training loss: 2.9506654739379883
Validation loss: 2.669081767400106

Epoch: 49| Step: 0
Training loss: 3.293886184692383
Validation loss: 2.6683895434102705

Epoch: 6| Step: 1
Training loss: 2.8667306900024414
Validation loss: 2.6754841650685957

Epoch: 6| Step: 2
Training loss: 2.9575390815734863
Validation loss: 2.6937519863087642

Epoch: 6| Step: 3
Training loss: 3.3101744651794434
Validation loss: 2.7034427324930825

Epoch: 6| Step: 4
Training loss: 3.606872081756592
Validation loss: 2.675965375797723

Epoch: 6| Step: 5
Training loss: 2.933487892150879
Validation loss: 2.6599171212924424

Epoch: 6| Step: 6
Training loss: 1.9161734580993652
Validation loss: 2.658373466102026

Epoch: 6| Step: 7
Training loss: 2.4111335277557373
Validation loss: 2.6583333605079242

Epoch: 6| Step: 8
Training loss: 2.4612300395965576
Validation loss: 2.660004068446416

Epoch: 6| Step: 9
Training loss: 3.664053440093994
Validation loss: 2.6803268873563377

Epoch: 6| Step: 10
Training loss: 2.719909191131592
Validation loss: 2.683160981824321

Epoch: 6| Step: 11
Training loss: 1.9256410598754883
Validation loss: 2.6919713609962055

Epoch: 6| Step: 12
Training loss: 2.4752397537231445
Validation loss: 2.6854912260527253

Epoch: 6| Step: 13
Training loss: 2.9271976947784424
Validation loss: 2.6731481526487615

Epoch: 50| Step: 0
Training loss: 2.595917224884033
Validation loss: 2.6700593579200005

Epoch: 6| Step: 1
Training loss: 2.9414570331573486
Validation loss: 2.657010060484691

Epoch: 6| Step: 2
Training loss: 3.242823362350464
Validation loss: 2.6585865789844143

Epoch: 6| Step: 3
Training loss: 2.678215503692627
Validation loss: 2.6604949018006683

Epoch: 6| Step: 4
Training loss: 3.6634039878845215
Validation loss: 2.6584109619099605

Epoch: 6| Step: 5
Training loss: 2.9760019779205322
Validation loss: 2.6544019124841176

Epoch: 6| Step: 6
Training loss: 2.254180431365967
Validation loss: 2.6561595419401764

Epoch: 6| Step: 7
Training loss: 2.453040599822998
Validation loss: 2.6518539023655716

Epoch: 6| Step: 8
Training loss: 2.609488010406494
Validation loss: 2.6564812814035723

Epoch: 6| Step: 9
Training loss: 3.3069515228271484
Validation loss: 2.6578303562697543

Epoch: 6| Step: 10
Training loss: 2.9598636627197266
Validation loss: 2.6565450417098178

Epoch: 6| Step: 11
Training loss: 2.537397623062134
Validation loss: 2.6570063867876605

Epoch: 6| Step: 12
Training loss: 2.2940149307250977
Validation loss: 2.6588116384321645

Epoch: 6| Step: 13
Training loss: 2.6499791145324707
Validation loss: 2.6553974510521017

Epoch: 51| Step: 0
Training loss: 2.9940943717956543
Validation loss: 2.6585142638093684

Epoch: 6| Step: 1
Training loss: 2.3085761070251465
Validation loss: 2.6569924764735724

Epoch: 6| Step: 2
Training loss: 2.305860996246338
Validation loss: 2.653555888001637

Epoch: 6| Step: 3
Training loss: 2.5413150787353516
Validation loss: 2.6586442352623068

Epoch: 6| Step: 4
Training loss: 2.872084856033325
Validation loss: 2.6594040598920596

Epoch: 6| Step: 5
Training loss: 3.2589359283447266
Validation loss: 2.653366432395033

Epoch: 6| Step: 6
Training loss: 2.3683505058288574
Validation loss: 2.6492079816838747

Epoch: 6| Step: 7
Training loss: 2.894702911376953
Validation loss: 2.6452987681153

Epoch: 6| Step: 8
Training loss: 3.2203006744384766
Validation loss: 2.6460194690253145

Epoch: 6| Step: 9
Training loss: 2.7294697761535645
Validation loss: 2.6483764904801563

Epoch: 6| Step: 10
Training loss: 2.8051328659057617
Validation loss: 2.6515158248204056

Epoch: 6| Step: 11
Training loss: 2.6181530952453613
Validation loss: 2.656401905962216

Epoch: 6| Step: 12
Training loss: 3.6132090091705322
Validation loss: 2.6656649471611105

Epoch: 6| Step: 13
Training loss: 2.524792194366455
Validation loss: 2.679713503006966

Epoch: 52| Step: 0
Training loss: 1.8546749353408813
Validation loss: 2.6877064807440645

Epoch: 6| Step: 1
Training loss: 2.4521307945251465
Validation loss: 2.6722485865316083

Epoch: 6| Step: 2
Training loss: 3.0816407203674316
Validation loss: 2.651892174956619

Epoch: 6| Step: 3
Training loss: 3.0680508613586426
Validation loss: 2.6535149774243756

Epoch: 6| Step: 4
Training loss: 2.987586498260498
Validation loss: 2.644912281344014

Epoch: 6| Step: 5
Training loss: 2.419661045074463
Validation loss: 2.6365607553912747

Epoch: 6| Step: 6
Training loss: 3.454293727874756
Validation loss: 2.6448858502090618

Epoch: 6| Step: 7
Training loss: 2.8999733924865723
Validation loss: 2.6450350258940007

Epoch: 6| Step: 8
Training loss: 2.429520606994629
Validation loss: 2.649606853403071

Epoch: 6| Step: 9
Training loss: 3.2302777767181396
Validation loss: 2.649172475261073

Epoch: 6| Step: 10
Training loss: 2.6935536861419678
Validation loss: 2.651214220190561

Epoch: 6| Step: 11
Training loss: 2.7016372680664062
Validation loss: 2.649566647826984

Epoch: 6| Step: 12
Training loss: 3.0078415870666504
Validation loss: 2.646739903316703

Epoch: 6| Step: 13
Training loss: 2.965333938598633
Validation loss: 2.645944400500226

Epoch: 53| Step: 0
Training loss: 2.1820507049560547
Validation loss: 2.645396239014082

Epoch: 6| Step: 1
Training loss: 3.3180177211761475
Validation loss: 2.6433073474514868

Epoch: 6| Step: 2
Training loss: 2.4581727981567383
Validation loss: 2.6454550399575183

Epoch: 6| Step: 3
Training loss: 2.937255382537842
Validation loss: 2.637628286115585

Epoch: 6| Step: 4
Training loss: 2.970156192779541
Validation loss: 2.6379486053220687

Epoch: 6| Step: 5
Training loss: 2.8928213119506836
Validation loss: 2.639047812390071

Epoch: 6| Step: 6
Training loss: 3.2569527626037598
Validation loss: 2.638741562443395

Epoch: 6| Step: 7
Training loss: 2.5433573722839355
Validation loss: 2.640655476559875

Epoch: 6| Step: 8
Training loss: 3.207843542098999
Validation loss: 2.6450253173869145

Epoch: 6| Step: 9
Training loss: 2.518770933151245
Validation loss: 2.640979287444904

Epoch: 6| Step: 10
Training loss: 2.572105884552002
Validation loss: 2.637237687264719

Epoch: 6| Step: 11
Training loss: 3.073575019836426
Validation loss: 2.6345947275879564

Epoch: 6| Step: 12
Training loss: 2.604018449783325
Validation loss: 2.6316637172493884

Epoch: 6| Step: 13
Training loss: 2.274057626724243
Validation loss: 2.630485737195579

Epoch: 54| Step: 0
Training loss: 2.5055129528045654
Validation loss: 2.6311728262132212

Epoch: 6| Step: 1
Training loss: 2.4873831272125244
Validation loss: 2.6321350348893033

Epoch: 6| Step: 2
Training loss: 2.949422597885132
Validation loss: 2.6295026425392396

Epoch: 6| Step: 3
Training loss: 2.935469150543213
Validation loss: 2.631212821570776

Epoch: 6| Step: 4
Training loss: 2.845521926879883
Validation loss: 2.63101468547698

Epoch: 6| Step: 5
Training loss: 3.2588295936584473
Validation loss: 2.6358421899939097

Epoch: 6| Step: 6
Training loss: 2.2684946060180664
Validation loss: 2.640952007744902

Epoch: 6| Step: 7
Training loss: 2.441096067428589
Validation loss: 2.64265521623755

Epoch: 6| Step: 8
Training loss: 3.2439188957214355
Validation loss: 2.645081280380167

Epoch: 6| Step: 9
Training loss: 2.760801315307617
Validation loss: 2.6489536480237077

Epoch: 6| Step: 10
Training loss: 3.003262996673584
Validation loss: 2.6423800940154702

Epoch: 6| Step: 11
Training loss: 2.313868761062622
Validation loss: 2.6358069142987652

Epoch: 6| Step: 12
Training loss: 2.663104295730591
Validation loss: 2.6320645783537175

Epoch: 6| Step: 13
Training loss: 3.6477572917938232
Validation loss: 2.6260638544636388

Epoch: 55| Step: 0
Training loss: 2.471052646636963
Validation loss: 2.625195787798974

Epoch: 6| Step: 1
Training loss: 2.628572463989258
Validation loss: 2.6297619701713644

Epoch: 6| Step: 2
Training loss: 2.574617624282837
Validation loss: 2.633321500593616

Epoch: 6| Step: 3
Training loss: 2.710880756378174
Validation loss: 2.6309623026078746

Epoch: 6| Step: 4
Training loss: 3.273695945739746
Validation loss: 2.6391943231705697

Epoch: 6| Step: 5
Training loss: 2.274850845336914
Validation loss: 2.6296029270336194

Epoch: 6| Step: 6
Training loss: 3.0470242500305176
Validation loss: 2.62525188538336

Epoch: 6| Step: 7
Training loss: 2.7764315605163574
Validation loss: 2.622891749105146

Epoch: 6| Step: 8
Training loss: 3.5402941703796387
Validation loss: 2.631942605459562

Epoch: 6| Step: 9
Training loss: 2.8513541221618652
Validation loss: 2.641573147107196

Epoch: 6| Step: 10
Training loss: 2.750247001647949
Validation loss: 2.6506731151252665

Epoch: 6| Step: 11
Training loss: 1.971924066543579
Validation loss: 2.661153562607304

Epoch: 6| Step: 12
Training loss: 3.2925567626953125
Validation loss: 2.670232424172022

Epoch: 6| Step: 13
Training loss: 2.7960762977600098
Validation loss: 2.681952948211342

Epoch: 56| Step: 0
Training loss: 2.9605422019958496
Validation loss: 2.672090907250681

Epoch: 6| Step: 1
Training loss: 2.788763999938965
Validation loss: 2.6535723440108763

Epoch: 6| Step: 2
Training loss: 2.464777946472168
Validation loss: 2.6457649866739907

Epoch: 6| Step: 3
Training loss: 2.670858144760132
Validation loss: 2.629822184962611

Epoch: 6| Step: 4
Training loss: 3.346930503845215
Validation loss: 2.619915677655128

Epoch: 6| Step: 5
Training loss: 3.6988940238952637
Validation loss: 2.6250171071739605

Epoch: 6| Step: 6
Training loss: 2.412665605545044
Validation loss: 2.631455498356973

Epoch: 6| Step: 7
Training loss: 2.6992557048797607
Validation loss: 2.640641371409098

Epoch: 6| Step: 8
Training loss: 2.3472166061401367
Validation loss: 2.644274527026761

Epoch: 6| Step: 9
Training loss: 2.8367061614990234
Validation loss: 2.655544473278907

Epoch: 6| Step: 10
Training loss: 2.459357500076294
Validation loss: 2.6628773545706146

Epoch: 6| Step: 11
Training loss: 2.103149652481079
Validation loss: 2.661488227946784

Epoch: 6| Step: 12
Training loss: 2.986356258392334
Validation loss: 2.650894267584688

Epoch: 6| Step: 13
Training loss: 4.058683395385742
Validation loss: 2.6427135339347263

Epoch: 57| Step: 0
Training loss: 2.843698024749756
Validation loss: 2.6352195227017967

Epoch: 6| Step: 1
Training loss: 2.304534912109375
Validation loss: 2.6312892744618077

Epoch: 6| Step: 2
Training loss: 3.3106682300567627
Validation loss: 2.6287004563116256

Epoch: 6| Step: 3
Training loss: 3.1848912239074707
Validation loss: 2.625521106104697

Epoch: 6| Step: 4
Training loss: 2.935936689376831
Validation loss: 2.625173138033959

Epoch: 6| Step: 5
Training loss: 3.4062089920043945
Validation loss: 2.6258792236287105

Epoch: 6| Step: 6
Training loss: 1.704892873764038
Validation loss: 2.6282330610418834

Epoch: 6| Step: 7
Training loss: 2.758394241333008
Validation loss: 2.6313598053429716

Epoch: 6| Step: 8
Training loss: 2.5834200382232666
Validation loss: 2.6339768466129097

Epoch: 6| Step: 9
Training loss: 2.745056390762329
Validation loss: 2.6288742608921503

Epoch: 6| Step: 10
Training loss: 2.5515732765197754
Validation loss: 2.6278019541053363

Epoch: 6| Step: 11
Training loss: 2.530404567718506
Validation loss: 2.627321304813508

Epoch: 6| Step: 12
Training loss: 3.4134678840637207
Validation loss: 2.6223800464343

Epoch: 6| Step: 13
Training loss: 2.6295111179351807
Validation loss: 2.6145160095666045

Epoch: 58| Step: 0
Training loss: 1.8761619329452515
Validation loss: 2.615302878041421

Epoch: 6| Step: 1
Training loss: 2.8533151149749756
Validation loss: 2.6155215899149575

Epoch: 6| Step: 2
Training loss: 3.3133039474487305
Validation loss: 2.6150124303756224

Epoch: 6| Step: 3
Training loss: 2.6639957427978516
Validation loss: 2.617602663655435

Epoch: 6| Step: 4
Training loss: 2.090959072113037
Validation loss: 2.6180710433631815

Epoch: 6| Step: 5
Training loss: 2.4911818504333496
Validation loss: 2.6276772945157942

Epoch: 6| Step: 6
Training loss: 3.6575369834899902
Validation loss: 2.635933219745595

Epoch: 6| Step: 7
Training loss: 3.069257974624634
Validation loss: 2.6441451093201995

Epoch: 6| Step: 8
Training loss: 2.6244421005249023
Validation loss: 2.6401656878891813

Epoch: 6| Step: 9
Training loss: 3.342231273651123
Validation loss: 2.654543138319446

Epoch: 6| Step: 10
Training loss: 2.0761499404907227
Validation loss: 2.6299254291801044

Epoch: 6| Step: 11
Training loss: 3.178845167160034
Validation loss: 2.6227142221184185

Epoch: 6| Step: 12
Training loss: 2.351099967956543
Validation loss: 2.612599552318614

Epoch: 6| Step: 13
Training loss: 3.752633810043335
Validation loss: 2.6135050404456353

Epoch: 59| Step: 0
Training loss: 1.5121148824691772
Validation loss: 2.616982188276065

Epoch: 6| Step: 1
Training loss: 2.84492564201355
Validation loss: 2.615167966452978

Epoch: 6| Step: 2
Training loss: 2.2964329719543457
Validation loss: 2.6174468917231404

Epoch: 6| Step: 3
Training loss: 2.6071486473083496
Validation loss: 2.6196281833033406

Epoch: 6| Step: 4
Training loss: 2.4962403774261475
Validation loss: 2.616762964956222

Epoch: 6| Step: 5
Training loss: 2.1979682445526123
Validation loss: 2.614522744250554

Epoch: 6| Step: 6
Training loss: 3.1691887378692627
Validation loss: 2.6190349184056765

Epoch: 6| Step: 7
Training loss: 2.491586685180664
Validation loss: 2.6367754269671697

Epoch: 6| Step: 8
Training loss: 3.855597496032715
Validation loss: 2.6339631157536663

Epoch: 6| Step: 9
Training loss: 3.5482077598571777
Validation loss: 2.6217243902144896

Epoch: 6| Step: 10
Training loss: 2.8372082710266113
Validation loss: 2.610532796511086

Epoch: 6| Step: 11
Training loss: 3.81915283203125
Validation loss: 2.6117044725725727

Epoch: 6| Step: 12
Training loss: 3.03442120552063
Validation loss: 2.6059027384686213

Epoch: 6| Step: 13
Training loss: 1.9725865125656128
Validation loss: 2.6019308925956808

Epoch: 60| Step: 0
Training loss: 2.0660035610198975
Validation loss: 2.606520116970103

Epoch: 6| Step: 1
Training loss: 2.615114212036133
Validation loss: 2.610443745889971

Epoch: 6| Step: 2
Training loss: 2.5546655654907227
Validation loss: 2.6066556617777836

Epoch: 6| Step: 3
Training loss: 2.3748061656951904
Validation loss: 2.6118168933417207

Epoch: 6| Step: 4
Training loss: 3.401838779449463
Validation loss: 2.611622828309254

Epoch: 6| Step: 5
Training loss: 2.3637771606445312
Validation loss: 2.6125092403863066

Epoch: 6| Step: 6
Training loss: 2.5513176918029785
Validation loss: 2.6080177009746595

Epoch: 6| Step: 7
Training loss: 3.779481887817383
Validation loss: 2.6089443801551737

Epoch: 6| Step: 8
Training loss: 3.3155441284179688
Validation loss: 2.6118418837106354

Epoch: 6| Step: 9
Training loss: 2.540606737136841
Validation loss: 2.609449748070009

Epoch: 6| Step: 10
Training loss: 2.821443557739258
Validation loss: 2.611099135491156

Epoch: 6| Step: 11
Training loss: 2.738330364227295
Validation loss: 2.6113200905502483

Epoch: 6| Step: 12
Training loss: 2.5881195068359375
Validation loss: 2.613062248435072

Epoch: 6| Step: 13
Training loss: 3.30576753616333
Validation loss: 2.6062881613290436

Epoch: 61| Step: 0
Training loss: 2.5091309547424316
Validation loss: 2.6107829540006575

Epoch: 6| Step: 1
Training loss: 2.706474781036377
Validation loss: 2.60668763806743

Epoch: 6| Step: 2
Training loss: 2.434222936630249
Validation loss: 2.6069802186822377

Epoch: 6| Step: 3
Training loss: 2.344815492630005
Validation loss: 2.6040615804733767

Epoch: 6| Step: 4
Training loss: 3.113879919052124
Validation loss: 2.6069064909412014

Epoch: 6| Step: 5
Training loss: 3.31559681892395
Validation loss: 2.607725812542823

Epoch: 6| Step: 6
Training loss: 3.5646848678588867
Validation loss: 2.6008123736227713

Epoch: 6| Step: 7
Training loss: 2.483431100845337
Validation loss: 2.6046072898372525

Epoch: 6| Step: 8
Training loss: 2.670063018798828
Validation loss: 2.6040083900574715

Epoch: 6| Step: 9
Training loss: 2.5474061965942383
Validation loss: 2.604955147671443

Epoch: 6| Step: 10
Training loss: 2.254574775695801
Validation loss: 2.605655748357055

Epoch: 6| Step: 11
Training loss: 2.732330799102783
Validation loss: 2.6049505382455806

Epoch: 6| Step: 12
Training loss: 2.7368826866149902
Validation loss: 2.6025060351176927

Epoch: 6| Step: 13
Training loss: 3.638622760772705
Validation loss: 2.6008563016050603

Epoch: 62| Step: 0
Training loss: 2.98514461517334
Validation loss: 2.6003316602399273

Epoch: 6| Step: 1
Training loss: 3.0681450366973877
Validation loss: 2.6034075060198383

Epoch: 6| Step: 2
Training loss: 3.4394478797912598
Validation loss: 2.615697588971866

Epoch: 6| Step: 3
Training loss: 2.8540358543395996
Validation loss: 2.620809314071491

Epoch: 6| Step: 4
Training loss: 3.166260004043579
Validation loss: 2.6097684521828928

Epoch: 6| Step: 5
Training loss: 3.059767007827759
Validation loss: 2.6012264451672955

Epoch: 6| Step: 6
Training loss: 2.60475754737854
Validation loss: 2.599842876516363

Epoch: 6| Step: 7
Training loss: 2.6157474517822266
Validation loss: 2.5974433998907767

Epoch: 6| Step: 8
Training loss: 1.981534481048584
Validation loss: 2.597595250734719

Epoch: 6| Step: 9
Training loss: 2.6258859634399414
Validation loss: 2.598579381101875

Epoch: 6| Step: 10
Training loss: 2.7196478843688965
Validation loss: 2.5958695950046664

Epoch: 6| Step: 11
Training loss: 2.6307263374328613
Validation loss: 2.6003103435680432

Epoch: 6| Step: 12
Training loss: 2.98207426071167
Validation loss: 2.603026061929682

Epoch: 6| Step: 13
Training loss: 1.3628469705581665
Validation loss: 2.5963904921726515

Epoch: 63| Step: 0
Training loss: 2.530580520629883
Validation loss: 2.5951660345959406

Epoch: 6| Step: 1
Training loss: 3.2601780891418457
Validation loss: 2.591319966059859

Epoch: 6| Step: 2
Training loss: 2.640489339828491
Validation loss: 2.5980533310162124

Epoch: 6| Step: 3
Training loss: 2.1427688598632812
Validation loss: 2.594590263981973

Epoch: 6| Step: 4
Training loss: 3.3683862686157227
Validation loss: 2.603906544305945

Epoch: 6| Step: 5
Training loss: 3.2858734130859375
Validation loss: 2.6117465778063704

Epoch: 6| Step: 6
Training loss: 2.8768115043640137
Validation loss: 2.6152388588074715

Epoch: 6| Step: 7
Training loss: 2.4150357246398926
Validation loss: 2.6227638106192313

Epoch: 6| Step: 8
Training loss: 2.6717958450317383
Validation loss: 2.623935907117782

Epoch: 6| Step: 9
Training loss: 3.608492374420166
Validation loss: 2.613351709099226

Epoch: 6| Step: 10
Training loss: 2.015803575515747
Validation loss: 2.6041422351714103

Epoch: 6| Step: 11
Training loss: 2.6635618209838867
Validation loss: 2.6066347911793697

Epoch: 6| Step: 12
Training loss: 2.5918092727661133
Validation loss: 2.600852817617437

Epoch: 6| Step: 13
Training loss: 2.508965492248535
Validation loss: 2.5967849300753687

Epoch: 64| Step: 0
Training loss: 2.562831401824951
Validation loss: 2.5957204782834618

Epoch: 6| Step: 1
Training loss: 3.086385726928711
Validation loss: 2.593159742252801

Epoch: 6| Step: 2
Training loss: 3.5610709190368652
Validation loss: 2.596739638236261

Epoch: 6| Step: 3
Training loss: 2.2777187824249268
Validation loss: 2.5986328381364063

Epoch: 6| Step: 4
Training loss: 2.5515236854553223
Validation loss: 2.5983367350793656

Epoch: 6| Step: 5
Training loss: 2.9556965827941895
Validation loss: 2.589809251087968

Epoch: 6| Step: 6
Training loss: 2.4282169342041016
Validation loss: 2.5861743932129233

Epoch: 6| Step: 7
Training loss: 2.0881757736206055
Validation loss: 2.588899535517539

Epoch: 6| Step: 8
Training loss: 3.530381917953491
Validation loss: 2.593356647799092

Epoch: 6| Step: 9
Training loss: 2.2616419792175293
Validation loss: 2.5969144016183834

Epoch: 6| Step: 10
Training loss: 2.717961311340332
Validation loss: 2.6019265344066005

Epoch: 6| Step: 11
Training loss: 3.2903671264648438
Validation loss: 2.6097519987372944

Epoch: 6| Step: 12
Training loss: 2.7856409549713135
Validation loss: 2.6187673486689085

Epoch: 6| Step: 13
Training loss: 2.3728184700012207
Validation loss: 2.6178302636710544

Epoch: 65| Step: 0
Training loss: 3.4333839416503906
Validation loss: 2.62027641265623

Epoch: 6| Step: 1
Training loss: 2.745917320251465
Validation loss: 2.6182461810368363

Epoch: 6| Step: 2
Training loss: 2.964620351791382
Validation loss: 2.6173706157233125

Epoch: 6| Step: 3
Training loss: 2.7815847396850586
Validation loss: 2.612800121307373

Epoch: 6| Step: 4
Training loss: 2.1396172046661377
Validation loss: 2.612953971791011

Epoch: 6| Step: 5
Training loss: 2.5551695823669434
Validation loss: 2.610231558481852

Epoch: 6| Step: 6
Training loss: 2.712125778198242
Validation loss: 2.608562295154859

Epoch: 6| Step: 7
Training loss: 3.325096607208252
Validation loss: 2.598130661954162

Epoch: 6| Step: 8
Training loss: 3.055051326751709
Validation loss: 2.5929952257422992

Epoch: 6| Step: 9
Training loss: 2.714658498764038
Validation loss: 2.585121882859097

Epoch: 6| Step: 10
Training loss: 2.26149320602417
Validation loss: 2.576575376654184

Epoch: 6| Step: 11
Training loss: 1.9900152683258057
Validation loss: 2.5753325441832184

Epoch: 6| Step: 12
Training loss: 3.049628257751465
Validation loss: 2.5790011652054323

Epoch: 6| Step: 13
Training loss: 2.898878335952759
Validation loss: 2.582870106543264

Epoch: 66| Step: 0
Training loss: 2.34501314163208
Validation loss: 2.5849780087829917

Epoch: 6| Step: 1
Training loss: 2.6958441734313965
Validation loss: 2.5838894433872674

Epoch: 6| Step: 2
Training loss: 2.7412242889404297
Validation loss: 2.5838949423964306

Epoch: 6| Step: 3
Training loss: 2.9110043048858643
Validation loss: 2.5814253207175963

Epoch: 6| Step: 4
Training loss: 2.9399919509887695
Validation loss: 2.5811598890571186

Epoch: 6| Step: 5
Training loss: 3.076206684112549
Validation loss: 2.5861611571363223

Epoch: 6| Step: 6
Training loss: 2.8318779468536377
Validation loss: 2.588239562126898

Epoch: 6| Step: 7
Training loss: 3.178551197052002
Validation loss: 2.5853545537558933

Epoch: 6| Step: 8
Training loss: 2.753211498260498
Validation loss: 2.5850756065819853

Epoch: 6| Step: 9
Training loss: 2.6021568775177
Validation loss: 2.5773750479503343

Epoch: 6| Step: 10
Training loss: 2.335641384124756
Validation loss: 2.578972383212018

Epoch: 6| Step: 11
Training loss: 2.320037841796875
Validation loss: 2.5760993931883123

Epoch: 6| Step: 12
Training loss: 3.4113755226135254
Validation loss: 2.5785795539937992

Epoch: 6| Step: 13
Training loss: 1.9868003129959106
Validation loss: 2.5789469544605543

Epoch: 67| Step: 0
Training loss: 2.581604480743408
Validation loss: 2.5760990547877487

Epoch: 6| Step: 1
Training loss: 2.071232557296753
Validation loss: 2.577648501242361

Epoch: 6| Step: 2
Training loss: 2.5872113704681396
Validation loss: 2.5865612709394066

Epoch: 6| Step: 3
Training loss: 3.093189239501953
Validation loss: 2.5896683405804377

Epoch: 6| Step: 4
Training loss: 2.54197359085083
Validation loss: 2.5928551176542878

Epoch: 6| Step: 5
Training loss: 3.0454158782958984
Validation loss: 2.597076533943094

Epoch: 6| Step: 6
Training loss: 3.0479111671447754
Validation loss: 2.5964292326281146

Epoch: 6| Step: 7
Training loss: 2.673501968383789
Validation loss: 2.5962874427918465

Epoch: 6| Step: 8
Training loss: 3.709469795227051
Validation loss: 2.5780278944200083

Epoch: 6| Step: 9
Training loss: 3.1732823848724365
Validation loss: 2.5737955365129697

Epoch: 6| Step: 10
Training loss: 2.6797990798950195
Validation loss: 2.576243213427964

Epoch: 6| Step: 11
Training loss: 2.5082194805145264
Validation loss: 2.581351444285403

Epoch: 6| Step: 12
Training loss: 2.579007387161255
Validation loss: 2.58529495680204

Epoch: 6| Step: 13
Training loss: 1.854480504989624
Validation loss: 2.5868533836897982

Epoch: 68| Step: 0
Training loss: 3.1253905296325684
Validation loss: 2.58740504582723

Epoch: 6| Step: 1
Training loss: 2.6129891872406006
Validation loss: 2.5885221265977427

Epoch: 6| Step: 2
Training loss: 2.6576614379882812
Validation loss: 2.5831099556338404

Epoch: 6| Step: 3
Training loss: 2.8860349655151367
Validation loss: 2.5894575195927776

Epoch: 6| Step: 4
Training loss: 2.9448227882385254
Validation loss: 2.587634430136732

Epoch: 6| Step: 5
Training loss: 2.678881883621216
Validation loss: 2.5824880574339177

Epoch: 6| Step: 6
Training loss: 2.813606023788452
Validation loss: 2.580715943408269

Epoch: 6| Step: 7
Training loss: 3.5342824459075928
Validation loss: 2.5820480315916

Epoch: 6| Step: 8
Training loss: 2.879270315170288
Validation loss: 2.5767347607561337

Epoch: 6| Step: 9
Training loss: 2.799738883972168
Validation loss: 2.574646621622065

Epoch: 6| Step: 10
Training loss: 1.9076963663101196
Validation loss: 2.5711356055351997

Epoch: 6| Step: 11
Training loss: 2.4243035316467285
Validation loss: 2.5634476856518815

Epoch: 6| Step: 12
Training loss: 2.1898646354675293
Validation loss: 2.561720012336649

Epoch: 6| Step: 13
Training loss: 3.2322590351104736
Validation loss: 2.562133865971719

Epoch: 69| Step: 0
Training loss: 2.920564651489258
Validation loss: 2.5701319812446513

Epoch: 6| Step: 1
Training loss: 2.4258697032928467
Validation loss: 2.570169618052821

Epoch: 6| Step: 2
Training loss: 2.5232431888580322
Validation loss: 2.5688317027143253

Epoch: 6| Step: 3
Training loss: 2.0740039348602295
Validation loss: 2.5681312648198937

Epoch: 6| Step: 4
Training loss: 2.77466082572937
Validation loss: 2.5644073973419848

Epoch: 6| Step: 5
Training loss: 3.356818914413452
Validation loss: 2.566675857831073

Epoch: 6| Step: 6
Training loss: 3.331120014190674
Validation loss: 2.5658100138428392

Epoch: 6| Step: 7
Training loss: 2.747885227203369
Validation loss: 2.570724048922139

Epoch: 6| Step: 8
Training loss: 3.502758741378784
Validation loss: 2.564466458494945

Epoch: 6| Step: 9
Training loss: 2.3478574752807617
Validation loss: 2.5629669338144283

Epoch: 6| Step: 10
Training loss: 2.8065500259399414
Validation loss: 2.569556164485152

Epoch: 6| Step: 11
Training loss: 1.7505253553390503
Validation loss: 2.568327796074652

Epoch: 6| Step: 12
Training loss: 3.2603747844696045
Validation loss: 2.570692603306104

Epoch: 6| Step: 13
Training loss: 2.563002586364746
Validation loss: 2.569980259864561

Epoch: 70| Step: 0
Training loss: 2.5248966217041016
Validation loss: 2.5652161157259377

Epoch: 6| Step: 1
Training loss: 2.355482578277588
Validation loss: 2.5657753547032676

Epoch: 6| Step: 2
Training loss: 2.1522679328918457
Validation loss: 2.561429677471038

Epoch: 6| Step: 3
Training loss: 2.8635196685791016
Validation loss: 2.5636549175426526

Epoch: 6| Step: 4
Training loss: 3.7128701210021973
Validation loss: 2.566044702324816

Epoch: 6| Step: 5
Training loss: 2.6047558784484863
Validation loss: 2.5620428721110025

Epoch: 6| Step: 6
Training loss: 2.7959697246551514
Validation loss: 2.5672773084332867

Epoch: 6| Step: 7
Training loss: 2.181014060974121
Validation loss: 2.5692293003041256

Epoch: 6| Step: 8
Training loss: 3.036115884780884
Validation loss: 2.564295868719778

Epoch: 6| Step: 9
Training loss: 2.3865013122558594
Validation loss: 2.5610975450085056

Epoch: 6| Step: 10
Training loss: 3.7058372497558594
Validation loss: 2.5557725044988815

Epoch: 6| Step: 11
Training loss: 2.3094322681427
Validation loss: 2.5530943921817246

Epoch: 6| Step: 12
Training loss: 3.032151222229004
Validation loss: 2.5587349271261566

Epoch: 6| Step: 13
Training loss: 2.722507953643799
Validation loss: 2.556717162491173

Epoch: 71| Step: 0
Training loss: 2.656571865081787
Validation loss: 2.5566626620549027

Epoch: 6| Step: 1
Training loss: 2.7084274291992188
Validation loss: 2.562728512671686

Epoch: 6| Step: 2
Training loss: 2.9349043369293213
Validation loss: 2.5581558776158158

Epoch: 6| Step: 3
Training loss: 2.9432711601257324
Validation loss: 2.5550334145945888

Epoch: 6| Step: 4
Training loss: 2.5282859802246094
Validation loss: 2.5589086137792116

Epoch: 6| Step: 5
Training loss: 2.4333479404449463
Validation loss: 2.5610886543027815

Epoch: 6| Step: 6
Training loss: 3.6699719429016113
Validation loss: 2.5688501096540883

Epoch: 6| Step: 7
Training loss: 1.735386610031128
Validation loss: 2.573509339363344

Epoch: 6| Step: 8
Training loss: 3.126750946044922
Validation loss: 2.576195263093518

Epoch: 6| Step: 9
Training loss: 3.236909866333008
Validation loss: 2.569342132537596

Epoch: 6| Step: 10
Training loss: 2.148358106613159
Validation loss: 2.5642737778284217

Epoch: 6| Step: 11
Training loss: 2.401564598083496
Validation loss: 2.5673131353111676

Epoch: 6| Step: 12
Training loss: 3.1317970752716064
Validation loss: 2.5658908146683888

Epoch: 6| Step: 13
Training loss: 2.6724977493286133
Validation loss: 2.5725655094269784

Epoch: 72| Step: 0
Training loss: 2.6459572315216064
Validation loss: 2.5688543986248713

Epoch: 6| Step: 1
Training loss: 2.2529475688934326
Validation loss: 2.5649080173943632

Epoch: 6| Step: 2
Training loss: 2.5882396697998047
Validation loss: 2.561605827782744

Epoch: 6| Step: 3
Training loss: 3.1669020652770996
Validation loss: 2.553072073126352

Epoch: 6| Step: 4
Training loss: 2.9973361492156982
Validation loss: 2.554013193294566

Epoch: 6| Step: 5
Training loss: 2.3588452339172363
Validation loss: 2.551891557631954

Epoch: 6| Step: 6
Training loss: 3.078617572784424
Validation loss: 2.5521283982902445

Epoch: 6| Step: 7
Training loss: 2.823850631713867
Validation loss: 2.559603468064339

Epoch: 6| Step: 8
Training loss: 2.799072504043579
Validation loss: 2.563522244012484

Epoch: 6| Step: 9
Training loss: 3.3100690841674805
Validation loss: 2.5632006891312136

Epoch: 6| Step: 10
Training loss: 2.640258312225342
Validation loss: 2.5785297629653767

Epoch: 6| Step: 11
Training loss: 2.0863771438598633
Validation loss: 2.578753753374982

Epoch: 6| Step: 12
Training loss: 2.845078945159912
Validation loss: 2.562925528454524

Epoch: 6| Step: 13
Training loss: 2.6774404048919678
Validation loss: 2.554609185905867

Epoch: 73| Step: 0
Training loss: 1.7236130237579346
Validation loss: 2.5463518711828415

Epoch: 6| Step: 1
Training loss: 3.440883159637451
Validation loss: 2.554357095431256

Epoch: 6| Step: 2
Training loss: 1.846047043800354
Validation loss: 2.5775244453901887

Epoch: 6| Step: 3
Training loss: 2.4483022689819336
Validation loss: 2.5920113261028

Epoch: 6| Step: 4
Training loss: 3.6910011768341064
Validation loss: 2.5859576066335044

Epoch: 6| Step: 5
Training loss: 2.745474338531494
Validation loss: 2.583097142557944

Epoch: 6| Step: 6
Training loss: 3.0749077796936035
Validation loss: 2.5484996175253265

Epoch: 6| Step: 7
Training loss: 2.6390185356140137
Validation loss: 2.5494157678337506

Epoch: 6| Step: 8
Training loss: 2.992464542388916
Validation loss: 2.5544531114639772

Epoch: 6| Step: 9
Training loss: 2.9190406799316406
Validation loss: 2.572014831727551

Epoch: 6| Step: 10
Training loss: 2.350984573364258
Validation loss: 2.579890868997061

Epoch: 6| Step: 11
Training loss: 2.93335223197937
Validation loss: 2.575792933023104

Epoch: 6| Step: 12
Training loss: 2.409513473510742
Validation loss: 2.5768550390838296

Epoch: 6| Step: 13
Training loss: 3.4042866230010986
Validation loss: 2.572876476472424

Epoch: 74| Step: 0
Training loss: 2.9664132595062256
Validation loss: 2.5545432336868776

Epoch: 6| Step: 1
Training loss: 2.7810487747192383
Validation loss: 2.5484756244126188

Epoch: 6| Step: 2
Training loss: 2.6134514808654785
Validation loss: 2.5383649590194866

Epoch: 6| Step: 3
Training loss: 2.718506336212158
Validation loss: 2.543147384479482

Epoch: 6| Step: 4
Training loss: 1.9547890424728394
Validation loss: 2.544053085388676

Epoch: 6| Step: 5
Training loss: 2.5725951194763184
Validation loss: 2.5467761152534076

Epoch: 6| Step: 6
Training loss: 3.0600461959838867
Validation loss: 2.542850932767314

Epoch: 6| Step: 7
Training loss: 3.997985363006592
Validation loss: 2.543627959425731

Epoch: 6| Step: 8
Training loss: 2.6722373962402344
Validation loss: 2.5452414020415275

Epoch: 6| Step: 9
Training loss: 2.60123872756958
Validation loss: 2.5486040756266606

Epoch: 6| Step: 10
Training loss: 3.0727310180664062
Validation loss: 2.548197502730995

Epoch: 6| Step: 11
Training loss: 1.4764633178710938
Validation loss: 2.546365168786818

Epoch: 6| Step: 12
Training loss: 3.1041316986083984
Validation loss: 2.547122965576828

Epoch: 6| Step: 13
Training loss: 2.5617451667785645
Validation loss: 2.5501435033736692

Epoch: 75| Step: 0
Training loss: 2.996467113494873
Validation loss: 2.5525653234092136

Epoch: 6| Step: 1
Training loss: 3.059577465057373
Validation loss: 2.5577237888049056

Epoch: 6| Step: 2
Training loss: 3.21095871925354
Validation loss: 2.5585678546659407

Epoch: 6| Step: 3
Training loss: 3.022770881652832
Validation loss: 2.548597164051507

Epoch: 6| Step: 4
Training loss: 2.6904728412628174
Validation loss: 2.5505955526905675

Epoch: 6| Step: 5
Training loss: 2.2877357006073
Validation loss: 2.5450777802416074

Epoch: 6| Step: 6
Training loss: 2.81730318069458
Validation loss: 2.5352881262379308

Epoch: 6| Step: 7
Training loss: 2.1404495239257812
Validation loss: 2.5427451556728733

Epoch: 6| Step: 8
Training loss: 2.5801339149475098
Validation loss: 2.535557885323801

Epoch: 6| Step: 9
Training loss: 2.6943633556365967
Validation loss: 2.5454545610694477

Epoch: 6| Step: 10
Training loss: 2.690769672393799
Validation loss: 2.5366533597310386

Epoch: 6| Step: 11
Training loss: 2.3824729919433594
Validation loss: 2.533655930590886

Epoch: 6| Step: 12
Training loss: 2.94276762008667
Validation loss: 2.5322727387951267

Epoch: 6| Step: 13
Training loss: 2.461759328842163
Validation loss: 2.533945060545398

Epoch: 76| Step: 0
Training loss: 3.3392162322998047
Validation loss: 2.5350199899365826

Epoch: 6| Step: 1
Training loss: 2.8621296882629395
Validation loss: 2.5343133711045787

Epoch: 6| Step: 2
Training loss: 2.468393325805664
Validation loss: 2.533849372658678

Epoch: 6| Step: 3
Training loss: 3.5175704956054688
Validation loss: 2.533943801797846

Epoch: 6| Step: 4
Training loss: 1.7830944061279297
Validation loss: 2.5365049480110087

Epoch: 6| Step: 5
Training loss: 1.6390186548233032
Validation loss: 2.531842895733413

Epoch: 6| Step: 6
Training loss: 3.17507266998291
Validation loss: 2.5284894461272867

Epoch: 6| Step: 7
Training loss: 2.8067188262939453
Validation loss: 2.5355711624186528

Epoch: 6| Step: 8
Training loss: 2.719428062438965
Validation loss: 2.531431955675925

Epoch: 6| Step: 9
Training loss: 3.131944179534912
Validation loss: 2.5307564607230564

Epoch: 6| Step: 10
Training loss: 2.877908229827881
Validation loss: 2.5336996483546432

Epoch: 6| Step: 11
Training loss: 2.4402871131896973
Validation loss: 2.5327916529870804

Epoch: 6| Step: 12
Training loss: 2.6578147411346436
Validation loss: 2.530846618836926

Epoch: 6| Step: 13
Training loss: 2.5084633827209473
Validation loss: 2.532893575647826

Epoch: 77| Step: 0
Training loss: 1.8714712858200073
Validation loss: 2.5300650814528107

Epoch: 6| Step: 1
Training loss: 2.473320245742798
Validation loss: 2.5293801035932315

Epoch: 6| Step: 2
Training loss: 3.0101990699768066
Validation loss: 2.530213015053862

Epoch: 6| Step: 3
Training loss: 3.0011515617370605
Validation loss: 2.550570716140091

Epoch: 6| Step: 4
Training loss: 2.0612547397613525
Validation loss: 2.57982991075003

Epoch: 6| Step: 5
Training loss: 2.904268264770508
Validation loss: 2.634913839319701

Epoch: 6| Step: 6
Training loss: 3.2066433429718018
Validation loss: 2.6642520530249483

Epoch: 6| Step: 7
Training loss: 2.22310209274292
Validation loss: 2.63458074549193

Epoch: 6| Step: 8
Training loss: 3.1777548789978027
Validation loss: 2.5956712974015104

Epoch: 6| Step: 9
Training loss: 3.6358728408813477
Validation loss: 2.576598713474889

Epoch: 6| Step: 10
Training loss: 2.325464963912964
Validation loss: 2.5671112819384505

Epoch: 6| Step: 11
Training loss: 2.6529111862182617
Validation loss: 2.5472133416001514

Epoch: 6| Step: 12
Training loss: 2.9426863193511963
Validation loss: 2.5381989735429005

Epoch: 6| Step: 13
Training loss: 2.725421190261841
Validation loss: 2.5340929210826917

Epoch: 78| Step: 0
Training loss: 2.3683528900146484
Validation loss: 2.529517524985857

Epoch: 6| Step: 1
Training loss: 2.996816635131836
Validation loss: 2.5273198030328237

Epoch: 6| Step: 2
Training loss: 3.1282849311828613
Validation loss: 2.5362023897068475

Epoch: 6| Step: 3
Training loss: 2.961820602416992
Validation loss: 2.538005692984468

Epoch: 6| Step: 4
Training loss: 2.5598649978637695
Validation loss: 2.5488950360205864

Epoch: 6| Step: 5
Training loss: 2.8204026222229004
Validation loss: 2.543756172221194

Epoch: 6| Step: 6
Training loss: 2.6901230812072754
Validation loss: 2.5421825916536394

Epoch: 6| Step: 7
Training loss: 2.5217514038085938
Validation loss: 2.5414891345526582

Epoch: 6| Step: 8
Training loss: 3.137599468231201
Validation loss: 2.5393490765684392

Epoch: 6| Step: 9
Training loss: 3.508352756500244
Validation loss: 2.5401515268510386

Epoch: 6| Step: 10
Training loss: 1.795013427734375
Validation loss: 2.543964316768031

Epoch: 6| Step: 11
Training loss: 2.156991481781006
Validation loss: 2.540820872911843

Epoch: 6| Step: 12
Training loss: 2.3071746826171875
Validation loss: 2.534484326198537

Epoch: 6| Step: 13
Training loss: 3.436755657196045
Validation loss: 2.5374225519036733

Epoch: 79| Step: 0
Training loss: 2.7934234142303467
Validation loss: 2.54160576481973

Epoch: 6| Step: 1
Training loss: 3.1220250129699707
Validation loss: 2.5426826938506095

Epoch: 6| Step: 2
Training loss: 3.6102957725524902
Validation loss: 2.5379669948290755

Epoch: 6| Step: 3
Training loss: 2.405029773712158
Validation loss: 2.5250013976968746

Epoch: 6| Step: 4
Training loss: 3.098701000213623
Validation loss: 2.527389164893858

Epoch: 6| Step: 5
Training loss: 2.5630862712860107
Validation loss: 2.5256190966534358

Epoch: 6| Step: 6
Training loss: 2.524416923522949
Validation loss: 2.5193326242508425

Epoch: 6| Step: 7
Training loss: 2.966015100479126
Validation loss: 2.5218891687290643

Epoch: 6| Step: 8
Training loss: 1.7896243333816528
Validation loss: 2.520483378441103

Epoch: 6| Step: 9
Training loss: 2.398540735244751
Validation loss: 2.5216455485231135

Epoch: 6| Step: 10
Training loss: 2.726754665374756
Validation loss: 2.5231143069523636

Epoch: 6| Step: 11
Training loss: 2.561685562133789
Validation loss: 2.525241941534063

Epoch: 6| Step: 12
Training loss: 2.6250011920928955
Validation loss: 2.5233835302373415

Epoch: 6| Step: 13
Training loss: 3.0006184577941895
Validation loss: 2.519241697044783

Epoch: 80| Step: 0
Training loss: 2.9152588844299316
Validation loss: 2.5191894872214204

Epoch: 6| Step: 1
Training loss: 2.3049089908599854
Validation loss: 2.5234579783613964

Epoch: 6| Step: 2
Training loss: 3.13859224319458
Validation loss: 2.5312800971410607

Epoch: 6| Step: 3
Training loss: 2.5362143516540527
Validation loss: 2.5438528701823246

Epoch: 6| Step: 4
Training loss: 2.211479663848877
Validation loss: 2.5493977300582396

Epoch: 6| Step: 5
Training loss: 2.9386239051818848
Validation loss: 2.5763272521316365

Epoch: 6| Step: 6
Training loss: 2.2643964290618896
Validation loss: 2.5884011637779976

Epoch: 6| Step: 7
Training loss: 2.7808408737182617
Validation loss: 2.5973327134245183

Epoch: 6| Step: 8
Training loss: 2.5032660961151123
Validation loss: 2.6069551360222603

Epoch: 6| Step: 9
Training loss: 2.9925403594970703
Validation loss: 2.612458513629052

Epoch: 6| Step: 10
Training loss: 2.682278633117676
Validation loss: 2.6251210346016833

Epoch: 6| Step: 11
Training loss: 2.7618248462677
Validation loss: 2.6268958583954842

Epoch: 6| Step: 12
Training loss: 3.0474116802215576
Validation loss: 2.6449170112609863

Epoch: 6| Step: 13
Training loss: 3.4896481037139893
Validation loss: 2.5965588810623332

Epoch: 81| Step: 0
Training loss: 2.82080340385437
Validation loss: 2.567098553462695

Epoch: 6| Step: 1
Training loss: 3.1454505920410156
Validation loss: 2.558742097629014

Epoch: 6| Step: 2
Training loss: 2.5270795822143555
Validation loss: 2.5274747135818645

Epoch: 6| Step: 3
Training loss: 2.446503162384033
Validation loss: 2.5241454737160796

Epoch: 6| Step: 4
Training loss: 2.347245693206787
Validation loss: 2.5198703504377797

Epoch: 6| Step: 5
Training loss: 1.6336305141448975
Validation loss: 2.5262998355332242

Epoch: 6| Step: 6
Training loss: 3.105499267578125
Validation loss: 2.534743232111777

Epoch: 6| Step: 7
Training loss: 2.4416918754577637
Validation loss: 2.5535824555222706

Epoch: 6| Step: 8
Training loss: 2.9373505115509033
Validation loss: 2.564458816282211

Epoch: 6| Step: 9
Training loss: 3.285221576690674
Validation loss: 2.578224256474485

Epoch: 6| Step: 10
Training loss: 3.187012195587158
Validation loss: 2.6437237801090365

Epoch: 6| Step: 11
Training loss: 2.732506275177002
Validation loss: 2.632163852773687

Epoch: 6| Step: 12
Training loss: 2.9199113845825195
Validation loss: 2.5929771469485376

Epoch: 6| Step: 13
Training loss: 3.0002119541168213
Validation loss: 2.5513915477260465

Epoch: 82| Step: 0
Training loss: 3.263009786605835
Validation loss: 2.5282531348607873

Epoch: 6| Step: 1
Training loss: 3.4746809005737305
Validation loss: 2.51687317509805

Epoch: 6| Step: 2
Training loss: 2.179332971572876
Validation loss: 2.5141998567888812

Epoch: 6| Step: 3
Training loss: 2.580983877182007
Validation loss: 2.5172079019649054

Epoch: 6| Step: 4
Training loss: 2.8235392570495605
Validation loss: 2.5399032177463656

Epoch: 6| Step: 5
Training loss: 3.0436580181121826
Validation loss: 2.559382397641418

Epoch: 6| Step: 6
Training loss: 2.4095797538757324
Validation loss: 2.5761480792876212

Epoch: 6| Step: 7
Training loss: 2.719489812850952
Validation loss: 2.6036989663236882

Epoch: 6| Step: 8
Training loss: 2.8313798904418945
Validation loss: 2.596962639080581

Epoch: 6| Step: 9
Training loss: 2.7250218391418457
Validation loss: 2.590356514018069

Epoch: 6| Step: 10
Training loss: 2.463574171066284
Validation loss: 2.555140064608666

Epoch: 6| Step: 11
Training loss: 2.5038199424743652
Validation loss: 2.526985793985346

Epoch: 6| Step: 12
Training loss: 2.3123955726623535
Validation loss: 2.5159827252869964

Epoch: 6| Step: 13
Training loss: 3.0083608627319336
Validation loss: 2.5126755558034426

Epoch: 83| Step: 0
Training loss: 3.105045795440674
Validation loss: 2.5089086973538963

Epoch: 6| Step: 1
Training loss: 2.5514469146728516
Validation loss: 2.5162336262323524

Epoch: 6| Step: 2
Training loss: 3.1050939559936523
Validation loss: 2.5215747074414323

Epoch: 6| Step: 3
Training loss: 2.096811294555664
Validation loss: 2.5254119826901342

Epoch: 6| Step: 4
Training loss: 2.3101823329925537
Validation loss: 2.529761101609917

Epoch: 6| Step: 5
Training loss: 2.3502771854400635
Validation loss: 2.537952966587518

Epoch: 6| Step: 6
Training loss: 3.0368990898132324
Validation loss: 2.5352255041881273

Epoch: 6| Step: 7
Training loss: 3.5827903747558594
Validation loss: 2.5402441255507933

Epoch: 6| Step: 8
Training loss: 2.3958449363708496
Validation loss: 2.5377164297206427

Epoch: 6| Step: 9
Training loss: 2.029571056365967
Validation loss: 2.526133483456027

Epoch: 6| Step: 10
Training loss: 3.0049796104431152
Validation loss: 2.5175554547258603

Epoch: 6| Step: 11
Training loss: 2.280517578125
Validation loss: 2.521329959233602

Epoch: 6| Step: 12
Training loss: 3.107114553451538
Validation loss: 2.530692623507592

Epoch: 6| Step: 13
Training loss: 3.5747265815734863
Validation loss: 2.5303349597479707

Epoch: 84| Step: 0
Training loss: 3.5538902282714844
Validation loss: 2.533460283792147

Epoch: 6| Step: 1
Training loss: 2.3997743129730225
Validation loss: 2.529432040388866

Epoch: 6| Step: 2
Training loss: 2.3583974838256836
Validation loss: 2.5255436307640484

Epoch: 6| Step: 3
Training loss: 2.5820703506469727
Validation loss: 2.522017103369518

Epoch: 6| Step: 4
Training loss: 3.0389151573181152
Validation loss: 2.516892389584613

Epoch: 6| Step: 5
Training loss: 3.4813084602355957
Validation loss: 2.523592072148477

Epoch: 6| Step: 6
Training loss: 2.668063163757324
Validation loss: 2.521332181910033

Epoch: 6| Step: 7
Training loss: 3.0507850646972656
Validation loss: 2.521576299462267

Epoch: 6| Step: 8
Training loss: 2.468953847885132
Validation loss: 2.515634731579852

Epoch: 6| Step: 9
Training loss: 2.2624151706695557
Validation loss: 2.5223483706033356

Epoch: 6| Step: 10
Training loss: 2.868823766708374
Validation loss: 2.5188058858276694

Epoch: 6| Step: 11
Training loss: 2.5650064945220947
Validation loss: 2.512859031718264

Epoch: 6| Step: 12
Training loss: 1.9931640625
Validation loss: 2.5101567288880706

Epoch: 6| Step: 13
Training loss: 2.450016736984253
Validation loss: 2.505273198568693

Epoch: 85| Step: 0
Training loss: 3.143826484680176
Validation loss: 2.509793468700942

Epoch: 6| Step: 1
Training loss: 1.9947006702423096
Validation loss: 2.5069398316003944

Epoch: 6| Step: 2
Training loss: 2.457793712615967
Validation loss: 2.506402564305131

Epoch: 6| Step: 3
Training loss: 2.4285898208618164
Validation loss: 2.5124813766889673

Epoch: 6| Step: 4
Training loss: 2.5423507690429688
Validation loss: 2.508732185568861

Epoch: 6| Step: 5
Training loss: 3.3782951831817627
Validation loss: 2.5092013856416107

Epoch: 6| Step: 6
Training loss: 3.390141487121582
Validation loss: 2.507071684765559

Epoch: 6| Step: 7
Training loss: 2.4894609451293945
Validation loss: 2.5055605109019945

Epoch: 6| Step: 8
Training loss: 2.3537285327911377
Validation loss: 2.500873678474016

Epoch: 6| Step: 9
Training loss: 2.8057467937469482
Validation loss: 2.5047363799105407

Epoch: 6| Step: 10
Training loss: 2.4468064308166504
Validation loss: 2.506850045214417

Epoch: 6| Step: 11
Training loss: 3.0945699214935303
Validation loss: 2.505164210514356

Epoch: 6| Step: 12
Training loss: 2.7364983558654785
Validation loss: 2.5070197633517686

Epoch: 6| Step: 13
Training loss: 2.538076162338257
Validation loss: 2.5062912997379097

Epoch: 86| Step: 0
Training loss: 2.74068546295166
Validation loss: 2.503480157544536

Epoch: 6| Step: 1
Training loss: 2.7782444953918457
Validation loss: 2.4995278799405662

Epoch: 6| Step: 2
Training loss: 2.6387274265289307
Validation loss: 2.49847597716957

Epoch: 6| Step: 3
Training loss: 2.9070444107055664
Validation loss: 2.499657033592142

Epoch: 6| Step: 4
Training loss: 2.4387199878692627
Validation loss: 2.4961062682572233

Epoch: 6| Step: 5
Training loss: 1.842347502708435
Validation loss: 2.497814614285705

Epoch: 6| Step: 6
Training loss: 2.472485303878784
Validation loss: 2.498073829117642

Epoch: 6| Step: 7
Training loss: 2.9284744262695312
Validation loss: 2.4980280450595322

Epoch: 6| Step: 8
Training loss: 3.920062303543091
Validation loss: 2.5017006397247314

Epoch: 6| Step: 9
Training loss: 2.9861769676208496
Validation loss: 2.507702504434893

Epoch: 6| Step: 10
Training loss: 2.6508023738861084
Validation loss: 2.508206529002036

Epoch: 6| Step: 11
Training loss: 2.610759973526001
Validation loss: 2.512344152696671

Epoch: 6| Step: 12
Training loss: 2.514042854309082
Validation loss: 2.514928020456786

Epoch: 6| Step: 13
Training loss: 2.0926363468170166
Validation loss: 2.5117573020278767

Epoch: 87| Step: 0
Training loss: 2.694540023803711
Validation loss: 2.5064440029923634

Epoch: 6| Step: 1
Training loss: 2.2016139030456543
Validation loss: 2.5033503834919264

Epoch: 6| Step: 2
Training loss: 2.5357677936553955
Validation loss: 2.5062512402893393

Epoch: 6| Step: 3
Training loss: 3.0630040168762207
Validation loss: 2.5015943383657806

Epoch: 6| Step: 4
Training loss: 2.8349599838256836
Validation loss: 2.4999493911702144

Epoch: 6| Step: 5
Training loss: 2.9612207412719727
Validation loss: 2.4954462871756604

Epoch: 6| Step: 6
Training loss: 3.488826274871826
Validation loss: 2.4921815318446003

Epoch: 6| Step: 7
Training loss: 1.9308631420135498
Validation loss: 2.492799643547304

Epoch: 6| Step: 8
Training loss: 2.94157075881958
Validation loss: 2.496311474871892

Epoch: 6| Step: 9
Training loss: 2.5100817680358887
Validation loss: 2.4950531272478003

Epoch: 6| Step: 10
Training loss: 2.9409894943237305
Validation loss: 2.4939133018575688

Epoch: 6| Step: 11
Training loss: 2.395160675048828
Validation loss: 2.4921554724375405

Epoch: 6| Step: 12
Training loss: 2.347405433654785
Validation loss: 2.4947372764669438

Epoch: 6| Step: 13
Training loss: 3.05647349357605
Validation loss: 2.4942640437874743

Epoch: 88| Step: 0
Training loss: 3.03422212600708
Validation loss: 2.495640798281598

Epoch: 6| Step: 1
Training loss: 2.680053472518921
Validation loss: 2.501620646445982

Epoch: 6| Step: 2
Training loss: 1.708978533744812
Validation loss: 2.503328502819102

Epoch: 6| Step: 3
Training loss: 2.5908899307250977
Validation loss: 2.510099813502322

Epoch: 6| Step: 4
Training loss: 2.9922399520874023
Validation loss: 2.5108508884265857

Epoch: 6| Step: 5
Training loss: 2.1713309288024902
Validation loss: 2.507216489443215

Epoch: 6| Step: 6
Training loss: 2.7411627769470215
Validation loss: 2.5044613576704458

Epoch: 6| Step: 7
Training loss: 2.4657723903656006
Validation loss: 2.5025578468076644

Epoch: 6| Step: 8
Training loss: 3.158158779144287
Validation loss: 2.5075940009086364

Epoch: 6| Step: 9
Training loss: 2.5662145614624023
Validation loss: 2.4986814811665523

Epoch: 6| Step: 10
Training loss: 2.850186347961426
Validation loss: 2.4972881219720326

Epoch: 6| Step: 11
Training loss: 2.6431164741516113
Validation loss: 2.495587859102475

Epoch: 6| Step: 12
Training loss: 3.537780523300171
Validation loss: 2.4943067412222586

Epoch: 6| Step: 13
Training loss: 2.3929150104522705
Validation loss: 2.496868864182503

Epoch: 89| Step: 0
Training loss: 1.9123871326446533
Validation loss: 2.4919909482361167

Epoch: 6| Step: 1
Training loss: 2.6751456260681152
Validation loss: 2.490202226946431

Epoch: 6| Step: 2
Training loss: 3.631451368331909
Validation loss: 2.489974888422156

Epoch: 6| Step: 3
Training loss: 3.0623221397399902
Validation loss: 2.493688596192227

Epoch: 6| Step: 4
Training loss: 2.477102518081665
Validation loss: 2.4953825345603367

Epoch: 6| Step: 5
Training loss: 2.092991352081299
Validation loss: 2.4970299992510068

Epoch: 6| Step: 6
Training loss: 3.5099542140960693
Validation loss: 2.5052023472324496

Epoch: 6| Step: 7
Training loss: 3.0328855514526367
Validation loss: 2.503290381482852

Epoch: 6| Step: 8
Training loss: 2.438016414642334
Validation loss: 2.524572010963194

Epoch: 6| Step: 9
Training loss: 2.4551949501037598
Validation loss: 2.5203423858970724

Epoch: 6| Step: 10
Training loss: 2.6552629470825195
Validation loss: 2.531583721919726

Epoch: 6| Step: 11
Training loss: 2.1190593242645264
Validation loss: 2.5346795692238757

Epoch: 6| Step: 12
Training loss: 3.075521945953369
Validation loss: 2.5341313654376614

Epoch: 6| Step: 13
Training loss: 2.776616096496582
Validation loss: 2.512377095478837

Epoch: 90| Step: 0
Training loss: 2.4457075595855713
Validation loss: 2.4933096849790184

Epoch: 6| Step: 1
Training loss: 2.7350287437438965
Validation loss: 2.4888974620449926

Epoch: 6| Step: 2
Training loss: 2.9203221797943115
Validation loss: 2.488548637718283

Epoch: 6| Step: 3
Training loss: 2.612429618835449
Validation loss: 2.491781298832227

Epoch: 6| Step: 4
Training loss: 2.612320899963379
Validation loss: 2.490703187963014

Epoch: 6| Step: 5
Training loss: 2.308380126953125
Validation loss: 2.487456698571482

Epoch: 6| Step: 6
Training loss: 2.9302639961242676
Validation loss: 2.4864692816170315

Epoch: 6| Step: 7
Training loss: 3.2526307106018066
Validation loss: 2.4888714052015737

Epoch: 6| Step: 8
Training loss: 2.9096617698669434
Validation loss: 2.495850832231583

Epoch: 6| Step: 9
Training loss: 2.468557357788086
Validation loss: 2.5089888675238496

Epoch: 6| Step: 10
Training loss: 2.5428943634033203
Validation loss: 2.504326443518362

Epoch: 6| Step: 11
Training loss: 2.524801254272461
Validation loss: 2.4938742114651586

Epoch: 6| Step: 12
Training loss: 2.2923316955566406
Validation loss: 2.491192751033332

Epoch: 6| Step: 13
Training loss: 3.5446460247039795
Validation loss: 2.4857311556416173

Epoch: 91| Step: 0
Training loss: 2.6173477172851562
Validation loss: 2.4814613224357687

Epoch: 6| Step: 1
Training loss: 2.8831281661987305
Validation loss: 2.4814874946430163

Epoch: 6| Step: 2
Training loss: 3.3859219551086426
Validation loss: 2.485591826900359

Epoch: 6| Step: 3
Training loss: 2.646376609802246
Validation loss: 2.4815816289635113

Epoch: 6| Step: 4
Training loss: 2.234013319015503
Validation loss: 2.480549443152643

Epoch: 6| Step: 5
Training loss: 3.074422597885132
Validation loss: 2.4803824834926154

Epoch: 6| Step: 6
Training loss: 2.046511173248291
Validation loss: 2.481628325677687

Epoch: 6| Step: 7
Training loss: 3.195693016052246
Validation loss: 2.4807832369240383

Epoch: 6| Step: 8
Training loss: 2.4444618225097656
Validation loss: 2.479685624440511

Epoch: 6| Step: 9
Training loss: 2.8867669105529785
Validation loss: 2.4838921998136785

Epoch: 6| Step: 10
Training loss: 2.644014596939087
Validation loss: 2.485804965419154

Epoch: 6| Step: 11
Training loss: 2.943190574645996
Validation loss: 2.4809194316146193

Epoch: 6| Step: 12
Training loss: 2.115011215209961
Validation loss: 2.487606417748236

Epoch: 6| Step: 13
Training loss: 2.402663469314575
Validation loss: 2.4829680483828307

Epoch: 92| Step: 0
Training loss: 2.530224084854126
Validation loss: 2.479439971267536

Epoch: 6| Step: 1
Training loss: 2.0242180824279785
Validation loss: 2.473724124252155

Epoch: 6| Step: 2
Training loss: 2.3695297241210938
Validation loss: 2.4796414554760022

Epoch: 6| Step: 3
Training loss: 3.0633630752563477
Validation loss: 2.4831273760846866

Epoch: 6| Step: 4
Training loss: 3.1726465225219727
Validation loss: 2.479130337315221

Epoch: 6| Step: 5
Training loss: 3.157635450363159
Validation loss: 2.480921747863934

Epoch: 6| Step: 6
Training loss: 2.511958360671997
Validation loss: 2.480249253652429

Epoch: 6| Step: 7
Training loss: 3.6170406341552734
Validation loss: 2.4805059176619335

Epoch: 6| Step: 8
Training loss: 2.9121413230895996
Validation loss: 2.473937096134309

Epoch: 6| Step: 9
Training loss: 2.467801332473755
Validation loss: 2.4777795268643286

Epoch: 6| Step: 10
Training loss: 1.943878412246704
Validation loss: 2.4794607521385275

Epoch: 6| Step: 11
Training loss: 2.9160237312316895
Validation loss: 2.4843544639566892

Epoch: 6| Step: 12
Training loss: 2.720458507537842
Validation loss: 2.4787803849866314

Epoch: 6| Step: 13
Training loss: 1.8582547903060913
Validation loss: 2.488339147260112

Epoch: 93| Step: 0
Training loss: 2.881957530975342
Validation loss: 2.484812080219228

Epoch: 6| Step: 1
Training loss: 2.4965522289276123
Validation loss: 2.48447988622932

Epoch: 6| Step: 2
Training loss: 3.164001703262329
Validation loss: 2.4851892558477258

Epoch: 6| Step: 3
Training loss: 2.0118093490600586
Validation loss: 2.490546698211342

Epoch: 6| Step: 4
Training loss: 2.617678642272949
Validation loss: 2.492499807829498

Epoch: 6| Step: 5
Training loss: 2.6268961429595947
Validation loss: 2.490502175464425

Epoch: 6| Step: 6
Training loss: 3.2563939094543457
Validation loss: 2.476328903628934

Epoch: 6| Step: 7
Training loss: 2.490372657775879
Validation loss: 2.474842151006063

Epoch: 6| Step: 8
Training loss: 2.618685245513916
Validation loss: 2.47756128926431

Epoch: 6| Step: 9
Training loss: 3.0706472396850586
Validation loss: 2.4785220571743545

Epoch: 6| Step: 10
Training loss: 3.081355094909668
Validation loss: 2.4758036521173294

Epoch: 6| Step: 11
Training loss: 2.0896987915039062
Validation loss: 2.4827695790157525

Epoch: 6| Step: 12
Training loss: 2.1563897132873535
Validation loss: 2.485182144308603

Epoch: 6| Step: 13
Training loss: 3.424049139022827
Validation loss: 2.4851070527107484

Epoch: 94| Step: 0
Training loss: 2.530797004699707
Validation loss: 2.4896687179483394

Epoch: 6| Step: 1
Training loss: 2.557053565979004
Validation loss: 2.490366828057074

Epoch: 6| Step: 2
Training loss: 3.2777528762817383
Validation loss: 2.489557807163526

Epoch: 6| Step: 3
Training loss: 3.237072229385376
Validation loss: 2.487875871760871

Epoch: 6| Step: 4
Training loss: 1.8776671886444092
Validation loss: 2.4841114218517015

Epoch: 6| Step: 5
Training loss: 3.0929298400878906
Validation loss: 2.4771175615249144

Epoch: 6| Step: 6
Training loss: 2.390300989151001
Validation loss: 2.4796816892521356

Epoch: 6| Step: 7
Training loss: 2.594372510910034
Validation loss: 2.4782117130935832

Epoch: 6| Step: 8
Training loss: 2.6674509048461914
Validation loss: 2.4752535204733572

Epoch: 6| Step: 9
Training loss: 3.005720615386963
Validation loss: 2.4780113517597155

Epoch: 6| Step: 10
Training loss: 2.9366111755371094
Validation loss: 2.48578413327535

Epoch: 6| Step: 11
Training loss: 3.361771821975708
Validation loss: 2.4877609642603065

Epoch: 6| Step: 12
Training loss: 2.0921337604522705
Validation loss: 2.491367881016065

Epoch: 6| Step: 13
Training loss: 1.35758638381958
Validation loss: 2.4875302981304865

Epoch: 95| Step: 0
Training loss: 2.8510689735412598
Validation loss: 2.482995484464912

Epoch: 6| Step: 1
Training loss: 2.80545973777771
Validation loss: 2.487497651448814

Epoch: 6| Step: 2
Training loss: 3.09977126121521
Validation loss: 2.489282000449396

Epoch: 6| Step: 3
Training loss: 3.1099963188171387
Validation loss: 2.484327754666728

Epoch: 6| Step: 4
Training loss: 2.295835256576538
Validation loss: 2.4948306878407798

Epoch: 6| Step: 5
Training loss: 2.9322597980499268
Validation loss: 2.506296511619322

Epoch: 6| Step: 6
Training loss: 2.562972068786621
Validation loss: 2.510593903962002

Epoch: 6| Step: 7
Training loss: 2.543579578399658
Validation loss: 2.5004706536569903

Epoch: 6| Step: 8
Training loss: 2.6539206504821777
Validation loss: 2.491658497882146

Epoch: 6| Step: 9
Training loss: 2.2717928886413574
Validation loss: 2.4850135721186155

Epoch: 6| Step: 10
Training loss: 2.3703012466430664
Validation loss: 2.4852282975309636

Epoch: 6| Step: 11
Training loss: 2.7213077545166016
Validation loss: 2.491340496206796

Epoch: 6| Step: 12
Training loss: 2.7402100563049316
Validation loss: 2.4933084082859818

Epoch: 6| Step: 13
Training loss: 2.6172780990600586
Validation loss: 2.4869061054721957

Epoch: 96| Step: 0
Training loss: 2.1633753776550293
Validation loss: 2.4873010394393757

Epoch: 6| Step: 1
Training loss: 2.552828788757324
Validation loss: 2.4842594105710267

Epoch: 6| Step: 2
Training loss: 3.632589340209961
Validation loss: 2.481244517910865

Epoch: 6| Step: 3
Training loss: 2.0573692321777344
Validation loss: 2.475184635449481

Epoch: 6| Step: 4
Training loss: 3.3070640563964844
Validation loss: 2.47800693204326

Epoch: 6| Step: 5
Training loss: 2.230062961578369
Validation loss: 2.479450994922269

Epoch: 6| Step: 6
Training loss: 2.273723840713501
Validation loss: 2.4746611374680714

Epoch: 6| Step: 7
Training loss: 2.8933873176574707
Validation loss: 2.475665097595543

Epoch: 6| Step: 8
Training loss: 2.1563353538513184
Validation loss: 2.4785468629611436

Epoch: 6| Step: 9
Training loss: 3.0923566818237305
Validation loss: 2.4733942452297417

Epoch: 6| Step: 10
Training loss: 2.9866440296173096
Validation loss: 2.479841260499852

Epoch: 6| Step: 11
Training loss: 3.4927730560302734
Validation loss: 2.4799398760641775

Epoch: 6| Step: 12
Training loss: 1.8495724201202393
Validation loss: 2.481104907169137

Epoch: 6| Step: 13
Training loss: 2.9179224967956543
Validation loss: 2.47855330282642

Epoch: 97| Step: 0
Training loss: 2.7637863159179688
Validation loss: 2.4842502583739576

Epoch: 6| Step: 1
Training loss: 2.5391573905944824
Validation loss: 2.483498142611596

Epoch: 6| Step: 2
Training loss: 3.104313850402832
Validation loss: 2.488893516602055

Epoch: 6| Step: 3
Training loss: 3.078930377960205
Validation loss: 2.491316021129649

Epoch: 6| Step: 4
Training loss: 2.661964178085327
Validation loss: 2.4874963273284254

Epoch: 6| Step: 5
Training loss: 2.386979818344116
Validation loss: 2.491061261905137

Epoch: 6| Step: 6
Training loss: 2.6647000312805176
Validation loss: 2.485428023081954

Epoch: 6| Step: 7
Training loss: 3.009675979614258
Validation loss: 2.4881850519487934

Epoch: 6| Step: 8
Training loss: 2.4890615940093994
Validation loss: 2.4761333286121325

Epoch: 6| Step: 9
Training loss: 2.6027679443359375
Validation loss: 2.4805547575796805

Epoch: 6| Step: 10
Training loss: 2.5943450927734375
Validation loss: 2.481642782047231

Epoch: 6| Step: 11
Training loss: 1.911191701889038
Validation loss: 2.4819520032534035

Epoch: 6| Step: 12
Training loss: 2.9958043098449707
Validation loss: 2.4731363609272945

Epoch: 6| Step: 13
Training loss: 2.8296995162963867
Validation loss: 2.467078470414685

Epoch: 98| Step: 0
Training loss: 2.3455612659454346
Validation loss: 2.466360302381618

Epoch: 6| Step: 1
Training loss: 1.9742155075073242
Validation loss: 2.4654127551663305

Epoch: 6| Step: 2
Training loss: 2.1761322021484375
Validation loss: 2.470711910596458

Epoch: 6| Step: 3
Training loss: 3.028146743774414
Validation loss: 2.469508244145301

Epoch: 6| Step: 4
Training loss: 2.8754796981811523
Validation loss: 2.4701871641220583

Epoch: 6| Step: 5
Training loss: 3.197007417678833
Validation loss: 2.4823375286594516

Epoch: 6| Step: 6
Training loss: 2.5040557384490967
Validation loss: 2.4984776896815144

Epoch: 6| Step: 7
Training loss: 2.060903549194336
Validation loss: 2.5245475230678434

Epoch: 6| Step: 8
Training loss: 3.1392133235931396
Validation loss: 2.527554714551536

Epoch: 6| Step: 9
Training loss: 3.02590274810791
Validation loss: 2.492898751330632

Epoch: 6| Step: 10
Training loss: 3.244751453399658
Validation loss: 2.4696125061281267

Epoch: 6| Step: 11
Training loss: 2.839704990386963
Validation loss: 2.4656927508692585

Epoch: 6| Step: 12
Training loss: 2.0752835273742676
Validation loss: 2.4703253007704213

Epoch: 6| Step: 13
Training loss: 3.6935207843780518
Validation loss: 2.4846886332317064

Epoch: 99| Step: 0
Training loss: 3.073669672012329
Validation loss: 2.484757302909769

Epoch: 6| Step: 1
Training loss: 2.543088436126709
Validation loss: 2.485847770526845

Epoch: 6| Step: 2
Training loss: 2.069638967514038
Validation loss: 2.4767265422369844

Epoch: 6| Step: 3
Training loss: 2.151710033416748
Validation loss: 2.4798717498779297

Epoch: 6| Step: 4
Training loss: 2.4542956352233887
Validation loss: 2.4728522582720687

Epoch: 6| Step: 5
Training loss: 3.010591983795166
Validation loss: 2.4724265080626293

Epoch: 6| Step: 6
Training loss: 3.021012783050537
Validation loss: 2.47648649830972

Epoch: 6| Step: 7
Training loss: 3.788557291030884
Validation loss: 2.4717126841186197

Epoch: 6| Step: 8
Training loss: 2.6243176460266113
Validation loss: 2.476131287954187

Epoch: 6| Step: 9
Training loss: 2.513859272003174
Validation loss: 2.478344307150892

Epoch: 6| Step: 10
Training loss: 2.528336524963379
Validation loss: 2.4828208569557435

Epoch: 6| Step: 11
Training loss: 2.9351601600646973
Validation loss: 2.49105514249494

Epoch: 6| Step: 12
Training loss: 2.8629350662231445
Validation loss: 2.4966992024452455

Epoch: 6| Step: 13
Training loss: 1.4907457828521729
Validation loss: 2.496937419778557

Epoch: 100| Step: 0
Training loss: 2.5569653511047363
Validation loss: 2.488229887459868

Epoch: 6| Step: 1
Training loss: 3.1327788829803467
Validation loss: 2.487004813327584

Epoch: 6| Step: 2
Training loss: 3.1099538803100586
Validation loss: 2.4798437805585962

Epoch: 6| Step: 3
Training loss: 2.2464399337768555
Validation loss: 2.4743147742363716

Epoch: 6| Step: 4
Training loss: 1.857831597328186
Validation loss: 2.4704134669355167

Epoch: 6| Step: 5
Training loss: 1.953838586807251
Validation loss: 2.4681788285573325

Epoch: 6| Step: 6
Training loss: 3.06358003616333
Validation loss: 2.4652546605756207

Epoch: 6| Step: 7
Training loss: 2.722189426422119
Validation loss: 2.461378848680886

Epoch: 6| Step: 8
Training loss: 2.9045462608337402
Validation loss: 2.458616038804413

Epoch: 6| Step: 9
Training loss: 2.4379830360412598
Validation loss: 2.4659189485734507

Epoch: 6| Step: 10
Training loss: 3.1083812713623047
Validation loss: 2.4682741011342695

Epoch: 6| Step: 11
Training loss: 2.578287363052368
Validation loss: 2.471450000680903

Epoch: 6| Step: 12
Training loss: 3.102564811706543
Validation loss: 2.469490057678633

Epoch: 6| Step: 13
Training loss: 2.789238929748535
Validation loss: 2.4732241758736233

Epoch: 101| Step: 0
Training loss: 2.3913726806640625
Validation loss: 2.4735911738487983

Epoch: 6| Step: 1
Training loss: 2.5698022842407227
Validation loss: 2.4719513385526595

Epoch: 6| Step: 2
Training loss: 3.6760852336883545
Validation loss: 2.470186712921307

Epoch: 6| Step: 3
Training loss: 1.8481374979019165
Validation loss: 2.465654447514524

Epoch: 6| Step: 4
Training loss: 3.0287208557128906
Validation loss: 2.460152518364691

Epoch: 6| Step: 5
Training loss: 2.9330618381500244
Validation loss: 2.4632644345683437

Epoch: 6| Step: 6
Training loss: 2.3599143028259277
Validation loss: 2.4665941961349978

Epoch: 6| Step: 7
Training loss: 3.5158486366271973
Validation loss: 2.492827792321482

Epoch: 6| Step: 8
Training loss: 2.0320804119110107
Validation loss: 2.5208831987073346

Epoch: 6| Step: 9
Training loss: 2.861717462539673
Validation loss: 2.578792183629928

Epoch: 6| Step: 10
Training loss: 2.9235341548919678
Validation loss: 2.6158964762123684

Epoch: 6| Step: 11
Training loss: 2.1483607292175293
Validation loss: 2.660613311234341

Epoch: 6| Step: 12
Training loss: 2.830167055130005
Validation loss: 2.6556540202069026

Epoch: 6| Step: 13
Training loss: 2.8187472820281982
Validation loss: 2.6411166473101546

Epoch: 102| Step: 0
Training loss: 2.96022629737854
Validation loss: 2.5439636092032156

Epoch: 6| Step: 1
Training loss: 2.837351083755493
Validation loss: 2.510426426446566

Epoch: 6| Step: 2
Training loss: 3.26029634475708
Validation loss: 2.494564390951587

Epoch: 6| Step: 3
Training loss: 2.7283778190612793
Validation loss: 2.486550074751659

Epoch: 6| Step: 4
Training loss: 2.6692557334899902
Validation loss: 2.480419848554878

Epoch: 6| Step: 5
Training loss: 2.6987524032592773
Validation loss: 2.481805282254373

Epoch: 6| Step: 6
Training loss: 3.0921177864074707
Validation loss: 2.4791100127722627

Epoch: 6| Step: 7
Training loss: 2.946807622909546
Validation loss: 2.4735613638354885

Epoch: 6| Step: 8
Training loss: 1.8746694326400757
Validation loss: 2.471414630131055

Epoch: 6| Step: 9
Training loss: 2.26008939743042
Validation loss: 2.4722361897909515

Epoch: 6| Step: 10
Training loss: 3.1249263286590576
Validation loss: 2.467717816752772

Epoch: 6| Step: 11
Training loss: 1.9612703323364258
Validation loss: 2.4670039863996607

Epoch: 6| Step: 12
Training loss: 2.6235296726226807
Validation loss: 2.465798624100224

Epoch: 6| Step: 13
Training loss: 2.432100772857666
Validation loss: 2.468152084658223

Epoch: 103| Step: 0
Training loss: 3.332414150238037
Validation loss: 2.466486930847168

Epoch: 6| Step: 1
Training loss: 2.4999985694885254
Validation loss: 2.483121100292411

Epoch: 6| Step: 2
Training loss: 2.8075942993164062
Validation loss: 2.496334878347253

Epoch: 6| Step: 3
Training loss: 2.866903066635132
Validation loss: 2.508780041048604

Epoch: 6| Step: 4
Training loss: 2.9374730587005615
Validation loss: 2.512554778847643

Epoch: 6| Step: 5
Training loss: 2.350961208343506
Validation loss: 2.508547167624197

Epoch: 6| Step: 6
Training loss: 2.150397300720215
Validation loss: 2.488581249790807

Epoch: 6| Step: 7
Training loss: 3.117575168609619
Validation loss: 2.4738566926730576

Epoch: 6| Step: 8
Training loss: 2.502067804336548
Validation loss: 2.4619457106436453

Epoch: 6| Step: 9
Training loss: 2.420943021774292
Validation loss: 2.4548318232259443

Epoch: 6| Step: 10
Training loss: 2.1321861743927
Validation loss: 2.460501365764167

Epoch: 6| Step: 11
Training loss: 2.876725673675537
Validation loss: 2.454677084440826

Epoch: 6| Step: 12
Training loss: 2.531877040863037
Validation loss: 2.4497518206155426

Epoch: 6| Step: 13
Training loss: 2.88797664642334
Validation loss: 2.4467291626878964

Epoch: 104| Step: 0
Training loss: 2.9130430221557617
Validation loss: 2.455750455138504

Epoch: 6| Step: 1
Training loss: 2.883169174194336
Validation loss: 2.4514898536025838

Epoch: 6| Step: 2
Training loss: 2.488137722015381
Validation loss: 2.45847842513874

Epoch: 6| Step: 3
Training loss: 2.974526882171631
Validation loss: 2.4647141425840315

Epoch: 6| Step: 4
Training loss: 2.0648555755615234
Validation loss: 2.4666497066456783

Epoch: 6| Step: 5
Training loss: 2.6806366443634033
Validation loss: 2.4680572863548034

Epoch: 6| Step: 6
Training loss: 2.999979019165039
Validation loss: 2.4610384587318666

Epoch: 6| Step: 7
Training loss: 2.1877458095550537
Validation loss: 2.457384617097916

Epoch: 6| Step: 8
Training loss: 2.514971971511841
Validation loss: 2.4486780961354575

Epoch: 6| Step: 9
Training loss: 2.403963327407837
Validation loss: 2.4433795636700046

Epoch: 6| Step: 10
Training loss: 2.7956953048706055
Validation loss: 2.4435985677985737

Epoch: 6| Step: 11
Training loss: 3.709226608276367
Validation loss: 2.440713620954944

Epoch: 6| Step: 12
Training loss: 2.534810781478882
Validation loss: 2.4423554969090286

Epoch: 6| Step: 13
Training loss: 1.8854429721832275
Validation loss: 2.447134333272134

Epoch: 105| Step: 0
Training loss: 2.197781801223755
Validation loss: 2.46049222125802

Epoch: 6| Step: 1
Training loss: 3.0976853370666504
Validation loss: 2.4643464908804944

Epoch: 6| Step: 2
Training loss: 2.891941547393799
Validation loss: 2.46640524812924

Epoch: 6| Step: 3
Training loss: 2.9334750175476074
Validation loss: 2.4697432415459746

Epoch: 6| Step: 4
Training loss: 2.7224411964416504
Validation loss: 2.468344059041751

Epoch: 6| Step: 5
Training loss: 2.9315743446350098
Validation loss: 2.476383275883172

Epoch: 6| Step: 6
Training loss: 3.0316615104675293
Validation loss: 2.477057110878729

Epoch: 6| Step: 7
Training loss: 2.190464496612549
Validation loss: 2.4937436452475925

Epoch: 6| Step: 8
Training loss: 2.336510181427002
Validation loss: 2.491731671876805

Epoch: 6| Step: 9
Training loss: 2.8939781188964844
Validation loss: 2.507738621004166

Epoch: 6| Step: 10
Training loss: 2.0580949783325195
Validation loss: 2.501007333878548

Epoch: 6| Step: 11
Training loss: 2.299527645111084
Validation loss: 2.5086726168150544

Epoch: 6| Step: 12
Training loss: 2.606499671936035
Validation loss: 2.4791255663799983

Epoch: 6| Step: 13
Training loss: 3.8629255294799805
Validation loss: 2.4585024259423696

Epoch: 106| Step: 0
Training loss: 2.2907297611236572
Validation loss: 2.445803347454276

Epoch: 6| Step: 1
Training loss: 2.6316258907318115
Validation loss: 2.445277774205772

Epoch: 6| Step: 2
Training loss: 2.809896230697632
Validation loss: 2.43851795247806

Epoch: 6| Step: 3
Training loss: 2.9522297382354736
Validation loss: 2.442573226908202

Epoch: 6| Step: 4
Training loss: 2.4239492416381836
Validation loss: 2.442725694307717

Epoch: 6| Step: 5
Training loss: 2.578768491744995
Validation loss: 2.445429471231276

Epoch: 6| Step: 6
Training loss: 2.8898332118988037
Validation loss: 2.446877492371426

Epoch: 6| Step: 7
Training loss: 2.7877931594848633
Validation loss: 2.4487916833610943

Epoch: 6| Step: 8
Training loss: 2.571604013442993
Validation loss: 2.4647094331761843

Epoch: 6| Step: 9
Training loss: 3.414518356323242
Validation loss: 2.4835262554948048

Epoch: 6| Step: 10
Training loss: 2.5382473468780518
Validation loss: 2.501543406517275

Epoch: 6| Step: 11
Training loss: 2.446754217147827
Validation loss: 2.485826035981537

Epoch: 6| Step: 12
Training loss: 2.612308979034424
Validation loss: 2.477147468956568

Epoch: 6| Step: 13
Training loss: 2.3599917888641357
Validation loss: 2.448686768931727

Epoch: 107| Step: 0
Training loss: 3.0546507835388184
Validation loss: 2.4384838637485298

Epoch: 6| Step: 1
Training loss: 3.1747303009033203
Validation loss: 2.4338642140870452

Epoch: 6| Step: 2
Training loss: 2.9024734497070312
Validation loss: 2.435126293090082

Epoch: 6| Step: 3
Training loss: 2.4935033321380615
Validation loss: 2.438054620578725

Epoch: 6| Step: 4
Training loss: 2.5400991439819336
Validation loss: 2.439968316785751

Epoch: 6| Step: 5
Training loss: 2.7796096801757812
Validation loss: 2.43768189543037

Epoch: 6| Step: 6
Training loss: 2.249023199081421
Validation loss: 2.4393819006540443

Epoch: 6| Step: 7
Training loss: 2.345064401626587
Validation loss: 2.4447498372806016

Epoch: 6| Step: 8
Training loss: 2.4202985763549805
Validation loss: 2.451535035205144

Epoch: 6| Step: 9
Training loss: 2.611513376235962
Validation loss: 2.4518879716114332

Epoch: 6| Step: 10
Training loss: 2.6578445434570312
Validation loss: 2.45000672084029

Epoch: 6| Step: 11
Training loss: 2.8564586639404297
Validation loss: 2.4599821900808685

Epoch: 6| Step: 12
Training loss: 2.565643787384033
Validation loss: 2.4689850012461343

Epoch: 6| Step: 13
Training loss: 2.804231643676758
Validation loss: 2.471132178460398

Epoch: 108| Step: 0
Training loss: 2.1396241188049316
Validation loss: 2.473418399851809

Epoch: 6| Step: 1
Training loss: 3.6750385761260986
Validation loss: 2.4591833494042836

Epoch: 6| Step: 2
Training loss: 2.5602264404296875
Validation loss: 2.452811041185933

Epoch: 6| Step: 3
Training loss: 2.7176547050476074
Validation loss: 2.446302916413994

Epoch: 6| Step: 4
Training loss: 2.714456558227539
Validation loss: 2.453098822665471

Epoch: 6| Step: 5
Training loss: 2.7527718544006348
Validation loss: 2.454565813464503

Epoch: 6| Step: 6
Training loss: 2.9590189456939697
Validation loss: 2.4633525776606735

Epoch: 6| Step: 7
Training loss: 2.677232265472412
Validation loss: 2.4716906060454664

Epoch: 6| Step: 8
Training loss: 2.9140167236328125
Validation loss: 2.487335738315377

Epoch: 6| Step: 9
Training loss: 2.7589972019195557
Validation loss: 2.4902011168900358

Epoch: 6| Step: 10
Training loss: 2.157144546508789
Validation loss: 2.4980469147364297

Epoch: 6| Step: 11
Training loss: 2.3131515979766846
Validation loss: 2.4925453919236378

Epoch: 6| Step: 12
Training loss: 2.3240416049957275
Validation loss: 2.4856503701979116

Epoch: 6| Step: 13
Training loss: 2.859464406967163
Validation loss: 2.4867007911846204

Epoch: 109| Step: 0
Training loss: 2.470313310623169
Validation loss: 2.4737616149328088

Epoch: 6| Step: 1
Training loss: 2.363452911376953
Validation loss: 2.4689908668559086

Epoch: 6| Step: 2
Training loss: 2.1822054386138916
Validation loss: 2.451565945020286

Epoch: 6| Step: 3
Training loss: 2.503718376159668
Validation loss: 2.4464817918756956

Epoch: 6| Step: 4
Training loss: 2.7826762199401855
Validation loss: 2.447568611432147

Epoch: 6| Step: 5
Training loss: 2.1254501342773438
Validation loss: 2.447541134331816

Epoch: 6| Step: 6
Training loss: 2.5656630992889404
Validation loss: 2.4325736415001655

Epoch: 6| Step: 7
Training loss: 2.451447010040283
Validation loss: 2.4312410585341917

Epoch: 6| Step: 8
Training loss: 2.8813881874084473
Validation loss: 2.45645736622554

Epoch: 6| Step: 9
Training loss: 2.7550230026245117
Validation loss: 2.456906021282237

Epoch: 6| Step: 10
Training loss: 3.536548137664795
Validation loss: 2.4364934839228147

Epoch: 6| Step: 11
Training loss: 3.0663952827453613
Validation loss: 2.4327512812870804

Epoch: 6| Step: 12
Training loss: 2.9128212928771973
Validation loss: 2.42882844965945

Epoch: 6| Step: 13
Training loss: 2.6881189346313477
Validation loss: 2.4287101991714968

Epoch: 110| Step: 0
Training loss: 3.207932233810425
Validation loss: 2.4279792206261748

Epoch: 6| Step: 1
Training loss: 2.806938886642456
Validation loss: 2.427326535665861

Epoch: 6| Step: 2
Training loss: 2.5134520530700684
Validation loss: 2.433943780519629

Epoch: 6| Step: 3
Training loss: 2.3001608848571777
Validation loss: 2.437629671506984

Epoch: 6| Step: 4
Training loss: 2.622851848602295
Validation loss: 2.445983358608779

Epoch: 6| Step: 5
Training loss: 3.559400796890259
Validation loss: 2.46174148590334

Epoch: 6| Step: 6
Training loss: 2.032477855682373
Validation loss: 2.487891625332576

Epoch: 6| Step: 7
Training loss: 2.481365919113159
Validation loss: 2.499381688333327

Epoch: 6| Step: 8
Training loss: 2.5400164127349854
Validation loss: 2.4732499455892913

Epoch: 6| Step: 9
Training loss: 2.1210734844207764
Validation loss: 2.4667106982200377

Epoch: 6| Step: 10
Training loss: 2.6176445484161377
Validation loss: 2.4640432147569555

Epoch: 6| Step: 11
Training loss: 3.1778650283813477
Validation loss: 2.442986351187511

Epoch: 6| Step: 12
Training loss: 2.2852413654327393
Validation loss: 2.4322991396791194

Epoch: 6| Step: 13
Training loss: 3.218749761581421
Validation loss: 2.4318800869808403

Epoch: 111| Step: 0
Training loss: 3.242882251739502
Validation loss: 2.4287944762937483

Epoch: 6| Step: 1
Training loss: 2.8264753818511963
Validation loss: 2.4268224726441088

Epoch: 6| Step: 2
Training loss: 1.899033546447754
Validation loss: 2.4271826231351463

Epoch: 6| Step: 3
Training loss: 3.913297414779663
Validation loss: 2.4293533704614125

Epoch: 6| Step: 4
Training loss: 2.1656718254089355
Validation loss: 2.427988411277853

Epoch: 6| Step: 5
Training loss: 2.5650854110717773
Validation loss: 2.431328294097736

Epoch: 6| Step: 6
Training loss: 2.048302173614502
Validation loss: 2.4304393773437827

Epoch: 6| Step: 7
Training loss: 2.8717126846313477
Validation loss: 2.4389974404406805

Epoch: 6| Step: 8
Training loss: 2.695606231689453
Validation loss: 2.4311249986771615

Epoch: 6| Step: 9
Training loss: 3.067251682281494
Validation loss: 2.4401221147147556

Epoch: 6| Step: 10
Training loss: 1.9735791683197021
Validation loss: 2.4512238630684475

Epoch: 6| Step: 11
Training loss: 2.1592671871185303
Validation loss: 2.470741679591517

Epoch: 6| Step: 12
Training loss: 3.260153293609619
Validation loss: 2.463203009738717

Epoch: 6| Step: 13
Training loss: 2.3615193367004395
Validation loss: 2.4715105410545104

Epoch: 112| Step: 0
Training loss: 2.320814609527588
Validation loss: 2.4835185645728983

Epoch: 6| Step: 1
Training loss: 2.650542736053467
Validation loss: 2.4822009558318765

Epoch: 6| Step: 2
Training loss: 2.7296228408813477
Validation loss: 2.490907181975662

Epoch: 6| Step: 3
Training loss: 3.008838653564453
Validation loss: 2.484448309867613

Epoch: 6| Step: 4
Training loss: 2.601243495941162
Validation loss: 2.4810227758140972

Epoch: 6| Step: 5
Training loss: 3.1649484634399414
Validation loss: 2.466421752847651

Epoch: 6| Step: 6
Training loss: 2.9184489250183105
Validation loss: 2.4460194367234425

Epoch: 6| Step: 7
Training loss: 2.686302661895752
Validation loss: 2.4312479060183287

Epoch: 6| Step: 8
Training loss: 2.10369873046875
Validation loss: 2.4337679596357447

Epoch: 6| Step: 9
Training loss: 2.608111619949341
Validation loss: 2.422411587930495

Epoch: 6| Step: 10
Training loss: 2.826122760772705
Validation loss: 2.4272164042278

Epoch: 6| Step: 11
Training loss: 3.210346221923828
Validation loss: 2.4318670457409275

Epoch: 6| Step: 12
Training loss: 2.097282648086548
Validation loss: 2.4329280776362263

Epoch: 6| Step: 13
Training loss: 2.0087532997131348
Validation loss: 2.4355436371218775

Epoch: 113| Step: 0
Training loss: 1.8952794075012207
Validation loss: 2.4313203647572506

Epoch: 6| Step: 1
Training loss: 2.4896492958068848
Validation loss: 2.4340463428087133

Epoch: 6| Step: 2
Training loss: 2.9174904823303223
Validation loss: 2.4317254840686755

Epoch: 6| Step: 3
Training loss: 2.274531602859497
Validation loss: 2.4310288506169475

Epoch: 6| Step: 4
Training loss: 2.631371021270752
Validation loss: 2.428110425190259

Epoch: 6| Step: 5
Training loss: 2.3983490467071533
Validation loss: 2.4240697712026615

Epoch: 6| Step: 6
Training loss: 3.516030788421631
Validation loss: 2.429210050131685

Epoch: 6| Step: 7
Training loss: 2.794846534729004
Validation loss: 2.4212039773182203

Epoch: 6| Step: 8
Training loss: 2.466226577758789
Validation loss: 2.4287105888448735

Epoch: 6| Step: 9
Training loss: 3.62324595451355
Validation loss: 2.4302249467501076

Epoch: 6| Step: 10
Training loss: 2.603192090988159
Validation loss: 2.436777924978605

Epoch: 6| Step: 11
Training loss: 2.367501735687256
Validation loss: 2.4345322039819535

Epoch: 6| Step: 12
Training loss: 2.887051582336426
Validation loss: 2.430607395787393

Epoch: 6| Step: 13
Training loss: 1.9197924137115479
Validation loss: 2.430668536052909

Epoch: 114| Step: 0
Training loss: 2.177217483520508
Validation loss: 2.431743347516624

Epoch: 6| Step: 1
Training loss: 2.307760238647461
Validation loss: 2.4300616351507043

Epoch: 6| Step: 2
Training loss: 2.723951816558838
Validation loss: 2.4260088910338697

Epoch: 6| Step: 3
Training loss: 2.487429141998291
Validation loss: 2.4254190075782036

Epoch: 6| Step: 4
Training loss: 2.7568459510803223
Validation loss: 2.430100030796502

Epoch: 6| Step: 5
Training loss: 3.011049747467041
Validation loss: 2.4358152599744898

Epoch: 6| Step: 6
Training loss: 3.461404800415039
Validation loss: 2.448344084524339

Epoch: 6| Step: 7
Training loss: 2.279651641845703
Validation loss: 2.456156876779372

Epoch: 6| Step: 8
Training loss: 2.904090642929077
Validation loss: 2.4809195405693463

Epoch: 6| Step: 9
Training loss: 1.9604854583740234
Validation loss: 2.467668825580228

Epoch: 6| Step: 10
Training loss: 3.6280481815338135
Validation loss: 2.4530196010425525

Epoch: 6| Step: 11
Training loss: 3.0629372596740723
Validation loss: 2.446951391876385

Epoch: 6| Step: 12
Training loss: 1.7843594551086426
Validation loss: 2.4345920060270574

Epoch: 6| Step: 13
Training loss: 2.683922052383423
Validation loss: 2.428409932762064

Epoch: 115| Step: 0
Training loss: 1.8485381603240967
Validation loss: 2.421662389591176

Epoch: 6| Step: 1
Training loss: 2.5569095611572266
Validation loss: 2.4230791855883855

Epoch: 6| Step: 2
Training loss: 2.751068115234375
Validation loss: 2.4245403043685423

Epoch: 6| Step: 3
Training loss: 2.9080593585968018
Validation loss: 2.4223822598816245

Epoch: 6| Step: 4
Training loss: 3.265300750732422
Validation loss: 2.420492131222961

Epoch: 6| Step: 5
Training loss: 3.3449809551239014
Validation loss: 2.418047440949307

Epoch: 6| Step: 6
Training loss: 1.802558422088623
Validation loss: 2.421583826823901

Epoch: 6| Step: 7
Training loss: 2.215826988220215
Validation loss: 2.4211769744914067

Epoch: 6| Step: 8
Training loss: 2.11702823638916
Validation loss: 2.4165966356954267

Epoch: 6| Step: 9
Training loss: 2.777956247329712
Validation loss: 2.4159484140334593

Epoch: 6| Step: 10
Training loss: 2.4958181381225586
Validation loss: 2.4231913320479856

Epoch: 6| Step: 11
Training loss: 2.6617069244384766
Validation loss: 2.4247281192451395

Epoch: 6| Step: 12
Training loss: 3.0910134315490723
Validation loss: 2.4228847437007452

Epoch: 6| Step: 13
Training loss: 3.7151777744293213
Validation loss: 2.4175403323224796

Epoch: 116| Step: 0
Training loss: 2.8087377548217773
Validation loss: 2.418482918893137

Epoch: 6| Step: 1
Training loss: 3.231764078140259
Validation loss: 2.4216906229654946

Epoch: 6| Step: 2
Training loss: 2.949984073638916
Validation loss: 2.4193654316727833

Epoch: 6| Step: 3
Training loss: 3.357645273208618
Validation loss: 2.4191474760732343

Epoch: 6| Step: 4
Training loss: 1.824324369430542
Validation loss: 2.4149673959260345

Epoch: 6| Step: 5
Training loss: 2.1612627506256104
Validation loss: 2.4209506793688704

Epoch: 6| Step: 6
Training loss: 3.0219979286193848
Validation loss: 2.4166234462491927

Epoch: 6| Step: 7
Training loss: 2.131899356842041
Validation loss: 2.423883761129072

Epoch: 6| Step: 8
Training loss: 2.803349494934082
Validation loss: 2.4274848250932592

Epoch: 6| Step: 9
Training loss: 2.5976343154907227
Validation loss: 2.438530304098642

Epoch: 6| Step: 10
Training loss: 2.117133378982544
Validation loss: 2.4427402916774956

Epoch: 6| Step: 11
Training loss: 2.2516250610351562
Validation loss: 2.4564453068599907

Epoch: 6| Step: 12
Training loss: 2.6823482513427734
Validation loss: 2.4735801168667373

Epoch: 6| Step: 13
Training loss: 3.3726813793182373
Validation loss: 2.4792921081666024

Epoch: 117| Step: 0
Training loss: 2.4179131984710693
Validation loss: 2.484675042090877

Epoch: 6| Step: 1
Training loss: 3.0034501552581787
Validation loss: 2.500467877234182

Epoch: 6| Step: 2
Training loss: 3.0870680809020996
Validation loss: 2.4810684778357066

Epoch: 6| Step: 3
Training loss: 2.6477508544921875
Validation loss: 2.4703751661444224

Epoch: 6| Step: 4
Training loss: 2.12373685836792
Validation loss: 2.4504027725547872

Epoch: 6| Step: 5
Training loss: 2.9914257526397705
Validation loss: 2.432608251930565

Epoch: 6| Step: 6
Training loss: 2.6981077194213867
Validation loss: 2.420407126026769

Epoch: 6| Step: 7
Training loss: 2.4333858489990234
Validation loss: 2.4173310264464347

Epoch: 6| Step: 8
Training loss: 2.1855573654174805
Validation loss: 2.413237479425246

Epoch: 6| Step: 9
Training loss: 2.943479061126709
Validation loss: 2.4130033549442085

Epoch: 6| Step: 10
Training loss: 2.320660352706909
Validation loss: 2.4176456479616064

Epoch: 6| Step: 11
Training loss: 2.4894754886627197
Validation loss: 2.4118861575280466

Epoch: 6| Step: 12
Training loss: 2.5213985443115234
Validation loss: 2.41243802860219

Epoch: 6| Step: 13
Training loss: 3.79004168510437
Validation loss: 2.415703122333814

Epoch: 118| Step: 0
Training loss: 2.7909257411956787
Validation loss: 2.4141960605498283

Epoch: 6| Step: 1
Training loss: 3.2123591899871826
Validation loss: 2.4145829908309446

Epoch: 6| Step: 2
Training loss: 2.425459861755371
Validation loss: 2.431728634783017

Epoch: 6| Step: 3
Training loss: 2.7890355587005615
Validation loss: 2.456184833280502

Epoch: 6| Step: 4
Training loss: 2.824648857116699
Validation loss: 2.4728803557734333

Epoch: 6| Step: 5
Training loss: 2.855299472808838
Validation loss: 2.4781288664828063

Epoch: 6| Step: 6
Training loss: 2.6314640045166016
Validation loss: 2.49589620354355

Epoch: 6| Step: 7
Training loss: 2.9855308532714844
Validation loss: 2.5039804571418354

Epoch: 6| Step: 8
Training loss: 2.4487643241882324
Validation loss: 2.4780199861013763

Epoch: 6| Step: 9
Training loss: 1.9131312370300293
Validation loss: 2.444205455882575

Epoch: 6| Step: 10
Training loss: 2.029780864715576
Validation loss: 2.429413375034127

Epoch: 6| Step: 11
Training loss: 2.548130512237549
Validation loss: 2.415517532697288

Epoch: 6| Step: 12
Training loss: 2.6229300498962402
Validation loss: 2.404404781197989

Epoch: 6| Step: 13
Training loss: 3.0640170574188232
Validation loss: 2.4085371699384464

Epoch: 119| Step: 0
Training loss: 2.6669750213623047
Validation loss: 2.4070779097977506

Epoch: 6| Step: 1
Training loss: 2.74314022064209
Validation loss: 2.4019546649789296

Epoch: 6| Step: 2
Training loss: 2.4718194007873535
Validation loss: 2.407805573555731

Epoch: 6| Step: 3
Training loss: 2.32952880859375
Validation loss: 2.4162079057385846

Epoch: 6| Step: 4
Training loss: 3.2428345680236816
Validation loss: 2.429592206913938

Epoch: 6| Step: 5
Training loss: 3.0812888145446777
Validation loss: 2.427699086486652

Epoch: 6| Step: 6
Training loss: 1.5642836093902588
Validation loss: 2.437644589331842

Epoch: 6| Step: 7
Training loss: 2.382620096206665
Validation loss: 2.44064082894274

Epoch: 6| Step: 8
Training loss: 2.5015172958374023
Validation loss: 2.433230456485543

Epoch: 6| Step: 9
Training loss: 2.3615355491638184
Validation loss: 2.422502512572914

Epoch: 6| Step: 10
Training loss: 3.1689364910125732
Validation loss: 2.439162697843326

Epoch: 6| Step: 11
Training loss: 2.6687352657318115
Validation loss: 2.434939738242857

Epoch: 6| Step: 12
Training loss: 3.066821813583374
Validation loss: 2.431560380484468

Epoch: 6| Step: 13
Training loss: 2.5150136947631836
Validation loss: 2.4427241715051795

Epoch: 120| Step: 0
Training loss: 3.0853934288024902
Validation loss: 2.4506933125116492

Epoch: 6| Step: 1
Training loss: 3.2681143283843994
Validation loss: 2.4261516794081657

Epoch: 6| Step: 2
Training loss: 2.3449952602386475
Validation loss: 2.4108669988570677

Epoch: 6| Step: 3
Training loss: 2.296675682067871
Validation loss: 2.4005971672714397

Epoch: 6| Step: 4
Training loss: 2.6801633834838867
Validation loss: 2.4015060906769126

Epoch: 6| Step: 5
Training loss: 2.473281145095825
Validation loss: 2.4077003386712845

Epoch: 6| Step: 6
Training loss: 2.24984073638916
Validation loss: 2.4081687952882502

Epoch: 6| Step: 7
Training loss: 2.8237717151641846
Validation loss: 2.401515345419607

Epoch: 6| Step: 8
Training loss: 2.6489012241363525
Validation loss: 2.4059168420812136

Epoch: 6| Step: 9
Training loss: 1.8045947551727295
Validation loss: 2.4144389475545576

Epoch: 6| Step: 10
Training loss: 3.299957275390625
Validation loss: 2.4180277611619685

Epoch: 6| Step: 11
Training loss: 3.0462522506713867
Validation loss: 2.4211353640402518

Epoch: 6| Step: 12
Training loss: 2.445140838623047
Validation loss: 2.4195709382334063

Epoch: 6| Step: 13
Training loss: 2.1893582344055176
Validation loss: 2.4101139268567486

Epoch: 121| Step: 0
Training loss: 2.6724870204925537
Validation loss: 2.422321250361781

Epoch: 6| Step: 1
Training loss: 3.2602996826171875
Validation loss: 2.4280115250618226

Epoch: 6| Step: 2
Training loss: 2.5125653743743896
Validation loss: 2.4325362995106685

Epoch: 6| Step: 3
Training loss: 2.5305285453796387
Validation loss: 2.425900331107519

Epoch: 6| Step: 4
Training loss: 2.822598457336426
Validation loss: 2.4318578858529367

Epoch: 6| Step: 5
Training loss: 3.244039535522461
Validation loss: 2.4337787935810704

Epoch: 6| Step: 6
Training loss: 2.32085919380188
Validation loss: 2.4240468163644113

Epoch: 6| Step: 7
Training loss: 2.0013489723205566
Validation loss: 2.4205847401772775

Epoch: 6| Step: 8
Training loss: 2.9708125591278076
Validation loss: 2.4275525052060365

Epoch: 6| Step: 9
Training loss: 1.9925236701965332
Validation loss: 2.4249758207669823

Epoch: 6| Step: 10
Training loss: 2.4938721656799316
Validation loss: 2.4354173726932977

Epoch: 6| Step: 11
Training loss: 2.706540584564209
Validation loss: 2.420245114193168

Epoch: 6| Step: 12
Training loss: 2.658287763595581
Validation loss: 2.413176011013728

Epoch: 6| Step: 13
Training loss: 2.941730260848999
Validation loss: 2.3958082135005663

Epoch: 122| Step: 0
Training loss: 2.447779655456543
Validation loss: 2.399467852807814

Epoch: 6| Step: 1
Training loss: 3.0459630489349365
Validation loss: 2.397860698802497

Epoch: 6| Step: 2
Training loss: 2.619100570678711
Validation loss: 2.4051231748314312

Epoch: 6| Step: 3
Training loss: 2.4776368141174316
Validation loss: 2.404500299884427

Epoch: 6| Step: 4
Training loss: 2.9789505004882812
Validation loss: 2.4045175480586227

Epoch: 6| Step: 5
Training loss: 2.9524214267730713
Validation loss: 2.4085188117078555

Epoch: 6| Step: 6
Training loss: 1.9180594682693481
Validation loss: 2.409232683079217

Epoch: 6| Step: 7
Training loss: 3.1531014442443848
Validation loss: 2.4044770348456597

Epoch: 6| Step: 8
Training loss: 2.4573559761047363
Validation loss: 2.4071899870390534

Epoch: 6| Step: 9
Training loss: 3.20987606048584
Validation loss: 2.4042684980618056

Epoch: 6| Step: 10
Training loss: 1.8078360557556152
Validation loss: 2.405057619976741

Epoch: 6| Step: 11
Training loss: 2.7319579124450684
Validation loss: 2.406085596289686

Epoch: 6| Step: 12
Training loss: 2.8686091899871826
Validation loss: 2.399029424113612

Epoch: 6| Step: 13
Training loss: 2.3414227962493896
Validation loss: 2.399658586389275

Epoch: 123| Step: 0
Training loss: 3.1236515045166016
Validation loss: 2.390813127640755

Epoch: 6| Step: 1
Training loss: 2.767916440963745
Validation loss: 2.3948020294148433

Epoch: 6| Step: 2
Training loss: 2.3920416831970215
Validation loss: 2.3996141854152886

Epoch: 6| Step: 3
Training loss: 3.476323366165161
Validation loss: 2.411206526141013

Epoch: 6| Step: 4
Training loss: 2.2349939346313477
Validation loss: 2.4177862674959245

Epoch: 6| Step: 5
Training loss: 1.890352487564087
Validation loss: 2.424032349740305

Epoch: 6| Step: 6
Training loss: 2.606436252593994
Validation loss: 2.435465169209306

Epoch: 6| Step: 7
Training loss: 2.9518632888793945
Validation loss: 2.4489824028425318

Epoch: 6| Step: 8
Training loss: 2.4548890590667725
Validation loss: 2.459621706316548

Epoch: 6| Step: 9
Training loss: 2.961256742477417
Validation loss: 2.4759238663540093

Epoch: 6| Step: 10
Training loss: 2.767181873321533
Validation loss: 2.491646356480096

Epoch: 6| Step: 11
Training loss: 2.685180187225342
Validation loss: 2.517492999312698

Epoch: 6| Step: 12
Training loss: 1.9928510189056396
Validation loss: 2.5490720528428272

Epoch: 6| Step: 13
Training loss: 2.6008005142211914
Validation loss: 2.555485115256361

Epoch: 124| Step: 0
Training loss: 2.8495407104492188
Validation loss: 2.5656186124329925

Epoch: 6| Step: 1
Training loss: 2.1580593585968018
Validation loss: 2.5506162592159805

Epoch: 6| Step: 2
Training loss: 2.7862625122070312
Validation loss: 2.5134781868227067

Epoch: 6| Step: 3
Training loss: 2.4991979598999023
Validation loss: 2.4936435555899017

Epoch: 6| Step: 4
Training loss: 3.014768600463867
Validation loss: 2.4639365416701122

Epoch: 6| Step: 5
Training loss: 3.0441436767578125
Validation loss: 2.429882718670753

Epoch: 6| Step: 6
Training loss: 2.895159959793091
Validation loss: 2.403466965562554

Epoch: 6| Step: 7
Training loss: 2.7681965827941895
Validation loss: 2.3968785116749425

Epoch: 6| Step: 8
Training loss: 2.79207181930542
Validation loss: 2.393992195847214

Epoch: 6| Step: 9
Training loss: 2.298855781555176
Validation loss: 2.4001703723784416

Epoch: 6| Step: 10
Training loss: 2.4908063411712646
Validation loss: 2.401585280254323

Epoch: 6| Step: 11
Training loss: 2.8613274097442627
Validation loss: 2.40540325257086

Epoch: 6| Step: 12
Training loss: 2.5462841987609863
Validation loss: 2.406821599570654

Epoch: 6| Step: 13
Training loss: 1.9216408729553223
Validation loss: 2.421744123581917

Epoch: 125| Step: 0
Training loss: 2.797839641571045
Validation loss: 2.4253146058769635

Epoch: 6| Step: 1
Training loss: 3.166020154953003
Validation loss: 2.4219819858510006

Epoch: 6| Step: 2
Training loss: 2.517752170562744
Validation loss: 2.4157754913453133

Epoch: 6| Step: 3
Training loss: 2.3233513832092285
Validation loss: 2.404561481168193

Epoch: 6| Step: 4
Training loss: 1.9241055250167847
Validation loss: 2.4002705927818053

Epoch: 6| Step: 5
Training loss: 1.971264123916626
Validation loss: 2.395680494205926

Epoch: 6| Step: 6
Training loss: 3.0765669345855713
Validation loss: 2.392470121383667

Epoch: 6| Step: 7
Training loss: 2.632857322692871
Validation loss: 2.398272160560854

Epoch: 6| Step: 8
Training loss: 2.601592540740967
Validation loss: 2.3973042529116393

Epoch: 6| Step: 9
Training loss: 3.4532408714294434
Validation loss: 2.397210433918943

Epoch: 6| Step: 10
Training loss: 3.473799228668213
Validation loss: 2.398896422437442

Epoch: 6| Step: 11
Training loss: 2.549091100692749
Validation loss: 2.394347695894139

Epoch: 6| Step: 12
Training loss: 2.0773026943206787
Validation loss: 2.3932464071499404

Epoch: 6| Step: 13
Training loss: 2.049616575241089
Validation loss: 2.3938071650843464

Epoch: 126| Step: 0
Training loss: 2.806213855743408
Validation loss: 2.3957403423965618

Epoch: 6| Step: 1
Training loss: 2.817476749420166
Validation loss: 2.401319767839165

Epoch: 6| Step: 2
Training loss: 2.3473381996154785
Validation loss: 2.393432314677905

Epoch: 6| Step: 3
Training loss: 3.444232940673828
Validation loss: 2.4068078417931833

Epoch: 6| Step: 4
Training loss: 2.6978344917297363
Validation loss: 2.408168456887686

Epoch: 6| Step: 5
Training loss: 2.4948835372924805
Validation loss: 2.40827702963224

Epoch: 6| Step: 6
Training loss: 3.0318470001220703
Validation loss: 2.4078515114322787

Epoch: 6| Step: 7
Training loss: 2.398857593536377
Validation loss: 2.396835519421485

Epoch: 6| Step: 8
Training loss: 1.7562938928604126
Validation loss: 2.3990988039201304

Epoch: 6| Step: 9
Training loss: 3.2036962509155273
Validation loss: 2.400367339452108

Epoch: 6| Step: 10
Training loss: 2.3359084129333496
Validation loss: 2.402609545697448

Epoch: 6| Step: 11
Training loss: 2.295639753341675
Validation loss: 2.404251601106377

Epoch: 6| Step: 12
Training loss: 2.4749293327331543
Validation loss: 2.4073297669810634

Epoch: 6| Step: 13
Training loss: 2.637312889099121
Validation loss: 2.413720456502771

Epoch: 127| Step: 0
Training loss: 2.6555287837982178
Validation loss: 2.4239386640569216

Epoch: 6| Step: 1
Training loss: 3.4439802169799805
Validation loss: 2.430946150133687

Epoch: 6| Step: 2
Training loss: 2.1025214195251465
Validation loss: 2.425020674223541

Epoch: 6| Step: 3
Training loss: 3.452176570892334
Validation loss: 2.4191118568502445

Epoch: 6| Step: 4
Training loss: 2.8154754638671875
Validation loss: 2.4106694652188208

Epoch: 6| Step: 5
Training loss: 2.4240541458129883
Validation loss: 2.3971636705501105

Epoch: 6| Step: 6
Training loss: 2.685899019241333
Validation loss: 2.3945888088595484

Epoch: 6| Step: 7
Training loss: 2.361057758331299
Validation loss: 2.391005439143027

Epoch: 6| Step: 8
Training loss: 2.8766896724700928
Validation loss: 2.385606050491333

Epoch: 6| Step: 9
Training loss: 3.0749497413635254
Validation loss: 2.384275628674415

Epoch: 6| Step: 10
Training loss: 2.3678526878356934
Validation loss: 2.3879990308515486

Epoch: 6| Step: 11
Training loss: 2.064450740814209
Validation loss: 2.394324537246458

Epoch: 6| Step: 12
Training loss: 2.642749309539795
Validation loss: 2.415857261227023

Epoch: 6| Step: 13
Training loss: 1.3716630935668945
Validation loss: 2.437699810151131

Epoch: 128| Step: 0
Training loss: 2.348153829574585
Validation loss: 2.4355232536151843

Epoch: 6| Step: 1
Training loss: 2.71492600440979
Validation loss: 2.4542863368988037

Epoch: 6| Step: 2
Training loss: 2.3567357063293457
Validation loss: 2.460008668643172

Epoch: 6| Step: 3
Training loss: 2.6274561882019043
Validation loss: 2.43914859525619

Epoch: 6| Step: 4
Training loss: 3.3206734657287598
Validation loss: 2.439517174997637

Epoch: 6| Step: 5
Training loss: 2.398329973220825
Validation loss: 2.427874763806661

Epoch: 6| Step: 6
Training loss: 2.2442972660064697
Validation loss: 2.429190235753213

Epoch: 6| Step: 7
Training loss: 2.092005729675293
Validation loss: 2.414792004451957

Epoch: 6| Step: 8
Training loss: 2.8360819816589355
Validation loss: 2.4061008884060766

Epoch: 6| Step: 9
Training loss: 3.0474019050598145
Validation loss: 2.4004921964419785

Epoch: 6| Step: 10
Training loss: 2.6419100761413574
Validation loss: 2.3887872542104414

Epoch: 6| Step: 11
Training loss: 2.766434669494629
Validation loss: 2.392904095752265

Epoch: 6| Step: 12
Training loss: 2.978938579559326
Validation loss: 2.387600209123345

Epoch: 6| Step: 13
Training loss: 1.924181580543518
Validation loss: 2.3938969386521207

Epoch: 129| Step: 0
Training loss: 2.5378079414367676
Validation loss: 2.3854609715041293

Epoch: 6| Step: 1
Training loss: 2.934633731842041
Validation loss: 2.3818554878234863

Epoch: 6| Step: 2
Training loss: 1.818489909172058
Validation loss: 2.389350134839294

Epoch: 6| Step: 3
Training loss: 2.2814624309539795
Validation loss: 2.3755078315734863

Epoch: 6| Step: 4
Training loss: 3.5000557899475098
Validation loss: 2.370797875106976

Epoch: 6| Step: 5
Training loss: 2.8908298015594482
Validation loss: 2.3727243356807257

Epoch: 6| Step: 6
Training loss: 2.5790910720825195
Validation loss: 2.380247413471181

Epoch: 6| Step: 7
Training loss: 2.320105791091919
Validation loss: 2.3782960086740474

Epoch: 6| Step: 8
Training loss: 2.932483196258545
Validation loss: 2.3779639018479215

Epoch: 6| Step: 9
Training loss: 2.4499690532684326
Validation loss: 2.3841797510782876

Epoch: 6| Step: 10
Training loss: 2.735675811767578
Validation loss: 2.3852572210373415

Epoch: 6| Step: 11
Training loss: 2.367919445037842
Validation loss: 2.3927006183132047

Epoch: 6| Step: 12
Training loss: 2.3776028156280518
Validation loss: 2.3979802913563226

Epoch: 6| Step: 13
Training loss: 3.0825064182281494
Validation loss: 2.4130488954564577

Epoch: 130| Step: 0
Training loss: 2.689216136932373
Validation loss: 2.4427074757955407

Epoch: 6| Step: 1
Training loss: 2.6115057468414307
Validation loss: 2.4668909426658385

Epoch: 6| Step: 2
Training loss: 3.1426544189453125
Validation loss: 2.4934289583595852

Epoch: 6| Step: 3
Training loss: 2.1022748947143555
Validation loss: 2.481360443176762

Epoch: 6| Step: 4
Training loss: 2.63071870803833
Validation loss: 2.496039141890823

Epoch: 6| Step: 5
Training loss: 2.7953944206237793
Validation loss: 2.4320616927198184

Epoch: 6| Step: 6
Training loss: 2.7013137340545654
Validation loss: 2.401984101982527

Epoch: 6| Step: 7
Training loss: 2.885577440261841
Validation loss: 2.3803352335447907

Epoch: 6| Step: 8
Training loss: 2.78151273727417
Validation loss: 2.372548959588492

Epoch: 6| Step: 9
Training loss: 2.2576889991760254
Validation loss: 2.3712394416973157

Epoch: 6| Step: 10
Training loss: 2.186429023742676
Validation loss: 2.380773672493555

Epoch: 6| Step: 11
Training loss: 3.1373467445373535
Validation loss: 2.3830741425996185

Epoch: 6| Step: 12
Training loss: 2.5333456993103027
Validation loss: 2.395642572833646

Epoch: 6| Step: 13
Training loss: 2.4258322715759277
Validation loss: 2.3896940254396006

Epoch: 131| Step: 0
Training loss: 3.249739408493042
Validation loss: 2.3905717326748754

Epoch: 6| Step: 1
Training loss: 2.1805734634399414
Validation loss: 2.401536528782178

Epoch: 6| Step: 2
Training loss: 3.099386692047119
Validation loss: 2.3972250364160024

Epoch: 6| Step: 3
Training loss: 3.1981189250946045
Validation loss: 2.38701359943677

Epoch: 6| Step: 4
Training loss: 2.5946102142333984
Validation loss: 2.3922290007273355

Epoch: 6| Step: 5
Training loss: 2.439953327178955
Validation loss: 2.391528375687138

Epoch: 6| Step: 6
Training loss: 2.6946520805358887
Validation loss: 2.386922081311544

Epoch: 6| Step: 7
Training loss: 2.301542282104492
Validation loss: 2.39005454637671

Epoch: 6| Step: 8
Training loss: 1.9909980297088623
Validation loss: 2.3895914939142044

Epoch: 6| Step: 9
Training loss: 2.4351911544799805
Validation loss: 2.3840344593089116

Epoch: 6| Step: 10
Training loss: 2.7760987281799316
Validation loss: 2.3834589348044446

Epoch: 6| Step: 11
Training loss: 2.7555973529815674
Validation loss: 2.400811949083882

Epoch: 6| Step: 12
Training loss: 2.6753435134887695
Validation loss: 2.4181494456465527

Epoch: 6| Step: 13
Training loss: 2.2502970695495605
Validation loss: 2.4393475568422707

Epoch: 132| Step: 0
Training loss: 2.961696147918701
Validation loss: 2.4463218412091656

Epoch: 6| Step: 1
Training loss: 2.8305070400238037
Validation loss: 2.470184585099579

Epoch: 6| Step: 2
Training loss: 2.495166301727295
Validation loss: 2.4836239712212675

Epoch: 6| Step: 3
Training loss: 2.857384204864502
Validation loss: 2.470515192195933

Epoch: 6| Step: 4
Training loss: 2.610398292541504
Validation loss: 2.4505414693586287

Epoch: 6| Step: 5
Training loss: 2.4885506629943848
Validation loss: 2.450638601856847

Epoch: 6| Step: 6
Training loss: 2.4869225025177
Validation loss: 2.433872030627343

Epoch: 6| Step: 7
Training loss: 2.7055201530456543
Validation loss: 2.430094178004931

Epoch: 6| Step: 8
Training loss: 2.8915371894836426
Validation loss: 2.430286668962048

Epoch: 6| Step: 9
Training loss: 2.5459980964660645
Validation loss: 2.4190132720496065

Epoch: 6| Step: 10
Training loss: 2.9364676475524902
Validation loss: 2.3925710237154396

Epoch: 6| Step: 11
Training loss: 2.4047019481658936
Validation loss: 2.3692558145010345

Epoch: 6| Step: 12
Training loss: 1.855820655822754
Validation loss: 2.3529301663880706

Epoch: 6| Step: 13
Training loss: 2.812744379043579
Validation loss: 2.3585520431559575

Epoch: 133| Step: 0
Training loss: 2.81630802154541
Validation loss: 2.38073702268703

Epoch: 6| Step: 1
Training loss: 2.8785440921783447
Validation loss: 2.413195771555747

Epoch: 6| Step: 2
Training loss: 2.419764518737793
Validation loss: 2.432514916184128

Epoch: 6| Step: 3
Training loss: 2.648298740386963
Validation loss: 2.46113097283148

Epoch: 6| Step: 4
Training loss: 2.2857470512390137
Validation loss: 2.4777244688362203

Epoch: 6| Step: 5
Training loss: 2.45877742767334
Validation loss: 2.495563463498187

Epoch: 6| Step: 6
Training loss: 3.2796807289123535
Validation loss: 2.447429382672874

Epoch: 6| Step: 7
Training loss: 2.295956611633301
Validation loss: 2.393008965317921

Epoch: 6| Step: 8
Training loss: 2.7678067684173584
Validation loss: 2.3548004114499657

Epoch: 6| Step: 9
Training loss: 2.9334654808044434
Validation loss: 2.3471142502241236

Epoch: 6| Step: 10
Training loss: 1.9680960178375244
Validation loss: 2.3515430265857327

Epoch: 6| Step: 11
Training loss: 2.536522150039673
Validation loss: 2.357575278128347

Epoch: 6| Step: 12
Training loss: 3.1544415950775146
Validation loss: 2.3760095463004163

Epoch: 6| Step: 13
Training loss: 2.3525898456573486
Validation loss: 2.38466154631748

Epoch: 134| Step: 0
Training loss: 2.9497451782226562
Validation loss: 2.4078985696197837

Epoch: 6| Step: 1
Training loss: 2.644779682159424
Validation loss: 2.444026553502647

Epoch: 6| Step: 2
Training loss: 2.0809340476989746
Validation loss: 2.444906711578369

Epoch: 6| Step: 3
Training loss: 2.8433661460876465
Validation loss: 2.424136910387265

Epoch: 6| Step: 4
Training loss: 2.6605210304260254
Validation loss: 2.4222915057213075

Epoch: 6| Step: 5
Training loss: 2.6175522804260254
Validation loss: 2.4119176659532773

Epoch: 6| Step: 6
Training loss: 2.4073710441589355
Validation loss: 2.3815898997809297

Epoch: 6| Step: 7
Training loss: 2.1966552734375
Validation loss: 2.357350495553786

Epoch: 6| Step: 8
Training loss: 2.6176233291625977
Validation loss: 2.340373385337091

Epoch: 6| Step: 9
Training loss: 3.44270396232605
Validation loss: 2.343674716129098

Epoch: 6| Step: 10
Training loss: 2.838604211807251
Validation loss: 2.342279908477619

Epoch: 6| Step: 11
Training loss: 2.3344671726226807
Validation loss: 2.337414954298286

Epoch: 6| Step: 12
Training loss: 2.3350234031677246
Validation loss: 2.335238879726779

Epoch: 6| Step: 13
Training loss: 2.474118232727051
Validation loss: 2.3414776953317786

Epoch: 135| Step: 0
Training loss: 2.911262035369873
Validation loss: 2.3388367775947816

Epoch: 6| Step: 1
Training loss: 2.545895576477051
Validation loss: 2.343312919780772

Epoch: 6| Step: 2
Training loss: 2.1519839763641357
Validation loss: 2.3410316257066626

Epoch: 6| Step: 3
Training loss: 2.137807607650757
Validation loss: 2.3409008261977986

Epoch: 6| Step: 4
Training loss: 1.751198410987854
Validation loss: 2.344361830783147

Epoch: 6| Step: 5
Training loss: 2.832693099975586
Validation loss: 2.3502805899548274

Epoch: 6| Step: 6
Training loss: 3.333007574081421
Validation loss: 2.3484532038370767

Epoch: 6| Step: 7
Training loss: 2.6021432876586914
Validation loss: 2.345353772563319

Epoch: 6| Step: 8
Training loss: 3.368165969848633
Validation loss: 2.3490509153694235

Epoch: 6| Step: 9
Training loss: 2.5494396686553955
Validation loss: 2.3435840401598202

Epoch: 6| Step: 10
Training loss: 2.1932976245880127
Validation loss: 2.3407372056796985

Epoch: 6| Step: 11
Training loss: 2.1616225242614746
Validation loss: 2.3493610940953737

Epoch: 6| Step: 12
Training loss: 3.3284406661987305
Validation loss: 2.353905206085533

Epoch: 6| Step: 13
Training loss: 2.2223265171051025
Validation loss: 2.344146177332888

Epoch: 136| Step: 0
Training loss: 2.2828612327575684
Validation loss: 2.3581988760220107

Epoch: 6| Step: 1
Training loss: 2.6722707748413086
Validation loss: 2.360524087823847

Epoch: 6| Step: 2
Training loss: 2.225895404815674
Validation loss: 2.3686496468000513

Epoch: 6| Step: 3
Training loss: 2.1187100410461426
Validation loss: 2.36640582289747

Epoch: 6| Step: 4
Training loss: 2.7888078689575195
Validation loss: 2.3682245362189507

Epoch: 6| Step: 5
Training loss: 2.749612331390381
Validation loss: 2.3505539483921503

Epoch: 6| Step: 6
Training loss: 1.8656009435653687
Validation loss: 2.342703421910604

Epoch: 6| Step: 7
Training loss: 2.9856600761413574
Validation loss: 2.33951836247598

Epoch: 6| Step: 8
Training loss: 2.9950051307678223
Validation loss: 2.344766609130367

Epoch: 6| Step: 9
Training loss: 2.5160045623779297
Validation loss: 2.3368508918311006

Epoch: 6| Step: 10
Training loss: 3.2359981536865234
Validation loss: 2.3518064329701085

Epoch: 6| Step: 11
Training loss: 3.051475763320923
Validation loss: 2.3481185948976906

Epoch: 6| Step: 12
Training loss: 2.4728617668151855
Validation loss: 2.344212270552112

Epoch: 6| Step: 13
Training loss: 1.9726943969726562
Validation loss: 2.3342392649701846

Epoch: 137| Step: 0
Training loss: 1.781868577003479
Validation loss: 2.334810318485383

Epoch: 6| Step: 1
Training loss: 3.0074572563171387
Validation loss: 2.3380850566330778

Epoch: 6| Step: 2
Training loss: 2.4909415245056152
Validation loss: 2.3226774379771244

Epoch: 6| Step: 3
Training loss: 2.760716438293457
Validation loss: 2.3309561308994087

Epoch: 6| Step: 4
Training loss: 2.8223977088928223
Validation loss: 2.3274094263712564

Epoch: 6| Step: 5
Training loss: 2.6815834045410156
Validation loss: 2.3269581615283923

Epoch: 6| Step: 6
Training loss: 2.341292381286621
Validation loss: 2.320748803436115

Epoch: 6| Step: 7
Training loss: 2.828071117401123
Validation loss: 2.314971685409546

Epoch: 6| Step: 8
Training loss: 3.2729320526123047
Validation loss: 2.3131398103570424

Epoch: 6| Step: 9
Training loss: 2.6086108684539795
Validation loss: 2.318009353453113

Epoch: 6| Step: 10
Training loss: 1.9097871780395508
Validation loss: 2.3125864587804323

Epoch: 6| Step: 11
Training loss: 2.774965763092041
Validation loss: 2.3121210836595103

Epoch: 6| Step: 12
Training loss: 2.397196054458618
Validation loss: 2.315445892272457

Epoch: 6| Step: 13
Training loss: 2.4521353244781494
Validation loss: 2.3146910231600524

Epoch: 138| Step: 0
Training loss: 3.3926401138305664
Validation loss: 2.320715437653244

Epoch: 6| Step: 1
Training loss: 2.036835193634033
Validation loss: 2.322796308866111

Epoch: 6| Step: 2
Training loss: 2.753079414367676
Validation loss: 2.3344107315104496

Epoch: 6| Step: 3
Training loss: 2.622953414916992
Validation loss: 2.34249956633455

Epoch: 6| Step: 4
Training loss: 2.0633857250213623
Validation loss: 2.3472609468685683

Epoch: 6| Step: 5
Training loss: 1.551095962524414
Validation loss: 2.3409409574283067

Epoch: 6| Step: 6
Training loss: 3.328716278076172
Validation loss: 2.348676714845883

Epoch: 6| Step: 7
Training loss: 3.186511278152466
Validation loss: 2.3364508382735716

Epoch: 6| Step: 8
Training loss: 2.3617682456970215
Validation loss: 2.345006440275459

Epoch: 6| Step: 9
Training loss: 2.7099802494049072
Validation loss: 2.366213462686026

Epoch: 6| Step: 10
Training loss: 2.583050489425659
Validation loss: 2.379157986692203

Epoch: 6| Step: 11
Training loss: 2.4736223220825195
Validation loss: 2.387126716234351

Epoch: 6| Step: 12
Training loss: 2.4992198944091797
Validation loss: 2.413636492144677

Epoch: 6| Step: 13
Training loss: 2.6904349327087402
Validation loss: 2.41325101032052

Epoch: 139| Step: 0
Training loss: 2.402144432067871
Validation loss: 2.39640284610051

Epoch: 6| Step: 1
Training loss: 3.5537197589874268
Validation loss: 2.3829833999756844

Epoch: 6| Step: 2
Training loss: 2.608513116836548
Validation loss: 2.3667015003901657

Epoch: 6| Step: 3
Training loss: 1.8360872268676758
Validation loss: 2.3470653692881265

Epoch: 6| Step: 4
Training loss: 2.311534881591797
Validation loss: 2.3338348352780907

Epoch: 6| Step: 5
Training loss: 3.198803424835205
Validation loss: 2.3280781033218547

Epoch: 6| Step: 6
Training loss: 3.388890266418457
Validation loss: 2.3268003694472776

Epoch: 6| Step: 7
Training loss: 1.8557106256484985
Validation loss: 2.323078368299751

Epoch: 6| Step: 8
Training loss: 1.9631000757217407
Validation loss: 2.3216580344784643

Epoch: 6| Step: 9
Training loss: 3.138364791870117
Validation loss: 2.3250324085194576

Epoch: 6| Step: 10
Training loss: 1.6914215087890625
Validation loss: 2.323256333669027

Epoch: 6| Step: 11
Training loss: 3.511539936065674
Validation loss: 2.324100508484789

Epoch: 6| Step: 12
Training loss: 1.733792781829834
Validation loss: 2.321461098168486

Epoch: 6| Step: 13
Training loss: 2.993774175643921
Validation loss: 2.328979637033196

Epoch: 140| Step: 0
Training loss: 3.2684013843536377
Validation loss: 2.346456430291617

Epoch: 6| Step: 1
Training loss: 2.309603691101074
Validation loss: 2.3778150812272103

Epoch: 6| Step: 2
Training loss: 1.9696851968765259
Validation loss: 2.4143947862809703

Epoch: 6| Step: 3
Training loss: 2.198307514190674
Validation loss: 2.45542920789411

Epoch: 6| Step: 4
Training loss: 3.41355037689209
Validation loss: 2.509446301767903

Epoch: 6| Step: 5
Training loss: 2.393657684326172
Validation loss: 2.508226599744571

Epoch: 6| Step: 6
Training loss: 2.9855690002441406
Validation loss: 2.527950809847924

Epoch: 6| Step: 7
Training loss: 3.3192734718322754
Validation loss: 2.5149037991800616

Epoch: 6| Step: 8
Training loss: 2.393303632736206
Validation loss: 2.5108718513160624

Epoch: 6| Step: 9
Training loss: 2.0960164070129395
Validation loss: 2.454505141063403

Epoch: 6| Step: 10
Training loss: 2.744147539138794
Validation loss: 2.3705710390562653

Epoch: 6| Step: 11
Training loss: 2.66925048828125
Validation loss: 2.3337663886367634

Epoch: 6| Step: 12
Training loss: 2.716324806213379
Validation loss: 2.319567347085604

Epoch: 6| Step: 13
Training loss: 1.8700815439224243
Validation loss: 2.323468631313693

Epoch: 141| Step: 0
Training loss: 2.4173622131347656
Validation loss: 2.3300780224543747

Epoch: 6| Step: 1
Training loss: 2.614119291305542
Validation loss: 2.3431004683176675

Epoch: 6| Step: 2
Training loss: 2.4081497192382812
Validation loss: 2.354637868942753

Epoch: 6| Step: 3
Training loss: 2.0076823234558105
Validation loss: 2.375402178815616

Epoch: 6| Step: 4
Training loss: 2.3510310649871826
Validation loss: 2.4105920971080823

Epoch: 6| Step: 5
Training loss: 2.5563926696777344
Validation loss: 2.4313370899487565

Epoch: 6| Step: 6
Training loss: 2.706643581390381
Validation loss: 2.457381684293029

Epoch: 6| Step: 7
Training loss: 2.4567747116088867
Validation loss: 2.4479644478008313

Epoch: 6| Step: 8
Training loss: 2.6968913078308105
Validation loss: 2.46365253643323

Epoch: 6| Step: 9
Training loss: 3.059455633163452
Validation loss: 2.511394544314313

Epoch: 6| Step: 10
Training loss: 3.0448596477508545
Validation loss: 2.51045290629069

Epoch: 6| Step: 11
Training loss: 3.498121500015259
Validation loss: 2.4771404163811797

Epoch: 6| Step: 12
Training loss: 2.8644890785217285
Validation loss: 2.434466977273264

Epoch: 6| Step: 13
Training loss: 1.7470101118087769
Validation loss: 2.395048020988382

Epoch: 142| Step: 0
Training loss: 2.501556873321533
Validation loss: 2.3413916069974183

Epoch: 6| Step: 1
Training loss: 2.9510483741760254
Validation loss: 2.3267339275729273

Epoch: 6| Step: 2
Training loss: 2.89797306060791
Validation loss: 2.325385101379887

Epoch: 6| Step: 3
Training loss: 2.3042221069335938
Validation loss: 2.322847380433031

Epoch: 6| Step: 4
Training loss: 2.954749345779419
Validation loss: 2.3379100343232513

Epoch: 6| Step: 5
Training loss: 1.8749008178710938
Validation loss: 2.3473499385259484

Epoch: 6| Step: 6
Training loss: 2.1856026649475098
Validation loss: 2.3750310482517367

Epoch: 6| Step: 7
Training loss: 2.3807506561279297
Validation loss: 2.419884104882517

Epoch: 6| Step: 8
Training loss: 2.777825355529785
Validation loss: 2.4082166097497426

Epoch: 6| Step: 9
Training loss: 2.9977612495422363
Validation loss: 2.3647813079177693

Epoch: 6| Step: 10
Training loss: 2.8932762145996094
Validation loss: 2.3457699257840394

Epoch: 6| Step: 11
Training loss: 2.659226894378662
Validation loss: 2.3453742842520438

Epoch: 6| Step: 12
Training loss: 2.255064010620117
Validation loss: 2.3374757407813944

Epoch: 6| Step: 13
Training loss: 2.647406816482544
Validation loss: 2.3449308538949616

Epoch: 143| Step: 0
Training loss: 2.4064536094665527
Validation loss: 2.337814223381781

Epoch: 6| Step: 1
Training loss: 2.8340601921081543
Validation loss: 2.353140596420534

Epoch: 6| Step: 2
Training loss: 2.1493358612060547
Validation loss: 2.356430148565641

Epoch: 6| Step: 3
Training loss: 2.4366657733917236
Validation loss: 2.3533608759603193

Epoch: 6| Step: 4
Training loss: 2.9317781925201416
Validation loss: 2.3514959401981805

Epoch: 6| Step: 5
Training loss: 2.1486358642578125
Validation loss: 2.3543680970386793

Epoch: 6| Step: 6
Training loss: 3.3554162979125977
Validation loss: 2.34784278561992

Epoch: 6| Step: 7
Training loss: 2.9289002418518066
Validation loss: 2.3481785123066237

Epoch: 6| Step: 8
Training loss: 2.3901095390319824
Validation loss: 2.362617156838858

Epoch: 6| Step: 9
Training loss: 2.960221290588379
Validation loss: 2.375117012249526

Epoch: 6| Step: 10
Training loss: 2.2660295963287354
Validation loss: 2.3714218293466875

Epoch: 6| Step: 11
Training loss: 2.4320058822631836
Validation loss: 2.378307539929626

Epoch: 6| Step: 12
Training loss: 2.1385483741760254
Validation loss: 2.37441768441149

Epoch: 6| Step: 13
Training loss: 2.486706256866455
Validation loss: 2.3767225896158526

Epoch: 144| Step: 0
Training loss: 2.449923515319824
Validation loss: 2.373543488082065

Epoch: 6| Step: 1
Training loss: 2.0847818851470947
Validation loss: 2.355824321828863

Epoch: 6| Step: 2
Training loss: 2.1454527378082275
Validation loss: 2.367379952502507

Epoch: 6| Step: 3
Training loss: 2.404588222503662
Validation loss: 2.37819577288884

Epoch: 6| Step: 4
Training loss: 2.633078098297119
Validation loss: 2.3736572265625

Epoch: 6| Step: 5
Training loss: 2.12178897857666
Validation loss: 2.3708701646456154

Epoch: 6| Step: 6
Training loss: 2.3591983318328857
Validation loss: 2.3723299477690007

Epoch: 6| Step: 7
Training loss: 2.996683120727539
Validation loss: 2.362995206668813

Epoch: 6| Step: 8
Training loss: 3.365846633911133
Validation loss: 2.3495051886445735

Epoch: 6| Step: 9
Training loss: 2.708268880844116
Validation loss: 2.336190426221458

Epoch: 6| Step: 10
Training loss: 2.563288688659668
Validation loss: 2.3413102370436474

Epoch: 6| Step: 11
Training loss: 2.701375722885132
Validation loss: 2.3142549914698445

Epoch: 6| Step: 12
Training loss: 2.3158819675445557
Validation loss: 2.325652860826062

Epoch: 6| Step: 13
Training loss: 3.1503660678863525
Validation loss: 2.308091184144379

Epoch: 145| Step: 0
Training loss: 2.9801578521728516
Validation loss: 2.307751768378801

Epoch: 6| Step: 1
Training loss: 2.6497881412506104
Validation loss: 2.3088027892574186

Epoch: 6| Step: 2
Training loss: 2.789447546005249
Validation loss: 2.3095471910251084

Epoch: 6| Step: 3
Training loss: 2.7483482360839844
Validation loss: 2.3092073830225135

Epoch: 6| Step: 4
Training loss: 2.52710223197937
Validation loss: 2.3102174599965415

Epoch: 6| Step: 5
Training loss: 2.591996192932129
Validation loss: 2.3095887040579193

Epoch: 6| Step: 6
Training loss: 2.3096799850463867
Validation loss: 2.3043984110637377

Epoch: 6| Step: 7
Training loss: 2.009859085083008
Validation loss: 2.31461400370444

Epoch: 6| Step: 8
Training loss: 2.753927707672119
Validation loss: 2.3035262528286187

Epoch: 6| Step: 9
Training loss: 2.755234718322754
Validation loss: 2.3096087055821575

Epoch: 6| Step: 10
Training loss: 2.678292989730835
Validation loss: 2.3050646282011464

Epoch: 6| Step: 11
Training loss: 1.8423755168914795
Validation loss: 2.302604026691888

Epoch: 6| Step: 12
Training loss: 2.882389545440674
Validation loss: 2.299051978254831

Epoch: 6| Step: 13
Training loss: 2.15651535987854
Validation loss: 2.3016798291155087

Epoch: 146| Step: 0
Training loss: 2.5870468616485596
Validation loss: 2.3069111352325766

Epoch: 6| Step: 1
Training loss: 2.9644551277160645
Validation loss: 2.2980396004133326

Epoch: 6| Step: 2
Training loss: 2.288810968399048
Validation loss: 2.297944040708644

Epoch: 6| Step: 3
Training loss: 2.765756845474243
Validation loss: 2.3047900071708103

Epoch: 6| Step: 4
Training loss: 1.5236154794692993
Validation loss: 2.3078282212698333

Epoch: 6| Step: 5
Training loss: 2.6632938385009766
Validation loss: 2.3098359184880413

Epoch: 6| Step: 6
Training loss: 3.098752498626709
Validation loss: 2.3114567161888204

Epoch: 6| Step: 7
Training loss: 2.4171369075775146
Validation loss: 2.3130914421491724

Epoch: 6| Step: 8
Training loss: 2.290391445159912
Validation loss: 2.313047875640213

Epoch: 6| Step: 9
Training loss: 2.437303066253662
Validation loss: 2.3168266204095658

Epoch: 6| Step: 10
Training loss: 2.880056142807007
Validation loss: 2.316967571935346

Epoch: 6| Step: 11
Training loss: 2.4515790939331055
Validation loss: 2.32704423832637

Epoch: 6| Step: 12
Training loss: 3.0809454917907715
Validation loss: 2.3136390691162436

Epoch: 6| Step: 13
Training loss: 2.2005414962768555
Validation loss: 2.322606991696101

Epoch: 147| Step: 0
Training loss: 3.0546717643737793
Validation loss: 2.3293368893284954

Epoch: 6| Step: 1
Training loss: 2.3724050521850586
Validation loss: 2.337593573395924

Epoch: 6| Step: 2
Training loss: 2.5567574501037598
Validation loss: 2.3457417898280646

Epoch: 6| Step: 3
Training loss: 2.6951422691345215
Validation loss: 2.3522792144488265

Epoch: 6| Step: 4
Training loss: 2.897468328475952
Validation loss: 2.3673615276172595

Epoch: 6| Step: 5
Training loss: 2.0138816833496094
Validation loss: 2.36461881924701

Epoch: 6| Step: 6
Training loss: 2.6183080673217773
Validation loss: 2.3717165839287544

Epoch: 6| Step: 7
Training loss: 2.3938417434692383
Validation loss: 2.3707659654719855

Epoch: 6| Step: 8
Training loss: 2.770020008087158
Validation loss: 2.3704900921031995

Epoch: 6| Step: 9
Training loss: 3.289323568344116
Validation loss: 2.366510916781682

Epoch: 6| Step: 10
Training loss: 2.5149970054626465
Validation loss: 2.348709326918407

Epoch: 6| Step: 11
Training loss: 2.143801689147949
Validation loss: 2.329015229337959

Epoch: 6| Step: 12
Training loss: 2.072984218597412
Validation loss: 2.31679702830571

Epoch: 6| Step: 13
Training loss: 2.4388880729675293
Validation loss: 2.301588068726242

Epoch: 148| Step: 0
Training loss: 2.278571367263794
Validation loss: 2.2914640365108365

Epoch: 6| Step: 1
Training loss: 2.015660524368286
Validation loss: 2.2888651611984416

Epoch: 6| Step: 2
Training loss: 2.8329415321350098
Validation loss: 2.288593110217843

Epoch: 6| Step: 3
Training loss: 2.4140405654907227
Validation loss: 2.2866038622394687

Epoch: 6| Step: 4
Training loss: 3.0196292400360107
Validation loss: 2.284415324529012

Epoch: 6| Step: 5
Training loss: 2.413588047027588
Validation loss: 2.284321815736832

Epoch: 6| Step: 6
Training loss: 3.03373122215271
Validation loss: 2.2871556384589082

Epoch: 6| Step: 7
Training loss: 2.2031331062316895
Validation loss: 2.2924188157563568

Epoch: 6| Step: 8
Training loss: 2.221456289291382
Validation loss: 2.28673622556912

Epoch: 6| Step: 9
Training loss: 2.553966999053955
Validation loss: 2.2937418773610103

Epoch: 6| Step: 10
Training loss: 1.9478440284729004
Validation loss: 2.294584280701094

Epoch: 6| Step: 11
Training loss: 3.1194815635681152
Validation loss: 2.299348538921725

Epoch: 6| Step: 12
Training loss: 2.7493488788604736
Validation loss: 2.3122211733172016

Epoch: 6| Step: 13
Training loss: 3.3156027793884277
Validation loss: 2.314561774653773

Epoch: 149| Step: 0
Training loss: 2.37931489944458
Validation loss: 2.323071810507005

Epoch: 6| Step: 1
Training loss: 2.4781346321105957
Validation loss: 2.3277695819895756

Epoch: 6| Step: 2
Training loss: 2.4095613956451416
Validation loss: 2.336930057053925

Epoch: 6| Step: 3
Training loss: 2.094972848892212
Validation loss: 2.3439357742186515

Epoch: 6| Step: 4
Training loss: 2.767446994781494
Validation loss: 2.3385496908618557

Epoch: 6| Step: 5
Training loss: 3.4357595443725586
Validation loss: 2.3462445735931396

Epoch: 6| Step: 6
Training loss: 3.0618348121643066
Validation loss: 2.3354557098880893

Epoch: 6| Step: 7
Training loss: 2.2404770851135254
Validation loss: 2.3319421686151975

Epoch: 6| Step: 8
Training loss: 2.9961729049682617
Validation loss: 2.330094437445364

Epoch: 6| Step: 9
Training loss: 2.0515332221984863
Validation loss: 2.324863541510797

Epoch: 6| Step: 10
Training loss: 2.421168565750122
Validation loss: 2.3216321083807174

Epoch: 6| Step: 11
Training loss: 2.653228998184204
Validation loss: 2.3222825347736316

Epoch: 6| Step: 12
Training loss: 2.3477983474731445
Validation loss: 2.3235513523060787

Epoch: 6| Step: 13
Training loss: 2.1320242881774902
Validation loss: 2.31473780703801

Epoch: 150| Step: 0
Training loss: 2.522660970687866
Validation loss: 2.3187492175768782

Epoch: 6| Step: 1
Training loss: 2.047274351119995
Validation loss: 2.3290291024792578

Epoch: 6| Step: 2
Training loss: 2.550631523132324
Validation loss: 2.326508834797849

Epoch: 6| Step: 3
Training loss: 2.983901262283325
Validation loss: 2.32826659499958

Epoch: 6| Step: 4
Training loss: 2.8890812397003174
Validation loss: 2.3294925612788044

Epoch: 6| Step: 5
Training loss: 2.307093620300293
Validation loss: 2.3321751586852537

Epoch: 6| Step: 6
Training loss: 2.224731922149658
Validation loss: 2.3309013766627156

Epoch: 6| Step: 7
Training loss: 3.037233829498291
Validation loss: 2.3261350406113492

Epoch: 6| Step: 8
Training loss: 2.9898619651794434
Validation loss: 2.3123184404065533

Epoch: 6| Step: 9
Training loss: 2.3410725593566895
Validation loss: 2.3225148954699115

Epoch: 6| Step: 10
Training loss: 2.420867443084717
Validation loss: 2.3169026092816423

Epoch: 6| Step: 11
Training loss: 2.193958282470703
Validation loss: 2.312473386846563

Epoch: 6| Step: 12
Training loss: 2.5669796466827393
Validation loss: 2.3224897115461287

Epoch: 6| Step: 13
Training loss: 2.5825788974761963
Validation loss: 2.3220147984002226

Epoch: 151| Step: 0
Training loss: 2.821730375289917
Validation loss: 2.328338881974579

Epoch: 6| Step: 1
Training loss: 3.2130002975463867
Validation loss: 2.332811983682776

Epoch: 6| Step: 2
Training loss: 2.063734769821167
Validation loss: 2.3287528407189155

Epoch: 6| Step: 3
Training loss: 1.737575650215149
Validation loss: 2.326386505557645

Epoch: 6| Step: 4
Training loss: 2.645108222961426
Validation loss: 2.331969068896386

Epoch: 6| Step: 5
Training loss: 3.033292770385742
Validation loss: 2.331196895209692

Epoch: 6| Step: 6
Training loss: 3.2584686279296875
Validation loss: 2.347528483278008

Epoch: 6| Step: 7
Training loss: 2.301848888397217
Validation loss: 2.3551381480309272

Epoch: 6| Step: 8
Training loss: 2.6025915145874023
Validation loss: 2.3634713413894817

Epoch: 6| Step: 9
Training loss: 1.876685619354248
Validation loss: 2.365932241562874

Epoch: 6| Step: 10
Training loss: 2.6418561935424805
Validation loss: 2.3629398551038516

Epoch: 6| Step: 11
Training loss: 2.978015899658203
Validation loss: 2.363587220509847

Epoch: 6| Step: 12
Training loss: 2.2011032104492188
Validation loss: 2.344201454552271

Epoch: 6| Step: 13
Training loss: 2.1594817638397217
Validation loss: 2.32589586447644

Epoch: 152| Step: 0
Training loss: 3.1468234062194824
Validation loss: 2.321387711391654

Epoch: 6| Step: 1
Training loss: 2.0739152431488037
Validation loss: 2.311201754436698

Epoch: 6| Step: 2
Training loss: 2.1279754638671875
Validation loss: 2.310429221840315

Epoch: 6| Step: 3
Training loss: 2.8746628761291504
Validation loss: 2.3141721961318806

Epoch: 6| Step: 4
Training loss: 2.3309926986694336
Validation loss: 2.3098109768282984

Epoch: 6| Step: 5
Training loss: 2.837618350982666
Validation loss: 2.3111370430197766

Epoch: 6| Step: 6
Training loss: 3.0272092819213867
Validation loss: 2.3206058984161704

Epoch: 6| Step: 7
Training loss: 2.12974214553833
Validation loss: 2.3160317097940752

Epoch: 6| Step: 8
Training loss: 2.254909038543701
Validation loss: 2.3203371160773822

Epoch: 6| Step: 9
Training loss: 1.9543414115905762
Validation loss: 2.3201061371834046

Epoch: 6| Step: 10
Training loss: 2.270263433456421
Validation loss: 2.3251731626449095

Epoch: 6| Step: 11
Training loss: 2.58549165725708
Validation loss: 2.3321726809265795

Epoch: 6| Step: 12
Training loss: 3.2816991806030273
Validation loss: 2.339893515392016

Epoch: 6| Step: 13
Training loss: 2.7866203784942627
Validation loss: 2.3404302340681835

Epoch: 153| Step: 0
Training loss: 2.3354642391204834
Validation loss: 2.3402311340455086

Epoch: 6| Step: 1
Training loss: 2.0935723781585693
Validation loss: 2.327152852089174

Epoch: 6| Step: 2
Training loss: 2.082066059112549
Validation loss: 2.315319707316737

Epoch: 6| Step: 3
Training loss: 2.4641082286834717
Validation loss: 2.3141742624262327

Epoch: 6| Step: 4
Training loss: 2.402460813522339
Validation loss: 2.2962751747459493

Epoch: 6| Step: 5
Training loss: 2.7089147567749023
Validation loss: 2.295390434162591

Epoch: 6| Step: 6
Training loss: 2.797781467437744
Validation loss: 2.3006120881726666

Epoch: 6| Step: 7
Training loss: 2.343440294265747
Validation loss: 2.295990402980517

Epoch: 6| Step: 8
Training loss: 2.5791845321655273
Validation loss: 2.314283242789648

Epoch: 6| Step: 9
Training loss: 2.9630684852600098
Validation loss: 2.3117610254595355

Epoch: 6| Step: 10
Training loss: 2.561842918395996
Validation loss: 2.3176802768502185

Epoch: 6| Step: 11
Training loss: 2.577216625213623
Validation loss: 2.3306828698804303

Epoch: 6| Step: 12
Training loss: 3.0326404571533203
Validation loss: 2.325716846732683

Epoch: 6| Step: 13
Training loss: 2.621222972869873
Validation loss: 2.327274707055861

Epoch: 154| Step: 0
Training loss: 2.9943318367004395
Validation loss: 2.321972377838627

Epoch: 6| Step: 1
Training loss: 1.5054657459259033
Validation loss: 2.305191610449104

Epoch: 6| Step: 2
Training loss: 2.859440803527832
Validation loss: 2.3119769711648264

Epoch: 6| Step: 3
Training loss: 2.498569965362549
Validation loss: 2.301823490409441

Epoch: 6| Step: 4
Training loss: 2.561979293823242
Validation loss: 2.306433211090744

Epoch: 6| Step: 5
Training loss: 1.8710520267486572
Validation loss: 2.3076064355911745

Epoch: 6| Step: 6
Training loss: 2.6693336963653564
Validation loss: 2.3152402088206303

Epoch: 6| Step: 7
Training loss: 2.945106029510498
Validation loss: 2.3204125178757535

Epoch: 6| Step: 8
Training loss: 2.8734703063964844
Validation loss: 2.3256556654489167

Epoch: 6| Step: 9
Training loss: 1.7456536293029785
Validation loss: 2.3306232472901702

Epoch: 6| Step: 10
Training loss: 3.1861088275909424
Validation loss: 2.3396269582932994

Epoch: 6| Step: 11
Training loss: 2.705658197402954
Validation loss: 2.346699660824191

Epoch: 6| Step: 12
Training loss: 2.5857295989990234
Validation loss: 2.3744360349511586

Epoch: 6| Step: 13
Training loss: 2.365373134613037
Validation loss: 2.3779488366137267

Epoch: 155| Step: 0
Training loss: 2.304285764694214
Validation loss: 2.37719270747195

Epoch: 6| Step: 1
Training loss: 2.127471446990967
Validation loss: 2.387818887669553

Epoch: 6| Step: 2
Training loss: 2.830589771270752
Validation loss: 2.3927034383179038

Epoch: 6| Step: 3
Training loss: 2.4267125129699707
Validation loss: 2.3745708593758206

Epoch: 6| Step: 4
Training loss: 1.8125370740890503
Validation loss: 2.351635589394518

Epoch: 6| Step: 5
Training loss: 2.7913293838500977
Validation loss: 2.3274843974780013

Epoch: 6| Step: 6
Training loss: 2.1568381786346436
Validation loss: 2.3130360828932894

Epoch: 6| Step: 7
Training loss: 3.5462284088134766
Validation loss: 2.2887578138741116

Epoch: 6| Step: 8
Training loss: 2.200056314468384
Validation loss: 2.2856117448499127

Epoch: 6| Step: 9
Training loss: 2.503748893737793
Validation loss: 2.287411335975893

Epoch: 6| Step: 10
Training loss: 2.9756436347961426
Validation loss: 2.279736647041895

Epoch: 6| Step: 11
Training loss: 2.2854793071746826
Validation loss: 2.285834934121819

Epoch: 6| Step: 12
Training loss: 2.9009013175964355
Validation loss: 2.278147553884855

Epoch: 6| Step: 13
Training loss: 2.614995241165161
Validation loss: 2.27959382149481

Epoch: 156| Step: 0
Training loss: 2.390744686126709
Validation loss: 2.2775688504660003

Epoch: 6| Step: 1
Training loss: 1.6278144121170044
Validation loss: 2.2812261478875273

Epoch: 6| Step: 2
Training loss: 2.3730106353759766
Validation loss: 2.2769042548312934

Epoch: 6| Step: 3
Training loss: 2.3765690326690674
Validation loss: 2.2827044507508636

Epoch: 6| Step: 4
Training loss: 2.2728257179260254
Validation loss: 2.2800041988331783

Epoch: 6| Step: 5
Training loss: 2.701598644256592
Validation loss: 2.287386261006837

Epoch: 6| Step: 6
Training loss: 2.5378975868225098
Validation loss: 2.282497190660046

Epoch: 6| Step: 7
Training loss: 1.762998342514038
Validation loss: 2.2862450333051783

Epoch: 6| Step: 8
Training loss: 3.061091899871826
Validation loss: 2.2982133511574037

Epoch: 6| Step: 9
Training loss: 3.704146385192871
Validation loss: 2.298093677848898

Epoch: 6| Step: 10
Training loss: 2.192028760910034
Validation loss: 2.299017348597127

Epoch: 6| Step: 11
Training loss: 2.4940805435180664
Validation loss: 2.2983476038902038

Epoch: 6| Step: 12
Training loss: 2.903564453125
Validation loss: 2.2940213090629986

Epoch: 6| Step: 13
Training loss: 3.2772092819213867
Validation loss: 2.3033420296125513

Epoch: 157| Step: 0
Training loss: 3.1175951957702637
Validation loss: 2.3047563440056256

Epoch: 6| Step: 1
Training loss: 2.7981743812561035
Validation loss: 2.308126454712242

Epoch: 6| Step: 2
Training loss: 2.02886700630188
Validation loss: 2.3058693562784502

Epoch: 6| Step: 3
Training loss: 3.2444071769714355
Validation loss: 2.3013110801737797

Epoch: 6| Step: 4
Training loss: 2.1740427017211914
Validation loss: 2.300041539694673

Epoch: 6| Step: 5
Training loss: 1.9667308330535889
Validation loss: 2.3058040321514173

Epoch: 6| Step: 6
Training loss: 2.30712628364563
Validation loss: 2.294256228272633

Epoch: 6| Step: 7
Training loss: 2.355039596557617
Validation loss: 2.2863976827231784

Epoch: 6| Step: 8
Training loss: 2.936199426651001
Validation loss: 2.28224790224465

Epoch: 6| Step: 9
Training loss: 2.1551806926727295
Validation loss: 2.283092439815562

Epoch: 6| Step: 10
Training loss: 2.373037338256836
Validation loss: 2.285988023204188

Epoch: 6| Step: 11
Training loss: 2.784984827041626
Validation loss: 2.2858660631282355

Epoch: 6| Step: 12
Training loss: 2.9574813842773438
Validation loss: 2.2865214296566543

Epoch: 6| Step: 13
Training loss: 1.924002766609192
Validation loss: 2.282661097024077

Epoch: 158| Step: 0
Training loss: 2.976104259490967
Validation loss: 2.2845441987437587

Epoch: 6| Step: 1
Training loss: 2.512817859649658
Validation loss: 2.2888143524046867

Epoch: 6| Step: 2
Training loss: 2.2816102504730225
Validation loss: 2.288221366943852

Epoch: 6| Step: 3
Training loss: 2.306166172027588
Validation loss: 2.2832410873905307

Epoch: 6| Step: 4
Training loss: 2.450435161590576
Validation loss: 2.293180452880039

Epoch: 6| Step: 5
Training loss: 2.1821954250335693
Validation loss: 2.2833480578596874

Epoch: 6| Step: 6
Training loss: 3.154695510864258
Validation loss: 2.290090207130678

Epoch: 6| Step: 7
Training loss: 3.155778169631958
Validation loss: 2.290144487093854

Epoch: 6| Step: 8
Training loss: 2.0369744300842285
Validation loss: 2.292550266429942

Epoch: 6| Step: 9
Training loss: 2.5915751457214355
Validation loss: 2.300717374329926

Epoch: 6| Step: 10
Training loss: 2.7664880752563477
Validation loss: 2.3077285264127996

Epoch: 6| Step: 11
Training loss: 2.42673921585083
Validation loss: 2.300879870691607

Epoch: 6| Step: 12
Training loss: 1.920989990234375
Validation loss: 2.3165486717736847

Epoch: 6| Step: 13
Training loss: 2.812222480773926
Validation loss: 2.321519241538099

Epoch: 159| Step: 0
Training loss: 3.3530430793762207
Validation loss: 2.317044240172191

Epoch: 6| Step: 1
Training loss: 2.1912550926208496
Validation loss: 2.3014343425791752

Epoch: 6| Step: 2
Training loss: 2.034820079803467
Validation loss: 2.2971883025220645

Epoch: 6| Step: 3
Training loss: 2.384469509124756
Validation loss: 2.281528406245734

Epoch: 6| Step: 4
Training loss: 2.3344507217407227
Validation loss: 2.2805165552323863

Epoch: 6| Step: 5
Training loss: 3.1127963066101074
Validation loss: 2.279956472817288

Epoch: 6| Step: 6
Training loss: 2.333617687225342
Validation loss: 2.2804184472689064

Epoch: 6| Step: 7
Training loss: 1.6058235168457031
Validation loss: 2.2872640035485707

Epoch: 6| Step: 8
Training loss: 2.8907876014709473
Validation loss: 2.2949136687863256

Epoch: 6| Step: 9
Training loss: 2.755807399749756
Validation loss: 2.296436107286843

Epoch: 6| Step: 10
Training loss: 2.3202013969421387
Validation loss: 2.2892488920560448

Epoch: 6| Step: 11
Training loss: 2.6475210189819336
Validation loss: 2.298506377845682

Epoch: 6| Step: 12
Training loss: 2.606384515762329
Validation loss: 2.2913828152482227

Epoch: 6| Step: 13
Training loss: 2.787889003753662
Validation loss: 2.301246891739548

Epoch: 160| Step: 0
Training loss: 2.438992738723755
Validation loss: 2.293315023504278

Epoch: 6| Step: 1
Training loss: 3.0017526149749756
Validation loss: 2.291634351976456

Epoch: 6| Step: 2
Training loss: 2.5883841514587402
Validation loss: 2.2868487501657135

Epoch: 6| Step: 3
Training loss: 2.199617385864258
Validation loss: 2.2926006163320234

Epoch: 6| Step: 4
Training loss: 2.4047722816467285
Validation loss: 2.2945634318936254

Epoch: 6| Step: 5
Training loss: 2.0784316062927246
Validation loss: 2.293897767220774

Epoch: 6| Step: 6
Training loss: 2.522946834564209
Validation loss: 2.3040301287046043

Epoch: 6| Step: 7
Training loss: 2.597402572631836
Validation loss: 2.2930121191086306

Epoch: 6| Step: 8
Training loss: 2.599529504776001
Validation loss: 2.301902888923563

Epoch: 6| Step: 9
Training loss: 2.6660680770874023
Validation loss: 2.293747273824548

Epoch: 6| Step: 10
Training loss: 2.594086170196533
Validation loss: 2.294583744900201

Epoch: 6| Step: 11
Training loss: 3.6245484352111816
Validation loss: 2.2982690001046784

Epoch: 6| Step: 12
Training loss: 1.8183586597442627
Validation loss: 2.297015167051746

Epoch: 6| Step: 13
Training loss: 1.737919807434082
Validation loss: 2.2992939564489547

Epoch: 161| Step: 0
Training loss: 3.1197359561920166
Validation loss: 2.2963151495943785

Epoch: 6| Step: 1
Training loss: 2.0443115234375
Validation loss: 2.311465455639747

Epoch: 6| Step: 2
Training loss: 2.2221760749816895
Validation loss: 2.3123520728080504

Epoch: 6| Step: 3
Training loss: 2.4841837882995605
Validation loss: 2.3169710225956415

Epoch: 6| Step: 4
Training loss: 2.8733925819396973
Validation loss: 2.3249760084254767

Epoch: 6| Step: 5
Training loss: 2.901665210723877
Validation loss: 2.323279596144153

Epoch: 6| Step: 6
Training loss: 2.310382604598999
Validation loss: 2.330711841583252

Epoch: 6| Step: 7
Training loss: 2.0349721908569336
Validation loss: 2.3265398907405075

Epoch: 6| Step: 8
Training loss: 2.8912081718444824
Validation loss: 2.3389803030157603

Epoch: 6| Step: 9
Training loss: 2.166261911392212
Validation loss: 2.3495533594521145

Epoch: 6| Step: 10
Training loss: 2.8717401027679443
Validation loss: 2.346234908667944

Epoch: 6| Step: 11
Training loss: 2.310222625732422
Validation loss: 2.33305440666855

Epoch: 6| Step: 12
Training loss: 2.258322238922119
Validation loss: 2.3183189002416467

Epoch: 6| Step: 13
Training loss: 3.0742454528808594
Validation loss: 2.3115460052285144

Epoch: 162| Step: 0
Training loss: 3.1565890312194824
Validation loss: 2.298424341345346

Epoch: 6| Step: 1
Training loss: 2.5683646202087402
Validation loss: 2.284393072128296

Epoch: 6| Step: 2
Training loss: 2.5508735179901123
Validation loss: 2.2878691252841743

Epoch: 6| Step: 3
Training loss: 3.3240561485290527
Validation loss: 2.2809469366586335

Epoch: 6| Step: 4
Training loss: 3.05810546875
Validation loss: 2.28625758488973

Epoch: 6| Step: 5
Training loss: 2.6597299575805664
Validation loss: 2.285912611151254

Epoch: 6| Step: 6
Training loss: 2.196810722351074
Validation loss: 2.2939509371275544

Epoch: 6| Step: 7
Training loss: 2.2320926189422607
Validation loss: 2.306559319137245

Epoch: 6| Step: 8
Training loss: 2.891857385635376
Validation loss: 2.315070726538217

Epoch: 6| Step: 9
Training loss: 1.9400237798690796
Validation loss: 2.3056272537477556

Epoch: 6| Step: 10
Training loss: 1.8914663791656494
Validation loss: 2.325106128569572

Epoch: 6| Step: 11
Training loss: 1.7217299938201904
Validation loss: 2.317834213215818

Epoch: 6| Step: 12
Training loss: 2.924109935760498
Validation loss: 2.3281291172068608

Epoch: 6| Step: 13
Training loss: 2.2917189598083496
Validation loss: 2.3171080261148433

Epoch: 163| Step: 0
Training loss: 2.4044971466064453
Validation loss: 2.2998875725653862

Epoch: 6| Step: 1
Training loss: 1.8681538105010986
Validation loss: 2.2937274389369513

Epoch: 6| Step: 2
Training loss: 1.843639612197876
Validation loss: 2.325589902939335

Epoch: 6| Step: 3
Training loss: 2.7996182441711426
Validation loss: 2.3503971868945706

Epoch: 6| Step: 4
Training loss: 2.134347915649414
Validation loss: 2.3591293775907127

Epoch: 6| Step: 5
Training loss: 2.8068058490753174
Validation loss: 2.3854255650633123

Epoch: 6| Step: 6
Training loss: 2.9035470485687256
Validation loss: 2.3800761468948854

Epoch: 6| Step: 7
Training loss: 2.7750296592712402
Validation loss: 2.387877602731028

Epoch: 6| Step: 8
Training loss: 2.699453353881836
Validation loss: 2.3634829828816075

Epoch: 6| Step: 9
Training loss: 2.6282474994659424
Validation loss: 2.3389441838828464

Epoch: 6| Step: 10
Training loss: 3.0529682636260986
Validation loss: 2.324071684191304

Epoch: 6| Step: 11
Training loss: 2.591146469116211
Validation loss: 2.298175399021436

Epoch: 6| Step: 12
Training loss: 2.1025490760803223
Validation loss: 2.2845600138428392

Epoch: 6| Step: 13
Training loss: 2.9174716472625732
Validation loss: 2.262142409560501

Epoch: 164| Step: 0
Training loss: 2.6762709617614746
Validation loss: 2.253894434180311

Epoch: 6| Step: 1
Training loss: 2.8626046180725098
Validation loss: 2.2577468810542936

Epoch: 6| Step: 2
Training loss: 2.778130054473877
Validation loss: 2.2705118758704073

Epoch: 6| Step: 3
Training loss: 2.3924012184143066
Validation loss: 2.2670005111284155

Epoch: 6| Step: 4
Training loss: 3.282318592071533
Validation loss: 2.262802629060643

Epoch: 6| Step: 5
Training loss: 2.656832695007324
Validation loss: 2.2663980376335884

Epoch: 6| Step: 6
Training loss: 1.9968414306640625
Validation loss: 2.2616927085384244

Epoch: 6| Step: 7
Training loss: 2.0274713039398193
Validation loss: 2.26805632088774

Epoch: 6| Step: 8
Training loss: 2.808511257171631
Validation loss: 2.2724399348740936

Epoch: 6| Step: 9
Training loss: 2.8585968017578125
Validation loss: 2.288397753110496

Epoch: 6| Step: 10
Training loss: 2.023245096206665
Validation loss: 2.3111394579692552

Epoch: 6| Step: 11
Training loss: 1.967961311340332
Validation loss: 2.307649730354227

Epoch: 6| Step: 12
Training loss: 2.813279628753662
Validation loss: 2.3252323878708707

Epoch: 6| Step: 13
Training loss: 1.6124297380447388
Validation loss: 2.3338827035760366

Epoch: 165| Step: 0
Training loss: 1.7548309564590454
Validation loss: 2.3468742396241877

Epoch: 6| Step: 1
Training loss: 2.2296576499938965
Validation loss: 2.3564929193066013

Epoch: 6| Step: 2
Training loss: 2.0602569580078125
Validation loss: 2.352994111276442

Epoch: 6| Step: 3
Training loss: 2.8080437183380127
Validation loss: 2.3534596338067004

Epoch: 6| Step: 4
Training loss: 2.2407522201538086
Validation loss: 2.3307436281634915

Epoch: 6| Step: 5
Training loss: 2.974620819091797
Validation loss: 2.3186504507577546

Epoch: 6| Step: 6
Training loss: 2.6541357040405273
Validation loss: 2.3001463823421027

Epoch: 6| Step: 7
Training loss: 1.92814302444458
Validation loss: 2.288781298104153

Epoch: 6| Step: 8
Training loss: 2.9235117435455322
Validation loss: 2.275412231363276

Epoch: 6| Step: 9
Training loss: 1.935004472732544
Validation loss: 2.277384836186645

Epoch: 6| Step: 10
Training loss: 2.4162018299102783
Validation loss: 2.26278418238445

Epoch: 6| Step: 11
Training loss: 2.6304759979248047
Validation loss: 2.270813936828285

Epoch: 6| Step: 12
Training loss: 3.0399718284606934
Validation loss: 2.2770564748394873

Epoch: 6| Step: 13
Training loss: 4.130853652954102
Validation loss: 2.2864530778700307

Epoch: 166| Step: 0
Training loss: 2.6744823455810547
Validation loss: 2.2777431857201362

Epoch: 6| Step: 1
Training loss: 2.2273025512695312
Validation loss: 2.272944434996574

Epoch: 6| Step: 2
Training loss: 2.571735382080078
Validation loss: 2.267857551574707

Epoch: 6| Step: 3
Training loss: 2.821181297302246
Validation loss: 2.282220655871976

Epoch: 6| Step: 4
Training loss: 2.19827938079834
Validation loss: 2.276604196076752

Epoch: 6| Step: 5
Training loss: 2.569662094116211
Validation loss: 2.285384234561715

Epoch: 6| Step: 6
Training loss: 3.194540023803711
Validation loss: 2.307315654652093

Epoch: 6| Step: 7
Training loss: 1.7860442399978638
Validation loss: 2.3275209191024944

Epoch: 6| Step: 8
Training loss: 2.8412728309631348
Validation loss: 2.3398153115344305

Epoch: 6| Step: 9
Training loss: 2.424329996109009
Validation loss: 2.3448957973910916

Epoch: 6| Step: 10
Training loss: 2.6457433700561523
Validation loss: 2.3352567559929303

Epoch: 6| Step: 11
Training loss: 1.6819545030593872
Validation loss: 2.3483977676719747

Epoch: 6| Step: 12
Training loss: 2.8056955337524414
Validation loss: 2.3334895769755044

Epoch: 6| Step: 13
Training loss: 2.401512622833252
Validation loss: 2.3274658828653316

Epoch: 167| Step: 0
Training loss: 2.229138135910034
Validation loss: 2.316918683308427

Epoch: 6| Step: 1
Training loss: 2.9241623878479004
Validation loss: 2.3173012220731346

Epoch: 6| Step: 2
Training loss: 2.4352662563323975
Validation loss: 2.2988421365778935

Epoch: 6| Step: 3
Training loss: 2.4443607330322266
Validation loss: 2.294333811729185

Epoch: 6| Step: 4
Training loss: 2.0862679481506348
Validation loss: 2.2917603164590816

Epoch: 6| Step: 5
Training loss: 2.384377956390381
Validation loss: 2.2988450783555225

Epoch: 6| Step: 6
Training loss: 2.1122798919677734
Validation loss: 2.295859049725276

Epoch: 6| Step: 7
Training loss: 2.2184031009674072
Validation loss: 2.290169031389298

Epoch: 6| Step: 8
Training loss: 2.8435511589050293
Validation loss: 2.295679762799253

Epoch: 6| Step: 9
Training loss: 2.640890121459961
Validation loss: 2.290310364897533

Epoch: 6| Step: 10
Training loss: 2.729889154434204
Validation loss: 2.2890546629505772

Epoch: 6| Step: 11
Training loss: 2.342381477355957
Validation loss: 2.2824429722242456

Epoch: 6| Step: 12
Training loss: 2.5702805519104004
Validation loss: 2.2839954694112143

Epoch: 6| Step: 13
Training loss: 3.2611472606658936
Validation loss: 2.2742360663670365

Epoch: 168| Step: 0
Training loss: 2.1243810653686523
Validation loss: 2.2918297013928814

Epoch: 6| Step: 1
Training loss: 3.2019400596618652
Validation loss: 2.3054538978043424

Epoch: 6| Step: 2
Training loss: 2.5748934745788574
Validation loss: 2.3261809656696935

Epoch: 6| Step: 3
Training loss: 3.102642059326172
Validation loss: 2.354499678457937

Epoch: 6| Step: 4
Training loss: 1.9533385038375854
Validation loss: 2.3862749043331353

Epoch: 6| Step: 5
Training loss: 2.7241430282592773
Validation loss: 2.409925373651648

Epoch: 6| Step: 6
Training loss: 2.571852207183838
Validation loss: 2.4424321702731553

Epoch: 6| Step: 7
Training loss: 2.457641839981079
Validation loss: 2.4458839829250048

Epoch: 6| Step: 8
Training loss: 1.7806787490844727
Validation loss: 2.4256130354378813

Epoch: 6| Step: 9
Training loss: 2.5277414321899414
Validation loss: 2.442274631992463

Epoch: 6| Step: 10
Training loss: 2.2965612411499023
Validation loss: 2.4152323840766825

Epoch: 6| Step: 11
Training loss: 2.855128288269043
Validation loss: 2.4216042282760784

Epoch: 6| Step: 12
Training loss: 2.7130327224731445
Validation loss: 2.3700434802680888

Epoch: 6| Step: 13
Training loss: 3.105536460876465
Validation loss: 2.341863721929571

Epoch: 169| Step: 0
Training loss: 2.3856468200683594
Validation loss: 2.3082748510504283

Epoch: 6| Step: 1
Training loss: 2.530451774597168
Validation loss: 2.2838203073829733

Epoch: 6| Step: 2
Training loss: 2.396008014678955
Validation loss: 2.279067554781514

Epoch: 6| Step: 3
Training loss: 2.59041690826416
Validation loss: 2.2911292916984967

Epoch: 6| Step: 4
Training loss: 3.1417949199676514
Validation loss: 2.2971190303884526

Epoch: 6| Step: 5
Training loss: 2.8683671951293945
Validation loss: 2.3026447808870705

Epoch: 6| Step: 6
Training loss: 2.244938850402832
Validation loss: 2.318024758369692

Epoch: 6| Step: 7
Training loss: 2.341780185699463
Validation loss: 2.327918544892342

Epoch: 6| Step: 8
Training loss: 2.5459673404693604
Validation loss: 2.3453895276592625

Epoch: 6| Step: 9
Training loss: 2.823516845703125
Validation loss: 2.336878143331056

Epoch: 6| Step: 10
Training loss: 2.1700806617736816
Validation loss: 2.3030156217595583

Epoch: 6| Step: 11
Training loss: 2.8000903129577637
Validation loss: 2.297562268472487

Epoch: 6| Step: 12
Training loss: 2.584561824798584
Validation loss: 2.291705564786029

Epoch: 6| Step: 13
Training loss: 1.7680844068527222
Validation loss: 2.2950106051660355

Epoch: 170| Step: 0
Training loss: 2.2890210151672363
Validation loss: 2.2895595617191766

Epoch: 6| Step: 1
Training loss: 2.186645984649658
Validation loss: 2.2866503423260105

Epoch: 6| Step: 2
Training loss: 2.2535338401794434
Validation loss: 2.289288551576676

Epoch: 6| Step: 3
Training loss: 2.7571120262145996
Validation loss: 2.2998628001059256

Epoch: 6| Step: 4
Training loss: 2.419236660003662
Validation loss: 2.3197612890633206

Epoch: 6| Step: 5
Training loss: 2.9377899169921875
Validation loss: 2.3431862733697377

Epoch: 6| Step: 6
Training loss: 2.574660301208496
Validation loss: 2.3660221663854455

Epoch: 6| Step: 7
Training loss: 2.322030782699585
Validation loss: 2.3775494226845364

Epoch: 6| Step: 8
Training loss: 2.5910284519195557
Validation loss: 2.371497372145294

Epoch: 6| Step: 9
Training loss: 1.8958706855773926
Validation loss: 2.3303322048597437

Epoch: 6| Step: 10
Training loss: 2.330446720123291
Validation loss: 2.31170593794956

Epoch: 6| Step: 11
Training loss: 3.826521873474121
Validation loss: 2.285906009776618

Epoch: 6| Step: 12
Training loss: 2.3235840797424316
Validation loss: 2.290610441597559

Epoch: 6| Step: 13
Training loss: 2.3341054916381836
Validation loss: 2.278662532888433

Epoch: 171| Step: 0
Training loss: 3.028524398803711
Validation loss: 2.282484121220086

Epoch: 6| Step: 1
Training loss: 2.6723034381866455
Validation loss: 2.2941738559353735

Epoch: 6| Step: 2
Training loss: 1.8084595203399658
Validation loss: 2.2968619638873684

Epoch: 6| Step: 3
Training loss: 2.2957892417907715
Validation loss: 2.2973373192612843

Epoch: 6| Step: 4
Training loss: 2.3584723472595215
Validation loss: 2.305439546544065

Epoch: 6| Step: 5
Training loss: 2.8392887115478516
Validation loss: 2.316503337634507

Epoch: 6| Step: 6
Training loss: 2.6264264583587646
Validation loss: 2.3261380400708926

Epoch: 6| Step: 7
Training loss: 2.6187241077423096
Validation loss: 2.326587265537631

Epoch: 6| Step: 8
Training loss: 2.2443630695343018
Validation loss: 2.3143302753407466

Epoch: 6| Step: 9
Training loss: 2.5141077041625977
Validation loss: 2.325355757949173

Epoch: 6| Step: 10
Training loss: 2.006756067276001
Validation loss: 2.3313808184798046

Epoch: 6| Step: 11
Training loss: 2.832235336303711
Validation loss: 2.3417800959720405

Epoch: 6| Step: 12
Training loss: 3.123077869415283
Validation loss: 2.341232907387518

Epoch: 6| Step: 13
Training loss: 1.341286540031433
Validation loss: 2.330006673771848

Epoch: 172| Step: 0
Training loss: 2.2908196449279785
Validation loss: 2.3159859282996065

Epoch: 6| Step: 1
Training loss: 2.202810287475586
Validation loss: 2.3176558351003997

Epoch: 6| Step: 2
Training loss: 2.0996739864349365
Validation loss: 2.310362667165777

Epoch: 6| Step: 3
Training loss: 2.317803382873535
Validation loss: 2.3138547482029086

Epoch: 6| Step: 4
Training loss: 2.1245012283325195
Validation loss: 2.3042203021305863

Epoch: 6| Step: 5
Training loss: 2.593291997909546
Validation loss: 2.2891225084181754

Epoch: 6| Step: 6
Training loss: 2.8784990310668945
Validation loss: 2.2823768328594904

Epoch: 6| Step: 7
Training loss: 3.19407057762146
Validation loss: 2.278704571467574

Epoch: 6| Step: 8
Training loss: 2.1530818939208984
Validation loss: 2.268836837942882

Epoch: 6| Step: 9
Training loss: 2.4613027572631836
Validation loss: 2.2654392437268327

Epoch: 6| Step: 10
Training loss: 2.605217933654785
Validation loss: 2.254593833800285

Epoch: 6| Step: 11
Training loss: 2.9060332775115967
Validation loss: 2.2552493669653453

Epoch: 6| Step: 12
Training loss: 2.4410693645477295
Validation loss: 2.2597282676286596

Epoch: 6| Step: 13
Training loss: 2.6579463481903076
Validation loss: 2.261257222903672

Epoch: 173| Step: 0
Training loss: 3.0536208152770996
Validation loss: 2.276341426757074

Epoch: 6| Step: 1
Training loss: 2.7059829235076904
Validation loss: 2.2776808918163343

Epoch: 6| Step: 2
Training loss: 1.9745831489562988
Validation loss: 2.289621348022133

Epoch: 6| Step: 3
Training loss: 1.9979774951934814
Validation loss: 2.2944808916379045

Epoch: 6| Step: 4
Training loss: 2.747258186340332
Validation loss: 2.3063055315325336

Epoch: 6| Step: 5
Training loss: 2.7403738498687744
Validation loss: 2.3292024238135225

Epoch: 6| Step: 6
Training loss: 3.1189184188842773
Validation loss: 2.328578531101186

Epoch: 6| Step: 7
Training loss: 2.3386096954345703
Validation loss: 2.351189581296777

Epoch: 6| Step: 8
Training loss: 2.275026321411133
Validation loss: 2.3628746771043345

Epoch: 6| Step: 9
Training loss: 2.5589852333068848
Validation loss: 2.372924297086654

Epoch: 6| Step: 10
Training loss: 2.4122138023376465
Validation loss: 2.366288087701285

Epoch: 6| Step: 11
Training loss: 2.418245792388916
Validation loss: 2.3691005014604136

Epoch: 6| Step: 12
Training loss: 2.861372947692871
Validation loss: 2.3581867730745705

Epoch: 6| Step: 13
Training loss: 1.5510611534118652
Validation loss: 2.3499172272220736

Epoch: 174| Step: 0
Training loss: 3.175548553466797
Validation loss: 2.34074148824138

Epoch: 6| Step: 1
Training loss: 2.2977194786071777
Validation loss: 2.3340832507738503

Epoch: 6| Step: 2
Training loss: 1.625437617301941
Validation loss: 2.331047660561018

Epoch: 6| Step: 3
Training loss: 2.612947702407837
Validation loss: 2.3169842304721957

Epoch: 6| Step: 4
Training loss: 2.695709228515625
Validation loss: 2.3033063488621868

Epoch: 6| Step: 5
Training loss: 3.020681381225586
Validation loss: 2.297592527122908

Epoch: 6| Step: 6
Training loss: 2.3746402263641357
Validation loss: 2.309072458615867

Epoch: 6| Step: 7
Training loss: 2.4560208320617676
Validation loss: 2.298292954762777

Epoch: 6| Step: 8
Training loss: 2.35568904876709
Validation loss: 2.3002354534723426

Epoch: 6| Step: 9
Training loss: 2.6113176345825195
Validation loss: 2.297843740832421

Epoch: 6| Step: 10
Training loss: 3.067404270172119
Validation loss: 2.2967434467807895

Epoch: 6| Step: 11
Training loss: 1.8213576078414917
Validation loss: 2.2942292677458895

Epoch: 6| Step: 12
Training loss: 2.2502694129943848
Validation loss: 2.2877400998146302

Epoch: 6| Step: 13
Training loss: 2.6267824172973633
Validation loss: 2.2713112344024

Epoch: 175| Step: 0
Training loss: 2.266268730163574
Validation loss: 2.2772631799021075

Epoch: 6| Step: 1
Training loss: 2.9445157051086426
Validation loss: 2.2838095131740777

Epoch: 6| Step: 2
Training loss: 1.9511743783950806
Validation loss: 2.2838766087767897

Epoch: 6| Step: 3
Training loss: 3.1187806129455566
Validation loss: 2.2853429471292803

Epoch: 6| Step: 4
Training loss: 2.9899086952209473
Validation loss: 2.288629129368772

Epoch: 6| Step: 5
Training loss: 2.170550584793091
Validation loss: 2.2978480656941733

Epoch: 6| Step: 6
Training loss: 2.058866500854492
Validation loss: 2.302546138404518

Epoch: 6| Step: 7
Training loss: 2.598325252532959
Validation loss: 2.3026614342966387

Epoch: 6| Step: 8
Training loss: 3.07580304145813
Validation loss: 2.3119055737731276

Epoch: 6| Step: 9
Training loss: 2.2925965785980225
Validation loss: 2.309268331014982

Epoch: 6| Step: 10
Training loss: 2.348112106323242
Validation loss: 2.3096235721342024

Epoch: 6| Step: 11
Training loss: 2.6430318355560303
Validation loss: 2.303840675661641

Epoch: 6| Step: 12
Training loss: 1.5981541872024536
Validation loss: 2.2954687700476697

Epoch: 6| Step: 13
Training loss: 2.397430181503296
Validation loss: 2.2928633100243023

Epoch: 176| Step: 0
Training loss: 2.3576748371124268
Validation loss: 2.299979414991153

Epoch: 6| Step: 1
Training loss: 2.790672779083252
Validation loss: 2.2964263500705844

Epoch: 6| Step: 2
Training loss: 3.121685028076172
Validation loss: 2.310548859257852

Epoch: 6| Step: 3
Training loss: 3.1344523429870605
Validation loss: 2.3140274247815533

Epoch: 6| Step: 4
Training loss: 2.754725456237793
Validation loss: 2.304619432777487

Epoch: 6| Step: 5
Training loss: 1.9694669246673584
Validation loss: 2.297884791128097

Epoch: 6| Step: 6
Training loss: 2.3538379669189453
Validation loss: 2.304013257385582

Epoch: 6| Step: 7
Training loss: 1.9228544235229492
Validation loss: 2.2963830655620945

Epoch: 6| Step: 8
Training loss: 2.08427095413208
Validation loss: 2.2936802371855705

Epoch: 6| Step: 9
Training loss: 2.593715190887451
Validation loss: 2.2928290418399278

Epoch: 6| Step: 10
Training loss: 2.0669941902160645
Validation loss: 2.302041035826488

Epoch: 6| Step: 11
Training loss: 2.62422513961792
Validation loss: 2.2989642927723546

Epoch: 6| Step: 12
Training loss: 2.5048553943634033
Validation loss: 2.295852176604732

Epoch: 6| Step: 13
Training loss: 2.171915054321289
Validation loss: 2.2837055421644643

Epoch: 177| Step: 0
Training loss: 2.4649524688720703
Validation loss: 2.2826216067037275

Epoch: 6| Step: 1
Training loss: 2.1553285121917725
Validation loss: 2.273361588037142

Epoch: 6| Step: 2
Training loss: 2.5960988998413086
Validation loss: 2.2867760324990876

Epoch: 6| Step: 3
Training loss: 2.9241089820861816
Validation loss: 2.2624780080651723

Epoch: 6| Step: 4
Training loss: 1.8094316720962524
Validation loss: 2.2597527414239864

Epoch: 6| Step: 5
Training loss: 2.86970853805542
Validation loss: 2.2536583844051568

Epoch: 6| Step: 6
Training loss: 2.067192554473877
Validation loss: 2.249536839864587

Epoch: 6| Step: 7
Training loss: 2.116793394088745
Validation loss: 2.256627710916663

Epoch: 6| Step: 8
Training loss: 3.00874662399292
Validation loss: 2.2682962763694023

Epoch: 6| Step: 9
Training loss: 1.6756458282470703
Validation loss: 2.2740898491233907

Epoch: 6| Step: 10
Training loss: 2.764976978302002
Validation loss: 2.277317390646986

Epoch: 6| Step: 11
Training loss: 2.29445219039917
Validation loss: 2.2779994574926232

Epoch: 6| Step: 12
Training loss: 3.0663862228393555
Validation loss: 2.278859917835523

Epoch: 6| Step: 13
Training loss: 2.81069278717041
Validation loss: 2.292434725710141

Epoch: 178| Step: 0
Training loss: 2.6427807807922363
Validation loss: 2.3079605922904065

Epoch: 6| Step: 1
Training loss: 3.0964903831481934
Validation loss: 2.3057659287606516

Epoch: 6| Step: 2
Training loss: 3.3708534240722656
Validation loss: 2.312127390215474

Epoch: 6| Step: 3
Training loss: 2.4420673847198486
Validation loss: 2.3073830655826035

Epoch: 6| Step: 4
Training loss: 2.178581476211548
Validation loss: 2.310412306939402

Epoch: 6| Step: 5
Training loss: 1.9642753601074219
Validation loss: 2.3060066392344813

Epoch: 6| Step: 6
Training loss: 1.9144935607910156
Validation loss: 2.3046898252220562

Epoch: 6| Step: 7
Training loss: 1.9740819931030273
Validation loss: 2.3009297258110455

Epoch: 6| Step: 8
Training loss: 2.7512106895446777
Validation loss: 2.2874007763401156

Epoch: 6| Step: 9
Training loss: 1.8417552709579468
Validation loss: 2.2867039429244174

Epoch: 6| Step: 10
Training loss: 2.6880946159362793
Validation loss: 2.2854775562081286

Epoch: 6| Step: 11
Training loss: 2.8379223346710205
Validation loss: 2.288717694179986

Epoch: 6| Step: 12
Training loss: 2.006864070892334
Validation loss: 2.2856763883303572

Epoch: 6| Step: 13
Training loss: 2.6535723209381104
Validation loss: 2.293757007968041

Epoch: 179| Step: 0
Training loss: 1.945385456085205
Validation loss: 2.3114633149998163

Epoch: 6| Step: 1
Training loss: 1.8595590591430664
Validation loss: 2.3154293183357484

Epoch: 6| Step: 2
Training loss: 2.815591335296631
Validation loss: 2.3066585909935737

Epoch: 6| Step: 3
Training loss: 2.5738091468811035
Validation loss: 2.3158256443597938

Epoch: 6| Step: 4
Training loss: 2.4340767860412598
Validation loss: 2.2993827404514438

Epoch: 6| Step: 5
Training loss: 1.701669692993164
Validation loss: 2.3008759662669194

Epoch: 6| Step: 6
Training loss: 3.204498052597046
Validation loss: 2.3061253704050535

Epoch: 6| Step: 7
Training loss: 2.642812728881836
Validation loss: 2.3017344859338578

Epoch: 6| Step: 8
Training loss: 2.655076026916504
Validation loss: 2.297735780797979

Epoch: 6| Step: 9
Training loss: 1.9343022108078003
Validation loss: 2.2923823248955513

Epoch: 6| Step: 10
Training loss: 2.742344379425049
Validation loss: 2.2928849368967037

Epoch: 6| Step: 11
Training loss: 2.7656970024108887
Validation loss: 2.306753009878179

Epoch: 6| Step: 12
Training loss: 2.664456367492676
Validation loss: 2.3091492678529475

Epoch: 6| Step: 13
Training loss: 2.547602415084839
Validation loss: 2.303092569433233

Epoch: 180| Step: 0
Training loss: 2.6987624168395996
Validation loss: 2.291964446344683

Epoch: 6| Step: 1
Training loss: 2.0931270122528076
Validation loss: 2.2847738522355274

Epoch: 6| Step: 2
Training loss: 2.6035637855529785
Validation loss: 2.283513699808428

Epoch: 6| Step: 3
Training loss: 2.821136951446533
Validation loss: 2.272853917973016

Epoch: 6| Step: 4
Training loss: 2.120929718017578
Validation loss: 2.2665288384242723

Epoch: 6| Step: 5
Training loss: 2.3138926029205322
Validation loss: 2.253897733585809

Epoch: 6| Step: 6
Training loss: 2.4778852462768555
Validation loss: 2.2552814714370237

Epoch: 6| Step: 7
Training loss: 2.348862409591675
Validation loss: 2.2581712712523756

Epoch: 6| Step: 8
Training loss: 2.5256638526916504
Validation loss: 2.2702973376038256

Epoch: 6| Step: 9
Training loss: 1.9787654876708984
Validation loss: 2.282183083154822

Epoch: 6| Step: 10
Training loss: 2.3626954555511475
Validation loss: 2.2710137854340258

Epoch: 6| Step: 11
Training loss: 2.7518458366394043
Validation loss: 2.2753290078973256

Epoch: 6| Step: 12
Training loss: 2.2202138900756836
Validation loss: 2.2783451798141643

Epoch: 6| Step: 13
Training loss: 3.2428669929504395
Validation loss: 2.29521603738108

Epoch: 181| Step: 0
Training loss: 2.9232563972473145
Validation loss: 2.3089580176978983

Epoch: 6| Step: 1
Training loss: 2.4521255493164062
Validation loss: 2.3336776328343216

Epoch: 6| Step: 2
Training loss: 1.882596492767334
Validation loss: 2.3262126522679485

Epoch: 6| Step: 3
Training loss: 2.7884726524353027
Validation loss: 2.3285795065664474

Epoch: 6| Step: 4
Training loss: 3.126406192779541
Validation loss: 2.3181489129220285

Epoch: 6| Step: 5
Training loss: 2.134174346923828
Validation loss: 2.318247789977699

Epoch: 6| Step: 6
Training loss: 2.1396632194519043
Validation loss: 2.3112770395894207

Epoch: 6| Step: 7
Training loss: 2.1041736602783203
Validation loss: 2.3095767062197448

Epoch: 6| Step: 8
Training loss: 1.9705994129180908
Validation loss: 2.311419215253604

Epoch: 6| Step: 9
Training loss: 2.475599765777588
Validation loss: 2.3239896259000226

Epoch: 6| Step: 10
Training loss: 2.9323525428771973
Validation loss: 2.3222378915356052

Epoch: 6| Step: 11
Training loss: 2.1846699714660645
Validation loss: 2.309705780398461

Epoch: 6| Step: 12
Training loss: 2.370802879333496
Validation loss: 2.3138965611816733

Epoch: 6| Step: 13
Training loss: 3.2707700729370117
Validation loss: 2.301340277476977

Epoch: 182| Step: 0
Training loss: 2.687753677368164
Validation loss: 2.2941515804618917

Epoch: 6| Step: 1
Training loss: 2.046492099761963
Validation loss: 2.284567786801246

Epoch: 6| Step: 2
Training loss: 2.4916820526123047
Validation loss: 2.292067112461213

Epoch: 6| Step: 3
Training loss: 1.9400888681411743
Validation loss: 2.277522930534937

Epoch: 6| Step: 4
Training loss: 2.653895854949951
Validation loss: 2.280842652884863

Epoch: 6| Step: 5
Training loss: 2.516725778579712
Validation loss: 2.269383445862801

Epoch: 6| Step: 6
Training loss: 1.9777188301086426
Validation loss: 2.264895041783651

Epoch: 6| Step: 7
Training loss: 2.505290985107422
Validation loss: 2.262280783345622

Epoch: 6| Step: 8
Training loss: 2.6563539505004883
Validation loss: 2.256767375494844

Epoch: 6| Step: 9
Training loss: 2.3762359619140625
Validation loss: 2.2481119735266573

Epoch: 6| Step: 10
Training loss: 2.8642635345458984
Validation loss: 2.25001883763139

Epoch: 6| Step: 11
Training loss: 2.5132339000701904
Validation loss: 2.256590404818135

Epoch: 6| Step: 12
Training loss: 2.5991950035095215
Validation loss: 2.2654822000893216

Epoch: 6| Step: 13
Training loss: 2.1931326389312744
Validation loss: 2.265615781148275

Epoch: 183| Step: 0
Training loss: 2.8197519779205322
Validation loss: 2.2700608571370444

Epoch: 6| Step: 1
Training loss: 1.8803074359893799
Validation loss: 2.270079525568152

Epoch: 6| Step: 2
Training loss: 3.056954860687256
Validation loss: 2.2853546450215

Epoch: 6| Step: 3
Training loss: 2.053253412246704
Validation loss: 2.290999634291536

Epoch: 6| Step: 4
Training loss: 2.48506498336792
Validation loss: 2.3021745656126287

Epoch: 6| Step: 5
Training loss: 2.245462656021118
Validation loss: 2.292922548068467

Epoch: 6| Step: 6
Training loss: 2.8426496982574463
Validation loss: 2.2901976108551025

Epoch: 6| Step: 7
Training loss: 2.6474857330322266
Validation loss: 2.2906431792884745

Epoch: 6| Step: 8
Training loss: 2.962167263031006
Validation loss: 2.278972469350343

Epoch: 6| Step: 9
Training loss: 2.296058177947998
Validation loss: 2.2707539489192348

Epoch: 6| Step: 10
Training loss: 2.105219841003418
Validation loss: 2.276227294757802

Epoch: 6| Step: 11
Training loss: 1.9350636005401611
Validation loss: 2.271875819852275

Epoch: 6| Step: 12
Training loss: 2.5163369178771973
Validation loss: 2.2729875400502193

Epoch: 6| Step: 13
Training loss: 2.4886293411254883
Validation loss: 2.2692680487068753

Epoch: 184| Step: 0
Training loss: 2.8646697998046875
Validation loss: 2.26990181656294

Epoch: 6| Step: 1
Training loss: 2.966555595397949
Validation loss: 2.284031844908191

Epoch: 6| Step: 2
Training loss: 2.485208511352539
Validation loss: 2.289069457720685

Epoch: 6| Step: 3
Training loss: 2.652233839035034
Validation loss: 2.2918589243324856

Epoch: 6| Step: 4
Training loss: 2.328601121902466
Validation loss: 2.299539468621695

Epoch: 6| Step: 5
Training loss: 3.013582944869995
Validation loss: 2.2779052795902377

Epoch: 6| Step: 6
Training loss: 2.0455610752105713
Validation loss: 2.2816304404248475

Epoch: 6| Step: 7
Training loss: 2.08270525932312
Validation loss: 2.2576942674575315

Epoch: 6| Step: 8
Training loss: 2.2380475997924805
Validation loss: 2.2454065122912006

Epoch: 6| Step: 9
Training loss: 1.588361382484436
Validation loss: 2.24200697611737

Epoch: 6| Step: 10
Training loss: 2.1521215438842773
Validation loss: 2.256177854794328

Epoch: 6| Step: 11
Training loss: 3.2466912269592285
Validation loss: 2.244887308407855

Epoch: 6| Step: 12
Training loss: 2.203666925430298
Validation loss: 2.246222206341323

Epoch: 6| Step: 13
Training loss: 2.406802177429199
Validation loss: 2.246070884889172

Epoch: 185| Step: 0
Training loss: 2.33678936958313
Validation loss: 2.255571126937866

Epoch: 6| Step: 1
Training loss: 2.422610282897949
Validation loss: 2.24985142677061

Epoch: 6| Step: 2
Training loss: 1.8281571865081787
Validation loss: 2.255144150026383

Epoch: 6| Step: 3
Training loss: 2.349985122680664
Validation loss: 2.263241365391721

Epoch: 6| Step: 4
Training loss: 2.4394829273223877
Validation loss: 2.2969216890232538

Epoch: 6| Step: 5
Training loss: 2.9409687519073486
Validation loss: 2.3035007817770845

Epoch: 6| Step: 6
Training loss: 2.8814802169799805
Validation loss: 2.327408572678925

Epoch: 6| Step: 7
Training loss: 2.7086544036865234
Validation loss: 2.3266632813279347

Epoch: 6| Step: 8
Training loss: 1.7945808172225952
Validation loss: 2.3011899891720025

Epoch: 6| Step: 9
Training loss: 2.991379976272583
Validation loss: 2.270676851272583

Epoch: 6| Step: 10
Training loss: 1.973022699356079
Validation loss: 2.2609613146833194

Epoch: 6| Step: 11
Training loss: 2.2439606189727783
Validation loss: 2.2481845271202827

Epoch: 6| Step: 12
Training loss: 2.6559436321258545
Validation loss: 2.2573635757610364

Epoch: 6| Step: 13
Training loss: 2.9352879524230957
Validation loss: 2.2805788158088602

Epoch: 186| Step: 0
Training loss: 2.9977011680603027
Validation loss: 2.3190823114046486

Epoch: 6| Step: 1
Training loss: 1.7103197574615479
Validation loss: 2.370033179560015

Epoch: 6| Step: 2
Training loss: 2.8176751136779785
Validation loss: 2.356435578356507

Epoch: 6| Step: 3
Training loss: 2.4872593879699707
Validation loss: 2.3278196165638585

Epoch: 6| Step: 4
Training loss: 1.5657249689102173
Validation loss: 2.273316808926162

Epoch: 6| Step: 5
Training loss: 2.441164493560791
Validation loss: 2.2442481722883

Epoch: 6| Step: 6
Training loss: 2.8348209857940674
Validation loss: 2.224646940026232

Epoch: 6| Step: 7
Training loss: 2.6089203357696533
Validation loss: 2.205404730253322

Epoch: 6| Step: 8
Training loss: 2.911785125732422
Validation loss: 2.2149149987005416

Epoch: 6| Step: 9
Training loss: 3.026580572128296
Validation loss: 2.212817265141395

Epoch: 6| Step: 10
Training loss: 2.315556049346924
Validation loss: 2.2075600918903144

Epoch: 6| Step: 11
Training loss: 2.5957493782043457
Validation loss: 2.205298213548558

Epoch: 6| Step: 12
Training loss: 2.0867888927459717
Validation loss: 2.2034523128181376

Epoch: 6| Step: 13
Training loss: 2.2846858501434326
Validation loss: 2.205977298880136

Epoch: 187| Step: 0
Training loss: 2.612945318222046
Validation loss: 2.2069309603783394

Epoch: 6| Step: 1
Training loss: 2.4632623195648193
Validation loss: 2.203503885576802

Epoch: 6| Step: 2
Training loss: 2.19464111328125
Validation loss: 2.2040385687223045

Epoch: 6| Step: 3
Training loss: 2.4882235527038574
Validation loss: 2.206196359408799

Epoch: 6| Step: 4
Training loss: 2.082101345062256
Validation loss: 2.219261925707581

Epoch: 6| Step: 5
Training loss: 2.64337158203125
Validation loss: 2.2493086220115743

Epoch: 6| Step: 6
Training loss: 2.574531078338623
Validation loss: 2.2881644297671575

Epoch: 6| Step: 7
Training loss: 2.7051243782043457
Validation loss: 2.3039274472062305

Epoch: 6| Step: 8
Training loss: 2.541285514831543
Validation loss: 2.2861925760904946

Epoch: 6| Step: 9
Training loss: 2.724400281906128
Validation loss: 2.2727789853208806

Epoch: 6| Step: 10
Training loss: 2.029717445373535
Validation loss: 2.2543497803390666

Epoch: 6| Step: 11
Training loss: 2.391969919204712
Validation loss: 2.2442667997011574

Epoch: 6| Step: 12
Training loss: 2.403404474258423
Validation loss: 2.2342316207065376

Epoch: 6| Step: 13
Training loss: 2.867589235305786
Validation loss: 2.2394707946367163

Epoch: 188| Step: 0
Training loss: 2.526289939880371
Validation loss: 2.227017689776677

Epoch: 6| Step: 1
Training loss: 2.5722084045410156
Validation loss: 2.212298716268232

Epoch: 6| Step: 2
Training loss: 1.8758379220962524
Validation loss: 2.2071727501448763

Epoch: 6| Step: 3
Training loss: 2.3661129474639893
Validation loss: 2.215938170750936

Epoch: 6| Step: 4
Training loss: 2.6456029415130615
Validation loss: 2.2074654640689975

Epoch: 6| Step: 5
Training loss: 2.673020124435425
Validation loss: 2.2042321953722226

Epoch: 6| Step: 6
Training loss: 2.8890106678009033
Validation loss: 2.2158199279539046

Epoch: 6| Step: 7
Training loss: 2.044527530670166
Validation loss: 2.210061437340193

Epoch: 6| Step: 8
Training loss: 1.9787650108337402
Validation loss: 2.2094896454964914

Epoch: 6| Step: 9
Training loss: 2.8922410011291504
Validation loss: 2.218727098998203

Epoch: 6| Step: 10
Training loss: 2.591226100921631
Validation loss: 2.226773205623832

Epoch: 6| Step: 11
Training loss: 2.609154224395752
Validation loss: 2.225200938922103

Epoch: 6| Step: 12
Training loss: 2.4012370109558105
Validation loss: 2.233201285844208

Epoch: 6| Step: 13
Training loss: 2.0587966442108154
Validation loss: 2.2451910485503492

Epoch: 189| Step: 0
Training loss: 1.8946195840835571
Validation loss: 2.2538077369812997

Epoch: 6| Step: 1
Training loss: 2.033858060836792
Validation loss: 2.2580524875271704

Epoch: 6| Step: 2
Training loss: 2.3826842308044434
Validation loss: 2.2720447099337013

Epoch: 6| Step: 3
Training loss: 2.0667314529418945
Validation loss: 2.280363746868667

Epoch: 6| Step: 4
Training loss: 2.3732190132141113
Validation loss: 2.271086287754838

Epoch: 6| Step: 5
Training loss: 2.583221435546875
Validation loss: 2.276164108707059

Epoch: 6| Step: 6
Training loss: 2.4307687282562256
Validation loss: 2.2588216771361647

Epoch: 6| Step: 7
Training loss: 2.382129192352295
Validation loss: 2.2649978565913376

Epoch: 6| Step: 8
Training loss: 2.8736443519592285
Validation loss: 2.2433125908656786

Epoch: 6| Step: 9
Training loss: 3.195523977279663
Validation loss: 2.2421896714036182

Epoch: 6| Step: 10
Training loss: 2.6163151264190674
Validation loss: 2.234382998558783

Epoch: 6| Step: 11
Training loss: 2.290602445602417
Validation loss: 2.248605079548333

Epoch: 6| Step: 12
Training loss: 2.075761556625366
Validation loss: 2.2579565458400275

Epoch: 6| Step: 13
Training loss: 3.2765684127807617
Validation loss: 2.2525798172079106

Epoch: 190| Step: 0
Training loss: 1.6186549663543701
Validation loss: 2.249482247137254

Epoch: 6| Step: 1
Training loss: 2.030008316040039
Validation loss: 2.2459319304394465

Epoch: 6| Step: 2
Training loss: 2.8400216102600098
Validation loss: 2.24990411214931

Epoch: 6| Step: 3
Training loss: 2.2091169357299805
Validation loss: 2.250727443284886

Epoch: 6| Step: 4
Training loss: 2.3646531105041504
Validation loss: 2.2533898045939784

Epoch: 6| Step: 5
Training loss: 3.168262004852295
Validation loss: 2.254865784798899

Epoch: 6| Step: 6
Training loss: 3.1485981941223145
Validation loss: 2.270152638035436

Epoch: 6| Step: 7
Training loss: 3.0867624282836914
Validation loss: 2.2609735304309475

Epoch: 6| Step: 8
Training loss: 2.547036647796631
Validation loss: 2.2624302166764454

Epoch: 6| Step: 9
Training loss: 2.369144916534424
Validation loss: 2.2509991122830297

Epoch: 6| Step: 10
Training loss: 2.2803096771240234
Validation loss: 2.248722481471236

Epoch: 6| Step: 11
Training loss: 1.5166821479797363
Validation loss: 2.246022403881114

Epoch: 6| Step: 12
Training loss: 2.2970211505889893
Validation loss: 2.2414219199970202

Epoch: 6| Step: 13
Training loss: 2.4348039627075195
Validation loss: 2.2404364180821243

Epoch: 191| Step: 0
Training loss: 1.7290910482406616
Validation loss: 2.2391000998917447

Epoch: 6| Step: 1
Training loss: 1.7222023010253906
Validation loss: 2.2347048380041636

Epoch: 6| Step: 2
Training loss: 2.510854482650757
Validation loss: 2.237710950195148

Epoch: 6| Step: 3
Training loss: 2.0231690406799316
Validation loss: 2.239355784590526

Epoch: 6| Step: 4
Training loss: 2.206414222717285
Validation loss: 2.256743226000058

Epoch: 6| Step: 5
Training loss: 1.8836891651153564
Validation loss: 2.2668125578152236

Epoch: 6| Step: 6
Training loss: 2.6844992637634277
Validation loss: 2.260404993129033

Epoch: 6| Step: 7
Training loss: 2.7936253547668457
Validation loss: 2.2646756979726974

Epoch: 6| Step: 8
Training loss: 2.6116652488708496
Validation loss: 2.2578863046502553

Epoch: 6| Step: 9
Training loss: 2.9042887687683105
Validation loss: 2.2621350365300334

Epoch: 6| Step: 10
Training loss: 2.61517333984375
Validation loss: 2.255448320860504

Epoch: 6| Step: 11
Training loss: 2.7251551151275635
Validation loss: 2.2496381882698304

Epoch: 6| Step: 12
Training loss: 2.8592686653137207
Validation loss: 2.255707215237361

Epoch: 6| Step: 13
Training loss: 2.3653724193573
Validation loss: 2.2628156267186648

Epoch: 192| Step: 0
Training loss: 1.6046003103256226
Validation loss: 2.256477014992827

Epoch: 6| Step: 1
Training loss: 2.634535312652588
Validation loss: 2.2807072131864485

Epoch: 6| Step: 2
Training loss: 2.692089796066284
Validation loss: 2.283020604041315

Epoch: 6| Step: 3
Training loss: 2.398183822631836
Validation loss: 2.2763965616944017

Epoch: 6| Step: 4
Training loss: 2.2520852088928223
Validation loss: 2.2697463753402873

Epoch: 6| Step: 5
Training loss: 2.3543105125427246
Validation loss: 2.274673900296611

Epoch: 6| Step: 6
Training loss: 2.7550864219665527
Validation loss: 2.2587664332441104

Epoch: 6| Step: 7
Training loss: 1.8528320789337158
Validation loss: 2.25232966228198

Epoch: 6| Step: 8
Training loss: 2.1984853744506836
Validation loss: 2.2435608076792892

Epoch: 6| Step: 9
Training loss: 2.84104585647583
Validation loss: 2.25442591533866

Epoch: 6| Step: 10
Training loss: 2.480870008468628
Validation loss: 2.260791908028305

Epoch: 6| Step: 11
Training loss: 3.174825429916382
Validation loss: 2.260497552092357

Epoch: 6| Step: 12
Training loss: 2.0227484703063965
Validation loss: 2.2511923851505404

Epoch: 6| Step: 13
Training loss: 2.68750262260437
Validation loss: 2.2426465480558333

Epoch: 193| Step: 0
Training loss: 3.4607365131378174
Validation loss: 2.237657106050881

Epoch: 6| Step: 1
Training loss: 2.378887176513672
Validation loss: 2.25075517418564

Epoch: 6| Step: 2
Training loss: 2.096007823944092
Validation loss: 2.240525250793785

Epoch: 6| Step: 3
Training loss: 3.136899948120117
Validation loss: 2.236459575673585

Epoch: 6| Step: 4
Training loss: 2.1518023014068604
Validation loss: 2.238652370309317

Epoch: 6| Step: 5
Training loss: 2.2414557933807373
Validation loss: 2.2366341006371284

Epoch: 6| Step: 6
Training loss: 2.4251484870910645
Validation loss: 2.235602665972966

Epoch: 6| Step: 7
Training loss: 2.1192626953125
Validation loss: 2.2497993643565843

Epoch: 6| Step: 8
Training loss: 2.6503796577453613
Validation loss: 2.254179528964463

Epoch: 6| Step: 9
Training loss: 2.475209951400757
Validation loss: 2.2491127374351665

Epoch: 6| Step: 10
Training loss: 2.1470730304718018
Validation loss: 2.2549469586341613

Epoch: 6| Step: 11
Training loss: 2.418489933013916
Validation loss: 2.256321612224784

Epoch: 6| Step: 12
Training loss: 2.101485252380371
Validation loss: 2.2460676367564867

Epoch: 6| Step: 13
Training loss: 1.6204488277435303
Validation loss: 2.2592948136791104

Epoch: 194| Step: 0
Training loss: 1.5974692106246948
Validation loss: 2.2599711392515447

Epoch: 6| Step: 1
Training loss: 1.8334667682647705
Validation loss: 2.2660948640556744

Epoch: 6| Step: 2
Training loss: 2.6124675273895264
Validation loss: 2.283888755306121

Epoch: 6| Step: 3
Training loss: 2.622239589691162
Validation loss: 2.2875382169600456

Epoch: 6| Step: 4
Training loss: 2.512866497039795
Validation loss: 2.280941809377363

Epoch: 6| Step: 5
Training loss: 2.0631446838378906
Validation loss: 2.289191012741417

Epoch: 6| Step: 6
Training loss: 3.213941812515259
Validation loss: 2.284271717071533

Epoch: 6| Step: 7
Training loss: 2.537675142288208
Validation loss: 2.2831033814337944

Epoch: 6| Step: 8
Training loss: 2.468794584274292
Validation loss: 2.2794810315614105

Epoch: 6| Step: 9
Training loss: 1.836138367652893
Validation loss: 2.27033019322221

Epoch: 6| Step: 10
Training loss: 2.438004970550537
Validation loss: 2.256921345187772

Epoch: 6| Step: 11
Training loss: 2.707019090652466
Validation loss: 2.2581238618461033

Epoch: 6| Step: 12
Training loss: 2.3979461193084717
Validation loss: 2.251192387714181

Epoch: 6| Step: 13
Training loss: 3.0929172039031982
Validation loss: 2.2350720128705426

Epoch: 195| Step: 0
Training loss: 3.291606903076172
Validation loss: 2.2221834364757744

Epoch: 6| Step: 1
Training loss: 1.5966161489486694
Validation loss: 2.2105893396562144

Epoch: 6| Step: 2
Training loss: 2.952644109725952
Validation loss: 2.2037910107643373

Epoch: 6| Step: 3
Training loss: 2.7348971366882324
Validation loss: 2.1995755434036255

Epoch: 6| Step: 4
Training loss: 1.9000605344772339
Validation loss: 2.1904764867598012

Epoch: 6| Step: 5
Training loss: 1.8518999814987183
Validation loss: 2.189731523554812

Epoch: 6| Step: 6
Training loss: 2.6699929237365723
Validation loss: 2.1956183705278622

Epoch: 6| Step: 7
Training loss: 2.851442575454712
Validation loss: 2.2000196313345306

Epoch: 6| Step: 8
Training loss: 2.0678608417510986
Validation loss: 2.204731361840361

Epoch: 6| Step: 9
Training loss: 2.2073802947998047
Validation loss: 2.2180428735671507

Epoch: 6| Step: 10
Training loss: 2.623213768005371
Validation loss: 2.2248799057417017

Epoch: 6| Step: 11
Training loss: 2.4496219158172607
Validation loss: 2.2346039792542816

Epoch: 6| Step: 12
Training loss: 2.589287519454956
Validation loss: 2.234655700704103

Epoch: 6| Step: 13
Training loss: 1.7815996408462524
Validation loss: 2.2407798408180155

Epoch: 196| Step: 0
Training loss: 1.9097819328308105
Validation loss: 2.2341062381703365

Epoch: 6| Step: 1
Training loss: 2.984407901763916
Validation loss: 2.2364318947638235

Epoch: 6| Step: 2
Training loss: 3.2162559032440186
Validation loss: 2.232714201814385

Epoch: 6| Step: 3
Training loss: 2.385256290435791
Validation loss: 2.2289206417657996

Epoch: 6| Step: 4
Training loss: 2.27530837059021
Validation loss: 2.2425019023238972

Epoch: 6| Step: 5
Training loss: 2.2119081020355225
Validation loss: 2.2300690220248316

Epoch: 6| Step: 6
Training loss: 2.7652692794799805
Validation loss: 2.250552192811043

Epoch: 6| Step: 7
Training loss: 2.3179931640625
Validation loss: 2.260535277346129

Epoch: 6| Step: 8
Training loss: 2.60125732421875
Validation loss: 2.2728904370338685

Epoch: 6| Step: 9
Training loss: 2.2322592735290527
Validation loss: 2.2831035839614047

Epoch: 6| Step: 10
Training loss: 1.4470869302749634
Validation loss: 2.2738786051350255

Epoch: 6| Step: 11
Training loss: 2.8744540214538574
Validation loss: 2.286055822526255

Epoch: 6| Step: 12
Training loss: 2.7488999366760254
Validation loss: 2.2775672328087593

Epoch: 6| Step: 13
Training loss: 1.3939141035079956
Validation loss: 2.272241441152429

Epoch: 197| Step: 0
Training loss: 2.248624801635742
Validation loss: 2.2689949376608736

Epoch: 6| Step: 1
Training loss: 2.4780399799346924
Validation loss: 2.26472964338077

Epoch: 6| Step: 2
Training loss: 1.7788758277893066
Validation loss: 2.2649778550670994

Epoch: 6| Step: 3
Training loss: 2.2653143405914307
Validation loss: 2.279968636010283

Epoch: 6| Step: 4
Training loss: 3.2694554328918457
Validation loss: 2.3122782681577947

Epoch: 6| Step: 5
Training loss: 3.0282602310180664
Validation loss: 2.2987740988372476

Epoch: 6| Step: 6
Training loss: 2.1409711837768555
Validation loss: 2.2975144822110414

Epoch: 6| Step: 7
Training loss: 2.300882339477539
Validation loss: 2.289821773447016

Epoch: 6| Step: 8
Training loss: 2.29477858543396
Validation loss: 2.256631556377616

Epoch: 6| Step: 9
Training loss: 1.9855767488479614
Validation loss: 2.231710554451071

Epoch: 6| Step: 10
Training loss: 3.0114688873291016
Validation loss: 2.211033223777689

Epoch: 6| Step: 11
Training loss: 2.448448657989502
Validation loss: 2.2120094376225627

Epoch: 6| Step: 12
Training loss: 2.3410797119140625
Validation loss: 2.205498687682613

Epoch: 6| Step: 13
Training loss: 2.385148048400879
Validation loss: 2.2092490375682874

Epoch: 198| Step: 0
Training loss: 2.8819949626922607
Validation loss: 2.2037498771503405

Epoch: 6| Step: 1
Training loss: 1.834288239479065
Validation loss: 2.2058502474138812

Epoch: 6| Step: 2
Training loss: 2.177457094192505
Validation loss: 2.208705150952903

Epoch: 6| Step: 3
Training loss: 2.363802909851074
Validation loss: 2.204048523338892

Epoch: 6| Step: 4
Training loss: 2.8159356117248535
Validation loss: 2.2105453193828626

Epoch: 6| Step: 5
Training loss: 2.321300983428955
Validation loss: 2.200553681260796

Epoch: 6| Step: 6
Training loss: 2.5325989723205566
Validation loss: 2.20588057784624

Epoch: 6| Step: 7
Training loss: 3.139760732650757
Validation loss: 2.202097544106104

Epoch: 6| Step: 8
Training loss: 2.20982027053833
Validation loss: 2.208892651783523

Epoch: 6| Step: 9
Training loss: 2.093338966369629
Validation loss: 2.214706038915983

Epoch: 6| Step: 10
Training loss: 2.3943893909454346
Validation loss: 2.225350486334934

Epoch: 6| Step: 11
Training loss: 2.374295234680176
Validation loss: 2.2304178258424163

Epoch: 6| Step: 12
Training loss: 2.1495416164398193
Validation loss: 2.2436050740621423

Epoch: 6| Step: 13
Training loss: 2.1599342823028564
Validation loss: 2.238693908978534

Epoch: 199| Step: 0
Training loss: 1.9369418621063232
Validation loss: 2.2475506977368425

Epoch: 6| Step: 1
Training loss: 2.748483180999756
Validation loss: 2.2539484013793287

Epoch: 6| Step: 2
Training loss: 2.3300251960754395
Validation loss: 2.2518188825217624

Epoch: 6| Step: 3
Training loss: 2.7239294052124023
Validation loss: 2.257464672929497

Epoch: 6| Step: 4
Training loss: 1.9924330711364746
Validation loss: 2.262211938058176

Epoch: 6| Step: 5
Training loss: 2.385371208190918
Validation loss: 2.2780571291523595

Epoch: 6| Step: 6
Training loss: 2.405233383178711
Validation loss: 2.2783964295541086

Epoch: 6| Step: 7
Training loss: 2.6255478858947754
Validation loss: 2.2576372828534854

Epoch: 6| Step: 8
Training loss: 2.6505932807922363
Validation loss: 2.244526191424298

Epoch: 6| Step: 9
Training loss: 2.306473970413208
Validation loss: 2.2464948213228615

Epoch: 6| Step: 10
Training loss: 1.9644062519073486
Validation loss: 2.2423799960843978

Epoch: 6| Step: 11
Training loss: 2.179354667663574
Validation loss: 2.2425586433820826

Epoch: 6| Step: 12
Training loss: 2.4991698265075684
Validation loss: 2.238809780407977

Epoch: 6| Step: 13
Training loss: 2.641038656234741
Validation loss: 2.2356072548897035

Epoch: 200| Step: 0
Training loss: 1.529295563697815
Validation loss: 2.2350075091085126

Epoch: 6| Step: 1
Training loss: 2.541893482208252
Validation loss: 2.2258046134825675

Epoch: 6| Step: 2
Training loss: 3.0682101249694824
Validation loss: 2.213349785856021

Epoch: 6| Step: 3
Training loss: 2.7061586380004883
Validation loss: 2.2104254717467935

Epoch: 6| Step: 4
Training loss: 2.349424123764038
Validation loss: 2.2094418823078112

Epoch: 6| Step: 5
Training loss: 2.036343812942505
Validation loss: 2.20229632495552

Epoch: 6| Step: 6
Training loss: 1.5770059823989868
Validation loss: 2.202459627582181

Epoch: 6| Step: 7
Training loss: 2.1906778812408447
Validation loss: 2.1972087147415325

Epoch: 6| Step: 8
Training loss: 2.536435127258301
Validation loss: 2.200973485105781

Epoch: 6| Step: 9
Training loss: 2.9209389686584473
Validation loss: 2.1986860716214744

Epoch: 6| Step: 10
Training loss: 2.5700273513793945
Validation loss: 2.200339883886358

Epoch: 6| Step: 11
Training loss: 2.1818647384643555
Validation loss: 2.2155921971926125

Epoch: 6| Step: 12
Training loss: 3.1315016746520996
Validation loss: 2.219763240506572

Epoch: 6| Step: 13
Training loss: 1.6143193244934082
Validation loss: 2.2126770686077815

Testing loss: 2.396482404073079

Epoch: 1| Step: 0
Training loss: 5.852062494284533
Validation loss: 5.8303533486812285

Epoch: 6| Step: 1
Training loss: 5.733385047383657
Validation loss: 5.82032221135168

Epoch: 6| Step: 2
Training loss: 5.815631310133096
Validation loss: 5.811785947489947

Epoch: 6| Step: 3
Training loss: 6.469809625056417
Validation loss: 5.802530129623127

Epoch: 6| Step: 4
Training loss: 4.846668570062053
Validation loss: 5.792650382542235

Epoch: 6| Step: 5
Training loss: 7.360113604954209
Validation loss: 5.781961185221083

Epoch: 6| Step: 6
Training loss: 6.5120564666661815
Validation loss: 5.769557768223906

Epoch: 6| Step: 7
Training loss: 4.756985998165558
Validation loss: 5.75637013680976

Epoch: 6| Step: 8
Training loss: 5.772610864578319
Validation loss: 5.742491370336543

Epoch: 6| Step: 9
Training loss: 5.8565859247387175
Validation loss: 5.728509699796721

Epoch: 6| Step: 10
Training loss: 4.232145841757267
Validation loss: 5.71232048006239

Epoch: 6| Step: 11
Training loss: 6.5589546436033315
Validation loss: 5.6962699936581025

Epoch: 6| Step: 12
Training loss: 4.902841732675456
Validation loss: 5.678099688744004

Epoch: 6| Step: 13
Training loss: 5.568938397475584
Validation loss: 5.659690371301968

Epoch: 2| Step: 0
Training loss: 6.9573563976868105
Validation loss: 5.640058609745133

Epoch: 6| Step: 1
Training loss: 5.20858743683527
Validation loss: 5.618167356165418

Epoch: 6| Step: 2
Training loss: 5.919792030393084
Validation loss: 5.5959002189599305

Epoch: 6| Step: 3
Training loss: 5.207451382033272
Validation loss: 5.57283006734403

Epoch: 6| Step: 4
Training loss: 5.9670816856126105
Validation loss: 5.546512973819772

Epoch: 6| Step: 5
Training loss: 4.8310613005627365
Validation loss: 5.52097138758315

Epoch: 6| Step: 6
Training loss: 5.834509740008152
Validation loss: 5.4937813413255805

Epoch: 6| Step: 7
Training loss: 5.888077486067383
Validation loss: 5.4650069602064315

Epoch: 6| Step: 8
Training loss: 4.878100851757974
Validation loss: 5.435147131071093

Epoch: 6| Step: 9
Training loss: 6.0729632193175345
Validation loss: 5.4030790279351

Epoch: 6| Step: 10
Training loss: 4.434600742607827
Validation loss: 5.370179319458553

Epoch: 6| Step: 11
Training loss: 4.740781773669984
Validation loss: 5.335063962456385

Epoch: 6| Step: 12
Training loss: 5.634168316207499
Validation loss: 5.299433344064183

Epoch: 6| Step: 13
Training loss: 4.906735924632793
Validation loss: 5.26185452614732

Epoch: 3| Step: 0
Training loss: 4.942726748894116
Validation loss: 5.222779295692556

Epoch: 6| Step: 1
Training loss: 3.5279020119032247
Validation loss: 5.182066975116685

Epoch: 6| Step: 2
Training loss: 4.956051897320643
Validation loss: 5.145175932531218

Epoch: 6| Step: 3
Training loss: 5.07908625968061
Validation loss: 5.102424712417106

Epoch: 6| Step: 4
Training loss: 5.323704650642764
Validation loss: 5.064728182088112

Epoch: 6| Step: 5
Training loss: 5.796216243524668
Validation loss: 5.023018756001379

Epoch: 6| Step: 6
Training loss: 4.770219033128713
Validation loss: 4.981325142067809

Epoch: 6| Step: 7
Training loss: 5.613676754072927
Validation loss: 4.944603606135699

Epoch: 6| Step: 8
Training loss: 4.535264965926162
Validation loss: 4.902010522229354

Epoch: 6| Step: 9
Training loss: 4.29851398074113
Validation loss: 4.862509874400857

Epoch: 6| Step: 10
Training loss: 6.299871824867997
Validation loss: 4.822646634637465

Epoch: 6| Step: 11
Training loss: 4.071728128871614
Validation loss: 4.782804580236333

Epoch: 6| Step: 12
Training loss: 4.549281790923972
Validation loss: 4.74618814707803

Epoch: 6| Step: 13
Training loss: 6.402655658490559
Validation loss: 4.710565560394648

Epoch: 4| Step: 0
Training loss: 5.2456692589917875
Validation loss: 4.678588205587168

Epoch: 6| Step: 1
Training loss: 3.971496832948027
Validation loss: 4.645625632872763

Epoch: 6| Step: 2
Training loss: 4.460231317585678
Validation loss: 4.611564855485075

Epoch: 6| Step: 3
Training loss: 4.639653267895033
Validation loss: 4.581609436805645

Epoch: 6| Step: 4
Training loss: 4.551561804370977
Validation loss: 4.550778595537495

Epoch: 6| Step: 5
Training loss: 5.407703005947822
Validation loss: 4.51700019582746

Epoch: 6| Step: 6
Training loss: 4.734775035311579
Validation loss: 4.487661587607504

Epoch: 6| Step: 7
Training loss: 3.854890635085732
Validation loss: 4.4565845694296415

Epoch: 6| Step: 8
Training loss: 4.380624479951527
Validation loss: 4.428526021088448

Epoch: 6| Step: 9
Training loss: 4.667511295903599
Validation loss: 4.399739469904942

Epoch: 6| Step: 10
Training loss: 4.474224445332066
Validation loss: 4.373830388734987

Epoch: 6| Step: 11
Training loss: 4.406817027733052
Validation loss: 4.348397453599209

Epoch: 6| Step: 12
Training loss: 4.357950613651489
Validation loss: 4.322516613667989

Epoch: 6| Step: 13
Training loss: 5.008923388022628
Validation loss: 4.29910732479538

Epoch: 5| Step: 0
Training loss: 5.378346310838178
Validation loss: 4.277754663753794

Epoch: 6| Step: 1
Training loss: 4.074279141001366
Validation loss: 4.256674262864649

Epoch: 6| Step: 2
Training loss: 4.4447551035921204
Validation loss: 4.241214630746502

Epoch: 6| Step: 3
Training loss: 4.32095392856522
Validation loss: 4.221846996592895

Epoch: 6| Step: 4
Training loss: 3.7477130274063164
Validation loss: 4.207555352744283

Epoch: 6| Step: 5
Training loss: 4.537947131948903
Validation loss: 4.196872549679583

Epoch: 6| Step: 6
Training loss: 4.793066445049064
Validation loss: 4.179505912938817

Epoch: 6| Step: 7
Training loss: 3.9550774016203043
Validation loss: 4.166465832371889

Epoch: 6| Step: 8
Training loss: 4.2172828666360145
Validation loss: 4.15259897368401

Epoch: 6| Step: 9
Training loss: 4.555177429964395
Validation loss: 4.139597802845696

Epoch: 6| Step: 10
Training loss: 3.1437415606345627
Validation loss: 4.123668524129806

Epoch: 6| Step: 11
Training loss: 4.194923993809992
Validation loss: 4.107675972918104

Epoch: 6| Step: 12
Training loss: 4.339839794524525
Validation loss: 4.0895906804872855

Epoch: 6| Step: 13
Training loss: 4.274491593318164
Validation loss: 4.0775259776568085

Epoch: 6| Step: 0
Training loss: 5.33874491483017
Validation loss: 4.065399747988648

Epoch: 6| Step: 1
Training loss: 4.641335583880707
Validation loss: 4.050425877290809

Epoch: 6| Step: 2
Training loss: 4.5241934716149705
Validation loss: 4.03811979218501

Epoch: 6| Step: 3
Training loss: 3.430079965576818
Validation loss: 4.022094844061255

Epoch: 6| Step: 4
Training loss: 4.9111121316240345
Validation loss: 4.011800970339757

Epoch: 6| Step: 5
Training loss: 4.005546062832708
Validation loss: 4.002453848636709

Epoch: 6| Step: 6
Training loss: 4.416958613313422
Validation loss: 3.9855487096068485

Epoch: 6| Step: 7
Training loss: 3.775564916922305
Validation loss: 3.9724857833681124

Epoch: 6| Step: 8
Training loss: 3.5946583802281453
Validation loss: 3.9581111895278505

Epoch: 6| Step: 9
Training loss: 2.413677098394676
Validation loss: 3.9458205594585674

Epoch: 6| Step: 10
Training loss: 3.114176048432767
Validation loss: 3.9366695480841134

Epoch: 6| Step: 11
Training loss: 4.848431693493744
Validation loss: 3.9214936496587143

Epoch: 6| Step: 12
Training loss: 4.23926332407929
Validation loss: 3.9020449581779566

Epoch: 6| Step: 13
Training loss: 3.3539851042769873
Validation loss: 3.891155308417897

Epoch: 7| Step: 0
Training loss: 3.92968337436103
Validation loss: 3.881285989576097

Epoch: 6| Step: 1
Training loss: 4.042785227567776
Validation loss: 3.8684264405027555

Epoch: 6| Step: 2
Training loss: 2.685311335551532
Validation loss: 3.857919622592239

Epoch: 6| Step: 3
Training loss: 4.989928210397608
Validation loss: 3.844867602848846

Epoch: 6| Step: 4
Training loss: 4.317208194027266
Validation loss: 3.836297761995779

Epoch: 6| Step: 5
Training loss: 4.383146929433792
Validation loss: 3.8250852656433474

Epoch: 6| Step: 6
Training loss: 3.9549262123603324
Validation loss: 3.811635755101191

Epoch: 6| Step: 7
Training loss: 3.202295112100906
Validation loss: 3.800584853759691

Epoch: 6| Step: 8
Training loss: 3.1747073601853355
Validation loss: 3.7901819254955464

Epoch: 6| Step: 9
Training loss: 3.4040116821518933
Validation loss: 3.778391133698919

Epoch: 6| Step: 10
Training loss: 4.284007722678467
Validation loss: 3.77205638169869

Epoch: 6| Step: 11
Training loss: 4.663215087380366
Validation loss: 3.7654643003424946

Epoch: 6| Step: 12
Training loss: 4.184029578722527
Validation loss: 3.755419617914453

Epoch: 6| Step: 13
Training loss: 3.8057897330177557
Validation loss: 3.7484388827815525

Epoch: 8| Step: 0
Training loss: 3.6179744119914936
Validation loss: 3.7444842241439598

Epoch: 6| Step: 1
Training loss: 3.8071029484113303
Validation loss: 3.7409829185036156

Epoch: 6| Step: 2
Training loss: 4.490319117227208
Validation loss: 3.730230338642143

Epoch: 6| Step: 3
Training loss: 3.5179880296287718
Validation loss: 3.718231522128163

Epoch: 6| Step: 4
Training loss: 3.6805054875133285
Validation loss: 3.7115380456339806

Epoch: 6| Step: 5
Training loss: 4.4949851173095094
Validation loss: 3.7087014999405596

Epoch: 6| Step: 6
Training loss: 3.8242192487550746
Validation loss: 3.696971949534206

Epoch: 6| Step: 7
Training loss: 4.590215036512004
Validation loss: 3.6914901599721692

Epoch: 6| Step: 8
Training loss: 4.115910080382578
Validation loss: 3.683233853341782

Epoch: 6| Step: 9
Training loss: 4.128368736077303
Validation loss: 3.676118530953517

Epoch: 6| Step: 10
Training loss: 2.16769159968338
Validation loss: 3.6690309686989795

Epoch: 6| Step: 11
Training loss: 3.838162835249455
Validation loss: 3.6637146935100153

Epoch: 6| Step: 12
Training loss: 3.9096750121412294
Validation loss: 3.649555365737347

Epoch: 6| Step: 13
Training loss: 3.299424762874453
Validation loss: 3.6287651013809423

Epoch: 9| Step: 0
Training loss: 4.077085632936937
Validation loss: 3.6097267624751344

Epoch: 6| Step: 1
Training loss: 3.9532284930406534
Validation loss: 3.598308911319444

Epoch: 6| Step: 2
Training loss: 2.4159678621744063
Validation loss: 3.5891770410983606

Epoch: 6| Step: 3
Training loss: 3.200314065303146
Validation loss: 3.585735633768292

Epoch: 6| Step: 4
Training loss: 1.891153309046591
Validation loss: 3.5823815997557014

Epoch: 6| Step: 5
Training loss: 3.508537506420583
Validation loss: 3.5834770550610036

Epoch: 6| Step: 6
Training loss: 4.946167882430943
Validation loss: 3.5747516351615927

Epoch: 6| Step: 7
Training loss: 3.7012459538678253
Validation loss: 3.557650348196818

Epoch: 6| Step: 8
Training loss: 4.1405955187629715
Validation loss: 3.5542194940487284

Epoch: 6| Step: 9
Training loss: 4.142608743183054
Validation loss: 3.54982110433115

Epoch: 6| Step: 10
Training loss: 4.168263904878492
Validation loss: 3.547823103917121

Epoch: 6| Step: 11
Training loss: 4.136701229011526
Validation loss: 3.5388984188493855

Epoch: 6| Step: 12
Training loss: 3.9963698604054687
Validation loss: 3.533117464250522

Epoch: 6| Step: 13
Training loss: 3.1140442108367536
Validation loss: 3.5285804074371736

Epoch: 10| Step: 0
Training loss: 4.071638656397028
Validation loss: 3.5226913057451315

Epoch: 6| Step: 1
Training loss: 3.9362465210527033
Validation loss: 3.5231954596361206

Epoch: 6| Step: 2
Training loss: 3.5187117525396903
Validation loss: 3.516787168134109

Epoch: 6| Step: 3
Training loss: 3.5527048471491454
Validation loss: 3.510252113656725

Epoch: 6| Step: 4
Training loss: 3.5883047231433416
Validation loss: 3.501930668248159

Epoch: 6| Step: 5
Training loss: 4.132342191014991
Validation loss: 3.4963725082642685

Epoch: 6| Step: 6
Training loss: 3.957708302725259
Validation loss: 3.4912153316246126

Epoch: 6| Step: 7
Training loss: 3.928487484518135
Validation loss: 3.488252236334343

Epoch: 6| Step: 8
Training loss: 3.7398837649800725
Validation loss: 3.482370485815964

Epoch: 6| Step: 9
Training loss: 3.265232774460518
Validation loss: 3.477157671980335

Epoch: 6| Step: 10
Training loss: 3.1302531530606426
Validation loss: 3.4750690765845453

Epoch: 6| Step: 11
Training loss: 2.81888178413701
Validation loss: 3.475686000766608

Epoch: 6| Step: 12
Training loss: 4.214420612014266
Validation loss: 3.4661464955422554

Epoch: 6| Step: 13
Training loss: 3.8524532874244057
Validation loss: 3.4621588483561014

Epoch: 11| Step: 0
Training loss: 2.6590356020744372
Validation loss: 3.4563286139276426

Epoch: 6| Step: 1
Training loss: 3.6937069136790153
Validation loss: 3.451978170919122

Epoch: 6| Step: 2
Training loss: 3.284455686207851
Validation loss: 3.449122999904339

Epoch: 6| Step: 3
Training loss: 3.688649143971844
Validation loss: 3.44355394165501

Epoch: 6| Step: 4
Training loss: 3.4406147110427447
Validation loss: 3.441595012579652

Epoch: 6| Step: 5
Training loss: 3.9058807198496055
Validation loss: 3.4406050812118214

Epoch: 6| Step: 6
Training loss: 4.090807607143375
Validation loss: 3.4397157003516714

Epoch: 6| Step: 7
Training loss: 4.287558036389161
Validation loss: 3.4368489347122915

Epoch: 6| Step: 8
Training loss: 3.3261701546898075
Validation loss: 3.4253014647037325

Epoch: 6| Step: 9
Training loss: 3.400001884908715
Validation loss: 3.419206229697531

Epoch: 6| Step: 10
Training loss: 3.5951092347167837
Validation loss: 3.4163021750607276

Epoch: 6| Step: 11
Training loss: 3.999918460014851
Validation loss: 3.413500121067271

Epoch: 6| Step: 12
Training loss: 3.4828074397464923
Validation loss: 3.413160580087588

Epoch: 6| Step: 13
Training loss: 4.238714380589912
Validation loss: 3.4125368959719915

Epoch: 12| Step: 0
Training loss: 4.003151605712718
Validation loss: 3.401228129892326

Epoch: 6| Step: 1
Training loss: 2.9263939820675633
Validation loss: 3.4035336532551517

Epoch: 6| Step: 2
Training loss: 3.2703461172797352
Validation loss: 3.399247828966055

Epoch: 6| Step: 3
Training loss: 3.4529079179149287
Validation loss: 3.395521541564884

Epoch: 6| Step: 4
Training loss: 3.110081424689824
Validation loss: 3.3938137737603253

Epoch: 6| Step: 5
Training loss: 5.109615775218458
Validation loss: 3.395948010371823

Epoch: 6| Step: 6
Training loss: 3.962436369857371
Validation loss: 3.3940585116275863

Epoch: 6| Step: 7
Training loss: 3.8158292148687547
Validation loss: 3.392236778555932

Epoch: 6| Step: 8
Training loss: 3.5358430142659336
Validation loss: 3.3855514928633528

Epoch: 6| Step: 9
Training loss: 3.6928870727725993
Validation loss: 3.3810384549421686

Epoch: 6| Step: 10
Training loss: 3.4861500414075777
Validation loss: 3.37659706724824

Epoch: 6| Step: 11
Training loss: 3.2760212245155804
Validation loss: 3.3698690494015118

Epoch: 6| Step: 12
Training loss: 3.1302430991444115
Validation loss: 3.367477060026651

Epoch: 6| Step: 13
Training loss: 3.0742753640454934
Validation loss: 3.3569114708612426

Epoch: 13| Step: 0
Training loss: 3.7037251304077317
Validation loss: 3.353653179759506

Epoch: 6| Step: 1
Training loss: 3.195919212437442
Validation loss: 3.3519180988470403

Epoch: 6| Step: 2
Training loss: 3.2303171548941774
Validation loss: 3.3500637798944664

Epoch: 6| Step: 3
Training loss: 3.0151572383537597
Validation loss: 3.352251317055577

Epoch: 6| Step: 4
Training loss: 3.7524416286292226
Validation loss: 3.348684644812472

Epoch: 6| Step: 5
Training loss: 3.8524464797969236
Validation loss: 3.337690241400926

Epoch: 6| Step: 6
Training loss: 3.5979327200196622
Validation loss: 3.339608700918788

Epoch: 6| Step: 7
Training loss: 2.3800030264314276
Validation loss: 3.3388193705553655

Epoch: 6| Step: 8
Training loss: 3.8689482408553206
Validation loss: 3.3369011663468306

Epoch: 6| Step: 9
Training loss: 3.926102263577685
Validation loss: 3.3404982154856735

Epoch: 6| Step: 10
Training loss: 3.845322574135714
Validation loss: 3.345088842125627

Epoch: 6| Step: 11
Training loss: 3.763010090567314
Validation loss: 3.334792710338479

Epoch: 6| Step: 12
Training loss: 3.875043807243557
Validation loss: 3.33266040921174

Epoch: 6| Step: 13
Training loss: 3.8648794357228455
Validation loss: 3.3306404624130543

Epoch: 14| Step: 0
Training loss: 3.1221989094175697
Validation loss: 3.3271694110391925

Epoch: 6| Step: 1
Training loss: 3.887272136915921
Validation loss: 3.3242007485788423

Epoch: 6| Step: 2
Training loss: 3.1631677940261187
Validation loss: 3.3198090719709406

Epoch: 6| Step: 3
Training loss: 3.5233633367679893
Validation loss: 3.318428487738269

Epoch: 6| Step: 4
Training loss: 3.8939313818749413
Validation loss: 3.3138598937735693

Epoch: 6| Step: 5
Training loss: 3.714121269945467
Validation loss: 3.30905927235572

Epoch: 6| Step: 6
Training loss: 3.7493225757350492
Validation loss: 3.306042438843808

Epoch: 6| Step: 7
Training loss: 3.5800723977467146
Validation loss: 3.303269462295366

Epoch: 6| Step: 8
Training loss: 3.075222757846328
Validation loss: 3.300876615228789

Epoch: 6| Step: 9
Training loss: 3.5665535870270815
Validation loss: 3.297966700965373

Epoch: 6| Step: 10
Training loss: 4.532087998346442
Validation loss: 3.29718241026032

Epoch: 6| Step: 11
Training loss: 3.3196360179432287
Validation loss: 3.293521643999044

Epoch: 6| Step: 12
Training loss: 3.1717409002210073
Validation loss: 3.296811269853625

Epoch: 6| Step: 13
Training loss: 2.6964171868609954
Validation loss: 3.2969954771474232

Epoch: 15| Step: 0
Training loss: 3.2766452995320248
Validation loss: 3.296311201627713

Epoch: 6| Step: 1
Training loss: 3.32235260165028
Validation loss: 3.285753076584613

Epoch: 6| Step: 2
Training loss: 4.092197735436082
Validation loss: 3.2859908615678375

Epoch: 6| Step: 3
Training loss: 3.0391985316097028
Validation loss: 3.277832232644531

Epoch: 6| Step: 4
Training loss: 3.804022446928172
Validation loss: 3.2725662018322934

Epoch: 6| Step: 5
Training loss: 4.152553893950455
Validation loss: 3.2693164166930213

Epoch: 6| Step: 6
Training loss: 3.458124606409814
Validation loss: 3.268094790339114

Epoch: 6| Step: 7
Training loss: 3.3676338342524064
Validation loss: 3.26745304660959

Epoch: 6| Step: 8
Training loss: 3.522415317359489
Validation loss: 3.2657016894891617

Epoch: 6| Step: 9
Training loss: 2.8647884965888823
Validation loss: 3.2617908305008836

Epoch: 6| Step: 10
Training loss: 3.935103398555649
Validation loss: 3.2558118649390755

Epoch: 6| Step: 11
Training loss: 2.7659802397258653
Validation loss: 3.250396355691601

Epoch: 6| Step: 12
Training loss: 3.901111977337872
Validation loss: 3.2501058458709666

Epoch: 6| Step: 13
Training loss: 3.193101038384631
Validation loss: 3.2456333304820215

Epoch: 16| Step: 0
Training loss: 3.965500469972984
Validation loss: 3.2439899295859815

Epoch: 6| Step: 1
Training loss: 4.112363475632096
Validation loss: 3.2422407952168633

Epoch: 6| Step: 2
Training loss: 2.2945108508254357
Validation loss: 3.2437011711913524

Epoch: 6| Step: 3
Training loss: 2.760414171517642
Validation loss: 3.2583905160698956

Epoch: 6| Step: 4
Training loss: 3.4328333302327305
Validation loss: 3.245658480755295

Epoch: 6| Step: 5
Training loss: 2.7898564424121894
Validation loss: 3.2339906151505664

Epoch: 6| Step: 6
Training loss: 3.4485575991667163
Validation loss: 3.235076156153189

Epoch: 6| Step: 7
Training loss: 3.8306489059093156
Validation loss: 3.233491989174201

Epoch: 6| Step: 8
Training loss: 4.068012664130049
Validation loss: 3.2284392401243296

Epoch: 6| Step: 9
Training loss: 3.211500577909657
Validation loss: 3.2253771523947083

Epoch: 6| Step: 10
Training loss: 3.5363110758367187
Validation loss: 3.2250635805545538

Epoch: 6| Step: 11
Training loss: 3.476591028675082
Validation loss: 3.225538703204756

Epoch: 6| Step: 12
Training loss: 3.8898882187421635
Validation loss: 3.2239064569560605

Epoch: 6| Step: 13
Training loss: 3.468863133140655
Validation loss: 3.2228510172617377

Epoch: 17| Step: 0
Training loss: 3.143881859867465
Validation loss: 3.219365374221774

Epoch: 6| Step: 1
Training loss: 3.5215313612627295
Validation loss: 3.2183490475170347

Epoch: 6| Step: 2
Training loss: 3.3074616315927248
Validation loss: 3.2157778248356927

Epoch: 6| Step: 3
Training loss: 3.89532689085382
Validation loss: 3.2111888204604644

Epoch: 6| Step: 4
Training loss: 2.953903822152533
Validation loss: 3.210533498912272

Epoch: 6| Step: 5
Training loss: 4.170717228744768
Validation loss: 3.2108576102253856

Epoch: 6| Step: 6
Training loss: 3.7722115430523124
Validation loss: 3.214836477351355

Epoch: 6| Step: 7
Training loss: 3.8854043396182356
Validation loss: 3.205781077024493

Epoch: 6| Step: 8
Training loss: 3.4148437164872165
Validation loss: 3.2051760209952334

Epoch: 6| Step: 9
Training loss: 3.806113224672687
Validation loss: 3.203899495802383

Epoch: 6| Step: 10
Training loss: 2.2793521040866627
Validation loss: 3.205770764966879

Epoch: 6| Step: 11
Training loss: 2.7515945146768015
Validation loss: 3.2055309800817273

Epoch: 6| Step: 12
Training loss: 3.8469328069488293
Validation loss: 3.2074548716991043

Epoch: 6| Step: 13
Training loss: 3.1754552619916794
Validation loss: 3.20480294566761

Epoch: 18| Step: 0
Training loss: 2.7135967656629107
Validation loss: 3.202565335717925

Epoch: 6| Step: 1
Training loss: 3.6911002218963787
Validation loss: 3.201484345238401

Epoch: 6| Step: 2
Training loss: 3.9882052093938607
Validation loss: 3.2020470142609896

Epoch: 6| Step: 3
Training loss: 2.6503364709256543
Validation loss: 3.1996446671782226

Epoch: 6| Step: 4
Training loss: 4.612565723974391
Validation loss: 3.1982302271320213

Epoch: 6| Step: 5
Training loss: 3.7032602019919185
Validation loss: 3.1954593487808554

Epoch: 6| Step: 6
Training loss: 3.385884993022374
Validation loss: 3.1959588790354623

Epoch: 6| Step: 7
Training loss: 4.111371269815184
Validation loss: 3.1947754250780984

Epoch: 6| Step: 8
Training loss: 3.7003036889891985
Validation loss: 3.191857457815294

Epoch: 6| Step: 9
Training loss: 2.963494397117708
Validation loss: 3.197086343609816

Epoch: 6| Step: 10
Training loss: 2.4405837481696535
Validation loss: 3.188074966102607

Epoch: 6| Step: 11
Training loss: 2.583507029528806
Validation loss: 3.1917346811406526

Epoch: 6| Step: 12
Training loss: 3.9034228050169717
Validation loss: 3.1918703279685716

Epoch: 6| Step: 13
Training loss: 2.820198881021625
Validation loss: 3.1877814145960666

Epoch: 19| Step: 0
Training loss: 3.1389951509776703
Validation loss: 3.187614501595196

Epoch: 6| Step: 1
Training loss: 3.337582327866698
Validation loss: 3.1843591714727846

Epoch: 6| Step: 2
Training loss: 3.242162791698019
Validation loss: 3.186451973881081

Epoch: 6| Step: 3
Training loss: 3.132337577000452
Validation loss: 3.1839391325118123

Epoch: 6| Step: 4
Training loss: 3.3001951159969165
Validation loss: 3.184098491143732

Epoch: 6| Step: 5
Training loss: 2.8268639719526782
Validation loss: 3.1834647421205853

Epoch: 6| Step: 6
Training loss: 3.4717820121611638
Validation loss: 3.1807910868221625

Epoch: 6| Step: 7
Training loss: 4.05962493103593
Validation loss: 3.1804572631442465

Epoch: 6| Step: 8
Training loss: 4.926984768527477
Validation loss: 3.1803241491571987

Epoch: 6| Step: 9
Training loss: 3.6941023088960745
Validation loss: 3.178222367074159

Epoch: 6| Step: 10
Training loss: 2.6584029504475506
Validation loss: 3.175675087141569

Epoch: 6| Step: 11
Training loss: 3.289189787007745
Validation loss: 3.175449361210447

Epoch: 6| Step: 12
Training loss: 3.17788772822578
Validation loss: 3.1756559094772436

Epoch: 6| Step: 13
Training loss: 3.328950161790876
Validation loss: 3.1721036868882067

Epoch: 20| Step: 0
Training loss: 3.8505419758380026
Validation loss: 3.165288982235249

Epoch: 6| Step: 1
Training loss: 3.0723991001282287
Validation loss: 3.1664105723591733

Epoch: 6| Step: 2
Training loss: 3.175956918284906
Validation loss: 3.1644796198047316

Epoch: 6| Step: 3
Training loss: 3.0271730042574503
Validation loss: 3.1601643867123763

Epoch: 6| Step: 4
Training loss: 3.00891061529257
Validation loss: 3.1596509426376387

Epoch: 6| Step: 5
Training loss: 3.5921106994834875
Validation loss: 3.1637540552726193

Epoch: 6| Step: 6
Training loss: 3.011058611331979
Validation loss: 3.1639681805207927

Epoch: 6| Step: 7
Training loss: 3.423177392881468
Validation loss: 3.158104104123422

Epoch: 6| Step: 8
Training loss: 3.5819614948565213
Validation loss: 3.1533497038383813

Epoch: 6| Step: 9
Training loss: 3.909773801226589
Validation loss: 3.1553805597612983

Epoch: 6| Step: 10
Training loss: 3.972838931164166
Validation loss: 3.151325095045596

Epoch: 6| Step: 11
Training loss: 3.3880252276115255
Validation loss: 3.1538786604494966

Epoch: 6| Step: 12
Training loss: 3.577172298284125
Validation loss: 3.152330043470309

Epoch: 6| Step: 13
Training loss: 2.924837945866834
Validation loss: 3.155667488718532

Epoch: 21| Step: 0
Training loss: 3.1846832096286937
Validation loss: 3.152269519003534

Epoch: 6| Step: 1
Training loss: 2.546443317548719
Validation loss: 3.1477767204923612

Epoch: 6| Step: 2
Training loss: 3.1683257592070735
Validation loss: 3.147473469536799

Epoch: 6| Step: 3
Training loss: 3.831224704677596
Validation loss: 3.14539107389646

Epoch: 6| Step: 4
Training loss: 4.422628277890808
Validation loss: 3.148025741854251

Epoch: 6| Step: 5
Training loss: 2.4705726558931382
Validation loss: 3.1450716767377176

Epoch: 6| Step: 6
Training loss: 3.2092163225002532
Validation loss: 3.1518112350703307

Epoch: 6| Step: 7
Training loss: 3.3788111326026797
Validation loss: 3.1423962309974867

Epoch: 6| Step: 8
Training loss: 2.9437905991137794
Validation loss: 3.1454936580773296

Epoch: 6| Step: 9
Training loss: 3.8232828462078152
Validation loss: 3.1464034646846875

Epoch: 6| Step: 10
Training loss: 3.695305015766073
Validation loss: 3.141218778749375

Epoch: 6| Step: 11
Training loss: 3.627727929071882
Validation loss: 3.143972199311114

Epoch: 6| Step: 12
Training loss: 3.8858246508119625
Validation loss: 3.1403502628448243

Epoch: 6| Step: 13
Training loss: 2.7573457058082345
Validation loss: 3.1432244584341005

Epoch: 22| Step: 0
Training loss: 3.5687947125147508
Validation loss: 3.1412601790829155

Epoch: 6| Step: 1
Training loss: 2.7185546980240587
Validation loss: 3.1370359183368404

Epoch: 6| Step: 2
Training loss: 3.5843693913596324
Validation loss: 3.1388394500133048

Epoch: 6| Step: 3
Training loss: 2.4624780102052948
Validation loss: 3.1367289752252336

Epoch: 6| Step: 4
Training loss: 3.9278569897103193
Validation loss: 3.1345494476364286

Epoch: 6| Step: 5
Training loss: 2.788079267450428
Validation loss: 3.136045660550404

Epoch: 6| Step: 6
Training loss: 3.201863407945768
Validation loss: 3.132135398616972

Epoch: 6| Step: 7
Training loss: 3.5104380681473146
Validation loss: 3.130772880824276

Epoch: 6| Step: 8
Training loss: 3.9576749287523354
Validation loss: 3.128087973831663

Epoch: 6| Step: 9
Training loss: 3.401829642379448
Validation loss: 3.126490919435521

Epoch: 6| Step: 10
Training loss: 4.201173918017286
Validation loss: 3.122234356146369

Epoch: 6| Step: 11
Training loss: 2.9006362348658272
Validation loss: 3.1186659611371117

Epoch: 6| Step: 12
Training loss: 3.769123019709959
Validation loss: 3.1176129949041127

Epoch: 6| Step: 13
Training loss: 2.929075456902154
Validation loss: 3.1153267877726423

Epoch: 23| Step: 0
Training loss: 3.2922628965285257
Validation loss: 3.1154725813736763

Epoch: 6| Step: 1
Training loss: 3.7508882106475014
Validation loss: 3.1130377734388786

Epoch: 6| Step: 2
Training loss: 2.875822198478331
Validation loss: 3.115657024712864

Epoch: 6| Step: 3
Training loss: 3.5509654921603606
Validation loss: 3.1129800212780547

Epoch: 6| Step: 4
Training loss: 2.8500203182098516
Validation loss: 3.11745818649297

Epoch: 6| Step: 5
Training loss: 3.4124377681661016
Validation loss: 3.1114693040906114

Epoch: 6| Step: 6
Training loss: 3.0905453817296977
Validation loss: 3.130456738735939

Epoch: 6| Step: 7
Training loss: 2.7831423890028693
Validation loss: 3.1080459406662517

Epoch: 6| Step: 8
Training loss: 3.163500022341935
Validation loss: 3.107337553327435

Epoch: 6| Step: 9
Training loss: 3.7202631332281517
Validation loss: 3.10999092016051

Epoch: 6| Step: 10
Training loss: 3.572556072459765
Validation loss: 3.107140873616841

Epoch: 6| Step: 11
Training loss: 3.6019734377816244
Validation loss: 3.108318035225877

Epoch: 6| Step: 12
Training loss: 3.9697439796281504
Validation loss: 3.106547770762932

Epoch: 6| Step: 13
Training loss: 3.604305654208922
Validation loss: 3.1056425097974967

Epoch: 24| Step: 0
Training loss: 3.150850871381266
Validation loss: 3.113004911628867

Epoch: 6| Step: 1
Training loss: 2.6687132214194
Validation loss: 3.1064202656058386

Epoch: 6| Step: 2
Training loss: 3.5975956782275387
Validation loss: 3.102281767187434

Epoch: 6| Step: 3
Training loss: 3.2225838947843295
Validation loss: 3.101361192013196

Epoch: 6| Step: 4
Training loss: 3.0361226039652274
Validation loss: 3.1026486036959575

Epoch: 6| Step: 5
Training loss: 4.33676112145183
Validation loss: 3.103624077733374

Epoch: 6| Step: 6
Training loss: 2.994734912279252
Validation loss: 3.101730317755944

Epoch: 6| Step: 7
Training loss: 2.8658034806740935
Validation loss: 3.102601966738076

Epoch: 6| Step: 8
Training loss: 3.031519612385564
Validation loss: 3.1002720044298884

Epoch: 6| Step: 9
Training loss: 3.060871236102398
Validation loss: 3.107820582906871

Epoch: 6| Step: 10
Training loss: 3.9328341010996715
Validation loss: 3.1055615276651736

Epoch: 6| Step: 11
Training loss: 3.525265103374818
Validation loss: 3.111126964380253

Epoch: 6| Step: 12
Training loss: 4.018146836267029
Validation loss: 3.0971930105911922

Epoch: 6| Step: 13
Training loss: 3.4952870381245997
Validation loss: 3.09555264704361

Epoch: 25| Step: 0
Training loss: 3.1762175500766694
Validation loss: 3.0934563086470823

Epoch: 6| Step: 1
Training loss: 3.5757542381408194
Validation loss: 3.0940032238835586

Epoch: 6| Step: 2
Training loss: 3.4252066222745507
Validation loss: 3.0951338335417264

Epoch: 6| Step: 3
Training loss: 4.225564609700336
Validation loss: 3.0968307679872487

Epoch: 6| Step: 4
Training loss: 2.9305153452769717
Validation loss: 3.0985818285544595

Epoch: 6| Step: 5
Training loss: 3.469834716554245
Validation loss: 3.097539929369109

Epoch: 6| Step: 6
Training loss: 3.214664570113007
Validation loss: 3.0957157328198717

Epoch: 6| Step: 7
Training loss: 2.6978081248086734
Validation loss: 3.093207875194538

Epoch: 6| Step: 8
Training loss: 3.516268116090324
Validation loss: 3.093549474749593

Epoch: 6| Step: 9
Training loss: 3.5800007397102145
Validation loss: 3.1000953403622193

Epoch: 6| Step: 10
Training loss: 3.523802609357101
Validation loss: 3.0921609277203275

Epoch: 6| Step: 11
Training loss: 3.0739756855248146
Validation loss: 3.091295770580025

Epoch: 6| Step: 12
Training loss: 3.3952961752332755
Validation loss: 3.0944185302181078

Epoch: 6| Step: 13
Training loss: 3.029759778109366
Validation loss: 3.0891601284157333

Epoch: 26| Step: 0
Training loss: 3.355861512046811
Validation loss: 3.0893308870054823

Epoch: 6| Step: 1
Training loss: 3.220161795016515
Validation loss: 3.087749311014791

Epoch: 6| Step: 2
Training loss: 3.4669664626214507
Validation loss: 3.087234354583258

Epoch: 6| Step: 3
Training loss: 4.174647406576786
Validation loss: 3.0919330573591464

Epoch: 6| Step: 4
Training loss: 3.4773802749131395
Validation loss: 3.0860550587657167

Epoch: 6| Step: 5
Training loss: 3.9069093681780265
Validation loss: 3.0803929742558247

Epoch: 6| Step: 6
Training loss: 3.699201992685778
Validation loss: 3.0779622327813008

Epoch: 6| Step: 7
Training loss: 3.2684623402046316
Validation loss: 3.078390828425444

Epoch: 6| Step: 8
Training loss: 2.617123913348358
Validation loss: 3.087352408689821

Epoch: 6| Step: 9
Training loss: 1.9679777053562617
Validation loss: 3.1219966312034035

Epoch: 6| Step: 10
Training loss: 2.7833706775223472
Validation loss: 3.16327899614107

Epoch: 6| Step: 11
Training loss: 3.650744213371077
Validation loss: 3.1720850565963543

Epoch: 6| Step: 12
Training loss: 3.3548002098846643
Validation loss: 3.1156507761731964

Epoch: 6| Step: 13
Training loss: 3.9953160279593707
Validation loss: 3.08065737369846

Epoch: 27| Step: 0
Training loss: 3.562589142336613
Validation loss: 3.0877218043152475

Epoch: 6| Step: 1
Training loss: 3.3904126360086653
Validation loss: 3.1134280402033734

Epoch: 6| Step: 2
Training loss: 3.6787413982429484
Validation loss: 3.104614561353665

Epoch: 6| Step: 3
Training loss: 4.216572743652675
Validation loss: 3.0960477100533157

Epoch: 6| Step: 4
Training loss: 2.297386268233165
Validation loss: 3.083008070277019

Epoch: 6| Step: 5
Training loss: 2.467344439239918
Validation loss: 3.0788683841332736

Epoch: 6| Step: 6
Training loss: 2.758679391487142
Validation loss: 3.0792886705980753

Epoch: 6| Step: 7
Training loss: 3.4289594498820697
Validation loss: 3.116673222363219

Epoch: 6| Step: 8
Training loss: 4.037711708966672
Validation loss: 3.091446059351767

Epoch: 6| Step: 9
Training loss: 2.9780418744649806
Validation loss: 3.0810706531397085

Epoch: 6| Step: 10
Training loss: 3.2262527280172986
Validation loss: 3.0762331199767066

Epoch: 6| Step: 11
Training loss: 4.173384604052774
Validation loss: 3.0727506203037938

Epoch: 6| Step: 12
Training loss: 2.8734236002799007
Validation loss: 3.067042659287417

Epoch: 6| Step: 13
Training loss: 3.4453875129128426
Validation loss: 3.067202669368124

Epoch: 28| Step: 0
Training loss: 3.1347484362359745
Validation loss: 3.0682195661613454

Epoch: 6| Step: 1
Training loss: 3.0599359171521283
Validation loss: 3.067206754871572

Epoch: 6| Step: 2
Training loss: 3.66640698351427
Validation loss: 3.068147638118867

Epoch: 6| Step: 3
Training loss: 3.062784142839231
Validation loss: 3.0665397109717336

Epoch: 6| Step: 4
Training loss: 3.553526166749506
Validation loss: 3.0679681064427022

Epoch: 6| Step: 5
Training loss: 3.4818237270235914
Validation loss: 3.064950916980143

Epoch: 6| Step: 6
Training loss: 2.618018175078061
Validation loss: 3.0637681472983984

Epoch: 6| Step: 7
Training loss: 2.4243189140376002
Validation loss: 3.0610634869119684

Epoch: 6| Step: 8
Training loss: 4.254316494078351
Validation loss: 3.0653693122227264

Epoch: 6| Step: 9
Training loss: 3.82559257355228
Validation loss: 3.0686083447899617

Epoch: 6| Step: 10
Training loss: 2.7045504830053573
Validation loss: 3.0643905419618016

Epoch: 6| Step: 11
Training loss: 2.5004839428754346
Validation loss: 3.065631384757863

Epoch: 6| Step: 12
Training loss: 4.30153444989776
Validation loss: 3.062589574177331

Epoch: 6| Step: 13
Training loss: 3.782130241398732
Validation loss: 3.062032138562378

Epoch: 29| Step: 0
Training loss: 3.219957588106039
Validation loss: 3.064300561964365

Epoch: 6| Step: 1
Training loss: 2.5554745829465206
Validation loss: 3.0639680830591893

Epoch: 6| Step: 2
Training loss: 3.6803675061695604
Validation loss: 3.0759584122658543

Epoch: 6| Step: 3
Training loss: 2.858991920366375
Validation loss: 3.0726111013840733

Epoch: 6| Step: 4
Training loss: 3.7260009854408063
Validation loss: 3.070904438364741

Epoch: 6| Step: 5
Training loss: 2.8472607080189745
Validation loss: 3.067028630058738

Epoch: 6| Step: 6
Training loss: 3.6027481874616636
Validation loss: 3.0611747216394023

Epoch: 6| Step: 7
Training loss: 3.386409688221
Validation loss: 3.0583170085626756

Epoch: 6| Step: 8
Training loss: 2.7464101074663914
Validation loss: 3.061477882331687

Epoch: 6| Step: 9
Training loss: 4.53126704771847
Validation loss: 3.0630837520966496

Epoch: 6| Step: 10
Training loss: 2.572741349356693
Validation loss: 3.056584831702808

Epoch: 6| Step: 11
Training loss: 3.9604735093282497
Validation loss: 3.0551790030263346

Epoch: 6| Step: 12
Training loss: 3.2822081120706503
Validation loss: 3.0580717462836953

Epoch: 6| Step: 13
Training loss: 3.2160934299827826
Validation loss: 3.0553751125609905

Epoch: 30| Step: 0
Training loss: 2.9314402309091308
Validation loss: 3.060030236190997

Epoch: 6| Step: 1
Training loss: 3.9354855667526167
Validation loss: 3.0574790524156623

Epoch: 6| Step: 2
Training loss: 3.7262401641337104
Validation loss: 3.059059716192965

Epoch: 6| Step: 3
Training loss: 3.2913208952909483
Validation loss: 3.0565932994802227

Epoch: 6| Step: 4
Training loss: 3.069305332382015
Validation loss: 3.056397897192748

Epoch: 6| Step: 5
Training loss: 3.361031766923521
Validation loss: 3.052816427613208

Epoch: 6| Step: 6
Training loss: 3.424810117483933
Validation loss: 3.054374209409091

Epoch: 6| Step: 7
Training loss: 3.9338978836389393
Validation loss: 3.054821086910722

Epoch: 6| Step: 8
Training loss: 2.839043043927555
Validation loss: 3.0518436748673836

Epoch: 6| Step: 9
Training loss: 3.6848088158711145
Validation loss: 3.0545215358611357

Epoch: 6| Step: 10
Training loss: 3.045540385285232
Validation loss: 3.051839822486781

Epoch: 6| Step: 11
Training loss: 2.2837919939394724
Validation loss: 3.04933467475838

Epoch: 6| Step: 12
Training loss: 3.249402945035679
Validation loss: 3.0493913691134473

Epoch: 6| Step: 13
Training loss: 3.6615830350923324
Validation loss: 3.0537744563347595

Epoch: 31| Step: 0
Training loss: 3.3055087031900605
Validation loss: 3.056866830628328

Epoch: 6| Step: 1
Training loss: 3.485087232113695
Validation loss: 3.0504397489643136

Epoch: 6| Step: 2
Training loss: 2.643366637515191
Validation loss: 3.056758763664582

Epoch: 6| Step: 3
Training loss: 2.8388203242068495
Validation loss: 3.0565377955232296

Epoch: 6| Step: 4
Training loss: 3.921518499577593
Validation loss: 3.056323469798724

Epoch: 6| Step: 5
Training loss: 2.8503823542517392
Validation loss: 3.0540184889466753

Epoch: 6| Step: 6
Training loss: 3.4179963726562397
Validation loss: 3.0503695784017753

Epoch: 6| Step: 7
Training loss: 3.4371473651689266
Validation loss: 3.0505343935487175

Epoch: 6| Step: 8
Training loss: 3.9159631435463957
Validation loss: 3.051497372119392

Epoch: 6| Step: 9
Training loss: 3.6642332382927503
Validation loss: 3.0549237878736597

Epoch: 6| Step: 10
Training loss: 3.043621967607288
Validation loss: 3.046578048737011

Epoch: 6| Step: 11
Training loss: 2.77087008420813
Validation loss: 3.0482505501465043

Epoch: 6| Step: 12
Training loss: 3.9260176098778885
Validation loss: 3.0445256576527355

Epoch: 6| Step: 13
Training loss: 2.7276483248232593
Validation loss: 3.0426666182649154

Epoch: 32| Step: 0
Training loss: 4.073962897572326
Validation loss: 3.0458842182977257

Epoch: 6| Step: 1
Training loss: 3.7461250630045573
Validation loss: 3.0441574324966263

Epoch: 6| Step: 2
Training loss: 2.821930144015159
Validation loss: 3.0428739054814935

Epoch: 6| Step: 3
Training loss: 2.788648216616359
Validation loss: 3.0459918950774103

Epoch: 6| Step: 4
Training loss: 3.416175310733059
Validation loss: 3.0568898531548583

Epoch: 6| Step: 5
Training loss: 2.716302756838619
Validation loss: 3.0536595200412253

Epoch: 6| Step: 6
Training loss: 3.2702309282521136
Validation loss: 3.0412958626826736

Epoch: 6| Step: 7
Training loss: 3.692050518347399
Validation loss: 3.0422750579021334

Epoch: 6| Step: 8
Training loss: 3.72579391518934
Validation loss: 3.037313294750456

Epoch: 6| Step: 9
Training loss: 3.144475900861871
Validation loss: 3.035606831542658

Epoch: 6| Step: 10
Training loss: 3.2541228333344625
Validation loss: 3.0358438326820227

Epoch: 6| Step: 11
Training loss: 3.1257451504179494
Validation loss: 3.0369348393615847

Epoch: 6| Step: 12
Training loss: 3.6199074852433992
Validation loss: 3.038049567187012

Epoch: 6| Step: 13
Training loss: 2.225683955820662
Validation loss: 3.0394913658426455

Epoch: 33| Step: 0
Training loss: 3.509086802224601
Validation loss: 3.0414769198751443

Epoch: 6| Step: 1
Training loss: 3.374205601766609
Validation loss: 3.059472394596261

Epoch: 6| Step: 2
Training loss: 3.5047796855474487
Validation loss: 3.072517887781878

Epoch: 6| Step: 3
Training loss: 3.047914611301177
Validation loss: 3.0745668085615967

Epoch: 6| Step: 4
Training loss: 2.274201884022074
Validation loss: 3.0814341893683492

Epoch: 6| Step: 5
Training loss: 3.0827663690795846
Validation loss: 3.1078875013593126

Epoch: 6| Step: 6
Training loss: 4.048630028242989
Validation loss: 3.1278720906360302

Epoch: 6| Step: 7
Training loss: 3.2109777443398757
Validation loss: 3.1023389739556406

Epoch: 6| Step: 8
Training loss: 2.937786007207725
Validation loss: 3.0430389951804657

Epoch: 6| Step: 9
Training loss: 3.5339175251186634
Validation loss: 3.035930172151485

Epoch: 6| Step: 10
Training loss: 3.2668142389193857
Validation loss: 3.041406933636021

Epoch: 6| Step: 11
Training loss: 3.559594692989208
Validation loss: 3.0493257849583135

Epoch: 6| Step: 12
Training loss: 3.05209310658076
Validation loss: 3.053476464979653

Epoch: 6| Step: 13
Training loss: 4.392464574802346
Validation loss: 3.0522163382011747

Epoch: 34| Step: 0
Training loss: 3.518571627552546
Validation loss: 3.0451631114802016

Epoch: 6| Step: 1
Training loss: 2.8726979866554334
Validation loss: 3.0391737672951544

Epoch: 6| Step: 2
Training loss: 3.176816952070177
Validation loss: 3.043663774003342

Epoch: 6| Step: 3
Training loss: 3.2392179195142985
Validation loss: 3.0471814493949814

Epoch: 6| Step: 4
Training loss: 3.533241048243479
Validation loss: 3.0472340294702955

Epoch: 6| Step: 5
Training loss: 3.3868875604464543
Validation loss: 3.04599552424394

Epoch: 6| Step: 6
Training loss: 2.3429099039623176
Validation loss: 3.042877290670188

Epoch: 6| Step: 7
Training loss: 3.2044421953477635
Validation loss: 3.045305718061483

Epoch: 6| Step: 8
Training loss: 3.574102014188278
Validation loss: 3.043940263447938

Epoch: 6| Step: 9
Training loss: 3.672115626465662
Validation loss: 3.045807839660311

Epoch: 6| Step: 10
Training loss: 3.5112711755936323
Validation loss: 3.040327682353747

Epoch: 6| Step: 11
Training loss: 3.682304731217247
Validation loss: 3.0400369188509684

Epoch: 6| Step: 12
Training loss: 3.250268044788933
Validation loss: 3.027582278611368

Epoch: 6| Step: 13
Training loss: 3.3062128825709225
Validation loss: 3.026188675893086

Epoch: 35| Step: 0
Training loss: 4.120005502789721
Validation loss: 3.029471165231921

Epoch: 6| Step: 1
Training loss: 3.4883035315200335
Validation loss: 3.0332397148544903

Epoch: 6| Step: 2
Training loss: 3.8528519458655888
Validation loss: 3.0324700465687395

Epoch: 6| Step: 3
Training loss: 2.771501666089692
Validation loss: 3.0365407492069605

Epoch: 6| Step: 4
Training loss: 3.9316881672485704
Validation loss: 3.0338794842221315

Epoch: 6| Step: 5
Training loss: 3.1084071814497656
Validation loss: 3.0309126268383872

Epoch: 6| Step: 6
Training loss: 2.754207513441863
Validation loss: 3.028967114961709

Epoch: 6| Step: 7
Training loss: 3.217982228383554
Validation loss: 3.027036318747607

Epoch: 6| Step: 8
Training loss: 3.2708132473922147
Validation loss: 3.031788482046127

Epoch: 6| Step: 9
Training loss: 2.811773418245894
Validation loss: 3.0341424448653913

Epoch: 6| Step: 10
Training loss: 3.0946219400408554
Validation loss: 3.032827575101415

Epoch: 6| Step: 11
Training loss: 2.7302032909227885
Validation loss: 3.0337867199426825

Epoch: 6| Step: 12
Training loss: 3.2822089837468122
Validation loss: 3.036007502682426

Epoch: 6| Step: 13
Training loss: 3.7393316469352493
Validation loss: 3.0371681832587147

Epoch: 36| Step: 0
Training loss: 3.5488012519620953
Validation loss: 3.033376349025044

Epoch: 6| Step: 1
Training loss: 2.838250176164885
Validation loss: 3.023819772708343

Epoch: 6| Step: 2
Training loss: 1.9128066945217699
Validation loss: 3.0214870374995435

Epoch: 6| Step: 3
Training loss: 3.0030940313260803
Validation loss: 3.021187731725335

Epoch: 6| Step: 4
Training loss: 3.1299119111849367
Validation loss: 3.021752537503014

Epoch: 6| Step: 5
Training loss: 3.2982111388309563
Validation loss: 3.0193013556592754

Epoch: 6| Step: 6
Training loss: 3.929596006871658
Validation loss: 3.0177402982545503

Epoch: 6| Step: 7
Training loss: 3.146920797247746
Validation loss: 3.0170226121040318

Epoch: 6| Step: 8
Training loss: 3.5002069412088512
Validation loss: 3.0210502874750245

Epoch: 6| Step: 9
Training loss: 3.2068751270425757
Validation loss: 3.018069536828122

Epoch: 6| Step: 10
Training loss: 3.918461625045983
Validation loss: 3.018931441418

Epoch: 6| Step: 11
Training loss: 3.576802904841217
Validation loss: 3.015480126011287

Epoch: 6| Step: 12
Training loss: 2.688496515800683
Validation loss: 3.016794916201853

Epoch: 6| Step: 13
Training loss: 4.308329680470161
Validation loss: 3.0182883863878796

Epoch: 37| Step: 0
Training loss: 2.484265403009576
Validation loss: 3.0216900372720614

Epoch: 6| Step: 1
Training loss: 3.606402076939253
Validation loss: 3.014940647822685

Epoch: 6| Step: 2
Training loss: 3.2871112608153386
Validation loss: 3.0172614604492454

Epoch: 6| Step: 3
Training loss: 3.7010052887560025
Validation loss: 3.0161617557331315

Epoch: 6| Step: 4
Training loss: 3.0316586563091206
Validation loss: 3.0197583337763643

Epoch: 6| Step: 5
Training loss: 2.4294286264949028
Validation loss: 3.0191032357731826

Epoch: 6| Step: 6
Training loss: 3.4411928628154214
Validation loss: 3.010262465605582

Epoch: 6| Step: 7
Training loss: 3.2860036657386065
Validation loss: 3.0137461120675937

Epoch: 6| Step: 8
Training loss: 3.6679665399936767
Validation loss: 3.0129159717022755

Epoch: 6| Step: 9
Training loss: 3.872809744773666
Validation loss: 3.0121840021107054

Epoch: 6| Step: 10
Training loss: 3.2202422007106537
Validation loss: 3.0104145822575648

Epoch: 6| Step: 11
Training loss: 3.703041043516896
Validation loss: 3.011131058505155

Epoch: 6| Step: 12
Training loss: 2.8592599314064957
Validation loss: 3.009813208136439

Epoch: 6| Step: 13
Training loss: 2.9863646899842076
Validation loss: 3.0110017690669415

Epoch: 38| Step: 0
Training loss: 3.7216834030993655
Validation loss: 3.0105725575286924

Epoch: 6| Step: 1
Training loss: 2.7810964649126535
Validation loss: 3.0092227590987375

Epoch: 6| Step: 2
Training loss: 3.0215104144502543
Validation loss: 3.0070505183577274

Epoch: 6| Step: 3
Training loss: 3.9837624471689614
Validation loss: 3.008587313150684

Epoch: 6| Step: 4
Training loss: 3.0993398394150886
Validation loss: 3.01348285357271

Epoch: 6| Step: 5
Training loss: 3.3293573667855245
Validation loss: 3.02266560885535

Epoch: 6| Step: 6
Training loss: 3.1702759569072745
Validation loss: 3.02451501068906

Epoch: 6| Step: 7
Training loss: 3.1401451845074226
Validation loss: 3.0315962867347426

Epoch: 6| Step: 8
Training loss: 3.6111609594050837
Validation loss: 3.0168722044163925

Epoch: 6| Step: 9
Training loss: 3.145730040077563
Validation loss: 3.005382462348047

Epoch: 6| Step: 10
Training loss: 2.88825157259655
Validation loss: 3.0023079987285812

Epoch: 6| Step: 11
Training loss: 3.421153928824588
Validation loss: 3.0056142401545567

Epoch: 6| Step: 12
Training loss: 3.606969915558033
Validation loss: 3.0053407922062885

Epoch: 6| Step: 13
Training loss: 2.595518313703924
Validation loss: 3.0048749950005615

Epoch: 39| Step: 0
Training loss: 3.380330009602009
Validation loss: 3.001551033179997

Epoch: 6| Step: 1
Training loss: 2.769505940524781
Validation loss: 3.001138339564076

Epoch: 6| Step: 2
Training loss: 3.442342780130702
Validation loss: 3.0040451400248287

Epoch: 6| Step: 3
Training loss: 3.112022614541541
Validation loss: 3.000856116725345

Epoch: 6| Step: 4
Training loss: 3.5076858055836984
Validation loss: 3.000834671932703

Epoch: 6| Step: 5
Training loss: 3.636687543921564
Validation loss: 2.9995850947812595

Epoch: 6| Step: 6
Training loss: 3.331971716935925
Validation loss: 2.9988973190756765

Epoch: 6| Step: 7
Training loss: 2.988393903615945
Validation loss: 3.001162194493692

Epoch: 6| Step: 8
Training loss: 2.901574424345281
Validation loss: 3.0004999990030945

Epoch: 6| Step: 9
Training loss: 3.4306767108096614
Validation loss: 2.997966998709916

Epoch: 6| Step: 10
Training loss: 2.8629178470999066
Validation loss: 3.001107954707163

Epoch: 6| Step: 11
Training loss: 3.361303299452557
Validation loss: 3.0086122158972888

Epoch: 6| Step: 12
Training loss: 3.321900183366001
Validation loss: 3.005708667132396

Epoch: 6| Step: 13
Training loss: 4.12453897385225
Validation loss: 3.0086487281915875

Epoch: 40| Step: 0
Training loss: 3.0405061328075194
Validation loss: 3.0063336861623404

Epoch: 6| Step: 1
Training loss: 2.7196055852806573
Validation loss: 3.0076774917387747

Epoch: 6| Step: 2
Training loss: 2.9055025513650423
Validation loss: 3.0081344221266724

Epoch: 6| Step: 3
Training loss: 2.7635648996522306
Validation loss: 3.0051622668550344

Epoch: 6| Step: 4
Training loss: 3.719278506381611
Validation loss: 2.998983385255351

Epoch: 6| Step: 5
Training loss: 3.537716506425294
Validation loss: 3.002219878553917

Epoch: 6| Step: 6
Training loss: 2.640302107893577
Validation loss: 2.9977343003875125

Epoch: 6| Step: 7
Training loss: 3.286443034205467
Validation loss: 2.9969890791719056

Epoch: 6| Step: 8
Training loss: 3.0186616304151133
Validation loss: 2.9972442254763676

Epoch: 6| Step: 9
Training loss: 4.716474578145107
Validation loss: 2.997571300494112

Epoch: 6| Step: 10
Training loss: 2.6313505103696944
Validation loss: 2.9961225377210696

Epoch: 6| Step: 11
Training loss: 3.3915120032535473
Validation loss: 2.9992723659275984

Epoch: 6| Step: 12
Training loss: 3.150689694422018
Validation loss: 2.9963238797499363

Epoch: 6| Step: 13
Training loss: 4.048074787459254
Validation loss: 2.9932967012068046

Epoch: 41| Step: 0
Training loss: 3.6598033103830754
Validation loss: 2.9932605523567326

Epoch: 6| Step: 1
Training loss: 3.37720502241539
Validation loss: 2.991433519125512

Epoch: 6| Step: 2
Training loss: 3.7020898329821397
Validation loss: 2.990932521318507

Epoch: 6| Step: 3
Training loss: 3.1334459230472023
Validation loss: 2.9870180919099356

Epoch: 6| Step: 4
Training loss: 3.138033732576358
Validation loss: 2.9930530171180196

Epoch: 6| Step: 5
Training loss: 3.5185849084967074
Validation loss: 2.992414642094471

Epoch: 6| Step: 6
Training loss: 3.4987078052414606
Validation loss: 2.998500454329021

Epoch: 6| Step: 7
Training loss: 3.3290891012068773
Validation loss: 2.9977818726722516

Epoch: 6| Step: 8
Training loss: 1.6262229572366997
Validation loss: 2.9951994918486426

Epoch: 6| Step: 9
Training loss: 3.765898508609657
Validation loss: 2.99024768856737

Epoch: 6| Step: 10
Training loss: 2.6962421969962715
Validation loss: 2.987982568164419

Epoch: 6| Step: 11
Training loss: 3.438753211721226
Validation loss: 2.994054101089089

Epoch: 6| Step: 12
Training loss: 3.455226345128396
Validation loss: 2.986677845921704

Epoch: 6| Step: 13
Training loss: 2.5930128773620713
Validation loss: 2.98529394221113

Epoch: 42| Step: 0
Training loss: 3.230703582900165
Validation loss: 2.9852945364705823

Epoch: 6| Step: 1
Training loss: 3.5655560686568024
Validation loss: 2.984060085204592

Epoch: 6| Step: 2
Training loss: 4.078090857585766
Validation loss: 2.9867825792858835

Epoch: 6| Step: 3
Training loss: 3.153267839597091
Validation loss: 2.9853921600135815

Epoch: 6| Step: 4
Training loss: 3.3775769038731225
Validation loss: 2.985732919150242

Epoch: 6| Step: 5
Training loss: 3.254607236168252
Validation loss: 2.9847748302863693

Epoch: 6| Step: 6
Training loss: 3.2963550030992597
Validation loss: 2.982816389137805

Epoch: 6| Step: 7
Training loss: 3.35807143358383
Validation loss: 2.9826783430593102

Epoch: 6| Step: 8
Training loss: 2.9986131959520868
Validation loss: 2.982461582335898

Epoch: 6| Step: 9
Training loss: 3.430909653757454
Validation loss: 2.981080291197334

Epoch: 6| Step: 10
Training loss: 3.450625244096396
Validation loss: 2.9831321485087297

Epoch: 6| Step: 11
Training loss: 2.5191568732273724
Validation loss: 2.9807789423584943

Epoch: 6| Step: 12
Training loss: 2.697464854777765
Validation loss: 2.9788643078092005

Epoch: 6| Step: 13
Training loss: 3.0495079206397273
Validation loss: 2.9787904200754616

Epoch: 43| Step: 0
Training loss: 3.6807732731625475
Validation loss: 2.9808947881031402

Epoch: 6| Step: 1
Training loss: 3.2142735435618723
Validation loss: 2.982536240391996

Epoch: 6| Step: 2
Training loss: 3.5682912234358093
Validation loss: 2.97913855698537

Epoch: 6| Step: 3
Training loss: 3.5152137346252013
Validation loss: 2.9803496594371794

Epoch: 6| Step: 4
Training loss: 3.4994201179780307
Validation loss: 2.981346366186319

Epoch: 6| Step: 5
Training loss: 3.3152939152059595
Validation loss: 2.978818022979173

Epoch: 6| Step: 6
Training loss: 1.995602481952744
Validation loss: 2.98131331513302

Epoch: 6| Step: 7
Training loss: 3.152132880474266
Validation loss: 2.9807200502143893

Epoch: 6| Step: 8
Training loss: 3.5576368902573408
Validation loss: 2.985460393781896

Epoch: 6| Step: 9
Training loss: 3.6531580705671365
Validation loss: 2.981869452902311

Epoch: 6| Step: 10
Training loss: 3.580221702914025
Validation loss: 2.9787770699309255

Epoch: 6| Step: 11
Training loss: 2.3704582757952894
Validation loss: 2.977709458493942

Epoch: 6| Step: 12
Training loss: 3.063278099395058
Validation loss: 2.977140311609673

Epoch: 6| Step: 13
Training loss: 2.8297637069778636
Validation loss: 2.9780512301435897

Epoch: 44| Step: 0
Training loss: 2.941930149226811
Validation loss: 2.973855671860695

Epoch: 6| Step: 1
Training loss: 3.6305774006716987
Validation loss: 2.9752783858012726

Epoch: 6| Step: 2
Training loss: 2.7392350085392154
Validation loss: 2.973328004957616

Epoch: 6| Step: 3
Training loss: 3.461947257201575
Validation loss: 2.9764586042344168

Epoch: 6| Step: 4
Training loss: 2.6137409347603193
Validation loss: 2.9731057751708287

Epoch: 6| Step: 5
Training loss: 3.0827790526766705
Validation loss: 2.9751487490000037

Epoch: 6| Step: 6
Training loss: 3.0161429637345685
Validation loss: 2.9736285742947004

Epoch: 6| Step: 7
Training loss: 2.9479583270001446
Validation loss: 2.9738497529540426

Epoch: 6| Step: 8
Training loss: 3.2811244395619354
Validation loss: 2.973811098738437

Epoch: 6| Step: 9
Training loss: 3.8152165974616534
Validation loss: 2.973162746988848

Epoch: 6| Step: 10
Training loss: 3.7110837245039785
Validation loss: 2.9744279941472995

Epoch: 6| Step: 11
Training loss: 3.815588528695475
Validation loss: 2.9742713968751078

Epoch: 6| Step: 12
Training loss: 2.741546903813934
Validation loss: 2.9746522685226164

Epoch: 6| Step: 13
Training loss: 3.6614358755682774
Validation loss: 2.9718054884440206

Epoch: 45| Step: 0
Training loss: 2.689225640853574
Validation loss: 2.973345987207581

Epoch: 6| Step: 1
Training loss: 3.006779005175125
Validation loss: 2.974070699665257

Epoch: 6| Step: 2
Training loss: 3.2742141130141946
Validation loss: 2.97945536264427

Epoch: 6| Step: 3
Training loss: 2.92542805596171
Validation loss: 2.9815661503267643

Epoch: 6| Step: 4
Training loss: 3.2430452613930396
Validation loss: 2.984180356686676

Epoch: 6| Step: 5
Training loss: 3.5566162242607517
Validation loss: 2.9784354289108363

Epoch: 6| Step: 6
Training loss: 2.9749719570344864
Validation loss: 2.9737925891231454

Epoch: 6| Step: 7
Training loss: 3.859341517005881
Validation loss: 2.973594861553119

Epoch: 6| Step: 8
Training loss: 3.9392483175256805
Validation loss: 2.9740214119805657

Epoch: 6| Step: 9
Training loss: 3.084564908103288
Validation loss: 2.9731653803310834

Epoch: 6| Step: 10
Training loss: 3.7028704205658434
Validation loss: 2.9705560549946073

Epoch: 6| Step: 11
Training loss: 2.912428001201509
Validation loss: 2.972976726585824

Epoch: 6| Step: 12
Training loss: 2.882543535788796
Validation loss: 2.9691862959064674

Epoch: 6| Step: 13
Training loss: 3.234712887128965
Validation loss: 2.965908106085172

Epoch: 46| Step: 0
Training loss: 3.0739100687824155
Validation loss: 2.9677011188118523

Epoch: 6| Step: 1
Training loss: 3.3305762491722497
Validation loss: 2.961094058733927

Epoch: 6| Step: 2
Training loss: 3.538928570434241
Validation loss: 2.9662862505692273

Epoch: 6| Step: 3
Training loss: 3.0663446699628802
Validation loss: 2.9678724078799124

Epoch: 6| Step: 4
Training loss: 3.5944677548354442
Validation loss: 2.9703990870255326

Epoch: 6| Step: 5
Training loss: 3.2813322511082426
Validation loss: 2.9740523959602463

Epoch: 6| Step: 6
Training loss: 3.132860749365434
Validation loss: 2.972592872732157

Epoch: 6| Step: 7
Training loss: 2.705174721876372
Validation loss: 2.972119428837545

Epoch: 6| Step: 8
Training loss: 3.252201508438439
Validation loss: 2.9657064712052623

Epoch: 6| Step: 9
Training loss: 3.2548729970505317
Validation loss: 2.9627531111129755

Epoch: 6| Step: 10
Training loss: 3.1612044624738296
Validation loss: 2.964588475143808

Epoch: 6| Step: 11
Training loss: 3.5241859470025547
Validation loss: 2.9613384966982585

Epoch: 6| Step: 12
Training loss: 2.75074766572463
Validation loss: 2.961681785312463

Epoch: 6| Step: 13
Training loss: 4.026170235867887
Validation loss: 2.9593072565146312

Epoch: 47| Step: 0
Training loss: 3.3782616854953726
Validation loss: 2.9607810083180097

Epoch: 6| Step: 1
Training loss: 3.0284218720373453
Validation loss: 2.964713285963831

Epoch: 6| Step: 2
Training loss: 2.9048038289054614
Validation loss: 2.963816655828648

Epoch: 6| Step: 3
Training loss: 3.5307217093073504
Validation loss: 2.964161663959031

Epoch: 6| Step: 4
Training loss: 3.408379825066653
Validation loss: 2.96371059101005

Epoch: 6| Step: 5
Training loss: 3.274331200720495
Validation loss: 2.959942402768452

Epoch: 6| Step: 6
Training loss: 3.6810090431406977
Validation loss: 2.9612516723057953

Epoch: 6| Step: 7
Training loss: 2.920874276375331
Validation loss: 2.9618609960665827

Epoch: 6| Step: 8
Training loss: 2.996108073731243
Validation loss: 2.963043109246804

Epoch: 6| Step: 9
Training loss: 3.442457196669745
Validation loss: 2.956777015650714

Epoch: 6| Step: 10
Training loss: 3.0171073472268004
Validation loss: 2.959208869465994

Epoch: 6| Step: 11
Training loss: 2.95230155646161
Validation loss: 2.9613096019768212

Epoch: 6| Step: 12
Training loss: 3.6418054328743437
Validation loss: 2.9602715735875176

Epoch: 6| Step: 13
Training loss: 3.1105310792484784
Validation loss: 2.9579802346738204

Epoch: 48| Step: 0
Training loss: 2.991806922394671
Validation loss: 2.956009378504744

Epoch: 6| Step: 1
Training loss: 3.3577945273847187
Validation loss: 2.958391680929938

Epoch: 6| Step: 2
Training loss: 3.4573710991144972
Validation loss: 2.9566369595431627

Epoch: 6| Step: 3
Training loss: 3.5474274999026307
Validation loss: 2.9583221520838405

Epoch: 6| Step: 4
Training loss: 3.084130330222578
Validation loss: 2.956279609096739

Epoch: 6| Step: 5
Training loss: 3.1422014976883954
Validation loss: 2.966322087731072

Epoch: 6| Step: 6
Training loss: 3.334001108673955
Validation loss: 2.9932204898046186

Epoch: 6| Step: 7
Training loss: 3.0568543380759685
Validation loss: 3.0069209920816173

Epoch: 6| Step: 8
Training loss: 3.240114289140038
Validation loss: 3.0100500973169706

Epoch: 6| Step: 9
Training loss: 3.5444443226026663
Validation loss: 2.9738534511945054

Epoch: 6| Step: 10
Training loss: 3.1167475263200686
Validation loss: 2.976846858947767

Epoch: 6| Step: 11
Training loss: 3.330106906263088
Validation loss: 2.9914517370489038

Epoch: 6| Step: 12
Training loss: 2.984157694150007
Validation loss: 2.9527164716402345

Epoch: 6| Step: 13
Training loss: 3.1490771682698964
Validation loss: 2.953285688204233

Epoch: 49| Step: 0
Training loss: 3.05572039612302
Validation loss: 2.9526408234211243

Epoch: 6| Step: 1
Training loss: 3.2378499258039755
Validation loss: 2.9588833069047036

Epoch: 6| Step: 2
Training loss: 4.075172027514054
Validation loss: 2.96601559021903

Epoch: 6| Step: 3
Training loss: 3.499684183313484
Validation loss: 2.9664773580837425

Epoch: 6| Step: 4
Training loss: 2.8632657621831292
Validation loss: 2.9651159836803687

Epoch: 6| Step: 5
Training loss: 3.727863204730178
Validation loss: 2.9689165656917553

Epoch: 6| Step: 6
Training loss: 3.1608946212296534
Validation loss: 2.9658424513054484

Epoch: 6| Step: 7
Training loss: 3.118512553194807
Validation loss: 2.9649580965164524

Epoch: 6| Step: 8
Training loss: 3.8967537158241794
Validation loss: 2.9616465325549193

Epoch: 6| Step: 9
Training loss: 2.810924258155867
Validation loss: 2.964467573109133

Epoch: 6| Step: 10
Training loss: 3.4068477001226594
Validation loss: 2.9570163555151883

Epoch: 6| Step: 11
Training loss: 2.2362211909818623
Validation loss: 2.954060514133186

Epoch: 6| Step: 12
Training loss: 3.06300941434454
Validation loss: 2.951878827468022

Epoch: 6| Step: 13
Training loss: 2.6637238318000627
Validation loss: 2.9550963050567116

Epoch: 50| Step: 0
Training loss: 2.6737682929356206
Validation loss: 2.9606811731109017

Epoch: 6| Step: 1
Training loss: 2.98474198726285
Validation loss: 2.962848936969493

Epoch: 6| Step: 2
Training loss: 3.813026016718848
Validation loss: 2.982939613935992

Epoch: 6| Step: 3
Training loss: 3.3221720436188047
Validation loss: 2.9906438268803814

Epoch: 6| Step: 4
Training loss: 3.050743268859685
Validation loss: 2.980903740938056

Epoch: 6| Step: 5
Training loss: 3.9458689080111347
Validation loss: 2.96759656380215

Epoch: 6| Step: 6
Training loss: 2.9389158042952386
Validation loss: 2.947016618426019

Epoch: 6| Step: 7
Training loss: 3.434244035717729
Validation loss: 2.9443912474855414

Epoch: 6| Step: 8
Training loss: 2.6419587688824007
Validation loss: 2.945716234658144

Epoch: 6| Step: 9
Training loss: 3.215842257649841
Validation loss: 2.9470199693192756

Epoch: 6| Step: 10
Training loss: 3.344760777653464
Validation loss: 2.9470092746326126

Epoch: 6| Step: 11
Training loss: 3.4283294422039177
Validation loss: 2.9486923950913453

Epoch: 6| Step: 12
Training loss: 2.8843254109890326
Validation loss: 2.9535271485311227

Epoch: 6| Step: 13
Training loss: 3.6322342607261002
Validation loss: 2.9501290907857634

Epoch: 51| Step: 0
Training loss: 3.0685127555056186
Validation loss: 2.950338850344774

Epoch: 6| Step: 1
Training loss: 2.3020074293876758
Validation loss: 2.947457808345386

Epoch: 6| Step: 2
Training loss: 3.042641226657404
Validation loss: 2.947435165268756

Epoch: 6| Step: 3
Training loss: 3.115699967656088
Validation loss: 2.951463963042178

Epoch: 6| Step: 4
Training loss: 3.122026783839894
Validation loss: 2.9447698636463038

Epoch: 6| Step: 5
Training loss: 3.929801074713122
Validation loss: 2.945066218305211

Epoch: 6| Step: 6
Training loss: 3.6725982603011977
Validation loss: 2.9441016883146807

Epoch: 6| Step: 7
Training loss: 2.6163706857081546
Validation loss: 2.94464993090377

Epoch: 6| Step: 8
Training loss: 3.4033200322811936
Validation loss: 2.9451626144120646

Epoch: 6| Step: 9
Training loss: 2.7107762005665657
Validation loss: 2.9417115694323552

Epoch: 6| Step: 10
Training loss: 3.759376121260808
Validation loss: 2.945174816468348

Epoch: 6| Step: 11
Training loss: 3.4250370552675857
Validation loss: 2.944331434223272

Epoch: 6| Step: 12
Training loss: 3.1418996473248257
Validation loss: 2.943379049758461

Epoch: 6| Step: 13
Training loss: 3.781212168102324
Validation loss: 2.9458395441048455

Epoch: 52| Step: 0
Training loss: 3.276342882501131
Validation loss: 2.9439165938613536

Epoch: 6| Step: 1
Training loss: 3.225403362746587
Validation loss: 2.9492967790444786

Epoch: 6| Step: 2
Training loss: 2.438042311372433
Validation loss: 2.9433192272496513

Epoch: 6| Step: 3
Training loss: 3.4390199942000645
Validation loss: 2.9553557499358134

Epoch: 6| Step: 4
Training loss: 2.8680112731158722
Validation loss: 2.961342552515642

Epoch: 6| Step: 5
Training loss: 2.9400900926864115
Validation loss: 2.9551377233471796

Epoch: 6| Step: 6
Training loss: 3.4510927045513737
Validation loss: 2.9464379928616515

Epoch: 6| Step: 7
Training loss: 3.012482898711104
Validation loss: 2.9512923383915313

Epoch: 6| Step: 8
Training loss: 2.7717986946827895
Validation loss: 2.948674263362881

Epoch: 6| Step: 9
Training loss: 2.8426353869241066
Validation loss: 2.9417897592558115

Epoch: 6| Step: 10
Training loss: 3.827296576099231
Validation loss: 2.937772505632402

Epoch: 6| Step: 11
Training loss: 3.5305666642139735
Validation loss: 2.93559945648714

Epoch: 6| Step: 12
Training loss: 3.7546803830349607
Validation loss: 2.9358428805184738

Epoch: 6| Step: 13
Training loss: 3.630475874044818
Validation loss: 2.9327104437339875

Epoch: 53| Step: 0
Training loss: 3.19766201202373
Validation loss: 2.934876584173359

Epoch: 6| Step: 1
Training loss: 3.104290355974579
Validation loss: 2.9319978884224738

Epoch: 6| Step: 2
Training loss: 2.6675195720780445
Validation loss: 2.9323516182699625

Epoch: 6| Step: 3
Training loss: 3.4632924078639626
Validation loss: 2.9328726070602027

Epoch: 6| Step: 4
Training loss: 3.57721482073949
Validation loss: 2.9322977702925095

Epoch: 6| Step: 5
Training loss: 3.2550864397750967
Validation loss: 2.929867303615244

Epoch: 6| Step: 6
Training loss: 3.8511360969278337
Validation loss: 2.9301963892494896

Epoch: 6| Step: 7
Training loss: 2.9926056333173103
Validation loss: 2.9302027042999876

Epoch: 6| Step: 8
Training loss: 3.024476338608509
Validation loss: 2.9321414558814425

Epoch: 6| Step: 9
Training loss: 3.4427620580100697
Validation loss: 2.928248913503093

Epoch: 6| Step: 10
Training loss: 3.3262247741040847
Validation loss: 2.9272396335087327

Epoch: 6| Step: 11
Training loss: 2.5505151303142974
Validation loss: 2.933146977750968

Epoch: 6| Step: 12
Training loss: 3.249927373221343
Validation loss: 2.9392858761080825

Epoch: 6| Step: 13
Training loss: 3.1257103684790097
Validation loss: 2.934693789768343

Epoch: 54| Step: 0
Training loss: 2.960425823357685
Validation loss: 2.939916271063554

Epoch: 6| Step: 1
Training loss: 3.422090985174916
Validation loss: 2.9351163691506885

Epoch: 6| Step: 2
Training loss: 2.820284180221605
Validation loss: 2.936125352284472

Epoch: 6| Step: 3
Training loss: 3.3804336888095405
Validation loss: 2.9397206008799683

Epoch: 6| Step: 4
Training loss: 2.6481253741050024
Validation loss: 2.9318682414912907

Epoch: 6| Step: 5
Training loss: 4.069725770451823
Validation loss: 2.9292166654097316

Epoch: 6| Step: 6
Training loss: 3.548724662687623
Validation loss: 2.92686161910463

Epoch: 6| Step: 7
Training loss: 3.013910151893471
Validation loss: 2.925657661209181

Epoch: 6| Step: 8
Training loss: 3.630306963171377
Validation loss: 2.9236233389409407

Epoch: 6| Step: 9
Training loss: 2.6949559957277023
Validation loss: 2.922145897410655

Epoch: 6| Step: 10
Training loss: 2.3679588748092915
Validation loss: 2.92259134089493

Epoch: 6| Step: 11
Training loss: 2.6875348643325863
Validation loss: 2.925023570113447

Epoch: 6| Step: 12
Training loss: 3.5248147848181968
Validation loss: 2.9277055575535615

Epoch: 6| Step: 13
Training loss: 4.077083059917448
Validation loss: 2.925532001761463

Epoch: 55| Step: 0
Training loss: 3.788127980560706
Validation loss: 2.9239810699167266

Epoch: 6| Step: 1
Training loss: 2.6997492002635597
Validation loss: 2.9226606724328574

Epoch: 6| Step: 2
Training loss: 3.2852698997044074
Validation loss: 2.920675071133893

Epoch: 6| Step: 3
Training loss: 3.1064726352539376
Validation loss: 2.9232101737103857

Epoch: 6| Step: 4
Training loss: 2.7146782035576242
Validation loss: 2.9238426203807526

Epoch: 6| Step: 5
Training loss: 2.8187624404531197
Validation loss: 2.92045317383847

Epoch: 6| Step: 6
Training loss: 3.3605448260684065
Validation loss: 2.9281733274523747

Epoch: 6| Step: 7
Training loss: 2.8728194882612774
Validation loss: 2.9227431032457174

Epoch: 6| Step: 8
Training loss: 3.8569648413980597
Validation loss: 2.922554458723687

Epoch: 6| Step: 9
Training loss: 3.3502013814948235
Validation loss: 2.9242579949113163

Epoch: 6| Step: 10
Training loss: 3.4708633171212164
Validation loss: 2.92251150232841

Epoch: 6| Step: 11
Training loss: 3.1188582529610076
Validation loss: 2.9258732275717523

Epoch: 6| Step: 12
Training loss: 3.3008404788509984
Validation loss: 2.9229765627320505

Epoch: 6| Step: 13
Training loss: 2.679447352378147
Validation loss: 2.9230754080479477

Epoch: 56| Step: 0
Training loss: 2.9018516482732926
Validation loss: 2.9191982222970845

Epoch: 6| Step: 1
Training loss: 3.4790499243118793
Validation loss: 2.920611970314948

Epoch: 6| Step: 2
Training loss: 2.8607352934818278
Validation loss: 2.920962160542197

Epoch: 6| Step: 3
Training loss: 3.0808185081823445
Validation loss: 2.919251753381269

Epoch: 6| Step: 4
Training loss: 3.0383125693957402
Validation loss: 2.9216669840037683

Epoch: 6| Step: 5
Training loss: 2.902624021734102
Validation loss: 2.918303567983537

Epoch: 6| Step: 6
Training loss: 3.146598941699171
Validation loss: 2.922274457987466

Epoch: 6| Step: 7
Training loss: 3.2688172721869533
Validation loss: 2.921626389873325

Epoch: 6| Step: 8
Training loss: 3.492183403678386
Validation loss: 2.9167084982431457

Epoch: 6| Step: 9
Training loss: 3.3808129724483864
Validation loss: 2.9184783693914125

Epoch: 6| Step: 10
Training loss: 3.3036810602680204
Validation loss: 2.916381891180194

Epoch: 6| Step: 11
Training loss: 2.443283067665856
Validation loss: 2.9177010549264493

Epoch: 6| Step: 12
Training loss: 3.7596847089507084
Validation loss: 2.917582130619121

Epoch: 6| Step: 13
Training loss: 3.8943821181169653
Validation loss: 2.917087962814009

Epoch: 57| Step: 0
Training loss: 4.119851801160497
Validation loss: 2.9161133075711314

Epoch: 6| Step: 1
Training loss: 3.1120878873638445
Validation loss: 2.91725491394334

Epoch: 6| Step: 2
Training loss: 3.9138452770389502
Validation loss: 2.919855636606472

Epoch: 6| Step: 3
Training loss: 2.730502454470537
Validation loss: 2.9174948932986027

Epoch: 6| Step: 4
Training loss: 2.9028109640346753
Validation loss: 2.9196688189097473

Epoch: 6| Step: 5
Training loss: 3.0973947036986567
Validation loss: 2.9203360119789434

Epoch: 6| Step: 6
Training loss: 2.879812442182379
Validation loss: 2.9143109020940403

Epoch: 6| Step: 7
Training loss: 3.786212785499526
Validation loss: 2.914056448207273

Epoch: 6| Step: 8
Training loss: 3.1427404765979436
Validation loss: 2.9110685255557582

Epoch: 6| Step: 9
Training loss: 3.259251119100957
Validation loss: 2.911068746599602

Epoch: 6| Step: 10
Training loss: 3.1799266828326274
Validation loss: 2.9136335472949533

Epoch: 6| Step: 11
Training loss: 2.0875318056527465
Validation loss: 2.910958210254405

Epoch: 6| Step: 12
Training loss: 2.647646085258686
Validation loss: 2.910930288846181

Epoch: 6| Step: 13
Training loss: 3.5530780879725903
Validation loss: 2.909306875387601

Epoch: 58| Step: 0
Training loss: 3.332236459188677
Validation loss: 2.91183973170364

Epoch: 6| Step: 1
Training loss: 3.006102395349853
Validation loss: 2.9096962965494417

Epoch: 6| Step: 2
Training loss: 2.967167161306347
Validation loss: 2.910326126006535

Epoch: 6| Step: 3
Training loss: 3.279400694920747
Validation loss: 2.9140907468116533

Epoch: 6| Step: 4
Training loss: 3.520699869067825
Validation loss: 2.9113079282194025

Epoch: 6| Step: 5
Training loss: 3.0230772607175584
Validation loss: 2.9108137785429506

Epoch: 6| Step: 6
Training loss: 3.9043948231366823
Validation loss: 2.9104562295776266

Epoch: 6| Step: 7
Training loss: 3.254598738494255
Validation loss: 2.916387001962715

Epoch: 6| Step: 8
Training loss: 3.0898105701541048
Validation loss: 2.9069156490877983

Epoch: 6| Step: 9
Training loss: 2.971300816147544
Validation loss: 2.9107347742401606

Epoch: 6| Step: 10
Training loss: 2.228418480162443
Validation loss: 2.9083986149910395

Epoch: 6| Step: 11
Training loss: 3.8722634035240904
Validation loss: 2.9083785422865254

Epoch: 6| Step: 12
Training loss: 2.8998539591881416
Validation loss: 2.9085844193213926

Epoch: 6| Step: 13
Training loss: 2.897031869930931
Validation loss: 2.9142415430105872

Epoch: 59| Step: 0
Training loss: 3.5949763320334753
Validation loss: 2.909863474421903

Epoch: 6| Step: 1
Training loss: 2.784010813450914
Validation loss: 2.9080854255288973

Epoch: 6| Step: 2
Training loss: 3.3266972447775656
Validation loss: 2.9084188444560333

Epoch: 6| Step: 3
Training loss: 3.225811961077015
Validation loss: 2.911378733065416

Epoch: 6| Step: 4
Training loss: 2.9399650457471163
Validation loss: 2.9094530111367454

Epoch: 6| Step: 5
Training loss: 3.9737223317208135
Validation loss: 2.905680423866059

Epoch: 6| Step: 6
Training loss: 2.6338591263970588
Validation loss: 2.9085552843356353

Epoch: 6| Step: 7
Training loss: 3.499648348990132
Validation loss: 2.9084558900152198

Epoch: 6| Step: 8
Training loss: 2.5954738541522415
Validation loss: 2.904568887573268

Epoch: 6| Step: 9
Training loss: 3.1778197555466665
Validation loss: 2.9085261931291595

Epoch: 6| Step: 10
Training loss: 3.542426054081131
Validation loss: 2.9047155388196004

Epoch: 6| Step: 11
Training loss: 2.995871723408343
Validation loss: 2.905614929286937

Epoch: 6| Step: 12
Training loss: 3.078568634750115
Validation loss: 2.9084709000842675

Epoch: 6| Step: 13
Training loss: 2.780339124015495
Validation loss: 2.9060083024806382

Epoch: 60| Step: 0
Training loss: 3.275715984091884
Validation loss: 2.9030852723033527

Epoch: 6| Step: 1
Training loss: 2.9293458459639266
Validation loss: 2.8998616946899833

Epoch: 6| Step: 2
Training loss: 3.616489944593811
Validation loss: 2.900929985887288

Epoch: 6| Step: 3
Training loss: 3.283022447251946
Validation loss: 2.899220861735029

Epoch: 6| Step: 4
Training loss: 2.2351507160820936
Validation loss: 2.901507472874929

Epoch: 6| Step: 5
Training loss: 3.9175706320738617
Validation loss: 2.8976141947722405

Epoch: 6| Step: 6
Training loss: 3.309741328985533
Validation loss: 2.8988193041249146

Epoch: 6| Step: 7
Training loss: 3.4062099629420244
Validation loss: 2.8987391203562174

Epoch: 6| Step: 8
Training loss: 3.621351049802176
Validation loss: 2.8969630312079295

Epoch: 6| Step: 9
Training loss: 2.6644617939287762
Validation loss: 2.899551738638927

Epoch: 6| Step: 10
Training loss: 3.471940369080759
Validation loss: 2.896631794877762

Epoch: 6| Step: 11
Training loss: 3.132304542818658
Validation loss: 2.898319025373792

Epoch: 6| Step: 12
Training loss: 2.2168723744714813
Validation loss: 2.897004643477922

Epoch: 6| Step: 13
Training loss: 2.882878000399175
Validation loss: 2.906543707586124

Epoch: 61| Step: 0
Training loss: 3.111781890021679
Validation loss: 2.9137151741480576

Epoch: 6| Step: 1
Training loss: 2.8904217622809285
Validation loss: 2.9147849571476483

Epoch: 6| Step: 2
Training loss: 3.5629754000233955
Validation loss: 2.910889981816589

Epoch: 6| Step: 3
Training loss: 2.803830156545316
Validation loss: 2.896044193103199

Epoch: 6| Step: 4
Training loss: 1.9209880487962865
Validation loss: 2.8982005910172037

Epoch: 6| Step: 5
Training loss: 3.5037374976782605
Validation loss: 2.898541432854435

Epoch: 6| Step: 6
Training loss: 3.1106273486937335
Validation loss: 2.9012935652267524

Epoch: 6| Step: 7
Training loss: 3.253036254493158
Validation loss: 2.8991198978998165

Epoch: 6| Step: 8
Training loss: 3.484105942263947
Validation loss: 2.9028145461211876

Epoch: 6| Step: 9
Training loss: 3.026837942876044
Validation loss: 2.903105415163291

Epoch: 6| Step: 10
Training loss: 3.5629468938706457
Validation loss: 2.8948419986537792

Epoch: 6| Step: 11
Training loss: 3.6678390073732503
Validation loss: 2.8954435416531736

Epoch: 6| Step: 12
Training loss: 3.450028355799812
Validation loss: 2.894216508545281

Epoch: 6| Step: 13
Training loss: 2.9313943595557648
Validation loss: 2.8957439812217753

Epoch: 62| Step: 0
Training loss: 1.9738689660716964
Validation loss: 2.8930053116316086

Epoch: 6| Step: 1
Training loss: 3.3385114187376974
Validation loss: 2.891693441034108

Epoch: 6| Step: 2
Training loss: 3.0551242369817886
Validation loss: 2.8884217437003286

Epoch: 6| Step: 3
Training loss: 3.3572363854919853
Validation loss: 2.893567808880387

Epoch: 6| Step: 4
Training loss: 3.5759702632956287
Validation loss: 2.8909031533219034

Epoch: 6| Step: 5
Training loss: 2.580333988691111
Validation loss: 2.892459920894318

Epoch: 6| Step: 6
Training loss: 3.601836419601381
Validation loss: 2.8901128765910467

Epoch: 6| Step: 7
Training loss: 3.6089902730563885
Validation loss: 2.887436564277437

Epoch: 6| Step: 8
Training loss: 2.4818697599779815
Validation loss: 2.8894921572618233

Epoch: 6| Step: 9
Training loss: 2.6022731967272907
Validation loss: 2.88995731428259

Epoch: 6| Step: 10
Training loss: 3.10062899822461
Validation loss: 2.8900364146574997

Epoch: 6| Step: 11
Training loss: 3.424688706556613
Validation loss: 2.890793631085117

Epoch: 6| Step: 12
Training loss: 3.6358042221658393
Validation loss: 2.8883130478026917

Epoch: 6| Step: 13
Training loss: 3.9058931721788226
Validation loss: 2.892831018128837

Epoch: 63| Step: 0
Training loss: 3.2898233942558477
Validation loss: 2.9009551703329852

Epoch: 6| Step: 1
Training loss: 3.122716603520223
Validation loss: 2.903646285674468

Epoch: 6| Step: 2
Training loss: 2.318192049333703
Validation loss: 2.897415600573389

Epoch: 6| Step: 3
Training loss: 2.938160111948432
Validation loss: 2.8909335365582916

Epoch: 6| Step: 4
Training loss: 3.8425671998342072
Validation loss: 2.8875523247497226

Epoch: 6| Step: 5
Training loss: 2.87916528579377
Validation loss: 2.8816181550827022

Epoch: 6| Step: 6
Training loss: 3.1614370497228554
Validation loss: 2.883872366154226

Epoch: 6| Step: 7
Training loss: 3.7726050301726635
Validation loss: 2.8857847868662683

Epoch: 6| Step: 8
Training loss: 3.350699787212498
Validation loss: 2.88317730057206

Epoch: 6| Step: 9
Training loss: 3.447668398920641
Validation loss: 2.885175315850561

Epoch: 6| Step: 10
Training loss: 2.9844865228876225
Validation loss: 2.884804257545571

Epoch: 6| Step: 11
Training loss: 2.70162234132502
Validation loss: 2.8846958688036484

Epoch: 6| Step: 12
Training loss: 3.127331893635407
Validation loss: 2.8791970928822304

Epoch: 6| Step: 13
Training loss: 3.5010507913591824
Validation loss: 2.8809203743598943

Epoch: 64| Step: 0
Training loss: 3.2100436783430406
Validation loss: 2.878618076191924

Epoch: 6| Step: 1
Training loss: 3.2289604531350578
Validation loss: 2.883267065537756

Epoch: 6| Step: 2
Training loss: 3.7683424898554017
Validation loss: 2.8865181103881303

Epoch: 6| Step: 3
Training loss: 3.2393536419820337
Validation loss: 2.8821081290372534

Epoch: 6| Step: 4
Training loss: 2.7398784947428947
Validation loss: 2.87848757649816

Epoch: 6| Step: 5
Training loss: 3.3649676942904168
Validation loss: 2.8785533489764257

Epoch: 6| Step: 6
Training loss: 3.178926493987366
Validation loss: 2.8795548071633807

Epoch: 6| Step: 7
Training loss: 3.2347078750996117
Validation loss: 2.882281354325102

Epoch: 6| Step: 8
Training loss: 2.972615506147355
Validation loss: 2.87979849076222

Epoch: 6| Step: 9
Training loss: 3.310773867586689
Validation loss: 2.8779505747467398

Epoch: 6| Step: 10
Training loss: 2.9147290651227413
Validation loss: 2.8778920441083082

Epoch: 6| Step: 11
Training loss: 3.5190128533144054
Validation loss: 2.880723307993854

Epoch: 6| Step: 12
Training loss: 2.4169889322894744
Validation loss: 2.886313951971245

Epoch: 6| Step: 13
Training loss: 3.0661644323938417
Validation loss: 2.888015337992184

Epoch: 65| Step: 0
Training loss: 3.4409015818065343
Validation loss: 2.8903742891301185

Epoch: 6| Step: 1
Training loss: 3.270425143354627
Validation loss: 2.8895331726705376

Epoch: 6| Step: 2
Training loss: 3.2802455272889737
Validation loss: 2.896914417464916

Epoch: 6| Step: 3
Training loss: 2.868713139315884
Validation loss: 2.8906491319335843

Epoch: 6| Step: 4
Training loss: 3.807711361123811
Validation loss: 2.8947233751004053

Epoch: 6| Step: 5
Training loss: 2.9404389309365673
Validation loss: 2.9034261534950754

Epoch: 6| Step: 6
Training loss: 3.9100866168640556
Validation loss: 2.896223083765023

Epoch: 6| Step: 7
Training loss: 3.031630659289766
Validation loss: 2.8845963118022278

Epoch: 6| Step: 8
Training loss: 2.6867917036804445
Validation loss: 2.8845173599809217

Epoch: 6| Step: 9
Training loss: 2.9963194203839025
Validation loss: 2.879345886658379

Epoch: 6| Step: 10
Training loss: 3.3350588782745074
Validation loss: 2.8788654117846453

Epoch: 6| Step: 11
Training loss: 3.3966562097130795
Validation loss: 2.8749323868614556

Epoch: 6| Step: 12
Training loss: 2.734813981922089
Validation loss: 2.875073218427615

Epoch: 6| Step: 13
Training loss: 1.465098041079079
Validation loss: 2.876361857871017

Epoch: 66| Step: 0
Training loss: 2.982143826317283
Validation loss: 2.876310787136602

Epoch: 6| Step: 1
Training loss: 3.521738662043984
Validation loss: 2.8763269819161854

Epoch: 6| Step: 2
Training loss: 3.371921406763312
Validation loss: 2.8790203766499953

Epoch: 6| Step: 3
Training loss: 3.1946553718110366
Validation loss: 2.877366250089481

Epoch: 6| Step: 4
Training loss: 3.587238815316766
Validation loss: 2.885929660160315

Epoch: 6| Step: 5
Training loss: 2.5012510983885528
Validation loss: 2.8922298996981897

Epoch: 6| Step: 6
Training loss: 2.8459909852475556
Validation loss: 2.8842849108190847

Epoch: 6| Step: 7
Training loss: 2.7748692215354502
Validation loss: 2.8742653441781782

Epoch: 6| Step: 8
Training loss: 3.0715213172318165
Validation loss: 2.868993744966577

Epoch: 6| Step: 9
Training loss: 3.763292025217473
Validation loss: 2.8715680911767474

Epoch: 6| Step: 10
Training loss: 2.356356679590897
Validation loss: 2.869662335517385

Epoch: 6| Step: 11
Training loss: 3.945917245576185
Validation loss: 2.8707247526736404

Epoch: 6| Step: 12
Training loss: 3.1181513699142704
Validation loss: 2.868696672776488

Epoch: 6| Step: 13
Training loss: 2.6393984140978577
Validation loss: 2.8696243960675543

Epoch: 67| Step: 0
Training loss: 3.108880391870133
Validation loss: 2.870047412026349

Epoch: 6| Step: 1
Training loss: 3.326380886067369
Validation loss: 2.8725618579167667

Epoch: 6| Step: 2
Training loss: 2.551711188969638
Validation loss: 2.870466478836519

Epoch: 6| Step: 3
Training loss: 3.4205860514752255
Validation loss: 2.8707995259162975

Epoch: 6| Step: 4
Training loss: 2.618988328526822
Validation loss: 2.868410066928885

Epoch: 6| Step: 5
Training loss: 3.319201330613214
Validation loss: 2.8715341015582454

Epoch: 6| Step: 6
Training loss: 3.279095041994604
Validation loss: 2.872686313835041

Epoch: 6| Step: 7
Training loss: 3.2388510576601166
Validation loss: 2.868542335259836

Epoch: 6| Step: 8
Training loss: 3.223167129250144
Validation loss: 2.8699596016357236

Epoch: 6| Step: 9
Training loss: 2.865500970151035
Validation loss: 2.867899663627511

Epoch: 6| Step: 10
Training loss: 3.224523754588992
Validation loss: 2.874113590938496

Epoch: 6| Step: 11
Training loss: 3.1698833658320167
Validation loss: 2.890408750791303

Epoch: 6| Step: 12
Training loss: 3.9278743497015607
Validation loss: 2.9096630272271637

Epoch: 6| Step: 13
Training loss: 2.559923970613394
Validation loss: 2.871061101296291

Epoch: 68| Step: 0
Training loss: 3.0546806315859483
Validation loss: 2.866396138685002

Epoch: 6| Step: 1
Training loss: 3.023464311190069
Validation loss: 2.8747730138771224

Epoch: 6| Step: 2
Training loss: 3.4731631686749522
Validation loss: 2.8801476440788534

Epoch: 6| Step: 3
Training loss: 3.2833881496236272
Validation loss: 2.8772882661863197

Epoch: 6| Step: 4
Training loss: 2.990077457509136
Validation loss: 2.871659055530049

Epoch: 6| Step: 5
Training loss: 3.1236340398433624
Validation loss: 2.8700754864166016

Epoch: 6| Step: 6
Training loss: 3.385996106762346
Validation loss: 2.869369872174861

Epoch: 6| Step: 7
Training loss: 2.3781869486736733
Validation loss: 2.8676068542284807

Epoch: 6| Step: 8
Training loss: 3.1028979138649064
Validation loss: 2.864021675482135

Epoch: 6| Step: 9
Training loss: 3.052153255629476
Validation loss: 2.8657583532134656

Epoch: 6| Step: 10
Training loss: 3.665994033384072
Validation loss: 2.8625347776763657

Epoch: 6| Step: 11
Training loss: 3.485288218069303
Validation loss: 2.8628208219023157

Epoch: 6| Step: 12
Training loss: 2.676231357557639
Validation loss: 2.864029307257803

Epoch: 6| Step: 13
Training loss: 3.596125207564927
Validation loss: 2.8617192877186386

Epoch: 69| Step: 0
Training loss: 3.2163275334026578
Validation loss: 2.8631594474926785

Epoch: 6| Step: 1
Training loss: 3.171550264318944
Validation loss: 2.8648426130235447

Epoch: 6| Step: 2
Training loss: 2.992617424339756
Validation loss: 2.8627111288741665

Epoch: 6| Step: 3
Training loss: 2.996022130316577
Validation loss: 2.861960333592064

Epoch: 6| Step: 4
Training loss: 3.3867992844735495
Validation loss: 2.8630955303517784

Epoch: 6| Step: 5
Training loss: 3.281243024546157
Validation loss: 2.860059174286122

Epoch: 6| Step: 6
Training loss: 3.1633061764847428
Validation loss: 2.862272248443778

Epoch: 6| Step: 7
Training loss: 3.2822920824779236
Validation loss: 2.862774775198792

Epoch: 6| Step: 8
Training loss: 3.2745178916117332
Validation loss: 2.864529438723712

Epoch: 6| Step: 9
Training loss: 2.829152631841876
Validation loss: 2.861024725504881

Epoch: 6| Step: 10
Training loss: 3.1699946801174064
Validation loss: 2.8611531831147827

Epoch: 6| Step: 11
Training loss: 3.4262376261629983
Validation loss: 2.861728857058161

Epoch: 6| Step: 12
Training loss: 2.981855037560823
Validation loss: 2.861716284863692

Epoch: 6| Step: 13
Training loss: 2.710104162881832
Validation loss: 2.8638000380911093

Epoch: 70| Step: 0
Training loss: 3.145449297034114
Validation loss: 2.8735142132657905

Epoch: 6| Step: 1
Training loss: 2.849051428092689
Validation loss: 2.893700604356172

Epoch: 6| Step: 2
Training loss: 3.048525006452435
Validation loss: 2.9075912060677265

Epoch: 6| Step: 3
Training loss: 2.938833055965405
Validation loss: 2.9234134996782695

Epoch: 6| Step: 4
Training loss: 3.276751386285199
Validation loss: 2.922580181381119

Epoch: 6| Step: 5
Training loss: 3.4381548171145164
Validation loss: 2.910720684787949

Epoch: 6| Step: 6
Training loss: 2.688666689267936
Validation loss: 2.865320594456062

Epoch: 6| Step: 7
Training loss: 3.315425516611883
Validation loss: 2.853660549101342

Epoch: 6| Step: 8
Training loss: 3.259587159022692
Validation loss: 2.8552246671758343

Epoch: 6| Step: 9
Training loss: 3.321070685493893
Validation loss: 2.8557759508542344

Epoch: 6| Step: 10
Training loss: 3.2782362403577956
Validation loss: 2.849480867519279

Epoch: 6| Step: 11
Training loss: 3.374796260759417
Validation loss: 2.8584037129437956

Epoch: 6| Step: 12
Training loss: 2.9916325545070745
Validation loss: 2.8592618519441624

Epoch: 6| Step: 13
Training loss: 3.1790094425892756
Validation loss: 2.866689193371889

Epoch: 71| Step: 0
Training loss: 3.0216765882017427
Validation loss: 2.8503306417441694

Epoch: 6| Step: 1
Training loss: 2.720603398825108
Validation loss: 2.853115959653194

Epoch: 6| Step: 2
Training loss: 3.4244299978151873
Validation loss: 2.851047001665839

Epoch: 6| Step: 3
Training loss: 3.5652136924012496
Validation loss: 2.8540986887363116

Epoch: 6| Step: 4
Training loss: 3.164806287963298
Validation loss: 2.8489120979664415

Epoch: 6| Step: 5
Training loss: 3.0519993654402926
Validation loss: 2.8489406091948184

Epoch: 6| Step: 6
Training loss: 3.717651589643653
Validation loss: 2.8492504474857308

Epoch: 6| Step: 7
Training loss: 3.7532128875631496
Validation loss: 2.851421913393271

Epoch: 6| Step: 8
Training loss: 2.904658712538897
Validation loss: 2.8518334772849836

Epoch: 6| Step: 9
Training loss: 2.370547487785407
Validation loss: 2.8491765204728625

Epoch: 6| Step: 10
Training loss: 2.665291332188099
Validation loss: 2.8499659257200056

Epoch: 6| Step: 11
Training loss: 3.080415754372711
Validation loss: 2.8472714037131985

Epoch: 6| Step: 12
Training loss: 3.4472866500775265
Validation loss: 2.8481246221186476

Epoch: 6| Step: 13
Training loss: 2.6647040973306146
Validation loss: 2.852688916922277

Epoch: 72| Step: 0
Training loss: 2.830038192814812
Validation loss: 2.8513235754277915

Epoch: 6| Step: 1
Training loss: 4.0255270855690926
Validation loss: 2.871241355652731

Epoch: 6| Step: 2
Training loss: 3.1469338283703396
Validation loss: 2.8524380910533615

Epoch: 6| Step: 3
Training loss: 3.015776161002584
Validation loss: 2.8498814312590803

Epoch: 6| Step: 4
Training loss: 2.8797904200591327
Validation loss: 2.846587074749043

Epoch: 6| Step: 5
Training loss: 3.1337323063281284
Validation loss: 2.8474305378237363

Epoch: 6| Step: 6
Training loss: 2.954915629052189
Validation loss: 2.848197536827881

Epoch: 6| Step: 7
Training loss: 3.0452031170228664
Validation loss: 2.84693470371987

Epoch: 6| Step: 8
Training loss: 2.787866587118986
Validation loss: 2.84576581483506

Epoch: 6| Step: 9
Training loss: 3.364371766484447
Validation loss: 2.850093937297037

Epoch: 6| Step: 10
Training loss: 2.7807374974938326
Validation loss: 2.848526462861784

Epoch: 6| Step: 11
Training loss: 3.1873294560304393
Validation loss: 2.8481061354899935

Epoch: 6| Step: 12
Training loss: 3.384056937223444
Validation loss: 2.851505182656363

Epoch: 6| Step: 13
Training loss: 3.377939710025967
Validation loss: 2.8478644913105806

Epoch: 73| Step: 0
Training loss: 2.902004627731493
Validation loss: 2.8448654947500303

Epoch: 6| Step: 1
Training loss: 3.33535082480265
Validation loss: 2.861818700863603

Epoch: 6| Step: 2
Training loss: 2.839998222672887
Validation loss: 2.8519497992578886

Epoch: 6| Step: 3
Training loss: 3.6949091039893207
Validation loss: 2.8588439082387174

Epoch: 6| Step: 4
Training loss: 3.4518734143896017
Validation loss: 2.8468280493042375

Epoch: 6| Step: 5
Training loss: 3.010623242815458
Validation loss: 2.844208700297424

Epoch: 6| Step: 6
Training loss: 3.8790749074297564
Validation loss: 2.8427092134955645

Epoch: 6| Step: 7
Training loss: 2.437906084713039
Validation loss: 2.8421963469756104

Epoch: 6| Step: 8
Training loss: 2.556174402588081
Validation loss: 2.8426878364353203

Epoch: 6| Step: 9
Training loss: 3.1285394079681135
Validation loss: 2.840598795272993

Epoch: 6| Step: 10
Training loss: 3.2696095665743443
Validation loss: 2.844138429823639

Epoch: 6| Step: 11
Training loss: 3.1618042975998373
Validation loss: 2.837734413267047

Epoch: 6| Step: 12
Training loss: 3.163135684812567
Validation loss: 2.840688063763077

Epoch: 6| Step: 13
Training loss: 2.4681241171616235
Validation loss: 2.8443389664912537

Epoch: 74| Step: 0
Training loss: 3.551285878865938
Validation loss: 2.8396507972070943

Epoch: 6| Step: 1
Training loss: 2.7144427325787714
Validation loss: 2.840114139362989

Epoch: 6| Step: 2
Training loss: 2.926406365754643
Validation loss: 2.8445912094743906

Epoch: 6| Step: 3
Training loss: 2.719763731231657
Validation loss: 2.841553483536818

Epoch: 6| Step: 4
Training loss: 4.013225625283681
Validation loss: 2.843496082990899

Epoch: 6| Step: 5
Training loss: 3.0105655901723005
Validation loss: 2.8436030589150585

Epoch: 6| Step: 6
Training loss: 1.8260217288710126
Validation loss: 2.8394575134945588

Epoch: 6| Step: 7
Training loss: 3.5500119491161137
Validation loss: 2.8376461829651323

Epoch: 6| Step: 8
Training loss: 4.012687588403676
Validation loss: 2.84257797705697

Epoch: 6| Step: 9
Training loss: 2.838844847747069
Validation loss: 2.8466046274184764

Epoch: 6| Step: 10
Training loss: 3.2331021517925755
Validation loss: 2.8437988635848934

Epoch: 6| Step: 11
Training loss: 2.8661971291724453
Validation loss: 2.8427702108230943

Epoch: 6| Step: 12
Training loss: 3.0452644982427737
Validation loss: 2.836745580449402

Epoch: 6| Step: 13
Training loss: 2.7620940863890624
Validation loss: 2.840953872566366

Epoch: 75| Step: 0
Training loss: 3.2308790691274987
Validation loss: 2.835634440522701

Epoch: 6| Step: 1
Training loss: 3.0648343278874735
Validation loss: 2.8338569492661954

Epoch: 6| Step: 2
Training loss: 3.5823286259637617
Validation loss: 2.8311467362204104

Epoch: 6| Step: 3
Training loss: 3.099226910575814
Validation loss: 2.834586433405971

Epoch: 6| Step: 4
Training loss: 3.378249123240007
Validation loss: 2.833312127421849

Epoch: 6| Step: 5
Training loss: 2.9268703903627884
Validation loss: 2.8336948409053075

Epoch: 6| Step: 6
Training loss: 3.26753098932119
Validation loss: 2.8379217799543675

Epoch: 6| Step: 7
Training loss: 3.2508357514000252
Validation loss: 2.834588953103219

Epoch: 6| Step: 8
Training loss: 2.64221576876325
Validation loss: 2.838843419109463

Epoch: 6| Step: 9
Training loss: 2.9339695269478505
Validation loss: 2.843208502133628

Epoch: 6| Step: 10
Training loss: 2.5413406246330217
Validation loss: 2.835427542383186

Epoch: 6| Step: 11
Training loss: 3.45156093060314
Validation loss: 2.8366347453318936

Epoch: 6| Step: 12
Training loss: 3.443578166793998
Validation loss: 2.844438913173429

Epoch: 6| Step: 13
Training loss: 2.5962554596580434
Validation loss: 2.83838918606958

Epoch: 76| Step: 0
Training loss: 3.2410933889030136
Validation loss: 2.8412101371908824

Epoch: 6| Step: 1
Training loss: 3.232436921698363
Validation loss: 2.8358796978035756

Epoch: 6| Step: 2
Training loss: 2.7941231429338287
Validation loss: 2.8314292404214294

Epoch: 6| Step: 3
Training loss: 3.323682802345234
Validation loss: 2.8304227112216283

Epoch: 6| Step: 4
Training loss: 2.708040916111224
Validation loss: 2.826472225792986

Epoch: 6| Step: 5
Training loss: 2.9245809988543425
Validation loss: 2.8275252087887925

Epoch: 6| Step: 6
Training loss: 2.728571722591273
Validation loss: 2.8266362902022375

Epoch: 6| Step: 7
Training loss: 2.885737395564972
Validation loss: 2.827463128210071

Epoch: 6| Step: 8
Training loss: 3.0295287288871897
Validation loss: 2.8264861646995074

Epoch: 6| Step: 9
Training loss: 2.788172988735151
Validation loss: 2.8280208815270043

Epoch: 6| Step: 10
Training loss: 3.761207236810392
Validation loss: 2.827009302892143

Epoch: 6| Step: 11
Training loss: 3.321308300811674
Validation loss: 2.826554813355398

Epoch: 6| Step: 12
Training loss: 3.73123739422533
Validation loss: 2.8262398310201338

Epoch: 6| Step: 13
Training loss: 3.076243178737104
Validation loss: 2.826530032623691

Epoch: 77| Step: 0
Training loss: 2.972947536237842
Validation loss: 2.8267718372797455

Epoch: 6| Step: 1
Training loss: 2.684020340286704
Validation loss: 2.828023549396567

Epoch: 6| Step: 2
Training loss: 2.648738337622077
Validation loss: 2.822821829668364

Epoch: 6| Step: 3
Training loss: 2.8822295470460424
Validation loss: 2.827096120455235

Epoch: 6| Step: 4
Training loss: 3.7427567783656412
Validation loss: 2.831578968044476

Epoch: 6| Step: 5
Training loss: 3.700701858348598
Validation loss: 2.822553059901671

Epoch: 6| Step: 6
Training loss: 3.3167025699940558
Validation loss: 2.83172630764481

Epoch: 6| Step: 7
Training loss: 2.364825893066253
Validation loss: 2.8291145641651947

Epoch: 6| Step: 8
Training loss: 3.4169588351741105
Validation loss: 2.8355573567025663

Epoch: 6| Step: 9
Training loss: 3.2695818570138835
Validation loss: 2.836053274416359

Epoch: 6| Step: 10
Training loss: 2.7153707088904895
Validation loss: 2.8265157338558096

Epoch: 6| Step: 11
Training loss: 2.968594035017043
Validation loss: 2.8398712500364205

Epoch: 6| Step: 12
Training loss: 3.493961847805568
Validation loss: 2.843546887500097

Epoch: 6| Step: 13
Training loss: 3.4902474905321563
Validation loss: 2.8295045990250385

Epoch: 78| Step: 0
Training loss: 2.8206037122691057
Validation loss: 2.831671048610832

Epoch: 6| Step: 1
Training loss: 3.048626987779928
Validation loss: 2.82506965735003

Epoch: 6| Step: 2
Training loss: 3.016764845400829
Validation loss: 2.819907306025382

Epoch: 6| Step: 3
Training loss: 3.88339763200653
Validation loss: 2.8211278058787657

Epoch: 6| Step: 4
Training loss: 2.53984963760468
Validation loss: 2.818978519179188

Epoch: 6| Step: 5
Training loss: 2.504465692304581
Validation loss: 2.8196681185300627

Epoch: 6| Step: 6
Training loss: 3.9464320039148575
Validation loss: 2.821624461616112

Epoch: 6| Step: 7
Training loss: 2.7558131764930853
Validation loss: 2.823440808842921

Epoch: 6| Step: 8
Training loss: 2.845872694630413
Validation loss: 2.8240897507276044

Epoch: 6| Step: 9
Training loss: 2.435055215824432
Validation loss: 2.819354219958223

Epoch: 6| Step: 10
Training loss: 3.7564330553080847
Validation loss: 2.8213125153572998

Epoch: 6| Step: 11
Training loss: 3.282872262199215
Validation loss: 2.817951272818811

Epoch: 6| Step: 12
Training loss: 3.4692723336317
Validation loss: 2.8192536486028326

Epoch: 6| Step: 13
Training loss: 2.7895101407063856
Validation loss: 2.828419681512311

Epoch: 79| Step: 0
Training loss: 2.6102631479368226
Validation loss: 2.8259073021025447

Epoch: 6| Step: 1
Training loss: 3.386740995727833
Validation loss: 2.8378572925780734

Epoch: 6| Step: 2
Training loss: 3.058748399666265
Validation loss: 2.8474211329115007

Epoch: 6| Step: 3
Training loss: 3.255127163701547
Validation loss: 2.8478536043006106

Epoch: 6| Step: 4
Training loss: 3.011481249755169
Validation loss: 2.8297423589565947

Epoch: 6| Step: 5
Training loss: 3.302005482105901
Validation loss: 2.8191379163141885

Epoch: 6| Step: 6
Training loss: 2.606211040397739
Validation loss: 2.8204510705614894

Epoch: 6| Step: 7
Training loss: 3.3442423805571497
Validation loss: 2.8193490842234525

Epoch: 6| Step: 8
Training loss: 2.6099101905425317
Validation loss: 2.812973802005386

Epoch: 6| Step: 9
Training loss: 3.3663734106877605
Validation loss: 2.8154675286373676

Epoch: 6| Step: 10
Training loss: 3.2224399193489206
Validation loss: 2.8111507447132387

Epoch: 6| Step: 11
Training loss: 3.1803439725623197
Validation loss: 2.8132827890872503

Epoch: 6| Step: 12
Training loss: 2.8745644280908222
Validation loss: 2.812750543808307

Epoch: 6| Step: 13
Training loss: 4.066457141627388
Validation loss: 2.823192981742883

Epoch: 80| Step: 0
Training loss: 3.14828225787205
Validation loss: 2.815338352389592

Epoch: 6| Step: 1
Training loss: 3.0887728642628995
Validation loss: 2.8140484210619268

Epoch: 6| Step: 2
Training loss: 3.6331548057815315
Validation loss: 2.812323257744835

Epoch: 6| Step: 3
Training loss: 3.4289212076830426
Validation loss: 2.811102237452596

Epoch: 6| Step: 4
Training loss: 2.9241824906853138
Validation loss: 2.810620603126185

Epoch: 6| Step: 5
Training loss: 3.301368002120455
Validation loss: 2.813613073391982

Epoch: 6| Step: 6
Training loss: 3.342912087670137
Validation loss: 2.8101106956675665

Epoch: 6| Step: 7
Training loss: 2.4207361312322377
Validation loss: 2.810186985024581

Epoch: 6| Step: 8
Training loss: 3.2037096932056506
Validation loss: 2.808969102036915

Epoch: 6| Step: 9
Training loss: 2.4628277981615394
Validation loss: 2.812647531484292

Epoch: 6| Step: 10
Training loss: 2.6084070294808885
Validation loss: 2.813278693868154

Epoch: 6| Step: 11
Training loss: 3.3129095148268353
Validation loss: 2.8114136323922816

Epoch: 6| Step: 12
Training loss: 3.0205662225600363
Validation loss: 2.8152958544740594

Epoch: 6| Step: 13
Training loss: 3.697614472985592
Validation loss: 2.8147029858167922

Epoch: 81| Step: 0
Training loss: 3.083722751529518
Validation loss: 2.812130065798729

Epoch: 6| Step: 1
Training loss: 3.6133833917150917
Validation loss: 2.8108117390530993

Epoch: 6| Step: 2
Training loss: 2.5752353079232035
Validation loss: 2.811961383358142

Epoch: 6| Step: 3
Training loss: 2.5315235131452805
Validation loss: 2.811324276939503

Epoch: 6| Step: 4
Training loss: 3.4805641032813606
Validation loss: 2.8083988464193186

Epoch: 6| Step: 5
Training loss: 3.3968544265418834
Validation loss: 2.811628166896192

Epoch: 6| Step: 6
Training loss: 3.35377624956423
Validation loss: 2.812262153367222

Epoch: 6| Step: 7
Training loss: 2.4489482140071717
Validation loss: 2.805491099440763

Epoch: 6| Step: 8
Training loss: 2.3657369166398112
Validation loss: 2.80540896278806

Epoch: 6| Step: 9
Training loss: 3.375569048345254
Validation loss: 2.80488604049876

Epoch: 6| Step: 10
Training loss: 3.292107629010815
Validation loss: 2.80882937176505

Epoch: 6| Step: 11
Training loss: 3.3962041422275537
Validation loss: 2.8133399174099067

Epoch: 6| Step: 12
Training loss: 3.26541180804551
Validation loss: 2.8080595377483135

Epoch: 6| Step: 13
Training loss: 2.9458416936414933
Validation loss: 2.808048061841837

Epoch: 82| Step: 0
Training loss: 3.049518397095455
Validation loss: 2.820669738380265

Epoch: 6| Step: 1
Training loss: 3.090245892268545
Validation loss: 2.8183236757635965

Epoch: 6| Step: 2
Training loss: 3.3689184143206305
Validation loss: 2.8223788537643553

Epoch: 6| Step: 3
Training loss: 2.8916998437156334
Validation loss: 2.8114657652772492

Epoch: 6| Step: 4
Training loss: 2.9021247380669726
Validation loss: 2.8075780918561306

Epoch: 6| Step: 5
Training loss: 3.226374807740789
Validation loss: 2.8116208360201456

Epoch: 6| Step: 6
Training loss: 3.121966300904668
Validation loss: 2.8046506436603615

Epoch: 6| Step: 7
Training loss: 3.020817846499679
Validation loss: 2.8104075643342967

Epoch: 6| Step: 8
Training loss: 2.806461973616726
Validation loss: 2.8225525930507858

Epoch: 6| Step: 9
Training loss: 3.603218806584946
Validation loss: 2.814575539697132

Epoch: 6| Step: 10
Training loss: 2.5302329198198414
Validation loss: 2.80601946653976

Epoch: 6| Step: 11
Training loss: 3.2634591399098793
Validation loss: 2.8053278454117048

Epoch: 6| Step: 12
Training loss: 3.0645071186587343
Validation loss: 2.79813296819155

Epoch: 6| Step: 13
Training loss: 3.607530791533904
Validation loss: 2.7989882675747597

Epoch: 83| Step: 0
Training loss: 3.1663511687936783
Validation loss: 2.799627690459289

Epoch: 6| Step: 1
Training loss: 2.3683641990173325
Validation loss: 2.8000699753081943

Epoch: 6| Step: 2
Training loss: 2.9317578947335536
Validation loss: 2.794883979498091

Epoch: 6| Step: 3
Training loss: 3.2935707926632123
Validation loss: 2.79718063938437

Epoch: 6| Step: 4
Training loss: 3.3948881710101366
Validation loss: 2.795405907830768

Epoch: 6| Step: 5
Training loss: 3.4757131160438233
Validation loss: 2.7997567931112894

Epoch: 6| Step: 6
Training loss: 3.699585845037886
Validation loss: 2.8032540295986283

Epoch: 6| Step: 7
Training loss: 2.7676737465286836
Validation loss: 2.798591145465996

Epoch: 6| Step: 8
Training loss: 2.8621689117724345
Validation loss: 2.8064415490928636

Epoch: 6| Step: 9
Training loss: 2.9429938703882104
Validation loss: 2.8120659569604025

Epoch: 6| Step: 10
Training loss: 2.961546339582851
Validation loss: 2.8050162245296355

Epoch: 6| Step: 11
Training loss: 3.751334143778233
Validation loss: 2.807082488932251

Epoch: 6| Step: 12
Training loss: 2.870485825307454
Validation loss: 2.80200122609724

Epoch: 6| Step: 13
Training loss: 2.2534288135403995
Validation loss: 2.8011023405346696

Epoch: 84| Step: 0
Training loss: 2.9809190802380474
Validation loss: 2.797302522316044

Epoch: 6| Step: 1
Training loss: 3.588070968223769
Validation loss: 2.8038003280290886

Epoch: 6| Step: 2
Training loss: 2.8843318584683333
Validation loss: 2.7971274703712843

Epoch: 6| Step: 3
Training loss: 2.535692249409581
Validation loss: 2.794006240324398

Epoch: 6| Step: 4
Training loss: 2.7934919814197334
Validation loss: 2.799754544235978

Epoch: 6| Step: 5
Training loss: 2.1426157361244194
Validation loss: 2.8027458241849743

Epoch: 6| Step: 6
Training loss: 3.147525626904646
Validation loss: 2.801214726156194

Epoch: 6| Step: 7
Training loss: 3.308821885351633
Validation loss: 2.7972097310550126

Epoch: 6| Step: 8
Training loss: 3.247325677223502
Validation loss: 2.7920893785449583

Epoch: 6| Step: 9
Training loss: 2.9304533504202483
Validation loss: 2.7954949632515573

Epoch: 6| Step: 10
Training loss: 3.23609119531036
Validation loss: 2.79069215212155

Epoch: 6| Step: 11
Training loss: 3.500317422914274
Validation loss: 2.794639341459123

Epoch: 6| Step: 12
Training loss: 3.5369182094773164
Validation loss: 2.7930424832037666

Epoch: 6| Step: 13
Training loss: 3.323468599949618
Validation loss: 2.794704977440964

Epoch: 85| Step: 0
Training loss: 3.3595508618121896
Validation loss: 2.790942244251565

Epoch: 6| Step: 1
Training loss: 3.502648577591744
Validation loss: 2.794442051796426

Epoch: 6| Step: 2
Training loss: 2.6350703759540215
Validation loss: 2.800521521152167

Epoch: 6| Step: 3
Training loss: 3.277415041778978
Validation loss: 2.8041787690600257

Epoch: 6| Step: 4
Training loss: 3.859207087891113
Validation loss: 2.811808201377663

Epoch: 6| Step: 5
Training loss: 2.578282761804239
Validation loss: 2.8058244919541147

Epoch: 6| Step: 6
Training loss: 3.30998220663676
Validation loss: 2.8117162289185154

Epoch: 6| Step: 7
Training loss: 2.4702663352170626
Validation loss: 2.7900865429961623

Epoch: 6| Step: 8
Training loss: 2.8330148817130003
Validation loss: 2.7904970749163587

Epoch: 6| Step: 9
Training loss: 3.3989246238527873
Validation loss: 2.7893187417616274

Epoch: 6| Step: 10
Training loss: 2.856544469432809
Validation loss: 2.787435539019349

Epoch: 6| Step: 11
Training loss: 2.6709865409086375
Validation loss: 2.7882836640281377

Epoch: 6| Step: 12
Training loss: 3.173773286785861
Validation loss: 2.7894035028175117

Epoch: 6| Step: 13
Training loss: 3.089724609547232
Validation loss: 2.7887795539372098

Epoch: 86| Step: 0
Training loss: 3.383979719331179
Validation loss: 2.787128113522402

Epoch: 6| Step: 1
Training loss: 3.364659894105633
Validation loss: 2.7855207655183736

Epoch: 6| Step: 2
Training loss: 2.579089544947863
Validation loss: 2.7855775732459005

Epoch: 6| Step: 3
Training loss: 3.0441262390587833
Validation loss: 2.792081882513347

Epoch: 6| Step: 4
Training loss: 3.3276025782937757
Validation loss: 2.7951302809877734

Epoch: 6| Step: 5
Training loss: 2.91544307292432
Validation loss: 2.7916142993387165

Epoch: 6| Step: 6
Training loss: 3.1418169331387564
Validation loss: 2.7984551687467167

Epoch: 6| Step: 7
Training loss: 2.9253625302742763
Validation loss: 2.8104342732864342

Epoch: 6| Step: 8
Training loss: 3.4622783603722778
Validation loss: 2.811978948858914

Epoch: 6| Step: 9
Training loss: 2.4228650438271147
Validation loss: 2.81252652409852

Epoch: 6| Step: 10
Training loss: 3.4869796528814696
Validation loss: 2.8049557680065074

Epoch: 6| Step: 11
Training loss: 3.3104981905527935
Validation loss: 2.7992217497822014

Epoch: 6| Step: 12
Training loss: 2.867034492255919
Validation loss: 2.795980243800492

Epoch: 6| Step: 13
Training loss: 2.772583650061461
Validation loss: 2.793944079285535

Epoch: 87| Step: 0
Training loss: 3.691403795675441
Validation loss: 2.7863593802229842

Epoch: 6| Step: 1
Training loss: 2.76212550598104
Validation loss: 2.7900514026919554

Epoch: 6| Step: 2
Training loss: 2.4549433292599505
Validation loss: 2.7882887034370785

Epoch: 6| Step: 3
Training loss: 2.8253954787801354
Validation loss: 2.7855432733893815

Epoch: 6| Step: 4
Training loss: 3.5435751466486907
Validation loss: 2.7902379525471233

Epoch: 6| Step: 5
Training loss: 3.141331531515506
Validation loss: 2.785712849661972

Epoch: 6| Step: 6
Training loss: 3.0756534781452465
Validation loss: 2.792602496231536

Epoch: 6| Step: 7
Training loss: 3.669779727572401
Validation loss: 2.7925804074153895

Epoch: 6| Step: 8
Training loss: 3.177855017493251
Validation loss: 2.7983124344065065

Epoch: 6| Step: 9
Training loss: 3.2234851857669344
Validation loss: 2.789474913184444

Epoch: 6| Step: 10
Training loss: 2.546304650598044
Validation loss: 2.7833855156971135

Epoch: 6| Step: 11
Training loss: 1.840180384459924
Validation loss: 2.7798098164884197

Epoch: 6| Step: 12
Training loss: 3.2410268889350187
Validation loss: 2.784510595538697

Epoch: 6| Step: 13
Training loss: 3.9176902786132324
Validation loss: 2.7802221545036465

Epoch: 88| Step: 0
Training loss: 3.0645660904728382
Validation loss: 2.783090477221553

Epoch: 6| Step: 1
Training loss: 2.6692143628109433
Validation loss: 2.786841399058393

Epoch: 6| Step: 2
Training loss: 3.181149008596199
Validation loss: 2.7803159433201285

Epoch: 6| Step: 3
Training loss: 2.349646919185184
Validation loss: 2.7814662674592587

Epoch: 6| Step: 4
Training loss: 3.2752715369488175
Validation loss: 2.7831489907458185

Epoch: 6| Step: 5
Training loss: 3.1028871566115708
Validation loss: 2.7786488312391397

Epoch: 6| Step: 6
Training loss: 2.8612576316508402
Validation loss: 2.7824960433554002

Epoch: 6| Step: 7
Training loss: 3.324925850636589
Validation loss: 2.781723955454241

Epoch: 6| Step: 8
Training loss: 3.531279167122354
Validation loss: 2.7792287433990617

Epoch: 6| Step: 9
Training loss: 3.142718324444123
Validation loss: 2.7892317373145947

Epoch: 6| Step: 10
Training loss: 3.423215977837275
Validation loss: 2.7837952980636356

Epoch: 6| Step: 11
Training loss: 2.5302564766895843
Validation loss: 2.780590103063899

Epoch: 6| Step: 12
Training loss: 3.2666927946434137
Validation loss: 2.7771855523532882

Epoch: 6| Step: 13
Training loss: 3.3304183293963683
Validation loss: 2.7788592538183083

Epoch: 89| Step: 0
Training loss: 3.6728701277864344
Validation loss: 2.7738472159427774

Epoch: 6| Step: 1
Training loss: 2.257998554523232
Validation loss: 2.7778185768275963

Epoch: 6| Step: 2
Training loss: 2.7893125277734643
Validation loss: 2.7801056226966554

Epoch: 6| Step: 3
Training loss: 3.122580239919597
Validation loss: 2.7754021528074366

Epoch: 6| Step: 4
Training loss: 3.0363781211208507
Validation loss: 2.7733975518651417

Epoch: 6| Step: 5
Training loss: 3.2451271793970458
Validation loss: 2.777167007062972

Epoch: 6| Step: 6
Training loss: 2.798563186660077
Validation loss: 2.771092269447773

Epoch: 6| Step: 7
Training loss: 2.70999023421664
Validation loss: 2.7776873848334875

Epoch: 6| Step: 8
Training loss: 3.456333654677814
Validation loss: 2.7741289570074446

Epoch: 6| Step: 9
Training loss: 2.913375814493319
Validation loss: 2.7748715413921468

Epoch: 6| Step: 10
Training loss: 2.703755123364607
Validation loss: 2.7751993155226784

Epoch: 6| Step: 11
Training loss: 3.8765612502674576
Validation loss: 2.7745885017867176

Epoch: 6| Step: 12
Training loss: 2.871850652249041
Validation loss: 2.7765177053642587

Epoch: 6| Step: 13
Training loss: 3.4501906134980738
Validation loss: 2.7787544453842723

Epoch: 90| Step: 0
Training loss: 3.100098848305114
Validation loss: 2.7881758464455744

Epoch: 6| Step: 1
Training loss: 3.055727886388023
Validation loss: 2.7793804714155916

Epoch: 6| Step: 2
Training loss: 2.786503662501238
Validation loss: 2.770668625051496

Epoch: 6| Step: 3
Training loss: 3.0500906383616995
Validation loss: 2.773899622327868

Epoch: 6| Step: 4
Training loss: 3.377182431485914
Validation loss: 2.768032903087218

Epoch: 6| Step: 5
Training loss: 3.3844101725236184
Validation loss: 2.7691503680260086

Epoch: 6| Step: 6
Training loss: 3.199643061519577
Validation loss: 2.774942941401458

Epoch: 6| Step: 7
Training loss: 2.6215192332910964
Validation loss: 2.773359051660391

Epoch: 6| Step: 8
Training loss: 2.7487811074918818
Validation loss: 2.77101709825257

Epoch: 6| Step: 9
Training loss: 3.0610500718077542
Validation loss: 2.7726488946378445

Epoch: 6| Step: 10
Training loss: 3.550976234848522
Validation loss: 2.773868903566966

Epoch: 6| Step: 11
Training loss: 3.2870554112103414
Validation loss: 2.77590568468163

Epoch: 6| Step: 12
Training loss: 3.012555868176841
Validation loss: 2.778552205274122

Epoch: 6| Step: 13
Training loss: 2.388459065924038
Validation loss: 2.7671994320373337

Epoch: 91| Step: 0
Training loss: 3.03283351161779
Validation loss: 2.773815740582372

Epoch: 6| Step: 1
Training loss: 3.614951049979792
Validation loss: 2.772224466262102

Epoch: 6| Step: 2
Training loss: 3.4337395646832753
Validation loss: 2.7720809417373617

Epoch: 6| Step: 3
Training loss: 2.996998238838908
Validation loss: 2.769626711534253

Epoch: 6| Step: 4
Training loss: 2.378191359768334
Validation loss: 2.771548073905013

Epoch: 6| Step: 5
Training loss: 2.8514183530561605
Validation loss: 2.771870913746883

Epoch: 6| Step: 6
Training loss: 3.550951392332828
Validation loss: 2.7709693039544185

Epoch: 6| Step: 7
Training loss: 3.433162935590091
Validation loss: 2.771218422915041

Epoch: 6| Step: 8
Training loss: 2.4669023190733284
Validation loss: 2.767763383964489

Epoch: 6| Step: 9
Training loss: 3.136110619754338
Validation loss: 2.765777264822723

Epoch: 6| Step: 10
Training loss: 3.0064909967274676
Validation loss: 2.767756523222768

Epoch: 6| Step: 11
Training loss: 2.8765324364644713
Validation loss: 2.766328725241053

Epoch: 6| Step: 12
Training loss: 2.8938570666022585
Validation loss: 2.7689645333960953

Epoch: 6| Step: 13
Training loss: 3.160508861131622
Validation loss: 2.7650365595679656

Epoch: 92| Step: 0
Training loss: 3.3853831754152686
Validation loss: 2.7657442571800095

Epoch: 6| Step: 1
Training loss: 2.7566099489766627
Validation loss: 2.77199051168173

Epoch: 6| Step: 2
Training loss: 3.506846407452694
Validation loss: 2.7669517320069485

Epoch: 6| Step: 3
Training loss: 3.1142916505589535
Validation loss: 2.7677571021303478

Epoch: 6| Step: 4
Training loss: 3.0507699963780914
Validation loss: 2.7649204262074405

Epoch: 6| Step: 5
Training loss: 3.277033539959804
Validation loss: 2.7685917748103797

Epoch: 6| Step: 6
Training loss: 3.002484564316686
Validation loss: 2.7666387853343086

Epoch: 6| Step: 7
Training loss: 3.027774036961227
Validation loss: 2.7667525124923795

Epoch: 6| Step: 8
Training loss: 3.187597535080973
Validation loss: 2.762317583554385

Epoch: 6| Step: 9
Training loss: 2.7829079990182493
Validation loss: 2.762339892555029

Epoch: 6| Step: 10
Training loss: 2.6446038240422545
Validation loss: 2.7604157299025456

Epoch: 6| Step: 11
Training loss: 3.22816202266176
Validation loss: 2.7596016112220907

Epoch: 6| Step: 12
Training loss: 3.116703311344348
Validation loss: 2.761165854508601

Epoch: 6| Step: 13
Training loss: 2.596268040571865
Validation loss: 2.7614472731198823

Epoch: 93| Step: 0
Training loss: 3.0071675191178007
Validation loss: 2.7604452406057742

Epoch: 6| Step: 1
Training loss: 2.6613635937773874
Validation loss: 2.764256528053758

Epoch: 6| Step: 2
Training loss: 3.473440349953495
Validation loss: 2.7609317298241702

Epoch: 6| Step: 3
Training loss: 2.8179138635123846
Validation loss: 2.758855466835705

Epoch: 6| Step: 4
Training loss: 2.803080573947265
Validation loss: 2.758047303102458

Epoch: 6| Step: 5
Training loss: 2.967651806542177
Validation loss: 2.757279005706171

Epoch: 6| Step: 6
Training loss: 3.591181093982908
Validation loss: 2.756546357797317

Epoch: 6| Step: 7
Training loss: 2.9008167300634518
Validation loss: 2.7626694346621834

Epoch: 6| Step: 8
Training loss: 2.9638985596053824
Validation loss: 2.7680317342733787

Epoch: 6| Step: 9
Training loss: 2.515053062106723
Validation loss: 2.7659771394207335

Epoch: 6| Step: 10
Training loss: 3.7793166837739918
Validation loss: 2.7775872255069816

Epoch: 6| Step: 11
Training loss: 2.8772282465616974
Validation loss: 2.762101996081322

Epoch: 6| Step: 12
Training loss: 2.9273758783911283
Validation loss: 2.7626760871836025

Epoch: 6| Step: 13
Training loss: 3.664457059979499
Validation loss: 2.759026649953589

Epoch: 94| Step: 0
Training loss: 2.7290164430541566
Validation loss: 2.762356504931194

Epoch: 6| Step: 1
Training loss: 3.087875491641443
Validation loss: 2.758410075577789

Epoch: 6| Step: 2
Training loss: 3.398782927398165
Validation loss: 2.756277312475779

Epoch: 6| Step: 3
Training loss: 3.8260837640202436
Validation loss: 2.7550958848947924

Epoch: 6| Step: 4
Training loss: 3.216997132266554
Validation loss: 2.755911347627133

Epoch: 6| Step: 5
Training loss: 3.1724947619864627
Validation loss: 2.7538524223587966

Epoch: 6| Step: 6
Training loss: 3.520222958959579
Validation loss: 2.7541320024031277

Epoch: 6| Step: 7
Training loss: 2.7517301145555773
Validation loss: 2.754494366334606

Epoch: 6| Step: 8
Training loss: 2.401209419221338
Validation loss: 2.757873640452007

Epoch: 6| Step: 9
Training loss: 2.621065096975699
Validation loss: 2.7539517786013366

Epoch: 6| Step: 10
Training loss: 3.461601383093815
Validation loss: 2.7521315542514455

Epoch: 6| Step: 11
Training loss: 2.450973928293419
Validation loss: 2.761182939105426

Epoch: 6| Step: 12
Training loss: 2.3051267043765686
Validation loss: 2.7609616333522666

Epoch: 6| Step: 13
Training loss: 3.828135836838934
Validation loss: 2.7628771529894385

Epoch: 95| Step: 0
Training loss: 3.0911333211866787
Validation loss: 2.7715059127640798

Epoch: 6| Step: 1
Training loss: 3.401579848855477
Validation loss: 2.75531573671384

Epoch: 6| Step: 2
Training loss: 2.8081969510099576
Validation loss: 2.7536234080657924

Epoch: 6| Step: 3
Training loss: 2.920247159974956
Validation loss: 2.753940842422199

Epoch: 6| Step: 4
Training loss: 2.8412571974670304
Validation loss: 2.7537079346889763

Epoch: 6| Step: 5
Training loss: 3.164390415106636
Validation loss: 2.7571877480103786

Epoch: 6| Step: 6
Training loss: 2.486628919507553
Validation loss: 2.7556025412269918

Epoch: 6| Step: 7
Training loss: 3.409730025984118
Validation loss: 2.75283014536382

Epoch: 6| Step: 8
Training loss: 3.2738827787187956
Validation loss: 2.7573591462334326

Epoch: 6| Step: 9
Training loss: 3.233872968992078
Validation loss: 2.7526148802867882

Epoch: 6| Step: 10
Training loss: 3.660214613995717
Validation loss: 2.7548939422220795

Epoch: 6| Step: 11
Training loss: 2.908175579117363
Validation loss: 2.754090152704228

Epoch: 6| Step: 12
Training loss: 2.6417789984886073
Validation loss: 2.7567841245844638

Epoch: 6| Step: 13
Training loss: 2.829103585065725
Validation loss: 2.7555379834148503

Epoch: 96| Step: 0
Training loss: 2.4909489821788053
Validation loss: 2.7610645721811804

Epoch: 6| Step: 1
Training loss: 3.4410157691175187
Validation loss: 2.7662529612217392

Epoch: 6| Step: 2
Training loss: 2.775949291722535
Validation loss: 2.7597222164930164

Epoch: 6| Step: 3
Training loss: 3.345912341130779
Validation loss: 2.7609919785327945

Epoch: 6| Step: 4
Training loss: 3.366887267284943
Validation loss: 2.768389344203261

Epoch: 6| Step: 5
Training loss: 3.259799854242846
Validation loss: 2.757849325544055

Epoch: 6| Step: 6
Training loss: 3.0621920060966374
Validation loss: 2.7529413689606814

Epoch: 6| Step: 7
Training loss: 3.139351809714268
Validation loss: 2.7532873780926597

Epoch: 6| Step: 8
Training loss: 2.7170775958884312
Validation loss: 2.74783405743061

Epoch: 6| Step: 9
Training loss: 2.2127395672598005
Validation loss: 2.7510350918479944

Epoch: 6| Step: 10
Training loss: 3.110140605581493
Validation loss: 2.7505569742302907

Epoch: 6| Step: 11
Training loss: 2.9282802609834384
Validation loss: 2.7462174288502665

Epoch: 6| Step: 12
Training loss: 3.5521741092899655
Validation loss: 2.7514779006105616

Epoch: 6| Step: 13
Training loss: 3.2616984292499485
Validation loss: 2.744900594656328

Epoch: 97| Step: 0
Training loss: 3.282086801546873
Validation loss: 2.7465859622321034

Epoch: 6| Step: 1
Training loss: 3.0937557027744944
Validation loss: 2.7465550070541784

Epoch: 6| Step: 2
Training loss: 2.854690132494852
Validation loss: 2.744848124944282

Epoch: 6| Step: 3
Training loss: 3.205079166428457
Validation loss: 2.74728919221129

Epoch: 6| Step: 4
Training loss: 2.3593863177501726
Validation loss: 2.7468365739464664

Epoch: 6| Step: 5
Training loss: 3.259974505552991
Validation loss: 2.758343363107543

Epoch: 6| Step: 6
Training loss: 3.3072903530488698
Validation loss: 2.759956075946875

Epoch: 6| Step: 7
Training loss: 3.117180769298811
Validation loss: 2.7764518499551034

Epoch: 6| Step: 8
Training loss: 3.3332646044957164
Validation loss: 2.7784913473121224

Epoch: 6| Step: 9
Training loss: 3.085834279325915
Validation loss: 2.752364925106428

Epoch: 6| Step: 10
Training loss: 3.4733279150434275
Validation loss: 2.7442051174266573

Epoch: 6| Step: 11
Training loss: 2.872300082720772
Validation loss: 2.7426612233765115

Epoch: 6| Step: 12
Training loss: 2.2089753087512887
Validation loss: 2.742988280888864

Epoch: 6| Step: 13
Training loss: 3.2255911115327107
Validation loss: 2.7467595974747874

Epoch: 98| Step: 0
Training loss: 2.3942044456715346
Validation loss: 2.7482289291289015

Epoch: 6| Step: 1
Training loss: 3.5650363392736684
Validation loss: 2.74981022811111

Epoch: 6| Step: 2
Training loss: 2.831283912708217
Validation loss: 2.7495619162966682

Epoch: 6| Step: 3
Training loss: 2.985261316155157
Validation loss: 2.752822121490664

Epoch: 6| Step: 4
Training loss: 3.436289348689228
Validation loss: 2.7493714129702385

Epoch: 6| Step: 5
Training loss: 3.5736928083841075
Validation loss: 2.7506346711323713

Epoch: 6| Step: 6
Training loss: 3.215906906004647
Validation loss: 2.748412290137786

Epoch: 6| Step: 7
Training loss: 3.196035289392926
Validation loss: 2.745378641222693

Epoch: 6| Step: 8
Training loss: 2.4785387114764035
Validation loss: 2.7469247728017447

Epoch: 6| Step: 9
Training loss: 3.1668381560888696
Validation loss: 2.7467151564773036

Epoch: 6| Step: 10
Training loss: 2.5983251642568153
Validation loss: 2.7481227202254197

Epoch: 6| Step: 11
Training loss: 3.0620677409387365
Validation loss: 2.7576096192933015

Epoch: 6| Step: 12
Training loss: 3.398320478858103
Validation loss: 2.7613566925774338

Epoch: 6| Step: 13
Training loss: 2.6367237232302636
Validation loss: 2.756471505127495

Epoch: 99| Step: 0
Training loss: 2.643408758266729
Validation loss: 2.7502146979206277

Epoch: 6| Step: 1
Training loss: 2.7952304885629546
Validation loss: 2.7463112475725087

Epoch: 6| Step: 2
Training loss: 3.1267917836850776
Validation loss: 2.747635097647861

Epoch: 6| Step: 3
Training loss: 3.40991573643541
Validation loss: 2.7468286333626475

Epoch: 6| Step: 4
Training loss: 3.2136177701094573
Validation loss: 2.7459660865685103

Epoch: 6| Step: 5
Training loss: 3.499651074046623
Validation loss: 2.7447449898360965

Epoch: 6| Step: 6
Training loss: 2.945690343283004
Validation loss: 2.745150068676425

Epoch: 6| Step: 7
Training loss: 2.9318481615830865
Validation loss: 2.7411939762622595

Epoch: 6| Step: 8
Training loss: 3.037130727416902
Validation loss: 2.745455113889398

Epoch: 6| Step: 9
Training loss: 2.659059900786223
Validation loss: 2.739676341806499

Epoch: 6| Step: 10
Training loss: 2.642006958264321
Validation loss: 2.7425576349281506

Epoch: 6| Step: 11
Training loss: 3.001373294584061
Validation loss: 2.7370135199966312

Epoch: 6| Step: 12
Training loss: 3.434479791154994
Validation loss: 2.7415909348597167

Epoch: 6| Step: 13
Training loss: 3.378686092899941
Validation loss: 2.7379580188140897

Epoch: 100| Step: 0
Training loss: 3.184647124907352
Validation loss: 2.739090663280603

Epoch: 6| Step: 1
Training loss: 3.4261456320786943
Validation loss: 2.7375288300216583

Epoch: 6| Step: 2
Training loss: 2.789028819332263
Validation loss: 2.738694640627468

Epoch: 6| Step: 3
Training loss: 3.0038156086475074
Validation loss: 2.733276285205956

Epoch: 6| Step: 4
Training loss: 2.7434552948988693
Validation loss: 2.734126422792924

Epoch: 6| Step: 5
Training loss: 3.6271448038506247
Validation loss: 2.735182208146977

Epoch: 6| Step: 6
Training loss: 3.040306797608423
Validation loss: 2.7379219940985817

Epoch: 6| Step: 7
Training loss: 2.6781817506988976
Validation loss: 2.733416910216836

Epoch: 6| Step: 8
Training loss: 3.161792986700279
Validation loss: 2.734007727455696

Epoch: 6| Step: 9
Training loss: 2.94869339665852
Validation loss: 2.7327604913095147

Epoch: 6| Step: 10
Training loss: 3.1058857571826306
Validation loss: 2.733405544858815

Epoch: 6| Step: 11
Training loss: 2.444410943996255
Validation loss: 2.7357548414290145

Epoch: 6| Step: 12
Training loss: 2.9589817966160386
Validation loss: 2.7358182074635584

Epoch: 6| Step: 13
Training loss: 3.5865602970540307
Validation loss: 2.732459030538502

Epoch: 101| Step: 0
Training loss: 2.6643809019198317
Validation loss: 2.739654409708606

Epoch: 6| Step: 1
Training loss: 2.9528698684847385
Validation loss: 2.7608387422210745

Epoch: 6| Step: 2
Training loss: 3.0008132150263953
Validation loss: 2.7749970315704684

Epoch: 6| Step: 3
Training loss: 3.2228561431850364
Validation loss: 2.783810752827048

Epoch: 6| Step: 4
Training loss: 3.0915400771335553
Validation loss: 2.76890481184427

Epoch: 6| Step: 5
Training loss: 2.8728966689894917
Validation loss: 2.764973112122936

Epoch: 6| Step: 6
Training loss: 3.144941258207874
Validation loss: 2.7505005225307904

Epoch: 6| Step: 7
Training loss: 2.8602835457302302
Validation loss: 2.7317451292764807

Epoch: 6| Step: 8
Training loss: 3.3942647638403725
Validation loss: 2.7300368841592135

Epoch: 6| Step: 9
Training loss: 2.845488299951545
Validation loss: 2.732894021057695

Epoch: 6| Step: 10
Training loss: 2.988633557663466
Validation loss: 2.729843926079033

Epoch: 6| Step: 11
Training loss: 3.480630958621897
Validation loss: 2.7277423085371666

Epoch: 6| Step: 12
Training loss: 2.5999261332069272
Validation loss: 2.7292704339783307

Epoch: 6| Step: 13
Training loss: 3.687251713039469
Validation loss: 2.7263683989623924

Epoch: 102| Step: 0
Training loss: 3.4133920251250807
Validation loss: 2.7305179667621284

Epoch: 6| Step: 1
Training loss: 3.286001779287626
Validation loss: 2.732488512923352

Epoch: 6| Step: 2
Training loss: 2.933382273034563
Validation loss: 2.7250242949636414

Epoch: 6| Step: 3
Training loss: 2.985153336412475
Validation loss: 2.725229176298437

Epoch: 6| Step: 4
Training loss: 3.05913374264204
Validation loss: 2.7283862567729193

Epoch: 6| Step: 5
Training loss: 2.229741126460162
Validation loss: 2.725582842979162

Epoch: 6| Step: 6
Training loss: 2.6843234073890097
Validation loss: 2.73120039784104

Epoch: 6| Step: 7
Training loss: 3.01166903483205
Validation loss: 2.74258069729702

Epoch: 6| Step: 8
Training loss: 3.7995401455555102
Validation loss: 2.7391482233477507

Epoch: 6| Step: 9
Training loss: 2.738550453176776
Validation loss: 2.7374915147905434

Epoch: 6| Step: 10
Training loss: 3.0514917071481524
Validation loss: 2.7361355346979934

Epoch: 6| Step: 11
Training loss: 3.3331241224120762
Validation loss: 2.724239988821307

Epoch: 6| Step: 12
Training loss: 3.014719456514861
Validation loss: 2.7259985740226402

Epoch: 6| Step: 13
Training loss: 2.7085649733641803
Validation loss: 2.720500543249481

Epoch: 103| Step: 0
Training loss: 3.2573944482221666
Validation loss: 2.7233278053036702

Epoch: 6| Step: 1
Training loss: 2.546403993454707
Validation loss: 2.724092687298318

Epoch: 6| Step: 2
Training loss: 3.2718296517888437
Validation loss: 2.725323279183129

Epoch: 6| Step: 3
Training loss: 3.2564758925328254
Validation loss: 2.725345734859338

Epoch: 6| Step: 4
Training loss: 2.920019366317698
Validation loss: 2.7207148948861075

Epoch: 6| Step: 5
Training loss: 2.8155557774238664
Validation loss: 2.724883359136418

Epoch: 6| Step: 6
Training loss: 2.889984719787118
Validation loss: 2.723800458532703

Epoch: 6| Step: 7
Training loss: 3.2082798115282993
Validation loss: 2.726934891133485

Epoch: 6| Step: 8
Training loss: 2.672507941695458
Validation loss: 2.725882839964509

Epoch: 6| Step: 9
Training loss: 3.160028292372356
Validation loss: 2.7248453862006903

Epoch: 6| Step: 10
Training loss: 2.3694409009522626
Validation loss: 2.730688113597188

Epoch: 6| Step: 11
Training loss: 3.0539587375973607
Validation loss: 2.739196798423652

Epoch: 6| Step: 12
Training loss: 3.681812102273188
Validation loss: 2.7374336689024408

Epoch: 6| Step: 13
Training loss: 3.431105891746983
Validation loss: 2.756403239029904

Epoch: 104| Step: 0
Training loss: 2.932474748351508
Validation loss: 2.7552279162603166

Epoch: 6| Step: 1
Training loss: 3.492620590629319
Validation loss: 2.7606620328160596

Epoch: 6| Step: 2
Training loss: 2.551832184143917
Validation loss: 2.7698245266470285

Epoch: 6| Step: 3
Training loss: 3.1479625106964026
Validation loss: 2.781812929114935

Epoch: 6| Step: 4
Training loss: 3.4565323119383855
Validation loss: 2.76587468373504

Epoch: 6| Step: 5
Training loss: 3.1608624889243697
Validation loss: 2.767909320981975

Epoch: 6| Step: 6
Training loss: 2.592251264214426
Validation loss: 2.7529500518000107

Epoch: 6| Step: 7
Training loss: 3.0678421449001236
Validation loss: 2.7246261647324923

Epoch: 6| Step: 8
Training loss: 3.00674633600055
Validation loss: 2.720154810098961

Epoch: 6| Step: 9
Training loss: 2.9220306844594996
Validation loss: 2.718801990842187

Epoch: 6| Step: 10
Training loss: 3.8561558444555266
Validation loss: 2.7166487575111558

Epoch: 6| Step: 11
Training loss: 2.461740707837052
Validation loss: 2.7217489780547797

Epoch: 6| Step: 12
Training loss: 2.6884166130257547
Validation loss: 2.724366774211218

Epoch: 6| Step: 13
Training loss: 2.9075211237027725
Validation loss: 2.725395036610649

Epoch: 105| Step: 0
Training loss: 2.8992448283908643
Validation loss: 2.7203261432247974

Epoch: 6| Step: 1
Training loss: 3.119696427272988
Validation loss: 2.7228390950983354

Epoch: 6| Step: 2
Training loss: 2.6635360421330323
Validation loss: 2.72230579388015

Epoch: 6| Step: 3
Training loss: 2.781241341909584
Validation loss: 2.7211542132108564

Epoch: 6| Step: 4
Training loss: 3.550887875272717
Validation loss: 2.721182680013615

Epoch: 6| Step: 5
Training loss: 3.5431480152071004
Validation loss: 2.7191399726111642

Epoch: 6| Step: 6
Training loss: 3.0809565653947675
Validation loss: 2.7184138841678576

Epoch: 6| Step: 7
Training loss: 2.936591149815847
Validation loss: 2.716625089029649

Epoch: 6| Step: 8
Training loss: 2.9375871685973696
Validation loss: 2.7192815502174517

Epoch: 6| Step: 9
Training loss: 3.114786471033261
Validation loss: 2.7335003772788014

Epoch: 6| Step: 10
Training loss: 3.463519302334203
Validation loss: 2.7447802198268287

Epoch: 6| Step: 11
Training loss: 2.563902308395753
Validation loss: 2.742613793027901

Epoch: 6| Step: 12
Training loss: 2.8981214489811213
Validation loss: 2.7366094513491412

Epoch: 6| Step: 13
Training loss: 2.716094463054013
Validation loss: 2.729287102985265

Epoch: 106| Step: 0
Training loss: 3.4431001302721476
Validation loss: 2.717345207071256

Epoch: 6| Step: 1
Training loss: 2.879361037793266
Validation loss: 2.715355335711977

Epoch: 6| Step: 2
Training loss: 3.432369912412881
Validation loss: 2.71270201269014

Epoch: 6| Step: 3
Training loss: 2.906242124485297
Validation loss: 2.7126089217197125

Epoch: 6| Step: 4
Training loss: 3.305737484182774
Validation loss: 2.7175446411285566

Epoch: 6| Step: 5
Training loss: 3.2036355704933532
Validation loss: 2.713590211055964

Epoch: 6| Step: 6
Training loss: 3.7050001330253384
Validation loss: 2.7141931232974943

Epoch: 6| Step: 7
Training loss: 2.9883565656358524
Validation loss: 2.7094004378570142

Epoch: 6| Step: 8
Training loss: 2.0197562529548208
Validation loss: 2.710921979689941

Epoch: 6| Step: 9
Training loss: 2.356323491981682
Validation loss: 2.718294274505161

Epoch: 6| Step: 10
Training loss: 3.3672837953893056
Validation loss: 2.713322632022647

Epoch: 6| Step: 11
Training loss: 2.8162951824493647
Validation loss: 2.7158891285998434

Epoch: 6| Step: 12
Training loss: 2.518693840972098
Validation loss: 2.724837192422072

Epoch: 6| Step: 13
Training loss: 3.208668150502452
Validation loss: 2.7165580704655157

Epoch: 107| Step: 0
Training loss: 2.8471890289935597
Validation loss: 2.721187822961608

Epoch: 6| Step: 1
Training loss: 3.0523638460753864
Validation loss: 2.7169989459735224

Epoch: 6| Step: 2
Training loss: 2.683469811122042
Validation loss: 2.710919517161216

Epoch: 6| Step: 3
Training loss: 3.400255440484858
Validation loss: 2.7122949394077525

Epoch: 6| Step: 4
Training loss: 3.3391926448666074
Validation loss: 2.710406008357652

Epoch: 6| Step: 5
Training loss: 3.573068435752361
Validation loss: 2.7108343126678998

Epoch: 6| Step: 6
Training loss: 3.027182140343628
Validation loss: 2.710194797241913

Epoch: 6| Step: 7
Training loss: 2.962161976003763
Validation loss: 2.7108898455946973

Epoch: 6| Step: 8
Training loss: 3.5413080968915107
Validation loss: 2.7145627726041583

Epoch: 6| Step: 9
Training loss: 3.2334087605543456
Validation loss: 2.7198416970098602

Epoch: 6| Step: 10
Training loss: 2.75076127352472
Validation loss: 2.722736291966291

Epoch: 6| Step: 11
Training loss: 3.0145304378428746
Validation loss: 2.7144005760246395

Epoch: 6| Step: 12
Training loss: 2.043819797268951
Validation loss: 2.7081019855698405

Epoch: 6| Step: 13
Training loss: 2.2365451757387143
Validation loss: 2.711349412037348

Epoch: 108| Step: 0
Training loss: 2.917205179546052
Validation loss: 2.7098017123619433

Epoch: 6| Step: 1
Training loss: 2.4235251434828875
Validation loss: 2.7161183485283753

Epoch: 6| Step: 2
Training loss: 3.433862877411423
Validation loss: 2.7245150037507253

Epoch: 6| Step: 3
Training loss: 2.7293574919247177
Validation loss: 2.7469490873496127

Epoch: 6| Step: 4
Training loss: 3.199927692788496
Validation loss: 2.752651508072548

Epoch: 6| Step: 5
Training loss: 3.202225423889654
Validation loss: 2.7285841415885543

Epoch: 6| Step: 6
Training loss: 3.0902955777421193
Validation loss: 2.7130868950407767

Epoch: 6| Step: 7
Training loss: 3.6168834978725286
Validation loss: 2.7036092280482396

Epoch: 6| Step: 8
Training loss: 3.1424383745243287
Validation loss: 2.7028091068733655

Epoch: 6| Step: 9
Training loss: 3.025053632276528
Validation loss: 2.7056821712663788

Epoch: 6| Step: 10
Training loss: 2.4940928288049524
Validation loss: 2.7048364982938615

Epoch: 6| Step: 11
Training loss: 2.7106365289634238
Validation loss: 2.706664707409032

Epoch: 6| Step: 12
Training loss: 3.305685988202766
Validation loss: 2.711326696720633

Epoch: 6| Step: 13
Training loss: 2.9473173972424296
Validation loss: 2.703926202416866

Epoch: 109| Step: 0
Training loss: 3.218781702570665
Validation loss: 2.7048634289133036

Epoch: 6| Step: 1
Training loss: 2.9746741364831406
Validation loss: 2.7035190613469147

Epoch: 6| Step: 2
Training loss: 2.510408477980896
Validation loss: 2.7037733700369824

Epoch: 6| Step: 3
Training loss: 3.025560683315622
Validation loss: 2.7030218160295783

Epoch: 6| Step: 4
Training loss: 3.348982380275901
Validation loss: 2.7136805239833257

Epoch: 6| Step: 5
Training loss: 2.9742951656111294
Validation loss: 2.713853848137429

Epoch: 6| Step: 6
Training loss: 3.0273634386960566
Validation loss: 2.712054885825646

Epoch: 6| Step: 7
Training loss: 3.3504228410465076
Validation loss: 2.7054112809520454

Epoch: 6| Step: 8
Training loss: 3.5212794967353016
Validation loss: 2.7030140540259717

Epoch: 6| Step: 9
Training loss: 2.9528336961566604
Validation loss: 2.7089270844082454

Epoch: 6| Step: 10
Training loss: 3.26808942316198
Validation loss: 2.708161505903447

Epoch: 6| Step: 11
Training loss: 3.1381835557632836
Validation loss: 2.7064464510198003

Epoch: 6| Step: 12
Training loss: 2.4021416803989646
Validation loss: 2.7071218451913226

Epoch: 6| Step: 13
Training loss: 2.123074612967086
Validation loss: 2.7070822575188034

Epoch: 110| Step: 0
Training loss: 3.6166037300599028
Validation loss: 2.7107252237810893

Epoch: 6| Step: 1
Training loss: 2.7418795246950394
Validation loss: 2.7075595646478394

Epoch: 6| Step: 2
Training loss: 3.1463128202378368
Validation loss: 2.7134449758744115

Epoch: 6| Step: 3
Training loss: 3.8553832874662315
Validation loss: 2.7098082316678718

Epoch: 6| Step: 4
Training loss: 3.432461183999476
Validation loss: 2.714020082462251

Epoch: 6| Step: 5
Training loss: 2.554351784582185
Validation loss: 2.72055345615398

Epoch: 6| Step: 6
Training loss: 3.5896484853212223
Validation loss: 2.7287605175686545

Epoch: 6| Step: 7
Training loss: 2.4140866966639805
Validation loss: 2.719565121658459

Epoch: 6| Step: 8
Training loss: 2.6637979415238315
Validation loss: 2.7114289311661346

Epoch: 6| Step: 9
Training loss: 2.483879471789482
Validation loss: 2.7083380797717687

Epoch: 6| Step: 10
Training loss: 2.5857206409258824
Validation loss: 2.701227097872197

Epoch: 6| Step: 11
Training loss: 2.9016056482807526
Validation loss: 2.709072823926691

Epoch: 6| Step: 12
Training loss: 2.709739398320717
Validation loss: 2.708434097523268

Epoch: 6| Step: 13
Training loss: 3.288681624371239
Validation loss: 2.714166605448848

Epoch: 111| Step: 0
Training loss: 3.429058182314817
Validation loss: 2.7321470488716075

Epoch: 6| Step: 1
Training loss: 3.8054406519216473
Validation loss: 2.7276002114048143

Epoch: 6| Step: 2
Training loss: 2.6954306756248823
Validation loss: 2.7289309073785053

Epoch: 6| Step: 3
Training loss: 2.545124692878725
Validation loss: 2.7226297623213975

Epoch: 6| Step: 4
Training loss: 2.75242369224664
Validation loss: 2.726944289477154

Epoch: 6| Step: 5
Training loss: 3.158111476571039
Validation loss: 2.708521930258683

Epoch: 6| Step: 6
Training loss: 2.8461556791509803
Validation loss: 2.703783627335947

Epoch: 6| Step: 7
Training loss: 3.2608795000007884
Validation loss: 2.704714960278162

Epoch: 6| Step: 8
Training loss: 3.213586461740608
Validation loss: 2.7039914586832214

Epoch: 6| Step: 9
Training loss: 3.3906912379672716
Validation loss: 2.6968901931931137

Epoch: 6| Step: 10
Training loss: 3.356226381470958
Validation loss: 2.6994849577820994

Epoch: 6| Step: 11
Training loss: 2.4330082123128234
Validation loss: 2.694945092207031

Epoch: 6| Step: 12
Training loss: 1.5994147064658613
Validation loss: 2.695497742990284

Epoch: 6| Step: 13
Training loss: 3.2359332325463774
Validation loss: 2.698653257687769

Epoch: 112| Step: 0
Training loss: 2.830875302882264
Validation loss: 2.698173805424845

Epoch: 6| Step: 1
Training loss: 3.445416991710167
Validation loss: 2.693498453914483

Epoch: 6| Step: 2
Training loss: 3.4996912683834016
Validation loss: 2.690105373468583

Epoch: 6| Step: 3
Training loss: 2.732690433940783
Validation loss: 2.6957069492367496

Epoch: 6| Step: 4
Training loss: 3.3079425481675977
Validation loss: 2.7036537072076112

Epoch: 6| Step: 5
Training loss: 3.1313390905684537
Validation loss: 2.694366639135954

Epoch: 6| Step: 6
Training loss: 2.154140822982959
Validation loss: 2.7013003172600243

Epoch: 6| Step: 7
Training loss: 2.3312950314015386
Validation loss: 2.6986705755920597

Epoch: 6| Step: 8
Training loss: 3.0949034419119283
Validation loss: 2.704693094438047

Epoch: 6| Step: 9
Training loss: 3.7557586957953624
Validation loss: 2.695235611917338

Epoch: 6| Step: 10
Training loss: 3.262245291051741
Validation loss: 2.6980527431011887

Epoch: 6| Step: 11
Training loss: 2.558636520843607
Validation loss: 2.6907913675089805

Epoch: 6| Step: 12
Training loss: 2.4678191349461565
Validation loss: 2.6943853471048134

Epoch: 6| Step: 13
Training loss: 3.254606650122482
Validation loss: 2.6915453031576435

Epoch: 113| Step: 0
Training loss: 3.4187769691756467
Validation loss: 2.689022399001197

Epoch: 6| Step: 1
Training loss: 2.5883643065902393
Validation loss: 2.6931196078666098

Epoch: 6| Step: 2
Training loss: 2.5550150530048827
Validation loss: 2.693614789452409

Epoch: 6| Step: 3
Training loss: 2.2274161860563377
Validation loss: 2.6919706995674586

Epoch: 6| Step: 4
Training loss: 3.3253607934227385
Validation loss: 2.6915109651147384

Epoch: 6| Step: 5
Training loss: 3.187463498374028
Validation loss: 2.692371317724564

Epoch: 6| Step: 6
Training loss: 3.154775747318522
Validation loss: 2.696632759803552

Epoch: 6| Step: 7
Training loss: 2.636099554754969
Validation loss: 2.6923147534211678

Epoch: 6| Step: 8
Training loss: 3.1138521865132143
Validation loss: 2.689193696511318

Epoch: 6| Step: 9
Training loss: 2.9687952941651647
Validation loss: 2.686925931202419

Epoch: 6| Step: 10
Training loss: 2.7440707703519958
Validation loss: 2.690257677753534

Epoch: 6| Step: 11
Training loss: 2.914915885431452
Validation loss: 2.6971889598801417

Epoch: 6| Step: 12
Training loss: 3.578559682504278
Validation loss: 2.698047165528206

Epoch: 6| Step: 13
Training loss: 3.7401536738601204
Validation loss: 2.7179170802940913

Epoch: 114| Step: 0
Training loss: 2.6113769709284895
Validation loss: 2.714003126045051

Epoch: 6| Step: 1
Training loss: 3.543275729405247
Validation loss: 2.71940261161347

Epoch: 6| Step: 2
Training loss: 2.7233978531228304
Validation loss: 2.7007532364567823

Epoch: 6| Step: 3
Training loss: 2.707725094695923
Validation loss: 2.6899794511413884

Epoch: 6| Step: 4
Training loss: 2.4826016604841525
Validation loss: 2.6867922570956617

Epoch: 6| Step: 5
Training loss: 2.896758300010831
Validation loss: 2.690620476612189

Epoch: 6| Step: 6
Training loss: 2.295700499562477
Validation loss: 2.6919773215760996

Epoch: 6| Step: 7
Training loss: 3.244140625
Validation loss: 2.6952154393232153

Epoch: 6| Step: 8
Training loss: 3.106784834381215
Validation loss: 2.6937067380943494

Epoch: 6| Step: 9
Training loss: 2.8851847825135994
Validation loss: 2.693095590759662

Epoch: 6| Step: 10
Training loss: 3.252792918966967
Validation loss: 2.688808820726425

Epoch: 6| Step: 11
Training loss: 3.714095336101298
Validation loss: 2.6886679936540663

Epoch: 6| Step: 12
Training loss: 3.0688839760803237
Validation loss: 2.6882034424722017

Epoch: 6| Step: 13
Training loss: 3.81664101260252
Validation loss: 2.6918450430686627

Epoch: 115| Step: 0
Training loss: 3.3201766401432513
Validation loss: 2.700404565411211

Epoch: 6| Step: 1
Training loss: 2.790595138407122
Validation loss: 2.7316949313892778

Epoch: 6| Step: 2
Training loss: 2.946948823782849
Validation loss: 2.7429995943917733

Epoch: 6| Step: 3
Training loss: 3.1850283146292804
Validation loss: 2.7273509048706126

Epoch: 6| Step: 4
Training loss: 2.5344043433657912
Validation loss: 2.69538226388817

Epoch: 6| Step: 5
Training loss: 3.33017434794999
Validation loss: 2.6934504557486285

Epoch: 6| Step: 6
Training loss: 2.6062644647064226
Validation loss: 2.6881384733831717

Epoch: 6| Step: 7
Training loss: 3.0103539765022274
Validation loss: 2.688771568283496

Epoch: 6| Step: 8
Training loss: 3.307621368234367
Validation loss: 2.6862376711889766

Epoch: 6| Step: 9
Training loss: 3.176486867144058
Validation loss: 2.6915158694877013

Epoch: 6| Step: 10
Training loss: 2.770107623601995
Validation loss: 2.689630806973223

Epoch: 6| Step: 11
Training loss: 2.6561164373589863
Validation loss: 2.6924546603997395

Epoch: 6| Step: 12
Training loss: 3.107458703034435
Validation loss: 2.693969789696666

Epoch: 6| Step: 13
Training loss: 3.445067658666594
Validation loss: 2.696415092339962

Epoch: 116| Step: 0
Training loss: 3.198809438006383
Validation loss: 2.693457988325876

Epoch: 6| Step: 1
Training loss: 2.372480762914201
Validation loss: 2.6894604571586105

Epoch: 6| Step: 2
Training loss: 3.3601655251739286
Validation loss: 2.689787880766498

Epoch: 6| Step: 3
Training loss: 3.109512402742993
Validation loss: 2.6900052410583517

Epoch: 6| Step: 4
Training loss: 3.1481936528359933
Validation loss: 2.691749317400397

Epoch: 6| Step: 5
Training loss: 3.007699305398453
Validation loss: 2.683360556549287

Epoch: 6| Step: 6
Training loss: 3.292847691094753
Validation loss: 2.6888690770406196

Epoch: 6| Step: 7
Training loss: 3.182415815679837
Validation loss: 2.6827302747166843

Epoch: 6| Step: 8
Training loss: 3.2051447757390408
Validation loss: 2.6881869554926388

Epoch: 6| Step: 9
Training loss: 3.3199163941116954
Validation loss: 2.6850177045015284

Epoch: 6| Step: 10
Training loss: 2.3959065854580923
Validation loss: 2.6907032284883727

Epoch: 6| Step: 11
Training loss: 2.1499242458748373
Validation loss: 2.68853880677294

Epoch: 6| Step: 12
Training loss: 3.1270322676489095
Validation loss: 2.6859917697607685

Epoch: 6| Step: 13
Training loss: 2.8066449573459518
Validation loss: 2.693264084855466

Epoch: 117| Step: 0
Training loss: 2.785806759148309
Validation loss: 2.698276486436975

Epoch: 6| Step: 1
Training loss: 3.011121163658388
Validation loss: 2.729936932684179

Epoch: 6| Step: 2
Training loss: 2.7211319584755556
Validation loss: 2.73305469801499

Epoch: 6| Step: 3
Training loss: 3.57687036098384
Validation loss: 2.719881245065866

Epoch: 6| Step: 4
Training loss: 3.464653414172112
Validation loss: 2.7033563235490474

Epoch: 6| Step: 5
Training loss: 3.3871971417277607
Validation loss: 2.68577403311342

Epoch: 6| Step: 6
Training loss: 2.612582126038416
Validation loss: 2.684428125662505

Epoch: 6| Step: 7
Training loss: 2.773425894699531
Validation loss: 2.6749463872297192

Epoch: 6| Step: 8
Training loss: 2.8326107300463352
Validation loss: 2.678953562666013

Epoch: 6| Step: 9
Training loss: 2.5799344273299876
Validation loss: 2.6777731797187596

Epoch: 6| Step: 10
Training loss: 3.239002400570787
Validation loss: 2.677960334796972

Epoch: 6| Step: 11
Training loss: 2.5160458609581386
Validation loss: 2.697951634260771

Epoch: 6| Step: 12
Training loss: 3.4561136008418223
Validation loss: 2.69657489306144

Epoch: 6| Step: 13
Training loss: 2.7986567272405365
Validation loss: 2.7046960962741533

Epoch: 118| Step: 0
Training loss: 2.895185841869216
Validation loss: 2.7604255129733732

Epoch: 6| Step: 1
Training loss: 3.4386831848242814
Validation loss: 2.8442320615103727

Epoch: 6| Step: 2
Training loss: 2.826534435160303
Validation loss: 2.9021685933636374

Epoch: 6| Step: 3
Training loss: 2.553355578465915
Validation loss: 2.87998715680972

Epoch: 6| Step: 4
Training loss: 3.7074333270322812
Validation loss: 2.8835066550908746

Epoch: 6| Step: 5
Training loss: 2.8989662037800956
Validation loss: 2.8417309626719174

Epoch: 6| Step: 6
Training loss: 3.9671853653314955
Validation loss: 2.8631742750912714

Epoch: 6| Step: 7
Training loss: 3.5466506147618935
Validation loss: 2.8111350152837615

Epoch: 6| Step: 8
Training loss: 2.921102656320001
Validation loss: 2.776087280433084

Epoch: 6| Step: 9
Training loss: 3.4403218564566287
Validation loss: 2.775099180947753

Epoch: 6| Step: 10
Training loss: 2.845262565363157
Validation loss: 2.8209007680695053

Epoch: 6| Step: 11
Training loss: 2.899285616523774
Validation loss: 2.857490007435812

Epoch: 6| Step: 12
Training loss: 2.460642382168194
Validation loss: 2.87568530385741

Epoch: 6| Step: 13
Training loss: 2.979407045141232
Validation loss: 2.864768748318946

Epoch: 119| Step: 0
Training loss: 2.5436225203079705
Validation loss: 2.820544510586683

Epoch: 6| Step: 1
Training loss: 3.103561103348345
Validation loss: 2.805238928510221

Epoch: 6| Step: 2
Training loss: 3.342147585794124
Validation loss: 2.786659546423167

Epoch: 6| Step: 3
Training loss: 2.6164156102867056
Validation loss: 2.7793454134345748

Epoch: 6| Step: 4
Training loss: 2.984768187500945
Validation loss: 2.7540092607832585

Epoch: 6| Step: 5
Training loss: 2.4853803411889803
Validation loss: 2.741705171773685

Epoch: 6| Step: 6
Training loss: 3.3133618655086017
Validation loss: 2.724723722660842

Epoch: 6| Step: 7
Training loss: 2.7735686795011345
Validation loss: 2.698504067366986

Epoch: 6| Step: 8
Training loss: 3.3862349396712554
Validation loss: 2.6976475342456627

Epoch: 6| Step: 9
Training loss: 2.737037626610643
Validation loss: 2.7003742290994293

Epoch: 6| Step: 10
Training loss: 3.656353288805472
Validation loss: 2.7063834965800324

Epoch: 6| Step: 11
Training loss: 3.6046500056465964
Validation loss: 2.73347218881361

Epoch: 6| Step: 12
Training loss: 2.8422488767467504
Validation loss: 2.7140521199872474

Epoch: 6| Step: 13
Training loss: 2.7391336939906177
Validation loss: 2.715272081305486

Epoch: 120| Step: 0
Training loss: 2.9469478529402617
Validation loss: 2.7073812327308584

Epoch: 6| Step: 1
Training loss: 3.272556231826477
Validation loss: 2.712014312381858

Epoch: 6| Step: 2
Training loss: 3.0103479573329643
Validation loss: 2.7184042365971206

Epoch: 6| Step: 3
Training loss: 2.9102679954426622
Validation loss: 2.73454686622355

Epoch: 6| Step: 4
Training loss: 2.4950018510830727
Validation loss: 2.7363981285848347

Epoch: 6| Step: 5
Training loss: 3.0232111725385984
Validation loss: 2.7505194508074657

Epoch: 6| Step: 6
Training loss: 2.9947107099564936
Validation loss: 2.7437416508018955

Epoch: 6| Step: 7
Training loss: 2.9617653044370167
Validation loss: 2.711215581435519

Epoch: 6| Step: 8
Training loss: 3.373465860449916
Validation loss: 2.6839605553821166

Epoch: 6| Step: 9
Training loss: 2.9726020316795045
Validation loss: 2.672350864315563

Epoch: 6| Step: 10
Training loss: 1.9533517934731934
Validation loss: 2.6712656564631976

Epoch: 6| Step: 11
Training loss: 3.4041579235124932
Validation loss: 2.673258801787059

Epoch: 6| Step: 12
Training loss: 3.408506293598916
Validation loss: 2.67479714456412

Epoch: 6| Step: 13
Training loss: 3.1711158783622944
Validation loss: 2.676401100133885

Epoch: 121| Step: 0
Training loss: 2.976061360782827
Validation loss: 2.6741960374070044

Epoch: 6| Step: 1
Training loss: 2.9710790232058435
Validation loss: 2.6765994367249077

Epoch: 6| Step: 2
Training loss: 3.4894305447204736
Validation loss: 2.6781350278090486

Epoch: 6| Step: 3
Training loss: 2.9926907188427374
Validation loss: 2.674462003968642

Epoch: 6| Step: 4
Training loss: 2.9898822559661133
Validation loss: 2.678743283432462

Epoch: 6| Step: 5
Training loss: 3.02437480424351
Validation loss: 2.67639510386959

Epoch: 6| Step: 6
Training loss: 3.0964426980330426
Validation loss: 2.676860973581163

Epoch: 6| Step: 7
Training loss: 2.992709360853887
Validation loss: 2.6764630284974764

Epoch: 6| Step: 8
Training loss: 2.871657211131689
Validation loss: 2.671199592081974

Epoch: 6| Step: 9
Training loss: 3.825763830574112
Validation loss: 2.6706412248790143

Epoch: 6| Step: 10
Training loss: 2.139856576480703
Validation loss: 2.6632390650102358

Epoch: 6| Step: 11
Training loss: 2.3036646755436028
Validation loss: 2.6640869491706725

Epoch: 6| Step: 12
Training loss: 3.123271311412504
Validation loss: 2.6754019572387153

Epoch: 6| Step: 13
Training loss: 2.9179775743465965
Validation loss: 2.6887886351932613

Epoch: 122| Step: 0
Training loss: 2.9890005006556826
Validation loss: 2.7079867192449516

Epoch: 6| Step: 1
Training loss: 2.836241146257081
Validation loss: 2.7564069248843177

Epoch: 6| Step: 2
Training loss: 3.4375109585674046
Validation loss: 2.820972461649938

Epoch: 6| Step: 3
Training loss: 2.8085470606885066
Validation loss: 2.81391516008221

Epoch: 6| Step: 4
Training loss: 3.230038892198852
Validation loss: 2.789466461674489

Epoch: 6| Step: 5
Training loss: 2.687672099436037
Validation loss: 2.738301328155048

Epoch: 6| Step: 6
Training loss: 2.938202469613251
Validation loss: 2.7093089120653997

Epoch: 6| Step: 7
Training loss: 2.5949794199573994
Validation loss: 2.702992856358579

Epoch: 6| Step: 8
Training loss: 3.405413323634175
Validation loss: 2.6915761785429098

Epoch: 6| Step: 9
Training loss: 3.017989264568349
Validation loss: 2.6872302370931345

Epoch: 6| Step: 10
Training loss: 2.7218088065331276
Validation loss: 2.6915243494884624

Epoch: 6| Step: 11
Training loss: 3.345993287731308
Validation loss: 2.671903013159158

Epoch: 6| Step: 12
Training loss: 2.859452043339629
Validation loss: 2.671407513204998

Epoch: 6| Step: 13
Training loss: 3.3736678426038385
Validation loss: 2.6818963018812956

Epoch: 123| Step: 0
Training loss: 2.6124638532118256
Validation loss: 2.6795405745793253

Epoch: 6| Step: 1
Training loss: 2.7861785082674873
Validation loss: 2.6842730857731993

Epoch: 6| Step: 2
Training loss: 3.460014823043912
Validation loss: 2.68463878754427

Epoch: 6| Step: 3
Training loss: 3.026827072844805
Validation loss: 2.6887387327699352

Epoch: 6| Step: 4
Training loss: 3.0918906433190907
Validation loss: 2.6850773775514947

Epoch: 6| Step: 5
Training loss: 2.8395344435881107
Validation loss: 2.702288341955921

Epoch: 6| Step: 6
Training loss: 3.8068906448486195
Validation loss: 2.7127557174934136

Epoch: 6| Step: 7
Training loss: 3.2202423487855865
Validation loss: 2.700103761417908

Epoch: 6| Step: 8
Training loss: 3.145412913812186
Validation loss: 2.697273177337368

Epoch: 6| Step: 9
Training loss: 2.411027006019402
Validation loss: 2.6898465806890415

Epoch: 6| Step: 10
Training loss: 2.877944765028794
Validation loss: 2.677990597138356

Epoch: 6| Step: 11
Training loss: 2.5594419514861717
Validation loss: 2.675742189322698

Epoch: 6| Step: 12
Training loss: 3.3020069261890694
Validation loss: 2.676119972959415

Epoch: 6| Step: 13
Training loss: 2.566585354528479
Validation loss: 2.675725705083979

Epoch: 124| Step: 0
Training loss: 2.584651200689105
Validation loss: 2.6649349772212747

Epoch: 6| Step: 1
Training loss: 3.632353985471273
Validation loss: 2.6670635072954707

Epoch: 6| Step: 2
Training loss: 2.6403035526889322
Validation loss: 2.6613122571549015

Epoch: 6| Step: 3
Training loss: 2.3344626986002597
Validation loss: 2.662603712952949

Epoch: 6| Step: 4
Training loss: 3.1802751527662267
Validation loss: 2.6668396609731686

Epoch: 6| Step: 5
Training loss: 3.2280837345347733
Validation loss: 2.682144388149193

Epoch: 6| Step: 6
Training loss: 2.9180542143085764
Validation loss: 2.6818097699667955

Epoch: 6| Step: 7
Training loss: 2.6593676054837156
Validation loss: 2.683614862075993

Epoch: 6| Step: 8
Training loss: 2.9345867239037773
Validation loss: 2.6859607958216327

Epoch: 6| Step: 9
Training loss: 2.33271674683393
Validation loss: 2.685722323088899

Epoch: 6| Step: 10
Training loss: 3.4754546382634537
Validation loss: 2.6770154707309657

Epoch: 6| Step: 11
Training loss: 3.299267953536942
Validation loss: 2.676366793849294

Epoch: 6| Step: 12
Training loss: 3.519343194372409
Validation loss: 2.6847963037878433

Epoch: 6| Step: 13
Training loss: 2.8082134217452763
Validation loss: 2.6819677969580384

Epoch: 125| Step: 0
Training loss: 3.345793482890324
Validation loss: 2.663738732058429

Epoch: 6| Step: 1
Training loss: 3.1004664408567475
Validation loss: 2.662312957373042

Epoch: 6| Step: 2
Training loss: 3.481766481285067
Validation loss: 2.6622225822411143

Epoch: 6| Step: 3
Training loss: 3.256591860934688
Validation loss: 2.6609985796981452

Epoch: 6| Step: 4
Training loss: 2.8131930027293666
Validation loss: 2.658284023693086

Epoch: 6| Step: 5
Training loss: 2.8512104810010457
Validation loss: 2.6574587345265934

Epoch: 6| Step: 6
Training loss: 2.536709772346102
Validation loss: 2.6584372175699253

Epoch: 6| Step: 7
Training loss: 3.0360751730919047
Validation loss: 2.65610874579791

Epoch: 6| Step: 8
Training loss: 3.0089541956063943
Validation loss: 2.661244177022028

Epoch: 6| Step: 9
Training loss: 2.629523240306782
Validation loss: 2.662240030238426

Epoch: 6| Step: 10
Training loss: 2.249431538296598
Validation loss: 2.6622815018751105

Epoch: 6| Step: 11
Training loss: 3.144168202547867
Validation loss: 2.6624425675139047

Epoch: 6| Step: 12
Training loss: 3.0201240145249586
Validation loss: 2.6670584007976776

Epoch: 6| Step: 13
Training loss: 3.279626062989058
Validation loss: 2.6609571805559087

Epoch: 126| Step: 0
Training loss: 3.0445615152680796
Validation loss: 2.6761291043066993

Epoch: 6| Step: 1
Training loss: 3.57985768573415
Validation loss: 2.671749461055925

Epoch: 6| Step: 2
Training loss: 2.922442702359069
Validation loss: 2.6685502353358443

Epoch: 6| Step: 3
Training loss: 2.4678567163554974
Validation loss: 2.6639218034460592

Epoch: 6| Step: 4
Training loss: 2.8255551290701355
Validation loss: 2.663544169430038

Epoch: 6| Step: 5
Training loss: 3.2300868702358487
Validation loss: 2.660838220552252

Epoch: 6| Step: 6
Training loss: 3.0894031234594044
Validation loss: 2.663610896257317

Epoch: 6| Step: 7
Training loss: 2.8181066789706235
Validation loss: 2.6566042456968075

Epoch: 6| Step: 8
Training loss: 3.0699242110946705
Validation loss: 2.6575144683067737

Epoch: 6| Step: 9
Training loss: 2.7995923017958195
Validation loss: 2.6549988533210507

Epoch: 6| Step: 10
Training loss: 3.154076914012148
Validation loss: 2.664820498921717

Epoch: 6| Step: 11
Training loss: 2.8633038986996464
Validation loss: 2.664775883386287

Epoch: 6| Step: 12
Training loss: 2.808311564948803
Validation loss: 2.673372810382639

Epoch: 6| Step: 13
Training loss: 2.9914911719856154
Validation loss: 2.67062677398603

Epoch: 127| Step: 0
Training loss: 3.0185875448106576
Validation loss: 2.7020663757978705

Epoch: 6| Step: 1
Training loss: 2.549936338639795
Validation loss: 2.716368944573733

Epoch: 6| Step: 2
Training loss: 2.727212997707235
Validation loss: 2.7513804080133655

Epoch: 6| Step: 3
Training loss: 2.659519113654488
Validation loss: 2.7668133988692647

Epoch: 6| Step: 4
Training loss: 3.903847405656424
Validation loss: 2.7487490475870304

Epoch: 6| Step: 5
Training loss: 2.7639979517221107
Validation loss: 2.7164056684916256

Epoch: 6| Step: 6
Training loss: 3.142422896890683
Validation loss: 2.6875618405010737

Epoch: 6| Step: 7
Training loss: 3.3387345106079067
Validation loss: 2.6686698295000264

Epoch: 6| Step: 8
Training loss: 2.913050580681343
Validation loss: 2.660219914381025

Epoch: 6| Step: 9
Training loss: 3.3268390015613276
Validation loss: 2.6545431725208792

Epoch: 6| Step: 10
Training loss: 2.172855512636859
Validation loss: 2.655467543187053

Epoch: 6| Step: 11
Training loss: 3.155680387860461
Validation loss: 2.6548237242603623

Epoch: 6| Step: 12
Training loss: 2.9736387456324227
Validation loss: 2.655641490670082

Epoch: 6| Step: 13
Training loss: 3.375605634814949
Validation loss: 2.6581789042910424

Epoch: 128| Step: 0
Training loss: 3.3840731414844605
Validation loss: 2.6587648824579007

Epoch: 6| Step: 1
Training loss: 2.565901011257596
Validation loss: 2.656176189287435

Epoch: 6| Step: 2
Training loss: 3.3616792095393517
Validation loss: 2.6562201964547953

Epoch: 6| Step: 3
Training loss: 2.959085252508993
Validation loss: 2.6591215301857125

Epoch: 6| Step: 4
Training loss: 2.967830475399983
Validation loss: 2.656242262499046

Epoch: 6| Step: 5
Training loss: 3.0205385963210656
Validation loss: 2.645377516068836

Epoch: 6| Step: 6
Training loss: 3.357364212437497
Validation loss: 2.6504839570750645

Epoch: 6| Step: 7
Training loss: 2.565728177819797
Validation loss: 2.658104977395351

Epoch: 6| Step: 8
Training loss: 2.66497277065201
Validation loss: 2.667511708691093

Epoch: 6| Step: 9
Training loss: 2.914664607701995
Validation loss: 2.6714904024772195

Epoch: 6| Step: 10
Training loss: 3.4070381862514667
Validation loss: 2.678727590947524

Epoch: 6| Step: 11
Training loss: 3.1557018446108116
Validation loss: 2.679267499518939

Epoch: 6| Step: 12
Training loss: 2.425504856058362
Validation loss: 2.688409895946598

Epoch: 6| Step: 13
Training loss: 2.88460941705331
Validation loss: 2.6883629541183147

Epoch: 129| Step: 0
Training loss: 2.5723388476076825
Validation loss: 2.678277312312476

Epoch: 6| Step: 1
Training loss: 3.2254429830852365
Validation loss: 2.685136556623114

Epoch: 6| Step: 2
Training loss: 2.906319237981527
Validation loss: 2.720115630758148

Epoch: 6| Step: 3
Training loss: 3.0188865614936566
Validation loss: 2.7258555724665765

Epoch: 6| Step: 4
Training loss: 2.658850799070213
Validation loss: 2.732514452355312

Epoch: 6| Step: 5
Training loss: 2.3725050821842792
Validation loss: 2.7147532708491875

Epoch: 6| Step: 6
Training loss: 3.2386717336569744
Validation loss: 2.7008528289616263

Epoch: 6| Step: 7
Training loss: 3.4065879164201536
Validation loss: 2.6846890507401686

Epoch: 6| Step: 8
Training loss: 3.0159385399564154
Validation loss: 2.66224507134887

Epoch: 6| Step: 9
Training loss: 3.0289050598378338
Validation loss: 2.6559683141879127

Epoch: 6| Step: 10
Training loss: 3.209458356181534
Validation loss: 2.648990007323839

Epoch: 6| Step: 11
Training loss: 3.454593674081357
Validation loss: 2.649974645414777

Epoch: 6| Step: 12
Training loss: 2.8440598539124875
Validation loss: 2.649880627857167

Epoch: 6| Step: 13
Training loss: 2.6909986389394867
Validation loss: 2.6408742918174997

Epoch: 130| Step: 0
Training loss: 3.5436330086370886
Validation loss: 2.640854296140353

Epoch: 6| Step: 1
Training loss: 2.2333618214436677
Validation loss: 2.6389508470199945

Epoch: 6| Step: 2
Training loss: 3.0614930657883446
Validation loss: 2.6395108102318816

Epoch: 6| Step: 3
Training loss: 2.338716620773345
Validation loss: 2.638134515818842

Epoch: 6| Step: 4
Training loss: 3.04124799660124
Validation loss: 2.6330687763702456

Epoch: 6| Step: 5
Training loss: 3.116465090456522
Validation loss: 2.6397291871321706

Epoch: 6| Step: 6
Training loss: 3.2598987367032692
Validation loss: 2.6379065367627916

Epoch: 6| Step: 7
Training loss: 2.4248588638921116
Validation loss: 2.6415808619258527

Epoch: 6| Step: 8
Training loss: 2.858322033058124
Validation loss: 2.642391391928644

Epoch: 6| Step: 9
Training loss: 2.740168249157596
Validation loss: 2.6452716560322207

Epoch: 6| Step: 10
Training loss: 3.447654844794213
Validation loss: 2.6415586860253337

Epoch: 6| Step: 11
Training loss: 2.6209793179542724
Validation loss: 2.654010832175447

Epoch: 6| Step: 12
Training loss: 3.389176582326298
Validation loss: 2.6484405307508623

Epoch: 6| Step: 13
Training loss: 3.4424335102649715
Validation loss: 2.65867823602187

Epoch: 131| Step: 0
Training loss: 2.614700821343015
Validation loss: 2.6528933728847073

Epoch: 6| Step: 1
Training loss: 3.1079538442653063
Validation loss: 2.6467464719699634

Epoch: 6| Step: 2
Training loss: 2.4542886683174903
Validation loss: 2.6466383429215643

Epoch: 6| Step: 3
Training loss: 3.106197708273068
Validation loss: 2.640574030084798

Epoch: 6| Step: 4
Training loss: 2.1560805986728866
Validation loss: 2.6412732893477235

Epoch: 6| Step: 5
Training loss: 3.0125153473902366
Validation loss: 2.6371236559375273

Epoch: 6| Step: 6
Training loss: 3.2757368001508156
Validation loss: 2.6379034467745526

Epoch: 6| Step: 7
Training loss: 3.1531740818041007
Validation loss: 2.6346703407705294

Epoch: 6| Step: 8
Training loss: 3.145303761621229
Validation loss: 2.640471635833081

Epoch: 6| Step: 9
Training loss: 3.2803344902587233
Validation loss: 2.6394083183981603

Epoch: 6| Step: 10
Training loss: 2.7712319644243015
Validation loss: 2.638156918740266

Epoch: 6| Step: 11
Training loss: 3.2781085280303657
Validation loss: 2.641475904704148

Epoch: 6| Step: 12
Training loss: 2.7028123972514866
Validation loss: 2.639739551498063

Epoch: 6| Step: 13
Training loss: 3.603790322355062
Validation loss: 2.6479110647102115

Epoch: 132| Step: 0
Training loss: 3.2766941959108156
Validation loss: 2.6398353976173725

Epoch: 6| Step: 1
Training loss: 3.266007688449049
Validation loss: 2.6373143722389005

Epoch: 6| Step: 2
Training loss: 2.6210550001210713
Validation loss: 2.6335259504838073

Epoch: 6| Step: 3
Training loss: 3.085598774922736
Validation loss: 2.6316723333081873

Epoch: 6| Step: 4
Training loss: 3.1144162816866285
Validation loss: 2.6316453833416453

Epoch: 6| Step: 5
Training loss: 3.2735709297463034
Validation loss: 2.6305567928568485

Epoch: 6| Step: 6
Training loss: 2.751734619995954
Validation loss: 2.6312014002709887

Epoch: 6| Step: 7
Training loss: 3.111087193472826
Validation loss: 2.633568253934922

Epoch: 6| Step: 8
Training loss: 2.49240637508698
Validation loss: 2.63294704810728

Epoch: 6| Step: 9
Training loss: 2.922217201144792
Validation loss: 2.636986080641353

Epoch: 6| Step: 10
Training loss: 2.7383415008751264
Validation loss: 2.6337437120158316

Epoch: 6| Step: 11
Training loss: 3.184709711425034
Validation loss: 2.642859637446574

Epoch: 6| Step: 12
Training loss: 2.679252744946111
Validation loss: 2.6450189569792912

Epoch: 6| Step: 13
Training loss: 3.0155878410619454
Validation loss: 2.6396212300136295

Epoch: 133| Step: 0
Training loss: 3.0267720919437964
Validation loss: 2.6460750350901425

Epoch: 6| Step: 1
Training loss: 2.3246582641486175
Validation loss: 2.6414961984799397

Epoch: 6| Step: 2
Training loss: 3.3733932061090153
Validation loss: 2.642113234373739

Epoch: 6| Step: 3
Training loss: 2.808875227658869
Validation loss: 2.638907525285141

Epoch: 6| Step: 4
Training loss: 2.596603845792119
Validation loss: 2.632995051905351

Epoch: 6| Step: 5
Training loss: 2.5067528122783136
Validation loss: 2.634761535456

Epoch: 6| Step: 6
Training loss: 2.6963818184255888
Validation loss: 2.6402869510882003

Epoch: 6| Step: 7
Training loss: 3.3580964250199017
Validation loss: 2.637383704826016

Epoch: 6| Step: 8
Training loss: 3.137055603410102
Validation loss: 2.643266153473336

Epoch: 6| Step: 9
Training loss: 2.8098609410695388
Validation loss: 2.635385315566863

Epoch: 6| Step: 10
Training loss: 3.8208292044170427
Validation loss: 2.6341830418424452

Epoch: 6| Step: 11
Training loss: 3.0488831773536997
Validation loss: 2.634970039623409

Epoch: 6| Step: 12
Training loss: 2.9513119576593967
Validation loss: 2.638963536226961

Epoch: 6| Step: 13
Training loss: 2.643845800372991
Validation loss: 2.633182390958997

Epoch: 134| Step: 0
Training loss: 2.8010725351384482
Validation loss: 2.635626662013451

Epoch: 6| Step: 1
Training loss: 3.0719059893111194
Validation loss: 2.63531271723708

Epoch: 6| Step: 2
Training loss: 2.655553748938435
Validation loss: 2.6306662377886645

Epoch: 6| Step: 3
Training loss: 2.848531873586614
Validation loss: 2.6350274729841017

Epoch: 6| Step: 4
Training loss: 3.3605881030976765
Validation loss: 2.629886947648142

Epoch: 6| Step: 5
Training loss: 3.2452507531100774
Validation loss: 2.6267023431234127

Epoch: 6| Step: 6
Training loss: 2.49061999158942
Validation loss: 2.6227204952931396

Epoch: 6| Step: 7
Training loss: 3.5473535693797715
Validation loss: 2.628265505123385

Epoch: 6| Step: 8
Training loss: 2.6715871198507894
Validation loss: 2.6347205131740394

Epoch: 6| Step: 9
Training loss: 2.082248583359545
Validation loss: 2.629301751739397

Epoch: 6| Step: 10
Training loss: 2.944824825964226
Validation loss: 2.631655424951895

Epoch: 6| Step: 11
Training loss: 3.3111995807523016
Validation loss: 2.6259128606441577

Epoch: 6| Step: 12
Training loss: 2.6906585762153075
Validation loss: 2.6271548218945995

Epoch: 6| Step: 13
Training loss: 3.7555406963899634
Validation loss: 2.6287523299042332

Epoch: 135| Step: 0
Training loss: 2.4686438622172973
Validation loss: 2.6408770720569597

Epoch: 6| Step: 1
Training loss: 2.7208394757873773
Validation loss: 2.6375043195098047

Epoch: 6| Step: 2
Training loss: 3.166386106677848
Validation loss: 2.634011231698336

Epoch: 6| Step: 3
Training loss: 3.3312595274254
Validation loss: 2.63468771629534

Epoch: 6| Step: 4
Training loss: 3.0150977746093246
Validation loss: 2.634983899943599

Epoch: 6| Step: 5
Training loss: 2.985905599298947
Validation loss: 2.645551140499598

Epoch: 6| Step: 6
Training loss: 2.657026648580531
Validation loss: 2.6540463528014704

Epoch: 6| Step: 7
Training loss: 2.9274495032668364
Validation loss: 2.635231959715131

Epoch: 6| Step: 8
Training loss: 2.946914358675142
Validation loss: 2.633116341769789

Epoch: 6| Step: 9
Training loss: 3.4312815513619395
Validation loss: 2.6360030194189505

Epoch: 6| Step: 10
Training loss: 2.918867389025544
Validation loss: 2.633411289596522

Epoch: 6| Step: 11
Training loss: 2.681479341528741
Validation loss: 2.6286206148378124

Epoch: 6| Step: 12
Training loss: 3.0140716821541544
Validation loss: 2.6230113069575682

Epoch: 6| Step: 13
Training loss: 3.135579777696202
Validation loss: 2.623546903276462

Epoch: 136| Step: 0
Training loss: 3.020152749752227
Validation loss: 2.6235936094375787

Epoch: 6| Step: 1
Training loss: 2.8618975076084006
Validation loss: 2.6244564548406077

Epoch: 6| Step: 2
Training loss: 2.349865779696081
Validation loss: 2.631765998977836

Epoch: 6| Step: 3
Training loss: 2.9524651653098575
Validation loss: 2.628952412374532

Epoch: 6| Step: 4
Training loss: 2.364231996580451
Validation loss: 2.6280350706154953

Epoch: 6| Step: 5
Training loss: 3.380025724504982
Validation loss: 2.630718687958432

Epoch: 6| Step: 6
Training loss: 3.412303619882424
Validation loss: 2.6302987965943054

Epoch: 6| Step: 7
Training loss: 2.9285365577774947
Validation loss: 2.633276782733803

Epoch: 6| Step: 8
Training loss: 2.9439405893882533
Validation loss: 2.643509177730945

Epoch: 6| Step: 9
Training loss: 2.453967976150547
Validation loss: 2.6404082166880345

Epoch: 6| Step: 10
Training loss: 3.1760175741367656
Validation loss: 2.645714916970289

Epoch: 6| Step: 11
Training loss: 3.3726682732115725
Validation loss: 2.661813569734083

Epoch: 6| Step: 12
Training loss: 3.211154308563854
Validation loss: 2.6758294396027806

Epoch: 6| Step: 13
Training loss: 2.550201396842851
Validation loss: 2.6892689222628112

Epoch: 137| Step: 0
Training loss: 3.1893115412255715
Validation loss: 2.682262926020385

Epoch: 6| Step: 1
Training loss: 2.486868896170098
Validation loss: 2.6726975603559007

Epoch: 6| Step: 2
Training loss: 3.1152490693227106
Validation loss: 2.641500284391074

Epoch: 6| Step: 3
Training loss: 2.9701969836637327
Validation loss: 2.6270319881572317

Epoch: 6| Step: 4
Training loss: 3.5048146510943186
Validation loss: 2.6216112663348383

Epoch: 6| Step: 5
Training loss: 2.862158249356655
Validation loss: 2.6185645379217015

Epoch: 6| Step: 6
Training loss: 2.828758548145468
Validation loss: 2.618853706414874

Epoch: 6| Step: 7
Training loss: 2.418450672642686
Validation loss: 2.6195969093423086

Epoch: 6| Step: 8
Training loss: 3.005335830989816
Validation loss: 2.625690222966995

Epoch: 6| Step: 9
Training loss: 3.068030521245979
Validation loss: 2.621895135102129

Epoch: 6| Step: 10
Training loss: 3.304719388190195
Validation loss: 2.620095749681729

Epoch: 6| Step: 11
Training loss: 3.0302997161384724
Validation loss: 2.6255589196309628

Epoch: 6| Step: 12
Training loss: 2.9634242422822443
Validation loss: 2.6245425763807053

Epoch: 6| Step: 13
Training loss: 2.2414836775768103
Validation loss: 2.62338188163391

Epoch: 138| Step: 0
Training loss: 2.6119607688329403
Validation loss: 2.624970929619162

Epoch: 6| Step: 1
Training loss: 3.1276508532154192
Validation loss: 2.617625502997048

Epoch: 6| Step: 2
Training loss: 2.71079238371278
Validation loss: 2.620630088083115

Epoch: 6| Step: 3
Training loss: 2.756260508130169
Validation loss: 2.62016077882028

Epoch: 6| Step: 4
Training loss: 3.067110289400065
Validation loss: 2.632055316423552

Epoch: 6| Step: 5
Training loss: 3.2593662572727005
Validation loss: 2.647798837323848

Epoch: 6| Step: 6
Training loss: 2.7275231535406963
Validation loss: 2.6448482597826253

Epoch: 6| Step: 7
Training loss: 3.0717586770837415
Validation loss: 2.647371413012325

Epoch: 6| Step: 8
Training loss: 3.2741558588641997
Validation loss: 2.6576032257058797

Epoch: 6| Step: 9
Training loss: 2.98372463309824
Validation loss: 2.6384221185693884

Epoch: 6| Step: 10
Training loss: 2.5925492414251012
Validation loss: 2.6227657079552267

Epoch: 6| Step: 11
Training loss: 2.983965301352838
Validation loss: 2.6207338347076967

Epoch: 6| Step: 12
Training loss: 2.836554510078638
Validation loss: 2.6148462565754134

Epoch: 6| Step: 13
Training loss: 3.5932992403704938
Validation loss: 2.61432353305416

Epoch: 139| Step: 0
Training loss: 2.74141393113059
Validation loss: 2.6124657314412243

Epoch: 6| Step: 1
Training loss: 2.4563386119748367
Validation loss: 2.6162364369780313

Epoch: 6| Step: 2
Training loss: 3.0602022231356503
Validation loss: 2.613821683446334

Epoch: 6| Step: 3
Training loss: 3.50704437885644
Validation loss: 2.6159693844343312

Epoch: 6| Step: 4
Training loss: 2.9041458224000514
Validation loss: 2.614472818353399

Epoch: 6| Step: 5
Training loss: 2.6608819405830824
Validation loss: 2.6123360156255715

Epoch: 6| Step: 6
Training loss: 2.7079926692237968
Validation loss: 2.614158346453506

Epoch: 6| Step: 7
Training loss: 3.288683219298346
Validation loss: 2.6206006141167526

Epoch: 6| Step: 8
Training loss: 2.6369409086096844
Validation loss: 2.626485381863675

Epoch: 6| Step: 9
Training loss: 2.719036481825106
Validation loss: 2.625170479984558

Epoch: 6| Step: 10
Training loss: 2.4210526577668
Validation loss: 2.6293620428279816

Epoch: 6| Step: 11
Training loss: 3.395949441683965
Validation loss: 2.6196897923143054

Epoch: 6| Step: 12
Training loss: 3.2635441771659037
Validation loss: 2.6258458221686642

Epoch: 6| Step: 13
Training loss: 3.5627957940694914
Validation loss: 2.6163810749958185

Epoch: 140| Step: 0
Training loss: 2.921260014240564
Validation loss: 2.6102348788835514

Epoch: 6| Step: 1
Training loss: 3.0893446257539487
Validation loss: 2.6152378147716546

Epoch: 6| Step: 2
Training loss: 3.1217576563321123
Validation loss: 2.609788108027077

Epoch: 6| Step: 3
Training loss: 3.0952574785205513
Validation loss: 2.612084427108791

Epoch: 6| Step: 4
Training loss: 2.7083882155115115
Validation loss: 2.610510193253566

Epoch: 6| Step: 5
Training loss: 2.840426336712852
Validation loss: 2.608445629161158

Epoch: 6| Step: 6
Training loss: 2.9477239395243613
Validation loss: 2.6140604516463455

Epoch: 6| Step: 7
Training loss: 3.093755856903389
Validation loss: 2.6145428125444288

Epoch: 6| Step: 8
Training loss: 3.243594533028125
Validation loss: 2.6113295583374136

Epoch: 6| Step: 9
Training loss: 2.5970384824044013
Validation loss: 2.6124811330676208

Epoch: 6| Step: 10
Training loss: 3.0874013784619123
Validation loss: 2.6099008373356813

Epoch: 6| Step: 11
Training loss: 2.6959701841386616
Validation loss: 2.6117341354573775

Epoch: 6| Step: 12
Training loss: 2.784809535035267
Validation loss: 2.6119027438456017

Epoch: 6| Step: 13
Training loss: 3.119875711997673
Validation loss: 2.6192958596414746

Epoch: 141| Step: 0
Training loss: 2.1944863064578364
Validation loss: 2.6163681234093255

Epoch: 6| Step: 1
Training loss: 2.7678836715762936
Validation loss: 2.6240098465782395

Epoch: 6| Step: 2
Training loss: 2.8105115855150737
Validation loss: 2.6139623315237146

Epoch: 6| Step: 3
Training loss: 3.0238229274273896
Validation loss: 2.6119925947722087

Epoch: 6| Step: 4
Training loss: 2.5802804894972136
Validation loss: 2.626618976950525

Epoch: 6| Step: 5
Training loss: 2.5173485579488255
Validation loss: 2.614297612372193

Epoch: 6| Step: 6
Training loss: 3.081357855775853
Validation loss: 2.6148008765842077

Epoch: 6| Step: 7
Training loss: 3.431611583675848
Validation loss: 2.61830036499937

Epoch: 6| Step: 8
Training loss: 3.011601902220155
Validation loss: 2.6152338348701556

Epoch: 6| Step: 9
Training loss: 3.1049424979126807
Validation loss: 2.6128212655628884

Epoch: 6| Step: 10
Training loss: 3.489659702357357
Validation loss: 2.6105832259979023

Epoch: 6| Step: 11
Training loss: 3.010836104550016
Validation loss: 2.6053345894403632

Epoch: 6| Step: 12
Training loss: 2.633066046304046
Validation loss: 2.609453785290072

Epoch: 6| Step: 13
Training loss: 3.6008596665428056
Validation loss: 2.612559405683412

Epoch: 142| Step: 0
Training loss: 3.4908073138623177
Validation loss: 2.6089168453637974

Epoch: 6| Step: 1
Training loss: 2.986369959137165
Validation loss: 2.609199810330525

Epoch: 6| Step: 2
Training loss: 2.232576513399618
Validation loss: 2.606874583101132

Epoch: 6| Step: 3
Training loss: 2.790099306561043
Validation loss: 2.6072708861057565

Epoch: 6| Step: 4
Training loss: 2.826634641351959
Validation loss: 2.607774916532584

Epoch: 6| Step: 5
Training loss: 2.6678950242318464
Validation loss: 2.613555200764299

Epoch: 6| Step: 6
Training loss: 3.019828910023379
Validation loss: 2.6100862813415984

Epoch: 6| Step: 7
Training loss: 3.1083516492806824
Validation loss: 2.63008760857292

Epoch: 6| Step: 8
Training loss: 3.1472214084798895
Validation loss: 2.6159019999925115

Epoch: 6| Step: 9
Training loss: 2.659037305681395
Validation loss: 2.6129166087358935

Epoch: 6| Step: 10
Training loss: 3.2958059882867436
Validation loss: 2.612789810795095

Epoch: 6| Step: 11
Training loss: 2.570724129688823
Validation loss: 2.611940072832737

Epoch: 6| Step: 12
Training loss: 2.802074607258839
Validation loss: 2.6166565401947146

Epoch: 6| Step: 13
Training loss: 3.7689994160122113
Validation loss: 2.6146862259978416

Epoch: 143| Step: 0
Training loss: 2.769416322462255
Validation loss: 2.6102993660169482

Epoch: 6| Step: 1
Training loss: 2.782670590500136
Validation loss: 2.6073937363900033

Epoch: 6| Step: 2
Training loss: 2.244840215761317
Validation loss: 2.6100152610373817

Epoch: 6| Step: 3
Training loss: 2.698629533023447
Validation loss: 2.612278763469247

Epoch: 6| Step: 4
Training loss: 2.4063664197625614
Validation loss: 2.6061742638254612

Epoch: 6| Step: 5
Training loss: 3.2283052997885093
Validation loss: 2.611063159063625

Epoch: 6| Step: 6
Training loss: 2.9883023130385133
Validation loss: 2.6101527001669846

Epoch: 6| Step: 7
Training loss: 2.808485429608848
Validation loss: 2.6058504960456874

Epoch: 6| Step: 8
Training loss: 3.6573909707288
Validation loss: 2.612730353008417

Epoch: 6| Step: 9
Training loss: 2.935474102973575
Validation loss: 2.6143183544297726

Epoch: 6| Step: 10
Training loss: 3.087157035034007
Validation loss: 2.6139124540826875

Epoch: 6| Step: 11
Training loss: 3.0909898563787506
Validation loss: 2.616196383896513

Epoch: 6| Step: 12
Training loss: 3.3601652413562566
Validation loss: 2.6093835877828675

Epoch: 6| Step: 13
Training loss: 2.890872759124525
Validation loss: 2.608184268661011

Epoch: 144| Step: 0
Training loss: 2.681716906746894
Validation loss: 2.613439583251893

Epoch: 6| Step: 1
Training loss: 2.7449356610760964
Validation loss: 2.606249632763679

Epoch: 6| Step: 2
Training loss: 2.8164578881470193
Validation loss: 2.6091178898533656

Epoch: 6| Step: 3
Training loss: 3.1230154220781694
Validation loss: 2.6042758182087624

Epoch: 6| Step: 4
Training loss: 3.010105596572095
Validation loss: 2.6073631138356306

Epoch: 6| Step: 5
Training loss: 3.4707520352138967
Validation loss: 2.6040388857757977

Epoch: 6| Step: 6
Training loss: 2.687888006545284
Validation loss: 2.6040784313061374

Epoch: 6| Step: 7
Training loss: 2.6784173830331923
Validation loss: 2.600279231179047

Epoch: 6| Step: 8
Training loss: 2.752253302832143
Validation loss: 2.602139027074368

Epoch: 6| Step: 9
Training loss: 3.1649467080497455
Validation loss: 2.5998379389816137

Epoch: 6| Step: 10
Training loss: 2.3664242360592374
Validation loss: 2.6014371067740463

Epoch: 6| Step: 11
Training loss: 3.267920458475819
Validation loss: 2.609655003552993

Epoch: 6| Step: 12
Training loss: 3.0368200035477497
Validation loss: 2.6059753424334717

Epoch: 6| Step: 13
Training loss: 3.4041245855176316
Validation loss: 2.6010869404594934

Epoch: 145| Step: 0
Training loss: 3.306976607727611
Validation loss: 2.605906861381407

Epoch: 6| Step: 1
Training loss: 3.2341205294105664
Validation loss: 2.60320044544352

Epoch: 6| Step: 2
Training loss: 3.192956181541816
Validation loss: 2.622858611640385

Epoch: 6| Step: 3
Training loss: 3.1248892192278257
Validation loss: 2.6219339722968416

Epoch: 6| Step: 4
Training loss: 2.916217733074679
Validation loss: 2.619489030726029

Epoch: 6| Step: 5
Training loss: 3.2561370419991995
Validation loss: 2.627630485210089

Epoch: 6| Step: 6
Training loss: 2.831395234347105
Validation loss: 2.644947501222411

Epoch: 6| Step: 7
Training loss: 2.650715436267379
Validation loss: 2.6538713057852945

Epoch: 6| Step: 8
Training loss: 2.465485744629284
Validation loss: 2.677829021069697

Epoch: 6| Step: 9
Training loss: 2.9792565996872082
Validation loss: 2.7066448468419884

Epoch: 6| Step: 10
Training loss: 2.4712899577914684
Validation loss: 2.6918153031515892

Epoch: 6| Step: 11
Training loss: 3.3011855741177567
Validation loss: 2.669608827926946

Epoch: 6| Step: 12
Training loss: 2.3643553255590133
Validation loss: 2.6198948773152524

Epoch: 6| Step: 13
Training loss: 2.92543604282326
Validation loss: 2.6028512594881534

Epoch: 146| Step: 0
Training loss: 2.9286876549070375
Validation loss: 2.593949239239191

Epoch: 6| Step: 1
Training loss: 2.957852409565092
Validation loss: 2.6032275741736743

Epoch: 6| Step: 2
Training loss: 3.266104193030505
Validation loss: 2.610953257676608

Epoch: 6| Step: 3
Training loss: 3.0919344420200763
Validation loss: 2.613928468987713

Epoch: 6| Step: 4
Training loss: 3.4799357647283866
Validation loss: 2.6128104510054846

Epoch: 6| Step: 5
Training loss: 2.8898878652603974
Validation loss: 2.620303813371966

Epoch: 6| Step: 6
Training loss: 2.7437564893224353
Validation loss: 2.614960570739576

Epoch: 6| Step: 7
Training loss: 3.0111604363537183
Validation loss: 2.618561644900677

Epoch: 6| Step: 8
Training loss: 2.9188409239760804
Validation loss: 2.622061151283154

Epoch: 6| Step: 9
Training loss: 3.0923501903418105
Validation loss: 2.6186417201928665

Epoch: 6| Step: 10
Training loss: 2.9073787671150093
Validation loss: 2.623697014142869

Epoch: 6| Step: 11
Training loss: 2.885553478389882
Validation loss: 2.6200296954416795

Epoch: 6| Step: 12
Training loss: 3.0751790312465954
Validation loss: 2.616999769265765

Epoch: 6| Step: 13
Training loss: 2.107361574344373
Validation loss: 2.618498082806818

Epoch: 147| Step: 0
Training loss: 3.002357510275356
Validation loss: 2.6143903129530264

Epoch: 6| Step: 1
Training loss: 2.522828681182285
Validation loss: 2.616682731428094

Epoch: 6| Step: 2
Training loss: 3.289918620485763
Validation loss: 2.6155778210790546

Epoch: 6| Step: 3
Training loss: 2.47983073041439
Validation loss: 2.6132088135866742

Epoch: 6| Step: 4
Training loss: 3.1213299371873884
Validation loss: 2.6095685429620796

Epoch: 6| Step: 5
Training loss: 3.2366727324355935
Validation loss: 2.607621385051985

Epoch: 6| Step: 6
Training loss: 2.65174789365043
Validation loss: 2.6011291416918136

Epoch: 6| Step: 7
Training loss: 3.3483756326219183
Validation loss: 2.6006409855963373

Epoch: 6| Step: 8
Training loss: 3.0231438230473096
Validation loss: 2.603850140111399

Epoch: 6| Step: 9
Training loss: 3.0914371977696566
Validation loss: 2.5964323037352304

Epoch: 6| Step: 10
Training loss: 2.1784965515669765
Validation loss: 2.5995416424161464

Epoch: 6| Step: 11
Training loss: 3.323280497915014
Validation loss: 2.6260308294045838

Epoch: 6| Step: 12
Training loss: 2.994005890742988
Validation loss: 2.6541765636629764

Epoch: 6| Step: 13
Training loss: 2.6791916102014026
Validation loss: 2.716470207837817

Epoch: 148| Step: 0
Training loss: 3.018663052082706
Validation loss: 2.7631015174116293

Epoch: 6| Step: 1
Training loss: 3.3098879898342517
Validation loss: 2.729291423797676

Epoch: 6| Step: 2
Training loss: 2.880894630345717
Validation loss: 2.6340703045001908

Epoch: 6| Step: 3
Training loss: 3.1283627536903
Validation loss: 2.607607931814812

Epoch: 6| Step: 4
Training loss: 3.057908021490429
Validation loss: 2.5933561573383908

Epoch: 6| Step: 5
Training loss: 2.4309945960351595
Validation loss: 2.60421053859713

Epoch: 6| Step: 6
Training loss: 3.5014194607403537
Validation loss: 2.606773044466041

Epoch: 6| Step: 7
Training loss: 3.634306176043178
Validation loss: 2.608195746211195

Epoch: 6| Step: 8
Training loss: 2.769108534218149
Validation loss: 2.6123995431571267

Epoch: 6| Step: 9
Training loss: 2.595200833760985
Validation loss: 2.612506948129072

Epoch: 6| Step: 10
Training loss: 2.152137483381461
Validation loss: 2.612432163529667

Epoch: 6| Step: 11
Training loss: 2.4927289129529666
Validation loss: 2.616517733940335

Epoch: 6| Step: 12
Training loss: 3.044548672456645
Validation loss: 2.6158905121533205

Epoch: 6| Step: 13
Training loss: 3.6447254059684813
Validation loss: 2.6122859962368947

Epoch: 149| Step: 0
Training loss: 2.284247741076862
Validation loss: 2.6154044682003486

Epoch: 6| Step: 1
Training loss: 2.7465846788181683
Validation loss: 2.6103627191946774

Epoch: 6| Step: 2
Training loss: 3.3038940920169715
Validation loss: 2.6128290992698546

Epoch: 6| Step: 3
Training loss: 2.7860640107172636
Validation loss: 2.6079678409880374

Epoch: 6| Step: 4
Training loss: 2.6942814278420415
Validation loss: 2.605862247532731

Epoch: 6| Step: 5
Training loss: 3.1433116751363266
Validation loss: 2.6092591451875387

Epoch: 6| Step: 6
Training loss: 3.0712618925169437
Validation loss: 2.6030604691117127

Epoch: 6| Step: 7
Training loss: 2.8356679572857346
Validation loss: 2.5984427299671577

Epoch: 6| Step: 8
Training loss: 2.8124165840494415
Validation loss: 2.5962575658618987

Epoch: 6| Step: 9
Training loss: 3.198055964282613
Validation loss: 2.594636987099166

Epoch: 6| Step: 10
Training loss: 2.6213999084770667
Validation loss: 2.5936069669004502

Epoch: 6| Step: 11
Training loss: 3.522643276790538
Validation loss: 2.588852971349326

Epoch: 6| Step: 12
Training loss: 3.3914071161057695
Validation loss: 2.596125417762358

Epoch: 6| Step: 13
Training loss: 2.559788083007503
Validation loss: 2.594750332166061

Epoch: 150| Step: 0
Training loss: 3.082372223494418
Validation loss: 2.6028518277958814

Epoch: 6| Step: 1
Training loss: 3.014685291674226
Validation loss: 2.6021329266772497

Epoch: 6| Step: 2
Training loss: 3.6657976218811976
Validation loss: 2.599615936925581

Epoch: 6| Step: 3
Training loss: 2.6510074715697893
Validation loss: 2.5973138542068135

Epoch: 6| Step: 4
Training loss: 2.4965550529205607
Validation loss: 2.5984093735719713

Epoch: 6| Step: 5
Training loss: 3.160833524312916
Validation loss: 2.593591484823315

Epoch: 6| Step: 6
Training loss: 3.176046100003226
Validation loss: 2.589634234904153

Epoch: 6| Step: 7
Training loss: 2.9043329951099572
Validation loss: 2.5897419752254756

Epoch: 6| Step: 8
Training loss: 2.1324963457583817
Validation loss: 2.5966426485735257

Epoch: 6| Step: 9
Training loss: 2.6918682703944206
Validation loss: 2.592014315647161

Epoch: 6| Step: 10
Training loss: 3.3100745480258853
Validation loss: 2.5922155178996498

Epoch: 6| Step: 11
Training loss: 2.8871845287708626
Validation loss: 2.59384119926364

Epoch: 6| Step: 12
Training loss: 2.7215789458728117
Validation loss: 2.592060169736441

Epoch: 6| Step: 13
Training loss: 3.005359947747914
Validation loss: 2.592215872942245

Epoch: 151| Step: 0
Training loss: 3.504988113143725
Validation loss: 2.596392354417878

Epoch: 6| Step: 1
Training loss: 3.3338522189316944
Validation loss: 2.600731190111357

Epoch: 6| Step: 2
Training loss: 3.23560976793765
Validation loss: 2.61707940647495

Epoch: 6| Step: 3
Training loss: 2.6783294350438056
Validation loss: 2.606537013739733

Epoch: 6| Step: 4
Training loss: 2.4730619606928133
Validation loss: 2.6225314098501764

Epoch: 6| Step: 5
Training loss: 2.67418744589615
Validation loss: 2.606177839498603

Epoch: 6| Step: 6
Training loss: 3.0393132201969357
Validation loss: 2.6106097963332178

Epoch: 6| Step: 7
Training loss: 3.045234120956947
Validation loss: 2.6071168372144533

Epoch: 6| Step: 8
Training loss: 2.484973571452596
Validation loss: 2.6146223814353906

Epoch: 6| Step: 9
Training loss: 2.5078647881401546
Validation loss: 2.608592110900872

Epoch: 6| Step: 10
Training loss: 2.6062203714346954
Validation loss: 2.604396672745994

Epoch: 6| Step: 11
Training loss: 2.924778439208179
Validation loss: 2.6214714242173915

Epoch: 6| Step: 12
Training loss: 3.3186841135614986
Validation loss: 2.618309510980355

Epoch: 6| Step: 13
Training loss: 2.9861566308467804
Validation loss: 2.648552073715216

Epoch: 152| Step: 0
Training loss: 3.159818237485396
Validation loss: 2.6593490358125367

Epoch: 6| Step: 1
Training loss: 2.5795233431087268
Validation loss: 2.6587867837151147

Epoch: 6| Step: 2
Training loss: 3.124152106651987
Validation loss: 2.6476041239060804

Epoch: 6| Step: 3
Training loss: 2.5624681796447533
Validation loss: 2.623970322007423

Epoch: 6| Step: 4
Training loss: 3.369169214099717
Validation loss: 2.6223197668790714

Epoch: 6| Step: 5
Training loss: 3.048406128206239
Validation loss: 2.5981662700761867

Epoch: 6| Step: 6
Training loss: 2.7914661980764985
Validation loss: 2.6015191124301142

Epoch: 6| Step: 7
Training loss: 2.684118672079699
Validation loss: 2.58943347461819

Epoch: 6| Step: 8
Training loss: 2.510764123026261
Validation loss: 2.589623188879648

Epoch: 6| Step: 9
Training loss: 3.337014232245055
Validation loss: 2.590110722359581

Epoch: 6| Step: 10
Training loss: 3.00034457453106
Validation loss: 2.599823095520769

Epoch: 6| Step: 11
Training loss: 3.129334152622492
Validation loss: 2.5892364849666456

Epoch: 6| Step: 12
Training loss: 2.7755486848025175
Validation loss: 2.584317410568372

Epoch: 6| Step: 13
Training loss: 2.8661136123275943
Validation loss: 2.595708511625837

Epoch: 153| Step: 0
Training loss: 2.7309149076097152
Validation loss: 2.5904036015948915

Epoch: 6| Step: 1
Training loss: 3.613145848392081
Validation loss: 2.5920956640148063

Epoch: 6| Step: 2
Training loss: 3.2061174510304897
Validation loss: 2.5903259500572475

Epoch: 6| Step: 3
Training loss: 2.8172580737659123
Validation loss: 2.5859524410093635

Epoch: 6| Step: 4
Training loss: 3.0983987425872748
Validation loss: 2.5812056555734477

Epoch: 6| Step: 5
Training loss: 2.702973289863015
Validation loss: 2.5786616537794895

Epoch: 6| Step: 6
Training loss: 2.418559308876712
Validation loss: 2.587571935737152

Epoch: 6| Step: 7
Training loss: 3.2323374941780956
Validation loss: 2.5821882648250973

Epoch: 6| Step: 8
Training loss: 2.6456756144295372
Validation loss: 2.585279045743353

Epoch: 6| Step: 9
Training loss: 2.888006065246046
Validation loss: 2.580890656466606

Epoch: 6| Step: 10
Training loss: 3.2564193710824973
Validation loss: 2.5812048222826807

Epoch: 6| Step: 11
Training loss: 3.4973625054644972
Validation loss: 2.5820085924368956

Epoch: 6| Step: 12
Training loss: 1.979845057888892
Validation loss: 2.5824143710033214

Epoch: 6| Step: 13
Training loss: 2.175767223922475
Validation loss: 2.580233446408747

Epoch: 154| Step: 0
Training loss: 2.532532920209204
Validation loss: 2.5793006635205784

Epoch: 6| Step: 1
Training loss: 3.145017977179499
Validation loss: 2.582804413024674

Epoch: 6| Step: 2
Training loss: 3.1417982652322705
Validation loss: 2.5845690803986345

Epoch: 6| Step: 3
Training loss: 1.9278322019171636
Validation loss: 2.5913180669930145

Epoch: 6| Step: 4
Training loss: 2.9828480913727495
Validation loss: 2.597973988404043

Epoch: 6| Step: 5
Training loss: 2.644533774347946
Validation loss: 2.596651339677309

Epoch: 6| Step: 6
Training loss: 2.3586907815701417
Validation loss: 2.6224652018939794

Epoch: 6| Step: 7
Training loss: 2.9157958410977423
Validation loss: 2.602748886232708

Epoch: 6| Step: 8
Training loss: 3.3090234902990017
Validation loss: 2.621022846030756

Epoch: 6| Step: 9
Training loss: 3.323320960120989
Validation loss: 2.5982351120281404

Epoch: 6| Step: 10
Training loss: 3.519718753676275
Validation loss: 2.5855805278563775

Epoch: 6| Step: 11
Training loss: 3.0425167898876166
Validation loss: 2.584468367595811

Epoch: 6| Step: 12
Training loss: 3.0723646454759415
Validation loss: 2.5803435812106983

Epoch: 6| Step: 13
Training loss: 2.506276643758879
Validation loss: 2.583774853096619

Epoch: 155| Step: 0
Training loss: 3.1806759059592977
Validation loss: 2.581414158517456

Epoch: 6| Step: 1
Training loss: 2.6372293161638285
Validation loss: 2.586597421498617

Epoch: 6| Step: 2
Training loss: 3.1764841650795264
Validation loss: 2.5886186669283826

Epoch: 6| Step: 3
Training loss: 3.4038704776350737
Validation loss: 2.589147868200786

Epoch: 6| Step: 4
Training loss: 3.227857722308074
Validation loss: 2.5859145495056146

Epoch: 6| Step: 5
Training loss: 2.363578840702857
Validation loss: 2.590305130684639

Epoch: 6| Step: 6
Training loss: 2.41174422368967
Validation loss: 2.584839318053093

Epoch: 6| Step: 7
Training loss: 2.876370642302451
Validation loss: 2.5868239252062617

Epoch: 6| Step: 8
Training loss: 2.7215482846374033
Validation loss: 2.5836043733353296

Epoch: 6| Step: 9
Training loss: 2.817025506751488
Validation loss: 2.585143601879436

Epoch: 6| Step: 10
Training loss: 3.399518983137383
Validation loss: 2.5862849446104974

Epoch: 6| Step: 11
Training loss: 3.0159213063828347
Validation loss: 2.5850635956963157

Epoch: 6| Step: 12
Training loss: 2.420842892162297
Validation loss: 2.5884679806643724

Epoch: 6| Step: 13
Training loss: 3.471689988776911
Validation loss: 2.595370475737533

Epoch: 156| Step: 0
Training loss: 2.8508918705200768
Validation loss: 2.595889039101037

Epoch: 6| Step: 1
Training loss: 2.8838915782447163
Validation loss: 2.6056729599960082

Epoch: 6| Step: 2
Training loss: 2.7602068743367303
Validation loss: 2.632923536617801

Epoch: 6| Step: 3
Training loss: 2.838751119649541
Validation loss: 2.634184326493041

Epoch: 6| Step: 4
Training loss: 2.669614493724699
Validation loss: 2.641503565732233

Epoch: 6| Step: 5
Training loss: 2.7768023643292357
Validation loss: 2.6531161299552557

Epoch: 6| Step: 6
Training loss: 3.0818206753852393
Validation loss: 2.6400276523458195

Epoch: 6| Step: 7
Training loss: 3.545114895528431
Validation loss: 2.6246655090404785

Epoch: 6| Step: 8
Training loss: 3.1244109552263883
Validation loss: 2.6060213699444903

Epoch: 6| Step: 9
Training loss: 2.4347312069214695
Validation loss: 2.5978139882010978

Epoch: 6| Step: 10
Training loss: 2.713639817076638
Validation loss: 2.5856817813195847

Epoch: 6| Step: 11
Training loss: 2.963203468981498
Validation loss: 2.5882657551232846

Epoch: 6| Step: 12
Training loss: 3.164123986671089
Validation loss: 2.5796716201889835

Epoch: 6| Step: 13
Training loss: 3.2561083391240655
Validation loss: 2.575921508581622

Epoch: 157| Step: 0
Training loss: 2.928063026187645
Validation loss: 2.5864457700157613

Epoch: 6| Step: 1
Training loss: 2.986762245414309
Validation loss: 2.574631282612937

Epoch: 6| Step: 2
Training loss: 2.7279589555510886
Validation loss: 2.5723567707334065

Epoch: 6| Step: 3
Training loss: 3.2869303626798128
Validation loss: 2.5786300164553517

Epoch: 6| Step: 4
Training loss: 3.5061630755034665
Validation loss: 2.579090131413659

Epoch: 6| Step: 5
Training loss: 3.4015299440226903
Validation loss: 2.5771573465428443

Epoch: 6| Step: 6
Training loss: 2.464770816261986
Validation loss: 2.581451572853729

Epoch: 6| Step: 7
Training loss: 2.0126893896231386
Validation loss: 2.5824829806515592

Epoch: 6| Step: 8
Training loss: 2.4079461993230304
Validation loss: 2.582635307547947

Epoch: 6| Step: 9
Training loss: 2.9357320456817866
Validation loss: 2.583813275051894

Epoch: 6| Step: 10
Training loss: 2.9173400283274056
Validation loss: 2.580547896886442

Epoch: 6| Step: 11
Training loss: 3.1854916397629287
Validation loss: 2.581477929604265

Epoch: 6| Step: 12
Training loss: 2.9949640285261405
Validation loss: 2.5751743230165323

Epoch: 6| Step: 13
Training loss: 2.787495234190284
Validation loss: 2.586814993956993

Epoch: 158| Step: 0
Training loss: 3.366127643932503
Validation loss: 2.5826765734660384

Epoch: 6| Step: 1
Training loss: 3.0372268114353203
Validation loss: 2.5815730341221577

Epoch: 6| Step: 2
Training loss: 2.5705772194014638
Validation loss: 2.5929632732087815

Epoch: 6| Step: 3
Training loss: 2.5688854766538514
Validation loss: 2.5837379486694974

Epoch: 6| Step: 4
Training loss: 3.046920071782016
Validation loss: 2.583522491661631

Epoch: 6| Step: 5
Training loss: 2.59324236578697
Validation loss: 2.580636921223437

Epoch: 6| Step: 6
Training loss: 3.5718477984015906
Validation loss: 2.585951843212027

Epoch: 6| Step: 7
Training loss: 2.7857432556043316
Validation loss: 2.5872423591134197

Epoch: 6| Step: 8
Training loss: 2.9679527416836806
Validation loss: 2.582470239267018

Epoch: 6| Step: 9
Training loss: 2.726110470090216
Validation loss: 2.585564562440288

Epoch: 6| Step: 10
Training loss: 3.1910926169807303
Validation loss: 2.5837017650705647

Epoch: 6| Step: 11
Training loss: 2.8375001075509343
Validation loss: 2.5921256055055584

Epoch: 6| Step: 12
Training loss: 2.8009315780241666
Validation loss: 2.5901127345822026

Epoch: 6| Step: 13
Training loss: 2.4064208131082854
Validation loss: 2.606045353351462

Epoch: 159| Step: 0
Training loss: 2.7274842548702614
Validation loss: 2.6106939707684913

Epoch: 6| Step: 1
Training loss: 2.860359397661256
Validation loss: 2.603692754795439

Epoch: 6| Step: 2
Training loss: 3.5494703300666477
Validation loss: 2.5907099106019555

Epoch: 6| Step: 3
Training loss: 3.259024049439
Validation loss: 2.6007613022740674

Epoch: 6| Step: 4
Training loss: 2.208291563202829
Validation loss: 2.603156255462931

Epoch: 6| Step: 5
Training loss: 2.7997828876244086
Validation loss: 2.5865636288806737

Epoch: 6| Step: 6
Training loss: 2.76021516652288
Validation loss: 2.594915288123821

Epoch: 6| Step: 7
Training loss: 3.13538618273612
Validation loss: 2.579039524391764

Epoch: 6| Step: 8
Training loss: 3.287836204133716
Validation loss: 2.581654860449672

Epoch: 6| Step: 9
Training loss: 2.660366593389784
Validation loss: 2.586013119198555

Epoch: 6| Step: 10
Training loss: 3.3469075063343223
Validation loss: 2.589105974682786

Epoch: 6| Step: 11
Training loss: 2.871681288203917
Validation loss: 2.5774999104982563

Epoch: 6| Step: 12
Training loss: 2.6436193519577764
Validation loss: 2.5695202573947307

Epoch: 6| Step: 13
Training loss: 2.1736120509049806
Validation loss: 2.574168865349105

Epoch: 160| Step: 0
Training loss: 2.387164534722863
Validation loss: 2.570487856637731

Epoch: 6| Step: 1
Training loss: 3.4168744062831733
Validation loss: 2.5704882745215722

Epoch: 6| Step: 2
Training loss: 2.793565550311415
Validation loss: 2.5667373963774383

Epoch: 6| Step: 3
Training loss: 2.5816031373762955
Validation loss: 2.571386889773245

Epoch: 6| Step: 4
Training loss: 2.571546820540496
Validation loss: 2.5672125973143554

Epoch: 6| Step: 5
Training loss: 3.1678703095255702
Validation loss: 2.572775020605402

Epoch: 6| Step: 6
Training loss: 3.4451414312334365
Validation loss: 2.5740556736999927

Epoch: 6| Step: 7
Training loss: 3.18238914494454
Validation loss: 2.578718570657298

Epoch: 6| Step: 8
Training loss: 2.2027379771428253
Validation loss: 2.5811999000018164

Epoch: 6| Step: 9
Training loss: 3.027734507291395
Validation loss: 2.596211915263219

Epoch: 6| Step: 10
Training loss: 3.219206249899893
Validation loss: 2.614027995759631

Epoch: 6| Step: 11
Training loss: 2.913316892087214
Validation loss: 2.622990701080775

Epoch: 6| Step: 12
Training loss: 2.9665475807148316
Validation loss: 2.613871762699143

Epoch: 6| Step: 13
Training loss: 2.7326343336529204
Validation loss: 2.5879179691999155

Epoch: 161| Step: 0
Training loss: 3.4016112492738677
Validation loss: 2.583689060455874

Epoch: 6| Step: 1
Training loss: 2.8005926050050522
Validation loss: 2.5730029194601993

Epoch: 6| Step: 2
Training loss: 2.544646522691424
Validation loss: 2.570772057812501

Epoch: 6| Step: 3
Training loss: 2.469719129183901
Validation loss: 2.5733238433754506

Epoch: 6| Step: 4
Training loss: 3.2870243671234745
Validation loss: 2.567170574520975

Epoch: 6| Step: 5
Training loss: 2.492907189996164
Validation loss: 2.5643299185869637

Epoch: 6| Step: 6
Training loss: 3.43630974710365
Validation loss: 2.5737789738873285

Epoch: 6| Step: 7
Training loss: 2.4942444351805615
Validation loss: 2.570508659989705

Epoch: 6| Step: 8
Training loss: 2.8734426841668332
Validation loss: 2.5727213034396805

Epoch: 6| Step: 9
Training loss: 2.9199759283771924
Validation loss: 2.5718176061455025

Epoch: 6| Step: 10
Training loss: 2.8906352274945757
Validation loss: 2.5673180142678413

Epoch: 6| Step: 11
Training loss: 2.880428993275608
Validation loss: 2.5697452874197952

Epoch: 6| Step: 12
Training loss: 2.9038845813713516
Validation loss: 2.5744610647073514

Epoch: 6| Step: 13
Training loss: 3.4670778660333235
Validation loss: 2.5791411534274284

Epoch: 162| Step: 0
Training loss: 2.6302949998129983
Validation loss: 2.5773327586869583

Epoch: 6| Step: 1
Training loss: 3.112141054589999
Validation loss: 2.5864526835067543

Epoch: 6| Step: 2
Training loss: 2.5360693558255405
Validation loss: 2.6122352094192456

Epoch: 6| Step: 3
Training loss: 2.75172924812389
Validation loss: 2.6291599331384994

Epoch: 6| Step: 4
Training loss: 2.8191422758326192
Validation loss: 2.6295144326385573

Epoch: 6| Step: 5
Training loss: 2.8227680537857927
Validation loss: 2.630595278027903

Epoch: 6| Step: 6
Training loss: 2.8810411092042414
Validation loss: 2.618342633439769

Epoch: 6| Step: 7
Training loss: 3.408314910192216
Validation loss: 2.5882651380505193

Epoch: 6| Step: 8
Training loss: 3.1688642071188817
Validation loss: 2.590157647419772

Epoch: 6| Step: 9
Training loss: 2.901890592251785
Validation loss: 2.5858806557636673

Epoch: 6| Step: 10
Training loss: 2.249775981347378
Validation loss: 2.57484308384541

Epoch: 6| Step: 11
Training loss: 3.3060689433086776
Validation loss: 2.5749245216895393

Epoch: 6| Step: 12
Training loss: 3.2896658373474854
Validation loss: 2.5719540079505205

Epoch: 6| Step: 13
Training loss: 2.6919688839524074
Validation loss: 2.5681527170534184

Epoch: 163| Step: 0
Training loss: 2.0642542459047624
Validation loss: 2.57199442855595

Epoch: 6| Step: 1
Training loss: 3.126278730075122
Validation loss: 2.56232555845118

Epoch: 6| Step: 2
Training loss: 3.4550331332747546
Validation loss: 2.5664880884077825

Epoch: 6| Step: 3
Training loss: 2.836908851920774
Validation loss: 2.562986462286494

Epoch: 6| Step: 4
Training loss: 2.297856957866594
Validation loss: 2.560732013737373

Epoch: 6| Step: 5
Training loss: 2.3149051015694924
Validation loss: 2.5640571079216326

Epoch: 6| Step: 6
Training loss: 3.251822327512226
Validation loss: 2.5653060957700853

Epoch: 6| Step: 7
Training loss: 2.9971867722363346
Validation loss: 2.5625655033074217

Epoch: 6| Step: 8
Training loss: 3.423398588822111
Validation loss: 2.5726188702218207

Epoch: 6| Step: 9
Training loss: 2.840677131199655
Validation loss: 2.567425060481997

Epoch: 6| Step: 10
Training loss: 3.5096725595879885
Validation loss: 2.5646449090446555

Epoch: 6| Step: 11
Training loss: 3.1331357717794956
Validation loss: 2.565980154255164

Epoch: 6| Step: 12
Training loss: 2.4604733952876745
Validation loss: 2.5658873192933114

Epoch: 6| Step: 13
Training loss: 2.3114716588914512
Validation loss: 2.562148447480209

Epoch: 164| Step: 0
Training loss: 2.5849506996220097
Validation loss: 2.5611478197766004

Epoch: 6| Step: 1
Training loss: 2.5786615911464716
Validation loss: 2.562753032093892

Epoch: 6| Step: 2
Training loss: 3.3463075072997466
Validation loss: 2.5642769547801754

Epoch: 6| Step: 3
Training loss: 3.3776726207203196
Validation loss: 2.572004027758341

Epoch: 6| Step: 4
Training loss: 3.086763295610378
Validation loss: 2.56818830517029

Epoch: 6| Step: 5
Training loss: 2.1007236096868276
Validation loss: 2.5625055664706355

Epoch: 6| Step: 6
Training loss: 2.7820216619277613
Validation loss: 2.5674226480420854

Epoch: 6| Step: 7
Training loss: 3.484002063448412
Validation loss: 2.5683379443565255

Epoch: 6| Step: 8
Training loss: 2.912297509484984
Validation loss: 2.5712336771397353

Epoch: 6| Step: 9
Training loss: 2.6539440243300807
Validation loss: 2.5668899106105734

Epoch: 6| Step: 10
Training loss: 3.1793613125937834
Validation loss: 2.5712080339619576

Epoch: 6| Step: 11
Training loss: 2.971944916842356
Validation loss: 2.5767585242979014

Epoch: 6| Step: 12
Training loss: 2.6205637548064673
Validation loss: 2.5618942539226963

Epoch: 6| Step: 13
Training loss: 2.6135746402011066
Validation loss: 2.5616263784476434

Epoch: 165| Step: 0
Training loss: 2.949005644722054
Validation loss: 2.5599189553375394

Epoch: 6| Step: 1
Training loss: 2.892349635957931
Validation loss: 2.5563322865405715

Epoch: 6| Step: 2
Training loss: 2.831737087362773
Validation loss: 2.563580437043557

Epoch: 6| Step: 3
Training loss: 2.8815361029848603
Validation loss: 2.5640222633454557

Epoch: 6| Step: 4
Training loss: 2.691948336418674
Validation loss: 2.571572092478092

Epoch: 6| Step: 5
Training loss: 3.171108811016833
Validation loss: 2.5645742157631517

Epoch: 6| Step: 6
Training loss: 2.4562061179708676
Validation loss: 2.561327799966786

Epoch: 6| Step: 7
Training loss: 2.9268834236996413
Validation loss: 2.571649677270283

Epoch: 6| Step: 8
Training loss: 2.798803421198984
Validation loss: 2.578034966103339

Epoch: 6| Step: 9
Training loss: 2.6387891538634465
Validation loss: 2.5756257065274673

Epoch: 6| Step: 10
Training loss: 3.392136478611432
Validation loss: 2.595819435707247

Epoch: 6| Step: 11
Training loss: 3.2803538233954335
Validation loss: 2.5857033418601096

Epoch: 6| Step: 12
Training loss: 3.012854059077344
Validation loss: 2.5596269858545377

Epoch: 6| Step: 13
Training loss: 2.674129405021115
Validation loss: 2.5584937677212265

Epoch: 166| Step: 0
Training loss: 2.5582817528046826
Validation loss: 2.5603576766970377

Epoch: 6| Step: 1
Training loss: 2.9625025817602313
Validation loss: 2.5591183330439575

Epoch: 6| Step: 2
Training loss: 3.5673535395647455
Validation loss: 2.5592040277785615

Epoch: 6| Step: 3
Training loss: 3.329185352742475
Validation loss: 2.571013066057796

Epoch: 6| Step: 4
Training loss: 3.037274224324356
Validation loss: 2.5652148154449805

Epoch: 6| Step: 5
Training loss: 3.0927097613925705
Validation loss: 2.5689393738214163

Epoch: 6| Step: 6
Training loss: 2.6956203119072235
Validation loss: 2.5728157441817303

Epoch: 6| Step: 7
Training loss: 2.8214431989956297
Validation loss: 2.5737184106791227

Epoch: 6| Step: 8
Training loss: 2.643574528924431
Validation loss: 2.573340501401774

Epoch: 6| Step: 9
Training loss: 2.997127429247618
Validation loss: 2.58271182844296

Epoch: 6| Step: 10
Training loss: 2.8426712840736394
Validation loss: 2.5825135195139524

Epoch: 6| Step: 11
Training loss: 2.471796015696459
Validation loss: 2.5759002229599726

Epoch: 6| Step: 12
Training loss: 3.0393759754578618
Validation loss: 2.58236791677032

Epoch: 6| Step: 13
Training loss: 2.210668325045477
Validation loss: 2.5660499386217825

Epoch: 167| Step: 0
Training loss: 2.6892474503781227
Validation loss: 2.5672356900348885

Epoch: 6| Step: 1
Training loss: 2.8166257479101273
Validation loss: 2.570149768343644

Epoch: 6| Step: 2
Training loss: 3.2176806701347007
Validation loss: 2.563492198453018

Epoch: 6| Step: 3
Training loss: 3.399317274548587
Validation loss: 2.5709904350394193

Epoch: 6| Step: 4
Training loss: 2.88277329829164
Validation loss: 2.5703541522617868

Epoch: 6| Step: 5
Training loss: 2.4593669421053073
Validation loss: 2.5763967857560077

Epoch: 6| Step: 6
Training loss: 3.0619919024559104
Validation loss: 2.5728379695422214

Epoch: 6| Step: 7
Training loss: 2.9579683177146294
Validation loss: 2.573879106939167

Epoch: 6| Step: 8
Training loss: 2.908024072098828
Validation loss: 2.5750602397329825

Epoch: 6| Step: 9
Training loss: 2.840169811813463
Validation loss: 2.5685076977006047

Epoch: 6| Step: 10
Training loss: 3.2497818213368577
Validation loss: 2.579975409530868

Epoch: 6| Step: 11
Training loss: 2.4343519177891175
Validation loss: 2.5921025377112676

Epoch: 6| Step: 12
Training loss: 2.5573680941367614
Validation loss: 2.586116296708585

Epoch: 6| Step: 13
Training loss: 3.206253533612362
Validation loss: 2.609496503544105

Epoch: 168| Step: 0
Training loss: 3.262455182114785
Validation loss: 2.613030286210312

Epoch: 6| Step: 1
Training loss: 2.6995422787691363
Validation loss: 2.615189421307205

Epoch: 6| Step: 2
Training loss: 2.7076061348440827
Validation loss: 2.622198436145245

Epoch: 6| Step: 3
Training loss: 2.910883993004108
Validation loss: 2.619927575604649

Epoch: 6| Step: 4
Training loss: 3.332013934002119
Validation loss: 2.6122748811232923

Epoch: 6| Step: 5
Training loss: 2.5579619814304517
Validation loss: 2.6024820789844174

Epoch: 6| Step: 6
Training loss: 3.419941952753742
Validation loss: 2.574766733340486

Epoch: 6| Step: 7
Training loss: 2.942050898658181
Validation loss: 2.570569725666723

Epoch: 6| Step: 8
Training loss: 2.328277787693092
Validation loss: 2.5634370296650886

Epoch: 6| Step: 9
Training loss: 2.912437169784841
Validation loss: 2.5556594350980144

Epoch: 6| Step: 10
Training loss: 3.0257144998426577
Validation loss: 2.556797467459931

Epoch: 6| Step: 11
Training loss: 2.9287097977969605
Validation loss: 2.559911463450494

Epoch: 6| Step: 12
Training loss: 2.6935355258182905
Validation loss: 2.5558387560937867

Epoch: 6| Step: 13
Training loss: 3.251153447801242
Validation loss: 2.549906108864102

Epoch: 169| Step: 0
Training loss: 3.1331490124266073
Validation loss: 2.553463000179959

Epoch: 6| Step: 1
Training loss: 2.8944282873788407
Validation loss: 2.55307556177578

Epoch: 6| Step: 2
Training loss: 2.5608904365837355
Validation loss: 2.5471533522500467

Epoch: 6| Step: 3
Training loss: 3.1656783887422466
Validation loss: 2.553350166748963

Epoch: 6| Step: 4
Training loss: 3.3632185838202235
Validation loss: 2.548809474672057

Epoch: 6| Step: 5
Training loss: 2.4464921634750487
Validation loss: 2.553693215797632

Epoch: 6| Step: 6
Training loss: 3.007676475718885
Validation loss: 2.5560214856790595

Epoch: 6| Step: 7
Training loss: 2.5764950310251833
Validation loss: 2.5642905953581687

Epoch: 6| Step: 8
Training loss: 2.8471644098465365
Validation loss: 2.564995954492329

Epoch: 6| Step: 9
Training loss: 2.7311771548964416
Validation loss: 2.5792749867389055

Epoch: 6| Step: 10
Training loss: 3.006707481423029
Validation loss: 2.602937770846008

Epoch: 6| Step: 11
Training loss: 3.30524961051249
Validation loss: 2.6084876779260138

Epoch: 6| Step: 12
Training loss: 2.848044370305612
Validation loss: 2.5841756460357073

Epoch: 6| Step: 13
Training loss: 2.5286927669005186
Validation loss: 2.5869117960742276

Epoch: 170| Step: 0
Training loss: 3.1143603973383165
Validation loss: 2.594556260869268

Epoch: 6| Step: 1
Training loss: 2.7900763199734975
Validation loss: 2.58310351571795

Epoch: 6| Step: 2
Training loss: 3.122990381661493
Validation loss: 2.5852993240044784

Epoch: 6| Step: 3
Training loss: 3.1865328649205473
Validation loss: 2.574495976118081

Epoch: 6| Step: 4
Training loss: 3.4413919784128737
Validation loss: 2.5748116589714027

Epoch: 6| Step: 5
Training loss: 2.4192306356440514
Validation loss: 2.5647305030853875

Epoch: 6| Step: 6
Training loss: 2.4773753664326437
Validation loss: 2.5616844494164033

Epoch: 6| Step: 7
Training loss: 3.1902851577590026
Validation loss: 2.5656306383894516

Epoch: 6| Step: 8
Training loss: 2.2775543795980737
Validation loss: 2.5514349225271022

Epoch: 6| Step: 9
Training loss: 2.579495984459754
Validation loss: 2.5472572946169367

Epoch: 6| Step: 10
Training loss: 3.2765135958345977
Validation loss: 2.553718117299319

Epoch: 6| Step: 11
Training loss: 3.0078971553630636
Validation loss: 2.548894150640636

Epoch: 6| Step: 12
Training loss: 2.9176734458274827
Validation loss: 2.547379930406627

Epoch: 6| Step: 13
Training loss: 2.4280380095974206
Validation loss: 2.551827787890451

Epoch: 171| Step: 0
Training loss: 2.8908871093686397
Validation loss: 2.546617752753961

Epoch: 6| Step: 1
Training loss: 2.499205367638884
Validation loss: 2.559209337962807

Epoch: 6| Step: 2
Training loss: 3.324789462200057
Validation loss: 2.5541598620042465

Epoch: 6| Step: 3
Training loss: 2.9565667980692254
Validation loss: 2.5524438124201927

Epoch: 6| Step: 4
Training loss: 2.7201513427816324
Validation loss: 2.5571599704658965

Epoch: 6| Step: 5
Training loss: 2.936504763565084
Validation loss: 2.5500656023781865

Epoch: 6| Step: 6
Training loss: 3.050218830700698
Validation loss: 2.5511403732315303

Epoch: 6| Step: 7
Training loss: 2.9143491711372875
Validation loss: 2.5504168733686177

Epoch: 6| Step: 8
Training loss: 2.237708319657579
Validation loss: 2.5416248835197703

Epoch: 6| Step: 9
Training loss: 3.2119437541383777
Validation loss: 2.544433626495331

Epoch: 6| Step: 10
Training loss: 3.0442313438358264
Validation loss: 2.5509382758426726

Epoch: 6| Step: 11
Training loss: 2.637623181110643
Validation loss: 2.5480490823858326

Epoch: 6| Step: 12
Training loss: 2.714679520943167
Validation loss: 2.5445513880701442

Epoch: 6| Step: 13
Training loss: 3.5941717812525122
Validation loss: 2.5462616031388454

Epoch: 172| Step: 0
Training loss: 3.133735958230719
Validation loss: 2.54557341954407

Epoch: 6| Step: 1
Training loss: 2.7388539280013133
Validation loss: 2.5445066596253336

Epoch: 6| Step: 2
Training loss: 2.9823659155891273
Validation loss: 2.545796280466008

Epoch: 6| Step: 3
Training loss: 3.2835375849775876
Validation loss: 2.547023504887452

Epoch: 6| Step: 4
Training loss: 2.8390526174605277
Validation loss: 2.546918118623527

Epoch: 6| Step: 5
Training loss: 2.5406751939758414
Validation loss: 2.5468138503657993

Epoch: 6| Step: 6
Training loss: 2.352959506229923
Validation loss: 2.5450085020552873

Epoch: 6| Step: 7
Training loss: 2.98309922036749
Validation loss: 2.554526079179707

Epoch: 6| Step: 8
Training loss: 3.2154200838005003
Validation loss: 2.562885604566322

Epoch: 6| Step: 9
Training loss: 2.789666802256991
Validation loss: 2.5820381862326363

Epoch: 6| Step: 10
Training loss: 2.774022773990231
Validation loss: 2.5841186469701114

Epoch: 6| Step: 11
Training loss: 3.478761264838986
Validation loss: 2.6081220332549275

Epoch: 6| Step: 12
Training loss: 2.5768870762873504
Validation loss: 2.6313855847587004

Epoch: 6| Step: 13
Training loss: 2.7713842394040187
Validation loss: 2.617109873218426

Epoch: 173| Step: 0
Training loss: 2.808691540369877
Validation loss: 2.6013518148381003

Epoch: 6| Step: 1
Training loss: 2.471389807768029
Validation loss: 2.6004041902649733

Epoch: 6| Step: 2
Training loss: 3.0554604101298866
Validation loss: 2.5954103380569786

Epoch: 6| Step: 3
Training loss: 2.7468758089413714
Validation loss: 2.5603330650616165

Epoch: 6| Step: 4
Training loss: 2.6668148198298582
Validation loss: 2.555729884455015

Epoch: 6| Step: 5
Training loss: 1.837239619380555
Validation loss: 2.551277326354204

Epoch: 6| Step: 6
Training loss: 3.2407310063856754
Validation loss: 2.5505029700263715

Epoch: 6| Step: 7
Training loss: 3.08985115754485
Validation loss: 2.5517920509713368

Epoch: 6| Step: 8
Training loss: 2.881053687832431
Validation loss: 2.554147408920406

Epoch: 6| Step: 9
Training loss: 3.508445631632628
Validation loss: 2.5471817225481024

Epoch: 6| Step: 10
Training loss: 3.1213381866108825
Validation loss: 2.545494589780611

Epoch: 6| Step: 11
Training loss: 3.0162716503291116
Validation loss: 2.548940538049005

Epoch: 6| Step: 12
Training loss: 2.7871128822737656
Validation loss: 2.548823391646694

Epoch: 6| Step: 13
Training loss: 3.239711027092154
Validation loss: 2.5467942154140752

Epoch: 174| Step: 0
Training loss: 2.6467633652818483
Validation loss: 2.5482834067913465

Epoch: 6| Step: 1
Training loss: 2.808803587649756
Validation loss: 2.5469362286907447

Epoch: 6| Step: 2
Training loss: 3.5227963695293876
Validation loss: 2.5507176210434537

Epoch: 6| Step: 3
Training loss: 3.2751354102176036
Validation loss: 2.5539324187918115

Epoch: 6| Step: 4
Training loss: 2.721867670165396
Validation loss: 2.5513754697780873

Epoch: 6| Step: 5
Training loss: 2.695490469101784
Validation loss: 2.5618709209218045

Epoch: 6| Step: 6
Training loss: 2.4497633333144058
Validation loss: 2.5645280861467774

Epoch: 6| Step: 7
Training loss: 2.5388977936361994
Validation loss: 2.5697422207205176

Epoch: 6| Step: 8
Training loss: 3.260284291140516
Validation loss: 2.560168913019735

Epoch: 6| Step: 9
Training loss: 3.192069574195385
Validation loss: 2.567956138500581

Epoch: 6| Step: 10
Training loss: 2.913756162685192
Validation loss: 2.561534012211379

Epoch: 6| Step: 11
Training loss: 2.726996706953253
Validation loss: 2.5608742671866858

Epoch: 6| Step: 12
Training loss: 2.515486340225792
Validation loss: 2.55463689978688

Epoch: 6| Step: 13
Training loss: 3.3027563201518113
Validation loss: 2.542543724580192

Epoch: 175| Step: 0
Training loss: 2.2287861745897106
Validation loss: 2.552689046763525

Epoch: 6| Step: 1
Training loss: 2.418330299573591
Validation loss: 2.5484903068561664

Epoch: 6| Step: 2
Training loss: 2.638710817943338
Validation loss: 2.5488544791366334

Epoch: 6| Step: 3
Training loss: 3.070599491596546
Validation loss: 2.5524610888174712

Epoch: 6| Step: 4
Training loss: 3.161745329665735
Validation loss: 2.554405915644024

Epoch: 6| Step: 5
Training loss: 3.0526190972116276
Validation loss: 2.554663381656805

Epoch: 6| Step: 6
Training loss: 2.821367652883838
Validation loss: 2.553239004884537

Epoch: 6| Step: 7
Training loss: 2.7042233214789086
Validation loss: 2.5719767984442177

Epoch: 6| Step: 8
Training loss: 3.344948384523952
Validation loss: 2.570618554226317

Epoch: 6| Step: 9
Training loss: 2.9625080543118854
Validation loss: 2.5569261116242123

Epoch: 6| Step: 10
Training loss: 2.9741414153513785
Validation loss: 2.549785817589799

Epoch: 6| Step: 11
Training loss: 3.105059518781136
Validation loss: 2.553548258236145

Epoch: 6| Step: 12
Training loss: 3.1189550297984856
Validation loss: 2.5482080593227314

Epoch: 6| Step: 13
Training loss: 2.651533629530558
Validation loss: 2.545575880889178

Epoch: 176| Step: 0
Training loss: 2.9028991742786783
Validation loss: 2.5471384231948524

Epoch: 6| Step: 1
Training loss: 3.3406640188795
Validation loss: 2.541640272630499

Epoch: 6| Step: 2
Training loss: 3.0544447546312306
Validation loss: 2.5467003176653744

Epoch: 6| Step: 3
Training loss: 2.8846085905337406
Validation loss: 2.545381281252428

Epoch: 6| Step: 4
Training loss: 3.1222830596402695
Validation loss: 2.549842283127633

Epoch: 6| Step: 5
Training loss: 3.0978151066612405
Validation loss: 2.5516643314984915

Epoch: 6| Step: 6
Training loss: 3.022100108459638
Validation loss: 2.547858609894143

Epoch: 6| Step: 7
Training loss: 3.0796823967011244
Validation loss: 2.5449963617930416

Epoch: 6| Step: 8
Training loss: 2.4141516807897774
Validation loss: 2.5431150792479946

Epoch: 6| Step: 9
Training loss: 2.6989534857728614
Validation loss: 2.539962359201098

Epoch: 6| Step: 10
Training loss: 2.2709527873780204
Validation loss: 2.5426941520765056

Epoch: 6| Step: 11
Training loss: 3.014713446061704
Validation loss: 2.541740300083422

Epoch: 6| Step: 12
Training loss: 2.164166940310342
Validation loss: 2.5452649027127277

Epoch: 6| Step: 13
Training loss: 3.460439125155044
Validation loss: 2.5486272907290233

Epoch: 177| Step: 0
Training loss: 3.2611630269008174
Validation loss: 2.542965394944354

Epoch: 6| Step: 1
Training loss: 2.8618235292656835
Validation loss: 2.5555230242907356

Epoch: 6| Step: 2
Training loss: 2.771071851593812
Validation loss: 2.5490158296708905

Epoch: 6| Step: 3
Training loss: 2.9211300803019578
Validation loss: 2.5664564034047253

Epoch: 6| Step: 4
Training loss: 2.7078741613588475
Validation loss: 2.543144098927177

Epoch: 6| Step: 5
Training loss: 2.573385239738126
Validation loss: 2.545953854425176

Epoch: 6| Step: 6
Training loss: 3.0774803454765447
Validation loss: 2.549075788014219

Epoch: 6| Step: 7
Training loss: 3.255407090286302
Validation loss: 2.5477566833983194

Epoch: 6| Step: 8
Training loss: 2.4580084914766145
Validation loss: 2.5521684710674952

Epoch: 6| Step: 9
Training loss: 3.0823125094454307
Validation loss: 2.5459469608565746

Epoch: 6| Step: 10
Training loss: 2.9798223311949794
Validation loss: 2.5518722754962475

Epoch: 6| Step: 11
Training loss: 2.8301579876801033
Validation loss: 2.5626142782985477

Epoch: 6| Step: 12
Training loss: 2.7296282739442774
Validation loss: 2.5793731601490393

Epoch: 6| Step: 13
Training loss: 2.8328894847207864
Validation loss: 2.5851389112188485

Epoch: 178| Step: 0
Training loss: 2.257165730379454
Validation loss: 2.584585300929971

Epoch: 6| Step: 1
Training loss: 3.380789700412181
Validation loss: 2.5915614389624593

Epoch: 6| Step: 2
Training loss: 2.9624956605774853
Validation loss: 2.5737555653533533

Epoch: 6| Step: 3
Training loss: 2.8194134830054773
Validation loss: 2.5778116900309995

Epoch: 6| Step: 4
Training loss: 3.1547736312497463
Validation loss: 2.5771979720244356

Epoch: 6| Step: 5
Training loss: 3.1590642185717823
Validation loss: 2.564125830336329

Epoch: 6| Step: 6
Training loss: 2.66464262879713
Validation loss: 2.5754360033746995

Epoch: 6| Step: 7
Training loss: 3.0799456019366107
Validation loss: 2.570974876597302

Epoch: 6| Step: 8
Training loss: 3.1903987494195536
Validation loss: 2.5520738157421756

Epoch: 6| Step: 9
Training loss: 2.400354470461041
Validation loss: 2.539563963587049

Epoch: 6| Step: 10
Training loss: 2.8140124175144097
Validation loss: 2.538560401108507

Epoch: 6| Step: 11
Training loss: 2.9191078505787598
Validation loss: 2.5390750957268096

Epoch: 6| Step: 12
Training loss: 2.6961612857660286
Validation loss: 2.539695870005093

Epoch: 6| Step: 13
Training loss: 2.6877015504001625
Validation loss: 2.53675787116962

Epoch: 179| Step: 0
Training loss: 2.9735399654532073
Validation loss: 2.538500775090561

Epoch: 6| Step: 1
Training loss: 1.9911349040243782
Validation loss: 2.5381683638109305

Epoch: 6| Step: 2
Training loss: 3.3519729327381333
Validation loss: 2.5427069284351917

Epoch: 6| Step: 3
Training loss: 2.956462608902219
Validation loss: 2.5571143768035682

Epoch: 6| Step: 4
Training loss: 2.893475750642411
Validation loss: 2.5389710788975384

Epoch: 6| Step: 5
Training loss: 2.965854236357269
Validation loss: 2.561040314987738

Epoch: 6| Step: 6
Training loss: 3.076288750259895
Validation loss: 2.5730869302804753

Epoch: 6| Step: 7
Training loss: 2.7953296846359437
Validation loss: 2.5885127497116924

Epoch: 6| Step: 8
Training loss: 3.2028007529292024
Validation loss: 2.5975272799919025

Epoch: 6| Step: 9
Training loss: 3.1174129700921296
Validation loss: 2.5732858535864014

Epoch: 6| Step: 10
Training loss: 2.8976519977767734
Validation loss: 2.5596716874498715

Epoch: 6| Step: 11
Training loss: 3.0420641356359117
Validation loss: 2.5410259844043765

Epoch: 6| Step: 12
Training loss: 2.242735260835597
Validation loss: 2.5413896052262963

Epoch: 6| Step: 13
Training loss: 2.6040647461337985
Validation loss: 2.5330090069319335

Epoch: 180| Step: 0
Training loss: 2.790660240232769
Validation loss: 2.5322655793031403

Epoch: 6| Step: 1
Training loss: 2.5359320959573024
Validation loss: 2.5348129830092487

Epoch: 6| Step: 2
Training loss: 3.1430897317008686
Validation loss: 2.535331173592911

Epoch: 6| Step: 3
Training loss: 2.4069737918759797
Validation loss: 2.5355372853333105

Epoch: 6| Step: 4
Training loss: 2.836194239573059
Validation loss: 2.5314694695206406

Epoch: 6| Step: 5
Training loss: 2.8927409907306125
Validation loss: 2.537745323589386

Epoch: 6| Step: 6
Training loss: 3.0087537209361406
Validation loss: 2.536395402639626

Epoch: 6| Step: 7
Training loss: 2.780982529712549
Validation loss: 2.5334849743384

Epoch: 6| Step: 8
Training loss: 3.0840173082966857
Validation loss: 2.5326558063888385

Epoch: 6| Step: 9
Training loss: 3.1572899946264767
Validation loss: 2.5398008261619496

Epoch: 6| Step: 10
Training loss: 2.975890556845672
Validation loss: 2.5466242871347085

Epoch: 6| Step: 11
Training loss: 3.040837492775749
Validation loss: 2.5637361729701706

Epoch: 6| Step: 12
Training loss: 2.849124231802578
Validation loss: 2.5769044474708487

Epoch: 6| Step: 13
Training loss: 3.1024161060188646
Validation loss: 2.5999861845433507

Epoch: 181| Step: 0
Training loss: 2.4463118683442935
Validation loss: 2.5611674607997688

Epoch: 6| Step: 1
Training loss: 3.0264113042323335
Validation loss: 2.547549613057249

Epoch: 6| Step: 2
Training loss: 2.352162433529729
Validation loss: 2.542112269711795

Epoch: 6| Step: 3
Training loss: 2.6776478292222947
Validation loss: 2.5418230859591486

Epoch: 6| Step: 4
Training loss: 3.1400668279254647
Validation loss: 2.534886448057146

Epoch: 6| Step: 5
Training loss: 3.351839494262858
Validation loss: 2.53467378194046

Epoch: 6| Step: 6
Training loss: 2.697290816800578
Validation loss: 2.530915042049921

Epoch: 6| Step: 7
Training loss: 2.7482307985270555
Validation loss: 2.5369976038374458

Epoch: 6| Step: 8
Training loss: 2.5661167579543416
Validation loss: 2.541844513223712

Epoch: 6| Step: 9
Training loss: 3.1729905745843885
Validation loss: 2.545453257890681

Epoch: 6| Step: 10
Training loss: 3.377232978481405
Validation loss: 2.538640239932824

Epoch: 6| Step: 11
Training loss: 2.952359216290694
Validation loss: 2.5439702993411375

Epoch: 6| Step: 12
Training loss: 2.7982206890009635
Validation loss: 2.542446147736262

Epoch: 6| Step: 13
Training loss: 3.0004094162362347
Validation loss: 2.5668224532216386

Epoch: 182| Step: 0
Training loss: 3.15645666673326
Validation loss: 2.5651351881027944

Epoch: 6| Step: 1
Training loss: 2.886557691982453
Validation loss: 2.5635351755811953

Epoch: 6| Step: 2
Training loss: 3.01669909071665
Validation loss: 2.563296662674304

Epoch: 6| Step: 3
Training loss: 2.860614278755325
Validation loss: 2.5580539314575943

Epoch: 6| Step: 4
Training loss: 3.0478476512410824
Validation loss: 2.5556430390007767

Epoch: 6| Step: 5
Training loss: 3.0736362627665392
Validation loss: 2.5673812278126973

Epoch: 6| Step: 6
Training loss: 3.1727239665883418
Validation loss: 2.585369723275928

Epoch: 6| Step: 7
Training loss: 2.3667606182881666
Validation loss: 2.5984213580080278

Epoch: 6| Step: 8
Training loss: 2.365367630497379
Validation loss: 2.601633459373279

Epoch: 6| Step: 9
Training loss: 3.387196015516991
Validation loss: 2.6068616904932336

Epoch: 6| Step: 10
Training loss: 3.1492191981866773
Validation loss: 2.6010203877110203

Epoch: 6| Step: 11
Training loss: 2.2870139892910877
Validation loss: 2.5447243037946894

Epoch: 6| Step: 12
Training loss: 2.9788652458740343
Validation loss: 2.5370530504756035

Epoch: 6| Step: 13
Training loss: 2.448078772405684
Validation loss: 2.5336192609110704

Epoch: 183| Step: 0
Training loss: 3.0558007594663588
Validation loss: 2.5677013657126384

Epoch: 6| Step: 1
Training loss: 2.7474741172786263
Validation loss: 2.6226568697726127

Epoch: 6| Step: 2
Training loss: 3.853712120402041
Validation loss: 2.7168141074465026

Epoch: 6| Step: 3
Training loss: 2.6150834009353616
Validation loss: 2.6371942106195734

Epoch: 6| Step: 4
Training loss: 2.471305586749762
Validation loss: 2.573429473221492

Epoch: 6| Step: 5
Training loss: 2.653546561997632
Validation loss: 2.5482346894924266

Epoch: 6| Step: 6
Training loss: 3.298239475307184
Validation loss: 2.554715247754762

Epoch: 6| Step: 7
Training loss: 2.567068911487513
Validation loss: 2.572890645022157

Epoch: 6| Step: 8
Training loss: 2.837885920290319
Validation loss: 2.59087230574125

Epoch: 6| Step: 9
Training loss: 2.697854874736664
Validation loss: 2.585120138574475

Epoch: 6| Step: 10
Training loss: 2.9325845051562984
Validation loss: 2.568257747046107

Epoch: 6| Step: 11
Training loss: 2.934045261629418
Validation loss: 2.563374110895944

Epoch: 6| Step: 12
Training loss: 3.0279948266613808
Validation loss: 2.5599500472857004

Epoch: 6| Step: 13
Training loss: 3.487115851133787
Validation loss: 2.5666713103545487

Epoch: 184| Step: 0
Training loss: 3.210702409514681
Validation loss: 2.556349900693954

Epoch: 6| Step: 1
Training loss: 2.796706892534329
Validation loss: 2.541674035147549

Epoch: 6| Step: 2
Training loss: 3.117174650466937
Validation loss: 2.5409250197766324

Epoch: 6| Step: 3
Training loss: 2.819129590106313
Validation loss: 2.5391732736155714

Epoch: 6| Step: 4
Training loss: 2.505295961450796
Validation loss: 2.5361969498908805

Epoch: 6| Step: 5
Training loss: 2.3707421936144772
Validation loss: 2.5365447029899193

Epoch: 6| Step: 6
Training loss: 2.5984612386402497
Validation loss: 2.538018392311092

Epoch: 6| Step: 7
Training loss: 3.1880304231807415
Validation loss: 2.544810502378917

Epoch: 6| Step: 8
Training loss: 2.8200947262701512
Validation loss: 2.5474342966654304

Epoch: 6| Step: 9
Training loss: 3.214529585291122
Validation loss: 2.551531576884759

Epoch: 6| Step: 10
Training loss: 3.2900702230733474
Validation loss: 2.556754593657681

Epoch: 6| Step: 11
Training loss: 3.050117527932998
Validation loss: 2.576684505973609

Epoch: 6| Step: 12
Training loss: 2.96505410876765
Validation loss: 2.5790312923256766

Epoch: 6| Step: 13
Training loss: 2.069811490454642
Validation loss: 2.6000426119332865

Epoch: 185| Step: 0
Training loss: 2.942248139083187
Validation loss: 2.608684230007819

Epoch: 6| Step: 1
Training loss: 2.6866963094144074
Validation loss: 2.6237426047230756

Epoch: 6| Step: 2
Training loss: 2.329939167391002
Validation loss: 2.6574607343412917

Epoch: 6| Step: 3
Training loss: 3.5848782040644105
Validation loss: 2.6533217134910236

Epoch: 6| Step: 4
Training loss: 2.6869401570432747
Validation loss: 2.6381295792598696

Epoch: 6| Step: 5
Training loss: 3.475878015921644
Validation loss: 2.6461088214597415

Epoch: 6| Step: 6
Training loss: 2.940730976136925
Validation loss: 2.618408872206197

Epoch: 6| Step: 7
Training loss: 3.1382725953639685
Validation loss: 2.6083047976193097

Epoch: 6| Step: 8
Training loss: 2.7952217884815074
Validation loss: 2.5639618956211367

Epoch: 6| Step: 9
Training loss: 2.9470132222935557
Validation loss: 2.5420959617122683

Epoch: 6| Step: 10
Training loss: 2.590054924872247
Validation loss: 2.5313993334222653

Epoch: 6| Step: 11
Training loss: 2.4855508960461736
Validation loss: 2.52977780870771

Epoch: 6| Step: 12
Training loss: 2.1214557711099316
Validation loss: 2.5244435149868525

Epoch: 6| Step: 13
Training loss: 3.5868115658426083
Validation loss: 2.5300844821786326

Epoch: 186| Step: 0
Training loss: 2.793802971732409
Validation loss: 2.528826372750637

Epoch: 6| Step: 1
Training loss: 3.5444440535409276
Validation loss: 2.5320571125112483

Epoch: 6| Step: 2
Training loss: 2.59036640859905
Validation loss: 2.5274838233792716

Epoch: 6| Step: 3
Training loss: 2.8326592110906597
Validation loss: 2.530953632375832

Epoch: 6| Step: 4
Training loss: 3.1885278577899148
Validation loss: 2.53193771072534

Epoch: 6| Step: 5
Training loss: 2.925699108075896
Validation loss: 2.527299523444075

Epoch: 6| Step: 6
Training loss: 2.9067098602387005
Validation loss: 2.529264022684111

Epoch: 6| Step: 7
Training loss: 2.071742531984959
Validation loss: 2.5324335343149156

Epoch: 6| Step: 8
Training loss: 2.413948624279904
Validation loss: 2.5274117468200674

Epoch: 6| Step: 9
Training loss: 2.755340419167927
Validation loss: 2.5258137156818

Epoch: 6| Step: 10
Training loss: 2.5797149379479465
Validation loss: 2.5432623496248787

Epoch: 6| Step: 11
Training loss: 3.5316199724951938
Validation loss: 2.552773943025257

Epoch: 6| Step: 12
Training loss: 3.2160751932188245
Validation loss: 2.546508434537065

Epoch: 6| Step: 13
Training loss: 2.8285134401748917
Validation loss: 2.5517146631336893

Epoch: 187| Step: 0
Training loss: 2.429618221166148
Validation loss: 2.536556143886866

Epoch: 6| Step: 1
Training loss: 2.70972039332204
Validation loss: 2.5514470693302975

Epoch: 6| Step: 2
Training loss: 3.0992887603952783
Validation loss: 2.542611161651613

Epoch: 6| Step: 3
Training loss: 2.6502229416833973
Validation loss: 2.5570918994845306

Epoch: 6| Step: 4
Training loss: 2.252424946676665
Validation loss: 2.556309583719533

Epoch: 6| Step: 5
Training loss: 3.061172353274581
Validation loss: 2.551883293048508

Epoch: 6| Step: 6
Training loss: 2.8852763411533626
Validation loss: 2.540050249465842

Epoch: 6| Step: 7
Training loss: 3.064144704942498
Validation loss: 2.5351595107275835

Epoch: 6| Step: 8
Training loss: 2.9655461742026756
Validation loss: 2.5362917337971727

Epoch: 6| Step: 9
Training loss: 2.6717914769017663
Validation loss: 2.5362350686676924

Epoch: 6| Step: 10
Training loss: 2.993313171754947
Validation loss: 2.536228742027489

Epoch: 6| Step: 11
Training loss: 3.204031318282891
Validation loss: 2.5297494357759933

Epoch: 6| Step: 12
Training loss: 2.8203314360513887
Validation loss: 2.5320381154003813

Epoch: 6| Step: 13
Training loss: 3.6880460431830544
Validation loss: 2.5272563044247836

Epoch: 188| Step: 0
Training loss: 3.2747320924626466
Validation loss: 2.526808094042061

Epoch: 6| Step: 1
Training loss: 2.9763173880992846
Validation loss: 2.5237311004439884

Epoch: 6| Step: 2
Training loss: 2.6368199307843514
Validation loss: 2.521973959939101

Epoch: 6| Step: 3
Training loss: 2.54892110253069
Validation loss: 2.527834635119189

Epoch: 6| Step: 4
Training loss: 2.333009651984938
Validation loss: 2.521050495613397

Epoch: 6| Step: 5
Training loss: 3.3505487931943585
Validation loss: 2.519314572746467

Epoch: 6| Step: 6
Training loss: 3.031865322936654
Validation loss: 2.5236035993979042

Epoch: 6| Step: 7
Training loss: 3.1411933953937994
Validation loss: 2.522889486452516

Epoch: 6| Step: 8
Training loss: 2.4283368935053833
Validation loss: 2.5225162910450756

Epoch: 6| Step: 9
Training loss: 3.471798493703773
Validation loss: 2.525004767900041

Epoch: 6| Step: 10
Training loss: 2.452079501563627
Validation loss: 2.52932447965661

Epoch: 6| Step: 11
Training loss: 2.5902231896460535
Validation loss: 2.520812011549931

Epoch: 6| Step: 12
Training loss: 2.58615895040507
Validation loss: 2.530293462052397

Epoch: 6| Step: 13
Training loss: 3.3794197530236554
Validation loss: 2.5327454057442575

Epoch: 189| Step: 0
Training loss: 2.786677946664105
Validation loss: 2.5400267309426705

Epoch: 6| Step: 1
Training loss: 2.4826028129132074
Validation loss: 2.522238791550185

Epoch: 6| Step: 2
Training loss: 3.3025859526451957
Validation loss: 2.5450396955750643

Epoch: 6| Step: 3
Training loss: 2.5764681954415005
Validation loss: 2.5364746021130475

Epoch: 6| Step: 4
Training loss: 2.9321400866902785
Validation loss: 2.557532911101632

Epoch: 6| Step: 5
Training loss: 3.091143193788802
Validation loss: 2.5728665598121583

Epoch: 6| Step: 6
Training loss: 3.0045846240066836
Validation loss: 2.5943177549807297

Epoch: 6| Step: 7
Training loss: 3.273034264394193
Validation loss: 2.606974047771779

Epoch: 6| Step: 8
Training loss: 2.587273935218889
Validation loss: 2.5734417503140476

Epoch: 6| Step: 9
Training loss: 3.0108446567159524
Validation loss: 2.5541302342646888

Epoch: 6| Step: 10
Training loss: 2.8292749922571194
Validation loss: 2.5450154394659004

Epoch: 6| Step: 11
Training loss: 2.738762871674133
Validation loss: 2.546553245607158

Epoch: 6| Step: 12
Training loss: 2.025279734563812
Validation loss: 2.5454290742089833

Epoch: 6| Step: 13
Training loss: 3.7069560004134745
Validation loss: 2.5224110191771825

Epoch: 190| Step: 0
Training loss: 2.9999183007877677
Validation loss: 2.5266394657665807

Epoch: 6| Step: 1
Training loss: 3.1644949911785822
Validation loss: 2.5299582345827374

Epoch: 6| Step: 2
Training loss: 2.6063840250732864
Validation loss: 2.5206244980594295

Epoch: 6| Step: 3
Training loss: 2.9517198883146527
Validation loss: 2.5237817666913003

Epoch: 6| Step: 4
Training loss: 3.383129924303351
Validation loss: 2.5218860597971924

Epoch: 6| Step: 5
Training loss: 3.2702110978329286
Validation loss: 2.535714429113094

Epoch: 6| Step: 6
Training loss: 3.202374924086645
Validation loss: 2.5313457580789263

Epoch: 6| Step: 7
Training loss: 2.71680231594306
Validation loss: 2.5363303929209695

Epoch: 6| Step: 8
Training loss: 2.8981864388612775
Validation loss: 2.550467547253654

Epoch: 6| Step: 9
Training loss: 2.092306522006575
Validation loss: 2.5505653740633765

Epoch: 6| Step: 10
Training loss: 2.3171998798556688
Validation loss: 2.555147844353277

Epoch: 6| Step: 11
Training loss: 2.8717063613542186
Validation loss: 2.5812777078878435

Epoch: 6| Step: 12
Training loss: 2.7663794743799617
Validation loss: 2.570678683855599

Epoch: 6| Step: 13
Training loss: 2.4815032483828463
Validation loss: 2.580450275840538

Epoch: 191| Step: 0
Training loss: 2.5680169983298273
Validation loss: 2.589155865617113

Epoch: 6| Step: 1
Training loss: 3.342484279187594
Validation loss: 2.6199856107364092

Epoch: 6| Step: 2
Training loss: 3.2206775116003197
Validation loss: 2.626415447276967

Epoch: 6| Step: 3
Training loss: 2.6516392801797877
Validation loss: 2.663044498606935

Epoch: 6| Step: 4
Training loss: 2.2607442967052798
Validation loss: 2.667111294989935

Epoch: 6| Step: 5
Training loss: 2.485865691542719
Validation loss: 2.6627775379702254

Epoch: 6| Step: 6
Training loss: 3.366676663550715
Validation loss: 2.6274762435255465

Epoch: 6| Step: 7
Training loss: 2.706342072715639
Validation loss: 2.608596691578112

Epoch: 6| Step: 8
Training loss: 2.843340833802372
Validation loss: 2.595421110537283

Epoch: 6| Step: 9
Training loss: 3.221417553133363
Validation loss: 2.5574413783962457

Epoch: 6| Step: 10
Training loss: 2.8231592578511098
Validation loss: 2.5410054199163143

Epoch: 6| Step: 11
Training loss: 2.9675608612478297
Validation loss: 2.521229901837942

Epoch: 6| Step: 12
Training loss: 2.863125202251395
Validation loss: 2.5306804162490697

Epoch: 6| Step: 13
Training loss: 2.7565665307667984
Validation loss: 2.529368106258543

Epoch: 192| Step: 0
Training loss: 3.228930917973761
Validation loss: 2.5348903447602047

Epoch: 6| Step: 1
Training loss: 2.7053169668362793
Validation loss: 2.5333524252599045

Epoch: 6| Step: 2
Training loss: 2.736491403330273
Validation loss: 2.537553604871549

Epoch: 6| Step: 3
Training loss: 2.8333853174564405
Validation loss: 2.5357170132613915

Epoch: 6| Step: 4
Training loss: 3.2841695240327424
Validation loss: 2.532760561344304

Epoch: 6| Step: 5
Training loss: 3.3433539610581677
Validation loss: 2.5356632228019813

Epoch: 6| Step: 6
Training loss: 2.5303591820791036
Validation loss: 2.5297510653179374

Epoch: 6| Step: 7
Training loss: 2.769492769175765
Validation loss: 2.5268447210103036

Epoch: 6| Step: 8
Training loss: 3.0032763074851374
Validation loss: 2.518723509969395

Epoch: 6| Step: 9
Training loss: 2.5311669230073437
Validation loss: 2.519836257746011

Epoch: 6| Step: 10
Training loss: 2.3872661056590476
Validation loss: 2.524678205577012

Epoch: 6| Step: 11
Training loss: 3.254229800840698
Validation loss: 2.5397949969482934

Epoch: 6| Step: 12
Training loss: 2.7323458744847295
Validation loss: 2.5766233683146087

Epoch: 6| Step: 13
Training loss: 3.361201725588647
Validation loss: 2.6018910016929877

Epoch: 193| Step: 0
Training loss: 3.144118610105481
Validation loss: 2.581445802933853

Epoch: 6| Step: 1
Training loss: 2.2751980286650024
Validation loss: 2.5982957016983597

Epoch: 6| Step: 2
Training loss: 2.9616656452349646
Validation loss: 2.591665383784305

Epoch: 6| Step: 3
Training loss: 2.4721337801463976
Validation loss: 2.5994791645890443

Epoch: 6| Step: 4
Training loss: 2.691014675229365
Validation loss: 2.5861369723599705

Epoch: 6| Step: 5
Training loss: 3.0553246343417944
Validation loss: 2.5789601593810203

Epoch: 6| Step: 6
Training loss: 2.8759878368836738
Validation loss: 2.5587376963372828

Epoch: 6| Step: 7
Training loss: 3.105428213178825
Validation loss: 2.5494375648553764

Epoch: 6| Step: 8
Training loss: 2.609232733040439
Validation loss: 2.553502432422424

Epoch: 6| Step: 9
Training loss: 3.1626485797221227
Validation loss: 2.567345347414477

Epoch: 6| Step: 10
Training loss: 3.0324720298691092
Validation loss: 2.5659881989011852

Epoch: 6| Step: 11
Training loss: 2.8391357546788942
Validation loss: 2.5677062160252997

Epoch: 6| Step: 12
Training loss: 3.095602386472112
Validation loss: 2.54427877314428

Epoch: 6| Step: 13
Training loss: 2.914509020592615
Validation loss: 2.5472866681648476

Epoch: 194| Step: 0
Training loss: 3.655402452096348
Validation loss: 2.5392637202418102

Epoch: 6| Step: 1
Training loss: 2.6212733018209597
Validation loss: 2.525408243815568

Epoch: 6| Step: 2
Training loss: 2.670307614325914
Validation loss: 2.5208733744548497

Epoch: 6| Step: 3
Training loss: 2.7044538638326077
Validation loss: 2.5149550756389316

Epoch: 6| Step: 4
Training loss: 2.1104074847620073
Validation loss: 2.507865681578081

Epoch: 6| Step: 5
Training loss: 3.052434456236389
Validation loss: 2.511738645877667

Epoch: 6| Step: 6
Training loss: 3.3252714576909335
Validation loss: 2.5113002988815905

Epoch: 6| Step: 7
Training loss: 2.9418155542433295
Validation loss: 2.5094283755056312

Epoch: 6| Step: 8
Training loss: 2.883656448017494
Validation loss: 2.5141574454273474

Epoch: 6| Step: 9
Training loss: 3.2530840399341088
Validation loss: 2.5151578020581464

Epoch: 6| Step: 10
Training loss: 2.9787692002715365
Validation loss: 2.5193907772103437

Epoch: 6| Step: 11
Training loss: 2.488492037529879
Validation loss: 2.5184774092670934

Epoch: 6| Step: 12
Training loss: 2.567957857606149
Validation loss: 2.5252240411243294

Epoch: 6| Step: 13
Training loss: 2.534684476760079
Validation loss: 2.529552565603105

Epoch: 195| Step: 0
Training loss: 3.3978867358572087
Validation loss: 2.522623714007444

Epoch: 6| Step: 1
Training loss: 2.6422995048979563
Validation loss: 2.5186262602281415

Epoch: 6| Step: 2
Training loss: 2.8683740307146355
Validation loss: 2.5268328343739457

Epoch: 6| Step: 3
Training loss: 2.807354777139249
Validation loss: 2.5157183772051797

Epoch: 6| Step: 4
Training loss: 3.051259646839987
Validation loss: 2.51070613227899

Epoch: 6| Step: 5
Training loss: 2.97208963575773
Validation loss: 2.5126724929219417

Epoch: 6| Step: 6
Training loss: 2.4318788710469987
Validation loss: 2.510293671536833

Epoch: 6| Step: 7
Training loss: 2.428596570582035
Validation loss: 2.512235815070136

Epoch: 6| Step: 8
Training loss: 2.869539133212184
Validation loss: 2.5141054880231026

Epoch: 6| Step: 9
Training loss: 2.462120425795494
Validation loss: 2.511323777115312

Epoch: 6| Step: 10
Training loss: 3.0427723969364036
Validation loss: 2.5197564508606702

Epoch: 6| Step: 11
Training loss: 3.239913547445653
Validation loss: 2.517923497251818

Epoch: 6| Step: 12
Training loss: 3.1035222316933297
Validation loss: 2.517877823346933

Epoch: 6| Step: 13
Training loss: 2.598423894593111
Validation loss: 2.5221553638666068

Epoch: 196| Step: 0
Training loss: 2.4535738783372185
Validation loss: 2.5274878440829065

Epoch: 6| Step: 1
Training loss: 3.0393905658703777
Validation loss: 2.5255256845568677

Epoch: 6| Step: 2
Training loss: 2.470568699251933
Validation loss: 2.5317761598373676

Epoch: 6| Step: 3
Training loss: 2.348935811568485
Validation loss: 2.527603737811654

Epoch: 6| Step: 4
Training loss: 3.2283456230290706
Validation loss: 2.530647926326703

Epoch: 6| Step: 5
Training loss: 3.14766136428444
Validation loss: 2.535098147772854

Epoch: 6| Step: 6
Training loss: 2.7268512212261666
Validation loss: 2.5500650605088135

Epoch: 6| Step: 7
Training loss: 3.3823286948347207
Validation loss: 2.538785234809649

Epoch: 6| Step: 8
Training loss: 2.2120806970207565
Validation loss: 2.537562316504364

Epoch: 6| Step: 9
Training loss: 2.577829517424405
Validation loss: 2.52865823796524

Epoch: 6| Step: 10
Training loss: 3.3159898332502094
Validation loss: 2.5226250981526768

Epoch: 6| Step: 11
Training loss: 3.0884225617591436
Validation loss: 2.532325371369686

Epoch: 6| Step: 12
Training loss: 2.884112966750864
Validation loss: 2.5320226173053624

Epoch: 6| Step: 13
Training loss: 3.083188457350215
Validation loss: 2.5351989864851117

Epoch: 197| Step: 0
Training loss: 2.8687886021495625
Validation loss: 2.5227765822625803

Epoch: 6| Step: 1
Training loss: 3.3354995046920304
Validation loss: 2.521168496259836

Epoch: 6| Step: 2
Training loss: 2.8797427324782383
Validation loss: 2.5218145938190566

Epoch: 6| Step: 3
Training loss: 3.4588851182116946
Validation loss: 2.5258024189721877

Epoch: 6| Step: 4
Training loss: 2.679141508926603
Validation loss: 2.5253026894769732

Epoch: 6| Step: 5
Training loss: 2.8716178571376174
Validation loss: 2.5320601782790413

Epoch: 6| Step: 6
Training loss: 2.925601643039408
Validation loss: 2.5347241920339547

Epoch: 6| Step: 7
Training loss: 2.579570203363737
Validation loss: 2.5375911456027236

Epoch: 6| Step: 8
Training loss: 2.9531469596571096
Validation loss: 2.5246315175837606

Epoch: 6| Step: 9
Training loss: 2.7640529841576766
Validation loss: 2.531453167899516

Epoch: 6| Step: 10
Training loss: 2.5769783938193807
Validation loss: 2.5219264167825886

Epoch: 6| Step: 11
Training loss: 1.9928439386480197
Validation loss: 2.515046002295449

Epoch: 6| Step: 12
Training loss: 3.1074656082423004
Validation loss: 2.5215462558153514

Epoch: 6| Step: 13
Training loss: 2.8525434087180175
Validation loss: 2.520532082727618

Epoch: 198| Step: 0
Training loss: 3.0941221995865194
Validation loss: 2.5140337427433663

Epoch: 6| Step: 1
Training loss: 3.112587347681707
Validation loss: 2.5201903536525605

Epoch: 6| Step: 2
Training loss: 3.5800835858608786
Validation loss: 2.5185120126352816

Epoch: 6| Step: 3
Training loss: 3.128751715226414
Validation loss: 2.5179402927084418

Epoch: 6| Step: 4
Training loss: 2.3903911201912154
Validation loss: 2.519555819621271

Epoch: 6| Step: 5
Training loss: 2.8862030022827123
Validation loss: 2.5130772979546023

Epoch: 6| Step: 6
Training loss: 2.6734002650848896
Validation loss: 2.5149238010184303

Epoch: 6| Step: 7
Training loss: 2.718635819766969
Validation loss: 2.5121171397051256

Epoch: 6| Step: 8
Training loss: 2.689074210496636
Validation loss: 2.512276115844026

Epoch: 6| Step: 9
Training loss: 2.4952061467446325
Validation loss: 2.5130514928643573

Epoch: 6| Step: 10
Training loss: 2.7330605099369025
Validation loss: 2.5195260917430797

Epoch: 6| Step: 11
Training loss: 2.6991399596179604
Validation loss: 2.5233527139795138

Epoch: 6| Step: 12
Training loss: 2.914202893840737
Validation loss: 2.537764380007272

Epoch: 6| Step: 13
Training loss: 2.8965816674383853
Validation loss: 2.535222834954875

Epoch: 199| Step: 0
Training loss: 2.2632337908234423
Validation loss: 2.5329728911485465

Epoch: 6| Step: 1
Training loss: 3.5719180180453076
Validation loss: 2.519426812953984

Epoch: 6| Step: 2
Training loss: 2.9302697989025432
Validation loss: 2.5375853244465807

Epoch: 6| Step: 3
Training loss: 2.790677668810264
Validation loss: 2.534161448839352

Epoch: 6| Step: 4
Training loss: 2.9225340727165126
Validation loss: 2.533761854783223

Epoch: 6| Step: 5
Training loss: 2.594498491870135
Validation loss: 2.531964124228381

Epoch: 6| Step: 6
Training loss: 2.9315918289993013
Validation loss: 2.5124124701822876

Epoch: 6| Step: 7
Training loss: 2.812710478642233
Validation loss: 2.5179608449982713

Epoch: 6| Step: 8
Training loss: 2.697266420534142
Validation loss: 2.532436511555019

Epoch: 6| Step: 9
Training loss: 2.7687098653050133
Validation loss: 2.5133552826460757

Epoch: 6| Step: 10
Training loss: 2.9226190772161553
Validation loss: 2.509689146663961

Epoch: 6| Step: 11
Training loss: 2.8730789068680886
Validation loss: 2.5119894961753504

Epoch: 6| Step: 12
Training loss: 3.0134319968936323
Validation loss: 2.509098471943008

Epoch: 6| Step: 13
Training loss: 2.809247573491646
Validation loss: 2.5076579660731646

Epoch: 200| Step: 0
Training loss: 2.29553889644093
Validation loss: 2.5273592980879043

Epoch: 6| Step: 1
Training loss: 2.8670517891928746
Validation loss: 2.524920284280409

Epoch: 6| Step: 2
Training loss: 2.6715701637599896
Validation loss: 2.5292581560094285

Epoch: 6| Step: 3
Training loss: 3.0223891537820564
Validation loss: 2.5569402897193685

Epoch: 6| Step: 4
Training loss: 3.0365046938742593
Validation loss: 2.5395022633812867

Epoch: 6| Step: 5
Training loss: 2.9128353204192363
Validation loss: 2.5314072925063242

Epoch: 6| Step: 6
Training loss: 2.763131865997861
Validation loss: 2.5342941209974534

Epoch: 6| Step: 7
Training loss: 2.700526316878448
Validation loss: 2.532591725084593

Epoch: 6| Step: 8
Training loss: 2.216880010314734
Validation loss: 2.51785514644783

Epoch: 6| Step: 9
Training loss: 2.684058358787391
Validation loss: 2.5165036317427196

Epoch: 6| Step: 10
Training loss: 3.0834354349363142
Validation loss: 2.5180239374234494

Epoch: 6| Step: 11
Training loss: 3.0929481160708234
Validation loss: 2.516272792976705

Epoch: 6| Step: 12
Training loss: 3.351115590423347
Validation loss: 2.5154034162777816

Epoch: 6| Step: 13
Training loss: 3.446288232324921
Validation loss: 2.5192693748640265

Testing loss: 2.7302376701043114

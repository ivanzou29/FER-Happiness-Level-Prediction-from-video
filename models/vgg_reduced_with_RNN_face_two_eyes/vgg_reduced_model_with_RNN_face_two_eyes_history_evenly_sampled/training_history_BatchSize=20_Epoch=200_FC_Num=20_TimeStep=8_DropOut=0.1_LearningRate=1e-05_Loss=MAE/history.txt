Epoch: 1| Step: 0
Training loss: 4.5388503074646
Validation loss: 5.235741712713755

Epoch: 5| Step: 1
Training loss: 6.24954891204834
Validation loss: 5.227723398516255

Epoch: 5| Step: 2
Training loss: 5.252708435058594
Validation loss: 5.220223811364943

Epoch: 5| Step: 3
Training loss: 4.295501708984375
Validation loss: 5.213153167437482

Epoch: 5| Step: 4
Training loss: 4.451444625854492
Validation loss: 5.206807654391053

Epoch: 5| Step: 5
Training loss: 6.169407844543457
Validation loss: 5.200602721142513

Epoch: 5| Step: 6
Training loss: 4.1816229820251465
Validation loss: 5.194182585644466

Epoch: 5| Step: 7
Training loss: 3.755688190460205
Validation loss: 5.1873136899804555

Epoch: 5| Step: 8
Training loss: 5.031002998352051
Validation loss: 5.179890125028549

Epoch: 5| Step: 9
Training loss: 5.614790439605713
Validation loss: 5.172138675566642

Epoch: 5| Step: 10
Training loss: 5.486572742462158
Validation loss: 5.163241727377779

Epoch: 2| Step: 0
Training loss: 3.778510570526123
Validation loss: 5.153710021767565

Epoch: 5| Step: 1
Training loss: 4.514275550842285
Validation loss: 5.143120437540034

Epoch: 5| Step: 2
Training loss: 4.619799613952637
Validation loss: 5.131663050702823

Epoch: 5| Step: 3
Training loss: 4.805675506591797
Validation loss: 5.118743670883999

Epoch: 5| Step: 4
Training loss: 5.149754524230957
Validation loss: 5.104635254029305

Epoch: 5| Step: 5
Training loss: 4.583379745483398
Validation loss: 5.089057214798466

Epoch: 5| Step: 6
Training loss: 4.401719093322754
Validation loss: 5.072135233109997

Epoch: 5| Step: 7
Training loss: 5.5910210609436035
Validation loss: 5.053819338480632

Epoch: 5| Step: 8
Training loss: 4.4452924728393555
Validation loss: 5.0338216033033145

Epoch: 5| Step: 9
Training loss: 6.106683731079102
Validation loss: 5.011421870159847

Epoch: 5| Step: 10
Training loss: 5.78309440612793
Validation loss: 4.987111035213675

Epoch: 3| Step: 0
Training loss: 4.689499855041504
Validation loss: 4.961016660095543

Epoch: 5| Step: 1
Training loss: 4.084657669067383
Validation loss: 4.934208644333706

Epoch: 5| Step: 2
Training loss: 5.417086124420166
Validation loss: 4.9043391750704854

Epoch: 5| Step: 3
Training loss: 4.111201286315918
Validation loss: 4.872320395643993

Epoch: 5| Step: 4
Training loss: 4.614030361175537
Validation loss: 4.839076452357794

Epoch: 5| Step: 5
Training loss: 4.490595817565918
Validation loss: 4.805158666385117

Epoch: 5| Step: 6
Training loss: 4.883264064788818
Validation loss: 4.769118437203028

Epoch: 5| Step: 7
Training loss: 4.151961326599121
Validation loss: 4.7320783830458115

Epoch: 5| Step: 8
Training loss: 4.662329196929932
Validation loss: 4.6956009095714935

Epoch: 5| Step: 9
Training loss: 5.293032646179199
Validation loss: 4.658536808465117

Epoch: 5| Step: 10
Training loss: 3.9670846462249756
Validation loss: 4.6210652115524455

Epoch: 4| Step: 0
Training loss: 3.477893829345703
Validation loss: 4.584884820445891

Epoch: 5| Step: 1
Training loss: 3.832474946975708
Validation loss: 4.547546960974253

Epoch: 5| Step: 2
Training loss: 4.7297515869140625
Validation loss: 4.511083787487399

Epoch: 5| Step: 3
Training loss: 3.426966905593872
Validation loss: 4.4735358453566025

Epoch: 5| Step: 4
Training loss: 4.674223899841309
Validation loss: 4.434688245096514

Epoch: 5| Step: 5
Training loss: 4.819478511810303
Validation loss: 4.396183357443861

Epoch: 5| Step: 6
Training loss: 4.49931001663208
Validation loss: 4.354413791369367

Epoch: 5| Step: 7
Training loss: 5.673040866851807
Validation loss: 4.313638307714975

Epoch: 5| Step: 8
Training loss: 4.257024765014648
Validation loss: 4.269664718258765

Epoch: 5| Step: 9
Training loss: 3.106645107269287
Validation loss: 4.2233032564963064

Epoch: 5| Step: 10
Training loss: 3.474804639816284
Validation loss: 4.173941735298403

Epoch: 5| Step: 0
Training loss: 4.28109073638916
Validation loss: 4.126742237357683

Epoch: 5| Step: 1
Training loss: 3.4443347454071045
Validation loss: 4.079095891726914

Epoch: 5| Step: 2
Training loss: 4.728879451751709
Validation loss: 4.033477014110934

Epoch: 5| Step: 3
Training loss: 4.791084289550781
Validation loss: 3.9918768790460404

Epoch: 5| Step: 4
Training loss: 3.478240966796875
Validation loss: 3.9537077975529495

Epoch: 5| Step: 5
Training loss: 3.989556074142456
Validation loss: 3.9124094901546353

Epoch: 5| Step: 6
Training loss: 3.4865100383758545
Validation loss: 3.878092499189479

Epoch: 5| Step: 7
Training loss: 3.5551419258117676
Validation loss: 3.848027288272817

Epoch: 5| Step: 8
Training loss: 3.6811130046844482
Validation loss: 3.8163925088861936

Epoch: 5| Step: 9
Training loss: 2.842780590057373
Validation loss: 3.7886339413222445

Epoch: 5| Step: 10
Training loss: 3.409895420074463
Validation loss: 3.7684491988151305

Epoch: 6| Step: 0
Training loss: 3.6337997913360596
Validation loss: 3.7480855629008305

Epoch: 5| Step: 1
Training loss: 3.63236665725708
Validation loss: 3.7269950015570528

Epoch: 5| Step: 2
Training loss: 3.4691853523254395
Validation loss: 3.7079296112060547

Epoch: 5| Step: 3
Training loss: 2.732046365737915
Validation loss: 3.6868893279824206

Epoch: 5| Step: 4
Training loss: 3.251539707183838
Validation loss: 3.669230161174651

Epoch: 5| Step: 5
Training loss: 3.468958616256714
Validation loss: 3.6546956057189615

Epoch: 5| Step: 6
Training loss: 3.6788196563720703
Validation loss: 3.639278324701453

Epoch: 5| Step: 7
Training loss: 4.44268798828125
Validation loss: 3.6294841868903047

Epoch: 5| Step: 8
Training loss: 3.627206325531006
Validation loss: 3.615452366490518

Epoch: 5| Step: 9
Training loss: 4.086447715759277
Validation loss: 3.6025650347432783

Epoch: 5| Step: 10
Training loss: 3.190340042114258
Validation loss: 3.589340009996968

Epoch: 7| Step: 0
Training loss: 3.140305280685425
Validation loss: 3.578135908290904

Epoch: 5| Step: 1
Training loss: 4.060883522033691
Validation loss: 3.566976524168445

Epoch: 5| Step: 2
Training loss: 2.719526767730713
Validation loss: 3.5556633421169814

Epoch: 5| Step: 3
Training loss: 3.7351150512695312
Validation loss: 3.546684234373031

Epoch: 5| Step: 4
Training loss: 3.4877288341522217
Validation loss: 3.535695058043285

Epoch: 5| Step: 5
Training loss: 2.9476096630096436
Validation loss: 3.5265229235413256

Epoch: 5| Step: 6
Training loss: 2.973607301712036
Validation loss: 3.5144582102375646

Epoch: 5| Step: 7
Training loss: 4.5400614738464355
Validation loss: 3.5043713251749673

Epoch: 5| Step: 8
Training loss: 2.9723994731903076
Validation loss: 3.4941976967678277

Epoch: 5| Step: 9
Training loss: 3.8054003715515137
Validation loss: 3.4840310799178256

Epoch: 5| Step: 10
Training loss: 3.691169500350952
Validation loss: 3.4732169694797967

Epoch: 8| Step: 0
Training loss: 3.6175765991210938
Validation loss: 3.464143153159849

Epoch: 5| Step: 1
Training loss: 3.7067933082580566
Validation loss: 3.457399691304853

Epoch: 5| Step: 2
Training loss: 3.0748214721679688
Validation loss: 3.446177969696701

Epoch: 5| Step: 3
Training loss: 3.281984329223633
Validation loss: 3.4397473540357364

Epoch: 5| Step: 4
Training loss: 3.734956741333008
Validation loss: 3.4267309968189528

Epoch: 5| Step: 5
Training loss: 2.359281301498413
Validation loss: 3.4186837673187256

Epoch: 5| Step: 6
Training loss: 3.574108600616455
Validation loss: 3.411814046162431

Epoch: 5| Step: 7
Training loss: 2.8560750484466553
Validation loss: 3.4009465504718084

Epoch: 5| Step: 8
Training loss: 3.288959503173828
Validation loss: 3.3939079674341346

Epoch: 5| Step: 9
Training loss: 4.473742485046387
Validation loss: 3.384834776642502

Epoch: 5| Step: 10
Training loss: 3.0516951084136963
Validation loss: 3.3748865896655666

Epoch: 9| Step: 0
Training loss: 3.0502145290374756
Validation loss: 3.367739139064666

Epoch: 5| Step: 1
Training loss: 3.035337448120117
Validation loss: 3.358802813355641

Epoch: 5| Step: 2
Training loss: 3.7403416633605957
Validation loss: 3.351681750307801

Epoch: 5| Step: 3
Training loss: 3.701113224029541
Validation loss: 3.3443267550519717

Epoch: 5| Step: 4
Training loss: 3.4579110145568848
Validation loss: 3.3337782172746557

Epoch: 5| Step: 5
Training loss: 2.7273852825164795
Validation loss: 3.3263035974194928

Epoch: 5| Step: 6
Training loss: 2.9167675971984863
Validation loss: 3.3196957880450833

Epoch: 5| Step: 7
Training loss: 3.607714891433716
Validation loss: 3.3131636932332027

Epoch: 5| Step: 8
Training loss: 3.6216607093811035
Validation loss: 3.3060786493362917

Epoch: 5| Step: 9
Training loss: 3.027320146560669
Validation loss: 3.2984733273906093

Epoch: 5| Step: 10
Training loss: 3.390665054321289
Validation loss: 3.292562315540929

Epoch: 10| Step: 0
Training loss: 3.7579009532928467
Validation loss: 3.284872729291198

Epoch: 5| Step: 1
Training loss: 3.5637309551239014
Validation loss: 3.2761927932821293

Epoch: 5| Step: 2
Training loss: 3.2434425354003906
Validation loss: 3.274249481898482

Epoch: 5| Step: 3
Training loss: 3.3415799140930176
Validation loss: 3.26894457878605

Epoch: 5| Step: 4
Training loss: 2.837930202484131
Validation loss: 3.2610378931927424

Epoch: 5| Step: 5
Training loss: 2.27129864692688
Validation loss: 3.252697583167784

Epoch: 5| Step: 6
Training loss: 3.144150495529175
Validation loss: 3.2488088966697775

Epoch: 5| Step: 7
Training loss: 3.435290575027466
Validation loss: 3.237849973863171

Epoch: 5| Step: 8
Training loss: 3.2392172813415527
Validation loss: 3.2335449264895533

Epoch: 5| Step: 9
Training loss: 4.02739143371582
Validation loss: 3.2222263479745514

Epoch: 5| Step: 10
Training loss: 2.6525180339813232
Validation loss: 3.214418590709727

Epoch: 11| Step: 0
Training loss: 3.676588773727417
Validation loss: 3.2128194096267864

Epoch: 5| Step: 1
Training loss: 3.1680235862731934
Validation loss: 3.200184509318362

Epoch: 5| Step: 2
Training loss: 3.88240385055542
Validation loss: 3.194464429732292

Epoch: 5| Step: 3
Training loss: 2.592582941055298
Validation loss: 3.1933303289515997

Epoch: 5| Step: 4
Training loss: 2.9549732208251953
Validation loss: 3.1889843761280017

Epoch: 5| Step: 5
Training loss: 3.082862138748169
Validation loss: 3.180848321607036

Epoch: 5| Step: 6
Training loss: 3.805284023284912
Validation loss: 3.1739720016397457

Epoch: 5| Step: 7
Training loss: 3.4756150245666504
Validation loss: 3.166616124491538

Epoch: 5| Step: 8
Training loss: 3.0824036598205566
Validation loss: 3.158325492694814

Epoch: 5| Step: 9
Training loss: 2.747384548187256
Validation loss: 3.146091133035639

Epoch: 5| Step: 10
Training loss: 2.4270143508911133
Validation loss: 3.131922188625541

Epoch: 12| Step: 0
Training loss: 3.40006685256958
Validation loss: 3.1234669634090957

Epoch: 5| Step: 1
Training loss: 2.81326961517334
Validation loss: 3.1161077714735463

Epoch: 5| Step: 2
Training loss: 2.615926504135132
Validation loss: 3.103721898089173

Epoch: 5| Step: 3
Training loss: 2.8563966751098633
Validation loss: 3.0982746847214235

Epoch: 5| Step: 4
Training loss: 3.1416537761688232
Validation loss: 3.0913271391263573

Epoch: 5| Step: 5
Training loss: 2.6880269050598145
Validation loss: 3.079376082266531

Epoch: 5| Step: 6
Training loss: 4.09979248046875
Validation loss: 3.0797122550267044

Epoch: 5| Step: 7
Training loss: 3.371519088745117
Validation loss: 3.0745482880582093

Epoch: 5| Step: 8
Training loss: 4.253580093383789
Validation loss: 3.06176491706602

Epoch: 5| Step: 9
Training loss: 2.459584951400757
Validation loss: 3.0543392294196674

Epoch: 5| Step: 10
Training loss: 2.495779514312744
Validation loss: 3.048861993256436

Epoch: 13| Step: 0
Training loss: 5.073167324066162
Validation loss: 3.0480371547001663

Epoch: 5| Step: 1
Training loss: 3.402682065963745
Validation loss: 3.033315074059271

Epoch: 5| Step: 2
Training loss: 2.713460683822632
Validation loss: 3.0213178998680523

Epoch: 5| Step: 3
Training loss: 2.844574451446533
Validation loss: 3.0145578871491137

Epoch: 5| Step: 4
Training loss: 3.275799512863159
Validation loss: 3.0071608379322994

Epoch: 5| Step: 5
Training loss: 2.9190778732299805
Validation loss: 3.0021727290204776

Epoch: 5| Step: 6
Training loss: 3.189332962036133
Validation loss: 3.0045107410800074

Epoch: 5| Step: 7
Training loss: 2.768131732940674
Validation loss: 2.986287281077395

Epoch: 5| Step: 8
Training loss: 3.150305986404419
Validation loss: 2.985048391485727

Epoch: 5| Step: 9
Training loss: 2.554089069366455
Validation loss: 2.98526329891656

Epoch: 5| Step: 10
Training loss: 1.6909983158111572
Validation loss: 2.9726132680011053

Epoch: 14| Step: 0
Training loss: 3.7674498558044434
Validation loss: 2.9639825923468477

Epoch: 5| Step: 1
Training loss: 2.9162707328796387
Validation loss: 2.9587638480688936

Epoch: 5| Step: 2
Training loss: 3.4490954875946045
Validation loss: 2.957363636262955

Epoch: 5| Step: 3
Training loss: 3.9147567749023438
Validation loss: 2.948344563925138

Epoch: 5| Step: 4
Training loss: 2.5026133060455322
Validation loss: 2.942551471853769

Epoch: 5| Step: 5
Training loss: 3.332556962966919
Validation loss: 2.9366810142353015

Epoch: 5| Step: 6
Training loss: 2.9467835426330566
Validation loss: 2.932653916779385

Epoch: 5| Step: 7
Training loss: 2.4578189849853516
Validation loss: 2.9315090640898673

Epoch: 5| Step: 8
Training loss: 2.5805609226226807
Validation loss: 2.92344771405702

Epoch: 5| Step: 9
Training loss: 3.131640911102295
Validation loss: 2.91637755978492

Epoch: 5| Step: 10
Training loss: 2.125227689743042
Validation loss: 2.915954648807485

Epoch: 15| Step: 0
Training loss: 3.39805269241333
Validation loss: 2.9090158221542195

Epoch: 5| Step: 1
Training loss: 3.114116668701172
Validation loss: 2.9034559649805867

Epoch: 5| Step: 2
Training loss: 3.2770843505859375
Validation loss: 2.9103569779344785

Epoch: 5| Step: 3
Training loss: 2.620151996612549
Validation loss: 2.8895708232797603

Epoch: 5| Step: 4
Training loss: 3.053905963897705
Validation loss: 2.889472779407296

Epoch: 5| Step: 5
Training loss: 2.9451136589050293
Validation loss: 2.884946500101397

Epoch: 5| Step: 6
Training loss: 2.6076579093933105
Validation loss: 2.8802614494036605

Epoch: 5| Step: 7
Training loss: 2.510328769683838
Validation loss: 2.8735073522854875

Epoch: 5| Step: 8
Training loss: 3.1029860973358154
Validation loss: 2.865860567297987

Epoch: 5| Step: 9
Training loss: 2.855750560760498
Validation loss: 2.8656717244014946

Epoch: 5| Step: 10
Training loss: 3.372662305831909
Validation loss: 2.857663054620066

Epoch: 16| Step: 0
Training loss: 3.2310128211975098
Validation loss: 2.8597318177582114

Epoch: 5| Step: 1
Training loss: 3.6376051902770996
Validation loss: 2.8523117547394126

Epoch: 5| Step: 2
Training loss: 3.965035915374756
Validation loss: 2.84641460962193

Epoch: 5| Step: 3
Training loss: 2.3814661502838135
Validation loss: 2.8433268659858295

Epoch: 5| Step: 4
Training loss: 2.213346242904663
Validation loss: 2.8380180430668656

Epoch: 5| Step: 5
Training loss: 2.9752609729766846
Validation loss: 2.8423461426970777

Epoch: 5| Step: 6
Training loss: 2.2531561851501465
Validation loss: 2.8267515602932183

Epoch: 5| Step: 7
Training loss: 2.4417757987976074
Validation loss: 2.8272446765694568

Epoch: 5| Step: 8
Training loss: 3.1418111324310303
Validation loss: 2.8297892334640666

Epoch: 5| Step: 9
Training loss: 3.4617340564727783
Validation loss: 2.8280934518383396

Epoch: 5| Step: 10
Training loss: 2.762624502182007
Validation loss: 2.8245720812069472

Epoch: 17| Step: 0
Training loss: 3.988799571990967
Validation loss: 2.814237415149648

Epoch: 5| Step: 1
Training loss: 2.7847437858581543
Validation loss: 2.810533413323023

Epoch: 5| Step: 2
Training loss: 3.3005783557891846
Validation loss: 2.81152428350141

Epoch: 5| Step: 3
Training loss: 3.00176739692688
Validation loss: 2.81255240337823

Epoch: 5| Step: 4
Training loss: 2.0332283973693848
Validation loss: 2.8078237502805647

Epoch: 5| Step: 5
Training loss: 2.990201950073242
Validation loss: 2.7975559132073515

Epoch: 5| Step: 6
Training loss: 3.372041702270508
Validation loss: 2.7953052213115077

Epoch: 5| Step: 7
Training loss: 3.198331356048584
Validation loss: 2.7925886928394275

Epoch: 5| Step: 8
Training loss: 2.423959255218506
Validation loss: 2.7959143474537838

Epoch: 5| Step: 9
Training loss: 2.519655466079712
Validation loss: 2.787333837119482

Epoch: 5| Step: 10
Training loss: 2.605983257293701
Validation loss: 2.788018129205191

Epoch: 18| Step: 0
Training loss: 2.857161283493042
Validation loss: 2.7840918853718746

Epoch: 5| Step: 1
Training loss: 2.964475154876709
Validation loss: 2.7807868603737123

Epoch: 5| Step: 2
Training loss: 2.4691219329833984
Validation loss: 2.802281246390394

Epoch: 5| Step: 3
Training loss: 2.9144368171691895
Validation loss: 2.8106880444352345

Epoch: 5| Step: 4
Training loss: 2.5889649391174316
Validation loss: 2.771666173012026

Epoch: 5| Step: 5
Training loss: 3.4360382556915283
Validation loss: 2.770981598925847

Epoch: 5| Step: 6
Training loss: 3.228494167327881
Validation loss: 2.7740528045162076

Epoch: 5| Step: 7
Training loss: 3.2187933921813965
Validation loss: 2.777877564071327

Epoch: 5| Step: 8
Training loss: 3.043358087539673
Validation loss: 2.777836897039926

Epoch: 5| Step: 9
Training loss: 2.2276551723480225
Validation loss: 2.775823341902866

Epoch: 5| Step: 10
Training loss: 3.2073917388916016
Validation loss: 2.7669792918748755

Epoch: 19| Step: 0
Training loss: 2.31062650680542
Validation loss: 2.765255505038846

Epoch: 5| Step: 1
Training loss: 3.3554885387420654
Validation loss: 2.7633790944212224

Epoch: 5| Step: 2
Training loss: 2.7743446826934814
Validation loss: 2.764793442141625

Epoch: 5| Step: 3
Training loss: 2.9692211151123047
Validation loss: 2.7672018748457714

Epoch: 5| Step: 4
Training loss: 2.41241192817688
Validation loss: 2.7614582302749797

Epoch: 5| Step: 5
Training loss: 3.9398598670959473
Validation loss: 2.7573706616637526

Epoch: 5| Step: 6
Training loss: 2.9382903575897217
Validation loss: 2.7480121120329826

Epoch: 5| Step: 7
Training loss: 2.584341526031494
Validation loss: 2.7462135848178657

Epoch: 5| Step: 8
Training loss: 2.8179502487182617
Validation loss: 2.743548995705061

Epoch: 5| Step: 9
Training loss: 2.982459545135498
Validation loss: 2.7389457405254407

Epoch: 5| Step: 10
Training loss: 2.790879011154175
Validation loss: 2.7400493955099456

Epoch: 20| Step: 0
Training loss: 2.573129653930664
Validation loss: 2.7584696123676915

Epoch: 5| Step: 1
Training loss: 2.264626979827881
Validation loss: 2.728629897999507

Epoch: 5| Step: 2
Training loss: 3.1404223442077637
Validation loss: 2.717547660232872

Epoch: 5| Step: 3
Training loss: 2.840519428253174
Validation loss: 2.7179504440676783

Epoch: 5| Step: 4
Training loss: 2.639918565750122
Validation loss: 2.719850222269694

Epoch: 5| Step: 5
Training loss: 3.1394457817077637
Validation loss: 2.725794058974071

Epoch: 5| Step: 6
Training loss: 3.4143402576446533
Validation loss: 2.7209649316726194

Epoch: 5| Step: 7
Training loss: 3.416855573654175
Validation loss: 2.7185305728707263

Epoch: 5| Step: 8
Training loss: 2.288280963897705
Validation loss: 2.7170335400489067

Epoch: 5| Step: 9
Training loss: 2.8497073650360107
Validation loss: 2.718701321591613

Epoch: 5| Step: 10
Training loss: 3.1317856311798096
Validation loss: 2.7144448859717256

Epoch: 21| Step: 0
Training loss: 2.7128448486328125
Validation loss: 2.7066784622848674

Epoch: 5| Step: 1
Training loss: 3.4008357524871826
Validation loss: 2.7020875305257817

Epoch: 5| Step: 2
Training loss: 3.0266425609588623
Validation loss: 2.6997234154773015

Epoch: 5| Step: 3
Training loss: 3.182990550994873
Validation loss: 2.6963214387175856

Epoch: 5| Step: 4
Training loss: 2.7602953910827637
Validation loss: 2.6945671035397436

Epoch: 5| Step: 5
Training loss: 2.2651872634887695
Validation loss: 2.700185901375227

Epoch: 5| Step: 6
Training loss: 2.637265682220459
Validation loss: 2.7004379713407127

Epoch: 5| Step: 7
Training loss: 2.763272762298584
Validation loss: 2.694913097607192

Epoch: 5| Step: 8
Training loss: 2.211813449859619
Validation loss: 2.691741426785787

Epoch: 5| Step: 9
Training loss: 3.5795738697052
Validation loss: 2.706765023610925

Epoch: 5| Step: 10
Training loss: 2.990560293197632
Validation loss: 2.6874620068457817

Epoch: 22| Step: 0
Training loss: 2.661787509918213
Validation loss: 2.6907529548932145

Epoch: 5| Step: 1
Training loss: 2.869851589202881
Validation loss: 2.6910651678680093

Epoch: 5| Step: 2
Training loss: 2.451195478439331
Validation loss: 2.6895086880653136

Epoch: 5| Step: 3
Training loss: 2.8778696060180664
Validation loss: 2.683994016339702

Epoch: 5| Step: 4
Training loss: 3.4394593238830566
Validation loss: 2.6881743349054807

Epoch: 5| Step: 5
Training loss: 2.8152177333831787
Validation loss: 2.6831093244655158

Epoch: 5| Step: 6
Training loss: 2.845623016357422
Validation loss: 2.682932074351977

Epoch: 5| Step: 7
Training loss: 2.7656705379486084
Validation loss: 2.681144711791828

Epoch: 5| Step: 8
Training loss: 3.071049213409424
Validation loss: 2.6775522719147387

Epoch: 5| Step: 9
Training loss: 3.1790287494659424
Validation loss: 2.676400935778054

Epoch: 5| Step: 10
Training loss: 2.298785924911499
Validation loss: 2.674510745592015

Epoch: 23| Step: 0
Training loss: 2.3934223651885986
Validation loss: 2.6780676944281465

Epoch: 5| Step: 1
Training loss: 2.419128656387329
Validation loss: 2.699507931227325

Epoch: 5| Step: 2
Training loss: 3.379673719406128
Validation loss: 2.6942172691386235

Epoch: 5| Step: 3
Training loss: 2.3126296997070312
Validation loss: 2.6772824589924147

Epoch: 5| Step: 4
Training loss: 2.9918627738952637
Validation loss: 2.6730448930494246

Epoch: 5| Step: 5
Training loss: 2.826240062713623
Validation loss: 2.675803179381996

Epoch: 5| Step: 6
Training loss: 2.1523897647857666
Validation loss: 2.683246986840361

Epoch: 5| Step: 7
Training loss: 2.9035990238189697
Validation loss: 2.6855921104390132

Epoch: 5| Step: 8
Training loss: 3.653566360473633
Validation loss: 2.6878562563209125

Epoch: 5| Step: 9
Training loss: 3.3169074058532715
Validation loss: 2.688363782821163

Epoch: 5| Step: 10
Training loss: 3.1149353981018066
Validation loss: 2.6723626313670987

Epoch: 24| Step: 0
Training loss: 2.798001527786255
Validation loss: 2.6687748637250674

Epoch: 5| Step: 1
Training loss: 2.951536178588867
Validation loss: 2.6721964343901603

Epoch: 5| Step: 2
Training loss: 2.0901100635528564
Validation loss: 2.6736576480250203

Epoch: 5| Step: 3
Training loss: 2.4014639854431152
Validation loss: 2.678094240926927

Epoch: 5| Step: 4
Training loss: 2.8502156734466553
Validation loss: 2.683361117557813

Epoch: 5| Step: 5
Training loss: 2.945129871368408
Validation loss: 2.7107287888885825

Epoch: 5| Step: 6
Training loss: 3.8122506141662598
Validation loss: 2.708849101938227

Epoch: 5| Step: 7
Training loss: 2.7265543937683105
Validation loss: 2.6885313872368104

Epoch: 5| Step: 8
Training loss: 3.2932019233703613
Validation loss: 2.6673120811421382

Epoch: 5| Step: 9
Training loss: 3.1215176582336426
Validation loss: 2.6633101099280903

Epoch: 5| Step: 10
Training loss: 2.279080867767334
Validation loss: 2.6592498338350685

Epoch: 25| Step: 0
Training loss: 2.860966920852661
Validation loss: 2.658471579192787

Epoch: 5| Step: 1
Training loss: 2.816649913787842
Validation loss: 2.659754127584478

Epoch: 5| Step: 2
Training loss: 3.222877025604248
Validation loss: 2.665528074387581

Epoch: 5| Step: 3
Training loss: 2.708447217941284
Validation loss: 2.6673735726264214

Epoch: 5| Step: 4
Training loss: 3.3338210582733154
Validation loss: 2.6598677853102326

Epoch: 5| Step: 5
Training loss: 2.8806629180908203
Validation loss: 2.651612574054349

Epoch: 5| Step: 6
Training loss: 2.2140583992004395
Validation loss: 2.6479128740167104

Epoch: 5| Step: 7
Training loss: 2.4954276084899902
Validation loss: 2.644897896756408

Epoch: 5| Step: 8
Training loss: 2.8656575679779053
Validation loss: 2.6630245536886235

Epoch: 5| Step: 9
Training loss: 2.986236333847046
Validation loss: 2.648788721330704

Epoch: 5| Step: 10
Training loss: 2.8366611003875732
Validation loss: 2.6386303183852986

Epoch: 26| Step: 0
Training loss: 2.6086926460266113
Validation loss: 2.6406943746792373

Epoch: 5| Step: 1
Training loss: 2.4833126068115234
Validation loss: 2.6356933373276905

Epoch: 5| Step: 2
Training loss: 3.7224578857421875
Validation loss: 2.636813807231124

Epoch: 5| Step: 3
Training loss: 2.878009080886841
Validation loss: 2.637604098166189

Epoch: 5| Step: 4
Training loss: 3.3722167015075684
Validation loss: 2.6354086706715245

Epoch: 5| Step: 5
Training loss: 2.766115427017212
Validation loss: 2.6363113695575344

Epoch: 5| Step: 6
Training loss: 2.3156723976135254
Validation loss: 2.6329826693381033

Epoch: 5| Step: 7
Training loss: 3.1966991424560547
Validation loss: 2.634957318664879

Epoch: 5| Step: 8
Training loss: 2.5704026222229004
Validation loss: 2.640154392488541

Epoch: 5| Step: 9
Training loss: 2.5477826595306396
Validation loss: 2.6466797141618628

Epoch: 5| Step: 10
Training loss: 2.586041212081909
Validation loss: 2.638486503272928

Epoch: 27| Step: 0
Training loss: 2.633363723754883
Validation loss: 2.63144782025327

Epoch: 5| Step: 1
Training loss: 2.9342989921569824
Validation loss: 2.631606314771919

Epoch: 5| Step: 2
Training loss: 3.4660885334014893
Validation loss: 2.6382327182318575

Epoch: 5| Step: 3
Training loss: 3.6455256938934326
Validation loss: 2.6354531908548005

Epoch: 5| Step: 4
Training loss: 2.154684543609619
Validation loss: 2.6260570890160015

Epoch: 5| Step: 5
Training loss: 3.3035340309143066
Validation loss: 2.6223139147604666

Epoch: 5| Step: 6
Training loss: 2.6878409385681152
Validation loss: 2.6205143569618143

Epoch: 5| Step: 7
Training loss: 2.231253147125244
Validation loss: 2.6179631602379585

Epoch: 5| Step: 8
Training loss: 2.455090284347534
Validation loss: 2.6298544304345244

Epoch: 5| Step: 9
Training loss: 2.9533748626708984
Validation loss: 2.646626246872769

Epoch: 5| Step: 10
Training loss: 2.4622864723205566
Validation loss: 2.6452590342490905

Epoch: 28| Step: 0
Training loss: 3.2923076152801514
Validation loss: 2.6359704361167005

Epoch: 5| Step: 1
Training loss: 2.2347207069396973
Validation loss: 2.6344028288318264

Epoch: 5| Step: 2
Training loss: 2.792102336883545
Validation loss: 2.6150647722264773

Epoch: 5| Step: 3
Training loss: 2.370119571685791
Validation loss: 2.615052653897193

Epoch: 5| Step: 4
Training loss: 3.006525754928589
Validation loss: 2.6250992487835627

Epoch: 5| Step: 5
Training loss: 3.370962619781494
Validation loss: 2.621694403309976

Epoch: 5| Step: 6
Training loss: 2.8781471252441406
Validation loss: 2.6323033045696955

Epoch: 5| Step: 7
Training loss: 3.3741374015808105
Validation loss: 2.6338045520167195

Epoch: 5| Step: 8
Training loss: 2.7818803787231445
Validation loss: 2.6274682501310944

Epoch: 5| Step: 9
Training loss: 2.320857286453247
Validation loss: 2.6252728252000708

Epoch: 5| Step: 10
Training loss: 2.4791057109832764
Validation loss: 2.622274911531838

Epoch: 29| Step: 0
Training loss: 2.142354965209961
Validation loss: 2.618682776727984

Epoch: 5| Step: 1
Training loss: 3.6675288677215576
Validation loss: 2.6208986492567163

Epoch: 5| Step: 2
Training loss: 3.0973892211914062
Validation loss: 2.6226517000506

Epoch: 5| Step: 3
Training loss: 3.515580654144287
Validation loss: 2.6163656609032744

Epoch: 5| Step: 4
Training loss: 2.85119366645813
Validation loss: 2.621459743028046

Epoch: 5| Step: 5
Training loss: 3.0693862438201904
Validation loss: 2.6195373919702347

Epoch: 5| Step: 6
Training loss: 2.4938158988952637
Validation loss: 2.6131452411733647

Epoch: 5| Step: 7
Training loss: 2.779785394668579
Validation loss: 2.6176526443932646

Epoch: 5| Step: 8
Training loss: 2.0606744289398193
Validation loss: 2.6188465292735765

Epoch: 5| Step: 9
Training loss: 2.7270114421844482
Validation loss: 2.615199419759935

Epoch: 5| Step: 10
Training loss: 2.4584298133850098
Validation loss: 2.612958595316897

Epoch: 30| Step: 0
Training loss: 2.5196404457092285
Validation loss: 2.6074828281197497

Epoch: 5| Step: 1
Training loss: 1.85784912109375
Validation loss: 2.612281673698015

Epoch: 5| Step: 2
Training loss: 2.879758596420288
Validation loss: 2.609273579812819

Epoch: 5| Step: 3
Training loss: 3.107553005218506
Validation loss: 2.609674019198264

Epoch: 5| Step: 4
Training loss: 2.3641419410705566
Validation loss: 2.6076685433746665

Epoch: 5| Step: 5
Training loss: 2.7450149059295654
Validation loss: 2.609621024900867

Epoch: 5| Step: 6
Training loss: 2.1899771690368652
Validation loss: 2.6050066076299196

Epoch: 5| Step: 7
Training loss: 3.4633994102478027
Validation loss: 2.6020650735465427

Epoch: 5| Step: 8
Training loss: 2.8908607959747314
Validation loss: 2.6045848143998014

Epoch: 5| Step: 9
Training loss: 2.9750397205352783
Validation loss: 2.6026056223018195

Epoch: 5| Step: 10
Training loss: 3.9712884426116943
Validation loss: 2.6090476077090026

Epoch: 31| Step: 0
Training loss: 2.9836325645446777
Validation loss: 2.601049315544867

Epoch: 5| Step: 1
Training loss: 2.8808839321136475
Validation loss: 2.6052082123294955

Epoch: 5| Step: 2
Training loss: 3.284306287765503
Validation loss: 2.6031101570334485

Epoch: 5| Step: 3
Training loss: 2.6035170555114746
Validation loss: 2.6000886258258613

Epoch: 5| Step: 4
Training loss: 3.105797290802002
Validation loss: 2.599553369706677

Epoch: 5| Step: 5
Training loss: 2.469665050506592
Validation loss: 2.600399112188688

Epoch: 5| Step: 6
Training loss: 2.1270413398742676
Validation loss: 2.5934233768011934

Epoch: 5| Step: 7
Training loss: 3.0313985347747803
Validation loss: 2.597788587693245

Epoch: 5| Step: 8
Training loss: 3.3708808422088623
Validation loss: 2.599010488038422

Epoch: 5| Step: 9
Training loss: 2.377469778060913
Validation loss: 2.604348682588147

Epoch: 5| Step: 10
Training loss: 2.4692907333374023
Validation loss: 2.599556182020454

Epoch: 32| Step: 0
Training loss: 3.0538554191589355
Validation loss: 2.61513421099673

Epoch: 5| Step: 1
Training loss: 2.7111504077911377
Validation loss: 2.594041265467162

Epoch: 5| Step: 2
Training loss: 2.5729401111602783
Validation loss: 2.5918651242409982

Epoch: 5| Step: 3
Training loss: 3.074397325515747
Validation loss: 2.596066942778967

Epoch: 5| Step: 4
Training loss: 2.9648730754852295
Validation loss: 2.598518171618062

Epoch: 5| Step: 5
Training loss: 2.9012529850006104
Validation loss: 2.6074740861051824

Epoch: 5| Step: 6
Training loss: 3.1234383583068848
Validation loss: 2.6165788122402724

Epoch: 5| Step: 7
Training loss: 2.91961932182312
Validation loss: 2.60154305478578

Epoch: 5| Step: 8
Training loss: 2.7460672855377197
Validation loss: 2.593464546306159

Epoch: 5| Step: 9
Training loss: 2.4548001289367676
Validation loss: 2.585118414253317

Epoch: 5| Step: 10
Training loss: 2.038012742996216
Validation loss: 2.5878581411095074

Epoch: 33| Step: 0
Training loss: 3.0802531242370605
Validation loss: 2.589253097452143

Epoch: 5| Step: 1
Training loss: 3.228921413421631
Validation loss: 2.5880062477563017

Epoch: 5| Step: 2
Training loss: 2.6083335876464844
Validation loss: 2.585761606052358

Epoch: 5| Step: 3
Training loss: 2.267817735671997
Validation loss: 2.587293573605117

Epoch: 5| Step: 4
Training loss: 1.9961858987808228
Validation loss: 2.5852360981766895

Epoch: 5| Step: 5
Training loss: 3.04089617729187
Validation loss: 2.5914221040664183

Epoch: 5| Step: 6
Training loss: 3.0577192306518555
Validation loss: 2.593310145921605

Epoch: 5| Step: 7
Training loss: 2.1499569416046143
Validation loss: 2.6002182755419003

Epoch: 5| Step: 8
Training loss: 3.227354049682617
Validation loss: 2.5961125871186614

Epoch: 5| Step: 9
Training loss: 2.8521904945373535
Validation loss: 2.5991612634351178

Epoch: 5| Step: 10
Training loss: 3.1844801902770996
Validation loss: 2.59379998073783

Epoch: 34| Step: 0
Training loss: 2.9095847606658936
Validation loss: 2.5790221793677217

Epoch: 5| Step: 1
Training loss: 2.7582333087921143
Validation loss: 2.587162199840751

Epoch: 5| Step: 2
Training loss: 2.8431103229522705
Validation loss: 2.584938495389877

Epoch: 5| Step: 3
Training loss: 2.304331064224243
Validation loss: 2.5749758058978665

Epoch: 5| Step: 4
Training loss: 3.037175416946411
Validation loss: 2.5770438973621657

Epoch: 5| Step: 5
Training loss: 2.8813436031341553
Validation loss: 2.5731901994315525

Epoch: 5| Step: 6
Training loss: 2.7406697273254395
Validation loss: 2.5776440353803736

Epoch: 5| Step: 7
Training loss: 3.2388253211975098
Validation loss: 2.5800289389907674

Epoch: 5| Step: 8
Training loss: 2.8709590435028076
Validation loss: 2.5801555213107856

Epoch: 5| Step: 9
Training loss: 2.519570827484131
Validation loss: 2.575336253771218

Epoch: 5| Step: 10
Training loss: 2.3774666786193848
Validation loss: 2.574541481592322

Epoch: 35| Step: 0
Training loss: 2.4885146617889404
Validation loss: 2.574195661852437

Epoch: 5| Step: 1
Training loss: 2.628220319747925
Validation loss: 2.5760785994991178

Epoch: 5| Step: 2
Training loss: 2.776271343231201
Validation loss: 2.577469712944441

Epoch: 5| Step: 3
Training loss: 3.0142955780029297
Validation loss: 2.568823045299899

Epoch: 5| Step: 4
Training loss: 2.6789634227752686
Validation loss: 2.5676621519109255

Epoch: 5| Step: 5
Training loss: 3.0355114936828613
Validation loss: 2.5782991557992916

Epoch: 5| Step: 6
Training loss: 3.024022102355957
Validation loss: 2.577978339246524

Epoch: 5| Step: 7
Training loss: 2.3511834144592285
Validation loss: 2.5870968193136235

Epoch: 5| Step: 8
Training loss: 2.4520134925842285
Validation loss: 2.595521398769912

Epoch: 5| Step: 9
Training loss: 2.6959214210510254
Validation loss: 2.6183806337336057

Epoch: 5| Step: 10
Training loss: 3.4781012535095215
Validation loss: 2.6032112490746284

Epoch: 36| Step: 0
Training loss: 2.3491976261138916
Validation loss: 2.586046234253914

Epoch: 5| Step: 1
Training loss: 3.360999345779419
Validation loss: 2.5912099346037833

Epoch: 5| Step: 2
Training loss: 2.750880002975464
Validation loss: 2.6060213811935915

Epoch: 5| Step: 3
Training loss: 2.747706890106201
Validation loss: 2.5734961648141184

Epoch: 5| Step: 4
Training loss: 2.9277355670928955
Validation loss: 2.566284889815956

Epoch: 5| Step: 5
Training loss: 2.569862127304077
Validation loss: 2.571656478348599

Epoch: 5| Step: 6
Training loss: 3.1179375648498535
Validation loss: 2.570087940462174

Epoch: 5| Step: 7
Training loss: 2.3941256999969482
Validation loss: 2.5715960841025076

Epoch: 5| Step: 8
Training loss: 2.68852162361145
Validation loss: 2.578511640589724

Epoch: 5| Step: 9
Training loss: 2.876570224761963
Validation loss: 2.586965718576985

Epoch: 5| Step: 10
Training loss: 2.6796720027923584
Validation loss: 2.5882721101084063

Epoch: 37| Step: 0
Training loss: 2.840550184249878
Validation loss: 2.5798527361244283

Epoch: 5| Step: 1
Training loss: 3.3934333324432373
Validation loss: 2.5701991537565827

Epoch: 5| Step: 2
Training loss: 2.686326026916504
Validation loss: 2.5708284096051286

Epoch: 5| Step: 3
Training loss: 2.8569869995117188
Validation loss: 2.5714282015318513

Epoch: 5| Step: 4
Training loss: 2.606924057006836
Validation loss: 2.5709440528705554

Epoch: 5| Step: 5
Training loss: 2.9146158695220947
Validation loss: 2.5650338126767065

Epoch: 5| Step: 6
Training loss: 2.2654058933258057
Validation loss: 2.5619500965200444

Epoch: 5| Step: 7
Training loss: 2.8564300537109375
Validation loss: 2.563591318745767

Epoch: 5| Step: 8
Training loss: 3.058814525604248
Validation loss: 2.566843981383949

Epoch: 5| Step: 9
Training loss: 2.4676756858825684
Validation loss: 2.569630730536676

Epoch: 5| Step: 10
Training loss: 2.443711280822754
Validation loss: 2.5639684559196554

Epoch: 38| Step: 0
Training loss: 3.1060776710510254
Validation loss: 2.560609586777226

Epoch: 5| Step: 1
Training loss: 2.4513683319091797
Validation loss: 2.5638822406850834

Epoch: 5| Step: 2
Training loss: 2.280648708343506
Validation loss: 2.5614816706667662

Epoch: 5| Step: 3
Training loss: 2.758553981781006
Validation loss: 2.557662638284827

Epoch: 5| Step: 4
Training loss: 2.9237639904022217
Validation loss: 2.556752968859929

Epoch: 5| Step: 5
Training loss: 2.818358898162842
Validation loss: 2.557114901081208

Epoch: 5| Step: 6
Training loss: 3.06775164604187
Validation loss: 2.5534577677326817

Epoch: 5| Step: 7
Training loss: 3.096219539642334
Validation loss: 2.5561577555953816

Epoch: 5| Step: 8
Training loss: 2.577455759048462
Validation loss: 2.5566194160010225

Epoch: 5| Step: 9
Training loss: 2.6254982948303223
Validation loss: 2.5578175642157115

Epoch: 5| Step: 10
Training loss: 2.6165034770965576
Validation loss: 2.560713776978113

Epoch: 39| Step: 0
Training loss: 3.4284019470214844
Validation loss: 2.555092168110673

Epoch: 5| Step: 1
Training loss: 3.5968070030212402
Validation loss: 2.553051256364392

Epoch: 5| Step: 2
Training loss: 1.8818483352661133
Validation loss: 2.55355486049447

Epoch: 5| Step: 3
Training loss: 3.334564208984375
Validation loss: 2.560958359831123

Epoch: 5| Step: 4
Training loss: 2.3975276947021484
Validation loss: 2.548993569548412

Epoch: 5| Step: 5
Training loss: 2.6579277515411377
Validation loss: 2.5522914778801704

Epoch: 5| Step: 6
Training loss: 3.2663369178771973
Validation loss: 2.5510573182054745

Epoch: 5| Step: 7
Training loss: 2.613193988800049
Validation loss: 2.5531738624777844

Epoch: 5| Step: 8
Training loss: 2.409557342529297
Validation loss: 2.549089144634944

Epoch: 5| Step: 9
Training loss: 2.373361587524414
Validation loss: 2.5553761759111957

Epoch: 5| Step: 10
Training loss: 2.2893974781036377
Validation loss: 2.558738339331842

Epoch: 40| Step: 0
Training loss: 2.5138163566589355
Validation loss: 2.548684168887395

Epoch: 5| Step: 1
Training loss: 2.170954942703247
Validation loss: 2.547779372943345

Epoch: 5| Step: 2
Training loss: 3.4215145111083984
Validation loss: 2.5496583548925256

Epoch: 5| Step: 3
Training loss: 3.1224589347839355
Validation loss: 2.546987877097181

Epoch: 5| Step: 4
Training loss: 2.4946584701538086
Validation loss: 2.54355421117557

Epoch: 5| Step: 5
Training loss: 3.0964341163635254
Validation loss: 2.543327552016063

Epoch: 5| Step: 6
Training loss: 2.2932562828063965
Validation loss: 2.541889741856565

Epoch: 5| Step: 7
Training loss: 2.611194133758545
Validation loss: 2.543519858391054

Epoch: 5| Step: 8
Training loss: 3.1014034748077393
Validation loss: 2.552729001609228

Epoch: 5| Step: 9
Training loss: 2.3382372856140137
Validation loss: 2.556156817302909

Epoch: 5| Step: 10
Training loss: 3.1825006008148193
Validation loss: 2.5623799318908365

Epoch: 41| Step: 0
Training loss: 2.133168935775757
Validation loss: 2.560267561225481

Epoch: 5| Step: 1
Training loss: 2.370924472808838
Validation loss: 2.5541941222324165

Epoch: 5| Step: 2
Training loss: 2.8273584842681885
Validation loss: 2.5491653052709435

Epoch: 5| Step: 3
Training loss: 2.741636037826538
Validation loss: 2.5465344767416678

Epoch: 5| Step: 4
Training loss: 2.920382261276245
Validation loss: 2.544870315059539

Epoch: 5| Step: 5
Training loss: 2.933173418045044
Validation loss: 2.54330442285025

Epoch: 5| Step: 6
Training loss: 3.3585891723632812
Validation loss: 2.5438159435026106

Epoch: 5| Step: 7
Training loss: 3.0519700050354004
Validation loss: 2.5482276998540407

Epoch: 5| Step: 8
Training loss: 3.2013421058654785
Validation loss: 2.545487262869394

Epoch: 5| Step: 9
Training loss: 1.9885120391845703
Validation loss: 2.54645380409815

Epoch: 5| Step: 10
Training loss: 2.629086971282959
Validation loss: 2.541925520025274

Epoch: 42| Step: 0
Training loss: 2.7950754165649414
Validation loss: 2.5372096518034577

Epoch: 5| Step: 1
Training loss: 2.4840035438537598
Validation loss: 2.538537279252083

Epoch: 5| Step: 2
Training loss: 2.1790993213653564
Validation loss: 2.539076543623401

Epoch: 5| Step: 3
Training loss: 2.7904934883117676
Validation loss: 2.5421614134183494

Epoch: 5| Step: 4
Training loss: 2.8190274238586426
Validation loss: 2.545038600121775

Epoch: 5| Step: 5
Training loss: 2.384075880050659
Validation loss: 2.5410557818669144

Epoch: 5| Step: 6
Training loss: 1.8701517581939697
Validation loss: 2.540369887505808

Epoch: 5| Step: 7
Training loss: 2.8470449447631836
Validation loss: 2.536882754295103

Epoch: 5| Step: 8
Training loss: 3.8879857063293457
Validation loss: 2.536817458368117

Epoch: 5| Step: 9
Training loss: 3.086513042449951
Validation loss: 2.5329426616750736

Epoch: 5| Step: 10
Training loss: 2.9925520420074463
Validation loss: 2.5376911547876175

Epoch: 43| Step: 0
Training loss: 2.53471040725708
Validation loss: 2.540297410821402

Epoch: 5| Step: 1
Training loss: 3.0417404174804688
Validation loss: 2.536865467666298

Epoch: 5| Step: 2
Training loss: 2.1634726524353027
Validation loss: 2.5353049437204995

Epoch: 5| Step: 3
Training loss: 2.7896130084991455
Validation loss: 2.5464303775500228

Epoch: 5| Step: 4
Training loss: 2.7337872982025146
Validation loss: 2.5753682377517864

Epoch: 5| Step: 5
Training loss: 3.0627217292785645
Validation loss: 2.5454253432571248

Epoch: 5| Step: 6
Training loss: 2.6124160289764404
Validation loss: 2.5346327135639806

Epoch: 5| Step: 7
Training loss: 2.5853965282440186
Validation loss: 2.5368364254633584

Epoch: 5| Step: 8
Training loss: 3.165210247039795
Validation loss: 2.5392955964611423

Epoch: 5| Step: 9
Training loss: 2.8831751346588135
Validation loss: 2.5406866970882622

Epoch: 5| Step: 10
Training loss: 2.548309326171875
Validation loss: 2.535655406213576

Epoch: 44| Step: 0
Training loss: 3.182202100753784
Validation loss: 2.540208103836224

Epoch: 5| Step: 1
Training loss: 2.5795223712921143
Validation loss: 2.544028648766138

Epoch: 5| Step: 2
Training loss: 2.3644089698791504
Validation loss: 2.534768937736429

Epoch: 5| Step: 3
Training loss: 3.142080068588257
Validation loss: 2.530486140199887

Epoch: 5| Step: 4
Training loss: 3.3689968585968018
Validation loss: 2.5289598036837835

Epoch: 5| Step: 5
Training loss: 2.7133708000183105
Validation loss: 2.5275241457005984

Epoch: 5| Step: 6
Training loss: 2.5677380561828613
Validation loss: 2.521964139835809

Epoch: 5| Step: 7
Training loss: 2.7424073219299316
Validation loss: 2.5203059681000246

Epoch: 5| Step: 8
Training loss: 2.3316738605499268
Validation loss: 2.523903772395144

Epoch: 5| Step: 9
Training loss: 3.3709349632263184
Validation loss: 2.5282532245882097

Epoch: 5| Step: 10
Training loss: 1.5795294046401978
Validation loss: 2.5222576433612454

Epoch: 45| Step: 0
Training loss: 2.4433350563049316
Validation loss: 2.522686150766188

Epoch: 5| Step: 1
Training loss: 3.438591718673706
Validation loss: 2.519177508610551

Epoch: 5| Step: 2
Training loss: 2.525850296020508
Validation loss: 2.5243288983580885

Epoch: 5| Step: 3
Training loss: 2.7597460746765137
Validation loss: 2.5345894162372877

Epoch: 5| Step: 4
Training loss: 1.9266523122787476
Validation loss: 2.5365444896041707

Epoch: 5| Step: 5
Training loss: 2.6993017196655273
Validation loss: 2.5386431858103764

Epoch: 5| Step: 6
Training loss: 2.7207682132720947
Validation loss: 2.5462922844835507

Epoch: 5| Step: 7
Training loss: 2.5157923698425293
Validation loss: 2.5347964507277294

Epoch: 5| Step: 8
Training loss: 2.935697555541992
Validation loss: 2.5377868785653064

Epoch: 5| Step: 9
Training loss: 3.046251058578491
Validation loss: 2.5396790427546345

Epoch: 5| Step: 10
Training loss: 3.155076742172241
Validation loss: 2.518579782978181

Epoch: 46| Step: 0
Training loss: 2.7234253883361816
Validation loss: 2.5159387178318475

Epoch: 5| Step: 1
Training loss: 2.799208641052246
Validation loss: 2.5316623744144233

Epoch: 5| Step: 2
Training loss: 3.0158984661102295
Validation loss: 2.518041732490704

Epoch: 5| Step: 3
Training loss: 2.9620418548583984
Validation loss: 2.516085796458747

Epoch: 5| Step: 4
Training loss: 3.1867032051086426
Validation loss: 2.519547907255029

Epoch: 5| Step: 5
Training loss: 2.9450671672821045
Validation loss: 2.5173888616664435

Epoch: 5| Step: 6
Training loss: 2.637294292449951
Validation loss: 2.5191422816245788

Epoch: 5| Step: 7
Training loss: 2.4258310794830322
Validation loss: 2.5186739070441133

Epoch: 5| Step: 8
Training loss: 2.541024684906006
Validation loss: 2.5212196585952595

Epoch: 5| Step: 9
Training loss: 2.746593952178955
Validation loss: 2.514183303361298

Epoch: 5| Step: 10
Training loss: 1.9281728267669678
Validation loss: 2.516739350493236

Epoch: 47| Step: 0
Training loss: 3.183148145675659
Validation loss: 2.525181124287267

Epoch: 5| Step: 1
Training loss: 1.8630352020263672
Validation loss: 2.532661122660483

Epoch: 5| Step: 2
Training loss: 2.8933372497558594
Validation loss: 2.5308294450083086

Epoch: 5| Step: 3
Training loss: 2.421611785888672
Validation loss: 2.536433522419263

Epoch: 5| Step: 4
Training loss: 2.725306749343872
Validation loss: 2.529448398979761

Epoch: 5| Step: 5
Training loss: 3.441007614135742
Validation loss: 2.5184013023171374

Epoch: 5| Step: 6
Training loss: 1.9991943836212158
Validation loss: 2.5155967448347356

Epoch: 5| Step: 7
Training loss: 3.0847249031066895
Validation loss: 2.5139414238673385

Epoch: 5| Step: 8
Training loss: 2.4539265632629395
Validation loss: 2.5178980955513577

Epoch: 5| Step: 9
Training loss: 3.581979751586914
Validation loss: 2.528185872621434

Epoch: 5| Step: 10
Training loss: 2.2081997394561768
Validation loss: 2.5197440039726997

Epoch: 48| Step: 0
Training loss: 2.8088111877441406
Validation loss: 2.515730752739855

Epoch: 5| Step: 1
Training loss: 2.664158344268799
Validation loss: 2.5187596274960424

Epoch: 5| Step: 2
Training loss: 2.829174757003784
Validation loss: 2.5139986904718543

Epoch: 5| Step: 3
Training loss: 2.9101977348327637
Validation loss: 2.51093449131135

Epoch: 5| Step: 4
Training loss: 2.6453452110290527
Validation loss: 2.514723321442963

Epoch: 5| Step: 5
Training loss: 2.976799249649048
Validation loss: 2.5216823444571546

Epoch: 5| Step: 6
Training loss: 2.423701047897339
Validation loss: 2.517925190669234

Epoch: 5| Step: 7
Training loss: 2.6372170448303223
Validation loss: 2.5133087609403875

Epoch: 5| Step: 8
Training loss: 1.7025420665740967
Validation loss: 2.5134928175198135

Epoch: 5| Step: 9
Training loss: 3.222637176513672
Validation loss: 2.5106491196540093

Epoch: 5| Step: 10
Training loss: 3.1010124683380127
Validation loss: 2.5112902502859793

Epoch: 49| Step: 0
Training loss: 2.8887648582458496
Validation loss: 2.5132188745724258

Epoch: 5| Step: 1
Training loss: 2.344825267791748
Validation loss: 2.512037971968292

Epoch: 5| Step: 2
Training loss: 2.5310492515563965
Validation loss: 2.5059892131436254

Epoch: 5| Step: 3
Training loss: 2.1842103004455566
Validation loss: 2.502037925104941

Epoch: 5| Step: 4
Training loss: 3.175429344177246
Validation loss: 2.4953046306487052

Epoch: 5| Step: 5
Training loss: 2.4814040660858154
Validation loss: 2.500994723330262

Epoch: 5| Step: 6
Training loss: 2.764458179473877
Validation loss: 2.4968675310893724

Epoch: 5| Step: 7
Training loss: 2.594627857208252
Validation loss: 2.4959165075773835

Epoch: 5| Step: 8
Training loss: 2.936561107635498
Validation loss: 2.49528238081163

Epoch: 5| Step: 9
Training loss: 3.0248045921325684
Validation loss: 2.5037062962849936

Epoch: 5| Step: 10
Training loss: 2.9838204383850098
Validation loss: 2.5205609747158584

Epoch: 50| Step: 0
Training loss: 2.682905912399292
Validation loss: 2.5301661183757167

Epoch: 5| Step: 1
Training loss: 2.6920132637023926
Validation loss: 2.548072163776685

Epoch: 5| Step: 2
Training loss: 2.5071375370025635
Validation loss: 2.520881311867827

Epoch: 5| Step: 3
Training loss: 2.972543478012085
Validation loss: 2.5118967948421353

Epoch: 5| Step: 4
Training loss: 2.2609612941741943
Validation loss: 2.500615199406942

Epoch: 5| Step: 5
Training loss: 2.7205073833465576
Validation loss: 2.499100077536798

Epoch: 5| Step: 6
Training loss: 2.679323434829712
Validation loss: 2.49389983248967

Epoch: 5| Step: 7
Training loss: 2.8209047317504883
Validation loss: 2.4987382606793473

Epoch: 5| Step: 8
Training loss: 2.8874714374542236
Validation loss: 2.4972766035346576

Epoch: 5| Step: 9
Training loss: 3.369053363800049
Validation loss: 2.5043523157796552

Epoch: 5| Step: 10
Training loss: 2.1114501953125
Validation loss: 2.510257372292139

Epoch: 51| Step: 0
Training loss: 2.233436107635498
Validation loss: 2.50449292121395

Epoch: 5| Step: 1
Training loss: 2.6940395832061768
Validation loss: 2.511397454046434

Epoch: 5| Step: 2
Training loss: 3.080587148666382
Validation loss: 2.501609251063357

Epoch: 5| Step: 3
Training loss: 2.6046078205108643
Validation loss: 2.4936702277070735

Epoch: 5| Step: 4
Training loss: 2.9344184398651123
Validation loss: 2.494042545236567

Epoch: 5| Step: 5
Training loss: 2.7560718059539795
Validation loss: 2.49406256726993

Epoch: 5| Step: 6
Training loss: 2.645108699798584
Validation loss: 2.492313692646642

Epoch: 5| Step: 7
Training loss: 2.738901138305664
Validation loss: 2.492567503324119

Epoch: 5| Step: 8
Training loss: 2.005833387374878
Validation loss: 2.495836063097882

Epoch: 5| Step: 9
Training loss: 3.2573513984680176
Validation loss: 2.4998802472186346

Epoch: 5| Step: 10
Training loss: 2.7966136932373047
Validation loss: 2.5208177669073946

Epoch: 52| Step: 0
Training loss: 2.594669818878174
Validation loss: 2.562125080375261

Epoch: 5| Step: 1
Training loss: 2.645117998123169
Validation loss: 2.544486464992646

Epoch: 5| Step: 2
Training loss: 2.8453495502471924
Validation loss: 2.5305670538256244

Epoch: 5| Step: 3
Training loss: 2.9469077587127686
Validation loss: 2.5085360260419947

Epoch: 5| Step: 4
Training loss: 2.8625645637512207
Validation loss: 2.495333589533324

Epoch: 5| Step: 5
Training loss: 2.8683676719665527
Validation loss: 2.489182943938881

Epoch: 5| Step: 6
Training loss: 2.181612491607666
Validation loss: 2.502579494189191

Epoch: 5| Step: 7
Training loss: 2.342848539352417
Validation loss: 2.5300044577608825

Epoch: 5| Step: 8
Training loss: 3.1458961963653564
Validation loss: 2.540154928802162

Epoch: 5| Step: 9
Training loss: 3.1822752952575684
Validation loss: 2.527904015715404

Epoch: 5| Step: 10
Training loss: 2.062178611755371
Validation loss: 2.4956955499546503

Epoch: 53| Step: 0
Training loss: 2.303183078765869
Validation loss: 2.5069083885479997

Epoch: 5| Step: 1
Training loss: 2.85939359664917
Validation loss: 2.5319966603350896

Epoch: 5| Step: 2
Training loss: 3.2189571857452393
Validation loss: 2.555199166779877

Epoch: 5| Step: 3
Training loss: 2.7575583457946777
Validation loss: 2.5518080034563617

Epoch: 5| Step: 4
Training loss: 2.681882381439209
Validation loss: 2.5637178164656445

Epoch: 5| Step: 5
Training loss: 2.6906402111053467
Validation loss: 2.554805322359967

Epoch: 5| Step: 6
Training loss: 1.9750617742538452
Validation loss: 2.575168332745952

Epoch: 5| Step: 7
Training loss: 3.1726601123809814
Validation loss: 2.5519744785883094

Epoch: 5| Step: 8
Training loss: 2.621140956878662
Validation loss: 2.520955462609568

Epoch: 5| Step: 9
Training loss: 2.1969451904296875
Validation loss: 2.4952340228583223

Epoch: 5| Step: 10
Training loss: 3.5060644149780273
Validation loss: 2.4874649586216098

Epoch: 54| Step: 0
Training loss: 2.3067994117736816
Validation loss: 2.492814576754006

Epoch: 5| Step: 1
Training loss: 2.6037216186523438
Validation loss: 2.5085618393395537

Epoch: 5| Step: 2
Training loss: 2.8461384773254395
Validation loss: 2.52026391285722

Epoch: 5| Step: 3
Training loss: 2.581943988800049
Validation loss: 2.522936144182759

Epoch: 5| Step: 4
Training loss: 2.7156307697296143
Validation loss: 2.527895094245993

Epoch: 5| Step: 5
Training loss: 2.434295177459717
Validation loss: 2.5138133033629386

Epoch: 5| Step: 6
Training loss: 3.0704543590545654
Validation loss: 2.495989376498807

Epoch: 5| Step: 7
Training loss: 3.0172672271728516
Validation loss: 2.4956126956529516

Epoch: 5| Step: 8
Training loss: 3.0308749675750732
Validation loss: 2.485652687729046

Epoch: 5| Step: 9
Training loss: 2.2493159770965576
Validation loss: 2.490845195708736

Epoch: 5| Step: 10
Training loss: 3.2163918018341064
Validation loss: 2.510197380537628

Epoch: 55| Step: 0
Training loss: 2.8600597381591797
Validation loss: 2.5117087902561313

Epoch: 5| Step: 1
Training loss: 2.6570801734924316
Validation loss: 2.499514707954981

Epoch: 5| Step: 2
Training loss: 3.4034271240234375
Validation loss: 2.5041159045311714

Epoch: 5| Step: 3
Training loss: 2.24845552444458
Validation loss: 2.4896570559470885

Epoch: 5| Step: 4
Training loss: 2.9831910133361816
Validation loss: 2.481129712955926

Epoch: 5| Step: 5
Training loss: 1.8383163213729858
Validation loss: 2.4735082682742866

Epoch: 5| Step: 6
Training loss: 2.636384963989258
Validation loss: 2.4752793106981503

Epoch: 5| Step: 7
Training loss: 3.139529228210449
Validation loss: 2.4770411547794136

Epoch: 5| Step: 8
Training loss: 2.968968629837036
Validation loss: 2.4791686175971903

Epoch: 5| Step: 9
Training loss: 2.5559051036834717
Validation loss: 2.4764311775084464

Epoch: 5| Step: 10
Training loss: 2.3131794929504395
Validation loss: 2.4799322851242556

Epoch: 56| Step: 0
Training loss: 2.1163337230682373
Validation loss: 2.480724039898124

Epoch: 5| Step: 1
Training loss: 2.8385443687438965
Validation loss: 2.4839222969547397

Epoch: 5| Step: 2
Training loss: 2.7630205154418945
Validation loss: 2.489155133565267

Epoch: 5| Step: 3
Training loss: 2.2944254875183105
Validation loss: 2.50007354059527

Epoch: 5| Step: 4
Training loss: 2.9599173069000244
Validation loss: 2.5024018108203845

Epoch: 5| Step: 5
Training loss: 2.4383673667907715
Validation loss: 2.495059618385889

Epoch: 5| Step: 6
Training loss: 2.6588456630706787
Validation loss: 2.48812525246733

Epoch: 5| Step: 7
Training loss: 3.435770034790039
Validation loss: 2.4813631273085073

Epoch: 5| Step: 8
Training loss: 2.8358511924743652
Validation loss: 2.479234903089462

Epoch: 5| Step: 9
Training loss: 2.630554676055908
Validation loss: 2.4840139189074115

Epoch: 5| Step: 10
Training loss: 2.5510010719299316
Validation loss: 2.4744028942559355

Epoch: 57| Step: 0
Training loss: 2.768139362335205
Validation loss: 2.4704088190550446

Epoch: 5| Step: 1
Training loss: 3.127401828765869
Validation loss: 2.473295114373648

Epoch: 5| Step: 2
Training loss: 2.8114006519317627
Validation loss: 2.4760651050075406

Epoch: 5| Step: 3
Training loss: 2.6040306091308594
Validation loss: 2.477559971553023

Epoch: 5| Step: 4
Training loss: 2.9665074348449707
Validation loss: 2.469828228796682

Epoch: 5| Step: 5
Training loss: 2.2564570903778076
Validation loss: 2.46280167436087

Epoch: 5| Step: 6
Training loss: 2.7150020599365234
Validation loss: 2.4659694497303297

Epoch: 5| Step: 7
Training loss: 2.4696948528289795
Validation loss: 2.474719055237309

Epoch: 5| Step: 8
Training loss: 2.264085054397583
Validation loss: 2.4829477520399195

Epoch: 5| Step: 9
Training loss: 3.1271214485168457
Validation loss: 2.5151525671764086

Epoch: 5| Step: 10
Training loss: 2.482123613357544
Validation loss: 2.5424367996954147

Epoch: 58| Step: 0
Training loss: 2.167048692703247
Validation loss: 2.536267613851896

Epoch: 5| Step: 1
Training loss: 2.899526357650757
Validation loss: 2.5503642930779407

Epoch: 5| Step: 2
Training loss: 2.4488751888275146
Validation loss: 2.516693010125109

Epoch: 5| Step: 3
Training loss: 2.548074245452881
Validation loss: 2.4865457960354385

Epoch: 5| Step: 4
Training loss: 2.0939571857452393
Validation loss: 2.460121923877347

Epoch: 5| Step: 5
Training loss: 3.2212653160095215
Validation loss: 2.461654786140688

Epoch: 5| Step: 6
Training loss: 2.204812526702881
Validation loss: 2.462033610190115

Epoch: 5| Step: 7
Training loss: 2.7209181785583496
Validation loss: 2.4711221853892007

Epoch: 5| Step: 8
Training loss: 2.505396604537964
Validation loss: 2.4745778627293085

Epoch: 5| Step: 9
Training loss: 2.9558074474334717
Validation loss: 2.4797589368717645

Epoch: 5| Step: 10
Training loss: 3.9920196533203125
Validation loss: 2.482608800293297

Epoch: 59| Step: 0
Training loss: 2.707737445831299
Validation loss: 2.470346494387555

Epoch: 5| Step: 1
Training loss: 2.6195483207702637
Validation loss: 2.477580742169452

Epoch: 5| Step: 2
Training loss: 3.0921545028686523
Validation loss: 2.4631530392554497

Epoch: 5| Step: 3
Training loss: 3.056807518005371
Validation loss: 2.469444005720077

Epoch: 5| Step: 4
Training loss: 2.239908218383789
Validation loss: 2.461339791615804

Epoch: 5| Step: 5
Training loss: 1.8051618337631226
Validation loss: 2.459082600890949

Epoch: 5| Step: 6
Training loss: 2.8468143939971924
Validation loss: 2.4620320925148587

Epoch: 5| Step: 7
Training loss: 2.9971814155578613
Validation loss: 2.4655972988374772

Epoch: 5| Step: 8
Training loss: 2.550645589828491
Validation loss: 2.4670598814564366

Epoch: 5| Step: 9
Training loss: 3.2287192344665527
Validation loss: 2.4705383751982

Epoch: 5| Step: 10
Training loss: 2.2857837677001953
Validation loss: 2.4738406776100077

Epoch: 60| Step: 0
Training loss: 2.139655590057373
Validation loss: 2.4735296516008276

Epoch: 5| Step: 1
Training loss: 3.722433567047119
Validation loss: 2.4830682098224597

Epoch: 5| Step: 2
Training loss: 2.6438236236572266
Validation loss: 2.471992249129921

Epoch: 5| Step: 3
Training loss: 2.6462764739990234
Validation loss: 2.4686002192958707

Epoch: 5| Step: 4
Training loss: 1.6599289178848267
Validation loss: 2.463176001784622

Epoch: 5| Step: 5
Training loss: 3.527531385421753
Validation loss: 2.4721018806580575

Epoch: 5| Step: 6
Training loss: 2.3506054878234863
Validation loss: 2.4830804870974634

Epoch: 5| Step: 7
Training loss: 3.028736114501953
Validation loss: 2.501757747383528

Epoch: 5| Step: 8
Training loss: 3.015817880630493
Validation loss: 2.492199536292784

Epoch: 5| Step: 9
Training loss: 2.352433443069458
Validation loss: 2.482569493273253

Epoch: 5| Step: 10
Training loss: 2.4626882076263428
Validation loss: 2.47004977092948

Epoch: 61| Step: 0
Training loss: 2.7128827571868896
Validation loss: 2.458917283242749

Epoch: 5| Step: 1
Training loss: 2.150822877883911
Validation loss: 2.4538167086980676

Epoch: 5| Step: 2
Training loss: 2.1522419452667236
Validation loss: 2.454235984433082

Epoch: 5| Step: 3
Training loss: 3.0912418365478516
Validation loss: 2.4505051130889566

Epoch: 5| Step: 4
Training loss: 2.724307060241699
Validation loss: 2.4564742965082966

Epoch: 5| Step: 5
Training loss: 3.372065782546997
Validation loss: 2.453964259034844

Epoch: 5| Step: 6
Training loss: 2.7247042655944824
Validation loss: 2.455115282407371

Epoch: 5| Step: 7
Training loss: 1.885570764541626
Validation loss: 2.4621989316837762

Epoch: 5| Step: 8
Training loss: 2.94673490524292
Validation loss: 2.464021849375899

Epoch: 5| Step: 9
Training loss: 2.9858438968658447
Validation loss: 2.468207718223654

Epoch: 5| Step: 10
Training loss: 2.719897508621216
Validation loss: 2.4632376445237028

Epoch: 62| Step: 0
Training loss: 3.1046390533447266
Validation loss: 2.4561609657861854

Epoch: 5| Step: 1
Training loss: 2.9655938148498535
Validation loss: 2.4486019944631927

Epoch: 5| Step: 2
Training loss: 2.643455743789673
Validation loss: 2.450763674192531

Epoch: 5| Step: 3
Training loss: 2.851233959197998
Validation loss: 2.453065655564749

Epoch: 5| Step: 4
Training loss: 2.328256607055664
Validation loss: 2.458298308874971

Epoch: 5| Step: 5
Training loss: 2.4346749782562256
Validation loss: 2.46532122037744

Epoch: 5| Step: 6
Training loss: 2.7972564697265625
Validation loss: 2.464450208089685

Epoch: 5| Step: 7
Training loss: 2.4483635425567627
Validation loss: 2.4718775723570134

Epoch: 5| Step: 8
Training loss: 2.3948111534118652
Validation loss: 2.470833589953761

Epoch: 5| Step: 9
Training loss: 2.8404715061187744
Validation loss: 2.4726724752815823

Epoch: 5| Step: 10
Training loss: 2.6558120250701904
Validation loss: 2.47357117232456

Epoch: 63| Step: 0
Training loss: 2.741197109222412
Validation loss: 2.4736450769568004

Epoch: 5| Step: 1
Training loss: 1.5545486211776733
Validation loss: 2.460201822301393

Epoch: 5| Step: 2
Training loss: 2.787139415740967
Validation loss: 2.4726673505639516

Epoch: 5| Step: 3
Training loss: 2.4816575050354004
Validation loss: 2.490362967214277

Epoch: 5| Step: 4
Training loss: 2.734435796737671
Validation loss: 2.483903806696656

Epoch: 5| Step: 5
Training loss: 2.7257840633392334
Validation loss: 2.4770219069655224

Epoch: 5| Step: 6
Training loss: 2.8916842937469482
Validation loss: 2.4759218385142665

Epoch: 5| Step: 7
Training loss: 2.592052459716797
Validation loss: 2.4758148808633127

Epoch: 5| Step: 8
Training loss: 3.4930198192596436
Validation loss: 2.475472811729677

Epoch: 5| Step: 9
Training loss: 2.8320279121398926
Validation loss: 2.4816685748356644

Epoch: 5| Step: 10
Training loss: 2.5243678092956543
Validation loss: 2.4776335044573714

Epoch: 64| Step: 0
Training loss: 2.77736234664917
Validation loss: 2.4748221981909966

Epoch: 5| Step: 1
Training loss: 2.5052905082702637
Validation loss: 2.4778110878441924

Epoch: 5| Step: 2
Training loss: 1.860518217086792
Validation loss: 2.4879923994823168

Epoch: 5| Step: 3
Training loss: 2.919318437576294
Validation loss: 2.497131606583954

Epoch: 5| Step: 4
Training loss: 2.291538715362549
Validation loss: 2.492827687212216

Epoch: 5| Step: 5
Training loss: 2.9303157329559326
Validation loss: 2.490797968320949

Epoch: 5| Step: 6
Training loss: 2.894482135772705
Validation loss: 2.4674085391465055

Epoch: 5| Step: 7
Training loss: 2.6006481647491455
Validation loss: 2.4622544550126597

Epoch: 5| Step: 8
Training loss: 2.795159101486206
Validation loss: 2.455745353493639

Epoch: 5| Step: 9
Training loss: 3.173896312713623
Validation loss: 2.4494036653990388

Epoch: 5| Step: 10
Training loss: 2.571251630783081
Validation loss: 2.4467178185780845

Epoch: 65| Step: 0
Training loss: 3.0907013416290283
Validation loss: 2.4499639311144428

Epoch: 5| Step: 1
Training loss: 2.6637606620788574
Validation loss: 2.4497507900320072

Epoch: 5| Step: 2
Training loss: 2.2891039848327637
Validation loss: 2.445323500581967

Epoch: 5| Step: 3
Training loss: 2.6890010833740234
Validation loss: 2.4494318551914667

Epoch: 5| Step: 4
Training loss: 2.1209211349487305
Validation loss: 2.4521383829014276

Epoch: 5| Step: 5
Training loss: 2.7446341514587402
Validation loss: 2.4447580986125494

Epoch: 5| Step: 6
Training loss: 2.3357112407684326
Validation loss: 2.447164963650447

Epoch: 5| Step: 7
Training loss: 2.8277530670166016
Validation loss: 2.452521280575824

Epoch: 5| Step: 8
Training loss: 2.5387332439422607
Validation loss: 2.4596173763275146

Epoch: 5| Step: 9
Training loss: 2.925924062728882
Validation loss: 2.468274759989913

Epoch: 5| Step: 10
Training loss: 3.203078031539917
Validation loss: 2.485168421140281

Epoch: 66| Step: 0
Training loss: 2.874484062194824
Validation loss: 2.4847069606986096

Epoch: 5| Step: 1
Training loss: 2.6134438514709473
Validation loss: 2.491865070917273

Epoch: 5| Step: 2
Training loss: 2.4682674407958984
Validation loss: 2.479199776085474

Epoch: 5| Step: 3
Training loss: 2.4042088985443115
Validation loss: 2.462105915110598

Epoch: 5| Step: 4
Training loss: 2.947125196456909
Validation loss: 2.455275299728558

Epoch: 5| Step: 5
Training loss: 2.7301015853881836
Validation loss: 2.4566538103165163

Epoch: 5| Step: 6
Training loss: 2.3671340942382812
Validation loss: 2.463812453772432

Epoch: 5| Step: 7
Training loss: 3.672602891921997
Validation loss: 2.4595586253750708

Epoch: 5| Step: 8
Training loss: 2.0673811435699463
Validation loss: 2.4535275300343833

Epoch: 5| Step: 9
Training loss: 2.639829635620117
Validation loss: 2.455490514796267

Epoch: 5| Step: 10
Training loss: 2.4965248107910156
Validation loss: 2.4519297051173385

Epoch: 67| Step: 0
Training loss: 2.7971243858337402
Validation loss: 2.453952407324186

Epoch: 5| Step: 1
Training loss: 2.836026430130005
Validation loss: 2.4499906109225367

Epoch: 5| Step: 2
Training loss: 2.400622606277466
Validation loss: 2.4433252965250323

Epoch: 5| Step: 3
Training loss: 3.056684732437134
Validation loss: 2.444802550859349

Epoch: 5| Step: 4
Training loss: 2.701802968978882
Validation loss: 2.4392865396315053

Epoch: 5| Step: 5
Training loss: 2.6828041076660156
Validation loss: 2.43830878503861

Epoch: 5| Step: 6
Training loss: 2.8076610565185547
Validation loss: 2.437014167026807

Epoch: 5| Step: 7
Training loss: 2.7838187217712402
Validation loss: 2.439462302833475

Epoch: 5| Step: 8
Training loss: 2.6557703018188477
Validation loss: 2.4375955981592976

Epoch: 5| Step: 9
Training loss: 2.3515191078186035
Validation loss: 2.441888065748317

Epoch: 5| Step: 10
Training loss: 2.018845796585083
Validation loss: 2.4457539076446206

Epoch: 68| Step: 0
Training loss: 2.655665874481201
Validation loss: 2.4500505744770007

Epoch: 5| Step: 1
Training loss: 2.680562973022461
Validation loss: 2.452632406706451

Epoch: 5| Step: 2
Training loss: 2.5876126289367676
Validation loss: 2.4602236850287325

Epoch: 5| Step: 3
Training loss: 2.6316230297088623
Validation loss: 2.4640056061488327

Epoch: 5| Step: 4
Training loss: 1.9759118556976318
Validation loss: 2.464079867127121

Epoch: 5| Step: 5
Training loss: 2.913289785385132
Validation loss: 2.469388856682726

Epoch: 5| Step: 6
Training loss: 3.45166277885437
Validation loss: 2.4534013707150697

Epoch: 5| Step: 7
Training loss: 2.876430034637451
Validation loss: 2.4476878220035183

Epoch: 5| Step: 8
Training loss: 1.7945706844329834
Validation loss: 2.449085307377641

Epoch: 5| Step: 9
Training loss: 2.7994046211242676
Validation loss: 2.451756008209721

Epoch: 5| Step: 10
Training loss: 2.850634813308716
Validation loss: 2.4415101235912693

Epoch: 69| Step: 0
Training loss: 2.1447196006774902
Validation loss: 2.435855168168263

Epoch: 5| Step: 1
Training loss: 2.3304176330566406
Validation loss: 2.43143525174869

Epoch: 5| Step: 2
Training loss: 2.329653739929199
Validation loss: 2.4297016154053392

Epoch: 5| Step: 3
Training loss: 2.645228147506714
Validation loss: 2.4334645168755644

Epoch: 5| Step: 4
Training loss: 2.496352195739746
Validation loss: 2.439544952043923

Epoch: 5| Step: 5
Training loss: 2.8264567852020264
Validation loss: 2.4414990512273644

Epoch: 5| Step: 6
Training loss: 2.6607794761657715
Validation loss: 2.436809027066795

Epoch: 5| Step: 7
Training loss: 2.659109115600586
Validation loss: 2.4395264092312066

Epoch: 5| Step: 8
Training loss: 3.250770092010498
Validation loss: 2.435806300050469

Epoch: 5| Step: 9
Training loss: 2.888828992843628
Validation loss: 2.43416343709474

Epoch: 5| Step: 10
Training loss: 3.02984356880188
Validation loss: 2.441952092673189

Epoch: 70| Step: 0
Training loss: 2.4889097213745117
Validation loss: 2.4323603876175417

Epoch: 5| Step: 1
Training loss: 3.031594753265381
Validation loss: 2.428576333548433

Epoch: 5| Step: 2
Training loss: 3.033266067504883
Validation loss: 2.426977170410977

Epoch: 5| Step: 3
Training loss: 2.98500394821167
Validation loss: 2.423967412723008

Epoch: 5| Step: 4
Training loss: 2.411485195159912
Validation loss: 2.4287344742846746

Epoch: 5| Step: 5
Training loss: 2.70332670211792
Validation loss: 2.425374528413178

Epoch: 5| Step: 6
Training loss: 1.8996868133544922
Validation loss: 2.434483746046661

Epoch: 5| Step: 7
Training loss: 2.3160133361816406
Validation loss: 2.431886655028148

Epoch: 5| Step: 8
Training loss: 2.8269741535186768
Validation loss: 2.4283349296098113

Epoch: 5| Step: 9
Training loss: 2.869548797607422
Validation loss: 2.429934306811261

Epoch: 5| Step: 10
Training loss: 2.5847690105438232
Validation loss: 2.4229668007102063

Epoch: 71| Step: 0
Training loss: 2.1542084217071533
Validation loss: 2.418577309577696

Epoch: 5| Step: 1
Training loss: 2.565037727355957
Validation loss: 2.4182199483276694

Epoch: 5| Step: 2
Training loss: 2.5952281951904297
Validation loss: 2.4161103387032785

Epoch: 5| Step: 3
Training loss: 2.1724843978881836
Validation loss: 2.417807420094808

Epoch: 5| Step: 4
Training loss: 2.9364778995513916
Validation loss: 2.4183077632739978

Epoch: 5| Step: 5
Training loss: 2.4300098419189453
Validation loss: 2.419405488557713

Epoch: 5| Step: 6
Training loss: 2.5685441493988037
Validation loss: 2.4164470318825013

Epoch: 5| Step: 7
Training loss: 3.1619508266448975
Validation loss: 2.4142590325365783

Epoch: 5| Step: 8
Training loss: 3.438692808151245
Validation loss: 2.415207180925595

Epoch: 5| Step: 9
Training loss: 2.5795297622680664
Validation loss: 2.4159680540843675

Epoch: 5| Step: 10
Training loss: 2.597249984741211
Validation loss: 2.4158155841212117

Epoch: 72| Step: 0
Training loss: 2.7360219955444336
Validation loss: 2.4184311307886595

Epoch: 5| Step: 1
Training loss: 2.220775842666626
Validation loss: 2.4186791194382535

Epoch: 5| Step: 2
Training loss: 3.4075114727020264
Validation loss: 2.4306352343610538

Epoch: 5| Step: 3
Training loss: 2.770050048828125
Validation loss: 2.433166150123842

Epoch: 5| Step: 4
Training loss: 2.618593454360962
Validation loss: 2.437840889858943

Epoch: 5| Step: 5
Training loss: 2.8250224590301514
Validation loss: 2.4350970432322514

Epoch: 5| Step: 6
Training loss: 2.7268075942993164
Validation loss: 2.432710570673789

Epoch: 5| Step: 7
Training loss: 2.099515438079834
Validation loss: 2.426585228212418

Epoch: 5| Step: 8
Training loss: 2.0780084133148193
Validation loss: 2.4211880571098736

Epoch: 5| Step: 9
Training loss: 2.7653918266296387
Validation loss: 2.412641804705384

Epoch: 5| Step: 10
Training loss: 2.917649745941162
Validation loss: 2.416391485480852

Epoch: 73| Step: 0
Training loss: 2.273376941680908
Validation loss: 2.4134877958605365

Epoch: 5| Step: 1
Training loss: 2.311525821685791
Validation loss: 2.4202660463189565

Epoch: 5| Step: 2
Training loss: 3.0051803588867188
Validation loss: 2.430758350638933

Epoch: 5| Step: 3
Training loss: 2.8573110103607178
Validation loss: 2.4269121616117415

Epoch: 5| Step: 4
Training loss: 3.3470332622528076
Validation loss: 2.442495330687492

Epoch: 5| Step: 5
Training loss: 2.477142333984375
Validation loss: 2.4382899807345484

Epoch: 5| Step: 6
Training loss: 3.0630040168762207
Validation loss: 2.4261484799846524

Epoch: 5| Step: 7
Training loss: 2.7606616020202637
Validation loss: 2.4198004122703307

Epoch: 5| Step: 8
Training loss: 2.1352968215942383
Validation loss: 2.418107860831804

Epoch: 5| Step: 9
Training loss: 2.5189852714538574
Validation loss: 2.4188308715820312

Epoch: 5| Step: 10
Training loss: 2.2674460411071777
Validation loss: 2.4172181160219255

Epoch: 74| Step: 0
Training loss: 2.6808090209960938
Validation loss: 2.4239478829086467

Epoch: 5| Step: 1
Training loss: 2.6075050830841064
Validation loss: 2.430160435297156

Epoch: 5| Step: 2
Training loss: 2.62933611869812
Validation loss: 2.4365411368749474

Epoch: 5| Step: 3
Training loss: 2.5632166862487793
Validation loss: 2.4311263663794405

Epoch: 5| Step: 4
Training loss: 2.754507303237915
Validation loss: 2.440980775381929

Epoch: 5| Step: 5
Training loss: 2.488962173461914
Validation loss: 2.443490389854677

Epoch: 5| Step: 6
Training loss: 3.1958305835723877
Validation loss: 2.4593793576763523

Epoch: 5| Step: 7
Training loss: 2.5674033164978027
Validation loss: 2.4540070872153006

Epoch: 5| Step: 8
Training loss: 2.0595860481262207
Validation loss: 2.4486570947913715

Epoch: 5| Step: 9
Training loss: 2.9687304496765137
Validation loss: 2.4301251980566208

Epoch: 5| Step: 10
Training loss: 2.55309796333313
Validation loss: 2.4221402137510237

Epoch: 75| Step: 0
Training loss: 2.7473907470703125
Validation loss: 2.4168816920249694

Epoch: 5| Step: 1
Training loss: 2.383143186569214
Validation loss: 2.42070335213856

Epoch: 5| Step: 2
Training loss: 2.422795057296753
Validation loss: 2.4226072962566088

Epoch: 5| Step: 3
Training loss: 3.2345192432403564
Validation loss: 2.418727033881731

Epoch: 5| Step: 4
Training loss: 2.4238216876983643
Validation loss: 2.4104559216448056

Epoch: 5| Step: 5
Training loss: 2.726109027862549
Validation loss: 2.412839674180554

Epoch: 5| Step: 6
Training loss: 3.0948498249053955
Validation loss: 2.4140789380637546

Epoch: 5| Step: 7
Training loss: 2.2223095893859863
Validation loss: 2.414003288874062

Epoch: 5| Step: 8
Training loss: 2.7642054557800293
Validation loss: 2.4176769179682576

Epoch: 5| Step: 9
Training loss: 2.1666955947875977
Validation loss: 2.414710861380382

Epoch: 5| Step: 10
Training loss: 3.0623910427093506
Validation loss: 2.4173057053678777

Epoch: 76| Step: 0
Training loss: 2.0097851753234863
Validation loss: 2.4217031360954366

Epoch: 5| Step: 1
Training loss: 3.0032196044921875
Validation loss: 2.4343752168839976

Epoch: 5| Step: 2
Training loss: 2.492276668548584
Validation loss: 2.461187183216054

Epoch: 5| Step: 3
Training loss: 2.9111335277557373
Validation loss: 2.460038305610739

Epoch: 5| Step: 4
Training loss: 2.7267868518829346
Validation loss: 2.4790714171624955

Epoch: 5| Step: 5
Training loss: 2.4792613983154297
Validation loss: 2.488100346698556

Epoch: 5| Step: 6
Training loss: 2.6262729167938232
Validation loss: 2.51259974510439

Epoch: 5| Step: 7
Training loss: 2.8998005390167236
Validation loss: 2.504029340641473

Epoch: 5| Step: 8
Training loss: 2.8484530448913574
Validation loss: 2.4705378124790807

Epoch: 5| Step: 9
Training loss: 2.3063464164733887
Validation loss: 2.4393241277305027

Epoch: 5| Step: 10
Training loss: 3.0180044174194336
Validation loss: 2.41419118706898

Epoch: 77| Step: 0
Training loss: 2.385197162628174
Validation loss: 2.40393433519589

Epoch: 5| Step: 1
Training loss: 2.419450283050537
Validation loss: 2.399239114535752

Epoch: 5| Step: 2
Training loss: 2.086571455001831
Validation loss: 2.4029257169333835

Epoch: 5| Step: 3
Training loss: 2.8403265476226807
Validation loss: 2.407659251202819

Epoch: 5| Step: 4
Training loss: 2.081413507461548
Validation loss: 2.4089754294323664

Epoch: 5| Step: 5
Training loss: 2.974921703338623
Validation loss: 2.412840889346215

Epoch: 5| Step: 6
Training loss: 2.753469705581665
Validation loss: 2.409042627580704

Epoch: 5| Step: 7
Training loss: 2.2879281044006348
Validation loss: 2.4039615149139077

Epoch: 5| Step: 8
Training loss: 2.741628646850586
Validation loss: 2.403821168407317

Epoch: 5| Step: 9
Training loss: 3.224332094192505
Validation loss: 2.3977267639611357

Epoch: 5| Step: 10
Training loss: 3.5148375034332275
Validation loss: 2.3967738856551466

Epoch: 78| Step: 0
Training loss: 2.9330878257751465
Validation loss: 2.3990922435637443

Epoch: 5| Step: 1
Training loss: 2.8641960620880127
Validation loss: 2.4114398315388668

Epoch: 5| Step: 2
Training loss: 3.2008426189422607
Validation loss: 2.4077020409286662

Epoch: 5| Step: 3
Training loss: 2.4140052795410156
Validation loss: 2.419622762228853

Epoch: 5| Step: 4
Training loss: 2.902902126312256
Validation loss: 2.416907287413074

Epoch: 5| Step: 5
Training loss: 2.479341983795166
Validation loss: 2.4183937144535843

Epoch: 5| Step: 6
Training loss: 3.0529253482818604
Validation loss: 2.4130788515972834

Epoch: 5| Step: 7
Training loss: 2.5049140453338623
Validation loss: 2.4084860829896826

Epoch: 5| Step: 8
Training loss: 2.413079023361206
Validation loss: 2.403179840375018

Epoch: 5| Step: 9
Training loss: 2.3340442180633545
Validation loss: 2.397016299668179

Epoch: 5| Step: 10
Training loss: 1.868332862854004
Validation loss: 2.4002564184127317

Epoch: 79| Step: 0
Training loss: 2.600135326385498
Validation loss: 2.3963410803066787

Epoch: 5| Step: 1
Training loss: 2.297893524169922
Validation loss: 2.3928440052975892

Epoch: 5| Step: 2
Training loss: 2.448817729949951
Validation loss: 2.3952613569075063

Epoch: 5| Step: 3
Training loss: 3.319063186645508
Validation loss: 2.3933390468679447

Epoch: 5| Step: 4
Training loss: 2.3381175994873047
Validation loss: 2.3927932221402406

Epoch: 5| Step: 5
Training loss: 2.407892942428589
Validation loss: 2.3945219696208997

Epoch: 5| Step: 6
Training loss: 2.7995529174804688
Validation loss: 2.3902912729529926

Epoch: 5| Step: 7
Training loss: 2.3700766563415527
Validation loss: 2.389826225978072

Epoch: 5| Step: 8
Training loss: 3.0222198963165283
Validation loss: 2.394242981428741

Epoch: 5| Step: 9
Training loss: 2.9371325969696045
Validation loss: 2.392481355256932

Epoch: 5| Step: 10
Training loss: 2.4767563343048096
Validation loss: 2.39087591889084

Epoch: 80| Step: 0
Training loss: 2.737306594848633
Validation loss: 2.394236315963089

Epoch: 5| Step: 1
Training loss: 2.545210599899292
Validation loss: 2.393375107037124

Epoch: 5| Step: 2
Training loss: 2.6281256675720215
Validation loss: 2.405692564543857

Epoch: 5| Step: 3
Training loss: 2.3981618881225586
Validation loss: 2.412476119174752

Epoch: 5| Step: 4
Training loss: 2.3841006755828857
Validation loss: 2.4322607927424933

Epoch: 5| Step: 5
Training loss: 2.6757209300994873
Validation loss: 2.4479418672541136

Epoch: 5| Step: 6
Training loss: 3.099876880645752
Validation loss: 2.4606557482032367

Epoch: 5| Step: 7
Training loss: 2.8225150108337402
Validation loss: 2.465890968999555

Epoch: 5| Step: 8
Training loss: 2.5286917686462402
Validation loss: 2.462438214209772

Epoch: 5| Step: 9
Training loss: 2.341142177581787
Validation loss: 2.4537401455704884

Epoch: 5| Step: 10
Training loss: 2.997129440307617
Validation loss: 2.4215510481147358

Epoch: 81| Step: 0
Training loss: 3.080669403076172
Validation loss: 2.392104297555903

Epoch: 5| Step: 1
Training loss: 2.9388723373413086
Validation loss: 2.3862571921399844

Epoch: 5| Step: 2
Training loss: 3.059945821762085
Validation loss: 2.3877557862189507

Epoch: 5| Step: 3
Training loss: 2.8421428203582764
Validation loss: 2.384053450758739

Epoch: 5| Step: 4
Training loss: 3.0293374061584473
Validation loss: 2.3871540587435485

Epoch: 5| Step: 5
Training loss: 2.306032657623291
Validation loss: 2.3824354756262993

Epoch: 5| Step: 6
Training loss: 1.521300196647644
Validation loss: 2.38816556110177

Epoch: 5| Step: 7
Training loss: 2.891331911087036
Validation loss: 2.386642248399796

Epoch: 5| Step: 8
Training loss: 2.9805643558502197
Validation loss: 2.385092194362353

Epoch: 5| Step: 9
Training loss: 2.043933153152466
Validation loss: 2.3882223995782996

Epoch: 5| Step: 10
Training loss: 2.2506916522979736
Validation loss: 2.3891684009182836

Epoch: 82| Step: 0
Training loss: 3.3604583740234375
Validation loss: 2.3956023903303247

Epoch: 5| Step: 1
Training loss: 2.8809151649475098
Validation loss: 2.3968191505760275

Epoch: 5| Step: 2
Training loss: 2.8903422355651855
Validation loss: 2.412483484514298

Epoch: 5| Step: 3
Training loss: 2.6982693672180176
Validation loss: 2.4154599123103644

Epoch: 5| Step: 4
Training loss: 2.285811185836792
Validation loss: 2.4319626900457565

Epoch: 5| Step: 5
Training loss: 2.5523695945739746
Validation loss: 2.4307056242419827

Epoch: 5| Step: 6
Training loss: 2.803391218185425
Validation loss: 2.431206621149535

Epoch: 5| Step: 7
Training loss: 2.5170681476593018
Validation loss: 2.428173731732112

Epoch: 5| Step: 8
Training loss: 2.391425371170044
Validation loss: 2.395374858251182

Epoch: 5| Step: 9
Training loss: 2.1292290687561035
Validation loss: 2.388124153178225

Epoch: 5| Step: 10
Training loss: 2.37349534034729
Validation loss: 2.3840577807477725

Epoch: 83| Step: 0
Training loss: 2.0874836444854736
Validation loss: 2.3907761983974005

Epoch: 5| Step: 1
Training loss: 2.8568153381347656
Validation loss: 2.3895829800636537

Epoch: 5| Step: 2
Training loss: 2.5130867958068848
Validation loss: 2.39886369628291

Epoch: 5| Step: 3
Training loss: 1.8333162069320679
Validation loss: 2.416579169611777

Epoch: 5| Step: 4
Training loss: 2.662799835205078
Validation loss: 2.427006116477392

Epoch: 5| Step: 5
Training loss: 2.76621675491333
Validation loss: 2.421667445090509

Epoch: 5| Step: 6
Training loss: 2.7031447887420654
Validation loss: 2.4069784469501947

Epoch: 5| Step: 7
Training loss: 3.7648403644561768
Validation loss: 2.4028177004988476

Epoch: 5| Step: 8
Training loss: 2.6983871459960938
Validation loss: 2.391631631441014

Epoch: 5| Step: 9
Training loss: 2.263312816619873
Validation loss: 2.3962434799440446

Epoch: 5| Step: 10
Training loss: 2.867886543273926
Validation loss: 2.3897402389075166

Epoch: 84| Step: 0
Training loss: 1.9861714839935303
Validation loss: 2.3894637118103685

Epoch: 5| Step: 1
Training loss: 2.4686367511749268
Validation loss: 2.3828207472319245

Epoch: 5| Step: 2
Training loss: 3.0835537910461426
Validation loss: 2.386512589711015

Epoch: 5| Step: 3
Training loss: 2.4511818885803223
Validation loss: 2.391197878827331

Epoch: 5| Step: 4
Training loss: 3.1238443851470947
Validation loss: 2.387698447832497

Epoch: 5| Step: 5
Training loss: 2.793684244155884
Validation loss: 2.3867504519800984

Epoch: 5| Step: 6
Training loss: 2.8169617652893066
Validation loss: 2.3898160662702335

Epoch: 5| Step: 7
Training loss: 2.3015530109405518
Validation loss: 2.3951412682892173

Epoch: 5| Step: 8
Training loss: 2.884446620941162
Validation loss: 2.39103175235051

Epoch: 5| Step: 9
Training loss: 2.7813491821289062
Validation loss: 2.400966836560157

Epoch: 5| Step: 10
Training loss: 2.13238263130188
Validation loss: 2.403506427682856

Epoch: 85| Step: 0
Training loss: 2.754744052886963
Validation loss: 2.4016968357947563

Epoch: 5| Step: 1
Training loss: 2.4286582469940186
Validation loss: 2.40819207314522

Epoch: 5| Step: 2
Training loss: 2.7183775901794434
Validation loss: 2.4008060655286236

Epoch: 5| Step: 3
Training loss: 2.528122663497925
Validation loss: 2.3967101907217376

Epoch: 5| Step: 4
Training loss: 2.552281618118286
Validation loss: 2.3920267320448354

Epoch: 5| Step: 5
Training loss: 2.625159740447998
Validation loss: 2.392644448946881

Epoch: 5| Step: 6
Training loss: 2.7984859943389893
Validation loss: 2.393607706151983

Epoch: 5| Step: 7
Training loss: 2.8370234966278076
Validation loss: 2.394262126697007

Epoch: 5| Step: 8
Training loss: 2.149392604827881
Validation loss: 2.379000897048622

Epoch: 5| Step: 9
Training loss: 2.616016387939453
Validation loss: 2.3794477216659056

Epoch: 5| Step: 10
Training loss: 2.8706579208374023
Validation loss: 2.3782088577106433

Epoch: 86| Step: 0
Training loss: 2.8200900554656982
Validation loss: 2.3784945728958293

Epoch: 5| Step: 1
Training loss: 1.7348819971084595
Validation loss: 2.381119474287956

Epoch: 5| Step: 2
Training loss: 1.8467267751693726
Validation loss: 2.3798357696943384

Epoch: 5| Step: 3
Training loss: 2.802232503890991
Validation loss: 2.375547880767494

Epoch: 5| Step: 4
Training loss: 2.234884262084961
Validation loss: 2.3780587821878414

Epoch: 5| Step: 5
Training loss: 2.606153964996338
Validation loss: 2.379288045308923

Epoch: 5| Step: 6
Training loss: 3.2033143043518066
Validation loss: 2.372472316988053

Epoch: 5| Step: 7
Training loss: 2.7957353591918945
Validation loss: 2.3769614696502686

Epoch: 5| Step: 8
Training loss: 2.9676058292388916
Validation loss: 2.3774683834404073

Epoch: 5| Step: 9
Training loss: 2.9603281021118164
Validation loss: 2.3744556006564888

Epoch: 5| Step: 10
Training loss: 2.851130962371826
Validation loss: 2.375481940084888

Epoch: 87| Step: 0
Training loss: 2.440633535385132
Validation loss: 2.3869889449047785

Epoch: 5| Step: 1
Training loss: 2.495368480682373
Validation loss: 2.3882544463680637

Epoch: 5| Step: 2
Training loss: 2.262164354324341
Validation loss: 2.3850366684698288

Epoch: 5| Step: 3
Training loss: 2.6648097038269043
Validation loss: 2.380640773363011

Epoch: 5| Step: 4
Training loss: 3.3560950756073
Validation loss: 2.3738468411148235

Epoch: 5| Step: 5
Training loss: 2.797764539718628
Validation loss: 2.3646561843092724

Epoch: 5| Step: 6
Training loss: 2.5175445079803467
Validation loss: 2.3688753804852887

Epoch: 5| Step: 7
Training loss: 2.4854345321655273
Validation loss: 2.370062658863683

Epoch: 5| Step: 8
Training loss: 2.6150431632995605
Validation loss: 2.3683117153824016

Epoch: 5| Step: 9
Training loss: 2.673043727874756
Validation loss: 2.370548971237675

Epoch: 5| Step: 10
Training loss: 2.5250117778778076
Validation loss: 2.3728351900654454

Epoch: 88| Step: 0
Training loss: 2.504241943359375
Validation loss: 2.38731066642269

Epoch: 5| Step: 1
Training loss: 2.839395523071289
Validation loss: 2.397068251845657

Epoch: 5| Step: 2
Training loss: 2.9545414447784424
Validation loss: 2.398952143166655

Epoch: 5| Step: 3
Training loss: 2.5431511402130127
Validation loss: 2.408016925217003

Epoch: 5| Step: 4
Training loss: 1.8824520111083984
Validation loss: 2.4030857265636487

Epoch: 5| Step: 5
Training loss: 2.3351492881774902
Validation loss: 2.393908785235497

Epoch: 5| Step: 6
Training loss: 2.122154474258423
Validation loss: 2.3987642411262757

Epoch: 5| Step: 7
Training loss: 3.0212082862854004
Validation loss: 2.3919139190386702

Epoch: 5| Step: 8
Training loss: 3.3969085216522217
Validation loss: 2.3795119485547467

Epoch: 5| Step: 9
Training loss: 2.458198308944702
Validation loss: 2.3892558210639545

Epoch: 5| Step: 10
Training loss: 2.7652413845062256
Validation loss: 2.389040744432839

Epoch: 89| Step: 0
Training loss: 2.119508981704712
Validation loss: 2.383323520742437

Epoch: 5| Step: 1
Training loss: 2.4085350036621094
Validation loss: 2.3891719015695716

Epoch: 5| Step: 2
Training loss: 2.2485358715057373
Validation loss: 2.3803822135412567

Epoch: 5| Step: 3
Training loss: 2.238882541656494
Validation loss: 2.386705408814133

Epoch: 5| Step: 4
Training loss: 2.609966278076172
Validation loss: 2.3868143866139073

Epoch: 5| Step: 5
Training loss: 3.0807738304138184
Validation loss: 2.3798720067547214

Epoch: 5| Step: 6
Training loss: 2.199537992477417
Validation loss: 2.3803353873632287

Epoch: 5| Step: 7
Training loss: 2.553368330001831
Validation loss: 2.374432566345379

Epoch: 5| Step: 8
Training loss: 2.7975776195526123
Validation loss: 2.377818525478404

Epoch: 5| Step: 9
Training loss: 3.0875959396362305
Validation loss: 2.377340216790476

Epoch: 5| Step: 10
Training loss: 3.4752907752990723
Validation loss: 2.3767689594658474

Epoch: 90| Step: 0
Training loss: 3.2763333320617676
Validation loss: 2.3677555976375455

Epoch: 5| Step: 1
Training loss: 1.8894093036651611
Validation loss: 2.3680253490324943

Epoch: 5| Step: 2
Training loss: 3.027848720550537
Validation loss: 2.3766184263331915

Epoch: 5| Step: 3
Training loss: 2.8103830814361572
Validation loss: 2.3759242437219106

Epoch: 5| Step: 4
Training loss: 2.6690585613250732
Validation loss: 2.380987508322603

Epoch: 5| Step: 5
Training loss: 2.0183329582214355
Validation loss: 2.385645562602628

Epoch: 5| Step: 6
Training loss: 2.972378969192505
Validation loss: 2.4048903501161965

Epoch: 5| Step: 7
Training loss: 2.6305737495422363
Validation loss: 2.4012664671867125

Epoch: 5| Step: 8
Training loss: 2.27978253364563
Validation loss: 2.395214514065814

Epoch: 5| Step: 9
Training loss: 2.4156908988952637
Validation loss: 2.3991331413228023

Epoch: 5| Step: 10
Training loss: 2.804234027862549
Validation loss: 2.4030717726676696

Epoch: 91| Step: 0
Training loss: 3.0993552207946777
Validation loss: 2.4016238668913483

Epoch: 5| Step: 1
Training loss: 2.7692017555236816
Validation loss: 2.3939175759592364

Epoch: 5| Step: 2
Training loss: 2.6120266914367676
Validation loss: 2.3912448498510543

Epoch: 5| Step: 3
Training loss: 2.796391248703003
Validation loss: 2.3788937061063704

Epoch: 5| Step: 4
Training loss: 2.7216315269470215
Validation loss: 2.3882822067506853

Epoch: 5| Step: 5
Training loss: 2.664170265197754
Validation loss: 2.395809245365922

Epoch: 5| Step: 6
Training loss: 2.2838072776794434
Validation loss: 2.385352998651484

Epoch: 5| Step: 7
Training loss: 2.539775848388672
Validation loss: 2.3945652387475453

Epoch: 5| Step: 8
Training loss: 2.1464309692382812
Validation loss: 2.3895263569329375

Epoch: 5| Step: 9
Training loss: 2.522087574005127
Validation loss: 2.3807686195578626

Epoch: 5| Step: 10
Training loss: 2.5529842376708984
Validation loss: 2.375880615685576

Epoch: 92| Step: 0
Training loss: 2.4830923080444336
Validation loss: 2.3681060344942155

Epoch: 5| Step: 1
Training loss: 2.5830776691436768
Validation loss: 2.353680593993074

Epoch: 5| Step: 2
Training loss: 2.866636276245117
Validation loss: 2.3596513912241948

Epoch: 5| Step: 3
Training loss: 2.4139742851257324
Validation loss: 2.355544210762106

Epoch: 5| Step: 4
Training loss: 2.594743251800537
Validation loss: 2.3531659136536303

Epoch: 5| Step: 5
Training loss: 2.566072463989258
Validation loss: 2.356975919456892

Epoch: 5| Step: 6
Training loss: 3.084979772567749
Validation loss: 2.35383532380545

Epoch: 5| Step: 7
Training loss: 2.691751003265381
Validation loss: 2.356755337407512

Epoch: 5| Step: 8
Training loss: 2.3873629570007324
Validation loss: 2.368146565652663

Epoch: 5| Step: 9
Training loss: 2.917537212371826
Validation loss: 2.3663985677944717

Epoch: 5| Step: 10
Training loss: 1.9393579959869385
Validation loss: 2.36865726594002

Epoch: 93| Step: 0
Training loss: 2.335141181945801
Validation loss: 2.372622864220732

Epoch: 5| Step: 1
Training loss: 2.861290454864502
Validation loss: 2.3679136819736932

Epoch: 5| Step: 2
Training loss: 2.3472132682800293
Validation loss: 2.3648567071524997

Epoch: 5| Step: 3
Training loss: 2.177685260772705
Validation loss: 2.3610302940491708

Epoch: 5| Step: 4
Training loss: 3.3129220008850098
Validation loss: 2.3626406090233916

Epoch: 5| Step: 5
Training loss: 2.3922715187072754
Validation loss: 2.3580901212589715

Epoch: 5| Step: 6
Training loss: 2.899033308029175
Validation loss: 2.357954240614368

Epoch: 5| Step: 7
Training loss: 2.7465593814849854
Validation loss: 2.363534777395187

Epoch: 5| Step: 8
Training loss: 2.2099902629852295
Validation loss: 2.3705608255119732

Epoch: 5| Step: 9
Training loss: 3.30485200881958
Validation loss: 2.3713293537016837

Epoch: 5| Step: 10
Training loss: 1.9370907545089722
Validation loss: 2.3778809501278784

Epoch: 94| Step: 0
Training loss: 2.669910192489624
Validation loss: 2.3825722048359532

Epoch: 5| Step: 1
Training loss: 1.9302856922149658
Validation loss: 2.3813515658019693

Epoch: 5| Step: 2
Training loss: 2.6255812644958496
Validation loss: 2.377349192096341

Epoch: 5| Step: 3
Training loss: 3.2032389640808105
Validation loss: 2.3652714631890737

Epoch: 5| Step: 4
Training loss: 3.0736215114593506
Validation loss: 2.363016515649775

Epoch: 5| Step: 5
Training loss: 2.6958727836608887
Validation loss: 2.3509670970260457

Epoch: 5| Step: 6
Training loss: 2.585458755493164
Validation loss: 2.3534498483903947

Epoch: 5| Step: 7
Training loss: 1.9139130115509033
Validation loss: 2.3539697739385788

Epoch: 5| Step: 8
Training loss: 2.854440212249756
Validation loss: 2.3565482195987495

Epoch: 5| Step: 9
Training loss: 2.6237268447875977
Validation loss: 2.352946532669888

Epoch: 5| Step: 10
Training loss: 2.41037654876709
Validation loss: 2.347717895302721

Epoch: 95| Step: 0
Training loss: 2.5482337474823
Validation loss: 2.3546168470895417

Epoch: 5| Step: 1
Training loss: 3.179675579071045
Validation loss: 2.3513032979862665

Epoch: 5| Step: 2
Training loss: 2.806710720062256
Validation loss: 2.3572287123690367

Epoch: 5| Step: 3
Training loss: 2.7357258796691895
Validation loss: 2.3473764068336895

Epoch: 5| Step: 4
Training loss: 2.226123094558716
Validation loss: 2.3487965958092802

Epoch: 5| Step: 5
Training loss: 2.471910238265991
Validation loss: 2.3494223010155464

Epoch: 5| Step: 6
Training loss: 2.6408944129943848
Validation loss: 2.3572788905071955

Epoch: 5| Step: 7
Training loss: 2.603498935699463
Validation loss: 2.3523425902089765

Epoch: 5| Step: 8
Training loss: 2.5737464427948
Validation loss: 2.3575178295053463

Epoch: 5| Step: 9
Training loss: 2.3221096992492676
Validation loss: 2.3522267033976894

Epoch: 5| Step: 10
Training loss: 2.3387763500213623
Validation loss: 2.3577300015316216

Epoch: 96| Step: 0
Training loss: 1.905648946762085
Validation loss: 2.34821218316273

Epoch: 5| Step: 1
Training loss: 3.305548906326294
Validation loss: 2.3601145616141697

Epoch: 5| Step: 2
Training loss: 2.327939510345459
Validation loss: 2.3589028901951288

Epoch: 5| Step: 3
Training loss: 2.319528579711914
Validation loss: 2.3582226896798737

Epoch: 5| Step: 4
Training loss: 2.9008383750915527
Validation loss: 2.358169994046611

Epoch: 5| Step: 5
Training loss: 2.869300365447998
Validation loss: 2.35670147531776

Epoch: 5| Step: 6
Training loss: 2.29626727104187
Validation loss: 2.3568699282984578

Epoch: 5| Step: 7
Training loss: 2.5718753337860107
Validation loss: 2.3598029408403622

Epoch: 5| Step: 8
Training loss: 2.733649969100952
Validation loss: 2.3623850422520793

Epoch: 5| Step: 9
Training loss: 3.291215181350708
Validation loss: 2.365810924960721

Epoch: 5| Step: 10
Training loss: 1.8401916027069092
Validation loss: 2.358054445635888

Epoch: 97| Step: 0
Training loss: 2.9467225074768066
Validation loss: 2.353521696982845

Epoch: 5| Step: 1
Training loss: 2.1387622356414795
Validation loss: 2.3595170564548944

Epoch: 5| Step: 2
Training loss: 2.3135478496551514
Validation loss: 2.357263872700353

Epoch: 5| Step: 3
Training loss: 2.4523749351501465
Validation loss: 2.350116933545759

Epoch: 5| Step: 4
Training loss: 2.0948128700256348
Validation loss: 2.3548295010802565

Epoch: 5| Step: 5
Training loss: 2.5295395851135254
Validation loss: 2.3500580659476658

Epoch: 5| Step: 6
Training loss: 3.039094924926758
Validation loss: 2.3552740286755305

Epoch: 5| Step: 7
Training loss: 2.9935085773468018
Validation loss: 2.355798808477258

Epoch: 5| Step: 8
Training loss: 2.588318347930908
Validation loss: 2.346744286116733

Epoch: 5| Step: 9
Training loss: 2.5960209369659424
Validation loss: 2.351825124473982

Epoch: 5| Step: 10
Training loss: 2.836534023284912
Validation loss: 2.352196526783769

Epoch: 98| Step: 0
Training loss: 2.793994188308716
Validation loss: 2.356250827030469

Epoch: 5| Step: 1
Training loss: 2.3019518852233887
Validation loss: 2.3492144999965543

Epoch: 5| Step: 2
Training loss: 2.893709182739258
Validation loss: 2.364474618306724

Epoch: 5| Step: 3
Training loss: 1.8193880319595337
Validation loss: 2.369622677885076

Epoch: 5| Step: 4
Training loss: 2.0073647499084473
Validation loss: 2.370476948317661

Epoch: 5| Step: 5
Training loss: 3.204742431640625
Validation loss: 2.3848774189590127

Epoch: 5| Step: 6
Training loss: 2.817845106124878
Validation loss: 2.3817709107552805

Epoch: 5| Step: 7
Training loss: 2.9934139251708984
Validation loss: 2.374602299864574

Epoch: 5| Step: 8
Training loss: 2.5536465644836426
Validation loss: 2.355240488565096

Epoch: 5| Step: 9
Training loss: 2.4102330207824707
Validation loss: 2.3573347906912527

Epoch: 5| Step: 10
Training loss: 2.732104778289795
Validation loss: 2.3837867885507564

Epoch: 99| Step: 0
Training loss: 2.535238027572632
Validation loss: 2.3797079542631745

Epoch: 5| Step: 1
Training loss: 3.2316832542419434
Validation loss: 2.3649068904179398

Epoch: 5| Step: 2
Training loss: 2.3301782608032227
Validation loss: 2.3536444043600433

Epoch: 5| Step: 3
Training loss: 2.4549431800842285
Validation loss: 2.3367533606867634

Epoch: 5| Step: 4
Training loss: 2.1574740409851074
Validation loss: 2.336117116353845

Epoch: 5| Step: 5
Training loss: 1.6515229940414429
Validation loss: 2.3382504191449893

Epoch: 5| Step: 6
Training loss: 3.13653826713562
Validation loss: 2.342269218096169

Epoch: 5| Step: 7
Training loss: 2.603895664215088
Validation loss: 2.3374729617949455

Epoch: 5| Step: 8
Training loss: 2.8114120960235596
Validation loss: 2.3441632101612706

Epoch: 5| Step: 9
Training loss: 2.4181056022644043
Validation loss: 2.3493253313085085

Epoch: 5| Step: 10
Training loss: 3.481760025024414
Validation loss: 2.3598036330233336

Epoch: 100| Step: 0
Training loss: 3.0484862327575684
Validation loss: 2.3503474779026483

Epoch: 5| Step: 1
Training loss: 2.199845552444458
Validation loss: 2.3516913126873713

Epoch: 5| Step: 2
Training loss: 2.0199475288391113
Validation loss: 2.355014665152437

Epoch: 5| Step: 3
Training loss: 1.8758280277252197
Validation loss: 2.3685509338173816

Epoch: 5| Step: 4
Training loss: 2.985926389694214
Validation loss: 2.3670862105584916

Epoch: 5| Step: 5
Training loss: 2.8826727867126465
Validation loss: 2.3760514131156345

Epoch: 5| Step: 6
Training loss: 2.2580337524414062
Validation loss: 2.3763559249139603

Epoch: 5| Step: 7
Training loss: 3.64753794670105
Validation loss: 2.3725242499382264

Epoch: 5| Step: 8
Training loss: 2.2621774673461914
Validation loss: 2.3561040662950083

Epoch: 5| Step: 9
Training loss: 2.254578113555908
Validation loss: 2.347600511325303

Epoch: 5| Step: 10
Training loss: 3.230952262878418
Validation loss: 2.3385933419709564

Epoch: 101| Step: 0
Training loss: 2.4285998344421387
Validation loss: 2.3335255935627925

Epoch: 5| Step: 1
Training loss: 2.621438503265381
Validation loss: 2.3318149710214264

Epoch: 5| Step: 2
Training loss: 1.6767175197601318
Validation loss: 2.33346273181259

Epoch: 5| Step: 3
Training loss: 2.1137118339538574
Validation loss: 2.3344429590368785

Epoch: 5| Step: 4
Training loss: 3.023674964904785
Validation loss: 2.3310974259530344

Epoch: 5| Step: 5
Training loss: 3.0043857097625732
Validation loss: 2.331546562974171

Epoch: 5| Step: 6
Training loss: 2.8013973236083984
Validation loss: 2.331001515029579

Epoch: 5| Step: 7
Training loss: 2.017193078994751
Validation loss: 2.3341680111423617

Epoch: 5| Step: 8
Training loss: 2.8644073009490967
Validation loss: 2.341949218062944

Epoch: 5| Step: 9
Training loss: 2.926549196243286
Validation loss: 2.3550863804355746

Epoch: 5| Step: 10
Training loss: 3.008680820465088
Validation loss: 2.3667508966179303

Epoch: 102| Step: 0
Training loss: 2.1448354721069336
Validation loss: 2.367573586843347

Epoch: 5| Step: 1
Training loss: 3.2282166481018066
Validation loss: 2.3785803612842353

Epoch: 5| Step: 2
Training loss: 2.0145862102508545
Validation loss: 2.371537995594804

Epoch: 5| Step: 3
Training loss: 3.081568717956543
Validation loss: 2.3566856076640468

Epoch: 5| Step: 4
Training loss: 1.8423312902450562
Validation loss: 2.3357904444458666

Epoch: 5| Step: 5
Training loss: 2.4537060260772705
Validation loss: 2.327791675444572

Epoch: 5| Step: 6
Training loss: 2.277833938598633
Validation loss: 2.328039917894589

Epoch: 5| Step: 7
Training loss: 2.4647021293640137
Validation loss: 2.323881362074165

Epoch: 5| Step: 8
Training loss: 2.959308385848999
Validation loss: 2.3297275074066652

Epoch: 5| Step: 9
Training loss: 3.130310297012329
Validation loss: 2.3290733368166032

Epoch: 5| Step: 10
Training loss: 2.915199041366577
Validation loss: 2.32372780769102

Epoch: 103| Step: 0
Training loss: 2.772982120513916
Validation loss: 2.3243227415187384

Epoch: 5| Step: 1
Training loss: 3.0842337608337402
Validation loss: 2.3261329512442313

Epoch: 5| Step: 2
Training loss: 1.8042538166046143
Validation loss: 2.3251660716149116

Epoch: 5| Step: 3
Training loss: 2.441606044769287
Validation loss: 2.324930501240556

Epoch: 5| Step: 4
Training loss: 3.2925961017608643
Validation loss: 2.3266178369522095

Epoch: 5| Step: 5
Training loss: 2.213250160217285
Validation loss: 2.3303562466816237

Epoch: 5| Step: 6
Training loss: 2.8953239917755127
Validation loss: 2.3336842803544897

Epoch: 5| Step: 7
Training loss: 2.4524478912353516
Validation loss: 2.3332708907383743

Epoch: 5| Step: 8
Training loss: 2.363905429840088
Validation loss: 2.340873815680063

Epoch: 5| Step: 9
Training loss: 2.2683322429656982
Validation loss: 2.3413645477705103

Epoch: 5| Step: 10
Training loss: 2.9029884338378906
Validation loss: 2.350651641045847

Epoch: 104| Step: 0
Training loss: 1.9932262897491455
Validation loss: 2.367101402692897

Epoch: 5| Step: 1
Training loss: 2.824337959289551
Validation loss: 2.373684831844863

Epoch: 5| Step: 2
Training loss: 3.1341168880462646
Validation loss: 2.381161984576974

Epoch: 5| Step: 3
Training loss: 3.542443037033081
Validation loss: 2.3912558094147713

Epoch: 5| Step: 4
Training loss: 3.1685290336608887
Validation loss: 2.3757190550527265

Epoch: 5| Step: 5
Training loss: 2.535703182220459
Validation loss: 2.3531507061373804

Epoch: 5| Step: 6
Training loss: 2.09255051612854
Validation loss: 2.337766908830212

Epoch: 5| Step: 7
Training loss: 1.861130952835083
Validation loss: 2.3340098191333074

Epoch: 5| Step: 8
Training loss: 2.3958077430725098
Validation loss: 2.330222596404373

Epoch: 5| Step: 9
Training loss: 2.7325127124786377
Validation loss: 2.3252112647538543

Epoch: 5| Step: 10
Training loss: 2.060748815536499
Validation loss: 2.322034720451601

Epoch: 105| Step: 0
Training loss: 2.7879440784454346
Validation loss: 2.3144727163417365

Epoch: 5| Step: 1
Training loss: 2.7743782997131348
Validation loss: 2.3113291930126887

Epoch: 5| Step: 2
Training loss: 2.409241199493408
Validation loss: 2.318513383147537

Epoch: 5| Step: 3
Training loss: 1.7806411981582642
Validation loss: 2.3161957353673954

Epoch: 5| Step: 4
Training loss: 2.7155139446258545
Validation loss: 2.3178398404070126

Epoch: 5| Step: 5
Training loss: 2.713841676712036
Validation loss: 2.336620279537734

Epoch: 5| Step: 6
Training loss: 2.7993533611297607
Validation loss: 2.3401871778631724

Epoch: 5| Step: 7
Training loss: 2.9134559631347656
Validation loss: 2.3438100045727146

Epoch: 5| Step: 8
Training loss: 2.7062816619873047
Validation loss: 2.356545291921144

Epoch: 5| Step: 9
Training loss: 2.3311564922332764
Validation loss: 2.3307399903574297

Epoch: 5| Step: 10
Training loss: 2.422102928161621
Validation loss: 2.328399622312156

Epoch: 106| Step: 0
Training loss: 2.2580254077911377
Validation loss: 2.325067609869024

Epoch: 5| Step: 1
Training loss: 2.999166250228882
Validation loss: 2.315989495605551

Epoch: 5| Step: 2
Training loss: 2.5791444778442383
Validation loss: 2.306008836274506

Epoch: 5| Step: 3
Training loss: 2.600255250930786
Validation loss: 2.3127808878498692

Epoch: 5| Step: 4
Training loss: 2.4833192825317383
Validation loss: 2.311141893427859

Epoch: 5| Step: 5
Training loss: 2.3460559844970703
Validation loss: 2.3131602912820797

Epoch: 5| Step: 6
Training loss: 2.7521984577178955
Validation loss: 2.314003775196691

Epoch: 5| Step: 7
Training loss: 2.2097713947296143
Validation loss: 2.3182915308142222

Epoch: 5| Step: 8
Training loss: 3.1294212341308594
Validation loss: 2.3214152448920795

Epoch: 5| Step: 9
Training loss: 2.4575984477996826
Validation loss: 2.323483549138551

Epoch: 5| Step: 10
Training loss: 2.553579092025757
Validation loss: 2.330494696094144

Epoch: 107| Step: 0
Training loss: 2.643301486968994
Validation loss: 2.3303392958897415

Epoch: 5| Step: 1
Training loss: 3.192084550857544
Validation loss: 2.351012886211436

Epoch: 5| Step: 2
Training loss: 2.6043577194213867
Validation loss: 2.3651436272487847

Epoch: 5| Step: 3
Training loss: 2.479144334793091
Validation loss: 2.3780506451924643

Epoch: 5| Step: 4
Training loss: 2.551403760910034
Validation loss: 2.386841409949846

Epoch: 5| Step: 5
Training loss: 1.7389169931411743
Validation loss: 2.3789097621876705

Epoch: 5| Step: 6
Training loss: 2.641366958618164
Validation loss: 2.3714508471950406

Epoch: 5| Step: 7
Training loss: 2.678760051727295
Validation loss: 2.372748872285248

Epoch: 5| Step: 8
Training loss: 3.0478415489196777
Validation loss: 2.363697313493298

Epoch: 5| Step: 9
Training loss: 2.863206386566162
Validation loss: 2.3660741057447208

Epoch: 5| Step: 10
Training loss: 1.7823591232299805
Validation loss: 2.349291016978602

Epoch: 108| Step: 0
Training loss: 3.056352138519287
Validation loss: 2.335475888303531

Epoch: 5| Step: 1
Training loss: 2.526564836502075
Validation loss: 2.327624064619823

Epoch: 5| Step: 2
Training loss: 2.3085362911224365
Validation loss: 2.326584239159861

Epoch: 5| Step: 3
Training loss: 2.0114846229553223
Validation loss: 2.3266582142922188

Epoch: 5| Step: 4
Training loss: 2.693223237991333
Validation loss: 2.3309990436800065

Epoch: 5| Step: 5
Training loss: 2.685107469558716
Validation loss: 2.334714921571875

Epoch: 5| Step: 6
Training loss: 2.9469549655914307
Validation loss: 2.3388091800033406

Epoch: 5| Step: 7
Training loss: 2.5774056911468506
Validation loss: 2.3329779473684167

Epoch: 5| Step: 8
Training loss: 2.7349438667297363
Validation loss: 2.3329288805684736

Epoch: 5| Step: 9
Training loss: 2.5040688514709473
Validation loss: 2.332090695699056

Epoch: 5| Step: 10
Training loss: 2.3105242252349854
Validation loss: 2.3317160785839124

Epoch: 109| Step: 0
Training loss: 2.945124387741089
Validation loss: 2.3220051821841987

Epoch: 5| Step: 1
Training loss: 2.1302456855773926
Validation loss: 2.3317065597862325

Epoch: 5| Step: 2
Training loss: 2.4674270153045654
Validation loss: 2.3432011424854235

Epoch: 5| Step: 3
Training loss: 2.319277286529541
Validation loss: 2.3494263182404223

Epoch: 5| Step: 4
Training loss: 2.4613709449768066
Validation loss: 2.3391219710790985

Epoch: 5| Step: 5
Training loss: 2.412891387939453
Validation loss: 2.3355471062403854

Epoch: 5| Step: 6
Training loss: 3.223663806915283
Validation loss: 2.333037604567825

Epoch: 5| Step: 7
Training loss: 2.3515496253967285
Validation loss: 2.344141894771207

Epoch: 5| Step: 8
Training loss: 2.4387288093566895
Validation loss: 2.360784694712649

Epoch: 5| Step: 9
Training loss: 2.6312508583068848
Validation loss: 2.36409560198425

Epoch: 5| Step: 10
Training loss: 3.077794075012207
Validation loss: 2.3611222569660475

Epoch: 110| Step: 0
Training loss: 2.567708730697632
Validation loss: 2.3513013675648677

Epoch: 5| Step: 1
Training loss: 3.1670119762420654
Validation loss: 2.3314218495481756

Epoch: 5| Step: 2
Training loss: 2.216550827026367
Validation loss: 2.3146947942754275

Epoch: 5| Step: 3
Training loss: 2.463559627532959
Validation loss: 2.3099694969833537

Epoch: 5| Step: 4
Training loss: 3.034924030303955
Validation loss: 2.304012083238171

Epoch: 5| Step: 5
Training loss: 2.6794204711914062
Validation loss: 2.312230892078851

Epoch: 5| Step: 6
Training loss: 2.137254476547241
Validation loss: 2.3142939639347855

Epoch: 5| Step: 7
Training loss: 2.3107142448425293
Validation loss: 2.3119532472343853

Epoch: 5| Step: 8
Training loss: 2.7042431831359863
Validation loss: 2.312507901140439

Epoch: 5| Step: 9
Training loss: 2.6092703342437744
Validation loss: 2.310922440662179

Epoch: 5| Step: 10
Training loss: 2.6154510974884033
Validation loss: 2.307148031009141

Epoch: 111| Step: 0
Training loss: 2.8250560760498047
Validation loss: 2.3124516497376146

Epoch: 5| Step: 1
Training loss: 2.478369951248169
Validation loss: 2.3076305312495076

Epoch: 5| Step: 2
Training loss: 2.284245014190674
Validation loss: 2.315276802227061

Epoch: 5| Step: 3
Training loss: 2.560037851333618
Validation loss: 2.3282614241364183

Epoch: 5| Step: 4
Training loss: 2.6424190998077393
Validation loss: 2.3702155351638794

Epoch: 5| Step: 5
Training loss: 2.393434762954712
Validation loss: 2.4041974288161083

Epoch: 5| Step: 6
Training loss: 2.9818480014801025
Validation loss: 2.397997961249403

Epoch: 5| Step: 7
Training loss: 2.2048447132110596
Validation loss: 2.4247222279989593

Epoch: 5| Step: 8
Training loss: 2.394472599029541
Validation loss: 2.412071256227391

Epoch: 5| Step: 9
Training loss: 2.7988743782043457
Validation loss: 2.3989769540807253

Epoch: 5| Step: 10
Training loss: 2.9997119903564453
Validation loss: 2.385524352391561

Epoch: 112| Step: 0
Training loss: 3.085106134414673
Validation loss: 2.3700952452998005

Epoch: 5| Step: 1
Training loss: 2.233522415161133
Validation loss: 2.3460974180570213

Epoch: 5| Step: 2
Training loss: 2.188509225845337
Validation loss: 2.3154564134536253

Epoch: 5| Step: 3
Training loss: 3.229971408843994
Validation loss: 2.3132982766756447

Epoch: 5| Step: 4
Training loss: 2.8562347888946533
Validation loss: 2.297360468936223

Epoch: 5| Step: 5
Training loss: 1.958726167678833
Validation loss: 2.3014507703883673

Epoch: 5| Step: 6
Training loss: 2.9635162353515625
Validation loss: 2.3020104541573474

Epoch: 5| Step: 7
Training loss: 2.499009370803833
Validation loss: 2.298379123851817

Epoch: 5| Step: 8
Training loss: 2.250180721282959
Validation loss: 2.2966028900556665

Epoch: 5| Step: 9
Training loss: 2.257460117340088
Validation loss: 2.2943342244753273

Epoch: 5| Step: 10
Training loss: 2.8089263439178467
Validation loss: 2.296544782577022

Epoch: 113| Step: 0
Training loss: 2.21061372756958
Validation loss: 2.298061283685828

Epoch: 5| Step: 1
Training loss: 2.7907705307006836
Validation loss: 2.3031255622063913

Epoch: 5| Step: 2
Training loss: 2.791135787963867
Validation loss: 2.30180533214282

Epoch: 5| Step: 3
Training loss: 2.6069722175598145
Validation loss: 2.3074204934540616

Epoch: 5| Step: 4
Training loss: 2.159698486328125
Validation loss: 2.3175595575763333

Epoch: 5| Step: 5
Training loss: 2.4368040561676025
Validation loss: 2.317721882174092

Epoch: 5| Step: 6
Training loss: 2.4124581813812256
Validation loss: 2.3279179655095583

Epoch: 5| Step: 7
Training loss: 2.8862991333007812
Validation loss: 2.3485614997084423

Epoch: 5| Step: 8
Training loss: 2.295381546020508
Validation loss: 2.337808216771772

Epoch: 5| Step: 9
Training loss: 3.0510828495025635
Validation loss: 2.3241312619178527

Epoch: 5| Step: 10
Training loss: 2.5795633792877197
Validation loss: 2.3104942690941597

Epoch: 114| Step: 0
Training loss: 2.0980896949768066
Validation loss: 2.2997040851141817

Epoch: 5| Step: 1
Training loss: 2.220480442047119
Validation loss: 2.298033873240153

Epoch: 5| Step: 2
Training loss: 2.693040132522583
Validation loss: 2.2938904082903298

Epoch: 5| Step: 3
Training loss: 2.9716172218322754
Validation loss: 2.2959403376425467

Epoch: 5| Step: 4
Training loss: 3.241474151611328
Validation loss: 2.299955939733854

Epoch: 5| Step: 5
Training loss: 2.5558321475982666
Validation loss: 2.3027172883351645

Epoch: 5| Step: 6
Training loss: 2.67856764793396
Validation loss: 2.3049652140627623

Epoch: 5| Step: 7
Training loss: 2.7900969982147217
Validation loss: 2.3189068737850396

Epoch: 5| Step: 8
Training loss: 2.2576746940612793
Validation loss: 2.3173416019767843

Epoch: 5| Step: 9
Training loss: 2.375967502593994
Validation loss: 2.323174427914363

Epoch: 5| Step: 10
Training loss: 2.381606101989746
Validation loss: 2.3311676415064

Epoch: 115| Step: 0
Training loss: 2.5136728286743164
Validation loss: 2.331410215746972

Epoch: 5| Step: 1
Training loss: 2.613377571105957
Validation loss: 2.321347262269707

Epoch: 5| Step: 2
Training loss: 2.7405409812927246
Validation loss: 2.3410778161018126

Epoch: 5| Step: 3
Training loss: 2.0978879928588867
Validation loss: 2.340991673930999

Epoch: 5| Step: 4
Training loss: 2.658128023147583
Validation loss: 2.3403137063467376

Epoch: 5| Step: 5
Training loss: 2.9193732738494873
Validation loss: 2.347276015948224

Epoch: 5| Step: 6
Training loss: 3.028043270111084
Validation loss: 2.3648024566711916

Epoch: 5| Step: 7
Training loss: 2.277592897415161
Validation loss: 2.34668840131452

Epoch: 5| Step: 8
Training loss: 2.0618557929992676
Validation loss: 2.341653403415475

Epoch: 5| Step: 9
Training loss: 2.9441821575164795
Validation loss: 2.335029279032061

Epoch: 5| Step: 10
Training loss: 2.2446274757385254
Validation loss: 2.3085868666248937

Epoch: 116| Step: 0
Training loss: 2.8644442558288574
Validation loss: 2.3107054464278685

Epoch: 5| Step: 1
Training loss: 2.3576958179473877
Validation loss: 2.2989735667423536

Epoch: 5| Step: 2
Training loss: 2.6584267616271973
Validation loss: 2.2988022450477845

Epoch: 5| Step: 3
Training loss: 2.8273377418518066
Validation loss: 2.3083976776369157

Epoch: 5| Step: 4
Training loss: 2.6129167079925537
Validation loss: 2.307196996545279

Epoch: 5| Step: 5
Training loss: 1.9591872692108154
Validation loss: 2.300884703154205

Epoch: 5| Step: 6
Training loss: 2.5777785778045654
Validation loss: 2.3156651707105738

Epoch: 5| Step: 7
Training loss: 2.452216625213623
Validation loss: 2.3191015002548054

Epoch: 5| Step: 8
Training loss: 2.6976046562194824
Validation loss: 2.326886838482272

Epoch: 5| Step: 9
Training loss: 2.9942193031311035
Validation loss: 2.3183861778628443

Epoch: 5| Step: 10
Training loss: 2.0415420532226562
Validation loss: 2.324270530413556

Epoch: 117| Step: 0
Training loss: 2.6129531860351562
Validation loss: 2.308100620905558

Epoch: 5| Step: 1
Training loss: 2.63731050491333
Validation loss: 2.2950987636402087

Epoch: 5| Step: 2
Training loss: 2.4347190856933594
Validation loss: 2.29709376570999

Epoch: 5| Step: 3
Training loss: 2.0994133949279785
Validation loss: 2.29734532935645

Epoch: 5| Step: 4
Training loss: 2.9416542053222656
Validation loss: 2.298684717506491

Epoch: 5| Step: 5
Training loss: 2.564997911453247
Validation loss: 2.3132789493888937

Epoch: 5| Step: 6
Training loss: 2.6302919387817383
Validation loss: 2.3220378980841687

Epoch: 5| Step: 7
Training loss: 2.281845808029175
Validation loss: 2.3189598206550843

Epoch: 5| Step: 8
Training loss: 2.523043394088745
Validation loss: 2.3209723413631482

Epoch: 5| Step: 9
Training loss: 2.709989070892334
Validation loss: 2.3254739622915945

Epoch: 5| Step: 10
Training loss: 2.6957149505615234
Validation loss: 2.3170271535073557

Epoch: 118| Step: 0
Training loss: 2.884791851043701
Validation loss: 2.310565317830732

Epoch: 5| Step: 1
Training loss: 2.5886948108673096
Validation loss: 2.300094496819281

Epoch: 5| Step: 2
Training loss: 2.710420608520508
Validation loss: 2.2987261100481917

Epoch: 5| Step: 3
Training loss: 1.7243064641952515
Validation loss: 2.304313111048873

Epoch: 5| Step: 4
Training loss: 2.909943103790283
Validation loss: 2.28851415264991

Epoch: 5| Step: 5
Training loss: 2.573617458343506
Validation loss: 2.2720605916874383

Epoch: 5| Step: 6
Training loss: 2.0528786182403564
Validation loss: 2.282147994605444

Epoch: 5| Step: 7
Training loss: 2.499690532684326
Validation loss: 2.276868958627024

Epoch: 5| Step: 8
Training loss: 2.64194393157959
Validation loss: 2.2801467705798406

Epoch: 5| Step: 9
Training loss: 2.6538867950439453
Validation loss: 2.272153659533429

Epoch: 5| Step: 10
Training loss: 2.9256036281585693
Validation loss: 2.2833783139464674

Epoch: 119| Step: 0
Training loss: 2.462291955947876
Validation loss: 2.281817320854433

Epoch: 5| Step: 1
Training loss: 2.6486876010894775
Validation loss: 2.2822619881681216

Epoch: 5| Step: 2
Training loss: 2.240030288696289
Validation loss: 2.28379144976216

Epoch: 5| Step: 3
Training loss: 2.540886163711548
Validation loss: 2.286079083719561

Epoch: 5| Step: 4
Training loss: 2.4685254096984863
Validation loss: 2.2832503139331775

Epoch: 5| Step: 5
Training loss: 2.4481358528137207
Validation loss: 2.281182260923488

Epoch: 5| Step: 6
Training loss: 2.844733715057373
Validation loss: 2.32158596797656

Epoch: 5| Step: 7
Training loss: 2.9207959175109863
Validation loss: 2.3366182029888196

Epoch: 5| Step: 8
Training loss: 2.9270596504211426
Validation loss: 2.3363441216048373

Epoch: 5| Step: 9
Training loss: 2.478234052658081
Validation loss: 2.3362804176986858

Epoch: 5| Step: 10
Training loss: 2.238204002380371
Validation loss: 2.308214441422493

Epoch: 120| Step: 0
Training loss: 2.8181910514831543
Validation loss: 2.3032731163886284

Epoch: 5| Step: 1
Training loss: 2.2759928703308105
Validation loss: 2.299461826201408

Epoch: 5| Step: 2
Training loss: 2.6185617446899414
Validation loss: 2.2943328503639466

Epoch: 5| Step: 3
Training loss: 2.716571807861328
Validation loss: 2.2999747645470405

Epoch: 5| Step: 4
Training loss: 1.996938705444336
Validation loss: 2.3075108092318297

Epoch: 5| Step: 5
Training loss: 2.8059945106506348
Validation loss: 2.3087125952525804

Epoch: 5| Step: 6
Training loss: 2.945190668106079
Validation loss: 2.304444128467191

Epoch: 5| Step: 7
Training loss: 2.903266429901123
Validation loss: 2.287133442458286

Epoch: 5| Step: 8
Training loss: 2.061494827270508
Validation loss: 2.2827991131813294

Epoch: 5| Step: 9
Training loss: 2.5499014854431152
Validation loss: 2.28606963926746

Epoch: 5| Step: 10
Training loss: 2.4408578872680664
Validation loss: 2.2777881237768356

Epoch: 121| Step: 0
Training loss: 2.3655848503112793
Validation loss: 2.2784980753416657

Epoch: 5| Step: 1
Training loss: 1.5969345569610596
Validation loss: 2.288014393980785

Epoch: 5| Step: 2
Training loss: 2.5476088523864746
Validation loss: 2.2822465896606445

Epoch: 5| Step: 3
Training loss: 2.6428871154785156
Validation loss: 2.2831033327246226

Epoch: 5| Step: 4
Training loss: 2.567821741104126
Validation loss: 2.2871144868994273

Epoch: 5| Step: 5
Training loss: 3.0351366996765137
Validation loss: 2.2885482977795344

Epoch: 5| Step: 6
Training loss: 2.327810287475586
Validation loss: 2.2871788676067064

Epoch: 5| Step: 7
Training loss: 2.398472309112549
Validation loss: 2.28719312144864

Epoch: 5| Step: 8
Training loss: 2.319326877593994
Validation loss: 2.29110288107267

Epoch: 5| Step: 9
Training loss: 2.629544734954834
Validation loss: 2.2935961677182104

Epoch: 5| Step: 10
Training loss: 3.8280253410339355
Validation loss: 2.2914754036934144

Epoch: 122| Step: 0
Training loss: 2.4437804222106934
Validation loss: 2.2883543173472085

Epoch: 5| Step: 1
Training loss: 2.7263050079345703
Validation loss: 2.285371982923118

Epoch: 5| Step: 2
Training loss: 2.4142539501190186
Validation loss: 2.28780452154016

Epoch: 5| Step: 3
Training loss: 2.524754047393799
Validation loss: 2.2866596201414704

Epoch: 5| Step: 4
Training loss: 2.7807133197784424
Validation loss: 2.2771328674849642

Epoch: 5| Step: 5
Training loss: 2.460749864578247
Validation loss: 2.2895733335966706

Epoch: 5| Step: 6
Training loss: 2.931277275085449
Validation loss: 2.290001816647027

Epoch: 5| Step: 7
Training loss: 2.857978343963623
Validation loss: 2.3031364307608655

Epoch: 5| Step: 8
Training loss: 2.3156509399414062
Validation loss: 2.310041509648805

Epoch: 5| Step: 9
Training loss: 1.912569284439087
Validation loss: 2.328748672239242

Epoch: 5| Step: 10
Training loss: 2.6202263832092285
Validation loss: 2.330079795211874

Epoch: 123| Step: 0
Training loss: 3.204514980316162
Validation loss: 2.330071182661159

Epoch: 5| Step: 1
Training loss: 2.9153175354003906
Validation loss: 2.3034547939095447

Epoch: 5| Step: 2
Training loss: 2.614128589630127
Validation loss: 2.2812984630625737

Epoch: 5| Step: 3
Training loss: 3.0247554779052734
Validation loss: 2.281921343136859

Epoch: 5| Step: 4
Training loss: 2.2453250885009766
Validation loss: 2.2709242861757994

Epoch: 5| Step: 5
Training loss: 1.9556376934051514
Validation loss: 2.266729700949884

Epoch: 5| Step: 6
Training loss: 2.2339906692504883
Validation loss: 2.2693502800438994

Epoch: 5| Step: 7
Training loss: 2.2147905826568604
Validation loss: 2.274951422086326

Epoch: 5| Step: 8
Training loss: 3.062058210372925
Validation loss: 2.27615362341686

Epoch: 5| Step: 9
Training loss: 1.7868921756744385
Validation loss: 2.2736539456152145

Epoch: 5| Step: 10
Training loss: 2.858426570892334
Validation loss: 2.271573802476288

Epoch: 124| Step: 0
Training loss: 3.0156872272491455
Validation loss: 2.285052320008637

Epoch: 5| Step: 1
Training loss: 2.274740219116211
Validation loss: 2.2905789370177896

Epoch: 5| Step: 2
Training loss: 2.6317877769470215
Validation loss: 2.2960279833885933

Epoch: 5| Step: 3
Training loss: 3.0615971088409424
Validation loss: 2.3032014523783038

Epoch: 5| Step: 4
Training loss: 2.953713893890381
Validation loss: 2.3130934879344

Epoch: 5| Step: 5
Training loss: 1.977367639541626
Validation loss: 2.314434920587847

Epoch: 5| Step: 6
Training loss: 2.4647183418273926
Validation loss: 2.3064653834988995

Epoch: 5| Step: 7
Training loss: 1.762020468711853
Validation loss: 2.2870337219648462

Epoch: 5| Step: 8
Training loss: 2.624444007873535
Validation loss: 2.282845245894565

Epoch: 5| Step: 9
Training loss: 2.519014358520508
Validation loss: 2.2679424055161013

Epoch: 5| Step: 10
Training loss: 2.641425609588623
Validation loss: 2.2771967226459133

Epoch: 125| Step: 0
Training loss: 2.154501438140869
Validation loss: 2.2703369894335346

Epoch: 5| Step: 1
Training loss: 1.9195276498794556
Validation loss: 2.2780686693806804

Epoch: 5| Step: 2
Training loss: 2.2281625270843506
Validation loss: 2.2626936845881964

Epoch: 5| Step: 3
Training loss: 2.299818277359009
Validation loss: 2.2622740243070867

Epoch: 5| Step: 4
Training loss: 2.253838539123535
Validation loss: 2.260763229862336

Epoch: 5| Step: 5
Training loss: 2.6317100524902344
Validation loss: 2.259726325670878

Epoch: 5| Step: 6
Training loss: 2.138561487197876
Validation loss: 2.262692223313034

Epoch: 5| Step: 7
Training loss: 2.8885269165039062
Validation loss: 2.257240879920221

Epoch: 5| Step: 8
Training loss: 3.0453124046325684
Validation loss: 2.2599684012833463

Epoch: 5| Step: 9
Training loss: 3.1378748416900635
Validation loss: 2.256809321782922

Epoch: 5| Step: 10
Training loss: 3.337022304534912
Validation loss: 2.2603519193587767

Epoch: 126| Step: 0
Training loss: 2.249922275543213
Validation loss: 2.259590853926956

Epoch: 5| Step: 1
Training loss: 2.518967390060425
Validation loss: 2.2593141448113228

Epoch: 5| Step: 2
Training loss: 2.291003465652466
Validation loss: 2.2734354208874445

Epoch: 5| Step: 3
Training loss: 3.2821497917175293
Validation loss: 2.2800138637583744

Epoch: 5| Step: 4
Training loss: 2.38885760307312
Validation loss: 2.293322729808028

Epoch: 5| Step: 5
Training loss: 2.575122356414795
Validation loss: 2.2994003859899377

Epoch: 5| Step: 6
Training loss: 2.456270217895508
Validation loss: 2.2994019626289286

Epoch: 5| Step: 7
Training loss: 2.7099409103393555
Validation loss: 2.287064157506471

Epoch: 5| Step: 8
Training loss: 2.805394172668457
Validation loss: 2.281470251339738

Epoch: 5| Step: 9
Training loss: 2.02657413482666
Validation loss: 2.2786212428923576

Epoch: 5| Step: 10
Training loss: 2.5465056896209717
Validation loss: 2.2677661295860045

Epoch: 127| Step: 0
Training loss: 2.4054598808288574
Validation loss: 2.2581226107894734

Epoch: 5| Step: 1
Training loss: 2.4080593585968018
Validation loss: 2.258600599022322

Epoch: 5| Step: 2
Training loss: 2.9191107749938965
Validation loss: 2.2532362091925835

Epoch: 5| Step: 3
Training loss: 3.156036853790283
Validation loss: 2.2613227931402062

Epoch: 5| Step: 4
Training loss: 2.785121440887451
Validation loss: 2.261386468846311

Epoch: 5| Step: 5
Training loss: 1.9167773723602295
Validation loss: 2.258082277031355

Epoch: 5| Step: 6
Training loss: 2.2856345176696777
Validation loss: 2.263277498624658

Epoch: 5| Step: 7
Training loss: 2.5922813415527344
Validation loss: 2.2772139131381945

Epoch: 5| Step: 8
Training loss: 1.7764930725097656
Validation loss: 2.278819736614022

Epoch: 5| Step: 9
Training loss: 3.446443557739258
Validation loss: 2.287893141469648

Epoch: 5| Step: 10
Training loss: 2.063596248626709
Validation loss: 2.3119686329236595

Epoch: 128| Step: 0
Training loss: 1.5040276050567627
Validation loss: 2.313957281010125

Epoch: 5| Step: 1
Training loss: 2.550452709197998
Validation loss: 2.3185024005110546

Epoch: 5| Step: 2
Training loss: 1.9322292804718018
Validation loss: 2.3370967629135295

Epoch: 5| Step: 3
Training loss: 2.2042651176452637
Validation loss: 2.356179521929833

Epoch: 5| Step: 4
Training loss: 2.794297456741333
Validation loss: 2.3534148764866654

Epoch: 5| Step: 5
Training loss: 2.667174816131592
Validation loss: 2.337598200767271

Epoch: 5| Step: 6
Training loss: 3.0739493370056152
Validation loss: 2.2826139798728367

Epoch: 5| Step: 7
Training loss: 3.074042797088623
Validation loss: 2.255106818291449

Epoch: 5| Step: 8
Training loss: 2.7535476684570312
Validation loss: 2.251766045888265

Epoch: 5| Step: 9
Training loss: 2.5957093238830566
Validation loss: 2.2502103800414712

Epoch: 5| Step: 10
Training loss: 2.923919439315796
Validation loss: 2.257605077118002

Epoch: 129| Step: 0
Training loss: 2.295952320098877
Validation loss: 2.269634508317517

Epoch: 5| Step: 1
Training loss: 3.101253032684326
Validation loss: 2.2677200866001908

Epoch: 5| Step: 2
Training loss: 2.716493844985962
Validation loss: 2.2732915339931363

Epoch: 5| Step: 3
Training loss: 1.9895919561386108
Validation loss: 2.266599726933305

Epoch: 5| Step: 4
Training loss: 2.6682534217834473
Validation loss: 2.2584783184912895

Epoch: 5| Step: 5
Training loss: 2.357637643814087
Validation loss: 2.2499493014427925

Epoch: 5| Step: 6
Training loss: 3.055609941482544
Validation loss: 2.2494191482502925

Epoch: 5| Step: 7
Training loss: 2.5287375450134277
Validation loss: 2.258784591510732

Epoch: 5| Step: 8
Training loss: 2.3701558113098145
Validation loss: 2.2734407455690446

Epoch: 5| Step: 9
Training loss: 2.2410264015197754
Validation loss: 2.2911284226243214

Epoch: 5| Step: 10
Training loss: 2.7070586681365967
Validation loss: 2.327892006084483

Epoch: 130| Step: 0
Training loss: 2.105107069015503
Validation loss: 2.34071244219298

Epoch: 5| Step: 1
Training loss: 2.468693256378174
Validation loss: 2.313802239715412

Epoch: 5| Step: 2
Training loss: 2.582514762878418
Validation loss: 2.2964507456748717

Epoch: 5| Step: 3
Training loss: 2.626708507537842
Validation loss: 2.3027967701676073

Epoch: 5| Step: 4
Training loss: 3.222541093826294
Validation loss: 2.2886630181343324

Epoch: 5| Step: 5
Training loss: 2.277506113052368
Validation loss: 2.2825961728249826

Epoch: 5| Step: 6
Training loss: 2.052311420440674
Validation loss: 2.2632139652006087

Epoch: 5| Step: 7
Training loss: 2.473327875137329
Validation loss: 2.258719264820058

Epoch: 5| Step: 8
Training loss: 2.809039354324341
Validation loss: 2.2532198224016415

Epoch: 5| Step: 9
Training loss: 2.9122703075408936
Validation loss: 2.2586501183048373

Epoch: 5| Step: 10
Training loss: 2.2580742835998535
Validation loss: 2.258485717158164

Epoch: 131| Step: 0
Training loss: 2.392082691192627
Validation loss: 2.26656537799425

Epoch: 5| Step: 1
Training loss: 2.3275668621063232
Validation loss: 2.2627459187661447

Epoch: 5| Step: 2
Training loss: 2.4316534996032715
Validation loss: 2.2640643991449827

Epoch: 5| Step: 3
Training loss: 2.5750339031219482
Validation loss: 2.2626503846978627

Epoch: 5| Step: 4
Training loss: 2.3307759761810303
Validation loss: 2.2604336892404864

Epoch: 5| Step: 5
Training loss: 2.488309621810913
Validation loss: 2.260615030924479

Epoch: 5| Step: 6
Training loss: 2.656359910964966
Validation loss: 2.2498384906399633

Epoch: 5| Step: 7
Training loss: 2.6484556198120117
Validation loss: 2.261579103367303

Epoch: 5| Step: 8
Training loss: 2.6549019813537598
Validation loss: 2.260489530460809

Epoch: 5| Step: 9
Training loss: 2.443350315093994
Validation loss: 2.270187001074514

Epoch: 5| Step: 10
Training loss: 3.0322322845458984
Validation loss: 2.281123004933839

Epoch: 132| Step: 0
Training loss: 2.6717658042907715
Validation loss: 2.288123666599233

Epoch: 5| Step: 1
Training loss: 2.0177812576293945
Validation loss: 2.2716814010374007

Epoch: 5| Step: 2
Training loss: 2.76517391204834
Validation loss: 2.2867284897835023

Epoch: 5| Step: 3
Training loss: 3.0076587200164795
Validation loss: 2.2798762629109044

Epoch: 5| Step: 4
Training loss: 2.2675065994262695
Validation loss: 2.2871866533833165

Epoch: 5| Step: 5
Training loss: 2.8755874633789062
Validation loss: 2.2564877566470893

Epoch: 5| Step: 6
Training loss: 2.2052085399627686
Validation loss: 2.2649695232350338

Epoch: 5| Step: 7
Training loss: 2.482442617416382
Validation loss: 2.262512742832143

Epoch: 5| Step: 8
Training loss: 2.6123454570770264
Validation loss: 2.251865219044429

Epoch: 5| Step: 9
Training loss: 2.24790620803833
Validation loss: 2.2488156069991407

Epoch: 5| Step: 10
Training loss: 2.716676712036133
Validation loss: 2.2493889639454503

Epoch: 133| Step: 0
Training loss: 2.2624154090881348
Validation loss: 2.2526126817990373

Epoch: 5| Step: 1
Training loss: 2.095996856689453
Validation loss: 2.2514133555914766

Epoch: 5| Step: 2
Training loss: 2.1878695487976074
Validation loss: 2.2614828745524087

Epoch: 5| Step: 3
Training loss: 2.877347946166992
Validation loss: 2.2583594629841466

Epoch: 5| Step: 4
Training loss: 1.9770981073379517
Validation loss: 2.2543811080276326

Epoch: 5| Step: 5
Training loss: 2.8499388694763184
Validation loss: 2.2747626458444903

Epoch: 5| Step: 6
Training loss: 2.424293279647827
Validation loss: 2.2716348030233897

Epoch: 5| Step: 7
Training loss: 2.8702824115753174
Validation loss: 2.2883915798638457

Epoch: 5| Step: 8
Training loss: 2.887652635574341
Validation loss: 2.2989980610468055

Epoch: 5| Step: 9
Training loss: 2.7879421710968018
Validation loss: 2.298068477261451

Epoch: 5| Step: 10
Training loss: 2.411006450653076
Validation loss: 2.285108332992882

Epoch: 134| Step: 0
Training loss: 2.170684337615967
Validation loss: 2.273228710697543

Epoch: 5| Step: 1
Training loss: 2.1931824684143066
Validation loss: 2.269378085290232

Epoch: 5| Step: 2
Training loss: 2.178894519805908
Validation loss: 2.2485762232093403

Epoch: 5| Step: 3
Training loss: 3.049081802368164
Validation loss: 2.2548555148545133

Epoch: 5| Step: 4
Training loss: 2.842766284942627
Validation loss: 2.2457616970103276

Epoch: 5| Step: 5
Training loss: 2.3489668369293213
Validation loss: 2.24739686904415

Epoch: 5| Step: 6
Training loss: 2.263016700744629
Validation loss: 2.245245292622556

Epoch: 5| Step: 7
Training loss: 2.495884418487549
Validation loss: 2.246587040603802

Epoch: 5| Step: 8
Training loss: 2.8942792415618896
Validation loss: 2.2470517978873303

Epoch: 5| Step: 9
Training loss: 2.7290642261505127
Validation loss: 2.2422449998958136

Epoch: 5| Step: 10
Training loss: 2.4957046508789062
Validation loss: 2.2457444026906

Epoch: 135| Step: 0
Training loss: 2.367249011993408
Validation loss: 2.2512758829260386

Epoch: 5| Step: 1
Training loss: 3.084677219390869
Validation loss: 2.2457173229545675

Epoch: 5| Step: 2
Training loss: 2.185581684112549
Validation loss: 2.2681787552372104

Epoch: 5| Step: 3
Training loss: 3.1343860626220703
Validation loss: 2.294049911601569

Epoch: 5| Step: 4
Training loss: 2.359442949295044
Validation loss: 2.3030554427895495

Epoch: 5| Step: 5
Training loss: 2.698986053466797
Validation loss: 2.2983401847142044

Epoch: 5| Step: 6
Training loss: 2.2684741020202637
Validation loss: 2.27736020857288

Epoch: 5| Step: 7
Training loss: 2.37691068649292
Validation loss: 2.280290739510649

Epoch: 5| Step: 8
Training loss: 2.360872745513916
Validation loss: 2.2566241346379763

Epoch: 5| Step: 9
Training loss: 2.9384920597076416
Validation loss: 2.25005163556786

Epoch: 5| Step: 10
Training loss: 1.7946337461471558
Validation loss: 2.2462100008482575

Epoch: 136| Step: 0
Training loss: 2.532766819000244
Validation loss: 2.2334906747264247

Epoch: 5| Step: 1
Training loss: 2.997021198272705
Validation loss: 2.2453320744217082

Epoch: 5| Step: 2
Training loss: 2.357673406600952
Validation loss: 2.245096657865791

Epoch: 5| Step: 3
Training loss: 1.6631923913955688
Validation loss: 2.2371724241523334

Epoch: 5| Step: 4
Training loss: 2.8176636695861816
Validation loss: 2.235471622918242

Epoch: 5| Step: 5
Training loss: 2.3124146461486816
Validation loss: 2.245413067520306

Epoch: 5| Step: 6
Training loss: 2.4921023845672607
Validation loss: 2.2603576260228313

Epoch: 5| Step: 7
Training loss: 2.200841188430786
Validation loss: 2.2755634989789737

Epoch: 5| Step: 8
Training loss: 2.552170991897583
Validation loss: 2.3086124209947485

Epoch: 5| Step: 9
Training loss: 2.4373393058776855
Validation loss: 2.3212518563834568

Epoch: 5| Step: 10
Training loss: 3.264139413833618
Validation loss: 2.3173064826637186

Epoch: 137| Step: 0
Training loss: 2.0179500579833984
Validation loss: 2.3074914204177035

Epoch: 5| Step: 1
Training loss: 2.44919490814209
Validation loss: 2.304026106352447

Epoch: 5| Step: 2
Training loss: 2.809262990951538
Validation loss: 2.2909342089006977

Epoch: 5| Step: 3
Training loss: 2.602036952972412
Validation loss: 2.3143600212630404

Epoch: 5| Step: 4
Training loss: 2.1321842670440674
Validation loss: 2.3297564650094635

Epoch: 5| Step: 5
Training loss: 2.7453248500823975
Validation loss: 2.3216865293441282

Epoch: 5| Step: 6
Training loss: 2.6180756092071533
Validation loss: 2.2906565768744356

Epoch: 5| Step: 7
Training loss: 2.873304843902588
Validation loss: 2.280623535956106

Epoch: 5| Step: 8
Training loss: 2.9386119842529297
Validation loss: 2.249279442653861

Epoch: 5| Step: 9
Training loss: 2.611640453338623
Validation loss: 2.2465412821820987

Epoch: 5| Step: 10
Training loss: 1.7993316650390625
Validation loss: 2.2402907815030826

Epoch: 138| Step: 0
Training loss: 2.659900426864624
Validation loss: 2.2390243725110124

Epoch: 5| Step: 1
Training loss: 2.7013344764709473
Validation loss: 2.2372856601592033

Epoch: 5| Step: 2
Training loss: 1.9570081233978271
Validation loss: 2.2327395536566295

Epoch: 5| Step: 3
Training loss: 2.4194769859313965
Validation loss: 2.2261531070996354

Epoch: 5| Step: 4
Training loss: 3.0985465049743652
Validation loss: 2.222131711180492

Epoch: 5| Step: 5
Training loss: 2.3753821849823
Validation loss: 2.222635531938204

Epoch: 5| Step: 6
Training loss: 2.90849232673645
Validation loss: 2.2320329989156416

Epoch: 5| Step: 7
Training loss: 2.5972931385040283
Validation loss: 2.230838588488999

Epoch: 5| Step: 8
Training loss: 2.2188785076141357
Validation loss: 2.243500935134067

Epoch: 5| Step: 9
Training loss: 1.789881944656372
Validation loss: 2.2490820551431305

Epoch: 5| Step: 10
Training loss: 2.999786615371704
Validation loss: 2.25675303705277

Epoch: 139| Step: 0
Training loss: 2.4972307682037354
Validation loss: 2.2485363201428483

Epoch: 5| Step: 1
Training loss: 2.281958818435669
Validation loss: 2.2655941735031786

Epoch: 5| Step: 2
Training loss: 2.017514705657959
Validation loss: 2.261873937422229

Epoch: 5| Step: 3
Training loss: 2.0752508640289307
Validation loss: 2.2745312119043

Epoch: 5| Step: 4
Training loss: 2.3160560131073
Validation loss: 2.292984826590425

Epoch: 5| Step: 5
Training loss: 2.098505735397339
Validation loss: 2.2966199382658927

Epoch: 5| Step: 6
Training loss: 3.4584147930145264
Validation loss: 2.280428650558636

Epoch: 5| Step: 7
Training loss: 3.536663770675659
Validation loss: 2.2473090207704933

Epoch: 5| Step: 8
Training loss: 2.673154592514038
Validation loss: 2.229529224416261

Epoch: 5| Step: 9
Training loss: 2.531411647796631
Validation loss: 2.214853045760944

Epoch: 5| Step: 10
Training loss: 1.9945818185806274
Validation loss: 2.2199691880133843

Epoch: 140| Step: 0
Training loss: 2.295897960662842
Validation loss: 2.2188599340377317

Epoch: 5| Step: 1
Training loss: 2.857900619506836
Validation loss: 2.223141554863222

Epoch: 5| Step: 2
Training loss: 2.3899290561676025
Validation loss: 2.2263026237487793

Epoch: 5| Step: 3
Training loss: 2.1658122539520264
Validation loss: 2.2231592337290444

Epoch: 5| Step: 4
Training loss: 2.652031421661377
Validation loss: 2.2161249691440212

Epoch: 5| Step: 5
Training loss: 2.882222890853882
Validation loss: 2.2195785122533

Epoch: 5| Step: 6
Training loss: 1.75282883644104
Validation loss: 2.2249717327856247

Epoch: 5| Step: 7
Training loss: 2.469452381134033
Validation loss: 2.238480624332223

Epoch: 5| Step: 8
Training loss: 2.8489181995391846
Validation loss: 2.2513527485632125

Epoch: 5| Step: 9
Training loss: 2.019930362701416
Validation loss: 2.2653306299640286

Epoch: 5| Step: 10
Training loss: 3.481626510620117
Validation loss: 2.2736316855235765

Epoch: 141| Step: 0
Training loss: 1.9784934520721436
Validation loss: 2.2766919623139086

Epoch: 5| Step: 1
Training loss: 2.5328755378723145
Validation loss: 2.2908199487193937

Epoch: 5| Step: 2
Training loss: 3.023555278778076
Validation loss: 2.3113401410400227

Epoch: 5| Step: 3
Training loss: 2.3338451385498047
Validation loss: 2.3033492718973467

Epoch: 5| Step: 4
Training loss: 2.513678789138794
Validation loss: 2.261980241344821

Epoch: 5| Step: 5
Training loss: 2.7209296226501465
Validation loss: 2.2401129071430494

Epoch: 5| Step: 6
Training loss: 2.4030253887176514
Validation loss: 2.230347797434817

Epoch: 5| Step: 7
Training loss: 3.1456596851348877
Validation loss: 2.223079119959185

Epoch: 5| Step: 8
Training loss: 2.1945271492004395
Validation loss: 2.2203828929572977

Epoch: 5| Step: 9
Training loss: 2.377345323562622
Validation loss: 2.2296367858045842

Epoch: 5| Step: 10
Training loss: 2.292483329772949
Validation loss: 2.226132624892778

Epoch: 142| Step: 0
Training loss: 2.827876329421997
Validation loss: 2.2283255297650575

Epoch: 5| Step: 1
Training loss: 3.4203288555145264
Validation loss: 2.2265658301691853

Epoch: 5| Step: 2
Training loss: 2.802642345428467
Validation loss: 2.2274962215013403

Epoch: 5| Step: 3
Training loss: 2.0840439796447754
Validation loss: 2.2347186637181107

Epoch: 5| Step: 4
Training loss: 2.1050124168395996
Validation loss: 2.227825149413078

Epoch: 5| Step: 5
Training loss: 2.3268072605133057
Validation loss: 2.229203035754542

Epoch: 5| Step: 6
Training loss: 2.484586715698242
Validation loss: 2.2292537407208513

Epoch: 5| Step: 7
Training loss: 2.2248947620391846
Validation loss: 2.234094988915228

Epoch: 5| Step: 8
Training loss: 2.3745741844177246
Validation loss: 2.2314233087724253

Epoch: 5| Step: 9
Training loss: 2.6258418560028076
Validation loss: 2.2476919261358117

Epoch: 5| Step: 10
Training loss: 2.171323776245117
Validation loss: 2.2593025263919624

Epoch: 143| Step: 0
Training loss: 2.5968146324157715
Validation loss: 2.27690633907113

Epoch: 5| Step: 1
Training loss: 3.2318642139434814
Validation loss: 2.2755047710992957

Epoch: 5| Step: 2
Training loss: 2.255631923675537
Validation loss: 2.271188320652131

Epoch: 5| Step: 3
Training loss: 2.101144313812256
Validation loss: 2.2571992335780973

Epoch: 5| Step: 4
Training loss: 2.2592427730560303
Validation loss: 2.2597072662845736

Epoch: 5| Step: 5
Training loss: 2.2107367515563965
Validation loss: 2.273622202616866

Epoch: 5| Step: 6
Training loss: 3.2039923667907715
Validation loss: 2.289061430961855

Epoch: 5| Step: 7
Training loss: 2.4887516498565674
Validation loss: 2.2853711984490834

Epoch: 5| Step: 8
Training loss: 1.8968448638916016
Validation loss: 2.2521098929066814

Epoch: 5| Step: 9
Training loss: 2.866738796234131
Validation loss: 2.2364119996306715

Epoch: 5| Step: 10
Training loss: 2.547814130783081
Validation loss: 2.226615713488671

Epoch: 144| Step: 0
Training loss: 2.9294753074645996
Validation loss: 2.228060644160035

Epoch: 5| Step: 1
Training loss: 2.695009231567383
Validation loss: 2.2156875620606127

Epoch: 5| Step: 2
Training loss: 1.896270751953125
Validation loss: 2.228882138447095

Epoch: 5| Step: 3
Training loss: 2.171959161758423
Validation loss: 2.224290294031943

Epoch: 5| Step: 4
Training loss: 3.2028870582580566
Validation loss: 2.2120194332574004

Epoch: 5| Step: 5
Training loss: 2.6076292991638184
Validation loss: 2.2020658062350367

Epoch: 5| Step: 6
Training loss: 2.0777788162231445
Validation loss: 2.201725689313745

Epoch: 5| Step: 7
Training loss: 2.635895252227783
Validation loss: 2.2081484512616227

Epoch: 5| Step: 8
Training loss: 2.6132524013519287
Validation loss: 2.2178041806785007

Epoch: 5| Step: 9
Training loss: 2.3061208724975586
Validation loss: 2.2161617240598126

Epoch: 5| Step: 10
Training loss: 2.3247392177581787
Validation loss: 2.235051190981301

Epoch: 145| Step: 0
Training loss: 2.8576109409332275
Validation loss: 2.2503117489558395

Epoch: 5| Step: 1
Training loss: 2.7324647903442383
Validation loss: 2.2555728509861934

Epoch: 5| Step: 2
Training loss: 2.127835512161255
Validation loss: 2.2676059058917466

Epoch: 5| Step: 3
Training loss: 2.0616416931152344
Validation loss: 2.263820025228685

Epoch: 5| Step: 4
Training loss: 2.4999377727508545
Validation loss: 2.2777602749486126

Epoch: 5| Step: 5
Training loss: 2.772294759750366
Validation loss: 2.2553303395548174

Epoch: 5| Step: 6
Training loss: 1.904873251914978
Validation loss: 2.233625888824463

Epoch: 5| Step: 7
Training loss: 2.1043038368225098
Validation loss: 2.2327148504154657

Epoch: 5| Step: 8
Training loss: 2.6342921257019043
Validation loss: 2.218662092762609

Epoch: 5| Step: 9
Training loss: 2.8578286170959473
Validation loss: 2.204961374241819

Epoch: 5| Step: 10
Training loss: 2.8881618976593018
Validation loss: 2.202854606413072

Epoch: 146| Step: 0
Training loss: 1.9939321279525757
Validation loss: 2.1964474544730237

Epoch: 5| Step: 1
Training loss: 2.830322742462158
Validation loss: 2.2016247472455426

Epoch: 5| Step: 2
Training loss: 2.8170838356018066
Validation loss: 2.1991975461283038

Epoch: 5| Step: 3
Training loss: 2.4433865547180176
Validation loss: 2.2024103146727367

Epoch: 5| Step: 4
Training loss: 2.12581205368042
Validation loss: 2.20592938956394

Epoch: 5| Step: 5
Training loss: 2.9350481033325195
Validation loss: 2.206032819645379

Epoch: 5| Step: 6
Training loss: 1.6568708419799805
Validation loss: 2.1986105954775246

Epoch: 5| Step: 7
Training loss: 2.516752004623413
Validation loss: 2.2101588544025215

Epoch: 5| Step: 8
Training loss: 2.8648715019226074
Validation loss: 2.227448868495162

Epoch: 5| Step: 9
Training loss: 3.1549391746520996
Validation loss: 2.242546696816721

Epoch: 5| Step: 10
Training loss: 2.0257694721221924
Validation loss: 2.2582298324954126

Epoch: 147| Step: 0
Training loss: 3.115617275238037
Validation loss: 2.2962493178664998

Epoch: 5| Step: 1
Training loss: 3.162057876586914
Validation loss: 2.3219238558123187

Epoch: 5| Step: 2
Training loss: 2.0123767852783203
Validation loss: 2.2920622748713337

Epoch: 5| Step: 3
Training loss: 2.5803189277648926
Validation loss: 2.2814178030977965

Epoch: 5| Step: 4
Training loss: 2.27156138420105
Validation loss: 2.2650776857970865

Epoch: 5| Step: 5
Training loss: 2.176292657852173
Validation loss: 2.2522068280045704

Epoch: 5| Step: 6
Training loss: 2.4760797023773193
Validation loss: 2.2454321666430404

Epoch: 5| Step: 7
Training loss: 1.9852468967437744
Validation loss: 2.2382341789942917

Epoch: 5| Step: 8
Training loss: 3.0994482040405273
Validation loss: 2.2478838351465042

Epoch: 5| Step: 9
Training loss: 2.2060015201568604
Validation loss: 2.236526414912234

Epoch: 5| Step: 10
Training loss: 2.3702759742736816
Validation loss: 2.242159938299528

Epoch: 148| Step: 0
Training loss: 2.5390994548797607
Validation loss: 2.239245758261732

Epoch: 5| Step: 1
Training loss: 2.3492958545684814
Validation loss: 2.230115995612196

Epoch: 5| Step: 2
Training loss: 2.5878968238830566
Validation loss: 2.228232419618996

Epoch: 5| Step: 3
Training loss: 2.7028558254241943
Validation loss: 2.215403167150354

Epoch: 5| Step: 4
Training loss: 2.9380085468292236
Validation loss: 2.2110913799655054

Epoch: 5| Step: 5
Training loss: 1.7543971538543701
Validation loss: 2.2080644728035055

Epoch: 5| Step: 6
Training loss: 2.6257357597351074
Validation loss: 2.203148183002267

Epoch: 5| Step: 7
Training loss: 2.19977068901062
Validation loss: 2.21271090866417

Epoch: 5| Step: 8
Training loss: 2.332798480987549
Validation loss: 2.212306563572217

Epoch: 5| Step: 9
Training loss: 2.9043784141540527
Validation loss: 2.214105080532771

Epoch: 5| Step: 10
Training loss: 2.3714611530303955
Validation loss: 2.2277066117973736

Epoch: 149| Step: 0
Training loss: 2.560051679611206
Validation loss: 2.2345695444332656

Epoch: 5| Step: 1
Training loss: 2.832653045654297
Validation loss: 2.2320357189383557

Epoch: 5| Step: 2
Training loss: 2.0134031772613525
Validation loss: 2.2313336172411518

Epoch: 5| Step: 3
Training loss: 1.8579556941986084
Validation loss: 2.227243541389383

Epoch: 5| Step: 4
Training loss: 2.6199145317077637
Validation loss: 2.233448964293285

Epoch: 5| Step: 5
Training loss: 2.498056650161743
Validation loss: 2.2290854812950216

Epoch: 5| Step: 6
Training loss: 1.8983583450317383
Validation loss: 2.2441550377876527

Epoch: 5| Step: 7
Training loss: 2.165212631225586
Validation loss: 2.23550667044937

Epoch: 5| Step: 8
Training loss: 3.1616663932800293
Validation loss: 2.249254988085839

Epoch: 5| Step: 9
Training loss: 2.6638104915618896
Validation loss: 2.2433281483188754

Epoch: 5| Step: 10
Training loss: 2.8776988983154297
Validation loss: 2.244565727890179

Epoch: 150| Step: 0
Training loss: 2.938328266143799
Validation loss: 2.2243049029381043

Epoch: 5| Step: 1
Training loss: 2.1287951469421387
Validation loss: 2.2158871748114146

Epoch: 5| Step: 2
Training loss: 2.0052127838134766
Validation loss: 2.203549854217037

Epoch: 5| Step: 3
Training loss: 2.4219841957092285
Validation loss: 2.2001009730882544

Epoch: 5| Step: 4
Training loss: 2.8203623294830322
Validation loss: 2.196272021980696

Epoch: 5| Step: 5
Training loss: 2.3466086387634277
Validation loss: 2.1975188921856623

Epoch: 5| Step: 6
Training loss: 2.4867045879364014
Validation loss: 2.2211095184408207

Epoch: 5| Step: 7
Training loss: 2.4766767024993896
Validation loss: 2.220907547140634

Epoch: 5| Step: 8
Training loss: 2.6754376888275146
Validation loss: 2.2175035656139417

Epoch: 5| Step: 9
Training loss: 2.209455966949463
Validation loss: 2.2247150841579644

Epoch: 5| Step: 10
Training loss: 2.7111051082611084
Validation loss: 2.220743884322464

Epoch: 151| Step: 0
Training loss: 1.9475780725479126
Validation loss: 2.220269758214233

Epoch: 5| Step: 1
Training loss: 3.186596632003784
Validation loss: 2.2302691269946355

Epoch: 5| Step: 2
Training loss: 2.04754376411438
Validation loss: 2.2278989899543022

Epoch: 5| Step: 3
Training loss: 3.131354808807373
Validation loss: 2.2336773744193454

Epoch: 5| Step: 4
Training loss: 2.875117540359497
Validation loss: 2.220292491297568

Epoch: 5| Step: 5
Training loss: 2.112128734588623
Validation loss: 2.2081652354168635

Epoch: 5| Step: 6
Training loss: 2.426447629928589
Validation loss: 2.2013688190009004

Epoch: 5| Step: 7
Training loss: 1.9340908527374268
Validation loss: 2.2080120912162204

Epoch: 5| Step: 8
Training loss: 2.6484570503234863
Validation loss: 2.2120883926268546

Epoch: 5| Step: 9
Training loss: 2.5492801666259766
Validation loss: 2.217008964989775

Epoch: 5| Step: 10
Training loss: 2.212087631225586
Validation loss: 2.2143170756678425

Epoch: 152| Step: 0
Training loss: 2.271613597869873
Validation loss: 2.214072512042138

Epoch: 5| Step: 1
Training loss: 2.49446439743042
Validation loss: 2.2095763503864245

Epoch: 5| Step: 2
Training loss: 1.9320247173309326
Validation loss: 2.223013298485869

Epoch: 5| Step: 3
Training loss: 2.7035555839538574
Validation loss: 2.227050204430857

Epoch: 5| Step: 4
Training loss: 3.1856462955474854
Validation loss: 2.2346978213197444

Epoch: 5| Step: 5
Training loss: 2.8065686225891113
Validation loss: 2.2185166728112007

Epoch: 5| Step: 6
Training loss: 1.9490686655044556
Validation loss: 2.2280051272402526

Epoch: 5| Step: 7
Training loss: 2.292124032974243
Validation loss: 2.2206013766668176

Epoch: 5| Step: 8
Training loss: 2.485853672027588
Validation loss: 2.216017985856661

Epoch: 5| Step: 9
Training loss: 2.760646343231201
Validation loss: 2.2141354571106615

Epoch: 5| Step: 10
Training loss: 2.1662967205047607
Validation loss: 2.2076720653041715

Epoch: 153| Step: 0
Training loss: 2.6356682777404785
Validation loss: 2.2088299079607894

Epoch: 5| Step: 1
Training loss: 1.7328681945800781
Validation loss: 2.209026502024743

Epoch: 5| Step: 2
Training loss: 2.905086040496826
Validation loss: 2.2162966241118727

Epoch: 5| Step: 3
Training loss: 1.9498239755630493
Validation loss: 2.224939174549554

Epoch: 5| Step: 4
Training loss: 2.527757406234741
Validation loss: 2.228777572672854

Epoch: 5| Step: 5
Training loss: 2.458860397338867
Validation loss: 2.2272042100147535

Epoch: 5| Step: 6
Training loss: 2.062716484069824
Validation loss: 2.236840242980629

Epoch: 5| Step: 7
Training loss: 3.0091915130615234
Validation loss: 2.2199282030905447

Epoch: 5| Step: 8
Training loss: 3.2251362800598145
Validation loss: 2.2077225254428003

Epoch: 5| Step: 9
Training loss: 2.9069716930389404
Validation loss: 2.1991740939437703

Epoch: 5| Step: 10
Training loss: 1.4813889265060425
Validation loss: 2.1955058010675574

Epoch: 154| Step: 0
Training loss: 2.4796040058135986
Validation loss: 2.200158316601989

Epoch: 5| Step: 1
Training loss: 1.9485937356948853
Validation loss: 2.2090205172056794

Epoch: 5| Step: 2
Training loss: 2.2084972858428955
Validation loss: 2.221827368582449

Epoch: 5| Step: 3
Training loss: 1.9743034839630127
Validation loss: 2.262827016974008

Epoch: 5| Step: 4
Training loss: 2.5132339000701904
Validation loss: 2.266002269201381

Epoch: 5| Step: 5
Training loss: 2.8970046043395996
Validation loss: 2.243309377342142

Epoch: 5| Step: 6
Training loss: 2.203723907470703
Validation loss: 2.205793156418749

Epoch: 5| Step: 7
Training loss: 2.4972329139709473
Validation loss: 2.174204182881181

Epoch: 5| Step: 8
Training loss: 2.932607889175415
Validation loss: 2.179564646495286

Epoch: 5| Step: 9
Training loss: 2.7500481605529785
Validation loss: 2.1768451724001157

Epoch: 5| Step: 10
Training loss: 2.9312679767608643
Validation loss: 2.1888528203451507

Epoch: 155| Step: 0
Training loss: 1.7565845251083374
Validation loss: 2.183115436184791

Epoch: 5| Step: 1
Training loss: 2.7486844062805176
Validation loss: 2.1875864639077136

Epoch: 5| Step: 2
Training loss: 2.5711770057678223
Validation loss: 2.18807646536058

Epoch: 5| Step: 3
Training loss: 2.6293492317199707
Validation loss: 2.1956561021907355

Epoch: 5| Step: 4
Training loss: 1.9724254608154297
Validation loss: 2.203088196375037

Epoch: 5| Step: 5
Training loss: 2.330253839492798
Validation loss: 2.212708474487387

Epoch: 5| Step: 6
Training loss: 2.963444232940674
Validation loss: 2.202470246181693

Epoch: 5| Step: 7
Training loss: 2.2341573238372803
Validation loss: 2.1865198612213135

Epoch: 5| Step: 8
Training loss: 2.6330132484436035
Validation loss: 2.187339923715079

Epoch: 5| Step: 9
Training loss: 2.5530736446380615
Validation loss: 2.1839274488469607

Epoch: 5| Step: 10
Training loss: 2.7422878742218018
Validation loss: 2.188855996695898

Epoch: 156| Step: 0
Training loss: 1.960976243019104
Validation loss: 2.1812345391960553

Epoch: 5| Step: 1
Training loss: 2.288191080093384
Validation loss: 2.1997949436146724

Epoch: 5| Step: 2
Training loss: 2.668982982635498
Validation loss: 2.2016150874476277

Epoch: 5| Step: 3
Training loss: 3.0107803344726562
Validation loss: 2.2107843660539195

Epoch: 5| Step: 4
Training loss: 2.8990843296051025
Validation loss: 2.2065723865262923

Epoch: 5| Step: 5
Training loss: 2.2879226207733154
Validation loss: 2.1997424274362545

Epoch: 5| Step: 6
Training loss: 3.151090145111084
Validation loss: 2.191671450932821

Epoch: 5| Step: 7
Training loss: 2.322523593902588
Validation loss: 2.178778612485496

Epoch: 5| Step: 8
Training loss: 1.9908866882324219
Validation loss: 2.1823396221283944

Epoch: 5| Step: 9
Training loss: 2.111950397491455
Validation loss: 2.1862618102822253

Epoch: 5| Step: 10
Training loss: 2.3053932189941406
Validation loss: 2.189611234972554

Epoch: 157| Step: 0
Training loss: 2.14715576171875
Validation loss: 2.1898293777178695

Epoch: 5| Step: 1
Training loss: 1.9074122905731201
Validation loss: 2.225346424246347

Epoch: 5| Step: 2
Training loss: 2.4132843017578125
Validation loss: 2.24154019612138

Epoch: 5| Step: 3
Training loss: 2.4495162963867188
Validation loss: 2.2956352156977498

Epoch: 5| Step: 4
Training loss: 2.823093891143799
Validation loss: 2.3043948963124263

Epoch: 5| Step: 5
Training loss: 2.921090602874756
Validation loss: 2.2689208651101715

Epoch: 5| Step: 6
Training loss: 2.5537333488464355
Validation loss: 2.2228659634949057

Epoch: 5| Step: 7
Training loss: 2.620919704437256
Validation loss: 2.1824027363972

Epoch: 5| Step: 8
Training loss: 2.276524782180786
Validation loss: 2.181167333356796

Epoch: 5| Step: 9
Training loss: 2.1434872150421143
Validation loss: 2.1870972264197563

Epoch: 5| Step: 10
Training loss: 2.8865885734558105
Validation loss: 2.185581022693265

Epoch: 158| Step: 0
Training loss: 2.576218366622925
Validation loss: 2.1914700026153238

Epoch: 5| Step: 1
Training loss: 2.7644739151000977
Validation loss: 2.2084232043194514

Epoch: 5| Step: 2
Training loss: 2.3030288219451904
Validation loss: 2.2176707329288607

Epoch: 5| Step: 3
Training loss: 2.3935954570770264
Validation loss: 2.214759983042235

Epoch: 5| Step: 4
Training loss: 2.3243157863616943
Validation loss: 2.2192172055603354

Epoch: 5| Step: 5
Training loss: 2.2941012382507324
Validation loss: 2.214842370761338

Epoch: 5| Step: 6
Training loss: 2.539928674697876
Validation loss: 2.231426828651018

Epoch: 5| Step: 7
Training loss: 2.991374969482422
Validation loss: 2.220893916263375

Epoch: 5| Step: 8
Training loss: 2.0238654613494873
Validation loss: 2.2199442822446107

Epoch: 5| Step: 9
Training loss: 2.7571964263916016
Validation loss: 2.2240438281848864

Epoch: 5| Step: 10
Training loss: 2.1713197231292725
Validation loss: 2.230344036574005

Epoch: 159| Step: 0
Training loss: 2.4132044315338135
Validation loss: 2.2490643455136206

Epoch: 5| Step: 1
Training loss: 1.7587379217147827
Validation loss: 2.2512856004058674

Epoch: 5| Step: 2
Training loss: 2.2582902908325195
Validation loss: 2.225371391542496

Epoch: 5| Step: 3
Training loss: 2.864602565765381
Validation loss: 2.2224208642077703

Epoch: 5| Step: 4
Training loss: 2.8322689533233643
Validation loss: 2.2064722917413198

Epoch: 5| Step: 5
Training loss: 2.378621816635132
Validation loss: 2.2003143628438315

Epoch: 5| Step: 6
Training loss: 2.0316741466522217
Validation loss: 2.2092695287478867

Epoch: 5| Step: 7
Training loss: 2.281491756439209
Validation loss: 2.1984150153334423

Epoch: 5| Step: 8
Training loss: 2.638615846633911
Validation loss: 2.2023601224345546

Epoch: 5| Step: 9
Training loss: 3.0954554080963135
Validation loss: 2.197360530976326

Epoch: 5| Step: 10
Training loss: 2.5080862045288086
Validation loss: 2.2024050604912544

Epoch: 160| Step: 0
Training loss: 2.089418649673462
Validation loss: 2.1969800520968694

Epoch: 5| Step: 1
Training loss: 2.400235176086426
Validation loss: 2.2014248319851455

Epoch: 5| Step: 2
Training loss: 2.3525168895721436
Validation loss: 2.207178731118479

Epoch: 5| Step: 3
Training loss: 2.4954395294189453
Validation loss: 2.240079741324148

Epoch: 5| Step: 4
Training loss: 1.8493549823760986
Validation loss: 2.245790366203554

Epoch: 5| Step: 5
Training loss: 2.814441680908203
Validation loss: 2.245020351102275

Epoch: 5| Step: 6
Training loss: 2.3451008796691895
Validation loss: 2.2157729095028293

Epoch: 5| Step: 7
Training loss: 2.8147425651550293
Validation loss: 2.204833238355575

Epoch: 5| Step: 8
Training loss: 2.0472049713134766
Validation loss: 2.2029890860280683

Epoch: 5| Step: 9
Training loss: 3.4322714805603027
Validation loss: 2.1986220959694154

Epoch: 5| Step: 10
Training loss: 2.200807571411133
Validation loss: 2.1784848192686677

Epoch: 161| Step: 0
Training loss: 2.4043209552764893
Validation loss: 2.1818149192358858

Epoch: 5| Step: 1
Training loss: 2.5300991535186768
Validation loss: 2.1830335483756116

Epoch: 5| Step: 2
Training loss: 2.0471718311309814
Validation loss: 2.1782696785465365

Epoch: 5| Step: 3
Training loss: 2.4342522621154785
Validation loss: 2.169570581887358

Epoch: 5| Step: 4
Training loss: 2.3175532817840576
Validation loss: 2.1890703760167605

Epoch: 5| Step: 5
Training loss: 2.57965350151062
Validation loss: 2.190405063731696

Epoch: 5| Step: 6
Training loss: 1.888171911239624
Validation loss: 2.210173963218607

Epoch: 5| Step: 7
Training loss: 3.0339620113372803
Validation loss: 2.2148610366288053

Epoch: 5| Step: 8
Training loss: 2.477285623550415
Validation loss: 2.2064709458299863

Epoch: 5| Step: 9
Training loss: 2.4212708473205566
Validation loss: 2.2054925375087286

Epoch: 5| Step: 10
Training loss: 2.7390060424804688
Validation loss: 2.205284813398956

Epoch: 162| Step: 0
Training loss: 2.2426795959472656
Validation loss: 2.19757959535045

Epoch: 5| Step: 1
Training loss: 2.274686336517334
Validation loss: 2.18477875186551

Epoch: 5| Step: 2
Training loss: 2.0003745555877686
Validation loss: 2.1889708452327277

Epoch: 5| Step: 3
Training loss: 2.212381362915039
Validation loss: 2.1960838494762296

Epoch: 5| Step: 4
Training loss: 2.522135019302368
Validation loss: 2.1943745587461736

Epoch: 5| Step: 5
Training loss: 2.821255683898926
Validation loss: 2.213869963922808

Epoch: 5| Step: 6
Training loss: 2.0210373401641846
Validation loss: 2.2230520222776677

Epoch: 5| Step: 7
Training loss: 2.4731197357177734
Validation loss: 2.2168240957362677

Epoch: 5| Step: 8
Training loss: 3.4021897315979004
Validation loss: 2.215425891260947

Epoch: 5| Step: 9
Training loss: 2.223301887512207
Validation loss: 2.190934478595693

Epoch: 5| Step: 10
Training loss: 2.61527943611145
Validation loss: 2.1803191400343374

Epoch: 163| Step: 0
Training loss: 2.2558159828186035
Validation loss: 2.188494590020949

Epoch: 5| Step: 1
Training loss: 2.5074667930603027
Validation loss: 2.1655513522445515

Epoch: 5| Step: 2
Training loss: 2.8382344245910645
Validation loss: 2.172803709583898

Epoch: 5| Step: 3
Training loss: 2.0403411388397217
Validation loss: 2.1698358751112417

Epoch: 5| Step: 4
Training loss: 2.4897780418395996
Validation loss: 2.176474545591621

Epoch: 5| Step: 5
Training loss: 2.443774938583374
Validation loss: 2.1858929985313007

Epoch: 5| Step: 6
Training loss: 2.44205904006958
Validation loss: 2.169600277818659

Epoch: 5| Step: 7
Training loss: 2.382112503051758
Validation loss: 2.180786519922236

Epoch: 5| Step: 8
Training loss: 2.766307830810547
Validation loss: 2.175288079887308

Epoch: 5| Step: 9
Training loss: 2.2243034839630127
Validation loss: 2.171087645715283

Epoch: 5| Step: 10
Training loss: 2.261654853820801
Validation loss: 2.1817053133441555

Epoch: 164| Step: 0
Training loss: 2.586543560028076
Validation loss: 2.1747465979668403

Epoch: 5| Step: 1
Training loss: 2.907200574874878
Validation loss: 2.186398656137528

Epoch: 5| Step: 2
Training loss: 2.703094005584717
Validation loss: 2.1897443532943726

Epoch: 5| Step: 3
Training loss: 2.3646769523620605
Validation loss: 2.1734350445449993

Epoch: 5| Step: 4
Training loss: 2.798175573348999
Validation loss: 2.1668742241397982

Epoch: 5| Step: 5
Training loss: 1.9883792400360107
Validation loss: 2.1599931152918006

Epoch: 5| Step: 6
Training loss: 2.1722543239593506
Validation loss: 2.1605134138496975

Epoch: 5| Step: 7
Training loss: 2.1579160690307617
Validation loss: 2.154668959238196

Epoch: 5| Step: 8
Training loss: 2.0816402435302734
Validation loss: 2.154679622701419

Epoch: 5| Step: 9
Training loss: 2.2219810485839844
Validation loss: 2.1604937866169918

Epoch: 5| Step: 10
Training loss: 2.883857250213623
Validation loss: 2.188710717744725

Epoch: 165| Step: 0
Training loss: 2.2975611686706543
Validation loss: 2.21872442512102

Epoch: 5| Step: 1
Training loss: 2.2249300479888916
Validation loss: 2.2263431831072737

Epoch: 5| Step: 2
Training loss: 1.9357140064239502
Validation loss: 2.2326840508368706

Epoch: 5| Step: 3
Training loss: 2.1630406379699707
Validation loss: 2.211201687012949

Epoch: 5| Step: 4
Training loss: 2.4592790603637695
Validation loss: 2.2042516636592087

Epoch: 5| Step: 5
Training loss: 2.3458919525146484
Validation loss: 2.2063202345243065

Epoch: 5| Step: 6
Training loss: 2.951124429702759
Validation loss: 2.1839727509406304

Epoch: 5| Step: 7
Training loss: 2.522256374359131
Validation loss: 2.1720316948429232

Epoch: 5| Step: 8
Training loss: 2.6342689990997314
Validation loss: 2.1746589188934653

Epoch: 5| Step: 9
Training loss: 2.8484463691711426
Validation loss: 2.1756417661584835

Epoch: 5| Step: 10
Training loss: 2.2535932064056396
Validation loss: 2.1755087196186023

Epoch: 166| Step: 0
Training loss: 2.2530429363250732
Validation loss: 2.1637028545461674

Epoch: 5| Step: 1
Training loss: 2.545719861984253
Validation loss: 2.161552982945596

Epoch: 5| Step: 2
Training loss: 3.0639939308166504
Validation loss: 2.1675689733156593

Epoch: 5| Step: 3
Training loss: 1.9851011037826538
Validation loss: 2.1685419415914886

Epoch: 5| Step: 4
Training loss: 2.5138185024261475
Validation loss: 2.1703762713299004

Epoch: 5| Step: 5
Training loss: 2.4079537391662598
Validation loss: 2.1644208456880305

Epoch: 5| Step: 6
Training loss: 2.7078115940093994
Validation loss: 2.166601604030978

Epoch: 5| Step: 7
Training loss: 2.5452828407287598
Validation loss: 2.1745836427134853

Epoch: 5| Step: 8
Training loss: 1.7423614263534546
Validation loss: 2.1805748426786034

Epoch: 5| Step: 9
Training loss: 2.8417813777923584
Validation loss: 2.1975557163197506

Epoch: 5| Step: 10
Training loss: 2.1902756690979004
Validation loss: 2.2094870280194026

Epoch: 167| Step: 0
Training loss: 2.307013988494873
Validation loss: 2.2150756133499967

Epoch: 5| Step: 1
Training loss: 1.8223865032196045
Validation loss: 2.1881396924295733

Epoch: 5| Step: 2
Training loss: 2.2997872829437256
Validation loss: 2.189529131817561

Epoch: 5| Step: 3
Training loss: 2.9446585178375244
Validation loss: 2.1795241396914244

Epoch: 5| Step: 4
Training loss: 2.081342935562134
Validation loss: 2.1687805832073255

Epoch: 5| Step: 5
Training loss: 2.0174036026000977
Validation loss: 2.1589656388887795

Epoch: 5| Step: 6
Training loss: 2.1859657764434814
Validation loss: 2.164338255441317

Epoch: 5| Step: 7
Training loss: 2.631866931915283
Validation loss: 2.1677705395606255

Epoch: 5| Step: 8
Training loss: 2.7549216747283936
Validation loss: 2.186668372923328

Epoch: 5| Step: 9
Training loss: 2.386920928955078
Validation loss: 2.18817953525051

Epoch: 5| Step: 10
Training loss: 3.4044301509857178
Validation loss: 2.1734843933454124

Epoch: 168| Step: 0
Training loss: 1.5862674713134766
Validation loss: 2.1752208099570325

Epoch: 5| Step: 1
Training loss: 1.8911978006362915
Validation loss: 2.171540550006333

Epoch: 5| Step: 2
Training loss: 2.792990207672119
Validation loss: 2.1830273674380396

Epoch: 5| Step: 3
Training loss: 2.1408238410949707
Validation loss: 2.1664649440396215

Epoch: 5| Step: 4
Training loss: 2.5722057819366455
Validation loss: 2.168677763272357

Epoch: 5| Step: 5
Training loss: 2.3737502098083496
Validation loss: 2.162243443150674

Epoch: 5| Step: 6
Training loss: 1.8231751918792725
Validation loss: 2.1632528920327463

Epoch: 5| Step: 7
Training loss: 2.9936065673828125
Validation loss: 2.179945008729094

Epoch: 5| Step: 8
Training loss: 2.765913963317871
Validation loss: 2.1929557502910657

Epoch: 5| Step: 9
Training loss: 2.586761474609375
Validation loss: 2.2000986670935028

Epoch: 5| Step: 10
Training loss: 3.2555735111236572
Validation loss: 2.1965035264210035

Epoch: 169| Step: 0
Training loss: 2.4855971336364746
Validation loss: 2.189680337905884

Epoch: 5| Step: 1
Training loss: 2.168182849884033
Validation loss: 2.191486458624563

Epoch: 5| Step: 2
Training loss: 2.3156790733337402
Validation loss: 2.17489755153656

Epoch: 5| Step: 3
Training loss: 2.0083155632019043
Validation loss: 2.164803943326396

Epoch: 5| Step: 4
Training loss: 2.5746965408325195
Validation loss: 2.1705111713819605

Epoch: 5| Step: 5
Training loss: 2.14393949508667
Validation loss: 2.181495153775779

Epoch: 5| Step: 6
Training loss: 2.4216957092285156
Validation loss: 2.166068261669528

Epoch: 5| Step: 7
Training loss: 2.763062000274658
Validation loss: 2.171325227265717

Epoch: 5| Step: 8
Training loss: 2.6992850303649902
Validation loss: 2.1704788938645394

Epoch: 5| Step: 9
Training loss: 1.9147536754608154
Validation loss: 2.185742812771951

Epoch: 5| Step: 10
Training loss: 3.162456750869751
Validation loss: 2.1957143045240834

Epoch: 170| Step: 0
Training loss: 1.4595844745635986
Validation loss: 2.2127886587573635

Epoch: 5| Step: 1
Training loss: 2.266340970993042
Validation loss: 2.225390375301402

Epoch: 5| Step: 2
Training loss: 1.994638442993164
Validation loss: 2.2044066306083434

Epoch: 5| Step: 3
Training loss: 2.8392891883850098
Validation loss: 2.199809928094187

Epoch: 5| Step: 4
Training loss: 2.9095590114593506
Validation loss: 2.1758083528087986

Epoch: 5| Step: 5
Training loss: 2.8007915019989014
Validation loss: 2.1760627262053953

Epoch: 5| Step: 6
Training loss: 2.347672939300537
Validation loss: 2.185205862086306

Epoch: 5| Step: 7
Training loss: 2.6588263511657715
Validation loss: 2.187735644719934

Epoch: 5| Step: 8
Training loss: 2.197779417037964
Validation loss: 2.1915194270431355

Epoch: 5| Step: 9
Training loss: 2.319610118865967
Validation loss: 2.191813161296229

Epoch: 5| Step: 10
Training loss: 2.7971882820129395
Validation loss: 2.1966803394338137

Epoch: 171| Step: 0
Training loss: 3.1843810081481934
Validation loss: 2.1920689793043238

Epoch: 5| Step: 1
Training loss: 2.6940886974334717
Validation loss: 2.190136096810782

Epoch: 5| Step: 2
Training loss: 1.9231021404266357
Validation loss: 2.17761589378439

Epoch: 5| Step: 3
Training loss: 1.8579318523406982
Validation loss: 2.1736268766464724

Epoch: 5| Step: 4
Training loss: 2.6472649574279785
Validation loss: 2.1782343246603526

Epoch: 5| Step: 5
Training loss: 1.842079758644104
Validation loss: 2.1724935500852522

Epoch: 5| Step: 6
Training loss: 2.432349443435669
Validation loss: 2.169290599002633

Epoch: 5| Step: 7
Training loss: 2.1947274208068848
Validation loss: 2.1717194408498783

Epoch: 5| Step: 8
Training loss: 2.248302459716797
Validation loss: 2.173146358100317

Epoch: 5| Step: 9
Training loss: 2.9784042835235596
Validation loss: 2.1768398028548046

Epoch: 5| Step: 10
Training loss: 2.431500196456909
Validation loss: 2.1826904358402377

Epoch: 172| Step: 0
Training loss: 2.250490665435791
Validation loss: 2.180080912446463

Epoch: 5| Step: 1
Training loss: 2.1762313842773438
Validation loss: 2.1759760572064306

Epoch: 5| Step: 2
Training loss: 2.3659846782684326
Validation loss: 2.2052182689789803

Epoch: 5| Step: 3
Training loss: 2.1694846153259277
Validation loss: 2.23168917112453

Epoch: 5| Step: 4
Training loss: 2.394524097442627
Validation loss: 2.240180494964764

Epoch: 5| Step: 5
Training loss: 2.6833577156066895
Validation loss: 2.26239655094762

Epoch: 5| Step: 6
Training loss: 3.0247371196746826
Validation loss: 2.221450731318484

Epoch: 5| Step: 7
Training loss: 2.661881446838379
Validation loss: 2.1837300792817147

Epoch: 5| Step: 8
Training loss: 2.198784351348877
Validation loss: 2.173110177440028

Epoch: 5| Step: 9
Training loss: 2.2959671020507812
Validation loss: 2.160439714308708

Epoch: 5| Step: 10
Training loss: 2.264413356781006
Validation loss: 2.17834258848621

Epoch: 173| Step: 0
Training loss: 2.179936170578003
Validation loss: 2.1686141183299403

Epoch: 5| Step: 1
Training loss: 2.1232540607452393
Validation loss: 2.155267873118001

Epoch: 5| Step: 2
Training loss: 2.202197551727295
Validation loss: 2.1675681632052184

Epoch: 5| Step: 3
Training loss: 1.9286038875579834
Validation loss: 2.152559954632995

Epoch: 5| Step: 4
Training loss: 2.777893543243408
Validation loss: 2.1512080020801996

Epoch: 5| Step: 5
Training loss: 2.7178597450256348
Validation loss: 2.1467415107193815

Epoch: 5| Step: 6
Training loss: 1.9288997650146484
Validation loss: 2.1420860264890935

Epoch: 5| Step: 7
Training loss: 1.9818671941757202
Validation loss: 2.1508218716549616

Epoch: 5| Step: 8
Training loss: 2.393394947052002
Validation loss: 2.1557119610489055

Epoch: 5| Step: 9
Training loss: 3.184682846069336
Validation loss: 2.1797130979517454

Epoch: 5| Step: 10
Training loss: 3.285679578781128
Validation loss: 2.1964936128226658

Epoch: 174| Step: 0
Training loss: 2.366396427154541
Validation loss: 2.234793798897856

Epoch: 5| Step: 1
Training loss: 2.1613597869873047
Validation loss: 2.2126395035815496

Epoch: 5| Step: 2
Training loss: 2.31809139251709
Validation loss: 2.228776281879794

Epoch: 5| Step: 3
Training loss: 2.3760952949523926
Validation loss: 2.204912454851212

Epoch: 5| Step: 4
Training loss: 2.4826807975769043
Validation loss: 2.1858332208407822

Epoch: 5| Step: 5
Training loss: 2.293334484100342
Validation loss: 2.171048943714429

Epoch: 5| Step: 6
Training loss: 2.731585741043091
Validation loss: 2.1750152226417296

Epoch: 5| Step: 7
Training loss: 2.8506176471710205
Validation loss: 2.171790692114061

Epoch: 5| Step: 8
Training loss: 2.463585138320923
Validation loss: 2.197162852492384

Epoch: 5| Step: 9
Training loss: 2.0538294315338135
Validation loss: 2.2085150800725466

Epoch: 5| Step: 10
Training loss: 2.486588716506958
Validation loss: 2.2163784529573176

Epoch: 175| Step: 0
Training loss: 2.4204487800598145
Validation loss: 2.1942984468193463

Epoch: 5| Step: 1
Training loss: 3.034224033355713
Validation loss: 2.169154969594812

Epoch: 5| Step: 2
Training loss: 2.629927158355713
Validation loss: 2.149911485692506

Epoch: 5| Step: 3
Training loss: 2.5532264709472656
Validation loss: 2.1535769149821293

Epoch: 5| Step: 4
Training loss: 1.909399390220642
Validation loss: 2.199810146003641

Epoch: 5| Step: 5
Training loss: 2.2500929832458496
Validation loss: 2.250467323487805

Epoch: 5| Step: 6
Training loss: 2.4793009757995605
Validation loss: 2.2761253720970562

Epoch: 5| Step: 7
Training loss: 2.1397836208343506
Validation loss: 2.2403793642597813

Epoch: 5| Step: 8
Training loss: 2.4417083263397217
Validation loss: 2.2124430364178074

Epoch: 5| Step: 9
Training loss: 2.514540195465088
Validation loss: 2.159238202597505

Epoch: 5| Step: 10
Training loss: 2.2425053119659424
Validation loss: 2.147369264274515

Epoch: 176| Step: 0
Training loss: 2.7044918537139893
Validation loss: 2.1446663243796236

Epoch: 5| Step: 1
Training loss: 2.5395936965942383
Validation loss: 2.1409282504871325

Epoch: 5| Step: 2
Training loss: 2.546764373779297
Validation loss: 2.1421333128406155

Epoch: 5| Step: 3
Training loss: 2.3374180793762207
Validation loss: 2.130408997176796

Epoch: 5| Step: 4
Training loss: 2.7804465293884277
Validation loss: 2.138467963023852

Epoch: 5| Step: 5
Training loss: 2.0351922512054443
Validation loss: 2.14683961355558

Epoch: 5| Step: 6
Training loss: 2.459859848022461
Validation loss: 2.152854359278115

Epoch: 5| Step: 7
Training loss: 2.602713108062744
Validation loss: 2.1488095791109147

Epoch: 5| Step: 8
Training loss: 2.126396417617798
Validation loss: 2.1430712566580823

Epoch: 5| Step: 9
Training loss: 1.6697015762329102
Validation loss: 2.145744712122025

Epoch: 5| Step: 10
Training loss: 2.6977107524871826
Validation loss: 2.149323086584768

Epoch: 177| Step: 0
Training loss: 2.6222221851348877
Validation loss: 2.171528470131659

Epoch: 5| Step: 1
Training loss: 2.3772521018981934
Validation loss: 2.205465985882667

Epoch: 5| Step: 2
Training loss: 2.7549095153808594
Validation loss: 2.2262707833320863

Epoch: 5| Step: 3
Training loss: 2.2137608528137207
Validation loss: 2.1972934776736843

Epoch: 5| Step: 4
Training loss: 1.9367561340332031
Validation loss: 2.1734042167663574

Epoch: 5| Step: 5
Training loss: 2.944927930831909
Validation loss: 2.151184735759612

Epoch: 5| Step: 6
Training loss: 1.8518133163452148
Validation loss: 2.1405225056473927

Epoch: 5| Step: 7
Training loss: 3.027036666870117
Validation loss: 2.1302459868051673

Epoch: 5| Step: 8
Training loss: 2.2669148445129395
Validation loss: 2.145940290984287

Epoch: 5| Step: 9
Training loss: 1.9993617534637451
Validation loss: 2.1266618928601666

Epoch: 5| Step: 10
Training loss: 2.3600659370422363
Validation loss: 2.139569849096319

Epoch: 178| Step: 0
Training loss: 1.9765377044677734
Validation loss: 2.1551711764386905

Epoch: 5| Step: 1
Training loss: 1.8211958408355713
Validation loss: 2.1769526158609698

Epoch: 5| Step: 2
Training loss: 2.342865228652954
Validation loss: 2.2344203046573106

Epoch: 5| Step: 3
Training loss: 2.8240129947662354
Validation loss: 2.228909238692253

Epoch: 5| Step: 4
Training loss: 2.2296509742736816
Validation loss: 2.25006075187396

Epoch: 5| Step: 5
Training loss: 2.706204891204834
Validation loss: 2.233279788365928

Epoch: 5| Step: 6
Training loss: 2.53566312789917
Validation loss: 2.2060773770014444

Epoch: 5| Step: 7
Training loss: 2.0790791511535645
Validation loss: 2.1771186167193997

Epoch: 5| Step: 8
Training loss: 2.675844192504883
Validation loss: 2.1646141941829393

Epoch: 5| Step: 9
Training loss: 2.791243076324463
Validation loss: 2.141138147282344

Epoch: 5| Step: 10
Training loss: 2.5034327507019043
Validation loss: 2.1246431591690227

Epoch: 179| Step: 0
Training loss: 2.523606061935425
Validation loss: 2.130915500784433

Epoch: 5| Step: 1
Training loss: 2.866722822189331
Validation loss: 2.1401581584766345

Epoch: 5| Step: 2
Training loss: 1.980040192604065
Validation loss: 2.131765970619776

Epoch: 5| Step: 3
Training loss: 2.2153737545013428
Validation loss: 2.13812820501225

Epoch: 5| Step: 4
Training loss: 1.7615066766738892
Validation loss: 2.1406436120310137

Epoch: 5| Step: 5
Training loss: 2.3860814571380615
Validation loss: 2.136288055809595

Epoch: 5| Step: 6
Training loss: 2.804079055786133
Validation loss: 2.165495599469831

Epoch: 5| Step: 7
Training loss: 2.648073196411133
Validation loss: 2.204944331158874

Epoch: 5| Step: 8
Training loss: 2.5851995944976807
Validation loss: 2.294235178219375

Epoch: 5| Step: 9
Training loss: 2.5068740844726562
Validation loss: 2.316776996017784

Epoch: 5| Step: 10
Training loss: 2.1390085220336914
Validation loss: 2.308805306752523

Epoch: 180| Step: 0
Training loss: 2.602342128753662
Validation loss: 2.301277296517485

Epoch: 5| Step: 1
Training loss: 2.427913188934326
Validation loss: 2.2965033643989154

Epoch: 5| Step: 2
Training loss: 1.8640811443328857
Validation loss: 2.277098690309832

Epoch: 5| Step: 3
Training loss: 2.8039932250976562
Validation loss: 2.2439814306074575

Epoch: 5| Step: 4
Training loss: 2.1889359951019287
Validation loss: 2.1777458216554377

Epoch: 5| Step: 5
Training loss: 2.794800281524658
Validation loss: 2.1467815214587795

Epoch: 5| Step: 6
Training loss: 3.104527235031128
Validation loss: 2.1367775881162254

Epoch: 5| Step: 7
Training loss: 2.147526741027832
Validation loss: 2.1305754928178686

Epoch: 5| Step: 8
Training loss: 1.9269955158233643
Validation loss: 2.114376791061894

Epoch: 5| Step: 9
Training loss: 2.391021728515625
Validation loss: 2.1278244731246785

Epoch: 5| Step: 10
Training loss: 1.9955909252166748
Validation loss: 2.141029906529252

Epoch: 181| Step: 0
Training loss: 2.516423463821411
Validation loss: 2.142707340178951

Epoch: 5| Step: 1
Training loss: 2.507429599761963
Validation loss: 2.1511076137583744

Epoch: 5| Step: 2
Training loss: 2.479078769683838
Validation loss: 2.1579986131319435

Epoch: 5| Step: 3
Training loss: 1.4625766277313232
Validation loss: 2.1674513175923336

Epoch: 5| Step: 4
Training loss: 2.2903223037719727
Validation loss: 2.178734443520987

Epoch: 5| Step: 5
Training loss: 2.4056620597839355
Validation loss: 2.208771203153877

Epoch: 5| Step: 6
Training loss: 2.733320713043213
Validation loss: 2.220232980225676

Epoch: 5| Step: 7
Training loss: 2.314497470855713
Validation loss: 2.2215079210137807

Epoch: 5| Step: 8
Training loss: 2.7511696815490723
Validation loss: 2.2310320138931274

Epoch: 5| Step: 9
Training loss: 2.5819907188415527
Validation loss: 2.220410236748316

Epoch: 5| Step: 10
Training loss: 2.434951066970825
Validation loss: 2.2136809569533153

Epoch: 182| Step: 0
Training loss: 2.8351993560791016
Validation loss: 2.1809162709020797

Epoch: 5| Step: 1
Training loss: 2.506737470626831
Validation loss: 2.177551054185437

Epoch: 5| Step: 2
Training loss: 2.101144552230835
Validation loss: 2.1522624185008388

Epoch: 5| Step: 3
Training loss: 2.190246343612671
Validation loss: 2.154380977794688

Epoch: 5| Step: 4
Training loss: 1.973318338394165
Validation loss: 2.1461994365979264

Epoch: 5| Step: 5
Training loss: 2.935163736343384
Validation loss: 2.1576555698148665

Epoch: 5| Step: 6
Training loss: 2.1560921669006348
Validation loss: 2.1542830595406155

Epoch: 5| Step: 7
Training loss: 2.3408966064453125
Validation loss: 2.1630720297495523

Epoch: 5| Step: 8
Training loss: 2.5701165199279785
Validation loss: 2.2159945580267135

Epoch: 5| Step: 9
Training loss: 2.5576446056365967
Validation loss: 2.1966587087159515

Epoch: 5| Step: 10
Training loss: 1.9710897207260132
Validation loss: 2.183865947108115

Epoch: 183| Step: 0
Training loss: 2.1524786949157715
Validation loss: 2.1561372100666003

Epoch: 5| Step: 1
Training loss: 2.406073808670044
Validation loss: 2.1458895385906263

Epoch: 5| Step: 2
Training loss: 2.2977874279022217
Validation loss: 2.1431008359437347

Epoch: 5| Step: 3
Training loss: 2.4860944747924805
Validation loss: 2.1305208308722383

Epoch: 5| Step: 4
Training loss: 2.969081163406372
Validation loss: 2.130838330074023

Epoch: 5| Step: 5
Training loss: 2.072768449783325
Validation loss: 2.1347439212183796

Epoch: 5| Step: 6
Training loss: 2.734339952468872
Validation loss: 2.1367715379243255

Epoch: 5| Step: 7
Training loss: 2.2696645259857178
Validation loss: 2.1451708911567606

Epoch: 5| Step: 8
Training loss: 2.214850902557373
Validation loss: 2.1470222165507655

Epoch: 5| Step: 9
Training loss: 2.592297315597534
Validation loss: 2.1514243284861245

Epoch: 5| Step: 10
Training loss: 2.1987359523773193
Validation loss: 2.1654506626949517

Epoch: 184| Step: 0
Training loss: 2.3747005462646484
Validation loss: 2.1722509937901653

Epoch: 5| Step: 1
Training loss: 1.941603422164917
Validation loss: 2.1844756462240733

Epoch: 5| Step: 2
Training loss: 2.682744264602661
Validation loss: 2.1822400298169864

Epoch: 5| Step: 3
Training loss: 2.6924564838409424
Validation loss: 2.1895065051253124

Epoch: 5| Step: 4
Training loss: 1.9190349578857422
Validation loss: 2.1868676575281287

Epoch: 5| Step: 5
Training loss: 2.470667600631714
Validation loss: 2.2078603621452086

Epoch: 5| Step: 6
Training loss: 1.8551762104034424
Validation loss: 2.1987408104763237

Epoch: 5| Step: 7
Training loss: 2.8599019050598145
Validation loss: 2.1939356557784544

Epoch: 5| Step: 8
Training loss: 2.7984209060668945
Validation loss: 2.176645230221492

Epoch: 5| Step: 9
Training loss: 1.9691661596298218
Validation loss: 2.140945893461986

Epoch: 5| Step: 10
Training loss: 2.5275845527648926
Validation loss: 2.1332579825514104

Epoch: 185| Step: 0
Training loss: 2.412842273712158
Validation loss: 2.139378654059543

Epoch: 5| Step: 1
Training loss: 2.5058882236480713
Validation loss: 2.139599935982817

Epoch: 5| Step: 2
Training loss: 2.056267738342285
Validation loss: 2.143974942545737

Epoch: 5| Step: 3
Training loss: 2.444241762161255
Validation loss: 2.1465306487134708

Epoch: 5| Step: 4
Training loss: 2.153679370880127
Validation loss: 2.1408581208157282

Epoch: 5| Step: 5
Training loss: 2.4900269508361816
Validation loss: 2.1254141151264148

Epoch: 5| Step: 6
Training loss: 2.622307777404785
Validation loss: 2.145820633057625

Epoch: 5| Step: 7
Training loss: 2.694732189178467
Validation loss: 2.1573883769332722

Epoch: 5| Step: 8
Training loss: 2.1406311988830566
Validation loss: 2.211601593161142

Epoch: 5| Step: 9
Training loss: 2.8546299934387207
Validation loss: 2.2386604124499905

Epoch: 5| Step: 10
Training loss: 1.6965919733047485
Validation loss: 2.2686337219771517

Epoch: 186| Step: 0
Training loss: 2.0325846672058105
Validation loss: 2.254530042730352

Epoch: 5| Step: 1
Training loss: 2.8569843769073486
Validation loss: 2.2293212234332995

Epoch: 5| Step: 2
Training loss: 3.0285422801971436
Validation loss: 2.2163672139567714

Epoch: 5| Step: 3
Training loss: 2.6087186336517334
Validation loss: 2.1736837240957443

Epoch: 5| Step: 4
Training loss: 2.45794677734375
Validation loss: 2.144986293649161

Epoch: 5| Step: 5
Training loss: 2.2369980812072754
Validation loss: 2.1352974394316315

Epoch: 5| Step: 6
Training loss: 2.470541477203369
Validation loss: 2.142596544757966

Epoch: 5| Step: 7
Training loss: 2.137073516845703
Validation loss: 2.1481871681828655

Epoch: 5| Step: 8
Training loss: 2.077122211456299
Validation loss: 2.1778017897759714

Epoch: 5| Step: 9
Training loss: 2.204449415206909
Validation loss: 2.19026429166076

Epoch: 5| Step: 10
Training loss: 2.0929901599884033
Validation loss: 2.193854785734607

Epoch: 187| Step: 0
Training loss: 2.0528035163879395
Validation loss: 2.173900329938499

Epoch: 5| Step: 1
Training loss: 2.6107256412506104
Validation loss: 2.157486415678455

Epoch: 5| Step: 2
Training loss: 1.8168156147003174
Validation loss: 2.170381360156562

Epoch: 5| Step: 3
Training loss: 2.615654706954956
Validation loss: 2.197873930777273

Epoch: 5| Step: 4
Training loss: 2.7392566204071045
Validation loss: 2.2071997183625416

Epoch: 5| Step: 5
Training loss: 2.1974916458129883
Validation loss: 2.218678402644332

Epoch: 5| Step: 6
Training loss: 2.376025438308716
Validation loss: 2.2152897273340533

Epoch: 5| Step: 7
Training loss: 2.867189884185791
Validation loss: 2.2113322058031635

Epoch: 5| Step: 8
Training loss: 2.423440456390381
Validation loss: 2.189168946717375

Epoch: 5| Step: 9
Training loss: 2.0646309852600098
Validation loss: 2.170805549108854

Epoch: 5| Step: 10
Training loss: 2.469219207763672
Validation loss: 2.155223297816451

Epoch: 188| Step: 0
Training loss: 1.8646122217178345
Validation loss: 2.154621566495588

Epoch: 5| Step: 1
Training loss: 2.4787535667419434
Validation loss: 2.149982503024481

Epoch: 5| Step: 2
Training loss: 2.5313663482666016
Validation loss: 2.152083440493512

Epoch: 5| Step: 3
Training loss: 1.8747594356536865
Validation loss: 2.1672111583012406

Epoch: 5| Step: 4
Training loss: 2.2377898693084717
Validation loss: 2.1988361215078704

Epoch: 5| Step: 5
Training loss: 2.8263192176818848
Validation loss: 2.212590153499316

Epoch: 5| Step: 6
Training loss: 2.4442667961120605
Validation loss: 2.2149379740479174

Epoch: 5| Step: 7
Training loss: 2.6987507343292236
Validation loss: 2.235632781059511

Epoch: 5| Step: 8
Training loss: 2.6404943466186523
Validation loss: 2.2404452523877545

Epoch: 5| Step: 9
Training loss: 2.775310754776001
Validation loss: 2.208287823584772

Epoch: 5| Step: 10
Training loss: 1.9411044120788574
Validation loss: 2.2038891546187864

Epoch: 189| Step: 0
Training loss: 2.088606595993042
Validation loss: 2.2279056426017516

Epoch: 5| Step: 1
Training loss: 2.248413562774658
Validation loss: 2.2790788681276384

Epoch: 5| Step: 2
Training loss: 2.7315409183502197
Validation loss: 2.3285517205474195

Epoch: 5| Step: 3
Training loss: 2.6837754249572754
Validation loss: 2.359272790211503

Epoch: 5| Step: 4
Training loss: 2.881035327911377
Validation loss: 2.344675992124824

Epoch: 5| Step: 5
Training loss: 2.910857915878296
Validation loss: 2.3363093817105858

Epoch: 5| Step: 6
Training loss: 1.7326761484146118
Validation loss: 2.320210929839842

Epoch: 5| Step: 7
Training loss: 2.3819985389709473
Validation loss: 2.258637576974848

Epoch: 5| Step: 8
Training loss: 2.7351937294006348
Validation loss: 2.199949872109198

Epoch: 5| Step: 9
Training loss: 2.467592716217041
Validation loss: 2.1582084394270376

Epoch: 5| Step: 10
Training loss: 2.129277467727661
Validation loss: 2.1571874669803086

Epoch: 190| Step: 0
Training loss: 2.9455385208129883
Validation loss: 2.1609357710807555

Epoch: 5| Step: 1
Training loss: 2.4213128089904785
Validation loss: 2.1724501732856996

Epoch: 5| Step: 2
Training loss: 2.032670497894287
Validation loss: 2.1805458543121174

Epoch: 5| Step: 3
Training loss: 2.4576056003570557
Validation loss: 2.18853134493674

Epoch: 5| Step: 4
Training loss: 2.893831253051758
Validation loss: 2.2088187304876183

Epoch: 5| Step: 5
Training loss: 2.67488956451416
Validation loss: 2.214110897433373

Epoch: 5| Step: 6
Training loss: 2.2611944675445557
Validation loss: 2.1998536586761475

Epoch: 5| Step: 7
Training loss: 2.068056106567383
Validation loss: 2.1657957056517243

Epoch: 5| Step: 8
Training loss: 2.2353641986846924
Validation loss: 2.1582396722609

Epoch: 5| Step: 9
Training loss: 2.4399654865264893
Validation loss: 2.1534411317558697

Epoch: 5| Step: 10
Training loss: 2.073469400405884
Validation loss: 2.1316807218777236

Epoch: 191| Step: 0
Training loss: 2.107304573059082
Validation loss: 2.127515095536427

Epoch: 5| Step: 1
Training loss: 1.8538944721221924
Validation loss: 2.1176348655454573

Epoch: 5| Step: 2
Training loss: 2.8909316062927246
Validation loss: 2.12115115504111

Epoch: 5| Step: 3
Training loss: 2.182374954223633
Validation loss: 2.1361571486278246

Epoch: 5| Step: 4
Training loss: 2.2987220287323
Validation loss: 2.1575165205104376

Epoch: 5| Step: 5
Training loss: 2.030614137649536
Validation loss: 2.179098070308726

Epoch: 5| Step: 6
Training loss: 2.3613836765289307
Validation loss: 2.1655296843539

Epoch: 5| Step: 7
Training loss: 2.8223485946655273
Validation loss: 2.146230338722147

Epoch: 5| Step: 8
Training loss: 2.791675567626953
Validation loss: 2.1233882955325547

Epoch: 5| Step: 9
Training loss: 2.2646565437316895
Validation loss: 2.116135972802357

Epoch: 5| Step: 10
Training loss: 2.403648853302002
Validation loss: 2.1364152431488037

Epoch: 192| Step: 0
Training loss: 1.665667176246643
Validation loss: 2.151496251424154

Epoch: 5| Step: 1
Training loss: 2.067136287689209
Validation loss: 2.1515976126476

Epoch: 5| Step: 2
Training loss: 2.2101693153381348
Validation loss: 2.164351560736215

Epoch: 5| Step: 3
Training loss: 2.5629751682281494
Validation loss: 2.1788342268236223

Epoch: 5| Step: 4
Training loss: 2.7609524726867676
Validation loss: 2.1965213847416702

Epoch: 5| Step: 5
Training loss: 2.843787670135498
Validation loss: 2.2187417963499665

Epoch: 5| Step: 6
Training loss: 3.1576220989227295
Validation loss: 2.2002814123707433

Epoch: 5| Step: 7
Training loss: 2.2489752769470215
Validation loss: 2.2071379769232964

Epoch: 5| Step: 8
Training loss: 1.821197271347046
Validation loss: 2.1884451553385746

Epoch: 5| Step: 9
Training loss: 1.7862727642059326
Validation loss: 2.1754994110394548

Epoch: 5| Step: 10
Training loss: 3.06620717048645
Validation loss: 2.168838939359111

Epoch: 193| Step: 0
Training loss: 2.6734366416931152
Validation loss: 2.145599260125109

Epoch: 5| Step: 1
Training loss: 1.508614420890808
Validation loss: 2.1437403232820573

Epoch: 5| Step: 2
Training loss: 2.465341091156006
Validation loss: 2.132184320880521

Epoch: 5| Step: 3
Training loss: 2.5369069576263428
Validation loss: 2.133220191924803

Epoch: 5| Step: 4
Training loss: 2.1457901000976562
Validation loss: 2.124167278248777

Epoch: 5| Step: 5
Training loss: 2.2702579498291016
Validation loss: 2.118134549869004

Epoch: 5| Step: 6
Training loss: 2.3899943828582764
Validation loss: 2.1349714007428897

Epoch: 5| Step: 7
Training loss: 2.4317829608917236
Validation loss: 2.14012316478196

Epoch: 5| Step: 8
Training loss: 2.57932710647583
Validation loss: 2.1495422304317517

Epoch: 5| Step: 9
Training loss: 2.8218469619750977
Validation loss: 2.1365043270972466

Epoch: 5| Step: 10
Training loss: 1.9864201545715332
Validation loss: 2.1265258327607186

Epoch: 194| Step: 0
Training loss: 2.2313246726989746
Validation loss: 2.123422145843506

Epoch: 5| Step: 1
Training loss: 2.5651185512542725
Validation loss: 2.1141236930765133

Epoch: 5| Step: 2
Training loss: 2.126971960067749
Validation loss: 2.118291001166067

Epoch: 5| Step: 3
Training loss: 2.49526047706604
Validation loss: 2.1287711666476343

Epoch: 5| Step: 4
Training loss: 2.2771246433258057
Validation loss: 2.1325794061024985

Epoch: 5| Step: 5
Training loss: 2.007061004638672
Validation loss: 2.1151390280774844

Epoch: 5| Step: 6
Training loss: 2.506235122680664
Validation loss: 2.1111862403090282

Epoch: 5| Step: 7
Training loss: 2.736534833908081
Validation loss: 2.103914330082555

Epoch: 5| Step: 8
Training loss: 2.3519203662872314
Validation loss: 2.1048082843903573

Epoch: 5| Step: 9
Training loss: 1.9099547863006592
Validation loss: 2.106104944341926

Epoch: 5| Step: 10
Training loss: 2.4849517345428467
Validation loss: 2.1146112744526198

Epoch: 195| Step: 0
Training loss: 2.817467212677002
Validation loss: 2.127094340580766

Epoch: 5| Step: 1
Training loss: 2.375059127807617
Validation loss: 2.1617280437100317

Epoch: 5| Step: 2
Training loss: 2.165334701538086
Validation loss: 2.1919356161548245

Epoch: 5| Step: 3
Training loss: 2.150794744491577
Validation loss: 2.2161462050612255

Epoch: 5| Step: 4
Training loss: 2.7301814556121826
Validation loss: 2.226967029674079

Epoch: 5| Step: 5
Training loss: 1.9514154195785522
Validation loss: 2.2306048447085964

Epoch: 5| Step: 6
Training loss: 2.2840468883514404
Validation loss: 2.2427779218201995

Epoch: 5| Step: 7
Training loss: 2.1202633380889893
Validation loss: 2.224574806869671

Epoch: 5| Step: 8
Training loss: 2.3482866287231445
Validation loss: 2.248261441466629

Epoch: 5| Step: 9
Training loss: 2.5368454456329346
Validation loss: 2.209903683713687

Epoch: 5| Step: 10
Training loss: 2.5501692295074463
Validation loss: 2.17127695263073

Epoch: 196| Step: 0
Training loss: 2.317412853240967
Validation loss: 2.142794160432713

Epoch: 5| Step: 1
Training loss: 2.216026782989502
Validation loss: 2.132459173920334

Epoch: 5| Step: 2
Training loss: 2.375269651412964
Validation loss: 2.1168983136453936

Epoch: 5| Step: 3
Training loss: 2.068262815475464
Validation loss: 2.1134346172373784

Epoch: 5| Step: 4
Training loss: 2.373772144317627
Validation loss: 2.114022616417177

Epoch: 5| Step: 5
Training loss: 2.671207904815674
Validation loss: 2.110204391582038

Epoch: 5| Step: 6
Training loss: 2.567028045654297
Validation loss: 2.110163924514606

Epoch: 5| Step: 7
Training loss: 2.2206883430480957
Validation loss: 2.109361248631631

Epoch: 5| Step: 8
Training loss: 2.552612066268921
Validation loss: 2.10208051179045

Epoch: 5| Step: 9
Training loss: 1.755519151687622
Validation loss: 2.1025601920261177

Epoch: 5| Step: 10
Training loss: 2.517275333404541
Validation loss: 2.123508661024032

Epoch: 197| Step: 0
Training loss: 2.7456798553466797
Validation loss: 2.147015115266205

Epoch: 5| Step: 1
Training loss: 2.3637783527374268
Validation loss: 2.1621746811815488

Epoch: 5| Step: 2
Training loss: 1.7199583053588867
Validation loss: 2.154817842668103

Epoch: 5| Step: 3
Training loss: 2.014369249343872
Validation loss: 2.1733423920087915

Epoch: 5| Step: 4
Training loss: 2.782115936279297
Validation loss: 2.161432373908258

Epoch: 5| Step: 5
Training loss: 2.225466251373291
Validation loss: 2.128260553524058

Epoch: 5| Step: 6
Training loss: 2.563253879547119
Validation loss: 2.149043675391905

Epoch: 5| Step: 7
Training loss: 1.9312655925750732
Validation loss: 2.1208256854805896

Epoch: 5| Step: 8
Training loss: 2.206712007522583
Validation loss: 2.1197225560424147

Epoch: 5| Step: 9
Training loss: 2.7483413219451904
Validation loss: 2.106464283440703

Epoch: 5| Step: 10
Training loss: 2.3281683921813965
Validation loss: 2.1119671688284924

Epoch: 198| Step: 0
Training loss: 2.8936941623687744
Validation loss: 2.1042991069055375

Epoch: 5| Step: 1
Training loss: 2.2580275535583496
Validation loss: 2.119189255981035

Epoch: 5| Step: 2
Training loss: 2.273937940597534
Validation loss: 2.106883905267203

Epoch: 5| Step: 3
Training loss: 2.4511165618896484
Validation loss: 2.1156839273309194

Epoch: 5| Step: 4
Training loss: 1.8660920858383179
Validation loss: 2.1046971762052147

Epoch: 5| Step: 5
Training loss: 1.9626916646957397
Validation loss: 2.1339723153780867

Epoch: 5| Step: 6
Training loss: 2.296020030975342
Validation loss: 2.175162740932998

Epoch: 5| Step: 7
Training loss: 2.094242572784424
Validation loss: 2.236724789424609

Epoch: 5| Step: 8
Training loss: 2.9153733253479004
Validation loss: 2.2385768992926485

Epoch: 5| Step: 9
Training loss: 1.9947173595428467
Validation loss: 2.233602951931697

Epoch: 5| Step: 10
Training loss: 2.5811493396759033
Validation loss: 2.201341905901509

Epoch: 199| Step: 0
Training loss: 2.8311586380004883
Validation loss: 2.151860567831224

Epoch: 5| Step: 1
Training loss: 1.9672590494155884
Validation loss: 2.1524665842774096

Epoch: 5| Step: 2
Training loss: 2.76918888092041
Validation loss: 2.1248479594466505

Epoch: 5| Step: 3
Training loss: 2.3394482135772705
Validation loss: 2.1117779388222644

Epoch: 5| Step: 4
Training loss: 2.049441337585449
Validation loss: 2.124675453350108

Epoch: 5| Step: 5
Training loss: 2.6163625717163086
Validation loss: 2.13404679811129

Epoch: 5| Step: 6
Training loss: 2.4476687908172607
Validation loss: 2.1447906853050314

Epoch: 5| Step: 7
Training loss: 2.1268296241760254
Validation loss: 2.128148607028428

Epoch: 5| Step: 8
Training loss: 2.5759713649749756
Validation loss: 2.125984098321648

Epoch: 5| Step: 9
Training loss: 1.606011152267456
Validation loss: 2.124354734215685

Epoch: 5| Step: 10
Training loss: 2.443627119064331
Validation loss: 2.1198000574624665

Epoch: 200| Step: 0
Training loss: 2.5771842002868652
Validation loss: 2.1267225460339616

Epoch: 5| Step: 1
Training loss: 2.393448829650879
Validation loss: 2.1308475130347797

Epoch: 5| Step: 2
Training loss: 1.9601757526397705
Validation loss: 2.1387070737859255

Epoch: 5| Step: 3
Training loss: 1.8828214406967163
Validation loss: 2.1397678352171376

Epoch: 5| Step: 4
Training loss: 3.013099193572998
Validation loss: 2.1539448179224485

Epoch: 5| Step: 5
Training loss: 2.11468505859375
Validation loss: 2.1740459549811577

Epoch: 5| Step: 6
Training loss: 2.1859347820281982
Validation loss: 2.150303639391417

Epoch: 5| Step: 7
Training loss: 2.4269397258758545
Validation loss: 2.136750299443481

Epoch: 5| Step: 8
Training loss: 2.6417436599731445
Validation loss: 2.1344048464170067

Epoch: 5| Step: 9
Training loss: 2.0278449058532715
Validation loss: 2.150206422293058

Epoch: 5| Step: 10
Training loss: 2.22037410736084
Validation loss: 2.118152005698091

Testing loss: 2.343819008933173

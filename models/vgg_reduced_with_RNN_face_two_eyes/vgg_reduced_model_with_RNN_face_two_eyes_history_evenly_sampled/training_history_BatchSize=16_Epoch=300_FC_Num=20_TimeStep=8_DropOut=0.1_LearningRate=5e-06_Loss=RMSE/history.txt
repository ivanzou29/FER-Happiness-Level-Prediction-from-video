Epoch: 1| Step: 0
Training loss: 5.576524520919985
Validation loss: 5.826803588358493

Epoch: 6| Step: 1
Training loss: 6.511251834452431
Validation loss: 5.82382428534056

Epoch: 6| Step: 2
Training loss: 5.87391847435834
Validation loss: 5.820981114792559

Epoch: 6| Step: 3
Training loss: 5.5017038653823676
Validation loss: 5.818346375160538

Epoch: 6| Step: 4
Training loss: 5.798799837839393
Validation loss: 5.815812499778934

Epoch: 6| Step: 5
Training loss: 4.504358723816009
Validation loss: 5.813513030745124

Epoch: 6| Step: 6
Training loss: 5.719644778765869
Validation loss: 5.81118509020984

Epoch: 6| Step: 7
Training loss: 6.204583522643008
Validation loss: 5.808916889130963

Epoch: 6| Step: 8
Training loss: 6.863027794836278
Validation loss: 5.8065675313150615

Epoch: 6| Step: 9
Training loss: 5.63445030665625
Validation loss: 5.804276747369647

Epoch: 6| Step: 10
Training loss: 6.700237372449425
Validation loss: 5.80155784115566

Epoch: 6| Step: 11
Training loss: 4.874740789320138
Validation loss: 5.799244437475566

Epoch: 6| Step: 12
Training loss: 5.9468364992965155
Validation loss: 5.7966478123763014

Epoch: 6| Step: 13
Training loss: 5.4964692313548404
Validation loss: 5.7940567277537465

Epoch: 2| Step: 0
Training loss: 6.238330169657908
Validation loss: 5.79126721067903

Epoch: 6| Step: 1
Training loss: 6.445432645660499
Validation loss: 5.788722105606662

Epoch: 6| Step: 2
Training loss: 5.1362274747249215
Validation loss: 5.785296383706279

Epoch: 6| Step: 3
Training loss: 6.671445024201091
Validation loss: 5.782280683891549

Epoch: 6| Step: 4
Training loss: 5.556195232863784
Validation loss: 5.779145122504805

Epoch: 6| Step: 5
Training loss: 5.450550947426614
Validation loss: 5.77547464574523

Epoch: 6| Step: 6
Training loss: 6.3100224533527935
Validation loss: 5.771993093261152

Epoch: 6| Step: 7
Training loss: 5.294011515133234
Validation loss: 5.767977716312507

Epoch: 6| Step: 8
Training loss: 3.737017091919743
Validation loss: 5.7641439802555725

Epoch: 6| Step: 9
Training loss: 4.592533080138137
Validation loss: 5.7597589272124115

Epoch: 6| Step: 10
Training loss: 6.213923589368614
Validation loss: 5.755410889395538

Epoch: 6| Step: 11
Training loss: 6.531537939413956
Validation loss: 5.751026101244191

Epoch: 6| Step: 12
Training loss: 6.051544673232522
Validation loss: 5.7456406827488555

Epoch: 6| Step: 13
Training loss: 6.538321868992749
Validation loss: 5.739983010775887

Epoch: 3| Step: 0
Training loss: 5.100538539501915
Validation loss: 5.733805787756605

Epoch: 6| Step: 1
Training loss: 5.505302993793057
Validation loss: 5.727788304358695

Epoch: 6| Step: 2
Training loss: 6.037119800925161
Validation loss: 5.7211572207113

Epoch: 6| Step: 3
Training loss: 5.039992419645876
Validation loss: 5.714164417344784

Epoch: 6| Step: 4
Training loss: 6.1645680886205065
Validation loss: 5.706011319721401

Epoch: 6| Step: 5
Training loss: 6.386224775430647
Validation loss: 5.6981497619406385

Epoch: 6| Step: 6
Training loss: 5.679275098937721
Validation loss: 5.689283266073535

Epoch: 6| Step: 7
Training loss: 5.656958772711035
Validation loss: 5.679556164250811

Epoch: 6| Step: 8
Training loss: 5.438431583272654
Validation loss: 5.670822123121884

Epoch: 6| Step: 9
Training loss: 6.198757527049303
Validation loss: 5.659516114864625

Epoch: 6| Step: 10
Training loss: 4.710938714629819
Validation loss: 5.648926664172286

Epoch: 6| Step: 11
Training loss: 6.088657853383323
Validation loss: 5.637788632744355

Epoch: 6| Step: 12
Training loss: 6.463942063251365
Validation loss: 5.624633747970362

Epoch: 6| Step: 13
Training loss: 4.77435263993907
Validation loss: 5.612023882053337

Epoch: 4| Step: 0
Training loss: 5.234649195535206
Validation loss: 5.599631768087025

Epoch: 6| Step: 1
Training loss: 6.572061312760376
Validation loss: 5.584731286808149

Epoch: 6| Step: 2
Training loss: 5.213306626221395
Validation loss: 5.569861251055264

Epoch: 6| Step: 3
Training loss: 4.654755627766471
Validation loss: 5.554736000782617

Epoch: 6| Step: 4
Training loss: 5.972063512935798
Validation loss: 5.538641370385951

Epoch: 6| Step: 5
Training loss: 4.152162305684128
Validation loss: 5.521143447360539

Epoch: 6| Step: 6
Training loss: 4.593671110345138
Validation loss: 5.504197483166354

Epoch: 6| Step: 7
Training loss: 6.163375526120128
Validation loss: 5.4857556571086485

Epoch: 6| Step: 8
Training loss: 4.568248523999801
Validation loss: 5.4683729678739095

Epoch: 6| Step: 9
Training loss: 6.349443258073892
Validation loss: 5.448154721036983

Epoch: 6| Step: 10
Training loss: 6.0788768313129555
Validation loss: 5.428115408682518

Epoch: 6| Step: 11
Training loss: 6.047824991994152
Validation loss: 5.406838218075704

Epoch: 6| Step: 12
Training loss: 5.984335159687064
Validation loss: 5.384146363961533

Epoch: 6| Step: 13
Training loss: 4.782309421026362
Validation loss: 5.360362745158845

Epoch: 5| Step: 0
Training loss: 4.9686384848310645
Validation loss: 5.3380863381092105

Epoch: 6| Step: 1
Training loss: 3.9088134899051843
Validation loss: 5.314298599633567

Epoch: 6| Step: 2
Training loss: 5.348503274203908
Validation loss: 5.288213593298223

Epoch: 6| Step: 3
Training loss: 5.878725210044678
Validation loss: 5.264805093080739

Epoch: 6| Step: 4
Training loss: 6.0000136693162975
Validation loss: 5.240656079580304

Epoch: 6| Step: 5
Training loss: 4.365908085595165
Validation loss: 5.213649627878015

Epoch: 6| Step: 6
Training loss: 4.747998468426117
Validation loss: 5.187164074364187

Epoch: 6| Step: 7
Training loss: 4.9976045592429985
Validation loss: 5.162540877772145

Epoch: 6| Step: 8
Training loss: 6.099963028592494
Validation loss: 5.136276341228604

Epoch: 6| Step: 9
Training loss: 5.877143631113858
Validation loss: 5.110979384058956

Epoch: 6| Step: 10
Training loss: 4.832581472563587
Validation loss: 5.084657269042592

Epoch: 6| Step: 11
Training loss: 5.632710914912691
Validation loss: 5.057204604153243

Epoch: 6| Step: 12
Training loss: 5.3636873651322095
Validation loss: 5.03309435405784

Epoch: 6| Step: 13
Training loss: 4.0013873555359485
Validation loss: 5.006238606766062

Epoch: 6| Step: 0
Training loss: 5.723051527517561
Validation loss: 4.982324118176616

Epoch: 6| Step: 1
Training loss: 5.083152559328339
Validation loss: 4.9573490278831205

Epoch: 6| Step: 2
Training loss: 4.57864429015474
Validation loss: 4.9349681239740155

Epoch: 6| Step: 3
Training loss: 5.670868082755207
Validation loss: 4.9135371461664565

Epoch: 6| Step: 4
Training loss: 4.714927691555479
Validation loss: 4.89147181804777

Epoch: 6| Step: 5
Training loss: 6.016817366103197
Validation loss: 4.870471408588467

Epoch: 6| Step: 6
Training loss: 4.3722207096996115
Validation loss: 4.850133511213463

Epoch: 6| Step: 7
Training loss: 4.30697786909431
Validation loss: 4.830584035151098

Epoch: 6| Step: 8
Training loss: 4.916910628823341
Validation loss: 4.812756520234505

Epoch: 6| Step: 9
Training loss: 5.724496757677666
Validation loss: 4.792854030710583

Epoch: 6| Step: 10
Training loss: 4.021695902644755
Validation loss: 4.7742501802586

Epoch: 6| Step: 11
Training loss: 4.060369784047153
Validation loss: 4.756809571491922

Epoch: 6| Step: 12
Training loss: 5.004230235657113
Validation loss: 4.739862495142351

Epoch: 6| Step: 13
Training loss: 3.3527272296906268
Validation loss: 4.7228672770915185

Epoch: 7| Step: 0
Training loss: 4.612336839720932
Validation loss: 4.705268314523955

Epoch: 6| Step: 1
Training loss: 4.407040572431307
Validation loss: 4.688259255231296

Epoch: 6| Step: 2
Training loss: 5.016784343838979
Validation loss: 4.671827230916806

Epoch: 6| Step: 3
Training loss: 5.707706649053575
Validation loss: 4.65603231959206

Epoch: 6| Step: 4
Training loss: 3.7798523644978386
Validation loss: 4.638698018283715

Epoch: 6| Step: 5
Training loss: 4.125463286306176
Validation loss: 4.621672178335208

Epoch: 6| Step: 6
Training loss: 5.258503884009646
Validation loss: 4.605798197915277

Epoch: 6| Step: 7
Training loss: 5.2982073433243855
Validation loss: 4.589079550242383

Epoch: 6| Step: 8
Training loss: 5.0074338963259635
Validation loss: 4.572063062055854

Epoch: 6| Step: 9
Training loss: 4.703349466131718
Validation loss: 4.555658965225585

Epoch: 6| Step: 10
Training loss: 4.472992916846088
Validation loss: 4.542240766215927

Epoch: 6| Step: 11
Training loss: 3.3815000600243708
Validation loss: 4.524861171172065

Epoch: 6| Step: 12
Training loss: 4.882899608597998
Validation loss: 4.511435454793667

Epoch: 6| Step: 13
Training loss: 4.464292637410926
Validation loss: 4.494178148431301

Epoch: 8| Step: 0
Training loss: 4.765259375382714
Validation loss: 4.478396434146045

Epoch: 6| Step: 1
Training loss: 3.7593917224794327
Validation loss: 4.46089637064573

Epoch: 6| Step: 2
Training loss: 4.326404215685353
Validation loss: 4.446080363375873

Epoch: 6| Step: 3
Training loss: 4.042639913408369
Validation loss: 4.431852260089222

Epoch: 6| Step: 4
Training loss: 4.254615521391243
Validation loss: 4.421256117394279

Epoch: 6| Step: 5
Training loss: 4.747797003400424
Validation loss: 4.410689294307829

Epoch: 6| Step: 6
Training loss: 4.577451480627087
Validation loss: 4.3967267281226805

Epoch: 6| Step: 7
Training loss: 4.385662056890167
Validation loss: 4.386111384801632

Epoch: 6| Step: 8
Training loss: 4.796012080109744
Validation loss: 4.375437980327835

Epoch: 6| Step: 9
Training loss: 3.19002178890237
Validation loss: 4.364503239129669

Epoch: 6| Step: 10
Training loss: 4.761870359114851
Validation loss: 4.352690728247325

Epoch: 6| Step: 11
Training loss: 5.160865706043457
Validation loss: 4.341402000126324

Epoch: 6| Step: 12
Training loss: 4.862132378731635
Validation loss: 4.332665495292427

Epoch: 6| Step: 13
Training loss: 5.3751155042436025
Validation loss: 4.321190387092132

Epoch: 9| Step: 0
Training loss: 3.481668147891326
Validation loss: 4.311493644267199

Epoch: 6| Step: 1
Training loss: 4.961187016978058
Validation loss: 4.300966850857546

Epoch: 6| Step: 2
Training loss: 4.668485196326568
Validation loss: 4.29196752936641

Epoch: 6| Step: 3
Training loss: 4.567326122663741
Validation loss: 4.282500994816727

Epoch: 6| Step: 4
Training loss: 4.793591894852391
Validation loss: 4.273002796620271

Epoch: 6| Step: 5
Training loss: 3.714055793059652
Validation loss: 4.264391127204198

Epoch: 6| Step: 6
Training loss: 4.741735798220621
Validation loss: 4.254273916733809

Epoch: 6| Step: 7
Training loss: 3.9867163865764352
Validation loss: 4.24457759814395

Epoch: 6| Step: 8
Training loss: 4.396373528769795
Validation loss: 4.236348910857451

Epoch: 6| Step: 9
Training loss: 5.752133844253542
Validation loss: 4.225591413522905

Epoch: 6| Step: 10
Training loss: 3.658927002676206
Validation loss: 4.216964833770975

Epoch: 6| Step: 11
Training loss: 4.3470567151062784
Validation loss: 4.207570968935169

Epoch: 6| Step: 12
Training loss: 4.081939433402199
Validation loss: 4.197448144113685

Epoch: 6| Step: 13
Training loss: 2.624354691886774
Validation loss: 4.189536784414061

Epoch: 10| Step: 0
Training loss: 4.227316064696351
Validation loss: 4.182422869067425

Epoch: 6| Step: 1
Training loss: 3.423664478628015
Validation loss: 4.1736056038329545

Epoch: 6| Step: 2
Training loss: 4.5105849918146745
Validation loss: 4.166740091148624

Epoch: 6| Step: 3
Training loss: 4.514661851219798
Validation loss: 4.159255436375255

Epoch: 6| Step: 4
Training loss: 3.3501070147829677
Validation loss: 4.151706854659166

Epoch: 6| Step: 5
Training loss: 3.7241161962096583
Validation loss: 4.144259980359232

Epoch: 6| Step: 6
Training loss: 5.340264326832927
Validation loss: 4.138522518952168

Epoch: 6| Step: 7
Training loss: 4.014193624847171
Validation loss: 4.1309861655660605

Epoch: 6| Step: 8
Training loss: 4.116508065899569
Validation loss: 4.126173600110047

Epoch: 6| Step: 9
Training loss: 3.6980224737512923
Validation loss: 4.118659856360885

Epoch: 6| Step: 10
Training loss: 5.117747688206843
Validation loss: 4.110913050643043

Epoch: 6| Step: 11
Training loss: 5.094164123551162
Validation loss: 4.106583352189353

Epoch: 6| Step: 12
Training loss: 3.874414461257667
Validation loss: 4.099260060190143

Epoch: 6| Step: 13
Training loss: 4.1532373050061
Validation loss: 4.0924263681818385

Epoch: 11| Step: 0
Training loss: 4.353163583862455
Validation loss: 4.086234525960423

Epoch: 6| Step: 1
Training loss: 3.2564899495287616
Validation loss: 4.081715628544105

Epoch: 6| Step: 2
Training loss: 5.217143833815093
Validation loss: 4.075248785557094

Epoch: 6| Step: 3
Training loss: 5.430485904091918
Validation loss: 4.068739709157002

Epoch: 6| Step: 4
Training loss: 3.789799632309307
Validation loss: 4.062802352571106

Epoch: 6| Step: 5
Training loss: 4.889694369073513
Validation loss: 4.055104841667378

Epoch: 6| Step: 6
Training loss: 3.9910760515575756
Validation loss: 4.048954901027699

Epoch: 6| Step: 7
Training loss: 4.318540461490351
Validation loss: 4.043626213623048

Epoch: 6| Step: 8
Training loss: 3.777159440791732
Validation loss: 4.037593588491545

Epoch: 6| Step: 9
Training loss: 3.7013321411541313
Validation loss: 4.029366514519249

Epoch: 6| Step: 10
Training loss: 3.7217461833553767
Validation loss: 4.023723989932329

Epoch: 6| Step: 11
Training loss: 3.4155828726387654
Validation loss: 4.018648308257873

Epoch: 6| Step: 12
Training loss: 3.9603959715956503
Validation loss: 4.01132882841919

Epoch: 6| Step: 13
Training loss: 4.315806130510538
Validation loss: 4.004955227050481

Epoch: 12| Step: 0
Training loss: 4.16540345434869
Validation loss: 3.999161465746776

Epoch: 6| Step: 1
Training loss: 3.7012834436515427
Validation loss: 3.992986638546102

Epoch: 6| Step: 2
Training loss: 3.718040847480064
Validation loss: 3.987010520240093

Epoch: 6| Step: 3
Training loss: 3.9751784758416724
Validation loss: 3.9811364398764124

Epoch: 6| Step: 4
Training loss: 4.463771367552033
Validation loss: 3.9745553026212104

Epoch: 6| Step: 5
Training loss: 4.63342692786076
Validation loss: 3.9648909458431003

Epoch: 6| Step: 6
Training loss: 4.357803991397663
Validation loss: 3.9569500270382716

Epoch: 6| Step: 7
Training loss: 3.9189793813152827
Validation loss: 3.9499536611513673

Epoch: 6| Step: 8
Training loss: 3.889019247927991
Validation loss: 3.9416557371349663

Epoch: 6| Step: 9
Training loss: 3.840830120323417
Validation loss: 3.9367357766705706

Epoch: 6| Step: 10
Training loss: 4.412930233650914
Validation loss: 3.9283036649343104

Epoch: 6| Step: 11
Training loss: 3.8419392360109867
Validation loss: 3.9213120584702392

Epoch: 6| Step: 12
Training loss: 4.78320990391006
Validation loss: 3.912007111377226

Epoch: 6| Step: 13
Training loss: 3.231794424326962
Validation loss: 3.9080253599552774

Epoch: 13| Step: 0
Training loss: 4.611977880022877
Validation loss: 3.8986627647352377

Epoch: 6| Step: 1
Training loss: 3.4535552572378108
Validation loss: 3.8934905254157854

Epoch: 6| Step: 2
Training loss: 3.5496025185294733
Validation loss: 3.8869091016798696

Epoch: 6| Step: 3
Training loss: 3.6553652744441036
Validation loss: 3.8819533583334676

Epoch: 6| Step: 4
Training loss: 3.876898638825253
Validation loss: 3.8737445786367153

Epoch: 6| Step: 5
Training loss: 3.7790407394631855
Validation loss: 3.867571651718966

Epoch: 6| Step: 6
Training loss: 4.074469437023342
Validation loss: 3.860875106232143

Epoch: 6| Step: 7
Training loss: 3.99615699218346
Validation loss: 3.8517468537357913

Epoch: 6| Step: 8
Training loss: 4.909324897301627
Validation loss: 3.8492924238599224

Epoch: 6| Step: 9
Training loss: 4.16226655689241
Validation loss: 3.840554561279609

Epoch: 6| Step: 10
Training loss: 4.19045915951096
Validation loss: 3.8347236810196947

Epoch: 6| Step: 11
Training loss: 3.74891685101806
Validation loss: 3.825541713101429

Epoch: 6| Step: 12
Training loss: 3.6893493411570817
Validation loss: 3.820982130262138

Epoch: 6| Step: 13
Training loss: 4.5511676673388415
Validation loss: 3.813589732788474

Epoch: 14| Step: 0
Training loss: 3.8545200220286477
Validation loss: 3.8071066722207396

Epoch: 6| Step: 1
Training loss: 4.170804118467913
Validation loss: 3.8026568731022428

Epoch: 6| Step: 2
Training loss: 4.538942739249477
Validation loss: 3.7948385334448256

Epoch: 6| Step: 3
Training loss: 3.751142836314045
Validation loss: 3.791863644709944

Epoch: 6| Step: 4
Training loss: 4.3207883934889235
Validation loss: 3.78533549948804

Epoch: 6| Step: 5
Training loss: 4.610409791817461
Validation loss: 3.7773403823450167

Epoch: 6| Step: 6
Training loss: 3.2617497425548514
Validation loss: 3.7742840597677656

Epoch: 6| Step: 7
Training loss: 4.0830048928550315
Validation loss: 3.7682730186911004

Epoch: 6| Step: 8
Training loss: 3.9457196620424293
Validation loss: 3.7633470185763995

Epoch: 6| Step: 9
Training loss: 3.7006273820299556
Validation loss: 3.7563341017866323

Epoch: 6| Step: 10
Training loss: 3.158511267676522
Validation loss: 3.7526612372212895

Epoch: 6| Step: 11
Training loss: 4.474748759968961
Validation loss: 3.751105410176207

Epoch: 6| Step: 12
Training loss: 3.4011135746051293
Validation loss: 3.745565282006672

Epoch: 6| Step: 13
Training loss: 3.435199730853642
Validation loss: 3.742080244730257

Epoch: 15| Step: 0
Training loss: 4.472422764443354
Validation loss: 3.739108097453223

Epoch: 6| Step: 1
Training loss: 4.663841050608062
Validation loss: 3.7315680386321226

Epoch: 6| Step: 2
Training loss: 3.5470029833049503
Validation loss: 3.7292327508360446

Epoch: 6| Step: 3
Training loss: 3.9726376453365364
Validation loss: 3.7204698649647274

Epoch: 6| Step: 4
Training loss: 2.6854708796065623
Validation loss: 3.7182644859483953

Epoch: 6| Step: 5
Training loss: 3.1810653662922466
Validation loss: 3.714058888155811

Epoch: 6| Step: 6
Training loss: 4.483038407354593
Validation loss: 3.7108031083076933

Epoch: 6| Step: 7
Training loss: 4.801685021597195
Validation loss: 3.7166616825964467

Epoch: 6| Step: 8
Training loss: 2.980321878036683
Validation loss: 3.7003853797592887

Epoch: 6| Step: 9
Training loss: 3.831609518037852
Validation loss: 3.6988779392583346

Epoch: 6| Step: 10
Training loss: 3.5023355865553563
Validation loss: 3.6953770019795953

Epoch: 6| Step: 11
Training loss: 3.9034230493344166
Validation loss: 3.6979339049216398

Epoch: 6| Step: 12
Training loss: 3.860411223016405
Validation loss: 3.6967161960043504

Epoch: 6| Step: 13
Training loss: 4.05547206101772
Validation loss: 3.693130071742035

Epoch: 16| Step: 0
Training loss: 2.4759334884547504
Validation loss: 3.6862995563739367

Epoch: 6| Step: 1
Training loss: 4.076487011281824
Validation loss: 3.6839340582217295

Epoch: 6| Step: 2
Training loss: 3.373671658805252
Validation loss: 3.6833813558141153

Epoch: 6| Step: 3
Training loss: 3.522920625587794
Validation loss: 3.6755149643465073

Epoch: 6| Step: 4
Training loss: 3.8835419059919647
Validation loss: 3.6737472128301976

Epoch: 6| Step: 5
Training loss: 5.057522811347932
Validation loss: 3.6667114103128533

Epoch: 6| Step: 6
Training loss: 4.700042627019545
Validation loss: 3.6615822425276177

Epoch: 6| Step: 7
Training loss: 2.9894640607445675
Validation loss: 3.6584764308012523

Epoch: 6| Step: 8
Training loss: 4.70328539197799
Validation loss: 3.6555628117183674

Epoch: 6| Step: 9
Training loss: 4.647624379053713
Validation loss: 3.6538792137004474

Epoch: 6| Step: 10
Training loss: 2.9539270674187112
Validation loss: 3.6458407330181544

Epoch: 6| Step: 11
Training loss: 3.6618450425435243
Validation loss: 3.6447820377680147

Epoch: 6| Step: 12
Training loss: 3.0311985601406146
Validation loss: 3.6418956525125625

Epoch: 6| Step: 13
Training loss: 3.7898327231418767
Validation loss: 3.6402250877696694

Epoch: 17| Step: 0
Training loss: 3.798881094987383
Validation loss: 3.633507532837924

Epoch: 6| Step: 1
Training loss: 3.4470726585952978
Validation loss: 3.630311063234891

Epoch: 6| Step: 2
Training loss: 3.646822128631779
Validation loss: 3.630589643442335

Epoch: 6| Step: 3
Training loss: 4.1792357013410095
Validation loss: 3.624453711727521

Epoch: 6| Step: 4
Training loss: 3.2398008087528325
Validation loss: 3.6194008260396644

Epoch: 6| Step: 5
Training loss: 4.321008001982667
Validation loss: 3.616912459252415

Epoch: 6| Step: 6
Training loss: 3.926976506865509
Validation loss: 3.610872262565692

Epoch: 6| Step: 7
Training loss: 3.5945474817862566
Validation loss: 3.612761830979677

Epoch: 6| Step: 8
Training loss: 3.2586381995627276
Validation loss: 3.608663608801303

Epoch: 6| Step: 9
Training loss: 4.116421188469288
Validation loss: 3.6099591463688543

Epoch: 6| Step: 10
Training loss: 4.565678573596957
Validation loss: 3.6022928547436353

Epoch: 6| Step: 11
Training loss: 3.855227570106539
Validation loss: 3.593680693469496

Epoch: 6| Step: 12
Training loss: 3.847468244732642
Validation loss: 3.593660992861711

Epoch: 6| Step: 13
Training loss: 2.700427169916718
Validation loss: 3.5892539799840764

Epoch: 18| Step: 0
Training loss: 3.324171556794484
Validation loss: 3.588888023058712

Epoch: 6| Step: 1
Training loss: 3.06939543794738
Validation loss: 3.583388154759239

Epoch: 6| Step: 2
Training loss: 3.7083178459158392
Validation loss: 3.5823904162407363

Epoch: 6| Step: 3
Training loss: 4.43746217523492
Validation loss: 3.577346945720679

Epoch: 6| Step: 4
Training loss: 3.9332911688489722
Validation loss: 3.572348275930081

Epoch: 6| Step: 5
Training loss: 4.044184788437263
Validation loss: 3.5648448101926125

Epoch: 6| Step: 6
Training loss: 3.593833192608748
Validation loss: 3.5669096170398538

Epoch: 6| Step: 7
Training loss: 4.223968668908183
Validation loss: 3.5693537569954223

Epoch: 6| Step: 8
Training loss: 3.3622766073108594
Validation loss: 3.555868144890237

Epoch: 6| Step: 9
Training loss: 3.7677492347095365
Validation loss: 3.5501154634566037

Epoch: 6| Step: 10
Training loss: 3.199934845499498
Validation loss: 3.5531497738111684

Epoch: 6| Step: 11
Training loss: 3.8696172922242904
Validation loss: 3.5502639791370645

Epoch: 6| Step: 12
Training loss: 4.267237096642844
Validation loss: 3.549697254033034

Epoch: 6| Step: 13
Training loss: 3.545825013010115
Validation loss: 3.5446226496265987

Epoch: 19| Step: 0
Training loss: 3.4536473496238678
Validation loss: 3.5377374388838945

Epoch: 6| Step: 1
Training loss: 3.3669461829855694
Validation loss: 3.5345839518585125

Epoch: 6| Step: 2
Training loss: 3.61381990283981
Validation loss: 3.5323544131987523

Epoch: 6| Step: 3
Training loss: 3.999742857297143
Validation loss: 3.5312460680536883

Epoch: 6| Step: 4
Training loss: 2.8134914769866746
Validation loss: 3.5281780593852434

Epoch: 6| Step: 5
Training loss: 3.977350842588144
Validation loss: 3.529442024774821

Epoch: 6| Step: 6
Training loss: 3.90961805483039
Validation loss: 3.527671736121283

Epoch: 6| Step: 7
Training loss: 3.951176098883988
Validation loss: 3.537932961457563

Epoch: 6| Step: 8
Training loss: 3.7101314914156007
Validation loss: 3.513269750379295

Epoch: 6| Step: 9
Training loss: 3.265332514609996
Validation loss: 3.5114010231350923

Epoch: 6| Step: 10
Training loss: 3.5996057983314436
Validation loss: 3.5151055633849095

Epoch: 6| Step: 11
Training loss: 3.7978007126079625
Validation loss: 3.5182542703534363

Epoch: 6| Step: 12
Training loss: 3.9502649628825584
Validation loss: 3.5214657337825845

Epoch: 6| Step: 13
Training loss: 5.0640422214477505
Validation loss: 3.510464340990926

Epoch: 20| Step: 0
Training loss: 4.082173059363932
Validation loss: 3.5049002984598543

Epoch: 6| Step: 1
Training loss: 4.1858845341810484
Validation loss: 3.5010626113011796

Epoch: 6| Step: 2
Training loss: 3.7346112164602023
Validation loss: 3.496926090229018

Epoch: 6| Step: 3
Training loss: 3.876535911115234
Validation loss: 3.498278148907083

Epoch: 6| Step: 4
Training loss: 4.038624014747488
Validation loss: 3.5011892832503686

Epoch: 6| Step: 5
Training loss: 3.6887605743101517
Validation loss: 3.49534179314954

Epoch: 6| Step: 6
Training loss: 3.3974529374815985
Validation loss: 3.4881546252362177

Epoch: 6| Step: 7
Training loss: 3.542663493775122
Validation loss: 3.486862069757808

Epoch: 6| Step: 8
Training loss: 3.9193319761591083
Validation loss: 3.481365924746917

Epoch: 6| Step: 9
Training loss: 3.743247883388395
Validation loss: 3.4801876040696675

Epoch: 6| Step: 10
Training loss: 3.6521779395016103
Validation loss: 3.478034376745999

Epoch: 6| Step: 11
Training loss: 2.888015476470648
Validation loss: 3.475116976650871

Epoch: 6| Step: 12
Training loss: 3.165892422706956
Validation loss: 3.4736251532299267

Epoch: 6| Step: 13
Training loss: 3.6752156097067328
Validation loss: 3.4724446178025348

Epoch: 21| Step: 0
Training loss: 3.7982354885977334
Validation loss: 3.4716064803551707

Epoch: 6| Step: 1
Training loss: 3.4803638035699938
Validation loss: 3.4699206968511125

Epoch: 6| Step: 2
Training loss: 4.718164470799078
Validation loss: 3.468047338679709

Epoch: 6| Step: 3
Training loss: 1.3248082652009225
Validation loss: 3.4634302689300966

Epoch: 6| Step: 4
Training loss: 3.287276192941013
Validation loss: 3.458015380329915

Epoch: 6| Step: 5
Training loss: 5.159245401253129
Validation loss: 3.45832477088656

Epoch: 6| Step: 6
Training loss: 3.2491976407780876
Validation loss: 3.457549810823284

Epoch: 6| Step: 7
Training loss: 3.755900382275812
Validation loss: 3.449271921877672

Epoch: 6| Step: 8
Training loss: 3.240825468096152
Validation loss: 3.4484953675191083

Epoch: 6| Step: 9
Training loss: 3.9513962173319466
Validation loss: 3.4457735031279912

Epoch: 6| Step: 10
Training loss: 2.88204689507647
Validation loss: 3.4444767752193117

Epoch: 6| Step: 11
Training loss: 3.3892577618513364
Validation loss: 3.4457356424673136

Epoch: 6| Step: 12
Training loss: 3.5262249326575583
Validation loss: 3.4406353177770206

Epoch: 6| Step: 13
Training loss: 4.5762466899960925
Validation loss: 3.437835471152753

Epoch: 22| Step: 0
Training loss: 4.16467555473368
Validation loss: 3.4368905221647243

Epoch: 6| Step: 1
Training loss: 4.4568861754708875
Validation loss: 3.4325894868244977

Epoch: 6| Step: 2
Training loss: 2.532112633922342
Validation loss: 3.4304532624303388

Epoch: 6| Step: 3
Training loss: 4.070762799737724
Validation loss: 3.4291204362651992

Epoch: 6| Step: 4
Training loss: 3.191219777293972
Validation loss: 3.427827796800003

Epoch: 6| Step: 5
Training loss: 3.251153007799995
Validation loss: 3.4275648612419074

Epoch: 6| Step: 6
Training loss: 4.18571570799537
Validation loss: 3.4263586365316483

Epoch: 6| Step: 7
Training loss: 3.267781836708843
Validation loss: 3.4239900804083088

Epoch: 6| Step: 8
Training loss: 3.4938059902993417
Validation loss: 3.4193245273944597

Epoch: 6| Step: 9
Training loss: 3.797838253705146
Validation loss: 3.4187612848443742

Epoch: 6| Step: 10
Training loss: 4.411230208238201
Validation loss: 3.4153472254541524

Epoch: 6| Step: 11
Training loss: 3.3266254324131723
Validation loss: 3.413959193269989

Epoch: 6| Step: 12
Training loss: 3.229542536035937
Validation loss: 3.414391327006718

Epoch: 6| Step: 13
Training loss: 2.4438083885612722
Validation loss: 3.411599768091374

Epoch: 23| Step: 0
Training loss: 3.490555691351139
Validation loss: 3.4130679841578475

Epoch: 6| Step: 1
Training loss: 3.7403964094715674
Validation loss: 3.4142738397580796

Epoch: 6| Step: 2
Training loss: 3.413304155220053
Validation loss: 3.406757119421621

Epoch: 6| Step: 3
Training loss: 3.7487196961971883
Validation loss: 3.4169676238219204

Epoch: 6| Step: 4
Training loss: 3.6291582658555868
Validation loss: 3.412936787142035

Epoch: 6| Step: 5
Training loss: 4.035927830772026
Validation loss: 3.4130616807111602

Epoch: 6| Step: 6
Training loss: 2.697415358070135
Validation loss: 3.4114001347658567

Epoch: 6| Step: 7
Training loss: 4.265517684089617
Validation loss: 3.4087419955433615

Epoch: 6| Step: 8
Training loss: 3.2806887192005947
Validation loss: 3.406696777606411

Epoch: 6| Step: 9
Training loss: 3.8632239789738763
Validation loss: 3.4050805111705778

Epoch: 6| Step: 10
Training loss: 3.8465816333259872
Validation loss: 3.4033261413472515

Epoch: 6| Step: 11
Training loss: 3.5443988508788524
Validation loss: 3.402063725578657

Epoch: 6| Step: 12
Training loss: 3.7567195452872655
Validation loss: 3.399679280849392

Epoch: 6| Step: 13
Training loss: 2.946660793120225
Validation loss: 3.399235401585135

Epoch: 24| Step: 0
Training loss: 3.0114798246971715
Validation loss: 3.3996137172175627

Epoch: 6| Step: 1
Training loss: 3.4745377377206204
Validation loss: 3.3964872854782957

Epoch: 6| Step: 2
Training loss: 3.8320631051173417
Validation loss: 3.397344082689893

Epoch: 6| Step: 3
Training loss: 3.9204018550342585
Validation loss: 3.394067773494603

Epoch: 6| Step: 4
Training loss: 3.0619218241593273
Validation loss: 3.3897315357928512

Epoch: 6| Step: 5
Training loss: 3.5870915303310364
Validation loss: 3.38629946775861

Epoch: 6| Step: 6
Training loss: 2.98877332039306
Validation loss: 3.388732239709002

Epoch: 6| Step: 7
Training loss: 3.4686959752180146
Validation loss: 3.38661321327094

Epoch: 6| Step: 8
Training loss: 4.202019187930643
Validation loss: 3.388516777735027

Epoch: 6| Step: 9
Training loss: 3.147654698746205
Validation loss: 3.3856255302019966

Epoch: 6| Step: 10
Training loss: 4.244565743204912
Validation loss: 3.3835819277525094

Epoch: 6| Step: 11
Training loss: 4.089860539005655
Validation loss: 3.3801016993694653

Epoch: 6| Step: 12
Training loss: 3.506195851280156
Validation loss: 3.3780619238606433

Epoch: 6| Step: 13
Training loss: 3.831584752753004
Validation loss: 3.3771754416273914

Epoch: 25| Step: 0
Training loss: 4.33818499078615
Validation loss: 3.376126025945614

Epoch: 6| Step: 1
Training loss: 3.458163215099256
Validation loss: 3.373291386781215

Epoch: 6| Step: 2
Training loss: 4.206325064410185
Validation loss: 3.37121246299594

Epoch: 6| Step: 3
Training loss: 3.464695390795701
Validation loss: 3.3730017072150873

Epoch: 6| Step: 4
Training loss: 2.944049756769901
Validation loss: 3.3701844948881257

Epoch: 6| Step: 5
Training loss: 4.313578595160966
Validation loss: 3.369855276686837

Epoch: 6| Step: 6
Training loss: 2.416889695585767
Validation loss: 3.372810134148286

Epoch: 6| Step: 7
Training loss: 3.3549912349398823
Validation loss: 3.3655467600651834

Epoch: 6| Step: 8
Training loss: 3.4006104987069623
Validation loss: 3.36456403565169

Epoch: 6| Step: 9
Training loss: 2.902787309435254
Validation loss: 3.3655602929474187

Epoch: 6| Step: 10
Training loss: 3.803892832019619
Validation loss: 3.3620917440240676

Epoch: 6| Step: 11
Training loss: 3.631062929255815
Validation loss: 3.3625487788542574

Epoch: 6| Step: 12
Training loss: 3.9548015431841432
Validation loss: 3.361543569493375

Epoch: 6| Step: 13
Training loss: 3.6520522057352256
Validation loss: 3.3593571671253204

Epoch: 26| Step: 0
Training loss: 3.6585689917734117
Validation loss: 3.357214869768421

Epoch: 6| Step: 1
Training loss: 3.660290824508745
Validation loss: 3.35649298909255

Epoch: 6| Step: 2
Training loss: 3.5291558191582357
Validation loss: 3.3556344905801243

Epoch: 6| Step: 3
Training loss: 3.5747704919091268
Validation loss: 3.3559801038498698

Epoch: 6| Step: 4
Training loss: 3.7509278738948226
Validation loss: 3.355785543774464

Epoch: 6| Step: 5
Training loss: 4.027278629520192
Validation loss: 3.3544017261239913

Epoch: 6| Step: 6
Training loss: 3.9954433713625135
Validation loss: 3.355654066769431

Epoch: 6| Step: 7
Training loss: 2.6378853032857617
Validation loss: 3.352622741075689

Epoch: 6| Step: 8
Training loss: 3.722103626266078
Validation loss: 3.353117471777917

Epoch: 6| Step: 9
Training loss: 3.1578079759658806
Validation loss: 3.3522567146702924

Epoch: 6| Step: 10
Training loss: 3.2354707934848004
Validation loss: 3.352704990729202

Epoch: 6| Step: 11
Training loss: 3.8113579211996362
Validation loss: 3.35330780677504

Epoch: 6| Step: 12
Training loss: 3.614884832085687
Validation loss: 3.3528128166595

Epoch: 6| Step: 13
Training loss: 3.620553676983326
Validation loss: 3.352081187555556

Epoch: 27| Step: 0
Training loss: 3.4770279819173346
Validation loss: 3.3463511232068828

Epoch: 6| Step: 1
Training loss: 2.9277693916247345
Validation loss: 3.3463112474481225

Epoch: 6| Step: 2
Training loss: 3.7696612874267403
Validation loss: 3.343202598462159

Epoch: 6| Step: 3
Training loss: 4.118060893412164
Validation loss: 3.3428877787519857

Epoch: 6| Step: 4
Training loss: 3.0187639887704805
Validation loss: 3.342792765368227

Epoch: 6| Step: 5
Training loss: 3.5187442759167475
Validation loss: 3.3395843902171274

Epoch: 6| Step: 6
Training loss: 3.098926875278007
Validation loss: 3.3389000990668407

Epoch: 6| Step: 7
Training loss: 4.360113047262893
Validation loss: 3.3411451376535073

Epoch: 6| Step: 8
Training loss: 3.5286182960852783
Validation loss: 3.335910638029248

Epoch: 6| Step: 9
Training loss: 3.5241411610279694
Validation loss: 3.3400818780235038

Epoch: 6| Step: 10
Training loss: 4.203182631313768
Validation loss: 3.3346549764686095

Epoch: 6| Step: 11
Training loss: 2.808622612095269
Validation loss: 3.3338179195063984

Epoch: 6| Step: 12
Training loss: 3.6105461037236815
Validation loss: 3.3375384130168837

Epoch: 6| Step: 13
Training loss: 3.737464680074214
Validation loss: 3.3305506848394786

Epoch: 28| Step: 0
Training loss: 3.3178046595285053
Validation loss: 3.329752746858365

Epoch: 6| Step: 1
Training loss: 2.312403496455565
Validation loss: 3.3284101035269202

Epoch: 6| Step: 2
Training loss: 4.359350498363044
Validation loss: 3.32760834562828

Epoch: 6| Step: 3
Training loss: 3.7913637704000327
Validation loss: 3.3239417277056327

Epoch: 6| Step: 4
Training loss: 3.6657792809321226
Validation loss: 3.3239785955210395

Epoch: 6| Step: 5
Training loss: 3.8760898657145098
Validation loss: 3.3224236599051222

Epoch: 6| Step: 6
Training loss: 3.438080894632514
Validation loss: 3.3224944867031

Epoch: 6| Step: 7
Training loss: 4.252117302755944
Validation loss: 3.321088254619049

Epoch: 6| Step: 8
Training loss: 3.888609582468171
Validation loss: 3.320689931381488

Epoch: 6| Step: 9
Training loss: 3.0005786655553686
Validation loss: 3.3142552677460237

Epoch: 6| Step: 10
Training loss: 3.3166492314735048
Validation loss: 3.3118360089585197

Epoch: 6| Step: 11
Training loss: 3.698475167808286
Validation loss: 3.311941960067879

Epoch: 6| Step: 12
Training loss: 2.846844676554511
Validation loss: 3.311199647336356

Epoch: 6| Step: 13
Training loss: 3.452955699239868
Validation loss: 3.3136794160905767

Epoch: 29| Step: 0
Training loss: 3.0879386497589767
Validation loss: 3.3185357031131684

Epoch: 6| Step: 1
Training loss: 3.7127434441994898
Validation loss: 3.3284689886144077

Epoch: 6| Step: 2
Training loss: 3.9379568894536923
Validation loss: 3.3220196279271246

Epoch: 6| Step: 3
Training loss: 3.373941184778888
Validation loss: 3.3170161048692366

Epoch: 6| Step: 4
Training loss: 4.084478464253566
Validation loss: 3.305091779072227

Epoch: 6| Step: 5
Training loss: 4.015352113706331
Validation loss: 3.3013063707598316

Epoch: 6| Step: 6
Training loss: 2.95437934515246
Validation loss: 3.3012092690966024

Epoch: 6| Step: 7
Training loss: 3.962505564399928
Validation loss: 3.3036540741488287

Epoch: 6| Step: 8
Training loss: 2.821631548445744
Validation loss: 3.3036127533606345

Epoch: 6| Step: 9
Training loss: 3.601876267938161
Validation loss: 3.3015711912452264

Epoch: 6| Step: 10
Training loss: 3.6096265300614463
Validation loss: 3.29973404882564

Epoch: 6| Step: 11
Training loss: 3.4435785822082328
Validation loss: 3.2978476570063973

Epoch: 6| Step: 12
Training loss: 3.2853214254142826
Validation loss: 3.2974152209154246

Epoch: 6| Step: 13
Training loss: 3.4175388959612025
Validation loss: 3.295454059548975

Epoch: 30| Step: 0
Training loss: 2.6687035728543598
Validation loss: 3.296767164172724

Epoch: 6| Step: 1
Training loss: 2.911178184137791
Validation loss: 3.290709704704901

Epoch: 6| Step: 2
Training loss: 3.552633442422776
Validation loss: 3.292727495851752

Epoch: 6| Step: 3
Training loss: 3.6662675611437607
Validation loss: 3.2923265940244946

Epoch: 6| Step: 4
Training loss: 4.199129722893483
Validation loss: 3.291309683650597

Epoch: 6| Step: 5
Training loss: 4.379561308050906
Validation loss: 3.2918461015631535

Epoch: 6| Step: 6
Training loss: 3.444576677908366
Validation loss: 3.287806884411256

Epoch: 6| Step: 7
Training loss: 3.25642727828742
Validation loss: 3.292759703073166

Epoch: 6| Step: 8
Training loss: 3.4207057958533453
Validation loss: 3.293240210144156

Epoch: 6| Step: 9
Training loss: 4.051454758452825
Validation loss: 3.2949487077802453

Epoch: 6| Step: 10
Training loss: 2.666876327696694
Validation loss: 3.293169415091938

Epoch: 6| Step: 11
Training loss: 3.575046997828438
Validation loss: 3.289978406456064

Epoch: 6| Step: 12
Training loss: 4.118120409901733
Validation loss: 3.289160187068308

Epoch: 6| Step: 13
Training loss: 2.5158756676172307
Validation loss: 3.2846130419086577

Epoch: 31| Step: 0
Training loss: 3.0461568915374873
Validation loss: 3.285463843797406

Epoch: 6| Step: 1
Training loss: 3.7801290064197257
Validation loss: 3.2853671400517954

Epoch: 6| Step: 2
Training loss: 3.1591488961984764
Validation loss: 3.283386747322299

Epoch: 6| Step: 3
Training loss: 3.5836929909649364
Validation loss: 3.283854030307818

Epoch: 6| Step: 4
Training loss: 3.8379768497788618
Validation loss: 3.278149661971781

Epoch: 6| Step: 5
Training loss: 3.521029510149032
Validation loss: 3.2806824685066904

Epoch: 6| Step: 6
Training loss: 3.452872426734051
Validation loss: 3.279716457576455

Epoch: 6| Step: 7
Training loss: 3.317571248925819
Validation loss: 3.2818559812675705

Epoch: 6| Step: 8
Training loss: 3.9869302372470483
Validation loss: 3.2809850754756575

Epoch: 6| Step: 9
Training loss: 4.278793409808803
Validation loss: 3.277871103512552

Epoch: 6| Step: 10
Training loss: 2.8155721204324435
Validation loss: 3.2772760909935266

Epoch: 6| Step: 11
Training loss: 3.036692187735563
Validation loss: 3.279580194064388

Epoch: 6| Step: 12
Training loss: 3.1650001926030344
Validation loss: 3.276683988883298

Epoch: 6| Step: 13
Training loss: 4.350521600078435
Validation loss: 3.276767875529489

Epoch: 32| Step: 0
Training loss: 2.4063786063567276
Validation loss: 3.2755867616001018

Epoch: 6| Step: 1
Training loss: 4.0233403166357675
Validation loss: 3.2734877678222523

Epoch: 6| Step: 2
Training loss: 3.632399275053364
Validation loss: 3.2717529648961445

Epoch: 6| Step: 3
Training loss: 3.494732298633377
Validation loss: 3.273014986713595

Epoch: 6| Step: 4
Training loss: 2.6024248223343744
Validation loss: 3.2722102983878454

Epoch: 6| Step: 5
Training loss: 4.040843814315204
Validation loss: 3.270689422992617

Epoch: 6| Step: 6
Training loss: 3.0371608717499097
Validation loss: 3.27601895864991

Epoch: 6| Step: 7
Training loss: 3.8484550150427403
Validation loss: 3.276314643030298

Epoch: 6| Step: 8
Training loss: 3.8067695200487215
Validation loss: 3.270951436590256

Epoch: 6| Step: 9
Training loss: 3.737425894624542
Validation loss: 3.267289389266923

Epoch: 6| Step: 10
Training loss: 3.5253957647988847
Validation loss: 3.2696853432605573

Epoch: 6| Step: 11
Training loss: 3.935999266415039
Validation loss: 3.270439955631815

Epoch: 6| Step: 12
Training loss: 3.264003223193204
Validation loss: 3.270535501673564

Epoch: 6| Step: 13
Training loss: 3.314226384407017
Validation loss: 3.2702342129280595

Epoch: 33| Step: 0
Training loss: 4.283805362999296
Validation loss: 3.270122337673253

Epoch: 6| Step: 1
Training loss: 3.9705424428721843
Validation loss: 3.2682245177104488

Epoch: 6| Step: 2
Training loss: 2.271292077253675
Validation loss: 3.268040790267699

Epoch: 6| Step: 3
Training loss: 3.23972589275211
Validation loss: 3.26548710354446

Epoch: 6| Step: 4
Training loss: 3.956438086017676
Validation loss: 3.2683377725600407

Epoch: 6| Step: 5
Training loss: 3.8385987538509942
Validation loss: 3.2690662454182324

Epoch: 6| Step: 6
Training loss: 3.5113504829844984
Validation loss: 3.266125896082175

Epoch: 6| Step: 7
Training loss: 2.7818937092488105
Validation loss: 3.264262869191997

Epoch: 6| Step: 8
Training loss: 3.6338672706411836
Validation loss: 3.2641355023539216

Epoch: 6| Step: 9
Training loss: 4.053841857339934
Validation loss: 3.2641588144099196

Epoch: 6| Step: 10
Training loss: 3.7252133404360475
Validation loss: 3.2617948868434823

Epoch: 6| Step: 11
Training loss: 3.2202638195787605
Validation loss: 3.2609824212272263

Epoch: 6| Step: 12
Training loss: 2.9239116241417107
Validation loss: 3.266014741968842

Epoch: 6| Step: 13
Training loss: 2.8117178783113315
Validation loss: 3.266015606977677

Epoch: 34| Step: 0
Training loss: 3.4152737701283566
Validation loss: 3.2638998733250726

Epoch: 6| Step: 1
Training loss: 3.2785906960009963
Validation loss: 3.26486071661967

Epoch: 6| Step: 2
Training loss: 3.4833060365425643
Validation loss: 3.2585509907141494

Epoch: 6| Step: 3
Training loss: 3.9118880210717046
Validation loss: 3.258902316431883

Epoch: 6| Step: 4
Training loss: 3.0826283929202187
Validation loss: 3.2602100722940497

Epoch: 6| Step: 5
Training loss: 3.564503558076433
Validation loss: 3.2605847113549977

Epoch: 6| Step: 6
Training loss: 3.707518856028929
Validation loss: 3.261570243851605

Epoch: 6| Step: 7
Training loss: 3.4098714073747036
Validation loss: 3.2575146714628245

Epoch: 6| Step: 8
Training loss: 3.4147980550144927
Validation loss: 3.2615101084922764

Epoch: 6| Step: 9
Training loss: 3.2215632024163496
Validation loss: 3.259454632353618

Epoch: 6| Step: 10
Training loss: 3.2157844288918103
Validation loss: 3.2587194629437346

Epoch: 6| Step: 11
Training loss: 3.2902566006202014
Validation loss: 3.2622337704471494

Epoch: 6| Step: 12
Training loss: 4.172951320149435
Validation loss: 3.274454010533256

Epoch: 6| Step: 13
Training loss: 4.010740166369408
Validation loss: 3.2613922822299894

Epoch: 35| Step: 0
Training loss: 3.6213025935089824
Validation loss: 3.253635469040478

Epoch: 6| Step: 1
Training loss: 2.905473831061536
Validation loss: 3.260762220167099

Epoch: 6| Step: 2
Training loss: 4.289515848294369
Validation loss: 3.2640823242937653

Epoch: 6| Step: 3
Training loss: 3.5775968336732804
Validation loss: 3.266191726481896

Epoch: 6| Step: 4
Training loss: 3.223635918801518
Validation loss: 3.2756403195225476

Epoch: 6| Step: 5
Training loss: 3.6262136598098844
Validation loss: 3.2693236434288195

Epoch: 6| Step: 6
Training loss: 3.596936214930332
Validation loss: 3.2686644407396117

Epoch: 6| Step: 7
Training loss: 4.050444571501583
Validation loss: 3.26689581213838

Epoch: 6| Step: 8
Training loss: 3.3594241116504593
Validation loss: 3.2622353594442806

Epoch: 6| Step: 9
Training loss: 2.9215066443006545
Validation loss: 3.259446534288221

Epoch: 6| Step: 10
Training loss: 3.3753770688251246
Validation loss: 3.2652697806898834

Epoch: 6| Step: 11
Training loss: 3.659805785899428
Validation loss: 3.2695510719970553

Epoch: 6| Step: 12
Training loss: 3.798278298095751
Validation loss: 3.261625446736097

Epoch: 6| Step: 13
Training loss: 2.0280113811436906
Validation loss: 3.2701284196186675

Epoch: 36| Step: 0
Training loss: 4.09245361301836
Validation loss: 3.2634965825929445

Epoch: 6| Step: 1
Training loss: 3.363021929199519
Validation loss: 3.263688116124697

Epoch: 6| Step: 2
Training loss: 3.7218623880962194
Validation loss: 3.262366325667707

Epoch: 6| Step: 3
Training loss: 2.828005962079574
Validation loss: 3.254848682774036

Epoch: 6| Step: 4
Training loss: 3.6759998284922033
Validation loss: 3.2555242070996053

Epoch: 6| Step: 5
Training loss: 3.103639613388204
Validation loss: 3.258857756687882

Epoch: 6| Step: 6
Training loss: 3.1625762085911044
Validation loss: 3.2586958185338046

Epoch: 6| Step: 7
Training loss: 3.273370491744053
Validation loss: 3.2598963302655624

Epoch: 6| Step: 8
Training loss: 3.699689857268853
Validation loss: 3.255936666146858

Epoch: 6| Step: 9
Training loss: 3.2140883975545727
Validation loss: 3.2532971240689252

Epoch: 6| Step: 10
Training loss: 3.988758503215337
Validation loss: 3.25477820421419

Epoch: 6| Step: 11
Training loss: 4.303823616313395
Validation loss: 3.252248259325674

Epoch: 6| Step: 12
Training loss: 2.637017669687972
Validation loss: 3.249422908753464

Epoch: 6| Step: 13
Training loss: 3.626736192270565
Validation loss: 3.2516965973387637

Epoch: 37| Step: 0
Training loss: 3.57502432328066
Validation loss: 3.251244727059136

Epoch: 6| Step: 1
Training loss: 3.381707625525686
Validation loss: 3.252827965574755

Epoch: 6| Step: 2
Training loss: 3.2536508155386876
Validation loss: 3.2512036406709326

Epoch: 6| Step: 3
Training loss: 3.5995120088652754
Validation loss: 3.250080959551705

Epoch: 6| Step: 4
Training loss: 3.620904121248371
Validation loss: 3.248905800700185

Epoch: 6| Step: 5
Training loss: 2.4572211405966633
Validation loss: 3.2459007849386348

Epoch: 6| Step: 6
Training loss: 3.388685946476548
Validation loss: 3.2464506932026826

Epoch: 6| Step: 7
Training loss: 2.8546490412451124
Validation loss: 3.2462853531786

Epoch: 6| Step: 8
Training loss: 3.3601580039975225
Validation loss: 3.245697982585936

Epoch: 6| Step: 9
Training loss: 3.5616507605514496
Validation loss: 3.243790496251563

Epoch: 6| Step: 10
Training loss: 4.534954161827651
Validation loss: 3.243733105684743

Epoch: 6| Step: 11
Training loss: 3.893699932235724
Validation loss: 3.243102167704247

Epoch: 6| Step: 12
Training loss: 3.8467172474127818
Validation loss: 3.2436361086081527

Epoch: 6| Step: 13
Training loss: 2.8959311782077997
Validation loss: 3.242027349725987

Epoch: 38| Step: 0
Training loss: 3.5588100128955666
Validation loss: 3.241652829378236

Epoch: 6| Step: 1
Training loss: 4.510838066778419
Validation loss: 3.2407489018908

Epoch: 6| Step: 2
Training loss: 2.8020284900237953
Validation loss: 3.240270236273902

Epoch: 6| Step: 3
Training loss: 3.2140955187520657
Validation loss: 3.2418909373370925

Epoch: 6| Step: 4
Training loss: 2.474763812730922
Validation loss: 3.2388981532215055

Epoch: 6| Step: 5
Training loss: 3.126654835280731
Validation loss: 3.2383098773464534

Epoch: 6| Step: 6
Training loss: 4.252738070633097
Validation loss: 3.2396179493861372

Epoch: 6| Step: 7
Training loss: 2.705069487520055
Validation loss: 3.2373059387110574

Epoch: 6| Step: 8
Training loss: 2.560437000050106
Validation loss: 3.237822713259453

Epoch: 6| Step: 9
Training loss: 4.137254026540666
Validation loss: 3.236059758964571

Epoch: 6| Step: 10
Training loss: 4.5249815629615115
Validation loss: 3.237355289890606

Epoch: 6| Step: 11
Training loss: 3.980295284186389
Validation loss: 3.234082413693604

Epoch: 6| Step: 12
Training loss: 3.052757180117154
Validation loss: 3.2351842875096692

Epoch: 6| Step: 13
Training loss: 2.419878130279555
Validation loss: 3.2353760961986944

Epoch: 39| Step: 0
Training loss: 3.535949415725407
Validation loss: 3.234434603129582

Epoch: 6| Step: 1
Training loss: 3.7536675638298784
Validation loss: 3.2371655118003746

Epoch: 6| Step: 2
Training loss: 3.655765338303354
Validation loss: 3.2355752891258374

Epoch: 6| Step: 3
Training loss: 3.319475135679073
Validation loss: 3.2352754768989223

Epoch: 6| Step: 4
Training loss: 3.0336753345004226
Validation loss: 3.2367339597175575

Epoch: 6| Step: 5
Training loss: 4.34126265410016
Validation loss: 3.2352068271654466

Epoch: 6| Step: 6
Training loss: 3.557043615194437
Validation loss: 3.2344733915759543

Epoch: 6| Step: 7
Training loss: 2.7852469880248965
Validation loss: 3.230938584403174

Epoch: 6| Step: 8
Training loss: 2.6967216014885667
Validation loss: 3.231799532892197

Epoch: 6| Step: 9
Training loss: 3.0903977236287825
Validation loss: 3.2301273204453222

Epoch: 6| Step: 10
Training loss: 3.7964870427359054
Validation loss: 3.2347532493744824

Epoch: 6| Step: 11
Training loss: 3.227987127323317
Validation loss: 3.232214698984056

Epoch: 6| Step: 12
Training loss: 4.153641649521143
Validation loss: 3.231748081973437

Epoch: 6| Step: 13
Training loss: 3.3224916734440404
Validation loss: 3.2325143351087635

Epoch: 40| Step: 0
Training loss: 3.4518269994887367
Validation loss: 3.229380410722294

Epoch: 6| Step: 1
Training loss: 2.458330014328952
Validation loss: 3.230474687572251

Epoch: 6| Step: 2
Training loss: 3.5154459589826357
Validation loss: 3.2298430852105176

Epoch: 6| Step: 3
Training loss: 3.5185372052802575
Validation loss: 3.2279313698181578

Epoch: 6| Step: 4
Training loss: 2.710390942798691
Validation loss: 3.227651161184622

Epoch: 6| Step: 5
Training loss: 3.9596421871944885
Validation loss: 3.228023684555184

Epoch: 6| Step: 6
Training loss: 3.7028813664251192
Validation loss: 3.228984592371094

Epoch: 6| Step: 7
Training loss: 3.7384054866862346
Validation loss: 3.2268690627903993

Epoch: 6| Step: 8
Training loss: 3.2746040978537643
Validation loss: 3.2311952446626817

Epoch: 6| Step: 9
Training loss: 3.3848098602309986
Validation loss: 3.2241607564829047

Epoch: 6| Step: 10
Training loss: 3.035628469840995
Validation loss: 3.2229132675644134

Epoch: 6| Step: 11
Training loss: 3.8696536437005045
Validation loss: 3.226123007090817

Epoch: 6| Step: 12
Training loss: 4.176672759136639
Validation loss: 3.22499999255948

Epoch: 6| Step: 13
Training loss: 3.500387034133927
Validation loss: 3.226519354471421

Epoch: 41| Step: 0
Training loss: 3.3761357409772352
Validation loss: 3.222865523189926

Epoch: 6| Step: 1
Training loss: 3.224911911818563
Validation loss: 3.2242638121856513

Epoch: 6| Step: 2
Training loss: 2.4929191448138357
Validation loss: 3.2242394340566864

Epoch: 6| Step: 3
Training loss: 2.703771789434763
Validation loss: 3.2259856885418863

Epoch: 6| Step: 4
Training loss: 3.6425155511548986
Validation loss: 3.231679984002989

Epoch: 6| Step: 5
Training loss: 3.429560700247145
Validation loss: 3.2335290454939565

Epoch: 6| Step: 6
Training loss: 3.687178258275667
Validation loss: 3.2331739940989968

Epoch: 6| Step: 7
Training loss: 4.708332208053769
Validation loss: 3.224807083364395

Epoch: 6| Step: 8
Training loss: 3.3172110399946377
Validation loss: 3.225619600355254

Epoch: 6| Step: 9
Training loss: 3.3518147406993184
Validation loss: 3.2249862959324878

Epoch: 6| Step: 10
Training loss: 3.1799163361106038
Validation loss: 3.2318447100263294

Epoch: 6| Step: 11
Training loss: 4.249624796742171
Validation loss: 3.2339942941499324

Epoch: 6| Step: 12
Training loss: 3.8675380355256745
Validation loss: 3.2325100302707357

Epoch: 6| Step: 13
Training loss: 2.1459759291394778
Validation loss: 3.2336632158040812

Epoch: 42| Step: 0
Training loss: 4.285181620965231
Validation loss: 3.226084210425153

Epoch: 6| Step: 1
Training loss: 3.9931455533855087
Validation loss: 3.224261049976384

Epoch: 6| Step: 2
Training loss: 3.1892017982633813
Validation loss: 3.2257191896428634

Epoch: 6| Step: 3
Training loss: 3.2127856947625983
Validation loss: 3.2270526620439037

Epoch: 6| Step: 4
Training loss: 3.029009905663228
Validation loss: 3.22659672711494

Epoch: 6| Step: 5
Training loss: 3.7720815935421976
Validation loss: 3.223145400207935

Epoch: 6| Step: 6
Training loss: 4.0145694517968895
Validation loss: 3.222928692739126

Epoch: 6| Step: 7
Training loss: 2.9210708245895147
Validation loss: 3.2182642021237267

Epoch: 6| Step: 8
Training loss: 3.889471289000471
Validation loss: 3.2223559883870605

Epoch: 6| Step: 9
Training loss: 2.455129788264091
Validation loss: 3.218953049863206

Epoch: 6| Step: 10
Training loss: 3.5073489284268975
Validation loss: 3.2178168325491794

Epoch: 6| Step: 11
Training loss: 3.3924819853605848
Validation loss: 3.221178757109573

Epoch: 6| Step: 12
Training loss: 2.729401517668905
Validation loss: 3.2198502893789787

Epoch: 6| Step: 13
Training loss: 3.8689582238681655
Validation loss: 3.218029167301762

Epoch: 43| Step: 0
Training loss: 3.858906705916304
Validation loss: 3.2198653836786764

Epoch: 6| Step: 1
Training loss: 3.830570483038382
Validation loss: 3.2181931948025015

Epoch: 6| Step: 2
Training loss: 3.4894278116822957
Validation loss: 3.220268220396999

Epoch: 6| Step: 3
Training loss: 2.2696001630028015
Validation loss: 3.217591157242054

Epoch: 6| Step: 4
Training loss: 3.4983005484949543
Validation loss: 3.2174387386582524

Epoch: 6| Step: 5
Training loss: 2.8854194576832923
Validation loss: 3.21912812782536

Epoch: 6| Step: 6
Training loss: 3.5170631010475897
Validation loss: 3.216886896122954

Epoch: 6| Step: 7
Training loss: 4.089348210114826
Validation loss: 3.2186544364126046

Epoch: 6| Step: 8
Training loss: 3.1202649674021075
Validation loss: 3.217809356290299

Epoch: 6| Step: 9
Training loss: 3.819016437631988
Validation loss: 3.2182139941879435

Epoch: 6| Step: 10
Training loss: 3.338068332930364
Validation loss: 3.214521372454908

Epoch: 6| Step: 11
Training loss: 3.1443278939971036
Validation loss: 3.219505376734452

Epoch: 6| Step: 12
Training loss: 3.6829271573008184
Validation loss: 3.218881923139643

Epoch: 6| Step: 13
Training loss: 3.6625718457983547
Validation loss: 3.2179137211897597

Epoch: 44| Step: 0
Training loss: 3.274426003743511
Validation loss: 3.214559166639933

Epoch: 6| Step: 1
Training loss: 4.02931983338406
Validation loss: 3.2129494697165817

Epoch: 6| Step: 2
Training loss: 3.4898015347863645
Validation loss: 3.2167255085298323

Epoch: 6| Step: 3
Training loss: 3.191713726868722
Validation loss: 3.2149345131850398

Epoch: 6| Step: 4
Training loss: 4.051767345094535
Validation loss: 3.213591694182753

Epoch: 6| Step: 5
Training loss: 3.8278119640426267
Validation loss: 3.2155161262018392

Epoch: 6| Step: 6
Training loss: 3.154103975332302
Validation loss: 3.2151830837004445

Epoch: 6| Step: 7
Training loss: 2.4731730181756455
Validation loss: 3.210667341441221

Epoch: 6| Step: 8
Training loss: 3.244194273760571
Validation loss: 3.212264249921672

Epoch: 6| Step: 9
Training loss: 3.5699020583878673
Validation loss: 3.2151554680067895

Epoch: 6| Step: 10
Training loss: 3.2057847308196545
Validation loss: 3.2136784043768487

Epoch: 6| Step: 11
Training loss: 2.968112274740271
Validation loss: 3.2098135461466355

Epoch: 6| Step: 12
Training loss: 3.8749942164224103
Validation loss: 3.2127015036070268

Epoch: 6| Step: 13
Training loss: 4.0084456451183375
Validation loss: 3.211548457713926

Epoch: 45| Step: 0
Training loss: 3.218218713263809
Validation loss: 3.214126739210142

Epoch: 6| Step: 1
Training loss: 2.761806632320339
Validation loss: 3.21241637294601

Epoch: 6| Step: 2
Training loss: 3.2992397704235046
Validation loss: 3.2130692511477292

Epoch: 6| Step: 3
Training loss: 3.3245374659014773
Validation loss: 3.214493242255207

Epoch: 6| Step: 4
Training loss: 3.5396666021732917
Validation loss: 3.2132205158800784

Epoch: 6| Step: 5
Training loss: 3.085746044545504
Validation loss: 3.211866229651111

Epoch: 6| Step: 6
Training loss: 4.04871812478709
Validation loss: 3.210856356692195

Epoch: 6| Step: 7
Training loss: 3.7475537268392665
Validation loss: 3.2113897683091714

Epoch: 6| Step: 8
Training loss: 3.343375211727886
Validation loss: 3.2172273586071776

Epoch: 6| Step: 9
Training loss: 3.1139019547195055
Validation loss: 3.216270446688119

Epoch: 6| Step: 10
Training loss: 4.014324289281039
Validation loss: 3.2197051183810106

Epoch: 6| Step: 11
Training loss: 3.4045319030136425
Validation loss: 3.2185670470329826

Epoch: 6| Step: 12
Training loss: 4.2043863552275305
Validation loss: 3.208971429684039

Epoch: 6| Step: 13
Training loss: 2.6253309495384562
Validation loss: 3.2088741144514854

Epoch: 46| Step: 0
Training loss: 3.790378365665785
Validation loss: 3.2109611503772606

Epoch: 6| Step: 1
Training loss: 3.9816432788577685
Validation loss: 3.205473805059972

Epoch: 6| Step: 2
Training loss: 4.008661905146827
Validation loss: 3.208684086787146

Epoch: 6| Step: 3
Training loss: 3.3478224723081285
Validation loss: 3.209195081354989

Epoch: 6| Step: 4
Training loss: 3.0874911103526026
Validation loss: 3.213954610547868

Epoch: 6| Step: 5
Training loss: 3.282382733239788
Validation loss: 3.2247769759350615

Epoch: 6| Step: 6
Training loss: 3.0904961631465957
Validation loss: 3.2232293273166284

Epoch: 6| Step: 7
Training loss: 3.1154860057072655
Validation loss: 3.2091689383368682

Epoch: 6| Step: 8
Training loss: 3.0026197121519105
Validation loss: 3.21066252103833

Epoch: 6| Step: 9
Training loss: 2.943328269991582
Validation loss: 3.204586409790005

Epoch: 6| Step: 10
Training loss: 4.010339249032928
Validation loss: 3.206574220769154

Epoch: 6| Step: 11
Training loss: 2.931547911904228
Validation loss: 3.203312297060211

Epoch: 6| Step: 12
Training loss: 3.1640172884501325
Validation loss: 3.2054509587589806

Epoch: 6| Step: 13
Training loss: 4.777281466785333
Validation loss: 3.2077138920974835

Epoch: 47| Step: 0
Training loss: 3.1730873534558
Validation loss: 3.2076966538663614

Epoch: 6| Step: 1
Training loss: 3.985293772361878
Validation loss: 3.2051744708972914

Epoch: 6| Step: 2
Training loss: 3.621747366340687
Validation loss: 3.207278807218298

Epoch: 6| Step: 3
Training loss: 4.083303620107069
Validation loss: 3.205744707580969

Epoch: 6| Step: 4
Training loss: 3.6090721895096247
Validation loss: 3.206314486245479

Epoch: 6| Step: 5
Training loss: 1.9019811390403905
Validation loss: 3.2050409085081175

Epoch: 6| Step: 6
Training loss: 3.498234439764814
Validation loss: 3.2043175214974577

Epoch: 6| Step: 7
Training loss: 3.8435954241181207
Validation loss: 3.204261476927652

Epoch: 6| Step: 8
Training loss: 3.5549498744172388
Validation loss: 3.2018032024909773

Epoch: 6| Step: 9
Training loss: 3.2180240145700605
Validation loss: 3.2016385186589424

Epoch: 6| Step: 10
Training loss: 3.2697425691957824
Validation loss: 3.199798654431031

Epoch: 6| Step: 11
Training loss: 2.7976559022990046
Validation loss: 3.20131695428938

Epoch: 6| Step: 12
Training loss: 3.030480670635068
Validation loss: 3.1971285167674366

Epoch: 6| Step: 13
Training loss: 4.573717931993134
Validation loss: 3.1994613300468715

Epoch: 48| Step: 0
Training loss: 3.229886686386532
Validation loss: 3.2009907044501786

Epoch: 6| Step: 1
Training loss: 3.248999221477609
Validation loss: 3.2002373838052645

Epoch: 6| Step: 2
Training loss: 3.134461385158354
Validation loss: 3.198154599473713

Epoch: 6| Step: 3
Training loss: 3.049000473087483
Validation loss: 3.1987212676250745

Epoch: 6| Step: 4
Training loss: 3.5269268207707776
Validation loss: 3.2032286892567234

Epoch: 6| Step: 5
Training loss: 3.6420576035596257
Validation loss: 3.198424964509633

Epoch: 6| Step: 6
Training loss: 4.265871369230249
Validation loss: 3.2039137178613397

Epoch: 6| Step: 7
Training loss: 3.2552195963346815
Validation loss: 3.1963127731412744

Epoch: 6| Step: 8
Training loss: 3.3812171751110744
Validation loss: 3.1924331151089484

Epoch: 6| Step: 9
Training loss: 3.8123614176593255
Validation loss: 3.1938206679961048

Epoch: 6| Step: 10
Training loss: 2.9575780163984677
Validation loss: 3.195808532405124

Epoch: 6| Step: 11
Training loss: 3.803736761786534
Validation loss: 3.1957249930143226

Epoch: 6| Step: 12
Training loss: 3.3382934541521596
Validation loss: 3.1905376446030584

Epoch: 6| Step: 13
Training loss: 3.3626213533987155
Validation loss: 3.1866166822965525

Epoch: 49| Step: 0
Training loss: 3.447008472393481
Validation loss: 3.1907461785139195

Epoch: 6| Step: 1
Training loss: 3.4030676858802877
Validation loss: 3.189453740700705

Epoch: 6| Step: 2
Training loss: 3.8211690180479776
Validation loss: 3.1882758236939823

Epoch: 6| Step: 3
Training loss: 3.7122733512563757
Validation loss: 3.1893247174581054

Epoch: 6| Step: 4
Training loss: 3.6143355197587357
Validation loss: 3.1890147738054715

Epoch: 6| Step: 5
Training loss: 3.584142430949422
Validation loss: 3.1891132899082346

Epoch: 6| Step: 6
Training loss: 3.6292250116492726
Validation loss: 3.1888645620414917

Epoch: 6| Step: 7
Training loss: 2.7512980778681997
Validation loss: 3.19103215930585

Epoch: 6| Step: 8
Training loss: 2.499072093422877
Validation loss: 3.1895193049114776

Epoch: 6| Step: 9
Training loss: 3.9315131555730356
Validation loss: 3.1930870427481133

Epoch: 6| Step: 10
Training loss: 3.774043759123146
Validation loss: 3.189172814507373

Epoch: 6| Step: 11
Training loss: 3.035311465424705
Validation loss: 3.184361285592993

Epoch: 6| Step: 12
Training loss: 3.4660770324405674
Validation loss: 3.183689818425981

Epoch: 6| Step: 13
Training loss: 2.9384981448089937
Validation loss: 3.18642167943944

Epoch: 50| Step: 0
Training loss: 3.112266231475234
Validation loss: 3.1846420356938814

Epoch: 6| Step: 1
Training loss: 4.1487386418181655
Validation loss: 3.1835113234890065

Epoch: 6| Step: 2
Training loss: 2.6927169022463744
Validation loss: 3.1817611612783585

Epoch: 6| Step: 3
Training loss: 3.8506208588005624
Validation loss: 3.1808760661632145

Epoch: 6| Step: 4
Training loss: 2.8419592951866934
Validation loss: 3.1836178603858216

Epoch: 6| Step: 5
Training loss: 3.587537886256694
Validation loss: 3.1817491091451813

Epoch: 6| Step: 6
Training loss: 4.0928320401510145
Validation loss: 3.1797895557998954

Epoch: 6| Step: 7
Training loss: 2.664086036913759
Validation loss: 3.178547326850864

Epoch: 6| Step: 8
Training loss: 4.087395314967153
Validation loss: 3.18065180631895

Epoch: 6| Step: 9
Training loss: 4.2827753844074214
Validation loss: 3.1791731109014085

Epoch: 6| Step: 10
Training loss: 3.300743256299458
Validation loss: 3.1781330254020923

Epoch: 6| Step: 11
Training loss: 2.503916343156822
Validation loss: 3.1785161706813065

Epoch: 6| Step: 12
Training loss: 2.8067654958195116
Validation loss: 3.1798415571874137

Epoch: 6| Step: 13
Training loss: 3.313158941406299
Validation loss: 3.1817346462028135

Epoch: 51| Step: 0
Training loss: 3.643366882179062
Validation loss: 3.1785841123371683

Epoch: 6| Step: 1
Training loss: 3.0128477283669968
Validation loss: 3.1792787174876938

Epoch: 6| Step: 2
Training loss: 3.756799636717004
Validation loss: 3.17477452093818

Epoch: 6| Step: 3
Training loss: 2.464607529724303
Validation loss: 3.1759001585207156

Epoch: 6| Step: 4
Training loss: 3.004378937634752
Validation loss: 3.179616129459363

Epoch: 6| Step: 5
Training loss: 4.0057041504542985
Validation loss: 3.176735303601887

Epoch: 6| Step: 6
Training loss: 3.756438259795867
Validation loss: 3.1754635629489867

Epoch: 6| Step: 7
Training loss: 3.4968439586580944
Validation loss: 3.1797058583854563

Epoch: 6| Step: 8
Training loss: 3.0641703818634753
Validation loss: 3.173891525809056

Epoch: 6| Step: 9
Training loss: 3.9604768804995794
Validation loss: 3.1729058155083827

Epoch: 6| Step: 10
Training loss: 3.2493825472659514
Validation loss: 3.180392879224633

Epoch: 6| Step: 11
Training loss: 3.2833843737151214
Validation loss: 3.1684222452567017

Epoch: 6| Step: 12
Training loss: 3.564681606547026
Validation loss: 3.17228379508019

Epoch: 6| Step: 13
Training loss: 3.3304506870954875
Validation loss: 3.1722703993692

Epoch: 52| Step: 0
Training loss: 3.7065632628235927
Validation loss: 3.1696489848838634

Epoch: 6| Step: 1
Training loss: 3.8046807322353824
Validation loss: 3.1693271353529853

Epoch: 6| Step: 2
Training loss: 3.2791418660319467
Validation loss: 3.1711918525438905

Epoch: 6| Step: 3
Training loss: 3.677745525324972
Validation loss: 3.1700739014171204

Epoch: 6| Step: 4
Training loss: 3.557300856184179
Validation loss: 3.1681202389649954

Epoch: 6| Step: 5
Training loss: 2.610819890531221
Validation loss: 3.1707768430781234

Epoch: 6| Step: 6
Training loss: 3.145383655332589
Validation loss: 3.167024029340812

Epoch: 6| Step: 7
Training loss: 3.9728251283472376
Validation loss: 3.1685070805789253

Epoch: 6| Step: 8
Training loss: 3.281031283400664
Validation loss: 3.168605019061555

Epoch: 6| Step: 9
Training loss: 3.651842248114607
Validation loss: 3.1695666810287695

Epoch: 6| Step: 10
Training loss: 3.3754938965150205
Validation loss: 3.166207272659048

Epoch: 6| Step: 11
Training loss: 2.450528368125842
Validation loss: 3.1667775510880585

Epoch: 6| Step: 12
Training loss: 3.7641612646843776
Validation loss: 3.1653681609461284

Epoch: 6| Step: 13
Training loss: 3.183743376228862
Validation loss: 3.168349378114001

Epoch: 53| Step: 0
Training loss: 3.008869094551874
Validation loss: 3.171682841256526

Epoch: 6| Step: 1
Training loss: 3.673910283047655
Validation loss: 3.167692959402281

Epoch: 6| Step: 2
Training loss: 3.0922211231256083
Validation loss: 3.1740900171104673

Epoch: 6| Step: 3
Training loss: 4.097774711183884
Validation loss: 3.168317242116455

Epoch: 6| Step: 4
Training loss: 2.976193703059309
Validation loss: 3.17031664139714

Epoch: 6| Step: 5
Training loss: 2.6786416907404855
Validation loss: 3.1635304876624035

Epoch: 6| Step: 6
Training loss: 3.9158095916809823
Validation loss: 3.171944677724278

Epoch: 6| Step: 7
Training loss: 3.746403304467708
Validation loss: 3.180588814316322

Epoch: 6| Step: 8
Training loss: 2.8548147389285203
Validation loss: 3.1816105732110125

Epoch: 6| Step: 9
Training loss: 3.7526993255233227
Validation loss: 3.1908610488955893

Epoch: 6| Step: 10
Training loss: 4.031437359381538
Validation loss: 3.182672395295212

Epoch: 6| Step: 11
Training loss: 2.8714926513917933
Validation loss: 3.1636937785299826

Epoch: 6| Step: 12
Training loss: 3.384937772869139
Validation loss: 3.1593121717174495

Epoch: 6| Step: 13
Training loss: 3.4034724679043187
Validation loss: 3.161335330790905

Epoch: 54| Step: 0
Training loss: 2.769765394721523
Validation loss: 3.1582833807400865

Epoch: 6| Step: 1
Training loss: 3.2639352907890955
Validation loss: 3.1567787305053723

Epoch: 6| Step: 2
Training loss: 2.822014883974054
Validation loss: 3.159722249748022

Epoch: 6| Step: 3
Training loss: 3.1724684587598615
Validation loss: 3.1588730416331985

Epoch: 6| Step: 4
Training loss: 4.16943265023887
Validation loss: 3.1590273478023705

Epoch: 6| Step: 5
Training loss: 3.536097077845193
Validation loss: 3.160970630212294

Epoch: 6| Step: 6
Training loss: 3.392632237487631
Validation loss: 3.1576163072802474

Epoch: 6| Step: 7
Training loss: 2.7919656299289994
Validation loss: 3.1606839717909123

Epoch: 6| Step: 8
Training loss: 3.2266538297644805
Validation loss: 3.1590636277858954

Epoch: 6| Step: 9
Training loss: 3.8013823956129147
Validation loss: 3.156940406218893

Epoch: 6| Step: 10
Training loss: 3.6322576283656973
Validation loss: 3.1579498129156063

Epoch: 6| Step: 11
Training loss: 3.438001422724227
Validation loss: 3.157158525012304

Epoch: 6| Step: 12
Training loss: 3.7484516762087345
Validation loss: 3.1570720822397207

Epoch: 6| Step: 13
Training loss: 3.9756689360502224
Validation loss: 3.1584678013038103

Epoch: 55| Step: 0
Training loss: 3.8859774241986837
Validation loss: 3.1561500243511937

Epoch: 6| Step: 1
Training loss: 3.3691719031609706
Validation loss: 3.155246371584851

Epoch: 6| Step: 2
Training loss: 2.770601267645228
Validation loss: 3.1546452098721742

Epoch: 6| Step: 3
Training loss: 3.1461144294371737
Validation loss: 3.1568415105888743

Epoch: 6| Step: 4
Training loss: 3.431026258278645
Validation loss: 3.1563947967874273

Epoch: 6| Step: 5
Training loss: 3.2671550469996453
Validation loss: 3.1552756019831882

Epoch: 6| Step: 6
Training loss: 2.8097387534502887
Validation loss: 3.1543186353733375

Epoch: 6| Step: 7
Training loss: 3.10660402671412
Validation loss: 3.1559686315030784

Epoch: 6| Step: 8
Training loss: 3.9705030519411055
Validation loss: 3.1559975627812453

Epoch: 6| Step: 9
Training loss: 3.0215473427403694
Validation loss: 3.156315740605099

Epoch: 6| Step: 10
Training loss: 3.3522103490448156
Validation loss: 3.156304936360878

Epoch: 6| Step: 11
Training loss: 3.5346641577394324
Validation loss: 3.159685996799516

Epoch: 6| Step: 12
Training loss: 3.7404539038871327
Validation loss: 3.1540812161712264

Epoch: 6| Step: 13
Training loss: 4.428886542997256
Validation loss: 3.154821021423858

Epoch: 56| Step: 0
Training loss: 3.851858007767097
Validation loss: 3.153043535775936

Epoch: 6| Step: 1
Training loss: 3.3491533775347526
Validation loss: 3.152597662158917

Epoch: 6| Step: 2
Training loss: 3.8889001906700003
Validation loss: 3.151042901146258

Epoch: 6| Step: 3
Training loss: 3.497531974379105
Validation loss: 3.1534461977281656

Epoch: 6| Step: 4
Training loss: 2.8749723018472997
Validation loss: 3.1510371490964113

Epoch: 6| Step: 5
Training loss: 3.4098005076474966
Validation loss: 3.1512831452089665

Epoch: 6| Step: 6
Training loss: 3.690062327698518
Validation loss: 3.1525544021223193

Epoch: 6| Step: 7
Training loss: 3.0313734835612682
Validation loss: 3.1507763969178746

Epoch: 6| Step: 8
Training loss: 3.4350607800832593
Validation loss: 3.151555735141718

Epoch: 6| Step: 9
Training loss: 3.31077919654039
Validation loss: 3.1488132918702294

Epoch: 6| Step: 10
Training loss: 2.2992634713492826
Validation loss: 3.1496972244053474

Epoch: 6| Step: 11
Training loss: 3.702886774955406
Validation loss: 3.1493975527954574

Epoch: 6| Step: 12
Training loss: 3.812026197665679
Validation loss: 3.1495726267962683

Epoch: 6| Step: 13
Training loss: 2.986898583564319
Validation loss: 3.1500898311610896

Epoch: 57| Step: 0
Training loss: 4.2506358849030335
Validation loss: 3.1479629960673994

Epoch: 6| Step: 1
Training loss: 3.9331709057735385
Validation loss: 3.149744923738802

Epoch: 6| Step: 2
Training loss: 3.4257673308939007
Validation loss: 3.150360870785361

Epoch: 6| Step: 3
Training loss: 3.3553554884388013
Validation loss: 3.148685258396329

Epoch: 6| Step: 4
Training loss: 2.6566695835603267
Validation loss: 3.1539055869125336

Epoch: 6| Step: 5
Training loss: 2.63943093293218
Validation loss: 3.150708713227793

Epoch: 6| Step: 6
Training loss: 3.066104869245973
Validation loss: 3.1508210238758303

Epoch: 6| Step: 7
Training loss: 3.938055211235709
Validation loss: 3.1531354120001445

Epoch: 6| Step: 8
Training loss: 2.1985916137901818
Validation loss: 3.149929380300759

Epoch: 6| Step: 9
Training loss: 4.067278355313845
Validation loss: 3.1480101043869033

Epoch: 6| Step: 10
Training loss: 3.3662127790011644
Validation loss: 3.148329793800308

Epoch: 6| Step: 11
Training loss: 3.627645382324113
Validation loss: 3.147496946812398

Epoch: 6| Step: 12
Training loss: 3.5542391651506544
Validation loss: 3.1471227882472625

Epoch: 6| Step: 13
Training loss: 2.364059748777212
Validation loss: 3.1468700001198044

Epoch: 58| Step: 0
Training loss: 3.295009283999155
Validation loss: 3.1458992324008386

Epoch: 6| Step: 1
Training loss: 3.8859398756338237
Validation loss: 3.147291071595222

Epoch: 6| Step: 2
Training loss: 3.935882720626638
Validation loss: 3.1446104720071117

Epoch: 6| Step: 3
Training loss: 3.4847923678879407
Validation loss: 3.147151100295802

Epoch: 6| Step: 4
Training loss: 2.3959058888823965
Validation loss: 3.145613066039477

Epoch: 6| Step: 5
Training loss: 3.517476454752926
Validation loss: 3.145674846503101

Epoch: 6| Step: 6
Training loss: 3.748497598411968
Validation loss: 3.1458464580594323

Epoch: 6| Step: 7
Training loss: 3.4358419407648753
Validation loss: 3.1446912719308586

Epoch: 6| Step: 8
Training loss: 3.831202301674369
Validation loss: 3.1451949259755554

Epoch: 6| Step: 9
Training loss: 3.0935715132580337
Validation loss: 3.1430482494312044

Epoch: 6| Step: 10
Training loss: 3.365886392599391
Validation loss: 3.1419991943914893

Epoch: 6| Step: 11
Training loss: 3.478435980234696
Validation loss: 3.14263013683249

Epoch: 6| Step: 12
Training loss: 2.7374439564720663
Validation loss: 3.1413338688250114

Epoch: 6| Step: 13
Training loss: 2.663243590200838
Validation loss: 3.143469390809315

Epoch: 59| Step: 0
Training loss: 3.4430195277732496
Validation loss: 3.1471892571312536

Epoch: 6| Step: 1
Training loss: 3.5660059224762573
Validation loss: 3.1476561550013646

Epoch: 6| Step: 2
Training loss: 2.6206327166490326
Validation loss: 3.152090207778525

Epoch: 6| Step: 3
Training loss: 3.5886961853393142
Validation loss: 3.148441346552763

Epoch: 6| Step: 4
Training loss: 3.725819127695579
Validation loss: 3.145209241501007

Epoch: 6| Step: 5
Training loss: 3.5592320494711247
Validation loss: 3.146948974253474

Epoch: 6| Step: 6
Training loss: 2.864377008579323
Validation loss: 3.1457227111438026

Epoch: 6| Step: 7
Training loss: 3.757076072306514
Validation loss: 3.148074992609565

Epoch: 6| Step: 8
Training loss: 3.2680167606628996
Validation loss: 3.1388659550053473

Epoch: 6| Step: 9
Training loss: 3.580299482978677
Validation loss: 3.1414623528844685

Epoch: 6| Step: 10
Training loss: 3.393571970504551
Validation loss: 3.140721695464971

Epoch: 6| Step: 11
Training loss: 3.368671983937256
Validation loss: 3.1427176163818364

Epoch: 6| Step: 12
Training loss: 3.305483891184892
Validation loss: 3.141168005303964

Epoch: 6| Step: 13
Training loss: 3.2326571560425617
Validation loss: 3.1389443129791346

Epoch: 60| Step: 0
Training loss: 2.471780679207752
Validation loss: 3.142200742188942

Epoch: 6| Step: 1
Training loss: 3.2641215536820796
Validation loss: 3.141832617750031

Epoch: 6| Step: 2
Training loss: 3.934190358259805
Validation loss: 3.1454338717078554

Epoch: 6| Step: 3
Training loss: 3.5060957550011813
Validation loss: 3.1431837478870026

Epoch: 6| Step: 4
Training loss: 3.587103228276613
Validation loss: 3.1443538455986655

Epoch: 6| Step: 5
Training loss: 3.131792849085801
Validation loss: 3.14297048888784

Epoch: 6| Step: 6
Training loss: 2.899403537051458
Validation loss: 3.1403296220571972

Epoch: 6| Step: 7
Training loss: 2.834285875157822
Validation loss: 3.141204161839875

Epoch: 6| Step: 8
Training loss: 3.4336037489188884
Validation loss: 3.1397687650570663

Epoch: 6| Step: 9
Training loss: 3.5407667624904953
Validation loss: 3.137107796695379

Epoch: 6| Step: 10
Training loss: 3.6159112050454776
Validation loss: 3.137844231895799

Epoch: 6| Step: 11
Training loss: 3.8721228962775642
Validation loss: 3.1382642728219867

Epoch: 6| Step: 12
Training loss: 3.5814135236172375
Validation loss: 3.1361468068034126

Epoch: 6| Step: 13
Training loss: 3.692249536820457
Validation loss: 3.134813984965028

Epoch: 61| Step: 0
Training loss: 3.7591552873113785
Validation loss: 3.1361308877576795

Epoch: 6| Step: 1
Training loss: 3.625292667544788
Validation loss: 3.137334414073356

Epoch: 6| Step: 2
Training loss: 4.335017146128324
Validation loss: 3.13718293722247

Epoch: 6| Step: 3
Training loss: 2.6298960484604725
Validation loss: 3.135555027301981

Epoch: 6| Step: 4
Training loss: 3.501829350460078
Validation loss: 3.136639231410725

Epoch: 6| Step: 5
Training loss: 2.8034329384501
Validation loss: 3.135461465089476

Epoch: 6| Step: 6
Training loss: 2.37862270525792
Validation loss: 3.13669620878148

Epoch: 6| Step: 7
Training loss: 3.3440494581516664
Validation loss: 3.135222189431923

Epoch: 6| Step: 8
Training loss: 3.4538264187929806
Validation loss: 3.1387650123574633

Epoch: 6| Step: 9
Training loss: 3.3242617827340983
Validation loss: 3.1358369128574

Epoch: 6| Step: 10
Training loss: 3.5834100995971045
Validation loss: 3.1378553300989713

Epoch: 6| Step: 11
Training loss: 3.3782666256953595
Validation loss: 3.134844619480961

Epoch: 6| Step: 12
Training loss: 3.279288586713394
Validation loss: 3.134526382101678

Epoch: 6| Step: 13
Training loss: 3.812290498574318
Validation loss: 3.1364792196096496

Epoch: 62| Step: 0
Training loss: 3.353155722215167
Validation loss: 3.136378687309699

Epoch: 6| Step: 1
Training loss: 3.211409411128358
Validation loss: 3.1318567063001

Epoch: 6| Step: 2
Training loss: 2.8735039176960675
Validation loss: 3.134915064647203

Epoch: 6| Step: 3
Training loss: 3.421916247254257
Validation loss: 3.1339629421297355

Epoch: 6| Step: 4
Training loss: 3.3887011436055894
Validation loss: 3.136265576650484

Epoch: 6| Step: 5
Training loss: 3.520105380734999
Validation loss: 3.1328837567893846

Epoch: 6| Step: 6
Training loss: 3.966125942421635
Validation loss: 3.13430723551509

Epoch: 6| Step: 7
Training loss: 2.9246926822773367
Validation loss: 3.134635960281622

Epoch: 6| Step: 8
Training loss: 3.192742020313631
Validation loss: 3.131735377298262

Epoch: 6| Step: 9
Training loss: 3.1854057163792957
Validation loss: 3.129580742242328

Epoch: 6| Step: 10
Training loss: 3.2451957993882568
Validation loss: 3.1312287240151613

Epoch: 6| Step: 11
Training loss: 3.222119095858424
Validation loss: 3.1294297223939394

Epoch: 6| Step: 12
Training loss: 4.23353123440072
Validation loss: 3.1303941993199054

Epoch: 6| Step: 13
Training loss: 3.496094704739744
Validation loss: 3.1283956873164427

Epoch: 63| Step: 0
Training loss: 3.4707252445922174
Validation loss: 3.1285621243153967

Epoch: 6| Step: 1
Training loss: 3.080736320962389
Validation loss: 3.1296261055909325

Epoch: 6| Step: 2
Training loss: 2.6986099196914743
Validation loss: 3.127429427181711

Epoch: 6| Step: 3
Training loss: 4.208321801097416
Validation loss: 3.128375814585846

Epoch: 6| Step: 4
Training loss: 3.917531803978197
Validation loss: 3.126650005064591

Epoch: 6| Step: 5
Training loss: 3.544981598153931
Validation loss: 3.1281898051808374

Epoch: 6| Step: 6
Training loss: 3.6029743728848587
Validation loss: 3.127801073752422

Epoch: 6| Step: 7
Training loss: 2.95787191598323
Validation loss: 3.127700173991711

Epoch: 6| Step: 8
Training loss: 3.5771160452311026
Validation loss: 3.1263633887695357

Epoch: 6| Step: 9
Training loss: 3.8948132136441727
Validation loss: 3.1259555364819396

Epoch: 6| Step: 10
Training loss: 3.009071463134677
Validation loss: 3.1255569519543247

Epoch: 6| Step: 11
Training loss: 2.662014887497999
Validation loss: 3.12602109736666

Epoch: 6| Step: 12
Training loss: 2.9840629374537424
Validation loss: 3.124918203616471

Epoch: 6| Step: 13
Training loss: 3.3372787173632537
Validation loss: 3.126036235501188

Epoch: 64| Step: 0
Training loss: 2.470885885965275
Validation loss: 3.124737593595806

Epoch: 6| Step: 1
Training loss: 2.6517473541905425
Validation loss: 3.126431953743643

Epoch: 6| Step: 2
Training loss: 3.1158814717815257
Validation loss: 3.1239493624062007

Epoch: 6| Step: 3
Training loss: 3.6230470066121945
Validation loss: 3.1262135665755784

Epoch: 6| Step: 4
Training loss: 3.4707262063104203
Validation loss: 3.1243577801763713

Epoch: 6| Step: 5
Training loss: 3.6959435728563
Validation loss: 3.1237040763399317

Epoch: 6| Step: 6
Training loss: 3.131108836268018
Validation loss: 3.1229623168488763

Epoch: 6| Step: 7
Training loss: 3.2665161669050935
Validation loss: 3.1246732955299024

Epoch: 6| Step: 8
Training loss: 3.2992049386532334
Validation loss: 3.123345732548778

Epoch: 6| Step: 9
Training loss: 3.6411662865865804
Validation loss: 3.1229586375716614

Epoch: 6| Step: 10
Training loss: 3.9737103319423093
Validation loss: 3.123816872046214

Epoch: 6| Step: 11
Training loss: 3.556866659055109
Validation loss: 3.1227489822068963

Epoch: 6| Step: 12
Training loss: 3.883228793940536
Validation loss: 3.123251462302817

Epoch: 6| Step: 13
Training loss: 3.078349458672532
Validation loss: 3.124675589509435

Epoch: 65| Step: 0
Training loss: 3.4076991367155745
Validation loss: 3.1238176877977653

Epoch: 6| Step: 1
Training loss: 3.6419510288790167
Validation loss: 3.1228260165176596

Epoch: 6| Step: 2
Training loss: 3.0069477373966063
Validation loss: 3.127352100488934

Epoch: 6| Step: 3
Training loss: 3.588325719144627
Validation loss: 3.1269945945090014

Epoch: 6| Step: 4
Training loss: 3.025517499810999
Validation loss: 3.127284957488327

Epoch: 6| Step: 5
Training loss: 3.027206713128258
Validation loss: 3.126560499177904

Epoch: 6| Step: 6
Training loss: 3.665597297323122
Validation loss: 3.125841525402227

Epoch: 6| Step: 7
Training loss: 2.957845477501219
Validation loss: 3.1245913904962004

Epoch: 6| Step: 8
Training loss: 3.823633166670375
Validation loss: 3.1256739630011654

Epoch: 6| Step: 9
Training loss: 3.5411360100020173
Validation loss: 3.1210601277906833

Epoch: 6| Step: 10
Training loss: 3.3503676198547803
Validation loss: 3.121801492590842

Epoch: 6| Step: 11
Training loss: 3.80556946231377
Validation loss: 3.12081247800806

Epoch: 6| Step: 12
Training loss: 3.227058427951392
Validation loss: 3.118324127045121

Epoch: 6| Step: 13
Training loss: 2.8193294259482737
Validation loss: 3.120660025844734

Epoch: 66| Step: 0
Training loss: 3.429231722011079
Validation loss: 3.1209360404435706

Epoch: 6| Step: 1
Training loss: 2.802683845091106
Validation loss: 3.122348694352833

Epoch: 6| Step: 2
Training loss: 3.7531026085178483
Validation loss: 3.122641738015185

Epoch: 6| Step: 3
Training loss: 3.8983255802211723
Validation loss: 3.1185292905278716

Epoch: 6| Step: 4
Training loss: 3.5570225685997316
Validation loss: 3.1194575868370014

Epoch: 6| Step: 5
Training loss: 2.8786983751379673
Validation loss: 3.1179218848406274

Epoch: 6| Step: 6
Training loss: 3.3472613855148787
Validation loss: 3.11893336627766

Epoch: 6| Step: 7
Training loss: 3.872499951534382
Validation loss: 3.1215106335013405

Epoch: 6| Step: 8
Training loss: 4.095263019540839
Validation loss: 3.116643149542896

Epoch: 6| Step: 9
Training loss: 2.342589739665273
Validation loss: 3.1187543035759417

Epoch: 6| Step: 10
Training loss: 2.62052335942787
Validation loss: 3.1161353439200554

Epoch: 6| Step: 11
Training loss: 3.6188087208254545
Validation loss: 3.117007251345164

Epoch: 6| Step: 12
Training loss: 3.2591889398708336
Validation loss: 3.1165406406668197

Epoch: 6| Step: 13
Training loss: 3.258856224257348
Validation loss: 3.118778865896894

Epoch: 67| Step: 0
Training loss: 3.477722111731439
Validation loss: 3.117922407777175

Epoch: 6| Step: 1
Training loss: 3.938842907738675
Validation loss: 3.1199838244715914

Epoch: 6| Step: 2
Training loss: 3.282922227815005
Validation loss: 3.1181117001122516

Epoch: 6| Step: 3
Training loss: 3.329350348901049
Validation loss: 3.119059899628104

Epoch: 6| Step: 4
Training loss: 3.697656513071444
Validation loss: 3.119530569579782

Epoch: 6| Step: 5
Training loss: 3.6423479841004034
Validation loss: 3.1185370146616975

Epoch: 6| Step: 6
Training loss: 3.0585591398305216
Validation loss: 3.115496982776579

Epoch: 6| Step: 7
Training loss: 2.8694698386367805
Validation loss: 3.1148435732439497

Epoch: 6| Step: 8
Training loss: 2.855278374082965
Validation loss: 3.114754073773152

Epoch: 6| Step: 9
Training loss: 4.346662903142443
Validation loss: 3.114091137448117

Epoch: 6| Step: 10
Training loss: 3.092983112268135
Validation loss: 3.1136231297176225

Epoch: 6| Step: 11
Training loss: 2.753046429105057
Validation loss: 3.1129783330356977

Epoch: 6| Step: 12
Training loss: 3.449719160508135
Validation loss: 3.113861096274373

Epoch: 6| Step: 13
Training loss: 2.7465177509839123
Validation loss: 3.113259082567615

Epoch: 68| Step: 0
Training loss: 2.4442407373840243
Validation loss: 3.1133765780418763

Epoch: 6| Step: 1
Training loss: 3.7722957297422792
Validation loss: 3.1152161551286133

Epoch: 6| Step: 2
Training loss: 3.534280472493935
Validation loss: 3.1146570248346417

Epoch: 6| Step: 3
Training loss: 3.2595491240267203
Validation loss: 3.1152184116333714

Epoch: 6| Step: 4
Training loss: 4.093213927135485
Validation loss: 3.113541669279539

Epoch: 6| Step: 5
Training loss: 3.261963758574865
Validation loss: 3.115237757267921

Epoch: 6| Step: 6
Training loss: 3.163393453811227
Validation loss: 3.1164401290272354

Epoch: 6| Step: 7
Training loss: 3.1710464071765467
Validation loss: 3.1136659813259215

Epoch: 6| Step: 8
Training loss: 2.6656001958553888
Validation loss: 3.1160912453099114

Epoch: 6| Step: 9
Training loss: 3.8768867083416825
Validation loss: 3.117390360001033

Epoch: 6| Step: 10
Training loss: 3.270058282921889
Validation loss: 3.116939422442983

Epoch: 6| Step: 11
Training loss: 3.1723082298233383
Validation loss: 3.113936388583607

Epoch: 6| Step: 12
Training loss: 3.746078729848016
Validation loss: 3.110737496774332

Epoch: 6| Step: 13
Training loss: 3.4288728133070823
Validation loss: 3.1112695995175184

Epoch: 69| Step: 0
Training loss: 2.87562123096719
Validation loss: 3.1118423731848446

Epoch: 6| Step: 1
Training loss: 2.744015684711349
Validation loss: 3.1125705388620615

Epoch: 6| Step: 2
Training loss: 3.6765919357917736
Validation loss: 3.117567255285578

Epoch: 6| Step: 3
Training loss: 3.6586329853724426
Validation loss: 3.1162473243703306

Epoch: 6| Step: 4
Training loss: 3.467885914995098
Validation loss: 3.1120510778649577

Epoch: 6| Step: 5
Training loss: 2.9493151112155074
Validation loss: 3.1088381018415685

Epoch: 6| Step: 6
Training loss: 3.790290303246671
Validation loss: 3.10779904469797

Epoch: 6| Step: 7
Training loss: 3.777281136373506
Validation loss: 3.1071717595182156

Epoch: 6| Step: 8
Training loss: 3.4861047667858927
Validation loss: 3.108331704897378

Epoch: 6| Step: 9
Training loss: 3.62494317371946
Validation loss: 3.111481036867076

Epoch: 6| Step: 10
Training loss: 2.357537065297534
Validation loss: 3.1093271711157175

Epoch: 6| Step: 11
Training loss: 3.817571300148023
Validation loss: 3.109045903263048

Epoch: 6| Step: 12
Training loss: 3.0836907428789426
Validation loss: 3.1086833784687657

Epoch: 6| Step: 13
Training loss: 3.5629875786388405
Validation loss: 3.1082649609646644

Epoch: 70| Step: 0
Training loss: 3.497867888488618
Validation loss: 3.1075640746720046

Epoch: 6| Step: 1
Training loss: 3.481460378649027
Validation loss: 3.106731849453812

Epoch: 6| Step: 2
Training loss: 3.0978441987137435
Validation loss: 3.103815700734011

Epoch: 6| Step: 3
Training loss: 3.7725800039455004
Validation loss: 3.1050299345466605

Epoch: 6| Step: 4
Training loss: 3.3322265853957855
Validation loss: 3.1047000552006443

Epoch: 6| Step: 5
Training loss: 3.4277795859286915
Validation loss: 3.101683118173442

Epoch: 6| Step: 6
Training loss: 3.315436447204081
Validation loss: 3.1038881980685513

Epoch: 6| Step: 7
Training loss: 3.2057655429506426
Validation loss: 3.10401442796502

Epoch: 6| Step: 8
Training loss: 3.3847842207726337
Validation loss: 3.1004670560378758

Epoch: 6| Step: 9
Training loss: 3.05053960060485
Validation loss: 3.102945678263161

Epoch: 6| Step: 10
Training loss: 3.309826617814026
Validation loss: 3.106846479280195

Epoch: 6| Step: 11
Training loss: 3.41485377030745
Validation loss: 3.108155068382729

Epoch: 6| Step: 12
Training loss: 3.1322744007266916
Validation loss: 3.109862993961228

Epoch: 6| Step: 13
Training loss: 3.828546617606482
Validation loss: 3.111864153624147

Epoch: 71| Step: 0
Training loss: 3.2565163062325957
Validation loss: 3.1053219104799568

Epoch: 6| Step: 1
Training loss: 3.463244769151422
Validation loss: 3.098955938693857

Epoch: 6| Step: 2
Training loss: 3.4313367210783046
Validation loss: 3.100867311324279

Epoch: 6| Step: 3
Training loss: 3.4691201777156815
Validation loss: 3.097760725290401

Epoch: 6| Step: 4
Training loss: 3.9309974140477997
Validation loss: 3.098803310527326

Epoch: 6| Step: 5
Training loss: 2.6383544369510417
Validation loss: 3.0983499102232392

Epoch: 6| Step: 6
Training loss: 3.5913657862649404
Validation loss: 3.0984390255227496

Epoch: 6| Step: 7
Training loss: 3.716403637689262
Validation loss: 3.101852984050583

Epoch: 6| Step: 8
Training loss: 3.093301143259515
Validation loss: 3.099941782515513

Epoch: 6| Step: 9
Training loss: 3.037299343435833
Validation loss: 3.0975513441264013

Epoch: 6| Step: 10
Training loss: 3.328805200260192
Validation loss: 3.0968462632173597

Epoch: 6| Step: 11
Training loss: 3.6954971493160467
Validation loss: 3.096986824376742

Epoch: 6| Step: 12
Training loss: 3.037994118519678
Validation loss: 3.0962357199777824

Epoch: 6| Step: 13
Training loss: 2.8959278850533705
Validation loss: 3.0941552669136767

Epoch: 72| Step: 0
Training loss: 2.251935550194322
Validation loss: 3.0949532066838263

Epoch: 6| Step: 1
Training loss: 4.024731474265259
Validation loss: 3.0951372245224005

Epoch: 6| Step: 2
Training loss: 3.1708667073308487
Validation loss: 3.0928546964484327

Epoch: 6| Step: 3
Training loss: 3.373442255028275
Validation loss: 3.094643334691145

Epoch: 6| Step: 4
Training loss: 3.0227873347084846
Validation loss: 3.095804478013675

Epoch: 6| Step: 5
Training loss: 3.1325010682368304
Validation loss: 3.0969889815848766

Epoch: 6| Step: 6
Training loss: 3.627808370910708
Validation loss: 3.0940667905147756

Epoch: 6| Step: 7
Training loss: 3.176900556132453
Validation loss: 3.094494881301168

Epoch: 6| Step: 8
Training loss: 2.8919483351781796
Validation loss: 3.0963783770680817

Epoch: 6| Step: 9
Training loss: 3.8285555850394566
Validation loss: 3.103550765541252

Epoch: 6| Step: 10
Training loss: 3.741845483620484
Validation loss: 3.0970215150151352

Epoch: 6| Step: 11
Training loss: 3.0169992428665804
Validation loss: 3.095490654663076

Epoch: 6| Step: 12
Training loss: 3.713475822874407
Validation loss: 3.090643310558993

Epoch: 6| Step: 13
Training loss: 3.8273288443859705
Validation loss: 3.091962713841611

Epoch: 73| Step: 0
Training loss: 3.1966724439720586
Validation loss: 3.092920899295396

Epoch: 6| Step: 1
Training loss: 3.4837465275424475
Validation loss: 3.0893171548346423

Epoch: 6| Step: 2
Training loss: 3.6031620338038786
Validation loss: 3.0906131511859134

Epoch: 6| Step: 3
Training loss: 2.848697090142994
Validation loss: 3.0905271515180495

Epoch: 6| Step: 4
Training loss: 3.2990322196707598
Validation loss: 3.091137177675636

Epoch: 6| Step: 5
Training loss: 3.3246579446024818
Validation loss: 3.0916367289703963

Epoch: 6| Step: 6
Training loss: 4.401253755714658
Validation loss: 3.0934253090102577

Epoch: 6| Step: 7
Training loss: 3.1627218538050625
Validation loss: 3.0902645928556067

Epoch: 6| Step: 8
Training loss: 3.235849680093413
Validation loss: 3.0906413231135583

Epoch: 6| Step: 9
Training loss: 3.1921426210689003
Validation loss: 3.0894469922774848

Epoch: 6| Step: 10
Training loss: 2.9100943129944192
Validation loss: 3.090139682866323

Epoch: 6| Step: 11
Training loss: 3.4238913530777615
Validation loss: 3.0870181522693056

Epoch: 6| Step: 12
Training loss: 3.254751986241557
Validation loss: 3.0879691299752134

Epoch: 6| Step: 13
Training loss: 3.3408822569892997
Validation loss: 3.087056512572562

Epoch: 74| Step: 0
Training loss: 3.4964064815173566
Validation loss: 3.086037198282961

Epoch: 6| Step: 1
Training loss: 3.3840583462927
Validation loss: 3.0866211193812476

Epoch: 6| Step: 2
Training loss: 2.8009126810601983
Validation loss: 3.0864090322090303

Epoch: 6| Step: 3
Training loss: 3.2464066227265813
Validation loss: 3.0864431166131086

Epoch: 6| Step: 4
Training loss: 3.5833707112573316
Validation loss: 3.0855595581679207

Epoch: 6| Step: 5
Training loss: 3.6852721903881855
Validation loss: 3.0869484876368602

Epoch: 6| Step: 6
Training loss: 3.4119787076529438
Validation loss: 3.0878585566046444

Epoch: 6| Step: 7
Training loss: 3.9015244854475637
Validation loss: 3.0915632759728187

Epoch: 6| Step: 8
Training loss: 3.1008787386354197
Validation loss: 3.087299877461707

Epoch: 6| Step: 9
Training loss: 3.549841627429875
Validation loss: 3.0829855853875903

Epoch: 6| Step: 10
Training loss: 2.9295694963214056
Validation loss: 3.0846303065052867

Epoch: 6| Step: 11
Training loss: 2.7591696363606126
Validation loss: 3.084616292441267

Epoch: 6| Step: 12
Training loss: 2.944793736434945
Validation loss: 3.087458498086597

Epoch: 6| Step: 13
Training loss: 4.138513336097474
Validation loss: 3.0858131492673118

Epoch: 75| Step: 0
Training loss: 2.3756403812591955
Validation loss: 3.0871830113301346

Epoch: 6| Step: 1
Training loss: 3.5505661096642998
Validation loss: 3.0868165551941678

Epoch: 6| Step: 2
Training loss: 2.6100979106266995
Validation loss: 3.0868550608490897

Epoch: 6| Step: 3
Training loss: 3.736715309672653
Validation loss: 3.087969455414696

Epoch: 6| Step: 4
Training loss: 3.3554200067854447
Validation loss: 3.08567603381983

Epoch: 6| Step: 5
Training loss: 3.464677636810409
Validation loss: 3.0855488036121943

Epoch: 6| Step: 6
Training loss: 3.4602945733775177
Validation loss: 3.0835014642196747

Epoch: 6| Step: 7
Training loss: 3.64943470692582
Validation loss: 3.0846680399412776

Epoch: 6| Step: 8
Training loss: 3.028013093846985
Validation loss: 3.0829362161774614

Epoch: 6| Step: 9
Training loss: 3.0788963629392763
Validation loss: 3.080922817205152

Epoch: 6| Step: 10
Training loss: 4.035493731349575
Validation loss: 3.0816714961423397

Epoch: 6| Step: 11
Training loss: 3.0455712292185275
Validation loss: 3.081663767787323

Epoch: 6| Step: 12
Training loss: 3.1872411884626204
Validation loss: 3.0802444832472045

Epoch: 6| Step: 13
Training loss: 4.181477101178562
Validation loss: 3.0793056144692894

Epoch: 76| Step: 0
Training loss: 2.8570553902452938
Validation loss: 3.0786425460092373

Epoch: 6| Step: 1
Training loss: 3.3873521329131218
Validation loss: 3.079039924969705

Epoch: 6| Step: 2
Training loss: 3.032861811980369
Validation loss: 3.0792953326010144

Epoch: 6| Step: 3
Training loss: 3.74404663541066
Validation loss: 3.08194514395187

Epoch: 6| Step: 4
Training loss: 3.5203911919397695
Validation loss: 3.0824524126287733

Epoch: 6| Step: 5
Training loss: 3.2647325674360284
Validation loss: 3.0862146571123246

Epoch: 6| Step: 6
Training loss: 3.348172551939496
Validation loss: 3.0872235759672813

Epoch: 6| Step: 7
Training loss: 3.189081285973652
Validation loss: 3.0858212734820847

Epoch: 6| Step: 8
Training loss: 3.168135820868129
Validation loss: 3.079350731900478

Epoch: 6| Step: 9
Training loss: 3.776058447789579
Validation loss: 3.0794889423264333

Epoch: 6| Step: 10
Training loss: 3.043296864831809
Validation loss: 3.078995015302924

Epoch: 6| Step: 11
Training loss: 3.8018585277967483
Validation loss: 3.07647012963037

Epoch: 6| Step: 12
Training loss: 2.459550933167817
Validation loss: 3.0757403387815763

Epoch: 6| Step: 13
Training loss: 4.247041233337733
Validation loss: 3.075545367775625

Epoch: 77| Step: 0
Training loss: 3.6226022617182867
Validation loss: 3.076165992951259

Epoch: 6| Step: 1
Training loss: 3.3266872112070436
Validation loss: 3.0737635098252576

Epoch: 6| Step: 2
Training loss: 3.217173069703401
Validation loss: 3.076056877157436

Epoch: 6| Step: 3
Training loss: 3.1421171221256654
Validation loss: 3.074882207135537

Epoch: 6| Step: 4
Training loss: 3.8488394684906546
Validation loss: 3.070900558136801

Epoch: 6| Step: 5
Training loss: 3.227660059428272
Validation loss: 3.073836524125002

Epoch: 6| Step: 6
Training loss: 2.727541160377818
Validation loss: 3.0717921720952717

Epoch: 6| Step: 7
Training loss: 2.8369026328298155
Validation loss: 3.0762989638419156

Epoch: 6| Step: 8
Training loss: 3.4969144571786703
Validation loss: 3.0737206765866563

Epoch: 6| Step: 9
Training loss: 3.582247695290631
Validation loss: 3.0726904789832328

Epoch: 6| Step: 10
Training loss: 3.484185594277534
Validation loss: 3.0715916255467914

Epoch: 6| Step: 11
Training loss: 3.641886218426697
Validation loss: 3.0714200626849992

Epoch: 6| Step: 12
Training loss: 3.350468810571194
Validation loss: 3.0717726479880856

Epoch: 6| Step: 13
Training loss: 2.750759020008337
Validation loss: 3.071068722450423

Epoch: 78| Step: 0
Training loss: 3.3892918088376667
Validation loss: 3.07217528651707

Epoch: 6| Step: 1
Training loss: 3.8492095135747717
Validation loss: 3.070368859602175

Epoch: 6| Step: 2
Training loss: 3.6213665872806438
Validation loss: 3.071275081048723

Epoch: 6| Step: 3
Training loss: 2.7198326323010193
Validation loss: 3.071139051143786

Epoch: 6| Step: 4
Training loss: 2.9065723855654686
Validation loss: 3.0701446767054343

Epoch: 6| Step: 5
Training loss: 3.101203494109576
Validation loss: 3.0709768820213674

Epoch: 6| Step: 6
Training loss: 2.9280053764669116
Validation loss: 3.070681046793471

Epoch: 6| Step: 7
Training loss: 3.470338062671427
Validation loss: 3.0718888210244883

Epoch: 6| Step: 8
Training loss: 3.528606539386652
Validation loss: 3.0759316452456766

Epoch: 6| Step: 9
Training loss: 3.5068292747865555
Validation loss: 3.070952783857354

Epoch: 6| Step: 10
Training loss: 3.214239274552333
Validation loss: 3.0705514594778327

Epoch: 6| Step: 11
Training loss: 3.862097764289529
Validation loss: 3.0679035511505752

Epoch: 6| Step: 12
Training loss: 3.413923666638688
Validation loss: 3.068006853672618

Epoch: 6| Step: 13
Training loss: 2.580802960051313
Validation loss: 3.0670908792564453

Epoch: 79| Step: 0
Training loss: 2.860007627783787
Validation loss: 3.0681005469144114

Epoch: 6| Step: 1
Training loss: 4.083881860337581
Validation loss: 3.070158947210981

Epoch: 6| Step: 2
Training loss: 3.646720269719894
Validation loss: 3.0688956018437685

Epoch: 6| Step: 3
Training loss: 2.9344791544181814
Validation loss: 3.0705711366236805

Epoch: 6| Step: 4
Training loss: 3.322748560624374
Validation loss: 3.068200507282652

Epoch: 6| Step: 5
Training loss: 2.9370758075016092
Validation loss: 3.070682100407922

Epoch: 6| Step: 6
Training loss: 3.4978524840424687
Validation loss: 3.065990226285596

Epoch: 6| Step: 7
Training loss: 3.2228980141321677
Validation loss: 3.0681278769132687

Epoch: 6| Step: 8
Training loss: 2.787893269256881
Validation loss: 3.066989075546725

Epoch: 6| Step: 9
Training loss: 3.2622435370305296
Validation loss: 3.067215189981359

Epoch: 6| Step: 10
Training loss: 3.249481013014674
Validation loss: 3.0669364571660473

Epoch: 6| Step: 11
Training loss: 3.285063933936268
Validation loss: 3.067441120596378

Epoch: 6| Step: 12
Training loss: 3.4940062017795177
Validation loss: 3.0655592856003215

Epoch: 6| Step: 13
Training loss: 4.1841959657892405
Validation loss: 3.065340484013386

Epoch: 80| Step: 0
Training loss: 3.3086727272524934
Validation loss: 3.0667098958413637

Epoch: 6| Step: 1
Training loss: 4.100270676400641
Validation loss: 3.066548381996923

Epoch: 6| Step: 2
Training loss: 2.065013394439336
Validation loss: 3.0669420819066504

Epoch: 6| Step: 3
Training loss: 3.2669855961197483
Validation loss: 3.066553352860893

Epoch: 6| Step: 4
Training loss: 3.356651444108597
Validation loss: 3.066844872499477

Epoch: 6| Step: 5
Training loss: 3.7159171975296332
Validation loss: 3.0652111710631655

Epoch: 6| Step: 6
Training loss: 3.3755862821757474
Validation loss: 3.0664419361925175

Epoch: 6| Step: 7
Training loss: 3.5894045888035557
Validation loss: 3.064827276440933

Epoch: 6| Step: 8
Training loss: 3.452599118610228
Validation loss: 3.067576064244091

Epoch: 6| Step: 9
Training loss: 3.379766136520875
Validation loss: 3.0680142437507874

Epoch: 6| Step: 10
Training loss: 3.0098302637445746
Validation loss: 3.07059558760605

Epoch: 6| Step: 11
Training loss: 3.158389735326349
Validation loss: 3.0773546399960474

Epoch: 6| Step: 12
Training loss: 3.1095564132752616
Validation loss: 3.0806596954609318

Epoch: 6| Step: 13
Training loss: 3.3968916260032547
Validation loss: 3.0722002087215414

Epoch: 81| Step: 0
Training loss: 3.5221989859193266
Validation loss: 3.068036965372164

Epoch: 6| Step: 1
Training loss: 2.5061307122435945
Validation loss: 3.0646787272150657

Epoch: 6| Step: 2
Training loss: 3.0570277934709247
Validation loss: 3.0629493613071843

Epoch: 6| Step: 3
Training loss: 3.4830770084915605
Validation loss: 3.061997249104539

Epoch: 6| Step: 4
Training loss: 3.6287645983689787
Validation loss: 3.0613822898063385

Epoch: 6| Step: 5
Training loss: 3.034929382407632
Validation loss: 3.0606444569870863

Epoch: 6| Step: 6
Training loss: 3.8417004338984992
Validation loss: 3.062112398211511

Epoch: 6| Step: 7
Training loss: 3.0583976205276224
Validation loss: 3.0631866639572287

Epoch: 6| Step: 8
Training loss: 3.9315838646238124
Validation loss: 3.063782882618062

Epoch: 6| Step: 9
Training loss: 3.4168692427957033
Validation loss: 3.063060298223891

Epoch: 6| Step: 10
Training loss: 3.4124618025098634
Validation loss: 3.060956256560261

Epoch: 6| Step: 11
Training loss: 3.2074635965804306
Validation loss: 3.064915732024415

Epoch: 6| Step: 12
Training loss: 3.0802143577195977
Validation loss: 3.062475393973692

Epoch: 6| Step: 13
Training loss: 3.0233435012438914
Validation loss: 3.061253171432316

Epoch: 82| Step: 0
Training loss: 3.8647307636007544
Validation loss: 3.0593213814790765

Epoch: 6| Step: 1
Training loss: 3.808760262786289
Validation loss: 3.061359342935506

Epoch: 6| Step: 2
Training loss: 3.5792780854746646
Validation loss: 3.0601754791444686

Epoch: 6| Step: 3
Training loss: 3.797291171652441
Validation loss: 3.0587921465889374

Epoch: 6| Step: 4
Training loss: 3.2054610503083554
Validation loss: 3.0636932680195716

Epoch: 6| Step: 5
Training loss: 2.970690283913733
Validation loss: 3.0637931311961073

Epoch: 6| Step: 6
Training loss: 3.0517754686989242
Validation loss: 3.0649732238572063

Epoch: 6| Step: 7
Training loss: 3.0793993485581552
Validation loss: 3.0685880552074174

Epoch: 6| Step: 8
Training loss: 3.7300001239520912
Validation loss: 3.0757255573932705

Epoch: 6| Step: 9
Training loss: 3.481366693537267
Validation loss: 3.070088307142522

Epoch: 6| Step: 10
Training loss: 2.8969951649982004
Validation loss: 3.06210909289238

Epoch: 6| Step: 11
Training loss: 2.961352799814345
Validation loss: 3.062890740037331

Epoch: 6| Step: 12
Training loss: 2.93619768208956
Validation loss: 3.0586991320843024

Epoch: 6| Step: 13
Training loss: 2.6253087906640413
Validation loss: 3.058102264308046

Epoch: 83| Step: 0
Training loss: 3.182600856480423
Validation loss: 3.0581254787269527

Epoch: 6| Step: 1
Training loss: 2.9204161566905626
Validation loss: 3.060125550928898

Epoch: 6| Step: 2
Training loss: 2.5229242233889804
Validation loss: 3.0601149985275065

Epoch: 6| Step: 3
Training loss: 3.9724105413889914
Validation loss: 3.0617353796138036

Epoch: 6| Step: 4
Training loss: 2.5589500586826683
Validation loss: 3.062726914598646

Epoch: 6| Step: 5
Training loss: 3.733168530800332
Validation loss: 3.0624482052315463

Epoch: 6| Step: 6
Training loss: 3.369456789966039
Validation loss: 3.0636087804136536

Epoch: 6| Step: 7
Training loss: 3.534337137493372
Validation loss: 3.0624641113321394

Epoch: 6| Step: 8
Training loss: 4.001804183340529
Validation loss: 3.0597376818642137

Epoch: 6| Step: 9
Training loss: 2.7395737409725918
Validation loss: 3.058195035421243

Epoch: 6| Step: 10
Training loss: 3.548500663164928
Validation loss: 3.0569264832037564

Epoch: 6| Step: 11
Training loss: 3.236433912630566
Validation loss: 3.0564941874399434

Epoch: 6| Step: 12
Training loss: 3.649969983957274
Validation loss: 3.0570110884016746

Epoch: 6| Step: 13
Training loss: 3.0023197106603305
Validation loss: 3.055642019063406

Epoch: 84| Step: 0
Training loss: 3.3201967466036146
Validation loss: 3.0550239998060853

Epoch: 6| Step: 1
Training loss: 3.6145405322498934
Validation loss: 3.056160048119773

Epoch: 6| Step: 2
Training loss: 3.1167011694264812
Validation loss: 3.054873606825361

Epoch: 6| Step: 3
Training loss: 3.65663979563639
Validation loss: 3.0586195497942965

Epoch: 6| Step: 4
Training loss: 3.409600526114557
Validation loss: 3.0587111225970864

Epoch: 6| Step: 5
Training loss: 3.705291243211724
Validation loss: 3.0584947547539247

Epoch: 6| Step: 6
Training loss: 3.199170756183808
Validation loss: 3.061760304741009

Epoch: 6| Step: 7
Training loss: 3.1655292643373976
Validation loss: 3.063436198079208

Epoch: 6| Step: 8
Training loss: 2.4283698823788216
Validation loss: 3.061661653034667

Epoch: 6| Step: 9
Training loss: 3.5621494321498264
Validation loss: 3.061810966563472

Epoch: 6| Step: 10
Training loss: 2.938046343703418
Validation loss: 3.0590752711730147

Epoch: 6| Step: 11
Training loss: 3.3091595198638086
Validation loss: 3.062390353743155

Epoch: 6| Step: 12
Training loss: 3.5957887920097815
Validation loss: 3.059019698273715

Epoch: 6| Step: 13
Training loss: 3.2495976712547683
Validation loss: 3.0529580721000307

Epoch: 85| Step: 0
Training loss: 3.19855034894197
Validation loss: 3.0554025463178367

Epoch: 6| Step: 1
Training loss: 3.534925455389221
Validation loss: 3.0539691584914483

Epoch: 6| Step: 2
Training loss: 3.1267149225129303
Validation loss: 3.052736629821979

Epoch: 6| Step: 3
Training loss: 4.418293395454891
Validation loss: 3.0519301429837755

Epoch: 6| Step: 4
Training loss: 3.230906667860175
Validation loss: 3.0530153138349005

Epoch: 6| Step: 5
Training loss: 3.2636140171038486
Validation loss: 3.0489905279296656

Epoch: 6| Step: 6
Training loss: 3.8026107352621126
Validation loss: 3.0514074647687317

Epoch: 6| Step: 7
Training loss: 3.8234049444523523
Validation loss: 3.0509233502954767

Epoch: 6| Step: 8
Training loss: 2.2532451327779346
Validation loss: 3.0490544814208973

Epoch: 6| Step: 9
Training loss: 3.1010291267904333
Validation loss: 3.051398725513688

Epoch: 6| Step: 10
Training loss: 3.185589535706451
Validation loss: 3.0520778881207677

Epoch: 6| Step: 11
Training loss: 2.662701300949585
Validation loss: 3.051161466431385

Epoch: 6| Step: 12
Training loss: 3.32185539749405
Validation loss: 3.0511669858181136

Epoch: 6| Step: 13
Training loss: 2.693068479634065
Validation loss: 3.051073925387902

Epoch: 86| Step: 0
Training loss: 3.381113237869285
Validation loss: 3.0513794741958127

Epoch: 6| Step: 1
Training loss: 3.316810969627263
Validation loss: 3.0515165092772523

Epoch: 6| Step: 2
Training loss: 2.8476445285466925
Validation loss: 3.054406248304912

Epoch: 6| Step: 3
Training loss: 3.8706005101697296
Validation loss: 3.051422687412494

Epoch: 6| Step: 4
Training loss: 2.421957888261521
Validation loss: 3.050711776372813

Epoch: 6| Step: 5
Training loss: 3.287641857263108
Validation loss: 3.0511675521239354

Epoch: 6| Step: 6
Training loss: 3.144091311179538
Validation loss: 3.049699546153712

Epoch: 6| Step: 7
Training loss: 4.090552559160903
Validation loss: 3.0476835748751507

Epoch: 6| Step: 8
Training loss: 3.7942161512481833
Validation loss: 3.0493779615098346

Epoch: 6| Step: 9
Training loss: 3.226272385233947
Validation loss: 3.0483561316902903

Epoch: 6| Step: 10
Training loss: 2.6056703320808032
Validation loss: 3.0472330888957795

Epoch: 6| Step: 11
Training loss: 3.430486425268673
Validation loss: 3.047853638409146

Epoch: 6| Step: 12
Training loss: 3.4966178629050977
Validation loss: 3.0451053180333667

Epoch: 6| Step: 13
Training loss: 2.908373641324104
Validation loss: 3.0454082348301266

Epoch: 87| Step: 0
Training loss: 3.3447701867593067
Validation loss: 3.044401376220907

Epoch: 6| Step: 1
Training loss: 3.196429292963408
Validation loss: 3.045460091275537

Epoch: 6| Step: 2
Training loss: 3.83533729472476
Validation loss: 3.0468301597810643

Epoch: 6| Step: 3
Training loss: 3.4385452068641524
Validation loss: 3.047277046562672

Epoch: 6| Step: 4
Training loss: 2.9521552214195954
Validation loss: 3.0462411764469044

Epoch: 6| Step: 5
Training loss: 3.5844063741679433
Validation loss: 3.044079547679059

Epoch: 6| Step: 6
Training loss: 3.3097228878687748
Validation loss: 3.0451203372958786

Epoch: 6| Step: 7
Training loss: 2.5447448996828093
Validation loss: 3.04674560171135

Epoch: 6| Step: 8
Training loss: 2.474804275171864
Validation loss: 3.047774535850475

Epoch: 6| Step: 9
Training loss: 3.745429241603347
Validation loss: 3.0525262977303322

Epoch: 6| Step: 10
Training loss: 3.5440680197997447
Validation loss: 3.046248465319877

Epoch: 6| Step: 11
Training loss: 3.3049229747666775
Validation loss: 3.043990756118281

Epoch: 6| Step: 12
Training loss: 2.962722441615523
Validation loss: 3.0426717326226584

Epoch: 6| Step: 13
Training loss: 4.098786030128478
Validation loss: 3.042117281979053

Epoch: 88| Step: 0
Training loss: 3.0437783179248505
Validation loss: 3.0438696423759097

Epoch: 6| Step: 1
Training loss: 3.1046510015606743
Validation loss: 3.0407281841809004

Epoch: 6| Step: 2
Training loss: 2.7516028328170203
Validation loss: 3.044188527926301

Epoch: 6| Step: 3
Training loss: 3.2069589883188074
Validation loss: 3.0438148109752574

Epoch: 6| Step: 4
Training loss: 4.099828269897578
Validation loss: 3.0421071196509133

Epoch: 6| Step: 5
Training loss: 3.3271626258594598
Validation loss: 3.042073113232101

Epoch: 6| Step: 6
Training loss: 3.622806016673849
Validation loss: 3.043226428479965

Epoch: 6| Step: 7
Training loss: 3.350254897483592
Validation loss: 3.043804390690778

Epoch: 6| Step: 8
Training loss: 3.223239767247849
Validation loss: 3.043928242543494

Epoch: 6| Step: 9
Training loss: 3.82966153195564
Validation loss: 3.047073439411549

Epoch: 6| Step: 10
Training loss: 3.5981217040788493
Validation loss: 3.0457409031454294

Epoch: 6| Step: 11
Training loss: 2.7960691516494482
Validation loss: 3.044491466049759

Epoch: 6| Step: 12
Training loss: 2.994093006847781
Validation loss: 3.04484791643394

Epoch: 6| Step: 13
Training loss: 2.9221700427574473
Validation loss: 3.043393704361963

Epoch: 89| Step: 0
Training loss: 4.27015194303167
Validation loss: 3.0427306052545138

Epoch: 6| Step: 1
Training loss: 3.2440287681555176
Validation loss: 3.039954281799412

Epoch: 6| Step: 2
Training loss: 3.8937786756411734
Validation loss: 3.039821425498707

Epoch: 6| Step: 3
Training loss: 2.968934866520009
Validation loss: 3.039994678031201

Epoch: 6| Step: 4
Training loss: 3.018636040283951
Validation loss: 3.038344983537164

Epoch: 6| Step: 5
Training loss: 3.260543447026322
Validation loss: 3.0396162638319537

Epoch: 6| Step: 6
Training loss: 3.6005132309329246
Validation loss: 3.0379464772060616

Epoch: 6| Step: 7
Training loss: 3.50580469795455
Validation loss: 3.0401644490023996

Epoch: 6| Step: 8
Training loss: 3.607159615872961
Validation loss: 3.0400621922251165

Epoch: 6| Step: 9
Training loss: 2.84670397582906
Validation loss: 3.0384439526995877

Epoch: 6| Step: 10
Training loss: 3.031996330329466
Validation loss: 3.0384986405603076

Epoch: 6| Step: 11
Training loss: 2.7930412229725285
Validation loss: 3.037412006341431

Epoch: 6| Step: 12
Training loss: 2.7216378144765105
Validation loss: 3.037539004425656

Epoch: 6| Step: 13
Training loss: 3.0302323668361613
Validation loss: 3.036851909386329

Epoch: 90| Step: 0
Training loss: 3.327695147196597
Validation loss: 3.0381844670395086

Epoch: 6| Step: 1
Training loss: 3.280460517002648
Validation loss: 3.0379335203542697

Epoch: 6| Step: 2
Training loss: 2.8836913384538283
Validation loss: 3.0366634030251003

Epoch: 6| Step: 3
Training loss: 3.560795694216694
Validation loss: 3.0370415199797054

Epoch: 6| Step: 4
Training loss: 3.2156072291707085
Validation loss: 3.0364661255244374

Epoch: 6| Step: 5
Training loss: 3.472171505027819
Validation loss: 3.039004030399893

Epoch: 6| Step: 6
Training loss: 3.088453286221418
Validation loss: 3.036574067040428

Epoch: 6| Step: 7
Training loss: 3.0078892289320414
Validation loss: 3.0379857322309607

Epoch: 6| Step: 8
Training loss: 2.7208564753422078
Validation loss: 3.0377985545035493

Epoch: 6| Step: 9
Training loss: 3.3544186957469924
Validation loss: 3.037292759817833

Epoch: 6| Step: 10
Training loss: 3.3620864216750177
Validation loss: 3.0368838013830115

Epoch: 6| Step: 11
Training loss: 3.667364978762159
Validation loss: 3.0343185977005382

Epoch: 6| Step: 12
Training loss: 3.4142718567370323
Validation loss: 3.0359171053619627

Epoch: 6| Step: 13
Training loss: 4.042646518710749
Validation loss: 3.0365361496557233

Epoch: 91| Step: 0
Training loss: 2.6420167043245915
Validation loss: 3.0386988114869284

Epoch: 6| Step: 1
Training loss: 3.0160871556790023
Validation loss: 3.035853044008813

Epoch: 6| Step: 2
Training loss: 3.968005613669147
Validation loss: 3.038074284128171

Epoch: 6| Step: 3
Training loss: 3.6071207512551813
Validation loss: 3.036745954034316

Epoch: 6| Step: 4
Training loss: 4.1415933880121525
Validation loss: 3.034734430180065

Epoch: 6| Step: 5
Training loss: 2.9905982515579854
Validation loss: 3.0345146833671235

Epoch: 6| Step: 6
Training loss: 3.3848708588210563
Validation loss: 3.033676519275678

Epoch: 6| Step: 7
Training loss: 3.6428781503450973
Validation loss: 3.036786643670208

Epoch: 6| Step: 8
Training loss: 3.5979046233652454
Validation loss: 3.037304009355436

Epoch: 6| Step: 9
Training loss: 2.226636597588328
Validation loss: 3.046243323304155

Epoch: 6| Step: 10
Training loss: 4.009299911307283
Validation loss: 3.0385773872582194

Epoch: 6| Step: 11
Training loss: 2.8423268875914487
Validation loss: 3.044857284068258

Epoch: 6| Step: 12
Training loss: 1.992856620167372
Validation loss: 3.0382635104546947

Epoch: 6| Step: 13
Training loss: 3.319121310912015
Validation loss: 3.03955640817118

Epoch: 92| Step: 0
Training loss: 4.173375234992749
Validation loss: 3.031356527175399

Epoch: 6| Step: 1
Training loss: 3.2642296543298013
Validation loss: 3.0302332010128885

Epoch: 6| Step: 2
Training loss: 2.943249695980309
Validation loss: 3.030772828853444

Epoch: 6| Step: 3
Training loss: 2.5060083667588056
Validation loss: 3.03465350980547

Epoch: 6| Step: 4
Training loss: 3.2986694080934513
Validation loss: 3.0324084080577185

Epoch: 6| Step: 5
Training loss: 3.341379341336443
Validation loss: 3.0320118503961555

Epoch: 6| Step: 6
Training loss: 2.4909201721117054
Validation loss: 3.0300568868188766

Epoch: 6| Step: 7
Training loss: 4.069433311357484
Validation loss: 3.032876689006531

Epoch: 6| Step: 8
Training loss: 2.9911111434544786
Validation loss: 3.033264696651177

Epoch: 6| Step: 9
Training loss: 3.5777122559349417
Validation loss: 3.0313536991285472

Epoch: 6| Step: 10
Training loss: 3.5654789031285525
Validation loss: 3.031578447895662

Epoch: 6| Step: 11
Training loss: 3.601150191200717
Validation loss: 3.0313050316146515

Epoch: 6| Step: 12
Training loss: 2.8860671943143763
Validation loss: 3.0300300544318493

Epoch: 6| Step: 13
Training loss: 2.7408003817284667
Validation loss: 3.032315980914079

Epoch: 93| Step: 0
Training loss: 3.209513030385126
Validation loss: 3.0298031837602846

Epoch: 6| Step: 1
Training loss: 3.9759644543922086
Validation loss: 3.0282985322467066

Epoch: 6| Step: 2
Training loss: 3.4309438433363972
Validation loss: 3.0298998572095237

Epoch: 6| Step: 3
Training loss: 3.0679846716327535
Validation loss: 3.0291776966289614

Epoch: 6| Step: 4
Training loss: 2.93553631667055
Validation loss: 3.0285333508084933

Epoch: 6| Step: 5
Training loss: 2.4791125337049444
Validation loss: 3.0290571754907867

Epoch: 6| Step: 6
Training loss: 3.960132283785243
Validation loss: 3.0296642839960795

Epoch: 6| Step: 7
Training loss: 3.852276285200044
Validation loss: 3.0270866826339295

Epoch: 6| Step: 8
Training loss: 3.3181524453579234
Validation loss: 3.0304198663921373

Epoch: 6| Step: 9
Training loss: 3.2355288598888787
Validation loss: 3.039923099204699

Epoch: 6| Step: 10
Training loss: 3.1817196062844912
Validation loss: 3.0496154375281272

Epoch: 6| Step: 11
Training loss: 2.4756803174809936
Validation loss: 3.0674107021715273

Epoch: 6| Step: 12
Training loss: 2.680446239300196
Validation loss: 3.069735068219641

Epoch: 6| Step: 13
Training loss: 4.302530679543504
Validation loss: 3.056374751014276

Epoch: 94| Step: 0
Training loss: 3.463926793917297
Validation loss: 3.0452791799838614

Epoch: 6| Step: 1
Training loss: 3.061943626467914
Validation loss: 3.0281706886185034

Epoch: 6| Step: 2
Training loss: 3.5892714748899928
Validation loss: 3.0255526040375456

Epoch: 6| Step: 3
Training loss: 3.406425261582021
Validation loss: 3.027608263922076

Epoch: 6| Step: 4
Training loss: 2.8282696597369346
Validation loss: 3.0295199121328236

Epoch: 6| Step: 5
Training loss: 3.344107778071733
Validation loss: 3.0320984566677494

Epoch: 6| Step: 6
Training loss: 3.3786414952272583
Validation loss: 3.0341775937862647

Epoch: 6| Step: 7
Training loss: 3.337818053446017
Validation loss: 3.0371299998030157

Epoch: 6| Step: 8
Training loss: 3.710149870156325
Validation loss: 3.027256913277757

Epoch: 6| Step: 9
Training loss: 2.323597236688658
Validation loss: 3.027572772849904

Epoch: 6| Step: 10
Training loss: 3.2856610394540886
Validation loss: 3.0273438312955245

Epoch: 6| Step: 11
Training loss: 3.24230883267885
Validation loss: 3.0282978566906364

Epoch: 6| Step: 12
Training loss: 3.1430704645116143
Validation loss: 3.035140270894737

Epoch: 6| Step: 13
Training loss: 4.231311548593964
Validation loss: 3.0437849717401972

Epoch: 95| Step: 0
Training loss: 4.052082496518765
Validation loss: 3.0452187234197123

Epoch: 6| Step: 1
Training loss: 3.141828619495307
Validation loss: 3.0422481790680274

Epoch: 6| Step: 2
Training loss: 2.936277906367245
Validation loss: 3.0428106797769225

Epoch: 6| Step: 3
Training loss: 3.525499100218244
Validation loss: 3.039169877766144

Epoch: 6| Step: 4
Training loss: 3.9221410167189674
Validation loss: 3.038538428425255

Epoch: 6| Step: 5
Training loss: 3.2290771143811536
Validation loss: 3.0277605945944646

Epoch: 6| Step: 6
Training loss: 2.499501369340876
Validation loss: 3.0260109916376066

Epoch: 6| Step: 7
Training loss: 3.2969047942781393
Validation loss: 3.0249160390870258

Epoch: 6| Step: 8
Training loss: 2.925983335394087
Validation loss: 3.0258197070096706

Epoch: 6| Step: 9
Training loss: 4.025098024679196
Validation loss: 3.022877804940783

Epoch: 6| Step: 10
Training loss: 3.1387075767551793
Validation loss: 3.0233939944801764

Epoch: 6| Step: 11
Training loss: 3.498582416649124
Validation loss: 3.022805378933673

Epoch: 6| Step: 12
Training loss: 2.7915024970069253
Validation loss: 3.022921160997095

Epoch: 6| Step: 13
Training loss: 2.1998902510231346
Validation loss: 3.021246288165469

Epoch: 96| Step: 0
Training loss: 3.096508453263975
Validation loss: 3.02207121190791

Epoch: 6| Step: 1
Training loss: 2.6496381260648936
Validation loss: 3.0209712612642585

Epoch: 6| Step: 2
Training loss: 2.456230676007151
Validation loss: 3.021150155827815

Epoch: 6| Step: 3
Training loss: 3.824318624406053
Validation loss: 3.0208804598234487

Epoch: 6| Step: 4
Training loss: 3.3828623990437827
Validation loss: 3.022258098083934

Epoch: 6| Step: 5
Training loss: 3.7359620875654724
Validation loss: 3.022484634319508

Epoch: 6| Step: 6
Training loss: 4.00557320482584
Validation loss: 3.024210311405662

Epoch: 6| Step: 7
Training loss: 2.789855587821208
Validation loss: 3.021703089227051

Epoch: 6| Step: 8
Training loss: 3.6430073421298523
Validation loss: 3.020987065823182

Epoch: 6| Step: 9
Training loss: 3.016273231211368
Validation loss: 3.020069629736023

Epoch: 6| Step: 10
Training loss: 3.4234467820111787
Validation loss: 3.0219574969165284

Epoch: 6| Step: 11
Training loss: 3.2960970715607525
Validation loss: 3.0181926168132613

Epoch: 6| Step: 12
Training loss: 2.9924765663854767
Validation loss: 3.0178510502712563

Epoch: 6| Step: 13
Training loss: 3.425180449918216
Validation loss: 3.017656111022273

Epoch: 97| Step: 0
Training loss: 3.2457482329652243
Validation loss: 3.019139792717353

Epoch: 6| Step: 1
Training loss: 2.588408335597765
Validation loss: 3.0195991707887724

Epoch: 6| Step: 2
Training loss: 3.770949402317197
Validation loss: 3.0181243212072575

Epoch: 6| Step: 3
Training loss: 2.896736242104766
Validation loss: 3.020758799707326

Epoch: 6| Step: 4
Training loss: 4.212515738709703
Validation loss: 3.021005535769145

Epoch: 6| Step: 5
Training loss: 2.8179884869439813
Validation loss: 3.0208324671409192

Epoch: 6| Step: 6
Training loss: 3.5043632013739034
Validation loss: 3.0232560518625404

Epoch: 6| Step: 7
Training loss: 3.4501311843920024
Validation loss: 3.0222562921519565

Epoch: 6| Step: 8
Training loss: 3.8435877323789738
Validation loss: 3.025535282873864

Epoch: 6| Step: 9
Training loss: 2.8560139230874957
Validation loss: 3.0186277564813544

Epoch: 6| Step: 10
Training loss: 3.9574726306694226
Validation loss: 3.0197367413633853

Epoch: 6| Step: 11
Training loss: 2.614494190494855
Validation loss: 3.01687410194713

Epoch: 6| Step: 12
Training loss: 2.8985383460913896
Validation loss: 3.0178077623917

Epoch: 6| Step: 13
Training loss: 2.4567563338286873
Validation loss: 3.0156204331563368

Epoch: 98| Step: 0
Training loss: 3.2647087600525513
Validation loss: 3.013992026411574

Epoch: 6| Step: 1
Training loss: 3.1027450036898556
Validation loss: 3.014142736872452

Epoch: 6| Step: 2
Training loss: 2.942391401570189
Validation loss: 3.0154059435363694

Epoch: 6| Step: 3
Training loss: 3.1856470144917095
Validation loss: 3.0154169057493814

Epoch: 6| Step: 4
Training loss: 3.2102149465973353
Validation loss: 3.0145687935957004

Epoch: 6| Step: 5
Training loss: 3.5120386438350897
Validation loss: 3.0140744838885585

Epoch: 6| Step: 6
Training loss: 3.333679292527589
Validation loss: 3.0163220477676695

Epoch: 6| Step: 7
Training loss: 3.6936911641193766
Validation loss: 3.0162316316337074

Epoch: 6| Step: 8
Training loss: 3.0669010224840756
Validation loss: 3.014265235560377

Epoch: 6| Step: 9
Training loss: 3.918863424457502
Validation loss: 3.013439826223826

Epoch: 6| Step: 10
Training loss: 3.281490062832844
Validation loss: 3.014506408057803

Epoch: 6| Step: 11
Training loss: 2.975453568191311
Validation loss: 3.0192032610831725

Epoch: 6| Step: 12
Training loss: 3.008256200786075
Validation loss: 3.0143071778078183

Epoch: 6| Step: 13
Training loss: 3.4045425475217286
Validation loss: 3.0160363344138714

Epoch: 99| Step: 0
Training loss: 3.0706043056207646
Validation loss: 3.0132584779597242

Epoch: 6| Step: 1
Training loss: 3.122525265227236
Validation loss: 3.013839186961482

Epoch: 6| Step: 2
Training loss: 3.5185054931250805
Validation loss: 3.0137379109477656

Epoch: 6| Step: 3
Training loss: 2.4376684277371328
Validation loss: 3.0129833694343793

Epoch: 6| Step: 4
Training loss: 4.054186957302984
Validation loss: 3.0138838970708233

Epoch: 6| Step: 5
Training loss: 3.3819602967677747
Validation loss: 3.0128863488967683

Epoch: 6| Step: 6
Training loss: 2.9196458769016473
Validation loss: 3.0126586109810476

Epoch: 6| Step: 7
Training loss: 2.8039984321481017
Validation loss: 3.017998352004991

Epoch: 6| Step: 8
Training loss: 3.397807727207974
Validation loss: 3.0142295176385616

Epoch: 6| Step: 9
Training loss: 3.275207186835547
Validation loss: 3.033969638061217

Epoch: 6| Step: 10
Training loss: 3.4685169863406595
Validation loss: 3.034680355378287

Epoch: 6| Step: 11
Training loss: 3.5916355712913486
Validation loss: 3.0290389272970297

Epoch: 6| Step: 12
Training loss: 2.848926234834507
Validation loss: 3.014090872366593

Epoch: 6| Step: 13
Training loss: 4.098750896447942
Validation loss: 3.011269431405857

Epoch: 100| Step: 0
Training loss: 3.9214005506250422
Validation loss: 3.01101049956097

Epoch: 6| Step: 1
Training loss: 3.8780006358011874
Validation loss: 3.0115974314265213

Epoch: 6| Step: 2
Training loss: 3.183882811359383
Validation loss: 3.0124439649408763

Epoch: 6| Step: 3
Training loss: 3.3926839597859346
Validation loss: 3.0123426394661714

Epoch: 6| Step: 4
Training loss: 4.21182609623681
Validation loss: 3.0116538334239666

Epoch: 6| Step: 5
Training loss: 3.40466075534829
Validation loss: 3.011459246409943

Epoch: 6| Step: 6
Training loss: 2.6232262022479875
Validation loss: 3.0126566265504486

Epoch: 6| Step: 7
Training loss: 2.1609187789403945
Validation loss: 3.0132757395243237

Epoch: 6| Step: 8
Training loss: 3.0530729978539015
Validation loss: 3.011809991933137

Epoch: 6| Step: 9
Training loss: 3.5008135258847446
Validation loss: 3.0119663261949072

Epoch: 6| Step: 10
Training loss: 2.683613650763183
Validation loss: 3.0131931989313387

Epoch: 6| Step: 11
Training loss: 3.204611531012342
Validation loss: 3.011809640389166

Epoch: 6| Step: 12
Training loss: 2.9424452042318863
Validation loss: 3.0149920487534128

Epoch: 6| Step: 13
Training loss: 3.197432656373081
Validation loss: 3.0181507922660686

Epoch: 101| Step: 0
Training loss: 3.231690255430346
Validation loss: 3.012538878208237

Epoch: 6| Step: 1
Training loss: 3.4024878207513582
Validation loss: 3.0108659041963057

Epoch: 6| Step: 2
Training loss: 3.745288304842387
Validation loss: 3.0121142953050453

Epoch: 6| Step: 3
Training loss: 3.7841634324247653
Validation loss: 3.0087418227377705

Epoch: 6| Step: 4
Training loss: 4.075546209319295
Validation loss: 3.0095031887489827

Epoch: 6| Step: 5
Training loss: 3.3148039956353346
Validation loss: 3.009884520105331

Epoch: 6| Step: 6
Training loss: 2.7458730421730917
Validation loss: 3.0096505412944006

Epoch: 6| Step: 7
Training loss: 2.9241184047203386
Validation loss: 3.007705730476397

Epoch: 6| Step: 8
Training loss: 3.0860353092193447
Validation loss: 3.0060711856745397

Epoch: 6| Step: 9
Training loss: 2.114563727327103
Validation loss: 3.007900787877484

Epoch: 6| Step: 10
Training loss: 3.231302912443043
Validation loss: 3.0076673485718004

Epoch: 6| Step: 11
Training loss: 3.4259857151515334
Validation loss: 3.0078624953978603

Epoch: 6| Step: 12
Training loss: 3.1667716025650634
Validation loss: 3.009038374115801

Epoch: 6| Step: 13
Training loss: 3.226459640082614
Validation loss: 3.008201658539408

Epoch: 102| Step: 0
Training loss: 3.173545810761283
Validation loss: 3.0077285173163575

Epoch: 6| Step: 1
Training loss: 2.960450305966834
Validation loss: 3.006426102853146

Epoch: 6| Step: 2
Training loss: 2.894156777976164
Validation loss: 3.007848629936242

Epoch: 6| Step: 3
Training loss: 4.084493874393857
Validation loss: 3.0101791866112277

Epoch: 6| Step: 4
Training loss: 3.785014776808241
Validation loss: 3.009620204896042

Epoch: 6| Step: 5
Training loss: 3.3972228938057265
Validation loss: 3.0106674931780812

Epoch: 6| Step: 6
Training loss: 3.489721054471962
Validation loss: 3.013346549274751

Epoch: 6| Step: 7
Training loss: 2.463684873605613
Validation loss: 3.0103259107879343

Epoch: 6| Step: 8
Training loss: 3.050487548132435
Validation loss: 3.00848427195791

Epoch: 6| Step: 9
Training loss: 2.6433670884903897
Validation loss: 3.009284603312622

Epoch: 6| Step: 10
Training loss: 3.232542541648808
Validation loss: 3.008557502012548

Epoch: 6| Step: 11
Training loss: 3.797581736405121
Validation loss: 3.004851987733437

Epoch: 6| Step: 12
Training loss: 3.147764072384636
Validation loss: 3.004376800967385

Epoch: 6| Step: 13
Training loss: 3.435330347781516
Validation loss: 3.0041897530600457

Epoch: 103| Step: 0
Training loss: 3.730667764038659
Validation loss: 3.0048894449451065

Epoch: 6| Step: 1
Training loss: 3.2612652309145402
Validation loss: 3.0027275350754645

Epoch: 6| Step: 2
Training loss: 3.6951638447900725
Validation loss: 3.005278452254273

Epoch: 6| Step: 3
Training loss: 2.7703303595245314
Validation loss: 3.0020866708336365

Epoch: 6| Step: 4
Training loss: 3.2572448382271215
Validation loss: 3.002104265627744

Epoch: 6| Step: 5
Training loss: 3.0730907498635593
Validation loss: 3.0034819106066393

Epoch: 6| Step: 6
Training loss: 2.543261626879702
Validation loss: 3.0019669357089827

Epoch: 6| Step: 7
Training loss: 3.3172775939120323
Validation loss: 3.0032488320649176

Epoch: 6| Step: 8
Training loss: 2.6719815417011947
Validation loss: 3.0037169161652604

Epoch: 6| Step: 9
Training loss: 2.846899782443501
Validation loss: 3.0029159751276877

Epoch: 6| Step: 10
Training loss: 3.2852265013668696
Validation loss: 3.000730575969743

Epoch: 6| Step: 11
Training loss: 3.628323971256297
Validation loss: 3.0019465005853196

Epoch: 6| Step: 12
Training loss: 3.7446738247994555
Validation loss: 3.001596080917615

Epoch: 6| Step: 13
Training loss: 3.994011688016658
Validation loss: 3.0020046005678336

Epoch: 104| Step: 0
Training loss: 3.0751148357503264
Validation loss: 3.0011832064242423

Epoch: 6| Step: 1
Training loss: 3.944576020949069
Validation loss: 3.0014781822387415

Epoch: 6| Step: 2
Training loss: 2.4717009086406274
Validation loss: 3.0009302966538964

Epoch: 6| Step: 3
Training loss: 3.697495055901431
Validation loss: 3.0015789776670103

Epoch: 6| Step: 4
Training loss: 3.5913218380640384
Validation loss: 3.0016109301327116

Epoch: 6| Step: 5
Training loss: 2.7453281426024807
Validation loss: 3.000806778608039

Epoch: 6| Step: 6
Training loss: 3.708801554291657
Validation loss: 3.00382135243715

Epoch: 6| Step: 7
Training loss: 4.131319464995486
Validation loss: 2.9996588813834197

Epoch: 6| Step: 8
Training loss: 3.1923802732572497
Validation loss: 2.999961647690943

Epoch: 6| Step: 9
Training loss: 3.2830238996850634
Validation loss: 2.999596144725728

Epoch: 6| Step: 10
Training loss: 2.483491560569695
Validation loss: 3.0000077558574434

Epoch: 6| Step: 11
Training loss: 2.5918833438618485
Validation loss: 3.0002710457528257

Epoch: 6| Step: 12
Training loss: 3.3045502762677628
Validation loss: 2.998783365636874

Epoch: 6| Step: 13
Training loss: 2.8840370782063878
Validation loss: 3.0005517028446094

Epoch: 105| Step: 0
Training loss: 2.9894650177798194
Validation loss: 3.000177913093686

Epoch: 6| Step: 1
Training loss: 3.3541269872620636
Validation loss: 2.998323508770057

Epoch: 6| Step: 2
Training loss: 2.5591248405149796
Validation loss: 3.000941736321907

Epoch: 6| Step: 3
Training loss: 3.6510661619111127
Validation loss: 3.0037077104141727

Epoch: 6| Step: 4
Training loss: 3.482452546485794
Validation loss: 3.0010859222901414

Epoch: 6| Step: 5
Training loss: 2.935770377802331
Validation loss: 3.0000440584044377

Epoch: 6| Step: 6
Training loss: 2.8515173503489017
Validation loss: 2.996118876380506

Epoch: 6| Step: 7
Training loss: 3.3137895485191873
Validation loss: 2.997572760387159

Epoch: 6| Step: 8
Training loss: 3.4008687760639686
Validation loss: 2.9953811597497615

Epoch: 6| Step: 9
Training loss: 3.2840748570839784
Validation loss: 2.9975473006741584

Epoch: 6| Step: 10
Training loss: 3.1814670691038107
Validation loss: 2.9958137749006637

Epoch: 6| Step: 11
Training loss: 3.0302693461944665
Validation loss: 2.994937238681904

Epoch: 6| Step: 12
Training loss: 3.606279507256814
Validation loss: 2.995538025349273

Epoch: 6| Step: 13
Training loss: 4.340581383153
Validation loss: 2.9954192523282335

Epoch: 106| Step: 0
Training loss: 3.16211691484972
Validation loss: 2.995181244509412

Epoch: 6| Step: 1
Training loss: 3.3285722745406656
Validation loss: 2.997705357104409

Epoch: 6| Step: 2
Training loss: 3.3127719119769914
Validation loss: 2.995397813116447

Epoch: 6| Step: 3
Training loss: 3.207839696650875
Validation loss: 2.993011173606712

Epoch: 6| Step: 4
Training loss: 3.213981280972009
Validation loss: 2.993703539440693

Epoch: 6| Step: 5
Training loss: 2.9337587272081893
Validation loss: 2.9947031852131443

Epoch: 6| Step: 6
Training loss: 3.3052995263828127
Validation loss: 2.993780621414345

Epoch: 6| Step: 7
Training loss: 2.245703303359816
Validation loss: 2.9943211439542163

Epoch: 6| Step: 8
Training loss: 3.0312320669371275
Validation loss: 2.9950162807443026

Epoch: 6| Step: 9
Training loss: 3.2175126150931703
Validation loss: 2.9936770680120266

Epoch: 6| Step: 10
Training loss: 3.5475111067465646
Validation loss: 2.9940702737806673

Epoch: 6| Step: 11
Training loss: 3.6213771211264327
Validation loss: 2.9943684802284234

Epoch: 6| Step: 12
Training loss: 4.061318798004944
Validation loss: 2.9941902407780443

Epoch: 6| Step: 13
Training loss: 3.2645831476701215
Validation loss: 2.9992388747617573

Epoch: 107| Step: 0
Training loss: 3.279669390042752
Validation loss: 2.9968131966733225

Epoch: 6| Step: 1
Training loss: 3.737118531219185
Validation loss: 2.9967918983195307

Epoch: 6| Step: 2
Training loss: 2.805861033646663
Validation loss: 2.99275239414229

Epoch: 6| Step: 3
Training loss: 2.732279295947252
Validation loss: 2.9929338765195217

Epoch: 6| Step: 4
Training loss: 2.5818991115074916
Validation loss: 2.991769962056844

Epoch: 6| Step: 5
Training loss: 3.7539291142969144
Validation loss: 2.991706983701545

Epoch: 6| Step: 6
Training loss: 3.9834234559998225
Validation loss: 2.9923292094508978

Epoch: 6| Step: 7
Training loss: 3.117976726922269
Validation loss: 2.9936208933783766

Epoch: 6| Step: 8
Training loss: 3.688090390287189
Validation loss: 2.997213301622752

Epoch: 6| Step: 9
Training loss: 3.2035380771660926
Validation loss: 2.9944915720999283

Epoch: 6| Step: 10
Training loss: 3.6124112128200205
Validation loss: 2.9947537675611153

Epoch: 6| Step: 11
Training loss: 2.8397587864482365
Validation loss: 2.994185670352301

Epoch: 6| Step: 12
Training loss: 3.2472077625983533
Validation loss: 2.991813184518746

Epoch: 6| Step: 13
Training loss: 2.5405055244384234
Validation loss: 2.990676135339438

Epoch: 108| Step: 0
Training loss: 3.2187633328948677
Validation loss: 2.9908290921680623

Epoch: 6| Step: 1
Training loss: 2.912936977627962
Validation loss: 2.9912378935601502

Epoch: 6| Step: 2
Training loss: 3.9036147116586233
Validation loss: 2.9910934719952023

Epoch: 6| Step: 3
Training loss: 4.271325853682887
Validation loss: 2.9902564796559927

Epoch: 6| Step: 4
Training loss: 2.8725696740527633
Validation loss: 2.989010288616248

Epoch: 6| Step: 5
Training loss: 3.146747144812209
Validation loss: 2.9883198328574947

Epoch: 6| Step: 6
Training loss: 3.2617093936991557
Validation loss: 2.989864425476009

Epoch: 6| Step: 7
Training loss: 2.890207621366412
Validation loss: 2.988374937860811

Epoch: 6| Step: 8
Training loss: 2.4978822321317242
Validation loss: 2.990651238398019

Epoch: 6| Step: 9
Training loss: 3.883153643291394
Validation loss: 2.987495844076057

Epoch: 6| Step: 10
Training loss: 3.0156144057962386
Validation loss: 2.9902071509750225

Epoch: 6| Step: 11
Training loss: 2.6522587129110167
Validation loss: 2.9896359156950525

Epoch: 6| Step: 12
Training loss: 3.661413866246066
Validation loss: 2.998164804291745

Epoch: 6| Step: 13
Training loss: 2.7999711614213565
Validation loss: 2.9999216763379652

Epoch: 109| Step: 0
Training loss: 3.205098358406579
Validation loss: 2.9947678023676447

Epoch: 6| Step: 1
Training loss: 3.111235708814622
Validation loss: 2.9932315315552094

Epoch: 6| Step: 2
Training loss: 3.460747362687662
Validation loss: 2.9961606756348957

Epoch: 6| Step: 3
Training loss: 3.3842279939434095
Validation loss: 2.9927768650015456

Epoch: 6| Step: 4
Training loss: 3.2295508043421686
Validation loss: 2.9893576049538706

Epoch: 6| Step: 5
Training loss: 2.954577376453307
Validation loss: 2.9893952082862367

Epoch: 6| Step: 6
Training loss: 3.3821864440797893
Validation loss: 2.9881451293647

Epoch: 6| Step: 7
Training loss: 2.9820050321110534
Validation loss: 2.987331193083353

Epoch: 6| Step: 8
Training loss: 3.9727957221860435
Validation loss: 2.986162408602913

Epoch: 6| Step: 9
Training loss: 3.3715576701351075
Validation loss: 2.9866703369911702

Epoch: 6| Step: 10
Training loss: 3.023942141347739
Validation loss: 2.986685509337516

Epoch: 6| Step: 11
Training loss: 2.6741514268224713
Validation loss: 2.985150426806928

Epoch: 6| Step: 12
Training loss: 2.950120962216876
Validation loss: 2.9858561987380297

Epoch: 6| Step: 13
Training loss: 4.12795868162428
Validation loss: 2.9864404333788643

Epoch: 110| Step: 0
Training loss: 3.3295929586091346
Validation loss: 2.9846509442854763

Epoch: 6| Step: 1
Training loss: 2.6614486982814425
Validation loss: 2.9845817969737913

Epoch: 6| Step: 2
Training loss: 3.0927319633803467
Validation loss: 2.987064738021985

Epoch: 6| Step: 3
Training loss: 3.712700547475464
Validation loss: 2.9850031015607392

Epoch: 6| Step: 4
Training loss: 3.304491835408767
Validation loss: 2.9860405994614103

Epoch: 6| Step: 5
Training loss: 3.2952194034309836
Validation loss: 2.9899310163902335

Epoch: 6| Step: 6
Training loss: 4.061921533334777
Validation loss: 2.9929276321584704

Epoch: 6| Step: 7
Training loss: 3.31504752606922
Validation loss: 2.9920578769514674

Epoch: 6| Step: 8
Training loss: 3.72254378478135
Validation loss: 2.99378825808547

Epoch: 6| Step: 9
Training loss: 3.256766098623384
Validation loss: 2.9830631499027342

Epoch: 6| Step: 10
Training loss: 2.7809828726396426
Validation loss: 2.983192258015244

Epoch: 6| Step: 11
Training loss: 2.9567709723835365
Validation loss: 2.9821921716895448

Epoch: 6| Step: 12
Training loss: 2.654337564792284
Validation loss: 2.982346590867252

Epoch: 6| Step: 13
Training loss: 3.109878575938308
Validation loss: 2.984005562047875

Epoch: 111| Step: 0
Training loss: 3.2160814204181443
Validation loss: 2.984940893399769

Epoch: 6| Step: 1
Training loss: 3.2967465633305864
Validation loss: 2.9851008668917705

Epoch: 6| Step: 2
Training loss: 3.3903562377730085
Validation loss: 2.9848462732219807

Epoch: 6| Step: 3
Training loss: 3.4822334585175563
Validation loss: 2.9847009737742733

Epoch: 6| Step: 4
Training loss: 3.0688886374177984
Validation loss: 2.9843812708463173

Epoch: 6| Step: 5
Training loss: 2.8962690359518195
Validation loss: 2.986315165672898

Epoch: 6| Step: 6
Training loss: 3.6712022530318342
Validation loss: 2.9843871877642942

Epoch: 6| Step: 7
Training loss: 3.2842630267451502
Validation loss: 2.9839274414392123

Epoch: 6| Step: 8
Training loss: 2.375445876179194
Validation loss: 2.9836772783223817

Epoch: 6| Step: 9
Training loss: 3.1104838632086245
Validation loss: 2.986072303524252

Epoch: 6| Step: 10
Training loss: 3.843836837656027
Validation loss: 2.9830205320160625

Epoch: 6| Step: 11
Training loss: 3.4182940693396207
Validation loss: 2.9826625228838677

Epoch: 6| Step: 12
Training loss: 2.9355969047217156
Validation loss: 2.981508362298302

Epoch: 6| Step: 13
Training loss: 3.526018977503087
Validation loss: 2.981312854224909

Epoch: 112| Step: 0
Training loss: 3.462558203252775
Validation loss: 2.981740812838419

Epoch: 6| Step: 1
Training loss: 3.219263572830746
Validation loss: 2.980365700912513

Epoch: 6| Step: 2
Training loss: 3.130986849449902
Validation loss: 2.98385384146474

Epoch: 6| Step: 3
Training loss: 2.3969995617983484
Validation loss: 2.985381338300349

Epoch: 6| Step: 4
Training loss: 2.9398109598432103
Validation loss: 2.982199136563362

Epoch: 6| Step: 5
Training loss: 3.465730057027817
Validation loss: 2.9826565372213385

Epoch: 6| Step: 6
Training loss: 3.118359032475241
Validation loss: 2.9802730498580217

Epoch: 6| Step: 7
Training loss: 3.627191867479009
Validation loss: 2.9816606458885637

Epoch: 6| Step: 8
Training loss: 3.405602209806364
Validation loss: 2.9799686393400773

Epoch: 6| Step: 9
Training loss: 3.0264125647009505
Validation loss: 2.980032629689335

Epoch: 6| Step: 10
Training loss: 3.3894192709643884
Validation loss: 2.9818716684585476

Epoch: 6| Step: 11
Training loss: 3.3453653302001087
Validation loss: 2.981443560094504

Epoch: 6| Step: 12
Training loss: 3.5252509007543056
Validation loss: 2.977332367040338

Epoch: 6| Step: 13
Training loss: 3.440554562158611
Validation loss: 2.9792637202893437

Epoch: 113| Step: 0
Training loss: 3.008163471304838
Validation loss: 2.9777784528735616

Epoch: 6| Step: 1
Training loss: 4.199045463357682
Validation loss: 2.978665670659881

Epoch: 6| Step: 2
Training loss: 3.3101173324764095
Validation loss: 2.9784132520168067

Epoch: 6| Step: 3
Training loss: 2.776855769257171
Validation loss: 2.9760276092699565

Epoch: 6| Step: 4
Training loss: 3.65532405246482
Validation loss: 2.975791572288826

Epoch: 6| Step: 5
Training loss: 2.457166707439569
Validation loss: 2.9765341122210507

Epoch: 6| Step: 6
Training loss: 3.0982202158431824
Validation loss: 2.977013267849317

Epoch: 6| Step: 7
Training loss: 3.940192618914145
Validation loss: 2.975629379534905

Epoch: 6| Step: 8
Training loss: 3.085371907227426
Validation loss: 2.975447190646548

Epoch: 6| Step: 9
Training loss: 2.5685584857765527
Validation loss: 2.9760610678998

Epoch: 6| Step: 10
Training loss: 3.2087027762266405
Validation loss: 2.9751623756531855

Epoch: 6| Step: 11
Training loss: 3.00568772607286
Validation loss: 2.9770424811951717

Epoch: 6| Step: 12
Training loss: 3.562608817596908
Validation loss: 2.9767586903921557

Epoch: 6| Step: 13
Training loss: 3.1523107742040737
Validation loss: 2.9777467147448937

Epoch: 114| Step: 0
Training loss: 2.775615084415034
Validation loss: 2.975488133522369

Epoch: 6| Step: 1
Training loss: 2.626048287475746
Validation loss: 2.9765177874029827

Epoch: 6| Step: 2
Training loss: 3.423234782622005
Validation loss: 2.97721604341112

Epoch: 6| Step: 3
Training loss: 3.9522986471871806
Validation loss: 2.97751685826574

Epoch: 6| Step: 4
Training loss: 2.7479401590086847
Validation loss: 2.9820637011503797

Epoch: 6| Step: 5
Training loss: 3.3432602568297702
Validation loss: 2.9817423260505516

Epoch: 6| Step: 6
Training loss: 3.5100399158523063
Validation loss: 2.9777926029851427

Epoch: 6| Step: 7
Training loss: 4.1108363348976145
Validation loss: 2.9745458419806603

Epoch: 6| Step: 8
Training loss: 2.6363038591406203
Validation loss: 2.9811925375729023

Epoch: 6| Step: 9
Training loss: 3.63311215057017
Validation loss: 2.9775608162726033

Epoch: 6| Step: 10
Training loss: 2.7680951310607176
Validation loss: 2.981162866125799

Epoch: 6| Step: 11
Training loss: 2.9004124907109285
Validation loss: 2.988769268347196

Epoch: 6| Step: 12
Training loss: 3.4924657655320095
Validation loss: 2.9765480236446122

Epoch: 6| Step: 13
Training loss: 3.008679232872517
Validation loss: 2.972821518877546

Epoch: 115| Step: 0
Training loss: 2.6964408834531794
Validation loss: 2.971372362177312

Epoch: 6| Step: 1
Training loss: 3.3391592294679824
Validation loss: 2.9710051696571993

Epoch: 6| Step: 2
Training loss: 2.5497966460879518
Validation loss: 2.9689339287715253

Epoch: 6| Step: 3
Training loss: 2.99185951766006
Validation loss: 2.971382713793676

Epoch: 6| Step: 4
Training loss: 3.247836640069228
Validation loss: 2.97064961838171

Epoch: 6| Step: 5
Training loss: 2.2128197304260446
Validation loss: 2.9719351537635696

Epoch: 6| Step: 6
Training loss: 3.0193984383692167
Validation loss: 2.96900609827251

Epoch: 6| Step: 7
Training loss: 3.3556424008289696
Validation loss: 2.9691255402095837

Epoch: 6| Step: 8
Training loss: 3.6635557039736906
Validation loss: 2.968906473160687

Epoch: 6| Step: 9
Training loss: 3.414723068335535
Validation loss: 2.9682353965629225

Epoch: 6| Step: 10
Training loss: 4.324461340022322
Validation loss: 2.9687865345147357

Epoch: 6| Step: 11
Training loss: 3.30315260717716
Validation loss: 2.969429452168373

Epoch: 6| Step: 12
Training loss: 3.4513998425875094
Validation loss: 2.968736697104009

Epoch: 6| Step: 13
Training loss: 3.5371544012008536
Validation loss: 2.9680654801985837

Epoch: 116| Step: 0
Training loss: 2.9654635257796533
Validation loss: 2.967466310640101

Epoch: 6| Step: 1
Training loss: 3.5854183311539813
Validation loss: 2.969348736992497

Epoch: 6| Step: 2
Training loss: 3.320335047028225
Validation loss: 2.970017621617256

Epoch: 6| Step: 3
Training loss: 2.8040194340031044
Validation loss: 2.9757653774434027

Epoch: 6| Step: 4
Training loss: 3.6656126472544686
Validation loss: 2.973726257529584

Epoch: 6| Step: 5
Training loss: 3.076092043855006
Validation loss: 2.9743818057692417

Epoch: 6| Step: 6
Training loss: 3.337060958044896
Validation loss: 2.972290011327296

Epoch: 6| Step: 7
Training loss: 3.6703298361260437
Validation loss: 2.97349095693444

Epoch: 6| Step: 8
Training loss: 2.597468089696146
Validation loss: 2.9675156674594914

Epoch: 6| Step: 9
Training loss: 2.709018874825378
Validation loss: 2.9677405168985596

Epoch: 6| Step: 10
Training loss: 3.4933498419548723
Validation loss: 2.9679886347345756

Epoch: 6| Step: 11
Training loss: 3.4689047839023015
Validation loss: 2.969194844568243

Epoch: 6| Step: 12
Training loss: 2.888428384398567
Validation loss: 2.9684821344241503

Epoch: 6| Step: 13
Training loss: 3.8475564856593536
Validation loss: 2.9701129852257147

Epoch: 117| Step: 0
Training loss: 4.175218249801042
Validation loss: 2.9695956983148726

Epoch: 6| Step: 1
Training loss: 3.3527538254107103
Validation loss: 2.966847005088975

Epoch: 6| Step: 2
Training loss: 2.903099403227442
Validation loss: 2.967437606039336

Epoch: 6| Step: 3
Training loss: 2.950020586362227
Validation loss: 2.967226802817894

Epoch: 6| Step: 4
Training loss: 2.907654617672358
Validation loss: 2.9682451769929923

Epoch: 6| Step: 5
Training loss: 3.1077037517993498
Validation loss: 2.967767862360067

Epoch: 6| Step: 6
Training loss: 2.866506053801724
Validation loss: 2.9656011024377644

Epoch: 6| Step: 7
Training loss: 3.8238976629202326
Validation loss: 2.967676802274234

Epoch: 6| Step: 8
Training loss: 3.0384368644759987
Validation loss: 2.966123554469784

Epoch: 6| Step: 9
Training loss: 2.665029758480461
Validation loss: 2.9676332144531496

Epoch: 6| Step: 10
Training loss: 3.1675939373487743
Validation loss: 2.9648433090132458

Epoch: 6| Step: 11
Training loss: 3.7502489643104395
Validation loss: 2.966666635983633

Epoch: 6| Step: 12
Training loss: 3.075766962438241
Validation loss: 2.9649138548190015

Epoch: 6| Step: 13
Training loss: 3.322871256710358
Validation loss: 2.965555814812606

Epoch: 118| Step: 0
Training loss: 3.3950520808364333
Validation loss: 2.9637461461863603

Epoch: 6| Step: 1
Training loss: 3.772638777397932
Validation loss: 2.96552402626955

Epoch: 6| Step: 2
Training loss: 2.7408709286316664
Validation loss: 2.9663838082892235

Epoch: 6| Step: 3
Training loss: 3.5595321339259143
Validation loss: 2.9671946070912876

Epoch: 6| Step: 4
Training loss: 3.510358875979974
Validation loss: 2.964892019494763

Epoch: 6| Step: 5
Training loss: 3.4353052241885877
Validation loss: 2.964062002297441

Epoch: 6| Step: 6
Training loss: 2.674893286171848
Validation loss: 2.963787746342958

Epoch: 6| Step: 7
Training loss: 2.9936357384779435
Validation loss: 2.9627952288042927

Epoch: 6| Step: 8
Training loss: 3.451851864691816
Validation loss: 2.9625690132154974

Epoch: 6| Step: 9
Training loss: 2.5471870839634945
Validation loss: 2.963183919795797

Epoch: 6| Step: 10
Training loss: 3.590862274962183
Validation loss: 2.9632028253035125

Epoch: 6| Step: 11
Training loss: 3.646461360063953
Validation loss: 2.963267772053638

Epoch: 6| Step: 12
Training loss: 2.859500069306225
Validation loss: 2.964186632905335

Epoch: 6| Step: 13
Training loss: 2.6052235912642385
Validation loss: 2.963635322899885

Epoch: 119| Step: 0
Training loss: 3.4326778921997154
Validation loss: 2.9627809542837205

Epoch: 6| Step: 1
Training loss: 3.1424597699511954
Validation loss: 2.9676305269614507

Epoch: 6| Step: 2
Training loss: 3.8958435602444865
Validation loss: 2.96463841601357

Epoch: 6| Step: 3
Training loss: 3.0333854782596945
Validation loss: 2.9697184754867116

Epoch: 6| Step: 4
Training loss: 3.3769880020520384
Validation loss: 2.9619780854406854

Epoch: 6| Step: 5
Training loss: 3.700300467379529
Validation loss: 2.9620949313132385

Epoch: 6| Step: 6
Training loss: 2.3472648205424855
Validation loss: 2.9631001015029597

Epoch: 6| Step: 7
Training loss: 2.743576263279893
Validation loss: 2.9604464073985257

Epoch: 6| Step: 8
Training loss: 2.8191366941200773
Validation loss: 2.9606050633778813

Epoch: 6| Step: 9
Training loss: 3.269686860422901
Validation loss: 2.960948792010525

Epoch: 6| Step: 10
Training loss: 3.0533138220662
Validation loss: 2.9616429921943963

Epoch: 6| Step: 11
Training loss: 3.190225072113594
Validation loss: 2.958577731925939

Epoch: 6| Step: 12
Training loss: 3.918719112390109
Validation loss: 2.958745007177435

Epoch: 6| Step: 13
Training loss: 2.8942515126086232
Validation loss: 2.9594811537087136

Epoch: 120| Step: 0
Training loss: 2.7416814353413685
Validation loss: 2.9616233928897238

Epoch: 6| Step: 1
Training loss: 2.336446751228387
Validation loss: 2.9603773615349342

Epoch: 6| Step: 2
Training loss: 3.153864799644791
Validation loss: 2.9650211912617106

Epoch: 6| Step: 3
Training loss: 3.1698403433176288
Validation loss: 2.9654548081576215

Epoch: 6| Step: 4
Training loss: 2.8761116863948777
Validation loss: 2.9691011524475472

Epoch: 6| Step: 5
Training loss: 3.5135508204202455
Validation loss: 2.9675818830273935

Epoch: 6| Step: 6
Training loss: 3.6361973897905395
Validation loss: 2.960759906214071

Epoch: 6| Step: 7
Training loss: 3.8461462475628254
Validation loss: 2.9579874298938633

Epoch: 6| Step: 8
Training loss: 3.1953772652551637
Validation loss: 2.958774690289746

Epoch: 6| Step: 9
Training loss: 2.705767359875535
Validation loss: 2.959669989453915

Epoch: 6| Step: 10
Training loss: 3.4829928132825554
Validation loss: 2.9589292303628696

Epoch: 6| Step: 11
Training loss: 3.6607277341588977
Validation loss: 2.9586099607666485

Epoch: 6| Step: 12
Training loss: 3.443693926955302
Validation loss: 2.9568983561193813

Epoch: 6| Step: 13
Training loss: 3.2075081957313207
Validation loss: 2.9580534739507462

Epoch: 121| Step: 0
Training loss: 3.1246217117228174
Validation loss: 2.9592481085917917

Epoch: 6| Step: 1
Training loss: 2.9521372924553932
Validation loss: 2.9584465721557813

Epoch: 6| Step: 2
Training loss: 3.9101224701572224
Validation loss: 2.9585056078565617

Epoch: 6| Step: 3
Training loss: 3.133045673016751
Validation loss: 2.9591096214543664

Epoch: 6| Step: 4
Training loss: 4.246510138708525
Validation loss: 2.95807887751843

Epoch: 6| Step: 5
Training loss: 2.4291571624029022
Validation loss: 2.9599221183638114

Epoch: 6| Step: 6
Training loss: 3.6141360369835613
Validation loss: 2.959032416051

Epoch: 6| Step: 7
Training loss: 3.4151658429247878
Validation loss: 2.9535226983281015

Epoch: 6| Step: 8
Training loss: 2.7840916550445973
Validation loss: 2.956816515944215

Epoch: 6| Step: 9
Training loss: 3.1913942969482396
Validation loss: 2.9573911639462374

Epoch: 6| Step: 10
Training loss: 2.814957371688788
Validation loss: 2.955317481005742

Epoch: 6| Step: 11
Training loss: 3.016309275102009
Validation loss: 2.9558909043936747

Epoch: 6| Step: 12
Training loss: 2.567582556150377
Validation loss: 2.9551518196436075

Epoch: 6| Step: 13
Training loss: 3.8798791255016463
Validation loss: 2.9553792006442343

Epoch: 122| Step: 0
Training loss: 3.7659073719822937
Validation loss: 2.9548226866740173

Epoch: 6| Step: 1
Training loss: 3.5256151460707
Validation loss: 2.9544965210458027

Epoch: 6| Step: 2
Training loss: 3.1501189254835986
Validation loss: 2.955350697868622

Epoch: 6| Step: 3
Training loss: 3.6046097911169
Validation loss: 2.9545156660958347

Epoch: 6| Step: 4
Training loss: 3.2419730609619397
Validation loss: 2.959544413292985

Epoch: 6| Step: 5
Training loss: 2.6922282830778568
Validation loss: 2.954307154165372

Epoch: 6| Step: 6
Training loss: 3.296610889432243
Validation loss: 2.953951284514606

Epoch: 6| Step: 7
Training loss: 3.2613918483264284
Validation loss: 2.953376843921679

Epoch: 6| Step: 8
Training loss: 3.045879807924352
Validation loss: 2.9522778694267813

Epoch: 6| Step: 9
Training loss: 2.989031768497273
Validation loss: 2.9524610191499727

Epoch: 6| Step: 10
Training loss: 3.633776071387353
Validation loss: 2.950796191456553

Epoch: 6| Step: 11
Training loss: 2.7714234682128844
Validation loss: 2.9509136590580125

Epoch: 6| Step: 12
Training loss: 3.1026120656051526
Validation loss: 2.950026019499395

Epoch: 6| Step: 13
Training loss: 2.7825441242842612
Validation loss: 2.9528117376426652

Epoch: 123| Step: 0
Training loss: 2.942768647692641
Validation loss: 2.954517226224521

Epoch: 6| Step: 1
Training loss: 2.9977728205869933
Validation loss: 2.9539495435703134

Epoch: 6| Step: 2
Training loss: 2.87702969761896
Validation loss: 2.955287263502878

Epoch: 6| Step: 3
Training loss: 3.082964041649609
Validation loss: 2.9535264411168134

Epoch: 6| Step: 4
Training loss: 3.434567119302965
Validation loss: 2.9625089707296515

Epoch: 6| Step: 5
Training loss: 3.4788570761230346
Validation loss: 2.97006776985043

Epoch: 6| Step: 6
Training loss: 3.7510755903762854
Validation loss: 2.966534365595623

Epoch: 6| Step: 7
Training loss: 3.4547725561774674
Validation loss: 2.9539552749702858

Epoch: 6| Step: 8
Training loss: 3.344106922530134
Validation loss: 2.9584281622526665

Epoch: 6| Step: 9
Training loss: 3.5999192069842296
Validation loss: 2.955249626084034

Epoch: 6| Step: 10
Training loss: 3.7395036665878956
Validation loss: 2.9494386700314372

Epoch: 6| Step: 11
Training loss: 2.3848873922766947
Validation loss: 2.950043519833848

Epoch: 6| Step: 12
Training loss: 2.677389867033106
Validation loss: 2.9495258977869723

Epoch: 6| Step: 13
Training loss: 3.097218428430291
Validation loss: 2.9528501467029655

Epoch: 124| Step: 0
Training loss: 3.6643329187091807
Validation loss: 2.9520914109707355

Epoch: 6| Step: 1
Training loss: 3.3369002013997306
Validation loss: 2.9509159508503697

Epoch: 6| Step: 2
Training loss: 2.7526987012054045
Validation loss: 2.94872803224184

Epoch: 6| Step: 3
Training loss: 2.2159903113501103
Validation loss: 2.947564012690877

Epoch: 6| Step: 4
Training loss: 3.459713273709152
Validation loss: 2.948614205726187

Epoch: 6| Step: 5
Training loss: 3.5859827109418454
Validation loss: 2.9493427561573244

Epoch: 6| Step: 6
Training loss: 3.1508962718532967
Validation loss: 2.9475896284249505

Epoch: 6| Step: 7
Training loss: 2.8585550769025856
Validation loss: 2.948198477563398

Epoch: 6| Step: 8
Training loss: 2.908875950544178
Validation loss: 2.9471867610447573

Epoch: 6| Step: 9
Training loss: 3.052866829740032
Validation loss: 2.947957687820122

Epoch: 6| Step: 10
Training loss: 3.8514727413713907
Validation loss: 2.946601179053457

Epoch: 6| Step: 11
Training loss: 2.809900905450174
Validation loss: 2.947688533753944

Epoch: 6| Step: 12
Training loss: 3.4711518091553053
Validation loss: 2.9468971981756287

Epoch: 6| Step: 13
Training loss: 4.060711745841177
Validation loss: 2.949276477053443

Epoch: 125| Step: 0
Training loss: 2.6852013272010984
Validation loss: 2.9480442384911423

Epoch: 6| Step: 1
Training loss: 2.403442925305828
Validation loss: 2.945510780551997

Epoch: 6| Step: 2
Training loss: 2.403117729741522
Validation loss: 2.948794492333827

Epoch: 6| Step: 3
Training loss: 3.6331983792034372
Validation loss: 2.947345675010315

Epoch: 6| Step: 4
Training loss: 2.8197210225299703
Validation loss: 2.950098345663932

Epoch: 6| Step: 5
Training loss: 3.54982067246229
Validation loss: 2.9575201809324194

Epoch: 6| Step: 6
Training loss: 3.3178595603759256
Validation loss: 2.9509464268282013

Epoch: 6| Step: 7
Training loss: 3.2488480507121613
Validation loss: 2.950330297445385

Epoch: 6| Step: 8
Training loss: 3.8208067404606085
Validation loss: 2.9522377969361346

Epoch: 6| Step: 9
Training loss: 3.0658314548855463
Validation loss: 2.9533723908880605

Epoch: 6| Step: 10
Training loss: 4.013682804881712
Validation loss: 2.9458822099382584

Epoch: 6| Step: 11
Training loss: 3.1084861826878734
Validation loss: 2.947369379068597

Epoch: 6| Step: 12
Training loss: 3.3248562946671827
Validation loss: 2.9454001571986397

Epoch: 6| Step: 13
Training loss: 3.4030986522148523
Validation loss: 2.9458728217152155

Epoch: 126| Step: 0
Training loss: 3.725988955716337
Validation loss: 2.9436105651626536

Epoch: 6| Step: 1
Training loss: 3.3560899863290152
Validation loss: 2.9440381378343106

Epoch: 6| Step: 2
Training loss: 3.006493058559995
Validation loss: 2.943479294133884

Epoch: 6| Step: 3
Training loss: 2.997161794231466
Validation loss: 2.9458433593137525

Epoch: 6| Step: 4
Training loss: 3.0793158846477326
Validation loss: 2.9449934447275603

Epoch: 6| Step: 5
Training loss: 2.9387425168882024
Validation loss: 2.9441952146833117

Epoch: 6| Step: 6
Training loss: 3.6859036885934655
Validation loss: 2.9421731679837198

Epoch: 6| Step: 7
Training loss: 3.615227780163488
Validation loss: 2.943301752259415

Epoch: 6| Step: 8
Training loss: 3.204837248384893
Validation loss: 2.9443481029300735

Epoch: 6| Step: 9
Training loss: 3.047513141807153
Validation loss: 2.9427772669985863

Epoch: 6| Step: 10
Training loss: 3.562638397205527
Validation loss: 2.944780883353768

Epoch: 6| Step: 11
Training loss: 2.9957410463028853
Validation loss: 2.9442388647618634

Epoch: 6| Step: 12
Training loss: 2.6739386904185345
Validation loss: 2.942217475418156

Epoch: 6| Step: 13
Training loss: 3.013565387933548
Validation loss: 2.940734601825543

Epoch: 127| Step: 0
Training loss: 3.763453320352229
Validation loss: 2.9405650370361722

Epoch: 6| Step: 1
Training loss: 3.5404825961371147
Validation loss: 2.9423811143739034

Epoch: 6| Step: 2
Training loss: 3.0646390645768338
Validation loss: 2.9414151121076104

Epoch: 6| Step: 3
Training loss: 3.3219211406499802
Validation loss: 2.9405581827867633

Epoch: 6| Step: 4
Training loss: 3.7378281465216787
Validation loss: 2.938901090162078

Epoch: 6| Step: 5
Training loss: 2.872091895745152
Validation loss: 2.9403111924740837

Epoch: 6| Step: 6
Training loss: 3.451261682210823
Validation loss: 2.940334677725766

Epoch: 6| Step: 7
Training loss: 2.6342874351781917
Validation loss: 2.941144331636434

Epoch: 6| Step: 8
Training loss: 2.9883023130385133
Validation loss: 2.941951273981651

Epoch: 6| Step: 9
Training loss: 2.611620273952985
Validation loss: 2.9438299209638275

Epoch: 6| Step: 10
Training loss: 3.426072703091707
Validation loss: 2.9465099007243336

Epoch: 6| Step: 11
Training loss: 2.8731740045590057
Validation loss: 2.950560832988148

Epoch: 6| Step: 12
Training loss: 3.4829251818357547
Validation loss: 2.961886971126932

Epoch: 6| Step: 13
Training loss: 3.0191729136806056
Validation loss: 2.9440368603842737

Epoch: 128| Step: 0
Training loss: 3.1222036438823255
Validation loss: 2.9404438638962

Epoch: 6| Step: 1
Training loss: 3.2035349513767417
Validation loss: 2.938758043105704

Epoch: 6| Step: 2
Training loss: 3.11382860376232
Validation loss: 2.939452333088746

Epoch: 6| Step: 3
Training loss: 3.0450249165735426
Validation loss: 2.939396740036652

Epoch: 6| Step: 4
Training loss: 3.2733062499325003
Validation loss: 2.9401281273308197

Epoch: 6| Step: 5
Training loss: 3.185881559096909
Validation loss: 2.9405132034491634

Epoch: 6| Step: 6
Training loss: 3.0208033242324546
Validation loss: 2.9400351900912423

Epoch: 6| Step: 7
Training loss: 3.9570560736974327
Validation loss: 2.943916995312351

Epoch: 6| Step: 8
Training loss: 2.9621218926941095
Validation loss: 2.9423459919575903

Epoch: 6| Step: 9
Training loss: 3.2455294746373426
Validation loss: 2.9393528366382755

Epoch: 6| Step: 10
Training loss: 2.610336218038514
Validation loss: 2.9414767556594357

Epoch: 6| Step: 11
Training loss: 3.4858526680343487
Validation loss: 2.9398912825973906

Epoch: 6| Step: 12
Training loss: 3.779023831383452
Validation loss: 2.941992385864611

Epoch: 6| Step: 13
Training loss: 2.6004525414353497
Validation loss: 2.940589858491764

Epoch: 129| Step: 0
Training loss: 2.997344431145012
Validation loss: 2.940223208301593

Epoch: 6| Step: 1
Training loss: 3.3137643668296075
Validation loss: 2.9380923502812704

Epoch: 6| Step: 2
Training loss: 4.183093585151863
Validation loss: 2.9367726528852955

Epoch: 6| Step: 3
Training loss: 2.909711355726719
Validation loss: 2.937630813887074

Epoch: 6| Step: 4
Training loss: 1.9612027175095568
Validation loss: 2.937433610775521

Epoch: 6| Step: 5
Training loss: 2.795234753298872
Validation loss: 2.9360631580541114

Epoch: 6| Step: 6
Training loss: 3.654021537679704
Validation loss: 2.9370431660431584

Epoch: 6| Step: 7
Training loss: 3.7990432639112357
Validation loss: 2.936081448865264

Epoch: 6| Step: 8
Training loss: 3.3192878129326187
Validation loss: 2.9367433759141552

Epoch: 6| Step: 9
Training loss: 3.1668553212504706
Validation loss: 2.9369639787106543

Epoch: 6| Step: 10
Training loss: 3.3169067148978537
Validation loss: 2.9371283232123484

Epoch: 6| Step: 11
Training loss: 3.039815853464422
Validation loss: 2.939945458013746

Epoch: 6| Step: 12
Training loss: 2.961323010956374
Validation loss: 2.938198625287291

Epoch: 6| Step: 13
Training loss: 3.0820541477682366
Validation loss: 2.9403879321233646

Epoch: 130| Step: 0
Training loss: 3.8878583148053347
Validation loss: 2.9403651413122645

Epoch: 6| Step: 1
Training loss: 3.3762281090748054
Validation loss: 2.9392406592473517

Epoch: 6| Step: 2
Training loss: 3.809822267807289
Validation loss: 2.9374477597393422

Epoch: 6| Step: 3
Training loss: 2.784257270216409
Validation loss: 2.941324144371288

Epoch: 6| Step: 4
Training loss: 3.182901993000397
Validation loss: 2.9359714447000984

Epoch: 6| Step: 5
Training loss: 3.013461270640603
Validation loss: 2.936494495032831

Epoch: 6| Step: 6
Training loss: 3.0333563967935357
Validation loss: 2.9365547891895054

Epoch: 6| Step: 7
Training loss: 2.916282973710107
Validation loss: 2.9346304103232668

Epoch: 6| Step: 8
Training loss: 2.8198280658117696
Validation loss: 2.9372194811202186

Epoch: 6| Step: 9
Training loss: 2.5064670839733747
Validation loss: 2.934291833170356

Epoch: 6| Step: 10
Training loss: 3.477918450459538
Validation loss: 2.9334436676477535

Epoch: 6| Step: 11
Training loss: 2.715378523375244
Validation loss: 2.937415420023219

Epoch: 6| Step: 12
Training loss: 4.026646077269741
Validation loss: 2.9338712531378413

Epoch: 6| Step: 13
Training loss: 2.9353124711491434
Validation loss: 2.935938093282322

Epoch: 131| Step: 0
Training loss: 3.210391552773453
Validation loss: 2.939924844667579

Epoch: 6| Step: 1
Training loss: 3.5309115897545618
Validation loss: 2.9472148756640473

Epoch: 6| Step: 2
Training loss: 3.702199957189003
Validation loss: 2.941011611617813

Epoch: 6| Step: 3
Training loss: 3.965961227412142
Validation loss: 2.940964867920038

Epoch: 6| Step: 4
Training loss: 3.4692063589862263
Validation loss: 2.938001374775281

Epoch: 6| Step: 5
Training loss: 3.0389321110464635
Validation loss: 2.9354336718348915

Epoch: 6| Step: 6
Training loss: 3.196928105514513
Validation loss: 2.9323329055279763

Epoch: 6| Step: 7
Training loss: 2.3132531254226874
Validation loss: 2.9321921487287694

Epoch: 6| Step: 8
Training loss: 2.4457685636384987
Validation loss: 2.9297548207851034

Epoch: 6| Step: 9
Training loss: 2.7176943077660276
Validation loss: 2.931867132745374

Epoch: 6| Step: 10
Training loss: 2.8152172949283285
Validation loss: 2.931434072442679

Epoch: 6| Step: 11
Training loss: 3.692617324690443
Validation loss: 2.9303335123746908

Epoch: 6| Step: 12
Training loss: 3.1052209143440797
Validation loss: 2.9293883818375823

Epoch: 6| Step: 13
Training loss: 3.5173128272647727
Validation loss: 2.92924950698021

Epoch: 132| Step: 0
Training loss: 3.7014524650401968
Validation loss: 2.928705412299562

Epoch: 6| Step: 1
Training loss: 2.854543804859597
Validation loss: 2.9276231535028554

Epoch: 6| Step: 2
Training loss: 2.6962980818053603
Validation loss: 2.92841139917805

Epoch: 6| Step: 3
Training loss: 3.3929508511789006
Validation loss: 2.926309409615106

Epoch: 6| Step: 4
Training loss: 2.648477109539718
Validation loss: 2.927510735067982

Epoch: 6| Step: 5
Training loss: 3.2410923590466543
Validation loss: 2.927917790391622

Epoch: 6| Step: 6
Training loss: 2.9079666810153633
Validation loss: 2.9261137739987095

Epoch: 6| Step: 7
Training loss: 3.7199774247171544
Validation loss: 2.927275785804133

Epoch: 6| Step: 8
Training loss: 2.6062222010458673
Validation loss: 2.9248753891253445

Epoch: 6| Step: 9
Training loss: 3.418101559919743
Validation loss: 2.925539349518972

Epoch: 6| Step: 10
Training loss: 3.7109544613099223
Validation loss: 2.927570488301828

Epoch: 6| Step: 11
Training loss: 3.075763396738651
Validation loss: 2.92452033774235

Epoch: 6| Step: 12
Training loss: 3.1453478777544985
Validation loss: 2.927172755839997

Epoch: 6| Step: 13
Training loss: 3.758284382526017
Validation loss: 2.927698505072996

Epoch: 133| Step: 0
Training loss: 2.683408328206852
Validation loss: 2.9256968456003527

Epoch: 6| Step: 1
Training loss: 3.1610530151439695
Validation loss: 2.92895975766869

Epoch: 6| Step: 2
Training loss: 3.447772404321659
Validation loss: 2.92871360468349

Epoch: 6| Step: 3
Training loss: 3.72556392318201
Validation loss: 2.927148579942555

Epoch: 6| Step: 4
Training loss: 3.024399084510458
Validation loss: 2.925127294529774

Epoch: 6| Step: 5
Training loss: 3.19642556350944
Validation loss: 2.9253564869603617

Epoch: 6| Step: 6
Training loss: 3.5727474665639143
Validation loss: 2.9267461126537517

Epoch: 6| Step: 7
Training loss: 2.9431665834546075
Validation loss: 2.923656118827234

Epoch: 6| Step: 8
Training loss: 2.6864848326526984
Validation loss: 2.9259879124701227

Epoch: 6| Step: 9
Training loss: 4.004321148472912
Validation loss: 2.926197728322549

Epoch: 6| Step: 10
Training loss: 3.557305011565989
Validation loss: 2.926302953876482

Epoch: 6| Step: 11
Training loss: 2.7987732652290953
Validation loss: 2.926022963137595

Epoch: 6| Step: 12
Training loss: 2.787406365571079
Validation loss: 2.9229719370872256

Epoch: 6| Step: 13
Training loss: 2.9269068835597243
Validation loss: 2.9257941436122032

Epoch: 134| Step: 0
Training loss: 3.1952304223298555
Validation loss: 2.9246729931352484

Epoch: 6| Step: 1
Training loss: 3.2172781532710437
Validation loss: 2.9242925604748757

Epoch: 6| Step: 2
Training loss: 2.8782300633774036
Validation loss: 2.9263853302754606

Epoch: 6| Step: 3
Training loss: 3.773835660727325
Validation loss: 2.9252348871527554

Epoch: 6| Step: 4
Training loss: 3.4970617222300944
Validation loss: 2.925125933448894

Epoch: 6| Step: 5
Training loss: 2.731601376485429
Validation loss: 2.922980069236086

Epoch: 6| Step: 6
Training loss: 3.4879386708003697
Validation loss: 2.9287032589391537

Epoch: 6| Step: 7
Training loss: 2.6353087773811885
Validation loss: 2.925797509172982

Epoch: 6| Step: 8
Training loss: 3.218453995510239
Validation loss: 2.931902374571317

Epoch: 6| Step: 9
Training loss: 3.036638170590852
Validation loss: 2.945365495537645

Epoch: 6| Step: 10
Training loss: 3.3222618932775925
Validation loss: 2.9366688317161973

Epoch: 6| Step: 11
Training loss: 2.7477221158104963
Validation loss: 2.934471877095069

Epoch: 6| Step: 12
Training loss: 3.7392770682531755
Validation loss: 2.9318293372000563

Epoch: 6| Step: 13
Training loss: 3.2957222175112935
Validation loss: 2.9214579844132333

Epoch: 135| Step: 0
Training loss: 3.3352870619733173
Validation loss: 2.918045576445746

Epoch: 6| Step: 1
Training loss: 2.838778835269442
Validation loss: 2.921776445740419

Epoch: 6| Step: 2
Training loss: 3.0319458467584446
Validation loss: 2.922268527597104

Epoch: 6| Step: 3
Training loss: 2.9854772639241793
Validation loss: 2.9245161431983493

Epoch: 6| Step: 4
Training loss: 3.3572750181486994
Validation loss: 2.922639959983616

Epoch: 6| Step: 5
Training loss: 3.4152539441707446
Validation loss: 2.9237739534460476

Epoch: 6| Step: 6
Training loss: 3.620189500348375
Validation loss: 2.924028183314543

Epoch: 6| Step: 7
Training loss: 3.0597529644450345
Validation loss: 2.9245448526936038

Epoch: 6| Step: 8
Training loss: 3.4350645280798955
Validation loss: 2.926880025227615

Epoch: 6| Step: 9
Training loss: 2.9466753571441138
Validation loss: 2.9264696464886133

Epoch: 6| Step: 10
Training loss: 3.356110730114262
Validation loss: 2.9249056762766648

Epoch: 6| Step: 11
Training loss: 3.369985457935143
Validation loss: 2.9247108162964652

Epoch: 6| Step: 12
Training loss: 2.7852659912745996
Validation loss: 2.9259896174816853

Epoch: 6| Step: 13
Training loss: 3.4408243923636195
Validation loss: 2.9236836617848945

Epoch: 136| Step: 0
Training loss: 3.1125456779835665
Validation loss: 2.922470061133229

Epoch: 6| Step: 1
Training loss: 2.656701621763362
Validation loss: 2.9211623870512695

Epoch: 6| Step: 2
Training loss: 3.2973189936335197
Validation loss: 2.921049630450098

Epoch: 6| Step: 3
Training loss: 3.1394385378903213
Validation loss: 2.920589306950196

Epoch: 6| Step: 4
Training loss: 2.483113670448095
Validation loss: 2.9223732752935834

Epoch: 6| Step: 5
Training loss: 4.0136010676540135
Validation loss: 2.919261012062137

Epoch: 6| Step: 6
Training loss: 2.7435531476078205
Validation loss: 2.9202796467996883

Epoch: 6| Step: 7
Training loss: 3.8798946108711876
Validation loss: 2.918904502225824

Epoch: 6| Step: 8
Training loss: 3.7846460146138656
Validation loss: 2.9181698179393076

Epoch: 6| Step: 9
Training loss: 2.6411539484730744
Validation loss: 2.917171262932113

Epoch: 6| Step: 10
Training loss: 3.188750377073307
Validation loss: 2.9180359879663573

Epoch: 6| Step: 11
Training loss: 3.5719698141601763
Validation loss: 2.9210950051516034

Epoch: 6| Step: 12
Training loss: 2.786978490652707
Validation loss: 2.918926394366111

Epoch: 6| Step: 13
Training loss: 3.0909363253306488
Validation loss: 2.9211682529890477

Epoch: 137| Step: 0
Training loss: 2.7006803714979197
Validation loss: 2.919357484838854

Epoch: 6| Step: 1
Training loss: 3.728857585238569
Validation loss: 2.9234956409301867

Epoch: 6| Step: 2
Training loss: 3.1817844983584513
Validation loss: 2.92081388409955

Epoch: 6| Step: 3
Training loss: 3.044862366119901
Validation loss: 2.9255511549650217

Epoch: 6| Step: 4
Training loss: 3.571061537819948
Validation loss: 2.9242387622262016

Epoch: 6| Step: 5
Training loss: 2.9390610138698228
Validation loss: 2.929217102132831

Epoch: 6| Step: 6
Training loss: 3.574266243707679
Validation loss: 2.9361745077581154

Epoch: 6| Step: 7
Training loss: 3.3790493444230583
Validation loss: 2.94694636709812

Epoch: 6| Step: 8
Training loss: 3.2488914946815117
Validation loss: 2.940824254032741

Epoch: 6| Step: 9
Training loss: 2.7604739393134845
Validation loss: 2.929398272713687

Epoch: 6| Step: 10
Training loss: 2.8268358021493274
Validation loss: 2.931810365793202

Epoch: 6| Step: 11
Training loss: 3.3251696435775027
Validation loss: 2.9207510065906486

Epoch: 6| Step: 12
Training loss: 3.3076850877386192
Validation loss: 2.9157428523624183

Epoch: 6| Step: 13
Training loss: 3.021283942628912
Validation loss: 2.916782277243359

Epoch: 138| Step: 0
Training loss: 3.8528842476457603
Validation loss: 2.9164124950872803

Epoch: 6| Step: 1
Training loss: 2.8617272212605815
Validation loss: 2.918230645320403

Epoch: 6| Step: 2
Training loss: 3.5033713860687943
Validation loss: 2.9214070403668373

Epoch: 6| Step: 3
Training loss: 3.4727411077406276
Validation loss: 2.9240641140472268

Epoch: 6| Step: 4
Training loss: 3.00969971739973
Validation loss: 2.9233023128584152

Epoch: 6| Step: 5
Training loss: 3.1843603790816055
Validation loss: 2.930557013873866

Epoch: 6| Step: 6
Training loss: 3.16805484527739
Validation loss: 2.9200606456341296

Epoch: 6| Step: 7
Training loss: 2.499642537310296
Validation loss: 2.916560884540972

Epoch: 6| Step: 8
Training loss: 2.9665252380377685
Validation loss: 2.9140405106159526

Epoch: 6| Step: 9
Training loss: 3.234844671231846
Validation loss: 2.9115748797723744

Epoch: 6| Step: 10
Training loss: 3.3245119353304022
Validation loss: 2.9141533818289718

Epoch: 6| Step: 11
Training loss: 3.1117210547160683
Validation loss: 2.910072904126804

Epoch: 6| Step: 12
Training loss: 3.655571507809149
Validation loss: 2.9128241684147107

Epoch: 6| Step: 13
Training loss: 2.486135662627746
Validation loss: 2.910191576794832

Epoch: 139| Step: 0
Training loss: 2.3864072605786055
Validation loss: 2.9141759096259134

Epoch: 6| Step: 1
Training loss: 3.1126645577067427
Validation loss: 2.9121373449656236

Epoch: 6| Step: 2
Training loss: 3.3541200212035025
Validation loss: 2.920949192062621

Epoch: 6| Step: 3
Training loss: 3.0298748239778175
Validation loss: 2.9207113099513147

Epoch: 6| Step: 4
Training loss: 2.936558024561444
Validation loss: 2.920486283374427

Epoch: 6| Step: 5
Training loss: 3.2611396321124118
Validation loss: 2.9199400790347374

Epoch: 6| Step: 6
Training loss: 3.5512360637193963
Validation loss: 2.9178406129819487

Epoch: 6| Step: 7
Training loss: 3.452741368590185
Validation loss: 2.9139514877647414

Epoch: 6| Step: 8
Training loss: 2.8933551164618656
Validation loss: 2.9105487099393645

Epoch: 6| Step: 9
Training loss: 2.851343099436321
Validation loss: 2.91136869819819

Epoch: 6| Step: 10
Training loss: 3.030191925063851
Validation loss: 2.9111660280391325

Epoch: 6| Step: 11
Training loss: 3.493049532255453
Validation loss: 2.909727286193275

Epoch: 6| Step: 12
Training loss: 3.8485391446194823
Validation loss: 2.91049605199414

Epoch: 6| Step: 13
Training loss: 3.459243118542846
Validation loss: 2.912569012511503

Epoch: 140| Step: 0
Training loss: 3.51123749657072
Validation loss: 2.910257930318557

Epoch: 6| Step: 1
Training loss: 3.111529806512611
Validation loss: 2.91171062587904

Epoch: 6| Step: 2
Training loss: 2.936777878328667
Validation loss: 2.913078821464311

Epoch: 6| Step: 3
Training loss: 3.6441115028247326
Validation loss: 2.911336556627308

Epoch: 6| Step: 4
Training loss: 3.2763990602492172
Validation loss: 2.9125293821574205

Epoch: 6| Step: 5
Training loss: 2.9919372932064423
Validation loss: 2.9108867258416846

Epoch: 6| Step: 6
Training loss: 2.5193228701983
Validation loss: 2.910762442107697

Epoch: 6| Step: 7
Training loss: 2.792619590008548
Validation loss: 2.9132629122743072

Epoch: 6| Step: 8
Training loss: 3.244224110906506
Validation loss: 2.9107223203551635

Epoch: 6| Step: 9
Training loss: 3.385126392929215
Validation loss: 2.914762133256372

Epoch: 6| Step: 10
Training loss: 3.0831191701657996
Validation loss: 2.91928756726781

Epoch: 6| Step: 11
Training loss: 3.358363367392857
Validation loss: 2.9217441036640883

Epoch: 6| Step: 12
Training loss: 3.7255836336821186
Validation loss: 2.915657565618179

Epoch: 6| Step: 13
Training loss: 2.747424046279657
Validation loss: 2.9175679090401028

Epoch: 141| Step: 0
Training loss: 3.488401677871159
Validation loss: 2.9213813459918767

Epoch: 6| Step: 1
Training loss: 3.0500277908810083
Validation loss: 2.9262410581353593

Epoch: 6| Step: 2
Training loss: 2.662726730220648
Validation loss: 2.933386020546138

Epoch: 6| Step: 3
Training loss: 3.8274530560625175
Validation loss: 2.9486240355888578

Epoch: 6| Step: 4
Training loss: 1.987854677618521
Validation loss: 2.9464288482928684

Epoch: 6| Step: 5
Training loss: 2.622296530627964
Validation loss: 2.9500163124958556

Epoch: 6| Step: 6
Training loss: 3.360684303808997
Validation loss: 2.9239148173894196

Epoch: 6| Step: 7
Training loss: 3.2076002166862145
Validation loss: 2.9155461568804504

Epoch: 6| Step: 8
Training loss: 2.4927271913313804
Validation loss: 2.9110905867053494

Epoch: 6| Step: 9
Training loss: 3.8321935092431274
Validation loss: 2.9105356060983403

Epoch: 6| Step: 10
Training loss: 3.3230260912697145
Validation loss: 2.9083955051941395

Epoch: 6| Step: 11
Training loss: 3.3819958271472346
Validation loss: 2.909143001889609

Epoch: 6| Step: 12
Training loss: 3.5396633690730286
Validation loss: 2.9100340467442494

Epoch: 6| Step: 13
Training loss: 3.5017003969737353
Validation loss: 2.9047783018571907

Epoch: 142| Step: 0
Training loss: 3.4925337583511853
Validation loss: 2.9077858396814706

Epoch: 6| Step: 1
Training loss: 3.651318476582348
Validation loss: 2.907812190227337

Epoch: 6| Step: 2
Training loss: 2.9739834884051795
Validation loss: 2.9077176664103885

Epoch: 6| Step: 3
Training loss: 2.683710664870425
Validation loss: 2.906375681611511

Epoch: 6| Step: 4
Training loss: 2.969725117405518
Validation loss: 2.9058928617028683

Epoch: 6| Step: 5
Training loss: 3.162951766604836
Validation loss: 2.905202336491325

Epoch: 6| Step: 6
Training loss: 2.9025579812421762
Validation loss: 2.9052980707368095

Epoch: 6| Step: 7
Training loss: 2.054998686110176
Validation loss: 2.903959225576746

Epoch: 6| Step: 8
Training loss: 3.294241192147815
Validation loss: 2.9060752605295948

Epoch: 6| Step: 9
Training loss: 3.655571507809149
Validation loss: 2.903592086346798

Epoch: 6| Step: 10
Training loss: 3.53662699248999
Validation loss: 2.909104874096466

Epoch: 6| Step: 11
Training loss: 3.839503229512703
Validation loss: 2.9060517560050383

Epoch: 6| Step: 12
Training loss: 3.2046344456961067
Validation loss: 2.905747362120195

Epoch: 6| Step: 13
Training loss: 2.7054498630517654
Validation loss: 2.9057699445000607

Epoch: 143| Step: 0
Training loss: 3.597387579247572
Validation loss: 2.904291439290606

Epoch: 6| Step: 1
Training loss: 3.663788547319593
Validation loss: 2.9043631937192735

Epoch: 6| Step: 2
Training loss: 3.1504034904127414
Validation loss: 2.9032802291837214

Epoch: 6| Step: 3
Training loss: 2.4675728611107184
Validation loss: 2.903277347020607

Epoch: 6| Step: 4
Training loss: 3.293158293820195
Validation loss: 2.902712822329478

Epoch: 6| Step: 5
Training loss: 3.3782870921612016
Validation loss: 2.9033116449293113

Epoch: 6| Step: 6
Training loss: 2.964628068519854
Validation loss: 2.9057298508618805

Epoch: 6| Step: 7
Training loss: 3.372312252624187
Validation loss: 2.90196860231891

Epoch: 6| Step: 8
Training loss: 2.954418565119432
Validation loss: 2.904258570597889

Epoch: 6| Step: 9
Training loss: 3.4109814181888316
Validation loss: 2.906009300233143

Epoch: 6| Step: 10
Training loss: 3.527466223735491
Validation loss: 2.902874795341395

Epoch: 6| Step: 11
Training loss: 2.4854737737020565
Validation loss: 2.904732200096228

Epoch: 6| Step: 12
Training loss: 2.5117895611737824
Validation loss: 2.9035608784752465

Epoch: 6| Step: 13
Training loss: 3.8572978866544854
Validation loss: 2.909336384396409

Epoch: 144| Step: 0
Training loss: 3.5649827035963186
Validation loss: 2.9047971620994653

Epoch: 6| Step: 1
Training loss: 2.7452175864333217
Validation loss: 2.9020277056690764

Epoch: 6| Step: 2
Training loss: 3.228863872154914
Validation loss: 2.8999534149515935

Epoch: 6| Step: 3
Training loss: 3.5550123799855586
Validation loss: 2.9018864930962267

Epoch: 6| Step: 4
Training loss: 3.3677527712151534
Validation loss: 2.899003585021203

Epoch: 6| Step: 5
Training loss: 3.744236649867361
Validation loss: 2.9018078464951826

Epoch: 6| Step: 6
Training loss: 2.6074807606880626
Validation loss: 2.900626587953287

Epoch: 6| Step: 7
Training loss: 3.176443033368973
Validation loss: 2.8986013879311936

Epoch: 6| Step: 8
Training loss: 2.9184452265347045
Validation loss: 2.897520971559512

Epoch: 6| Step: 9
Training loss: 3.177051248023607
Validation loss: 2.899252322351393

Epoch: 6| Step: 10
Training loss: 2.4312395737922126
Validation loss: 2.8951845570316994

Epoch: 6| Step: 11
Training loss: 3.6574646326051226
Validation loss: 2.8954093603854374

Epoch: 6| Step: 12
Training loss: 2.905948089997103
Validation loss: 2.8970143820946124

Epoch: 6| Step: 13
Training loss: 3.3449093243863426
Validation loss: 2.896113519281794

Epoch: 145| Step: 0
Training loss: 2.9931793560987754
Validation loss: 2.896371937562136

Epoch: 6| Step: 1
Training loss: 2.8556160321327573
Validation loss: 2.900104085690119

Epoch: 6| Step: 2
Training loss: 2.712367930910749
Validation loss: 2.9120937612828106

Epoch: 6| Step: 3
Training loss: 3.718711404039141
Validation loss: 2.913746991213542

Epoch: 6| Step: 4
Training loss: 3.0059491615691836
Validation loss: 2.9178028545215753

Epoch: 6| Step: 5
Training loss: 3.5407445417729893
Validation loss: 2.9231072530301767

Epoch: 6| Step: 6
Training loss: 3.2101434992864015
Validation loss: 2.9269627217198386

Epoch: 6| Step: 7
Training loss: 2.5865348748260493
Validation loss: 2.9122883052506103

Epoch: 6| Step: 8
Training loss: 3.850780724966362
Validation loss: 2.9145398420765254

Epoch: 6| Step: 9
Training loss: 3.3608891822821314
Validation loss: 2.902997802454342

Epoch: 6| Step: 10
Training loss: 3.0989867307927272
Validation loss: 2.896958036598035

Epoch: 6| Step: 11
Training loss: 3.267565429079116
Validation loss: 2.8959721245522934

Epoch: 6| Step: 12
Training loss: 3.0325855577040066
Validation loss: 2.8955211446041824

Epoch: 6| Step: 13
Training loss: 3.1706878996358143
Validation loss: 2.892126022797647

Epoch: 146| Step: 0
Training loss: 2.5022671909631677
Validation loss: 2.891295103775733

Epoch: 6| Step: 1
Training loss: 3.9227082480589415
Validation loss: 2.891762594918183

Epoch: 6| Step: 2
Training loss: 2.9735510302793347
Validation loss: 2.8933283826115463

Epoch: 6| Step: 3
Training loss: 2.975582572251543
Validation loss: 2.89408793110203

Epoch: 6| Step: 4
Training loss: 3.828695698451729
Validation loss: 2.896066273984555

Epoch: 6| Step: 5
Training loss: 2.6999951362566055
Validation loss: 2.8927831715378356

Epoch: 6| Step: 6
Training loss: 3.549038533852585
Validation loss: 2.8937510714000396

Epoch: 6| Step: 7
Training loss: 3.0458685361930034
Validation loss: 2.8904057085599906

Epoch: 6| Step: 8
Training loss: 3.2353033677647183
Validation loss: 2.89078806445193

Epoch: 6| Step: 9
Training loss: 3.145657128121276
Validation loss: 2.893633235926099

Epoch: 6| Step: 10
Training loss: 2.955033266038397
Validation loss: 2.894525321223799

Epoch: 6| Step: 11
Training loss: 3.625804022709472
Validation loss: 2.8892901344141864

Epoch: 6| Step: 12
Training loss: 3.008553391401112
Validation loss: 2.892336256439626

Epoch: 6| Step: 13
Training loss: 2.330085492182402
Validation loss: 2.890685807475116

Epoch: 147| Step: 0
Training loss: 2.885509025849137
Validation loss: 2.8948655206913574

Epoch: 6| Step: 1
Training loss: 3.0479031906473186
Validation loss: 2.901314526381528

Epoch: 6| Step: 2
Training loss: 3.713933823233712
Validation loss: 2.903453593450267

Epoch: 6| Step: 3
Training loss: 3.1031576133773666
Validation loss: 2.906226629206677

Epoch: 6| Step: 4
Training loss: 3.7670142432567104
Validation loss: 2.9048920474709

Epoch: 6| Step: 5
Training loss: 2.465710277557273
Validation loss: 2.894982265490565

Epoch: 6| Step: 6
Training loss: 3.07910032360029
Validation loss: 2.896304950803843

Epoch: 6| Step: 7
Training loss: 2.9600769241750613
Validation loss: 2.892562308363807

Epoch: 6| Step: 8
Training loss: 3.1930640034459907
Validation loss: 2.8943755584702866

Epoch: 6| Step: 9
Training loss: 3.3397376684998736
Validation loss: 2.892292567604692

Epoch: 6| Step: 10
Training loss: 2.8986319504980695
Validation loss: 2.892010105887144

Epoch: 6| Step: 11
Training loss: 2.959607311181902
Validation loss: 2.8925674275535176

Epoch: 6| Step: 12
Training loss: 3.707095050162256
Validation loss: 2.89501073389084

Epoch: 6| Step: 13
Training loss: 3.252957538908088
Validation loss: 2.891746947582639

Epoch: 148| Step: 0
Training loss: 3.602235942113427
Validation loss: 2.8905022636947493

Epoch: 6| Step: 1
Training loss: 3.048047901247745
Validation loss: 2.893760279669069

Epoch: 6| Step: 2
Training loss: 2.7779028821959555
Validation loss: 2.896774587565652

Epoch: 6| Step: 3
Training loss: 3.4635462863671416
Validation loss: 2.893762092261195

Epoch: 6| Step: 4
Training loss: 3.2606547373343706
Validation loss: 2.897168052335589

Epoch: 6| Step: 5
Training loss: 3.233433683221145
Validation loss: 2.8948703639349582

Epoch: 6| Step: 6
Training loss: 3.2064714024415206
Validation loss: 2.9010624417100375

Epoch: 6| Step: 7
Training loss: 2.90863202008633
Validation loss: 2.8958400184151087

Epoch: 6| Step: 8
Training loss: 3.193278591055784
Validation loss: 2.9071904342495167

Epoch: 6| Step: 9
Training loss: 2.960036007172434
Validation loss: 2.9028162011567074

Epoch: 6| Step: 10
Training loss: 3.6179596507288134
Validation loss: 2.902951753663871

Epoch: 6| Step: 11
Training loss: 3.4783951290279
Validation loss: 2.88817961494232

Epoch: 6| Step: 12
Training loss: 2.673780598298165
Validation loss: 2.88839242219903

Epoch: 6| Step: 13
Training loss: 2.8704275175670184
Validation loss: 2.888020057796106

Epoch: 149| Step: 0
Training loss: 3.600984539402403
Validation loss: 2.886067295134346

Epoch: 6| Step: 1
Training loss: 3.0122737942051154
Validation loss: 2.885151835655413

Epoch: 6| Step: 2
Training loss: 3.7065397204014388
Validation loss: 2.886911050472901

Epoch: 6| Step: 3
Training loss: 3.6243207887139897
Validation loss: 2.8862037546218007

Epoch: 6| Step: 4
Training loss: 3.2052614111974282
Validation loss: 2.8863093865901024

Epoch: 6| Step: 5
Training loss: 3.791159389782734
Validation loss: 2.886117028263324

Epoch: 6| Step: 6
Training loss: 2.687706518000858
Validation loss: 2.887751648748446

Epoch: 6| Step: 7
Training loss: 2.9962949125753626
Validation loss: 2.8879575954445715

Epoch: 6| Step: 8
Training loss: 3.1650132999472174
Validation loss: 2.8934390395830842

Epoch: 6| Step: 9
Training loss: 2.9029939521308683
Validation loss: 2.898848969417035

Epoch: 6| Step: 10
Training loss: 2.8247281745508395
Validation loss: 2.9005107562108017

Epoch: 6| Step: 11
Training loss: 3.423724645692747
Validation loss: 2.892012460314171

Epoch: 6| Step: 12
Training loss: 2.7565956781265575
Validation loss: 2.8859847090747714

Epoch: 6| Step: 13
Training loss: 1.8447275964462428
Validation loss: 2.884388477276755

Epoch: 150| Step: 0
Training loss: 3.44070978294084
Validation loss: 2.8827370325461583

Epoch: 6| Step: 1
Training loss: 3.2297378691135346
Validation loss: 2.880910656978185

Epoch: 6| Step: 2
Training loss: 2.5431928169693836
Validation loss: 2.881940138150941

Epoch: 6| Step: 3
Training loss: 2.8854315214543633
Validation loss: 2.8802709898462093

Epoch: 6| Step: 4
Training loss: 3.0229113217040817
Validation loss: 2.881952598992827

Epoch: 6| Step: 5
Training loss: 2.962551673406312
Validation loss: 2.881084977554717

Epoch: 6| Step: 6
Training loss: 3.514137054914429
Validation loss: 2.881582319631463

Epoch: 6| Step: 7
Training loss: 3.054273182120504
Validation loss: 2.8800718077130028

Epoch: 6| Step: 8
Training loss: 3.073275856534269
Validation loss: 2.8821186153559886

Epoch: 6| Step: 9
Training loss: 3.458107094469292
Validation loss: 2.878726574439368

Epoch: 6| Step: 10
Training loss: 3.0493302845085504
Validation loss: 2.8806340199839924

Epoch: 6| Step: 11
Training loss: 3.24209807858613
Validation loss: 2.8845468390760765

Epoch: 6| Step: 12
Training loss: 3.4512790907228217
Validation loss: 2.8912263510239136

Epoch: 6| Step: 13
Training loss: 3.5504813660408114
Validation loss: 2.908113109786932

Epoch: 151| Step: 0
Training loss: 3.264430653585006
Validation loss: 2.9292695565670357

Epoch: 6| Step: 1
Training loss: 3.3963462270639897
Validation loss: 2.9177913559958775

Epoch: 6| Step: 2
Training loss: 2.358050852685866
Validation loss: 2.8830379888448747

Epoch: 6| Step: 3
Training loss: 2.770219251996506
Validation loss: 2.879297380587869

Epoch: 6| Step: 4
Training loss: 3.302601257204223
Validation loss: 2.8835182272089748

Epoch: 6| Step: 5
Training loss: 3.3246443192540673
Validation loss: 2.8808853026443977

Epoch: 6| Step: 6
Training loss: 2.7143695664181258
Validation loss: 2.8849712410216717

Epoch: 6| Step: 7
Training loss: 2.8898032180158877
Validation loss: 2.8834909567307405

Epoch: 6| Step: 8
Training loss: 3.7853676310785183
Validation loss: 2.8855084554619244

Epoch: 6| Step: 9
Training loss: 3.6807002073129076
Validation loss: 2.884923569765941

Epoch: 6| Step: 10
Training loss: 3.0068923930028064
Validation loss: 2.888210169736884

Epoch: 6| Step: 11
Training loss: 3.0823692842289208
Validation loss: 2.8849954060290997

Epoch: 6| Step: 12
Training loss: 3.3874073142571897
Validation loss: 2.882023731861975

Epoch: 6| Step: 13
Training loss: 3.341736231687847
Validation loss: 2.883284610080792

Epoch: 152| Step: 0
Training loss: 2.8235822411461418
Validation loss: 2.8823080109713874

Epoch: 6| Step: 1
Training loss: 3.0327223515267923
Validation loss: 2.8781639726497836

Epoch: 6| Step: 2
Training loss: 2.9522726453671257
Validation loss: 2.8793158476973315

Epoch: 6| Step: 3
Training loss: 3.1289117363013697
Validation loss: 2.8811598458498633

Epoch: 6| Step: 4
Training loss: 3.273789125189045
Validation loss: 2.8788774228750618

Epoch: 6| Step: 5
Training loss: 3.896746618485631
Validation loss: 2.8792686108016623

Epoch: 6| Step: 6
Training loss: 2.7355711146436508
Validation loss: 2.882531296286811

Epoch: 6| Step: 7
Training loss: 2.8804918992323114
Validation loss: 2.883100491089972

Epoch: 6| Step: 8
Training loss: 3.579854755332247
Validation loss: 2.8808452773683446

Epoch: 6| Step: 9
Training loss: 3.842848012896129
Validation loss: 2.8863058639583943

Epoch: 6| Step: 10
Training loss: 2.6850328876036933
Validation loss: 2.882717017714494

Epoch: 6| Step: 11
Training loss: 2.8639098514349217
Validation loss: 2.8860749578858766

Epoch: 6| Step: 12
Training loss: 2.9353243298669383
Validation loss: 2.8834677082065627

Epoch: 6| Step: 13
Training loss: 3.711781616577818
Validation loss: 2.885901672488047

Epoch: 153| Step: 0
Training loss: 2.1670282746826186
Validation loss: 2.8866279396052685

Epoch: 6| Step: 1
Training loss: 2.905504028401547
Validation loss: 2.889682676266078

Epoch: 6| Step: 2
Training loss: 3.092244562255756
Validation loss: 2.8961977448682115

Epoch: 6| Step: 3
Training loss: 2.97592132150566
Validation loss: 2.906863091131777

Epoch: 6| Step: 4
Training loss: 2.864390492739679
Validation loss: 2.9046814295927117

Epoch: 6| Step: 5
Training loss: 3.954597530600089
Validation loss: 2.9024759169569703

Epoch: 6| Step: 6
Training loss: 2.9220979166929384
Validation loss: 2.8997362771168196

Epoch: 6| Step: 7
Training loss: 3.2788326986851275
Validation loss: 2.8905418678929986

Epoch: 6| Step: 8
Training loss: 2.784189449682552
Validation loss: 2.8886563440454065

Epoch: 6| Step: 9
Training loss: 2.6784132883528446
Validation loss: 2.881677213211454

Epoch: 6| Step: 10
Training loss: 3.2437311013907864
Validation loss: 2.8774156342280985

Epoch: 6| Step: 11
Training loss: 3.199709378396915
Validation loss: 2.8751100962256264

Epoch: 6| Step: 12
Training loss: 3.8772977353927613
Validation loss: 2.875979028106657

Epoch: 6| Step: 13
Training loss: 4.586629526150575
Validation loss: 2.8794467470020604

Epoch: 154| Step: 0
Training loss: 3.357547564592129
Validation loss: 2.8773987613202974

Epoch: 6| Step: 1
Training loss: 3.7231562819998993
Validation loss: 2.876931875852829

Epoch: 6| Step: 2
Training loss: 3.6194565579014943
Validation loss: 2.880626880739465

Epoch: 6| Step: 3
Training loss: 2.95651954244693
Validation loss: 2.876485485401863

Epoch: 6| Step: 4
Training loss: 2.718096775198724
Validation loss: 2.877557463242113

Epoch: 6| Step: 5
Training loss: 3.2748838156555236
Validation loss: 2.87818622989611

Epoch: 6| Step: 6
Training loss: 3.599034148032782
Validation loss: 2.8776454159566955

Epoch: 6| Step: 7
Training loss: 3.340876119683325
Validation loss: 2.87627531517036

Epoch: 6| Step: 8
Training loss: 3.107507499507776
Validation loss: 2.8767883317297405

Epoch: 6| Step: 9
Training loss: 3.226797469406677
Validation loss: 2.875194407449067

Epoch: 6| Step: 10
Training loss: 3.0915538044059834
Validation loss: 2.8761658135045636

Epoch: 6| Step: 11
Training loss: 2.7372411039373175
Validation loss: 2.8754212337040808

Epoch: 6| Step: 12
Training loss: 2.383550210790794
Validation loss: 2.8733762335069923

Epoch: 6| Step: 13
Training loss: 2.956287446702248
Validation loss: 2.8756515171328734

Epoch: 155| Step: 0
Training loss: 3.0567621468954482
Validation loss: 2.874989026705437

Epoch: 6| Step: 1
Training loss: 3.0676142748848885
Validation loss: 2.875051855517572

Epoch: 6| Step: 2
Training loss: 3.308805889007817
Validation loss: 2.8763491553360714

Epoch: 6| Step: 3
Training loss: 3.0604540160211013
Validation loss: 2.8738618758557894

Epoch: 6| Step: 4
Training loss: 3.4823469751917946
Validation loss: 2.8753987731732997

Epoch: 6| Step: 5
Training loss: 2.989835686394927
Validation loss: 2.875520672404491

Epoch: 6| Step: 6
Training loss: 3.278770745372114
Validation loss: 2.8768322595833347

Epoch: 6| Step: 7
Training loss: 2.806869805415496
Validation loss: 2.87622068474147

Epoch: 6| Step: 8
Training loss: 2.6796833182877813
Validation loss: 2.8758522587769493

Epoch: 6| Step: 9
Training loss: 3.7008872695241712
Validation loss: 2.869343431271838

Epoch: 6| Step: 10
Training loss: 3.355812632444229
Validation loss: 2.8730970088469823

Epoch: 6| Step: 11
Training loss: 2.7283800073669373
Validation loss: 2.868799023664332

Epoch: 6| Step: 12
Training loss: 3.2813121971411716
Validation loss: 2.8718696162581203

Epoch: 6| Step: 13
Training loss: 3.5283043652767203
Validation loss: 2.8744082374644364

Epoch: 156| Step: 0
Training loss: 2.7572697005329476
Validation loss: 2.8715515606592295

Epoch: 6| Step: 1
Training loss: 3.4562027277826495
Validation loss: 2.874091380647757

Epoch: 6| Step: 2
Training loss: 2.801123689921424
Validation loss: 2.8712393815200508

Epoch: 6| Step: 3
Training loss: 2.9754886642596534
Validation loss: 2.871862465938016

Epoch: 6| Step: 4
Training loss: 3.3938926034546495
Validation loss: 2.8730872007597936

Epoch: 6| Step: 5
Training loss: 3.201585353101391
Validation loss: 2.868258563416263

Epoch: 6| Step: 6
Training loss: 3.6842488326325618
Validation loss: 2.872623979457273

Epoch: 6| Step: 7
Training loss: 3.063085694651444
Validation loss: 2.870302557319707

Epoch: 6| Step: 8
Training loss: 3.3424423370294942
Validation loss: 2.8713183216617866

Epoch: 6| Step: 9
Training loss: 3.162667275328468
Validation loss: 2.8708879312365685

Epoch: 6| Step: 10
Training loss: 3.3157928222983526
Validation loss: 2.867398612602614

Epoch: 6| Step: 11
Training loss: 3.329619882327563
Validation loss: 2.8717315012393057

Epoch: 6| Step: 12
Training loss: 2.9038090453300685
Validation loss: 2.8668291394889778

Epoch: 6| Step: 13
Training loss: 2.37391929131263
Validation loss: 2.8722708661915277

Epoch: 157| Step: 0
Training loss: 3.4416406835232554
Validation loss: 2.8720695046872207

Epoch: 6| Step: 1
Training loss: 2.9025979014206467
Validation loss: 2.876809852843784

Epoch: 6| Step: 2
Training loss: 3.237873194331551
Validation loss: 2.8755099168502833

Epoch: 6| Step: 3
Training loss: 2.9161467679192516
Validation loss: 2.8682253524122783

Epoch: 6| Step: 4
Training loss: 2.853784484521254
Validation loss: 2.868978868810076

Epoch: 6| Step: 5
Training loss: 3.768045747410405
Validation loss: 2.870095068654834

Epoch: 6| Step: 6
Training loss: 2.322628624909296
Validation loss: 2.868212192857841

Epoch: 6| Step: 7
Training loss: 2.9070177602313994
Validation loss: 2.867749136168666

Epoch: 6| Step: 8
Training loss: 3.203537035236648
Validation loss: 2.8678599800613225

Epoch: 6| Step: 9
Training loss: 2.8320278825411016
Validation loss: 2.8657482927862974

Epoch: 6| Step: 10
Training loss: 3.8925623225892863
Validation loss: 2.8663095473107503

Epoch: 6| Step: 11
Training loss: 2.893883760133272
Validation loss: 2.866049965011673

Epoch: 6| Step: 12
Training loss: 3.083666310931728
Validation loss: 2.867306717345007

Epoch: 6| Step: 13
Training loss: 4.014496997379345
Validation loss: 2.865336377156382

Epoch: 158| Step: 0
Training loss: 2.520427976021535
Validation loss: 2.8690822979632116

Epoch: 6| Step: 1
Training loss: 2.9158242961972083
Validation loss: 2.867122778506133

Epoch: 6| Step: 2
Training loss: 3.5544564979027946
Validation loss: 2.866726098537958

Epoch: 6| Step: 3
Training loss: 2.3893988961899026
Validation loss: 2.8667234765233247

Epoch: 6| Step: 4
Training loss: 3.4175694521228106
Validation loss: 2.866472916400272

Epoch: 6| Step: 5
Training loss: 3.343807326921693
Validation loss: 2.866592056145778

Epoch: 6| Step: 6
Training loss: 3.3822680733416575
Validation loss: 2.865209968170418

Epoch: 6| Step: 7
Training loss: 3.1972936630184794
Validation loss: 2.8676904813486024

Epoch: 6| Step: 8
Training loss: 3.030251879404865
Validation loss: 2.8656185310433804

Epoch: 6| Step: 9
Training loss: 3.711270801232781
Validation loss: 2.8649514924718953

Epoch: 6| Step: 10
Training loss: 3.4628904873008293
Validation loss: 2.865057108938654

Epoch: 6| Step: 11
Training loss: 3.1018319961730536
Validation loss: 2.86698987955451

Epoch: 6| Step: 12
Training loss: 2.9743557657195847
Validation loss: 2.866716401900935

Epoch: 6| Step: 13
Training loss: 2.8959963818943755
Validation loss: 2.86571723191151

Epoch: 159| Step: 0
Training loss: 3.366763342674773
Validation loss: 2.8656754659057033

Epoch: 6| Step: 1
Training loss: 3.141423973065757
Validation loss: 2.8678217923903

Epoch: 6| Step: 2
Training loss: 3.2038050976263617
Validation loss: 2.864918240446821

Epoch: 6| Step: 3
Training loss: 2.953180383233562
Validation loss: 2.869672312540985

Epoch: 6| Step: 4
Training loss: 2.732105643110674
Validation loss: 2.8714428921889974

Epoch: 6| Step: 5
Training loss: 3.035269677437329
Validation loss: 2.8646638031486784

Epoch: 6| Step: 6
Training loss: 3.2379622906285097
Validation loss: 2.8670076077587345

Epoch: 6| Step: 7
Training loss: 3.302514771075428
Validation loss: 2.8759680032807307

Epoch: 6| Step: 8
Training loss: 3.7371450708581877
Validation loss: 2.8651126054169156

Epoch: 6| Step: 9
Training loss: 3.2484057991422315
Validation loss: 2.8647882397584405

Epoch: 6| Step: 10
Training loss: 3.2687083021024375
Validation loss: 2.8647785759370334

Epoch: 6| Step: 11
Training loss: 3.0525944165773384
Validation loss: 2.8652255233206385

Epoch: 6| Step: 12
Training loss: 3.140465575769423
Validation loss: 2.8636790588773025

Epoch: 6| Step: 13
Training loss: 2.215776519544381
Validation loss: 2.8654928538108124

Epoch: 160| Step: 0
Training loss: 2.8777688003786226
Validation loss: 2.8627055694251675

Epoch: 6| Step: 1
Training loss: 2.5166170995348867
Validation loss: 2.8616645729138077

Epoch: 6| Step: 2
Training loss: 3.0604216081974513
Validation loss: 2.862444870367715

Epoch: 6| Step: 3
Training loss: 3.196233863715241
Validation loss: 2.8657833529447325

Epoch: 6| Step: 4
Training loss: 3.8633290163487257
Validation loss: 2.8625074693956982

Epoch: 6| Step: 5
Training loss: 3.145784002932024
Validation loss: 2.8624864640887364

Epoch: 6| Step: 6
Training loss: 3.1801785926332
Validation loss: 2.860916889310188

Epoch: 6| Step: 7
Training loss: 3.219218099685053
Validation loss: 2.86270506882348

Epoch: 6| Step: 8
Training loss: 2.091494697806659
Validation loss: 2.8601229405757778

Epoch: 6| Step: 9
Training loss: 2.9297355139294723
Validation loss: 2.864619876632211

Epoch: 6| Step: 10
Training loss: 3.580744288557084
Validation loss: 2.8647563254884867

Epoch: 6| Step: 11
Training loss: 3.5697237359870764
Validation loss: 2.863112804524232

Epoch: 6| Step: 12
Training loss: 3.138323039886995
Validation loss: 2.8636378674313123

Epoch: 6| Step: 13
Training loss: 3.6293135661816165
Validation loss: 2.8646381367710996

Epoch: 161| Step: 0
Training loss: 3.6162723841997955
Validation loss: 2.8611774758126542

Epoch: 6| Step: 1
Training loss: 3.113644529659749
Validation loss: 2.8635738526427548

Epoch: 6| Step: 2
Training loss: 3.2503664837126265
Validation loss: 2.865604113305649

Epoch: 6| Step: 3
Training loss: 2.3674997054586258
Validation loss: 2.861830611504535

Epoch: 6| Step: 4
Training loss: 2.555473929866158
Validation loss: 2.8612616295279967

Epoch: 6| Step: 5
Training loss: 2.378380026658586
Validation loss: 2.8623050117274875

Epoch: 6| Step: 6
Training loss: 3.299437047162142
Validation loss: 2.865095753112529

Epoch: 6| Step: 7
Training loss: 3.5855297779340987
Validation loss: 2.8628351721675873

Epoch: 6| Step: 8
Training loss: 2.271725878747294
Validation loss: 2.8652629797042843

Epoch: 6| Step: 9
Training loss: 3.4917322327156404
Validation loss: 2.86529251284473

Epoch: 6| Step: 10
Training loss: 3.3544488318249885
Validation loss: 2.8649748053474906

Epoch: 6| Step: 11
Training loss: 3.1205797595781255
Validation loss: 2.8657507385665233

Epoch: 6| Step: 12
Training loss: 3.6824030160142813
Validation loss: 2.8592542415146878

Epoch: 6| Step: 13
Training loss: 3.813121182224555
Validation loss: 2.8628207914554986

Epoch: 162| Step: 0
Training loss: 2.2719392329269708
Validation loss: 2.8596807422082713

Epoch: 6| Step: 1
Training loss: 2.9883731603512658
Validation loss: 2.8588636687403812

Epoch: 6| Step: 2
Training loss: 3.519243472037494
Validation loss: 2.8586698052855395

Epoch: 6| Step: 3
Training loss: 2.8704667217622695
Validation loss: 2.8565825861923386

Epoch: 6| Step: 4
Training loss: 3.1396914181058837
Validation loss: 2.8572447128349556

Epoch: 6| Step: 5
Training loss: 2.7241135231483127
Validation loss: 2.859269250754152

Epoch: 6| Step: 6
Training loss: 3.3264107027623857
Validation loss: 2.85511851997637

Epoch: 6| Step: 7
Training loss: 3.642111937116677
Validation loss: 2.855595038078948

Epoch: 6| Step: 8
Training loss: 2.7338422855800473
Validation loss: 2.854334038076372

Epoch: 6| Step: 9
Training loss: 2.7717946519355974
Validation loss: 2.857078159184998

Epoch: 6| Step: 10
Training loss: 3.5550257930544227
Validation loss: 2.857111381322226

Epoch: 6| Step: 11
Training loss: 3.472373238564361
Validation loss: 2.8577163290333063

Epoch: 6| Step: 12
Training loss: 3.574478623578538
Validation loss: 2.857332502152885

Epoch: 6| Step: 13
Training loss: 3.3107374288355866
Validation loss: 2.854486116287333

Epoch: 163| Step: 0
Training loss: 3.0018155644108546
Validation loss: 2.8546266866958705

Epoch: 6| Step: 1
Training loss: 3.014901029819137
Validation loss: 2.8596530929026844

Epoch: 6| Step: 2
Training loss: 3.2843620437961776
Validation loss: 2.8562526770095165

Epoch: 6| Step: 3
Training loss: 2.271319579348261
Validation loss: 2.8589834734972923

Epoch: 6| Step: 4
Training loss: 3.124509849255672
Validation loss: 2.8569080423294584

Epoch: 6| Step: 5
Training loss: 2.908378559915872
Validation loss: 2.855802787580228

Epoch: 6| Step: 6
Training loss: 3.428939703054374
Validation loss: 2.8530108509259287

Epoch: 6| Step: 7
Training loss: 3.5916832329765556
Validation loss: 2.8531860728162517

Epoch: 6| Step: 8
Training loss: 3.0929182071126085
Validation loss: 2.8573405223435566

Epoch: 6| Step: 9
Training loss: 3.2694641616847577
Validation loss: 2.8542321771502235

Epoch: 6| Step: 10
Training loss: 3.3332777654466623
Validation loss: 2.855755293719698

Epoch: 6| Step: 11
Training loss: 3.0820411517584922
Validation loss: 2.8537378608057664

Epoch: 6| Step: 12
Training loss: 3.6657125503218215
Validation loss: 2.853677488699979

Epoch: 6| Step: 13
Training loss: 2.6631788650934562
Validation loss: 2.8562721053944458

Epoch: 164| Step: 0
Training loss: 2.9842465358067534
Validation loss: 2.854814226167074

Epoch: 6| Step: 1
Training loss: 3.0733324879488606
Validation loss: 2.8538464416789315

Epoch: 6| Step: 2
Training loss: 3.6051333386103983
Validation loss: 2.8563393147752167

Epoch: 6| Step: 3
Training loss: 3.2371055392176653
Validation loss: 2.8596602217551226

Epoch: 6| Step: 4
Training loss: 2.806241595759052
Validation loss: 2.8556120263517246

Epoch: 6| Step: 5
Training loss: 3.447775446985623
Validation loss: 2.8566222603956124

Epoch: 6| Step: 6
Training loss: 2.9830351213413913
Validation loss: 2.8582091841554518

Epoch: 6| Step: 7
Training loss: 2.165309615516586
Validation loss: 2.856828080327418

Epoch: 6| Step: 8
Training loss: 2.709208880021014
Validation loss: 2.85578447543584

Epoch: 6| Step: 9
Training loss: 3.0384188168947928
Validation loss: 2.858370365015468

Epoch: 6| Step: 10
Training loss: 3.5124570687816496
Validation loss: 2.853815144364473

Epoch: 6| Step: 11
Training loss: 3.188851313161015
Validation loss: 2.8557696660237197

Epoch: 6| Step: 12
Training loss: 3.6294795015720287
Validation loss: 2.8567869076560903

Epoch: 6| Step: 13
Training loss: 3.6152028515727883
Validation loss: 2.856795598826482

Epoch: 165| Step: 0
Training loss: 3.113384020172884
Validation loss: 2.8555206208376918

Epoch: 6| Step: 1
Training loss: 3.7500532782266673
Validation loss: 2.8554073559614377

Epoch: 6| Step: 2
Training loss: 3.6717748750062347
Validation loss: 2.851804630902099

Epoch: 6| Step: 3
Training loss: 2.3045099739625283
Validation loss: 2.850158387068157

Epoch: 6| Step: 4
Training loss: 3.024507239695212
Validation loss: 2.8483055125522476

Epoch: 6| Step: 5
Training loss: 3.29931795990464
Validation loss: 2.849280247370085

Epoch: 6| Step: 6
Training loss: 3.166592563632128
Validation loss: 2.8494803861867415

Epoch: 6| Step: 7
Training loss: 3.17497455256319
Validation loss: 2.8514403443602667

Epoch: 6| Step: 8
Training loss: 2.2005416116730423
Validation loss: 2.8499609153124204

Epoch: 6| Step: 9
Training loss: 3.2014241983190868
Validation loss: 2.85038653107327

Epoch: 6| Step: 10
Training loss: 3.2684950194826383
Validation loss: 2.8492699110203814

Epoch: 6| Step: 11
Training loss: 2.8856491564424718
Validation loss: 2.849936719507432

Epoch: 6| Step: 12
Training loss: 3.004651278606267
Validation loss: 2.8471868121805426

Epoch: 6| Step: 13
Training loss: 3.9109496875044196
Validation loss: 2.8511555950994447

Epoch: 166| Step: 0
Training loss: 3.019596469268574
Validation loss: 2.849829686272854

Epoch: 6| Step: 1
Training loss: 3.606833748205595
Validation loss: 2.8496132200858493

Epoch: 6| Step: 2
Training loss: 3.14785677947348
Validation loss: 2.8537179264645043

Epoch: 6| Step: 3
Training loss: 3.5181676189574262
Validation loss: 2.8509246908519508

Epoch: 6| Step: 4
Training loss: 2.456186995535428
Validation loss: 2.8533137566344653

Epoch: 6| Step: 5
Training loss: 2.53860948482729
Validation loss: 2.853332091823239

Epoch: 6| Step: 6
Training loss: 3.671743577269088
Validation loss: 2.8514702346401117

Epoch: 6| Step: 7
Training loss: 3.7673821999017725
Validation loss: 2.8534552751452624

Epoch: 6| Step: 8
Training loss: 2.749457739339268
Validation loss: 2.8496901333080102

Epoch: 6| Step: 9
Training loss: 3.3701303460243692
Validation loss: 2.8584205903522726

Epoch: 6| Step: 10
Training loss: 3.000310246002174
Validation loss: 2.856025988989679

Epoch: 6| Step: 11
Training loss: 2.617815449095849
Validation loss: 2.854597721254855

Epoch: 6| Step: 12
Training loss: 2.842267666651331
Validation loss: 2.852749271306778

Epoch: 6| Step: 13
Training loss: 3.5010333579336628
Validation loss: 2.848713926066845

Epoch: 167| Step: 0
Training loss: 2.9862731968460574
Validation loss: 2.8601494362431845

Epoch: 6| Step: 1
Training loss: 2.917068853350985
Validation loss: 2.857474207415537

Epoch: 6| Step: 2
Training loss: 3.42482654661366
Validation loss: 2.8646605411688864

Epoch: 6| Step: 3
Training loss: 3.3145195904318387
Validation loss: 2.8622220808421086

Epoch: 6| Step: 4
Training loss: 3.552106184154108
Validation loss: 2.860420305404378

Epoch: 6| Step: 5
Training loss: 3.087324308887006
Validation loss: 2.8583288549022616

Epoch: 6| Step: 6
Training loss: 3.488771683923846
Validation loss: 2.857016922662797

Epoch: 6| Step: 7
Training loss: 2.78536340229366
Validation loss: 2.8540656659715604

Epoch: 6| Step: 8
Training loss: 3.061619534725868
Validation loss: 2.8555130956025505

Epoch: 6| Step: 9
Training loss: 3.445410902214018
Validation loss: 2.853335810598809

Epoch: 6| Step: 10
Training loss: 2.7843919644336683
Validation loss: 2.849493580966636

Epoch: 6| Step: 11
Training loss: 2.908341178409813
Validation loss: 2.8442561237737283

Epoch: 6| Step: 12
Training loss: 2.8776607640196787
Validation loss: 2.842487807396032

Epoch: 6| Step: 13
Training loss: 3.3112880000997333
Validation loss: 2.844963765783983

Epoch: 168| Step: 0
Training loss: 3.523131904967483
Validation loss: 2.844149843034844

Epoch: 6| Step: 1
Training loss: 3.699750819707955
Validation loss: 2.843040402116495

Epoch: 6| Step: 2
Training loss: 2.76492317906976
Validation loss: 2.841385727709249

Epoch: 6| Step: 3
Training loss: 3.434078108612949
Validation loss: 2.844559389448415

Epoch: 6| Step: 4
Training loss: 3.4938701356490447
Validation loss: 2.845562328189036

Epoch: 6| Step: 5
Training loss: 3.046816155281121
Validation loss: 2.844239733767483

Epoch: 6| Step: 6
Training loss: 2.4568833638672105
Validation loss: 2.8411661133455435

Epoch: 6| Step: 7
Training loss: 2.668294459466236
Validation loss: 2.8406007284275034

Epoch: 6| Step: 8
Training loss: 2.9895910247455686
Validation loss: 2.842337757888596

Epoch: 6| Step: 9
Training loss: 3.335343104704317
Validation loss: 2.844665082617769

Epoch: 6| Step: 10
Training loss: 2.390384537315403
Validation loss: 2.844250858142996

Epoch: 6| Step: 11
Training loss: 3.6642397449300717
Validation loss: 2.842382680853765

Epoch: 6| Step: 12
Training loss: 3.1848742571911144
Validation loss: 2.8463173219069504

Epoch: 6| Step: 13
Training loss: 2.918046697471699
Validation loss: 2.851819231672488

Epoch: 169| Step: 0
Training loss: 3.8965591461732574
Validation loss: 2.8454772542919318

Epoch: 6| Step: 1
Training loss: 2.7704764019137196
Validation loss: 2.8506582055976355

Epoch: 6| Step: 2
Training loss: 3.2978768656954225
Validation loss: 2.8560115856608843

Epoch: 6| Step: 3
Training loss: 2.6836326629615974
Validation loss: 2.8537680243705004

Epoch: 6| Step: 4
Training loss: 3.5264516993040487
Validation loss: 2.858430173425654

Epoch: 6| Step: 5
Training loss: 3.1765902944419313
Validation loss: 2.858341895826417

Epoch: 6| Step: 6
Training loss: 3.8387612323602864
Validation loss: 2.847788429903324

Epoch: 6| Step: 7
Training loss: 2.9536232502750632
Validation loss: 2.8511616895888814

Epoch: 6| Step: 8
Training loss: 2.837432719369868
Validation loss: 2.8446400108449224

Epoch: 6| Step: 9
Training loss: 2.7718848812094876
Validation loss: 2.8480331617316375

Epoch: 6| Step: 10
Training loss: 3.1785283705732534
Validation loss: 2.842892465771051

Epoch: 6| Step: 11
Training loss: 2.555845039207369
Validation loss: 2.846831238963004

Epoch: 6| Step: 12
Training loss: 3.29340371441508
Validation loss: 2.8456492580966

Epoch: 6| Step: 13
Training loss: 2.621484127621617
Validation loss: 2.8503194934277762

Epoch: 170| Step: 0
Training loss: 3.1705892427978446
Validation loss: 2.8452087937382107

Epoch: 6| Step: 1
Training loss: 2.647052774235414
Validation loss: 2.839028418988787

Epoch: 6| Step: 2
Training loss: 2.697311588810033
Validation loss: 2.847318247661405

Epoch: 6| Step: 3
Training loss: 3.9497796781878334
Validation loss: 2.842699016484728

Epoch: 6| Step: 4
Training loss: 2.941580677306593
Validation loss: 2.841107012584062

Epoch: 6| Step: 5
Training loss: 3.2661066749564114
Validation loss: 2.840732273703547

Epoch: 6| Step: 6
Training loss: 2.2995798183256935
Validation loss: 2.8400296036469865

Epoch: 6| Step: 7
Training loss: 2.8571269886393615
Validation loss: 2.838486064642154

Epoch: 6| Step: 8
Training loss: 3.151542098395867
Validation loss: 2.842441797481233

Epoch: 6| Step: 9
Training loss: 3.8030736739520097
Validation loss: 2.8416827626996852

Epoch: 6| Step: 10
Training loss: 3.5166689530737316
Validation loss: 2.839754056847773

Epoch: 6| Step: 11
Training loss: 3.5190146148542927
Validation loss: 2.839748881722378

Epoch: 6| Step: 12
Training loss: 2.962006630034306
Validation loss: 2.840457308617063

Epoch: 6| Step: 13
Training loss: 2.3044165500488223
Validation loss: 2.839075144302338

Epoch: 171| Step: 0
Training loss: 3.314914039659555
Validation loss: 2.8424791256903874

Epoch: 6| Step: 1
Training loss: 3.339074118651551
Validation loss: 2.842415548028513

Epoch: 6| Step: 2
Training loss: 2.8104466890315547
Validation loss: 2.8393547351474946

Epoch: 6| Step: 3
Training loss: 2.931083000973147
Validation loss: 2.8372646955022867

Epoch: 6| Step: 4
Training loss: 3.3943076109443653
Validation loss: 2.8384713943843116

Epoch: 6| Step: 5
Training loss: 2.8664195516074407
Validation loss: 2.8364914331900026

Epoch: 6| Step: 6
Training loss: 3.4588829124729368
Validation loss: 2.841449627953759

Epoch: 6| Step: 7
Training loss: 3.469238521782627
Validation loss: 2.8402303690435033

Epoch: 6| Step: 8
Training loss: 2.7956102804320513
Validation loss: 2.8427363025379946

Epoch: 6| Step: 9
Training loss: 2.4658842233363614
Validation loss: 2.837065009711326

Epoch: 6| Step: 10
Training loss: 3.1342444441162582
Validation loss: 2.8393666407027003

Epoch: 6| Step: 11
Training loss: 3.1177138264245814
Validation loss: 2.8374454018708244

Epoch: 6| Step: 12
Training loss: 3.003898789256421
Validation loss: 2.839016853350373

Epoch: 6| Step: 13
Training loss: 3.937447926010444
Validation loss: 2.8399034602088946

Epoch: 172| Step: 0
Training loss: 2.9627792548696466
Validation loss: 2.8403140684009816

Epoch: 6| Step: 1
Training loss: 3.4621246575137987
Validation loss: 2.8372874362247322

Epoch: 6| Step: 2
Training loss: 2.689900147098603
Validation loss: 2.83410589089736

Epoch: 6| Step: 3
Training loss: 3.1728426955766373
Validation loss: 2.837897015370819

Epoch: 6| Step: 4
Training loss: 3.494415460037434
Validation loss: 2.838265947714955

Epoch: 6| Step: 5
Training loss: 3.2836408351705297
Validation loss: 2.8357051793868737

Epoch: 6| Step: 6
Training loss: 3.298892014349495
Validation loss: 2.8409948009489843

Epoch: 6| Step: 7
Training loss: 3.174645327484988
Validation loss: 2.838350740988406

Epoch: 6| Step: 8
Training loss: 3.4613711944335437
Validation loss: 2.8441334488106165

Epoch: 6| Step: 9
Training loss: 2.8490927674389868
Validation loss: 2.845146532246924

Epoch: 6| Step: 10
Training loss: 1.9125316492590978
Validation loss: 2.8465980197382277

Epoch: 6| Step: 11
Training loss: 3.361733961237155
Validation loss: 2.8510006820754956

Epoch: 6| Step: 12
Training loss: 3.346020792028351
Validation loss: 2.8509344331069033

Epoch: 6| Step: 13
Training loss: 2.94979703172631
Validation loss: 2.838063227709556

Epoch: 173| Step: 0
Training loss: 2.819268791686133
Validation loss: 2.836820561601715

Epoch: 6| Step: 1
Training loss: 3.6580790321985317
Validation loss: 2.832117655634335

Epoch: 6| Step: 2
Training loss: 3.2809078946473305
Validation loss: 2.8327915794477394

Epoch: 6| Step: 3
Training loss: 3.5464415432414653
Validation loss: 2.8385752342988675

Epoch: 6| Step: 4
Training loss: 2.792004996527543
Validation loss: 2.834335185009866

Epoch: 6| Step: 5
Training loss: 3.114250922352444
Validation loss: 2.832692447205336

Epoch: 6| Step: 6
Training loss: 2.4837894349406517
Validation loss: 2.8403290306051105

Epoch: 6| Step: 7
Training loss: 2.812143091232017
Validation loss: 2.8378313819459287

Epoch: 6| Step: 8
Training loss: 3.7719747737150207
Validation loss: 2.8406983320677193

Epoch: 6| Step: 9
Training loss: 2.968307783419188
Validation loss: 2.8307629446734

Epoch: 6| Step: 10
Training loss: 2.759294063292669
Validation loss: 2.831249780837579

Epoch: 6| Step: 11
Training loss: 3.0066494365065926
Validation loss: 2.8354165063549233

Epoch: 6| Step: 12
Training loss: 3.3854095145908025
Validation loss: 2.8351395264947326

Epoch: 6| Step: 13
Training loss: 3.1709856562342655
Validation loss: 2.831676712446603

Epoch: 174| Step: 0
Training loss: 3.1253195027098517
Validation loss: 2.8295924616687085

Epoch: 6| Step: 1
Training loss: 3.560591470896321
Validation loss: 2.8340977506809746

Epoch: 6| Step: 2
Training loss: 3.346202058217122
Validation loss: 2.8310581865144617

Epoch: 6| Step: 3
Training loss: 2.6525179506605907
Validation loss: 2.8332397227111614

Epoch: 6| Step: 4
Training loss: 3.2591099339613425
Validation loss: 2.830410741778867

Epoch: 6| Step: 5
Training loss: 2.923455448391172
Validation loss: 2.8318862446800384

Epoch: 6| Step: 6
Training loss: 2.8387208841192137
Validation loss: 2.832050989384188

Epoch: 6| Step: 7
Training loss: 3.189004580370346
Validation loss: 2.829200430924762

Epoch: 6| Step: 8
Training loss: 3.3171725157346743
Validation loss: 2.832822893648058

Epoch: 6| Step: 9
Training loss: 2.780535145304749
Validation loss: 2.8277468952358666

Epoch: 6| Step: 10
Training loss: 3.719639134900626
Validation loss: 2.8319364535965748

Epoch: 6| Step: 11
Training loss: 2.843668339154183
Validation loss: 2.833367463026357

Epoch: 6| Step: 12
Training loss: 3.2361639852574746
Validation loss: 2.827653494661654

Epoch: 6| Step: 13
Training loss: 2.636869932008418
Validation loss: 2.8227584904135616

Epoch: 175| Step: 0
Training loss: 3.289376214440506
Validation loss: 2.831096783146095

Epoch: 6| Step: 1
Training loss: 2.5250667827574884
Validation loss: 2.830383820990231

Epoch: 6| Step: 2
Training loss: 3.125043639832009
Validation loss: 2.8297976764899007

Epoch: 6| Step: 3
Training loss: 3.6078979640666407
Validation loss: 2.8285075524790213

Epoch: 6| Step: 4
Training loss: 2.8805238483105873
Validation loss: 2.8277973081969154

Epoch: 6| Step: 5
Training loss: 3.204159602110722
Validation loss: 2.826388721523673

Epoch: 6| Step: 6
Training loss: 2.8743895836360847
Validation loss: 2.828546398718985

Epoch: 6| Step: 7
Training loss: 2.7319544950747585
Validation loss: 2.8271090968846657

Epoch: 6| Step: 8
Training loss: 2.7212561971500118
Validation loss: 2.8256519051134594

Epoch: 6| Step: 9
Training loss: 3.283654340229927
Validation loss: 2.827232422586217

Epoch: 6| Step: 10
Training loss: 3.367643462638983
Validation loss: 2.829267045641817

Epoch: 6| Step: 11
Training loss: 3.734027543124188
Validation loss: 2.8269442573448442

Epoch: 6| Step: 12
Training loss: 2.7196867633421027
Validation loss: 2.827960224209027

Epoch: 6| Step: 13
Training loss: 3.7694912765043744
Validation loss: 2.8308754532118576

Epoch: 176| Step: 0
Training loss: 4.031914707946859
Validation loss: 2.841360712714347

Epoch: 6| Step: 1
Training loss: 3.300404038403696
Validation loss: 2.838215623148153

Epoch: 6| Step: 2
Training loss: 3.4750987196697296
Validation loss: 2.8398879893333873

Epoch: 6| Step: 3
Training loss: 3.626499129123391
Validation loss: 2.839218915648082

Epoch: 6| Step: 4
Training loss: 2.5041831305328124
Validation loss: 2.8357404446439314

Epoch: 6| Step: 5
Training loss: 2.5330134712701367
Validation loss: 2.839694102453096

Epoch: 6| Step: 6
Training loss: 3.390556932460486
Validation loss: 2.832980577947703

Epoch: 6| Step: 7
Training loss: 2.1536365095226553
Validation loss: 2.829664436949794

Epoch: 6| Step: 8
Training loss: 3.076846559196661
Validation loss: 2.8273512382836476

Epoch: 6| Step: 9
Training loss: 3.255936409462848
Validation loss: 2.829779464221947

Epoch: 6| Step: 10
Training loss: 3.170799185677187
Validation loss: 2.826975698888114

Epoch: 6| Step: 11
Training loss: 3.2721133956846224
Validation loss: 2.828093641940449

Epoch: 6| Step: 12
Training loss: 2.802660706512231
Validation loss: 2.827868572188579

Epoch: 6| Step: 13
Training loss: 2.2067072537695416
Validation loss: 2.8267339851318694

Epoch: 177| Step: 0
Training loss: 3.233599141262267
Validation loss: 2.8237748656527435

Epoch: 6| Step: 1
Training loss: 3.3161001255118085
Validation loss: 2.82538949022163

Epoch: 6| Step: 2
Training loss: 2.890226759412693
Validation loss: 2.827079123146874

Epoch: 6| Step: 3
Training loss: 3.0399568707769657
Validation loss: 2.827694396153786

Epoch: 6| Step: 4
Training loss: 2.5767599480125027
Validation loss: 2.8290398789878917

Epoch: 6| Step: 5
Training loss: 3.5828300093012113
Validation loss: 2.82858710693951

Epoch: 6| Step: 6
Training loss: 4.23283284950927
Validation loss: 2.834518623224869

Epoch: 6| Step: 7
Training loss: 3.2175178021084463
Validation loss: 2.8358083774848377

Epoch: 6| Step: 8
Training loss: 3.0860317553841305
Validation loss: 2.829400878345157

Epoch: 6| Step: 9
Training loss: 2.770848228777857
Validation loss: 2.825312380952795

Epoch: 6| Step: 10
Training loss: 2.8049273800076713
Validation loss: 2.825694716329536

Epoch: 6| Step: 11
Training loss: 2.8720432502018967
Validation loss: 2.8232451531846143

Epoch: 6| Step: 12
Training loss: 3.2727804685373045
Validation loss: 2.8232407673118987

Epoch: 6| Step: 13
Training loss: 1.9655533527361684
Validation loss: 2.8218878534823046

Epoch: 178| Step: 0
Training loss: 2.4570314440704415
Validation loss: 2.824091982039903

Epoch: 6| Step: 1
Training loss: 3.117588409418336
Validation loss: 2.8253512230330795

Epoch: 6| Step: 2
Training loss: 3.282895211617176
Validation loss: 2.82374413566026

Epoch: 6| Step: 3
Training loss: 3.963332796880131
Validation loss: 2.821568393263431

Epoch: 6| Step: 4
Training loss: 2.945018318520844
Validation loss: 2.8219224374483423

Epoch: 6| Step: 5
Training loss: 2.8286670144349344
Validation loss: 2.823444460754424

Epoch: 6| Step: 6
Training loss: 3.62912279022941
Validation loss: 2.822936190378079

Epoch: 6| Step: 7
Training loss: 3.511712368120244
Validation loss: 2.828069330661394

Epoch: 6| Step: 8
Training loss: 3.2552298501893038
Validation loss: 2.827274402859357

Epoch: 6| Step: 9
Training loss: 3.6622031238000305
Validation loss: 2.8257018165397216

Epoch: 6| Step: 10
Training loss: 2.322023114374395
Validation loss: 2.82523957431893

Epoch: 6| Step: 11
Training loss: 2.6430593251250283
Validation loss: 2.8279350197462447

Epoch: 6| Step: 12
Training loss: 2.5090507706408256
Validation loss: 2.8230777115290846

Epoch: 6| Step: 13
Training loss: 3.1050298800543406
Validation loss: 2.8260912247692525

Epoch: 179| Step: 0
Training loss: 3.329252669871085
Validation loss: 2.8218176758362525

Epoch: 6| Step: 1
Training loss: 2.948994002715461
Validation loss: 2.824507659834165

Epoch: 6| Step: 2
Training loss: 2.8986832753570995
Validation loss: 2.8228304592162257

Epoch: 6| Step: 3
Training loss: 3.4584053369096623
Validation loss: 2.8238182845429263

Epoch: 6| Step: 4
Training loss: 2.9342393025150195
Validation loss: 2.8241811205256924

Epoch: 6| Step: 5
Training loss: 2.9227509027208374
Validation loss: 2.828537564554445

Epoch: 6| Step: 6
Training loss: 2.793153983422658
Validation loss: 2.8319842627237373

Epoch: 6| Step: 7
Training loss: 2.959405749442199
Validation loss: 2.8282525471119726

Epoch: 6| Step: 8
Training loss: 3.231292435092563
Validation loss: 2.8354359635870656

Epoch: 6| Step: 9
Training loss: 3.3534187920553062
Validation loss: 2.827115653994173

Epoch: 6| Step: 10
Training loss: 3.290016887586668
Validation loss: 2.8292097967319556

Epoch: 6| Step: 11
Training loss: 3.2223073319635382
Validation loss: 2.8342534518978186

Epoch: 6| Step: 12
Training loss: 3.232776781403843
Validation loss: 2.822661501638997

Epoch: 6| Step: 13
Training loss: 2.9996309053344055
Validation loss: 2.819695005247307

Epoch: 180| Step: 0
Training loss: 3.4976235905175423
Validation loss: 2.821301843922498

Epoch: 6| Step: 1
Training loss: 3.217194264549788
Validation loss: 2.8252210522775925

Epoch: 6| Step: 2
Training loss: 2.311603475457368
Validation loss: 2.8180629173588896

Epoch: 6| Step: 3
Training loss: 3.0860621946227185
Validation loss: 2.8168708664145234

Epoch: 6| Step: 4
Training loss: 3.334157428596765
Validation loss: 2.818827976743231

Epoch: 6| Step: 5
Training loss: 2.8687430587385387
Validation loss: 2.8200345775489035

Epoch: 6| Step: 6
Training loss: 3.0678001782971394
Validation loss: 2.815725879742878

Epoch: 6| Step: 7
Training loss: 2.6886346772599827
Validation loss: 2.8173946616736396

Epoch: 6| Step: 8
Training loss: 3.161994465465395
Validation loss: 2.8217321640606006

Epoch: 6| Step: 9
Training loss: 3.918164568768349
Validation loss: 2.818682907932569

Epoch: 6| Step: 10
Training loss: 3.2497256236532217
Validation loss: 2.814134281104289

Epoch: 6| Step: 11
Training loss: 2.345114348984811
Validation loss: 2.8153407509029185

Epoch: 6| Step: 12
Training loss: 3.2667256375913154
Validation loss: 2.816500916482331

Epoch: 6| Step: 13
Training loss: 3.4014224218527054
Validation loss: 2.812447699421718

Epoch: 181| Step: 0
Training loss: 2.849808328440721
Validation loss: 2.816109302720726

Epoch: 6| Step: 1
Training loss: 2.944742567536677
Validation loss: 2.8162600523830696

Epoch: 6| Step: 2
Training loss: 2.720481759490981
Validation loss: 2.8162673092827997

Epoch: 6| Step: 3
Training loss: 3.3899982769716996
Validation loss: 2.8178826820393024

Epoch: 6| Step: 4
Training loss: 3.2043692800471915
Validation loss: 2.8205000778329614

Epoch: 6| Step: 5
Training loss: 3.5524649913762154
Validation loss: 2.8147108077842597

Epoch: 6| Step: 6
Training loss: 2.894739305686815
Validation loss: 2.8194805062604784

Epoch: 6| Step: 7
Training loss: 2.9978690526458163
Validation loss: 2.812635386135527

Epoch: 6| Step: 8
Training loss: 3.3695691530009393
Validation loss: 2.815458321080814

Epoch: 6| Step: 9
Training loss: 2.9117451880759075
Validation loss: 2.815777754339199

Epoch: 6| Step: 10
Training loss: 3.120657688659562
Validation loss: 2.8172131569328727

Epoch: 6| Step: 11
Training loss: 3.377491631712807
Validation loss: 2.8147005995112577

Epoch: 6| Step: 12
Training loss: 3.191125640298867
Validation loss: 2.8119818079034133

Epoch: 6| Step: 13
Training loss: 2.9487621231767798
Validation loss: 2.825805737405944

Epoch: 182| Step: 0
Training loss: 3.2854874633308375
Validation loss: 2.820623178067081

Epoch: 6| Step: 1
Training loss: 3.0996032214809746
Validation loss: 2.8265645888145046

Epoch: 6| Step: 2
Training loss: 2.956145180261319
Validation loss: 2.8183928652500216

Epoch: 6| Step: 3
Training loss: 2.491247878927887
Validation loss: 2.8195995313279694

Epoch: 6| Step: 4
Training loss: 3.0661280414697316
Validation loss: 2.8230490735001634

Epoch: 6| Step: 5
Training loss: 2.8426992969548706
Validation loss: 2.8256264115050467

Epoch: 6| Step: 6
Training loss: 2.9190892285845726
Validation loss: 2.8170647424745487

Epoch: 6| Step: 7
Training loss: 2.9232704787708617
Validation loss: 2.8149333394819265

Epoch: 6| Step: 8
Training loss: 3.2274576566416853
Validation loss: 2.8177083354562744

Epoch: 6| Step: 9
Training loss: 3.399408311408881
Validation loss: 2.816757363450576

Epoch: 6| Step: 10
Training loss: 3.533130516373198
Validation loss: 2.8122412294777686

Epoch: 6| Step: 11
Training loss: 3.399762897077921
Validation loss: 2.8120047603823553

Epoch: 6| Step: 12
Training loss: 3.3473693653510694
Validation loss: 2.8129966689249755

Epoch: 6| Step: 13
Training loss: 2.873117245059349
Validation loss: 2.814809188937298

Epoch: 183| Step: 0
Training loss: 3.582045893082802
Validation loss: 2.8144321044488487

Epoch: 6| Step: 1
Training loss: 3.2707023027419693
Validation loss: 2.8187465206679834

Epoch: 6| Step: 2
Training loss: 3.3406767224696097
Validation loss: 2.8156693315566415

Epoch: 6| Step: 3
Training loss: 2.686309195780879
Validation loss: 2.8139413838608283

Epoch: 6| Step: 4
Training loss: 3.240764995262168
Validation loss: 2.813833675674199

Epoch: 6| Step: 5
Training loss: 2.846151323178575
Validation loss: 2.8175919036954378

Epoch: 6| Step: 6
Training loss: 2.9274100848689915
Validation loss: 2.8355492857487308

Epoch: 6| Step: 7
Training loss: 3.4071866768677923
Validation loss: 2.8409638628783687

Epoch: 6| Step: 8
Training loss: 2.999519945518691
Validation loss: 2.8495271983194046

Epoch: 6| Step: 9
Training loss: 3.210403138036689
Validation loss: 2.8559150331900853

Epoch: 6| Step: 10
Training loss: 3.176662946727559
Validation loss: 2.8454407654704874

Epoch: 6| Step: 11
Training loss: 2.6511540617034535
Validation loss: 2.8254829302501

Epoch: 6| Step: 12
Training loss: 3.1357729045132077
Validation loss: 2.821476439116483

Epoch: 6| Step: 13
Training loss: 2.923706622966007
Validation loss: 2.8070001275948995

Epoch: 184| Step: 0
Training loss: 2.61388149695095
Validation loss: 2.818493983051332

Epoch: 6| Step: 1
Training loss: 3.2060111092549284
Validation loss: 2.8162381368329084

Epoch: 6| Step: 2
Training loss: 3.305283945787453
Validation loss: 2.8078099312200657

Epoch: 6| Step: 3
Training loss: 3.0669663227571897
Validation loss: 2.815431939334164

Epoch: 6| Step: 4
Training loss: 3.2258237865966657
Validation loss: 2.819589489867372

Epoch: 6| Step: 5
Training loss: 3.0287833648619786
Validation loss: 2.8161895551622975

Epoch: 6| Step: 6
Training loss: 3.1446166222397762
Validation loss: 2.816501722938762

Epoch: 6| Step: 7
Training loss: 3.521857810465066
Validation loss: 2.8136012656956715

Epoch: 6| Step: 8
Training loss: 2.9395342337231707
Validation loss: 2.812509622865027

Epoch: 6| Step: 9
Training loss: 2.9631910781557136
Validation loss: 2.810867369855759

Epoch: 6| Step: 10
Training loss: 3.005082275990731
Validation loss: 2.8112968213842007

Epoch: 6| Step: 11
Training loss: 3.1128800922243847
Validation loss: 2.815315924278843

Epoch: 6| Step: 12
Training loss: 2.8537102958600675
Validation loss: 2.814899023925345

Epoch: 6| Step: 13
Training loss: 3.7971973674797592
Validation loss: 2.819905987797729

Epoch: 185| Step: 0
Training loss: 3.6381539402583805
Validation loss: 2.82030041765653

Epoch: 6| Step: 1
Training loss: 3.3022906762779565
Validation loss: 2.818582966589876

Epoch: 6| Step: 2
Training loss: 2.9662973009687303
Validation loss: 2.826974728559309

Epoch: 6| Step: 3
Training loss: 2.6976576181204446
Validation loss: 2.822008010229127

Epoch: 6| Step: 4
Training loss: 3.4214815932618703
Validation loss: 2.812746384925148

Epoch: 6| Step: 5
Training loss: 2.8854062370543305
Validation loss: 2.812138100040431

Epoch: 6| Step: 6
Training loss: 3.492516009349352
Validation loss: 2.80898061159955

Epoch: 6| Step: 7
Training loss: 3.5735500356337164
Validation loss: 2.809090270879628

Epoch: 6| Step: 8
Training loss: 2.769814459305361
Validation loss: 2.809337202144537

Epoch: 6| Step: 9
Training loss: 2.6539071914900267
Validation loss: 2.8097046782881

Epoch: 6| Step: 10
Training loss: 2.9472834218109134
Validation loss: 2.8065057210965523

Epoch: 6| Step: 11
Training loss: 2.872456130048843
Validation loss: 2.8064652018453216

Epoch: 6| Step: 12
Training loss: 3.030148964924627
Validation loss: 2.8094091889916855

Epoch: 6| Step: 13
Training loss: 3.2005667542199108
Validation loss: 2.8059447988887674

Epoch: 186| Step: 0
Training loss: 1.841736421371659
Validation loss: 2.805773827645965

Epoch: 6| Step: 1
Training loss: 3.3918557460358634
Validation loss: 2.807859647592415

Epoch: 6| Step: 2
Training loss: 3.6086817484348512
Validation loss: 2.8060526865464186

Epoch: 6| Step: 3
Training loss: 3.3865343019445513
Validation loss: 2.830446121910503

Epoch: 6| Step: 4
Training loss: 2.922710442059733
Validation loss: 2.8351906787717605

Epoch: 6| Step: 5
Training loss: 3.3567476155869294
Validation loss: 2.8442382780952915

Epoch: 6| Step: 6
Training loss: 3.132753138562445
Validation loss: 2.838899028884574

Epoch: 6| Step: 7
Training loss: 2.756464209850283
Validation loss: 2.843307478716403

Epoch: 6| Step: 8
Training loss: 3.209544972763264
Validation loss: 2.843742678007544

Epoch: 6| Step: 9
Training loss: 2.574492143344504
Validation loss: 2.84568388391286

Epoch: 6| Step: 10
Training loss: 3.5360338333634362
Validation loss: 2.8395283909532587

Epoch: 6| Step: 11
Training loss: 3.5051755785533505
Validation loss: 2.824662726143959

Epoch: 6| Step: 12
Training loss: 3.381659965552366
Validation loss: 2.802144484320252

Epoch: 6| Step: 13
Training loss: 2.116469595838755
Validation loss: 2.80358267348382

Epoch: 187| Step: 0
Training loss: 3.220962208632457
Validation loss: 2.8059970041435807

Epoch: 6| Step: 1
Training loss: 3.4113784120627964
Validation loss: 2.80169701847597

Epoch: 6| Step: 2
Training loss: 3.3041424673829036
Validation loss: 2.802650114072486

Epoch: 6| Step: 3
Training loss: 2.7663921434483956
Validation loss: 2.8040597476885996

Epoch: 6| Step: 4
Training loss: 2.952734542949625
Validation loss: 2.802539406045905

Epoch: 6| Step: 5
Training loss: 3.2848308098129926
Validation loss: 2.801011613664551

Epoch: 6| Step: 6
Training loss: 2.9404613096626604
Validation loss: 2.8041271543794664

Epoch: 6| Step: 7
Training loss: 2.9449974316519403
Validation loss: 2.8029381208784194

Epoch: 6| Step: 8
Training loss: 2.6999507193129935
Validation loss: 2.800844732030698

Epoch: 6| Step: 9
Training loss: 3.1072435597604593
Validation loss: 2.803356306170112

Epoch: 6| Step: 10
Training loss: 3.1546422810594157
Validation loss: 2.8034602744119628

Epoch: 6| Step: 11
Training loss: 3.381618086263979
Validation loss: 2.803522961488221

Epoch: 6| Step: 12
Training loss: 2.9587932461993494
Validation loss: 2.807597802279384

Epoch: 6| Step: 13
Training loss: 3.446388682057632
Validation loss: 2.8124661533593334

Epoch: 188| Step: 0
Training loss: 4.234896131590635
Validation loss: 2.8117809648088308

Epoch: 6| Step: 1
Training loss: 3.7325823323699
Validation loss: 2.8138202517211233

Epoch: 6| Step: 2
Training loss: 3.214029943795524
Validation loss: 2.8071416475783644

Epoch: 6| Step: 3
Training loss: 3.0300935721977984
Validation loss: 2.8048227495629594

Epoch: 6| Step: 4
Training loss: 3.0815447869414103
Validation loss: 2.800824934725036

Epoch: 6| Step: 5
Training loss: 2.2743533672196046
Validation loss: 2.8053072088412376

Epoch: 6| Step: 6
Training loss: 3.0285355516966654
Validation loss: 2.805375891423889

Epoch: 6| Step: 7
Training loss: 3.097226896030642
Validation loss: 2.799977635588743

Epoch: 6| Step: 8
Training loss: 3.3490760667829917
Validation loss: 2.8011917493293335

Epoch: 6| Step: 9
Training loss: 2.7690599737316197
Validation loss: 2.803606412535069

Epoch: 6| Step: 10
Training loss: 2.8573731465855357
Validation loss: 2.8016977651414607

Epoch: 6| Step: 11
Training loss: 2.821037896616766
Validation loss: 2.8051503451072946

Epoch: 6| Step: 12
Training loss: 2.491071780156405
Validation loss: 2.800427241844649

Epoch: 6| Step: 13
Training loss: 3.2636785958002315
Validation loss: 2.7988071986871654

Epoch: 189| Step: 0
Training loss: 3.001646861410455
Validation loss: 2.802903530416817

Epoch: 6| Step: 1
Training loss: 2.752490216618058
Validation loss: 2.7975591667288047

Epoch: 6| Step: 2
Training loss: 3.189146327291208
Validation loss: 2.7974230252935905

Epoch: 6| Step: 3
Training loss: 2.7561273802731803
Validation loss: 2.79870349635837

Epoch: 6| Step: 4
Training loss: 3.55819039932473
Validation loss: 2.8009327088541345

Epoch: 6| Step: 5
Training loss: 1.943061537419454
Validation loss: 2.7999531122316252

Epoch: 6| Step: 6
Training loss: 3.488376116343699
Validation loss: 2.8002933033616304

Epoch: 6| Step: 7
Training loss: 3.0826763449762247
Validation loss: 2.802755671708912

Epoch: 6| Step: 8
Training loss: 3.5368343522290413
Validation loss: 2.805082442979475

Epoch: 6| Step: 9
Training loss: 3.4532536961665428
Validation loss: 2.798879122958035

Epoch: 6| Step: 10
Training loss: 3.5266384294048874
Validation loss: 2.8019513801575484

Epoch: 6| Step: 11
Training loss: 2.9820720315033
Validation loss: 2.7986967398598064

Epoch: 6| Step: 12
Training loss: 3.4108312753658265
Validation loss: 2.7996468689756844

Epoch: 6| Step: 13
Training loss: 1.4244131552995276
Validation loss: 2.803813260494058

Epoch: 190| Step: 0
Training loss: 3.682144672889679
Validation loss: 2.804975442018988

Epoch: 6| Step: 1
Training loss: 3.1439370677949143
Validation loss: 2.802474824420316

Epoch: 6| Step: 2
Training loss: 3.6126147506070603
Validation loss: 2.8030027366452352

Epoch: 6| Step: 3
Training loss: 2.748946681738172
Validation loss: 2.805813129340527

Epoch: 6| Step: 4
Training loss: 3.4127733952482773
Validation loss: 2.8056965365449584

Epoch: 6| Step: 5
Training loss: 2.3399689757091853
Validation loss: 2.8066591006780035

Epoch: 6| Step: 6
Training loss: 3.089734177991598
Validation loss: 2.8073952054748172

Epoch: 6| Step: 7
Training loss: 3.2330438943155104
Validation loss: 2.8115290315916024

Epoch: 6| Step: 8
Training loss: 3.5390751651090984
Validation loss: 2.805841296404111

Epoch: 6| Step: 9
Training loss: 2.872065829805665
Validation loss: 2.8083059434518756

Epoch: 6| Step: 10
Training loss: 3.2844522018868676
Validation loss: 2.809233108275864

Epoch: 6| Step: 11
Training loss: 2.1951794567044973
Validation loss: 2.8105972434776296

Epoch: 6| Step: 12
Training loss: 3.0044941618277496
Validation loss: 2.802277206323857

Epoch: 6| Step: 13
Training loss: 2.840137744571923
Validation loss: 2.7969975149343282

Epoch: 191| Step: 0
Training loss: 3.558144567219054
Validation loss: 2.7969253057961696

Epoch: 6| Step: 1
Training loss: 2.916295236824488
Validation loss: 2.795580449494814

Epoch: 6| Step: 2
Training loss: 2.984531738000016
Validation loss: 2.7965990641814207

Epoch: 6| Step: 3
Training loss: 3.477227650994513
Validation loss: 2.7958376453567464

Epoch: 6| Step: 4
Training loss: 2.939152678789974
Validation loss: 2.7948218664172177

Epoch: 6| Step: 5
Training loss: 3.0215702254287033
Validation loss: 2.794115562441997

Epoch: 6| Step: 6
Training loss: 2.980070835014104
Validation loss: 2.7929268406222696

Epoch: 6| Step: 7
Training loss: 3.245492156432269
Validation loss: 2.7977665405835985

Epoch: 6| Step: 8
Training loss: 2.750017512872592
Validation loss: 2.7965781523569198

Epoch: 6| Step: 9
Training loss: 2.9155979560014855
Validation loss: 2.796412831417685

Epoch: 6| Step: 10
Training loss: 3.006731269999478
Validation loss: 2.793153362051651

Epoch: 6| Step: 11
Training loss: 3.124460402632299
Validation loss: 2.7970250677730966

Epoch: 6| Step: 12
Training loss: 3.351622965518455
Validation loss: 2.8018916077778453

Epoch: 6| Step: 13
Training loss: 3.1484693047594883
Validation loss: 2.7978817027483167

Epoch: 192| Step: 0
Training loss: 3.018587070909681
Validation loss: 2.797902482049452

Epoch: 6| Step: 1
Training loss: 2.5670646391971523
Validation loss: 2.8024011911904436

Epoch: 6| Step: 2
Training loss: 2.7270886807040795
Validation loss: 2.808589290131751

Epoch: 6| Step: 3
Training loss: 3.405823007121034
Validation loss: 2.800868602293292

Epoch: 6| Step: 4
Training loss: 2.7063603085772834
Validation loss: 2.798779700948137

Epoch: 6| Step: 5
Training loss: 3.42904928260647
Validation loss: 2.8040612388468276

Epoch: 6| Step: 6
Training loss: 3.5371286527549626
Validation loss: 2.794139503954311

Epoch: 6| Step: 7
Training loss: 3.4035762827430753
Validation loss: 2.8043073340023597

Epoch: 6| Step: 8
Training loss: 2.7992571696892883
Validation loss: 2.806762834231871

Epoch: 6| Step: 9
Training loss: 2.6961188395594857
Validation loss: 2.8048567268257907

Epoch: 6| Step: 10
Training loss: 3.39521528053045
Validation loss: 2.798145940592801

Epoch: 6| Step: 11
Training loss: 3.1529628141069335
Validation loss: 2.797273918260411

Epoch: 6| Step: 12
Training loss: 3.3315804322993436
Validation loss: 2.795142716101319

Epoch: 6| Step: 13
Training loss: 2.9626112261144915
Validation loss: 2.7916273911097433

Epoch: 193| Step: 0
Training loss: 3.0656902279073024
Validation loss: 2.7930903899264914

Epoch: 6| Step: 1
Training loss: 2.9162031804531483
Validation loss: 2.7921317843663958

Epoch: 6| Step: 2
Training loss: 3.443809544763332
Validation loss: 2.7995230341014365

Epoch: 6| Step: 3
Training loss: 2.684989022315971
Validation loss: 2.793201123693606

Epoch: 6| Step: 4
Training loss: 2.6269235375308684
Validation loss: 2.8036264736643437

Epoch: 6| Step: 5
Training loss: 3.2060001030515584
Validation loss: 2.797140436437506

Epoch: 6| Step: 6
Training loss: 3.8018757105884204
Validation loss: 2.7931184521601122

Epoch: 6| Step: 7
Training loss: 3.073204173630881
Validation loss: 2.7962226695864123

Epoch: 6| Step: 8
Training loss: 3.2687141372726454
Validation loss: 2.7966688287177526

Epoch: 6| Step: 9
Training loss: 3.725576850210084
Validation loss: 2.7912812163545513

Epoch: 6| Step: 10
Training loss: 2.4549606161590636
Validation loss: 2.790584729837936

Epoch: 6| Step: 11
Training loss: 3.1529572184207333
Validation loss: 2.7919383898608237

Epoch: 6| Step: 12
Training loss: 2.9468807022185186
Validation loss: 2.7884315114066407

Epoch: 6| Step: 13
Training loss: 2.308022675341678
Validation loss: 2.7975600757823567

Epoch: 194| Step: 0
Training loss: 2.7492123256043506
Validation loss: 2.795107766852904

Epoch: 6| Step: 1
Training loss: 3.817288627643427
Validation loss: 2.803567230785197

Epoch: 6| Step: 2
Training loss: 3.23866643328906
Validation loss: 2.807310101836315

Epoch: 6| Step: 3
Training loss: 2.874361381921528
Validation loss: 2.8083669303412404

Epoch: 6| Step: 4
Training loss: 3.627462405766464
Validation loss: 2.813438024842563

Epoch: 6| Step: 5
Training loss: 3.0822175215019323
Validation loss: 2.8007059081292605

Epoch: 6| Step: 6
Training loss: 3.674122613504473
Validation loss: 2.8018600704897936

Epoch: 6| Step: 7
Training loss: 3.321381376666579
Validation loss: 2.7920288330990712

Epoch: 6| Step: 8
Training loss: 2.8058680013114823
Validation loss: 2.7911716711690366

Epoch: 6| Step: 9
Training loss: 2.9501933729511998
Validation loss: 2.790160701515176

Epoch: 6| Step: 10
Training loss: 2.1388417633722034
Validation loss: 2.7872884417629424

Epoch: 6| Step: 11
Training loss: 3.215483554290869
Validation loss: 2.7883711678732053

Epoch: 6| Step: 12
Training loss: 2.615666553891891
Validation loss: 2.789604278012599

Epoch: 6| Step: 13
Training loss: 2.7988277842594003
Validation loss: 2.7892438035050855

Epoch: 195| Step: 0
Training loss: 3.7499716439764503
Validation loss: 2.7837709765752714

Epoch: 6| Step: 1
Training loss: 2.423277319724723
Validation loss: 2.7832394635358253

Epoch: 6| Step: 2
Training loss: 2.128414552234726
Validation loss: 2.787842954969501

Epoch: 6| Step: 3
Training loss: 3.2877770310791012
Validation loss: 2.7971885790887256

Epoch: 6| Step: 4
Training loss: 3.4481994574402184
Validation loss: 2.8114595728836074

Epoch: 6| Step: 5
Training loss: 3.4556119084916603
Validation loss: 2.8183306699241877

Epoch: 6| Step: 6
Training loss: 2.6195048989899825
Validation loss: 2.8239603660947994

Epoch: 6| Step: 7
Training loss: 3.625179154803104
Validation loss: 2.8420438411577065

Epoch: 6| Step: 8
Training loss: 3.152572150860601
Validation loss: 2.839196967832281

Epoch: 6| Step: 9
Training loss: 3.6052868963567244
Validation loss: 2.819647493265644

Epoch: 6| Step: 10
Training loss: 2.878734319514156
Validation loss: 2.806876170509914

Epoch: 6| Step: 11
Training loss: 1.8928537394449325
Validation loss: 2.79685277764452

Epoch: 6| Step: 12
Training loss: 3.4953829421003744
Validation loss: 2.7867083981933636

Epoch: 6| Step: 13
Training loss: 3.0265948544468033
Validation loss: 2.7892835541011616

Epoch: 196| Step: 0
Training loss: 3.228027011384891
Validation loss: 2.788224893237153

Epoch: 6| Step: 1
Training loss: 3.110086484240129
Validation loss: 2.7888718287064047

Epoch: 6| Step: 2
Training loss: 2.9569135312392185
Validation loss: 2.7859378917875257

Epoch: 6| Step: 3
Training loss: 2.9203105144789934
Validation loss: 2.7893851315846074

Epoch: 6| Step: 4
Training loss: 3.1022015352874077
Validation loss: 2.7862437914987086

Epoch: 6| Step: 5
Training loss: 3.6736883352840706
Validation loss: 2.787371247581743

Epoch: 6| Step: 6
Training loss: 2.6769683654876792
Validation loss: 2.7874631615420107

Epoch: 6| Step: 7
Training loss: 3.0449293918284384
Validation loss: 2.786203411418348

Epoch: 6| Step: 8
Training loss: 2.7185397011989045
Validation loss: 2.786731897372657

Epoch: 6| Step: 9
Training loss: 3.3866286392279656
Validation loss: 2.7925885171795453

Epoch: 6| Step: 10
Training loss: 2.857610773190549
Validation loss: 2.792225449505812

Epoch: 6| Step: 11
Training loss: 2.975811400477345
Validation loss: 2.8030893630445575

Epoch: 6| Step: 12
Training loss: 2.8163498702206193
Validation loss: 2.8050379901309643

Epoch: 6| Step: 13
Training loss: 4.036538846433247
Validation loss: 2.7975838622731826

Epoch: 197| Step: 0
Training loss: 3.0563005252086426
Validation loss: 2.79888014882477

Epoch: 6| Step: 1
Training loss: 2.896934428011824
Validation loss: 2.798249163361785

Epoch: 6| Step: 2
Training loss: 2.2921639220852486
Validation loss: 2.7947912840579425

Epoch: 6| Step: 3
Training loss: 3.737017091919743
Validation loss: 2.789922307616422

Epoch: 6| Step: 4
Training loss: 2.755618597876729
Validation loss: 2.7869914864582266

Epoch: 6| Step: 5
Training loss: 3.2773836153899896
Validation loss: 2.790436173368117

Epoch: 6| Step: 6
Training loss: 3.6032668443933344
Validation loss: 2.7964245439244366

Epoch: 6| Step: 7
Training loss: 2.827050442224344
Validation loss: 2.7993815896079264

Epoch: 6| Step: 8
Training loss: 2.634854936605958
Validation loss: 2.792740590633352

Epoch: 6| Step: 9
Training loss: 3.2473634148728094
Validation loss: 2.785492505255643

Epoch: 6| Step: 10
Training loss: 3.509624327637159
Validation loss: 2.784876990656096

Epoch: 6| Step: 11
Training loss: 2.5973572064124406
Validation loss: 2.783139298604354

Epoch: 6| Step: 12
Training loss: 3.4463893738500477
Validation loss: 2.779837301751508

Epoch: 6| Step: 13
Training loss: 2.98974828674844
Validation loss: 2.780039554422853

Epoch: 198| Step: 0
Training loss: 3.0926007534874578
Validation loss: 2.7802449763652763

Epoch: 6| Step: 1
Training loss: 3.7246845239780537
Validation loss: 2.780143221691304

Epoch: 6| Step: 2
Training loss: 2.8246283227609195
Validation loss: 2.779631730704821

Epoch: 6| Step: 3
Training loss: 3.6373669367307273
Validation loss: 2.7794142118142897

Epoch: 6| Step: 4
Training loss: 2.9649749847437583
Validation loss: 2.7785250846677645

Epoch: 6| Step: 5
Training loss: 3.4825260747958944
Validation loss: 2.7833206941203303

Epoch: 6| Step: 6
Training loss: 2.6796412338844884
Validation loss: 2.779169192602572

Epoch: 6| Step: 7
Training loss: 2.3424758499512817
Validation loss: 2.7794165066630536

Epoch: 6| Step: 8
Training loss: 2.87497163841522
Validation loss: 2.7805163826348944

Epoch: 6| Step: 9
Training loss: 3.2754465279218636
Validation loss: 2.7823780441155157

Epoch: 6| Step: 10
Training loss: 2.674842480451335
Validation loss: 2.7802806444850994

Epoch: 6| Step: 11
Training loss: 2.9563753516818085
Validation loss: 2.7825512590525676

Epoch: 6| Step: 12
Training loss: 3.294433847045436
Validation loss: 2.780591053622214

Epoch: 6| Step: 13
Training loss: 3.1935478131519646
Validation loss: 2.790404327456973

Epoch: 199| Step: 0
Training loss: 3.4301807509416844
Validation loss: 2.789829928834782

Epoch: 6| Step: 1
Training loss: 3.0230936648401228
Validation loss: 2.788663024871293

Epoch: 6| Step: 2
Training loss: 3.1214344282740667
Validation loss: 2.7833752385885857

Epoch: 6| Step: 3
Training loss: 2.847114333509663
Validation loss: 2.7781464554241606

Epoch: 6| Step: 4
Training loss: 3.845267019735296
Validation loss: 2.783861998263632

Epoch: 6| Step: 5
Training loss: 2.8087182793313654
Validation loss: 2.7817846440862826

Epoch: 6| Step: 6
Training loss: 3.37670092636558
Validation loss: 2.783628471038031

Epoch: 6| Step: 7
Training loss: 3.163764694704243
Validation loss: 2.7768823328547514

Epoch: 6| Step: 8
Training loss: 3.1405263097639122
Validation loss: 2.7846367901597304

Epoch: 6| Step: 9
Training loss: 2.7428850552864352
Validation loss: 2.7779103214335272

Epoch: 6| Step: 10
Training loss: 2.465455573271775
Validation loss: 2.7801001027739756

Epoch: 6| Step: 11
Training loss: 2.9946992456300756
Validation loss: 2.782393559269681

Epoch: 6| Step: 12
Training loss: 3.177413539715623
Validation loss: 2.784814812716793

Epoch: 6| Step: 13
Training loss: 2.7228388239370536
Validation loss: 2.784910730741454

Epoch: 200| Step: 0
Training loss: 3.0576728613483763
Validation loss: 2.781264246661817

Epoch: 6| Step: 1
Training loss: 3.292478115324774
Validation loss: 2.7809133792593586

Epoch: 6| Step: 2
Training loss: 3.1761893260074334
Validation loss: 2.7780280167312603

Epoch: 6| Step: 3
Training loss: 3.0839064469255018
Validation loss: 2.778796405212577

Epoch: 6| Step: 4
Training loss: 3.483457709712642
Validation loss: 2.7746677081110125

Epoch: 6| Step: 5
Training loss: 3.3500358465753535
Validation loss: 2.774995225471573

Epoch: 6| Step: 6
Training loss: 2.0945362493675184
Validation loss: 2.775200750134187

Epoch: 6| Step: 7
Training loss: 3.54846155921913
Validation loss: 2.7739563371870952

Epoch: 6| Step: 8
Training loss: 3.3587597660651016
Validation loss: 2.776715933433049

Epoch: 6| Step: 9
Training loss: 2.3529943624964735
Validation loss: 2.7740660206402468

Epoch: 6| Step: 10
Training loss: 3.003192315751388
Validation loss: 2.781079303535632

Epoch: 6| Step: 11
Training loss: 2.9725155690573892
Validation loss: 2.776216667015707

Epoch: 6| Step: 12
Training loss: 3.305283368726955
Validation loss: 2.776565208412732

Epoch: 6| Step: 13
Training loss: 2.5456338678397996
Validation loss: 2.7761326484259787

Epoch: 201| Step: 0
Training loss: 2.877938634614528
Validation loss: 2.7794919275472956

Epoch: 6| Step: 1
Training loss: 2.3697195077047692
Validation loss: 2.7821802848904835

Epoch: 6| Step: 2
Training loss: 3.2531916278974813
Validation loss: 2.7787081736083747

Epoch: 6| Step: 3
Training loss: 2.996718837579513
Validation loss: 2.7845922935598124

Epoch: 6| Step: 4
Training loss: 2.691454528464169
Validation loss: 2.782204869995178

Epoch: 6| Step: 5
Training loss: 2.988617602598323
Validation loss: 2.7845870366315792

Epoch: 6| Step: 6
Training loss: 3.183201902648863
Validation loss: 2.7831506358826177

Epoch: 6| Step: 7
Training loss: 3.3845204893560763
Validation loss: 2.780825261840574

Epoch: 6| Step: 8
Training loss: 3.538150762542694
Validation loss: 2.779315684643334

Epoch: 6| Step: 9
Training loss: 3.2818940030217782
Validation loss: 2.7805151895645297

Epoch: 6| Step: 10
Training loss: 3.0506193188858823
Validation loss: 2.7736880147548257

Epoch: 6| Step: 11
Training loss: 3.18268895309736
Validation loss: 2.772545278177197

Epoch: 6| Step: 12
Training loss: 3.2849298097494364
Validation loss: 2.770479821053582

Epoch: 6| Step: 13
Training loss: 2.757586591507484
Validation loss: 2.7712111294555286

Epoch: 202| Step: 0
Training loss: 2.9368847141726246
Validation loss: 2.7731660961120026

Epoch: 6| Step: 1
Training loss: 3.4409511927785292
Validation loss: 2.771848533474328

Epoch: 6| Step: 2
Training loss: 3.3311653398416454
Validation loss: 2.774649919338962

Epoch: 6| Step: 3
Training loss: 2.9800285924269465
Validation loss: 2.7715484919982405

Epoch: 6| Step: 4
Training loss: 3.064600788397819
Validation loss: 2.7712962352734065

Epoch: 6| Step: 5
Training loss: 2.973205435472537
Validation loss: 2.778987058389637

Epoch: 6| Step: 6
Training loss: 3.4291983496746195
Validation loss: 2.7711458019967035

Epoch: 6| Step: 7
Training loss: 2.9829930805838347
Validation loss: 2.777552223345572

Epoch: 6| Step: 8
Training loss: 3.2325913675823714
Validation loss: 2.7739420206461194

Epoch: 6| Step: 9
Training loss: 3.204013012863194
Validation loss: 2.774468437411287

Epoch: 6| Step: 10
Training loss: 2.878214821672
Validation loss: 2.7707830176702983

Epoch: 6| Step: 11
Training loss: 3.0671895769979916
Validation loss: 2.7697168883289476

Epoch: 6| Step: 12
Training loss: 2.684858310164691
Validation loss: 2.7731736784027916

Epoch: 6| Step: 13
Training loss: 2.6819390716402336
Validation loss: 2.7720977731787584

Epoch: 203| Step: 0
Training loss: 3.217880576033133
Validation loss: 2.7689583209481734

Epoch: 6| Step: 1
Training loss: 3.3764043641207597
Validation loss: 2.7693800210128834

Epoch: 6| Step: 2
Training loss: 2.877773274189466
Validation loss: 2.7707667250626176

Epoch: 6| Step: 3
Training loss: 3.754936910371025
Validation loss: 2.7677371848612737

Epoch: 6| Step: 4
Training loss: 3.3283702777868163
Validation loss: 2.766161801251037

Epoch: 6| Step: 5
Training loss: 2.879108313168346
Validation loss: 2.774086953381037

Epoch: 6| Step: 6
Training loss: 2.8864045546409045
Validation loss: 2.7718489413485043

Epoch: 6| Step: 7
Training loss: 2.3924372168328296
Validation loss: 2.7679922128861008

Epoch: 6| Step: 8
Training loss: 2.801322597395333
Validation loss: 2.765856548294047

Epoch: 6| Step: 9
Training loss: 3.2981908983417405
Validation loss: 2.7686956920078614

Epoch: 6| Step: 10
Training loss: 3.3042017802953905
Validation loss: 2.767824985382936

Epoch: 6| Step: 11
Training loss: 3.149625568237084
Validation loss: 2.7638133113517442

Epoch: 6| Step: 12
Training loss: 2.9508987997212457
Validation loss: 2.7729487523307808

Epoch: 6| Step: 13
Training loss: 2.332053435202524
Validation loss: 2.7673527756274305

Epoch: 204| Step: 0
Training loss: 2.4570595841222
Validation loss: 2.7708180581565474

Epoch: 6| Step: 1
Training loss: 3.292317064629823
Validation loss: 2.7677862261197097

Epoch: 6| Step: 2
Training loss: 3.082379648994768
Validation loss: 2.772384256382347

Epoch: 6| Step: 3
Training loss: 2.921272582920523
Validation loss: 2.781666583333507

Epoch: 6| Step: 4
Training loss: 2.8372534018756728
Validation loss: 2.780139990562081

Epoch: 6| Step: 5
Training loss: 3.187671432372948
Validation loss: 2.784928637160059

Epoch: 6| Step: 6
Training loss: 2.867515773846211
Validation loss: 2.767650081827941

Epoch: 6| Step: 7
Training loss: 2.1931080457630503
Validation loss: 2.7693278507894963

Epoch: 6| Step: 8
Training loss: 3.111100681226898
Validation loss: 2.7695425374589178

Epoch: 6| Step: 9
Training loss: 3.1119714372397906
Validation loss: 2.766776923354678

Epoch: 6| Step: 10
Training loss: 3.8177969984277076
Validation loss: 2.764273894959922

Epoch: 6| Step: 11
Training loss: 3.3749509737197974
Validation loss: 2.7664172376889473

Epoch: 6| Step: 12
Training loss: 3.223216984834133
Validation loss: 2.7676728258062835

Epoch: 6| Step: 13
Training loss: 3.4662249198086017
Validation loss: 2.76720224377735

Epoch: 205| Step: 0
Training loss: 3.006194078079958
Validation loss: 2.76734370072196

Epoch: 6| Step: 1
Training loss: 3.3005284319784085
Validation loss: 2.761293718559033

Epoch: 6| Step: 2
Training loss: 2.5252174737019675
Validation loss: 2.765575282653016

Epoch: 6| Step: 3
Training loss: 2.9381075291701473
Validation loss: 2.7653770421365818

Epoch: 6| Step: 4
Training loss: 3.289462611129595
Validation loss: 2.7686524364748846

Epoch: 6| Step: 5
Training loss: 3.2678051839445454
Validation loss: 2.7640340261295226

Epoch: 6| Step: 6
Training loss: 3.5870019334373655
Validation loss: 2.7656968686022885

Epoch: 6| Step: 7
Training loss: 3.8063085341689784
Validation loss: 2.7635769220622604

Epoch: 6| Step: 8
Training loss: 2.432515647971152
Validation loss: 2.764397075740827

Epoch: 6| Step: 9
Training loss: 2.9023179557547425
Validation loss: 2.763902341227585

Epoch: 6| Step: 10
Training loss: 3.135141776740904
Validation loss: 2.767820063395989

Epoch: 6| Step: 11
Training loss: 2.7112200586996913
Validation loss: 2.768837250424807

Epoch: 6| Step: 12
Training loss: 2.8280192824357853
Validation loss: 2.7687405413049833

Epoch: 6| Step: 13
Training loss: 3.0465334969874536
Validation loss: 2.765562727569884

Epoch: 206| Step: 0
Training loss: 3.4432316938714225
Validation loss: 2.76838102093879

Epoch: 6| Step: 1
Training loss: 3.0081200380520188
Validation loss: 2.7746139709435793

Epoch: 6| Step: 2
Training loss: 3.2242293154093007
Validation loss: 2.770473458396429

Epoch: 6| Step: 3
Training loss: 2.493104006816834
Validation loss: 2.770944238910888

Epoch: 6| Step: 4
Training loss: 3.384528238168282
Validation loss: 2.7789298770020783

Epoch: 6| Step: 5
Training loss: 3.1717577381726687
Validation loss: 2.775135783567249

Epoch: 6| Step: 6
Training loss: 3.046331895205621
Validation loss: 2.7697262340624174

Epoch: 6| Step: 7
Training loss: 3.218426438151565
Validation loss: 2.7718921821449127

Epoch: 6| Step: 8
Training loss: 3.3002835412024134
Validation loss: 2.7684474063333497

Epoch: 6| Step: 9
Training loss: 3.443265484131868
Validation loss: 2.7692073634543926

Epoch: 6| Step: 10
Training loss: 3.2161718617156945
Validation loss: 2.771249065596333

Epoch: 6| Step: 11
Training loss: 2.3330832301932114
Validation loss: 2.7704208844855427

Epoch: 6| Step: 12
Training loss: 2.509337154877388
Validation loss: 2.769852398840709

Epoch: 6| Step: 13
Training loss: 3.002317645961769
Validation loss: 2.7684454505761544

Epoch: 207| Step: 0
Training loss: 3.0060081717760148
Validation loss: 2.771411768450805

Epoch: 6| Step: 1
Training loss: 3.4149335016391866
Validation loss: 2.7700412354228945

Epoch: 6| Step: 2
Training loss: 2.94232755014691
Validation loss: 2.7660831641762145

Epoch: 6| Step: 3
Training loss: 3.1688952049678702
Validation loss: 2.765006911042106

Epoch: 6| Step: 4
Training loss: 2.8677103257140586
Validation loss: 2.7716099343672855

Epoch: 6| Step: 5
Training loss: 2.9791780129241068
Validation loss: 2.7791079182811176

Epoch: 6| Step: 6
Training loss: 3.0641503071981537
Validation loss: 2.774380879475641

Epoch: 6| Step: 7
Training loss: 3.94733339428001
Validation loss: 2.7689971128742803

Epoch: 6| Step: 8
Training loss: 2.7390386429143216
Validation loss: 2.768764735568088

Epoch: 6| Step: 9
Training loss: 3.265851026679998
Validation loss: 2.7650588651745314

Epoch: 6| Step: 10
Training loss: 2.5806284454019672
Validation loss: 2.7682868925955972

Epoch: 6| Step: 11
Training loss: 2.968998627541245
Validation loss: 2.7628155136279866

Epoch: 6| Step: 12
Training loss: 3.1188779754985294
Validation loss: 2.7641897229682524

Epoch: 6| Step: 13
Training loss: 2.4596416634190352
Validation loss: 2.7599144520356127

Epoch: 208| Step: 0
Training loss: 2.6747939912847474
Validation loss: 2.7568443551353363

Epoch: 6| Step: 1
Training loss: 2.839928375200602
Validation loss: 2.7578324861571226

Epoch: 6| Step: 2
Training loss: 2.744011687918369
Validation loss: 2.759672368650331

Epoch: 6| Step: 3
Training loss: 3.515670301357435
Validation loss: 2.7600857673142727

Epoch: 6| Step: 4
Training loss: 2.586380842760797
Validation loss: 2.7597023926544697

Epoch: 6| Step: 5
Training loss: 3.6985745699921315
Validation loss: 2.7607262098659358

Epoch: 6| Step: 6
Training loss: 3.160470086394141
Validation loss: 2.758920594685352

Epoch: 6| Step: 7
Training loss: 2.909819021463517
Validation loss: 2.762283986999115

Epoch: 6| Step: 8
Training loss: 2.8789445105899754
Validation loss: 2.75613612562769

Epoch: 6| Step: 9
Training loss: 3.3151922261702955
Validation loss: 2.756412907993038

Epoch: 6| Step: 10
Training loss: 2.9315510023879936
Validation loss: 2.7658935679731105

Epoch: 6| Step: 11
Training loss: 3.689613124756575
Validation loss: 2.766896978669344

Epoch: 6| Step: 12
Training loss: 2.6762319811695905
Validation loss: 2.7734723830871033

Epoch: 6| Step: 13
Training loss: 3.1897915849197123
Validation loss: 2.771323078799814

Epoch: 209| Step: 0
Training loss: 2.3520092714023155
Validation loss: 2.786142453741789

Epoch: 6| Step: 1
Training loss: 3.2132939882626492
Validation loss: 2.803572038806476

Epoch: 6| Step: 2
Training loss: 3.2545574219010445
Validation loss: 2.826619242966675

Epoch: 6| Step: 3
Training loss: 3.0139470150953627
Validation loss: 2.849520181766643

Epoch: 6| Step: 4
Training loss: 3.1039851261451106
Validation loss: 2.814527040453306

Epoch: 6| Step: 5
Training loss: 2.601868757487388
Validation loss: 2.8006955883960503

Epoch: 6| Step: 6
Training loss: 2.940326061722265
Validation loss: 2.7758069276891213

Epoch: 6| Step: 7
Training loss: 2.879405419644603
Validation loss: 2.7703654816173047

Epoch: 6| Step: 8
Training loss: 3.248658123295029
Validation loss: 2.7585763134689336

Epoch: 6| Step: 9
Training loss: 3.1190788630782587
Validation loss: 2.7596382577524787

Epoch: 6| Step: 10
Training loss: 3.362272778175989
Validation loss: 2.758141433270792

Epoch: 6| Step: 11
Training loss: 3.377260651934997
Validation loss: 2.758592499643568

Epoch: 6| Step: 12
Training loss: 3.5303262498210817
Validation loss: 2.761039072745712

Epoch: 6| Step: 13
Training loss: 2.73615411917185
Validation loss: 2.765088238244526

Epoch: 210| Step: 0
Training loss: 3.549602384194104
Validation loss: 2.7638695500388932

Epoch: 6| Step: 1
Training loss: 3.4095058454515748
Validation loss: 2.7627519929056366

Epoch: 6| Step: 2
Training loss: 3.4525772972210387
Validation loss: 2.7622390059547066

Epoch: 6| Step: 3
Training loss: 2.7106433016190197
Validation loss: 2.7594107140201927

Epoch: 6| Step: 4
Training loss: 2.863319719376209
Validation loss: 2.7578926835872575

Epoch: 6| Step: 5
Training loss: 3.251584767050438
Validation loss: 2.7574233192697006

Epoch: 6| Step: 6
Training loss: 2.4270310184805526
Validation loss: 2.754358398072053

Epoch: 6| Step: 7
Training loss: 3.2505416418769615
Validation loss: 2.760252975158058

Epoch: 6| Step: 8
Training loss: 2.840399812306359
Validation loss: 2.7603060780107085

Epoch: 6| Step: 9
Training loss: 2.7220240205073494
Validation loss: 2.7805556172154233

Epoch: 6| Step: 10
Training loss: 2.9128949073615766
Validation loss: 2.7772240715132646

Epoch: 6| Step: 11
Training loss: 2.9103374655197647
Validation loss: 2.784996528062913

Epoch: 6| Step: 12
Training loss: 3.8060973138360685
Validation loss: 2.8018000454527674

Epoch: 6| Step: 13
Training loss: 2.2606849218246627
Validation loss: 2.7904091379465017

Epoch: 211| Step: 0
Training loss: 2.9364959948938028
Validation loss: 2.790207297894197

Epoch: 6| Step: 1
Training loss: 3.057779839623419
Validation loss: 2.796201097580061

Epoch: 6| Step: 2
Training loss: 3.4309847036057963
Validation loss: 2.7783670096480524

Epoch: 6| Step: 3
Training loss: 3.5683068583214705
Validation loss: 2.7651637476963367

Epoch: 6| Step: 4
Training loss: 3.2678358269382644
Validation loss: 2.7546620103643256

Epoch: 6| Step: 5
Training loss: 3.570547150428093
Validation loss: 2.7522519466118847

Epoch: 6| Step: 6
Training loss: 2.7097472290452855
Validation loss: 2.75089769263704

Epoch: 6| Step: 7
Training loss: 2.8312714498014735
Validation loss: 2.7539532289357576

Epoch: 6| Step: 8
Training loss: 2.700687080840386
Validation loss: 2.7519948814365476

Epoch: 6| Step: 9
Training loss: 2.4725954069392824
Validation loss: 2.7554477761526357

Epoch: 6| Step: 10
Training loss: 2.867718639602367
Validation loss: 2.75505663948118

Epoch: 6| Step: 11
Training loss: 2.8169300370037633
Validation loss: 2.755303604752558

Epoch: 6| Step: 12
Training loss: 3.854647687158226
Validation loss: 2.7534272324723568

Epoch: 6| Step: 13
Training loss: 2.584210605766499
Validation loss: 2.7530113374885854

Epoch: 212| Step: 0
Training loss: 3.233952296549821
Validation loss: 2.753074450674393

Epoch: 6| Step: 1
Training loss: 3.0867495470337856
Validation loss: 2.753171491265883

Epoch: 6| Step: 2
Training loss: 2.950813155523168
Validation loss: 2.754887692460601

Epoch: 6| Step: 3
Training loss: 2.8969538508180506
Validation loss: 2.7564809636618866

Epoch: 6| Step: 4
Training loss: 2.513516982255475
Validation loss: 2.7666243002719524

Epoch: 6| Step: 5
Training loss: 3.750690396652073
Validation loss: 2.7631051683521295

Epoch: 6| Step: 6
Training loss: 3.1288081045390586
Validation loss: 2.7567397912535916

Epoch: 6| Step: 7
Training loss: 3.129962338108252
Validation loss: 2.755394244722395

Epoch: 6| Step: 8
Training loss: 3.67758060079142
Validation loss: 2.7500230018198177

Epoch: 6| Step: 9
Training loss: 2.3059919383494525
Validation loss: 2.7507991543182286

Epoch: 6| Step: 10
Training loss: 2.485272994982367
Validation loss: 2.750600803258928

Epoch: 6| Step: 11
Training loss: 3.193985866489085
Validation loss: 2.7516358246911485

Epoch: 6| Step: 12
Training loss: 3.47528368114442
Validation loss: 2.752887123875782

Epoch: 6| Step: 13
Training loss: 2.500938620795271
Validation loss: 2.753600027606281

Epoch: 213| Step: 0
Training loss: 3.2961932738496658
Validation loss: 2.7536604888645244

Epoch: 6| Step: 1
Training loss: 2.9582497670007446
Validation loss: 2.752639309419959

Epoch: 6| Step: 2
Training loss: 2.909605160960502
Validation loss: 2.7545153072723485

Epoch: 6| Step: 3
Training loss: 2.6597514691804602
Validation loss: 2.756253726649677

Epoch: 6| Step: 4
Training loss: 3.4865822408263796
Validation loss: 2.7512892220920815

Epoch: 6| Step: 5
Training loss: 2.9928736285988915
Validation loss: 2.7520598027548924

Epoch: 6| Step: 6
Training loss: 2.9582201080862363
Validation loss: 2.7578477768639087

Epoch: 6| Step: 7
Training loss: 2.1959080007213045
Validation loss: 2.751163275781519

Epoch: 6| Step: 8
Training loss: 3.339176794014077
Validation loss: 2.7546589029047253

Epoch: 6| Step: 9
Training loss: 2.6161897038320383
Validation loss: 2.7490538724942946

Epoch: 6| Step: 10
Training loss: 3.934076183021671
Validation loss: 2.748920418941122

Epoch: 6| Step: 11
Training loss: 3.425649155404752
Validation loss: 2.7542052124815077

Epoch: 6| Step: 12
Training loss: 2.6788782616124145
Validation loss: 2.7474460423231397

Epoch: 6| Step: 13
Training loss: 3.0381973721929096
Validation loss: 2.744698180201637

Epoch: 214| Step: 0
Training loss: 3.56292026115566
Validation loss: 2.7512503696465314

Epoch: 6| Step: 1
Training loss: 2.995286735857668
Validation loss: 2.7521458035373194

Epoch: 6| Step: 2
Training loss: 3.211285574579736
Validation loss: 2.7509700410418834

Epoch: 6| Step: 3
Training loss: 2.6666778524482093
Validation loss: 2.746298450395427

Epoch: 6| Step: 4
Training loss: 3.245071488872858
Validation loss: 2.7446899121227455

Epoch: 6| Step: 5
Training loss: 3.5435668036794046
Validation loss: 2.7492495824589636

Epoch: 6| Step: 6
Training loss: 2.229320009845789
Validation loss: 2.74625897284276

Epoch: 6| Step: 7
Training loss: 3.177812853162395
Validation loss: 2.752481509974736

Epoch: 6| Step: 8
Training loss: 2.7704150426726666
Validation loss: 2.748598970550009

Epoch: 6| Step: 9
Training loss: 3.7185397048601074
Validation loss: 2.75652918959212

Epoch: 6| Step: 10
Training loss: 3.097296021574895
Validation loss: 2.7617349985168618

Epoch: 6| Step: 11
Training loss: 2.5409778120084767
Validation loss: 2.77187360143917

Epoch: 6| Step: 12
Training loss: 2.9095338706329494
Validation loss: 2.7722243035044425

Epoch: 6| Step: 13
Training loss: 2.6665817386295747
Validation loss: 2.7803199709106665

Epoch: 215| Step: 0
Training loss: 2.5821263559805727
Validation loss: 2.7920904762289367

Epoch: 6| Step: 1
Training loss: 3.3510613767079627
Validation loss: 2.812552501958662

Epoch: 6| Step: 2
Training loss: 3.240075436879506
Validation loss: 2.8292354610232624

Epoch: 6| Step: 3
Training loss: 3.6804830739943615
Validation loss: 2.8691140819089007

Epoch: 6| Step: 4
Training loss: 3.149586810911516
Validation loss: 2.890132703601747

Epoch: 6| Step: 5
Training loss: 2.498636636915212
Validation loss: 2.8602343695160513

Epoch: 6| Step: 6
Training loss: 3.318886556168693
Validation loss: 2.8306808675103223

Epoch: 6| Step: 7
Training loss: 3.3947085212450667
Validation loss: 2.8206652630684865

Epoch: 6| Step: 8
Training loss: 3.378388716913849
Validation loss: 2.802210871266572

Epoch: 6| Step: 9
Training loss: 2.7514059634200136
Validation loss: 2.793955068088103

Epoch: 6| Step: 10
Training loss: 2.436142274325458
Validation loss: 2.8020750793513565

Epoch: 6| Step: 11
Training loss: 2.735491541011828
Validation loss: 2.827194888536883

Epoch: 6| Step: 12
Training loss: 3.293448018485166
Validation loss: 2.802071946704945

Epoch: 6| Step: 13
Training loss: 3.6855189287493726
Validation loss: 2.764618084233941

Epoch: 216| Step: 0
Training loss: 3.7080718530485095
Validation loss: 2.758310457967579

Epoch: 6| Step: 1
Training loss: 3.3506269238421456
Validation loss: 2.75420568719436

Epoch: 6| Step: 2
Training loss: 3.2524828596785014
Validation loss: 2.7644867721267055

Epoch: 6| Step: 3
Training loss: 2.700523756584172
Validation loss: 2.7796217772786282

Epoch: 6| Step: 4
Training loss: 2.6899514661189863
Validation loss: 2.7870715130870276

Epoch: 6| Step: 5
Training loss: 2.698942885262023
Validation loss: 2.801241359857067

Epoch: 6| Step: 6
Training loss: 3.3564571039671405
Validation loss: 2.8009234054644336

Epoch: 6| Step: 7
Training loss: 2.571047228586736
Validation loss: 2.7824054118714754

Epoch: 6| Step: 8
Training loss: 3.1782295202361928
Validation loss: 2.784841118123112

Epoch: 6| Step: 9
Training loss: 3.2260014598107625
Validation loss: 2.7685959879833444

Epoch: 6| Step: 10
Training loss: 3.185803279696703
Validation loss: 2.759717782616842

Epoch: 6| Step: 11
Training loss: 2.989259567185454
Validation loss: 2.7615275219828717

Epoch: 6| Step: 12
Training loss: 2.5852739571882553
Validation loss: 2.757313176472575

Epoch: 6| Step: 13
Training loss: 3.2593852758931376
Validation loss: 2.752397692687455

Epoch: 217| Step: 0
Training loss: 2.8632016450607036
Validation loss: 2.749027956702033

Epoch: 6| Step: 1
Training loss: 2.6058216685233377
Validation loss: 2.7543005759966808

Epoch: 6| Step: 2
Training loss: 3.105074568389034
Validation loss: 2.752255764705533

Epoch: 6| Step: 3
Training loss: 3.1171608830513056
Validation loss: 2.7514139401766093

Epoch: 6| Step: 4
Training loss: 3.3432987657346587
Validation loss: 2.7494098499944526

Epoch: 6| Step: 5
Training loss: 3.4646795636039114
Validation loss: 2.748053961265281

Epoch: 6| Step: 6
Training loss: 3.3577578888539796
Validation loss: 2.751919852797205

Epoch: 6| Step: 7
Training loss: 2.864699112835426
Validation loss: 2.743290523712918

Epoch: 6| Step: 8
Training loss: 3.4170403314967173
Validation loss: 2.743047551723683

Epoch: 6| Step: 9
Training loss: 2.631518490247439
Validation loss: 2.745025576853322

Epoch: 6| Step: 10
Training loss: 3.0891945949129296
Validation loss: 2.7450710827747473

Epoch: 6| Step: 11
Training loss: 3.10065052836266
Validation loss: 2.7418830197000172

Epoch: 6| Step: 12
Training loss: 2.71410191781294
Validation loss: 2.7407723712336947

Epoch: 6| Step: 13
Training loss: 3.1256182249804723
Validation loss: 2.7399679177830207

Epoch: 218| Step: 0
Training loss: 3.106592514834003
Validation loss: 2.7431041642781784

Epoch: 6| Step: 1
Training loss: 3.0847329408368855
Validation loss: 2.7445776661909913

Epoch: 6| Step: 2
Training loss: 2.572681112484963
Validation loss: 2.7394517573518837

Epoch: 6| Step: 3
Training loss: 3.2337205012700885
Validation loss: 2.745277606633734

Epoch: 6| Step: 4
Training loss: 3.0284215571293154
Validation loss: 2.744402923735992

Epoch: 6| Step: 5
Training loss: 3.7727848854972277
Validation loss: 2.744122799957892

Epoch: 6| Step: 6
Training loss: 3.265551406542168
Validation loss: 2.7468895759155196

Epoch: 6| Step: 7
Training loss: 2.572814558425204
Validation loss: 2.7450513315081237

Epoch: 6| Step: 8
Training loss: 2.9343004048608923
Validation loss: 2.745067892547159

Epoch: 6| Step: 9
Training loss: 2.862964482499173
Validation loss: 2.7385648414649344

Epoch: 6| Step: 10
Training loss: 3.043574496933464
Validation loss: 2.7443742437978607

Epoch: 6| Step: 11
Training loss: 3.3079932882847443
Validation loss: 2.7445456874893583

Epoch: 6| Step: 12
Training loss: 3.2044634743982403
Validation loss: 2.7414984283559165

Epoch: 6| Step: 13
Training loss: 2.382156732093534
Validation loss: 2.739328444986904

Epoch: 219| Step: 0
Training loss: 2.989888316336421
Validation loss: 2.736620370603848

Epoch: 6| Step: 1
Training loss: 3.0735870836914896
Validation loss: 2.739679779734418

Epoch: 6| Step: 2
Training loss: 2.8407605565862037
Validation loss: 2.73893616046395

Epoch: 6| Step: 3
Training loss: 3.153076389266954
Validation loss: 2.7397890725269494

Epoch: 6| Step: 4
Training loss: 2.6951440730890495
Validation loss: 2.7431019119479356

Epoch: 6| Step: 5
Training loss: 2.7820018652135654
Validation loss: 2.7371260400642856

Epoch: 6| Step: 6
Training loss: 3.932872293144303
Validation loss: 2.743422489660013

Epoch: 6| Step: 7
Training loss: 2.9913424501268095
Validation loss: 2.7367369180058194

Epoch: 6| Step: 8
Training loss: 3.631255441684552
Validation loss: 2.737632949619175

Epoch: 6| Step: 9
Training loss: 2.4667899161125124
Validation loss: 2.74355932040276

Epoch: 6| Step: 10
Training loss: 3.056485088644624
Validation loss: 2.7454845426223007

Epoch: 6| Step: 11
Training loss: 2.905793841259285
Validation loss: 2.7443596991377954

Epoch: 6| Step: 12
Training loss: 3.4435240240431892
Validation loss: 2.747780155621519

Epoch: 6| Step: 13
Training loss: 1.9818437907908362
Validation loss: 2.753216552478529

Epoch: 220| Step: 0
Training loss: 2.312350500274689
Validation loss: 2.752231765304821

Epoch: 6| Step: 1
Training loss: 3.8228763644028256
Validation loss: 2.7453535190269625

Epoch: 6| Step: 2
Training loss: 3.6878990345372022
Validation loss: 2.7501142502336173

Epoch: 6| Step: 3
Training loss: 3.2934607594145873
Validation loss: 2.748801919416141

Epoch: 6| Step: 4
Training loss: 2.786488432439412
Validation loss: 2.7458363220994295

Epoch: 6| Step: 5
Training loss: 3.2089766480810806
Validation loss: 2.744459858625032

Epoch: 6| Step: 6
Training loss: 2.2282813147115594
Validation loss: 2.7412222695609327

Epoch: 6| Step: 7
Training loss: 3.1917409172750553
Validation loss: 2.7416088343385283

Epoch: 6| Step: 8
Training loss: 2.365617388734453
Validation loss: 2.736261147731063

Epoch: 6| Step: 9
Training loss: 3.1036781763008063
Validation loss: 2.74003788357869

Epoch: 6| Step: 10
Training loss: 2.6620749837286968
Validation loss: 2.7370173481077726

Epoch: 6| Step: 11
Training loss: 3.3721536006855044
Validation loss: 2.7417169749268533

Epoch: 6| Step: 12
Training loss: 3.337977194514461
Validation loss: 2.737883427633062

Epoch: 6| Step: 13
Training loss: 2.7479912617362627
Validation loss: 2.7374703424832294

Epoch: 221| Step: 0
Training loss: 3.476778653743851
Validation loss: 2.738634422796284

Epoch: 6| Step: 1
Training loss: 3.0714070550665533
Validation loss: 2.7460505398800357

Epoch: 6| Step: 2
Training loss: 2.147579285264929
Validation loss: 2.7415453655619264

Epoch: 6| Step: 3
Training loss: 3.4794144837222443
Validation loss: 2.735814648486781

Epoch: 6| Step: 4
Training loss: 3.1112416860638716
Validation loss: 2.7395108595211437

Epoch: 6| Step: 5
Training loss: 2.780511650918506
Validation loss: 2.739588044345498

Epoch: 6| Step: 6
Training loss: 2.4360893275602655
Validation loss: 2.7351944106111796

Epoch: 6| Step: 7
Training loss: 2.997845193745678
Validation loss: 2.7410776674707185

Epoch: 6| Step: 8
Training loss: 3.2158845164692407
Validation loss: 2.7442080456798092

Epoch: 6| Step: 9
Training loss: 2.812686320597043
Validation loss: 2.745349395315582

Epoch: 6| Step: 10
Training loss: 3.7767336016315567
Validation loss: 2.739418595344495

Epoch: 6| Step: 11
Training loss: 2.947809511715165
Validation loss: 2.744939669599039

Epoch: 6| Step: 12
Training loss: 3.292245805873107
Validation loss: 2.7425334188582067

Epoch: 6| Step: 13
Training loss: 2.799861570751071
Validation loss: 2.7488684241858525

Epoch: 222| Step: 0
Training loss: 3.1495604677695455
Validation loss: 2.7515502300327666

Epoch: 6| Step: 1
Training loss: 3.314146820103575
Validation loss: 2.755363221956386

Epoch: 6| Step: 2
Training loss: 3.010704651535799
Validation loss: 2.7506407627943483

Epoch: 6| Step: 3
Training loss: 3.2088690636616213
Validation loss: 2.7508172809219085

Epoch: 6| Step: 4
Training loss: 3.500112395525469
Validation loss: 2.7471921873975758

Epoch: 6| Step: 5
Training loss: 2.7319013469248543
Validation loss: 2.735072847493133

Epoch: 6| Step: 6
Training loss: 2.7431325126400794
Validation loss: 2.737565391824482

Epoch: 6| Step: 7
Training loss: 3.097321269718652
Validation loss: 2.737661509146956

Epoch: 6| Step: 8
Training loss: 2.8951153494036364
Validation loss: 2.736567705594994

Epoch: 6| Step: 9
Training loss: 2.8559327798727785
Validation loss: 2.7354169659842884

Epoch: 6| Step: 10
Training loss: 3.2339615857090505
Validation loss: 2.7341704101903925

Epoch: 6| Step: 11
Training loss: 2.4026563487549364
Validation loss: 2.7390370906181505

Epoch: 6| Step: 12
Training loss: 3.620830374051821
Validation loss: 2.7342165346417358

Epoch: 6| Step: 13
Training loss: 2.6492310181999925
Validation loss: 2.7404432465873936

Epoch: 223| Step: 0
Training loss: 3.714714874374666
Validation loss: 2.739858450602002

Epoch: 6| Step: 1
Training loss: 3.3043882266047406
Validation loss: 2.7374841464565933

Epoch: 6| Step: 2
Training loss: 3.284700450506948
Validation loss: 2.7425100588320768

Epoch: 6| Step: 3
Training loss: 3.256738279785844
Validation loss: 2.741386747042016

Epoch: 6| Step: 4
Training loss: 2.9001344715379886
Validation loss: 2.736778264939272

Epoch: 6| Step: 5
Training loss: 3.33051354031172
Validation loss: 2.740510391059841

Epoch: 6| Step: 6
Training loss: 2.765456544735041
Validation loss: 2.745926525813792

Epoch: 6| Step: 7
Training loss: 2.6356990396558597
Validation loss: 2.736867476414907

Epoch: 6| Step: 8
Training loss: 3.3586565535805635
Validation loss: 2.7363319690602363

Epoch: 6| Step: 9
Training loss: 3.13829948908107
Validation loss: 2.732249571102784

Epoch: 6| Step: 10
Training loss: 2.4343371289188083
Validation loss: 2.737415667201473

Epoch: 6| Step: 11
Training loss: 2.961396113889423
Validation loss: 2.7396707114182233

Epoch: 6| Step: 12
Training loss: 2.6216511027202936
Validation loss: 2.739251401654397

Epoch: 6| Step: 13
Training loss: 2.541269041966719
Validation loss: 2.7389209860112866

Epoch: 224| Step: 0
Training loss: 2.6601784771006054
Validation loss: 2.7484011211092336

Epoch: 6| Step: 1
Training loss: 2.750348589217944
Validation loss: 2.745066383351552

Epoch: 6| Step: 2
Training loss: 3.3565814090825117
Validation loss: 2.74075160029326

Epoch: 6| Step: 3
Training loss: 2.988109229628073
Validation loss: 2.7465465224145804

Epoch: 6| Step: 4
Training loss: 2.5010750366509877
Validation loss: 2.7619569486611555

Epoch: 6| Step: 5
Training loss: 3.3201581133684153
Validation loss: 2.7565123672513714

Epoch: 6| Step: 6
Training loss: 3.319245577662789
Validation loss: 2.7439610865957444

Epoch: 6| Step: 7
Training loss: 2.869647973831177
Validation loss: 2.7477017659459166

Epoch: 6| Step: 8
Training loss: 3.0997295200112225
Validation loss: 2.753792843199602

Epoch: 6| Step: 9
Training loss: 3.273412881908846
Validation loss: 2.749662262955776

Epoch: 6| Step: 10
Training loss: 3.1208743518396735
Validation loss: 2.75192735202687

Epoch: 6| Step: 11
Training loss: 3.024718020829344
Validation loss: 2.745790513282512

Epoch: 6| Step: 12
Training loss: 3.131998998181413
Validation loss: 2.752144561839393

Epoch: 6| Step: 13
Training loss: 3.2621609507975986
Validation loss: 2.7404497145156967

Epoch: 225| Step: 0
Training loss: 3.0054559368736
Validation loss: 2.7386775411783404

Epoch: 6| Step: 1
Training loss: 2.6570614641149106
Validation loss: 2.7415453262873957

Epoch: 6| Step: 2
Training loss: 3.3148588023314858
Validation loss: 2.7448958108842665

Epoch: 6| Step: 3
Training loss: 2.0330183108246804
Validation loss: 2.7425070843638886

Epoch: 6| Step: 4
Training loss: 2.6396378697936353
Validation loss: 2.7417740179532073

Epoch: 6| Step: 5
Training loss: 3.3350990546070034
Validation loss: 2.7375233225780184

Epoch: 6| Step: 6
Training loss: 2.6008549091683144
Validation loss: 2.7278511755210437

Epoch: 6| Step: 7
Training loss: 3.2902148622425376
Validation loss: 2.7319117031862397

Epoch: 6| Step: 8
Training loss: 3.751285586927167
Validation loss: 2.7366358257339156

Epoch: 6| Step: 9
Training loss: 3.1130265307596368
Validation loss: 2.7330527488234337

Epoch: 6| Step: 10
Training loss: 2.971948927993721
Validation loss: 2.728395823006252

Epoch: 6| Step: 11
Training loss: 3.1157106806721155
Validation loss: 2.728399713947666

Epoch: 6| Step: 12
Training loss: 3.044314359947109
Validation loss: 2.7376363315854966

Epoch: 6| Step: 13
Training loss: 3.716585571248245
Validation loss: 2.7280020650551444

Epoch: 226| Step: 0
Training loss: 3.095498255740737
Validation loss: 2.7301014327414714

Epoch: 6| Step: 1
Training loss: 3.3478495342848706
Validation loss: 2.726917020371451

Epoch: 6| Step: 2
Training loss: 3.5532615400532346
Validation loss: 2.7327854384589108

Epoch: 6| Step: 3
Training loss: 2.7130881801269435
Validation loss: 2.7321559150858437

Epoch: 6| Step: 4
Training loss: 3.2767541511889453
Validation loss: 2.7280670545893826

Epoch: 6| Step: 5
Training loss: 2.545424721283022
Validation loss: 2.7249131869151326

Epoch: 6| Step: 6
Training loss: 3.2533972663798965
Validation loss: 2.729756522074813

Epoch: 6| Step: 7
Training loss: 2.7511527506423454
Validation loss: 2.7242175550852936

Epoch: 6| Step: 8
Training loss: 2.8755714843652096
Validation loss: 2.728007618973496

Epoch: 6| Step: 9
Training loss: 3.1724302811172707
Validation loss: 2.7281537120977912

Epoch: 6| Step: 10
Training loss: 3.0229165271537286
Validation loss: 2.7289647717552037

Epoch: 6| Step: 11
Training loss: 3.353664352682615
Validation loss: 2.729199610698161

Epoch: 6| Step: 12
Training loss: 3.0105470587310994
Validation loss: 2.7305928117204057

Epoch: 6| Step: 13
Training loss: 1.952063615415061
Validation loss: 2.7344774340122293

Epoch: 227| Step: 0
Training loss: 2.755365945289692
Validation loss: 2.7264532317737906

Epoch: 6| Step: 1
Training loss: 2.869650133984889
Validation loss: 2.739501466857092

Epoch: 6| Step: 2
Training loss: 2.9159356608991596
Validation loss: 2.738174031695284

Epoch: 6| Step: 3
Training loss: 3.0611264009395787
Validation loss: 2.7384373681484395

Epoch: 6| Step: 4
Training loss: 3.0981771216395586
Validation loss: 2.7494606764506746

Epoch: 6| Step: 5
Training loss: 2.5866246534979704
Validation loss: 2.7415302055512925

Epoch: 6| Step: 6
Training loss: 2.3003597683034607
Validation loss: 2.7328134229362706

Epoch: 6| Step: 7
Training loss: 2.9657574478335014
Validation loss: 2.724985193161721

Epoch: 6| Step: 8
Training loss: 3.0602371263565917
Validation loss: 2.729102665547542

Epoch: 6| Step: 9
Training loss: 3.9682511143749815
Validation loss: 2.7276989665431515

Epoch: 6| Step: 10
Training loss: 3.498914550313498
Validation loss: 2.730113041915186

Epoch: 6| Step: 11
Training loss: 3.1733274845528863
Validation loss: 2.724490966999661

Epoch: 6| Step: 12
Training loss: 3.1290388329236722
Validation loss: 2.724781463912722

Epoch: 6| Step: 13
Training loss: 2.838343080812637
Validation loss: 2.723347560680579

Epoch: 228| Step: 0
Training loss: 3.2104958186375603
Validation loss: 2.725984010315487

Epoch: 6| Step: 1
Training loss: 2.928587684187484
Validation loss: 2.7242389978975665

Epoch: 6| Step: 2
Training loss: 3.4133091844012418
Validation loss: 2.726060197598966

Epoch: 6| Step: 3
Training loss: 3.717897333435959
Validation loss: 2.727198493126833

Epoch: 6| Step: 4
Training loss: 3.2944092411121004
Validation loss: 2.728492387372328

Epoch: 6| Step: 5
Training loss: 2.93329302875688
Validation loss: 2.727755657988238

Epoch: 6| Step: 6
Training loss: 2.8750666735008417
Validation loss: 2.7290199827146355

Epoch: 6| Step: 7
Training loss: 2.8026503281170014
Validation loss: 2.7355113744235835

Epoch: 6| Step: 8
Training loss: 2.321250751517836
Validation loss: 2.7309065076805847

Epoch: 6| Step: 9
Training loss: 2.756192344702618
Validation loss: 2.729323009667937

Epoch: 6| Step: 10
Training loss: 3.442374085760215
Validation loss: 2.7261828716444967

Epoch: 6| Step: 11
Training loss: 2.948114089735367
Validation loss: 2.7276779813871648

Epoch: 6| Step: 12
Training loss: 2.925870560427606
Validation loss: 2.7292094652742955

Epoch: 6| Step: 13
Training loss: 2.8065780176069923
Validation loss: 2.738940326590914

Epoch: 229| Step: 0
Training loss: 3.133318548641089
Validation loss: 2.737312176195933

Epoch: 6| Step: 1
Training loss: 2.973382968773397
Validation loss: 2.725770137932344

Epoch: 6| Step: 2
Training loss: 3.2333823629640452
Validation loss: 2.728264090552851

Epoch: 6| Step: 3
Training loss: 3.0128822305770844
Validation loss: 2.7275932844162494

Epoch: 6| Step: 4
Training loss: 2.9072237741945814
Validation loss: 2.7247705687204635

Epoch: 6| Step: 5
Training loss: 2.991888046220266
Validation loss: 2.7318502660337023

Epoch: 6| Step: 6
Training loss: 3.1971692797220235
Validation loss: 2.7275454256817766

Epoch: 6| Step: 7
Training loss: 2.8280269542580405
Validation loss: 2.728706494534344

Epoch: 6| Step: 8
Training loss: 2.9184357500505045
Validation loss: 2.7217339507925526

Epoch: 6| Step: 9
Training loss: 3.04673477241415
Validation loss: 2.728829555987884

Epoch: 6| Step: 10
Training loss: 3.1601717916702965
Validation loss: 2.72128164778924

Epoch: 6| Step: 11
Training loss: 3.091032433726255
Validation loss: 2.725401138595118

Epoch: 6| Step: 12
Training loss: 3.1245866120619628
Validation loss: 2.7257181455049695

Epoch: 6| Step: 13
Training loss: 2.8947305752262014
Validation loss: 2.7237647265864924

Epoch: 230| Step: 0
Training loss: 3.1733803770708175
Validation loss: 2.7305904561230783

Epoch: 6| Step: 1
Training loss: 2.9381476155092106
Validation loss: 2.738895270120309

Epoch: 6| Step: 2
Training loss: 3.0770097445239766
Validation loss: 2.7247479182478367

Epoch: 6| Step: 3
Training loss: 2.545382103105891
Validation loss: 2.7239737222706952

Epoch: 6| Step: 4
Training loss: 2.957691033353433
Validation loss: 2.725248020464684

Epoch: 6| Step: 5
Training loss: 2.654639260593016
Validation loss: 2.723259163997975

Epoch: 6| Step: 6
Training loss: 3.0504647260476294
Validation loss: 2.7298862789516765

Epoch: 6| Step: 7
Training loss: 3.159617223613623
Validation loss: 2.733525694691978

Epoch: 6| Step: 8
Training loss: 2.8848667837290654
Validation loss: 2.7291259562194146

Epoch: 6| Step: 9
Training loss: 3.728554758912622
Validation loss: 2.7328456500873664

Epoch: 6| Step: 10
Training loss: 3.115447129784334
Validation loss: 2.7261573854183156

Epoch: 6| Step: 11
Training loss: 3.230011286051493
Validation loss: 2.7454279539405873

Epoch: 6| Step: 12
Training loss: 3.1823458419825146
Validation loss: 2.7584914295212886

Epoch: 6| Step: 13
Training loss: 2.401516208934706
Validation loss: 2.7402087528287824

Epoch: 231| Step: 0
Training loss: 3.1069584182594028
Validation loss: 2.741938761528802

Epoch: 6| Step: 1
Training loss: 2.9637511882675023
Validation loss: 2.757220347630415

Epoch: 6| Step: 2
Training loss: 3.1003907726470925
Validation loss: 2.7524017508851655

Epoch: 6| Step: 3
Training loss: 2.9553208034028033
Validation loss: 2.7485727268157647

Epoch: 6| Step: 4
Training loss: 3.715695320663875
Validation loss: 2.745764716076511

Epoch: 6| Step: 5
Training loss: 3.323452961055803
Validation loss: 2.732678073007034

Epoch: 6| Step: 6
Training loss: 2.9614692150179485
Validation loss: 2.7323852134102267

Epoch: 6| Step: 7
Training loss: 2.7582912299716247
Validation loss: 2.727445674285399

Epoch: 6| Step: 8
Training loss: 3.5642639611620894
Validation loss: 2.726071593580725

Epoch: 6| Step: 9
Training loss: 2.5085362611050313
Validation loss: 2.723732162331564

Epoch: 6| Step: 10
Training loss: 2.320260300996395
Validation loss: 2.7220071196006423

Epoch: 6| Step: 11
Training loss: 3.20471137236469
Validation loss: 2.7174021871633305

Epoch: 6| Step: 12
Training loss: 2.7699862645769247
Validation loss: 2.722713949413906

Epoch: 6| Step: 13
Training loss: 3.146520139655523
Validation loss: 2.716318369097383

Epoch: 232| Step: 0
Training loss: 3.4271796927002716
Validation loss: 2.715931456497673

Epoch: 6| Step: 1
Training loss: 3.5109433345975383
Validation loss: 2.720769475171086

Epoch: 6| Step: 2
Training loss: 2.8095989948462075
Validation loss: 2.7208460016239457

Epoch: 6| Step: 3
Training loss: 3.5184915342653276
Validation loss: 2.7240086816328906

Epoch: 6| Step: 4
Training loss: 2.771026422948007
Validation loss: 2.7197749443402746

Epoch: 6| Step: 5
Training loss: 2.893218325718513
Validation loss: 2.7266040933637274

Epoch: 6| Step: 6
Training loss: 2.882262304055598
Validation loss: 2.7219597569464598

Epoch: 6| Step: 7
Training loss: 2.9390095829192546
Validation loss: 2.7181371088306006

Epoch: 6| Step: 8
Training loss: 3.5232482994691012
Validation loss: 2.7188051223117373

Epoch: 6| Step: 9
Training loss: 2.510750353995373
Validation loss: 2.715551093415671

Epoch: 6| Step: 10
Training loss: 2.5843686418558898
Validation loss: 2.7138990841902277

Epoch: 6| Step: 11
Training loss: 2.5417691402935922
Validation loss: 2.717938934070103

Epoch: 6| Step: 12
Training loss: 3.2033856541716963
Validation loss: 2.725100084799794

Epoch: 6| Step: 13
Training loss: 3.1517658672996935
Validation loss: 2.728482904161006

Epoch: 233| Step: 0
Training loss: 2.5807502096468546
Validation loss: 2.7212661454950644

Epoch: 6| Step: 1
Training loss: 3.2011248042222644
Validation loss: 2.728790985028566

Epoch: 6| Step: 2
Training loss: 3.3257985468336915
Validation loss: 2.725262563646873

Epoch: 6| Step: 3
Training loss: 3.0568697809967618
Validation loss: 2.7208280088954284

Epoch: 6| Step: 4
Training loss: 2.7812447708595065
Validation loss: 2.721509984454144

Epoch: 6| Step: 5
Training loss: 2.6398297980380976
Validation loss: 2.7267145025813555

Epoch: 6| Step: 6
Training loss: 3.1602866165940453
Validation loss: 2.721464890923481

Epoch: 6| Step: 7
Training loss: 3.016516361039817
Validation loss: 2.716219399335633

Epoch: 6| Step: 8
Training loss: 3.5332074436817167
Validation loss: 2.726769054912549

Epoch: 6| Step: 9
Training loss: 2.9634216677612777
Validation loss: 2.7156100492645145

Epoch: 6| Step: 10
Training loss: 2.931842957083479
Validation loss: 2.7165011226852656

Epoch: 6| Step: 11
Training loss: 3.4475964784573447
Validation loss: 2.720247763468665

Epoch: 6| Step: 12
Training loss: 2.9573524532451057
Validation loss: 2.7223838507107527

Epoch: 6| Step: 13
Training loss: 2.5435689988283268
Validation loss: 2.720929385098212

Epoch: 234| Step: 0
Training loss: 2.709238008865977
Validation loss: 2.7181790772802246

Epoch: 6| Step: 1
Training loss: 3.413457681606176
Validation loss: 2.720172887389067

Epoch: 6| Step: 2
Training loss: 3.0279090009089846
Validation loss: 2.7172415828289407

Epoch: 6| Step: 3
Training loss: 2.409191068167029
Validation loss: 2.722405546211642

Epoch: 6| Step: 4
Training loss: 3.075459211842761
Validation loss: 2.719516627676831

Epoch: 6| Step: 5
Training loss: 2.6806755357507206
Validation loss: 2.722745830972275

Epoch: 6| Step: 6
Training loss: 3.6844741562045535
Validation loss: 2.7128227978619734

Epoch: 6| Step: 7
Training loss: 3.3646846948872193
Validation loss: 2.7139155585526096

Epoch: 6| Step: 8
Training loss: 2.9179167838620663
Validation loss: 2.7198545790807653

Epoch: 6| Step: 9
Training loss: 2.7538511013704583
Validation loss: 2.7189702864245606

Epoch: 6| Step: 10
Training loss: 2.919100826507187
Validation loss: 2.7174390650435813

Epoch: 6| Step: 11
Training loss: 3.086101131688222
Validation loss: 2.71914058072551

Epoch: 6| Step: 12
Training loss: 3.2008688819961755
Validation loss: 2.7138794262909522

Epoch: 6| Step: 13
Training loss: 2.9994988022802027
Validation loss: 2.7140169624743256

Epoch: 235| Step: 0
Training loss: 3.2625341070153544
Validation loss: 2.7229180715554953

Epoch: 6| Step: 1
Training loss: 3.318588132197503
Validation loss: 2.717834203375924

Epoch: 6| Step: 2
Training loss: 2.3948146036340283
Validation loss: 2.7174057815758204

Epoch: 6| Step: 3
Training loss: 2.962102897201311
Validation loss: 2.7197180026478565

Epoch: 6| Step: 4
Training loss: 2.7966568504800797
Validation loss: 2.725759089624507

Epoch: 6| Step: 5
Training loss: 3.284691159668485
Validation loss: 2.7371347833763435

Epoch: 6| Step: 6
Training loss: 2.798294985585862
Validation loss: 2.7411633548647045

Epoch: 6| Step: 7
Training loss: 3.111821271436747
Validation loss: 2.7296960432029733

Epoch: 6| Step: 8
Training loss: 3.5314533546431712
Validation loss: 2.7363937833930354

Epoch: 6| Step: 9
Training loss: 3.234998265167196
Validation loss: 2.7249487946685402

Epoch: 6| Step: 10
Training loss: 2.851150273874793
Validation loss: 2.717119909989623

Epoch: 6| Step: 11
Training loss: 2.8837900546053152
Validation loss: 2.7175711005439442

Epoch: 6| Step: 12
Training loss: 3.0305794829641868
Validation loss: 2.716258130879454

Epoch: 6| Step: 13
Training loss: 2.6323353139692665
Validation loss: 2.7152725307237415

Epoch: 236| Step: 0
Training loss: 3.0666171056087372
Validation loss: 2.711133913326659

Epoch: 6| Step: 1
Training loss: 2.9998636214729033
Validation loss: 2.716495296096745

Epoch: 6| Step: 2
Training loss: 2.7259869774208596
Validation loss: 2.7066701260977064

Epoch: 6| Step: 3
Training loss: 3.505976886958151
Validation loss: 2.710861743552812

Epoch: 6| Step: 4
Training loss: 3.606960529429766
Validation loss: 2.707435971028489

Epoch: 6| Step: 5
Training loss: 2.5944180834636077
Validation loss: 2.7137691911089177

Epoch: 6| Step: 6
Training loss: 3.1082476388472355
Validation loss: 2.714848239222448

Epoch: 6| Step: 7
Training loss: 2.369106106927279
Validation loss: 2.713293564980628

Epoch: 6| Step: 8
Training loss: 3.5517038420425187
Validation loss: 2.718528443388227

Epoch: 6| Step: 9
Training loss: 2.8682106125989244
Validation loss: 2.712081090642784

Epoch: 6| Step: 10
Training loss: 2.9435774248404396
Validation loss: 2.713530895772413

Epoch: 6| Step: 11
Training loss: 3.3132232740097582
Validation loss: 2.7201157212357345

Epoch: 6| Step: 12
Training loss: 2.8619798145981883
Validation loss: 2.723718352665813

Epoch: 6| Step: 13
Training loss: 2.4057838682034203
Validation loss: 2.7277686060663053

Epoch: 237| Step: 0
Training loss: 2.832511408649037
Validation loss: 2.7237089573093014

Epoch: 6| Step: 1
Training loss: 3.433539310930782
Validation loss: 2.7159429402875004

Epoch: 6| Step: 2
Training loss: 3.1210772498390504
Validation loss: 2.7305795756593128

Epoch: 6| Step: 3
Training loss: 3.1924022301719033
Validation loss: 2.7216586823071247

Epoch: 6| Step: 4
Training loss: 3.1113592362988634
Validation loss: 2.7388188492368952

Epoch: 6| Step: 5
Training loss: 3.3051047636617055
Validation loss: 2.7212151400582556

Epoch: 6| Step: 6
Training loss: 3.4321536015636833
Validation loss: 2.722447056184861

Epoch: 6| Step: 7
Training loss: 3.0489293142140457
Validation loss: 2.7309253520988817

Epoch: 6| Step: 8
Training loss: 2.7433842930129915
Validation loss: 2.7208710514195418

Epoch: 6| Step: 9
Training loss: 2.854526432130801
Validation loss: 2.718638986318435

Epoch: 6| Step: 10
Training loss: 2.6539569606209
Validation loss: 2.711194397776822

Epoch: 6| Step: 11
Training loss: 2.8831221251408277
Validation loss: 2.713243217054423

Epoch: 6| Step: 12
Training loss: 3.0141885923882397
Validation loss: 2.7113951616809477

Epoch: 6| Step: 13
Training loss: 2.4286322065169212
Validation loss: 2.706697840706835

Epoch: 238| Step: 0
Training loss: 3.2944232809906953
Validation loss: 2.711042529273175

Epoch: 6| Step: 1
Training loss: 2.706256265610981
Validation loss: 2.708181610464272

Epoch: 6| Step: 2
Training loss: 3.479923980584135
Validation loss: 2.707218092631499

Epoch: 6| Step: 3
Training loss: 3.0408448628898475
Validation loss: 2.7054472211866623

Epoch: 6| Step: 4
Training loss: 2.841708613857046
Validation loss: 2.7069458994421325

Epoch: 6| Step: 5
Training loss: 3.417104103804535
Validation loss: 2.713968159653222

Epoch: 6| Step: 6
Training loss: 3.331818586139165
Validation loss: 2.708138264966311

Epoch: 6| Step: 7
Training loss: 2.9495215658384653
Validation loss: 2.7072030992939866

Epoch: 6| Step: 8
Training loss: 2.524809945384987
Validation loss: 2.7165804702529197

Epoch: 6| Step: 9
Training loss: 2.860102826650838
Validation loss: 2.7139499711325974

Epoch: 6| Step: 10
Training loss: 2.703185858068986
Validation loss: 2.724136914823259

Epoch: 6| Step: 11
Training loss: 3.486556665959802
Validation loss: 2.722746948609028

Epoch: 6| Step: 12
Training loss: 2.4054554209474612
Validation loss: 2.735273645166554

Epoch: 6| Step: 13
Training loss: 3.130096095014637
Validation loss: 2.752599977757144

Epoch: 239| Step: 0
Training loss: 2.997975142922397
Validation loss: 2.7466086024887053

Epoch: 6| Step: 1
Training loss: 2.9397240193903684
Validation loss: 2.734283231962036

Epoch: 6| Step: 2
Training loss: 3.334448024018058
Validation loss: 2.7488578240438493

Epoch: 6| Step: 3
Training loss: 2.3924699034835752
Validation loss: 2.7346138161059828

Epoch: 6| Step: 4
Training loss: 3.4282032876825212
Validation loss: 2.7410872506715758

Epoch: 6| Step: 5
Training loss: 2.471065159987155
Validation loss: 2.7254574611386304

Epoch: 6| Step: 6
Training loss: 2.8141455181103963
Validation loss: 2.723342466055017

Epoch: 6| Step: 7
Training loss: 3.067115575303694
Validation loss: 2.7129518224758953

Epoch: 6| Step: 8
Training loss: 2.7154156638211835
Validation loss: 2.706689216920013

Epoch: 6| Step: 9
Training loss: 3.3533858028071064
Validation loss: 2.70311464630995

Epoch: 6| Step: 10
Training loss: 3.584433512388858
Validation loss: 2.7022108089505172

Epoch: 6| Step: 11
Training loss: 3.197362116509382
Validation loss: 2.705293691985576

Epoch: 6| Step: 12
Training loss: 2.826159811356639
Validation loss: 2.7058849178361903

Epoch: 6| Step: 13
Training loss: 3.1587808867296654
Validation loss: 2.7083160595465263

Epoch: 240| Step: 0
Training loss: 2.9039908211302916
Validation loss: 2.7024675241027536

Epoch: 6| Step: 1
Training loss: 2.865901981059203
Validation loss: 2.7020165459162317

Epoch: 6| Step: 2
Training loss: 3.6062270138964956
Validation loss: 2.705225543595008

Epoch: 6| Step: 3
Training loss: 3.14240438432679
Validation loss: 2.705418197456464

Epoch: 6| Step: 4
Training loss: 3.286600455441905
Validation loss: 2.7034609996745207

Epoch: 6| Step: 5
Training loss: 3.1753177093382035
Validation loss: 2.70373893128351

Epoch: 6| Step: 6
Training loss: 3.514653185994639
Validation loss: 2.70099743331183

Epoch: 6| Step: 7
Training loss: 2.4729998743016344
Validation loss: 2.7063172503001853

Epoch: 6| Step: 8
Training loss: 2.98046667016433
Validation loss: 2.7101248896553387

Epoch: 6| Step: 9
Training loss: 3.2665458001593324
Validation loss: 2.708843011597676

Epoch: 6| Step: 10
Training loss: 2.319146373832251
Validation loss: 2.7111027008746333

Epoch: 6| Step: 11
Training loss: 2.8010781528474666
Validation loss: 2.70673429141221

Epoch: 6| Step: 12
Training loss: 3.1186160683780555
Validation loss: 2.710628207114169

Epoch: 6| Step: 13
Training loss: 2.4413716794427387
Validation loss: 2.7103103678772333

Epoch: 241| Step: 0
Training loss: 3.4395322860974766
Validation loss: 2.705000345934707

Epoch: 6| Step: 1
Training loss: 2.8971465904220386
Validation loss: 2.708797486684929

Epoch: 6| Step: 2
Training loss: 2.7781386384747218
Validation loss: 2.714018879996848

Epoch: 6| Step: 3
Training loss: 2.9459117816010014
Validation loss: 2.716626946202752

Epoch: 6| Step: 4
Training loss: 3.0347100399964853
Validation loss: 2.7223203320626497

Epoch: 6| Step: 5
Training loss: 3.127726624203975
Validation loss: 2.715743304591962

Epoch: 6| Step: 6
Training loss: 3.589525874927621
Validation loss: 2.7084196826304825

Epoch: 6| Step: 7
Training loss: 3.342939189397742
Validation loss: 2.707002374891941

Epoch: 6| Step: 8
Training loss: 3.218639038543622
Validation loss: 2.703977083629442

Epoch: 6| Step: 9
Training loss: 2.468213856098133
Validation loss: 2.705292119852738

Epoch: 6| Step: 10
Training loss: 2.9550335887671713
Validation loss: 2.7047213743128626

Epoch: 6| Step: 11
Training loss: 2.483566920497631
Validation loss: 2.700426752204298

Epoch: 6| Step: 12
Training loss: 2.933820164625265
Validation loss: 2.705696730550702

Epoch: 6| Step: 13
Training loss: 2.906455084014805
Validation loss: 2.7018025933618324

Epoch: 242| Step: 0
Training loss: 2.9121503104307633
Validation loss: 2.7060953844034326

Epoch: 6| Step: 1
Training loss: 3.344315810773184
Validation loss: 2.7069693371856784

Epoch: 6| Step: 2
Training loss: 2.827896783377665
Validation loss: 2.7109621382624445

Epoch: 6| Step: 3
Training loss: 3.297894794699233
Validation loss: 2.7017806308885306

Epoch: 6| Step: 4
Training loss: 2.598683456735927
Validation loss: 2.7056355878522034

Epoch: 6| Step: 5
Training loss: 2.9100382736153576
Validation loss: 2.7042823583848583

Epoch: 6| Step: 6
Training loss: 2.6749963207754344
Validation loss: 2.710553623326649

Epoch: 6| Step: 7
Training loss: 3.118229206765483
Validation loss: 2.720558819852341

Epoch: 6| Step: 8
Training loss: 2.817093552362731
Validation loss: 2.7234750531234244

Epoch: 6| Step: 9
Training loss: 2.820398133898388
Validation loss: 2.71699760706724

Epoch: 6| Step: 10
Training loss: 3.454270392970693
Validation loss: 2.7305637294805076

Epoch: 6| Step: 11
Training loss: 3.298592937932509
Validation loss: 2.7227099910278003

Epoch: 6| Step: 12
Training loss: 3.1112557861960934
Validation loss: 2.717237105580125

Epoch: 6| Step: 13
Training loss: 2.9856757879219735
Validation loss: 2.714451152288845

Epoch: 243| Step: 0
Training loss: 3.3322610719943166
Validation loss: 2.7052948935910113

Epoch: 6| Step: 1
Training loss: 2.852655906595737
Validation loss: 2.7028324965469284

Epoch: 6| Step: 2
Training loss: 3.024962205887191
Validation loss: 2.7040106992535975

Epoch: 6| Step: 3
Training loss: 2.886133281715212
Validation loss: 2.69725634660239

Epoch: 6| Step: 4
Training loss: 3.408354502816589
Validation loss: 2.701182062929454

Epoch: 6| Step: 5
Training loss: 2.879809296175083
Validation loss: 2.7020749507712707

Epoch: 6| Step: 6
Training loss: 2.9905960193214414
Validation loss: 2.700214676961033

Epoch: 6| Step: 7
Training loss: 3.0872521799067316
Validation loss: 2.6943765202556182

Epoch: 6| Step: 8
Training loss: 2.327698329256591
Validation loss: 2.697221956754009

Epoch: 6| Step: 9
Training loss: 3.2998938456714324
Validation loss: 2.697760818154096

Epoch: 6| Step: 10
Training loss: 2.7762528005806555
Validation loss: 2.700594688273688

Epoch: 6| Step: 11
Training loss: 3.0615673591347035
Validation loss: 2.6956175710108017

Epoch: 6| Step: 12
Training loss: 3.1657695168649176
Validation loss: 2.6987081160191533

Epoch: 6| Step: 13
Training loss: 3.1908013693908073
Validation loss: 2.696977237443278

Epoch: 244| Step: 0
Training loss: 2.7521251789832264
Validation loss: 2.695422910783262

Epoch: 6| Step: 1
Training loss: 3.4915940294623526
Validation loss: 2.7012514061697854

Epoch: 6| Step: 2
Training loss: 3.25706784453574
Validation loss: 2.7017192695664134

Epoch: 6| Step: 3
Training loss: 3.430704231146492
Validation loss: 2.705610087158069

Epoch: 6| Step: 4
Training loss: 3.6850634785002243
Validation loss: 2.7122157273449448

Epoch: 6| Step: 5
Training loss: 3.158340517168422
Validation loss: 2.7054932961571265

Epoch: 6| Step: 6
Training loss: 2.678567012601573
Validation loss: 2.701457688096031

Epoch: 6| Step: 7
Training loss: 3.2671176839089
Validation loss: 2.7108419236645496

Epoch: 6| Step: 8
Training loss: 1.911517884646004
Validation loss: 2.703922325560292

Epoch: 6| Step: 9
Training loss: 3.1728874808004637
Validation loss: 2.707265734018749

Epoch: 6| Step: 10
Training loss: 2.7272642265534133
Validation loss: 2.711299820843717

Epoch: 6| Step: 11
Training loss: 2.0156706575648875
Validation loss: 2.707915228540557

Epoch: 6| Step: 12
Training loss: 3.1902587022784497
Validation loss: 2.713903472010392

Epoch: 6| Step: 13
Training loss: 2.8765012719032024
Validation loss: 2.7052126619888113

Epoch: 245| Step: 0
Training loss: 2.844342348216871
Validation loss: 2.717279929967096

Epoch: 6| Step: 1
Training loss: 2.6089446118325057
Validation loss: 2.700748386827579

Epoch: 6| Step: 2
Training loss: 4.03770131651497
Validation loss: 2.7083671905319706

Epoch: 6| Step: 3
Training loss: 3.582196580239217
Validation loss: 2.694921522301292

Epoch: 6| Step: 4
Training loss: 3.3340209887415435
Validation loss: 2.6952463706358802

Epoch: 6| Step: 5
Training loss: 2.551924304808178
Validation loss: 2.69330557565154

Epoch: 6| Step: 6
Training loss: 3.0962166248997605
Validation loss: 2.6954810286152378

Epoch: 6| Step: 7
Training loss: 2.8146761634439965
Validation loss: 2.696858017383327

Epoch: 6| Step: 8
Training loss: 2.906174361362397
Validation loss: 2.69689800419137

Epoch: 6| Step: 9
Training loss: 2.297659396182711
Validation loss: 2.7017003495364196

Epoch: 6| Step: 10
Training loss: 2.9758610737480713
Validation loss: 2.7013445383414556

Epoch: 6| Step: 11
Training loss: 2.9774421733682677
Validation loss: 2.711449014336468

Epoch: 6| Step: 12
Training loss: 3.0972153492971515
Validation loss: 2.7047755020179407

Epoch: 6| Step: 13
Training loss: 2.555628611953209
Validation loss: 2.6960846244258754

Epoch: 246| Step: 0
Training loss: 3.1474541200379935
Validation loss: 2.6948764404891143

Epoch: 6| Step: 1
Training loss: 2.678070915325927
Validation loss: 2.7008944983760133

Epoch: 6| Step: 2
Training loss: 3.1495310964204397
Validation loss: 2.6933443406836632

Epoch: 6| Step: 3
Training loss: 3.133471184261578
Validation loss: 2.6924996028062007

Epoch: 6| Step: 4
Training loss: 3.1124938964783904
Validation loss: 2.69968061848356

Epoch: 6| Step: 5
Training loss: 3.056954793658819
Validation loss: 2.693161082802248

Epoch: 6| Step: 6
Training loss: 2.558862197421316
Validation loss: 2.6908429267687466

Epoch: 6| Step: 7
Training loss: 3.271326955167799
Validation loss: 2.6955434002683782

Epoch: 6| Step: 8
Training loss: 2.8549962937953963
Validation loss: 2.6900949410785615

Epoch: 6| Step: 9
Training loss: 3.2035260205332183
Validation loss: 2.694078464628306

Epoch: 6| Step: 10
Training loss: 3.094005227398294
Validation loss: 2.697264058642962

Epoch: 6| Step: 11
Training loss: 3.229379232650059
Validation loss: 2.6955654164135123

Epoch: 6| Step: 12
Training loss: 2.561856817146989
Validation loss: 2.701831865628195

Epoch: 6| Step: 13
Training loss: 3.218198266009368
Validation loss: 2.7058905531529476

Epoch: 247| Step: 0
Training loss: 3.2006407990202566
Validation loss: 2.7081796405306156

Epoch: 6| Step: 1
Training loss: 2.673886529077036
Validation loss: 2.715043019379377

Epoch: 6| Step: 2
Training loss: 2.949169921470787
Validation loss: 2.710096011562454

Epoch: 6| Step: 3
Training loss: 2.7542237876551265
Validation loss: 2.716895031108515

Epoch: 6| Step: 4
Training loss: 3.217974226711472
Validation loss: 2.720340918162293

Epoch: 6| Step: 5
Training loss: 2.3810994979275355
Validation loss: 2.7149534731954263

Epoch: 6| Step: 6
Training loss: 3.462944189562109
Validation loss: 2.715856902243604

Epoch: 6| Step: 7
Training loss: 3.1398082069574014
Validation loss: 2.701179242733722

Epoch: 6| Step: 8
Training loss: 2.9653811966557506
Validation loss: 2.695455731448286

Epoch: 6| Step: 9
Training loss: 3.3160059387374727
Validation loss: 2.7017007120157954

Epoch: 6| Step: 10
Training loss: 3.5114805758184944
Validation loss: 2.701332423074677

Epoch: 6| Step: 11
Training loss: 2.747943976562775
Validation loss: 2.7030454803289627

Epoch: 6| Step: 12
Training loss: 2.4428095576305315
Validation loss: 2.6986426597975046

Epoch: 6| Step: 13
Training loss: 3.3981307669585914
Validation loss: 2.693643504467503

Epoch: 248| Step: 0
Training loss: 3.8147045078056254
Validation loss: 2.697488300769845

Epoch: 6| Step: 1
Training loss: 3.4866812561877487
Validation loss: 2.695188593977594

Epoch: 6| Step: 2
Training loss: 1.9876546115837004
Validation loss: 2.692595704542246

Epoch: 6| Step: 3
Training loss: 2.350854000184164
Validation loss: 2.701868029960438

Epoch: 6| Step: 4
Training loss: 3.052889946251876
Validation loss: 2.69298040657371

Epoch: 6| Step: 5
Training loss: 2.7338421111599005
Validation loss: 2.6924201168597457

Epoch: 6| Step: 6
Training loss: 2.6706684801600535
Validation loss: 2.6888372476420592

Epoch: 6| Step: 7
Training loss: 3.307102772682529
Validation loss: 2.6922778837182597

Epoch: 6| Step: 8
Training loss: 2.757059226754838
Validation loss: 2.696159382166867

Epoch: 6| Step: 9
Training loss: 3.2087825774384373
Validation loss: 2.698587486637647

Epoch: 6| Step: 10
Training loss: 3.147645154882794
Validation loss: 2.6947391689089835

Epoch: 6| Step: 11
Training loss: 3.310530454881845
Validation loss: 2.699173353397046

Epoch: 6| Step: 12
Training loss: 2.9358697791052113
Validation loss: 2.7017017652932878

Epoch: 6| Step: 13
Training loss: 3.0312032794301387
Validation loss: 2.6911110538620866

Epoch: 249| Step: 0
Training loss: 3.2887499155575934
Validation loss: 2.696046012901546

Epoch: 6| Step: 1
Training loss: 2.69119168825669
Validation loss: 2.698618503747944

Epoch: 6| Step: 2
Training loss: 2.8713687687158944
Validation loss: 2.7072144771169198

Epoch: 6| Step: 3
Training loss: 3.289567704714242
Validation loss: 2.7001608157680668

Epoch: 6| Step: 4
Training loss: 2.666173283313381
Validation loss: 2.6992333987586634

Epoch: 6| Step: 5
Training loss: 3.372450536728547
Validation loss: 2.6935077081373846

Epoch: 6| Step: 6
Training loss: 2.7366849025470317
Validation loss: 2.6971620914050294

Epoch: 6| Step: 7
Training loss: 3.422368122956787
Validation loss: 2.692600466979881

Epoch: 6| Step: 8
Training loss: 2.897944405864264
Validation loss: 2.7015258043734294

Epoch: 6| Step: 9
Training loss: 2.5350394435123835
Validation loss: 2.6910647088303463

Epoch: 6| Step: 10
Training loss: 3.134908607525754
Validation loss: 2.6854971347651144

Epoch: 6| Step: 11
Training loss: 2.865337554592445
Validation loss: 2.6829575231965417

Epoch: 6| Step: 12
Training loss: 3.2076072036298835
Validation loss: 2.6924730526853833

Epoch: 6| Step: 13
Training loss: 3.0900091809380195
Validation loss: 2.6831964910252246

Epoch: 250| Step: 0
Training loss: 2.4941820157648666
Validation loss: 2.6917665378145563

Epoch: 6| Step: 1
Training loss: 2.797732600001444
Validation loss: 2.6885622600442423

Epoch: 6| Step: 2
Training loss: 3.0661779622426644
Validation loss: 2.686050609907725

Epoch: 6| Step: 3
Training loss: 3.0841569230149832
Validation loss: 2.69160589824312

Epoch: 6| Step: 4
Training loss: 2.7828261806145336
Validation loss: 2.6887182187132836

Epoch: 6| Step: 5
Training loss: 3.404996028679368
Validation loss: 2.690930961180528

Epoch: 6| Step: 6
Training loss: 3.3225219555912258
Validation loss: 2.6922367341720945

Epoch: 6| Step: 7
Training loss: 3.6220799227679867
Validation loss: 2.6942913197408043

Epoch: 6| Step: 8
Training loss: 2.4879336031919346
Validation loss: 2.693179544035924

Epoch: 6| Step: 9
Training loss: 2.3978376104302086
Validation loss: 2.6887601295794834

Epoch: 6| Step: 10
Training loss: 2.86721427551573
Validation loss: 2.696128841672682

Epoch: 6| Step: 11
Training loss: 3.43809642819403
Validation loss: 2.7061054813242866

Epoch: 6| Step: 12
Training loss: 3.1760969955180784
Validation loss: 2.702990090690923

Epoch: 6| Step: 13
Training loss: 2.8465939229147343
Validation loss: 2.6984553621038923

Epoch: 251| Step: 0
Training loss: 2.5788493670477077
Validation loss: 2.6951085386048166

Epoch: 6| Step: 1
Training loss: 2.1592302596830786
Validation loss: 2.6965411267474515

Epoch: 6| Step: 2
Training loss: 2.9908635569487587
Validation loss: 2.6952804232997107

Epoch: 6| Step: 3
Training loss: 3.337239281689941
Validation loss: 2.687384301242337

Epoch: 6| Step: 4
Training loss: 3.124699387396599
Validation loss: 2.6963689192896214

Epoch: 6| Step: 5
Training loss: 2.8754658528931945
Validation loss: 2.695995554452744

Epoch: 6| Step: 6
Training loss: 3.2283457707323224
Validation loss: 2.6883986569120544

Epoch: 6| Step: 7
Training loss: 3.5796674710349894
Validation loss: 2.691557673927705

Epoch: 6| Step: 8
Training loss: 3.0838206309938108
Validation loss: 2.691727389094505

Epoch: 6| Step: 9
Training loss: 3.0346921273802057
Validation loss: 2.690846683371508

Epoch: 6| Step: 10
Training loss: 3.496294239624889
Validation loss: 2.6870555944280867

Epoch: 6| Step: 11
Training loss: 3.3916435996980945
Validation loss: 2.6848594960898304

Epoch: 6| Step: 12
Training loss: 2.5153609427480275
Validation loss: 2.692072660124521

Epoch: 6| Step: 13
Training loss: 1.7944864070671935
Validation loss: 2.6900363933396116

Epoch: 252| Step: 0
Training loss: 2.6828268362916403
Validation loss: 2.68412655080756

Epoch: 6| Step: 1
Training loss: 2.829331283012508
Validation loss: 2.689362977469287

Epoch: 6| Step: 2
Training loss: 3.0542357127740765
Validation loss: 2.6852803757219124

Epoch: 6| Step: 3
Training loss: 2.6710342958477202
Validation loss: 2.6843138693711825

Epoch: 6| Step: 4
Training loss: 2.641330241091576
Validation loss: 2.6828286413710685

Epoch: 6| Step: 5
Training loss: 2.684928994971437
Validation loss: 2.698961283200897

Epoch: 6| Step: 6
Training loss: 3.000175788815373
Validation loss: 2.693023296356771

Epoch: 6| Step: 7
Training loss: 3.1990783437023715
Validation loss: 2.6930891157185886

Epoch: 6| Step: 8
Training loss: 3.2085694725691596
Validation loss: 2.6945266840761946

Epoch: 6| Step: 9
Training loss: 2.722062296505561
Validation loss: 2.7074624818896385

Epoch: 6| Step: 10
Training loss: 3.6027078193480797
Validation loss: 2.7085623837552175

Epoch: 6| Step: 11
Training loss: 3.788063405248468
Validation loss: 2.698769415664863

Epoch: 6| Step: 12
Training loss: 2.9683493896027273
Validation loss: 2.703944563604363

Epoch: 6| Step: 13
Training loss: 2.638203520755184
Validation loss: 2.710449523782115

Epoch: 253| Step: 0
Training loss: 3.07859589513578
Validation loss: 2.7011650150077817

Epoch: 6| Step: 1
Training loss: 2.334418475909258
Validation loss: 2.6975536053844644

Epoch: 6| Step: 2
Training loss: 2.9337039525530835
Validation loss: 2.688562331559434

Epoch: 6| Step: 3
Training loss: 3.1548123248544773
Validation loss: 2.6905226711546026

Epoch: 6| Step: 4
Training loss: 3.156237309496455
Validation loss: 2.6928543511713086

Epoch: 6| Step: 5
Training loss: 3.23696206237415
Validation loss: 2.69154561461829

Epoch: 6| Step: 6
Training loss: 3.1098322699844316
Validation loss: 2.681629997159284

Epoch: 6| Step: 7
Training loss: 3.139213890272046
Validation loss: 2.6828901710150665

Epoch: 6| Step: 8
Training loss: 3.020640417494164
Validation loss: 2.69032253658497

Epoch: 6| Step: 9
Training loss: 2.6776005484029652
Validation loss: 2.689721086589994

Epoch: 6| Step: 10
Training loss: 2.8838704140023874
Validation loss: 2.6919310771691

Epoch: 6| Step: 11
Training loss: 2.410073549406699
Validation loss: 2.6923653360799613

Epoch: 6| Step: 12
Training loss: 3.427915493345278
Validation loss: 2.713053582749164

Epoch: 6| Step: 13
Training loss: 3.546041113342406
Validation loss: 2.709842511927919

Epoch: 254| Step: 0
Training loss: 2.9817758796127243
Validation loss: 2.7225502864398137

Epoch: 6| Step: 1
Training loss: 3.3746836478706514
Validation loss: 2.735440425008774

Epoch: 6| Step: 2
Training loss: 2.9130841369316425
Validation loss: 2.7181900535951136

Epoch: 6| Step: 3
Training loss: 2.6627793787846126
Validation loss: 2.7164037205660154

Epoch: 6| Step: 4
Training loss: 3.473610985971278
Validation loss: 2.6927284269645715

Epoch: 6| Step: 5
Training loss: 2.5305773936377505
Validation loss: 2.6906585924127886

Epoch: 6| Step: 6
Training loss: 3.224467116694712
Validation loss: 2.6807211719674737

Epoch: 6| Step: 7
Training loss: 3.446693195656578
Validation loss: 2.679361825439171

Epoch: 6| Step: 8
Training loss: 3.107846138198186
Validation loss: 2.6853023738589035

Epoch: 6| Step: 9
Training loss: 2.7232425447020674
Validation loss: 2.684683162748973

Epoch: 6| Step: 10
Training loss: 3.332158040432791
Validation loss: 2.6820942693885246

Epoch: 6| Step: 11
Training loss: 2.3784511989088237
Validation loss: 2.680087163505944

Epoch: 6| Step: 12
Training loss: 2.8007990412233483
Validation loss: 2.6832836584732442

Epoch: 6| Step: 13
Training loss: 2.917316164633369
Validation loss: 2.678729650489728

Epoch: 255| Step: 0
Training loss: 2.0895427736686134
Validation loss: 2.680635115682381

Epoch: 6| Step: 1
Training loss: 2.926802778996411
Validation loss: 2.681406582875138

Epoch: 6| Step: 2
Training loss: 3.017623002295743
Validation loss: 2.6820752720927694

Epoch: 6| Step: 3
Training loss: 2.6790325821407763
Validation loss: 2.6826404354743274

Epoch: 6| Step: 4
Training loss: 3.005548749889755
Validation loss: 2.680986301871312

Epoch: 6| Step: 5
Training loss: 3.3841290808153457
Validation loss: 2.68544981267045

Epoch: 6| Step: 6
Training loss: 3.019810751233693
Validation loss: 2.6844858068302457

Epoch: 6| Step: 7
Training loss: 3.3645809555438984
Validation loss: 2.6867806000479297

Epoch: 6| Step: 8
Training loss: 3.2907555361339424
Validation loss: 2.6885327450855088

Epoch: 6| Step: 9
Training loss: 3.1893711955567157
Validation loss: 2.6932723318552996

Epoch: 6| Step: 10
Training loss: 3.2288753911391357
Validation loss: 2.697391820204228

Epoch: 6| Step: 11
Training loss: 3.1571762689796405
Validation loss: 2.710965441440736

Epoch: 6| Step: 12
Training loss: 2.8746384103031524
Validation loss: 2.701351418756254

Epoch: 6| Step: 13
Training loss: 2.230014841688914
Validation loss: 2.704844004849497

Epoch: 256| Step: 0
Training loss: 3.568705190721314
Validation loss: 2.702930903207274

Epoch: 6| Step: 1
Training loss: 2.9148093895074245
Validation loss: 2.716594152000523

Epoch: 6| Step: 2
Training loss: 2.484571053308641
Validation loss: 2.7044254560320184

Epoch: 6| Step: 3
Training loss: 3.1848615310248194
Validation loss: 2.6902961026761587

Epoch: 6| Step: 4
Training loss: 3.1792920215802583
Validation loss: 2.690666009895963

Epoch: 6| Step: 5
Training loss: 2.909436191853634
Validation loss: 2.6892588269521256

Epoch: 6| Step: 6
Training loss: 2.4000144520960234
Validation loss: 2.6877997217651792

Epoch: 6| Step: 7
Training loss: 2.4031492790116795
Validation loss: 2.6841104437557584

Epoch: 6| Step: 8
Training loss: 3.57199624590874
Validation loss: 2.6832212636305033

Epoch: 6| Step: 9
Training loss: 3.031972897249941
Validation loss: 2.693656390006175

Epoch: 6| Step: 10
Training loss: 2.989517016234637
Validation loss: 2.685402232938896

Epoch: 6| Step: 11
Training loss: 2.978280280148936
Validation loss: 2.6876961278071247

Epoch: 6| Step: 12
Training loss: 3.0414044689581865
Validation loss: 2.6807289573978736

Epoch: 6| Step: 13
Training loss: 3.2487290171304846
Validation loss: 2.688699722556425

Epoch: 257| Step: 0
Training loss: 2.9874654217239223
Validation loss: 2.679756930202292

Epoch: 6| Step: 1
Training loss: 2.5130088898106457
Validation loss: 2.6835170391633967

Epoch: 6| Step: 2
Training loss: 2.5284471417409304
Validation loss: 2.6799312717335053

Epoch: 6| Step: 3
Training loss: 3.389675306226339
Validation loss: 2.6853658163091754

Epoch: 6| Step: 4
Training loss: 2.6649000355936487
Validation loss: 2.681174126675549

Epoch: 6| Step: 5
Training loss: 3.3574207388014368
Validation loss: 2.681285089057196

Epoch: 6| Step: 6
Training loss: 3.060355233410508
Validation loss: 2.683898577934377

Epoch: 6| Step: 7
Training loss: 3.0808483798301327
Validation loss: 2.6926331925630196

Epoch: 6| Step: 8
Training loss: 3.6427793227359837
Validation loss: 2.696300848632393

Epoch: 6| Step: 9
Training loss: 3.3081097569736966
Validation loss: 2.6918777473600426

Epoch: 6| Step: 10
Training loss: 3.00268275788616
Validation loss: 2.7020119727546286

Epoch: 6| Step: 11
Training loss: 3.036193591834949
Validation loss: 2.718899266501016

Epoch: 6| Step: 12
Training loss: 2.3283384084498318
Validation loss: 2.722668869232477

Epoch: 6| Step: 13
Training loss: 2.838975860331834
Validation loss: 2.708180610824883

Epoch: 258| Step: 0
Training loss: 2.751606038760183
Validation loss: 2.703056891775511

Epoch: 6| Step: 1
Training loss: 2.580035986834171
Validation loss: 2.686585074325522

Epoch: 6| Step: 2
Training loss: 3.1484465871069234
Validation loss: 2.6786168949235494

Epoch: 6| Step: 3
Training loss: 3.3396548569116327
Validation loss: 2.6760799640327075

Epoch: 6| Step: 4
Training loss: 3.057597381712842
Validation loss: 2.6845709313099944

Epoch: 6| Step: 5
Training loss: 3.862179744739526
Validation loss: 2.6764130974114946

Epoch: 6| Step: 6
Training loss: 2.455398963472987
Validation loss: 2.6737095813670178

Epoch: 6| Step: 7
Training loss: 2.7863700115906536
Validation loss: 2.679120966358031

Epoch: 6| Step: 8
Training loss: 3.0529616375639264
Validation loss: 2.685564149504559

Epoch: 6| Step: 9
Training loss: 2.880436111650168
Validation loss: 2.689961732283479

Epoch: 6| Step: 10
Training loss: 3.3817187648900626
Validation loss: 2.6857521476564705

Epoch: 6| Step: 11
Training loss: 2.9381450188398457
Validation loss: 2.690618717730845

Epoch: 6| Step: 12
Training loss: 2.8597808716597735
Validation loss: 2.6969341035043737

Epoch: 6| Step: 13
Training loss: 2.382102885659524
Validation loss: 2.717382379129313

Epoch: 259| Step: 0
Training loss: 2.399426582499387
Validation loss: 2.7205261550292774

Epoch: 6| Step: 1
Training loss: 2.740537141372506
Validation loss: 2.7291502874979265

Epoch: 6| Step: 2
Training loss: 3.342601543245066
Validation loss: 2.761645779947655

Epoch: 6| Step: 3
Training loss: 3.402167717532846
Validation loss: 2.7530403818766502

Epoch: 6| Step: 4
Training loss: 2.952449822318426
Validation loss: 2.7723383924297953

Epoch: 6| Step: 5
Training loss: 3.5605961581202528
Validation loss: 2.7622572208322436

Epoch: 6| Step: 6
Training loss: 2.8971508697211936
Validation loss: 2.7548384681140172

Epoch: 6| Step: 7
Training loss: 2.5225337154982084
Validation loss: 2.7483267723168407

Epoch: 6| Step: 8
Training loss: 3.4420042178778156
Validation loss: 2.7488600520728665

Epoch: 6| Step: 9
Training loss: 3.579039077941948
Validation loss: 2.744431257748347

Epoch: 6| Step: 10
Training loss: 2.7816252723144816
Validation loss: 2.7281946253030074

Epoch: 6| Step: 11
Training loss: 3.032596407094547
Validation loss: 2.731743190412062

Epoch: 6| Step: 12
Training loss: 2.698030908785057
Validation loss: 2.7209664554294934

Epoch: 6| Step: 13
Training loss: 2.9400008898363907
Validation loss: 2.7212690904243226

Epoch: 260| Step: 0
Training loss: 3.6050317567651406
Validation loss: 2.718589094315844

Epoch: 6| Step: 1
Training loss: 2.644727420715188
Validation loss: 2.7116857464607165

Epoch: 6| Step: 2
Training loss: 2.9146616629126836
Validation loss: 2.70242128655279

Epoch: 6| Step: 3
Training loss: 3.786431537657544
Validation loss: 2.7030072821717748

Epoch: 6| Step: 4
Training loss: 2.6066479166467116
Validation loss: 2.7003560876237285

Epoch: 6| Step: 5
Training loss: 3.3782885036370343
Validation loss: 2.705381017365324

Epoch: 6| Step: 6
Training loss: 2.3173496840368024
Validation loss: 2.7114362038986393

Epoch: 6| Step: 7
Training loss: 2.2296112068880114
Validation loss: 2.709594641735641

Epoch: 6| Step: 8
Training loss: 3.5442425205614336
Validation loss: 2.718958486357331

Epoch: 6| Step: 9
Training loss: 3.1833205404240927
Validation loss: 2.7108728469014545

Epoch: 6| Step: 10
Training loss: 2.6366856551742495
Validation loss: 2.7188347414412344

Epoch: 6| Step: 11
Training loss: 3.174875878738876
Validation loss: 2.7026486436738804

Epoch: 6| Step: 12
Training loss: 2.307708725504061
Validation loss: 2.683489102343081

Epoch: 6| Step: 13
Training loss: 3.5877674229068393
Validation loss: 2.6906609601023175

Epoch: 261| Step: 0
Training loss: 3.4818133187775024
Validation loss: 2.692433919475156

Epoch: 6| Step: 1
Training loss: 1.8650122065822206
Validation loss: 2.69233370988903

Epoch: 6| Step: 2
Training loss: 3.1983417804149927
Validation loss: 2.686271319840539

Epoch: 6| Step: 3
Training loss: 2.5822138873213984
Validation loss: 2.6885352672144704

Epoch: 6| Step: 4
Training loss: 3.239726628674098
Validation loss: 2.687254580419171

Epoch: 6| Step: 5
Training loss: 2.749835442908126
Validation loss: 2.686725186157695

Epoch: 6| Step: 6
Training loss: 2.6643864499015426
Validation loss: 2.687133525597606

Epoch: 6| Step: 7
Training loss: 3.078502651226469
Validation loss: 2.6862718151476797

Epoch: 6| Step: 8
Training loss: 2.871944794413821
Validation loss: 2.689096605650761

Epoch: 6| Step: 9
Training loss: 3.121660508127514
Validation loss: 2.691676794447253

Epoch: 6| Step: 10
Training loss: 3.704036938369825
Validation loss: 2.695934375264692

Epoch: 6| Step: 11
Training loss: 3.0288167409435203
Validation loss: 2.696486885089943

Epoch: 6| Step: 12
Training loss: 3.2977292368858926
Validation loss: 2.697730466842628

Epoch: 6| Step: 13
Training loss: 2.5394792889589617
Validation loss: 2.6943827457705654

Epoch: 262| Step: 0
Training loss: 3.062466991013115
Validation loss: 2.6847385213880615

Epoch: 6| Step: 1
Training loss: 3.0727625576535806
Validation loss: 2.68614563962093

Epoch: 6| Step: 2
Training loss: 2.683374654163472
Validation loss: 2.67228253776695

Epoch: 6| Step: 3
Training loss: 2.5828364930189465
Validation loss: 2.678837721596877

Epoch: 6| Step: 4
Training loss: 3.200941430130132
Validation loss: 2.6704204432983225

Epoch: 6| Step: 5
Training loss: 3.501595269815424
Validation loss: 2.664317498137079

Epoch: 6| Step: 6
Training loss: 3.6485535564383893
Validation loss: 2.669791707086367

Epoch: 6| Step: 7
Training loss: 2.8557471101818543
Validation loss: 2.671105089258241

Epoch: 6| Step: 8
Training loss: 3.1106474300018085
Validation loss: 2.6685664218964322

Epoch: 6| Step: 9
Training loss: 3.077324932706177
Validation loss: 2.671125534153517

Epoch: 6| Step: 10
Training loss: 3.328218413998591
Validation loss: 2.6721766297469074

Epoch: 6| Step: 11
Training loss: 2.624184981478364
Validation loss: 2.670604630965421

Epoch: 6| Step: 12
Training loss: 2.700382760079363
Validation loss: 2.6689347061469917

Epoch: 6| Step: 13
Training loss: 1.7397310875677197
Validation loss: 2.6677494318363104

Epoch: 263| Step: 0
Training loss: 2.4840962955404082
Validation loss: 2.666479902751689

Epoch: 6| Step: 1
Training loss: 3.388196787662108
Validation loss: 2.668864500671099

Epoch: 6| Step: 2
Training loss: 3.5380790640890316
Validation loss: 2.6693507640205136

Epoch: 6| Step: 3
Training loss: 2.5157176883278702
Validation loss: 2.6754367137988915

Epoch: 6| Step: 4
Training loss: 3.6511208838096443
Validation loss: 2.6741800593988394

Epoch: 6| Step: 5
Training loss: 2.9527516608420474
Validation loss: 2.679378298789174

Epoch: 6| Step: 6
Training loss: 3.250566726570042
Validation loss: 2.678875536123784

Epoch: 6| Step: 7
Training loss: 2.440129548995575
Validation loss: 2.6832957014108563

Epoch: 6| Step: 8
Training loss: 3.026671894954025
Validation loss: 2.6820987493852795

Epoch: 6| Step: 9
Training loss: 2.7552107947374007
Validation loss: 2.695200290754791

Epoch: 6| Step: 10
Training loss: 3.9198687159683354
Validation loss: 2.6886194591938457

Epoch: 6| Step: 11
Training loss: 1.9539414797310024
Validation loss: 2.6887357703267103

Epoch: 6| Step: 12
Training loss: 2.514375269598476
Validation loss: 2.678709254061436

Epoch: 6| Step: 13
Training loss: 2.946518709199402
Validation loss: 2.6867629908159105

Epoch: 264| Step: 0
Training loss: 2.5577799427381613
Validation loss: 2.6759599346215133

Epoch: 6| Step: 1
Training loss: 2.825088135011199
Validation loss: 2.665399692319657

Epoch: 6| Step: 2
Training loss: 2.8096272526110115
Validation loss: 2.6713677168037133

Epoch: 6| Step: 3
Training loss: 2.911983290257562
Validation loss: 2.663390094988449

Epoch: 6| Step: 4
Training loss: 3.009518147170642
Validation loss: 2.665668079143673

Epoch: 6| Step: 5
Training loss: 3.044319528799928
Validation loss: 2.6646394586906297

Epoch: 6| Step: 6
Training loss: 2.720459674532831
Validation loss: 2.6648554738198538

Epoch: 6| Step: 7
Training loss: 3.157263111642762
Validation loss: 2.665231371002739

Epoch: 6| Step: 8
Training loss: 3.6436638326254944
Validation loss: 2.6649874494078887

Epoch: 6| Step: 9
Training loss: 2.3306913632074466
Validation loss: 2.663799137787461

Epoch: 6| Step: 10
Training loss: 3.163449677825734
Validation loss: 2.664117358518004

Epoch: 6| Step: 11
Training loss: 3.374731971552967
Validation loss: 2.662392855902347

Epoch: 6| Step: 12
Training loss: 3.173161588664126
Validation loss: 2.669807970620738

Epoch: 6| Step: 13
Training loss: 3.0011022450190854
Validation loss: 2.663346751102256

Epoch: 265| Step: 0
Training loss: 3.11379981418795
Validation loss: 2.666122764319632

Epoch: 6| Step: 1
Training loss: 3.236656084865109
Validation loss: 2.6616174938336266

Epoch: 6| Step: 2
Training loss: 2.6436410867969165
Validation loss: 2.6619782135214605

Epoch: 6| Step: 3
Training loss: 3.111639683720027
Validation loss: 2.66680604399972

Epoch: 6| Step: 4
Training loss: 2.4659993746865774
Validation loss: 2.6618629521897614

Epoch: 6| Step: 5
Training loss: 3.829633392238128
Validation loss: 2.6618361923173026

Epoch: 6| Step: 6
Training loss: 3.302856948355661
Validation loss: 2.668176553935245

Epoch: 6| Step: 7
Training loss: 2.7386253238715916
Validation loss: 2.6600734942424658

Epoch: 6| Step: 8
Training loss: 2.9226980427127525
Validation loss: 2.6737985384833234

Epoch: 6| Step: 9
Training loss: 3.0898584107486418
Validation loss: 2.6666990590307242

Epoch: 6| Step: 10
Training loss: 2.7448519290531426
Validation loss: 2.669020507663818

Epoch: 6| Step: 11
Training loss: 3.3365277878107067
Validation loss: 2.6718253861667383

Epoch: 6| Step: 12
Training loss: 2.5090933407707645
Validation loss: 2.6653652078847117

Epoch: 6| Step: 13
Training loss: 2.203260052376577
Validation loss: 2.6737514609411903

Epoch: 266| Step: 0
Training loss: 2.5641340302712945
Validation loss: 2.6845305603436294

Epoch: 6| Step: 1
Training loss: 2.360865444293073
Validation loss: 2.6781690520322408

Epoch: 6| Step: 2
Training loss: 2.9675173157827093
Validation loss: 2.6757735641868514

Epoch: 6| Step: 3
Training loss: 3.1625451488061485
Validation loss: 2.6676956070809443

Epoch: 6| Step: 4
Training loss: 2.3256534027440168
Validation loss: 2.66602915756251

Epoch: 6| Step: 5
Training loss: 3.2201902259886346
Validation loss: 2.6652906685037063

Epoch: 6| Step: 6
Training loss: 3.1820636840336185
Validation loss: 2.670091563578908

Epoch: 6| Step: 7
Training loss: 3.203082423973899
Validation loss: 2.6615946883104766

Epoch: 6| Step: 8
Training loss: 2.822047579569012
Validation loss: 2.66449525959483

Epoch: 6| Step: 9
Training loss: 3.3278802772775014
Validation loss: 2.6593189430876683

Epoch: 6| Step: 10
Training loss: 3.1315198405144495
Validation loss: 2.6568831671778432

Epoch: 6| Step: 11
Training loss: 3.637494095735248
Validation loss: 2.6627983375583204

Epoch: 6| Step: 12
Training loss: 2.918907902957417
Validation loss: 2.667582007431915

Epoch: 6| Step: 13
Training loss: 2.6193285027693496
Validation loss: 2.6630694365303333

Epoch: 267| Step: 0
Training loss: 3.2029473976582143
Validation loss: 2.66308530214573

Epoch: 6| Step: 1
Training loss: 3.1357450767368347
Validation loss: 2.6632989353262255

Epoch: 6| Step: 2
Training loss: 3.0276675734067293
Validation loss: 2.664817167405364

Epoch: 6| Step: 3
Training loss: 2.688117200374042
Validation loss: 2.6593774556569096

Epoch: 6| Step: 4
Training loss: 2.886350368177082
Validation loss: 2.6592723245273713

Epoch: 6| Step: 5
Training loss: 2.7906550287252214
Validation loss: 2.662578056239253

Epoch: 6| Step: 6
Training loss: 3.1755732882305425
Validation loss: 2.666047034517504

Epoch: 6| Step: 7
Training loss: 3.2107058253556424
Validation loss: 2.668063139071183

Epoch: 6| Step: 8
Training loss: 2.0507579983800923
Validation loss: 2.6673491383998686

Epoch: 6| Step: 9
Training loss: 2.9231828832533404
Validation loss: 2.669629473459852

Epoch: 6| Step: 10
Training loss: 3.1968229496541634
Validation loss: 2.685105851609608

Epoch: 6| Step: 11
Training loss: 3.016502134216454
Validation loss: 2.6817177193211834

Epoch: 6| Step: 12
Training loss: 3.22584049006931
Validation loss: 2.6879577504441774

Epoch: 6| Step: 13
Training loss: 3.263046926204823
Validation loss: 2.684474601503662

Epoch: 268| Step: 0
Training loss: 2.7776268494187293
Validation loss: 2.6676170042660265

Epoch: 6| Step: 1
Training loss: 2.952114840716038
Validation loss: 2.663180912594511

Epoch: 6| Step: 2
Training loss: 2.184206308555522
Validation loss: 2.6633770408813655

Epoch: 6| Step: 3
Training loss: 3.651670408143971
Validation loss: 2.6640054361464434

Epoch: 6| Step: 4
Training loss: 2.7197779323582236
Validation loss: 2.6584424809391876

Epoch: 6| Step: 5
Training loss: 3.1305605435794726
Validation loss: 2.6600637353118386

Epoch: 6| Step: 6
Training loss: 2.895990289692907
Validation loss: 2.664283543833421

Epoch: 6| Step: 7
Training loss: 3.0288038313706918
Validation loss: 2.6606655561177743

Epoch: 6| Step: 8
Training loss: 3.5005665729252935
Validation loss: 2.659446485857678

Epoch: 6| Step: 9
Training loss: 2.7180141461630907
Validation loss: 2.6646539218499727

Epoch: 6| Step: 10
Training loss: 3.169156868901347
Validation loss: 2.6629763867064566

Epoch: 6| Step: 11
Training loss: 3.4142937832843585
Validation loss: 2.660597766330214

Epoch: 6| Step: 12
Training loss: 2.2927572487502417
Validation loss: 2.661853267236932

Epoch: 6| Step: 13
Training loss: 3.2744905148303403
Validation loss: 2.6633007392021857

Epoch: 269| Step: 0
Training loss: 2.5239540728759913
Validation loss: 2.6653702469317264

Epoch: 6| Step: 1
Training loss: 3.2708593152870993
Validation loss: 2.6578775041755747

Epoch: 6| Step: 2
Training loss: 2.8065606028192875
Validation loss: 2.660171665589511

Epoch: 6| Step: 3
Training loss: 1.9227687959309072
Validation loss: 2.6574898544826477

Epoch: 6| Step: 4
Training loss: 3.1186157625775524
Validation loss: 2.656888596694389

Epoch: 6| Step: 5
Training loss: 3.3741097865669163
Validation loss: 2.6580906300233167

Epoch: 6| Step: 6
Training loss: 2.6369953377790614
Validation loss: 2.660391429215739

Epoch: 6| Step: 7
Training loss: 2.5246296714924212
Validation loss: 2.6598960179740847

Epoch: 6| Step: 8
Training loss: 3.3538442105238513
Validation loss: 2.6608947265667875

Epoch: 6| Step: 9
Training loss: 3.4129068264877405
Validation loss: 2.662074800754235

Epoch: 6| Step: 10
Training loss: 3.290277034841203
Validation loss: 2.671069114999587

Epoch: 6| Step: 11
Training loss: 2.988122315013631
Validation loss: 2.67201653751923

Epoch: 6| Step: 12
Training loss: 2.814499208071718
Validation loss: 2.674121325234147

Epoch: 6| Step: 13
Training loss: 3.7031906701554496
Validation loss: 2.6815210892395887

Epoch: 270| Step: 0
Training loss: 2.670855768340612
Validation loss: 2.67688118393375

Epoch: 6| Step: 1
Training loss: 2.547793357362659
Validation loss: 2.6933859425591473

Epoch: 6| Step: 2
Training loss: 2.608890602698658
Validation loss: 2.702965007966776

Epoch: 6| Step: 3
Training loss: 2.9165568376480246
Validation loss: 2.6955421909872372

Epoch: 6| Step: 4
Training loss: 2.9835879900912423
Validation loss: 2.6962763863601267

Epoch: 6| Step: 5
Training loss: 2.4474199372737884
Validation loss: 2.693062965989732

Epoch: 6| Step: 6
Training loss: 3.114069629437085
Validation loss: 2.67685342494708

Epoch: 6| Step: 7
Training loss: 3.1347855516792675
Validation loss: 2.672078390966948

Epoch: 6| Step: 8
Training loss: 3.6677227811231754
Validation loss: 2.6604027846412768

Epoch: 6| Step: 9
Training loss: 3.8291669400740487
Validation loss: 2.661463480275912

Epoch: 6| Step: 10
Training loss: 2.6494646359322602
Validation loss: 2.6531593646258687

Epoch: 6| Step: 11
Training loss: 2.692318522253367
Validation loss: 2.6579105212869316

Epoch: 6| Step: 12
Training loss: 3.3060610106020056
Validation loss: 2.652114264045542

Epoch: 6| Step: 13
Training loss: 2.886665230178769
Validation loss: 2.653602519961863

Epoch: 271| Step: 0
Training loss: 2.69899049889672
Validation loss: 2.6550035364289553

Epoch: 6| Step: 1
Training loss: 3.463422378359713
Validation loss: 2.654332301009899

Epoch: 6| Step: 2
Training loss: 3.2181874496555674
Validation loss: 2.6563863855584136

Epoch: 6| Step: 3
Training loss: 2.852520507417827
Validation loss: 2.656647416846376

Epoch: 6| Step: 4
Training loss: 2.6186817970589797
Validation loss: 2.6547894230499023

Epoch: 6| Step: 5
Training loss: 3.084454993975967
Validation loss: 2.657104352860747

Epoch: 6| Step: 6
Training loss: 2.699037493348866
Validation loss: 2.6544872421123005

Epoch: 6| Step: 7
Training loss: 2.9552128591537556
Validation loss: 2.653287578375725

Epoch: 6| Step: 8
Training loss: 3.4050601665989957
Validation loss: 2.6592524893633285

Epoch: 6| Step: 9
Training loss: 2.8935064027885815
Validation loss: 2.6569614468614184

Epoch: 6| Step: 10
Training loss: 2.50987011881206
Validation loss: 2.656256425863042

Epoch: 6| Step: 11
Training loss: 3.0367516996982347
Validation loss: 2.655335521659724

Epoch: 6| Step: 12
Training loss: 3.041275591509382
Validation loss: 2.6554813447909837

Epoch: 6| Step: 13
Training loss: 3.4416765676237464
Validation loss: 2.656170919497045

Epoch: 272| Step: 0
Training loss: 2.7030577899317003
Validation loss: 2.6603729871459

Epoch: 6| Step: 1
Training loss: 2.6716942084644493
Validation loss: 2.6639310237611564

Epoch: 6| Step: 2
Training loss: 2.751751255404211
Validation loss: 2.669993015721233

Epoch: 6| Step: 3
Training loss: 2.920199153327179
Validation loss: 2.6734761615332787

Epoch: 6| Step: 4
Training loss: 3.377050553919142
Validation loss: 2.664687047894278

Epoch: 6| Step: 5
Training loss: 2.881636216764115
Validation loss: 2.66842308254387

Epoch: 6| Step: 6
Training loss: 3.6839150281585464
Validation loss: 2.684070008555315

Epoch: 6| Step: 7
Training loss: 2.921878263910276
Validation loss: 2.6733058889640127

Epoch: 6| Step: 8
Training loss: 3.160980607482258
Validation loss: 2.6770632569792516

Epoch: 6| Step: 9
Training loss: 2.791284867174398
Validation loss: 2.6710746078189933

Epoch: 6| Step: 10
Training loss: 2.471670427308809
Validation loss: 2.6751760017580475

Epoch: 6| Step: 11
Training loss: 3.7656731186954127
Validation loss: 2.6599211646832757

Epoch: 6| Step: 12
Training loss: 2.437584508751376
Validation loss: 2.660595065957156

Epoch: 6| Step: 13
Training loss: 2.933471677147649
Validation loss: 2.663155851535378

Epoch: 273| Step: 0
Training loss: 2.893558807318616
Validation loss: 2.6696647632287007

Epoch: 6| Step: 1
Training loss: 2.5145473659906563
Validation loss: 2.662932405027899

Epoch: 6| Step: 2
Training loss: 3.2440621345298597
Validation loss: 2.6626663791131686

Epoch: 6| Step: 3
Training loss: 2.8421555964528475
Validation loss: 2.6654920737424455

Epoch: 6| Step: 4
Training loss: 3.309634282632725
Validation loss: 2.6701944925574086

Epoch: 6| Step: 5
Training loss: 3.2049997193430095
Validation loss: 2.6626486912800575

Epoch: 6| Step: 6
Training loss: 3.0597372243910455
Validation loss: 2.6684672912419027

Epoch: 6| Step: 7
Training loss: 2.7667087651309172
Validation loss: 2.6659704774975412

Epoch: 6| Step: 8
Training loss: 2.560313245413993
Validation loss: 2.6683225986061125

Epoch: 6| Step: 9
Training loss: 2.8846560323736803
Validation loss: 2.6607477009059437

Epoch: 6| Step: 10
Training loss: 3.355311859690728
Validation loss: 2.655433236776989

Epoch: 6| Step: 11
Training loss: 2.9155644286836084
Validation loss: 2.6619896893013184

Epoch: 6| Step: 12
Training loss: 3.213696262261986
Validation loss: 2.6594252619295413

Epoch: 6| Step: 13
Training loss: 2.7313080945154335
Validation loss: 2.6526555737930035

Epoch: 274| Step: 0
Training loss: 3.015075633553793
Validation loss: 2.6533447794941245

Epoch: 6| Step: 1
Training loss: 3.4272353458412645
Validation loss: 2.6524451417935024

Epoch: 6| Step: 2
Training loss: 2.6006260925011975
Validation loss: 2.6498112294562723

Epoch: 6| Step: 3
Training loss: 2.369864885365859
Validation loss: 2.652154715702019

Epoch: 6| Step: 4
Training loss: 2.6342597402244308
Validation loss: 2.6523627650018966

Epoch: 6| Step: 5
Training loss: 3.0425976586948384
Validation loss: 2.6525666827344616

Epoch: 6| Step: 6
Training loss: 3.0406927526596075
Validation loss: 2.647018510821297

Epoch: 6| Step: 7
Training loss: 3.6347850407528184
Validation loss: 2.6537173392157443

Epoch: 6| Step: 8
Training loss: 2.5761131991398813
Validation loss: 2.6523496943170746

Epoch: 6| Step: 9
Training loss: 2.803606170216891
Validation loss: 2.6515605707251173

Epoch: 6| Step: 10
Training loss: 3.2465092912896734
Validation loss: 2.655969138500745

Epoch: 6| Step: 11
Training loss: 3.5100249723757893
Validation loss: 2.6506588738039434

Epoch: 6| Step: 12
Training loss: 2.380848860306714
Validation loss: 2.653139660587483

Epoch: 6| Step: 13
Training loss: 3.196340978324941
Validation loss: 2.65851384880621

Epoch: 275| Step: 0
Training loss: 2.801763769218572
Validation loss: 2.656705324355397

Epoch: 6| Step: 1
Training loss: 3.3997891248494847
Validation loss: 2.6767678065084106

Epoch: 6| Step: 2
Training loss: 3.071136131178569
Validation loss: 2.6865144082810035

Epoch: 6| Step: 3
Training loss: 2.8598619056921097
Validation loss: 2.6914552523725987

Epoch: 6| Step: 4
Training loss: 3.053768868624231
Validation loss: 2.700066805331717

Epoch: 6| Step: 5
Training loss: 2.253822047414377
Validation loss: 2.706466740242499

Epoch: 6| Step: 6
Training loss: 3.5961091632334523
Validation loss: 2.6806368371220635

Epoch: 6| Step: 7
Training loss: 3.0802942368301798
Validation loss: 2.6774745147979124

Epoch: 6| Step: 8
Training loss: 2.4750428937797837
Validation loss: 2.6800123132846165

Epoch: 6| Step: 9
Training loss: 3.0844625690555536
Validation loss: 2.671020245397665

Epoch: 6| Step: 10
Training loss: 3.6546417350888207
Validation loss: 2.674540302092175

Epoch: 6| Step: 11
Training loss: 2.6183249663525436
Validation loss: 2.6634083862240447

Epoch: 6| Step: 12
Training loss: 2.476019959333885
Validation loss: 2.6652214347332586

Epoch: 6| Step: 13
Training loss: 3.051964055530871
Validation loss: 2.658348266210387

Epoch: 276| Step: 0
Training loss: 2.6306373372222542
Validation loss: 2.6559837589107733

Epoch: 6| Step: 1
Training loss: 2.893518268048311
Validation loss: 2.64533290502042

Epoch: 6| Step: 2
Training loss: 3.1040981891085457
Validation loss: 2.6431124876586005

Epoch: 6| Step: 3
Training loss: 2.350813128431762
Validation loss: 2.6476696141551592

Epoch: 6| Step: 4
Training loss: 3.5592840301972513
Validation loss: 2.6474749047901156

Epoch: 6| Step: 5
Training loss: 3.512373713602228
Validation loss: 2.65029127881025

Epoch: 6| Step: 6
Training loss: 3.183438275057117
Validation loss: 2.6450929336509343

Epoch: 6| Step: 7
Training loss: 2.7694790812368835
Validation loss: 2.6450346875900976

Epoch: 6| Step: 8
Training loss: 3.079394857978628
Validation loss: 2.64399819992409

Epoch: 6| Step: 9
Training loss: 2.758644821312194
Validation loss: 2.64749081834741

Epoch: 6| Step: 10
Training loss: 2.938536927657626
Validation loss: 2.6478728341178357

Epoch: 6| Step: 11
Training loss: 2.832878375457852
Validation loss: 2.649261173257921

Epoch: 6| Step: 12
Training loss: 2.9685795785274633
Validation loss: 2.649932561307665

Epoch: 6| Step: 13
Training loss: 3.1288849142921134
Validation loss: 2.6535429622373625

Epoch: 277| Step: 0
Training loss: 3.5344044597285027
Validation loss: 2.6484033764679586

Epoch: 6| Step: 1
Training loss: 3.0174539188636866
Validation loss: 2.6470553194224298

Epoch: 6| Step: 2
Training loss: 1.991748776465761
Validation loss: 2.645309273933251

Epoch: 6| Step: 3
Training loss: 3.178558674108593
Validation loss: 2.6476171744460064

Epoch: 6| Step: 4
Training loss: 3.1814027700859024
Validation loss: 2.64450341124573

Epoch: 6| Step: 5
Training loss: 3.2171530604545517
Validation loss: 2.646970298391904

Epoch: 6| Step: 6
Training loss: 2.417206331264771
Validation loss: 2.646939180666789

Epoch: 6| Step: 7
Training loss: 3.434889756034158
Validation loss: 2.6455820284181772

Epoch: 6| Step: 8
Training loss: 2.8595656034385106
Validation loss: 2.6444568407567792

Epoch: 6| Step: 9
Training loss: 2.8847821544465506
Validation loss: 2.6454518776118254

Epoch: 6| Step: 10
Training loss: 3.1533209173698706
Validation loss: 2.6435142768656763

Epoch: 6| Step: 11
Training loss: 3.2286287577539023
Validation loss: 2.6411674900005724

Epoch: 6| Step: 12
Training loss: 2.619543216722359
Validation loss: 2.6413030577232757

Epoch: 6| Step: 13
Training loss: 2.4861794882049044
Validation loss: 2.6433992520370873

Epoch: 278| Step: 0
Training loss: 3.676378580625044
Validation loss: 2.6451465757660033

Epoch: 6| Step: 1
Training loss: 3.078649795642399
Validation loss: 2.645616111788213

Epoch: 6| Step: 2
Training loss: 3.521400691861351
Validation loss: 2.646480688137857

Epoch: 6| Step: 3
Training loss: 2.716166178309221
Validation loss: 2.6476147595490485

Epoch: 6| Step: 4
Training loss: 2.817547063155198
Validation loss: 2.6672713413889113

Epoch: 6| Step: 5
Training loss: 2.8202482517353
Validation loss: 2.6684141390705416

Epoch: 6| Step: 6
Training loss: 2.5162710935766897
Validation loss: 2.662732513680287

Epoch: 6| Step: 7
Training loss: 3.473478651124739
Validation loss: 2.6655451572275024

Epoch: 6| Step: 8
Training loss: 2.530575132474559
Validation loss: 2.682865249151814

Epoch: 6| Step: 9
Training loss: 3.0965370956211884
Validation loss: 2.680348899487112

Epoch: 6| Step: 10
Training loss: 2.5762720095801592
Validation loss: 2.6861048074823732

Epoch: 6| Step: 11
Training loss: 2.535911600353196
Validation loss: 2.6832368772863506

Epoch: 6| Step: 12
Training loss: 3.087095405540249
Validation loss: 2.680946865682853

Epoch: 6| Step: 13
Training loss: 2.9263631855659082
Validation loss: 2.686914009489047

Epoch: 279| Step: 0
Training loss: 3.301536410472817
Validation loss: 2.677228493144773

Epoch: 6| Step: 1
Training loss: 3.0657289570983135
Validation loss: 2.6540278106168196

Epoch: 6| Step: 2
Training loss: 3.263563025329876
Validation loss: 2.654075682339767

Epoch: 6| Step: 3
Training loss: 2.953213322040261
Validation loss: 2.6473120288635212

Epoch: 6| Step: 4
Training loss: 3.1538270015743373
Validation loss: 2.642106066763049

Epoch: 6| Step: 5
Training loss: 2.3573110281002645
Validation loss: 2.6424361428641006

Epoch: 6| Step: 6
Training loss: 3.3243339330349526
Validation loss: 2.6410153503299885

Epoch: 6| Step: 7
Training loss: 2.7225538806470153
Validation loss: 2.639278304622201

Epoch: 6| Step: 8
Training loss: 2.8978175402502537
Validation loss: 2.6423997443428315

Epoch: 6| Step: 9
Training loss: 3.158014993720189
Validation loss: 2.6437552051041555

Epoch: 6| Step: 10
Training loss: 3.583220871927462
Validation loss: 2.643120214129399

Epoch: 6| Step: 11
Training loss: 2.5851969508508863
Validation loss: 2.6432909909133717

Epoch: 6| Step: 12
Training loss: 2.016715885644139
Validation loss: 2.6432148028850837

Epoch: 6| Step: 13
Training loss: 3.256528605954716
Validation loss: 2.645745248243127

Epoch: 280| Step: 0
Training loss: 2.81707197091028
Validation loss: 2.6384520930379063

Epoch: 6| Step: 1
Training loss: 3.2072355365621816
Validation loss: 2.641747555656105

Epoch: 6| Step: 2
Training loss: 2.595469261179947
Validation loss: 2.640073894095524

Epoch: 6| Step: 3
Training loss: 3.37115909871119
Validation loss: 2.6403195774360144

Epoch: 6| Step: 4
Training loss: 2.891560501069908
Validation loss: 2.6435361328210867

Epoch: 6| Step: 5
Training loss: 3.179226778661165
Validation loss: 2.6433346355367626

Epoch: 6| Step: 6
Training loss: 2.531652253766833
Validation loss: 2.6510990253547337

Epoch: 6| Step: 7
Training loss: 3.3620737989814207
Validation loss: 2.6482946800398794

Epoch: 6| Step: 8
Training loss: 2.7748374307112518
Validation loss: 2.6576751611189278

Epoch: 6| Step: 9
Training loss: 3.4475749020257886
Validation loss: 2.6643964874277186

Epoch: 6| Step: 10
Training loss: 3.1402867073323173
Validation loss: 2.6618476022551505

Epoch: 6| Step: 11
Training loss: 2.8082778604463963
Validation loss: 2.6635659255161306

Epoch: 6| Step: 12
Training loss: 2.8129982401018165
Validation loss: 2.6530581479711457

Epoch: 6| Step: 13
Training loss: 2.163402360286097
Validation loss: 2.6488613629569

Epoch: 281| Step: 0
Training loss: 3.454625972922824
Validation loss: 2.646370217445972

Epoch: 6| Step: 1
Training loss: 3.2072835583701518
Validation loss: 2.650043637099641

Epoch: 6| Step: 2
Training loss: 3.218975466636795
Validation loss: 2.6407754923996296

Epoch: 6| Step: 3
Training loss: 3.1264739565915334
Validation loss: 2.6440079832462406

Epoch: 6| Step: 4
Training loss: 3.196568771417313
Validation loss: 2.6442626781848837

Epoch: 6| Step: 5
Training loss: 3.0242398402454995
Validation loss: 2.6445541532314802

Epoch: 6| Step: 6
Training loss: 2.7712035732484117
Validation loss: 2.639467407359448

Epoch: 6| Step: 7
Training loss: 2.3762824962627396
Validation loss: 2.6441526897699585

Epoch: 6| Step: 8
Training loss: 2.6035104967597658
Validation loss: 2.639631111140024

Epoch: 6| Step: 9
Training loss: 2.7532494160731122
Validation loss: 2.6477648890401797

Epoch: 6| Step: 10
Training loss: 3.36197380855418
Validation loss: 2.64102463023115

Epoch: 6| Step: 11
Training loss: 2.7853681957180765
Validation loss: 2.6539145465071172

Epoch: 6| Step: 12
Training loss: 2.5272675256569563
Validation loss: 2.644580082715082

Epoch: 6| Step: 13
Training loss: 2.9705907636130697
Validation loss: 2.6505016371031815

Epoch: 282| Step: 0
Training loss: 2.832202816732054
Validation loss: 2.645483525950468

Epoch: 6| Step: 1
Training loss: 2.891637841190015
Validation loss: 2.64439204263384

Epoch: 6| Step: 2
Training loss: 3.0013636033158466
Validation loss: 2.638015758889054

Epoch: 6| Step: 3
Training loss: 2.9053461453612086
Validation loss: 2.6414302834163865

Epoch: 6| Step: 4
Training loss: 2.9491618371992048
Validation loss: 2.6427958516951287

Epoch: 6| Step: 5
Training loss: 2.5952706532833805
Validation loss: 2.6362841224759994

Epoch: 6| Step: 6
Training loss: 2.409638038457103
Validation loss: 2.638345842412705

Epoch: 6| Step: 7
Training loss: 2.9822351263569
Validation loss: 2.640726699030883

Epoch: 6| Step: 8
Training loss: 2.8130811302834196
Validation loss: 2.635128123797496

Epoch: 6| Step: 9
Training loss: 3.767053863294227
Validation loss: 2.64446941241695

Epoch: 6| Step: 10
Training loss: 3.525764187704533
Validation loss: 2.6398612180696364

Epoch: 6| Step: 11
Training loss: 2.4460582631892693
Validation loss: 2.6407809783329186

Epoch: 6| Step: 12
Training loss: 3.0381286284568856
Validation loss: 2.6435586325223492

Epoch: 6| Step: 13
Training loss: 3.3364907728307984
Validation loss: 2.6584764987556726

Epoch: 283| Step: 0
Training loss: 3.1778604192888054
Validation loss: 2.6465977372074745

Epoch: 6| Step: 1
Training loss: 2.4734866900682575
Validation loss: 2.6557573231947766

Epoch: 6| Step: 2
Training loss: 2.6144455851912514
Validation loss: 2.6549627728358303

Epoch: 6| Step: 3
Training loss: 2.854602269997179
Validation loss: 2.6539133786314544

Epoch: 6| Step: 4
Training loss: 2.9028991742786783
Validation loss: 2.6597087291471406

Epoch: 6| Step: 5
Training loss: 2.954534930763976
Validation loss: 2.6553342810334604

Epoch: 6| Step: 6
Training loss: 2.6316156128788553
Validation loss: 2.664931252393629

Epoch: 6| Step: 7
Training loss: 3.2623680701930367
Validation loss: 2.672266610675385

Epoch: 6| Step: 8
Training loss: 3.050573676517152
Validation loss: 2.6656362698870755

Epoch: 6| Step: 9
Training loss: 2.645524755390559
Validation loss: 2.656900355945162

Epoch: 6| Step: 10
Training loss: 2.957988951790729
Validation loss: 2.6528039582805056

Epoch: 6| Step: 11
Training loss: 3.75223334882906
Validation loss: 2.6536626849778147

Epoch: 6| Step: 12
Training loss: 2.7916069498839517
Validation loss: 2.647194461305144

Epoch: 6| Step: 13
Training loss: 3.502059875324996
Validation loss: 2.6450793724821495

Epoch: 284| Step: 0
Training loss: 3.402824989735235
Validation loss: 2.6460284060331416

Epoch: 6| Step: 1
Training loss: 2.6769523341167845
Validation loss: 2.6456734933034363

Epoch: 6| Step: 2
Training loss: 3.430531460930092
Validation loss: 2.6627648919646063

Epoch: 6| Step: 3
Training loss: 2.834135802079517
Validation loss: 2.6469906914463013

Epoch: 6| Step: 4
Training loss: 2.552776496849066
Validation loss: 2.6633806812530096

Epoch: 6| Step: 5
Training loss: 2.4929682067851373
Validation loss: 2.6599379126394433

Epoch: 6| Step: 6
Training loss: 2.8491699214565998
Validation loss: 2.6579929450839526

Epoch: 6| Step: 7
Training loss: 3.369529529184583
Validation loss: 2.660348204148448

Epoch: 6| Step: 8
Training loss: 3.348229518310774
Validation loss: 2.646710763293585

Epoch: 6| Step: 9
Training loss: 2.8869542914941966
Validation loss: 2.6577601744154373

Epoch: 6| Step: 10
Training loss: 2.945716243361089
Validation loss: 2.6426923561446682

Epoch: 6| Step: 11
Training loss: 2.9334170596860463
Validation loss: 2.6498073537266422

Epoch: 6| Step: 12
Training loss: 2.4754792260038188
Validation loss: 2.637290031622393

Epoch: 6| Step: 13
Training loss: 3.270659585861377
Validation loss: 2.642400100404041

Epoch: 285| Step: 0
Training loss: 2.9508247903525846
Validation loss: 2.6450213548615595

Epoch: 6| Step: 1
Training loss: 3.091686601051849
Validation loss: 2.640292804091646

Epoch: 6| Step: 2
Training loss: 3.49098625260386
Validation loss: 2.6477905319558

Epoch: 6| Step: 3
Training loss: 3.3359695182889673
Validation loss: 2.6463003414176107

Epoch: 6| Step: 4
Training loss: 3.0143577007925977
Validation loss: 2.6414750593694065

Epoch: 6| Step: 5
Training loss: 2.9486136718904987
Validation loss: 2.642031906515485

Epoch: 6| Step: 6
Training loss: 2.1435861641695193
Validation loss: 2.642434080259056

Epoch: 6| Step: 7
Training loss: 3.145712911225154
Validation loss: 2.636026653212746

Epoch: 6| Step: 8
Training loss: 3.024484694545751
Validation loss: 2.6342662528175755

Epoch: 6| Step: 9
Training loss: 2.352039174785174
Validation loss: 2.6423366732122147

Epoch: 6| Step: 10
Training loss: 2.9652639701893038
Validation loss: 2.6422289021923246

Epoch: 6| Step: 11
Training loss: 3.2343083618965562
Validation loss: 2.6414835592901165

Epoch: 6| Step: 12
Training loss: 2.840646916169581
Validation loss: 2.6376656180962517

Epoch: 6| Step: 13
Training loss: 2.626449184934847
Validation loss: 2.645775180442743

Epoch: 286| Step: 0
Training loss: 3.2192811990564962
Validation loss: 2.647439590372698

Epoch: 6| Step: 1
Training loss: 2.8666867018894044
Validation loss: 2.6444791009304405

Epoch: 6| Step: 2
Training loss: 2.7054916341212008
Validation loss: 2.6419600371374337

Epoch: 6| Step: 3
Training loss: 2.629437918557426
Validation loss: 2.6513344340965146

Epoch: 6| Step: 4
Training loss: 3.1232248986744766
Validation loss: 2.6382261903161033

Epoch: 6| Step: 5
Training loss: 3.3357633951749137
Validation loss: 2.6366045653588137

Epoch: 6| Step: 6
Training loss: 3.075388665176246
Validation loss: 2.6421064684680204

Epoch: 6| Step: 7
Training loss: 3.1368434021386875
Validation loss: 2.639378632582283

Epoch: 6| Step: 8
Training loss: 3.2715368465941275
Validation loss: 2.634365165955915

Epoch: 6| Step: 9
Training loss: 2.4946456792136504
Validation loss: 2.63530682301592

Epoch: 6| Step: 10
Training loss: 3.0264073652645216
Validation loss: 2.635403501558882

Epoch: 6| Step: 11
Training loss: 2.217962313243291
Validation loss: 2.642561751118642

Epoch: 6| Step: 12
Training loss: 2.9855849125250917
Validation loss: 2.6407945877862415

Epoch: 6| Step: 13
Training loss: 3.4284657450691123
Validation loss: 2.64020747207591

Epoch: 287| Step: 0
Training loss: 2.7734194472893976
Validation loss: 2.638339656659816

Epoch: 6| Step: 1
Training loss: 3.0623358857693104
Validation loss: 2.666915108723433

Epoch: 6| Step: 2
Training loss: 2.8491505076271952
Validation loss: 2.653985598554741

Epoch: 6| Step: 3
Training loss: 3.231919983222682
Validation loss: 2.661888423177832

Epoch: 6| Step: 4
Training loss: 2.7648374653895975
Validation loss: 2.6769829890021315

Epoch: 6| Step: 5
Training loss: 3.4605307587600227
Validation loss: 2.689222950637706

Epoch: 6| Step: 6
Training loss: 2.9014235587022084
Validation loss: 2.675339194593475

Epoch: 6| Step: 7
Training loss: 3.212313837379426
Validation loss: 2.666842173817702

Epoch: 6| Step: 8
Training loss: 2.8556010036917194
Validation loss: 2.6422597668520345

Epoch: 6| Step: 9
Training loss: 2.5985640925653857
Validation loss: 2.6322404949623612

Epoch: 6| Step: 10
Training loss: 3.07364324395947
Validation loss: 2.6321709967078584

Epoch: 6| Step: 11
Training loss: 2.393683378323378
Validation loss: 2.6354520116774807

Epoch: 6| Step: 12
Training loss: 2.843188722994481
Validation loss: 2.632472602542661

Epoch: 6| Step: 13
Training loss: 3.791553803046192
Validation loss: 2.6345407247111354

Epoch: 288| Step: 0
Training loss: 3.0195874681458066
Validation loss: 2.6326021724076356

Epoch: 6| Step: 1
Training loss: 3.043691057265043
Validation loss: 2.633442967278193

Epoch: 6| Step: 2
Training loss: 2.4495979757747843
Validation loss: 2.6347849167033353

Epoch: 6| Step: 3
Training loss: 3.1684461579626384
Validation loss: 2.6329386433118622

Epoch: 6| Step: 4
Training loss: 2.7957575606317495
Validation loss: 2.635270883449173

Epoch: 6| Step: 5
Training loss: 2.5356852915466686
Validation loss: 2.635167246068815

Epoch: 6| Step: 6
Training loss: 2.7186851932211806
Validation loss: 2.635149316255168

Epoch: 6| Step: 7
Training loss: 3.212005363412372
Validation loss: 2.634813413716665

Epoch: 6| Step: 8
Training loss: 3.402962454284648
Validation loss: 2.6322915648855663

Epoch: 6| Step: 9
Training loss: 3.2461889370069166
Validation loss: 2.6363494689803892

Epoch: 6| Step: 10
Training loss: 2.8363438675042567
Validation loss: 2.6378372611660383

Epoch: 6| Step: 11
Training loss: 2.8780849325598994
Validation loss: 2.6370835045053567

Epoch: 6| Step: 12
Training loss: 3.51647694755428
Validation loss: 2.6375014433754527

Epoch: 6| Step: 13
Training loss: 2.198206829666219
Validation loss: 2.636179242224241

Epoch: 289| Step: 0
Training loss: 3.1917893216010245
Validation loss: 2.637710248993063

Epoch: 6| Step: 1
Training loss: 2.665226388444358
Validation loss: 2.6620510756011626

Epoch: 6| Step: 2
Training loss: 2.903458105385122
Validation loss: 2.6623625847766377

Epoch: 6| Step: 3
Training loss: 3.1048723139900036
Validation loss: 2.671372144733277

Epoch: 6| Step: 4
Training loss: 3.2614380492988153
Validation loss: 2.6779114157628445

Epoch: 6| Step: 5
Training loss: 3.0041167305769956
Validation loss: 2.660417547392915

Epoch: 6| Step: 6
Training loss: 2.779895409736616
Validation loss: 2.6728439574628173

Epoch: 6| Step: 7
Training loss: 2.7083282666281234
Validation loss: 2.7040323383320803

Epoch: 6| Step: 8
Training loss: 2.6279685174574596
Validation loss: 2.6747212788899417

Epoch: 6| Step: 9
Training loss: 2.9649942834808667
Validation loss: 2.6945077763365153

Epoch: 6| Step: 10
Training loss: 2.993985027094869
Validation loss: 2.6827602959544983

Epoch: 6| Step: 11
Training loss: 3.467731360628984
Validation loss: 2.7092877295847457

Epoch: 6| Step: 12
Training loss: 3.2764632414198527
Validation loss: 2.6655182159977877

Epoch: 6| Step: 13
Training loss: 2.2467807415754018
Validation loss: 2.6615959385401897

Epoch: 290| Step: 0
Training loss: 2.879009602113633
Validation loss: 2.6499796556824498

Epoch: 6| Step: 1
Training loss: 2.9302138199106746
Validation loss: 2.6407443094432295

Epoch: 6| Step: 2
Training loss: 2.555959402415632
Validation loss: 2.6384200683728696

Epoch: 6| Step: 3
Training loss: 2.886191602589459
Validation loss: 2.6363011674323493

Epoch: 6| Step: 4
Training loss: 3.4871891443771887
Validation loss: 2.6320667570863954

Epoch: 6| Step: 5
Training loss: 2.816947303048869
Validation loss: 2.6364927143179773

Epoch: 6| Step: 6
Training loss: 3.5640382624583435
Validation loss: 2.636487886507401

Epoch: 6| Step: 7
Training loss: 2.00753509148774
Validation loss: 2.633859809681702

Epoch: 6| Step: 8
Training loss: 2.9580859944995668
Validation loss: 2.6360230373075697

Epoch: 6| Step: 9
Training loss: 3.1165627066125343
Validation loss: 2.6300502769003153

Epoch: 6| Step: 10
Training loss: 3.2518215943276236
Validation loss: 2.6288003849453547

Epoch: 6| Step: 11
Training loss: 2.785782538964704
Validation loss: 2.633043753902931

Epoch: 6| Step: 12
Training loss: 3.3334674490333107
Validation loss: 2.6293641780876453

Epoch: 6| Step: 13
Training loss: 2.3858425198918978
Validation loss: 2.628689244382713

Epoch: 291| Step: 0
Training loss: 2.475919910904309
Validation loss: 2.6275857474971214

Epoch: 6| Step: 1
Training loss: 2.59784292826642
Validation loss: 2.624723962938133

Epoch: 6| Step: 2
Training loss: 3.231773620325568
Validation loss: 2.6340260134055793

Epoch: 6| Step: 3
Training loss: 2.8929129129244693
Validation loss: 2.6460233388716055

Epoch: 6| Step: 4
Training loss: 3.4485415596379005
Validation loss: 2.6463031392067076

Epoch: 6| Step: 5
Training loss: 3.1733779728845826
Validation loss: 2.6438903986974727

Epoch: 6| Step: 6
Training loss: 2.787002443803223
Validation loss: 2.65509861814803

Epoch: 6| Step: 7
Training loss: 2.881437640586191
Validation loss: 2.6660875401711466

Epoch: 6| Step: 8
Training loss: 2.729922522399125
Validation loss: 2.6702163163898325

Epoch: 6| Step: 9
Training loss: 3.0797805594014944
Validation loss: 2.6421517151936573

Epoch: 6| Step: 10
Training loss: 3.5912424378360273
Validation loss: 2.6493155438121576

Epoch: 6| Step: 11
Training loss: 2.255120067910114
Validation loss: 2.6410641801053774

Epoch: 6| Step: 12
Training loss: 3.2367133933982197
Validation loss: 2.635908650381651

Epoch: 6| Step: 13
Training loss: 2.766337071361499
Validation loss: 2.629732069141427

Epoch: 292| Step: 0
Training loss: 2.613425029910537
Validation loss: 2.626637952233892

Epoch: 6| Step: 1
Training loss: 2.4167869033718747
Validation loss: 2.6327950390694106

Epoch: 6| Step: 2
Training loss: 2.7704792417895883
Validation loss: 2.6340829344913916

Epoch: 6| Step: 3
Training loss: 2.603317905710822
Validation loss: 2.6356252107635694

Epoch: 6| Step: 4
Training loss: 3.5827210073189035
Validation loss: 2.6322954040742603

Epoch: 6| Step: 5
Training loss: 3.3225320017402225
Validation loss: 2.635168714109467

Epoch: 6| Step: 6
Training loss: 2.8028714136521686
Validation loss: 2.6364922096590373

Epoch: 6| Step: 7
Training loss: 2.3759502718310137
Validation loss: 2.645816862678244

Epoch: 6| Step: 8
Training loss: 2.9766006267038168
Validation loss: 2.644167297468331

Epoch: 6| Step: 9
Training loss: 3.0163114883093285
Validation loss: 2.6524028562170465

Epoch: 6| Step: 10
Training loss: 3.4363942535306164
Validation loss: 2.661129440722437

Epoch: 6| Step: 11
Training loss: 3.432759293155594
Validation loss: 2.6453394339626244

Epoch: 6| Step: 12
Training loss: 3.2409330216136736
Validation loss: 2.6480907082822913

Epoch: 6| Step: 13
Training loss: 2.1692772567916476
Validation loss: 2.6414941603746365

Epoch: 293| Step: 0
Training loss: 2.9486127015960695
Validation loss: 2.634281058883203

Epoch: 6| Step: 1
Training loss: 3.031497905858201
Validation loss: 2.636833292342938

Epoch: 6| Step: 2
Training loss: 2.8996704440473975
Validation loss: 2.6326342658968507

Epoch: 6| Step: 3
Training loss: 2.8793686556223514
Validation loss: 2.6305959660585043

Epoch: 6| Step: 4
Training loss: 2.484909671853387
Validation loss: 2.6319539641606253

Epoch: 6| Step: 5
Training loss: 3.0159894495711215
Validation loss: 2.6339670529998687

Epoch: 6| Step: 6
Training loss: 2.6150248687903694
Validation loss: 2.6298318186287775

Epoch: 6| Step: 7
Training loss: 3.1843356713139617
Validation loss: 2.6384491081406223

Epoch: 6| Step: 8
Training loss: 3.1955491703496084
Validation loss: 2.6286792519075917

Epoch: 6| Step: 9
Training loss: 3.423677709901926
Validation loss: 2.6377995415653817

Epoch: 6| Step: 10
Training loss: 2.6725686048617856
Validation loss: 2.64743150853488

Epoch: 6| Step: 11
Training loss: 3.280301638201196
Validation loss: 2.6451816009689137

Epoch: 6| Step: 12
Training loss: 3.0098505422803363
Validation loss: 2.63780146006779

Epoch: 6| Step: 13
Training loss: 2.44830957600714
Validation loss: 2.640249580112658

Epoch: 294| Step: 0
Training loss: 3.4666743211172646
Validation loss: 2.627448575324761

Epoch: 6| Step: 1
Training loss: 3.3244602998682944
Validation loss: 2.637833631220968

Epoch: 6| Step: 2
Training loss: 3.0463077897517747
Validation loss: 2.6387924784082344

Epoch: 6| Step: 3
Training loss: 2.5377975364594336
Validation loss: 2.634158869856826

Epoch: 6| Step: 4
Training loss: 3.7047460685743614
Validation loss: 2.623729630849626

Epoch: 6| Step: 5
Training loss: 2.9305849862290687
Validation loss: 2.624045088452095

Epoch: 6| Step: 6
Training loss: 2.5483508368373973
Validation loss: 2.631637132203266

Epoch: 6| Step: 7
Training loss: 2.8259177683578316
Validation loss: 2.631309233233001

Epoch: 6| Step: 8
Training loss: 2.476313533362675
Validation loss: 2.6272541862182477

Epoch: 6| Step: 9
Training loss: 2.4216358682030883
Validation loss: 2.6227116315455126

Epoch: 6| Step: 10
Training loss: 2.84417821992647
Validation loss: 2.6216829254333813

Epoch: 6| Step: 11
Training loss: 3.1772951316068387
Validation loss: 2.6235950390054867

Epoch: 6| Step: 12
Training loss: 3.0823771738299723
Validation loss: 2.6277655698654305

Epoch: 6| Step: 13
Training loss: 2.4781064777235264
Validation loss: 2.6387952559835

Epoch: 295| Step: 0
Training loss: 2.6195357534550183
Validation loss: 2.6483195003283337

Epoch: 6| Step: 1
Training loss: 2.9297314449829153
Validation loss: 2.6440867255512694

Epoch: 6| Step: 2
Training loss: 3.061218927988203
Validation loss: 2.652747410390297

Epoch: 6| Step: 3
Training loss: 2.4420618260435663
Validation loss: 2.674570200550494

Epoch: 6| Step: 4
Training loss: 3.1302453841282087
Validation loss: 2.665944606047386

Epoch: 6| Step: 5
Training loss: 3.4390873798473898
Validation loss: 2.694860526115386

Epoch: 6| Step: 6
Training loss: 3.182054842780202
Validation loss: 2.710217462452721

Epoch: 6| Step: 7
Training loss: 3.17907169011853
Validation loss: 2.6858621859434315

Epoch: 6| Step: 8
Training loss: 2.7317598752995087
Validation loss: 2.6912972995122346

Epoch: 6| Step: 9
Training loss: 3.1472582253213712
Validation loss: 2.6931614768919863

Epoch: 6| Step: 10
Training loss: 2.647087811018354
Validation loss: 2.684252583909154

Epoch: 6| Step: 11
Training loss: 3.292262317185727
Validation loss: 2.670334705964243

Epoch: 6| Step: 12
Training loss: 2.7423383448483762
Validation loss: 2.6605046584868655

Epoch: 6| Step: 13
Training loss: 2.7533277837437202
Validation loss: 2.6260482947975095

Epoch: 296| Step: 0
Training loss: 2.2487844786616895
Validation loss: 2.628027962158388

Epoch: 6| Step: 1
Training loss: 2.588574036383661
Validation loss: 2.6176696264700117

Epoch: 6| Step: 2
Training loss: 3.166859537240842
Validation loss: 2.6193408094126704

Epoch: 6| Step: 3
Training loss: 3.079328272755141
Validation loss: 2.6188631960198996

Epoch: 6| Step: 4
Training loss: 3.1134498770694745
Validation loss: 2.6213036885237937

Epoch: 6| Step: 5
Training loss: 3.2544407949285716
Validation loss: 2.627358365033877

Epoch: 6| Step: 6
Training loss: 3.0732966473666097
Validation loss: 2.625563095763593

Epoch: 6| Step: 7
Training loss: 3.1767057267383128
Validation loss: 2.6209889641873616

Epoch: 6| Step: 8
Training loss: 2.977534098009554
Validation loss: 2.6200638675740957

Epoch: 6| Step: 9
Training loss: 2.3942522443362724
Validation loss: 2.621782550919635

Epoch: 6| Step: 10
Training loss: 2.475732610145873
Validation loss: 2.6305013660094496

Epoch: 6| Step: 11
Training loss: 3.151757697517737
Validation loss: 2.6324443850085237

Epoch: 6| Step: 12
Training loss: 3.2290109576349177
Validation loss: 2.628943304394904

Epoch: 6| Step: 13
Training loss: 3.7962647249792236
Validation loss: 2.639278316278299

Epoch: 297| Step: 0
Training loss: 2.8606957892372216
Validation loss: 2.6416150124485545

Epoch: 6| Step: 1
Training loss: 3.1395489571623094
Validation loss: 2.6349425785392584

Epoch: 6| Step: 2
Training loss: 2.3224105853883854
Validation loss: 2.6326357061362002

Epoch: 6| Step: 3
Training loss: 2.901983266959661
Validation loss: 2.638518734460046

Epoch: 6| Step: 4
Training loss: 3.1589399904237094
Validation loss: 2.627603267435895

Epoch: 6| Step: 5
Training loss: 3.3456662115319897
Validation loss: 2.6292845863301078

Epoch: 6| Step: 6
Training loss: 3.354609458239449
Validation loss: 2.6292177089046826

Epoch: 6| Step: 7
Training loss: 2.211927320427982
Validation loss: 2.6291194202125148

Epoch: 6| Step: 8
Training loss: 2.886390016908996
Validation loss: 2.628143064706947

Epoch: 6| Step: 9
Training loss: 2.8661374032304834
Validation loss: 2.627982997087969

Epoch: 6| Step: 10
Training loss: 3.2494970079142034
Validation loss: 2.617408206592147

Epoch: 6| Step: 11
Training loss: 3.3621475488803125
Validation loss: 2.6251576078947947

Epoch: 6| Step: 12
Training loss: 2.9166621253568628
Validation loss: 2.626356857276646

Epoch: 6| Step: 13
Training loss: 2.292137190169814
Validation loss: 2.624598729783427

Epoch: 298| Step: 0
Training loss: 2.89018650337517
Validation loss: 2.6225610331853844

Epoch: 6| Step: 1
Training loss: 2.5769633132342276
Validation loss: 2.6281718483354486

Epoch: 6| Step: 2
Training loss: 2.385036143862759
Validation loss: 2.6318871828132924

Epoch: 6| Step: 3
Training loss: 2.782802448543707
Validation loss: 2.6363454567697793

Epoch: 6| Step: 4
Training loss: 2.950692279859071
Validation loss: 2.6425863594387873

Epoch: 6| Step: 5
Training loss: 2.2546514538628273
Validation loss: 2.648536429815771

Epoch: 6| Step: 6
Training loss: 2.99308472570222
Validation loss: 2.6582161109665408

Epoch: 6| Step: 7
Training loss: 3.1535167378915414
Validation loss: 2.641645397104405

Epoch: 6| Step: 8
Training loss: 3.0897394251904435
Validation loss: 2.660566889412224

Epoch: 6| Step: 9
Training loss: 3.1764091068127187
Validation loss: 2.6521794960000573

Epoch: 6| Step: 10
Training loss: 2.86291451597089
Validation loss: 2.6605518962313175

Epoch: 6| Step: 11
Training loss: 3.3273942171932873
Validation loss: 2.63992747335227

Epoch: 6| Step: 12
Training loss: 3.4498121846256926
Validation loss: 2.6379367197175028

Epoch: 6| Step: 13
Training loss: 3.5190230160323934
Validation loss: 2.6277216968947514

Epoch: 299| Step: 0
Training loss: 3.2143208668315504
Validation loss: 2.6124463151551085

Epoch: 6| Step: 1
Training loss: 2.7668046752182067
Validation loss: 2.6177178379546677

Epoch: 6| Step: 2
Training loss: 3.135954615342401
Validation loss: 2.6179661520146245

Epoch: 6| Step: 3
Training loss: 3.1909808432142386
Validation loss: 2.609856722987607

Epoch: 6| Step: 4
Training loss: 3.0380816997619915
Validation loss: 2.61491200057553

Epoch: 6| Step: 5
Training loss: 3.16568335943476
Validation loss: 2.6133375472791767

Epoch: 6| Step: 6
Training loss: 3.030963844871538
Validation loss: 2.6116735444198

Epoch: 6| Step: 7
Training loss: 2.8607431275921
Validation loss: 2.618496094355445

Epoch: 6| Step: 8
Training loss: 3.029643468748343
Validation loss: 2.6153390777007783

Epoch: 6| Step: 9
Training loss: 2.3695664742477485
Validation loss: 2.61657035994752

Epoch: 6| Step: 10
Training loss: 3.126218939515927
Validation loss: 2.6149936277780537

Epoch: 6| Step: 11
Training loss: 3.1081080323520744
Validation loss: 2.616629801999245

Epoch: 6| Step: 12
Training loss: 2.763344206779018
Validation loss: 2.623053141774824

Epoch: 6| Step: 13
Training loss: 1.9985956983904973
Validation loss: 2.6138792293884276

Epoch: 300| Step: 0
Training loss: 2.3470736524925266
Validation loss: 2.6158030365127565

Epoch: 6| Step: 1
Training loss: 2.791511208679993
Validation loss: 2.629721777443856

Epoch: 6| Step: 2
Training loss: 2.9245733357477963
Validation loss: 2.620599806070124

Epoch: 6| Step: 3
Training loss: 2.943608689206726
Validation loss: 2.6361447732961487

Epoch: 6| Step: 4
Training loss: 3.262408118580159
Validation loss: 2.63162640958136

Epoch: 6| Step: 5
Training loss: 3.12048593649425
Validation loss: 2.662752993014946

Epoch: 6| Step: 6
Training loss: 2.7534326457230964
Validation loss: 2.656549842892591

Epoch: 6| Step: 7
Training loss: 2.782179002231482
Validation loss: 2.666106027330037

Epoch: 6| Step: 8
Training loss: 2.678295608125054
Validation loss: 2.688849739562725

Epoch: 6| Step: 9
Training loss: 3.1815180277753528
Validation loss: 2.675883857610626

Epoch: 6| Step: 10
Training loss: 2.619409238736713
Validation loss: 2.671723254071528

Epoch: 6| Step: 11
Training loss: 3.292011452174053
Validation loss: 2.6569313155157754

Epoch: 6| Step: 12
Training loss: 3.2509591081218883
Validation loss: 2.6390903044860456

Epoch: 6| Step: 13
Training loss: 3.5707815183591767
Validation loss: 2.62622753765573

Testing loss: 2.848104185466817

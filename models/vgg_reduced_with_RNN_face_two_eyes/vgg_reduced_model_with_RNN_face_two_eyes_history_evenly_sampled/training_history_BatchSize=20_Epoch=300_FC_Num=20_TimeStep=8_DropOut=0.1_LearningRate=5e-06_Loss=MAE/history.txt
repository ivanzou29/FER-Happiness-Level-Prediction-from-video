Epoch: 1| Step: 0
Training loss: 5.263703346252441
Validation loss: 5.064966852946948

Epoch: 5| Step: 1
Training loss: 5.879920482635498
Validation loss: 5.059291342253326

Epoch: 5| Step: 2
Training loss: 4.97840690612793
Validation loss: 5.053637966032951

Epoch: 5| Step: 3
Training loss: 5.115616798400879
Validation loss: 5.048146104299894

Epoch: 5| Step: 4
Training loss: 4.597386837005615
Validation loss: 5.0423010497964835

Epoch: 5| Step: 5
Training loss: 4.6639838218688965
Validation loss: 5.03634157488423

Epoch: 5| Step: 6
Training loss: 5.290863990783691
Validation loss: 5.030209930994177

Epoch: 5| Step: 7
Training loss: 5.918851375579834
Validation loss: 5.023787375419371

Epoch: 5| Step: 8
Training loss: 3.9440200328826904
Validation loss: 5.0173673783579185

Epoch: 5| Step: 9
Training loss: 3.5418009757995605
Validation loss: 5.010598905624882

Epoch: 5| Step: 10
Training loss: 3.802868604660034
Validation loss: 5.003789255695958

Epoch: 2| Step: 0
Training loss: 6.040378570556641
Validation loss: 4.996245902071717

Epoch: 5| Step: 1
Training loss: 2.6236519813537598
Validation loss: 4.988841518279044

Epoch: 5| Step: 2
Training loss: 5.168465614318848
Validation loss: 4.980744146531628

Epoch: 5| Step: 3
Training loss: 4.205690383911133
Validation loss: 4.973331800071142

Epoch: 5| Step: 4
Training loss: 4.994715690612793
Validation loss: 4.964375026764408

Epoch: 5| Step: 5
Training loss: 6.1750898361206055
Validation loss: 4.955767149566322

Epoch: 5| Step: 6
Training loss: 4.3187761306762695
Validation loss: 4.94650230100078

Epoch: 5| Step: 7
Training loss: 4.532129287719727
Validation loss: 4.937198244115358

Epoch: 5| Step: 8
Training loss: 5.120784282684326
Validation loss: 4.927255558711226

Epoch: 5| Step: 9
Training loss: 4.2993998527526855
Validation loss: 4.916110197703044

Epoch: 5| Step: 10
Training loss: 4.741271018981934
Validation loss: 4.905745208904308

Epoch: 3| Step: 0
Training loss: 5.5731096267700195
Validation loss: 4.893603529981387

Epoch: 5| Step: 1
Training loss: 4.440046310424805
Validation loss: 4.881935745157222

Epoch: 5| Step: 2
Training loss: 4.120048999786377
Validation loss: 4.868867340908255

Epoch: 5| Step: 3
Training loss: 4.902031898498535
Validation loss: 4.855294960801319

Epoch: 5| Step: 4
Training loss: 4.0360002517700195
Validation loss: 4.841165163183725

Epoch: 5| Step: 5
Training loss: 5.013308525085449
Validation loss: 4.826917186860116

Epoch: 5| Step: 6
Training loss: 4.363961696624756
Validation loss: 4.810322966626895

Epoch: 5| Step: 7
Training loss: 4.660393714904785
Validation loss: 4.794250037080499

Epoch: 5| Step: 8
Training loss: 3.428431272506714
Validation loss: 4.7769882704622

Epoch: 5| Step: 9
Training loss: 4.932764053344727
Validation loss: 4.759024476492277

Epoch: 5| Step: 10
Training loss: 5.365087985992432
Validation loss: 4.740327512064288

Epoch: 4| Step: 0
Training loss: 5.111435413360596
Validation loss: 4.721754884207121

Epoch: 5| Step: 1
Training loss: 3.723849058151245
Validation loss: 4.702365767571234

Epoch: 5| Step: 2
Training loss: 4.676773548126221
Validation loss: 4.682063143740418

Epoch: 5| Step: 3
Training loss: 4.2295732498168945
Validation loss: 4.659540043082289

Epoch: 5| Step: 4
Training loss: 4.800804138183594
Validation loss: 4.637266271857805

Epoch: 5| Step: 5
Training loss: 4.581043243408203
Validation loss: 4.615652632969682

Epoch: 5| Step: 6
Training loss: 2.988675594329834
Validation loss: 4.591493068202849

Epoch: 5| Step: 7
Training loss: 4.441616058349609
Validation loss: 4.567402306423392

Epoch: 5| Step: 8
Training loss: 5.156887531280518
Validation loss: 4.541309664326329

Epoch: 5| Step: 9
Training loss: 4.630982398986816
Validation loss: 4.515612135651291

Epoch: 5| Step: 10
Training loss: 3.941119909286499
Validation loss: 4.487879183984572

Epoch: 5| Step: 0
Training loss: 3.5281982421875
Validation loss: 4.459128810513404

Epoch: 5| Step: 1
Training loss: 5.661988258361816
Validation loss: 4.4282964121910835

Epoch: 5| Step: 2
Training loss: 2.9800784587860107
Validation loss: 4.400414718094693

Epoch: 5| Step: 3
Training loss: 4.940205097198486
Validation loss: 4.368330560704713

Epoch: 5| Step: 4
Training loss: 4.1015496253967285
Validation loss: 4.33808401579498

Epoch: 5| Step: 5
Training loss: 3.7696337699890137
Validation loss: 4.305278208947951

Epoch: 5| Step: 6
Training loss: 4.044924259185791
Validation loss: 4.273742978290845

Epoch: 5| Step: 7
Training loss: 2.99298357963562
Validation loss: 4.239232714458178

Epoch: 5| Step: 8
Training loss: 3.9230716228485107
Validation loss: 4.206058835470548

Epoch: 5| Step: 9
Training loss: 5.2430195808410645
Validation loss: 4.17510450014504

Epoch: 5| Step: 10
Training loss: 3.954305648803711
Validation loss: 4.142709024490848

Epoch: 6| Step: 0
Training loss: 3.897265911102295
Validation loss: 4.109578932485273

Epoch: 5| Step: 1
Training loss: 4.4228949546813965
Validation loss: 4.077711128419446

Epoch: 5| Step: 2
Training loss: 3.678115129470825
Validation loss: 4.046248953829529

Epoch: 5| Step: 3
Training loss: 4.686817169189453
Validation loss: 4.015049716477753

Epoch: 5| Step: 4
Training loss: 3.93817138671875
Validation loss: 3.9809978854271675

Epoch: 5| Step: 5
Training loss: 3.1869986057281494
Validation loss: 3.9525535106658936

Epoch: 5| Step: 6
Training loss: 4.281900405883789
Validation loss: 3.9222568491453766

Epoch: 5| Step: 7
Training loss: 4.037355899810791
Validation loss: 3.891779068977602

Epoch: 5| Step: 8
Training loss: 4.356169700622559
Validation loss: 3.8634908327492337

Epoch: 5| Step: 9
Training loss: 3.191868305206299
Validation loss: 3.8365876725924912

Epoch: 5| Step: 10
Training loss: 2.025624990463257
Validation loss: 3.8111981755943707

Epoch: 7| Step: 0
Training loss: 3.698497772216797
Validation loss: 3.7900553903272076

Epoch: 5| Step: 1
Training loss: 4.037680625915527
Validation loss: 3.765152762013097

Epoch: 5| Step: 2
Training loss: 2.9018969535827637
Validation loss: 3.7431325502293085

Epoch: 5| Step: 3
Training loss: 3.715930223464966
Validation loss: 3.719579827400946

Epoch: 5| Step: 4
Training loss: 4.393475532531738
Validation loss: 3.6983683134919856

Epoch: 5| Step: 5
Training loss: 3.132542133331299
Validation loss: 3.6771984664342736

Epoch: 5| Step: 6
Training loss: 2.938486337661743
Validation loss: 3.654756389638429

Epoch: 5| Step: 7
Training loss: 3.128405809402466
Validation loss: 3.6337245715561735

Epoch: 5| Step: 8
Training loss: 3.4874367713928223
Validation loss: 3.6129910510073424

Epoch: 5| Step: 9
Training loss: 3.9369959831237793
Validation loss: 3.5914754201007146

Epoch: 5| Step: 10
Training loss: 4.080739974975586
Validation loss: 3.5738379314381588

Epoch: 8| Step: 0
Training loss: 3.424147367477417
Validation loss: 3.554421199265347

Epoch: 5| Step: 1
Training loss: 3.234172821044922
Validation loss: 3.534325066433158

Epoch: 5| Step: 2
Training loss: 3.643253803253174
Validation loss: 3.5147390365600586

Epoch: 5| Step: 3
Training loss: 4.138777732849121
Validation loss: 3.493355022963657

Epoch: 5| Step: 4
Training loss: 3.5964560508728027
Validation loss: 3.4728495100493073

Epoch: 5| Step: 5
Training loss: 3.652822971343994
Validation loss: 3.4530994584483485

Epoch: 5| Step: 6
Training loss: 3.6368472576141357
Validation loss: 3.4331487891494588

Epoch: 5| Step: 7
Training loss: 4.087808132171631
Validation loss: 3.4151661011480514

Epoch: 5| Step: 8
Training loss: 2.8043904304504395
Validation loss: 3.3961407317910144

Epoch: 5| Step: 9
Training loss: 2.8046138286590576
Validation loss: 3.379862464884276

Epoch: 5| Step: 10
Training loss: 2.2007205486297607
Validation loss: 3.364438141545942

Epoch: 9| Step: 0
Training loss: 3.388014316558838
Validation loss: 3.350474388368668

Epoch: 5| Step: 1
Training loss: 3.3752453327178955
Validation loss: 3.338079665296821

Epoch: 5| Step: 2
Training loss: 3.094130039215088
Validation loss: 3.325027504274922

Epoch: 5| Step: 3
Training loss: 3.056938648223877
Validation loss: 3.3144086919805056

Epoch: 5| Step: 4
Training loss: 3.998493194580078
Validation loss: 3.3042639840033745

Epoch: 5| Step: 5
Training loss: 2.4123849868774414
Validation loss: 3.2908071471798803

Epoch: 5| Step: 6
Training loss: 2.6427347660064697
Validation loss: 3.279147225041543

Epoch: 5| Step: 7
Training loss: 3.923928737640381
Validation loss: 3.2673231786297214

Epoch: 5| Step: 8
Training loss: 3.1190707683563232
Validation loss: 3.2536411029036327

Epoch: 5| Step: 9
Training loss: 3.2651169300079346
Validation loss: 3.2439173139551634

Epoch: 5| Step: 10
Training loss: 3.6160411834716797
Validation loss: 3.2312971648349555

Epoch: 10| Step: 0
Training loss: 1.9305013418197632
Validation loss: 3.223144236431327

Epoch: 5| Step: 1
Training loss: 3.655824661254883
Validation loss: 3.2143670564056723

Epoch: 5| Step: 2
Training loss: 2.526221990585327
Validation loss: 3.2077288166169198

Epoch: 5| Step: 3
Training loss: 3.591597080230713
Validation loss: 3.198723526411159

Epoch: 5| Step: 4
Training loss: 3.184718370437622
Validation loss: 3.1868427133047454

Epoch: 5| Step: 5
Training loss: 2.5128071308135986
Validation loss: 3.1799684724500104

Epoch: 5| Step: 6
Training loss: 3.0929384231567383
Validation loss: 3.170593612937517

Epoch: 5| Step: 7
Training loss: 3.7391490936279297
Validation loss: 3.1624314554276003

Epoch: 5| Step: 8
Training loss: 3.960458278656006
Validation loss: 3.1554837816505024

Epoch: 5| Step: 9
Training loss: 3.048227071762085
Validation loss: 3.148192867155998

Epoch: 5| Step: 10
Training loss: 3.7803568840026855
Validation loss: 3.142488248886601

Epoch: 11| Step: 0
Training loss: 3.2253708839416504
Validation loss: 3.133896217551283

Epoch: 5| Step: 1
Training loss: 3.467951536178589
Validation loss: 3.1261981353964856

Epoch: 5| Step: 2
Training loss: 3.320507526397705
Validation loss: 3.1198478155238654

Epoch: 5| Step: 3
Training loss: 2.4523568153381348
Validation loss: 3.1112383334867415

Epoch: 5| Step: 4
Training loss: 3.0965678691864014
Validation loss: 3.103205962847638

Epoch: 5| Step: 5
Training loss: 2.8671295642852783
Validation loss: 3.0958066089178926

Epoch: 5| Step: 6
Training loss: 2.4125821590423584
Validation loss: 3.0918647781495125

Epoch: 5| Step: 7
Training loss: 3.407599687576294
Validation loss: 3.087526895666635

Epoch: 5| Step: 8
Training loss: 2.762943983078003
Validation loss: 3.07831480169809

Epoch: 5| Step: 9
Training loss: 4.014613151550293
Validation loss: 3.0726898152341127

Epoch: 5| Step: 10
Training loss: 3.3665201663970947
Validation loss: 3.07063042220249

Epoch: 12| Step: 0
Training loss: 2.4826338291168213
Validation loss: 3.063444757974276

Epoch: 5| Step: 1
Training loss: 3.5068352222442627
Validation loss: 3.057181514719481

Epoch: 5| Step: 2
Training loss: 2.82143235206604
Validation loss: 3.0478221524146294

Epoch: 5| Step: 3
Training loss: 3.1980528831481934
Validation loss: 3.0450979740388933

Epoch: 5| Step: 4
Training loss: 3.9302756786346436
Validation loss: 3.039160115744478

Epoch: 5| Step: 5
Training loss: 3.755051374435425
Validation loss: 3.033182854293495

Epoch: 5| Step: 6
Training loss: 2.8421120643615723
Validation loss: 3.0296174069886566

Epoch: 5| Step: 7
Training loss: 3.718949794769287
Validation loss: 3.0241771257051857

Epoch: 5| Step: 8
Training loss: 2.8481311798095703
Validation loss: 3.019184502222205

Epoch: 5| Step: 9
Training loss: 2.472227096557617
Validation loss: 3.0166765361703853

Epoch: 5| Step: 10
Training loss: 2.2028591632843018
Validation loss: 3.008334767433905

Epoch: 13| Step: 0
Training loss: 3.4014549255371094
Validation loss: 3.0003299661861953

Epoch: 5| Step: 1
Training loss: 2.776846408843994
Validation loss: 2.9985202281705794

Epoch: 5| Step: 2
Training loss: 3.4448044300079346
Validation loss: 2.99510036489015

Epoch: 5| Step: 3
Training loss: 3.106215000152588
Validation loss: 2.9881788556293776

Epoch: 5| Step: 4
Training loss: 3.0241103172302246
Validation loss: 2.9873019059499106

Epoch: 5| Step: 5
Training loss: 2.819176197052002
Validation loss: 2.9802573803932435

Epoch: 5| Step: 6
Training loss: 2.855992317199707
Validation loss: 2.9779969492266254

Epoch: 5| Step: 7
Training loss: 3.1552939414978027
Validation loss: 2.9727439649643435

Epoch: 5| Step: 8
Training loss: 3.273383617401123
Validation loss: 2.969465891520182

Epoch: 5| Step: 9
Training loss: 2.58039927482605
Validation loss: 2.964734008235316

Epoch: 5| Step: 10
Training loss: 3.115540027618408
Validation loss: 2.96010600110536

Epoch: 14| Step: 0
Training loss: 2.7249324321746826
Validation loss: 2.959724997961393

Epoch: 5| Step: 1
Training loss: 2.7711222171783447
Validation loss: 2.9510371761937297

Epoch: 5| Step: 2
Training loss: 3.108043670654297
Validation loss: 2.9488868354469218

Epoch: 5| Step: 3
Training loss: 3.304171085357666
Validation loss: 2.9447752121956117

Epoch: 5| Step: 4
Training loss: 3.3326973915100098
Validation loss: 2.9401870414774907

Epoch: 5| Step: 5
Training loss: 2.5551540851593018
Validation loss: 2.934043945804719

Epoch: 5| Step: 6
Training loss: 3.042667865753174
Validation loss: 2.9295251677113194

Epoch: 5| Step: 7
Training loss: 3.308366298675537
Validation loss: 2.9296577643322688

Epoch: 5| Step: 8
Training loss: 3.150536298751831
Validation loss: 2.921220200036162

Epoch: 5| Step: 9
Training loss: 2.947936534881592
Validation loss: 2.9214800609055387

Epoch: 5| Step: 10
Training loss: 2.9610607624053955
Validation loss: 2.9176348024798977

Epoch: 15| Step: 0
Training loss: 2.1734466552734375
Validation loss: 2.9131974584312847

Epoch: 5| Step: 1
Training loss: 2.9319634437561035
Validation loss: 2.9092963741671656

Epoch: 5| Step: 2
Training loss: 3.7801673412323
Validation loss: 2.90681032980642

Epoch: 5| Step: 3
Training loss: 3.4324867725372314
Validation loss: 2.9006952829258417

Epoch: 5| Step: 4
Training loss: 3.138737678527832
Validation loss: 2.900208014313893

Epoch: 5| Step: 5
Training loss: 2.935239791870117
Validation loss: 2.89483582076206

Epoch: 5| Step: 6
Training loss: 2.4912588596343994
Validation loss: 2.888813885309363

Epoch: 5| Step: 7
Training loss: 3.1408183574676514
Validation loss: 2.8851809014556227

Epoch: 5| Step: 8
Training loss: 2.7762038707733154
Validation loss: 2.880885818953155

Epoch: 5| Step: 9
Training loss: 3.04040265083313
Validation loss: 2.8778208148094917

Epoch: 5| Step: 10
Training loss: 3.053330183029175
Validation loss: 2.8758340817625805

Epoch: 16| Step: 0
Training loss: 2.9311470985412598
Validation loss: 2.8688226463974162

Epoch: 5| Step: 1
Training loss: 3.127537250518799
Validation loss: 2.8654717860683316

Epoch: 5| Step: 2
Training loss: 2.83520770072937
Validation loss: 2.861609733232888

Epoch: 5| Step: 3
Training loss: 4.123461723327637
Validation loss: 2.860138172744423

Epoch: 5| Step: 4
Training loss: 2.5647130012512207
Validation loss: 2.854026589342343

Epoch: 5| Step: 5
Training loss: 3.138800621032715
Validation loss: 2.8507786963575628

Epoch: 5| Step: 6
Training loss: 2.917564630508423
Validation loss: 2.841309283369331

Epoch: 5| Step: 7
Training loss: 2.7205405235290527
Validation loss: 2.839637574329171

Epoch: 5| Step: 8
Training loss: 3.0998997688293457
Validation loss: 2.8382671904820267

Epoch: 5| Step: 9
Training loss: 2.648009777069092
Validation loss: 2.8341792732156734

Epoch: 5| Step: 10
Training loss: 2.4024720191955566
Validation loss: 2.8312015687265704

Epoch: 17| Step: 0
Training loss: 3.2817466259002686
Validation loss: 2.829550112447431

Epoch: 5| Step: 1
Training loss: 1.8782867193222046
Validation loss: 2.8296199562729045

Epoch: 5| Step: 2
Training loss: 2.464954376220703
Validation loss: 2.8303996029720513

Epoch: 5| Step: 3
Training loss: 2.787830352783203
Validation loss: 2.819916635431269

Epoch: 5| Step: 4
Training loss: 3.403193950653076
Validation loss: 2.8124247879110356

Epoch: 5| Step: 5
Training loss: 3.1490671634674072
Validation loss: 2.810616047151627

Epoch: 5| Step: 6
Training loss: 3.302680492401123
Validation loss: 2.8086035866891184

Epoch: 5| Step: 7
Training loss: 3.4058635234832764
Validation loss: 2.8090571946995233

Epoch: 5| Step: 8
Training loss: 3.186917304992676
Validation loss: 2.8028544995092575

Epoch: 5| Step: 9
Training loss: 2.059300184249878
Validation loss: 2.8015808802779003

Epoch: 5| Step: 10
Training loss: 3.503462076187134
Validation loss: 2.79963360294219

Epoch: 18| Step: 0
Training loss: 2.356022596359253
Validation loss: 2.7934099320442445

Epoch: 5| Step: 1
Training loss: 3.362835645675659
Validation loss: 2.7922667918666715

Epoch: 5| Step: 2
Training loss: 3.316791534423828
Validation loss: 2.787550328880228

Epoch: 5| Step: 3
Training loss: 2.259517192840576
Validation loss: 2.7902452689345165

Epoch: 5| Step: 4
Training loss: 3.6571316719055176
Validation loss: 2.781047010934481

Epoch: 5| Step: 5
Training loss: 3.264460802078247
Validation loss: 2.7802000994323404

Epoch: 5| Step: 6
Training loss: 2.9475903511047363
Validation loss: 2.7747773431962535

Epoch: 5| Step: 7
Training loss: 3.140773057937622
Validation loss: 2.7749810936630412

Epoch: 5| Step: 8
Training loss: 3.2466728687286377
Validation loss: 2.7727157044154342

Epoch: 5| Step: 9
Training loss: 2.6393611431121826
Validation loss: 2.7686275923123924

Epoch: 5| Step: 10
Training loss: 1.7253221273422241
Validation loss: 2.764359340872816

Epoch: 19| Step: 0
Training loss: 2.758535146713257
Validation loss: 2.7584151888406403

Epoch: 5| Step: 1
Training loss: 2.293808698654175
Validation loss: 2.7530538215432117

Epoch: 5| Step: 2
Training loss: 2.959648609161377
Validation loss: 2.7498212219566427

Epoch: 5| Step: 3
Training loss: 3.2005550861358643
Validation loss: 2.7586361182633268

Epoch: 5| Step: 4
Training loss: 2.9715588092803955
Validation loss: 2.778401626053677

Epoch: 5| Step: 5
Training loss: 2.708728790283203
Validation loss: 2.743962090502503

Epoch: 5| Step: 6
Training loss: 2.8576996326446533
Validation loss: 2.7390799906946

Epoch: 5| Step: 7
Training loss: 3.7726263999938965
Validation loss: 2.741825683142549

Epoch: 5| Step: 8
Training loss: 2.361898899078369
Validation loss: 2.7359907191286803

Epoch: 5| Step: 9
Training loss: 3.3247313499450684
Validation loss: 2.7374403502351496

Epoch: 5| Step: 10
Training loss: 2.639105796813965
Validation loss: 2.7327723554385606

Epoch: 20| Step: 0
Training loss: 2.254721164703369
Validation loss: 2.7349544853292485

Epoch: 5| Step: 1
Training loss: 2.343616008758545
Validation loss: 2.7420584617122525

Epoch: 5| Step: 2
Training loss: 2.3311214447021484
Validation loss: 2.7674520092625774

Epoch: 5| Step: 3
Training loss: 3.623781204223633
Validation loss: 2.755439401954733

Epoch: 5| Step: 4
Training loss: 3.4792754650115967
Validation loss: 2.736732913601783

Epoch: 5| Step: 5
Training loss: 2.624436140060425
Validation loss: 2.7152905079626266

Epoch: 5| Step: 6
Training loss: 2.775979518890381
Validation loss: 2.714578490103445

Epoch: 5| Step: 7
Training loss: 2.9392788410186768
Validation loss: 2.719721758237449

Epoch: 5| Step: 8
Training loss: 3.2155909538269043
Validation loss: 2.722708866160403

Epoch: 5| Step: 9
Training loss: 3.1879515647888184
Validation loss: 2.7277333454419206

Epoch: 5| Step: 10
Training loss: 2.939732551574707
Validation loss: 2.7142995147294897

Epoch: 21| Step: 0
Training loss: 3.029052734375
Validation loss: 2.706180675055391

Epoch: 5| Step: 1
Training loss: 2.425231456756592
Validation loss: 2.703896353321691

Epoch: 5| Step: 2
Training loss: 2.9749550819396973
Validation loss: 2.7023415693672757

Epoch: 5| Step: 3
Training loss: 2.4801888465881348
Validation loss: 2.7053267699415966

Epoch: 5| Step: 4
Training loss: 2.407679796218872
Validation loss: 2.7074007039429038

Epoch: 5| Step: 5
Training loss: 2.541796922683716
Validation loss: 2.702334847501529

Epoch: 5| Step: 6
Training loss: 3.2128539085388184
Validation loss: 2.7059183710364887

Epoch: 5| Step: 7
Training loss: 2.691406488418579
Validation loss: 2.699325135959092

Epoch: 5| Step: 8
Training loss: 3.6429710388183594
Validation loss: 2.69965930907957

Epoch: 5| Step: 9
Training loss: 3.1612555980682373
Validation loss: 2.693141629618983

Epoch: 5| Step: 10
Training loss: 2.9769833087921143
Validation loss: 2.693062946360598

Epoch: 22| Step: 0
Training loss: 1.9632794857025146
Validation loss: 2.68728704349969

Epoch: 5| Step: 1
Training loss: 3.1041674613952637
Validation loss: 2.681433270054479

Epoch: 5| Step: 2
Training loss: 2.606208562850952
Validation loss: 2.677861257265973

Epoch: 5| Step: 3
Training loss: 2.121969699859619
Validation loss: 2.681755737591815

Epoch: 5| Step: 4
Training loss: 2.9960453510284424
Validation loss: 2.710362826624224

Epoch: 5| Step: 5
Training loss: 3.6483681201934814
Validation loss: 2.680917263031006

Epoch: 5| Step: 6
Training loss: 2.826029062271118
Validation loss: 2.6705134043129544

Epoch: 5| Step: 7
Training loss: 3.5294861793518066
Validation loss: 2.67408666815809

Epoch: 5| Step: 8
Training loss: 3.1252028942108154
Validation loss: 2.6696547385184997

Epoch: 5| Step: 9
Training loss: 2.839312791824341
Validation loss: 2.6705515794856574

Epoch: 5| Step: 10
Training loss: 2.589719772338867
Validation loss: 2.6711348461848434

Epoch: 23| Step: 0
Training loss: 3.683741807937622
Validation loss: 2.669241605266448

Epoch: 5| Step: 1
Training loss: 3.418539047241211
Validation loss: 2.668018635883126

Epoch: 5| Step: 2
Training loss: 1.9848220348358154
Validation loss: 2.6652940165612007

Epoch: 5| Step: 3
Training loss: 3.035595417022705
Validation loss: 2.661424526604273

Epoch: 5| Step: 4
Training loss: 2.5149312019348145
Validation loss: 2.65433507837275

Epoch: 5| Step: 5
Training loss: 2.885528087615967
Validation loss: 2.6528166160788587

Epoch: 5| Step: 6
Training loss: 2.8177261352539062
Validation loss: 2.651169602588941

Epoch: 5| Step: 7
Training loss: 2.9009413719177246
Validation loss: 2.6510382288245746

Epoch: 5| Step: 8
Training loss: 2.4804863929748535
Validation loss: 2.650332917449295

Epoch: 5| Step: 9
Training loss: 2.7553019523620605
Validation loss: 2.6575262418357273

Epoch: 5| Step: 10
Training loss: 2.741934061050415
Validation loss: 2.651549893040811

Epoch: 24| Step: 0
Training loss: 2.6377968788146973
Validation loss: 2.646698236465454

Epoch: 5| Step: 1
Training loss: 3.0439445972442627
Validation loss: 2.6422981626244

Epoch: 5| Step: 2
Training loss: 3.3255181312561035
Validation loss: 2.6377730882295998

Epoch: 5| Step: 3
Training loss: 2.7653088569641113
Validation loss: 2.6375729627506708

Epoch: 5| Step: 4
Training loss: 2.9178287982940674
Validation loss: 2.6358615454807075

Epoch: 5| Step: 5
Training loss: 3.2971768379211426
Validation loss: 2.6356333481368197

Epoch: 5| Step: 6
Training loss: 3.3392422199249268
Validation loss: 2.6306437574407107

Epoch: 5| Step: 7
Training loss: 2.6048367023468018
Validation loss: 2.6278891665961153

Epoch: 5| Step: 8
Training loss: 2.788360118865967
Validation loss: 2.6266929949483564

Epoch: 5| Step: 9
Training loss: 1.945157766342163
Validation loss: 2.628585607774796

Epoch: 5| Step: 10
Training loss: 2.3465769290924072
Validation loss: 2.6250510754123813

Epoch: 25| Step: 0
Training loss: 2.5958991050720215
Validation loss: 2.623032018702517

Epoch: 5| Step: 1
Training loss: 2.7944376468658447
Validation loss: 2.6225050392971245

Epoch: 5| Step: 2
Training loss: 3.1814258098602295
Validation loss: 2.61888716554129

Epoch: 5| Step: 3
Training loss: 2.6832971572875977
Validation loss: 2.617911418279012

Epoch: 5| Step: 4
Training loss: 2.3177151679992676
Validation loss: 2.616040172115449

Epoch: 5| Step: 5
Training loss: 3.3853919506073
Validation loss: 2.6142268744848107

Epoch: 5| Step: 6
Training loss: 3.1095547676086426
Validation loss: 2.61603029312626

Epoch: 5| Step: 7
Training loss: 3.327667236328125
Validation loss: 2.61425180076271

Epoch: 5| Step: 8
Training loss: 2.4373817443847656
Validation loss: 2.613028777542935

Epoch: 5| Step: 9
Training loss: 2.491382122039795
Validation loss: 2.6093097784185924

Epoch: 5| Step: 10
Training loss: 2.524932384490967
Validation loss: 2.6070520724019697

Epoch: 26| Step: 0
Training loss: 2.8022987842559814
Validation loss: 2.6095216248625066

Epoch: 5| Step: 1
Training loss: 3.0617783069610596
Validation loss: 2.61496473896888

Epoch: 5| Step: 2
Training loss: 2.305504322052002
Validation loss: 2.615500593698153

Epoch: 5| Step: 3
Training loss: 3.321840286254883
Validation loss: 2.6436415462083716

Epoch: 5| Step: 4
Training loss: 2.931447982788086
Validation loss: 2.604888605815108

Epoch: 5| Step: 5
Training loss: 2.9300076961517334
Validation loss: 2.596312489560855

Epoch: 5| Step: 6
Training loss: 2.717853546142578
Validation loss: 2.602068075569727

Epoch: 5| Step: 7
Training loss: 2.746568202972412
Validation loss: 2.604632539133872

Epoch: 5| Step: 8
Training loss: 2.6480906009674072
Validation loss: 2.6105401336505847

Epoch: 5| Step: 9
Training loss: 3.069286346435547
Validation loss: 2.61251736199984

Epoch: 5| Step: 10
Training loss: 2.281587839126587
Validation loss: 2.615020590443765

Epoch: 27| Step: 0
Training loss: 3.043893814086914
Validation loss: 2.615239622772381

Epoch: 5| Step: 1
Training loss: 3.012460947036743
Validation loss: 2.6064213834783083

Epoch: 5| Step: 2
Training loss: 2.451005458831787
Validation loss: 2.606216615246188

Epoch: 5| Step: 3
Training loss: 2.169416904449463
Validation loss: 2.604205692968061

Epoch: 5| Step: 4
Training loss: 3.051152467727661
Validation loss: 2.6069198475089124

Epoch: 5| Step: 5
Training loss: 2.433650493621826
Validation loss: 2.6012195592285483

Epoch: 5| Step: 6
Training loss: 3.2369894981384277
Validation loss: 2.5914140965348933

Epoch: 5| Step: 7
Training loss: 2.8669333457946777
Validation loss: 2.5884719458959435

Epoch: 5| Step: 8
Training loss: 2.703115463256836
Validation loss: 2.5892118510379585

Epoch: 5| Step: 9
Training loss: 3.189840316772461
Validation loss: 2.589786316758843

Epoch: 5| Step: 10
Training loss: 2.6406545639038086
Validation loss: 2.6007080949762815

Epoch: 28| Step: 0
Training loss: 2.526940107345581
Validation loss: 2.58313855560877

Epoch: 5| Step: 1
Training loss: 2.6511588096618652
Validation loss: 2.5842727999533377

Epoch: 5| Step: 2
Training loss: 3.498857021331787
Validation loss: 2.580658351221392

Epoch: 5| Step: 3
Training loss: 2.7862868309020996
Validation loss: 2.5813998099296325

Epoch: 5| Step: 4
Training loss: 2.511117458343506
Validation loss: 2.5786497387834775

Epoch: 5| Step: 5
Training loss: 3.240891933441162
Validation loss: 2.5712624160192346

Epoch: 5| Step: 6
Training loss: 3.155803680419922
Validation loss: 2.568036781844272

Epoch: 5| Step: 7
Training loss: 2.610639810562134
Validation loss: 2.567589754699379

Epoch: 5| Step: 8
Training loss: 2.0611698627471924
Validation loss: 2.5704130434220835

Epoch: 5| Step: 9
Training loss: 2.9731364250183105
Validation loss: 2.5628849921687955

Epoch: 5| Step: 10
Training loss: 2.5765087604522705
Validation loss: 2.566796915505522

Epoch: 29| Step: 0
Training loss: 2.5101418495178223
Validation loss: 2.5703349318555606

Epoch: 5| Step: 1
Training loss: 2.2557835578918457
Validation loss: 2.5658367577419487

Epoch: 5| Step: 2
Training loss: 2.478410482406616
Validation loss: 2.5683095608988116

Epoch: 5| Step: 3
Training loss: 2.8624765872955322
Validation loss: 2.563670345531997

Epoch: 5| Step: 4
Training loss: 3.111616849899292
Validation loss: 2.558153067865679

Epoch: 5| Step: 5
Training loss: 2.7995452880859375
Validation loss: 2.5569291704444477

Epoch: 5| Step: 6
Training loss: 3.112541913986206
Validation loss: 2.5524457039371615

Epoch: 5| Step: 7
Training loss: 3.6331372261047363
Validation loss: 2.5510224450019097

Epoch: 5| Step: 8
Training loss: 2.297978162765503
Validation loss: 2.5473746586871404

Epoch: 5| Step: 9
Training loss: 2.914402723312378
Validation loss: 2.5422729433223767

Epoch: 5| Step: 10
Training loss: 2.539961099624634
Validation loss: 2.5421367409408733

Epoch: 30| Step: 0
Training loss: 2.4115958213806152
Validation loss: 2.553458167660621

Epoch: 5| Step: 1
Training loss: 2.7871999740600586
Validation loss: 2.557179794516615

Epoch: 5| Step: 2
Training loss: 2.9480233192443848
Validation loss: 2.556687634478333

Epoch: 5| Step: 3
Training loss: 2.1161773204803467
Validation loss: 2.550071603508406

Epoch: 5| Step: 4
Training loss: 3.141399621963501
Validation loss: 2.558349842666298

Epoch: 5| Step: 5
Training loss: 2.6805472373962402
Validation loss: 2.5774993896484375

Epoch: 5| Step: 6
Training loss: 2.846459150314331
Validation loss: 2.573914538147629

Epoch: 5| Step: 7
Training loss: 2.9792962074279785
Validation loss: 2.54402510581478

Epoch: 5| Step: 8
Training loss: 3.1216397285461426
Validation loss: 2.5286520706709994

Epoch: 5| Step: 9
Training loss: 2.9760146141052246
Validation loss: 2.5345450934543403

Epoch: 5| Step: 10
Training loss: 2.4638195037841797
Validation loss: 2.5356898923074045

Epoch: 31| Step: 0
Training loss: 2.4333856105804443
Validation loss: 2.5394202663052465

Epoch: 5| Step: 1
Training loss: 3.045301914215088
Validation loss: 2.5355932122917584

Epoch: 5| Step: 2
Training loss: 2.432847023010254
Validation loss: 2.532289512695805

Epoch: 5| Step: 3
Training loss: 2.7954678535461426
Validation loss: 2.5295357652889785

Epoch: 5| Step: 4
Training loss: 3.5848655700683594
Validation loss: 2.5313982976380216

Epoch: 5| Step: 5
Training loss: 2.7329821586608887
Validation loss: 2.534313396740985

Epoch: 5| Step: 6
Training loss: 3.2700424194335938
Validation loss: 2.53044887768325

Epoch: 5| Step: 7
Training loss: 2.788421392440796
Validation loss: 2.524867400046318

Epoch: 5| Step: 8
Training loss: 1.9859901666641235
Validation loss: 2.520468388834307

Epoch: 5| Step: 9
Training loss: 2.5059080123901367
Validation loss: 2.5231666513668594

Epoch: 5| Step: 10
Training loss: 2.790658950805664
Validation loss: 2.53029304165994

Epoch: 32| Step: 0
Training loss: 2.8130688667297363
Validation loss: 2.53200860433681

Epoch: 5| Step: 1
Training loss: 2.9969122409820557
Validation loss: 2.5425148087163127

Epoch: 5| Step: 2
Training loss: 3.2141449451446533
Validation loss: 2.5296339578525995

Epoch: 5| Step: 3
Training loss: 2.3344242572784424
Validation loss: 2.5138470639464674

Epoch: 5| Step: 4
Training loss: 2.6953659057617188
Validation loss: 2.5121783518022105

Epoch: 5| Step: 5
Training loss: 2.9799399375915527
Validation loss: 2.510822860143518

Epoch: 5| Step: 6
Training loss: 3.1181092262268066
Validation loss: 2.5107776939228015

Epoch: 5| Step: 7
Training loss: 2.3611416816711426
Validation loss: 2.508642569664986

Epoch: 5| Step: 8
Training loss: 2.622744083404541
Validation loss: 2.5167765207188104

Epoch: 5| Step: 9
Training loss: 2.4964609146118164
Validation loss: 2.519054200059624

Epoch: 5| Step: 10
Training loss: 2.6124300956726074
Validation loss: 2.5383855271083053

Epoch: 33| Step: 0
Training loss: 2.1877641677856445
Validation loss: 2.5419923387547976

Epoch: 5| Step: 1
Training loss: 2.237293243408203
Validation loss: 2.5612585826586654

Epoch: 5| Step: 2
Training loss: 2.9839468002319336
Validation loss: 2.574672081137216

Epoch: 5| Step: 3
Training loss: 2.9440197944641113
Validation loss: 2.557530954319944

Epoch: 5| Step: 4
Training loss: 2.788147449493408
Validation loss: 2.5364205427067255

Epoch: 5| Step: 5
Training loss: 2.6527459621429443
Validation loss: 2.524219271957233

Epoch: 5| Step: 6
Training loss: 3.3521525859832764
Validation loss: 2.5259129949795303

Epoch: 5| Step: 7
Training loss: 3.0115954875946045
Validation loss: 2.510166134885562

Epoch: 5| Step: 8
Training loss: 2.481890916824341
Validation loss: 2.5087175215444257

Epoch: 5| Step: 9
Training loss: 3.0059845447540283
Validation loss: 2.5024164697175384

Epoch: 5| Step: 10
Training loss: 2.5016896724700928
Validation loss: 2.507471256358649

Epoch: 34| Step: 0
Training loss: 3.0153086185455322
Validation loss: 2.4990437594793176

Epoch: 5| Step: 1
Training loss: 2.81475830078125
Validation loss: 2.4971132970625356

Epoch: 5| Step: 2
Training loss: 2.5842816829681396
Validation loss: 2.4992737795716975

Epoch: 5| Step: 3
Training loss: 2.738579511642456
Validation loss: 2.505673216235253

Epoch: 5| Step: 4
Training loss: 2.6372573375701904
Validation loss: 2.510119458680512

Epoch: 5| Step: 5
Training loss: 2.556509017944336
Validation loss: 2.5169935559713714

Epoch: 5| Step: 6
Training loss: 2.7795660495758057
Validation loss: 2.5399986185053343

Epoch: 5| Step: 7
Training loss: 3.071760654449463
Validation loss: 2.543697487923407

Epoch: 5| Step: 8
Training loss: 2.3349366188049316
Validation loss: 2.5549452484294934

Epoch: 5| Step: 9
Training loss: 2.622310161590576
Validation loss: 2.5665096493177515

Epoch: 5| Step: 10
Training loss: 2.884267807006836
Validation loss: 2.569606275968654

Epoch: 35| Step: 0
Training loss: 3.2391719818115234
Validation loss: 2.576106015072074

Epoch: 5| Step: 1
Training loss: 2.4720702171325684
Validation loss: 2.5303068058465117

Epoch: 5| Step: 2
Training loss: 2.3175458908081055
Validation loss: 2.491103249211465

Epoch: 5| Step: 3
Training loss: 2.698298692703247
Validation loss: 2.478672553134221

Epoch: 5| Step: 4
Training loss: 2.7025809288024902
Validation loss: 2.481391365810107

Epoch: 5| Step: 5
Training loss: 3.108222484588623
Validation loss: 2.4914492945517264

Epoch: 5| Step: 6
Training loss: 3.1415581703186035
Validation loss: 2.4941546609324794

Epoch: 5| Step: 7
Training loss: 2.3932456970214844
Validation loss: 2.4957421492504817

Epoch: 5| Step: 8
Training loss: 2.2767398357391357
Validation loss: 2.4976465086783133

Epoch: 5| Step: 9
Training loss: 2.5277352333068848
Validation loss: 2.508517970320999

Epoch: 5| Step: 10
Training loss: 3.196244955062866
Validation loss: 2.496816240331178

Epoch: 36| Step: 0
Training loss: 2.497758388519287
Validation loss: 2.493608223494663

Epoch: 5| Step: 1
Training loss: 2.522855281829834
Validation loss: 2.486959500979352

Epoch: 5| Step: 2
Training loss: 2.787074565887451
Validation loss: 2.4756942410622873

Epoch: 5| Step: 3
Training loss: 2.863421678543091
Validation loss: 2.471991144200807

Epoch: 5| Step: 4
Training loss: 2.964155673980713
Validation loss: 2.4729081071833128

Epoch: 5| Step: 5
Training loss: 2.9654510021209717
Validation loss: 2.4746105427383096

Epoch: 5| Step: 6
Training loss: 3.1520309448242188
Validation loss: 2.4723533558589157

Epoch: 5| Step: 7
Training loss: 2.019643783569336
Validation loss: 2.463710679802843

Epoch: 5| Step: 8
Training loss: 2.5216221809387207
Validation loss: 2.4669417463323122

Epoch: 5| Step: 9
Training loss: 2.839724063873291
Validation loss: 2.4706327863918838

Epoch: 5| Step: 10
Training loss: 2.7053515911102295
Validation loss: 2.47617325475139

Epoch: 37| Step: 0
Training loss: 3.046550750732422
Validation loss: 2.4857546462807605

Epoch: 5| Step: 1
Training loss: 2.2013933658599854
Validation loss: 2.4875891875195246

Epoch: 5| Step: 2
Training loss: 2.484086275100708
Validation loss: 2.4902459882920787

Epoch: 5| Step: 3
Training loss: 3.3588008880615234
Validation loss: 2.493915375842843

Epoch: 5| Step: 4
Training loss: 2.851459503173828
Validation loss: 2.5055485925366803

Epoch: 5| Step: 5
Training loss: 2.531282424926758
Validation loss: 2.4982266195358767

Epoch: 5| Step: 6
Training loss: 2.1952052116394043
Validation loss: 2.488483662246376

Epoch: 5| Step: 7
Training loss: 2.604078531265259
Validation loss: 2.513444149365989

Epoch: 5| Step: 8
Training loss: 3.5693202018737793
Validation loss: 2.50486490290652

Epoch: 5| Step: 9
Training loss: 2.5378291606903076
Validation loss: 2.4975631570303314

Epoch: 5| Step: 10
Training loss: 2.5703814029693604
Validation loss: 2.4680637826201735

Epoch: 38| Step: 0
Training loss: 2.1716835498809814
Validation loss: 2.454109925095753

Epoch: 5| Step: 1
Training loss: 2.491565227508545
Validation loss: 2.450326006899598

Epoch: 5| Step: 2
Training loss: 2.8966803550720215
Validation loss: 2.452484699987596

Epoch: 5| Step: 3
Training loss: 2.622441291809082
Validation loss: 2.4513613857248777

Epoch: 5| Step: 4
Training loss: 2.1457676887512207
Validation loss: 2.462569785374467

Epoch: 5| Step: 5
Training loss: 3.1069939136505127
Validation loss: 2.4837851037261305

Epoch: 5| Step: 6
Training loss: 2.837010145187378
Validation loss: 2.502486915998561

Epoch: 5| Step: 7
Training loss: 2.5627059936523438
Validation loss: 2.5126113096872964

Epoch: 5| Step: 8
Training loss: 3.565448045730591
Validation loss: 2.487829969775292

Epoch: 5| Step: 9
Training loss: 2.590832233428955
Validation loss: 2.4764202461447766

Epoch: 5| Step: 10
Training loss: 2.8844666481018066
Validation loss: 2.456115645747031

Epoch: 39| Step: 0
Training loss: 2.2073144912719727
Validation loss: 2.4548324359360563

Epoch: 5| Step: 1
Training loss: 3.053349733352661
Validation loss: 2.4586756203764226

Epoch: 5| Step: 2
Training loss: 2.8836276531219482
Validation loss: 2.461261405739733

Epoch: 5| Step: 3
Training loss: 2.5848889350891113
Validation loss: 2.4725702090929915

Epoch: 5| Step: 4
Training loss: 2.516587734222412
Validation loss: 2.474272892039309

Epoch: 5| Step: 5
Training loss: 2.217531204223633
Validation loss: 2.4744567742911716

Epoch: 5| Step: 6
Training loss: 2.997803211212158
Validation loss: 2.481249914374403

Epoch: 5| Step: 7
Training loss: 1.8606462478637695
Validation loss: 2.4702866692696848

Epoch: 5| Step: 8
Training loss: 3.3204307556152344
Validation loss: 2.4611172855541272

Epoch: 5| Step: 9
Training loss: 2.9774558544158936
Validation loss: 2.463145435497325

Epoch: 5| Step: 10
Training loss: 3.420267105102539
Validation loss: 2.4698309949649278

Epoch: 40| Step: 0
Training loss: 2.8835320472717285
Validation loss: 2.4549386885858353

Epoch: 5| Step: 1
Training loss: 2.102163314819336
Validation loss: 2.4502062951364825

Epoch: 5| Step: 2
Training loss: 3.176201820373535
Validation loss: 2.4459872015060915

Epoch: 5| Step: 3
Training loss: 2.961103916168213
Validation loss: 2.4476710878392702

Epoch: 5| Step: 4
Training loss: 2.8834927082061768
Validation loss: 2.441657579073342

Epoch: 5| Step: 5
Training loss: 2.7692947387695312
Validation loss: 2.4442854696704495

Epoch: 5| Step: 6
Training loss: 2.794785261154175
Validation loss: 2.4447916246229604

Epoch: 5| Step: 7
Training loss: 2.4405272006988525
Validation loss: 2.4434007008870444

Epoch: 5| Step: 8
Training loss: 2.9016823768615723
Validation loss: 2.4569460243307133

Epoch: 5| Step: 9
Training loss: 2.1853461265563965
Validation loss: 2.4759555555159047

Epoch: 5| Step: 10
Training loss: 2.730588436126709
Validation loss: 2.477528351609425

Epoch: 41| Step: 0
Training loss: 2.4096851348876953
Validation loss: 2.4726713524069837

Epoch: 5| Step: 1
Training loss: 3.1573333740234375
Validation loss: 2.4545618744306665

Epoch: 5| Step: 2
Training loss: 2.9996016025543213
Validation loss: 2.4511289622194026

Epoch: 5| Step: 3
Training loss: 2.7201690673828125
Validation loss: 2.447711080633184

Epoch: 5| Step: 4
Training loss: 2.2585253715515137
Validation loss: 2.443412793579922

Epoch: 5| Step: 5
Training loss: 2.1474339962005615
Validation loss: 2.4367843186983498

Epoch: 5| Step: 6
Training loss: 2.7465481758117676
Validation loss: 2.4250986909353607

Epoch: 5| Step: 7
Training loss: 2.746009111404419
Validation loss: 2.4304368572850383

Epoch: 5| Step: 8
Training loss: 2.5782241821289062
Validation loss: 2.4218086427257908

Epoch: 5| Step: 9
Training loss: 2.978532314300537
Validation loss: 2.428870884321069

Epoch: 5| Step: 10
Training loss: 2.89544677734375
Validation loss: 2.43208553970501

Epoch: 42| Step: 0
Training loss: 2.026291608810425
Validation loss: 2.437453144340105

Epoch: 5| Step: 1
Training loss: 3.111351728439331
Validation loss: 2.4472382401907318

Epoch: 5| Step: 2
Training loss: 2.4695181846618652
Validation loss: 2.4490383850630892

Epoch: 5| Step: 3
Training loss: 2.8419907093048096
Validation loss: 2.4663290490386305

Epoch: 5| Step: 4
Training loss: 2.672926425933838
Validation loss: 2.4736978802629697

Epoch: 5| Step: 5
Training loss: 3.260240077972412
Validation loss: 2.4677132021996284

Epoch: 5| Step: 6
Training loss: 1.9862110614776611
Validation loss: 2.454619658890591

Epoch: 5| Step: 7
Training loss: 2.670804977416992
Validation loss: 2.4412245391517557

Epoch: 5| Step: 8
Training loss: 2.694851875305176
Validation loss: 2.4554075810217086

Epoch: 5| Step: 9
Training loss: 3.3600800037384033
Validation loss: 2.465334633345245

Epoch: 5| Step: 10
Training loss: 2.45967960357666
Validation loss: 2.4479552674037155

Epoch: 43| Step: 0
Training loss: 2.6876416206359863
Validation loss: 2.434267892632433

Epoch: 5| Step: 1
Training loss: 2.893693685531616
Validation loss: 2.428431587834512

Epoch: 5| Step: 2
Training loss: 2.585277557373047
Validation loss: 2.4260188661595827

Epoch: 5| Step: 3
Training loss: 2.4565377235412598
Validation loss: 2.42589190313893

Epoch: 5| Step: 4
Training loss: 3.1149024963378906
Validation loss: 2.427526838035994

Epoch: 5| Step: 5
Training loss: 2.8906829357147217
Validation loss: 2.426816366052115

Epoch: 5| Step: 6
Training loss: 2.6589407920837402
Validation loss: 2.426856356282388

Epoch: 5| Step: 7
Training loss: 2.2885537147521973
Validation loss: 2.4215922740197953

Epoch: 5| Step: 8
Training loss: 2.438929319381714
Validation loss: 2.419626118034445

Epoch: 5| Step: 9
Training loss: 2.974092483520508
Validation loss: 2.423013242342139

Epoch: 5| Step: 10
Training loss: 2.3586363792419434
Validation loss: 2.4225595561406945

Epoch: 44| Step: 0
Training loss: 3.2197349071502686
Validation loss: 2.425295542645198

Epoch: 5| Step: 1
Training loss: 2.626832962036133
Validation loss: 2.424567855814452

Epoch: 5| Step: 2
Training loss: 3.0363051891326904
Validation loss: 2.42760758502509

Epoch: 5| Step: 3
Training loss: 2.36962890625
Validation loss: 2.4225339556253083

Epoch: 5| Step: 4
Training loss: 2.8487682342529297
Validation loss: 2.4226484708888556

Epoch: 5| Step: 5
Training loss: 3.0922293663024902
Validation loss: 2.413110912487071

Epoch: 5| Step: 6
Training loss: 2.272432804107666
Validation loss: 2.410499947045439

Epoch: 5| Step: 7
Training loss: 2.9033870697021484
Validation loss: 2.4123137176677747

Epoch: 5| Step: 8
Training loss: 2.444502592086792
Validation loss: 2.412355499882852

Epoch: 5| Step: 9
Training loss: 1.8751246929168701
Validation loss: 2.4297413364533456

Epoch: 5| Step: 10
Training loss: 3.088257074356079
Validation loss: 2.4399985318542807

Epoch: 45| Step: 0
Training loss: 3.1168391704559326
Validation loss: 2.4484814854078394

Epoch: 5| Step: 1
Training loss: 2.2204883098602295
Validation loss: 2.443279109975343

Epoch: 5| Step: 2
Training loss: 3.2504398822784424
Validation loss: 2.44628123570514

Epoch: 5| Step: 3
Training loss: 2.589831829071045
Validation loss: 2.4428686916187243

Epoch: 5| Step: 4
Training loss: 2.948538303375244
Validation loss: 2.4653355460013113

Epoch: 5| Step: 5
Training loss: 2.7754032611846924
Validation loss: 2.4577486899591263

Epoch: 5| Step: 6
Training loss: 2.3923840522766113
Validation loss: 2.4585988188302643

Epoch: 5| Step: 7
Training loss: 2.2566072940826416
Validation loss: 2.4386313294851654

Epoch: 5| Step: 8
Training loss: 2.895656108856201
Validation loss: 2.4248307648525445

Epoch: 5| Step: 9
Training loss: 2.0509490966796875
Validation loss: 2.400695736690234

Epoch: 5| Step: 10
Training loss: 3.033385992050171
Validation loss: 2.3906984765042543

Epoch: 46| Step: 0
Training loss: 2.6964011192321777
Validation loss: 2.385518886709726

Epoch: 5| Step: 1
Training loss: 2.9877820014953613
Validation loss: 2.397165336916524

Epoch: 5| Step: 2
Training loss: 2.703047752380371
Validation loss: 2.4016482137864634

Epoch: 5| Step: 3
Training loss: 2.7841949462890625
Validation loss: 2.4132317830157537

Epoch: 5| Step: 4
Training loss: 3.3818225860595703
Validation loss: 2.416329091595065

Epoch: 5| Step: 5
Training loss: 2.002568006515503
Validation loss: 2.4418060510389266

Epoch: 5| Step: 6
Training loss: 2.5970449447631836
Validation loss: 2.446964639489369

Epoch: 5| Step: 7
Training loss: 2.6135292053222656
Validation loss: 2.4461692174275718

Epoch: 5| Step: 8
Training loss: 2.732837677001953
Validation loss: 2.437775129913002

Epoch: 5| Step: 9
Training loss: 2.3059654235839844
Validation loss: 2.4263812495816137

Epoch: 5| Step: 10
Training loss: 2.6563782691955566
Validation loss: 2.4088743630275933

Epoch: 47| Step: 0
Training loss: 3.179871082305908
Validation loss: 2.391050939918846

Epoch: 5| Step: 1
Training loss: 2.600651264190674
Validation loss: 2.387148752007433

Epoch: 5| Step: 2
Training loss: 2.8672516345977783
Validation loss: 2.382398295146163

Epoch: 5| Step: 3
Training loss: 2.568408966064453
Validation loss: 2.389180580774943

Epoch: 5| Step: 4
Training loss: 3.085143566131592
Validation loss: 2.3879584958476405

Epoch: 5| Step: 5
Training loss: 2.194059371948242
Validation loss: 2.3929636042605162

Epoch: 5| Step: 6
Training loss: 2.490302562713623
Validation loss: 2.405936115531511

Epoch: 5| Step: 7
Training loss: 2.223449230194092
Validation loss: 2.4008914296345045

Epoch: 5| Step: 8
Training loss: 2.76149320602417
Validation loss: 2.400729038382089

Epoch: 5| Step: 9
Training loss: 2.7260611057281494
Validation loss: 2.39297660704582

Epoch: 5| Step: 10
Training loss: 2.660965919494629
Validation loss: 2.4026691272694576

Epoch: 48| Step: 0
Training loss: 2.485172986984253
Validation loss: 2.418676435306508

Epoch: 5| Step: 1
Training loss: 2.847860336303711
Validation loss: 2.4161446479059037

Epoch: 5| Step: 2
Training loss: 3.0119614601135254
Validation loss: 2.4167306500096477

Epoch: 5| Step: 3
Training loss: 2.3276374340057373
Validation loss: 2.4157239160230084

Epoch: 5| Step: 4
Training loss: 2.294806957244873
Validation loss: 2.4353919849600842

Epoch: 5| Step: 5
Training loss: 2.626680850982666
Validation loss: 2.464058368436752

Epoch: 5| Step: 6
Training loss: 2.7144527435302734
Validation loss: 2.4690118194908224

Epoch: 5| Step: 7
Training loss: 2.8042986392974854
Validation loss: 2.45802818318849

Epoch: 5| Step: 8
Training loss: 3.617144823074341
Validation loss: 2.4482295384971042

Epoch: 5| Step: 9
Training loss: 1.820370078086853
Validation loss: 2.4227719050581737

Epoch: 5| Step: 10
Training loss: 2.854922294616699
Validation loss: 2.4114755712529665

Epoch: 49| Step: 0
Training loss: 2.5090889930725098
Validation loss: 2.4046434125592633

Epoch: 5| Step: 1
Training loss: 2.5647671222686768
Validation loss: 2.380200247610769

Epoch: 5| Step: 2
Training loss: 2.0512890815734863
Validation loss: 2.389533737654327

Epoch: 5| Step: 3
Training loss: 3.0682761669158936
Validation loss: 2.4012870173300467

Epoch: 5| Step: 4
Training loss: 2.9044137001037598
Validation loss: 2.419707534133747

Epoch: 5| Step: 5
Training loss: 2.666213274002075
Validation loss: 2.4161678462900142

Epoch: 5| Step: 6
Training loss: 2.6997132301330566
Validation loss: 2.418509098791307

Epoch: 5| Step: 7
Training loss: 2.6558194160461426
Validation loss: 2.4100657201582387

Epoch: 5| Step: 8
Training loss: 2.6775219440460205
Validation loss: 2.396299200673257

Epoch: 5| Step: 9
Training loss: 2.1929287910461426
Validation loss: 2.393354062111147

Epoch: 5| Step: 10
Training loss: 3.278867721557617
Validation loss: 2.393201919012172

Epoch: 50| Step: 0
Training loss: 2.393460988998413
Validation loss: 2.3767673148903796

Epoch: 5| Step: 1
Training loss: 3.325141191482544
Validation loss: 2.3842450059870237

Epoch: 5| Step: 2
Training loss: 2.8023180961608887
Validation loss: 2.3922108142606673

Epoch: 5| Step: 3
Training loss: 2.5024142265319824
Validation loss: 2.382972337866342

Epoch: 5| Step: 4
Training loss: 2.543687343597412
Validation loss: 2.390378831535257

Epoch: 5| Step: 5
Training loss: 2.3593404293060303
Validation loss: 2.38840421553581

Epoch: 5| Step: 6
Training loss: 1.9667125940322876
Validation loss: 2.382104307092646

Epoch: 5| Step: 7
Training loss: 2.366062879562378
Validation loss: 2.3851049715472805

Epoch: 5| Step: 8
Training loss: 2.783043146133423
Validation loss: 2.3812542038579143

Epoch: 5| Step: 9
Training loss: 2.8799808025360107
Validation loss: 2.378022921982632

Epoch: 5| Step: 10
Training loss: 3.2279651165008545
Validation loss: 2.372485145445793

Epoch: 51| Step: 0
Training loss: 2.668323040008545
Validation loss: 2.370509470662763

Epoch: 5| Step: 1
Training loss: 2.4399824142456055
Validation loss: 2.3635133030594035

Epoch: 5| Step: 2
Training loss: 3.1266229152679443
Validation loss: 2.365404700720182

Epoch: 5| Step: 3
Training loss: 3.1873602867126465
Validation loss: 2.353356163988831

Epoch: 5| Step: 4
Training loss: 2.531201124191284
Validation loss: 2.3603648575403358

Epoch: 5| Step: 5
Training loss: 2.3675780296325684
Validation loss: 2.35413255230073

Epoch: 5| Step: 6
Training loss: 2.839812994003296
Validation loss: 2.3582375331591536

Epoch: 5| Step: 7
Training loss: 1.5570422410964966
Validation loss: 2.3656946330942135

Epoch: 5| Step: 8
Training loss: 2.5785069465637207
Validation loss: 2.369463412992416

Epoch: 5| Step: 9
Training loss: 2.800926685333252
Validation loss: 2.3724475086376233

Epoch: 5| Step: 10
Training loss: 3.00215220451355
Validation loss: 2.3784576449342953

Epoch: 52| Step: 0
Training loss: 2.359530210494995
Validation loss: 2.3764676535001366

Epoch: 5| Step: 1
Training loss: 2.5442068576812744
Validation loss: 2.3799455370954288

Epoch: 5| Step: 2
Training loss: 2.8323848247528076
Validation loss: 2.3872887472952566

Epoch: 5| Step: 3
Training loss: 2.7667131423950195
Validation loss: 2.389224003720027

Epoch: 5| Step: 4
Training loss: 3.0900230407714844
Validation loss: 2.395951094165925

Epoch: 5| Step: 5
Training loss: 2.679112195968628
Validation loss: 2.3835166731188373

Epoch: 5| Step: 6
Training loss: 2.987685203552246
Validation loss: 2.39222954421915

Epoch: 5| Step: 7
Training loss: 2.340195655822754
Validation loss: 2.3836937565957346

Epoch: 5| Step: 8
Training loss: 2.2684199810028076
Validation loss: 2.3629970217263825

Epoch: 5| Step: 9
Training loss: 2.434638261795044
Validation loss: 2.3600981953323528

Epoch: 5| Step: 10
Training loss: 2.7810161113739014
Validation loss: 2.3713237213832077

Epoch: 53| Step: 0
Training loss: 3.0416693687438965
Validation loss: 2.388130077751734

Epoch: 5| Step: 1
Training loss: 2.7130515575408936
Validation loss: 2.417366794360581

Epoch: 5| Step: 2
Training loss: 3.0349535942077637
Validation loss: 2.4177703575421403

Epoch: 5| Step: 3
Training loss: 2.3005318641662598
Validation loss: 2.424831364744453

Epoch: 5| Step: 4
Training loss: 2.8537464141845703
Validation loss: 2.416280105549802

Epoch: 5| Step: 5
Training loss: 2.5457937717437744
Validation loss: 2.4113182918999785

Epoch: 5| Step: 6
Training loss: 2.5170979499816895
Validation loss: 2.3998255857857327

Epoch: 5| Step: 7
Training loss: 2.3987085819244385
Validation loss: 2.3898800214131675

Epoch: 5| Step: 8
Training loss: 2.4381911754608154
Validation loss: 2.38188567725561

Epoch: 5| Step: 9
Training loss: 2.4731850624084473
Validation loss: 2.368871817024805

Epoch: 5| Step: 10
Training loss: 2.9242305755615234
Validation loss: 2.3605109389110277

Epoch: 54| Step: 0
Training loss: 2.327709436416626
Validation loss: 2.362149143731722

Epoch: 5| Step: 1
Training loss: 2.287815809249878
Validation loss: 2.3620173110756824

Epoch: 5| Step: 2
Training loss: 2.4308829307556152
Validation loss: 2.3628079301567486

Epoch: 5| Step: 3
Training loss: 2.949266195297241
Validation loss: 2.366431887431811

Epoch: 5| Step: 4
Training loss: 2.4089324474334717
Validation loss: 2.369365571647562

Epoch: 5| Step: 5
Training loss: 3.1863811016082764
Validation loss: 2.374183270239061

Epoch: 5| Step: 6
Training loss: 3.083894729614258
Validation loss: 2.368854855978361

Epoch: 5| Step: 7
Training loss: 2.780050039291382
Validation loss: 2.359857436149351

Epoch: 5| Step: 8
Training loss: 2.212907075881958
Validation loss: 2.3545811714664584

Epoch: 5| Step: 9
Training loss: 2.667585849761963
Validation loss: 2.3478028851170696

Epoch: 5| Step: 10
Training loss: 2.7255516052246094
Validation loss: 2.3456725176944526

Epoch: 55| Step: 0
Training loss: 2.4863905906677246
Validation loss: 2.356454505715319

Epoch: 5| Step: 1
Training loss: 2.2792460918426514
Validation loss: 2.3560927016760713

Epoch: 5| Step: 2
Training loss: 2.3544983863830566
Validation loss: 2.3494789190189813

Epoch: 5| Step: 3
Training loss: 3.136918783187866
Validation loss: 2.3439767001777567

Epoch: 5| Step: 4
Training loss: 2.5919182300567627
Validation loss: 2.343160024253271

Epoch: 5| Step: 5
Training loss: 2.6915714740753174
Validation loss: 2.3423385568844375

Epoch: 5| Step: 6
Training loss: 2.538547992706299
Validation loss: 2.347429149894304

Epoch: 5| Step: 7
Training loss: 3.0841176509857178
Validation loss: 2.3541633057337936

Epoch: 5| Step: 8
Training loss: 2.740523099899292
Validation loss: 2.3555401781553864

Epoch: 5| Step: 9
Training loss: 2.58552622795105
Validation loss: 2.3725517462658625

Epoch: 5| Step: 10
Training loss: 2.474562883377075
Validation loss: 2.3827208190835933

Epoch: 56| Step: 0
Training loss: 2.7622151374816895
Validation loss: 2.3704614024008475

Epoch: 5| Step: 1
Training loss: 2.2123329639434814
Validation loss: 2.3806798099189677

Epoch: 5| Step: 2
Training loss: 2.663397789001465
Validation loss: 2.3828820156794723

Epoch: 5| Step: 3
Training loss: 2.7160706520080566
Validation loss: 2.383665923149355

Epoch: 5| Step: 4
Training loss: 2.5186614990234375
Validation loss: 2.3849361096659014

Epoch: 5| Step: 5
Training loss: 2.5786805152893066
Validation loss: 2.3872187599059074

Epoch: 5| Step: 6
Training loss: 2.6165518760681152
Validation loss: 2.39261297769444

Epoch: 5| Step: 7
Training loss: 3.3032066822052
Validation loss: 2.3740665476809264

Epoch: 5| Step: 8
Training loss: 2.3582077026367188
Validation loss: 2.359884497939899

Epoch: 5| Step: 9
Training loss: 2.1080808639526367
Validation loss: 2.3748539570839173

Epoch: 5| Step: 10
Training loss: 3.1026508808135986
Validation loss: 2.367319389056134

Epoch: 57| Step: 0
Training loss: 3.150264263153076
Validation loss: 2.3545981837857153

Epoch: 5| Step: 1
Training loss: 2.597155809402466
Validation loss: 2.344511280777634

Epoch: 5| Step: 2
Training loss: 3.102872371673584
Validation loss: 2.3343818162077214

Epoch: 5| Step: 3
Training loss: 2.242448568344116
Validation loss: 2.3211404226159535

Epoch: 5| Step: 4
Training loss: 2.407268524169922
Validation loss: 2.325878902148175

Epoch: 5| Step: 5
Training loss: 2.6209444999694824
Validation loss: 2.3264690650406705

Epoch: 5| Step: 6
Training loss: 1.945735216140747
Validation loss: 2.3374235373671337

Epoch: 5| Step: 7
Training loss: 2.712648868560791
Validation loss: 2.3549603698074177

Epoch: 5| Step: 8
Training loss: 2.5644912719726562
Validation loss: 2.3670585719488

Epoch: 5| Step: 9
Training loss: 2.7964868545532227
Validation loss: 2.3769245301523516

Epoch: 5| Step: 10
Training loss: 2.790919780731201
Validation loss: 2.3807853601312123

Epoch: 58| Step: 0
Training loss: 1.9468997716903687
Validation loss: 2.376391133954448

Epoch: 5| Step: 1
Training loss: 2.7591562271118164
Validation loss: 2.376426807013891

Epoch: 5| Step: 2
Training loss: 3.133192777633667
Validation loss: 2.3775871543474096

Epoch: 5| Step: 3
Training loss: 3.329622268676758
Validation loss: 2.3666771304222847

Epoch: 5| Step: 4
Training loss: 2.6802706718444824
Validation loss: 2.3674705874535347

Epoch: 5| Step: 5
Training loss: 1.963658332824707
Validation loss: 2.3648668232784478

Epoch: 5| Step: 6
Training loss: 2.5392935276031494
Validation loss: 2.370251977315513

Epoch: 5| Step: 7
Training loss: 2.732725143432617
Validation loss: 2.3744612099021993

Epoch: 5| Step: 8
Training loss: 2.551818370819092
Validation loss: 2.357812314905146

Epoch: 5| Step: 9
Training loss: 2.6672937870025635
Validation loss: 2.3383859075525755

Epoch: 5| Step: 10
Training loss: 2.408968925476074
Validation loss: 2.3423712022842897

Epoch: 59| Step: 0
Training loss: 3.1569249629974365
Validation loss: 2.336615195838354

Epoch: 5| Step: 1
Training loss: 2.070345640182495
Validation loss: 2.321865415060392

Epoch: 5| Step: 2
Training loss: 2.5642573833465576
Validation loss: 2.3267708683526642

Epoch: 5| Step: 3
Training loss: 2.941814422607422
Validation loss: 2.3299888923604

Epoch: 5| Step: 4
Training loss: 2.999021530151367
Validation loss: 2.330019163829024

Epoch: 5| Step: 5
Training loss: 2.8159477710723877
Validation loss: 2.335715780975998

Epoch: 5| Step: 6
Training loss: 2.9371914863586426
Validation loss: 2.3311509419513006

Epoch: 5| Step: 7
Training loss: 2.664970874786377
Validation loss: 2.3271049812275875

Epoch: 5| Step: 8
Training loss: 2.220855474472046
Validation loss: 2.3313761629084104

Epoch: 5| Step: 9
Training loss: 2.5374293327331543
Validation loss: 2.3309670673903597

Epoch: 5| Step: 10
Training loss: 1.819412350654602
Validation loss: 2.341347902051864

Epoch: 60| Step: 0
Training loss: 2.7785465717315674
Validation loss: 2.353032612031506

Epoch: 5| Step: 1
Training loss: 2.7238070964813232
Validation loss: 2.3573226441619215

Epoch: 5| Step: 2
Training loss: 2.699763774871826
Validation loss: 2.3773768589060795

Epoch: 5| Step: 3
Training loss: 2.29791259765625
Validation loss: 2.3809113323047595

Epoch: 5| Step: 4
Training loss: 2.7691378593444824
Validation loss: 2.402690077340731

Epoch: 5| Step: 5
Training loss: 2.8852925300598145
Validation loss: 2.403209650388328

Epoch: 5| Step: 6
Training loss: 2.469580888748169
Validation loss: 2.387887042055848

Epoch: 5| Step: 7
Training loss: 2.785393238067627
Validation loss: 2.371031798342223

Epoch: 5| Step: 8
Training loss: 2.362243413925171
Validation loss: 2.342493995543449

Epoch: 5| Step: 9
Training loss: 3.1702792644500732
Validation loss: 2.328180641256353

Epoch: 5| Step: 10
Training loss: 1.9154350757598877
Validation loss: 2.3308780872693626

Epoch: 61| Step: 0
Training loss: 2.4857280254364014
Validation loss: 2.327088722618677

Epoch: 5| Step: 1
Training loss: 1.886556625366211
Validation loss: 2.320501730006228

Epoch: 5| Step: 2
Training loss: 2.363036632537842
Validation loss: 2.3230572721009612

Epoch: 5| Step: 3
Training loss: 2.378098964691162
Validation loss: 2.335105583231936

Epoch: 5| Step: 4
Training loss: 2.9404492378234863
Validation loss: 2.3299238040883052

Epoch: 5| Step: 5
Training loss: 2.9884605407714844
Validation loss: 2.3472712757766887

Epoch: 5| Step: 6
Training loss: 2.4269349575042725
Validation loss: 2.343445929147864

Epoch: 5| Step: 7
Training loss: 2.4250338077545166
Validation loss: 2.3499026247250137

Epoch: 5| Step: 8
Training loss: 3.105482578277588
Validation loss: 2.3514060743393435

Epoch: 5| Step: 9
Training loss: 2.960198402404785
Validation loss: 2.345433791478475

Epoch: 5| Step: 10
Training loss: 2.7071285247802734
Validation loss: 2.3414233807594544

Epoch: 62| Step: 0
Training loss: 2.308967351913452
Validation loss: 2.346543137745191

Epoch: 5| Step: 1
Training loss: 2.9016456604003906
Validation loss: 2.336994489034017

Epoch: 5| Step: 2
Training loss: 2.3396732807159424
Validation loss: 2.3444095234717093

Epoch: 5| Step: 3
Training loss: 2.392024040222168
Validation loss: 2.348071728983233

Epoch: 5| Step: 4
Training loss: 2.495736598968506
Validation loss: 2.3567520392838346

Epoch: 5| Step: 5
Training loss: 2.8283989429473877
Validation loss: 2.3472985811130975

Epoch: 5| Step: 6
Training loss: 2.1902859210968018
Validation loss: 2.3501142224957867

Epoch: 5| Step: 7
Training loss: 2.425901412963867
Validation loss: 2.347851868598692

Epoch: 5| Step: 8
Training loss: 2.761695384979248
Validation loss: 2.34322383839597

Epoch: 5| Step: 9
Training loss: 3.2725861072540283
Validation loss: 2.330986499786377

Epoch: 5| Step: 10
Training loss: 2.7606136798858643
Validation loss: 2.3160746635929232

Epoch: 63| Step: 0
Training loss: 3.003821611404419
Validation loss: 2.323887699393816

Epoch: 5| Step: 1
Training loss: 2.820307970046997
Validation loss: 2.332172865508705

Epoch: 5| Step: 2
Training loss: 2.4358770847320557
Validation loss: 2.331923977021248

Epoch: 5| Step: 3
Training loss: 2.778773784637451
Validation loss: 2.3415476211937527

Epoch: 5| Step: 4
Training loss: 2.512432098388672
Validation loss: 2.353835273814458

Epoch: 5| Step: 5
Training loss: 2.5545129776000977
Validation loss: 2.3544702068451913

Epoch: 5| Step: 6
Training loss: 2.8375000953674316
Validation loss: 2.359893193808935

Epoch: 5| Step: 7
Training loss: 2.441230535507202
Validation loss: 2.3507706631896315

Epoch: 5| Step: 8
Training loss: 2.1268186569213867
Validation loss: 2.3363365870650097

Epoch: 5| Step: 9
Training loss: 2.374835252761841
Validation loss: 2.3377590243534376

Epoch: 5| Step: 10
Training loss: 2.706233024597168
Validation loss: 2.330690107037944

Epoch: 64| Step: 0
Training loss: 2.2946856021881104
Validation loss: 2.331738656566989

Epoch: 5| Step: 1
Training loss: 2.7904934883117676
Validation loss: 2.3395635594603834

Epoch: 5| Step: 2
Training loss: 2.4492604732513428
Validation loss: 2.3367492614253873

Epoch: 5| Step: 3
Training loss: 2.0492749214172363
Validation loss: 2.3419814314893497

Epoch: 5| Step: 4
Training loss: 2.447434663772583
Validation loss: 2.345509477840957

Epoch: 5| Step: 5
Training loss: 3.177218437194824
Validation loss: 2.3260200100560344

Epoch: 5| Step: 6
Training loss: 2.617184638977051
Validation loss: 2.324475496046005

Epoch: 5| Step: 7
Training loss: 2.954223394393921
Validation loss: 2.3194375371420257

Epoch: 5| Step: 8
Training loss: 2.427084445953369
Validation loss: 2.3179108788890224

Epoch: 5| Step: 9
Training loss: 2.6641576290130615
Validation loss: 2.3200759580058437

Epoch: 5| Step: 10
Training loss: 2.639066219329834
Validation loss: 2.3191962601036153

Epoch: 65| Step: 0
Training loss: 2.879678726196289
Validation loss: 2.3126360985540573

Epoch: 5| Step: 1
Training loss: 2.4265544414520264
Validation loss: 2.3089946085406887

Epoch: 5| Step: 2
Training loss: 2.5005974769592285
Validation loss: 2.3127089854209655

Epoch: 5| Step: 3
Training loss: 2.5208542346954346
Validation loss: 2.311628290401992

Epoch: 5| Step: 4
Training loss: 2.1630988121032715
Validation loss: 2.322311516731016

Epoch: 5| Step: 5
Training loss: 3.3988749980926514
Validation loss: 2.3278380517036683

Epoch: 5| Step: 6
Training loss: 2.4646658897399902
Validation loss: 2.3378919145112396

Epoch: 5| Step: 7
Training loss: 2.996462345123291
Validation loss: 2.3301474560973463

Epoch: 5| Step: 8
Training loss: 2.1838889122009277
Validation loss: 2.33164886249009

Epoch: 5| Step: 9
Training loss: 2.3989675045013428
Validation loss: 2.3288515870289137

Epoch: 5| Step: 10
Training loss: 2.653541326522827
Validation loss: 2.3452984274074598

Epoch: 66| Step: 0
Training loss: 2.3183698654174805
Validation loss: 2.343828070548273

Epoch: 5| Step: 1
Training loss: 1.5146021842956543
Validation loss: 2.3429441041843866

Epoch: 5| Step: 2
Training loss: 2.5685040950775146
Validation loss: 2.3697150202207666

Epoch: 5| Step: 3
Training loss: 3.3455703258514404
Validation loss: 2.372428109568934

Epoch: 5| Step: 4
Training loss: 2.7451012134552
Validation loss: 2.3765322674987135

Epoch: 5| Step: 5
Training loss: 2.43690824508667
Validation loss: 2.373631659374442

Epoch: 5| Step: 6
Training loss: 2.2561452388763428
Validation loss: 2.364999541672327

Epoch: 5| Step: 7
Training loss: 2.9549899101257324
Validation loss: 2.373968529444869

Epoch: 5| Step: 8
Training loss: 3.370638370513916
Validation loss: 2.374141477769421

Epoch: 5| Step: 9
Training loss: 2.7769787311553955
Validation loss: 2.3558066224539154

Epoch: 5| Step: 10
Training loss: 2.334977626800537
Validation loss: 2.3403537324679795

Epoch: 67| Step: 0
Training loss: 2.2832610607147217
Validation loss: 2.347471296146352

Epoch: 5| Step: 1
Training loss: 1.973615050315857
Validation loss: 2.335741642982729

Epoch: 5| Step: 2
Training loss: 2.425365924835205
Validation loss: 2.3298451349299443

Epoch: 5| Step: 3
Training loss: 2.813882350921631
Validation loss: 2.3298357173960698

Epoch: 5| Step: 4
Training loss: 2.9407081604003906
Validation loss: 2.322945867815325

Epoch: 5| Step: 5
Training loss: 2.612128734588623
Validation loss: 2.3201136999232794

Epoch: 5| Step: 6
Training loss: 2.767688035964966
Validation loss: 2.3108013573513237

Epoch: 5| Step: 7
Training loss: 2.7174880504608154
Validation loss: 2.3006166899076073

Epoch: 5| Step: 8
Training loss: 2.437318801879883
Validation loss: 2.3017255131916334

Epoch: 5| Step: 9
Training loss: 2.938671350479126
Validation loss: 2.2989623187690653

Epoch: 5| Step: 10
Training loss: 2.87990665435791
Validation loss: 2.3002390169328257

Epoch: 68| Step: 0
Training loss: 2.442305088043213
Validation loss: 2.302446937048307

Epoch: 5| Step: 1
Training loss: 2.742494583129883
Validation loss: 2.3108186029618785

Epoch: 5| Step: 2
Training loss: 2.4543380737304688
Validation loss: 2.3214564579789356

Epoch: 5| Step: 3
Training loss: 2.481050729751587
Validation loss: 2.3339956934734056

Epoch: 5| Step: 4
Training loss: 2.8429481983184814
Validation loss: 2.3325749289604927

Epoch: 5| Step: 5
Training loss: 3.1501121520996094
Validation loss: 2.3285108432974866

Epoch: 5| Step: 6
Training loss: 2.776857376098633
Validation loss: 2.3333682091005388

Epoch: 5| Step: 7
Training loss: 2.445603609085083
Validation loss: 2.319455292917067

Epoch: 5| Step: 8
Training loss: 2.159947156906128
Validation loss: 2.3073713138539302

Epoch: 5| Step: 9
Training loss: 3.0686185359954834
Validation loss: 2.3047183290604623

Epoch: 5| Step: 10
Training loss: 2.039215326309204
Validation loss: 2.301609294388884

Epoch: 69| Step: 0
Training loss: 3.3303093910217285
Validation loss: 2.295579253986318

Epoch: 5| Step: 1
Training loss: 2.477858543395996
Validation loss: 2.296462053893715

Epoch: 5| Step: 2
Training loss: 2.587324857711792
Validation loss: 2.2924278500259563

Epoch: 5| Step: 3
Training loss: 2.6947174072265625
Validation loss: 2.2842231565906155

Epoch: 5| Step: 4
Training loss: 2.384981870651245
Validation loss: 2.291825120167066

Epoch: 5| Step: 5
Training loss: 2.2430036067962646
Validation loss: 2.287958634796963

Epoch: 5| Step: 6
Training loss: 2.6032137870788574
Validation loss: 2.2935832059511574

Epoch: 5| Step: 7
Training loss: 2.6802544593811035
Validation loss: 2.2927148444678194

Epoch: 5| Step: 8
Training loss: 2.7387001514434814
Validation loss: 2.2920221462044665

Epoch: 5| Step: 9
Training loss: 2.2935996055603027
Validation loss: 2.303471683174051

Epoch: 5| Step: 10
Training loss: 2.3735828399658203
Validation loss: 2.3250095152085826

Epoch: 70| Step: 0
Training loss: 2.7424728870391846
Validation loss: 2.334500038495628

Epoch: 5| Step: 1
Training loss: 2.5645742416381836
Validation loss: 2.3538121318304412

Epoch: 5| Step: 2
Training loss: 2.5544586181640625
Validation loss: 2.372769727501818

Epoch: 5| Step: 3
Training loss: 2.2836852073669434
Validation loss: 2.405693765609495

Epoch: 5| Step: 4
Training loss: 3.007965564727783
Validation loss: 2.387792569334789

Epoch: 5| Step: 5
Training loss: 1.9028667211532593
Validation loss: 2.3487189790253997

Epoch: 5| Step: 6
Training loss: 2.753093719482422
Validation loss: 2.3303638017305763

Epoch: 5| Step: 7
Training loss: 3.124574899673462
Validation loss: 2.299540065949963

Epoch: 5| Step: 8
Training loss: 2.3528900146484375
Validation loss: 2.29877124550522

Epoch: 5| Step: 9
Training loss: 2.966958522796631
Validation loss: 2.2856011775232132

Epoch: 5| Step: 10
Training loss: 2.1077747344970703
Validation loss: 2.2811868690675303

Epoch: 71| Step: 0
Training loss: 2.739063024520874
Validation loss: 2.27909412435306

Epoch: 5| Step: 1
Training loss: 2.4569008350372314
Validation loss: 2.284615480771629

Epoch: 5| Step: 2
Training loss: 2.503652572631836
Validation loss: 2.2828706746460288

Epoch: 5| Step: 3
Training loss: 2.6064114570617676
Validation loss: 2.286596573809142

Epoch: 5| Step: 4
Training loss: 2.6774063110351562
Validation loss: 2.285357100989229

Epoch: 5| Step: 5
Training loss: 2.4914333820343018
Validation loss: 2.2934644068441083

Epoch: 5| Step: 6
Training loss: 3.195261240005493
Validation loss: 2.3071320108188096

Epoch: 5| Step: 7
Training loss: 2.8076725006103516
Validation loss: 2.3198032840605705

Epoch: 5| Step: 8
Training loss: 2.0978989601135254
Validation loss: 2.328543560479277

Epoch: 5| Step: 9
Training loss: 2.741802453994751
Validation loss: 2.3404600056268836

Epoch: 5| Step: 10
Training loss: 1.90643310546875
Validation loss: 2.3299222825675883

Epoch: 72| Step: 0
Training loss: 2.144118070602417
Validation loss: 2.3347387595843245

Epoch: 5| Step: 1
Training loss: 2.6498990058898926
Validation loss: 2.3191160232790056

Epoch: 5| Step: 2
Training loss: 2.6886160373687744
Validation loss: 2.3250825353848037

Epoch: 5| Step: 3
Training loss: 3.104403257369995
Validation loss: 2.3130418075028287

Epoch: 5| Step: 4
Training loss: 2.7492027282714844
Validation loss: 2.3153600487657773

Epoch: 5| Step: 5
Training loss: 2.0089335441589355
Validation loss: 2.305280057332849

Epoch: 5| Step: 6
Training loss: 3.3939616680145264
Validation loss: 2.30361738768957

Epoch: 5| Step: 7
Training loss: 2.506474733352661
Validation loss: 2.302857345150363

Epoch: 5| Step: 8
Training loss: 2.6237151622772217
Validation loss: 2.2977410465158443

Epoch: 5| Step: 9
Training loss: 1.87998366355896
Validation loss: 2.3033119606715378

Epoch: 5| Step: 10
Training loss: 2.661299467086792
Validation loss: 2.308281165297313

Epoch: 73| Step: 0
Training loss: 2.2996153831481934
Validation loss: 2.3331309492870043

Epoch: 5| Step: 1
Training loss: 2.8757996559143066
Validation loss: 2.346415506896152

Epoch: 5| Step: 2
Training loss: 2.937152862548828
Validation loss: 2.3621283884971374

Epoch: 5| Step: 3
Training loss: 2.3366360664367676
Validation loss: 2.3812054818676365

Epoch: 5| Step: 4
Training loss: 2.5977799892425537
Validation loss: 2.409364074789068

Epoch: 5| Step: 5
Training loss: 3.021594285964966
Validation loss: 2.435077154508201

Epoch: 5| Step: 6
Training loss: 2.9066274166107178
Validation loss: 2.4285558372415523

Epoch: 5| Step: 7
Training loss: 2.5769710540771484
Validation loss: 2.3922209406411774

Epoch: 5| Step: 8
Training loss: 2.467111587524414
Validation loss: 2.3599927322838896

Epoch: 5| Step: 9
Training loss: 2.2374205589294434
Validation loss: 2.3250502412037184

Epoch: 5| Step: 10
Training loss: 2.21455454826355
Validation loss: 2.3059808259369223

Epoch: 74| Step: 0
Training loss: 2.054089069366455
Validation loss: 2.2910661774296917

Epoch: 5| Step: 1
Training loss: 2.4887771606445312
Validation loss: 2.288077759486373

Epoch: 5| Step: 2
Training loss: 2.4564826488494873
Validation loss: 2.2836887785183486

Epoch: 5| Step: 3
Training loss: 2.5217232704162598
Validation loss: 2.289831884445683

Epoch: 5| Step: 4
Training loss: 2.2403860092163086
Validation loss: 2.2893648750038555

Epoch: 5| Step: 5
Training loss: 2.8250625133514404
Validation loss: 2.2848621427371936

Epoch: 5| Step: 6
Training loss: 2.6793835163116455
Validation loss: 2.292534820495113

Epoch: 5| Step: 7
Training loss: 2.8643736839294434
Validation loss: 2.2974332737666305

Epoch: 5| Step: 8
Training loss: 3.180912733078003
Validation loss: 2.2911124306340374

Epoch: 5| Step: 9
Training loss: 2.5034446716308594
Validation loss: 2.2859540498384865

Epoch: 5| Step: 10
Training loss: 2.752269983291626
Validation loss: 2.285894855376213

Epoch: 75| Step: 0
Training loss: 2.1148102283477783
Validation loss: 2.2918856169587825

Epoch: 5| Step: 1
Training loss: 2.3065500259399414
Validation loss: 2.292298178518972

Epoch: 5| Step: 2
Training loss: 2.7675564289093018
Validation loss: 2.3026020911432084

Epoch: 5| Step: 3
Training loss: 2.746278762817383
Validation loss: 2.293835065698111

Epoch: 5| Step: 4
Training loss: 2.312800168991089
Validation loss: 2.299049772242064

Epoch: 5| Step: 5
Training loss: 2.8088696002960205
Validation loss: 2.3001832859490507

Epoch: 5| Step: 6
Training loss: 2.8631255626678467
Validation loss: 2.3049522446047876

Epoch: 5| Step: 7
Training loss: 2.68180775642395
Validation loss: 2.295373309043146

Epoch: 5| Step: 8
Training loss: 2.390045642852783
Validation loss: 2.3218233995540167

Epoch: 5| Step: 9
Training loss: 2.624556303024292
Validation loss: 2.332372534659601

Epoch: 5| Step: 10
Training loss: 2.736971616744995
Validation loss: 2.3405307287810952

Epoch: 76| Step: 0
Training loss: 2.562429189682007
Validation loss: 2.334450537158597

Epoch: 5| Step: 1
Training loss: 2.3708441257476807
Validation loss: 2.317096528186593

Epoch: 5| Step: 2
Training loss: 2.9598135948181152
Validation loss: 2.3191153336596746

Epoch: 5| Step: 3
Training loss: 2.61392879486084
Validation loss: 2.3023799157911733

Epoch: 5| Step: 4
Training loss: 2.702603816986084
Validation loss: 2.2977371113274687

Epoch: 5| Step: 5
Training loss: 2.5569777488708496
Validation loss: 2.287472786441926

Epoch: 5| Step: 6
Training loss: 2.4332385063171387
Validation loss: 2.294276568197435

Epoch: 5| Step: 7
Training loss: 2.3824992179870605
Validation loss: 2.290840030998312

Epoch: 5| Step: 8
Training loss: 2.536562442779541
Validation loss: 2.303434633439587

Epoch: 5| Step: 9
Training loss: 2.9337356090545654
Validation loss: 2.30797077250737

Epoch: 5| Step: 10
Training loss: 2.195720911026001
Validation loss: 2.3147016494504866

Epoch: 77| Step: 0
Training loss: 2.4016964435577393
Validation loss: 2.3207188524225706

Epoch: 5| Step: 1
Training loss: 2.8770618438720703
Validation loss: 2.3231300384767595

Epoch: 5| Step: 2
Training loss: 1.8141292333602905
Validation loss: 2.32205932371078

Epoch: 5| Step: 3
Training loss: 3.2519726753234863
Validation loss: 2.3288576949027275

Epoch: 5| Step: 4
Training loss: 2.5147128105163574
Validation loss: 2.326949752787108

Epoch: 5| Step: 5
Training loss: 2.434051036834717
Validation loss: 2.3266555596423406

Epoch: 5| Step: 6
Training loss: 2.7278518676757812
Validation loss: 2.3341688340710056

Epoch: 5| Step: 7
Training loss: 1.9552650451660156
Validation loss: 2.3277327399100027

Epoch: 5| Step: 8
Training loss: 2.6537258625030518
Validation loss: 2.341452480644308

Epoch: 5| Step: 9
Training loss: 3.0167598724365234
Validation loss: 2.3150967603088706

Epoch: 5| Step: 10
Training loss: 2.4939751625061035
Validation loss: 2.305139018643287

Epoch: 78| Step: 0
Training loss: 2.7280707359313965
Validation loss: 2.2857957296474005

Epoch: 5| Step: 1
Training loss: 3.073833703994751
Validation loss: 2.2664898210956204

Epoch: 5| Step: 2
Training loss: 2.5873541831970215
Validation loss: 2.2706123577651156

Epoch: 5| Step: 3
Training loss: 2.701691150665283
Validation loss: 2.26874868587781

Epoch: 5| Step: 4
Training loss: 2.3805601596832275
Validation loss: 2.2792901249342066

Epoch: 5| Step: 5
Training loss: 2.9885318279266357
Validation loss: 2.2901595279734623

Epoch: 5| Step: 6
Training loss: 2.387772798538208
Validation loss: 2.2867249135048158

Epoch: 5| Step: 7
Training loss: 2.601963520050049
Validation loss: 2.286715074252057

Epoch: 5| Step: 8
Training loss: 2.680567502975464
Validation loss: 2.28351911165381

Epoch: 5| Step: 9
Training loss: 2.5143725872039795
Validation loss: 2.2704362946171917

Epoch: 5| Step: 10
Training loss: 1.8831212520599365
Validation loss: 2.2569574822661695

Epoch: 79| Step: 0
Training loss: 2.728353261947632
Validation loss: 2.259414603633265

Epoch: 5| Step: 1
Training loss: 3.075171947479248
Validation loss: 2.268640190042475

Epoch: 5| Step: 2
Training loss: 2.7197587490081787
Validation loss: 2.280636054213329

Epoch: 5| Step: 3
Training loss: 2.2558860778808594
Validation loss: 2.2996905760098527

Epoch: 5| Step: 4
Training loss: 2.344705104827881
Validation loss: 2.307291248793243

Epoch: 5| Step: 5
Training loss: 2.085113763809204
Validation loss: 2.3302380243937173

Epoch: 5| Step: 6
Training loss: 2.582517147064209
Validation loss: 2.348827603042767

Epoch: 5| Step: 7
Training loss: 2.8396999835968018
Validation loss: 2.348458141408941

Epoch: 5| Step: 8
Training loss: 2.9390292167663574
Validation loss: 2.3692330673176754

Epoch: 5| Step: 9
Training loss: 2.2883007526397705
Validation loss: 2.347500649831628

Epoch: 5| Step: 10
Training loss: 2.6136081218719482
Validation loss: 2.326239098784744

Epoch: 80| Step: 0
Training loss: 2.4895424842834473
Validation loss: 2.3054626834008003

Epoch: 5| Step: 1
Training loss: 2.8717215061187744
Validation loss: 2.2732940386700373

Epoch: 5| Step: 2
Training loss: 3.0538103580474854
Validation loss: 2.27255815844382

Epoch: 5| Step: 3
Training loss: 2.3124585151672363
Validation loss: 2.2603184817939677

Epoch: 5| Step: 4
Training loss: 2.9766783714294434
Validation loss: 2.2688375134621896

Epoch: 5| Step: 5
Training loss: 2.4550182819366455
Validation loss: 2.27641212812034

Epoch: 5| Step: 6
Training loss: 2.1927192211151123
Validation loss: 2.2836966617133028

Epoch: 5| Step: 7
Training loss: 2.3429224491119385
Validation loss: 2.2900211016337075

Epoch: 5| Step: 8
Training loss: 2.4768996238708496
Validation loss: 2.2916550995201193

Epoch: 5| Step: 9
Training loss: 2.5213093757629395
Validation loss: 2.2970551598456597

Epoch: 5| Step: 10
Training loss: 2.4873533248901367
Validation loss: 2.3169233593889462

Epoch: 81| Step: 0
Training loss: 2.3873202800750732
Validation loss: 2.3205150263283842

Epoch: 5| Step: 1
Training loss: 2.7356982231140137
Validation loss: 2.310362590256558

Epoch: 5| Step: 2
Training loss: 1.7811400890350342
Validation loss: 2.3056538463920675

Epoch: 5| Step: 3
Training loss: 2.5698673725128174
Validation loss: 2.317176462501608

Epoch: 5| Step: 4
Training loss: 2.765185832977295
Validation loss: 2.3141233331413678

Epoch: 5| Step: 5
Training loss: 2.2982373237609863
Validation loss: 2.3174723297037105

Epoch: 5| Step: 6
Training loss: 2.81959867477417
Validation loss: 2.307533020614296

Epoch: 5| Step: 7
Training loss: 2.653191328048706
Validation loss: 2.296626570404217

Epoch: 5| Step: 8
Training loss: 2.6671833992004395
Validation loss: 2.283540751344414

Epoch: 5| Step: 9
Training loss: 2.3718340396881104
Validation loss: 2.27702957840376

Epoch: 5| Step: 10
Training loss: 3.1874403953552246
Validation loss: 2.2800651083710375

Epoch: 82| Step: 0
Training loss: 2.8151488304138184
Validation loss: 2.282853267526114

Epoch: 5| Step: 1
Training loss: 1.962423324584961
Validation loss: 2.2764404922403316

Epoch: 5| Step: 2
Training loss: 2.514838695526123
Validation loss: 2.271035340524489

Epoch: 5| Step: 3
Training loss: 2.2632038593292236
Validation loss: 2.2745454183188816

Epoch: 5| Step: 4
Training loss: 2.6179981231689453
Validation loss: 2.2803423686694075

Epoch: 5| Step: 5
Training loss: 2.1499762535095215
Validation loss: 2.2691057933274137

Epoch: 5| Step: 6
Training loss: 3.0245704650878906
Validation loss: 2.2708421522571194

Epoch: 5| Step: 7
Training loss: 3.726879119873047
Validation loss: 2.274868767748597

Epoch: 5| Step: 8
Training loss: 1.9070396423339844
Validation loss: 2.2747358122179584

Epoch: 5| Step: 9
Training loss: 2.440207004547119
Validation loss: 2.2823757561304236

Epoch: 5| Step: 10
Training loss: 2.7237555980682373
Validation loss: 2.2931749359253915

Epoch: 83| Step: 0
Training loss: 2.4520928859710693
Validation loss: 2.298560018180519

Epoch: 5| Step: 1
Training loss: 2.4612247943878174
Validation loss: 2.3006758997517247

Epoch: 5| Step: 2
Training loss: 2.5075459480285645
Validation loss: 2.3036041259765625

Epoch: 5| Step: 3
Training loss: 2.387477397918701
Validation loss: 2.306973506045598

Epoch: 5| Step: 4
Training loss: 2.625603199005127
Validation loss: 2.304369121469477

Epoch: 5| Step: 5
Training loss: 2.5955119132995605
Validation loss: 2.3133365159393637

Epoch: 5| Step: 6
Training loss: 2.782977342605591
Validation loss: 2.3186437135101645

Epoch: 5| Step: 7
Training loss: 1.614935278892517
Validation loss: 2.3109947327644593

Epoch: 5| Step: 8
Training loss: 2.541740894317627
Validation loss: 2.3067735523305912

Epoch: 5| Step: 9
Training loss: 3.1246886253356934
Validation loss: 2.308772717752764

Epoch: 5| Step: 10
Training loss: 3.0615248680114746
Validation loss: 2.318655567784463

Epoch: 84| Step: 0
Training loss: 2.540935516357422
Validation loss: 2.3029576783539145

Epoch: 5| Step: 1
Training loss: 2.0145375728607178
Validation loss: 2.313439102583034

Epoch: 5| Step: 2
Training loss: 2.627542734146118
Validation loss: 2.3212902469019734

Epoch: 5| Step: 3
Training loss: 2.6424965858459473
Validation loss: 2.3184180439159436

Epoch: 5| Step: 4
Training loss: 2.4007551670074463
Validation loss: 2.304962060784781

Epoch: 5| Step: 5
Training loss: 2.9366345405578613
Validation loss: 2.3057756398313787

Epoch: 5| Step: 6
Training loss: 3.2852072715759277
Validation loss: 2.300838952423424

Epoch: 5| Step: 7
Training loss: 2.218876600265503
Validation loss: 2.2856897538708103

Epoch: 5| Step: 8
Training loss: 2.03035044670105
Validation loss: 2.274164976612214

Epoch: 5| Step: 9
Training loss: 2.5747103691101074
Validation loss: 2.2699297474276636

Epoch: 5| Step: 10
Training loss: 2.830509901046753
Validation loss: 2.2672385361886795

Epoch: 85| Step: 0
Training loss: 2.5315310955047607
Validation loss: 2.2657723016636346

Epoch: 5| Step: 1
Training loss: 2.0005264282226562
Validation loss: 2.2677078067615466

Epoch: 5| Step: 2
Training loss: 2.260232925415039
Validation loss: 2.2827376883517028

Epoch: 5| Step: 3
Training loss: 2.7834877967834473
Validation loss: 2.3045259342398694

Epoch: 5| Step: 4
Training loss: 2.206454038619995
Validation loss: 2.326841510752196

Epoch: 5| Step: 5
Training loss: 2.4956376552581787
Validation loss: 2.349646255534182

Epoch: 5| Step: 6
Training loss: 2.42893123626709
Validation loss: 2.369188949625979

Epoch: 5| Step: 7
Training loss: 2.975935935974121
Validation loss: 2.372568832930698

Epoch: 5| Step: 8
Training loss: 2.885878562927246
Validation loss: 2.385120489264047

Epoch: 5| Step: 9
Training loss: 2.7830650806427
Validation loss: 2.328837586987403

Epoch: 5| Step: 10
Training loss: 2.8548667430877686
Validation loss: 2.2825286055123932

Epoch: 86| Step: 0
Training loss: 2.374166965484619
Validation loss: 2.2748702418419624

Epoch: 5| Step: 1
Training loss: 2.819192886352539
Validation loss: 2.2717828186609412

Epoch: 5| Step: 2
Training loss: 2.1857171058654785
Validation loss: 2.258353679410873

Epoch: 5| Step: 3
Training loss: 2.475947380065918
Validation loss: 2.2583280942773305

Epoch: 5| Step: 4
Training loss: 2.8280563354492188
Validation loss: 2.255323158797397

Epoch: 5| Step: 5
Training loss: 2.510568141937256
Validation loss: 2.263709463098998

Epoch: 5| Step: 6
Training loss: 3.190488338470459
Validation loss: 2.277771298603345

Epoch: 5| Step: 7
Training loss: 2.1269335746765137
Validation loss: 2.276599822505828

Epoch: 5| Step: 8
Training loss: 2.180007219314575
Validation loss: 2.284801342154062

Epoch: 5| Step: 9
Training loss: 3.0823798179626465
Validation loss: 2.2790010898343978

Epoch: 5| Step: 10
Training loss: 2.228292942047119
Validation loss: 2.2771750034824496

Epoch: 87| Step: 0
Training loss: 3.4165961742401123
Validation loss: 2.2738144910463722

Epoch: 5| Step: 1
Training loss: 2.380570411682129
Validation loss: 2.2709456848841842

Epoch: 5| Step: 2
Training loss: 1.901196837425232
Validation loss: 2.26128230556365

Epoch: 5| Step: 3
Training loss: 2.5056509971618652
Validation loss: 2.2687161532781457

Epoch: 5| Step: 4
Training loss: 3.1688709259033203
Validation loss: 2.2723880506330922

Epoch: 5| Step: 5
Training loss: 1.8076671361923218
Validation loss: 2.2661341185210855

Epoch: 5| Step: 6
Training loss: 2.7925376892089844
Validation loss: 2.2766989866892495

Epoch: 5| Step: 7
Training loss: 2.1158783435821533
Validation loss: 2.28341483300732

Epoch: 5| Step: 8
Training loss: 2.536426544189453
Validation loss: 2.288130460246917

Epoch: 5| Step: 9
Training loss: 2.9894919395446777
Validation loss: 2.290379419121691

Epoch: 5| Step: 10
Training loss: 2.2167932987213135
Validation loss: 2.3078415829648256

Epoch: 88| Step: 0
Training loss: 2.698392152786255
Validation loss: 2.3044489122206167

Epoch: 5| Step: 1
Training loss: 2.2026753425598145
Validation loss: 2.319804888899608

Epoch: 5| Step: 2
Training loss: 1.980858564376831
Validation loss: 2.310915393214072

Epoch: 5| Step: 3
Training loss: 2.364582061767578
Validation loss: 2.3097167655985844

Epoch: 5| Step: 4
Training loss: 2.930983066558838
Validation loss: 2.3146304468954764

Epoch: 5| Step: 5
Training loss: 2.423067092895508
Validation loss: 2.3170374567790697

Epoch: 5| Step: 6
Training loss: 2.7192039489746094
Validation loss: 2.317359744861562

Epoch: 5| Step: 7
Training loss: 2.8901352882385254
Validation loss: 2.297701815123199

Epoch: 5| Step: 8
Training loss: 2.7227697372436523
Validation loss: 2.268862955031856

Epoch: 5| Step: 9
Training loss: 2.908970832824707
Validation loss: 2.2702103276406564

Epoch: 5| Step: 10
Training loss: 2.1475212574005127
Validation loss: 2.2422060633218415

Epoch: 89| Step: 0
Training loss: 2.5023083686828613
Validation loss: 2.2426239213635846

Epoch: 5| Step: 1
Training loss: 2.7636513710021973
Validation loss: 2.238989535198417

Epoch: 5| Step: 2
Training loss: 2.4863405227661133
Validation loss: 2.2410517482347387

Epoch: 5| Step: 3
Training loss: 2.769434690475464
Validation loss: 2.2465490628314275

Epoch: 5| Step: 4
Training loss: 3.0507278442382812
Validation loss: 2.255608258708831

Epoch: 5| Step: 5
Training loss: 2.270338773727417
Validation loss: 2.2595647791380524

Epoch: 5| Step: 6
Training loss: 2.6462905406951904
Validation loss: 2.261689201478035

Epoch: 5| Step: 7
Training loss: 2.004030466079712
Validation loss: 2.270220643730574

Epoch: 5| Step: 8
Training loss: 2.8661303520202637
Validation loss: 2.268882474591655

Epoch: 5| Step: 9
Training loss: 2.194251298904419
Validation loss: 2.2711013029980403

Epoch: 5| Step: 10
Training loss: 2.439636707305908
Validation loss: 2.269969718430632

Epoch: 90| Step: 0
Training loss: 2.433422803878784
Validation loss: 2.268108501229235

Epoch: 5| Step: 1
Training loss: 3.1431658267974854
Validation loss: 2.264359372918324

Epoch: 5| Step: 2
Training loss: 2.8746092319488525
Validation loss: 2.2626170086604294

Epoch: 5| Step: 3
Training loss: 2.7177205085754395
Validation loss: 2.2746715199562813

Epoch: 5| Step: 4
Training loss: 2.0252511501312256
Validation loss: 2.268249252791046

Epoch: 5| Step: 5
Training loss: 1.9873968362808228
Validation loss: 2.293243904267588

Epoch: 5| Step: 6
Training loss: 1.7469971179962158
Validation loss: 2.308267511347289

Epoch: 5| Step: 7
Training loss: 3.242380142211914
Validation loss: 2.3118725412635395

Epoch: 5| Step: 8
Training loss: 2.468923568725586
Validation loss: 2.3117023565435924

Epoch: 5| Step: 9
Training loss: 2.9898715019226074
Validation loss: 2.3104356693965133

Epoch: 5| Step: 10
Training loss: 2.1237235069274902
Validation loss: 2.303935102237168

Epoch: 91| Step: 0
Training loss: 2.691559314727783
Validation loss: 2.2811768157507784

Epoch: 5| Step: 1
Training loss: 2.7546775341033936
Validation loss: 2.2942561052178823

Epoch: 5| Step: 2
Training loss: 2.1820836067199707
Validation loss: 2.2873280740553334

Epoch: 5| Step: 3
Training loss: 2.6703708171844482
Validation loss: 2.2902637297107327

Epoch: 5| Step: 4
Training loss: 2.2181878089904785
Validation loss: 2.2957228127346245

Epoch: 5| Step: 5
Training loss: 2.3345937728881836
Validation loss: 2.286265624466763

Epoch: 5| Step: 6
Training loss: 2.5380067825317383
Validation loss: 2.2822718235754196

Epoch: 5| Step: 7
Training loss: 2.929730176925659
Validation loss: 2.288743888178179

Epoch: 5| Step: 8
Training loss: 2.304271697998047
Validation loss: 2.2685370740070137

Epoch: 5| Step: 9
Training loss: 1.872216820716858
Validation loss: 2.270036025713849

Epoch: 5| Step: 10
Training loss: 3.4892466068267822
Validation loss: 2.265132446442881

Epoch: 92| Step: 0
Training loss: 2.238767147064209
Validation loss: 2.2765641597009476

Epoch: 5| Step: 1
Training loss: 2.5340492725372314
Validation loss: 2.2692164067299134

Epoch: 5| Step: 2
Training loss: 2.5304081439971924
Validation loss: 2.2786888448140954

Epoch: 5| Step: 3
Training loss: 2.640768051147461
Validation loss: 2.2580325475303074

Epoch: 5| Step: 4
Training loss: 3.0317602157592773
Validation loss: 2.262052874411306

Epoch: 5| Step: 5
Training loss: 1.7782970666885376
Validation loss: 2.2519455878965315

Epoch: 5| Step: 6
Training loss: 2.970219135284424
Validation loss: 2.255672485597672

Epoch: 5| Step: 7
Training loss: 2.291614055633545
Validation loss: 2.281247720923475

Epoch: 5| Step: 8
Training loss: 2.922081470489502
Validation loss: 2.284395287113805

Epoch: 5| Step: 9
Training loss: 2.653592824935913
Validation loss: 2.2987790774273615

Epoch: 5| Step: 10
Training loss: 2.2286646366119385
Validation loss: 2.2991026960393435

Epoch: 93| Step: 0
Training loss: 1.5356467962265015
Validation loss: 2.3138277274306103

Epoch: 5| Step: 1
Training loss: 2.216752529144287
Validation loss: 2.306019195946314

Epoch: 5| Step: 2
Training loss: 3.2579574584960938
Validation loss: 2.307748040845317

Epoch: 5| Step: 3
Training loss: 2.6150810718536377
Validation loss: 2.315473961573775

Epoch: 5| Step: 4
Training loss: 2.511101484298706
Validation loss: 2.315649470975322

Epoch: 5| Step: 5
Training loss: 1.6353839635849
Validation loss: 2.2856725826058337

Epoch: 5| Step: 6
Training loss: 2.4250192642211914
Validation loss: 2.2919110457102456

Epoch: 5| Step: 7
Training loss: 2.7189342975616455
Validation loss: 2.2859846084348616

Epoch: 5| Step: 8
Training loss: 2.7952077388763428
Validation loss: 2.3110685925329886

Epoch: 5| Step: 9
Training loss: 3.4755001068115234
Validation loss: 2.286315287313154

Epoch: 5| Step: 10
Training loss: 2.6090340614318848
Validation loss: 2.2740118811207433

Epoch: 94| Step: 0
Training loss: 2.356447696685791
Validation loss: 2.275346153525896

Epoch: 5| Step: 1
Training loss: 2.606257200241089
Validation loss: 2.2655371363444994

Epoch: 5| Step: 2
Training loss: 2.927077054977417
Validation loss: 2.260013411121984

Epoch: 5| Step: 3
Training loss: 1.6974923610687256
Validation loss: 2.268716658315351

Epoch: 5| Step: 4
Training loss: 2.7716498374938965
Validation loss: 2.2650198577552714

Epoch: 5| Step: 5
Training loss: 2.581997871398926
Validation loss: 2.285714867294476

Epoch: 5| Step: 6
Training loss: 2.919572591781616
Validation loss: 2.2828747226345922

Epoch: 5| Step: 7
Training loss: 2.272077798843384
Validation loss: 2.28343225807272

Epoch: 5| Step: 8
Training loss: 2.3362040519714355
Validation loss: 2.2780974526559152

Epoch: 5| Step: 9
Training loss: 2.2775988578796387
Validation loss: 2.2652993022754626

Epoch: 5| Step: 10
Training loss: 2.9928925037384033
Validation loss: 2.2589276144581456

Epoch: 95| Step: 0
Training loss: 2.3569133281707764
Validation loss: 2.259681360695952

Epoch: 5| Step: 1
Training loss: 3.0541393756866455
Validation loss: 2.2604056378846527

Epoch: 5| Step: 2
Training loss: 2.2368710041046143
Validation loss: 2.2647746480921263

Epoch: 5| Step: 3
Training loss: 2.291902542114258
Validation loss: 2.281703846428984

Epoch: 5| Step: 4
Training loss: 2.7318358421325684
Validation loss: 2.3083983877653718

Epoch: 5| Step: 5
Training loss: 2.699571371078491
Validation loss: 2.3005336715329077

Epoch: 5| Step: 6
Training loss: 2.270493984222412
Validation loss: 2.303855656295694

Epoch: 5| Step: 7
Training loss: 2.4404349327087402
Validation loss: 2.274829269737326

Epoch: 5| Step: 8
Training loss: 2.5036838054656982
Validation loss: 2.265345678534559

Epoch: 5| Step: 9
Training loss: 2.176935911178589
Validation loss: 2.250624354167651

Epoch: 5| Step: 10
Training loss: 2.915822744369507
Validation loss: 2.248341465509066

Epoch: 96| Step: 0
Training loss: 2.6644020080566406
Validation loss: 2.2453764254047024

Epoch: 5| Step: 1
Training loss: 2.1598780155181885
Validation loss: 2.239794144066431

Epoch: 5| Step: 2
Training loss: 3.0513648986816406
Validation loss: 2.244806794710057

Epoch: 5| Step: 3
Training loss: 2.5561110973358154
Validation loss: 2.2455729361503356

Epoch: 5| Step: 4
Training loss: 2.310368061065674
Validation loss: 2.2531125904411398

Epoch: 5| Step: 5
Training loss: 2.360159158706665
Validation loss: 2.257890652584773

Epoch: 5| Step: 6
Training loss: 2.5514214038848877
Validation loss: 2.2614649675225698

Epoch: 5| Step: 7
Training loss: 2.3765931129455566
Validation loss: 2.265555768884638

Epoch: 5| Step: 8
Training loss: 2.529196262359619
Validation loss: 2.2755090267427507

Epoch: 5| Step: 9
Training loss: 3.113617420196533
Validation loss: 2.2829524599095827

Epoch: 5| Step: 10
Training loss: 1.8636654615402222
Validation loss: 2.274547314131132

Epoch: 97| Step: 0
Training loss: 2.708786725997925
Validation loss: 2.2827753354144353

Epoch: 5| Step: 1
Training loss: 2.5436158180236816
Validation loss: 2.276336113611857

Epoch: 5| Step: 2
Training loss: 2.3926939964294434
Validation loss: 2.27468284996607

Epoch: 5| Step: 3
Training loss: 2.58603572845459
Validation loss: 2.302700670816565

Epoch: 5| Step: 4
Training loss: 2.220407724380493
Validation loss: 2.305970404737739

Epoch: 5| Step: 5
Training loss: 2.524146556854248
Validation loss: 2.302799411999282

Epoch: 5| Step: 6
Training loss: 2.501582384109497
Validation loss: 2.279716399408156

Epoch: 5| Step: 7
Training loss: 2.9135582447052
Validation loss: 2.2674642301374868

Epoch: 5| Step: 8
Training loss: 1.8798589706420898
Validation loss: 2.2414845856287147

Epoch: 5| Step: 9
Training loss: 3.367945909500122
Validation loss: 2.2312766377643873

Epoch: 5| Step: 10
Training loss: 2.1789655685424805
Validation loss: 2.22559037516194

Epoch: 98| Step: 0
Training loss: 2.4494376182556152
Validation loss: 2.219847063864431

Epoch: 5| Step: 1
Training loss: 2.573854923248291
Validation loss: 2.2212916420352076

Epoch: 5| Step: 2
Training loss: 1.8859808444976807
Validation loss: 2.2208525134671118

Epoch: 5| Step: 3
Training loss: 2.7247519493103027
Validation loss: 2.227697092999694

Epoch: 5| Step: 4
Training loss: 2.3591859340667725
Validation loss: 2.2252698841915337

Epoch: 5| Step: 5
Training loss: 2.7700276374816895
Validation loss: 2.235143612789851

Epoch: 5| Step: 6
Training loss: 2.1513524055480957
Validation loss: 2.228574739989414

Epoch: 5| Step: 7
Training loss: 2.8221428394317627
Validation loss: 2.2437832304226455

Epoch: 5| Step: 8
Training loss: 3.089649200439453
Validation loss: 2.24833768926641

Epoch: 5| Step: 9
Training loss: 2.4663777351379395
Validation loss: 2.2601187459884153

Epoch: 5| Step: 10
Training loss: 2.62567400932312
Validation loss: 2.262930650864878

Epoch: 99| Step: 0
Training loss: 2.5012917518615723
Validation loss: 2.257518360691686

Epoch: 5| Step: 1
Training loss: 2.41825532913208
Validation loss: 2.2542756295973256

Epoch: 5| Step: 2
Training loss: 2.9661238193511963
Validation loss: 2.248851519758983

Epoch: 5| Step: 3
Training loss: 2.35662579536438
Validation loss: 2.2472215608883928

Epoch: 5| Step: 4
Training loss: 2.119377851486206
Validation loss: 2.2441453882443008

Epoch: 5| Step: 5
Training loss: 2.2055370807647705
Validation loss: 2.2489742361089236

Epoch: 5| Step: 6
Training loss: 3.610508441925049
Validation loss: 2.2598212842018373

Epoch: 5| Step: 7
Training loss: 2.550358295440674
Validation loss: 2.2496195300932853

Epoch: 5| Step: 8
Training loss: 1.9475139379501343
Validation loss: 2.2570336762294976

Epoch: 5| Step: 9
Training loss: 2.498126268386841
Validation loss: 2.2783503250409196

Epoch: 5| Step: 10
Training loss: 2.543107748031616
Validation loss: 2.2868486194200415

Epoch: 100| Step: 0
Training loss: 1.4620964527130127
Validation loss: 2.2965429393194055

Epoch: 5| Step: 1
Training loss: 2.5337982177734375
Validation loss: 2.3030539174233713

Epoch: 5| Step: 2
Training loss: 2.558628797531128
Validation loss: 2.337327887935023

Epoch: 5| Step: 3
Training loss: 2.7509286403656006
Validation loss: 2.336250061629921

Epoch: 5| Step: 4
Training loss: 2.558539867401123
Validation loss: 2.291881030605685

Epoch: 5| Step: 5
Training loss: 3.068742513656616
Validation loss: 2.2538588149573213

Epoch: 5| Step: 6
Training loss: 2.4430840015411377
Validation loss: 2.2336988910551994

Epoch: 5| Step: 7
Training loss: 2.5945181846618652
Validation loss: 2.219891499447566

Epoch: 5| Step: 8
Training loss: 2.3736045360565186
Validation loss: 2.229920066812987

Epoch: 5| Step: 9
Training loss: 2.8528285026550293
Validation loss: 2.2327429761168776

Epoch: 5| Step: 10
Training loss: 2.3504581451416016
Validation loss: 2.2346017001777567

Epoch: 101| Step: 0
Training loss: 2.6602730751037598
Validation loss: 2.238332538194554

Epoch: 5| Step: 1
Training loss: 2.6387438774108887
Validation loss: 2.2404447370959866

Epoch: 5| Step: 2
Training loss: 2.424851179122925
Validation loss: 2.2332744008751324

Epoch: 5| Step: 3
Training loss: 1.816769003868103
Validation loss: 2.2357373493973927

Epoch: 5| Step: 4
Training loss: 2.87387752532959
Validation loss: 2.2365908084377164

Epoch: 5| Step: 5
Training loss: 2.88653302192688
Validation loss: 2.240107656807028

Epoch: 5| Step: 6
Training loss: 2.530566692352295
Validation loss: 2.240042713380629

Epoch: 5| Step: 7
Training loss: 2.4819908142089844
Validation loss: 2.2270219710565384

Epoch: 5| Step: 8
Training loss: 2.5429625511169434
Validation loss: 2.2224611133657475

Epoch: 5| Step: 9
Training loss: 2.5887579917907715
Validation loss: 2.211551789314516

Epoch: 5| Step: 10
Training loss: 2.2234787940979004
Validation loss: 2.2197265573727187

Epoch: 102| Step: 0
Training loss: 2.006826639175415
Validation loss: 2.220014913107759

Epoch: 5| Step: 1
Training loss: 2.0829689502716064
Validation loss: 2.214810538035567

Epoch: 5| Step: 2
Training loss: 3.5538058280944824
Validation loss: 2.219639471782151

Epoch: 5| Step: 3
Training loss: 2.459657669067383
Validation loss: 2.2186188185086815

Epoch: 5| Step: 4
Training loss: 2.72090482711792
Validation loss: 2.2291236833859513

Epoch: 5| Step: 5
Training loss: 2.118643283843994
Validation loss: 2.236741672280014

Epoch: 5| Step: 6
Training loss: 2.307347059249878
Validation loss: 2.239135467877952

Epoch: 5| Step: 7
Training loss: 2.7421631813049316
Validation loss: 2.264780459865447

Epoch: 5| Step: 8
Training loss: 1.8826442956924438
Validation loss: 2.2595066332047984

Epoch: 5| Step: 9
Training loss: 2.702237367630005
Validation loss: 2.277509735476586

Epoch: 5| Step: 10
Training loss: 3.004958152770996
Validation loss: 2.28727658589681

Epoch: 103| Step: 0
Training loss: 2.0786423683166504
Validation loss: 2.2799246285551336

Epoch: 5| Step: 1
Training loss: 3.413198471069336
Validation loss: 2.2967305029592207

Epoch: 5| Step: 2
Training loss: 1.993328332901001
Validation loss: 2.260172369659588

Epoch: 5| Step: 3
Training loss: 2.7946226596832275
Validation loss: 2.23472644564926

Epoch: 5| Step: 4
Training loss: 2.7228987216949463
Validation loss: 2.222750033101728

Epoch: 5| Step: 5
Training loss: 1.967224359512329
Validation loss: 2.204749035578902

Epoch: 5| Step: 6
Training loss: 2.878188133239746
Validation loss: 2.19750581249114

Epoch: 5| Step: 7
Training loss: 1.8849693536758423
Validation loss: 2.197165346914722

Epoch: 5| Step: 8
Training loss: 2.2714920043945312
Validation loss: 2.1954768037283294

Epoch: 5| Step: 9
Training loss: 2.6203651428222656
Validation loss: 2.1915171120756414

Epoch: 5| Step: 10
Training loss: 2.978353500366211
Validation loss: 2.2009114629478863

Epoch: 104| Step: 0
Training loss: 2.6023049354553223
Validation loss: 2.1977434542871292

Epoch: 5| Step: 1
Training loss: 2.669609785079956
Validation loss: 2.1967665969684558

Epoch: 5| Step: 2
Training loss: 2.3012773990631104
Validation loss: 2.2001656896324566

Epoch: 5| Step: 3
Training loss: 2.4444894790649414
Validation loss: 2.2044747029581377

Epoch: 5| Step: 4
Training loss: 2.2319321632385254
Validation loss: 2.2090102395703717

Epoch: 5| Step: 5
Training loss: 2.6806654930114746
Validation loss: 2.212004369305026

Epoch: 5| Step: 6
Training loss: 2.8560097217559814
Validation loss: 2.218128027454499

Epoch: 5| Step: 7
Training loss: 2.3569772243499756
Validation loss: 2.212301828527963

Epoch: 5| Step: 8
Training loss: 2.5115931034088135
Validation loss: 2.218003706265521

Epoch: 5| Step: 9
Training loss: 2.5144107341766357
Validation loss: 2.232453892307897

Epoch: 5| Step: 10
Training loss: 2.178709030151367
Validation loss: 2.2838496546591482

Epoch: 105| Step: 0
Training loss: 2.1594791412353516
Validation loss: 2.2868472606905046

Epoch: 5| Step: 1
Training loss: 2.253162384033203
Validation loss: 2.315921579637835

Epoch: 5| Step: 2
Training loss: 2.608814239501953
Validation loss: 2.318277417972524

Epoch: 5| Step: 3
Training loss: 2.8847599029541016
Validation loss: 2.3116655913732385

Epoch: 5| Step: 4
Training loss: 2.3796608448028564
Validation loss: 2.3046540393624255

Epoch: 5| Step: 5
Training loss: 2.0286812782287598
Validation loss: 2.2940909644608856

Epoch: 5| Step: 6
Training loss: 2.3984525203704834
Validation loss: 2.2771523203901065

Epoch: 5| Step: 7
Training loss: 2.6972904205322266
Validation loss: 2.277083909639748

Epoch: 5| Step: 8
Training loss: 2.632983446121216
Validation loss: 2.2514977942230883

Epoch: 5| Step: 9
Training loss: 2.5980112552642822
Validation loss: 2.2338623154547905

Epoch: 5| Step: 10
Training loss: 2.8907854557037354
Validation loss: 2.2206613479122037

Epoch: 106| Step: 0
Training loss: 2.218583583831787
Validation loss: 2.213732438702737

Epoch: 5| Step: 1
Training loss: 2.1699225902557373
Validation loss: 2.2190600133711293

Epoch: 5| Step: 2
Training loss: 1.8940279483795166
Validation loss: 2.2210750836198048

Epoch: 5| Step: 3
Training loss: 2.8416314125061035
Validation loss: 2.221345160597114

Epoch: 5| Step: 4
Training loss: 2.7446956634521484
Validation loss: 2.214558675724973

Epoch: 5| Step: 5
Training loss: 2.0991435050964355
Validation loss: 2.2164776632862706

Epoch: 5| Step: 6
Training loss: 2.999022960662842
Validation loss: 2.2124098090715307

Epoch: 5| Step: 7
Training loss: 2.607187509536743
Validation loss: 2.2342478485517603

Epoch: 5| Step: 8
Training loss: 2.442880630493164
Validation loss: 2.2412186335491877

Epoch: 5| Step: 9
Training loss: 2.652186632156372
Validation loss: 2.2481829402267293

Epoch: 5| Step: 10
Training loss: 2.574953317642212
Validation loss: 2.2670399604305143

Epoch: 107| Step: 0
Training loss: 2.466242790222168
Validation loss: 2.278101246844056

Epoch: 5| Step: 1
Training loss: 2.7761077880859375
Validation loss: 2.2991172190635436

Epoch: 5| Step: 2
Training loss: 3.004490852355957
Validation loss: 2.2901657524929253

Epoch: 5| Step: 3
Training loss: 2.3703341484069824
Validation loss: 2.2899510219532955

Epoch: 5| Step: 4
Training loss: 2.2899796962738037
Validation loss: 2.3017906117182907

Epoch: 5| Step: 5
Training loss: 2.344578266143799
Validation loss: 2.287738230920607

Epoch: 5| Step: 6
Training loss: 1.9214235544204712
Validation loss: 2.291031245262392

Epoch: 5| Step: 7
Training loss: 2.565692901611328
Validation loss: 2.2728123229037047

Epoch: 5| Step: 8
Training loss: 2.24531888961792
Validation loss: 2.2826978583489694

Epoch: 5| Step: 9
Training loss: 2.86136794090271
Validation loss: 2.279966726098009

Epoch: 5| Step: 10
Training loss: 2.4751956462860107
Validation loss: 2.24616704833123

Epoch: 108| Step: 0
Training loss: 2.5150108337402344
Validation loss: 2.2466341295549945

Epoch: 5| Step: 1
Training loss: 2.1934478282928467
Validation loss: 2.2327277480915027

Epoch: 5| Step: 2
Training loss: 2.6834285259246826
Validation loss: 2.2204260364655526

Epoch: 5| Step: 3
Training loss: 2.530365467071533
Validation loss: 2.2165083936465684

Epoch: 5| Step: 4
Training loss: 2.5697169303894043
Validation loss: 2.206286555977278

Epoch: 5| Step: 5
Training loss: 2.9375953674316406
Validation loss: 2.2076989399489535

Epoch: 5| Step: 6
Training loss: 2.481496572494507
Validation loss: 2.197939042122133

Epoch: 5| Step: 7
Training loss: 2.7521469593048096
Validation loss: 2.2047049281417683

Epoch: 5| Step: 8
Training loss: 2.432612180709839
Validation loss: 2.2211047052055277

Epoch: 5| Step: 9
Training loss: 2.161431074142456
Validation loss: 2.2447377481768207

Epoch: 5| Step: 10
Training loss: 1.8023954629898071
Validation loss: 2.2610855589630785

Epoch: 109| Step: 0
Training loss: 2.9300920963287354
Validation loss: 2.2393121565541914

Epoch: 5| Step: 1
Training loss: 2.559617519378662
Validation loss: 2.2232681602560063

Epoch: 5| Step: 2
Training loss: 2.4744296073913574
Validation loss: 2.247422718232678

Epoch: 5| Step: 3
Training loss: 2.6420931816101074
Validation loss: 2.24461527024546

Epoch: 5| Step: 4
Training loss: 2.206583023071289
Validation loss: 2.2485412525874313

Epoch: 5| Step: 5
Training loss: 2.5685579776763916
Validation loss: 2.2610916783732753

Epoch: 5| Step: 6
Training loss: 2.1575586795806885
Validation loss: 2.262060611478744

Epoch: 5| Step: 7
Training loss: 2.4668631553649902
Validation loss: 2.243943178525535

Epoch: 5| Step: 8
Training loss: 2.475637674331665
Validation loss: 2.2345544907354538

Epoch: 5| Step: 9
Training loss: 1.9652559757232666
Validation loss: 2.2272916557968303

Epoch: 5| Step: 10
Training loss: 2.7001826763153076
Validation loss: 2.219297409057617

Epoch: 110| Step: 0
Training loss: 1.7859792709350586
Validation loss: 2.2250823179880777

Epoch: 5| Step: 1
Training loss: 2.5991835594177246
Validation loss: 2.2428252209899244

Epoch: 5| Step: 2
Training loss: 2.372105121612549
Validation loss: 2.2538156804218086

Epoch: 5| Step: 3
Training loss: 2.313568115234375
Validation loss: 2.2769660565160934

Epoch: 5| Step: 4
Training loss: 2.6055634021759033
Validation loss: 2.2492705980936685

Epoch: 5| Step: 5
Training loss: 2.6383697986602783
Validation loss: 2.21920717916181

Epoch: 5| Step: 6
Training loss: 2.79498553276062
Validation loss: 2.2003365101352816

Epoch: 5| Step: 7
Training loss: 2.3623948097229004
Validation loss: 2.184459863170501

Epoch: 5| Step: 8
Training loss: 2.5692052841186523
Validation loss: 2.1966792793684107

Epoch: 5| Step: 9
Training loss: 2.490609645843506
Validation loss: 2.2099474142956477

Epoch: 5| Step: 10
Training loss: 2.7333855628967285
Validation loss: 2.2250434121777936

Epoch: 111| Step: 0
Training loss: 2.14070463180542
Validation loss: 2.2429641856942126

Epoch: 5| Step: 1
Training loss: 3.057669162750244
Validation loss: 2.2419782838513775

Epoch: 5| Step: 2
Training loss: 2.3979852199554443
Validation loss: 2.255740705356803

Epoch: 5| Step: 3
Training loss: 2.1144490242004395
Validation loss: 2.2508758601321968

Epoch: 5| Step: 4
Training loss: 2.3182015419006348
Validation loss: 2.2310386447496313

Epoch: 5| Step: 5
Training loss: 2.570033311843872
Validation loss: 2.208390910138366

Epoch: 5| Step: 6
Training loss: 2.0598528385162354
Validation loss: 2.218898700129601

Epoch: 5| Step: 7
Training loss: 2.703423500061035
Validation loss: 2.208752388595253

Epoch: 5| Step: 8
Training loss: 2.210569381713867
Validation loss: 2.204525424588111

Epoch: 5| Step: 9
Training loss: 3.166936159133911
Validation loss: 2.216154238229157

Epoch: 5| Step: 10
Training loss: 2.623333215713501
Validation loss: 2.2130600303731938

Epoch: 112| Step: 0
Training loss: 1.8702430725097656
Validation loss: 2.2202152667507047

Epoch: 5| Step: 1
Training loss: 2.724963426589966
Validation loss: 2.229057309448078

Epoch: 5| Step: 2
Training loss: 2.6828465461730957
Validation loss: 2.2208399747007634

Epoch: 5| Step: 3
Training loss: 2.6261978149414062
Validation loss: 2.2407743623179774

Epoch: 5| Step: 4
Training loss: 2.6958606243133545
Validation loss: 2.233784314124815

Epoch: 5| Step: 5
Training loss: 2.7842953205108643
Validation loss: 2.221394192787909

Epoch: 5| Step: 6
Training loss: 1.7633100748062134
Validation loss: 2.216757430825182

Epoch: 5| Step: 7
Training loss: 2.7869880199432373
Validation loss: 2.2258457932420956

Epoch: 5| Step: 8
Training loss: 2.1851322650909424
Validation loss: 2.212287861813781

Epoch: 5| Step: 9
Training loss: 3.002340316772461
Validation loss: 2.2028870967126664

Epoch: 5| Step: 10
Training loss: 1.9905352592468262
Validation loss: 2.2184811561338362

Epoch: 113| Step: 0
Training loss: 1.9085884094238281
Validation loss: 2.212356910910658

Epoch: 5| Step: 1
Training loss: 2.397082805633545
Validation loss: 2.2284081059117473

Epoch: 5| Step: 2
Training loss: 2.5599422454833984
Validation loss: 2.2209447660753803

Epoch: 5| Step: 3
Training loss: 2.1168723106384277
Validation loss: 2.2273154694546937

Epoch: 5| Step: 4
Training loss: 2.4616432189941406
Validation loss: 2.2365671447528306

Epoch: 5| Step: 5
Training loss: 2.650195837020874
Validation loss: 2.225886162891183

Epoch: 5| Step: 6
Training loss: 1.9734216928482056
Validation loss: 2.2229973141865065

Epoch: 5| Step: 7
Training loss: 2.342346668243408
Validation loss: 2.227656879732686

Epoch: 5| Step: 8
Training loss: 3.0528836250305176
Validation loss: 2.2090483762884654

Epoch: 5| Step: 9
Training loss: 2.572610378265381
Validation loss: 2.210960402283617

Epoch: 5| Step: 10
Training loss: 3.0281660556793213
Validation loss: 2.2079750453272173

Epoch: 114| Step: 0
Training loss: 1.7457542419433594
Validation loss: 2.221781740906418

Epoch: 5| Step: 1
Training loss: 2.705409049987793
Validation loss: 2.2334372074373308

Epoch: 5| Step: 2
Training loss: 3.133068084716797
Validation loss: 2.2189630885278024

Epoch: 5| Step: 3
Training loss: 2.3285136222839355
Validation loss: 2.2248710381087435

Epoch: 5| Step: 4
Training loss: 2.6456246376037598
Validation loss: 2.219092766443888

Epoch: 5| Step: 5
Training loss: 1.975555419921875
Validation loss: 2.227027177810669

Epoch: 5| Step: 6
Training loss: 2.7734975814819336
Validation loss: 2.216378774694217

Epoch: 5| Step: 7
Training loss: 2.526221752166748
Validation loss: 2.2142374746261106

Epoch: 5| Step: 8
Training loss: 2.3428139686584473
Validation loss: 2.214065892722017

Epoch: 5| Step: 9
Training loss: 2.69364595413208
Validation loss: 2.2161226426401446

Epoch: 5| Step: 10
Training loss: 2.060014009475708
Validation loss: 2.22790495041878

Epoch: 115| Step: 0
Training loss: 2.1997501850128174
Validation loss: 2.237370373100363

Epoch: 5| Step: 1
Training loss: 2.168790340423584
Validation loss: 2.251000224903066

Epoch: 5| Step: 2
Training loss: 2.0094032287597656
Validation loss: 2.237984372723487

Epoch: 5| Step: 3
Training loss: 2.424745798110962
Validation loss: 2.2446264143913024

Epoch: 5| Step: 4
Training loss: 3.0346672534942627
Validation loss: 2.2414207740496566

Epoch: 5| Step: 5
Training loss: 2.3376624584198
Validation loss: 2.2377176156608005

Epoch: 5| Step: 6
Training loss: 2.511096954345703
Validation loss: 2.2282344577133015

Epoch: 5| Step: 7
Training loss: 2.888866901397705
Validation loss: 2.236563818429106

Epoch: 5| Step: 8
Training loss: 2.295074939727783
Validation loss: 2.2262706346409296

Epoch: 5| Step: 9
Training loss: 2.355867624282837
Validation loss: 2.2009753052906325

Epoch: 5| Step: 10
Training loss: 2.7615513801574707
Validation loss: 2.213594341790804

Epoch: 116| Step: 0
Training loss: 2.5886826515197754
Validation loss: 2.21459041231422

Epoch: 5| Step: 1
Training loss: 2.2882580757141113
Validation loss: 2.2094167150476927

Epoch: 5| Step: 2
Training loss: 2.6781277656555176
Validation loss: 2.2242474786696897

Epoch: 5| Step: 3
Training loss: 2.1210784912109375
Validation loss: 2.2285088416068786

Epoch: 5| Step: 4
Training loss: 2.0751161575317383
Validation loss: 2.244857350985209

Epoch: 5| Step: 5
Training loss: 3.1326537132263184
Validation loss: 2.2500468454053326

Epoch: 5| Step: 6
Training loss: 2.428118944168091
Validation loss: 2.260621270825786

Epoch: 5| Step: 7
Training loss: 2.5714592933654785
Validation loss: 2.2677675447156354

Epoch: 5| Step: 8
Training loss: 2.3749489784240723
Validation loss: 2.2352981349473358

Epoch: 5| Step: 9
Training loss: 2.135622978210449
Validation loss: 2.2137141125176543

Epoch: 5| Step: 10
Training loss: 2.663602828979492
Validation loss: 2.2051320768171743

Epoch: 117| Step: 0
Training loss: 1.9517850875854492
Validation loss: 2.191679339255056

Epoch: 5| Step: 1
Training loss: 2.997231960296631
Validation loss: 2.1838286640823528

Epoch: 5| Step: 2
Training loss: 2.463310718536377
Validation loss: 2.178564630528932

Epoch: 5| Step: 3
Training loss: 2.5795254707336426
Validation loss: 2.1708241854944537

Epoch: 5| Step: 4
Training loss: 2.230119228363037
Validation loss: 2.172232122831447

Epoch: 5| Step: 5
Training loss: 3.0561130046844482
Validation loss: 2.202896579619377

Epoch: 5| Step: 6
Training loss: 2.608018636703491
Validation loss: 2.2288034564705304

Epoch: 5| Step: 7
Training loss: 2.506441593170166
Validation loss: 2.2394445301384054

Epoch: 5| Step: 8
Training loss: 2.24992036819458
Validation loss: 2.244911786048643

Epoch: 5| Step: 9
Training loss: 2.1942248344421387
Validation loss: 2.2468630934274323

Epoch: 5| Step: 10
Training loss: 2.086876630783081
Validation loss: 2.236753782918376

Epoch: 118| Step: 0
Training loss: 3.0186405181884766
Validation loss: 2.2275203222869546

Epoch: 5| Step: 1
Training loss: 2.1202526092529297
Validation loss: 2.218539595603943

Epoch: 5| Step: 2
Training loss: 2.1275181770324707
Validation loss: 2.190172126216273

Epoch: 5| Step: 3
Training loss: 2.4752938747406006
Validation loss: 2.1867838572430354

Epoch: 5| Step: 4
Training loss: 2.8508248329162598
Validation loss: 2.1857818480460875

Epoch: 5| Step: 5
Training loss: 2.3134303092956543
Validation loss: 2.1804657072149296

Epoch: 5| Step: 6
Training loss: 2.5801658630371094
Validation loss: 2.1858753735019314

Epoch: 5| Step: 7
Training loss: 2.2290890216827393
Validation loss: 2.1964122454325357

Epoch: 5| Step: 8
Training loss: 2.3997857570648193
Validation loss: 2.206528216279963

Epoch: 5| Step: 9
Training loss: 2.8752334117889404
Validation loss: 2.196010947227478

Epoch: 5| Step: 10
Training loss: 1.9062199592590332
Validation loss: 2.205515871765793

Epoch: 119| Step: 0
Training loss: 2.6796205043792725
Validation loss: 2.21457407295063

Epoch: 5| Step: 1
Training loss: 2.3093554973602295
Validation loss: 2.2293077489381194

Epoch: 5| Step: 2
Training loss: 2.8101861476898193
Validation loss: 2.2460641130324333

Epoch: 5| Step: 3
Training loss: 2.049386978149414
Validation loss: 2.2382780044309554

Epoch: 5| Step: 4
Training loss: 3.14367413520813
Validation loss: 2.2458808242633777

Epoch: 5| Step: 5
Training loss: 2.306049108505249
Validation loss: 2.239030830321773

Epoch: 5| Step: 6
Training loss: 2.5519070625305176
Validation loss: 2.2279286769128617

Epoch: 5| Step: 7
Training loss: 2.302975654602051
Validation loss: 2.2235088322752263

Epoch: 5| Step: 8
Training loss: 2.5649056434631348
Validation loss: 2.2084202958691503

Epoch: 5| Step: 9
Training loss: 2.4079153537750244
Validation loss: 2.207263961915047

Epoch: 5| Step: 10
Training loss: 1.8183093070983887
Validation loss: 2.2172808262609665

Epoch: 120| Step: 0
Training loss: 2.581705093383789
Validation loss: 2.2189759695401756

Epoch: 5| Step: 1
Training loss: 2.6290183067321777
Validation loss: 2.2413252489541167

Epoch: 5| Step: 2
Training loss: 2.761409044265747
Validation loss: 2.233460892913162

Epoch: 5| Step: 3
Training loss: 2.9305806159973145
Validation loss: 2.2421542175354494

Epoch: 5| Step: 4
Training loss: 2.31388521194458
Validation loss: 2.2284169914901897

Epoch: 5| Step: 5
Training loss: 1.8654584884643555
Validation loss: 2.2082530093449417

Epoch: 5| Step: 6
Training loss: 2.1945900917053223
Validation loss: 2.2233585029520015

Epoch: 5| Step: 7
Training loss: 2.2228610515594482
Validation loss: 2.2129744022123274

Epoch: 5| Step: 8
Training loss: 2.298621416091919
Validation loss: 2.2114726343462543

Epoch: 5| Step: 9
Training loss: 2.6144356727600098
Validation loss: 2.2188412450974986

Epoch: 5| Step: 10
Training loss: 2.50472092628479
Validation loss: 2.2242261491796023

Epoch: 121| Step: 0
Training loss: 2.174952983856201
Validation loss: 2.2075364846055225

Epoch: 5| Step: 1
Training loss: 2.0654311180114746
Validation loss: 2.2107087745461413

Epoch: 5| Step: 2
Training loss: 2.326845169067383
Validation loss: 2.205820816819386

Epoch: 5| Step: 3
Training loss: 2.5834603309631348
Validation loss: 2.2056912414489256

Epoch: 5| Step: 4
Training loss: 2.1393096446990967
Validation loss: 2.205196398560719

Epoch: 5| Step: 5
Training loss: 2.3193962574005127
Validation loss: 2.217660250202302

Epoch: 5| Step: 6
Training loss: 2.4531357288360596
Validation loss: 2.2125134596260647

Epoch: 5| Step: 7
Training loss: 2.3265223503112793
Validation loss: 2.22354196733044

Epoch: 5| Step: 8
Training loss: 2.6899256706237793
Validation loss: 2.219821204421341

Epoch: 5| Step: 9
Training loss: 2.8811259269714355
Validation loss: 2.238027803359493

Epoch: 5| Step: 10
Training loss: 3.098409652709961
Validation loss: 2.247366868039613

Epoch: 122| Step: 0
Training loss: 2.287282705307007
Validation loss: 2.234456426353865

Epoch: 5| Step: 1
Training loss: 2.9878551959991455
Validation loss: 2.2258336928582962

Epoch: 5| Step: 2
Training loss: 2.5837514400482178
Validation loss: 2.2209502702118247

Epoch: 5| Step: 3
Training loss: 1.827314019203186
Validation loss: 2.2137087314359603

Epoch: 5| Step: 4
Training loss: 2.633432388305664
Validation loss: 2.208546228306268

Epoch: 5| Step: 5
Training loss: 2.3646390438079834
Validation loss: 2.205441008331955

Epoch: 5| Step: 6
Training loss: 3.0700411796569824
Validation loss: 2.1947660728167464

Epoch: 5| Step: 7
Training loss: 3.152688503265381
Validation loss: 2.1842954671511086

Epoch: 5| Step: 8
Training loss: 1.9395118951797485
Validation loss: 2.1692768963434363

Epoch: 5| Step: 9
Training loss: 1.6098473072052002
Validation loss: 2.177391991820387

Epoch: 5| Step: 10
Training loss: 2.4595515727996826
Validation loss: 2.1796835237933743

Epoch: 123| Step: 0
Training loss: 2.8834550380706787
Validation loss: 2.197537406798332

Epoch: 5| Step: 1
Training loss: 2.0800819396972656
Validation loss: 2.225463831296531

Epoch: 5| Step: 2
Training loss: 2.7111778259277344
Validation loss: 2.242118994394938

Epoch: 5| Step: 3
Training loss: 2.710880756378174
Validation loss: 2.251753476358229

Epoch: 5| Step: 4
Training loss: 2.3516650199890137
Validation loss: 2.276746834478071

Epoch: 5| Step: 5
Training loss: 2.3312888145446777
Validation loss: 2.2876038551330566

Epoch: 5| Step: 6
Training loss: 2.7227141857147217
Validation loss: 2.293993893490043

Epoch: 5| Step: 7
Training loss: 2.5358691215515137
Validation loss: 2.2933865811235163

Epoch: 5| Step: 8
Training loss: 2.4711716175079346
Validation loss: 2.3012398058368313

Epoch: 5| Step: 9
Training loss: 1.8943630456924438
Validation loss: 2.2682511114305064

Epoch: 5| Step: 10
Training loss: 2.349895715713501
Validation loss: 2.2764957822779173

Epoch: 124| Step: 0
Training loss: 2.115149974822998
Validation loss: 2.2489345560791674

Epoch: 5| Step: 1
Training loss: 2.9258816242218018
Validation loss: 2.211898019236903

Epoch: 5| Step: 2
Training loss: 2.2119574546813965
Validation loss: 2.1955298082802885

Epoch: 5| Step: 3
Training loss: 1.970973253250122
Validation loss: 2.189758241817515

Epoch: 5| Step: 4
Training loss: 2.177967071533203
Validation loss: 2.1771948555464387

Epoch: 5| Step: 5
Training loss: 2.0592751502990723
Validation loss: 2.175128311239263

Epoch: 5| Step: 6
Training loss: 2.770866870880127
Validation loss: 2.169983486975393

Epoch: 5| Step: 7
Training loss: 2.173752784729004
Validation loss: 2.172730677871294

Epoch: 5| Step: 8
Training loss: 2.536898374557495
Validation loss: 2.1767943674518215

Epoch: 5| Step: 9
Training loss: 2.882760524749756
Validation loss: 2.1830281262756674

Epoch: 5| Step: 10
Training loss: 3.193861246109009
Validation loss: 2.203430837200534

Epoch: 125| Step: 0
Training loss: 2.532090425491333
Validation loss: 2.205906255270845

Epoch: 5| Step: 1
Training loss: 2.109001398086548
Validation loss: 2.222454555573002

Epoch: 5| Step: 2
Training loss: 2.8207509517669678
Validation loss: 2.220621657627885

Epoch: 5| Step: 3
Training loss: 1.9664008617401123
Validation loss: 2.2375331655625375

Epoch: 5| Step: 4
Training loss: 1.7895044088363647
Validation loss: 2.2588541302629697

Epoch: 5| Step: 5
Training loss: 2.0109119415283203
Validation loss: 2.259357426756172

Epoch: 5| Step: 6
Training loss: 3.1987268924713135
Validation loss: 2.2688720046832995

Epoch: 5| Step: 7
Training loss: 2.4533281326293945
Validation loss: 2.252285562535768

Epoch: 5| Step: 8
Training loss: 2.999457836151123
Validation loss: 2.2363106922436784

Epoch: 5| Step: 9
Training loss: 2.9018988609313965
Validation loss: 2.2374703473942255

Epoch: 5| Step: 10
Training loss: 2.075723886489868
Validation loss: 2.2083558497890348

Epoch: 126| Step: 0
Training loss: 2.9027211666107178
Validation loss: 2.1916596710041003

Epoch: 5| Step: 1
Training loss: 2.928417682647705
Validation loss: 2.177500342810026

Epoch: 5| Step: 2
Training loss: 2.259143114089966
Validation loss: 2.168535719635666

Epoch: 5| Step: 3
Training loss: 2.1925153732299805
Validation loss: 2.1667833840975197

Epoch: 5| Step: 4
Training loss: 2.344264268875122
Validation loss: 2.154581128910024

Epoch: 5| Step: 5
Training loss: 2.2276058197021484
Validation loss: 2.1650617007286317

Epoch: 5| Step: 6
Training loss: 2.9865546226501465
Validation loss: 2.1699240925491496

Epoch: 5| Step: 7
Training loss: 2.2086825370788574
Validation loss: 2.1837357141638316

Epoch: 5| Step: 8
Training loss: 1.8927576541900635
Validation loss: 2.2019789052265946

Epoch: 5| Step: 9
Training loss: 2.450632333755493
Validation loss: 2.213612392384519

Epoch: 5| Step: 10
Training loss: 2.558826446533203
Validation loss: 2.2240751481825307

Epoch: 127| Step: 0
Training loss: 2.8082337379455566
Validation loss: 2.229690659430719

Epoch: 5| Step: 1
Training loss: 2.176638603210449
Validation loss: 2.2183950331903275

Epoch: 5| Step: 2
Training loss: 2.899101734161377
Validation loss: 2.231943525293822

Epoch: 5| Step: 3
Training loss: 2.390195608139038
Validation loss: 2.2227696808435584

Epoch: 5| Step: 4
Training loss: 2.600526809692383
Validation loss: 2.21224368515835

Epoch: 5| Step: 5
Training loss: 2.279728412628174
Validation loss: 2.208089141435521

Epoch: 5| Step: 6
Training loss: 2.1863017082214355
Validation loss: 2.20633509466725

Epoch: 5| Step: 7
Training loss: 1.969347357749939
Validation loss: 2.200045117767908

Epoch: 5| Step: 8
Training loss: 2.4173130989074707
Validation loss: 2.195953292231406

Epoch: 5| Step: 9
Training loss: 2.543911933898926
Validation loss: 2.1991007033214776

Epoch: 5| Step: 10
Training loss: 2.572835922241211
Validation loss: 2.191465377807617

Epoch: 128| Step: 0
Training loss: 1.7774391174316406
Validation loss: 2.191960250177691

Epoch: 5| Step: 1
Training loss: 1.9626293182373047
Validation loss: 2.1908097574787755

Epoch: 5| Step: 2
Training loss: 2.502537965774536
Validation loss: 2.2060242724675003

Epoch: 5| Step: 3
Training loss: 2.477456569671631
Validation loss: 2.212925293112314

Epoch: 5| Step: 4
Training loss: 2.945256471633911
Validation loss: 2.2264386453936176

Epoch: 5| Step: 5
Training loss: 2.5501186847686768
Validation loss: 2.2359246489822224

Epoch: 5| Step: 6
Training loss: 2.4521265029907227
Validation loss: 2.236698942799722

Epoch: 5| Step: 7
Training loss: 1.9795188903808594
Validation loss: 2.246146863506686

Epoch: 5| Step: 8
Training loss: 3.0695486068725586
Validation loss: 2.2391539209632465

Epoch: 5| Step: 9
Training loss: 2.457468032836914
Validation loss: 2.239402953014579

Epoch: 5| Step: 10
Training loss: 2.6636180877685547
Validation loss: 2.216286047812431

Epoch: 129| Step: 0
Training loss: 2.628809928894043
Validation loss: 2.2024393107301448

Epoch: 5| Step: 1
Training loss: 2.6801490783691406
Validation loss: 2.1771733427560456

Epoch: 5| Step: 2
Training loss: 2.113312244415283
Validation loss: 2.1679794147450435

Epoch: 5| Step: 3
Training loss: 1.9903333187103271
Validation loss: 2.157345582080144

Epoch: 5| Step: 4
Training loss: 2.9684250354766846
Validation loss: 2.1536695098364227

Epoch: 5| Step: 5
Training loss: 1.9264929294586182
Validation loss: 2.1591079491440968

Epoch: 5| Step: 6
Training loss: 3.446873426437378
Validation loss: 2.171422504609631

Epoch: 5| Step: 7
Training loss: 2.0455210208892822
Validation loss: 2.186609575825353

Epoch: 5| Step: 8
Training loss: 2.172579765319824
Validation loss: 2.207918490132978

Epoch: 5| Step: 9
Training loss: 2.0385775566101074
Validation loss: 2.22431242850519

Epoch: 5| Step: 10
Training loss: 2.8443374633789062
Validation loss: 2.2373697501356884

Epoch: 130| Step: 0
Training loss: 2.4417471885681152
Validation loss: 2.235224280306088

Epoch: 5| Step: 1
Training loss: 1.605791449546814
Validation loss: 2.219870954431513

Epoch: 5| Step: 2
Training loss: 2.171438217163086
Validation loss: 2.2120219071706138

Epoch: 5| Step: 3
Training loss: 2.3281846046447754
Validation loss: 2.2201776632698635

Epoch: 5| Step: 4
Training loss: 2.634225845336914
Validation loss: 2.227034148349557

Epoch: 5| Step: 5
Training loss: 2.2914745807647705
Validation loss: 2.200109494629727

Epoch: 5| Step: 6
Training loss: 2.89188289642334
Validation loss: 2.1973102144015733

Epoch: 5| Step: 7
Training loss: 2.767594814300537
Validation loss: 2.198550275577012

Epoch: 5| Step: 8
Training loss: 1.917972207069397
Validation loss: 2.1941334150170766

Epoch: 5| Step: 9
Training loss: 2.6175732612609863
Validation loss: 2.2041283781810472

Epoch: 5| Step: 10
Training loss: 2.924490213394165
Validation loss: 2.1979206274914485

Epoch: 131| Step: 0
Training loss: 2.397428274154663
Validation loss: 2.204939921696981

Epoch: 5| Step: 1
Training loss: 2.254028081893921
Validation loss: 2.203891828495969

Epoch: 5| Step: 2
Training loss: 2.7304277420043945
Validation loss: 2.2038651204878286

Epoch: 5| Step: 3
Training loss: 2.7740345001220703
Validation loss: 2.222282691668439

Epoch: 5| Step: 4
Training loss: 2.1557059288024902
Validation loss: 2.2398110320491176

Epoch: 5| Step: 5
Training loss: 2.8416519165039062
Validation loss: 2.2508579761751237

Epoch: 5| Step: 6
Training loss: 2.243955135345459
Validation loss: 2.2594383083364016

Epoch: 5| Step: 7
Training loss: 2.070897340774536
Validation loss: 2.2398114101861113

Epoch: 5| Step: 8
Training loss: 2.409327745437622
Validation loss: 2.2292828303511425

Epoch: 5| Step: 9
Training loss: 2.7416467666625977
Validation loss: 2.2129258391677693

Epoch: 5| Step: 10
Training loss: 1.8893706798553467
Validation loss: 2.2098401438805366

Epoch: 132| Step: 0
Training loss: 2.5363781452178955
Validation loss: 2.200807676520399

Epoch: 5| Step: 1
Training loss: 2.0213005542755127
Validation loss: 2.210396476971206

Epoch: 5| Step: 2
Training loss: 2.590339183807373
Validation loss: 2.203122833723663

Epoch: 5| Step: 3
Training loss: 2.4723496437072754
Validation loss: 2.211686157411145

Epoch: 5| Step: 4
Training loss: 2.601105213165283
Validation loss: 2.204415282895488

Epoch: 5| Step: 5
Training loss: 2.4039440155029297
Validation loss: 2.1887707505174863

Epoch: 5| Step: 6
Training loss: 2.402757167816162
Validation loss: 2.1861867622662614

Epoch: 5| Step: 7
Training loss: 2.923022508621216
Validation loss: 2.16903870080107

Epoch: 5| Step: 8
Training loss: 2.9280848503112793
Validation loss: 2.169147883692095

Epoch: 5| Step: 9
Training loss: 2.1013290882110596
Validation loss: 2.1611807025888914

Epoch: 5| Step: 10
Training loss: 1.8433741331100464
Validation loss: 2.165401654858743

Epoch: 133| Step: 0
Training loss: 1.8306169509887695
Validation loss: 2.1769933136560584

Epoch: 5| Step: 1
Training loss: 2.9748947620391846
Validation loss: 2.1812968023361696

Epoch: 5| Step: 2
Training loss: 2.9510319232940674
Validation loss: 2.1961736807259182

Epoch: 5| Step: 3
Training loss: 2.224388599395752
Validation loss: 2.2013356993275304

Epoch: 5| Step: 4
Training loss: 2.4543442726135254
Validation loss: 2.2201205094655356

Epoch: 5| Step: 5
Training loss: 1.802093744277954
Validation loss: 2.224810590026199

Epoch: 5| Step: 6
Training loss: 2.740675449371338
Validation loss: 2.233021401589917

Epoch: 5| Step: 7
Training loss: 2.643090009689331
Validation loss: 2.2344904817560667

Epoch: 5| Step: 8
Training loss: 2.723499059677124
Validation loss: 2.2309752484803558

Epoch: 5| Step: 9
Training loss: 1.9202827215194702
Validation loss: 2.2377746810195265

Epoch: 5| Step: 10
Training loss: 2.3575453758239746
Validation loss: 2.22403319292171

Epoch: 134| Step: 0
Training loss: 2.3830373287200928
Validation loss: 2.2438623879545476

Epoch: 5| Step: 1
Training loss: 2.59995698928833
Validation loss: 2.258702616537771

Epoch: 5| Step: 2
Training loss: 1.8447166681289673
Validation loss: 2.2447609465609313

Epoch: 5| Step: 3
Training loss: 2.850729465484619
Validation loss: 2.240859898187781

Epoch: 5| Step: 4
Training loss: 2.455624580383301
Validation loss: 2.2477298449444514

Epoch: 5| Step: 5
Training loss: 2.0775818824768066
Validation loss: 2.259865773621426

Epoch: 5| Step: 6
Training loss: 3.2258472442626953
Validation loss: 2.2388911195980605

Epoch: 5| Step: 7
Training loss: 1.5818493366241455
Validation loss: 2.2220264480959986

Epoch: 5| Step: 8
Training loss: 2.300693988800049
Validation loss: 2.200821227924798

Epoch: 5| Step: 9
Training loss: 2.7059826850891113
Validation loss: 2.1982873678207397

Epoch: 5| Step: 10
Training loss: 2.5781967639923096
Validation loss: 2.1967766720761537

Epoch: 135| Step: 0
Training loss: 2.292466402053833
Validation loss: 2.18354868888855

Epoch: 5| Step: 1
Training loss: 2.6373581886291504
Validation loss: 2.1875158894446587

Epoch: 5| Step: 2
Training loss: 2.3753528594970703
Validation loss: 2.183918600441307

Epoch: 5| Step: 3
Training loss: 2.8645129203796387
Validation loss: 2.2018519575877855

Epoch: 5| Step: 4
Training loss: 2.016526460647583
Validation loss: 2.2000689121984665

Epoch: 5| Step: 5
Training loss: 2.049982786178589
Validation loss: 2.1955420150551745

Epoch: 5| Step: 6
Training loss: 2.518482208251953
Validation loss: 2.202097200578259

Epoch: 5| Step: 7
Training loss: 1.7973381280899048
Validation loss: 2.196347777561475

Epoch: 5| Step: 8
Training loss: 2.536734104156494
Validation loss: 2.2132496577437206

Epoch: 5| Step: 9
Training loss: 3.1755168437957764
Validation loss: 2.196197602056688

Epoch: 5| Step: 10
Training loss: 2.2968642711639404
Validation loss: 2.2129115186711794

Epoch: 136| Step: 0
Training loss: 2.609128952026367
Validation loss: 2.204700695571079

Epoch: 5| Step: 1
Training loss: 2.147449016571045
Validation loss: 2.2196857313955984

Epoch: 5| Step: 2
Training loss: 2.677940607070923
Validation loss: 2.2240703759654874

Epoch: 5| Step: 3
Training loss: 2.040083408355713
Validation loss: 2.196452122862621

Epoch: 5| Step: 4
Training loss: 2.9381449222564697
Validation loss: 2.199985611823297

Epoch: 5| Step: 5
Training loss: 2.7615270614624023
Validation loss: 2.195759427162909

Epoch: 5| Step: 6
Training loss: 2.347266435623169
Validation loss: 2.2024597429460093

Epoch: 5| Step: 7
Training loss: 2.154649019241333
Validation loss: 2.2030551843745734

Epoch: 5| Step: 8
Training loss: 2.0189502239227295
Validation loss: 2.211865709673974

Epoch: 5| Step: 9
Training loss: 2.580980062484741
Validation loss: 2.2090693955780356

Epoch: 5| Step: 10
Training loss: 2.1410117149353027
Validation loss: 2.200243038515891

Epoch: 137| Step: 0
Training loss: 3.001739263534546
Validation loss: 2.2051164181001726

Epoch: 5| Step: 1
Training loss: 2.5070555210113525
Validation loss: 2.2089866361310406

Epoch: 5| Step: 2
Training loss: 2.6451170444488525
Validation loss: 2.21738685587401

Epoch: 5| Step: 3
Training loss: 2.2249064445495605
Validation loss: 2.236332566507401

Epoch: 5| Step: 4
Training loss: 3.014315128326416
Validation loss: 2.248063043881488

Epoch: 5| Step: 5
Training loss: 1.8640419244766235
Validation loss: 2.235051566554654

Epoch: 5| Step: 6
Training loss: 1.8637231588363647
Validation loss: 2.241172517499616

Epoch: 5| Step: 7
Training loss: 2.5162580013275146
Validation loss: 2.24148057353112

Epoch: 5| Step: 8
Training loss: 2.2595584392547607
Validation loss: 2.2348157128980084

Epoch: 5| Step: 9
Training loss: 2.16660737991333
Validation loss: 2.23392931876644

Epoch: 5| Step: 10
Training loss: 2.440636157989502
Validation loss: 2.211168425057524

Epoch: 138| Step: 0
Training loss: 2.162933111190796
Validation loss: 2.2071222079697477

Epoch: 5| Step: 1
Training loss: 2.596172332763672
Validation loss: 2.1882400884423205

Epoch: 5| Step: 2
Training loss: 2.2339677810668945
Validation loss: 2.1880357342381633

Epoch: 5| Step: 3
Training loss: 2.0863571166992188
Validation loss: 2.175805519985896

Epoch: 5| Step: 4
Training loss: 2.381425380706787
Validation loss: 2.191246827443441

Epoch: 5| Step: 5
Training loss: 2.863708019256592
Validation loss: 2.181011320442282

Epoch: 5| Step: 6
Training loss: 2.1551625728607178
Validation loss: 2.2159169976429274

Epoch: 5| Step: 7
Training loss: 2.0461344718933105
Validation loss: 2.211638701859341

Epoch: 5| Step: 8
Training loss: 2.7791130542755127
Validation loss: 2.215355521889143

Epoch: 5| Step: 9
Training loss: 1.8392692804336548
Validation loss: 2.2102992791001514

Epoch: 5| Step: 10
Training loss: 3.4055614471435547
Validation loss: 2.2143353441710114

Epoch: 139| Step: 0
Training loss: 2.272839069366455
Validation loss: 2.1916416537377144

Epoch: 5| Step: 1
Training loss: 2.612084150314331
Validation loss: 2.1719149030664915

Epoch: 5| Step: 2
Training loss: 2.226872682571411
Validation loss: 2.1790714007551952

Epoch: 5| Step: 3
Training loss: 2.661022186279297
Validation loss: 2.176607108885242

Epoch: 5| Step: 4
Training loss: 2.60810923576355
Validation loss: 2.1733646803004767

Epoch: 5| Step: 5
Training loss: 2.4867405891418457
Validation loss: 2.1692420077580277

Epoch: 5| Step: 6
Training loss: 2.425010919570923
Validation loss: 2.177645234651463

Epoch: 5| Step: 7
Training loss: 2.2040905952453613
Validation loss: 2.183811215944188

Epoch: 5| Step: 8
Training loss: 2.5030930042266846
Validation loss: 2.186919007250058

Epoch: 5| Step: 9
Training loss: 2.2945382595062256
Validation loss: 2.1854087511698403

Epoch: 5| Step: 10
Training loss: 2.1366207599639893
Validation loss: 2.192643462970693

Epoch: 140| Step: 0
Training loss: 2.7930104732513428
Validation loss: 2.192261219024658

Epoch: 5| Step: 1
Training loss: 2.8951287269592285
Validation loss: 2.2111342491642123

Epoch: 5| Step: 2
Training loss: 2.1653714179992676
Validation loss: 2.2072989607370026

Epoch: 5| Step: 3
Training loss: 2.078364849090576
Validation loss: 2.2177572916912776

Epoch: 5| Step: 4
Training loss: 2.449099540710449
Validation loss: 2.2251925288989978

Epoch: 5| Step: 5
Training loss: 2.4207663536071777
Validation loss: 2.2353560386165494

Epoch: 5| Step: 6
Training loss: 2.365267753601074
Validation loss: 2.2060646087892595

Epoch: 5| Step: 7
Training loss: 2.6419990062713623
Validation loss: 2.2033661488563783

Epoch: 5| Step: 8
Training loss: 2.27565598487854
Validation loss: 2.181560221538749

Epoch: 5| Step: 9
Training loss: 2.2138895988464355
Validation loss: 2.17310441693952

Epoch: 5| Step: 10
Training loss: 2.102060079574585
Validation loss: 2.1761191275811966

Epoch: 141| Step: 0
Training loss: 2.587785243988037
Validation loss: 2.211881551691281

Epoch: 5| Step: 1
Training loss: 2.2418506145477295
Validation loss: 2.213018145612491

Epoch: 5| Step: 2
Training loss: 2.652003526687622
Validation loss: 2.2225359229631323

Epoch: 5| Step: 3
Training loss: 2.15812349319458
Validation loss: 2.242150511792911

Epoch: 5| Step: 4
Training loss: 2.511361598968506
Validation loss: 2.250223289253891

Epoch: 5| Step: 5
Training loss: 2.5707459449768066
Validation loss: 2.2464270053371305

Epoch: 5| Step: 6
Training loss: 3.2162537574768066
Validation loss: 2.23108031160088

Epoch: 5| Step: 7
Training loss: 2.1353096961975098
Validation loss: 2.234706160842731

Epoch: 5| Step: 8
Training loss: 2.5198521614074707
Validation loss: 2.2292593884211716

Epoch: 5| Step: 9
Training loss: 1.9965922832489014
Validation loss: 2.226435520315683

Epoch: 5| Step: 10
Training loss: 1.8755784034729004
Validation loss: 2.2233513837219565

Epoch: 142| Step: 0
Training loss: 2.293701171875
Validation loss: 2.226131740436759

Epoch: 5| Step: 1
Training loss: 3.2433485984802246
Validation loss: 2.2224591649988645

Epoch: 5| Step: 2
Training loss: 1.919032096862793
Validation loss: 2.226626906343686

Epoch: 5| Step: 3
Training loss: 1.9428646564483643
Validation loss: 2.2463059476626817

Epoch: 5| Step: 4
Training loss: 1.9792436361312866
Validation loss: 2.2502208448225454

Epoch: 5| Step: 5
Training loss: 2.954798698425293
Validation loss: 2.254263459995229

Epoch: 5| Step: 6
Training loss: 2.1260788440704346
Validation loss: 2.2165180893354517

Epoch: 5| Step: 7
Training loss: 2.563109874725342
Validation loss: 2.205229651543402

Epoch: 5| Step: 8
Training loss: 2.8916985988616943
Validation loss: 2.2147497733434043

Epoch: 5| Step: 9
Training loss: 2.4712891578674316
Validation loss: 2.192841492673402

Epoch: 5| Step: 10
Training loss: 2.1323132514953613
Validation loss: 2.2054500990016486

Epoch: 143| Step: 0
Training loss: 2.2890126705169678
Validation loss: 2.2287069136096584

Epoch: 5| Step: 1
Training loss: 2.2083921432495117
Validation loss: 2.2532588153757076

Epoch: 5| Step: 2
Training loss: 2.6507046222686768
Validation loss: 2.275657738408735

Epoch: 5| Step: 3
Training loss: 2.027451515197754
Validation loss: 2.289957561800557

Epoch: 5| Step: 4
Training loss: 2.197314500808716
Validation loss: 2.296210760711342

Epoch: 5| Step: 5
Training loss: 2.765746593475342
Validation loss: 2.289281834838211

Epoch: 5| Step: 6
Training loss: 2.2826149463653564
Validation loss: 2.2752492581644366

Epoch: 5| Step: 7
Training loss: 2.636967897415161
Validation loss: 2.261698729248457

Epoch: 5| Step: 8
Training loss: 2.914288282394409
Validation loss: 2.2509115716462493

Epoch: 5| Step: 9
Training loss: 2.8442375659942627
Validation loss: 2.2264654200564147

Epoch: 5| Step: 10
Training loss: 2.046375274658203
Validation loss: 2.211168912149245

Epoch: 144| Step: 0
Training loss: 2.0106194019317627
Validation loss: 2.181720241423576

Epoch: 5| Step: 1
Training loss: 3.1766915321350098
Validation loss: 2.197097509138046

Epoch: 5| Step: 2
Training loss: 2.2338008880615234
Validation loss: 2.1752885439062632

Epoch: 5| Step: 3
Training loss: 1.8908510208129883
Validation loss: 2.180912394677439

Epoch: 5| Step: 4
Training loss: 2.916635274887085
Validation loss: 2.2005013945282146

Epoch: 5| Step: 5
Training loss: 2.8090462684631348
Validation loss: 2.20173535039348

Epoch: 5| Step: 6
Training loss: 2.9113523960113525
Validation loss: 2.185311801971928

Epoch: 5| Step: 7
Training loss: 1.9026695489883423
Validation loss: 2.190443718305198

Epoch: 5| Step: 8
Training loss: 1.971571683883667
Validation loss: 2.1790316015161495

Epoch: 5| Step: 9
Training loss: 2.4019482135772705
Validation loss: 2.167767579837512

Epoch: 5| Step: 10
Training loss: 2.513206720352173
Validation loss: 2.171037282994998

Epoch: 145| Step: 0
Training loss: 1.765771508216858
Validation loss: 2.168027634261757

Epoch: 5| Step: 1
Training loss: 2.705692768096924
Validation loss: 2.155712112303703

Epoch: 5| Step: 2
Training loss: 1.9958922863006592
Validation loss: 2.180732432232108

Epoch: 5| Step: 3
Training loss: 3.09967303276062
Validation loss: 2.1888547405119865

Epoch: 5| Step: 4
Training loss: 2.5252158641815186
Validation loss: 2.211457298647973

Epoch: 5| Step: 5
Training loss: 2.3791849613189697
Validation loss: 2.2346524858987458

Epoch: 5| Step: 6
Training loss: 2.0984771251678467
Validation loss: 2.241940300951722

Epoch: 5| Step: 7
Training loss: 2.292187213897705
Validation loss: 2.242734496311475

Epoch: 5| Step: 8
Training loss: 2.9844889640808105
Validation loss: 2.2712373720702304

Epoch: 5| Step: 9
Training loss: 2.3025851249694824
Validation loss: 2.260608329567858

Epoch: 5| Step: 10
Training loss: 2.35890531539917
Validation loss: 2.2557445469722954

Epoch: 146| Step: 0
Training loss: 1.8724720478057861
Validation loss: 2.2296841772653724

Epoch: 5| Step: 1
Training loss: 2.3547399044036865
Validation loss: 2.239698547188954

Epoch: 5| Step: 2
Training loss: 2.164580821990967
Validation loss: 2.225612081507201

Epoch: 5| Step: 3
Training loss: 2.3914055824279785
Validation loss: 2.205705576045539

Epoch: 5| Step: 4
Training loss: 2.470777988433838
Validation loss: 2.191332686331964

Epoch: 5| Step: 5
Training loss: 2.6072146892547607
Validation loss: 2.1740034831467496

Epoch: 5| Step: 6
Training loss: 2.2240467071533203
Validation loss: 2.1847710173617125

Epoch: 5| Step: 7
Training loss: 2.5348217487335205
Validation loss: 2.1742226116118895

Epoch: 5| Step: 8
Training loss: 2.33491587638855
Validation loss: 2.1685846544081167

Epoch: 5| Step: 9
Training loss: 2.8455991744995117
Validation loss: 2.177255312601725

Epoch: 5| Step: 10
Training loss: 2.447996139526367
Validation loss: 2.1778940769933883

Epoch: 147| Step: 0
Training loss: 2.6654160022735596
Validation loss: 2.1866111550279843

Epoch: 5| Step: 1
Training loss: 2.1774230003356934
Validation loss: 2.214526422562138

Epoch: 5| Step: 2
Training loss: 2.2472167015075684
Validation loss: 2.2217370258864535

Epoch: 5| Step: 3
Training loss: 2.647871494293213
Validation loss: 2.2522055666933776

Epoch: 5| Step: 4
Training loss: 1.9761874675750732
Validation loss: 2.2341874132874193

Epoch: 5| Step: 5
Training loss: 2.4425485134124756
Validation loss: 2.2558413244062856

Epoch: 5| Step: 6
Training loss: 2.5861945152282715
Validation loss: 2.2573213115815194

Epoch: 5| Step: 7
Training loss: 2.0531742572784424
Validation loss: 2.2666322723511727

Epoch: 5| Step: 8
Training loss: 2.4727883338928223
Validation loss: 2.2516189134249123

Epoch: 5| Step: 9
Training loss: 2.8728229999542236
Validation loss: 2.234325408935547

Epoch: 5| Step: 10
Training loss: 2.570965528488159
Validation loss: 2.2183906570557625

Epoch: 148| Step: 0
Training loss: 1.8761152029037476
Validation loss: 2.1974700804679625

Epoch: 5| Step: 1
Training loss: 2.424980878829956
Validation loss: 2.196583999100552

Epoch: 5| Step: 2
Training loss: 2.6315605640411377
Validation loss: 2.204821563536121

Epoch: 5| Step: 3
Training loss: 2.5407791137695312
Validation loss: 2.186758545137221

Epoch: 5| Step: 4
Training loss: 2.561335325241089
Validation loss: 2.1995113075420423

Epoch: 5| Step: 5
Training loss: 3.051198720932007
Validation loss: 2.201966688197146

Epoch: 5| Step: 6
Training loss: 2.182745933532715
Validation loss: 2.2069937157374557

Epoch: 5| Step: 7
Training loss: 2.7659192085266113
Validation loss: 2.2077833452532367

Epoch: 5| Step: 8
Training loss: 2.061311960220337
Validation loss: 2.1984034533141763

Epoch: 5| Step: 9
Training loss: 2.0497961044311523
Validation loss: 2.2034541842758015

Epoch: 5| Step: 10
Training loss: 2.065337657928467
Validation loss: 2.1969730290033485

Epoch: 149| Step: 0
Training loss: 2.027890682220459
Validation loss: 2.1896120040647444

Epoch: 5| Step: 1
Training loss: 2.7331368923187256
Validation loss: 2.190434427671535

Epoch: 5| Step: 2
Training loss: 2.436504364013672
Validation loss: 2.1854120454480572

Epoch: 5| Step: 3
Training loss: 2.2111456394195557
Validation loss: 2.188197310252856

Epoch: 5| Step: 4
Training loss: 2.509523868560791
Validation loss: 2.183814602513467

Epoch: 5| Step: 5
Training loss: 2.5008082389831543
Validation loss: 2.1665528000042005

Epoch: 5| Step: 6
Training loss: 2.417557954788208
Validation loss: 2.178440128603289

Epoch: 5| Step: 7
Training loss: 2.5127880573272705
Validation loss: 2.173081754356302

Epoch: 5| Step: 8
Training loss: 2.2976391315460205
Validation loss: 2.1726579384137223

Epoch: 5| Step: 9
Training loss: 2.4887008666992188
Validation loss: 2.185125009987944

Epoch: 5| Step: 10
Training loss: 1.9423877000808716
Validation loss: 2.1985911707724295

Epoch: 150| Step: 0
Training loss: 2.305300235748291
Validation loss: 2.2225689426545174

Epoch: 5| Step: 1
Training loss: 2.812304735183716
Validation loss: 2.2489878310952136

Epoch: 5| Step: 2
Training loss: 2.396772861480713
Validation loss: 2.258511471491988

Epoch: 5| Step: 3
Training loss: 2.677938461303711
Validation loss: 2.2449908666713263

Epoch: 5| Step: 4
Training loss: 2.319485902786255
Validation loss: 2.2396626754473616

Epoch: 5| Step: 5
Training loss: 2.4089274406433105
Validation loss: 2.222597549038549

Epoch: 5| Step: 6
Training loss: 2.0509181022644043
Validation loss: 2.2263933022816977

Epoch: 5| Step: 7
Training loss: 2.3692519664764404
Validation loss: 2.2131570757076306

Epoch: 5| Step: 8
Training loss: 2.6399917602539062
Validation loss: 2.197653926828856

Epoch: 5| Step: 9
Training loss: 2.2087695598602295
Validation loss: 2.206480949155746

Epoch: 5| Step: 10
Training loss: 1.8705332279205322
Validation loss: 2.192694058982275

Epoch: 151| Step: 0
Training loss: 2.2607016563415527
Validation loss: 2.1854000245371172

Epoch: 5| Step: 1
Training loss: 2.6590709686279297
Validation loss: 2.1818778822498937

Epoch: 5| Step: 2
Training loss: 2.225313901901245
Validation loss: 2.205232525384554

Epoch: 5| Step: 3
Training loss: 2.160062074661255
Validation loss: 2.2278650268431632

Epoch: 5| Step: 4
Training loss: 2.297065258026123
Validation loss: 2.213099113074682

Epoch: 5| Step: 5
Training loss: 2.6568491458892822
Validation loss: 2.216846590401024

Epoch: 5| Step: 6
Training loss: 2.4995017051696777
Validation loss: 2.2176911664265457

Epoch: 5| Step: 7
Training loss: 2.696463108062744
Validation loss: 2.220467910971693

Epoch: 5| Step: 8
Training loss: 2.815473794937134
Validation loss: 2.216225060083533

Epoch: 5| Step: 9
Training loss: 1.8211427927017212
Validation loss: 2.1946836389521116

Epoch: 5| Step: 10
Training loss: 2.0082263946533203
Validation loss: 2.205492270890103

Epoch: 152| Step: 0
Training loss: 3.0436782836914062
Validation loss: 2.185869914229198

Epoch: 5| Step: 1
Training loss: 1.9401943683624268
Validation loss: 2.165333076189923

Epoch: 5| Step: 2
Training loss: 2.1519153118133545
Validation loss: 2.15604805561804

Epoch: 5| Step: 3
Training loss: 2.604935646057129
Validation loss: 2.142179609626852

Epoch: 5| Step: 4
Training loss: 2.3395698070526123
Validation loss: 2.1355277645972466

Epoch: 5| Step: 5
Training loss: 2.307154417037964
Validation loss: 2.122038015755274

Epoch: 5| Step: 6
Training loss: 2.450268268585205
Validation loss: 2.133282558892363

Epoch: 5| Step: 7
Training loss: 2.4901251792907715
Validation loss: 2.1390714081384803

Epoch: 5| Step: 8
Training loss: 2.0363059043884277
Validation loss: 2.1452658868605092

Epoch: 5| Step: 9
Training loss: 2.589623212814331
Validation loss: 2.1595740651571624

Epoch: 5| Step: 10
Training loss: 2.367074966430664
Validation loss: 2.1700428890925583

Epoch: 153| Step: 0
Training loss: 1.8047891855239868
Validation loss: 2.187825656706287

Epoch: 5| Step: 1
Training loss: 2.193209648132324
Validation loss: 2.2131797959727626

Epoch: 5| Step: 2
Training loss: 2.9119386672973633
Validation loss: 2.227225593341294

Epoch: 5| Step: 3
Training loss: 2.5308876037597656
Validation loss: 2.2439650002346245

Epoch: 5| Step: 4
Training loss: 2.0087571144104004
Validation loss: 2.260136422290597

Epoch: 5| Step: 5
Training loss: 2.404946804046631
Validation loss: 2.258174955203969

Epoch: 5| Step: 6
Training loss: 2.6642651557922363
Validation loss: 2.2529322588315575

Epoch: 5| Step: 7
Training loss: 3.0425620079040527
Validation loss: 2.239686071231801

Epoch: 5| Step: 8
Training loss: 2.14091157913208
Validation loss: 2.2214977061876686

Epoch: 5| Step: 9
Training loss: 2.344545364379883
Validation loss: 2.1976818948663692

Epoch: 5| Step: 10
Training loss: 2.08773136138916
Validation loss: 2.1718183307237524

Epoch: 154| Step: 0
Training loss: 2.6547303199768066
Validation loss: 2.1537636428750973

Epoch: 5| Step: 1
Training loss: 1.6365768909454346
Validation loss: 2.141052576803392

Epoch: 5| Step: 2
Training loss: 2.606844425201416
Validation loss: 2.1325000896248767

Epoch: 5| Step: 3
Training loss: 3.3070759773254395
Validation loss: 2.1343805443856025

Epoch: 5| Step: 4
Training loss: 2.197554111480713
Validation loss: 2.135010834663145

Epoch: 5| Step: 5
Training loss: 1.896768569946289
Validation loss: 2.1354247652074343

Epoch: 5| Step: 6
Training loss: 2.2218480110168457
Validation loss: 2.1294687383918354

Epoch: 5| Step: 7
Training loss: 2.9010698795318604
Validation loss: 2.1402395694486556

Epoch: 5| Step: 8
Training loss: 2.5328831672668457
Validation loss: 2.1515823128402873

Epoch: 5| Step: 9
Training loss: 2.1212987899780273
Validation loss: 2.1586108566612325

Epoch: 5| Step: 10
Training loss: 2.1770129203796387
Validation loss: 2.162152003216487

Epoch: 155| Step: 0
Training loss: 1.935980200767517
Validation loss: 2.1974582569573515

Epoch: 5| Step: 1
Training loss: 2.844057321548462
Validation loss: 2.212116241455078

Epoch: 5| Step: 2
Training loss: 2.475548267364502
Validation loss: 2.229938330188874

Epoch: 5| Step: 3
Training loss: 2.5506789684295654
Validation loss: 2.222742437034525

Epoch: 5| Step: 4
Training loss: 1.818373680114746
Validation loss: 2.207643057710381

Epoch: 5| Step: 5
Training loss: 2.4096226692199707
Validation loss: 2.210475693466843

Epoch: 5| Step: 6
Training loss: 2.520245313644409
Validation loss: 2.1983026227643414

Epoch: 5| Step: 7
Training loss: 2.082446813583374
Validation loss: 2.1856205053226923

Epoch: 5| Step: 8
Training loss: 2.277702569961548
Validation loss: 2.1812982905295586

Epoch: 5| Step: 9
Training loss: 2.7305195331573486
Validation loss: 2.156223781647221

Epoch: 5| Step: 10
Training loss: 2.6453137397766113
Validation loss: 2.167917490005493

Epoch: 156| Step: 0
Training loss: 2.6035447120666504
Validation loss: 2.167506928084999

Epoch: 5| Step: 1
Training loss: 1.821031928062439
Validation loss: 2.1715828039312877

Epoch: 5| Step: 2
Training loss: 2.076737642288208
Validation loss: 2.168173700250605

Epoch: 5| Step: 3
Training loss: 2.7537455558776855
Validation loss: 2.1758040689652964

Epoch: 5| Step: 4
Training loss: 2.6142399311065674
Validation loss: 2.176854472006521

Epoch: 5| Step: 5
Training loss: 1.867587685585022
Validation loss: 2.1797449306775163

Epoch: 5| Step: 6
Training loss: 2.407810688018799
Validation loss: 2.188874929181991

Epoch: 5| Step: 7
Training loss: 2.493049383163452
Validation loss: 2.1976659182579286

Epoch: 5| Step: 8
Training loss: 3.25028657913208
Validation loss: 2.1924147349531933

Epoch: 5| Step: 9
Training loss: 2.223792791366577
Validation loss: 2.2016058685959026

Epoch: 5| Step: 10
Training loss: 1.8932147026062012
Validation loss: 2.1897486973834295

Epoch: 157| Step: 0
Training loss: 2.0748848915100098
Validation loss: 2.176191124864804

Epoch: 5| Step: 1
Training loss: 2.025959014892578
Validation loss: 2.188520146954444

Epoch: 5| Step: 2
Training loss: 2.520434856414795
Validation loss: 2.1851509950494252

Epoch: 5| Step: 3
Training loss: 2.533710479736328
Validation loss: 2.2000101074095695

Epoch: 5| Step: 4
Training loss: 2.2587332725524902
Validation loss: 2.194514251524402

Epoch: 5| Step: 5
Training loss: 1.869574785232544
Validation loss: 2.206301796820856

Epoch: 5| Step: 6
Training loss: 2.901655673980713
Validation loss: 2.2041361921577045

Epoch: 5| Step: 7
Training loss: 2.265090227127075
Validation loss: 2.195379759675713

Epoch: 5| Step: 8
Training loss: 2.4791805744171143
Validation loss: 2.1691767195219636

Epoch: 5| Step: 9
Training loss: 2.6646220684051514
Validation loss: 2.1758887716518935

Epoch: 5| Step: 10
Training loss: 2.436109781265259
Validation loss: 2.170397186792025

Epoch: 158| Step: 0
Training loss: 1.750441551208496
Validation loss: 2.1988798392716276

Epoch: 5| Step: 1
Training loss: 1.7739343643188477
Validation loss: 2.187611351731003

Epoch: 5| Step: 2
Training loss: 2.1557226181030273
Validation loss: 2.195700832592544

Epoch: 5| Step: 3
Training loss: 2.8606810569763184
Validation loss: 2.1943157488299954

Epoch: 5| Step: 4
Training loss: 2.419346332550049
Validation loss: 2.185179543751542

Epoch: 5| Step: 5
Training loss: 1.89825439453125
Validation loss: 2.1986594071952243

Epoch: 5| Step: 6
Training loss: 3.1019527912139893
Validation loss: 2.201079257072941

Epoch: 5| Step: 7
Training loss: 2.299851179122925
Validation loss: 2.1923030691762126

Epoch: 5| Step: 8
Training loss: 2.4635047912597656
Validation loss: 2.19080110262799

Epoch: 5| Step: 9
Training loss: 2.2789785861968994
Validation loss: 2.1814771352275724

Epoch: 5| Step: 10
Training loss: 2.966993808746338
Validation loss: 2.188817436977099

Epoch: 159| Step: 0
Training loss: 2.3737926483154297
Validation loss: 2.1903033333439983

Epoch: 5| Step: 1
Training loss: 2.7747764587402344
Validation loss: 2.1865616101090626

Epoch: 5| Step: 2
Training loss: 2.197655200958252
Validation loss: 2.2035457165010515

Epoch: 5| Step: 3
Training loss: 2.327338457107544
Validation loss: 2.201877360702843

Epoch: 5| Step: 4
Training loss: 2.440317392349243
Validation loss: 2.1836542262825915

Epoch: 5| Step: 5
Training loss: 2.5496315956115723
Validation loss: 2.192434613422681

Epoch: 5| Step: 6
Training loss: 1.7848443984985352
Validation loss: 2.1870820419762724

Epoch: 5| Step: 7
Training loss: 2.5400328636169434
Validation loss: 2.1894562680234193

Epoch: 5| Step: 8
Training loss: 2.213951587677002
Validation loss: 2.2045960836513068

Epoch: 5| Step: 9
Training loss: 2.78999400138855
Validation loss: 2.202683525700723

Epoch: 5| Step: 10
Training loss: 1.9680966138839722
Validation loss: 2.1968091380211616

Epoch: 160| Step: 0
Training loss: 2.695176362991333
Validation loss: 2.1928430911033385

Epoch: 5| Step: 1
Training loss: 2.59749174118042
Validation loss: 2.1775363081245014

Epoch: 5| Step: 2
Training loss: 2.3854269981384277
Validation loss: 2.1657247210061676

Epoch: 5| Step: 3
Training loss: 2.3082451820373535
Validation loss: 2.1576075451348418

Epoch: 5| Step: 4
Training loss: 2.293060302734375
Validation loss: 2.1681638994524555

Epoch: 5| Step: 5
Training loss: 2.659052610397339
Validation loss: 2.1700496468492734

Epoch: 5| Step: 6
Training loss: 2.320713758468628
Validation loss: 2.166715920612376

Epoch: 5| Step: 7
Training loss: 2.077385425567627
Validation loss: 2.164759620543449

Epoch: 5| Step: 8
Training loss: 1.74127995967865
Validation loss: 2.1629420685511764

Epoch: 5| Step: 9
Training loss: 2.3437304496765137
Validation loss: 2.1670312394378004

Epoch: 5| Step: 10
Training loss: 2.526909112930298
Validation loss: 2.1620347474211004

Epoch: 161| Step: 0
Training loss: 2.3996596336364746
Validation loss: 2.162697622852941

Epoch: 5| Step: 1
Training loss: 2.636366367340088
Validation loss: 2.179423891088014

Epoch: 5| Step: 2
Training loss: 2.225311040878296
Validation loss: 2.1822417782199

Epoch: 5| Step: 3
Training loss: 3.0243465900421143
Validation loss: 2.1720301925495105

Epoch: 5| Step: 4
Training loss: 2.0459632873535156
Validation loss: 2.1702019078757173

Epoch: 5| Step: 5
Training loss: 1.9929091930389404
Validation loss: 2.176899717700097

Epoch: 5| Step: 6
Training loss: 1.9484131336212158
Validation loss: 2.164634176479873

Epoch: 5| Step: 7
Training loss: 2.1518898010253906
Validation loss: 2.1660131100685365

Epoch: 5| Step: 8
Training loss: 1.7565559148788452
Validation loss: 2.157970792503767

Epoch: 5| Step: 9
Training loss: 3.1322593688964844
Validation loss: 2.1885937490770893

Epoch: 5| Step: 10
Training loss: 2.4165611267089844
Validation loss: 2.1772640546162925

Epoch: 162| Step: 0
Training loss: 2.332751512527466
Validation loss: 2.187094229523854

Epoch: 5| Step: 1
Training loss: 3.1480934619903564
Validation loss: 2.1822403733448317

Epoch: 5| Step: 2
Training loss: 2.4024863243103027
Validation loss: 2.1996693790599866

Epoch: 5| Step: 3
Training loss: 2.566272258758545
Validation loss: 2.208826676491768

Epoch: 5| Step: 4
Training loss: 2.3793365955352783
Validation loss: 2.188444777201581

Epoch: 5| Step: 5
Training loss: 2.474076986312866
Validation loss: 2.1883544370692265

Epoch: 5| Step: 6
Training loss: 2.493356227874756
Validation loss: 2.1921964101893927

Epoch: 5| Step: 7
Training loss: 2.1002860069274902
Validation loss: 2.1914706307072795

Epoch: 5| Step: 8
Training loss: 1.5529565811157227
Validation loss: 2.195039141562677

Epoch: 5| Step: 9
Training loss: 2.062690258026123
Validation loss: 2.186790686781688

Epoch: 5| Step: 10
Training loss: 2.280473232269287
Validation loss: 2.1867316935652044

Epoch: 163| Step: 0
Training loss: 2.7343802452087402
Validation loss: 2.1980614828807052

Epoch: 5| Step: 1
Training loss: 1.701338529586792
Validation loss: 2.1759255855314192

Epoch: 5| Step: 2
Training loss: 2.6464452743530273
Validation loss: 2.167604106728749

Epoch: 5| Step: 3
Training loss: 2.0951104164123535
Validation loss: 2.165936541813676

Epoch: 5| Step: 4
Training loss: 2.7256925106048584
Validation loss: 2.165209072892384

Epoch: 5| Step: 5
Training loss: 2.108659505844116
Validation loss: 2.156158898466377

Epoch: 5| Step: 6
Training loss: 2.3017313480377197
Validation loss: 2.1432184327033257

Epoch: 5| Step: 7
Training loss: 2.233891725540161
Validation loss: 2.14663682189039

Epoch: 5| Step: 8
Training loss: 2.5264244079589844
Validation loss: 2.156644836548836

Epoch: 5| Step: 9
Training loss: 2.5872018337249756
Validation loss: 2.1594898675077703

Epoch: 5| Step: 10
Training loss: 2.10966157913208
Validation loss: 2.163736338256508

Epoch: 164| Step: 0
Training loss: 2.144594669342041
Validation loss: 2.182111119711271

Epoch: 5| Step: 1
Training loss: 3.02886962890625
Validation loss: 2.1684252600516043

Epoch: 5| Step: 2
Training loss: 2.1557729244232178
Validation loss: 2.1658309851923296

Epoch: 5| Step: 3
Training loss: 2.1394643783569336
Validation loss: 2.155914871923385

Epoch: 5| Step: 4
Training loss: 2.8788657188415527
Validation loss: 2.15772412156546

Epoch: 5| Step: 5
Training loss: 1.9973609447479248
Validation loss: 2.1605420561246973

Epoch: 5| Step: 6
Training loss: 2.5124576091766357
Validation loss: 2.168751332067674

Epoch: 5| Step: 7
Training loss: 1.9591068029403687
Validation loss: 2.1764714230773268

Epoch: 5| Step: 8
Training loss: 2.3538551330566406
Validation loss: 2.176221498879053

Epoch: 5| Step: 9
Training loss: 2.351907730102539
Validation loss: 2.198018694436678

Epoch: 5| Step: 10
Training loss: 2.2696614265441895
Validation loss: 2.189121679593158

Epoch: 165| Step: 0
Training loss: 1.9117755889892578
Validation loss: 2.207508481958861

Epoch: 5| Step: 1
Training loss: 2.2103590965270996
Validation loss: 2.213735662480836

Epoch: 5| Step: 2
Training loss: 1.7404873371124268
Validation loss: 2.21194318802126

Epoch: 5| Step: 3
Training loss: 2.1562469005584717
Validation loss: 2.201695449890629

Epoch: 5| Step: 4
Training loss: 2.052833318710327
Validation loss: 2.192584289017544

Epoch: 5| Step: 5
Training loss: 2.8389506340026855
Validation loss: 2.181598109583701

Epoch: 5| Step: 6
Training loss: 2.311177968978882
Validation loss: 2.1922609882970012

Epoch: 5| Step: 7
Training loss: 2.6853249073028564
Validation loss: 2.1813820408236597

Epoch: 5| Step: 8
Training loss: 2.8037352561950684
Validation loss: 2.15361048072897

Epoch: 5| Step: 9
Training loss: 2.531815767288208
Validation loss: 2.151714483896891

Epoch: 5| Step: 10
Training loss: 2.4371848106384277
Validation loss: 2.1456150226695563

Epoch: 166| Step: 0
Training loss: 2.0531575679779053
Validation loss: 2.1390815755372405

Epoch: 5| Step: 1
Training loss: 2.123379945755005
Validation loss: 2.1392089987313874

Epoch: 5| Step: 2
Training loss: 2.1498475074768066
Validation loss: 2.1497213661029773

Epoch: 5| Step: 3
Training loss: 2.205111503601074
Validation loss: 2.1666206826445875

Epoch: 5| Step: 4
Training loss: 2.4426217079162598
Validation loss: 2.180375301709739

Epoch: 5| Step: 5
Training loss: 2.915238618850708
Validation loss: 2.191100747354569

Epoch: 5| Step: 6
Training loss: 2.32769775390625
Validation loss: 2.1792014593719156

Epoch: 5| Step: 7
Training loss: 2.2146430015563965
Validation loss: 2.1715412934621177

Epoch: 5| Step: 8
Training loss: 2.830618381500244
Validation loss: 2.1651873255288727

Epoch: 5| Step: 9
Training loss: 2.2260539531707764
Validation loss: 2.1705418145784767

Epoch: 5| Step: 10
Training loss: 2.2097809314727783
Validation loss: 2.152328404047156

Epoch: 167| Step: 0
Training loss: 2.081144332885742
Validation loss: 2.1527580650903846

Epoch: 5| Step: 1
Training loss: 2.4439682960510254
Validation loss: 2.1779243792257

Epoch: 5| Step: 2
Training loss: 2.6729893684387207
Validation loss: 2.1859942802818875

Epoch: 5| Step: 3
Training loss: 1.6303144693374634
Validation loss: 2.1750080970025834

Epoch: 5| Step: 4
Training loss: 2.2194600105285645
Validation loss: 2.20092378124114

Epoch: 5| Step: 5
Training loss: 2.4231255054473877
Validation loss: 2.2241512831821235

Epoch: 5| Step: 6
Training loss: 2.05723237991333
Validation loss: 2.224340779806978

Epoch: 5| Step: 7
Training loss: 1.8686853647232056
Validation loss: 2.236615885970413

Epoch: 5| Step: 8
Training loss: 3.0326690673828125
Validation loss: 2.244370683546989

Epoch: 5| Step: 9
Training loss: 2.816265106201172
Validation loss: 2.2217443578986713

Epoch: 5| Step: 10
Training loss: 2.614380121231079
Validation loss: 2.1809370927913214

Epoch: 168| Step: 0
Training loss: 2.4966344833374023
Validation loss: 2.1707377638868106

Epoch: 5| Step: 1
Training loss: 2.321866989135742
Validation loss: 2.1662958886033747

Epoch: 5| Step: 2
Training loss: 2.627018451690674
Validation loss: 2.1414350104588333

Epoch: 5| Step: 3
Training loss: 2.2293057441711426
Validation loss: 2.1451170277851883

Epoch: 5| Step: 4
Training loss: 1.958905577659607
Validation loss: 2.121874968210856

Epoch: 5| Step: 5
Training loss: 2.1554040908813477
Validation loss: 2.11254886914325

Epoch: 5| Step: 6
Training loss: 2.841979503631592
Validation loss: 2.1061483070414555

Epoch: 5| Step: 7
Training loss: 2.4235477447509766
Validation loss: 2.111680266677692

Epoch: 5| Step: 8
Training loss: 2.003208875656128
Validation loss: 2.12742079201565

Epoch: 5| Step: 9
Training loss: 2.310981035232544
Validation loss: 2.135285154465706

Epoch: 5| Step: 10
Training loss: 2.5985159873962402
Validation loss: 2.1554854621169386

Epoch: 169| Step: 0
Training loss: 2.5248169898986816
Validation loss: 2.1640815606681247

Epoch: 5| Step: 1
Training loss: 2.292163372039795
Validation loss: 2.170964869119788

Epoch: 5| Step: 2
Training loss: 2.50396728515625
Validation loss: 2.19751569532579

Epoch: 5| Step: 3
Training loss: 2.0756680965423584
Validation loss: 2.222212879888473

Epoch: 5| Step: 4
Training loss: 2.836857318878174
Validation loss: 2.2197097450174312

Epoch: 5| Step: 5
Training loss: 2.870422840118408
Validation loss: 2.1923813640430407

Epoch: 5| Step: 6
Training loss: 2.585770845413208
Validation loss: 2.1837659740960724

Epoch: 5| Step: 7
Training loss: 2.584015130996704
Validation loss: 2.1664921160667174

Epoch: 5| Step: 8
Training loss: 1.4212207794189453
Validation loss: 2.144523371932327

Epoch: 5| Step: 9
Training loss: 2.21561861038208
Validation loss: 2.1427170217678113

Epoch: 5| Step: 10
Training loss: 1.8966091871261597
Validation loss: 2.144920733667189

Epoch: 170| Step: 0
Training loss: 2.764893054962158
Validation loss: 2.1358380881688928

Epoch: 5| Step: 1
Training loss: 2.2888407707214355
Validation loss: 2.1451076435786423

Epoch: 5| Step: 2
Training loss: 2.1520793437957764
Validation loss: 2.1489803483409267

Epoch: 5| Step: 3
Training loss: 3.0859901905059814
Validation loss: 2.1568860315507457

Epoch: 5| Step: 4
Training loss: 2.2019667625427246
Validation loss: 2.1505885175479356

Epoch: 5| Step: 5
Training loss: 1.852503776550293
Validation loss: 2.1709058336032334

Epoch: 5| Step: 6
Training loss: 2.1995208263397217
Validation loss: 2.1775942938302153

Epoch: 5| Step: 7
Training loss: 2.844731569290161
Validation loss: 2.1878476322338147

Epoch: 5| Step: 8
Training loss: 2.444077968597412
Validation loss: 2.1889161755961757

Epoch: 5| Step: 9
Training loss: 2.096067428588867
Validation loss: 2.1731898759001043

Epoch: 5| Step: 10
Training loss: 1.6508331298828125
Validation loss: 2.186277727926931

Epoch: 171| Step: 0
Training loss: 1.7673699855804443
Validation loss: 2.18964490454684

Epoch: 5| Step: 1
Training loss: 2.1498236656188965
Validation loss: 2.179851708873626

Epoch: 5| Step: 2
Training loss: 2.688410997390747
Validation loss: 2.160619040971161

Epoch: 5| Step: 3
Training loss: 2.1328670978546143
Validation loss: 2.161788740465718

Epoch: 5| Step: 4
Training loss: 2.2684988975524902
Validation loss: 2.1479398486434773

Epoch: 5| Step: 5
Training loss: 2.6571507453918457
Validation loss: 2.144255486867761

Epoch: 5| Step: 6
Training loss: 2.2752606868743896
Validation loss: 2.1510011124354538

Epoch: 5| Step: 7
Training loss: 1.774132490158081
Validation loss: 2.145959800289523

Epoch: 5| Step: 8
Training loss: 2.751633405685425
Validation loss: 2.1447435322628228

Epoch: 5| Step: 9
Training loss: 2.3359627723693848
Validation loss: 2.1456920882707

Epoch: 5| Step: 10
Training loss: 2.821223258972168
Validation loss: 2.150312369869601

Epoch: 172| Step: 0
Training loss: 2.0893077850341797
Validation loss: 2.1501455960735196

Epoch: 5| Step: 1
Training loss: 2.773122787475586
Validation loss: 2.1498801092947684

Epoch: 5| Step: 2
Training loss: 2.597634792327881
Validation loss: 2.155331088650611

Epoch: 5| Step: 3
Training loss: 2.311751127243042
Validation loss: 2.1560134503149215

Epoch: 5| Step: 4
Training loss: 2.0995681285858154
Validation loss: 2.1446453948174753

Epoch: 5| Step: 5
Training loss: 2.3407373428344727
Validation loss: 2.167803964307231

Epoch: 5| Step: 6
Training loss: 2.541248083114624
Validation loss: 2.1692121233991397

Epoch: 5| Step: 7
Training loss: 1.921148657798767
Validation loss: 2.148840727344636

Epoch: 5| Step: 8
Training loss: 2.284719944000244
Validation loss: 2.159019157450686

Epoch: 5| Step: 9
Training loss: 2.0831611156463623
Validation loss: 2.1632223180545274

Epoch: 5| Step: 10
Training loss: 2.4759013652801514
Validation loss: 2.167029857635498

Epoch: 173| Step: 0
Training loss: 2.684649705886841
Validation loss: 2.180567438884448

Epoch: 5| Step: 1
Training loss: 2.0978972911834717
Validation loss: 2.216560317624

Epoch: 5| Step: 2
Training loss: 1.814436912536621
Validation loss: 2.227137704049387

Epoch: 5| Step: 3
Training loss: 1.7112823724746704
Validation loss: 2.2201299821176836

Epoch: 5| Step: 4
Training loss: 2.043790578842163
Validation loss: 2.20237208438176

Epoch: 5| Step: 5
Training loss: 2.827643394470215
Validation loss: 2.2054762276270057

Epoch: 5| Step: 6
Training loss: 2.616907835006714
Validation loss: 2.164250158494519

Epoch: 5| Step: 7
Training loss: 2.862924098968506
Validation loss: 2.1442689446992773

Epoch: 5| Step: 8
Training loss: 2.1379776000976562
Validation loss: 2.1274785328936834

Epoch: 5| Step: 9
Training loss: 2.3591604232788086
Validation loss: 2.117055308434271

Epoch: 5| Step: 10
Training loss: 2.4185025691986084
Validation loss: 2.1164968488036946

Epoch: 174| Step: 0
Training loss: 2.275956869125366
Validation loss: 2.1216801725408083

Epoch: 5| Step: 1
Training loss: 1.7279084920883179
Validation loss: 2.109346910189557

Epoch: 5| Step: 2
Training loss: 2.200495481491089
Validation loss: 2.108942078005883

Epoch: 5| Step: 3
Training loss: 2.7568485736846924
Validation loss: 2.1187275609662457

Epoch: 5| Step: 4
Training loss: 1.8399444818496704
Validation loss: 2.1192425361243625

Epoch: 5| Step: 5
Training loss: 2.8854758739471436
Validation loss: 2.1209952139085337

Epoch: 5| Step: 6
Training loss: 2.28058123588562
Validation loss: 2.142525775458223

Epoch: 5| Step: 7
Training loss: 2.786482334136963
Validation loss: 2.150189950901975

Epoch: 5| Step: 8
Training loss: 2.648317575454712
Validation loss: 2.1987174134100638

Epoch: 5| Step: 9
Training loss: 2.3664193153381348
Validation loss: 2.186473943853891

Epoch: 5| Step: 10
Training loss: 1.6887588500976562
Validation loss: 2.2373078279597785

Epoch: 175| Step: 0
Training loss: 2.799299716949463
Validation loss: 2.2431587634548062

Epoch: 5| Step: 1
Training loss: 1.889542818069458
Validation loss: 2.258487142542357

Epoch: 5| Step: 2
Training loss: 2.410160779953003
Validation loss: 2.2171666006888113

Epoch: 5| Step: 3
Training loss: 2.4709384441375732
Validation loss: 2.196540271082232

Epoch: 5| Step: 4
Training loss: 2.3938345909118652
Validation loss: 2.1716953169914985

Epoch: 5| Step: 5
Training loss: 2.713280439376831
Validation loss: 2.146191684148645

Epoch: 5| Step: 6
Training loss: 2.1037001609802246
Validation loss: 2.13034975400535

Epoch: 5| Step: 7
Training loss: 1.8715074062347412
Validation loss: 2.136402286509032

Epoch: 5| Step: 8
Training loss: 2.602397918701172
Validation loss: 2.141894668661138

Epoch: 5| Step: 9
Training loss: 2.2381534576416016
Validation loss: 2.1320672855582288

Epoch: 5| Step: 10
Training loss: 2.33807373046875
Validation loss: 2.1368759139891593

Epoch: 176| Step: 0
Training loss: 1.6995891332626343
Validation loss: 2.1369555329763763

Epoch: 5| Step: 1
Training loss: 2.436638593673706
Validation loss: 2.138022438172371

Epoch: 5| Step: 2
Training loss: 2.188476085662842
Validation loss: 2.1640177567799888

Epoch: 5| Step: 3
Training loss: 2.096057415008545
Validation loss: 2.1533452849234305

Epoch: 5| Step: 4
Training loss: 1.6843068599700928
Validation loss: 2.162075960507957

Epoch: 5| Step: 5
Training loss: 2.757439613342285
Validation loss: 2.1647551444268998

Epoch: 5| Step: 6
Training loss: 2.8126015663146973
Validation loss: 2.1669815407004407

Epoch: 5| Step: 7
Training loss: 2.4801063537597656
Validation loss: 2.1481304425065235

Epoch: 5| Step: 8
Training loss: 2.7354462146759033
Validation loss: 2.1547509495930006

Epoch: 5| Step: 9
Training loss: 2.275980234146118
Validation loss: 2.147258366307905

Epoch: 5| Step: 10
Training loss: 2.2754008769989014
Validation loss: 2.161595421452676

Epoch: 177| Step: 0
Training loss: 2.1575229167938232
Validation loss: 2.170553586816275

Epoch: 5| Step: 1
Training loss: 2.19309663772583
Validation loss: 2.1638332836089598

Epoch: 5| Step: 2
Training loss: 2.3253238201141357
Validation loss: 2.168372905382546

Epoch: 5| Step: 3
Training loss: 1.9171602725982666
Validation loss: 2.1636407862427416

Epoch: 5| Step: 4
Training loss: 2.133066177368164
Validation loss: 2.158916029878842

Epoch: 5| Step: 5
Training loss: 2.818934917449951
Validation loss: 2.1838646191422657

Epoch: 5| Step: 6
Training loss: 2.349191665649414
Validation loss: 2.16040700738148

Epoch: 5| Step: 7
Training loss: 2.2886624336242676
Validation loss: 2.151879031171081

Epoch: 5| Step: 8
Training loss: 2.3828883171081543
Validation loss: 2.1694951313798145

Epoch: 5| Step: 9
Training loss: 2.444359540939331
Validation loss: 2.1518698225739183

Epoch: 5| Step: 10
Training loss: 2.366973400115967
Validation loss: 2.1389633660675376

Epoch: 178| Step: 0
Training loss: 2.4200921058654785
Validation loss: 2.1467819957322973

Epoch: 5| Step: 1
Training loss: 2.2574868202209473
Validation loss: 2.1501937937992874

Epoch: 5| Step: 2
Training loss: 2.373713970184326
Validation loss: 2.1391506707796486

Epoch: 5| Step: 3
Training loss: 1.9453155994415283
Validation loss: 2.1653729664382113

Epoch: 5| Step: 4
Training loss: 2.450962543487549
Validation loss: 2.1786220406973236

Epoch: 5| Step: 5
Training loss: 2.3276925086975098
Validation loss: 2.1937849649818997

Epoch: 5| Step: 6
Training loss: 1.9513124227523804
Validation loss: 2.199128020194269

Epoch: 5| Step: 7
Training loss: 1.6563583612442017
Validation loss: 2.2027402706043695

Epoch: 5| Step: 8
Training loss: 3.108259677886963
Validation loss: 2.1915219804292083

Epoch: 5| Step: 9
Training loss: 2.6037368774414062
Validation loss: 2.161249701694776

Epoch: 5| Step: 10
Training loss: 2.4407379627227783
Validation loss: 2.1441942068838302

Epoch: 179| Step: 0
Training loss: 2.431475877761841
Validation loss: 2.1522987311886204

Epoch: 5| Step: 1
Training loss: 2.709635019302368
Validation loss: 2.1387780404859975

Epoch: 5| Step: 2
Training loss: 2.382129669189453
Validation loss: 2.132440008142943

Epoch: 5| Step: 3
Training loss: 2.399625778198242
Validation loss: 2.1287514573784283

Epoch: 5| Step: 4
Training loss: 2.7271857261657715
Validation loss: 2.110378014144077

Epoch: 5| Step: 5
Training loss: 1.7397477626800537
Validation loss: 2.1234074843827115

Epoch: 5| Step: 6
Training loss: 2.490278959274292
Validation loss: 2.135860966097924

Epoch: 5| Step: 7
Training loss: 2.2753634452819824
Validation loss: 2.1428931425976496

Epoch: 5| Step: 8
Training loss: 2.4077506065368652
Validation loss: 2.1596386548011535

Epoch: 5| Step: 9
Training loss: 2.537616729736328
Validation loss: 2.1703484827472317

Epoch: 5| Step: 10
Training loss: 1.2972407341003418
Validation loss: 2.1729402080658944

Epoch: 180| Step: 0
Training loss: 2.7003514766693115
Validation loss: 2.1872773221744004

Epoch: 5| Step: 1
Training loss: 2.0540590286254883
Validation loss: 2.1593178113301597

Epoch: 5| Step: 2
Training loss: 2.498473882675171
Validation loss: 2.165183286513052

Epoch: 5| Step: 3
Training loss: 2.001502275466919
Validation loss: 2.143402627719346

Epoch: 5| Step: 4
Training loss: 2.556126832962036
Validation loss: 2.140266336420531

Epoch: 5| Step: 5
Training loss: 2.0832436084747314
Validation loss: 2.1301844350753294

Epoch: 5| Step: 6
Training loss: 2.720991611480713
Validation loss: 2.129952771689302

Epoch: 5| Step: 7
Training loss: 2.3681881427764893
Validation loss: 2.122378913305139

Epoch: 5| Step: 8
Training loss: 2.224252700805664
Validation loss: 2.1286384879901843

Epoch: 5| Step: 9
Training loss: 2.1793389320373535
Validation loss: 2.1380949097294963

Epoch: 5| Step: 10
Training loss: 2.039926052093506
Validation loss: 2.1383366559141423

Epoch: 181| Step: 0
Training loss: 2.362682819366455
Validation loss: 2.139389632850565

Epoch: 5| Step: 1
Training loss: 1.9220205545425415
Validation loss: 2.1334265765323432

Epoch: 5| Step: 2
Training loss: 2.5045599937438965
Validation loss: 2.1356378165624474

Epoch: 5| Step: 3
Training loss: 2.433897018432617
Validation loss: 2.1614482364346905

Epoch: 5| Step: 4
Training loss: 2.285565137863159
Validation loss: 2.1569437749924196

Epoch: 5| Step: 5
Training loss: 2.32200288772583
Validation loss: 2.179793204030683

Epoch: 5| Step: 6
Training loss: 2.6157913208007812
Validation loss: 2.205286850211441

Epoch: 5| Step: 7
Training loss: 1.777719497680664
Validation loss: 2.210318262859057

Epoch: 5| Step: 8
Training loss: 2.3010520935058594
Validation loss: 2.2199726450827812

Epoch: 5| Step: 9
Training loss: 2.4568305015563965
Validation loss: 2.2060513701490176

Epoch: 5| Step: 10
Training loss: 2.6660280227661133
Validation loss: 2.180791083202567

Epoch: 182| Step: 0
Training loss: 2.623157501220703
Validation loss: 2.1575183099316013

Epoch: 5| Step: 1
Training loss: 1.9690347909927368
Validation loss: 2.1282088500197216

Epoch: 5| Step: 2
Training loss: 2.563305377960205
Validation loss: 2.1230910772918374

Epoch: 5| Step: 3
Training loss: 2.690990686416626
Validation loss: 2.1322687249029837

Epoch: 5| Step: 4
Training loss: 2.2225613594055176
Validation loss: 2.1407804014862224

Epoch: 5| Step: 5
Training loss: 2.259550094604492
Validation loss: 2.1366225775851997

Epoch: 5| Step: 6
Training loss: 2.529538154602051
Validation loss: 2.1339446908684185

Epoch: 5| Step: 7
Training loss: 3.034198760986328
Validation loss: 2.13157162615048

Epoch: 5| Step: 8
Training loss: 2.3043246269226074
Validation loss: 2.12919480569901

Epoch: 5| Step: 9
Training loss: 1.4961622953414917
Validation loss: 2.1319297257290093

Epoch: 5| Step: 10
Training loss: 1.9907397031784058
Validation loss: 2.140042015301284

Epoch: 183| Step: 0
Training loss: 2.4769465923309326
Validation loss: 2.1535766611817064

Epoch: 5| Step: 1
Training loss: 2.547250509262085
Validation loss: 2.145617313282464

Epoch: 5| Step: 2
Training loss: 2.0599160194396973
Validation loss: 2.160130803303052

Epoch: 5| Step: 3
Training loss: 2.155022144317627
Validation loss: 2.172771715348767

Epoch: 5| Step: 4
Training loss: 2.233733654022217
Validation loss: 2.181073845073741

Epoch: 5| Step: 5
Training loss: 2.509230852127075
Validation loss: 2.1791757870745916

Epoch: 5| Step: 6
Training loss: 2.7786598205566406
Validation loss: 2.1693688848967194

Epoch: 5| Step: 7
Training loss: 2.3368701934814453
Validation loss: 2.185863740982548

Epoch: 5| Step: 8
Training loss: 2.3192787170410156
Validation loss: 2.1753420137589976

Epoch: 5| Step: 9
Training loss: 1.7342849969863892
Validation loss: 2.1803188067610546

Epoch: 5| Step: 10
Training loss: 2.18436861038208
Validation loss: 2.16891251712717

Epoch: 184| Step: 0
Training loss: 2.3515987396240234
Validation loss: 2.1622404475365915

Epoch: 5| Step: 1
Training loss: 2.0039544105529785
Validation loss: 2.1473150714751212

Epoch: 5| Step: 2
Training loss: 2.5935966968536377
Validation loss: 2.1450612211740143

Epoch: 5| Step: 3
Training loss: 2.5689828395843506
Validation loss: 2.1464480225757887

Epoch: 5| Step: 4
Training loss: 2.405759334564209
Validation loss: 2.1246746919488393

Epoch: 5| Step: 5
Training loss: 1.8722152709960938
Validation loss: 2.1231467877664874

Epoch: 5| Step: 6
Training loss: 2.071420192718506
Validation loss: 2.1245755585291053

Epoch: 5| Step: 7
Training loss: 2.20750093460083
Validation loss: 2.1253023327037854

Epoch: 5| Step: 8
Training loss: 2.6674084663391113
Validation loss: 2.134037697187034

Epoch: 5| Step: 9
Training loss: 2.52225923538208
Validation loss: 2.130714037085092

Epoch: 5| Step: 10
Training loss: 2.2514233589172363
Validation loss: 2.1376113789055937

Epoch: 185| Step: 0
Training loss: 2.3743698596954346
Validation loss: 2.1553444272728375

Epoch: 5| Step: 1
Training loss: 2.2027573585510254
Validation loss: 2.146925918517574

Epoch: 5| Step: 2
Training loss: 2.2331786155700684
Validation loss: 2.153182950071109

Epoch: 5| Step: 3
Training loss: 2.5115907192230225
Validation loss: 2.149189974672051

Epoch: 5| Step: 4
Training loss: 2.8529820442199707
Validation loss: 2.1518842251070085

Epoch: 5| Step: 5
Training loss: 2.5238559246063232
Validation loss: 2.146335353133499

Epoch: 5| Step: 6
Training loss: 1.9250915050506592
Validation loss: 2.1462748563417824

Epoch: 5| Step: 7
Training loss: 2.248497486114502
Validation loss: 2.145740560306016

Epoch: 5| Step: 8
Training loss: 1.7467355728149414
Validation loss: 2.1661124126885527

Epoch: 5| Step: 9
Training loss: 2.2448084354400635
Validation loss: 2.1552120203612954

Epoch: 5| Step: 10
Training loss: 2.435859441757202
Validation loss: 2.150342195264755

Epoch: 186| Step: 0
Training loss: 2.7319908142089844
Validation loss: 2.14982250941697

Epoch: 5| Step: 1
Training loss: 1.9811782836914062
Validation loss: 2.122966815066594

Epoch: 5| Step: 2
Training loss: 3.066659927368164
Validation loss: 2.1321042814562396

Epoch: 5| Step: 3
Training loss: 2.167975425720215
Validation loss: 2.1179567037090177

Epoch: 5| Step: 4
Training loss: 2.144042491912842
Validation loss: 2.109935470806655

Epoch: 5| Step: 5
Training loss: 1.730678915977478
Validation loss: 2.112066279175461

Epoch: 5| Step: 6
Training loss: 2.285031318664551
Validation loss: 2.126899116782732

Epoch: 5| Step: 7
Training loss: 2.1351237297058105
Validation loss: 2.1371226810639903

Epoch: 5| Step: 8
Training loss: 2.522841453552246
Validation loss: 2.1476491805045836

Epoch: 5| Step: 9
Training loss: 2.2212576866149902
Validation loss: 2.1598902261385353

Epoch: 5| Step: 10
Training loss: 2.2802774906158447
Validation loss: 2.1620598582811255

Epoch: 187| Step: 0
Training loss: 2.2317347526550293
Validation loss: 2.1903732156240814

Epoch: 5| Step: 1
Training loss: 2.2562026977539062
Validation loss: 2.176198923459617

Epoch: 5| Step: 2
Training loss: 2.6349596977233887
Validation loss: 2.190832197025258

Epoch: 5| Step: 3
Training loss: 2.1526694297790527
Validation loss: 2.1839611607213176

Epoch: 5| Step: 4
Training loss: 2.171743869781494
Validation loss: 2.181474124231646

Epoch: 5| Step: 5
Training loss: 2.218970537185669
Validation loss: 2.160548704926686

Epoch: 5| Step: 6
Training loss: 2.550920248031616
Validation loss: 2.1478210726091937

Epoch: 5| Step: 7
Training loss: 1.8574234247207642
Validation loss: 2.1376861718393143

Epoch: 5| Step: 8
Training loss: 2.2105751037597656
Validation loss: 2.13591666119073

Epoch: 5| Step: 9
Training loss: 2.434998035430908
Validation loss: 2.125420526791644

Epoch: 5| Step: 10
Training loss: 2.5309205055236816
Validation loss: 2.1264618186540503

Epoch: 188| Step: 0
Training loss: 2.4056992530822754
Validation loss: 2.1222798144945534

Epoch: 5| Step: 1
Training loss: 1.6343787908554077
Validation loss: 2.1251232175416845

Epoch: 5| Step: 2
Training loss: 2.4696266651153564
Validation loss: 2.1323157356631373

Epoch: 5| Step: 3
Training loss: 2.6264615058898926
Validation loss: 2.120665398977136

Epoch: 5| Step: 4
Training loss: 2.1853089332580566
Validation loss: 2.1291873890866517

Epoch: 5| Step: 5
Training loss: 2.239779472351074
Validation loss: 2.112626907646015

Epoch: 5| Step: 6
Training loss: 1.9439111948013306
Validation loss: 2.1191525126016266

Epoch: 5| Step: 7
Training loss: 2.1168830394744873
Validation loss: 2.132156166979062

Epoch: 5| Step: 8
Training loss: 2.561363935470581
Validation loss: 2.140865034954522

Epoch: 5| Step: 9
Training loss: 2.2262587547302246
Validation loss: 2.1498709519704184

Epoch: 5| Step: 10
Training loss: 2.9542605876922607
Validation loss: 2.162242897095219

Epoch: 189| Step: 0
Training loss: 1.5466444492340088
Validation loss: 2.1482607113417758

Epoch: 5| Step: 1
Training loss: 2.030524730682373
Validation loss: 2.1576117546327653

Epoch: 5| Step: 2
Training loss: 1.3545395135879517
Validation loss: 2.160083141378177

Epoch: 5| Step: 3
Training loss: 3.052056312561035
Validation loss: 2.165180710054213

Epoch: 5| Step: 4
Training loss: 2.391122341156006
Validation loss: 2.1708796895960325

Epoch: 5| Step: 5
Training loss: 2.0945827960968018
Validation loss: 2.1462586669511694

Epoch: 5| Step: 6
Training loss: 2.4816033840179443
Validation loss: 2.136281946653961

Epoch: 5| Step: 7
Training loss: 2.6160809993743896
Validation loss: 2.131009263377036

Epoch: 5| Step: 8
Training loss: 2.1624691486358643
Validation loss: 2.1221655338041243

Epoch: 5| Step: 9
Training loss: 2.526986598968506
Validation loss: 2.1244217618819206

Epoch: 5| Step: 10
Training loss: 2.9922332763671875
Validation loss: 2.115982796556206

Epoch: 190| Step: 0
Training loss: 1.892033576965332
Validation loss: 2.132834352472777

Epoch: 5| Step: 1
Training loss: 2.735288381576538
Validation loss: 2.130189549538397

Epoch: 5| Step: 2
Training loss: 2.140780448913574
Validation loss: 2.133153651350288

Epoch: 5| Step: 3
Training loss: 2.0586109161376953
Validation loss: 2.132804770623484

Epoch: 5| Step: 4
Training loss: 1.8757832050323486
Validation loss: 2.1283173176550094

Epoch: 5| Step: 5
Training loss: 2.9951179027557373
Validation loss: 2.1304784385106896

Epoch: 5| Step: 6
Training loss: 2.5055594444274902
Validation loss: 2.1380907079224944

Epoch: 5| Step: 7
Training loss: 2.3638854026794434
Validation loss: 2.133731142167122

Epoch: 5| Step: 8
Training loss: 1.8063777685165405
Validation loss: 2.11652962879468

Epoch: 5| Step: 9
Training loss: 2.6599223613739014
Validation loss: 2.1252593225048435

Epoch: 5| Step: 10
Training loss: 2.131108045578003
Validation loss: 2.125705733094164

Epoch: 191| Step: 0
Training loss: 2.0795798301696777
Validation loss: 2.1314892012585878

Epoch: 5| Step: 1
Training loss: 2.3183791637420654
Validation loss: 2.1525270990146104

Epoch: 5| Step: 2
Training loss: 2.443537473678589
Validation loss: 2.154883782068888

Epoch: 5| Step: 3
Training loss: 2.4302196502685547
Validation loss: 2.1528123399262786

Epoch: 5| Step: 4
Training loss: 2.304892063140869
Validation loss: 2.159414134999757

Epoch: 5| Step: 5
Training loss: 2.4485886096954346
Validation loss: 2.147525538680374

Epoch: 5| Step: 6
Training loss: 2.1170217990875244
Validation loss: 2.1411982069733324

Epoch: 5| Step: 7
Training loss: 2.3473758697509766
Validation loss: 2.1521271133935578

Epoch: 5| Step: 8
Training loss: 2.0974907875061035
Validation loss: 2.1574187637657247

Epoch: 5| Step: 9
Training loss: 2.2455852031707764
Validation loss: 2.1335736179864533

Epoch: 5| Step: 10
Training loss: 2.220829725265503
Validation loss: 2.127361794953705

Epoch: 192| Step: 0
Training loss: 2.2099385261535645
Validation loss: 2.1353480636432605

Epoch: 5| Step: 1
Training loss: 2.364487886428833
Validation loss: 2.13996156056722

Epoch: 5| Step: 2
Training loss: 2.4882664680480957
Validation loss: 2.1291561280527422

Epoch: 5| Step: 3
Training loss: 2.4564621448516846
Validation loss: 2.13054391389252

Epoch: 5| Step: 4
Training loss: 1.969067931175232
Validation loss: 2.140992490194177

Epoch: 5| Step: 5
Training loss: 1.9373767375946045
Validation loss: 2.1490521328423613

Epoch: 5| Step: 6
Training loss: 2.0245766639709473
Validation loss: 2.1516064366986676

Epoch: 5| Step: 7
Training loss: 2.2359728813171387
Validation loss: 2.1529672171479914

Epoch: 5| Step: 8
Training loss: 2.297849655151367
Validation loss: 2.153513959659043

Epoch: 5| Step: 9
Training loss: 2.5737531185150146
Validation loss: 2.149377651112054

Epoch: 5| Step: 10
Training loss: 2.8287529945373535
Validation loss: 2.183292947789674

Epoch: 193| Step: 0
Training loss: 2.1269564628601074
Validation loss: 2.1878967746611564

Epoch: 5| Step: 1
Training loss: 2.0264744758605957
Validation loss: 2.1918531028173303

Epoch: 5| Step: 2
Training loss: 1.7520185708999634
Validation loss: 2.1806668312318864

Epoch: 5| Step: 3
Training loss: 3.2148520946502686
Validation loss: 2.176891573013798

Epoch: 5| Step: 4
Training loss: 2.1307313442230225
Validation loss: 2.1502246625961794

Epoch: 5| Step: 5
Training loss: 2.4327073097229004
Validation loss: 2.1356896790125037

Epoch: 5| Step: 6
Training loss: 2.0836987495422363
Validation loss: 2.1234909129399124

Epoch: 5| Step: 7
Training loss: 2.4336538314819336
Validation loss: 2.111350638892061

Epoch: 5| Step: 8
Training loss: 2.0942959785461426
Validation loss: 2.101250765144184

Epoch: 5| Step: 9
Training loss: 1.9914371967315674
Validation loss: 2.1214852640705724

Epoch: 5| Step: 10
Training loss: 3.069819688796997
Validation loss: 2.11671107815158

Epoch: 194| Step: 0
Training loss: 2.3779513835906982
Validation loss: 2.1329788315680718

Epoch: 5| Step: 1
Training loss: 2.2623813152313232
Validation loss: 2.1163939711868123

Epoch: 5| Step: 2
Training loss: 1.499781847000122
Validation loss: 2.1098175638465473

Epoch: 5| Step: 3
Training loss: 2.104240894317627
Validation loss: 2.1204592976518857

Epoch: 5| Step: 4
Training loss: 1.9913305044174194
Validation loss: 2.1085436523601575

Epoch: 5| Step: 5
Training loss: 2.8431003093719482
Validation loss: 2.1164727928817912

Epoch: 5| Step: 6
Training loss: 2.79689359664917
Validation loss: 2.13575158580657

Epoch: 5| Step: 7
Training loss: 1.661806344985962
Validation loss: 2.1325522494572464

Epoch: 5| Step: 8
Training loss: 2.552380084991455
Validation loss: 2.142627690428047

Epoch: 5| Step: 9
Training loss: 2.500894546508789
Validation loss: 2.1441859045336322

Epoch: 5| Step: 10
Training loss: 2.396702527999878
Validation loss: 2.155952888150369

Epoch: 195| Step: 0
Training loss: 2.254706859588623
Validation loss: 2.186027075654717

Epoch: 5| Step: 1
Training loss: 2.2143309116363525
Validation loss: 2.2006380327286257

Epoch: 5| Step: 2
Training loss: 2.787557601928711
Validation loss: 2.2199089732221378

Epoch: 5| Step: 3
Training loss: 2.5871777534484863
Validation loss: 2.221750114553718

Epoch: 5| Step: 4
Training loss: 1.9970550537109375
Validation loss: 2.2152674967242825

Epoch: 5| Step: 5
Training loss: 2.3426809310913086
Validation loss: 2.1669234178399526

Epoch: 5| Step: 6
Training loss: 2.0832619667053223
Validation loss: 2.145131085508613

Epoch: 5| Step: 7
Training loss: 1.7967169284820557
Validation loss: 2.117448592698702

Epoch: 5| Step: 8
Training loss: 2.281960964202881
Validation loss: 2.1166431955111924

Epoch: 5| Step: 9
Training loss: 2.4459586143493652
Validation loss: 2.1142923755030476

Epoch: 5| Step: 10
Training loss: 2.6680939197540283
Validation loss: 2.12291851223156

Epoch: 196| Step: 0
Training loss: 2.595733880996704
Validation loss: 2.1211260685356716

Epoch: 5| Step: 1
Training loss: 1.6121304035186768
Validation loss: 2.131581203911894

Epoch: 5| Step: 2
Training loss: 2.978607177734375
Validation loss: 2.138125245289136

Epoch: 5| Step: 3
Training loss: 2.0747604370117188
Validation loss: 2.1430438000668763

Epoch: 5| Step: 4
Training loss: 2.427821636199951
Validation loss: 2.148235095444546

Epoch: 5| Step: 5
Training loss: 2.1718528270721436
Validation loss: 2.141419190232472

Epoch: 5| Step: 6
Training loss: 2.6451690196990967
Validation loss: 2.143998920276601

Epoch: 5| Step: 7
Training loss: 1.5741946697235107
Validation loss: 2.134497070825228

Epoch: 5| Step: 8
Training loss: 2.0128445625305176
Validation loss: 2.1252158611051497

Epoch: 5| Step: 9
Training loss: 2.506286859512329
Validation loss: 2.116976244475252

Epoch: 5| Step: 10
Training loss: 2.5085229873657227
Validation loss: 2.1177855781329575

Epoch: 197| Step: 0
Training loss: 2.1267619132995605
Validation loss: 2.1028765516896404

Epoch: 5| Step: 1
Training loss: 2.6722934246063232
Validation loss: 2.1020184704052505

Epoch: 5| Step: 2
Training loss: 2.045548677444458
Validation loss: 2.1191322213859967

Epoch: 5| Step: 3
Training loss: 2.68757963180542
Validation loss: 2.1124463863270257

Epoch: 5| Step: 4
Training loss: 2.2814583778381348
Validation loss: 2.1259270175810783

Epoch: 5| Step: 5
Training loss: 1.8635921478271484
Validation loss: 2.1235536862445135

Epoch: 5| Step: 6
Training loss: 2.2665657997131348
Validation loss: 2.131460723056588

Epoch: 5| Step: 7
Training loss: 2.6082894802093506
Validation loss: 2.1540397943988925

Epoch: 5| Step: 8
Training loss: 2.488001585006714
Validation loss: 2.1466474584353867

Epoch: 5| Step: 9
Training loss: 1.8957315683364868
Validation loss: 2.148811709496283

Epoch: 5| Step: 10
Training loss: 1.9582288265228271
Validation loss: 2.1560260723995905

Epoch: 198| Step: 0
Training loss: 2.5205235481262207
Validation loss: 2.157912387642809

Epoch: 5| Step: 1
Training loss: 2.1275906562805176
Validation loss: 2.1351831472048195

Epoch: 5| Step: 2
Training loss: 2.062939167022705
Validation loss: 2.1594665332507064

Epoch: 5| Step: 3
Training loss: 2.8539700508117676
Validation loss: 2.1681209123262795

Epoch: 5| Step: 4
Training loss: 2.1072587966918945
Validation loss: 2.157856915586738

Epoch: 5| Step: 5
Training loss: 2.5382373332977295
Validation loss: 2.121805565331572

Epoch: 5| Step: 6
Training loss: 2.0545618534088135
Validation loss: 2.1186766880814747

Epoch: 5| Step: 7
Training loss: 1.6124799251556396
Validation loss: 2.106599933357649

Epoch: 5| Step: 8
Training loss: 2.1590850353240967
Validation loss: 2.1040105973520586

Epoch: 5| Step: 9
Training loss: 2.535430908203125
Validation loss: 2.110199310446298

Epoch: 5| Step: 10
Training loss: 2.3264846801757812
Validation loss: 2.104634136281988

Epoch: 199| Step: 0
Training loss: 2.4596593379974365
Validation loss: 2.111728855358657

Epoch: 5| Step: 1
Training loss: 1.8659225702285767
Validation loss: 2.1070945288545344

Epoch: 5| Step: 2
Training loss: 2.7371268272399902
Validation loss: 2.1244085745144914

Epoch: 5| Step: 3
Training loss: 2.3868567943573
Validation loss: 2.128424195833104

Epoch: 5| Step: 4
Training loss: 1.8825912475585938
Validation loss: 2.128120514654344

Epoch: 5| Step: 5
Training loss: 2.4891111850738525
Validation loss: 2.1405326269006215

Epoch: 5| Step: 6
Training loss: 2.166630983352661
Validation loss: 2.1488497769960793

Epoch: 5| Step: 7
Training loss: 2.3222312927246094
Validation loss: 2.1418063614958074

Epoch: 5| Step: 8
Training loss: 2.1249144077301025
Validation loss: 2.1657652085827244

Epoch: 5| Step: 9
Training loss: 2.031581163406372
Validation loss: 2.179756167114422

Epoch: 5| Step: 10
Training loss: 2.474207639694214
Validation loss: 2.1561474966746506

Epoch: 200| Step: 0
Training loss: 2.2958972454071045
Validation loss: 2.1436488038750103

Epoch: 5| Step: 1
Training loss: 2.7418313026428223
Validation loss: 2.126893690837327

Epoch: 5| Step: 2
Training loss: 1.9353973865509033
Validation loss: 2.1268407426854616

Epoch: 5| Step: 3
Training loss: 2.704606294631958
Validation loss: 2.104407689904654

Epoch: 5| Step: 4
Training loss: 1.7754558324813843
Validation loss: 2.115577649044734

Epoch: 5| Step: 5
Training loss: 1.7984235286712646
Validation loss: 2.1082365256483837

Epoch: 5| Step: 6
Training loss: 2.587754011154175
Validation loss: 2.0969771492865776

Epoch: 5| Step: 7
Training loss: 2.0787739753723145
Validation loss: 2.1114133250328804

Epoch: 5| Step: 8
Training loss: 2.6334383487701416
Validation loss: 2.107352552875396

Epoch: 5| Step: 9
Training loss: 1.8720260858535767
Validation loss: 2.127077411579829

Epoch: 5| Step: 10
Training loss: 2.7039012908935547
Validation loss: 2.1375719860035884

Epoch: 201| Step: 0
Training loss: 2.697801351547241
Validation loss: 2.1499566314040974

Epoch: 5| Step: 1
Training loss: 1.9336761236190796
Validation loss: 2.1662094234138407

Epoch: 5| Step: 2
Training loss: 2.3068840503692627
Validation loss: 2.160444219907125

Epoch: 5| Step: 3
Training loss: 2.4061028957366943
Validation loss: 2.176316717619537

Epoch: 5| Step: 4
Training loss: 2.4574952125549316
Validation loss: 2.1835177342096963

Epoch: 5| Step: 5
Training loss: 1.9766619205474854
Validation loss: 2.1841558025729273

Epoch: 5| Step: 6
Training loss: 2.431232452392578
Validation loss: 2.1564592930578415

Epoch: 5| Step: 7
Training loss: 2.216949462890625
Validation loss: 2.130087125685907

Epoch: 5| Step: 8
Training loss: 2.5284881591796875
Validation loss: 2.14174053489521

Epoch: 5| Step: 9
Training loss: 1.2197721004486084
Validation loss: 2.1170833136445735

Epoch: 5| Step: 10
Training loss: 2.7587969303131104
Validation loss: 2.1127789712721303

Epoch: 202| Step: 0
Training loss: 2.1951959133148193
Validation loss: 2.0943175631184734

Epoch: 5| Step: 1
Training loss: 1.9582151174545288
Validation loss: 2.1013347205295356

Epoch: 5| Step: 2
Training loss: 2.284817934036255
Validation loss: 2.1213224933993433

Epoch: 5| Step: 3
Training loss: 1.9691396951675415
Validation loss: 2.104461393048686

Epoch: 5| Step: 4
Training loss: 2.4963345527648926
Validation loss: 2.1135411775240334

Epoch: 5| Step: 5
Training loss: 1.971449613571167
Validation loss: 2.114023812355534

Epoch: 5| Step: 6
Training loss: 2.1316170692443848
Validation loss: 2.111539758661742

Epoch: 5| Step: 7
Training loss: 2.357231855392456
Validation loss: 2.1370759702497915

Epoch: 5| Step: 8
Training loss: 2.8229267597198486
Validation loss: 2.139807075582525

Epoch: 5| Step: 9
Training loss: 2.5936973094940186
Validation loss: 2.136997051136468

Epoch: 5| Step: 10
Training loss: 2.002134323120117
Validation loss: 2.1293143354436403

Epoch: 203| Step: 0
Training loss: 2.462550163269043
Validation loss: 2.125405273129863

Epoch: 5| Step: 1
Training loss: 2.8992505073547363
Validation loss: 2.1123686246974493

Epoch: 5| Step: 2
Training loss: 2.118170976638794
Validation loss: 2.115186781011602

Epoch: 5| Step: 3
Training loss: 2.017543315887451
Validation loss: 2.111967585420096

Epoch: 5| Step: 4
Training loss: 1.9810688495635986
Validation loss: 2.125932529408445

Epoch: 5| Step: 5
Training loss: 2.1328346729278564
Validation loss: 2.1173414363655993

Epoch: 5| Step: 6
Training loss: 2.5013740062713623
Validation loss: 2.138620659869204

Epoch: 5| Step: 7
Training loss: 2.0934534072875977
Validation loss: 2.1270973913131224

Epoch: 5| Step: 8
Training loss: 1.9841010570526123
Validation loss: 2.1512544872940227

Epoch: 5| Step: 9
Training loss: 2.6966283321380615
Validation loss: 2.142114964864587

Epoch: 5| Step: 10
Training loss: 1.9368337392807007
Validation loss: 2.134513880616875

Epoch: 204| Step: 0
Training loss: 2.936291456222534
Validation loss: 2.128958479050667

Epoch: 5| Step: 1
Training loss: 2.0451345443725586
Validation loss: 2.114451459659043

Epoch: 5| Step: 2
Training loss: 2.097374439239502
Validation loss: 2.118359564453043

Epoch: 5| Step: 3
Training loss: 2.966362476348877
Validation loss: 2.1125263244875017

Epoch: 5| Step: 4
Training loss: 2.8049709796905518
Validation loss: 2.098551183618525

Epoch: 5| Step: 5
Training loss: 1.9653465747833252
Validation loss: 2.094148146208896

Epoch: 5| Step: 6
Training loss: 1.5983703136444092
Validation loss: 2.102634511968141

Epoch: 5| Step: 7
Training loss: 1.8954591751098633
Validation loss: 2.1079415557205037

Epoch: 5| Step: 8
Training loss: 2.3584492206573486
Validation loss: 2.0959708511188464

Epoch: 5| Step: 9
Training loss: 1.6603374481201172
Validation loss: 2.120287738820558

Epoch: 5| Step: 10
Training loss: 2.386963367462158
Validation loss: 2.131542690338627

Epoch: 205| Step: 0
Training loss: 2.674818277359009
Validation loss: 2.1499938631570465

Epoch: 5| Step: 1
Training loss: 2.1259968280792236
Validation loss: 2.172840392717751

Epoch: 5| Step: 2
Training loss: 2.2190840244293213
Validation loss: 2.185055604545019

Epoch: 5| Step: 3
Training loss: 2.013843059539795
Validation loss: 2.1638369252604823

Epoch: 5| Step: 4
Training loss: 2.1792640686035156
Validation loss: 2.158631452950098

Epoch: 5| Step: 5
Training loss: 1.8331950902938843
Validation loss: 2.1297831484066543

Epoch: 5| Step: 6
Training loss: 1.970994234085083
Validation loss: 2.1204552188996346

Epoch: 5| Step: 7
Training loss: 2.3277907371520996
Validation loss: 2.1187474573812177

Epoch: 5| Step: 8
Training loss: 2.4424033164978027
Validation loss: 2.107868972645011

Epoch: 5| Step: 9
Training loss: 2.7545976638793945
Validation loss: 2.0983454155665573

Epoch: 5| Step: 10
Training loss: 2.569737195968628
Validation loss: 2.1015968758572816

Epoch: 206| Step: 0
Training loss: 2.069911479949951
Validation loss: 2.1034937340726136

Epoch: 5| Step: 1
Training loss: 2.8735084533691406
Validation loss: 2.1050969862168833

Epoch: 5| Step: 2
Training loss: 1.7681337594985962
Validation loss: 2.1193349540874524

Epoch: 5| Step: 3
Training loss: 2.2952380180358887
Validation loss: 2.1110670464013213

Epoch: 5| Step: 4
Training loss: 2.4052202701568604
Validation loss: 2.120713628748412

Epoch: 5| Step: 5
Training loss: 2.2286646366119385
Validation loss: 2.1205883461941957

Epoch: 5| Step: 6
Training loss: 2.4231550693511963
Validation loss: 2.127104533615933

Epoch: 5| Step: 7
Training loss: 2.2410006523132324
Validation loss: 2.150795082892141

Epoch: 5| Step: 8
Training loss: 1.9544150829315186
Validation loss: 2.163469717066775

Epoch: 5| Step: 9
Training loss: 2.6200273036956787
Validation loss: 2.171383218098712

Epoch: 5| Step: 10
Training loss: 2.0930190086364746
Validation loss: 2.17267777586496

Epoch: 207| Step: 0
Training loss: 2.2206642627716064
Validation loss: 2.169271957489752

Epoch: 5| Step: 1
Training loss: 1.8966604471206665
Validation loss: 2.1520467419778146

Epoch: 5| Step: 2
Training loss: 2.000584602355957
Validation loss: 2.1348673784604637

Epoch: 5| Step: 3
Training loss: 2.4597442150115967
Validation loss: 2.1230668098695817

Epoch: 5| Step: 4
Training loss: 2.099783182144165
Validation loss: 2.1182018197992796

Epoch: 5| Step: 5
Training loss: 2.2699966430664062
Validation loss: 2.106119671175557

Epoch: 5| Step: 6
Training loss: 2.382946252822876
Validation loss: 2.089938607267154

Epoch: 5| Step: 7
Training loss: 2.058865785598755
Validation loss: 2.1014018443322953

Epoch: 5| Step: 8
Training loss: 2.2749195098876953
Validation loss: 2.097081712497178

Epoch: 5| Step: 9
Training loss: 2.6099557876586914
Validation loss: 2.0899830120866016

Epoch: 5| Step: 10
Training loss: 2.568002462387085
Validation loss: 2.096624587171821

Epoch: 208| Step: 0
Training loss: 1.622588872909546
Validation loss: 2.0971319983082433

Epoch: 5| Step: 1
Training loss: 2.860962152481079
Validation loss: 2.102414709265514

Epoch: 5| Step: 2
Training loss: 2.8741374015808105
Validation loss: 2.1106659468784126

Epoch: 5| Step: 3
Training loss: 2.6075692176818848
Validation loss: 2.1272148803998063

Epoch: 5| Step: 4
Training loss: 2.0952162742614746
Validation loss: 2.139487812595983

Epoch: 5| Step: 5
Training loss: 2.033446788787842
Validation loss: 2.1274067714650142

Epoch: 5| Step: 6
Training loss: 1.8065211772918701
Validation loss: 2.147028197524368

Epoch: 5| Step: 7
Training loss: 2.4703774452209473
Validation loss: 2.159732425084678

Epoch: 5| Step: 8
Training loss: 1.9566009044647217
Validation loss: 2.1501098089320685

Epoch: 5| Step: 9
Training loss: 2.0512282848358154
Validation loss: 2.1613487479507283

Epoch: 5| Step: 10
Training loss: 2.479748249053955
Validation loss: 2.150069166255254

Epoch: 209| Step: 0
Training loss: 2.4146599769592285
Validation loss: 2.1139080101443875

Epoch: 5| Step: 1
Training loss: 1.5382957458496094
Validation loss: 2.11697797108722

Epoch: 5| Step: 2
Training loss: 2.0212020874023438
Validation loss: 2.117301150034833

Epoch: 5| Step: 3
Training loss: 2.335803270339966
Validation loss: 2.095459971376645

Epoch: 5| Step: 4
Training loss: 2.1306769847869873
Validation loss: 2.103823974568357

Epoch: 5| Step: 5
Training loss: 1.9060518741607666
Validation loss: 2.0976699552228375

Epoch: 5| Step: 6
Training loss: 2.5399317741394043
Validation loss: 2.1005290708234234

Epoch: 5| Step: 7
Training loss: 2.4469122886657715
Validation loss: 2.0909897473550614

Epoch: 5| Step: 8
Training loss: 2.3708527088165283
Validation loss: 2.1035307133069603

Epoch: 5| Step: 9
Training loss: 2.3890812397003174
Validation loss: 2.1124325567676174

Epoch: 5| Step: 10
Training loss: 2.6217634677886963
Validation loss: 2.1110582159411524

Epoch: 210| Step: 0
Training loss: 2.3644328117370605
Validation loss: 2.129778737662941

Epoch: 5| Step: 1
Training loss: 2.031731605529785
Validation loss: 2.1491587802927983

Epoch: 5| Step: 2
Training loss: 2.4672160148620605
Validation loss: 2.167344367632302

Epoch: 5| Step: 3
Training loss: 1.7132294178009033
Validation loss: 2.1968061565071024

Epoch: 5| Step: 4
Training loss: 2.2149579524993896
Validation loss: 2.1833621327595045

Epoch: 5| Step: 5
Training loss: 2.3877148628234863
Validation loss: 2.1495589235777497

Epoch: 5| Step: 6
Training loss: 2.4890949726104736
Validation loss: 2.132592043569011

Epoch: 5| Step: 7
Training loss: 2.5990617275238037
Validation loss: 2.11659748067138

Epoch: 5| Step: 8
Training loss: 2.033813953399658
Validation loss: 2.101185203880392

Epoch: 5| Step: 9
Training loss: 1.9128644466400146
Validation loss: 2.0774955288056405

Epoch: 5| Step: 10
Training loss: 2.599210262298584
Validation loss: 2.0722813811353458

Epoch: 211| Step: 0
Training loss: 1.8438152074813843
Validation loss: 2.07516840452789

Epoch: 5| Step: 1
Training loss: 2.5745129585266113
Validation loss: 2.078323947486057

Epoch: 5| Step: 2
Training loss: 1.7667919397354126
Validation loss: 2.078246514002482

Epoch: 5| Step: 3
Training loss: 2.436281681060791
Validation loss: 2.096923435887983

Epoch: 5| Step: 4
Training loss: 2.257750988006592
Validation loss: 2.1069620193973666

Epoch: 5| Step: 5
Training loss: 2.666106700897217
Validation loss: 2.1047204643167476

Epoch: 5| Step: 6
Training loss: 2.6788065433502197
Validation loss: 2.0971557606932936

Epoch: 5| Step: 7
Training loss: 1.8639386892318726
Validation loss: 2.1157564475972164

Epoch: 5| Step: 8
Training loss: 1.9179168939590454
Validation loss: 2.113827295200799

Epoch: 5| Step: 9
Training loss: 2.1038477420806885
Validation loss: 2.1083570757219867

Epoch: 5| Step: 10
Training loss: 2.4556000232696533
Validation loss: 2.1261986711973786

Epoch: 212| Step: 0
Training loss: 2.0789742469787598
Validation loss: 2.114117355756862

Epoch: 5| Step: 1
Training loss: 2.3643622398376465
Validation loss: 2.1163990318134265

Epoch: 5| Step: 2
Training loss: 1.8325647115707397
Validation loss: 2.116656950725022

Epoch: 5| Step: 3
Training loss: 2.05623197555542
Validation loss: 2.1222632392760246

Epoch: 5| Step: 4
Training loss: 2.741520643234253
Validation loss: 2.109845656220631

Epoch: 5| Step: 5
Training loss: 2.156789779663086
Validation loss: 2.124031202767485

Epoch: 5| Step: 6
Training loss: 2.3047046661376953
Validation loss: 2.122937371653895

Epoch: 5| Step: 7
Training loss: 2.3888626098632812
Validation loss: 2.112257162729899

Epoch: 5| Step: 8
Training loss: 2.356670379638672
Validation loss: 2.0991132374732726

Epoch: 5| Step: 9
Training loss: 2.1520345211029053
Validation loss: 2.099770471613894

Epoch: 5| Step: 10
Training loss: 2.0351874828338623
Validation loss: 2.0961471501217095

Epoch: 213| Step: 0
Training loss: 1.7503621578216553
Validation loss: 2.0865283537936468

Epoch: 5| Step: 1
Training loss: 2.4002909660339355
Validation loss: 2.1008624056334138

Epoch: 5| Step: 2
Training loss: 2.0508148670196533
Validation loss: 2.09549351661436

Epoch: 5| Step: 3
Training loss: 2.328949451446533
Validation loss: 2.084766598157985

Epoch: 5| Step: 4
Training loss: 1.845001220703125
Validation loss: 2.104185870898667

Epoch: 5| Step: 5
Training loss: 2.6053481101989746
Validation loss: 2.119582783791327

Epoch: 5| Step: 6
Training loss: 2.4319796562194824
Validation loss: 2.1154023421707975

Epoch: 5| Step: 7
Training loss: 2.5343711376190186
Validation loss: 2.1346907461843183

Epoch: 5| Step: 8
Training loss: 2.2873635292053223
Validation loss: 2.132822293107228

Epoch: 5| Step: 9
Training loss: 2.328855276107788
Validation loss: 2.1353439002908687

Epoch: 5| Step: 10
Training loss: 1.8391669988632202
Validation loss: 2.1445289811780377

Epoch: 214| Step: 0
Training loss: 1.9167083501815796
Validation loss: 2.1536760048199723

Epoch: 5| Step: 1
Training loss: 2.4828433990478516
Validation loss: 2.1567349395444317

Epoch: 5| Step: 2
Training loss: 2.475484848022461
Validation loss: 2.1779154398108043

Epoch: 5| Step: 3
Training loss: 2.4882731437683105
Validation loss: 2.184904730448159

Epoch: 5| Step: 4
Training loss: 2.2100584506988525
Validation loss: 2.1707687480475313

Epoch: 5| Step: 5
Training loss: 1.4250683784484863
Validation loss: 2.1649039996567594

Epoch: 5| Step: 6
Training loss: 2.075505018234253
Validation loss: 2.137057291564121

Epoch: 5| Step: 7
Training loss: 2.677278518676758
Validation loss: 2.1282280029789096

Epoch: 5| Step: 8
Training loss: 2.495575428009033
Validation loss: 2.1205831214945805

Epoch: 5| Step: 9
Training loss: 1.7768182754516602
Validation loss: 2.12146278478766

Epoch: 5| Step: 10
Training loss: 2.5794267654418945
Validation loss: 2.120450383873396

Epoch: 215| Step: 0
Training loss: 2.374476671218872
Validation loss: 2.105476129439569

Epoch: 5| Step: 1
Training loss: 2.396746873855591
Validation loss: 2.0964234234184347

Epoch: 5| Step: 2
Training loss: 2.0864484310150146
Validation loss: 2.1102272913020146

Epoch: 5| Step: 3
Training loss: 1.972028374671936
Validation loss: 2.0972786282980316

Epoch: 5| Step: 4
Training loss: 2.1619837284088135
Validation loss: 2.1000707149505615

Epoch: 5| Step: 5
Training loss: 2.029815196990967
Validation loss: 2.1093402729239514

Epoch: 5| Step: 6
Training loss: 2.4243862628936768
Validation loss: 2.1092852802686792

Epoch: 5| Step: 7
Training loss: 2.855617046356201
Validation loss: 2.1403057113770516

Epoch: 5| Step: 8
Training loss: 2.6411778926849365
Validation loss: 2.1557632787253267

Epoch: 5| Step: 9
Training loss: 1.7028858661651611
Validation loss: 2.1743133785904094

Epoch: 5| Step: 10
Training loss: 2.005972385406494
Validation loss: 2.178498157890894

Epoch: 216| Step: 0
Training loss: 2.4422638416290283
Validation loss: 2.165997394951441

Epoch: 5| Step: 1
Training loss: 2.7239389419555664
Validation loss: 2.1708847348408034

Epoch: 5| Step: 2
Training loss: 2.678351879119873
Validation loss: 2.1738481906152542

Epoch: 5| Step: 3
Training loss: 1.9171249866485596
Validation loss: 2.158842058591945

Epoch: 5| Step: 4
Training loss: 2.0513575077056885
Validation loss: 2.143976983203683

Epoch: 5| Step: 5
Training loss: 2.1398487091064453
Validation loss: 2.132426820775514

Epoch: 5| Step: 6
Training loss: 1.8885958194732666
Validation loss: 2.1196464312973844

Epoch: 5| Step: 7
Training loss: 2.4364688396453857
Validation loss: 2.120922932060816

Epoch: 5| Step: 8
Training loss: 2.138012647628784
Validation loss: 2.1093235631142893

Epoch: 5| Step: 9
Training loss: 2.0863308906555176
Validation loss: 2.0970275863524406

Epoch: 5| Step: 10
Training loss: 1.9824607372283936
Validation loss: 2.0958561794732207

Epoch: 217| Step: 0
Training loss: 1.6832454204559326
Validation loss: 2.1063640489373157

Epoch: 5| Step: 1
Training loss: 2.0182502269744873
Validation loss: 2.108351895886083

Epoch: 5| Step: 2
Training loss: 1.9888532161712646
Validation loss: 2.0938893543776644

Epoch: 5| Step: 3
Training loss: 2.2264842987060547
Validation loss: 2.118506239306542

Epoch: 5| Step: 4
Training loss: 2.4906790256500244
Validation loss: 2.1077882141195317

Epoch: 5| Step: 5
Training loss: 2.4036803245544434
Validation loss: 2.098794787160812

Epoch: 5| Step: 6
Training loss: 2.4301905632019043
Validation loss: 2.0991531341306624

Epoch: 5| Step: 7
Training loss: 2.585452079772949
Validation loss: 2.1176742097382903

Epoch: 5| Step: 8
Training loss: 2.3203933238983154
Validation loss: 2.1167984995790707

Epoch: 5| Step: 9
Training loss: 2.5519731044769287
Validation loss: 2.098786647601794

Epoch: 5| Step: 10
Training loss: 1.640294075012207
Validation loss: 2.1066180454787387

Epoch: 218| Step: 0
Training loss: 2.4272117614746094
Validation loss: 2.110918652626776

Epoch: 5| Step: 1
Training loss: 1.8743293285369873
Validation loss: 2.1248106597572245

Epoch: 5| Step: 2
Training loss: 2.2851860523223877
Validation loss: 2.1114244512332383

Epoch: 5| Step: 3
Training loss: 1.9852861166000366
Validation loss: 2.1034069125370314

Epoch: 5| Step: 4
Training loss: 2.8136157989501953
Validation loss: 2.0973025342469573

Epoch: 5| Step: 5
Training loss: 2.1164469718933105
Validation loss: 2.097298588804019

Epoch: 5| Step: 6
Training loss: 1.7350122928619385
Validation loss: 2.11219004020896

Epoch: 5| Step: 7
Training loss: 2.6328728199005127
Validation loss: 2.1168505299475884

Epoch: 5| Step: 8
Training loss: 2.43094539642334
Validation loss: 2.1221095413290043

Epoch: 5| Step: 9
Training loss: 2.0391430854797363
Validation loss: 2.1227213336575415

Epoch: 5| Step: 10
Training loss: 1.8350226879119873
Validation loss: 2.1072331807946645

Epoch: 219| Step: 0
Training loss: 2.283817768096924
Validation loss: 2.1035398693494898

Epoch: 5| Step: 1
Training loss: 1.4181808233261108
Validation loss: 2.109094489005304

Epoch: 5| Step: 2
Training loss: 1.972887635231018
Validation loss: 2.103192001260737

Epoch: 5| Step: 3
Training loss: 2.34907603263855
Validation loss: 2.0952302153392504

Epoch: 5| Step: 4
Training loss: 2.3432679176330566
Validation loss: 2.0828926255626063

Epoch: 5| Step: 5
Training loss: 2.288914203643799
Validation loss: 2.08222714290824

Epoch: 5| Step: 6
Training loss: 2.373131036758423
Validation loss: 2.0885204627949703

Epoch: 5| Step: 7
Training loss: 2.2158050537109375
Validation loss: 2.1034931418716267

Epoch: 5| Step: 8
Training loss: 3.18290376663208
Validation loss: 2.1057003390404487

Epoch: 5| Step: 9
Training loss: 1.6841007471084595
Validation loss: 2.1294161081314087

Epoch: 5| Step: 10
Training loss: 2.2261698246002197
Validation loss: 2.129496519283582

Epoch: 220| Step: 0
Training loss: 2.555955410003662
Validation loss: 2.1413867960694017

Epoch: 5| Step: 1
Training loss: 2.645738363265991
Validation loss: 2.13781564722779

Epoch: 5| Step: 2
Training loss: 1.5052273273468018
Validation loss: 2.125731739946591

Epoch: 5| Step: 3
Training loss: 2.504075527191162
Validation loss: 2.1081205209096274

Epoch: 5| Step: 4
Training loss: 1.7893474102020264
Validation loss: 2.1059010118566532

Epoch: 5| Step: 5
Training loss: 1.9075275659561157
Validation loss: 2.111276493277601

Epoch: 5| Step: 6
Training loss: 2.1634411811828613
Validation loss: 2.093677641243063

Epoch: 5| Step: 7
Training loss: 2.0736749172210693
Validation loss: 2.087097429460095

Epoch: 5| Step: 8
Training loss: 2.609747886657715
Validation loss: 2.0824100381584576

Epoch: 5| Step: 9
Training loss: 2.924194812774658
Validation loss: 2.0948555623331377

Epoch: 5| Step: 10
Training loss: 1.7088594436645508
Validation loss: 2.0925157890524915

Epoch: 221| Step: 0
Training loss: 2.313657760620117
Validation loss: 2.0941200563984532

Epoch: 5| Step: 1
Training loss: 2.20930552482605
Validation loss: 2.087372162008798

Epoch: 5| Step: 2
Training loss: 2.173706293106079
Validation loss: 2.0954358987910773

Epoch: 5| Step: 3
Training loss: 1.9150673151016235
Validation loss: 2.1040324857158046

Epoch: 5| Step: 4
Training loss: 2.1465229988098145
Validation loss: 2.135170131601313

Epoch: 5| Step: 5
Training loss: 2.437561511993408
Validation loss: 2.1124064653150496

Epoch: 5| Step: 6
Training loss: 2.193746566772461
Validation loss: 2.115335766987134

Epoch: 5| Step: 7
Training loss: 2.446575403213501
Validation loss: 2.147723144100558

Epoch: 5| Step: 8
Training loss: 2.5277373790740967
Validation loss: 2.1511532183616393

Epoch: 5| Step: 9
Training loss: 1.7706104516983032
Validation loss: 2.1247547031730734

Epoch: 5| Step: 10
Training loss: 2.1370842456817627
Validation loss: 2.101371939464282

Epoch: 222| Step: 0
Training loss: 2.4315578937530518
Validation loss: 2.0923722610678723

Epoch: 5| Step: 1
Training loss: 2.368131160736084
Validation loss: 2.0934796794768302

Epoch: 5| Step: 2
Training loss: 1.9503281116485596
Validation loss: 2.109488324452472

Epoch: 5| Step: 3
Training loss: 1.817633867263794
Validation loss: 2.1043104766517557

Epoch: 5| Step: 4
Training loss: 2.8662281036376953
Validation loss: 2.1128982267072125

Epoch: 5| Step: 5
Training loss: 2.608459949493408
Validation loss: 2.1092198074504895

Epoch: 5| Step: 6
Training loss: 1.9793497323989868
Validation loss: 2.112256670510897

Epoch: 5| Step: 7
Training loss: 2.185180187225342
Validation loss: 2.105295696566182

Epoch: 5| Step: 8
Training loss: 2.417968273162842
Validation loss: 2.092555363972982

Epoch: 5| Step: 9
Training loss: 1.7000086307525635
Validation loss: 2.0919642294606855

Epoch: 5| Step: 10
Training loss: 2.202373743057251
Validation loss: 2.1037834088007608

Epoch: 223| Step: 0
Training loss: 2.344843626022339
Validation loss: 2.110361586334885

Epoch: 5| Step: 1
Training loss: 2.4451956748962402
Validation loss: 2.109914373326045

Epoch: 5| Step: 2
Training loss: 2.5937259197235107
Validation loss: 2.1002089797809558

Epoch: 5| Step: 3
Training loss: 2.574953079223633
Validation loss: 2.100183063937772

Epoch: 5| Step: 4
Training loss: 2.060347557067871
Validation loss: 2.082639016130919

Epoch: 5| Step: 5
Training loss: 2.223037004470825
Validation loss: 2.0730447230800504

Epoch: 5| Step: 6
Training loss: 1.5524038076400757
Validation loss: 2.092761101261262

Epoch: 5| Step: 7
Training loss: 2.0273818969726562
Validation loss: 2.1085284525348293

Epoch: 5| Step: 8
Training loss: 1.919455885887146
Validation loss: 2.110611484896752

Epoch: 5| Step: 9
Training loss: 2.7601566314697266
Validation loss: 2.12970821831816

Epoch: 5| Step: 10
Training loss: 1.7561075687408447
Validation loss: 2.131118974378032

Epoch: 224| Step: 0
Training loss: 2.8376097679138184
Validation loss: 2.137438092180478

Epoch: 5| Step: 1
Training loss: 1.6708030700683594
Validation loss: 2.1404267357241724

Epoch: 5| Step: 2
Training loss: 2.7434985637664795
Validation loss: 2.1610614048537387

Epoch: 5| Step: 3
Training loss: 1.514577865600586
Validation loss: 2.1427566671884186

Epoch: 5| Step: 4
Training loss: 1.8073428869247437
Validation loss: 2.1259035192510134

Epoch: 5| Step: 5
Training loss: 2.042092800140381
Validation loss: 2.0989098766798615

Epoch: 5| Step: 6
Training loss: 2.627145290374756
Validation loss: 2.099644127712455

Epoch: 5| Step: 7
Training loss: 2.5605921745300293
Validation loss: 2.0980632792236986

Epoch: 5| Step: 8
Training loss: 2.0510144233703613
Validation loss: 2.0979697935042845

Epoch: 5| Step: 9
Training loss: 2.071190357208252
Validation loss: 2.089357255607523

Epoch: 5| Step: 10
Training loss: 2.340567111968994
Validation loss: 2.1134422799592376

Epoch: 225| Step: 0
Training loss: 2.2903056144714355
Validation loss: 2.136767363035551

Epoch: 5| Step: 1
Training loss: 2.155243396759033
Validation loss: 2.1354626468432847

Epoch: 5| Step: 2
Training loss: 1.9342933893203735
Validation loss: 2.1609018528333275

Epoch: 5| Step: 3
Training loss: 2.1168129444122314
Validation loss: 2.126282920119583

Epoch: 5| Step: 4
Training loss: 2.175114870071411
Validation loss: 2.1214720869577057

Epoch: 5| Step: 5
Training loss: 2.416541337966919
Validation loss: 2.1055568443831576

Epoch: 5| Step: 6
Training loss: 1.754105806350708
Validation loss: 2.1239646352747434

Epoch: 5| Step: 7
Training loss: 2.688988208770752
Validation loss: 2.118247620521053

Epoch: 5| Step: 8
Training loss: 2.209317207336426
Validation loss: 2.104863469318677

Epoch: 5| Step: 9
Training loss: 2.185891628265381
Validation loss: 2.1049281640719344

Epoch: 5| Step: 10
Training loss: 2.295625925064087
Validation loss: 2.084819487346116

Epoch: 226| Step: 0
Training loss: 2.491020441055298
Validation loss: 2.0803235961544897

Epoch: 5| Step: 1
Training loss: 2.7042675018310547
Validation loss: 2.0681912873380925

Epoch: 5| Step: 2
Training loss: 2.3620989322662354
Validation loss: 2.0787383510220434

Epoch: 5| Step: 3
Training loss: 1.8237988948822021
Validation loss: 2.0677581961436937

Epoch: 5| Step: 4
Training loss: 1.7530152797698975
Validation loss: 2.0896350747795513

Epoch: 5| Step: 5
Training loss: 2.36860728263855
Validation loss: 2.1205889422406434

Epoch: 5| Step: 6
Training loss: 2.5240211486816406
Validation loss: 2.1201302364308345

Epoch: 5| Step: 7
Training loss: 1.828556776046753
Validation loss: 2.144539128067673

Epoch: 5| Step: 8
Training loss: 2.171988010406494
Validation loss: 2.155252866847541

Epoch: 5| Step: 9
Training loss: 2.126084566116333
Validation loss: 2.143985873909407

Epoch: 5| Step: 10
Training loss: 2.1031041145324707
Validation loss: 2.1394683571271997

Epoch: 227| Step: 0
Training loss: 3.081080913543701
Validation loss: 2.1126408423146894

Epoch: 5| Step: 1
Training loss: 1.4628709554672241
Validation loss: 2.092033838713041

Epoch: 5| Step: 2
Training loss: 1.941558837890625
Validation loss: 2.067295334672415

Epoch: 5| Step: 3
Training loss: 2.0167782306671143
Validation loss: 2.054936452578473

Epoch: 5| Step: 4
Training loss: 2.1542537212371826
Validation loss: 2.051878668928659

Epoch: 5| Step: 5
Training loss: 1.4432302713394165
Validation loss: 2.0475177098346014

Epoch: 5| Step: 6
Training loss: 2.473371982574463
Validation loss: 2.048647851072332

Epoch: 5| Step: 7
Training loss: 2.3056626319885254
Validation loss: 2.066754394961942

Epoch: 5| Step: 8
Training loss: 2.2273240089416504
Validation loss: 2.078616939565187

Epoch: 5| Step: 9
Training loss: 2.766199827194214
Validation loss: 2.096736696458632

Epoch: 5| Step: 10
Training loss: 2.769291639328003
Validation loss: 2.120411483190393

Epoch: 228| Step: 0
Training loss: 2.4055323600769043
Validation loss: 2.1178900836616434

Epoch: 5| Step: 1
Training loss: 1.9631980657577515
Validation loss: 2.154377738634745

Epoch: 5| Step: 2
Training loss: 2.928130626678467
Validation loss: 2.180893854428363

Epoch: 5| Step: 3
Training loss: 2.107983112335205
Validation loss: 2.161265283502558

Epoch: 5| Step: 4
Training loss: 1.394383430480957
Validation loss: 2.1354710466118267

Epoch: 5| Step: 5
Training loss: 2.037780284881592
Validation loss: 2.122058645371468

Epoch: 5| Step: 6
Training loss: 1.9464622735977173
Validation loss: 2.090549107520811

Epoch: 5| Step: 7
Training loss: 2.015761137008667
Validation loss: 2.0789764517097065

Epoch: 5| Step: 8
Training loss: 2.595337152481079
Validation loss: 2.080868767153832

Epoch: 5| Step: 9
Training loss: 2.633113145828247
Validation loss: 2.059067157007033

Epoch: 5| Step: 10
Training loss: 2.227308988571167
Validation loss: 2.072395150379468

Epoch: 229| Step: 0
Training loss: 2.5509440898895264
Validation loss: 2.065080850355087

Epoch: 5| Step: 1
Training loss: 1.9392436742782593
Validation loss: 2.071268455956572

Epoch: 5| Step: 2
Training loss: 2.3866195678710938
Validation loss: 2.069310821512694

Epoch: 5| Step: 3
Training loss: 2.0430994033813477
Validation loss: 2.088324098176854

Epoch: 5| Step: 4
Training loss: 2.2137603759765625
Validation loss: 2.100012489544448

Epoch: 5| Step: 5
Training loss: 2.28947114944458
Validation loss: 2.108877830607917

Epoch: 5| Step: 6
Training loss: 2.261737585067749
Validation loss: 2.1126867584002915

Epoch: 5| Step: 7
Training loss: 2.0264334678649902
Validation loss: 2.1230974082023866

Epoch: 5| Step: 8
Training loss: 2.004861831665039
Validation loss: 2.116792437850788

Epoch: 5| Step: 9
Training loss: 2.1558732986450195
Validation loss: 2.110737282742736

Epoch: 5| Step: 10
Training loss: 2.1636133193969727
Validation loss: 2.0947559110579954

Epoch: 230| Step: 0
Training loss: 2.4785609245300293
Validation loss: 2.089739473917151

Epoch: 5| Step: 1
Training loss: 1.9616073369979858
Validation loss: 2.0714257878641926

Epoch: 5| Step: 2
Training loss: 2.503892183303833
Validation loss: 2.0686250399517756

Epoch: 5| Step: 3
Training loss: 2.425896406173706
Validation loss: 2.0597952924748903

Epoch: 5| Step: 4
Training loss: 2.4905078411102295
Validation loss: 2.060029149055481

Epoch: 5| Step: 5
Training loss: 2.0215389728546143
Validation loss: 2.056006336724886

Epoch: 5| Step: 6
Training loss: 1.7654807567596436
Validation loss: 2.058260429290033

Epoch: 5| Step: 7
Training loss: 2.264066219329834
Validation loss: 2.0805352298162316

Epoch: 5| Step: 8
Training loss: 2.33481502532959
Validation loss: 2.0705567277887815

Epoch: 5| Step: 9
Training loss: 1.7538543939590454
Validation loss: 2.1032283280485418

Epoch: 5| Step: 10
Training loss: 2.204408645629883
Validation loss: 2.1044049493728147

Epoch: 231| Step: 0
Training loss: 2.2760281562805176
Validation loss: 2.135509847312845

Epoch: 5| Step: 1
Training loss: 2.2081987857818604
Validation loss: 2.1691290768243934

Epoch: 5| Step: 2
Training loss: 2.205651044845581
Validation loss: 2.191692175403718

Epoch: 5| Step: 3
Training loss: 2.0997824668884277
Validation loss: 2.1918496213933474

Epoch: 5| Step: 4
Training loss: 2.4107720851898193
Validation loss: 2.1943811575571694

Epoch: 5| Step: 5
Training loss: 1.8959085941314697
Validation loss: 2.1583989525354035

Epoch: 5| Step: 6
Training loss: 1.865265130996704
Validation loss: 2.132071425837855

Epoch: 5| Step: 7
Training loss: 2.22037672996521
Validation loss: 2.0979568778827624

Epoch: 5| Step: 8
Training loss: 2.6300930976867676
Validation loss: 2.0759106118191957

Epoch: 5| Step: 9
Training loss: 2.2081966400146484
Validation loss: 2.0732421336635465

Epoch: 5| Step: 10
Training loss: 2.209798574447632
Validation loss: 2.070975831759873

Epoch: 232| Step: 0
Training loss: 2.6570682525634766
Validation loss: 2.0606865036872124

Epoch: 5| Step: 1
Training loss: 2.0273966789245605
Validation loss: 2.063538351366597

Epoch: 5| Step: 2
Training loss: 2.5366222858428955
Validation loss: 2.056652315201298

Epoch: 5| Step: 3
Training loss: 2.3568644523620605
Validation loss: 2.0632165888304352

Epoch: 5| Step: 4
Training loss: 1.812904715538025
Validation loss: 2.0694112059890584

Epoch: 5| Step: 5
Training loss: 2.342632293701172
Validation loss: 2.0863944048522622

Epoch: 5| Step: 6
Training loss: 2.0002360343933105
Validation loss: 2.103983866271152

Epoch: 5| Step: 7
Training loss: 1.3064351081848145
Validation loss: 2.111739984122656

Epoch: 5| Step: 8
Training loss: 2.28583025932312
Validation loss: 2.149972220902802

Epoch: 5| Step: 9
Training loss: 2.350182294845581
Validation loss: 2.153199142025363

Epoch: 5| Step: 10
Training loss: 2.5433144569396973
Validation loss: 2.1383454286923973

Epoch: 233| Step: 0
Training loss: 1.8443979024887085
Validation loss: 2.1255576020927838

Epoch: 5| Step: 1
Training loss: 1.9466254711151123
Validation loss: 2.114828235359602

Epoch: 5| Step: 2
Training loss: 2.3601837158203125
Validation loss: 2.106621887094231

Epoch: 5| Step: 3
Training loss: 1.8311353921890259
Validation loss: 2.100047460166357

Epoch: 5| Step: 4
Training loss: 2.394845962524414
Validation loss: 2.1024514423903597

Epoch: 5| Step: 5
Training loss: 2.7094502449035645
Validation loss: 2.113568990461288

Epoch: 5| Step: 6
Training loss: 1.9193636178970337
Validation loss: 2.1050246441236107

Epoch: 5| Step: 7
Training loss: 2.007890462875366
Validation loss: 2.0996110618755384

Epoch: 5| Step: 8
Training loss: 2.27213978767395
Validation loss: 2.0750297051604076

Epoch: 5| Step: 9
Training loss: 2.2730135917663574
Validation loss: 2.06431572668014

Epoch: 5| Step: 10
Training loss: 2.4907171726226807
Validation loss: 2.0558835652566727

Epoch: 234| Step: 0
Training loss: 2.5168368816375732
Validation loss: 2.052151546683363

Epoch: 5| Step: 1
Training loss: 2.090517044067383
Validation loss: 2.0846016150648876

Epoch: 5| Step: 2
Training loss: 2.42372465133667
Validation loss: 2.09861090619077

Epoch: 5| Step: 3
Training loss: 2.0722594261169434
Validation loss: 2.097789905404532

Epoch: 5| Step: 4
Training loss: 1.6842715740203857
Validation loss: 2.106783168290251

Epoch: 5| Step: 5
Training loss: 2.3165695667266846
Validation loss: 2.0860385676865936

Epoch: 5| Step: 6
Training loss: 1.8722946643829346
Validation loss: 2.1037555817634828

Epoch: 5| Step: 7
Training loss: 2.3787765502929688
Validation loss: 2.1011314827908754

Epoch: 5| Step: 8
Training loss: 2.2250964641571045
Validation loss: 2.106572324229825

Epoch: 5| Step: 9
Training loss: 2.2312469482421875
Validation loss: 2.114665469815654

Epoch: 5| Step: 10
Training loss: 2.3166427612304688
Validation loss: 2.1397193183181105

Epoch: 235| Step: 0
Training loss: 1.9103904962539673
Validation loss: 2.1561660587146716

Epoch: 5| Step: 1
Training loss: 2.0666239261627197
Validation loss: 2.179069144751436

Epoch: 5| Step: 2
Training loss: 2.366565227508545
Validation loss: 2.1672486810274023

Epoch: 5| Step: 3
Training loss: 1.5938198566436768
Validation loss: 2.1601765335247083

Epoch: 5| Step: 4
Training loss: 2.3043322563171387
Validation loss: 2.145867173389722

Epoch: 5| Step: 5
Training loss: 2.2304062843322754
Validation loss: 2.1360818647569224

Epoch: 5| Step: 6
Training loss: 2.0948591232299805
Validation loss: 2.1306682555906233

Epoch: 5| Step: 7
Training loss: 2.377870559692383
Validation loss: 2.1257268126292894

Epoch: 5| Step: 8
Training loss: 2.489813804626465
Validation loss: 2.117721160252889

Epoch: 5| Step: 9
Training loss: 2.5422821044921875
Validation loss: 2.093098727605676

Epoch: 5| Step: 10
Training loss: 1.9341199398040771
Validation loss: 2.0789983734007804

Epoch: 236| Step: 0
Training loss: 2.016741991043091
Validation loss: 2.0690805758199384

Epoch: 5| Step: 1
Training loss: 1.7983367443084717
Validation loss: 2.0687548665590185

Epoch: 5| Step: 2
Training loss: 2.4857442378997803
Validation loss: 2.0667645251879128

Epoch: 5| Step: 3
Training loss: 2.336418867111206
Validation loss: 2.0583564619864188

Epoch: 5| Step: 4
Training loss: 2.162397861480713
Validation loss: 2.081462703725343

Epoch: 5| Step: 5
Training loss: 2.0750622749328613
Validation loss: 2.078398161036994

Epoch: 5| Step: 6
Training loss: 1.6901317834854126
Validation loss: 2.0866169955140803

Epoch: 5| Step: 7
Training loss: 2.1334245204925537
Validation loss: 2.1085709756420505

Epoch: 5| Step: 8
Training loss: 2.482212543487549
Validation loss: 2.115502183155347

Epoch: 5| Step: 9
Training loss: 2.375991106033325
Validation loss: 2.1362833117925994

Epoch: 5| Step: 10
Training loss: 2.3104217052459717
Validation loss: 2.122264433932561

Epoch: 237| Step: 0
Training loss: 2.3331055641174316
Validation loss: 2.1160372739197104

Epoch: 5| Step: 1
Training loss: 1.9171056747436523
Validation loss: 2.108836555993685

Epoch: 5| Step: 2
Training loss: 2.222459554672241
Validation loss: 2.0910283621921333

Epoch: 5| Step: 3
Training loss: 2.0315842628479004
Validation loss: 2.0721283446076098

Epoch: 5| Step: 4
Training loss: 1.9338172674179077
Validation loss: 2.0657807075849144

Epoch: 5| Step: 5
Training loss: 2.2548348903656006
Validation loss: 2.0900819224696003

Epoch: 5| Step: 6
Training loss: 2.7306058406829834
Validation loss: 2.0766555929696686

Epoch: 5| Step: 7
Training loss: 2.208463668823242
Validation loss: 2.0858733448930966

Epoch: 5| Step: 8
Training loss: 1.4872443675994873
Validation loss: 2.0845039454839562

Epoch: 5| Step: 9
Training loss: 2.3539140224456787
Validation loss: 2.0944188871691303

Epoch: 5| Step: 10
Training loss: 2.1480233669281006
Validation loss: 2.094792225027597

Epoch: 238| Step: 0
Training loss: 2.4306082725524902
Validation loss: 2.091958507414787

Epoch: 5| Step: 1
Training loss: 2.128187656402588
Validation loss: 2.1084058541123585

Epoch: 5| Step: 2
Training loss: 2.1462998390197754
Validation loss: 2.1141299637415076

Epoch: 5| Step: 3
Training loss: 1.8859180212020874
Validation loss: 2.1197472644108597

Epoch: 5| Step: 4
Training loss: 2.315749168395996
Validation loss: 2.102820914278748

Epoch: 5| Step: 5
Training loss: 2.3052995204925537
Validation loss: 2.0738558384679977

Epoch: 5| Step: 6
Training loss: 2.6161739826202393
Validation loss: 2.067911540308306

Epoch: 5| Step: 7
Training loss: 1.6737308502197266
Validation loss: 2.0675003579867783

Epoch: 5| Step: 8
Training loss: 2.2180893421173096
Validation loss: 2.056053328257735

Epoch: 5| Step: 9
Training loss: 2.013658285140991
Validation loss: 2.0573222637176514

Epoch: 5| Step: 10
Training loss: 1.9474846124649048
Validation loss: 2.056901429289131

Epoch: 239| Step: 0
Training loss: 1.999202013015747
Validation loss: 2.064026394198018

Epoch: 5| Step: 1
Training loss: 2.5238282680511475
Validation loss: 2.0557304082378263

Epoch: 5| Step: 2
Training loss: 1.696332335472107
Validation loss: 2.069655087686354

Epoch: 5| Step: 3
Training loss: 2.096564531326294
Validation loss: 2.0662690862532584

Epoch: 5| Step: 4
Training loss: 2.1784818172454834
Validation loss: 2.0880582832521006

Epoch: 5| Step: 5
Training loss: 2.4855711460113525
Validation loss: 2.077822103295275

Epoch: 5| Step: 6
Training loss: 2.219855785369873
Validation loss: 2.097977101161916

Epoch: 5| Step: 7
Training loss: 2.3914902210235596
Validation loss: 2.088677170456097

Epoch: 5| Step: 8
Training loss: 2.431575298309326
Validation loss: 2.1004222734000093

Epoch: 5| Step: 9
Training loss: 1.9480760097503662
Validation loss: 2.1125436059890257

Epoch: 5| Step: 10
Training loss: 1.4333810806274414
Validation loss: 2.131975467487048

Epoch: 240| Step: 0
Training loss: 2.3788697719573975
Validation loss: 2.1318070683428036

Epoch: 5| Step: 1
Training loss: 2.0100607872009277
Validation loss: 2.152435907753565

Epoch: 5| Step: 2
Training loss: 2.177232265472412
Validation loss: 2.1076492712061894

Epoch: 5| Step: 3
Training loss: 2.075772523880005
Validation loss: 2.1068707973726335

Epoch: 5| Step: 4
Training loss: 1.8098361492156982
Validation loss: 2.084596336528819

Epoch: 5| Step: 5
Training loss: 2.1602606773376465
Validation loss: 2.0813058883913103

Epoch: 5| Step: 6
Training loss: 2.4144647121429443
Validation loss: 2.072501633756904

Epoch: 5| Step: 7
Training loss: 2.95592999458313
Validation loss: 2.065379350416122

Epoch: 5| Step: 8
Training loss: 1.2012367248535156
Validation loss: 2.0545613970807803

Epoch: 5| Step: 9
Training loss: 2.0665276050567627
Validation loss: 2.0557201895662534

Epoch: 5| Step: 10
Training loss: 2.507681369781494
Validation loss: 2.0526172217502388

Epoch: 241| Step: 0
Training loss: 1.7294120788574219
Validation loss: 2.0623101188290502

Epoch: 5| Step: 1
Training loss: 2.6445679664611816
Validation loss: 2.064437318873662

Epoch: 5| Step: 2
Training loss: 1.4223029613494873
Validation loss: 2.076534337894891

Epoch: 5| Step: 3
Training loss: 2.2016775608062744
Validation loss: 2.1047675891589095

Epoch: 5| Step: 4
Training loss: 2.0681021213531494
Validation loss: 2.0990571898798787

Epoch: 5| Step: 5
Training loss: 2.8849775791168213
Validation loss: 2.104200432377477

Epoch: 5| Step: 6
Training loss: 1.9392484426498413
Validation loss: 2.109902802334037

Epoch: 5| Step: 7
Training loss: 2.125519037246704
Validation loss: 2.104330298721149

Epoch: 5| Step: 8
Training loss: 1.9725608825683594
Validation loss: 2.119756239716725

Epoch: 5| Step: 9
Training loss: 2.4776618480682373
Validation loss: 2.1218038374377834

Epoch: 5| Step: 10
Training loss: 2.2235019207000732
Validation loss: 2.112928899385596

Epoch: 242| Step: 0
Training loss: 2.2459731101989746
Validation loss: 2.1009570834457234

Epoch: 5| Step: 1
Training loss: 2.441694736480713
Validation loss: 2.080681380405221

Epoch: 5| Step: 2
Training loss: 2.2804956436157227
Validation loss: 2.0734462802128126

Epoch: 5| Step: 3
Training loss: 1.6554590463638306
Validation loss: 2.0514374497116252

Epoch: 5| Step: 4
Training loss: 1.8773601055145264
Validation loss: 2.0502974935757217

Epoch: 5| Step: 5
Training loss: 2.1940689086914062
Validation loss: 2.0547475558455273

Epoch: 5| Step: 6
Training loss: 2.291614532470703
Validation loss: 2.058321791310464

Epoch: 5| Step: 7
Training loss: 2.317587375640869
Validation loss: 2.0574611925309703

Epoch: 5| Step: 8
Training loss: 2.3690218925476074
Validation loss: 2.0710390870289137

Epoch: 5| Step: 9
Training loss: 1.934997797012329
Validation loss: 2.0970313818224016

Epoch: 5| Step: 10
Training loss: 2.0876381397247314
Validation loss: 2.1034986434444303

Epoch: 243| Step: 0
Training loss: 2.4015188217163086
Validation loss: 2.13371891103765

Epoch: 5| Step: 1
Training loss: 2.8071470260620117
Validation loss: 2.1507876675616027

Epoch: 5| Step: 2
Training loss: 2.15885591506958
Validation loss: 2.137158234914144

Epoch: 5| Step: 3
Training loss: 2.004040479660034
Validation loss: 2.11693912424067

Epoch: 5| Step: 4
Training loss: 2.2120578289031982
Validation loss: 2.1106841000177528

Epoch: 5| Step: 5
Training loss: 1.861797571182251
Validation loss: 2.09097122633329

Epoch: 5| Step: 6
Training loss: 2.39501690864563
Validation loss: 2.074543627359534

Epoch: 5| Step: 7
Training loss: 2.098560094833374
Validation loss: 2.0917791999796385

Epoch: 5| Step: 8
Training loss: 1.7924305200576782
Validation loss: 2.0839053046318794

Epoch: 5| Step: 9
Training loss: 2.177187204360962
Validation loss: 2.082568637786373

Epoch: 5| Step: 10
Training loss: 1.887939214706421
Validation loss: 2.10189284816865

Epoch: 244| Step: 0
Training loss: 1.9547319412231445
Validation loss: 2.116876317608741

Epoch: 5| Step: 1
Training loss: 1.9997615814208984
Validation loss: 2.109579259349454

Epoch: 5| Step: 2
Training loss: 2.3115344047546387
Validation loss: 2.1292916959331882

Epoch: 5| Step: 3
Training loss: 1.8889869451522827
Validation loss: 2.125797220455703

Epoch: 5| Step: 4
Training loss: 2.9168267250061035
Validation loss: 2.1129901896240892

Epoch: 5| Step: 5
Training loss: 2.4798665046691895
Validation loss: 2.113869913162724

Epoch: 5| Step: 6
Training loss: 1.8920116424560547
Validation loss: 2.0908693805817635

Epoch: 5| Step: 7
Training loss: 2.2291362285614014
Validation loss: 2.086785549758583

Epoch: 5| Step: 8
Training loss: 1.6397854089736938
Validation loss: 2.082106703071184

Epoch: 5| Step: 9
Training loss: 2.0221915245056152
Validation loss: 2.1016572252396615

Epoch: 5| Step: 10
Training loss: 2.3096821308135986
Validation loss: 2.093895868588519

Epoch: 245| Step: 0
Training loss: 2.0375759601593018
Validation loss: 2.0970481595685406

Epoch: 5| Step: 1
Training loss: 2.4324684143066406
Validation loss: 2.0968517065048218

Epoch: 5| Step: 2
Training loss: 1.8484913110733032
Validation loss: 2.0975059540041032

Epoch: 5| Step: 3
Training loss: 2.128969430923462
Validation loss: 2.0857631750004266

Epoch: 5| Step: 4
Training loss: 2.468982458114624
Validation loss: 2.0853452836313555

Epoch: 5| Step: 5
Training loss: 2.0024254322052
Validation loss: 2.075785744574762

Epoch: 5| Step: 6
Training loss: 2.4656758308410645
Validation loss: 2.089604708456224

Epoch: 5| Step: 7
Training loss: 2.179123640060425
Validation loss: 2.094122043219946

Epoch: 5| Step: 8
Training loss: 2.348235607147217
Validation loss: 2.0889802607156898

Epoch: 5| Step: 9
Training loss: 1.9640289545059204
Validation loss: 2.097621733142484

Epoch: 5| Step: 10
Training loss: 1.4842801094055176
Validation loss: 2.097285273254559

Epoch: 246| Step: 0
Training loss: 1.8649104833602905
Validation loss: 2.1338344030482794

Epoch: 5| Step: 1
Training loss: 2.5824952125549316
Validation loss: 2.1238500482292584

Epoch: 5| Step: 2
Training loss: 1.8970896005630493
Validation loss: 2.1125752618235927

Epoch: 5| Step: 3
Training loss: 2.8435420989990234
Validation loss: 2.0937807547148837

Epoch: 5| Step: 4
Training loss: 1.8115289211273193
Validation loss: 2.101832038612776

Epoch: 5| Step: 5
Training loss: 2.659217357635498
Validation loss: 2.091969382378363

Epoch: 5| Step: 6
Training loss: 1.6821578741073608
Validation loss: 2.1039934183961604

Epoch: 5| Step: 7
Training loss: 2.1209442615509033
Validation loss: 2.079587782582929

Epoch: 5| Step: 8
Training loss: 2.1089699268341064
Validation loss: 2.097134699103653

Epoch: 5| Step: 9
Training loss: 2.174940586090088
Validation loss: 2.0960819003402547

Epoch: 5| Step: 10
Training loss: 1.7614730596542358
Validation loss: 2.107090502656916

Epoch: 247| Step: 0
Training loss: 1.951866865158081
Validation loss: 2.1067761298148864

Epoch: 5| Step: 1
Training loss: 2.27522611618042
Validation loss: 2.0890313143371255

Epoch: 5| Step: 2
Training loss: 1.874367356300354
Validation loss: 2.0923434047288794

Epoch: 5| Step: 3
Training loss: 2.8656458854675293
Validation loss: 2.08372522682272

Epoch: 5| Step: 4
Training loss: 1.2516248226165771
Validation loss: 2.090948961114371

Epoch: 5| Step: 5
Training loss: 2.1895620822906494
Validation loss: 2.0839386473419848

Epoch: 5| Step: 6
Training loss: 2.4615261554718018
Validation loss: 2.0833709983415503

Epoch: 5| Step: 7
Training loss: 2.60148286819458
Validation loss: 2.0806114083977154

Epoch: 5| Step: 8
Training loss: 1.4842902421951294
Validation loss: 2.102904565872685

Epoch: 5| Step: 9
Training loss: 2.4700398445129395
Validation loss: 2.0974991167745283

Epoch: 5| Step: 10
Training loss: 2.0465188026428223
Validation loss: 2.1060044970563663

Epoch: 248| Step: 0
Training loss: 2.0995450019836426
Validation loss: 2.1079556711258425

Epoch: 5| Step: 1
Training loss: 2.1203880310058594
Validation loss: 2.1452162868233136

Epoch: 5| Step: 2
Training loss: 1.5463498830795288
Validation loss: 2.14842543807081

Epoch: 5| Step: 3
Training loss: 2.2789483070373535
Validation loss: 2.167772757109775

Epoch: 5| Step: 4
Training loss: 2.1711061000823975
Validation loss: 2.1601898900924192

Epoch: 5| Step: 5
Training loss: 2.153773307800293
Validation loss: 2.160868126858947

Epoch: 5| Step: 6
Training loss: 2.11779522895813
Validation loss: 2.149170652512581

Epoch: 5| Step: 7
Training loss: 2.158043384552002
Validation loss: 2.1374139029492616

Epoch: 5| Step: 8
Training loss: 2.5524983406066895
Validation loss: 2.1246060222707768

Epoch: 5| Step: 9
Training loss: 2.1124629974365234
Validation loss: 2.0978835705787904

Epoch: 5| Step: 10
Training loss: 2.16914439201355
Validation loss: 2.088134801515969

Epoch: 249| Step: 0
Training loss: 2.0599796772003174
Validation loss: 2.071620718125374

Epoch: 5| Step: 1
Training loss: 2.2502527236938477
Validation loss: 2.074179948017161

Epoch: 5| Step: 2
Training loss: 2.558004379272461
Validation loss: 2.044949423882269

Epoch: 5| Step: 3
Training loss: 1.4212943315505981
Validation loss: 2.060757666505793

Epoch: 5| Step: 4
Training loss: 2.836115598678589
Validation loss: 2.0603076052922074

Epoch: 5| Step: 5
Training loss: 1.8069713115692139
Validation loss: 2.0781703367028186

Epoch: 5| Step: 6
Training loss: 1.7563865184783936
Validation loss: 2.0958643395413636

Epoch: 5| Step: 7
Training loss: 2.472883701324463
Validation loss: 2.1286018035745107

Epoch: 5| Step: 8
Training loss: 2.2903892993927
Validation loss: 2.1468109417987127

Epoch: 5| Step: 9
Training loss: 2.109222888946533
Validation loss: 2.173108349564255

Epoch: 5| Step: 10
Training loss: 2.306607723236084
Validation loss: 2.124302787165488

Epoch: 250| Step: 0
Training loss: 1.709306001663208
Validation loss: 2.1085922666775283

Epoch: 5| Step: 1
Training loss: 2.1322779655456543
Validation loss: 2.1076837919091664

Epoch: 5| Step: 2
Training loss: 2.43196177482605
Validation loss: 2.062483436317854

Epoch: 5| Step: 3
Training loss: 2.1113791465759277
Validation loss: 2.0473873948538177

Epoch: 5| Step: 4
Training loss: 2.4416275024414062
Validation loss: 2.0665947737232333

Epoch: 5| Step: 5
Training loss: 2.430131435394287
Validation loss: 2.072566004209621

Epoch: 5| Step: 6
Training loss: 2.1658482551574707
Validation loss: 2.064212811890469

Epoch: 5| Step: 7
Training loss: 2.2403171062469482
Validation loss: 2.084962744866648

Epoch: 5| Step: 8
Training loss: 1.3762460947036743
Validation loss: 2.0915279798610236

Epoch: 5| Step: 9
Training loss: 2.1836280822753906
Validation loss: 2.100185017431936

Epoch: 5| Step: 10
Training loss: 2.279308557510376
Validation loss: 2.1056231837118826

Epoch: 251| Step: 0
Training loss: 2.318557024002075
Validation loss: 2.0904658584184546

Epoch: 5| Step: 1
Training loss: 2.2065865993499756
Validation loss: 2.086870854900729

Epoch: 5| Step: 2
Training loss: 1.9793561697006226
Validation loss: 2.0783033037698395

Epoch: 5| Step: 3
Training loss: 2.3723645210266113
Validation loss: 2.063948074976603

Epoch: 5| Step: 4
Training loss: 2.221212387084961
Validation loss: 2.0635294350244666

Epoch: 5| Step: 5
Training loss: 2.713285207748413
Validation loss: 2.0695879587563137

Epoch: 5| Step: 6
Training loss: 1.4825060367584229
Validation loss: 2.064434620641893

Epoch: 5| Step: 7
Training loss: 1.8100265264511108
Validation loss: 2.0698531443072903

Epoch: 5| Step: 8
Training loss: 2.199953556060791
Validation loss: 2.062559789226901

Epoch: 5| Step: 9
Training loss: 1.9701569080352783
Validation loss: 2.064511182487652

Epoch: 5| Step: 10
Training loss: 2.1334285736083984
Validation loss: 2.098328172519643

Epoch: 252| Step: 0
Training loss: 2.0436413288116455
Validation loss: 2.1250496167008595

Epoch: 5| Step: 1
Training loss: 2.569415807723999
Validation loss: 2.126512224956225

Epoch: 5| Step: 2
Training loss: 1.871294379234314
Validation loss: 2.1284408402699295

Epoch: 5| Step: 3
Training loss: 2.0433952808380127
Validation loss: 2.1100776272435344

Epoch: 5| Step: 4
Training loss: 2.3134636878967285
Validation loss: 2.0949351710657917

Epoch: 5| Step: 5
Training loss: 1.6811907291412354
Validation loss: 2.1032185169958297

Epoch: 5| Step: 6
Training loss: 2.3112916946411133
Validation loss: 2.0969778824877996

Epoch: 5| Step: 7
Training loss: 2.1330654621124268
Validation loss: 2.114393038134421

Epoch: 5| Step: 8
Training loss: 2.213261604309082
Validation loss: 2.115194482188071

Epoch: 5| Step: 9
Training loss: 1.924791932106018
Validation loss: 2.1065438485914663

Epoch: 5| Step: 10
Training loss: 2.1955769062042236
Validation loss: 2.0922417615049627

Epoch: 253| Step: 0
Training loss: 1.9740667343139648
Validation loss: 2.102024516751689

Epoch: 5| Step: 1
Training loss: 1.819593071937561
Validation loss: 2.0996587814823275

Epoch: 5| Step: 2
Training loss: 1.934104561805725
Validation loss: 2.117478291193644

Epoch: 5| Step: 3
Training loss: 2.1322195529937744
Validation loss: 2.11954144764972

Epoch: 5| Step: 4
Training loss: 1.994647741317749
Validation loss: 2.105127966532143

Epoch: 5| Step: 5
Training loss: 2.0523481369018555
Validation loss: 2.1098417697414273

Epoch: 5| Step: 6
Training loss: 2.578615427017212
Validation loss: 2.0833675476812545

Epoch: 5| Step: 7
Training loss: 2.062373399734497
Validation loss: 2.1066294485522854

Epoch: 5| Step: 8
Training loss: 2.2332088947296143
Validation loss: 2.102666990731352

Epoch: 5| Step: 9
Training loss: 2.119119167327881
Validation loss: 2.0997790982646327

Epoch: 5| Step: 10
Training loss: 2.4591262340545654
Validation loss: 2.0867668826092958

Epoch: 254| Step: 0
Training loss: 2.38972544670105
Validation loss: 2.0957053169127433

Epoch: 5| Step: 1
Training loss: 3.158489465713501
Validation loss: 2.0910446669465754

Epoch: 5| Step: 2
Training loss: 2.0731005668640137
Validation loss: 2.070206756232887

Epoch: 5| Step: 3
Training loss: 1.7111587524414062
Validation loss: 2.085369357498743

Epoch: 5| Step: 4
Training loss: 2.226435422897339
Validation loss: 2.105231403022684

Epoch: 5| Step: 5
Training loss: 2.0505356788635254
Validation loss: 2.111515034911453

Epoch: 5| Step: 6
Training loss: 2.0778968334198
Validation loss: 2.133779453974898

Epoch: 5| Step: 7
Training loss: 1.8598930835723877
Validation loss: 2.1305186697231826

Epoch: 5| Step: 8
Training loss: 2.4498095512390137
Validation loss: 2.1492552129171227

Epoch: 5| Step: 9
Training loss: 1.8783127069473267
Validation loss: 2.134520774246544

Epoch: 5| Step: 10
Training loss: 1.5126768350601196
Validation loss: 2.127546687279978

Epoch: 255| Step: 0
Training loss: 2.2690505981445312
Validation loss: 2.1030983155773533

Epoch: 5| Step: 1
Training loss: 2.220884084701538
Validation loss: 2.081975484407076

Epoch: 5| Step: 2
Training loss: 1.9777870178222656
Validation loss: 2.0844034264164586

Epoch: 5| Step: 3
Training loss: 1.7774906158447266
Validation loss: 2.077934503555298

Epoch: 5| Step: 4
Training loss: 2.0545248985290527
Validation loss: 2.0575718213153142

Epoch: 5| Step: 5
Training loss: 1.9958827495574951
Validation loss: 2.054430495026291

Epoch: 5| Step: 6
Training loss: 1.9882898330688477
Validation loss: 2.0518930278798586

Epoch: 5| Step: 7
Training loss: 2.041308879852295
Validation loss: 2.0499815428128807

Epoch: 5| Step: 8
Training loss: 2.27223539352417
Validation loss: 2.0671367696536485

Epoch: 5| Step: 9
Training loss: 2.383427619934082
Validation loss: 2.083065640541815

Epoch: 5| Step: 10
Training loss: 2.5859615802764893
Validation loss: 2.1152933592437417

Epoch: 256| Step: 0
Training loss: 1.8330923318862915
Validation loss: 2.137297467518878

Epoch: 5| Step: 1
Training loss: 1.9240548610687256
Validation loss: 2.1108788751786753

Epoch: 5| Step: 2
Training loss: 2.605868101119995
Validation loss: 2.1249212987961306

Epoch: 5| Step: 3
Training loss: 2.1533589363098145
Validation loss: 2.107510083465166

Epoch: 5| Step: 4
Training loss: 2.4162509441375732
Validation loss: 2.103996956220237

Epoch: 5| Step: 5
Training loss: 2.4649150371551514
Validation loss: 2.1107270615075224

Epoch: 5| Step: 6
Training loss: 1.3953824043273926
Validation loss: 2.101131021335561

Epoch: 5| Step: 7
Training loss: 2.0010509490966797
Validation loss: 2.1140640102406985

Epoch: 5| Step: 8
Training loss: 1.9559307098388672
Validation loss: 2.116476138432821

Epoch: 5| Step: 9
Training loss: 2.224090099334717
Validation loss: 2.0904987806914956

Epoch: 5| Step: 10
Training loss: 2.604403257369995
Validation loss: 2.0975249326357277

Epoch: 257| Step: 0
Training loss: 2.389509916305542
Validation loss: 2.085908874388664

Epoch: 5| Step: 1
Training loss: 2.112071990966797
Validation loss: 2.0963942209879556

Epoch: 5| Step: 2
Training loss: 2.145718812942505
Validation loss: 2.092842919852144

Epoch: 5| Step: 3
Training loss: 2.103698253631592
Validation loss: 2.093441042848813

Epoch: 5| Step: 4
Training loss: 2.0518605709075928
Validation loss: 2.086366054832294

Epoch: 5| Step: 5
Training loss: 2.3330464363098145
Validation loss: 2.0783355223235263

Epoch: 5| Step: 6
Training loss: 2.2508606910705566
Validation loss: 2.0776886132455643

Epoch: 5| Step: 7
Training loss: 1.7420142889022827
Validation loss: 2.0934266736430507

Epoch: 5| Step: 8
Training loss: 2.2331929206848145
Validation loss: 2.1102482682915142

Epoch: 5| Step: 9
Training loss: 1.3451647758483887
Validation loss: 2.1116622148021573

Epoch: 5| Step: 10
Training loss: 2.771354913711548
Validation loss: 2.1633309471991753

Epoch: 258| Step: 0
Training loss: 2.6308228969573975
Validation loss: 2.151702839841125

Epoch: 5| Step: 1
Training loss: 2.201167106628418
Validation loss: 2.1492309403675858

Epoch: 5| Step: 2
Training loss: 1.6951549053192139
Validation loss: 2.1636015804865028

Epoch: 5| Step: 3
Training loss: 1.9600293636322021
Validation loss: 2.1293425457451933

Epoch: 5| Step: 4
Training loss: 2.134082317352295
Validation loss: 2.132768231053506

Epoch: 5| Step: 5
Training loss: 2.2821249961853027
Validation loss: 2.124949052769651

Epoch: 5| Step: 6
Training loss: 2.2668280601501465
Validation loss: 2.1069267334476596

Epoch: 5| Step: 7
Training loss: 1.949135422706604
Validation loss: 2.099411432461072

Epoch: 5| Step: 8
Training loss: 2.4320120811462402
Validation loss: 2.091658471733011

Epoch: 5| Step: 9
Training loss: 1.9859838485717773
Validation loss: 2.0822960907413113

Epoch: 5| Step: 10
Training loss: 1.6297627687454224
Validation loss: 2.09132025190579

Epoch: 259| Step: 0
Training loss: 2.58290433883667
Validation loss: 2.075880596714635

Epoch: 5| Step: 1
Training loss: 1.7771581411361694
Validation loss: 2.0845700361395396

Epoch: 5| Step: 2
Training loss: 2.4466071128845215
Validation loss: 2.100152066958848

Epoch: 5| Step: 3
Training loss: 2.200970411300659
Validation loss: 2.0952703696425243

Epoch: 5| Step: 4
Training loss: 2.123609781265259
Validation loss: 2.0916776221285582

Epoch: 5| Step: 5
Training loss: 2.695103883743286
Validation loss: 2.1070680054285194

Epoch: 5| Step: 6
Training loss: 1.520753264427185
Validation loss: 2.1192294320752545

Epoch: 5| Step: 7
Training loss: 1.9441038370132446
Validation loss: 2.1144331655194684

Epoch: 5| Step: 8
Training loss: 1.9855884313583374
Validation loss: 2.09418402435959

Epoch: 5| Step: 9
Training loss: 1.7754729986190796
Validation loss: 2.0925166478721042

Epoch: 5| Step: 10
Training loss: 2.225853204727173
Validation loss: 2.0758974808518604

Epoch: 260| Step: 0
Training loss: 2.367644786834717
Validation loss: 2.103299290903153

Epoch: 5| Step: 1
Training loss: 2.247724771499634
Validation loss: 2.1020062713212866

Epoch: 5| Step: 2
Training loss: 2.147581100463867
Validation loss: 2.115810504523657

Epoch: 5| Step: 3
Training loss: 2.406526565551758
Validation loss: 2.1343374765047463

Epoch: 5| Step: 4
Training loss: 2.1914284229278564
Validation loss: 2.1550367955238587

Epoch: 5| Step: 5
Training loss: 2.319117784500122
Validation loss: 2.1576272992677588

Epoch: 5| Step: 6
Training loss: 1.5927457809448242
Validation loss: 2.135774766245196

Epoch: 5| Step: 7
Training loss: 2.0835299491882324
Validation loss: 2.1149285634358725

Epoch: 5| Step: 8
Training loss: 2.4424891471862793
Validation loss: 2.0914812613559026

Epoch: 5| Step: 9
Training loss: 1.7255346775054932
Validation loss: 2.0836498980881064

Epoch: 5| Step: 10
Training loss: 1.790508508682251
Validation loss: 2.0685184540287143

Epoch: 261| Step: 0
Training loss: 1.8785957098007202
Validation loss: 2.068373595514605

Epoch: 5| Step: 1
Training loss: 1.8709278106689453
Validation loss: 2.0685631818668817

Epoch: 5| Step: 2
Training loss: 1.9800469875335693
Validation loss: 2.0636699225312922

Epoch: 5| Step: 3
Training loss: 1.8595737218856812
Validation loss: 2.057231012211051

Epoch: 5| Step: 4
Training loss: 2.253556966781616
Validation loss: 2.081658594069942

Epoch: 5| Step: 5
Training loss: 2.6356353759765625
Validation loss: 2.110914147028359

Epoch: 5| Step: 6
Training loss: 2.791656017303467
Validation loss: 2.127851252914757

Epoch: 5| Step: 7
Training loss: 1.8297008275985718
Validation loss: 2.120071589305837

Epoch: 5| Step: 8
Training loss: 2.272458553314209
Validation loss: 2.1301773209725656

Epoch: 5| Step: 9
Training loss: 2.1657698154449463
Validation loss: 2.1489186543290333

Epoch: 5| Step: 10
Training loss: 1.5068210363388062
Validation loss: 2.1281268545376357

Epoch: 262| Step: 0
Training loss: 2.0130646228790283
Validation loss: 2.1242149465827533

Epoch: 5| Step: 1
Training loss: 2.042942523956299
Validation loss: 2.097569327200613

Epoch: 5| Step: 2
Training loss: 1.8507452011108398
Validation loss: 2.1086345808480376

Epoch: 5| Step: 3
Training loss: 2.389779567718506
Validation loss: 2.1031589636238675

Epoch: 5| Step: 4
Training loss: 1.346484899520874
Validation loss: 2.0906583724483365

Epoch: 5| Step: 5
Training loss: 2.504164218902588
Validation loss: 2.0923071907412623

Epoch: 5| Step: 6
Training loss: 1.9800727367401123
Validation loss: 2.097200137312694

Epoch: 5| Step: 7
Training loss: 2.0342719554901123
Validation loss: 2.08260787686994

Epoch: 5| Step: 8
Training loss: 2.6892120838165283
Validation loss: 2.0691567749105473

Epoch: 5| Step: 9
Training loss: 1.7013870477676392
Validation loss: 2.072642905737764

Epoch: 5| Step: 10
Training loss: 2.6290202140808105
Validation loss: 2.073752659623341

Epoch: 263| Step: 0
Training loss: 2.4072341918945312
Validation loss: 2.0594358803123556

Epoch: 5| Step: 1
Training loss: 1.9715499877929688
Validation loss: 2.084279106509301

Epoch: 5| Step: 2
Training loss: 1.7043977975845337
Validation loss: 2.06815109714385

Epoch: 5| Step: 3
Training loss: 2.426027297973633
Validation loss: 2.089798991398145

Epoch: 5| Step: 4
Training loss: 2.132861852645874
Validation loss: 2.0875955717537993

Epoch: 5| Step: 5
Training loss: 2.0323855876922607
Validation loss: 2.0751204618843655

Epoch: 5| Step: 6
Training loss: 2.128411054611206
Validation loss: 2.0745466473282024

Epoch: 5| Step: 7
Training loss: 1.848328948020935
Validation loss: 2.0841459330692085

Epoch: 5| Step: 8
Training loss: 1.836187720298767
Validation loss: 2.109790370028506

Epoch: 5| Step: 9
Training loss: 2.6762242317199707
Validation loss: 2.093673062580888

Epoch: 5| Step: 10
Training loss: 1.771405577659607
Validation loss: 2.101491497408959

Epoch: 264| Step: 0
Training loss: 2.4117519855499268
Validation loss: 2.1169733514067945

Epoch: 5| Step: 1
Training loss: 1.8379653692245483
Validation loss: 2.1234825016349874

Epoch: 5| Step: 2
Training loss: 2.1798582077026367
Validation loss: 2.1105105107830417

Epoch: 5| Step: 3
Training loss: 2.054337739944458
Validation loss: 2.099182164797219

Epoch: 5| Step: 4
Training loss: 2.4384846687316895
Validation loss: 2.1194317084486767

Epoch: 5| Step: 5
Training loss: 1.6850614547729492
Validation loss: 2.1186575261495446

Epoch: 5| Step: 6
Training loss: 2.462125778198242
Validation loss: 2.122301801558464

Epoch: 5| Step: 7
Training loss: 2.8911373615264893
Validation loss: 2.088860173379221

Epoch: 5| Step: 8
Training loss: 1.8409630060195923
Validation loss: 2.076962891445365

Epoch: 5| Step: 9
Training loss: 1.493079423904419
Validation loss: 2.0714650538659867

Epoch: 5| Step: 10
Training loss: 1.722523808479309
Validation loss: 2.0903094455760014

Epoch: 265| Step: 0
Training loss: 1.4703915119171143
Validation loss: 2.0862043416628273

Epoch: 5| Step: 1
Training loss: 1.3885308504104614
Validation loss: 2.0964519669932704

Epoch: 5| Step: 2
Training loss: 2.476802349090576
Validation loss: 2.099093790977232

Epoch: 5| Step: 3
Training loss: 2.662749767303467
Validation loss: 2.1223864555358887

Epoch: 5| Step: 4
Training loss: 2.758457899093628
Validation loss: 2.1243469907391455

Epoch: 5| Step: 5
Training loss: 1.6905038356781006
Validation loss: 2.121319217066611

Epoch: 5| Step: 6
Training loss: 2.36147403717041
Validation loss: 2.1520593550897416

Epoch: 5| Step: 7
Training loss: 1.7241125106811523
Validation loss: 2.1564023007628736

Epoch: 5| Step: 8
Training loss: 2.193795680999756
Validation loss: 2.15495260941085

Epoch: 5| Step: 9
Training loss: 2.3034071922302246
Validation loss: 2.1526198335873183

Epoch: 5| Step: 10
Training loss: 2.066221237182617
Validation loss: 2.103103860732048

Epoch: 266| Step: 0
Training loss: 1.9606704711914062
Validation loss: 2.0770717077357794

Epoch: 5| Step: 1
Training loss: 2.3068456649780273
Validation loss: 2.055699210013113

Epoch: 5| Step: 2
Training loss: 2.017815113067627
Validation loss: 2.0615303977843253

Epoch: 5| Step: 3
Training loss: 2.2258999347686768
Validation loss: 2.0565976071101364

Epoch: 5| Step: 4
Training loss: 1.5402953624725342
Validation loss: 2.045437538495628

Epoch: 5| Step: 5
Training loss: 2.072172164916992
Validation loss: 2.050484493214597

Epoch: 5| Step: 6
Training loss: 2.1693291664123535
Validation loss: 2.056851981788553

Epoch: 5| Step: 7
Training loss: 1.9921720027923584
Validation loss: 2.0642994808894333

Epoch: 5| Step: 8
Training loss: 2.3141298294067383
Validation loss: 2.0729001683573567

Epoch: 5| Step: 9
Training loss: 1.9000965356826782
Validation loss: 2.0956335157476444

Epoch: 5| Step: 10
Training loss: 2.6844289302825928
Validation loss: 2.109555505937146

Epoch: 267| Step: 0
Training loss: 1.8297497034072876
Validation loss: 2.1321054376581663

Epoch: 5| Step: 1
Training loss: 1.5823848247528076
Validation loss: 2.163642043708473

Epoch: 5| Step: 2
Training loss: 2.372298002243042
Validation loss: 2.137761254464426

Epoch: 5| Step: 3
Training loss: 2.251023530960083
Validation loss: 2.147774286167596

Epoch: 5| Step: 4
Training loss: 2.222353219985962
Validation loss: 2.122041766361524

Epoch: 5| Step: 5
Training loss: 2.455551862716675
Validation loss: 2.1008537636008313

Epoch: 5| Step: 6
Training loss: 1.5641601085662842
Validation loss: 2.0953264197995587

Epoch: 5| Step: 7
Training loss: 2.588977336883545
Validation loss: 2.08328083766404

Epoch: 5| Step: 8
Training loss: 1.8825361728668213
Validation loss: 2.0886980589999946

Epoch: 5| Step: 9
Training loss: 2.225130558013916
Validation loss: 2.0740646662250644

Epoch: 5| Step: 10
Training loss: 2.2322838306427
Validation loss: 2.077792365063903

Epoch: 268| Step: 0
Training loss: 1.9475390911102295
Validation loss: 2.0776475244952786

Epoch: 5| Step: 1
Training loss: 2.334585666656494
Validation loss: 2.059082661905596

Epoch: 5| Step: 2
Training loss: 1.4703201055526733
Validation loss: 2.0542814026596727

Epoch: 5| Step: 3
Training loss: 2.3919949531555176
Validation loss: 2.0463165198602984

Epoch: 5| Step: 4
Training loss: 1.4066534042358398
Validation loss: 2.0744119921038227

Epoch: 5| Step: 5
Training loss: 1.825187087059021
Validation loss: 2.090702751631378

Epoch: 5| Step: 6
Training loss: 2.6901297569274902
Validation loss: 2.104204889266722

Epoch: 5| Step: 7
Training loss: 2.3498215675354004
Validation loss: 2.1344576317776918

Epoch: 5| Step: 8
Training loss: 2.464285373687744
Validation loss: 2.143506993529617

Epoch: 5| Step: 9
Training loss: 2.1125967502593994
Validation loss: 2.1289621117294475

Epoch: 5| Step: 10
Training loss: 2.076045274734497
Validation loss: 2.125668357777339

Epoch: 269| Step: 0
Training loss: 1.6559569835662842
Validation loss: 2.1280627173762166

Epoch: 5| Step: 1
Training loss: 1.7543690204620361
Validation loss: 2.111949020816434

Epoch: 5| Step: 2
Training loss: 2.063997507095337
Validation loss: 2.121192145091231

Epoch: 5| Step: 3
Training loss: 2.4622902870178223
Validation loss: 2.0989610033650554

Epoch: 5| Step: 4
Training loss: 2.148906707763672
Validation loss: 2.1007291373386177

Epoch: 5| Step: 5
Training loss: 2.0724453926086426
Validation loss: 2.0792015957575973

Epoch: 5| Step: 6
Training loss: 1.9573322534561157
Validation loss: 2.0898065349107147

Epoch: 5| Step: 7
Training loss: 1.6541221141815186
Validation loss: 2.0811848409714235

Epoch: 5| Step: 8
Training loss: 2.4861769676208496
Validation loss: 2.0718660739160355

Epoch: 5| Step: 9
Training loss: 2.9898569583892822
Validation loss: 2.0697659113073863

Epoch: 5| Step: 10
Training loss: 1.6396491527557373
Validation loss: 2.057827691878042

Epoch: 270| Step: 0
Training loss: 2.1888883113861084
Validation loss: 2.0527340109630297

Epoch: 5| Step: 1
Training loss: 2.261735200881958
Validation loss: 2.0424413296484176

Epoch: 5| Step: 2
Training loss: 2.050703287124634
Validation loss: 2.0479712152993805

Epoch: 5| Step: 3
Training loss: 2.0688657760620117
Validation loss: 2.059476174334044

Epoch: 5| Step: 4
Training loss: 2.3530924320220947
Validation loss: 2.0677369948356383

Epoch: 5| Step: 5
Training loss: 2.2481963634490967
Validation loss: 2.0755119785185783

Epoch: 5| Step: 6
Training loss: 1.902003288269043
Validation loss: 2.0711264559017715

Epoch: 5| Step: 7
Training loss: 2.21724271774292
Validation loss: 2.09657347074119

Epoch: 5| Step: 8
Training loss: 2.4684338569641113
Validation loss: 2.0978120808960288

Epoch: 5| Step: 9
Training loss: 1.2671772241592407
Validation loss: 2.094388234999872

Epoch: 5| Step: 10
Training loss: 2.006391763687134
Validation loss: 2.098036148214853

Epoch: 271| Step: 0
Training loss: 1.9224882125854492
Validation loss: 2.099164650004397

Epoch: 5| Step: 1
Training loss: 2.4431257247924805
Validation loss: 2.104611817226615

Epoch: 5| Step: 2
Training loss: 2.107593297958374
Validation loss: 2.1018332922330467

Epoch: 5| Step: 3
Training loss: 1.8162221908569336
Validation loss: 2.127548087027765

Epoch: 5| Step: 4
Training loss: 2.424001693725586
Validation loss: 2.1110611333641955

Epoch: 5| Step: 5
Training loss: 2.3572349548339844
Validation loss: 2.1030971721936296

Epoch: 5| Step: 6
Training loss: 1.91645085811615
Validation loss: 2.1256871454177366

Epoch: 5| Step: 7
Training loss: 2.259990930557251
Validation loss: 2.104812246496959

Epoch: 5| Step: 8
Training loss: 1.8176755905151367
Validation loss: 2.098708325816739

Epoch: 5| Step: 9
Training loss: 2.063119888305664
Validation loss: 2.1035741657339115

Epoch: 5| Step: 10
Training loss: 1.7547439336776733
Validation loss: 2.0957052566671885

Epoch: 272| Step: 0
Training loss: 2.5758697986602783
Validation loss: 2.105693483865389

Epoch: 5| Step: 1
Training loss: 1.9501224756240845
Validation loss: 2.08760130277244

Epoch: 5| Step: 2
Training loss: 1.896501898765564
Validation loss: 2.0960229109692317

Epoch: 5| Step: 3
Training loss: 2.2510108947753906
Validation loss: 2.091549486242315

Epoch: 5| Step: 4
Training loss: 2.6837589740753174
Validation loss: 2.108180893364773

Epoch: 5| Step: 5
Training loss: 1.8179343938827515
Validation loss: 2.1035383221923665

Epoch: 5| Step: 6
Training loss: 2.1042063236236572
Validation loss: 2.108877969044511

Epoch: 5| Step: 7
Training loss: 1.2799574136734009
Validation loss: 2.1114714453297276

Epoch: 5| Step: 8
Training loss: 2.389303684234619
Validation loss: 2.0862131375138477

Epoch: 5| Step: 9
Training loss: 2.165748119354248
Validation loss: 2.0780995353575675

Epoch: 5| Step: 10
Training loss: 1.6927299499511719
Validation loss: 2.072804635570895

Epoch: 273| Step: 0
Training loss: 1.887444257736206
Validation loss: 2.0721068433535996

Epoch: 5| Step: 1
Training loss: 2.2886810302734375
Validation loss: 2.081506093343099

Epoch: 5| Step: 2
Training loss: 1.8185046911239624
Validation loss: 2.0733464328191613

Epoch: 5| Step: 3
Training loss: 2.697291374206543
Validation loss: 2.0891355955472557

Epoch: 5| Step: 4
Training loss: 2.6112592220306396
Validation loss: 2.0776962823765253

Epoch: 5| Step: 5
Training loss: 2.098653793334961
Validation loss: 2.076039934671053

Epoch: 5| Step: 6
Training loss: 1.6264493465423584
Validation loss: 2.0793361048544607

Epoch: 5| Step: 7
Training loss: 1.5327597856521606
Validation loss: 2.0924090672564764

Epoch: 5| Step: 8
Training loss: 1.7812846899032593
Validation loss: 2.1102476568632227

Epoch: 5| Step: 9
Training loss: 2.5988476276397705
Validation loss: 2.1195049978071645

Epoch: 5| Step: 10
Training loss: 2.051240921020508
Validation loss: 2.128350216855285

Epoch: 274| Step: 0
Training loss: 2.4323642253875732
Validation loss: 2.119022310421031

Epoch: 5| Step: 1
Training loss: 1.6121355295181274
Validation loss: 2.119231116387152

Epoch: 5| Step: 2
Training loss: 2.2585301399230957
Validation loss: 2.113898520828575

Epoch: 5| Step: 3
Training loss: 2.0500597953796387
Validation loss: 2.094787633547219

Epoch: 5| Step: 4
Training loss: 2.5465307235717773
Validation loss: 2.0963703932300692

Epoch: 5| Step: 5
Training loss: 2.1221141815185547
Validation loss: 2.0848415231191986

Epoch: 5| Step: 6
Training loss: 2.025630235671997
Validation loss: 2.0706319744868944

Epoch: 5| Step: 7
Training loss: 1.7736780643463135
Validation loss: 2.0494134336389522

Epoch: 5| Step: 8
Training loss: 1.8401308059692383
Validation loss: 2.038759213621898

Epoch: 5| Step: 9
Training loss: 2.170660972595215
Validation loss: 2.0425922819363174

Epoch: 5| Step: 10
Training loss: 1.9262217283248901
Validation loss: 2.0362144439451155

Epoch: 275| Step: 0
Training loss: 1.8028247356414795
Validation loss: 2.0346195620875203

Epoch: 5| Step: 1
Training loss: 2.1572012901306152
Validation loss: 2.0500704255155338

Epoch: 5| Step: 2
Training loss: 2.061893939971924
Validation loss: 2.0527791054018083

Epoch: 5| Step: 3
Training loss: 2.320751667022705
Validation loss: 2.079447586049316

Epoch: 5| Step: 4
Training loss: 2.210217237472534
Validation loss: 2.104480351171186

Epoch: 5| Step: 5
Training loss: 2.6287708282470703
Validation loss: 2.119031439545334

Epoch: 5| Step: 6
Training loss: 1.8600127696990967
Validation loss: 2.1486126927919287

Epoch: 5| Step: 7
Training loss: 2.356797695159912
Validation loss: 2.141625032630018

Epoch: 5| Step: 8
Training loss: 1.849802017211914
Validation loss: 2.125134330923839

Epoch: 5| Step: 9
Training loss: 1.779016137123108
Validation loss: 2.0861566387197024

Epoch: 5| Step: 10
Training loss: 2.014951705932617
Validation loss: 2.076010496385636

Epoch: 276| Step: 0
Training loss: 2.374157667160034
Validation loss: 2.05876947218372

Epoch: 5| Step: 1
Training loss: 1.5541616678237915
Validation loss: 2.0570382213079803

Epoch: 5| Step: 2
Training loss: 1.9426933526992798
Validation loss: 2.0617109652488463

Epoch: 5| Step: 3
Training loss: 2.0730631351470947
Validation loss: 2.0544268546565885

Epoch: 5| Step: 4
Training loss: 2.7960004806518555
Validation loss: 2.0658346683748308

Epoch: 5| Step: 5
Training loss: 2.210846185684204
Validation loss: 2.075865800662707

Epoch: 5| Step: 6
Training loss: 1.961709976196289
Validation loss: 2.0753597290285173

Epoch: 5| Step: 7
Training loss: 2.145364284515381
Validation loss: 2.086174211194438

Epoch: 5| Step: 8
Training loss: 1.8040657043457031
Validation loss: 2.0928197906863306

Epoch: 5| Step: 9
Training loss: 2.2842860221862793
Validation loss: 2.100114535259944

Epoch: 5| Step: 10
Training loss: 1.607314109802246
Validation loss: 2.089629893661827

Epoch: 277| Step: 0
Training loss: 2.5503482818603516
Validation loss: 2.1078750189914497

Epoch: 5| Step: 1
Training loss: 2.172506809234619
Validation loss: 2.102808401148806

Epoch: 5| Step: 2
Training loss: 1.4939464330673218
Validation loss: 2.1300599190496627

Epoch: 5| Step: 3
Training loss: 2.3353006839752197
Validation loss: 2.1182424073578208

Epoch: 5| Step: 4
Training loss: 2.092931032180786
Validation loss: 2.115972964994369

Epoch: 5| Step: 5
Training loss: 2.202564239501953
Validation loss: 2.1061351760741203

Epoch: 5| Step: 6
Training loss: 2.154113292694092
Validation loss: 2.0987941116415043

Epoch: 5| Step: 7
Training loss: 2.160501003265381
Validation loss: 2.132925139960422

Epoch: 5| Step: 8
Training loss: 1.5971143245697021
Validation loss: 2.101528847089378

Epoch: 5| Step: 9
Training loss: 2.220961570739746
Validation loss: 2.0953699183720413

Epoch: 5| Step: 10
Training loss: 1.8874293565750122
Validation loss: 2.065173233709028

Epoch: 278| Step: 0
Training loss: 1.766519546508789
Validation loss: 2.0591173120724258

Epoch: 5| Step: 1
Training loss: 2.260763645172119
Validation loss: 2.050424098968506

Epoch: 5| Step: 2
Training loss: 2.4825470447540283
Validation loss: 2.0832352381880566

Epoch: 5| Step: 3
Training loss: 1.462446928024292
Validation loss: 2.0762353251057286

Epoch: 5| Step: 4
Training loss: 2.46494460105896
Validation loss: 2.0796556088232223

Epoch: 5| Step: 5
Training loss: 1.7791564464569092
Validation loss: 2.085101865953015

Epoch: 5| Step: 6
Training loss: 2.2617239952087402
Validation loss: 2.084545902026597

Epoch: 5| Step: 7
Training loss: 2.382766008377075
Validation loss: 2.092455579388526

Epoch: 5| Step: 8
Training loss: 1.7852054834365845
Validation loss: 2.0930866579855643

Epoch: 5| Step: 9
Training loss: 2.083014726638794
Validation loss: 2.0786153693352976

Epoch: 5| Step: 10
Training loss: 1.892328143119812
Validation loss: 2.0919386571453464

Epoch: 279| Step: 0
Training loss: 2.0987648963928223
Validation loss: 2.0861451036186627

Epoch: 5| Step: 1
Training loss: 2.3631749153137207
Validation loss: 2.1034911319773686

Epoch: 5| Step: 2
Training loss: 1.8294169902801514
Validation loss: 2.1020426519455446

Epoch: 5| Step: 3
Training loss: 2.2872843742370605
Validation loss: 2.1114467561885877

Epoch: 5| Step: 4
Training loss: 1.4260472059249878
Validation loss: 2.0971923438451623

Epoch: 5| Step: 5
Training loss: 2.6026034355163574
Validation loss: 2.118519793274582

Epoch: 5| Step: 6
Training loss: 1.6789146661758423
Validation loss: 2.094882054995465

Epoch: 5| Step: 7
Training loss: 1.9621212482452393
Validation loss: 2.0828326235535326

Epoch: 5| Step: 8
Training loss: 2.1366961002349854
Validation loss: 2.06464635684926

Epoch: 5| Step: 9
Training loss: 1.806253433227539
Validation loss: 2.050530784873552

Epoch: 5| Step: 10
Training loss: 2.5480387210845947
Validation loss: 2.047745702087238

Epoch: 280| Step: 0
Training loss: 1.8455219268798828
Validation loss: 2.051955856302733

Epoch: 5| Step: 1
Training loss: 2.00189208984375
Validation loss: 2.0743070802380963

Epoch: 5| Step: 2
Training loss: 1.639446496963501
Validation loss: 2.0665983128291305

Epoch: 5| Step: 3
Training loss: 1.6021270751953125
Validation loss: 2.068104831121301

Epoch: 5| Step: 4
Training loss: 1.6611350774765015
Validation loss: 2.0658774593824982

Epoch: 5| Step: 5
Training loss: 1.989640235900879
Validation loss: 2.080215759174798

Epoch: 5| Step: 6
Training loss: 2.032632827758789
Validation loss: 2.0800055419245074

Epoch: 5| Step: 7
Training loss: 2.7030551433563232
Validation loss: 2.077638623534992

Epoch: 5| Step: 8
Training loss: 2.8121891021728516
Validation loss: 2.0969441731770835

Epoch: 5| Step: 9
Training loss: 2.331874132156372
Validation loss: 2.1016699921700264

Epoch: 5| Step: 10
Training loss: 2.01914644241333
Validation loss: 2.101844326142342

Epoch: 281| Step: 0
Training loss: 2.855180263519287
Validation loss: 2.089705264696511

Epoch: 5| Step: 1
Training loss: 1.9509456157684326
Validation loss: 2.0963064470598773

Epoch: 5| Step: 2
Training loss: 2.507864236831665
Validation loss: 2.085699414694181

Epoch: 5| Step: 3
Training loss: 2.3804564476013184
Validation loss: 2.071799175713652

Epoch: 5| Step: 4
Training loss: 1.5043039321899414
Validation loss: 2.0596862454568186

Epoch: 5| Step: 5
Training loss: 1.9619516134262085
Validation loss: 2.0608896042710994

Epoch: 5| Step: 6
Training loss: 1.7276512384414673
Validation loss: 2.072482652561639

Epoch: 5| Step: 7
Training loss: 1.667104959487915
Validation loss: 2.064244931743991

Epoch: 5| Step: 8
Training loss: 1.7481130361557007
Validation loss: 2.054371254418486

Epoch: 5| Step: 9
Training loss: 2.435997247695923
Validation loss: 2.0660298114181845

Epoch: 5| Step: 10
Training loss: 1.8655754327774048
Validation loss: 2.0783824100289294

Epoch: 282| Step: 0
Training loss: 1.8857616186141968
Validation loss: 2.081573917019752

Epoch: 5| Step: 1
Training loss: 2.210235118865967
Validation loss: 2.0862409350692586

Epoch: 5| Step: 2
Training loss: 2.333434581756592
Validation loss: 2.1108601862384426

Epoch: 5| Step: 3
Training loss: 1.9768950939178467
Validation loss: 2.1188454320353847

Epoch: 5| Step: 4
Training loss: 2.033085346221924
Validation loss: 2.1439542847294963

Epoch: 5| Step: 5
Training loss: 1.2446831464767456
Validation loss: 2.122887344770534

Epoch: 5| Step: 6
Training loss: 2.128347396850586
Validation loss: 2.1434438382425616

Epoch: 5| Step: 7
Training loss: 2.5486857891082764
Validation loss: 2.1452232330076155

Epoch: 5| Step: 8
Training loss: 1.853647232055664
Validation loss: 2.1274862468883557

Epoch: 5| Step: 9
Training loss: 1.9530607461929321
Validation loss: 2.0995221291818926

Epoch: 5| Step: 10
Training loss: 2.481729507446289
Validation loss: 2.0854025553631526

Epoch: 283| Step: 0
Training loss: 1.8206636905670166
Validation loss: 2.059616611849877

Epoch: 5| Step: 1
Training loss: 2.4310765266418457
Validation loss: 2.0499704883944605

Epoch: 5| Step: 2
Training loss: 1.9991004467010498
Validation loss: 2.0360292850002164

Epoch: 5| Step: 3
Training loss: 1.9283912181854248
Validation loss: 2.0382733857759865

Epoch: 5| Step: 4
Training loss: 1.7228796482086182
Validation loss: 2.031054719801872

Epoch: 5| Step: 5
Training loss: 2.0953140258789062
Validation loss: 2.0259063154138546

Epoch: 5| Step: 6
Training loss: 2.055529832839966
Validation loss: 2.0458165291816957

Epoch: 5| Step: 7
Training loss: 2.517728805541992
Validation loss: 2.074785481217087

Epoch: 5| Step: 8
Training loss: 2.445485830307007
Validation loss: 2.092969218889872

Epoch: 5| Step: 9
Training loss: 2.3178675174713135
Validation loss: 2.110533304111932

Epoch: 5| Step: 10
Training loss: 1.4045438766479492
Validation loss: 2.1116757495428926

Epoch: 284| Step: 0
Training loss: 1.5907751321792603
Validation loss: 2.1194740828647407

Epoch: 5| Step: 1
Training loss: 2.4266231060028076
Validation loss: 2.1354936835586384

Epoch: 5| Step: 2
Training loss: 2.031879186630249
Validation loss: 2.140477116389941

Epoch: 5| Step: 3
Training loss: 2.3942337036132812
Validation loss: 2.1439586031821465

Epoch: 5| Step: 4
Training loss: 2.5385074615478516
Validation loss: 2.12622655335293

Epoch: 5| Step: 5
Training loss: 2.42903470993042
Validation loss: 2.1016669927104825

Epoch: 5| Step: 6
Training loss: 1.6665170192718506
Validation loss: 2.1005067235680035

Epoch: 5| Step: 7
Training loss: 2.1048834323883057
Validation loss: 2.0699603621677687

Epoch: 5| Step: 8
Training loss: 1.5712169408798218
Validation loss: 2.0632575570896106

Epoch: 5| Step: 9
Training loss: 2.111431837081909
Validation loss: 2.0559462962612027

Epoch: 5| Step: 10
Training loss: 1.686664342880249
Validation loss: 2.046419815350604

Epoch: 285| Step: 0
Training loss: 2.188960313796997
Validation loss: 2.055127274605536

Epoch: 5| Step: 1
Training loss: 2.3652913570404053
Validation loss: 2.0381813203134844

Epoch: 5| Step: 2
Training loss: 2.35634446144104
Validation loss: 2.0428330565011628

Epoch: 5| Step: 3
Training loss: 2.2476813793182373
Validation loss: 2.05017194824834

Epoch: 5| Step: 4
Training loss: 1.9407342672348022
Validation loss: 2.075315701064243

Epoch: 5| Step: 5
Training loss: 1.8373677730560303
Validation loss: 2.0695546339916926

Epoch: 5| Step: 6
Training loss: 1.8542674779891968
Validation loss: 2.0802747664913053

Epoch: 5| Step: 7
Training loss: 1.8119564056396484
Validation loss: 2.0941039323806763

Epoch: 5| Step: 8
Training loss: 2.3955092430114746
Validation loss: 2.1245966983097855

Epoch: 5| Step: 9
Training loss: 2.254566192626953
Validation loss: 2.1350159388716503

Epoch: 5| Step: 10
Training loss: 1.2897940874099731
Validation loss: 2.1176454982449933

Epoch: 286| Step: 0
Training loss: 2.2472217082977295
Validation loss: 2.102711804451481

Epoch: 5| Step: 1
Training loss: 1.7609342336654663
Validation loss: 2.1033762065313195

Epoch: 5| Step: 2
Training loss: 2.802241802215576
Validation loss: 2.1118703067943616

Epoch: 5| Step: 3
Training loss: 2.1085753440856934
Validation loss: 2.097759421153735

Epoch: 5| Step: 4
Training loss: 2.4032998085021973
Validation loss: 2.109297085833806

Epoch: 5| Step: 5
Training loss: 1.9541553258895874
Validation loss: 2.092876931672455

Epoch: 5| Step: 6
Training loss: 1.8105058670043945
Validation loss: 2.065556054474205

Epoch: 5| Step: 7
Training loss: 2.055996894836426
Validation loss: 2.062601066404773

Epoch: 5| Step: 8
Training loss: 2.024462938308716
Validation loss: 2.0670790826120684

Epoch: 5| Step: 9
Training loss: 1.9458633661270142
Validation loss: 2.0824577987834973

Epoch: 5| Step: 10
Training loss: 1.6608744859695435
Validation loss: 2.101463367862086

Epoch: 287| Step: 0
Training loss: 2.020127534866333
Validation loss: 2.0975813122205835

Epoch: 5| Step: 1
Training loss: 1.8868408203125
Validation loss: 2.0737767322089082

Epoch: 5| Step: 2
Training loss: 2.051990270614624
Validation loss: 2.095462683708437

Epoch: 5| Step: 3
Training loss: 2.506711006164551
Validation loss: 2.0884567819615847

Epoch: 5| Step: 4
Training loss: 2.3742613792419434
Validation loss: 2.070835005852484

Epoch: 5| Step: 5
Training loss: 1.717725396156311
Validation loss: 2.0856081619057605

Epoch: 5| Step: 6
Training loss: 1.4087855815887451
Validation loss: 2.0865604108379734

Epoch: 5| Step: 7
Training loss: 2.308401107788086
Validation loss: 2.0957483091662006

Epoch: 5| Step: 8
Training loss: 1.8600387573242188
Validation loss: 2.0920565641054543

Epoch: 5| Step: 9
Training loss: 2.2102267742156982
Validation loss: 2.0728543022627473

Epoch: 5| Step: 10
Training loss: 2.02181339263916
Validation loss: 2.0625474350426787

Epoch: 288| Step: 0
Training loss: 1.91593337059021
Validation loss: 2.0552185402121594

Epoch: 5| Step: 1
Training loss: 2.204789638519287
Validation loss: 2.0464984242634108

Epoch: 5| Step: 2
Training loss: 2.1096372604370117
Validation loss: 2.045619697980983

Epoch: 5| Step: 3
Training loss: 1.914899230003357
Validation loss: 2.0430524503031084

Epoch: 5| Step: 4
Training loss: 2.046441078186035
Validation loss: 2.040502611026969

Epoch: 5| Step: 5
Training loss: 2.5488014221191406
Validation loss: 2.0625077678311254

Epoch: 5| Step: 6
Training loss: 2.7280967235565186
Validation loss: 2.0728860721793225

Epoch: 5| Step: 7
Training loss: 1.7684749364852905
Validation loss: 2.0967567556647846

Epoch: 5| Step: 8
Training loss: 2.016798734664917
Validation loss: 2.11759852850309

Epoch: 5| Step: 9
Training loss: 1.8276779651641846
Validation loss: 2.133282846020114

Epoch: 5| Step: 10
Training loss: 1.396226167678833
Validation loss: 2.1145327270671888

Epoch: 289| Step: 0
Training loss: 1.7826862335205078
Validation loss: 2.121099260545546

Epoch: 5| Step: 1
Training loss: 1.594844937324524
Validation loss: 2.125118322269891

Epoch: 5| Step: 2
Training loss: 2.6164650917053223
Validation loss: 2.1065129926127772

Epoch: 5| Step: 3
Training loss: 2.5315303802490234
Validation loss: 2.109017677204583

Epoch: 5| Step: 4
Training loss: 1.8448035717010498
Validation loss: 2.0723124780962543

Epoch: 5| Step: 5
Training loss: 2.309203624725342
Validation loss: 2.054503212692917

Epoch: 5| Step: 6
Training loss: 1.9259426593780518
Validation loss: 2.0609879852623068

Epoch: 5| Step: 7
Training loss: 1.8288719654083252
Validation loss: 2.066605865314443

Epoch: 5| Step: 8
Training loss: 1.7894766330718994
Validation loss: 2.0710918364986295

Epoch: 5| Step: 9
Training loss: 2.3069968223571777
Validation loss: 2.073868251615955

Epoch: 5| Step: 10
Training loss: 1.8557093143463135
Validation loss: 2.0611408179806125

Epoch: 290| Step: 0
Training loss: 2.1892924308776855
Validation loss: 2.0722051076991583

Epoch: 5| Step: 1
Training loss: 1.7569830417633057
Validation loss: 2.0696543211578042

Epoch: 5| Step: 2
Training loss: 1.3660552501678467
Validation loss: 2.087178058521722

Epoch: 5| Step: 3
Training loss: 2.088263750076294
Validation loss: 2.098382330709888

Epoch: 5| Step: 4
Training loss: 1.7291427850723267
Validation loss: 2.1081893777334564

Epoch: 5| Step: 5
Training loss: 1.9612817764282227
Validation loss: 2.123079171744726

Epoch: 5| Step: 6
Training loss: 2.670626640319824
Validation loss: 2.1084330620304232

Epoch: 5| Step: 7
Training loss: 2.14914608001709
Validation loss: 2.1148291813429965

Epoch: 5| Step: 8
Training loss: 1.9856983423233032
Validation loss: 2.088103977582788

Epoch: 5| Step: 9
Training loss: 2.026273488998413
Validation loss: 2.0860335275691044

Epoch: 5| Step: 10
Training loss: 2.464693069458008
Validation loss: 2.0622365718246787

Epoch: 291| Step: 0
Training loss: 2.3706936836242676
Validation loss: 2.0754176109067854

Epoch: 5| Step: 1
Training loss: 2.160865068435669
Validation loss: 2.069996718437441

Epoch: 5| Step: 2
Training loss: 2.161208391189575
Validation loss: 2.0703382351065196

Epoch: 5| Step: 3
Training loss: 2.2135376930236816
Validation loss: 2.054399703138618

Epoch: 5| Step: 4
Training loss: 1.5490974187850952
Validation loss: 2.0483203126538183

Epoch: 5| Step: 5
Training loss: 1.985823392868042
Validation loss: 2.0617346097064275

Epoch: 5| Step: 6
Training loss: 2.0906455516815186
Validation loss: 2.0717041800099034

Epoch: 5| Step: 7
Training loss: 1.6133426427841187
Validation loss: 2.0797010724262526

Epoch: 5| Step: 8
Training loss: 1.9843194484710693
Validation loss: 2.096270722727622

Epoch: 5| Step: 9
Training loss: 1.8053077459335327
Validation loss: 2.0803863822772937

Epoch: 5| Step: 10
Training loss: 2.6379878520965576
Validation loss: 2.0824710604965047

Epoch: 292| Step: 0
Training loss: 2.197974920272827
Validation loss: 2.096559123326373

Epoch: 5| Step: 1
Training loss: 1.9811216592788696
Validation loss: 2.1037337036542993

Epoch: 5| Step: 2
Training loss: 2.348848342895508
Validation loss: 2.1149087887938305

Epoch: 5| Step: 3
Training loss: 1.904820203781128
Validation loss: 2.115332009971783

Epoch: 5| Step: 4
Training loss: 2.1886425018310547
Validation loss: 2.130431716160108

Epoch: 5| Step: 5
Training loss: 2.075009346008301
Validation loss: 2.0904142023414694

Epoch: 5| Step: 6
Training loss: 1.8828990459442139
Validation loss: 2.0810346577757146

Epoch: 5| Step: 7
Training loss: 1.4862353801727295
Validation loss: 2.085013549814942

Epoch: 5| Step: 8
Training loss: 2.287277936935425
Validation loss: 2.0677968737899617

Epoch: 5| Step: 9
Training loss: 1.6524326801300049
Validation loss: 2.067111002501621

Epoch: 5| Step: 10
Training loss: 2.2975149154663086
Validation loss: 2.0578224300056376

Epoch: 293| Step: 0
Training loss: 2.2458138465881348
Validation loss: 2.0657037509384977

Epoch: 5| Step: 1
Training loss: 2.2665441036224365
Validation loss: 2.057619248667071

Epoch: 5| Step: 2
Training loss: 2.0816826820373535
Validation loss: 2.078847986395641

Epoch: 5| Step: 3
Training loss: 1.9774421453475952
Validation loss: 2.0903607850433676

Epoch: 5| Step: 4
Training loss: 2.226083278656006
Validation loss: 2.092194422598808

Epoch: 5| Step: 5
Training loss: 1.1748307943344116
Validation loss: 2.0917985670028196

Epoch: 5| Step: 6
Training loss: 1.7655441761016846
Validation loss: 2.121402612296484

Epoch: 5| Step: 7
Training loss: 2.122873544692993
Validation loss: 2.1180462298854703

Epoch: 5| Step: 8
Training loss: 2.4804794788360596
Validation loss: 2.118946244639735

Epoch: 5| Step: 9
Training loss: 2.184654712677002
Validation loss: 2.125263855021487

Epoch: 5| Step: 10
Training loss: 1.771196722984314
Validation loss: 2.09198586915129

Epoch: 294| Step: 0
Training loss: 1.7810767889022827
Validation loss: 2.0620839095884755

Epoch: 5| Step: 1
Training loss: 1.8866260051727295
Validation loss: 2.0577293595960064

Epoch: 5| Step: 2
Training loss: 1.5551795959472656
Validation loss: 2.0544290670784573

Epoch: 5| Step: 3
Training loss: 1.8392343521118164
Validation loss: 2.0355017928666967

Epoch: 5| Step: 4
Training loss: 2.5423343181610107
Validation loss: 2.062877752447641

Epoch: 5| Step: 5
Training loss: 1.6355574131011963
Validation loss: 2.0730707132688133

Epoch: 5| Step: 6
Training loss: 1.8761522769927979
Validation loss: 2.078817775172572

Epoch: 5| Step: 7
Training loss: 2.4964418411254883
Validation loss: 2.083121904762842

Epoch: 5| Step: 8
Training loss: 2.282806158065796
Validation loss: 2.1090787969609743

Epoch: 5| Step: 9
Training loss: 2.010547161102295
Validation loss: 2.1109389874242965

Epoch: 5| Step: 10
Training loss: 2.4641034603118896
Validation loss: 2.1337947127639607

Epoch: 295| Step: 0
Training loss: 2.247023820877075
Validation loss: 2.11709794434168

Epoch: 5| Step: 1
Training loss: 1.8847334384918213
Validation loss: 2.102988302066762

Epoch: 5| Step: 2
Training loss: 1.876165747642517
Validation loss: 2.0862010832755797

Epoch: 5| Step: 3
Training loss: 1.8411762714385986
Validation loss: 2.075314624335176

Epoch: 5| Step: 4
Training loss: 1.8092620372772217
Validation loss: 2.0669006480965564

Epoch: 5| Step: 5
Training loss: 1.8371570110321045
Validation loss: 2.068504500132735

Epoch: 5| Step: 6
Training loss: 2.289370536804199
Validation loss: 2.06743097818026

Epoch: 5| Step: 7
Training loss: 2.2001497745513916
Validation loss: 2.0643541723169307

Epoch: 5| Step: 8
Training loss: 2.03023099899292
Validation loss: 2.0728729283937843

Epoch: 5| Step: 9
Training loss: 2.5613791942596436
Validation loss: 2.0622989772468485

Epoch: 5| Step: 10
Training loss: 1.4951995611190796
Validation loss: 2.075314824299146

Epoch: 296| Step: 0
Training loss: 1.6018213033676147
Validation loss: 2.0677111738471576

Epoch: 5| Step: 1
Training loss: 2.059638500213623
Validation loss: 2.0721654379239647

Epoch: 5| Step: 2
Training loss: 1.959151029586792
Validation loss: 2.0738737070432274

Epoch: 5| Step: 3
Training loss: 2.559748649597168
Validation loss: 2.0897172202346144

Epoch: 5| Step: 4
Training loss: 1.9082286357879639
Validation loss: 2.121355152899219

Epoch: 5| Step: 5
Training loss: 2.0774853229522705
Validation loss: 2.096738807616695

Epoch: 5| Step: 6
Training loss: 2.291043758392334
Validation loss: 2.094010337706535

Epoch: 5| Step: 7
Training loss: 1.9312912225723267
Validation loss: 2.078257732493903

Epoch: 5| Step: 8
Training loss: 1.8935158252716064
Validation loss: 2.061658490088678

Epoch: 5| Step: 9
Training loss: 2.2440102100372314
Validation loss: 2.0617653605758504

Epoch: 5| Step: 10
Training loss: 1.6864615678787231
Validation loss: 2.049150497682633

Epoch: 297| Step: 0
Training loss: 1.827328085899353
Validation loss: 2.0515697758684874

Epoch: 5| Step: 1
Training loss: 1.305116891860962
Validation loss: 2.0603890406188143

Epoch: 5| Step: 2
Training loss: 2.601416826248169
Validation loss: 2.0550142757354246

Epoch: 5| Step: 3
Training loss: 1.8369020223617554
Validation loss: 2.050895724245297

Epoch: 5| Step: 4
Training loss: 2.285773277282715
Validation loss: 2.070265740476629

Epoch: 5| Step: 5
Training loss: 2.082127809524536
Validation loss: 2.0691331163529427

Epoch: 5| Step: 6
Training loss: 1.6795021295547485
Validation loss: 2.081460460539787

Epoch: 5| Step: 7
Training loss: 2.105783224105835
Validation loss: 2.063848249373897

Epoch: 5| Step: 8
Training loss: 1.9997018575668335
Validation loss: 2.069779690875802

Epoch: 5| Step: 9
Training loss: 2.0529849529266357
Validation loss: 2.0635591988922446

Epoch: 5| Step: 10
Training loss: 2.4780898094177246
Validation loss: 2.0615965102308538

Epoch: 298| Step: 0
Training loss: 2.143068313598633
Validation loss: 2.0628060243463002

Epoch: 5| Step: 1
Training loss: 1.6963183879852295
Validation loss: 2.068375100371658

Epoch: 5| Step: 2
Training loss: 2.0230178833007812
Validation loss: 2.0739302045555523

Epoch: 5| Step: 3
Training loss: 1.3717457056045532
Validation loss: 2.0683958094607116

Epoch: 5| Step: 4
Training loss: 2.3794593811035156
Validation loss: 2.0824689429293395

Epoch: 5| Step: 5
Training loss: 2.115974187850952
Validation loss: 2.1016009110276417

Epoch: 5| Step: 6
Training loss: 1.7001413106918335
Validation loss: 2.091573012772427

Epoch: 5| Step: 7
Training loss: 1.9502058029174805
Validation loss: 2.091932771026447

Epoch: 5| Step: 8
Training loss: 2.165454387664795
Validation loss: 2.060764152516601

Epoch: 5| Step: 9
Training loss: 2.6922943592071533
Validation loss: 2.0553275154482935

Epoch: 5| Step: 10
Training loss: 1.7778581380844116
Validation loss: 2.061440888271537

Epoch: 299| Step: 0
Training loss: 2.258756160736084
Validation loss: 2.044293475407426

Epoch: 5| Step: 1
Training loss: 2.2467098236083984
Validation loss: 2.0502800787648847

Epoch: 5| Step: 2
Training loss: 1.88985276222229
Validation loss: 2.047974496759394

Epoch: 5| Step: 3
Training loss: 2.1278436183929443
Validation loss: 2.0423932742047053

Epoch: 5| Step: 4
Training loss: 2.059997081756592
Validation loss: 2.0635866298470447

Epoch: 5| Step: 5
Training loss: 2.3446266651153564
Validation loss: 2.0643473709783247

Epoch: 5| Step: 6
Training loss: 1.6082627773284912
Validation loss: 2.1063862231469925

Epoch: 5| Step: 7
Training loss: 1.9389997720718384
Validation loss: 2.0980204407886793

Epoch: 5| Step: 8
Training loss: 1.9523261785507202
Validation loss: 2.1028115236631004

Epoch: 5| Step: 9
Training loss: 2.308927536010742
Validation loss: 2.0944463758058447

Epoch: 5| Step: 10
Training loss: 1.540376901626587
Validation loss: 2.045277485283472

Epoch: 300| Step: 0
Training loss: 2.0479888916015625
Validation loss: 2.0405224036144953

Epoch: 5| Step: 1
Training loss: 2.074019193649292
Validation loss: 2.0295818672385266

Epoch: 5| Step: 2
Training loss: 2.314211130142212
Validation loss: 2.0141337879242434

Epoch: 5| Step: 3
Training loss: 2.1102144718170166
Validation loss: 2.024444713387438

Epoch: 5| Step: 4
Training loss: 2.0956459045410156
Validation loss: 2.0302381412957304

Epoch: 5| Step: 5
Training loss: 1.583619475364685
Validation loss: 2.020998449735744

Epoch: 5| Step: 6
Training loss: 1.7366859912872314
Validation loss: 2.0418676663470525

Epoch: 5| Step: 7
Training loss: 2.447096824645996
Validation loss: 2.0582938527548187

Epoch: 5| Step: 8
Training loss: 2.297175645828247
Validation loss: 2.0627427511317755

Epoch: 5| Step: 9
Training loss: 1.9800622463226318
Validation loss: 2.0794468182389454

Epoch: 5| Step: 10
Training loss: 1.429404377937317
Validation loss: 2.0877811472903014

Testing loss: 2.254045327504476

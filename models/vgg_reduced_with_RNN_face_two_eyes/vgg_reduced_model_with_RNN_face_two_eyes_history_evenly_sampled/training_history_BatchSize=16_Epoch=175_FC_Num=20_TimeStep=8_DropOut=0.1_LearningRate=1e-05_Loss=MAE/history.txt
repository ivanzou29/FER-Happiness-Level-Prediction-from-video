Epoch: 1| Step: 0
Training loss: 5.279689788818359
Validation loss: 5.210338874529767

Epoch: 6| Step: 1
Training loss: 5.739119529724121
Validation loss: 5.202024085547334

Epoch: 6| Step: 2
Training loss: 4.352212905883789
Validation loss: 5.1946881150686615

Epoch: 6| Step: 3
Training loss: 5.9511399269104
Validation loss: 5.188463621242072

Epoch: 6| Step: 4
Training loss: 3.6832187175750732
Validation loss: 5.1826329487626275

Epoch: 6| Step: 5
Training loss: 4.539732456207275
Validation loss: 5.176721947167509

Epoch: 6| Step: 6
Training loss: 5.899083614349365
Validation loss: 5.170874067532119

Epoch: 6| Step: 7
Training loss: 4.194613933563232
Validation loss: 5.164222112265966

Epoch: 6| Step: 8
Training loss: 5.157158374786377
Validation loss: 5.157084706009075

Epoch: 6| Step: 9
Training loss: 6.146893501281738
Validation loss: 5.149513167719687

Epoch: 6| Step: 10
Training loss: 5.558971405029297
Validation loss: 5.14118992897772

Epoch: 6| Step: 11
Training loss: 4.737765312194824
Validation loss: 5.132407414015903

Epoch: 6| Step: 12
Training loss: 3.5605671405792236
Validation loss: 5.122218044855261

Epoch: 6| Step: 13
Training loss: 4.431650638580322
Validation loss: 5.1118614391614035

Epoch: 2| Step: 0
Training loss: 4.478913307189941
Validation loss: 5.099894313402073

Epoch: 6| Step: 1
Training loss: 6.300436973571777
Validation loss: 5.08700010853429

Epoch: 6| Step: 2
Training loss: 4.48880672454834
Validation loss: 5.073235116979127

Epoch: 6| Step: 3
Training loss: 4.527070999145508
Validation loss: 5.057647325659311

Epoch: 6| Step: 4
Training loss: 5.8909783363342285
Validation loss: 5.0418044418417

Epoch: 6| Step: 5
Training loss: 4.801827430725098
Validation loss: 5.024615456981044

Epoch: 6| Step: 6
Training loss: 4.437747955322266
Validation loss: 5.006226508848129

Epoch: 6| Step: 7
Training loss: 4.456441402435303
Validation loss: 4.986218447326332

Epoch: 6| Step: 8
Training loss: 4.424071311950684
Validation loss: 4.964845913712696

Epoch: 6| Step: 9
Training loss: 4.0977396965026855
Validation loss: 4.9420458168111825

Epoch: 6| Step: 10
Training loss: 5.224729537963867
Validation loss: 4.91824693577264

Epoch: 6| Step: 11
Training loss: 4.289531230926514
Validation loss: 4.893197449304724

Epoch: 6| Step: 12
Training loss: 4.342031478881836
Validation loss: 4.866473895247265

Epoch: 6| Step: 13
Training loss: 5.49812126159668
Validation loss: 4.837500367113339

Epoch: 3| Step: 0
Training loss: 4.536533355712891
Validation loss: 4.808179788692023

Epoch: 6| Step: 1
Training loss: 3.8165535926818848
Validation loss: 4.776079265020227

Epoch: 6| Step: 2
Training loss: 5.041782855987549
Validation loss: 4.7440727756869405

Epoch: 6| Step: 3
Training loss: 4.834709644317627
Validation loss: 4.711112991456063

Epoch: 6| Step: 4
Training loss: 4.229620456695557
Validation loss: 4.676194088433379

Epoch: 6| Step: 5
Training loss: 4.252877712249756
Validation loss: 4.640325746228618

Epoch: 6| Step: 6
Training loss: 3.0009660720825195
Validation loss: 4.604812109342185

Epoch: 6| Step: 7
Training loss: 3.7294440269470215
Validation loss: 4.568205213034025

Epoch: 6| Step: 8
Training loss: 4.973858833312988
Validation loss: 4.531132246858331

Epoch: 6| Step: 9
Training loss: 4.712255954742432
Validation loss: 4.494497053084835

Epoch: 6| Step: 10
Training loss: 4.533544063568115
Validation loss: 4.4576347207510345

Epoch: 6| Step: 11
Training loss: 4.857787132263184
Validation loss: 4.4220571107761835

Epoch: 6| Step: 12
Training loss: 4.103214263916016
Validation loss: 4.38800476699747

Epoch: 6| Step: 13
Training loss: 5.006010055541992
Validation loss: 4.352719963237804

Epoch: 4| Step: 0
Training loss: 4.903179168701172
Validation loss: 4.318963548188568

Epoch: 6| Step: 1
Training loss: 4.0897979736328125
Validation loss: 4.2855261243799685

Epoch: 6| Step: 2
Training loss: 4.052956581115723
Validation loss: 4.248788592635944

Epoch: 6| Step: 3
Training loss: 5.124777793884277
Validation loss: 4.211535192305042

Epoch: 6| Step: 4
Training loss: 5.320171356201172
Validation loss: 4.175986489941997

Epoch: 6| Step: 5
Training loss: 4.042669773101807
Validation loss: 4.140471186689151

Epoch: 6| Step: 6
Training loss: 3.5564796924591064
Validation loss: 4.108798842276296

Epoch: 6| Step: 7
Training loss: 2.578115463256836
Validation loss: 4.079168904212214

Epoch: 6| Step: 8
Training loss: 2.7907228469848633
Validation loss: 4.051733468168525

Epoch: 6| Step: 9
Training loss: 3.5797204971313477
Validation loss: 4.0246536039537

Epoch: 6| Step: 10
Training loss: 3.5445075035095215
Validation loss: 4.001258875734063

Epoch: 6| Step: 11
Training loss: 3.7639811038970947
Validation loss: 3.975381887087258

Epoch: 6| Step: 12
Training loss: 4.1740546226501465
Validation loss: 3.952369700195969

Epoch: 6| Step: 13
Training loss: 3.62201189994812
Validation loss: 3.9273074185976418

Epoch: 5| Step: 0
Training loss: 4.3573174476623535
Validation loss: 3.900147074012346

Epoch: 6| Step: 1
Training loss: 4.905250072479248
Validation loss: 3.8797681998181086

Epoch: 6| Step: 2
Training loss: 3.212615966796875
Validation loss: 3.8562589819713304

Epoch: 6| Step: 3
Training loss: 3.422440767288208
Validation loss: 3.839754819869995

Epoch: 6| Step: 4
Training loss: 3.2274155616760254
Validation loss: 3.8229593230832006

Epoch: 6| Step: 5
Training loss: 3.2869720458984375
Validation loss: 3.8125434511451313

Epoch: 6| Step: 6
Training loss: 3.9131057262420654
Validation loss: 3.797248537822436

Epoch: 6| Step: 7
Training loss: 4.336215972900391
Validation loss: 3.7791864333614225

Epoch: 6| Step: 8
Training loss: 3.316539764404297
Validation loss: 3.7695075952878563

Epoch: 6| Step: 9
Training loss: 3.0963211059570312
Validation loss: 3.758291526507306

Epoch: 6| Step: 10
Training loss: 3.540517568588257
Validation loss: 3.7502054399059666

Epoch: 6| Step: 11
Training loss: 3.84840726852417
Validation loss: 3.740089454958516

Epoch: 6| Step: 12
Training loss: 3.692999839782715
Validation loss: 3.730812959773566

Epoch: 6| Step: 13
Training loss: 3.1171462535858154
Validation loss: 3.722804010555308

Epoch: 6| Step: 0
Training loss: 3.782140016555786
Validation loss: 3.709978683020479

Epoch: 6| Step: 1
Training loss: 4.056673049926758
Validation loss: 3.6998866450402046

Epoch: 6| Step: 2
Training loss: 3.747947931289673
Validation loss: 3.690440018971761

Epoch: 6| Step: 3
Training loss: 2.313861846923828
Validation loss: 3.679478019796392

Epoch: 6| Step: 4
Training loss: 3.8756065368652344
Validation loss: 3.669259432823427

Epoch: 6| Step: 5
Training loss: 3.499265193939209
Validation loss: 3.659521902761152

Epoch: 6| Step: 6
Training loss: 3.531245231628418
Validation loss: 3.648798332419447

Epoch: 6| Step: 7
Training loss: 4.736416339874268
Validation loss: 3.638955318799583

Epoch: 6| Step: 8
Training loss: 3.1098787784576416
Validation loss: 3.6270850038015716

Epoch: 6| Step: 9
Training loss: 2.404545307159424
Validation loss: 3.6175939652227584

Epoch: 6| Step: 10
Training loss: 4.003310680389404
Validation loss: 3.6024070555163967

Epoch: 6| Step: 11
Training loss: 3.9527058601379395
Validation loss: 3.593551828015235

Epoch: 6| Step: 12
Training loss: 4.058093070983887
Validation loss: 3.5795807325711815

Epoch: 6| Step: 13
Training loss: 2.0701754093170166
Validation loss: 3.5711655437305407

Epoch: 7| Step: 0
Training loss: 2.8815665245056152
Validation loss: 3.561177263977707

Epoch: 6| Step: 1
Training loss: 2.9206881523132324
Validation loss: 3.5468845828886955

Epoch: 6| Step: 2
Training loss: 3.137507438659668
Validation loss: 3.540431594335905

Epoch: 6| Step: 3
Training loss: 4.210123062133789
Validation loss: 3.5320561829433648

Epoch: 6| Step: 4
Training loss: 3.3472766876220703
Validation loss: 3.524037394472348

Epoch: 6| Step: 5
Training loss: 4.270464897155762
Validation loss: 3.513193776530604

Epoch: 6| Step: 6
Training loss: 2.6959171295166016
Validation loss: 3.505612552806895

Epoch: 6| Step: 7
Training loss: 3.4635119438171387
Validation loss: 3.493858516857188

Epoch: 6| Step: 8
Training loss: 3.403097629547119
Validation loss: 3.4840145751994145

Epoch: 6| Step: 9
Training loss: 2.5380122661590576
Validation loss: 3.4751643006519606

Epoch: 6| Step: 10
Training loss: 2.2601068019866943
Validation loss: 3.4732495123340237

Epoch: 6| Step: 11
Training loss: 4.350861549377441
Validation loss: 3.4609794975608907

Epoch: 6| Step: 12
Training loss: 4.000101089477539
Validation loss: 3.4487136128128215

Epoch: 6| Step: 13
Training loss: 5.503747463226318
Validation loss: 3.4315638337084042

Epoch: 8| Step: 0
Training loss: 2.7033627033233643
Validation loss: 3.4200447220956125

Epoch: 6| Step: 1
Training loss: 3.822105884552002
Validation loss: 3.409705590176326

Epoch: 6| Step: 2
Training loss: 3.036402940750122
Validation loss: 3.3951829530859507

Epoch: 6| Step: 3
Training loss: 3.464409589767456
Validation loss: 3.3955025057638846

Epoch: 6| Step: 4
Training loss: 4.1172590255737305
Validation loss: 3.3804401351559545

Epoch: 6| Step: 5
Training loss: 2.6874215602874756
Validation loss: 3.367686184503699

Epoch: 6| Step: 6
Training loss: 3.171036720275879
Validation loss: 3.3619972121331

Epoch: 6| Step: 7
Training loss: 3.8129875659942627
Validation loss: 3.3545818610857894

Epoch: 6| Step: 8
Training loss: 3.5021867752075195
Validation loss: 3.3409320077588482

Epoch: 6| Step: 9
Training loss: 3.7875866889953613
Validation loss: 3.3345239470081944

Epoch: 6| Step: 10
Training loss: 2.2508485317230225
Validation loss: 3.327665585343556

Epoch: 6| Step: 11
Training loss: 3.0525379180908203
Validation loss: 3.325948328100225

Epoch: 6| Step: 12
Training loss: 3.126732587814331
Validation loss: 3.3090934471417497

Epoch: 6| Step: 13
Training loss: 4.38217830657959
Validation loss: 3.303626793687062

Epoch: 9| Step: 0
Training loss: 3.4484682083129883
Validation loss: 3.3009712080801688

Epoch: 6| Step: 1
Training loss: 2.7441976070404053
Validation loss: 3.2961076690304663

Epoch: 6| Step: 2
Training loss: 3.9987449645996094
Validation loss: 3.292542657544536

Epoch: 6| Step: 3
Training loss: 4.509854316711426
Validation loss: 3.2818886567187566

Epoch: 6| Step: 4
Training loss: 2.8553285598754883
Validation loss: 3.2783931942396265

Epoch: 6| Step: 5
Training loss: 2.4149374961853027
Validation loss: 3.2745914689956175

Epoch: 6| Step: 6
Training loss: 2.733766794204712
Validation loss: 3.279595749352568

Epoch: 6| Step: 7
Training loss: 3.331785202026367
Validation loss: 3.2673322231538835

Epoch: 6| Step: 8
Training loss: 4.355493545532227
Validation loss: 3.262153676761094

Epoch: 6| Step: 9
Training loss: 3.6645145416259766
Validation loss: 3.2576745453701226

Epoch: 6| Step: 10
Training loss: 3.7352657318115234
Validation loss: 3.255910760612898

Epoch: 6| Step: 11
Training loss: 2.1709299087524414
Validation loss: 3.250166518713838

Epoch: 6| Step: 12
Training loss: 2.7272098064422607
Validation loss: 3.247135875045612

Epoch: 6| Step: 13
Training loss: 2.386603355407715
Validation loss: 3.241244116137105

Epoch: 10| Step: 0
Training loss: 3.7563390731811523
Validation loss: 3.237116241967806

Epoch: 6| Step: 1
Training loss: 2.6092171669006348
Validation loss: 3.237265440725511

Epoch: 6| Step: 2
Training loss: 3.376286506652832
Validation loss: 3.2490042563407653

Epoch: 6| Step: 3
Training loss: 2.767778158187866
Validation loss: 3.2304791353082143

Epoch: 6| Step: 4
Training loss: 2.812047243118286
Validation loss: 3.2299238174192366

Epoch: 6| Step: 5
Training loss: 2.738852024078369
Validation loss: 3.230639701248497

Epoch: 6| Step: 6
Training loss: 2.9532251358032227
Validation loss: 3.232313735510713

Epoch: 6| Step: 7
Training loss: 2.8803558349609375
Validation loss: 3.2402851940483175

Epoch: 6| Step: 8
Training loss: 2.690272092819214
Validation loss: 3.246063568258798

Epoch: 6| Step: 9
Training loss: 3.086932420730591
Validation loss: 3.246746716960784

Epoch: 6| Step: 10
Training loss: 3.4606070518493652
Validation loss: 3.2332176418714624

Epoch: 6| Step: 11
Training loss: 4.0972490310668945
Validation loss: 3.2297243969414824

Epoch: 6| Step: 12
Training loss: 3.7005679607391357
Validation loss: 3.2208016867278726

Epoch: 6| Step: 13
Training loss: 4.910533428192139
Validation loss: 3.2205967287863455

Epoch: 11| Step: 0
Training loss: 3.5040693283081055
Validation loss: 3.2269797889135217

Epoch: 6| Step: 1
Training loss: 2.2590131759643555
Validation loss: 3.236877484988141

Epoch: 6| Step: 2
Training loss: 2.9285521507263184
Validation loss: 3.217761944699031

Epoch: 6| Step: 3
Training loss: 3.780819892883301
Validation loss: 3.208024578709756

Epoch: 6| Step: 4
Training loss: 3.1577882766723633
Validation loss: 3.200153799467189

Epoch: 6| Step: 5
Training loss: 2.4007575511932373
Validation loss: 3.199001327637703

Epoch: 6| Step: 6
Training loss: 2.8390774726867676
Validation loss: 3.197328903341806

Epoch: 6| Step: 7
Training loss: 3.5031867027282715
Validation loss: 3.2000074540415118

Epoch: 6| Step: 8
Training loss: 3.834171772003174
Validation loss: 3.196174265235983

Epoch: 6| Step: 9
Training loss: 3.3445215225219727
Validation loss: 3.1962859399857058

Epoch: 6| Step: 10
Training loss: 4.437822341918945
Validation loss: 3.1930356128241426

Epoch: 6| Step: 11
Training loss: 2.7192468643188477
Validation loss: 3.191378390917214

Epoch: 6| Step: 12
Training loss: 2.6440563201904297
Validation loss: 3.1853748495860765

Epoch: 6| Step: 13
Training loss: 3.6435890197753906
Validation loss: 3.1852630005087903

Epoch: 12| Step: 0
Training loss: 4.98309326171875
Validation loss: 3.185255845387777

Epoch: 6| Step: 1
Training loss: 2.861417770385742
Validation loss: 3.1746145653468307

Epoch: 6| Step: 2
Training loss: 4.206855773925781
Validation loss: 3.176018043230939

Epoch: 6| Step: 3
Training loss: 2.833277702331543
Validation loss: 3.1727648396645822

Epoch: 6| Step: 4
Training loss: 2.6039674282073975
Validation loss: 3.1729148459690872

Epoch: 6| Step: 5
Training loss: 3.8876237869262695
Validation loss: 3.1818969736817064

Epoch: 6| Step: 6
Training loss: 2.516498565673828
Validation loss: 3.1689301460020003

Epoch: 6| Step: 7
Training loss: 3.2705445289611816
Validation loss: 3.1647245191758677

Epoch: 6| Step: 8
Training loss: 2.4557547569274902
Validation loss: 3.1629770032821165

Epoch: 6| Step: 9
Training loss: 3.234015464782715
Validation loss: 3.1601425883590535

Epoch: 6| Step: 10
Training loss: 2.3371098041534424
Validation loss: 3.162619326704292

Epoch: 6| Step: 11
Training loss: 3.2631783485412598
Validation loss: 3.1580565129556963

Epoch: 6| Step: 12
Training loss: 3.06404447555542
Validation loss: 3.1570751897750364

Epoch: 6| Step: 13
Training loss: 2.7954766750335693
Validation loss: 3.157371167213686

Epoch: 13| Step: 0
Training loss: 3.358349561691284
Validation loss: 3.1532398398204515

Epoch: 6| Step: 1
Training loss: 2.706295967102051
Validation loss: 3.147407957302627

Epoch: 6| Step: 2
Training loss: 2.9503204822540283
Validation loss: 3.1473962081375944

Epoch: 6| Step: 3
Training loss: 3.4708480834960938
Validation loss: 3.1436957595168904

Epoch: 6| Step: 4
Training loss: 3.1637301445007324
Validation loss: 3.138849601950697

Epoch: 6| Step: 5
Training loss: 2.807128429412842
Validation loss: 3.137420562005812

Epoch: 6| Step: 6
Training loss: 3.429340124130249
Validation loss: 3.135826208258188

Epoch: 6| Step: 7
Training loss: 3.205902099609375
Validation loss: 3.13487289028783

Epoch: 6| Step: 8
Training loss: 3.543311834335327
Validation loss: 3.1305226279843237

Epoch: 6| Step: 9
Training loss: 2.389876365661621
Validation loss: 3.128858802139118

Epoch: 6| Step: 10
Training loss: 3.689324378967285
Validation loss: 3.126892538480861

Epoch: 6| Step: 11
Training loss: 2.6215436458587646
Validation loss: 3.124787376772973

Epoch: 6| Step: 12
Training loss: 3.6371078491210938
Validation loss: 3.1203851033282537

Epoch: 6| Step: 13
Training loss: 3.1842966079711914
Validation loss: 3.118613904522311

Epoch: 14| Step: 0
Training loss: 3.539370536804199
Validation loss: 3.1160227714046353

Epoch: 6| Step: 1
Training loss: 3.873364210128784
Validation loss: 3.111962951639647

Epoch: 6| Step: 2
Training loss: 3.1714377403259277
Validation loss: 3.11199858368084

Epoch: 6| Step: 3
Training loss: 3.6303586959838867
Validation loss: 3.108832807951076

Epoch: 6| Step: 4
Training loss: 3.162987232208252
Validation loss: 3.1066932216767342

Epoch: 6| Step: 5
Training loss: 3.5547122955322266
Validation loss: 3.1032270590464273

Epoch: 6| Step: 6
Training loss: 1.9854625463485718
Validation loss: 3.099176155623569

Epoch: 6| Step: 7
Training loss: 3.7149147987365723
Validation loss: 3.0970404096828994

Epoch: 6| Step: 8
Training loss: 2.96854829788208
Validation loss: 3.1002424929731633

Epoch: 6| Step: 9
Training loss: 2.810713052749634
Validation loss: 3.1028772861726823

Epoch: 6| Step: 10
Training loss: 3.3841259479522705
Validation loss: 3.120282619230209

Epoch: 6| Step: 11
Training loss: 2.748438835144043
Validation loss: 3.119248215870191

Epoch: 6| Step: 12
Training loss: 2.5707924365997314
Validation loss: 3.099581318516885

Epoch: 6| Step: 13
Training loss: 2.3607640266418457
Validation loss: 3.0938038569624706

Epoch: 15| Step: 0
Training loss: 2.5972766876220703
Validation loss: 3.089889734022079

Epoch: 6| Step: 1
Training loss: 3.0914971828460693
Validation loss: 3.0872487611668085

Epoch: 6| Step: 2
Training loss: 2.798560619354248
Validation loss: 3.0831403373390116

Epoch: 6| Step: 3
Training loss: 3.622523784637451
Validation loss: 3.088575863069104

Epoch: 6| Step: 4
Training loss: 3.3194191455841064
Validation loss: 3.0871466641784995

Epoch: 6| Step: 5
Training loss: 3.010838031768799
Validation loss: 3.0890219083396335

Epoch: 6| Step: 6
Training loss: 3.576012134552002
Validation loss: 3.102872820310695

Epoch: 6| Step: 7
Training loss: 3.2961177825927734
Validation loss: 3.0802921249020483

Epoch: 6| Step: 8
Training loss: 2.450206756591797
Validation loss: 3.0882760888786724

Epoch: 6| Step: 9
Training loss: 3.366471290588379
Validation loss: 3.0866948096982894

Epoch: 6| Step: 10
Training loss: 2.4683055877685547
Validation loss: 3.0877296770772626

Epoch: 6| Step: 11
Training loss: 3.212658405303955
Validation loss: 3.0990206118552917

Epoch: 6| Step: 12
Training loss: 3.2936177253723145
Validation loss: 3.093767519920103

Epoch: 6| Step: 13
Training loss: 3.8141140937805176
Validation loss: 3.085217998873803

Epoch: 16| Step: 0
Training loss: 3.9131336212158203
Validation loss: 3.0780394513119935

Epoch: 6| Step: 1
Training loss: 2.7710487842559814
Validation loss: 3.0744004480300413

Epoch: 6| Step: 2
Training loss: 2.6841320991516113
Validation loss: 3.0819771982008413

Epoch: 6| Step: 3
Training loss: 2.8081085681915283
Validation loss: 3.0841273107836322

Epoch: 6| Step: 4
Training loss: 3.5338282585144043
Validation loss: 3.0820985353121193

Epoch: 6| Step: 5
Training loss: 2.9073190689086914
Validation loss: 3.078694241021269

Epoch: 6| Step: 6
Training loss: 3.5738210678100586
Validation loss: 3.0763459256900254

Epoch: 6| Step: 7
Training loss: 3.008603096008301
Validation loss: 3.073642656367312

Epoch: 6| Step: 8
Training loss: 2.9941062927246094
Validation loss: 3.0795527119790354

Epoch: 6| Step: 9
Training loss: 2.9677305221557617
Validation loss: 3.0758665633457962

Epoch: 6| Step: 10
Training loss: 3.0552713871002197
Validation loss: 3.0736663546613467

Epoch: 6| Step: 11
Training loss: 2.9045543670654297
Validation loss: 3.072850270937848

Epoch: 6| Step: 12
Training loss: 3.4712955951690674
Validation loss: 3.070873316898141

Epoch: 6| Step: 13
Training loss: 2.9832403659820557
Validation loss: 3.069442802859891

Epoch: 17| Step: 0
Training loss: 3.315135955810547
Validation loss: 3.066121914053476

Epoch: 6| Step: 1
Training loss: 4.597714424133301
Validation loss: 3.0635803643093316

Epoch: 6| Step: 2
Training loss: 2.335674285888672
Validation loss: 3.0609771128623717

Epoch: 6| Step: 3
Training loss: 3.2000913619995117
Validation loss: 3.0599811666755268

Epoch: 6| Step: 4
Training loss: 2.8524255752563477
Validation loss: 3.0528109048002507

Epoch: 6| Step: 5
Training loss: 2.9000067710876465
Validation loss: 3.053499114128851

Epoch: 6| Step: 6
Training loss: 2.200929641723633
Validation loss: 3.053349733352661

Epoch: 6| Step: 7
Training loss: 1.9465150833129883
Validation loss: 3.0502947863712104

Epoch: 6| Step: 8
Training loss: 3.217085361480713
Validation loss: 3.0538241324886197

Epoch: 6| Step: 9
Training loss: 3.691531181335449
Validation loss: 3.055579162413074

Epoch: 6| Step: 10
Training loss: 3.6044557094573975
Validation loss: 3.051804188759096

Epoch: 6| Step: 11
Training loss: 3.194533586502075
Validation loss: 3.045438592151929

Epoch: 6| Step: 12
Training loss: 3.7099289894104004
Validation loss: 3.0422329646284862

Epoch: 6| Step: 13
Training loss: 2.3236312866210938
Validation loss: 3.038575085260535

Epoch: 18| Step: 0
Training loss: 3.2511587142944336
Validation loss: 3.0372666005165345

Epoch: 6| Step: 1
Training loss: 3.6397385597229004
Validation loss: 3.045351210460868

Epoch: 6| Step: 2
Training loss: 2.6883976459503174
Validation loss: 3.0492935667755785

Epoch: 6| Step: 3
Training loss: 3.1607518196105957
Validation loss: 3.107596702473138

Epoch: 6| Step: 4
Training loss: 3.6088650226593018
Validation loss: 3.084778993360458

Epoch: 6| Step: 5
Training loss: 3.67441987991333
Validation loss: 3.0471767174300326

Epoch: 6| Step: 6
Training loss: 2.204895496368408
Validation loss: 3.038370881029355

Epoch: 6| Step: 7
Training loss: 2.8751630783081055
Validation loss: 3.029126895371304

Epoch: 6| Step: 8
Training loss: 3.1011619567871094
Validation loss: 3.0316252298252557

Epoch: 6| Step: 9
Training loss: 3.574842691421509
Validation loss: 3.0433131546102543

Epoch: 6| Step: 10
Training loss: 3.2824182510375977
Validation loss: 3.0416938874029342

Epoch: 6| Step: 11
Training loss: 2.274794101715088
Validation loss: 3.0402550953690723

Epoch: 6| Step: 12
Training loss: 3.157925605773926
Validation loss: 3.038435423245994

Epoch: 6| Step: 13
Training loss: 2.495708465576172
Validation loss: 3.0306183958566315

Epoch: 19| Step: 0
Training loss: 2.6930789947509766
Validation loss: 3.028882908564742

Epoch: 6| Step: 1
Training loss: 3.4175236225128174
Validation loss: 3.0260319043231267

Epoch: 6| Step: 2
Training loss: 1.9694387912750244
Validation loss: 3.024814195530389

Epoch: 6| Step: 3
Training loss: 3.5248804092407227
Validation loss: 3.0237681558055263

Epoch: 6| Step: 4
Training loss: 3.2241950035095215
Validation loss: 3.0236375870243197

Epoch: 6| Step: 5
Training loss: 3.7061097621917725
Validation loss: 3.0235691583284767

Epoch: 6| Step: 6
Training loss: 2.8952038288116455
Validation loss: 3.0205776204345045

Epoch: 6| Step: 7
Training loss: 3.6437125205993652
Validation loss: 3.0119961410440426

Epoch: 6| Step: 8
Training loss: 3.1506166458129883
Validation loss: 3.017114649536789

Epoch: 6| Step: 9
Training loss: 3.1630077362060547
Validation loss: 3.0112227419371247

Epoch: 6| Step: 10
Training loss: 3.2392876148223877
Validation loss: 3.011053531400619

Epoch: 6| Step: 11
Training loss: 3.113773822784424
Validation loss: 3.003254070076891

Epoch: 6| Step: 12
Training loss: 2.3842415809631348
Validation loss: 3.0028789402336202

Epoch: 6| Step: 13
Training loss: 2.7880871295928955
Validation loss: 3.0009818871816

Epoch: 20| Step: 0
Training loss: 3.523479461669922
Validation loss: 2.9992106396664857

Epoch: 6| Step: 1
Training loss: 2.9159069061279297
Validation loss: 2.996437700845862

Epoch: 6| Step: 2
Training loss: 3.219897747039795
Validation loss: 2.9965192887090866

Epoch: 6| Step: 3
Training loss: 3.0033531188964844
Validation loss: 2.996757927761283

Epoch: 6| Step: 4
Training loss: 3.147860288619995
Validation loss: 2.9940696275362404

Epoch: 6| Step: 5
Training loss: 2.7304468154907227
Validation loss: 2.9931001842662854

Epoch: 6| Step: 6
Training loss: 3.170351982116699
Validation loss: 2.994476492686938

Epoch: 6| Step: 7
Training loss: 2.3597259521484375
Validation loss: 2.991153191494685

Epoch: 6| Step: 8
Training loss: 3.4251821041107178
Validation loss: 2.9917540242595058

Epoch: 6| Step: 9
Training loss: 2.790395736694336
Validation loss: 3.0012490851904756

Epoch: 6| Step: 10
Training loss: 3.141470432281494
Validation loss: 3.0176772840561403

Epoch: 6| Step: 11
Training loss: 3.784332275390625
Validation loss: 3.029136832042407

Epoch: 6| Step: 12
Training loss: 3.0528745651245117
Validation loss: 3.019312415071713

Epoch: 6| Step: 13
Training loss: 2.167876720428467
Validation loss: 2.9860004609630955

Epoch: 21| Step: 0
Training loss: 2.9202146530151367
Validation loss: 2.9869178802736345

Epoch: 6| Step: 1
Training loss: 2.4215495586395264
Validation loss: 2.9862952565634124

Epoch: 6| Step: 2
Training loss: 3.5006394386291504
Validation loss: 2.9860735375394105

Epoch: 6| Step: 3
Training loss: 3.4355292320251465
Validation loss: 2.9903030523689846

Epoch: 6| Step: 4
Training loss: 2.266791582107544
Validation loss: 2.985857358542822

Epoch: 6| Step: 5
Training loss: 3.0239882469177246
Validation loss: 2.9903678483860467

Epoch: 6| Step: 6
Training loss: 3.2009925842285156
Validation loss: 2.983975438661473

Epoch: 6| Step: 7
Training loss: 2.9684009552001953
Validation loss: 2.98517015159771

Epoch: 6| Step: 8
Training loss: 3.002603054046631
Validation loss: 2.9820781548817954

Epoch: 6| Step: 9
Training loss: 3.2243590354919434
Validation loss: 2.9857072753290974

Epoch: 6| Step: 10
Training loss: 3.5087642669677734
Validation loss: 2.980158708428824

Epoch: 6| Step: 11
Training loss: 3.1892917156219482
Validation loss: 2.9839413140409734

Epoch: 6| Step: 12
Training loss: 3.5637447834014893
Validation loss: 2.983068594368555

Epoch: 6| Step: 13
Training loss: 2.1921956539154053
Validation loss: 2.9878870082157913

Epoch: 22| Step: 0
Training loss: 3.2131776809692383
Validation loss: 2.9919506760053736

Epoch: 6| Step: 1
Training loss: 3.9616289138793945
Validation loss: 3.0048508721013225

Epoch: 6| Step: 2
Training loss: 3.718445301055908
Validation loss: 3.008668217607724

Epoch: 6| Step: 3
Training loss: 2.992332696914673
Validation loss: 2.994688275039837

Epoch: 6| Step: 4
Training loss: 2.5674819946289062
Validation loss: 2.993068351540514

Epoch: 6| Step: 5
Training loss: 2.9480485916137695
Validation loss: 2.983857811138194

Epoch: 6| Step: 6
Training loss: 2.8615715503692627
Validation loss: 2.9803511929768387

Epoch: 6| Step: 7
Training loss: 2.768195629119873
Validation loss: 2.9778632451129217

Epoch: 6| Step: 8
Training loss: 3.4683499336242676
Validation loss: 2.9776297538511214

Epoch: 6| Step: 9
Training loss: 2.0901310443878174
Validation loss: 2.9770224453300558

Epoch: 6| Step: 10
Training loss: 3.136235237121582
Validation loss: 2.972262079997729

Epoch: 6| Step: 11
Training loss: 2.992467164993286
Validation loss: 2.974218140366257

Epoch: 6| Step: 12
Training loss: 2.671809434890747
Validation loss: 2.9744181350995134

Epoch: 6| Step: 13
Training loss: 3.4941487312316895
Validation loss: 2.9720932309345534

Epoch: 23| Step: 0
Training loss: 1.6589977741241455
Validation loss: 2.973499651878111

Epoch: 6| Step: 1
Training loss: 2.8161425590515137
Validation loss: 2.9726132885102303

Epoch: 6| Step: 2
Training loss: 3.0775208473205566
Validation loss: 2.9738145361664476

Epoch: 6| Step: 3
Training loss: 3.2738022804260254
Validation loss: 2.973868675129388

Epoch: 6| Step: 4
Training loss: 2.3575828075408936
Validation loss: 2.9821981153180523

Epoch: 6| Step: 5
Training loss: 2.8795628547668457
Validation loss: 2.9849934013940955

Epoch: 6| Step: 6
Training loss: 2.6191859245300293
Validation loss: 2.982093567489296

Epoch: 6| Step: 7
Training loss: 3.71254301071167
Validation loss: 2.9710162865218295

Epoch: 6| Step: 8
Training loss: 3.4300498962402344
Validation loss: 2.9706040761804067

Epoch: 6| Step: 9
Training loss: 2.87955641746521
Validation loss: 2.9670370804366244

Epoch: 6| Step: 10
Training loss: 3.564413070678711
Validation loss: 2.965932165422747

Epoch: 6| Step: 11
Training loss: 2.582988739013672
Validation loss: 2.96651804062628

Epoch: 6| Step: 12
Training loss: 4.347456932067871
Validation loss: 2.9643634365450953

Epoch: 6| Step: 13
Training loss: 3.6370325088500977
Validation loss: 2.9647332955432195

Epoch: 24| Step: 0
Training loss: 2.5659618377685547
Validation loss: 2.96358121338711

Epoch: 6| Step: 1
Training loss: 2.913634777069092
Validation loss: 2.9643666769868586

Epoch: 6| Step: 2
Training loss: 2.4152302742004395
Validation loss: 2.9636497087376092

Epoch: 6| Step: 3
Training loss: 3.2677054405212402
Validation loss: 2.961163500303863

Epoch: 6| Step: 4
Training loss: 2.935912609100342
Validation loss: 2.959572263943252

Epoch: 6| Step: 5
Training loss: 3.130990505218506
Validation loss: 2.962499931294431

Epoch: 6| Step: 6
Training loss: 3.5076637268066406
Validation loss: 2.9611811509696384

Epoch: 6| Step: 7
Training loss: 3.153885841369629
Validation loss: 2.9622737079538326

Epoch: 6| Step: 8
Training loss: 3.037303924560547
Validation loss: 2.968477767000916

Epoch: 6| Step: 9
Training loss: 3.1618478298187256
Validation loss: 2.9700289105856292

Epoch: 6| Step: 10
Training loss: 2.172734022140503
Validation loss: 2.966845853354341

Epoch: 6| Step: 11
Training loss: 3.6166152954101562
Validation loss: 2.966692973208684

Epoch: 6| Step: 12
Training loss: 3.350158929824829
Validation loss: 2.958225662990283

Epoch: 6| Step: 13
Training loss: 3.3195297718048096
Validation loss: 2.956854445959932

Epoch: 25| Step: 0
Training loss: 3.0120081901550293
Validation loss: 2.9557015921479914

Epoch: 6| Step: 1
Training loss: 4.042426109313965
Validation loss: 2.9527381645735873

Epoch: 6| Step: 2
Training loss: 3.7539472579956055
Validation loss: 2.95566495772331

Epoch: 6| Step: 3
Training loss: 3.5933103561401367
Validation loss: 2.9528599400674143

Epoch: 6| Step: 4
Training loss: 2.5074501037597656
Validation loss: 2.951503663934687

Epoch: 6| Step: 5
Training loss: 2.9379305839538574
Validation loss: 2.9500540841010308

Epoch: 6| Step: 6
Training loss: 2.2853994369506836
Validation loss: 2.950950214939733

Epoch: 6| Step: 7
Training loss: 2.7529659271240234
Validation loss: 2.954165845788935

Epoch: 6| Step: 8
Training loss: 3.2568159103393555
Validation loss: 2.957862059275309

Epoch: 6| Step: 9
Training loss: 2.3722736835479736
Validation loss: 2.9434684886727283

Epoch: 6| Step: 10
Training loss: 2.878263473510742
Validation loss: 2.944790432530065

Epoch: 6| Step: 11
Training loss: 3.3832194805145264
Validation loss: 2.939820440866614

Epoch: 6| Step: 12
Training loss: 2.6671996116638184
Validation loss: 2.940606406939927

Epoch: 6| Step: 13
Training loss: 2.905529737472534
Validation loss: 2.943434520434308

Epoch: 26| Step: 0
Training loss: 2.648728609085083
Validation loss: 2.942932777507331

Epoch: 6| Step: 1
Training loss: 3.060002088546753
Validation loss: 2.9420477369780182

Epoch: 6| Step: 2
Training loss: 2.6637401580810547
Validation loss: 2.9417287354828208

Epoch: 6| Step: 3
Training loss: 3.182581901550293
Validation loss: 2.9430413271791194

Epoch: 6| Step: 4
Training loss: 3.743605136871338
Validation loss: 2.9436928892648346

Epoch: 6| Step: 5
Training loss: 2.335836887359619
Validation loss: 2.9400940889953286

Epoch: 6| Step: 6
Training loss: 3.3460042476654053
Validation loss: 2.9382490317026773

Epoch: 6| Step: 7
Training loss: 2.9415221214294434
Validation loss: 2.9383188832190728

Epoch: 6| Step: 8
Training loss: 3.055163860321045
Validation loss: 2.9396720112011

Epoch: 6| Step: 9
Training loss: 3.0807955265045166
Validation loss: 2.9397217176293813

Epoch: 6| Step: 10
Training loss: 2.439774513244629
Validation loss: 2.94026949585125

Epoch: 6| Step: 11
Training loss: 3.695108413696289
Validation loss: 2.938368402501588

Epoch: 6| Step: 12
Training loss: 2.875229597091675
Validation loss: 2.9443237960979505

Epoch: 6| Step: 13
Training loss: 3.380997657775879
Validation loss: 2.9389317753494426

Epoch: 27| Step: 0
Training loss: 2.704252004623413
Validation loss: 2.9416200371198755

Epoch: 6| Step: 1
Training loss: 2.419302463531494
Validation loss: 2.9447967083223405

Epoch: 6| Step: 2
Training loss: 2.4802565574645996
Validation loss: 2.9436357405877884

Epoch: 6| Step: 3
Training loss: 2.4436240196228027
Validation loss: 2.9425062492329586

Epoch: 6| Step: 4
Training loss: 3.4690675735473633
Validation loss: 2.9356107442609725

Epoch: 6| Step: 5
Training loss: 3.077847957611084
Validation loss: 2.9345465706240748

Epoch: 6| Step: 6
Training loss: 3.7731385231018066
Validation loss: 2.9332089116496425

Epoch: 6| Step: 7
Training loss: 2.7028796672821045
Validation loss: 2.9329337227729058

Epoch: 6| Step: 8
Training loss: 3.212984561920166
Validation loss: 2.931363941520773

Epoch: 6| Step: 9
Training loss: 2.913633108139038
Validation loss: 2.929996580205938

Epoch: 6| Step: 10
Training loss: 3.1572153568267822
Validation loss: 2.9299499296372935

Epoch: 6| Step: 11
Training loss: 3.0152952671051025
Validation loss: 2.929380216906148

Epoch: 6| Step: 12
Training loss: 4.0226593017578125
Validation loss: 2.9287227353742047

Epoch: 6| Step: 13
Training loss: 2.59016489982605
Validation loss: 2.9264817237854004

Epoch: 28| Step: 0
Training loss: 2.9092276096343994
Validation loss: 2.925985118394257

Epoch: 6| Step: 1
Training loss: 3.219665288925171
Validation loss: 2.92611627681281

Epoch: 6| Step: 2
Training loss: 2.5834805965423584
Validation loss: 2.9239762726650445

Epoch: 6| Step: 3
Training loss: 3.0972554683685303
Validation loss: 2.9247713396626134

Epoch: 6| Step: 4
Training loss: 3.4671778678894043
Validation loss: 2.9233062985122844

Epoch: 6| Step: 5
Training loss: 3.0490386486053467
Validation loss: 2.922246704819382

Epoch: 6| Step: 6
Training loss: 2.813474178314209
Validation loss: 2.9216578442563295

Epoch: 6| Step: 7
Training loss: 3.2986388206481934
Validation loss: 2.925142898354479

Epoch: 6| Step: 8
Training loss: 2.8807332515716553
Validation loss: 2.9317118890823854

Epoch: 6| Step: 9
Training loss: 2.7766313552856445
Validation loss: 2.9351649412544827

Epoch: 6| Step: 10
Training loss: 3.6218650341033936
Validation loss: 2.9237500980336177

Epoch: 6| Step: 11
Training loss: 3.3300719261169434
Validation loss: 2.9215418010629635

Epoch: 6| Step: 12
Training loss: 2.4038898944854736
Validation loss: 2.9182708647943314

Epoch: 6| Step: 13
Training loss: 2.32094669342041
Validation loss: 2.9181683191689114

Epoch: 29| Step: 0
Training loss: 2.8003687858581543
Validation loss: 2.9193522494326354

Epoch: 6| Step: 1
Training loss: 3.1819472312927246
Validation loss: 2.919674563151534

Epoch: 6| Step: 2
Training loss: 2.5732834339141846
Validation loss: 2.9190539672810543

Epoch: 6| Step: 3
Training loss: 3.432374954223633
Validation loss: 2.919234545000138

Epoch: 6| Step: 4
Training loss: 3.276947498321533
Validation loss: 2.9159494241078696

Epoch: 6| Step: 5
Training loss: 2.369616746902466
Validation loss: 2.91654771630482

Epoch: 6| Step: 6
Training loss: 3.8067727088928223
Validation loss: 2.919119927190965

Epoch: 6| Step: 7
Training loss: 3.556735038757324
Validation loss: 2.915283610743861

Epoch: 6| Step: 8
Training loss: 3.360614776611328
Validation loss: 2.921120318033362

Epoch: 6| Step: 9
Training loss: 3.0196657180786133
Validation loss: 2.930841297231695

Epoch: 6| Step: 10
Training loss: 2.5579235553741455
Validation loss: 2.933363755544027

Epoch: 6| Step: 11
Training loss: 2.658588409423828
Validation loss: 2.9360161186546407

Epoch: 6| Step: 12
Training loss: 2.424377918243408
Validation loss: 2.9179526323913247

Epoch: 6| Step: 13
Training loss: 2.958735704421997
Validation loss: 2.9104790251742125

Epoch: 30| Step: 0
Training loss: 3.0841238498687744
Validation loss: 2.913739304388723

Epoch: 6| Step: 1
Training loss: 2.3194870948791504
Validation loss: 2.917896483534126

Epoch: 6| Step: 2
Training loss: 3.23175048828125
Validation loss: 2.918585895210184

Epoch: 6| Step: 3
Training loss: 3.1899662017822266
Validation loss: 2.930619696135162

Epoch: 6| Step: 4
Training loss: 2.5462470054626465
Validation loss: 2.93015129335465

Epoch: 6| Step: 5
Training loss: 1.8476225137710571
Validation loss: 2.9259785118923394

Epoch: 6| Step: 6
Training loss: 2.95662784576416
Validation loss: 2.932854257604127

Epoch: 6| Step: 7
Training loss: 3.9037539958953857
Validation loss: 2.9181876285101778

Epoch: 6| Step: 8
Training loss: 3.862382411956787
Validation loss: 2.9117216653721307

Epoch: 6| Step: 9
Training loss: 2.3103339672088623
Validation loss: 2.909449561949699

Epoch: 6| Step: 10
Training loss: 1.9444626569747925
Validation loss: 2.908304432386993

Epoch: 6| Step: 11
Training loss: 2.9725117683410645
Validation loss: 2.911798605354883

Epoch: 6| Step: 12
Training loss: 3.8046298027038574
Validation loss: 2.9124270921112387

Epoch: 6| Step: 13
Training loss: 5.10696268081665
Validation loss: 2.9191361704180316

Epoch: 31| Step: 0
Training loss: 2.9128193855285645
Validation loss: 2.9169122711304696

Epoch: 6| Step: 1
Training loss: 2.8334646224975586
Validation loss: 2.9153829748912523

Epoch: 6| Step: 2
Training loss: 2.788118600845337
Validation loss: 2.9134024830274683

Epoch: 6| Step: 3
Training loss: 3.384047508239746
Validation loss: 2.9154200323166384

Epoch: 6| Step: 4
Training loss: 2.726431369781494
Validation loss: 2.915443733174314

Epoch: 6| Step: 5
Training loss: 2.3147706985473633
Validation loss: 2.915704591299898

Epoch: 6| Step: 6
Training loss: 3.397747039794922
Validation loss: 2.9115435743844635

Epoch: 6| Step: 7
Training loss: 3.740093231201172
Validation loss: 2.911117825456845

Epoch: 6| Step: 8
Training loss: 3.287842273712158
Validation loss: 2.908877331723449

Epoch: 6| Step: 9
Training loss: 2.4478464126586914
Validation loss: 2.904372710053639

Epoch: 6| Step: 10
Training loss: 3.608410358428955
Validation loss: 2.9089205239408757

Epoch: 6| Step: 11
Training loss: 2.5939700603485107
Validation loss: 2.906313321923697

Epoch: 6| Step: 12
Training loss: 2.512843608856201
Validation loss: 2.899830138811501

Epoch: 6| Step: 13
Training loss: 3.6569039821624756
Validation loss: 2.8984099280449653

Epoch: 32| Step: 0
Training loss: 3.3439440727233887
Validation loss: 2.898224053844329

Epoch: 6| Step: 1
Training loss: 2.656471014022827
Validation loss: 2.893985284272061

Epoch: 6| Step: 2
Training loss: 3.582517623901367
Validation loss: 2.8949132606547368

Epoch: 6| Step: 3
Training loss: 2.590792417526245
Validation loss: 2.8962349917299006

Epoch: 6| Step: 4
Training loss: 2.3929636478424072
Validation loss: 2.8930863718832693

Epoch: 6| Step: 5
Training loss: 3.4191513061523438
Validation loss: 2.8943433300141366

Epoch: 6| Step: 6
Training loss: 3.4546070098876953
Validation loss: 2.8946715272882932

Epoch: 6| Step: 7
Training loss: 3.304548978805542
Validation loss: 2.895067725130307

Epoch: 6| Step: 8
Training loss: 2.788443088531494
Validation loss: 2.894406731410693

Epoch: 6| Step: 9
Training loss: 2.772664785385132
Validation loss: 2.891194828094975

Epoch: 6| Step: 10
Training loss: 3.2049400806427
Validation loss: 2.8914211667994016

Epoch: 6| Step: 11
Training loss: 2.9632301330566406
Validation loss: 2.8882880236512873

Epoch: 6| Step: 12
Training loss: 2.852036952972412
Validation loss: 2.8871119509461107

Epoch: 6| Step: 13
Training loss: 2.237330436706543
Validation loss: 2.88670317844678

Epoch: 33| Step: 0
Training loss: 2.7384400367736816
Validation loss: 2.8869280225487164

Epoch: 6| Step: 1
Training loss: 2.2847702503204346
Validation loss: 2.885184493116153

Epoch: 6| Step: 2
Training loss: 3.5725739002227783
Validation loss: 2.8877245200577604

Epoch: 6| Step: 3
Training loss: 2.406986713409424
Validation loss: 2.8861328068599907

Epoch: 6| Step: 4
Training loss: 4.0206804275512695
Validation loss: 2.8849289160902782

Epoch: 6| Step: 5
Training loss: 2.7359466552734375
Validation loss: 2.88370926918522

Epoch: 6| Step: 6
Training loss: 3.6829967498779297
Validation loss: 2.8815499198052192

Epoch: 6| Step: 7
Training loss: 3.3957786560058594
Validation loss: 2.8832230157749628

Epoch: 6| Step: 8
Training loss: 3.059767246246338
Validation loss: 2.8814421187164965

Epoch: 6| Step: 9
Training loss: 2.7315187454223633
Validation loss: 2.882441582218293

Epoch: 6| Step: 10
Training loss: 3.410264015197754
Validation loss: 2.8818150617743052

Epoch: 6| Step: 11
Training loss: 2.214780807495117
Validation loss: 2.880011115022885

Epoch: 6| Step: 12
Training loss: 2.915601968765259
Validation loss: 2.881203523246191

Epoch: 6| Step: 13
Training loss: 2.2754204273223877
Validation loss: 2.8814783685950824

Epoch: 34| Step: 0
Training loss: 2.9972076416015625
Validation loss: 2.8800715964327575

Epoch: 6| Step: 1
Training loss: 3.167030096054077
Validation loss: 2.882900184200656

Epoch: 6| Step: 2
Training loss: 3.0137012004852295
Validation loss: 2.886160747979277

Epoch: 6| Step: 3
Training loss: 2.6262717247009277
Validation loss: 2.880812342448901

Epoch: 6| Step: 4
Training loss: 2.4200150966644287
Validation loss: 2.8801157474517822

Epoch: 6| Step: 5
Training loss: 2.4017481803894043
Validation loss: 2.880084035217121

Epoch: 6| Step: 6
Training loss: 2.69392728805542
Validation loss: 2.8812864954753588

Epoch: 6| Step: 7
Training loss: 3.150480270385742
Validation loss: 2.8830226236774075

Epoch: 6| Step: 8
Training loss: 3.682608127593994
Validation loss: 2.8861539415133897

Epoch: 6| Step: 9
Training loss: 4.098394393920898
Validation loss: 2.8823568128770396

Epoch: 6| Step: 10
Training loss: 2.77577805519104
Validation loss: 2.882399756421325

Epoch: 6| Step: 11
Training loss: 2.8319756984710693
Validation loss: 2.880267127867668

Epoch: 6| Step: 12
Training loss: 3.2432944774627686
Validation loss: 2.8781195250890588

Epoch: 6| Step: 13
Training loss: 2.213566780090332
Validation loss: 2.878612972074939

Epoch: 35| Step: 0
Training loss: 2.910492420196533
Validation loss: 2.883286117225565

Epoch: 6| Step: 1
Training loss: 3.9311816692352295
Validation loss: 2.883065172421035

Epoch: 6| Step: 2
Training loss: 2.699357032775879
Validation loss: 2.886567992548789

Epoch: 6| Step: 3
Training loss: 2.684025764465332
Validation loss: 2.880257034814486

Epoch: 6| Step: 4
Training loss: 2.2690787315368652
Validation loss: 2.873948904775804

Epoch: 6| Step: 5
Training loss: 2.9054510593414307
Validation loss: 2.8771182311478483

Epoch: 6| Step: 6
Training loss: 2.8238704204559326
Validation loss: 2.874813736125987

Epoch: 6| Step: 7
Training loss: 3.1303858757019043
Validation loss: 2.8735810095264065

Epoch: 6| Step: 8
Training loss: 3.5812511444091797
Validation loss: 2.8720033578975226

Epoch: 6| Step: 9
Training loss: 2.7500245571136475
Validation loss: 2.875031509707051

Epoch: 6| Step: 10
Training loss: 3.4487833976745605
Validation loss: 2.872753620147705

Epoch: 6| Step: 11
Training loss: 2.572422981262207
Validation loss: 2.8715453916980374

Epoch: 6| Step: 12
Training loss: 2.610743522644043
Validation loss: 2.8729483004539245

Epoch: 6| Step: 13
Training loss: 3.5040180683135986
Validation loss: 2.8729194184785247

Epoch: 36| Step: 0
Training loss: 3.7630271911621094
Validation loss: 2.871267141834382

Epoch: 6| Step: 1
Training loss: 3.155350685119629
Validation loss: 2.8757057959033596

Epoch: 6| Step: 2
Training loss: 3.0827856063842773
Validation loss: 2.870062271753947

Epoch: 6| Step: 3
Training loss: 3.618812322616577
Validation loss: 2.872053630890385

Epoch: 6| Step: 4
Training loss: 2.779287338256836
Validation loss: 2.869061916105209

Epoch: 6| Step: 5
Training loss: 2.939260244369507
Validation loss: 2.8721821538863646

Epoch: 6| Step: 6
Training loss: 2.8403584957122803
Validation loss: 2.8683931545544694

Epoch: 6| Step: 7
Training loss: 3.140604019165039
Validation loss: 2.8708912095715924

Epoch: 6| Step: 8
Training loss: 3.1317343711853027
Validation loss: 2.870353898694438

Epoch: 6| Step: 9
Training loss: 2.581343650817871
Validation loss: 2.8735736672596266

Epoch: 6| Step: 10
Training loss: 3.1498594284057617
Validation loss: 2.8784943472954536

Epoch: 6| Step: 11
Training loss: 2.610809803009033
Validation loss: 2.877119010494601

Epoch: 6| Step: 12
Training loss: 1.831399917602539
Validation loss: 2.87831045991631

Epoch: 6| Step: 13
Training loss: 2.7875046730041504
Validation loss: 2.8817882768569456

Epoch: 37| Step: 0
Training loss: 2.931945323944092
Validation loss: 2.874152068168886

Epoch: 6| Step: 1
Training loss: 3.0534238815307617
Validation loss: 2.8735136498687086

Epoch: 6| Step: 2
Training loss: 2.462700366973877
Validation loss: 2.871878459889402

Epoch: 6| Step: 3
Training loss: 2.9562196731567383
Validation loss: 2.870312052388345

Epoch: 6| Step: 4
Training loss: 4.174902439117432
Validation loss: 2.8699516429696033

Epoch: 6| Step: 5
Training loss: 3.041973829269409
Validation loss: 2.8691291014353433

Epoch: 6| Step: 6
Training loss: 3.2713074684143066
Validation loss: 2.869589208274759

Epoch: 6| Step: 7
Training loss: 3.0997562408447266
Validation loss: 2.8675867844653387

Epoch: 6| Step: 8
Training loss: 2.597201347351074
Validation loss: 2.8691630568555606

Epoch: 6| Step: 9
Training loss: 2.236647367477417
Validation loss: 2.866244544265091

Epoch: 6| Step: 10
Training loss: 2.645721435546875
Validation loss: 2.8673001796968522

Epoch: 6| Step: 11
Training loss: 2.647791862487793
Validation loss: 2.8680976026801654

Epoch: 6| Step: 12
Training loss: 3.017103672027588
Validation loss: 2.8648508748700543

Epoch: 6| Step: 13
Training loss: 3.689666748046875
Validation loss: 2.861730585816086

Epoch: 38| Step: 0
Training loss: 2.4522762298583984
Validation loss: 2.8657913669463126

Epoch: 6| Step: 1
Training loss: 3.3987903594970703
Validation loss: 2.8768508280477216

Epoch: 6| Step: 2
Training loss: 2.7936506271362305
Validation loss: 2.881505243239864

Epoch: 6| Step: 3
Training loss: 3.400789260864258
Validation loss: 2.923554530707739

Epoch: 6| Step: 4
Training loss: 3.102169990539551
Validation loss: 2.963684238413329

Epoch: 6| Step: 5
Training loss: 2.4234418869018555
Validation loss: 2.92992470341344

Epoch: 6| Step: 6
Training loss: 3.597684860229492
Validation loss: 2.8781348300236527

Epoch: 6| Step: 7
Training loss: 3.0107364654541016
Validation loss: 2.8652642721770913

Epoch: 6| Step: 8
Training loss: 3.6432244777679443
Validation loss: 2.860756870239012

Epoch: 6| Step: 9
Training loss: 2.761705160140991
Validation loss: 2.8566800676366335

Epoch: 6| Step: 10
Training loss: 1.7175829410552979
Validation loss: 2.8620162317829747

Epoch: 6| Step: 11
Training loss: 2.601973056793213
Validation loss: 2.8646366237312235

Epoch: 6| Step: 12
Training loss: 3.7270302772521973
Validation loss: 2.873065707504108

Epoch: 6| Step: 13
Training loss: 3.1823530197143555
Validation loss: 2.8788943444528887

Epoch: 39| Step: 0
Training loss: 3.533874750137329
Validation loss: 2.8696252299893286

Epoch: 6| Step: 1
Training loss: 1.7701091766357422
Validation loss: 2.870006327987999

Epoch: 6| Step: 2
Training loss: 2.968165874481201
Validation loss: 2.8669737692802184

Epoch: 6| Step: 3
Training loss: 2.195138931274414
Validation loss: 2.870604653512278

Epoch: 6| Step: 4
Training loss: 2.740692377090454
Validation loss: 2.886876575408443

Epoch: 6| Step: 5
Training loss: 3.686730146408081
Validation loss: 2.9063034006344375

Epoch: 6| Step: 6
Training loss: 3.5434370040893555
Validation loss: 2.882504504214051

Epoch: 6| Step: 7
Training loss: 3.2531208992004395
Validation loss: 2.8632930068559546

Epoch: 6| Step: 8
Training loss: 2.0099644660949707
Validation loss: 2.8556930044645905

Epoch: 6| Step: 9
Training loss: 2.690412998199463
Validation loss: 2.8562428105262017

Epoch: 6| Step: 10
Training loss: 3.3017430305480957
Validation loss: 2.86078574580531

Epoch: 6| Step: 11
Training loss: 3.055955648422241
Validation loss: 2.8637643783323226

Epoch: 6| Step: 12
Training loss: 3.926693916320801
Validation loss: 2.8672860412187475

Epoch: 6| Step: 13
Training loss: 2.7123706340789795
Validation loss: 2.865625035378241

Epoch: 40| Step: 0
Training loss: 2.8058905601501465
Validation loss: 2.860055374842818

Epoch: 6| Step: 1
Training loss: 4.031202793121338
Validation loss: 2.859749445351221

Epoch: 6| Step: 2
Training loss: 3.0158705711364746
Validation loss: 2.8572840690612793

Epoch: 6| Step: 3
Training loss: 2.6920454502105713
Validation loss: 2.8575143788450506

Epoch: 6| Step: 4
Training loss: 2.4798991680145264
Validation loss: 2.8549771590899398

Epoch: 6| Step: 5
Training loss: 3.3550450801849365
Validation loss: 2.858519067046463

Epoch: 6| Step: 6
Training loss: 3.163325786590576
Validation loss: 2.859564212060744

Epoch: 6| Step: 7
Training loss: 2.946941375732422
Validation loss: 2.8545897724807903

Epoch: 6| Step: 8
Training loss: 3.7476494312286377
Validation loss: 2.858324714886245

Epoch: 6| Step: 9
Training loss: 2.652082920074463
Validation loss: 2.8544735729053454

Epoch: 6| Step: 10
Training loss: 2.3997225761413574
Validation loss: 2.854481533009519

Epoch: 6| Step: 11
Training loss: 2.230836868286133
Validation loss: 2.8530578510735625

Epoch: 6| Step: 12
Training loss: 2.792416572570801
Validation loss: 2.848949388791156

Epoch: 6| Step: 13
Training loss: 3.3379502296447754
Validation loss: 2.8490969365642917

Epoch: 41| Step: 0
Training loss: 3.059192419052124
Validation loss: 2.8484935119587886

Epoch: 6| Step: 1
Training loss: 2.3068666458129883
Validation loss: 2.8501243591308594

Epoch: 6| Step: 2
Training loss: 2.5649473667144775
Validation loss: 2.8464619164825766

Epoch: 6| Step: 3
Training loss: 3.2500510215759277
Validation loss: 2.8475571806712816

Epoch: 6| Step: 4
Training loss: 3.68107271194458
Validation loss: 2.84680078875634

Epoch: 6| Step: 5
Training loss: 3.212965726852417
Validation loss: 2.8457510496980403

Epoch: 6| Step: 6
Training loss: 2.3408069610595703
Validation loss: 2.8472360872453257

Epoch: 6| Step: 7
Training loss: 2.070173740386963
Validation loss: 2.8473263427775395

Epoch: 6| Step: 8
Training loss: 2.5252938270568848
Validation loss: 2.847747307951732

Epoch: 6| Step: 9
Training loss: 3.3586177825927734
Validation loss: 2.849229927985899

Epoch: 6| Step: 10
Training loss: 3.1720082759857178
Validation loss: 2.8454767196409163

Epoch: 6| Step: 11
Training loss: 2.58526349067688
Validation loss: 2.8469117226139193

Epoch: 6| Step: 12
Training loss: 3.456825017929077
Validation loss: 2.846147975613994

Epoch: 6| Step: 13
Training loss: 4.414386749267578
Validation loss: 2.845870533297139

Epoch: 42| Step: 0
Training loss: 3.036607503890991
Validation loss: 2.844193148356612

Epoch: 6| Step: 1
Training loss: 2.4602627754211426
Validation loss: 2.844074713286533

Epoch: 6| Step: 2
Training loss: 3.260922908782959
Validation loss: 2.8435779617678736

Epoch: 6| Step: 3
Training loss: 2.8950107097625732
Validation loss: 2.8446292646469606

Epoch: 6| Step: 4
Training loss: 3.502466917037964
Validation loss: 2.8455911733770884

Epoch: 6| Step: 5
Training loss: 3.586911201477051
Validation loss: 2.8444014928674184

Epoch: 6| Step: 6
Training loss: 2.8253090381622314
Validation loss: 2.8435918643910396

Epoch: 6| Step: 7
Training loss: 3.490830421447754
Validation loss: 2.841455000703053

Epoch: 6| Step: 8
Training loss: 2.511791229248047
Validation loss: 2.8431533049511653

Epoch: 6| Step: 9
Training loss: 2.6415867805480957
Validation loss: 2.842920582781556

Epoch: 6| Step: 10
Training loss: 2.9669620990753174
Validation loss: 2.841524303600352

Epoch: 6| Step: 11
Training loss: 2.4061293601989746
Validation loss: 2.8394098256223943

Epoch: 6| Step: 12
Training loss: 3.1384105682373047
Validation loss: 2.841465369347603

Epoch: 6| Step: 13
Training loss: 2.204130172729492
Validation loss: 2.841001679820399

Epoch: 43| Step: 0
Training loss: 3.326136350631714
Validation loss: 2.842554692299135

Epoch: 6| Step: 1
Training loss: 2.733567237854004
Validation loss: 2.840405136026362

Epoch: 6| Step: 2
Training loss: 2.9994025230407715
Validation loss: 2.843700901154549

Epoch: 6| Step: 3
Training loss: 2.3662185668945312
Validation loss: 2.8445984343046784

Epoch: 6| Step: 4
Training loss: 2.503696918487549
Validation loss: 2.844945384610084

Epoch: 6| Step: 5
Training loss: 2.8137550354003906
Validation loss: 2.842941273925125

Epoch: 6| Step: 6
Training loss: 3.5909836292266846
Validation loss: 2.843579738370834

Epoch: 6| Step: 7
Training loss: 2.490929126739502
Validation loss: 2.841136558081514

Epoch: 6| Step: 8
Training loss: 3.297485828399658
Validation loss: 2.839172024880686

Epoch: 6| Step: 9
Training loss: 2.4857311248779297
Validation loss: 2.838042938581077

Epoch: 6| Step: 10
Training loss: 3.2738795280456543
Validation loss: 2.834789414559641

Epoch: 6| Step: 11
Training loss: 3.558198928833008
Validation loss: 2.832419031409807

Epoch: 6| Step: 12
Training loss: 3.206517457962036
Validation loss: 2.832836822796893

Epoch: 6| Step: 13
Training loss: 2.269011974334717
Validation loss: 2.8304239575580885

Epoch: 44| Step: 0
Training loss: 2.823578119277954
Validation loss: 2.832080566754905

Epoch: 6| Step: 1
Training loss: 2.8015494346618652
Validation loss: 2.8294703165690103

Epoch: 6| Step: 2
Training loss: 2.7326231002807617
Validation loss: 2.8284637979281846

Epoch: 6| Step: 3
Training loss: 3.5614399909973145
Validation loss: 2.830664521904402

Epoch: 6| Step: 4
Training loss: 3.4155712127685547
Validation loss: 2.8324011141254055

Epoch: 6| Step: 5
Training loss: 3.035451650619507
Validation loss: 2.8329272834203576

Epoch: 6| Step: 6
Training loss: 2.486974000930786
Validation loss: 2.833624788509902

Epoch: 6| Step: 7
Training loss: 2.417459011077881
Validation loss: 2.8328990603006012

Epoch: 6| Step: 8
Training loss: 3.263530731201172
Validation loss: 2.837389334555595

Epoch: 6| Step: 9
Training loss: 2.370793342590332
Validation loss: 2.836609617356331

Epoch: 6| Step: 10
Training loss: 3.272826910018921
Validation loss: 2.8388706509784987

Epoch: 6| Step: 11
Training loss: 2.6063766479492188
Validation loss: 2.8362519997422413

Epoch: 6| Step: 12
Training loss: 3.0197906494140625
Validation loss: 2.8365872342099427

Epoch: 6| Step: 13
Training loss: 3.525247097015381
Validation loss: 2.838505042496548

Epoch: 45| Step: 0
Training loss: 2.3062169551849365
Validation loss: 2.8369600952312513

Epoch: 6| Step: 1
Training loss: 3.3200297355651855
Validation loss: 2.8429065699218423

Epoch: 6| Step: 2
Training loss: 3.40352201461792
Validation loss: 2.8441937482485207

Epoch: 6| Step: 3
Training loss: 3.4522604942321777
Validation loss: 2.8459011944391395

Epoch: 6| Step: 4
Training loss: 3.4983158111572266
Validation loss: 2.848824626655989

Epoch: 6| Step: 5
Training loss: 3.122647762298584
Validation loss: 2.8436501179972002

Epoch: 6| Step: 6
Training loss: 3.1718759536743164
Validation loss: 2.839030783663514

Epoch: 6| Step: 7
Training loss: 2.413090467453003
Validation loss: 2.8355332497627503

Epoch: 6| Step: 8
Training loss: 2.7604970932006836
Validation loss: 2.8330245556369906

Epoch: 6| Step: 9
Training loss: 2.5017921924591064
Validation loss: 2.8344260133722776

Epoch: 6| Step: 10
Training loss: 3.495776414871216
Validation loss: 2.834374035558393

Epoch: 6| Step: 11
Training loss: 2.4725489616394043
Validation loss: 2.835114832847349

Epoch: 6| Step: 12
Training loss: 2.487677574157715
Validation loss: 2.8274290587312434

Epoch: 6| Step: 13
Training loss: 2.4183614253997803
Validation loss: 2.8321244460280224

Epoch: 46| Step: 0
Training loss: 3.5587191581726074
Validation loss: 2.8286785771769862

Epoch: 6| Step: 1
Training loss: 3.178978443145752
Validation loss: 2.8280433301002748

Epoch: 6| Step: 2
Training loss: 3.246694326400757
Validation loss: 2.824853999640352

Epoch: 6| Step: 3
Training loss: 2.6993284225463867
Validation loss: 2.82172562742746

Epoch: 6| Step: 4
Training loss: 2.4794297218322754
Validation loss: 2.822804904753162

Epoch: 6| Step: 5
Training loss: 2.2374496459960938
Validation loss: 2.824580474566388

Epoch: 6| Step: 6
Training loss: 2.952366352081299
Validation loss: 2.823138577963716

Epoch: 6| Step: 7
Training loss: 3.567277669906616
Validation loss: 2.824035470203687

Epoch: 6| Step: 8
Training loss: 2.569481372833252
Validation loss: 2.8205898500257924

Epoch: 6| Step: 9
Training loss: 3.0304627418518066
Validation loss: 2.8191528807404223

Epoch: 6| Step: 10
Training loss: 2.4293293952941895
Validation loss: 2.818410937504102

Epoch: 6| Step: 11
Training loss: 3.08011794090271
Validation loss: 2.81757374476361

Epoch: 6| Step: 12
Training loss: 2.8705055713653564
Validation loss: 2.819066891106226

Epoch: 6| Step: 13
Training loss: 3.2935190200805664
Validation loss: 2.8234457892756306

Epoch: 47| Step: 0
Training loss: 3.0895936489105225
Validation loss: 2.8218040491945002

Epoch: 6| Step: 1
Training loss: 2.781475067138672
Validation loss: 2.824948433906801

Epoch: 6| Step: 2
Training loss: 2.317013740539551
Validation loss: 2.8269679187446513

Epoch: 6| Step: 3
Training loss: 3.2050533294677734
Validation loss: 2.828532313787809

Epoch: 6| Step: 4
Training loss: 3.336526393890381
Validation loss: 2.831233432216029

Epoch: 6| Step: 5
Training loss: 3.3289804458618164
Validation loss: 2.8259274421199674

Epoch: 6| Step: 6
Training loss: 2.591684103012085
Validation loss: 2.8186883977664414

Epoch: 6| Step: 7
Training loss: 2.4222095012664795
Validation loss: 2.817132506319272

Epoch: 6| Step: 8
Training loss: 2.308119058609009
Validation loss: 2.8145833066714707

Epoch: 6| Step: 9
Training loss: 3.135944366455078
Validation loss: 2.8154818473323697

Epoch: 6| Step: 10
Training loss: 3.0874810218811035
Validation loss: 2.814057760341193

Epoch: 6| Step: 11
Training loss: 2.998188018798828
Validation loss: 2.8159133670150593

Epoch: 6| Step: 12
Training loss: 3.435558557510376
Validation loss: 2.814377474528487

Epoch: 6| Step: 13
Training loss: 2.8075978755950928
Validation loss: 2.8128507624390306

Epoch: 48| Step: 0
Training loss: 3.688661575317383
Validation loss: 2.81303515741902

Epoch: 6| Step: 1
Training loss: 2.3220529556274414
Validation loss: 2.8154845237731934

Epoch: 6| Step: 2
Training loss: 2.6260485649108887
Validation loss: 2.8167697973148798

Epoch: 6| Step: 3
Training loss: 2.866969108581543
Validation loss: 2.820402988823511

Epoch: 6| Step: 4
Training loss: 2.7094264030456543
Validation loss: 2.819194350191342

Epoch: 6| Step: 5
Training loss: 2.7536087036132812
Validation loss: 2.8213235152665006

Epoch: 6| Step: 6
Training loss: 2.585636615753174
Validation loss: 2.823239408513551

Epoch: 6| Step: 7
Training loss: 3.360599994659424
Validation loss: 2.824242689276254

Epoch: 6| Step: 8
Training loss: 3.166060209274292
Validation loss: 2.820367843874039

Epoch: 6| Step: 9
Training loss: 3.434522867202759
Validation loss: 2.817647308431646

Epoch: 6| Step: 10
Training loss: 3.150444984436035
Validation loss: 2.8142013985623597

Epoch: 6| Step: 11
Training loss: 2.3467888832092285
Validation loss: 2.814161044295116

Epoch: 6| Step: 12
Training loss: 2.3785762786865234
Validation loss: 2.810171809247745

Epoch: 6| Step: 13
Training loss: 3.841566324234009
Validation loss: 2.8078092452018493

Epoch: 49| Step: 0
Training loss: 3.251641273498535
Validation loss: 2.8087736714270806

Epoch: 6| Step: 1
Training loss: 3.303680658340454
Validation loss: 2.8064231359830467

Epoch: 6| Step: 2
Training loss: 2.3568594455718994
Validation loss: 2.8054925113595943

Epoch: 6| Step: 3
Training loss: 3.328796863555908
Validation loss: 2.8068109712293072

Epoch: 6| Step: 4
Training loss: 2.9636921882629395
Validation loss: 2.806211010102303

Epoch: 6| Step: 5
Training loss: 2.315152168273926
Validation loss: 2.8092337500664497

Epoch: 6| Step: 6
Training loss: 2.800466537475586
Validation loss: 2.8056349139059744

Epoch: 6| Step: 7
Training loss: 3.804198980331421
Validation loss: 2.804439947169314

Epoch: 6| Step: 8
Training loss: 3.0495591163635254
Validation loss: 2.805726748640819

Epoch: 6| Step: 9
Training loss: 2.692394256591797
Validation loss: 2.8035364894456762

Epoch: 6| Step: 10
Training loss: 2.3486881256103516
Validation loss: 2.8066066567615797

Epoch: 6| Step: 11
Training loss: 3.7191689014434814
Validation loss: 2.8077844983787945

Epoch: 6| Step: 12
Training loss: 2.22153902053833
Validation loss: 2.8078899511726956

Epoch: 6| Step: 13
Training loss: 2.2266664505004883
Validation loss: 2.813815798810733

Epoch: 50| Step: 0
Training loss: 2.6406877040863037
Validation loss: 2.817749531038346

Epoch: 6| Step: 1
Training loss: 3.209123373031616
Validation loss: 2.8226530936456498

Epoch: 6| Step: 2
Training loss: 2.9031734466552734
Validation loss: 2.830315177158643

Epoch: 6| Step: 3
Training loss: 3.298487424850464
Validation loss: 2.8314225032765377

Epoch: 6| Step: 4
Training loss: 2.0645904541015625
Validation loss: 2.8290485156479703

Epoch: 6| Step: 5
Training loss: 2.705658435821533
Validation loss: 2.83142674866543

Epoch: 6| Step: 6
Training loss: 2.8438076972961426
Validation loss: 2.8341025178150465

Epoch: 6| Step: 7
Training loss: 3.3798842430114746
Validation loss: 2.828756183706304

Epoch: 6| Step: 8
Training loss: 2.961230754852295
Validation loss: 2.814634400029336

Epoch: 6| Step: 9
Training loss: 4.016540050506592
Validation loss: 2.80477879124303

Epoch: 6| Step: 10
Training loss: 3.012664794921875
Validation loss: 2.8024672949185936

Epoch: 6| Step: 11
Training loss: 3.0012760162353516
Validation loss: 2.8017332476954304

Epoch: 6| Step: 12
Training loss: 1.896228551864624
Validation loss: 2.8036117220437653

Epoch: 6| Step: 13
Training loss: 2.681839942932129
Validation loss: 2.8067286604194233

Epoch: 51| Step: 0
Training loss: 3.1799964904785156
Validation loss: 2.8119844775046072

Epoch: 6| Step: 1
Training loss: 2.6945948600769043
Validation loss: 2.806199760847194

Epoch: 6| Step: 2
Training loss: 3.2067999839782715
Validation loss: 2.8132695869732927

Epoch: 6| Step: 3
Training loss: 3.031383991241455
Validation loss: 2.8082516270299114

Epoch: 6| Step: 4
Training loss: 2.5785553455352783
Validation loss: 2.801395995642549

Epoch: 6| Step: 5
Training loss: 2.6867575645446777
Validation loss: 2.804204120430895

Epoch: 6| Step: 6
Training loss: 3.5441532135009766
Validation loss: 2.7997194054306194

Epoch: 6| Step: 7
Training loss: 3.138155937194824
Validation loss: 2.797582539178992

Epoch: 6| Step: 8
Training loss: 2.9761619567871094
Validation loss: 2.798765241458852

Epoch: 6| Step: 9
Training loss: 3.0079212188720703
Validation loss: 2.7994157268155004

Epoch: 6| Step: 10
Training loss: 2.7999937534332275
Validation loss: 2.80298258924997

Epoch: 6| Step: 11
Training loss: 2.0989980697631836
Validation loss: 2.8030536918229956

Epoch: 6| Step: 12
Training loss: 2.3372015953063965
Validation loss: 2.804023752930344

Epoch: 6| Step: 13
Training loss: 3.99662184715271
Validation loss: 2.8011452408247095

Epoch: 52| Step: 0
Training loss: 2.277050018310547
Validation loss: 2.8051270669506443

Epoch: 6| Step: 1
Training loss: 3.1557838916778564
Validation loss: 2.806244234884939

Epoch: 6| Step: 2
Training loss: 3.839154005050659
Validation loss: 2.8107950789954073

Epoch: 6| Step: 3
Training loss: 3.0088953971862793
Validation loss: 2.818649717556533

Epoch: 6| Step: 4
Training loss: 3.423916816711426
Validation loss: 2.8199658804042365

Epoch: 6| Step: 5
Training loss: 2.051802158355713
Validation loss: 2.814991766406644

Epoch: 6| Step: 6
Training loss: 2.26486873626709
Validation loss: 2.8077192537246214

Epoch: 6| Step: 7
Training loss: 2.4630584716796875
Validation loss: 2.802547767598142

Epoch: 6| Step: 8
Training loss: 3.3111953735351562
Validation loss: 2.7986027450971704

Epoch: 6| Step: 9
Training loss: 2.7182092666625977
Validation loss: 2.7946206882435787

Epoch: 6| Step: 10
Training loss: 3.8012757301330566
Validation loss: 2.793353198676981

Epoch: 6| Step: 11
Training loss: 2.4857771396636963
Validation loss: 2.792642339583366

Epoch: 6| Step: 12
Training loss: 3.0282273292541504
Validation loss: 2.794207754955497

Epoch: 6| Step: 13
Training loss: 2.6011552810668945
Validation loss: 2.794591913941086

Epoch: 53| Step: 0
Training loss: 2.818347454071045
Validation loss: 2.792831331171015

Epoch: 6| Step: 1
Training loss: 3.810215950012207
Validation loss: 2.7931061765199066

Epoch: 6| Step: 2
Training loss: 2.693944215774536
Validation loss: 2.796228829250541

Epoch: 6| Step: 3
Training loss: 3.186793088912964
Validation loss: 2.792231913535826

Epoch: 6| Step: 4
Training loss: 2.4345743656158447
Validation loss: 2.7896543497680337

Epoch: 6| Step: 5
Training loss: 2.9276866912841797
Validation loss: 2.7849890724305184

Epoch: 6| Step: 6
Training loss: 2.897010326385498
Validation loss: 2.7835937930691625

Epoch: 6| Step: 7
Training loss: 2.406949043273926
Validation loss: 2.7846828276111233

Epoch: 6| Step: 8
Training loss: 3.2923386096954346
Validation loss: 2.785172462463379

Epoch: 6| Step: 9
Training loss: 2.6690497398376465
Validation loss: 2.7931827960475797

Epoch: 6| Step: 10
Training loss: 2.914720058441162
Validation loss: 2.8010563619675173

Epoch: 6| Step: 11
Training loss: 3.2855865955352783
Validation loss: 2.8116769226648475

Epoch: 6| Step: 12
Training loss: 2.798807144165039
Validation loss: 2.8091903143031622

Epoch: 6| Step: 13
Training loss: 2.2802908420562744
Validation loss: 2.812587115072435

Epoch: 54| Step: 0
Training loss: 2.725660562515259
Validation loss: 2.81923996761281

Epoch: 6| Step: 1
Training loss: 2.046363353729248
Validation loss: 2.818006933376353

Epoch: 6| Step: 2
Training loss: 3.5599260330200195
Validation loss: 2.817319041939192

Epoch: 6| Step: 3
Training loss: 2.993682861328125
Validation loss: 2.817739681531024

Epoch: 6| Step: 4
Training loss: 3.2402634620666504
Validation loss: 2.8332963733262915

Epoch: 6| Step: 5
Training loss: 2.6542654037475586
Validation loss: 2.837975996796803

Epoch: 6| Step: 6
Training loss: 2.5660035610198975
Validation loss: 2.8163410053458264

Epoch: 6| Step: 7
Training loss: 3.0290842056274414
Validation loss: 2.7961860651611

Epoch: 6| Step: 8
Training loss: 2.5509610176086426
Validation loss: 2.7932805758650585

Epoch: 6| Step: 9
Training loss: 2.8096907138824463
Validation loss: 2.7938474865369898

Epoch: 6| Step: 10
Training loss: 2.7639124393463135
Validation loss: 2.7902778143523843

Epoch: 6| Step: 11
Training loss: 3.006124973297119
Validation loss: 2.7863241498188307

Epoch: 6| Step: 12
Training loss: 3.253859043121338
Validation loss: 2.7835482243568666

Epoch: 6| Step: 13
Training loss: 3.762568473815918
Validation loss: 2.783910330905709

Epoch: 55| Step: 0
Training loss: 3.0605006217956543
Validation loss: 2.781510332579254

Epoch: 6| Step: 1
Training loss: 1.7739578485488892
Validation loss: 2.7821010492181264

Epoch: 6| Step: 2
Training loss: 2.740114212036133
Validation loss: 2.7824435054614978

Epoch: 6| Step: 3
Training loss: 3.2045974731445312
Validation loss: 2.7806787003753004

Epoch: 6| Step: 4
Training loss: 2.817765474319458
Validation loss: 2.7831774501390356

Epoch: 6| Step: 5
Training loss: 2.845139503479004
Validation loss: 2.781419720700992

Epoch: 6| Step: 6
Training loss: 2.0751442909240723
Validation loss: 2.7825680394326486

Epoch: 6| Step: 7
Training loss: 2.749859571456909
Validation loss: 2.7818785354655278

Epoch: 6| Step: 8
Training loss: 3.5622365474700928
Validation loss: 2.782337363048266

Epoch: 6| Step: 9
Training loss: 2.2951600551605225
Validation loss: 2.7870245851496214

Epoch: 6| Step: 10
Training loss: 3.0968849658966064
Validation loss: 2.7868676057425876

Epoch: 6| Step: 11
Training loss: 3.85854434967041
Validation loss: 2.7839110692342124

Epoch: 6| Step: 12
Training loss: 2.6746859550476074
Validation loss: 2.784469355819046

Epoch: 6| Step: 13
Training loss: 4.219274997711182
Validation loss: 2.7818420574229252

Epoch: 56| Step: 0
Training loss: 2.5160417556762695
Validation loss: 2.785184462865194

Epoch: 6| Step: 1
Training loss: 2.510585308074951
Validation loss: 2.779401192101099

Epoch: 6| Step: 2
Training loss: 3.4235119819641113
Validation loss: 2.779639854226061

Epoch: 6| Step: 3
Training loss: 3.1699419021606445
Validation loss: 2.7775998423176427

Epoch: 6| Step: 4
Training loss: 2.764893054962158
Validation loss: 2.774576497334306

Epoch: 6| Step: 5
Training loss: 3.459240198135376
Validation loss: 2.7729748525927143

Epoch: 6| Step: 6
Training loss: 2.732236623764038
Validation loss: 2.7742789201838995

Epoch: 6| Step: 7
Training loss: 3.375260353088379
Validation loss: 2.773944826536281

Epoch: 6| Step: 8
Training loss: 2.6860697269439697
Validation loss: 2.772602258190032

Epoch: 6| Step: 9
Training loss: 2.716369152069092
Validation loss: 2.7742722265182005

Epoch: 6| Step: 10
Training loss: 3.380192279815674
Validation loss: 2.7735833455157537

Epoch: 6| Step: 11
Training loss: 2.9315004348754883
Validation loss: 2.7719985720931843

Epoch: 6| Step: 12
Training loss: 1.7053641080856323
Validation loss: 2.7739790126841557

Epoch: 6| Step: 13
Training loss: 2.9706833362579346
Validation loss: 2.771088533504035

Epoch: 57| Step: 0
Training loss: 2.0547192096710205
Validation loss: 2.7699302319557435

Epoch: 6| Step: 1
Training loss: 2.6746320724487305
Validation loss: 2.767523350254182

Epoch: 6| Step: 2
Training loss: 2.6293487548828125
Validation loss: 2.7670981909639094

Epoch: 6| Step: 3
Training loss: 3.4737443923950195
Validation loss: 2.771747901875486

Epoch: 6| Step: 4
Training loss: 2.587629556655884
Validation loss: 2.771760448332756

Epoch: 6| Step: 5
Training loss: 1.9898780584335327
Validation loss: 2.7734062056387625

Epoch: 6| Step: 6
Training loss: 3.2638120651245117
Validation loss: 2.7773323956356255

Epoch: 6| Step: 7
Training loss: 2.64432954788208
Validation loss: 2.7807053648015505

Epoch: 6| Step: 8
Training loss: 3.010246753692627
Validation loss: 2.784671506574077

Epoch: 6| Step: 9
Training loss: 3.3429481983184814
Validation loss: 2.778369877928047

Epoch: 6| Step: 10
Training loss: 2.747300863265991
Validation loss: 2.777958039314516

Epoch: 6| Step: 11
Training loss: 2.678615093231201
Validation loss: 2.7746714981653358

Epoch: 6| Step: 12
Training loss: 3.341024875640869
Validation loss: 2.7704250530530046

Epoch: 6| Step: 13
Training loss: 4.5937323570251465
Validation loss: 2.77135742351573

Epoch: 58| Step: 0
Training loss: 3.2224764823913574
Validation loss: 2.769800506612306

Epoch: 6| Step: 1
Training loss: 1.8847596645355225
Validation loss: 2.7673970960801646

Epoch: 6| Step: 2
Training loss: 2.7716307640075684
Validation loss: 2.7672817937789427

Epoch: 6| Step: 3
Training loss: 2.486630439758301
Validation loss: 2.7656696304198234

Epoch: 6| Step: 4
Training loss: 2.9414288997650146
Validation loss: 2.764758392046857

Epoch: 6| Step: 5
Training loss: 3.02463436126709
Validation loss: 2.774250384299986

Epoch: 6| Step: 6
Training loss: 3.43019962310791
Validation loss: 2.7729197727736605

Epoch: 6| Step: 7
Training loss: 3.188678741455078
Validation loss: 2.7727385413262153

Epoch: 6| Step: 8
Training loss: 3.0079381465911865
Validation loss: 2.7705424421577045

Epoch: 6| Step: 9
Training loss: 2.9283640384674072
Validation loss: 2.7646638501075005

Epoch: 6| Step: 10
Training loss: 2.563387393951416
Validation loss: 2.7662963969733125

Epoch: 6| Step: 11
Training loss: 2.883707046508789
Validation loss: 2.7635108245316373

Epoch: 6| Step: 12
Training loss: 2.741994857788086
Validation loss: 2.7606784400119575

Epoch: 6| Step: 13
Training loss: 3.3457422256469727
Validation loss: 2.7616933443213023

Epoch: 59| Step: 0
Training loss: 2.9165380001068115
Validation loss: 2.7639695418778287

Epoch: 6| Step: 1
Training loss: 2.419175386428833
Validation loss: 2.768932665548017

Epoch: 6| Step: 2
Training loss: 2.83492374420166
Validation loss: 2.7676257446248043

Epoch: 6| Step: 3
Training loss: 3.1018362045288086
Validation loss: 2.768292829554568

Epoch: 6| Step: 4
Training loss: 2.9010443687438965
Validation loss: 2.7657061058987855

Epoch: 6| Step: 5
Training loss: 2.5542988777160645
Validation loss: 2.7621116561274373

Epoch: 6| Step: 6
Training loss: 3.0679261684417725
Validation loss: 2.7596602747517247

Epoch: 6| Step: 7
Training loss: 3.391021251678467
Validation loss: 2.757561578545519

Epoch: 6| Step: 8
Training loss: 2.9619879722595215
Validation loss: 2.7564378066729476

Epoch: 6| Step: 9
Training loss: 3.728304386138916
Validation loss: 2.7582334472287084

Epoch: 6| Step: 10
Training loss: 2.3941664695739746
Validation loss: 2.7657451834729923

Epoch: 6| Step: 11
Training loss: 2.243955373764038
Validation loss: 2.768706101243214

Epoch: 6| Step: 12
Training loss: 2.859466552734375
Validation loss: 2.7722336656303814

Epoch: 6| Step: 13
Training loss: 2.873779773712158
Validation loss: 2.779867108150195

Epoch: 60| Step: 0
Training loss: 2.5628201961517334
Validation loss: 2.7812091304409887

Epoch: 6| Step: 1
Training loss: 2.8614680767059326
Validation loss: 2.7779431958352365

Epoch: 6| Step: 2
Training loss: 2.0773420333862305
Validation loss: 2.77021389110114

Epoch: 6| Step: 3
Training loss: 3.506819486618042
Validation loss: 2.764298987644975

Epoch: 6| Step: 4
Training loss: 3.7511754035949707
Validation loss: 2.7608977748501684

Epoch: 6| Step: 5
Training loss: 2.3214683532714844
Validation loss: 2.758458629731209

Epoch: 6| Step: 6
Training loss: 2.6500983238220215
Validation loss: 2.755546062223373

Epoch: 6| Step: 7
Training loss: 3.6230368614196777
Validation loss: 2.7563658350257465

Epoch: 6| Step: 8
Training loss: 2.7185683250427246
Validation loss: 2.7524675220571537

Epoch: 6| Step: 9
Training loss: 2.962759256362915
Validation loss: 2.7552431552640853

Epoch: 6| Step: 10
Training loss: 2.7789483070373535
Validation loss: 2.755483017172865

Epoch: 6| Step: 11
Training loss: 2.443511724472046
Validation loss: 2.7538507599984445

Epoch: 6| Step: 12
Training loss: 2.8974037170410156
Validation loss: 2.756265424912976

Epoch: 6| Step: 13
Training loss: 3.3356523513793945
Validation loss: 2.7588624518404723

Epoch: 61| Step: 0
Training loss: 2.565648317337036
Validation loss: 2.762792082243068

Epoch: 6| Step: 1
Training loss: 2.534295082092285
Validation loss: 2.7555627592148317

Epoch: 6| Step: 2
Training loss: 2.218662738800049
Validation loss: 2.754776544468377

Epoch: 6| Step: 3
Training loss: 3.7204132080078125
Validation loss: 2.7547318166302097

Epoch: 6| Step: 4
Training loss: 3.489208459854126
Validation loss: 2.761024767352689

Epoch: 6| Step: 5
Training loss: 2.7429921627044678
Validation loss: 2.7647206091111705

Epoch: 6| Step: 6
Training loss: 2.759340286254883
Validation loss: 2.7641973854393087

Epoch: 6| Step: 7
Training loss: 2.799905776977539
Validation loss: 2.766871900968654

Epoch: 6| Step: 8
Training loss: 2.3668899536132812
Validation loss: 2.7629704321584394

Epoch: 6| Step: 9
Training loss: 1.9441386461257935
Validation loss: 2.7614240082361365

Epoch: 6| Step: 10
Training loss: 2.9792613983154297
Validation loss: 2.757929766049949

Epoch: 6| Step: 11
Training loss: 3.1648073196411133
Validation loss: 2.754762823863696

Epoch: 6| Step: 12
Training loss: 3.6596899032592773
Validation loss: 2.7549924799191055

Epoch: 6| Step: 13
Training loss: 3.4995815753936768
Validation loss: 2.7539245825941845

Epoch: 62| Step: 0
Training loss: 2.9792613983154297
Validation loss: 2.7551328648803053

Epoch: 6| Step: 1
Training loss: 4.341634273529053
Validation loss: 2.7545008659362793

Epoch: 6| Step: 2
Training loss: 2.3276009559631348
Validation loss: 2.7549247844244844

Epoch: 6| Step: 3
Training loss: 1.6861903667449951
Validation loss: 2.75170668478935

Epoch: 6| Step: 4
Training loss: 2.5959413051605225
Validation loss: 2.7493159950420423

Epoch: 6| Step: 5
Training loss: 3.347745895385742
Validation loss: 2.7486161365303943

Epoch: 6| Step: 6
Training loss: 2.3691234588623047
Validation loss: 2.7471659568048294

Epoch: 6| Step: 7
Training loss: 2.8309550285339355
Validation loss: 2.7477011039692867

Epoch: 6| Step: 8
Training loss: 3.314091920852661
Validation loss: 2.746130099860571

Epoch: 6| Step: 9
Training loss: 2.4088799953460693
Validation loss: 2.7466857715319564

Epoch: 6| Step: 10
Training loss: 2.438776969909668
Validation loss: 2.7463805778052217

Epoch: 6| Step: 11
Training loss: 3.964763879776001
Validation loss: 2.7471238720801567

Epoch: 6| Step: 12
Training loss: 2.969803810119629
Validation loss: 2.7458741895614134

Epoch: 6| Step: 13
Training loss: 2.2412917613983154
Validation loss: 2.7428686900805404

Epoch: 63| Step: 0
Training loss: 2.305152654647827
Validation loss: 2.744423476598596

Epoch: 6| Step: 1
Training loss: 3.003216505050659
Validation loss: 2.745144826109691

Epoch: 6| Step: 2
Training loss: 2.9971230030059814
Validation loss: 2.748936694155457

Epoch: 6| Step: 3
Training loss: 2.908421516418457
Validation loss: 2.7512213953079714

Epoch: 6| Step: 4
Training loss: 3.2681260108947754
Validation loss: 2.7509404638762116

Epoch: 6| Step: 5
Training loss: 2.609380006790161
Validation loss: 2.75518476065769

Epoch: 6| Step: 6
Training loss: 3.685290575027466
Validation loss: 2.754345847714332

Epoch: 6| Step: 7
Training loss: 3.2013254165649414
Validation loss: 2.7567078657047723

Epoch: 6| Step: 8
Training loss: 2.6863765716552734
Validation loss: 2.7490930736705823

Epoch: 6| Step: 9
Training loss: 2.870943307876587
Validation loss: 2.7453777046613794

Epoch: 6| Step: 10
Training loss: 2.698162078857422
Validation loss: 2.7421347043847524

Epoch: 6| Step: 11
Training loss: 3.163524627685547
Validation loss: 2.736391467432822

Epoch: 6| Step: 12
Training loss: 2.062258720397949
Validation loss: 2.7387797858125422

Epoch: 6| Step: 13
Training loss: 2.4370243549346924
Validation loss: 2.7397052216273483

Epoch: 64| Step: 0
Training loss: 2.7563300132751465
Validation loss: 2.742545795697038

Epoch: 6| Step: 1
Training loss: 2.9019641876220703
Validation loss: 2.743821967032648

Epoch: 6| Step: 2
Training loss: 3.4633073806762695
Validation loss: 2.742741541195941

Epoch: 6| Step: 3
Training loss: 2.7895941734313965
Validation loss: 2.741405769061017

Epoch: 6| Step: 4
Training loss: 3.074692487716675
Validation loss: 2.741988712741483

Epoch: 6| Step: 5
Training loss: 3.654038429260254
Validation loss: 2.7399522360935005

Epoch: 6| Step: 6
Training loss: 2.087541103363037
Validation loss: 2.7362810052851194

Epoch: 6| Step: 7
Training loss: 3.8142452239990234
Validation loss: 2.735650654762022

Epoch: 6| Step: 8
Training loss: 2.514909267425537
Validation loss: 2.7343065969405638

Epoch: 6| Step: 9
Training loss: 3.41640043258667
Validation loss: 2.733800347133349

Epoch: 6| Step: 10
Training loss: 2.219965934753418
Validation loss: 2.7345814038348455

Epoch: 6| Step: 11
Training loss: 2.431201457977295
Validation loss: 2.7385525626520955

Epoch: 6| Step: 12
Training loss: 2.7395100593566895
Validation loss: 2.7393617450550036

Epoch: 6| Step: 13
Training loss: 1.5451830625534058
Validation loss: 2.7435893653541483

Epoch: 65| Step: 0
Training loss: 2.832510471343994
Validation loss: 2.749259246292935

Epoch: 6| Step: 1
Training loss: 2.274646282196045
Validation loss: 2.747939194402387

Epoch: 6| Step: 2
Training loss: 3.905733585357666
Validation loss: 2.744171957815847

Epoch: 6| Step: 3
Training loss: 3.3812410831451416
Validation loss: 2.7475091334312194

Epoch: 6| Step: 4
Training loss: 2.1146326065063477
Validation loss: 2.7373883288393737

Epoch: 6| Step: 5
Training loss: 2.32403564453125
Validation loss: 2.7347499273156606

Epoch: 6| Step: 6
Training loss: 2.040679931640625
Validation loss: 2.7301291970796484

Epoch: 6| Step: 7
Training loss: 2.8606655597686768
Validation loss: 2.729164126098797

Epoch: 6| Step: 8
Training loss: 2.0634634494781494
Validation loss: 2.729430421706169

Epoch: 6| Step: 9
Training loss: 3.5116076469421387
Validation loss: 2.7323969615403043

Epoch: 6| Step: 10
Training loss: 2.8179306983947754
Validation loss: 2.7320197372026342

Epoch: 6| Step: 11
Training loss: 3.375432014465332
Validation loss: 2.7304691319824546

Epoch: 6| Step: 12
Training loss: 3.453789234161377
Validation loss: 2.7296370921596402

Epoch: 6| Step: 13
Training loss: 3.155181884765625
Validation loss: 2.729193095237978

Epoch: 66| Step: 0
Training loss: 2.3616538047790527
Validation loss: 2.727394506495486

Epoch: 6| Step: 1
Training loss: 2.951930046081543
Validation loss: 2.7285817900011615

Epoch: 6| Step: 2
Training loss: 2.722411870956421
Validation loss: 2.7275469380040325

Epoch: 6| Step: 3
Training loss: 3.2439703941345215
Validation loss: 2.726234133525561

Epoch: 6| Step: 4
Training loss: 3.4767022132873535
Validation loss: 2.7273112061203166

Epoch: 6| Step: 5
Training loss: 2.6328084468841553
Validation loss: 2.726522616160813

Epoch: 6| Step: 6
Training loss: 2.596801280975342
Validation loss: 2.7294070438672136

Epoch: 6| Step: 7
Training loss: 3.220828056335449
Validation loss: 2.725387547605781

Epoch: 6| Step: 8
Training loss: 2.740536689758301
Validation loss: 2.731522713938067

Epoch: 6| Step: 9
Training loss: 2.7558257579803467
Validation loss: 2.7311438821977183

Epoch: 6| Step: 10
Training loss: 2.8860573768615723
Validation loss: 2.7338367713394987

Epoch: 6| Step: 11
Training loss: 2.782076120376587
Validation loss: 2.7332235074812368

Epoch: 6| Step: 12
Training loss: 3.025128126144409
Validation loss: 2.734653324209234

Epoch: 6| Step: 13
Training loss: 2.0941178798675537
Validation loss: 2.7359642059572282

Epoch: 67| Step: 0
Training loss: 2.2398784160614014
Validation loss: 2.7323148301852647

Epoch: 6| Step: 1
Training loss: 2.5200576782226562
Validation loss: 2.728804701118059

Epoch: 6| Step: 2
Training loss: 2.9758801460266113
Validation loss: 2.726144390721475

Epoch: 6| Step: 3
Training loss: 3.1567115783691406
Validation loss: 2.7222215052573913

Epoch: 6| Step: 4
Training loss: 2.1608142852783203
Validation loss: 2.722159165208058

Epoch: 6| Step: 5
Training loss: 3.2271838188171387
Validation loss: 2.721275568008423

Epoch: 6| Step: 6
Training loss: 2.498061180114746
Validation loss: 2.7198500581966933

Epoch: 6| Step: 7
Training loss: 3.0373125076293945
Validation loss: 2.7198875719501125

Epoch: 6| Step: 8
Training loss: 2.866779327392578
Validation loss: 2.7216928197491552

Epoch: 6| Step: 9
Training loss: 2.875019073486328
Validation loss: 2.7206252672339

Epoch: 6| Step: 10
Training loss: 2.3412299156188965
Validation loss: 2.7191833475584626

Epoch: 6| Step: 11
Training loss: 3.411604642868042
Validation loss: 2.719790412533668

Epoch: 6| Step: 12
Training loss: 2.901146650314331
Validation loss: 2.7191990806210424

Epoch: 6| Step: 13
Training loss: 4.05383825302124
Validation loss: 2.719538257968041

Epoch: 68| Step: 0
Training loss: 2.668454647064209
Validation loss: 2.720014128633725

Epoch: 6| Step: 1
Training loss: 1.9879261255264282
Validation loss: 2.7190326490709857

Epoch: 6| Step: 2
Training loss: 2.4100544452667236
Validation loss: 2.7179135866062616

Epoch: 6| Step: 3
Training loss: 3.1117238998413086
Validation loss: 2.7199291388193765

Epoch: 6| Step: 4
Training loss: 2.2256767749786377
Validation loss: 2.7184260224783294

Epoch: 6| Step: 5
Training loss: 2.5874032974243164
Validation loss: 2.7173596351377425

Epoch: 6| Step: 6
Training loss: 3.062225818634033
Validation loss: 2.7171464068915254

Epoch: 6| Step: 7
Training loss: 2.4337663650512695
Validation loss: 2.722075785360029

Epoch: 6| Step: 8
Training loss: 2.603269100189209
Validation loss: 2.725380233539048

Epoch: 6| Step: 9
Training loss: 4.208219528198242
Validation loss: 2.727634181258499

Epoch: 6| Step: 10
Training loss: 3.3191115856170654
Validation loss: 2.729519926091676

Epoch: 6| Step: 11
Training loss: 2.888514757156372
Validation loss: 2.7288261895538657

Epoch: 6| Step: 12
Training loss: 3.2448511123657227
Validation loss: 2.7250716173520653

Epoch: 6| Step: 13
Training loss: 3.1602232456207275
Validation loss: 2.723428762087258

Epoch: 69| Step: 0
Training loss: 2.8855628967285156
Validation loss: 2.72478715578715

Epoch: 6| Step: 1
Training loss: 2.4780800342559814
Validation loss: 2.71927591933999

Epoch: 6| Step: 2
Training loss: 3.115777015686035
Validation loss: 2.7179941028677006

Epoch: 6| Step: 3
Training loss: 2.6999282836914062
Validation loss: 2.7095133130268385

Epoch: 6| Step: 4
Training loss: 2.4200730323791504
Validation loss: 2.711092315694337

Epoch: 6| Step: 5
Training loss: 3.324382781982422
Validation loss: 2.7116899849266134

Epoch: 6| Step: 6
Training loss: 2.919265031814575
Validation loss: 2.710903295906641

Epoch: 6| Step: 7
Training loss: 2.7831761837005615
Validation loss: 2.711496114730835

Epoch: 6| Step: 8
Training loss: 3.248455286026001
Validation loss: 2.712580345010245

Epoch: 6| Step: 9
Training loss: 2.8960022926330566
Validation loss: 2.713067526458412

Epoch: 6| Step: 10
Training loss: 3.217064380645752
Validation loss: 2.712359610424247

Epoch: 6| Step: 11
Training loss: 2.297616958618164
Validation loss: 2.7126955088748725

Epoch: 6| Step: 12
Training loss: 2.948153495788574
Validation loss: 2.712811577704645

Epoch: 6| Step: 13
Training loss: 2.2315073013305664
Validation loss: 2.7095273976684897

Epoch: 70| Step: 0
Training loss: 1.941733479499817
Validation loss: 2.7086301029369397

Epoch: 6| Step: 1
Training loss: 3.0484328269958496
Validation loss: 2.708874420453143

Epoch: 6| Step: 2
Training loss: 2.8388686180114746
Validation loss: 2.7102770369539977

Epoch: 6| Step: 3
Training loss: 2.932021141052246
Validation loss: 2.708515444109517

Epoch: 6| Step: 4
Training loss: 2.6840660572052
Validation loss: 2.7080970707760064

Epoch: 6| Step: 5
Training loss: 3.573528289794922
Validation loss: 2.71168601641091

Epoch: 6| Step: 6
Training loss: 3.249019145965576
Validation loss: 2.7096319788245746

Epoch: 6| Step: 7
Training loss: 2.410330295562744
Validation loss: 2.7101867839854252

Epoch: 6| Step: 8
Training loss: 3.426929473876953
Validation loss: 2.7091485428553757

Epoch: 6| Step: 9
Training loss: 2.9192962646484375
Validation loss: 2.7177808976942495

Epoch: 6| Step: 10
Training loss: 2.4310202598571777
Validation loss: 2.7168897787729898

Epoch: 6| Step: 11
Training loss: 3.0037100315093994
Validation loss: 2.7174019916083223

Epoch: 6| Step: 12
Training loss: 2.338019847869873
Validation loss: 2.717347785990725

Epoch: 6| Step: 13
Training loss: 2.8501014709472656
Validation loss: 2.714360821631647

Epoch: 71| Step: 0
Training loss: 2.509462594985962
Validation loss: 2.7154940277017574

Epoch: 6| Step: 1
Training loss: 2.9230504035949707
Validation loss: 2.7123610896448933

Epoch: 6| Step: 2
Training loss: 3.0378870964050293
Validation loss: 2.7065019530634724

Epoch: 6| Step: 3
Training loss: 2.3665037155151367
Validation loss: 2.7065699151767197

Epoch: 6| Step: 4
Training loss: 2.7799386978149414
Validation loss: 2.7035446730993127

Epoch: 6| Step: 5
Training loss: 2.6523518562316895
Validation loss: 2.7050549650704987

Epoch: 6| Step: 6
Training loss: 2.9532694816589355
Validation loss: 2.7031649645938667

Epoch: 6| Step: 7
Training loss: 3.803802013397217
Validation loss: 2.7065767677881385

Epoch: 6| Step: 8
Training loss: 3.009512424468994
Validation loss: 2.705306678689936

Epoch: 6| Step: 9
Training loss: 3.0213871002197266
Validation loss: 2.703688931721513

Epoch: 6| Step: 10
Training loss: 3.191236972808838
Validation loss: 2.703084058659051

Epoch: 6| Step: 11
Training loss: 2.4182374477386475
Validation loss: 2.7035096563318723

Epoch: 6| Step: 12
Training loss: 2.1311192512512207
Validation loss: 2.703256091763896

Epoch: 6| Step: 13
Training loss: 2.932847023010254
Validation loss: 2.7099892400926158

Epoch: 72| Step: 0
Training loss: 3.0765891075134277
Validation loss: 2.712501997588783

Epoch: 6| Step: 1
Training loss: 3.263878107070923
Validation loss: 2.7059427897135415

Epoch: 6| Step: 2
Training loss: 3.141185760498047
Validation loss: 2.706783266477687

Epoch: 6| Step: 3
Training loss: 2.2525546550750732
Validation loss: 2.7041082010474256

Epoch: 6| Step: 4
Training loss: 3.5039360523223877
Validation loss: 2.7035028678114696

Epoch: 6| Step: 5
Training loss: 2.730839967727661
Validation loss: 2.6990147098418205

Epoch: 6| Step: 6
Training loss: 2.6369428634643555
Validation loss: 2.698489576257685

Epoch: 6| Step: 7
Training loss: 2.771756410598755
Validation loss: 2.698111034208728

Epoch: 6| Step: 8
Training loss: 2.4746298789978027
Validation loss: 2.6969283832016813

Epoch: 6| Step: 9
Training loss: 3.0761232376098633
Validation loss: 2.6970526454269246

Epoch: 6| Step: 10
Training loss: 3.4582176208496094
Validation loss: 2.696481599602648

Epoch: 6| Step: 11
Training loss: 2.1115829944610596
Validation loss: 2.695768512705321

Epoch: 6| Step: 12
Training loss: 2.4215807914733887
Validation loss: 2.6957913342342583

Epoch: 6| Step: 13
Training loss: 2.5633068084716797
Validation loss: 2.6955907703727804

Epoch: 73| Step: 0
Training loss: 2.454819679260254
Validation loss: 2.694868455650986

Epoch: 6| Step: 1
Training loss: 2.963846206665039
Validation loss: 2.695217760660315

Epoch: 6| Step: 2
Training loss: 2.4016294479370117
Validation loss: 2.6944043251775924

Epoch: 6| Step: 3
Training loss: 3.3521218299865723
Validation loss: 2.6956405947285313

Epoch: 6| Step: 4
Training loss: 2.56388258934021
Validation loss: 2.7010283367608183

Epoch: 6| Step: 5
Training loss: 2.981204032897949
Validation loss: 2.6957739553143902

Epoch: 6| Step: 6
Training loss: 2.571021318435669
Validation loss: 2.698823034122426

Epoch: 6| Step: 7
Training loss: 3.2393798828125
Validation loss: 2.6964055081849456

Epoch: 6| Step: 8
Training loss: 2.413123846054077
Validation loss: 2.6950758939148276

Epoch: 6| Step: 9
Training loss: 2.1205363273620605
Validation loss: 2.691747385968444

Epoch: 6| Step: 10
Training loss: 3.6435141563415527
Validation loss: 2.6909615967863347

Epoch: 6| Step: 11
Training loss: 3.0760669708251953
Validation loss: 2.6908378498528593

Epoch: 6| Step: 12
Training loss: 3.2078309059143066
Validation loss: 2.692662708220943

Epoch: 6| Step: 13
Training loss: 2.320552349090576
Validation loss: 2.6942475816254974

Epoch: 74| Step: 0
Training loss: 2.771914005279541
Validation loss: 2.6927639668987644

Epoch: 6| Step: 1
Training loss: 3.774226665496826
Validation loss: 2.6909439820115284

Epoch: 6| Step: 2
Training loss: 2.3706374168395996
Validation loss: 2.69255401754892

Epoch: 6| Step: 3
Training loss: 2.976653575897217
Validation loss: 2.6928906235643613

Epoch: 6| Step: 4
Training loss: 2.8491008281707764
Validation loss: 2.6951909167792207

Epoch: 6| Step: 5
Training loss: 3.5792107582092285
Validation loss: 2.6957899396137526

Epoch: 6| Step: 6
Training loss: 2.8037946224212646
Validation loss: 2.696179307917113

Epoch: 6| Step: 7
Training loss: 2.1595489978790283
Validation loss: 2.6910335786880983

Epoch: 6| Step: 8
Training loss: 2.308619976043701
Validation loss: 2.6897252913444274

Epoch: 6| Step: 9
Training loss: 3.165019989013672
Validation loss: 2.690202231048256

Epoch: 6| Step: 10
Training loss: 2.7367453575134277
Validation loss: 2.692611707154141

Epoch: 6| Step: 11
Training loss: 2.7766799926757812
Validation loss: 2.691852682380266

Epoch: 6| Step: 12
Training loss: 2.481494903564453
Validation loss: 2.6950002793342835

Epoch: 6| Step: 13
Training loss: 2.7599329948425293
Validation loss: 2.6948434793820946

Epoch: 75| Step: 0
Training loss: 2.2531185150146484
Validation loss: 2.6938570135383197

Epoch: 6| Step: 1
Training loss: 3.4900379180908203
Validation loss: 2.6953226186895884

Epoch: 6| Step: 2
Training loss: 2.5364794731140137
Validation loss: 2.6905962882503385

Epoch: 6| Step: 3
Training loss: 2.9907217025756836
Validation loss: 2.691631127429265

Epoch: 6| Step: 4
Training loss: 2.830108165740967
Validation loss: 2.6899254142597155

Epoch: 6| Step: 5
Training loss: 2.854189395904541
Validation loss: 2.686769708510368

Epoch: 6| Step: 6
Training loss: 3.6967663764953613
Validation loss: 2.6860624692773305

Epoch: 6| Step: 7
Training loss: 2.059058666229248
Validation loss: 2.6862616744092715

Epoch: 6| Step: 8
Training loss: 3.1017448902130127
Validation loss: 2.68664014211265

Epoch: 6| Step: 9
Training loss: 2.361752986907959
Validation loss: 2.6844086518851658

Epoch: 6| Step: 10
Training loss: 2.9602556228637695
Validation loss: 2.683770920640679

Epoch: 6| Step: 11
Training loss: 2.8590402603149414
Validation loss: 2.683446712391351

Epoch: 6| Step: 12
Training loss: 2.7488505840301514
Validation loss: 2.6826010775822464

Epoch: 6| Step: 13
Training loss: 2.6247756481170654
Validation loss: 2.684202089104601

Epoch: 76| Step: 0
Training loss: 2.076432228088379
Validation loss: 2.6843078033898466

Epoch: 6| Step: 1
Training loss: 2.1300039291381836
Validation loss: 2.684691165083198

Epoch: 6| Step: 2
Training loss: 2.7497920989990234
Validation loss: 2.6841098518781763

Epoch: 6| Step: 3
Training loss: 2.912471294403076
Validation loss: 2.685309886932373

Epoch: 6| Step: 4
Training loss: 2.77571702003479
Validation loss: 2.681805770884278

Epoch: 6| Step: 5
Training loss: 2.762765407562256
Validation loss: 2.683146579291231

Epoch: 6| Step: 6
Training loss: 3.9609713554382324
Validation loss: 2.6858470388638076

Epoch: 6| Step: 7
Training loss: 2.454314708709717
Validation loss: 2.6836907094524753

Epoch: 6| Step: 8
Training loss: 3.1806023120880127
Validation loss: 2.6846136636631464

Epoch: 6| Step: 9
Training loss: 3.3668861389160156
Validation loss: 2.6846917393387004

Epoch: 6| Step: 10
Training loss: 2.4935388565063477
Validation loss: 2.686880583404213

Epoch: 6| Step: 11
Training loss: 2.953150510787964
Validation loss: 2.6926737934030514

Epoch: 6| Step: 12
Training loss: 2.4770891666412354
Validation loss: 2.6946499296413955

Epoch: 6| Step: 13
Training loss: 3.478297472000122
Validation loss: 2.7001933461876324

Epoch: 77| Step: 0
Training loss: 2.4788732528686523
Validation loss: 2.7023572383388395

Epoch: 6| Step: 1
Training loss: 3.0428900718688965
Validation loss: 2.7028567714075886

Epoch: 6| Step: 2
Training loss: 1.7106075286865234
Validation loss: 2.6858395120149017

Epoch: 6| Step: 3
Training loss: 2.742246150970459
Validation loss: 2.683217392172865

Epoch: 6| Step: 4
Training loss: 2.798251152038574
Validation loss: 2.6788383376213813

Epoch: 6| Step: 5
Training loss: 3.7807908058166504
Validation loss: 2.6804625744460733

Epoch: 6| Step: 6
Training loss: 2.808678150177002
Validation loss: 2.6866125727212555

Epoch: 6| Step: 7
Training loss: 2.9760775566101074
Validation loss: 2.6935678656383226

Epoch: 6| Step: 8
Training loss: 2.806853771209717
Validation loss: 2.686781855039699

Epoch: 6| Step: 9
Training loss: 2.870786190032959
Validation loss: 2.684464213668659

Epoch: 6| Step: 10
Training loss: 3.2673654556274414
Validation loss: 2.684561380776026

Epoch: 6| Step: 11
Training loss: 2.7664055824279785
Validation loss: 2.6834095011475267

Epoch: 6| Step: 12
Training loss: 2.2528932094573975
Validation loss: 2.681091265011859

Epoch: 6| Step: 13
Training loss: 3.543840169906616
Validation loss: 2.6795222118336666

Epoch: 78| Step: 0
Training loss: 3.258204221725464
Validation loss: 2.680874929633192

Epoch: 6| Step: 1
Training loss: 2.7506771087646484
Validation loss: 2.6972622666307675

Epoch: 6| Step: 2
Training loss: 2.421907901763916
Validation loss: 2.7309175358023694

Epoch: 6| Step: 3
Training loss: 3.148608684539795
Validation loss: 2.7882238382934244

Epoch: 6| Step: 4
Training loss: 3.2860240936279297
Validation loss: 2.7924545503431752

Epoch: 6| Step: 5
Training loss: 3.5451395511627197
Validation loss: 2.7941974029746106

Epoch: 6| Step: 6
Training loss: 2.6704792976379395
Validation loss: 2.7941169815678752

Epoch: 6| Step: 7
Training loss: 2.7091574668884277
Validation loss: 2.7817320182759273

Epoch: 6| Step: 8
Training loss: 3.103766441345215
Validation loss: 2.77638106448676

Epoch: 6| Step: 9
Training loss: 3.080756664276123
Validation loss: 2.767041398632911

Epoch: 6| Step: 10
Training loss: 3.165132999420166
Validation loss: 2.7654281175264748

Epoch: 6| Step: 11
Training loss: 2.3391528129577637
Validation loss: 2.7566346199281755

Epoch: 6| Step: 12
Training loss: 2.494570255279541
Validation loss: 2.7538115721876903

Epoch: 6| Step: 13
Training loss: 1.7446941137313843
Validation loss: 2.750281810760498

Epoch: 79| Step: 0
Training loss: 2.4952311515808105
Validation loss: 2.750427140984484

Epoch: 6| Step: 1
Training loss: 3.1405398845672607
Validation loss: 2.7489302978720715

Epoch: 6| Step: 2
Training loss: 2.973604679107666
Validation loss: 2.713594175154163

Epoch: 6| Step: 3
Training loss: 1.4837801456451416
Validation loss: 2.689579448392314

Epoch: 6| Step: 4
Training loss: 1.9368362426757812
Validation loss: 2.6976718005313667

Epoch: 6| Step: 5
Training loss: 2.9833157062530518
Validation loss: 2.6991882221673125

Epoch: 6| Step: 6
Training loss: 3.526834487915039
Validation loss: 2.696847479830506

Epoch: 6| Step: 7
Training loss: 3.565319538116455
Validation loss: 2.6953768986527638

Epoch: 6| Step: 8
Training loss: 2.2458879947662354
Validation loss: 2.707870944853752

Epoch: 6| Step: 9
Training loss: 3.381826639175415
Validation loss: 2.7168495385877547

Epoch: 6| Step: 10
Training loss: 3.555471181869507
Validation loss: 2.706507685363934

Epoch: 6| Step: 11
Training loss: 2.807870864868164
Validation loss: 2.6970917845285065

Epoch: 6| Step: 12
Training loss: 2.805464744567871
Validation loss: 2.6828619921079246

Epoch: 6| Step: 13
Training loss: 2.83644700050354
Validation loss: 2.6704862143403743

Epoch: 80| Step: 0
Training loss: 2.383939743041992
Validation loss: 2.673195413363877

Epoch: 6| Step: 1
Training loss: 2.8623876571655273
Validation loss: 2.6758755868481052

Epoch: 6| Step: 2
Training loss: 2.7224912643432617
Validation loss: 2.6842921780001734

Epoch: 6| Step: 3
Training loss: 2.7997260093688965
Validation loss: 2.6897492229297595

Epoch: 6| Step: 4
Training loss: 3.4767651557922363
Validation loss: 2.704960992259364

Epoch: 6| Step: 5
Training loss: 2.5225906372070312
Validation loss: 2.7123509196824926

Epoch: 6| Step: 6
Training loss: 3.080282688140869
Validation loss: 2.7182372718729

Epoch: 6| Step: 7
Training loss: 3.2885239124298096
Validation loss: 2.7154337283103698

Epoch: 6| Step: 8
Training loss: 2.0315663814544678
Validation loss: 2.7050928197881228

Epoch: 6| Step: 9
Training loss: 2.5277273654937744
Validation loss: 2.69386310731211

Epoch: 6| Step: 10
Training loss: 3.0758795738220215
Validation loss: 2.680196157065771

Epoch: 6| Step: 11
Training loss: 2.8297388553619385
Validation loss: 2.6744435576982397

Epoch: 6| Step: 12
Training loss: 3.107069253921509
Validation loss: 2.671941493147163

Epoch: 6| Step: 13
Training loss: 2.6183419227600098
Validation loss: 2.6637608799883115

Epoch: 81| Step: 0
Training loss: 1.9322081804275513
Validation loss: 2.6646955782367336

Epoch: 6| Step: 1
Training loss: 3.000356674194336
Validation loss: 2.6671458675015356

Epoch: 6| Step: 2
Training loss: 3.1762685775756836
Validation loss: 2.664128347109723

Epoch: 6| Step: 3
Training loss: 2.8810436725616455
Validation loss: 2.664215205818094

Epoch: 6| Step: 4
Training loss: 3.2345995903015137
Validation loss: 2.666713499253796

Epoch: 6| Step: 5
Training loss: 3.685731887817383
Validation loss: 2.6670778618063977

Epoch: 6| Step: 6
Training loss: 3.1198267936706543
Validation loss: 2.672533929988902

Epoch: 6| Step: 7
Training loss: 2.4632482528686523
Validation loss: 2.6743086179097495

Epoch: 6| Step: 8
Training loss: 2.48549222946167
Validation loss: 2.6764708129308556

Epoch: 6| Step: 9
Training loss: 2.5985167026519775
Validation loss: 2.675459741264261

Epoch: 6| Step: 10
Training loss: 2.5197560787200928
Validation loss: 2.677457824830086

Epoch: 6| Step: 11
Training loss: 2.7986879348754883
Validation loss: 2.6738886192280757

Epoch: 6| Step: 12
Training loss: 2.7196013927459717
Validation loss: 2.6738481290878786

Epoch: 6| Step: 13
Training loss: 2.5262718200683594
Validation loss: 2.6725647628948255

Epoch: 82| Step: 0
Training loss: 2.471104621887207
Validation loss: 2.673113666554933

Epoch: 6| Step: 1
Training loss: 2.58306884765625
Validation loss: 2.671018305645194

Epoch: 6| Step: 2
Training loss: 3.101685047149658
Validation loss: 2.6733832384950373

Epoch: 6| Step: 3
Training loss: 3.477485418319702
Validation loss: 2.6720008286096717

Epoch: 6| Step: 4
Training loss: 2.7545714378356934
Validation loss: 2.670543901381954

Epoch: 6| Step: 5
Training loss: 2.8307628631591797
Validation loss: 2.66722886536711

Epoch: 6| Step: 6
Training loss: 2.4890518188476562
Validation loss: 2.666376024164179

Epoch: 6| Step: 7
Training loss: 2.664536476135254
Validation loss: 2.6681296774136123

Epoch: 6| Step: 8
Training loss: 2.6151857376098633
Validation loss: 2.665483238876507

Epoch: 6| Step: 9
Training loss: 2.8744089603424072
Validation loss: 2.660244123910063

Epoch: 6| Step: 10
Training loss: 3.4101314544677734
Validation loss: 2.6605778509570706

Epoch: 6| Step: 11
Training loss: 2.727078676223755
Validation loss: 2.6580329300254903

Epoch: 6| Step: 12
Training loss: 2.498048782348633
Validation loss: 2.6552711481689126

Epoch: 6| Step: 13
Training loss: 2.6582822799682617
Validation loss: 2.653716195014215

Epoch: 83| Step: 0
Training loss: 2.403247833251953
Validation loss: 2.6511645240168416

Epoch: 6| Step: 1
Training loss: 3.4047110080718994
Validation loss: 2.6563208385180404

Epoch: 6| Step: 2
Training loss: 2.4486401081085205
Validation loss: 2.6539789809975574

Epoch: 6| Step: 3
Training loss: 2.1364352703094482
Validation loss: 2.6559378024070495

Epoch: 6| Step: 4
Training loss: 2.8810789585113525
Validation loss: 2.6560491900290213

Epoch: 6| Step: 5
Training loss: 3.7361044883728027
Validation loss: 2.653074159417101

Epoch: 6| Step: 6
Training loss: 3.065626621246338
Validation loss: 2.653959246091945

Epoch: 6| Step: 7
Training loss: 1.9564464092254639
Validation loss: 2.654335129645563

Epoch: 6| Step: 8
Training loss: 3.783738136291504
Validation loss: 2.653142916258945

Epoch: 6| Step: 9
Training loss: 3.0471863746643066
Validation loss: 2.6524786923521306

Epoch: 6| Step: 10
Training loss: 2.6879496574401855
Validation loss: 2.6532504763654483

Epoch: 6| Step: 11
Training loss: 3.265864610671997
Validation loss: 2.653246548867995

Epoch: 6| Step: 12
Training loss: 1.866887092590332
Validation loss: 2.6529122065472346

Epoch: 6| Step: 13
Training loss: 2.164545774459839
Validation loss: 2.650234007066296

Epoch: 84| Step: 0
Training loss: 2.6017889976501465
Validation loss: 2.65233100357876

Epoch: 6| Step: 1
Training loss: 2.5659780502319336
Validation loss: 2.65115999919112

Epoch: 6| Step: 2
Training loss: 2.8383421897888184
Validation loss: 2.6481663386027017

Epoch: 6| Step: 3
Training loss: 2.9875378608703613
Validation loss: 2.6482068877066336

Epoch: 6| Step: 4
Training loss: 2.611821174621582
Validation loss: 2.649121072984511

Epoch: 6| Step: 5
Training loss: 2.573737859725952
Validation loss: 2.6461488482772664

Epoch: 6| Step: 6
Training loss: 3.069561004638672
Validation loss: 2.6488740059637252

Epoch: 6| Step: 7
Training loss: 2.4300537109375
Validation loss: 2.6486581525494977

Epoch: 6| Step: 8
Training loss: 2.7876033782958984
Validation loss: 2.644426476570868

Epoch: 6| Step: 9
Training loss: 3.0301434993743896
Validation loss: 2.648166915421845

Epoch: 6| Step: 10
Training loss: 2.299271583557129
Validation loss: 2.64608924106885

Epoch: 6| Step: 11
Training loss: 3.4601759910583496
Validation loss: 2.64682948717507

Epoch: 6| Step: 12
Training loss: 2.9853973388671875
Validation loss: 2.6510502189718266

Epoch: 6| Step: 13
Training loss: 2.8292887210845947
Validation loss: 2.6503842646075833

Epoch: 85| Step: 0
Training loss: 3.658900737762451
Validation loss: 2.651830987263751

Epoch: 6| Step: 1
Training loss: 1.9597363471984863
Validation loss: 2.6539331302847913

Epoch: 6| Step: 2
Training loss: 2.336986541748047
Validation loss: 2.648544757596908

Epoch: 6| Step: 3
Training loss: 2.67012357711792
Validation loss: 2.6460049895830053

Epoch: 6| Step: 4
Training loss: 2.514078140258789
Validation loss: 2.6462720209552395

Epoch: 6| Step: 5
Training loss: 2.595576763153076
Validation loss: 2.64227956597523

Epoch: 6| Step: 6
Training loss: 2.8431029319763184
Validation loss: 2.6421347561702935

Epoch: 6| Step: 7
Training loss: 2.860203981399536
Validation loss: 2.6432995411657516

Epoch: 6| Step: 8
Training loss: 3.061534881591797
Validation loss: 2.6441105719535583

Epoch: 6| Step: 9
Training loss: 3.347046136856079
Validation loss: 2.6433587484462286

Epoch: 6| Step: 10
Training loss: 2.968290090560913
Validation loss: 2.642104289864981

Epoch: 6| Step: 11
Training loss: 2.2057764530181885
Validation loss: 2.640379521154588

Epoch: 6| Step: 12
Training loss: 3.2239832878112793
Validation loss: 2.6423083633504887

Epoch: 6| Step: 13
Training loss: 2.7982137203216553
Validation loss: 2.6400994664879254

Epoch: 86| Step: 0
Training loss: 2.738234043121338
Validation loss: 2.6399589071991625

Epoch: 6| Step: 1
Training loss: 3.3367879390716553
Validation loss: 2.639897085005237

Epoch: 6| Step: 2
Training loss: 2.8525564670562744
Validation loss: 2.640583766404019

Epoch: 6| Step: 3
Training loss: 2.9756174087524414
Validation loss: 2.641595648181054

Epoch: 6| Step: 4
Training loss: 2.751441240310669
Validation loss: 2.636566241582235

Epoch: 6| Step: 5
Training loss: 2.822882652282715
Validation loss: 2.639093980994276

Epoch: 6| Step: 6
Training loss: 1.9832309484481812
Validation loss: 2.636562534557876

Epoch: 6| Step: 7
Training loss: 3.0328421592712402
Validation loss: 2.63644931649649

Epoch: 6| Step: 8
Training loss: 2.9444398880004883
Validation loss: 2.635037506780317

Epoch: 6| Step: 9
Training loss: 1.986756682395935
Validation loss: 2.6378896133874052

Epoch: 6| Step: 10
Training loss: 2.8258016109466553
Validation loss: 2.6354444001310613

Epoch: 6| Step: 11
Training loss: 3.7148566246032715
Validation loss: 2.6342397402691584

Epoch: 6| Step: 12
Training loss: 2.3727574348449707
Validation loss: 2.6337973507501746

Epoch: 6| Step: 13
Training loss: 2.472970724105835
Validation loss: 2.6347931636277067

Epoch: 87| Step: 0
Training loss: 3.0007472038269043
Validation loss: 2.6341361102237495

Epoch: 6| Step: 1
Training loss: 2.029543876647949
Validation loss: 2.6325756913872174

Epoch: 6| Step: 2
Training loss: 2.7586512565612793
Validation loss: 2.6339862526104016

Epoch: 6| Step: 3
Training loss: 2.711460828781128
Validation loss: 2.6340294653369534

Epoch: 6| Step: 4
Training loss: 2.6105079650878906
Validation loss: 2.6338363232151156

Epoch: 6| Step: 5
Training loss: 3.3403449058532715
Validation loss: 2.6349308721480833

Epoch: 6| Step: 6
Training loss: 2.9948782920837402
Validation loss: 2.630382917260611

Epoch: 6| Step: 7
Training loss: 3.9540467262268066
Validation loss: 2.631491002216134

Epoch: 6| Step: 8
Training loss: 2.510970115661621
Validation loss: 2.6304710116437686

Epoch: 6| Step: 9
Training loss: 2.5385751724243164
Validation loss: 2.630033154641428

Epoch: 6| Step: 10
Training loss: 2.658399820327759
Validation loss: 2.631127062664237

Epoch: 6| Step: 11
Training loss: 2.573289632797241
Validation loss: 2.6329836537761073

Epoch: 6| Step: 12
Training loss: 2.058013677597046
Validation loss: 2.637159055279147

Epoch: 6| Step: 13
Training loss: 3.460099458694458
Validation loss: 2.6391557980609197

Epoch: 88| Step: 0
Training loss: 3.3776772022247314
Validation loss: 2.6332372029622397

Epoch: 6| Step: 1
Training loss: 2.536069869995117
Validation loss: 2.62853007419135

Epoch: 6| Step: 2
Training loss: 2.5345306396484375
Validation loss: 2.635498862112722

Epoch: 6| Step: 3
Training loss: 2.758082628250122
Validation loss: 2.63444608770391

Epoch: 6| Step: 4
Training loss: 2.6553196907043457
Validation loss: 2.6317035267429967

Epoch: 6| Step: 5
Training loss: 3.0041306018829346
Validation loss: 2.631533186922791

Epoch: 6| Step: 6
Training loss: 2.915191173553467
Validation loss: 2.6323493321736655

Epoch: 6| Step: 7
Training loss: 1.6441893577575684
Validation loss: 2.633169807413573

Epoch: 6| Step: 8
Training loss: 3.0203981399536133
Validation loss: 2.6321794627815165

Epoch: 6| Step: 9
Training loss: 3.622431755065918
Validation loss: 2.632371425628662

Epoch: 6| Step: 10
Training loss: 2.9817395210266113
Validation loss: 2.6314961141155613

Epoch: 6| Step: 11
Training loss: 2.503025531768799
Validation loss: 2.6331807310863207

Epoch: 6| Step: 12
Training loss: 2.508324146270752
Validation loss: 2.630777141099335

Epoch: 6| Step: 13
Training loss: 2.9036576747894287
Validation loss: 2.6296392153668147

Epoch: 89| Step: 0
Training loss: 3.66690993309021
Validation loss: 2.6271956377131964

Epoch: 6| Step: 1
Training loss: 2.645902633666992
Validation loss: 2.6296281455665507

Epoch: 6| Step: 2
Training loss: 3.693850517272949
Validation loss: 2.627918034471491

Epoch: 6| Step: 3
Training loss: 2.1102888584136963
Validation loss: 2.6273257245299635

Epoch: 6| Step: 4
Training loss: 2.1505115032196045
Validation loss: 2.6271678222123014

Epoch: 6| Step: 5
Training loss: 2.169598340988159
Validation loss: 2.627479099458264

Epoch: 6| Step: 6
Training loss: 2.8438453674316406
Validation loss: 2.623410419751239

Epoch: 6| Step: 7
Training loss: 2.939164161682129
Validation loss: 2.627612749735514

Epoch: 6| Step: 8
Training loss: 2.777541160583496
Validation loss: 2.627839414022302

Epoch: 6| Step: 9
Training loss: 2.8543028831481934
Validation loss: 2.6289048117976033

Epoch: 6| Step: 10
Training loss: 2.653097152709961
Validation loss: 2.6258344060631207

Epoch: 6| Step: 11
Training loss: 2.824646234512329
Validation loss: 2.6255178656629337

Epoch: 6| Step: 12
Training loss: 2.9878616333007812
Validation loss: 2.6225352825657016

Epoch: 6| Step: 13
Training loss: 2.4610579013824463
Validation loss: 2.6236965220461608

Epoch: 90| Step: 0
Training loss: 3.967236280441284
Validation loss: 2.6252809314317602

Epoch: 6| Step: 1
Training loss: 3.4566245079040527
Validation loss: 2.6284651089740056

Epoch: 6| Step: 2
Training loss: 2.3735008239746094
Validation loss: 2.6260567890700472

Epoch: 6| Step: 3
Training loss: 2.9459228515625
Validation loss: 2.622532329251689

Epoch: 6| Step: 4
Training loss: 3.1329782009124756
Validation loss: 2.6225342750549316

Epoch: 6| Step: 5
Training loss: 1.8118290901184082
Validation loss: 2.6231715345895417

Epoch: 6| Step: 6
Training loss: 2.3511910438537598
Validation loss: 2.626703708402572

Epoch: 6| Step: 7
Training loss: 1.9312231540679932
Validation loss: 2.6277317359883297

Epoch: 6| Step: 8
Training loss: 2.5555222034454346
Validation loss: 2.624571930977606

Epoch: 6| Step: 9
Training loss: 2.5863373279571533
Validation loss: 2.627704417833718

Epoch: 6| Step: 10
Training loss: 3.038957118988037
Validation loss: 2.6289516956575456

Epoch: 6| Step: 11
Training loss: 2.756345510482788
Validation loss: 2.6269175185952136

Epoch: 6| Step: 12
Training loss: 2.973968029022217
Validation loss: 2.625610589981079

Epoch: 6| Step: 13
Training loss: 3.136869430541992
Validation loss: 2.6237641226860786

Epoch: 91| Step: 0
Training loss: 1.704030156135559
Validation loss: 2.6249346015273884

Epoch: 6| Step: 1
Training loss: 3.1159281730651855
Validation loss: 2.6207516731754428

Epoch: 6| Step: 2
Training loss: 2.592703104019165
Validation loss: 2.6206709313136276

Epoch: 6| Step: 3
Training loss: 2.839055061340332
Validation loss: 2.6169844673525904

Epoch: 6| Step: 4
Training loss: 2.60833740234375
Validation loss: 2.6184765497843423

Epoch: 6| Step: 5
Training loss: 3.2205123901367188
Validation loss: 2.6167172744709957

Epoch: 6| Step: 6
Training loss: 3.89939284324646
Validation loss: 2.614748772754464

Epoch: 6| Step: 7
Training loss: 2.8125405311584473
Validation loss: 2.6167661502797115

Epoch: 6| Step: 8
Training loss: 2.9375150203704834
Validation loss: 2.6179632781654276

Epoch: 6| Step: 9
Training loss: 2.703277587890625
Validation loss: 2.6138880457929385

Epoch: 6| Step: 10
Training loss: 2.114466667175293
Validation loss: 2.617933539934056

Epoch: 6| Step: 11
Training loss: 2.972855567932129
Validation loss: 2.6198664531912854

Epoch: 6| Step: 12
Training loss: 2.7852063179016113
Validation loss: 2.618776616229806

Epoch: 6| Step: 13
Training loss: 2.2841567993164062
Validation loss: 2.6174389559735536

Epoch: 92| Step: 0
Training loss: 2.8737432956695557
Validation loss: 2.620105387062155

Epoch: 6| Step: 1
Training loss: 2.4724698066711426
Validation loss: 2.6290494934205086

Epoch: 6| Step: 2
Training loss: 2.675854444503784
Validation loss: 2.6261658053244314

Epoch: 6| Step: 3
Training loss: 3.0546071529388428
Validation loss: 2.62783392014042

Epoch: 6| Step: 4
Training loss: 3.33711838722229
Validation loss: 2.6258083774197485

Epoch: 6| Step: 5
Training loss: 3.209272861480713
Validation loss: 2.62212005225561

Epoch: 6| Step: 6
Training loss: 2.539375066757202
Validation loss: 2.6201370736604095

Epoch: 6| Step: 7
Training loss: 2.6808278560638428
Validation loss: 2.626355750586397

Epoch: 6| Step: 8
Training loss: 2.456780433654785
Validation loss: 2.6244315280709216

Epoch: 6| Step: 9
Training loss: 2.751948595046997
Validation loss: 2.619714275483162

Epoch: 6| Step: 10
Training loss: 1.6985557079315186
Validation loss: 2.619032705983808

Epoch: 6| Step: 11
Training loss: 3.107717514038086
Validation loss: 2.610658981466806

Epoch: 6| Step: 12
Training loss: 2.8949031829833984
Validation loss: 2.609701159179852

Epoch: 6| Step: 13
Training loss: 3.3256888389587402
Validation loss: 2.6120508281133508

Epoch: 93| Step: 0
Training loss: 2.8004941940307617
Validation loss: 2.6117781003316245

Epoch: 6| Step: 1
Training loss: 1.9885425567626953
Validation loss: 2.6147151736802954

Epoch: 6| Step: 2
Training loss: 2.4552786350250244
Validation loss: 2.622374347461167

Epoch: 6| Step: 3
Training loss: 3.354104518890381
Validation loss: 2.6192443704092376

Epoch: 6| Step: 4
Training loss: 2.893660068511963
Validation loss: 2.6218116847417687

Epoch: 6| Step: 5
Training loss: 2.290708303451538
Validation loss: 2.6207240550748763

Epoch: 6| Step: 6
Training loss: 2.646798849105835
Validation loss: 2.6208822470839306

Epoch: 6| Step: 7
Training loss: 3.6425750255584717
Validation loss: 2.6180883017919396

Epoch: 6| Step: 8
Training loss: 3.1110832691192627
Validation loss: 2.612594199436967

Epoch: 6| Step: 9
Training loss: 2.7916419506073
Validation loss: 2.6102678416877665

Epoch: 6| Step: 10
Training loss: 2.9132277965545654
Validation loss: 2.6064712744887157

Epoch: 6| Step: 11
Training loss: 2.4566588401794434
Validation loss: 2.6062373666353125

Epoch: 6| Step: 12
Training loss: 2.9157767295837402
Validation loss: 2.612883931847029

Epoch: 6| Step: 13
Training loss: 2.308438777923584
Validation loss: 2.6159079382496495

Epoch: 94| Step: 0
Training loss: 3.125021457672119
Validation loss: 2.6264744753478677

Epoch: 6| Step: 1
Training loss: 2.738509178161621
Validation loss: 2.635912605511245

Epoch: 6| Step: 2
Training loss: 2.7418198585510254
Validation loss: 2.6488253121734946

Epoch: 6| Step: 3
Training loss: 2.865006446838379
Validation loss: 2.649725725573878

Epoch: 6| Step: 4
Training loss: 2.4427404403686523
Validation loss: 2.6394295282261346

Epoch: 6| Step: 5
Training loss: 2.2129201889038086
Validation loss: 2.6197220868961786

Epoch: 6| Step: 6
Training loss: 2.895059108734131
Validation loss: 2.605711055058305

Epoch: 6| Step: 7
Training loss: 2.564861297607422
Validation loss: 2.6022964651866625

Epoch: 6| Step: 8
Training loss: 3.289947509765625
Validation loss: 2.6054009801598004

Epoch: 6| Step: 9
Training loss: 2.3057143688201904
Validation loss: 2.6068744838878675

Epoch: 6| Step: 10
Training loss: 3.159113883972168
Validation loss: 2.6120965942259757

Epoch: 6| Step: 11
Training loss: 2.7019906044006348
Validation loss: 2.6173876382971324

Epoch: 6| Step: 12
Training loss: 2.7078492641448975
Validation loss: 2.618603526905019

Epoch: 6| Step: 13
Training loss: 3.5707778930664062
Validation loss: 2.6221678667171027

Epoch: 95| Step: 0
Training loss: 2.5505990982055664
Validation loss: 2.6148461321348786

Epoch: 6| Step: 1
Training loss: 3.068692207336426
Validation loss: 2.6121411144092517

Epoch: 6| Step: 2
Training loss: 2.6601459980010986
Validation loss: 2.608941029476863

Epoch: 6| Step: 3
Training loss: 2.909956455230713
Validation loss: 2.606251626886347

Epoch: 6| Step: 4
Training loss: 3.3303985595703125
Validation loss: 2.601675869316183

Epoch: 6| Step: 5
Training loss: 2.0124731063842773
Validation loss: 2.601980975879136

Epoch: 6| Step: 6
Training loss: 2.771841526031494
Validation loss: 2.6004041920426073

Epoch: 6| Step: 7
Training loss: 3.2845587730407715
Validation loss: 2.597299293805194

Epoch: 6| Step: 8
Training loss: 2.3160128593444824
Validation loss: 2.5993651061929683

Epoch: 6| Step: 9
Training loss: 3.0293829441070557
Validation loss: 2.5990605021035798

Epoch: 6| Step: 10
Training loss: 2.3015241622924805
Validation loss: 2.598368949787591

Epoch: 6| Step: 11
Training loss: 2.3846468925476074
Validation loss: 2.6010729830752135

Epoch: 6| Step: 12
Training loss: 2.76187801361084
Validation loss: 2.5992323224262526

Epoch: 6| Step: 13
Training loss: 3.735220193862915
Validation loss: 2.596252426024406

Epoch: 96| Step: 0
Training loss: 2.4135305881500244
Validation loss: 2.599989829524871

Epoch: 6| Step: 1
Training loss: 2.914735794067383
Validation loss: 2.602104304939188

Epoch: 6| Step: 2
Training loss: 2.6061370372772217
Validation loss: 2.600560490803052

Epoch: 6| Step: 3
Training loss: 2.2876381874084473
Validation loss: 2.6013850601770545

Epoch: 6| Step: 4
Training loss: 2.8858768939971924
Validation loss: 2.6004445937372025

Epoch: 6| Step: 5
Training loss: 2.750302314758301
Validation loss: 2.601053986498105

Epoch: 6| Step: 6
Training loss: 3.249859094619751
Validation loss: 2.6011962403533277

Epoch: 6| Step: 7
Training loss: 2.981940746307373
Validation loss: 2.5994766014878468

Epoch: 6| Step: 8
Training loss: 3.011230945587158
Validation loss: 2.6009193620374127

Epoch: 6| Step: 9
Training loss: 3.500515937805176
Validation loss: 2.601427621738885

Epoch: 6| Step: 10
Training loss: 2.931627035140991
Validation loss: 2.5984974368926017

Epoch: 6| Step: 11
Training loss: 2.598870277404785
Validation loss: 2.6007760763168335

Epoch: 6| Step: 12
Training loss: 2.3582754135131836
Validation loss: 2.596969437855546

Epoch: 6| Step: 13
Training loss: 1.6608306169509888
Validation loss: 2.5939731085172264

Epoch: 97| Step: 0
Training loss: 2.8684000968933105
Validation loss: 2.596854973864812

Epoch: 6| Step: 1
Training loss: 2.8014326095581055
Validation loss: 2.5961211060964935

Epoch: 6| Step: 2
Training loss: 2.1198782920837402
Validation loss: 2.5946647838879655

Epoch: 6| Step: 3
Training loss: 3.1186296939849854
Validation loss: 2.59888276233468

Epoch: 6| Step: 4
Training loss: 3.363241195678711
Validation loss: 2.6010896390484226

Epoch: 6| Step: 5
Training loss: 2.4442696571350098
Validation loss: 2.60097663505103

Epoch: 6| Step: 6
Training loss: 1.8444020748138428
Validation loss: 2.6046957969665527

Epoch: 6| Step: 7
Training loss: 3.2971925735473633
Validation loss: 2.603788962928198

Epoch: 6| Step: 8
Training loss: 3.1338119506835938
Validation loss: 2.5960056243404264

Epoch: 6| Step: 9
Training loss: 2.3463644981384277
Validation loss: 2.595309511307747

Epoch: 6| Step: 10
Training loss: 2.200373888015747
Validation loss: 2.593336741129557

Epoch: 6| Step: 11
Training loss: 3.0397751331329346
Validation loss: 2.5927218878141014

Epoch: 6| Step: 12
Training loss: 3.091313362121582
Validation loss: 2.5923934111031155

Epoch: 6| Step: 13
Training loss: 2.985840082168579
Validation loss: 2.5937223895903556

Epoch: 98| Step: 0
Training loss: 3.340334415435791
Validation loss: 2.5949006772810415

Epoch: 6| Step: 1
Training loss: 2.527904510498047
Validation loss: 2.5933217387045584

Epoch: 6| Step: 2
Training loss: 2.220484972000122
Validation loss: 2.5932081642971245

Epoch: 6| Step: 3
Training loss: 3.0248939990997314
Validation loss: 2.5942032516643567

Epoch: 6| Step: 4
Training loss: 2.2940444946289062
Validation loss: 2.5948679985538607

Epoch: 6| Step: 5
Training loss: 2.7027268409729004
Validation loss: 2.592251262357158

Epoch: 6| Step: 6
Training loss: 3.1986076831817627
Validation loss: 2.591415043800108

Epoch: 6| Step: 7
Training loss: 2.99383282661438
Validation loss: 2.594062771848453

Epoch: 6| Step: 8
Training loss: 2.703073024749756
Validation loss: 2.5911844879068355

Epoch: 6| Step: 9
Training loss: 3.0936732292175293
Validation loss: 2.5877931553830384

Epoch: 6| Step: 10
Training loss: 2.7653191089630127
Validation loss: 2.589400242733699

Epoch: 6| Step: 11
Training loss: 2.6272032260894775
Validation loss: 2.5896439372852282

Epoch: 6| Step: 12
Training loss: 2.3308284282684326
Validation loss: 2.58641876200194

Epoch: 6| Step: 13
Training loss: 2.7299299240112305
Validation loss: 2.5856574991697907

Epoch: 99| Step: 0
Training loss: 2.6412267684936523
Validation loss: 2.5877855798249603

Epoch: 6| Step: 1
Training loss: 2.4710183143615723
Validation loss: 2.5864607018809163

Epoch: 6| Step: 2
Training loss: 3.1917917728424072
Validation loss: 2.585176747332337

Epoch: 6| Step: 3
Training loss: 2.9540345668792725
Validation loss: 2.5895306448782645

Epoch: 6| Step: 4
Training loss: 3.2605509757995605
Validation loss: 2.585691491762797

Epoch: 6| Step: 5
Training loss: 2.4693944454193115
Validation loss: 2.5835240810148177

Epoch: 6| Step: 6
Training loss: 2.357396125793457
Validation loss: 2.58572833512419

Epoch: 6| Step: 7
Training loss: 2.0986745357513428
Validation loss: 2.5820986583668697

Epoch: 6| Step: 8
Training loss: 2.80155611038208
Validation loss: 2.58567375521506

Epoch: 6| Step: 9
Training loss: 2.7497506141662598
Validation loss: 2.5876348069919053

Epoch: 6| Step: 10
Training loss: 3.727717399597168
Validation loss: 2.5864379303429716

Epoch: 6| Step: 11
Training loss: 2.288855791091919
Validation loss: 2.5857860760022233

Epoch: 6| Step: 12
Training loss: 2.6869213581085205
Validation loss: 2.58471235921306

Epoch: 6| Step: 13
Training loss: 2.8146300315856934
Validation loss: 2.5840381114713606

Epoch: 100| Step: 0
Training loss: 2.9265236854553223
Validation loss: 2.581735349470569

Epoch: 6| Step: 1
Training loss: 2.022928237915039
Validation loss: 2.5815629907833633

Epoch: 6| Step: 2
Training loss: 2.666994094848633
Validation loss: 2.5817488803658435

Epoch: 6| Step: 3
Training loss: 3.1505401134490967
Validation loss: 2.583243969948061

Epoch: 6| Step: 4
Training loss: 2.4123754501342773
Validation loss: 2.580673622828658

Epoch: 6| Step: 5
Training loss: 3.1410884857177734
Validation loss: 2.581745501487486

Epoch: 6| Step: 6
Training loss: 2.7638683319091797
Validation loss: 2.579440921865484

Epoch: 6| Step: 7
Training loss: 2.312314510345459
Validation loss: 2.5831461414214103

Epoch: 6| Step: 8
Training loss: 2.8440139293670654
Validation loss: 2.5901634334236063

Epoch: 6| Step: 9
Training loss: 2.9416074752807617
Validation loss: 2.597061646881924

Epoch: 6| Step: 10
Training loss: 2.691476345062256
Validation loss: 2.603883320285428

Epoch: 6| Step: 11
Training loss: 2.316042184829712
Validation loss: 2.5944742720614196

Epoch: 6| Step: 12
Training loss: 3.5559425354003906
Validation loss: 2.5869221712953303

Epoch: 6| Step: 13
Training loss: 2.7067711353302
Validation loss: 2.5780861403352473

Epoch: 101| Step: 0
Training loss: 3.0464231967926025
Validation loss: 2.577039057208646

Epoch: 6| Step: 1
Training loss: 2.6338510513305664
Validation loss: 2.581575898713963

Epoch: 6| Step: 2
Training loss: 2.84793758392334
Validation loss: 2.58467609395263

Epoch: 6| Step: 3
Training loss: 2.6931400299072266
Validation loss: 2.5894661000979844

Epoch: 6| Step: 4
Training loss: 2.544750213623047
Validation loss: 2.5916853104868243

Epoch: 6| Step: 5
Training loss: 2.4001426696777344
Validation loss: 2.591289327990624

Epoch: 6| Step: 6
Training loss: 2.965773105621338
Validation loss: 2.5873089093033985

Epoch: 6| Step: 7
Training loss: 3.028076171875
Validation loss: 2.586105723534861

Epoch: 6| Step: 8
Training loss: 3.168273448944092
Validation loss: 2.5884797957635697

Epoch: 6| Step: 9
Training loss: 3.439404010772705
Validation loss: 2.581332301580778

Epoch: 6| Step: 10
Training loss: 2.3295555114746094
Validation loss: 2.5797017261546147

Epoch: 6| Step: 11
Training loss: 2.4247374534606934
Validation loss: 2.5779197062215498

Epoch: 6| Step: 12
Training loss: 2.4517788887023926
Validation loss: 2.57656168681319

Epoch: 6| Step: 13
Training loss: 2.269148349761963
Validation loss: 2.575730385318879

Epoch: 102| Step: 0
Training loss: 2.8576691150665283
Validation loss: 2.5770015793461956

Epoch: 6| Step: 1
Training loss: 2.342867136001587
Validation loss: 2.5763280750602804

Epoch: 6| Step: 2
Training loss: 2.4469947814941406
Validation loss: 2.576368203727148

Epoch: 6| Step: 3
Training loss: 2.614961624145508
Validation loss: 2.581107883043187

Epoch: 6| Step: 4
Training loss: 3.8307478427886963
Validation loss: 2.578031780899212

Epoch: 6| Step: 5
Training loss: 2.926304340362549
Validation loss: 2.580718178902903

Epoch: 6| Step: 6
Training loss: 3.311086416244507
Validation loss: 2.5801843725224978

Epoch: 6| Step: 7
Training loss: 2.1446800231933594
Validation loss: 2.5780227825205815

Epoch: 6| Step: 8
Training loss: 2.64376163482666
Validation loss: 2.57963288599445

Epoch: 6| Step: 9
Training loss: 3.023751735687256
Validation loss: 2.5767526113858787

Epoch: 6| Step: 10
Training loss: 2.4385900497436523
Validation loss: 2.580331943368399

Epoch: 6| Step: 11
Training loss: 2.4901301860809326
Validation loss: 2.5783989121837

Epoch: 6| Step: 12
Training loss: 2.4503591060638428
Validation loss: 2.572828274901195

Epoch: 6| Step: 13
Training loss: 2.968576192855835
Validation loss: 2.57330108457996

Epoch: 103| Step: 0
Training loss: 2.673929214477539
Validation loss: 2.579469765386274

Epoch: 6| Step: 1
Training loss: 3.517642021179199
Validation loss: 2.593881178927678

Epoch: 6| Step: 2
Training loss: 2.7131638526916504
Validation loss: 2.596662082979756

Epoch: 6| Step: 3
Training loss: 2.733105421066284
Validation loss: 2.579070001520136

Epoch: 6| Step: 4
Training loss: 2.6647591590881348
Validation loss: 2.5797092632580827

Epoch: 6| Step: 5
Training loss: 2.800626754760742
Validation loss: 2.5741357393162225

Epoch: 6| Step: 6
Training loss: 3.49462628364563
Validation loss: 2.5745850506649224

Epoch: 6| Step: 7
Training loss: 2.0920016765594482
Validation loss: 2.5760956733457503

Epoch: 6| Step: 8
Training loss: 2.362473964691162
Validation loss: 2.570903149984216

Epoch: 6| Step: 9
Training loss: 2.1727499961853027
Validation loss: 2.571052187232561

Epoch: 6| Step: 10
Training loss: 2.5528430938720703
Validation loss: 2.5735884917679654

Epoch: 6| Step: 11
Training loss: 2.690901517868042
Validation loss: 2.5751575885280484

Epoch: 6| Step: 12
Training loss: 2.7112436294555664
Validation loss: 2.5718296266371206

Epoch: 6| Step: 13
Training loss: 3.468524217605591
Validation loss: 2.568863404694424

Epoch: 104| Step: 0
Training loss: 2.3103654384613037
Validation loss: 2.57011676860112

Epoch: 6| Step: 1
Training loss: 3.0839853286743164
Validation loss: 2.568965350427935

Epoch: 6| Step: 2
Training loss: 2.927086353302002
Validation loss: 2.5681502639606433

Epoch: 6| Step: 3
Training loss: 2.566565752029419
Validation loss: 2.563185853342856

Epoch: 6| Step: 4
Training loss: 2.8116612434387207
Validation loss: 2.5643957635407806

Epoch: 6| Step: 5
Training loss: 3.0405771732330322
Validation loss: 2.5638917928100913

Epoch: 6| Step: 6
Training loss: 2.847632646560669
Validation loss: 2.562696931182697

Epoch: 6| Step: 7
Training loss: 2.675593137741089
Validation loss: 2.562960281166979

Epoch: 6| Step: 8
Training loss: 2.419416904449463
Validation loss: 2.5616382860368296

Epoch: 6| Step: 9
Training loss: 1.80816650390625
Validation loss: 2.560113919678555

Epoch: 6| Step: 10
Training loss: 3.669938802719116
Validation loss: 2.5613823321557816

Epoch: 6| Step: 11
Training loss: 2.1663594245910645
Validation loss: 2.5621523805843887

Epoch: 6| Step: 12
Training loss: 3.254056692123413
Validation loss: 2.5591002484803558

Epoch: 6| Step: 13
Training loss: 2.739954948425293
Validation loss: 2.5584203889293056

Epoch: 105| Step: 0
Training loss: 2.561140537261963
Validation loss: 2.5601358131695817

Epoch: 6| Step: 1
Training loss: 1.440650463104248
Validation loss: 2.558888466127457

Epoch: 6| Step: 2
Training loss: 3.466798782348633
Validation loss: 2.5612945915550314

Epoch: 6| Step: 3
Training loss: 2.7996668815612793
Validation loss: 2.5707759216267574

Epoch: 6| Step: 4
Training loss: 3.3511905670166016
Validation loss: 2.5719344872300343

Epoch: 6| Step: 5
Training loss: 2.4267468452453613
Validation loss: 2.5671198188617663

Epoch: 6| Step: 6
Training loss: 2.8194735050201416
Validation loss: 2.575216426644274

Epoch: 6| Step: 7
Training loss: 3.151878833770752
Validation loss: 2.5743808028518513

Epoch: 6| Step: 8
Training loss: 2.55625319480896
Validation loss: 2.5738246133250575

Epoch: 6| Step: 9
Training loss: 2.3017444610595703
Validation loss: 2.5658077040026264

Epoch: 6| Step: 10
Training loss: 3.221545457839966
Validation loss: 2.5601401893041467

Epoch: 6| Step: 11
Training loss: 2.982347011566162
Validation loss: 2.5596957616908576

Epoch: 6| Step: 12
Training loss: 2.6748709678649902
Validation loss: 2.5615839048098494

Epoch: 6| Step: 13
Training loss: 2.4234302043914795
Validation loss: 2.561361051374866

Epoch: 106| Step: 0
Training loss: 2.664240837097168
Validation loss: 2.563001125089584

Epoch: 6| Step: 1
Training loss: 2.4980931282043457
Validation loss: 2.566073256154214

Epoch: 6| Step: 2
Training loss: 2.3068714141845703
Validation loss: 2.565622845003682

Epoch: 6| Step: 3
Training loss: 2.9821338653564453
Validation loss: 2.5628229751381824

Epoch: 6| Step: 4
Training loss: 3.127020835876465
Validation loss: 2.5594439839804046

Epoch: 6| Step: 5
Training loss: 1.6814171075820923
Validation loss: 2.558210990762198

Epoch: 6| Step: 6
Training loss: 4.249030113220215
Validation loss: 2.558037304109143

Epoch: 6| Step: 7
Training loss: 2.924731492996216
Validation loss: 2.5559182064507597

Epoch: 6| Step: 8
Training loss: 2.1636135578155518
Validation loss: 2.5574023723602295

Epoch: 6| Step: 9
Training loss: 3.275116443634033
Validation loss: 2.553918433445756

Epoch: 6| Step: 10
Training loss: 2.1894257068634033
Validation loss: 2.554618763667281

Epoch: 6| Step: 11
Training loss: 2.1764070987701416
Validation loss: 2.556965599777878

Epoch: 6| Step: 12
Training loss: 3.1164963245391846
Validation loss: 2.5617447873597503

Epoch: 6| Step: 13
Training loss: 3.0131888389587402
Validation loss: 2.5635093489000873

Epoch: 107| Step: 0
Training loss: 2.6152453422546387
Validation loss: 2.572558218433011

Epoch: 6| Step: 1
Training loss: 2.474482297897339
Validation loss: 2.581827522605978

Epoch: 6| Step: 2
Training loss: 2.5197322368621826
Validation loss: 2.5841290258592173

Epoch: 6| Step: 3
Training loss: 2.16923189163208
Validation loss: 2.586268340387652

Epoch: 6| Step: 4
Training loss: 2.068481206893921
Validation loss: 2.560257973209504

Epoch: 6| Step: 5
Training loss: 2.3892648220062256
Validation loss: 2.555918001359509

Epoch: 6| Step: 6
Training loss: 2.435159206390381
Validation loss: 2.558078040358841

Epoch: 6| Step: 7
Training loss: 2.5551095008850098
Validation loss: 2.5668936749940277

Epoch: 6| Step: 8
Training loss: 3.006808280944824
Validation loss: 2.56851170139928

Epoch: 6| Step: 9
Training loss: 3.503979444503784
Validation loss: 2.5794088712302585

Epoch: 6| Step: 10
Training loss: 3.550034523010254
Validation loss: 2.5824919157130743

Epoch: 6| Step: 11
Training loss: 3.0853095054626465
Validation loss: 2.5835034103803736

Epoch: 6| Step: 12
Training loss: 3.6372077465057373
Validation loss: 2.5691770327988492

Epoch: 6| Step: 13
Training loss: 2.2488768100738525
Validation loss: 2.5634527872967463

Epoch: 108| Step: 0
Training loss: 2.5639126300811768
Validation loss: 2.5597157657787366

Epoch: 6| Step: 1
Training loss: 1.906058430671692
Validation loss: 2.557643682725968

Epoch: 6| Step: 2
Training loss: 3.5309886932373047
Validation loss: 2.5575059947147163

Epoch: 6| Step: 3
Training loss: 3.20041561126709
Validation loss: 2.5551295806002874

Epoch: 6| Step: 4
Training loss: 3.4493143558502197
Validation loss: 2.553915318622384

Epoch: 6| Step: 5
Training loss: 2.687689781188965
Validation loss: 2.5495191261332524

Epoch: 6| Step: 6
Training loss: 2.8900976181030273
Validation loss: 2.553716036581224

Epoch: 6| Step: 7
Training loss: 2.2530624866485596
Validation loss: 2.5591914371777604

Epoch: 6| Step: 8
Training loss: 1.7136067152023315
Validation loss: 2.563744778274208

Epoch: 6| Step: 9
Training loss: 3.3258800506591797
Validation loss: 2.567135210960142

Epoch: 6| Step: 10
Training loss: 2.4980826377868652
Validation loss: 2.5641606674399426

Epoch: 6| Step: 11
Training loss: 2.839937925338745
Validation loss: 2.5622163075272755

Epoch: 6| Step: 12
Training loss: 3.1055166721343994
Validation loss: 2.559255599975586

Epoch: 6| Step: 13
Training loss: 1.7725999355316162
Validation loss: 2.5533978528873895

Epoch: 109| Step: 0
Training loss: 1.4666645526885986
Validation loss: 2.5499663019693024

Epoch: 6| Step: 1
Training loss: 3.0177440643310547
Validation loss: 2.5433355069929555

Epoch: 6| Step: 2
Training loss: 2.434281349182129
Validation loss: 2.5460722061895553

Epoch: 6| Step: 3
Training loss: 2.017453908920288
Validation loss: 2.551247996668662

Epoch: 6| Step: 4
Training loss: 3.242036819458008
Validation loss: 2.5485697574512933

Epoch: 6| Step: 5
Training loss: 2.622932195663452
Validation loss: 2.549307110489056

Epoch: 6| Step: 6
Training loss: 2.287491798400879
Validation loss: 2.5530535072408695

Epoch: 6| Step: 7
Training loss: 3.48199462890625
Validation loss: 2.5609685528662895

Epoch: 6| Step: 8
Training loss: 2.661405086517334
Validation loss: 2.5523333318771853

Epoch: 6| Step: 9
Training loss: 2.7479405403137207
Validation loss: 2.54873562371859

Epoch: 6| Step: 10
Training loss: 2.778188705444336
Validation loss: 2.544186503656449

Epoch: 6| Step: 11
Training loss: 3.2622952461242676
Validation loss: 2.5451507337631716

Epoch: 6| Step: 12
Training loss: 2.454371690750122
Validation loss: 2.5466268549683275

Epoch: 6| Step: 13
Training loss: 4.313296794891357
Validation loss: 2.54730631971872

Epoch: 110| Step: 0
Training loss: 3.388248920440674
Validation loss: 2.5487287223979993

Epoch: 6| Step: 1
Training loss: 2.252150297164917
Validation loss: 2.5460103275955364

Epoch: 6| Step: 2
Training loss: 2.5749995708465576
Validation loss: 2.54458031090357

Epoch: 6| Step: 3
Training loss: 2.8943746089935303
Validation loss: 2.543993724289761

Epoch: 6| Step: 4
Training loss: 2.3486738204956055
Validation loss: 2.5469222145695842

Epoch: 6| Step: 5
Training loss: 2.1016907691955566
Validation loss: 2.554469405963857

Epoch: 6| Step: 6
Training loss: 3.1754188537597656
Validation loss: 2.556761618583433

Epoch: 6| Step: 7
Training loss: 2.4541454315185547
Validation loss: 2.5492427913091515

Epoch: 6| Step: 8
Training loss: 2.088986396789551
Validation loss: 2.5445464323925715

Epoch: 6| Step: 9
Training loss: 2.9525082111358643
Validation loss: 2.5432789658987396

Epoch: 6| Step: 10
Training loss: 2.6994447708129883
Validation loss: 2.542126694033223

Epoch: 6| Step: 11
Training loss: 3.1155295372009277
Validation loss: 2.542677399932697

Epoch: 6| Step: 12
Training loss: 2.7756216526031494
Validation loss: 2.5389268372648504

Epoch: 6| Step: 13
Training loss: 3.61673903465271
Validation loss: 2.540866249351091

Epoch: 111| Step: 0
Training loss: 2.481203079223633
Validation loss: 2.543180145243163

Epoch: 6| Step: 1
Training loss: 1.778206467628479
Validation loss: 2.542380215019308

Epoch: 6| Step: 2
Training loss: 2.7267396450042725
Validation loss: 2.5462064563587146

Epoch: 6| Step: 3
Training loss: 2.657120943069458
Validation loss: 2.5449539794716785

Epoch: 6| Step: 4
Training loss: 2.537454605102539
Validation loss: 2.543286290220035

Epoch: 6| Step: 5
Training loss: 2.5241055488586426
Validation loss: 2.550278009906892

Epoch: 6| Step: 6
Training loss: 2.53989315032959
Validation loss: 2.54114176893747

Epoch: 6| Step: 7
Training loss: 2.821671962738037
Validation loss: 2.5368692797999226

Epoch: 6| Step: 8
Training loss: 3.369978666305542
Validation loss: 2.5382203312330347

Epoch: 6| Step: 9
Training loss: 2.8813111782073975
Validation loss: 2.5386890313958608

Epoch: 6| Step: 10
Training loss: 3.399585723876953
Validation loss: 2.537910281970937

Epoch: 6| Step: 11
Training loss: 2.5887813568115234
Validation loss: 2.537495208042924

Epoch: 6| Step: 12
Training loss: 2.92199444770813
Validation loss: 2.5370452711659093

Epoch: 6| Step: 13
Training loss: 2.7931299209594727
Validation loss: 2.5369923448049896

Epoch: 112| Step: 0
Training loss: 2.27217435836792
Validation loss: 2.5397040049235025

Epoch: 6| Step: 1
Training loss: 2.604377269744873
Validation loss: 2.535192871606478

Epoch: 6| Step: 2
Training loss: 2.6619672775268555
Validation loss: 2.5369472631844143

Epoch: 6| Step: 3
Training loss: 2.8858225345611572
Validation loss: 2.53873953511638

Epoch: 6| Step: 4
Training loss: 2.6773550510406494
Validation loss: 2.5375066495710805

Epoch: 6| Step: 5
Training loss: 1.965591549873352
Validation loss: 2.5399284516611407

Epoch: 6| Step: 6
Training loss: 3.6063506603240967
Validation loss: 2.542299444957446

Epoch: 6| Step: 7
Training loss: 2.5128140449523926
Validation loss: 2.5411218033042005

Epoch: 6| Step: 8
Training loss: 2.35440731048584
Validation loss: 2.5403693363230717

Epoch: 6| Step: 9
Training loss: 3.8924176692962646
Validation loss: 2.5398132749783096

Epoch: 6| Step: 10
Training loss: 2.4992504119873047
Validation loss: 2.5413343009128364

Epoch: 6| Step: 11
Training loss: 2.7160587310791016
Validation loss: 2.541944883202994

Epoch: 6| Step: 12
Training loss: 2.4813692569732666
Validation loss: 2.54889258774378

Epoch: 6| Step: 13
Training loss: 2.947681427001953
Validation loss: 2.544772381423622

Epoch: 113| Step: 0
Training loss: 2.7543060779571533
Validation loss: 2.541207616047193

Epoch: 6| Step: 1
Training loss: 2.8627066612243652
Validation loss: 2.5354357381020822

Epoch: 6| Step: 2
Training loss: 2.5623421669006348
Validation loss: 2.5343910750522407

Epoch: 6| Step: 3
Training loss: 2.897210121154785
Validation loss: 2.5337394129845405

Epoch: 6| Step: 4
Training loss: 3.102745294570923
Validation loss: 2.5312139167580554

Epoch: 6| Step: 5
Training loss: 2.6800873279571533
Validation loss: 2.5305823305601716

Epoch: 6| Step: 6
Training loss: 2.6703715324401855
Validation loss: 2.5323466665001324

Epoch: 6| Step: 7
Training loss: 2.731330156326294
Validation loss: 2.5317525530374176

Epoch: 6| Step: 8
Training loss: 3.035316228866577
Validation loss: 2.530702401232976

Epoch: 6| Step: 9
Training loss: 2.169168472290039
Validation loss: 2.528654121583508

Epoch: 6| Step: 10
Training loss: 2.0843265056610107
Validation loss: 2.53028259482435

Epoch: 6| Step: 11
Training loss: 3.0556442737579346
Validation loss: 2.528971525930589

Epoch: 6| Step: 12
Training loss: 2.263354539871216
Validation loss: 2.529120393978652

Epoch: 6| Step: 13
Training loss: 3.4261159896850586
Validation loss: 2.529590686162313

Epoch: 114| Step: 0
Training loss: 3.2835216522216797
Validation loss: 2.5288279543640795

Epoch: 6| Step: 1
Training loss: 2.3382833003997803
Validation loss: 2.528359992529756

Epoch: 6| Step: 2
Training loss: 3.492621898651123
Validation loss: 2.529392921796409

Epoch: 6| Step: 3
Training loss: 3.431774139404297
Validation loss: 2.53193425619474

Epoch: 6| Step: 4
Training loss: 2.4409942626953125
Validation loss: 2.528991199308826

Epoch: 6| Step: 5
Training loss: 2.9158565998077393
Validation loss: 2.533807167442896

Epoch: 6| Step: 6
Training loss: 2.120260238647461
Validation loss: 2.532364091565532

Epoch: 6| Step: 7
Training loss: 3.073787212371826
Validation loss: 2.5276679479947655

Epoch: 6| Step: 8
Training loss: 1.8474009037017822
Validation loss: 2.5308372820577314

Epoch: 6| Step: 9
Training loss: 2.531782627105713
Validation loss: 2.529276854248457

Epoch: 6| Step: 10
Training loss: 2.0798206329345703
Validation loss: 2.5373561715566986

Epoch: 6| Step: 11
Training loss: 3.0104146003723145
Validation loss: 2.547262937791886

Epoch: 6| Step: 12
Training loss: 2.48101806640625
Validation loss: 2.5448998943451913

Epoch: 6| Step: 13
Training loss: 2.983126640319824
Validation loss: 2.5294261927245767

Epoch: 115| Step: 0
Training loss: 2.3361291885375977
Validation loss: 2.528185042001868

Epoch: 6| Step: 1
Training loss: 2.337559938430786
Validation loss: 2.5306354799578266

Epoch: 6| Step: 2
Training loss: 2.9746150970458984
Validation loss: 2.528830441095496

Epoch: 6| Step: 3
Training loss: 3.1127712726593018
Validation loss: 2.529496274968629

Epoch: 6| Step: 4
Training loss: 2.3189728260040283
Validation loss: 2.530331491142191

Epoch: 6| Step: 5
Training loss: 2.718904495239258
Validation loss: 2.529499912774691

Epoch: 6| Step: 6
Training loss: 2.786956310272217
Validation loss: 2.527415342228387

Epoch: 6| Step: 7
Training loss: 3.3669300079345703
Validation loss: 2.526566848959974

Epoch: 6| Step: 8
Training loss: 3.1398305892944336
Validation loss: 2.5278428113588722

Epoch: 6| Step: 9
Training loss: 3.247781753540039
Validation loss: 2.53041668604779

Epoch: 6| Step: 10
Training loss: 2.3296217918395996
Validation loss: 2.531781506794755

Epoch: 6| Step: 11
Training loss: 2.223986864089966
Validation loss: 2.5390968194571872

Epoch: 6| Step: 12
Training loss: 1.9057272672653198
Validation loss: 2.532353913912209

Epoch: 6| Step: 13
Training loss: 3.426530599594116
Validation loss: 2.533459158353908

Epoch: 116| Step: 0
Training loss: 3.852933406829834
Validation loss: 2.528001308441162

Epoch: 6| Step: 1
Training loss: 2.3156399726867676
Validation loss: 2.5272784361275296

Epoch: 6| Step: 2
Training loss: 2.782560110092163
Validation loss: 2.526808969436153

Epoch: 6| Step: 3
Training loss: 1.8389569520950317
Validation loss: 2.5294244968762962

Epoch: 6| Step: 4
Training loss: 2.5087718963623047
Validation loss: 2.525503299569571

Epoch: 6| Step: 5
Training loss: 3.2689695358276367
Validation loss: 2.5237155370814826

Epoch: 6| Step: 6
Training loss: 2.6584231853485107
Validation loss: 2.5230390025723364

Epoch: 6| Step: 7
Training loss: 2.7795565128326416
Validation loss: 2.5278949660639607

Epoch: 6| Step: 8
Training loss: 2.846222400665283
Validation loss: 2.5263829795263146

Epoch: 6| Step: 9
Training loss: 3.3164870738983154
Validation loss: 2.5282136547950005

Epoch: 6| Step: 10
Training loss: 1.9696587324142456
Validation loss: 2.535672095514113

Epoch: 6| Step: 11
Training loss: 2.9328951835632324
Validation loss: 2.5341793593539985

Epoch: 6| Step: 12
Training loss: 2.4809670448303223
Validation loss: 2.533770845782372

Epoch: 6| Step: 13
Training loss: 1.9408049583435059
Validation loss: 2.5266612883537047

Epoch: 117| Step: 0
Training loss: 2.5430095195770264
Validation loss: 2.5269105485690537

Epoch: 6| Step: 1
Training loss: 2.1075074672698975
Validation loss: 2.530113579124533

Epoch: 6| Step: 2
Training loss: 3.4892168045043945
Validation loss: 2.5300536360791934

Epoch: 6| Step: 3
Training loss: 2.672083854675293
Validation loss: 2.5258816596000426

Epoch: 6| Step: 4
Training loss: 3.1078615188598633
Validation loss: 2.5189315144733717

Epoch: 6| Step: 5
Training loss: 3.1791067123413086
Validation loss: 2.518862129539572

Epoch: 6| Step: 6
Training loss: 2.923250198364258
Validation loss: 2.5266258203855125

Epoch: 6| Step: 7
Training loss: 2.3061208724975586
Validation loss: 2.53365312340439

Epoch: 6| Step: 8
Training loss: 2.7778658866882324
Validation loss: 2.5430084428479596

Epoch: 6| Step: 9
Training loss: 2.8900036811828613
Validation loss: 2.5766961574554443

Epoch: 6| Step: 10
Training loss: 2.042313575744629
Validation loss: 2.5846114902086157

Epoch: 6| Step: 11
Training loss: 2.30729603767395
Validation loss: 2.5937069462191675

Epoch: 6| Step: 12
Training loss: 3.247836112976074
Validation loss: 2.573765936718192

Epoch: 6| Step: 13
Training loss: 2.296027660369873
Validation loss: 2.5386078690969818

Epoch: 118| Step: 0
Training loss: 3.6891427040100098
Validation loss: 2.5284582261116273

Epoch: 6| Step: 1
Training loss: 2.1137232780456543
Validation loss: 2.5165064129778134

Epoch: 6| Step: 2
Training loss: 3.2238802909851074
Validation loss: 2.5179595126900622

Epoch: 6| Step: 3
Training loss: 3.067178249359131
Validation loss: 2.519353189776021

Epoch: 6| Step: 4
Training loss: 3.0484986305236816
Validation loss: 2.525290145668932

Epoch: 6| Step: 5
Training loss: 2.2002899646759033
Validation loss: 2.5394360019314672

Epoch: 6| Step: 6
Training loss: 2.094944477081299
Validation loss: 2.5321504454458914

Epoch: 6| Step: 7
Training loss: 2.5780029296875
Validation loss: 2.5267989174012215

Epoch: 6| Step: 8
Training loss: 2.7832353115081787
Validation loss: 2.5264649339901504

Epoch: 6| Step: 9
Training loss: 2.226825475692749
Validation loss: 2.5207006828759306

Epoch: 6| Step: 10
Training loss: 2.4953882694244385
Validation loss: 2.5191661901371454

Epoch: 6| Step: 11
Training loss: 3.116408109664917
Validation loss: 2.519219749717302

Epoch: 6| Step: 12
Training loss: 2.405215263366699
Validation loss: 2.517647925243583

Epoch: 6| Step: 13
Training loss: 2.728792190551758
Validation loss: 2.5203403324209233

Epoch: 119| Step: 0
Training loss: 2.4777116775512695
Validation loss: 2.514431307392736

Epoch: 6| Step: 1
Training loss: 3.507596969604492
Validation loss: 2.5148350705382643

Epoch: 6| Step: 2
Training loss: 3.028859853744507
Validation loss: 2.5124036983777116

Epoch: 6| Step: 3
Training loss: 2.6779019832611084
Validation loss: 2.5138768431960896

Epoch: 6| Step: 4
Training loss: 2.3194100856781006
Validation loss: 2.5163307228396015

Epoch: 6| Step: 5
Training loss: 2.245758533477783
Validation loss: 2.515638989786948

Epoch: 6| Step: 6
Training loss: 2.798375129699707
Validation loss: 2.5177574080805623

Epoch: 6| Step: 7
Training loss: 3.5401439666748047
Validation loss: 2.5149444610841813

Epoch: 6| Step: 8
Training loss: 1.9348914623260498
Validation loss: 2.516391031203731

Epoch: 6| Step: 9
Training loss: 2.486025810241699
Validation loss: 2.514184146799067

Epoch: 6| Step: 10
Training loss: 2.673872947692871
Validation loss: 2.516431180379724

Epoch: 6| Step: 11
Training loss: 2.398940086364746
Validation loss: 2.517588310344245

Epoch: 6| Step: 12
Training loss: 2.8187367916107178
Validation loss: 2.5219798190619356

Epoch: 6| Step: 13
Training loss: 3.016848087310791
Validation loss: 2.516284317098638

Epoch: 120| Step: 0
Training loss: 2.4321718215942383
Validation loss: 2.5152786136955343

Epoch: 6| Step: 1
Training loss: 2.8279759883880615
Validation loss: 2.515753830632856

Epoch: 6| Step: 2
Training loss: 2.938701629638672
Validation loss: 2.5111024789912726

Epoch: 6| Step: 3
Training loss: 1.8525733947753906
Validation loss: 2.5086961612906507

Epoch: 6| Step: 4
Training loss: 2.5774717330932617
Validation loss: 2.514032784328666

Epoch: 6| Step: 5
Training loss: 2.4414377212524414
Validation loss: 2.5120062212790213

Epoch: 6| Step: 6
Training loss: 2.6594529151916504
Validation loss: 2.5157044113323255

Epoch: 6| Step: 7
Training loss: 2.3115506172180176
Validation loss: 2.5154924341427383

Epoch: 6| Step: 8
Training loss: 2.837942123413086
Validation loss: 2.513771439111361

Epoch: 6| Step: 9
Training loss: 2.4700794219970703
Validation loss: 2.5110419386176654

Epoch: 6| Step: 10
Training loss: 3.0125138759613037
Validation loss: 2.5069805550318893

Epoch: 6| Step: 11
Training loss: 3.103952169418335
Validation loss: 2.5091033366418656

Epoch: 6| Step: 12
Training loss: 3.0835275650024414
Validation loss: 2.512174642214211

Epoch: 6| Step: 13
Training loss: 3.48956561088562
Validation loss: 2.509798801073464

Epoch: 121| Step: 0
Training loss: 2.469697952270508
Validation loss: 2.5055918796088106

Epoch: 6| Step: 1
Training loss: 1.9304800033569336
Validation loss: 2.5097419882333405

Epoch: 6| Step: 2
Training loss: 3.094407081604004
Validation loss: 2.5103822933730258

Epoch: 6| Step: 3
Training loss: 2.905085563659668
Validation loss: 2.507051390986289

Epoch: 6| Step: 4
Training loss: 2.4088821411132812
Validation loss: 2.510462608388675

Epoch: 6| Step: 5
Training loss: 2.6602797508239746
Validation loss: 2.5057004702988492

Epoch: 6| Step: 6
Training loss: 2.7777838706970215
Validation loss: 2.5060611847908265

Epoch: 6| Step: 7
Training loss: 2.2161459922790527
Validation loss: 2.5049563095133793

Epoch: 6| Step: 8
Training loss: 3.3669328689575195
Validation loss: 2.506614444076374

Epoch: 6| Step: 9
Training loss: 2.7622551918029785
Validation loss: 2.512021618504678

Epoch: 6| Step: 10
Training loss: 3.0507493019104004
Validation loss: 2.510551734637189

Epoch: 6| Step: 11
Training loss: 3.6024203300476074
Validation loss: 2.5135358200278333

Epoch: 6| Step: 12
Training loss: 2.7186942100524902
Validation loss: 2.5182914426249843

Epoch: 6| Step: 13
Training loss: 0.91926109790802
Validation loss: 2.5124102202794885

Epoch: 122| Step: 0
Training loss: 2.446653366088867
Validation loss: 2.5111122785076017

Epoch: 6| Step: 1
Training loss: 1.8399577140808105
Validation loss: 2.509681776005735

Epoch: 6| Step: 2
Training loss: 2.573820114135742
Validation loss: 2.5153918907206547

Epoch: 6| Step: 3
Training loss: 2.546215772628784
Validation loss: 2.521988426485369

Epoch: 6| Step: 4
Training loss: 2.3823068141937256
Validation loss: 2.509895360598

Epoch: 6| Step: 5
Training loss: 3.115342617034912
Validation loss: 2.5097029875683528

Epoch: 6| Step: 6
Training loss: 2.590266227722168
Validation loss: 2.503491673418271

Epoch: 6| Step: 7
Training loss: 2.390544891357422
Validation loss: 2.5026253654110815

Epoch: 6| Step: 8
Training loss: 3.1102943420410156
Validation loss: 2.5011388153158207

Epoch: 6| Step: 9
Training loss: 2.9454760551452637
Validation loss: 2.50187418025027

Epoch: 6| Step: 10
Training loss: 2.8396213054656982
Validation loss: 2.504200070135055

Epoch: 6| Step: 11
Training loss: 3.1024680137634277
Validation loss: 2.50428964502068

Epoch: 6| Step: 12
Training loss: 3.0005321502685547
Validation loss: 2.5071080858989427

Epoch: 6| Step: 13
Training loss: 2.794321298599243
Validation loss: 2.506963150475615

Epoch: 123| Step: 0
Training loss: 2.1645708084106445
Validation loss: 2.5062396423791045

Epoch: 6| Step: 1
Training loss: 3.2658395767211914
Validation loss: 2.5065602128223707

Epoch: 6| Step: 2
Training loss: 2.290386915206909
Validation loss: 2.505080223083496

Epoch: 6| Step: 3
Training loss: 3.092636823654175
Validation loss: 2.5040928881655455

Epoch: 6| Step: 4
Training loss: 2.8878729343414307
Validation loss: 2.502279968671901

Epoch: 6| Step: 5
Training loss: 3.1569933891296387
Validation loss: 2.5003044477073093

Epoch: 6| Step: 6
Training loss: 2.4129433631896973
Validation loss: 2.5003433714630785

Epoch: 6| Step: 7
Training loss: 3.3869705200195312
Validation loss: 2.504819964849821

Epoch: 6| Step: 8
Training loss: 2.391526460647583
Validation loss: 2.5134164415380007

Epoch: 6| Step: 9
Training loss: 2.929868221282959
Validation loss: 2.5173990239379225

Epoch: 6| Step: 10
Training loss: 3.3881442546844482
Validation loss: 2.5220745584016204

Epoch: 6| Step: 11
Training loss: 1.9425921440124512
Validation loss: 2.5163565656190277

Epoch: 6| Step: 12
Training loss: 2.4204139709472656
Validation loss: 2.5142370962327525

Epoch: 6| Step: 13
Training loss: 1.480133295059204
Validation loss: 2.520184706616145

Epoch: 124| Step: 0
Training loss: 2.7942681312561035
Validation loss: 2.524639137329594

Epoch: 6| Step: 1
Training loss: 2.5077919960021973
Validation loss: 2.5225829001395934

Epoch: 6| Step: 2
Training loss: 2.132305145263672
Validation loss: 2.5188356009862756

Epoch: 6| Step: 3
Training loss: 2.8053817749023438
Validation loss: 2.499070131650535

Epoch: 6| Step: 4
Training loss: 2.4290549755096436
Validation loss: 2.4970830948122087

Epoch: 6| Step: 5
Training loss: 2.9045698642730713
Validation loss: 2.4951493611899753

Epoch: 6| Step: 6
Training loss: 3.0368399620056152
Validation loss: 2.495993160432385

Epoch: 6| Step: 7
Training loss: 3.1445472240448
Validation loss: 2.497577582636187

Epoch: 6| Step: 8
Training loss: 2.7576632499694824
Validation loss: 2.4963683594939527

Epoch: 6| Step: 9
Training loss: 2.2462105751037598
Validation loss: 2.4967425305356263

Epoch: 6| Step: 10
Training loss: 2.651484489440918
Validation loss: 2.498537145635133

Epoch: 6| Step: 11
Training loss: 2.6745524406433105
Validation loss: 2.496890160345262

Epoch: 6| Step: 12
Training loss: 2.9585177898406982
Validation loss: 2.4986404065162904

Epoch: 6| Step: 13
Training loss: 2.6288974285125732
Validation loss: 2.508521910636656

Epoch: 125| Step: 0
Training loss: 2.7703144550323486
Validation loss: 2.5082106564634588

Epoch: 6| Step: 1
Training loss: 2.26743221282959
Validation loss: 2.5231682305694907

Epoch: 6| Step: 2
Training loss: 2.8236539363861084
Validation loss: 2.524808563211913

Epoch: 6| Step: 3
Training loss: 2.953063488006592
Validation loss: 2.5150338936877508

Epoch: 6| Step: 4
Training loss: 2.7618556022644043
Validation loss: 2.4996324405875257

Epoch: 6| Step: 5
Training loss: 2.994999408721924
Validation loss: 2.4959004579051847

Epoch: 6| Step: 6
Training loss: 2.330406904220581
Validation loss: 2.4974982430857997

Epoch: 6| Step: 7
Training loss: 3.7596232891082764
Validation loss: 2.4942726730018534

Epoch: 6| Step: 8
Training loss: 1.9534400701522827
Validation loss: 2.496163116988315

Epoch: 6| Step: 9
Training loss: 2.696908473968506
Validation loss: 2.49576388635943

Epoch: 6| Step: 10
Training loss: 3.0369014739990234
Validation loss: 2.493365773590662

Epoch: 6| Step: 11
Training loss: 2.5615527629852295
Validation loss: 2.4936307835322555

Epoch: 6| Step: 12
Training loss: 2.1098246574401855
Validation loss: 2.492997310494864

Epoch: 6| Step: 13
Training loss: 2.5834057331085205
Validation loss: 2.493666333536948

Epoch: 126| Step: 0
Training loss: 3.0081255435943604
Validation loss: 2.4955044484907583

Epoch: 6| Step: 1
Training loss: 3.1071245670318604
Validation loss: 2.497298220152496

Epoch: 6| Step: 2
Training loss: 3.4840619564056396
Validation loss: 2.4986488050030125

Epoch: 6| Step: 3
Training loss: 2.311183452606201
Validation loss: 2.506553267919889

Epoch: 6| Step: 4
Training loss: 2.3931636810302734
Validation loss: 2.512744257527013

Epoch: 6| Step: 5
Training loss: 2.154179573059082
Validation loss: 2.505394481843518

Epoch: 6| Step: 6
Training loss: 2.8433756828308105
Validation loss: 2.5110862614006124

Epoch: 6| Step: 7
Training loss: 2.2770180702209473
Validation loss: 2.509303795394077

Epoch: 6| Step: 8
Training loss: 2.585214138031006
Validation loss: 2.504990819961794

Epoch: 6| Step: 9
Training loss: 3.412599563598633
Validation loss: 2.504869846887486

Epoch: 6| Step: 10
Training loss: 2.042431592941284
Validation loss: 2.4924130311576267

Epoch: 6| Step: 11
Training loss: 2.406416416168213
Validation loss: 2.4902009297442693

Epoch: 6| Step: 12
Training loss: 2.508934497833252
Validation loss: 2.4869702503245366

Epoch: 6| Step: 13
Training loss: 3.2777504920959473
Validation loss: 2.4865953640271257

Epoch: 127| Step: 0
Training loss: 1.5613120794296265
Validation loss: 2.4886842927625104

Epoch: 6| Step: 1
Training loss: 2.2443299293518066
Validation loss: 2.488577627366589

Epoch: 6| Step: 2
Training loss: 3.6464481353759766
Validation loss: 2.4912943993845293

Epoch: 6| Step: 3
Training loss: 2.733020544052124
Validation loss: 2.491378099687638

Epoch: 6| Step: 4
Training loss: 2.580517530441284
Validation loss: 2.491335763726183

Epoch: 6| Step: 5
Training loss: 2.8612759113311768
Validation loss: 2.493884337845669

Epoch: 6| Step: 6
Training loss: 2.8225255012512207
Validation loss: 2.4947437496595484

Epoch: 6| Step: 7
Training loss: 2.8970589637756348
Validation loss: 2.4946465440975722

Epoch: 6| Step: 8
Training loss: 2.223627805709839
Validation loss: 2.4919242166703746

Epoch: 6| Step: 9
Training loss: 3.5497353076934814
Validation loss: 2.4925421309727493

Epoch: 6| Step: 10
Training loss: 2.4341084957122803
Validation loss: 2.49263624734776

Epoch: 6| Step: 11
Training loss: 2.9899301528930664
Validation loss: 2.489108016414027

Epoch: 6| Step: 12
Training loss: 2.4243876934051514
Validation loss: 2.4932054088961695

Epoch: 6| Step: 13
Training loss: 2.567944049835205
Validation loss: 2.4863520899126605

Epoch: 128| Step: 0
Training loss: 3.356360912322998
Validation loss: 2.4855755195822766

Epoch: 6| Step: 1
Training loss: 2.6455273628234863
Validation loss: 2.490242194103938

Epoch: 6| Step: 2
Training loss: 2.719667673110962
Validation loss: 2.501944165075979

Epoch: 6| Step: 3
Training loss: 2.2695486545562744
Validation loss: 2.504837618079237

Epoch: 6| Step: 4
Training loss: 2.417469024658203
Validation loss: 2.514555920836746

Epoch: 6| Step: 5
Training loss: 3.1366682052612305
Validation loss: 2.5080130305341495

Epoch: 6| Step: 6
Training loss: 2.4377198219299316
Validation loss: 2.507014669397826

Epoch: 6| Step: 7
Training loss: 2.531797409057617
Validation loss: 2.504238377335251

Epoch: 6| Step: 8
Training loss: 2.7223927974700928
Validation loss: 2.491458231402982

Epoch: 6| Step: 9
Training loss: 2.7720367908477783
Validation loss: 2.4832819713059293

Epoch: 6| Step: 10
Training loss: 2.6169092655181885
Validation loss: 2.4838461440096617

Epoch: 6| Step: 11
Training loss: 2.449939250946045
Validation loss: 2.4846962651898785

Epoch: 6| Step: 12
Training loss: 2.8613662719726562
Validation loss: 2.486743134836997

Epoch: 6| Step: 13
Training loss: 2.5648367404937744
Validation loss: 2.4866823688630135

Epoch: 129| Step: 0
Training loss: 3.1681089401245117
Validation loss: 2.493764600446147

Epoch: 6| Step: 1
Training loss: 2.677196741104126
Validation loss: 2.4927791395495014

Epoch: 6| Step: 2
Training loss: 3.1278085708618164
Validation loss: 2.4923131376184444

Epoch: 6| Step: 3
Training loss: 2.2683677673339844
Validation loss: 2.4924133362308627

Epoch: 6| Step: 4
Training loss: 2.3773930072784424
Validation loss: 2.4891293023222234

Epoch: 6| Step: 5
Training loss: 2.730952739715576
Validation loss: 2.4888199080703077

Epoch: 6| Step: 6
Training loss: 2.9915852546691895
Validation loss: 2.484221863490279

Epoch: 6| Step: 7
Training loss: 2.696110248565674
Validation loss: 2.4859125152710946

Epoch: 6| Step: 8
Training loss: 1.9910703897476196
Validation loss: 2.48433082847185

Epoch: 6| Step: 9
Training loss: 2.21498966217041
Validation loss: 2.4855573484974522

Epoch: 6| Step: 10
Training loss: 3.243687152862549
Validation loss: 2.484504158778857

Epoch: 6| Step: 11
Training loss: 3.3781542778015137
Validation loss: 2.4882575952878563

Epoch: 6| Step: 12
Training loss: 2.4138355255126953
Validation loss: 2.4892144639004945

Epoch: 6| Step: 13
Training loss: 2.029919147491455
Validation loss: 2.4882747819346767

Epoch: 130| Step: 0
Training loss: 2.8899755477905273
Validation loss: 2.4913840473339124

Epoch: 6| Step: 1
Training loss: 2.163907527923584
Validation loss: 2.488816471510036

Epoch: 6| Step: 2
Training loss: 2.721904993057251
Validation loss: 2.4913496637857087

Epoch: 6| Step: 3
Training loss: 1.9605324268341064
Validation loss: 2.4982653446094965

Epoch: 6| Step: 4
Training loss: 2.5971567630767822
Validation loss: 2.5011639056667203

Epoch: 6| Step: 5
Training loss: 3.587965965270996
Validation loss: 2.5069211670147475

Epoch: 6| Step: 6
Training loss: 2.2157676219940186
Validation loss: 2.5128577652797905

Epoch: 6| Step: 7
Training loss: 2.8191821575164795
Validation loss: 2.510638975328015

Epoch: 6| Step: 8
Training loss: 2.4087624549865723
Validation loss: 2.501915454864502

Epoch: 6| Step: 9
Training loss: 2.2367701530456543
Validation loss: 2.5016911004179265

Epoch: 6| Step: 10
Training loss: 3.1062536239624023
Validation loss: 2.486996830150645

Epoch: 6| Step: 11
Training loss: 2.893711805343628
Validation loss: 2.485174689241635

Epoch: 6| Step: 12
Training loss: 2.814500093460083
Validation loss: 2.4796395814546974

Epoch: 6| Step: 13
Training loss: 3.294804573059082
Validation loss: 2.4777064143970446

Epoch: 131| Step: 0
Training loss: 2.144469976425171
Validation loss: 2.480023830167709

Epoch: 6| Step: 1
Training loss: 2.3265230655670166
Validation loss: 2.4815217961547194

Epoch: 6| Step: 2
Training loss: 2.622844696044922
Validation loss: 2.4828495620399393

Epoch: 6| Step: 3
Training loss: 1.8803818225860596
Validation loss: 2.4828915903645177

Epoch: 6| Step: 4
Training loss: 2.5666255950927734
Validation loss: 2.4810217272850776

Epoch: 6| Step: 5
Training loss: 3.0440073013305664
Validation loss: 2.478457781576341

Epoch: 6| Step: 6
Training loss: 3.2317428588867188
Validation loss: 2.4858054140562653

Epoch: 6| Step: 7
Training loss: 2.1199562549591064
Validation loss: 2.482753645989203

Epoch: 6| Step: 8
Training loss: 2.8387274742126465
Validation loss: 2.478118101755778

Epoch: 6| Step: 9
Training loss: 2.509577751159668
Validation loss: 2.475978648790749

Epoch: 6| Step: 10
Training loss: 3.1598052978515625
Validation loss: 2.477697105817897

Epoch: 6| Step: 11
Training loss: 3.5124623775482178
Validation loss: 2.4808860389135217

Epoch: 6| Step: 12
Training loss: 3.1763553619384766
Validation loss: 2.4807687728635726

Epoch: 6| Step: 13
Training loss: 2.06677508354187
Validation loss: 2.4852843694789435

Epoch: 132| Step: 0
Training loss: 2.638256549835205
Validation loss: 2.4829169601522465

Epoch: 6| Step: 1
Training loss: 2.985480546951294
Validation loss: 2.4841071482627624

Epoch: 6| Step: 2
Training loss: 1.804227590560913
Validation loss: 2.4902108792335755

Epoch: 6| Step: 3
Training loss: 2.096290111541748
Validation loss: 2.485926433276105

Epoch: 6| Step: 4
Training loss: 2.588768243789673
Validation loss: 2.4935184986360612

Epoch: 6| Step: 5
Training loss: 3.129807233810425
Validation loss: 2.4930917678340787

Epoch: 6| Step: 6
Training loss: 2.8433244228363037
Validation loss: 2.501396353526782

Epoch: 6| Step: 7
Training loss: 2.4949233531951904
Validation loss: 2.49688248608702

Epoch: 6| Step: 8
Training loss: 2.734691619873047
Validation loss: 2.4954754896061395

Epoch: 6| Step: 9
Training loss: 3.308882713317871
Validation loss: 2.4800913333892822

Epoch: 6| Step: 10
Training loss: 2.63521146774292
Validation loss: 2.473255879135542

Epoch: 6| Step: 11
Training loss: 2.8916988372802734
Validation loss: 2.473280458040135

Epoch: 6| Step: 12
Training loss: 2.9397668838500977
Validation loss: 2.4720331930345103

Epoch: 6| Step: 13
Training loss: 2.0450193881988525
Validation loss: 2.477503025403587

Epoch: 133| Step: 0
Training loss: 3.0087194442749023
Validation loss: 2.472717854284471

Epoch: 6| Step: 1
Training loss: 2.940145969390869
Validation loss: 2.4702755507602485

Epoch: 6| Step: 2
Training loss: 2.9386892318725586
Validation loss: 2.4733600770273516

Epoch: 6| Step: 3
Training loss: 2.8301143646240234
Validation loss: 2.4678184588750205

Epoch: 6| Step: 4
Training loss: 2.8638510704040527
Validation loss: 2.4681224592270388

Epoch: 6| Step: 5
Training loss: 3.348059892654419
Validation loss: 2.4671970746850453

Epoch: 6| Step: 6
Training loss: 2.3478517532348633
Validation loss: 2.470056462031539

Epoch: 6| Step: 7
Training loss: 2.8614697456359863
Validation loss: 2.468327106968049

Epoch: 6| Step: 8
Training loss: 2.7347590923309326
Validation loss: 2.4690677119839575

Epoch: 6| Step: 9
Training loss: 2.4820444583892822
Validation loss: 2.469431161880493

Epoch: 6| Step: 10
Training loss: 2.1782186031341553
Validation loss: 2.4690329592715026

Epoch: 6| Step: 11
Training loss: 2.2944998741149902
Validation loss: 2.4821526799150693

Epoch: 6| Step: 12
Training loss: 2.212930679321289
Validation loss: 2.4856124872802408

Epoch: 6| Step: 13
Training loss: 1.967934489250183
Validation loss: 2.4936480778519825

Epoch: 134| Step: 0
Training loss: 2.672210216522217
Validation loss: 2.486472791241061

Epoch: 6| Step: 1
Training loss: 2.9066226482391357
Validation loss: 2.4874407552903697

Epoch: 6| Step: 2
Training loss: 2.4769134521484375
Validation loss: 2.481840918141027

Epoch: 6| Step: 3
Training loss: 3.199129819869995
Validation loss: 2.4811422286495084

Epoch: 6| Step: 4
Training loss: 2.521812915802002
Validation loss: 2.478621931486232

Epoch: 6| Step: 5
Training loss: 2.434093713760376
Validation loss: 2.476745328595561

Epoch: 6| Step: 6
Training loss: 2.593428611755371
Validation loss: 2.469919507221509

Epoch: 6| Step: 7
Training loss: 2.3969991207122803
Validation loss: 2.4762438804872575

Epoch: 6| Step: 8
Training loss: 2.4102401733398438
Validation loss: 2.47265713701966

Epoch: 6| Step: 9
Training loss: 3.024056911468506
Validation loss: 2.4797343105398197

Epoch: 6| Step: 10
Training loss: 1.851372241973877
Validation loss: 2.4781906809858096

Epoch: 6| Step: 11
Training loss: 3.3682541847229004
Validation loss: 2.4788967845260457

Epoch: 6| Step: 12
Training loss: 2.770634651184082
Validation loss: 2.476090823450396

Epoch: 6| Step: 13
Training loss: 2.6160190105438232
Validation loss: 2.469869316265147

Epoch: 135| Step: 0
Training loss: 2.8204410076141357
Validation loss: 2.4717166731434483

Epoch: 6| Step: 1
Training loss: 1.745920181274414
Validation loss: 2.470061363712434

Epoch: 6| Step: 2
Training loss: 2.540578842163086
Validation loss: 2.4667368217181136

Epoch: 6| Step: 3
Training loss: 2.6792354583740234
Validation loss: 2.4646902238169024

Epoch: 6| Step: 4
Training loss: 2.753371000289917
Validation loss: 2.4661205327639015

Epoch: 6| Step: 5
Training loss: 3.182542324066162
Validation loss: 2.4609689738160823

Epoch: 6| Step: 6
Training loss: 3.00516939163208
Validation loss: 2.461611404213854

Epoch: 6| Step: 7
Training loss: 2.959937334060669
Validation loss: 2.465790053849579

Epoch: 6| Step: 8
Training loss: 2.1331653594970703
Validation loss: 2.470564865296887

Epoch: 6| Step: 9
Training loss: 2.506887435913086
Validation loss: 2.476455701294766

Epoch: 6| Step: 10
Training loss: 2.8958375453948975
Validation loss: 2.4907788845800583

Epoch: 6| Step: 11
Training loss: 2.7525720596313477
Validation loss: 2.493866494906846

Epoch: 6| Step: 12
Training loss: 3.155466079711914
Validation loss: 2.494757402327753

Epoch: 6| Step: 13
Training loss: 1.8748899698257446
Validation loss: 2.4967265077816543

Epoch: 136| Step: 0
Training loss: 2.63930606842041
Validation loss: 2.4853798958563034

Epoch: 6| Step: 1
Training loss: 2.832484483718872
Validation loss: 2.4757653872172036

Epoch: 6| Step: 2
Training loss: 3.2919089794158936
Validation loss: 2.4711853099125687

Epoch: 6| Step: 3
Training loss: 2.402278423309326
Validation loss: 2.4616076561712448

Epoch: 6| Step: 4
Training loss: 2.706791400909424
Validation loss: 2.4622091542008104

Epoch: 6| Step: 5
Training loss: 2.3568239212036133
Validation loss: 2.4637718457047657

Epoch: 6| Step: 6
Training loss: 1.9179925918579102
Validation loss: 2.461167735438193

Epoch: 6| Step: 7
Training loss: 2.7592201232910156
Validation loss: 2.4601518748908915

Epoch: 6| Step: 8
Training loss: 2.5537428855895996
Validation loss: 2.4625361119547198

Epoch: 6| Step: 9
Training loss: 2.923271656036377
Validation loss: 2.460211697445121

Epoch: 6| Step: 10
Training loss: 2.1313228607177734
Validation loss: 2.4654086841050016

Epoch: 6| Step: 11
Training loss: 2.9121856689453125
Validation loss: 2.468000417114586

Epoch: 6| Step: 12
Training loss: 3.0316452980041504
Validation loss: 2.467767312962522

Epoch: 6| Step: 13
Training loss: 3.0841376781463623
Validation loss: 2.465952565593104

Epoch: 137| Step: 0
Training loss: 2.0389163494110107
Validation loss: 2.472699731908819

Epoch: 6| Step: 1
Training loss: 3.107205867767334
Validation loss: 2.470311677584084

Epoch: 6| Step: 2
Training loss: 3.008467674255371
Validation loss: 2.464493315706971

Epoch: 6| Step: 3
Training loss: 2.591264009475708
Validation loss: 2.467873788649036

Epoch: 6| Step: 4
Training loss: 2.7098066806793213
Validation loss: 2.4631479017196165

Epoch: 6| Step: 5
Training loss: 3.4151053428649902
Validation loss: 2.4611462828933552

Epoch: 6| Step: 6
Training loss: 2.47959566116333
Validation loss: 2.460413004762383

Epoch: 6| Step: 7
Training loss: 1.9786567687988281
Validation loss: 2.4641018452182895

Epoch: 6| Step: 8
Training loss: 3.2810802459716797
Validation loss: 2.465186421589185

Epoch: 6| Step: 9
Training loss: 2.8884530067443848
Validation loss: 2.461608450899842

Epoch: 6| Step: 10
Training loss: 2.9003403186798096
Validation loss: 2.468825319761871

Epoch: 6| Step: 11
Training loss: 2.1656227111816406
Validation loss: 2.4627525447517313

Epoch: 6| Step: 12
Training loss: 2.2248809337615967
Validation loss: 2.4759743854563725

Epoch: 6| Step: 13
Training loss: 2.302412986755371
Validation loss: 2.473541416147704

Epoch: 138| Step: 0
Training loss: 2.9615678787231445
Validation loss: 2.4744759067412345

Epoch: 6| Step: 1
Training loss: 2.261809825897217
Validation loss: 2.478147696423274

Epoch: 6| Step: 2
Training loss: 2.6568126678466797
Validation loss: 2.4761564590597667

Epoch: 6| Step: 3
Training loss: 2.6743111610412598
Validation loss: 2.4769357327492005

Epoch: 6| Step: 4
Training loss: 3.157597541809082
Validation loss: 2.5004164249666276

Epoch: 6| Step: 5
Training loss: 2.7371163368225098
Validation loss: 2.5021613797833844

Epoch: 6| Step: 6
Training loss: 2.9956672191619873
Validation loss: 2.5046337522486204

Epoch: 6| Step: 7
Training loss: 2.632613182067871
Validation loss: 2.49842930352816

Epoch: 6| Step: 8
Training loss: 2.2801475524902344
Validation loss: 2.4792659846685265

Epoch: 6| Step: 9
Training loss: 2.659890651702881
Validation loss: 2.4712500110749276

Epoch: 6| Step: 10
Training loss: 2.5663323402404785
Validation loss: 2.4645559710841023

Epoch: 6| Step: 11
Training loss: 2.5885167121887207
Validation loss: 2.46039088567098

Epoch: 6| Step: 12
Training loss: 2.2032265663146973
Validation loss: 2.458966701261459

Epoch: 6| Step: 13
Training loss: 3.028975486755371
Validation loss: 2.4562389081524265

Epoch: 139| Step: 0
Training loss: 2.2355217933654785
Validation loss: 2.4598495985872004

Epoch: 6| Step: 1
Training loss: 2.9980885982513428
Validation loss: 2.4611020703469553

Epoch: 6| Step: 2
Training loss: 2.233717441558838
Validation loss: 2.460548557260985

Epoch: 6| Step: 3
Training loss: 2.3102502822875977
Validation loss: 2.468527340119885

Epoch: 6| Step: 4
Training loss: 2.8605823516845703
Validation loss: 2.469114480480071

Epoch: 6| Step: 5
Training loss: 2.0960419178009033
Validation loss: 2.470054295755202

Epoch: 6| Step: 6
Training loss: 3.346831798553467
Validation loss: 2.4851497116909234

Epoch: 6| Step: 7
Training loss: 2.734013557434082
Validation loss: 2.4973113870108

Epoch: 6| Step: 8
Training loss: 2.4032833576202393
Validation loss: 2.4998418438819145

Epoch: 6| Step: 9
Training loss: 3.535031318664551
Validation loss: 2.4919519296256443

Epoch: 6| Step: 10
Training loss: 2.6395487785339355
Validation loss: 2.4773624814966673

Epoch: 6| Step: 11
Training loss: 2.931272506713867
Validation loss: 2.463938013199837

Epoch: 6| Step: 12
Training loss: 2.8123738765716553
Validation loss: 2.455570997730378

Epoch: 6| Step: 13
Training loss: 1.7615416049957275
Validation loss: 2.4587435645441853

Epoch: 140| Step: 0
Training loss: 2.8285350799560547
Validation loss: 2.465557263743493

Epoch: 6| Step: 1
Training loss: 2.492340087890625
Validation loss: 2.466038680845691

Epoch: 6| Step: 2
Training loss: 2.8880739212036133
Validation loss: 2.4598778627252065

Epoch: 6| Step: 3
Training loss: 2.648200511932373
Validation loss: 2.4547007058256414

Epoch: 6| Step: 4
Training loss: 2.8825466632843018
Validation loss: 2.449971052908128

Epoch: 6| Step: 5
Training loss: 3.8138928413391113
Validation loss: 2.45538471334724

Epoch: 6| Step: 6
Training loss: 2.827207565307617
Validation loss: 2.4521345707678024

Epoch: 6| Step: 7
Training loss: 2.354856252670288
Validation loss: 2.4477770482340167

Epoch: 6| Step: 8
Training loss: 1.5846478939056396
Validation loss: 2.448451557467061

Epoch: 6| Step: 9
Training loss: 2.571455717086792
Validation loss: 2.449558993821503

Epoch: 6| Step: 10
Training loss: 2.0978708267211914
Validation loss: 2.445700871047153

Epoch: 6| Step: 11
Training loss: 2.5430116653442383
Validation loss: 2.4449688926819833

Epoch: 6| Step: 12
Training loss: 3.1960504055023193
Validation loss: 2.4438349969925417

Epoch: 6| Step: 13
Training loss: 2.3877696990966797
Validation loss: 2.4409540750647105

Epoch: 141| Step: 0
Training loss: 2.697455644607544
Validation loss: 2.444704555696057

Epoch: 6| Step: 1
Training loss: 2.4956181049346924
Validation loss: 2.446736825409756

Epoch: 6| Step: 2
Training loss: 2.2759928703308105
Validation loss: 2.442977915528

Epoch: 6| Step: 3
Training loss: 3.1452128887176514
Validation loss: 2.446684903995965

Epoch: 6| Step: 4
Training loss: 2.400219440460205
Validation loss: 2.458714667186942

Epoch: 6| Step: 5
Training loss: 2.905910015106201
Validation loss: 2.4607329701864593

Epoch: 6| Step: 6
Training loss: 2.7488479614257812
Validation loss: 2.463545207054384

Epoch: 6| Step: 7
Training loss: 2.7114434242248535
Validation loss: 2.4704842387989

Epoch: 6| Step: 8
Training loss: 3.847594738006592
Validation loss: 2.478374922147361

Epoch: 6| Step: 9
Training loss: 2.814739227294922
Validation loss: 2.474884061403172

Epoch: 6| Step: 10
Training loss: 2.1525468826293945
Validation loss: 2.466656143947314

Epoch: 6| Step: 11
Training loss: 1.9911439418792725
Validation loss: 2.4569293068301294

Epoch: 6| Step: 12
Training loss: 1.9132375717163086
Validation loss: 2.4580413321013093

Epoch: 6| Step: 13
Training loss: 3.443739891052246
Validation loss: 2.4569127675025695

Epoch: 142| Step: 0
Training loss: 1.8159246444702148
Validation loss: 2.464364338946599

Epoch: 6| Step: 1
Training loss: 3.04829740524292
Validation loss: 2.4590970393150084

Epoch: 6| Step: 2
Training loss: 2.627162456512451
Validation loss: 2.457244719228437

Epoch: 6| Step: 3
Training loss: 2.770451545715332
Validation loss: 2.4659620382452525

Epoch: 6| Step: 4
Training loss: 2.53244686126709
Validation loss: 2.4723584908311085

Epoch: 6| Step: 5
Training loss: 2.3163177967071533
Validation loss: 2.483990248813424

Epoch: 6| Step: 6
Training loss: 2.7531118392944336
Validation loss: 2.4709924369729976

Epoch: 6| Step: 7
Training loss: 2.3198838233947754
Validation loss: 2.455326149540563

Epoch: 6| Step: 8
Training loss: 2.6121606826782227
Validation loss: 2.45373511058028

Epoch: 6| Step: 9
Training loss: 2.9259302616119385
Validation loss: 2.4455423662739415

Epoch: 6| Step: 10
Training loss: 3.05476975440979
Validation loss: 2.4481983518087738

Epoch: 6| Step: 11
Training loss: 2.868987798690796
Validation loss: 2.4410864332670807

Epoch: 6| Step: 12
Training loss: 2.910015821456909
Validation loss: 2.4437902614634526

Epoch: 6| Step: 13
Training loss: 2.463927984237671
Validation loss: 2.4449478810833347

Epoch: 143| Step: 0
Training loss: 3.3217275142669678
Validation loss: 2.4516776787337435

Epoch: 6| Step: 1
Training loss: 2.7297186851501465
Validation loss: 2.4510287520706013

Epoch: 6| Step: 2
Training loss: 2.618162155151367
Validation loss: 2.4512624099690425

Epoch: 6| Step: 3
Training loss: 2.6751136779785156
Validation loss: 2.4529787032834944

Epoch: 6| Step: 4
Training loss: 2.4892072677612305
Validation loss: 2.446288121643887

Epoch: 6| Step: 5
Training loss: 2.8067009449005127
Validation loss: 2.447029782879737

Epoch: 6| Step: 6
Training loss: 1.3348972797393799
Validation loss: 2.4408996592285814

Epoch: 6| Step: 7
Training loss: 2.919405698776245
Validation loss: 2.441776344853063

Epoch: 6| Step: 8
Training loss: 2.6266589164733887
Validation loss: 2.4395122604985393

Epoch: 6| Step: 9
Training loss: 2.9342217445373535
Validation loss: 2.4419040987568517

Epoch: 6| Step: 10
Training loss: 2.9475412368774414
Validation loss: 2.4435669658004597

Epoch: 6| Step: 11
Training loss: 2.1704392433166504
Validation loss: 2.451186039114511

Epoch: 6| Step: 12
Training loss: 2.327808380126953
Validation loss: 2.452765751910466

Epoch: 6| Step: 13
Training loss: 3.790713310241699
Validation loss: 2.4650471851389897

Epoch: 144| Step: 0
Training loss: 3.1898789405822754
Validation loss: 2.476027834799982

Epoch: 6| Step: 1
Training loss: 3.422187328338623
Validation loss: 2.473921616872152

Epoch: 6| Step: 2
Training loss: 3.492596387863159
Validation loss: 2.467128340915967

Epoch: 6| Step: 3
Training loss: 2.2426950931549072
Validation loss: 2.45350980758667

Epoch: 6| Step: 4
Training loss: 2.3395800590515137
Validation loss: 2.44778012716642

Epoch: 6| Step: 5
Training loss: 2.9794135093688965
Validation loss: 2.438619985375353

Epoch: 6| Step: 6
Training loss: 2.6577844619750977
Validation loss: 2.4370685726083736

Epoch: 6| Step: 7
Training loss: 2.6902194023132324
Validation loss: 2.436067286358085

Epoch: 6| Step: 8
Training loss: 1.424775242805481
Validation loss: 2.434560689874875

Epoch: 6| Step: 9
Training loss: 2.222236156463623
Validation loss: 2.43599578385712

Epoch: 6| Step: 10
Training loss: 3.09785795211792
Validation loss: 2.434467169546312

Epoch: 6| Step: 11
Training loss: 2.2490415573120117
Validation loss: 2.437850780384515

Epoch: 6| Step: 12
Training loss: 2.713219165802002
Validation loss: 2.4379575355078584

Epoch: 6| Step: 13
Training loss: 2.40472674369812
Validation loss: 2.4401957911829792

Epoch: 145| Step: 0
Training loss: 2.2310292720794678
Validation loss: 2.435910778660928

Epoch: 6| Step: 1
Training loss: 2.817305088043213
Validation loss: 2.440267047574443

Epoch: 6| Step: 2
Training loss: 2.4960508346557617
Validation loss: 2.442476567401681

Epoch: 6| Step: 3
Training loss: 2.9935154914855957
Validation loss: 2.447862953268072

Epoch: 6| Step: 4
Training loss: 1.3411271572113037
Validation loss: 2.4680506670346825

Epoch: 6| Step: 5
Training loss: 3.6976218223571777
Validation loss: 2.490213865874916

Epoch: 6| Step: 6
Training loss: 2.679551601409912
Validation loss: 2.5029493403691117

Epoch: 6| Step: 7
Training loss: 2.5502891540527344
Validation loss: 2.51472310866079

Epoch: 6| Step: 8
Training loss: 3.115753173828125
Validation loss: 2.5235235896161807

Epoch: 6| Step: 9
Training loss: 2.3437752723693848
Validation loss: 2.5179861335344214

Epoch: 6| Step: 10
Training loss: 2.5609092712402344
Validation loss: 2.503700884439612

Epoch: 6| Step: 11
Training loss: 2.6912145614624023
Validation loss: 2.506182614193168

Epoch: 6| Step: 12
Training loss: 2.942223072052002
Validation loss: 2.4882374809634302

Epoch: 6| Step: 13
Training loss: 2.991603374481201
Validation loss: 2.4573506719322613

Epoch: 146| Step: 0
Training loss: 2.256054401397705
Validation loss: 2.4382452349508963

Epoch: 6| Step: 1
Training loss: 2.347264051437378
Validation loss: 2.438264590437694

Epoch: 6| Step: 2
Training loss: 3.1944425106048584
Validation loss: 2.4399061305548555

Epoch: 6| Step: 3
Training loss: 2.138704538345337
Validation loss: 2.4417895706751014

Epoch: 6| Step: 4
Training loss: 3.3160431385040283
Validation loss: 2.4436218456555436

Epoch: 6| Step: 5
Training loss: 2.3899130821228027
Validation loss: 2.4493726068927395

Epoch: 6| Step: 6
Training loss: 2.5889720916748047
Validation loss: 2.444468149574854

Epoch: 6| Step: 7
Training loss: 2.0246329307556152
Validation loss: 2.4385195073261055

Epoch: 6| Step: 8
Training loss: 3.0438594818115234
Validation loss: 2.4378024916495047

Epoch: 6| Step: 9
Training loss: 2.4849729537963867
Validation loss: 2.432869747120847

Epoch: 6| Step: 10
Training loss: 2.750627040863037
Validation loss: 2.4340118849149315

Epoch: 6| Step: 11
Training loss: 3.225918769836426
Validation loss: 2.432321192115866

Epoch: 6| Step: 12
Training loss: 2.8460187911987305
Validation loss: 2.4343694435652865

Epoch: 6| Step: 13
Training loss: 2.3337604999542236
Validation loss: 2.4382389540313394

Epoch: 147| Step: 0
Training loss: 2.5106358528137207
Validation loss: 2.4588660822119763

Epoch: 6| Step: 1
Training loss: 3.08689546585083
Validation loss: 2.4839343793930544

Epoch: 6| Step: 2
Training loss: 2.6060051918029785
Validation loss: 2.5486613806857856

Epoch: 6| Step: 3
Training loss: 2.9461207389831543
Validation loss: 2.603685473883024

Epoch: 6| Step: 4
Training loss: 2.8777732849121094
Validation loss: 2.601010778898834

Epoch: 6| Step: 5
Training loss: 2.5600805282592773
Validation loss: 2.53821966725011

Epoch: 6| Step: 6
Training loss: 2.6682443618774414
Validation loss: 2.477607370704733

Epoch: 6| Step: 7
Training loss: 2.263840675354004
Validation loss: 2.44283720754808

Epoch: 6| Step: 8
Training loss: 2.8962433338165283
Validation loss: 2.425377071544688

Epoch: 6| Step: 9
Training loss: 2.110409736633301
Validation loss: 2.432084386066724

Epoch: 6| Step: 10
Training loss: 3.000065803527832
Validation loss: 2.4508121808369956

Epoch: 6| Step: 11
Training loss: 2.8392586708068848
Validation loss: 2.484888590792174

Epoch: 6| Step: 12
Training loss: 2.5386321544647217
Validation loss: 2.4797110634465374

Epoch: 6| Step: 13
Training loss: 2.987208366394043
Validation loss: 2.4558023278431227

Epoch: 148| Step: 0
Training loss: 2.4861395359039307
Validation loss: 2.4452965951734975

Epoch: 6| Step: 1
Training loss: 3.6242587566375732
Validation loss: 2.4365035641577935

Epoch: 6| Step: 2
Training loss: 1.990360975265503
Validation loss: 2.434537708118398

Epoch: 6| Step: 3
Training loss: 2.6896347999572754
Validation loss: 2.4297133953340593

Epoch: 6| Step: 4
Training loss: 2.202913522720337
Validation loss: 2.426596333903651

Epoch: 6| Step: 5
Training loss: 2.3537347316741943
Validation loss: 2.4239527307530886

Epoch: 6| Step: 6
Training loss: 3.015470027923584
Validation loss: 2.424015914240191

Epoch: 6| Step: 7
Training loss: 2.4304115772247314
Validation loss: 2.4221452333593882

Epoch: 6| Step: 8
Training loss: 2.7513294219970703
Validation loss: 2.4238247102306736

Epoch: 6| Step: 9
Training loss: 2.902055263519287
Validation loss: 2.418158451716105

Epoch: 6| Step: 10
Training loss: 2.4534099102020264
Validation loss: 2.4188362757364907

Epoch: 6| Step: 11
Training loss: 2.5989797115325928
Validation loss: 2.4261207580566406

Epoch: 6| Step: 12
Training loss: 3.0948147773742676
Validation loss: 2.428990266656363

Epoch: 6| Step: 13
Training loss: 2.394801139831543
Validation loss: 2.427812853167134

Epoch: 149| Step: 0
Training loss: 2.7740907669067383
Validation loss: 2.423554789635443

Epoch: 6| Step: 1
Training loss: 2.3977603912353516
Validation loss: 2.4262577410667174

Epoch: 6| Step: 2
Training loss: 2.0598793029785156
Validation loss: 2.4225144001745407

Epoch: 6| Step: 3
Training loss: 2.6608288288116455
Validation loss: 2.4222381884051907

Epoch: 6| Step: 4
Training loss: 2.480055570602417
Validation loss: 2.425281032439201

Epoch: 6| Step: 5
Training loss: 3.243340492248535
Validation loss: 2.42653247105178

Epoch: 6| Step: 6
Training loss: 2.4610652923583984
Validation loss: 2.4266229291116037

Epoch: 6| Step: 7
Training loss: 2.5193307399749756
Validation loss: 2.421849807103475

Epoch: 6| Step: 8
Training loss: 3.0549814701080322
Validation loss: 2.422800999815746

Epoch: 6| Step: 9
Training loss: 2.3632700443267822
Validation loss: 2.425923257745722

Epoch: 6| Step: 10
Training loss: 3.578199863433838
Validation loss: 2.432574156791933

Epoch: 6| Step: 11
Training loss: 2.4731550216674805
Validation loss: 2.433244466781616

Epoch: 6| Step: 12
Training loss: 2.9305896759033203
Validation loss: 2.445206803660239

Epoch: 6| Step: 13
Training loss: 1.5675822496414185
Validation loss: 2.45088275530005

Epoch: 150| Step: 0
Training loss: 3.497861862182617
Validation loss: 2.4505671249922885

Epoch: 6| Step: 1
Training loss: 3.247830867767334
Validation loss: 2.450905087173626

Epoch: 6| Step: 2
Training loss: 3.2443184852600098
Validation loss: 2.4441078298835346

Epoch: 6| Step: 3
Training loss: 2.619232654571533
Validation loss: 2.442562815963581

Epoch: 6| Step: 4
Training loss: 2.558303117752075
Validation loss: 2.430217609610609

Epoch: 6| Step: 5
Training loss: 2.3270039558410645
Validation loss: 2.428820481864355

Epoch: 6| Step: 6
Training loss: 2.5585408210754395
Validation loss: 2.4269712099464993

Epoch: 6| Step: 7
Training loss: 3.2825074195861816
Validation loss: 2.424129796284501

Epoch: 6| Step: 8
Training loss: 2.065269947052002
Validation loss: 2.4252853944737423

Epoch: 6| Step: 9
Training loss: 2.0962257385253906
Validation loss: 2.4250139728669198

Epoch: 6| Step: 10
Training loss: 2.225548505783081
Validation loss: 2.43149225173458

Epoch: 6| Step: 11
Training loss: 2.2795908451080322
Validation loss: 2.428696688785348

Epoch: 6| Step: 12
Training loss: 2.514378309249878
Validation loss: 2.426389753177602

Epoch: 6| Step: 13
Training loss: 2.3797411918640137
Validation loss: 2.4326026670394407

Epoch: 151| Step: 0
Training loss: 3.4797749519348145
Validation loss: 2.432792294409967

Epoch: 6| Step: 1
Training loss: 2.866629123687744
Validation loss: 2.4328858903659287

Epoch: 6| Step: 2
Training loss: 2.4154224395751953
Validation loss: 2.432755701003536

Epoch: 6| Step: 3
Training loss: 2.174368381500244
Validation loss: 2.4349112843954437

Epoch: 6| Step: 4
Training loss: 2.783085346221924
Validation loss: 2.4352042777563936

Epoch: 6| Step: 5
Training loss: 2.6557836532592773
Validation loss: 2.434010164712065

Epoch: 6| Step: 6
Training loss: 2.5162224769592285
Validation loss: 2.4301838720998457

Epoch: 6| Step: 7
Training loss: 2.770673990249634
Validation loss: 2.4310227645340787

Epoch: 6| Step: 8
Training loss: 2.8802690505981445
Validation loss: 2.425796019133701

Epoch: 6| Step: 9
Training loss: 2.269315719604492
Validation loss: 2.4287725751117994

Epoch: 6| Step: 10
Training loss: 2.2972021102905273
Validation loss: 2.4267494678497314

Epoch: 6| Step: 11
Training loss: 2.5835812091827393
Validation loss: 2.423037241863948

Epoch: 6| Step: 12
Training loss: 2.8176772594451904
Validation loss: 2.426624685205439

Epoch: 6| Step: 13
Training loss: 2.1402249336242676
Validation loss: 2.434912309851698

Epoch: 152| Step: 0
Training loss: 2.5592098236083984
Validation loss: 2.440811518699892

Epoch: 6| Step: 1
Training loss: 3.025907516479492
Validation loss: 2.4329784929111438

Epoch: 6| Step: 2
Training loss: 2.6050074100494385
Validation loss: 2.427928393886935

Epoch: 6| Step: 3
Training loss: 2.5928611755371094
Validation loss: 2.4280270299603863

Epoch: 6| Step: 4
Training loss: 2.390775203704834
Validation loss: 2.4373741636994066

Epoch: 6| Step: 5
Training loss: 2.032299518585205
Validation loss: 2.4436104554002003

Epoch: 6| Step: 6
Training loss: 3.5902528762817383
Validation loss: 2.446106997869348

Epoch: 6| Step: 7
Training loss: 2.634643793106079
Validation loss: 2.4456573327382407

Epoch: 6| Step: 8
Training loss: 3.1743383407592773
Validation loss: 2.4342778139216925

Epoch: 6| Step: 9
Training loss: 2.9679503440856934
Validation loss: 2.4270460938894622

Epoch: 6| Step: 10
Training loss: 1.9923938512802124
Validation loss: 2.416408215799639

Epoch: 6| Step: 11
Training loss: 2.9083752632141113
Validation loss: 2.4160700869816605

Epoch: 6| Step: 12
Training loss: 1.8772222995758057
Validation loss: 2.419241166883899

Epoch: 6| Step: 13
Training loss: 2.445335865020752
Validation loss: 2.4112874974486647

Epoch: 153| Step: 0
Training loss: 2.506511926651001
Validation loss: 2.4125282149161063

Epoch: 6| Step: 1
Training loss: 3.154874324798584
Validation loss: 2.4128212057134157

Epoch: 6| Step: 2
Training loss: 3.1806702613830566
Validation loss: 2.4108468127507034

Epoch: 6| Step: 3
Training loss: 2.684108018875122
Validation loss: 2.4066746901440363

Epoch: 6| Step: 4
Training loss: 3.0583114624023438
Validation loss: 2.411807474269662

Epoch: 6| Step: 5
Training loss: 2.717832565307617
Validation loss: 2.411714125705022

Epoch: 6| Step: 6
Training loss: 2.280320405960083
Validation loss: 2.412736902954758

Epoch: 6| Step: 7
Training loss: 3.0594024658203125
Validation loss: 2.4099760337542464

Epoch: 6| Step: 8
Training loss: 2.120084762573242
Validation loss: 2.41363021122512

Epoch: 6| Step: 9
Training loss: 1.9264165163040161
Validation loss: 2.4165978893156974

Epoch: 6| Step: 10
Training loss: 2.3101768493652344
Validation loss: 2.419204363258936

Epoch: 6| Step: 11
Training loss: 2.88810396194458
Validation loss: 2.418204266537902

Epoch: 6| Step: 12
Training loss: 2.1085174083709717
Validation loss: 2.419298356579196

Epoch: 6| Step: 13
Training loss: 3.034043788909912
Validation loss: 2.42265462106274

Epoch: 154| Step: 0
Training loss: 2.729478120803833
Validation loss: 2.42007286830615

Epoch: 6| Step: 1
Training loss: 1.9734585285186768
Validation loss: 2.4294498146221204

Epoch: 6| Step: 2
Training loss: 2.550981283187866
Validation loss: 2.4256669398277038

Epoch: 6| Step: 3
Training loss: 2.8394665718078613
Validation loss: 2.4231337629338747

Epoch: 6| Step: 4
Training loss: 2.415100574493408
Validation loss: 2.4244910286318873

Epoch: 6| Step: 5
Training loss: 3.137014389038086
Validation loss: 2.430235770440871

Epoch: 6| Step: 6
Training loss: 2.6005125045776367
Validation loss: 2.4289506173902944

Epoch: 6| Step: 7
Training loss: 2.816753387451172
Validation loss: 2.428783461611758

Epoch: 6| Step: 8
Training loss: 2.651773452758789
Validation loss: 2.4234015505800963

Epoch: 6| Step: 9
Training loss: 2.834111452102661
Validation loss: 2.4216317771583475

Epoch: 6| Step: 10
Training loss: 2.378722667694092
Validation loss: 2.4198001815426733

Epoch: 6| Step: 11
Training loss: 2.4277334213256836
Validation loss: 2.4174771385808147

Epoch: 6| Step: 12
Training loss: 2.6366734504699707
Validation loss: 2.4143633457922165

Epoch: 6| Step: 13
Training loss: 2.970370054244995
Validation loss: 2.4160808170995405

Epoch: 155| Step: 0
Training loss: 3.6120457649230957
Validation loss: 2.412870043067522

Epoch: 6| Step: 1
Training loss: 2.5534777641296387
Validation loss: 2.4155063116422264

Epoch: 6| Step: 2
Training loss: 2.5111517906188965
Validation loss: 2.4279388637952906

Epoch: 6| Step: 3
Training loss: 2.61515212059021
Validation loss: 2.4546576443538872

Epoch: 6| Step: 4
Training loss: 2.459007740020752
Validation loss: 2.482212394796392

Epoch: 6| Step: 5
Training loss: 2.9267289638519287
Validation loss: 2.51109524824286

Epoch: 6| Step: 6
Training loss: 1.7573928833007812
Validation loss: 2.5430825525714504

Epoch: 6| Step: 7
Training loss: 2.6899678707122803
Validation loss: 2.53190948629892

Epoch: 6| Step: 8
Training loss: 2.4351439476013184
Validation loss: 2.5395103628917406

Epoch: 6| Step: 9
Training loss: 2.691161632537842
Validation loss: 2.495043739195793

Epoch: 6| Step: 10
Training loss: 2.6469383239746094
Validation loss: 2.4629861616319224

Epoch: 6| Step: 11
Training loss: 2.7438278198242188
Validation loss: 2.4256000262434765

Epoch: 6| Step: 12
Training loss: 2.862863063812256
Validation loss: 2.404615053566553

Epoch: 6| Step: 13
Training loss: 2.8561296463012695
Validation loss: 2.4066959657976703

Epoch: 156| Step: 0
Training loss: 3.048786163330078
Validation loss: 2.4110133724827922

Epoch: 6| Step: 1
Training loss: 3.0760316848754883
Validation loss: 2.4177069305091776

Epoch: 6| Step: 2
Training loss: 2.5655009746551514
Validation loss: 2.4272183602856052

Epoch: 6| Step: 3
Training loss: 2.219891309738159
Validation loss: 2.4306864251372633

Epoch: 6| Step: 4
Training loss: 2.526923418045044
Validation loss: 2.429884100473055

Epoch: 6| Step: 5
Training loss: 2.854060649871826
Validation loss: 2.4351822483924126

Epoch: 6| Step: 6
Training loss: 2.8180971145629883
Validation loss: 2.4273719659415622

Epoch: 6| Step: 7
Training loss: 3.01033616065979
Validation loss: 2.4199332447462183

Epoch: 6| Step: 8
Training loss: 3.186025619506836
Validation loss: 2.41625367441485

Epoch: 6| Step: 9
Training loss: 2.8200058937072754
Validation loss: 2.4089192651933238

Epoch: 6| Step: 10
Training loss: 2.1284570693969727
Validation loss: 2.4067757232214815

Epoch: 6| Step: 11
Training loss: 2.1030735969543457
Validation loss: 2.4056675331566924

Epoch: 6| Step: 12
Training loss: 2.6913037300109863
Validation loss: 2.403655598240514

Epoch: 6| Step: 13
Training loss: 1.8207452297210693
Validation loss: 2.401886083746469

Epoch: 157| Step: 0
Training loss: 3.2398524284362793
Validation loss: 2.395560710660873

Epoch: 6| Step: 1
Training loss: 2.8567185401916504
Validation loss: 2.3961155312035674

Epoch: 6| Step: 2
Training loss: 2.761509418487549
Validation loss: 2.401938392269996

Epoch: 6| Step: 3
Training loss: 2.4168143272399902
Validation loss: 2.415745489058956

Epoch: 6| Step: 4
Training loss: 2.739583969116211
Validation loss: 2.427609012972924

Epoch: 6| Step: 5
Training loss: 2.349977493286133
Validation loss: 2.4468384660700315

Epoch: 6| Step: 6
Training loss: 2.956460475921631
Validation loss: 2.4330181460226736

Epoch: 6| Step: 7
Training loss: 3.136345863342285
Validation loss: 2.4296694006971133

Epoch: 6| Step: 8
Training loss: 1.6601908206939697
Validation loss: 2.422951590630316

Epoch: 6| Step: 9
Training loss: 2.629108190536499
Validation loss: 2.4227486733467347

Epoch: 6| Step: 10
Training loss: 2.5394937992095947
Validation loss: 2.4072304899974535

Epoch: 6| Step: 11
Training loss: 2.378143787384033
Validation loss: 2.415422316520445

Epoch: 6| Step: 12
Training loss: 2.40755558013916
Validation loss: 2.4128855607842885

Epoch: 6| Step: 13
Training loss: 2.78432559967041
Validation loss: 2.408834800925306

Epoch: 158| Step: 0
Training loss: 1.8737610578536987
Validation loss: 2.3966621263052827

Epoch: 6| Step: 1
Training loss: 2.4867451190948486
Validation loss: 2.3936419461363103

Epoch: 6| Step: 2
Training loss: 2.493109703063965
Validation loss: 2.3972856190896805

Epoch: 6| Step: 3
Training loss: 2.6309714317321777
Validation loss: 2.399794501643027

Epoch: 6| Step: 4
Training loss: 1.88193678855896
Validation loss: 2.400963826846051

Epoch: 6| Step: 5
Training loss: 2.7531988620758057
Validation loss: 2.40692316075807

Epoch: 6| Step: 6
Training loss: 3.6283466815948486
Validation loss: 2.407872264103223

Epoch: 6| Step: 7
Training loss: 3.0135486125946045
Validation loss: 2.4066523095612884

Epoch: 6| Step: 8
Training loss: 2.1648826599121094
Validation loss: 2.405017706655687

Epoch: 6| Step: 9
Training loss: 2.5711443424224854
Validation loss: 2.40538388939314

Epoch: 6| Step: 10
Training loss: 3.599463701248169
Validation loss: 2.4101234738544752

Epoch: 6| Step: 11
Training loss: 3.4585390090942383
Validation loss: 2.4068998803374586

Epoch: 6| Step: 12
Training loss: 1.985832929611206
Validation loss: 2.401263616418326

Epoch: 6| Step: 13
Training loss: 2.2747325897216797
Validation loss: 2.406138153486354

Epoch: 159| Step: 0
Training loss: 2.6724791526794434
Validation loss: 2.405052956714425

Epoch: 6| Step: 1
Training loss: 2.7286324501037598
Validation loss: 2.402314001514066

Epoch: 6| Step: 2
Training loss: 2.5138745307922363
Validation loss: 2.408537454502557

Epoch: 6| Step: 3
Training loss: 1.9989877939224243
Validation loss: 2.4110887665902414

Epoch: 6| Step: 4
Training loss: 2.584986925125122
Validation loss: 2.419209145730542

Epoch: 6| Step: 5
Training loss: 2.3295798301696777
Validation loss: 2.4125717916796283

Epoch: 6| Step: 6
Training loss: 2.7837257385253906
Validation loss: 2.4243473006832983

Epoch: 6| Step: 7
Training loss: 3.021237373352051
Validation loss: 2.420592256771621

Epoch: 6| Step: 8
Training loss: 2.0741701126098633
Validation loss: 2.4209103199743454

Epoch: 6| Step: 9
Training loss: 3.1306800842285156
Validation loss: 2.421320089729883

Epoch: 6| Step: 10
Training loss: 2.7139182090759277
Validation loss: 2.42202853643766

Epoch: 6| Step: 11
Training loss: 2.69350004196167
Validation loss: 2.4179525631730274

Epoch: 6| Step: 12
Training loss: 2.724365472793579
Validation loss: 2.4080109211706344

Epoch: 6| Step: 13
Training loss: 2.907900094985962
Validation loss: 2.4156484039880897

Epoch: 160| Step: 0
Training loss: 2.7264230251312256
Validation loss: 2.408583974325529

Epoch: 6| Step: 1
Training loss: 2.1243162155151367
Validation loss: 2.4066909154256186

Epoch: 6| Step: 2
Training loss: 2.396151065826416
Validation loss: 2.3980147966774563

Epoch: 6| Step: 3
Training loss: 2.6839709281921387
Validation loss: 2.3825655342430196

Epoch: 6| Step: 4
Training loss: 2.9372200965881348
Validation loss: 2.3906155324751333

Epoch: 6| Step: 5
Training loss: 2.2479958534240723
Validation loss: 2.3869095066542267

Epoch: 6| Step: 6
Training loss: 2.5719027519226074
Validation loss: 2.39136996833227

Epoch: 6| Step: 7
Training loss: 2.629936933517456
Validation loss: 2.389207140091927

Epoch: 6| Step: 8
Training loss: 3.444291591644287
Validation loss: 2.3913194594844693

Epoch: 6| Step: 9
Training loss: 2.4983017444610596
Validation loss: 2.3889585156594553

Epoch: 6| Step: 10
Training loss: 2.8807373046875
Validation loss: 2.380932464394518

Epoch: 6| Step: 11
Training loss: 2.162534236907959
Validation loss: 2.3839881650863157

Epoch: 6| Step: 12
Training loss: 2.828639030456543
Validation loss: 2.3799577682249007

Epoch: 6| Step: 13
Training loss: 2.3270556926727295
Validation loss: 2.379193090623425

Epoch: 161| Step: 0
Training loss: 3.3062753677368164
Validation loss: 2.37982274639991

Epoch: 6| Step: 1
Training loss: 1.9319944381713867
Validation loss: 2.381960945744668

Epoch: 6| Step: 2
Training loss: 2.7704787254333496
Validation loss: 2.3827529107370684

Epoch: 6| Step: 3
Training loss: 2.298840045928955
Validation loss: 2.380626052938482

Epoch: 6| Step: 4
Training loss: 2.357292890548706
Validation loss: 2.38562354733867

Epoch: 6| Step: 5
Training loss: 2.361448287963867
Validation loss: 2.3845191591529438

Epoch: 6| Step: 6
Training loss: 2.9453418254852295
Validation loss: 2.3829605425557783

Epoch: 6| Step: 7
Training loss: 2.738490104675293
Validation loss: 2.3850120959743375

Epoch: 6| Step: 8
Training loss: 2.2923812866210938
Validation loss: 2.3828745529215825

Epoch: 6| Step: 9
Training loss: 2.8093364238739014
Validation loss: 2.3849991982983005

Epoch: 6| Step: 10
Training loss: 2.695258140563965
Validation loss: 2.3897824543778614

Epoch: 6| Step: 11
Training loss: 2.437680721282959
Validation loss: 2.3955398451897407

Epoch: 6| Step: 12
Training loss: 3.169290542602539
Validation loss: 2.407138483498686

Epoch: 6| Step: 13
Training loss: 2.214531898498535
Validation loss: 2.413865066343738

Epoch: 162| Step: 0
Training loss: 2.173140048980713
Validation loss: 2.425411885784518

Epoch: 6| Step: 1
Training loss: 3.3357157707214355
Validation loss: 2.4465028855108444

Epoch: 6| Step: 2
Training loss: 2.5196073055267334
Validation loss: 2.428548748775195

Epoch: 6| Step: 3
Training loss: 2.463122844696045
Validation loss: 2.4374272669515302

Epoch: 6| Step: 4
Training loss: 2.741868734359741
Validation loss: 2.4234348317628265

Epoch: 6| Step: 5
Training loss: 3.3729209899902344
Validation loss: 2.411232994448754

Epoch: 6| Step: 6
Training loss: 2.293957471847534
Validation loss: 2.3902160839367936

Epoch: 6| Step: 7
Training loss: 2.4791793823242188
Validation loss: 2.385789978888727

Epoch: 6| Step: 8
Training loss: 2.7626771926879883
Validation loss: 2.385396780506257

Epoch: 6| Step: 9
Training loss: 2.248727321624756
Validation loss: 2.380668624754875

Epoch: 6| Step: 10
Training loss: 2.1043283939361572
Validation loss: 2.380466938018799

Epoch: 6| Step: 11
Training loss: 2.83345103263855
Validation loss: 2.3808400348950456

Epoch: 6| Step: 12
Training loss: 2.373018980026245
Validation loss: 2.3804973325421734

Epoch: 6| Step: 13
Training loss: 3.1167004108428955
Validation loss: 2.3750911912610455

Epoch: 163| Step: 0
Training loss: 2.7129721641540527
Validation loss: 2.3777188049849642

Epoch: 6| Step: 1
Training loss: 2.061774969100952
Validation loss: 2.3792632369584936

Epoch: 6| Step: 2
Training loss: 1.989917516708374
Validation loss: 2.3861840822363414

Epoch: 6| Step: 3
Training loss: 3.2352418899536133
Validation loss: 2.391443997301081

Epoch: 6| Step: 4
Training loss: 2.935098171234131
Validation loss: 2.386080990555466

Epoch: 6| Step: 5
Training loss: 2.0498270988464355
Validation loss: 2.387476836481402

Epoch: 6| Step: 6
Training loss: 2.4244391918182373
Validation loss: 2.3934877969885386

Epoch: 6| Step: 7
Training loss: 2.1115269660949707
Validation loss: 2.3887262190541914

Epoch: 6| Step: 8
Training loss: 2.787508964538574
Validation loss: 2.382715314947149

Epoch: 6| Step: 9
Training loss: 2.3478400707244873
Validation loss: 2.381755785275531

Epoch: 6| Step: 10
Training loss: 3.4560163021087646
Validation loss: 2.3873858144206386

Epoch: 6| Step: 11
Training loss: 2.4983439445495605
Validation loss: 2.384032835242569

Epoch: 6| Step: 12
Training loss: 3.206270933151245
Validation loss: 2.3821461251986924

Epoch: 6| Step: 13
Training loss: 2.650397777557373
Validation loss: 2.3849578647203344

Epoch: 164| Step: 0
Training loss: 3.2071568965911865
Validation loss: 2.3802958662791918

Epoch: 6| Step: 1
Training loss: 1.7926702499389648
Validation loss: 2.388257449673068

Epoch: 6| Step: 2
Training loss: 2.722623348236084
Validation loss: 2.381282696159937

Epoch: 6| Step: 3
Training loss: 2.8155436515808105
Validation loss: 2.3833397844786286

Epoch: 6| Step: 4
Training loss: 2.154930591583252
Validation loss: 2.384561041350006

Epoch: 6| Step: 5
Training loss: 2.3647942543029785
Validation loss: 2.375949098217872

Epoch: 6| Step: 6
Training loss: 2.845615863800049
Validation loss: 2.3736532965014057

Epoch: 6| Step: 7
Training loss: 2.4835147857666016
Validation loss: 2.379878067201184

Epoch: 6| Step: 8
Training loss: 2.948125123977661
Validation loss: 2.3760138865440124

Epoch: 6| Step: 9
Training loss: 2.888723134994507
Validation loss: 2.3725296297381

Epoch: 6| Step: 10
Training loss: 2.989072799682617
Validation loss: 2.3742462640167563

Epoch: 6| Step: 11
Training loss: 1.889496088027954
Validation loss: 2.3730067181330856

Epoch: 6| Step: 12
Training loss: 3.0872550010681152
Validation loss: 2.37303267499452

Epoch: 6| Step: 13
Training loss: 1.9721482992172241
Validation loss: 2.3769277859759588

Epoch: 165| Step: 0
Training loss: 2.468559741973877
Validation loss: 2.385900705091415

Epoch: 6| Step: 1
Training loss: 2.7484123706817627
Validation loss: 2.3924842547344904

Epoch: 6| Step: 2
Training loss: 1.8541080951690674
Validation loss: 2.408787299227971

Epoch: 6| Step: 3
Training loss: 2.672464370727539
Validation loss: 2.4379574150167485

Epoch: 6| Step: 4
Training loss: 2.652430772781372
Validation loss: 2.4726547220701813

Epoch: 6| Step: 5
Training loss: 2.7503669261932373
Validation loss: 2.4961485119276148

Epoch: 6| Step: 6
Training loss: 2.379127025604248
Validation loss: 2.5020864548221713

Epoch: 6| Step: 7
Training loss: 2.658146381378174
Validation loss: 2.507156500252344

Epoch: 6| Step: 8
Training loss: 2.6695194244384766
Validation loss: 2.452557876545896

Epoch: 6| Step: 9
Training loss: 2.903322458267212
Validation loss: 2.398796009761031

Epoch: 6| Step: 10
Training loss: 2.9397671222686768
Validation loss: 2.370987066658594

Epoch: 6| Step: 11
Training loss: 2.820295810699463
Validation loss: 2.3730947368888446

Epoch: 6| Step: 12
Training loss: 2.568356990814209
Validation loss: 2.3840663740711827

Epoch: 6| Step: 13
Training loss: 2.477389097213745
Validation loss: 2.391031301149758

Epoch: 166| Step: 0
Training loss: 2.5545520782470703
Validation loss: 2.4048397746137393

Epoch: 6| Step: 1
Training loss: 2.3184738159179688
Validation loss: 2.411060596025118

Epoch: 6| Step: 2
Training loss: 2.9403064250946045
Validation loss: 2.404102094711796

Epoch: 6| Step: 3
Training loss: 3.6516451835632324
Validation loss: 2.3798616599011164

Epoch: 6| Step: 4
Training loss: 2.2653236389160156
Validation loss: 2.3729861602988294

Epoch: 6| Step: 5
Training loss: 2.382689952850342
Validation loss: 2.3709386548688336

Epoch: 6| Step: 6
Training loss: 3.2153658866882324
Validation loss: 2.367752129031766

Epoch: 6| Step: 7
Training loss: 3.061953544616699
Validation loss: 2.3708040278445006

Epoch: 6| Step: 8
Training loss: 2.5232696533203125
Validation loss: 2.368309223523704

Epoch: 6| Step: 9
Training loss: 2.6717395782470703
Validation loss: 2.366985155690101

Epoch: 6| Step: 10
Training loss: 2.296712875366211
Validation loss: 2.3709156615759737

Epoch: 6| Step: 11
Training loss: 2.079972267150879
Validation loss: 2.3838751341706965

Epoch: 6| Step: 12
Training loss: 2.0222725868225098
Validation loss: 2.3972518751698155

Epoch: 6| Step: 13
Training loss: 2.817601442337036
Validation loss: 2.4077192224482054

Epoch: 167| Step: 0
Training loss: 2.7642006874084473
Validation loss: 2.427065969795309

Epoch: 6| Step: 1
Training loss: 2.0138819217681885
Validation loss: 2.434243184263988

Epoch: 6| Step: 2
Training loss: 2.474849224090576
Validation loss: 2.425951193737727

Epoch: 6| Step: 3
Training loss: 2.8386683464050293
Validation loss: 2.4150685571855113

Epoch: 6| Step: 4
Training loss: 2.8178863525390625
Validation loss: 2.3970229112973778

Epoch: 6| Step: 5
Training loss: 2.5497543811798096
Validation loss: 2.383122326225363

Epoch: 6| Step: 6
Training loss: 2.343358039855957
Validation loss: 2.3660189592710106

Epoch: 6| Step: 7
Training loss: 2.6328155994415283
Validation loss: 2.356063876100766

Epoch: 6| Step: 8
Training loss: 2.9270763397216797
Validation loss: 2.3534015686281267

Epoch: 6| Step: 9
Training loss: 2.610227108001709
Validation loss: 2.3603418796293196

Epoch: 6| Step: 10
Training loss: 3.029552459716797
Validation loss: 2.364642363722606

Epoch: 6| Step: 11
Training loss: 3.1129117012023926
Validation loss: 2.373721421405833

Epoch: 6| Step: 12
Training loss: 2.352022647857666
Validation loss: 2.377513201005997

Epoch: 6| Step: 13
Training loss: 2.318145751953125
Validation loss: 2.3763186162517917

Epoch: 168| Step: 0
Training loss: 2.6710031032562256
Validation loss: 2.3794842432903986

Epoch: 6| Step: 1
Training loss: 2.2770848274230957
Validation loss: 2.379161011788153

Epoch: 6| Step: 2
Training loss: 2.711611747741699
Validation loss: 2.3756010378560712

Epoch: 6| Step: 3
Training loss: 2.3156561851501465
Validation loss: 2.3899848768788

Epoch: 6| Step: 4
Training loss: 2.527261734008789
Validation loss: 2.398582912260486

Epoch: 6| Step: 5
Training loss: 3.083932399749756
Validation loss: 2.406983216603597

Epoch: 6| Step: 6
Training loss: 3.1816132068634033
Validation loss: 2.4177163057429816

Epoch: 6| Step: 7
Training loss: 3.060335159301758
Validation loss: 2.4215024363610054

Epoch: 6| Step: 8
Training loss: 1.7928649187088013
Validation loss: 2.4210834169900544

Epoch: 6| Step: 9
Training loss: 2.769211769104004
Validation loss: 2.4211267912259666

Epoch: 6| Step: 10
Training loss: 3.5006840229034424
Validation loss: 2.4276330701766478

Epoch: 6| Step: 11
Training loss: 1.9059386253356934
Validation loss: 2.423906659567228

Epoch: 6| Step: 12
Training loss: 2.6743884086608887
Validation loss: 2.4309876631664973

Epoch: 6| Step: 13
Training loss: 2.4238781929016113
Validation loss: 2.4548105116813415

Epoch: 169| Step: 0
Training loss: 3.0135765075683594
Validation loss: 2.4535468752666185

Epoch: 6| Step: 1
Training loss: 2.4631147384643555
Validation loss: 2.452406129529399

Epoch: 6| Step: 2
Training loss: 2.6127588748931885
Validation loss: 2.4586664092156196

Epoch: 6| Step: 3
Training loss: 2.507232189178467
Validation loss: 2.458724291093888

Epoch: 6| Step: 4
Training loss: 2.684998035430908
Validation loss: 2.459909551887102

Epoch: 6| Step: 5
Training loss: 1.5217374563217163
Validation loss: 2.4561249133079284

Epoch: 6| Step: 6
Training loss: 2.924063205718994
Validation loss: 2.4413149869570168

Epoch: 6| Step: 7
Training loss: 2.639742612838745
Validation loss: 2.427849672173941

Epoch: 6| Step: 8
Training loss: 2.2484958171844482
Validation loss: 2.4107745552575714

Epoch: 6| Step: 9
Training loss: 3.046888828277588
Validation loss: 2.401465205736058

Epoch: 6| Step: 10
Training loss: 3.1066527366638184
Validation loss: 2.3750333862919963

Epoch: 6| Step: 11
Training loss: 2.804230213165283
Validation loss: 2.3535021761412263

Epoch: 6| Step: 12
Training loss: 2.7773537635803223
Validation loss: 2.3578215696478404

Epoch: 6| Step: 13
Training loss: 2.2820165157318115
Validation loss: 2.3657302318080777

Epoch: 170| Step: 0
Training loss: 2.537358522415161
Validation loss: 2.370827033955564

Epoch: 6| Step: 1
Training loss: 2.680190086364746
Validation loss: 2.37838549511407

Epoch: 6| Step: 2
Training loss: 2.8925061225891113
Validation loss: 2.375356356302897

Epoch: 6| Step: 3
Training loss: 3.1933274269104004
Validation loss: 2.375474541418014

Epoch: 6| Step: 4
Training loss: 2.8924856185913086
Validation loss: 2.3742513400252148

Epoch: 6| Step: 5
Training loss: 2.641860246658325
Validation loss: 2.375288709517448

Epoch: 6| Step: 6
Training loss: 2.3554365634918213
Validation loss: 2.372028402102891

Epoch: 6| Step: 7
Training loss: 2.6448116302490234
Validation loss: 2.369973285223848

Epoch: 6| Step: 8
Training loss: 2.4199509620666504
Validation loss: 2.3627126011797177

Epoch: 6| Step: 9
Training loss: 1.952317476272583
Validation loss: 2.358039471410936

Epoch: 6| Step: 10
Training loss: 2.961937427520752
Validation loss: 2.35447782598516

Epoch: 6| Step: 11
Training loss: 2.184244394302368
Validation loss: 2.354966660981537

Epoch: 6| Step: 12
Training loss: 2.56632924079895
Validation loss: 2.353306829288442

Epoch: 6| Step: 13
Training loss: 3.111811637878418
Validation loss: 2.356358879355974

Epoch: 171| Step: 0
Training loss: 3.375643730163574
Validation loss: 2.3582432141868015

Epoch: 6| Step: 1
Training loss: 2.9220166206359863
Validation loss: 2.355711001221852

Epoch: 6| Step: 2
Training loss: 2.0165133476257324
Validation loss: 2.3652725706818285

Epoch: 6| Step: 3
Training loss: 2.607330560684204
Validation loss: 2.3613847968398884

Epoch: 6| Step: 4
Training loss: 2.9695208072662354
Validation loss: 2.3586556424376783

Epoch: 6| Step: 5
Training loss: 2.0450356006622314
Validation loss: 2.3619696735053934

Epoch: 6| Step: 6
Training loss: 2.724374771118164
Validation loss: 2.3615457575808287

Epoch: 6| Step: 7
Training loss: 2.4262022972106934
Validation loss: 2.360519004124467

Epoch: 6| Step: 8
Training loss: 2.4823243618011475
Validation loss: 2.367513874525665

Epoch: 6| Step: 9
Training loss: 3.0400867462158203
Validation loss: 2.366977773686891

Epoch: 6| Step: 10
Training loss: 2.2282612323760986
Validation loss: 2.3657945561152633

Epoch: 6| Step: 11
Training loss: 2.520512580871582
Validation loss: 2.3604738455946728

Epoch: 6| Step: 12
Training loss: 3.002101182937622
Validation loss: 2.3635552878020913

Epoch: 6| Step: 13
Training loss: 1.7331496477127075
Validation loss: 2.362800636599141

Epoch: 172| Step: 0
Training loss: 2.9355406761169434
Validation loss: 2.3668256523788616

Epoch: 6| Step: 1
Training loss: 2.946530818939209
Validation loss: 2.3681954132613314

Epoch: 6| Step: 2
Training loss: 2.932513952255249
Validation loss: 2.379274273431429

Epoch: 6| Step: 3
Training loss: 2.4246976375579834
Validation loss: 2.376848448989212

Epoch: 6| Step: 4
Training loss: 2.3535714149475098
Validation loss: 2.3798558635096394

Epoch: 6| Step: 5
Training loss: 2.7636842727661133
Validation loss: 2.388817528242706

Epoch: 6| Step: 6
Training loss: 2.170107126235962
Validation loss: 2.3780378936439432

Epoch: 6| Step: 7
Training loss: 2.624748706817627
Validation loss: 2.386563047286003

Epoch: 6| Step: 8
Training loss: 2.746619462966919
Validation loss: 2.378756397513933

Epoch: 6| Step: 9
Training loss: 2.8460779190063477
Validation loss: 2.3726333136199624

Epoch: 6| Step: 10
Training loss: 2.7602415084838867
Validation loss: 2.3644047578175864

Epoch: 6| Step: 11
Training loss: 1.987822413444519
Validation loss: 2.3575519925804547

Epoch: 6| Step: 12
Training loss: 2.402480363845825
Validation loss: 2.3514025749698764

Epoch: 6| Step: 13
Training loss: 2.2349507808685303
Validation loss: 2.3461147533949984

Epoch: 173| Step: 0
Training loss: 2.930634021759033
Validation loss: 2.3478096505647064

Epoch: 6| Step: 1
Training loss: 2.8588614463806152
Validation loss: 2.349299838466029

Epoch: 6| Step: 2
Training loss: 2.4284749031066895
Validation loss: 2.346986796266289

Epoch: 6| Step: 3
Training loss: 3.7060837745666504
Validation loss: 2.346391370219569

Epoch: 6| Step: 4
Training loss: 2.8621997833251953
Validation loss: 2.3471882932929584

Epoch: 6| Step: 5
Training loss: 2.161552906036377
Validation loss: 2.350852832999281

Epoch: 6| Step: 6
Training loss: 2.0278754234313965
Validation loss: 2.3574520593048423

Epoch: 6| Step: 7
Training loss: 1.5781551599502563
Validation loss: 2.35789627926324

Epoch: 6| Step: 8
Training loss: 2.531304359436035
Validation loss: 2.3599948216510076

Epoch: 6| Step: 9
Training loss: 2.776263475418091
Validation loss: 2.3675038840181086

Epoch: 6| Step: 10
Training loss: 2.7808330059051514
Validation loss: 2.3636876101134927

Epoch: 6| Step: 11
Training loss: 2.875460624694824
Validation loss: 2.3725273814252628

Epoch: 6| Step: 12
Training loss: 2.0551977157592773
Validation loss: 2.3741416905515935

Epoch: 6| Step: 13
Training loss: 2.758103609085083
Validation loss: 2.3777187178211827

Epoch: 174| Step: 0
Training loss: 2.39578914642334
Validation loss: 2.3785230216159614

Epoch: 6| Step: 1
Training loss: 2.4051098823547363
Validation loss: 2.382326582426666

Epoch: 6| Step: 2
Training loss: 2.8170878887176514
Validation loss: 2.398217820352124

Epoch: 6| Step: 3
Training loss: 2.834294319152832
Validation loss: 2.394327514915056

Epoch: 6| Step: 4
Training loss: 2.263158082962036
Validation loss: 2.3896267978093957

Epoch: 6| Step: 5
Training loss: 2.6564717292785645
Validation loss: 2.3770505612896335

Epoch: 6| Step: 6
Training loss: 1.82596755027771
Validation loss: 2.3683952131579

Epoch: 6| Step: 7
Training loss: 2.9959568977355957
Validation loss: 2.3637859488046296

Epoch: 6| Step: 8
Training loss: 2.5181074142456055
Validation loss: 2.3544055492647233

Epoch: 6| Step: 9
Training loss: 1.9469531774520874
Validation loss: 2.35222880301937

Epoch: 6| Step: 10
Training loss: 2.809189796447754
Validation loss: 2.351221728068526

Epoch: 6| Step: 11
Training loss: 3.1685118675231934
Validation loss: 2.351133256830195

Epoch: 6| Step: 12
Training loss: 2.1743316650390625
Validation loss: 2.3476104890146563

Epoch: 6| Step: 13
Training loss: 4.092466354370117
Validation loss: 2.3499200318449285

Epoch: 175| Step: 0
Training loss: 2.827859401702881
Validation loss: 2.35034470276166

Epoch: 6| Step: 1
Training loss: 2.7294750213623047
Validation loss: 2.358013488913095

Epoch: 6| Step: 2
Training loss: 3.4775142669677734
Validation loss: 2.3461752963322464

Epoch: 6| Step: 3
Training loss: 2.4880685806274414
Validation loss: 2.3422027992945846

Epoch: 6| Step: 4
Training loss: 2.175421953201294
Validation loss: 2.3435917413362892

Epoch: 6| Step: 5
Training loss: 2.84550404548645
Validation loss: 2.3376840186375443

Epoch: 6| Step: 6
Training loss: 2.2416675090789795
Validation loss: 2.3413276313453593

Epoch: 6| Step: 7
Training loss: 1.6988296508789062
Validation loss: 2.338158337018823

Epoch: 6| Step: 8
Training loss: 2.9643399715423584
Validation loss: 2.342822890127859

Epoch: 6| Step: 9
Training loss: 3.439094305038452
Validation loss: 2.3374935965384207

Epoch: 6| Step: 10
Training loss: 2.196125030517578
Validation loss: 2.3412011336254817

Epoch: 6| Step: 11
Training loss: 2.4622254371643066
Validation loss: 2.344365822371616

Epoch: 6| Step: 12
Training loss: 1.842026710510254
Validation loss: 2.3479118693259453

Epoch: 6| Step: 13
Training loss: 3.0321593284606934
Validation loss: 2.3555491175702823

Testing loss: 2.4991974671681723

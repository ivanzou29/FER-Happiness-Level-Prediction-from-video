Epoch: 1| Step: 0
Training loss: 5.91657018661499
Validation loss: 5.163309774091167

Epoch: 5| Step: 1
Training loss: 5.340266227722168
Validation loss: 5.152871583097724

Epoch: 5| Step: 2
Training loss: 4.165652275085449
Validation loss: 5.142036243151593

Epoch: 5| Step: 3
Training loss: 4.646849632263184
Validation loss: 5.131681719133931

Epoch: 5| Step: 4
Training loss: 4.513792991638184
Validation loss: 5.120867170313353

Epoch: 5| Step: 5
Training loss: 5.868490695953369
Validation loss: 5.109348112537015

Epoch: 5| Step: 6
Training loss: 4.827081680297852
Validation loss: 5.097459290617255

Epoch: 5| Step: 7
Training loss: 5.110048770904541
Validation loss: 5.084374058631159

Epoch: 5| Step: 8
Training loss: 3.7044806480407715
Validation loss: 5.070852166862898

Epoch: 5| Step: 9
Training loss: 5.078197479248047
Validation loss: 5.056382153623847

Epoch: 5| Step: 10
Training loss: 4.729960918426514
Validation loss: 5.04123661082278

Epoch: 2| Step: 0
Training loss: 4.30915641784668
Validation loss: 5.024796696119411

Epoch: 5| Step: 1
Training loss: 4.2685017585754395
Validation loss: 5.007035014449909

Epoch: 5| Step: 2
Training loss: 4.663952350616455
Validation loss: 4.988500518183554

Epoch: 5| Step: 3
Training loss: 4.5593180656433105
Validation loss: 4.968396791847804

Epoch: 5| Step: 4
Training loss: 5.500316619873047
Validation loss: 4.947981962593653

Epoch: 5| Step: 5
Training loss: 5.475369930267334
Validation loss: 4.925307202082808

Epoch: 5| Step: 6
Training loss: 4.791867256164551
Validation loss: 4.902202083218482

Epoch: 5| Step: 7
Training loss: 3.4054534435272217
Validation loss: 4.8765538687347085

Epoch: 5| Step: 8
Training loss: 5.502976417541504
Validation loss: 4.84953446542063

Epoch: 5| Step: 9
Training loss: 4.200793266296387
Validation loss: 4.820253305537726

Epoch: 5| Step: 10
Training loss: 5.235843658447266
Validation loss: 4.791356102112801

Epoch: 3| Step: 0
Training loss: 5.009668350219727
Validation loss: 4.7600580133417605

Epoch: 5| Step: 1
Training loss: 4.512844085693359
Validation loss: 4.72740852191884

Epoch: 5| Step: 2
Training loss: 4.0222578048706055
Validation loss: 4.693561707773516

Epoch: 5| Step: 3
Training loss: 5.062090873718262
Validation loss: 4.658519821782266

Epoch: 5| Step: 4
Training loss: 4.783606052398682
Validation loss: 4.6219951773202546

Epoch: 5| Step: 5
Training loss: 5.128358840942383
Validation loss: 4.585237097996537

Epoch: 5| Step: 6
Training loss: 3.531003475189209
Validation loss: 4.544549501070413

Epoch: 5| Step: 7
Training loss: 4.960017204284668
Validation loss: 4.503647635059972

Epoch: 5| Step: 8
Training loss: 2.676640748977661
Validation loss: 4.462487969347226

Epoch: 5| Step: 9
Training loss: 4.113164901733398
Validation loss: 4.418961678781817

Epoch: 5| Step: 10
Training loss: 4.250690937042236
Validation loss: 4.376077390486194

Epoch: 4| Step: 0
Training loss: 4.004471778869629
Validation loss: 4.333033223305979

Epoch: 5| Step: 1
Training loss: 3.428928852081299
Validation loss: 4.290318840293474

Epoch: 5| Step: 2
Training loss: 3.525104522705078
Validation loss: 4.247520964632752

Epoch: 5| Step: 3
Training loss: 4.43595027923584
Validation loss: 4.203576744243663

Epoch: 5| Step: 4
Training loss: 5.132809638977051
Validation loss: 4.160867347512194

Epoch: 5| Step: 5
Training loss: 4.642260551452637
Validation loss: 4.11669861373081

Epoch: 5| Step: 6
Training loss: 4.580565452575684
Validation loss: 4.075872041845835

Epoch: 5| Step: 7
Training loss: 2.6846158504486084
Validation loss: 4.0386451751955095

Epoch: 5| Step: 8
Training loss: 3.507232666015625
Validation loss: 4.003379865359235

Epoch: 5| Step: 9
Training loss: 3.8634490966796875
Validation loss: 3.9688891723591793

Epoch: 5| Step: 10
Training loss: 3.5281152725219727
Validation loss: 3.938830103925479

Epoch: 5| Step: 0
Training loss: 3.6587815284729004
Validation loss: 3.9093793925418647

Epoch: 5| Step: 1
Training loss: 4.124205112457275
Validation loss: 3.884840355124525

Epoch: 5| Step: 2
Training loss: 3.126661777496338
Validation loss: 3.861645006364392

Epoch: 5| Step: 3
Training loss: 3.685936689376831
Validation loss: 3.838767323442685

Epoch: 5| Step: 4
Training loss: 3.905008316040039
Validation loss: 3.818310729918941

Epoch: 5| Step: 5
Training loss: 4.387456893920898
Validation loss: 3.7977863460458736

Epoch: 5| Step: 6
Training loss: 3.459522247314453
Validation loss: 3.784253130676926

Epoch: 5| Step: 7
Training loss: 2.988776922225952
Validation loss: 3.766586188347109

Epoch: 5| Step: 8
Training loss: 3.6588921546936035
Validation loss: 3.7506034886965187

Epoch: 5| Step: 9
Training loss: 3.447667360305786
Validation loss: 3.7367987632751465

Epoch: 5| Step: 10
Training loss: 4.2006049156188965
Validation loss: 3.7227233584209154

Epoch: 6| Step: 0
Training loss: 2.486675500869751
Validation loss: 3.707796981257777

Epoch: 5| Step: 1
Training loss: 4.400469779968262
Validation loss: 3.697007753515756

Epoch: 5| Step: 2
Training loss: 3.7560646533966064
Validation loss: 3.682327119253015

Epoch: 5| Step: 3
Training loss: 3.842576265335083
Validation loss: 3.673130771165253

Epoch: 5| Step: 4
Training loss: 3.9368202686309814
Validation loss: 3.6597032931543167

Epoch: 5| Step: 5
Training loss: 4.36810302734375
Validation loss: 3.648484512041974

Epoch: 5| Step: 6
Training loss: 3.385946750640869
Validation loss: 3.6360031199711624

Epoch: 5| Step: 7
Training loss: 3.077808380126953
Validation loss: 3.6256767190912718

Epoch: 5| Step: 8
Training loss: 3.4777183532714844
Validation loss: 3.6125821349441365

Epoch: 5| Step: 9
Training loss: 3.3885769844055176
Validation loss: 3.5984397037054903

Epoch: 5| Step: 10
Training loss: 2.927949905395508
Validation loss: 3.5872344893793904

Epoch: 7| Step: 0
Training loss: 3.434263229370117
Validation loss: 3.5724568008094706

Epoch: 5| Step: 1
Training loss: 3.5375030040740967
Validation loss: 3.559867253867529

Epoch: 5| Step: 2
Training loss: 3.5579707622528076
Validation loss: 3.546684572773595

Epoch: 5| Step: 3
Training loss: 3.3179049491882324
Validation loss: 3.532937336993474

Epoch: 5| Step: 4
Training loss: 2.915029525756836
Validation loss: 3.5228028861425256

Epoch: 5| Step: 5
Training loss: 4.141962051391602
Validation loss: 3.5090463135832097

Epoch: 5| Step: 6
Training loss: 3.469834566116333
Validation loss: 3.4941967815481205

Epoch: 5| Step: 7
Training loss: 2.757128953933716
Validation loss: 3.4793160294973724

Epoch: 5| Step: 8
Training loss: 3.8989219665527344
Validation loss: 3.4641301811382337

Epoch: 5| Step: 9
Training loss: 3.3711929321289062
Validation loss: 3.449651943740024

Epoch: 5| Step: 10
Training loss: 3.4740560054779053
Validation loss: 3.4358362074821227

Epoch: 8| Step: 0
Training loss: 3.196483612060547
Validation loss: 3.4247353000025593

Epoch: 5| Step: 1
Training loss: 2.834526538848877
Validation loss: 3.412738892339891

Epoch: 5| Step: 2
Training loss: 2.717897891998291
Validation loss: 3.3997330614315566

Epoch: 5| Step: 3
Training loss: 3.399941921234131
Validation loss: 3.3867187551272813

Epoch: 5| Step: 4
Training loss: 3.6380584239959717
Validation loss: 3.3776411497464744

Epoch: 5| Step: 5
Training loss: 4.086320877075195
Validation loss: 3.3669278621673584

Epoch: 5| Step: 6
Training loss: 2.60768461227417
Validation loss: 3.3548711192223335

Epoch: 5| Step: 7
Training loss: 4.534424781799316
Validation loss: 3.3391357929475847

Epoch: 5| Step: 8
Training loss: 4.300492286682129
Validation loss: 3.330065517015355

Epoch: 5| Step: 9
Training loss: 1.8148475885391235
Validation loss: 3.3197751096499863

Epoch: 5| Step: 10
Training loss: 3.353628396987915
Validation loss: 3.310606764208886

Epoch: 9| Step: 0
Training loss: 3.563668727874756
Validation loss: 3.298303311870944

Epoch: 5| Step: 1
Training loss: 3.6228561401367188
Validation loss: 3.292060711050546

Epoch: 5| Step: 2
Training loss: 3.408876895904541
Validation loss: 3.285956318660449

Epoch: 5| Step: 3
Training loss: 2.9842112064361572
Validation loss: 3.2752799910883748

Epoch: 5| Step: 4
Training loss: 3.2948200702667236
Validation loss: 3.2645098829782135

Epoch: 5| Step: 5
Training loss: 2.493631601333618
Validation loss: 3.25280176695957

Epoch: 5| Step: 6
Training loss: 3.740358829498291
Validation loss: 3.244773736564062

Epoch: 5| Step: 7
Training loss: 3.351860761642456
Validation loss: 3.236106082957278

Epoch: 5| Step: 8
Training loss: 3.1896119117736816
Validation loss: 3.22842671537912

Epoch: 5| Step: 9
Training loss: 2.5551059246063232
Validation loss: 3.2173865149098058

Epoch: 5| Step: 10
Training loss: 3.4038076400756836
Validation loss: 3.208476997190906

Epoch: 10| Step: 0
Training loss: 3.1593804359436035
Validation loss: 3.1995312577934674

Epoch: 5| Step: 1
Training loss: 4.162146091461182
Validation loss: 3.1937211508392007

Epoch: 5| Step: 2
Training loss: 2.543713092803955
Validation loss: 3.1880267717505015

Epoch: 5| Step: 3
Training loss: 2.9551076889038086
Validation loss: 3.1790165157728296

Epoch: 5| Step: 4
Training loss: 2.3629002571105957
Validation loss: 3.1719810091039187

Epoch: 5| Step: 5
Training loss: 3.097342014312744
Validation loss: 3.162041089868033

Epoch: 5| Step: 6
Training loss: 3.595705032348633
Validation loss: 3.1564201334471345

Epoch: 5| Step: 7
Training loss: 3.589838743209839
Validation loss: 3.150719796457598

Epoch: 5| Step: 8
Training loss: 3.2501473426818848
Validation loss: 3.14602259666689

Epoch: 5| Step: 9
Training loss: 3.499864101409912
Validation loss: 3.140560439837876

Epoch: 5| Step: 10
Training loss: 2.5918173789978027
Validation loss: 3.133800504028156

Epoch: 11| Step: 0
Training loss: 3.244112014770508
Validation loss: 3.126797601740847

Epoch: 5| Step: 1
Training loss: 3.2465877532958984
Validation loss: 3.122513396765596

Epoch: 5| Step: 2
Training loss: 2.9708054065704346
Validation loss: 3.1156875164278093

Epoch: 5| Step: 3
Training loss: 2.7997384071350098
Validation loss: 3.1107373519610335

Epoch: 5| Step: 4
Training loss: 2.5149059295654297
Validation loss: 3.106734347599809

Epoch: 5| Step: 5
Training loss: 3.073803424835205
Validation loss: 3.1054097939563055

Epoch: 5| Step: 6
Training loss: 3.112654685974121
Validation loss: 3.09629221629071

Epoch: 5| Step: 7
Training loss: 3.968923568725586
Validation loss: 3.090318951555478

Epoch: 5| Step: 8
Training loss: 3.2660934925079346
Validation loss: 3.0851507853436213

Epoch: 5| Step: 9
Training loss: 2.9111862182617188
Validation loss: 3.07824106113885

Epoch: 5| Step: 10
Training loss: 3.329578399658203
Validation loss: 3.0716862755437053

Epoch: 12| Step: 0
Training loss: 3.8016674518585205
Validation loss: 3.0651185486906316

Epoch: 5| Step: 1
Training loss: 2.6934266090393066
Validation loss: 3.0574355484336935

Epoch: 5| Step: 2
Training loss: 2.7645134925842285
Validation loss: 3.0511595279939714

Epoch: 5| Step: 3
Training loss: 2.429337978363037
Validation loss: 3.044752593963377

Epoch: 5| Step: 4
Training loss: 2.9572081565856934
Validation loss: 3.0377356724072526

Epoch: 5| Step: 5
Training loss: 2.664954423904419
Validation loss: 3.0314340642703477

Epoch: 5| Step: 6
Training loss: 2.6422080993652344
Validation loss: 3.0258964671883533

Epoch: 5| Step: 7
Training loss: 4.134143352508545
Validation loss: 3.019693818143619

Epoch: 5| Step: 8
Training loss: 3.166578769683838
Validation loss: 3.014345230594758

Epoch: 5| Step: 9
Training loss: 3.8669192790985107
Validation loss: 3.0087326803515033

Epoch: 5| Step: 10
Training loss: 2.727220058441162
Validation loss: 3.0025597951745473

Epoch: 13| Step: 0
Training loss: 2.391895294189453
Validation loss: 2.99667888815685

Epoch: 5| Step: 1
Training loss: 3.420042037963867
Validation loss: 2.9894586737437914

Epoch: 5| Step: 2
Training loss: 3.055779457092285
Validation loss: 2.982670863469442

Epoch: 5| Step: 3
Training loss: 2.9560132026672363
Validation loss: 2.9770773303124214

Epoch: 5| Step: 4
Training loss: 4.027651786804199
Validation loss: 2.968740511966008

Epoch: 5| Step: 5
Training loss: 2.557710647583008
Validation loss: 2.9632130079371954

Epoch: 5| Step: 6
Training loss: 2.6692702770233154
Validation loss: 2.957686483219106

Epoch: 5| Step: 7
Training loss: 2.7233545780181885
Validation loss: 2.980282240016486

Epoch: 5| Step: 8
Training loss: 3.446362257003784
Validation loss: 2.978602691363263

Epoch: 5| Step: 9
Training loss: 2.910104274749756
Validation loss: 2.9461184599066295

Epoch: 5| Step: 10
Training loss: 3.386375904083252
Validation loss: 2.94231608862518

Epoch: 14| Step: 0
Training loss: 2.685922622680664
Validation loss: 2.9450634602577455

Epoch: 5| Step: 1
Training loss: 3.385411024093628
Validation loss: 2.9454935212289133

Epoch: 5| Step: 2
Training loss: 2.273181438446045
Validation loss: 2.9428447933607202

Epoch: 5| Step: 3
Training loss: 2.7396466732025146
Validation loss: 2.940066073530464

Epoch: 5| Step: 4
Training loss: 3.636462450027466
Validation loss: 2.933296175413234

Epoch: 5| Step: 5
Training loss: 3.3030662536621094
Validation loss: 2.9281927898365963

Epoch: 5| Step: 6
Training loss: 3.365589141845703
Validation loss: 2.922781639201667

Epoch: 5| Step: 7
Training loss: 3.3169732093811035
Validation loss: 2.9151269389737036

Epoch: 5| Step: 8
Training loss: 2.4420828819274902
Validation loss: 2.914509288726314

Epoch: 5| Step: 9
Training loss: 2.9954288005828857
Validation loss: 2.9127254768084456

Epoch: 5| Step: 10
Training loss: 3.0140433311462402
Validation loss: 2.9149927528955604

Epoch: 15| Step: 0
Training loss: 2.2204842567443848
Validation loss: 2.909836135884767

Epoch: 5| Step: 1
Training loss: 2.714608669281006
Validation loss: 2.9255943144521406

Epoch: 5| Step: 2
Training loss: 2.111999034881592
Validation loss: 2.9274034397576445

Epoch: 5| Step: 3
Training loss: 3.642902374267578
Validation loss: 2.899914018569454

Epoch: 5| Step: 4
Training loss: 2.343780040740967
Validation loss: 2.887886780564503

Epoch: 5| Step: 5
Training loss: 3.397116184234619
Validation loss: 2.8943378899687078

Epoch: 5| Step: 6
Training loss: 3.8424811363220215
Validation loss: 2.9018167962310133

Epoch: 5| Step: 7
Training loss: 2.913008689880371
Validation loss: 2.8990697296716834

Epoch: 5| Step: 8
Training loss: 3.033963441848755
Validation loss: 2.8933963724361953

Epoch: 5| Step: 9
Training loss: 3.364588975906372
Validation loss: 2.890951964163011

Epoch: 5| Step: 10
Training loss: 3.4562346935272217
Validation loss: 2.8864913243119434

Epoch: 16| Step: 0
Training loss: 3.3383777141571045
Validation loss: 2.880169371122955

Epoch: 5| Step: 1
Training loss: 2.796898603439331
Validation loss: 2.880199514409547

Epoch: 5| Step: 2
Training loss: 2.1820340156555176
Validation loss: 2.8770371431945474

Epoch: 5| Step: 3
Training loss: 3.2290444374084473
Validation loss: 2.8788875431142826

Epoch: 5| Step: 4
Training loss: 2.9737865924835205
Validation loss: 2.8765984478817193

Epoch: 5| Step: 5
Training loss: 3.1284873485565186
Validation loss: 2.8715963491829495

Epoch: 5| Step: 6
Training loss: 3.2854907512664795
Validation loss: 2.8638243905959593

Epoch: 5| Step: 7
Training loss: 2.79876971244812
Validation loss: 2.855931105152253

Epoch: 5| Step: 8
Training loss: 2.9247026443481445
Validation loss: 2.852007145522743

Epoch: 5| Step: 9
Training loss: 2.823357343673706
Validation loss: 2.8469920991569437

Epoch: 5| Step: 10
Training loss: 3.2613866329193115
Validation loss: 2.8432092666625977

Epoch: 17| Step: 0
Training loss: 2.869788646697998
Validation loss: 2.8461477551408993

Epoch: 5| Step: 1
Training loss: 2.9464828968048096
Validation loss: 2.8621515048447477

Epoch: 5| Step: 2
Training loss: 3.3787670135498047
Validation loss: 2.8405467335895827

Epoch: 5| Step: 3
Training loss: 2.0973944664001465
Validation loss: 2.834505186286024

Epoch: 5| Step: 4
Training loss: 2.675178050994873
Validation loss: 2.831026577180432

Epoch: 5| Step: 5
Training loss: 3.06551456451416
Validation loss: 2.8276929547709804

Epoch: 5| Step: 6
Training loss: 3.7836098670959473
Validation loss: 2.825688146775769

Epoch: 5| Step: 7
Training loss: 3.1493420600891113
Validation loss: 2.826125832014186

Epoch: 5| Step: 8
Training loss: 2.4758691787719727
Validation loss: 2.8215105969418763

Epoch: 5| Step: 9
Training loss: 3.0070903301239014
Validation loss: 2.816532204228063

Epoch: 5| Step: 10
Training loss: 3.0223169326782227
Validation loss: 2.8179135835298927

Epoch: 18| Step: 0
Training loss: 2.3986575603485107
Validation loss: 2.8165834539680072

Epoch: 5| Step: 1
Training loss: 2.130723714828491
Validation loss: 2.8133149121397283

Epoch: 5| Step: 2
Training loss: 1.9480106830596924
Validation loss: 2.8148748105572117

Epoch: 5| Step: 3
Training loss: 3.8186442852020264
Validation loss: 2.810944962245162

Epoch: 5| Step: 4
Training loss: 3.081043243408203
Validation loss: 2.8072389043787473

Epoch: 5| Step: 5
Training loss: 3.443033218383789
Validation loss: 2.808383082830778

Epoch: 5| Step: 6
Training loss: 3.7955307960510254
Validation loss: 2.8036305084023425

Epoch: 5| Step: 7
Training loss: 3.6655662059783936
Validation loss: 2.8013994514301257

Epoch: 5| Step: 8
Training loss: 2.677886962890625
Validation loss: 2.8042715672523744

Epoch: 5| Step: 9
Training loss: 2.4808707237243652
Validation loss: 2.808638172764932

Epoch: 5| Step: 10
Training loss: 2.863492250442505
Validation loss: 2.806227837839434

Epoch: 19| Step: 0
Training loss: 2.6217944622039795
Validation loss: 2.802696351082094

Epoch: 5| Step: 1
Training loss: 2.225956439971924
Validation loss: 2.8046964163421304

Epoch: 5| Step: 2
Training loss: 2.740199565887451
Validation loss: 2.803500016530355

Epoch: 5| Step: 3
Training loss: 3.432584047317505
Validation loss: 2.800957561821066

Epoch: 5| Step: 4
Training loss: 2.8706297874450684
Validation loss: 2.7983896424693446

Epoch: 5| Step: 5
Training loss: 3.5730881690979004
Validation loss: 2.798543691635132

Epoch: 5| Step: 6
Training loss: 2.5090432167053223
Validation loss: 2.796439068291777

Epoch: 5| Step: 7
Training loss: 3.2816429138183594
Validation loss: 2.791488086023638

Epoch: 5| Step: 8
Training loss: 2.9152562618255615
Validation loss: 2.7911601348589827

Epoch: 5| Step: 9
Training loss: 2.8472485542297363
Validation loss: 2.788041868517476

Epoch: 5| Step: 10
Training loss: 3.238438844680786
Validation loss: 2.784673313940725

Epoch: 20| Step: 0
Training loss: 2.4872305393218994
Validation loss: 2.7855384939460346

Epoch: 5| Step: 1
Training loss: 3.156749725341797
Validation loss: 2.7814333977237826

Epoch: 5| Step: 2
Training loss: 2.5030336380004883
Validation loss: 2.780658127159201

Epoch: 5| Step: 3
Training loss: 2.842880964279175
Validation loss: 2.7787234449899323

Epoch: 5| Step: 4
Training loss: 2.5566017627716064
Validation loss: 2.777589754391742

Epoch: 5| Step: 5
Training loss: 3.804011583328247
Validation loss: 2.779135124657744

Epoch: 5| Step: 6
Training loss: 3.236513137817383
Validation loss: 2.776103388878607

Epoch: 5| Step: 7
Training loss: 3.4809746742248535
Validation loss: 2.773587703704834

Epoch: 5| Step: 8
Training loss: 2.2845253944396973
Validation loss: 2.7727635034950833

Epoch: 5| Step: 9
Training loss: 2.536736488342285
Validation loss: 2.771825567368538

Epoch: 5| Step: 10
Training loss: 3.2141101360321045
Validation loss: 2.7738256710831837

Epoch: 21| Step: 0
Training loss: 4.129726409912109
Validation loss: 2.806621043912826

Epoch: 5| Step: 1
Training loss: 2.919299602508545
Validation loss: 2.7853085123082644

Epoch: 5| Step: 2
Training loss: 2.1788482666015625
Validation loss: 2.763762325368902

Epoch: 5| Step: 3
Training loss: 2.518068313598633
Validation loss: 2.7691427892254246

Epoch: 5| Step: 4
Training loss: 2.9885478019714355
Validation loss: 2.774836124912385

Epoch: 5| Step: 5
Training loss: 2.57083797454834
Validation loss: 2.7713943450681624

Epoch: 5| Step: 6
Training loss: 2.022580146789551
Validation loss: 2.765260801520399

Epoch: 5| Step: 7
Training loss: 3.1470227241516113
Validation loss: 2.7603697751158025

Epoch: 5| Step: 8
Training loss: 3.1193299293518066
Validation loss: 2.7569184200738066

Epoch: 5| Step: 9
Training loss: 2.8027195930480957
Validation loss: 2.7580822437040267

Epoch: 5| Step: 10
Training loss: 3.7590932846069336
Validation loss: 2.7623848428008375

Epoch: 22| Step: 0
Training loss: 3.0807929039001465
Validation loss: 2.762748877207438

Epoch: 5| Step: 1
Training loss: 2.4141831398010254
Validation loss: 2.767106750959991

Epoch: 5| Step: 2
Training loss: 3.700953960418701
Validation loss: 2.76587781342127

Epoch: 5| Step: 3
Training loss: 2.4635512828826904
Validation loss: 2.7592484694655224

Epoch: 5| Step: 4
Training loss: 2.8386058807373047
Validation loss: 2.7574805521195933

Epoch: 5| Step: 5
Training loss: 2.564142942428589
Validation loss: 2.752183909057289

Epoch: 5| Step: 6
Training loss: 2.8487536907196045
Validation loss: 2.7528800836173435

Epoch: 5| Step: 7
Training loss: 2.9267919063568115
Validation loss: 2.7518607929188716

Epoch: 5| Step: 8
Training loss: 3.2989323139190674
Validation loss: 2.752752575823056

Epoch: 5| Step: 9
Training loss: 3.534825563430786
Validation loss: 2.7480557067419893

Epoch: 5| Step: 10
Training loss: 2.0788767337799072
Validation loss: 2.74781213268157

Epoch: 23| Step: 0
Training loss: 2.752429485321045
Validation loss: 2.7449907102892475

Epoch: 5| Step: 1
Training loss: 2.7383155822753906
Validation loss: 2.736944019153554

Epoch: 5| Step: 2
Training loss: 3.304197311401367
Validation loss: 2.736964323187387

Epoch: 5| Step: 3
Training loss: 3.3445191383361816
Validation loss: 2.7361647518732215

Epoch: 5| Step: 4
Training loss: 2.161663055419922
Validation loss: 2.7442797858227967

Epoch: 5| Step: 5
Training loss: 3.1653475761413574
Validation loss: 2.75031179253773

Epoch: 5| Step: 6
Training loss: 2.8771889209747314
Validation loss: 2.73892988440811

Epoch: 5| Step: 7
Training loss: 3.285369873046875
Validation loss: 2.7366923721887733

Epoch: 5| Step: 8
Training loss: 2.7356386184692383
Validation loss: 2.7311849414661364

Epoch: 5| Step: 9
Training loss: 2.4810738563537598
Validation loss: 2.7295426143113004

Epoch: 5| Step: 10
Training loss: 2.937504768371582
Validation loss: 2.7282611964851298

Epoch: 24| Step: 0
Training loss: 2.9234964847564697
Validation loss: 2.7310776505419003

Epoch: 5| Step: 1
Training loss: 2.4411377906799316
Validation loss: 2.7299570088745444

Epoch: 5| Step: 2
Training loss: 3.8935508728027344
Validation loss: 2.7254559557924987

Epoch: 5| Step: 3
Training loss: 2.5003228187561035
Validation loss: 2.7296604443621892

Epoch: 5| Step: 4
Training loss: 2.51806902885437
Validation loss: 2.724262532367501

Epoch: 5| Step: 5
Training loss: 3.3155059814453125
Validation loss: 2.7296822993986067

Epoch: 5| Step: 6
Training loss: 2.3931756019592285
Validation loss: 2.7720841746176443

Epoch: 5| Step: 7
Training loss: 3.2599740028381348
Validation loss: 2.77174130306449

Epoch: 5| Step: 8
Training loss: 3.325855255126953
Validation loss: 2.778475361485635

Epoch: 5| Step: 9
Training loss: 2.6510353088378906
Validation loss: 2.753087869254492

Epoch: 5| Step: 10
Training loss: 2.486534595489502
Validation loss: 2.720839294054175

Epoch: 25| Step: 0
Training loss: 2.5854015350341797
Validation loss: 2.734946638025263

Epoch: 5| Step: 1
Training loss: 3.1118509769439697
Validation loss: 2.7604080246340845

Epoch: 5| Step: 2
Training loss: 2.818589210510254
Validation loss: 2.7534590818548716

Epoch: 5| Step: 3
Training loss: 2.380509376525879
Validation loss: 2.7445866805250927

Epoch: 5| Step: 4
Training loss: 3.250115156173706
Validation loss: 2.7352323993559806

Epoch: 5| Step: 5
Training loss: 3.41416597366333
Validation loss: 2.726307133192657

Epoch: 5| Step: 6
Training loss: 2.3514456748962402
Validation loss: 2.7246705460292038

Epoch: 5| Step: 7
Training loss: 3.892263889312744
Validation loss: 2.7244016483265865

Epoch: 5| Step: 8
Training loss: 2.492549180984497
Validation loss: 2.724151534418906

Epoch: 5| Step: 9
Training loss: 1.9775947332382202
Validation loss: 2.7245804853336786

Epoch: 5| Step: 10
Training loss: 3.4989686012268066
Validation loss: 2.7219411890993834

Epoch: 26| Step: 0
Training loss: 2.4487037658691406
Validation loss: 2.735486053651379

Epoch: 5| Step: 1
Training loss: 2.586876630783081
Validation loss: 2.744087234620125

Epoch: 5| Step: 2
Training loss: 3.5225014686584473
Validation loss: 2.744580953351913

Epoch: 5| Step: 3
Training loss: 3.1974034309387207
Validation loss: 2.7189185414262997

Epoch: 5| Step: 4
Training loss: 2.408944606781006
Validation loss: 2.7111969763232815

Epoch: 5| Step: 5
Training loss: 2.884838342666626
Validation loss: 2.7141516618831183

Epoch: 5| Step: 6
Training loss: 2.913318634033203
Validation loss: 2.7143414123083955

Epoch: 5| Step: 7
Training loss: 2.372074842453003
Validation loss: 2.7206522982607604

Epoch: 5| Step: 8
Training loss: 2.963421583175659
Validation loss: 2.7231561317238757

Epoch: 5| Step: 9
Training loss: 3.2060370445251465
Validation loss: 2.727358159198556

Epoch: 5| Step: 10
Training loss: 3.168360948562622
Validation loss: 2.7282182247407976

Epoch: 27| Step: 0
Training loss: 3.542509078979492
Validation loss: 2.723190684472361

Epoch: 5| Step: 1
Training loss: 2.833028793334961
Validation loss: 2.7163565440844466

Epoch: 5| Step: 2
Training loss: 2.586029052734375
Validation loss: 2.708098009068479

Epoch: 5| Step: 3
Training loss: 2.302778482437134
Validation loss: 2.7084785097388813

Epoch: 5| Step: 4
Training loss: 3.2163147926330566
Validation loss: 2.705132563908895

Epoch: 5| Step: 5
Training loss: 3.4417572021484375
Validation loss: 2.7077152754670832

Epoch: 5| Step: 6
Training loss: 2.683960199356079
Validation loss: 2.702507518952893

Epoch: 5| Step: 7
Training loss: 2.4204819202423096
Validation loss: 2.7038164369521605

Epoch: 5| Step: 8
Training loss: 2.48980712890625
Validation loss: 2.699988321591449

Epoch: 5| Step: 9
Training loss: 2.9428038597106934
Validation loss: 2.704924439871183

Epoch: 5| Step: 10
Training loss: 3.049835681915283
Validation loss: 2.7075241304213002

Epoch: 28| Step: 0
Training loss: 2.7761282920837402
Validation loss: 2.7118577700789257

Epoch: 5| Step: 1
Training loss: 1.7066638469696045
Validation loss: 2.7132167713616484

Epoch: 5| Step: 2
Training loss: 3.415735960006714
Validation loss: 2.7067729196240826

Epoch: 5| Step: 3
Training loss: 2.961272954940796
Validation loss: 2.702882018140567

Epoch: 5| Step: 4
Training loss: 2.29437518119812
Validation loss: 2.6979517372705604

Epoch: 5| Step: 5
Training loss: 3.771054744720459
Validation loss: 2.69727058820827

Epoch: 5| Step: 6
Training loss: 2.7114672660827637
Validation loss: 2.7007768025962253

Epoch: 5| Step: 7
Training loss: 2.798422336578369
Validation loss: 2.702613745966265

Epoch: 5| Step: 8
Training loss: 2.7530884742736816
Validation loss: 2.7047812349052838

Epoch: 5| Step: 9
Training loss: 3.394730806350708
Validation loss: 2.7017127083193873

Epoch: 5| Step: 10
Training loss: 2.9459497928619385
Validation loss: 2.6964645334469375

Epoch: 29| Step: 0
Training loss: 2.8888518810272217
Validation loss: 2.6927434705918833

Epoch: 5| Step: 1
Training loss: 3.1553680896759033
Validation loss: 2.690175497403709

Epoch: 5| Step: 2
Training loss: 2.8989291191101074
Validation loss: 2.6970486922930648

Epoch: 5| Step: 3
Training loss: 3.669537305831909
Validation loss: 2.6989508828809186

Epoch: 5| Step: 4
Training loss: 2.3870317935943604
Validation loss: 2.6954512878130843

Epoch: 5| Step: 5
Training loss: 2.26216197013855
Validation loss: 2.695413138276787

Epoch: 5| Step: 6
Training loss: 2.616828680038452
Validation loss: 2.6932971887691046

Epoch: 5| Step: 7
Training loss: 3.068347215652466
Validation loss: 2.6889891470632246

Epoch: 5| Step: 8
Training loss: 3.0793747901916504
Validation loss: 2.6878171454193773

Epoch: 5| Step: 9
Training loss: 2.6016337871551514
Validation loss: 2.6854466110147457

Epoch: 5| Step: 10
Training loss: 2.765596628189087
Validation loss: 2.687748767996347

Epoch: 30| Step: 0
Training loss: 2.4714579582214355
Validation loss: 2.687804381052653

Epoch: 5| Step: 1
Training loss: 2.1995434761047363
Validation loss: 2.6849452987793954

Epoch: 5| Step: 2
Training loss: 2.574119806289673
Validation loss: 2.6851901085146013

Epoch: 5| Step: 3
Training loss: 4.030085563659668
Validation loss: 2.6877229367533038

Epoch: 5| Step: 4
Training loss: 2.755131959915161
Validation loss: 2.6821910206989577

Epoch: 5| Step: 5
Training loss: 2.4342455863952637
Validation loss: 2.684147265649611

Epoch: 5| Step: 6
Training loss: 2.866598606109619
Validation loss: 2.6806579046351935

Epoch: 5| Step: 7
Training loss: 2.525210380554199
Validation loss: 2.6797983466937976

Epoch: 5| Step: 8
Training loss: 3.424203395843506
Validation loss: 2.6819635437380884

Epoch: 5| Step: 9
Training loss: 3.308422565460205
Validation loss: 2.681855311957739

Epoch: 5| Step: 10
Training loss: 2.779825448989868
Validation loss: 2.6823148778689805

Epoch: 31| Step: 0
Training loss: 2.830007791519165
Validation loss: 2.681499858056345

Epoch: 5| Step: 1
Training loss: 3.2677841186523438
Validation loss: 2.6826471820954354

Epoch: 5| Step: 2
Training loss: 2.292160749435425
Validation loss: 2.6815008912035214

Epoch: 5| Step: 3
Training loss: 3.160827159881592
Validation loss: 2.680741507519958

Epoch: 5| Step: 4
Training loss: 3.117349147796631
Validation loss: 2.677972996106712

Epoch: 5| Step: 5
Training loss: 3.3910858631134033
Validation loss: 2.6789091479393745

Epoch: 5| Step: 6
Training loss: 2.338522434234619
Validation loss: 2.679725065026232

Epoch: 5| Step: 7
Training loss: 3.0584585666656494
Validation loss: 2.682845679662561

Epoch: 5| Step: 8
Training loss: 2.3407301902770996
Validation loss: 2.677602514143913

Epoch: 5| Step: 9
Training loss: 2.523533344268799
Validation loss: 2.6844060523535616

Epoch: 5| Step: 10
Training loss: 3.059631824493408
Validation loss: 2.684267326067853

Epoch: 32| Step: 0
Training loss: 2.7291693687438965
Validation loss: 2.685060011443271

Epoch: 5| Step: 1
Training loss: 2.5227417945861816
Validation loss: 2.680529538021293

Epoch: 5| Step: 2
Training loss: 2.7521073818206787
Validation loss: 2.680188525107599

Epoch: 5| Step: 3
Training loss: 2.734905242919922
Validation loss: 2.683390135406166

Epoch: 5| Step: 4
Training loss: 2.7069742679595947
Validation loss: 2.6924907263889106

Epoch: 5| Step: 5
Training loss: 2.503370761871338
Validation loss: 2.722625517076062

Epoch: 5| Step: 6
Training loss: 2.758434772491455
Validation loss: 2.722514516563826

Epoch: 5| Step: 7
Training loss: 3.906466245651245
Validation loss: 2.719815223447738

Epoch: 5| Step: 8
Training loss: 3.447678327560425
Validation loss: 2.6873654063029955

Epoch: 5| Step: 9
Training loss: 2.440603494644165
Validation loss: 2.6746252967465307

Epoch: 5| Step: 10
Training loss: 2.902306079864502
Validation loss: 2.669765741594376

Epoch: 33| Step: 0
Training loss: 2.069802761077881
Validation loss: 2.6765898607110463

Epoch: 5| Step: 1
Training loss: 3.396221876144409
Validation loss: 2.6872097407617876

Epoch: 5| Step: 2
Training loss: 2.762655258178711
Validation loss: 2.698714410105059

Epoch: 5| Step: 3
Training loss: 2.9380531311035156
Validation loss: 2.7130796858059463

Epoch: 5| Step: 4
Training loss: 3.2983970642089844
Validation loss: 2.7077855704933085

Epoch: 5| Step: 5
Training loss: 3.0147175788879395
Validation loss: 2.692649608017296

Epoch: 5| Step: 6
Training loss: 3.3530852794647217
Validation loss: 2.6931961018552064

Epoch: 5| Step: 7
Training loss: 2.6468682289123535
Validation loss: 2.6852226846961567

Epoch: 5| Step: 8
Training loss: 2.473008155822754
Validation loss: 2.6783119465715144

Epoch: 5| Step: 9
Training loss: 2.5114779472351074
Validation loss: 2.6778378999361427

Epoch: 5| Step: 10
Training loss: 2.980530261993408
Validation loss: 2.6836947612864996

Epoch: 34| Step: 0
Training loss: 2.189845085144043
Validation loss: 2.6900211841829362

Epoch: 5| Step: 1
Training loss: 2.085085391998291
Validation loss: 2.6929923026792464

Epoch: 5| Step: 2
Training loss: 3.948214292526245
Validation loss: 2.6983949753545944

Epoch: 5| Step: 3
Training loss: 2.435664653778076
Validation loss: 2.706620390697192

Epoch: 5| Step: 4
Training loss: 2.6709747314453125
Validation loss: 2.709158533362932

Epoch: 5| Step: 5
Training loss: 3.086509943008423
Validation loss: 2.6939493199830413

Epoch: 5| Step: 6
Training loss: 3.1557745933532715
Validation loss: 2.6863854572337162

Epoch: 5| Step: 7
Training loss: 3.1014933586120605
Validation loss: 2.6796800474966727

Epoch: 5| Step: 8
Training loss: 3.339782238006592
Validation loss: 2.674720097613591

Epoch: 5| Step: 9
Training loss: 2.5911765098571777
Validation loss: 2.6686769121436664

Epoch: 5| Step: 10
Training loss: 2.6190404891967773
Validation loss: 2.6679056024038665

Epoch: 35| Step: 0
Training loss: 2.3364920616149902
Validation loss: 2.6655761657222623

Epoch: 5| Step: 1
Training loss: 2.5686817169189453
Validation loss: 2.668572138714534

Epoch: 5| Step: 2
Training loss: 3.3160712718963623
Validation loss: 2.6644459975663053

Epoch: 5| Step: 3
Training loss: 2.358600616455078
Validation loss: 2.6615294717973277

Epoch: 5| Step: 4
Training loss: 2.662137508392334
Validation loss: 2.655479383724992

Epoch: 5| Step: 5
Training loss: 3.233232021331787
Validation loss: 2.655850295097597

Epoch: 5| Step: 6
Training loss: 3.626406192779541
Validation loss: 2.6571465820394535

Epoch: 5| Step: 7
Training loss: 2.3382251262664795
Validation loss: 2.6545377956923617

Epoch: 5| Step: 8
Training loss: 2.8206706047058105
Validation loss: 2.6537246960465626

Epoch: 5| Step: 9
Training loss: 2.7911229133605957
Validation loss: 2.6539147361632316

Epoch: 5| Step: 10
Training loss: 3.045551061630249
Validation loss: 2.658028712836645

Epoch: 36| Step: 0
Training loss: 2.9677436351776123
Validation loss: 2.6569617281677904

Epoch: 5| Step: 1
Training loss: 2.1019651889801025
Validation loss: 2.6582399004249164

Epoch: 5| Step: 2
Training loss: 2.4485602378845215
Validation loss: 2.659161647160848

Epoch: 5| Step: 3
Training loss: 2.703385353088379
Validation loss: 2.659535038855768

Epoch: 5| Step: 4
Training loss: 3.728280544281006
Validation loss: 2.6535926531719904

Epoch: 5| Step: 5
Training loss: 2.232419013977051
Validation loss: 2.6492081995933288

Epoch: 5| Step: 6
Training loss: 3.229640245437622
Validation loss: 2.641782734983711

Epoch: 5| Step: 7
Training loss: 2.901482343673706
Validation loss: 2.6421773408048894

Epoch: 5| Step: 8
Training loss: 3.278348922729492
Validation loss: 2.640655181741202

Epoch: 5| Step: 9
Training loss: 2.2519845962524414
Validation loss: 2.6397671699523926

Epoch: 5| Step: 10
Training loss: 3.166929244995117
Validation loss: 2.6406022374347975

Epoch: 37| Step: 0
Training loss: 2.670849561691284
Validation loss: 2.644676095695906

Epoch: 5| Step: 1
Training loss: 2.037626266479492
Validation loss: 2.657653870121125

Epoch: 5| Step: 2
Training loss: 2.410010576248169
Validation loss: 2.6591661104591946

Epoch: 5| Step: 3
Training loss: 4.156494140625
Validation loss: 2.6454710396387244

Epoch: 5| Step: 4
Training loss: 2.710601568222046
Validation loss: 2.6384193871610906

Epoch: 5| Step: 5
Training loss: 2.2462031841278076
Validation loss: 2.634855672877322

Epoch: 5| Step: 6
Training loss: 3.7103874683380127
Validation loss: 2.6362505548743793

Epoch: 5| Step: 7
Training loss: 2.098877191543579
Validation loss: 2.634777740765643

Epoch: 5| Step: 8
Training loss: 3.246065616607666
Validation loss: 2.63803937614605

Epoch: 5| Step: 9
Training loss: 2.690154790878296
Validation loss: 2.6360476324635167

Epoch: 5| Step: 10
Training loss: 2.9197375774383545
Validation loss: 2.6388055560409382

Epoch: 38| Step: 0
Training loss: 2.269470453262329
Validation loss: 2.6464432260041595

Epoch: 5| Step: 1
Training loss: 2.767367124557495
Validation loss: 2.649862984175323

Epoch: 5| Step: 2
Training loss: 2.4635801315307617
Validation loss: 2.6547990024730725

Epoch: 5| Step: 3
Training loss: 2.3381152153015137
Validation loss: 2.65899395942688

Epoch: 5| Step: 4
Training loss: 3.253528118133545
Validation loss: 2.641055994136359

Epoch: 5| Step: 5
Training loss: 2.924586057662964
Validation loss: 2.636862139548025

Epoch: 5| Step: 6
Training loss: 2.8666720390319824
Validation loss: 2.6354076708516767

Epoch: 5| Step: 7
Training loss: 2.8068599700927734
Validation loss: 2.6345867905565488

Epoch: 5| Step: 8
Training loss: 3.365798234939575
Validation loss: 2.635474028125886

Epoch: 5| Step: 9
Training loss: 3.3890979290008545
Validation loss: 2.6390225477116083

Epoch: 5| Step: 10
Training loss: 2.32613205909729
Validation loss: 2.6458737722007175

Epoch: 39| Step: 0
Training loss: 3.274890184402466
Validation loss: 2.6457595876468125

Epoch: 5| Step: 1
Training loss: 3.1036763191223145
Validation loss: 2.633645796006726

Epoch: 5| Step: 2
Training loss: 2.6076865196228027
Validation loss: 2.628397818534605

Epoch: 5| Step: 3
Training loss: 2.356292724609375
Validation loss: 2.6270211281314975

Epoch: 5| Step: 4
Training loss: 3.058636426925659
Validation loss: 2.6308306801703667

Epoch: 5| Step: 5
Training loss: 2.4419307708740234
Validation loss: 2.6346783381636425

Epoch: 5| Step: 6
Training loss: 2.319380283355713
Validation loss: 2.632688355702226

Epoch: 5| Step: 7
Training loss: 2.786820888519287
Validation loss: 2.6330147815007034

Epoch: 5| Step: 8
Training loss: 2.3000824451446533
Validation loss: 2.6331027400109077

Epoch: 5| Step: 9
Training loss: 3.7474265098571777
Validation loss: 2.64581964605598

Epoch: 5| Step: 10
Training loss: 2.8073911666870117
Validation loss: 2.6453696348333873

Epoch: 40| Step: 0
Training loss: 2.6381168365478516
Validation loss: 2.65426222226953

Epoch: 5| Step: 1
Training loss: 2.95093035697937
Validation loss: 2.641458883080431

Epoch: 5| Step: 2
Training loss: 3.7028915882110596
Validation loss: 2.6371499107730005

Epoch: 5| Step: 3
Training loss: 2.808263063430786
Validation loss: 2.62121598182186

Epoch: 5| Step: 4
Training loss: 3.4449870586395264
Validation loss: 2.6226038907163884

Epoch: 5| Step: 5
Training loss: 3.152667284011841
Validation loss: 2.6185483291584957

Epoch: 5| Step: 6
Training loss: 2.1715540885925293
Validation loss: 2.619225735305458

Epoch: 5| Step: 7
Training loss: 2.0202133655548096
Validation loss: 2.6164911024032103

Epoch: 5| Step: 8
Training loss: 2.8623194694519043
Validation loss: 2.616865696445588

Epoch: 5| Step: 9
Training loss: 2.1424410343170166
Validation loss: 2.615375493162422

Epoch: 5| Step: 10
Training loss: 2.827349901199341
Validation loss: 2.6180232314653296

Epoch: 41| Step: 0
Training loss: 2.4737963676452637
Validation loss: 2.622664156780448

Epoch: 5| Step: 1
Training loss: 2.1500308513641357
Validation loss: 2.635052914260536

Epoch: 5| Step: 2
Training loss: 3.678525924682617
Validation loss: 2.6437435919238674

Epoch: 5| Step: 3
Training loss: 3.027029514312744
Validation loss: 2.647096526238226

Epoch: 5| Step: 4
Training loss: 2.5820517539978027
Validation loss: 2.621689719538535

Epoch: 5| Step: 5
Training loss: 3.1250526905059814
Validation loss: 2.61122295677021

Epoch: 5| Step: 6
Training loss: 2.17279314994812
Validation loss: 2.6189255919507755

Epoch: 5| Step: 7
Training loss: 2.5945096015930176
Validation loss: 2.6264527766935286

Epoch: 5| Step: 8
Training loss: 2.569906711578369
Validation loss: 2.626704244203465

Epoch: 5| Step: 9
Training loss: 3.374286651611328
Validation loss: 2.62575125950639

Epoch: 5| Step: 10
Training loss: 3.1002519130706787
Validation loss: 2.6207187175750732

Epoch: 42| Step: 0
Training loss: 3.291437864303589
Validation loss: 2.6163576238898822

Epoch: 5| Step: 1
Training loss: 2.5271575450897217
Validation loss: 2.611942316896172

Epoch: 5| Step: 2
Training loss: 2.532658338546753
Validation loss: 2.6095484302889917

Epoch: 5| Step: 3
Training loss: 2.5683114528656006
Validation loss: 2.6183735580854517

Epoch: 5| Step: 4
Training loss: 3.84370493888855
Validation loss: 2.615745416251562

Epoch: 5| Step: 5
Training loss: 2.385840654373169
Validation loss: 2.6124621924533638

Epoch: 5| Step: 6
Training loss: 2.749501943588257
Validation loss: 2.61407377899334

Epoch: 5| Step: 7
Training loss: 2.30320405960083
Validation loss: 2.610867525941582

Epoch: 5| Step: 8
Training loss: 3.2448506355285645
Validation loss: 2.612904728099864

Epoch: 5| Step: 9
Training loss: 2.580280065536499
Validation loss: 2.612946899988318

Epoch: 5| Step: 10
Training loss: 2.4701337814331055
Validation loss: 2.612260905645227

Epoch: 43| Step: 0
Training loss: 3.348170757293701
Validation loss: 2.613104389559838

Epoch: 5| Step: 1
Training loss: 2.387221574783325
Validation loss: 2.613515059153239

Epoch: 5| Step: 2
Training loss: 2.126655101776123
Validation loss: 2.6095591411795667

Epoch: 5| Step: 3
Training loss: 3.1163058280944824
Validation loss: 2.6108907217620523

Epoch: 5| Step: 4
Training loss: 2.603321075439453
Validation loss: 2.609261617865614

Epoch: 5| Step: 5
Training loss: 3.176456928253174
Validation loss: 2.607530583617508

Epoch: 5| Step: 6
Training loss: 2.303046703338623
Validation loss: 2.606928692069105

Epoch: 5| Step: 7
Training loss: 2.0195651054382324
Validation loss: 2.605924191013459

Epoch: 5| Step: 8
Training loss: 3.463413953781128
Validation loss: 2.606553795517132

Epoch: 5| Step: 9
Training loss: 3.0001511573791504
Validation loss: 2.6052982217522076

Epoch: 5| Step: 10
Training loss: 3.06374192237854
Validation loss: 2.6053232967212634

Epoch: 44| Step: 0
Training loss: 2.6681511402130127
Validation loss: 2.6056389013926187

Epoch: 5| Step: 1
Training loss: 2.712494373321533
Validation loss: 2.6098113777816936

Epoch: 5| Step: 2
Training loss: 2.3881382942199707
Validation loss: 2.6079441937067176

Epoch: 5| Step: 3
Training loss: 2.997551441192627
Validation loss: 2.6118826712331464

Epoch: 5| Step: 4
Training loss: 3.47182035446167
Validation loss: 2.6086368073699293

Epoch: 5| Step: 5
Training loss: 2.3837080001831055
Validation loss: 2.6114978636464765

Epoch: 5| Step: 6
Training loss: 2.899517059326172
Validation loss: 2.617689671054963

Epoch: 5| Step: 7
Training loss: 3.014827251434326
Validation loss: 2.620619635428152

Epoch: 5| Step: 8
Training loss: 3.248447895050049
Validation loss: 2.6244095935616443

Epoch: 5| Step: 9
Training loss: 2.3228142261505127
Validation loss: 2.618788711486324

Epoch: 5| Step: 10
Training loss: 2.383202075958252
Validation loss: 2.6023175229308424

Epoch: 45| Step: 0
Training loss: 2.2045300006866455
Validation loss: 2.5937582190318773

Epoch: 5| Step: 1
Training loss: 2.9535164833068848
Validation loss: 2.600611727724793

Epoch: 5| Step: 2
Training loss: 2.368445873260498
Validation loss: 2.614851156870524

Epoch: 5| Step: 3
Training loss: 3.1687073707580566
Validation loss: 2.622504690641998

Epoch: 5| Step: 4
Training loss: 3.048703908920288
Validation loss: 2.638570788086102

Epoch: 5| Step: 5
Training loss: 2.290417432785034
Validation loss: 2.6298082003029446

Epoch: 5| Step: 6
Training loss: 2.3665976524353027
Validation loss: 2.6194248302008516

Epoch: 5| Step: 7
Training loss: 2.967782974243164
Validation loss: 2.6136704209030315

Epoch: 5| Step: 8
Training loss: 3.186835765838623
Validation loss: 2.6078103434654976

Epoch: 5| Step: 9
Training loss: 3.109142541885376
Validation loss: 2.6011314468999065

Epoch: 5| Step: 10
Training loss: 3.095438003540039
Validation loss: 2.5960377877758396

Epoch: 46| Step: 0
Training loss: 3.3894448280334473
Validation loss: 2.5936622183809996

Epoch: 5| Step: 1
Training loss: 2.6791770458221436
Validation loss: 2.5923425587274695

Epoch: 5| Step: 2
Training loss: 2.3533782958984375
Validation loss: 2.5923868610012915

Epoch: 5| Step: 3
Training loss: 2.0775368213653564
Validation loss: 2.5948528500013452

Epoch: 5| Step: 4
Training loss: 2.7316393852233887
Validation loss: 2.595319660761023

Epoch: 5| Step: 5
Training loss: 2.9965131282806396
Validation loss: 2.6005482622372207

Epoch: 5| Step: 6
Training loss: 2.899780035018921
Validation loss: 2.602208447712724

Epoch: 5| Step: 7
Training loss: 2.9420437812805176
Validation loss: 2.5971916824258785

Epoch: 5| Step: 8
Training loss: 2.425429582595825
Validation loss: 2.5949584361045592

Epoch: 5| Step: 9
Training loss: 3.133546829223633
Validation loss: 2.5977678273313787

Epoch: 5| Step: 10
Training loss: 2.7682225704193115
Validation loss: 2.5945818834407355

Epoch: 47| Step: 0
Training loss: 2.9854400157928467
Validation loss: 2.6000254974570325

Epoch: 5| Step: 1
Training loss: 2.967717409133911
Validation loss: 2.597065612833987

Epoch: 5| Step: 2
Training loss: 2.3709359169006348
Validation loss: 2.59644990838984

Epoch: 5| Step: 3
Training loss: 2.244070053100586
Validation loss: 2.593451379447855

Epoch: 5| Step: 4
Training loss: 2.9969868659973145
Validation loss: 2.597184363231864

Epoch: 5| Step: 5
Training loss: 2.7351059913635254
Validation loss: 2.593175047187395

Epoch: 5| Step: 6
Training loss: 3.0547289848327637
Validation loss: 2.5903114067610873

Epoch: 5| Step: 7
Training loss: 2.7682957649230957
Validation loss: 2.590072031944029

Epoch: 5| Step: 8
Training loss: 3.0040993690490723
Validation loss: 2.5854567276534213

Epoch: 5| Step: 9
Training loss: 2.9915149211883545
Validation loss: 2.5839101217126332

Epoch: 5| Step: 10
Training loss: 2.1317410469055176
Validation loss: 2.5798668989571194

Epoch: 48| Step: 0
Training loss: 3.0373024940490723
Validation loss: 2.5812557179440736

Epoch: 5| Step: 1
Training loss: 3.2273776531219482
Validation loss: 2.584963067885368

Epoch: 5| Step: 2
Training loss: 2.0210726261138916
Validation loss: 2.5856194214154313

Epoch: 5| Step: 3
Training loss: 3.3706307411193848
Validation loss: 2.591113067442371

Epoch: 5| Step: 4
Training loss: 2.7512314319610596
Validation loss: 2.5886207447257092

Epoch: 5| Step: 5
Training loss: 2.4170069694519043
Validation loss: 2.5899389225949525

Epoch: 5| Step: 6
Training loss: 3.0267398357391357
Validation loss: 2.593652340673631

Epoch: 5| Step: 7
Training loss: 2.4713730812072754
Validation loss: 2.5980735978772564

Epoch: 5| Step: 8
Training loss: 2.5745413303375244
Validation loss: 2.5889628805140013

Epoch: 5| Step: 9
Training loss: 2.5313808917999268
Validation loss: 2.583110555525749

Epoch: 5| Step: 10
Training loss: 2.8747527599334717
Validation loss: 2.5769815598764727

Epoch: 49| Step: 0
Training loss: 2.440408706665039
Validation loss: 2.575480863612185

Epoch: 5| Step: 1
Training loss: 2.2970130443573
Validation loss: 2.5764879949631228

Epoch: 5| Step: 2
Training loss: 2.8574156761169434
Validation loss: 2.5767770403174945

Epoch: 5| Step: 3
Training loss: 1.9984813928604126
Validation loss: 2.5815578942657798

Epoch: 5| Step: 4
Training loss: 2.919431447982788
Validation loss: 2.57999167647413

Epoch: 5| Step: 5
Training loss: 2.8201260566711426
Validation loss: 2.5766026127722954

Epoch: 5| Step: 6
Training loss: 2.362168788909912
Validation loss: 2.5770815982613513

Epoch: 5| Step: 7
Training loss: 3.1867103576660156
Validation loss: 2.579171939562726

Epoch: 5| Step: 8
Training loss: 2.9257254600524902
Validation loss: 2.579284168058826

Epoch: 5| Step: 9
Training loss: 2.961364269256592
Validation loss: 2.5764418930135746

Epoch: 5| Step: 10
Training loss: 3.604954481124878
Validation loss: 2.5749675484113794

Epoch: 50| Step: 0
Training loss: 2.344125270843506
Validation loss: 2.5760925559587378

Epoch: 5| Step: 1
Training loss: 2.9694573879241943
Validation loss: 2.572886518252793

Epoch: 5| Step: 2
Training loss: 3.175279140472412
Validation loss: 2.5735311482542302

Epoch: 5| Step: 3
Training loss: 2.691453456878662
Validation loss: 2.5729010335860716

Epoch: 5| Step: 4
Training loss: 2.701326370239258
Validation loss: 2.5738405489152476

Epoch: 5| Step: 5
Training loss: 3.0936684608459473
Validation loss: 2.571240343073363

Epoch: 5| Step: 6
Training loss: 2.578744649887085
Validation loss: 2.572107820100682

Epoch: 5| Step: 7
Training loss: 2.642186164855957
Validation loss: 2.57475983711981

Epoch: 5| Step: 8
Training loss: 2.1673591136932373
Validation loss: 2.572232489944786

Epoch: 5| Step: 9
Training loss: 2.769479274749756
Validation loss: 2.578738994495843

Epoch: 5| Step: 10
Training loss: 3.115915536880493
Validation loss: 2.582786957422892

Epoch: 51| Step: 0
Training loss: 2.779080390930176
Validation loss: 2.5785888856457126

Epoch: 5| Step: 1
Training loss: 2.2278892993927
Validation loss: 2.577399969100952

Epoch: 5| Step: 2
Training loss: 3.308181047439575
Validation loss: 2.574429670969645

Epoch: 5| Step: 3
Training loss: 3.023022413253784
Validation loss: 2.571176536621586

Epoch: 5| Step: 4
Training loss: 2.3825786113739014
Validation loss: 2.572464980104918

Epoch: 5| Step: 5
Training loss: 2.6842129230499268
Validation loss: 2.5754457032808693

Epoch: 5| Step: 6
Training loss: 2.66756010055542
Validation loss: 2.568238701871646

Epoch: 5| Step: 7
Training loss: 2.381207227706909
Validation loss: 2.5665442482117684

Epoch: 5| Step: 8
Training loss: 2.691579818725586
Validation loss: 2.5673215209796862

Epoch: 5| Step: 9
Training loss: 3.3980178833007812
Validation loss: 2.5645229175526607

Epoch: 5| Step: 10
Training loss: 2.6032607555389404
Validation loss: 2.564822132869433

Epoch: 52| Step: 0
Training loss: 1.9570820331573486
Validation loss: 2.5647183361873833

Epoch: 5| Step: 1
Training loss: 3.0256032943725586
Validation loss: 2.5638248356439735

Epoch: 5| Step: 2
Training loss: 2.718365430831909
Validation loss: 2.5619995388933408

Epoch: 5| Step: 3
Training loss: 2.947476863861084
Validation loss: 2.5629024223614763

Epoch: 5| Step: 4
Training loss: 2.970902442932129
Validation loss: 2.563786277206995

Epoch: 5| Step: 5
Training loss: 2.765852689743042
Validation loss: 2.562339293059482

Epoch: 5| Step: 6
Training loss: 3.1390957832336426
Validation loss: 2.56525061720161

Epoch: 5| Step: 7
Training loss: 2.826215982437134
Validation loss: 2.5642122581440914

Epoch: 5| Step: 8
Training loss: 2.630357503890991
Validation loss: 2.5641460495610393

Epoch: 5| Step: 9
Training loss: 2.9936280250549316
Validation loss: 2.5678995270882883

Epoch: 5| Step: 10
Training loss: 2.064980983734131
Validation loss: 2.5760264447940293

Epoch: 53| Step: 0
Training loss: 1.9072380065917969
Validation loss: 2.5781142275820494

Epoch: 5| Step: 1
Training loss: 2.21651029586792
Validation loss: 2.577074755904495

Epoch: 5| Step: 2
Training loss: 3.556730270385742
Validation loss: 2.5725747769878757

Epoch: 5| Step: 3
Training loss: 3.0495972633361816
Validation loss: 2.567607759147562

Epoch: 5| Step: 4
Training loss: 2.933323383331299
Validation loss: 2.5616807399257535

Epoch: 5| Step: 5
Training loss: 2.465357780456543
Validation loss: 2.5538952401889268

Epoch: 5| Step: 6
Training loss: 2.6899795532226562
Validation loss: 2.556638774051461

Epoch: 5| Step: 7
Training loss: 2.626131057739258
Validation loss: 2.5563856106932445

Epoch: 5| Step: 8
Training loss: 2.863137722015381
Validation loss: 2.561383342230192

Epoch: 5| Step: 9
Training loss: 2.7544314861297607
Validation loss: 2.562005191720942

Epoch: 5| Step: 10
Training loss: 3.193943738937378
Validation loss: 2.564538573706022

Epoch: 54| Step: 0
Training loss: 3.152606725692749
Validation loss: 2.568632751382807

Epoch: 5| Step: 1
Training loss: 2.969376802444458
Validation loss: 2.5648723776622484

Epoch: 5| Step: 2
Training loss: 3.1771080493927
Validation loss: 2.5622042891799763

Epoch: 5| Step: 3
Training loss: 2.2406625747680664
Validation loss: 2.555748942077801

Epoch: 5| Step: 4
Training loss: 2.340700626373291
Validation loss: 2.554628397828789

Epoch: 5| Step: 5
Training loss: 2.337639331817627
Validation loss: 2.5596826409780853

Epoch: 5| Step: 6
Training loss: 3.179046869277954
Validation loss: 2.5663445431699037

Epoch: 5| Step: 7
Training loss: 3.353071689605713
Validation loss: 2.57280594559126

Epoch: 5| Step: 8
Training loss: 2.525326728820801
Validation loss: 2.5669974998761247

Epoch: 5| Step: 9
Training loss: 2.6288886070251465
Validation loss: 2.5645246505737305

Epoch: 5| Step: 10
Training loss: 2.1496682167053223
Validation loss: 2.5576188231027253

Epoch: 55| Step: 0
Training loss: 2.880030870437622
Validation loss: 2.554349450654881

Epoch: 5| Step: 1
Training loss: 2.6029458045959473
Validation loss: 2.5521854687762517

Epoch: 5| Step: 2
Training loss: 2.6958322525024414
Validation loss: 2.552746267728908

Epoch: 5| Step: 3
Training loss: 2.649685859680176
Validation loss: 2.550758264398062

Epoch: 5| Step: 4
Training loss: 3.1003127098083496
Validation loss: 2.5575280522787445

Epoch: 5| Step: 5
Training loss: 2.6761090755462646
Validation loss: 2.55678221999958

Epoch: 5| Step: 6
Training loss: 2.6797542572021484
Validation loss: 2.5579166860990625

Epoch: 5| Step: 7
Training loss: 3.0120162963867188
Validation loss: 2.552509077133671

Epoch: 5| Step: 8
Training loss: 2.801884412765503
Validation loss: 2.55152371621901

Epoch: 5| Step: 9
Training loss: 1.8222618103027344
Validation loss: 2.5529284874598184

Epoch: 5| Step: 10
Training loss: 3.1913883686065674
Validation loss: 2.553030588293588

Epoch: 56| Step: 0
Training loss: 2.45473575592041
Validation loss: 2.558174284555579

Epoch: 5| Step: 1
Training loss: 2.5282466411590576
Validation loss: 2.559459791388563

Epoch: 5| Step: 2
Training loss: 3.4700188636779785
Validation loss: 2.558895126465828

Epoch: 5| Step: 3
Training loss: 2.5994420051574707
Validation loss: 2.5583354350059264

Epoch: 5| Step: 4
Training loss: 2.733734607696533
Validation loss: 2.55931362541773

Epoch: 5| Step: 5
Training loss: 3.4322586059570312
Validation loss: 2.5558690947871052

Epoch: 5| Step: 6
Training loss: 2.2859272956848145
Validation loss: 2.549246798279465

Epoch: 5| Step: 7
Training loss: 2.289402484893799
Validation loss: 2.5494909773590746

Epoch: 5| Step: 8
Training loss: 2.6839823722839355
Validation loss: 2.548679356933922

Epoch: 5| Step: 9
Training loss: 2.5705559253692627
Validation loss: 2.549514070633919

Epoch: 5| Step: 10
Training loss: 3.0603442192077637
Validation loss: 2.548038141701811

Epoch: 57| Step: 0
Training loss: 2.8599588871002197
Validation loss: 2.5500096531324488

Epoch: 5| Step: 1
Training loss: 3.264448881149292
Validation loss: 2.5464191744404454

Epoch: 5| Step: 2
Training loss: 2.5337281227111816
Validation loss: 2.5446235236301216

Epoch: 5| Step: 3
Training loss: 2.2290961742401123
Validation loss: 2.547040770130773

Epoch: 5| Step: 4
Training loss: 3.0575695037841797
Validation loss: 2.550199362539476

Epoch: 5| Step: 5
Training loss: 2.7721991539001465
Validation loss: 2.5543748435153755

Epoch: 5| Step: 6
Training loss: 2.3979897499084473
Validation loss: 2.5613583621158393

Epoch: 5| Step: 7
Training loss: 2.797105312347412
Validation loss: 2.559229594404979

Epoch: 5| Step: 8
Training loss: 2.669461727142334
Validation loss: 2.5554484013588197

Epoch: 5| Step: 9
Training loss: 3.012422561645508
Validation loss: 2.54973155067813

Epoch: 5| Step: 10
Training loss: 2.406189203262329
Validation loss: 2.5487964948018393

Epoch: 58| Step: 0
Training loss: 2.9692678451538086
Validation loss: 2.5457794281744186

Epoch: 5| Step: 1
Training loss: 2.3575687408447266
Validation loss: 2.5425865470722155

Epoch: 5| Step: 2
Training loss: 2.5223920345306396
Validation loss: 2.5424368586591495

Epoch: 5| Step: 3
Training loss: 2.219386577606201
Validation loss: 2.542610917040097

Epoch: 5| Step: 4
Training loss: 3.441272735595703
Validation loss: 2.5454389305524927

Epoch: 5| Step: 5
Training loss: 2.8512821197509766
Validation loss: 2.5436230987630863

Epoch: 5| Step: 6
Training loss: 1.950664758682251
Validation loss: 2.546327237159975

Epoch: 5| Step: 7
Training loss: 2.6779565811157227
Validation loss: 2.5467006109094106

Epoch: 5| Step: 8
Training loss: 3.129793167114258
Validation loss: 2.5500027159208893

Epoch: 5| Step: 9
Training loss: 2.9495816230773926
Validation loss: 2.551954778291846

Epoch: 5| Step: 10
Training loss: 2.969167470932007
Validation loss: 2.5442103596143824

Epoch: 59| Step: 0
Training loss: 2.6192049980163574
Validation loss: 2.544117958314957

Epoch: 5| Step: 1
Training loss: 2.550110340118408
Validation loss: 2.5427450800454743

Epoch: 5| Step: 2
Training loss: 2.5366358757019043
Validation loss: 2.5382218283991658

Epoch: 5| Step: 3
Training loss: 2.3680920600891113
Validation loss: 2.5384314162756807

Epoch: 5| Step: 4
Training loss: 2.781043529510498
Validation loss: 2.540138613793158

Epoch: 5| Step: 5
Training loss: 3.3455443382263184
Validation loss: 2.5416458678501908

Epoch: 5| Step: 6
Training loss: 2.7579398155212402
Validation loss: 2.537753671728155

Epoch: 5| Step: 7
Training loss: 3.2251968383789062
Validation loss: 2.5389942302498767

Epoch: 5| Step: 8
Training loss: 2.771697998046875
Validation loss: 2.539686200439289

Epoch: 5| Step: 9
Training loss: 2.383486270904541
Validation loss: 2.5333052501883557

Epoch: 5| Step: 10
Training loss: 2.6189229488372803
Validation loss: 2.53494191938831

Epoch: 60| Step: 0
Training loss: 2.9650988578796387
Validation loss: 2.5327369577141217

Epoch: 5| Step: 1
Training loss: 3.174344539642334
Validation loss: 2.532586395099599

Epoch: 5| Step: 2
Training loss: 2.722116708755493
Validation loss: 2.5371597838658158

Epoch: 5| Step: 3
Training loss: 2.8729329109191895
Validation loss: 2.5397698879241943

Epoch: 5| Step: 4
Training loss: 2.1170456409454346
Validation loss: 2.547674166258945

Epoch: 5| Step: 5
Training loss: 2.523934841156006
Validation loss: 2.5612516685198714

Epoch: 5| Step: 6
Training loss: 2.4060821533203125
Validation loss: 2.5686515787596345

Epoch: 5| Step: 7
Training loss: 2.1913301944732666
Validation loss: 2.568669008952315

Epoch: 5| Step: 8
Training loss: 3.267409563064575
Validation loss: 2.559243184263988

Epoch: 5| Step: 9
Training loss: 2.97131085395813
Validation loss: 2.5467562060202322

Epoch: 5| Step: 10
Training loss: 2.7759976387023926
Validation loss: 2.53913894520011

Epoch: 61| Step: 0
Training loss: 2.8223912715911865
Validation loss: 2.529915314848705

Epoch: 5| Step: 1
Training loss: 2.515188217163086
Validation loss: 2.5312543838254866

Epoch: 5| Step: 2
Training loss: 2.735403537750244
Validation loss: 2.530830490973688

Epoch: 5| Step: 3
Training loss: 3.0768721103668213
Validation loss: 2.532507893859699

Epoch: 5| Step: 4
Training loss: 2.1026060581207275
Validation loss: 2.5291071015019573

Epoch: 5| Step: 5
Training loss: 3.7041594982147217
Validation loss: 2.528680000253903

Epoch: 5| Step: 6
Training loss: 2.334786891937256
Validation loss: 2.5284920520679925

Epoch: 5| Step: 7
Training loss: 2.792090654373169
Validation loss: 2.536577478531868

Epoch: 5| Step: 8
Training loss: 2.6453561782836914
Validation loss: 2.535319915381811

Epoch: 5| Step: 9
Training loss: 1.9555896520614624
Validation loss: 2.541778608035016

Epoch: 5| Step: 10
Training loss: 3.3204567432403564
Validation loss: 2.531524068565779

Epoch: 62| Step: 0
Training loss: 2.39302921295166
Validation loss: 2.531949781602429

Epoch: 5| Step: 1
Training loss: 2.077653408050537
Validation loss: 2.5277779486871537

Epoch: 5| Step: 2
Training loss: 3.351625442504883
Validation loss: 2.521841736250026

Epoch: 5| Step: 3
Training loss: 2.877427577972412
Validation loss: 2.518772094480453

Epoch: 5| Step: 4
Training loss: 2.135158061981201
Validation loss: 2.5238263478843113

Epoch: 5| Step: 5
Training loss: 2.503079414367676
Validation loss: 2.5184367472125637

Epoch: 5| Step: 6
Training loss: 3.629976272583008
Validation loss: 2.5179021896854525

Epoch: 5| Step: 7
Training loss: 3.2118027210235596
Validation loss: 2.520442995973813

Epoch: 5| Step: 8
Training loss: 2.106870412826538
Validation loss: 2.5178770429344586

Epoch: 5| Step: 9
Training loss: 2.7802155017852783
Validation loss: 2.523135969715734

Epoch: 5| Step: 10
Training loss: 2.7121422290802
Validation loss: 2.5278193104651665

Epoch: 63| Step: 0
Training loss: 2.3835513591766357
Validation loss: 2.53989242481929

Epoch: 5| Step: 1
Training loss: 3.2671051025390625
Validation loss: 2.5603978146788893

Epoch: 5| Step: 2
Training loss: 2.9446628093719482
Validation loss: 2.5462926562114427

Epoch: 5| Step: 3
Training loss: 2.2677478790283203
Validation loss: 2.5402786629174345

Epoch: 5| Step: 4
Training loss: 2.76710844039917
Validation loss: 2.5354026927742908

Epoch: 5| Step: 5
Training loss: 1.8918917179107666
Validation loss: 2.5374185756970475

Epoch: 5| Step: 6
Training loss: 3.206172227859497
Validation loss: 2.5271833891509683

Epoch: 5| Step: 7
Training loss: 2.359225273132324
Validation loss: 2.523471930975555

Epoch: 5| Step: 8
Training loss: 2.8497471809387207
Validation loss: 2.5189690948814474

Epoch: 5| Step: 9
Training loss: 2.5739667415618896
Validation loss: 2.513755941903719

Epoch: 5| Step: 10
Training loss: 3.3820993900299072
Validation loss: 2.5118770086637108

Epoch: 64| Step: 0
Training loss: 2.5604500770568848
Validation loss: 2.512645031816216

Epoch: 5| Step: 1
Training loss: 2.559875965118408
Validation loss: 2.5133723981918825

Epoch: 5| Step: 2
Training loss: 2.6491763591766357
Validation loss: 2.512754512089555

Epoch: 5| Step: 3
Training loss: 2.023313522338867
Validation loss: 2.5206121398556616

Epoch: 5| Step: 4
Training loss: 2.4207983016967773
Validation loss: 2.5185691643786687

Epoch: 5| Step: 5
Training loss: 2.866527557373047
Validation loss: 2.5309782720381215

Epoch: 5| Step: 6
Training loss: 2.90277361869812
Validation loss: 2.533179421578684

Epoch: 5| Step: 7
Training loss: 2.8445215225219727
Validation loss: 2.528764268403412

Epoch: 5| Step: 8
Training loss: 3.1953492164611816
Validation loss: 2.536882508185602

Epoch: 5| Step: 9
Training loss: 3.1135964393615723
Validation loss: 2.529927856178694

Epoch: 5| Step: 10
Training loss: 2.684584617614746
Validation loss: 2.525694852234215

Epoch: 65| Step: 0
Training loss: 2.0836429595947266
Validation loss: 2.5176907457331175

Epoch: 5| Step: 1
Training loss: 2.2754247188568115
Validation loss: 2.5120664181247836

Epoch: 5| Step: 2
Training loss: 2.58660888671875
Validation loss: 2.509436274087557

Epoch: 5| Step: 3
Training loss: 3.636913776397705
Validation loss: 2.507785904792047

Epoch: 5| Step: 4
Training loss: 2.977613687515259
Validation loss: 2.510275338285713

Epoch: 5| Step: 5
Training loss: 2.319490432739258
Validation loss: 2.5065086682637534

Epoch: 5| Step: 6
Training loss: 2.116637706756592
Validation loss: 2.5053215155037503

Epoch: 5| Step: 7
Training loss: 2.789900302886963
Validation loss: 2.506222296786565

Epoch: 5| Step: 8
Training loss: 3.1906495094299316
Validation loss: 2.5044172399787494

Epoch: 5| Step: 9
Training loss: 2.8407068252563477
Validation loss: 2.5042754809061685

Epoch: 5| Step: 10
Training loss: 2.920232057571411
Validation loss: 2.508576580273208

Epoch: 66| Step: 0
Training loss: 2.3275885581970215
Validation loss: 2.504761193388252

Epoch: 5| Step: 1
Training loss: 2.240269422531128
Validation loss: 2.5033138362310265

Epoch: 5| Step: 2
Training loss: 2.4963274002075195
Validation loss: 2.5022190796431674

Epoch: 5| Step: 3
Training loss: 2.7201874256134033
Validation loss: 2.503472715295771

Epoch: 5| Step: 4
Training loss: 1.9712085723876953
Validation loss: 2.5038467402099283

Epoch: 5| Step: 5
Training loss: 3.374736785888672
Validation loss: 2.5019225971673125

Epoch: 5| Step: 6
Training loss: 3.391629695892334
Validation loss: 2.5019801637177825

Epoch: 5| Step: 7
Training loss: 3.057006359100342
Validation loss: 2.503552170209987

Epoch: 5| Step: 8
Training loss: 3.0461814403533936
Validation loss: 2.5026653300049486

Epoch: 5| Step: 9
Training loss: 2.3403820991516113
Validation loss: 2.5051119327545166

Epoch: 5| Step: 10
Training loss: 2.7086126804351807
Validation loss: 2.509664463740523

Epoch: 67| Step: 0
Training loss: 3.4098267555236816
Validation loss: 2.5027814244711273

Epoch: 5| Step: 1
Training loss: 2.9631142616271973
Validation loss: 2.4993973111593597

Epoch: 5| Step: 2
Training loss: 2.8151705265045166
Validation loss: 2.5017861294490036

Epoch: 5| Step: 3
Training loss: 3.059607744216919
Validation loss: 2.501107067190191

Epoch: 5| Step: 4
Training loss: 2.3234353065490723
Validation loss: 2.5016170624763734

Epoch: 5| Step: 5
Training loss: 2.9449450969696045
Validation loss: 2.498378446025233

Epoch: 5| Step: 6
Training loss: 2.3432271480560303
Validation loss: 2.4982675070403726

Epoch: 5| Step: 7
Training loss: 2.3354885578155518
Validation loss: 2.5004282638590825

Epoch: 5| Step: 8
Training loss: 2.4161009788513184
Validation loss: 2.50880257544979

Epoch: 5| Step: 9
Training loss: 2.719243049621582
Validation loss: 2.50803239499369

Epoch: 5| Step: 10
Training loss: 2.238603115081787
Validation loss: 2.5033827007457776

Epoch: 68| Step: 0
Training loss: 2.5740785598754883
Validation loss: 2.5009506979296283

Epoch: 5| Step: 1
Training loss: 1.9304397106170654
Validation loss: 2.49781390928453

Epoch: 5| Step: 2
Training loss: 2.9200804233551025
Validation loss: 2.49536568375044

Epoch: 5| Step: 3
Training loss: 3.3628978729248047
Validation loss: 2.49180011082721

Epoch: 5| Step: 4
Training loss: 2.4025511741638184
Validation loss: 2.4959243215540403

Epoch: 5| Step: 5
Training loss: 2.8767781257629395
Validation loss: 2.496248404184977

Epoch: 5| Step: 6
Training loss: 3.006596326828003
Validation loss: 2.494066958786339

Epoch: 5| Step: 7
Training loss: 2.4272000789642334
Validation loss: 2.4945781923109487

Epoch: 5| Step: 8
Training loss: 2.3473691940307617
Validation loss: 2.4982958352693947

Epoch: 5| Step: 9
Training loss: 1.9935729503631592
Validation loss: 2.4956435593225623

Epoch: 5| Step: 10
Training loss: 3.9469096660614014
Validation loss: 2.4915185910399242

Epoch: 69| Step: 0
Training loss: 2.7303378582000732
Validation loss: 2.491975086991505

Epoch: 5| Step: 1
Training loss: 2.8192718029022217
Validation loss: 2.4937020578692035

Epoch: 5| Step: 2
Training loss: 2.1037795543670654
Validation loss: 2.493431757855159

Epoch: 5| Step: 3
Training loss: 2.789677619934082
Validation loss: 2.4933287430835027

Epoch: 5| Step: 4
Training loss: 2.5954155921936035
Validation loss: 2.493673668112806

Epoch: 5| Step: 5
Training loss: 2.7588655948638916
Validation loss: 2.497674552343225

Epoch: 5| Step: 6
Training loss: 2.5763025283813477
Validation loss: 2.5117969923121954

Epoch: 5| Step: 7
Training loss: 2.9740748405456543
Validation loss: 2.507774870882752

Epoch: 5| Step: 8
Training loss: 2.5746395587921143
Validation loss: 2.509230206089635

Epoch: 5| Step: 9
Training loss: 2.8663322925567627
Validation loss: 2.5083693688915623

Epoch: 5| Step: 10
Training loss: 2.7712268829345703
Validation loss: 2.5015527817510788

Epoch: 70| Step: 0
Training loss: 3.453202486038208
Validation loss: 2.492853759437479

Epoch: 5| Step: 1
Training loss: 2.6535351276397705
Validation loss: 2.4880067199789067

Epoch: 5| Step: 2
Training loss: 2.775890827178955
Validation loss: 2.484024668252596

Epoch: 5| Step: 3
Training loss: 2.1049792766571045
Validation loss: 2.491058688009939

Epoch: 5| Step: 4
Training loss: 2.366257429122925
Validation loss: 2.4934580992626887

Epoch: 5| Step: 5
Training loss: 3.0127923488616943
Validation loss: 2.4915323154900664

Epoch: 5| Step: 6
Training loss: 2.985795736312866
Validation loss: 2.4934406844518517

Epoch: 5| Step: 7
Training loss: 2.6702170372009277
Validation loss: 2.4833400223844793

Epoch: 5| Step: 8
Training loss: 2.519382953643799
Validation loss: 2.4860248924583517

Epoch: 5| Step: 9
Training loss: 2.9639363288879395
Validation loss: 2.4809044458532847

Epoch: 5| Step: 10
Training loss: 1.8877383470535278
Validation loss: 2.47732364490468

Epoch: 71| Step: 0
Training loss: 2.669218063354492
Validation loss: 2.479074514040383

Epoch: 5| Step: 1
Training loss: 3.5465426445007324
Validation loss: 2.4782738198516188

Epoch: 5| Step: 2
Training loss: 2.258700132369995
Validation loss: 2.49064298342633

Epoch: 5| Step: 3
Training loss: 2.8406994342803955
Validation loss: 2.492517327749601

Epoch: 5| Step: 4
Training loss: 2.820148229598999
Validation loss: 2.4914764435060563

Epoch: 5| Step: 5
Training loss: 2.916832447052002
Validation loss: 2.48895066015182

Epoch: 5| Step: 6
Training loss: 2.5688090324401855
Validation loss: 2.488252347515475

Epoch: 5| Step: 7
Training loss: 3.040942430496216
Validation loss: 2.48462652391003

Epoch: 5| Step: 8
Training loss: 2.543809413909912
Validation loss: 2.482213374107115

Epoch: 5| Step: 9
Training loss: 1.7795467376708984
Validation loss: 2.4797943561307845

Epoch: 5| Step: 10
Training loss: 2.490354299545288
Validation loss: 2.4799332311076503

Epoch: 72| Step: 0
Training loss: 1.9763166904449463
Validation loss: 2.4795674611163396

Epoch: 5| Step: 1
Training loss: 1.8581037521362305
Validation loss: 2.482011056715442

Epoch: 5| Step: 2
Training loss: 3.2029521465301514
Validation loss: 2.479516762559132

Epoch: 5| Step: 3
Training loss: 2.9266610145568848
Validation loss: 2.4768781636350896

Epoch: 5| Step: 4
Training loss: 2.546593189239502
Validation loss: 2.480939811275851

Epoch: 5| Step: 5
Training loss: 3.044456720352173
Validation loss: 2.4797983605374574

Epoch: 5| Step: 6
Training loss: 3.2295470237731934
Validation loss: 2.4801042669562885

Epoch: 5| Step: 7
Training loss: 2.5178003311157227
Validation loss: 2.4793598959522862

Epoch: 5| Step: 8
Training loss: 2.7402446269989014
Validation loss: 2.4901211569386144

Epoch: 5| Step: 9
Training loss: 2.880182981491089
Validation loss: 2.538665110065091

Epoch: 5| Step: 10
Training loss: 2.523319959640503
Validation loss: 2.4990339048447145

Epoch: 73| Step: 0
Training loss: 2.3845677375793457
Validation loss: 2.5085050598267586

Epoch: 5| Step: 1
Training loss: 2.877972364425659
Validation loss: 2.521591845379081

Epoch: 5| Step: 2
Training loss: 2.871027946472168
Validation loss: 2.5309633337041384

Epoch: 5| Step: 3
Training loss: 2.9747154712677
Validation loss: 2.531666914621989

Epoch: 5| Step: 4
Training loss: 2.9955410957336426
Validation loss: 2.5302597758590535

Epoch: 5| Step: 5
Training loss: 2.747745990753174
Validation loss: 2.5185410617500223

Epoch: 5| Step: 6
Training loss: 3.073715925216675
Validation loss: 2.5063084428028395

Epoch: 5| Step: 7
Training loss: 2.135143756866455
Validation loss: 2.4901272071305143

Epoch: 5| Step: 8
Training loss: 2.6499722003936768
Validation loss: 2.4797428410540343

Epoch: 5| Step: 9
Training loss: 2.5543079376220703
Validation loss: 2.476553231157282

Epoch: 5| Step: 10
Training loss: 2.282942771911621
Validation loss: 2.4718112484101327

Epoch: 74| Step: 0
Training loss: 2.859261989593506
Validation loss: 2.473349525082496

Epoch: 5| Step: 1
Training loss: 2.6019866466522217
Validation loss: 2.4760036340323825

Epoch: 5| Step: 2
Training loss: 3.368447780609131
Validation loss: 2.4797640615893948

Epoch: 5| Step: 3
Training loss: 1.80268132686615
Validation loss: 2.4806117729474138

Epoch: 5| Step: 4
Training loss: 3.559157133102417
Validation loss: 2.4723212616417998

Epoch: 5| Step: 5
Training loss: 2.0270144939422607
Validation loss: 2.475728519501225

Epoch: 5| Step: 6
Training loss: 2.7407195568084717
Validation loss: 2.4731297364798923

Epoch: 5| Step: 7
Training loss: 2.902226209640503
Validation loss: 2.473360202645743

Epoch: 5| Step: 8
Training loss: 2.795760154724121
Validation loss: 2.480612390784807

Epoch: 5| Step: 9
Training loss: 2.4787635803222656
Validation loss: 2.483854547623665

Epoch: 5| Step: 10
Training loss: 2.3372676372528076
Validation loss: 2.4990917892866236

Epoch: 75| Step: 0
Training loss: 2.620671510696411
Validation loss: 2.5061247861513527

Epoch: 5| Step: 1
Training loss: 3.0055348873138428
Validation loss: 2.497464515829599

Epoch: 5| Step: 2
Training loss: 3.0056259632110596
Validation loss: 2.5087552070617676

Epoch: 5| Step: 3
Training loss: 2.7804627418518066
Validation loss: 2.505406556590911

Epoch: 5| Step: 4
Training loss: 2.987086057662964
Validation loss: 2.497355256029355

Epoch: 5| Step: 5
Training loss: 2.582122802734375
Validation loss: 2.491366909396264

Epoch: 5| Step: 6
Training loss: 3.0995967388153076
Validation loss: 2.488146141011228

Epoch: 5| Step: 7
Training loss: 1.9700065851211548
Validation loss: 2.476988918037825

Epoch: 5| Step: 8
Training loss: 2.0674736499786377
Validation loss: 2.4731927969122447

Epoch: 5| Step: 9
Training loss: 2.6140389442443848
Validation loss: 2.4711448812997467

Epoch: 5| Step: 10
Training loss: 2.762568473815918
Validation loss: 2.4704446049146753

Epoch: 76| Step: 0
Training loss: 3.5885748863220215
Validation loss: 2.468687903496527

Epoch: 5| Step: 1
Training loss: 2.5987486839294434
Validation loss: 2.4735208967680573

Epoch: 5| Step: 2
Training loss: 2.6488685607910156
Validation loss: 2.470792765258461

Epoch: 5| Step: 3
Training loss: 2.499851942062378
Validation loss: 2.4710383927950295

Epoch: 5| Step: 4
Training loss: 2.709843873977661
Validation loss: 2.468730383021857

Epoch: 5| Step: 5
Training loss: 2.642134189605713
Validation loss: 2.4659559239623365

Epoch: 5| Step: 6
Training loss: 3.338085651397705
Validation loss: 2.467020615454643

Epoch: 5| Step: 7
Training loss: 2.705911159515381
Validation loss: 2.463585420321393

Epoch: 5| Step: 8
Training loss: 2.351189374923706
Validation loss: 2.4659233272716565

Epoch: 5| Step: 9
Training loss: 2.3234164714813232
Validation loss: 2.4699969394232637

Epoch: 5| Step: 10
Training loss: 1.920735478401184
Validation loss: 2.471633644514186

Epoch: 77| Step: 0
Training loss: 3.178297519683838
Validation loss: 2.4863091796957035

Epoch: 5| Step: 1
Training loss: 2.428722858428955
Validation loss: 2.494475031411776

Epoch: 5| Step: 2
Training loss: 2.789750099182129
Validation loss: 2.50529303601993

Epoch: 5| Step: 3
Training loss: 2.4328689575195312
Validation loss: 2.505471649990287

Epoch: 5| Step: 4
Training loss: 2.039262294769287
Validation loss: 2.4927592995346233

Epoch: 5| Step: 5
Training loss: 2.971555471420288
Validation loss: 2.474713827974053

Epoch: 5| Step: 6
Training loss: 3.2630200386047363
Validation loss: 2.46728277462785

Epoch: 5| Step: 7
Training loss: 2.227480888366699
Validation loss: 2.4630098727441605

Epoch: 5| Step: 8
Training loss: 2.789334774017334
Validation loss: 2.45491430323611

Epoch: 5| Step: 9
Training loss: 3.0528862476348877
Validation loss: 2.456001825230096

Epoch: 5| Step: 10
Training loss: 2.183030605316162
Validation loss: 2.4562124001082553

Epoch: 78| Step: 0
Training loss: 2.337113618850708
Validation loss: 2.4538680122744654

Epoch: 5| Step: 1
Training loss: 2.7681093215942383
Validation loss: 2.455437414107784

Epoch: 5| Step: 2
Training loss: 2.6853363513946533
Validation loss: 2.456722879922518

Epoch: 5| Step: 3
Training loss: 2.5575978755950928
Validation loss: 2.4582814811378397

Epoch: 5| Step: 4
Training loss: 2.6437878608703613
Validation loss: 2.460771729869227

Epoch: 5| Step: 5
Training loss: 3.11232328414917
Validation loss: 2.4596766733354136

Epoch: 5| Step: 6
Training loss: 2.3970680236816406
Validation loss: 2.4638872582425355

Epoch: 5| Step: 7
Training loss: 2.5257134437561035
Validation loss: 2.4618130268589145

Epoch: 5| Step: 8
Training loss: 2.691251516342163
Validation loss: 2.46078888575236

Epoch: 5| Step: 9
Training loss: 2.7363102436065674
Validation loss: 2.46238891027307

Epoch: 5| Step: 10
Training loss: 3.0175859928131104
Validation loss: 2.4586533833575506

Epoch: 79| Step: 0
Training loss: 2.6119134426116943
Validation loss: 2.459964524033249

Epoch: 5| Step: 1
Training loss: 2.6006476879119873
Validation loss: 2.4612122043486564

Epoch: 5| Step: 2
Training loss: 2.6951727867126465
Validation loss: 2.4659746667390228

Epoch: 5| Step: 3
Training loss: 2.4142098426818848
Validation loss: 2.4657078327671176

Epoch: 5| Step: 4
Training loss: 2.994126558303833
Validation loss: 2.468543424401232

Epoch: 5| Step: 5
Training loss: 2.6268932819366455
Validation loss: 2.4757800896962485

Epoch: 5| Step: 6
Training loss: 2.2796261310577393
Validation loss: 2.4828645042193833

Epoch: 5| Step: 7
Training loss: 2.823873519897461
Validation loss: 2.4831918952285603

Epoch: 5| Step: 8
Training loss: 3.2270991802215576
Validation loss: 2.472630205974784

Epoch: 5| Step: 9
Training loss: 2.7008745670318604
Validation loss: 2.4708573126023814

Epoch: 5| Step: 10
Training loss: 2.240793228149414
Validation loss: 2.4643228797502417

Epoch: 80| Step: 0
Training loss: 2.3798887729644775
Validation loss: 2.462751421877133

Epoch: 5| Step: 1
Training loss: 3.256018877029419
Validation loss: 2.4659549343970513

Epoch: 5| Step: 2
Training loss: 2.9190564155578613
Validation loss: 2.463994069765973

Epoch: 5| Step: 3
Training loss: 2.910313367843628
Validation loss: 2.4684552710543395

Epoch: 5| Step: 4
Training loss: 2.946021318435669
Validation loss: 2.461245888022966

Epoch: 5| Step: 5
Training loss: 2.854752779006958
Validation loss: 2.4573984581937074

Epoch: 5| Step: 6
Training loss: 2.667654514312744
Validation loss: 2.4562696077490367

Epoch: 5| Step: 7
Training loss: 2.0672662258148193
Validation loss: 2.457137671850061

Epoch: 5| Step: 8
Training loss: 2.6991188526153564
Validation loss: 2.4524249569062264

Epoch: 5| Step: 9
Training loss: 2.2398247718811035
Validation loss: 2.454122668953352

Epoch: 5| Step: 10
Training loss: 2.32824444770813
Validation loss: 2.4546307389454176

Epoch: 81| Step: 0
Training loss: 2.7538957595825195
Validation loss: 2.4539243175137426

Epoch: 5| Step: 1
Training loss: 2.911159038543701
Validation loss: 2.4576034494625625

Epoch: 5| Step: 2
Training loss: 2.2178120613098145
Validation loss: 2.4549087042449624

Epoch: 5| Step: 3
Training loss: 2.6539015769958496
Validation loss: 2.4570620162512666

Epoch: 5| Step: 4
Training loss: 2.5173239707946777
Validation loss: 2.457552550941385

Epoch: 5| Step: 5
Training loss: 2.65897536277771
Validation loss: 2.457490655683702

Epoch: 5| Step: 6
Training loss: 2.9098916053771973
Validation loss: 2.45329652806764

Epoch: 5| Step: 7
Training loss: 3.082571029663086
Validation loss: 2.4533880910565777

Epoch: 5| Step: 8
Training loss: 2.162163257598877
Validation loss: 2.4533555917842413

Epoch: 5| Step: 9
Training loss: 2.328242301940918
Validation loss: 2.4618492152101252

Epoch: 5| Step: 10
Training loss: 3.1874303817749023
Validation loss: 2.463733314186014

Epoch: 82| Step: 0
Training loss: 2.21919584274292
Validation loss: 2.4578360767774683

Epoch: 5| Step: 1
Training loss: 2.818702220916748
Validation loss: 2.449086558434271

Epoch: 5| Step: 2
Training loss: 3.104834794998169
Validation loss: 2.4487597121987292

Epoch: 5| Step: 3
Training loss: 3.0634379386901855
Validation loss: 2.4508319772699827

Epoch: 5| Step: 4
Training loss: 2.9512457847595215
Validation loss: 2.452951761984056

Epoch: 5| Step: 5
Training loss: 2.427704334259033
Validation loss: 2.4614359947942916

Epoch: 5| Step: 6
Training loss: 3.217042922973633
Validation loss: 2.4573796487623647

Epoch: 5| Step: 7
Training loss: 2.6020445823669434
Validation loss: 2.4541835118365545

Epoch: 5| Step: 8
Training loss: 2.5029265880584717
Validation loss: 2.4470237865242908

Epoch: 5| Step: 9
Training loss: 1.926194429397583
Validation loss: 2.44620854367492

Epoch: 5| Step: 10
Training loss: 2.4995412826538086
Validation loss: 2.4449137154445855

Epoch: 83| Step: 0
Training loss: 2.347510576248169
Validation loss: 2.4507503432612263

Epoch: 5| Step: 1
Training loss: 1.9968414306640625
Validation loss: 2.464926199246478

Epoch: 5| Step: 2
Training loss: 2.870164155960083
Validation loss: 2.4756464830008884

Epoch: 5| Step: 3
Training loss: 2.7726874351501465
Validation loss: 2.480261778318754

Epoch: 5| Step: 4
Training loss: 2.0680675506591797
Validation loss: 2.4840030670166016

Epoch: 5| Step: 5
Training loss: 1.9530525207519531
Validation loss: 2.4809876154827815

Epoch: 5| Step: 6
Training loss: 2.8242273330688477
Validation loss: 2.4728394105870235

Epoch: 5| Step: 7
Training loss: 2.6952121257781982
Validation loss: 2.4765146060656478

Epoch: 5| Step: 8
Training loss: 2.749603748321533
Validation loss: 2.4651085253684752

Epoch: 5| Step: 9
Training loss: 3.426474094390869
Validation loss: 2.4559086932930896

Epoch: 5| Step: 10
Training loss: 3.738570213317871
Validation loss: 2.4529787443017446

Epoch: 84| Step: 0
Training loss: 3.2307586669921875
Validation loss: 2.4506294419688563

Epoch: 5| Step: 1
Training loss: 2.460958957672119
Validation loss: 2.4487310071145334

Epoch: 5| Step: 2
Training loss: 2.302692174911499
Validation loss: 2.4449028609901347

Epoch: 5| Step: 3
Training loss: 2.7097365856170654
Validation loss: 2.4477142569839314

Epoch: 5| Step: 4
Training loss: 2.0820720195770264
Validation loss: 2.4453614296451693

Epoch: 5| Step: 5
Training loss: 2.480167865753174
Validation loss: 2.448674013537745

Epoch: 5| Step: 6
Training loss: 2.013617753982544
Validation loss: 2.451692665776899

Epoch: 5| Step: 7
Training loss: 2.8989975452423096
Validation loss: 2.45062316361294

Epoch: 5| Step: 8
Training loss: 3.2716407775878906
Validation loss: 2.4597310917351836

Epoch: 5| Step: 9
Training loss: 3.0097496509552
Validation loss: 2.459783008021693

Epoch: 5| Step: 10
Training loss: 2.7934775352478027
Validation loss: 2.4595386520508797

Epoch: 85| Step: 0
Training loss: 3.328472852706909
Validation loss: 2.4581837397749706

Epoch: 5| Step: 1
Training loss: 2.6843132972717285
Validation loss: 2.4542631846602245

Epoch: 5| Step: 2
Training loss: 2.562098979949951
Validation loss: 2.4425450883885866

Epoch: 5| Step: 3
Training loss: 2.61078143119812
Validation loss: 2.4460180651757026

Epoch: 5| Step: 4
Training loss: 2.5241665840148926
Validation loss: 2.4401339330980854

Epoch: 5| Step: 5
Training loss: 2.2612221240997314
Validation loss: 2.437019440435594

Epoch: 5| Step: 6
Training loss: 2.2958176136016846
Validation loss: 2.441223736732237

Epoch: 5| Step: 7
Training loss: 2.2992777824401855
Validation loss: 2.4418872351287515

Epoch: 5| Step: 8
Training loss: 2.621309757232666
Validation loss: 2.443066732857817

Epoch: 5| Step: 9
Training loss: 3.011012554168701
Validation loss: 2.442057007102556

Epoch: 5| Step: 10
Training loss: 3.0834829807281494
Validation loss: 2.441113894985568

Epoch: 86| Step: 0
Training loss: 2.8310256004333496
Validation loss: 2.438623946200135

Epoch: 5| Step: 1
Training loss: 2.3452136516571045
Validation loss: 2.439755052648565

Epoch: 5| Step: 2
Training loss: 2.995269775390625
Validation loss: 2.4372190788228023

Epoch: 5| Step: 3
Training loss: 1.8573821783065796
Validation loss: 2.4395186003818305

Epoch: 5| Step: 4
Training loss: 2.54158353805542
Validation loss: 2.4381353880769465

Epoch: 5| Step: 5
Training loss: 2.7713472843170166
Validation loss: 2.434513358659642

Epoch: 5| Step: 6
Training loss: 2.5279898643493652
Validation loss: 2.4393849680500646

Epoch: 5| Step: 7
Training loss: 3.4276554584503174
Validation loss: 2.4437820219224498

Epoch: 5| Step: 8
Training loss: 2.1811187267303467
Validation loss: 2.4462454549727903

Epoch: 5| Step: 9
Training loss: 2.936556339263916
Validation loss: 2.441851233923307

Epoch: 5| Step: 10
Training loss: 2.7890851497650146
Validation loss: 2.4401531706574144

Epoch: 87| Step: 0
Training loss: 4.0967698097229
Validation loss: 2.4390856924877373

Epoch: 5| Step: 1
Training loss: 2.189981460571289
Validation loss: 2.435354314824586

Epoch: 5| Step: 2
Training loss: 1.7770097255706787
Validation loss: 2.434313566453995

Epoch: 5| Step: 3
Training loss: 2.7843024730682373
Validation loss: 2.432130350861498

Epoch: 5| Step: 4
Training loss: 3.1307668685913086
Validation loss: 2.435755068256009

Epoch: 5| Step: 5
Training loss: 3.0792508125305176
Validation loss: 2.4383127176633446

Epoch: 5| Step: 6
Training loss: 2.0108771324157715
Validation loss: 2.4401233811532297

Epoch: 5| Step: 7
Training loss: 2.853492259979248
Validation loss: 2.4480930220696235

Epoch: 5| Step: 8
Training loss: 2.1222434043884277
Validation loss: 2.4495856838841594

Epoch: 5| Step: 9
Training loss: 2.16113543510437
Validation loss: 2.443597965343024

Epoch: 5| Step: 10
Training loss: 2.9803709983825684
Validation loss: 2.444158425895117

Epoch: 88| Step: 0
Training loss: 3.327320098876953
Validation loss: 2.4368619380458707

Epoch: 5| Step: 1
Training loss: 2.3724236488342285
Validation loss: 2.43414205889548

Epoch: 5| Step: 2
Training loss: 3.2324957847595215
Validation loss: 2.4323368790329143

Epoch: 5| Step: 3
Training loss: 2.8353660106658936
Validation loss: 2.4295172460617556

Epoch: 5| Step: 4
Training loss: 2.3167355060577393
Validation loss: 2.433874467367767

Epoch: 5| Step: 5
Training loss: 2.3965160846710205
Validation loss: 2.4314839275934363

Epoch: 5| Step: 6
Training loss: 2.750548839569092
Validation loss: 2.435917720999769

Epoch: 5| Step: 7
Training loss: 2.52596378326416
Validation loss: 2.4324871288832797

Epoch: 5| Step: 8
Training loss: 2.573456287384033
Validation loss: 2.4369699801168134

Epoch: 5| Step: 9
Training loss: 2.125415086746216
Validation loss: 2.4367199687547583

Epoch: 5| Step: 10
Training loss: 2.694106340408325
Validation loss: 2.442335867112683

Epoch: 89| Step: 0
Training loss: 3.0103583335876465
Validation loss: 2.442756986105314

Epoch: 5| Step: 1
Training loss: 2.999849557876587
Validation loss: 2.443959625818396

Epoch: 5| Step: 2
Training loss: 2.568425416946411
Validation loss: 2.4486708564143025

Epoch: 5| Step: 3
Training loss: 2.9176928997039795
Validation loss: 2.451336427401471

Epoch: 5| Step: 4
Training loss: 2.306473970413208
Validation loss: 2.451059510630946

Epoch: 5| Step: 5
Training loss: 3.275460720062256
Validation loss: 2.447586449243689

Epoch: 5| Step: 6
Training loss: 2.563220977783203
Validation loss: 2.446073309067757

Epoch: 5| Step: 7
Training loss: 2.357677698135376
Validation loss: 2.4339865356363277

Epoch: 5| Step: 8
Training loss: 2.325139284133911
Validation loss: 2.4303568922063357

Epoch: 5| Step: 9
Training loss: 2.575648069381714
Validation loss: 2.431280759073073

Epoch: 5| Step: 10
Training loss: 2.169067144393921
Validation loss: 2.427986475729173

Epoch: 90| Step: 0
Training loss: 3.003453254699707
Validation loss: 2.432478292014009

Epoch: 5| Step: 1
Training loss: 2.0743346214294434
Validation loss: 2.430686540501092

Epoch: 5| Step: 2
Training loss: 3.1017534732818604
Validation loss: 2.4426940333458687

Epoch: 5| Step: 3
Training loss: 3.016143321990967
Validation loss: 2.434535857169859

Epoch: 5| Step: 4
Training loss: 2.467193841934204
Validation loss: 2.4259795552940777

Epoch: 5| Step: 5
Training loss: 2.7869486808776855
Validation loss: 2.421381405604783

Epoch: 5| Step: 6
Training loss: 2.8052878379821777
Validation loss: 2.422416869030204

Epoch: 5| Step: 7
Training loss: 1.9133970737457275
Validation loss: 2.420968986326648

Epoch: 5| Step: 8
Training loss: 2.2399449348449707
Validation loss: 2.4218331818939536

Epoch: 5| Step: 9
Training loss: 2.902501106262207
Validation loss: 2.432907144228617

Epoch: 5| Step: 10
Training loss: 2.8032801151275635
Validation loss: 2.4389034625022643

Epoch: 91| Step: 0
Training loss: 2.445652484893799
Validation loss: 2.4655072765965618

Epoch: 5| Step: 1
Training loss: 2.087002992630005
Validation loss: 2.4885103215453444

Epoch: 5| Step: 2
Training loss: 2.411494016647339
Validation loss: 2.497965928046934

Epoch: 5| Step: 3
Training loss: 3.0704169273376465
Validation loss: 2.488198641807802

Epoch: 5| Step: 4
Training loss: 3.0233190059661865
Validation loss: 2.4758755442916707

Epoch: 5| Step: 5
Training loss: 2.171313762664795
Validation loss: 2.452806334341726

Epoch: 5| Step: 6
Training loss: 2.5925440788269043
Validation loss: 2.4299569873399633

Epoch: 5| Step: 7
Training loss: 2.7579963207244873
Validation loss: 2.4220036588689333

Epoch: 5| Step: 8
Training loss: 3.24755859375
Validation loss: 2.4181119652204615

Epoch: 5| Step: 9
Training loss: 3.0594964027404785
Validation loss: 2.41887673254936

Epoch: 5| Step: 10
Training loss: 2.452324867248535
Validation loss: 2.4198468116021927

Epoch: 92| Step: 0
Training loss: 2.2568907737731934
Validation loss: 2.4223208119792323

Epoch: 5| Step: 1
Training loss: 2.8756632804870605
Validation loss: 2.4237122099886657

Epoch: 5| Step: 2
Training loss: 2.2258384227752686
Validation loss: 2.417510276199669

Epoch: 5| Step: 3
Training loss: 2.8571906089782715
Validation loss: 2.4219348135814873

Epoch: 5| Step: 4
Training loss: 2.2721493244171143
Validation loss: 2.4193216190543225

Epoch: 5| Step: 5
Training loss: 2.501110553741455
Validation loss: 2.422574325274396

Epoch: 5| Step: 6
Training loss: 2.8029799461364746
Validation loss: 2.423321129173361

Epoch: 5| Step: 7
Training loss: 2.0263209342956543
Validation loss: 2.432000767800116

Epoch: 5| Step: 8
Training loss: 2.3733251094818115
Validation loss: 2.4282912772188903

Epoch: 5| Step: 9
Training loss: 3.139742851257324
Validation loss: 2.4314911442418254

Epoch: 5| Step: 10
Training loss: 4.0145697593688965
Validation loss: 2.4320189004303305

Epoch: 93| Step: 0
Training loss: 2.644035816192627
Validation loss: 2.419859734914636

Epoch: 5| Step: 1
Training loss: 2.161407947540283
Validation loss: 2.4380534977041264

Epoch: 5| Step: 2
Training loss: 2.7604033946990967
Validation loss: 2.453566360217269

Epoch: 5| Step: 3
Training loss: 2.797994613647461
Validation loss: 2.448230871590235

Epoch: 5| Step: 4
Training loss: 2.530860185623169
Validation loss: 2.4409236343958045

Epoch: 5| Step: 5
Training loss: 2.8296329975128174
Validation loss: 2.434408621121478

Epoch: 5| Step: 6
Training loss: 2.2073214054107666
Validation loss: 2.4271458861648396

Epoch: 5| Step: 7
Training loss: 3.078559160232544
Validation loss: 2.4191835900788665

Epoch: 5| Step: 8
Training loss: 3.044445753097534
Validation loss: 2.413053251081897

Epoch: 5| Step: 9
Training loss: 2.3671929836273193
Validation loss: 2.4106448209413918

Epoch: 5| Step: 10
Training loss: 2.7570979595184326
Validation loss: 2.412764254436698

Epoch: 94| Step: 0
Training loss: 2.9537341594696045
Validation loss: 2.41311296596322

Epoch: 5| Step: 1
Training loss: 2.716517925262451
Validation loss: 2.4094596370573966

Epoch: 5| Step: 2
Training loss: 2.1063246726989746
Validation loss: 2.415022219381025

Epoch: 5| Step: 3
Training loss: 2.9486842155456543
Validation loss: 2.4130080079519622

Epoch: 5| Step: 4
Training loss: 2.1444075107574463
Validation loss: 2.408450521448607

Epoch: 5| Step: 5
Training loss: 2.102881908416748
Validation loss: 2.415056561910978

Epoch: 5| Step: 6
Training loss: 2.9617209434509277
Validation loss: 2.415323911174651

Epoch: 5| Step: 7
Training loss: 2.5514636039733887
Validation loss: 2.413754745196271

Epoch: 5| Step: 8
Training loss: 3.2678627967834473
Validation loss: 2.4114323046899613

Epoch: 5| Step: 9
Training loss: 3.172680616378784
Validation loss: 2.4153052965799966

Epoch: 5| Step: 10
Training loss: 2.1298961639404297
Validation loss: 2.4139939072311565

Epoch: 95| Step: 0
Training loss: 2.4882068634033203
Validation loss: 2.4273784519523702

Epoch: 5| Step: 1
Training loss: 2.6246867179870605
Validation loss: 2.4190283795838714

Epoch: 5| Step: 2
Training loss: 3.1585683822631836
Validation loss: 2.4232990536638486

Epoch: 5| Step: 3
Training loss: 2.632321834564209
Validation loss: 2.424304954467281

Epoch: 5| Step: 4
Training loss: 2.492097854614258
Validation loss: 2.4200219672213317

Epoch: 5| Step: 5
Training loss: 2.351767063140869
Validation loss: 2.418823226805656

Epoch: 5| Step: 6
Training loss: 3.4555251598358154
Validation loss: 2.416742294065414

Epoch: 5| Step: 7
Training loss: 2.5017611980438232
Validation loss: 2.416375239690145

Epoch: 5| Step: 8
Training loss: 2.455448627471924
Validation loss: 2.413219900541408

Epoch: 5| Step: 9
Training loss: 2.2916743755340576
Validation loss: 2.4109889820057857

Epoch: 5| Step: 10
Training loss: 2.525115489959717
Validation loss: 2.415291578538956

Epoch: 96| Step: 0
Training loss: 2.887385606765747
Validation loss: 2.4096746906157462

Epoch: 5| Step: 1
Training loss: 2.330406665802002
Validation loss: 2.415827774232434

Epoch: 5| Step: 2
Training loss: 2.339019536972046
Validation loss: 2.4166157194363174

Epoch: 5| Step: 3
Training loss: 3.1868908405303955
Validation loss: 2.417208549796894

Epoch: 5| Step: 4
Training loss: 2.9388487339019775
Validation loss: 2.4219359633743123

Epoch: 5| Step: 5
Training loss: 2.615225315093994
Validation loss: 2.4237893678808726

Epoch: 5| Step: 6
Training loss: 3.1310486793518066
Validation loss: 2.4267887479515484

Epoch: 5| Step: 7
Training loss: 2.0995116233825684
Validation loss: 2.420593725737705

Epoch: 5| Step: 8
Training loss: 2.317209005355835
Validation loss: 2.4165016323007564

Epoch: 5| Step: 9
Training loss: 2.6540722846984863
Validation loss: 2.4096158319903958

Epoch: 5| Step: 10
Training loss: 2.4682810306549072
Validation loss: 2.413522130699568

Epoch: 97| Step: 0
Training loss: 2.2589011192321777
Validation loss: 2.4098568167737735

Epoch: 5| Step: 1
Training loss: 2.9381110668182373
Validation loss: 2.410236491951891

Epoch: 5| Step: 2
Training loss: 2.447221279144287
Validation loss: 2.4148533523723645

Epoch: 5| Step: 3
Training loss: 2.4347519874572754
Validation loss: 2.4091819999038533

Epoch: 5| Step: 4
Training loss: 2.3665475845336914
Validation loss: 2.4098801433399157

Epoch: 5| Step: 5
Training loss: 2.6330456733703613
Validation loss: 2.4159185912019465

Epoch: 5| Step: 6
Training loss: 2.860682249069214
Validation loss: 2.4247028468757548

Epoch: 5| Step: 7
Training loss: 2.973036527633667
Validation loss: 2.4240623238266155

Epoch: 5| Step: 8
Training loss: 2.492426872253418
Validation loss: 2.419918332048642

Epoch: 5| Step: 9
Training loss: 2.3557541370391846
Validation loss: 2.419353590216688

Epoch: 5| Step: 10
Training loss: 3.2913103103637695
Validation loss: 2.4133129042963826

Epoch: 98| Step: 0
Training loss: 2.4423210620880127
Validation loss: 2.4084394131937334

Epoch: 5| Step: 1
Training loss: 2.4876205921173096
Validation loss: 2.4074785837563137

Epoch: 5| Step: 2
Training loss: 2.3500475883483887
Validation loss: 2.407432330551968

Epoch: 5| Step: 3
Training loss: 2.4298832416534424
Validation loss: 2.402186155319214

Epoch: 5| Step: 4
Training loss: 2.144824981689453
Validation loss: 2.4031856418937765

Epoch: 5| Step: 5
Training loss: 2.6364009380340576
Validation loss: 2.4035191484676894

Epoch: 5| Step: 6
Training loss: 2.6744701862335205
Validation loss: 2.405605734035533

Epoch: 5| Step: 7
Training loss: 3.0459494590759277
Validation loss: 2.400472051353865

Epoch: 5| Step: 8
Training loss: 2.7825303077697754
Validation loss: 2.404004796858757

Epoch: 5| Step: 9
Training loss: 3.2256340980529785
Validation loss: 2.4107059560796267

Epoch: 5| Step: 10
Training loss: 2.7635974884033203
Validation loss: 2.4217310028691448

Epoch: 99| Step: 0
Training loss: 2.610715389251709
Validation loss: 2.4311336548097673

Epoch: 5| Step: 1
Training loss: 2.825561046600342
Validation loss: 2.4404606306424705

Epoch: 5| Step: 2
Training loss: 3.132629156112671
Validation loss: 2.4505196694404847

Epoch: 5| Step: 3
Training loss: 3.2605795860290527
Validation loss: 2.4435852419945503

Epoch: 5| Step: 4
Training loss: 2.5154340267181396
Validation loss: 2.4532114715986353

Epoch: 5| Step: 5
Training loss: 2.321321964263916
Validation loss: 2.4446874613402994

Epoch: 5| Step: 6
Training loss: 2.3373708724975586
Validation loss: 2.437127977289179

Epoch: 5| Step: 7
Training loss: 2.7544379234313965
Validation loss: 2.4279327982215473

Epoch: 5| Step: 8
Training loss: 2.4531898498535156
Validation loss: 2.4230645625822005

Epoch: 5| Step: 9
Training loss: 2.3084778785705566
Validation loss: 2.4280265620959702

Epoch: 5| Step: 10
Training loss: 2.514068126678467
Validation loss: 2.4248946943590717

Epoch: 100| Step: 0
Training loss: 3.274390459060669
Validation loss: 2.4246156702759447

Epoch: 5| Step: 1
Training loss: 2.5003161430358887
Validation loss: 2.421864522400723

Epoch: 5| Step: 2
Training loss: 2.2779409885406494
Validation loss: 2.4260632068880144

Epoch: 5| Step: 3
Training loss: 2.5170931816101074
Validation loss: 2.434167279992052

Epoch: 5| Step: 4
Training loss: 2.9210734367370605
Validation loss: 2.432195617306617

Epoch: 5| Step: 5
Training loss: 2.3780946731567383
Validation loss: 2.4340500600876345

Epoch: 5| Step: 6
Training loss: 2.860485792160034
Validation loss: 2.4305443661187285

Epoch: 5| Step: 7
Training loss: 2.604416847229004
Validation loss: 2.4273178603059504

Epoch: 5| Step: 8
Training loss: 2.209460973739624
Validation loss: 2.425722186283399

Epoch: 5| Step: 9
Training loss: 2.729407548904419
Validation loss: 2.4233773908307477

Epoch: 5| Step: 10
Training loss: 2.6388440132141113
Validation loss: 2.429121537875104

Epoch: 101| Step: 0
Training loss: 2.905426025390625
Validation loss: 2.432374720932335

Epoch: 5| Step: 1
Training loss: 3.0684409141540527
Validation loss: 2.4295564005451817

Epoch: 5| Step: 2
Training loss: 1.8600540161132812
Validation loss: 2.431350815680719

Epoch: 5| Step: 3
Training loss: 2.0312740802764893
Validation loss: 2.4260595767728743

Epoch: 5| Step: 4
Training loss: 2.687044858932495
Validation loss: 2.424346858455289

Epoch: 5| Step: 5
Training loss: 2.6899263858795166
Validation loss: 2.4261835595612884

Epoch: 5| Step: 6
Training loss: 2.691736936569214
Validation loss: 2.42702059079242

Epoch: 5| Step: 7
Training loss: 3.128681182861328
Validation loss: 2.4308369364789737

Epoch: 5| Step: 8
Training loss: 3.0959725379943848
Validation loss: 2.4278677432767806

Epoch: 5| Step: 9
Training loss: 2.2018191814422607
Validation loss: 2.4271459220558085

Epoch: 5| Step: 10
Training loss: 2.4616286754608154
Validation loss: 2.4294250447263

Epoch: 102| Step: 0
Training loss: 2.5619897842407227
Validation loss: 2.4302977772169214

Epoch: 5| Step: 1
Training loss: 2.661369800567627
Validation loss: 2.4323464388488443

Epoch: 5| Step: 2
Training loss: 2.210191488265991
Validation loss: 2.4308109232174453

Epoch: 5| Step: 3
Training loss: 2.7588353157043457
Validation loss: 2.428549684504027

Epoch: 5| Step: 4
Training loss: 2.9186387062072754
Validation loss: 2.4217851341411634

Epoch: 5| Step: 5
Training loss: 2.8052005767822266
Validation loss: 2.4171209668600433

Epoch: 5| Step: 6
Training loss: 2.632387161254883
Validation loss: 2.4181144416973157

Epoch: 5| Step: 7
Training loss: 2.5708632469177246
Validation loss: 2.4186178432997836

Epoch: 5| Step: 8
Training loss: 2.7408695220947266
Validation loss: 2.4198760268508748

Epoch: 5| Step: 9
Training loss: 2.466221809387207
Validation loss: 2.420838232963316

Epoch: 5| Step: 10
Training loss: 2.5436317920684814
Validation loss: 2.4266661649109214

Epoch: 103| Step: 0
Training loss: 2.2153782844543457
Validation loss: 2.424611186468473

Epoch: 5| Step: 1
Training loss: 2.636854410171509
Validation loss: 2.4228448508888163

Epoch: 5| Step: 2
Training loss: 2.4277284145355225
Validation loss: 2.414930579482868

Epoch: 5| Step: 3
Training loss: 2.851545810699463
Validation loss: 2.417534764095019

Epoch: 5| Step: 4
Training loss: 2.3326973915100098
Validation loss: 2.416376401019353

Epoch: 5| Step: 5
Training loss: 2.7132177352905273
Validation loss: 2.406424117344682

Epoch: 5| Step: 6
Training loss: 2.4592859745025635
Validation loss: 2.4069429072000648

Epoch: 5| Step: 7
Training loss: 3.0985941886901855
Validation loss: 2.4087901423054356

Epoch: 5| Step: 8
Training loss: 3.139474391937256
Validation loss: 2.401138041609077

Epoch: 5| Step: 9
Training loss: 2.360342502593994
Validation loss: 2.40060172542449

Epoch: 5| Step: 10
Training loss: 2.5960264205932617
Validation loss: 2.394891082599599

Epoch: 104| Step: 0
Training loss: 2.990206718444824
Validation loss: 2.389507237301078

Epoch: 5| Step: 1
Training loss: 2.272528886795044
Validation loss: 2.3881093814808834

Epoch: 5| Step: 2
Training loss: 2.980323314666748
Validation loss: 2.3817370681352514

Epoch: 5| Step: 3
Training loss: 2.4417917728424072
Validation loss: 2.3852836034631215

Epoch: 5| Step: 4
Training loss: 2.225666046142578
Validation loss: 2.386440089953843

Epoch: 5| Step: 5
Training loss: 2.04113507270813
Validation loss: 2.3835356363686184

Epoch: 5| Step: 6
Training loss: 3.170380115509033
Validation loss: 2.3860198682354343

Epoch: 5| Step: 7
Training loss: 2.4770052433013916
Validation loss: 2.38458683670208

Epoch: 5| Step: 8
Training loss: 2.801246166229248
Validation loss: 2.38347710845291

Epoch: 5| Step: 9
Training loss: 2.729930877685547
Validation loss: 2.389319424988121

Epoch: 5| Step: 10
Training loss: 2.7705085277557373
Validation loss: 2.389600646111273

Epoch: 105| Step: 0
Training loss: 2.7860755920410156
Validation loss: 2.387440022601876

Epoch: 5| Step: 1
Training loss: 3.054948091506958
Validation loss: 2.3970187364086026

Epoch: 5| Step: 2
Training loss: 2.8409156799316406
Validation loss: 2.3984624442233833

Epoch: 5| Step: 3
Training loss: 1.7221260070800781
Validation loss: 2.4059022985478884

Epoch: 5| Step: 4
Training loss: 2.7074029445648193
Validation loss: 2.4158187937992874

Epoch: 5| Step: 5
Training loss: 2.781534194946289
Validation loss: 2.4074495761625228

Epoch: 5| Step: 6
Training loss: 2.4983932971954346
Validation loss: 2.3976680642815045

Epoch: 5| Step: 7
Training loss: 2.3822927474975586
Validation loss: 2.3840329313790924

Epoch: 5| Step: 8
Training loss: 2.6934025287628174
Validation loss: 2.3832886116479033

Epoch: 5| Step: 9
Training loss: 2.186889410018921
Validation loss: 2.386399269104004

Epoch: 5| Step: 10
Training loss: 3.2010135650634766
Validation loss: 2.3808488845825195

Epoch: 106| Step: 0
Training loss: 1.8955459594726562
Validation loss: 2.3812000008039576

Epoch: 5| Step: 1
Training loss: 2.432605504989624
Validation loss: 2.3850720338923956

Epoch: 5| Step: 2
Training loss: 2.9929585456848145
Validation loss: 2.383136926158782

Epoch: 5| Step: 3
Training loss: 2.6099634170532227
Validation loss: 2.3829801774794057

Epoch: 5| Step: 4
Training loss: 2.907641887664795
Validation loss: 2.3801528715318248

Epoch: 5| Step: 5
Training loss: 2.7003448009490967
Validation loss: 2.37810666074035

Epoch: 5| Step: 6
Training loss: 2.1910929679870605
Validation loss: 2.380138243398359

Epoch: 5| Step: 7
Training loss: 2.6832892894744873
Validation loss: 2.374021235332694

Epoch: 5| Step: 8
Training loss: 2.748037815093994
Validation loss: 2.3753020199396278

Epoch: 5| Step: 9
Training loss: 2.8752760887145996
Validation loss: 2.3798819382985434

Epoch: 5| Step: 10
Training loss: 2.720674514770508
Validation loss: 2.3837193673656834

Epoch: 107| Step: 0
Training loss: 2.1057615280151367
Validation loss: 2.4023667689292663

Epoch: 5| Step: 1
Training loss: 1.8801214694976807
Validation loss: 2.418944958717592

Epoch: 5| Step: 2
Training loss: 2.4559993743896484
Validation loss: 2.414856133922454

Epoch: 5| Step: 3
Training loss: 2.25486159324646
Validation loss: 2.413417805907547

Epoch: 5| Step: 4
Training loss: 2.547499895095825
Validation loss: 2.427161180844871

Epoch: 5| Step: 5
Training loss: 3.2501730918884277
Validation loss: 2.4431051541400213

Epoch: 5| Step: 6
Training loss: 2.6485629081726074
Validation loss: 2.447522214663926

Epoch: 5| Step: 7
Training loss: 3.5851712226867676
Validation loss: 2.4497368053723405

Epoch: 5| Step: 8
Training loss: 2.580577850341797
Validation loss: 2.4428746469559206

Epoch: 5| Step: 9
Training loss: 2.4757275581359863
Validation loss: 2.4253400525739117

Epoch: 5| Step: 10
Training loss: 3.1782867908477783
Validation loss: 2.4094945307700866

Epoch: 108| Step: 0
Training loss: 2.5554935932159424
Validation loss: 2.3957017596049974

Epoch: 5| Step: 1
Training loss: 2.561685085296631
Validation loss: 2.3971584330322924

Epoch: 5| Step: 2
Training loss: 1.948091745376587
Validation loss: 2.396737939567976

Epoch: 5| Step: 3
Training loss: 2.330892562866211
Validation loss: 2.401941822421166

Epoch: 5| Step: 4
Training loss: 3.0978803634643555
Validation loss: 2.4090453194033716

Epoch: 5| Step: 5
Training loss: 3.051194429397583
Validation loss: 2.4258709710131408

Epoch: 5| Step: 6
Training loss: 1.9959427118301392
Validation loss: 2.4358510355795584

Epoch: 5| Step: 7
Training loss: 2.8282253742218018
Validation loss: 2.4518697723265617

Epoch: 5| Step: 8
Training loss: 2.892390012741089
Validation loss: 2.453293210716658

Epoch: 5| Step: 9
Training loss: 2.524672031402588
Validation loss: 2.453900565383255

Epoch: 5| Step: 10
Training loss: 3.126281976699829
Validation loss: 2.433096993354059

Epoch: 109| Step: 0
Training loss: 3.0950779914855957
Validation loss: 2.4203200109543337

Epoch: 5| Step: 1
Training loss: 2.8563358783721924
Validation loss: 2.4099808969805316

Epoch: 5| Step: 2
Training loss: 2.345510482788086
Validation loss: 2.3971669007373113

Epoch: 5| Step: 3
Training loss: 2.12069034576416
Validation loss: 2.395889174553656

Epoch: 5| Step: 4
Training loss: 2.9181175231933594
Validation loss: 2.39853403645177

Epoch: 5| Step: 5
Training loss: 2.7141666412353516
Validation loss: 2.4004121454813148

Epoch: 5| Step: 6
Training loss: 2.877332925796509
Validation loss: 2.407311813805693

Epoch: 5| Step: 7
Training loss: 2.7884585857391357
Validation loss: 2.410496211821033

Epoch: 5| Step: 8
Training loss: 2.00459623336792
Validation loss: 2.414478922402987

Epoch: 5| Step: 9
Training loss: 2.373729705810547
Validation loss: 2.409230793676069

Epoch: 5| Step: 10
Training loss: 2.8105881214141846
Validation loss: 2.406306351384809

Epoch: 110| Step: 0
Training loss: 2.0601696968078613
Validation loss: 2.412375229661183

Epoch: 5| Step: 1
Training loss: 2.017047166824341
Validation loss: 2.4125135047461397

Epoch: 5| Step: 2
Training loss: 2.8738789558410645
Validation loss: 2.413628380785706

Epoch: 5| Step: 3
Training loss: 2.478635311126709
Validation loss: 2.427741007138324

Epoch: 5| Step: 4
Training loss: 3.2480201721191406
Validation loss: 2.42542137253669

Epoch: 5| Step: 5
Training loss: 2.787928342819214
Validation loss: 2.4337197760100007

Epoch: 5| Step: 6
Training loss: 2.8535563945770264
Validation loss: 2.4479223041124243

Epoch: 5| Step: 7
Training loss: 2.4665491580963135
Validation loss: 2.4317118314004715

Epoch: 5| Step: 8
Training loss: 2.8823702335357666
Validation loss: 2.4179926738944104

Epoch: 5| Step: 9
Training loss: 2.4411749839782715
Validation loss: 2.4107233478176977

Epoch: 5| Step: 10
Training loss: 2.6112632751464844
Validation loss: 2.397659819613221

Epoch: 111| Step: 0
Training loss: 3.197397232055664
Validation loss: 2.3891911352834394

Epoch: 5| Step: 1
Training loss: 2.327064037322998
Validation loss: 2.378283541689637

Epoch: 5| Step: 2
Training loss: 2.1975791454315186
Validation loss: 2.381877153150497

Epoch: 5| Step: 3
Training loss: 2.686363697052002
Validation loss: 2.3828931290616273

Epoch: 5| Step: 4
Training loss: 2.253484010696411
Validation loss: 2.382524444210914

Epoch: 5| Step: 5
Training loss: 2.6282689571380615
Validation loss: 2.3799267763732583

Epoch: 5| Step: 6
Training loss: 2.2258238792419434
Validation loss: 2.393127841334189

Epoch: 5| Step: 7
Training loss: 2.976522445678711
Validation loss: 2.3844243993041334

Epoch: 5| Step: 8
Training loss: 2.7080349922180176
Validation loss: 2.3929861104616554

Epoch: 5| Step: 9
Training loss: 2.8695266246795654
Validation loss: 2.403788561462074

Epoch: 5| Step: 10
Training loss: 2.571413278579712
Validation loss: 2.4071532705778718

Epoch: 112| Step: 0
Training loss: 2.5525715351104736
Validation loss: 2.4079904812638477

Epoch: 5| Step: 1
Training loss: 2.837555408477783
Validation loss: 2.4226565104658886

Epoch: 5| Step: 2
Training loss: 2.9993693828582764
Validation loss: 2.4286632537841797

Epoch: 5| Step: 3
Training loss: 2.300443172454834
Validation loss: 2.4279791924261276

Epoch: 5| Step: 4
Training loss: 3.411668300628662
Validation loss: 2.418082623071568

Epoch: 5| Step: 5
Training loss: 2.195432186126709
Validation loss: 2.4032703163803264

Epoch: 5| Step: 6
Training loss: 3.0487725734710693
Validation loss: 2.4009211601749545

Epoch: 5| Step: 7
Training loss: 2.6136507987976074
Validation loss: 2.392179981354744

Epoch: 5| Step: 8
Training loss: 2.5646398067474365
Validation loss: 2.3842189619618077

Epoch: 5| Step: 9
Training loss: 2.4107937812805176
Validation loss: 2.386796341147474

Epoch: 5| Step: 10
Training loss: 1.6290476322174072
Validation loss: 2.388798067646642

Epoch: 113| Step: 0
Training loss: 2.277341842651367
Validation loss: 2.3850348175212903

Epoch: 5| Step: 1
Training loss: 2.7272424697875977
Validation loss: 2.390546916633524

Epoch: 5| Step: 2
Training loss: 3.502394199371338
Validation loss: 2.4135714705272386

Epoch: 5| Step: 3
Training loss: 2.7487475872039795
Validation loss: 2.436655629065729

Epoch: 5| Step: 4
Training loss: 2.476985454559326
Validation loss: 2.4561479014735066

Epoch: 5| Step: 5
Training loss: 2.320993185043335
Validation loss: 2.4733106679813837

Epoch: 5| Step: 6
Training loss: 2.7819182872772217
Validation loss: 2.4522547978226856

Epoch: 5| Step: 7
Training loss: 2.7216601371765137
Validation loss: 2.43196915811108

Epoch: 5| Step: 8
Training loss: 2.0429282188415527
Validation loss: 2.4087078699501614

Epoch: 5| Step: 9
Training loss: 3.0579609870910645
Validation loss: 2.3992353203476116

Epoch: 5| Step: 10
Training loss: 2.2097208499908447
Validation loss: 2.392360618037562

Epoch: 114| Step: 0
Training loss: 2.853128433227539
Validation loss: 2.3920581776608705

Epoch: 5| Step: 1
Training loss: 3.0260510444641113
Validation loss: 2.3855908083659347

Epoch: 5| Step: 2
Training loss: 2.3598079681396484
Validation loss: 2.382446212153281

Epoch: 5| Step: 3
Training loss: 2.3138842582702637
Validation loss: 2.37808241639086

Epoch: 5| Step: 4
Training loss: 2.3108010292053223
Validation loss: 2.3807773038905156

Epoch: 5| Step: 5
Training loss: 3.7991931438446045
Validation loss: 2.3786693875507643

Epoch: 5| Step: 6
Training loss: 2.8714559078216553
Validation loss: 2.376385388835784

Epoch: 5| Step: 7
Training loss: 2.846938133239746
Validation loss: 2.3688857760480655

Epoch: 5| Step: 8
Training loss: 2.2538716793060303
Validation loss: 2.364781787318568

Epoch: 5| Step: 9
Training loss: 1.4872397184371948
Validation loss: 2.3591731876455326

Epoch: 5| Step: 10
Training loss: 2.6030995845794678
Validation loss: 2.3655670406997844

Epoch: 115| Step: 0
Training loss: 2.566232681274414
Validation loss: 2.3715447918061288

Epoch: 5| Step: 1
Training loss: 2.335737705230713
Validation loss: 2.3749460507464666

Epoch: 5| Step: 2
Training loss: 3.3410696983337402
Validation loss: 2.3806176852154475

Epoch: 5| Step: 3
Training loss: 2.9003303050994873
Validation loss: 2.3848908921723724

Epoch: 5| Step: 4
Training loss: 2.499329090118408
Validation loss: 2.379942055671446

Epoch: 5| Step: 5
Training loss: 2.1769888401031494
Validation loss: 2.371029853820801

Epoch: 5| Step: 6
Training loss: 2.7811357975006104
Validation loss: 2.3721530488742295

Epoch: 5| Step: 7
Training loss: 2.6686999797821045
Validation loss: 2.374084602120102

Epoch: 5| Step: 8
Training loss: 3.46185040473938
Validation loss: 2.3737531272313928

Epoch: 5| Step: 9
Training loss: 2.132732391357422
Validation loss: 2.377724334757815

Epoch: 5| Step: 10
Training loss: 1.5580418109893799
Validation loss: 2.382715945602745

Epoch: 116| Step: 0
Training loss: 2.6977100372314453
Validation loss: 2.389844271444505

Epoch: 5| Step: 1
Training loss: 2.6564736366271973
Validation loss: 2.394021431605021

Epoch: 5| Step: 2
Training loss: 2.8521313667297363
Validation loss: 2.3962901664036576

Epoch: 5| Step: 3
Training loss: 2.981184244155884
Validation loss: 2.401055666708177

Epoch: 5| Step: 4
Training loss: 1.9509048461914062
Validation loss: 2.409634790112895

Epoch: 5| Step: 5
Training loss: 3.0709197521209717
Validation loss: 2.3910790720293598

Epoch: 5| Step: 6
Training loss: 2.5740699768066406
Validation loss: 2.383828429765599

Epoch: 5| Step: 7
Training loss: 2.074632167816162
Validation loss: 2.37722473759805

Epoch: 5| Step: 8
Training loss: 2.4964537620544434
Validation loss: 2.3732269246091127

Epoch: 5| Step: 9
Training loss: 2.2576167583465576
Validation loss: 2.368598162486989

Epoch: 5| Step: 10
Training loss: 3.0926554203033447
Validation loss: 2.371923421018867

Epoch: 117| Step: 0
Training loss: 2.6631298065185547
Validation loss: 2.3760349263427076

Epoch: 5| Step: 1
Training loss: 2.878485679626465
Validation loss: 2.3749930320247525

Epoch: 5| Step: 2
Training loss: 2.452054738998413
Validation loss: 2.3736941147876043

Epoch: 5| Step: 3
Training loss: 3.121396541595459
Validation loss: 2.373494258490942

Epoch: 5| Step: 4
Training loss: 2.3661208152770996
Validation loss: 2.3671117790283693

Epoch: 5| Step: 5
Training loss: 2.3682644367218018
Validation loss: 2.3695075153022684

Epoch: 5| Step: 6
Training loss: 2.4249162673950195
Validation loss: 2.3662808172164427

Epoch: 5| Step: 7
Training loss: 2.8803551197052
Validation loss: 2.3793158620916386

Epoch: 5| Step: 8
Training loss: 2.5873124599456787
Validation loss: 2.389487297304215

Epoch: 5| Step: 9
Training loss: 2.4675774574279785
Validation loss: 2.398276275204074

Epoch: 5| Step: 10
Training loss: 2.209639549255371
Validation loss: 2.39690004369264

Epoch: 118| Step: 0
Training loss: 2.9113636016845703
Validation loss: 2.395042875761627

Epoch: 5| Step: 1
Training loss: 3.045139789581299
Validation loss: 2.39612324776188

Epoch: 5| Step: 2
Training loss: 3.1610350608825684
Validation loss: 2.393346430152975

Epoch: 5| Step: 3
Training loss: 2.4001379013061523
Validation loss: 2.3781602510841946

Epoch: 5| Step: 4
Training loss: 1.9936048984527588
Validation loss: 2.3718423266564646

Epoch: 5| Step: 5
Training loss: 2.569558620452881
Validation loss: 2.364791544534827

Epoch: 5| Step: 6
Training loss: 2.8974645137786865
Validation loss: 2.3609020556173017

Epoch: 5| Step: 7
Training loss: 2.208332061767578
Validation loss: 2.3593866953285794

Epoch: 5| Step: 8
Training loss: 2.392069101333618
Validation loss: 2.3566811674384662

Epoch: 5| Step: 9
Training loss: 2.2399039268493652
Validation loss: 2.356060822804769

Epoch: 5| Step: 10
Training loss: 2.7206637859344482
Validation loss: 2.359191466403264

Epoch: 119| Step: 0
Training loss: 2.789764881134033
Validation loss: 2.3575532103097565

Epoch: 5| Step: 1
Training loss: 3.0071520805358887
Validation loss: 2.3566954546077277

Epoch: 5| Step: 2
Training loss: 2.529261827468872
Validation loss: 2.35345555633627

Epoch: 5| Step: 3
Training loss: 2.692469835281372
Validation loss: 2.3506998426170758

Epoch: 5| Step: 4
Training loss: 2.7085630893707275
Validation loss: 2.3553183770948842

Epoch: 5| Step: 5
Training loss: 2.474442958831787
Validation loss: 2.363051332453246

Epoch: 5| Step: 6
Training loss: 2.4267823696136475
Validation loss: 2.3599045353551067

Epoch: 5| Step: 7
Training loss: 2.272306203842163
Validation loss: 2.3667218121149207

Epoch: 5| Step: 8
Training loss: 2.650893211364746
Validation loss: 2.3667779045720256

Epoch: 5| Step: 9
Training loss: 2.6481919288635254
Validation loss: 2.3851088913538123

Epoch: 5| Step: 10
Training loss: 2.3117024898529053
Validation loss: 2.3829173067564606

Epoch: 120| Step: 0
Training loss: 2.8079030513763428
Validation loss: 2.3741468203965055

Epoch: 5| Step: 1
Training loss: 2.575172185897827
Validation loss: 2.362548351287842

Epoch: 5| Step: 2
Training loss: 2.897416353225708
Validation loss: 2.354165002863894

Epoch: 5| Step: 3
Training loss: 2.8160529136657715
Validation loss: 2.347434878349304

Epoch: 5| Step: 4
Training loss: 2.5212531089782715
Validation loss: 2.348985625851539

Epoch: 5| Step: 5
Training loss: 3.0746452808380127
Validation loss: 2.346881515236311

Epoch: 5| Step: 6
Training loss: 2.33343243598938
Validation loss: 2.346709951277702

Epoch: 5| Step: 7
Training loss: 2.3490443229675293
Validation loss: 2.349066144676619

Epoch: 5| Step: 8
Training loss: 2.0544919967651367
Validation loss: 2.3615465830731135

Epoch: 5| Step: 9
Training loss: 2.287909984588623
Validation loss: 2.3773397912261305

Epoch: 5| Step: 10
Training loss: 2.835197925567627
Validation loss: 2.3879800611926663

Epoch: 121| Step: 0
Training loss: 2.487804889678955
Validation loss: 2.393796049138551

Epoch: 5| Step: 1
Training loss: 3.236114025115967
Validation loss: 2.393976157711398

Epoch: 5| Step: 2
Training loss: 2.583696126937866
Validation loss: 2.3898118721541537

Epoch: 5| Step: 3
Training loss: 2.8195788860321045
Validation loss: 2.3829082750505015

Epoch: 5| Step: 4
Training loss: 2.1368584632873535
Validation loss: 2.3718641881019837

Epoch: 5| Step: 5
Training loss: 2.726339817047119
Validation loss: 2.3641214268181914

Epoch: 5| Step: 6
Training loss: 2.6122419834136963
Validation loss: 2.35471567415422

Epoch: 5| Step: 7
Training loss: 2.5805296897888184
Validation loss: 2.35277436881937

Epoch: 5| Step: 8
Training loss: 3.156627893447876
Validation loss: 2.346359860512518

Epoch: 5| Step: 9
Training loss: 2.249453067779541
Validation loss: 2.3285243165108467

Epoch: 5| Step: 10
Training loss: 1.8197613954544067
Validation loss: 2.3283109100916053

Epoch: 122| Step: 0
Training loss: 2.0576729774475098
Validation loss: 2.3242401615265877

Epoch: 5| Step: 1
Training loss: 2.6474897861480713
Validation loss: 2.326740652002314

Epoch: 5| Step: 2
Training loss: 2.0514373779296875
Validation loss: 2.32702136552462

Epoch: 5| Step: 3
Training loss: 2.6054484844207764
Validation loss: 2.330433207173501

Epoch: 5| Step: 4
Training loss: 2.5493133068084717
Validation loss: 2.3214741624811643

Epoch: 5| Step: 5
Training loss: 3.3177692890167236
Validation loss: 2.33578521461897

Epoch: 5| Step: 6
Training loss: 2.2353062629699707
Validation loss: 2.337694307809235

Epoch: 5| Step: 7
Training loss: 3.2180912494659424
Validation loss: 2.3484820217214604

Epoch: 5| Step: 8
Training loss: 2.7902960777282715
Validation loss: 2.3531595686430573

Epoch: 5| Step: 9
Training loss: 2.6202590465545654
Validation loss: 2.3558201174582205

Epoch: 5| Step: 10
Training loss: 2.4290196895599365
Validation loss: 2.3615612394066265

Epoch: 123| Step: 0
Training loss: 3.119936943054199
Validation loss: 2.3473254480669574

Epoch: 5| Step: 1
Training loss: 2.8944287300109863
Validation loss: 2.3314811927016064

Epoch: 5| Step: 2
Training loss: 2.844634532928467
Validation loss: 2.3289366973343717

Epoch: 5| Step: 3
Training loss: 2.3184268474578857
Validation loss: 2.3241823334847727

Epoch: 5| Step: 4
Training loss: 2.0826449394226074
Validation loss: 2.3282644594869306

Epoch: 5| Step: 5
Training loss: 2.310737133026123
Validation loss: 2.322765819488033

Epoch: 5| Step: 6
Training loss: 2.5822200775146484
Validation loss: 2.3258716316633326

Epoch: 5| Step: 7
Training loss: 2.459399938583374
Validation loss: 2.330870920611966

Epoch: 5| Step: 8
Training loss: 2.6828701496124268
Validation loss: 2.3260518722636725

Epoch: 5| Step: 9
Training loss: 2.830436944961548
Validation loss: 2.3263346431075886

Epoch: 5| Step: 10
Training loss: 2.3088512420654297
Validation loss: 2.3283286838121313

Epoch: 124| Step: 0
Training loss: 1.7545782327651978
Validation loss: 2.330364683622955

Epoch: 5| Step: 1
Training loss: 2.5269970893859863
Validation loss: 2.3259778227857364

Epoch: 5| Step: 2
Training loss: 2.610820770263672
Validation loss: 2.3318270406415387

Epoch: 5| Step: 3
Training loss: 2.5265238285064697
Validation loss: 2.336987744095505

Epoch: 5| Step: 4
Training loss: 3.189663887023926
Validation loss: 2.3442428663212764

Epoch: 5| Step: 5
Training loss: 2.8287484645843506
Validation loss: 2.3508058837665025

Epoch: 5| Step: 6
Training loss: 2.839094400405884
Validation loss: 2.3552927458158104

Epoch: 5| Step: 7
Training loss: 2.0579729080200195
Validation loss: 2.365685034823674

Epoch: 5| Step: 8
Training loss: 2.2557384967803955
Validation loss: 2.362323127767091

Epoch: 5| Step: 9
Training loss: 3.052269697189331
Validation loss: 2.3484756408199186

Epoch: 5| Step: 10
Training loss: 2.8378829956054688
Validation loss: 2.340566994041525

Epoch: 125| Step: 0
Training loss: 2.690767765045166
Validation loss: 2.3280550549107213

Epoch: 5| Step: 1
Training loss: 2.5032172203063965
Validation loss: 2.3279630625119774

Epoch: 5| Step: 2
Training loss: 2.454982042312622
Validation loss: 2.332135664519443

Epoch: 5| Step: 3
Training loss: 2.52323317527771
Validation loss: 2.331281151822818

Epoch: 5| Step: 4
Training loss: 2.0649421215057373
Validation loss: 2.331502555519022

Epoch: 5| Step: 5
Training loss: 2.6146721839904785
Validation loss: 2.3298119806474253

Epoch: 5| Step: 6
Training loss: 2.391009569168091
Validation loss: 2.338196152000017

Epoch: 5| Step: 7
Training loss: 2.418996810913086
Validation loss: 2.343273042350687

Epoch: 5| Step: 8
Training loss: 3.6090641021728516
Validation loss: 2.3594517246369393

Epoch: 5| Step: 9
Training loss: 2.559117078781128
Validation loss: 2.3706075247897895

Epoch: 5| Step: 10
Training loss: 2.483104944229126
Validation loss: 2.361748792791879

Epoch: 126| Step: 0
Training loss: 3.3758749961853027
Validation loss: 2.3660628282895653

Epoch: 5| Step: 1
Training loss: 2.725867748260498
Validation loss: 2.365589198245797

Epoch: 5| Step: 2
Training loss: 2.5988967418670654
Validation loss: 2.3655212617689565

Epoch: 5| Step: 3
Training loss: 2.8783528804779053
Validation loss: 2.353215966173398

Epoch: 5| Step: 4
Training loss: 2.2585082054138184
Validation loss: 2.3483874746548232

Epoch: 5| Step: 5
Training loss: 2.4682111740112305
Validation loss: 2.3435011627853557

Epoch: 5| Step: 6
Training loss: 2.387935161590576
Validation loss: 2.334704351681535

Epoch: 5| Step: 7
Training loss: 1.5173722505569458
Validation loss: 2.319358100173294

Epoch: 5| Step: 8
Training loss: 2.7456717491149902
Validation loss: 2.3274750606988066

Epoch: 5| Step: 9
Training loss: 2.5919017791748047
Validation loss: 2.3242239388086463

Epoch: 5| Step: 10
Training loss: 2.9750711917877197
Validation loss: 2.331755904741185

Epoch: 127| Step: 0
Training loss: 2.3962607383728027
Validation loss: 2.3577235180844545

Epoch: 5| Step: 1
Training loss: 2.480227470397949
Validation loss: 2.3672665011498237

Epoch: 5| Step: 2
Training loss: 3.1397407054901123
Validation loss: 2.3649119689900386

Epoch: 5| Step: 3
Training loss: 2.392625331878662
Validation loss: 2.364983603518496

Epoch: 5| Step: 4
Training loss: 2.478177070617676
Validation loss: 2.3439726419346307

Epoch: 5| Step: 5
Training loss: 2.7812139987945557
Validation loss: 2.340050953690724

Epoch: 5| Step: 6
Training loss: 2.412334442138672
Validation loss: 2.334497697891728

Epoch: 5| Step: 7
Training loss: 2.618803024291992
Validation loss: 2.3246233899106263

Epoch: 5| Step: 8
Training loss: 3.256330966949463
Validation loss: 2.319454082878687

Epoch: 5| Step: 9
Training loss: 2.273545026779175
Validation loss: 2.3179894878018286

Epoch: 5| Step: 10
Training loss: 2.010941982269287
Validation loss: 2.318402818454209

Epoch: 128| Step: 0
Training loss: 3.43522310256958
Validation loss: 2.3173219824350006

Epoch: 5| Step: 1
Training loss: 2.0088584423065186
Validation loss: 2.3206022811192337

Epoch: 5| Step: 2
Training loss: 2.6358964443206787
Validation loss: 2.319754336469917

Epoch: 5| Step: 3
Training loss: 2.165384292602539
Validation loss: 2.3220591442559355

Epoch: 5| Step: 4
Training loss: 2.9043517112731934
Validation loss: 2.3091879942083873

Epoch: 5| Step: 5
Training loss: 2.184156656265259
Validation loss: 2.3119570029679166

Epoch: 5| Step: 6
Training loss: 2.702122926712036
Validation loss: 2.310319400602771

Epoch: 5| Step: 7
Training loss: 2.6163699626922607
Validation loss: 2.3152937017461306

Epoch: 5| Step: 8
Training loss: 1.95809805393219
Validation loss: 2.3213170113102084

Epoch: 5| Step: 9
Training loss: 2.836901903152466
Validation loss: 2.325654216991958

Epoch: 5| Step: 10
Training loss: 2.967146873474121
Validation loss: 2.3317900806344967

Epoch: 129| Step: 0
Training loss: 2.3925087451934814
Validation loss: 2.333963574901704

Epoch: 5| Step: 1
Training loss: 2.9618284702301025
Validation loss: 2.3320597807566323

Epoch: 5| Step: 2
Training loss: 2.976940155029297
Validation loss: 2.3338165437021563

Epoch: 5| Step: 3
Training loss: 2.1692538261413574
Validation loss: 2.3375880897686048

Epoch: 5| Step: 4
Training loss: 3.0783257484436035
Validation loss: 2.334112071221875

Epoch: 5| Step: 5
Training loss: 2.669308662414551
Validation loss: 2.3372766381950787

Epoch: 5| Step: 6
Training loss: 2.017817735671997
Validation loss: 2.336199273345291

Epoch: 5| Step: 7
Training loss: 2.2574737071990967
Validation loss: 2.3349528671592794

Epoch: 5| Step: 8
Training loss: 2.7412021160125732
Validation loss: 2.335105162794872

Epoch: 5| Step: 9
Training loss: 2.6860122680664062
Validation loss: 2.3185887054730485

Epoch: 5| Step: 10
Training loss: 2.4121243953704834
Validation loss: 2.319222616892989

Epoch: 130| Step: 0
Training loss: 2.969147205352783
Validation loss: 2.320341561430244

Epoch: 5| Step: 1
Training loss: 2.752105951309204
Validation loss: 2.312731090412345

Epoch: 5| Step: 2
Training loss: 2.804443359375
Validation loss: 2.3156515218878306

Epoch: 5| Step: 3
Training loss: 2.89078950881958
Validation loss: 2.330058608003842

Epoch: 5| Step: 4
Training loss: 2.791755199432373
Validation loss: 2.3341042303269908

Epoch: 5| Step: 5
Training loss: 1.9561188220977783
Validation loss: 2.3422775422373125

Epoch: 5| Step: 6
Training loss: 2.7048587799072266
Validation loss: 2.3481297031525643

Epoch: 5| Step: 7
Training loss: 2.476074457168579
Validation loss: 2.3466299785080778

Epoch: 5| Step: 8
Training loss: 1.7779951095581055
Validation loss: 2.345348827300533

Epoch: 5| Step: 9
Training loss: 2.717703342437744
Validation loss: 2.3312201935757875

Epoch: 5| Step: 10
Training loss: 2.4831387996673584
Validation loss: 2.319139593391008

Epoch: 131| Step: 0
Training loss: 2.2250308990478516
Validation loss: 2.3168368980448735

Epoch: 5| Step: 1
Training loss: 2.8562192916870117
Validation loss: 2.3152693599782963

Epoch: 5| Step: 2
Training loss: 3.0375354290008545
Validation loss: 2.307510828459135

Epoch: 5| Step: 3
Training loss: 2.5081253051757812
Validation loss: 2.306852432989305

Epoch: 5| Step: 4
Training loss: 2.2031090259552
Validation loss: 2.304339493474653

Epoch: 5| Step: 5
Training loss: 2.6165356636047363
Validation loss: 2.3105341337060414

Epoch: 5| Step: 6
Training loss: 2.4639809131622314
Validation loss: 2.3076011801278717

Epoch: 5| Step: 7
Training loss: 2.3798344135284424
Validation loss: 2.3035517072164886

Epoch: 5| Step: 8
Training loss: 2.5010898113250732
Validation loss: 2.3052880007733583

Epoch: 5| Step: 9
Training loss: 2.6082282066345215
Validation loss: 2.3031925206543296

Epoch: 5| Step: 10
Training loss: 2.840549945831299
Validation loss: 2.3038221995035806

Epoch: 132| Step: 0
Training loss: 2.8875813484191895
Validation loss: 2.3079589541240404

Epoch: 5| Step: 1
Training loss: 2.530402660369873
Validation loss: 2.313814483663087

Epoch: 5| Step: 2
Training loss: 2.1372456550598145
Validation loss: 2.320154715609807

Epoch: 5| Step: 3
Training loss: 2.737982988357544
Validation loss: 2.3285798770125195

Epoch: 5| Step: 4
Training loss: 2.9450321197509766
Validation loss: 2.323659366176974

Epoch: 5| Step: 5
Training loss: 1.8603473901748657
Validation loss: 2.3177947023863434

Epoch: 5| Step: 6
Training loss: 3.1944310665130615
Validation loss: 2.3126124053873043

Epoch: 5| Step: 7
Training loss: 2.1060471534729004
Validation loss: 2.312087866567796

Epoch: 5| Step: 8
Training loss: 2.804189920425415
Validation loss: 2.308848060587401

Epoch: 5| Step: 9
Training loss: 2.6199069023132324
Validation loss: 2.30311462443362

Epoch: 5| Step: 10
Training loss: 2.426900863647461
Validation loss: 2.303649571634108

Epoch: 133| Step: 0
Training loss: 2.5920538902282715
Validation loss: 2.305505314180928

Epoch: 5| Step: 1
Training loss: 2.3581888675689697
Validation loss: 2.303554314439015

Epoch: 5| Step: 2
Training loss: 2.1762757301330566
Validation loss: 2.298546246303025

Epoch: 5| Step: 3
Training loss: 2.8539843559265137
Validation loss: 2.306372001606931

Epoch: 5| Step: 4
Training loss: 2.5768728256225586
Validation loss: 2.2992387689569944

Epoch: 5| Step: 5
Training loss: 2.707965135574341
Validation loss: 2.2987341214251775

Epoch: 5| Step: 6
Training loss: 2.4148454666137695
Validation loss: 2.2932542216393257

Epoch: 5| Step: 7
Training loss: 2.0110907554626465
Validation loss: 2.29563582071694

Epoch: 5| Step: 8
Training loss: 2.6109158992767334
Validation loss: 2.2966357059376215

Epoch: 5| Step: 9
Training loss: 3.060452938079834
Validation loss: 2.299561649240473

Epoch: 5| Step: 10
Training loss: 2.866579055786133
Validation loss: 2.3012091934040027

Epoch: 134| Step: 0
Training loss: 2.8660664558410645
Validation loss: 2.296658310838925

Epoch: 5| Step: 1
Training loss: 2.3014156818389893
Validation loss: 2.2963326925872476

Epoch: 5| Step: 2
Training loss: 2.987499713897705
Validation loss: 2.2955794616412093

Epoch: 5| Step: 3
Training loss: 2.466352939605713
Validation loss: 2.2956953279433714

Epoch: 5| Step: 4
Training loss: 2.4227097034454346
Validation loss: 2.298371675193951

Epoch: 5| Step: 5
Training loss: 2.7761387825012207
Validation loss: 2.2985905216586207

Epoch: 5| Step: 6
Training loss: 2.3719191551208496
Validation loss: 2.29809094244434

Epoch: 5| Step: 7
Training loss: 2.0845882892608643
Validation loss: 2.291113148453415

Epoch: 5| Step: 8
Training loss: 2.4090633392333984
Validation loss: 2.293924356019625

Epoch: 5| Step: 9
Training loss: 2.390399932861328
Validation loss: 2.296841254798315

Epoch: 5| Step: 10
Training loss: 3.1264500617980957
Validation loss: 2.3003961988674697

Epoch: 135| Step: 0
Training loss: 2.1468708515167236
Validation loss: 2.2924589572414273

Epoch: 5| Step: 1
Training loss: 2.2051193714141846
Validation loss: 2.2920176893152218

Epoch: 5| Step: 2
Training loss: 2.775118350982666
Validation loss: 2.2896556623520388

Epoch: 5| Step: 3
Training loss: 2.2731826305389404
Validation loss: 2.2973632633045153

Epoch: 5| Step: 4
Training loss: 2.893023729324341
Validation loss: 2.298119347582581

Epoch: 5| Step: 5
Training loss: 2.9974236488342285
Validation loss: 2.2978583048748713

Epoch: 5| Step: 6
Training loss: 2.802812337875366
Validation loss: 2.2899974699943297

Epoch: 5| Step: 7
Training loss: 2.7988247871398926
Validation loss: 2.30138300823909

Epoch: 5| Step: 8
Training loss: 2.492213487625122
Validation loss: 2.308130389900618

Epoch: 5| Step: 9
Training loss: 2.526071310043335
Validation loss: 2.3246247947856946

Epoch: 5| Step: 10
Training loss: 2.1927876472473145
Validation loss: 2.3220388940585557

Epoch: 136| Step: 0
Training loss: 2.2936649322509766
Validation loss: 2.326098390804824

Epoch: 5| Step: 1
Training loss: 2.4238269329071045
Validation loss: 2.3196640040284846

Epoch: 5| Step: 2
Training loss: 2.604806900024414
Validation loss: 2.314351394612302

Epoch: 5| Step: 3
Training loss: 2.848079204559326
Validation loss: 2.3176233537735476

Epoch: 5| Step: 4
Training loss: 2.309694766998291
Validation loss: 2.308316946029663

Epoch: 5| Step: 5
Training loss: 3.2040157318115234
Validation loss: 2.3062391537491993

Epoch: 5| Step: 6
Training loss: 2.9293465614318848
Validation loss: 2.2988408816758024

Epoch: 5| Step: 7
Training loss: 2.6806790828704834
Validation loss: 2.292548518027029

Epoch: 5| Step: 8
Training loss: 2.396157741546631
Validation loss: 2.292819315387357

Epoch: 5| Step: 9
Training loss: 1.9130713939666748
Validation loss: 2.295984196406539

Epoch: 5| Step: 10
Training loss: 2.4571115970611572
Validation loss: 2.297408419270669

Epoch: 137| Step: 0
Training loss: 2.9258828163146973
Validation loss: 2.2911483267302155

Epoch: 5| Step: 1
Training loss: 2.260510206222534
Validation loss: 2.2913624983961864

Epoch: 5| Step: 2
Training loss: 2.210983991622925
Validation loss: 2.2954332520884853

Epoch: 5| Step: 3
Training loss: 2.539785146713257
Validation loss: 2.3092232981035785

Epoch: 5| Step: 4
Training loss: 2.958313226699829
Validation loss: 2.316696346447032

Epoch: 5| Step: 5
Training loss: 2.6788907051086426
Validation loss: 2.324725133116527

Epoch: 5| Step: 6
Training loss: 2.4668049812316895
Validation loss: 2.3171477445992092

Epoch: 5| Step: 7
Training loss: 2.4945309162139893
Validation loss: 2.3057630677377023

Epoch: 5| Step: 8
Training loss: 2.581462860107422
Validation loss: 2.300230133918024

Epoch: 5| Step: 9
Training loss: 2.7821404933929443
Validation loss: 2.292775059259066

Epoch: 5| Step: 10
Training loss: 2.1924455165863037
Validation loss: 2.2836199242581605

Epoch: 138| Step: 0
Training loss: 2.540863275527954
Validation loss: 2.280438064247049

Epoch: 5| Step: 1
Training loss: 2.2522056102752686
Validation loss: 2.2809922669523504

Epoch: 5| Step: 2
Training loss: 2.8223366737365723
Validation loss: 2.282335700527314

Epoch: 5| Step: 3
Training loss: 2.961637020111084
Validation loss: 2.28667894230094

Epoch: 5| Step: 4
Training loss: 2.52760910987854
Validation loss: 2.278278294429984

Epoch: 5| Step: 5
Training loss: 2.958052158355713
Validation loss: 2.2795571229791127

Epoch: 5| Step: 6
Training loss: 2.9003052711486816
Validation loss: 2.282228592903383

Epoch: 5| Step: 7
Training loss: 2.2741856575012207
Validation loss: 2.285108984157603

Epoch: 5| Step: 8
Training loss: 1.872218132019043
Validation loss: 2.2793618902083366

Epoch: 5| Step: 9
Training loss: 3.08314847946167
Validation loss: 2.283076964398866

Epoch: 5| Step: 10
Training loss: 1.8425626754760742
Validation loss: 2.2800525926774546

Epoch: 139| Step: 0
Training loss: 3.1744227409362793
Validation loss: 2.2820738105363745

Epoch: 5| Step: 1
Training loss: 2.542592763900757
Validation loss: 2.287686235161238

Epoch: 5| Step: 2
Training loss: 2.763807535171509
Validation loss: 2.291175478248186

Epoch: 5| Step: 3
Training loss: 2.572511672973633
Validation loss: 2.296859433574061

Epoch: 5| Step: 4
Training loss: 2.973855972290039
Validation loss: 2.3091867764790854

Epoch: 5| Step: 5
Training loss: 2.1306509971618652
Validation loss: 2.317284458427019

Epoch: 5| Step: 6
Training loss: 2.7322018146514893
Validation loss: 2.313226366555819

Epoch: 5| Step: 7
Training loss: 2.029082775115967
Validation loss: 2.3115658734434392

Epoch: 5| Step: 8
Training loss: 2.395341157913208
Validation loss: 2.295320631355368

Epoch: 5| Step: 9
Training loss: 2.3204562664031982
Validation loss: 2.2812675096655406

Epoch: 5| Step: 10
Training loss: 2.4422953128814697
Validation loss: 2.2795896889061056

Epoch: 140| Step: 0
Training loss: 2.2512059211730957
Validation loss: 2.276885494109123

Epoch: 5| Step: 1
Training loss: 2.1202280521392822
Validation loss: 2.2755390290291078

Epoch: 5| Step: 2
Training loss: 2.3029754161834717
Validation loss: 2.2792801959540254

Epoch: 5| Step: 3
Training loss: 3.0909361839294434
Validation loss: 2.2788348864483576

Epoch: 5| Step: 4
Training loss: 3.193981885910034
Validation loss: 2.278528041737054

Epoch: 5| Step: 5
Training loss: 2.4211583137512207
Validation loss: 2.2831950033864667

Epoch: 5| Step: 6
Training loss: 2.3558738231658936
Validation loss: 2.27981488679045

Epoch: 5| Step: 7
Training loss: 2.8813765048980713
Validation loss: 2.2798111336205595

Epoch: 5| Step: 8
Training loss: 2.5098862648010254
Validation loss: 2.281835707285071

Epoch: 5| Step: 9
Training loss: 2.5243935585021973
Validation loss: 2.2917269263216244

Epoch: 5| Step: 10
Training loss: 2.428833484649658
Validation loss: 2.3190353993446595

Epoch: 141| Step: 0
Training loss: 2.783848762512207
Validation loss: 2.3512106121227307

Epoch: 5| Step: 1
Training loss: 2.905747652053833
Validation loss: 2.371891237074329

Epoch: 5| Step: 2
Training loss: 2.30312180519104
Validation loss: 2.3883620590291996

Epoch: 5| Step: 3
Training loss: 2.880091905593872
Validation loss: 2.347094799882622

Epoch: 5| Step: 4
Training loss: 2.324329376220703
Validation loss: 2.3045795450928392

Epoch: 5| Step: 5
Training loss: 2.1070406436920166
Validation loss: 2.2997955365847518

Epoch: 5| Step: 6
Training loss: 1.7936782836914062
Validation loss: 2.2863265006772933

Epoch: 5| Step: 7
Training loss: 2.924687623977661
Validation loss: 2.2777627539891068

Epoch: 5| Step: 8
Training loss: 3.2290568351745605
Validation loss: 2.2745877773531022

Epoch: 5| Step: 9
Training loss: 1.9610822200775146
Validation loss: 2.278892191507483

Epoch: 5| Step: 10
Training loss: 3.1128082275390625
Validation loss: 2.2982361034680436

Epoch: 142| Step: 0
Training loss: 2.9609334468841553
Validation loss: 2.3059361878261773

Epoch: 5| Step: 1
Training loss: 2.695862054824829
Validation loss: 2.311281793860979

Epoch: 5| Step: 2
Training loss: 1.9499305486679077
Validation loss: 2.3256456954504854

Epoch: 5| Step: 3
Training loss: 2.889606475830078
Validation loss: 2.310022402835149

Epoch: 5| Step: 4
Training loss: 2.4926083087921143
Validation loss: 2.302801424457181

Epoch: 5| Step: 5
Training loss: 2.4129672050476074
Validation loss: 2.2992967713263726

Epoch: 5| Step: 6
Training loss: 3.189513683319092
Validation loss: 2.3125498499921573

Epoch: 5| Step: 7
Training loss: 2.0710902214050293
Validation loss: 2.309404460332727

Epoch: 5| Step: 8
Training loss: 2.325071334838867
Validation loss: 2.3064082899401264

Epoch: 5| Step: 9
Training loss: 1.8976932764053345
Validation loss: 2.3162668469131633

Epoch: 5| Step: 10
Training loss: 3.2319295406341553
Validation loss: 2.306635067027102

Epoch: 143| Step: 0
Training loss: 2.6940841674804688
Validation loss: 2.3024579196847896

Epoch: 5| Step: 1
Training loss: 2.272214651107788
Validation loss: 2.3174519692697833

Epoch: 5| Step: 2
Training loss: 2.0961337089538574
Validation loss: 2.3303816446693997

Epoch: 5| Step: 3
Training loss: 2.8870110511779785
Validation loss: 2.3252600239169214

Epoch: 5| Step: 4
Training loss: 2.249352216720581
Validation loss: 2.3069702066401

Epoch: 5| Step: 5
Training loss: 3.575389862060547
Validation loss: 2.29761855576628

Epoch: 5| Step: 6
Training loss: 2.7208163738250732
Validation loss: 2.2970020899208645

Epoch: 5| Step: 7
Training loss: 2.1633763313293457
Validation loss: 2.290010654798118

Epoch: 5| Step: 8
Training loss: 1.8023617267608643
Validation loss: 2.302714655476232

Epoch: 5| Step: 9
Training loss: 2.9263577461242676
Validation loss: 2.3057771267429477

Epoch: 5| Step: 10
Training loss: 2.675687313079834
Validation loss: 2.2919038957165134

Epoch: 144| Step: 0
Training loss: 2.5587081909179688
Validation loss: 2.297791049044619

Epoch: 5| Step: 1
Training loss: 2.745640754699707
Validation loss: 2.2871786099608227

Epoch: 5| Step: 2
Training loss: 2.2964272499084473
Validation loss: 2.2924053335702546

Epoch: 5| Step: 3
Training loss: 2.210104465484619
Validation loss: 2.2891492330899803

Epoch: 5| Step: 4
Training loss: 2.2018065452575684
Validation loss: 2.286528020776728

Epoch: 5| Step: 5
Training loss: 2.380370616912842
Validation loss: 2.2872043783946703

Epoch: 5| Step: 6
Training loss: 2.832277774810791
Validation loss: 2.2787693085209018

Epoch: 5| Step: 7
Training loss: 2.6265902519226074
Validation loss: 2.276397051349763

Epoch: 5| Step: 8
Training loss: 2.8060431480407715
Validation loss: 2.273667743129115

Epoch: 5| Step: 9
Training loss: 2.5613577365875244
Validation loss: 2.278774820348268

Epoch: 5| Step: 10
Training loss: 2.70725417137146
Validation loss: 2.2840061367198987

Epoch: 145| Step: 0
Training loss: 2.6867194175720215
Validation loss: 2.2822376015365764

Epoch: 5| Step: 1
Training loss: 2.222069025039673
Validation loss: 2.296578891815678

Epoch: 5| Step: 2
Training loss: 2.938986301422119
Validation loss: 2.2941719870413504

Epoch: 5| Step: 3
Training loss: 2.431914806365967
Validation loss: 2.300224757963611

Epoch: 5| Step: 4
Training loss: 2.370523691177368
Validation loss: 2.3073175184188353

Epoch: 5| Step: 5
Training loss: 3.0211098194122314
Validation loss: 2.3082766020169823

Epoch: 5| Step: 6
Training loss: 3.0888254642486572
Validation loss: 2.308675548081757

Epoch: 5| Step: 7
Training loss: 2.624253273010254
Validation loss: 2.3034822915190007

Epoch: 5| Step: 8
Training loss: 1.9583489894866943
Validation loss: 2.320723746412544

Epoch: 5| Step: 9
Training loss: 2.0680205821990967
Validation loss: 2.2982464041761173

Epoch: 5| Step: 10
Training loss: 2.471534252166748
Validation loss: 2.30289957087527

Epoch: 146| Step: 0
Training loss: 1.7978893518447876
Validation loss: 2.295298114899666

Epoch: 5| Step: 1
Training loss: 2.2263660430908203
Validation loss: 2.302595146240727

Epoch: 5| Step: 2
Training loss: 3.052691698074341
Validation loss: 2.3051560104534192

Epoch: 5| Step: 3
Training loss: 2.5322091579437256
Validation loss: 2.300808705309386

Epoch: 5| Step: 4
Training loss: 1.8966691493988037
Validation loss: 2.2973303077041463

Epoch: 5| Step: 5
Training loss: 3.0799660682678223
Validation loss: 2.296800721076227

Epoch: 5| Step: 6
Training loss: 3.1236987113952637
Validation loss: 2.300434713722557

Epoch: 5| Step: 7
Training loss: 2.6390511989593506
Validation loss: 2.288548675916528

Epoch: 5| Step: 8
Training loss: 2.287501096725464
Validation loss: 2.2715020589931036

Epoch: 5| Step: 9
Training loss: 2.6503372192382812
Validation loss: 2.2733539355698453

Epoch: 5| Step: 10
Training loss: 2.4736270904541016
Validation loss: 2.2736740868578673

Epoch: 147| Step: 0
Training loss: 2.2082741260528564
Validation loss: 2.2693781006720757

Epoch: 5| Step: 1
Training loss: 2.2438158988952637
Validation loss: 2.2619998249956357

Epoch: 5| Step: 2
Training loss: 2.3510403633117676
Validation loss: 2.263610657825265

Epoch: 5| Step: 3
Training loss: 2.832902431488037
Validation loss: 2.2638335407421155

Epoch: 5| Step: 4
Training loss: 2.3193817138671875
Validation loss: 2.2678414057659846

Epoch: 5| Step: 5
Training loss: 2.1075243949890137
Validation loss: 2.2752446461749334

Epoch: 5| Step: 6
Training loss: 1.9744861125946045
Validation loss: 2.273016401516494

Epoch: 5| Step: 7
Training loss: 3.084843158721924
Validation loss: 2.2701353949885212

Epoch: 5| Step: 8
Training loss: 3.312406539916992
Validation loss: 2.2675026386014876

Epoch: 5| Step: 9
Training loss: 2.7125720977783203
Validation loss: 2.269723476902131

Epoch: 5| Step: 10
Training loss: 2.5867342948913574
Validation loss: 2.265336864738054

Epoch: 148| Step: 0
Training loss: 2.060467481613159
Validation loss: 2.2648797483854395

Epoch: 5| Step: 1
Training loss: 2.1375887393951416
Validation loss: 2.263915495205951

Epoch: 5| Step: 2
Training loss: 2.9426329135894775
Validation loss: 2.260971102663266

Epoch: 5| Step: 3
Training loss: 2.3217315673828125
Validation loss: 2.2533386855997066

Epoch: 5| Step: 4
Training loss: 2.123455762863159
Validation loss: 2.2533574437582367

Epoch: 5| Step: 5
Training loss: 2.667515516281128
Validation loss: 2.2615606528456493

Epoch: 5| Step: 6
Training loss: 2.7304165363311768
Validation loss: 2.258695422962148

Epoch: 5| Step: 7
Training loss: 2.176443338394165
Validation loss: 2.2644919169846403

Epoch: 5| Step: 8
Training loss: 2.6677334308624268
Validation loss: 2.2611227381613945

Epoch: 5| Step: 9
Training loss: 3.2019336223602295
Validation loss: 2.268115543550061

Epoch: 5| Step: 10
Training loss: 2.9323675632476807
Validation loss: 2.260407829797396

Epoch: 149| Step: 0
Training loss: 2.5698628425598145
Validation loss: 2.261657517443421

Epoch: 5| Step: 1
Training loss: 1.9508764743804932
Validation loss: 2.254690465106759

Epoch: 5| Step: 2
Training loss: 2.519057512283325
Validation loss: 2.256812357133435

Epoch: 5| Step: 3
Training loss: 2.6934471130371094
Validation loss: 2.2632361278739026

Epoch: 5| Step: 4
Training loss: 2.6625115871429443
Validation loss: 2.2561504353759108

Epoch: 5| Step: 5
Training loss: 2.2029032707214355
Validation loss: 2.250652290159656

Epoch: 5| Step: 6
Training loss: 3.0537796020507812
Validation loss: 2.2423094113667807

Epoch: 5| Step: 7
Training loss: 2.9226949214935303
Validation loss: 2.2448766436628116

Epoch: 5| Step: 8
Training loss: 1.9208730459213257
Validation loss: 2.2426414951201408

Epoch: 5| Step: 9
Training loss: 2.8454182147979736
Validation loss: 2.243596435875021

Epoch: 5| Step: 10
Training loss: 2.4329833984375
Validation loss: 2.2486976295389156

Epoch: 150| Step: 0
Training loss: 2.4528818130493164
Validation loss: 2.24338468428581

Epoch: 5| Step: 1
Training loss: 2.845311403274536
Validation loss: 2.24576215077472

Epoch: 5| Step: 2
Training loss: 2.8625380992889404
Validation loss: 2.2403467368054133

Epoch: 5| Step: 3
Training loss: 2.5545248985290527
Validation loss: 2.255598627110963

Epoch: 5| Step: 4
Training loss: 2.2587592601776123
Validation loss: 2.2557822350532777

Epoch: 5| Step: 5
Training loss: 2.642261266708374
Validation loss: 2.2602946578815417

Epoch: 5| Step: 6
Training loss: 2.1561222076416016
Validation loss: 2.256916762680136

Epoch: 5| Step: 7
Training loss: 2.2976105213165283
Validation loss: 2.2621453731290755

Epoch: 5| Step: 8
Training loss: 3.091968536376953
Validation loss: 2.278836001632034

Epoch: 5| Step: 9
Training loss: 2.1505751609802246
Validation loss: 2.2876726106930803

Epoch: 5| Step: 10
Training loss: 2.394667863845825
Validation loss: 2.2967775432012414

Epoch: 151| Step: 0
Training loss: 2.5329079627990723
Validation loss: 2.3106648729693506

Epoch: 5| Step: 1
Training loss: 2.223456382751465
Validation loss: 2.312297257043982

Epoch: 5| Step: 2
Training loss: 1.9624210596084595
Validation loss: 2.3324753802309752

Epoch: 5| Step: 3
Training loss: 2.7430267333984375
Validation loss: 2.344349076670985

Epoch: 5| Step: 4
Training loss: 3.095776081085205
Validation loss: 2.3594928197963263

Epoch: 5| Step: 5
Training loss: 2.9652512073516846
Validation loss: 2.3188250705760014

Epoch: 5| Step: 6
Training loss: 2.3241820335388184
Validation loss: 2.296122697091872

Epoch: 5| Step: 7
Training loss: 2.7038204669952393
Validation loss: 2.2709467231586413

Epoch: 5| Step: 8
Training loss: 2.0461788177490234
Validation loss: 2.251464828368156

Epoch: 5| Step: 9
Training loss: 2.9217638969421387
Validation loss: 2.2484459159194783

Epoch: 5| Step: 10
Training loss: 2.3018639087677
Validation loss: 2.248841222896371

Epoch: 152| Step: 0
Training loss: 2.0914440155029297
Validation loss: 2.25248150415318

Epoch: 5| Step: 1
Training loss: 3.1357460021972656
Validation loss: 2.2652930854469218

Epoch: 5| Step: 2
Training loss: 2.5663952827453613
Validation loss: 2.2716965636899396

Epoch: 5| Step: 3
Training loss: 2.480342149734497
Validation loss: 2.270231049547913

Epoch: 5| Step: 4
Training loss: 2.41563081741333
Validation loss: 2.264299705464353

Epoch: 5| Step: 5
Training loss: 2.681063175201416
Validation loss: 2.2649606299656693

Epoch: 5| Step: 6
Training loss: 2.5771186351776123
Validation loss: 2.2497830621657835

Epoch: 5| Step: 7
Training loss: 2.3720223903656006
Validation loss: 2.2417888333720546

Epoch: 5| Step: 8
Training loss: 2.1450493335723877
Validation loss: 2.229448180044851

Epoch: 5| Step: 9
Training loss: 2.7780213356018066
Validation loss: 2.243034683248048

Epoch: 5| Step: 10
Training loss: 2.873711585998535
Validation loss: 2.245737821825089

Epoch: 153| Step: 0
Training loss: 2.9089741706848145
Validation loss: 2.252691904703776

Epoch: 5| Step: 1
Training loss: 2.2933921813964844
Validation loss: 2.264103904847176

Epoch: 5| Step: 2
Training loss: 2.6782755851745605
Validation loss: 2.274287813453264

Epoch: 5| Step: 3
Training loss: 2.690952777862549
Validation loss: 2.2832771706324753

Epoch: 5| Step: 4
Training loss: 2.6113498210906982
Validation loss: 2.2961435958903325

Epoch: 5| Step: 5
Training loss: 2.699044704437256
Validation loss: 2.29251322566822

Epoch: 5| Step: 6
Training loss: 2.838552951812744
Validation loss: 2.2748230529087845

Epoch: 5| Step: 7
Training loss: 2.463031768798828
Validation loss: 2.2737652230006393

Epoch: 5| Step: 8
Training loss: 2.1810014247894287
Validation loss: 2.2602047612590175

Epoch: 5| Step: 9
Training loss: 1.6481902599334717
Validation loss: 2.258669530191729

Epoch: 5| Step: 10
Training loss: 2.7517268657684326
Validation loss: 2.2530043150788996

Epoch: 154| Step: 0
Training loss: 2.420642137527466
Validation loss: 2.24012525363635

Epoch: 5| Step: 1
Training loss: 2.1984689235687256
Validation loss: 2.235929996736588

Epoch: 5| Step: 2
Training loss: 2.4052207469940186
Validation loss: 2.2347623763545865

Epoch: 5| Step: 3
Training loss: 2.646728754043579
Validation loss: 2.230164873984552

Epoch: 5| Step: 4
Training loss: 2.3889708518981934
Validation loss: 2.233772764923752

Epoch: 5| Step: 5
Training loss: 3.029012680053711
Validation loss: 2.2407786628251434

Epoch: 5| Step: 6
Training loss: 2.3668372631073
Validation loss: 2.237046739106537

Epoch: 5| Step: 7
Training loss: 2.3670341968536377
Validation loss: 2.2537084266703618

Epoch: 5| Step: 8
Training loss: 3.1113076210021973
Validation loss: 2.250254823315528

Epoch: 5| Step: 9
Training loss: 2.652620792388916
Validation loss: 2.2535287000799693

Epoch: 5| Step: 10
Training loss: 2.036564588546753
Validation loss: 2.249574456163632

Epoch: 155| Step: 0
Training loss: 2.173957109451294
Validation loss: 2.23876707015499

Epoch: 5| Step: 1
Training loss: 3.3182053565979004
Validation loss: 2.2550168216869397

Epoch: 5| Step: 2
Training loss: 2.720738172531128
Validation loss: 2.2561678937686387

Epoch: 5| Step: 3
Training loss: 2.998490810394287
Validation loss: 2.2551364309044293

Epoch: 5| Step: 4
Training loss: 3.007582664489746
Validation loss: 2.258025948719312

Epoch: 5| Step: 5
Training loss: 1.8540573120117188
Validation loss: 2.264199600424818

Epoch: 5| Step: 6
Training loss: 2.396704912185669
Validation loss: 2.2605381024781095

Epoch: 5| Step: 7
Training loss: 2.197805881500244
Validation loss: 2.2657864247598956

Epoch: 5| Step: 8
Training loss: 2.3281664848327637
Validation loss: 2.264436665401664

Epoch: 5| Step: 9
Training loss: 1.9528424739837646
Validation loss: 2.265381436194143

Epoch: 5| Step: 10
Training loss: 2.74183988571167
Validation loss: 2.269738834391358

Epoch: 156| Step: 0
Training loss: 2.088263750076294
Validation loss: 2.2648766015165593

Epoch: 5| Step: 1
Training loss: 2.7550277709960938
Validation loss: 2.264358840962892

Epoch: 5| Step: 2
Training loss: 2.6864051818847656
Validation loss: 2.277644493246591

Epoch: 5| Step: 3
Training loss: 2.3446526527404785
Validation loss: 2.275365665394773

Epoch: 5| Step: 4
Training loss: 2.9073917865753174
Validation loss: 2.2673735746773342

Epoch: 5| Step: 5
Training loss: 2.476118564605713
Validation loss: 2.2608295025364047

Epoch: 5| Step: 6
Training loss: 1.8492200374603271
Validation loss: 2.2613444251398884

Epoch: 5| Step: 7
Training loss: 1.680842638015747
Validation loss: 2.2563542499337146

Epoch: 5| Step: 8
Training loss: 3.0268421173095703
Validation loss: 2.25961454965735

Epoch: 5| Step: 9
Training loss: 3.1655092239379883
Validation loss: 2.2588211823535222

Epoch: 5| Step: 10
Training loss: 2.6545259952545166
Validation loss: 2.251339679123253

Epoch: 157| Step: 0
Training loss: 2.4332966804504395
Validation loss: 2.252786256933725

Epoch: 5| Step: 1
Training loss: 1.8923801183700562
Validation loss: 2.2618827281459684

Epoch: 5| Step: 2
Training loss: 2.3842499256134033
Validation loss: 2.2599564008815314

Epoch: 5| Step: 3
Training loss: 2.256314754486084
Validation loss: 2.237490322000237

Epoch: 5| Step: 4
Training loss: 2.6782445907592773
Validation loss: 2.229278269634452

Epoch: 5| Step: 5
Training loss: 2.6902859210968018
Validation loss: 2.229329752665694

Epoch: 5| Step: 6
Training loss: 2.5801384449005127
Validation loss: 2.2217286017633255

Epoch: 5| Step: 7
Training loss: 2.6897025108337402
Validation loss: 2.223926833880845

Epoch: 5| Step: 8
Training loss: 2.777939558029175
Validation loss: 2.228678955826708

Epoch: 5| Step: 9
Training loss: 2.484802484512329
Validation loss: 2.232625953612789

Epoch: 5| Step: 10
Training loss: 2.7747578620910645
Validation loss: 2.2389538211207234

Epoch: 158| Step: 0
Training loss: 2.5739877223968506
Validation loss: 2.2511877590610134

Epoch: 5| Step: 1
Training loss: 1.94643235206604
Validation loss: 2.2508928506605086

Epoch: 5| Step: 2
Training loss: 2.3773303031921387
Validation loss: 2.257411026185559

Epoch: 5| Step: 3
Training loss: 2.537865400314331
Validation loss: 2.2587845453652005

Epoch: 5| Step: 4
Training loss: 2.750063896179199
Validation loss: 2.244991725490939

Epoch: 5| Step: 5
Training loss: 2.336134433746338
Validation loss: 2.2458001016288676

Epoch: 5| Step: 6
Training loss: 2.373511791229248
Validation loss: 2.235011162296418

Epoch: 5| Step: 7
Training loss: 2.2332115173339844
Validation loss: 2.2314777463995

Epoch: 5| Step: 8
Training loss: 3.022665023803711
Validation loss: 2.2363343469558226

Epoch: 5| Step: 9
Training loss: 3.1537394523620605
Validation loss: 2.235993932652217

Epoch: 5| Step: 10
Training loss: 2.2580244541168213
Validation loss: 2.237571488144577

Epoch: 159| Step: 0
Training loss: 2.7078914642333984
Validation loss: 2.252529569851455

Epoch: 5| Step: 1
Training loss: 2.5098891258239746
Validation loss: 2.2557562217917493

Epoch: 5| Step: 2
Training loss: 2.7718303203582764
Validation loss: 2.2658762034549507

Epoch: 5| Step: 3
Training loss: 2.8312439918518066
Validation loss: 2.2654426661870812

Epoch: 5| Step: 4
Training loss: 2.7996506690979004
Validation loss: 2.276526448547199

Epoch: 5| Step: 5
Training loss: 3.1549980640411377
Validation loss: 2.271531039668668

Epoch: 5| Step: 6
Training loss: 2.5761003494262695
Validation loss: 2.2561983267466226

Epoch: 5| Step: 7
Training loss: 2.02622389793396
Validation loss: 2.2458656757108626

Epoch: 5| Step: 8
Training loss: 1.4754579067230225
Validation loss: 2.2444989911971556

Epoch: 5| Step: 9
Training loss: 2.699584484100342
Validation loss: 2.2524433494896017

Epoch: 5| Step: 10
Training loss: 1.9786453247070312
Validation loss: 2.2584195752297678

Epoch: 160| Step: 0
Training loss: 2.5220344066619873
Validation loss: 2.2706441007634646

Epoch: 5| Step: 1
Training loss: 2.272031545639038
Validation loss: 2.277031449861424

Epoch: 5| Step: 2
Training loss: 2.397216320037842
Validation loss: 2.279424718631211

Epoch: 5| Step: 3
Training loss: 2.2550039291381836
Validation loss: 2.280896632902084

Epoch: 5| Step: 4
Training loss: 2.122016191482544
Validation loss: 2.2783914817276822

Epoch: 5| Step: 5
Training loss: 2.576693296432495
Validation loss: 2.255206859239968

Epoch: 5| Step: 6
Training loss: 2.7543938159942627
Validation loss: 2.2406959277327343

Epoch: 5| Step: 7
Training loss: 2.881303071975708
Validation loss: 2.2286711200591056

Epoch: 5| Step: 8
Training loss: 2.463773012161255
Validation loss: 2.2268468205646803

Epoch: 5| Step: 9
Training loss: 2.857386350631714
Validation loss: 2.228629494226107

Epoch: 5| Step: 10
Training loss: 2.480790615081787
Validation loss: 2.223386802980977

Epoch: 161| Step: 0
Training loss: 2.231959104537964
Validation loss: 2.233774244144399

Epoch: 5| Step: 1
Training loss: 2.4335315227508545
Validation loss: 2.236147565226401

Epoch: 5| Step: 2
Training loss: 3.0359957218170166
Validation loss: 2.2413847959169777

Epoch: 5| Step: 3
Training loss: 3.0263874530792236
Validation loss: 2.2442802741963375

Epoch: 5| Step: 4
Training loss: 2.4469685554504395
Validation loss: 2.2443658818480787

Epoch: 5| Step: 5
Training loss: 1.7449146509170532
Validation loss: 2.2527679089576966

Epoch: 5| Step: 6
Training loss: 2.684039831161499
Validation loss: 2.247421536394345

Epoch: 5| Step: 7
Training loss: 2.189173698425293
Validation loss: 2.2548761470343477

Epoch: 5| Step: 8
Training loss: 2.553004264831543
Validation loss: 2.259061474953928

Epoch: 5| Step: 9
Training loss: 2.8297007083892822
Validation loss: 2.272586303372537

Epoch: 5| Step: 10
Training loss: 2.3583569526672363
Validation loss: 2.2689792853529736

Epoch: 162| Step: 0
Training loss: 2.1461575031280518
Validation loss: 2.2752514090589298

Epoch: 5| Step: 1
Training loss: 2.7860145568847656
Validation loss: 2.2794623810757875

Epoch: 5| Step: 2
Training loss: 2.971822738647461
Validation loss: 2.2794416335321244

Epoch: 5| Step: 3
Training loss: 2.160953998565674
Validation loss: 2.2792893994239067

Epoch: 5| Step: 4
Training loss: 2.0667147636413574
Validation loss: 2.2958826711100917

Epoch: 5| Step: 5
Training loss: 2.295945405960083
Validation loss: 2.2825457088408934

Epoch: 5| Step: 6
Training loss: 2.1419646739959717
Validation loss: 2.2801740477162022

Epoch: 5| Step: 7
Training loss: 2.7993977069854736
Validation loss: 2.2837782726492932

Epoch: 5| Step: 8
Training loss: 3.4350414276123047
Validation loss: 2.271135281491023

Epoch: 5| Step: 9
Training loss: 2.4462027549743652
Validation loss: 2.2710759537194365

Epoch: 5| Step: 10
Training loss: 2.050262689590454
Validation loss: 2.2619225645578034

Epoch: 163| Step: 0
Training loss: 3.1536998748779297
Validation loss: 2.262555253121161

Epoch: 5| Step: 1
Training loss: 2.6125285625457764
Validation loss: 2.2461485119276148

Epoch: 5| Step: 2
Training loss: 2.007807970046997
Validation loss: 2.2486653686851583

Epoch: 5| Step: 3
Training loss: 2.686418056488037
Validation loss: 2.245626295766523

Epoch: 5| Step: 4
Training loss: 2.065974712371826
Validation loss: 2.2546448938308226

Epoch: 5| Step: 5
Training loss: 2.366243839263916
Validation loss: 2.250123582860475

Epoch: 5| Step: 6
Training loss: 3.069836139678955
Validation loss: 2.2569945627643215

Epoch: 5| Step: 7
Training loss: 1.8251819610595703
Validation loss: 2.2457138825488347

Epoch: 5| Step: 8
Training loss: 2.7114500999450684
Validation loss: 2.24626951448379

Epoch: 5| Step: 9
Training loss: 2.542445659637451
Validation loss: 2.2408515586647937

Epoch: 5| Step: 10
Training loss: 2.291072130203247
Validation loss: 2.236334703301871

Epoch: 164| Step: 0
Training loss: 2.098315715789795
Validation loss: 2.2302759488423667

Epoch: 5| Step: 1
Training loss: 2.1137239933013916
Validation loss: 2.232469038296771

Epoch: 5| Step: 2
Training loss: 2.6572587490081787
Validation loss: 2.2378858225319975

Epoch: 5| Step: 3
Training loss: 2.630911111831665
Validation loss: 2.2423796192292245

Epoch: 5| Step: 4
Training loss: 2.9387447834014893
Validation loss: 2.2357718226730183

Epoch: 5| Step: 5
Training loss: 2.2871673107147217
Validation loss: 2.2438698942943285

Epoch: 5| Step: 6
Training loss: 2.524625778198242
Validation loss: 2.2418582798332296

Epoch: 5| Step: 7
Training loss: 3.019404888153076
Validation loss: 2.252663864884325

Epoch: 5| Step: 8
Training loss: 2.0490353107452393
Validation loss: 2.2561691704616753

Epoch: 5| Step: 9
Training loss: 2.9292778968811035
Validation loss: 2.2556768848050024

Epoch: 5| Step: 10
Training loss: 1.8035885095596313
Validation loss: 2.2602262266220583

Epoch: 165| Step: 0
Training loss: 2.668102264404297
Validation loss: 2.2556754132752777

Epoch: 5| Step: 1
Training loss: 2.4940662384033203
Validation loss: 2.2535959059192288

Epoch: 5| Step: 2
Training loss: 2.123783588409424
Validation loss: 2.249121912064091

Epoch: 5| Step: 3
Training loss: 2.475771427154541
Validation loss: 2.251433087933448

Epoch: 5| Step: 4
Training loss: 2.9434173107147217
Validation loss: 2.2499376599506666

Epoch: 5| Step: 5
Training loss: 2.653102397918701
Validation loss: 2.2413617410967426

Epoch: 5| Step: 6
Training loss: 1.8106340169906616
Validation loss: 2.249144720774825

Epoch: 5| Step: 7
Training loss: 2.558077096939087
Validation loss: 2.2564544626461562

Epoch: 5| Step: 8
Training loss: 2.927625894546509
Validation loss: 2.2536371600243355

Epoch: 5| Step: 9
Training loss: 2.027794361114502
Validation loss: 2.249444007873535

Epoch: 5| Step: 10
Training loss: 2.536149024963379
Validation loss: 2.2476805820259997

Epoch: 166| Step: 0
Training loss: 2.160768985748291
Validation loss: 2.238073220816992

Epoch: 5| Step: 1
Training loss: 2.5681915283203125
Validation loss: 2.2366941282826085

Epoch: 5| Step: 2
Training loss: 2.4149765968322754
Validation loss: 2.2176301146066315

Epoch: 5| Step: 3
Training loss: 1.924228310585022
Validation loss: 2.2270217300743185

Epoch: 5| Step: 4
Training loss: 2.738018035888672
Validation loss: 2.2168179506896646

Epoch: 5| Step: 5
Training loss: 3.072812557220459
Validation loss: 2.2185951855874833

Epoch: 5| Step: 6
Training loss: 2.4633946418762207
Validation loss: 2.207980896836968

Epoch: 5| Step: 7
Training loss: 2.1940929889678955
Validation loss: 2.2159169002245833

Epoch: 5| Step: 8
Training loss: 2.113719940185547
Validation loss: 2.2180172935608895

Epoch: 5| Step: 9
Training loss: 2.995018720626831
Validation loss: 2.2234793273351525

Epoch: 5| Step: 10
Training loss: 2.576934814453125
Validation loss: 2.22975908043564

Epoch: 167| Step: 0
Training loss: 2.320241928100586
Validation loss: 2.2284174708909887

Epoch: 5| Step: 1
Training loss: 2.679386615753174
Validation loss: 2.2322154301469044

Epoch: 5| Step: 2
Training loss: 2.817842960357666
Validation loss: 2.235107519293344

Epoch: 5| Step: 3
Training loss: 2.4329593181610107
Validation loss: 2.244707102416664

Epoch: 5| Step: 4
Training loss: 2.323197841644287
Validation loss: 2.253896128746771

Epoch: 5| Step: 5
Training loss: 1.7883678674697876
Validation loss: 2.2502350499553065

Epoch: 5| Step: 6
Training loss: 2.3977153301239014
Validation loss: 2.255385680865216

Epoch: 5| Step: 7
Training loss: 2.116558790206909
Validation loss: 2.263448980546767

Epoch: 5| Step: 8
Training loss: 2.8778064250946045
Validation loss: 2.27438856709388

Epoch: 5| Step: 9
Training loss: 2.596886157989502
Validation loss: 2.270474298025972

Epoch: 5| Step: 10
Training loss: 2.950711965560913
Validation loss: 2.258323359233077

Epoch: 168| Step: 0
Training loss: 2.3201770782470703
Validation loss: 2.250694167229437

Epoch: 5| Step: 1
Training loss: 2.4153974056243896
Validation loss: 2.24121025044431

Epoch: 5| Step: 2
Training loss: 2.6290371417999268
Validation loss: 2.2517889289445776

Epoch: 5| Step: 3
Training loss: 1.7062957286834717
Validation loss: 2.267919466059695

Epoch: 5| Step: 4
Training loss: 3.1197447776794434
Validation loss: 2.258963320844917

Epoch: 5| Step: 5
Training loss: 3.5550029277801514
Validation loss: 2.252515278836732

Epoch: 5| Step: 6
Training loss: 1.7814571857452393
Validation loss: 2.2499826518438195

Epoch: 5| Step: 7
Training loss: 3.0331294536590576
Validation loss: 2.238439267681491

Epoch: 5| Step: 8
Training loss: 1.712484359741211
Validation loss: 2.23274653445008

Epoch: 5| Step: 9
Training loss: 1.9234663248062134
Validation loss: 2.2233885693293747

Epoch: 5| Step: 10
Training loss: 2.9863367080688477
Validation loss: 2.226994570865426

Epoch: 169| Step: 0
Training loss: 1.8881194591522217
Validation loss: 2.2250953233370216

Epoch: 5| Step: 1
Training loss: 2.4643750190734863
Validation loss: 2.2235049201596166

Epoch: 5| Step: 2
Training loss: 2.2186100482940674
Validation loss: 2.226502909455248

Epoch: 5| Step: 3
Training loss: 2.769552230834961
Validation loss: 2.221963455600123

Epoch: 5| Step: 4
Training loss: 2.5359160900115967
Validation loss: 2.2308243295197845

Epoch: 5| Step: 5
Training loss: 2.448272466659546
Validation loss: 2.228834459858556

Epoch: 5| Step: 6
Training loss: 2.5093581676483154
Validation loss: 2.2367451421676146

Epoch: 5| Step: 7
Training loss: 2.6961495876312256
Validation loss: 2.248389713225826

Epoch: 5| Step: 8
Training loss: 2.421337366104126
Validation loss: 2.24924797396506

Epoch: 5| Step: 9
Training loss: 2.281860828399658
Validation loss: 2.2605403828364548

Epoch: 5| Step: 10
Training loss: 2.8709545135498047
Validation loss: 2.2592579677540767

Epoch: 170| Step: 0
Training loss: 2.2084436416625977
Validation loss: 2.23871938900281

Epoch: 5| Step: 1
Training loss: 1.910325288772583
Validation loss: 2.231393546186468

Epoch: 5| Step: 2
Training loss: 2.110059976577759
Validation loss: 2.2308807167955624

Epoch: 5| Step: 3
Training loss: 2.726550579071045
Validation loss: 2.2368434321495796

Epoch: 5| Step: 4
Training loss: 2.856126308441162
Validation loss: 2.2391518315961285

Epoch: 5| Step: 5
Training loss: 2.6655759811401367
Validation loss: 2.2296911208860335

Epoch: 5| Step: 6
Training loss: 2.457047700881958
Validation loss: 2.22367198236527

Epoch: 5| Step: 7
Training loss: 2.254699230194092
Validation loss: 2.2160967037241948

Epoch: 5| Step: 8
Training loss: 2.1806793212890625
Validation loss: 2.2202010564906622

Epoch: 5| Step: 9
Training loss: 3.2936389446258545
Validation loss: 2.232335088073566

Epoch: 5| Step: 10
Training loss: 2.2799887657165527
Validation loss: 2.225298748221449

Epoch: 171| Step: 0
Training loss: 2.1953845024108887
Validation loss: 2.228265969983993

Epoch: 5| Step: 1
Training loss: 2.592332124710083
Validation loss: 2.236485163370768

Epoch: 5| Step: 2
Training loss: 2.6240763664245605
Validation loss: 2.2280111928139963

Epoch: 5| Step: 3
Training loss: 2.0342187881469727
Validation loss: 2.2226332746526247

Epoch: 5| Step: 4
Training loss: 2.5530166625976562
Validation loss: 2.2160046920981458

Epoch: 5| Step: 5
Training loss: 2.5914597511291504
Validation loss: 2.222266633023498

Epoch: 5| Step: 6
Training loss: 2.6298036575317383
Validation loss: 2.228625939738366

Epoch: 5| Step: 7
Training loss: 2.322046995162964
Validation loss: 2.219623165745889

Epoch: 5| Step: 8
Training loss: 2.7845776081085205
Validation loss: 2.2075533020880913

Epoch: 5| Step: 9
Training loss: 2.284208297729492
Validation loss: 2.2020153614782516

Epoch: 5| Step: 10
Training loss: 2.6041910648345947
Validation loss: 2.1928142424552672

Epoch: 172| Step: 0
Training loss: 2.8964672088623047
Validation loss: 2.1958204366827525

Epoch: 5| Step: 1
Training loss: 2.4393787384033203
Validation loss: 2.203749223421979

Epoch: 5| Step: 2
Training loss: 3.297934055328369
Validation loss: 2.2233235887301865

Epoch: 5| Step: 3
Training loss: 2.2649102210998535
Validation loss: 2.2256396560258764

Epoch: 5| Step: 4
Training loss: 2.263960361480713
Validation loss: 2.243112156468053

Epoch: 5| Step: 5
Training loss: 2.4570963382720947
Validation loss: 2.2454352378845215

Epoch: 5| Step: 6
Training loss: 2.2641665935516357
Validation loss: 2.22820431699035

Epoch: 5| Step: 7
Training loss: 1.931913137435913
Validation loss: 2.218792361597861

Epoch: 5| Step: 8
Training loss: 2.339120626449585
Validation loss: 2.2052221503309024

Epoch: 5| Step: 9
Training loss: 2.808136224746704
Validation loss: 2.208189713057651

Epoch: 5| Step: 10
Training loss: 2.1682324409484863
Validation loss: 2.203452064144996

Epoch: 173| Step: 0
Training loss: 1.934798002243042
Validation loss: 2.21267742623565

Epoch: 5| Step: 1
Training loss: 2.342520236968994
Validation loss: 2.2139642238616943

Epoch: 5| Step: 2
Training loss: 2.0906693935394287
Validation loss: 2.2224859217161774

Epoch: 5| Step: 3
Training loss: 3.265101671218872
Validation loss: 2.2221528996703444

Epoch: 5| Step: 4
Training loss: 2.2057206630706787
Validation loss: 2.22258823661394

Epoch: 5| Step: 5
Training loss: 2.0182528495788574
Validation loss: 2.2199222272442234

Epoch: 5| Step: 6
Training loss: 2.115340232849121
Validation loss: 2.2235178280902166

Epoch: 5| Step: 7
Training loss: 2.835434675216675
Validation loss: 2.23103024882655

Epoch: 5| Step: 8
Training loss: 3.052086353302002
Validation loss: 2.230252624839865

Epoch: 5| Step: 9
Training loss: 3.312321901321411
Validation loss: 2.236391111086774

Epoch: 5| Step: 10
Training loss: 1.7375186681747437
Validation loss: 2.2308408534655007

Epoch: 174| Step: 0
Training loss: 2.503174304962158
Validation loss: 2.2386953241081646

Epoch: 5| Step: 1
Training loss: 2.9953722953796387
Validation loss: 2.2332092536393033

Epoch: 5| Step: 2
Training loss: 2.6858699321746826
Validation loss: 2.2312541802724204

Epoch: 5| Step: 3
Training loss: 1.8453319072723389
Validation loss: 2.228930265672745

Epoch: 5| Step: 4
Training loss: 2.6025161743164062
Validation loss: 2.23687566736693

Epoch: 5| Step: 5
Training loss: 2.4331040382385254
Validation loss: 2.2224607185650895

Epoch: 5| Step: 6
Training loss: 2.4116663932800293
Validation loss: 2.228715888915523

Epoch: 5| Step: 7
Training loss: 2.442734956741333
Validation loss: 2.234249645663846

Epoch: 5| Step: 8
Training loss: 2.3936047554016113
Validation loss: 2.2427521572318128

Epoch: 5| Step: 9
Training loss: 2.340949773788452
Validation loss: 2.248919233199089

Epoch: 5| Step: 10
Training loss: 2.32631254196167
Validation loss: 2.2338793636650167

Epoch: 175| Step: 0
Training loss: 2.959449052810669
Validation loss: 2.227035255842311

Epoch: 5| Step: 1
Training loss: 2.262707471847534
Validation loss: 2.2222853578546995

Epoch: 5| Step: 2
Training loss: 2.43532657623291
Validation loss: 2.227945262385953

Epoch: 5| Step: 3
Training loss: 2.6741013526916504
Validation loss: 2.2314490477244058

Epoch: 5| Step: 4
Training loss: 2.851642608642578
Validation loss: 2.236418342077604

Epoch: 5| Step: 5
Training loss: 2.2108707427978516
Validation loss: 2.2294335493477444

Epoch: 5| Step: 6
Training loss: 1.6546109914779663
Validation loss: 2.2233127778576267

Epoch: 5| Step: 7
Training loss: 2.1413583755493164
Validation loss: 2.215149523109518

Epoch: 5| Step: 8
Training loss: 2.9241435527801514
Validation loss: 2.212251332498366

Epoch: 5| Step: 9
Training loss: 2.279219627380371
Validation loss: 2.2094438499019993

Epoch: 5| Step: 10
Training loss: 2.541494846343994
Validation loss: 2.219564430175289

Testing loss: 2.4151459799872503

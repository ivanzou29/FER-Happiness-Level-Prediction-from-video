Epoch: 1| Step: 0
Training loss: 5.005187034606934
Validation loss: 5.213037660045009

Epoch: 5| Step: 1
Training loss: 4.300150394439697
Validation loss: 5.207507369338825

Epoch: 5| Step: 2
Training loss: 4.695775508880615
Validation loss: 5.202086176923526

Epoch: 5| Step: 3
Training loss: 4.95651912689209
Validation loss: 5.196577390034993

Epoch: 5| Step: 4
Training loss: 4.578962802886963
Validation loss: 5.191211777348673

Epoch: 5| Step: 5
Training loss: 5.3566179275512695
Validation loss: 5.185722525401782

Epoch: 5| Step: 6
Training loss: 4.795791149139404
Validation loss: 5.18000405321839

Epoch: 5| Step: 7
Training loss: 5.061099052429199
Validation loss: 5.174686765158048

Epoch: 5| Step: 8
Training loss: 5.363447189331055
Validation loss: 5.168973004946145

Epoch: 5| Step: 9
Training loss: 5.655722618103027
Validation loss: 5.163045329432333

Epoch: 5| Step: 10
Training loss: 5.0262770652771
Validation loss: 5.156772869889454

Epoch: 2| Step: 0
Training loss: 5.675446510314941
Validation loss: 5.1506332018042125

Epoch: 5| Step: 1
Training loss: 3.998460292816162
Validation loss: 5.1437533593946885

Epoch: 5| Step: 2
Training loss: 4.530876636505127
Validation loss: 5.137603072709934

Epoch: 5| Step: 3
Training loss: 4.3766865730285645
Validation loss: 5.130607656253281

Epoch: 5| Step: 4
Training loss: 5.391728401184082
Validation loss: 5.124012244644986

Epoch: 5| Step: 5
Training loss: 4.714280128479004
Validation loss: 5.115545898355464

Epoch: 5| Step: 6
Training loss: 5.209973335266113
Validation loss: 5.108306710438062

Epoch: 5| Step: 7
Training loss: 4.83870792388916
Validation loss: 5.100402903813188

Epoch: 5| Step: 8
Training loss: 5.1645708084106445
Validation loss: 5.091420965809976

Epoch: 5| Step: 9
Training loss: 5.3120012283325195
Validation loss: 5.0834110270264325

Epoch: 5| Step: 10
Training loss: 4.745809555053711
Validation loss: 5.073295790662048

Epoch: 3| Step: 0
Training loss: 4.705320835113525
Validation loss: 5.063103678405926

Epoch: 5| Step: 1
Training loss: 5.0791730880737305
Validation loss: 5.053521033256285

Epoch: 5| Step: 2
Training loss: 6.1932172775268555
Validation loss: 5.042657913700227

Epoch: 5| Step: 3
Training loss: 5.424263000488281
Validation loss: 5.032139388463831

Epoch: 5| Step: 4
Training loss: 5.197237491607666
Validation loss: 5.020622161126906

Epoch: 5| Step: 5
Training loss: 4.179288387298584
Validation loss: 5.007724418435045

Epoch: 5| Step: 6
Training loss: 4.381631851196289
Validation loss: 4.995265227492138

Epoch: 5| Step: 7
Training loss: 4.945235729217529
Validation loss: 4.982140746167911

Epoch: 5| Step: 8
Training loss: 3.8099780082702637
Validation loss: 4.9683635004105104

Epoch: 5| Step: 9
Training loss: 4.373266696929932
Validation loss: 4.953901034529491

Epoch: 5| Step: 10
Training loss: 4.4203619956970215
Validation loss: 4.939601082955638

Epoch: 4| Step: 0
Training loss: 3.9126415252685547
Validation loss: 4.923175637440015

Epoch: 5| Step: 1
Training loss: 4.781809329986572
Validation loss: 4.9066982371832735

Epoch: 5| Step: 2
Training loss: 5.12909460067749
Validation loss: 4.889536109021915

Epoch: 5| Step: 3
Training loss: 5.879695892333984
Validation loss: 4.87194287905129

Epoch: 5| Step: 4
Training loss: 4.234583854675293
Validation loss: 4.853337267393707

Epoch: 5| Step: 5
Training loss: 4.709429740905762
Validation loss: 4.834060033162435

Epoch: 5| Step: 6
Training loss: 4.041970729827881
Validation loss: 4.81465527062775

Epoch: 5| Step: 7
Training loss: 4.98124885559082
Validation loss: 4.7939369447769655

Epoch: 5| Step: 8
Training loss: 4.814125061035156
Validation loss: 4.772599399730724

Epoch: 5| Step: 9
Training loss: 4.819890975952148
Validation loss: 4.750373435276811

Epoch: 5| Step: 10
Training loss: 3.2662947177886963
Validation loss: 4.726666963228616

Epoch: 5| Step: 0
Training loss: 4.428065299987793
Validation loss: 4.703395059031825

Epoch: 5| Step: 1
Training loss: 4.367610931396484
Validation loss: 4.679322883646975

Epoch: 5| Step: 2
Training loss: 4.484557151794434
Validation loss: 4.654706965210617

Epoch: 5| Step: 3
Training loss: 5.174842834472656
Validation loss: 4.627783134419431

Epoch: 5| Step: 4
Training loss: 3.731423854827881
Validation loss: 4.604138394837738

Epoch: 5| Step: 5
Training loss: 3.2230236530303955
Validation loss: 4.575029424441758

Epoch: 5| Step: 6
Training loss: 4.8058061599731445
Validation loss: 4.549175949506862

Epoch: 5| Step: 7
Training loss: 4.995020866394043
Validation loss: 4.520891917649136

Epoch: 5| Step: 8
Training loss: 4.337574481964111
Validation loss: 4.492531940501223

Epoch: 5| Step: 9
Training loss: 4.667206764221191
Validation loss: 4.46181333193215

Epoch: 5| Step: 10
Training loss: 3.589655637741089
Validation loss: 4.433620724626767

Epoch: 6| Step: 0
Training loss: 3.983135223388672
Validation loss: 4.405145542595976

Epoch: 5| Step: 1
Training loss: 4.287709712982178
Validation loss: 4.376126158622004

Epoch: 5| Step: 2
Training loss: 4.307939529418945
Validation loss: 4.346595751341953

Epoch: 5| Step: 3
Training loss: 4.838751792907715
Validation loss: 4.3175549917323615

Epoch: 5| Step: 4
Training loss: 4.118527412414551
Validation loss: 4.2876718736463975

Epoch: 5| Step: 5
Training loss: 5.285809516906738
Validation loss: 4.258864961644655

Epoch: 5| Step: 6
Training loss: 3.967728853225708
Validation loss: 4.2329684918926604

Epoch: 5| Step: 7
Training loss: 3.9878406524658203
Validation loss: 4.200395696906633

Epoch: 5| Step: 8
Training loss: 2.5002198219299316
Validation loss: 4.171209360963555

Epoch: 5| Step: 9
Training loss: 3.4461581707000732
Validation loss: 4.140434085681874

Epoch: 5| Step: 10
Training loss: 4.0038604736328125
Validation loss: 4.1135297770141275

Epoch: 7| Step: 0
Training loss: 4.255593299865723
Validation loss: 4.08661695705947

Epoch: 5| Step: 1
Training loss: 4.728941917419434
Validation loss: 4.056672526944068

Epoch: 5| Step: 2
Training loss: 3.7984020709991455
Validation loss: 4.0257223908619215

Epoch: 5| Step: 3
Training loss: 2.7029144763946533
Validation loss: 3.995490615085889

Epoch: 5| Step: 4
Training loss: 4.583802223205566
Validation loss: 3.9647095126490437

Epoch: 5| Step: 5
Training loss: 2.762770414352417
Validation loss: 3.9352655051856913

Epoch: 5| Step: 6
Training loss: 3.7917017936706543
Validation loss: 3.9065898259480796

Epoch: 5| Step: 7
Training loss: 4.043759822845459
Validation loss: 3.8791381774410123

Epoch: 5| Step: 8
Training loss: 3.4829394817352295
Validation loss: 3.856904491301506

Epoch: 5| Step: 9
Training loss: 3.8633017539978027
Validation loss: 3.83031524637694

Epoch: 5| Step: 10
Training loss: 3.890038013458252
Validation loss: 3.8075124781618834

Epoch: 8| Step: 0
Training loss: 4.636549472808838
Validation loss: 3.7871320401468584

Epoch: 5| Step: 1
Training loss: 4.062932014465332
Validation loss: 3.7678831367082495

Epoch: 5| Step: 2
Training loss: 3.59031343460083
Validation loss: 3.7459488376494376

Epoch: 5| Step: 3
Training loss: 2.5548532009124756
Validation loss: 3.7248431098076606

Epoch: 5| Step: 4
Training loss: 3.464244842529297
Validation loss: 3.7077153575035835

Epoch: 5| Step: 5
Training loss: 3.3944315910339355
Validation loss: 3.6901963962021695

Epoch: 5| Step: 6
Training loss: 3.881953477859497
Validation loss: 3.672346225348852

Epoch: 5| Step: 7
Training loss: 3.309893846511841
Validation loss: 3.6563816737103205

Epoch: 5| Step: 8
Training loss: 3.946690797805786
Validation loss: 3.6399389595113774

Epoch: 5| Step: 9
Training loss: 3.4231467247009277
Validation loss: 3.6224531819743495

Epoch: 5| Step: 10
Training loss: 3.1800544261932373
Validation loss: 3.6089926099264495

Epoch: 9| Step: 0
Training loss: 3.4820990562438965
Validation loss: 3.5912566287543184

Epoch: 5| Step: 1
Training loss: 3.2919418811798096
Validation loss: 3.575362674651607

Epoch: 5| Step: 2
Training loss: 3.3280482292175293
Validation loss: 3.561454460185061

Epoch: 5| Step: 3
Training loss: 3.5054314136505127
Validation loss: 3.542942113773797

Epoch: 5| Step: 4
Training loss: 5.101016044616699
Validation loss: 3.526674537248509

Epoch: 5| Step: 5
Training loss: 2.9675540924072266
Validation loss: 3.508096166836318

Epoch: 5| Step: 6
Training loss: 3.4433281421661377
Validation loss: 3.486624415202807

Epoch: 5| Step: 7
Training loss: 2.7311010360717773
Validation loss: 3.466083367665609

Epoch: 5| Step: 8
Training loss: 4.274407386779785
Validation loss: 3.4462320830232356

Epoch: 5| Step: 9
Training loss: 2.976353406906128
Validation loss: 3.4279776952599965

Epoch: 5| Step: 10
Training loss: 2.606518268585205
Validation loss: 3.4124182039691555

Epoch: 10| Step: 0
Training loss: 4.363799095153809
Validation loss: 3.399604602526593

Epoch: 5| Step: 1
Training loss: 3.4740588665008545
Validation loss: 3.38803683557818

Epoch: 5| Step: 2
Training loss: 3.3200325965881348
Validation loss: 3.3750192119229223

Epoch: 5| Step: 3
Training loss: 2.665558338165283
Validation loss: 3.359490507392473

Epoch: 5| Step: 4
Training loss: 3.823319911956787
Validation loss: 3.3488997233811246

Epoch: 5| Step: 5
Training loss: 3.39385986328125
Validation loss: 3.335842950369722

Epoch: 5| Step: 6
Training loss: 3.601769208908081
Validation loss: 3.326500592693206

Epoch: 5| Step: 7
Training loss: 3.1287307739257812
Validation loss: 3.316811371875066

Epoch: 5| Step: 8
Training loss: 2.6981544494628906
Validation loss: 3.309665943986626

Epoch: 5| Step: 9
Training loss: 3.242412567138672
Validation loss: 3.299547326180243

Epoch: 5| Step: 10
Training loss: 2.5439224243164062
Validation loss: 3.2886961403713433

Epoch: 11| Step: 0
Training loss: 3.690056324005127
Validation loss: 3.2808064132608394

Epoch: 5| Step: 1
Training loss: 2.920037269592285
Validation loss: 3.270134643841815

Epoch: 5| Step: 2
Training loss: 3.381873607635498
Validation loss: 3.2585686304235972

Epoch: 5| Step: 3
Training loss: 3.3355841636657715
Validation loss: 3.2515683481770177

Epoch: 5| Step: 4
Training loss: 3.780198335647583
Validation loss: 3.2387913529590895

Epoch: 5| Step: 5
Training loss: 2.4442429542541504
Validation loss: 3.2297769643927134

Epoch: 5| Step: 6
Training loss: 3.0645382404327393
Validation loss: 3.221114553431029

Epoch: 5| Step: 7
Training loss: 3.4158401489257812
Validation loss: 3.211582014637609

Epoch: 5| Step: 8
Training loss: 3.982234239578247
Validation loss: 3.1997702249916653

Epoch: 5| Step: 9
Training loss: 2.767563819885254
Validation loss: 3.1938455720101633

Epoch: 5| Step: 10
Training loss: 2.5396363735198975
Validation loss: 3.184636208318895

Epoch: 12| Step: 0
Training loss: 3.543907880783081
Validation loss: 3.1746780000707155

Epoch: 5| Step: 1
Training loss: 3.1509430408477783
Validation loss: 3.1668420555771037

Epoch: 5| Step: 2
Training loss: 3.2189903259277344
Validation loss: 3.156219918240783

Epoch: 5| Step: 3
Training loss: 3.2952628135681152
Validation loss: 3.1485390432419313

Epoch: 5| Step: 4
Training loss: 4.195982933044434
Validation loss: 3.1400563870706866

Epoch: 5| Step: 5
Training loss: 3.60148549079895
Validation loss: 3.1343138371744463

Epoch: 5| Step: 6
Training loss: 3.0923609733581543
Validation loss: 3.1268622567576747

Epoch: 5| Step: 7
Training loss: 2.752377986907959
Validation loss: 3.119998234574513

Epoch: 5| Step: 8
Training loss: 2.2515480518341064
Validation loss: 3.11282750098936

Epoch: 5| Step: 9
Training loss: 2.6507232189178467
Validation loss: 3.1070643471133326

Epoch: 5| Step: 10
Training loss: 2.868478536605835
Validation loss: 3.1019784609476724

Epoch: 13| Step: 0
Training loss: 2.1609997749328613
Validation loss: 3.097680760968116

Epoch: 5| Step: 1
Training loss: 2.2927207946777344
Validation loss: 3.0896405737887145

Epoch: 5| Step: 2
Training loss: 3.3643295764923096
Validation loss: 3.0857199674011557

Epoch: 5| Step: 3
Training loss: 2.853309154510498
Validation loss: 3.0817588478006344

Epoch: 5| Step: 4
Training loss: 3.2269396781921387
Validation loss: 3.0760629818003666

Epoch: 5| Step: 5
Training loss: 2.9993278980255127
Validation loss: 3.070012172063192

Epoch: 5| Step: 6
Training loss: 3.590278148651123
Validation loss: 3.065388733340848

Epoch: 5| Step: 7
Training loss: 3.8058979511260986
Validation loss: 3.061391515116538

Epoch: 5| Step: 8
Training loss: 3.3265833854675293
Validation loss: 3.056535582388601

Epoch: 5| Step: 9
Training loss: 3.606901168823242
Validation loss: 3.050894191188197

Epoch: 5| Step: 10
Training loss: 2.8854336738586426
Validation loss: 3.0477003410298336

Epoch: 14| Step: 0
Training loss: 3.251506805419922
Validation loss: 3.0413783980954077

Epoch: 5| Step: 1
Training loss: 3.326747179031372
Validation loss: 3.0362609970954155

Epoch: 5| Step: 2
Training loss: 3.123218059539795
Validation loss: 3.0323927787042435

Epoch: 5| Step: 3
Training loss: 2.7494757175445557
Validation loss: 3.03071758567646

Epoch: 5| Step: 4
Training loss: 3.665182590484619
Validation loss: 3.0248967011769614

Epoch: 5| Step: 5
Training loss: 2.5723845958709717
Validation loss: 3.021246853695121

Epoch: 5| Step: 6
Training loss: 2.5164501667022705
Validation loss: 3.0162420811191684

Epoch: 5| Step: 7
Training loss: 3.147876739501953
Validation loss: 3.0115283868646108

Epoch: 5| Step: 8
Training loss: 2.9269485473632812
Validation loss: 3.005687980241673

Epoch: 5| Step: 9
Training loss: 3.5947673320770264
Validation loss: 3.0013463779162337

Epoch: 5| Step: 10
Training loss: 2.90902042388916
Validation loss: 2.996515474011821

Epoch: 15| Step: 0
Training loss: 3.0822155475616455
Validation loss: 2.993642391697053

Epoch: 5| Step: 1
Training loss: 3.424954652786255
Validation loss: 2.991417800226519

Epoch: 5| Step: 2
Training loss: 3.352534532546997
Validation loss: 2.9854964543414373

Epoch: 5| Step: 3
Training loss: 3.0552303791046143
Validation loss: 2.9827906752145417

Epoch: 5| Step: 4
Training loss: 2.999783992767334
Validation loss: 2.978857304460259

Epoch: 5| Step: 5
Training loss: 2.9793202877044678
Validation loss: 2.979174498588808

Epoch: 5| Step: 6
Training loss: 2.87986421585083
Validation loss: 2.973087562027798

Epoch: 5| Step: 7
Training loss: 3.153221845626831
Validation loss: 2.9755670639776413

Epoch: 5| Step: 8
Training loss: 2.9019787311553955
Validation loss: 2.967319175761233

Epoch: 5| Step: 9
Training loss: 3.0139522552490234
Validation loss: 2.9619071663066907

Epoch: 5| Step: 10
Training loss: 2.564237117767334
Validation loss: 2.9583885951708724

Epoch: 16| Step: 0
Training loss: 2.7079811096191406
Validation loss: 2.953481343484694

Epoch: 5| Step: 1
Training loss: 2.5562222003936768
Validation loss: 2.9535692276493197

Epoch: 5| Step: 2
Training loss: 3.2833619117736816
Validation loss: 2.947462320327759

Epoch: 5| Step: 3
Training loss: 3.101357936859131
Validation loss: 2.9483758864864225

Epoch: 5| Step: 4
Training loss: 2.8829777240753174
Validation loss: 2.9460167551553376

Epoch: 5| Step: 5
Training loss: 3.296339511871338
Validation loss: 2.9432203949138684

Epoch: 5| Step: 6
Training loss: 3.4018759727478027
Validation loss: 2.938465197881063

Epoch: 5| Step: 7
Training loss: 3.760331630706787
Validation loss: 2.9342347703954226

Epoch: 5| Step: 8
Training loss: 2.664212226867676
Validation loss: 2.934625948629072

Epoch: 5| Step: 9
Training loss: 3.417604446411133
Validation loss: 2.9298000489511797

Epoch: 5| Step: 10
Training loss: 1.9938453435897827
Validation loss: 2.9286650790963122

Epoch: 17| Step: 0
Training loss: 3.43315052986145
Validation loss: 2.929011975565264

Epoch: 5| Step: 1
Training loss: 3.001223087310791
Validation loss: 2.9287595928356214

Epoch: 5| Step: 2
Training loss: 2.8831706047058105
Validation loss: 2.9309990559854815

Epoch: 5| Step: 3
Training loss: 3.660491466522217
Validation loss: 2.921602646509806

Epoch: 5| Step: 4
Training loss: 3.371061325073242
Validation loss: 2.918498762192265

Epoch: 5| Step: 5
Training loss: 2.6913199424743652
Validation loss: 2.914249071510889

Epoch: 5| Step: 6
Training loss: 2.7313027381896973
Validation loss: 2.912479523689516

Epoch: 5| Step: 7
Training loss: 2.760232448577881
Validation loss: 2.9082878097411125

Epoch: 5| Step: 8
Training loss: 2.744091749191284
Validation loss: 2.912290750011321

Epoch: 5| Step: 9
Training loss: 3.363090991973877
Validation loss: 2.912253784876998

Epoch: 5| Step: 10
Training loss: 2.337339162826538
Validation loss: 2.9107911356033815

Epoch: 18| Step: 0
Training loss: 3.089496612548828
Validation loss: 2.903741826293289

Epoch: 5| Step: 1
Training loss: 2.9749510288238525
Validation loss: 2.9017683588048464

Epoch: 5| Step: 2
Training loss: 3.479780673980713
Validation loss: 2.8982802719198246

Epoch: 5| Step: 3
Training loss: 2.4559051990509033
Validation loss: 2.893078045178485

Epoch: 5| Step: 4
Training loss: 3.2323031425476074
Validation loss: 2.8914159651725524

Epoch: 5| Step: 5
Training loss: 3.1977508068084717
Validation loss: 2.8929174433472338

Epoch: 5| Step: 6
Training loss: 2.486846446990967
Validation loss: 2.8915334132409867

Epoch: 5| Step: 7
Training loss: 3.029663562774658
Validation loss: 2.893915914720105

Epoch: 5| Step: 8
Training loss: 2.6727852821350098
Validation loss: 2.890125218258109

Epoch: 5| Step: 9
Training loss: 3.1320133209228516
Validation loss: 2.8909003555133777

Epoch: 5| Step: 10
Training loss: 3.193852424621582
Validation loss: 2.889703143027521

Epoch: 19| Step: 0
Training loss: 3.2242331504821777
Validation loss: 2.882238026588194

Epoch: 5| Step: 1
Training loss: 3.055990219116211
Validation loss: 2.8784684135067846

Epoch: 5| Step: 2
Training loss: 2.4957809448242188
Validation loss: 2.8775925431200253

Epoch: 5| Step: 3
Training loss: 2.941988706588745
Validation loss: 2.871801007178522

Epoch: 5| Step: 4
Training loss: 3.2885775566101074
Validation loss: 2.87059752146403

Epoch: 5| Step: 5
Training loss: 2.806222438812256
Validation loss: 2.8706509067166235

Epoch: 5| Step: 6
Training loss: 2.978700876235962
Validation loss: 2.8730239099071873

Epoch: 5| Step: 7
Training loss: 3.2629077434539795
Validation loss: 2.871184056805026

Epoch: 5| Step: 8
Training loss: 3.047266721725464
Validation loss: 2.867576224829561

Epoch: 5| Step: 9
Training loss: 2.903496265411377
Validation loss: 2.8626665197392946

Epoch: 5| Step: 10
Training loss: 2.6174144744873047
Validation loss: 2.8636777759880148

Epoch: 20| Step: 0
Training loss: 3.52790904045105
Validation loss: 2.8621608928967546

Epoch: 5| Step: 1
Training loss: 2.5330381393432617
Validation loss: 2.860236898545296

Epoch: 5| Step: 2
Training loss: 2.522099018096924
Validation loss: 2.859992111882856

Epoch: 5| Step: 3
Training loss: 3.5933094024658203
Validation loss: 2.85357557060898

Epoch: 5| Step: 4
Training loss: 1.771393060684204
Validation loss: 2.853857901788527

Epoch: 5| Step: 5
Training loss: 3.1482245922088623
Validation loss: 2.8476768821798344

Epoch: 5| Step: 6
Training loss: 3.4071857929229736
Validation loss: 2.8515900258095033

Epoch: 5| Step: 7
Training loss: 2.9698455333709717
Validation loss: 2.8485371348678425

Epoch: 5| Step: 8
Training loss: 2.9309277534484863
Validation loss: 2.849973142787974

Epoch: 5| Step: 9
Training loss: 3.628681182861328
Validation loss: 2.8502316090368454

Epoch: 5| Step: 10
Training loss: 2.4594547748565674
Validation loss: 2.8448653631312872

Epoch: 21| Step: 0
Training loss: 2.8274731636047363
Validation loss: 2.8418046915402977

Epoch: 5| Step: 1
Training loss: 4.240233898162842
Validation loss: 2.8386290278486026

Epoch: 5| Step: 2
Training loss: 3.0296378135681152
Validation loss: 2.8350910499531734

Epoch: 5| Step: 3
Training loss: 3.8235626220703125
Validation loss: 2.836178313019455

Epoch: 5| Step: 4
Training loss: 3.1270484924316406
Validation loss: 2.835042994509461

Epoch: 5| Step: 5
Training loss: 2.599137783050537
Validation loss: 2.832973687879501

Epoch: 5| Step: 6
Training loss: 3.0859146118164062
Validation loss: 2.8329686144346833

Epoch: 5| Step: 7
Training loss: 2.571648359298706
Validation loss: 2.832864769043461

Epoch: 5| Step: 8
Training loss: 2.773644208908081
Validation loss: 2.825976679402013

Epoch: 5| Step: 9
Training loss: 1.911635398864746
Validation loss: 2.8265737769424275

Epoch: 5| Step: 10
Training loss: 2.3869106769561768
Validation loss: 2.822176610269854

Epoch: 22| Step: 0
Training loss: 3.1357429027557373
Validation loss: 2.826234135576474

Epoch: 5| Step: 1
Training loss: 2.936269760131836
Validation loss: 2.8238079496609267

Epoch: 5| Step: 2
Training loss: 3.4840705394744873
Validation loss: 2.8222132857127855

Epoch: 5| Step: 3
Training loss: 2.8843538761138916
Validation loss: 2.8220918716922885

Epoch: 5| Step: 4
Training loss: 2.381645679473877
Validation loss: 2.8238258720726095

Epoch: 5| Step: 5
Training loss: 3.1747374534606934
Validation loss: 2.820004601632395

Epoch: 5| Step: 6
Training loss: 2.6429505348205566
Validation loss: 2.8172386128415345

Epoch: 5| Step: 7
Training loss: 3.0113818645477295
Validation loss: 2.816071661569739

Epoch: 5| Step: 8
Training loss: 3.210134983062744
Validation loss: 2.8137322753988285

Epoch: 5| Step: 9
Training loss: 2.6286673545837402
Validation loss: 2.8125846206500964

Epoch: 5| Step: 10
Training loss: 2.8626532554626465
Validation loss: 2.8134171193645847

Epoch: 23| Step: 0
Training loss: 3.3646628856658936
Validation loss: 2.8265764508196103

Epoch: 5| Step: 1
Training loss: 3.5982375144958496
Validation loss: 2.8249442679907686

Epoch: 5| Step: 2
Training loss: 3.4531776905059814
Validation loss: 2.8146548142997165

Epoch: 5| Step: 3
Training loss: 2.232489585876465
Validation loss: 2.812003017753683

Epoch: 5| Step: 4
Training loss: 2.80285382270813
Validation loss: 2.807389097829019

Epoch: 5| Step: 5
Training loss: 2.6976497173309326
Validation loss: 2.803325714603547

Epoch: 5| Step: 6
Training loss: 2.5056710243225098
Validation loss: 2.803141865679013

Epoch: 5| Step: 7
Training loss: 3.053373336791992
Validation loss: 2.806449738881921

Epoch: 5| Step: 8
Training loss: 2.3000831604003906
Validation loss: 2.801973686423353

Epoch: 5| Step: 9
Training loss: 2.6081433296203613
Validation loss: 2.8012070258458457

Epoch: 5| Step: 10
Training loss: 3.760991096496582
Validation loss: 2.7997327261073615

Epoch: 24| Step: 0
Training loss: 3.311427354812622
Validation loss: 2.79601219648956

Epoch: 5| Step: 1
Training loss: 2.9794440269470215
Validation loss: 2.797875068520987

Epoch: 5| Step: 2
Training loss: 3.1257171630859375
Validation loss: 2.7955846427589335

Epoch: 5| Step: 3
Training loss: 2.3690898418426514
Validation loss: 2.7942189375559487

Epoch: 5| Step: 4
Training loss: 3.1900596618652344
Validation loss: 2.7956964661998134

Epoch: 5| Step: 5
Training loss: 2.620344400405884
Validation loss: 2.7911302992092666

Epoch: 5| Step: 6
Training loss: 3.1500678062438965
Validation loss: 2.7922335440112698

Epoch: 5| Step: 7
Training loss: 3.5459442138671875
Validation loss: 2.793819678727017

Epoch: 5| Step: 8
Training loss: 2.253875732421875
Validation loss: 2.7945652546421176

Epoch: 5| Step: 9
Training loss: 2.8614916801452637
Validation loss: 2.7884025932640157

Epoch: 5| Step: 10
Training loss: 2.6653566360473633
Validation loss: 2.7863639016305246

Epoch: 25| Step: 0
Training loss: 2.5481441020965576
Validation loss: 2.7828480018082487

Epoch: 5| Step: 1
Training loss: 3.21527099609375
Validation loss: 2.7802730350084204

Epoch: 5| Step: 2
Training loss: 2.936239719390869
Validation loss: 2.78181702603576

Epoch: 5| Step: 3
Training loss: 3.030071973800659
Validation loss: 2.7799659441876154

Epoch: 5| Step: 4
Training loss: 3.2440154552459717
Validation loss: 2.776411187264227

Epoch: 5| Step: 5
Training loss: 3.4865074157714844
Validation loss: 2.776676813761393

Epoch: 5| Step: 6
Training loss: 1.9938427209854126
Validation loss: 2.775015233665384

Epoch: 5| Step: 7
Training loss: 2.408773422241211
Validation loss: 2.7740350359229633

Epoch: 5| Step: 8
Training loss: 2.5553293228149414
Validation loss: 2.7740754106993317

Epoch: 5| Step: 9
Training loss: 3.503121852874756
Validation loss: 2.773398337825652

Epoch: 5| Step: 10
Training loss: 3.162909984588623
Validation loss: 2.7703720164555374

Epoch: 26| Step: 0
Training loss: 2.094365358352661
Validation loss: 2.7693231682623587

Epoch: 5| Step: 1
Training loss: 3.7022831439971924
Validation loss: 2.7716067298766105

Epoch: 5| Step: 2
Training loss: 2.7732138633728027
Validation loss: 2.768770358895743

Epoch: 5| Step: 3
Training loss: 3.271369457244873
Validation loss: 2.7664057952101513

Epoch: 5| Step: 4
Training loss: 2.9134509563446045
Validation loss: 2.7673470973968506

Epoch: 5| Step: 5
Training loss: 2.469921350479126
Validation loss: 2.7681050454416583

Epoch: 5| Step: 6
Training loss: 3.22047758102417
Validation loss: 2.7691973460617887

Epoch: 5| Step: 7
Training loss: 3.1042215824127197
Validation loss: 2.7666167418162027

Epoch: 5| Step: 8
Training loss: 2.2145538330078125
Validation loss: 2.7610142102805515

Epoch: 5| Step: 9
Training loss: 3.138341188430786
Validation loss: 2.7586885934234946

Epoch: 5| Step: 10
Training loss: 3.1315317153930664
Validation loss: 2.758993338513118

Epoch: 27| Step: 0
Training loss: 3.6898193359375
Validation loss: 2.7595874878668014

Epoch: 5| Step: 1
Training loss: 3.2663040161132812
Validation loss: 2.7570971314625075

Epoch: 5| Step: 2
Training loss: 2.5047097206115723
Validation loss: 2.757158443491946

Epoch: 5| Step: 3
Training loss: 2.6592047214508057
Validation loss: 2.7574078062529206

Epoch: 5| Step: 4
Training loss: 2.282149076461792
Validation loss: 2.7554944279373332

Epoch: 5| Step: 5
Training loss: 3.6407437324523926
Validation loss: 2.7543288430859967

Epoch: 5| Step: 6
Training loss: 2.725308895111084
Validation loss: 2.7526019619357203

Epoch: 5| Step: 7
Training loss: 2.8697173595428467
Validation loss: 2.7522972476097847

Epoch: 5| Step: 8
Training loss: 2.8307242393493652
Validation loss: 2.752182334981939

Epoch: 5| Step: 9
Training loss: 2.2417428493499756
Validation loss: 2.749484501859193

Epoch: 5| Step: 10
Training loss: 3.2664058208465576
Validation loss: 2.7497728511851323

Epoch: 28| Step: 0
Training loss: 3.361375331878662
Validation loss: 2.747940391622564

Epoch: 5| Step: 1
Training loss: 2.9271328449249268
Validation loss: 2.751305003320017

Epoch: 5| Step: 2
Training loss: 3.144912004470825
Validation loss: 2.751331190909109

Epoch: 5| Step: 3
Training loss: 2.43986439704895
Validation loss: 2.752276030919885

Epoch: 5| Step: 4
Training loss: 2.928067445755005
Validation loss: 2.7527907561230403

Epoch: 5| Step: 5
Training loss: 2.5613999366760254
Validation loss: 2.7552569399597826

Epoch: 5| Step: 6
Training loss: 2.5157084465026855
Validation loss: 2.7566091809221493

Epoch: 5| Step: 7
Training loss: 3.7150962352752686
Validation loss: 2.745740008610551

Epoch: 5| Step: 8
Training loss: 2.9105937480926514
Validation loss: 2.7424181815116637

Epoch: 5| Step: 9
Training loss: 2.632474899291992
Validation loss: 2.7405491131608204

Epoch: 5| Step: 10
Training loss: 2.662830114364624
Validation loss: 2.740649777074014

Epoch: 29| Step: 0
Training loss: 2.9094607830047607
Validation loss: 2.7401835328789166

Epoch: 5| Step: 1
Training loss: 3.188076972961426
Validation loss: 2.7411140934113534

Epoch: 5| Step: 2
Training loss: 3.3646323680877686
Validation loss: 2.7380514657625588

Epoch: 5| Step: 3
Training loss: 3.076961040496826
Validation loss: 2.73651292503521

Epoch: 5| Step: 4
Training loss: 3.403791904449463
Validation loss: 2.7336395991745817

Epoch: 5| Step: 5
Training loss: 2.9606475830078125
Validation loss: 2.736398740481305

Epoch: 5| Step: 6
Training loss: 2.285676956176758
Validation loss: 2.7327192137318272

Epoch: 5| Step: 7
Training loss: 2.2761757373809814
Validation loss: 2.736526732803673

Epoch: 5| Step: 8
Training loss: 2.8693745136260986
Validation loss: 2.7376429111726823

Epoch: 5| Step: 9
Training loss: 2.927335262298584
Validation loss: 2.7360448273279334

Epoch: 5| Step: 10
Training loss: 2.4614505767822266
Validation loss: 2.73709342556615

Epoch: 30| Step: 0
Training loss: 2.930131196975708
Validation loss: 2.740722281958467

Epoch: 5| Step: 1
Training loss: 3.029136896133423
Validation loss: 2.7354819697718464

Epoch: 5| Step: 2
Training loss: 2.6270878314971924
Validation loss: 2.7302242812289985

Epoch: 5| Step: 3
Training loss: 3.2212772369384766
Validation loss: 2.7305614307362545

Epoch: 5| Step: 4
Training loss: 2.602295398712158
Validation loss: 2.729582740414527

Epoch: 5| Step: 5
Training loss: 2.419017791748047
Validation loss: 2.7295012012604745

Epoch: 5| Step: 6
Training loss: 2.2060492038726807
Validation loss: 2.7287374363150647

Epoch: 5| Step: 7
Training loss: 3.6719651222229004
Validation loss: 2.731053965066069

Epoch: 5| Step: 8
Training loss: 2.9024226665496826
Validation loss: 2.726516377541327

Epoch: 5| Step: 9
Training loss: 3.5120162963867188
Validation loss: 2.727310155027656

Epoch: 5| Step: 10
Training loss: 2.5623719692230225
Validation loss: 2.728148732134091

Epoch: 31| Step: 0
Training loss: 2.6370668411254883
Validation loss: 2.7281954006482194

Epoch: 5| Step: 1
Training loss: 2.5051956176757812
Validation loss: 2.7271676986448226

Epoch: 5| Step: 2
Training loss: 3.455104112625122
Validation loss: 2.7227198693060104

Epoch: 5| Step: 3
Training loss: 2.779841899871826
Validation loss: 2.7262798893836235

Epoch: 5| Step: 4
Training loss: 3.2717490196228027
Validation loss: 2.722140806977467

Epoch: 5| Step: 5
Training loss: 3.6092610359191895
Validation loss: 2.7215006018197663

Epoch: 5| Step: 6
Training loss: 2.3999900817871094
Validation loss: 2.7203184609772055

Epoch: 5| Step: 7
Training loss: 2.402048110961914
Validation loss: 2.7199183100013324

Epoch: 5| Step: 8
Training loss: 2.926938772201538
Validation loss: 2.7194144828345186

Epoch: 5| Step: 9
Training loss: 2.487856149673462
Validation loss: 2.7182642106086976

Epoch: 5| Step: 10
Training loss: 3.2690627574920654
Validation loss: 2.719328680346089

Epoch: 32| Step: 0
Training loss: 2.7988879680633545
Validation loss: 2.720698784756404

Epoch: 5| Step: 1
Training loss: 2.980056047439575
Validation loss: 2.7196830575184157

Epoch: 5| Step: 2
Training loss: 2.349815845489502
Validation loss: 2.716269095738729

Epoch: 5| Step: 3
Training loss: 2.8777177333831787
Validation loss: 2.718320467138803

Epoch: 5| Step: 4
Training loss: 3.4223434925079346
Validation loss: 2.7183288681891655

Epoch: 5| Step: 5
Training loss: 2.3665685653686523
Validation loss: 2.7178005736361266

Epoch: 5| Step: 6
Training loss: 2.507918357849121
Validation loss: 2.712981388133059

Epoch: 5| Step: 7
Training loss: 4.026803016662598
Validation loss: 2.7135566434552594

Epoch: 5| Step: 8
Training loss: 2.6522889137268066
Validation loss: 2.7153173492800806

Epoch: 5| Step: 9
Training loss: 2.9279911518096924
Validation loss: 2.7135979744695846

Epoch: 5| Step: 10
Training loss: 2.6549551486968994
Validation loss: 2.7114382610526135

Epoch: 33| Step: 0
Training loss: 3.3664984703063965
Validation loss: 2.710789601008097

Epoch: 5| Step: 1
Training loss: 1.9866106510162354
Validation loss: 2.713391298888832

Epoch: 5| Step: 2
Training loss: 3.0704636573791504
Validation loss: 2.7129938781902356

Epoch: 5| Step: 3
Training loss: 3.5449180603027344
Validation loss: 2.7082041668635544

Epoch: 5| Step: 4
Training loss: 2.777667999267578
Validation loss: 2.7078148575239283

Epoch: 5| Step: 5
Training loss: 2.7254672050476074
Validation loss: 2.7071294938364336

Epoch: 5| Step: 6
Training loss: 2.706571340560913
Validation loss: 2.7055019973426737

Epoch: 5| Step: 7
Training loss: 2.7659265995025635
Validation loss: 2.7120299826386156

Epoch: 5| Step: 8
Training loss: 2.8793537616729736
Validation loss: 2.7099617142831125

Epoch: 5| Step: 9
Training loss: 2.85862398147583
Validation loss: 2.7099948160109983

Epoch: 5| Step: 10
Training loss: 2.876687526702881
Validation loss: 2.7103799748164352

Epoch: 34| Step: 0
Training loss: 3.4093265533447266
Validation loss: 2.7078479336154078

Epoch: 5| Step: 1
Training loss: 2.303276538848877
Validation loss: 2.708089454199678

Epoch: 5| Step: 2
Training loss: 2.839332103729248
Validation loss: 2.7139337037199285

Epoch: 5| Step: 3
Training loss: 1.7931945323944092
Validation loss: 2.7041036390489146

Epoch: 5| Step: 4
Training loss: 3.225142002105713
Validation loss: 2.703724635544644

Epoch: 5| Step: 5
Training loss: 3.1202783584594727
Validation loss: 2.7043671659244004

Epoch: 5| Step: 6
Training loss: 2.5041420459747314
Validation loss: 2.701100057171237

Epoch: 5| Step: 7
Training loss: 3.0649335384368896
Validation loss: 2.7031100027022825

Epoch: 5| Step: 8
Training loss: 3.157322645187378
Validation loss: 2.7051613946114816

Epoch: 5| Step: 9
Training loss: 2.93759822845459
Validation loss: 2.70215206761514

Epoch: 5| Step: 10
Training loss: 3.1417794227600098
Validation loss: 2.6974320719319005

Epoch: 35| Step: 0
Training loss: 3.019256114959717
Validation loss: 2.696990636087233

Epoch: 5| Step: 1
Training loss: 2.559171199798584
Validation loss: 2.6925082181089666

Epoch: 5| Step: 2
Training loss: 2.8780319690704346
Validation loss: 2.698161848129765

Epoch: 5| Step: 3
Training loss: 2.8386282920837402
Validation loss: 2.691757386730563

Epoch: 5| Step: 4
Training loss: 2.729410171508789
Validation loss: 2.691716455644177

Epoch: 5| Step: 5
Training loss: 3.2289040088653564
Validation loss: 2.6923247434759654

Epoch: 5| Step: 6
Training loss: 3.187112808227539
Validation loss: 2.6907076553631852

Epoch: 5| Step: 7
Training loss: 2.483365774154663
Validation loss: 2.691870022845525

Epoch: 5| Step: 8
Training loss: 2.486112117767334
Validation loss: 2.6868513835373746

Epoch: 5| Step: 9
Training loss: 2.821972608566284
Validation loss: 2.6917738017215522

Epoch: 5| Step: 10
Training loss: 3.259336471557617
Validation loss: 2.69063328671199

Epoch: 36| Step: 0
Training loss: 3.0479681491851807
Validation loss: 2.6901479075031896

Epoch: 5| Step: 1
Training loss: 2.4392549991607666
Validation loss: 2.6974543679145073

Epoch: 5| Step: 2
Training loss: 2.918287754058838
Validation loss: 2.700529054928851

Epoch: 5| Step: 3
Training loss: 2.7496862411499023
Validation loss: 2.709291817039572

Epoch: 5| Step: 4
Training loss: 3.3079781532287598
Validation loss: 2.7226167212250414

Epoch: 5| Step: 5
Training loss: 2.7392303943634033
Validation loss: 2.716385074841079

Epoch: 5| Step: 6
Training loss: 2.360062599182129
Validation loss: 2.710136593029063

Epoch: 5| Step: 7
Training loss: 2.9909393787384033
Validation loss: 2.6934042284565587

Epoch: 5| Step: 8
Training loss: 2.924943208694458
Validation loss: 2.6864254295185046

Epoch: 5| Step: 9
Training loss: 3.0319361686706543
Validation loss: 2.6828994238248436

Epoch: 5| Step: 10
Training loss: 2.9042093753814697
Validation loss: 2.688401201719879

Epoch: 37| Step: 0
Training loss: 3.889829635620117
Validation loss: 2.685018265119163

Epoch: 5| Step: 1
Training loss: 3.261594295501709
Validation loss: 2.6852801076827513

Epoch: 5| Step: 2
Training loss: 2.388840913772583
Validation loss: 2.683600494938512

Epoch: 5| Step: 3
Training loss: 2.473891019821167
Validation loss: 2.6820855755959787

Epoch: 5| Step: 4
Training loss: 2.5044729709625244
Validation loss: 2.679200332651856

Epoch: 5| Step: 5
Training loss: 3.095777750015259
Validation loss: 2.679043521163284

Epoch: 5| Step: 6
Training loss: 2.7763705253601074
Validation loss: 2.68282123022182

Epoch: 5| Step: 7
Training loss: 2.526682138442993
Validation loss: 2.681772106437273

Epoch: 5| Step: 8
Training loss: 3.1020824909210205
Validation loss: 2.6783328569063576

Epoch: 5| Step: 9
Training loss: 2.6817736625671387
Validation loss: 2.6772037577885452

Epoch: 5| Step: 10
Training loss: 2.6006555557250977
Validation loss: 2.681666453679403

Epoch: 38| Step: 0
Training loss: 3.809169292449951
Validation loss: 2.6792967114397275

Epoch: 5| Step: 1
Training loss: 2.341923236846924
Validation loss: 2.6833403213049776

Epoch: 5| Step: 2
Training loss: 2.2023141384124756
Validation loss: 2.682580014710785

Epoch: 5| Step: 3
Training loss: 3.046187162399292
Validation loss: 2.6782947663337953

Epoch: 5| Step: 4
Training loss: 2.57680082321167
Validation loss: 2.6747226381814606

Epoch: 5| Step: 5
Training loss: 3.292343854904175
Validation loss: 2.6746150088566605

Epoch: 5| Step: 6
Training loss: 3.3478667736053467
Validation loss: 2.67244916577493

Epoch: 5| Step: 7
Training loss: 2.8373379707336426
Validation loss: 2.672281821568807

Epoch: 5| Step: 8
Training loss: 2.527339220046997
Validation loss: 2.67517118043797

Epoch: 5| Step: 9
Training loss: 2.9032516479492188
Validation loss: 2.6730886428586897

Epoch: 5| Step: 10
Training loss: 2.3030898571014404
Validation loss: 2.675160805384318

Epoch: 39| Step: 0
Training loss: 2.366908550262451
Validation loss: 2.6719167411968274

Epoch: 5| Step: 1
Training loss: 2.8513646125793457
Validation loss: 2.678316080442039

Epoch: 5| Step: 2
Training loss: 2.3537728786468506
Validation loss: 2.676231604750438

Epoch: 5| Step: 3
Training loss: 2.68315052986145
Validation loss: 2.680720211357199

Epoch: 5| Step: 4
Training loss: 2.580967664718628
Validation loss: 2.6767878327318417

Epoch: 5| Step: 5
Training loss: 3.114488124847412
Validation loss: 2.6791297543433403

Epoch: 5| Step: 6
Training loss: 3.0198657512664795
Validation loss: 2.6838880200539865

Epoch: 5| Step: 7
Training loss: 3.6920933723449707
Validation loss: 2.692298286704607

Epoch: 5| Step: 8
Training loss: 2.6033072471618652
Validation loss: 2.6873539288838706

Epoch: 5| Step: 9
Training loss: 2.997868061065674
Validation loss: 2.676099802858086

Epoch: 5| Step: 10
Training loss: 2.9522595405578613
Validation loss: 2.673844393863473

Epoch: 40| Step: 0
Training loss: 2.166883945465088
Validation loss: 2.670056858370381

Epoch: 5| Step: 1
Training loss: 2.911494493484497
Validation loss: 2.6693000639638593

Epoch: 5| Step: 2
Training loss: 2.6090872287750244
Validation loss: 2.6654998461405435

Epoch: 5| Step: 3
Training loss: 2.7407140731811523
Validation loss: 2.6631263481673373

Epoch: 5| Step: 4
Training loss: 2.277479410171509
Validation loss: 2.662597179412842

Epoch: 5| Step: 5
Training loss: 2.163102626800537
Validation loss: 2.664738339762534

Epoch: 5| Step: 6
Training loss: 2.594003200531006
Validation loss: 2.665930619803808

Epoch: 5| Step: 7
Training loss: 3.5596556663513184
Validation loss: 2.6652580307376

Epoch: 5| Step: 8
Training loss: 3.2269020080566406
Validation loss: 2.6653996103553363

Epoch: 5| Step: 9
Training loss: 3.6299221515655518
Validation loss: 2.6656906553494033

Epoch: 5| Step: 10
Training loss: 3.3408544063568115
Validation loss: 2.661377686326222

Epoch: 41| Step: 0
Training loss: 3.3030288219451904
Validation loss: 2.6619954263010333

Epoch: 5| Step: 1
Training loss: 3.2392146587371826
Validation loss: 2.6643707598409345

Epoch: 5| Step: 2
Training loss: 3.0437774658203125
Validation loss: 2.667281466145669

Epoch: 5| Step: 3
Training loss: 2.184577226638794
Validation loss: 2.674878804914413

Epoch: 5| Step: 4
Training loss: 2.5080435276031494
Validation loss: 2.6643645071214244

Epoch: 5| Step: 5
Training loss: 2.8605427742004395
Validation loss: 2.6667241383624334

Epoch: 5| Step: 6
Training loss: 3.052943706512451
Validation loss: 2.663956936969552

Epoch: 5| Step: 7
Training loss: 2.9319591522216797
Validation loss: 2.6619550976701962

Epoch: 5| Step: 8
Training loss: 2.050429105758667
Validation loss: 2.663325942972655

Epoch: 5| Step: 9
Training loss: 3.2173633575439453
Validation loss: 2.663278420766195

Epoch: 5| Step: 10
Training loss: 2.674497604370117
Validation loss: 2.659149900559456

Epoch: 42| Step: 0
Training loss: 2.6875510215759277
Validation loss: 2.661275394501225

Epoch: 5| Step: 1
Training loss: 3.270646572113037
Validation loss: 2.6607558957992063

Epoch: 5| Step: 2
Training loss: 3.338322401046753
Validation loss: 2.661694239544612

Epoch: 5| Step: 3
Training loss: 2.150155782699585
Validation loss: 2.661952180247153

Epoch: 5| Step: 4
Training loss: 2.6743340492248535
Validation loss: 2.6605716495103735

Epoch: 5| Step: 5
Training loss: 2.9753940105438232
Validation loss: 2.655799904177266

Epoch: 5| Step: 6
Training loss: 2.829615831375122
Validation loss: 2.6541031252953315

Epoch: 5| Step: 7
Training loss: 2.40106201171875
Validation loss: 2.660472833982078

Epoch: 5| Step: 8
Training loss: 2.739443778991699
Validation loss: 2.6585809799932663

Epoch: 5| Step: 9
Training loss: 2.987536907196045
Validation loss: 2.655835013235769

Epoch: 5| Step: 10
Training loss: 3.0279910564422607
Validation loss: 2.6610013528536727

Epoch: 43| Step: 0
Training loss: 2.7386655807495117
Validation loss: 2.6737897703724522

Epoch: 5| Step: 1
Training loss: 2.7101681232452393
Validation loss: 2.6671520920209986

Epoch: 5| Step: 2
Training loss: 2.839489459991455
Validation loss: 2.661150150401618

Epoch: 5| Step: 3
Training loss: 3.1216368675231934
Validation loss: 2.6595272120609077

Epoch: 5| Step: 4
Training loss: 2.7317299842834473
Validation loss: 2.656380318826245

Epoch: 5| Step: 5
Training loss: 3.3104987144470215
Validation loss: 2.6512605913223757

Epoch: 5| Step: 6
Training loss: 2.8237860202789307
Validation loss: 2.657376725186584

Epoch: 5| Step: 7
Training loss: 2.4603028297424316
Validation loss: 2.6555118124972106

Epoch: 5| Step: 8
Training loss: 2.7854042053222656
Validation loss: 2.6558498926060174

Epoch: 5| Step: 9
Training loss: 2.7444095611572266
Validation loss: 2.652096838079473

Epoch: 5| Step: 10
Training loss: 2.7606208324432373
Validation loss: 2.655609160341242

Epoch: 44| Step: 0
Training loss: 2.4823012351989746
Validation loss: 2.653650665795931

Epoch: 5| Step: 1
Training loss: 2.438894510269165
Validation loss: 2.6630970521639754

Epoch: 5| Step: 2
Training loss: 3.9467880725860596
Validation loss: 2.6630724655684603

Epoch: 5| Step: 3
Training loss: 2.860076904296875
Validation loss: 2.6641627870580202

Epoch: 5| Step: 4
Training loss: 2.2495503425598145
Validation loss: 2.6582272693675053

Epoch: 5| Step: 5
Training loss: 3.3748600482940674
Validation loss: 2.652259631823468

Epoch: 5| Step: 6
Training loss: 2.8997621536254883
Validation loss: 2.6454691117809666

Epoch: 5| Step: 7
Training loss: 3.304025173187256
Validation loss: 2.6498784467738163

Epoch: 5| Step: 8
Training loss: 2.2981162071228027
Validation loss: 2.6497938530419463

Epoch: 5| Step: 9
Training loss: 2.164334535598755
Validation loss: 2.6479261177842335

Epoch: 5| Step: 10
Training loss: 2.912762403488159
Validation loss: 2.6491536350660425

Epoch: 45| Step: 0
Training loss: 2.754868984222412
Validation loss: 2.648180882136027

Epoch: 5| Step: 1
Training loss: 2.666398525238037
Validation loss: 2.6520991838106545

Epoch: 5| Step: 2
Training loss: 3.0733065605163574
Validation loss: 2.655761908459407

Epoch: 5| Step: 3
Training loss: 2.5525641441345215
Validation loss: 2.654273627906717

Epoch: 5| Step: 4
Training loss: 3.4405550956726074
Validation loss: 2.6515675565247894

Epoch: 5| Step: 5
Training loss: 3.0035548210144043
Validation loss: 2.6496540218271236

Epoch: 5| Step: 6
Training loss: 2.2034573554992676
Validation loss: 2.650796676194796

Epoch: 5| Step: 7
Training loss: 2.9793684482574463
Validation loss: 2.645899672662058

Epoch: 5| Step: 8
Training loss: 3.0455894470214844
Validation loss: 2.6431520421017884

Epoch: 5| Step: 9
Training loss: 2.737597703933716
Validation loss: 2.645942393169608

Epoch: 5| Step: 10
Training loss: 2.432955741882324
Validation loss: 2.6449250482743785

Epoch: 46| Step: 0
Training loss: 3.6601951122283936
Validation loss: 2.650026265011039

Epoch: 5| Step: 1
Training loss: 2.94085693359375
Validation loss: 2.6480177551187496

Epoch: 5| Step: 2
Training loss: 2.081591844558716
Validation loss: 2.6454568191241195

Epoch: 5| Step: 3
Training loss: 3.3724396228790283
Validation loss: 2.6463024975151144

Epoch: 5| Step: 4
Training loss: 3.801280975341797
Validation loss: 2.6480725285827473

Epoch: 5| Step: 5
Training loss: 2.5385336875915527
Validation loss: 2.6412066951874764

Epoch: 5| Step: 6
Training loss: 2.2173094749450684
Validation loss: 2.642310122007965

Epoch: 5| Step: 7
Training loss: 2.6401994228363037
Validation loss: 2.642401426069198

Epoch: 5| Step: 8
Training loss: 1.941490888595581
Validation loss: 2.643042069609447

Epoch: 5| Step: 9
Training loss: 3.1159770488739014
Validation loss: 2.6485474622377785

Epoch: 5| Step: 10
Training loss: 2.606567144393921
Validation loss: 2.6567526735285276

Epoch: 47| Step: 0
Training loss: 2.14974045753479
Validation loss: 2.6516818077333513

Epoch: 5| Step: 1
Training loss: 2.9635729789733887
Validation loss: 2.6519633646934264

Epoch: 5| Step: 2
Training loss: 3.452524185180664
Validation loss: 2.6566686860976683

Epoch: 5| Step: 3
Training loss: 3.0075905323028564
Validation loss: 2.6556341955738683

Epoch: 5| Step: 4
Training loss: 2.71803617477417
Validation loss: 2.652487924022059

Epoch: 5| Step: 5
Training loss: 2.945033550262451
Validation loss: 2.6485693095832743

Epoch: 5| Step: 6
Training loss: 2.9311537742614746
Validation loss: 2.640701545182095

Epoch: 5| Step: 7
Training loss: 3.0455384254455566
Validation loss: 2.6408969125440045

Epoch: 5| Step: 8
Training loss: 2.039379835128784
Validation loss: 2.64187930732645

Epoch: 5| Step: 9
Training loss: 2.631484031677246
Validation loss: 2.6392922042518534

Epoch: 5| Step: 10
Training loss: 3.054593563079834
Validation loss: 2.6430616199329333

Epoch: 48| Step: 0
Training loss: 2.1756749153137207
Validation loss: 2.646306965940742

Epoch: 5| Step: 1
Training loss: 2.8878233432769775
Validation loss: 2.6445267815743723

Epoch: 5| Step: 2
Training loss: 2.664155960083008
Validation loss: 2.642917956075361

Epoch: 5| Step: 3
Training loss: 2.6279797554016113
Validation loss: 2.6403820668497393

Epoch: 5| Step: 4
Training loss: 2.929351329803467
Validation loss: 2.638867416689473

Epoch: 5| Step: 5
Training loss: 2.5176713466644287
Validation loss: 2.63876945741715

Epoch: 5| Step: 6
Training loss: 3.1922645568847656
Validation loss: 2.6393708208555817

Epoch: 5| Step: 7
Training loss: 3.210369110107422
Validation loss: 2.640921108184322

Epoch: 5| Step: 8
Training loss: 2.792788028717041
Validation loss: 2.639639513466948

Epoch: 5| Step: 9
Training loss: 3.341130495071411
Validation loss: 2.638359744061706

Epoch: 5| Step: 10
Training loss: 2.488431215286255
Validation loss: 2.6410705427969656

Epoch: 49| Step: 0
Training loss: 2.272052764892578
Validation loss: 2.638972597737466

Epoch: 5| Step: 1
Training loss: 2.9399051666259766
Validation loss: 2.6395470352583033

Epoch: 5| Step: 2
Training loss: 3.1338067054748535
Validation loss: 2.6365898347670034

Epoch: 5| Step: 3
Training loss: 2.519746780395508
Validation loss: 2.6344883954653175

Epoch: 5| Step: 4
Training loss: 3.563577175140381
Validation loss: 2.6376982555594495

Epoch: 5| Step: 5
Training loss: 3.129633665084839
Validation loss: 2.635766726668163

Epoch: 5| Step: 6
Training loss: 2.953477621078491
Validation loss: 2.636598192235475

Epoch: 5| Step: 7
Training loss: 2.437824249267578
Validation loss: 2.641951730174403

Epoch: 5| Step: 8
Training loss: 2.985403299331665
Validation loss: 2.6399913936532955

Epoch: 5| Step: 9
Training loss: 2.6646130084991455
Validation loss: 2.634324289137317

Epoch: 5| Step: 10
Training loss: 2.0707483291625977
Validation loss: 2.637325345828969

Epoch: 50| Step: 0
Training loss: 2.99894380569458
Validation loss: 2.637439038163872

Epoch: 5| Step: 1
Training loss: 2.3087639808654785
Validation loss: 2.6347677169307584

Epoch: 5| Step: 2
Training loss: 2.6966331005096436
Validation loss: 2.637107951666719

Epoch: 5| Step: 3
Training loss: 2.4803385734558105
Validation loss: 2.6359790884038454

Epoch: 5| Step: 4
Training loss: 3.200200319290161
Validation loss: 2.635567475390691

Epoch: 5| Step: 5
Training loss: 2.2845616340637207
Validation loss: 2.6370819742961595

Epoch: 5| Step: 6
Training loss: 3.587390184402466
Validation loss: 2.6565017264376403

Epoch: 5| Step: 7
Training loss: 2.4475958347320557
Validation loss: 2.6894281115583194

Epoch: 5| Step: 8
Training loss: 3.445793628692627
Validation loss: 2.6963378203812467

Epoch: 5| Step: 9
Training loss: 2.4283242225646973
Validation loss: 2.677475844660113

Epoch: 5| Step: 10
Training loss: 3.1467597484588623
Validation loss: 2.6566602491563365

Epoch: 51| Step: 0
Training loss: 2.966860771179199
Validation loss: 2.648404952018492

Epoch: 5| Step: 1
Training loss: 2.990483283996582
Validation loss: 2.6346664864529847

Epoch: 5| Step: 2
Training loss: 2.780344009399414
Validation loss: 2.6345623539340113

Epoch: 5| Step: 3
Training loss: 3.021411895751953
Validation loss: 2.63359768672656

Epoch: 5| Step: 4
Training loss: 2.7952847480773926
Validation loss: 2.6324554617686937

Epoch: 5| Step: 5
Training loss: 2.8405234813690186
Validation loss: 2.6315011542330504

Epoch: 5| Step: 6
Training loss: 2.513737201690674
Validation loss: 2.634040824828609

Epoch: 5| Step: 7
Training loss: 2.696949005126953
Validation loss: 2.6286073141200568

Epoch: 5| Step: 8
Training loss: 2.9040980339050293
Validation loss: 2.629767148725448

Epoch: 5| Step: 9
Training loss: 2.7847301959991455
Validation loss: 2.6308027390510804

Epoch: 5| Step: 10
Training loss: 2.3342103958129883
Validation loss: 2.6308525916068786

Epoch: 52| Step: 0
Training loss: 3.7129623889923096
Validation loss: 2.6330379209210797

Epoch: 5| Step: 1
Training loss: 2.962923526763916
Validation loss: 2.6294467936279955

Epoch: 5| Step: 2
Training loss: 2.3265254497528076
Validation loss: 2.63215111917065

Epoch: 5| Step: 3
Training loss: 3.0274126529693604
Validation loss: 2.63378579385819

Epoch: 5| Step: 4
Training loss: 2.7773215770721436
Validation loss: 2.633357588962842

Epoch: 5| Step: 5
Training loss: 2.5801761150360107
Validation loss: 2.6391742485825733

Epoch: 5| Step: 6
Training loss: 2.748439073562622
Validation loss: 2.640593813311669

Epoch: 5| Step: 7
Training loss: 2.8782174587249756
Validation loss: 2.6448766672483055

Epoch: 5| Step: 8
Training loss: 3.1574718952178955
Validation loss: 2.64173476670378

Epoch: 5| Step: 9
Training loss: 2.029209613800049
Validation loss: 2.641795676241639

Epoch: 5| Step: 10
Training loss: 2.4565505981445312
Validation loss: 2.6319699415596585

Epoch: 53| Step: 0
Training loss: 2.740070343017578
Validation loss: 2.630496701886577

Epoch: 5| Step: 1
Training loss: 3.129829168319702
Validation loss: 2.626117355080061

Epoch: 5| Step: 2
Training loss: 2.6366240978240967
Validation loss: 2.630862807714811

Epoch: 5| Step: 3
Training loss: 2.666311740875244
Validation loss: 2.628152685780679

Epoch: 5| Step: 4
Training loss: 3.61358380317688
Validation loss: 2.633548846808813

Epoch: 5| Step: 5
Training loss: 2.882112503051758
Validation loss: 2.632447970810757

Epoch: 5| Step: 6
Training loss: 2.451789379119873
Validation loss: 2.632602745486844

Epoch: 5| Step: 7
Training loss: 2.5800681114196777
Validation loss: 2.625087066363263

Epoch: 5| Step: 8
Training loss: 1.8554723262786865
Validation loss: 2.6279630737919963

Epoch: 5| Step: 9
Training loss: 2.5901665687561035
Validation loss: 2.6269435395476637

Epoch: 5| Step: 10
Training loss: 3.7133569717407227
Validation loss: 2.631583777807092

Epoch: 54| Step: 0
Training loss: 2.2296128273010254
Validation loss: 2.6373476648843415

Epoch: 5| Step: 1
Training loss: 2.293056011199951
Validation loss: 2.6416450469724593

Epoch: 5| Step: 2
Training loss: 3.821357011795044
Validation loss: 2.6497062995869625

Epoch: 5| Step: 3
Training loss: 2.2772374153137207
Validation loss: 2.6479983868137484

Epoch: 5| Step: 4
Training loss: 3.305873394012451
Validation loss: 2.654942709912536

Epoch: 5| Step: 5
Training loss: 2.773300886154175
Validation loss: 2.654506278294389

Epoch: 5| Step: 6
Training loss: 3.1844916343688965
Validation loss: 2.635142857028592

Epoch: 5| Step: 7
Training loss: 2.110771656036377
Validation loss: 2.631810188293457

Epoch: 5| Step: 8
Training loss: 3.471614122390747
Validation loss: 2.625836031411284

Epoch: 5| Step: 9
Training loss: 2.927753210067749
Validation loss: 2.6243048790962464

Epoch: 5| Step: 10
Training loss: 2.2343292236328125
Validation loss: 2.621464603690691

Epoch: 55| Step: 0
Training loss: 2.3643205165863037
Validation loss: 2.621086879443097

Epoch: 5| Step: 1
Training loss: 2.6660923957824707
Validation loss: 2.6252825260162354

Epoch: 5| Step: 2
Training loss: 2.6404824256896973
Validation loss: 2.6247779271935903

Epoch: 5| Step: 3
Training loss: 3.604979991912842
Validation loss: 2.622207462146718

Epoch: 5| Step: 4
Training loss: 2.590841054916382
Validation loss: 2.627698390714584

Epoch: 5| Step: 5
Training loss: 2.678189516067505
Validation loss: 2.620997246875558

Epoch: 5| Step: 6
Training loss: 3.352508544921875
Validation loss: 2.6184714378849154

Epoch: 5| Step: 7
Training loss: 3.098102569580078
Validation loss: 2.623075162210772

Epoch: 5| Step: 8
Training loss: 2.671360492706299
Validation loss: 2.624498536509852

Epoch: 5| Step: 9
Training loss: 2.173550844192505
Validation loss: 2.6240246783020678

Epoch: 5| Step: 10
Training loss: 2.8098702430725098
Validation loss: 2.625152995509486

Epoch: 56| Step: 0
Training loss: 3.4918296337127686
Validation loss: 2.6333423301737797

Epoch: 5| Step: 1
Training loss: 2.6631665229797363
Validation loss: 2.6371990173093733

Epoch: 5| Step: 2
Training loss: 3.2804551124572754
Validation loss: 2.630522304965604

Epoch: 5| Step: 3
Training loss: 2.2868194580078125
Validation loss: 2.6293264922275337

Epoch: 5| Step: 4
Training loss: 2.9075400829315186
Validation loss: 2.621283103060979

Epoch: 5| Step: 5
Training loss: 2.503248929977417
Validation loss: 2.618811627869965

Epoch: 5| Step: 6
Training loss: 2.9940738677978516
Validation loss: 2.6201464642760572

Epoch: 5| Step: 7
Training loss: 3.004225254058838
Validation loss: 2.617086249013101

Epoch: 5| Step: 8
Training loss: 2.7319939136505127
Validation loss: 2.6197754900942565

Epoch: 5| Step: 9
Training loss: 2.2758469581604004
Validation loss: 2.617901982799653

Epoch: 5| Step: 10
Training loss: 2.423839569091797
Validation loss: 2.6188040548755276

Epoch: 57| Step: 0
Training loss: 3.0031745433807373
Validation loss: 2.618177129376319

Epoch: 5| Step: 1
Training loss: 2.9176697731018066
Validation loss: 2.6187850582984185

Epoch: 5| Step: 2
Training loss: 1.9321272373199463
Validation loss: 2.6173951036186627

Epoch: 5| Step: 3
Training loss: 1.9692100286483765
Validation loss: 2.620628897861768

Epoch: 5| Step: 4
Training loss: 2.4395973682403564
Validation loss: 2.620363727692635

Epoch: 5| Step: 5
Training loss: 2.7834439277648926
Validation loss: 2.6220087953793105

Epoch: 5| Step: 6
Training loss: 3.9510626792907715
Validation loss: 2.619446905710364

Epoch: 5| Step: 7
Training loss: 2.76423716545105
Validation loss: 2.623495796675323

Epoch: 5| Step: 8
Training loss: 3.780139446258545
Validation loss: 2.6256491086816274

Epoch: 5| Step: 9
Training loss: 2.675762891769409
Validation loss: 2.625205114323606

Epoch: 5| Step: 10
Training loss: 2.259672164916992
Validation loss: 2.622104396102249

Epoch: 58| Step: 0
Training loss: 2.831176280975342
Validation loss: 2.61964742342631

Epoch: 5| Step: 1
Training loss: 3.273283004760742
Validation loss: 2.6143356715479205

Epoch: 5| Step: 2
Training loss: 3.0133304595947266
Validation loss: 2.6170437874332553

Epoch: 5| Step: 3
Training loss: 2.6417860984802246
Validation loss: 2.6143366598313853

Epoch: 5| Step: 4
Training loss: 3.0693583488464355
Validation loss: 2.6123385275563886

Epoch: 5| Step: 5
Training loss: 2.1553940773010254
Validation loss: 2.6161492101607786

Epoch: 5| Step: 6
Training loss: 2.959918260574341
Validation loss: 2.612247318349859

Epoch: 5| Step: 7
Training loss: 2.7077980041503906
Validation loss: 2.6150599807821293

Epoch: 5| Step: 8
Training loss: 2.5880961418151855
Validation loss: 2.615617846929899

Epoch: 5| Step: 9
Training loss: 2.179129123687744
Validation loss: 2.619910301700715

Epoch: 5| Step: 10
Training loss: 3.183786392211914
Validation loss: 2.6175663907040834

Epoch: 59| Step: 0
Training loss: 2.5754916667938232
Validation loss: 2.612988641185145

Epoch: 5| Step: 1
Training loss: 2.0866990089416504
Validation loss: 2.613955815633138

Epoch: 5| Step: 2
Training loss: 2.8717432022094727
Validation loss: 2.6123343667676373

Epoch: 5| Step: 3
Training loss: 2.872833728790283
Validation loss: 2.615518070036365

Epoch: 5| Step: 4
Training loss: 2.4615061283111572
Validation loss: 2.614274578709756

Epoch: 5| Step: 5
Training loss: 2.684887170791626
Validation loss: 2.611283679162302

Epoch: 5| Step: 6
Training loss: 3.7850825786590576
Validation loss: 2.6099350747241767

Epoch: 5| Step: 7
Training loss: 2.6995747089385986
Validation loss: 2.6104318377792195

Epoch: 5| Step: 8
Training loss: 2.8134517669677734
Validation loss: 2.6108826309122066

Epoch: 5| Step: 9
Training loss: 3.0570285320281982
Validation loss: 2.6096541753379245

Epoch: 5| Step: 10
Training loss: 2.537651300430298
Validation loss: 2.6080427246709026

Epoch: 60| Step: 0
Training loss: 2.830049991607666
Validation loss: 2.610301266434372

Epoch: 5| Step: 1
Training loss: 3.233583450317383
Validation loss: 2.6106479244847454

Epoch: 5| Step: 2
Training loss: 2.1260993480682373
Validation loss: 2.6087505279048795

Epoch: 5| Step: 3
Training loss: 3.280949354171753
Validation loss: 2.610330645756055

Epoch: 5| Step: 4
Training loss: 3.0938289165496826
Validation loss: 2.6084435191205753

Epoch: 5| Step: 5
Training loss: 2.60307240486145
Validation loss: 2.6060162334031958

Epoch: 5| Step: 6
Training loss: 3.005342721939087
Validation loss: 2.6082191544194377

Epoch: 5| Step: 7
Training loss: 2.7736358642578125
Validation loss: 2.608979671232162

Epoch: 5| Step: 8
Training loss: 2.4307732582092285
Validation loss: 2.604093213235178

Epoch: 5| Step: 9
Training loss: 2.0515246391296387
Validation loss: 2.607133185991677

Epoch: 5| Step: 10
Training loss: 3.11664080619812
Validation loss: 2.6129019952589467

Epoch: 61| Step: 0
Training loss: 2.530959367752075
Validation loss: 2.6257500571589314

Epoch: 5| Step: 1
Training loss: 2.667053699493408
Validation loss: 2.6329921137902046

Epoch: 5| Step: 2
Training loss: 3.693967819213867
Validation loss: 2.63796486649462

Epoch: 5| Step: 3
Training loss: 2.3330318927764893
Validation loss: 2.6370326293412076

Epoch: 5| Step: 4
Training loss: 2.4059243202209473
Validation loss: 2.6377727934109267

Epoch: 5| Step: 5
Training loss: 2.6524124145507812
Validation loss: 2.6193945100230556

Epoch: 5| Step: 6
Training loss: 3.150362491607666
Validation loss: 2.6134027768206853

Epoch: 5| Step: 7
Training loss: 2.3957958221435547
Validation loss: 2.609247415296493

Epoch: 5| Step: 8
Training loss: 3.2934622764587402
Validation loss: 2.606048104583576

Epoch: 5| Step: 9
Training loss: 2.6683197021484375
Validation loss: 2.611308838731499

Epoch: 5| Step: 10
Training loss: 2.719717025756836
Validation loss: 2.620291071553384

Epoch: 62| Step: 0
Training loss: 2.3588595390319824
Validation loss: 2.626993433121712

Epoch: 5| Step: 1
Training loss: 2.331817150115967
Validation loss: 2.632465518930907

Epoch: 5| Step: 2
Training loss: 2.7952542304992676
Validation loss: 2.629185632992816

Epoch: 5| Step: 3
Training loss: 2.7971389293670654
Validation loss: 2.6263093897091445

Epoch: 5| Step: 4
Training loss: 2.4801597595214844
Validation loss: 2.6179192707102787

Epoch: 5| Step: 5
Training loss: 2.3461780548095703
Validation loss: 2.6150206827348277

Epoch: 5| Step: 6
Training loss: 3.159119129180908
Validation loss: 2.6115006041783158

Epoch: 5| Step: 7
Training loss: 3.1365089416503906
Validation loss: 2.613113428956719

Epoch: 5| Step: 8
Training loss: 3.0582404136657715
Validation loss: 2.6082865627863074

Epoch: 5| Step: 9
Training loss: 2.984178066253662
Validation loss: 2.6043209337419078

Epoch: 5| Step: 10
Training loss: 3.2079434394836426
Validation loss: 2.6015401091626895

Epoch: 63| Step: 0
Training loss: 2.855351686477661
Validation loss: 2.604607423146566

Epoch: 5| Step: 1
Training loss: 3.2603812217712402
Validation loss: 2.617710744180987

Epoch: 5| Step: 2
Training loss: 2.4373514652252197
Validation loss: 2.623197647833055

Epoch: 5| Step: 3
Training loss: 2.7346949577331543
Validation loss: 2.641477584838867

Epoch: 5| Step: 4
Training loss: 2.856346845626831
Validation loss: 2.6452578370289137

Epoch: 5| Step: 5
Training loss: 3.2985241413116455
Validation loss: 2.6610243653738372

Epoch: 5| Step: 6
Training loss: 2.1815030574798584
Validation loss: 2.6495267473241335

Epoch: 5| Step: 7
Training loss: 2.6316781044006348
Validation loss: 2.6366071547231367

Epoch: 5| Step: 8
Training loss: 2.3916594982147217
Validation loss: 2.6328934366985033

Epoch: 5| Step: 9
Training loss: 2.515428066253662
Validation loss: 2.613987350976595

Epoch: 5| Step: 10
Training loss: 3.6412882804870605
Validation loss: 2.6104695156056392

Epoch: 64| Step: 0
Training loss: 3.7442822456359863
Validation loss: 2.602943379391906

Epoch: 5| Step: 1
Training loss: 2.65714168548584
Validation loss: 2.602055318893925

Epoch: 5| Step: 2
Training loss: 2.516479015350342
Validation loss: 2.6049990371991227

Epoch: 5| Step: 3
Training loss: 2.0391907691955566
Validation loss: 2.6069228726048626

Epoch: 5| Step: 4
Training loss: 2.1825575828552246
Validation loss: 2.6062548545099076

Epoch: 5| Step: 5
Training loss: 3.37420916557312
Validation loss: 2.606975540038078

Epoch: 5| Step: 6
Training loss: 3.280247211456299
Validation loss: 2.6102263081458306

Epoch: 5| Step: 7
Training loss: 2.783362627029419
Validation loss: 2.6084987835217546

Epoch: 5| Step: 8
Training loss: 2.546173095703125
Validation loss: 2.609012242286436

Epoch: 5| Step: 9
Training loss: 2.9445762634277344
Validation loss: 2.620230620907199

Epoch: 5| Step: 10
Training loss: 2.4292922019958496
Validation loss: 2.6277545395717827

Epoch: 65| Step: 0
Training loss: 2.864736557006836
Validation loss: 2.617063524902508

Epoch: 5| Step: 1
Training loss: 3.3283896446228027
Validation loss: 2.606130922994306

Epoch: 5| Step: 2
Training loss: 2.7884328365325928
Validation loss: 2.6056196971606185

Epoch: 5| Step: 3
Training loss: 2.5088820457458496
Validation loss: 2.5996703640107186

Epoch: 5| Step: 4
Training loss: 2.3682491779327393
Validation loss: 2.597020228703817

Epoch: 5| Step: 5
Training loss: 2.859862804412842
Validation loss: 2.5966732476347234

Epoch: 5| Step: 6
Training loss: 3.0296876430511475
Validation loss: 2.596311997341853

Epoch: 5| Step: 7
Training loss: 3.026826858520508
Validation loss: 2.5962274074554443

Epoch: 5| Step: 8
Training loss: 2.679128646850586
Validation loss: 2.5971968507253997

Epoch: 5| Step: 9
Training loss: 2.0118601322174072
Validation loss: 2.596465177433465

Epoch: 5| Step: 10
Training loss: 2.996656894683838
Validation loss: 2.5978647432019635

Epoch: 66| Step: 0
Training loss: 2.530327320098877
Validation loss: 2.598334338075371

Epoch: 5| Step: 1
Training loss: 2.9716620445251465
Validation loss: 2.5984393319775982

Epoch: 5| Step: 2
Training loss: 3.537553310394287
Validation loss: 2.5986936605104836

Epoch: 5| Step: 3
Training loss: 2.657729387283325
Validation loss: 2.5970358951117403

Epoch: 5| Step: 4
Training loss: 1.9931144714355469
Validation loss: 2.602717217578683

Epoch: 5| Step: 5
Training loss: 3.027265787124634
Validation loss: 2.606846614550519

Epoch: 5| Step: 6
Training loss: 3.4018661975860596
Validation loss: 2.60542070481085

Epoch: 5| Step: 7
Training loss: 2.1426985263824463
Validation loss: 2.602122258114558

Epoch: 5| Step: 8
Training loss: 2.614276885986328
Validation loss: 2.6027364859016995

Epoch: 5| Step: 9
Training loss: 2.850205898284912
Validation loss: 2.6006974199766755

Epoch: 5| Step: 10
Training loss: 2.674936294555664
Validation loss: 2.596145724737516

Epoch: 67| Step: 0
Training loss: 2.5841803550720215
Validation loss: 2.6004696456334924

Epoch: 5| Step: 1
Training loss: 2.972862958908081
Validation loss: 2.597611094033846

Epoch: 5| Step: 2
Training loss: 2.6997668743133545
Validation loss: 2.5956997512489237

Epoch: 5| Step: 3
Training loss: 3.281081438064575
Validation loss: 2.593331108811081

Epoch: 5| Step: 4
Training loss: 2.6023573875427246
Validation loss: 2.592764295557494

Epoch: 5| Step: 5
Training loss: 3.000152587890625
Validation loss: 2.5961979435336207

Epoch: 5| Step: 6
Training loss: 2.2613675594329834
Validation loss: 2.5923989588214504

Epoch: 5| Step: 7
Training loss: 2.315075159072876
Validation loss: 2.590844646576912

Epoch: 5| Step: 8
Training loss: 3.6468422412872314
Validation loss: 2.5908655966481855

Epoch: 5| Step: 9
Training loss: 2.2448770999908447
Validation loss: 2.58956515789032

Epoch: 5| Step: 10
Training loss: 2.7787318229675293
Validation loss: 2.5903570831462903

Epoch: 68| Step: 0
Training loss: 2.4161758422851562
Validation loss: 2.5935732267236196

Epoch: 5| Step: 1
Training loss: 2.658830165863037
Validation loss: 2.5924161070136615

Epoch: 5| Step: 2
Training loss: 2.304980754852295
Validation loss: 2.593085872229709

Epoch: 5| Step: 3
Training loss: 3.201824903488159
Validation loss: 2.592283479629024

Epoch: 5| Step: 4
Training loss: 2.880237340927124
Validation loss: 2.589636354036229

Epoch: 5| Step: 5
Training loss: 2.5278453826904297
Validation loss: 2.5918770913154847

Epoch: 5| Step: 6
Training loss: 3.4363341331481934
Validation loss: 2.5881547722765195

Epoch: 5| Step: 7
Training loss: 2.781193494796753
Validation loss: 2.590082271124727

Epoch: 5| Step: 8
Training loss: 2.962498188018799
Validation loss: 2.58894613224973

Epoch: 5| Step: 9
Training loss: 2.450928211212158
Validation loss: 2.5844252032618367

Epoch: 5| Step: 10
Training loss: 2.728893280029297
Validation loss: 2.5927469653467976

Epoch: 69| Step: 0
Training loss: 2.6986331939697266
Validation loss: 2.5939219638865483

Epoch: 5| Step: 1
Training loss: 2.141974925994873
Validation loss: 2.592026751528504

Epoch: 5| Step: 2
Training loss: 3.171464681625366
Validation loss: 2.5939145062559392

Epoch: 5| Step: 3
Training loss: 3.6991240978240967
Validation loss: 2.590064920404906

Epoch: 5| Step: 4
Training loss: 3.2469067573547363
Validation loss: 2.592455724234222

Epoch: 5| Step: 5
Training loss: 2.5410945415496826
Validation loss: 2.594727113682737

Epoch: 5| Step: 6
Training loss: 3.0187809467315674
Validation loss: 2.5955242264655327

Epoch: 5| Step: 7
Training loss: 2.746065616607666
Validation loss: 2.59337112211412

Epoch: 5| Step: 8
Training loss: 2.164350748062134
Validation loss: 2.5990606610492994

Epoch: 5| Step: 9
Training loss: 2.8353962898254395
Validation loss: 2.5892576402233494

Epoch: 5| Step: 10
Training loss: 1.8950340747833252
Validation loss: 2.5856797105522564

Epoch: 70| Step: 0
Training loss: 3.194070339202881
Validation loss: 2.588849595797959

Epoch: 5| Step: 1
Training loss: 2.579993724822998
Validation loss: 2.5874548650556997

Epoch: 5| Step: 2
Training loss: 2.132619857788086
Validation loss: 2.5860508718798236

Epoch: 5| Step: 3
Training loss: 2.043879747390747
Validation loss: 2.5825354719674714

Epoch: 5| Step: 4
Training loss: 2.6796295642852783
Validation loss: 2.5862854937071442

Epoch: 5| Step: 5
Training loss: 3.1943907737731934
Validation loss: 2.5886590275713193

Epoch: 5| Step: 6
Training loss: 2.715780019760132
Validation loss: 2.5885541669784056

Epoch: 5| Step: 7
Training loss: 2.966869354248047
Validation loss: 2.585393918457852

Epoch: 5| Step: 8
Training loss: 2.81373929977417
Validation loss: 2.5853925315282678

Epoch: 5| Step: 9
Training loss: 3.1810874938964844
Validation loss: 2.5875287158514864

Epoch: 5| Step: 10
Training loss: 2.757838487625122
Validation loss: 2.585464310902421

Epoch: 71| Step: 0
Training loss: 2.669356346130371
Validation loss: 2.58026389152773

Epoch: 5| Step: 1
Training loss: 2.3816323280334473
Validation loss: 2.584702302050847

Epoch: 5| Step: 2
Training loss: 2.7918198108673096
Validation loss: 2.581056384630101

Epoch: 5| Step: 3
Training loss: 2.6762852668762207
Validation loss: 2.5794833219179543

Epoch: 5| Step: 4
Training loss: 3.1884377002716064
Validation loss: 2.5845905196282173

Epoch: 5| Step: 5
Training loss: 2.8340375423431396
Validation loss: 2.583253783564414

Epoch: 5| Step: 6
Training loss: 2.1033947467803955
Validation loss: 2.580552008844191

Epoch: 5| Step: 7
Training loss: 2.9198572635650635
Validation loss: 2.5836324025225896

Epoch: 5| Step: 8
Training loss: 3.1945509910583496
Validation loss: 2.585371086674352

Epoch: 5| Step: 9
Training loss: 2.867546796798706
Validation loss: 2.589505200744957

Epoch: 5| Step: 10
Training loss: 2.611126184463501
Validation loss: 2.590038320069672

Epoch: 72| Step: 0
Training loss: 2.8197407722473145
Validation loss: 2.5841703056007304

Epoch: 5| Step: 1
Training loss: 3.148918628692627
Validation loss: 2.592237141824538

Epoch: 5| Step: 2
Training loss: 3.1908600330352783
Validation loss: 2.5858284094000377

Epoch: 5| Step: 3
Training loss: 3.300963878631592
Validation loss: 2.584481759737897

Epoch: 5| Step: 4
Training loss: 2.388859272003174
Validation loss: 2.5842391419154342

Epoch: 5| Step: 5
Training loss: 3.42292857170105
Validation loss: 2.580083075390067

Epoch: 5| Step: 6
Training loss: 2.66658353805542
Validation loss: 2.5769027894543064

Epoch: 5| Step: 7
Training loss: 2.516044855117798
Validation loss: 2.578755473577848

Epoch: 5| Step: 8
Training loss: 2.240048885345459
Validation loss: 2.576965561477087

Epoch: 5| Step: 9
Training loss: 2.232206344604492
Validation loss: 2.5760132471720376

Epoch: 5| Step: 10
Training loss: 2.2497305870056152
Validation loss: 2.5786311370070263

Epoch: 73| Step: 0
Training loss: 2.513296365737915
Validation loss: 2.5791092277855

Epoch: 5| Step: 1
Training loss: 2.7312731742858887
Validation loss: 2.5770247751666653

Epoch: 5| Step: 2
Training loss: 2.8527672290802
Validation loss: 2.5797863929502425

Epoch: 5| Step: 3
Training loss: 2.8613224029541016
Validation loss: 2.578909694507558

Epoch: 5| Step: 4
Training loss: 2.5496833324432373
Validation loss: 2.577864903275685

Epoch: 5| Step: 5
Training loss: 3.1003072261810303
Validation loss: 2.5791205770225933

Epoch: 5| Step: 6
Training loss: 2.2350096702575684
Validation loss: 2.57707408935793

Epoch: 5| Step: 7
Training loss: 2.9877960681915283
Validation loss: 2.5753606006663334

Epoch: 5| Step: 8
Training loss: 3.1436209678649902
Validation loss: 2.577034286273423

Epoch: 5| Step: 9
Training loss: 2.342627763748169
Validation loss: 2.574074791323754

Epoch: 5| Step: 10
Training loss: 2.9233226776123047
Validation loss: 2.578214681276711

Epoch: 74| Step: 0
Training loss: 3.020759105682373
Validation loss: 2.581594823509134

Epoch: 5| Step: 1
Training loss: 3.3224830627441406
Validation loss: 2.585956435049734

Epoch: 5| Step: 2
Training loss: 2.4239792823791504
Validation loss: 2.6007880292912966

Epoch: 5| Step: 3
Training loss: 2.1425535678863525
Validation loss: 2.601619448713077

Epoch: 5| Step: 4
Training loss: 2.41813325881958
Validation loss: 2.5936520766186457

Epoch: 5| Step: 5
Training loss: 2.263993740081787
Validation loss: 2.592993467084823

Epoch: 5| Step: 6
Training loss: 2.8136608600616455
Validation loss: 2.5863894493349138

Epoch: 5| Step: 7
Training loss: 2.384464740753174
Validation loss: 2.5838015130771104

Epoch: 5| Step: 8
Training loss: 3.4465324878692627
Validation loss: 2.5816782443754134

Epoch: 5| Step: 9
Training loss: 2.7587249279022217
Validation loss: 2.571593020551948

Epoch: 5| Step: 10
Training loss: 3.344780445098877
Validation loss: 2.570501083968788

Epoch: 75| Step: 0
Training loss: 2.5265889167785645
Validation loss: 2.568559487660726

Epoch: 5| Step: 1
Training loss: 2.1113345623016357
Validation loss: 2.575685578007852

Epoch: 5| Step: 2
Training loss: 3.1233198642730713
Validation loss: 2.571288606171967

Epoch: 5| Step: 3
Training loss: 2.944392681121826
Validation loss: 2.570302632547194

Epoch: 5| Step: 4
Training loss: 2.2609705924987793
Validation loss: 2.571358429488315

Epoch: 5| Step: 5
Training loss: 2.440505266189575
Validation loss: 2.575019692861906

Epoch: 5| Step: 6
Training loss: 2.901837110519409
Validation loss: 2.5755905541040565

Epoch: 5| Step: 7
Training loss: 3.1281354427337646
Validation loss: 2.5771272925920385

Epoch: 5| Step: 8
Training loss: 3.4401214122772217
Validation loss: 2.5796036105002127

Epoch: 5| Step: 9
Training loss: 2.689772367477417
Validation loss: 2.578389142149238

Epoch: 5| Step: 10
Training loss: 2.615116596221924
Validation loss: 2.574934249283165

Epoch: 76| Step: 0
Training loss: 2.587296962738037
Validation loss: 2.575014470725931

Epoch: 5| Step: 1
Training loss: 2.876836061477661
Validation loss: 2.572038596676242

Epoch: 5| Step: 2
Training loss: 3.1715195178985596
Validation loss: 2.5686476128075713

Epoch: 5| Step: 3
Training loss: 2.246960163116455
Validation loss: 2.5648423523031254

Epoch: 5| Step: 4
Training loss: 2.493722915649414
Validation loss: 2.567900267980432

Epoch: 5| Step: 5
Training loss: 2.843808650970459
Validation loss: 2.5692767840559765

Epoch: 5| Step: 6
Training loss: 2.7802391052246094
Validation loss: 2.5649590646066973

Epoch: 5| Step: 7
Training loss: 2.7196152210235596
Validation loss: 2.5683888363581833

Epoch: 5| Step: 8
Training loss: 3.0244669914245605
Validation loss: 2.571203483048306

Epoch: 5| Step: 9
Training loss: 2.7194879055023193
Validation loss: 2.5688429827331216

Epoch: 5| Step: 10
Training loss: 2.6988608837127686
Validation loss: 2.565857450167338

Epoch: 77| Step: 0
Training loss: 3.154775619506836
Validation loss: 2.5669218314591276

Epoch: 5| Step: 1
Training loss: 2.8217146396636963
Validation loss: 2.56768403514739

Epoch: 5| Step: 2
Training loss: 2.807055950164795
Validation loss: 2.570021278114729

Epoch: 5| Step: 3
Training loss: 2.4632866382598877
Validation loss: 2.571987259772516

Epoch: 5| Step: 4
Training loss: 2.1281328201293945
Validation loss: 2.569739685263685

Epoch: 5| Step: 5
Training loss: 3.425867795944214
Validation loss: 2.5707077569859003

Epoch: 5| Step: 6
Training loss: 2.6500420570373535
Validation loss: 2.5689287621487855

Epoch: 5| Step: 7
Training loss: 2.4319052696228027
Validation loss: 2.5723337665680917

Epoch: 5| Step: 8
Training loss: 2.788825750350952
Validation loss: 2.5650802555904595

Epoch: 5| Step: 9
Training loss: 2.39469575881958
Validation loss: 2.565653452309229

Epoch: 5| Step: 10
Training loss: 3.119748115539551
Validation loss: 2.5647437751934095

Epoch: 78| Step: 0
Training loss: 2.5458133220672607
Validation loss: 2.5654281364974154

Epoch: 5| Step: 1
Training loss: 2.319371223449707
Validation loss: 2.5660534340848207

Epoch: 5| Step: 2
Training loss: 2.6514503955841064
Validation loss: 2.565149978924823

Epoch: 5| Step: 3
Training loss: 2.49835205078125
Validation loss: 2.563455440664804

Epoch: 5| Step: 4
Training loss: 2.0570273399353027
Validation loss: 2.563465069699031

Epoch: 5| Step: 5
Training loss: 3.5428364276885986
Validation loss: 2.56266652512294

Epoch: 5| Step: 6
Training loss: 2.9542176723480225
Validation loss: 2.5628069652024137

Epoch: 5| Step: 7
Training loss: 3.005544662475586
Validation loss: 2.564494809796733

Epoch: 5| Step: 8
Training loss: 2.6848971843719482
Validation loss: 2.5660101239399244

Epoch: 5| Step: 9
Training loss: 2.5611305236816406
Validation loss: 2.56831774403972

Epoch: 5| Step: 10
Training loss: 3.381195068359375
Validation loss: 2.5709855556488037

Epoch: 79| Step: 0
Training loss: 3.126711368560791
Validation loss: 2.570771442946567

Epoch: 5| Step: 1
Training loss: 2.4243457317352295
Validation loss: 2.5690462973810013

Epoch: 5| Step: 2
Training loss: 3.0285630226135254
Validation loss: 2.5660051299679663

Epoch: 5| Step: 3
Training loss: 2.595057249069214
Validation loss: 2.5671902625791487

Epoch: 5| Step: 4
Training loss: 2.592028856277466
Validation loss: 2.566254054346392

Epoch: 5| Step: 5
Training loss: 3.0562233924865723
Validation loss: 2.565129315981301

Epoch: 5| Step: 6
Training loss: 2.384326219558716
Validation loss: 2.5649064048644035

Epoch: 5| Step: 7
Training loss: 3.2786548137664795
Validation loss: 2.563284570170987

Epoch: 5| Step: 8
Training loss: 2.173893928527832
Validation loss: 2.5625400850849767

Epoch: 5| Step: 9
Training loss: 3.0401816368103027
Validation loss: 2.560133672529651

Epoch: 5| Step: 10
Training loss: 2.317056655883789
Validation loss: 2.562796320966495

Epoch: 80| Step: 0
Training loss: 3.2114250659942627
Validation loss: 2.5623700157288583

Epoch: 5| Step: 1
Training loss: 2.4705512523651123
Validation loss: 2.563324583474026

Epoch: 5| Step: 2
Training loss: 2.695857524871826
Validation loss: 2.5605073231522755

Epoch: 5| Step: 3
Training loss: 2.2411484718322754
Validation loss: 2.5632909908089587

Epoch: 5| Step: 4
Training loss: 2.1619820594787598
Validation loss: 2.5620227654774985

Epoch: 5| Step: 5
Training loss: 2.6092593669891357
Validation loss: 2.566966155523895

Epoch: 5| Step: 6
Training loss: 2.5045433044433594
Validation loss: 2.563828831077904

Epoch: 5| Step: 7
Training loss: 3.537778854370117
Validation loss: 2.5672130251443512

Epoch: 5| Step: 8
Training loss: 3.2839932441711426
Validation loss: 2.5649049358983196

Epoch: 5| Step: 9
Training loss: 3.1704165935516357
Validation loss: 2.5626958031808176

Epoch: 5| Step: 10
Training loss: 2.1022815704345703
Validation loss: 2.568390246360533

Epoch: 81| Step: 0
Training loss: 2.8243470191955566
Validation loss: 2.562916689021613

Epoch: 5| Step: 1
Training loss: 3.092165946960449
Validation loss: 2.559996763865153

Epoch: 5| Step: 2
Training loss: 2.8919389247894287
Validation loss: 2.5580084759701966

Epoch: 5| Step: 3
Training loss: 2.623966932296753
Validation loss: 2.557138978794057

Epoch: 5| Step: 4
Training loss: 2.494518280029297
Validation loss: 2.554629792449295

Epoch: 5| Step: 5
Training loss: 2.7963733673095703
Validation loss: 2.5556487268017185

Epoch: 5| Step: 6
Training loss: 2.7546939849853516
Validation loss: 2.5548242599733415

Epoch: 5| Step: 7
Training loss: 1.9034862518310547
Validation loss: 2.559071122959096

Epoch: 5| Step: 8
Training loss: 2.8493685722351074
Validation loss: 2.5582231116551224

Epoch: 5| Step: 9
Training loss: 2.651214122772217
Validation loss: 2.5582303129216677

Epoch: 5| Step: 10
Training loss: 3.3145625591278076
Validation loss: 2.5571576267160396

Epoch: 82| Step: 0
Training loss: 3.028093099594116
Validation loss: 2.5566660178604947

Epoch: 5| Step: 1
Training loss: 3.22039794921875
Validation loss: 2.5595112923652894

Epoch: 5| Step: 2
Training loss: 2.5560669898986816
Validation loss: 2.5537332668099353

Epoch: 5| Step: 3
Training loss: 2.2873339653015137
Validation loss: 2.554526509777192

Epoch: 5| Step: 4
Training loss: 2.094583511352539
Validation loss: 2.5537559037567465

Epoch: 5| Step: 5
Training loss: 2.835718870162964
Validation loss: 2.554116079884191

Epoch: 5| Step: 6
Training loss: 3.374828338623047
Validation loss: 2.559599430330338

Epoch: 5| Step: 7
Training loss: 2.283524513244629
Validation loss: 2.5535539862930134

Epoch: 5| Step: 8
Training loss: 2.077183485031128
Validation loss: 2.5580000749198337

Epoch: 5| Step: 9
Training loss: 3.153374671936035
Validation loss: 2.5590884864971204

Epoch: 5| Step: 10
Training loss: 3.2261643409729004
Validation loss: 2.561367755295128

Epoch: 83| Step: 0
Training loss: 3.3935256004333496
Validation loss: 2.5544637992817867

Epoch: 5| Step: 1
Training loss: 2.8940155506134033
Validation loss: 2.556841737480574

Epoch: 5| Step: 2
Training loss: 3.0890870094299316
Validation loss: 2.555774739993516

Epoch: 5| Step: 3
Training loss: 2.815772533416748
Validation loss: 2.553905484496906

Epoch: 5| Step: 4
Training loss: 3.1665401458740234
Validation loss: 2.5508611253512803

Epoch: 5| Step: 5
Training loss: 2.640514850616455
Validation loss: 2.55333960440851

Epoch: 5| Step: 6
Training loss: 2.4800078868865967
Validation loss: 2.5518084623480357

Epoch: 5| Step: 7
Training loss: 2.0874533653259277
Validation loss: 2.5518142843759186

Epoch: 5| Step: 8
Training loss: 2.249638795852661
Validation loss: 2.5546082937589256

Epoch: 5| Step: 9
Training loss: 2.3494677543640137
Validation loss: 2.5540396577568463

Epoch: 5| Step: 10
Training loss: 2.9076240062713623
Validation loss: 2.5537528094424995

Epoch: 84| Step: 0
Training loss: 2.920879364013672
Validation loss: 2.5526871424849316

Epoch: 5| Step: 1
Training loss: 3.1568691730499268
Validation loss: 2.547592742468721

Epoch: 5| Step: 2
Training loss: 2.670567035675049
Validation loss: 2.5495776771217264

Epoch: 5| Step: 3
Training loss: 3.1446328163146973
Validation loss: 2.5483545257199194

Epoch: 5| Step: 4
Training loss: 2.51871919631958
Validation loss: 2.5491124481283207

Epoch: 5| Step: 5
Training loss: 2.6449697017669678
Validation loss: 2.5483156532369633

Epoch: 5| Step: 6
Training loss: 2.6361639499664307
Validation loss: 2.551614466533866

Epoch: 5| Step: 7
Training loss: 2.2430737018585205
Validation loss: 2.54927336400555

Epoch: 5| Step: 8
Training loss: 3.073643207550049
Validation loss: 2.548837310524397

Epoch: 5| Step: 9
Training loss: 2.855555772781372
Validation loss: 2.5515442355986564

Epoch: 5| Step: 10
Training loss: 2.029325246810913
Validation loss: 2.5487909983563166

Epoch: 85| Step: 0
Training loss: 3.0953667163848877
Validation loss: 2.5538985601035495

Epoch: 5| Step: 1
Training loss: 2.2636771202087402
Validation loss: 2.5502695652746383

Epoch: 5| Step: 2
Training loss: 2.464733600616455
Validation loss: 2.5538056460759972

Epoch: 5| Step: 3
Training loss: 2.800137758255005
Validation loss: 2.5557291892267044

Epoch: 5| Step: 4
Training loss: 1.6121361255645752
Validation loss: 2.555660342657438

Epoch: 5| Step: 5
Training loss: 2.3750550746917725
Validation loss: 2.5530206772588913

Epoch: 5| Step: 6
Training loss: 3.5412590503692627
Validation loss: 2.5533975375595914

Epoch: 5| Step: 7
Training loss: 3.1921916007995605
Validation loss: 2.546843728711528

Epoch: 5| Step: 8
Training loss: 2.936000347137451
Validation loss: 2.5487300554911294

Epoch: 5| Step: 9
Training loss: 2.7269625663757324
Validation loss: 2.5481255490292787

Epoch: 5| Step: 10
Training loss: 3.0005245208740234
Validation loss: 2.5470022283574587

Epoch: 86| Step: 0
Training loss: 3.1750948429107666
Validation loss: 2.551758881538145

Epoch: 5| Step: 1
Training loss: 3.075103282928467
Validation loss: 2.551589071109731

Epoch: 5| Step: 2
Training loss: 1.725388526916504
Validation loss: 2.5546177202655422

Epoch: 5| Step: 3
Training loss: 2.2482144832611084
Validation loss: 2.553178302703365

Epoch: 5| Step: 4
Training loss: 2.8417725563049316
Validation loss: 2.5527169140436317

Epoch: 5| Step: 5
Training loss: 2.980741024017334
Validation loss: 2.5514513318256666

Epoch: 5| Step: 6
Training loss: 2.7082457542419434
Validation loss: 2.5524112204069733

Epoch: 5| Step: 7
Training loss: 2.9307093620300293
Validation loss: 2.547544656261321

Epoch: 5| Step: 8
Training loss: 3.331263780593872
Validation loss: 2.5457689275023756

Epoch: 5| Step: 9
Training loss: 2.2016470432281494
Validation loss: 2.5467694318422707

Epoch: 5| Step: 10
Training loss: 2.844419002532959
Validation loss: 2.545821794899561

Epoch: 87| Step: 0
Training loss: 3.1403653621673584
Validation loss: 2.542724947775564

Epoch: 5| Step: 1
Training loss: 3.1399307250976562
Validation loss: 2.5416868630275933

Epoch: 5| Step: 2
Training loss: 2.1188857555389404
Validation loss: 2.5476306356409544

Epoch: 5| Step: 3
Training loss: 3.0802741050720215
Validation loss: 2.551746306880828

Epoch: 5| Step: 4
Training loss: 2.0577220916748047
Validation loss: 2.551880854432301

Epoch: 5| Step: 5
Training loss: 2.97224497795105
Validation loss: 2.54777765786776

Epoch: 5| Step: 6
Training loss: 2.306236743927002
Validation loss: 2.5435646939021286

Epoch: 5| Step: 7
Training loss: 2.6274523735046387
Validation loss: 2.543730804997106

Epoch: 5| Step: 8
Training loss: 2.944891929626465
Validation loss: 2.5441275770946215

Epoch: 5| Step: 9
Training loss: 2.5081191062927246
Validation loss: 2.541675721445391

Epoch: 5| Step: 10
Training loss: 3.1248605251312256
Validation loss: 2.541630196314986

Epoch: 88| Step: 0
Training loss: 2.51204514503479
Validation loss: 2.5451185446913525

Epoch: 5| Step: 1
Training loss: 2.9786102771759033
Validation loss: 2.5440892737398864

Epoch: 5| Step: 2
Training loss: 2.349128007888794
Validation loss: 2.5440732253495084

Epoch: 5| Step: 3
Training loss: 2.3307807445526123
Validation loss: 2.5421599213794996

Epoch: 5| Step: 4
Training loss: 2.683302402496338
Validation loss: 2.5434493172553276

Epoch: 5| Step: 5
Training loss: 2.371079921722412
Validation loss: 2.541635790178853

Epoch: 5| Step: 6
Training loss: 2.359112024307251
Validation loss: 2.5437736972685783

Epoch: 5| Step: 7
Training loss: 2.408083438873291
Validation loss: 2.543322265789073

Epoch: 5| Step: 8
Training loss: 2.654522180557251
Validation loss: 2.5414779878431752

Epoch: 5| Step: 9
Training loss: 2.983874559402466
Validation loss: 2.5430990290898148

Epoch: 5| Step: 10
Training loss: 4.5470075607299805
Validation loss: 2.5413209879270164

Epoch: 89| Step: 0
Training loss: 2.9748573303222656
Validation loss: 2.5418675279104583

Epoch: 5| Step: 1
Training loss: 2.86116623878479
Validation loss: 2.5423839553709953

Epoch: 5| Step: 2
Training loss: 2.3240227699279785
Validation loss: 2.5394884053096978

Epoch: 5| Step: 3
Training loss: 2.6305489540100098
Validation loss: 2.5398471611802296

Epoch: 5| Step: 4
Training loss: 2.6238248348236084
Validation loss: 2.539716761599305

Epoch: 5| Step: 5
Training loss: 2.977487325668335
Validation loss: 2.54142019953779

Epoch: 5| Step: 6
Training loss: 2.299649953842163
Validation loss: 2.5402827775606545

Epoch: 5| Step: 7
Training loss: 2.591740608215332
Validation loss: 2.5388910052596882

Epoch: 5| Step: 8
Training loss: 2.9983248710632324
Validation loss: 2.540158805026803

Epoch: 5| Step: 9
Training loss: 2.536384105682373
Validation loss: 2.538522171717818

Epoch: 5| Step: 10
Training loss: 3.1521785259246826
Validation loss: 2.540416694456531

Epoch: 90| Step: 0
Training loss: 2.882340669631958
Validation loss: 2.5386743109713317

Epoch: 5| Step: 1
Training loss: 1.978776216506958
Validation loss: 2.541994184576055

Epoch: 5| Step: 2
Training loss: 2.5181632041931152
Validation loss: 2.539769846905944

Epoch: 5| Step: 3
Training loss: 2.3592448234558105
Validation loss: 2.5381169780608146

Epoch: 5| Step: 4
Training loss: 3.3102593421936035
Validation loss: 2.537441345953172

Epoch: 5| Step: 5
Training loss: 2.5592775344848633
Validation loss: 2.540532171085317

Epoch: 5| Step: 6
Training loss: 2.700953245162964
Validation loss: 2.538605800238989

Epoch: 5| Step: 7
Training loss: 2.6700916290283203
Validation loss: 2.5385966736783265

Epoch: 5| Step: 8
Training loss: 3.352489948272705
Validation loss: 2.542211660774805

Epoch: 5| Step: 9
Training loss: 3.2315704822540283
Validation loss: 2.5404118055938394

Epoch: 5| Step: 10
Training loss: 2.2435431480407715
Validation loss: 2.5352860202071485

Epoch: 91| Step: 0
Training loss: 2.167036533355713
Validation loss: 2.531045834223429

Epoch: 5| Step: 1
Training loss: 2.0560622215270996
Validation loss: 2.532073505463139

Epoch: 5| Step: 2
Training loss: 2.823183536529541
Validation loss: 2.535347395045783

Epoch: 5| Step: 3
Training loss: 2.6110689640045166
Validation loss: 2.5358059637008177

Epoch: 5| Step: 4
Training loss: 2.98801851272583
Validation loss: 2.5334590019718295

Epoch: 5| Step: 5
Training loss: 2.818822145462036
Validation loss: 2.5390073483990085

Epoch: 5| Step: 6
Training loss: 2.95212984085083
Validation loss: 2.5394023067207745

Epoch: 5| Step: 7
Training loss: 2.911207675933838
Validation loss: 2.5411725762069866

Epoch: 5| Step: 8
Training loss: 2.9247593879699707
Validation loss: 2.5386055951477378

Epoch: 5| Step: 9
Training loss: 2.6868693828582764
Validation loss: 2.5368697156188307

Epoch: 5| Step: 10
Training loss: 3.005980968475342
Validation loss: 2.5352061281922045

Epoch: 92| Step: 0
Training loss: 2.3450605869293213
Validation loss: 2.5352377481358026

Epoch: 5| Step: 1
Training loss: 2.7541210651397705
Validation loss: 2.5326052352946293

Epoch: 5| Step: 2
Training loss: 1.9120210409164429
Validation loss: 2.5304058495388237

Epoch: 5| Step: 3
Training loss: 3.478543758392334
Validation loss: 2.5341535563110025

Epoch: 5| Step: 4
Training loss: 3.038177013397217
Validation loss: 2.533396569631433

Epoch: 5| Step: 5
Training loss: 2.530585765838623
Validation loss: 2.531181819977299

Epoch: 5| Step: 6
Training loss: 3.1952524185180664
Validation loss: 2.533540766726258

Epoch: 5| Step: 7
Training loss: 2.335536241531372
Validation loss: 2.5345748932130876

Epoch: 5| Step: 8
Training loss: 2.6600537300109863
Validation loss: 2.534975664589995

Epoch: 5| Step: 9
Training loss: 2.7673869132995605
Validation loss: 2.5353988191132903

Epoch: 5| Step: 10
Training loss: 2.842005491256714
Validation loss: 2.5370080137765534

Epoch: 93| Step: 0
Training loss: 2.80212140083313
Validation loss: 2.5404407465329735

Epoch: 5| Step: 1
Training loss: 2.8602776527404785
Validation loss: 2.5410528541893087

Epoch: 5| Step: 2
Training loss: 2.2095088958740234
Validation loss: 2.537376501226938

Epoch: 5| Step: 3
Training loss: 2.4914677143096924
Validation loss: 2.5417024858536257

Epoch: 5| Step: 4
Training loss: 2.1463963985443115
Validation loss: 2.537523546526509

Epoch: 5| Step: 5
Training loss: 2.745720148086548
Validation loss: 2.531537409751646

Epoch: 5| Step: 6
Training loss: 2.231863498687744
Validation loss: 2.534219805912305

Epoch: 5| Step: 7
Training loss: 2.9662041664123535
Validation loss: 2.5337534514806603

Epoch: 5| Step: 8
Training loss: 3.9351582527160645
Validation loss: 2.5344205056467364

Epoch: 5| Step: 9
Training loss: 2.845153331756592
Validation loss: 2.5338295018801125

Epoch: 5| Step: 10
Training loss: 2.6086206436157227
Validation loss: 2.5344632671725367

Epoch: 94| Step: 0
Training loss: 2.7399134635925293
Validation loss: 2.529258756227391

Epoch: 5| Step: 1
Training loss: 2.4029412269592285
Validation loss: 2.533053969824186

Epoch: 5| Step: 2
Training loss: 3.248121738433838
Validation loss: 2.5291996732834847

Epoch: 5| Step: 3
Training loss: 2.3338866233825684
Validation loss: 2.531561686146644

Epoch: 5| Step: 4
Training loss: 3.11653995513916
Validation loss: 2.5304016784955095

Epoch: 5| Step: 5
Training loss: 1.8274071216583252
Validation loss: 2.529362496509347

Epoch: 5| Step: 6
Training loss: 3.117844820022583
Validation loss: 2.52693094489395

Epoch: 5| Step: 7
Training loss: 2.924180507659912
Validation loss: 2.53016488013729

Epoch: 5| Step: 8
Training loss: 3.0016913414001465
Validation loss: 2.5249917789172103

Epoch: 5| Step: 9
Training loss: 2.718189239501953
Validation loss: 2.525681411066363

Epoch: 5| Step: 10
Training loss: 2.3433187007904053
Validation loss: 2.5265691177819365

Epoch: 95| Step: 0
Training loss: 2.926622152328491
Validation loss: 2.5295359549983853

Epoch: 5| Step: 1
Training loss: 3.2899482250213623
Validation loss: 2.5310950997055217

Epoch: 5| Step: 2
Training loss: 3.8681392669677734
Validation loss: 2.5340501928842194

Epoch: 5| Step: 3
Training loss: 2.002826690673828
Validation loss: 2.5329176020878617

Epoch: 5| Step: 4
Training loss: 2.0925354957580566
Validation loss: 2.5343143786153486

Epoch: 5| Step: 5
Training loss: 2.493131160736084
Validation loss: 2.5345597420969317

Epoch: 5| Step: 6
Training loss: 2.106963634490967
Validation loss: 2.5290660294153358

Epoch: 5| Step: 7
Training loss: 2.4345543384552
Validation loss: 2.529677839689357

Epoch: 5| Step: 8
Training loss: 3.6876220703125
Validation loss: 2.5256978568210395

Epoch: 5| Step: 9
Training loss: 2.682526111602783
Validation loss: 2.5246695292893278

Epoch: 5| Step: 10
Training loss: 2.0968642234802246
Validation loss: 2.5248443362533406

Epoch: 96| Step: 0
Training loss: 1.925636649131775
Validation loss: 2.5214525602197133

Epoch: 5| Step: 1
Training loss: 3.2767627239227295
Validation loss: 2.526403739888181

Epoch: 5| Step: 2
Training loss: 3.1360204219818115
Validation loss: 2.525384618389991

Epoch: 5| Step: 3
Training loss: 2.6401963233947754
Validation loss: 2.5253594485662316

Epoch: 5| Step: 4
Training loss: 2.457975149154663
Validation loss: 2.5240031160334104

Epoch: 5| Step: 5
Training loss: 2.6789774894714355
Validation loss: 2.524990709879065

Epoch: 5| Step: 6
Training loss: 2.633530616760254
Validation loss: 2.524755459959789

Epoch: 5| Step: 7
Training loss: 2.914888858795166
Validation loss: 2.522656189498081

Epoch: 5| Step: 8
Training loss: 2.8445558547973633
Validation loss: 2.5251126725186586

Epoch: 5| Step: 9
Training loss: 2.6208205223083496
Validation loss: 2.5281525863114225

Epoch: 5| Step: 10
Training loss: 2.71431565284729
Validation loss: 2.5385206591698433

Epoch: 97| Step: 0
Training loss: 2.4286141395568848
Validation loss: 2.539117800292148

Epoch: 5| Step: 1
Training loss: 2.3348350524902344
Validation loss: 2.5387573960006877

Epoch: 5| Step: 2
Training loss: 2.615201234817505
Validation loss: 2.537191293572867

Epoch: 5| Step: 3
Training loss: 2.658393383026123
Validation loss: 2.533926979187996

Epoch: 5| Step: 4
Training loss: 3.107442855834961
Validation loss: 2.5346420272704093

Epoch: 5| Step: 5
Training loss: 2.527771472930908
Validation loss: 2.5334039118982132

Epoch: 5| Step: 6
Training loss: 2.4309544563293457
Validation loss: 2.537446815480468

Epoch: 5| Step: 7
Training loss: 3.0323805809020996
Validation loss: 2.5350740468630226

Epoch: 5| Step: 8
Training loss: 2.635446310043335
Validation loss: 2.5303870990712154

Epoch: 5| Step: 9
Training loss: 2.755398988723755
Validation loss: 2.5298797776622157

Epoch: 5| Step: 10
Training loss: 3.434451103210449
Validation loss: 2.5285399190841185

Epoch: 98| Step: 0
Training loss: 2.3786091804504395
Validation loss: 2.5272182726090953

Epoch: 5| Step: 1
Training loss: 2.3073878288269043
Validation loss: 2.5255474762250016

Epoch: 5| Step: 2
Training loss: 3.5160059928894043
Validation loss: 2.5203374278160835

Epoch: 5| Step: 3
Training loss: 2.763134002685547
Validation loss: 2.520058754951723

Epoch: 5| Step: 4
Training loss: 3.232959270477295
Validation loss: 2.519471501791349

Epoch: 5| Step: 5
Training loss: 2.217320203781128
Validation loss: 2.5213565852052424

Epoch: 5| Step: 6
Training loss: 3.7437262535095215
Validation loss: 2.522528584285449

Epoch: 5| Step: 7
Training loss: 2.3497655391693115
Validation loss: 2.5197534445793397

Epoch: 5| Step: 8
Training loss: 2.775604009628296
Validation loss: 2.524451832617483

Epoch: 5| Step: 9
Training loss: 2.086233139038086
Validation loss: 2.523763838634696

Epoch: 5| Step: 10
Training loss: 2.3610434532165527
Validation loss: 2.524576617825416

Epoch: 99| Step: 0
Training loss: 2.575488567352295
Validation loss: 2.5248690189853793

Epoch: 5| Step: 1
Training loss: 2.9939157962799072
Validation loss: 2.5310011294580277

Epoch: 5| Step: 2
Training loss: 3.3141276836395264
Validation loss: 2.536657871738557

Epoch: 5| Step: 3
Training loss: 2.6115546226501465
Validation loss: 2.5383097817820888

Epoch: 5| Step: 4
Training loss: 3.068726062774658
Validation loss: 2.533424413332375

Epoch: 5| Step: 5
Training loss: 2.4642767906188965
Validation loss: 2.5334379724276963

Epoch: 5| Step: 6
Training loss: 2.824791431427002
Validation loss: 2.525066452641641

Epoch: 5| Step: 7
Training loss: 2.119014263153076
Validation loss: 2.520164320545812

Epoch: 5| Step: 8
Training loss: 2.501863956451416
Validation loss: 2.520729813524472

Epoch: 5| Step: 9
Training loss: 3.0263068675994873
Validation loss: 2.522836723635274

Epoch: 5| Step: 10
Training loss: 2.188936233520508
Validation loss: 2.519042863640734

Epoch: 100| Step: 0
Training loss: 2.657433271408081
Validation loss: 2.5166815814151557

Epoch: 5| Step: 1
Training loss: 2.0690441131591797
Validation loss: 2.5213284902675177

Epoch: 5| Step: 2
Training loss: 2.6408071517944336
Validation loss: 2.5161071490216

Epoch: 5| Step: 3
Training loss: 2.7062435150146484
Validation loss: 2.5153051986489245

Epoch: 5| Step: 4
Training loss: 3.0544769763946533
Validation loss: 2.513785639116841

Epoch: 5| Step: 5
Training loss: 2.546372652053833
Validation loss: 2.5159789772443872

Epoch: 5| Step: 6
Training loss: 2.8606114387512207
Validation loss: 2.5119204674997637

Epoch: 5| Step: 7
Training loss: 1.91376531124115
Validation loss: 2.514791729629681

Epoch: 5| Step: 8
Training loss: 3.06807541847229
Validation loss: 2.5150991947420183

Epoch: 5| Step: 9
Training loss: 3.338919162750244
Validation loss: 2.5107947036784184

Epoch: 5| Step: 10
Training loss: 2.917884349822998
Validation loss: 2.5088128864124255

Epoch: 101| Step: 0
Training loss: 2.860639810562134
Validation loss: 2.507276722179946

Epoch: 5| Step: 1
Training loss: 2.6549320220947266
Validation loss: 2.506280750356695

Epoch: 5| Step: 2
Training loss: 2.6369194984436035
Validation loss: 2.514742851257324

Epoch: 5| Step: 3
Training loss: 2.7255444526672363
Validation loss: 2.515765525961435

Epoch: 5| Step: 4
Training loss: 1.9078922271728516
Validation loss: 2.52685228214469

Epoch: 5| Step: 5
Training loss: 3.3400750160217285
Validation loss: 2.531273272729689

Epoch: 5| Step: 6
Training loss: 3.181455373764038
Validation loss: 2.5283625843704387

Epoch: 5| Step: 7
Training loss: 2.043818235397339
Validation loss: 2.522556135731359

Epoch: 5| Step: 8
Training loss: 3.0341451168060303
Validation loss: 2.5156026860719085

Epoch: 5| Step: 9
Training loss: 2.9558920860290527
Validation loss: 2.504012042476285

Epoch: 5| Step: 10
Training loss: 2.3312036991119385
Validation loss: 2.5041597145859913

Epoch: 102| Step: 0
Training loss: 3.0314018726348877
Validation loss: 2.5004560178326023

Epoch: 5| Step: 1
Training loss: 2.789346218109131
Validation loss: 2.5004433560115036

Epoch: 5| Step: 2
Training loss: 2.963782548904419
Validation loss: 2.499749175963863

Epoch: 5| Step: 3
Training loss: 2.364590644836426
Validation loss: 2.499086123640819

Epoch: 5| Step: 4
Training loss: 2.878600597381592
Validation loss: 2.4969496316807245

Epoch: 5| Step: 5
Training loss: 2.3634796142578125
Validation loss: 2.4999915143494964

Epoch: 5| Step: 6
Training loss: 2.8383915424346924
Validation loss: 2.494404068557165

Epoch: 5| Step: 7
Training loss: 2.8210670948028564
Validation loss: 2.4961761249009

Epoch: 5| Step: 8
Training loss: 2.997199058532715
Validation loss: 2.4987091428490094

Epoch: 5| Step: 9
Training loss: 2.4378180503845215
Validation loss: 2.498955847114645

Epoch: 5| Step: 10
Training loss: 2.034914493560791
Validation loss: 2.500280918613557

Epoch: 103| Step: 0
Training loss: 2.9184811115264893
Validation loss: 2.5015995528108332

Epoch: 5| Step: 1
Training loss: 2.9944119453430176
Validation loss: 2.498183286318215

Epoch: 5| Step: 2
Training loss: 2.7939577102661133
Validation loss: 2.4988627074867167

Epoch: 5| Step: 3
Training loss: 2.540461778640747
Validation loss: 2.4982028238234983

Epoch: 5| Step: 4
Training loss: 2.082690715789795
Validation loss: 2.49604635084829

Epoch: 5| Step: 5
Training loss: 2.530332565307617
Validation loss: 2.5054020971380253

Epoch: 5| Step: 6
Training loss: 3.27008056640625
Validation loss: 2.5042476859143985

Epoch: 5| Step: 7
Training loss: 2.437504291534424
Validation loss: 2.5032896687907558

Epoch: 5| Step: 8
Training loss: 2.726928472518921
Validation loss: 2.5004228033045286

Epoch: 5| Step: 9
Training loss: 3.076190233230591
Validation loss: 2.507791870383806

Epoch: 5| Step: 10
Training loss: 2.09324312210083
Validation loss: 2.5007063111951275

Epoch: 104| Step: 0
Training loss: 2.9934301376342773
Validation loss: 2.5107260365639963

Epoch: 5| Step: 1
Training loss: 3.013549327850342
Validation loss: 2.516210130465928

Epoch: 5| Step: 2
Training loss: 2.4925787448883057
Validation loss: 2.504939358721497

Epoch: 5| Step: 3
Training loss: 3.2216219902038574
Validation loss: 2.4990240168827835

Epoch: 5| Step: 4
Training loss: 2.3540549278259277
Validation loss: 2.4979834454033965

Epoch: 5| Step: 5
Training loss: 2.5694656372070312
Validation loss: 2.495346433372908

Epoch: 5| Step: 6
Training loss: 2.037588119506836
Validation loss: 2.4968516698447605

Epoch: 5| Step: 7
Training loss: 2.073997974395752
Validation loss: 2.4955617535498833

Epoch: 5| Step: 8
Training loss: 3.3398642539978027
Validation loss: 2.495896070234237

Epoch: 5| Step: 9
Training loss: 2.6563267707824707
Validation loss: 2.5021375840710056

Epoch: 5| Step: 10
Training loss: 2.885287284851074
Validation loss: 2.50543350045399

Epoch: 105| Step: 0
Training loss: 2.929527521133423
Validation loss: 2.5128969453996226

Epoch: 5| Step: 1
Training loss: 2.4238030910491943
Validation loss: 2.5223761912315124

Epoch: 5| Step: 2
Training loss: 2.6431050300598145
Validation loss: 2.514346803388288

Epoch: 5| Step: 3
Training loss: 2.8452234268188477
Validation loss: 2.500650936557401

Epoch: 5| Step: 4
Training loss: 1.7593638896942139
Validation loss: 2.5002400695636706

Epoch: 5| Step: 5
Training loss: 2.787245035171509
Validation loss: 2.495230331215807

Epoch: 5| Step: 6
Training loss: 3.0941240787506104
Validation loss: 2.4905097535861436

Epoch: 5| Step: 7
Training loss: 2.71518874168396
Validation loss: 2.48757404153065

Epoch: 5| Step: 8
Training loss: 2.513035297393799
Validation loss: 2.4815149537978636

Epoch: 5| Step: 9
Training loss: 3.322551727294922
Validation loss: 2.4877689269281205

Epoch: 5| Step: 10
Training loss: 2.5514261722564697
Validation loss: 2.481494539527483

Epoch: 106| Step: 0
Training loss: 2.337836742401123
Validation loss: 2.4825056752850934

Epoch: 5| Step: 1
Training loss: 3.368321657180786
Validation loss: 2.485931582348321

Epoch: 5| Step: 2
Training loss: 2.653430223464966
Validation loss: 2.4844801964298373

Epoch: 5| Step: 3
Training loss: 2.671537399291992
Validation loss: 2.4826867734232256

Epoch: 5| Step: 4
Training loss: 2.976066827774048
Validation loss: 2.483400919104135

Epoch: 5| Step: 5
Training loss: 2.6629061698913574
Validation loss: 2.4827306065508115

Epoch: 5| Step: 6
Training loss: 2.98964786529541
Validation loss: 2.4808208762958484

Epoch: 5| Step: 7
Training loss: 2.1782824993133545
Validation loss: 2.479515673011862

Epoch: 5| Step: 8
Training loss: 3.043034791946411
Validation loss: 2.4757541379620953

Epoch: 5| Step: 9
Training loss: 2.011007785797119
Validation loss: 2.4815194991327103

Epoch: 5| Step: 10
Training loss: 2.581299304962158
Validation loss: 2.483493694695093

Epoch: 107| Step: 0
Training loss: 2.4685287475585938
Validation loss: 2.486118091050015

Epoch: 5| Step: 1
Training loss: 3.142198085784912
Validation loss: 2.487252673795146

Epoch: 5| Step: 2
Training loss: 2.788325786590576
Validation loss: 2.485994936317526

Epoch: 5| Step: 3
Training loss: 2.469454526901245
Validation loss: 2.488908754881992

Epoch: 5| Step: 4
Training loss: 2.38476824760437
Validation loss: 2.491559454189834

Epoch: 5| Step: 5
Training loss: 2.7806806564331055
Validation loss: 2.494473954682709

Epoch: 5| Step: 6
Training loss: 2.0265212059020996
Validation loss: 2.492785128214026

Epoch: 5| Step: 7
Training loss: 2.74580979347229
Validation loss: 2.5032528036384174

Epoch: 5| Step: 8
Training loss: 2.9234561920166016
Validation loss: 2.5015182033661874

Epoch: 5| Step: 9
Training loss: 2.7161550521850586
Validation loss: 2.5133770076177453

Epoch: 5| Step: 10
Training loss: 3.1469767093658447
Validation loss: 2.5027945016020086

Epoch: 108| Step: 0
Training loss: 2.4678900241851807
Validation loss: 2.4915237554939846

Epoch: 5| Step: 1
Training loss: 1.8020012378692627
Validation loss: 2.4776343658406246

Epoch: 5| Step: 2
Training loss: 3.262803316116333
Validation loss: 2.476172899687162

Epoch: 5| Step: 3
Training loss: 2.769319772720337
Validation loss: 2.47314445177714

Epoch: 5| Step: 4
Training loss: 2.5507054328918457
Validation loss: 2.4712040193619265

Epoch: 5| Step: 5
Training loss: 2.6931118965148926
Validation loss: 2.4738282593347694

Epoch: 5| Step: 6
Training loss: 3.04720401763916
Validation loss: 2.473612895575903

Epoch: 5| Step: 7
Training loss: 2.5500741004943848
Validation loss: 2.4745319376709642

Epoch: 5| Step: 8
Training loss: 3.141155958175659
Validation loss: 2.4725393454233804

Epoch: 5| Step: 9
Training loss: 2.116783618927002
Validation loss: 2.4703651192367717

Epoch: 5| Step: 10
Training loss: 3.205418586730957
Validation loss: 2.4734600282484487

Epoch: 109| Step: 0
Training loss: 3.5782196521759033
Validation loss: 2.4755218746841594

Epoch: 5| Step: 1
Training loss: 2.7986690998077393
Validation loss: 2.4760535019700245

Epoch: 5| Step: 2
Training loss: 2.304748773574829
Validation loss: 2.482258765928207

Epoch: 5| Step: 3
Training loss: 2.5983500480651855
Validation loss: 2.4764203615086053

Epoch: 5| Step: 4
Training loss: 2.7192187309265137
Validation loss: 2.4808132443376767

Epoch: 5| Step: 5
Training loss: 2.605408191680908
Validation loss: 2.47981422562753

Epoch: 5| Step: 6
Training loss: 2.686631679534912
Validation loss: 2.480821601806148

Epoch: 5| Step: 7
Training loss: 3.0911433696746826
Validation loss: 2.4741909298845517

Epoch: 5| Step: 8
Training loss: 2.2826523780822754
Validation loss: 2.4769378016071935

Epoch: 5| Step: 9
Training loss: 1.797827124595642
Validation loss: 2.4709604145378194

Epoch: 5| Step: 10
Training loss: 3.072381019592285
Validation loss: 2.474876585827079

Epoch: 110| Step: 0
Training loss: 2.516806125640869
Validation loss: 2.474619193743634

Epoch: 5| Step: 1
Training loss: 2.637798309326172
Validation loss: 2.476272343307413

Epoch: 5| Step: 2
Training loss: 2.2757999897003174
Validation loss: 2.4711922676332536

Epoch: 5| Step: 3
Training loss: 3.1667733192443848
Validation loss: 2.482195800350558

Epoch: 5| Step: 4
Training loss: 2.952425003051758
Validation loss: 2.482932093322918

Epoch: 5| Step: 5
Training loss: 2.1690421104431152
Validation loss: 2.4833057362546205

Epoch: 5| Step: 6
Training loss: 3.0894808769226074
Validation loss: 2.4779027610696773

Epoch: 5| Step: 7
Training loss: 3.1850907802581787
Validation loss: 2.477038857757404

Epoch: 5| Step: 8
Training loss: 2.5945699214935303
Validation loss: 2.4769075096294446

Epoch: 5| Step: 9
Training loss: 2.2117762565612793
Validation loss: 2.4759714654696885

Epoch: 5| Step: 10
Training loss: 2.640798568725586
Validation loss: 2.4719092179370183

Epoch: 111| Step: 0
Training loss: 2.429624557495117
Validation loss: 2.474566603219637

Epoch: 5| Step: 1
Training loss: 3.047471523284912
Validation loss: 2.4712669246940204

Epoch: 5| Step: 2
Training loss: 2.6165883541107178
Validation loss: 2.473153806501819

Epoch: 5| Step: 3
Training loss: 3.094114065170288
Validation loss: 2.475132547399049

Epoch: 5| Step: 4
Training loss: 3.03541898727417
Validation loss: 2.472977110134658

Epoch: 5| Step: 5
Training loss: 2.5744242668151855
Validation loss: 2.4713175245510635

Epoch: 5| Step: 6
Training loss: 2.7072315216064453
Validation loss: 2.471636595264558

Epoch: 5| Step: 7
Training loss: 2.6089236736297607
Validation loss: 2.466455323721773

Epoch: 5| Step: 8
Training loss: 2.5579657554626465
Validation loss: 2.465233605395081

Epoch: 5| Step: 9
Training loss: 2.3270747661590576
Validation loss: 2.4659958065197034

Epoch: 5| Step: 10
Training loss: 2.3564841747283936
Validation loss: 2.4698377886126117

Epoch: 112| Step: 0
Training loss: 2.6468536853790283
Validation loss: 2.4711445326446206

Epoch: 5| Step: 1
Training loss: 3.064608097076416
Validation loss: 2.468303347146639

Epoch: 5| Step: 2
Training loss: 3.286623477935791
Validation loss: 2.4754340879378782

Epoch: 5| Step: 3
Training loss: 2.6033921241760254
Validation loss: 2.4765832449800227

Epoch: 5| Step: 4
Training loss: 2.435098171234131
Validation loss: 2.4726702859324794

Epoch: 5| Step: 5
Training loss: 2.7417426109313965
Validation loss: 2.47293084923939

Epoch: 5| Step: 6
Training loss: 3.0977084636688232
Validation loss: 2.4669090599142094

Epoch: 5| Step: 7
Training loss: 1.9056384563446045
Validation loss: 2.4666631760135775

Epoch: 5| Step: 8
Training loss: 2.0350213050842285
Validation loss: 2.4670346988144742

Epoch: 5| Step: 9
Training loss: 2.7163443565368652
Validation loss: 2.4658732824428107

Epoch: 5| Step: 10
Training loss: 2.948338747024536
Validation loss: 2.46681869799091

Epoch: 113| Step: 0
Training loss: 2.4083988666534424
Validation loss: 2.4668307509473575

Epoch: 5| Step: 1
Training loss: 2.35298490524292
Validation loss: 2.4673519570340394

Epoch: 5| Step: 2
Training loss: 2.124788522720337
Validation loss: 2.4661659450941187

Epoch: 5| Step: 3
Training loss: 2.4767370223999023
Validation loss: 2.472464571716965

Epoch: 5| Step: 4
Training loss: 2.762380599975586
Validation loss: 2.475677719680212

Epoch: 5| Step: 5
Training loss: 1.9202102422714233
Validation loss: 2.473724575452907

Epoch: 5| Step: 6
Training loss: 3.0562450885772705
Validation loss: 2.4910334899861324

Epoch: 5| Step: 7
Training loss: 3.1460890769958496
Validation loss: 2.492816209793091

Epoch: 5| Step: 8
Training loss: 3.1773319244384766
Validation loss: 2.4911106273692143

Epoch: 5| Step: 9
Training loss: 2.8597452640533447
Validation loss: 2.485855464012392

Epoch: 5| Step: 10
Training loss: 3.195866346359253
Validation loss: 2.476767255413917

Epoch: 114| Step: 0
Training loss: 2.5182836055755615
Validation loss: 2.469220669038834

Epoch: 5| Step: 1
Training loss: 2.684619903564453
Validation loss: 2.4656929636514313

Epoch: 5| Step: 2
Training loss: 2.370286226272583
Validation loss: 2.4655983371119343

Epoch: 5| Step: 3
Training loss: 2.4889590740203857
Validation loss: 2.465196801770118

Epoch: 5| Step: 4
Training loss: 3.0496363639831543
Validation loss: 2.466579750020017

Epoch: 5| Step: 5
Training loss: 2.46224308013916
Validation loss: 2.463612487239222

Epoch: 5| Step: 6
Training loss: 2.963911533355713
Validation loss: 2.4622581158914874

Epoch: 5| Step: 7
Training loss: 2.1614201068878174
Validation loss: 2.4598380622043403

Epoch: 5| Step: 8
Training loss: 2.58587646484375
Validation loss: 2.463478595979752

Epoch: 5| Step: 9
Training loss: 2.8131039142608643
Validation loss: 2.4644527281484296

Epoch: 5| Step: 10
Training loss: 3.4587326049804688
Validation loss: 2.467264603543025

Epoch: 115| Step: 0
Training loss: 2.580777406692505
Validation loss: 2.4649875779305734

Epoch: 5| Step: 1
Training loss: 2.917696237564087
Validation loss: 2.4766625537667224

Epoch: 5| Step: 2
Training loss: 2.3007891178131104
Validation loss: 2.4704774425875757

Epoch: 5| Step: 3
Training loss: 2.1070516109466553
Validation loss: 2.4707016970521662

Epoch: 5| Step: 4
Training loss: 3.216346025466919
Validation loss: 2.4670651817834504

Epoch: 5| Step: 5
Training loss: 3.02824068069458
Validation loss: 2.4700387882929977

Epoch: 5| Step: 6
Training loss: 2.4125030040740967
Validation loss: 2.469134542249864

Epoch: 5| Step: 7
Training loss: 2.3215105533599854
Validation loss: 2.4718751368984098

Epoch: 5| Step: 8
Training loss: 2.7230422496795654
Validation loss: 2.468922781687911

Epoch: 5| Step: 9
Training loss: 2.663546085357666
Validation loss: 2.4716500402778707

Epoch: 5| Step: 10
Training loss: 3.157289743423462
Validation loss: 2.4707049503121326

Epoch: 116| Step: 0
Training loss: 2.4044857025146484
Validation loss: 2.47546826383119

Epoch: 5| Step: 1
Training loss: 2.6341872215270996
Validation loss: 2.473580014321112

Epoch: 5| Step: 2
Training loss: 3.119413375854492
Validation loss: 2.4730663863561486

Epoch: 5| Step: 3
Training loss: 3.309910535812378
Validation loss: 2.480373926060174

Epoch: 5| Step: 4
Training loss: 2.3984358310699463
Validation loss: 2.47313513294343

Epoch: 5| Step: 5
Training loss: 3.193445920944214
Validation loss: 2.477586500106319

Epoch: 5| Step: 6
Training loss: 3.0319457054138184
Validation loss: 2.4725394889872563

Epoch: 5| Step: 7
Training loss: 2.6972579956054688
Validation loss: 2.4717762521518174

Epoch: 5| Step: 8
Training loss: 1.763418436050415
Validation loss: 2.47160699290614

Epoch: 5| Step: 9
Training loss: 2.0012595653533936
Validation loss: 2.4657828064375025

Epoch: 5| Step: 10
Training loss: 2.8636691570281982
Validation loss: 2.4713543717579176

Epoch: 117| Step: 0
Training loss: 2.7234010696411133
Validation loss: 2.4667160511016846

Epoch: 5| Step: 1
Training loss: 3.377537250518799
Validation loss: 2.466481662565662

Epoch: 5| Step: 2
Training loss: 2.521824598312378
Validation loss: 2.4667764889296664

Epoch: 5| Step: 3
Training loss: 2.225893497467041
Validation loss: 2.4639284021110943

Epoch: 5| Step: 4
Training loss: 2.4880311489105225
Validation loss: 2.4757634414139615

Epoch: 5| Step: 5
Training loss: 2.406769275665283
Validation loss: 2.478466308245095

Epoch: 5| Step: 6
Training loss: 2.7362465858459473
Validation loss: 2.4746960004170737

Epoch: 5| Step: 7
Training loss: 2.564307689666748
Validation loss: 2.4691054667195966

Epoch: 5| Step: 8
Training loss: 2.5977680683135986
Validation loss: 2.463279237029373

Epoch: 5| Step: 9
Training loss: 2.724687337875366
Validation loss: 2.4599836462287494

Epoch: 5| Step: 10
Training loss: 2.979750871658325
Validation loss: 2.4653576010016987

Epoch: 118| Step: 0
Training loss: 2.9017627239227295
Validation loss: 2.460415527384768

Epoch: 5| Step: 1
Training loss: 2.5206425189971924
Validation loss: 2.465270314165341

Epoch: 5| Step: 2
Training loss: 2.3889341354370117
Validation loss: 2.458375815422304

Epoch: 5| Step: 3
Training loss: 3.096954822540283
Validation loss: 2.4663740716954714

Epoch: 5| Step: 4
Training loss: 2.011413097381592
Validation loss: 2.4717116766078497

Epoch: 5| Step: 5
Training loss: 2.791196346282959
Validation loss: 2.4755522589529715

Epoch: 5| Step: 6
Training loss: 2.1189334392547607
Validation loss: 2.489514486764067

Epoch: 5| Step: 7
Training loss: 3.3273892402648926
Validation loss: 2.483721084492181

Epoch: 5| Step: 8
Training loss: 2.3093490600585938
Validation loss: 2.4849509064869215

Epoch: 5| Step: 9
Training loss: 2.77164363861084
Validation loss: 2.4737957344260266

Epoch: 5| Step: 10
Training loss: 3.214779853820801
Validation loss: 2.462271093040384

Epoch: 119| Step: 0
Training loss: 2.1714720726013184
Validation loss: 2.4603636956984

Epoch: 5| Step: 1
Training loss: 2.4951462745666504
Validation loss: 2.461290610733853

Epoch: 5| Step: 2
Training loss: 2.3518428802490234
Validation loss: 2.459171987348987

Epoch: 5| Step: 3
Training loss: 3.106043577194214
Validation loss: 2.460741860892183

Epoch: 5| Step: 4
Training loss: 2.308910846710205
Validation loss: 2.4582487511378464

Epoch: 5| Step: 5
Training loss: 3.0126893520355225
Validation loss: 2.4583394732526553

Epoch: 5| Step: 6
Training loss: 2.079596996307373
Validation loss: 2.457883947639055

Epoch: 5| Step: 7
Training loss: 3.217346668243408
Validation loss: 2.4604802798199397

Epoch: 5| Step: 8
Training loss: 3.4291930198669434
Validation loss: 2.4584464744854997

Epoch: 5| Step: 9
Training loss: 2.6651763916015625
Validation loss: 2.457570557953209

Epoch: 5| Step: 10
Training loss: 2.3774116039276123
Validation loss: 2.4551471740968767

Epoch: 120| Step: 0
Training loss: 3.100411891937256
Validation loss: 2.46276750615848

Epoch: 5| Step: 1
Training loss: 2.591265916824341
Validation loss: 2.4654931406820975

Epoch: 5| Step: 2
Training loss: 3.1115193367004395
Validation loss: 2.461359570103307

Epoch: 5| Step: 3
Training loss: 2.3893771171569824
Validation loss: 2.4674551717696653

Epoch: 5| Step: 4
Training loss: 2.63334059715271
Validation loss: 2.457682071193572

Epoch: 5| Step: 5
Training loss: 2.3212900161743164
Validation loss: 2.459860950387934

Epoch: 5| Step: 6
Training loss: 2.466280937194824
Validation loss: 2.462708565496629

Epoch: 5| Step: 7
Training loss: 2.6757287979125977
Validation loss: 2.4595940497613724

Epoch: 5| Step: 8
Training loss: 2.197909355163574
Validation loss: 2.459904847606536

Epoch: 5| Step: 9
Training loss: 2.989717721939087
Validation loss: 2.462562989163142

Epoch: 5| Step: 10
Training loss: 2.8110578060150146
Validation loss: 2.4596340758826143

Epoch: 121| Step: 0
Training loss: 3.0030617713928223
Validation loss: 2.4626013591725338

Epoch: 5| Step: 1
Training loss: 2.506381034851074
Validation loss: 2.4683315343754266

Epoch: 5| Step: 2
Training loss: 3.3799004554748535
Validation loss: 2.4606047215000277

Epoch: 5| Step: 3
Training loss: 2.8528542518615723
Validation loss: 2.454010381493517

Epoch: 5| Step: 4
Training loss: 2.5406460762023926
Validation loss: 2.4559717332163165

Epoch: 5| Step: 5
Training loss: 3.1537935733795166
Validation loss: 2.4558190735437537

Epoch: 5| Step: 6
Training loss: 2.143860101699829
Validation loss: 2.45973644205319

Epoch: 5| Step: 7
Training loss: 3.3233799934387207
Validation loss: 2.4630112237827753

Epoch: 5| Step: 8
Training loss: 1.7327768802642822
Validation loss: 2.468391390256984

Epoch: 5| Step: 9
Training loss: 1.7402549982070923
Validation loss: 2.4666518165219213

Epoch: 5| Step: 10
Training loss: 2.9043898582458496
Validation loss: 2.47036212746815

Epoch: 122| Step: 0
Training loss: 3.606931209564209
Validation loss: 2.474750634162657

Epoch: 5| Step: 1
Training loss: 2.914114475250244
Validation loss: 2.4665712759058964

Epoch: 5| Step: 2
Training loss: 2.1736629009246826
Validation loss: 2.4690768180354947

Epoch: 5| Step: 3
Training loss: 2.5194101333618164
Validation loss: 2.458378109880673

Epoch: 5| Step: 4
Training loss: 1.596897006034851
Validation loss: 2.460164508511943

Epoch: 5| Step: 5
Training loss: 2.7778515815734863
Validation loss: 2.463579749548307

Epoch: 5| Step: 6
Training loss: 2.691159725189209
Validation loss: 2.4651012805200394

Epoch: 5| Step: 7
Training loss: 2.6071617603302
Validation loss: 2.4697657964562856

Epoch: 5| Step: 8
Training loss: 2.759571075439453
Validation loss: 2.4733961525783745

Epoch: 5| Step: 9
Training loss: 2.350440502166748
Validation loss: 2.4707397414791967

Epoch: 5| Step: 10
Training loss: 3.395845413208008
Validation loss: 2.4724454367032616

Epoch: 123| Step: 0
Training loss: 2.5805585384368896
Validation loss: 2.4706135103779454

Epoch: 5| Step: 1
Training loss: 1.9992179870605469
Validation loss: 2.4643537075288835

Epoch: 5| Step: 2
Training loss: 2.3073322772979736
Validation loss: 2.459105965911701

Epoch: 5| Step: 3
Training loss: 2.5526986122131348
Validation loss: 2.4575471672960507

Epoch: 5| Step: 4
Training loss: 2.558903932571411
Validation loss: 2.4554827982379543

Epoch: 5| Step: 5
Training loss: 3.5131843090057373
Validation loss: 2.457027591684813

Epoch: 5| Step: 6
Training loss: 3.138584852218628
Validation loss: 2.4540316520198697

Epoch: 5| Step: 7
Training loss: 2.200347423553467
Validation loss: 2.4552921531020955

Epoch: 5| Step: 8
Training loss: 2.671705722808838
Validation loss: 2.453699537502822

Epoch: 5| Step: 9
Training loss: 3.123997211456299
Validation loss: 2.4532224337259927

Epoch: 5| Step: 10
Training loss: 2.636263608932495
Validation loss: 2.4557412465413413

Epoch: 124| Step: 0
Training loss: 2.174161911010742
Validation loss: 2.4542518559322564

Epoch: 5| Step: 1
Training loss: 3.12392520904541
Validation loss: 2.4546941467510757

Epoch: 5| Step: 2
Training loss: 2.2417781352996826
Validation loss: 2.4587661963637157

Epoch: 5| Step: 3
Training loss: 3.0561485290527344
Validation loss: 2.454457595784177

Epoch: 5| Step: 4
Training loss: 2.935415267944336
Validation loss: 2.451405873862646

Epoch: 5| Step: 5
Training loss: 3.087118625640869
Validation loss: 2.4527399668129544

Epoch: 5| Step: 6
Training loss: 2.6835124492645264
Validation loss: 2.450771326659828

Epoch: 5| Step: 7
Training loss: 2.2923688888549805
Validation loss: 2.4540592803749988

Epoch: 5| Step: 8
Training loss: 2.265434503555298
Validation loss: 2.449165831330002

Epoch: 5| Step: 9
Training loss: 2.6116256713867188
Validation loss: 2.4511791736848894

Epoch: 5| Step: 10
Training loss: 2.7734997272491455
Validation loss: 2.4580024365455873

Epoch: 125| Step: 0
Training loss: 2.5025672912597656
Validation loss: 2.4660861133247294

Epoch: 5| Step: 1
Training loss: 2.282735586166382
Validation loss: 2.472019915939659

Epoch: 5| Step: 2
Training loss: 2.6805310249328613
Validation loss: 2.4888349733045025

Epoch: 5| Step: 3
Training loss: 2.6059186458587646
Validation loss: 2.499594085959978

Epoch: 5| Step: 4
Training loss: 2.4316563606262207
Validation loss: 2.4916145545180126

Epoch: 5| Step: 5
Training loss: 3.5278191566467285
Validation loss: 2.4840410678617415

Epoch: 5| Step: 6
Training loss: 2.7520759105682373
Validation loss: 2.4762676890178392

Epoch: 5| Step: 7
Training loss: 3.0965378284454346
Validation loss: 2.4742317456071095

Epoch: 5| Step: 8
Training loss: 2.598100423812866
Validation loss: 2.459009624296619

Epoch: 5| Step: 9
Training loss: 2.814736843109131
Validation loss: 2.4569912982243363

Epoch: 5| Step: 10
Training loss: 1.9255162477493286
Validation loss: 2.4541183056369906

Epoch: 126| Step: 0
Training loss: 2.8299665451049805
Validation loss: 2.452604498914493

Epoch: 5| Step: 1
Training loss: 3.2938430309295654
Validation loss: 2.4494709378929547

Epoch: 5| Step: 2
Training loss: 2.7867953777313232
Validation loss: 2.454487116106095

Epoch: 5| Step: 3
Training loss: 2.391742706298828
Validation loss: 2.451634563425536

Epoch: 5| Step: 4
Training loss: 3.290785312652588
Validation loss: 2.4522226036235852

Epoch: 5| Step: 5
Training loss: 2.5031824111938477
Validation loss: 2.4509226019664476

Epoch: 5| Step: 6
Training loss: 2.1659154891967773
Validation loss: 2.446917277510448

Epoch: 5| Step: 7
Training loss: 2.2348036766052246
Validation loss: 2.450285452668385

Epoch: 5| Step: 8
Training loss: 2.5934975147247314
Validation loss: 2.459034691574753

Epoch: 5| Step: 9
Training loss: 2.60813570022583
Validation loss: 2.466559825404998

Epoch: 5| Step: 10
Training loss: 2.5482330322265625
Validation loss: 2.465850278895388

Epoch: 127| Step: 0
Training loss: 2.757124423980713
Validation loss: 2.473345297639088

Epoch: 5| Step: 1
Training loss: 3.054213285446167
Validation loss: 2.4684539225793656

Epoch: 5| Step: 2
Training loss: 1.9682931900024414
Validation loss: 2.4632840156555176

Epoch: 5| Step: 3
Training loss: 2.146210193634033
Validation loss: 2.4488482603462796

Epoch: 5| Step: 4
Training loss: 2.2296204566955566
Validation loss: 2.449449795548634

Epoch: 5| Step: 5
Training loss: 2.9225082397460938
Validation loss: 2.446135018461494

Epoch: 5| Step: 6
Training loss: 2.7923741340637207
Validation loss: 2.4470695193095873

Epoch: 5| Step: 7
Training loss: 2.6683883666992188
Validation loss: 2.445976741852299

Epoch: 5| Step: 8
Training loss: 2.693500280380249
Validation loss: 2.4438084428028395

Epoch: 5| Step: 9
Training loss: 3.4309463500976562
Validation loss: 2.4487085701316915

Epoch: 5| Step: 10
Training loss: 2.5524163246154785
Validation loss: 2.4453613783723567

Epoch: 128| Step: 0
Training loss: 3.0938971042633057
Validation loss: 2.452628638154717

Epoch: 5| Step: 1
Training loss: 2.0132369995117188
Validation loss: 2.4542553809381302

Epoch: 5| Step: 2
Training loss: 1.8259518146514893
Validation loss: 2.4603971409541305

Epoch: 5| Step: 3
Training loss: 2.6065165996551514
Validation loss: 2.4542039158523723

Epoch: 5| Step: 4
Training loss: 3.028648853302002
Validation loss: 2.4598286228795208

Epoch: 5| Step: 5
Training loss: 2.5873160362243652
Validation loss: 2.45570344309653

Epoch: 5| Step: 6
Training loss: 2.203016757965088
Validation loss: 2.455900853680026

Epoch: 5| Step: 7
Training loss: 3.5085983276367188
Validation loss: 2.4531757831573486

Epoch: 5| Step: 8
Training loss: 3.020179271697998
Validation loss: 2.4533245178960983

Epoch: 5| Step: 9
Training loss: 2.608558177947998
Validation loss: 2.4477228092890915

Epoch: 5| Step: 10
Training loss: 2.785283327102661
Validation loss: 2.450045249795401

Epoch: 129| Step: 0
Training loss: 2.890514373779297
Validation loss: 2.442794192221857

Epoch: 5| Step: 1
Training loss: 2.9638419151306152
Validation loss: 2.447871149227183

Epoch: 5| Step: 2
Training loss: 2.7004714012145996
Validation loss: 2.4431661021324897

Epoch: 5| Step: 3
Training loss: 1.9709889888763428
Validation loss: 2.4486467402468444

Epoch: 5| Step: 4
Training loss: 2.7851552963256836
Validation loss: 2.444210857473394

Epoch: 5| Step: 5
Training loss: 2.94530987739563
Validation loss: 2.441155041417768

Epoch: 5| Step: 6
Training loss: 2.417052984237671
Validation loss: 2.4437790070810625

Epoch: 5| Step: 7
Training loss: 2.5375561714172363
Validation loss: 2.443587872289842

Epoch: 5| Step: 8
Training loss: 2.846036434173584
Validation loss: 2.4406334943668817

Epoch: 5| Step: 9
Training loss: 2.1759984493255615
Validation loss: 2.4393231458561395

Epoch: 5| Step: 10
Training loss: 3.0893337726593018
Validation loss: 2.4439451668852117

Epoch: 130| Step: 0
Training loss: 2.7621347904205322
Validation loss: 2.444488489499656

Epoch: 5| Step: 1
Training loss: 3.2684597969055176
Validation loss: 2.4468733687554636

Epoch: 5| Step: 2
Training loss: 2.394460439682007
Validation loss: 2.444732327615061

Epoch: 5| Step: 3
Training loss: 2.187901020050049
Validation loss: 2.441226061954293

Epoch: 5| Step: 4
Training loss: 2.599424362182617
Validation loss: 2.4440066673422374

Epoch: 5| Step: 5
Training loss: 2.732374906539917
Validation loss: 2.4446082166446153

Epoch: 5| Step: 6
Training loss: 2.741750955581665
Validation loss: 2.44508292085381

Epoch: 5| Step: 7
Training loss: 2.9230778217315674
Validation loss: 2.4454917394986717

Epoch: 5| Step: 8
Training loss: 2.41418194770813
Validation loss: 2.453092562255039

Epoch: 5| Step: 9
Training loss: 2.60313081741333
Validation loss: 2.4520318943967103

Epoch: 5| Step: 10
Training loss: 2.540903091430664
Validation loss: 2.4431411194544967

Epoch: 131| Step: 0
Training loss: 2.65071702003479
Validation loss: 2.44718732244225

Epoch: 5| Step: 1
Training loss: 1.821046233177185
Validation loss: 2.448865718739007

Epoch: 5| Step: 2
Training loss: 3.606843948364258
Validation loss: 2.454034061842067

Epoch: 5| Step: 3
Training loss: 2.846844434738159
Validation loss: 2.453840873574698

Epoch: 5| Step: 4
Training loss: 2.5911059379577637
Validation loss: 2.455394939709735

Epoch: 5| Step: 5
Training loss: 2.658583879470825
Validation loss: 2.4545229737476637

Epoch: 5| Step: 6
Training loss: 2.5394835472106934
Validation loss: 2.451023737589518

Epoch: 5| Step: 7
Training loss: 2.3135199546813965
Validation loss: 2.4485352962247786

Epoch: 5| Step: 8
Training loss: 2.9194884300231934
Validation loss: 2.4440322691394436

Epoch: 5| Step: 9
Training loss: 3.1211555004119873
Validation loss: 2.4453057524978474

Epoch: 5| Step: 10
Training loss: 2.014930248260498
Validation loss: 2.4464232895963933

Epoch: 132| Step: 0
Training loss: 2.2016139030456543
Validation loss: 2.4404662834700717

Epoch: 5| Step: 1
Training loss: 2.9796595573425293
Validation loss: 2.4433232084397347

Epoch: 5| Step: 2
Training loss: 2.3991668224334717
Validation loss: 2.4400428187462593

Epoch: 5| Step: 3
Training loss: 2.735142946243286
Validation loss: 2.4413962082196305

Epoch: 5| Step: 4
Training loss: 2.7423152923583984
Validation loss: 2.440232161552675

Epoch: 5| Step: 5
Training loss: 2.565521240234375
Validation loss: 2.440796252219908

Epoch: 5| Step: 6
Training loss: 3.1363296508789062
Validation loss: 2.4410896916543283

Epoch: 5| Step: 7
Training loss: 2.387787342071533
Validation loss: 2.442815067947552

Epoch: 5| Step: 8
Training loss: 2.181304693222046
Validation loss: 2.4419882605152745

Epoch: 5| Step: 9
Training loss: 2.572204113006592
Validation loss: 2.4442535523445375

Epoch: 5| Step: 10
Training loss: 3.425919771194458
Validation loss: 2.4489183733540196

Epoch: 133| Step: 0
Training loss: 2.5783956050872803
Validation loss: 2.4466120145654164

Epoch: 5| Step: 1
Training loss: 2.7235605716705322
Validation loss: 2.4470921049835863

Epoch: 5| Step: 2
Training loss: 2.6995387077331543
Validation loss: 2.4399938019373084

Epoch: 5| Step: 3
Training loss: 3.0104079246520996
Validation loss: 2.4411560489285375

Epoch: 5| Step: 4
Training loss: 2.406325340270996
Validation loss: 2.4402452002289476

Epoch: 5| Step: 5
Training loss: 2.2035999298095703
Validation loss: 2.441099102779101

Epoch: 5| Step: 6
Training loss: 3.095567226409912
Validation loss: 2.4435242965657222

Epoch: 5| Step: 7
Training loss: 1.983605980873108
Validation loss: 2.4407416953835437

Epoch: 5| Step: 8
Training loss: 2.62306809425354
Validation loss: 2.449276139659266

Epoch: 5| Step: 9
Training loss: 2.507225751876831
Validation loss: 2.447665329902403

Epoch: 5| Step: 10
Training loss: 3.466249465942383
Validation loss: 2.4477855031208327

Epoch: 134| Step: 0
Training loss: 2.710061550140381
Validation loss: 2.443184362944736

Epoch: 5| Step: 1
Training loss: 2.3411386013031006
Validation loss: 2.4356402171555387

Epoch: 5| Step: 2
Training loss: 2.2147204875946045
Validation loss: 2.4330441900478896

Epoch: 5| Step: 3
Training loss: 2.128690004348755
Validation loss: 2.434559458045549

Epoch: 5| Step: 4
Training loss: 3.0245487689971924
Validation loss: 2.4363610052293345

Epoch: 5| Step: 5
Training loss: 3.2596497535705566
Validation loss: 2.436572144108434

Epoch: 5| Step: 6
Training loss: 2.8422698974609375
Validation loss: 2.4362053050789783

Epoch: 5| Step: 7
Training loss: 2.4556472301483154
Validation loss: 2.4394000832752516

Epoch: 5| Step: 8
Training loss: 2.8318066596984863
Validation loss: 2.4411060681907077

Epoch: 5| Step: 9
Training loss: 2.29487681388855
Validation loss: 2.4387330034727692

Epoch: 5| Step: 10
Training loss: 3.0843143463134766
Validation loss: 2.445124098049697

Epoch: 135| Step: 0
Training loss: 3.1752309799194336
Validation loss: 2.4438953835477113

Epoch: 5| Step: 1
Training loss: 2.655705690383911
Validation loss: 2.4452487999393093

Epoch: 5| Step: 2
Training loss: 2.3181166648864746
Validation loss: 2.444957994645642

Epoch: 5| Step: 3
Training loss: 2.8425495624542236
Validation loss: 2.4493237541567896

Epoch: 5| Step: 4
Training loss: 2.371220588684082
Validation loss: 2.445877423850439

Epoch: 5| Step: 5
Training loss: 3.738018751144409
Validation loss: 2.4387572785859466

Epoch: 5| Step: 6
Training loss: 2.3501405715942383
Validation loss: 2.4375591765167894

Epoch: 5| Step: 7
Training loss: 2.7827060222625732
Validation loss: 2.436111463013516

Epoch: 5| Step: 8
Training loss: 2.3914811611175537
Validation loss: 2.4358286267967633

Epoch: 5| Step: 9
Training loss: 2.4837257862091064
Validation loss: 2.434421605961297

Epoch: 5| Step: 10
Training loss: 1.937846064567566
Validation loss: 2.4358689067184285

Epoch: 136| Step: 0
Training loss: 2.515285015106201
Validation loss: 2.4353929770890104

Epoch: 5| Step: 1
Training loss: 2.7517549991607666
Validation loss: 2.4343162172584125

Epoch: 5| Step: 2
Training loss: 3.236241102218628
Validation loss: 2.4408792885400916

Epoch: 5| Step: 3
Training loss: 1.9549601078033447
Validation loss: 2.440825352104761

Epoch: 5| Step: 4
Training loss: 2.6930599212646484
Validation loss: 2.4502586600601033

Epoch: 5| Step: 5
Training loss: 2.8908584117889404
Validation loss: 2.4499612316008537

Epoch: 5| Step: 6
Training loss: 2.6193888187408447
Validation loss: 2.4436054255372737

Epoch: 5| Step: 7
Training loss: 2.4869916439056396
Validation loss: 2.4452837846612416

Epoch: 5| Step: 8
Training loss: 2.058997631072998
Validation loss: 2.4445980184821674

Epoch: 5| Step: 9
Training loss: 3.1148478984832764
Validation loss: 2.4388727680329354

Epoch: 5| Step: 10
Training loss: 2.82209849357605
Validation loss: 2.4359119579356205

Epoch: 137| Step: 0
Training loss: 2.3653080463409424
Validation loss: 2.4360457671585904

Epoch: 5| Step: 1
Training loss: 2.501614809036255
Validation loss: 2.4375972799075547

Epoch: 5| Step: 2
Training loss: 2.631748676300049
Validation loss: 2.4416964900109077

Epoch: 5| Step: 3
Training loss: 2.816303253173828
Validation loss: 2.43782114982605

Epoch: 5| Step: 4
Training loss: 2.3995048999786377
Validation loss: 2.4374533596859185

Epoch: 5| Step: 5
Training loss: 2.3080592155456543
Validation loss: 2.4375196323599866

Epoch: 5| Step: 6
Training loss: 2.060220718383789
Validation loss: 2.449576880342217

Epoch: 5| Step: 7
Training loss: 3.4173827171325684
Validation loss: 2.443906799439461

Epoch: 5| Step: 8
Training loss: 2.7212328910827637
Validation loss: 2.4521210398725284

Epoch: 5| Step: 9
Training loss: 3.2088093757629395
Validation loss: 2.4540172699959046

Epoch: 5| Step: 10
Training loss: 2.7748162746429443
Validation loss: 2.453415588666034

Epoch: 138| Step: 0
Training loss: 2.8582005500793457
Validation loss: 2.4433780639402327

Epoch: 5| Step: 1
Training loss: 2.8238399028778076
Validation loss: 2.4345177424851285

Epoch: 5| Step: 2
Training loss: 2.1001763343811035
Validation loss: 2.4301170995158534

Epoch: 5| Step: 3
Training loss: 2.4083993434906006
Validation loss: 2.430540871876542

Epoch: 5| Step: 4
Training loss: 2.3049330711364746
Validation loss: 2.435213747844901

Epoch: 5| Step: 5
Training loss: 3.1176769733428955
Validation loss: 2.4294343609963693

Epoch: 5| Step: 6
Training loss: 2.8715901374816895
Validation loss: 2.428472436884398

Epoch: 5| Step: 7
Training loss: 2.8579251766204834
Validation loss: 2.4244951663478727

Epoch: 5| Step: 8
Training loss: 2.3250720500946045
Validation loss: 2.4224497195213073

Epoch: 5| Step: 9
Training loss: 2.4706554412841797
Validation loss: 2.4299880381553405

Epoch: 5| Step: 10
Training loss: 2.9986515045166016
Validation loss: 2.4319501871703775

Epoch: 139| Step: 0
Training loss: 2.7396233081817627
Validation loss: 2.433261230427732

Epoch: 5| Step: 1
Training loss: 1.7675650119781494
Validation loss: 2.432721217473348

Epoch: 5| Step: 2
Training loss: 2.86879301071167
Validation loss: 2.4307449351074877

Epoch: 5| Step: 3
Training loss: 3.2165024280548096
Validation loss: 2.4323124347194547

Epoch: 5| Step: 4
Training loss: 2.2935385704040527
Validation loss: 2.436532615333475

Epoch: 5| Step: 5
Training loss: 3.016435384750366
Validation loss: 2.4394974811102754

Epoch: 5| Step: 6
Training loss: 1.9464082717895508
Validation loss: 2.441633475724087

Epoch: 5| Step: 7
Training loss: 3.18428897857666
Validation loss: 2.434488963055354

Epoch: 5| Step: 8
Training loss: 2.932276964187622
Validation loss: 2.4332459921477945

Epoch: 5| Step: 9
Training loss: 2.223947048187256
Validation loss: 2.4293108063359417

Epoch: 5| Step: 10
Training loss: 2.9620611667633057
Validation loss: 2.4358583650281354

Epoch: 140| Step: 0
Training loss: 2.5217134952545166
Validation loss: 2.428548700066023

Epoch: 5| Step: 1
Training loss: 2.817793607711792
Validation loss: 2.4316351182999147

Epoch: 5| Step: 2
Training loss: 2.0488157272338867
Validation loss: 2.428537017555647

Epoch: 5| Step: 3
Training loss: 3.16839337348938
Validation loss: 2.4242805255356656

Epoch: 5| Step: 4
Training loss: 2.517085313796997
Validation loss: 2.43127001741881

Epoch: 5| Step: 5
Training loss: 2.3544270992279053
Validation loss: 2.429813859283283

Epoch: 5| Step: 6
Training loss: 2.473728656768799
Validation loss: 2.429415515674058

Epoch: 5| Step: 7
Training loss: 2.8370845317840576
Validation loss: 2.4300878432489212

Epoch: 5| Step: 8
Training loss: 2.8002681732177734
Validation loss: 2.431978514117579

Epoch: 5| Step: 9
Training loss: 2.7410709857940674
Validation loss: 2.4330821344929356

Epoch: 5| Step: 10
Training loss: 2.840786933898926
Validation loss: 2.4344131613290436

Epoch: 141| Step: 0
Training loss: 2.808894157409668
Validation loss: 2.4378840102944324

Epoch: 5| Step: 1
Training loss: 2.773068904876709
Validation loss: 2.444594362730621

Epoch: 5| Step: 2
Training loss: 1.8188282251358032
Validation loss: 2.439611586191321

Epoch: 5| Step: 3
Training loss: 2.805372714996338
Validation loss: 2.4413120054429576

Epoch: 5| Step: 4
Training loss: 2.7301876544952393
Validation loss: 2.437794672545566

Epoch: 5| Step: 5
Training loss: 2.749526023864746
Validation loss: 2.436595719347718

Epoch: 5| Step: 6
Training loss: 2.558105945587158
Validation loss: 2.4339689464979273

Epoch: 5| Step: 7
Training loss: 2.150203227996826
Validation loss: 2.4323693090869534

Epoch: 5| Step: 8
Training loss: 3.1444833278656006
Validation loss: 2.4306793482072893

Epoch: 5| Step: 9
Training loss: 3.2334187030792236
Validation loss: 2.426035855406074

Epoch: 5| Step: 10
Training loss: 2.222914695739746
Validation loss: 2.4291321436564126

Epoch: 142| Step: 0
Training loss: 2.4153172969818115
Validation loss: 2.424929611144527

Epoch: 5| Step: 1
Training loss: 2.787186622619629
Validation loss: 2.425424850115212

Epoch: 5| Step: 2
Training loss: 2.5209648609161377
Validation loss: 2.4278087359602734

Epoch: 5| Step: 3
Training loss: 2.9763591289520264
Validation loss: 2.4326428546700427

Epoch: 5| Step: 4
Training loss: 2.7439870834350586
Validation loss: 2.437645355860392

Epoch: 5| Step: 5
Training loss: 2.68495774269104
Validation loss: 2.43801821431806

Epoch: 5| Step: 6
Training loss: 3.0508627891540527
Validation loss: 2.438991262066749

Epoch: 5| Step: 7
Training loss: 2.680109977722168
Validation loss: 2.446801813699866

Epoch: 5| Step: 8
Training loss: 2.5187859535217285
Validation loss: 2.439002962522609

Epoch: 5| Step: 9
Training loss: 2.300523281097412
Validation loss: 2.428121377063054

Epoch: 5| Step: 10
Training loss: 2.350951910018921
Validation loss: 2.4256860927868913

Epoch: 143| Step: 0
Training loss: 2.526456356048584
Validation loss: 2.4298446114345262

Epoch: 5| Step: 1
Training loss: 2.3989906311035156
Validation loss: 2.426862726929367

Epoch: 5| Step: 2
Training loss: 3.05574369430542
Validation loss: 2.4292650120232695

Epoch: 5| Step: 3
Training loss: 2.83359694480896
Validation loss: 2.428659515996133

Epoch: 5| Step: 4
Training loss: 2.0283169746398926
Validation loss: 2.4264649960302536

Epoch: 5| Step: 5
Training loss: 2.755378484725952
Validation loss: 2.4285290266877864

Epoch: 5| Step: 6
Training loss: 2.611128568649292
Validation loss: 2.429820286330356

Epoch: 5| Step: 7
Training loss: 3.0315616130828857
Validation loss: 2.426858550758772

Epoch: 5| Step: 8
Training loss: 2.8493189811706543
Validation loss: 2.4324462234332995

Epoch: 5| Step: 9
Training loss: 2.696580410003662
Validation loss: 2.4278425990894275

Epoch: 5| Step: 10
Training loss: 2.153510332107544
Validation loss: 2.4319527687564975

Epoch: 144| Step: 0
Training loss: 2.5697178840637207
Validation loss: 2.4280911081580707

Epoch: 5| Step: 1
Training loss: 2.799896240234375
Validation loss: 2.428941075519849

Epoch: 5| Step: 2
Training loss: 2.128624677658081
Validation loss: 2.4309963615991736

Epoch: 5| Step: 3
Training loss: 2.1621837615966797
Validation loss: 2.424695950682445

Epoch: 5| Step: 4
Training loss: 2.6325466632843018
Validation loss: 2.434503327133835

Epoch: 5| Step: 5
Training loss: 2.664904832839966
Validation loss: 2.4288287983145764

Epoch: 5| Step: 6
Training loss: 2.2670047283172607
Validation loss: 2.4290230940747004

Epoch: 5| Step: 7
Training loss: 3.16190767288208
Validation loss: 2.4332956678123883

Epoch: 5| Step: 8
Training loss: 2.6014766693115234
Validation loss: 2.4375352269859722

Epoch: 5| Step: 9
Training loss: 3.157263994216919
Validation loss: 2.4452505342421995

Epoch: 5| Step: 10
Training loss: 2.956216812133789
Validation loss: 2.4337473889832855

Epoch: 145| Step: 0
Training loss: 2.7514712810516357
Validation loss: 2.4267759041119645

Epoch: 5| Step: 1
Training loss: 2.849947214126587
Validation loss: 2.4258825881506807

Epoch: 5| Step: 2
Training loss: 2.3547661304473877
Validation loss: 2.4278609624472995

Epoch: 5| Step: 3
Training loss: 2.708329439163208
Validation loss: 2.427900363040227

Epoch: 5| Step: 4
Training loss: 2.9994876384735107
Validation loss: 2.425345133709651

Epoch: 5| Step: 5
Training loss: 2.540914535522461
Validation loss: 2.4273480984472458

Epoch: 5| Step: 6
Training loss: 2.6745035648345947
Validation loss: 2.4228133386181248

Epoch: 5| Step: 7
Training loss: 2.5209412574768066
Validation loss: 2.427034501106508

Epoch: 5| Step: 8
Training loss: 2.6641416549682617
Validation loss: 2.4219472510840303

Epoch: 5| Step: 9
Training loss: 1.866241455078125
Validation loss: 2.4263421463710007

Epoch: 5| Step: 10
Training loss: 3.0983595848083496
Validation loss: 2.419449278103408

Epoch: 146| Step: 0
Training loss: 2.743478298187256
Validation loss: 2.4202057623094126

Epoch: 5| Step: 1
Training loss: 2.898751735687256
Validation loss: 2.4256367093773297

Epoch: 5| Step: 2
Training loss: 3.2364513874053955
Validation loss: 2.425812693052394

Epoch: 5| Step: 3
Training loss: 2.1724228858947754
Validation loss: 2.425279135345131

Epoch: 5| Step: 4
Training loss: 3.0162487030029297
Validation loss: 2.429112695878552

Epoch: 5| Step: 5
Training loss: 2.234734535217285
Validation loss: 2.4226262518154678

Epoch: 5| Step: 6
Training loss: 2.941194772720337
Validation loss: 2.43749830543354

Epoch: 5| Step: 7
Training loss: 2.6869373321533203
Validation loss: 2.425724824269613

Epoch: 5| Step: 8
Training loss: 2.3558731079101562
Validation loss: 2.4272824871924614

Epoch: 5| Step: 9
Training loss: 2.22662091255188
Validation loss: 2.430479280410274

Epoch: 5| Step: 10
Training loss: 2.4194273948669434
Validation loss: 2.4302363190599667

Epoch: 147| Step: 0
Training loss: 2.722822666168213
Validation loss: 2.4265800368401313

Epoch: 5| Step: 1
Training loss: 2.620436906814575
Validation loss: 2.424408314048603

Epoch: 5| Step: 2
Training loss: 1.8336786031723022
Validation loss: 2.42749955320871

Epoch: 5| Step: 3
Training loss: 2.3827364444732666
Validation loss: 2.4277768468344085

Epoch: 5| Step: 4
Training loss: 3.004328966140747
Validation loss: 2.433428431069979

Epoch: 5| Step: 5
Training loss: 2.463130474090576
Validation loss: 2.437405847734021

Epoch: 5| Step: 6
Training loss: 3.234661102294922
Validation loss: 2.430693264930479

Epoch: 5| Step: 7
Training loss: 2.904973268508911
Validation loss: 2.4267419435644664

Epoch: 5| Step: 8
Training loss: 2.1441001892089844
Validation loss: 2.4209613723139607

Epoch: 5| Step: 9
Training loss: 2.6305980682373047
Validation loss: 2.4239363080711773

Epoch: 5| Step: 10
Training loss: 3.1017837524414062
Validation loss: 2.421448947280966

Epoch: 148| Step: 0
Training loss: 2.2906880378723145
Validation loss: 2.4202133250492874

Epoch: 5| Step: 1
Training loss: 3.4776434898376465
Validation loss: 2.423431968176237

Epoch: 5| Step: 2
Training loss: 3.0174012184143066
Validation loss: 2.422508349982641

Epoch: 5| Step: 3
Training loss: 2.380495071411133
Validation loss: 2.4291242809705835

Epoch: 5| Step: 4
Training loss: 2.298950433731079
Validation loss: 2.422790468380015

Epoch: 5| Step: 5
Training loss: 2.6338093280792236
Validation loss: 2.433725313473773

Epoch: 5| Step: 6
Training loss: 3.136547088623047
Validation loss: 2.4340829464697067

Epoch: 5| Step: 7
Training loss: 2.1246371269226074
Validation loss: 2.438464072442824

Epoch: 5| Step: 8
Training loss: 2.5280508995056152
Validation loss: 2.4410229703431487

Epoch: 5| Step: 9
Training loss: 2.565363645553589
Validation loss: 2.4310485829589186

Epoch: 5| Step: 10
Training loss: 2.5853140354156494
Validation loss: 2.4314012219828944

Epoch: 149| Step: 0
Training loss: 2.881056547164917
Validation loss: 2.4293215684993292

Epoch: 5| Step: 1
Training loss: 3.326247453689575
Validation loss: 2.4281767209370932

Epoch: 5| Step: 2
Training loss: 2.697767972946167
Validation loss: 2.4231217215138097

Epoch: 5| Step: 3
Training loss: 2.5737624168395996
Validation loss: 2.4201993198804956

Epoch: 5| Step: 4
Training loss: 2.601274013519287
Validation loss: 2.421342254966818

Epoch: 5| Step: 5
Training loss: 2.8228259086608887
Validation loss: 2.4238609396001345

Epoch: 5| Step: 6
Training loss: 2.6803388595581055
Validation loss: 2.421764730125345

Epoch: 5| Step: 7
Training loss: 2.206516742706299
Validation loss: 2.419055743884015

Epoch: 5| Step: 8
Training loss: 2.5029022693634033
Validation loss: 2.4182436543126262

Epoch: 5| Step: 9
Training loss: 2.536159038543701
Validation loss: 2.4228406208817677

Epoch: 5| Step: 10
Training loss: 2.1264917850494385
Validation loss: 2.42001235741441

Epoch: 150| Step: 0
Training loss: 2.570645809173584
Validation loss: 2.4234335243061023

Epoch: 5| Step: 1
Training loss: 1.8017498254776
Validation loss: 2.421554139865342

Epoch: 5| Step: 2
Training loss: 2.533947467803955
Validation loss: 2.4265351962017756

Epoch: 5| Step: 3
Training loss: 2.1671242713928223
Validation loss: 2.4239515078965055

Epoch: 5| Step: 4
Training loss: 2.8771274089813232
Validation loss: 2.428749453636908

Epoch: 5| Step: 5
Training loss: 3.0820441246032715
Validation loss: 2.4295323971779115

Epoch: 5| Step: 6
Training loss: 2.806915521621704
Validation loss: 2.4317276439359112

Epoch: 5| Step: 7
Training loss: 3.029334545135498
Validation loss: 2.4307136151098434

Epoch: 5| Step: 8
Training loss: 2.8084232807159424
Validation loss: 2.427896738052368

Epoch: 5| Step: 9
Training loss: 2.8750734329223633
Validation loss: 2.430963626471899

Epoch: 5| Step: 10
Training loss: 2.340925931930542
Validation loss: 2.424651545862998

Epoch: 151| Step: 0
Training loss: 2.9162893295288086
Validation loss: 2.42278883277729

Epoch: 5| Step: 1
Training loss: 2.378185510635376
Validation loss: 2.4138228380551903

Epoch: 5| Step: 2
Training loss: 3.0502238273620605
Validation loss: 2.415979867340416

Epoch: 5| Step: 3
Training loss: 1.7040354013442993
Validation loss: 2.4149905097100044

Epoch: 5| Step: 4
Training loss: 3.6532349586486816
Validation loss: 2.411535632225775

Epoch: 5| Step: 5
Training loss: 2.0382580757141113
Validation loss: 2.419968153840752

Epoch: 5| Step: 6
Training loss: 2.715022563934326
Validation loss: 2.4103275896400533

Epoch: 5| Step: 7
Training loss: 2.627922534942627
Validation loss: 2.413522974137337

Epoch: 5| Step: 8
Training loss: 2.4247500896453857
Validation loss: 2.4115926732299147

Epoch: 5| Step: 9
Training loss: 2.9696083068847656
Validation loss: 2.4115957649805213

Epoch: 5| Step: 10
Training loss: 2.421678066253662
Validation loss: 2.4135022419755177

Epoch: 152| Step: 0
Training loss: 2.5233592987060547
Validation loss: 2.4230281870852233

Epoch: 5| Step: 1
Training loss: 1.9651333093643188
Validation loss: 2.443955247120191

Epoch: 5| Step: 2
Training loss: 2.1566004753112793
Validation loss: 2.4597874585018364

Epoch: 5| Step: 3
Training loss: 2.033522605895996
Validation loss: 2.4764993036946943

Epoch: 5| Step: 4
Training loss: 2.7975986003875732
Validation loss: 2.4850755583855415

Epoch: 5| Step: 5
Training loss: 3.140942335128784
Validation loss: 2.4943230921222317

Epoch: 5| Step: 6
Training loss: 2.9415907859802246
Validation loss: 2.484673851279802

Epoch: 5| Step: 7
Training loss: 2.9827301502227783
Validation loss: 2.466144210548811

Epoch: 5| Step: 8
Training loss: 2.7660152912139893
Validation loss: 2.463090650496944

Epoch: 5| Step: 9
Training loss: 2.888587713241577
Validation loss: 2.44362905204937

Epoch: 5| Step: 10
Training loss: 2.998971939086914
Validation loss: 2.4343870224491244

Epoch: 153| Step: 0
Training loss: 2.562407970428467
Validation loss: 2.4279017012606383

Epoch: 5| Step: 1
Training loss: 2.2248241901397705
Validation loss: 2.420702939392418

Epoch: 5| Step: 2
Training loss: 2.1690192222595215
Validation loss: 2.4201487879599295

Epoch: 5| Step: 3
Training loss: 2.117361545562744
Validation loss: 2.418143774873467

Epoch: 5| Step: 4
Training loss: 3.131542682647705
Validation loss: 2.410303892627839

Epoch: 5| Step: 5
Training loss: 3.017383098602295
Validation loss: 2.413205018607519

Epoch: 5| Step: 6
Training loss: 2.2703299522399902
Validation loss: 2.4106142213267665

Epoch: 5| Step: 7
Training loss: 3.150184154510498
Validation loss: 2.4109493224851546

Epoch: 5| Step: 8
Training loss: 2.027251720428467
Validation loss: 2.412844478443105

Epoch: 5| Step: 9
Training loss: 3.403860569000244
Validation loss: 2.418550004241287

Epoch: 5| Step: 10
Training loss: 2.951044797897339
Validation loss: 2.417352355936522

Epoch: 154| Step: 0
Training loss: 2.8509795665740967
Validation loss: 2.4176006137683825

Epoch: 5| Step: 1
Training loss: 2.461798667907715
Validation loss: 2.4254648570091493

Epoch: 5| Step: 2
Training loss: 2.8612492084503174
Validation loss: 2.430922600530809

Epoch: 5| Step: 3
Training loss: 3.0739505290985107
Validation loss: 2.4328518811092583

Epoch: 5| Step: 4
Training loss: 2.806769609451294
Validation loss: 2.4205386792459795

Epoch: 5| Step: 5
Training loss: 2.2554824352264404
Validation loss: 2.4162555715089202

Epoch: 5| Step: 6
Training loss: 2.224761486053467
Validation loss: 2.4148660808481197

Epoch: 5| Step: 7
Training loss: 2.325005054473877
Validation loss: 2.4122355522647982

Epoch: 5| Step: 8
Training loss: 2.3402328491210938
Validation loss: 2.415295341963409

Epoch: 5| Step: 9
Training loss: 2.788841724395752
Validation loss: 2.4113554454618886

Epoch: 5| Step: 10
Training loss: 3.061995029449463
Validation loss: 2.415945412010275

Epoch: 155| Step: 0
Training loss: 2.754326581954956
Validation loss: 2.410743219878084

Epoch: 5| Step: 1
Training loss: 2.9093079566955566
Validation loss: 2.414339896171324

Epoch: 5| Step: 2
Training loss: 2.812220811843872
Validation loss: 2.4097816969758723

Epoch: 5| Step: 3
Training loss: 2.273730516433716
Validation loss: 2.409416269230586

Epoch: 5| Step: 4
Training loss: 2.2427124977111816
Validation loss: 2.417112147936257

Epoch: 5| Step: 5
Training loss: 2.6718101501464844
Validation loss: 2.4170620236345517

Epoch: 5| Step: 6
Training loss: 2.210477352142334
Validation loss: 2.41644670245468

Epoch: 5| Step: 7
Training loss: 2.6402361392974854
Validation loss: 2.419055941284344

Epoch: 5| Step: 8
Training loss: 3.183619260787964
Validation loss: 2.4258217888493694

Epoch: 5| Step: 9
Training loss: 2.5140392780303955
Validation loss: 2.416929709014072

Epoch: 5| Step: 10
Training loss: 2.6749515533447266
Validation loss: 2.424646664691228

Epoch: 156| Step: 0
Training loss: 2.475802183151245
Validation loss: 2.4223152898973033

Epoch: 5| Step: 1
Training loss: 2.8412184715270996
Validation loss: 2.417100332116568

Epoch: 5| Step: 2
Training loss: 1.7754989862442017
Validation loss: 2.4167655885860486

Epoch: 5| Step: 3
Training loss: 2.957740068435669
Validation loss: 2.4197053704210507

Epoch: 5| Step: 4
Training loss: 3.1678645610809326
Validation loss: 2.4127224747852614

Epoch: 5| Step: 5
Training loss: 2.3651678562164307
Validation loss: 2.400547101933469

Epoch: 5| Step: 6
Training loss: 2.851430654525757
Validation loss: 2.4054597808468725

Epoch: 5| Step: 7
Training loss: 3.138880729675293
Validation loss: 2.402254686560682

Epoch: 5| Step: 8
Training loss: 1.9580955505371094
Validation loss: 2.403705745614985

Epoch: 5| Step: 9
Training loss: 2.6947550773620605
Validation loss: 2.3977773227999286

Epoch: 5| Step: 10
Training loss: 2.6339452266693115
Validation loss: 2.402246512392516

Epoch: 157| Step: 0
Training loss: 2.798229455947876
Validation loss: 2.3964775839159564

Epoch: 5| Step: 1
Training loss: 2.5743720531463623
Validation loss: 2.4027897568159204

Epoch: 5| Step: 2
Training loss: 1.8194128274917603
Validation loss: 2.4013788802649385

Epoch: 5| Step: 3
Training loss: 2.365410566329956
Validation loss: 2.4023339492018505

Epoch: 5| Step: 4
Training loss: 2.4884629249572754
Validation loss: 2.400425159803001

Epoch: 5| Step: 5
Training loss: 3.0876355171203613
Validation loss: 2.408303124930269

Epoch: 5| Step: 6
Training loss: 2.6438047885894775
Validation loss: 2.419876565215408

Epoch: 5| Step: 7
Training loss: 2.150573968887329
Validation loss: 2.433539390563965

Epoch: 5| Step: 8
Training loss: 2.8575549125671387
Validation loss: 2.4426454882467947

Epoch: 5| Step: 9
Training loss: 3.1787800788879395
Validation loss: 2.4366149210160777

Epoch: 5| Step: 10
Training loss: 3.0391011238098145
Validation loss: 2.4287917870347218

Epoch: 158| Step: 0
Training loss: 2.7493581771850586
Validation loss: 2.423124728664275

Epoch: 5| Step: 1
Training loss: 2.3428702354431152
Validation loss: 2.4157473194983696

Epoch: 5| Step: 2
Training loss: 3.098973512649536
Validation loss: 2.4180341151452835

Epoch: 5| Step: 3
Training loss: 2.974616527557373
Validation loss: 2.411852559735698

Epoch: 5| Step: 4
Training loss: 2.096188545227051
Validation loss: 2.414475420469879

Epoch: 5| Step: 5
Training loss: 2.7973361015319824
Validation loss: 2.410764027667302

Epoch: 5| Step: 6
Training loss: 3.3232834339141846
Validation loss: 2.4053360031497095

Epoch: 5| Step: 7
Training loss: 2.046590805053711
Validation loss: 2.4064815454585577

Epoch: 5| Step: 8
Training loss: 2.5741169452667236
Validation loss: 2.4065593237517984

Epoch: 5| Step: 9
Training loss: 2.426687479019165
Validation loss: 2.404727294880857

Epoch: 5| Step: 10
Training loss: 2.3934247493743896
Validation loss: 2.4038963164052656

Epoch: 159| Step: 0
Training loss: 2.43821120262146
Validation loss: 2.4061182352804367

Epoch: 5| Step: 1
Training loss: 2.370655059814453
Validation loss: 2.410198491106751

Epoch: 5| Step: 2
Training loss: 2.5176098346710205
Validation loss: 2.4089030347844607

Epoch: 5| Step: 3
Training loss: 3.639094591140747
Validation loss: 2.4115036456815657

Epoch: 5| Step: 4
Training loss: 2.645930051803589
Validation loss: 2.4111766558821484

Epoch: 5| Step: 5
Training loss: 2.7541534900665283
Validation loss: 2.4141199152956725

Epoch: 5| Step: 6
Training loss: 2.2996506690979004
Validation loss: 2.4125783802360616

Epoch: 5| Step: 7
Training loss: 2.5004754066467285
Validation loss: 2.409996287797087

Epoch: 5| Step: 8
Training loss: 2.1794943809509277
Validation loss: 2.4097012704418552

Epoch: 5| Step: 9
Training loss: 2.697514057159424
Validation loss: 2.410913558416469

Epoch: 5| Step: 10
Training loss: 2.7612903118133545
Validation loss: 2.3996961975610382

Epoch: 160| Step: 0
Training loss: 2.8394389152526855
Validation loss: 2.403219384531821

Epoch: 5| Step: 1
Training loss: 3.1113061904907227
Validation loss: 2.4057031062341507

Epoch: 5| Step: 2
Training loss: 3.6399917602539062
Validation loss: 2.4016285762991956

Epoch: 5| Step: 3
Training loss: 2.739366054534912
Validation loss: 2.399402754281157

Epoch: 5| Step: 4
Training loss: 2.2991442680358887
Validation loss: 2.4060863359000093

Epoch: 5| Step: 5
Training loss: 1.7679550647735596
Validation loss: 2.4073545650769304

Epoch: 5| Step: 6
Training loss: 2.278008460998535
Validation loss: 2.3990703154635686

Epoch: 5| Step: 7
Training loss: 2.4558475017547607
Validation loss: 2.409117132104853

Epoch: 5| Step: 8
Training loss: 1.7659209966659546
Validation loss: 2.405318703702701

Epoch: 5| Step: 9
Training loss: 2.9530110359191895
Validation loss: 2.4073994364789737

Epoch: 5| Step: 10
Training loss: 3.0046591758728027
Validation loss: 2.4088572840536795

Epoch: 161| Step: 0
Training loss: 2.9174342155456543
Validation loss: 2.406559516024846

Epoch: 5| Step: 1
Training loss: 2.8460910320281982
Validation loss: 2.4102761617270847

Epoch: 5| Step: 2
Training loss: 3.246147632598877
Validation loss: 2.418075310286655

Epoch: 5| Step: 3
Training loss: 2.380760908126831
Validation loss: 2.418085869922433

Epoch: 5| Step: 4
Training loss: 3.020219564437866
Validation loss: 2.410838273263747

Epoch: 5| Step: 5
Training loss: 1.9334373474121094
Validation loss: 2.4023105149628012

Epoch: 5| Step: 6
Training loss: 2.0664725303649902
Validation loss: 2.4088592093477965

Epoch: 5| Step: 7
Training loss: 2.6091792583465576
Validation loss: 2.405629914294007

Epoch: 5| Step: 8
Training loss: 2.8731484413146973
Validation loss: 2.4121702230104836

Epoch: 5| Step: 9
Training loss: 2.6850426197052
Validation loss: 2.4193519597412436

Epoch: 5| Step: 10
Training loss: 2.1258318424224854
Validation loss: 2.415634625701494

Epoch: 162| Step: 0
Training loss: 2.859130859375
Validation loss: 2.4089692702857395

Epoch: 5| Step: 1
Training loss: 2.8927817344665527
Validation loss: 2.407883503103769

Epoch: 5| Step: 2
Training loss: 2.398794174194336
Validation loss: 2.407552578115976

Epoch: 5| Step: 3
Training loss: 3.3551032543182373
Validation loss: 2.4039018692508822

Epoch: 5| Step: 4
Training loss: 2.4956700801849365
Validation loss: 2.4017963511969453

Epoch: 5| Step: 5
Training loss: 1.5559887886047363
Validation loss: 2.3990533146806943

Epoch: 5| Step: 6
Training loss: 2.4972236156463623
Validation loss: 2.403415390240249

Epoch: 5| Step: 7
Training loss: 2.404592514038086
Validation loss: 2.39597596404373

Epoch: 5| Step: 8
Training loss: 2.5833730697631836
Validation loss: 2.4013484075505245

Epoch: 5| Step: 9
Training loss: 2.903137445449829
Validation loss: 2.3972505190039195

Epoch: 5| Step: 10
Training loss: 2.959660530090332
Validation loss: 2.3942726222417687

Epoch: 163| Step: 0
Training loss: 1.861175298690796
Validation loss: 2.4050464271217264

Epoch: 5| Step: 1
Training loss: 2.3566646575927734
Validation loss: 2.400095525608268

Epoch: 5| Step: 2
Training loss: 1.7668392658233643
Validation loss: 2.4067705472310386

Epoch: 5| Step: 3
Training loss: 2.9156508445739746
Validation loss: 2.4106931865856214

Epoch: 5| Step: 4
Training loss: 3.4716200828552246
Validation loss: 2.4180029822934057

Epoch: 5| Step: 5
Training loss: 2.356106758117676
Validation loss: 2.4077467559486307

Epoch: 5| Step: 6
Training loss: 3.0895817279815674
Validation loss: 2.4112583027091077

Epoch: 5| Step: 7
Training loss: 2.5160555839538574
Validation loss: 2.404257061660931

Epoch: 5| Step: 8
Training loss: 2.6961312294006348
Validation loss: 2.394485322378015

Epoch: 5| Step: 9
Training loss: 2.873279571533203
Validation loss: 2.3933829722865934

Epoch: 5| Step: 10
Training loss: 2.959287643432617
Validation loss: 2.395256077089617

Epoch: 164| Step: 0
Training loss: 2.055687189102173
Validation loss: 2.3872604293207966

Epoch: 5| Step: 1
Training loss: 3.048325538635254
Validation loss: 2.389850116545154

Epoch: 5| Step: 2
Training loss: 2.432785749435425
Validation loss: 2.395088406019313

Epoch: 5| Step: 3
Training loss: 2.776819944381714
Validation loss: 2.388915979734031

Epoch: 5| Step: 4
Training loss: 2.6308486461639404
Validation loss: 2.3864225110700055

Epoch: 5| Step: 5
Training loss: 3.1482861042022705
Validation loss: 2.391267994398712

Epoch: 5| Step: 6
Training loss: 2.3386690616607666
Validation loss: 2.3867913241027505

Epoch: 5| Step: 7
Training loss: 3.119260311126709
Validation loss: 2.38634156668058

Epoch: 5| Step: 8
Training loss: 2.709878921508789
Validation loss: 2.391816431476224

Epoch: 5| Step: 9
Training loss: 2.5571932792663574
Validation loss: 2.3910503720724456

Epoch: 5| Step: 10
Training loss: 1.8596415519714355
Validation loss: 2.39083396363002

Epoch: 165| Step: 0
Training loss: 2.6806557178497314
Validation loss: 2.3931404929007254

Epoch: 5| Step: 1
Training loss: 2.4537885189056396
Validation loss: 2.3931506103084934

Epoch: 5| Step: 2
Training loss: 2.203551769256592
Validation loss: 2.3960917675366966

Epoch: 5| Step: 3
Training loss: 2.8066277503967285
Validation loss: 2.387822256293348

Epoch: 5| Step: 4
Training loss: 2.835759162902832
Validation loss: 2.3946016937173824

Epoch: 5| Step: 5
Training loss: 3.04125714302063
Validation loss: 2.3988749416925574

Epoch: 5| Step: 6
Training loss: 2.797335147857666
Validation loss: 2.4073446489149526

Epoch: 5| Step: 7
Training loss: 2.133739709854126
Validation loss: 2.4012561767332015

Epoch: 5| Step: 8
Training loss: 2.8763723373413086
Validation loss: 2.3981808077904487

Epoch: 5| Step: 9
Training loss: 2.734426975250244
Validation loss: 2.3962782659838275

Epoch: 5| Step: 10
Training loss: 2.1580944061279297
Validation loss: 2.3979684768184537

Epoch: 166| Step: 0
Training loss: 2.034848213195801
Validation loss: 2.398177000784105

Epoch: 5| Step: 1
Training loss: 2.9917140007019043
Validation loss: 2.393596858106634

Epoch: 5| Step: 2
Training loss: 1.8354917764663696
Validation loss: 2.390262665287141

Epoch: 5| Step: 3
Training loss: 2.8149876594543457
Validation loss: 2.388333656454599

Epoch: 5| Step: 4
Training loss: 2.6529412269592285
Validation loss: 2.3938264359710035

Epoch: 5| Step: 5
Training loss: 2.4563040733337402
Validation loss: 2.391327226033775

Epoch: 5| Step: 6
Training loss: 3.033761978149414
Validation loss: 2.3907543613064672

Epoch: 5| Step: 7
Training loss: 2.832744598388672
Validation loss: 2.398555330050889

Epoch: 5| Step: 8
Training loss: 2.027308225631714
Validation loss: 2.397876088337232

Epoch: 5| Step: 9
Training loss: 2.978987216949463
Validation loss: 2.4051679052332395

Epoch: 5| Step: 10
Training loss: 3.1427536010742188
Validation loss: 2.405840742972589

Epoch: 167| Step: 0
Training loss: 2.8093461990356445
Validation loss: 2.403387213266024

Epoch: 5| Step: 1
Training loss: 2.330479860305786
Validation loss: 2.38914236714763

Epoch: 5| Step: 2
Training loss: 2.366661548614502
Validation loss: 2.3938302199045816

Epoch: 5| Step: 3
Training loss: 2.734611988067627
Validation loss: 2.3875849913525324

Epoch: 5| Step: 4
Training loss: 1.7673269510269165
Validation loss: 2.3882137139638266

Epoch: 5| Step: 5
Training loss: 2.4236369132995605
Validation loss: 2.3855783426633446

Epoch: 5| Step: 6
Training loss: 2.031445026397705
Validation loss: 2.3951431474378033

Epoch: 5| Step: 7
Training loss: 3.4167168140411377
Validation loss: 2.3893444051024733

Epoch: 5| Step: 8
Training loss: 3.1130354404449463
Validation loss: 2.391137125671551

Epoch: 5| Step: 9
Training loss: 3.4041786193847656
Validation loss: 2.39851999539201

Epoch: 5| Step: 10
Training loss: 2.2740399837493896
Validation loss: 2.3909195648726596

Epoch: 168| Step: 0
Training loss: 2.7407679557800293
Validation loss: 2.4002827803293862

Epoch: 5| Step: 1
Training loss: 2.5511701107025146
Validation loss: 2.40481311275113

Epoch: 5| Step: 2
Training loss: 2.5993056297302246
Validation loss: 2.405018316802158

Epoch: 5| Step: 3
Training loss: 2.6734726428985596
Validation loss: 2.4090664694386144

Epoch: 5| Step: 4
Training loss: 2.5275750160217285
Validation loss: 2.405868063690842

Epoch: 5| Step: 5
Training loss: 2.3739750385284424
Validation loss: 2.395067097038351

Epoch: 5| Step: 6
Training loss: 2.3875393867492676
Validation loss: 2.3908205288712696

Epoch: 5| Step: 7
Training loss: 2.6139683723449707
Validation loss: 2.382240759429111

Epoch: 5| Step: 8
Training loss: 2.052145481109619
Validation loss: 2.3803769337233676

Epoch: 5| Step: 9
Training loss: 3.10420560836792
Validation loss: 2.3824578074998755

Epoch: 5| Step: 10
Training loss: 3.2447197437286377
Validation loss: 2.3767301446648053

Epoch: 169| Step: 0
Training loss: 1.9572429656982422
Validation loss: 2.3793335499302035

Epoch: 5| Step: 1
Training loss: 2.737492084503174
Validation loss: 2.3809999522342475

Epoch: 5| Step: 2
Training loss: 2.8412766456604004
Validation loss: 2.378644766346101

Epoch: 5| Step: 3
Training loss: 2.6821486949920654
Validation loss: 2.380202806124123

Epoch: 5| Step: 4
Training loss: 2.468848705291748
Validation loss: 2.3840766888792797

Epoch: 5| Step: 5
Training loss: 2.643874406814575
Validation loss: 2.3820419465341875

Epoch: 5| Step: 6
Training loss: 3.331890821456909
Validation loss: 2.392204074449437

Epoch: 5| Step: 7
Training loss: 2.569565534591675
Validation loss: 2.3873862707486717

Epoch: 5| Step: 8
Training loss: 2.1861205101013184
Validation loss: 2.403312842051188

Epoch: 5| Step: 9
Training loss: 2.44484806060791
Validation loss: 2.408625348921745

Epoch: 5| Step: 10
Training loss: 2.9827334880828857
Validation loss: 2.396949588611562

Epoch: 170| Step: 0
Training loss: 2.2103614807128906
Validation loss: 2.3986865576877388

Epoch: 5| Step: 1
Training loss: 2.8701486587524414
Validation loss: 2.4014460015040573

Epoch: 5| Step: 2
Training loss: 2.376716136932373
Validation loss: 2.414751322038712

Epoch: 5| Step: 3
Training loss: 2.1338069438934326
Validation loss: 2.416815386023573

Epoch: 5| Step: 4
Training loss: 3.3542637825012207
Validation loss: 2.426091447953255

Epoch: 5| Step: 5
Training loss: 2.792630910873413
Validation loss: 2.4139001164385068

Epoch: 5| Step: 6
Training loss: 2.8649182319641113
Validation loss: 2.39851616403108

Epoch: 5| Step: 7
Training loss: 3.1079297065734863
Validation loss: 2.3874260661422566

Epoch: 5| Step: 8
Training loss: 2.0512287616729736
Validation loss: 2.3829658005827214

Epoch: 5| Step: 9
Training loss: 2.438321352005005
Validation loss: 2.3847381273905435

Epoch: 5| Step: 10
Training loss: 2.492460250854492
Validation loss: 2.3833189728439494

Epoch: 171| Step: 0
Training loss: 2.9255690574645996
Validation loss: 2.379874029467183

Epoch: 5| Step: 1
Training loss: 2.4778175354003906
Validation loss: 2.3853145978784047

Epoch: 5| Step: 2
Training loss: 2.6788907051086426
Validation loss: 2.385609152496502

Epoch: 5| Step: 3
Training loss: 2.3229546546936035
Validation loss: 2.386621677747337

Epoch: 5| Step: 4
Training loss: 3.0723838806152344
Validation loss: 2.3858535725583314

Epoch: 5| Step: 5
Training loss: 2.477445363998413
Validation loss: 2.394793507873371

Epoch: 5| Step: 6
Training loss: 1.8092918395996094
Validation loss: 2.4015305452449347

Epoch: 5| Step: 7
Training loss: 3.193331003189087
Validation loss: 2.4173566756709928

Epoch: 5| Step: 8
Training loss: 3.08796763420105
Validation loss: 2.4210853371568906

Epoch: 5| Step: 9
Training loss: 1.9288676977157593
Validation loss: 2.4247227458543676

Epoch: 5| Step: 10
Training loss: 2.7699763774871826
Validation loss: 2.4244596650523524

Epoch: 172| Step: 0
Training loss: 2.2230701446533203
Validation loss: 2.426577321944698

Epoch: 5| Step: 1
Training loss: 3.0284364223480225
Validation loss: 2.4193312967977216

Epoch: 5| Step: 2
Training loss: 2.6248955726623535
Validation loss: 2.4091270969760035

Epoch: 5| Step: 3
Training loss: 2.1458632946014404
Validation loss: 2.390537941327659

Epoch: 5| Step: 4
Training loss: 2.9895730018615723
Validation loss: 2.388384134538712

Epoch: 5| Step: 5
Training loss: 2.7267091274261475
Validation loss: 2.3882773255789154

Epoch: 5| Step: 6
Training loss: 2.680164098739624
Validation loss: 2.3896057067378873

Epoch: 5| Step: 7
Training loss: 2.0377774238586426
Validation loss: 2.387709363814323

Epoch: 5| Step: 8
Training loss: 2.582740306854248
Validation loss: 2.379678485214069

Epoch: 5| Step: 9
Training loss: 2.7442822456359863
Validation loss: 2.3824800111914195

Epoch: 5| Step: 10
Training loss: 3.0540270805358887
Validation loss: 2.3938055371725433

Epoch: 173| Step: 0
Training loss: 2.403245449066162
Validation loss: 2.4066086584521877

Epoch: 5| Step: 1
Training loss: 2.1516103744506836
Validation loss: 2.415455392611924

Epoch: 5| Step: 2
Training loss: 2.8264009952545166
Validation loss: 2.424668553054974

Epoch: 5| Step: 3
Training loss: 2.6267051696777344
Validation loss: 2.420473003900179

Epoch: 5| Step: 4
Training loss: 3.4823813438415527
Validation loss: 2.41960725989393

Epoch: 5| Step: 5
Training loss: 2.824323892593384
Validation loss: 2.41209146540652

Epoch: 5| Step: 6
Training loss: 2.361201763153076
Validation loss: 2.4014235465757308

Epoch: 5| Step: 7
Training loss: 2.137953281402588
Validation loss: 2.3922451003905265

Epoch: 5| Step: 8
Training loss: 2.570195436477661
Validation loss: 2.391470868100402

Epoch: 5| Step: 9
Training loss: 2.789602518081665
Validation loss: 2.3879128092078754

Epoch: 5| Step: 10
Training loss: 2.4833593368530273
Validation loss: 2.3814055483828307

Epoch: 174| Step: 0
Training loss: 2.2513926029205322
Validation loss: 2.3844514021309475

Epoch: 5| Step: 1
Training loss: 3.002656936645508
Validation loss: 2.381857192644509

Epoch: 5| Step: 2
Training loss: 2.076594352722168
Validation loss: 2.3781120136219966

Epoch: 5| Step: 3
Training loss: 2.0775580406188965
Validation loss: 2.373753524595691

Epoch: 5| Step: 4
Training loss: 2.709028482437134
Validation loss: 2.382988300374759

Epoch: 5| Step: 5
Training loss: 2.2981820106506348
Validation loss: 2.378038185898976

Epoch: 5| Step: 6
Training loss: 3.841517925262451
Validation loss: 2.3820498502382668

Epoch: 5| Step: 7
Training loss: 1.9464117288589478
Validation loss: 2.3781399752504084

Epoch: 5| Step: 8
Training loss: 2.5937397480010986
Validation loss: 2.3817705005727787

Epoch: 5| Step: 9
Training loss: 3.237395763397217
Validation loss: 2.382322326783211

Epoch: 5| Step: 10
Training loss: 2.6090598106384277
Validation loss: 2.382397646545082

Epoch: 175| Step: 0
Training loss: 2.8921279907226562
Validation loss: 2.388853278211368

Epoch: 5| Step: 1
Training loss: 2.2428367137908936
Validation loss: 2.3965046636519896

Epoch: 5| Step: 2
Training loss: 2.6549201011657715
Validation loss: 2.3962715992363552

Epoch: 5| Step: 3
Training loss: 2.89125394821167
Validation loss: 2.3948265121829126

Epoch: 5| Step: 4
Training loss: 2.6962785720825195
Validation loss: 2.393452810984786

Epoch: 5| Step: 5
Training loss: 2.275439977645874
Validation loss: 2.389829356183288

Epoch: 5| Step: 6
Training loss: 2.259521245956421
Validation loss: 2.385613062048471

Epoch: 5| Step: 7
Training loss: 2.5300440788269043
Validation loss: 2.382612787267213

Epoch: 5| Step: 8
Training loss: 2.226480722427368
Validation loss: 2.3724718914237073

Epoch: 5| Step: 9
Training loss: 2.900151252746582
Validation loss: 2.378132732965613

Epoch: 5| Step: 10
Training loss: 3.181253433227539
Validation loss: 2.373949384176603

Epoch: 176| Step: 0
Training loss: 2.6071581840515137
Validation loss: 2.3715531774746474

Epoch: 5| Step: 1
Training loss: 2.41424560546875
Validation loss: 2.3733820453766854

Epoch: 5| Step: 2
Training loss: 2.3011341094970703
Validation loss: 2.373492325505903

Epoch: 5| Step: 3
Training loss: 2.8367533683776855
Validation loss: 2.375739914114757

Epoch: 5| Step: 4
Training loss: 2.9813668727874756
Validation loss: 2.376399068422215

Epoch: 5| Step: 5
Training loss: 2.5927090644836426
Validation loss: 2.3786116415454495

Epoch: 5| Step: 6
Training loss: 2.709049940109253
Validation loss: 2.376927060465659

Epoch: 5| Step: 7
Training loss: 2.3638434410095215
Validation loss: 2.3949643181216334

Epoch: 5| Step: 8
Training loss: 2.7631168365478516
Validation loss: 2.391858646946569

Epoch: 5| Step: 9
Training loss: 2.8538596630096436
Validation loss: 2.3892774684454805

Epoch: 5| Step: 10
Training loss: 2.2165791988372803
Validation loss: 2.394693987343901

Epoch: 177| Step: 0
Training loss: 2.424666166305542
Validation loss: 2.3892007040721115

Epoch: 5| Step: 1
Training loss: 2.2414355278015137
Validation loss: 2.3788318326396327

Epoch: 5| Step: 2
Training loss: 2.7756459712982178
Validation loss: 2.376195474337506

Epoch: 5| Step: 3
Training loss: 3.0313804149627686
Validation loss: 2.3716518391845045

Epoch: 5| Step: 4
Training loss: 2.7481837272644043
Validation loss: 2.371583920653148

Epoch: 5| Step: 5
Training loss: 1.8791996240615845
Validation loss: 2.370740836666476

Epoch: 5| Step: 6
Training loss: 2.691624879837036
Validation loss: 2.378978729248047

Epoch: 5| Step: 7
Training loss: 2.938690185546875
Validation loss: 2.3767792230011313

Epoch: 5| Step: 8
Training loss: 2.841292381286621
Validation loss: 2.375242169185351

Epoch: 5| Step: 9
Training loss: 2.734020709991455
Validation loss: 2.37357400566019

Epoch: 5| Step: 10
Training loss: 2.267611026763916
Validation loss: 2.3826195578421316

Epoch: 178| Step: 0
Training loss: 2.288062810897827
Validation loss: 2.388051184274817

Epoch: 5| Step: 1
Training loss: 2.985668897628784
Validation loss: 2.3970578357737553

Epoch: 5| Step: 2
Training loss: 2.736569881439209
Validation loss: 2.4033859032456593

Epoch: 5| Step: 3
Training loss: 2.2420458793640137
Validation loss: 2.4084704434999855

Epoch: 5| Step: 4
Training loss: 2.213099718093872
Validation loss: 2.4122104132047264

Epoch: 5| Step: 5
Training loss: 2.9441449642181396
Validation loss: 2.411650929399716

Epoch: 5| Step: 6
Training loss: 2.4014668464660645
Validation loss: 2.406215798470282

Epoch: 5| Step: 7
Training loss: 2.082576036453247
Validation loss: 2.3872710850930985

Epoch: 5| Step: 8
Training loss: 2.7126450538635254
Validation loss: 2.3756564996575795

Epoch: 5| Step: 9
Training loss: 2.760629177093506
Validation loss: 2.3754453223238707

Epoch: 5| Step: 10
Training loss: 3.4453887939453125
Validation loss: 2.377793553054974

Epoch: 179| Step: 0
Training loss: 2.126110553741455
Validation loss: 2.3760516771706204

Epoch: 5| Step: 1
Training loss: 3.226567506790161
Validation loss: 2.37362640519296

Epoch: 5| Step: 2
Training loss: 2.4029126167297363
Validation loss: 2.373029766544219

Epoch: 5| Step: 3
Training loss: 2.4579524993896484
Validation loss: 2.3709906583191245

Epoch: 5| Step: 4
Training loss: 2.5843043327331543
Validation loss: 2.3743145952942553

Epoch: 5| Step: 5
Training loss: 1.7266902923583984
Validation loss: 2.3742151132193943

Epoch: 5| Step: 6
Training loss: 2.2244770526885986
Validation loss: 2.3755340499262654

Epoch: 5| Step: 7
Training loss: 2.826939821243286
Validation loss: 2.375342575452661

Epoch: 5| Step: 8
Training loss: 2.8535637855529785
Validation loss: 2.380667414716495

Epoch: 5| Step: 9
Training loss: 3.110053539276123
Validation loss: 2.388644310735887

Epoch: 5| Step: 10
Training loss: 3.2308788299560547
Validation loss: 2.395662348757508

Epoch: 180| Step: 0
Training loss: 2.194863796234131
Validation loss: 2.4081654651190645

Epoch: 5| Step: 1
Training loss: 3.455463409423828
Validation loss: 2.4015268138659898

Epoch: 5| Step: 2
Training loss: 2.22194242477417
Validation loss: 2.38479140240659

Epoch: 5| Step: 3
Training loss: 2.9103965759277344
Validation loss: 2.382669069433725

Epoch: 5| Step: 4
Training loss: 3.232025146484375
Validation loss: 2.3791458452901533

Epoch: 5| Step: 5
Training loss: 2.008504867553711
Validation loss: 2.374557646371985

Epoch: 5| Step: 6
Training loss: 2.3936076164245605
Validation loss: 2.3643231930271273

Epoch: 5| Step: 7
Training loss: 1.9259525537490845
Validation loss: 2.3635565593678463

Epoch: 5| Step: 8
Training loss: 2.7491612434387207
Validation loss: 2.3634881909175585

Epoch: 5| Step: 9
Training loss: 2.78534197807312
Validation loss: 2.363246992070188

Epoch: 5| Step: 10
Training loss: 2.896970272064209
Validation loss: 2.3652494799706245

Epoch: 181| Step: 0
Training loss: 2.5528807640075684
Validation loss: 2.3663026184164067

Epoch: 5| Step: 1
Training loss: 2.954704999923706
Validation loss: 2.3722580350855345

Epoch: 5| Step: 2
Training loss: 2.497673511505127
Validation loss: 2.372795066525859

Epoch: 5| Step: 3
Training loss: 2.580984592437744
Validation loss: 2.3693967762813775

Epoch: 5| Step: 4
Training loss: 2.3526556491851807
Validation loss: 2.366460579697804

Epoch: 5| Step: 5
Training loss: 2.7493200302124023
Validation loss: 2.363318943208264

Epoch: 5| Step: 6
Training loss: 3.220402240753174
Validation loss: 2.366773509210156

Epoch: 5| Step: 7
Training loss: 2.403616428375244
Validation loss: 2.363937288202265

Epoch: 5| Step: 8
Training loss: 2.834872245788574
Validation loss: 2.3672209042374805

Epoch: 5| Step: 9
Training loss: 2.521822452545166
Validation loss: 2.3701369608602216

Epoch: 5| Step: 10
Training loss: 2.0068399906158447
Validation loss: 2.376124817837951

Epoch: 182| Step: 0
Training loss: 2.2731263637542725
Validation loss: 2.386944763122066

Epoch: 5| Step: 1
Training loss: 2.620122194290161
Validation loss: 2.384619248810635

Epoch: 5| Step: 2
Training loss: 2.8087172508239746
Validation loss: 2.385052837351317

Epoch: 5| Step: 3
Training loss: 2.7465689182281494
Validation loss: 2.3833375489839943

Epoch: 5| Step: 4
Training loss: 2.193958044052124
Validation loss: 2.3772803724453015

Epoch: 5| Step: 5
Training loss: 2.531137704849243
Validation loss: 2.3794145097014723

Epoch: 5| Step: 6
Training loss: 2.91410493850708
Validation loss: 2.377645492553711

Epoch: 5| Step: 7
Training loss: 2.410001754760742
Validation loss: 2.3693760107922297

Epoch: 5| Step: 8
Training loss: 2.842600107192993
Validation loss: 2.3681483089282946

Epoch: 5| Step: 9
Training loss: 2.4132511615753174
Validation loss: 2.3634487864791707

Epoch: 5| Step: 10
Training loss: 2.8137757778167725
Validation loss: 2.3632331689198813

Epoch: 183| Step: 0
Training loss: 2.878248929977417
Validation loss: 2.3635693788528442

Epoch: 5| Step: 1
Training loss: 2.6398119926452637
Validation loss: 2.363353536974999

Epoch: 5| Step: 2
Training loss: 3.176891803741455
Validation loss: 2.3656046134169384

Epoch: 5| Step: 3
Training loss: 2.067241668701172
Validation loss: 2.361904436542142

Epoch: 5| Step: 4
Training loss: 2.770923376083374
Validation loss: 2.358779681626187

Epoch: 5| Step: 5
Training loss: 2.4130043983459473
Validation loss: 2.3643644650777182

Epoch: 5| Step: 6
Training loss: 2.190656900405884
Validation loss: 2.3624446135695263

Epoch: 5| Step: 7
Training loss: 2.641907215118408
Validation loss: 2.360560676102997

Epoch: 5| Step: 8
Training loss: 2.5989344120025635
Validation loss: 2.3681591146735737

Epoch: 5| Step: 9
Training loss: 2.414911985397339
Validation loss: 2.381008530175814

Epoch: 5| Step: 10
Training loss: 2.857154130935669
Validation loss: 2.3867608244701097

Epoch: 184| Step: 0
Training loss: 2.1737759113311768
Validation loss: 2.408304061940921

Epoch: 5| Step: 1
Training loss: 3.191070556640625
Validation loss: 2.425591694411411

Epoch: 5| Step: 2
Training loss: 2.9658827781677246
Validation loss: 2.4274271201061945

Epoch: 5| Step: 3
Training loss: 2.178873062133789
Validation loss: 2.437056872152513

Epoch: 5| Step: 4
Training loss: 2.6975913047790527
Validation loss: 2.4403613126406105

Epoch: 5| Step: 5
Training loss: 2.801912784576416
Validation loss: 2.4543276884222545

Epoch: 5| Step: 6
Training loss: 2.785111665725708
Validation loss: 2.454244218846803

Epoch: 5| Step: 7
Training loss: 1.8712730407714844
Validation loss: 2.4326396578101703

Epoch: 5| Step: 8
Training loss: 2.8469550609588623
Validation loss: 2.40232950897627

Epoch: 5| Step: 9
Training loss: 2.105452060699463
Validation loss: 2.374608075746926

Epoch: 5| Step: 10
Training loss: 3.282844066619873
Validation loss: 2.364398822989515

Epoch: 185| Step: 0
Training loss: 2.7999119758605957
Validation loss: 2.3672240575154624

Epoch: 5| Step: 1
Training loss: 3.1217236518859863
Validation loss: 2.3693613108768257

Epoch: 5| Step: 2
Training loss: 3.127845048904419
Validation loss: 2.368334151083423

Epoch: 5| Step: 3
Training loss: 2.4190452098846436
Validation loss: 2.382678506194904

Epoch: 5| Step: 4
Training loss: 2.866750955581665
Validation loss: 2.375050026883361

Epoch: 5| Step: 5
Training loss: 2.7992348670959473
Validation loss: 2.3822518471748597

Epoch: 5| Step: 6
Training loss: 2.190551519393921
Validation loss: 2.380476438870994

Epoch: 5| Step: 7
Training loss: 2.8265461921691895
Validation loss: 2.369383645314042

Epoch: 5| Step: 8
Training loss: 2.4138247966766357
Validation loss: 2.36651837313047

Epoch: 5| Step: 9
Training loss: 2.63126540184021
Validation loss: 2.3632020411952848

Epoch: 5| Step: 10
Training loss: 1.4147266149520874
Validation loss: 2.3676156561861754

Epoch: 186| Step: 0
Training loss: 2.484104871749878
Validation loss: 2.365291198094686

Epoch: 5| Step: 1
Training loss: 2.4259676933288574
Validation loss: 2.3732266220995175

Epoch: 5| Step: 2
Training loss: 2.644965410232544
Validation loss: 2.392506112334549

Epoch: 5| Step: 3
Training loss: 2.8268349170684814
Validation loss: 2.3849541576959754

Epoch: 5| Step: 4
Training loss: 2.403757095336914
Validation loss: 2.3739019747703307

Epoch: 5| Step: 5
Training loss: 2.5570685863494873
Validation loss: 2.370187908090571

Epoch: 5| Step: 6
Training loss: 2.1667323112487793
Validation loss: 2.364985167339284

Epoch: 5| Step: 7
Training loss: 2.996086597442627
Validation loss: 2.3655502539809032

Epoch: 5| Step: 8
Training loss: 2.986299753189087
Validation loss: 2.3634667268363376

Epoch: 5| Step: 9
Training loss: 2.36783504486084
Validation loss: 2.3612304195280998

Epoch: 5| Step: 10
Training loss: 2.661283254623413
Validation loss: 2.3642921883572816

Epoch: 187| Step: 0
Training loss: 2.6348929405212402
Validation loss: 2.363281862710112

Epoch: 5| Step: 1
Training loss: 2.1781489849090576
Validation loss: 2.361184004814394

Epoch: 5| Step: 2
Training loss: 3.3294692039489746
Validation loss: 2.3651673716883503

Epoch: 5| Step: 3
Training loss: 2.2948710918426514
Validation loss: 2.3647008890746744

Epoch: 5| Step: 4
Training loss: 2.9851431846618652
Validation loss: 2.3617421580899145

Epoch: 5| Step: 5
Training loss: 2.4207968711853027
Validation loss: 2.360029230835617

Epoch: 5| Step: 6
Training loss: 2.485841751098633
Validation loss: 2.3568713665008545

Epoch: 5| Step: 7
Training loss: 3.176131248474121
Validation loss: 2.3547873689282324

Epoch: 5| Step: 8
Training loss: 2.218597888946533
Validation loss: 2.3578258996368735

Epoch: 5| Step: 9
Training loss: 2.5681629180908203
Validation loss: 2.3485511374729935

Epoch: 5| Step: 10
Training loss: 2.3410720825195312
Validation loss: 2.3601764427718295

Epoch: 188| Step: 0
Training loss: 3.0149474143981934
Validation loss: 2.3611179423588577

Epoch: 5| Step: 1
Training loss: 3.0833580493927
Validation loss: 2.368377567619406

Epoch: 5| Step: 2
Training loss: 2.528491497039795
Validation loss: 2.3687723631499917

Epoch: 5| Step: 3
Training loss: 1.8534488677978516
Validation loss: 2.366375838556597

Epoch: 5| Step: 4
Training loss: 2.8218514919281006
Validation loss: 2.3751056578851517

Epoch: 5| Step: 5
Training loss: 2.784562587738037
Validation loss: 2.3758861095674577

Epoch: 5| Step: 6
Training loss: 1.9555861949920654
Validation loss: 2.369823061009889

Epoch: 5| Step: 7
Training loss: 2.2133517265319824
Validation loss: 2.3713329761259017

Epoch: 5| Step: 8
Training loss: 3.1647696495056152
Validation loss: 2.3635839339225524

Epoch: 5| Step: 9
Training loss: 2.853384494781494
Validation loss: 2.356879735505709

Epoch: 5| Step: 10
Training loss: 2.2389745712280273
Validation loss: 2.362138263640865

Epoch: 189| Step: 0
Training loss: 2.281991958618164
Validation loss: 2.355271424016645

Epoch: 5| Step: 1
Training loss: 2.84468150138855
Validation loss: 2.3578005170309417

Epoch: 5| Step: 2
Training loss: 2.8549885749816895
Validation loss: 2.3540506106550976

Epoch: 5| Step: 3
Training loss: 3.0533339977264404
Validation loss: 2.357475539689423

Epoch: 5| Step: 4
Training loss: 3.0074563026428223
Validation loss: 2.3559670781576507

Epoch: 5| Step: 5
Training loss: 3.0615153312683105
Validation loss: 2.3514890529776133

Epoch: 5| Step: 6
Training loss: 2.7661683559417725
Validation loss: 2.3535633138431016

Epoch: 5| Step: 7
Training loss: 2.4088282585144043
Validation loss: 2.3558336073352444

Epoch: 5| Step: 8
Training loss: 2.0396461486816406
Validation loss: 2.350892331010552

Epoch: 5| Step: 9
Training loss: 2.4483084678649902
Validation loss: 2.3578816383115706

Epoch: 5| Step: 10
Training loss: 1.6771026849746704
Validation loss: 2.357772413120475

Epoch: 190| Step: 0
Training loss: 2.475893259048462
Validation loss: 2.385044951592722

Epoch: 5| Step: 1
Training loss: 2.141275644302368
Validation loss: 2.419546035028273

Epoch: 5| Step: 2
Training loss: 3.2957077026367188
Validation loss: 2.435165354000625

Epoch: 5| Step: 3
Training loss: 2.419459342956543
Validation loss: 2.455524913726314

Epoch: 5| Step: 4
Training loss: 3.072722911834717
Validation loss: 2.487050889640726

Epoch: 5| Step: 5
Training loss: 2.906588077545166
Validation loss: 2.479282130477249

Epoch: 5| Step: 6
Training loss: 2.733069896697998
Validation loss: 2.4437689217188026

Epoch: 5| Step: 7
Training loss: 1.911745309829712
Validation loss: 2.4278569541951662

Epoch: 5| Step: 8
Training loss: 2.2559618949890137
Validation loss: 2.4193282537562872

Epoch: 5| Step: 9
Training loss: 2.6391000747680664
Validation loss: 2.3943490341145504

Epoch: 5| Step: 10
Training loss: 2.8632900714874268
Validation loss: 2.3902630395786737

Epoch: 191| Step: 0
Training loss: 3.022705078125
Validation loss: 2.3767200105933735

Epoch: 5| Step: 1
Training loss: 2.195613145828247
Validation loss: 2.361582822697137

Epoch: 5| Step: 2
Training loss: 2.531893491744995
Validation loss: 2.3551185720710346

Epoch: 5| Step: 3
Training loss: 2.568619966506958
Validation loss: 2.358316813745806

Epoch: 5| Step: 4
Training loss: 2.43440580368042
Validation loss: 2.356645234169499

Epoch: 5| Step: 5
Training loss: 2.6655831336975098
Validation loss: 2.357056417772847

Epoch: 5| Step: 6
Training loss: 3.0151712894439697
Validation loss: 2.355615744026758

Epoch: 5| Step: 7
Training loss: 2.4774014949798584
Validation loss: 2.35151187322473

Epoch: 5| Step: 8
Training loss: 2.3157131671905518
Validation loss: 2.3558776660632064

Epoch: 5| Step: 9
Training loss: 2.8008525371551514
Validation loss: 2.353857595433471

Epoch: 5| Step: 10
Training loss: 2.6134536266326904
Validation loss: 2.346372622315602

Epoch: 192| Step: 0
Training loss: 2.9649593830108643
Validation loss: 2.3523897073602162

Epoch: 5| Step: 1
Training loss: 2.8817152976989746
Validation loss: 2.3547364537433912

Epoch: 5| Step: 2
Training loss: 2.454177141189575
Validation loss: 2.3625969732961347

Epoch: 5| Step: 3
Training loss: 2.010939121246338
Validation loss: 2.3686157323980845

Epoch: 5| Step: 4
Training loss: 2.489877462387085
Validation loss: 2.3897072909980692

Epoch: 5| Step: 5
Training loss: 2.942054271697998
Validation loss: 2.3888909047649753

Epoch: 5| Step: 6
Training loss: 2.744619369506836
Validation loss: 2.391851561043852

Epoch: 5| Step: 7
Training loss: 2.3838322162628174
Validation loss: 2.392965219354117

Epoch: 5| Step: 8
Training loss: 2.4164116382598877
Validation loss: 2.383133201188939

Epoch: 5| Step: 9
Training loss: 3.370281934738159
Validation loss: 2.3764630338197112

Epoch: 5| Step: 10
Training loss: 1.8892611265182495
Validation loss: 2.361511153559531

Epoch: 193| Step: 0
Training loss: 2.759504795074463
Validation loss: 2.3572794468172136

Epoch: 5| Step: 1
Training loss: 2.6093647480010986
Validation loss: 2.353546957815847

Epoch: 5| Step: 2
Training loss: 2.1917455196380615
Validation loss: 2.357787557827529

Epoch: 5| Step: 3
Training loss: 2.62723970413208
Validation loss: 2.3439706243494505

Epoch: 5| Step: 4
Training loss: 3.0307252407073975
Validation loss: 2.3494218100783644

Epoch: 5| Step: 5
Training loss: 3.093174934387207
Validation loss: 2.353318768162881

Epoch: 5| Step: 6
Training loss: 2.1292724609375
Validation loss: 2.350212981623988

Epoch: 5| Step: 7
Training loss: 2.853630781173706
Validation loss: 2.348981862427086

Epoch: 5| Step: 8
Training loss: 2.3886032104492188
Validation loss: 2.3510342336470083

Epoch: 5| Step: 9
Training loss: 2.410956621170044
Validation loss: 2.3514783715689056

Epoch: 5| Step: 10
Training loss: 2.440208911895752
Validation loss: 2.347283048014487

Epoch: 194| Step: 0
Training loss: 2.749133586883545
Validation loss: 2.3535221494654173

Epoch: 5| Step: 1
Training loss: 2.436030387878418
Validation loss: 2.35824486260773

Epoch: 5| Step: 2
Training loss: 2.6217775344848633
Validation loss: 2.356150004171556

Epoch: 5| Step: 3
Training loss: 2.2374448776245117
Validation loss: 2.361557517000424

Epoch: 5| Step: 4
Training loss: 3.0827536582946777
Validation loss: 2.349513115421418

Epoch: 5| Step: 5
Training loss: 2.4352569580078125
Validation loss: 2.355261154072259

Epoch: 5| Step: 6
Training loss: 2.0129387378692627
Validation loss: 2.350101977266291

Epoch: 5| Step: 7
Training loss: 3.072482109069824
Validation loss: 2.351971557063441

Epoch: 5| Step: 8
Training loss: 2.7823495864868164
Validation loss: 2.352058090189452

Epoch: 5| Step: 9
Training loss: 2.534877300262451
Validation loss: 2.351221344804251

Epoch: 5| Step: 10
Training loss: 2.422400712966919
Validation loss: 2.3523005054843042

Epoch: 195| Step: 0
Training loss: 2.5862908363342285
Validation loss: 2.3628767280168432

Epoch: 5| Step: 1
Training loss: 2.517141103744507
Validation loss: 2.3765329289179977

Epoch: 5| Step: 2
Training loss: 2.732339382171631
Validation loss: 2.3861893633360505

Epoch: 5| Step: 3
Training loss: 2.7856807708740234
Validation loss: 2.3845829425319547

Epoch: 5| Step: 4
Training loss: 2.883633852005005
Validation loss: 2.3941127843754266

Epoch: 5| Step: 5
Training loss: 2.7684402465820312
Validation loss: 2.3822034225668958

Epoch: 5| Step: 6
Training loss: 2.4860622882843018
Validation loss: 2.3563763095486547

Epoch: 5| Step: 7
Training loss: 2.293337106704712
Validation loss: 2.347775759235505

Epoch: 5| Step: 8
Training loss: 2.4801182746887207
Validation loss: 2.342317009484896

Epoch: 5| Step: 9
Training loss: 2.8409981727600098
Validation loss: 2.345674837789228

Epoch: 5| Step: 10
Training loss: 2.114142894744873
Validation loss: 2.34905594395053

Epoch: 196| Step: 0
Training loss: 2.8870973587036133
Validation loss: 2.347604469586444

Epoch: 5| Step: 1
Training loss: 2.420886993408203
Validation loss: 2.3527765966230825

Epoch: 5| Step: 2
Training loss: 2.4883384704589844
Validation loss: 2.3534160070521857

Epoch: 5| Step: 3
Training loss: 2.651637554168701
Validation loss: 2.3547980349550963

Epoch: 5| Step: 4
Training loss: 2.7913920879364014
Validation loss: 2.3580480134615334

Epoch: 5| Step: 5
Training loss: 2.9309322834014893
Validation loss: 2.367400046317808

Epoch: 5| Step: 6
Training loss: 2.458543300628662
Validation loss: 2.3677041581881944

Epoch: 5| Step: 7
Training loss: 1.9246292114257812
Validation loss: 2.37698382459661

Epoch: 5| Step: 8
Training loss: 2.5772435665130615
Validation loss: 2.3824857973283335

Epoch: 5| Step: 9
Training loss: 2.8461709022521973
Validation loss: 2.3913432321240826

Epoch: 5| Step: 10
Training loss: 2.4883580207824707
Validation loss: 2.368731406427199

Epoch: 197| Step: 0
Training loss: 2.6497998237609863
Validation loss: 2.3552779484820623

Epoch: 5| Step: 1
Training loss: 2.5093235969543457
Validation loss: 2.3493543645387054

Epoch: 5| Step: 2
Training loss: 2.268704652786255
Validation loss: 2.3542172447327645

Epoch: 5| Step: 3
Training loss: 2.490736722946167
Validation loss: 2.349809670961031

Epoch: 5| Step: 4
Training loss: 1.7888681888580322
Validation loss: 2.3575614883053686

Epoch: 5| Step: 5
Training loss: 2.8723769187927246
Validation loss: 2.358520423212359

Epoch: 5| Step: 6
Training loss: 2.4603474140167236
Validation loss: 2.359028688041113

Epoch: 5| Step: 7
Training loss: 2.7819292545318604
Validation loss: 2.363699766897386

Epoch: 5| Step: 8
Training loss: 2.705141544342041
Validation loss: 2.3679646856041363

Epoch: 5| Step: 9
Training loss: 2.9864816665649414
Validation loss: 2.3708306922707507

Epoch: 5| Step: 10
Training loss: 3.0071051120758057
Validation loss: 2.3615744447195404

Epoch: 198| Step: 0
Training loss: 2.632218599319458
Validation loss: 2.373308879072948

Epoch: 5| Step: 1
Training loss: 3.073129892349243
Validation loss: 2.369685452471497

Epoch: 5| Step: 2
Training loss: 2.8500816822052
Validation loss: 2.368816796169486

Epoch: 5| Step: 3
Training loss: 2.278794288635254
Validation loss: 2.377030393128754

Epoch: 5| Step: 4
Training loss: 2.6355440616607666
Validation loss: 2.369298060735067

Epoch: 5| Step: 5
Training loss: 2.3427786827087402
Validation loss: 2.3642058000769666

Epoch: 5| Step: 6
Training loss: 2.4476077556610107
Validation loss: 2.3666086401990665

Epoch: 5| Step: 7
Training loss: 2.6426708698272705
Validation loss: 2.357688844844859

Epoch: 5| Step: 8
Training loss: 2.2927470207214355
Validation loss: 2.355104569465883

Epoch: 5| Step: 9
Training loss: 2.3201637268066406
Validation loss: 2.3492413823322584

Epoch: 5| Step: 10
Training loss: 2.9404356479644775
Validation loss: 2.3584165932029806

Epoch: 199| Step: 0
Training loss: 2.7130141258239746
Validation loss: 2.356680049691149

Epoch: 5| Step: 1
Training loss: 2.5495147705078125
Validation loss: 2.3513914551786197

Epoch: 5| Step: 2
Training loss: 3.1922740936279297
Validation loss: 2.3442375172850904

Epoch: 5| Step: 3
Training loss: 2.1884560585021973
Validation loss: 2.354245380688739

Epoch: 5| Step: 4
Training loss: 2.7974467277526855
Validation loss: 2.3539717863964778

Epoch: 5| Step: 5
Training loss: 2.4121265411376953
Validation loss: 2.3549171160626154

Epoch: 5| Step: 6
Training loss: 1.6723581552505493
Validation loss: 2.3616574143850677

Epoch: 5| Step: 7
Training loss: 2.824143648147583
Validation loss: 2.3580504540474183

Epoch: 5| Step: 8
Training loss: 2.4631667137145996
Validation loss: 2.363279047832694

Epoch: 5| Step: 9
Training loss: 2.449807643890381
Validation loss: 2.3701410985762075

Epoch: 5| Step: 10
Training loss: 3.2261486053466797
Validation loss: 2.3606077701814714

Epoch: 200| Step: 0
Training loss: 3.052431106567383
Validation loss: 2.3649533230771302

Epoch: 5| Step: 1
Training loss: 2.6661581993103027
Validation loss: 2.3651070338423534

Epoch: 5| Step: 2
Training loss: 2.110844135284424
Validation loss: 2.36012618003353

Epoch: 5| Step: 3
Training loss: 2.834430694580078
Validation loss: 2.3620203823171635

Epoch: 5| Step: 4
Training loss: 2.1207430362701416
Validation loss: 2.3667749845853416

Epoch: 5| Step: 5
Training loss: 3.1194682121276855
Validation loss: 2.373535870223917

Epoch: 5| Step: 6
Training loss: 3.5771853923797607
Validation loss: 2.3587479924642913

Epoch: 5| Step: 7
Training loss: 2.5249741077423096
Validation loss: 2.350826496719032

Epoch: 5| Step: 8
Training loss: 1.8111501932144165
Validation loss: 2.3404408501040552

Epoch: 5| Step: 9
Training loss: 2.546316623687744
Validation loss: 2.340333543797975

Epoch: 5| Step: 10
Training loss: 1.9019025564193726
Validation loss: 2.337136358343145

Epoch: 201| Step: 0
Training loss: 3.2223496437072754
Validation loss: 2.3326446574221373

Epoch: 5| Step: 1
Training loss: 2.3863673210144043
Validation loss: 2.332112063643753

Epoch: 5| Step: 2
Training loss: 2.3375332355499268
Validation loss: 2.3387008969501784

Epoch: 5| Step: 3
Training loss: 2.525624990463257
Validation loss: 2.3377994978299705

Epoch: 5| Step: 4
Training loss: 1.8924440145492554
Validation loss: 2.3295841575950704

Epoch: 5| Step: 5
Training loss: 2.762533664703369
Validation loss: 2.334191750454646

Epoch: 5| Step: 6
Training loss: 2.436129093170166
Validation loss: 2.3467245153201524

Epoch: 5| Step: 7
Training loss: 2.60978627204895
Validation loss: 2.3510064566007225

Epoch: 5| Step: 8
Training loss: 2.618194103240967
Validation loss: 2.349218549266938

Epoch: 5| Step: 9
Training loss: 3.0633914470672607
Validation loss: 2.366020438491657

Epoch: 5| Step: 10
Training loss: 2.577329397201538
Validation loss: 2.359630464225687

Epoch: 202| Step: 0
Training loss: 2.5063891410827637
Validation loss: 2.3621270336130613

Epoch: 5| Step: 1
Training loss: 2.841951847076416
Validation loss: 2.3584901004709224

Epoch: 5| Step: 2
Training loss: 2.4787774085998535
Validation loss: 2.3505883396312757

Epoch: 5| Step: 3
Training loss: 2.571913957595825
Validation loss: 2.341005786772697

Epoch: 5| Step: 4
Training loss: 2.6548333168029785
Validation loss: 2.343577564403575

Epoch: 5| Step: 5
Training loss: 2.6494388580322266
Validation loss: 2.337994667791551

Epoch: 5| Step: 6
Training loss: 2.771583080291748
Validation loss: 2.3338962165258264

Epoch: 5| Step: 7
Training loss: 2.567384719848633
Validation loss: 2.3342706695679696

Epoch: 5| Step: 8
Training loss: 2.3121817111968994
Validation loss: 2.3389538616262455

Epoch: 5| Step: 9
Training loss: 2.5690081119537354
Validation loss: 2.3372100886478218

Epoch: 5| Step: 10
Training loss: 2.430197238922119
Validation loss: 2.336120661868844

Epoch: 203| Step: 0
Training loss: 2.739497661590576
Validation loss: 2.3344102726187757

Epoch: 5| Step: 1
Training loss: 2.1854758262634277
Validation loss: 2.3359670818492932

Epoch: 5| Step: 2
Training loss: 2.646956443786621
Validation loss: 2.3401032263232815

Epoch: 5| Step: 3
Training loss: 3.0777392387390137
Validation loss: 2.339313586552938

Epoch: 5| Step: 4
Training loss: 2.43284273147583
Validation loss: 2.3412084707649807

Epoch: 5| Step: 5
Training loss: 2.1704883575439453
Validation loss: 2.345968679715228

Epoch: 5| Step: 6
Training loss: 2.246591329574585
Validation loss: 2.3498078033488285

Epoch: 5| Step: 7
Training loss: 2.8353230953216553
Validation loss: 2.3505165141115905

Epoch: 5| Step: 8
Training loss: 3.146268844604492
Validation loss: 2.35304940900495

Epoch: 5| Step: 9
Training loss: 2.243727922439575
Validation loss: 2.3601209220065864

Epoch: 5| Step: 10
Training loss: 2.5835862159729004
Validation loss: 2.3665664811288156

Epoch: 204| Step: 0
Training loss: 2.457395076751709
Validation loss: 2.3692261967607724

Epoch: 5| Step: 1
Training loss: 2.5615217685699463
Validation loss: 2.369364517991261

Epoch: 5| Step: 2
Training loss: 2.5796611309051514
Validation loss: 2.3493807290190007

Epoch: 5| Step: 3
Training loss: 2.6400885581970215
Validation loss: 2.3521509196168635

Epoch: 5| Step: 4
Training loss: 2.705507755279541
Validation loss: 2.3537955168754823

Epoch: 5| Step: 5
Training loss: 2.098926067352295
Validation loss: 2.363779598666776

Epoch: 5| Step: 6
Training loss: 2.871764898300171
Validation loss: 2.368793328603109

Epoch: 5| Step: 7
Training loss: 2.9876906871795654
Validation loss: 2.362694396767565

Epoch: 5| Step: 8
Training loss: 2.918724536895752
Validation loss: 2.3718673695800123

Epoch: 5| Step: 9
Training loss: 2.4797136783599854
Validation loss: 2.3612593425217496

Epoch: 5| Step: 10
Training loss: 1.9481096267700195
Validation loss: 2.357471368646109

Epoch: 205| Step: 0
Training loss: 2.5487618446350098
Validation loss: 2.355122689277895

Epoch: 5| Step: 1
Training loss: 2.848625898361206
Validation loss: 2.3656118095562024

Epoch: 5| Step: 2
Training loss: 2.435337543487549
Validation loss: 2.3756245695134646

Epoch: 5| Step: 3
Training loss: 3.2690017223358154
Validation loss: 2.356779149783555

Epoch: 5| Step: 4
Training loss: 2.608516216278076
Validation loss: 2.353980182319559

Epoch: 5| Step: 5
Training loss: 2.561171770095825
Validation loss: 2.3574050062446186

Epoch: 5| Step: 6
Training loss: 2.549125909805298
Validation loss: 2.3478627897077993

Epoch: 5| Step: 7
Training loss: 2.6854374408721924
Validation loss: 2.3482252910572994

Epoch: 5| Step: 8
Training loss: 2.356760025024414
Validation loss: 2.3422950544664936

Epoch: 5| Step: 9
Training loss: 1.713404893875122
Validation loss: 2.3454056324497348

Epoch: 5| Step: 10
Training loss: 2.853710889816284
Validation loss: 2.3454556439512517

Epoch: 206| Step: 0
Training loss: 2.525270462036133
Validation loss: 2.339299217347176

Epoch: 5| Step: 1
Training loss: 2.4828195571899414
Validation loss: 2.332512435092721

Epoch: 5| Step: 2
Training loss: 3.099057674407959
Validation loss: 2.3413738896769862

Epoch: 5| Step: 3
Training loss: 2.7321181297302246
Validation loss: 2.340582916813512

Epoch: 5| Step: 4
Training loss: 2.5942912101745605
Validation loss: 2.336761390009234

Epoch: 5| Step: 5
Training loss: 2.2286627292633057
Validation loss: 2.3330612951709377

Epoch: 5| Step: 6
Training loss: 2.3043556213378906
Validation loss: 2.3356737065058883

Epoch: 5| Step: 7
Training loss: 2.859781265258789
Validation loss: 2.330378618291629

Epoch: 5| Step: 8
Training loss: 2.7582364082336426
Validation loss: 2.3361953612296813

Epoch: 5| Step: 9
Training loss: 2.2460415363311768
Validation loss: 2.342918485723516

Epoch: 5| Step: 10
Training loss: 2.5416064262390137
Validation loss: 2.3502387962033673

Epoch: 207| Step: 0
Training loss: 1.9838947057724
Validation loss: 2.3452247701665407

Epoch: 5| Step: 1
Training loss: 2.8920531272888184
Validation loss: 2.3522426979516142

Epoch: 5| Step: 2
Training loss: 2.5537877082824707
Validation loss: 2.348131587428431

Epoch: 5| Step: 3
Training loss: 2.250955820083618
Validation loss: 2.3562713053918656

Epoch: 5| Step: 4
Training loss: 2.2377402782440186
Validation loss: 2.352472230952273

Epoch: 5| Step: 5
Training loss: 3.0132663249969482
Validation loss: 2.3467290786004837

Epoch: 5| Step: 6
Training loss: 2.4769108295440674
Validation loss: 2.3510432781711703

Epoch: 5| Step: 7
Training loss: 2.539572238922119
Validation loss: 2.3497329681150374

Epoch: 5| Step: 8
Training loss: 2.5805447101593018
Validation loss: 2.3394038267033075

Epoch: 5| Step: 9
Training loss: 2.8691906929016113
Validation loss: 2.3388930033611994

Epoch: 5| Step: 10
Training loss: 2.894243001937866
Validation loss: 2.3338056610476587

Epoch: 208| Step: 0
Training loss: 2.1834311485290527
Validation loss: 2.3323217720113774

Epoch: 5| Step: 1
Training loss: 2.739914894104004
Validation loss: 2.3289534507259244

Epoch: 5| Step: 2
Training loss: 2.836897850036621
Validation loss: 2.335527609753352

Epoch: 5| Step: 3
Training loss: 2.5102314949035645
Validation loss: 2.34153813187794

Epoch: 5| Step: 4
Training loss: 2.7404515743255615
Validation loss: 2.3474225280105427

Epoch: 5| Step: 5
Training loss: 2.5709214210510254
Validation loss: 2.352510154888194

Epoch: 5| Step: 6
Training loss: 1.887769341468811
Validation loss: 2.352116307904643

Epoch: 5| Step: 7
Training loss: 1.9634870290756226
Validation loss: 2.348824167764315

Epoch: 5| Step: 8
Training loss: 3.0742688179016113
Validation loss: 2.356901814860682

Epoch: 5| Step: 9
Training loss: 3.071580410003662
Validation loss: 2.3569372264287805

Epoch: 5| Step: 10
Training loss: 2.800907611846924
Validation loss: 2.352557736058389

Epoch: 209| Step: 0
Training loss: 2.528952121734619
Validation loss: 2.34225719974887

Epoch: 5| Step: 1
Training loss: 2.971193790435791
Validation loss: 2.3355812436790875

Epoch: 5| Step: 2
Training loss: 2.1295087337493896
Validation loss: 2.333202008278139

Epoch: 5| Step: 3
Training loss: 2.069744110107422
Validation loss: 2.3324353976916243

Epoch: 5| Step: 4
Training loss: 2.337432384490967
Validation loss: 2.3355803515321467

Epoch: 5| Step: 5
Training loss: 2.7711563110351562
Validation loss: 2.336000200240843

Epoch: 5| Step: 6
Training loss: 2.8907878398895264
Validation loss: 2.339469322594263

Epoch: 5| Step: 7
Training loss: 1.7501585483551025
Validation loss: 2.340590236007526

Epoch: 5| Step: 8
Training loss: 2.9333224296569824
Validation loss: 2.343641842565229

Epoch: 5| Step: 9
Training loss: 3.4427597522735596
Validation loss: 2.342389532314834

Epoch: 5| Step: 10
Training loss: 2.419954299926758
Validation loss: 2.3421714049513622

Epoch: 210| Step: 0
Training loss: 2.9003958702087402
Validation loss: 2.356210844491118

Epoch: 5| Step: 1
Training loss: 2.3913817405700684
Validation loss: 2.349803569496319

Epoch: 5| Step: 2
Training loss: 2.833836555480957
Validation loss: 2.3625983320256716

Epoch: 5| Step: 3
Training loss: 2.575345516204834
Validation loss: 2.3599636298353954

Epoch: 5| Step: 4
Training loss: 2.397416591644287
Validation loss: 2.363830776624782

Epoch: 5| Step: 5
Training loss: 2.7564263343811035
Validation loss: 2.368796939490944

Epoch: 5| Step: 6
Training loss: 3.165421962738037
Validation loss: 2.363537326935799

Epoch: 5| Step: 7
Training loss: 2.5221335887908936
Validation loss: 2.3687059135847193

Epoch: 5| Step: 8
Training loss: 2.529616117477417
Validation loss: 2.353233127183812

Epoch: 5| Step: 9
Training loss: 2.0198781490325928
Validation loss: 2.3362082307056715

Epoch: 5| Step: 10
Training loss: 2.112673044204712
Validation loss: 2.3223380196479058

Epoch: 211| Step: 0
Training loss: 2.7385060787200928
Validation loss: 2.318820056094918

Epoch: 5| Step: 1
Training loss: 2.59855318069458
Validation loss: 2.3219696014158187

Epoch: 5| Step: 2
Training loss: 2.21067476272583
Validation loss: 2.317579992355839

Epoch: 5| Step: 3
Training loss: 2.7723581790924072
Validation loss: 2.326188430991224

Epoch: 5| Step: 4
Training loss: 2.845550775527954
Validation loss: 2.3211564338335426

Epoch: 5| Step: 5
Training loss: 2.396078586578369
Validation loss: 2.330469331433696

Epoch: 5| Step: 6
Training loss: 2.743677854537964
Validation loss: 2.3421999203261508

Epoch: 5| Step: 7
Training loss: 2.7495124340057373
Validation loss: 2.340531249200144

Epoch: 5| Step: 8
Training loss: 2.4725987911224365
Validation loss: 2.3439607453602616

Epoch: 5| Step: 9
Training loss: 2.540215015411377
Validation loss: 2.3613020912293465

Epoch: 5| Step: 10
Training loss: 2.197662591934204
Validation loss: 2.3604344168016986

Epoch: 212| Step: 0
Training loss: 3.193087577819824
Validation loss: 2.351764932755501

Epoch: 5| Step: 1
Training loss: 2.14024019241333
Validation loss: 2.3563415132543093

Epoch: 5| Step: 2
Training loss: 2.6654696464538574
Validation loss: 2.3496183861968336

Epoch: 5| Step: 3
Training loss: 2.7740890979766846
Validation loss: 2.3475339284507175

Epoch: 5| Step: 4
Training loss: 1.9734060764312744
Validation loss: 2.351091351560367

Epoch: 5| Step: 5
Training loss: 2.688840627670288
Validation loss: 2.3616813459704

Epoch: 5| Step: 6
Training loss: 2.495694398880005
Validation loss: 2.3511545940112044

Epoch: 5| Step: 7
Training loss: 2.266321897506714
Validation loss: 2.3411101320738434

Epoch: 5| Step: 8
Training loss: 2.149756669998169
Validation loss: 2.334710157045754

Epoch: 5| Step: 9
Training loss: 3.092928647994995
Validation loss: 2.3332446006036576

Epoch: 5| Step: 10
Training loss: 2.8183414936065674
Validation loss: 2.3267553621722805

Epoch: 213| Step: 0
Training loss: 3.03436279296875
Validation loss: 2.327439699121701

Epoch: 5| Step: 1
Training loss: 2.3149635791778564
Validation loss: 2.3300182768093642

Epoch: 5| Step: 2
Training loss: 2.449449062347412
Validation loss: 2.333875709964383

Epoch: 5| Step: 3
Training loss: 2.926928997039795
Validation loss: 2.324070874080863

Epoch: 5| Step: 4
Training loss: 2.367849111557007
Validation loss: 2.322126241140468

Epoch: 5| Step: 5
Training loss: 2.8003368377685547
Validation loss: 2.319245353821785

Epoch: 5| Step: 6
Training loss: 2.681053876876831
Validation loss: 2.324344542718703

Epoch: 5| Step: 7
Training loss: 2.7027697563171387
Validation loss: 2.3248206556484265

Epoch: 5| Step: 8
Training loss: 1.4649581909179688
Validation loss: 2.320932113996116

Epoch: 5| Step: 9
Training loss: 3.0029165744781494
Validation loss: 2.328299104526479

Epoch: 5| Step: 10
Training loss: 2.4497458934783936
Validation loss: 2.338688968330301

Epoch: 214| Step: 0
Training loss: 2.689190149307251
Validation loss: 2.3323081821523686

Epoch: 5| Step: 1
Training loss: 2.4866387844085693
Validation loss: 2.3379951907742407

Epoch: 5| Step: 2
Training loss: 2.5699591636657715
Validation loss: 2.3408022337062384

Epoch: 5| Step: 3
Training loss: 2.240288734436035
Validation loss: 2.333019213009906

Epoch: 5| Step: 4
Training loss: 2.6486589908599854
Validation loss: 2.3414340429408576

Epoch: 5| Step: 5
Training loss: 2.54524302482605
Validation loss: 2.335209869569348

Epoch: 5| Step: 6
Training loss: 2.183340549468994
Validation loss: 2.3493174942590858

Epoch: 5| Step: 7
Training loss: 2.6510581970214844
Validation loss: 2.34891422845984

Epoch: 5| Step: 8
Training loss: 2.7937541007995605
Validation loss: 2.3579604856429563

Epoch: 5| Step: 9
Training loss: 2.8743045330047607
Validation loss: 2.349928517495432

Epoch: 5| Step: 10
Training loss: 2.501068592071533
Validation loss: 2.344621269933639

Epoch: 215| Step: 0
Training loss: 2.4926466941833496
Validation loss: 2.336754181051767

Epoch: 5| Step: 1
Training loss: 2.614428997039795
Validation loss: 2.334641090003393

Epoch: 5| Step: 2
Training loss: 3.4053795337677
Validation loss: 2.3340351376482236

Epoch: 5| Step: 3
Training loss: 2.4883458614349365
Validation loss: 2.3187740246454873

Epoch: 5| Step: 4
Training loss: 2.4593236446380615
Validation loss: 2.3275519699178715

Epoch: 5| Step: 5
Training loss: 2.308929204940796
Validation loss: 2.3211084309444634

Epoch: 5| Step: 6
Training loss: 2.4860236644744873
Validation loss: 2.317265054231049

Epoch: 5| Step: 7
Training loss: 2.0887789726257324
Validation loss: 2.3150640508180023

Epoch: 5| Step: 8
Training loss: 2.4081389904022217
Validation loss: 2.320743978664439

Epoch: 5| Step: 9
Training loss: 2.059495449066162
Validation loss: 2.3295986331919187

Epoch: 5| Step: 10
Training loss: 3.5086357593536377
Validation loss: 2.345304258408085

Epoch: 216| Step: 0
Training loss: 2.4552316665649414
Validation loss: 2.3462790237959994

Epoch: 5| Step: 1
Training loss: 3.3260912895202637
Validation loss: 2.362331267326109

Epoch: 5| Step: 2
Training loss: 2.6545398235321045
Validation loss: 2.366455611362252

Epoch: 5| Step: 3
Training loss: 2.4436497688293457
Validation loss: 2.3601862743336666

Epoch: 5| Step: 4
Training loss: 2.2475802898406982
Validation loss: 2.3390163196030485

Epoch: 5| Step: 5
Training loss: 2.4876787662506104
Validation loss: 2.3427797837923934

Epoch: 5| Step: 6
Training loss: 3.5062224864959717
Validation loss: 2.3210550405645884

Epoch: 5| Step: 7
Training loss: 2.634002208709717
Validation loss: 2.314839437443723

Epoch: 5| Step: 8
Training loss: 2.1677768230438232
Validation loss: 2.3085560785826815

Epoch: 5| Step: 9
Training loss: 2.5924417972564697
Validation loss: 2.310994966055757

Epoch: 5| Step: 10
Training loss: 1.731805682182312
Validation loss: 2.3089343347857074

Epoch: 217| Step: 0
Training loss: 2.9889705181121826
Validation loss: 2.3108038248554355

Epoch: 5| Step: 1
Training loss: 3.191427230834961
Validation loss: 2.3067120787917927

Epoch: 5| Step: 2
Training loss: 1.997575044631958
Validation loss: 2.311124704217398

Epoch: 5| Step: 3
Training loss: 2.909137725830078
Validation loss: 2.320169020724553

Epoch: 5| Step: 4
Training loss: 2.3849148750305176
Validation loss: 2.3144924999565206

Epoch: 5| Step: 5
Training loss: 1.8494428396224976
Validation loss: 2.322119541065667

Epoch: 5| Step: 6
Training loss: 2.0653631687164307
Validation loss: 2.332267149802177

Epoch: 5| Step: 7
Training loss: 2.176664352416992
Validation loss: 2.3393449193687847

Epoch: 5| Step: 8
Training loss: 2.8519959449768066
Validation loss: 2.33934510138727

Epoch: 5| Step: 9
Training loss: 2.949258804321289
Validation loss: 2.3475108018485447

Epoch: 5| Step: 10
Training loss: 2.8896238803863525
Validation loss: 2.3499898602885585

Epoch: 218| Step: 0
Training loss: 2.6761837005615234
Validation loss: 2.3418166868148313

Epoch: 5| Step: 1
Training loss: 2.380702257156372
Validation loss: 2.339074468099943

Epoch: 5| Step: 2
Training loss: 2.2736656665802
Validation loss: 2.321435389980193

Epoch: 5| Step: 3
Training loss: 2.8257904052734375
Validation loss: 2.317833856869769

Epoch: 5| Step: 4
Training loss: 2.6122021675109863
Validation loss: 2.3234377599531606

Epoch: 5| Step: 5
Training loss: 2.607544422149658
Validation loss: 2.318112457952192

Epoch: 5| Step: 6
Training loss: 2.208599090576172
Validation loss: 2.3180478054990052

Epoch: 5| Step: 7
Training loss: 2.689117908477783
Validation loss: 2.3174593679366575

Epoch: 5| Step: 8
Training loss: 2.8775954246520996
Validation loss: 2.3147139651800996

Epoch: 5| Step: 9
Training loss: 2.334847927093506
Validation loss: 2.317129942678636

Epoch: 5| Step: 10
Training loss: 2.625798463821411
Validation loss: 2.3185727339918896

Epoch: 219| Step: 0
Training loss: 2.5647037029266357
Validation loss: 2.319508626896848

Epoch: 5| Step: 1
Training loss: 2.979465961456299
Validation loss: 2.322045257014613

Epoch: 5| Step: 2
Training loss: 3.125800132751465
Validation loss: 2.3302097782011955

Epoch: 5| Step: 3
Training loss: 2.870649576187134
Validation loss: 2.3377521217510266

Epoch: 5| Step: 4
Training loss: 2.3947136402130127
Validation loss: 2.335758040028234

Epoch: 5| Step: 5
Training loss: 2.5109047889709473
Validation loss: 2.336428085962931

Epoch: 5| Step: 6
Training loss: 2.229496479034424
Validation loss: 2.344116913375034

Epoch: 5| Step: 7
Training loss: 2.0475335121154785
Validation loss: 2.344479327560753

Epoch: 5| Step: 8
Training loss: 2.6688613891601562
Validation loss: 2.342169843694215

Epoch: 5| Step: 9
Training loss: 2.5957589149475098
Validation loss: 2.325721186976279

Epoch: 5| Step: 10
Training loss: 2.0995373725891113
Validation loss: 2.3220092583728094

Epoch: 220| Step: 0
Training loss: 2.6046204566955566
Validation loss: 2.3225611563651793

Epoch: 5| Step: 1
Training loss: 2.615813970565796
Validation loss: 2.3150665375494186

Epoch: 5| Step: 2
Training loss: 2.2181613445281982
Validation loss: 2.3081911456200386

Epoch: 5| Step: 3
Training loss: 2.243253469467163
Validation loss: 2.3111734492804414

Epoch: 5| Step: 4
Training loss: 2.814761161804199
Validation loss: 2.3178455521983485

Epoch: 5| Step: 5
Training loss: 3.0080349445343018
Validation loss: 2.323026277685678

Epoch: 5| Step: 6
Training loss: 2.919670343399048
Validation loss: 2.324457178833664

Epoch: 5| Step: 7
Training loss: 2.6187148094177246
Validation loss: 2.3300394332537087

Epoch: 5| Step: 8
Training loss: 2.714723825454712
Validation loss: 2.326128100836149

Epoch: 5| Step: 9
Training loss: 1.8735564947128296
Validation loss: 2.337322847817534

Epoch: 5| Step: 10
Training loss: 2.492617130279541
Validation loss: 2.3419341066832184

Epoch: 221| Step: 0
Training loss: 2.7595791816711426
Validation loss: 2.3379156384416806

Epoch: 5| Step: 1
Training loss: 2.596550226211548
Validation loss: 2.3443412934580157

Epoch: 5| Step: 2
Training loss: 2.945380926132202
Validation loss: 2.3406431316047587

Epoch: 5| Step: 3
Training loss: 2.7417452335357666
Validation loss: 2.3509705938318723

Epoch: 5| Step: 4
Training loss: 2.163691282272339
Validation loss: 2.351860994933754

Epoch: 5| Step: 5
Training loss: 2.3734824657440186
Validation loss: 2.348349699410059

Epoch: 5| Step: 6
Training loss: 2.292311191558838
Validation loss: 2.3310119285378406

Epoch: 5| Step: 7
Training loss: 3.1583149433135986
Validation loss: 2.3199640038192912

Epoch: 5| Step: 8
Training loss: 1.8857978582382202
Validation loss: 2.3122204478069017

Epoch: 5| Step: 9
Training loss: 2.6795918941497803
Validation loss: 2.3102769505593086

Epoch: 5| Step: 10
Training loss: 2.4872188568115234
Validation loss: 2.30721035311299

Epoch: 222| Step: 0
Training loss: 2.252392053604126
Validation loss: 2.307204561848794

Epoch: 5| Step: 1
Training loss: 2.7881178855895996
Validation loss: 2.31346389555162

Epoch: 5| Step: 2
Training loss: 2.4900801181793213
Validation loss: 2.32052029332807

Epoch: 5| Step: 3
Training loss: 2.828625202178955
Validation loss: 2.3205613756692536

Epoch: 5| Step: 4
Training loss: 2.3124642372131348
Validation loss: 2.3269371268569783

Epoch: 5| Step: 5
Training loss: 2.570206642150879
Validation loss: 2.3349025095662763

Epoch: 5| Step: 6
Training loss: 2.811622142791748
Validation loss: 2.3377202813343336

Epoch: 5| Step: 7
Training loss: 2.158949375152588
Validation loss: 2.3420212038101687

Epoch: 5| Step: 8
Training loss: 2.5646088123321533
Validation loss: 2.352986384463567

Epoch: 5| Step: 9
Training loss: 2.6947598457336426
Validation loss: 2.345043225954938

Epoch: 5| Step: 10
Training loss: 2.637589931488037
Validation loss: 2.3494014304171325

Epoch: 223| Step: 0
Training loss: 2.643637180328369
Validation loss: 2.343618785181353

Epoch: 5| Step: 1
Training loss: 3.165419101715088
Validation loss: 2.3292069588938067

Epoch: 5| Step: 2
Training loss: 2.076688289642334
Validation loss: 2.3195577283059396

Epoch: 5| Step: 3
Training loss: 2.3257923126220703
Validation loss: 2.3234735458127913

Epoch: 5| Step: 4
Training loss: 2.2268054485321045
Validation loss: 2.314199527104696

Epoch: 5| Step: 5
Training loss: 2.8927597999572754
Validation loss: 2.3156322189556655

Epoch: 5| Step: 6
Training loss: 2.6332812309265137
Validation loss: 2.3154720593524236

Epoch: 5| Step: 7
Training loss: 2.583803653717041
Validation loss: 2.322234365247911

Epoch: 5| Step: 8
Training loss: 3.032801389694214
Validation loss: 2.3209827997351207

Epoch: 5| Step: 9
Training loss: 2.5672268867492676
Validation loss: 2.327201074169528

Epoch: 5| Step: 10
Training loss: 1.8760366439819336
Validation loss: 2.324820755630411

Epoch: 224| Step: 0
Training loss: 2.6446375846862793
Validation loss: 2.3290236098791963

Epoch: 5| Step: 1
Training loss: 2.5981996059417725
Validation loss: 2.335684989088325

Epoch: 5| Step: 2
Training loss: 2.5970749855041504
Validation loss: 2.330149063500025

Epoch: 5| Step: 3
Training loss: 2.9669387340545654
Validation loss: 2.3371039513618714

Epoch: 5| Step: 4
Training loss: 1.9560962915420532
Validation loss: 2.3318062777160318

Epoch: 5| Step: 5
Training loss: 2.9446215629577637
Validation loss: 2.3238685489982687

Epoch: 5| Step: 6
Training loss: 2.1502163410186768
Validation loss: 2.333926239321309

Epoch: 5| Step: 7
Training loss: 3.2620983123779297
Validation loss: 2.319357325953822

Epoch: 5| Step: 8
Training loss: 2.506772518157959
Validation loss: 2.3155025512941423

Epoch: 5| Step: 9
Training loss: 1.96695077419281
Validation loss: 2.3174478161719536

Epoch: 5| Step: 10
Training loss: 2.4438509941101074
Validation loss: 2.303783192429491

Epoch: 225| Step: 0
Training loss: 2.9830639362335205
Validation loss: 2.300329969775292

Epoch: 5| Step: 1
Training loss: 2.6286747455596924
Validation loss: 2.3053460403155257

Epoch: 5| Step: 2
Training loss: 2.4105637073516846
Validation loss: 2.298408172463858

Epoch: 5| Step: 3
Training loss: 2.404428005218506
Validation loss: 2.3024427724140946

Epoch: 5| Step: 4
Training loss: 2.1534721851348877
Validation loss: 2.3006059354351414

Epoch: 5| Step: 5
Training loss: 3.1823837757110596
Validation loss: 2.294953200124925

Epoch: 5| Step: 6
Training loss: 2.4510250091552734
Validation loss: 2.2979813826981412

Epoch: 5| Step: 7
Training loss: 2.84954571723938
Validation loss: 2.3000588378598614

Epoch: 5| Step: 8
Training loss: 2.2076520919799805
Validation loss: 2.2967269958988314

Epoch: 5| Step: 9
Training loss: 1.9135395288467407
Validation loss: 2.2967448593467794

Epoch: 5| Step: 10
Training loss: 2.994783401489258
Validation loss: 2.2976259877604823

Epoch: 226| Step: 0
Training loss: 2.8584818840026855
Validation loss: 2.3042570903737056

Epoch: 5| Step: 1
Training loss: 2.614060401916504
Validation loss: 2.311166767151125

Epoch: 5| Step: 2
Training loss: 3.063605308532715
Validation loss: 2.3096523413094143

Epoch: 5| Step: 3
Training loss: 2.5567800998687744
Validation loss: 2.322307930197767

Epoch: 5| Step: 4
Training loss: 2.1254215240478516
Validation loss: 2.3224702573591665

Epoch: 5| Step: 5
Training loss: 2.310279130935669
Validation loss: 2.328132565303515

Epoch: 5| Step: 6
Training loss: 2.3612735271453857
Validation loss: 2.319823864967592

Epoch: 5| Step: 7
Training loss: 2.968806266784668
Validation loss: 2.3244463500156196

Epoch: 5| Step: 8
Training loss: 2.6193604469299316
Validation loss: 2.3245681896004626

Epoch: 5| Step: 9
Training loss: 2.2787082195281982
Validation loss: 2.323748552671043

Epoch: 5| Step: 10
Training loss: 2.18025803565979
Validation loss: 2.3215784924004668

Epoch: 227| Step: 0
Training loss: 2.6785149574279785
Validation loss: 2.3167468668312154

Epoch: 5| Step: 1
Training loss: 2.7210984230041504
Validation loss: 2.309243940537976

Epoch: 5| Step: 2
Training loss: 2.853426218032837
Validation loss: 2.3234977465803905

Epoch: 5| Step: 3
Training loss: 1.8855139017105103
Validation loss: 2.3171640442263697

Epoch: 5| Step: 4
Training loss: 2.528107166290283
Validation loss: 2.3187271010491157

Epoch: 5| Step: 5
Training loss: 2.2186694145202637
Validation loss: 2.3169508723802466

Epoch: 5| Step: 6
Training loss: 1.7066209316253662
Validation loss: 2.3227901740740706

Epoch: 5| Step: 7
Training loss: 2.9133284091949463
Validation loss: 2.3175233371796145

Epoch: 5| Step: 8
Training loss: 2.946805477142334
Validation loss: 2.319150294027021

Epoch: 5| Step: 9
Training loss: 2.4881837368011475
Validation loss: 2.315437243830773

Epoch: 5| Step: 10
Training loss: 3.1673343181610107
Validation loss: 2.3252744943864885

Epoch: 228| Step: 0
Training loss: 1.7615611553192139
Validation loss: 2.321069727661789

Epoch: 5| Step: 1
Training loss: 2.6007800102233887
Validation loss: 2.3288593164054294

Epoch: 5| Step: 2
Training loss: 2.4143290519714355
Validation loss: 2.3238577573530135

Epoch: 5| Step: 3
Training loss: 3.401484727859497
Validation loss: 2.3314431328927316

Epoch: 5| Step: 4
Training loss: 2.374168872833252
Validation loss: 2.3347925063102477

Epoch: 5| Step: 5
Training loss: 2.6759490966796875
Validation loss: 2.3252677776480235

Epoch: 5| Step: 6
Training loss: 2.7698681354522705
Validation loss: 2.328483530270156

Epoch: 5| Step: 7
Training loss: 2.4605884552001953
Validation loss: 2.3188690267583376

Epoch: 5| Step: 8
Training loss: 2.551259756088257
Validation loss: 2.3129028427985405

Epoch: 5| Step: 9
Training loss: 2.274754285812378
Validation loss: 2.3083951498872493

Epoch: 5| Step: 10
Training loss: 2.724259853363037
Validation loss: 2.3076020133110786

Epoch: 229| Step: 0
Training loss: 2.3676350116729736
Validation loss: 2.3104340748120378

Epoch: 5| Step: 1
Training loss: 2.6698179244995117
Validation loss: 2.301762483453238

Epoch: 5| Step: 2
Training loss: 2.089740753173828
Validation loss: 2.293993889644582

Epoch: 5| Step: 3
Training loss: 2.454235076904297
Validation loss: 2.2942032762753066

Epoch: 5| Step: 4
Training loss: 3.115802526473999
Validation loss: 2.3116579491605043

Epoch: 5| Step: 5
Training loss: 2.7382760047912598
Validation loss: 2.3129677259793846

Epoch: 5| Step: 6
Training loss: 2.533719539642334
Validation loss: 2.321213230010002

Epoch: 5| Step: 7
Training loss: 2.715892791748047
Validation loss: 2.3198734047592326

Epoch: 5| Step: 8
Training loss: 2.6787126064300537
Validation loss: 2.3125136616409465

Epoch: 5| Step: 9
Training loss: 2.251737117767334
Validation loss: 2.3182099429509972

Epoch: 5| Step: 10
Training loss: 2.2792253494262695
Validation loss: 2.309209603135304

Epoch: 230| Step: 0
Training loss: 2.427366256713867
Validation loss: 2.3000846191119124

Epoch: 5| Step: 1
Training loss: 2.59260630607605
Validation loss: 2.3055898912491335

Epoch: 5| Step: 2
Training loss: 2.8324639797210693
Validation loss: 2.3037664890289307

Epoch: 5| Step: 3
Training loss: 2.714355707168579
Validation loss: 2.2933349071010465

Epoch: 5| Step: 4
Training loss: 2.6620190143585205
Validation loss: 2.3060713045058714

Epoch: 5| Step: 5
Training loss: 2.671800136566162
Validation loss: 2.2998916948995283

Epoch: 5| Step: 6
Training loss: 2.0679993629455566
Validation loss: 2.303341414338799

Epoch: 5| Step: 7
Training loss: 2.135683536529541
Validation loss: 2.304456028887021

Epoch: 5| Step: 8
Training loss: 3.2025749683380127
Validation loss: 2.3158964444232244

Epoch: 5| Step: 9
Training loss: 2.704206943511963
Validation loss: 2.3177252764342935

Epoch: 5| Step: 10
Training loss: 1.8227015733718872
Validation loss: 2.30678827019148

Epoch: 231| Step: 0
Training loss: 1.9929237365722656
Validation loss: 2.3274298662780435

Epoch: 5| Step: 1
Training loss: 2.25492525100708
Validation loss: 2.3181194400274627

Epoch: 5| Step: 2
Training loss: 3.1573143005371094
Validation loss: 2.32332435987329

Epoch: 5| Step: 3
Training loss: 2.966482162475586
Validation loss: 2.333115459770285

Epoch: 5| Step: 4
Training loss: 1.6438919305801392
Validation loss: 2.329017077722857

Epoch: 5| Step: 5
Training loss: 2.3103644847869873
Validation loss: 2.3256966208898895

Epoch: 5| Step: 6
Training loss: 2.1629478931427
Validation loss: 2.3359701659089778

Epoch: 5| Step: 7
Training loss: 2.858379364013672
Validation loss: 2.333163120413339

Epoch: 5| Step: 8
Training loss: 2.891139507293701
Validation loss: 2.3363248917364303

Epoch: 5| Step: 9
Training loss: 2.960848569869995
Validation loss: 2.3304552391011226

Epoch: 5| Step: 10
Training loss: 2.741162061691284
Validation loss: 2.3165160968739498

Epoch: 232| Step: 0
Training loss: 1.7723281383514404
Validation loss: 2.315936826890515

Epoch: 5| Step: 1
Training loss: 2.672027587890625
Validation loss: 2.3158472686685543

Epoch: 5| Step: 2
Training loss: 3.398510456085205
Validation loss: 2.313527925040132

Epoch: 5| Step: 3
Training loss: 2.3370144367218018
Validation loss: 2.312426672186903

Epoch: 5| Step: 4
Training loss: 3.0781736373901367
Validation loss: 2.315426616258519

Epoch: 5| Step: 5
Training loss: 2.717531681060791
Validation loss: 2.307926684297541

Epoch: 5| Step: 6
Training loss: 2.0938608646392822
Validation loss: 2.301577898763841

Epoch: 5| Step: 7
Training loss: 2.845884084701538
Validation loss: 2.304652057668214

Epoch: 5| Step: 8
Training loss: 1.926262617111206
Validation loss: 2.2944211498383553

Epoch: 5| Step: 9
Training loss: 2.2054035663604736
Validation loss: 2.3075775946340253

Epoch: 5| Step: 10
Training loss: 2.857534170150757
Validation loss: 2.302020339555638

Epoch: 233| Step: 0
Training loss: 2.190342903137207
Validation loss: 2.3116372554532942

Epoch: 5| Step: 1
Training loss: 2.7853569984436035
Validation loss: 2.3053354473524195

Epoch: 5| Step: 2
Training loss: 2.6349985599517822
Validation loss: 2.316389937554636

Epoch: 5| Step: 3
Training loss: 2.0569941997528076
Validation loss: 2.3152701341977684

Epoch: 5| Step: 4
Training loss: 2.7312448024749756
Validation loss: 2.309760985835906

Epoch: 5| Step: 5
Training loss: 2.8500778675079346
Validation loss: 2.3052827863283056

Epoch: 5| Step: 6
Training loss: 2.8251922130584717
Validation loss: 2.311340408940469

Epoch: 5| Step: 7
Training loss: 3.2414016723632812
Validation loss: 2.3033352974922425

Epoch: 5| Step: 8
Training loss: 2.129950761795044
Validation loss: 2.2904186402597735

Epoch: 5| Step: 9
Training loss: 1.9841220378875732
Validation loss: 2.28926742974148

Epoch: 5| Step: 10
Training loss: 2.5366570949554443
Validation loss: 2.2811601674684914

Epoch: 234| Step: 0
Training loss: 2.7979190349578857
Validation loss: 2.281460315950455

Epoch: 5| Step: 1
Training loss: 2.4025278091430664
Validation loss: 2.2864449152382473

Epoch: 5| Step: 2
Training loss: 1.7168645858764648
Validation loss: 2.2834628397418606

Epoch: 5| Step: 3
Training loss: 2.797989845275879
Validation loss: 2.2835679823352444

Epoch: 5| Step: 4
Training loss: 2.602914333343506
Validation loss: 2.290784371796475

Epoch: 5| Step: 5
Training loss: 2.524284601211548
Validation loss: 2.2903351117205877

Epoch: 5| Step: 6
Training loss: 2.9105238914489746
Validation loss: 2.295935256506807

Epoch: 5| Step: 7
Training loss: 2.5527758598327637
Validation loss: 2.2959224780400596

Epoch: 5| Step: 8
Training loss: 2.5951030254364014
Validation loss: 2.30306682022669

Epoch: 5| Step: 9
Training loss: 2.4908957481384277
Validation loss: 2.313512399632444

Epoch: 5| Step: 10
Training loss: 2.479483127593994
Validation loss: 2.328178669816704

Epoch: 235| Step: 0
Training loss: 2.489837169647217
Validation loss: 2.3443598285798104

Epoch: 5| Step: 1
Training loss: 2.2036361694335938
Validation loss: 2.341426626328499

Epoch: 5| Step: 2
Training loss: 2.825899839401245
Validation loss: 2.349506885774674

Epoch: 5| Step: 3
Training loss: 2.262927532196045
Validation loss: 2.3433591960578837

Epoch: 5| Step: 4
Training loss: 2.420848846435547
Validation loss: 2.3610923495343936

Epoch: 5| Step: 5
Training loss: 2.9023396968841553
Validation loss: 2.35529589140287

Epoch: 5| Step: 6
Training loss: 2.561878204345703
Validation loss: 2.3582102380773073

Epoch: 5| Step: 7
Training loss: 2.7179064750671387
Validation loss: 2.3579680227464244

Epoch: 5| Step: 8
Training loss: 2.524336814880371
Validation loss: 2.3480701856715704

Epoch: 5| Step: 9
Training loss: 2.8864316940307617
Validation loss: 2.3425044372517574

Epoch: 5| Step: 10
Training loss: 2.128553628921509
Validation loss: 2.3277621705044984

Epoch: 236| Step: 0
Training loss: 2.0860595703125
Validation loss: 2.3091670069643246

Epoch: 5| Step: 1
Training loss: 2.193291664123535
Validation loss: 2.299144250090404

Epoch: 5| Step: 2
Training loss: 1.9080333709716797
Validation loss: 2.2864748213880803

Epoch: 5| Step: 3
Training loss: 2.621814250946045
Validation loss: 2.2932063533413793

Epoch: 5| Step: 4
Training loss: 2.791665554046631
Validation loss: 2.2833115618716002

Epoch: 5| Step: 5
Training loss: 2.891936779022217
Validation loss: 2.2788397753110496

Epoch: 5| Step: 6
Training loss: 2.9972984790802
Validation loss: 2.273836826765409

Epoch: 5| Step: 7
Training loss: 2.390228271484375
Validation loss: 2.2829338581331315

Epoch: 5| Step: 8
Training loss: 2.9612369537353516
Validation loss: 2.289948260912331

Epoch: 5| Step: 9
Training loss: 2.707132339477539
Validation loss: 2.28637198991673

Epoch: 5| Step: 10
Training loss: 2.354750633239746
Validation loss: 2.292342749975061

Epoch: 237| Step: 0
Training loss: 3.1886420249938965
Validation loss: 2.306493479718444

Epoch: 5| Step: 1
Training loss: 2.769160270690918
Validation loss: 2.301652713488507

Epoch: 5| Step: 2
Training loss: 2.9150989055633545
Validation loss: 2.316882730812155

Epoch: 5| Step: 3
Training loss: 2.3214125633239746
Validation loss: 2.3186446210389495

Epoch: 5| Step: 4
Training loss: 3.096374034881592
Validation loss: 2.3196314252832884

Epoch: 5| Step: 5
Training loss: 2.37434458732605
Validation loss: 2.305090149243673

Epoch: 5| Step: 6
Training loss: 2.2307348251342773
Validation loss: 2.303154448027252

Epoch: 5| Step: 7
Training loss: 2.528888463973999
Validation loss: 2.2884787795364216

Epoch: 5| Step: 8
Training loss: 2.041865587234497
Validation loss: 2.2852409590956984

Epoch: 5| Step: 9
Training loss: 2.031787157058716
Validation loss: 2.278472487644483

Epoch: 5| Step: 10
Training loss: 2.3842685222625732
Validation loss: 2.2839716531897105

Epoch: 238| Step: 0
Training loss: 2.2746269702911377
Validation loss: 2.275884377059116

Epoch: 5| Step: 1
Training loss: 2.5780863761901855
Validation loss: 2.283491547389697

Epoch: 5| Step: 2
Training loss: 2.6546268463134766
Validation loss: 2.2819044051631803

Epoch: 5| Step: 3
Training loss: 2.1341969966888428
Validation loss: 2.300561939516375

Epoch: 5| Step: 4
Training loss: 2.9306225776672363
Validation loss: 2.307872055679239

Epoch: 5| Step: 5
Training loss: 2.272569417953491
Validation loss: 2.3194901866297566

Epoch: 5| Step: 6
Training loss: 3.2408604621887207
Validation loss: 2.3285638465676257

Epoch: 5| Step: 7
Training loss: 2.134596347808838
Validation loss: 2.3312215343598397

Epoch: 5| Step: 8
Training loss: 2.395620107650757
Validation loss: 2.3273403208742858

Epoch: 5| Step: 9
Training loss: 2.4855175018310547
Validation loss: 2.3316233158111572

Epoch: 5| Step: 10
Training loss: 2.8539559841156006
Validation loss: 2.319113987748341

Epoch: 239| Step: 0
Training loss: 2.341230869293213
Validation loss: 2.3228335970191547

Epoch: 5| Step: 1
Training loss: 2.9942994117736816
Validation loss: 2.3215827275347967

Epoch: 5| Step: 2
Training loss: 3.080681324005127
Validation loss: 2.300059779997795

Epoch: 5| Step: 3
Training loss: 2.3651986122131348
Validation loss: 2.2921353347839846

Epoch: 5| Step: 4
Training loss: 2.36029052734375
Validation loss: 2.2904892121591875

Epoch: 5| Step: 5
Training loss: 1.88912832736969
Validation loss: 2.2929325898488364

Epoch: 5| Step: 6
Training loss: 2.3662314414978027
Validation loss: 2.301624980024112

Epoch: 5| Step: 7
Training loss: 2.38885498046875
Validation loss: 2.3058736657583587

Epoch: 5| Step: 8
Training loss: 2.4493587017059326
Validation loss: 2.317000327571746

Epoch: 5| Step: 9
Training loss: 2.586082935333252
Validation loss: 2.323790757886825

Epoch: 5| Step: 10
Training loss: 3.1551334857940674
Validation loss: 2.321052348741921

Epoch: 240| Step: 0
Training loss: 2.7782235145568848
Validation loss: 2.3132439967124694

Epoch: 5| Step: 1
Training loss: 2.713292121887207
Validation loss: 2.319451819184006

Epoch: 5| Step: 2
Training loss: 2.8053081035614014
Validation loss: 2.3049705592534875

Epoch: 5| Step: 3
Training loss: 1.851771593093872
Validation loss: 2.300225983383835

Epoch: 5| Step: 4
Training loss: 2.488848924636841
Validation loss: 2.3012758865151355

Epoch: 5| Step: 5
Training loss: 2.747678756713867
Validation loss: 2.3022430737813315

Epoch: 5| Step: 6
Training loss: 2.399702548980713
Validation loss: 2.287689533284915

Epoch: 5| Step: 7
Training loss: 2.5742697715759277
Validation loss: 2.2907633781433105

Epoch: 5| Step: 8
Training loss: 2.733135461807251
Validation loss: 2.3008073632435133

Epoch: 5| Step: 9
Training loss: 2.7251675128936768
Validation loss: 2.2891029593765095

Epoch: 5| Step: 10
Training loss: 1.800079584121704
Validation loss: 2.287905216217041

Epoch: 241| Step: 0
Training loss: 1.8325252532958984
Validation loss: 2.295429551473228

Epoch: 5| Step: 1
Training loss: 2.7134690284729004
Validation loss: 2.3027337161443566

Epoch: 5| Step: 2
Training loss: 1.9747031927108765
Validation loss: 2.3042826293617167

Epoch: 5| Step: 3
Training loss: 2.7316386699676514
Validation loss: 2.312180780595349

Epoch: 5| Step: 4
Training loss: 2.822134017944336
Validation loss: 2.3184681220721175

Epoch: 5| Step: 5
Training loss: 2.9595890045166016
Validation loss: 2.3116472485244914

Epoch: 5| Step: 6
Training loss: 1.9467443227767944
Validation loss: 2.324588878180391

Epoch: 5| Step: 7
Training loss: 2.794614553451538
Validation loss: 2.31404923623608

Epoch: 5| Step: 8
Training loss: 2.4143776893615723
Validation loss: 2.3094582608951035

Epoch: 5| Step: 9
Training loss: 2.6648004055023193
Validation loss: 2.3054754041856333

Epoch: 5| Step: 10
Training loss: 3.0161736011505127
Validation loss: 2.2997494359170236

Epoch: 242| Step: 0
Training loss: 2.551642656326294
Validation loss: 2.3036965042032223

Epoch: 5| Step: 1
Training loss: 2.536835193634033
Validation loss: 2.312331763646936

Epoch: 5| Step: 2
Training loss: 2.971426010131836
Validation loss: 2.309778200682773

Epoch: 5| Step: 3
Training loss: 2.251335620880127
Validation loss: 2.3073932073449575

Epoch: 5| Step: 4
Training loss: 2.7439351081848145
Validation loss: 2.2934950064587336

Epoch: 5| Step: 5
Training loss: 2.5239431858062744
Validation loss: 2.2982666710371613

Epoch: 5| Step: 6
Training loss: 2.84039044380188
Validation loss: 2.287395969513924

Epoch: 5| Step: 7
Training loss: 2.8943448066711426
Validation loss: 2.2901165023926766

Epoch: 5| Step: 8
Training loss: 2.510204315185547
Validation loss: 2.283380430231812

Epoch: 5| Step: 9
Training loss: 1.6815879344940186
Validation loss: 2.283813773944814

Epoch: 5| Step: 10
Training loss: 2.179126501083374
Validation loss: 2.2801487189467236

Epoch: 243| Step: 0
Training loss: 2.3117854595184326
Validation loss: 2.2964678451579106

Epoch: 5| Step: 1
Training loss: 2.7519724369049072
Validation loss: 2.322930956399569

Epoch: 5| Step: 2
Training loss: 3.0651535987854004
Validation loss: 2.3334241964483775

Epoch: 5| Step: 3
Training loss: 2.8403143882751465
Validation loss: 2.3329754157732894

Epoch: 5| Step: 4
Training loss: 2.3493242263793945
Validation loss: 2.3257443648512646

Epoch: 5| Step: 5
Training loss: 2.5380663871765137
Validation loss: 2.320707422430797

Epoch: 5| Step: 6
Training loss: 2.20170259475708
Validation loss: 2.308822839490829

Epoch: 5| Step: 7
Training loss: 2.4244205951690674
Validation loss: 2.3119788477497716

Epoch: 5| Step: 8
Training loss: 2.7738466262817383
Validation loss: 2.3022470833152853

Epoch: 5| Step: 9
Training loss: 2.538327217102051
Validation loss: 2.2996661201600106

Epoch: 5| Step: 10
Training loss: 1.9535082578659058
Validation loss: 2.2944973091925345

Epoch: 244| Step: 0
Training loss: 2.9060685634613037
Validation loss: 2.2870344218387397

Epoch: 5| Step: 1
Training loss: 3.057431221008301
Validation loss: 2.292220882190171

Epoch: 5| Step: 2
Training loss: 2.5793466567993164
Validation loss: 2.2819811233910183

Epoch: 5| Step: 3
Training loss: 2.9353833198547363
Validation loss: 2.2914427634208434

Epoch: 5| Step: 4
Training loss: 2.056286096572876
Validation loss: 2.2992435783468266

Epoch: 5| Step: 5
Training loss: 2.519705295562744
Validation loss: 2.2922980875097294

Epoch: 5| Step: 6
Training loss: 2.607032060623169
Validation loss: 2.3005906074277815

Epoch: 5| Step: 7
Training loss: 2.3884787559509277
Validation loss: 2.2874394950046333

Epoch: 5| Step: 8
Training loss: 2.3102877140045166
Validation loss: 2.294064442316691

Epoch: 5| Step: 9
Training loss: 2.4480977058410645
Validation loss: 2.2936127262730754

Epoch: 5| Step: 10
Training loss: 1.7851115465164185
Validation loss: 2.2967896897305726

Epoch: 245| Step: 0
Training loss: 2.0025136470794678
Validation loss: 2.2868099238282893

Epoch: 5| Step: 1
Training loss: 2.9546926021575928
Validation loss: 2.2963248132377543

Epoch: 5| Step: 2
Training loss: 2.9291067123413086
Validation loss: 2.303492610172559

Epoch: 5| Step: 3
Training loss: 2.3625893592834473
Validation loss: 2.2997969401779996

Epoch: 5| Step: 4
Training loss: 2.783379316329956
Validation loss: 2.3057064138432986

Epoch: 5| Step: 5
Training loss: 2.884880781173706
Validation loss: 2.297191012290216

Epoch: 5| Step: 6
Training loss: 2.2298364639282227
Validation loss: 2.2789193635345786

Epoch: 5| Step: 7
Training loss: 2.832108974456787
Validation loss: 2.2904411092881234

Epoch: 5| Step: 8
Training loss: 2.4433481693267822
Validation loss: 2.28598181919385

Epoch: 5| Step: 9
Training loss: 2.0749175548553467
Validation loss: 2.2954160474961802

Epoch: 5| Step: 10
Training loss: 2.153909683227539
Validation loss: 2.3132103489291285

Epoch: 246| Step: 0
Training loss: 2.485184907913208
Validation loss: 2.3311430177380963

Epoch: 5| Step: 1
Training loss: 2.9697632789611816
Validation loss: 2.3499646955920803

Epoch: 5| Step: 2
Training loss: 2.6714749336242676
Validation loss: 2.3625673991377636

Epoch: 5| Step: 3
Training loss: 2.4947779178619385
Validation loss: 2.3529940843582153

Epoch: 5| Step: 4
Training loss: 2.3495829105377197
Validation loss: 2.345911108037477

Epoch: 5| Step: 5
Training loss: 1.9003387689590454
Validation loss: 2.3254700860669537

Epoch: 5| Step: 6
Training loss: 2.5979807376861572
Validation loss: 2.306567991933515

Epoch: 5| Step: 7
Training loss: 2.2318363189697266
Validation loss: 2.290554900323191

Epoch: 5| Step: 8
Training loss: 2.5360913276672363
Validation loss: 2.2766145531849196

Epoch: 5| Step: 9
Training loss: 2.4289393424987793
Validation loss: 2.274146818345593

Epoch: 5| Step: 10
Training loss: 3.1928765773773193
Validation loss: 2.267944928138487

Epoch: 247| Step: 0
Training loss: 2.7065184116363525
Validation loss: 2.270672822511324

Epoch: 5| Step: 1
Training loss: 2.5492403507232666
Validation loss: 2.2831077703865628

Epoch: 5| Step: 2
Training loss: 2.4748175144195557
Validation loss: 2.2801586363905217

Epoch: 5| Step: 3
Training loss: 2.9894347190856934
Validation loss: 2.2826411724090576

Epoch: 5| Step: 4
Training loss: 2.402006149291992
Validation loss: 2.287545475908505

Epoch: 5| Step: 5
Training loss: 2.3756749629974365
Validation loss: 2.2853779562057985

Epoch: 5| Step: 6
Training loss: 1.7321422100067139
Validation loss: 2.3055777524107244

Epoch: 5| Step: 7
Training loss: 2.2482781410217285
Validation loss: 2.3057334807611283

Epoch: 5| Step: 8
Training loss: 3.066638708114624
Validation loss: 2.3161879611271683

Epoch: 5| Step: 9
Training loss: 2.8834922313690186
Validation loss: 2.3089420385258173

Epoch: 5| Step: 10
Training loss: 2.2363882064819336
Validation loss: 2.300537706703268

Epoch: 248| Step: 0
Training loss: 2.7081151008605957
Validation loss: 2.306140891967281

Epoch: 5| Step: 1
Training loss: 2.3438937664031982
Validation loss: 2.2993840427808863

Epoch: 5| Step: 2
Training loss: 2.896815299987793
Validation loss: 2.30755393351278

Epoch: 5| Step: 3
Training loss: 2.9858345985412598
Validation loss: 2.309735013592628

Epoch: 5| Step: 4
Training loss: 2.5865302085876465
Validation loss: 2.3065730807601765

Epoch: 5| Step: 5
Training loss: 2.4958040714263916
Validation loss: 2.300666501445155

Epoch: 5| Step: 6
Training loss: 2.3171093463897705
Validation loss: 2.2941381905668523

Epoch: 5| Step: 7
Training loss: 2.580578565597534
Validation loss: 2.2825808396903415

Epoch: 5| Step: 8
Training loss: 2.5773794651031494
Validation loss: 2.2877087208532516

Epoch: 5| Step: 9
Training loss: 2.0975444316864014
Validation loss: 2.283620775386851

Epoch: 5| Step: 10
Training loss: 1.9788312911987305
Validation loss: 2.2774997283053655

Epoch: 249| Step: 0
Training loss: 2.816316843032837
Validation loss: 2.291480323319794

Epoch: 5| Step: 1
Training loss: 2.0012807846069336
Validation loss: 2.307639209173059

Epoch: 5| Step: 2
Training loss: 2.583813428878784
Validation loss: 2.3223167234851467

Epoch: 5| Step: 3
Training loss: 1.9271091222763062
Validation loss: 2.337546569044872

Epoch: 5| Step: 4
Training loss: 2.8524134159088135
Validation loss: 2.3618985658050864

Epoch: 5| Step: 5
Training loss: 2.678598403930664
Validation loss: 2.3593819859207317

Epoch: 5| Step: 6
Training loss: 2.473677396774292
Validation loss: 2.3514128218414965

Epoch: 5| Step: 7
Training loss: 2.8779752254486084
Validation loss: 2.3460560614062893

Epoch: 5| Step: 8
Training loss: 2.4741532802581787
Validation loss: 2.3265214889280257

Epoch: 5| Step: 9
Training loss: 2.2953755855560303
Validation loss: 2.3127330041700795

Epoch: 5| Step: 10
Training loss: 2.834460973739624
Validation loss: 2.281140496653895

Epoch: 250| Step: 0
Training loss: 2.667893648147583
Validation loss: 2.2754723410452566

Epoch: 5| Step: 1
Training loss: 2.980619430541992
Validation loss: 2.2748872874885477

Epoch: 5| Step: 2
Training loss: 2.2795629501342773
Validation loss: 2.274915779790571

Epoch: 5| Step: 3
Training loss: 2.841409683227539
Validation loss: 2.265387317185761

Epoch: 5| Step: 4
Training loss: 2.314993381500244
Validation loss: 2.264389576450471

Epoch: 5| Step: 5
Training loss: 2.36669921875
Validation loss: 2.2662343363608084

Epoch: 5| Step: 6
Training loss: 2.8040902614593506
Validation loss: 2.269436761897097

Epoch: 5| Step: 7
Training loss: 1.758744478225708
Validation loss: 2.2668974143202587

Epoch: 5| Step: 8
Training loss: 2.1834278106689453
Validation loss: 2.2644863795208674

Epoch: 5| Step: 9
Training loss: 2.5020413398742676
Validation loss: 2.2691316348250195

Epoch: 5| Step: 10
Training loss: 3.1076066493988037
Validation loss: 2.269456285302357

Epoch: 251| Step: 0
Training loss: 2.7761523723602295
Validation loss: 2.277024043503628

Epoch: 5| Step: 1
Training loss: 2.616976499557495
Validation loss: 2.2816706370281916

Epoch: 5| Step: 2
Training loss: 2.083225727081299
Validation loss: 2.2932747512735348

Epoch: 5| Step: 3
Training loss: 2.7164154052734375
Validation loss: 2.302262334413426

Epoch: 5| Step: 4
Training loss: 2.560972213745117
Validation loss: 2.305189001944757

Epoch: 5| Step: 5
Training loss: 1.7542057037353516
Validation loss: 2.3160996360163533

Epoch: 5| Step: 6
Training loss: 2.4659430980682373
Validation loss: 2.3314102388197377

Epoch: 5| Step: 7
Training loss: 2.9023168087005615
Validation loss: 2.315873981803976

Epoch: 5| Step: 8
Training loss: 2.5436346530914307
Validation loss: 2.3127576766475553

Epoch: 5| Step: 9
Training loss: 2.450784683227539
Validation loss: 2.3117242910528697

Epoch: 5| Step: 10
Training loss: 2.833012819290161
Validation loss: 2.299787808490056

Epoch: 252| Step: 0
Training loss: 2.41813063621521
Validation loss: 2.3101702146632697

Epoch: 5| Step: 1
Training loss: 2.3849167823791504
Validation loss: 2.295217985747963

Epoch: 5| Step: 2
Training loss: 2.106276512145996
Validation loss: 2.286157836196243

Epoch: 5| Step: 3
Training loss: 2.8452465534210205
Validation loss: 2.2816762514011835

Epoch: 5| Step: 4
Training loss: 3.1854846477508545
Validation loss: 2.279943025240334

Epoch: 5| Step: 5
Training loss: 2.904651403427124
Validation loss: 2.2878396485441472

Epoch: 5| Step: 6
Training loss: 2.3137917518615723
Validation loss: 2.285886565844218

Epoch: 5| Step: 7
Training loss: 2.457313060760498
Validation loss: 2.279462065747989

Epoch: 5| Step: 8
Training loss: 1.969323754310608
Validation loss: 2.2808041623843613

Epoch: 5| Step: 9
Training loss: 2.5166802406311035
Validation loss: 2.278853944552842

Epoch: 5| Step: 10
Training loss: 2.441296339035034
Validation loss: 2.2707141112255793

Epoch: 253| Step: 0
Training loss: 2.1189613342285156
Validation loss: 2.273796284070579

Epoch: 5| Step: 1
Training loss: 3.040386438369751
Validation loss: 2.278820478787986

Epoch: 5| Step: 2
Training loss: 2.7651517391204834
Validation loss: 2.276698015069449

Epoch: 5| Step: 3
Training loss: 2.5440220832824707
Validation loss: 2.2752919889265493

Epoch: 5| Step: 4
Training loss: 2.7795138359069824
Validation loss: 2.2697162743537658

Epoch: 5| Step: 5
Training loss: 2.8496322631835938
Validation loss: 2.2731589027630386

Epoch: 5| Step: 6
Training loss: 2.3605761528015137
Validation loss: 2.28555606513895

Epoch: 5| Step: 7
Training loss: 3.519152879714966
Validation loss: 2.299960364577591

Epoch: 5| Step: 8
Training loss: 2.0084948539733887
Validation loss: 2.3003100989967264

Epoch: 5| Step: 9
Training loss: 1.6431792974472046
Validation loss: 2.2890575265371673

Epoch: 5| Step: 10
Training loss: 1.7903900146484375
Validation loss: 2.304986963989914

Epoch: 254| Step: 0
Training loss: 2.622593402862549
Validation loss: 2.3110440341375207

Epoch: 5| Step: 1
Training loss: 2.963552951812744
Validation loss: 2.3025090514972644

Epoch: 5| Step: 2
Training loss: 2.003206491470337
Validation loss: 2.2991318048969394

Epoch: 5| Step: 3
Training loss: 1.9581983089447021
Validation loss: 2.298790095954813

Epoch: 5| Step: 4
Training loss: 2.734825372695923
Validation loss: 2.2934077734588296

Epoch: 5| Step: 5
Training loss: 2.618743896484375
Validation loss: 2.3043582003603698

Epoch: 5| Step: 6
Training loss: 2.663980007171631
Validation loss: 2.3027364156579457

Epoch: 5| Step: 7
Training loss: 2.1809208393096924
Validation loss: 2.3205831140600224

Epoch: 5| Step: 8
Training loss: 2.8128786087036133
Validation loss: 2.295489858555537

Epoch: 5| Step: 9
Training loss: 2.2264108657836914
Validation loss: 2.3087845463906564

Epoch: 5| Step: 10
Training loss: 2.7658143043518066
Validation loss: 2.297521291240569

Epoch: 255| Step: 0
Training loss: 2.5991508960723877
Validation loss: 2.3005483150482178

Epoch: 5| Step: 1
Training loss: 2.439608573913574
Validation loss: 2.306423177001297

Epoch: 5| Step: 2
Training loss: 2.7497076988220215
Validation loss: 2.291812612164405

Epoch: 5| Step: 3
Training loss: 2.3956246376037598
Validation loss: 2.2865925758115706

Epoch: 5| Step: 4
Training loss: 2.574352264404297
Validation loss: 2.2901558594037126

Epoch: 5| Step: 5
Training loss: 2.630255937576294
Validation loss: 2.272021834568311

Epoch: 5| Step: 6
Training loss: 2.436555862426758
Validation loss: 2.2816016904769407

Epoch: 5| Step: 7
Training loss: 2.798374652862549
Validation loss: 2.281939075839135

Epoch: 5| Step: 8
Training loss: 1.6925370693206787
Validation loss: 2.2746556638389506

Epoch: 5| Step: 9
Training loss: 2.5445876121520996
Validation loss: 2.258305226602862

Epoch: 5| Step: 10
Training loss: 2.682126998901367
Validation loss: 2.2618489265441895

Epoch: 256| Step: 0
Training loss: 1.9141308069229126
Validation loss: 2.2531782760415027

Epoch: 5| Step: 1
Training loss: 2.370443344116211
Validation loss: 2.252740947149133

Epoch: 5| Step: 2
Training loss: 2.1590123176574707
Validation loss: 2.2595572445982244

Epoch: 5| Step: 3
Training loss: 2.9728779792785645
Validation loss: 2.2542914113690777

Epoch: 5| Step: 4
Training loss: 2.8488926887512207
Validation loss: 2.25833002854419

Epoch: 5| Step: 5
Training loss: 3.0188605785369873
Validation loss: 2.267029975050239

Epoch: 5| Step: 6
Training loss: 1.9049447774887085
Validation loss: 2.268717364598346

Epoch: 5| Step: 7
Training loss: 2.9291884899139404
Validation loss: 2.2776114402278775

Epoch: 5| Step: 8
Training loss: 2.601034164428711
Validation loss: 2.2749997390213834

Epoch: 5| Step: 9
Training loss: 2.1706924438476562
Validation loss: 2.2772595933688584

Epoch: 5| Step: 10
Training loss: 2.5536108016967773
Validation loss: 2.283824413053451

Epoch: 257| Step: 0
Training loss: 2.417160749435425
Validation loss: 2.286210362629224

Epoch: 5| Step: 1
Training loss: 2.9689064025878906
Validation loss: 2.29720470213121

Epoch: 5| Step: 2
Training loss: 2.8419716358184814
Validation loss: 2.320096533785584

Epoch: 5| Step: 3
Training loss: 2.313373327255249
Validation loss: 2.3050424745005946

Epoch: 5| Step: 4
Training loss: 2.3411059379577637
Validation loss: 2.3002901846362698

Epoch: 5| Step: 5
Training loss: 3.1841530799865723
Validation loss: 2.293415538726314

Epoch: 5| Step: 6
Training loss: 2.0219852924346924
Validation loss: 2.276778728731217

Epoch: 5| Step: 7
Training loss: 2.1256110668182373
Validation loss: 2.288013394160937

Epoch: 5| Step: 8
Training loss: 2.6586523056030273
Validation loss: 2.2889064537581576

Epoch: 5| Step: 9
Training loss: 2.364091396331787
Validation loss: 2.2876364928419872

Epoch: 5| Step: 10
Training loss: 2.2378063201904297
Validation loss: 2.274014083288049

Epoch: 258| Step: 0
Training loss: 2.9756579399108887
Validation loss: 2.2780906487536687

Epoch: 5| Step: 1
Training loss: 1.8990825414657593
Validation loss: 2.2673199228061143

Epoch: 5| Step: 2
Training loss: 2.209831476211548
Validation loss: 2.2762198217453493

Epoch: 5| Step: 3
Training loss: 2.1166326999664307
Validation loss: 2.2791194274861324

Epoch: 5| Step: 4
Training loss: 2.338625192642212
Validation loss: 2.2851974682141374

Epoch: 5| Step: 5
Training loss: 3.1048645973205566
Validation loss: 2.2837017197762766

Epoch: 5| Step: 6
Training loss: 2.5733602046966553
Validation loss: 2.2884248841193413

Epoch: 5| Step: 7
Training loss: 2.523702621459961
Validation loss: 2.296886864528861

Epoch: 5| Step: 8
Training loss: 2.53071928024292
Validation loss: 2.3089207756903862

Epoch: 5| Step: 9
Training loss: 2.616406202316284
Validation loss: 2.298756371262253

Epoch: 5| Step: 10
Training loss: 2.536093235015869
Validation loss: 2.286411467418876

Epoch: 259| Step: 0
Training loss: 2.947681188583374
Validation loss: 2.2716784156778806

Epoch: 5| Step: 1
Training loss: 2.7977781295776367
Validation loss: 2.2820537731211674

Epoch: 5| Step: 2
Training loss: 2.254361391067505
Validation loss: 2.2700358026771137

Epoch: 5| Step: 3
Training loss: 2.3768038749694824
Validation loss: 2.26021094347841

Epoch: 5| Step: 4
Training loss: 2.1380977630615234
Validation loss: 2.2760726200636996

Epoch: 5| Step: 5
Training loss: 2.1845669746398926
Validation loss: 2.2728448273033224

Epoch: 5| Step: 6
Training loss: 2.0904784202575684
Validation loss: 2.2763352842741114

Epoch: 5| Step: 7
Training loss: 2.586202621459961
Validation loss: 2.270353877416221

Epoch: 5| Step: 8
Training loss: 2.6787638664245605
Validation loss: 2.284849336070399

Epoch: 5| Step: 9
Training loss: 2.9239132404327393
Validation loss: 2.3009549289621334

Epoch: 5| Step: 10
Training loss: 2.426356077194214
Validation loss: 2.3017133025712866

Epoch: 260| Step: 0
Training loss: 2.5908687114715576
Validation loss: 2.3120371680105887

Epoch: 5| Step: 1
Training loss: 2.0727880001068115
Validation loss: 2.293708816651375

Epoch: 5| Step: 2
Training loss: 2.861478567123413
Validation loss: 2.2919319445087063

Epoch: 5| Step: 3
Training loss: 2.3171188831329346
Validation loss: 2.2888556808553715

Epoch: 5| Step: 4
Training loss: 2.4795467853546143
Validation loss: 2.2768542125660884

Epoch: 5| Step: 5
Training loss: 2.1850898265838623
Validation loss: 2.2743313030530046

Epoch: 5| Step: 6
Training loss: 2.175352096557617
Validation loss: 2.2697366488877164

Epoch: 5| Step: 7
Training loss: 2.6696937084198
Validation loss: 2.2712267739798433

Epoch: 5| Step: 8
Training loss: 2.7028608322143555
Validation loss: 2.2720238957353818

Epoch: 5| Step: 9
Training loss: 2.5450825691223145
Validation loss: 2.2791923938259

Epoch: 5| Step: 10
Training loss: 2.8491909503936768
Validation loss: 2.2841677563164824

Epoch: 261| Step: 0
Training loss: 2.209233522415161
Validation loss: 2.3023024951258013

Epoch: 5| Step: 1
Training loss: 2.279761791229248
Validation loss: 2.3010318638176046

Epoch: 5| Step: 2
Training loss: 2.7411532402038574
Validation loss: 2.317003321904008

Epoch: 5| Step: 3
Training loss: 2.516524076461792
Validation loss: 2.331126086173519

Epoch: 5| Step: 4
Training loss: 2.656960964202881
Validation loss: 2.312854322054053

Epoch: 5| Step: 5
Training loss: 2.4455742835998535
Validation loss: 2.3060773726432555

Epoch: 5| Step: 6
Training loss: 2.1119494438171387
Validation loss: 2.289931034529081

Epoch: 5| Step: 7
Training loss: 2.904233455657959
Validation loss: 2.2975290975263043

Epoch: 5| Step: 8
Training loss: 2.5733418464660645
Validation loss: 2.2969357659739833

Epoch: 5| Step: 9
Training loss: 2.208369493484497
Validation loss: 2.280577128933322

Epoch: 5| Step: 10
Training loss: 2.7702362537384033
Validation loss: 2.274204236204906

Epoch: 262| Step: 0
Training loss: 1.5027215480804443
Validation loss: 2.2708568008997108

Epoch: 5| Step: 1
Training loss: 1.7476708889007568
Validation loss: 2.264680072825442

Epoch: 5| Step: 2
Training loss: 2.4850735664367676
Validation loss: 2.2607800781085925

Epoch: 5| Step: 3
Training loss: 3.3319296836853027
Validation loss: 2.2677688854996876

Epoch: 5| Step: 4
Training loss: 2.5203099250793457
Validation loss: 2.2578318452322357

Epoch: 5| Step: 5
Training loss: 2.187384843826294
Validation loss: 2.2654716840354343

Epoch: 5| Step: 6
Training loss: 3.280656099319458
Validation loss: 2.2625447678309616

Epoch: 5| Step: 7
Training loss: 2.484160900115967
Validation loss: 2.2783728491875435

Epoch: 5| Step: 8
Training loss: 2.720155715942383
Validation loss: 2.2680286143415715

Epoch: 5| Step: 9
Training loss: 2.205686092376709
Validation loss: 2.2780485640289965

Epoch: 5| Step: 10
Training loss: 2.969467878341675
Validation loss: 2.292209909808251

Epoch: 263| Step: 0
Training loss: 2.1375508308410645
Validation loss: 2.287690867659866

Epoch: 5| Step: 1
Training loss: 1.929405927658081
Validation loss: 2.3074231711767053

Epoch: 5| Step: 2
Training loss: 2.5875892639160156
Validation loss: 2.315143603150563

Epoch: 5| Step: 3
Training loss: 2.376444101333618
Validation loss: 2.316704173241892

Epoch: 5| Step: 4
Training loss: 3.229228973388672
Validation loss: 2.3196758480482202

Epoch: 5| Step: 5
Training loss: 2.378150463104248
Validation loss: 2.3116261830893894

Epoch: 5| Step: 6
Training loss: 2.723417043685913
Validation loss: 2.312718150436237

Epoch: 5| Step: 7
Training loss: 2.3195087909698486
Validation loss: 2.303401254838513

Epoch: 5| Step: 8
Training loss: 2.5346906185150146
Validation loss: 2.311408278762653

Epoch: 5| Step: 9
Training loss: 2.4715383052825928
Validation loss: 2.2853825579407396

Epoch: 5| Step: 10
Training loss: 2.775691509246826
Validation loss: 2.2783999904509513

Epoch: 264| Step: 0
Training loss: 2.9908089637756348
Validation loss: 2.270665595608373

Epoch: 5| Step: 1
Training loss: 2.3563218116760254
Validation loss: 2.263554980677943

Epoch: 5| Step: 2
Training loss: 2.477104663848877
Validation loss: 2.2702998525352887

Epoch: 5| Step: 3
Training loss: 2.626823663711548
Validation loss: 2.2689445710951284

Epoch: 5| Step: 4
Training loss: 3.0200793743133545
Validation loss: 2.2681208733589417

Epoch: 5| Step: 5
Training loss: 2.2080061435699463
Validation loss: 2.26908201812416

Epoch: 5| Step: 6
Training loss: 2.988206148147583
Validation loss: 2.253567272616971

Epoch: 5| Step: 7
Training loss: 2.119340419769287
Validation loss: 2.249458679588892

Epoch: 5| Step: 8
Training loss: 2.0822975635528564
Validation loss: 2.248270816700433

Epoch: 5| Step: 9
Training loss: 2.1047935485839844
Validation loss: 2.248949699504401

Epoch: 5| Step: 10
Training loss: 2.4341986179351807
Validation loss: 2.2584679383103565

Epoch: 265| Step: 0
Training loss: 2.324960231781006
Validation loss: 2.2712414239042547

Epoch: 5| Step: 1
Training loss: 2.638136863708496
Validation loss: 2.2719039955446796

Epoch: 5| Step: 2
Training loss: 2.184000253677368
Validation loss: 2.2855747733064877

Epoch: 5| Step: 3
Training loss: 2.843914031982422
Validation loss: 2.3048738305286696

Epoch: 5| Step: 4
Training loss: 2.8499884605407715
Validation loss: 2.3016003947104178

Epoch: 5| Step: 5
Training loss: 2.752044200897217
Validation loss: 2.310550617915328

Epoch: 5| Step: 6
Training loss: 2.476186990737915
Validation loss: 2.309787445170905

Epoch: 5| Step: 7
Training loss: 1.8997875452041626
Validation loss: 2.311572979855281

Epoch: 5| Step: 8
Training loss: 2.2486236095428467
Validation loss: 2.2981501804885043

Epoch: 5| Step: 9
Training loss: 2.721146583557129
Validation loss: 2.282315372138895

Epoch: 5| Step: 10
Training loss: 2.4450535774230957
Validation loss: 2.2798750041633524

Epoch: 266| Step: 0
Training loss: 2.298405408859253
Validation loss: 2.272239822213368

Epoch: 5| Step: 1
Training loss: 2.7034430503845215
Validation loss: 2.280435287824241

Epoch: 5| Step: 2
Training loss: 1.9736757278442383
Validation loss: 2.2714248062461935

Epoch: 5| Step: 3
Training loss: 2.39164137840271
Validation loss: 2.277942595943328

Epoch: 5| Step: 4
Training loss: 2.5443177223205566
Validation loss: 2.276901207944398

Epoch: 5| Step: 5
Training loss: 2.1240105628967285
Validation loss: 2.271503608713868

Epoch: 5| Step: 6
Training loss: 3.2544288635253906
Validation loss: 2.272039346797492

Epoch: 5| Step: 7
Training loss: 2.408594846725464
Validation loss: 2.2861218298635175

Epoch: 5| Step: 8
Training loss: 2.333085536956787
Validation loss: 2.2812836823924894

Epoch: 5| Step: 9
Training loss: 2.532827854156494
Validation loss: 2.272875770445793

Epoch: 5| Step: 10
Training loss: 2.780578136444092
Validation loss: 2.2740854114614506

Epoch: 267| Step: 0
Training loss: 2.5587449073791504
Validation loss: 2.2828850066790016

Epoch: 5| Step: 1
Training loss: 2.1256110668182373
Validation loss: 2.29420228158274

Epoch: 5| Step: 2
Training loss: 2.7032394409179688
Validation loss: 2.299849993439131

Epoch: 5| Step: 3
Training loss: 1.5706931352615356
Validation loss: 2.3138587372277373

Epoch: 5| Step: 4
Training loss: 2.413088083267212
Validation loss: 2.3217987424583844

Epoch: 5| Step: 5
Training loss: 2.770068645477295
Validation loss: 2.3342294616083943

Epoch: 5| Step: 6
Training loss: 2.8908708095550537
Validation loss: 2.3077398807771745

Epoch: 5| Step: 7
Training loss: 2.9116744995117188
Validation loss: 2.303143924282443

Epoch: 5| Step: 8
Training loss: 2.6433122158050537
Validation loss: 2.303554783585251

Epoch: 5| Step: 9
Training loss: 2.5861217975616455
Validation loss: 2.2986053625742593

Epoch: 5| Step: 10
Training loss: 2.1157424449920654
Validation loss: 2.2801313131086287

Epoch: 268| Step: 0
Training loss: 2.0495686531066895
Validation loss: 2.2663387713893766

Epoch: 5| Step: 1
Training loss: 2.6842525005340576
Validation loss: 2.2702617991355156

Epoch: 5| Step: 2
Training loss: 2.426340341567993
Validation loss: 2.2599119396619898

Epoch: 5| Step: 3
Training loss: 1.9883003234863281
Validation loss: 2.2498320969202186

Epoch: 5| Step: 4
Training loss: 2.1736807823181152
Validation loss: 2.2510997428688952

Epoch: 5| Step: 5
Training loss: 2.9022765159606934
Validation loss: 2.2370148063987814

Epoch: 5| Step: 6
Training loss: 2.448697805404663
Validation loss: 2.2359901461549985

Epoch: 5| Step: 7
Training loss: 2.165795087814331
Validation loss: 2.2429031813016502

Epoch: 5| Step: 8
Training loss: 2.383957862854004
Validation loss: 2.2530146696234263

Epoch: 5| Step: 9
Training loss: 2.5451583862304688
Validation loss: 2.269881994493546

Epoch: 5| Step: 10
Training loss: 3.7967731952667236
Validation loss: 2.2922010408934725

Epoch: 269| Step: 0
Training loss: 2.396674394607544
Validation loss: 2.3232420003542336

Epoch: 5| Step: 1
Training loss: 2.5142569541931152
Validation loss: 2.3601024073939167

Epoch: 5| Step: 2
Training loss: 2.0365214347839355
Validation loss: 2.3588686604653635

Epoch: 5| Step: 3
Training loss: 2.7659873962402344
Validation loss: 2.3542397509339037

Epoch: 5| Step: 4
Training loss: 3.0358386039733887
Validation loss: 2.3376259701226347

Epoch: 5| Step: 5
Training loss: 2.437204360961914
Validation loss: 2.3183704755639516

Epoch: 5| Step: 6
Training loss: 1.964237928390503
Validation loss: 2.2995872702649844

Epoch: 5| Step: 7
Training loss: 2.4373950958251953
Validation loss: 2.2686944943602367

Epoch: 5| Step: 8
Training loss: 2.7789065837860107
Validation loss: 2.2588713861280874

Epoch: 5| Step: 9
Training loss: 2.545978546142578
Validation loss: 2.2602039870395454

Epoch: 5| Step: 10
Training loss: 2.482717275619507
Validation loss: 2.255743570225213

Epoch: 270| Step: 0
Training loss: 2.5794429779052734
Validation loss: 2.2576332066648748

Epoch: 5| Step: 1
Training loss: 2.766944646835327
Validation loss: 2.2661513769498436

Epoch: 5| Step: 2
Training loss: 2.9964067935943604
Validation loss: 2.263255738442944

Epoch: 5| Step: 3
Training loss: 2.608560085296631
Validation loss: 2.263124309560304

Epoch: 5| Step: 4
Training loss: 1.9663026332855225
Validation loss: 2.2641320690031974

Epoch: 5| Step: 5
Training loss: 2.3104920387268066
Validation loss: 2.270466036694024

Epoch: 5| Step: 6
Training loss: 2.2716574668884277
Validation loss: 2.277593566525367

Epoch: 5| Step: 7
Training loss: 2.7285702228546143
Validation loss: 2.2941116543226343

Epoch: 5| Step: 8
Training loss: 2.525933265686035
Validation loss: 2.2844697493378834

Epoch: 5| Step: 9
Training loss: 2.358978271484375
Validation loss: 2.2964906128503944

Epoch: 5| Step: 10
Training loss: 2.2970385551452637
Validation loss: 2.2904710718380508

Epoch: 271| Step: 0
Training loss: 2.976058006286621
Validation loss: 2.2868454738329818

Epoch: 5| Step: 1
Training loss: 2.595334053039551
Validation loss: 2.280694333455896

Epoch: 5| Step: 2
Training loss: 2.1537322998046875
Validation loss: 2.2803531923601703

Epoch: 5| Step: 3
Training loss: 2.6427695751190186
Validation loss: 2.286787630409323

Epoch: 5| Step: 4
Training loss: 2.4242920875549316
Validation loss: 2.297145579450874

Epoch: 5| Step: 5
Training loss: 2.5519490242004395
Validation loss: 2.2858535987074657

Epoch: 5| Step: 6
Training loss: 2.805346965789795
Validation loss: 2.2818354393846247

Epoch: 5| Step: 7
Training loss: 2.7796719074249268
Validation loss: 2.2766663541076

Epoch: 5| Step: 8
Training loss: 1.9791123867034912
Validation loss: 2.2625441051298574

Epoch: 5| Step: 9
Training loss: 2.097949981689453
Validation loss: 2.2793589202306603

Epoch: 5| Step: 10
Training loss: 2.2339065074920654
Validation loss: 2.28625423677506

Epoch: 272| Step: 0
Training loss: 2.6996185779571533
Validation loss: 2.2865767901943577

Epoch: 5| Step: 1
Training loss: 2.030954599380493
Validation loss: 2.2942344629636375

Epoch: 5| Step: 2
Training loss: 3.457965850830078
Validation loss: 2.2887059719331804

Epoch: 5| Step: 3
Training loss: 2.3079605102539062
Validation loss: 2.3025473266519527

Epoch: 5| Step: 4
Training loss: 2.318880081176758
Validation loss: 2.2877985174937914

Epoch: 5| Step: 5
Training loss: 1.965200662612915
Validation loss: 2.2806081976941837

Epoch: 5| Step: 6
Training loss: 2.329057216644287
Validation loss: 2.2869885583077707

Epoch: 5| Step: 7
Training loss: 2.214853048324585
Validation loss: 2.2918879934536514

Epoch: 5| Step: 8
Training loss: 2.760624408721924
Validation loss: 2.279557207579254

Epoch: 5| Step: 9
Training loss: 2.6278328895568848
Validation loss: 2.275793280652774

Epoch: 5| Step: 10
Training loss: 2.499204158782959
Validation loss: 2.2837560740850305

Epoch: 273| Step: 0
Training loss: 2.822864532470703
Validation loss: 2.2705186208089194

Epoch: 5| Step: 1
Training loss: 2.507534980773926
Validation loss: 2.2720557592248403

Epoch: 5| Step: 2
Training loss: 2.5636799335479736
Validation loss: 2.2735037803649902

Epoch: 5| Step: 3
Training loss: 2.388495922088623
Validation loss: 2.280747405944332

Epoch: 5| Step: 4
Training loss: 2.8104634284973145
Validation loss: 2.263901391336995

Epoch: 5| Step: 5
Training loss: 2.609426736831665
Validation loss: 2.2520695296666955

Epoch: 5| Step: 6
Training loss: 2.866318702697754
Validation loss: 2.2461278464204524

Epoch: 5| Step: 7
Training loss: 2.2322065830230713
Validation loss: 2.255408684412638

Epoch: 5| Step: 8
Training loss: 1.7362744808197021
Validation loss: 2.261178501190678

Epoch: 5| Step: 9
Training loss: 2.303248167037964
Validation loss: 2.254713278944774

Epoch: 5| Step: 10
Training loss: 2.255727767944336
Validation loss: 2.262568671216247

Epoch: 274| Step: 0
Training loss: 2.6211631298065186
Validation loss: 2.261022621585477

Epoch: 5| Step: 1
Training loss: 2.260833740234375
Validation loss: 2.2683737431803057

Epoch: 5| Step: 2
Training loss: 2.4399943351745605
Validation loss: 2.272385513910683

Epoch: 5| Step: 3
Training loss: 2.3238320350646973
Validation loss: 2.2937921580447944

Epoch: 5| Step: 4
Training loss: 2.9480998516082764
Validation loss: 2.3188205739503265

Epoch: 5| Step: 5
Training loss: 2.459381580352783
Validation loss: 2.334798588547655

Epoch: 5| Step: 6
Training loss: 2.277946949005127
Validation loss: 2.3748572718712593

Epoch: 5| Step: 7
Training loss: 2.154782772064209
Validation loss: 2.4116941780172367

Epoch: 5| Step: 8
Training loss: 2.589919328689575
Validation loss: 2.4112450948325534

Epoch: 5| Step: 9
Training loss: 2.8584423065185547
Validation loss: 2.3703513735084125

Epoch: 5| Step: 10
Training loss: 2.36271333694458
Validation loss: 2.2919900391691472

Epoch: 275| Step: 0
Training loss: 2.374145984649658
Validation loss: 2.2646083575423046

Epoch: 5| Step: 1
Training loss: 2.0438549518585205
Validation loss: 2.2414432494871077

Epoch: 5| Step: 2
Training loss: 2.2803101539611816
Validation loss: 2.2362601808322373

Epoch: 5| Step: 3
Training loss: 2.963056802749634
Validation loss: 2.239603437403197

Epoch: 5| Step: 4
Training loss: 2.428011417388916
Validation loss: 2.2510019297240884

Epoch: 5| Step: 5
Training loss: 2.9557464122772217
Validation loss: 2.2553968557747464

Epoch: 5| Step: 6
Training loss: 3.0661001205444336
Validation loss: 2.2620930825510333

Epoch: 5| Step: 7
Training loss: 2.3907759189605713
Validation loss: 2.2758985898827993

Epoch: 5| Step: 8
Training loss: 2.873994827270508
Validation loss: 2.2701648384012203

Epoch: 5| Step: 9
Training loss: 1.9162667989730835
Validation loss: 2.2559307262461674

Epoch: 5| Step: 10
Training loss: 2.3159422874450684
Validation loss: 2.2526389450155277

Epoch: 276| Step: 0
Training loss: 2.534968137741089
Validation loss: 2.2597141778597267

Epoch: 5| Step: 1
Training loss: 2.2874577045440674
Validation loss: 2.245379496646184

Epoch: 5| Step: 2
Training loss: 3.020581007003784
Validation loss: 2.2335573191283853

Epoch: 5| Step: 3
Training loss: 2.1590826511383057
Validation loss: 2.2248333833550893

Epoch: 5| Step: 4
Training loss: 2.517199993133545
Validation loss: 2.2215828075203845

Epoch: 5| Step: 5
Training loss: 2.5514888763427734
Validation loss: 2.2191550808568157

Epoch: 5| Step: 6
Training loss: 2.452608108520508
Validation loss: 2.222885734291487

Epoch: 5| Step: 7
Training loss: 2.411349058151245
Validation loss: 2.221784109710365

Epoch: 5| Step: 8
Training loss: 3.349151611328125
Validation loss: 2.2287961834220478

Epoch: 5| Step: 9
Training loss: 2.387460470199585
Validation loss: 2.2298018599069245

Epoch: 5| Step: 10
Training loss: 1.6370482444763184
Validation loss: 2.2411385325975317

Epoch: 277| Step: 0
Training loss: 1.7983957529067993
Validation loss: 2.2477413146726546

Epoch: 5| Step: 1
Training loss: 2.7780935764312744
Validation loss: 2.240257865639143

Epoch: 5| Step: 2
Training loss: 2.6785569190979004
Validation loss: 2.2490142494119625

Epoch: 5| Step: 3
Training loss: 2.2998030185699463
Validation loss: 2.243980469242219

Epoch: 5| Step: 4
Training loss: 3.0969154834747314
Validation loss: 2.242267467642343

Epoch: 5| Step: 5
Training loss: 2.018173933029175
Validation loss: 2.2427006357459613

Epoch: 5| Step: 6
Training loss: 2.6746647357940674
Validation loss: 2.2567369566168836

Epoch: 5| Step: 7
Training loss: 2.824615716934204
Validation loss: 2.2568551135319534

Epoch: 5| Step: 8
Training loss: 2.8383498191833496
Validation loss: 2.254236240540781

Epoch: 5| Step: 9
Training loss: 2.097693681716919
Validation loss: 2.263765219719179

Epoch: 5| Step: 10
Training loss: 2.0387766361236572
Validation loss: 2.2622027884247484

Epoch: 278| Step: 0
Training loss: 2.146369457244873
Validation loss: 2.2778078535551667

Epoch: 5| Step: 1
Training loss: 2.2585060596466064
Validation loss: 2.2782066458015033

Epoch: 5| Step: 2
Training loss: 2.678090810775757
Validation loss: 2.292671493304673

Epoch: 5| Step: 3
Training loss: 2.5614495277404785
Validation loss: 2.3128072754029305

Epoch: 5| Step: 4
Training loss: 1.5894200801849365
Validation loss: 2.318363453752251

Epoch: 5| Step: 5
Training loss: 2.7524945735931396
Validation loss: 2.325117632906924

Epoch: 5| Step: 6
Training loss: 3.2038967609405518
Validation loss: 2.3171416918436685

Epoch: 5| Step: 7
Training loss: 2.5700297355651855
Validation loss: 2.305526206570287

Epoch: 5| Step: 8
Training loss: 2.4444520473480225
Validation loss: 2.2798372007185415

Epoch: 5| Step: 9
Training loss: 2.972102165222168
Validation loss: 2.264698472074283

Epoch: 5| Step: 10
Training loss: 1.9509259462356567
Validation loss: 2.258409333485429

Epoch: 279| Step: 0
Training loss: 2.648378372192383
Validation loss: 2.2577873737581315

Epoch: 5| Step: 1
Training loss: 2.654371500015259
Validation loss: 2.2463380752071256

Epoch: 5| Step: 2
Training loss: 1.9802649021148682
Validation loss: 2.2527677141210085

Epoch: 5| Step: 3
Training loss: 1.8499433994293213
Validation loss: 2.2486831629148094

Epoch: 5| Step: 4
Training loss: 3.0129125118255615
Validation loss: 2.251267115275065

Epoch: 5| Step: 5
Training loss: 2.4780139923095703
Validation loss: 2.2573875304191344

Epoch: 5| Step: 6
Training loss: 2.3262438774108887
Validation loss: 2.2590934025344027

Epoch: 5| Step: 7
Training loss: 2.0406124591827393
Validation loss: 2.2554288679553616

Epoch: 5| Step: 8
Training loss: 2.6690430641174316
Validation loss: 2.237601641685732

Epoch: 5| Step: 9
Training loss: 2.614861011505127
Validation loss: 2.247196529501228

Epoch: 5| Step: 10
Training loss: 2.8813817501068115
Validation loss: 2.253549539914695

Epoch: 280| Step: 0
Training loss: 2.811180830001831
Validation loss: 2.259526785983834

Epoch: 5| Step: 1
Training loss: 2.7965800762176514
Validation loss: 2.2537593841552734

Epoch: 5| Step: 2
Training loss: 2.228731632232666
Validation loss: 2.254260286208122

Epoch: 5| Step: 3
Training loss: 2.4225833415985107
Validation loss: 2.251371793849494

Epoch: 5| Step: 4
Training loss: 2.1567888259887695
Validation loss: 2.243218337335894

Epoch: 5| Step: 5
Training loss: 2.944044589996338
Validation loss: 2.247827378652429

Epoch: 5| Step: 6
Training loss: 2.4149253368377686
Validation loss: 2.242704365843086

Epoch: 5| Step: 7
Training loss: 2.2686870098114014
Validation loss: 2.2400498877289476

Epoch: 5| Step: 8
Training loss: 2.862027645111084
Validation loss: 2.244518963239526

Epoch: 5| Step: 9
Training loss: 1.4407840967178345
Validation loss: 2.2384461972021286

Epoch: 5| Step: 10
Training loss: 2.8142361640930176
Validation loss: 2.2505253668754333

Epoch: 281| Step: 0
Training loss: 2.1871652603149414
Validation loss: 2.2624928617990143

Epoch: 5| Step: 1
Training loss: 2.201846122741699
Validation loss: 2.2687179221901843

Epoch: 5| Step: 2
Training loss: 2.5341053009033203
Validation loss: 2.2629320006216727

Epoch: 5| Step: 3
Training loss: 2.4212985038757324
Validation loss: 2.2794205809152253

Epoch: 5| Step: 4
Training loss: 2.4408984184265137
Validation loss: 2.3011163203947005

Epoch: 5| Step: 5
Training loss: 2.9174985885620117
Validation loss: 2.3070574088763167

Epoch: 5| Step: 6
Training loss: 2.920053482055664
Validation loss: 2.3020846484809794

Epoch: 5| Step: 7
Training loss: 2.080725908279419
Validation loss: 2.2796523340286745

Epoch: 5| Step: 8
Training loss: 2.5218570232391357
Validation loss: 2.282958035827965

Epoch: 5| Step: 9
Training loss: 2.3044915199279785
Validation loss: 2.267313539340932

Epoch: 5| Step: 10
Training loss: 2.5114715099334717
Validation loss: 2.2394174452750915

Epoch: 282| Step: 0
Training loss: 2.0493083000183105
Validation loss: 2.2312296257224133

Epoch: 5| Step: 1
Training loss: 2.6754159927368164
Validation loss: 2.2350694992208995

Epoch: 5| Step: 2
Training loss: 2.217393159866333
Validation loss: 2.230837989878911

Epoch: 5| Step: 3
Training loss: 2.138701915740967
Validation loss: 2.2329599626602663

Epoch: 5| Step: 4
Training loss: 1.4760303497314453
Validation loss: 2.2293022294198312

Epoch: 5| Step: 5
Training loss: 2.1666018962860107
Validation loss: 2.232752497478198

Epoch: 5| Step: 6
Training loss: 2.4943196773529053
Validation loss: 2.2369754852787143

Epoch: 5| Step: 7
Training loss: 3.184149980545044
Validation loss: 2.23007990724297

Epoch: 5| Step: 8
Training loss: 2.613492965698242
Validation loss: 2.2299645818689817

Epoch: 5| Step: 9
Training loss: 2.9922642707824707
Validation loss: 2.231266365256361

Epoch: 5| Step: 10
Training loss: 3.5383427143096924
Validation loss: 2.237469173246814

Epoch: 283| Step: 0
Training loss: 1.726560354232788
Validation loss: 2.236484921106728

Epoch: 5| Step: 1
Training loss: 3.021674394607544
Validation loss: 2.241481534896358

Epoch: 5| Step: 2
Training loss: 3.0129945278167725
Validation loss: 2.248779276365875

Epoch: 5| Step: 3
Training loss: 2.3810629844665527
Validation loss: 2.256546997254895

Epoch: 5| Step: 4
Training loss: 2.009322166442871
Validation loss: 2.281549676772087

Epoch: 5| Step: 5
Training loss: 2.444998264312744
Validation loss: 2.2971690547081733

Epoch: 5| Step: 6
Training loss: 2.883523941040039
Validation loss: 2.3059444863309144

Epoch: 5| Step: 7
Training loss: 2.502943515777588
Validation loss: 2.3133086876202653

Epoch: 5| Step: 8
Training loss: 2.114398241043091
Validation loss: 2.3018374596872637

Epoch: 5| Step: 9
Training loss: 2.680626153945923
Validation loss: 2.3012687826669342

Epoch: 5| Step: 10
Training loss: 2.2693259716033936
Validation loss: 2.2673404447494017

Epoch: 284| Step: 0
Training loss: 2.294579267501831
Validation loss: 2.256288695079024

Epoch: 5| Step: 1
Training loss: 2.1164188385009766
Validation loss: 2.2396600938612417

Epoch: 5| Step: 2
Training loss: 2.8371453285217285
Validation loss: 2.230656031639345

Epoch: 5| Step: 3
Training loss: 2.538180112838745
Validation loss: 2.2458399124042963

Epoch: 5| Step: 4
Training loss: 2.498140335083008
Validation loss: 2.2368998976163965

Epoch: 5| Step: 5
Training loss: 1.8370288610458374
Validation loss: 2.23175246484818

Epoch: 5| Step: 6
Training loss: 2.073627233505249
Validation loss: 2.2325252332995014

Epoch: 5| Step: 7
Training loss: 3.2286407947540283
Validation loss: 2.2296357026664158

Epoch: 5| Step: 8
Training loss: 3.0078577995300293
Validation loss: 2.2387317457506732

Epoch: 5| Step: 9
Training loss: 1.8638908863067627
Validation loss: 2.231662045242966

Epoch: 5| Step: 10
Training loss: 2.9284920692443848
Validation loss: 2.2567697289169475

Epoch: 285| Step: 0
Training loss: 2.1600654125213623
Validation loss: 2.267839524053758

Epoch: 5| Step: 1
Training loss: 2.3814992904663086
Validation loss: 2.2832574690541914

Epoch: 5| Step: 2
Training loss: 2.2589757442474365
Validation loss: 2.2876707110353696

Epoch: 5| Step: 3
Training loss: 3.0209479331970215
Validation loss: 2.2881424683396534

Epoch: 5| Step: 4
Training loss: 3.0557641983032227
Validation loss: 2.2802077236995903

Epoch: 5| Step: 5
Training loss: 2.277876377105713
Validation loss: 2.2885377381437566

Epoch: 5| Step: 6
Training loss: 1.8317248821258545
Validation loss: 2.2698191981161795

Epoch: 5| Step: 7
Training loss: 1.9565455913543701
Validation loss: 2.2816778126583306

Epoch: 5| Step: 8
Training loss: 2.6437387466430664
Validation loss: 2.27422922144654

Epoch: 5| Step: 9
Training loss: 2.4662232398986816
Validation loss: 2.2831527981706845

Epoch: 5| Step: 10
Training loss: 2.9618797302246094
Validation loss: 2.2831797497246855

Epoch: 286| Step: 0
Training loss: 2.5621981620788574
Validation loss: 2.276045799255371

Epoch: 5| Step: 1
Training loss: 2.486043930053711
Validation loss: 2.2710444106850574

Epoch: 5| Step: 2
Training loss: 2.6653573513031006
Validation loss: 2.2524955887948312

Epoch: 5| Step: 3
Training loss: 1.773714303970337
Validation loss: 2.2364940668946955

Epoch: 5| Step: 4
Training loss: 2.1250522136688232
Validation loss: 2.2339627332584833

Epoch: 5| Step: 5
Training loss: 2.462421417236328
Validation loss: 2.2223254147396294

Epoch: 5| Step: 6
Training loss: 2.7540791034698486
Validation loss: 2.2260099175155803

Epoch: 5| Step: 7
Training loss: 2.780139923095703
Validation loss: 2.2386318073477796

Epoch: 5| Step: 8
Training loss: 2.3754351139068604
Validation loss: 2.241591489443215

Epoch: 5| Step: 9
Training loss: 2.497539520263672
Validation loss: 2.2582856224429224

Epoch: 5| Step: 10
Training loss: 2.3832051753997803
Validation loss: 2.2683845925074753

Epoch: 287| Step: 0
Training loss: 2.535649061203003
Validation loss: 2.283153695444907

Epoch: 5| Step: 1
Training loss: 2.2945492267608643
Validation loss: 2.300247646147205

Epoch: 5| Step: 2
Training loss: 3.4970550537109375
Validation loss: 2.309901575888357

Epoch: 5| Step: 3
Training loss: 2.3439414501190186
Validation loss: 2.3167484729520735

Epoch: 5| Step: 4
Training loss: 2.6352627277374268
Validation loss: 2.310131942072222

Epoch: 5| Step: 5
Training loss: 2.3041939735412598
Validation loss: 2.3090801777378207

Epoch: 5| Step: 6
Training loss: 2.6890673637390137
Validation loss: 2.296570413856096

Epoch: 5| Step: 7
Training loss: 1.4833170175552368
Validation loss: 2.3036850037113314

Epoch: 5| Step: 8
Training loss: 1.8240407705307007
Validation loss: 2.2998690630799983

Epoch: 5| Step: 9
Training loss: 2.74275279045105
Validation loss: 2.280500065895819

Epoch: 5| Step: 10
Training loss: 2.564513683319092
Validation loss: 2.2872367956305064

Epoch: 288| Step: 0
Training loss: 3.3333499431610107
Validation loss: 2.264612228639664

Epoch: 5| Step: 1
Training loss: 1.633249044418335
Validation loss: 2.2590028521835164

Epoch: 5| Step: 2
Training loss: 2.3721861839294434
Validation loss: 2.2406103457173994

Epoch: 5| Step: 3
Training loss: 2.3574178218841553
Validation loss: 2.238650602679099

Epoch: 5| Step: 4
Training loss: 2.588618755340576
Validation loss: 2.219353481005597

Epoch: 5| Step: 5
Training loss: 2.1772494316101074
Validation loss: 2.224116443305887

Epoch: 5| Step: 6
Training loss: 2.8819823265075684
Validation loss: 2.220704250438239

Epoch: 5| Step: 7
Training loss: 2.605863094329834
Validation loss: 2.2200122353851155

Epoch: 5| Step: 8
Training loss: 2.855193853378296
Validation loss: 2.2279190914605254

Epoch: 5| Step: 9
Training loss: 2.1292598247528076
Validation loss: 2.2240533700553318

Epoch: 5| Step: 10
Training loss: 2.205657482147217
Validation loss: 2.222222948587069

Epoch: 289| Step: 0
Training loss: 2.5507683753967285
Validation loss: 2.2249687205078783

Epoch: 5| Step: 1
Training loss: 2.277228832244873
Validation loss: 2.2280024866903982

Epoch: 5| Step: 2
Training loss: 2.2511563301086426
Validation loss: 2.2468411409726707

Epoch: 5| Step: 3
Training loss: 2.6493146419525146
Validation loss: 2.2595483897834696

Epoch: 5| Step: 4
Training loss: 2.5554797649383545
Validation loss: 2.26705507052842

Epoch: 5| Step: 5
Training loss: 2.575207233428955
Validation loss: 2.2830829927998204

Epoch: 5| Step: 6
Training loss: 2.434727191925049
Validation loss: 2.280737782037386

Epoch: 5| Step: 7
Training loss: 2.2108864784240723
Validation loss: 2.2788483878617645

Epoch: 5| Step: 8
Training loss: 2.6199004650115967
Validation loss: 2.275101086144806

Epoch: 5| Step: 9
Training loss: 2.4182403087615967
Validation loss: 2.2652535797447286

Epoch: 5| Step: 10
Training loss: 2.188462972640991
Validation loss: 2.2497508654030423

Epoch: 290| Step: 0
Training loss: 2.5222442150115967
Validation loss: 2.246302249611065

Epoch: 5| Step: 1
Training loss: 2.2338881492614746
Validation loss: 2.2551041315960627

Epoch: 5| Step: 2
Training loss: 1.8301864862442017
Validation loss: 2.2587623339827343

Epoch: 5| Step: 3
Training loss: 2.115708827972412
Validation loss: 2.2640649272549536

Epoch: 5| Step: 4
Training loss: 2.7759861946105957
Validation loss: 2.2768280083133328

Epoch: 5| Step: 5
Training loss: 2.8988311290740967
Validation loss: 2.2610824364487843

Epoch: 5| Step: 6
Training loss: 2.2034099102020264
Validation loss: 2.2603199097418014

Epoch: 5| Step: 7
Training loss: 2.4762046337127686
Validation loss: 2.255821843301096

Epoch: 5| Step: 8
Training loss: 2.621805191040039
Validation loss: 2.2597143342418056

Epoch: 5| Step: 9
Training loss: 2.6048648357391357
Validation loss: 2.250868566574589

Epoch: 5| Step: 10
Training loss: 2.465592622756958
Validation loss: 2.2565866721573697

Epoch: 291| Step: 0
Training loss: 2.4423890113830566
Validation loss: 2.2581225543893795

Epoch: 5| Step: 1
Training loss: 2.0931499004364014
Validation loss: 2.2681370281404063

Epoch: 5| Step: 2
Training loss: 1.8602523803710938
Validation loss: 2.27468357804001

Epoch: 5| Step: 3
Training loss: 2.7431652545928955
Validation loss: 2.289872387404083

Epoch: 5| Step: 4
Training loss: 2.2922072410583496
Validation loss: 2.300192235618509

Epoch: 5| Step: 5
Training loss: 3.2967498302459717
Validation loss: 2.3074390298576763

Epoch: 5| Step: 6
Training loss: 2.710345506668091
Validation loss: 2.3124402415367866

Epoch: 5| Step: 7
Training loss: 2.116999387741089
Validation loss: 2.2864664972469373

Epoch: 5| Step: 8
Training loss: 2.9943432807922363
Validation loss: 2.26185538179131

Epoch: 5| Step: 9
Training loss: 1.9074987173080444
Validation loss: 2.2616085262708765

Epoch: 5| Step: 10
Training loss: 2.34942364692688
Validation loss: 2.2434905523894937

Epoch: 292| Step: 0
Training loss: 2.6014976501464844
Validation loss: 2.243230655629148

Epoch: 5| Step: 1
Training loss: 2.491793155670166
Validation loss: 2.2373850166156726

Epoch: 5| Step: 2
Training loss: 2.2661349773406982
Validation loss: 2.2382926864008748

Epoch: 5| Step: 3
Training loss: 2.397796630859375
Validation loss: 2.2330642182339906

Epoch: 5| Step: 4
Training loss: 2.4673407077789307
Validation loss: 2.2357541912345478

Epoch: 5| Step: 5
Training loss: 2.8267102241516113
Validation loss: 2.238764760314777

Epoch: 5| Step: 6
Training loss: 2.228511333465576
Validation loss: 2.2464832105944232

Epoch: 5| Step: 7
Training loss: 2.193873167037964
Validation loss: 2.255977758797266

Epoch: 5| Step: 8
Training loss: 1.7340309619903564
Validation loss: 2.264586769124513

Epoch: 5| Step: 9
Training loss: 3.20007061958313
Validation loss: 2.2670417754880843

Epoch: 5| Step: 10
Training loss: 2.3305888175964355
Validation loss: 2.2585277429191013

Epoch: 293| Step: 0
Training loss: 2.4510490894317627
Validation loss: 2.2703752556154804

Epoch: 5| Step: 1
Training loss: 2.417349338531494
Validation loss: 2.2526708892596665

Epoch: 5| Step: 2
Training loss: 2.734488010406494
Validation loss: 2.27218633826061

Epoch: 5| Step: 3
Training loss: 2.4157967567443848
Validation loss: 2.2802358622192056

Epoch: 5| Step: 4
Training loss: 2.2455801963806152
Validation loss: 2.3077751231449906

Epoch: 5| Step: 5
Training loss: 2.4488959312438965
Validation loss: 2.324372640220068

Epoch: 5| Step: 6
Training loss: 2.8055388927459717
Validation loss: 2.342201091909921

Epoch: 5| Step: 7
Training loss: 3.5163674354553223
Validation loss: 2.3176013833733013

Epoch: 5| Step: 8
Training loss: 2.2970871925354004
Validation loss: 2.291983084012103

Epoch: 5| Step: 9
Training loss: 1.5108764171600342
Validation loss: 2.2727857071866273

Epoch: 5| Step: 10
Training loss: 1.8974313735961914
Validation loss: 2.2603449334380445

Epoch: 294| Step: 0
Training loss: 2.607257127761841
Validation loss: 2.255785447294994

Epoch: 5| Step: 1
Training loss: 1.8487647771835327
Validation loss: 2.2583701379837526

Epoch: 5| Step: 2
Training loss: 2.681675672531128
Validation loss: 2.258626704574913

Epoch: 5| Step: 3
Training loss: 2.069812774658203
Validation loss: 2.261703637338454

Epoch: 5| Step: 4
Training loss: 2.7419581413269043
Validation loss: 2.281946938524964

Epoch: 5| Step: 5
Training loss: 2.5949130058288574
Validation loss: 2.3008500324782504

Epoch: 5| Step: 6
Training loss: 1.8178993463516235
Validation loss: 2.299532695483136

Epoch: 5| Step: 7
Training loss: 2.1738572120666504
Validation loss: 2.292736073975922

Epoch: 5| Step: 8
Training loss: 3.2259433269500732
Validation loss: 2.2728996943402033

Epoch: 5| Step: 9
Training loss: 2.4612059593200684
Validation loss: 2.2760867277781167

Epoch: 5| Step: 10
Training loss: 2.4308605194091797
Validation loss: 2.274223035381686

Epoch: 295| Step: 0
Training loss: 2.7990710735321045
Validation loss: 2.2629266836309947

Epoch: 5| Step: 1
Training loss: 2.862069606781006
Validation loss: 2.27014478816781

Epoch: 5| Step: 2
Training loss: 2.394596815109253
Validation loss: 2.259071175770093

Epoch: 5| Step: 3
Training loss: 2.2578372955322266
Validation loss: 2.2625385715115454

Epoch: 5| Step: 4
Training loss: 2.304234266281128
Validation loss: 2.2791159511894308

Epoch: 5| Step: 5
Training loss: 2.4274306297302246
Validation loss: 2.2754776734177784

Epoch: 5| Step: 6
Training loss: 2.817923069000244
Validation loss: 2.2676640146522113

Epoch: 5| Step: 7
Training loss: 1.9596998691558838
Validation loss: 2.263441785689323

Epoch: 5| Step: 8
Training loss: 2.2317795753479004
Validation loss: 2.2723983744139313

Epoch: 5| Step: 9
Training loss: 2.5119290351867676
Validation loss: 2.258744373116442

Epoch: 5| Step: 10
Training loss: 2.2372515201568604
Validation loss: 2.2466500882179505

Epoch: 296| Step: 0
Training loss: 2.77966046333313
Validation loss: 2.240347211078931

Epoch: 5| Step: 1
Training loss: 3.0021567344665527
Validation loss: 2.229354320033904

Epoch: 5| Step: 2
Training loss: 2.1198930740356445
Validation loss: 2.2439990428186234

Epoch: 5| Step: 3
Training loss: 2.3543577194213867
Validation loss: 2.2363876693992206

Epoch: 5| Step: 4
Training loss: 1.7055613994598389
Validation loss: 2.230272795564385

Epoch: 5| Step: 5
Training loss: 1.9577770233154297
Validation loss: 2.229427232537218

Epoch: 5| Step: 6
Training loss: 2.8524575233459473
Validation loss: 2.2463986130170923

Epoch: 5| Step: 7
Training loss: 2.556259870529175
Validation loss: 2.2575116747169086

Epoch: 5| Step: 8
Training loss: 2.8215365409851074
Validation loss: 2.297410552219678

Epoch: 5| Step: 9
Training loss: 2.2413268089294434
Validation loss: 2.325695755661175

Epoch: 5| Step: 10
Training loss: 2.476435899734497
Validation loss: 2.345333945366644

Epoch: 297| Step: 0
Training loss: 1.9409023523330688
Validation loss: 2.351173475224485

Epoch: 5| Step: 1
Training loss: 1.8663746118545532
Validation loss: 2.3509003013692875

Epoch: 5| Step: 2
Training loss: 2.9532535076141357
Validation loss: 2.3357952333265737

Epoch: 5| Step: 3
Training loss: 3.5789172649383545
Validation loss: 2.320781648799937

Epoch: 5| Step: 4
Training loss: 2.1611180305480957
Validation loss: 2.2922384456921647

Epoch: 5| Step: 5
Training loss: 2.8299918174743652
Validation loss: 2.253719401615922

Epoch: 5| Step: 6
Training loss: 2.4501285552978516
Validation loss: 2.2436752883336877

Epoch: 5| Step: 7
Training loss: 1.8244287967681885
Validation loss: 2.2370620286592873

Epoch: 5| Step: 8
Training loss: 2.810230016708374
Validation loss: 2.238156936501944

Epoch: 5| Step: 9
Training loss: 2.2716474533081055
Validation loss: 2.2257765121357416

Epoch: 5| Step: 10
Training loss: 2.234398603439331
Validation loss: 2.2367138708791425

Epoch: 298| Step: 0
Training loss: 2.265429973602295
Validation loss: 2.24679369823907

Epoch: 5| Step: 1
Training loss: 2.672011137008667
Validation loss: 2.254889758684302

Epoch: 5| Step: 2
Training loss: 2.30696177482605
Validation loss: 2.2534967724994948

Epoch: 5| Step: 3
Training loss: 2.6523334980010986
Validation loss: 2.263692832762195

Epoch: 5| Step: 4
Training loss: 1.5340880155563354
Validation loss: 2.2674492277124876

Epoch: 5| Step: 5
Training loss: 2.3953590393066406
Validation loss: 2.2717652923317364

Epoch: 5| Step: 6
Training loss: 2.550830125808716
Validation loss: 2.27283045553392

Epoch: 5| Step: 7
Training loss: 3.2273921966552734
Validation loss: 2.2704984513662194

Epoch: 5| Step: 8
Training loss: 2.6795010566711426
Validation loss: 2.271871502681445

Epoch: 5| Step: 9
Training loss: 2.2557644844055176
Validation loss: 2.2671716315771944

Epoch: 5| Step: 10
Training loss: 2.0742197036743164
Validation loss: 2.2856073071879726

Epoch: 299| Step: 0
Training loss: 2.1280651092529297
Validation loss: 2.2787434516414518

Epoch: 5| Step: 1
Training loss: 2.1851820945739746
Validation loss: 2.3021811439144995

Epoch: 5| Step: 2
Training loss: 2.1888036727905273
Validation loss: 2.323634104062152

Epoch: 5| Step: 3
Training loss: 2.6505255699157715
Validation loss: 2.3214565374517955

Epoch: 5| Step: 4
Training loss: 3.302415370941162
Validation loss: 2.3016114440015567

Epoch: 5| Step: 5
Training loss: 2.2066915035247803
Validation loss: 2.2563836190008346

Epoch: 5| Step: 6
Training loss: 2.400864839553833
Validation loss: 2.2495100498199463

Epoch: 5| Step: 7
Training loss: 2.516555070877075
Validation loss: 2.247843059160376

Epoch: 5| Step: 8
Training loss: 2.1437435150146484
Validation loss: 2.2126281248625888

Epoch: 5| Step: 9
Training loss: 2.0679988861083984
Validation loss: 2.2167822699392996

Epoch: 5| Step: 10
Training loss: 2.9534504413604736
Validation loss: 2.2188684350700787

Epoch: 300| Step: 0
Training loss: 2.36765718460083
Validation loss: 2.2081485307344826

Epoch: 5| Step: 1
Training loss: 2.74491810798645
Validation loss: 2.2111118480723393

Epoch: 5| Step: 2
Training loss: 2.5215578079223633
Validation loss: 2.2273891318228936

Epoch: 5| Step: 3
Training loss: 2.0909080505371094
Validation loss: 2.228171458808325

Epoch: 5| Step: 4
Training loss: 2.2811646461486816
Validation loss: 2.235929912136447

Epoch: 5| Step: 5
Training loss: 2.3861243724823
Validation loss: 2.233449084784395

Epoch: 5| Step: 6
Training loss: 2.714247226715088
Validation loss: 2.254443453204247

Epoch: 5| Step: 7
Training loss: 2.318345069885254
Validation loss: 2.25400189686847

Epoch: 5| Step: 8
Training loss: 2.514125347137451
Validation loss: 2.256521540303384

Epoch: 5| Step: 9
Training loss: 2.714223861694336
Validation loss: 2.3037278626554754

Epoch: 5| Step: 10
Training loss: 1.788587212562561
Validation loss: 2.3258815837162796

Epoch: 301| Step: 0
Training loss: 2.3083224296569824
Validation loss: 2.3285082694022887

Epoch: 5| Step: 1
Training loss: 2.370389461517334
Validation loss: 2.3264598179888982

Epoch: 5| Step: 2
Training loss: 2.5722737312316895
Validation loss: 2.329560249082504

Epoch: 5| Step: 3
Training loss: 2.2489447593688965
Validation loss: 2.3168639559899606

Epoch: 5| Step: 4
Training loss: 3.587632656097412
Validation loss: 2.2744403141801075

Epoch: 5| Step: 5
Training loss: 2.280813217163086
Validation loss: 2.2417461333736295

Epoch: 5| Step: 6
Training loss: 2.300665855407715
Validation loss: 2.2281885941823325

Epoch: 5| Step: 7
Training loss: 2.399505138397217
Validation loss: 2.2211212727331344

Epoch: 5| Step: 8
Training loss: 1.6890290975570679
Validation loss: 2.2277016075708533

Epoch: 5| Step: 9
Training loss: 2.287233829498291
Validation loss: 2.2232753461407078

Epoch: 5| Step: 10
Training loss: 2.498461961746216
Validation loss: 2.2298647831845027

Epoch: 302| Step: 0
Training loss: 2.458392858505249
Validation loss: 2.2274242703632643

Epoch: 5| Step: 1
Training loss: 2.2445662021636963
Validation loss: 2.2342782789661038

Epoch: 5| Step: 2
Training loss: 2.9115190505981445
Validation loss: 2.2519736443796465

Epoch: 5| Step: 3
Training loss: 2.0161731243133545
Validation loss: 2.2592728445606847

Epoch: 5| Step: 4
Training loss: 2.6117441654205322
Validation loss: 2.2895171885849326

Epoch: 5| Step: 5
Training loss: 2.1430752277374268
Validation loss: 2.285058039490895

Epoch: 5| Step: 6
Training loss: 2.5131077766418457
Validation loss: 2.303175644208026

Epoch: 5| Step: 7
Training loss: 2.596564531326294
Validation loss: 2.293762676177486

Epoch: 5| Step: 8
Training loss: 1.980539321899414
Validation loss: 2.2683629066713396

Epoch: 5| Step: 9
Training loss: 2.448786497116089
Validation loss: 2.260799664323048

Epoch: 5| Step: 10
Training loss: 2.8207340240478516
Validation loss: 2.238352032117946

Epoch: 303| Step: 0
Training loss: 1.7986774444580078
Validation loss: 2.213718698870751

Epoch: 5| Step: 1
Training loss: 3.100665330886841
Validation loss: 2.2193169311810563

Epoch: 5| Step: 2
Training loss: 2.755026340484619
Validation loss: 2.2115056976195304

Epoch: 5| Step: 3
Training loss: 2.20464825630188
Validation loss: 2.211429234473936

Epoch: 5| Step: 4
Training loss: 3.2619991302490234
Validation loss: 2.2096004844993673

Epoch: 5| Step: 5
Training loss: 2.1703286170959473
Validation loss: 2.2032430864149526

Epoch: 5| Step: 6
Training loss: 2.3388843536376953
Validation loss: 2.2031458782893356

Epoch: 5| Step: 7
Training loss: 2.161634683609009
Validation loss: 2.201698504468446

Epoch: 5| Step: 8
Training loss: 2.405944585800171
Validation loss: 2.2019232908884683

Epoch: 5| Step: 9
Training loss: 2.420398712158203
Validation loss: 2.197886595162012

Epoch: 5| Step: 10
Training loss: 2.1445131301879883
Validation loss: 2.2105027885847193

Epoch: 304| Step: 0
Training loss: 2.650620937347412
Validation loss: 2.220351687041662

Epoch: 5| Step: 1
Training loss: 2.4636571407318115
Validation loss: 2.211086521866501

Epoch: 5| Step: 2
Training loss: 1.9255584478378296
Validation loss: 2.2301911410465034

Epoch: 5| Step: 3
Training loss: 2.647010326385498
Validation loss: 2.2307928505764214

Epoch: 5| Step: 4
Training loss: 2.370936870574951
Validation loss: 2.246364483269312

Epoch: 5| Step: 5
Training loss: 2.4334158897399902
Validation loss: 2.262564520682058

Epoch: 5| Step: 6
Training loss: 2.281128406524658
Validation loss: 2.2428673492964877

Epoch: 5| Step: 7
Training loss: 1.9902856349945068
Validation loss: 2.2521795406136462

Epoch: 5| Step: 8
Training loss: 2.895331859588623
Validation loss: 2.2624265557976178

Epoch: 5| Step: 9
Training loss: 2.1689765453338623
Validation loss: 2.2506696793340866

Epoch: 5| Step: 10
Training loss: 2.698960304260254
Validation loss: 2.2612351025304487

Epoch: 305| Step: 0
Training loss: 2.048983097076416
Validation loss: 2.2745060497714626

Epoch: 5| Step: 1
Training loss: 2.805981397628784
Validation loss: 2.2985066906098397

Epoch: 5| Step: 2
Training loss: 2.3936047554016113
Validation loss: 2.2960947892999135

Epoch: 5| Step: 3
Training loss: 2.0569024085998535
Validation loss: 2.2937636811246156

Epoch: 5| Step: 4
Training loss: 2.4526619911193848
Validation loss: 2.26935290521191

Epoch: 5| Step: 5
Training loss: 2.422765016555786
Validation loss: 2.2644157332758748

Epoch: 5| Step: 6
Training loss: 2.7824127674102783
Validation loss: 2.2512003567910965

Epoch: 5| Step: 7
Training loss: 2.256681442260742
Validation loss: 2.2399753883320797

Epoch: 5| Step: 8
Training loss: 2.231386184692383
Validation loss: 2.2339783419844923

Epoch: 5| Step: 9
Training loss: 2.242027759552002
Validation loss: 2.236438394874655

Epoch: 5| Step: 10
Training loss: 2.828989267349243
Validation loss: 2.239282641359555

Epoch: 306| Step: 0
Training loss: 2.6551406383514404
Validation loss: 2.2394496497287544

Epoch: 5| Step: 1
Training loss: 2.7173962593078613
Validation loss: 2.240076639318979

Epoch: 5| Step: 2
Training loss: 2.719536304473877
Validation loss: 2.251624604707123

Epoch: 5| Step: 3
Training loss: 2.1166951656341553
Validation loss: 2.286529543579266

Epoch: 5| Step: 4
Training loss: 2.2502732276916504
Validation loss: 2.2938841337798745

Epoch: 5| Step: 5
Training loss: 2.9623489379882812
Validation loss: 2.322410470695906

Epoch: 5| Step: 6
Training loss: 2.363664388656616
Validation loss: 2.316407221619801

Epoch: 5| Step: 7
Training loss: 2.266726016998291
Validation loss: 2.298513707294259

Epoch: 5| Step: 8
Training loss: 1.61065673828125
Validation loss: 2.2874743887173232

Epoch: 5| Step: 9
Training loss: 2.549503803253174
Validation loss: 2.278060249103013

Epoch: 5| Step: 10
Training loss: 2.2954647541046143
Validation loss: 2.259150407647574

Epoch: 307| Step: 0
Training loss: 2.971418857574463
Validation loss: 2.2374999805163314

Epoch: 5| Step: 1
Training loss: 2.4560770988464355
Validation loss: 2.2133306841696463

Epoch: 5| Step: 2
Training loss: 2.4934165477752686
Validation loss: 2.207152666584138

Epoch: 5| Step: 3
Training loss: 2.692718029022217
Validation loss: 2.2079902054161153

Epoch: 5| Step: 4
Training loss: 1.8804206848144531
Validation loss: 2.2043262271470923

Epoch: 5| Step: 5
Training loss: 2.3097004890441895
Validation loss: 2.2081103478708575

Epoch: 5| Step: 6
Training loss: 2.8650081157684326
Validation loss: 2.213005501736877

Epoch: 5| Step: 7
Training loss: 2.872846841812134
Validation loss: 2.206007895931121

Epoch: 5| Step: 8
Training loss: 2.344921827316284
Validation loss: 2.2160252396778395

Epoch: 5| Step: 9
Training loss: 1.961206078529358
Validation loss: 2.2168055426689888

Epoch: 5| Step: 10
Training loss: 2.024664878845215
Validation loss: 2.220560545562416

Epoch: 308| Step: 0
Training loss: 2.131865978240967
Validation loss: 2.22822021284411

Epoch: 5| Step: 1
Training loss: 2.040837049484253
Validation loss: 2.2423623864368727

Epoch: 5| Step: 2
Training loss: 2.382869243621826
Validation loss: 2.269657900256495

Epoch: 5| Step: 3
Training loss: 1.981870412826538
Validation loss: 2.2858239348216722

Epoch: 5| Step: 4
Training loss: 2.4855546951293945
Validation loss: 2.282429882275161

Epoch: 5| Step: 5
Training loss: 2.3241817951202393
Validation loss: 2.2813519098425425

Epoch: 5| Step: 6
Training loss: 2.7478580474853516
Validation loss: 2.2794629117493987

Epoch: 5| Step: 7
Training loss: 2.1157588958740234
Validation loss: 2.261373714734149

Epoch: 5| Step: 8
Training loss: 2.9985997676849365
Validation loss: 2.2416319129287556

Epoch: 5| Step: 9
Training loss: 2.9413905143737793
Validation loss: 2.241744725934921

Epoch: 5| Step: 10
Training loss: 2.4935476779937744
Validation loss: 2.227025060243504

Epoch: 309| Step: 0
Training loss: 2.611112356185913
Validation loss: 2.2237463048709336

Epoch: 5| Step: 1
Training loss: 2.2620766162872314
Validation loss: 2.2300255234523485

Epoch: 5| Step: 2
Training loss: 2.7115235328674316
Validation loss: 2.252290553944085

Epoch: 5| Step: 3
Training loss: 2.471569538116455
Validation loss: 2.2602525116294943

Epoch: 5| Step: 4
Training loss: 1.766908884048462
Validation loss: 2.280284584209483

Epoch: 5| Step: 5
Training loss: 2.1006271839141846
Validation loss: 2.304871771925239

Epoch: 5| Step: 6
Training loss: 2.3621649742126465
Validation loss: 2.3143127451660814

Epoch: 5| Step: 7
Training loss: 2.6330599784851074
Validation loss: 2.32955478852795

Epoch: 5| Step: 8
Training loss: 3.034867286682129
Validation loss: 2.3204199408972137

Epoch: 5| Step: 9
Training loss: 2.0693206787109375
Validation loss: 2.313086789141419

Epoch: 5| Step: 10
Training loss: 2.5775530338287354
Validation loss: 2.29543323157936

Epoch: 310| Step: 0
Training loss: 2.1142947673797607
Validation loss: 2.263468693661433

Epoch: 5| Step: 1
Training loss: 2.549063205718994
Validation loss: 2.2555775539849394

Epoch: 5| Step: 2
Training loss: 2.503363847732544
Validation loss: 2.234184024154499

Epoch: 5| Step: 3
Training loss: 2.246027708053589
Validation loss: 2.2265132780997985

Epoch: 5| Step: 4
Training loss: 2.705880880355835
Validation loss: 2.2323941799902145

Epoch: 5| Step: 5
Training loss: 2.412449359893799
Validation loss: 2.2179287761770268

Epoch: 5| Step: 6
Training loss: 2.623983860015869
Validation loss: 2.231891652589203

Epoch: 5| Step: 7
Training loss: 1.8549484014511108
Validation loss: 2.213278406409807

Epoch: 5| Step: 8
Training loss: 2.629242420196533
Validation loss: 2.2185851784162622

Epoch: 5| Step: 9
Training loss: 2.60508394241333
Validation loss: 2.221416086278936

Epoch: 5| Step: 10
Training loss: 2.1139676570892334
Validation loss: 2.219536873602098

Epoch: 311| Step: 0
Training loss: 2.15891432762146
Validation loss: 2.221820014779286

Epoch: 5| Step: 1
Training loss: 2.9132080078125
Validation loss: 2.2371692272924606

Epoch: 5| Step: 2
Training loss: 2.9097790718078613
Validation loss: 2.2423120762712214

Epoch: 5| Step: 3
Training loss: 2.0293240547180176
Validation loss: 2.2580520081263717

Epoch: 5| Step: 4
Training loss: 1.7013835906982422
Validation loss: 2.2636624741297897

Epoch: 5| Step: 5
Training loss: 2.443722724914551
Validation loss: 2.251381871520832

Epoch: 5| Step: 6
Training loss: 1.7939125299453735
Validation loss: 2.250610775845025

Epoch: 5| Step: 7
Training loss: 2.4534718990325928
Validation loss: 2.2663361513486473

Epoch: 5| Step: 8
Training loss: 2.4420251846313477
Validation loss: 2.257054244318316

Epoch: 5| Step: 9
Training loss: 3.0747227668762207
Validation loss: 2.260698131335679

Epoch: 5| Step: 10
Training loss: 2.5882673263549805
Validation loss: 2.249746755887103

Epoch: 312| Step: 0
Training loss: 1.9870014190673828
Validation loss: 2.241408573683872

Epoch: 5| Step: 1
Training loss: 2.6329197883605957
Validation loss: 2.216681429134902

Epoch: 5| Step: 2
Training loss: 2.598659038543701
Validation loss: 2.2063270538083968

Epoch: 5| Step: 3
Training loss: 2.734609603881836
Validation loss: 2.1987507343292236

Epoch: 5| Step: 4
Training loss: 2.701167583465576
Validation loss: 2.1937149058106127

Epoch: 5| Step: 5
Training loss: 2.7086596488952637
Validation loss: 2.1955623242162887

Epoch: 5| Step: 6
Training loss: 2.640291452407837
Validation loss: 2.19239983635564

Epoch: 5| Step: 7
Training loss: 2.0275492668151855
Validation loss: 2.193043606255644

Epoch: 5| Step: 8
Training loss: 2.5257863998413086
Validation loss: 2.1911380880622455

Epoch: 5| Step: 9
Training loss: 1.8212296962738037
Validation loss: 2.1978504478290515

Epoch: 5| Step: 10
Training loss: 2.031909942626953
Validation loss: 2.2057782065483833

Epoch: 313| Step: 0
Training loss: 2.898195743560791
Validation loss: 2.206248865332655

Epoch: 5| Step: 1
Training loss: 2.273235321044922
Validation loss: 2.2255872782840522

Epoch: 5| Step: 2
Training loss: 2.5742239952087402
Validation loss: 2.253659702116443

Epoch: 5| Step: 3
Training loss: 2.626375675201416
Validation loss: 2.2855605117736326

Epoch: 5| Step: 4
Training loss: 2.314081907272339
Validation loss: 2.3039646251227266

Epoch: 5| Step: 5
Training loss: 2.3813586235046387
Validation loss: 2.3418351963002193

Epoch: 5| Step: 6
Training loss: 2.2552688121795654
Validation loss: 2.368108387916319

Epoch: 5| Step: 7
Training loss: 1.9859176874160767
Validation loss: 2.390320021619079

Epoch: 5| Step: 8
Training loss: 2.3425345420837402
Validation loss: 2.39324458952873

Epoch: 5| Step: 9
Training loss: 2.6129186153411865
Validation loss: 2.3823951803227907

Epoch: 5| Step: 10
Training loss: 2.438234806060791
Validation loss: 2.347953583604546

Epoch: 314| Step: 0
Training loss: 2.5106565952301025
Validation loss: 2.3237547489904586

Epoch: 5| Step: 1
Training loss: 1.9746005535125732
Validation loss: 2.290388386736634

Epoch: 5| Step: 2
Training loss: 3.1282906532287598
Validation loss: 2.262412219919184

Epoch: 5| Step: 3
Training loss: 2.970395088195801
Validation loss: 2.2483417346913326

Epoch: 5| Step: 4
Training loss: 2.6656157970428467
Validation loss: 2.234760126759929

Epoch: 5| Step: 5
Training loss: 2.050351142883301
Validation loss: 2.2370943459131385

Epoch: 5| Step: 6
Training loss: 2.383756637573242
Validation loss: 2.224655451313142

Epoch: 5| Step: 7
Training loss: 1.477940559387207
Validation loss: 2.217470274176649

Epoch: 5| Step: 8
Training loss: 2.640101671218872
Validation loss: 2.220222165507655

Epoch: 5| Step: 9
Training loss: 2.4488601684570312
Validation loss: 2.2204795601547405

Epoch: 5| Step: 10
Training loss: 2.327604293823242
Validation loss: 2.243470585474404

Epoch: 315| Step: 0
Training loss: 2.55456280708313
Validation loss: 2.2500202566064815

Epoch: 5| Step: 1
Training loss: 2.674983501434326
Validation loss: 2.2633747964776973

Epoch: 5| Step: 2
Training loss: 2.2917847633361816
Validation loss: 2.2674056342853013

Epoch: 5| Step: 3
Training loss: 2.265279769897461
Validation loss: 2.2959383482574136

Epoch: 5| Step: 4
Training loss: 2.488623857498169
Validation loss: 2.29863989737726

Epoch: 5| Step: 5
Training loss: 2.5229101181030273
Validation loss: 2.304574686993835

Epoch: 5| Step: 6
Training loss: 2.9675276279449463
Validation loss: 2.2855954862410024

Epoch: 5| Step: 7
Training loss: 2.6696255207061768
Validation loss: 2.2813838194775324

Epoch: 5| Step: 8
Training loss: 2.0072362422943115
Validation loss: 2.2574477170103338

Epoch: 5| Step: 9
Training loss: 1.9738366603851318
Validation loss: 2.2452627663971274

Epoch: 5| Step: 10
Training loss: 1.906703233718872
Validation loss: 2.233822989207442

Epoch: 316| Step: 0
Training loss: 2.0029802322387695
Validation loss: 2.226368555458643

Epoch: 5| Step: 1
Training loss: 1.8078243732452393
Validation loss: 2.231226821099558

Epoch: 5| Step: 2
Training loss: 2.6335678100585938
Validation loss: 2.226855598470216

Epoch: 5| Step: 3
Training loss: 2.847027540206909
Validation loss: 2.231479137174545

Epoch: 5| Step: 4
Training loss: 2.679248094558716
Validation loss: 2.235647204101727

Epoch: 5| Step: 5
Training loss: 2.3926239013671875
Validation loss: 2.23661893926641

Epoch: 5| Step: 6
Training loss: 2.6268362998962402
Validation loss: 2.2477349081347064

Epoch: 5| Step: 7
Training loss: 2.3362488746643066
Validation loss: 2.2553012909427768

Epoch: 5| Step: 8
Training loss: 2.1363203525543213
Validation loss: 2.2622753907275457

Epoch: 5| Step: 9
Training loss: 3.38531756401062
Validation loss: 2.2633875544353197

Epoch: 5| Step: 10
Training loss: 1.5213289260864258
Validation loss: 2.259027217024116

Epoch: 317| Step: 0
Training loss: 2.1100971698760986
Validation loss: 2.267494438796915

Epoch: 5| Step: 1
Training loss: 2.360299825668335
Validation loss: 2.297733811921971

Epoch: 5| Step: 2
Training loss: 1.9514026641845703
Validation loss: 2.2965852034989225

Epoch: 5| Step: 3
Training loss: 2.869433879852295
Validation loss: 2.298904567636469

Epoch: 5| Step: 4
Training loss: 3.0112526416778564
Validation loss: 2.309718560147029

Epoch: 5| Step: 5
Training loss: 2.8263423442840576
Validation loss: 2.3166147611474477

Epoch: 5| Step: 6
Training loss: 2.502894639968872
Validation loss: 2.319122160634687

Epoch: 5| Step: 7
Training loss: 2.418344497680664
Validation loss: 2.3210528025063137

Epoch: 5| Step: 8
Training loss: 2.097177505493164
Validation loss: 2.3182350435564594

Epoch: 5| Step: 9
Training loss: 2.1406142711639404
Validation loss: 2.295590246877363

Epoch: 5| Step: 10
Training loss: 1.904926061630249
Validation loss: 2.2874014300684773

Epoch: 318| Step: 0
Training loss: 2.6990067958831787
Validation loss: 2.2520815685231197

Epoch: 5| Step: 1
Training loss: 2.798039197921753
Validation loss: 2.223284844429262

Epoch: 5| Step: 2
Training loss: 1.8188148736953735
Validation loss: 2.2165010154888196

Epoch: 5| Step: 3
Training loss: 2.3924899101257324
Validation loss: 2.215367101853894

Epoch: 5| Step: 4
Training loss: 2.4122204780578613
Validation loss: 2.2143397664511077

Epoch: 5| Step: 5
Training loss: 2.987401247024536
Validation loss: 2.210524712839434

Epoch: 5| Step: 6
Training loss: 2.3936705589294434
Validation loss: 2.2199565979742233

Epoch: 5| Step: 7
Training loss: 2.5828208923339844
Validation loss: 2.2137126050969607

Epoch: 5| Step: 8
Training loss: 1.9528785943984985
Validation loss: 2.2284377903066654

Epoch: 5| Step: 9
Training loss: 2.1662449836730957
Validation loss: 2.2430359907047723

Epoch: 5| Step: 10
Training loss: 2.1183087825775146
Validation loss: 2.2581269459057878

Epoch: 319| Step: 0
Training loss: 3.0443758964538574
Validation loss: 2.2709579057590936

Epoch: 5| Step: 1
Training loss: 2.4954586029052734
Validation loss: 2.2756099675291326

Epoch: 5| Step: 2
Training loss: 2.3398098945617676
Validation loss: 2.271935957734303

Epoch: 5| Step: 3
Training loss: 2.5418624877929688
Validation loss: 2.276490540914638

Epoch: 5| Step: 4
Training loss: 1.8936809301376343
Validation loss: 2.2783168900397515

Epoch: 5| Step: 5
Training loss: 1.9921178817749023
Validation loss: 2.26871657371521

Epoch: 5| Step: 6
Training loss: 1.7838528156280518
Validation loss: 2.2806888344467326

Epoch: 5| Step: 7
Training loss: 2.6689021587371826
Validation loss: 2.274716167039769

Epoch: 5| Step: 8
Training loss: 2.8287620544433594
Validation loss: 2.2783511941150953

Epoch: 5| Step: 9
Training loss: 1.8833414316177368
Validation loss: 2.2695092872906755

Epoch: 5| Step: 10
Training loss: 2.5675933361053467
Validation loss: 2.2597962502510316

Epoch: 320| Step: 0
Training loss: 2.19041109085083
Validation loss: 2.2347784388449883

Epoch: 5| Step: 1
Training loss: 2.4178342819213867
Validation loss: 2.2340078738427933

Epoch: 5| Step: 2
Training loss: 2.30092453956604
Validation loss: 2.236317934528474

Epoch: 5| Step: 3
Training loss: 2.598128318786621
Validation loss: 2.233649625573107

Epoch: 5| Step: 4
Training loss: 2.689699649810791
Validation loss: 2.2215124330212994

Epoch: 5| Step: 5
Training loss: 2.2911343574523926
Validation loss: 2.2438943232259443

Epoch: 5| Step: 6
Training loss: 2.3891730308532715
Validation loss: 2.2573564283309446

Epoch: 5| Step: 7
Training loss: 2.3423876762390137
Validation loss: 2.2542242439844276

Epoch: 5| Step: 8
Training loss: 1.4765284061431885
Validation loss: 2.2627730215749433

Epoch: 5| Step: 9
Training loss: 2.638890027999878
Validation loss: 2.2515181828570623

Epoch: 5| Step: 10
Training loss: 2.8061342239379883
Validation loss: 2.2468193807909564

Epoch: 321| Step: 0
Training loss: 2.613856792449951
Validation loss: 2.237411058077248

Epoch: 5| Step: 1
Training loss: 2.429630756378174
Validation loss: 2.2374255939196517

Epoch: 5| Step: 2
Training loss: 2.0579943656921387
Validation loss: 2.238426205932453

Epoch: 5| Step: 3
Training loss: 1.5007178783416748
Validation loss: 2.2408641487039547

Epoch: 5| Step: 4
Training loss: 2.4572765827178955
Validation loss: 2.2246815312293267

Epoch: 5| Step: 5
Training loss: 2.452422618865967
Validation loss: 2.2379238784954114

Epoch: 5| Step: 6
Training loss: 2.3530707359313965
Validation loss: 2.2388288744034304

Epoch: 5| Step: 7
Training loss: 2.4035773277282715
Validation loss: 2.229418437968018

Epoch: 5| Step: 8
Training loss: 2.086516857147217
Validation loss: 2.2535197324650262

Epoch: 5| Step: 9
Training loss: 2.959282398223877
Validation loss: 2.2515662793190248

Epoch: 5| Step: 10
Training loss: 2.7435832023620605
Validation loss: 2.2517559630896455

Epoch: 322| Step: 0
Training loss: 2.134598970413208
Validation loss: 2.250039223701723

Epoch: 5| Step: 1
Training loss: 1.9045209884643555
Validation loss: 2.2357340448646137

Epoch: 5| Step: 2
Training loss: 2.6997084617614746
Validation loss: 2.2290276737623316

Epoch: 5| Step: 3
Training loss: 2.6106228828430176
Validation loss: 2.227480785821074

Epoch: 5| Step: 4
Training loss: 2.240548610687256
Validation loss: 2.2302754771324897

Epoch: 5| Step: 5
Training loss: 2.269946575164795
Validation loss: 2.237080773999614

Epoch: 5| Step: 6
Training loss: 2.3536741733551025
Validation loss: 2.2214410433205227

Epoch: 5| Step: 7
Training loss: 2.5577239990234375
Validation loss: 2.225189778112596

Epoch: 5| Step: 8
Training loss: 2.629627227783203
Validation loss: 2.235212000467444

Epoch: 5| Step: 9
Training loss: 2.281433343887329
Validation loss: 2.2380942990702968

Epoch: 5| Step: 10
Training loss: 2.303912878036499
Validation loss: 2.2338506970354306

Epoch: 323| Step: 0
Training loss: 2.0294389724731445
Validation loss: 2.232693442734339

Epoch: 5| Step: 1
Training loss: 2.3476314544677734
Validation loss: 2.225849134947664

Epoch: 5| Step: 2
Training loss: 2.1536619663238525
Validation loss: 2.239655666453864

Epoch: 5| Step: 3
Training loss: 2.8641321659088135
Validation loss: 2.2415609334104802

Epoch: 5| Step: 4
Training loss: 2.550964117050171
Validation loss: 2.2497006436829925

Epoch: 5| Step: 5
Training loss: 1.77056086063385
Validation loss: 2.2550225052782285

Epoch: 5| Step: 6
Training loss: 2.5888285636901855
Validation loss: 2.2520471003747757

Epoch: 5| Step: 7
Training loss: 2.249499797821045
Validation loss: 2.273045100191588

Epoch: 5| Step: 8
Training loss: 2.1487178802490234
Validation loss: 2.264996502989082

Epoch: 5| Step: 9
Training loss: 2.3085789680480957
Validation loss: 2.275999670387596

Epoch: 5| Step: 10
Training loss: 3.002061128616333
Validation loss: 2.2841419507098455

Epoch: 324| Step: 0
Training loss: 2.284850597381592
Validation loss: 2.2833121873999156

Epoch: 5| Step: 1
Training loss: 2.1868338584899902
Validation loss: 2.2484499869808072

Epoch: 5| Step: 2
Training loss: 2.850269317626953
Validation loss: 2.234241036958592

Epoch: 5| Step: 3
Training loss: 1.6841843128204346
Validation loss: 2.220868073483949

Epoch: 5| Step: 4
Training loss: 2.599388599395752
Validation loss: 2.2092843235179944

Epoch: 5| Step: 5
Training loss: 2.112567186355591
Validation loss: 2.19601535284391

Epoch: 5| Step: 6
Training loss: 2.6376357078552246
Validation loss: 2.2047971064044583

Epoch: 5| Step: 7
Training loss: 2.5940301418304443
Validation loss: 2.2222556093687653

Epoch: 5| Step: 8
Training loss: 2.6703739166259766
Validation loss: 2.222289331497685

Epoch: 5| Step: 9
Training loss: 2.1500706672668457
Validation loss: 2.239061265863398

Epoch: 5| Step: 10
Training loss: 2.2705180644989014
Validation loss: 2.2538038505020963

Epoch: 325| Step: 0
Training loss: 2.4367852210998535
Validation loss: 2.269066474770987

Epoch: 5| Step: 1
Training loss: 2.4455997943878174
Validation loss: 2.263693226281033

Epoch: 5| Step: 2
Training loss: 2.0926928520202637
Validation loss: 2.2618373491430797

Epoch: 5| Step: 3
Training loss: 2.2084765434265137
Validation loss: 2.2549801513712895

Epoch: 5| Step: 4
Training loss: 2.0516414642333984
Validation loss: 2.246646547830233

Epoch: 5| Step: 5
Training loss: 1.4432369470596313
Validation loss: 2.2452426520727014

Epoch: 5| Step: 6
Training loss: 2.9242358207702637
Validation loss: 2.244871077998992

Epoch: 5| Step: 7
Training loss: 2.9917714595794678
Validation loss: 2.2477962804097

Epoch: 5| Step: 8
Training loss: 2.496262550354004
Validation loss: 2.250211415752288

Epoch: 5| Step: 9
Training loss: 2.3846242427825928
Validation loss: 2.2537074345414356

Epoch: 5| Step: 10
Training loss: 2.3914871215820312
Validation loss: 2.245119574249432

Epoch: 326| Step: 0
Training loss: 1.4938279390335083
Validation loss: 2.254764026211154

Epoch: 5| Step: 1
Training loss: 2.9192612171173096
Validation loss: 2.24831558299321

Epoch: 5| Step: 2
Training loss: 3.058995246887207
Validation loss: 2.250778296942352

Epoch: 5| Step: 3
Training loss: 2.7912704944610596
Validation loss: 2.249256190433297

Epoch: 5| Step: 4
Training loss: 1.4543763399124146
Validation loss: 2.246284325917562

Epoch: 5| Step: 5
Training loss: 2.51336407661438
Validation loss: 2.252302808146323

Epoch: 5| Step: 6
Training loss: 2.51861572265625
Validation loss: 2.2722562564316617

Epoch: 5| Step: 7
Training loss: 2.5761618614196777
Validation loss: 2.2660139042844056

Epoch: 5| Step: 8
Training loss: 2.041757822036743
Validation loss: 2.245837301336309

Epoch: 5| Step: 9
Training loss: 2.150139331817627
Validation loss: 2.2427615504111014

Epoch: 5| Step: 10
Training loss: 2.360025405883789
Validation loss: 2.21971026543648

Epoch: 327| Step: 0
Training loss: 2.9601216316223145
Validation loss: 2.2229564254001906

Epoch: 5| Step: 1
Training loss: 1.861954689025879
Validation loss: 2.204275982354277

Epoch: 5| Step: 2
Training loss: 2.327908515930176
Validation loss: 2.196293874453473

Epoch: 5| Step: 3
Training loss: 3.0515143871307373
Validation loss: 2.1987587072516

Epoch: 5| Step: 4
Training loss: 2.4985549449920654
Validation loss: 2.192660682944841

Epoch: 5| Step: 5
Training loss: 2.000415325164795
Validation loss: 2.1832237038561093

Epoch: 5| Step: 6
Training loss: 2.155830144882202
Validation loss: 2.185611968399376

Epoch: 5| Step: 7
Training loss: 2.849334716796875
Validation loss: 2.1849216722672984

Epoch: 5| Step: 8
Training loss: 1.3889915943145752
Validation loss: 2.181701062827982

Epoch: 5| Step: 9
Training loss: 2.2965035438537598
Validation loss: 2.195222523904616

Epoch: 5| Step: 10
Training loss: 2.6518161296844482
Validation loss: 2.210504961270158

Epoch: 328| Step: 0
Training loss: 1.7379169464111328
Validation loss: 2.2400525090514973

Epoch: 5| Step: 1
Training loss: 2.712416410446167
Validation loss: 2.2458478814812115

Epoch: 5| Step: 2
Training loss: 2.890751361846924
Validation loss: 2.273474890698669

Epoch: 5| Step: 3
Training loss: 2.2100372314453125
Validation loss: 2.273302411520353

Epoch: 5| Step: 4
Training loss: 2.312781810760498
Validation loss: 2.2822864875998548

Epoch: 5| Step: 5
Training loss: 2.545149326324463
Validation loss: 2.271325380571427

Epoch: 5| Step: 6
Training loss: 2.149887800216675
Validation loss: 2.2375594428790513

Epoch: 5| Step: 7
Training loss: 2.5663273334503174
Validation loss: 2.230602664332236

Epoch: 5| Step: 8
Training loss: 2.302699565887451
Validation loss: 2.2164481532189155

Epoch: 5| Step: 9
Training loss: 2.395158052444458
Validation loss: 2.20697804163861

Epoch: 5| Step: 10
Training loss: 2.051602840423584
Validation loss: 2.211457190975066

Epoch: 329| Step: 0
Training loss: 2.6027321815490723
Validation loss: 2.2284878287264096

Epoch: 5| Step: 1
Training loss: 1.6276123523712158
Validation loss: 2.229925024893976

Epoch: 5| Step: 2
Training loss: 2.0639216899871826
Validation loss: 2.238531712562807

Epoch: 5| Step: 3
Training loss: 2.7213215827941895
Validation loss: 2.2370529097895466

Epoch: 5| Step: 4
Training loss: 1.9909166097640991
Validation loss: 2.2575127001731627

Epoch: 5| Step: 5
Training loss: 2.5300095081329346
Validation loss: 2.2552102740092943

Epoch: 5| Step: 6
Training loss: 3.041156053543091
Validation loss: 2.2516015421959663

Epoch: 5| Step: 7
Training loss: 1.9376423358917236
Validation loss: 2.2429842666913102

Epoch: 5| Step: 8
Training loss: 2.4834678173065186
Validation loss: 2.2402724091724684

Epoch: 5| Step: 9
Training loss: 2.5925140380859375
Validation loss: 2.241413798383487

Epoch: 5| Step: 10
Training loss: 2.1936562061309814
Validation loss: 2.2524178284470753

Epoch: 330| Step: 0
Training loss: 2.7305331230163574
Validation loss: 2.2514202261483796

Epoch: 5| Step: 1
Training loss: 2.679457426071167
Validation loss: 2.254502986067085

Epoch: 5| Step: 2
Training loss: 1.7088918685913086
Validation loss: 2.236556988890453

Epoch: 5| Step: 3
Training loss: 2.2965304851531982
Validation loss: 2.23116643967167

Epoch: 5| Step: 4
Training loss: 2.2881226539611816
Validation loss: 2.2184938307731383

Epoch: 5| Step: 5
Training loss: 2.1961629390716553
Validation loss: 2.2055219860487085

Epoch: 5| Step: 6
Training loss: 1.828956961631775
Validation loss: 2.2151884776289745

Epoch: 5| Step: 7
Training loss: 2.2739555835723877
Validation loss: 2.223082011745822

Epoch: 5| Step: 8
Training loss: 3.0507102012634277
Validation loss: 2.244341258079775

Epoch: 5| Step: 9
Training loss: 2.3494303226470947
Validation loss: 2.241938198766401

Epoch: 5| Step: 10
Training loss: 2.2904317378997803
Validation loss: 2.245852255052136

Epoch: 331| Step: 0
Training loss: 2.457732677459717
Validation loss: 2.244529608757265

Epoch: 5| Step: 1
Training loss: 2.104182481765747
Validation loss: 2.267030933851837

Epoch: 5| Step: 2
Training loss: 2.0304863452911377
Validation loss: 2.2671656685490764

Epoch: 5| Step: 3
Training loss: 1.7449731826782227
Validation loss: 2.255353271320302

Epoch: 5| Step: 4
Training loss: 3.1476478576660156
Validation loss: 2.2726629652002805

Epoch: 5| Step: 5
Training loss: 2.0890629291534424
Validation loss: 2.267613399413324

Epoch: 5| Step: 6
Training loss: 2.0129265785217285
Validation loss: 2.266936727749404

Epoch: 5| Step: 7
Training loss: 2.26324462890625
Validation loss: 2.248965714567451

Epoch: 5| Step: 8
Training loss: 2.4836089611053467
Validation loss: 2.246472635576802

Epoch: 5| Step: 9
Training loss: 3.0443382263183594
Validation loss: 2.228367487589518

Epoch: 5| Step: 10
Training loss: 2.243927478790283
Validation loss: 2.2089397958529893

Epoch: 332| Step: 0
Training loss: 2.5611789226531982
Validation loss: 2.194032790840313

Epoch: 5| Step: 1
Training loss: 1.9297475814819336
Validation loss: 2.1916908218014624

Epoch: 5| Step: 2
Training loss: 1.4440350532531738
Validation loss: 2.191273873852145

Epoch: 5| Step: 3
Training loss: 2.4276845455169678
Validation loss: 2.1878957466412614

Epoch: 5| Step: 4
Training loss: 3.3087592124938965
Validation loss: 2.1933689809614614

Epoch: 5| Step: 5
Training loss: 2.3446526527404785
Validation loss: 2.194729269191783

Epoch: 5| Step: 6
Training loss: 2.115607500076294
Validation loss: 2.1855741957182526

Epoch: 5| Step: 7
Training loss: 2.3877880573272705
Validation loss: 2.1923307834133023

Epoch: 5| Step: 8
Training loss: 2.0547311305999756
Validation loss: 2.20212132956392

Epoch: 5| Step: 9
Training loss: 2.432802438735962
Validation loss: 2.210871955399872

Epoch: 5| Step: 10
Training loss: 2.9463047981262207
Validation loss: 2.22825514629323

Epoch: 333| Step: 0
Training loss: 2.4310460090637207
Validation loss: 2.2378908921313543

Epoch: 5| Step: 1
Training loss: 2.387051582336426
Validation loss: 2.250032768454603

Epoch: 5| Step: 2
Training loss: 2.196596622467041
Validation loss: 2.2485165493462675

Epoch: 5| Step: 3
Training loss: 2.3804237842559814
Validation loss: 2.2400000518368137

Epoch: 5| Step: 4
Training loss: 2.049964427947998
Validation loss: 2.2336856395967546

Epoch: 5| Step: 5
Training loss: 2.395224094390869
Validation loss: 2.218515473027383

Epoch: 5| Step: 6
Training loss: 2.5424001216888428
Validation loss: 2.230268442502586

Epoch: 5| Step: 7
Training loss: 2.2934365272521973
Validation loss: 2.204621443184473

Epoch: 5| Step: 8
Training loss: 2.156982421875
Validation loss: 2.1929279476083736

Epoch: 5| Step: 9
Training loss: 2.7547049522399902
Validation loss: 2.200809150613764

Epoch: 5| Step: 10
Training loss: 2.193202257156372
Validation loss: 2.203991279807142

Epoch: 334| Step: 0
Training loss: 2.578338146209717
Validation loss: 2.2078819556902816

Epoch: 5| Step: 1
Training loss: 2.001779079437256
Validation loss: 2.225557783598541

Epoch: 5| Step: 2
Training loss: 1.8188756704330444
Validation loss: 2.2688858842337005

Epoch: 5| Step: 3
Training loss: 2.608762741088867
Validation loss: 2.260278258272397

Epoch: 5| Step: 4
Training loss: 2.4466071128845215
Validation loss: 2.269918869900447

Epoch: 5| Step: 5
Training loss: 2.0162200927734375
Validation loss: 2.256974907331569

Epoch: 5| Step: 6
Training loss: 3.144360065460205
Validation loss: 2.271736434710923

Epoch: 5| Step: 7
Training loss: 2.253009557723999
Validation loss: 2.270417672331615

Epoch: 5| Step: 8
Training loss: 1.9483158588409424
Validation loss: 2.2782394475834344

Epoch: 5| Step: 9
Training loss: 2.48124623298645
Validation loss: 2.278102567118983

Epoch: 5| Step: 10
Training loss: 2.45107364654541
Validation loss: 2.2729201393742717

Epoch: 335| Step: 0
Training loss: 1.8496687412261963
Validation loss: 2.2760610990626837

Epoch: 5| Step: 1
Training loss: 2.806579113006592
Validation loss: 2.298309269771781

Epoch: 5| Step: 2
Training loss: 2.345818519592285
Validation loss: 2.308259512788506

Epoch: 5| Step: 3
Training loss: 2.4740116596221924
Validation loss: 2.2779585571699243

Epoch: 5| Step: 4
Training loss: 1.5969018936157227
Validation loss: 2.263420589508549

Epoch: 5| Step: 5
Training loss: 2.2325599193573
Validation loss: 2.260577894026233

Epoch: 5| Step: 6
Training loss: 1.6732012033462524
Validation loss: 2.2624014962104058

Epoch: 5| Step: 7
Training loss: 3.111809492111206
Validation loss: 2.2473431120636644

Epoch: 5| Step: 8
Training loss: 2.4795031547546387
Validation loss: 2.2336185350213

Epoch: 5| Step: 9
Training loss: 2.495757579803467
Validation loss: 2.21518349775704

Epoch: 5| Step: 10
Training loss: 2.689634323120117
Validation loss: 2.2115747851710164

Epoch: 336| Step: 0
Training loss: 2.654322862625122
Validation loss: 2.217962790560979

Epoch: 5| Step: 1
Training loss: 2.4200379848480225
Validation loss: 2.207846628722324

Epoch: 5| Step: 2
Training loss: 2.1271872520446777
Validation loss: 2.2226179030633744

Epoch: 5| Step: 3
Training loss: 1.7636150121688843
Validation loss: 2.228137844352312

Epoch: 5| Step: 4
Training loss: 2.2971041202545166
Validation loss: 2.2429964978207826

Epoch: 5| Step: 5
Training loss: 2.511457920074463
Validation loss: 2.2515750854246077

Epoch: 5| Step: 6
Training loss: 2.161343574523926
Validation loss: 2.237884239483905

Epoch: 5| Step: 7
Training loss: 1.9829788208007812
Validation loss: 2.226739900086516

Epoch: 5| Step: 8
Training loss: 2.3562135696411133
Validation loss: 2.2384971854507283

Epoch: 5| Step: 9
Training loss: 2.9739787578582764
Validation loss: 2.2317448931355632

Epoch: 5| Step: 10
Training loss: 2.4137516021728516
Validation loss: 2.2247187270913074

Epoch: 337| Step: 0
Training loss: 2.4721827507019043
Validation loss: 2.227785671910932

Epoch: 5| Step: 1
Training loss: 2.0913994312286377
Validation loss: 2.2325230144685313

Epoch: 5| Step: 2
Training loss: 2.2916696071624756
Validation loss: 2.244107623254099

Epoch: 5| Step: 3
Training loss: 2.7940187454223633
Validation loss: 2.245982402114458

Epoch: 5| Step: 4
Training loss: 2.754944324493408
Validation loss: 2.247002758005614

Epoch: 5| Step: 5
Training loss: 1.5872392654418945
Validation loss: 2.2411689860846407

Epoch: 5| Step: 6
Training loss: 1.9159214496612549
Validation loss: 2.2206520931695097

Epoch: 5| Step: 7
Training loss: 2.704002857208252
Validation loss: 2.234416771960515

Epoch: 5| Step: 8
Training loss: 2.241837739944458
Validation loss: 2.229785878171203

Epoch: 5| Step: 9
Training loss: 2.426351547241211
Validation loss: 2.231032633012341

Epoch: 5| Step: 10
Training loss: 2.348496675491333
Validation loss: 2.2238082270468436

Epoch: 338| Step: 0
Training loss: 2.1873674392700195
Validation loss: 2.22463805701143

Epoch: 5| Step: 1
Training loss: 2.4267494678497314
Validation loss: 2.206449600958055

Epoch: 5| Step: 2
Training loss: 2.367579936981201
Validation loss: 2.226309816042582

Epoch: 5| Step: 3
Training loss: 2.1553235054016113
Validation loss: 2.2082732057058685

Epoch: 5| Step: 4
Training loss: 2.2010951042175293
Validation loss: 2.2190869213432394

Epoch: 5| Step: 5
Training loss: 2.854815721511841
Validation loss: 2.23196070168608

Epoch: 5| Step: 6
Training loss: 2.1578145027160645
Validation loss: 2.250264116512832

Epoch: 5| Step: 7
Training loss: 1.7078815698623657
Validation loss: 2.268044456358879

Epoch: 5| Step: 8
Training loss: 2.890193462371826
Validation loss: 2.2529089438017977

Epoch: 5| Step: 9
Training loss: 2.833054304122925
Validation loss: 2.2555343130583405

Epoch: 5| Step: 10
Training loss: 1.6691431999206543
Validation loss: 2.2474325344126713

Epoch: 339| Step: 0
Training loss: 2.125369071960449
Validation loss: 2.231170205659764

Epoch: 5| Step: 1
Training loss: 1.9371364116668701
Validation loss: 2.221511415255967

Epoch: 5| Step: 2
Training loss: 2.0826730728149414
Validation loss: 2.206687081244684

Epoch: 5| Step: 3
Training loss: 2.901304244995117
Validation loss: 2.2195834395706013

Epoch: 5| Step: 4
Training loss: 2.7962701320648193
Validation loss: 2.2107386717232327

Epoch: 5| Step: 5
Training loss: 2.3419315814971924
Validation loss: 2.2030953643142537

Epoch: 5| Step: 6
Training loss: 1.9505813121795654
Validation loss: 2.2019829160423687

Epoch: 5| Step: 7
Training loss: 2.2432847023010254
Validation loss: 2.2029834601186935

Epoch: 5| Step: 8
Training loss: 2.4057133197784424
Validation loss: 2.2065135496918873

Epoch: 5| Step: 9
Training loss: 2.508225679397583
Validation loss: 2.2087917071516796

Epoch: 5| Step: 10
Training loss: 2.1081318855285645
Validation loss: 2.2212512211133073

Epoch: 340| Step: 0
Training loss: 2.4003422260284424
Validation loss: 2.243124182506274

Epoch: 5| Step: 1
Training loss: 2.1770453453063965
Validation loss: 2.248166673926897

Epoch: 5| Step: 2
Training loss: 2.2234976291656494
Validation loss: 2.2510283711136028

Epoch: 5| Step: 3
Training loss: 3.0201773643493652
Validation loss: 2.254122039323212

Epoch: 5| Step: 4
Training loss: 1.96963369846344
Validation loss: 2.256792711955245

Epoch: 5| Step: 5
Training loss: 1.853271245956421
Validation loss: 2.259567533769915

Epoch: 5| Step: 6
Training loss: 2.3717098236083984
Validation loss: 2.2489634816364577

Epoch: 5| Step: 7
Training loss: 2.485210418701172
Validation loss: 2.2267608693850938

Epoch: 5| Step: 8
Training loss: 2.360269546508789
Validation loss: 2.221554098590728

Epoch: 5| Step: 9
Training loss: 2.2047030925750732
Validation loss: 2.200165940869239

Epoch: 5| Step: 10
Training loss: 2.376326322555542
Validation loss: 2.181252802571943

Epoch: 341| Step: 0
Training loss: 3.19010591506958
Validation loss: 2.1812869912834576

Epoch: 5| Step: 1
Training loss: 2.222414970397949
Validation loss: 2.1717215994352936

Epoch: 5| Step: 2
Training loss: 2.0796866416931152
Validation loss: 2.1727181224412817

Epoch: 5| Step: 3
Training loss: 2.3261475563049316
Validation loss: 2.174578642332426

Epoch: 5| Step: 4
Training loss: 2.465610980987549
Validation loss: 2.170320701855485

Epoch: 5| Step: 5
Training loss: 2.0430188179016113
Validation loss: 2.162832926678401

Epoch: 5| Step: 6
Training loss: 2.314455509185791
Validation loss: 2.170426992959874

Epoch: 5| Step: 7
Training loss: 1.8613412380218506
Validation loss: 2.1963988068283244

Epoch: 5| Step: 8
Training loss: 2.1735680103302
Validation loss: 2.2381819268708587

Epoch: 5| Step: 9
Training loss: 2.4342501163482666
Validation loss: 2.2683107724753757

Epoch: 5| Step: 10
Training loss: 2.401049852371216
Validation loss: 2.2846368769163727

Epoch: 342| Step: 0
Training loss: 1.858633041381836
Validation loss: 2.3169904139734085

Epoch: 5| Step: 1
Training loss: 1.96023690700531
Validation loss: 2.3449017873374363

Epoch: 5| Step: 2
Training loss: 2.625089645385742
Validation loss: 2.3630708520130446

Epoch: 5| Step: 3
Training loss: 2.2474567890167236
Validation loss: 2.358327806636851

Epoch: 5| Step: 4
Training loss: 2.1851494312286377
Validation loss: 2.319411047043339

Epoch: 5| Step: 5
Training loss: 2.606480360031128
Validation loss: 2.2980989435667634

Epoch: 5| Step: 6
Training loss: 2.3083252906799316
Validation loss: 2.2721544042710335

Epoch: 5| Step: 7
Training loss: 2.0976061820983887
Validation loss: 2.251468522574312

Epoch: 5| Step: 8
Training loss: 2.4869284629821777
Validation loss: 2.228861324248775

Epoch: 5| Step: 9
Training loss: 2.5418782234191895
Validation loss: 2.223331594979891

Epoch: 5| Step: 10
Training loss: 2.867042064666748
Validation loss: 2.19632432024966

Epoch: 343| Step: 0
Training loss: 3.0405490398406982
Validation loss: 2.168772168056939

Epoch: 5| Step: 1
Training loss: 2.615588665008545
Validation loss: 2.184922952805796

Epoch: 5| Step: 2
Training loss: 1.969488501548767
Validation loss: 2.1748345898043726

Epoch: 5| Step: 3
Training loss: 2.027355670928955
Validation loss: 2.182613362548172

Epoch: 5| Step: 4
Training loss: 1.6707305908203125
Validation loss: 2.191419155366959

Epoch: 5| Step: 5
Training loss: 2.784829616546631
Validation loss: 2.199912917229437

Epoch: 5| Step: 6
Training loss: 2.49654483795166
Validation loss: 2.220068088141821

Epoch: 5| Step: 7
Training loss: 2.459092855453491
Validation loss: 2.23993908333522

Epoch: 5| Step: 8
Training loss: 2.2904152870178223
Validation loss: 2.249534865861298

Epoch: 5| Step: 9
Training loss: 2.076582908630371
Validation loss: 2.2593217844604165

Epoch: 5| Step: 10
Training loss: 2.271939516067505
Validation loss: 2.281361195348924

Epoch: 344| Step: 0
Training loss: 2.3628861904144287
Validation loss: 2.2639130084745345

Epoch: 5| Step: 1
Training loss: 1.856797218322754
Validation loss: 2.271191347029901

Epoch: 5| Step: 2
Training loss: 1.759219765663147
Validation loss: 2.274222521371739

Epoch: 5| Step: 3
Training loss: 2.507216691970825
Validation loss: 2.2589064785229263

Epoch: 5| Step: 4
Training loss: 2.4795913696289062
Validation loss: 2.2571646974932764

Epoch: 5| Step: 5
Training loss: 2.41283917427063
Validation loss: 2.242945650572418

Epoch: 5| Step: 6
Training loss: 2.706874132156372
Validation loss: 2.2347005272424347

Epoch: 5| Step: 7
Training loss: 1.7059762477874756
Validation loss: 2.2276602598928634

Epoch: 5| Step: 8
Training loss: 2.683699131011963
Validation loss: 2.1970406475887505

Epoch: 5| Step: 9
Training loss: 2.08180570602417
Validation loss: 2.1919066188155965

Epoch: 5| Step: 10
Training loss: 2.7775919437408447
Validation loss: 2.180443427895987

Epoch: 345| Step: 0
Training loss: 2.4148430824279785
Validation loss: 2.1848678383775937

Epoch: 5| Step: 1
Training loss: 2.794933557510376
Validation loss: 2.180926187064058

Epoch: 5| Step: 2
Training loss: 1.7530171871185303
Validation loss: 2.176707047288136

Epoch: 5| Step: 3
Training loss: 2.4050967693328857
Validation loss: 2.178867120896616

Epoch: 5| Step: 4
Training loss: 2.2787086963653564
Validation loss: 2.185143316945722

Epoch: 5| Step: 5
Training loss: 2.2513084411621094
Validation loss: 2.1929864614240584

Epoch: 5| Step: 6
Training loss: 2.2291030883789062
Validation loss: 2.1973391271406606

Epoch: 5| Step: 7
Training loss: 2.112941026687622
Validation loss: 2.211781886316115

Epoch: 5| Step: 8
Training loss: 2.673487663269043
Validation loss: 2.225986667858657

Epoch: 5| Step: 9
Training loss: 2.391845941543579
Validation loss: 2.2479855604069208

Epoch: 5| Step: 10
Training loss: 2.0036470890045166
Validation loss: 2.2541393080065326

Epoch: 346| Step: 0
Training loss: 1.776368498802185
Validation loss: 2.2876658567818264

Epoch: 5| Step: 1
Training loss: 2.4304842948913574
Validation loss: 2.299671167968422

Epoch: 5| Step: 2
Training loss: 2.063291311264038
Validation loss: 2.283040485074443

Epoch: 5| Step: 3
Training loss: 1.7240701913833618
Validation loss: 2.255685415319217

Epoch: 5| Step: 4
Training loss: 2.6207873821258545
Validation loss: 2.2379332947474655

Epoch: 5| Step: 5
Training loss: 2.257103204727173
Validation loss: 2.216681093297979

Epoch: 5| Step: 6
Training loss: 2.026231288909912
Validation loss: 2.2096506793011903

Epoch: 5| Step: 7
Training loss: 2.590402126312256
Validation loss: 2.226396086395428

Epoch: 5| Step: 8
Training loss: 2.8951635360717773
Validation loss: 2.224917751486583

Epoch: 5| Step: 9
Training loss: 2.7262797355651855
Validation loss: 2.2369975479700233

Epoch: 5| Step: 10
Training loss: 2.3523573875427246
Validation loss: 2.2193871313525784

Epoch: 347| Step: 0
Training loss: 2.2400479316711426
Validation loss: 2.2244795701837026

Epoch: 5| Step: 1
Training loss: 1.8764415979385376
Validation loss: 2.2470501110117924

Epoch: 5| Step: 2
Training loss: 2.636444330215454
Validation loss: 2.2620885602889524

Epoch: 5| Step: 3
Training loss: 2.429596185684204
Validation loss: 2.2751972675323486

Epoch: 5| Step: 4
Training loss: 2.432544708251953
Validation loss: 2.2843776415753108

Epoch: 5| Step: 5
Training loss: 2.388639211654663
Validation loss: 2.3108957377813195

Epoch: 5| Step: 6
Training loss: 2.451214075088501
Validation loss: 2.3142108814690703

Epoch: 5| Step: 7
Training loss: 2.272552251815796
Validation loss: 2.297468444352509

Epoch: 5| Step: 8
Training loss: 2.3103790283203125
Validation loss: 2.2800065266188754

Epoch: 5| Step: 9
Training loss: 2.2186689376831055
Validation loss: 2.25439795883753

Epoch: 5| Step: 10
Training loss: 2.279874801635742
Validation loss: 2.220964293326101

Epoch: 348| Step: 0
Training loss: 2.4573183059692383
Validation loss: 2.203694433294317

Epoch: 5| Step: 1
Training loss: 2.4974188804626465
Validation loss: 2.1911865870157876

Epoch: 5| Step: 2
Training loss: 2.594435214996338
Validation loss: 2.196625769779246

Epoch: 5| Step: 3
Training loss: 1.5350561141967773
Validation loss: 2.1879546193666357

Epoch: 5| Step: 4
Training loss: 2.2666592597961426
Validation loss: 2.1870994567871094

Epoch: 5| Step: 5
Training loss: 2.181328058242798
Validation loss: 2.1880362520935717

Epoch: 5| Step: 6
Training loss: 2.4122586250305176
Validation loss: 2.1945193147146576

Epoch: 5| Step: 7
Training loss: 2.9756484031677246
Validation loss: 2.2094705720101633

Epoch: 5| Step: 8
Training loss: 2.0382602214813232
Validation loss: 2.2165311921027397

Epoch: 5| Step: 9
Training loss: 2.3465771675109863
Validation loss: 2.2401021039614113

Epoch: 5| Step: 10
Training loss: 2.0937962532043457
Validation loss: 2.250272827763711

Epoch: 349| Step: 0
Training loss: 2.424870729446411
Validation loss: 2.266723011129646

Epoch: 5| Step: 1
Training loss: 2.6827452182769775
Validation loss: 2.2903789243390484

Epoch: 5| Step: 2
Training loss: 2.099714756011963
Validation loss: 2.28415822726424

Epoch: 5| Step: 3
Training loss: 2.379567861557007
Validation loss: 2.2826364066011164

Epoch: 5| Step: 4
Training loss: 2.5562641620635986
Validation loss: 2.259528734350717

Epoch: 5| Step: 5
Training loss: 2.6303374767303467
Validation loss: 2.256838829286637

Epoch: 5| Step: 6
Training loss: 2.561298370361328
Validation loss: 2.250767646297332

Epoch: 5| Step: 7
Training loss: 1.682076096534729
Validation loss: 2.2329460036370063

Epoch: 5| Step: 8
Training loss: 2.183135986328125
Validation loss: 2.227359861455938

Epoch: 5| Step: 9
Training loss: 2.219876766204834
Validation loss: 2.2146802922730804

Epoch: 5| Step: 10
Training loss: 1.9793157577514648
Validation loss: 2.197201653193402

Epoch: 350| Step: 0
Training loss: 2.509580135345459
Validation loss: 2.1954981691093853

Epoch: 5| Step: 1
Training loss: 2.4974331855773926
Validation loss: 2.1853385522801387

Epoch: 5| Step: 2
Training loss: 2.1333885192871094
Validation loss: 2.1972183719758065

Epoch: 5| Step: 3
Training loss: 1.2625820636749268
Validation loss: 2.21156866704264

Epoch: 5| Step: 4
Training loss: 2.2793126106262207
Validation loss: 2.2286271408040035

Epoch: 5| Step: 5
Training loss: 2.1799397468566895
Validation loss: 2.238816338200723

Epoch: 5| Step: 6
Training loss: 2.3358445167541504
Validation loss: 2.254150567516204

Epoch: 5| Step: 7
Training loss: 2.862628221511841
Validation loss: 2.2840665745478805

Epoch: 5| Step: 8
Training loss: 2.131289005279541
Validation loss: 2.2714107959501204

Epoch: 5| Step: 9
Training loss: 2.7848994731903076
Validation loss: 2.2652633574701126

Epoch: 5| Step: 10
Training loss: 2.3332736492156982
Validation loss: 2.2618034142319874

Testing loss: 2.3233075539271035

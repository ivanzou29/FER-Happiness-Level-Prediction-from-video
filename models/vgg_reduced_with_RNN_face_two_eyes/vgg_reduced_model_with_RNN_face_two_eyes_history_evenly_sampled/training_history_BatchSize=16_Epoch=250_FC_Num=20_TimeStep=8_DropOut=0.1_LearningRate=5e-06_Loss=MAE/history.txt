Epoch: 1| Step: 0
Training loss: 4.729828357696533
Validation loss: 5.135648265961678

Epoch: 6| Step: 1
Training loss: 5.994421005249023
Validation loss: 5.131245243933893

Epoch: 6| Step: 2
Training loss: 4.669580936431885
Validation loss: 5.126882588991555

Epoch: 6| Step: 3
Training loss: 3.299269914627075
Validation loss: 5.1224941797153924

Epoch: 6| Step: 4
Training loss: 5.51264762878418
Validation loss: 5.118021052370789

Epoch: 6| Step: 5
Training loss: 5.92149019241333
Validation loss: 5.113722175680181

Epoch: 6| Step: 6
Training loss: 5.332058429718018
Validation loss: 5.109065835193921

Epoch: 6| Step: 7
Training loss: 5.045877456665039
Validation loss: 5.104628383472401

Epoch: 6| Step: 8
Training loss: 5.049966335296631
Validation loss: 5.09978247714299

Epoch: 6| Step: 9
Training loss: 4.903962135314941
Validation loss: 5.094790643261325

Epoch: 6| Step: 10
Training loss: 4.675047874450684
Validation loss: 5.0897324315963255

Epoch: 6| Step: 11
Training loss: 3.9265506267547607
Validation loss: 5.084502932845905

Epoch: 6| Step: 12
Training loss: 5.08901309967041
Validation loss: 5.078953589162519

Epoch: 6| Step: 13
Training loss: 4.171427249908447
Validation loss: 5.072979762989988

Epoch: 2| Step: 0
Training loss: 4.836935043334961
Validation loss: 5.066789134856193

Epoch: 6| Step: 1
Training loss: 4.989831924438477
Validation loss: 5.060446329014276

Epoch: 6| Step: 2
Training loss: 4.217134952545166
Validation loss: 5.05410094927716

Epoch: 6| Step: 3
Training loss: 4.1873884201049805
Validation loss: 5.046673010754329

Epoch: 6| Step: 4
Training loss: 5.524749755859375
Validation loss: 5.0394577518586186

Epoch: 6| Step: 5
Training loss: 5.1449294090271
Validation loss: 5.031536712441393

Epoch: 6| Step: 6
Training loss: 3.4695706367492676
Validation loss: 5.02367825149208

Epoch: 6| Step: 7
Training loss: 5.201987266540527
Validation loss: 5.01529384428455

Epoch: 6| Step: 8
Training loss: 4.743715763092041
Validation loss: 5.0062851444367436

Epoch: 6| Step: 9
Training loss: 4.710939407348633
Validation loss: 4.996605380888908

Epoch: 6| Step: 10
Training loss: 4.060914039611816
Validation loss: 4.986637769206878

Epoch: 6| Step: 11
Training loss: 4.514240741729736
Validation loss: 4.976936207022718

Epoch: 6| Step: 12
Training loss: 6.429381847381592
Validation loss: 4.9655273601573

Epoch: 6| Step: 13
Training loss: 5.745720386505127
Validation loss: 4.955193816974599

Epoch: 3| Step: 0
Training loss: 6.3430681228637695
Validation loss: 4.943153273674749

Epoch: 6| Step: 1
Training loss: 5.024524211883545
Validation loss: 4.930814661005492

Epoch: 6| Step: 2
Training loss: 4.007935523986816
Validation loss: 4.917591864062894

Epoch: 6| Step: 3
Training loss: 4.236446380615234
Validation loss: 4.90431192357053

Epoch: 6| Step: 4
Training loss: 4.603246688842773
Validation loss: 4.890137974933912

Epoch: 6| Step: 5
Training loss: 4.137827396392822
Validation loss: 4.876170263495497

Epoch: 6| Step: 6
Training loss: 4.20074462890625
Validation loss: 4.860871735439505

Epoch: 6| Step: 7
Training loss: 3.3041367530822754
Validation loss: 4.846070781830819

Epoch: 6| Step: 8
Training loss: 5.024863243103027
Validation loss: 4.828781876512753

Epoch: 6| Step: 9
Training loss: 5.490307807922363
Validation loss: 4.812591829607563

Epoch: 6| Step: 10
Training loss: 4.910480499267578
Validation loss: 4.794603665669759

Epoch: 6| Step: 11
Training loss: 4.0955915451049805
Validation loss: 4.77663940511724

Epoch: 6| Step: 12
Training loss: 5.11078405380249
Validation loss: 4.758422146561325

Epoch: 6| Step: 13
Training loss: 4.347870349884033
Validation loss: 4.738596582925448

Epoch: 4| Step: 0
Training loss: 3.541936159133911
Validation loss: 4.719562269026233

Epoch: 6| Step: 1
Training loss: 4.913629531860352
Validation loss: 4.6991970821093485

Epoch: 6| Step: 2
Training loss: 4.605945110321045
Validation loss: 4.680088279067829

Epoch: 6| Step: 3
Training loss: 5.208954334259033
Validation loss: 4.658439482412031

Epoch: 6| Step: 4
Training loss: 3.4625742435455322
Validation loss: 4.637105726426648

Epoch: 6| Step: 5
Training loss: 3.6407060623168945
Validation loss: 4.614643189214891

Epoch: 6| Step: 6
Training loss: 3.821239471435547
Validation loss: 4.591726754301337

Epoch: 6| Step: 7
Training loss: 4.975968360900879
Validation loss: 4.569073051534673

Epoch: 6| Step: 8
Training loss: 4.1369218826293945
Validation loss: 4.547775283936532

Epoch: 6| Step: 9
Training loss: 4.0032172203063965
Validation loss: 4.523593169386669

Epoch: 6| Step: 10
Training loss: 5.2495269775390625
Validation loss: 4.5016817174932005

Epoch: 6| Step: 11
Training loss: 5.078089237213135
Validation loss: 4.4788496673748055

Epoch: 6| Step: 12
Training loss: 3.8480777740478516
Validation loss: 4.456051267603392

Epoch: 6| Step: 13
Training loss: 4.9540557861328125
Validation loss: 4.432718697414603

Epoch: 5| Step: 0
Training loss: 4.221994876861572
Validation loss: 4.410677033085977

Epoch: 6| Step: 1
Training loss: 3.0502023696899414
Validation loss: 4.386888739883259

Epoch: 6| Step: 2
Training loss: 4.446199417114258
Validation loss: 4.366189682355491

Epoch: 6| Step: 3
Training loss: 3.8391387462615967
Validation loss: 4.342940066450385

Epoch: 6| Step: 4
Training loss: 4.259116172790527
Validation loss: 4.322956572296799

Epoch: 6| Step: 5
Training loss: 4.45594596862793
Validation loss: 4.302517608929706

Epoch: 6| Step: 6
Training loss: 4.3891119956970215
Validation loss: 4.28186164363738

Epoch: 6| Step: 7
Training loss: 4.491755485534668
Validation loss: 4.263319438503634

Epoch: 6| Step: 8
Training loss: 3.7536392211914062
Validation loss: 4.242524244452036

Epoch: 6| Step: 9
Training loss: 4.615789890289307
Validation loss: 4.224118930037304

Epoch: 6| Step: 10
Training loss: 4.2602858543396
Validation loss: 4.205019058719758

Epoch: 6| Step: 11
Training loss: 4.5455193519592285
Validation loss: 4.186381745082076

Epoch: 6| Step: 12
Training loss: 3.2528176307678223
Validation loss: 4.167569160461426

Epoch: 6| Step: 13
Training loss: 3.2731637954711914
Validation loss: 4.146941538779966

Epoch: 6| Step: 0
Training loss: 4.152257919311523
Validation loss: 4.126779956202353

Epoch: 6| Step: 1
Training loss: 3.8731915950775146
Validation loss: 4.107354148741691

Epoch: 6| Step: 2
Training loss: 3.9562880992889404
Validation loss: 4.0855417225950506

Epoch: 6| Step: 3
Training loss: 3.600451707839966
Validation loss: 4.063299455950337

Epoch: 6| Step: 4
Training loss: 4.553461074829102
Validation loss: 4.041930260196809

Epoch: 6| Step: 5
Training loss: 2.6331467628479004
Validation loss: 4.0203145678325365

Epoch: 6| Step: 6
Training loss: 4.286962985992432
Validation loss: 3.9994993158566055

Epoch: 6| Step: 7
Training loss: 5.939302444458008
Validation loss: 3.978045061070432

Epoch: 6| Step: 8
Training loss: 3.2607169151306152
Validation loss: 3.9570805436821392

Epoch: 6| Step: 9
Training loss: 3.649728298187256
Validation loss: 3.9361816862578034

Epoch: 6| Step: 10
Training loss: 4.086833953857422
Validation loss: 3.917223330466978

Epoch: 6| Step: 11
Training loss: 3.541393280029297
Validation loss: 3.901873911580732

Epoch: 6| Step: 12
Training loss: 3.4154138565063477
Validation loss: 3.8867046294673795

Epoch: 6| Step: 13
Training loss: 2.129467248916626
Validation loss: 3.872062257541123

Epoch: 7| Step: 0
Training loss: 3.3808910846710205
Validation loss: 3.8584595034199376

Epoch: 6| Step: 1
Training loss: 2.8591132164001465
Validation loss: 3.843544816458097

Epoch: 6| Step: 2
Training loss: 3.786119222640991
Validation loss: 3.83083317869453

Epoch: 6| Step: 3
Training loss: 3.7988555431365967
Validation loss: 3.819639990406652

Epoch: 6| Step: 4
Training loss: 3.2901852130889893
Validation loss: 3.802091552365211

Epoch: 6| Step: 5
Training loss: 3.6005892753601074
Validation loss: 3.7899989389604136

Epoch: 6| Step: 6
Training loss: 4.272616386413574
Validation loss: 3.7764844509863083

Epoch: 6| Step: 7
Training loss: 3.7018465995788574
Validation loss: 3.7604336687313613

Epoch: 6| Step: 8
Training loss: 3.2414400577545166
Validation loss: 3.7482004986014417

Epoch: 6| Step: 9
Training loss: 3.0366768836975098
Validation loss: 3.7346295259332143

Epoch: 6| Step: 10
Training loss: 4.047392845153809
Validation loss: 3.722179784569689

Epoch: 6| Step: 11
Training loss: 3.8489248752593994
Validation loss: 3.7100073829773934

Epoch: 6| Step: 12
Training loss: 4.1343092918396
Validation loss: 3.696770170683502

Epoch: 6| Step: 13
Training loss: 4.544244289398193
Validation loss: 3.683101092615435

Epoch: 8| Step: 0
Training loss: 3.5380218029022217
Validation loss: 3.670658529445689

Epoch: 6| Step: 1
Training loss: 4.034413814544678
Validation loss: 3.659223074554115

Epoch: 6| Step: 2
Training loss: 3.6715941429138184
Validation loss: 3.6463273084291847

Epoch: 6| Step: 3
Training loss: 2.9113428592681885
Validation loss: 3.634985170056743

Epoch: 6| Step: 4
Training loss: 2.596848964691162
Validation loss: 3.623039425060313

Epoch: 6| Step: 5
Training loss: 2.801003932952881
Validation loss: 3.6145163864217777

Epoch: 6| Step: 6
Training loss: 2.696976661682129
Validation loss: 3.6018059279329036

Epoch: 6| Step: 7
Training loss: 4.373003005981445
Validation loss: 3.5927204701208297

Epoch: 6| Step: 8
Training loss: 5.446800231933594
Validation loss: 3.5815689256114345

Epoch: 6| Step: 9
Training loss: 4.291825771331787
Validation loss: 3.5715786769825923

Epoch: 6| Step: 10
Training loss: 3.547278881072998
Validation loss: 3.5615987675164336

Epoch: 6| Step: 11
Training loss: 2.5975255966186523
Validation loss: 3.5527308781941733

Epoch: 6| Step: 12
Training loss: 3.55517840385437
Validation loss: 3.5429954169898905

Epoch: 6| Step: 13
Training loss: 2.8481481075286865
Validation loss: 3.5342912238131285

Epoch: 9| Step: 0
Training loss: 3.463141918182373
Validation loss: 3.523975823515205

Epoch: 6| Step: 1
Training loss: 2.380955696105957
Validation loss: 3.5169567882373767

Epoch: 6| Step: 2
Training loss: 4.157650470733643
Validation loss: 3.507749767713649

Epoch: 6| Step: 3
Training loss: 4.508936882019043
Validation loss: 3.5005314350128174

Epoch: 6| Step: 4
Training loss: 2.6553142070770264
Validation loss: 3.4904753931107058

Epoch: 6| Step: 5
Training loss: 3.782275438308716
Validation loss: 3.4839876646636636

Epoch: 6| Step: 6
Training loss: 3.776459217071533
Validation loss: 3.475641268555836

Epoch: 6| Step: 7
Training loss: 3.878466844558716
Validation loss: 3.4666490811173634

Epoch: 6| Step: 8
Training loss: 3.4146804809570312
Validation loss: 3.4590283363096175

Epoch: 6| Step: 9
Training loss: 3.3384811878204346
Validation loss: 3.450236233331824

Epoch: 6| Step: 10
Training loss: 3.8829784393310547
Validation loss: 3.4447853795943724

Epoch: 6| Step: 11
Training loss: 3.287899971008301
Validation loss: 3.4366216377545427

Epoch: 6| Step: 12
Training loss: 2.931978702545166
Validation loss: 3.4309450400772916

Epoch: 6| Step: 13
Training loss: 1.495309591293335
Validation loss: 3.422662914440196

Epoch: 10| Step: 0
Training loss: 3.8484768867492676
Validation loss: 3.418557023489347

Epoch: 6| Step: 1
Training loss: 2.651245594024658
Validation loss: 3.4147065583095757

Epoch: 6| Step: 2
Training loss: 3.973355293273926
Validation loss: 3.4102916717529297

Epoch: 6| Step: 3
Training loss: 4.215076923370361
Validation loss: 3.4050572226124425

Epoch: 6| Step: 4
Training loss: 3.232058525085449
Validation loss: 3.398177857040077

Epoch: 6| Step: 5
Training loss: 3.097485065460205
Validation loss: 3.394908361537482

Epoch: 6| Step: 6
Training loss: 2.3676648139953613
Validation loss: 3.3908970689260833

Epoch: 6| Step: 7
Training loss: 3.7362051010131836
Validation loss: 3.3848054280845066

Epoch: 6| Step: 8
Training loss: 3.7879981994628906
Validation loss: 3.378403986653974

Epoch: 6| Step: 9
Training loss: 3.57782244682312
Validation loss: 3.3693563758686023

Epoch: 6| Step: 10
Training loss: 3.4316844940185547
Validation loss: 3.3654411351808937

Epoch: 6| Step: 11
Training loss: 3.2303690910339355
Validation loss: 3.3587860599640877

Epoch: 6| Step: 12
Training loss: 2.5643959045410156
Validation loss: 3.349773847928611

Epoch: 6| Step: 13
Training loss: 2.817749500274658
Validation loss: 3.343956026979672

Epoch: 11| Step: 0
Training loss: 4.157634735107422
Validation loss: 3.339554727718394

Epoch: 6| Step: 1
Training loss: 4.080819606781006
Validation loss: 3.3340944423470447

Epoch: 6| Step: 2
Training loss: 2.939582347869873
Validation loss: 3.327992421324535

Epoch: 6| Step: 3
Training loss: 2.737748622894287
Validation loss: 3.32061517366799

Epoch: 6| Step: 4
Training loss: 3.04194974899292
Validation loss: 3.3143576806591404

Epoch: 6| Step: 5
Training loss: 3.378291130065918
Validation loss: 3.3092037170164046

Epoch: 6| Step: 6
Training loss: 3.0502352714538574
Validation loss: 3.305440959110055

Epoch: 6| Step: 7
Training loss: 3.565242290496826
Validation loss: 3.296979570901522

Epoch: 6| Step: 8
Training loss: 3.669071674346924
Validation loss: 3.2902245188272126

Epoch: 6| Step: 9
Training loss: 2.423672914505005
Validation loss: 3.285963245617446

Epoch: 6| Step: 10
Training loss: 4.181944847106934
Validation loss: 3.278282332163985

Epoch: 6| Step: 11
Training loss: 3.199836492538452
Validation loss: 3.274465976222869

Epoch: 6| Step: 12
Training loss: 2.878920793533325
Validation loss: 3.270917243854974

Epoch: 6| Step: 13
Training loss: 1.9832221269607544
Validation loss: 3.2638350353446057

Epoch: 12| Step: 0
Training loss: 3.389897346496582
Validation loss: 3.2597404782490065

Epoch: 6| Step: 1
Training loss: 3.2526283264160156
Validation loss: 3.255693430541664

Epoch: 6| Step: 2
Training loss: 2.5517406463623047
Validation loss: 3.252871682566981

Epoch: 6| Step: 3
Training loss: 3.1322429180145264
Validation loss: 3.248389877298827

Epoch: 6| Step: 4
Training loss: 2.0608997344970703
Validation loss: 3.2445318416882585

Epoch: 6| Step: 5
Training loss: 2.1160120964050293
Validation loss: 3.2404821713765464

Epoch: 6| Step: 6
Training loss: 4.012703895568848
Validation loss: 3.2353033814378964

Epoch: 6| Step: 7
Training loss: 3.109191417694092
Validation loss: 3.2321861943890973

Epoch: 6| Step: 8
Training loss: 2.8736143112182617
Validation loss: 3.22793064835251

Epoch: 6| Step: 9
Training loss: 3.799234390258789
Validation loss: 3.222776238636304

Epoch: 6| Step: 10
Training loss: 3.9866116046905518
Validation loss: 3.218434541456161

Epoch: 6| Step: 11
Training loss: 3.6073379516601562
Validation loss: 3.215184768040975

Epoch: 6| Step: 12
Training loss: 3.7170135974884033
Validation loss: 3.2107111202773226

Epoch: 6| Step: 13
Training loss: 3.6610450744628906
Validation loss: 3.2040483208112818

Epoch: 13| Step: 0
Training loss: 5.164353370666504
Validation loss: 3.1985163201567945

Epoch: 6| Step: 1
Training loss: 3.2777981758117676
Validation loss: 3.197560848728303

Epoch: 6| Step: 2
Training loss: 2.594897508621216
Validation loss: 3.194159097568963

Epoch: 6| Step: 3
Training loss: 3.54913330078125
Validation loss: 3.19029563729481

Epoch: 6| Step: 4
Training loss: 1.8351300954818726
Validation loss: 3.187694780288204

Epoch: 6| Step: 5
Training loss: 3.1697463989257812
Validation loss: 3.1823526864410727

Epoch: 6| Step: 6
Training loss: 3.438504934310913
Validation loss: 3.1779243689711376

Epoch: 6| Step: 7
Training loss: 3.4900622367858887
Validation loss: 3.1734076546084498

Epoch: 6| Step: 8
Training loss: 2.9996235370635986
Validation loss: 3.1689186250009844

Epoch: 6| Step: 9
Training loss: 2.2679283618927
Validation loss: 3.166322277438256

Epoch: 6| Step: 10
Training loss: 2.782960891723633
Validation loss: 3.1626965538147958

Epoch: 6| Step: 11
Training loss: 2.622265338897705
Validation loss: 3.159123864225162

Epoch: 6| Step: 12
Training loss: 4.162666320800781
Validation loss: 3.157084157389979

Epoch: 6| Step: 13
Training loss: 3.1912646293640137
Validation loss: 3.1533681833615868

Epoch: 14| Step: 0
Training loss: 3.299112319946289
Validation loss: 3.1482311346197642

Epoch: 6| Step: 1
Training loss: 3.580099105834961
Validation loss: 3.146164304466658

Epoch: 6| Step: 2
Training loss: 2.5235438346862793
Validation loss: 3.1439901321165022

Epoch: 6| Step: 3
Training loss: 3.6878459453582764
Validation loss: 3.1423030412325295

Epoch: 6| Step: 4
Training loss: 3.3328280448913574
Validation loss: 3.1380567294295116

Epoch: 6| Step: 5
Training loss: 3.093174457550049
Validation loss: 3.1341089638330604

Epoch: 6| Step: 6
Training loss: 3.7000319957733154
Validation loss: 3.1301511231289116

Epoch: 6| Step: 7
Training loss: 2.7695300579071045
Validation loss: 3.127288833741219

Epoch: 6| Step: 8
Training loss: 2.905592918395996
Validation loss: 3.126961502977597

Epoch: 6| Step: 9
Training loss: 4.038637161254883
Validation loss: 3.125018117248371

Epoch: 6| Step: 10
Training loss: 2.538679599761963
Validation loss: 3.11934160160762

Epoch: 6| Step: 11
Training loss: 3.1525299549102783
Validation loss: 3.114180334152714

Epoch: 6| Step: 12
Training loss: 2.9989659786224365
Validation loss: 3.1112489700317383

Epoch: 6| Step: 13
Training loss: 1.8914896249771118
Validation loss: 3.1107685514675674

Epoch: 15| Step: 0
Training loss: 3.3646240234375
Validation loss: 3.106991767883301

Epoch: 6| Step: 1
Training loss: 2.6122937202453613
Validation loss: 3.1057864645475983

Epoch: 6| Step: 2
Training loss: 3.3911187648773193
Validation loss: 3.102070052136657

Epoch: 6| Step: 3
Training loss: 3.3954203128814697
Validation loss: 3.104050692691598

Epoch: 6| Step: 4
Training loss: 2.3467373847961426
Validation loss: 3.102425349655972

Epoch: 6| Step: 5
Training loss: 2.9515647888183594
Validation loss: 3.1014330310206257

Epoch: 6| Step: 6
Training loss: 2.4387006759643555
Validation loss: 3.0959978513820197

Epoch: 6| Step: 7
Training loss: 4.1868438720703125
Validation loss: 3.0930191214366625

Epoch: 6| Step: 8
Training loss: 2.7965030670166016
Validation loss: 3.084494816359653

Epoch: 6| Step: 9
Training loss: 2.8109183311462402
Validation loss: 3.0805750867371917

Epoch: 6| Step: 10
Training loss: 3.838564157485962
Validation loss: 3.0820893984968945

Epoch: 6| Step: 11
Training loss: 3.849102258682251
Validation loss: 3.0781255178554083

Epoch: 6| Step: 12
Training loss: 2.7183008193969727
Validation loss: 3.0711949973978023

Epoch: 6| Step: 13
Training loss: 2.9805688858032227
Validation loss: 3.070464708471811

Epoch: 16| Step: 0
Training loss: 2.548332452774048
Validation loss: 3.0714639976460445

Epoch: 6| Step: 1
Training loss: 3.0022854804992676
Validation loss: 3.0736917423945602

Epoch: 6| Step: 2
Training loss: 3.123075008392334
Validation loss: 3.074204893522365

Epoch: 6| Step: 3
Training loss: 3.4100427627563477
Validation loss: 3.0770492502438125

Epoch: 6| Step: 4
Training loss: 3.769562244415283
Validation loss: 3.0666657468324066

Epoch: 6| Step: 5
Training loss: 2.6257693767547607
Validation loss: 3.061601895158009

Epoch: 6| Step: 6
Training loss: 3.424865484237671
Validation loss: 3.058272079754901

Epoch: 6| Step: 7
Training loss: 2.0182044506073
Validation loss: 3.051685728052611

Epoch: 6| Step: 8
Training loss: 2.753330707550049
Validation loss: 3.0440672879577964

Epoch: 6| Step: 9
Training loss: 3.079477548599243
Validation loss: 3.0391114629724973

Epoch: 6| Step: 10
Training loss: 3.4726755619049072
Validation loss: 3.034487544849355

Epoch: 6| Step: 11
Training loss: 3.2337594032287598
Validation loss: 3.0335364085371777

Epoch: 6| Step: 12
Training loss: 3.80842661857605
Validation loss: 3.029186112906343

Epoch: 6| Step: 13
Training loss: 3.0569190979003906
Validation loss: 3.0270836045665126

Epoch: 17| Step: 0
Training loss: 2.6597681045532227
Validation loss: 3.029599371776786

Epoch: 6| Step: 1
Training loss: 3.421599864959717
Validation loss: 3.0225449685127503

Epoch: 6| Step: 2
Training loss: 3.0138819217681885
Validation loss: 3.0165162112123225

Epoch: 6| Step: 3
Training loss: 3.7871623039245605
Validation loss: 3.013786208245062

Epoch: 6| Step: 4
Training loss: 4.136010646820068
Validation loss: 3.0131404758781515

Epoch: 6| Step: 5
Training loss: 3.166482448577881
Validation loss: 3.008621282474969

Epoch: 6| Step: 6
Training loss: 2.000239133834839
Validation loss: 3.002085357583979

Epoch: 6| Step: 7
Training loss: 3.642575263977051
Validation loss: 3.00048126200194

Epoch: 6| Step: 8
Training loss: 2.8455650806427
Validation loss: 2.9970718378661783

Epoch: 6| Step: 9
Training loss: 2.958759307861328
Validation loss: 2.9908668405266217

Epoch: 6| Step: 10
Training loss: 2.492229461669922
Validation loss: 2.9901600345488517

Epoch: 6| Step: 11
Training loss: 3.274883985519409
Validation loss: 2.9840406730610836

Epoch: 6| Step: 12
Training loss: 2.7916340827941895
Validation loss: 2.984755193033526

Epoch: 6| Step: 13
Training loss: 2.4252750873565674
Validation loss: 2.979666899609309

Epoch: 18| Step: 0
Training loss: 2.703720808029175
Validation loss: 2.9753264611767185

Epoch: 6| Step: 1
Training loss: 3.0377535820007324
Validation loss: 2.973394565684821

Epoch: 6| Step: 2
Training loss: 2.2635176181793213
Validation loss: 2.970162614699333

Epoch: 6| Step: 3
Training loss: 3.520320415496826
Validation loss: 2.9655306364900325

Epoch: 6| Step: 4
Training loss: 3.3822147846221924
Validation loss: 2.9619554499144196

Epoch: 6| Step: 5
Training loss: 2.5655722618103027
Validation loss: 2.9586873900505806

Epoch: 6| Step: 6
Training loss: 3.235665798187256
Validation loss: 2.958432589807818

Epoch: 6| Step: 7
Training loss: 3.2376694679260254
Validation loss: 2.954634399824245

Epoch: 6| Step: 8
Training loss: 2.914285659790039
Validation loss: 2.9503832965768795

Epoch: 6| Step: 9
Training loss: 2.6240837574005127
Validation loss: 2.9486459609000915

Epoch: 6| Step: 10
Training loss: 3.3487329483032227
Validation loss: 2.9460102178717174

Epoch: 6| Step: 11
Training loss: 3.5065407752990723
Validation loss: 2.9436018261858212

Epoch: 6| Step: 12
Training loss: 2.362086534500122
Validation loss: 2.938745657602946

Epoch: 6| Step: 13
Training loss: 4.379440784454346
Validation loss: 2.935731521216772

Epoch: 19| Step: 0
Training loss: 4.209033012390137
Validation loss: 2.933955687348561

Epoch: 6| Step: 1
Training loss: 2.6711905002593994
Validation loss: 2.930663119080246

Epoch: 6| Step: 2
Training loss: 2.7189412117004395
Validation loss: 2.9289593491502988

Epoch: 6| Step: 3
Training loss: 2.889510154724121
Validation loss: 2.926231507332094

Epoch: 6| Step: 4
Training loss: 2.940854787826538
Validation loss: 2.9215734799702964

Epoch: 6| Step: 5
Training loss: 2.933893918991089
Validation loss: 2.918113516223046

Epoch: 6| Step: 6
Training loss: 3.1054491996765137
Validation loss: 2.914469652278449

Epoch: 6| Step: 7
Training loss: 3.056051015853882
Validation loss: 2.9111977469536567

Epoch: 6| Step: 8
Training loss: 2.9476919174194336
Validation loss: 2.9090193856147026

Epoch: 6| Step: 9
Training loss: 2.448040723800659
Validation loss: 2.9044893954389837

Epoch: 6| Step: 10
Training loss: 3.3306832313537598
Validation loss: 2.9060544967651367

Epoch: 6| Step: 11
Training loss: 2.8670406341552734
Validation loss: 2.900987289285147

Epoch: 6| Step: 12
Training loss: 2.9149601459503174
Validation loss: 2.896455608388429

Epoch: 6| Step: 13
Training loss: 3.1249678134918213
Validation loss: 2.8979821256411973

Epoch: 20| Step: 0
Training loss: 3.7597241401672363
Validation loss: 2.8954636896810224

Epoch: 6| Step: 1
Training loss: 2.583951473236084
Validation loss: 2.89401440722968

Epoch: 6| Step: 2
Training loss: 2.759601593017578
Validation loss: 2.8928090116029144

Epoch: 6| Step: 3
Training loss: 2.7451276779174805
Validation loss: 2.8923159260903635

Epoch: 6| Step: 4
Training loss: 3.7810606956481934
Validation loss: 2.889438888078095

Epoch: 6| Step: 5
Training loss: 3.1601176261901855
Validation loss: 2.8871487007346204

Epoch: 6| Step: 6
Training loss: 3.38637113571167
Validation loss: 2.8857796243442

Epoch: 6| Step: 7
Training loss: 2.4035470485687256
Validation loss: 2.881741372487878

Epoch: 6| Step: 8
Training loss: 3.115939140319824
Validation loss: 2.8777914226696057

Epoch: 6| Step: 9
Training loss: 2.6549198627471924
Validation loss: 2.877018802909441

Epoch: 6| Step: 10
Training loss: 2.840555191040039
Validation loss: 2.8739477075556272

Epoch: 6| Step: 11
Training loss: 2.9243812561035156
Validation loss: 2.8714517188328568

Epoch: 6| Step: 12
Training loss: 2.7728383541107178
Validation loss: 2.8705393422034478

Epoch: 6| Step: 13
Training loss: 2.8062143325805664
Validation loss: 2.8659295087219565

Epoch: 21| Step: 0
Training loss: 2.4198336601257324
Validation loss: 2.8645982357763473

Epoch: 6| Step: 1
Training loss: 2.801553964614868
Validation loss: 2.8677453917841755

Epoch: 6| Step: 2
Training loss: 2.672027111053467
Validation loss: 2.8644009508112425

Epoch: 6| Step: 3
Training loss: 2.9312500953674316
Validation loss: 2.862210555743146

Epoch: 6| Step: 4
Training loss: 2.7073521614074707
Validation loss: 2.860812638395576

Epoch: 6| Step: 5
Training loss: 3.0552196502685547
Validation loss: 2.8566118183956353

Epoch: 6| Step: 6
Training loss: 3.391638994216919
Validation loss: 2.855493494259414

Epoch: 6| Step: 7
Training loss: 2.640643358230591
Validation loss: 2.8540889217007543

Epoch: 6| Step: 8
Training loss: 3.1521782875061035
Validation loss: 2.8526916606451875

Epoch: 6| Step: 9
Training loss: 2.8731765747070312
Validation loss: 2.84709140305878

Epoch: 6| Step: 10
Training loss: 3.285984992980957
Validation loss: 2.8450409135510846

Epoch: 6| Step: 11
Training loss: 3.1037254333496094
Validation loss: 2.8428588374968498

Epoch: 6| Step: 12
Training loss: 3.0078463554382324
Validation loss: 2.844195786342826

Epoch: 6| Step: 13
Training loss: 3.8365511894226074
Validation loss: 2.838392524309056

Epoch: 22| Step: 0
Training loss: 2.5648531913757324
Validation loss: 2.8367128141464724

Epoch: 6| Step: 1
Training loss: 3.450806140899658
Validation loss: 2.835222972336636

Epoch: 6| Step: 2
Training loss: 2.2039997577667236
Validation loss: 2.8316514953490226

Epoch: 6| Step: 3
Training loss: 2.846832513809204
Validation loss: 2.8289231433663318

Epoch: 6| Step: 4
Training loss: 3.5948004722595215
Validation loss: 2.829877625229538

Epoch: 6| Step: 5
Training loss: 3.6484227180480957
Validation loss: 2.8321791977010746

Epoch: 6| Step: 6
Training loss: 3.505298137664795
Validation loss: 2.8317453963782198

Epoch: 6| Step: 7
Training loss: 2.60588002204895
Validation loss: 2.827215087029242

Epoch: 6| Step: 8
Training loss: 2.456913709640503
Validation loss: 2.825028701495099

Epoch: 6| Step: 9
Training loss: 3.287907361984253
Validation loss: 2.8214725012420327

Epoch: 6| Step: 10
Training loss: 3.3487582206726074
Validation loss: 2.821488324032035

Epoch: 6| Step: 11
Training loss: 2.9871652126312256
Validation loss: 2.8170785519384567

Epoch: 6| Step: 12
Training loss: 2.3416080474853516
Validation loss: 2.8143415451049805

Epoch: 6| Step: 13
Training loss: 2.044215679168701
Validation loss: 2.8125207988164758

Epoch: 23| Step: 0
Training loss: 3.5340211391448975
Validation loss: 2.8102660743139123

Epoch: 6| Step: 1
Training loss: 2.93990159034729
Validation loss: 2.8058247771314395

Epoch: 6| Step: 2
Training loss: 2.646230936050415
Validation loss: 2.8026992121050434

Epoch: 6| Step: 3
Training loss: 3.1740353107452393
Validation loss: 2.8024807565955707

Epoch: 6| Step: 4
Training loss: 1.9212206602096558
Validation loss: 2.803183035183978

Epoch: 6| Step: 5
Training loss: 4.0815534591674805
Validation loss: 2.8048363193388908

Epoch: 6| Step: 6
Training loss: 3.1886720657348633
Validation loss: 2.797048863544259

Epoch: 6| Step: 7
Training loss: 2.8672237396240234
Validation loss: 2.793800885959338

Epoch: 6| Step: 8
Training loss: 2.9197733402252197
Validation loss: 2.7944102979475454

Epoch: 6| Step: 9
Training loss: 3.9644861221313477
Validation loss: 2.792314583255399

Epoch: 6| Step: 10
Training loss: 2.64392352104187
Validation loss: 2.790379601140176

Epoch: 6| Step: 11
Training loss: 2.260526657104492
Validation loss: 2.7869613760261127

Epoch: 6| Step: 12
Training loss: 2.350468397140503
Validation loss: 2.7894715262997534

Epoch: 6| Step: 13
Training loss: 2.209332227706909
Validation loss: 2.7850717985501854

Epoch: 24| Step: 0
Training loss: 2.8301949501037598
Validation loss: 2.780208513300906

Epoch: 6| Step: 1
Training loss: 2.8739166259765625
Validation loss: 2.780230237591651

Epoch: 6| Step: 2
Training loss: 3.0461413860321045
Validation loss: 2.7773545429270756

Epoch: 6| Step: 3
Training loss: 2.369823455810547
Validation loss: 2.776258045627225

Epoch: 6| Step: 4
Training loss: 2.551055431365967
Validation loss: 2.772735257302561

Epoch: 6| Step: 5
Training loss: 2.601562976837158
Validation loss: 2.7737837247951056

Epoch: 6| Step: 6
Training loss: 2.7089591026306152
Validation loss: 2.7699620492996706

Epoch: 6| Step: 7
Training loss: 4.349398612976074
Validation loss: 2.7711617510805846

Epoch: 6| Step: 8
Training loss: 2.9271397590637207
Validation loss: 2.767440821534844

Epoch: 6| Step: 9
Training loss: 2.7401297092437744
Validation loss: 2.768975534746724

Epoch: 6| Step: 10
Training loss: 2.825289249420166
Validation loss: 2.7652674157132386

Epoch: 6| Step: 11
Training loss: 3.381761074066162
Validation loss: 2.765781738424814

Epoch: 6| Step: 12
Training loss: 2.0570363998413086
Validation loss: 2.7625072335684173

Epoch: 6| Step: 13
Training loss: 3.9747798442840576
Validation loss: 2.756844130895471

Epoch: 25| Step: 0
Training loss: 2.876276731491089
Validation loss: 2.7580011506234445

Epoch: 6| Step: 1
Training loss: 2.9247491359710693
Validation loss: 2.7576236519762265

Epoch: 6| Step: 2
Training loss: 2.487894296646118
Validation loss: 2.7542860815601964

Epoch: 6| Step: 3
Training loss: 3.225419044494629
Validation loss: 2.752958031110866

Epoch: 6| Step: 4
Training loss: 3.560783863067627
Validation loss: 2.7520195848198346

Epoch: 6| Step: 5
Training loss: 2.5175986289978027
Validation loss: 2.74946080997426

Epoch: 6| Step: 6
Training loss: 3.035160779953003
Validation loss: 2.7499304791932464

Epoch: 6| Step: 7
Training loss: 3.281137704849243
Validation loss: 2.7460733126568537

Epoch: 6| Step: 8
Training loss: 2.567662000656128
Validation loss: 2.74516418416013

Epoch: 6| Step: 9
Training loss: 3.3114514350891113
Validation loss: 2.7430672671205256

Epoch: 6| Step: 10
Training loss: 2.73640513420105
Validation loss: 2.742171618246263

Epoch: 6| Step: 11
Training loss: 2.766190528869629
Validation loss: 2.7413637176636727

Epoch: 6| Step: 12
Training loss: 2.7411890029907227
Validation loss: 2.7379709456556585

Epoch: 6| Step: 13
Training loss: 2.3081326484680176
Validation loss: 2.7408059848252164

Epoch: 26| Step: 0
Training loss: 2.311061382293701
Validation loss: 2.7392566255343858

Epoch: 6| Step: 1
Training loss: 2.0014870166778564
Validation loss: 2.7350633631470385

Epoch: 6| Step: 2
Training loss: 2.456697463989258
Validation loss: 2.738033245968562

Epoch: 6| Step: 3
Training loss: 3.659538745880127
Validation loss: 2.738782936526883

Epoch: 6| Step: 4
Training loss: 2.480173110961914
Validation loss: 2.7361096720541678

Epoch: 6| Step: 5
Training loss: 2.674812078475952
Validation loss: 2.7335490642055387

Epoch: 6| Step: 6
Training loss: 2.547837734222412
Validation loss: 2.7314328121882614

Epoch: 6| Step: 7
Training loss: 3.0385379791259766
Validation loss: 2.7272088732770694

Epoch: 6| Step: 8
Training loss: 2.9873876571655273
Validation loss: 2.7286607732055006

Epoch: 6| Step: 9
Training loss: 2.8319921493530273
Validation loss: 2.72662530919557

Epoch: 6| Step: 10
Training loss: 3.872039318084717
Validation loss: 2.7249216905204197

Epoch: 6| Step: 11
Training loss: 2.85913348197937
Validation loss: 2.721297658899779

Epoch: 6| Step: 12
Training loss: 4.093629360198975
Validation loss: 2.7181605010904293

Epoch: 6| Step: 13
Training loss: 2.3868908882141113
Validation loss: 2.7168573487189507

Epoch: 27| Step: 0
Training loss: 3.2762997150421143
Validation loss: 2.7184627440667923

Epoch: 6| Step: 1
Training loss: 2.6270196437835693
Validation loss: 2.7186414554554927

Epoch: 6| Step: 2
Training loss: 3.0786242485046387
Validation loss: 2.722323715045888

Epoch: 6| Step: 3
Training loss: 2.660593032836914
Validation loss: 2.721229196876608

Epoch: 6| Step: 4
Training loss: 2.872856616973877
Validation loss: 2.720061012493667

Epoch: 6| Step: 5
Training loss: 3.4102706909179688
Validation loss: 2.720433109550066

Epoch: 6| Step: 6
Training loss: 2.8556582927703857
Validation loss: 2.725329501654512

Epoch: 6| Step: 7
Training loss: 3.456056594848633
Validation loss: 2.7190619027742775

Epoch: 6| Step: 8
Training loss: 2.964111328125
Validation loss: 2.7119530375285814

Epoch: 6| Step: 9
Training loss: 2.7587428092956543
Validation loss: 2.70826478158274

Epoch: 6| Step: 10
Training loss: 2.818091630935669
Validation loss: 2.710882276617071

Epoch: 6| Step: 11
Training loss: 2.474064350128174
Validation loss: 2.7054747458427184

Epoch: 6| Step: 12
Training loss: 2.172483205795288
Validation loss: 2.7148586421884517

Epoch: 6| Step: 13
Training loss: 2.8289928436279297
Validation loss: 2.7064731454336517

Epoch: 28| Step: 0
Training loss: 1.9977169036865234
Validation loss: 2.702621774006915

Epoch: 6| Step: 1
Training loss: 2.5855066776275635
Validation loss: 2.7020505653914584

Epoch: 6| Step: 2
Training loss: 2.600064754486084
Validation loss: 2.702028013044788

Epoch: 6| Step: 3
Training loss: 3.0816378593444824
Validation loss: 2.699510725595618

Epoch: 6| Step: 4
Training loss: 2.9173426628112793
Validation loss: 2.703423648752192

Epoch: 6| Step: 5
Training loss: 3.2319531440734863
Validation loss: 2.715981355277441

Epoch: 6| Step: 6
Training loss: 2.9342691898345947
Validation loss: 2.7025467503455376

Epoch: 6| Step: 7
Training loss: 3.501171588897705
Validation loss: 2.7079046234007804

Epoch: 6| Step: 8
Training loss: 3.15261173248291
Validation loss: 2.708749396826631

Epoch: 6| Step: 9
Training loss: 2.4209892749786377
Validation loss: 2.7000005091390302

Epoch: 6| Step: 10
Training loss: 2.802243709564209
Validation loss: 2.69786472218011

Epoch: 6| Step: 11
Training loss: 2.63080096244812
Validation loss: 2.6957722428024455

Epoch: 6| Step: 12
Training loss: 3.0378031730651855
Validation loss: 2.691195172648276

Epoch: 6| Step: 13
Training loss: 3.5834219455718994
Validation loss: 2.6877720945624897

Epoch: 29| Step: 0
Training loss: 3.025557041168213
Validation loss: 2.6873366602005495

Epoch: 6| Step: 1
Training loss: 3.4586057662963867
Validation loss: 2.685716885392384

Epoch: 6| Step: 2
Training loss: 2.680734634399414
Validation loss: 2.6840391902513403

Epoch: 6| Step: 3
Training loss: 3.0691843032836914
Validation loss: 2.687089604716147

Epoch: 6| Step: 4
Training loss: 2.8672800064086914
Validation loss: 2.688099245871267

Epoch: 6| Step: 5
Training loss: 2.789985179901123
Validation loss: 2.693033113274523

Epoch: 6| Step: 6
Training loss: 3.5695056915283203
Validation loss: 2.6829308822590816

Epoch: 6| Step: 7
Training loss: 2.1667392253875732
Validation loss: 2.6819946124989498

Epoch: 6| Step: 8
Training loss: 2.7235052585601807
Validation loss: 2.684244901903214

Epoch: 6| Step: 9
Training loss: 3.119703769683838
Validation loss: 2.6821354153335735

Epoch: 6| Step: 10
Training loss: 2.6213431358337402
Validation loss: 2.680540810349167

Epoch: 6| Step: 11
Training loss: 2.4105658531188965
Validation loss: 2.680643876393636

Epoch: 6| Step: 12
Training loss: 3.192084312438965
Validation loss: 2.681733521082068

Epoch: 6| Step: 13
Training loss: 1.885721206665039
Validation loss: 2.6790175207199587

Epoch: 30| Step: 0
Training loss: 3.56398868560791
Validation loss: 2.6792968421854

Epoch: 6| Step: 1
Training loss: 2.737288475036621
Validation loss: 2.680792321440994

Epoch: 6| Step: 2
Training loss: 3.2682905197143555
Validation loss: 2.6834123339704288

Epoch: 6| Step: 3
Training loss: 3.4859113693237305
Validation loss: 2.688178580294373

Epoch: 6| Step: 4
Training loss: 2.30949068069458
Validation loss: 2.686456334206366

Epoch: 6| Step: 5
Training loss: 2.287169933319092
Validation loss: 2.6812081721521195

Epoch: 6| Step: 6
Training loss: 2.6021461486816406
Validation loss: 2.680366685313563

Epoch: 6| Step: 7
Training loss: 3.0586490631103516
Validation loss: 2.6979035869721444

Epoch: 6| Step: 8
Training loss: 3.632209062576294
Validation loss: 2.725854919802758

Epoch: 6| Step: 9
Training loss: 2.725738048553467
Validation loss: 2.6907954446731077

Epoch: 6| Step: 10
Training loss: 2.6278328895568848
Validation loss: 2.677980979283651

Epoch: 6| Step: 11
Training loss: 2.9367241859436035
Validation loss: 2.6851040137711393

Epoch: 6| Step: 12
Training loss: 2.554844617843628
Validation loss: 2.67893474589112

Epoch: 6| Step: 13
Training loss: 1.8480873107910156
Validation loss: 2.6728576434555875

Epoch: 31| Step: 0
Training loss: 2.4783005714416504
Validation loss: 2.668914759030906

Epoch: 6| Step: 1
Training loss: 2.832292318344116
Validation loss: 2.6631490850961335

Epoch: 6| Step: 2
Training loss: 2.4821219444274902
Validation loss: 2.6624389592037407

Epoch: 6| Step: 3
Training loss: 3.988600254058838
Validation loss: 2.6674520431026334

Epoch: 6| Step: 4
Training loss: 2.8797757625579834
Validation loss: 2.6641483511976016

Epoch: 6| Step: 5
Training loss: 2.70379376411438
Validation loss: 2.670563574760191

Epoch: 6| Step: 6
Training loss: 2.2458343505859375
Validation loss: 2.674893089520034

Epoch: 6| Step: 7
Training loss: 2.843848466873169
Validation loss: 2.683011160101942

Epoch: 6| Step: 8
Training loss: 2.680607795715332
Validation loss: 2.6861529760463263

Epoch: 6| Step: 9
Training loss: 2.778707981109619
Validation loss: 2.703246839584843

Epoch: 6| Step: 10
Training loss: 3.4744176864624023
Validation loss: 2.703071373765187

Epoch: 6| Step: 11
Training loss: 2.3823795318603516
Validation loss: 2.683799833379766

Epoch: 6| Step: 12
Training loss: 2.803635597229004
Validation loss: 2.659462764698972

Epoch: 6| Step: 13
Training loss: 3.6953344345092773
Validation loss: 2.654954271931802

Epoch: 32| Step: 0
Training loss: 2.856658458709717
Validation loss: 2.6568576392307075

Epoch: 6| Step: 1
Training loss: 3.011653184890747
Validation loss: 2.6545641960636264

Epoch: 6| Step: 2
Training loss: 3.046900749206543
Validation loss: 2.662082008136216

Epoch: 6| Step: 3
Training loss: 2.7595958709716797
Validation loss: 2.6727949188601587

Epoch: 6| Step: 4
Training loss: 2.215263843536377
Validation loss: 2.6729750197420836

Epoch: 6| Step: 5
Training loss: 2.7411460876464844
Validation loss: 2.6742074207593034

Epoch: 6| Step: 6
Training loss: 2.724277973175049
Validation loss: 2.6562219383896037

Epoch: 6| Step: 7
Training loss: 3.1673154830932617
Validation loss: 2.654398597696776

Epoch: 6| Step: 8
Training loss: 2.5241029262542725
Validation loss: 2.654078301563058

Epoch: 6| Step: 9
Training loss: 2.931041717529297
Validation loss: 2.6573025744448424

Epoch: 6| Step: 10
Training loss: 2.906256675720215
Validation loss: 2.6605943300390757

Epoch: 6| Step: 11
Training loss: 3.1368470191955566
Validation loss: 2.65489766674657

Epoch: 6| Step: 12
Training loss: 3.4374313354492188
Validation loss: 2.6565210870517197

Epoch: 6| Step: 13
Training loss: 1.9351931810379028
Validation loss: 2.655557991355978

Epoch: 33| Step: 0
Training loss: 2.7565250396728516
Validation loss: 2.6516278456616145

Epoch: 6| Step: 1
Training loss: 2.8070826530456543
Validation loss: 2.6513998380271335

Epoch: 6| Step: 2
Training loss: 2.8423962593078613
Validation loss: 2.6460302722069526

Epoch: 6| Step: 3
Training loss: 2.3693151473999023
Validation loss: 2.6447108022628294

Epoch: 6| Step: 4
Training loss: 3.141146183013916
Validation loss: 2.6452964480205248

Epoch: 6| Step: 5
Training loss: 3.39152455329895
Validation loss: 2.6406578094728532

Epoch: 6| Step: 6
Training loss: 3.2610466480255127
Validation loss: 2.6402027478782077

Epoch: 6| Step: 7
Training loss: 2.082601547241211
Validation loss: 2.6360821185573453

Epoch: 6| Step: 8
Training loss: 2.6285815238952637
Validation loss: 2.634626873077885

Epoch: 6| Step: 9
Training loss: 3.081672191619873
Validation loss: 2.6383221482717865

Epoch: 6| Step: 10
Training loss: 3.095590353012085
Validation loss: 2.634764145779353

Epoch: 6| Step: 11
Training loss: 2.7581543922424316
Validation loss: 2.6412317906656573

Epoch: 6| Step: 12
Training loss: 2.4067070484161377
Validation loss: 2.639759396993986

Epoch: 6| Step: 13
Training loss: 3.161630630493164
Validation loss: 2.640127697298604

Epoch: 34| Step: 0
Training loss: 2.9121391773223877
Validation loss: 2.639892362779187

Epoch: 6| Step: 1
Training loss: 3.4081320762634277
Validation loss: 2.6336524409632527

Epoch: 6| Step: 2
Training loss: 2.6961183547973633
Validation loss: 2.6314089990431264

Epoch: 6| Step: 3
Training loss: 3.4350457191467285
Validation loss: 2.631629979738625

Epoch: 6| Step: 4
Training loss: 2.4311108589172363
Validation loss: 2.6288130565356185

Epoch: 6| Step: 5
Training loss: 2.3797364234924316
Validation loss: 2.625337621217133

Epoch: 6| Step: 6
Training loss: 2.283859968185425
Validation loss: 2.630692010284752

Epoch: 6| Step: 7
Training loss: 2.9985733032226562
Validation loss: 2.6251456429881435

Epoch: 6| Step: 8
Training loss: 3.1173923015594482
Validation loss: 2.629640410023351

Epoch: 6| Step: 9
Training loss: 2.5235390663146973
Validation loss: 2.63133071314904

Epoch: 6| Step: 10
Training loss: 3.506809711456299
Validation loss: 2.627300677760955

Epoch: 6| Step: 11
Training loss: 1.9261524677276611
Validation loss: 2.6281128801325315

Epoch: 6| Step: 12
Training loss: 2.530883312225342
Validation loss: 2.629111966779155

Epoch: 6| Step: 13
Training loss: 3.8023996353149414
Validation loss: 2.626610573901925

Epoch: 35| Step: 0
Training loss: 2.8366940021514893
Validation loss: 2.623376300258021

Epoch: 6| Step: 1
Training loss: 3.289364814758301
Validation loss: 2.620939644434119

Epoch: 6| Step: 2
Training loss: 3.0214102268218994
Validation loss: 2.6209178637432795

Epoch: 6| Step: 3
Training loss: 2.7351996898651123
Validation loss: 2.619216049871137

Epoch: 6| Step: 4
Training loss: 3.088487148284912
Validation loss: 2.615051136221937

Epoch: 6| Step: 5
Training loss: 2.5893869400024414
Validation loss: 2.6179995690622637

Epoch: 6| Step: 6
Training loss: 2.5373284816741943
Validation loss: 2.613226649581745

Epoch: 6| Step: 7
Training loss: 3.237706184387207
Validation loss: 2.6154297833801596

Epoch: 6| Step: 8
Training loss: 2.8375418186187744
Validation loss: 2.6140358178846297

Epoch: 6| Step: 9
Training loss: 3.0607500076293945
Validation loss: 2.6149247000294347

Epoch: 6| Step: 10
Training loss: 2.6756677627563477
Validation loss: 2.615285006902551

Epoch: 6| Step: 11
Training loss: 2.61686372756958
Validation loss: 2.612742518865934

Epoch: 6| Step: 12
Training loss: 2.4701108932495117
Validation loss: 2.6154066875416744

Epoch: 6| Step: 13
Training loss: 2.0672638416290283
Validation loss: 2.613456453046491

Epoch: 36| Step: 0
Training loss: 3.2166757583618164
Validation loss: 2.6239442389498473

Epoch: 6| Step: 1
Training loss: 3.0274081230163574
Validation loss: 2.6249189133285196

Epoch: 6| Step: 2
Training loss: 2.803156852722168
Validation loss: 2.6199948172415457

Epoch: 6| Step: 3
Training loss: 2.761974811553955
Validation loss: 2.633940389079432

Epoch: 6| Step: 4
Training loss: 2.8917183876037598
Validation loss: 2.62505183681365

Epoch: 6| Step: 5
Training loss: 2.3495383262634277
Validation loss: 2.6132945347857732

Epoch: 6| Step: 6
Training loss: 3.060502052307129
Validation loss: 2.6140429537783385

Epoch: 6| Step: 7
Training loss: 3.577930450439453
Validation loss: 2.611072535155922

Epoch: 6| Step: 8
Training loss: 2.366624593734741
Validation loss: 2.6096173512038363

Epoch: 6| Step: 9
Training loss: 1.7689789533615112
Validation loss: 2.609338229702365

Epoch: 6| Step: 10
Training loss: 2.2067089080810547
Validation loss: 2.6083291628027476

Epoch: 6| Step: 11
Training loss: 2.2161996364593506
Validation loss: 2.6060026743078746

Epoch: 6| Step: 12
Training loss: 4.2791643142700195
Validation loss: 2.6001273880722704

Epoch: 6| Step: 13
Training loss: 2.6927683353424072
Validation loss: 2.604869791256484

Epoch: 37| Step: 0
Training loss: 3.5518150329589844
Validation loss: 2.6050228893115954

Epoch: 6| Step: 1
Training loss: 3.167933940887451
Validation loss: 2.6017676579054965

Epoch: 6| Step: 2
Training loss: 2.2230284214019775
Validation loss: 2.605047059315507

Epoch: 6| Step: 3
Training loss: 2.7136216163635254
Validation loss: 2.6037927494254163

Epoch: 6| Step: 4
Training loss: 2.654531955718994
Validation loss: 2.6056361018970446

Epoch: 6| Step: 5
Training loss: 2.8050951957702637
Validation loss: 2.6151359209450344

Epoch: 6| Step: 6
Training loss: 3.252397298812866
Validation loss: 2.6108480948273853

Epoch: 6| Step: 7
Training loss: 2.931067943572998
Validation loss: 2.6087072254509054

Epoch: 6| Step: 8
Training loss: 3.020235061645508
Validation loss: 2.6100965546023462

Epoch: 6| Step: 9
Training loss: 2.8832170963287354
Validation loss: 2.606512290175243

Epoch: 6| Step: 10
Training loss: 2.7369155883789062
Validation loss: 2.601035879504296

Epoch: 6| Step: 11
Training loss: 2.734217405319214
Validation loss: 2.5963327961583293

Epoch: 6| Step: 12
Training loss: 2.238844871520996
Validation loss: 2.5969159141663583

Epoch: 6| Step: 13
Training loss: 1.8177059888839722
Validation loss: 2.59584447901736

Epoch: 38| Step: 0
Training loss: 2.028097152709961
Validation loss: 2.5965805592075473

Epoch: 6| Step: 1
Training loss: 3.008993625640869
Validation loss: 2.594717356466478

Epoch: 6| Step: 2
Training loss: 2.9522533416748047
Validation loss: 2.594318918002549

Epoch: 6| Step: 3
Training loss: 2.321382522583008
Validation loss: 2.5931746267503306

Epoch: 6| Step: 4
Training loss: 2.703601360321045
Validation loss: 2.5965897908774753

Epoch: 6| Step: 5
Training loss: 2.9063644409179688
Validation loss: 2.5971573604050504

Epoch: 6| Step: 6
Training loss: 2.3582711219787598
Validation loss: 2.6010462135396977

Epoch: 6| Step: 7
Training loss: 2.816859722137451
Validation loss: 2.598633030409454

Epoch: 6| Step: 8
Training loss: 2.530264139175415
Validation loss: 2.598283588245351

Epoch: 6| Step: 9
Training loss: 3.2682690620422363
Validation loss: 2.5934844529756935

Epoch: 6| Step: 10
Training loss: 3.050600051879883
Validation loss: 2.5899485541928198

Epoch: 6| Step: 11
Training loss: 3.163175344467163
Validation loss: 2.5931095077145483

Epoch: 6| Step: 12
Training loss: 2.725220203399658
Validation loss: 2.5912175998892835

Epoch: 6| Step: 13
Training loss: 3.5763347148895264
Validation loss: 2.587215572275141

Epoch: 39| Step: 0
Training loss: 2.9699485301971436
Validation loss: 2.5863686530820784

Epoch: 6| Step: 1
Training loss: 2.5983808040618896
Validation loss: 2.5880804933527464

Epoch: 6| Step: 2
Training loss: 2.3948330879211426
Validation loss: 2.588634378166609

Epoch: 6| Step: 3
Training loss: 2.5806145668029785
Validation loss: 2.583901684771302

Epoch: 6| Step: 4
Training loss: 2.8655080795288086
Validation loss: 2.5859133299960884

Epoch: 6| Step: 5
Training loss: 2.852907180786133
Validation loss: 2.5862978196913198

Epoch: 6| Step: 6
Training loss: 2.6930196285247803
Validation loss: 2.588119952909408

Epoch: 6| Step: 7
Training loss: 2.9396748542785645
Validation loss: 2.5881017766973025

Epoch: 6| Step: 8
Training loss: 3.185943126678467
Validation loss: 2.588546173546904

Epoch: 6| Step: 9
Training loss: 2.5504612922668457
Validation loss: 2.5860665434150287

Epoch: 6| Step: 10
Training loss: 2.351088523864746
Validation loss: 2.5844167381204586

Epoch: 6| Step: 11
Training loss: 3.5767884254455566
Validation loss: 2.5910370631884505

Epoch: 6| Step: 12
Training loss: 2.4127159118652344
Validation loss: 2.590651576237012

Epoch: 6| Step: 13
Training loss: 3.2069621086120605
Validation loss: 2.5903973605043147

Epoch: 40| Step: 0
Training loss: 2.6918399333953857
Validation loss: 2.585863615876885

Epoch: 6| Step: 1
Training loss: 2.6845669746398926
Validation loss: 2.58299349200341

Epoch: 6| Step: 2
Training loss: 3.0433380603790283
Validation loss: 2.5851138535366265

Epoch: 6| Step: 3
Training loss: 2.4117417335510254
Validation loss: 2.5817317373009137

Epoch: 6| Step: 4
Training loss: 2.355193614959717
Validation loss: 2.5769656935045795

Epoch: 6| Step: 5
Training loss: 2.5857291221618652
Validation loss: 2.577254246639949

Epoch: 6| Step: 6
Training loss: 2.260190010070801
Validation loss: 2.5865425268809

Epoch: 6| Step: 7
Training loss: 3.4814743995666504
Validation loss: 2.6037885578729774

Epoch: 6| Step: 8
Training loss: 2.587892532348633
Validation loss: 2.6207828880638204

Epoch: 6| Step: 9
Training loss: 2.8504483699798584
Validation loss: 2.6238321206902944

Epoch: 6| Step: 10
Training loss: 3.581216335296631
Validation loss: 2.623887554291756

Epoch: 6| Step: 11
Training loss: 2.4714198112487793
Validation loss: 2.6140021816376717

Epoch: 6| Step: 12
Training loss: 3.190943956375122
Validation loss: 2.59678316116333

Epoch: 6| Step: 13
Training loss: 2.873922824859619
Validation loss: 2.577879157117618

Epoch: 41| Step: 0
Training loss: 3.016601324081421
Validation loss: 2.5818711865332817

Epoch: 6| Step: 1
Training loss: 3.260373830795288
Validation loss: 2.588226374759469

Epoch: 6| Step: 2
Training loss: 3.1914174556732178
Validation loss: 2.6064653832425355

Epoch: 6| Step: 3
Training loss: 3.113572359085083
Validation loss: 2.607570186738045

Epoch: 6| Step: 4
Training loss: 2.4595272541046143
Validation loss: 2.628335137521067

Epoch: 6| Step: 5
Training loss: 2.5435893535614014
Validation loss: 2.627125019668251

Epoch: 6| Step: 6
Training loss: 2.6522257328033447
Validation loss: 2.639288890746332

Epoch: 6| Step: 7
Training loss: 1.6342084407806396
Validation loss: 2.618942786288518

Epoch: 6| Step: 8
Training loss: 2.792158603668213
Validation loss: 2.6058009286080637

Epoch: 6| Step: 9
Training loss: 2.7911694049835205
Validation loss: 2.5818682768011607

Epoch: 6| Step: 10
Training loss: 3.298107624053955
Validation loss: 2.5840940039644957

Epoch: 6| Step: 11
Training loss: 2.8416266441345215
Validation loss: 2.580664065576369

Epoch: 6| Step: 12
Training loss: 2.926028251647949
Validation loss: 2.583415828725343

Epoch: 6| Step: 13
Training loss: 2.594008207321167
Validation loss: 2.5847997434677614

Epoch: 42| Step: 0
Training loss: 2.756762742996216
Validation loss: 2.592181795386858

Epoch: 6| Step: 1
Training loss: 2.169139862060547
Validation loss: 2.5854698432389127

Epoch: 6| Step: 2
Training loss: 2.912269115447998
Validation loss: 2.5870886874455277

Epoch: 6| Step: 3
Training loss: 2.3758091926574707
Validation loss: 2.5914661294670513

Epoch: 6| Step: 4
Training loss: 2.682166576385498
Validation loss: 2.6007571399852796

Epoch: 6| Step: 5
Training loss: 2.51167631149292
Validation loss: 2.616022135621758

Epoch: 6| Step: 6
Training loss: 3.1776797771453857
Validation loss: 2.6202812323006253

Epoch: 6| Step: 7
Training loss: 3.1075453758239746
Validation loss: 2.609628105676302

Epoch: 6| Step: 8
Training loss: 2.8955419063568115
Validation loss: 2.6031407463935112

Epoch: 6| Step: 9
Training loss: 2.7017312049865723
Validation loss: 2.597502895580825

Epoch: 6| Step: 10
Training loss: 2.6599831581115723
Validation loss: 2.5908812451106247

Epoch: 6| Step: 11
Training loss: 4.125024795532227
Validation loss: 2.589136787640151

Epoch: 6| Step: 12
Training loss: 2.409534215927124
Validation loss: 2.5809463608649468

Epoch: 6| Step: 13
Training loss: 2.4601333141326904
Validation loss: 2.5810341886294785

Epoch: 43| Step: 0
Training loss: 2.360342264175415
Validation loss: 2.583632233322308

Epoch: 6| Step: 1
Training loss: 3.0266549587249756
Validation loss: 2.5851309863469933

Epoch: 6| Step: 2
Training loss: 2.5278234481811523
Validation loss: 2.589689762361588

Epoch: 6| Step: 3
Training loss: 2.4035449028015137
Validation loss: 2.6000405844821723

Epoch: 6| Step: 4
Training loss: 2.699772834777832
Validation loss: 2.5956098033535864

Epoch: 6| Step: 5
Training loss: 2.801429510116577
Validation loss: 2.5710643978529077

Epoch: 6| Step: 6
Training loss: 3.6959104537963867
Validation loss: 2.5671220928110103

Epoch: 6| Step: 7
Training loss: 2.688382148742676
Validation loss: 2.566494882747691

Epoch: 6| Step: 8
Training loss: 2.6553139686584473
Validation loss: 2.56555208083122

Epoch: 6| Step: 9
Training loss: 2.509697914123535
Validation loss: 2.566125472386678

Epoch: 6| Step: 10
Training loss: 3.194566249847412
Validation loss: 2.567404495772495

Epoch: 6| Step: 11
Training loss: 2.840393543243408
Validation loss: 2.578030211951143

Epoch: 6| Step: 12
Training loss: 2.6216673851013184
Validation loss: 2.580102748768304

Epoch: 6| Step: 13
Training loss: 2.893569231033325
Validation loss: 2.5947292338135424

Epoch: 44| Step: 0
Training loss: 2.589840888977051
Validation loss: 2.587640122700763

Epoch: 6| Step: 1
Training loss: 2.7037453651428223
Validation loss: 2.5796409371078655

Epoch: 6| Step: 2
Training loss: 3.0322766304016113
Validation loss: 2.5777808953357

Epoch: 6| Step: 3
Training loss: 3.0883986949920654
Validation loss: 2.5701507778577906

Epoch: 6| Step: 4
Training loss: 3.203550338745117
Validation loss: 2.5654937503158406

Epoch: 6| Step: 5
Training loss: 1.2817528247833252
Validation loss: 2.562934165359825

Epoch: 6| Step: 6
Training loss: 2.281284809112549
Validation loss: 2.5639211490590084

Epoch: 6| Step: 7
Training loss: 3.0044074058532715
Validation loss: 2.5615768817163285

Epoch: 6| Step: 8
Training loss: 2.6230263710021973
Validation loss: 2.5610852882426274

Epoch: 6| Step: 9
Training loss: 2.934015989303589
Validation loss: 2.561937588517384

Epoch: 6| Step: 10
Training loss: 3.315967082977295
Validation loss: 2.5608789895170476

Epoch: 6| Step: 11
Training loss: 2.6826019287109375
Validation loss: 2.5645548348785727

Epoch: 6| Step: 12
Training loss: 2.734438896179199
Validation loss: 2.562830040531774

Epoch: 6| Step: 13
Training loss: 3.707045555114746
Validation loss: 2.5640934846734487

Epoch: 45| Step: 0
Training loss: 2.3254685401916504
Validation loss: 2.5638017808237383

Epoch: 6| Step: 1
Training loss: 2.9936487674713135
Validation loss: 2.5655165795356996

Epoch: 6| Step: 2
Training loss: 3.1252217292785645
Validation loss: 2.5657583590476745

Epoch: 6| Step: 3
Training loss: 2.6152400970458984
Validation loss: 2.565555005945185

Epoch: 6| Step: 4
Training loss: 2.7092247009277344
Validation loss: 2.5591135307024886

Epoch: 6| Step: 5
Training loss: 2.4439239501953125
Validation loss: 2.554021294398974

Epoch: 6| Step: 6
Training loss: 3.1487226486206055
Validation loss: 2.5526621341705322

Epoch: 6| Step: 7
Training loss: 2.5187783241271973
Validation loss: 2.552830247468846

Epoch: 6| Step: 8
Training loss: 2.7811360359191895
Validation loss: 2.5555935239279144

Epoch: 6| Step: 9
Training loss: 3.259943723678589
Validation loss: 2.5560080312913462

Epoch: 6| Step: 10
Training loss: 2.8736939430236816
Validation loss: 2.552990168653509

Epoch: 6| Step: 11
Training loss: 2.7030975818634033
Validation loss: 2.5588511523380073

Epoch: 6| Step: 12
Training loss: 2.5054166316986084
Validation loss: 2.5527322112873034

Epoch: 6| Step: 13
Training loss: 2.67514967918396
Validation loss: 2.549758744496171

Epoch: 46| Step: 0
Training loss: 2.9866180419921875
Validation loss: 2.5469733720184653

Epoch: 6| Step: 1
Training loss: 2.3195722103118896
Validation loss: 2.54672170454456

Epoch: 6| Step: 2
Training loss: 2.825683832168579
Validation loss: 2.5481715484332015

Epoch: 6| Step: 3
Training loss: 2.693082094192505
Validation loss: 2.547788317485522

Epoch: 6| Step: 4
Training loss: 3.000244617462158
Validation loss: 2.551630222669212

Epoch: 6| Step: 5
Training loss: 3.005101203918457
Validation loss: 2.5540060586826776

Epoch: 6| Step: 6
Training loss: 2.5283560752868652
Validation loss: 2.557710096400271

Epoch: 6| Step: 7
Training loss: 3.139751434326172
Validation loss: 2.5584582128832416

Epoch: 6| Step: 8
Training loss: 3.231544256210327
Validation loss: 2.5596480267022246

Epoch: 6| Step: 9
Training loss: 2.6315557956695557
Validation loss: 2.5529843145801174

Epoch: 6| Step: 10
Training loss: 2.3859434127807617
Validation loss: 2.547928123063939

Epoch: 6| Step: 11
Training loss: 2.2137742042541504
Validation loss: 2.5433572799928728

Epoch: 6| Step: 12
Training loss: 2.78818678855896
Validation loss: 2.545708946002427

Epoch: 6| Step: 13
Training loss: 2.943845748901367
Validation loss: 2.542399260305589

Epoch: 47| Step: 0
Training loss: 3.514918327331543
Validation loss: 2.544893544207337

Epoch: 6| Step: 1
Training loss: 2.0533294677734375
Validation loss: 2.546626628086131

Epoch: 6| Step: 2
Training loss: 3.160585880279541
Validation loss: 2.5460347770362772

Epoch: 6| Step: 3
Training loss: 2.7125959396362305
Validation loss: 2.5479962825775146

Epoch: 6| Step: 4
Training loss: 2.9849681854248047
Validation loss: 2.545298425100183

Epoch: 6| Step: 5
Training loss: 2.7055153846740723
Validation loss: 2.5463809761949765

Epoch: 6| Step: 6
Training loss: 2.572573184967041
Validation loss: 2.5423490103854927

Epoch: 6| Step: 7
Training loss: 1.8979251384735107
Validation loss: 2.5428752565896637

Epoch: 6| Step: 8
Training loss: 2.895050048828125
Validation loss: 2.539616151522565

Epoch: 6| Step: 9
Training loss: 2.879183769226074
Validation loss: 2.5419492670284805

Epoch: 6| Step: 10
Training loss: 2.364145517349243
Validation loss: 2.545708904984177

Epoch: 6| Step: 11
Training loss: 2.9129793643951416
Validation loss: 2.549849235883323

Epoch: 6| Step: 12
Training loss: 2.840017795562744
Validation loss: 2.550845810162124

Epoch: 6| Step: 13
Training loss: 3.318028450012207
Validation loss: 2.549245185749505

Epoch: 48| Step: 0
Training loss: 2.9528186321258545
Validation loss: 2.5461879891733967

Epoch: 6| Step: 1
Training loss: 2.5644330978393555
Validation loss: 2.5432029513902563

Epoch: 6| Step: 2
Training loss: 2.309813976287842
Validation loss: 2.5414972279661443

Epoch: 6| Step: 3
Training loss: 2.7188496589660645
Validation loss: 2.5393799761290192

Epoch: 6| Step: 4
Training loss: 2.688544988632202
Validation loss: 2.5385862268427366

Epoch: 6| Step: 5
Training loss: 3.0781116485595703
Validation loss: 2.536893262658068

Epoch: 6| Step: 6
Training loss: 2.751821994781494
Validation loss: 2.5348514562012046

Epoch: 6| Step: 7
Training loss: 2.304624080657959
Validation loss: 2.5334522595969577

Epoch: 6| Step: 8
Training loss: 3.103243112564087
Validation loss: 2.536209996028613

Epoch: 6| Step: 9
Training loss: 2.3532848358154297
Validation loss: 2.5364571104767504

Epoch: 6| Step: 10
Training loss: 3.317075490951538
Validation loss: 2.5363022076186312

Epoch: 6| Step: 11
Training loss: 2.7618579864501953
Validation loss: 2.538065310447447

Epoch: 6| Step: 12
Training loss: 2.9849793910980225
Validation loss: 2.539093138069235

Epoch: 6| Step: 13
Training loss: 2.558424234390259
Validation loss: 2.5409006559720604

Epoch: 49| Step: 0
Training loss: 1.7407712936401367
Validation loss: 2.5402068604705152

Epoch: 6| Step: 1
Training loss: 3.2328248023986816
Validation loss: 2.5343229232295865

Epoch: 6| Step: 2
Training loss: 2.7092771530151367
Validation loss: 2.530771552875478

Epoch: 6| Step: 3
Training loss: 2.1351470947265625
Validation loss: 2.5311805073932936

Epoch: 6| Step: 4
Training loss: 2.6803197860717773
Validation loss: 2.532111806254233

Epoch: 6| Step: 5
Training loss: 2.703385829925537
Validation loss: 2.5311365332654727

Epoch: 6| Step: 6
Training loss: 3.2584173679351807
Validation loss: 2.529788153145903

Epoch: 6| Step: 7
Training loss: 2.938107490539551
Validation loss: 2.5280431547472553

Epoch: 6| Step: 8
Training loss: 3.1639883518218994
Validation loss: 2.529088338216146

Epoch: 6| Step: 9
Training loss: 3.010695457458496
Validation loss: 2.5292173816311743

Epoch: 6| Step: 10
Training loss: 2.720639705657959
Validation loss: 2.5305924005405878

Epoch: 6| Step: 11
Training loss: 2.6313185691833496
Validation loss: 2.5302298761183217

Epoch: 6| Step: 12
Training loss: 3.197755813598633
Validation loss: 2.5257971568774154

Epoch: 6| Step: 13
Training loss: 2.0014395713806152
Validation loss: 2.5322578619885188

Epoch: 50| Step: 0
Training loss: 2.8505444526672363
Validation loss: 2.531348297672887

Epoch: 6| Step: 1
Training loss: 2.7046971321105957
Validation loss: 2.527831836413312

Epoch: 6| Step: 2
Training loss: 2.7559046745300293
Validation loss: 2.5295646011188464

Epoch: 6| Step: 3
Training loss: 2.2760796546936035
Validation loss: 2.530736207962036

Epoch: 6| Step: 4
Training loss: 3.0353152751922607
Validation loss: 2.5338199446278233

Epoch: 6| Step: 5
Training loss: 2.324819326400757
Validation loss: 2.5292698978095927

Epoch: 6| Step: 6
Training loss: 2.8782825469970703
Validation loss: 2.531657013841855

Epoch: 6| Step: 7
Training loss: 2.8193399906158447
Validation loss: 2.531333887448875

Epoch: 6| Step: 8
Training loss: 2.5769617557525635
Validation loss: 2.531215880506782

Epoch: 6| Step: 9
Training loss: 2.878962993621826
Validation loss: 2.5322378348278742

Epoch: 6| Step: 10
Training loss: 3.4777164459228516
Validation loss: 2.532918501925725

Epoch: 6| Step: 11
Training loss: 2.4713597297668457
Validation loss: 2.532002354180941

Epoch: 6| Step: 12
Training loss: 2.7533390522003174
Validation loss: 2.527020513370473

Epoch: 6| Step: 13
Training loss: 2.4555909633636475
Validation loss: 2.523062398356776

Epoch: 51| Step: 0
Training loss: 1.7372193336486816
Validation loss: 2.5242211690513034

Epoch: 6| Step: 1
Training loss: 2.9438912868499756
Validation loss: 2.5211966806842434

Epoch: 6| Step: 2
Training loss: 2.5905139446258545
Validation loss: 2.5174981394121723

Epoch: 6| Step: 3
Training loss: 2.245833396911621
Validation loss: 2.5215554903912287

Epoch: 6| Step: 4
Training loss: 2.9328396320343018
Validation loss: 2.5213931555389077

Epoch: 6| Step: 5
Training loss: 2.01481032371521
Validation loss: 2.520637148170061

Epoch: 6| Step: 6
Training loss: 2.91697359085083
Validation loss: 2.521300659384779

Epoch: 6| Step: 7
Training loss: 2.4603214263916016
Validation loss: 2.528780791067308

Epoch: 6| Step: 8
Training loss: 2.5784902572631836
Validation loss: 2.5242875852892475

Epoch: 6| Step: 9
Training loss: 3.1876330375671387
Validation loss: 2.526654258851082

Epoch: 6| Step: 10
Training loss: 3.2857654094696045
Validation loss: 2.528398847067228

Epoch: 6| Step: 11
Training loss: 3.3874075412750244
Validation loss: 2.519375879277465

Epoch: 6| Step: 12
Training loss: 2.9257445335388184
Validation loss: 2.5130187490934968

Epoch: 6| Step: 13
Training loss: 3.450090169906616
Validation loss: 2.513054588789581

Epoch: 52| Step: 0
Training loss: 1.7582025527954102
Validation loss: 2.5175043306043072

Epoch: 6| Step: 1
Training loss: 3.1166343688964844
Validation loss: 2.517553185903898

Epoch: 6| Step: 2
Training loss: 2.872429847717285
Validation loss: 2.5134218098014913

Epoch: 6| Step: 3
Training loss: 2.8035993576049805
Validation loss: 2.511423918508714

Epoch: 6| Step: 4
Training loss: 3.627779960632324
Validation loss: 2.515361924325266

Epoch: 6| Step: 5
Training loss: 2.647479772567749
Validation loss: 2.51279386397331

Epoch: 6| Step: 6
Training loss: 2.7918155193328857
Validation loss: 2.512101129818988

Epoch: 6| Step: 7
Training loss: 2.977297067642212
Validation loss: 2.511306050003216

Epoch: 6| Step: 8
Training loss: 2.9236669540405273
Validation loss: 2.511512989638954

Epoch: 6| Step: 9
Training loss: 2.377091407775879
Validation loss: 2.5112579586685344

Epoch: 6| Step: 10
Training loss: 2.3058395385742188
Validation loss: 2.51422441902981

Epoch: 6| Step: 11
Training loss: 2.2534923553466797
Validation loss: 2.518348111901232

Epoch: 6| Step: 12
Training loss: 3.0557351112365723
Validation loss: 2.516316651016153

Epoch: 6| Step: 13
Training loss: 2.7702603340148926
Validation loss: 2.516512647751839

Epoch: 53| Step: 0
Training loss: 2.6889567375183105
Validation loss: 2.5150888325065694

Epoch: 6| Step: 1
Training loss: 2.510953664779663
Validation loss: 2.5144928988590034

Epoch: 6| Step: 2
Training loss: 3.3060965538024902
Validation loss: 2.5198613264227427

Epoch: 6| Step: 3
Training loss: 2.6898255348205566
Validation loss: 2.5164989066380326

Epoch: 6| Step: 4
Training loss: 2.870098114013672
Validation loss: 2.519521010819302

Epoch: 6| Step: 5
Training loss: 2.800957202911377
Validation loss: 2.5062073584525817

Epoch: 6| Step: 6
Training loss: 2.8578104972839355
Validation loss: 2.508072499305971

Epoch: 6| Step: 7
Training loss: 3.0488014221191406
Validation loss: 2.5055929999197684

Epoch: 6| Step: 8
Training loss: 2.6453027725219727
Validation loss: 2.519369581694244

Epoch: 6| Step: 9
Training loss: 2.0230042934417725
Validation loss: 2.5130077638933734

Epoch: 6| Step: 10
Training loss: 2.9883008003234863
Validation loss: 2.512416729363062

Epoch: 6| Step: 11
Training loss: 2.962940216064453
Validation loss: 2.5186595839838826

Epoch: 6| Step: 12
Training loss: 1.9521517753601074
Validation loss: 2.52093324353618

Epoch: 6| Step: 13
Training loss: 3.11099910736084
Validation loss: 2.519903887984573

Epoch: 54| Step: 0
Training loss: 2.906242847442627
Validation loss: 2.5184513702187488

Epoch: 6| Step: 1
Training loss: 2.162001132965088
Validation loss: 2.5139559750915854

Epoch: 6| Step: 2
Training loss: 3.21762752532959
Validation loss: 2.5067253035883748

Epoch: 6| Step: 3
Training loss: 2.5744919776916504
Validation loss: 2.506378783974596

Epoch: 6| Step: 4
Training loss: 2.4109208583831787
Validation loss: 2.510874443156745

Epoch: 6| Step: 5
Training loss: 3.2606170177459717
Validation loss: 2.5093887877720658

Epoch: 6| Step: 6
Training loss: 2.9551687240600586
Validation loss: 2.5124851452407015

Epoch: 6| Step: 7
Training loss: 2.1342906951904297
Validation loss: 2.516903961858442

Epoch: 6| Step: 8
Training loss: 2.8656394481658936
Validation loss: 2.5131788305056992

Epoch: 6| Step: 9
Training loss: 2.7636301517486572
Validation loss: 2.510883072371124

Epoch: 6| Step: 10
Training loss: 1.9154850244522095
Validation loss: 2.505730277748518

Epoch: 6| Step: 11
Training loss: 3.0467424392700195
Validation loss: 2.5036607967909945

Epoch: 6| Step: 12
Training loss: 2.4546563625335693
Validation loss: 2.498489543955813

Epoch: 6| Step: 13
Training loss: 4.11970853805542
Validation loss: 2.4963383290075485

Epoch: 55| Step: 0
Training loss: 2.378793239593506
Validation loss: 2.497731525410888

Epoch: 6| Step: 1
Training loss: 3.045926332473755
Validation loss: 2.4998304177356023

Epoch: 6| Step: 2
Training loss: 3.136951446533203
Validation loss: 2.500468627099068

Epoch: 6| Step: 3
Training loss: 2.5899362564086914
Validation loss: 2.508709235857892

Epoch: 6| Step: 4
Training loss: 2.673165798187256
Validation loss: 2.5084450526904036

Epoch: 6| Step: 5
Training loss: 2.7843990325927734
Validation loss: 2.5255074603583223

Epoch: 6| Step: 6
Training loss: 2.384953260421753
Validation loss: 2.528280799106885

Epoch: 6| Step: 7
Training loss: 2.1520578861236572
Validation loss: 2.5273851169052945

Epoch: 6| Step: 8
Training loss: 2.667593002319336
Validation loss: 2.5249587951167936

Epoch: 6| Step: 9
Training loss: 3.19484281539917
Validation loss: 2.514388176702684

Epoch: 6| Step: 10
Training loss: 2.6313369274139404
Validation loss: 2.5055446573483047

Epoch: 6| Step: 11
Training loss: 3.0087392330169678
Validation loss: 2.498171721735308

Epoch: 6| Step: 12
Training loss: 2.616605520248413
Validation loss: 2.4961081935513403

Epoch: 6| Step: 13
Training loss: 3.044405460357666
Validation loss: 2.4929085470015004

Epoch: 56| Step: 0
Training loss: 2.407392740249634
Validation loss: 2.4953914278297016

Epoch: 6| Step: 1
Training loss: 3.0026330947875977
Validation loss: 2.4963481836421515

Epoch: 6| Step: 2
Training loss: 2.6072452068328857
Validation loss: 2.500767989825177

Epoch: 6| Step: 3
Training loss: 2.986666202545166
Validation loss: 2.5017021035635345

Epoch: 6| Step: 4
Training loss: 2.9669222831726074
Validation loss: 2.5027287134560208

Epoch: 6| Step: 5
Training loss: 3.563286066055298
Validation loss: 2.5089832505872174

Epoch: 6| Step: 6
Training loss: 2.666938304901123
Validation loss: 2.505038062731425

Epoch: 6| Step: 7
Training loss: 2.8788223266601562
Validation loss: 2.506076697380312

Epoch: 6| Step: 8
Training loss: 1.7879047393798828
Validation loss: 2.5168332848497617

Epoch: 6| Step: 9
Training loss: 2.451477289199829
Validation loss: 2.5329526701281146

Epoch: 6| Step: 10
Training loss: 2.937124252319336
Validation loss: 2.546435422794793

Epoch: 6| Step: 11
Training loss: 2.339569330215454
Validation loss: 2.53538159144822

Epoch: 6| Step: 12
Training loss: 2.6776437759399414
Validation loss: 2.4964034300978466

Epoch: 6| Step: 13
Training loss: 3.15777325630188
Validation loss: 2.4946381763745378

Epoch: 57| Step: 0
Training loss: 2.7505550384521484
Validation loss: 2.4904211721112652

Epoch: 6| Step: 1
Training loss: 2.965036392211914
Validation loss: 2.4987469129664923

Epoch: 6| Step: 2
Training loss: 2.5367231369018555
Validation loss: 2.510337254052521

Epoch: 6| Step: 3
Training loss: 3.013753652572632
Validation loss: 2.5242926843704714

Epoch: 6| Step: 4
Training loss: 2.577133893966675
Validation loss: 2.5252904584330897

Epoch: 6| Step: 5
Training loss: 2.5133557319641113
Validation loss: 2.524857436456988

Epoch: 6| Step: 6
Training loss: 2.111238956451416
Validation loss: 2.517831413976608

Epoch: 6| Step: 7
Training loss: 2.9105327129364014
Validation loss: 2.511162637382425

Epoch: 6| Step: 8
Training loss: 1.6852898597717285
Validation loss: 2.500940804840416

Epoch: 6| Step: 9
Training loss: 2.739807605743408
Validation loss: 2.497741565909437

Epoch: 6| Step: 10
Training loss: 2.6832385063171387
Validation loss: 2.493112967860314

Epoch: 6| Step: 11
Training loss: 3.400674819946289
Validation loss: 2.497165536367765

Epoch: 6| Step: 12
Training loss: 3.430562973022461
Validation loss: 2.496937446696784

Epoch: 6| Step: 13
Training loss: 3.0860776901245117
Validation loss: 2.4994750894526

Epoch: 58| Step: 0
Training loss: 2.5822033882141113
Validation loss: 2.496060771326865

Epoch: 6| Step: 1
Training loss: 2.9157485961914062
Validation loss: 2.5018621542120494

Epoch: 6| Step: 2
Training loss: 2.7248611450195312
Validation loss: 2.5008750269489903

Epoch: 6| Step: 3
Training loss: 3.2711634635925293
Validation loss: 2.504952282033941

Epoch: 6| Step: 4
Training loss: 2.6988601684570312
Validation loss: 2.505031757457282

Epoch: 6| Step: 5
Training loss: 2.608452320098877
Validation loss: 2.5192183217694684

Epoch: 6| Step: 6
Training loss: 2.660902738571167
Validation loss: 2.510037778526224

Epoch: 6| Step: 7
Training loss: 3.1045565605163574
Validation loss: 2.506392619943106

Epoch: 6| Step: 8
Training loss: 2.5189037322998047
Validation loss: 2.509087803543255

Epoch: 6| Step: 9
Training loss: 2.3919196128845215
Validation loss: 2.505935384381202

Epoch: 6| Step: 10
Training loss: 2.059741735458374
Validation loss: 2.5025552652215444

Epoch: 6| Step: 11
Training loss: 2.677215814590454
Validation loss: 2.498933579332085

Epoch: 6| Step: 12
Training loss: 2.954521656036377
Validation loss: 2.502621299477034

Epoch: 6| Step: 13
Training loss: 3.2095048427581787
Validation loss: 2.500074932652135

Epoch: 59| Step: 0
Training loss: 3.4005990028381348
Validation loss: 2.4945967812691965

Epoch: 6| Step: 1
Training loss: 3.005385637283325
Validation loss: 2.4949951915330786

Epoch: 6| Step: 2
Training loss: 2.2171096801757812
Validation loss: 2.4928527237266622

Epoch: 6| Step: 3
Training loss: 3.098198413848877
Validation loss: 2.4998178917874574

Epoch: 6| Step: 4
Training loss: 2.5060582160949707
Validation loss: 2.494646312088095

Epoch: 6| Step: 5
Training loss: 3.189364433288574
Validation loss: 2.4937628956251245

Epoch: 6| Step: 6
Training loss: 2.83115816116333
Validation loss: 2.4936495416907856

Epoch: 6| Step: 7
Training loss: 2.7760651111602783
Validation loss: 2.4879925648371377

Epoch: 6| Step: 8
Training loss: 2.5752716064453125
Validation loss: 2.4854062911002868

Epoch: 6| Step: 9
Training loss: 3.0557172298431396
Validation loss: 2.4816100571745183

Epoch: 6| Step: 10
Training loss: 2.0146021842956543
Validation loss: 2.4869654742620324

Epoch: 6| Step: 11
Training loss: 2.097689628601074
Validation loss: 2.4857113566449893

Epoch: 6| Step: 12
Training loss: 2.4574155807495117
Validation loss: 2.484702258981684

Epoch: 6| Step: 13
Training loss: 2.7600960731506348
Validation loss: 2.482869230290895

Epoch: 60| Step: 0
Training loss: 1.9613876342773438
Validation loss: 2.48227515784643

Epoch: 6| Step: 1
Training loss: 2.431178092956543
Validation loss: 2.481234479975957

Epoch: 6| Step: 2
Training loss: 2.1883058547973633
Validation loss: 2.4840261859278523

Epoch: 6| Step: 3
Training loss: 2.9658188819885254
Validation loss: 2.4885596254820466

Epoch: 6| Step: 4
Training loss: 3.030505657196045
Validation loss: 2.4907740392992572

Epoch: 6| Step: 5
Training loss: 3.0532145500183105
Validation loss: 2.493887516760057

Epoch: 6| Step: 6
Training loss: 2.43562650680542
Validation loss: 2.493426469064528

Epoch: 6| Step: 7
Training loss: 2.886601209640503
Validation loss: 2.491868967651039

Epoch: 6| Step: 8
Training loss: 2.3832623958587646
Validation loss: 2.4910622924886723

Epoch: 6| Step: 9
Training loss: 2.905993938446045
Validation loss: 2.4859368416570846

Epoch: 6| Step: 10
Training loss: 3.2756948471069336
Validation loss: 2.4840646174646195

Epoch: 6| Step: 11
Training loss: 2.901236057281494
Validation loss: 2.48090107979313

Epoch: 6| Step: 12
Training loss: 3.0267090797424316
Validation loss: 2.480548192096013

Epoch: 6| Step: 13
Training loss: 2.417228937149048
Validation loss: 2.477418279135099

Epoch: 61| Step: 0
Training loss: 2.4874167442321777
Validation loss: 2.4798298446081017

Epoch: 6| Step: 1
Training loss: 3.4867489337921143
Validation loss: 2.4822335986680883

Epoch: 6| Step: 2
Training loss: 3.214583396911621
Validation loss: 2.483822076551376

Epoch: 6| Step: 3
Training loss: 2.6701698303222656
Validation loss: 2.482490590823594

Epoch: 6| Step: 4
Training loss: 2.7802183628082275
Validation loss: 2.4885479916808424

Epoch: 6| Step: 5
Training loss: 2.872523307800293
Validation loss: 2.4934835613414807

Epoch: 6| Step: 6
Training loss: 2.911410331726074
Validation loss: 2.4896129023644233

Epoch: 6| Step: 7
Training loss: 3.026524543762207
Validation loss: 2.4902766776341263

Epoch: 6| Step: 8
Training loss: 2.2281579971313477
Validation loss: 2.493135134379069

Epoch: 6| Step: 9
Training loss: 2.7555418014526367
Validation loss: 2.493243519977857

Epoch: 6| Step: 10
Training loss: 2.162888288497925
Validation loss: 2.495180972160832

Epoch: 6| Step: 11
Training loss: 2.4227848052978516
Validation loss: 2.4973059546562935

Epoch: 6| Step: 12
Training loss: 2.2671260833740234
Validation loss: 2.494204649361231

Epoch: 6| Step: 13
Training loss: 2.5440585613250732
Validation loss: 2.4927853358689176

Epoch: 62| Step: 0
Training loss: 1.9401671886444092
Validation loss: 2.4821395976569063

Epoch: 6| Step: 1
Training loss: 2.6466164588928223
Validation loss: 2.4725136654351347

Epoch: 6| Step: 2
Training loss: 3.0728912353515625
Validation loss: 2.4798386660955285

Epoch: 6| Step: 3
Training loss: 2.930497646331787
Validation loss: 2.4803830244207896

Epoch: 6| Step: 4
Training loss: 2.335326671600342
Validation loss: 2.4855346936051563

Epoch: 6| Step: 5
Training loss: 3.5041935443878174
Validation loss: 2.4849741433256414

Epoch: 6| Step: 6
Training loss: 3.096966028213501
Validation loss: 2.486240258780859

Epoch: 6| Step: 7
Training loss: 2.192095994949341
Validation loss: 2.483355536255785

Epoch: 6| Step: 8
Training loss: 2.4396796226501465
Validation loss: 2.4834469646535893

Epoch: 6| Step: 9
Training loss: 2.9461891651153564
Validation loss: 2.482019829493697

Epoch: 6| Step: 10
Training loss: 3.325505256652832
Validation loss: 2.483318672385267

Epoch: 6| Step: 11
Training loss: 2.393681049346924
Validation loss: 2.482575447328629

Epoch: 6| Step: 12
Training loss: 2.3934078216552734
Validation loss: 2.4814863204956055

Epoch: 6| Step: 13
Training loss: 2.6910765171051025
Validation loss: 2.4772505452555995

Epoch: 63| Step: 0
Training loss: 3.3754396438598633
Validation loss: 2.4782710408651702

Epoch: 6| Step: 1
Training loss: 2.7311792373657227
Validation loss: 2.4745175428287958

Epoch: 6| Step: 2
Training loss: 1.758087396621704
Validation loss: 2.4735625072192122

Epoch: 6| Step: 3
Training loss: 2.8656630516052246
Validation loss: 2.476643459771269

Epoch: 6| Step: 4
Training loss: 2.9072346687316895
Validation loss: 2.4788730221409954

Epoch: 6| Step: 5
Training loss: 3.0102107524871826
Validation loss: 2.478320147401543

Epoch: 6| Step: 6
Training loss: 2.5412793159484863
Validation loss: 2.4757318753068165

Epoch: 6| Step: 7
Training loss: 2.2862846851348877
Validation loss: 2.47950941260143

Epoch: 6| Step: 8
Training loss: 2.709331512451172
Validation loss: 2.4810993671417236

Epoch: 6| Step: 9
Training loss: 2.7505903244018555
Validation loss: 2.4818365112427743

Epoch: 6| Step: 10
Training loss: 2.127725124359131
Validation loss: 2.479814032072662

Epoch: 6| Step: 11
Training loss: 2.7431435585021973
Validation loss: 2.485774632423155

Epoch: 6| Step: 12
Training loss: 3.1455941200256348
Validation loss: 2.4853381162048667

Epoch: 6| Step: 13
Training loss: 3.092003107070923
Validation loss: 2.487557790612662

Epoch: 64| Step: 0
Training loss: 2.6836419105529785
Validation loss: 2.4866194007217244

Epoch: 6| Step: 1
Training loss: 2.639740467071533
Validation loss: 2.4828890677421325

Epoch: 6| Step: 2
Training loss: 2.625215530395508
Validation loss: 2.476837488912767

Epoch: 6| Step: 3
Training loss: 2.3838565349578857
Validation loss: 2.474732106731784

Epoch: 6| Step: 4
Training loss: 2.846738338470459
Validation loss: 2.4744061167522142

Epoch: 6| Step: 5
Training loss: 2.1514699459075928
Validation loss: 2.473924498404226

Epoch: 6| Step: 6
Training loss: 2.6693077087402344
Validation loss: 2.4741653370600876

Epoch: 6| Step: 7
Training loss: 3.6542701721191406
Validation loss: 2.476826754949426

Epoch: 6| Step: 8
Training loss: 2.7860710620880127
Validation loss: 2.477638442029235

Epoch: 6| Step: 9
Training loss: 2.576016426086426
Validation loss: 2.4784929880531887

Epoch: 6| Step: 10
Training loss: 3.2152938842773438
Validation loss: 2.4767470795621156

Epoch: 6| Step: 11
Training loss: 2.6952288150787354
Validation loss: 2.4851083165855816

Epoch: 6| Step: 12
Training loss: 2.1133127212524414
Validation loss: 2.4859388951332337

Epoch: 6| Step: 13
Training loss: 2.9203805923461914
Validation loss: 2.498229395958685

Epoch: 65| Step: 0
Training loss: 2.3898727893829346
Validation loss: 2.4990581722669702

Epoch: 6| Step: 1
Training loss: 1.8770990371704102
Validation loss: 2.5044022042264222

Epoch: 6| Step: 2
Training loss: 2.727522850036621
Validation loss: 2.4923708541418916

Epoch: 6| Step: 3
Training loss: 2.5864527225494385
Validation loss: 2.488378999053791

Epoch: 6| Step: 4
Training loss: 3.1586151123046875
Validation loss: 2.4867264532273814

Epoch: 6| Step: 5
Training loss: 3.009272575378418
Validation loss: 2.4791676023954987

Epoch: 6| Step: 6
Training loss: 2.2573983669281006
Validation loss: 2.4750775855074645

Epoch: 6| Step: 7
Training loss: 3.01486873626709
Validation loss: 2.479187544956002

Epoch: 6| Step: 8
Training loss: 2.9147796630859375
Validation loss: 2.479200206777101

Epoch: 6| Step: 9
Training loss: 2.9813241958618164
Validation loss: 2.4821942929298646

Epoch: 6| Step: 10
Training loss: 2.8467955589294434
Validation loss: 2.484496767802905

Epoch: 6| Step: 11
Training loss: 3.2151246070861816
Validation loss: 2.4875328438256377

Epoch: 6| Step: 12
Training loss: 2.2791805267333984
Validation loss: 2.4834769028489307

Epoch: 6| Step: 13
Training loss: 2.470613956451416
Validation loss: 2.478939986998035

Epoch: 66| Step: 0
Training loss: 2.3381495475769043
Validation loss: 2.4756055365326586

Epoch: 6| Step: 1
Training loss: 2.8354687690734863
Validation loss: 2.4819312916007092

Epoch: 6| Step: 2
Training loss: 2.5751564502716064
Validation loss: 2.4957864387061006

Epoch: 6| Step: 3
Training loss: 3.4071836471557617
Validation loss: 2.527291646567724

Epoch: 6| Step: 4
Training loss: 2.4050655364990234
Validation loss: 2.501752415011006

Epoch: 6| Step: 5
Training loss: 2.749706268310547
Validation loss: 2.4982953789413616

Epoch: 6| Step: 6
Training loss: 3.2895641326904297
Validation loss: 2.4896956079749653

Epoch: 6| Step: 7
Training loss: 3.0068020820617676
Validation loss: 2.484883064864784

Epoch: 6| Step: 8
Training loss: 2.42739200592041
Validation loss: 2.4785511955138175

Epoch: 6| Step: 9
Training loss: 2.160545587539673
Validation loss: 2.485114694923483

Epoch: 6| Step: 10
Training loss: 2.8512954711914062
Validation loss: 2.482916914006715

Epoch: 6| Step: 11
Training loss: 2.8696670532226562
Validation loss: 2.489622321180118

Epoch: 6| Step: 12
Training loss: 2.2924246788024902
Validation loss: 2.494016308938303

Epoch: 6| Step: 13
Training loss: 2.538620948791504
Validation loss: 2.4875653943707867

Epoch: 67| Step: 0
Training loss: 2.756040573120117
Validation loss: 2.4852568616149244

Epoch: 6| Step: 1
Training loss: 3.083251953125
Validation loss: 2.482474660360685

Epoch: 6| Step: 2
Training loss: 2.9790000915527344
Validation loss: 2.468155325099986

Epoch: 6| Step: 3
Training loss: 2.083855152130127
Validation loss: 2.4656218636420464

Epoch: 6| Step: 4
Training loss: 2.8162240982055664
Validation loss: 2.465191625779675

Epoch: 6| Step: 5
Training loss: 3.2101240158081055
Validation loss: 2.46840335476783

Epoch: 6| Step: 6
Training loss: 3.147310733795166
Validation loss: 2.4693747464046685

Epoch: 6| Step: 7
Training loss: 2.0339255332946777
Validation loss: 2.4635537285958566

Epoch: 6| Step: 8
Training loss: 2.7420287132263184
Validation loss: 2.466723054967901

Epoch: 6| Step: 9
Training loss: 2.76755952835083
Validation loss: 2.4668775091889086

Epoch: 6| Step: 10
Training loss: 2.3146424293518066
Validation loss: 2.471990221290178

Epoch: 6| Step: 11
Training loss: 2.375324249267578
Validation loss: 2.475147206296203

Epoch: 6| Step: 12
Training loss: 2.732451915740967
Validation loss: 2.4789610062876055

Epoch: 6| Step: 13
Training loss: 2.977158308029175
Validation loss: 2.475555189194218

Epoch: 68| Step: 0
Training loss: 1.777735710144043
Validation loss: 2.4718288811304236

Epoch: 6| Step: 1
Training loss: 2.463469982147217
Validation loss: 2.475153761525308

Epoch: 6| Step: 2
Training loss: 2.907545566558838
Validation loss: 2.4697037845529537

Epoch: 6| Step: 3
Training loss: 3.5351643562316895
Validation loss: 2.4681931746903287

Epoch: 6| Step: 4
Training loss: 2.1344809532165527
Validation loss: 2.4675146918142996

Epoch: 6| Step: 5
Training loss: 2.812288284301758
Validation loss: 2.464921635966147

Epoch: 6| Step: 6
Training loss: 2.5101795196533203
Validation loss: 2.4621190588961364

Epoch: 6| Step: 7
Training loss: 2.5321216583251953
Validation loss: 2.465153140406455

Epoch: 6| Step: 8
Training loss: 2.6082301139831543
Validation loss: 2.4644507233814528

Epoch: 6| Step: 9
Training loss: 3.192131757736206
Validation loss: 2.4629108085427234

Epoch: 6| Step: 10
Training loss: 3.077603340148926
Validation loss: 2.4646341467416413

Epoch: 6| Step: 11
Training loss: 2.915858507156372
Validation loss: 2.467228320337111

Epoch: 6| Step: 12
Training loss: 2.952293634414673
Validation loss: 2.4666437077265915

Epoch: 6| Step: 13
Training loss: 2.135985851287842
Validation loss: 2.4657298928947857

Epoch: 69| Step: 0
Training loss: 2.265512466430664
Validation loss: 2.46595343466728

Epoch: 6| Step: 1
Training loss: 3.1431796550750732
Validation loss: 2.466844884298181

Epoch: 6| Step: 2
Training loss: 2.3099303245544434
Validation loss: 2.465655613971013

Epoch: 6| Step: 3
Training loss: 2.780308961868286
Validation loss: 2.46846757652939

Epoch: 6| Step: 4
Training loss: 2.786750555038452
Validation loss: 2.466303017831618

Epoch: 6| Step: 5
Training loss: 2.978776454925537
Validation loss: 2.4671846948644167

Epoch: 6| Step: 6
Training loss: 3.047488212585449
Validation loss: 2.464746464965164

Epoch: 6| Step: 7
Training loss: 2.5152862071990967
Validation loss: 2.4642534204708633

Epoch: 6| Step: 8
Training loss: 2.33056640625
Validation loss: 2.4653074510635866

Epoch: 6| Step: 9
Training loss: 2.9124574661254883
Validation loss: 2.46251009356591

Epoch: 6| Step: 10
Training loss: 2.6617794036865234
Validation loss: 2.4600667363853863

Epoch: 6| Step: 11
Training loss: 2.7005419731140137
Validation loss: 2.4598696283114854

Epoch: 6| Step: 12
Training loss: 2.8639004230499268
Validation loss: 2.4614672378827165

Epoch: 6| Step: 13
Training loss: 2.268573760986328
Validation loss: 2.4564258949730986

Epoch: 70| Step: 0
Training loss: 2.083515167236328
Validation loss: 2.457401321780297

Epoch: 6| Step: 1
Training loss: 3.1157102584838867
Validation loss: 2.457519192849436

Epoch: 6| Step: 2
Training loss: 3.0370283126831055
Validation loss: 2.4537263224201817

Epoch: 6| Step: 3
Training loss: 2.989834785461426
Validation loss: 2.453029155731201

Epoch: 6| Step: 4
Training loss: 2.6898703575134277
Validation loss: 2.4578957044950096

Epoch: 6| Step: 5
Training loss: 2.609865188598633
Validation loss: 2.453252964122321

Epoch: 6| Step: 6
Training loss: 2.77443790435791
Validation loss: 2.455406645292877

Epoch: 6| Step: 7
Training loss: 2.9111366271972656
Validation loss: 2.4543315774650982

Epoch: 6| Step: 8
Training loss: 2.777066469192505
Validation loss: 2.4540794177721907

Epoch: 6| Step: 9
Training loss: 2.6619415283203125
Validation loss: 2.4563259437520015

Epoch: 6| Step: 10
Training loss: 2.3884716033935547
Validation loss: 2.4659715134610414

Epoch: 6| Step: 11
Training loss: 2.615025043487549
Validation loss: 2.46683935452533

Epoch: 6| Step: 12
Training loss: 2.711297035217285
Validation loss: 2.470529346055882

Epoch: 6| Step: 13
Training loss: 2.0901918411254883
Validation loss: 2.4739749380337295

Epoch: 71| Step: 0
Training loss: 2.8202240467071533
Validation loss: 2.4715141301514

Epoch: 6| Step: 1
Training loss: 3.0494000911712646
Validation loss: 2.472989620700959

Epoch: 6| Step: 2
Training loss: 2.581400156021118
Validation loss: 2.4748160326352684

Epoch: 6| Step: 3
Training loss: 2.814342737197876
Validation loss: 2.4762525173925583

Epoch: 6| Step: 4
Training loss: 2.097683906555176
Validation loss: 2.4712378235273462

Epoch: 6| Step: 5
Training loss: 2.800138473510742
Validation loss: 2.46862466104569

Epoch: 6| Step: 6
Training loss: 2.8398311138153076
Validation loss: 2.4594128362594114

Epoch: 6| Step: 7
Training loss: 3.061462640762329
Validation loss: 2.4553847543654905

Epoch: 6| Step: 8
Training loss: 2.474599838256836
Validation loss: 2.4509238043139057

Epoch: 6| Step: 9
Training loss: 1.8340888023376465
Validation loss: 2.4476628585528304

Epoch: 6| Step: 10
Training loss: 3.354482412338257
Validation loss: 2.4481510193117204

Epoch: 6| Step: 11
Training loss: 2.9748377799987793
Validation loss: 2.4508488101343953

Epoch: 6| Step: 12
Training loss: 2.252329111099243
Validation loss: 2.4505060616359917

Epoch: 6| Step: 13
Training loss: 2.762871742248535
Validation loss: 2.4521604609745804

Epoch: 72| Step: 0
Training loss: 2.5465598106384277
Validation loss: 2.4505530736779653

Epoch: 6| Step: 1
Training loss: 2.3591268062591553
Validation loss: 2.450455242587674

Epoch: 6| Step: 2
Training loss: 2.7594099044799805
Validation loss: 2.449400394193588

Epoch: 6| Step: 3
Training loss: 2.248924493789673
Validation loss: 2.449099927820185

Epoch: 6| Step: 4
Training loss: 2.195035934448242
Validation loss: 2.4490176272648636

Epoch: 6| Step: 5
Training loss: 2.616393566131592
Validation loss: 2.4494419379900862

Epoch: 6| Step: 6
Training loss: 3.1090710163116455
Validation loss: 2.4505343462831233

Epoch: 6| Step: 7
Training loss: 2.42978835105896
Validation loss: 2.453219690630513

Epoch: 6| Step: 8
Training loss: 2.7259891033172607
Validation loss: 2.456668328213435

Epoch: 6| Step: 9
Training loss: 3.439452648162842
Validation loss: 2.462558565601226

Epoch: 6| Step: 10
Training loss: 2.569282293319702
Validation loss: 2.460870732543289

Epoch: 6| Step: 11
Training loss: 2.848668336868286
Validation loss: 2.4513571518723682

Epoch: 6| Step: 12
Training loss: 2.7589950561523438
Validation loss: 2.457437592168008

Epoch: 6| Step: 13
Training loss: 3.385970115661621
Validation loss: 2.4541020290825957

Epoch: 73| Step: 0
Training loss: 2.981100082397461
Validation loss: 2.4534412404542327

Epoch: 6| Step: 1
Training loss: 2.6643998622894287
Validation loss: 2.451668591909511

Epoch: 6| Step: 2
Training loss: 2.097975254058838
Validation loss: 2.4521098341993106

Epoch: 6| Step: 3
Training loss: 2.8005282878875732
Validation loss: 2.453232196069533

Epoch: 6| Step: 4
Training loss: 3.4167089462280273
Validation loss: 2.45945389809147

Epoch: 6| Step: 5
Training loss: 2.080735206604004
Validation loss: 2.4614736239115396

Epoch: 6| Step: 6
Training loss: 3.245314121246338
Validation loss: 2.4680659847874797

Epoch: 6| Step: 7
Training loss: 2.583225727081299
Validation loss: 2.4650817776239045

Epoch: 6| Step: 8
Training loss: 2.397904634475708
Validation loss: 2.463437477747599

Epoch: 6| Step: 9
Training loss: 2.6118311882019043
Validation loss: 2.463731212000693

Epoch: 6| Step: 10
Training loss: 2.6816353797912598
Validation loss: 2.4656117808434272

Epoch: 6| Step: 11
Training loss: 2.7918248176574707
Validation loss: 2.4636185502493255

Epoch: 6| Step: 12
Training loss: 2.556337833404541
Validation loss: 2.4646984659215456

Epoch: 6| Step: 13
Training loss: 2.496563673019409
Validation loss: 2.461807538104314

Epoch: 74| Step: 0
Training loss: 3.023210287094116
Validation loss: 2.4612921220000072

Epoch: 6| Step: 1
Training loss: 2.3420352935791016
Validation loss: 2.44968500444966

Epoch: 6| Step: 2
Training loss: 2.3160033226013184
Validation loss: 2.455430171822989

Epoch: 6| Step: 3
Training loss: 2.912212371826172
Validation loss: 2.4528254078280542

Epoch: 6| Step: 4
Training loss: 2.0093190670013428
Validation loss: 2.4476031154714604

Epoch: 6| Step: 5
Training loss: 2.3774595260620117
Validation loss: 2.4477663860526135

Epoch: 6| Step: 6
Training loss: 2.7068886756896973
Validation loss: 2.44272933724106

Epoch: 6| Step: 7
Training loss: 2.6280245780944824
Validation loss: 2.4441383192616124

Epoch: 6| Step: 8
Training loss: 2.86114501953125
Validation loss: 2.44537789078169

Epoch: 6| Step: 9
Training loss: 2.7959864139556885
Validation loss: 2.4569666308741414

Epoch: 6| Step: 10
Training loss: 2.424424648284912
Validation loss: 2.4538941178270566

Epoch: 6| Step: 11
Training loss: 2.964885711669922
Validation loss: 2.451548722482497

Epoch: 6| Step: 12
Training loss: 3.000687599182129
Validation loss: 2.450583924529373

Epoch: 6| Step: 13
Training loss: 3.4197115898132324
Validation loss: 2.4475851315324024

Epoch: 75| Step: 0
Training loss: 3.585850715637207
Validation loss: 2.4461562402786745

Epoch: 6| Step: 1
Training loss: 2.4780080318450928
Validation loss: 2.4448841951226674

Epoch: 6| Step: 2
Training loss: 3.124861717224121
Validation loss: 2.4442062416384296

Epoch: 6| Step: 3
Training loss: 2.2732934951782227
Validation loss: 2.436393532701718

Epoch: 6| Step: 4
Training loss: 2.383963108062744
Validation loss: 2.4391167804759037

Epoch: 6| Step: 5
Training loss: 3.2155919075012207
Validation loss: 2.445678849374094

Epoch: 6| Step: 6
Training loss: 3.2457194328308105
Validation loss: 2.4490594402436288

Epoch: 6| Step: 7
Training loss: 1.9580971002578735
Validation loss: 2.4528150712290118

Epoch: 6| Step: 8
Training loss: 2.2566709518432617
Validation loss: 2.4595318814759612

Epoch: 6| Step: 9
Training loss: 2.8628182411193848
Validation loss: 2.45855898498207

Epoch: 6| Step: 10
Training loss: 2.3228535652160645
Validation loss: 2.449203311756093

Epoch: 6| Step: 11
Training loss: 2.8640778064727783
Validation loss: 2.4440018925615536

Epoch: 6| Step: 12
Training loss: 2.6469476222991943
Validation loss: 2.4420356878670315

Epoch: 6| Step: 13
Training loss: 1.9102587699890137
Validation loss: 2.4402487021620556

Epoch: 76| Step: 0
Training loss: 2.295987606048584
Validation loss: 2.43703051792678

Epoch: 6| Step: 1
Training loss: 3.176138162612915
Validation loss: 2.443392503646112

Epoch: 6| Step: 2
Training loss: 3.3414342403411865
Validation loss: 2.4397978526289745

Epoch: 6| Step: 3
Training loss: 3.2927372455596924
Validation loss: 2.449679508004137

Epoch: 6| Step: 4
Training loss: 1.8019531965255737
Validation loss: 2.452770548482095

Epoch: 6| Step: 5
Training loss: 2.557778835296631
Validation loss: 2.46500277262862

Epoch: 6| Step: 6
Training loss: 2.4920003414154053
Validation loss: 2.472407499949137

Epoch: 6| Step: 7
Training loss: 2.1890933513641357
Validation loss: 2.4792001311497023

Epoch: 6| Step: 8
Training loss: 2.638084888458252
Validation loss: 2.4843518708341863

Epoch: 6| Step: 9
Training loss: 2.6534736156463623
Validation loss: 2.4717382102884273

Epoch: 6| Step: 10
Training loss: 2.5807204246520996
Validation loss: 2.461300147477017

Epoch: 6| Step: 11
Training loss: 2.68415904045105
Validation loss: 2.4574242894367506

Epoch: 6| Step: 12
Training loss: 2.7374107837677
Validation loss: 2.4489309198112896

Epoch: 6| Step: 13
Training loss: 3.47045636177063
Validation loss: 2.448771702345981

Epoch: 77| Step: 0
Training loss: 2.448512315750122
Validation loss: 2.440426303494361

Epoch: 6| Step: 1
Training loss: 2.1713876724243164
Validation loss: 2.438821679802351

Epoch: 6| Step: 2
Training loss: 2.539144515991211
Validation loss: 2.4395015880625737

Epoch: 6| Step: 3
Training loss: 2.5680997371673584
Validation loss: 2.4445270697275796

Epoch: 6| Step: 4
Training loss: 2.351235866546631
Validation loss: 2.47273354120152

Epoch: 6| Step: 5
Training loss: 3.273510694503784
Validation loss: 2.4826583375212965

Epoch: 6| Step: 6
Training loss: 2.3509724140167236
Validation loss: 2.4642382770456295

Epoch: 6| Step: 7
Training loss: 2.0559701919555664
Validation loss: 2.443830869531119

Epoch: 6| Step: 8
Training loss: 2.977630853652954
Validation loss: 2.437515763826268

Epoch: 6| Step: 9
Training loss: 2.9413833618164062
Validation loss: 2.436312965167466

Epoch: 6| Step: 10
Training loss: 2.308126449584961
Validation loss: 2.4509171439755346

Epoch: 6| Step: 11
Training loss: 3.3177525997161865
Validation loss: 2.468032390840592

Epoch: 6| Step: 12
Training loss: 3.5570003986358643
Validation loss: 2.497796545746506

Epoch: 6| Step: 13
Training loss: 3.100778818130493
Validation loss: 2.494485475683725

Epoch: 78| Step: 0
Training loss: 2.9894425868988037
Validation loss: 2.4819348089156614

Epoch: 6| Step: 1
Training loss: 1.8809438943862915
Validation loss: 2.4641023579464165

Epoch: 6| Step: 2
Training loss: 2.1601853370666504
Validation loss: 2.443486211120441

Epoch: 6| Step: 3
Training loss: 2.6209869384765625
Validation loss: 2.435949105088429

Epoch: 6| Step: 4
Training loss: 3.749314308166504
Validation loss: 2.440200605700093

Epoch: 6| Step: 5
Training loss: 2.158756732940674
Validation loss: 2.44308409126856

Epoch: 6| Step: 6
Training loss: 2.7088005542755127
Validation loss: 2.4452989075773504

Epoch: 6| Step: 7
Training loss: 2.3186745643615723
Validation loss: 2.4555573540349163

Epoch: 6| Step: 8
Training loss: 2.673171043395996
Validation loss: 2.4584784776933732

Epoch: 6| Step: 9
Training loss: 2.5905909538269043
Validation loss: 2.468593853776173

Epoch: 6| Step: 10
Training loss: 3.215940237045288
Validation loss: 2.462905958134641

Epoch: 6| Step: 11
Training loss: 3.2547430992126465
Validation loss: 2.4605970844145744

Epoch: 6| Step: 12
Training loss: 2.4124932289123535
Validation loss: 2.4473554498405865

Epoch: 6| Step: 13
Training loss: 2.9021098613739014
Validation loss: 2.4456153095409436

Epoch: 79| Step: 0
Training loss: 2.249238967895508
Validation loss: 2.4453169786801903

Epoch: 6| Step: 1
Training loss: 2.7406728267669678
Validation loss: 2.439210102122317

Epoch: 6| Step: 2
Training loss: 3.0591602325439453
Validation loss: 2.4382257077001754

Epoch: 6| Step: 3
Training loss: 2.2335166931152344
Validation loss: 2.430572214946952

Epoch: 6| Step: 4
Training loss: 2.0950419902801514
Validation loss: 2.4259248625847603

Epoch: 6| Step: 5
Training loss: 2.6482067108154297
Validation loss: 2.4302609697464974

Epoch: 6| Step: 6
Training loss: 2.521940231323242
Validation loss: 2.4285044003558416

Epoch: 6| Step: 7
Training loss: 3.107534170150757
Validation loss: 2.430816291480936

Epoch: 6| Step: 8
Training loss: 3.2972304821014404
Validation loss: 2.4293152927070536

Epoch: 6| Step: 9
Training loss: 3.055715322494507
Validation loss: 2.4284062847014396

Epoch: 6| Step: 10
Training loss: 2.3364131450653076
Validation loss: 2.429131564273629

Epoch: 6| Step: 11
Training loss: 3.4317798614501953
Validation loss: 2.4255850392003215

Epoch: 6| Step: 12
Training loss: 2.4217514991760254
Validation loss: 2.428535928008377

Epoch: 6| Step: 13
Training loss: 1.8271688222885132
Validation loss: 2.431882478857553

Epoch: 80| Step: 0
Training loss: 3.5481648445129395
Validation loss: 2.431868892844005

Epoch: 6| Step: 1
Training loss: 2.5705366134643555
Validation loss: 2.4285522763447096

Epoch: 6| Step: 2
Training loss: 2.407853603363037
Validation loss: 2.4305240928485827

Epoch: 6| Step: 3
Training loss: 2.8202896118164062
Validation loss: 2.4280250867207847

Epoch: 6| Step: 4
Training loss: 2.445399761199951
Validation loss: 2.4293586592520438

Epoch: 6| Step: 5
Training loss: 1.9511096477508545
Validation loss: 2.432640532011627

Epoch: 6| Step: 6
Training loss: 3.0608577728271484
Validation loss: 2.4414190323122087

Epoch: 6| Step: 7
Training loss: 2.765683174133301
Validation loss: 2.4489293739359868

Epoch: 6| Step: 8
Training loss: 2.3983280658721924
Validation loss: 2.44706985258287

Epoch: 6| Step: 9
Training loss: 3.193976879119873
Validation loss: 2.4445232281120877

Epoch: 6| Step: 10
Training loss: 3.1095616817474365
Validation loss: 2.4387184907031316

Epoch: 6| Step: 11
Training loss: 2.263638973236084
Validation loss: 2.4344254847495788

Epoch: 6| Step: 12
Training loss: 2.1930272579193115
Validation loss: 2.435935430629279

Epoch: 6| Step: 13
Training loss: 2.6915385723114014
Validation loss: 2.4338430563608804

Epoch: 81| Step: 0
Training loss: 3.2931480407714844
Validation loss: 2.432236744511512

Epoch: 6| Step: 1
Training loss: 2.7100489139556885
Validation loss: 2.4315006143303326

Epoch: 6| Step: 2
Training loss: 2.0208969116210938
Validation loss: 2.4241471393134004

Epoch: 6| Step: 3
Training loss: 1.7973251342773438
Validation loss: 2.4170244175900697

Epoch: 6| Step: 4
Training loss: 2.465249538421631
Validation loss: 2.417791349913484

Epoch: 6| Step: 5
Training loss: 4.0522637367248535
Validation loss: 2.4167603856773785

Epoch: 6| Step: 6
Training loss: 2.5925538539886475
Validation loss: 2.415560927442325

Epoch: 6| Step: 7
Training loss: 3.1286234855651855
Validation loss: 2.41837796344552

Epoch: 6| Step: 8
Training loss: 2.2472567558288574
Validation loss: 2.4188201581278155

Epoch: 6| Step: 9
Training loss: 1.7414040565490723
Validation loss: 2.411855912977649

Epoch: 6| Step: 10
Training loss: 2.57975435256958
Validation loss: 2.4176002779314594

Epoch: 6| Step: 11
Training loss: 2.9052891731262207
Validation loss: 2.417603290209206

Epoch: 6| Step: 12
Training loss: 2.8089487552642822
Validation loss: 2.4227274746023197

Epoch: 6| Step: 13
Training loss: 3.244898796081543
Validation loss: 2.4230102082734466

Epoch: 82| Step: 0
Training loss: 2.5539495944976807
Validation loss: 2.4285177902508805

Epoch: 6| Step: 1
Training loss: 2.5180747509002686
Validation loss: 2.4358448290055796

Epoch: 6| Step: 2
Training loss: 2.407139778137207
Validation loss: 2.446117821560111

Epoch: 6| Step: 3
Training loss: 1.9796030521392822
Validation loss: 2.45168274705128

Epoch: 6| Step: 4
Training loss: 2.673494338989258
Validation loss: 2.458010212067635

Epoch: 6| Step: 5
Training loss: 2.790469169616699
Validation loss: 2.4464839607156734

Epoch: 6| Step: 6
Training loss: 2.6362593173980713
Validation loss: 2.449946577830981

Epoch: 6| Step: 7
Training loss: 3.098642349243164
Validation loss: 2.436425562827818

Epoch: 6| Step: 8
Training loss: 3.0815510749816895
Validation loss: 2.42627687351678

Epoch: 6| Step: 9
Training loss: 2.6843953132629395
Validation loss: 2.4216654403235323

Epoch: 6| Step: 10
Training loss: 2.9208450317382812
Validation loss: 2.4173003140316216

Epoch: 6| Step: 11
Training loss: 2.408128499984741
Validation loss: 2.4182295440345682

Epoch: 6| Step: 12
Training loss: 2.82181453704834
Validation loss: 2.4180558471269507

Epoch: 6| Step: 13
Training loss: 2.802079200744629
Validation loss: 2.418139103920229

Epoch: 83| Step: 0
Training loss: 2.862104892730713
Validation loss: 2.4143862878122637

Epoch: 6| Step: 1
Training loss: 2.23427677154541
Validation loss: 2.4133681712612027

Epoch: 6| Step: 2
Training loss: 3.037824869155884
Validation loss: 2.4114550185459915

Epoch: 6| Step: 3
Training loss: 2.3014955520629883
Validation loss: 2.41323685133329

Epoch: 6| Step: 4
Training loss: 2.7276697158813477
Validation loss: 2.4124289661325435

Epoch: 6| Step: 5
Training loss: 2.0856168270111084
Validation loss: 2.412302770922261

Epoch: 6| Step: 6
Training loss: 3.195866107940674
Validation loss: 2.4130693584360103

Epoch: 6| Step: 7
Training loss: 2.738701581954956
Validation loss: 2.418486049098353

Epoch: 6| Step: 8
Training loss: 3.074824333190918
Validation loss: 2.418573853790119

Epoch: 6| Step: 9
Training loss: 2.46893310546875
Validation loss: 2.4233954183516966

Epoch: 6| Step: 10
Training loss: 2.8387813568115234
Validation loss: 2.426546376238587

Epoch: 6| Step: 11
Training loss: 2.403451442718506
Validation loss: 2.424028060769522

Epoch: 6| Step: 12
Training loss: 2.9020333290100098
Validation loss: 2.431632544404717

Epoch: 6| Step: 13
Training loss: 2.1281139850616455
Validation loss: 2.431628386179606

Epoch: 84| Step: 0
Training loss: 2.420860767364502
Validation loss: 2.4440940016059467

Epoch: 6| Step: 1
Training loss: 3.1913275718688965
Validation loss: 2.449385036704361

Epoch: 6| Step: 2
Training loss: 2.583467960357666
Validation loss: 2.4502670200922156

Epoch: 6| Step: 3
Training loss: 2.595885753631592
Validation loss: 2.4512215481009534

Epoch: 6| Step: 4
Training loss: 2.5199429988861084
Validation loss: 2.4472690833512174

Epoch: 6| Step: 5
Training loss: 2.16156268119812
Validation loss: 2.4476085042440765

Epoch: 6| Step: 6
Training loss: 2.2940573692321777
Validation loss: 2.450437027920959

Epoch: 6| Step: 7
Training loss: 2.486778974533081
Validation loss: 2.45288263341432

Epoch: 6| Step: 8
Training loss: 2.425349473953247
Validation loss: 2.437907131769324

Epoch: 6| Step: 9
Training loss: 3.6039512157440186
Validation loss: 2.427125184766708

Epoch: 6| Step: 10
Training loss: 2.923145294189453
Validation loss: 2.4214626460947017

Epoch: 6| Step: 11
Training loss: 3.4331088066101074
Validation loss: 2.4188695543555805

Epoch: 6| Step: 12
Training loss: 2.1979212760925293
Validation loss: 2.4151373422274025

Epoch: 6| Step: 13
Training loss: 2.5935070514678955
Validation loss: 2.414834448086318

Epoch: 85| Step: 0
Training loss: 1.9940378665924072
Validation loss: 2.407730620394471

Epoch: 6| Step: 1
Training loss: 2.5360426902770996
Validation loss: 2.409563336321103

Epoch: 6| Step: 2
Training loss: 2.2740488052368164
Validation loss: 2.4048165326477378

Epoch: 6| Step: 3
Training loss: 2.8611392974853516
Validation loss: 2.4082451789609847

Epoch: 6| Step: 4
Training loss: 1.939438819885254
Validation loss: 2.409746318735102

Epoch: 6| Step: 5
Training loss: 3.5229711532592773
Validation loss: 2.4067104196035736

Epoch: 6| Step: 6
Training loss: 2.889129161834717
Validation loss: 2.4118091701179423

Epoch: 6| Step: 7
Training loss: 2.2616188526153564
Validation loss: 2.4119705820596344

Epoch: 6| Step: 8
Training loss: 3.781947374343872
Validation loss: 2.412691275278727

Epoch: 6| Step: 9
Training loss: 3.270418167114258
Validation loss: 2.4142140726889334

Epoch: 6| Step: 10
Training loss: 3.0006558895111084
Validation loss: 2.4150681111120407

Epoch: 6| Step: 11
Training loss: 2.2387876510620117
Validation loss: 2.4218781609689035

Epoch: 6| Step: 12
Training loss: 2.282003879547119
Validation loss: 2.423161401543566

Epoch: 6| Step: 13
Training loss: 2.1152162551879883
Validation loss: 2.427012807579451

Epoch: 86| Step: 0
Training loss: 2.7584352493286133
Validation loss: 2.430285087195776

Epoch: 6| Step: 1
Training loss: 2.6402578353881836
Validation loss: 2.432254191367857

Epoch: 6| Step: 2
Training loss: 2.861288547515869
Validation loss: 2.4340390877057145

Epoch: 6| Step: 3
Training loss: 2.8387680053710938
Validation loss: 2.4333293925049486

Epoch: 6| Step: 4
Training loss: 3.127034902572632
Validation loss: 2.4312097616093133

Epoch: 6| Step: 5
Training loss: 2.4472312927246094
Validation loss: 2.4199615242660686

Epoch: 6| Step: 6
Training loss: 3.174875020980835
Validation loss: 2.4106247194351687

Epoch: 6| Step: 7
Training loss: 2.5154051780700684
Validation loss: 2.405290026818552

Epoch: 6| Step: 8
Training loss: 2.9099440574645996
Validation loss: 2.4043932858333794

Epoch: 6| Step: 9
Training loss: 2.273320436477661
Validation loss: 2.407636829601821

Epoch: 6| Step: 10
Training loss: 2.7545862197875977
Validation loss: 2.4105765101730183

Epoch: 6| Step: 11
Training loss: 2.841566801071167
Validation loss: 2.416045352976809

Epoch: 6| Step: 12
Training loss: 2.275369644165039
Validation loss: 2.413535387285294

Epoch: 6| Step: 13
Training loss: 1.4483883380889893
Validation loss: 2.4117258569245696

Epoch: 87| Step: 0
Training loss: 2.631559133529663
Validation loss: 2.4075084194060294

Epoch: 6| Step: 1
Training loss: 3.0426487922668457
Validation loss: 2.4043807111760622

Epoch: 6| Step: 2
Training loss: 2.810086965560913
Validation loss: 2.405466382221509

Epoch: 6| Step: 3
Training loss: 3.3493566513061523
Validation loss: 2.406917720712641

Epoch: 6| Step: 4
Training loss: 2.7456464767456055
Validation loss: 2.4053982534716205

Epoch: 6| Step: 5
Training loss: 2.218876838684082
Validation loss: 2.4131301551736812

Epoch: 6| Step: 6
Training loss: 2.5657424926757812
Validation loss: 2.412396543769426

Epoch: 6| Step: 7
Training loss: 2.901865243911743
Validation loss: 2.4187564567853044

Epoch: 6| Step: 8
Training loss: 2.1899988651275635
Validation loss: 2.4255110345860964

Epoch: 6| Step: 9
Training loss: 2.2647287845611572
Validation loss: 2.419791942001671

Epoch: 6| Step: 10
Training loss: 2.6733086109161377
Validation loss: 2.4276205929376746

Epoch: 6| Step: 11
Training loss: 2.476125717163086
Validation loss: 2.4343900372905116

Epoch: 6| Step: 12
Training loss: 2.5434930324554443
Validation loss: 2.4400046410099154

Epoch: 6| Step: 13
Training loss: 2.8792169094085693
Validation loss: 2.4489121257617907

Epoch: 88| Step: 0
Training loss: 2.8289918899536133
Validation loss: 2.4413109184593282

Epoch: 6| Step: 1
Training loss: 2.473876714706421
Validation loss: 2.427345432260985

Epoch: 6| Step: 2
Training loss: 2.1890435218811035
Validation loss: 2.4162116819812405

Epoch: 6| Step: 3
Training loss: 3.129319190979004
Validation loss: 2.4111685086322088

Epoch: 6| Step: 4
Training loss: 2.580857515335083
Validation loss: 2.407120530323316

Epoch: 6| Step: 5
Training loss: 2.702378988265991
Validation loss: 2.4078159383548203

Epoch: 6| Step: 6
Training loss: 2.3083090782165527
Validation loss: 2.4076125980705343

Epoch: 6| Step: 7
Training loss: 2.5425572395324707
Validation loss: 2.414156554847635

Epoch: 6| Step: 8
Training loss: 2.2659475803375244
Validation loss: 2.4150812574612197

Epoch: 6| Step: 9
Training loss: 3.151904582977295
Validation loss: 2.433558153849776

Epoch: 6| Step: 10
Training loss: 3.206942558288574
Validation loss: 2.440898423553795

Epoch: 6| Step: 11
Training loss: 2.761859655380249
Validation loss: 2.440226762525497

Epoch: 6| Step: 12
Training loss: 2.4535269737243652
Validation loss: 2.4402779174107376

Epoch: 6| Step: 13
Training loss: 2.49135160446167
Validation loss: 2.432162143850839

Epoch: 89| Step: 0
Training loss: 3.622262477874756
Validation loss: 2.4332064556819137

Epoch: 6| Step: 1
Training loss: 3.49802827835083
Validation loss: 2.43187112962046

Epoch: 6| Step: 2
Training loss: 1.9511768817901611
Validation loss: 2.4198063214619956

Epoch: 6| Step: 3
Training loss: 2.83748722076416
Validation loss: 2.417626706502771

Epoch: 6| Step: 4
Training loss: 2.998201370239258
Validation loss: 2.41242108550123

Epoch: 6| Step: 5
Training loss: 1.935152292251587
Validation loss: 2.4232955491670998

Epoch: 6| Step: 6
Training loss: 3.0582380294799805
Validation loss: 2.4145962358802877

Epoch: 6| Step: 7
Training loss: 2.787569999694824
Validation loss: 2.413317447067589

Epoch: 6| Step: 8
Training loss: 2.4950599670410156
Validation loss: 2.4083762655976

Epoch: 6| Step: 9
Training loss: 2.130795955657959
Validation loss: 2.414541095815679

Epoch: 6| Step: 10
Training loss: 2.15460205078125
Validation loss: 2.421688900198988

Epoch: 6| Step: 11
Training loss: 2.313368797302246
Validation loss: 2.4231966567295853

Epoch: 6| Step: 12
Training loss: 2.0434343814849854
Validation loss: 2.4129248524224884

Epoch: 6| Step: 13
Training loss: 3.647806406021118
Validation loss: 2.4020743421328965

Epoch: 90| Step: 0
Training loss: 2.719460964202881
Validation loss: 2.400576760691981

Epoch: 6| Step: 1
Training loss: 2.319780111312866
Validation loss: 2.399957605587539

Epoch: 6| Step: 2
Training loss: 2.1245994567871094
Validation loss: 2.3951329544026363

Epoch: 6| Step: 3
Training loss: 3.4401602745056152
Validation loss: 2.403173497928086

Epoch: 6| Step: 4
Training loss: 2.0995047092437744
Validation loss: 2.3993201383980374

Epoch: 6| Step: 5
Training loss: 2.53722882270813
Validation loss: 2.4083776320180585

Epoch: 6| Step: 6
Training loss: 3.0367398262023926
Validation loss: 2.4117198708236858

Epoch: 6| Step: 7
Training loss: 2.9132657051086426
Validation loss: 2.4105458439037366

Epoch: 6| Step: 8
Training loss: 3.2300102710723877
Validation loss: 2.415739890067808

Epoch: 6| Step: 9
Training loss: 2.695280075073242
Validation loss: 2.4168915876778225

Epoch: 6| Step: 10
Training loss: 2.3116397857666016
Validation loss: 2.4229986898360716

Epoch: 6| Step: 11
Training loss: 2.8451266288757324
Validation loss: 2.424059378203525

Epoch: 6| Step: 12
Training loss: 2.322852849960327
Validation loss: 2.428919671684183

Epoch: 6| Step: 13
Training loss: 2.5768089294433594
Validation loss: 2.4300861897007113

Epoch: 91| Step: 0
Training loss: 2.4572196006774902
Validation loss: 2.432688731019215

Epoch: 6| Step: 1
Training loss: 1.9475021362304688
Validation loss: 2.4391776874501216

Epoch: 6| Step: 2
Training loss: 2.543715476989746
Validation loss: 2.4410193504825717

Epoch: 6| Step: 3
Training loss: 2.433098316192627
Validation loss: 2.453512945482808

Epoch: 6| Step: 4
Training loss: 2.8002285957336426
Validation loss: 2.459526769576534

Epoch: 6| Step: 5
Training loss: 2.5098772048950195
Validation loss: 2.46230145167279

Epoch: 6| Step: 6
Training loss: 2.1623239517211914
Validation loss: 2.473669421288275

Epoch: 6| Step: 7
Training loss: 2.4080958366394043
Validation loss: 2.482806263431426

Epoch: 6| Step: 8
Training loss: 3.1851892471313477
Validation loss: 2.4840991061220885

Epoch: 6| Step: 9
Training loss: 3.222623348236084
Validation loss: 2.4685329570565173

Epoch: 6| Step: 10
Training loss: 2.5437004566192627
Validation loss: 2.4590085193675053

Epoch: 6| Step: 11
Training loss: 2.8377695083618164
Validation loss: 2.449067105529129

Epoch: 6| Step: 12
Training loss: 3.2200937271118164
Validation loss: 2.4388314677822973

Epoch: 6| Step: 13
Training loss: 3.099667549133301
Validation loss: 2.4291462129162205

Epoch: 92| Step: 0
Training loss: 3.0951688289642334
Validation loss: 2.423569517750894

Epoch: 6| Step: 1
Training loss: 2.5540060997009277
Validation loss: 2.420399127467986

Epoch: 6| Step: 2
Training loss: 2.0866851806640625
Validation loss: 2.415316236916409

Epoch: 6| Step: 3
Training loss: 3.0461628437042236
Validation loss: 2.41705294065578

Epoch: 6| Step: 4
Training loss: 2.504154682159424
Validation loss: 2.4135057054540163

Epoch: 6| Step: 5
Training loss: 2.565430164337158
Validation loss: 2.416026871691468

Epoch: 6| Step: 6
Training loss: 2.0480751991271973
Validation loss: 2.41160318415652

Epoch: 6| Step: 7
Training loss: 2.4175777435302734
Validation loss: 2.4089876323617916

Epoch: 6| Step: 8
Training loss: 2.8101816177368164
Validation loss: 2.4017021963673253

Epoch: 6| Step: 9
Training loss: 2.523983955383301
Validation loss: 2.405357221121429

Epoch: 6| Step: 10
Training loss: 2.983243942260742
Validation loss: 2.413296132959345

Epoch: 6| Step: 11
Training loss: 2.1579885482788086
Validation loss: 2.4145583926990466

Epoch: 6| Step: 12
Training loss: 3.506143569946289
Validation loss: 2.4193230188021095

Epoch: 6| Step: 13
Training loss: 2.734179973602295
Validation loss: 2.4165003735532045

Epoch: 93| Step: 0
Training loss: 2.1399638652801514
Validation loss: 2.4118579741447204

Epoch: 6| Step: 1
Training loss: 3.1010067462921143
Validation loss: 2.419177838551101

Epoch: 6| Step: 2
Training loss: 3.299859046936035
Validation loss: 2.4116371805949877

Epoch: 6| Step: 3
Training loss: 2.6659226417541504
Validation loss: 2.4096685686419086

Epoch: 6| Step: 4
Training loss: 3.154268503189087
Validation loss: 2.4080994282999346

Epoch: 6| Step: 5
Training loss: 2.670034646987915
Validation loss: 2.399364650890391

Epoch: 6| Step: 6
Training loss: 2.193774700164795
Validation loss: 2.399677315065938

Epoch: 6| Step: 7
Training loss: 2.1229665279388428
Validation loss: 2.399580858087027

Epoch: 6| Step: 8
Training loss: 2.1697287559509277
Validation loss: 2.3970985207506406

Epoch: 6| Step: 9
Training loss: 2.715228796005249
Validation loss: 2.402134480014924

Epoch: 6| Step: 10
Training loss: 2.9757556915283203
Validation loss: 2.406532087633687

Epoch: 6| Step: 11
Training loss: 1.8768718242645264
Validation loss: 2.404248732392506

Epoch: 6| Step: 12
Training loss: 2.8776156902313232
Validation loss: 2.400885484551871

Epoch: 6| Step: 13
Training loss: 3.3850443363189697
Validation loss: 2.405087499208348

Epoch: 94| Step: 0
Training loss: 2.392871618270874
Validation loss: 2.3989828837815153

Epoch: 6| Step: 1
Training loss: 2.271649122238159
Validation loss: 2.406045575295725

Epoch: 6| Step: 2
Training loss: 2.757200241088867
Validation loss: 2.405444288766512

Epoch: 6| Step: 3
Training loss: 2.3934004306793213
Validation loss: 2.4128370669580277

Epoch: 6| Step: 4
Training loss: 2.5110816955566406
Validation loss: 2.419281759569722

Epoch: 6| Step: 5
Training loss: 2.3421826362609863
Validation loss: 2.4231021147902294

Epoch: 6| Step: 6
Training loss: 3.0482308864593506
Validation loss: 2.4298111700242564

Epoch: 6| Step: 7
Training loss: 2.707524299621582
Validation loss: 2.432733807512509

Epoch: 6| Step: 8
Training loss: 2.4031972885131836
Validation loss: 2.433977770548995

Epoch: 6| Step: 9
Training loss: 2.9773340225219727
Validation loss: 2.4334773504605858

Epoch: 6| Step: 10
Training loss: 2.714693069458008
Validation loss: 2.430730653065507

Epoch: 6| Step: 11
Training loss: 3.1219019889831543
Validation loss: 2.4256130085196546

Epoch: 6| Step: 12
Training loss: 3.3483262062072754
Validation loss: 2.4345313528532624

Epoch: 6| Step: 13
Training loss: 1.5095592737197876
Validation loss: 2.4316944537624234

Epoch: 95| Step: 0
Training loss: 2.018516778945923
Validation loss: 2.425056626719813

Epoch: 6| Step: 1
Training loss: 2.3332509994506836
Validation loss: 2.4306425766278337

Epoch: 6| Step: 2
Training loss: 2.8384757041931152
Validation loss: 2.417976503731102

Epoch: 6| Step: 3
Training loss: 2.4243717193603516
Validation loss: 2.415300061625819

Epoch: 6| Step: 4
Training loss: 2.6662843227386475
Validation loss: 2.4085034939550583

Epoch: 6| Step: 5
Training loss: 2.9524519443511963
Validation loss: 2.4078557363120456

Epoch: 6| Step: 6
Training loss: 2.8155713081359863
Validation loss: 2.406438325041084

Epoch: 6| Step: 7
Training loss: 3.1431195735931396
Validation loss: 2.403756777445475

Epoch: 6| Step: 8
Training loss: 3.471327304840088
Validation loss: 2.395720786945794

Epoch: 6| Step: 9
Training loss: 2.2109899520874023
Validation loss: 2.3992474425223564

Epoch: 6| Step: 10
Training loss: 2.2795701026916504
Validation loss: 2.394926860768308

Epoch: 6| Step: 11
Training loss: 2.092416763305664
Validation loss: 2.394627950524771

Epoch: 6| Step: 12
Training loss: 2.338320016860962
Validation loss: 2.3961521169190765

Epoch: 6| Step: 13
Training loss: 3.8218905925750732
Validation loss: 2.3980341572915354

Epoch: 96| Step: 0
Training loss: 2.811368465423584
Validation loss: 2.400564880781276

Epoch: 6| Step: 1
Training loss: 2.585113048553467
Validation loss: 2.408575656593487

Epoch: 6| Step: 2
Training loss: 2.652635335922241
Validation loss: 2.413898724381642

Epoch: 6| Step: 3
Training loss: 2.3006834983825684
Validation loss: 2.4101481437683105

Epoch: 6| Step: 4
Training loss: 3.101235866546631
Validation loss: 2.4116890097177155

Epoch: 6| Step: 5
Training loss: 2.0509512424468994
Validation loss: 2.4081302714604202

Epoch: 6| Step: 6
Training loss: 2.4172208309173584
Validation loss: 2.407721068269463

Epoch: 6| Step: 7
Training loss: 2.71559476852417
Validation loss: 2.400883282384565

Epoch: 6| Step: 8
Training loss: 3.121217727661133
Validation loss: 2.4045173429673716

Epoch: 6| Step: 9
Training loss: 2.964198589324951
Validation loss: 2.405078903321297

Epoch: 6| Step: 10
Training loss: 2.4390711784362793
Validation loss: 2.4030095890004146

Epoch: 6| Step: 11
Training loss: 2.4533913135528564
Validation loss: 2.396560643308906

Epoch: 6| Step: 12
Training loss: 2.673553943634033
Validation loss: 2.400343461703229

Epoch: 6| Step: 13
Training loss: 2.677781820297241
Validation loss: 2.415193857685212

Epoch: 97| Step: 0
Training loss: 2.3087902069091797
Validation loss: 2.4072475561531643

Epoch: 6| Step: 1
Training loss: 2.343350648880005
Validation loss: 2.4126429891073577

Epoch: 6| Step: 2
Training loss: 2.5093564987182617
Validation loss: 2.4149976340673303

Epoch: 6| Step: 3
Training loss: 2.912646770477295
Validation loss: 2.407060976951353

Epoch: 6| Step: 4
Training loss: 2.710277795791626
Validation loss: 2.418221298084464

Epoch: 6| Step: 5
Training loss: 2.363020181655884
Validation loss: 2.4214620436391523

Epoch: 6| Step: 6
Training loss: 2.7864980697631836
Validation loss: 2.427569104779151

Epoch: 6| Step: 7
Training loss: 2.7638156414031982
Validation loss: 2.432055027254166

Epoch: 6| Step: 8
Training loss: 2.513312339782715
Validation loss: 2.4318023240694435

Epoch: 6| Step: 9
Training loss: 3.2449841499328613
Validation loss: 2.4365812681054555

Epoch: 6| Step: 10
Training loss: 2.041175603866577
Validation loss: 2.432914072467435

Epoch: 6| Step: 11
Training loss: 3.339031219482422
Validation loss: 2.4267481809021323

Epoch: 6| Step: 12
Training loss: 2.584686040878296
Validation loss: 2.4165316371507544

Epoch: 6| Step: 13
Training loss: 2.453169584274292
Validation loss: 2.4094465086537022

Epoch: 98| Step: 0
Training loss: 2.7775890827178955
Validation loss: 2.396917432867071

Epoch: 6| Step: 1
Training loss: 2.3109824657440186
Validation loss: 2.3963540446373726

Epoch: 6| Step: 2
Training loss: 2.5061757564544678
Validation loss: 2.39620118500084

Epoch: 6| Step: 3
Training loss: 3.300123453140259
Validation loss: 2.387363979893346

Epoch: 6| Step: 4
Training loss: 1.9619722366333008
Validation loss: 2.385168296034618

Epoch: 6| Step: 5
Training loss: 2.6485695838928223
Validation loss: 2.3853969035610074

Epoch: 6| Step: 6
Training loss: 2.9198427200317383
Validation loss: 2.385421532456593

Epoch: 6| Step: 7
Training loss: 3.119314670562744
Validation loss: 2.388143841938306

Epoch: 6| Step: 8
Training loss: 2.963567018508911
Validation loss: 2.3851586067548363

Epoch: 6| Step: 9
Training loss: 3.3807551860809326
Validation loss: 2.3878747852899695

Epoch: 6| Step: 10
Training loss: 2.7288715839385986
Validation loss: 2.388033925846059

Epoch: 6| Step: 11
Training loss: 1.8394050598144531
Validation loss: 2.38541909956163

Epoch: 6| Step: 12
Training loss: 1.8820064067840576
Validation loss: 2.380917469660441

Epoch: 6| Step: 13
Training loss: 2.341067314147949
Validation loss: 2.3830861404377925

Epoch: 99| Step: 0
Training loss: 3.224919319152832
Validation loss: 2.3839373050197477

Epoch: 6| Step: 1
Training loss: 3.2158117294311523
Validation loss: 2.3831007506257746

Epoch: 6| Step: 2
Training loss: 2.4547314643859863
Validation loss: 2.390352120963476

Epoch: 6| Step: 3
Training loss: 2.3887741565704346
Validation loss: 2.3941636111146662

Epoch: 6| Step: 4
Training loss: 3.093540906906128
Validation loss: 2.4050346189929592

Epoch: 6| Step: 5
Training loss: 2.672032117843628
Validation loss: 2.3996630612240044

Epoch: 6| Step: 6
Training loss: 2.3598129749298096
Validation loss: 2.3932915041523595

Epoch: 6| Step: 7
Training loss: 2.28143572807312
Validation loss: 2.385771438639651

Epoch: 6| Step: 8
Training loss: 2.5452637672424316
Validation loss: 2.382726114283326

Epoch: 6| Step: 9
Training loss: 2.5049514770507812
Validation loss: 2.3812712700136247

Epoch: 6| Step: 10
Training loss: 2.2919259071350098
Validation loss: 2.3797377822219685

Epoch: 6| Step: 11
Training loss: 2.0786638259887695
Validation loss: 2.3850036154511156

Epoch: 6| Step: 12
Training loss: 2.9304423332214355
Validation loss: 2.3892663832633727

Epoch: 6| Step: 13
Training loss: 3.031299591064453
Validation loss: 2.392938261391014

Epoch: 100| Step: 0
Training loss: 2.5924839973449707
Validation loss: 2.39186720437901

Epoch: 6| Step: 1
Training loss: 3.072805643081665
Validation loss: 2.3928187252372823

Epoch: 6| Step: 2
Training loss: 2.669466733932495
Validation loss: 2.4003949267889864

Epoch: 6| Step: 3
Training loss: 2.702249526977539
Validation loss: 2.398948456651421

Epoch: 6| Step: 4
Training loss: 2.486314535140991
Validation loss: 2.401219226980722

Epoch: 6| Step: 5
Training loss: 2.797769069671631
Validation loss: 2.4059483389700613

Epoch: 6| Step: 6
Training loss: 2.3956637382507324
Validation loss: 2.4086495522529847

Epoch: 6| Step: 7
Training loss: 2.6515555381774902
Validation loss: 2.416913891351351

Epoch: 6| Step: 8
Training loss: 2.288029432296753
Validation loss: 2.420339976587603

Epoch: 6| Step: 9
Training loss: 1.8241335153579712
Validation loss: 2.4277757495962162

Epoch: 6| Step: 10
Training loss: 3.097616195678711
Validation loss: 2.435783114484561

Epoch: 6| Step: 11
Training loss: 2.8755767345428467
Validation loss: 2.4364323513482207

Epoch: 6| Step: 12
Training loss: 2.786499500274658
Validation loss: 2.4304022148091304

Epoch: 6| Step: 13
Training loss: 2.8671178817749023
Validation loss: 2.4398036208204044

Epoch: 101| Step: 0
Training loss: 2.730672836303711
Validation loss: 2.4340657316228396

Epoch: 6| Step: 1
Training loss: 2.9996581077575684
Validation loss: 2.430717929717033

Epoch: 6| Step: 2
Training loss: 2.1393237113952637
Validation loss: 2.435186873200119

Epoch: 6| Step: 3
Training loss: 3.3006699085235596
Validation loss: 2.43007073351132

Epoch: 6| Step: 4
Training loss: 1.754197120666504
Validation loss: 2.4215479550823087

Epoch: 6| Step: 5
Training loss: 3.6515188217163086
Validation loss: 2.420803754560409

Epoch: 6| Step: 6
Training loss: 2.8759007453918457
Validation loss: 2.415205012085617

Epoch: 6| Step: 7
Training loss: 2.789155960083008
Validation loss: 2.414310803977392

Epoch: 6| Step: 8
Training loss: 3.0589752197265625
Validation loss: 2.411949557642783

Epoch: 6| Step: 9
Training loss: 2.480027675628662
Validation loss: 2.4100209487381803

Epoch: 6| Step: 10
Training loss: 2.500126361846924
Validation loss: 2.4024712398488033

Epoch: 6| Step: 11
Training loss: 2.3752288818359375
Validation loss: 2.4017125560391333

Epoch: 6| Step: 12
Training loss: 2.2017569541931152
Validation loss: 2.3972023046144875

Epoch: 6| Step: 13
Training loss: 1.6213864088058472
Validation loss: 2.401291972847395

Epoch: 102| Step: 0
Training loss: 2.971498489379883
Validation loss: 2.3949391508615143

Epoch: 6| Step: 1
Training loss: 2.5348386764526367
Validation loss: 2.389174352410019

Epoch: 6| Step: 2
Training loss: 2.6675028800964355
Validation loss: 2.398658211513232

Epoch: 6| Step: 3
Training loss: 2.521916627883911
Validation loss: 2.384605012914186

Epoch: 6| Step: 4
Training loss: 2.611116647720337
Validation loss: 2.3919605990891815

Epoch: 6| Step: 5
Training loss: 2.94364595413208
Validation loss: 2.388644003099011

Epoch: 6| Step: 6
Training loss: 2.406155586242676
Validation loss: 2.3866691512446248

Epoch: 6| Step: 7
Training loss: 1.9733442068099976
Validation loss: 2.389668610788161

Epoch: 6| Step: 8
Training loss: 2.5424251556396484
Validation loss: 2.389378939905474

Epoch: 6| Step: 9
Training loss: 3.2301836013793945
Validation loss: 2.39314361797866

Epoch: 6| Step: 10
Training loss: 2.5966649055480957
Validation loss: 2.3929228705744587

Epoch: 6| Step: 11
Training loss: 2.826740264892578
Validation loss: 2.393661545168969

Epoch: 6| Step: 12
Training loss: 2.576547622680664
Validation loss: 2.3959778662650817

Epoch: 6| Step: 13
Training loss: 2.406374216079712
Validation loss: 2.40716326749453

Epoch: 103| Step: 0
Training loss: 2.43979549407959
Validation loss: 2.40474380729019

Epoch: 6| Step: 1
Training loss: 3.8314085006713867
Validation loss: 2.3952086407651185

Epoch: 6| Step: 2
Training loss: 2.5615198612213135
Validation loss: 2.3954317313368603

Epoch: 6| Step: 3
Training loss: 2.660507917404175
Validation loss: 2.381018933429513

Epoch: 6| Step: 4
Training loss: 2.475090742111206
Validation loss: 2.3766290474963445

Epoch: 6| Step: 5
Training loss: 2.573873281478882
Validation loss: 2.383245604012602

Epoch: 6| Step: 6
Training loss: 2.1022372245788574
Validation loss: 2.3911533458258516

Epoch: 6| Step: 7
Training loss: 2.7819156646728516
Validation loss: 2.3918991601595314

Epoch: 6| Step: 8
Training loss: 2.233489990234375
Validation loss: 2.408160312201387

Epoch: 6| Step: 9
Training loss: 2.1162891387939453
Validation loss: 2.4158145407194733

Epoch: 6| Step: 10
Training loss: 2.6382179260253906
Validation loss: 2.410039012150098

Epoch: 6| Step: 11
Training loss: 3.058485507965088
Validation loss: 2.4179643366926458

Epoch: 6| Step: 12
Training loss: 2.906038761138916
Validation loss: 2.4283480618589666

Epoch: 6| Step: 13
Training loss: 2.3680195808410645
Validation loss: 2.428590823245305

Epoch: 104| Step: 0
Training loss: 2.99285888671875
Validation loss: 2.425838755023095

Epoch: 6| Step: 1
Training loss: 2.7749600410461426
Validation loss: 2.4375571768770934

Epoch: 6| Step: 2
Training loss: 2.359586715698242
Validation loss: 2.4447872971975677

Epoch: 6| Step: 3
Training loss: 2.6820554733276367
Validation loss: 2.4432376892335954

Epoch: 6| Step: 4
Training loss: 2.75254487991333
Validation loss: 2.44608671434464

Epoch: 6| Step: 5
Training loss: 2.959017753601074
Validation loss: 2.4289442082887054

Epoch: 6| Step: 6
Training loss: 2.0287976264953613
Validation loss: 2.422045005265103

Epoch: 6| Step: 7
Training loss: 2.8462398052215576
Validation loss: 2.411935898565477

Epoch: 6| Step: 8
Training loss: 2.571389675140381
Validation loss: 2.3938739633047454

Epoch: 6| Step: 9
Training loss: 2.537332534790039
Validation loss: 2.3953634846595024

Epoch: 6| Step: 10
Training loss: 2.80503249168396
Validation loss: 2.391084796638899

Epoch: 6| Step: 11
Training loss: 2.9595513343811035
Validation loss: 2.3849724415809876

Epoch: 6| Step: 12
Training loss: 2.5356132984161377
Validation loss: 2.3818348787164174

Epoch: 6| Step: 13
Training loss: 1.7239413261413574
Validation loss: 2.3782892573264336

Epoch: 105| Step: 0
Training loss: 2.6366305351257324
Validation loss: 2.388378449665603

Epoch: 6| Step: 1
Training loss: 1.6864516735076904
Validation loss: 2.3878438472747803

Epoch: 6| Step: 2
Training loss: 2.7520804405212402
Validation loss: 2.40150051219489

Epoch: 6| Step: 3
Training loss: 3.0577878952026367
Validation loss: 2.404444968828591

Epoch: 6| Step: 4
Training loss: 2.5090274810791016
Validation loss: 2.4099944663304154

Epoch: 6| Step: 5
Training loss: 3.7443857192993164
Validation loss: 2.410653305310075

Epoch: 6| Step: 6
Training loss: 2.3080053329467773
Validation loss: 2.409761817224564

Epoch: 6| Step: 7
Training loss: 2.0525569915771484
Validation loss: 2.3997923020393617

Epoch: 6| Step: 8
Training loss: 2.231930732727051
Validation loss: 2.3924092554276988

Epoch: 6| Step: 9
Training loss: 2.166485071182251
Validation loss: 2.3899020815408356

Epoch: 6| Step: 10
Training loss: 3.082794189453125
Validation loss: 2.3972227573394775

Epoch: 6| Step: 11
Training loss: 3.077948570251465
Validation loss: 2.40185078882402

Epoch: 6| Step: 12
Training loss: 2.545616626739502
Validation loss: 2.4050504238374772

Epoch: 6| Step: 13
Training loss: 3.282668113708496
Validation loss: 2.4007968569314606

Epoch: 106| Step: 0
Training loss: 2.44834303855896
Validation loss: 2.412262375636767

Epoch: 6| Step: 1
Training loss: 2.989442825317383
Validation loss: 2.413616729039018

Epoch: 6| Step: 2
Training loss: 2.210820436477661
Validation loss: 2.4224902788798013

Epoch: 6| Step: 3
Training loss: 3.0825510025024414
Validation loss: 2.4223872307808167

Epoch: 6| Step: 4
Training loss: 2.553201913833618
Validation loss: 2.441162504175658

Epoch: 6| Step: 5
Training loss: 3.4815807342529297
Validation loss: 2.481550162838351

Epoch: 6| Step: 6
Training loss: 2.0136561393737793
Validation loss: 2.4817517444651616

Epoch: 6| Step: 7
Training loss: 2.3931121826171875
Validation loss: 2.463916701655234

Epoch: 6| Step: 8
Training loss: 2.279726028442383
Validation loss: 2.440964901319114

Epoch: 6| Step: 9
Training loss: 2.2940709590911865
Validation loss: 2.42988262637969

Epoch: 6| Step: 10
Training loss: 2.758411169052124
Validation loss: 2.409655709420481

Epoch: 6| Step: 11
Training loss: 2.8529281616210938
Validation loss: 2.400245774176813

Epoch: 6| Step: 12
Training loss: 3.2138986587524414
Validation loss: 2.3921730800341536

Epoch: 6| Step: 13
Training loss: 2.0116677284240723
Validation loss: 2.3853482200253393

Epoch: 107| Step: 0
Training loss: 2.6304216384887695
Validation loss: 2.3776354559006228

Epoch: 6| Step: 1
Training loss: 2.987428665161133
Validation loss: 2.37297756953906

Epoch: 6| Step: 2
Training loss: 2.56911039352417
Validation loss: 2.3828011866538756

Epoch: 6| Step: 3
Training loss: 2.329089641571045
Validation loss: 2.3864093083207325

Epoch: 6| Step: 4
Training loss: 3.020341157913208
Validation loss: 2.386463847211612

Epoch: 6| Step: 5
Training loss: 2.9468941688537598
Validation loss: 2.3827171940957346

Epoch: 6| Step: 6
Training loss: 2.82834529876709
Validation loss: 2.3852577004381406

Epoch: 6| Step: 7
Training loss: 2.934109687805176
Validation loss: 2.393161678826937

Epoch: 6| Step: 8
Training loss: 2.668391227722168
Validation loss: 2.4017906547874532

Epoch: 6| Step: 9
Training loss: 2.1099929809570312
Validation loss: 2.4222427824492097

Epoch: 6| Step: 10
Training loss: 3.1386942863464355
Validation loss: 2.466216177068731

Epoch: 6| Step: 11
Training loss: 1.8010056018829346
Validation loss: 2.4846630250253985

Epoch: 6| Step: 12
Training loss: 3.0818333625793457
Validation loss: 2.520463953736008

Epoch: 6| Step: 13
Training loss: 1.7921831607818604
Validation loss: 2.5259431844116538

Epoch: 108| Step: 0
Training loss: 3.1093902587890625
Validation loss: 2.541558886087069

Epoch: 6| Step: 1
Training loss: 3.346116781234741
Validation loss: 2.5076436329913396

Epoch: 6| Step: 2
Training loss: 2.4640212059020996
Validation loss: 2.4679515259240263

Epoch: 6| Step: 3
Training loss: 3.4776408672332764
Validation loss: 2.425097042514432

Epoch: 6| Step: 4
Training loss: 2.819352626800537
Validation loss: 2.3977870889889297

Epoch: 6| Step: 5
Training loss: 2.957653522491455
Validation loss: 2.382237793296896

Epoch: 6| Step: 6
Training loss: 2.345226764678955
Validation loss: 2.3756339037290184

Epoch: 6| Step: 7
Training loss: 2.5508666038513184
Validation loss: 2.371190353106427

Epoch: 6| Step: 8
Training loss: 1.7965344190597534
Validation loss: 2.368078031847554

Epoch: 6| Step: 9
Training loss: 2.8304591178894043
Validation loss: 2.372268763921594

Epoch: 6| Step: 10
Training loss: 1.8779804706573486
Validation loss: 2.373749279206799

Epoch: 6| Step: 11
Training loss: 2.734905481338501
Validation loss: 2.3773711112237748

Epoch: 6| Step: 12
Training loss: 1.939777135848999
Validation loss: 2.3914562015123266

Epoch: 6| Step: 13
Training loss: 2.868190288543701
Validation loss: 2.392164276492211

Epoch: 109| Step: 0
Training loss: 2.5319883823394775
Validation loss: 2.3930029971625215

Epoch: 6| Step: 1
Training loss: 3.1316418647766113
Validation loss: 2.3924649838478333

Epoch: 6| Step: 2
Training loss: 2.4923689365386963
Validation loss: 2.388480883772655

Epoch: 6| Step: 3
Training loss: 2.8129520416259766
Validation loss: 2.3732373970811085

Epoch: 6| Step: 4
Training loss: 2.3937342166900635
Validation loss: 2.372184286835373

Epoch: 6| Step: 5
Training loss: 2.455239772796631
Validation loss: 2.37096817006347

Epoch: 6| Step: 6
Training loss: 2.6994612216949463
Validation loss: 2.3674495809821674

Epoch: 6| Step: 7
Training loss: 2.3386764526367188
Validation loss: 2.365139199841407

Epoch: 6| Step: 8
Training loss: 2.4532594680786133
Validation loss: 2.362088317512184

Epoch: 6| Step: 9
Training loss: 2.652812957763672
Validation loss: 2.358541211774272

Epoch: 6| Step: 10
Training loss: 2.4627819061279297
Validation loss: 2.3592804196060344

Epoch: 6| Step: 11
Training loss: 2.5888640880584717
Validation loss: 2.3613253767772386

Epoch: 6| Step: 12
Training loss: 2.751744270324707
Validation loss: 2.362427460250034

Epoch: 6| Step: 13
Training loss: 3.2434613704681396
Validation loss: 2.3646936775535665

Epoch: 110| Step: 0
Training loss: 2.0221729278564453
Validation loss: 2.365191428892074

Epoch: 6| Step: 1
Training loss: 2.904270887374878
Validation loss: 2.3688286094255346

Epoch: 6| Step: 2
Training loss: 2.310171365737915
Validation loss: 2.3751690772271927

Epoch: 6| Step: 3
Training loss: 3.242929220199585
Validation loss: 2.380778012737151

Epoch: 6| Step: 4
Training loss: 2.5521655082702637
Validation loss: 2.373645149251466

Epoch: 6| Step: 5
Training loss: 1.692762851715088
Validation loss: 2.3750009331651913

Epoch: 6| Step: 6
Training loss: 3.4364888668060303
Validation loss: 2.3766021164514686

Epoch: 6| Step: 7
Training loss: 3.2502779960632324
Validation loss: 2.3759999864844867

Epoch: 6| Step: 8
Training loss: 2.6415836811065674
Validation loss: 2.3737002854706137

Epoch: 6| Step: 9
Training loss: 3.0160744190216064
Validation loss: 2.37504772217043

Epoch: 6| Step: 10
Training loss: 2.220370054244995
Validation loss: 2.368919007239803

Epoch: 6| Step: 11
Training loss: 2.536635160446167
Validation loss: 2.3690193314706125

Epoch: 6| Step: 12
Training loss: 2.145562171936035
Validation loss: 2.371096254676901

Epoch: 6| Step: 13
Training loss: 2.754800319671631
Validation loss: 2.372099516212299

Epoch: 111| Step: 0
Training loss: 2.7475388050079346
Validation loss: 2.3698355459397837

Epoch: 6| Step: 1
Training loss: 3.2747836112976074
Validation loss: 2.364780410643547

Epoch: 6| Step: 2
Training loss: 2.138524293899536
Validation loss: 2.3693589830911286

Epoch: 6| Step: 3
Training loss: 2.882504940032959
Validation loss: 2.3719134612749984

Epoch: 6| Step: 4
Training loss: 3.3268959522247314
Validation loss: 2.3686905484045706

Epoch: 6| Step: 5
Training loss: 2.3662796020507812
Validation loss: 2.3714012997124785

Epoch: 6| Step: 6
Training loss: 2.069117784500122
Validation loss: 2.3771754157158638

Epoch: 6| Step: 7
Training loss: 2.2524232864379883
Validation loss: 2.3808995344305552

Epoch: 6| Step: 8
Training loss: 2.404646396636963
Validation loss: 2.375354000317153

Epoch: 6| Step: 9
Training loss: 2.6519665718078613
Validation loss: 2.3903071957249797

Epoch: 6| Step: 10
Training loss: 2.918715000152588
Validation loss: 2.3894238959076586

Epoch: 6| Step: 11
Training loss: 2.3209128379821777
Validation loss: 2.380605936050415

Epoch: 6| Step: 12
Training loss: 2.340576648712158
Validation loss: 2.3844106222993586

Epoch: 6| Step: 13
Training loss: 3.130099296569824
Validation loss: 2.381536296618882

Epoch: 112| Step: 0
Training loss: 2.4899704456329346
Validation loss: 2.376728610325885

Epoch: 6| Step: 1
Training loss: 2.017944097518921
Validation loss: 2.3802477582808463

Epoch: 6| Step: 2
Training loss: 3.210141658782959
Validation loss: 2.3746689673393004

Epoch: 6| Step: 3
Training loss: 2.289595127105713
Validation loss: 2.3785538135036344

Epoch: 6| Step: 4
Training loss: 2.819260358810425
Validation loss: 2.3828294533555225

Epoch: 6| Step: 5
Training loss: 2.5223636627197266
Validation loss: 2.3842001832941526

Epoch: 6| Step: 6
Training loss: 2.6522862911224365
Validation loss: 2.3767114967428227

Epoch: 6| Step: 7
Training loss: 2.720209836959839
Validation loss: 2.3752626065284974

Epoch: 6| Step: 8
Training loss: 2.205523729324341
Validation loss: 2.3765924669081167

Epoch: 6| Step: 9
Training loss: 2.746825933456421
Validation loss: 2.373914077717771

Epoch: 6| Step: 10
Training loss: 2.9711384773254395
Validation loss: 2.3721621369802826

Epoch: 6| Step: 11
Training loss: 2.7121963500976562
Validation loss: 2.3750524341419177

Epoch: 6| Step: 12
Training loss: 2.390071153640747
Validation loss: 2.3774452286381877

Epoch: 6| Step: 13
Training loss: 2.874829053878784
Validation loss: 2.3806535428570164

Epoch: 113| Step: 0
Training loss: 3.5500006675720215
Validation loss: 2.37684771578799

Epoch: 6| Step: 1
Training loss: 1.703249216079712
Validation loss: 2.3808459261412263

Epoch: 6| Step: 2
Training loss: 2.325260877609253
Validation loss: 2.3855190738554923

Epoch: 6| Step: 3
Training loss: 2.487659454345703
Validation loss: 2.386067498114801

Epoch: 6| Step: 4
Training loss: 3.226250171661377
Validation loss: 2.382112968352533

Epoch: 6| Step: 5
Training loss: 1.9312716722488403
Validation loss: 2.3749612992809666

Epoch: 6| Step: 6
Training loss: 2.618217945098877
Validation loss: 2.3675813341653473

Epoch: 6| Step: 7
Training loss: 1.5270061492919922
Validation loss: 2.366108450838315

Epoch: 6| Step: 8
Training loss: 2.3490138053894043
Validation loss: 2.3674567822487123

Epoch: 6| Step: 9
Training loss: 2.7795000076293945
Validation loss: 2.3654230410052883

Epoch: 6| Step: 10
Training loss: 3.2105276584625244
Validation loss: 2.361849125995431

Epoch: 6| Step: 11
Training loss: 2.5490689277648926
Validation loss: 2.3541449141758743

Epoch: 6| Step: 12
Training loss: 3.5215330123901367
Validation loss: 2.3565512164946525

Epoch: 6| Step: 13
Training loss: 3.025052070617676
Validation loss: 2.357637272086195

Epoch: 114| Step: 0
Training loss: 2.955874443054199
Validation loss: 2.3559374578537478

Epoch: 6| Step: 1
Training loss: 2.3798370361328125
Validation loss: 2.3502715172306186

Epoch: 6| Step: 2
Training loss: 2.0899696350097656
Validation loss: 2.3539390051236717

Epoch: 6| Step: 3
Training loss: 3.0674705505371094
Validation loss: 2.3601179610016527

Epoch: 6| Step: 4
Training loss: 2.761013984680176
Validation loss: 2.361610330561156

Epoch: 6| Step: 5
Training loss: 2.10727596282959
Validation loss: 2.3602657395024456

Epoch: 6| Step: 6
Training loss: 2.490568161010742
Validation loss: 2.360448673207273

Epoch: 6| Step: 7
Training loss: 2.3799922466278076
Validation loss: 2.3597468176195697

Epoch: 6| Step: 8
Training loss: 2.9630277156829834
Validation loss: 2.357347512757906

Epoch: 6| Step: 9
Training loss: 2.65045166015625
Validation loss: 2.3536543923039592

Epoch: 6| Step: 10
Training loss: 2.984513759613037
Validation loss: 2.3523869129919235

Epoch: 6| Step: 11
Training loss: 2.8172030448913574
Validation loss: 2.357192127935348

Epoch: 6| Step: 12
Training loss: 2.385223865509033
Validation loss: 2.363960617332048

Epoch: 6| Step: 13
Training loss: 2.5615713596343994
Validation loss: 2.3734933458348757

Epoch: 115| Step: 0
Training loss: 3.248258113861084
Validation loss: 2.371981064478556

Epoch: 6| Step: 1
Training loss: 2.325247049331665
Validation loss: 2.3721928442678144

Epoch: 6| Step: 2
Training loss: 2.764904499053955
Validation loss: 2.3768091432509886

Epoch: 6| Step: 3
Training loss: 3.393871307373047
Validation loss: 2.376286368216238

Epoch: 6| Step: 4
Training loss: 2.696193218231201
Validation loss: 2.378826518212595

Epoch: 6| Step: 5
Training loss: 2.7844295501708984
Validation loss: 2.3735525608062744

Epoch: 6| Step: 6
Training loss: 2.387694835662842
Validation loss: 2.369195061345254

Epoch: 6| Step: 7
Training loss: 2.4340686798095703
Validation loss: 2.3692666587009223

Epoch: 6| Step: 8
Training loss: 2.7509982585906982
Validation loss: 2.366991117436399

Epoch: 6| Step: 9
Training loss: 1.5851130485534668
Validation loss: 2.363390273945306

Epoch: 6| Step: 10
Training loss: 2.6028664112091064
Validation loss: 2.358408417752994

Epoch: 6| Step: 11
Training loss: 3.0604400634765625
Validation loss: 2.361331009095715

Epoch: 6| Step: 12
Training loss: 1.9062126874923706
Validation loss: 2.361199096966815

Epoch: 6| Step: 13
Training loss: 2.6403441429138184
Validation loss: 2.3570158430325088

Epoch: 116| Step: 0
Training loss: 2.6638011932373047
Validation loss: 2.347515042110156

Epoch: 6| Step: 1
Training loss: 3.5651395320892334
Validation loss: 2.350089524381904

Epoch: 6| Step: 2
Training loss: 2.3843750953674316
Validation loss: 2.353047242728613

Epoch: 6| Step: 3
Training loss: 2.803499698638916
Validation loss: 2.348952031904651

Epoch: 6| Step: 4
Training loss: 3.165057420730591
Validation loss: 2.3518517760820288

Epoch: 6| Step: 5
Training loss: 2.803678512573242
Validation loss: 2.3528924834343696

Epoch: 6| Step: 6
Training loss: 2.340595006942749
Validation loss: 2.3525334942725395

Epoch: 6| Step: 7
Training loss: 2.6297430992126465
Validation loss: 2.357715563107562

Epoch: 6| Step: 8
Training loss: 2.899324417114258
Validation loss: 2.3538517721237673

Epoch: 6| Step: 9
Training loss: 2.039156436920166
Validation loss: 2.3553925944912817

Epoch: 6| Step: 10
Training loss: 2.4673120975494385
Validation loss: 2.3570559973357827

Epoch: 6| Step: 11
Training loss: 1.6365926265716553
Validation loss: 2.3540463550116426

Epoch: 6| Step: 12
Training loss: 2.736959934234619
Validation loss: 2.3582419964574997

Epoch: 6| Step: 13
Training loss: 2.2339041233062744
Validation loss: 2.358619056722169

Epoch: 117| Step: 0
Training loss: 2.8374054431915283
Validation loss: 2.355634045857255

Epoch: 6| Step: 1
Training loss: 2.672356128692627
Validation loss: 2.3582352258825816

Epoch: 6| Step: 2
Training loss: 2.6008543968200684
Validation loss: 2.3602516381971297

Epoch: 6| Step: 3
Training loss: 2.0619115829467773
Validation loss: 2.3542847940998692

Epoch: 6| Step: 4
Training loss: 2.0058984756469727
Validation loss: 2.361625614986625

Epoch: 6| Step: 5
Training loss: 2.7439632415771484
Validation loss: 2.3609682872731197

Epoch: 6| Step: 6
Training loss: 2.3907151222229004
Validation loss: 2.362452062227393

Epoch: 6| Step: 7
Training loss: 3.283968687057495
Validation loss: 2.362912767676897

Epoch: 6| Step: 8
Training loss: 2.7928645610809326
Validation loss: 2.3655537610412924

Epoch: 6| Step: 9
Training loss: 2.419921875
Validation loss: 2.369857826540547

Epoch: 6| Step: 10
Training loss: 2.6124660968780518
Validation loss: 2.3683554100733932

Epoch: 6| Step: 11
Training loss: 3.099449396133423
Validation loss: 2.3709518217271373

Epoch: 6| Step: 12
Training loss: 2.3809709548950195
Validation loss: 2.365589754555815

Epoch: 6| Step: 13
Training loss: 2.6262221336364746
Validation loss: 2.3654663485865437

Epoch: 118| Step: 0
Training loss: 2.9545350074768066
Validation loss: 2.368932997026751

Epoch: 6| Step: 1
Training loss: 2.817457437515259
Validation loss: 2.3655776234083277

Epoch: 6| Step: 2
Training loss: 3.7368977069854736
Validation loss: 2.358718487524217

Epoch: 6| Step: 3
Training loss: 2.0981640815734863
Validation loss: 2.3609105669042116

Epoch: 6| Step: 4
Training loss: 2.0122005939483643
Validation loss: 2.362609109570903

Epoch: 6| Step: 5
Training loss: 2.906951427459717
Validation loss: 2.373518664349792

Epoch: 6| Step: 6
Training loss: 2.684812068939209
Validation loss: 2.3715025994085495

Epoch: 6| Step: 7
Training loss: 3.143479824066162
Validation loss: 2.3670641581217446

Epoch: 6| Step: 8
Training loss: 2.859248161315918
Validation loss: 2.3641728944675897

Epoch: 6| Step: 9
Training loss: 1.788435935974121
Validation loss: 2.3484465024804555

Epoch: 6| Step: 10
Training loss: 1.9979783296585083
Validation loss: 2.3561333738347536

Epoch: 6| Step: 11
Training loss: 2.243513584136963
Validation loss: 2.356109942159345

Epoch: 6| Step: 12
Training loss: 2.4745001792907715
Validation loss: 2.3511872265928533

Epoch: 6| Step: 13
Training loss: 2.8278348445892334
Validation loss: 2.357100832846857

Epoch: 119| Step: 0
Training loss: 2.6628975868225098
Validation loss: 2.3628315630779473

Epoch: 6| Step: 1
Training loss: 3.0093791484832764
Validation loss: 2.3580970969251407

Epoch: 6| Step: 2
Training loss: 2.203096866607666
Validation loss: 2.3656439191551617

Epoch: 6| Step: 3
Training loss: 2.581023931503296
Validation loss: 2.3669979059568016

Epoch: 6| Step: 4
Training loss: 3.0844802856445312
Validation loss: 2.3754736761892996

Epoch: 6| Step: 5
Training loss: 2.527266025543213
Validation loss: 2.3780423723241335

Epoch: 6| Step: 6
Training loss: 2.996795654296875
Validation loss: 2.3768793562407136

Epoch: 6| Step: 7
Training loss: 2.5554635524749756
Validation loss: 2.3858913401121735

Epoch: 6| Step: 8
Training loss: 2.8983805179595947
Validation loss: 2.3741054317002654

Epoch: 6| Step: 9
Training loss: 2.70314359664917
Validation loss: 2.3665419586243166

Epoch: 6| Step: 10
Training loss: 1.7958660125732422
Validation loss: 2.359657838780393

Epoch: 6| Step: 11
Training loss: 3.0807199478149414
Validation loss: 2.3529056759290796

Epoch: 6| Step: 12
Training loss: 2.1369690895080566
Validation loss: 2.3511626669155654

Epoch: 6| Step: 13
Training loss: 1.8197786808013916
Validation loss: 2.343984939718759

Epoch: 120| Step: 0
Training loss: 2.4463934898376465
Validation loss: 2.3446795837853545

Epoch: 6| Step: 1
Training loss: 3.8280935287475586
Validation loss: 2.348906409355902

Epoch: 6| Step: 2
Training loss: 2.553194046020508
Validation loss: 2.341259366722517

Epoch: 6| Step: 3
Training loss: 2.1155881881713867
Validation loss: 2.3367045028235323

Epoch: 6| Step: 4
Training loss: 2.5296549797058105
Validation loss: 2.3373607050987983

Epoch: 6| Step: 5
Training loss: 2.5689992904663086
Validation loss: 2.3415106573412494

Epoch: 6| Step: 6
Training loss: 2.0733718872070312
Validation loss: 2.343078890154439

Epoch: 6| Step: 7
Training loss: 2.8955280780792236
Validation loss: 2.345413766881471

Epoch: 6| Step: 8
Training loss: 3.716312885284424
Validation loss: 2.3459205499259372

Epoch: 6| Step: 9
Training loss: 1.9407645463943481
Validation loss: 2.3468464318142144

Epoch: 6| Step: 10
Training loss: 3.016042709350586
Validation loss: 2.346443742834112

Epoch: 6| Step: 11
Training loss: 2.053175926208496
Validation loss: 2.342855002290459

Epoch: 6| Step: 12
Training loss: 2.2159788608551025
Validation loss: 2.3537661695993073

Epoch: 6| Step: 13
Training loss: 2.0783135890960693
Validation loss: 2.3607080777486167

Epoch: 121| Step: 0
Training loss: 2.751659393310547
Validation loss: 2.3595279775640017

Epoch: 6| Step: 1
Training loss: 2.7921080589294434
Validation loss: 2.370353296238889

Epoch: 6| Step: 2
Training loss: 2.9031271934509277
Validation loss: 2.3792241055478334

Epoch: 6| Step: 3
Training loss: 2.3450510501861572
Validation loss: 2.389384279968918

Epoch: 6| Step: 4
Training loss: 2.304107427597046
Validation loss: 2.382294443345839

Epoch: 6| Step: 5
Training loss: 2.840924024581909
Validation loss: 2.3993247683330248

Epoch: 6| Step: 6
Training loss: 2.6088550090789795
Validation loss: 2.4044959263135026

Epoch: 6| Step: 7
Training loss: 2.337203025817871
Validation loss: 2.3920942352664087

Epoch: 6| Step: 8
Training loss: 2.723453998565674
Validation loss: 2.3805210410907702

Epoch: 6| Step: 9
Training loss: 2.87664532661438
Validation loss: 2.3854687854807866

Epoch: 6| Step: 10
Training loss: 3.124037027359009
Validation loss: 2.377976009922643

Epoch: 6| Step: 11
Training loss: 2.1959240436553955
Validation loss: 2.366619074216453

Epoch: 6| Step: 12
Training loss: 2.5605039596557617
Validation loss: 2.3637473531948623

Epoch: 6| Step: 13
Training loss: 1.6614336967468262
Validation loss: 2.3612270457770235

Epoch: 122| Step: 0
Training loss: 2.654520034790039
Validation loss: 2.363685882219704

Epoch: 6| Step: 1
Training loss: 2.7184395790100098
Validation loss: 2.3614588757996917

Epoch: 6| Step: 2
Training loss: 2.127714157104492
Validation loss: 2.364045307200442

Epoch: 6| Step: 3
Training loss: 2.7769603729248047
Validation loss: 2.3499794083256877

Epoch: 6| Step: 4
Training loss: 2.557295322418213
Validation loss: 2.36304913028594

Epoch: 6| Step: 5
Training loss: 2.1323230266571045
Validation loss: 2.366059011028659

Epoch: 6| Step: 6
Training loss: 2.506152629852295
Validation loss: 2.3713809238967074

Epoch: 6| Step: 7
Training loss: 2.212186813354492
Validation loss: 2.36734341549617

Epoch: 6| Step: 8
Training loss: 2.410602569580078
Validation loss: 2.3640022406014065

Epoch: 6| Step: 9
Training loss: 2.104947805404663
Validation loss: 2.3630986636684788

Epoch: 6| Step: 10
Training loss: 3.3062591552734375
Validation loss: 2.3606594070311515

Epoch: 6| Step: 11
Training loss: 3.211336851119995
Validation loss: 2.366479922366399

Epoch: 6| Step: 12
Training loss: 2.7801270484924316
Validation loss: 2.377434733093426

Epoch: 6| Step: 13
Training loss: 3.092714786529541
Validation loss: 2.3870916828032462

Epoch: 123| Step: 0
Training loss: 2.8654637336730957
Validation loss: 2.3903255180646013

Epoch: 6| Step: 1
Training loss: 1.7347230911254883
Validation loss: 2.3936615990054224

Epoch: 6| Step: 2
Training loss: 2.7727432250976562
Validation loss: 2.389027472465269

Epoch: 6| Step: 3
Training loss: 2.0844063758850098
Validation loss: 2.3866924060288297

Epoch: 6| Step: 4
Training loss: 2.3998305797576904
Validation loss: 2.37982205934422

Epoch: 6| Step: 5
Training loss: 1.8487298488616943
Validation loss: 2.3697042285755114

Epoch: 6| Step: 6
Training loss: 2.6672329902648926
Validation loss: 2.3673832544716458

Epoch: 6| Step: 7
Training loss: 2.628950834274292
Validation loss: 2.364094606009863

Epoch: 6| Step: 8
Training loss: 2.659005641937256
Validation loss: 2.366111291352139

Epoch: 6| Step: 9
Training loss: 2.706230401992798
Validation loss: 2.3623279038295952

Epoch: 6| Step: 10
Training loss: 3.3421783447265625
Validation loss: 2.3660629462170344

Epoch: 6| Step: 11
Training loss: 3.465710163116455
Validation loss: 2.357435785314088

Epoch: 6| Step: 12
Training loss: 2.166064500808716
Validation loss: 2.3576386103066067

Epoch: 6| Step: 13
Training loss: 3.314225673675537
Validation loss: 2.3489711592274327

Epoch: 124| Step: 0
Training loss: 1.7745361328125
Validation loss: 2.3537538846333823

Epoch: 6| Step: 1
Training loss: 3.607065200805664
Validation loss: 2.350469466178648

Epoch: 6| Step: 2
Training loss: 2.3652563095092773
Validation loss: 2.3491546428331764

Epoch: 6| Step: 3
Training loss: 2.0523953437805176
Validation loss: 2.3417755480735534

Epoch: 6| Step: 4
Training loss: 2.711833953857422
Validation loss: 2.3417415670169297

Epoch: 6| Step: 5
Training loss: 3.225264072418213
Validation loss: 2.341248712232036

Epoch: 6| Step: 6
Training loss: 2.463057279586792
Validation loss: 2.344176259092105

Epoch: 6| Step: 7
Training loss: 2.9068715572357178
Validation loss: 2.345570238687659

Epoch: 6| Step: 8
Training loss: 2.180143356323242
Validation loss: 2.3452382779890493

Epoch: 6| Step: 9
Training loss: 2.803948402404785
Validation loss: 2.34419919085759

Epoch: 6| Step: 10
Training loss: 2.293123722076416
Validation loss: 2.3434654435803814

Epoch: 6| Step: 11
Training loss: 2.554589033126831
Validation loss: 2.352706404142482

Epoch: 6| Step: 12
Training loss: 2.8240010738372803
Validation loss: 2.351653181096559

Epoch: 6| Step: 13
Training loss: 2.5564591884613037
Validation loss: 2.3546140527212494

Epoch: 125| Step: 0
Training loss: 2.9304146766662598
Validation loss: 2.3509257019207044

Epoch: 6| Step: 1
Training loss: 2.7478153705596924
Validation loss: 2.3491270516508367

Epoch: 6| Step: 2
Training loss: 2.2266416549682617
Validation loss: 2.347638145569832

Epoch: 6| Step: 3
Training loss: 3.1246917247772217
Validation loss: 2.3444783738864365

Epoch: 6| Step: 4
Training loss: 2.374709129333496
Validation loss: 2.3465186011406685

Epoch: 6| Step: 5
Training loss: 2.399113655090332
Validation loss: 2.349449537133658

Epoch: 6| Step: 6
Training loss: 1.9186958074569702
Validation loss: 2.357367964201076

Epoch: 6| Step: 7
Training loss: 1.877407431602478
Validation loss: 2.347299886006181

Epoch: 6| Step: 8
Training loss: 3.1925811767578125
Validation loss: 2.357167825903944

Epoch: 6| Step: 9
Training loss: 2.8632516860961914
Validation loss: 2.3602568872513308

Epoch: 6| Step: 10
Training loss: 2.8541769981384277
Validation loss: 2.357945316581316

Epoch: 6| Step: 11
Training loss: 2.548964500427246
Validation loss: 2.372831331786289

Epoch: 6| Step: 12
Training loss: 2.266364574432373
Validation loss: 2.376738061187088

Epoch: 6| Step: 13
Training loss: 3.1726770401000977
Validation loss: 2.364294034178539

Epoch: 126| Step: 0
Training loss: 2.5426025390625
Validation loss: 2.3800353747542187

Epoch: 6| Step: 1
Training loss: 1.4215052127838135
Validation loss: 2.372428619733421

Epoch: 6| Step: 2
Training loss: 2.85012149810791
Validation loss: 2.3849220147696872

Epoch: 6| Step: 3
Training loss: 2.4746551513671875
Validation loss: 2.3791331911599762

Epoch: 6| Step: 4
Training loss: 2.993560314178467
Validation loss: 2.3888813885309363

Epoch: 6| Step: 5
Training loss: 2.548157215118408
Validation loss: 2.383937615220265

Epoch: 6| Step: 6
Training loss: 2.513505458831787
Validation loss: 2.3796817256558325

Epoch: 6| Step: 7
Training loss: 2.8592495918273926
Validation loss: 2.3811445236206055

Epoch: 6| Step: 8
Training loss: 3.149444818496704
Validation loss: 2.367884894852997

Epoch: 6| Step: 9
Training loss: 2.6357977390289307
Validation loss: 2.373141614339685

Epoch: 6| Step: 10
Training loss: 2.4542503356933594
Validation loss: 2.3594546151417557

Epoch: 6| Step: 11
Training loss: 2.5651252269744873
Validation loss: 2.3602849462980866

Epoch: 6| Step: 12
Training loss: 2.582462787628174
Validation loss: 2.353297148981402

Epoch: 6| Step: 13
Training loss: 2.686591148376465
Validation loss: 2.3500433724413634

Epoch: 127| Step: 0
Training loss: 2.321794033050537
Validation loss: 2.343140330365909

Epoch: 6| Step: 1
Training loss: 2.24094820022583
Validation loss: 2.3384692874006046

Epoch: 6| Step: 2
Training loss: 2.4953224658966064
Validation loss: 2.338027941283359

Epoch: 6| Step: 3
Training loss: 2.7933409214019775
Validation loss: 2.3346342015010055

Epoch: 6| Step: 4
Training loss: 3.073922634124756
Validation loss: 2.3258666646096016

Epoch: 6| Step: 5
Training loss: 3.256903886795044
Validation loss: 2.33030278195617

Epoch: 6| Step: 6
Training loss: 2.551806926727295
Validation loss: 2.324007595739057

Epoch: 6| Step: 7
Training loss: 2.2484278678894043
Validation loss: 2.3256878032479236

Epoch: 6| Step: 8
Training loss: 2.389915943145752
Validation loss: 2.3193358862271873

Epoch: 6| Step: 9
Training loss: 2.662468433380127
Validation loss: 2.3253568577510055

Epoch: 6| Step: 10
Training loss: 2.5924923419952393
Validation loss: 2.326782807227104

Epoch: 6| Step: 11
Training loss: 2.3145456314086914
Validation loss: 2.3378715181863434

Epoch: 6| Step: 12
Training loss: 2.8196136951446533
Validation loss: 2.334661704237743

Epoch: 6| Step: 13
Training loss: 2.267958402633667
Validation loss: 2.339710186886531

Epoch: 128| Step: 0
Training loss: 2.598886013031006
Validation loss: 2.3448064070875927

Epoch: 6| Step: 1
Training loss: 3.127464771270752
Validation loss: 2.3362756288179787

Epoch: 6| Step: 2
Training loss: 2.5625081062316895
Validation loss: 2.328828837281914

Epoch: 6| Step: 3
Training loss: 1.9621447324752808
Validation loss: 2.32213262332383

Epoch: 6| Step: 4
Training loss: 2.1830434799194336
Validation loss: 2.3297811144141742

Epoch: 6| Step: 5
Training loss: 2.5705747604370117
Validation loss: 2.336015716675789

Epoch: 6| Step: 6
Training loss: 2.3500142097473145
Validation loss: 2.3410814987715853

Epoch: 6| Step: 7
Training loss: 2.4207115173339844
Validation loss: 2.3342440487236105

Epoch: 6| Step: 8
Training loss: 3.1309854984283447
Validation loss: 2.3403216613236295

Epoch: 6| Step: 9
Training loss: 2.443897008895874
Validation loss: 2.3443322899521037

Epoch: 6| Step: 10
Training loss: 2.463038444519043
Validation loss: 2.335602888496973

Epoch: 6| Step: 11
Training loss: 3.342317581176758
Validation loss: 2.348563504475419

Epoch: 6| Step: 12
Training loss: 2.39325213432312
Validation loss: 2.3445165952046714

Epoch: 6| Step: 13
Training loss: 2.3336355686187744
Validation loss: 2.3377826700928392

Epoch: 129| Step: 0
Training loss: 2.4764020442962646
Validation loss: 2.3527654986227713

Epoch: 6| Step: 1
Training loss: 2.3541030883789062
Validation loss: 2.3764543597416212

Epoch: 6| Step: 2
Training loss: 3.4063897132873535
Validation loss: 2.367685635884603

Epoch: 6| Step: 3
Training loss: 2.24849534034729
Validation loss: 2.3523198532801803

Epoch: 6| Step: 4
Training loss: 2.7294039726257324
Validation loss: 2.3592147058056248

Epoch: 6| Step: 5
Training loss: 2.712289333343506
Validation loss: 2.363190856031192

Epoch: 6| Step: 6
Training loss: 3.2865993976593018
Validation loss: 2.360794436547064

Epoch: 6| Step: 7
Training loss: 2.0309579372406006
Validation loss: 2.3654760929845993

Epoch: 6| Step: 8
Training loss: 2.185434103012085
Validation loss: 2.3612714480328303

Epoch: 6| Step: 9
Training loss: 3.0846052169799805
Validation loss: 2.3639336375780005

Epoch: 6| Step: 10
Training loss: 2.8778581619262695
Validation loss: 2.3656524304420716

Epoch: 6| Step: 11
Training loss: 1.9795185327529907
Validation loss: 2.3599188276516494

Epoch: 6| Step: 12
Training loss: 2.1255927085876465
Validation loss: 2.355465888977051

Epoch: 6| Step: 13
Training loss: 2.4390974044799805
Validation loss: 2.3641911911708053

Epoch: 130| Step: 0
Training loss: 2.955392837524414
Validation loss: 2.356918301633609

Epoch: 6| Step: 1
Training loss: 2.598632335662842
Validation loss: 2.3693315136817192

Epoch: 6| Step: 2
Training loss: 2.054978370666504
Validation loss: 2.3876708553683375

Epoch: 6| Step: 3
Training loss: 3.103915214538574
Validation loss: 2.42032039806407

Epoch: 6| Step: 4
Training loss: 2.8320939540863037
Validation loss: 2.434105867980629

Epoch: 6| Step: 5
Training loss: 3.13144588470459
Validation loss: 2.4180262729685795

Epoch: 6| Step: 6
Training loss: 2.831596851348877
Validation loss: 2.3934276616701515

Epoch: 6| Step: 7
Training loss: 2.601162910461426
Validation loss: 2.3843859549491637

Epoch: 6| Step: 8
Training loss: 2.319396495819092
Validation loss: 2.355938442291752

Epoch: 6| Step: 9
Training loss: 1.9320632219314575
Validation loss: 2.3478729212155907

Epoch: 6| Step: 10
Training loss: 2.494393825531006
Validation loss: 2.348743123392905

Epoch: 6| Step: 11
Training loss: 2.2327253818511963
Validation loss: 2.354371714335616

Epoch: 6| Step: 12
Training loss: 2.7328343391418457
Validation loss: 2.3557567314435075

Epoch: 6| Step: 13
Training loss: 2.6272621154785156
Validation loss: 2.355245197972944

Epoch: 131| Step: 0
Training loss: 2.5760107040405273
Validation loss: 2.372561293263589

Epoch: 6| Step: 1
Training loss: 2.3673312664031982
Validation loss: 2.382071574529012

Epoch: 6| Step: 2
Training loss: 2.238508939743042
Validation loss: 2.3807159572519283

Epoch: 6| Step: 3
Training loss: 2.742405414581299
Validation loss: 2.373738488843364

Epoch: 6| Step: 4
Training loss: 2.6504440307617188
Validation loss: 2.3674002616636214

Epoch: 6| Step: 5
Training loss: 2.836665391921997
Validation loss: 2.367164255470358

Epoch: 6| Step: 6
Training loss: 3.051443576812744
Validation loss: 2.3532599736285467

Epoch: 6| Step: 7
Training loss: 2.092165946960449
Validation loss: 2.3349100543606665

Epoch: 6| Step: 8
Training loss: 1.9788947105407715
Validation loss: 2.329572573784859

Epoch: 6| Step: 9
Training loss: 2.524170160293579
Validation loss: 2.3186210252905406

Epoch: 6| Step: 10
Training loss: 2.460850238800049
Validation loss: 2.3114657196947324

Epoch: 6| Step: 11
Training loss: 2.6787943840026855
Validation loss: 2.3077975447459886

Epoch: 6| Step: 12
Training loss: 3.0358362197875977
Validation loss: 2.3150594234466553

Epoch: 6| Step: 13
Training loss: 3.089820384979248
Validation loss: 2.318171854942076

Epoch: 132| Step: 0
Training loss: 2.390059471130371
Validation loss: 2.3273252723037556

Epoch: 6| Step: 1
Training loss: 2.27079439163208
Validation loss: 2.3321473008842877

Epoch: 6| Step: 2
Training loss: 2.8219807147979736
Validation loss: 2.3297614692359843

Epoch: 6| Step: 3
Training loss: 3.071969985961914
Validation loss: 2.3220386735854612

Epoch: 6| Step: 4
Training loss: 2.592060089111328
Validation loss: 2.323914022855861

Epoch: 6| Step: 5
Training loss: 2.2481932640075684
Validation loss: 2.3241962181624545

Epoch: 6| Step: 6
Training loss: 1.909285068511963
Validation loss: 2.329315418838173

Epoch: 6| Step: 7
Training loss: 4.009866714477539
Validation loss: 2.331502858028617

Epoch: 6| Step: 8
Training loss: 2.5469777584075928
Validation loss: 2.324763705653529

Epoch: 6| Step: 9
Training loss: 2.3690061569213867
Validation loss: 2.3265696622992076

Epoch: 6| Step: 10
Training loss: 2.2191436290740967
Validation loss: 2.329766966963327

Epoch: 6| Step: 11
Training loss: 2.32204270362854
Validation loss: 2.3230562594629105

Epoch: 6| Step: 12
Training loss: 2.4175853729248047
Validation loss: 2.3204371057530886

Epoch: 6| Step: 13
Training loss: 3.178647041320801
Validation loss: 2.3313175965380926

Epoch: 133| Step: 0
Training loss: 3.0571131706237793
Validation loss: 2.3418833594168387

Epoch: 6| Step: 1
Training loss: 2.2576208114624023
Validation loss: 2.355300070137106

Epoch: 6| Step: 2
Training loss: 1.5618805885314941
Validation loss: 2.363178901774909

Epoch: 6| Step: 3
Training loss: 2.714622974395752
Validation loss: 2.3735259117618686

Epoch: 6| Step: 4
Training loss: 2.354283571243286
Validation loss: 2.3751394107777584

Epoch: 6| Step: 5
Training loss: 1.965008020401001
Validation loss: 2.3874843812757924

Epoch: 6| Step: 6
Training loss: 2.847543239593506
Validation loss: 2.374886125646612

Epoch: 6| Step: 7
Training loss: 3.107851505279541
Validation loss: 2.3703260575571368

Epoch: 6| Step: 8
Training loss: 2.8749656677246094
Validation loss: 2.368927929991035

Epoch: 6| Step: 9
Training loss: 2.541393280029297
Validation loss: 2.3548279552049536

Epoch: 6| Step: 10
Training loss: 2.7149643898010254
Validation loss: 2.3502789184611332

Epoch: 6| Step: 11
Training loss: 2.784393787384033
Validation loss: 2.33449645708966

Epoch: 6| Step: 12
Training loss: 2.865863800048828
Validation loss: 2.32893301851006

Epoch: 6| Step: 13
Training loss: 2.2368478775024414
Validation loss: 2.3189238117587183

Epoch: 134| Step: 0
Training loss: 2.7016308307647705
Validation loss: 2.3160477299844064

Epoch: 6| Step: 1
Training loss: 2.264164447784424
Validation loss: 2.3088560412006993

Epoch: 6| Step: 2
Training loss: 2.3143553733825684
Validation loss: 2.320595766908379

Epoch: 6| Step: 3
Training loss: 2.178393840789795
Validation loss: 2.316589319577781

Epoch: 6| Step: 4
Training loss: 2.560279369354248
Validation loss: 2.326559282118274

Epoch: 6| Step: 5
Training loss: 2.3532395362854004
Validation loss: 2.332723499626242

Epoch: 6| Step: 6
Training loss: 2.4288830757141113
Validation loss: 2.350749087590043

Epoch: 6| Step: 7
Training loss: 2.6193602085113525
Validation loss: 2.351918811439186

Epoch: 6| Step: 8
Training loss: 2.3777334690093994
Validation loss: 2.340051676637383

Epoch: 6| Step: 9
Training loss: 3.5310635566711426
Validation loss: 2.3422206447970484

Epoch: 6| Step: 10
Training loss: 2.7757952213287354
Validation loss: 2.335842696569299

Epoch: 6| Step: 11
Training loss: 3.33194899559021
Validation loss: 2.328313548077819

Epoch: 6| Step: 12
Training loss: 2.0196685791015625
Validation loss: 2.325985070197813

Epoch: 6| Step: 13
Training loss: 2.749917984008789
Validation loss: 2.3157267955041703

Epoch: 135| Step: 0
Training loss: 3.1579298973083496
Validation loss: 2.3114578595725437

Epoch: 6| Step: 1
Training loss: 3.1063132286071777
Validation loss: 2.302494725873393

Epoch: 6| Step: 2
Training loss: 2.01743221282959
Validation loss: 2.304099803329796

Epoch: 6| Step: 3
Training loss: 3.2207274436950684
Validation loss: 2.301604106862058

Epoch: 6| Step: 4
Training loss: 1.8154488801956177
Validation loss: 2.3050897262429677

Epoch: 6| Step: 5
Training loss: 2.0532965660095215
Validation loss: 2.310318677656112

Epoch: 6| Step: 6
Training loss: 2.4658942222595215
Validation loss: 2.314202267636535

Epoch: 6| Step: 7
Training loss: 2.2453348636627197
Validation loss: 2.313127208781499

Epoch: 6| Step: 8
Training loss: 2.6344094276428223
Validation loss: 2.3206253128667034

Epoch: 6| Step: 9
Training loss: 2.486929416656494
Validation loss: 2.3273386083623415

Epoch: 6| Step: 10
Training loss: 2.2355563640594482
Validation loss: 2.330583539060367

Epoch: 6| Step: 11
Training loss: 2.6854658126831055
Validation loss: 2.324981668944

Epoch: 6| Step: 12
Training loss: 2.743706226348877
Validation loss: 2.3289311188523487

Epoch: 6| Step: 13
Training loss: 3.5916261672973633
Validation loss: 2.3242235875898793

Epoch: 136| Step: 0
Training loss: 2.632443428039551
Validation loss: 2.3248136017912175

Epoch: 6| Step: 1
Training loss: 2.340845823287964
Validation loss: 2.323057977102136

Epoch: 6| Step: 2
Training loss: 2.929262161254883
Validation loss: 2.3174565120409896

Epoch: 6| Step: 3
Training loss: 2.835195541381836
Validation loss: 2.3201048322903213

Epoch: 6| Step: 4
Training loss: 2.312443256378174
Validation loss: 2.3236776769802137

Epoch: 6| Step: 5
Training loss: 2.609722137451172
Validation loss: 2.3291346898642917

Epoch: 6| Step: 6
Training loss: 2.473707437515259
Validation loss: 2.347371957635367

Epoch: 6| Step: 7
Training loss: 3.330422878265381
Validation loss: 2.341881118794923

Epoch: 6| Step: 8
Training loss: 2.6284031867980957
Validation loss: 2.3587175210316977

Epoch: 6| Step: 9
Training loss: 2.876269578933716
Validation loss: 2.3526372499363397

Epoch: 6| Step: 10
Training loss: 1.5247502326965332
Validation loss: 2.343904067111272

Epoch: 6| Step: 11
Training loss: 2.758322238922119
Validation loss: 2.3433370564573552

Epoch: 6| Step: 12
Training loss: 2.7965617179870605
Validation loss: 2.333554314028832

Epoch: 6| Step: 13
Training loss: 1.3813972473144531
Validation loss: 2.332798934751941

Epoch: 137| Step: 0
Training loss: 2.4801809787750244
Validation loss: 2.3232064118949314

Epoch: 6| Step: 1
Training loss: 3.2144384384155273
Validation loss: 2.312831699207265

Epoch: 6| Step: 2
Training loss: 2.147289276123047
Validation loss: 2.3079720799640944

Epoch: 6| Step: 3
Training loss: 1.9728291034698486
Validation loss: 2.3067623658846785

Epoch: 6| Step: 4
Training loss: 2.3981711864471436
Validation loss: 2.304934517029793

Epoch: 6| Step: 5
Training loss: 3.029690742492676
Validation loss: 2.2993435628952517

Epoch: 6| Step: 6
Training loss: 3.758404016494751
Validation loss: 2.300245113270257

Epoch: 6| Step: 7
Training loss: 2.792290210723877
Validation loss: 2.3039190077012583

Epoch: 6| Step: 8
Training loss: 2.0045759677886963
Validation loss: 2.301844858354138

Epoch: 6| Step: 9
Training loss: 2.4868574142456055
Validation loss: 2.2975042763576714

Epoch: 6| Step: 10
Training loss: 2.163788318634033
Validation loss: 2.3037078124220653

Epoch: 6| Step: 11
Training loss: 2.9407589435577393
Validation loss: 2.301584647547814

Epoch: 6| Step: 12
Training loss: 1.6224443912506104
Validation loss: 2.3056796750714703

Epoch: 6| Step: 13
Training loss: 3.0942413806915283
Validation loss: 2.3062919929463375

Epoch: 138| Step: 0
Training loss: 2.787830352783203
Validation loss: 2.3015667059088267

Epoch: 6| Step: 1
Training loss: 2.774754524230957
Validation loss: 2.3080825856936875

Epoch: 6| Step: 2
Training loss: 2.2919881343841553
Validation loss: 2.313393956871443

Epoch: 6| Step: 3
Training loss: 3.223703384399414
Validation loss: 2.3124891301637054

Epoch: 6| Step: 4
Training loss: 2.761561393737793
Validation loss: 2.317692783571059

Epoch: 6| Step: 5
Training loss: 2.984060525894165
Validation loss: 2.3157731691996255

Epoch: 6| Step: 6
Training loss: 2.3961172103881836
Validation loss: 2.3184704344759703

Epoch: 6| Step: 7
Training loss: 3.0613837242126465
Validation loss: 2.312975360501197

Epoch: 6| Step: 8
Training loss: 2.600825309753418
Validation loss: 2.3158736536579747

Epoch: 6| Step: 9
Training loss: 2.439910411834717
Validation loss: 2.3255792381942912

Epoch: 6| Step: 10
Training loss: 1.9128069877624512
Validation loss: 2.324209072256601

Epoch: 6| Step: 11
Training loss: 2.1519598960876465
Validation loss: 2.3381402056704284

Epoch: 6| Step: 12
Training loss: 2.2253055572509766
Validation loss: 2.3438043363632692

Epoch: 6| Step: 13
Training loss: 1.8653454780578613
Validation loss: 2.3414599664749636

Epoch: 139| Step: 0
Training loss: 3.1783902645111084
Validation loss: 2.3389742758966263

Epoch: 6| Step: 1
Training loss: 2.7813620567321777
Validation loss: 2.3383980053727345

Epoch: 6| Step: 2
Training loss: 2.89070987701416
Validation loss: 2.3313043809706167

Epoch: 6| Step: 3
Training loss: 2.4192872047424316
Validation loss: 2.337144651720601

Epoch: 6| Step: 4
Training loss: 2.339478015899658
Validation loss: 2.334536575501965

Epoch: 6| Step: 5
Training loss: 2.253246784210205
Validation loss: 2.335919672443021

Epoch: 6| Step: 6
Training loss: 2.862842559814453
Validation loss: 2.3338921223917315

Epoch: 6| Step: 7
Training loss: 1.9649555683135986
Validation loss: 2.329388295450518

Epoch: 6| Step: 8
Training loss: 3.227431297302246
Validation loss: 2.318869388231667

Epoch: 6| Step: 9
Training loss: 2.806436061859131
Validation loss: 2.326973017825875

Epoch: 6| Step: 10
Training loss: 1.8165348768234253
Validation loss: 2.312734626954602

Epoch: 6| Step: 11
Training loss: 2.5661110877990723
Validation loss: 2.3169632496372348

Epoch: 6| Step: 12
Training loss: 2.3472466468811035
Validation loss: 2.3085111879533335

Epoch: 6| Step: 13
Training loss: 2.0813419818878174
Validation loss: 2.3152278930910173

Epoch: 140| Step: 0
Training loss: 2.2499771118164062
Validation loss: 2.310317293290169

Epoch: 6| Step: 1
Training loss: 2.500291585922241
Validation loss: 2.3129148021821053

Epoch: 6| Step: 2
Training loss: 3.0250096321105957
Validation loss: 2.310601962509976

Epoch: 6| Step: 3
Training loss: 2.7216637134552
Validation loss: 2.3117093655370895

Epoch: 6| Step: 4
Training loss: 2.961980104446411
Validation loss: 2.3017854075278006

Epoch: 6| Step: 5
Training loss: 2.6443824768066406
Validation loss: 2.3034159009174635

Epoch: 6| Step: 6
Training loss: 2.745278835296631
Validation loss: 2.297690822232154

Epoch: 6| Step: 7
Training loss: 2.1714019775390625
Validation loss: 2.2973738126857306

Epoch: 6| Step: 8
Training loss: 2.2886929512023926
Validation loss: 2.2937714310102564

Epoch: 6| Step: 9
Training loss: 2.6706204414367676
Validation loss: 2.294888286180394

Epoch: 6| Step: 10
Training loss: 2.858987808227539
Validation loss: 2.2975346221718738

Epoch: 6| Step: 11
Training loss: 2.2026500701904297
Validation loss: 2.301731603119963

Epoch: 6| Step: 12
Training loss: 2.721823215484619
Validation loss: 2.3017865970570552

Epoch: 6| Step: 13
Training loss: 1.7864384651184082
Validation loss: 2.3021866429236626

Epoch: 141| Step: 0
Training loss: 2.3504209518432617
Validation loss: 2.3011925348671536

Epoch: 6| Step: 1
Training loss: 1.9226151704788208
Validation loss: 2.3061109050627677

Epoch: 6| Step: 2
Training loss: 2.7262825965881348
Validation loss: 2.309547103861327

Epoch: 6| Step: 3
Training loss: 2.916038751602173
Validation loss: 2.30833823321968

Epoch: 6| Step: 4
Training loss: 2.843973398208618
Validation loss: 2.312043479693833

Epoch: 6| Step: 5
Training loss: 3.224081516265869
Validation loss: 2.3158302717311408

Epoch: 6| Step: 6
Training loss: 2.411362648010254
Validation loss: 2.3195495349104687

Epoch: 6| Step: 7
Training loss: 2.32540225982666
Validation loss: 2.3207936543290333

Epoch: 6| Step: 8
Training loss: 3.020315408706665
Validation loss: 2.3160060823604627

Epoch: 6| Step: 9
Training loss: 2.024278402328491
Validation loss: 2.3212090217938988

Epoch: 6| Step: 10
Training loss: 2.3507182598114014
Validation loss: 2.3198550080740326

Epoch: 6| Step: 11
Training loss: 2.9431841373443604
Validation loss: 2.323282831458635

Epoch: 6| Step: 12
Training loss: 2.3507375717163086
Validation loss: 2.314921094525245

Epoch: 6| Step: 13
Training loss: 2.1239445209503174
Validation loss: 2.3221628960742744

Epoch: 142| Step: 0
Training loss: 2.6786141395568848
Validation loss: 2.3214474493457424

Epoch: 6| Step: 1
Training loss: 2.462068557739258
Validation loss: 2.3266488736675632

Epoch: 6| Step: 2
Training loss: 2.900481700897217
Validation loss: 2.3316576378319853

Epoch: 6| Step: 3
Training loss: 2.937697410583496
Validation loss: 2.3310453558480866

Epoch: 6| Step: 4
Training loss: 2.461399555206299
Validation loss: 2.329527529337073

Epoch: 6| Step: 5
Training loss: 2.3072237968444824
Validation loss: 2.3227821293697564

Epoch: 6| Step: 6
Training loss: 2.507824420928955
Validation loss: 2.3274351217413463

Epoch: 6| Step: 7
Training loss: 2.893406867980957
Validation loss: 2.330277096840643

Epoch: 6| Step: 8
Training loss: 2.2069499492645264
Validation loss: 2.320110800445721

Epoch: 6| Step: 9
Training loss: 2.80673885345459
Validation loss: 2.3222971603434575

Epoch: 6| Step: 10
Training loss: 2.590973377227783
Validation loss: 2.3167480653332126

Epoch: 6| Step: 11
Training loss: 1.870795488357544
Validation loss: 2.313350546744562

Epoch: 6| Step: 12
Training loss: 2.7089948654174805
Validation loss: 2.3120243523710515

Epoch: 6| Step: 13
Training loss: 2.119980812072754
Validation loss: 2.3109490845793035

Epoch: 143| Step: 0
Training loss: 2.7696495056152344
Validation loss: 2.313335121318858

Epoch: 6| Step: 1
Training loss: 2.9928317070007324
Validation loss: 2.306745434320101

Epoch: 6| Step: 2
Training loss: 2.259625196456909
Validation loss: 2.309809511707675

Epoch: 6| Step: 3
Training loss: 3.214641571044922
Validation loss: 2.3123687390358216

Epoch: 6| Step: 4
Training loss: 2.3614041805267334
Validation loss: 2.2993807715754353

Epoch: 6| Step: 5
Training loss: 2.423834800720215
Validation loss: 2.297359264025124

Epoch: 6| Step: 6
Training loss: 1.6955265998840332
Validation loss: 2.2917792168996667

Epoch: 6| Step: 7
Training loss: 2.1725733280181885
Validation loss: 2.294926380598417

Epoch: 6| Step: 8
Training loss: 2.9100558757781982
Validation loss: 2.294709260745715

Epoch: 6| Step: 9
Training loss: 2.4925830364227295
Validation loss: 2.295329281078872

Epoch: 6| Step: 10
Training loss: 2.3594553470611572
Validation loss: 2.2932554547504713

Epoch: 6| Step: 11
Training loss: 1.9222562313079834
Validation loss: 2.2991920773701002

Epoch: 6| Step: 12
Training loss: 3.040593147277832
Validation loss: 2.2963479180489816

Epoch: 6| Step: 13
Training loss: 3.4470620155334473
Validation loss: 2.29794369974444

Epoch: 144| Step: 0
Training loss: 2.9764084815979004
Validation loss: 2.3026907354272823

Epoch: 6| Step: 1
Training loss: 2.154407501220703
Validation loss: 2.29663602254724

Epoch: 6| Step: 2
Training loss: 2.379847764968872
Validation loss: 2.3019482986901396

Epoch: 6| Step: 3
Training loss: 2.4593420028686523
Validation loss: 2.303520421827993

Epoch: 6| Step: 4
Training loss: 2.2975478172302246
Validation loss: 2.303098868298274

Epoch: 6| Step: 5
Training loss: 2.512025833129883
Validation loss: 2.306555219875869

Epoch: 6| Step: 6
Training loss: 2.1267518997192383
Validation loss: 2.2991082514486005

Epoch: 6| Step: 7
Training loss: 3.1713104248046875
Validation loss: 2.304436150417533

Epoch: 6| Step: 8
Training loss: 2.7836451530456543
Validation loss: 2.2901429053275817

Epoch: 6| Step: 9
Training loss: 2.484163761138916
Validation loss: 2.287050485610962

Epoch: 6| Step: 10
Training loss: 2.306267738342285
Validation loss: 2.2832350705259588

Epoch: 6| Step: 11
Training loss: 2.9144299030303955
Validation loss: 2.2773301691137333

Epoch: 6| Step: 12
Training loss: 2.4714975357055664
Validation loss: 2.280741580070988

Epoch: 6| Step: 13
Training loss: 2.7875280380249023
Validation loss: 2.2844918850929505

Epoch: 145| Step: 0
Training loss: 3.2647712230682373
Validation loss: 2.2771865296107467

Epoch: 6| Step: 1
Training loss: 2.119504451751709
Validation loss: 2.2815002215805875

Epoch: 6| Step: 2
Training loss: 2.0135719776153564
Validation loss: 2.2869922255957

Epoch: 6| Step: 3
Training loss: 3.176244020462036
Validation loss: 2.2840455937129196

Epoch: 6| Step: 4
Training loss: 2.083232879638672
Validation loss: 2.287303134959231

Epoch: 6| Step: 5
Training loss: 2.2062907218933105
Validation loss: 2.2955031189867245

Epoch: 6| Step: 6
Training loss: 2.34220552444458
Validation loss: 2.2936201659581994

Epoch: 6| Step: 7
Training loss: 2.783339500427246
Validation loss: 2.296847644672599

Epoch: 6| Step: 8
Training loss: 2.5465540885925293
Validation loss: 2.2988153760151198

Epoch: 6| Step: 9
Training loss: 2.476487874984741
Validation loss: 2.303634157744787

Epoch: 6| Step: 10
Training loss: 2.067355155944824
Validation loss: 2.307014011567639

Epoch: 6| Step: 11
Training loss: 2.2807466983795166
Validation loss: 2.312256541303409

Epoch: 6| Step: 12
Training loss: 3.3378512859344482
Validation loss: 2.308073284805462

Epoch: 6| Step: 13
Training loss: 3.3572916984558105
Validation loss: 2.305270915390343

Epoch: 146| Step: 0
Training loss: 2.6478307247161865
Validation loss: 2.3084254469922794

Epoch: 6| Step: 1
Training loss: 2.0677809715270996
Validation loss: 2.308247989223849

Epoch: 6| Step: 2
Training loss: 1.9163541793823242
Validation loss: 2.3042591105225267

Epoch: 6| Step: 3
Training loss: 2.7414088249206543
Validation loss: 2.3045972367768646

Epoch: 6| Step: 4
Training loss: 2.7307276725769043
Validation loss: 2.3031174470019597

Epoch: 6| Step: 5
Training loss: 2.209136486053467
Validation loss: 2.303758599424875

Epoch: 6| Step: 6
Training loss: 2.881138324737549
Validation loss: 2.293537255256407

Epoch: 6| Step: 7
Training loss: 2.4181361198425293
Validation loss: 2.2947554126862557

Epoch: 6| Step: 8
Training loss: 2.550185203552246
Validation loss: 2.293340572746851

Epoch: 6| Step: 9
Training loss: 2.4182167053222656
Validation loss: 2.2906001895986576

Epoch: 6| Step: 10
Training loss: 2.296158790588379
Validation loss: 2.291001796722412

Epoch: 6| Step: 11
Training loss: 2.54280424118042
Validation loss: 2.2902194197459886

Epoch: 6| Step: 12
Training loss: 2.8336145877838135
Validation loss: 2.282744023107713

Epoch: 6| Step: 13
Training loss: 4.00721549987793
Validation loss: 2.290581085348642

Epoch: 147| Step: 0
Training loss: 1.9847742319107056
Validation loss: 2.2876264459343365

Epoch: 6| Step: 1
Training loss: 3.0194332599639893
Validation loss: 2.2922446445752214

Epoch: 6| Step: 2
Training loss: 2.441549301147461
Validation loss: 2.287983420074627

Epoch: 6| Step: 3
Training loss: 1.4222782850265503
Validation loss: 2.2953920620743946

Epoch: 6| Step: 4
Training loss: 2.8411593437194824
Validation loss: 2.2928141060695855

Epoch: 6| Step: 5
Training loss: 2.2526743412017822
Validation loss: 2.2969247910284225

Epoch: 6| Step: 6
Training loss: 2.4907186031341553
Validation loss: 2.3003389194447506

Epoch: 6| Step: 7
Training loss: 2.7375807762145996
Validation loss: 2.302085562418866

Epoch: 6| Step: 8
Training loss: 3.0257041454315186
Validation loss: 2.2992080257784937

Epoch: 6| Step: 9
Training loss: 2.137141704559326
Validation loss: 2.306597440473495

Epoch: 6| Step: 10
Training loss: 2.6740951538085938
Validation loss: 2.3028913826070805

Epoch: 6| Step: 11
Training loss: 3.178246021270752
Validation loss: 2.2985342984558432

Epoch: 6| Step: 12
Training loss: 2.1516122817993164
Validation loss: 2.299021787540887

Epoch: 6| Step: 13
Training loss: 3.517482280731201
Validation loss: 2.3042177282353884

Epoch: 148| Step: 0
Training loss: 2.0056636333465576
Validation loss: 2.306961421043642

Epoch: 6| Step: 1
Training loss: 2.772113084793091
Validation loss: 2.308439846961729

Epoch: 6| Step: 2
Training loss: 2.966372013092041
Validation loss: 2.3102293783618557

Epoch: 6| Step: 3
Training loss: 2.5941662788391113
Validation loss: 2.319794157499908

Epoch: 6| Step: 4
Training loss: 2.209702968597412
Validation loss: 2.3212626211104856

Epoch: 6| Step: 5
Training loss: 2.3051114082336426
Validation loss: 2.3164723791101927

Epoch: 6| Step: 6
Training loss: 2.3144493103027344
Validation loss: 2.312524862186883

Epoch: 6| Step: 7
Training loss: 3.24570894241333
Validation loss: 2.31030975362306

Epoch: 6| Step: 8
Training loss: 1.8853645324707031
Validation loss: 2.3047744202357467

Epoch: 6| Step: 9
Training loss: 2.6281449794769287
Validation loss: 2.3079274597988335

Epoch: 6| Step: 10
Training loss: 2.657222032546997
Validation loss: 2.3233829288072485

Epoch: 6| Step: 11
Training loss: 2.787492036819458
Validation loss: 2.3282678319561865

Epoch: 6| Step: 12
Training loss: 2.412369728088379
Validation loss: 2.327512361670053

Epoch: 6| Step: 13
Training loss: 2.8841938972473145
Validation loss: 2.3385148971311507

Epoch: 149| Step: 0
Training loss: 2.1780738830566406
Validation loss: 2.334542089892972

Epoch: 6| Step: 1
Training loss: 2.1695218086242676
Validation loss: 2.3258577149401427

Epoch: 6| Step: 2
Training loss: 1.9769104719161987
Validation loss: 2.331196915718817

Epoch: 6| Step: 3
Training loss: 3.3809053897857666
Validation loss: 2.3149909665507655

Epoch: 6| Step: 4
Training loss: 2.5028738975524902
Validation loss: 2.3169946029622066

Epoch: 6| Step: 5
Training loss: 2.4475433826446533
Validation loss: 2.313868599553262

Epoch: 6| Step: 6
Training loss: 2.279116153717041
Validation loss: 2.3080970241177465

Epoch: 6| Step: 7
Training loss: 2.358562707901001
Validation loss: 2.305050734550722

Epoch: 6| Step: 8
Training loss: 3.221557855606079
Validation loss: 2.2911269434036745

Epoch: 6| Step: 9
Training loss: 2.4525492191314697
Validation loss: 2.2934560275846914

Epoch: 6| Step: 10
Training loss: 2.0897340774536133
Validation loss: 2.293921865442748

Epoch: 6| Step: 11
Training loss: 2.626988410949707
Validation loss: 2.2873536591888755

Epoch: 6| Step: 12
Training loss: 3.022104024887085
Validation loss: 2.2899139568369877

Epoch: 6| Step: 13
Training loss: 2.828669309616089
Validation loss: 2.290577842343238

Epoch: 150| Step: 0
Training loss: 2.4704463481903076
Validation loss: 2.288679120361164

Epoch: 6| Step: 1
Training loss: 2.431729316711426
Validation loss: 2.2840239155677056

Epoch: 6| Step: 2
Training loss: 2.470400333404541
Validation loss: 2.282679460381949

Epoch: 6| Step: 3
Training loss: 2.8478076457977295
Validation loss: 2.2804061110301683

Epoch: 6| Step: 4
Training loss: 2.242957830429077
Validation loss: 2.285241478232927

Epoch: 6| Step: 5
Training loss: 2.718642234802246
Validation loss: 2.280538615360055

Epoch: 6| Step: 6
Training loss: 2.510468006134033
Validation loss: 2.2811405633085515

Epoch: 6| Step: 7
Training loss: 2.244378089904785
Validation loss: 2.284075662653933

Epoch: 6| Step: 8
Training loss: 2.320080041885376
Validation loss: 2.286364575868012

Epoch: 6| Step: 9
Training loss: 2.0360865592956543
Validation loss: 2.2881118738523094

Epoch: 6| Step: 10
Training loss: 2.883608818054199
Validation loss: 2.2849420398794194

Epoch: 6| Step: 11
Training loss: 3.012993812561035
Validation loss: 2.2900160384434525

Epoch: 6| Step: 12
Training loss: 2.433879852294922
Validation loss: 2.293306666035806

Epoch: 6| Step: 13
Training loss: 2.8891613483428955
Validation loss: 2.3002327001223

Epoch: 151| Step: 0
Training loss: 2.2858200073242188
Validation loss: 2.2937050250268753

Epoch: 6| Step: 1
Training loss: 2.637916088104248
Validation loss: 2.2998325004372546

Epoch: 6| Step: 2
Training loss: 2.6947498321533203
Validation loss: 2.313912787745076

Epoch: 6| Step: 3
Training loss: 3.0127670764923096
Validation loss: 2.30652093630965

Epoch: 6| Step: 4
Training loss: 2.331308364868164
Validation loss: 2.3189677448682886

Epoch: 6| Step: 5
Training loss: 1.8976900577545166
Validation loss: 2.326881339473109

Epoch: 6| Step: 6
Training loss: 2.145432472229004
Validation loss: 2.3151671168624715

Epoch: 6| Step: 7
Training loss: 2.6022109985351562
Validation loss: 2.311383219175441

Epoch: 6| Step: 8
Training loss: 2.812870502471924
Validation loss: 2.31063675367704

Epoch: 6| Step: 9
Training loss: 2.3534047603607178
Validation loss: 2.3092160071096113

Epoch: 6| Step: 10
Training loss: 2.173539638519287
Validation loss: 2.321359944599931

Epoch: 6| Step: 11
Training loss: 2.4493415355682373
Validation loss: 2.311311347510225

Epoch: 6| Step: 12
Training loss: 3.0402541160583496
Validation loss: 2.302153851396294

Epoch: 6| Step: 13
Training loss: 3.2151501178741455
Validation loss: 2.3046179817568873

Epoch: 152| Step: 0
Training loss: 1.942851185798645
Validation loss: 2.2996259453476116

Epoch: 6| Step: 1
Training loss: 1.7764441967010498
Validation loss: 2.288063579990018

Epoch: 6| Step: 2
Training loss: 2.8236570358276367
Validation loss: 2.2901552313117572

Epoch: 6| Step: 3
Training loss: 3.196868896484375
Validation loss: 2.2882120993829544

Epoch: 6| Step: 4
Training loss: 3.0395028591156006
Validation loss: 2.283734658712982

Epoch: 6| Step: 5
Training loss: 2.316267490386963
Validation loss: 2.2795771757761636

Epoch: 6| Step: 6
Training loss: 2.628629684448242
Validation loss: 2.2822978099187217

Epoch: 6| Step: 7
Training loss: 2.67852520942688
Validation loss: 2.2737326045190134

Epoch: 6| Step: 8
Training loss: 2.1499907970428467
Validation loss: 2.280520354547808

Epoch: 6| Step: 9
Training loss: 2.1843230724334717
Validation loss: 2.2832571101445023

Epoch: 6| Step: 10
Training loss: 2.5324323177337646
Validation loss: 2.2822842828689085

Epoch: 6| Step: 11
Training loss: 2.8533310890197754
Validation loss: 2.290186194963353

Epoch: 6| Step: 12
Training loss: 2.9081523418426514
Validation loss: 2.288417103470013

Epoch: 6| Step: 13
Training loss: 2.1793477535247803
Validation loss: 2.290155464603055

Epoch: 153| Step: 0
Training loss: 2.244779586791992
Validation loss: 2.2907838436865036

Epoch: 6| Step: 1
Training loss: 2.661318302154541
Validation loss: 2.2960142345838648

Epoch: 6| Step: 2
Training loss: 2.7270898818969727
Validation loss: 2.3038661608131985

Epoch: 6| Step: 3
Training loss: 2.173990249633789
Validation loss: 2.303967455381988

Epoch: 6| Step: 4
Training loss: 2.0080487728118896
Validation loss: 2.3071480489546254

Epoch: 6| Step: 5
Training loss: 2.170971155166626
Validation loss: 2.303622120170183

Epoch: 6| Step: 6
Training loss: 2.878448247909546
Validation loss: 2.307824937246179

Epoch: 6| Step: 7
Training loss: 3.1138885021209717
Validation loss: 2.3040293057759604

Epoch: 6| Step: 8
Training loss: 2.165801763534546
Validation loss: 2.311578168663927

Epoch: 6| Step: 9
Training loss: 2.863725185394287
Validation loss: 2.304638449863721

Epoch: 6| Step: 10
Training loss: 3.105039119720459
Validation loss: 2.3038365815275457

Epoch: 6| Step: 11
Training loss: 2.402556896209717
Validation loss: 2.2972479225486837

Epoch: 6| Step: 12
Training loss: 2.764173746109009
Validation loss: 2.3098896139411518

Epoch: 6| Step: 13
Training loss: 1.6385173797607422
Validation loss: 2.3159030124705327

Epoch: 154| Step: 0
Training loss: 2.3535828590393066
Validation loss: 2.306178028865527

Epoch: 6| Step: 1
Training loss: 2.7395718097686768
Validation loss: 2.3135780775418846

Epoch: 6| Step: 2
Training loss: 1.810468077659607
Validation loss: 2.3100768545622468

Epoch: 6| Step: 3
Training loss: 2.8342337608337402
Validation loss: 2.3170864248788483

Epoch: 6| Step: 4
Training loss: 2.25622296333313
Validation loss: 2.3041871350298644

Epoch: 6| Step: 5
Training loss: 2.9868359565734863
Validation loss: 2.301636493334206

Epoch: 6| Step: 6
Training loss: 1.7877565622329712
Validation loss: 2.307818901154303

Epoch: 6| Step: 7
Training loss: 2.581177234649658
Validation loss: 2.3044532011913996

Epoch: 6| Step: 8
Training loss: 2.6299211978912354
Validation loss: 2.3036064422258766

Epoch: 6| Step: 9
Training loss: 2.5360219478607178
Validation loss: 2.309600660877843

Epoch: 6| Step: 10
Training loss: 2.8195745944976807
Validation loss: 2.304548604514009

Epoch: 6| Step: 11
Training loss: 2.84147047996521
Validation loss: 2.3019248900874967

Epoch: 6| Step: 12
Training loss: 2.7645585536956787
Validation loss: 2.2796127155262935

Epoch: 6| Step: 13
Training loss: 2.238717794418335
Validation loss: 2.2840895216952086

Epoch: 155| Step: 0
Training loss: 2.1552443504333496
Validation loss: 2.284617749593591

Epoch: 6| Step: 1
Training loss: 2.164658784866333
Validation loss: 2.279541718062534

Epoch: 6| Step: 2
Training loss: 2.481355667114258
Validation loss: 2.282681985567975

Epoch: 6| Step: 3
Training loss: 2.4725501537323
Validation loss: 2.283885867364945

Epoch: 6| Step: 4
Training loss: 2.8721022605895996
Validation loss: 2.283554842395167

Epoch: 6| Step: 5
Training loss: 2.5103976726531982
Validation loss: 2.2840291223218365

Epoch: 6| Step: 6
Training loss: 2.186525821685791
Validation loss: 2.2860066788170927

Epoch: 6| Step: 7
Training loss: 2.9994006156921387
Validation loss: 2.292427196297594

Epoch: 6| Step: 8
Training loss: 2.6416015625
Validation loss: 2.3033628938018635

Epoch: 6| Step: 9
Training loss: 1.9065333604812622
Validation loss: 2.3051966800484607

Epoch: 6| Step: 10
Training loss: 2.9360527992248535
Validation loss: 2.3140017499205885

Epoch: 6| Step: 11
Training loss: 3.1117606163024902
Validation loss: 2.3043345764119136

Epoch: 6| Step: 12
Training loss: 2.2945404052734375
Validation loss: 2.3077335998576176

Epoch: 6| Step: 13
Training loss: 2.5186409950256348
Validation loss: 2.299890664315993

Epoch: 156| Step: 0
Training loss: 1.8713393211364746
Validation loss: 2.298431234975015

Epoch: 6| Step: 1
Training loss: 3.163637161254883
Validation loss: 2.2886808328731085

Epoch: 6| Step: 2
Training loss: 2.726288080215454
Validation loss: 2.292795158201648

Epoch: 6| Step: 3
Training loss: 2.492360830307007
Validation loss: 2.2897878795541744

Epoch: 6| Step: 4
Training loss: 3.0820560455322266
Validation loss: 2.2881847504646546

Epoch: 6| Step: 5
Training loss: 2.442063808441162
Validation loss: 2.2879114715001916

Epoch: 6| Step: 6
Training loss: 2.7114663124084473
Validation loss: 2.2932458590435725

Epoch: 6| Step: 7
Training loss: 2.670261859893799
Validation loss: 2.2873598221809632

Epoch: 6| Step: 8
Training loss: 2.4236648082733154
Validation loss: 2.286073002763974

Epoch: 6| Step: 9
Training loss: 2.0542778968811035
Validation loss: 2.2769325497329875

Epoch: 6| Step: 10
Training loss: 2.5975277423858643
Validation loss: 2.276237590338594

Epoch: 6| Step: 11
Training loss: 2.4277596473693848
Validation loss: 2.274835478874945

Epoch: 6| Step: 12
Training loss: 2.624075174331665
Validation loss: 2.265561242257395

Epoch: 6| Step: 13
Training loss: 1.4931141138076782
Validation loss: 2.2663697324773318

Epoch: 157| Step: 0
Training loss: 1.9774932861328125
Validation loss: 2.271084616261144

Epoch: 6| Step: 1
Training loss: 2.006894111633301
Validation loss: 2.270639158064319

Epoch: 6| Step: 2
Training loss: 2.3970389366149902
Validation loss: 2.272135244902744

Epoch: 6| Step: 3
Training loss: 2.8485167026519775
Validation loss: 2.266744218846803

Epoch: 6| Step: 4
Training loss: 2.4386239051818848
Validation loss: 2.2701960353441137

Epoch: 6| Step: 5
Training loss: 2.8911948204040527
Validation loss: 2.2748008466536

Epoch: 6| Step: 6
Training loss: 2.864001750946045
Validation loss: 2.288693897185787

Epoch: 6| Step: 7
Training loss: 3.5342869758605957
Validation loss: 2.289832253609934

Epoch: 6| Step: 8
Training loss: 2.246859312057495
Validation loss: 2.2945268359235538

Epoch: 6| Step: 9
Training loss: 2.4130654335021973
Validation loss: 2.2894414881224274

Epoch: 6| Step: 10
Training loss: 2.282092332839966
Validation loss: 2.2943332964374172

Epoch: 6| Step: 11
Training loss: 1.6162421703338623
Validation loss: 2.2938530547644502

Epoch: 6| Step: 12
Training loss: 3.190619945526123
Validation loss: 2.2995735394057406

Epoch: 6| Step: 13
Training loss: 2.459805488586426
Validation loss: 2.2928208279353317

Epoch: 158| Step: 0
Training loss: 2.22786283493042
Validation loss: 2.2879640184422976

Epoch: 6| Step: 1
Training loss: 2.9901442527770996
Validation loss: 2.2890783868810183

Epoch: 6| Step: 2
Training loss: 2.437556743621826
Validation loss: 2.2866565053180983

Epoch: 6| Step: 3
Training loss: 2.6969072818756104
Validation loss: 2.2878869682229976

Epoch: 6| Step: 4
Training loss: 2.645026683807373
Validation loss: 2.2839857403950026

Epoch: 6| Step: 5
Training loss: 2.7878599166870117
Validation loss: 2.2764390668561383

Epoch: 6| Step: 6
Training loss: 3.0239930152893066
Validation loss: 2.275753785205144

Epoch: 6| Step: 7
Training loss: 2.6843762397766113
Validation loss: 2.272904767785021

Epoch: 6| Step: 8
Training loss: 2.7458643913269043
Validation loss: 2.2675980598695817

Epoch: 6| Step: 9
Training loss: 2.312415361404419
Validation loss: 2.268292104044268

Epoch: 6| Step: 10
Training loss: 2.2882156372070312
Validation loss: 2.265476826698549

Epoch: 6| Step: 11
Training loss: 2.0263633728027344
Validation loss: 2.267477038086102

Epoch: 6| Step: 12
Training loss: 2.1266942024230957
Validation loss: 2.266751317567723

Epoch: 6| Step: 13
Training loss: 1.9106650352478027
Validation loss: 2.266633020934238

Epoch: 159| Step: 0
Training loss: 1.677612543106079
Validation loss: 2.26701101949138

Epoch: 6| Step: 1
Training loss: 2.055168390274048
Validation loss: 2.2623035369380826

Epoch: 6| Step: 2
Training loss: 2.2066991329193115
Validation loss: 2.256769031606695

Epoch: 6| Step: 3
Training loss: 3.408608913421631
Validation loss: 2.2638239322170133

Epoch: 6| Step: 4
Training loss: 2.5891125202178955
Validation loss: 2.2572704053694204

Epoch: 6| Step: 5
Training loss: 2.2717456817626953
Validation loss: 2.2603878180185952

Epoch: 6| Step: 6
Training loss: 2.95906400680542
Validation loss: 2.2686500087861092

Epoch: 6| Step: 7
Training loss: 2.132514238357544
Validation loss: 2.2778460261642293

Epoch: 6| Step: 8
Training loss: 3.05342435836792
Validation loss: 2.2948476268399145

Epoch: 6| Step: 9
Training loss: 3.0555410385131836
Validation loss: 2.291181905295259

Epoch: 6| Step: 10
Training loss: 2.46574330329895
Validation loss: 2.305995305379232

Epoch: 6| Step: 11
Training loss: 1.9588468074798584
Validation loss: 2.2943581483697377

Epoch: 6| Step: 12
Training loss: 2.9427707195281982
Validation loss: 2.2934888127029582

Epoch: 6| Step: 13
Training loss: 2.3366339206695557
Validation loss: 2.297516671560144

Epoch: 160| Step: 0
Training loss: 3.0339927673339844
Validation loss: 2.2956497771765596

Epoch: 6| Step: 1
Training loss: 2.112602710723877
Validation loss: 2.2938131183706303

Epoch: 6| Step: 2
Training loss: 2.558192729949951
Validation loss: 2.283783507603471

Epoch: 6| Step: 3
Training loss: 2.5139079093933105
Validation loss: 2.2919684404967935

Epoch: 6| Step: 4
Training loss: 1.9379123449325562
Validation loss: 2.2883364385174167

Epoch: 6| Step: 5
Training loss: 2.676038980484009
Validation loss: 2.28338534344909

Epoch: 6| Step: 6
Training loss: 2.739905834197998
Validation loss: 2.2886713192027104

Epoch: 6| Step: 7
Training loss: 2.853376865386963
Validation loss: 2.282980493319932

Epoch: 6| Step: 8
Training loss: 1.8430376052856445
Validation loss: 2.273581268966839

Epoch: 6| Step: 9
Training loss: 2.182596445083618
Validation loss: 2.282044285087175

Epoch: 6| Step: 10
Training loss: 1.9647544622421265
Validation loss: 2.2705067614073395

Epoch: 6| Step: 11
Training loss: 3.0951290130615234
Validation loss: 2.2854599773242907

Epoch: 6| Step: 12
Training loss: 3.3892245292663574
Validation loss: 2.271807752629762

Epoch: 6| Step: 13
Training loss: 1.6753164529800415
Validation loss: 2.274342011379939

Epoch: 161| Step: 0
Training loss: 1.9588831663131714
Validation loss: 2.273585322082684

Epoch: 6| Step: 1
Training loss: 2.7062127590179443
Validation loss: 2.276677413653302

Epoch: 6| Step: 2
Training loss: 2.385498046875
Validation loss: 2.2748826831899662

Epoch: 6| Step: 3
Training loss: 2.3585205078125
Validation loss: 2.2743029953331075

Epoch: 6| Step: 4
Training loss: 2.3725814819335938
Validation loss: 2.2772991529075046

Epoch: 6| Step: 5
Training loss: 2.4881038665771484
Validation loss: 2.269578513278756

Epoch: 6| Step: 6
Training loss: 2.6225361824035645
Validation loss: 2.2766684652656637

Epoch: 6| Step: 7
Training loss: 2.251929759979248
Validation loss: 2.279407642220938

Epoch: 6| Step: 8
Training loss: 2.623427391052246
Validation loss: 2.287826935450236

Epoch: 6| Step: 9
Training loss: 2.546444892883301
Validation loss: 2.278849437672605

Epoch: 6| Step: 10
Training loss: 2.663571834564209
Validation loss: 2.272505074419001

Epoch: 6| Step: 11
Training loss: 3.170109748840332
Validation loss: 2.266655029789094

Epoch: 6| Step: 12
Training loss: 2.6722049713134766
Validation loss: 2.2643460919780116

Epoch: 6| Step: 13
Training loss: 2.226740837097168
Validation loss: 2.2589276170217865

Epoch: 162| Step: 0
Training loss: 2.699828624725342
Validation loss: 2.256631787105273

Epoch: 6| Step: 1
Training loss: 1.9158146381378174
Validation loss: 2.265700332580074

Epoch: 6| Step: 2
Training loss: 2.6563334465026855
Validation loss: 2.285682705140883

Epoch: 6| Step: 3
Training loss: 3.3637561798095703
Validation loss: 2.2997001012166343

Epoch: 6| Step: 4
Training loss: 2.8864924907684326
Validation loss: 2.3183890747767624

Epoch: 6| Step: 5
Training loss: 2.352755069732666
Validation loss: 2.321523934282282

Epoch: 6| Step: 6
Training loss: 2.3510351181030273
Validation loss: 2.31712334386764

Epoch: 6| Step: 7
Training loss: 2.5661654472351074
Validation loss: 2.2910921368547665

Epoch: 6| Step: 8
Training loss: 2.2489681243896484
Validation loss: 2.2850487334753877

Epoch: 6| Step: 9
Training loss: 2.458505630493164
Validation loss: 2.278330131243634

Epoch: 6| Step: 10
Training loss: 2.303670883178711
Validation loss: 2.2689484165560816

Epoch: 6| Step: 11
Training loss: 2.0879323482513428
Validation loss: 2.270068214785668

Epoch: 6| Step: 12
Training loss: 2.8864381313323975
Validation loss: 2.2694899625675653

Epoch: 6| Step: 13
Training loss: 2.4767093658447266
Validation loss: 2.270749463829943

Epoch: 163| Step: 0
Training loss: 3.0039639472961426
Validation loss: 2.271462896818756

Epoch: 6| Step: 1
Training loss: 2.848942995071411
Validation loss: 2.272570222936651

Epoch: 6| Step: 2
Training loss: 2.461221694946289
Validation loss: 2.277237315331736

Epoch: 6| Step: 3
Training loss: 2.349867105484009
Validation loss: 2.283051106237596

Epoch: 6| Step: 4
Training loss: 2.8420004844665527
Validation loss: 2.2813541043189263

Epoch: 6| Step: 5
Training loss: 1.9912965297698975
Validation loss: 2.2942516316649733

Epoch: 6| Step: 6
Training loss: 2.339048147201538
Validation loss: 2.2923377534394622

Epoch: 6| Step: 7
Training loss: 1.5914578437805176
Validation loss: 2.292299732085197

Epoch: 6| Step: 8
Training loss: 2.396644115447998
Validation loss: 2.2961963222872828

Epoch: 6| Step: 9
Training loss: 2.5015463829040527
Validation loss: 2.2902008948787564

Epoch: 6| Step: 10
Training loss: 3.407963514328003
Validation loss: 2.2790386933152393

Epoch: 6| Step: 11
Training loss: 2.603140115737915
Validation loss: 2.2754900096565165

Epoch: 6| Step: 12
Training loss: 2.632634162902832
Validation loss: 2.262185058286113

Epoch: 6| Step: 13
Training loss: 2.41440749168396
Validation loss: 2.256508127335579

Epoch: 164| Step: 0
Training loss: 3.237215995788574
Validation loss: 2.2627566270930792

Epoch: 6| Step: 1
Training loss: 2.6265509128570557
Validation loss: 2.265147393749606

Epoch: 6| Step: 2
Training loss: 2.539616107940674
Validation loss: 2.2679450332477527

Epoch: 6| Step: 3
Training loss: 2.6796770095825195
Validation loss: 2.259896784700373

Epoch: 6| Step: 4
Training loss: 2.7135090827941895
Validation loss: 2.2678375961960002

Epoch: 6| Step: 5
Training loss: 2.898585081100464
Validation loss: 2.259028193771198

Epoch: 6| Step: 6
Training loss: 2.118241548538208
Validation loss: 2.2643712515472085

Epoch: 6| Step: 7
Training loss: 1.898754358291626
Validation loss: 2.268987765876196

Epoch: 6| Step: 8
Training loss: 2.4913179874420166
Validation loss: 2.2729178449159027

Epoch: 6| Step: 9
Training loss: 2.5102791786193848
Validation loss: 2.2722091187712965

Epoch: 6| Step: 10
Training loss: 2.0476267337799072
Validation loss: 2.2717041841117283

Epoch: 6| Step: 11
Training loss: 2.70955753326416
Validation loss: 2.262190503458823

Epoch: 6| Step: 12
Training loss: 2.0433783531188965
Validation loss: 2.270459621183334

Epoch: 6| Step: 13
Training loss: 2.736912488937378
Validation loss: 2.269512414932251

Epoch: 165| Step: 0
Training loss: 2.459414482116699
Validation loss: 2.2677350351887364

Epoch: 6| Step: 1
Training loss: 3.1918933391571045
Validation loss: 2.2717173971155638

Epoch: 6| Step: 2
Training loss: 2.858196973800659
Validation loss: 2.272690119281892

Epoch: 6| Step: 3
Training loss: 1.8907182216644287
Validation loss: 2.2763386849434144

Epoch: 6| Step: 4
Training loss: 2.7752277851104736
Validation loss: 2.2780871750206075

Epoch: 6| Step: 5
Training loss: 2.502483367919922
Validation loss: 2.285336507264004

Epoch: 6| Step: 6
Training loss: 3.695174217224121
Validation loss: 2.2793058246694584

Epoch: 6| Step: 7
Training loss: 2.6110894680023193
Validation loss: 2.2761641548525904

Epoch: 6| Step: 8
Training loss: 1.5911954641342163
Validation loss: 2.271367734478366

Epoch: 6| Step: 9
Training loss: 1.984515905380249
Validation loss: 2.2753850926635084

Epoch: 6| Step: 10
Training loss: 2.7011678218841553
Validation loss: 2.2805380129045054

Epoch: 6| Step: 11
Training loss: 2.0072507858276367
Validation loss: 2.275306168422904

Epoch: 6| Step: 12
Training loss: 1.8659894466400146
Validation loss: 2.2752494119828746

Epoch: 6| Step: 13
Training loss: 3.146421194076538
Validation loss: 2.278270624017203

Epoch: 166| Step: 0
Training loss: 2.693338394165039
Validation loss: 2.275620343864605

Epoch: 6| Step: 1
Training loss: 1.1200165748596191
Validation loss: 2.267143916058284

Epoch: 6| Step: 2
Training loss: 2.884906768798828
Validation loss: 2.262567186868319

Epoch: 6| Step: 3
Training loss: 2.5357933044433594
Validation loss: 2.2638827036785822

Epoch: 6| Step: 4
Training loss: 2.4358856678009033
Validation loss: 2.2576562550760086

Epoch: 6| Step: 5
Training loss: 3.0049750804901123
Validation loss: 2.25433176050904

Epoch: 6| Step: 6
Training loss: 2.2018203735351562
Validation loss: 2.2535486375131915

Epoch: 6| Step: 7
Training loss: 2.652256965637207
Validation loss: 2.253546691709949

Epoch: 6| Step: 8
Training loss: 2.0179758071899414
Validation loss: 2.250160824867987

Epoch: 6| Step: 9
Training loss: 3.3007149696350098
Validation loss: 2.2605830418166293

Epoch: 6| Step: 10
Training loss: 2.5226612091064453
Validation loss: 2.264733455514395

Epoch: 6| Step: 11
Training loss: 1.738461971282959
Validation loss: 2.266487093381984

Epoch: 6| Step: 12
Training loss: 2.7843282222747803
Validation loss: 2.275415387204898

Epoch: 6| Step: 13
Training loss: 3.4321961402893066
Validation loss: 2.270886951877225

Epoch: 167| Step: 0
Training loss: 2.83149790763855
Validation loss: 2.2759352319984028

Epoch: 6| Step: 1
Training loss: 2.658069610595703
Validation loss: 2.278793696434267

Epoch: 6| Step: 2
Training loss: 2.013291358947754
Validation loss: 2.277694045856435

Epoch: 6| Step: 3
Training loss: 3.0020737648010254
Validation loss: 2.281371816512077

Epoch: 6| Step: 4
Training loss: 2.674011707305908
Validation loss: 2.2805921121310164

Epoch: 6| Step: 5
Training loss: 2.5368356704711914
Validation loss: 2.2835515442714898

Epoch: 6| Step: 6
Training loss: 2.4783365726470947
Validation loss: 2.293462686641242

Epoch: 6| Step: 7
Training loss: 2.72021484375
Validation loss: 2.3034889544210126

Epoch: 6| Step: 8
Training loss: 3.328950881958008
Validation loss: 2.307388469737063

Epoch: 6| Step: 9
Training loss: 2.3278701305389404
Validation loss: 2.3089502934486634

Epoch: 6| Step: 10
Training loss: 2.2557179927825928
Validation loss: 2.3028103664357173

Epoch: 6| Step: 11
Training loss: 1.5804201364517212
Validation loss: 2.296153155706262

Epoch: 6| Step: 12
Training loss: 2.22477126121521
Validation loss: 2.2983201139716694

Epoch: 6| Step: 13
Training loss: 2.0371294021606445
Validation loss: 2.287358150687269

Epoch: 168| Step: 0
Training loss: 2.389021396636963
Validation loss: 2.2809953894666446

Epoch: 6| Step: 1
Training loss: 2.486518383026123
Validation loss: 2.2825257624349287

Epoch: 6| Step: 2
Training loss: 2.1457972526550293
Validation loss: 2.2820916816752446

Epoch: 6| Step: 3
Training loss: 2.305792808532715
Validation loss: 2.269858378236012

Epoch: 6| Step: 4
Training loss: 1.9638174772262573
Validation loss: 2.260528523434875

Epoch: 6| Step: 5
Training loss: 3.2845969200134277
Validation loss: 2.271712153188644

Epoch: 6| Step: 6
Training loss: 3.345343828201294
Validation loss: 2.265619267699539

Epoch: 6| Step: 7
Training loss: 2.358751058578491
Validation loss: 2.2604652399657876

Epoch: 6| Step: 8
Training loss: 1.8362846374511719
Validation loss: 2.257729753371208

Epoch: 6| Step: 9
Training loss: 3.138007640838623
Validation loss: 2.2584786402281893

Epoch: 6| Step: 10
Training loss: 2.643732786178589
Validation loss: 2.2604985852395334

Epoch: 6| Step: 11
Training loss: 2.395395040512085
Validation loss: 2.254922510475241

Epoch: 6| Step: 12
Training loss: 1.8063678741455078
Validation loss: 2.2562076853167627

Epoch: 6| Step: 13
Training loss: 2.848205804824829
Validation loss: 2.261491083329724

Epoch: 169| Step: 0
Training loss: 2.298752784729004
Validation loss: 2.2599362993753083

Epoch: 6| Step: 1
Training loss: 2.921030044555664
Validation loss: 2.2666379610697427

Epoch: 6| Step: 2
Training loss: 2.1185290813446045
Validation loss: 2.262680399802423

Epoch: 6| Step: 3
Training loss: 2.2924814224243164
Validation loss: 2.2687449647534277

Epoch: 6| Step: 4
Training loss: 2.5674192905426025
Validation loss: 2.263204554075836

Epoch: 6| Step: 5
Training loss: 2.153015613555908
Validation loss: 2.2672220045520413

Epoch: 6| Step: 6
Training loss: 2.4755053520202637
Validation loss: 2.259824396461569

Epoch: 6| Step: 7
Training loss: 2.405059814453125
Validation loss: 2.2682062528466664

Epoch: 6| Step: 8
Training loss: 3.356041193008423
Validation loss: 2.2779495318730674

Epoch: 6| Step: 9
Training loss: 3.0121376514434814
Validation loss: 2.2841014092968357

Epoch: 6| Step: 10
Training loss: 2.4222211837768555
Validation loss: 2.2893609436609412

Epoch: 6| Step: 11
Training loss: 2.338040828704834
Validation loss: 2.2861222041550504

Epoch: 6| Step: 12
Training loss: 2.066920042037964
Validation loss: 2.2842805770135697

Epoch: 6| Step: 13
Training loss: 2.2788870334625244
Validation loss: 2.2775945253269647

Epoch: 170| Step: 0
Training loss: 2.7339303493499756
Validation loss: 2.272843260918894

Epoch: 6| Step: 1
Training loss: 2.2672019004821777
Validation loss: 2.271330779598605

Epoch: 6| Step: 2
Training loss: 3.11488676071167
Validation loss: 2.274487785113755

Epoch: 6| Step: 3
Training loss: 2.1866283416748047
Validation loss: 2.2761323708359913

Epoch: 6| Step: 4
Training loss: 2.221893787384033
Validation loss: 2.2781440224698795

Epoch: 6| Step: 5
Training loss: 2.1690897941589355
Validation loss: 2.269901070543515

Epoch: 6| Step: 6
Training loss: 2.3452703952789307
Validation loss: 2.283267701825788

Epoch: 6| Step: 7
Training loss: 2.8452916145324707
Validation loss: 2.2827184610469367

Epoch: 6| Step: 8
Training loss: 2.4470345973968506
Validation loss: 2.273065528561992

Epoch: 6| Step: 9
Training loss: 2.4276018142700195
Validation loss: 2.265902853781177

Epoch: 6| Step: 10
Training loss: 2.3090403079986572
Validation loss: 2.25972585011554

Epoch: 6| Step: 11
Training loss: 1.839646816253662
Validation loss: 2.253093170863326

Epoch: 6| Step: 12
Training loss: 3.0820910930633545
Validation loss: 2.259738411954654

Epoch: 6| Step: 13
Training loss: 2.916198492050171
Validation loss: 2.2591048145806916

Epoch: 171| Step: 0
Training loss: 2.358311176300049
Validation loss: 2.2498734689527944

Epoch: 6| Step: 1
Training loss: 1.5445353984832764
Validation loss: 2.2490131931920208

Epoch: 6| Step: 2
Training loss: 2.356161594390869
Validation loss: 2.2578298250834146

Epoch: 6| Step: 3
Training loss: 1.8112235069274902
Validation loss: 2.2406606443466677

Epoch: 6| Step: 4
Training loss: 2.5745768547058105
Validation loss: 2.2481691811674382

Epoch: 6| Step: 5
Training loss: 2.799018144607544
Validation loss: 2.256130277469594

Epoch: 6| Step: 6
Training loss: 2.4307870864868164
Validation loss: 2.2535729459536973

Epoch: 6| Step: 7
Training loss: 2.8684189319610596
Validation loss: 2.2496049942508822

Epoch: 6| Step: 8
Training loss: 2.316965103149414
Validation loss: 2.2505587582947104

Epoch: 6| Step: 9
Training loss: 2.26766300201416
Validation loss: 2.253683082519039

Epoch: 6| Step: 10
Training loss: 4.2437214851379395
Validation loss: 2.264372766658824

Epoch: 6| Step: 11
Training loss: 1.9810601472854614
Validation loss: 2.2575918346322994

Epoch: 6| Step: 12
Training loss: 2.105379343032837
Validation loss: 2.2617218955870597

Epoch: 6| Step: 13
Training loss: 3.665231466293335
Validation loss: 2.2616011199130805

Epoch: 172| Step: 0
Training loss: 1.7725621461868286
Validation loss: 2.2645485119153093

Epoch: 6| Step: 1
Training loss: 2.9267263412475586
Validation loss: 2.2646101597816712

Epoch: 6| Step: 2
Training loss: 2.782822847366333
Validation loss: 2.2594960876690444

Epoch: 6| Step: 3
Training loss: 1.9636762142181396
Validation loss: 2.260612428829234

Epoch: 6| Step: 4
Training loss: 2.267810821533203
Validation loss: 2.257260784026115

Epoch: 6| Step: 5
Training loss: 2.2773208618164062
Validation loss: 2.2593594494686333

Epoch: 6| Step: 6
Training loss: 2.6993284225463867
Validation loss: 2.2602101833589616

Epoch: 6| Step: 7
Training loss: 2.736405372619629
Validation loss: 2.2560865238148677

Epoch: 6| Step: 8
Training loss: 3.002603054046631
Validation loss: 2.2489768779405983

Epoch: 6| Step: 9
Training loss: 2.3947243690490723
Validation loss: 2.2473655490465063

Epoch: 6| Step: 10
Training loss: 2.6991233825683594
Validation loss: 2.2462025150176017

Epoch: 6| Step: 11
Training loss: 2.1245594024658203
Validation loss: 2.2567181253945954

Epoch: 6| Step: 12
Training loss: 2.7621169090270996
Validation loss: 2.2629174775974725

Epoch: 6| Step: 13
Training loss: 1.8860212564468384
Validation loss: 2.2600788454855643

Epoch: 173| Step: 0
Training loss: 2.0134899616241455
Validation loss: 2.2647982951133483

Epoch: 6| Step: 1
Training loss: 3.327211856842041
Validation loss: 2.274735407162738

Epoch: 6| Step: 2
Training loss: 2.7985270023345947
Validation loss: 2.2786643710187686

Epoch: 6| Step: 3
Training loss: 2.062404155731201
Validation loss: 2.2765660721768617

Epoch: 6| Step: 4
Training loss: 2.4453577995300293
Validation loss: 2.288115098912229

Epoch: 6| Step: 5
Training loss: 2.63777494430542
Validation loss: 2.2847212655569917

Epoch: 6| Step: 6
Training loss: 2.2200863361358643
Validation loss: 2.284998583537276

Epoch: 6| Step: 7
Training loss: 1.982495665550232
Validation loss: 2.294099284756568

Epoch: 6| Step: 8
Training loss: 2.596344470977783
Validation loss: 2.2847288526514524

Epoch: 6| Step: 9
Training loss: 1.3944185972213745
Validation loss: 2.2719454560228574

Epoch: 6| Step: 10
Training loss: 2.635936737060547
Validation loss: 2.267409383609731

Epoch: 6| Step: 11
Training loss: 2.916051149368286
Validation loss: 2.2598201356908327

Epoch: 6| Step: 12
Training loss: 2.8582444190979004
Validation loss: 2.252285821463472

Epoch: 6| Step: 13
Training loss: 2.7609775066375732
Validation loss: 2.255893768802766

Epoch: 174| Step: 0
Training loss: 2.4905824661254883
Validation loss: 2.2416071635420605

Epoch: 6| Step: 1
Training loss: 2.3204283714294434
Validation loss: 2.238273004049896

Epoch: 6| Step: 2
Training loss: 2.3483874797821045
Validation loss: 2.2316452380149596

Epoch: 6| Step: 3
Training loss: 2.391836166381836
Validation loss: 2.23201810672719

Epoch: 6| Step: 4
Training loss: 2.7767231464385986
Validation loss: 2.2238889112267444

Epoch: 6| Step: 5
Training loss: 2.652783155441284
Validation loss: 2.220424780281641

Epoch: 6| Step: 6
Training loss: 1.9636247158050537
Validation loss: 2.2256484121404667

Epoch: 6| Step: 7
Training loss: 2.606962203979492
Validation loss: 2.2246745171085482

Epoch: 6| Step: 8
Training loss: 2.085268497467041
Validation loss: 2.2269549779994513

Epoch: 6| Step: 9
Training loss: 3.0841922760009766
Validation loss: 2.2293616546097623

Epoch: 6| Step: 10
Training loss: 1.9993336200714111
Validation loss: 2.220643333209458

Epoch: 6| Step: 11
Training loss: 2.403467893600464
Validation loss: 2.232519115171125

Epoch: 6| Step: 12
Training loss: 2.629610061645508
Validation loss: 2.2324031270960325

Epoch: 6| Step: 13
Training loss: 2.98639178276062
Validation loss: 2.2407611416232203

Epoch: 175| Step: 0
Training loss: 2.833095073699951
Validation loss: 2.2407647871202037

Epoch: 6| Step: 1
Training loss: 2.7390270233154297
Validation loss: 2.2522571086883545

Epoch: 6| Step: 2
Training loss: 2.234290361404419
Validation loss: 2.2595132243248726

Epoch: 6| Step: 3
Training loss: 3.253981828689575
Validation loss: 2.2600044383797595

Epoch: 6| Step: 4
Training loss: 2.129638671875
Validation loss: 2.254294587719825

Epoch: 6| Step: 5
Training loss: 2.831972599029541
Validation loss: 2.266273324207593

Epoch: 6| Step: 6
Training loss: 2.8402891159057617
Validation loss: 2.2541155225487164

Epoch: 6| Step: 7
Training loss: 2.0828399658203125
Validation loss: 2.25092980938573

Epoch: 6| Step: 8
Training loss: 2.0692479610443115
Validation loss: 2.253025067749844

Epoch: 6| Step: 9
Training loss: 2.042551279067993
Validation loss: 2.256704886754354

Epoch: 6| Step: 10
Training loss: 2.837449550628662
Validation loss: 2.257454172257454

Epoch: 6| Step: 11
Training loss: 2.2563395500183105
Validation loss: 2.263069055413687

Epoch: 6| Step: 12
Training loss: 2.106419563293457
Validation loss: 2.256572074787591

Epoch: 6| Step: 13
Training loss: 2.1832516193389893
Validation loss: 2.2653962566006567

Epoch: 176| Step: 0
Training loss: 2.751845598220825
Validation loss: 2.257522636844266

Epoch: 6| Step: 1
Training loss: 2.1329922676086426
Validation loss: 2.249442067197574

Epoch: 6| Step: 2
Training loss: 2.182743549346924
Validation loss: 2.262894135649486

Epoch: 6| Step: 3
Training loss: 2.6065945625305176
Validation loss: 2.259126706789899

Epoch: 6| Step: 4
Training loss: 2.787593364715576
Validation loss: 2.2561922996274886

Epoch: 6| Step: 5
Training loss: 3.1788458824157715
Validation loss: 2.2617247899373374

Epoch: 6| Step: 6
Training loss: 2.1979644298553467
Validation loss: 2.2600828652740805

Epoch: 6| Step: 7
Training loss: 2.2851715087890625
Validation loss: 2.253375509733795

Epoch: 6| Step: 8
Training loss: 3.0954456329345703
Validation loss: 2.236386009441909

Epoch: 6| Step: 9
Training loss: 2.0512328147888184
Validation loss: 2.232712427775065

Epoch: 6| Step: 10
Training loss: 2.1267905235290527
Validation loss: 2.228455058989986

Epoch: 6| Step: 11
Training loss: 2.0059428215026855
Validation loss: 2.2238536598861858

Epoch: 6| Step: 12
Training loss: 2.451321601867676
Validation loss: 2.2246304660715084

Epoch: 6| Step: 13
Training loss: 2.870293617248535
Validation loss: 2.225533295703191

Epoch: 177| Step: 0
Training loss: 2.0086019039154053
Validation loss: 2.2309033486150924

Epoch: 6| Step: 1
Training loss: 2.4687418937683105
Validation loss: 2.2329681842557845

Epoch: 6| Step: 2
Training loss: 2.601947546005249
Validation loss: 2.2464795368973927

Epoch: 6| Step: 3
Training loss: 1.9532538652420044
Validation loss: 2.261003919827041

Epoch: 6| Step: 4
Training loss: 2.968827247619629
Validation loss: 2.2620643774668374

Epoch: 6| Step: 5
Training loss: 2.5901083946228027
Validation loss: 2.2821153774056384

Epoch: 6| Step: 6
Training loss: 1.471787929534912
Validation loss: 2.283996699958719

Epoch: 6| Step: 7
Training loss: 2.211871385574341
Validation loss: 2.273584806790916

Epoch: 6| Step: 8
Training loss: 2.5298235416412354
Validation loss: 2.286501193559298

Epoch: 6| Step: 9
Training loss: 2.897003650665283
Validation loss: 2.273550243787868

Epoch: 6| Step: 10
Training loss: 3.036816120147705
Validation loss: 2.298291990833898

Epoch: 6| Step: 11
Training loss: 2.244053840637207
Validation loss: 2.2910094876443186

Epoch: 6| Step: 12
Training loss: 3.4315733909606934
Validation loss: 2.3057515903185775

Epoch: 6| Step: 13
Training loss: 1.9317667484283447
Validation loss: 2.2831804111439693

Epoch: 178| Step: 0
Training loss: 3.1687116622924805
Validation loss: 2.2695897086974113

Epoch: 6| Step: 1
Training loss: 3.043503761291504
Validation loss: 2.265090412991021

Epoch: 6| Step: 2
Training loss: 2.615783214569092
Validation loss: 2.2500793395503873

Epoch: 6| Step: 3
Training loss: 2.7757086753845215
Validation loss: 2.2448788304482736

Epoch: 6| Step: 4
Training loss: 1.9364100694656372
Validation loss: 2.2351344221381733

Epoch: 6| Step: 5
Training loss: 2.0303094387054443
Validation loss: 2.2307714582771383

Epoch: 6| Step: 6
Training loss: 2.2696919441223145
Validation loss: 2.234933102002708

Epoch: 6| Step: 7
Training loss: 2.4476828575134277
Validation loss: 2.232913911983531

Epoch: 6| Step: 8
Training loss: 2.4007136821746826
Validation loss: 2.22526874080781

Epoch: 6| Step: 9
Training loss: 2.447869062423706
Validation loss: 2.230244598081035

Epoch: 6| Step: 10
Training loss: 2.514704465866089
Validation loss: 2.22446322184737

Epoch: 6| Step: 11
Training loss: 2.4924774169921875
Validation loss: 2.228091921857608

Epoch: 6| Step: 12
Training loss: 2.436704397201538
Validation loss: 2.2306151595166934

Epoch: 6| Step: 13
Training loss: 1.2871105670928955
Validation loss: 2.230873379656064

Epoch: 179| Step: 0
Training loss: 1.8666765689849854
Validation loss: 2.24324538887188

Epoch: 6| Step: 1
Training loss: 2.2056403160095215
Validation loss: 2.261026850310705

Epoch: 6| Step: 2
Training loss: 2.2188241481781006
Validation loss: 2.261236298468805

Epoch: 6| Step: 3
Training loss: 2.3974382877349854
Validation loss: 2.2595394016594015

Epoch: 6| Step: 4
Training loss: 2.3818953037261963
Validation loss: 2.2478174163449194

Epoch: 6| Step: 5
Training loss: 2.535618543624878
Validation loss: 2.247013117677422

Epoch: 6| Step: 6
Training loss: 3.0206947326660156
Validation loss: 2.2491750665890273

Epoch: 6| Step: 7
Training loss: 2.263176441192627
Validation loss: 2.246813858709028

Epoch: 6| Step: 8
Training loss: 3.2233402729034424
Validation loss: 2.2364155810366393

Epoch: 6| Step: 9
Training loss: 2.5272722244262695
Validation loss: 2.23694021214721

Epoch: 6| Step: 10
Training loss: 2.1541130542755127
Validation loss: 2.2358224417573664

Epoch: 6| Step: 11
Training loss: 2.192061424255371
Validation loss: 2.2394683155962216

Epoch: 6| Step: 12
Training loss: 2.8023762702941895
Validation loss: 2.2360785597114154

Epoch: 6| Step: 13
Training loss: 2.7737550735473633
Validation loss: 2.2258241099696003

Epoch: 180| Step: 0
Training loss: 2.17218279838562
Validation loss: 2.2261770040758195

Epoch: 6| Step: 1
Training loss: 1.9907457828521729
Validation loss: 2.228160858154297

Epoch: 6| Step: 2
Training loss: 1.9626734256744385
Validation loss: 2.224379406180433

Epoch: 6| Step: 3
Training loss: 2.1640267372131348
Validation loss: 2.2197888820402083

Epoch: 6| Step: 4
Training loss: 2.8433024883270264
Validation loss: 2.211886047035135

Epoch: 6| Step: 5
Training loss: 2.3982598781585693
Validation loss: 2.2281879660903767

Epoch: 6| Step: 6
Training loss: 2.344911813735962
Validation loss: 2.224587543036348

Epoch: 6| Step: 7
Training loss: 2.9131240844726562
Validation loss: 2.2347445257248415

Epoch: 6| Step: 8
Training loss: 2.5538108348846436
Validation loss: 2.2264405655604538

Epoch: 6| Step: 9
Training loss: 2.4985873699188232
Validation loss: 2.2361316116907264

Epoch: 6| Step: 10
Training loss: 2.2065930366516113
Validation loss: 2.240069978980608

Epoch: 6| Step: 11
Training loss: 3.1221237182617188
Validation loss: 2.237197083811606

Epoch: 6| Step: 12
Training loss: 2.7345714569091797
Validation loss: 2.2420458293730214

Epoch: 6| Step: 13
Training loss: 2.292431116104126
Validation loss: 2.245969790284352

Epoch: 181| Step: 0
Training loss: 2.5202763080596924
Validation loss: 2.245801633404147

Epoch: 6| Step: 1
Training loss: 2.2424912452697754
Validation loss: 2.2591300536227483

Epoch: 6| Step: 2
Training loss: 2.2983570098876953
Validation loss: 2.266212632579188

Epoch: 6| Step: 3
Training loss: 3.4919991493225098
Validation loss: 2.2656707327852965

Epoch: 6| Step: 4
Training loss: 2.5570549964904785
Validation loss: 2.271530330822032

Epoch: 6| Step: 5
Training loss: 1.6922099590301514
Validation loss: 2.2708125652805453

Epoch: 6| Step: 6
Training loss: 2.77251935005188
Validation loss: 2.266515662593226

Epoch: 6| Step: 7
Training loss: 2.4901175498962402
Validation loss: 2.2640576131882204

Epoch: 6| Step: 8
Training loss: 2.1608352661132812
Validation loss: 2.2530827573550645

Epoch: 6| Step: 9
Training loss: 2.406277894973755
Validation loss: 2.2535505576800277

Epoch: 6| Step: 10
Training loss: 2.1928348541259766
Validation loss: 2.238467393382903

Epoch: 6| Step: 11
Training loss: 2.6995506286621094
Validation loss: 2.231324444534958

Epoch: 6| Step: 12
Training loss: 2.2973217964172363
Validation loss: 2.2317501550079673

Epoch: 6| Step: 13
Training loss: 2.404103994369507
Validation loss: 2.2320096544040147

Epoch: 182| Step: 0
Training loss: 2.723599433898926
Validation loss: 2.2178530411053727

Epoch: 6| Step: 1
Training loss: 2.3256306648254395
Validation loss: 2.223398962328511

Epoch: 6| Step: 2
Training loss: 2.3066606521606445
Validation loss: 2.2261951456787767

Epoch: 6| Step: 3
Training loss: 2.605245351791382
Validation loss: 2.227273295002599

Epoch: 6| Step: 4
Training loss: 1.726690649986267
Validation loss: 2.221561324211859

Epoch: 6| Step: 5
Training loss: 2.5471301078796387
Validation loss: 2.216474477962781

Epoch: 6| Step: 6
Training loss: 3.070216178894043
Validation loss: 2.2154052795902377

Epoch: 6| Step: 7
Training loss: 2.46759033203125
Validation loss: 2.215550420104816

Epoch: 6| Step: 8
Training loss: 2.1037096977233887
Validation loss: 2.214365565648643

Epoch: 6| Step: 9
Training loss: 2.163160562515259
Validation loss: 2.2168382137052474

Epoch: 6| Step: 10
Training loss: 2.296177864074707
Validation loss: 2.2152156317105858

Epoch: 6| Step: 11
Training loss: 2.731207847595215
Validation loss: 2.2108192854030158

Epoch: 6| Step: 12
Training loss: 2.6662368774414062
Validation loss: 2.2128342877152147

Epoch: 6| Step: 13
Training loss: 2.603506326675415
Validation loss: 2.2122389347322526

Epoch: 183| Step: 0
Training loss: 2.1574037075042725
Validation loss: 2.209679852249802

Epoch: 6| Step: 1
Training loss: 2.616358757019043
Validation loss: 2.210073906888244

Epoch: 6| Step: 2
Training loss: 2.590766191482544
Validation loss: 2.2155408692616287

Epoch: 6| Step: 3
Training loss: 1.8385568857192993
Validation loss: 2.212156864904588

Epoch: 6| Step: 4
Training loss: 2.375947952270508
Validation loss: 2.216746364870379

Epoch: 6| Step: 5
Training loss: 3.380845546722412
Validation loss: 2.2089511604719263

Epoch: 6| Step: 6
Training loss: 2.634225606918335
Validation loss: 2.212479547787738

Epoch: 6| Step: 7
Training loss: 2.8878283500671387
Validation loss: 2.2164545674477854

Epoch: 6| Step: 8
Training loss: 2.127216100692749
Validation loss: 2.2237583180909515

Epoch: 6| Step: 9
Training loss: 1.7898287773132324
Validation loss: 2.2270962627985145

Epoch: 6| Step: 10
Training loss: 3.085848808288574
Validation loss: 2.2290856094770533

Epoch: 6| Step: 11
Training loss: 2.4274566173553467
Validation loss: 2.234921379755902

Epoch: 6| Step: 12
Training loss: 1.6304214000701904
Validation loss: 2.2291760906096427

Epoch: 6| Step: 13
Training loss: 2.831071376800537
Validation loss: 2.2248110437905915

Epoch: 184| Step: 0
Training loss: 2.160327196121216
Validation loss: 2.2160322281622116

Epoch: 6| Step: 1
Training loss: 2.0529870986938477
Validation loss: 2.2114554297539497

Epoch: 6| Step: 2
Training loss: 2.6711957454681396
Validation loss: 2.2102486087429907

Epoch: 6| Step: 3
Training loss: 2.803896427154541
Validation loss: 2.2062476501669934

Epoch: 6| Step: 4
Training loss: 1.7893877029418945
Validation loss: 2.2083026157912387

Epoch: 6| Step: 5
Training loss: 2.3615283966064453
Validation loss: 2.2107575990820445

Epoch: 6| Step: 6
Training loss: 2.2192881107330322
Validation loss: 2.2116415654459307

Epoch: 6| Step: 7
Training loss: 3.198312997817993
Validation loss: 2.211776956435173

Epoch: 6| Step: 8
Training loss: 2.4642720222473145
Validation loss: 2.222453994135703

Epoch: 6| Step: 9
Training loss: 2.3817391395568848
Validation loss: 2.2172106773622575

Epoch: 6| Step: 10
Training loss: 2.389244318008423
Validation loss: 2.2333231638836604

Epoch: 6| Step: 11
Training loss: 2.4504687786102295
Validation loss: 2.2286470192734913

Epoch: 6| Step: 12
Training loss: 2.8421599864959717
Validation loss: 2.2234431864112936

Epoch: 6| Step: 13
Training loss: 2.3054420948028564
Validation loss: 2.2274443616149244

Epoch: 185| Step: 0
Training loss: 2.7056336402893066
Validation loss: 2.2249384336574103

Epoch: 6| Step: 1
Training loss: 2.274810791015625
Validation loss: 2.2145903495050248

Epoch: 6| Step: 2
Training loss: 2.266630172729492
Validation loss: 2.227615171863187

Epoch: 6| Step: 3
Training loss: 2.407310962677002
Validation loss: 2.216676886363696

Epoch: 6| Step: 4
Training loss: 3.006348133087158
Validation loss: 2.2227498177559144

Epoch: 6| Step: 5
Training loss: 2.8245630264282227
Validation loss: 2.2263015624015563

Epoch: 6| Step: 6
Training loss: 2.101774215698242
Validation loss: 2.227516879317581

Epoch: 6| Step: 7
Training loss: 3.122974395751953
Validation loss: 2.2347450640893753

Epoch: 6| Step: 8
Training loss: 1.998738169670105
Validation loss: 2.2379151903172976

Epoch: 6| Step: 9
Training loss: 2.213409423828125
Validation loss: 2.2441758930042224

Epoch: 6| Step: 10
Training loss: 2.4975099563598633
Validation loss: 2.2537720036763016

Epoch: 6| Step: 11
Training loss: 2.562868595123291
Validation loss: 2.2495593460657264

Epoch: 6| Step: 12
Training loss: 1.984493374824524
Validation loss: 2.2483106710577525

Epoch: 6| Step: 13
Training loss: 2.1469662189483643
Validation loss: 2.2505734377009894

Epoch: 186| Step: 0
Training loss: 2.7439889907836914
Validation loss: 2.248117730181704

Epoch: 6| Step: 1
Training loss: 3.308590888977051
Validation loss: 2.236820295292844

Epoch: 6| Step: 2
Training loss: 2.471292495727539
Validation loss: 2.2339456260845227

Epoch: 6| Step: 3
Training loss: 1.79086172580719
Validation loss: 2.2228331450493104

Epoch: 6| Step: 4
Training loss: 2.2774689197540283
Validation loss: 2.2194816502191688

Epoch: 6| Step: 5
Training loss: 3.0165700912475586
Validation loss: 2.2209535055263068

Epoch: 6| Step: 6
Training loss: 2.5998454093933105
Validation loss: 2.2271218197320097

Epoch: 6| Step: 7
Training loss: 1.663261890411377
Validation loss: 2.241449092024116

Epoch: 6| Step: 8
Training loss: 1.9622461795806885
Validation loss: 2.2464238392409457

Epoch: 6| Step: 9
Training loss: 1.9184136390686035
Validation loss: 2.2487209253413702

Epoch: 6| Step: 10
Training loss: 2.6108078956604004
Validation loss: 2.241671695504137

Epoch: 6| Step: 11
Training loss: 3.136733055114746
Validation loss: 2.225064928813647

Epoch: 6| Step: 12
Training loss: 2.408480644226074
Validation loss: 2.21386153979968

Epoch: 6| Step: 13
Training loss: 2.1040077209472656
Validation loss: 2.2110410236543223

Epoch: 187| Step: 0
Training loss: 2.148128032684326
Validation loss: 2.2154425472341557

Epoch: 6| Step: 1
Training loss: 2.2304527759552
Validation loss: 2.2094362704984603

Epoch: 6| Step: 2
Training loss: 2.590420722961426
Validation loss: 2.200901872368269

Epoch: 6| Step: 3
Training loss: 2.6836061477661133
Validation loss: 2.2040334645137993

Epoch: 6| Step: 4
Training loss: 2.9941370487213135
Validation loss: 2.2001105636678715

Epoch: 6| Step: 5
Training loss: 2.1703720092773438
Validation loss: 2.1989796238560833

Epoch: 6| Step: 6
Training loss: 2.507504940032959
Validation loss: 2.2003239406052457

Epoch: 6| Step: 7
Training loss: 3.0367565155029297
Validation loss: 2.196763861563898

Epoch: 6| Step: 8
Training loss: 2.4329872131347656
Validation loss: 2.1977696098307127

Epoch: 6| Step: 9
Training loss: 2.0038843154907227
Validation loss: 2.1989368341302358

Epoch: 6| Step: 10
Training loss: 2.3283944129943848
Validation loss: 2.1975948708031767

Epoch: 6| Step: 11
Training loss: 2.721428871154785
Validation loss: 2.207596568651097

Epoch: 6| Step: 12
Training loss: 2.2363991737365723
Validation loss: 2.2018337852211407

Epoch: 6| Step: 13
Training loss: 2.0511486530303955
Validation loss: 2.2052186496796145

Epoch: 188| Step: 0
Training loss: 2.057864189147949
Validation loss: 2.211576118264147

Epoch: 6| Step: 1
Training loss: 3.425245523452759
Validation loss: 2.216913810340307

Epoch: 6| Step: 2
Training loss: 2.47129487991333
Validation loss: 2.2178336369094027

Epoch: 6| Step: 3
Training loss: 2.89357328414917
Validation loss: 2.21663317372722

Epoch: 6| Step: 4
Training loss: 2.892820119857788
Validation loss: 2.223622411809942

Epoch: 6| Step: 5
Training loss: 2.064751148223877
Validation loss: 2.227041898235198

Epoch: 6| Step: 6
Training loss: 2.084500312805176
Validation loss: 2.228433080898818

Epoch: 6| Step: 7
Training loss: 2.366536855697632
Validation loss: 2.2249583633997108

Epoch: 6| Step: 8
Training loss: 2.6403651237487793
Validation loss: 2.2324710815183577

Epoch: 6| Step: 9
Training loss: 2.46549129486084
Validation loss: 2.237256973020492

Epoch: 6| Step: 10
Training loss: 2.2693707942962646
Validation loss: 2.234257956986786

Epoch: 6| Step: 11
Training loss: 2.300234317779541
Validation loss: 2.2303142970608127

Epoch: 6| Step: 12
Training loss: 2.128783702850342
Validation loss: 2.2270206097633607

Epoch: 6| Step: 13
Training loss: 1.6327688694000244
Validation loss: 2.217709625920942

Epoch: 189| Step: 0
Training loss: 2.150022029876709
Validation loss: 2.214729683373564

Epoch: 6| Step: 1
Training loss: 2.0142929553985596
Validation loss: 2.2166073322296143

Epoch: 6| Step: 2
Training loss: 2.3377468585968018
Validation loss: 2.226757521270424

Epoch: 6| Step: 3
Training loss: 2.2836177349090576
Validation loss: 2.223815264240388

Epoch: 6| Step: 4
Training loss: 2.2063632011413574
Validation loss: 2.229265343758368

Epoch: 6| Step: 5
Training loss: 2.4654369354248047
Validation loss: 2.2285821924927416

Epoch: 6| Step: 6
Training loss: 2.4153172969818115
Validation loss: 2.2280559437249297

Epoch: 6| Step: 7
Training loss: 2.477729320526123
Validation loss: 2.230716141321326

Epoch: 6| Step: 8
Training loss: 3.073551654815674
Validation loss: 2.2300905284061225

Epoch: 6| Step: 9
Training loss: 2.594524383544922
Validation loss: 2.2402658257433163

Epoch: 6| Step: 10
Training loss: 3.012134313583374
Validation loss: 2.218432605907481

Epoch: 6| Step: 11
Training loss: 2.5488903522491455
Validation loss: 2.2211393233268493

Epoch: 6| Step: 12
Training loss: 2.306630849838257
Validation loss: 2.222204815956854

Epoch: 6| Step: 13
Training loss: 1.8132139444351196
Validation loss: 2.2136163801275273

Epoch: 190| Step: 0
Training loss: 2.1950061321258545
Validation loss: 2.2101738222183718

Epoch: 6| Step: 1
Training loss: 2.682441234588623
Validation loss: 2.208217474722093

Epoch: 6| Step: 2
Training loss: 2.50193452835083
Validation loss: 2.204582909102081

Epoch: 6| Step: 3
Training loss: 2.541530132293701
Validation loss: 2.208766752673734

Epoch: 6| Step: 4
Training loss: 2.526306629180908
Validation loss: 2.214392562066355

Epoch: 6| Step: 5
Training loss: 2.662440299987793
Validation loss: 2.208793123563131

Epoch: 6| Step: 6
Training loss: 2.31864070892334
Validation loss: 2.2184839479384886

Epoch: 6| Step: 7
Training loss: 2.485665798187256
Validation loss: 2.228091960312218

Epoch: 6| Step: 8
Training loss: 2.662001609802246
Validation loss: 2.225119900959794

Epoch: 6| Step: 9
Training loss: 2.103806734085083
Validation loss: 2.215562956307524

Epoch: 6| Step: 10
Training loss: 2.264730453491211
Validation loss: 2.2232087837752474

Epoch: 6| Step: 11
Training loss: 2.2674787044525146
Validation loss: 2.221277044665429

Epoch: 6| Step: 12
Training loss: 2.6148996353149414
Validation loss: 2.227398359647361

Epoch: 6| Step: 13
Training loss: 1.985781192779541
Validation loss: 2.2324163093361804

Epoch: 191| Step: 0
Training loss: 2.4351541996002197
Validation loss: 2.2308132802286456

Epoch: 6| Step: 1
Training loss: 2.2187585830688477
Validation loss: 2.229710889118974

Epoch: 6| Step: 2
Training loss: 3.133810520172119
Validation loss: 2.235350344770698

Epoch: 6| Step: 3
Training loss: 2.3166604042053223
Validation loss: 2.241460164388021

Epoch: 6| Step: 4
Training loss: 2.5907437801361084
Validation loss: 2.2407509332062094

Epoch: 6| Step: 5
Training loss: 1.9118759632110596
Validation loss: 2.243527804651568

Epoch: 6| Step: 6
Training loss: 3.3053038120269775
Validation loss: 2.243414237935056

Epoch: 6| Step: 7
Training loss: 1.8249964714050293
Validation loss: 2.2347457306359404

Epoch: 6| Step: 8
Training loss: 2.3765745162963867
Validation loss: 2.2236708646179526

Epoch: 6| Step: 9
Training loss: 2.8103315830230713
Validation loss: 2.2272195687858005

Epoch: 6| Step: 10
Training loss: 2.181757926940918
Validation loss: 2.239886368474653

Epoch: 6| Step: 11
Training loss: 2.4406626224517822
Validation loss: 2.23580398098115

Epoch: 6| Step: 12
Training loss: 2.4509830474853516
Validation loss: 2.250729207069643

Epoch: 6| Step: 13
Training loss: 1.7957704067230225
Validation loss: 2.241673061924596

Epoch: 192| Step: 0
Training loss: 2.092324733734131
Validation loss: 2.2313551261860836

Epoch: 6| Step: 1
Training loss: 2.1712350845336914
Validation loss: 2.2288770188567457

Epoch: 6| Step: 2
Training loss: 2.303819179534912
Validation loss: 2.234491520030524

Epoch: 6| Step: 3
Training loss: 2.4169845581054688
Validation loss: 2.236262562454388

Epoch: 6| Step: 4
Training loss: 2.0920019149780273
Validation loss: 2.226637053233321

Epoch: 6| Step: 5
Training loss: 3.1127212047576904
Validation loss: 2.219857654263896

Epoch: 6| Step: 6
Training loss: 2.9495720863342285
Validation loss: 2.2175292212476014

Epoch: 6| Step: 7
Training loss: 2.221001386642456
Validation loss: 2.2070151836641374

Epoch: 6| Step: 8
Training loss: 2.7636327743530273
Validation loss: 2.1952253195547287

Epoch: 6| Step: 9
Training loss: 2.2950704097747803
Validation loss: 2.2112749699623353

Epoch: 6| Step: 10
Training loss: 1.9496694803237915
Validation loss: 2.202384243729294

Epoch: 6| Step: 11
Training loss: 2.1570467948913574
Validation loss: 2.208262735797513

Epoch: 6| Step: 12
Training loss: 2.6298375129699707
Validation loss: 2.1979433939021122

Epoch: 6| Step: 13
Training loss: 3.3433825969696045
Validation loss: 2.198980736476119

Epoch: 193| Step: 0
Training loss: 2.5139851570129395
Validation loss: 2.1961692853640487

Epoch: 6| Step: 1
Training loss: 2.741779327392578
Validation loss: 2.199053910470778

Epoch: 6| Step: 2
Training loss: 1.669177770614624
Validation loss: 2.1987674479843466

Epoch: 6| Step: 3
Training loss: 2.4173836708068848
Validation loss: 2.206105250184254

Epoch: 6| Step: 4
Training loss: 2.9547393321990967
Validation loss: 2.2025210216481197

Epoch: 6| Step: 5
Training loss: 2.564305543899536
Validation loss: 2.2031787774896108

Epoch: 6| Step: 6
Training loss: 2.22337007522583
Validation loss: 2.1987369650153705

Epoch: 6| Step: 7
Training loss: 3.01480770111084
Validation loss: 2.193126313148006

Epoch: 6| Step: 8
Training loss: 2.5969252586364746
Validation loss: 2.193850273727089

Epoch: 6| Step: 9
Training loss: 2.2623660564422607
Validation loss: 2.19930092237329

Epoch: 6| Step: 10
Training loss: 2.318861722946167
Validation loss: 2.1843530926653134

Epoch: 6| Step: 11
Training loss: 2.54447603225708
Validation loss: 2.1899198639777397

Epoch: 6| Step: 12
Training loss: 1.504624366760254
Validation loss: 2.1896389428005425

Epoch: 6| Step: 13
Training loss: 3.003157615661621
Validation loss: 2.180494227717

Epoch: 194| Step: 0
Training loss: 1.827972650527954
Validation loss: 2.1945827955840738

Epoch: 6| Step: 1
Training loss: 2.058683395385742
Validation loss: 2.195301176399313

Epoch: 6| Step: 2
Training loss: 2.0762932300567627
Validation loss: 2.195449959847235

Epoch: 6| Step: 3
Training loss: 2.3385143280029297
Validation loss: 2.2008686373310704

Epoch: 6| Step: 4
Training loss: 2.8125667572021484
Validation loss: 2.2133847462233676

Epoch: 6| Step: 5
Training loss: 2.779417037963867
Validation loss: 2.214051887553225

Epoch: 6| Step: 6
Training loss: 2.989823818206787
Validation loss: 2.22206776372848

Epoch: 6| Step: 7
Training loss: 2.3272652626037598
Validation loss: 2.2335470953295307

Epoch: 6| Step: 8
Training loss: 2.2049295902252197
Validation loss: 2.2448539656977498

Epoch: 6| Step: 9
Training loss: 2.2160890102386475
Validation loss: 2.2462352180993683

Epoch: 6| Step: 10
Training loss: 2.586738109588623
Validation loss: 2.238971576895765

Epoch: 6| Step: 11
Training loss: 2.656526803970337
Validation loss: 2.233394930439611

Epoch: 6| Step: 12
Training loss: 2.5916666984558105
Validation loss: 2.237568568157893

Epoch: 6| Step: 13
Training loss: 2.5799803733825684
Validation loss: 2.230956364703435

Epoch: 195| Step: 0
Training loss: 2.306507110595703
Validation loss: 2.2326001364697694

Epoch: 6| Step: 1
Training loss: 2.582719564437866
Validation loss: 2.2304693114372993

Epoch: 6| Step: 2
Training loss: 2.5492708683013916
Validation loss: 2.224763616438835

Epoch: 6| Step: 3
Training loss: 2.6828112602233887
Validation loss: 2.2298589944839478

Epoch: 6| Step: 4
Training loss: 2.4401042461395264
Validation loss: 2.221562977760069

Epoch: 6| Step: 5
Training loss: 2.1665003299713135
Validation loss: 2.2154466259864067

Epoch: 6| Step: 6
Training loss: 2.8278589248657227
Validation loss: 2.1969363022876043

Epoch: 6| Step: 7
Training loss: 2.1593868732452393
Validation loss: 2.189557839465398

Epoch: 6| Step: 8
Training loss: 1.8622019290924072
Validation loss: 2.182614882787069

Epoch: 6| Step: 9
Training loss: 2.7570724487304688
Validation loss: 2.1736401409231205

Epoch: 6| Step: 10
Training loss: 2.5083179473876953
Validation loss: 2.1786940764355403

Epoch: 6| Step: 11
Training loss: 2.631779193878174
Validation loss: 2.1746816788950274

Epoch: 6| Step: 12
Training loss: 2.6661713123321533
Validation loss: 2.1731231597162064

Epoch: 6| Step: 13
Training loss: 1.7830140590667725
Validation loss: 2.1822147292475544

Epoch: 196| Step: 0
Training loss: 2.7090625762939453
Validation loss: 2.1787765179910967

Epoch: 6| Step: 1
Training loss: 2.7528581619262695
Validation loss: 2.1871366705945743

Epoch: 6| Step: 2
Training loss: 2.5005850791931152
Validation loss: 2.1989006765427126

Epoch: 6| Step: 3
Training loss: 2.0637288093566895
Validation loss: 2.194359815248879

Epoch: 6| Step: 4
Training loss: 1.7474358081817627
Validation loss: 2.22423319919135

Epoch: 6| Step: 5
Training loss: 1.9975056648254395
Validation loss: 2.2244670237264326

Epoch: 6| Step: 6
Training loss: 2.674220085144043
Validation loss: 2.2301103838028444

Epoch: 6| Step: 7
Training loss: 2.626400947570801
Validation loss: 2.229442152925717

Epoch: 6| Step: 8
Training loss: 1.818363904953003
Validation loss: 2.2358084929886686

Epoch: 6| Step: 9
Training loss: 3.636836528778076
Validation loss: 2.2347607048608924

Epoch: 6| Step: 10
Training loss: 2.0445899963378906
Validation loss: 2.2443854167897213

Epoch: 6| Step: 11
Training loss: 2.775355100631714
Validation loss: 2.243015138051843

Epoch: 6| Step: 12
Training loss: 1.8192073106765747
Validation loss: 2.2377325693766275

Epoch: 6| Step: 13
Training loss: 3.009610414505005
Validation loss: 2.2341230992347962

Epoch: 197| Step: 0
Training loss: 2.010788679122925
Validation loss: 2.229398956862829

Epoch: 6| Step: 1
Training loss: 2.5917649269104004
Validation loss: 2.2350504423982356

Epoch: 6| Step: 2
Training loss: 2.322201728820801
Validation loss: 2.2359334550878054

Epoch: 6| Step: 3
Training loss: 2.3625328540802
Validation loss: 2.2387166177072833

Epoch: 6| Step: 4
Training loss: 2.970705270767212
Validation loss: 2.229634415718817

Epoch: 6| Step: 5
Training loss: 2.371873617172241
Validation loss: 2.2216243308077575

Epoch: 6| Step: 6
Training loss: 2.349033832550049
Validation loss: 2.2130657575463735

Epoch: 6| Step: 7
Training loss: 2.3586678504943848
Validation loss: 2.201678217098277

Epoch: 6| Step: 8
Training loss: 2.6463260650634766
Validation loss: 2.198527853976014

Epoch: 6| Step: 9
Training loss: 1.9963024854660034
Validation loss: 2.202610795215894

Epoch: 6| Step: 10
Training loss: 2.124511241912842
Validation loss: 2.212975573796098

Epoch: 6| Step: 11
Training loss: 2.599402904510498
Validation loss: 2.224640788570527

Epoch: 6| Step: 12
Training loss: 2.8590996265411377
Validation loss: 2.2314124286815686

Epoch: 6| Step: 13
Training loss: 2.3121531009674072
Validation loss: 2.2433919445160897

Epoch: 198| Step: 0
Training loss: 1.7095972299575806
Validation loss: 2.236574729283651

Epoch: 6| Step: 1
Training loss: 2.728163480758667
Validation loss: 2.2403488518089376

Epoch: 6| Step: 2
Training loss: 1.9354426860809326
Validation loss: 2.2361437633473384

Epoch: 6| Step: 3
Training loss: 2.095285177230835
Validation loss: 2.22435801388115

Epoch: 6| Step: 4
Training loss: 2.6311357021331787
Validation loss: 2.2250233542534614

Epoch: 6| Step: 5
Training loss: 2.351113796234131
Validation loss: 2.2108551840628348

Epoch: 6| Step: 6
Training loss: 2.2480320930480957
Validation loss: 2.219102313441615

Epoch: 6| Step: 7
Training loss: 2.4366331100463867
Validation loss: 2.2185055799381708

Epoch: 6| Step: 8
Training loss: 2.5923938751220703
Validation loss: 2.21459138008856

Epoch: 6| Step: 9
Training loss: 1.780625581741333
Validation loss: 2.2246699487009356

Epoch: 6| Step: 10
Training loss: 3.1450352668762207
Validation loss: 2.2220131120374127

Epoch: 6| Step: 11
Training loss: 2.7383041381835938
Validation loss: 2.2259513831907705

Epoch: 6| Step: 12
Training loss: 3.196897268295288
Validation loss: 2.219963612095002

Epoch: 6| Step: 13
Training loss: 2.1154861450195312
Validation loss: 2.2125507311154435

Epoch: 199| Step: 0
Training loss: 3.0490779876708984
Validation loss: 2.219059508333924

Epoch: 6| Step: 1
Training loss: 2.719613552093506
Validation loss: 2.2134282076230614

Epoch: 6| Step: 2
Training loss: 2.3542304039001465
Validation loss: 2.207397796774423

Epoch: 6| Step: 3
Training loss: 2.9455373287200928
Validation loss: 2.218413490121083

Epoch: 6| Step: 4
Training loss: 2.571157932281494
Validation loss: 2.2076199849446616

Epoch: 6| Step: 5
Training loss: 2.1366918087005615
Validation loss: 2.225210071891867

Epoch: 6| Step: 6
Training loss: 2.4406003952026367
Validation loss: 2.2281469222038024

Epoch: 6| Step: 7
Training loss: 2.2860820293426514
Validation loss: 2.2184042289692867

Epoch: 6| Step: 8
Training loss: 1.6630982160568237
Validation loss: 2.2248893117391937

Epoch: 6| Step: 9
Training loss: 3.1372623443603516
Validation loss: 2.2258577064801286

Epoch: 6| Step: 10
Training loss: 2.551656723022461
Validation loss: 2.21561538275852

Epoch: 6| Step: 11
Training loss: 1.5244817733764648
Validation loss: 2.2263071152471725

Epoch: 6| Step: 12
Training loss: 2.081381320953369
Validation loss: 2.2249378876019548

Epoch: 6| Step: 13
Training loss: 1.9486238956451416
Validation loss: 2.218110704934725

Epoch: 200| Step: 0
Training loss: 3.0677261352539062
Validation loss: 2.2213561509245183

Epoch: 6| Step: 1
Training loss: 2.7227821350097656
Validation loss: 2.213442749874566

Epoch: 6| Step: 2
Training loss: 2.8419361114501953
Validation loss: 2.20554437944966

Epoch: 6| Step: 3
Training loss: 1.6810879707336426
Validation loss: 2.210499812197942

Epoch: 6| Step: 4
Training loss: 2.431657552719116
Validation loss: 2.2145576207868514

Epoch: 6| Step: 5
Training loss: 2.888524055480957
Validation loss: 2.2159675731453845

Epoch: 6| Step: 6
Training loss: 1.7617559432983398
Validation loss: 2.2115028378784016

Epoch: 6| Step: 7
Training loss: 1.9512457847595215
Validation loss: 2.2054227885379585

Epoch: 6| Step: 8
Training loss: 2.8107500076293945
Validation loss: 2.207965671375234

Epoch: 6| Step: 9
Training loss: 1.6938462257385254
Validation loss: 2.2019204760110505

Epoch: 6| Step: 10
Training loss: 2.154916286468506
Validation loss: 2.200799362633818

Epoch: 6| Step: 11
Training loss: 2.608813762664795
Validation loss: 2.211870962573636

Epoch: 6| Step: 12
Training loss: 2.5533409118652344
Validation loss: 2.218565317892259

Epoch: 6| Step: 13
Training loss: 2.2848503589630127
Validation loss: 2.2250875170512865

Epoch: 201| Step: 0
Training loss: 1.88651704788208
Validation loss: 2.2181971098787043

Epoch: 6| Step: 1
Training loss: 2.539517402648926
Validation loss: 2.2027845754418323

Epoch: 6| Step: 2
Training loss: 2.753431797027588
Validation loss: 2.203305802037639

Epoch: 6| Step: 3
Training loss: 2.5345535278320312
Validation loss: 2.2048957629870345

Epoch: 6| Step: 4
Training loss: 2.715265989303589
Validation loss: 2.196137282156175

Epoch: 6| Step: 5
Training loss: 1.8921271562576294
Validation loss: 2.1936663837843042

Epoch: 6| Step: 6
Training loss: 2.0014328956604004
Validation loss: 2.1969651099174254

Epoch: 6| Step: 7
Training loss: 2.2235264778137207
Validation loss: 2.2048241681950067

Epoch: 6| Step: 8
Training loss: 3.1758246421813965
Validation loss: 2.216479693689654

Epoch: 6| Step: 9
Training loss: 2.9811413288116455
Validation loss: 2.2079555731947704

Epoch: 6| Step: 10
Training loss: 2.9024248123168945
Validation loss: 2.2333638629605694

Epoch: 6| Step: 11
Training loss: 2.2219860553741455
Validation loss: 2.215001788190616

Epoch: 6| Step: 12
Training loss: 1.6161812543869019
Validation loss: 2.2073644489370365

Epoch: 6| Step: 13
Training loss: 1.980513095855713
Validation loss: 2.2013540447399182

Epoch: 202| Step: 0
Training loss: 2.8618879318237305
Validation loss: 2.1920890462013984

Epoch: 6| Step: 1
Training loss: 3.031851291656494
Validation loss: 2.2014976675792406

Epoch: 6| Step: 2
Training loss: 2.5809834003448486
Validation loss: 2.213647319424537

Epoch: 6| Step: 3
Training loss: 2.7515907287597656
Validation loss: 2.205508134698355

Epoch: 6| Step: 4
Training loss: 1.3402328491210938
Validation loss: 2.208995649891515

Epoch: 6| Step: 5
Training loss: 3.003159999847412
Validation loss: 2.1986123105531097

Epoch: 6| Step: 6
Training loss: 2.491260528564453
Validation loss: 2.2024196963156424

Epoch: 6| Step: 7
Training loss: 2.793008327484131
Validation loss: 2.1971605157339447

Epoch: 6| Step: 8
Training loss: 2.073915481567383
Validation loss: 2.1988596775198497

Epoch: 6| Step: 9
Training loss: 2.5816426277160645
Validation loss: 2.198900297123899

Epoch: 6| Step: 10
Training loss: 1.7831666469573975
Validation loss: 2.2002422425054733

Epoch: 6| Step: 11
Training loss: 1.810973882675171
Validation loss: 2.201134343301096

Epoch: 6| Step: 12
Training loss: 2.035299062728882
Validation loss: 2.2006627731425787

Epoch: 6| Step: 13
Training loss: 2.754581928253174
Validation loss: 2.1998083232551493

Epoch: 203| Step: 0
Training loss: 2.658198356628418
Validation loss: 2.2189252940557336

Epoch: 6| Step: 1
Training loss: 2.5957977771759033
Validation loss: 2.216062694467524

Epoch: 6| Step: 2
Training loss: 2.498708486557007
Validation loss: 2.223687102717738

Epoch: 6| Step: 3
Training loss: 2.1926276683807373
Validation loss: 2.238980236873832

Epoch: 6| Step: 4
Training loss: 3.4094738960266113
Validation loss: 2.2450717341515327

Epoch: 6| Step: 5
Training loss: 2.0263545513153076
Validation loss: 2.2282811108455864

Epoch: 6| Step: 6
Training loss: 2.253243923187256
Validation loss: 2.229101501485353

Epoch: 6| Step: 7
Training loss: 2.8811662197113037
Validation loss: 2.2273238807596187

Epoch: 6| Step: 8
Training loss: 2.381833791732788
Validation loss: 2.2252650260925293

Epoch: 6| Step: 9
Training loss: 1.6023098230361938
Validation loss: 2.2131249494450067

Epoch: 6| Step: 10
Training loss: 2.5351226329803467
Validation loss: 2.2084044871791715

Epoch: 6| Step: 11
Training loss: 1.62801992893219
Validation loss: 2.207474970048474

Epoch: 6| Step: 12
Training loss: 2.8417482376098633
Validation loss: 2.2063291470209756

Epoch: 6| Step: 13
Training loss: 1.7552307844161987
Validation loss: 2.199312480547095

Epoch: 204| Step: 0
Training loss: 2.4965882301330566
Validation loss: 2.1915843794422765

Epoch: 6| Step: 1
Training loss: 1.7963448762893677
Validation loss: 2.1946675328798193

Epoch: 6| Step: 2
Training loss: 2.4809632301330566
Validation loss: 2.1874775117443455

Epoch: 6| Step: 3
Training loss: 2.2567286491394043
Validation loss: 2.190803171485983

Epoch: 6| Step: 4
Training loss: 2.1960325241088867
Validation loss: 2.1826059792631414

Epoch: 6| Step: 5
Training loss: 1.8883308172225952
Validation loss: 2.186911570128574

Epoch: 6| Step: 6
Training loss: 2.4700632095336914
Validation loss: 2.1907629069461616

Epoch: 6| Step: 7
Training loss: 2.351809024810791
Validation loss: 2.1923409726030085

Epoch: 6| Step: 8
Training loss: 3.5437028408050537
Validation loss: 2.1993315348061184

Epoch: 6| Step: 9
Training loss: 2.9357361793518066
Validation loss: 2.187662337415962

Epoch: 6| Step: 10
Training loss: 1.5338151454925537
Validation loss: 2.191562050132341

Epoch: 6| Step: 11
Training loss: 2.345418930053711
Validation loss: 2.1867760714664253

Epoch: 6| Step: 12
Training loss: 2.660611867904663
Validation loss: 2.188522046612155

Epoch: 6| Step: 13
Training loss: 2.6660821437835693
Validation loss: 2.1939944092945387

Epoch: 205| Step: 0
Training loss: 2.938225269317627
Validation loss: 2.1983035751568374

Epoch: 6| Step: 1
Training loss: 2.0591542720794678
Validation loss: 2.1971242094552643

Epoch: 6| Step: 2
Training loss: 2.13938570022583
Validation loss: 2.2035721591723862

Epoch: 6| Step: 3
Training loss: 2.0634686946868896
Validation loss: 2.2182176574583976

Epoch: 6| Step: 4
Training loss: 1.6309585571289062
Validation loss: 2.2263724214287213

Epoch: 6| Step: 5
Training loss: 3.0085320472717285
Validation loss: 2.2221709477004183

Epoch: 6| Step: 6
Training loss: 2.899813175201416
Validation loss: 2.2234102397836666

Epoch: 6| Step: 7
Training loss: 2.0811071395874023
Validation loss: 2.233908062340111

Epoch: 6| Step: 8
Training loss: 1.9846798181533813
Validation loss: 2.2290452680280133

Epoch: 6| Step: 9
Training loss: 2.5472211837768555
Validation loss: 2.221829786095568

Epoch: 6| Step: 10
Training loss: 3.0348153114318848
Validation loss: 2.2022995859064083

Epoch: 6| Step: 11
Training loss: 2.092609405517578
Validation loss: 2.1973730774335962

Epoch: 6| Step: 12
Training loss: 2.5985374450683594
Validation loss: 2.1973759461474676

Epoch: 6| Step: 13
Training loss: 2.4772825241088867
Validation loss: 2.182507922572474

Epoch: 206| Step: 0
Training loss: 3.007502555847168
Validation loss: 2.186162681989772

Epoch: 6| Step: 1
Training loss: 1.786962866783142
Validation loss: 2.1984997000745548

Epoch: 6| Step: 2
Training loss: 2.601810932159424
Validation loss: 2.1881807363161476

Epoch: 6| Step: 3
Training loss: 2.4795432090759277
Validation loss: 2.1992620037448023

Epoch: 6| Step: 4
Training loss: 1.8876776695251465
Validation loss: 2.196581614914761

Epoch: 6| Step: 5
Training loss: 1.8608698844909668
Validation loss: 2.1966375714989117

Epoch: 6| Step: 6
Training loss: 2.8748350143432617
Validation loss: 2.196758711209861

Epoch: 6| Step: 7
Training loss: 2.2054734230041504
Validation loss: 2.1970160802205405

Epoch: 6| Step: 8
Training loss: 2.853977680206299
Validation loss: 2.198490540186564

Epoch: 6| Step: 9
Training loss: 1.6423966884613037
Validation loss: 2.195949726207282

Epoch: 6| Step: 10
Training loss: 1.9471890926361084
Validation loss: 2.1976637814634588

Epoch: 6| Step: 11
Training loss: 3.2522971630096436
Validation loss: 2.1980854157478578

Epoch: 6| Step: 12
Training loss: 2.2472939491271973
Validation loss: 2.2144378257054154

Epoch: 6| Step: 13
Training loss: 3.02116060256958
Validation loss: 2.21800325762841

Epoch: 207| Step: 0
Training loss: 1.9351674318313599
Validation loss: 2.228253577345161

Epoch: 6| Step: 1
Training loss: 2.386176586151123
Validation loss: 2.223897777577882

Epoch: 6| Step: 2
Training loss: 1.4836013317108154
Validation loss: 2.245941597928283

Epoch: 6| Step: 3
Training loss: 2.778850793838501
Validation loss: 2.2537270874105473

Epoch: 6| Step: 4
Training loss: 2.586049795150757
Validation loss: 2.240910424981066

Epoch: 6| Step: 5
Training loss: 2.858804225921631
Validation loss: 2.2485193206417944

Epoch: 6| Step: 6
Training loss: 2.920755386352539
Validation loss: 2.254076676983987

Epoch: 6| Step: 7
Training loss: 2.6106605529785156
Validation loss: 2.2554195568125737

Epoch: 6| Step: 8
Training loss: 2.625826358795166
Validation loss: 2.2294089307067213

Epoch: 6| Step: 9
Training loss: 2.324214220046997
Validation loss: 2.222928954708961

Epoch: 6| Step: 10
Training loss: 2.2240352630615234
Validation loss: 2.214021405866069

Epoch: 6| Step: 11
Training loss: 2.0926260948181152
Validation loss: 2.2060281909922117

Epoch: 6| Step: 12
Training loss: 2.4813055992126465
Validation loss: 2.2048896974132908

Epoch: 6| Step: 13
Training loss: 2.6666924953460693
Validation loss: 2.2029735798476846

Epoch: 208| Step: 0
Training loss: 3.0176963806152344
Validation loss: 2.207663215616698

Epoch: 6| Step: 1
Training loss: 1.2064570188522339
Validation loss: 2.192885673174294

Epoch: 6| Step: 2
Training loss: 2.0956149101257324
Validation loss: 2.1896053309081704

Epoch: 6| Step: 3
Training loss: 2.684843063354492
Validation loss: 2.1715034912991267

Epoch: 6| Step: 4
Training loss: 2.442091941833496
Validation loss: 2.173813289211642

Epoch: 6| Step: 5
Training loss: 2.5904288291931152
Validation loss: 2.172990042676208

Epoch: 6| Step: 6
Training loss: 2.4474873542785645
Validation loss: 2.183983566940472

Epoch: 6| Step: 7
Training loss: 3.023800849914551
Validation loss: 2.1848199572614444

Epoch: 6| Step: 8
Training loss: 2.5290675163269043
Validation loss: 2.1843455811982513

Epoch: 6| Step: 9
Training loss: 2.0791256427764893
Validation loss: 2.1907960702014226

Epoch: 6| Step: 10
Training loss: 1.8931409120559692
Validation loss: 2.1863550627103416

Epoch: 6| Step: 11
Training loss: 2.5209250450134277
Validation loss: 2.191932224458264

Epoch: 6| Step: 12
Training loss: 3.177537679672241
Validation loss: 2.19838454902813

Epoch: 6| Step: 13
Training loss: 1.358456015586853
Validation loss: 2.196162221252277

Epoch: 209| Step: 0
Training loss: 2.0622196197509766
Validation loss: 2.2057918271710797

Epoch: 6| Step: 1
Training loss: 2.6503536701202393
Validation loss: 2.2029661722080682

Epoch: 6| Step: 2
Training loss: 3.3199803829193115
Validation loss: 2.2085475024356636

Epoch: 6| Step: 3
Training loss: 2.6973977088928223
Validation loss: 2.203770714421426

Epoch: 6| Step: 4
Training loss: 3.178360939025879
Validation loss: 2.191518167013763

Epoch: 6| Step: 5
Training loss: 1.839505672454834
Validation loss: 2.1962408724651543

Epoch: 6| Step: 6
Training loss: 1.8098185062408447
Validation loss: 2.1874776373627367

Epoch: 6| Step: 7
Training loss: 2.5615124702453613
Validation loss: 2.1918106412374847

Epoch: 6| Step: 8
Training loss: 2.065234661102295
Validation loss: 2.1890873344995643

Epoch: 6| Step: 9
Training loss: 2.139467477798462
Validation loss: 2.1836660241567962

Epoch: 6| Step: 10
Training loss: 1.9107028245925903
Validation loss: 2.200510468534244

Epoch: 6| Step: 11
Training loss: 1.9530106782913208
Validation loss: 2.2006391735487085

Epoch: 6| Step: 12
Training loss: 2.7755398750305176
Validation loss: 2.2016804090110202

Epoch: 6| Step: 13
Training loss: 2.354461908340454
Validation loss: 2.2017987556354974

Epoch: 210| Step: 0
Training loss: 2.4405317306518555
Validation loss: 2.2027137958875267

Epoch: 6| Step: 1
Training loss: 2.955162286758423
Validation loss: 2.192158511889878

Epoch: 6| Step: 2
Training loss: 1.863551378250122
Validation loss: 2.205848034992013

Epoch: 6| Step: 3
Training loss: 1.9804844856262207
Validation loss: 2.2147753956497356

Epoch: 6| Step: 4
Training loss: 2.19057035446167
Validation loss: 2.2237454204149145

Epoch: 6| Step: 5
Training loss: 2.1270203590393066
Validation loss: 2.218675251929991

Epoch: 6| Step: 6
Training loss: 2.869572639465332
Validation loss: 2.2112428270360476

Epoch: 6| Step: 7
Training loss: 1.6127146482467651
Validation loss: 2.224846701468191

Epoch: 6| Step: 8
Training loss: 2.9386110305786133
Validation loss: 2.2151305137142057

Epoch: 6| Step: 9
Training loss: 2.496518611907959
Validation loss: 2.218142937588435

Epoch: 6| Step: 10
Training loss: 3.036900281906128
Validation loss: 2.212540949544599

Epoch: 6| Step: 11
Training loss: 1.7248618602752686
Validation loss: 2.2078839963482273

Epoch: 6| Step: 12
Training loss: 2.211966037750244
Validation loss: 2.205854049292944

Epoch: 6| Step: 13
Training loss: 3.193509101867676
Validation loss: 2.198731776206724

Epoch: 211| Step: 0
Training loss: 2.385862112045288
Validation loss: 2.1969270936904417

Epoch: 6| Step: 1
Training loss: 2.7018797397613525
Validation loss: 2.194633701796173

Epoch: 6| Step: 2
Training loss: 2.870461940765381
Validation loss: 2.1934400220071115

Epoch: 6| Step: 3
Training loss: 2.3501415252685547
Validation loss: 2.200925928290172

Epoch: 6| Step: 4
Training loss: 2.414492130279541
Validation loss: 2.2123662015443206

Epoch: 6| Step: 5
Training loss: 1.8474020957946777
Validation loss: 2.224705342323549

Epoch: 6| Step: 6
Training loss: 1.7792571783065796
Validation loss: 2.23097639442772

Epoch: 6| Step: 7
Training loss: 2.3397111892700195
Validation loss: 2.2284086840127104

Epoch: 6| Step: 8
Training loss: 2.5141806602478027
Validation loss: 2.2247037118481052

Epoch: 6| Step: 9
Training loss: 1.64794921875
Validation loss: 2.2242800779240106

Epoch: 6| Step: 10
Training loss: 2.463730573654175
Validation loss: 2.2081359406953216

Epoch: 6| Step: 11
Training loss: 3.1675562858581543
Validation loss: 2.2066827256192445

Epoch: 6| Step: 12
Training loss: 2.529773712158203
Validation loss: 2.213369195179273

Epoch: 6| Step: 13
Training loss: 2.423347234725952
Validation loss: 2.22567480738445

Epoch: 212| Step: 0
Training loss: 1.6387908458709717
Validation loss: 2.2178784621659147

Epoch: 6| Step: 1
Training loss: 2.240720272064209
Validation loss: 2.2084312438964844

Epoch: 6| Step: 2
Training loss: 2.8418445587158203
Validation loss: 2.2091719078761276

Epoch: 6| Step: 3
Training loss: 3.0957529544830322
Validation loss: 2.2078286755469536

Epoch: 6| Step: 4
Training loss: 2.2571396827697754
Validation loss: 2.2026724379549742

Epoch: 6| Step: 5
Training loss: 2.1531267166137695
Validation loss: 2.204192305123934

Epoch: 6| Step: 6
Training loss: 2.451964855194092
Validation loss: 2.201054378222394

Epoch: 6| Step: 7
Training loss: 1.9268202781677246
Validation loss: 2.193196330019223

Epoch: 6| Step: 8
Training loss: 2.8219492435455322
Validation loss: 2.183414146464358

Epoch: 6| Step: 9
Training loss: 2.910276174545288
Validation loss: 2.184760873035718

Epoch: 6| Step: 10
Training loss: 2.0940616130828857
Validation loss: 2.177805846737277

Epoch: 6| Step: 11
Training loss: 2.183284282684326
Validation loss: 2.187382303258424

Epoch: 6| Step: 12
Training loss: 2.410463333129883
Validation loss: 2.190204237097053

Epoch: 6| Step: 13
Training loss: 2.432864189147949
Validation loss: 2.1942377500636603

Epoch: 213| Step: 0
Training loss: 2.6027050018310547
Validation loss: 2.20666717713879

Epoch: 6| Step: 1
Training loss: 2.2716076374053955
Validation loss: 2.2203682878965973

Epoch: 6| Step: 2
Training loss: 2.23203182220459
Validation loss: 2.2195209854392597

Epoch: 6| Step: 3
Training loss: 2.442031145095825
Validation loss: 2.224201379283782

Epoch: 6| Step: 4
Training loss: 2.7349162101745605
Validation loss: 2.214983824760683

Epoch: 6| Step: 5
Training loss: 1.6628445386886597
Validation loss: 2.1996463806398454

Epoch: 6| Step: 6
Training loss: 2.827284336090088
Validation loss: 2.1971071125358663

Epoch: 6| Step: 7
Training loss: 1.9203615188598633
Validation loss: 2.197981793393371

Epoch: 6| Step: 8
Training loss: 2.0212149620056152
Validation loss: 2.210304726836502

Epoch: 6| Step: 9
Training loss: 2.084439754486084
Validation loss: 2.1951360574332615

Epoch: 6| Step: 10
Training loss: 2.4289398193359375
Validation loss: 2.1912832055040585

Epoch: 6| Step: 11
Training loss: 3.001814842224121
Validation loss: 2.1835166933715984

Epoch: 6| Step: 12
Training loss: 2.5527853965759277
Validation loss: 2.1820734957213044

Epoch: 6| Step: 13
Training loss: 2.596012830734253
Validation loss: 2.185349492616551

Epoch: 214| Step: 0
Training loss: 2.9714138507843018
Validation loss: 2.171026284976672

Epoch: 6| Step: 1
Training loss: 2.60603666305542
Validation loss: 2.179235799338228

Epoch: 6| Step: 2
Training loss: 2.3384432792663574
Validation loss: 2.1700559213597286

Epoch: 6| Step: 3
Training loss: 1.726316213607788
Validation loss: 2.1920278136448195

Epoch: 6| Step: 4
Training loss: 2.244642734527588
Validation loss: 2.2021682236784246

Epoch: 6| Step: 5
Training loss: 3.356259346008301
Validation loss: 2.2085678961969193

Epoch: 6| Step: 6
Training loss: 2.013233184814453
Validation loss: 2.195537068510568

Epoch: 6| Step: 7
Training loss: 1.5402050018310547
Validation loss: 2.1990541155620287

Epoch: 6| Step: 8
Training loss: 2.3144137859344482
Validation loss: 2.1829903023217314

Epoch: 6| Step: 9
Training loss: 2.606367588043213
Validation loss: 2.1727361422713085

Epoch: 6| Step: 10
Training loss: 2.5039501190185547
Validation loss: 2.1736019016594015

Epoch: 6| Step: 11
Training loss: 2.580907106399536
Validation loss: 2.175974061412196

Epoch: 6| Step: 12
Training loss: 2.0295372009277344
Validation loss: 2.1831610843699467

Epoch: 6| Step: 13
Training loss: 2.2426555156707764
Validation loss: 2.192791595253893

Epoch: 215| Step: 0
Training loss: 2.866356372833252
Validation loss: 2.199437411882544

Epoch: 6| Step: 1
Training loss: 1.979912281036377
Validation loss: 2.1975882822467434

Epoch: 6| Step: 2
Training loss: 2.103559732437134
Validation loss: 2.2012468948159167

Epoch: 6| Step: 3
Training loss: 2.9999380111694336
Validation loss: 2.190743002840268

Epoch: 6| Step: 4
Training loss: 2.6411147117614746
Validation loss: 2.186345572112709

Epoch: 6| Step: 5
Training loss: 2.4089536666870117
Validation loss: 2.1894932421304847

Epoch: 6| Step: 6
Training loss: 2.7059638500213623
Validation loss: 2.181163239222701

Epoch: 6| Step: 7
Training loss: 2.5503597259521484
Validation loss: 2.1837315918296896

Epoch: 6| Step: 8
Training loss: 2.2510859966278076
Validation loss: 2.19363909639338

Epoch: 6| Step: 9
Training loss: 2.83561110496521
Validation loss: 2.185683765719014

Epoch: 6| Step: 10
Training loss: 2.60736083984375
Validation loss: 2.195108562387446

Epoch: 6| Step: 11
Training loss: 1.4773716926574707
Validation loss: 2.2041098379319712

Epoch: 6| Step: 12
Training loss: 1.8394464254379272
Validation loss: 2.2029988970807803

Epoch: 6| Step: 13
Training loss: 1.736712098121643
Validation loss: 2.203512866009948

Epoch: 216| Step: 0
Training loss: 2.039274215698242
Validation loss: 2.2060325863540813

Epoch: 6| Step: 1
Training loss: 2.1657347679138184
Validation loss: 2.223797772520332

Epoch: 6| Step: 2
Training loss: 2.655545711517334
Validation loss: 2.236373446320975

Epoch: 6| Step: 3
Training loss: 2.2762820720672607
Validation loss: 2.238994008751326

Epoch: 6| Step: 4
Training loss: 2.022840976715088
Validation loss: 2.2290477111775386

Epoch: 6| Step: 5
Training loss: 2.46675968170166
Validation loss: 2.2190627333938435

Epoch: 6| Step: 6
Training loss: 2.20219087600708
Validation loss: 2.2181015758104223

Epoch: 6| Step: 7
Training loss: 1.9162530899047852
Validation loss: 2.196680861134683

Epoch: 6| Step: 8
Training loss: 2.433773994445801
Validation loss: 2.186555108716411

Epoch: 6| Step: 9
Training loss: 2.290835380554199
Validation loss: 2.1860627076959096

Epoch: 6| Step: 10
Training loss: 1.9803533554077148
Validation loss: 2.181765990872537

Epoch: 6| Step: 11
Training loss: 2.929988145828247
Validation loss: 2.178688032652742

Epoch: 6| Step: 12
Training loss: 2.984175443649292
Validation loss: 2.182274523601737

Epoch: 6| Step: 13
Training loss: 3.1176583766937256
Validation loss: 2.1763066386663787

Epoch: 217| Step: 0
Training loss: 1.9416335821151733
Validation loss: 2.176115951230449

Epoch: 6| Step: 1
Training loss: 2.073181629180908
Validation loss: 2.181075590913014

Epoch: 6| Step: 2
Training loss: 1.9207525253295898
Validation loss: 2.1773093336371967

Epoch: 6| Step: 3
Training loss: 2.045647144317627
Validation loss: 2.176243353915471

Epoch: 6| Step: 4
Training loss: 2.299895763397217
Validation loss: 2.1841270744159655

Epoch: 6| Step: 5
Training loss: 2.1551144123077393
Validation loss: 2.1929119479271675

Epoch: 6| Step: 6
Training loss: 3.269381046295166
Validation loss: 2.1927790180329354

Epoch: 6| Step: 7
Training loss: 1.935010552406311
Validation loss: 2.2055803691187212

Epoch: 6| Step: 8
Training loss: 2.0340209007263184
Validation loss: 2.2004092688201577

Epoch: 6| Step: 9
Training loss: 2.6735951900482178
Validation loss: 2.201118311574382

Epoch: 6| Step: 10
Training loss: 3.3438215255737305
Validation loss: 2.1894528788904988

Epoch: 6| Step: 11
Training loss: 2.1675374507904053
Validation loss: 2.196540553082702

Epoch: 6| Step: 12
Training loss: 2.895110845565796
Validation loss: 2.192128073784613

Epoch: 6| Step: 13
Training loss: 2.201569080352783
Validation loss: 2.1954561100211194

Epoch: 218| Step: 0
Training loss: 2.3026275634765625
Validation loss: 2.2009490843742125

Epoch: 6| Step: 1
Training loss: 2.073275089263916
Validation loss: 2.1908907890319824

Epoch: 6| Step: 2
Training loss: 2.843791961669922
Validation loss: 2.195050172908332

Epoch: 6| Step: 3
Training loss: 2.6851806640625
Validation loss: 2.185665935598394

Epoch: 6| Step: 4
Training loss: 2.022825241088867
Validation loss: 2.178515616283622

Epoch: 6| Step: 5
Training loss: 2.6220057010650635
Validation loss: 2.1836176662034887

Epoch: 6| Step: 6
Training loss: 2.5758771896362305
Validation loss: 2.187381409829663

Epoch: 6| Step: 7
Training loss: 2.914963722229004
Validation loss: 2.189494655978295

Epoch: 6| Step: 8
Training loss: 2.769186496734619
Validation loss: 2.1890515140307847

Epoch: 6| Step: 9
Training loss: 2.1760809421539307
Validation loss: 2.1795340673897856

Epoch: 6| Step: 10
Training loss: 2.5341808795928955
Validation loss: 2.177335613517351

Epoch: 6| Step: 11
Training loss: 1.456868290901184
Validation loss: 2.18449794220668

Epoch: 6| Step: 12
Training loss: 1.5436846017837524
Validation loss: 2.1813877526149956

Epoch: 6| Step: 13
Training loss: 2.3417043685913086
Validation loss: 2.178450092192619

Epoch: 219| Step: 0
Training loss: 2.5988636016845703
Validation loss: 2.197216982482582

Epoch: 6| Step: 1
Training loss: 2.3792219161987305
Validation loss: 2.204259580181491

Epoch: 6| Step: 2
Training loss: 3.2798662185668945
Validation loss: 2.203104055056008

Epoch: 6| Step: 3
Training loss: 2.1272315979003906
Validation loss: 2.197461853745163

Epoch: 6| Step: 4
Training loss: 2.331836700439453
Validation loss: 2.1950059629255727

Epoch: 6| Step: 5
Training loss: 2.108902931213379
Validation loss: 2.2073784592331096

Epoch: 6| Step: 6
Training loss: 2.0386641025543213
Validation loss: 2.202350872819142

Epoch: 6| Step: 7
Training loss: 2.2802438735961914
Validation loss: 2.1979628839800434

Epoch: 6| Step: 8
Training loss: 2.7924599647521973
Validation loss: 2.1950305815665954

Epoch: 6| Step: 9
Training loss: 1.7018009424209595
Validation loss: 2.19479981289115

Epoch: 6| Step: 10
Training loss: 1.6575291156768799
Validation loss: 2.1922863952575193

Epoch: 6| Step: 11
Training loss: 2.7381725311279297
Validation loss: 2.1903796580529984

Epoch: 6| Step: 12
Training loss: 2.595798969268799
Validation loss: 2.187044323131602

Epoch: 6| Step: 13
Training loss: 2.179581880569458
Validation loss: 2.189038171563097

Epoch: 220| Step: 0
Training loss: 1.8396894931793213
Validation loss: 2.18386584712613

Epoch: 6| Step: 1
Training loss: 1.569326400756836
Validation loss: 2.175438945011426

Epoch: 6| Step: 2
Training loss: 2.089054584503174
Validation loss: 2.175328511063771

Epoch: 6| Step: 3
Training loss: 2.2996559143066406
Validation loss: 2.1707776002986456

Epoch: 6| Step: 4
Training loss: 2.142930030822754
Validation loss: 2.1616146051755516

Epoch: 6| Step: 5
Training loss: 2.4632794857025146
Validation loss: 2.162780059281216

Epoch: 6| Step: 6
Training loss: 2.388620376586914
Validation loss: 2.1634665612251527

Epoch: 6| Step: 7
Training loss: 2.83253812789917
Validation loss: 2.1585127640795965

Epoch: 6| Step: 8
Training loss: 2.7125778198242188
Validation loss: 2.165138275392594

Epoch: 6| Step: 9
Training loss: 2.9223251342773438
Validation loss: 2.164297293591243

Epoch: 6| Step: 10
Training loss: 2.467989444732666
Validation loss: 2.1700020861882034

Epoch: 6| Step: 11
Training loss: 2.767404794692993
Validation loss: 2.172996056977139

Epoch: 6| Step: 12
Training loss: 2.1711695194244385
Validation loss: 2.1838063334905975

Epoch: 6| Step: 13
Training loss: 2.398071527481079
Validation loss: 2.186811245897765

Epoch: 221| Step: 0
Training loss: 2.444859504699707
Validation loss: 2.21909737330611

Epoch: 6| Step: 1
Training loss: 1.7742247581481934
Validation loss: 2.2045338692203647

Epoch: 6| Step: 2
Training loss: 1.8745174407958984
Validation loss: 2.1917931008082565

Epoch: 6| Step: 3
Training loss: 2.593346118927002
Validation loss: 2.1898768922334075

Epoch: 6| Step: 4
Training loss: 2.400563955307007
Validation loss: 2.191086733213035

Epoch: 6| Step: 5
Training loss: 2.710554838180542
Validation loss: 2.1871535188408306

Epoch: 6| Step: 6
Training loss: 2.629718780517578
Validation loss: 2.1770366199554934

Epoch: 6| Step: 7
Training loss: 2.703777551651001
Validation loss: 2.1849805847291024

Epoch: 6| Step: 8
Training loss: 2.0887908935546875
Validation loss: 2.183332553473852

Epoch: 6| Step: 9
Training loss: 1.665008306503296
Validation loss: 2.1772333729651665

Epoch: 6| Step: 10
Training loss: 2.760178565979004
Validation loss: 2.183840197901572

Epoch: 6| Step: 11
Training loss: 2.2525269985198975
Validation loss: 2.1673733893261162

Epoch: 6| Step: 12
Training loss: 2.8528332710266113
Validation loss: 2.182825801193073

Epoch: 6| Step: 13
Training loss: 2.0240955352783203
Validation loss: 2.1731269257042998

Epoch: 222| Step: 0
Training loss: 2.0656893253326416
Validation loss: 2.1825689628560054

Epoch: 6| Step: 1
Training loss: 1.6348021030426025
Validation loss: 2.187799576790102

Epoch: 6| Step: 2
Training loss: 3.1843109130859375
Validation loss: 2.1871197736391457

Epoch: 6| Step: 3
Training loss: 2.0134506225585938
Validation loss: 2.182970844289308

Epoch: 6| Step: 4
Training loss: 2.509767532348633
Validation loss: 2.1932777922640563

Epoch: 6| Step: 5
Training loss: 1.466288447380066
Validation loss: 2.1961639581188077

Epoch: 6| Step: 6
Training loss: 1.4574575424194336
Validation loss: 2.1967125733693442

Epoch: 6| Step: 7
Training loss: 2.8327653408050537
Validation loss: 2.207892243580152

Epoch: 6| Step: 8
Training loss: 1.8161251544952393
Validation loss: 2.204450685490844

Epoch: 6| Step: 9
Training loss: 3.245058059692383
Validation loss: 2.1980584257392475

Epoch: 6| Step: 10
Training loss: 3.2717103958129883
Validation loss: 2.1887300142677883

Epoch: 6| Step: 11
Training loss: 2.585455894470215
Validation loss: 2.1822945994715535

Epoch: 6| Step: 12
Training loss: 2.0206916332244873
Validation loss: 2.1754111884742655

Epoch: 6| Step: 13
Training loss: 2.851494550704956
Validation loss: 2.1834275543048816

Epoch: 223| Step: 0
Training loss: 2.464834451675415
Validation loss: 2.179577589035034

Epoch: 6| Step: 1
Training loss: 1.8941248655319214
Validation loss: 2.1740050802948656

Epoch: 6| Step: 2
Training loss: 2.23486065864563
Validation loss: 2.1711895363305205

Epoch: 6| Step: 3
Training loss: 2.894320487976074
Validation loss: 2.1679377350755917

Epoch: 6| Step: 4
Training loss: 2.7352676391601562
Validation loss: 2.159388301193073

Epoch: 6| Step: 5
Training loss: 2.741671323776245
Validation loss: 2.1577026305660123

Epoch: 6| Step: 6
Training loss: 1.9205865859985352
Validation loss: 2.1533401102148075

Epoch: 6| Step: 7
Training loss: 1.9331822395324707
Validation loss: 2.1556506336376233

Epoch: 6| Step: 8
Training loss: 2.391444683074951
Validation loss: 2.1570824077052455

Epoch: 6| Step: 9
Training loss: 2.96284556388855
Validation loss: 2.1551821642024542

Epoch: 6| Step: 10
Training loss: 1.6211340427398682
Validation loss: 2.1618148998547624

Epoch: 6| Step: 11
Training loss: 2.2573142051696777
Validation loss: 2.175703469143119

Epoch: 6| Step: 12
Training loss: 2.7555599212646484
Validation loss: 2.170694048686694

Epoch: 6| Step: 13
Training loss: 1.8827383518218994
Validation loss: 2.1777679997105754

Epoch: 224| Step: 0
Training loss: 2.0930075645446777
Validation loss: 2.176096708543839

Epoch: 6| Step: 1
Training loss: 2.3756496906280518
Validation loss: 2.1787192898411907

Epoch: 6| Step: 2
Training loss: 2.2346596717834473
Validation loss: 2.1836899788148942

Epoch: 6| Step: 3
Training loss: 2.2482359409332275
Validation loss: 2.1797132761247697

Epoch: 6| Step: 4
Training loss: 2.065471887588501
Validation loss: 2.1970059076944985

Epoch: 6| Step: 5
Training loss: 2.392145872116089
Validation loss: 2.2162658117150746

Epoch: 6| Step: 6
Training loss: 2.508610725402832
Validation loss: 2.2079448635860155

Epoch: 6| Step: 7
Training loss: 1.9384629726409912
Validation loss: 2.188514951736696

Epoch: 6| Step: 8
Training loss: 2.941425323486328
Validation loss: 2.190906222148608

Epoch: 6| Step: 9
Training loss: 2.2640867233276367
Validation loss: 2.1740478008024153

Epoch: 6| Step: 10
Training loss: 2.7914741039276123
Validation loss: 2.153831242233194

Epoch: 6| Step: 11
Training loss: 2.6351189613342285
Validation loss: 2.1561929628413212

Epoch: 6| Step: 12
Training loss: 2.2371747493743896
Validation loss: 2.1513361636028496

Epoch: 6| Step: 13
Training loss: 2.268327474594116
Validation loss: 2.155882194478025

Epoch: 225| Step: 0
Training loss: 2.6067380905151367
Validation loss: 2.151672863191174

Epoch: 6| Step: 1
Training loss: 2.028409719467163
Validation loss: 2.1397824389960176

Epoch: 6| Step: 2
Training loss: 2.5352039337158203
Validation loss: 2.146342603109216

Epoch: 6| Step: 3
Training loss: 2.1063437461853027
Validation loss: 2.137458788451328

Epoch: 6| Step: 4
Training loss: 2.371591091156006
Validation loss: 2.142801171989851

Epoch: 6| Step: 5
Training loss: 2.315488338470459
Validation loss: 2.1480981124344694

Epoch: 6| Step: 6
Training loss: 3.0241827964782715
Validation loss: 2.146100467251193

Epoch: 6| Step: 7
Training loss: 2.3736894130706787
Validation loss: 2.1529641356519473

Epoch: 6| Step: 8
Training loss: 1.724749207496643
Validation loss: 2.1412439436040898

Epoch: 6| Step: 9
Training loss: 1.7461371421813965
Validation loss: 2.1547982000535533

Epoch: 6| Step: 10
Training loss: 2.154752731323242
Validation loss: 2.1636323287922847

Epoch: 6| Step: 11
Training loss: 2.668962001800537
Validation loss: 2.160653119446129

Epoch: 6| Step: 12
Training loss: 2.4572784900665283
Validation loss: 2.166003732271092

Epoch: 6| Step: 13
Training loss: 3.1434786319732666
Validation loss: 2.1607274573336364

Epoch: 226| Step: 0
Training loss: 2.0386316776275635
Validation loss: 2.1670450138789352

Epoch: 6| Step: 1
Training loss: 2.3442797660827637
Validation loss: 2.1750772819724133

Epoch: 6| Step: 2
Training loss: 3.050553798675537
Validation loss: 2.18017868585484

Epoch: 6| Step: 3
Training loss: 2.4293696880340576
Validation loss: 2.1892054439872823

Epoch: 6| Step: 4
Training loss: 2.199380397796631
Validation loss: 2.1778517641046995

Epoch: 6| Step: 5
Training loss: 2.117520809173584
Validation loss: 2.175508288926976

Epoch: 6| Step: 6
Training loss: 2.725497245788574
Validation loss: 2.171570570238175

Epoch: 6| Step: 7
Training loss: 2.952892303466797
Validation loss: 2.181776567171979

Epoch: 6| Step: 8
Training loss: 2.226101875305176
Validation loss: 2.1736171553211827

Epoch: 6| Step: 9
Training loss: 2.486900806427002
Validation loss: 2.1595801281672653

Epoch: 6| Step: 10
Training loss: 1.8624686002731323
Validation loss: 2.171142000024037

Epoch: 6| Step: 11
Training loss: 2.048250436782837
Validation loss: 2.176849688253095

Epoch: 6| Step: 12
Training loss: 1.6903553009033203
Validation loss: 2.1703636620634343

Epoch: 6| Step: 13
Training loss: 2.635061502456665
Validation loss: 2.1745271349465973

Epoch: 227| Step: 0
Training loss: 2.1658756732940674
Validation loss: 2.1693756862353255

Epoch: 6| Step: 1
Training loss: 1.9059866666793823
Validation loss: 2.170238794819001

Epoch: 6| Step: 2
Training loss: 2.146868944168091
Validation loss: 2.159191285410235

Epoch: 6| Step: 3
Training loss: 2.313525676727295
Validation loss: 2.172763386080342

Epoch: 6| Step: 4
Training loss: 2.9485442638397217
Validation loss: 2.1690860063798967

Epoch: 6| Step: 5
Training loss: 2.6159396171569824
Validation loss: 2.170398642939906

Epoch: 6| Step: 6
Training loss: 1.8746113777160645
Validation loss: 2.1811554278096845

Epoch: 6| Step: 7
Training loss: 2.123016357421875
Validation loss: 2.170917562259141

Epoch: 6| Step: 8
Training loss: 1.559748649597168
Validation loss: 2.16955340421328

Epoch: 6| Step: 9
Training loss: 2.2636868953704834
Validation loss: 2.163635319279086

Epoch: 6| Step: 10
Training loss: 2.0210928916931152
Validation loss: 2.174430258812443

Epoch: 6| Step: 11
Training loss: 3.035663604736328
Validation loss: 2.169161073623165

Epoch: 6| Step: 12
Training loss: 3.2007253170013428
Validation loss: 2.1716917996765464

Epoch: 6| Step: 13
Training loss: 2.413327932357788
Validation loss: 2.1605459720857683

Epoch: 228| Step: 0
Training loss: 2.204357147216797
Validation loss: 2.1656957070032754

Epoch: 6| Step: 1
Training loss: 2.2673277854919434
Validation loss: 2.1693308891788607

Epoch: 6| Step: 2
Training loss: 2.7673490047454834
Validation loss: 2.1744034828678256

Epoch: 6| Step: 3
Training loss: 1.9121109247207642
Validation loss: 2.1750245094299316

Epoch: 6| Step: 4
Training loss: 2.9105238914489746
Validation loss: 2.168352223211719

Epoch: 6| Step: 5
Training loss: 2.24103045463562
Validation loss: 2.1670554812236498

Epoch: 6| Step: 6
Training loss: 2.9546709060668945
Validation loss: 2.161044159243184

Epoch: 6| Step: 7
Training loss: 2.451021194458008
Validation loss: 2.161777315601226

Epoch: 6| Step: 8
Training loss: 2.3053789138793945
Validation loss: 2.1631992196524017

Epoch: 6| Step: 9
Training loss: 2.6977896690368652
Validation loss: 2.1579327250039704

Epoch: 6| Step: 10
Training loss: 1.6370599269866943
Validation loss: 2.1648400034955753

Epoch: 6| Step: 11
Training loss: 1.595110297203064
Validation loss: 2.159988216174546

Epoch: 6| Step: 12
Training loss: 2.306149482727051
Validation loss: 2.1629845044946157

Epoch: 6| Step: 13
Training loss: 2.140354871749878
Validation loss: 2.1625143892021588

Epoch: 229| Step: 0
Training loss: 2.4057531356811523
Validation loss: 2.16449987247426

Epoch: 6| Step: 1
Training loss: 1.9294806718826294
Validation loss: 2.1735071700106383

Epoch: 6| Step: 2
Training loss: 2.241908073425293
Validation loss: 2.1677407603110037

Epoch: 6| Step: 3
Training loss: 1.8483918905258179
Validation loss: 2.191462655221262

Epoch: 6| Step: 4
Training loss: 1.7568564414978027
Validation loss: 2.208721968435472

Epoch: 6| Step: 5
Training loss: 2.6823577880859375
Validation loss: 2.2280541696856098

Epoch: 6| Step: 6
Training loss: 2.450153350830078
Validation loss: 2.234564901680075

Epoch: 6| Step: 7
Training loss: 2.0308775901794434
Validation loss: 2.212443179981683

Epoch: 6| Step: 8
Training loss: 2.2149126529693604
Validation loss: 2.1882999379147767

Epoch: 6| Step: 9
Training loss: 2.8408665657043457
Validation loss: 2.1725585306844404

Epoch: 6| Step: 10
Training loss: 2.8130898475646973
Validation loss: 2.167674336382138

Epoch: 6| Step: 11
Training loss: 2.4202723503112793
Validation loss: 2.157472687382852

Epoch: 6| Step: 12
Training loss: 2.7168397903442383
Validation loss: 2.155239235970282

Epoch: 6| Step: 13
Training loss: 2.0347671508789062
Validation loss: 2.1443008299796813

Epoch: 230| Step: 0
Training loss: 1.6947846412658691
Validation loss: 2.148522197559316

Epoch: 6| Step: 1
Training loss: 2.9149351119995117
Validation loss: 2.147574850307998

Epoch: 6| Step: 2
Training loss: 2.022233247756958
Validation loss: 2.1460850700255363

Epoch: 6| Step: 3
Training loss: 1.8451099395751953
Validation loss: 2.1425909790941464

Epoch: 6| Step: 4
Training loss: 2.38204288482666
Validation loss: 2.145536438111336

Epoch: 6| Step: 5
Training loss: 2.3464584350585938
Validation loss: 2.1585259668288694

Epoch: 6| Step: 6
Training loss: 2.100839138031006
Validation loss: 2.1552272868412796

Epoch: 6| Step: 7
Training loss: 2.6232213973999023
Validation loss: 2.147345148107057

Epoch: 6| Step: 8
Training loss: 2.8198955059051514
Validation loss: 2.1542762761474936

Epoch: 6| Step: 9
Training loss: 2.3038320541381836
Validation loss: 2.1428438463518695

Epoch: 6| Step: 10
Training loss: 2.655034303665161
Validation loss: 2.1483313870686356

Epoch: 6| Step: 11
Training loss: 2.337820529937744
Validation loss: 2.1554277917390228

Epoch: 6| Step: 12
Training loss: 2.270641326904297
Validation loss: 2.159018902368443

Epoch: 6| Step: 13
Training loss: 2.5990400314331055
Validation loss: 2.1667525960553076

Epoch: 231| Step: 0
Training loss: 2.4383223056793213
Validation loss: 2.1661604219867336

Epoch: 6| Step: 1
Training loss: 2.6925320625305176
Validation loss: 2.169981610390448

Epoch: 6| Step: 2
Training loss: 2.9528849124908447
Validation loss: 2.1598955149291665

Epoch: 6| Step: 3
Training loss: 2.6846377849578857
Validation loss: 2.172518144371689

Epoch: 6| Step: 4
Training loss: 2.6511781215667725
Validation loss: 2.169572199544599

Epoch: 6| Step: 5
Training loss: 2.4672064781188965
Validation loss: 2.1720965934056107

Epoch: 6| Step: 6
Training loss: 2.48539137840271
Validation loss: 2.1706249124260357

Epoch: 6| Step: 7
Training loss: 2.222872018814087
Validation loss: 2.1646633686557895

Epoch: 6| Step: 8
Training loss: 1.5203899145126343
Validation loss: 2.165406665494365

Epoch: 6| Step: 9
Training loss: 2.085597038269043
Validation loss: 2.1727650575740363

Epoch: 6| Step: 10
Training loss: 2.3499064445495605
Validation loss: 2.1755266433121054

Epoch: 6| Step: 11
Training loss: 1.790558099746704
Validation loss: 2.1808192012130574

Epoch: 6| Step: 12
Training loss: 1.7615658044815063
Validation loss: 2.1762303831756755

Epoch: 6| Step: 13
Training loss: 2.3354103565216064
Validation loss: 2.1787818939455095

Epoch: 232| Step: 0
Training loss: 2.1163995265960693
Validation loss: 2.181869652963454

Epoch: 6| Step: 1
Training loss: 2.1993331909179688
Validation loss: 2.1798878280065392

Epoch: 6| Step: 2
Training loss: 1.9275691509246826
Validation loss: 2.1805551052093506

Epoch: 6| Step: 3
Training loss: 2.3426241874694824
Validation loss: 2.177038070976093

Epoch: 6| Step: 4
Training loss: 1.9048752784729004
Validation loss: 2.1693989807559597

Epoch: 6| Step: 5
Training loss: 2.1590328216552734
Validation loss: 2.168932775015472

Epoch: 6| Step: 6
Training loss: 2.3510115146636963
Validation loss: 2.1703861234008626

Epoch: 6| Step: 7
Training loss: 2.430800437927246
Validation loss: 2.156550065163643

Epoch: 6| Step: 8
Training loss: 2.021310806274414
Validation loss: 2.15943101016424

Epoch: 6| Step: 9
Training loss: 3.42541766166687
Validation loss: 2.146393258084533

Epoch: 6| Step: 10
Training loss: 2.7457566261291504
Validation loss: 2.147808746625018

Epoch: 6| Step: 11
Training loss: 2.188369035720825
Validation loss: 2.156568980986072

Epoch: 6| Step: 12
Training loss: 2.6108384132385254
Validation loss: 2.155919994077375

Epoch: 6| Step: 13
Training loss: 1.619223713874817
Validation loss: 2.151510479629681

Epoch: 233| Step: 0
Training loss: 2.1180524826049805
Validation loss: 2.1604768255705475

Epoch: 6| Step: 1
Training loss: 2.3931455612182617
Validation loss: 2.1660458528867332

Epoch: 6| Step: 2
Training loss: 2.0924453735351562
Validation loss: 2.168251834889894

Epoch: 6| Step: 3
Training loss: 2.786400556564331
Validation loss: 2.170337297583139

Epoch: 6| Step: 4
Training loss: 2.839327812194824
Validation loss: 2.1740641952842794

Epoch: 6| Step: 5
Training loss: 2.246807098388672
Validation loss: 2.160549138181953

Epoch: 6| Step: 6
Training loss: 2.7008488178253174
Validation loss: 2.1690843425771242

Epoch: 6| Step: 7
Training loss: 1.6938048601150513
Validation loss: 2.1663058368108605

Epoch: 6| Step: 8
Training loss: 2.347749948501587
Validation loss: 2.1652774451881327

Epoch: 6| Step: 9
Training loss: 3.05094313621521
Validation loss: 2.1640736082548737

Epoch: 6| Step: 10
Training loss: 1.3471622467041016
Validation loss: 2.157696503464894

Epoch: 6| Step: 11
Training loss: 2.401658058166504
Validation loss: 2.165521829358993

Epoch: 6| Step: 12
Training loss: 2.0437655448913574
Validation loss: 2.1716837908632014

Epoch: 6| Step: 13
Training loss: 2.3879566192626953
Validation loss: 2.1581956263511413

Epoch: 234| Step: 0
Training loss: 2.7898783683776855
Validation loss: 2.1635650024619153

Epoch: 6| Step: 1
Training loss: 2.242802619934082
Validation loss: 2.1638759746346423

Epoch: 6| Step: 2
Training loss: 1.6946160793304443
Validation loss: 2.177335459698913

Epoch: 6| Step: 3
Training loss: 2.378648281097412
Validation loss: 2.1674627975750993

Epoch: 6| Step: 4
Training loss: 2.2939095497131348
Validation loss: 2.1716120499436573

Epoch: 6| Step: 5
Training loss: 2.391627073287964
Validation loss: 2.173569025531892

Epoch: 6| Step: 6
Training loss: 2.067807912826538
Validation loss: 2.163090734071629

Epoch: 6| Step: 7
Training loss: 2.3898937702178955
Validation loss: 2.1563319083183043

Epoch: 6| Step: 8
Training loss: 2.656461715698242
Validation loss: 2.156590700149536

Epoch: 6| Step: 9
Training loss: 2.4645588397979736
Validation loss: 2.160065189484627

Epoch: 6| Step: 10
Training loss: 2.266162872314453
Validation loss: 2.154600465169517

Epoch: 6| Step: 11
Training loss: 2.4929311275482178
Validation loss: 2.1540534393761748

Epoch: 6| Step: 12
Training loss: 1.6385658979415894
Validation loss: 2.1478663900847077

Epoch: 6| Step: 13
Training loss: 2.818317174911499
Validation loss: 2.147344430287679

Epoch: 235| Step: 0
Training loss: 2.171541690826416
Validation loss: 2.141418239121796

Epoch: 6| Step: 1
Training loss: 2.662868022918701
Validation loss: 2.143498630933864

Epoch: 6| Step: 2
Training loss: 2.2288572788238525
Validation loss: 2.153373041460591

Epoch: 6| Step: 3
Training loss: 2.4174370765686035
Validation loss: 2.1555296785087994

Epoch: 6| Step: 4
Training loss: 2.754542827606201
Validation loss: 2.153584646922286

Epoch: 6| Step: 5
Training loss: 2.629424571990967
Validation loss: 2.153585719805892

Epoch: 6| Step: 6
Training loss: 2.2881546020507812
Validation loss: 2.150308093717021

Epoch: 6| Step: 7
Training loss: 1.834266185760498
Validation loss: 2.1499394703936834

Epoch: 6| Step: 8
Training loss: 2.1734564304351807
Validation loss: 2.1606300261712845

Epoch: 6| Step: 9
Training loss: 2.93527889251709
Validation loss: 2.157904545466105

Epoch: 6| Step: 10
Training loss: 2.107403039932251
Validation loss: 2.1551803824722127

Epoch: 6| Step: 11
Training loss: 2.1787075996398926
Validation loss: 2.1504442025256414

Epoch: 6| Step: 12
Training loss: 1.9587926864624023
Validation loss: 2.1516062367346978

Epoch: 6| Step: 13
Training loss: 1.6511931419372559
Validation loss: 2.1622960567474365

Epoch: 236| Step: 0
Training loss: 1.5646514892578125
Validation loss: 2.1591459243528304

Epoch: 6| Step: 1
Training loss: 2.5876832008361816
Validation loss: 2.1679524221727924

Epoch: 6| Step: 2
Training loss: 1.9366520643234253
Validation loss: 2.177245593840076

Epoch: 6| Step: 3
Training loss: 2.6546082496643066
Validation loss: 2.1739439528475524

Epoch: 6| Step: 4
Training loss: 3.0371899604797363
Validation loss: 2.1759559069910357

Epoch: 6| Step: 5
Training loss: 1.9050705432891846
Validation loss: 2.177411153752317

Epoch: 6| Step: 6
Training loss: 2.584893226623535
Validation loss: 2.175050397073069

Epoch: 6| Step: 7
Training loss: 2.74399471282959
Validation loss: 2.162353459224906

Epoch: 6| Step: 8
Training loss: 1.9572335481643677
Validation loss: 2.162323333883798

Epoch: 6| Step: 9
Training loss: 2.1018362045288086
Validation loss: 2.1583002075072257

Epoch: 6| Step: 10
Training loss: 2.0999531745910645
Validation loss: 2.1631799256929787

Epoch: 6| Step: 11
Training loss: 2.692763090133667
Validation loss: 2.1778072041849934

Epoch: 6| Step: 12
Training loss: 2.447248935699463
Validation loss: 2.159095310395764

Epoch: 6| Step: 13
Training loss: 1.6635583639144897
Validation loss: 2.1593057494009695

Epoch: 237| Step: 0
Training loss: 1.9313238859176636
Validation loss: 2.151577503450455

Epoch: 6| Step: 1
Training loss: 2.056739330291748
Validation loss: 2.1394008000691733

Epoch: 6| Step: 2
Training loss: 2.483799457550049
Validation loss: 2.1337645028227117

Epoch: 6| Step: 3
Training loss: 1.8662961721420288
Validation loss: 2.1369176346768617

Epoch: 6| Step: 4
Training loss: 1.6625382900238037
Validation loss: 2.137226322645782

Epoch: 6| Step: 5
Training loss: 1.7334318161010742
Validation loss: 2.1393423516263246

Epoch: 6| Step: 6
Training loss: 2.8641233444213867
Validation loss: 2.1475003201474427

Epoch: 6| Step: 7
Training loss: 2.7768666744232178
Validation loss: 2.1353607741735314

Epoch: 6| Step: 8
Training loss: 2.594226360321045
Validation loss: 2.1499907355154715

Epoch: 6| Step: 9
Training loss: 3.1148669719696045
Validation loss: 2.1606561676148446

Epoch: 6| Step: 10
Training loss: 2.015245199203491
Validation loss: 2.1640736031275924

Epoch: 6| Step: 11
Training loss: 2.553225040435791
Validation loss: 2.1673243314989152

Epoch: 6| Step: 12
Training loss: 1.8975414037704468
Validation loss: 2.15673219003985

Epoch: 6| Step: 13
Training loss: 3.21610164642334
Validation loss: 2.1535857877423688

Epoch: 238| Step: 0
Training loss: 2.226938247680664
Validation loss: 2.158129317786104

Epoch: 6| Step: 1
Training loss: 2.3096137046813965
Validation loss: 2.1518771699679795

Epoch: 6| Step: 2
Training loss: 1.4043153524398804
Validation loss: 2.1605373044167795

Epoch: 6| Step: 3
Training loss: 2.5675487518310547
Validation loss: 2.1681738335599183

Epoch: 6| Step: 4
Training loss: 2.683206558227539
Validation loss: 2.171579558362243

Epoch: 6| Step: 5
Training loss: 2.4974894523620605
Validation loss: 2.163273108902798

Epoch: 6| Step: 6
Training loss: 2.329334020614624
Validation loss: 2.157192891643893

Epoch: 6| Step: 7
Training loss: 2.272952079772949
Validation loss: 2.1550450248102986

Epoch: 6| Step: 8
Training loss: 2.319847583770752
Validation loss: 2.1467548929234987

Epoch: 6| Step: 9
Training loss: 1.9819198846817017
Validation loss: 2.155824112635787

Epoch: 6| Step: 10
Training loss: 2.4807276725769043
Validation loss: 2.1684217119729645

Epoch: 6| Step: 11
Training loss: 1.775403618812561
Validation loss: 2.170451764137514

Epoch: 6| Step: 12
Training loss: 3.2377548217773438
Validation loss: 2.1668312395772626

Epoch: 6| Step: 13
Training loss: 1.8501611948013306
Validation loss: 2.1884202367515972

Epoch: 239| Step: 0
Training loss: 2.4822397232055664
Validation loss: 2.194243479800481

Epoch: 6| Step: 1
Training loss: 1.849318027496338
Validation loss: 2.195009603295275

Epoch: 6| Step: 2
Training loss: 2.576932430267334
Validation loss: 2.2078380020715858

Epoch: 6| Step: 3
Training loss: 2.1190507411956787
Validation loss: 2.1864815424847346

Epoch: 6| Step: 4
Training loss: 2.1578681468963623
Validation loss: 2.1716227736524356

Epoch: 6| Step: 5
Training loss: 2.2929253578186035
Validation loss: 2.1510875737795265

Epoch: 6| Step: 6
Training loss: 1.7993314266204834
Validation loss: 2.155883926217274

Epoch: 6| Step: 7
Training loss: 1.9628517627716064
Validation loss: 2.1505904325874905

Epoch: 6| Step: 8
Training loss: 2.9291718006134033
Validation loss: 2.169117089240782

Epoch: 6| Step: 9
Training loss: 2.537348985671997
Validation loss: 2.1518432760751374

Epoch: 6| Step: 10
Training loss: 2.488459825515747
Validation loss: 2.1612014385961715

Epoch: 6| Step: 11
Training loss: 2.274064540863037
Validation loss: 2.1659549333715953

Epoch: 6| Step: 12
Training loss: 2.6300461292266846
Validation loss: 2.16055812117874

Epoch: 6| Step: 13
Training loss: 1.98295259475708
Validation loss: 2.1597391584868073

Epoch: 240| Step: 0
Training loss: 2.0824713706970215
Validation loss: 2.1593516847138763

Epoch: 6| Step: 1
Training loss: 2.244968891143799
Validation loss: 2.1381961171345045

Epoch: 6| Step: 2
Training loss: 1.4569246768951416
Validation loss: 2.140555671466294

Epoch: 6| Step: 3
Training loss: 1.7082685232162476
Validation loss: 2.1338975711535384

Epoch: 6| Step: 4
Training loss: 2.072493076324463
Validation loss: 2.135103935836464

Epoch: 6| Step: 5
Training loss: 1.8148454427719116
Validation loss: 2.13934987847523

Epoch: 6| Step: 6
Training loss: 3.209587574005127
Validation loss: 2.1479328781045894

Epoch: 6| Step: 7
Training loss: 3.1530065536499023
Validation loss: 2.1375084051521878

Epoch: 6| Step: 8
Training loss: 2.564171314239502
Validation loss: 2.152839970845048

Epoch: 6| Step: 9
Training loss: 2.254887580871582
Validation loss: 2.1423525643605057

Epoch: 6| Step: 10
Training loss: 2.4034299850463867
Validation loss: 2.1411445858658

Epoch: 6| Step: 11
Training loss: 1.8263698816299438
Validation loss: 2.1309550641685404

Epoch: 6| Step: 12
Training loss: 2.7157957553863525
Validation loss: 2.1337184444550545

Epoch: 6| Step: 13
Training loss: 2.9541141986846924
Validation loss: 2.1292551512359292

Epoch: 241| Step: 0
Training loss: 2.4575655460357666
Validation loss: 2.130852109642439

Epoch: 6| Step: 1
Training loss: 1.9352989196777344
Validation loss: 2.1356624454580326

Epoch: 6| Step: 2
Training loss: 1.959388017654419
Validation loss: 2.132585676767493

Epoch: 6| Step: 3
Training loss: 2.653120279312134
Validation loss: 2.13602949214238

Epoch: 6| Step: 4
Training loss: 1.8384252786636353
Validation loss: 2.1423721569840626

Epoch: 6| Step: 5
Training loss: 2.4530229568481445
Validation loss: 2.138230618610177

Epoch: 6| Step: 6
Training loss: 2.568138599395752
Validation loss: 2.1368091260233233

Epoch: 6| Step: 7
Training loss: 2.410374641418457
Validation loss: 2.139182899587898

Epoch: 6| Step: 8
Training loss: 2.7131357192993164
Validation loss: 2.1406394127876527

Epoch: 6| Step: 9
Training loss: 1.8791347742080688
Validation loss: 2.1304317520510767

Epoch: 6| Step: 10
Training loss: 2.3537778854370117
Validation loss: 2.151838510267196

Epoch: 6| Step: 11
Training loss: 1.9969482421875
Validation loss: 2.162956691557361

Epoch: 6| Step: 12
Training loss: 1.7155117988586426
Validation loss: 2.1779948947250203

Epoch: 6| Step: 13
Training loss: 3.8359861373901367
Validation loss: 2.17742217997069

Epoch: 242| Step: 0
Training loss: 2.3782825469970703
Validation loss: 2.1633837812690326

Epoch: 6| Step: 1
Training loss: 2.206850290298462
Validation loss: 2.1565954890302432

Epoch: 6| Step: 2
Training loss: 2.227682113647461
Validation loss: 2.144854120028916

Epoch: 6| Step: 3
Training loss: 2.3455593585968018
Validation loss: 2.142253347622451

Epoch: 6| Step: 4
Training loss: 1.4891712665557861
Validation loss: 2.149192735713015

Epoch: 6| Step: 5
Training loss: 3.0278987884521484
Validation loss: 2.144015622395341

Epoch: 6| Step: 6
Training loss: 1.9542615413665771
Validation loss: 2.150000197913057

Epoch: 6| Step: 7
Training loss: 2.795620918273926
Validation loss: 2.1499466844784316

Epoch: 6| Step: 8
Training loss: 3.453282594680786
Validation loss: 2.157740282756026

Epoch: 6| Step: 9
Training loss: 2.5260138511657715
Validation loss: 2.1525033238113567

Epoch: 6| Step: 10
Training loss: 1.7805712223052979
Validation loss: 2.1472089969983665

Epoch: 6| Step: 11
Training loss: 1.9026626348495483
Validation loss: 2.152192602875412

Epoch: 6| Step: 12
Training loss: 1.7879103422164917
Validation loss: 2.1437224341977026

Epoch: 6| Step: 13
Training loss: 2.2175090312957764
Validation loss: 2.1539219079479093

Epoch: 243| Step: 0
Training loss: 2.0429067611694336
Validation loss: 2.14559668238445

Epoch: 6| Step: 1
Training loss: 2.2569665908813477
Validation loss: 2.1382400322985906

Epoch: 6| Step: 2
Training loss: 2.364377975463867
Validation loss: 2.138018949057466

Epoch: 6| Step: 3
Training loss: 1.2396295070648193
Validation loss: 2.134378933137463

Epoch: 6| Step: 4
Training loss: 2.7346129417419434
Validation loss: 2.1354435259296047

Epoch: 6| Step: 5
Training loss: 2.4132933616638184
Validation loss: 2.1388304156641804

Epoch: 6| Step: 6
Training loss: 1.578331708908081
Validation loss: 2.1420226622653264

Epoch: 6| Step: 7
Training loss: 2.322323799133301
Validation loss: 2.1432888379660984

Epoch: 6| Step: 8
Training loss: 2.847686290740967
Validation loss: 2.1460800965627036

Epoch: 6| Step: 9
Training loss: 2.1116373538970947
Validation loss: 2.151764008306688

Epoch: 6| Step: 10
Training loss: 2.7319161891937256
Validation loss: 2.153382307739668

Epoch: 6| Step: 11
Training loss: 2.5244932174682617
Validation loss: 2.1558748663112683

Epoch: 6| Step: 12
Training loss: 2.0705502033233643
Validation loss: 2.1627083580981017

Epoch: 6| Step: 13
Training loss: 3.109316110610962
Validation loss: 2.1438016096750894

Epoch: 244| Step: 0
Training loss: 2.17099666595459
Validation loss: 2.1485658089319863

Epoch: 6| Step: 1
Training loss: 2.6778416633605957
Validation loss: 2.139003333225045

Epoch: 6| Step: 2
Training loss: 2.8348913192749023
Validation loss: 2.1257154326285086

Epoch: 6| Step: 3
Training loss: 2.130190372467041
Validation loss: 2.1306793330818095

Epoch: 6| Step: 4
Training loss: 1.8535585403442383
Validation loss: 2.1202338331489154

Epoch: 6| Step: 5
Training loss: 2.6180386543273926
Validation loss: 2.135176750921434

Epoch: 6| Step: 6
Training loss: 1.8212833404541016
Validation loss: 2.134696961731039

Epoch: 6| Step: 7
Training loss: 2.709829568862915
Validation loss: 2.1327401386794222

Epoch: 6| Step: 8
Training loss: 2.2107186317443848
Validation loss: 2.143812071892523

Epoch: 6| Step: 9
Training loss: 2.6386215686798096
Validation loss: 2.1484698608357418

Epoch: 6| Step: 10
Training loss: 2.1235461235046387
Validation loss: 2.1641187001300115

Epoch: 6| Step: 11
Training loss: 1.9988569021224976
Validation loss: 2.1608771765103905

Epoch: 6| Step: 12
Training loss: 2.039241313934326
Validation loss: 2.155971159217178

Epoch: 6| Step: 13
Training loss: 1.9920220375061035
Validation loss: 2.1545644883186585

Epoch: 245| Step: 0
Training loss: 2.356508255004883
Validation loss: 2.153340353760668

Epoch: 6| Step: 1
Training loss: 2.6840248107910156
Validation loss: 2.1548754105003933

Epoch: 6| Step: 2
Training loss: 2.6570186614990234
Validation loss: 2.142344287646714

Epoch: 6| Step: 3
Training loss: 1.736127495765686
Validation loss: 2.1370925441865

Epoch: 6| Step: 4
Training loss: 1.8844987154006958
Validation loss: 2.1369865966099564

Epoch: 6| Step: 5
Training loss: 2.35672664642334
Validation loss: 2.1327496241497736

Epoch: 6| Step: 6
Training loss: 1.991459846496582
Validation loss: 2.127683747199274

Epoch: 6| Step: 7
Training loss: 2.176759719848633
Validation loss: 2.128833632315359

Epoch: 6| Step: 8
Training loss: 2.1887731552124023
Validation loss: 2.121309623923353

Epoch: 6| Step: 9
Training loss: 1.8312420845031738
Validation loss: 2.1365154161248157

Epoch: 6| Step: 10
Training loss: 2.7275938987731934
Validation loss: 2.1379156651035434

Epoch: 6| Step: 11
Training loss: 2.5534660816192627
Validation loss: 2.1349861198855984

Epoch: 6| Step: 12
Training loss: 2.023149013519287
Validation loss: 2.1348077430520007

Epoch: 6| Step: 13
Training loss: 2.8977603912353516
Validation loss: 2.1319255495584137

Epoch: 246| Step: 0
Training loss: 2.1239306926727295
Validation loss: 2.139594083191246

Epoch: 6| Step: 1
Training loss: 2.864387273788452
Validation loss: 2.1422802812309674

Epoch: 6| Step: 2
Training loss: 1.938080072402954
Validation loss: 2.1449268992229173

Epoch: 6| Step: 3
Training loss: 2.6598293781280518
Validation loss: 2.1351901254346295

Epoch: 6| Step: 4
Training loss: 2.283553123474121
Validation loss: 2.146926082590575

Epoch: 6| Step: 5
Training loss: 1.9226229190826416
Validation loss: 2.1425192740655716

Epoch: 6| Step: 6
Training loss: 2.3801045417785645
Validation loss: 2.135909670142717

Epoch: 6| Step: 7
Training loss: 1.8037986755371094
Validation loss: 2.1545344270685667

Epoch: 6| Step: 8
Training loss: 2.504897117614746
Validation loss: 2.1498826601172007

Epoch: 6| Step: 9
Training loss: 2.4117026329040527
Validation loss: 2.1491258041833037

Epoch: 6| Step: 10
Training loss: 2.061624526977539
Validation loss: 2.1552560124346005

Epoch: 6| Step: 11
Training loss: 2.343562602996826
Validation loss: 2.1490775718483874

Epoch: 6| Step: 12
Training loss: 2.1916699409484863
Validation loss: 2.1483066492183234

Epoch: 6| Step: 13
Training loss: 2.1467292308807373
Validation loss: 2.146110603886266

Epoch: 247| Step: 0
Training loss: 2.585570812225342
Validation loss: 2.170821189880371

Epoch: 6| Step: 1
Training loss: 1.4910495281219482
Validation loss: 2.1763398698581162

Epoch: 6| Step: 2
Training loss: 2.5942487716674805
Validation loss: 2.178756731812672

Epoch: 6| Step: 3
Training loss: 2.5092926025390625
Validation loss: 2.190791024956652

Epoch: 6| Step: 4
Training loss: 2.137075424194336
Validation loss: 2.197526393398162

Epoch: 6| Step: 5
Training loss: 2.7796082496643066
Validation loss: 2.185032226706064

Epoch: 6| Step: 6
Training loss: 2.3805620670318604
Validation loss: 2.185062272574312

Epoch: 6| Step: 7
Training loss: 2.210942029953003
Validation loss: 2.175762589259814

Epoch: 6| Step: 8
Training loss: 2.15934419631958
Validation loss: 2.1582992179419405

Epoch: 6| Step: 9
Training loss: 2.3573389053344727
Validation loss: 2.1502275441282537

Epoch: 6| Step: 10
Training loss: 2.564375400543213
Validation loss: 2.1358460508367068

Epoch: 6| Step: 11
Training loss: 1.7794675827026367
Validation loss: 2.150737057449997

Epoch: 6| Step: 12
Training loss: 2.1109211444854736
Validation loss: 2.1549240235359437

Epoch: 6| Step: 13
Training loss: 2.2923381328582764
Validation loss: 2.151148872990762

Epoch: 248| Step: 0
Training loss: 2.553288459777832
Validation loss: 2.143630991699875

Epoch: 6| Step: 1
Training loss: 2.2148330211639404
Validation loss: 2.1492377109425043

Epoch: 6| Step: 2
Training loss: 2.3972904682159424
Validation loss: 2.1428346608274724

Epoch: 6| Step: 3
Training loss: 2.3122949600219727
Validation loss: 2.1322274836160804

Epoch: 6| Step: 4
Training loss: 2.480337142944336
Validation loss: 2.142764129946309

Epoch: 6| Step: 5
Training loss: 2.2625479698181152
Validation loss: 2.139903614597936

Epoch: 6| Step: 6
Training loss: 2.70513916015625
Validation loss: 2.1411844504776822

Epoch: 6| Step: 7
Training loss: 2.4908499717712402
Validation loss: 2.15447614910782

Epoch: 6| Step: 8
Training loss: 2.528151035308838
Validation loss: 2.162576690796883

Epoch: 6| Step: 9
Training loss: 2.3202059268951416
Validation loss: 2.1573784594894736

Epoch: 6| Step: 10
Training loss: 1.7183146476745605
Validation loss: 2.135510020358588

Epoch: 6| Step: 11
Training loss: 2.2640860080718994
Validation loss: 2.139162991636543

Epoch: 6| Step: 12
Training loss: 1.4718517065048218
Validation loss: 2.120556490395659

Epoch: 6| Step: 13
Training loss: 2.2757186889648438
Validation loss: 2.1308508380766837

Epoch: 249| Step: 0
Training loss: 1.7139811515808105
Validation loss: 2.110573363560502

Epoch: 6| Step: 1
Training loss: 2.513214349746704
Validation loss: 2.1092956373768468

Epoch: 6| Step: 2
Training loss: 2.544290065765381
Validation loss: 2.1304258223502868

Epoch: 6| Step: 3
Training loss: 1.3876402378082275
Validation loss: 2.1156112891371532

Epoch: 6| Step: 4
Training loss: 2.075329065322876
Validation loss: 2.1372737410247966

Epoch: 6| Step: 5
Training loss: 2.184347152709961
Validation loss: 2.133995863699144

Epoch: 6| Step: 6
Training loss: 2.5551834106445312
Validation loss: 2.153611554894396

Epoch: 6| Step: 7
Training loss: 2.2446134090423584
Validation loss: 2.1499531897165443

Epoch: 6| Step: 8
Training loss: 2.3107805252075195
Validation loss: 2.1423425430892618

Epoch: 6| Step: 9
Training loss: 2.3622517585754395
Validation loss: 2.142068686023835

Epoch: 6| Step: 10
Training loss: 2.314659595489502
Validation loss: 2.142749519758327

Epoch: 6| Step: 11
Training loss: 2.450857400894165
Validation loss: 2.1371883423097673

Epoch: 6| Step: 12
Training loss: 2.6225497722625732
Validation loss: 2.1377233433467087

Epoch: 6| Step: 13
Training loss: 2.850698709487915
Validation loss: 2.1420816170272006

Epoch: 250| Step: 0
Training loss: 2.146101951599121
Validation loss: 2.137014704365884

Epoch: 6| Step: 1
Training loss: 2.0341312885284424
Validation loss: 2.13659066795021

Epoch: 6| Step: 2
Training loss: 1.2608075141906738
Validation loss: 2.1376039238386255

Epoch: 6| Step: 3
Training loss: 1.8291367292404175
Validation loss: 2.141537643248035

Epoch: 6| Step: 4
Training loss: 3.1186490058898926
Validation loss: 2.1427902047352125

Epoch: 6| Step: 5
Training loss: 3.168257236480713
Validation loss: 2.132672812349053

Epoch: 6| Step: 6
Training loss: 2.456127166748047
Validation loss: 2.1265376652440717

Epoch: 6| Step: 7
Training loss: 1.7541851997375488
Validation loss: 2.1298489801345335

Epoch: 6| Step: 8
Training loss: 2.327047348022461
Validation loss: 2.1165030566594933

Epoch: 6| Step: 9
Training loss: 2.5551095008850098
Validation loss: 2.118769112453666

Epoch: 6| Step: 10
Training loss: 1.8316044807434082
Validation loss: 2.122868387929855

Epoch: 6| Step: 11
Training loss: 2.6126275062561035
Validation loss: 2.1270234033625615

Epoch: 6| Step: 12
Training loss: 2.546074390411377
Validation loss: 2.129465518459197

Epoch: 6| Step: 13
Training loss: 1.9536993503570557
Validation loss: 2.1362173839281966

Testing loss: 2.362776290045844

Epoch: 1| Step: 0
Training loss: 5.072131633758545
Validation loss: 5.227394652623002

Epoch: 6| Step: 1
Training loss: 5.7989726066589355
Validation loss: 5.223448117574056

Epoch: 6| Step: 2
Training loss: 5.801570892333984
Validation loss: 5.219402600360173

Epoch: 6| Step: 3
Training loss: 4.040204048156738
Validation loss: 5.2152910027452695

Epoch: 6| Step: 4
Training loss: 4.1309814453125
Validation loss: 5.211202144622803

Epoch: 6| Step: 5
Training loss: 3.700942039489746
Validation loss: 5.207040750852195

Epoch: 6| Step: 6
Training loss: 5.35642147064209
Validation loss: 5.202951846584197

Epoch: 6| Step: 7
Training loss: 6.035784721374512
Validation loss: 5.199025256659395

Epoch: 6| Step: 8
Training loss: 4.485566139221191
Validation loss: 5.194803412242602

Epoch: 6| Step: 9
Training loss: 3.9392123222351074
Validation loss: 5.1909821007841375

Epoch: 6| Step: 10
Training loss: 4.53978157043457
Validation loss: 5.187011939223095

Epoch: 6| Step: 11
Training loss: 5.9746198654174805
Validation loss: 5.182535535545759

Epoch: 6| Step: 12
Training loss: 6.660678863525391
Validation loss: 5.178192953909597

Epoch: 6| Step: 13
Training loss: 3.9765870571136475
Validation loss: 5.17401700122382

Epoch: 2| Step: 0
Training loss: 4.076576232910156
Validation loss: 5.169096234024212

Epoch: 6| Step: 1
Training loss: 4.8960161209106445
Validation loss: 5.1639754438912995

Epoch: 6| Step: 2
Training loss: 5.047584533691406
Validation loss: 5.159012666312597

Epoch: 6| Step: 3
Training loss: 3.9144351482391357
Validation loss: 5.153529674776139

Epoch: 6| Step: 4
Training loss: 4.8233418464660645
Validation loss: 5.148315193832562

Epoch: 6| Step: 5
Training loss: 5.6411895751953125
Validation loss: 5.1422616281817035

Epoch: 6| Step: 6
Training loss: 5.059845924377441
Validation loss: 5.136267733830278

Epoch: 6| Step: 7
Training loss: 5.040748596191406
Validation loss: 5.129505516380392

Epoch: 6| Step: 8
Training loss: 5.473592758178711
Validation loss: 5.123299316693378

Epoch: 6| Step: 9
Training loss: 4.319071292877197
Validation loss: 5.11561099431848

Epoch: 6| Step: 10
Training loss: 5.724931716918945
Validation loss: 5.108429683152066

Epoch: 6| Step: 11
Training loss: 5.658899307250977
Validation loss: 5.100547759763656

Epoch: 6| Step: 12
Training loss: 4.99751091003418
Validation loss: 5.0924982665687475

Epoch: 6| Step: 13
Training loss: 3.800175666809082
Validation loss: 5.083175715579782

Epoch: 3| Step: 0
Training loss: 6.320526599884033
Validation loss: 5.074379033939813

Epoch: 6| Step: 1
Training loss: 5.307146072387695
Validation loss: 5.064891369112076

Epoch: 6| Step: 2
Training loss: 5.659974098205566
Validation loss: 5.05561436376264

Epoch: 6| Step: 3
Training loss: 4.306319236755371
Validation loss: 5.044404209301036

Epoch: 6| Step: 4
Training loss: 4.357100486755371
Validation loss: 5.033282438913981

Epoch: 6| Step: 5
Training loss: 5.3410515785217285
Validation loss: 5.021952931598951

Epoch: 6| Step: 6
Training loss: 4.563208103179932
Validation loss: 5.009737901790167

Epoch: 6| Step: 7
Training loss: 4.882451057434082
Validation loss: 4.997194895180323

Epoch: 6| Step: 8
Training loss: 3.4850525856018066
Validation loss: 4.983883519326487

Epoch: 6| Step: 9
Training loss: 4.286505222320557
Validation loss: 4.970241367176015

Epoch: 6| Step: 10
Training loss: 4.091078281402588
Validation loss: 4.95540807067707

Epoch: 6| Step: 11
Training loss: 4.236571311950684
Validation loss: 4.9411965083050475

Epoch: 6| Step: 12
Training loss: 4.913511276245117
Validation loss: 4.925685728749921

Epoch: 6| Step: 13
Training loss: 5.837963104248047
Validation loss: 4.909888585408528

Epoch: 4| Step: 0
Training loss: 3.9735958576202393
Validation loss: 4.89284433344359

Epoch: 6| Step: 1
Training loss: 4.689155578613281
Validation loss: 4.87745056357435

Epoch: 6| Step: 2
Training loss: 5.045175552368164
Validation loss: 4.858707207505421

Epoch: 6| Step: 3
Training loss: 5.644559383392334
Validation loss: 4.840005177323536

Epoch: 6| Step: 4
Training loss: 3.86344051361084
Validation loss: 4.822068511798817

Epoch: 6| Step: 5
Training loss: 5.0549139976501465
Validation loss: 4.803449484609788

Epoch: 6| Step: 6
Training loss: 3.739758014678955
Validation loss: 4.7846214694361535

Epoch: 6| Step: 7
Training loss: 4.6866374015808105
Validation loss: 4.763033395172448

Epoch: 6| Step: 8
Training loss: 4.083833694458008
Validation loss: 4.743635449358212

Epoch: 6| Step: 9
Training loss: 4.747170448303223
Validation loss: 4.723981703481367

Epoch: 6| Step: 10
Training loss: 5.436947822570801
Validation loss: 4.702864539238714

Epoch: 6| Step: 11
Training loss: 5.334994316101074
Validation loss: 4.682873120871923

Epoch: 6| Step: 12
Training loss: 2.7678303718566895
Validation loss: 4.662211100260417

Epoch: 6| Step: 13
Training loss: 4.928347587585449
Validation loss: 4.642042949635496

Epoch: 5| Step: 0
Training loss: 4.880016326904297
Validation loss: 4.620900764260241

Epoch: 6| Step: 1
Training loss: 3.5204620361328125
Validation loss: 4.600339535743959

Epoch: 6| Step: 2
Training loss: 4.5548095703125
Validation loss: 4.57963175927439

Epoch: 6| Step: 3
Training loss: 5.400973320007324
Validation loss: 4.559518042431082

Epoch: 6| Step: 4
Training loss: 4.405307769775391
Validation loss: 4.539038914506153

Epoch: 6| Step: 5
Training loss: 4.802371978759766
Validation loss: 4.517664273579915

Epoch: 6| Step: 6
Training loss: 5.1390380859375
Validation loss: 4.496754197664158

Epoch: 6| Step: 7
Training loss: 3.614732265472412
Validation loss: 4.477256231410529

Epoch: 6| Step: 8
Training loss: 3.0727434158325195
Validation loss: 4.4563140766595

Epoch: 6| Step: 9
Training loss: 4.301339149475098
Validation loss: 4.438148649789953

Epoch: 6| Step: 10
Training loss: 3.9533634185791016
Validation loss: 4.416634539122223

Epoch: 6| Step: 11
Training loss: 3.0913262367248535
Validation loss: 4.398449420928955

Epoch: 6| Step: 12
Training loss: 4.197007656097412
Validation loss: 4.379718877935923

Epoch: 6| Step: 13
Training loss: 5.629182815551758
Validation loss: 4.360475278669788

Epoch: 6| Step: 0
Training loss: 4.002838134765625
Validation loss: 4.34421460346509

Epoch: 6| Step: 1
Training loss: 4.29792594909668
Validation loss: 4.324253959040488

Epoch: 6| Step: 2
Training loss: 2.904285192489624
Validation loss: 4.308170349367203

Epoch: 6| Step: 3
Training loss: 4.622518539428711
Validation loss: 4.291227515025805

Epoch: 6| Step: 4
Training loss: 4.273742198944092
Validation loss: 4.27354129668205

Epoch: 6| Step: 5
Training loss: 3.6390655040740967
Validation loss: 4.258472478517922

Epoch: 6| Step: 6
Training loss: 4.017578601837158
Validation loss: 4.243115512273645

Epoch: 6| Step: 7
Training loss: 5.413407802581787
Validation loss: 4.227231415369177

Epoch: 6| Step: 8
Training loss: 4.666118621826172
Validation loss: 4.212001897955454

Epoch: 6| Step: 9
Training loss: 4.459311485290527
Validation loss: 4.199541163700883

Epoch: 6| Step: 10
Training loss: 3.070495128631592
Validation loss: 4.184347983329527

Epoch: 6| Step: 11
Training loss: 3.7599270343780518
Validation loss: 4.169063209205546

Epoch: 6| Step: 12
Training loss: 4.125043869018555
Validation loss: 4.153881031979797

Epoch: 6| Step: 13
Training loss: 3.0464842319488525
Validation loss: 4.13778664476128

Epoch: 7| Step: 0
Training loss: 4.177569389343262
Validation loss: 4.12481503845543

Epoch: 6| Step: 1
Training loss: 3.329181671142578
Validation loss: 4.107869189272645

Epoch: 6| Step: 2
Training loss: 4.708666801452637
Validation loss: 4.095247637841009

Epoch: 6| Step: 3
Training loss: 4.326570510864258
Validation loss: 4.082147921285322

Epoch: 6| Step: 4
Training loss: 2.6153793334960938
Validation loss: 4.0698857461252524

Epoch: 6| Step: 5
Training loss: 4.412814617156982
Validation loss: 4.057043034543273

Epoch: 6| Step: 6
Training loss: 3.7711520195007324
Validation loss: 4.0435903943995

Epoch: 6| Step: 7
Training loss: 4.845803260803223
Validation loss: 4.033106247584025

Epoch: 6| Step: 8
Training loss: 2.363875150680542
Validation loss: 4.018514874160931

Epoch: 6| Step: 9
Training loss: 4.076320648193359
Validation loss: 4.005459570115613

Epoch: 6| Step: 10
Training loss: 3.241013765335083
Validation loss: 3.9922690135176464

Epoch: 6| Step: 11
Training loss: 5.161693572998047
Validation loss: 3.9821084750595914

Epoch: 6| Step: 12
Training loss: 3.7393040657043457
Validation loss: 3.9686208284029396

Epoch: 6| Step: 13
Training loss: 3.296605110168457
Validation loss: 3.957049059611495

Epoch: 8| Step: 0
Training loss: 5.727909088134766
Validation loss: 3.9466931794279363

Epoch: 6| Step: 1
Training loss: 2.196312427520752
Validation loss: 3.9331867720491145

Epoch: 6| Step: 2
Training loss: 4.128477096557617
Validation loss: 3.9223754611066592

Epoch: 6| Step: 3
Training loss: 4.554880142211914
Validation loss: 3.909875172440724

Epoch: 6| Step: 4
Training loss: 3.608001232147217
Validation loss: 3.8974285535914923

Epoch: 6| Step: 5
Training loss: 3.3932571411132812
Validation loss: 3.883718175272788

Epoch: 6| Step: 6
Training loss: 3.927915573120117
Validation loss: 3.8737817323336037

Epoch: 6| Step: 7
Training loss: 4.035624027252197
Validation loss: 3.8621999576527584

Epoch: 6| Step: 8
Training loss: 4.384475231170654
Validation loss: 3.849363004007647

Epoch: 6| Step: 9
Training loss: 3.4328694343566895
Validation loss: 3.83894597330401

Epoch: 6| Step: 10
Training loss: 2.824836254119873
Validation loss: 3.827299705115698

Epoch: 6| Step: 11
Training loss: 3.508965253829956
Validation loss: 3.8169396128705753

Epoch: 6| Step: 12
Training loss: 3.153043270111084
Validation loss: 3.8056740658257597

Epoch: 6| Step: 13
Training loss: 3.4198720455169678
Validation loss: 3.7940465481050554

Epoch: 9| Step: 0
Training loss: 4.374795913696289
Validation loss: 3.7852285344113588

Epoch: 6| Step: 1
Training loss: 2.561136484146118
Validation loss: 3.776026077167962

Epoch: 6| Step: 2
Training loss: 5.3022780418396
Validation loss: 3.7672835652546217

Epoch: 6| Step: 3
Training loss: 3.2477447986602783
Validation loss: 3.758499219853391

Epoch: 6| Step: 4
Training loss: 3.5717759132385254
Validation loss: 3.7493478303314536

Epoch: 6| Step: 5
Training loss: 3.786468744277954
Validation loss: 3.7401116817228255

Epoch: 6| Step: 6
Training loss: 3.184441566467285
Validation loss: 3.7288341701671643

Epoch: 6| Step: 7
Training loss: 3.2561850547790527
Validation loss: 3.721285004769602

Epoch: 6| Step: 8
Training loss: 3.9729065895080566
Validation loss: 3.71414606032833

Epoch: 6| Step: 9
Training loss: 3.9932074546813965
Validation loss: 3.70870638919133

Epoch: 6| Step: 10
Training loss: 3.6385064125061035
Validation loss: 3.6998151502301617

Epoch: 6| Step: 11
Training loss: 3.0128791332244873
Validation loss: 3.6911769246542327

Epoch: 6| Step: 12
Training loss: 3.3610153198242188
Validation loss: 3.683368641843078

Epoch: 6| Step: 13
Training loss: 3.4183456897735596
Validation loss: 3.6752517864268315

Epoch: 10| Step: 0
Training loss: 4.563445091247559
Validation loss: 3.666437525903025

Epoch: 6| Step: 1
Training loss: 3.9079508781433105
Validation loss: 3.661378788691695

Epoch: 6| Step: 2
Training loss: 3.245443105697632
Validation loss: 3.652760046784596

Epoch: 6| Step: 3
Training loss: 3.815518856048584
Validation loss: 3.646586133587745

Epoch: 6| Step: 4
Training loss: 3.023275852203369
Validation loss: 3.6401995202546478

Epoch: 6| Step: 5
Training loss: 3.0648653507232666
Validation loss: 3.6290377622009604

Epoch: 6| Step: 6
Training loss: 3.9291810989379883
Validation loss: 3.6156542762633292

Epoch: 6| Step: 7
Training loss: 3.335148811340332
Validation loss: 3.607693241488549

Epoch: 6| Step: 8
Training loss: 3.3213071823120117
Validation loss: 3.6005563325779413

Epoch: 6| Step: 9
Training loss: 3.7060797214508057
Validation loss: 3.592162809064311

Epoch: 6| Step: 10
Training loss: 4.741085529327393
Validation loss: 3.5833259808119906

Epoch: 6| Step: 11
Training loss: 2.4427740573883057
Validation loss: 3.577696651540777

Epoch: 6| Step: 12
Training loss: 2.4690990447998047
Validation loss: 3.568742646965929

Epoch: 6| Step: 13
Training loss: 4.093733787536621
Validation loss: 3.5600395151363906

Epoch: 11| Step: 0
Training loss: 4.529425621032715
Validation loss: 3.5533431678689937

Epoch: 6| Step: 1
Training loss: 2.222398519515991
Validation loss: 3.5458487079989527

Epoch: 6| Step: 2
Training loss: 3.921203851699829
Validation loss: 3.5360388807071153

Epoch: 6| Step: 3
Training loss: 3.194420576095581
Validation loss: 3.528532338398759

Epoch: 6| Step: 4
Training loss: 2.2392120361328125
Validation loss: 3.520535812583021

Epoch: 6| Step: 5
Training loss: 3.956237316131592
Validation loss: 3.510421778566094

Epoch: 6| Step: 6
Training loss: 4.341951370239258
Validation loss: 3.499001415826941

Epoch: 6| Step: 7
Training loss: 2.8203232288360596
Validation loss: 3.4938339981981503

Epoch: 6| Step: 8
Training loss: 3.8494246006011963
Validation loss: 3.482160929710634

Epoch: 6| Step: 9
Training loss: 4.550806999206543
Validation loss: 3.4749388617853962

Epoch: 6| Step: 10
Training loss: 3.1423182487487793
Validation loss: 3.4680528256200973

Epoch: 6| Step: 11
Training loss: 2.8139474391937256
Validation loss: 3.457242247878864

Epoch: 6| Step: 12
Training loss: 2.2690272331237793
Validation loss: 3.4544760155421432

Epoch: 6| Step: 13
Training loss: 4.877463340759277
Validation loss: 3.4470978065203597

Epoch: 12| Step: 0
Training loss: 3.816319227218628
Validation loss: 3.4394736648887716

Epoch: 6| Step: 1
Training loss: 3.514026165008545
Validation loss: 3.429912333847374

Epoch: 6| Step: 2
Training loss: 4.356003284454346
Validation loss: 3.4213444853341706

Epoch: 6| Step: 3
Training loss: 3.9785847663879395
Validation loss: 3.4153712564899075

Epoch: 6| Step: 4
Training loss: 3.214189291000366
Validation loss: 3.4074357068666847

Epoch: 6| Step: 5
Training loss: 3.3967437744140625
Validation loss: 3.4006666573145057

Epoch: 6| Step: 6
Training loss: 3.3369336128234863
Validation loss: 3.3933018894605738

Epoch: 6| Step: 7
Training loss: 3.137502670288086
Validation loss: 3.3909732910894577

Epoch: 6| Step: 8
Training loss: 3.004187822341919
Validation loss: 3.3851233707961215

Epoch: 6| Step: 9
Training loss: 3.0036919116973877
Validation loss: 3.375480995383314

Epoch: 6| Step: 10
Training loss: 3.115697145462036
Validation loss: 3.3726417761977

Epoch: 6| Step: 11
Training loss: 3.005067825317383
Validation loss: 3.365868894002771

Epoch: 6| Step: 12
Training loss: 2.3005754947662354
Validation loss: 3.359556044301679

Epoch: 6| Step: 13
Training loss: 4.006566524505615
Validation loss: 3.354195248696112

Epoch: 13| Step: 0
Training loss: 4.001138687133789
Validation loss: 3.3483951194311983

Epoch: 6| Step: 1
Training loss: 2.739193916320801
Validation loss: 3.3416432513985583

Epoch: 6| Step: 2
Training loss: 2.98225736618042
Validation loss: 3.3379960008846816

Epoch: 6| Step: 3
Training loss: 3.302527904510498
Validation loss: 3.335319137060514

Epoch: 6| Step: 4
Training loss: 2.920198917388916
Validation loss: 3.329112873282484

Epoch: 6| Step: 5
Training loss: 3.406708240509033
Validation loss: 3.322443316059728

Epoch: 6| Step: 6
Training loss: 4.129615783691406
Validation loss: 3.3239502906799316

Epoch: 6| Step: 7
Training loss: 3.023056983947754
Validation loss: 3.3184772973419516

Epoch: 6| Step: 8
Training loss: 3.196519374847412
Validation loss: 3.3082357016942834

Epoch: 6| Step: 9
Training loss: 3.5789310932159424
Validation loss: 3.302831103724818

Epoch: 6| Step: 10
Training loss: 2.83152174949646
Validation loss: 3.2974493298479306

Epoch: 6| Step: 11
Training loss: 3.8819503784179688
Validation loss: 3.2950809745378393

Epoch: 6| Step: 12
Training loss: 3.0495238304138184
Validation loss: 3.291374070670015

Epoch: 6| Step: 13
Training loss: 2.5786852836608887
Validation loss: 3.287204429667483

Epoch: 14| Step: 0
Training loss: 3.392874002456665
Validation loss: 3.2813471978710544

Epoch: 6| Step: 1
Training loss: 3.099687337875366
Validation loss: 3.277880891676872

Epoch: 6| Step: 2
Training loss: 3.5588314533233643
Validation loss: 3.272716529907719

Epoch: 6| Step: 3
Training loss: 3.9495127201080322
Validation loss: 3.269305295841668

Epoch: 6| Step: 4
Training loss: 2.8765335083007812
Validation loss: 3.2635042590479695

Epoch: 6| Step: 5
Training loss: 3.198925495147705
Validation loss: 3.2587203543673278

Epoch: 6| Step: 6
Training loss: 3.3338847160339355
Validation loss: 3.253792337192002

Epoch: 6| Step: 7
Training loss: 2.2545852661132812
Validation loss: 3.251535971959432

Epoch: 6| Step: 8
Training loss: 3.142256021499634
Validation loss: 3.2506060292643886

Epoch: 6| Step: 9
Training loss: 4.040230751037598
Validation loss: 3.253523667653402

Epoch: 6| Step: 10
Training loss: 3.181133270263672
Validation loss: 3.245292676392422

Epoch: 6| Step: 11
Training loss: 3.363835573196411
Validation loss: 3.236618834157144

Epoch: 6| Step: 12
Training loss: 2.439530611038208
Validation loss: 3.234477109806512

Epoch: 6| Step: 13
Training loss: 3.7165963649749756
Validation loss: 3.2321476756885485

Epoch: 15| Step: 0
Training loss: 3.3057847023010254
Validation loss: 3.232409497743012

Epoch: 6| Step: 1
Training loss: 2.4697928428649902
Validation loss: 3.2308832701816352

Epoch: 6| Step: 2
Training loss: 3.426079750061035
Validation loss: 3.2274921453127297

Epoch: 6| Step: 3
Training loss: 3.644033908843994
Validation loss: 3.2215512644860054

Epoch: 6| Step: 4
Training loss: 2.961899518966675
Validation loss: 3.2186726652165896

Epoch: 6| Step: 5
Training loss: 3.2987170219421387
Validation loss: 3.2141850635569584

Epoch: 6| Step: 6
Training loss: 2.9231505393981934
Validation loss: 3.2170074139871905

Epoch: 6| Step: 7
Training loss: 3.5042073726654053
Validation loss: 3.217570315125168

Epoch: 6| Step: 8
Training loss: 2.933138847351074
Validation loss: 3.212295914209017

Epoch: 6| Step: 9
Training loss: 2.966198444366455
Validation loss: 3.2044912102401897

Epoch: 6| Step: 10
Training loss: 4.653683662414551
Validation loss: 3.1999665844825005

Epoch: 6| Step: 11
Training loss: 2.6574292182922363
Validation loss: 3.1962391432895454

Epoch: 6| Step: 12
Training loss: 2.8795933723449707
Validation loss: 3.1938519631662676

Epoch: 6| Step: 13
Training loss: 3.274359703063965
Validation loss: 3.1985687055895404

Epoch: 16| Step: 0
Training loss: 3.2556467056274414
Validation loss: 3.208596788426881

Epoch: 6| Step: 1
Training loss: 3.7950212955474854
Validation loss: 3.2027765858557915

Epoch: 6| Step: 2
Training loss: 3.37628173828125
Validation loss: 3.1973698626282396

Epoch: 6| Step: 3
Training loss: 3.226533889770508
Validation loss: 3.1865228222262476

Epoch: 6| Step: 4
Training loss: 3.4782421588897705
Validation loss: 3.1776916288560435

Epoch: 6| Step: 5
Training loss: 2.372464895248413
Validation loss: 3.1775102640992854

Epoch: 6| Step: 6
Training loss: 2.8929176330566406
Validation loss: 3.176007460522395

Epoch: 6| Step: 7
Training loss: 4.119889259338379
Validation loss: 3.1763276207831597

Epoch: 6| Step: 8
Training loss: 3.364114761352539
Validation loss: 3.1807494932605374

Epoch: 6| Step: 9
Training loss: 2.615363121032715
Validation loss: 3.1763064604933544

Epoch: 6| Step: 10
Training loss: 2.840806484222412
Validation loss: 3.1755008415509294

Epoch: 6| Step: 11
Training loss: 3.6302249431610107
Validation loss: 3.169710172119961

Epoch: 6| Step: 12
Training loss: 2.473621368408203
Validation loss: 3.1636650357195126

Epoch: 6| Step: 13
Training loss: 3.038120746612549
Validation loss: 3.1596891777489775

Epoch: 17| Step: 0
Training loss: 3.8703789710998535
Validation loss: 3.154485353859522

Epoch: 6| Step: 1
Training loss: 3.1196236610412598
Validation loss: 3.149460043958438

Epoch: 6| Step: 2
Training loss: 2.6328587532043457
Validation loss: 3.152282953262329

Epoch: 6| Step: 3
Training loss: 2.588855266571045
Validation loss: 3.150079822027555

Epoch: 6| Step: 4
Training loss: 3.2071948051452637
Validation loss: 3.1458676117722706

Epoch: 6| Step: 5
Training loss: 3.7263097763061523
Validation loss: 3.1454918282006377

Epoch: 6| Step: 6
Training loss: 2.750166416168213
Validation loss: 3.1438881812557096

Epoch: 6| Step: 7
Training loss: 3.639566421508789
Validation loss: 3.139505858062416

Epoch: 6| Step: 8
Training loss: 3.388324499130249
Validation loss: 3.1401984973620345

Epoch: 6| Step: 9
Training loss: 2.5134897232055664
Validation loss: 3.136248201452276

Epoch: 6| Step: 10
Training loss: 2.4072234630584717
Validation loss: 3.1341884597655265

Epoch: 6| Step: 11
Training loss: 4.338483810424805
Validation loss: 3.1284230063038487

Epoch: 6| Step: 12
Training loss: 3.589085578918457
Validation loss: 3.124635919447868

Epoch: 6| Step: 13
Training loss: 1.7927613258361816
Validation loss: 3.1155247226838143

Epoch: 18| Step: 0
Training loss: 3.6515374183654785
Validation loss: 3.120930851146739

Epoch: 6| Step: 1
Training loss: 3.2561416625976562
Validation loss: 3.125005086263021

Epoch: 6| Step: 2
Training loss: 2.336805582046509
Validation loss: 3.1356075604756675

Epoch: 6| Step: 3
Training loss: 2.241453170776367
Validation loss: 3.1310106118520102

Epoch: 6| Step: 4
Training loss: 3.0615739822387695
Validation loss: 3.117292865630119

Epoch: 6| Step: 5
Training loss: 3.5457284450531006
Validation loss: 3.1069744069089174

Epoch: 6| Step: 6
Training loss: 2.7414357662200928
Validation loss: 3.099932357829104

Epoch: 6| Step: 7
Training loss: 2.7344048023223877
Validation loss: 3.095701038196523

Epoch: 6| Step: 8
Training loss: 3.3326916694641113
Validation loss: 3.0942262372662945

Epoch: 6| Step: 9
Training loss: 2.6010422706604004
Validation loss: 3.097300201333979

Epoch: 6| Step: 10
Training loss: 4.064154148101807
Validation loss: 3.0922118925279185

Epoch: 6| Step: 11
Training loss: 4.804024696350098
Validation loss: 3.091383139292399

Epoch: 6| Step: 12
Training loss: 2.6378941535949707
Validation loss: 3.09029245632951

Epoch: 6| Step: 13
Training loss: 2.5311529636383057
Validation loss: 3.0852505955644833

Epoch: 19| Step: 0
Training loss: 2.727832794189453
Validation loss: 3.0867578598760788

Epoch: 6| Step: 1
Training loss: 3.0421860218048096
Validation loss: 3.0872790454536356

Epoch: 6| Step: 2
Training loss: 3.5891213417053223
Validation loss: 3.0801167641916583

Epoch: 6| Step: 3
Training loss: 3.2986133098602295
Validation loss: 3.0807934961011334

Epoch: 6| Step: 4
Training loss: 3.2879509925842285
Validation loss: 3.076903550855575

Epoch: 6| Step: 5
Training loss: 3.2841196060180664
Validation loss: 3.0768793680334605

Epoch: 6| Step: 6
Training loss: 2.4619698524475098
Validation loss: 3.07650311018831

Epoch: 6| Step: 7
Training loss: 2.786329507827759
Validation loss: 3.076906568260603

Epoch: 6| Step: 8
Training loss: 3.869196891784668
Validation loss: 3.080050014680432

Epoch: 6| Step: 9
Training loss: 2.518803358078003
Validation loss: 3.070729829931772

Epoch: 6| Step: 10
Training loss: 3.8638927936553955
Validation loss: 3.0659255878899687

Epoch: 6| Step: 11
Training loss: 3.036133050918579
Validation loss: 3.0617790939987346

Epoch: 6| Step: 12
Training loss: 2.3848018646240234
Validation loss: 3.0595623472685456

Epoch: 6| Step: 13
Training loss: 3.5123167037963867
Validation loss: 3.06012221049237

Epoch: 20| Step: 0
Training loss: 3.1952502727508545
Validation loss: 3.0578599463226976

Epoch: 6| Step: 1
Training loss: 3.057098627090454
Validation loss: 3.053474267323812

Epoch: 6| Step: 2
Training loss: 3.275482654571533
Validation loss: 3.0483287816406577

Epoch: 6| Step: 3
Training loss: 3.9587340354919434
Validation loss: 3.0508783273799445

Epoch: 6| Step: 4
Training loss: 2.6609182357788086
Validation loss: 3.053686821332542

Epoch: 6| Step: 5
Training loss: 3.8061740398406982
Validation loss: 3.0486138584793254

Epoch: 6| Step: 6
Training loss: 2.272731065750122
Validation loss: 3.0493998373708417

Epoch: 6| Step: 7
Training loss: 3.0042805671691895
Validation loss: 3.0475296717818066

Epoch: 6| Step: 8
Training loss: 2.2238874435424805
Validation loss: 3.0515180762096117

Epoch: 6| Step: 9
Training loss: 3.0118837356567383
Validation loss: 3.044344053473524

Epoch: 6| Step: 10
Training loss: 3.263388156890869
Validation loss: 3.0412342676552395

Epoch: 6| Step: 11
Training loss: 4.798002243041992
Validation loss: 3.0397830599097797

Epoch: 6| Step: 12
Training loss: 1.936667561531067
Validation loss: 3.036760007181475

Epoch: 6| Step: 13
Training loss: 2.59120774269104
Validation loss: 3.033020004149406

Epoch: 21| Step: 0
Training loss: 3.1141417026519775
Validation loss: 3.034711527568038

Epoch: 6| Step: 1
Training loss: 3.297085762023926
Validation loss: 3.037685978797174

Epoch: 6| Step: 2
Training loss: 3.8073415756225586
Validation loss: 3.0313321313550396

Epoch: 6| Step: 3
Training loss: 2.5818209648132324
Validation loss: 3.0322152286447506

Epoch: 6| Step: 4
Training loss: 4.016351222991943
Validation loss: 3.0265915240010908

Epoch: 6| Step: 5
Training loss: 3.674834728240967
Validation loss: 3.031677281984719

Epoch: 6| Step: 6
Training loss: 2.4349567890167236
Validation loss: 3.025913110343359

Epoch: 6| Step: 7
Training loss: 2.5934383869171143
Validation loss: 3.0290600099871234

Epoch: 6| Step: 8
Training loss: 3.0288918018341064
Validation loss: 3.0367175302197857

Epoch: 6| Step: 9
Training loss: 2.8685712814331055
Validation loss: 3.0505748615469983

Epoch: 6| Step: 10
Training loss: 2.4834117889404297
Validation loss: 3.0318765999168478

Epoch: 6| Step: 11
Training loss: 2.978533983230591
Validation loss: 3.018767972146311

Epoch: 6| Step: 12
Training loss: 3.5865588188171387
Validation loss: 3.016812998761413

Epoch: 6| Step: 13
Training loss: 2.277057647705078
Validation loss: 3.0117197293107227

Epoch: 22| Step: 0
Training loss: 4.05549955368042
Validation loss: 3.0136522964764665

Epoch: 6| Step: 1
Training loss: 2.939387321472168
Validation loss: 3.011208841877599

Epoch: 6| Step: 2
Training loss: 3.2666268348693848
Validation loss: 3.0109442510912494

Epoch: 6| Step: 3
Training loss: 3.3999075889587402
Validation loss: 3.009239314704813

Epoch: 6| Step: 4
Training loss: 3.8697876930236816
Validation loss: 3.0060194615394837

Epoch: 6| Step: 5
Training loss: 3.702087640762329
Validation loss: 3.001047526636431

Epoch: 6| Step: 6
Training loss: 3.6194210052490234
Validation loss: 2.9949709164199008

Epoch: 6| Step: 7
Training loss: 2.9446425437927246
Validation loss: 2.996081316342918

Epoch: 6| Step: 8
Training loss: 2.2276527881622314
Validation loss: 2.995694475789224

Epoch: 6| Step: 9
Training loss: 3.196046829223633
Validation loss: 2.992772102355957

Epoch: 6| Step: 10
Training loss: 1.6816201210021973
Validation loss: 2.9913656250123055

Epoch: 6| Step: 11
Training loss: 1.7915797233581543
Validation loss: 2.9996404750372774

Epoch: 6| Step: 12
Training loss: 3.2311477661132812
Validation loss: 3.009217431468348

Epoch: 6| Step: 13
Training loss: 2.9243857860565186
Validation loss: 3.0222604479841007

Epoch: 23| Step: 0
Training loss: 2.7223801612854004
Validation loss: 3.0388571985306276

Epoch: 6| Step: 1
Training loss: 3.676362991333008
Validation loss: 3.0350178569875736

Epoch: 6| Step: 2
Training loss: 3.3721237182617188
Validation loss: 3.000310556862944

Epoch: 6| Step: 3
Training loss: 2.819418430328369
Validation loss: 2.9857031837586434

Epoch: 6| Step: 4
Training loss: 3.0951902866363525
Validation loss: 2.979986136959445

Epoch: 6| Step: 5
Training loss: 3.010371685028076
Validation loss: 2.992880464881979

Epoch: 6| Step: 6
Training loss: 2.7091522216796875
Validation loss: 2.9988341690391622

Epoch: 6| Step: 7
Training loss: 2.165371894836426
Validation loss: 3.0026432032226236

Epoch: 6| Step: 8
Training loss: 3.534932851791382
Validation loss: 3.0070622326225362

Epoch: 6| Step: 9
Training loss: 2.984860897064209
Validation loss: 2.999953909586835

Epoch: 6| Step: 10
Training loss: 3.196640968322754
Validation loss: 2.9963850616126932

Epoch: 6| Step: 11
Training loss: 3.9560067653656006
Validation loss: 2.9871495564778647

Epoch: 6| Step: 12
Training loss: 2.8031349182128906
Validation loss: 2.9792238076527915

Epoch: 6| Step: 13
Training loss: 2.6516811847686768
Validation loss: 2.9727805352980092

Epoch: 24| Step: 0
Training loss: 3.4221205711364746
Validation loss: 2.965869449800061

Epoch: 6| Step: 1
Training loss: 3.287682056427002
Validation loss: 2.991072993124685

Epoch: 6| Step: 2
Training loss: 3.181506633758545
Validation loss: 2.9614259094320317

Epoch: 6| Step: 3
Training loss: 3.0032036304473877
Validation loss: 2.9624181716672835

Epoch: 6| Step: 4
Training loss: 2.9988698959350586
Validation loss: 2.9683378383677494

Epoch: 6| Step: 5
Training loss: 2.8935697078704834
Validation loss: 2.967448567831388

Epoch: 6| Step: 6
Training loss: 3.73878812789917
Validation loss: 2.9710127845887215

Epoch: 6| Step: 7
Training loss: 2.5640337467193604
Validation loss: 2.9682603292567755

Epoch: 6| Step: 8
Training loss: 2.603788137435913
Validation loss: 2.9674147047022337

Epoch: 6| Step: 9
Training loss: 2.89275860786438
Validation loss: 2.9711107643701697

Epoch: 6| Step: 10
Training loss: 2.5220537185668945
Validation loss: 2.969119769270702

Epoch: 6| Step: 11
Training loss: 2.9172778129577637
Validation loss: 2.970154272612705

Epoch: 6| Step: 12
Training loss: 2.780181646347046
Validation loss: 2.9602059907810663

Epoch: 6| Step: 13
Training loss: 4.203184127807617
Validation loss: 2.964384501980197

Epoch: 25| Step: 0
Training loss: 2.8221516609191895
Validation loss: 2.960774052527643

Epoch: 6| Step: 1
Training loss: 3.049002170562744
Validation loss: 2.9597059398569088

Epoch: 6| Step: 2
Training loss: 3.994124412536621
Validation loss: 2.9571156809406896

Epoch: 6| Step: 3
Training loss: 2.9501402378082275
Validation loss: 2.952233350405129

Epoch: 6| Step: 4
Training loss: 2.915534496307373
Validation loss: 2.9503724728861163

Epoch: 6| Step: 5
Training loss: 2.653501033782959
Validation loss: 2.948044205224642

Epoch: 6| Step: 6
Training loss: 4.010862350463867
Validation loss: 2.9420845329120593

Epoch: 6| Step: 7
Training loss: 2.8211727142333984
Validation loss: 2.9466459058946177

Epoch: 6| Step: 8
Training loss: 2.6705753803253174
Validation loss: 2.940424152599868

Epoch: 6| Step: 9
Training loss: 3.6150777339935303
Validation loss: 2.941660036322891

Epoch: 6| Step: 10
Training loss: 2.685737133026123
Validation loss: 2.937456751382479

Epoch: 6| Step: 11
Training loss: 3.2026073932647705
Validation loss: 2.9380827847347466

Epoch: 6| Step: 12
Training loss: 2.353104591369629
Validation loss: 2.934780602814049

Epoch: 6| Step: 13
Training loss: 2.2437551021575928
Validation loss: 2.9329118318455194

Epoch: 26| Step: 0
Training loss: 3.279916524887085
Validation loss: 2.934468315493676

Epoch: 6| Step: 1
Training loss: 3.4084417819976807
Validation loss: 2.9348359389971663

Epoch: 6| Step: 2
Training loss: 2.53674054145813
Validation loss: 2.931781115070466

Epoch: 6| Step: 3
Training loss: 3.5339622497558594
Validation loss: 2.9266302303601335

Epoch: 6| Step: 4
Training loss: 2.0108108520507812
Validation loss: 2.9313481700035835

Epoch: 6| Step: 5
Training loss: 2.458555221557617
Validation loss: 2.926492947404103

Epoch: 6| Step: 6
Training loss: 3.541306495666504
Validation loss: 2.92959778283232

Epoch: 6| Step: 7
Training loss: 3.7163186073303223
Validation loss: 2.928548676993257

Epoch: 6| Step: 8
Training loss: 3.029844045639038
Validation loss: 2.92944061627952

Epoch: 6| Step: 9
Training loss: 3.410630464553833
Validation loss: 2.9320537505611295

Epoch: 6| Step: 10
Training loss: 2.777360439300537
Validation loss: 2.9272264972809823

Epoch: 6| Step: 11
Training loss: 2.292402744293213
Validation loss: 2.9238118817729335

Epoch: 6| Step: 12
Training loss: 2.8935465812683105
Validation loss: 2.9232197371862267

Epoch: 6| Step: 13
Training loss: 3.4688963890075684
Validation loss: 2.9203127225240073

Epoch: 27| Step: 0
Training loss: 2.6351842880249023
Validation loss: 2.923748746995003

Epoch: 6| Step: 1
Training loss: 2.400050163269043
Validation loss: 2.9165879372627503

Epoch: 6| Step: 2
Training loss: 3.544630527496338
Validation loss: 2.9205270198083695

Epoch: 6| Step: 3
Training loss: 1.9172542095184326
Validation loss: 2.9156078087386263

Epoch: 6| Step: 4
Training loss: 2.9874649047851562
Validation loss: 2.9188444742592434

Epoch: 6| Step: 5
Training loss: 3.4853928089141846
Validation loss: 2.9219277392151537

Epoch: 6| Step: 6
Training loss: 3.271374464035034
Validation loss: 2.9138628718673543

Epoch: 6| Step: 7
Training loss: 2.7570345401763916
Validation loss: 2.9178686757241525

Epoch: 6| Step: 8
Training loss: 2.906280040740967
Validation loss: 2.9139394144858084

Epoch: 6| Step: 9
Training loss: 2.8788228034973145
Validation loss: 2.9147247268307592

Epoch: 6| Step: 10
Training loss: 2.277376413345337
Validation loss: 2.9143862596122165

Epoch: 6| Step: 11
Training loss: 3.8223814964294434
Validation loss: 2.908500973896314

Epoch: 6| Step: 12
Training loss: 4.231223106384277
Validation loss: 2.9115663420769478

Epoch: 6| Step: 13
Training loss: 2.8719544410705566
Validation loss: 2.909223443718367

Epoch: 28| Step: 0
Training loss: 3.1218886375427246
Validation loss: 2.911765734354655

Epoch: 6| Step: 1
Training loss: 2.28092360496521
Validation loss: 2.9067642970751693

Epoch: 6| Step: 2
Training loss: 2.5449423789978027
Validation loss: 2.9060023830782984

Epoch: 6| Step: 3
Training loss: 3.1906473636627197
Validation loss: 2.9058946870988414

Epoch: 6| Step: 4
Training loss: 2.739454746246338
Validation loss: 2.903452891175465

Epoch: 6| Step: 5
Training loss: 3.023937225341797
Validation loss: 2.9048821900480535

Epoch: 6| Step: 6
Training loss: 2.584796190261841
Validation loss: 2.9014344241029475

Epoch: 6| Step: 7
Training loss: 4.550999641418457
Validation loss: 2.9059177239735923

Epoch: 6| Step: 8
Training loss: 3.4067225456237793
Validation loss: 2.902086837317354

Epoch: 6| Step: 9
Training loss: 3.2836012840270996
Validation loss: 2.905823089743173

Epoch: 6| Step: 10
Training loss: 3.419954776763916
Validation loss: 2.9035171975371656

Epoch: 6| Step: 11
Training loss: 2.1532223224639893
Validation loss: 2.898707010412729

Epoch: 6| Step: 12
Training loss: 2.938169479370117
Validation loss: 2.8980748371411393

Epoch: 6| Step: 13
Training loss: 2.4433977603912354
Validation loss: 2.9023311573971986

Epoch: 29| Step: 0
Training loss: 2.3470401763916016
Validation loss: 2.8970925474679596

Epoch: 6| Step: 1
Training loss: 2.8032655715942383
Validation loss: 2.8954942046955066

Epoch: 6| Step: 2
Training loss: 2.9542243480682373
Validation loss: 2.899615595417638

Epoch: 6| Step: 3
Training loss: 2.280214309692383
Validation loss: 2.896368606116182

Epoch: 6| Step: 4
Training loss: 2.4859135150909424
Validation loss: 2.8964902790643836

Epoch: 6| Step: 5
Training loss: 4.397242546081543
Validation loss: 2.895353924843573

Epoch: 6| Step: 6
Training loss: 3.5355868339538574
Validation loss: 2.8969607507028887

Epoch: 6| Step: 7
Training loss: 3.026820421218872
Validation loss: 2.8956149214057514

Epoch: 6| Step: 8
Training loss: 3.2543418407440186
Validation loss: 2.896692955365745

Epoch: 6| Step: 9
Training loss: 3.013018846511841
Validation loss: 2.8914917566443004

Epoch: 6| Step: 10
Training loss: 3.124990940093994
Validation loss: 2.890385663637551

Epoch: 6| Step: 11
Training loss: 3.047985076904297
Validation loss: 2.8900135742720736

Epoch: 6| Step: 12
Training loss: 2.712433338165283
Validation loss: 2.8885298416178715

Epoch: 6| Step: 13
Training loss: 2.8040685653686523
Validation loss: 2.8909080669444096

Epoch: 30| Step: 0
Training loss: 3.4215760231018066
Validation loss: 2.8866727557233585

Epoch: 6| Step: 1
Training loss: 3.5568699836730957
Validation loss: 2.89038360247048

Epoch: 6| Step: 2
Training loss: 3.7877919673919678
Validation loss: 2.8871147735144502

Epoch: 6| Step: 3
Training loss: 2.2379090785980225
Validation loss: 2.8873199468017905

Epoch: 6| Step: 4
Training loss: 3.3887600898742676
Validation loss: 2.882588137862503

Epoch: 6| Step: 5
Training loss: 2.7770144939422607
Validation loss: 2.8859291871388755

Epoch: 6| Step: 6
Training loss: 3.275801658630371
Validation loss: 2.8884356073153916

Epoch: 6| Step: 7
Training loss: 2.800793170928955
Validation loss: 2.8860204065999677

Epoch: 6| Step: 8
Training loss: 2.214539051055908
Validation loss: 2.8906662259050595

Epoch: 6| Step: 9
Training loss: 3.099761962890625
Validation loss: 2.888378973930113

Epoch: 6| Step: 10
Training loss: 3.740266799926758
Validation loss: 2.8829570380590295

Epoch: 6| Step: 11
Training loss: 3.06199312210083
Validation loss: 2.8855049917774815

Epoch: 6| Step: 12
Training loss: 2.2526695728302
Validation loss: 2.8839565553972797

Epoch: 6| Step: 13
Training loss: 1.5434660911560059
Validation loss: 2.885605550581409

Epoch: 31| Step: 0
Training loss: 2.1711533069610596
Validation loss: 2.885494755160424

Epoch: 6| Step: 1
Training loss: 2.5467069149017334
Validation loss: 2.881646725439256

Epoch: 6| Step: 2
Training loss: 2.707207202911377
Validation loss: 2.877652273383192

Epoch: 6| Step: 3
Training loss: 2.4986276626586914
Validation loss: 2.880280222944034

Epoch: 6| Step: 4
Training loss: 3.891155242919922
Validation loss: 2.880469399113809

Epoch: 6| Step: 5
Training loss: 3.0588035583496094
Validation loss: 2.8798849480126494

Epoch: 6| Step: 6
Training loss: 3.066004991531372
Validation loss: 2.879541407349289

Epoch: 6| Step: 7
Training loss: 3.3181746006011963
Validation loss: 2.8796522642976496

Epoch: 6| Step: 8
Training loss: 3.032548666000366
Validation loss: 2.8747356014866985

Epoch: 6| Step: 9
Training loss: 3.786485433578491
Validation loss: 2.8773409961372294

Epoch: 6| Step: 10
Training loss: 2.725978136062622
Validation loss: 2.8791371494211178

Epoch: 6| Step: 11
Training loss: 2.5534276962280273
Validation loss: 2.877320066575081

Epoch: 6| Step: 12
Training loss: 3.283482074737549
Validation loss: 2.872989621213687

Epoch: 6| Step: 13
Training loss: 3.013591766357422
Validation loss: 2.8683975793982066

Epoch: 32| Step: 0
Training loss: 2.3350815773010254
Validation loss: 2.8700905282010316

Epoch: 6| Step: 1
Training loss: 3.0705971717834473
Validation loss: 2.870988756097773

Epoch: 6| Step: 2
Training loss: 3.3781442642211914
Validation loss: 2.870707009428291

Epoch: 6| Step: 3
Training loss: 2.840698719024658
Validation loss: 2.8736213766118532

Epoch: 6| Step: 4
Training loss: 3.137755870819092
Validation loss: 2.8671993260742514

Epoch: 6| Step: 5
Training loss: 2.018653392791748
Validation loss: 2.8681194038801294

Epoch: 6| Step: 6
Training loss: 3.8367176055908203
Validation loss: 2.8673475455212336

Epoch: 6| Step: 7
Training loss: 2.67423415184021
Validation loss: 2.8666228402045464

Epoch: 6| Step: 8
Training loss: 3.780961036682129
Validation loss: 2.8680005893912366

Epoch: 6| Step: 9
Training loss: 3.4461822509765625
Validation loss: 2.870894688431935

Epoch: 6| Step: 10
Training loss: 2.5871715545654297
Validation loss: 2.8703165849049888

Epoch: 6| Step: 11
Training loss: 2.4159727096557617
Validation loss: 2.8705185613324566

Epoch: 6| Step: 12
Training loss: 2.5037035942077637
Validation loss: 2.864618811556088

Epoch: 6| Step: 13
Training loss: 4.002979278564453
Validation loss: 2.8659557655293453

Epoch: 33| Step: 0
Training loss: 2.9354772567749023
Validation loss: 2.866917553768363

Epoch: 6| Step: 1
Training loss: 2.6846907138824463
Validation loss: 2.864159776318458

Epoch: 6| Step: 2
Training loss: 2.398486375808716
Validation loss: 2.86266896288882

Epoch: 6| Step: 3
Training loss: 3.4441769123077393
Validation loss: 2.8588348383544595

Epoch: 6| Step: 4
Training loss: 2.8502395153045654
Validation loss: 2.858540722118911

Epoch: 6| Step: 5
Training loss: 3.3061418533325195
Validation loss: 2.86030246878183

Epoch: 6| Step: 6
Training loss: 2.707448959350586
Validation loss: 2.861084217666298

Epoch: 6| Step: 7
Training loss: 2.5379366874694824
Validation loss: 2.861038374644454

Epoch: 6| Step: 8
Training loss: 2.7384612560272217
Validation loss: 2.8604922832981234

Epoch: 6| Step: 9
Training loss: 2.8980469703674316
Validation loss: 2.8582983222059024

Epoch: 6| Step: 10
Training loss: 4.184218883514404
Validation loss: 2.862825278312929

Epoch: 6| Step: 11
Training loss: 3.145811080932617
Validation loss: 2.8620989091934694

Epoch: 6| Step: 12
Training loss: 2.62432861328125
Validation loss: 2.865858672767557

Epoch: 6| Step: 13
Training loss: 3.1093904972076416
Validation loss: 2.869495704609861

Epoch: 34| Step: 0
Training loss: 2.419948101043701
Validation loss: 2.8688815639865015

Epoch: 6| Step: 1
Training loss: 2.214366912841797
Validation loss: 2.869193095032887

Epoch: 6| Step: 2
Training loss: 3.308924674987793
Validation loss: 2.8667928685424147

Epoch: 6| Step: 3
Training loss: 2.3233251571655273
Validation loss: 2.8625272948254823

Epoch: 6| Step: 4
Training loss: 2.566679000854492
Validation loss: 2.863222322156352

Epoch: 6| Step: 5
Training loss: 3.675886631011963
Validation loss: 2.8633225117960284

Epoch: 6| Step: 6
Training loss: 2.95415997505188
Validation loss: 2.860688296697473

Epoch: 6| Step: 7
Training loss: 3.027292013168335
Validation loss: 2.8556347662402737

Epoch: 6| Step: 8
Training loss: 2.472072124481201
Validation loss: 2.853269423207929

Epoch: 6| Step: 9
Training loss: 3.5072197914123535
Validation loss: 2.85535123784055

Epoch: 6| Step: 10
Training loss: 3.806231737136841
Validation loss: 2.8575498903951337

Epoch: 6| Step: 11
Training loss: 2.9239907264709473
Validation loss: 2.85600535843962

Epoch: 6| Step: 12
Training loss: 3.0344629287719727
Validation loss: 2.8538868709277083

Epoch: 6| Step: 13
Training loss: 3.3680644035339355
Validation loss: 2.8626567804685203

Epoch: 35| Step: 0
Training loss: 3.317199945449829
Validation loss: 2.861854827532204

Epoch: 6| Step: 1
Training loss: 2.5843071937561035
Validation loss: 2.8612076005628033

Epoch: 6| Step: 2
Training loss: 2.222640037536621
Validation loss: 2.8606517007274013

Epoch: 6| Step: 3
Training loss: 1.936354160308838
Validation loss: 2.8584061643128753

Epoch: 6| Step: 4
Training loss: 3.1942050457000732
Validation loss: 2.8626744106251705

Epoch: 6| Step: 5
Training loss: 3.5047245025634766
Validation loss: 2.8613043369785434

Epoch: 6| Step: 6
Training loss: 3.2557902336120605
Validation loss: 2.860611161878032

Epoch: 6| Step: 7
Training loss: 3.2340948581695557
Validation loss: 2.8592013800016014

Epoch: 6| Step: 8
Training loss: 3.632840633392334
Validation loss: 2.8514173338490147

Epoch: 6| Step: 9
Training loss: 2.6151182651519775
Validation loss: 2.850140979213099

Epoch: 6| Step: 10
Training loss: 3.6853623390197754
Validation loss: 2.8475471722182406

Epoch: 6| Step: 11
Training loss: 2.80679988861084
Validation loss: 2.8471519434323875

Epoch: 6| Step: 12
Training loss: 2.4126925468444824
Validation loss: 2.8512206051939275

Epoch: 6| Step: 13
Training loss: 3.11199688911438
Validation loss: 2.8484104166748705

Epoch: 36| Step: 0
Training loss: 2.2700278759002686
Validation loss: 2.8468057699100946

Epoch: 6| Step: 1
Training loss: 2.7360944747924805
Validation loss: 2.854518618634952

Epoch: 6| Step: 2
Training loss: 2.587736129760742
Validation loss: 2.844125486189319

Epoch: 6| Step: 3
Training loss: 2.4544029235839844
Validation loss: 2.8503123944805515

Epoch: 6| Step: 4
Training loss: 3.1488819122314453
Validation loss: 2.8495646112708637

Epoch: 6| Step: 5
Training loss: 2.824880838394165
Validation loss: 2.8479166697430354

Epoch: 6| Step: 6
Training loss: 3.9525225162506104
Validation loss: 2.845915925118231

Epoch: 6| Step: 7
Training loss: 2.4884772300720215
Validation loss: 2.8436163779227965

Epoch: 6| Step: 8
Training loss: 2.2840704917907715
Validation loss: 2.8461527824401855

Epoch: 6| Step: 9
Training loss: 2.904477834701538
Validation loss: 2.8402036056723645

Epoch: 6| Step: 10
Training loss: 3.572713851928711
Validation loss: 2.844426196108582

Epoch: 6| Step: 11
Training loss: 2.2590463161468506
Validation loss: 2.8430884730431343

Epoch: 6| Step: 12
Training loss: 4.484229564666748
Validation loss: 2.8467839969101774

Epoch: 6| Step: 13
Training loss: 3.6742515563964844
Validation loss: 2.8471430757994294

Epoch: 37| Step: 0
Training loss: 3.457927703857422
Validation loss: 2.845359920173563

Epoch: 6| Step: 1
Training loss: 3.6648640632629395
Validation loss: 2.8425368814058203

Epoch: 6| Step: 2
Training loss: 3.0769145488739014
Validation loss: 2.841266183442967

Epoch: 6| Step: 3
Training loss: 2.4101462364196777
Validation loss: 2.8401190644951275

Epoch: 6| Step: 4
Training loss: 3.4254438877105713
Validation loss: 2.840791392069991

Epoch: 6| Step: 5
Training loss: 3.6804921627044678
Validation loss: 2.8361376357334915

Epoch: 6| Step: 6
Training loss: 3.202963352203369
Validation loss: 2.8423951210514193

Epoch: 6| Step: 7
Training loss: 2.5932164192199707
Validation loss: 2.8337928210535357

Epoch: 6| Step: 8
Training loss: 1.8608771562576294
Validation loss: 2.8350835948862056

Epoch: 6| Step: 9
Training loss: 2.5538716316223145
Validation loss: 2.835604226717385

Epoch: 6| Step: 10
Training loss: 2.8253676891326904
Validation loss: 2.835662869996922

Epoch: 6| Step: 11
Training loss: 2.9992001056671143
Validation loss: 2.8349015225646315

Epoch: 6| Step: 12
Training loss: 3.0862088203430176
Validation loss: 2.83474039518705

Epoch: 6| Step: 13
Training loss: 1.9826253652572632
Validation loss: 2.8424170709425405

Epoch: 38| Step: 0
Training loss: 2.1830992698669434
Validation loss: 2.852643835929132

Epoch: 6| Step: 1
Training loss: 2.5880470275878906
Validation loss: 2.863576655746788

Epoch: 6| Step: 2
Training loss: 3.0803747177124023
Validation loss: 2.86918633214889

Epoch: 6| Step: 3
Training loss: 3.6097331047058105
Validation loss: 2.8679311198572957

Epoch: 6| Step: 4
Training loss: 2.264526844024658
Validation loss: 2.8604937215005197

Epoch: 6| Step: 5
Training loss: 3.0307397842407227
Validation loss: 2.8589227327736477

Epoch: 6| Step: 6
Training loss: 2.9142963886260986
Validation loss: 2.8676825646431214

Epoch: 6| Step: 7
Training loss: 2.9485278129577637
Validation loss: 2.8644968719892603

Epoch: 6| Step: 8
Training loss: 3.507652759552002
Validation loss: 2.8510936690915014

Epoch: 6| Step: 9
Training loss: 3.2314467430114746
Validation loss: 2.8475544016848326

Epoch: 6| Step: 10
Training loss: 2.54860520362854
Validation loss: 2.8485053739240094

Epoch: 6| Step: 11
Training loss: 3.1444625854492188
Validation loss: 2.843888149466566

Epoch: 6| Step: 12
Training loss: 2.955238103866577
Validation loss: 2.843994843062534

Epoch: 6| Step: 13
Training loss: 3.5683114528656006
Validation loss: 2.8447638634712464

Epoch: 39| Step: 0
Training loss: 2.964137554168701
Validation loss: 2.844094476392192

Epoch: 6| Step: 1
Training loss: 2.9237000942230225
Validation loss: 2.8364396608004006

Epoch: 6| Step: 2
Training loss: 3.081669569015503
Validation loss: 2.8325191902857956

Epoch: 6| Step: 3
Training loss: 2.8893117904663086
Validation loss: 2.830997315786218

Epoch: 6| Step: 4
Training loss: 2.2153639793395996
Validation loss: 2.837718609840639

Epoch: 6| Step: 5
Training loss: 4.033642292022705
Validation loss: 2.833741472613427

Epoch: 6| Step: 6
Training loss: 3.2674202919006348
Validation loss: 2.835249275289556

Epoch: 6| Step: 7
Training loss: 4.045582294464111
Validation loss: 2.835096343871086

Epoch: 6| Step: 8
Training loss: 2.3190088272094727
Validation loss: 2.83628588081688

Epoch: 6| Step: 9
Training loss: 2.9179584980010986
Validation loss: 2.8301148850430726

Epoch: 6| Step: 10
Training loss: 2.5060129165649414
Validation loss: 2.8304034048511135

Epoch: 6| Step: 11
Training loss: 2.301171064376831
Validation loss: 2.832858384296458

Epoch: 6| Step: 12
Training loss: 2.7045018672943115
Validation loss: 2.828424379389773

Epoch: 6| Step: 13
Training loss: 3.2437241077423096
Validation loss: 2.8280165528738372

Epoch: 40| Step: 0
Training loss: 3.04651141166687
Validation loss: 2.8249918465973227

Epoch: 6| Step: 1
Training loss: 3.4960460662841797
Validation loss: 2.8268874486287436

Epoch: 6| Step: 2
Training loss: 2.978602409362793
Validation loss: 2.826784905566964

Epoch: 6| Step: 3
Training loss: 3.623067855834961
Validation loss: 2.822857395295174

Epoch: 6| Step: 4
Training loss: 3.0056495666503906
Validation loss: 2.8245582298565934

Epoch: 6| Step: 5
Training loss: 2.5860652923583984
Validation loss: 2.823250768005207

Epoch: 6| Step: 6
Training loss: 3.0053305625915527
Validation loss: 2.8295377454450055

Epoch: 6| Step: 7
Training loss: 2.590364694595337
Validation loss: 2.825499903771185

Epoch: 6| Step: 8
Training loss: 2.4246416091918945
Validation loss: 2.8287741086816274

Epoch: 6| Step: 9
Training loss: 2.691448450088501
Validation loss: 2.823650388307469

Epoch: 6| Step: 10
Training loss: 2.6265830993652344
Validation loss: 2.824867010116577

Epoch: 6| Step: 11
Training loss: 3.3768532276153564
Validation loss: 2.825435730718797

Epoch: 6| Step: 12
Training loss: 2.851388454437256
Validation loss: 2.824299112443001

Epoch: 6| Step: 13
Training loss: 2.7562665939331055
Validation loss: 2.8227184972455426

Epoch: 41| Step: 0
Training loss: 3.222654342651367
Validation loss: 2.826230074769707

Epoch: 6| Step: 1
Training loss: 2.0969834327697754
Validation loss: 2.825642237099268

Epoch: 6| Step: 2
Training loss: 3.402523994445801
Validation loss: 2.8277036733524774

Epoch: 6| Step: 3
Training loss: 2.4914438724517822
Validation loss: 2.8212319907321723

Epoch: 6| Step: 4
Training loss: 3.379607677459717
Validation loss: 2.825211294235722

Epoch: 6| Step: 5
Training loss: 3.1394639015197754
Validation loss: 2.8257219586321103

Epoch: 6| Step: 6
Training loss: 3.156794309616089
Validation loss: 2.825706802388673

Epoch: 6| Step: 7
Training loss: 2.2469115257263184
Validation loss: 2.825836709750596

Epoch: 6| Step: 8
Training loss: 2.8965983390808105
Validation loss: 2.8289112942193144

Epoch: 6| Step: 9
Training loss: 3.2707746028900146
Validation loss: 2.8236334528974307

Epoch: 6| Step: 10
Training loss: 3.2084226608276367
Validation loss: 2.8231882126100603

Epoch: 6| Step: 11
Training loss: 2.6198487281799316
Validation loss: 2.8238013200862433

Epoch: 6| Step: 12
Training loss: 3.2036843299865723
Validation loss: 2.820365931398125

Epoch: 6| Step: 13
Training loss: 2.6465327739715576
Validation loss: 2.818391946054274

Epoch: 42| Step: 0
Training loss: 2.919351816177368
Validation loss: 2.819883700340025

Epoch: 6| Step: 1
Training loss: 3.2063283920288086
Validation loss: 2.817771504002233

Epoch: 6| Step: 2
Training loss: 3.0899922847747803
Validation loss: 2.8212173241440968

Epoch: 6| Step: 3
Training loss: 2.648375988006592
Validation loss: 2.8295630588326404

Epoch: 6| Step: 4
Training loss: 2.919222831726074
Validation loss: 2.839626345583188

Epoch: 6| Step: 5
Training loss: 2.999328851699829
Validation loss: 2.8406085327107418

Epoch: 6| Step: 6
Training loss: 2.2495484352111816
Validation loss: 2.842435283045615

Epoch: 6| Step: 7
Training loss: 3.1111719608306885
Validation loss: 2.830754398017801

Epoch: 6| Step: 8
Training loss: 3.31040096282959
Validation loss: 2.829836312160697

Epoch: 6| Step: 9
Training loss: 2.422010898590088
Validation loss: 2.821842183348953

Epoch: 6| Step: 10
Training loss: 2.4752302169799805
Validation loss: 2.8221964605392946

Epoch: 6| Step: 11
Training loss: 2.8803858757019043
Validation loss: 2.8225054151268414

Epoch: 6| Step: 12
Training loss: 3.502336025238037
Validation loss: 2.820594900397844

Epoch: 6| Step: 13
Training loss: 3.652418375015259
Validation loss: 2.81811918750886

Epoch: 43| Step: 0
Training loss: 2.9866080284118652
Validation loss: 2.8163786806086057

Epoch: 6| Step: 1
Training loss: 2.6035051345825195
Validation loss: 2.8171421071534515

Epoch: 6| Step: 2
Training loss: 3.3457136154174805
Validation loss: 2.815188125897479

Epoch: 6| Step: 3
Training loss: 2.31978702545166
Validation loss: 2.819578524558775

Epoch: 6| Step: 4
Training loss: 2.4244680404663086
Validation loss: 2.818168019735685

Epoch: 6| Step: 5
Training loss: 3.291623592376709
Validation loss: 2.8178469468188543

Epoch: 6| Step: 6
Training loss: 3.02787446975708
Validation loss: 2.8195909633431384

Epoch: 6| Step: 7
Training loss: 3.4344584941864014
Validation loss: 2.8227722362805436

Epoch: 6| Step: 8
Training loss: 3.4020519256591797
Validation loss: 2.8202455966703353

Epoch: 6| Step: 9
Training loss: 3.7468786239624023
Validation loss: 2.8172739116094445

Epoch: 6| Step: 10
Training loss: 2.755096912384033
Validation loss: 2.8192548521103395

Epoch: 6| Step: 11
Training loss: 2.7816004753112793
Validation loss: 2.822644051685128

Epoch: 6| Step: 12
Training loss: 2.461064338684082
Validation loss: 2.818613698405604

Epoch: 6| Step: 13
Training loss: 2.266173839569092
Validation loss: 2.815946138033303

Epoch: 44| Step: 0
Training loss: 2.4626803398132324
Validation loss: 2.8165263616910545

Epoch: 6| Step: 1
Training loss: 2.3714120388031006
Validation loss: 2.814031167696881

Epoch: 6| Step: 2
Training loss: 3.537642478942871
Validation loss: 2.8102518743084324

Epoch: 6| Step: 3
Training loss: 3.9968314170837402
Validation loss: 2.8101722578848563

Epoch: 6| Step: 4
Training loss: 3.310248613357544
Validation loss: 2.810856480752268

Epoch: 6| Step: 5
Training loss: 2.6410534381866455
Validation loss: 2.812877539665468

Epoch: 6| Step: 6
Training loss: 3.7490100860595703
Validation loss: 2.8099551149593887

Epoch: 6| Step: 7
Training loss: 3.583207130432129
Validation loss: 2.8096663772418933

Epoch: 6| Step: 8
Training loss: 2.300433874130249
Validation loss: 2.808860630117437

Epoch: 6| Step: 9
Training loss: 1.8525135517120361
Validation loss: 2.8081075478625555

Epoch: 6| Step: 10
Training loss: 3.017009735107422
Validation loss: 2.8097588708323817

Epoch: 6| Step: 11
Training loss: 2.172459125518799
Validation loss: 2.814061846784366

Epoch: 6| Step: 12
Training loss: 2.6358141899108887
Validation loss: 2.809230440406389

Epoch: 6| Step: 13
Training loss: 3.7573606967926025
Validation loss: 2.814406584667903

Epoch: 45| Step: 0
Training loss: 3.1827950477600098
Validation loss: 2.8144001730026735

Epoch: 6| Step: 1
Training loss: 2.333658456802368
Validation loss: 2.8149264371523293

Epoch: 6| Step: 2
Training loss: 2.636476993560791
Validation loss: 2.817426545645601

Epoch: 6| Step: 3
Training loss: 3.322263479232788
Validation loss: 2.8106043313139226

Epoch: 6| Step: 4
Training loss: 2.2563862800598145
Validation loss: 2.818622012292185

Epoch: 6| Step: 5
Training loss: 3.225740671157837
Validation loss: 2.8123394058596705

Epoch: 6| Step: 6
Training loss: 2.4310333728790283
Validation loss: 2.8153520527706353

Epoch: 6| Step: 7
Training loss: 3.2789883613586426
Validation loss: 2.8144764361842984

Epoch: 6| Step: 8
Training loss: 2.9811630249023438
Validation loss: 2.815395819243564

Epoch: 6| Step: 9
Training loss: 3.2768075466156006
Validation loss: 2.812810931154477

Epoch: 6| Step: 10
Training loss: 2.960923194885254
Validation loss: 2.808755518287741

Epoch: 6| Step: 11
Training loss: 2.1047427654266357
Validation loss: 2.806568730262018

Epoch: 6| Step: 12
Training loss: 3.8447346687316895
Validation loss: 2.809489850075014

Epoch: 6| Step: 13
Training loss: 3.3364269733428955
Validation loss: 2.8032302907718125

Epoch: 46| Step: 0
Training loss: 2.83150577545166
Validation loss: 2.8031113301554034

Epoch: 6| Step: 1
Training loss: 2.5502309799194336
Validation loss: 2.802090747382051

Epoch: 6| Step: 2
Training loss: 1.7945853471755981
Validation loss: 2.8055285074377574

Epoch: 6| Step: 3
Training loss: 2.236135959625244
Validation loss: 2.8058075392118065

Epoch: 6| Step: 4
Training loss: 2.646697998046875
Validation loss: 2.809663729001117

Epoch: 6| Step: 5
Training loss: 3.3641624450683594
Validation loss: 2.8079894973385717

Epoch: 6| Step: 6
Training loss: 3.3456242084503174
Validation loss: 2.8097349315561275

Epoch: 6| Step: 7
Training loss: 2.914423942565918
Validation loss: 2.8071041722451486

Epoch: 6| Step: 8
Training loss: 3.010995388031006
Validation loss: 2.8035550322583926

Epoch: 6| Step: 9
Training loss: 3.4196114540100098
Validation loss: 2.8054263745584795

Epoch: 6| Step: 10
Training loss: 2.889303207397461
Validation loss: 2.810166315365863

Epoch: 6| Step: 11
Training loss: 3.6178970336914062
Validation loss: 2.811300926311042

Epoch: 6| Step: 12
Training loss: 3.107651710510254
Validation loss: 2.810401834467406

Epoch: 6| Step: 13
Training loss: 3.4298524856567383
Validation loss: 2.8035599365029285

Epoch: 47| Step: 0
Training loss: 2.9204416275024414
Validation loss: 2.8045418929028254

Epoch: 6| Step: 1
Training loss: 3.4044036865234375
Validation loss: 2.80076160482181

Epoch: 6| Step: 2
Training loss: 2.0843968391418457
Validation loss: 2.8060242129910375

Epoch: 6| Step: 3
Training loss: 2.2150588035583496
Validation loss: 2.7996885212518836

Epoch: 6| Step: 4
Training loss: 3.865448236465454
Validation loss: 2.7987590630849204

Epoch: 6| Step: 5
Training loss: 2.747434377670288
Validation loss: 2.797074143604566

Epoch: 6| Step: 6
Training loss: 3.3164267539978027
Validation loss: 2.798519008903093

Epoch: 6| Step: 7
Training loss: 3.1170449256896973
Validation loss: 2.8008539369029384

Epoch: 6| Step: 8
Training loss: 2.8913235664367676
Validation loss: 2.796207456178563

Epoch: 6| Step: 9
Training loss: 2.8613083362579346
Validation loss: 2.7993641796932427

Epoch: 6| Step: 10
Training loss: 2.7951841354370117
Validation loss: 2.7950114280946794

Epoch: 6| Step: 11
Training loss: 2.990354299545288
Validation loss: 2.7982112976812545

Epoch: 6| Step: 12
Training loss: 2.1079447269439697
Validation loss: 2.796089267217985

Epoch: 6| Step: 13
Training loss: 4.137228488922119
Validation loss: 2.801629258740333

Epoch: 48| Step: 0
Training loss: 2.423400640487671
Validation loss: 2.8002718443511636

Epoch: 6| Step: 1
Training loss: 2.843672275543213
Validation loss: 2.804670303098617

Epoch: 6| Step: 2
Training loss: 3.843313455581665
Validation loss: 2.8088416207221245

Epoch: 6| Step: 3
Training loss: 3.281925678253174
Validation loss: 2.804767306132983

Epoch: 6| Step: 4
Training loss: 2.942882776260376
Validation loss: 2.814535184573102

Epoch: 6| Step: 5
Training loss: 2.933138132095337
Validation loss: 2.821789182642455

Epoch: 6| Step: 6
Training loss: 2.7390785217285156
Validation loss: 2.820829840116603

Epoch: 6| Step: 7
Training loss: 2.7737903594970703
Validation loss: 2.8123899198347524

Epoch: 6| Step: 8
Training loss: 2.926826000213623
Validation loss: 2.8055762783173592

Epoch: 6| Step: 9
Training loss: 3.06722354888916
Validation loss: 2.7989201391896894

Epoch: 6| Step: 10
Training loss: 2.5543923377990723
Validation loss: 2.7975584460842993

Epoch: 6| Step: 11
Training loss: 3.0149312019348145
Validation loss: 2.795795202255249

Epoch: 6| Step: 12
Training loss: 2.112210273742676
Validation loss: 2.7965217790296

Epoch: 6| Step: 13
Training loss: 3.879380226135254
Validation loss: 2.800647063921857

Epoch: 49| Step: 0
Training loss: 2.9364137649536133
Validation loss: 2.798744196532875

Epoch: 6| Step: 1
Training loss: 3.3509228229522705
Validation loss: 2.7976576846132994

Epoch: 6| Step: 2
Training loss: 3.2939088344573975
Validation loss: 2.8008972829388035

Epoch: 6| Step: 3
Training loss: 2.3664565086364746
Validation loss: 2.8040065714108047

Epoch: 6| Step: 4
Training loss: 2.9512593746185303
Validation loss: 2.7968249115892636

Epoch: 6| Step: 5
Training loss: 2.1677775382995605
Validation loss: 2.804484736534857

Epoch: 6| Step: 6
Training loss: 2.7431864738464355
Validation loss: 2.801595434065788

Epoch: 6| Step: 7
Training loss: 2.549830913543701
Validation loss: 2.799538730293192

Epoch: 6| Step: 8
Training loss: 2.5342304706573486
Validation loss: 2.800688756409512

Epoch: 6| Step: 9
Training loss: 2.365447759628296
Validation loss: 2.8069707578228367

Epoch: 6| Step: 10
Training loss: 2.949906587600708
Validation loss: 2.8000839141107376

Epoch: 6| Step: 11
Training loss: 3.7384469509124756
Validation loss: 2.7987572941728818

Epoch: 6| Step: 12
Training loss: 3.3544139862060547
Validation loss: 2.7987509312168246

Epoch: 6| Step: 13
Training loss: 3.9230642318725586
Validation loss: 2.800450655721849

Epoch: 50| Step: 0
Training loss: 2.1906652450561523
Validation loss: 2.7969708955416115

Epoch: 6| Step: 1
Training loss: 2.5358898639678955
Validation loss: 2.7989597858921176

Epoch: 6| Step: 2
Training loss: 2.4056906700134277
Validation loss: 2.8029808536652596

Epoch: 6| Step: 3
Training loss: 2.5746073722839355
Validation loss: 2.801248194068991

Epoch: 6| Step: 4
Training loss: 3.618403911590576
Validation loss: 2.800165068718695

Epoch: 6| Step: 5
Training loss: 3.4967737197875977
Validation loss: 2.80085059647919

Epoch: 6| Step: 6
Training loss: 3.392343521118164
Validation loss: 2.800162353823262

Epoch: 6| Step: 7
Training loss: 3.205620288848877
Validation loss: 2.803821640629922

Epoch: 6| Step: 8
Training loss: 3.239553689956665
Validation loss: 2.7998883903667493

Epoch: 6| Step: 9
Training loss: 3.615647792816162
Validation loss: 2.7938792423535417

Epoch: 6| Step: 10
Training loss: 3.143599510192871
Validation loss: 2.794255487380489

Epoch: 6| Step: 11
Training loss: 1.967381477355957
Validation loss: 2.7935654886307253

Epoch: 6| Step: 12
Training loss: 2.664175033569336
Validation loss: 2.7916943437309674

Epoch: 6| Step: 13
Training loss: 2.5383012294769287
Validation loss: 2.794402394243466

Epoch: 51| Step: 0
Training loss: 2.4655916690826416
Validation loss: 2.791643568264541

Epoch: 6| Step: 1
Training loss: 3.4449462890625
Validation loss: 2.790681218588224

Epoch: 6| Step: 2
Training loss: 2.3507378101348877
Validation loss: 2.7938851387270036

Epoch: 6| Step: 3
Training loss: 2.347571849822998
Validation loss: 2.7919812433181272

Epoch: 6| Step: 4
Training loss: 3.3286843299865723
Validation loss: 2.7981004330419723

Epoch: 6| Step: 5
Training loss: 2.5692014694213867
Validation loss: 2.794158330527685

Epoch: 6| Step: 6
Training loss: 2.6580421924591064
Validation loss: 2.7877764830025296

Epoch: 6| Step: 7
Training loss: 2.6286263465881348
Validation loss: 2.7897950039115003

Epoch: 6| Step: 8
Training loss: 3.5693166255950928
Validation loss: 2.7910321220274894

Epoch: 6| Step: 9
Training loss: 2.723889112472534
Validation loss: 2.791696512570945

Epoch: 6| Step: 10
Training loss: 3.1070261001586914
Validation loss: 2.791526227869013

Epoch: 6| Step: 11
Training loss: 3.31894850730896
Validation loss: 2.786031369240053

Epoch: 6| Step: 12
Training loss: 3.400815486907959
Validation loss: 2.78892421209684

Epoch: 6| Step: 13
Training loss: 2.6157643795013428
Validation loss: 2.7887865779220418

Epoch: 52| Step: 0
Training loss: 2.8666787147521973
Validation loss: 2.786143615681638

Epoch: 6| Step: 1
Training loss: 3.3395090103149414
Validation loss: 2.7908307224191646

Epoch: 6| Step: 2
Training loss: 2.541790723800659
Validation loss: 2.7904229394851194

Epoch: 6| Step: 3
Training loss: 1.801132321357727
Validation loss: 2.791614570925313

Epoch: 6| Step: 4
Training loss: 3.277590274810791
Validation loss: 2.790448001635972

Epoch: 6| Step: 5
Training loss: 2.7617664337158203
Validation loss: 2.790173348560128

Epoch: 6| Step: 6
Training loss: 2.7932419776916504
Validation loss: 2.786513900244108

Epoch: 6| Step: 7
Training loss: 3.288179397583008
Validation loss: 2.782101833692161

Epoch: 6| Step: 8
Training loss: 3.0067782402038574
Validation loss: 2.7814644023936284

Epoch: 6| Step: 9
Training loss: 3.4158031940460205
Validation loss: 2.7848584985220306

Epoch: 6| Step: 10
Training loss: 2.4309897422790527
Validation loss: 2.7791466661678847

Epoch: 6| Step: 11
Training loss: 3.3378896713256836
Validation loss: 2.778946071542719

Epoch: 6| Step: 12
Training loss: 2.680445432662964
Validation loss: 2.7789492145661385

Epoch: 6| Step: 13
Training loss: 3.1957757472991943
Validation loss: 2.7765091465365503

Epoch: 53| Step: 0
Training loss: 2.7963671684265137
Validation loss: 2.7750384115403697

Epoch: 6| Step: 1
Training loss: 3.586693048477173
Validation loss: 2.7742103376696186

Epoch: 6| Step: 2
Training loss: 2.8342716693878174
Validation loss: 2.771778445090017

Epoch: 6| Step: 3
Training loss: 2.8865537643432617
Validation loss: 2.775607088560699

Epoch: 6| Step: 4
Training loss: 3.2467494010925293
Validation loss: 2.7706411653949368

Epoch: 6| Step: 5
Training loss: 2.5530781745910645
Validation loss: 2.7716346889413814

Epoch: 6| Step: 6
Training loss: 2.1850008964538574
Validation loss: 2.770524458218646

Epoch: 6| Step: 7
Training loss: 3.531140089035034
Validation loss: 2.771538770327004

Epoch: 6| Step: 8
Training loss: 2.415053367614746
Validation loss: 2.7757401927824943

Epoch: 6| Step: 9
Training loss: 2.9339895248413086
Validation loss: 2.7787684727740545

Epoch: 6| Step: 10
Training loss: 2.5919110774993896
Validation loss: 2.771861671119608

Epoch: 6| Step: 11
Training loss: 2.786625862121582
Validation loss: 2.774394712140483

Epoch: 6| Step: 12
Training loss: 2.757983922958374
Validation loss: 2.772244945649178

Epoch: 6| Step: 13
Training loss: 3.8076443672180176
Validation loss: 2.7706989037093295

Epoch: 54| Step: 0
Training loss: 3.7272543907165527
Validation loss: 2.7680145104726157

Epoch: 6| Step: 1
Training loss: 3.243412494659424
Validation loss: 2.7670999342395413

Epoch: 6| Step: 2
Training loss: 1.894715428352356
Validation loss: 2.766214141281702

Epoch: 6| Step: 3
Training loss: 1.8129209280014038
Validation loss: 2.7659585296466784

Epoch: 6| Step: 4
Training loss: 2.588042736053467
Validation loss: 2.765229804541475

Epoch: 6| Step: 5
Training loss: 2.678361415863037
Validation loss: 2.7648133782930273

Epoch: 6| Step: 6
Training loss: 3.018423557281494
Validation loss: 2.7661932540196243

Epoch: 6| Step: 7
Training loss: 3.3348987102508545
Validation loss: 2.762249941466957

Epoch: 6| Step: 8
Training loss: 4.306960582733154
Validation loss: 2.767303265551085

Epoch: 6| Step: 9
Training loss: 2.8387017250061035
Validation loss: 2.7657616753732004

Epoch: 6| Step: 10
Training loss: 2.682530641555786
Validation loss: 2.7625482697640695

Epoch: 6| Step: 11
Training loss: 2.6460745334625244
Validation loss: 2.7653697716292513

Epoch: 6| Step: 12
Training loss: 2.8351588249206543
Validation loss: 2.7641268878854732

Epoch: 6| Step: 13
Training loss: 2.747170925140381
Validation loss: 2.764689258349839

Epoch: 55| Step: 0
Training loss: 2.8618738651275635
Validation loss: 2.7698642259003012

Epoch: 6| Step: 1
Training loss: 3.608107089996338
Validation loss: 2.774811285798268

Epoch: 6| Step: 2
Training loss: 2.0935680866241455
Validation loss: 2.7719243008603334

Epoch: 6| Step: 3
Training loss: 2.781226873397827
Validation loss: 2.767851186055009

Epoch: 6| Step: 4
Training loss: 3.09315824508667
Validation loss: 2.768250811484552

Epoch: 6| Step: 5
Training loss: 3.667300224304199
Validation loss: 2.7735108329403784

Epoch: 6| Step: 6
Training loss: 3.296865224838257
Validation loss: 2.770313519303517

Epoch: 6| Step: 7
Training loss: 2.875112295150757
Validation loss: 2.7663193492479223

Epoch: 6| Step: 8
Training loss: 2.900625705718994
Validation loss: 2.7653288789974746

Epoch: 6| Step: 9
Training loss: 2.568835735321045
Validation loss: 2.76620554154919

Epoch: 6| Step: 10
Training loss: 2.986621856689453
Validation loss: 2.764958409852879

Epoch: 6| Step: 11
Training loss: 2.891176223754883
Validation loss: 2.763997636815553

Epoch: 6| Step: 12
Training loss: 2.3150715827941895
Validation loss: 2.758852522860291

Epoch: 6| Step: 13
Training loss: 2.056648015975952
Validation loss: 2.7620463755822953

Epoch: 56| Step: 0
Training loss: 3.663203239440918
Validation loss: 2.762853299417803

Epoch: 6| Step: 1
Training loss: 2.6707630157470703
Validation loss: 2.758475798432545

Epoch: 6| Step: 2
Training loss: 2.2989566326141357
Validation loss: 2.7600393884925434

Epoch: 6| Step: 3
Training loss: 2.8815577030181885
Validation loss: 2.7581001738066315

Epoch: 6| Step: 4
Training loss: 2.7724928855895996
Validation loss: 2.755348395275813

Epoch: 6| Step: 5
Training loss: 2.9797608852386475
Validation loss: 2.756468362705682

Epoch: 6| Step: 6
Training loss: 2.5678772926330566
Validation loss: 2.7600502788379626

Epoch: 6| Step: 7
Training loss: 2.804795265197754
Validation loss: 2.7621752036515104

Epoch: 6| Step: 8
Training loss: 3.2634055614471436
Validation loss: 2.763025793977963

Epoch: 6| Step: 9
Training loss: 2.6431992053985596
Validation loss: 2.7684031942839264

Epoch: 6| Step: 10
Training loss: 3.687227249145508
Validation loss: 2.764894316273351

Epoch: 6| Step: 11
Training loss: 2.1824376583099365
Validation loss: 2.7786276648121495

Epoch: 6| Step: 12
Training loss: 3.056001663208008
Validation loss: 2.7691921931441112

Epoch: 6| Step: 13
Training loss: 2.84497332572937
Validation loss: 2.7660293861102034

Epoch: 57| Step: 0
Training loss: 3.1314635276794434
Validation loss: 2.7641474508470103

Epoch: 6| Step: 1
Training loss: 3.3252272605895996
Validation loss: 2.7600907689781597

Epoch: 6| Step: 2
Training loss: 2.8519833087921143
Validation loss: 2.7517283372981574

Epoch: 6| Step: 3
Training loss: 2.792861223220825
Validation loss: 2.7484533658591648

Epoch: 6| Step: 4
Training loss: 2.851874828338623
Validation loss: 2.7493655861064954

Epoch: 6| Step: 5
Training loss: 2.72845458984375
Validation loss: 2.750394839112477

Epoch: 6| Step: 6
Training loss: 2.7045626640319824
Validation loss: 2.7478908492672827

Epoch: 6| Step: 7
Training loss: 3.059265613555908
Validation loss: 2.750619033331512

Epoch: 6| Step: 8
Training loss: 2.9489030838012695
Validation loss: 2.7457417749589488

Epoch: 6| Step: 9
Training loss: 2.813101291656494
Validation loss: 2.7457073093742452

Epoch: 6| Step: 10
Training loss: 2.4115068912506104
Validation loss: 2.748937181247178

Epoch: 6| Step: 11
Training loss: 3.1119484901428223
Validation loss: 2.7497320739171838

Epoch: 6| Step: 12
Training loss: 2.6108248233795166
Validation loss: 2.745687900050994

Epoch: 6| Step: 13
Training loss: 2.9445736408233643
Validation loss: 2.7458652270737516

Epoch: 58| Step: 0
Training loss: 2.8610610961914062
Validation loss: 2.745807642577797

Epoch: 6| Step: 1
Training loss: 2.1008543968200684
Validation loss: 2.743600663318429

Epoch: 6| Step: 2
Training loss: 3.59932279586792
Validation loss: 2.74263180968582

Epoch: 6| Step: 3
Training loss: 2.486823320388794
Validation loss: 2.7434107026746197

Epoch: 6| Step: 4
Training loss: 3.6144442558288574
Validation loss: 2.7418364094149683

Epoch: 6| Step: 5
Training loss: 2.92167592048645
Validation loss: 2.743466256767191

Epoch: 6| Step: 6
Training loss: 2.614182949066162
Validation loss: 2.7416476613731793

Epoch: 6| Step: 7
Training loss: 1.6655584573745728
Validation loss: 2.7469024683839534

Epoch: 6| Step: 8
Training loss: 3.213052749633789
Validation loss: 2.7544138636640323

Epoch: 6| Step: 9
Training loss: 3.287970542907715
Validation loss: 2.756327816235122

Epoch: 6| Step: 10
Training loss: 3.1784589290618896
Validation loss: 2.7519728496510494

Epoch: 6| Step: 11
Training loss: 2.667936086654663
Validation loss: 2.7634998111314673

Epoch: 6| Step: 12
Training loss: 2.897824764251709
Validation loss: 2.757027696537715

Epoch: 6| Step: 13
Training loss: 3.20507550239563
Validation loss: 2.7590947792094243

Epoch: 59| Step: 0
Training loss: 3.0656113624572754
Validation loss: 2.7540584123262795

Epoch: 6| Step: 1
Training loss: 2.7915337085723877
Validation loss: 2.7507057189941406

Epoch: 6| Step: 2
Training loss: 3.692955255508423
Validation loss: 2.74859909601109

Epoch: 6| Step: 3
Training loss: 2.206325054168701
Validation loss: 2.7530343327471005

Epoch: 6| Step: 4
Training loss: 2.934389591217041
Validation loss: 2.751389523988129

Epoch: 6| Step: 5
Training loss: 2.139864921569824
Validation loss: 2.747384919915148

Epoch: 6| Step: 6
Training loss: 3.0768065452575684
Validation loss: 2.746425369734405

Epoch: 6| Step: 7
Training loss: 2.6794917583465576
Validation loss: 2.747266765563719

Epoch: 6| Step: 8
Training loss: 2.444504737854004
Validation loss: 2.743557891538066

Epoch: 6| Step: 9
Training loss: 2.788733959197998
Validation loss: 2.7416131957884757

Epoch: 6| Step: 10
Training loss: 3.655911445617676
Validation loss: 2.744969798672584

Epoch: 6| Step: 11
Training loss: 2.9267947673797607
Validation loss: 2.743214858475552

Epoch: 6| Step: 12
Training loss: 2.5933923721313477
Validation loss: 2.7445566628568914

Epoch: 6| Step: 13
Training loss: 3.3721678256988525
Validation loss: 2.740648459362727

Epoch: 60| Step: 0
Training loss: 2.2563624382019043
Validation loss: 2.738644628114598

Epoch: 6| Step: 1
Training loss: 3.1555027961730957
Validation loss: 2.7399051753423547

Epoch: 6| Step: 2
Training loss: 2.5374252796173096
Validation loss: 2.7391760221091648

Epoch: 6| Step: 3
Training loss: 3.9802799224853516
Validation loss: 2.738355408432663

Epoch: 6| Step: 4
Training loss: 3.713071584701538
Validation loss: 2.7340041924548406

Epoch: 6| Step: 5
Training loss: 2.5142323970794678
Validation loss: 2.735952438846711

Epoch: 6| Step: 6
Training loss: 2.054332971572876
Validation loss: 2.7387548441527994

Epoch: 6| Step: 7
Training loss: 2.594554901123047
Validation loss: 2.7342854110143517

Epoch: 6| Step: 8
Training loss: 3.010324239730835
Validation loss: 2.733198332530196

Epoch: 6| Step: 9
Training loss: 3.019179344177246
Validation loss: 2.739401883976434

Epoch: 6| Step: 10
Training loss: 3.70678973197937
Validation loss: 2.7392753990747596

Epoch: 6| Step: 11
Training loss: 2.461164951324463
Validation loss: 2.7343607743581138

Epoch: 6| Step: 12
Training loss: 2.4874653816223145
Validation loss: 2.7344781455173286

Epoch: 6| Step: 13
Training loss: 2.2289328575134277
Validation loss: 2.737562500020509

Epoch: 61| Step: 0
Training loss: 2.6501224040985107
Validation loss: 2.7404457420431156

Epoch: 6| Step: 1
Training loss: 2.156862735748291
Validation loss: 2.7375633562764814

Epoch: 6| Step: 2
Training loss: 3.2781333923339844
Validation loss: 2.7332050338868172

Epoch: 6| Step: 3
Training loss: 1.9363155364990234
Validation loss: 2.729252838319348

Epoch: 6| Step: 4
Training loss: 3.6119656562805176
Validation loss: 2.731494270345216

Epoch: 6| Step: 5
Training loss: 3.0984036922454834
Validation loss: 2.735045797081404

Epoch: 6| Step: 6
Training loss: 3.332958221435547
Validation loss: 2.7321271460543395

Epoch: 6| Step: 7
Training loss: 3.540278911590576
Validation loss: 2.734372874741913

Epoch: 6| Step: 8
Training loss: 2.9538538455963135
Validation loss: 2.734966262694328

Epoch: 6| Step: 9
Training loss: 2.0722851753234863
Validation loss: 2.7272851979860695

Epoch: 6| Step: 10
Training loss: 3.0600051879882812
Validation loss: 2.7323975563049316

Epoch: 6| Step: 11
Training loss: 2.679194927215576
Validation loss: 2.737160933915005

Epoch: 6| Step: 12
Training loss: 2.8566908836364746
Validation loss: 2.7603516963220414

Epoch: 6| Step: 13
Training loss: 2.77004337310791
Validation loss: 2.730294391673098

Epoch: 62| Step: 0
Training loss: 3.249779224395752
Validation loss: 2.7318392799746607

Epoch: 6| Step: 1
Training loss: 2.808119297027588
Validation loss: 2.7346748767360562

Epoch: 6| Step: 2
Training loss: 3.1897220611572266
Validation loss: 2.7385812190271195

Epoch: 6| Step: 3
Training loss: 2.421198606491089
Validation loss: 2.749170664818056

Epoch: 6| Step: 4
Training loss: 1.9614876508712769
Validation loss: 2.7541041169115292

Epoch: 6| Step: 5
Training loss: 3.5947418212890625
Validation loss: 2.75739566228723

Epoch: 6| Step: 6
Training loss: 3.4312984943389893
Validation loss: 2.7633529427231

Epoch: 6| Step: 7
Training loss: 2.9786787033081055
Validation loss: 2.7587458318279636

Epoch: 6| Step: 8
Training loss: 2.7313785552978516
Validation loss: 2.7446890390047463

Epoch: 6| Step: 9
Training loss: 2.374695301055908
Validation loss: 2.7474328061585784

Epoch: 6| Step: 10
Training loss: 2.5534677505493164
Validation loss: 2.7425524368081042

Epoch: 6| Step: 11
Training loss: 2.898221015930176
Validation loss: 2.7490143417030253

Epoch: 6| Step: 12
Training loss: 2.940763473510742
Validation loss: 2.743421123873803

Epoch: 6| Step: 13
Training loss: 2.9164915084838867
Validation loss: 2.738846758360504

Epoch: 63| Step: 0
Training loss: 2.7674732208251953
Validation loss: 2.7394349164860223

Epoch: 6| Step: 1
Training loss: 2.713632106781006
Validation loss: 2.7440455370051886

Epoch: 6| Step: 2
Training loss: 2.7766668796539307
Validation loss: 2.746635365229781

Epoch: 6| Step: 3
Training loss: 2.8888978958129883
Validation loss: 2.7423842517278527

Epoch: 6| Step: 4
Training loss: 2.951063632965088
Validation loss: 2.7434039577361076

Epoch: 6| Step: 5
Training loss: 2.891590118408203
Validation loss: 2.738009093910135

Epoch: 6| Step: 6
Training loss: 2.7245864868164062
Validation loss: 2.7444195491011425

Epoch: 6| Step: 7
Training loss: 2.981414318084717
Validation loss: 2.741281996491135

Epoch: 6| Step: 8
Training loss: 2.9118597507476807
Validation loss: 2.741074603091004

Epoch: 6| Step: 9
Training loss: 3.0196409225463867
Validation loss: 2.743895935755904

Epoch: 6| Step: 10
Training loss: 3.0122530460357666
Validation loss: 2.7350398750715357

Epoch: 6| Step: 11
Training loss: 2.5208616256713867
Validation loss: 2.7376150649080992

Epoch: 6| Step: 12
Training loss: 2.6215600967407227
Validation loss: 2.73180369664264

Epoch: 6| Step: 13
Training loss: 3.462265968322754
Validation loss: 2.7317914808950117

Epoch: 64| Step: 0
Training loss: 2.871445655822754
Validation loss: 2.731172900046072

Epoch: 6| Step: 1
Training loss: 3.693398952484131
Validation loss: 2.734289297493555

Epoch: 6| Step: 2
Training loss: 2.1093637943267822
Validation loss: 2.7330982505634265

Epoch: 6| Step: 3
Training loss: 3.189558506011963
Validation loss: 2.7434540640923286

Epoch: 6| Step: 4
Training loss: 2.6374049186706543
Validation loss: 2.748492835670389

Epoch: 6| Step: 5
Training loss: 2.571812868118286
Validation loss: 2.7504789675435712

Epoch: 6| Step: 6
Training loss: 3.4106578826904297
Validation loss: 2.748859033789686

Epoch: 6| Step: 7
Training loss: 2.43019962310791
Validation loss: 2.7373008010207966

Epoch: 6| Step: 8
Training loss: 3.0359573364257812
Validation loss: 2.7305759332513295

Epoch: 6| Step: 9
Training loss: 2.184636116027832
Validation loss: 2.727904809418545

Epoch: 6| Step: 10
Training loss: 2.972768783569336
Validation loss: 2.7246669466777513

Epoch: 6| Step: 11
Training loss: 2.5205464363098145
Validation loss: 2.7274303179915234

Epoch: 6| Step: 12
Training loss: 3.559746265411377
Validation loss: 2.7255822484211256

Epoch: 6| Step: 13
Training loss: 2.645428419113159
Validation loss: 2.72347528191023

Epoch: 65| Step: 0
Training loss: 2.3406028747558594
Validation loss: 2.7247908448660247

Epoch: 6| Step: 1
Training loss: 2.6548352241516113
Validation loss: 2.7227337668018956

Epoch: 6| Step: 2
Training loss: 2.1193976402282715
Validation loss: 2.7255159731834167

Epoch: 6| Step: 3
Training loss: 3.107881784439087
Validation loss: 2.722552890418678

Epoch: 6| Step: 4
Training loss: 2.339883804321289
Validation loss: 2.7276626043422247

Epoch: 6| Step: 5
Training loss: 3.18949556350708
Validation loss: 2.722354978643438

Epoch: 6| Step: 6
Training loss: 2.915158271789551
Validation loss: 2.723248397150347

Epoch: 6| Step: 7
Training loss: 2.8775460720062256
Validation loss: 2.720293221935149

Epoch: 6| Step: 8
Training loss: 3.0128304958343506
Validation loss: 2.7253366260118383

Epoch: 6| Step: 9
Training loss: 3.0088109970092773
Validation loss: 2.736531998521538

Epoch: 6| Step: 10
Training loss: 2.412782907485962
Validation loss: 2.7397349290950324

Epoch: 6| Step: 11
Training loss: 3.5346436500549316
Validation loss: 2.750229863710301

Epoch: 6| Step: 12
Training loss: 3.2404284477233887
Validation loss: 2.7557573113390195

Epoch: 6| Step: 13
Training loss: 3.297025442123413
Validation loss: 2.755645631462015

Epoch: 66| Step: 0
Training loss: 2.1574583053588867
Validation loss: 2.7573684338600404

Epoch: 6| Step: 1
Training loss: 3.264108896255493
Validation loss: 2.7565990545416392

Epoch: 6| Step: 2
Training loss: 4.0263519287109375
Validation loss: 2.7487544295608357

Epoch: 6| Step: 3
Training loss: 2.2215073108673096
Validation loss: 2.7447511124354538

Epoch: 6| Step: 4
Training loss: 2.6712799072265625
Validation loss: 2.738150768382575

Epoch: 6| Step: 5
Training loss: 2.770057201385498
Validation loss: 2.7360610808095625

Epoch: 6| Step: 6
Training loss: 3.7906529903411865
Validation loss: 2.7284070471281647

Epoch: 6| Step: 7
Training loss: 2.9380011558532715
Validation loss: 2.725849490011892

Epoch: 6| Step: 8
Training loss: 3.225281238555908
Validation loss: 2.7191658430202033

Epoch: 6| Step: 9
Training loss: 2.8301846981048584
Validation loss: 2.7221466392599125

Epoch: 6| Step: 10
Training loss: 2.544921636581421
Validation loss: 2.7189452930163314

Epoch: 6| Step: 11
Training loss: 2.591696262359619
Validation loss: 2.7200513116775022

Epoch: 6| Step: 12
Training loss: 2.576993465423584
Validation loss: 2.715844876022749

Epoch: 6| Step: 13
Training loss: 1.8027825355529785
Validation loss: 2.7170739686617287

Epoch: 67| Step: 0
Training loss: 3.8327226638793945
Validation loss: 2.7166485504437516

Epoch: 6| Step: 1
Training loss: 3.022477865219116
Validation loss: 2.7187384456716557

Epoch: 6| Step: 2
Training loss: 3.8573851585388184
Validation loss: 2.7160465537860827

Epoch: 6| Step: 3
Training loss: 1.8375829458236694
Validation loss: 2.716645384347567

Epoch: 6| Step: 4
Training loss: 2.73720645904541
Validation loss: 2.714656960579657

Epoch: 6| Step: 5
Training loss: 3.1720175743103027
Validation loss: 2.715297119591826

Epoch: 6| Step: 6
Training loss: 2.7249112129211426
Validation loss: 2.7142069698661886

Epoch: 6| Step: 7
Training loss: 3.2111573219299316
Validation loss: 2.716909254750898

Epoch: 6| Step: 8
Training loss: 2.8649826049804688
Validation loss: 2.711401095954321

Epoch: 6| Step: 9
Training loss: 1.6404660940170288
Validation loss: 2.7086881078699583

Epoch: 6| Step: 10
Training loss: 2.7810516357421875
Validation loss: 2.7145886882658927

Epoch: 6| Step: 11
Training loss: 2.596895694732666
Validation loss: 2.7123647633419243

Epoch: 6| Step: 12
Training loss: 2.9566256999969482
Validation loss: 2.7169494731451875

Epoch: 6| Step: 13
Training loss: 2.433847665786743
Validation loss: 2.7163401701117076

Epoch: 68| Step: 0
Training loss: 3.007132053375244
Validation loss: 2.724815196888421

Epoch: 6| Step: 1
Training loss: 2.5174601078033447
Validation loss: 2.725228314758629

Epoch: 6| Step: 2
Training loss: 2.7009377479553223
Validation loss: 2.7267997880135812

Epoch: 6| Step: 3
Training loss: 2.415287733078003
Validation loss: 2.722628693426809

Epoch: 6| Step: 4
Training loss: 2.5119872093200684
Validation loss: 2.7235047894139446

Epoch: 6| Step: 5
Training loss: 3.3230717182159424
Validation loss: 2.7198817396676667

Epoch: 6| Step: 6
Training loss: 2.13288950920105
Validation loss: 2.7237613918960735

Epoch: 6| Step: 7
Training loss: 4.18833589553833
Validation loss: 2.7250199779387443

Epoch: 6| Step: 8
Training loss: 3.146129846572876
Validation loss: 2.7195635841738794

Epoch: 6| Step: 9
Training loss: 2.3458199501037598
Validation loss: 2.722251087106684

Epoch: 6| Step: 10
Training loss: 2.73152494430542
Validation loss: 2.717886145396899

Epoch: 6| Step: 11
Training loss: 3.335899591445923
Validation loss: 2.7196842342294674

Epoch: 6| Step: 12
Training loss: 2.7140612602233887
Validation loss: 2.7176422201177126

Epoch: 6| Step: 13
Training loss: 2.494887351989746
Validation loss: 2.7172401989659956

Epoch: 69| Step: 0
Training loss: 3.176473617553711
Validation loss: 2.708748371370377

Epoch: 6| Step: 1
Training loss: 2.930863857269287
Validation loss: 2.700908507070234

Epoch: 6| Step: 2
Training loss: 3.018683910369873
Validation loss: 2.7048770535376763

Epoch: 6| Step: 3
Training loss: 3.335394859313965
Validation loss: 2.707641188816358

Epoch: 6| Step: 4
Training loss: 2.539112091064453
Validation loss: 2.714035539216893

Epoch: 6| Step: 5
Training loss: 2.9733080863952637
Validation loss: 2.709399423291606

Epoch: 6| Step: 6
Training loss: 2.473346710205078
Validation loss: 2.7106615445947133

Epoch: 6| Step: 7
Training loss: 2.1757664680480957
Validation loss: 2.7148469186598256

Epoch: 6| Step: 8
Training loss: 2.932016372680664
Validation loss: 2.7091015667043705

Epoch: 6| Step: 9
Training loss: 2.523219108581543
Validation loss: 2.7151813660898516

Epoch: 6| Step: 10
Training loss: 2.9560604095458984
Validation loss: 2.71611189329496

Epoch: 6| Step: 11
Training loss: 3.67109751701355
Validation loss: 2.7137116744954097

Epoch: 6| Step: 12
Training loss: 2.759471893310547
Validation loss: 2.7105854659952144

Epoch: 6| Step: 13
Training loss: 2.018155336380005
Validation loss: 2.7135663622169086

Epoch: 70| Step: 0
Training loss: 2.4333066940307617
Validation loss: 2.7078829529464885

Epoch: 6| Step: 1
Training loss: 3.3622140884399414
Validation loss: 2.7024372854540424

Epoch: 6| Step: 2
Training loss: 3.344672918319702
Validation loss: 2.7047585697584253

Epoch: 6| Step: 3
Training loss: 2.3211443424224854
Validation loss: 2.707949335857104

Epoch: 6| Step: 4
Training loss: 3.3068106174468994
Validation loss: 2.7038836376641386

Epoch: 6| Step: 5
Training loss: 2.770977258682251
Validation loss: 2.7029765446980796

Epoch: 6| Step: 6
Training loss: 3.0280628204345703
Validation loss: 2.7032410867752565

Epoch: 6| Step: 7
Training loss: 2.4800405502319336
Validation loss: 2.7010599079952446

Epoch: 6| Step: 8
Training loss: 2.234076499938965
Validation loss: 2.7059760888417563

Epoch: 6| Step: 9
Training loss: 2.42757248878479
Validation loss: 2.6989344807081324

Epoch: 6| Step: 10
Training loss: 2.9387197494506836
Validation loss: 2.6961006784951813

Epoch: 6| Step: 11
Training loss: 3.3829143047332764
Validation loss: 2.694278149194615

Epoch: 6| Step: 12
Training loss: 2.9730820655822754
Validation loss: 2.6986241981547368

Epoch: 6| Step: 13
Training loss: 2.613368034362793
Validation loss: 2.6983579820202244

Epoch: 71| Step: 0
Training loss: 2.6835341453552246
Validation loss: 2.693358880217357

Epoch: 6| Step: 1
Training loss: 3.018462657928467
Validation loss: 2.6971468899839666

Epoch: 6| Step: 2
Training loss: 2.905632972717285
Validation loss: 2.695933829071701

Epoch: 6| Step: 3
Training loss: 2.7119343280792236
Validation loss: 2.705406332528719

Epoch: 6| Step: 4
Training loss: 3.1381995677948
Validation loss: 2.711470134796635

Epoch: 6| Step: 5
Training loss: 3.9736671447753906
Validation loss: 2.7115050003092778

Epoch: 6| Step: 6
Training loss: 2.192452907562256
Validation loss: 2.715009779058477

Epoch: 6| Step: 7
Training loss: 3.445974588394165
Validation loss: 2.7164585539089736

Epoch: 6| Step: 8
Training loss: 2.786604404449463
Validation loss: 2.7148594292261268

Epoch: 6| Step: 9
Training loss: 3.0151798725128174
Validation loss: 2.7034814101393505

Epoch: 6| Step: 10
Training loss: 2.638573169708252
Validation loss: 2.6947289743731098

Epoch: 6| Step: 11
Training loss: 2.5113487243652344
Validation loss: 2.6916381774410123

Epoch: 6| Step: 12
Training loss: 1.8806538581848145
Validation loss: 2.6919745527287966

Epoch: 6| Step: 13
Training loss: 2.5625994205474854
Validation loss: 2.6863489509910665

Epoch: 72| Step: 0
Training loss: 2.9781384468078613
Validation loss: 2.6882138918804865

Epoch: 6| Step: 1
Training loss: 3.337217092514038
Validation loss: 2.6886107690872683

Epoch: 6| Step: 2
Training loss: 2.8150177001953125
Validation loss: 2.6888672305691625

Epoch: 6| Step: 3
Training loss: 2.0679986476898193
Validation loss: 2.689714111307616

Epoch: 6| Step: 4
Training loss: 2.43276309967041
Validation loss: 2.6918859943266837

Epoch: 6| Step: 5
Training loss: 3.437373161315918
Validation loss: 2.6933720086210515

Epoch: 6| Step: 6
Training loss: 2.9884636402130127
Validation loss: 2.6974241682278213

Epoch: 6| Step: 7
Training loss: 2.327324867248535
Validation loss: 2.6944832801818848

Epoch: 6| Step: 8
Training loss: 2.409562587738037
Validation loss: 2.6914009945366972

Epoch: 6| Step: 9
Training loss: 2.38218355178833
Validation loss: 2.694758822841029

Epoch: 6| Step: 10
Training loss: 3.1880600452423096
Validation loss: 2.693734351024833

Epoch: 6| Step: 11
Training loss: 2.717438220977783
Validation loss: 2.6946412722269693

Epoch: 6| Step: 12
Training loss: 2.952576160430908
Validation loss: 2.6943725155245875

Epoch: 6| Step: 13
Training loss: 4.03132438659668
Validation loss: 2.701184441966395

Epoch: 73| Step: 0
Training loss: 2.964465618133545
Validation loss: 2.701276130573724

Epoch: 6| Step: 1
Training loss: 3.3182425498962402
Validation loss: 2.7063905064777662

Epoch: 6| Step: 2
Training loss: 2.5081300735473633
Validation loss: 2.7079830656769457

Epoch: 6| Step: 3
Training loss: 3.180367946624756
Validation loss: 2.7092891687987954

Epoch: 6| Step: 4
Training loss: 2.615631580352783
Validation loss: 2.701322360705304

Epoch: 6| Step: 5
Training loss: 2.508969306945801
Validation loss: 2.698414059095485

Epoch: 6| Step: 6
Training loss: 3.3456954956054688
Validation loss: 2.694340944290161

Epoch: 6| Step: 7
Training loss: 3.2341740131378174
Validation loss: 2.687903173508183

Epoch: 6| Step: 8
Training loss: 2.7573890686035156
Validation loss: 2.6936148725530153

Epoch: 6| Step: 9
Training loss: 3.0121045112609863
Validation loss: 2.690614584953554

Epoch: 6| Step: 10
Training loss: 2.9866504669189453
Validation loss: 2.688239256540934

Epoch: 6| Step: 11
Training loss: 1.5442715883255005
Validation loss: 2.6871652885149886

Epoch: 6| Step: 12
Training loss: 2.1129770278930664
Validation loss: 2.686842341576853

Epoch: 6| Step: 13
Training loss: 3.8076913356781006
Validation loss: 2.6827495354478077

Epoch: 74| Step: 0
Training loss: 2.044311046600342
Validation loss: 2.6829983239532798

Epoch: 6| Step: 1
Training loss: 3.165071487426758
Validation loss: 2.679862704328311

Epoch: 6| Step: 2
Training loss: 2.9407358169555664
Validation loss: 2.6787698884164133

Epoch: 6| Step: 3
Training loss: 2.3197922706604004
Validation loss: 2.6791035564996863

Epoch: 6| Step: 4
Training loss: 3.640064239501953
Validation loss: 2.6815855938901185

Epoch: 6| Step: 5
Training loss: 2.8079628944396973
Validation loss: 2.6826707906620477

Epoch: 6| Step: 6
Training loss: 3.726963996887207
Validation loss: 2.680449426815074

Epoch: 6| Step: 7
Training loss: 2.0502800941467285
Validation loss: 2.68032173187502

Epoch: 6| Step: 8
Training loss: 2.4871835708618164
Validation loss: 2.6823856676778486

Epoch: 6| Step: 9
Training loss: 2.8708696365356445
Validation loss: 2.681256976178897

Epoch: 6| Step: 10
Training loss: 3.9338111877441406
Validation loss: 2.68004088247976

Epoch: 6| Step: 11
Training loss: 2.691761016845703
Validation loss: 2.679464045391288

Epoch: 6| Step: 12
Training loss: 2.2064337730407715
Validation loss: 2.6822436445502826

Epoch: 6| Step: 13
Training loss: 2.3676657676696777
Validation loss: 2.687624475007416

Epoch: 75| Step: 0
Training loss: 3.017869710922241
Validation loss: 2.6940380937309674

Epoch: 6| Step: 1
Training loss: 2.404806613922119
Validation loss: 2.711243996056177

Epoch: 6| Step: 2
Training loss: 2.5751757621765137
Validation loss: 2.727490504582723

Epoch: 6| Step: 3
Training loss: 3.3301339149475098
Validation loss: 2.726788951504615

Epoch: 6| Step: 4
Training loss: 3.468749761581421
Validation loss: 2.7272119265730663

Epoch: 6| Step: 5
Training loss: 2.7201828956604004
Validation loss: 2.7129348452373216

Epoch: 6| Step: 6
Training loss: 3.039435863494873
Validation loss: 2.7064651494385092

Epoch: 6| Step: 7
Training loss: 2.7406604290008545
Validation loss: 2.6896678068304576

Epoch: 6| Step: 8
Training loss: 2.025175094604492
Validation loss: 2.6881385567367717

Epoch: 6| Step: 9
Training loss: 2.5946991443634033
Validation loss: 2.682404266890659

Epoch: 6| Step: 10
Training loss: 2.668792724609375
Validation loss: 2.6770755039748324

Epoch: 6| Step: 11
Training loss: 2.45145320892334
Validation loss: 2.6769182528218916

Epoch: 6| Step: 12
Training loss: 3.1122145652770996
Validation loss: 2.6759331457076536

Epoch: 6| Step: 13
Training loss: 3.833826780319214
Validation loss: 2.675792158290904

Epoch: 76| Step: 0
Training loss: 3.0045690536499023
Validation loss: 2.673763164909937

Epoch: 6| Step: 1
Training loss: 2.8605942726135254
Validation loss: 2.6766328286099177

Epoch: 6| Step: 2
Training loss: 3.8230953216552734
Validation loss: 2.676042746472102

Epoch: 6| Step: 3
Training loss: 2.7424099445343018
Validation loss: 2.6788845805711645

Epoch: 6| Step: 4
Training loss: 3.0262346267700195
Validation loss: 2.6766546644190305

Epoch: 6| Step: 5
Training loss: 2.785696506500244
Validation loss: 2.681056478972076

Epoch: 6| Step: 6
Training loss: 2.65634822845459
Validation loss: 2.675816325731175

Epoch: 6| Step: 7
Training loss: 3.3100838661193848
Validation loss: 2.674722609981414

Epoch: 6| Step: 8
Training loss: 2.3882834911346436
Validation loss: 2.682054163307272

Epoch: 6| Step: 9
Training loss: 2.6633143424987793
Validation loss: 2.6875348321853147

Epoch: 6| Step: 10
Training loss: 2.7216362953186035
Validation loss: 2.686843328578498

Epoch: 6| Step: 11
Training loss: 2.5876896381378174
Validation loss: 2.6861898283804617

Epoch: 6| Step: 12
Training loss: 2.491360902786255
Validation loss: 2.6851142247517905

Epoch: 6| Step: 13
Training loss: 1.9464869499206543
Validation loss: 2.683499228569769

Epoch: 77| Step: 0
Training loss: 2.677051544189453
Validation loss: 2.6788956708805536

Epoch: 6| Step: 1
Training loss: 3.0454020500183105
Validation loss: 2.6835307972405547

Epoch: 6| Step: 2
Training loss: 2.7675230503082275
Validation loss: 2.6771395590997513

Epoch: 6| Step: 3
Training loss: 2.7739310264587402
Validation loss: 2.6824034901075464

Epoch: 6| Step: 4
Training loss: 2.610680341720581
Validation loss: 2.683613246487033

Epoch: 6| Step: 5
Training loss: 2.903362274169922
Validation loss: 2.6809807951732347

Epoch: 6| Step: 6
Training loss: 3.6640892028808594
Validation loss: 2.6760696057350404

Epoch: 6| Step: 7
Training loss: 2.845770835876465
Validation loss: 2.6741678509660947

Epoch: 6| Step: 8
Training loss: 1.9932019710540771
Validation loss: 2.672192429983488

Epoch: 6| Step: 9
Training loss: 2.5127663612365723
Validation loss: 2.6709212487743748

Epoch: 6| Step: 10
Training loss: 2.35749888420105
Validation loss: 2.668538203803442

Epoch: 6| Step: 11
Training loss: 2.6648306846618652
Validation loss: 2.6650572694757932

Epoch: 6| Step: 12
Training loss: 3.5982627868652344
Validation loss: 2.6671423168592554

Epoch: 6| Step: 13
Training loss: 2.960414409637451
Validation loss: 2.6697201908275647

Epoch: 78| Step: 0
Training loss: 2.0786654949188232
Validation loss: 2.668009099139962

Epoch: 6| Step: 1
Training loss: 1.807223916053772
Validation loss: 2.6663813975549515

Epoch: 6| Step: 2
Training loss: 3.4864375591278076
Validation loss: 2.669181449438936

Epoch: 6| Step: 3
Training loss: 2.762216329574585
Validation loss: 2.6684357658509286

Epoch: 6| Step: 4
Training loss: 2.6415038108825684
Validation loss: 2.6676734749988844

Epoch: 6| Step: 5
Training loss: 3.1586649417877197
Validation loss: 2.6653287487645305

Epoch: 6| Step: 6
Training loss: 2.9239931106567383
Validation loss: 2.665196831508349

Epoch: 6| Step: 7
Training loss: 2.5143022537231445
Validation loss: 2.6655977438854914

Epoch: 6| Step: 8
Training loss: 2.4309306144714355
Validation loss: 2.6670635566916516

Epoch: 6| Step: 9
Training loss: 3.325380563735962
Validation loss: 2.6657811082819456

Epoch: 6| Step: 10
Training loss: 2.987664222717285
Validation loss: 2.663304741664599

Epoch: 6| Step: 11
Training loss: 3.965913772583008
Validation loss: 2.666482279377599

Epoch: 6| Step: 12
Training loss: 2.6843111515045166
Validation loss: 2.666332208982078

Epoch: 6| Step: 13
Training loss: 2.2873425483703613
Validation loss: 2.6644586414419194

Epoch: 79| Step: 0
Training loss: 2.9785869121551514
Validation loss: 2.6658235057707755

Epoch: 6| Step: 1
Training loss: 2.880737781524658
Validation loss: 2.6709179493688766

Epoch: 6| Step: 2
Training loss: 3.1701576709747314
Validation loss: 2.666735697818059

Epoch: 6| Step: 3
Training loss: 2.941480875015259
Validation loss: 2.6672982733736754

Epoch: 6| Step: 4
Training loss: 2.318734884262085
Validation loss: 2.6662853687040267

Epoch: 6| Step: 5
Training loss: 2.2375781536102295
Validation loss: 2.667914767419138

Epoch: 6| Step: 6
Training loss: 2.296273708343506
Validation loss: 2.664301908144387

Epoch: 6| Step: 7
Training loss: 2.670515298843384
Validation loss: 2.66606726184968

Epoch: 6| Step: 8
Training loss: 2.0098936557769775
Validation loss: 2.667509804489792

Epoch: 6| Step: 9
Training loss: 3.2841134071350098
Validation loss: 2.6676864290750153

Epoch: 6| Step: 10
Training loss: 2.8084909915924072
Validation loss: 2.664029070126113

Epoch: 6| Step: 11
Training loss: 3.808182954788208
Validation loss: 2.6649466483823714

Epoch: 6| Step: 12
Training loss: 3.3328158855438232
Validation loss: 2.6647042817966913

Epoch: 6| Step: 13
Training loss: 2.144770383834839
Validation loss: 2.666717526733234

Epoch: 80| Step: 0
Training loss: 3.313361883163452
Validation loss: 2.662118419524162

Epoch: 6| Step: 1
Training loss: 2.80172061920166
Validation loss: 2.6666210287360737

Epoch: 6| Step: 2
Training loss: 2.6150074005126953
Validation loss: 2.6727536263004428

Epoch: 6| Step: 3
Training loss: 2.6020421981811523
Validation loss: 2.672942579433482

Epoch: 6| Step: 4
Training loss: 3.341127395629883
Validation loss: 2.670009346418483

Epoch: 6| Step: 5
Training loss: 2.225874423980713
Validation loss: 2.673348831874068

Epoch: 6| Step: 6
Training loss: 2.6078076362609863
Validation loss: 2.6632936616097727

Epoch: 6| Step: 7
Training loss: 2.950565814971924
Validation loss: 2.663052543517082

Epoch: 6| Step: 8
Training loss: 3.21832275390625
Validation loss: 2.661995239155267

Epoch: 6| Step: 9
Training loss: 2.774657726287842
Validation loss: 2.659715583247523

Epoch: 6| Step: 10
Training loss: 2.1911983489990234
Validation loss: 2.668624947147985

Epoch: 6| Step: 11
Training loss: 3.138849973678589
Validation loss: 2.6649057634415163

Epoch: 6| Step: 12
Training loss: 2.449326515197754
Validation loss: 2.6620664340193554

Epoch: 6| Step: 13
Training loss: 3.2024240493774414
Validation loss: 2.6632091435053016

Epoch: 81| Step: 0
Training loss: 3.064985513687134
Validation loss: 2.660541549805672

Epoch: 6| Step: 1
Training loss: 2.312187671661377
Validation loss: 2.663267720130182

Epoch: 6| Step: 2
Training loss: 2.6056108474731445
Validation loss: 2.6677615668184016

Epoch: 6| Step: 3
Training loss: 3.433759927749634
Validation loss: 2.662772196595387

Epoch: 6| Step: 4
Training loss: 2.7571935653686523
Validation loss: 2.660731764249904

Epoch: 6| Step: 5
Training loss: 2.3087992668151855
Validation loss: 2.6619869483414518

Epoch: 6| Step: 6
Training loss: 2.5107595920562744
Validation loss: 2.6586714226712465

Epoch: 6| Step: 7
Training loss: 2.4162063598632812
Validation loss: 2.6598627516018447

Epoch: 6| Step: 8
Training loss: 2.299832820892334
Validation loss: 2.6633274247569423

Epoch: 6| Step: 9
Training loss: 3.109701156616211
Validation loss: 2.6612299052617883

Epoch: 6| Step: 10
Training loss: 2.4088692665100098
Validation loss: 2.655189665414954

Epoch: 6| Step: 11
Training loss: 3.937305212020874
Validation loss: 2.664829756623955

Epoch: 6| Step: 12
Training loss: 3.2567086219787598
Validation loss: 2.662497684519778

Epoch: 6| Step: 13
Training loss: 2.766531229019165
Validation loss: 2.6634089152018228

Epoch: 82| Step: 0
Training loss: 2.5841665267944336
Validation loss: 2.656333159374934

Epoch: 6| Step: 1
Training loss: 2.704063653945923
Validation loss: 2.6594702941115185

Epoch: 6| Step: 2
Training loss: 2.682234048843384
Validation loss: 2.6569276420019006

Epoch: 6| Step: 3
Training loss: 2.9899680614471436
Validation loss: 2.659192018611457

Epoch: 6| Step: 4
Training loss: 2.94842529296875
Validation loss: 2.655513325045186

Epoch: 6| Step: 5
Training loss: 3.2504544258117676
Validation loss: 2.658101035702613

Epoch: 6| Step: 6
Training loss: 2.641876459121704
Validation loss: 2.659994553494197

Epoch: 6| Step: 7
Training loss: 1.9488492012023926
Validation loss: 2.6557712836932112

Epoch: 6| Step: 8
Training loss: 2.6952595710754395
Validation loss: 2.6565122168551207

Epoch: 6| Step: 9
Training loss: 2.762531280517578
Validation loss: 2.6565780203829528

Epoch: 6| Step: 10
Training loss: 3.186405658721924
Validation loss: 2.6548548590752388

Epoch: 6| Step: 11
Training loss: 2.8116073608398438
Validation loss: 2.651973757692563

Epoch: 6| Step: 12
Training loss: 3.2644309997558594
Validation loss: 2.654673740427981

Epoch: 6| Step: 13
Training loss: 2.661881923675537
Validation loss: 2.6559475980779177

Epoch: 83| Step: 0
Training loss: 3.0902254581451416
Validation loss: 2.6566817273375807

Epoch: 6| Step: 1
Training loss: 3.160874843597412
Validation loss: 2.6552070366439

Epoch: 6| Step: 2
Training loss: 1.7999554872512817
Validation loss: 2.6503300923173145

Epoch: 6| Step: 3
Training loss: 3.8771653175354004
Validation loss: 2.653507314702516

Epoch: 6| Step: 4
Training loss: 2.4402518272399902
Validation loss: 2.652197563520042

Epoch: 6| Step: 5
Training loss: 4.019256591796875
Validation loss: 2.649346923315397

Epoch: 6| Step: 6
Training loss: 3.002261161804199
Validation loss: 2.6496382195462465

Epoch: 6| Step: 7
Training loss: 3.6473822593688965
Validation loss: 2.6485722090608332

Epoch: 6| Step: 8
Training loss: 2.562974452972412
Validation loss: 2.655660634399742

Epoch: 6| Step: 9
Training loss: 1.9724476337432861
Validation loss: 2.6506875497038647

Epoch: 6| Step: 10
Training loss: 1.5825181007385254
Validation loss: 2.6517408765772337

Epoch: 6| Step: 11
Training loss: 2.5244369506835938
Validation loss: 2.657267047512916

Epoch: 6| Step: 12
Training loss: 2.843209981918335
Validation loss: 2.652690851560203

Epoch: 6| Step: 13
Training loss: 2.3801896572113037
Validation loss: 2.6496994136482157

Epoch: 84| Step: 0
Training loss: 2.4515414237976074
Validation loss: 2.654483305510654

Epoch: 6| Step: 1
Training loss: 2.548642158508301
Validation loss: 2.653935881071193

Epoch: 6| Step: 2
Training loss: 2.6317474842071533
Validation loss: 2.6521201082455215

Epoch: 6| Step: 3
Training loss: 3.2965025901794434
Validation loss: 2.6580140154848815

Epoch: 6| Step: 4
Training loss: 2.1685991287231445
Validation loss: 2.6523578269507295

Epoch: 6| Step: 5
Training loss: 2.7259814739227295
Validation loss: 2.6584246209872666

Epoch: 6| Step: 6
Training loss: 2.707831382751465
Validation loss: 2.6572128777862876

Epoch: 6| Step: 7
Training loss: 3.041922092437744
Validation loss: 2.65426347332616

Epoch: 6| Step: 8
Training loss: 3.0845372676849365
Validation loss: 2.654098823506345

Epoch: 6| Step: 9
Training loss: 3.1838059425354004
Validation loss: 2.6543088856563775

Epoch: 6| Step: 10
Training loss: 3.379899024963379
Validation loss: 2.648711835184405

Epoch: 6| Step: 11
Training loss: 2.356072425842285
Validation loss: 2.648615316678119

Epoch: 6| Step: 12
Training loss: 3.1736083030700684
Validation loss: 2.6465483609066216

Epoch: 6| Step: 13
Training loss: 2.05718994140625
Validation loss: 2.647660199032035

Epoch: 85| Step: 0
Training loss: 2.419226884841919
Validation loss: 2.6470872484227663

Epoch: 6| Step: 1
Training loss: 2.8304975032806396
Validation loss: 2.6493249426605883

Epoch: 6| Step: 2
Training loss: 4.281013011932373
Validation loss: 2.6488125042248796

Epoch: 6| Step: 3
Training loss: 3.2018191814422607
Validation loss: 2.6489015702278382

Epoch: 6| Step: 4
Training loss: 2.8617303371429443
Validation loss: 2.6511652418362197

Epoch: 6| Step: 5
Training loss: 1.9334475994110107
Validation loss: 2.6529844935222338

Epoch: 6| Step: 6
Training loss: 3.175105571746826
Validation loss: 2.6531732723277104

Epoch: 6| Step: 7
Training loss: 2.1250107288360596
Validation loss: 2.6550279484000257

Epoch: 6| Step: 8
Training loss: 3.186244010925293
Validation loss: 2.660131090430803

Epoch: 6| Step: 9
Training loss: 2.9166712760925293
Validation loss: 2.668621693888018

Epoch: 6| Step: 10
Training loss: 3.067720413208008
Validation loss: 2.660470788196851

Epoch: 6| Step: 11
Training loss: 2.5500738620758057
Validation loss: 2.6577000489798923

Epoch: 6| Step: 12
Training loss: 2.182493209838867
Validation loss: 2.655309131068568

Epoch: 6| Step: 13
Training loss: 2.089766502380371
Validation loss: 2.6544392749827397

Epoch: 86| Step: 0
Training loss: 3.1314234733581543
Validation loss: 2.6484113277927523

Epoch: 6| Step: 1
Training loss: 2.308697462081909
Validation loss: 2.6473103364308677

Epoch: 6| Step: 2
Training loss: 2.8060784339904785
Validation loss: 2.6445368361729447

Epoch: 6| Step: 3
Training loss: 3.3516383171081543
Validation loss: 2.6411879370289464

Epoch: 6| Step: 4
Training loss: 3.1071102619171143
Validation loss: 2.6454813711104856

Epoch: 6| Step: 5
Training loss: 2.3794121742248535
Validation loss: 2.644168781977828

Epoch: 6| Step: 6
Training loss: 2.502180576324463
Validation loss: 2.645995304148684

Epoch: 6| Step: 7
Training loss: 2.824253559112549
Validation loss: 2.6443844533735708

Epoch: 6| Step: 8
Training loss: 3.467156410217285
Validation loss: 2.6464395266707226

Epoch: 6| Step: 9
Training loss: 2.8518686294555664
Validation loss: 2.6432549927824285

Epoch: 6| Step: 10
Training loss: 3.122300863265991
Validation loss: 2.6465276748903337

Epoch: 6| Step: 11
Training loss: 2.8842835426330566
Validation loss: 2.6419168620981197

Epoch: 6| Step: 12
Training loss: 1.755704641342163
Validation loss: 2.6446083899467223

Epoch: 6| Step: 13
Training loss: 2.31442928314209
Validation loss: 2.643071507894865

Epoch: 87| Step: 0
Training loss: 2.6185054779052734
Validation loss: 2.6467310997747604

Epoch: 6| Step: 1
Training loss: 2.32185697555542
Validation loss: 2.646740569863268

Epoch: 6| Step: 2
Training loss: 2.097559928894043
Validation loss: 2.6511429535445346

Epoch: 6| Step: 3
Training loss: 3.6804957389831543
Validation loss: 2.655834964526597

Epoch: 6| Step: 4
Training loss: 2.4570765495300293
Validation loss: 2.6533861006459882

Epoch: 6| Step: 5
Training loss: 2.723501205444336
Validation loss: 2.655018837221207

Epoch: 6| Step: 6
Training loss: 3.8912808895111084
Validation loss: 2.654591962855349

Epoch: 6| Step: 7
Training loss: 2.826690673828125
Validation loss: 2.647851723496632

Epoch: 6| Step: 8
Training loss: 2.581131935119629
Validation loss: 2.6482915339931363

Epoch: 6| Step: 9
Training loss: 2.477997303009033
Validation loss: 2.64376893607519

Epoch: 6| Step: 10
Training loss: 2.7337207794189453
Validation loss: 2.6485984402318157

Epoch: 6| Step: 11
Training loss: 3.2819666862487793
Validation loss: 2.6569464591241654

Epoch: 6| Step: 12
Training loss: 3.2506651878356934
Validation loss: 2.653849027490103

Epoch: 6| Step: 13
Training loss: 1.5320966243743896
Validation loss: 2.659585278521302

Epoch: 88| Step: 0
Training loss: 3.0976858139038086
Validation loss: 2.6463239346781084

Epoch: 6| Step: 1
Training loss: 3.144167423248291
Validation loss: 2.6411217387004564

Epoch: 6| Step: 2
Training loss: 2.8742966651916504
Validation loss: 2.6350532065155687

Epoch: 6| Step: 3
Training loss: 2.942436933517456
Validation loss: 2.6399403669500865

Epoch: 6| Step: 4
Training loss: 1.862250804901123
Validation loss: 2.63968494630629

Epoch: 6| Step: 5
Training loss: 2.760652542114258
Validation loss: 2.6398874098254788

Epoch: 6| Step: 6
Training loss: 2.5205163955688477
Validation loss: 2.639227787653605

Epoch: 6| Step: 7
Training loss: 2.537018299102783
Validation loss: 2.640417229744696

Epoch: 6| Step: 8
Training loss: 2.2676854133605957
Validation loss: 2.6401731942289617

Epoch: 6| Step: 9
Training loss: 3.118851661682129
Validation loss: 2.6373941616345475

Epoch: 6| Step: 10
Training loss: 3.178525686264038
Validation loss: 2.6379210846398466

Epoch: 6| Step: 11
Training loss: 2.449284315109253
Validation loss: 2.64021647104653

Epoch: 6| Step: 12
Training loss: 2.551164150238037
Validation loss: 2.6377313624146166

Epoch: 6| Step: 13
Training loss: 4.28987979888916
Validation loss: 2.6410564402098298

Epoch: 89| Step: 0
Training loss: 2.3376822471618652
Validation loss: 2.638265925069009

Epoch: 6| Step: 1
Training loss: 3.297637939453125
Validation loss: 2.638219725701117

Epoch: 6| Step: 2
Training loss: 2.8686156272888184
Validation loss: 2.646776250613633

Epoch: 6| Step: 3
Training loss: 2.6506667137145996
Validation loss: 2.6448438013753583

Epoch: 6| Step: 4
Training loss: 2.9607911109924316
Validation loss: 2.648760695611277

Epoch: 6| Step: 5
Training loss: 2.6866490840911865
Validation loss: 2.6448848965347453

Epoch: 6| Step: 6
Training loss: 2.6604409217834473
Validation loss: 2.657044674760552

Epoch: 6| Step: 7
Training loss: 2.486335039138794
Validation loss: 2.6550574430855374

Epoch: 6| Step: 8
Training loss: 2.807094097137451
Validation loss: 2.650704529977614

Epoch: 6| Step: 9
Training loss: 3.3703792095184326
Validation loss: 2.643316138175226

Epoch: 6| Step: 10
Training loss: 2.803074836730957
Validation loss: 2.6415064745051886

Epoch: 6| Step: 11
Training loss: 2.7041521072387695
Validation loss: 2.6410989915170977

Epoch: 6| Step: 12
Training loss: 2.6113476753234863
Validation loss: 2.6396465083604217

Epoch: 6| Step: 13
Training loss: 2.599151134490967
Validation loss: 2.6360355141342326

Epoch: 90| Step: 0
Training loss: 3.1017417907714844
Validation loss: 2.6307484770333893

Epoch: 6| Step: 1
Training loss: 2.262425184249878
Validation loss: 2.633151787583546

Epoch: 6| Step: 2
Training loss: 2.5000109672546387
Validation loss: 2.63650542946272

Epoch: 6| Step: 3
Training loss: 2.8146677017211914
Validation loss: 2.6338986889008553

Epoch: 6| Step: 4
Training loss: 2.974170684814453
Validation loss: 2.633210515463224

Epoch: 6| Step: 5
Training loss: 2.6161794662475586
Validation loss: 2.6358166740786646

Epoch: 6| Step: 6
Training loss: 2.130288600921631
Validation loss: 2.633085302127305

Epoch: 6| Step: 7
Training loss: 2.3155460357666016
Validation loss: 2.634729887849541

Epoch: 6| Step: 8
Training loss: 3.2697365283966064
Validation loss: 2.6312159389577885

Epoch: 6| Step: 9
Training loss: 3.010953903198242
Validation loss: 2.63331252016047

Epoch: 6| Step: 10
Training loss: 2.977499485015869
Validation loss: 2.6338541930721653

Epoch: 6| Step: 11
Training loss: 2.9627609252929688
Validation loss: 2.6289594352886243

Epoch: 6| Step: 12
Training loss: 3.1348485946655273
Validation loss: 2.6322635886489705

Epoch: 6| Step: 13
Training loss: 2.9489736557006836
Validation loss: 2.6330223365496566

Epoch: 91| Step: 0
Training loss: 2.5377893447875977
Validation loss: 2.6311279625021

Epoch: 6| Step: 1
Training loss: 2.791440486907959
Validation loss: 2.6291157840400614

Epoch: 6| Step: 2
Training loss: 3.1768758296966553
Validation loss: 2.6286297049573673

Epoch: 6| Step: 3
Training loss: 3.1238768100738525
Validation loss: 2.6297824664782454

Epoch: 6| Step: 4
Training loss: 2.2211389541625977
Validation loss: 2.6299710504470335

Epoch: 6| Step: 5
Training loss: 3.5370514392852783
Validation loss: 2.6302834480039534

Epoch: 6| Step: 6
Training loss: 2.3382763862609863
Validation loss: 2.636008039597542

Epoch: 6| Step: 7
Training loss: 3.282918930053711
Validation loss: 2.6280686624588503

Epoch: 6| Step: 8
Training loss: 2.7268309593200684
Validation loss: 2.6314863415174585

Epoch: 6| Step: 9
Training loss: 2.3933792114257812
Validation loss: 2.63026947616249

Epoch: 6| Step: 10
Training loss: 2.2998950481414795
Validation loss: 2.6362770398457847

Epoch: 6| Step: 11
Training loss: 2.673537492752075
Validation loss: 2.6347211663440993

Epoch: 6| Step: 12
Training loss: 2.5723800659179688
Validation loss: 2.639286151496313

Epoch: 6| Step: 13
Training loss: 3.4249277114868164
Validation loss: 2.6352058379880843

Epoch: 92| Step: 0
Training loss: 3.164837598800659
Validation loss: 2.6377005551450994

Epoch: 6| Step: 1
Training loss: 2.611875057220459
Validation loss: 2.6405956822056926

Epoch: 6| Step: 2
Training loss: 2.7275960445404053
Validation loss: 2.6482963664557344

Epoch: 6| Step: 3
Training loss: 2.7984933853149414
Validation loss: 2.6431269389326855

Epoch: 6| Step: 4
Training loss: 3.569368839263916
Validation loss: 2.639958276543566

Epoch: 6| Step: 5
Training loss: 2.156341314315796
Validation loss: 2.6375634695893977

Epoch: 6| Step: 6
Training loss: 2.5679891109466553
Validation loss: 2.6392077810020855

Epoch: 6| Step: 7
Training loss: 3.3088440895080566
Validation loss: 2.634538955585931

Epoch: 6| Step: 8
Training loss: 3.079249858856201
Validation loss: 2.6324202706736903

Epoch: 6| Step: 9
Training loss: 2.2820067405700684
Validation loss: 2.6286313123600458

Epoch: 6| Step: 10
Training loss: 2.6879196166992188
Validation loss: 2.6278526603534655

Epoch: 6| Step: 11
Training loss: 2.3509132862091064
Validation loss: 2.630179371885074

Epoch: 6| Step: 12
Training loss: 3.0038557052612305
Validation loss: 2.62787809166857

Epoch: 6| Step: 13
Training loss: 2.3222360610961914
Validation loss: 2.625733155076222

Epoch: 93| Step: 0
Training loss: 2.4540772438049316
Validation loss: 2.626968096661311

Epoch: 6| Step: 1
Training loss: 3.210141181945801
Validation loss: 2.6286904222221783

Epoch: 6| Step: 2
Training loss: 3.2641892433166504
Validation loss: 2.6253171659285024

Epoch: 6| Step: 3
Training loss: 3.231947183609009
Validation loss: 2.6292514262660855

Epoch: 6| Step: 4
Training loss: 3.193242311477661
Validation loss: 2.6256073918393863

Epoch: 6| Step: 5
Training loss: 2.1311516761779785
Validation loss: 2.627959333440309

Epoch: 6| Step: 6
Training loss: 3.085949659347534
Validation loss: 2.629471681451285

Epoch: 6| Step: 7
Training loss: 3.1726536750793457
Validation loss: 2.6262605728641635

Epoch: 6| Step: 8
Training loss: 2.1675589084625244
Validation loss: 2.629114717565557

Epoch: 6| Step: 9
Training loss: 2.6702518463134766
Validation loss: 2.6327258028009886

Epoch: 6| Step: 10
Training loss: 2.9963791370391846
Validation loss: 2.6259548766638643

Epoch: 6| Step: 11
Training loss: 2.6494498252868652
Validation loss: 2.626386370710147

Epoch: 6| Step: 12
Training loss: 2.2122066020965576
Validation loss: 2.622464441484021

Epoch: 6| Step: 13
Training loss: 2.2526636123657227
Validation loss: 2.6193698657456266

Epoch: 94| Step: 0
Training loss: 2.111968994140625
Validation loss: 2.6231499461717505

Epoch: 6| Step: 1
Training loss: 3.173825979232788
Validation loss: 2.622994540840067

Epoch: 6| Step: 2
Training loss: 3.156074285507202
Validation loss: 2.624917839163093

Epoch: 6| Step: 3
Training loss: 3.717851161956787
Validation loss: 2.6239523195451304

Epoch: 6| Step: 4
Training loss: 3.6512155532836914
Validation loss: 2.623457649702667

Epoch: 6| Step: 5
Training loss: 2.8883395195007324
Validation loss: 2.626803695514638

Epoch: 6| Step: 6
Training loss: 2.1702284812927246
Validation loss: 2.630122725681592

Epoch: 6| Step: 7
Training loss: 2.3835091590881348
Validation loss: 2.634925303920623

Epoch: 6| Step: 8
Training loss: 2.2845213413238525
Validation loss: 2.6335970278709167

Epoch: 6| Step: 9
Training loss: 2.5052638053894043
Validation loss: 2.629605177910097

Epoch: 6| Step: 10
Training loss: 3.881523370742798
Validation loss: 2.6289764399169595

Epoch: 6| Step: 11
Training loss: 2.4073877334594727
Validation loss: 2.6222164246343795

Epoch: 6| Step: 12
Training loss: 2.1252548694610596
Validation loss: 2.6207867207065707

Epoch: 6| Step: 13
Training loss: 2.0064148902893066
Validation loss: 2.625122718913581

Epoch: 95| Step: 0
Training loss: 3.1706321239471436
Validation loss: 2.625213853774532

Epoch: 6| Step: 1
Training loss: 2.799642324447632
Validation loss: 2.6215151740658666

Epoch: 6| Step: 2
Training loss: 2.3223252296447754
Validation loss: 2.6246471174301638

Epoch: 6| Step: 3
Training loss: 2.989483118057251
Validation loss: 2.6252792266107376

Epoch: 6| Step: 4
Training loss: 3.122068405151367
Validation loss: 2.6234636691308792

Epoch: 6| Step: 5
Training loss: 1.823043942451477
Validation loss: 2.6244186560312905

Epoch: 6| Step: 6
Training loss: 3.239903450012207
Validation loss: 2.623407745874056

Epoch: 6| Step: 7
Training loss: 2.8038206100463867
Validation loss: 2.6235080765139673

Epoch: 6| Step: 8
Training loss: 3.1441500186920166
Validation loss: 2.6209889996436333

Epoch: 6| Step: 9
Training loss: 3.001587152481079
Validation loss: 2.620672292606805

Epoch: 6| Step: 10
Training loss: 2.606422185897827
Validation loss: 2.6272678426516953

Epoch: 6| Step: 11
Training loss: 2.169541835784912
Validation loss: 2.624645807409799

Epoch: 6| Step: 12
Training loss: 2.865574836730957
Validation loss: 2.6228438474798716

Epoch: 6| Step: 13
Training loss: 2.6840124130249023
Validation loss: 2.6278303028434835

Epoch: 96| Step: 0
Training loss: 2.3028202056884766
Validation loss: 2.623792540642523

Epoch: 6| Step: 1
Training loss: 3.329498291015625
Validation loss: 2.638616218361803

Epoch: 6| Step: 2
Training loss: 3.2615809440612793
Validation loss: 2.634312768136301

Epoch: 6| Step: 3
Training loss: 3.4381895065307617
Validation loss: 2.638106815276607

Epoch: 6| Step: 4
Training loss: 2.764237880706787
Validation loss: 2.6342560501508814

Epoch: 6| Step: 5
Training loss: 2.7372801303863525
Validation loss: 2.6254705126567552

Epoch: 6| Step: 6
Training loss: 2.483642101287842
Validation loss: 2.622072890240659

Epoch: 6| Step: 7
Training loss: 2.495903491973877
Validation loss: 2.6161653944241103

Epoch: 6| Step: 8
Training loss: 2.77988338470459
Validation loss: 2.618596871693929

Epoch: 6| Step: 9
Training loss: 3.520785331726074
Validation loss: 2.6219772087630404

Epoch: 6| Step: 10
Training loss: 2.7273571491241455
Validation loss: 2.6247770196648053

Epoch: 6| Step: 11
Training loss: 2.218092679977417
Validation loss: 2.627791363705871

Epoch: 6| Step: 12
Training loss: 2.542996883392334
Validation loss: 2.6265757160802043

Epoch: 6| Step: 13
Training loss: 1.9700303077697754
Validation loss: 2.6307544118614605

Epoch: 97| Step: 0
Training loss: 2.051455020904541
Validation loss: 2.624536583500524

Epoch: 6| Step: 1
Training loss: 3.0279407501220703
Validation loss: 2.6263158270107803

Epoch: 6| Step: 2
Training loss: 3.0552382469177246
Validation loss: 2.627970257113057

Epoch: 6| Step: 3
Training loss: 2.9309816360473633
Validation loss: 2.622793884687526

Epoch: 6| Step: 4
Training loss: 2.216024875640869
Validation loss: 2.623725365566951

Epoch: 6| Step: 5
Training loss: 3.180614709854126
Validation loss: 2.620660420387022

Epoch: 6| Step: 6
Training loss: 3.0524628162384033
Validation loss: 2.620145905402399

Epoch: 6| Step: 7
Training loss: 2.5646817684173584
Validation loss: 2.6195827581549205

Epoch: 6| Step: 8
Training loss: 2.9048733711242676
Validation loss: 2.61620323375989

Epoch: 6| Step: 9
Training loss: 3.466930866241455
Validation loss: 2.618899937598936

Epoch: 6| Step: 10
Training loss: 2.8837637901306152
Validation loss: 2.622296953714022

Epoch: 6| Step: 11
Training loss: 2.5622479915618896
Validation loss: 2.6321216552488265

Epoch: 6| Step: 12
Training loss: 2.190371036529541
Validation loss: 2.6319728461644982

Epoch: 6| Step: 13
Training loss: 2.7700390815734863
Validation loss: 2.641758895689441

Epoch: 98| Step: 0
Training loss: 1.8477939367294312
Validation loss: 2.642235989211708

Epoch: 6| Step: 1
Training loss: 2.2735815048217773
Validation loss: 2.6358346169994724

Epoch: 6| Step: 2
Training loss: 3.5329957008361816
Validation loss: 2.6299540970915105

Epoch: 6| Step: 3
Training loss: 2.5753896236419678
Validation loss: 2.632712653888169

Epoch: 6| Step: 4
Training loss: 2.6070287227630615
Validation loss: 2.6301153052237725

Epoch: 6| Step: 5
Training loss: 3.4439454078674316
Validation loss: 2.640524366850494

Epoch: 6| Step: 6
Training loss: 3.1167643070220947
Validation loss: 2.639314912980603

Epoch: 6| Step: 7
Training loss: 2.7882161140441895
Validation loss: 2.6382897361632316

Epoch: 6| Step: 8
Training loss: 2.3605716228485107
Validation loss: 2.638088259645688

Epoch: 6| Step: 9
Training loss: 2.5566391944885254
Validation loss: 2.6314861056625203

Epoch: 6| Step: 10
Training loss: 3.4026989936828613
Validation loss: 2.6268682710586058

Epoch: 6| Step: 11
Training loss: 2.806882381439209
Validation loss: 2.6201235350742134

Epoch: 6| Step: 12
Training loss: 3.0267724990844727
Validation loss: 2.6110542512709096

Epoch: 6| Step: 13
Training loss: 2.2801313400268555
Validation loss: 2.6113892063017814

Epoch: 99| Step: 0
Training loss: 2.6881744861602783
Validation loss: 2.6161894823915217

Epoch: 6| Step: 1
Training loss: 2.30373215675354
Validation loss: 2.614759709245415

Epoch: 6| Step: 2
Training loss: 3.1667897701263428
Validation loss: 2.6113391666002173

Epoch: 6| Step: 3
Training loss: 2.0960755348205566
Validation loss: 2.612236933041644

Epoch: 6| Step: 4
Training loss: 2.933429479598999
Validation loss: 2.619442824394472

Epoch: 6| Step: 5
Training loss: 3.023042917251587
Validation loss: 2.6160733776707805

Epoch: 6| Step: 6
Training loss: 2.2869648933410645
Validation loss: 2.613795498365997

Epoch: 6| Step: 7
Training loss: 2.189457893371582
Validation loss: 2.6111242027692896

Epoch: 6| Step: 8
Training loss: 2.8231468200683594
Validation loss: 2.6235343025576685

Epoch: 6| Step: 9
Training loss: 2.618122100830078
Validation loss: 2.6252492140698176

Epoch: 6| Step: 10
Training loss: 3.5576746463775635
Validation loss: 2.6141143178427093

Epoch: 6| Step: 11
Training loss: 3.010122537612915
Validation loss: 2.611717149775515

Epoch: 6| Step: 12
Training loss: 3.3215818405151367
Validation loss: 2.6120612313670497

Epoch: 6| Step: 13
Training loss: 2.8184597492218018
Validation loss: 2.6102827800217496

Epoch: 100| Step: 0
Training loss: 2.886375904083252
Validation loss: 2.60961909960675

Epoch: 6| Step: 1
Training loss: 3.366611957550049
Validation loss: 2.607345491327265

Epoch: 6| Step: 2
Training loss: 3.0298662185668945
Validation loss: 2.610539605540614

Epoch: 6| Step: 3
Training loss: 2.0632574558258057
Validation loss: 2.6071311350791686

Epoch: 6| Step: 4
Training loss: 2.963228940963745
Validation loss: 2.6061966726856847

Epoch: 6| Step: 5
Training loss: 2.87003231048584
Validation loss: 2.6053438366100354

Epoch: 6| Step: 6
Training loss: 2.361940622329712
Validation loss: 2.6080051801537953

Epoch: 6| Step: 7
Training loss: 2.6432032585144043
Validation loss: 2.6082787001004784

Epoch: 6| Step: 8
Training loss: 2.9738969802856445
Validation loss: 2.608929367475612

Epoch: 6| Step: 9
Training loss: 2.4966378211975098
Validation loss: 2.6129809528268795

Epoch: 6| Step: 10
Training loss: 3.751408100128174
Validation loss: 2.608136769263975

Epoch: 6| Step: 11
Training loss: 2.212939977645874
Validation loss: 2.611599919616535

Epoch: 6| Step: 12
Training loss: 2.8793444633483887
Validation loss: 2.6087313826366136

Epoch: 6| Step: 13
Training loss: 1.6903691291809082
Validation loss: 2.617731499415572

Epoch: 101| Step: 0
Training loss: 3.462418794631958
Validation loss: 2.6167949732913764

Epoch: 6| Step: 1
Training loss: 2.701956272125244
Validation loss: 2.628570984768611

Epoch: 6| Step: 2
Training loss: 2.6996445655822754
Validation loss: 2.6131366068317043

Epoch: 6| Step: 3
Training loss: 2.6654515266418457
Validation loss: 2.6075330472761586

Epoch: 6| Step: 4
Training loss: 3.253939628601074
Validation loss: 2.603984511026772

Epoch: 6| Step: 5
Training loss: 2.2079830169677734
Validation loss: 2.602332774028983

Epoch: 6| Step: 6
Training loss: 2.628753900527954
Validation loss: 2.6050291061401367

Epoch: 6| Step: 7
Training loss: 3.1942224502563477
Validation loss: 2.6067655009608113

Epoch: 6| Step: 8
Training loss: 2.3191423416137695
Validation loss: 2.605510844979235

Epoch: 6| Step: 9
Training loss: 2.4712305068969727
Validation loss: 2.6051930330132924

Epoch: 6| Step: 10
Training loss: 3.3210372924804688
Validation loss: 2.6037895987110753

Epoch: 6| Step: 11
Training loss: 2.5024495124816895
Validation loss: 2.60534163700637

Epoch: 6| Step: 12
Training loss: 2.7011842727661133
Validation loss: 2.603931478274766

Epoch: 6| Step: 13
Training loss: 2.522883415222168
Validation loss: 2.607497053761636

Epoch: 102| Step: 0
Training loss: 3.2491567134857178
Validation loss: 2.6076431556414534

Epoch: 6| Step: 1
Training loss: 1.820678949356079
Validation loss: 2.608676487399686

Epoch: 6| Step: 2
Training loss: 2.357821464538574
Validation loss: 2.612463133309477

Epoch: 6| Step: 3
Training loss: 2.655459403991699
Validation loss: 2.6163908743089244

Epoch: 6| Step: 4
Training loss: 2.2361674308776855
Validation loss: 2.621462183613931

Epoch: 6| Step: 5
Training loss: 2.863582134246826
Validation loss: 2.6169116497039795

Epoch: 6| Step: 6
Training loss: 3.1946465969085693
Validation loss: 2.617584443861438

Epoch: 6| Step: 7
Training loss: 2.309664249420166
Validation loss: 2.6185158657771286

Epoch: 6| Step: 8
Training loss: 3.1558620929718018
Validation loss: 2.6093920943557576

Epoch: 6| Step: 9
Training loss: 2.721525192260742
Validation loss: 2.6213676288563716

Epoch: 6| Step: 10
Training loss: 2.816040277481079
Validation loss: 2.6162209331348376

Epoch: 6| Step: 11
Training loss: 2.6345155239105225
Validation loss: 2.6159627206863894

Epoch: 6| Step: 12
Training loss: 3.4833459854125977
Validation loss: 2.6105452558045745

Epoch: 6| Step: 13
Training loss: 3.3662221431732178
Validation loss: 2.6103733457544798

Epoch: 103| Step: 0
Training loss: 2.8956832885742188
Validation loss: 2.607391436894735

Epoch: 6| Step: 1
Training loss: 2.11183500289917
Validation loss: 2.598462471397974

Epoch: 6| Step: 2
Training loss: 2.8317203521728516
Validation loss: 2.6028205604963404

Epoch: 6| Step: 3
Training loss: 3.2725696563720703
Validation loss: 2.600874780326761

Epoch: 6| Step: 4
Training loss: 3.4113824367523193
Validation loss: 2.60376973049615

Epoch: 6| Step: 5
Training loss: 2.3877718448638916
Validation loss: 2.6038155786452757

Epoch: 6| Step: 6
Training loss: 3.227231502532959
Validation loss: 2.6070564023910032

Epoch: 6| Step: 7
Training loss: 2.1058878898620605
Validation loss: 2.6077083746592202

Epoch: 6| Step: 8
Training loss: 1.817433476448059
Validation loss: 2.6105008535487677

Epoch: 6| Step: 9
Training loss: 2.644792318344116
Validation loss: 2.6093202649906115

Epoch: 6| Step: 10
Training loss: 2.280402183532715
Validation loss: 2.610012733808128

Epoch: 6| Step: 11
Training loss: 3.376188039779663
Validation loss: 2.60786691019612

Epoch: 6| Step: 12
Training loss: 3.1062965393066406
Validation loss: 2.6038227799118205

Epoch: 6| Step: 13
Training loss: 3.5380985736846924
Validation loss: 2.6065405132949993

Epoch: 104| Step: 0
Training loss: 2.8454818725585938
Validation loss: 2.604694042154538

Epoch: 6| Step: 1
Training loss: 3.1909568309783936
Validation loss: 2.604364443850774

Epoch: 6| Step: 2
Training loss: 2.965789794921875
Validation loss: 2.6003773520069737

Epoch: 6| Step: 3
Training loss: 2.9716320037841797
Validation loss: 2.599881331125895

Epoch: 6| Step: 4
Training loss: 2.3409647941589355
Validation loss: 2.5966119304780038

Epoch: 6| Step: 5
Training loss: 2.6449990272521973
Validation loss: 2.6028825467632664

Epoch: 6| Step: 6
Training loss: 2.412008762359619
Validation loss: 2.59848524678138

Epoch: 6| Step: 7
Training loss: 2.0982789993286133
Validation loss: 2.5944733235143844

Epoch: 6| Step: 8
Training loss: 2.8579390048980713
Validation loss: 2.602758261465257

Epoch: 6| Step: 9
Training loss: 2.552964448928833
Validation loss: 2.6095776122103453

Epoch: 6| Step: 10
Training loss: 2.5548770427703857
Validation loss: 2.60762962859164

Epoch: 6| Step: 11
Training loss: 3.6715564727783203
Validation loss: 2.6221432429487987

Epoch: 6| Step: 12
Training loss: 2.87748384475708
Validation loss: 2.614608931285079

Epoch: 6| Step: 13
Training loss: 2.4415485858917236
Validation loss: 2.6092670425291984

Epoch: 105| Step: 0
Training loss: 2.980435371398926
Validation loss: 2.6052057358526413

Epoch: 6| Step: 1
Training loss: 2.1284656524658203
Validation loss: 2.614113384677518

Epoch: 6| Step: 2
Training loss: 3.950648784637451
Validation loss: 2.614715653081094

Epoch: 6| Step: 3
Training loss: 2.3182215690612793
Validation loss: 2.6107845280760076

Epoch: 6| Step: 4
Training loss: 2.855743646621704
Validation loss: 2.6076697072675152

Epoch: 6| Step: 5
Training loss: 2.713690757751465
Validation loss: 2.606348304338353

Epoch: 6| Step: 6
Training loss: 2.930156707763672
Validation loss: 2.603071774205854

Epoch: 6| Step: 7
Training loss: 3.064990997314453
Validation loss: 2.6019562700743317

Epoch: 6| Step: 8
Training loss: 2.7364730834960938
Validation loss: 2.600084561173634

Epoch: 6| Step: 9
Training loss: 3.1603822708129883
Validation loss: 2.6004759752622215

Epoch: 6| Step: 10
Training loss: 2.655122756958008
Validation loss: 2.5993357268712853

Epoch: 6| Step: 11
Training loss: 2.327463388442993
Validation loss: 2.5988821188608804

Epoch: 6| Step: 12
Training loss: 2.396129608154297
Validation loss: 2.59860747860324

Epoch: 6| Step: 13
Training loss: 2.1451284885406494
Validation loss: 2.5949247139756397

Epoch: 106| Step: 0
Training loss: 2.331456422805786
Validation loss: 2.591440026478101

Epoch: 6| Step: 1
Training loss: 2.630560874938965
Validation loss: 2.5943359508309314

Epoch: 6| Step: 2
Training loss: 2.051994800567627
Validation loss: 2.599128520616921

Epoch: 6| Step: 3
Training loss: 3.7008683681488037
Validation loss: 2.595134494125202

Epoch: 6| Step: 4
Training loss: 3.193836212158203
Validation loss: 2.5949963702950427

Epoch: 6| Step: 5
Training loss: 2.2732248306274414
Validation loss: 2.592559358125092

Epoch: 6| Step: 6
Training loss: 3.08907413482666
Validation loss: 2.594212960171443

Epoch: 6| Step: 7
Training loss: 2.8162951469421387
Validation loss: 2.6005196571350098

Epoch: 6| Step: 8
Training loss: 3.136629104614258
Validation loss: 2.596229721141118

Epoch: 6| Step: 9
Training loss: 2.1203956604003906
Validation loss: 2.596281805346089

Epoch: 6| Step: 10
Training loss: 2.598705291748047
Validation loss: 2.594397078278244

Epoch: 6| Step: 11
Training loss: 2.46663236618042
Validation loss: 2.5919719152553107

Epoch: 6| Step: 12
Training loss: 3.424177646636963
Validation loss: 2.592600243065947

Epoch: 6| Step: 13
Training loss: 2.4773733615875244
Validation loss: 2.592450649507584

Epoch: 107| Step: 0
Training loss: 2.499406337738037
Validation loss: 2.590200506230836

Epoch: 6| Step: 1
Training loss: 3.201991558074951
Validation loss: 2.5870750668228313

Epoch: 6| Step: 2
Training loss: 2.935692310333252
Validation loss: 2.5906583365573677

Epoch: 6| Step: 3
Training loss: 2.2304327487945557
Validation loss: 2.5895574913229993

Epoch: 6| Step: 4
Training loss: 2.315680980682373
Validation loss: 2.5891087978116927

Epoch: 6| Step: 5
Training loss: 3.337419033050537
Validation loss: 2.5906461977189585

Epoch: 6| Step: 6
Training loss: 3.9676051139831543
Validation loss: 2.5873466178935063

Epoch: 6| Step: 7
Training loss: 2.4749755859375
Validation loss: 2.5857231719519502

Epoch: 6| Step: 8
Training loss: 2.73020601272583
Validation loss: 2.5877650835180797

Epoch: 6| Step: 9
Training loss: 2.5511298179626465
Validation loss: 2.588823664572931

Epoch: 6| Step: 10
Training loss: 2.798142910003662
Validation loss: 2.5861450831095376

Epoch: 6| Step: 11
Training loss: 3.1874163150787354
Validation loss: 2.5920166354025564

Epoch: 6| Step: 12
Training loss: 1.8591020107269287
Validation loss: 2.58721750269654

Epoch: 6| Step: 13
Training loss: 2.064823865890503
Validation loss: 2.585632085800171

Epoch: 108| Step: 0
Training loss: 2.8532209396362305
Validation loss: 2.584727115528558

Epoch: 6| Step: 1
Training loss: 2.4651131629943848
Validation loss: 2.586541178405926

Epoch: 6| Step: 2
Training loss: 3.122265338897705
Validation loss: 2.58497295841094

Epoch: 6| Step: 3
Training loss: 2.9706344604492188
Validation loss: 2.585491503438642

Epoch: 6| Step: 4
Training loss: 2.584442615509033
Validation loss: 2.5837517861397035

Epoch: 6| Step: 5
Training loss: 3.017404556274414
Validation loss: 2.586092682294948

Epoch: 6| Step: 6
Training loss: 3.109781265258789
Validation loss: 2.584604199214648

Epoch: 6| Step: 7
Training loss: 1.276099681854248
Validation loss: 2.5849717816998883

Epoch: 6| Step: 8
Training loss: 2.747776985168457
Validation loss: 2.5816093516606156

Epoch: 6| Step: 9
Training loss: 3.662357807159424
Validation loss: 2.581528966144849

Epoch: 6| Step: 10
Training loss: 2.479654312133789
Validation loss: 2.5836751871211554

Epoch: 6| Step: 11
Training loss: 2.8573265075683594
Validation loss: 2.584240339135611

Epoch: 6| Step: 12
Training loss: 2.6239395141601562
Validation loss: 2.587280850256643

Epoch: 6| Step: 13
Training loss: 2.581037998199463
Validation loss: 2.587929589774019

Epoch: 109| Step: 0
Training loss: 3.538722038269043
Validation loss: 2.585127620286839

Epoch: 6| Step: 1
Training loss: 2.761479377746582
Validation loss: 2.5892099641984507

Epoch: 6| Step: 2
Training loss: 2.6075263023376465
Validation loss: 2.5900345181906097

Epoch: 6| Step: 3
Training loss: 2.778691291809082
Validation loss: 2.580992308996057

Epoch: 6| Step: 4
Training loss: 2.757261037826538
Validation loss: 2.581260491442937

Epoch: 6| Step: 5
Training loss: 2.342682361602783
Validation loss: 2.5852424483145438

Epoch: 6| Step: 6
Training loss: 2.695974111557007
Validation loss: 2.5801284569565968

Epoch: 6| Step: 7
Training loss: 1.579190731048584
Validation loss: 2.5812792137104976

Epoch: 6| Step: 8
Training loss: 3.4792137145996094
Validation loss: 2.5777961951430126

Epoch: 6| Step: 9
Training loss: 2.6235337257385254
Validation loss: 2.5799907125452513

Epoch: 6| Step: 10
Training loss: 2.7047672271728516
Validation loss: 2.580866870059762

Epoch: 6| Step: 11
Training loss: 2.8868207931518555
Validation loss: 2.580432391935779

Epoch: 6| Step: 12
Training loss: 3.2230026721954346
Validation loss: 2.5843104162523822

Epoch: 6| Step: 13
Training loss: 2.087057113647461
Validation loss: 2.5828530044965845

Epoch: 110| Step: 0
Training loss: 2.560912847518921
Validation loss: 2.5849154380060013

Epoch: 6| Step: 1
Training loss: 2.9246485233306885
Validation loss: 2.5807094035610074

Epoch: 6| Step: 2
Training loss: 2.978574275970459
Validation loss: 2.584785258898171

Epoch: 6| Step: 3
Training loss: 2.8323144912719727
Validation loss: 2.5828094636240313

Epoch: 6| Step: 4
Training loss: 2.8488662242889404
Validation loss: 2.5795178849210023

Epoch: 6| Step: 5
Training loss: 2.5951881408691406
Validation loss: 2.582908950826173

Epoch: 6| Step: 6
Training loss: 2.2758548259735107
Validation loss: 2.581415443010228

Epoch: 6| Step: 7
Training loss: 2.474712371826172
Validation loss: 2.5827335593520955

Epoch: 6| Step: 8
Training loss: 2.1839284896850586
Validation loss: 2.579487677543394

Epoch: 6| Step: 9
Training loss: 2.970410108566284
Validation loss: 2.577004119914065

Epoch: 6| Step: 10
Training loss: 2.4773030281066895
Validation loss: 2.5783273097007506

Epoch: 6| Step: 11
Training loss: 2.852281332015991
Validation loss: 2.581177460250034

Epoch: 6| Step: 12
Training loss: 3.2262942790985107
Validation loss: 2.5841251521982174

Epoch: 6| Step: 13
Training loss: 3.348510265350342
Validation loss: 2.577976611352736

Epoch: 111| Step: 0
Training loss: 2.7243709564208984
Validation loss: 2.58631310924407

Epoch: 6| Step: 1
Training loss: 3.040142297744751
Validation loss: 2.576189092410508

Epoch: 6| Step: 2
Training loss: 2.5743863582611084
Validation loss: 2.581645452848045

Epoch: 6| Step: 3
Training loss: 2.511162042617798
Validation loss: 2.585624517933015

Epoch: 6| Step: 4
Training loss: 3.33268404006958
Validation loss: 2.585315219817623

Epoch: 6| Step: 5
Training loss: 2.907839775085449
Validation loss: 2.5791724958727436

Epoch: 6| Step: 6
Training loss: 3.0210559368133545
Validation loss: 2.5819187036124607

Epoch: 6| Step: 7
Training loss: 2.4372262954711914
Validation loss: 2.5759605207750873

Epoch: 6| Step: 8
Training loss: 3.247110366821289
Validation loss: 2.5873062533717

Epoch: 6| Step: 9
Training loss: 2.3904898166656494
Validation loss: 2.577717609302972

Epoch: 6| Step: 10
Training loss: 2.2785823345184326
Validation loss: 2.573786368934057

Epoch: 6| Step: 11
Training loss: 2.7922892570495605
Validation loss: 2.5766814242127123

Epoch: 6| Step: 12
Training loss: 2.354809522628784
Validation loss: 2.5779727325644544

Epoch: 6| Step: 13
Training loss: 2.619582176208496
Validation loss: 2.5764754638876965

Epoch: 112| Step: 0
Training loss: 3.116708755493164
Validation loss: 2.5742083006007697

Epoch: 6| Step: 1
Training loss: 2.9577248096466064
Validation loss: 2.5777587506078903

Epoch: 6| Step: 2
Training loss: 2.5001401901245117
Validation loss: 2.575275005832795

Epoch: 6| Step: 3
Training loss: 2.1385955810546875
Validation loss: 2.5739383210418043

Epoch: 6| Step: 4
Training loss: 2.7656443119049072
Validation loss: 2.5754833259890155

Epoch: 6| Step: 5
Training loss: 3.5697999000549316
Validation loss: 2.579351555916571

Epoch: 6| Step: 6
Training loss: 2.8356831073760986
Validation loss: 2.576448817406931

Epoch: 6| Step: 7
Training loss: 2.2266268730163574
Validation loss: 2.577225100609564

Epoch: 6| Step: 8
Training loss: 3.2954368591308594
Validation loss: 2.5753934434665147

Epoch: 6| Step: 9
Training loss: 2.7880184650421143
Validation loss: 2.575162495336225

Epoch: 6| Step: 10
Training loss: 2.481377124786377
Validation loss: 2.5705951593255483

Epoch: 6| Step: 11
Training loss: 3.461007595062256
Validation loss: 2.5775209319206978

Epoch: 6| Step: 12
Training loss: 1.9992876052856445
Validation loss: 2.576771095234861

Epoch: 6| Step: 13
Training loss: 1.7499728202819824
Validation loss: 2.5770888969462407

Epoch: 113| Step: 0
Training loss: 3.2106428146362305
Validation loss: 2.5844237496775966

Epoch: 6| Step: 1
Training loss: 2.389352798461914
Validation loss: 2.591774714890347

Epoch: 6| Step: 2
Training loss: 3.344761610031128
Validation loss: 2.591379498922697

Epoch: 6| Step: 3
Training loss: 2.4023122787475586
Validation loss: 2.5799151030919885

Epoch: 6| Step: 4
Training loss: 2.6042652130126953
Validation loss: 2.5784423428197063

Epoch: 6| Step: 5
Training loss: 2.6110427379608154
Validation loss: 2.583849847957652

Epoch: 6| Step: 6
Training loss: 2.452293872833252
Validation loss: 2.576946238035797

Epoch: 6| Step: 7
Training loss: 2.72475004196167
Validation loss: 2.57265640330571

Epoch: 6| Step: 8
Training loss: 2.394702434539795
Validation loss: 2.568404359202231

Epoch: 6| Step: 9
Training loss: 2.7056288719177246
Validation loss: 2.5706621857099634

Epoch: 6| Step: 10
Training loss: 2.947463035583496
Validation loss: 2.5704595196631645

Epoch: 6| Step: 11
Training loss: 3.2632970809936523
Validation loss: 2.568885439185686

Epoch: 6| Step: 12
Training loss: 2.388730049133301
Validation loss: 2.567926455569524

Epoch: 6| Step: 13
Training loss: 2.986518383026123
Validation loss: 2.5684594210758003

Epoch: 114| Step: 0
Training loss: 2.8614113330841064
Validation loss: 2.5688043563596663

Epoch: 6| Step: 1
Training loss: 3.22664737701416
Validation loss: 2.569466895954583

Epoch: 6| Step: 2
Training loss: 4.17421293258667
Validation loss: 2.5677628209514003

Epoch: 6| Step: 3
Training loss: 1.8240079879760742
Validation loss: 2.5670560765010055

Epoch: 6| Step: 4
Training loss: 2.5265274047851562
Validation loss: 2.571868988775438

Epoch: 6| Step: 5
Training loss: 2.1170554161071777
Validation loss: 2.5717879956768406

Epoch: 6| Step: 6
Training loss: 2.9686577320098877
Validation loss: 2.5708542023935625

Epoch: 6| Step: 7
Training loss: 2.675553798675537
Validation loss: 2.5683023468140633

Epoch: 6| Step: 8
Training loss: 2.9257426261901855
Validation loss: 2.569244876984627

Epoch: 6| Step: 9
Training loss: 2.7962656021118164
Validation loss: 2.567591187774494

Epoch: 6| Step: 10
Training loss: 2.2021026611328125
Validation loss: 2.561992836254899

Epoch: 6| Step: 11
Training loss: 3.386659860610962
Validation loss: 2.56397275001772

Epoch: 6| Step: 12
Training loss: 2.111199140548706
Validation loss: 2.566376950151177

Epoch: 6| Step: 13
Training loss: 2.276475191116333
Validation loss: 2.567682150871523

Epoch: 115| Step: 0
Training loss: 2.5154247283935547
Validation loss: 2.566474755605062

Epoch: 6| Step: 1
Training loss: 2.7044835090637207
Validation loss: 2.565884118439049

Epoch: 6| Step: 2
Training loss: 2.7340328693389893
Validation loss: 2.5745400767172537

Epoch: 6| Step: 3
Training loss: 2.6794209480285645
Validation loss: 2.573128036273423

Epoch: 6| Step: 4
Training loss: 3.0291848182678223
Validation loss: 2.580462440367668

Epoch: 6| Step: 5
Training loss: 2.768428325653076
Validation loss: 2.5725064867286274

Epoch: 6| Step: 6
Training loss: 2.373682737350464
Validation loss: 2.5827765387873494

Epoch: 6| Step: 7
Training loss: 2.301581621170044
Validation loss: 2.5714747803185576

Epoch: 6| Step: 8
Training loss: 2.9714317321777344
Validation loss: 2.564772669987012

Epoch: 6| Step: 9
Training loss: 2.308412551879883
Validation loss: 2.566318088962186

Epoch: 6| Step: 10
Training loss: 3.7912704944610596
Validation loss: 2.5663659470055693

Epoch: 6| Step: 11
Training loss: 2.215076446533203
Validation loss: 2.565911005902034

Epoch: 6| Step: 12
Training loss: 2.813652753829956
Validation loss: 2.5691739795028523

Epoch: 6| Step: 13
Training loss: 3.206817626953125
Validation loss: 2.5654239295631327

Epoch: 116| Step: 0
Training loss: 3.26127552986145
Validation loss: 2.5685219585254626

Epoch: 6| Step: 1
Training loss: 2.4427967071533203
Validation loss: 2.5681980707312144

Epoch: 6| Step: 2
Training loss: 2.5460288524627686
Validation loss: 2.570181490272604

Epoch: 6| Step: 3
Training loss: 3.31514310836792
Validation loss: 2.566519065569806

Epoch: 6| Step: 4
Training loss: 2.7888898849487305
Validation loss: 2.568770226611886

Epoch: 6| Step: 5
Training loss: 2.6504018306732178
Validation loss: 2.5634405971855245

Epoch: 6| Step: 6
Training loss: 2.669828414916992
Validation loss: 2.561820166085356

Epoch: 6| Step: 7
Training loss: 2.8608694076538086
Validation loss: 2.5657657090053765

Epoch: 6| Step: 8
Training loss: 3.297783851623535
Validation loss: 2.5643298677218858

Epoch: 6| Step: 9
Training loss: 2.466905117034912
Validation loss: 2.5627136768833285

Epoch: 6| Step: 10
Training loss: 2.7808632850646973
Validation loss: 2.567117278293897

Epoch: 6| Step: 11
Training loss: 1.6813138723373413
Validation loss: 2.570100477946702

Epoch: 6| Step: 12
Training loss: 2.593874454498291
Validation loss: 2.569484362038233

Epoch: 6| Step: 13
Training loss: 2.901813507080078
Validation loss: 2.5714388162859025

Epoch: 117| Step: 0
Training loss: 2.463991641998291
Validation loss: 2.567600850136049

Epoch: 6| Step: 1
Training loss: 2.959855079650879
Validation loss: 2.5656771454759824

Epoch: 6| Step: 2
Training loss: 3.2876052856445312
Validation loss: 2.5624166534792994

Epoch: 6| Step: 3
Training loss: 2.143841028213501
Validation loss: 2.5636075824819584

Epoch: 6| Step: 4
Training loss: 2.0860342979431152
Validation loss: 2.560859262302358

Epoch: 6| Step: 5
Training loss: 2.499575138092041
Validation loss: 2.5637171242826726

Epoch: 6| Step: 6
Training loss: 2.4722750186920166
Validation loss: 2.5629252669631795

Epoch: 6| Step: 7
Training loss: 2.683455228805542
Validation loss: 2.5648215483593684

Epoch: 6| Step: 8
Training loss: 3.1384615898132324
Validation loss: 2.5605254557824906

Epoch: 6| Step: 9
Training loss: 2.7268614768981934
Validation loss: 2.5593953773539555

Epoch: 6| Step: 10
Training loss: 3.0021016597747803
Validation loss: 2.5680299035964476

Epoch: 6| Step: 11
Training loss: 3.531416416168213
Validation loss: 2.566649329277777

Epoch: 6| Step: 12
Training loss: 2.221330165863037
Validation loss: 2.569020084155503

Epoch: 6| Step: 13
Training loss: 3.188727378845215
Validation loss: 2.5719122399565992

Epoch: 118| Step: 0
Training loss: 2.2016711235046387
Validation loss: 2.571924901777698

Epoch: 6| Step: 1
Training loss: 1.6431832313537598
Validation loss: 2.5732655140661422

Epoch: 6| Step: 2
Training loss: 2.4964559078216553
Validation loss: 2.562775304240565

Epoch: 6| Step: 3
Training loss: 2.5324044227600098
Validation loss: 2.563375703750118

Epoch: 6| Step: 4
Training loss: 3.297431230545044
Validation loss: 2.562750406162713

Epoch: 6| Step: 5
Training loss: 2.9217231273651123
Validation loss: 2.5606797587487007

Epoch: 6| Step: 6
Training loss: 3.3377461433410645
Validation loss: 2.5613254321518766

Epoch: 6| Step: 7
Training loss: 2.4365811347961426
Validation loss: 2.5622959393326954

Epoch: 6| Step: 8
Training loss: 3.0792298316955566
Validation loss: 2.5578914893570768

Epoch: 6| Step: 9
Training loss: 2.357815980911255
Validation loss: 2.5554099441856466

Epoch: 6| Step: 10
Training loss: 2.4093971252441406
Validation loss: 2.5571190490517566

Epoch: 6| Step: 11
Training loss: 3.549203634262085
Validation loss: 2.5595717583933184

Epoch: 6| Step: 12
Training loss: 2.6760129928588867
Validation loss: 2.5614966500189995

Epoch: 6| Step: 13
Training loss: 3.618363380432129
Validation loss: 2.5562341315771944

Epoch: 119| Step: 0
Training loss: 2.1792986392974854
Validation loss: 2.5593122282335834

Epoch: 6| Step: 1
Training loss: 2.8183536529541016
Validation loss: 2.557327634544783

Epoch: 6| Step: 2
Training loss: 2.6659581661224365
Validation loss: 2.5576055383169525

Epoch: 6| Step: 3
Training loss: 2.740370273590088
Validation loss: 2.5601476520620365

Epoch: 6| Step: 4
Training loss: 2.3734500408172607
Validation loss: 2.561439314196187

Epoch: 6| Step: 5
Training loss: 3.360894203186035
Validation loss: 2.5572411475643033

Epoch: 6| Step: 6
Training loss: 2.5981929302215576
Validation loss: 2.557946740940053

Epoch: 6| Step: 7
Training loss: 2.3366756439208984
Validation loss: 2.564525927266767

Epoch: 6| Step: 8
Training loss: 1.6974698305130005
Validation loss: 2.56502172511111

Epoch: 6| Step: 9
Training loss: 3.238332748413086
Validation loss: 2.5722870801084783

Epoch: 6| Step: 10
Training loss: 2.5427005290985107
Validation loss: 2.5659611660947084

Epoch: 6| Step: 11
Training loss: 2.7950847148895264
Validation loss: 2.568020789853988

Epoch: 6| Step: 12
Training loss: 3.432407855987549
Validation loss: 2.56541411594678

Epoch: 6| Step: 13
Training loss: 3.804615020751953
Validation loss: 2.56019938120278

Epoch: 120| Step: 0
Training loss: 1.8290753364562988
Validation loss: 2.5573819221988803

Epoch: 6| Step: 1
Training loss: 2.762890338897705
Validation loss: 2.565016177392775

Epoch: 6| Step: 2
Training loss: 3.2017736434936523
Validation loss: 2.5521860776409024

Epoch: 6| Step: 3
Training loss: 2.2974693775177
Validation loss: 2.550955500653995

Epoch: 6| Step: 4
Training loss: 3.0321455001831055
Validation loss: 2.5557240183635423

Epoch: 6| Step: 5
Training loss: 2.773658275604248
Validation loss: 2.5536328361880396

Epoch: 6| Step: 6
Training loss: 2.1712894439697266
Validation loss: 2.5554277743062666

Epoch: 6| Step: 7
Training loss: 3.1568284034729004
Validation loss: 2.555033512012933

Epoch: 6| Step: 8
Training loss: 3.51292085647583
Validation loss: 2.553941698484523

Epoch: 6| Step: 9
Training loss: 3.1172664165496826
Validation loss: 2.556230919335478

Epoch: 6| Step: 10
Training loss: 2.4834625720977783
Validation loss: 2.5532550273403043

Epoch: 6| Step: 11
Training loss: 2.7522430419921875
Validation loss: 2.554664656680117

Epoch: 6| Step: 12
Training loss: 2.3762245178222656
Validation loss: 2.552632203666113

Epoch: 6| Step: 13
Training loss: 2.6152615547180176
Validation loss: 2.5537907385057017

Epoch: 121| Step: 0
Training loss: 2.2992806434631348
Validation loss: 2.5519016071032454

Epoch: 6| Step: 1
Training loss: 2.432326316833496
Validation loss: 2.553428185883389

Epoch: 6| Step: 2
Training loss: 3.0901904106140137
Validation loss: 2.5531238561035483

Epoch: 6| Step: 3
Training loss: 2.021820545196533
Validation loss: 2.5505030719182824

Epoch: 6| Step: 4
Training loss: 2.671940326690674
Validation loss: 2.552915373156148

Epoch: 6| Step: 5
Training loss: 2.8201563358306885
Validation loss: 2.550689456283405

Epoch: 6| Step: 6
Training loss: 3.282668352127075
Validation loss: 2.5539366455488306

Epoch: 6| Step: 7
Training loss: 2.7195491790771484
Validation loss: 2.5536807403769544

Epoch: 6| Step: 8
Training loss: 1.9309616088867188
Validation loss: 2.551939525911885

Epoch: 6| Step: 9
Training loss: 2.6148200035095215
Validation loss: 2.552679620763307

Epoch: 6| Step: 10
Training loss: 3.136166572570801
Validation loss: 2.55453299706982

Epoch: 6| Step: 11
Training loss: 3.5178937911987305
Validation loss: 2.5511113136045394

Epoch: 6| Step: 12
Training loss: 3.3248586654663086
Validation loss: 2.551652628888366

Epoch: 6| Step: 13
Training loss: 1.8140538930892944
Validation loss: 2.5522405332134617

Epoch: 122| Step: 0
Training loss: 2.4768126010894775
Validation loss: 2.5497569550750074

Epoch: 6| Step: 1
Training loss: 2.5544919967651367
Validation loss: 2.5514207181110176

Epoch: 6| Step: 2
Training loss: 3.356708526611328
Validation loss: 2.5495664073574926

Epoch: 6| Step: 3
Training loss: 2.9464802742004395
Validation loss: 2.551721513912242

Epoch: 6| Step: 4
Training loss: 2.752582550048828
Validation loss: 2.5471388498942056

Epoch: 6| Step: 5
Training loss: 3.292410373687744
Validation loss: 2.554504730368173

Epoch: 6| Step: 6
Training loss: 2.595828056335449
Validation loss: 2.555011118611982

Epoch: 6| Step: 7
Training loss: 2.521650791168213
Validation loss: 2.545826599162112

Epoch: 6| Step: 8
Training loss: 3.1693224906921387
Validation loss: 2.552127215170091

Epoch: 6| Step: 9
Training loss: 2.4956984519958496
Validation loss: 2.553482747847034

Epoch: 6| Step: 10
Training loss: 1.9055484533309937
Validation loss: 2.5510509757585424

Epoch: 6| Step: 11
Training loss: 2.736581325531006
Validation loss: 2.5486052215740247

Epoch: 6| Step: 12
Training loss: 2.6300482749938965
Validation loss: 2.5503457156560754

Epoch: 6| Step: 13
Training loss: 2.4361891746520996
Validation loss: 2.5524947156188307

Epoch: 123| Step: 0
Training loss: 3.237265110015869
Validation loss: 2.5584003028049263

Epoch: 6| Step: 1
Training loss: 3.526958703994751
Validation loss: 2.5621466790476153

Epoch: 6| Step: 2
Training loss: 2.377584457397461
Validation loss: 2.5611690013639388

Epoch: 6| Step: 3
Training loss: 3.0173938274383545
Validation loss: 2.561965475800217

Epoch: 6| Step: 4
Training loss: 2.752230167388916
Validation loss: 2.5637459703671035

Epoch: 6| Step: 5
Training loss: 2.4056687355041504
Validation loss: 2.5629787316886325

Epoch: 6| Step: 6
Training loss: 2.3550126552581787
Validation loss: 2.555737351858488

Epoch: 6| Step: 7
Training loss: 2.66884446144104
Validation loss: 2.546694847845262

Epoch: 6| Step: 8
Training loss: 2.3435070514678955
Validation loss: 2.5459143295082995

Epoch: 6| Step: 9
Training loss: 1.776026964187622
Validation loss: 2.5480911552265124

Epoch: 6| Step: 10
Training loss: 2.8739137649536133
Validation loss: 2.551207632146856

Epoch: 6| Step: 11
Training loss: 2.8143720626831055
Validation loss: 2.54707686362728

Epoch: 6| Step: 12
Training loss: 3.1460368633270264
Validation loss: 2.54935396358531

Epoch: 6| Step: 13
Training loss: 2.856105089187622
Validation loss: 2.550411537129392

Epoch: 124| Step: 0
Training loss: 2.198374032974243
Validation loss: 2.5489687304342947

Epoch: 6| Step: 1
Training loss: 1.9868686199188232
Validation loss: 2.549985196000786

Epoch: 6| Step: 2
Training loss: 2.1686882972717285
Validation loss: 2.547572630707936

Epoch: 6| Step: 3
Training loss: 3.4527626037597656
Validation loss: 2.551111029040429

Epoch: 6| Step: 4
Training loss: 2.692619800567627
Validation loss: 2.545708784493067

Epoch: 6| Step: 5
Training loss: 2.8726816177368164
Validation loss: 2.5467616409383793

Epoch: 6| Step: 6
Training loss: 3.00970196723938
Validation loss: 2.545439140771025

Epoch: 6| Step: 7
Training loss: 2.2283029556274414
Validation loss: 2.5452685074139665

Epoch: 6| Step: 8
Training loss: 2.828103542327881
Validation loss: 2.5455998502751833

Epoch: 6| Step: 9
Training loss: 2.708726644515991
Validation loss: 2.542302332898622

Epoch: 6| Step: 10
Training loss: 2.7533328533172607
Validation loss: 2.5443387595556115

Epoch: 6| Step: 11
Training loss: 2.9133667945861816
Validation loss: 2.551829784147201

Epoch: 6| Step: 12
Training loss: 2.93898868560791
Validation loss: 2.5504878874747985

Epoch: 6| Step: 13
Training loss: 3.655622720718384
Validation loss: 2.554586018285444

Epoch: 125| Step: 0
Training loss: 2.248493194580078
Validation loss: 2.5568183058051654

Epoch: 6| Step: 1
Training loss: 2.5465362071990967
Validation loss: 2.5679014036732335

Epoch: 6| Step: 2
Training loss: 3.128695487976074
Validation loss: 2.56879359932356

Epoch: 6| Step: 3
Training loss: 2.9810378551483154
Validation loss: 2.5660790089638

Epoch: 6| Step: 4
Training loss: 2.079664707183838
Validation loss: 2.566861093685191

Epoch: 6| Step: 5
Training loss: 2.3168036937713623
Validation loss: 2.5652881873551237

Epoch: 6| Step: 6
Training loss: 3.168121337890625
Validation loss: 2.559890834234094

Epoch: 6| Step: 7
Training loss: 2.809709072113037
Validation loss: 2.5575761513043473

Epoch: 6| Step: 8
Training loss: 3.9693105220794678
Validation loss: 2.5598592732542302

Epoch: 6| Step: 9
Training loss: 2.0022401809692383
Validation loss: 2.543775504635226

Epoch: 6| Step: 10
Training loss: 2.7301747798919678
Validation loss: 2.5488680101210073

Epoch: 6| Step: 11
Training loss: 2.600646495819092
Validation loss: 2.541981404827487

Epoch: 6| Step: 12
Training loss: 3.1990201473236084
Validation loss: 2.5442999896182807

Epoch: 6| Step: 13
Training loss: 1.9116919040679932
Validation loss: 2.5441893018702024

Epoch: 126| Step: 0
Training loss: 2.8803300857543945
Validation loss: 2.5440871536090808

Epoch: 6| Step: 1
Training loss: 2.5155580043792725
Validation loss: 2.5424238379283617

Epoch: 6| Step: 2
Training loss: 2.7367193698883057
Validation loss: 2.544627845928233

Epoch: 6| Step: 3
Training loss: 1.9750134944915771
Validation loss: 2.5423380123671664

Epoch: 6| Step: 4
Training loss: 2.934628486633301
Validation loss: 2.540557110181419

Epoch: 6| Step: 5
Training loss: 2.9739484786987305
Validation loss: 2.5399712157505814

Epoch: 6| Step: 6
Training loss: 2.6710662841796875
Validation loss: 2.543087161997313

Epoch: 6| Step: 7
Training loss: 3.113813877105713
Validation loss: 2.541255430508685

Epoch: 6| Step: 8
Training loss: 3.0962724685668945
Validation loss: 2.5403178456009075

Epoch: 6| Step: 9
Training loss: 2.220973491668701
Validation loss: 2.543009924632247

Epoch: 6| Step: 10
Training loss: 2.2068612575531006
Validation loss: 2.5448490265877015

Epoch: 6| Step: 11
Training loss: 2.306269645690918
Validation loss: 2.5414947181619625

Epoch: 6| Step: 12
Training loss: 3.3777170181274414
Validation loss: 2.545093326158421

Epoch: 6| Step: 13
Training loss: 3.287142515182495
Validation loss: 2.539161871838313

Epoch: 127| Step: 0
Training loss: 2.7619917392730713
Validation loss: 2.5457805356671734

Epoch: 6| Step: 1
Training loss: 3.2102601528167725
Validation loss: 2.5393427238669446

Epoch: 6| Step: 2
Training loss: 2.7984039783477783
Validation loss: 2.5391440340267715

Epoch: 6| Step: 3
Training loss: 1.967606782913208
Validation loss: 2.53737235325639

Epoch: 6| Step: 4
Training loss: 3.2562761306762695
Validation loss: 2.5360737026378675

Epoch: 6| Step: 5
Training loss: 2.328235149383545
Validation loss: 2.5377726759961856

Epoch: 6| Step: 6
Training loss: 3.4992501735687256
Validation loss: 2.5370589238341137

Epoch: 6| Step: 7
Training loss: 2.341481924057007
Validation loss: 2.5375389668249313

Epoch: 6| Step: 8
Training loss: 3.0282089710235596
Validation loss: 2.5415749011501187

Epoch: 6| Step: 9
Training loss: 2.248849391937256
Validation loss: 2.535136899640483

Epoch: 6| Step: 10
Training loss: 2.085573196411133
Validation loss: 2.5403768823992823

Epoch: 6| Step: 11
Training loss: 3.0376534461975098
Validation loss: 2.5393625382454164

Epoch: 6| Step: 12
Training loss: 1.945723295211792
Validation loss: 2.5345270966970794

Epoch: 6| Step: 13
Training loss: 3.964693784713745
Validation loss: 2.5361665705198884

Epoch: 128| Step: 0
Training loss: 2.223205089569092
Validation loss: 2.540226326193861

Epoch: 6| Step: 1
Training loss: 3.2200303077697754
Validation loss: 2.5412915137506302

Epoch: 6| Step: 2
Training loss: 2.6560251712799072
Validation loss: 2.540008132175733

Epoch: 6| Step: 3
Training loss: 2.323519706726074
Validation loss: 2.540530261173043

Epoch: 6| Step: 4
Training loss: 2.604893207550049
Validation loss: 2.53902833179761

Epoch: 6| Step: 5
Training loss: 2.5989575386047363
Validation loss: 2.537647726715252

Epoch: 6| Step: 6
Training loss: 2.9310994148254395
Validation loss: 2.5343659898286224

Epoch: 6| Step: 7
Training loss: 3.095046281814575
Validation loss: 2.539005087267968

Epoch: 6| Step: 8
Training loss: 3.236237049102783
Validation loss: 2.534686562835529

Epoch: 6| Step: 9
Training loss: 2.5682640075683594
Validation loss: 2.5375141853927285

Epoch: 6| Step: 10
Training loss: 2.9303534030914307
Validation loss: 2.5374692024723178

Epoch: 6| Step: 11
Training loss: 2.570443630218506
Validation loss: 2.5334803596619637

Epoch: 6| Step: 12
Training loss: 2.1979243755340576
Validation loss: 2.533358771313903

Epoch: 6| Step: 13
Training loss: 2.768272876739502
Validation loss: 2.5325820651105655

Epoch: 129| Step: 0
Training loss: 2.7516026496887207
Validation loss: 2.533717409256966

Epoch: 6| Step: 1
Training loss: 2.4062886238098145
Validation loss: 2.5438855540367866

Epoch: 6| Step: 2
Training loss: 2.9382026195526123
Validation loss: 2.549959187866539

Epoch: 6| Step: 3
Training loss: 2.9722695350646973
Validation loss: 2.549598309301561

Epoch: 6| Step: 4
Training loss: 2.225827932357788
Validation loss: 2.554225713975968

Epoch: 6| Step: 5
Training loss: 2.567234992980957
Validation loss: 2.5544120163045902

Epoch: 6| Step: 6
Training loss: 2.813558578491211
Validation loss: 2.54626904508119

Epoch: 6| Step: 7
Training loss: 3.1737799644470215
Validation loss: 2.542288144429525

Epoch: 6| Step: 8
Training loss: 2.3617801666259766
Validation loss: 2.5402439614777923

Epoch: 6| Step: 9
Training loss: 3.5601348876953125
Validation loss: 2.533129958696263

Epoch: 6| Step: 10
Training loss: 2.079840660095215
Validation loss: 2.5330778527003464

Epoch: 6| Step: 11
Training loss: 3.148871421813965
Validation loss: 2.534447116236533

Epoch: 6| Step: 12
Training loss: 2.8348846435546875
Validation loss: 2.5386850039164224

Epoch: 6| Step: 13
Training loss: 1.7367968559265137
Validation loss: 2.5369586329306326

Epoch: 130| Step: 0
Training loss: 2.4189741611480713
Validation loss: 2.543840262197679

Epoch: 6| Step: 1
Training loss: 2.2817556858062744
Validation loss: 2.540806301178471

Epoch: 6| Step: 2
Training loss: 2.72271466255188
Validation loss: 2.5442426230317805

Epoch: 6| Step: 3
Training loss: 3.254016160964966
Validation loss: 2.5439151512679232

Epoch: 6| Step: 4
Training loss: 2.8307628631591797
Validation loss: 2.535706102207143

Epoch: 6| Step: 5
Training loss: 2.310330390930176
Validation loss: 2.5405896632902083

Epoch: 6| Step: 6
Training loss: 3.5007667541503906
Validation loss: 2.5328257570984545

Epoch: 6| Step: 7
Training loss: 2.8739991188049316
Validation loss: 2.5391369583786174

Epoch: 6| Step: 8
Training loss: 2.5288820266723633
Validation loss: 2.537367815612465

Epoch: 6| Step: 9
Training loss: 2.497183322906494
Validation loss: 2.5380614214046027

Epoch: 6| Step: 10
Training loss: 2.8878138065338135
Validation loss: 2.5369207448856805

Epoch: 6| Step: 11
Training loss: 2.6635043621063232
Validation loss: 2.535786615904941

Epoch: 6| Step: 12
Training loss: 2.743551254272461
Validation loss: 2.540702409641717

Epoch: 6| Step: 13
Training loss: 2.298280954360962
Validation loss: 2.5365659959854616

Epoch: 131| Step: 0
Training loss: 2.2636661529541016
Validation loss: 2.5309217232529835

Epoch: 6| Step: 1
Training loss: 2.075730800628662
Validation loss: 2.532522306647352

Epoch: 6| Step: 2
Training loss: 3.1073861122131348
Validation loss: 2.535750366026355

Epoch: 6| Step: 3
Training loss: 2.259535312652588
Validation loss: 2.5335763859492477

Epoch: 6| Step: 4
Training loss: 3.613982677459717
Validation loss: 2.5307780183771604

Epoch: 6| Step: 5
Training loss: 3.08697247505188
Validation loss: 2.531516280225528

Epoch: 6| Step: 6
Training loss: 2.3414547443389893
Validation loss: 2.530964361724033

Epoch: 6| Step: 7
Training loss: 2.3328614234924316
Validation loss: 2.5332760990306897

Epoch: 6| Step: 8
Training loss: 3.570464849472046
Validation loss: 2.5319680552328787

Epoch: 6| Step: 9
Training loss: 2.2988157272338867
Validation loss: 2.5329205502745924

Epoch: 6| Step: 10
Training loss: 2.8673477172851562
Validation loss: 2.5316034491344164

Epoch: 6| Step: 11
Training loss: 3.488377571105957
Validation loss: 2.5292134643882833

Epoch: 6| Step: 12
Training loss: 1.8850098848342896
Validation loss: 2.5304662181485083

Epoch: 6| Step: 13
Training loss: 2.696843385696411
Validation loss: 2.527006226201211

Epoch: 132| Step: 0
Training loss: 2.439701557159424
Validation loss: 2.538047972545829

Epoch: 6| Step: 1
Training loss: 2.9495229721069336
Validation loss: 2.5378604601788264

Epoch: 6| Step: 2
Training loss: 3.37204647064209
Validation loss: 2.5589364933711227

Epoch: 6| Step: 3
Training loss: 3.0808610916137695
Validation loss: 2.5725899152858283

Epoch: 6| Step: 4
Training loss: 3.396045207977295
Validation loss: 2.5729400547601844

Epoch: 6| Step: 5
Training loss: 1.6943333148956299
Validation loss: 2.5690441336683048

Epoch: 6| Step: 6
Training loss: 2.109170913696289
Validation loss: 2.5494346234106247

Epoch: 6| Step: 7
Training loss: 2.9188809394836426
Validation loss: 2.546418615566787

Epoch: 6| Step: 8
Training loss: 2.9612793922424316
Validation loss: 2.537377765101771

Epoch: 6| Step: 9
Training loss: 2.192267417907715
Validation loss: 2.529698307796191

Epoch: 6| Step: 10
Training loss: 2.0527350902557373
Validation loss: 2.5293639680390716

Epoch: 6| Step: 11
Training loss: 3.1074883937835693
Validation loss: 2.531371283274825

Epoch: 6| Step: 12
Training loss: 3.1862101554870605
Validation loss: 2.5307402277505524

Epoch: 6| Step: 13
Training loss: 2.3941519260406494
Validation loss: 2.533636318740024

Epoch: 133| Step: 0
Training loss: 2.6255176067352295
Validation loss: 2.5263012250264487

Epoch: 6| Step: 1
Training loss: 2.6969330310821533
Validation loss: 2.5289001849389847

Epoch: 6| Step: 2
Training loss: 2.6999619007110596
Validation loss: 2.5292386701030116

Epoch: 6| Step: 3
Training loss: 2.0103681087493896
Validation loss: 2.5284021695454917

Epoch: 6| Step: 4
Training loss: 2.765612840652466
Validation loss: 2.528158974903886

Epoch: 6| Step: 5
Training loss: 2.882411479949951
Validation loss: 2.525584820778139

Epoch: 6| Step: 6
Training loss: 3.5444705486297607
Validation loss: 2.5251076708557787

Epoch: 6| Step: 7
Training loss: 2.85044527053833
Validation loss: 2.526815393919586

Epoch: 6| Step: 8
Training loss: 2.6185379028320312
Validation loss: 2.533315896987915

Epoch: 6| Step: 9
Training loss: 2.7071340084075928
Validation loss: 2.527440701761553

Epoch: 6| Step: 10
Training loss: 2.5307300090789795
Validation loss: 2.528485672448271

Epoch: 6| Step: 11
Training loss: 2.5490036010742188
Validation loss: 2.529469656687911

Epoch: 6| Step: 12
Training loss: 2.5349183082580566
Validation loss: 2.5282378735080844

Epoch: 6| Step: 13
Training loss: 2.932490348815918
Validation loss: 2.53184502355514

Epoch: 134| Step: 0
Training loss: 2.981748580932617
Validation loss: 2.5288284312012377

Epoch: 6| Step: 1
Training loss: 2.2610225677490234
Validation loss: 2.527339825066187

Epoch: 6| Step: 2
Training loss: 3.0848147869110107
Validation loss: 2.5285004338910504

Epoch: 6| Step: 3
Training loss: 3.003415584564209
Validation loss: 2.5236602419166156

Epoch: 6| Step: 4
Training loss: 2.421943426132202
Validation loss: 2.5262350548980055

Epoch: 6| Step: 5
Training loss: 3.0219573974609375
Validation loss: 2.527395484268024

Epoch: 6| Step: 6
Training loss: 3.178640365600586
Validation loss: 2.5263332192615797

Epoch: 6| Step: 7
Training loss: 2.493009567260742
Validation loss: 2.5233273172891266

Epoch: 6| Step: 8
Training loss: 3.0055131912231445
Validation loss: 2.522367174907397

Epoch: 6| Step: 9
Training loss: 2.468000888824463
Validation loss: 2.523842345001877

Epoch: 6| Step: 10
Training loss: 2.4354872703552246
Validation loss: 2.5266415124298423

Epoch: 6| Step: 11
Training loss: 2.3844175338745117
Validation loss: 2.528908867989817

Epoch: 6| Step: 12
Training loss: 2.560856342315674
Validation loss: 2.5234165089104765

Epoch: 6| Step: 13
Training loss: 2.393091917037964
Validation loss: 2.5266775854172243

Epoch: 135| Step: 0
Training loss: 1.9422396421432495
Validation loss: 2.525433824908349

Epoch: 6| Step: 1
Training loss: 2.6743316650390625
Validation loss: 2.5302388104059363

Epoch: 6| Step: 2
Training loss: 2.7647719383239746
Validation loss: 2.528736158083844

Epoch: 6| Step: 3
Training loss: 2.9405462741851807
Validation loss: 2.527203398366128

Epoch: 6| Step: 4
Training loss: 1.9245110750198364
Validation loss: 2.5265705585479736

Epoch: 6| Step: 5
Training loss: 2.3908352851867676
Validation loss: 2.523624345820437

Epoch: 6| Step: 6
Training loss: 2.580543041229248
Validation loss: 2.5281658300789456

Epoch: 6| Step: 7
Training loss: 2.824263095855713
Validation loss: 2.527656457757437

Epoch: 6| Step: 8
Training loss: 3.0116381645202637
Validation loss: 2.5310996809313373

Epoch: 6| Step: 9
Training loss: 3.3437154293060303
Validation loss: 2.533283859170893

Epoch: 6| Step: 10
Training loss: 2.853729248046875
Validation loss: 2.5302845457548737

Epoch: 6| Step: 11
Training loss: 2.4334282875061035
Validation loss: 2.523953878751365

Epoch: 6| Step: 12
Training loss: 2.8256027698516846
Validation loss: 2.521921250127977

Epoch: 6| Step: 13
Training loss: 3.8431196212768555
Validation loss: 2.527453040563932

Epoch: 136| Step: 0
Training loss: 3.0155863761901855
Validation loss: 2.520189598042478

Epoch: 6| Step: 1
Training loss: 3.27486252784729
Validation loss: 2.5221644242604575

Epoch: 6| Step: 2
Training loss: 2.377748966217041
Validation loss: 2.527128255495461

Epoch: 6| Step: 3
Training loss: 2.4112555980682373
Validation loss: 2.5259021687251266

Epoch: 6| Step: 4
Training loss: 2.3738675117492676
Validation loss: 2.5266924955511607

Epoch: 6| Step: 5
Training loss: 2.6590781211853027
Validation loss: 2.5231546330195602

Epoch: 6| Step: 6
Training loss: 3.027967929840088
Validation loss: 2.523016652753276

Epoch: 6| Step: 7
Training loss: 1.9545156955718994
Validation loss: 2.522496085013113

Epoch: 6| Step: 8
Training loss: 3.538433313369751
Validation loss: 2.525072797652214

Epoch: 6| Step: 9
Training loss: 3.114359140396118
Validation loss: 2.519941920875221

Epoch: 6| Step: 10
Training loss: 2.7937920093536377
Validation loss: 2.5217638349020355

Epoch: 6| Step: 11
Training loss: 2.4571399688720703
Validation loss: 2.524688877085204

Epoch: 6| Step: 12
Training loss: 2.2539491653442383
Validation loss: 2.5239177955094205

Epoch: 6| Step: 13
Training loss: 2.454268455505371
Validation loss: 2.522492144697456

Epoch: 137| Step: 0
Training loss: 2.900681972503662
Validation loss: 2.524722494104857

Epoch: 6| Step: 1
Training loss: 2.884768486022949
Validation loss: 2.534833026188676

Epoch: 6| Step: 2
Training loss: 2.8142151832580566
Validation loss: 2.5359012721687235

Epoch: 6| Step: 3
Training loss: 2.859855890274048
Validation loss: 2.524937693790723

Epoch: 6| Step: 4
Training loss: 2.8767168521881104
Validation loss: 2.5299127383898665

Epoch: 6| Step: 5
Training loss: 1.7865127325057983
Validation loss: 2.5271070106055147

Epoch: 6| Step: 6
Training loss: 2.4989371299743652
Validation loss: 2.52450757129218

Epoch: 6| Step: 7
Training loss: 2.5779242515563965
Validation loss: 2.517634498175754

Epoch: 6| Step: 8
Training loss: 3.237884044647217
Validation loss: 2.5183783808062152

Epoch: 6| Step: 9
Training loss: 2.3521156311035156
Validation loss: 2.5166225176985546

Epoch: 6| Step: 10
Training loss: 2.8627729415893555
Validation loss: 2.5220406798906225

Epoch: 6| Step: 11
Training loss: 2.314537525177002
Validation loss: 2.5219245726062405

Epoch: 6| Step: 12
Training loss: 3.1914451122283936
Validation loss: 2.5187722175352034

Epoch: 6| Step: 13
Training loss: 2.5718679428100586
Validation loss: 2.5187579739478325

Epoch: 138| Step: 0
Training loss: 2.7766902446746826
Validation loss: 2.520625009331652

Epoch: 6| Step: 1
Training loss: 2.6667447090148926
Validation loss: 2.5215055199079615

Epoch: 6| Step: 2
Training loss: 3.4347076416015625
Validation loss: 2.520574228737944

Epoch: 6| Step: 3
Training loss: 2.0889453887939453
Validation loss: 2.5207446518764702

Epoch: 6| Step: 4
Training loss: 2.954716682434082
Validation loss: 2.52528049612558

Epoch: 6| Step: 5
Training loss: 2.925179958343506
Validation loss: 2.5242531453409502

Epoch: 6| Step: 6
Training loss: 3.2514169216156006
Validation loss: 2.522387299486386

Epoch: 6| Step: 7
Training loss: 1.9009599685668945
Validation loss: 2.52791093754512

Epoch: 6| Step: 8
Training loss: 2.1939220428466797
Validation loss: 2.540000725817937

Epoch: 6| Step: 9
Training loss: 2.92326021194458
Validation loss: 2.5359033461539977

Epoch: 6| Step: 10
Training loss: 2.2168495655059814
Validation loss: 2.53994127755524

Epoch: 6| Step: 11
Training loss: 2.6185789108276367
Validation loss: 2.542438619880266

Epoch: 6| Step: 12
Training loss: 2.8329310417175293
Validation loss: 2.5458523278595298

Epoch: 6| Step: 13
Training loss: 3.197093963623047
Validation loss: 2.5325537548270276

Epoch: 139| Step: 0
Training loss: 2.490513801574707
Validation loss: 2.5233573170118433

Epoch: 6| Step: 1
Training loss: 2.653031587600708
Validation loss: 2.522078157753073

Epoch: 6| Step: 2
Training loss: 2.1017699241638184
Validation loss: 2.516046844502931

Epoch: 6| Step: 3
Training loss: 2.5065391063690186
Validation loss: 2.517799567150813

Epoch: 6| Step: 4
Training loss: 2.5840365886688232
Validation loss: 2.522942009792533

Epoch: 6| Step: 5
Training loss: 2.8517351150512695
Validation loss: 2.519715714198287

Epoch: 6| Step: 6
Training loss: 3.0724945068359375
Validation loss: 2.5156930005678566

Epoch: 6| Step: 7
Training loss: 3.778299331665039
Validation loss: 2.519639015197754

Epoch: 6| Step: 8
Training loss: 2.990025043487549
Validation loss: 2.5154582967040358

Epoch: 6| Step: 9
Training loss: 2.881899833679199
Validation loss: 2.5187926112964587

Epoch: 6| Step: 10
Training loss: 2.508054256439209
Validation loss: 2.5188160942446802

Epoch: 6| Step: 11
Training loss: 2.8986778259277344
Validation loss: 2.517493663295623

Epoch: 6| Step: 12
Training loss: 1.733715295791626
Validation loss: 2.513438804175264

Epoch: 6| Step: 13
Training loss: 2.6174542903900146
Validation loss: 2.5194891370752805

Epoch: 140| Step: 0
Training loss: 2.9932827949523926
Validation loss: 2.5167125450667513

Epoch: 6| Step: 1
Training loss: 2.4345903396606445
Validation loss: 2.518223424111643

Epoch: 6| Step: 2
Training loss: 3.4716646671295166
Validation loss: 2.522458532805084

Epoch: 6| Step: 3
Training loss: 2.9085707664489746
Validation loss: 2.5219664496760212

Epoch: 6| Step: 4
Training loss: 2.2974355220794678
Validation loss: 2.5159758726755777

Epoch: 6| Step: 5
Training loss: 2.8396835327148438
Validation loss: 2.5173192049867366

Epoch: 6| Step: 6
Training loss: 2.660064935684204
Validation loss: 2.5174508863879788

Epoch: 6| Step: 7
Training loss: 2.230635404586792
Validation loss: 2.5197638106602493

Epoch: 6| Step: 8
Training loss: 3.0614452362060547
Validation loss: 2.5174976600113737

Epoch: 6| Step: 9
Training loss: 1.9218497276306152
Validation loss: 2.5212069762650358

Epoch: 6| Step: 10
Training loss: 1.7782461643218994
Validation loss: 2.5131100352092455

Epoch: 6| Step: 11
Training loss: 3.7940125465393066
Validation loss: 2.5143411620970695

Epoch: 6| Step: 12
Training loss: 2.7922134399414062
Validation loss: 2.5131468695978962

Epoch: 6| Step: 13
Training loss: 2.290064811706543
Validation loss: 2.5167157444902646

Epoch: 141| Step: 0
Training loss: 2.039476156234741
Validation loss: 2.5175392832807315

Epoch: 6| Step: 1
Training loss: 2.950681686401367
Validation loss: 2.5154414971669516

Epoch: 6| Step: 2
Training loss: 2.6261775493621826
Validation loss: 2.5157212621422222

Epoch: 6| Step: 3
Training loss: 2.8585727214813232
Validation loss: 2.5165221511676745

Epoch: 6| Step: 4
Training loss: 2.97006893157959
Validation loss: 2.5184100879135953

Epoch: 6| Step: 5
Training loss: 2.4150736331939697
Validation loss: 2.514805019542735

Epoch: 6| Step: 6
Training loss: 2.200779438018799
Validation loss: 2.5153871505491194

Epoch: 6| Step: 7
Training loss: 2.2830629348754883
Validation loss: 2.5122441220027145

Epoch: 6| Step: 8
Training loss: 2.499614715576172
Validation loss: 2.518416622633575

Epoch: 6| Step: 9
Training loss: 3.4297733306884766
Validation loss: 2.5153905358365787

Epoch: 6| Step: 10
Training loss: 2.79573392868042
Validation loss: 2.516170814473142

Epoch: 6| Step: 11
Training loss: 2.2278566360473633
Validation loss: 2.522258935436126

Epoch: 6| Step: 12
Training loss: 3.156329393386841
Validation loss: 2.525067788298412

Epoch: 6| Step: 13
Training loss: 3.71584415435791
Validation loss: 2.523620449086671

Epoch: 142| Step: 0
Training loss: 1.8530800342559814
Validation loss: 2.5215420287142516

Epoch: 6| Step: 1
Training loss: 2.249192237854004
Validation loss: 2.515691613638273

Epoch: 6| Step: 2
Training loss: 2.7319650650024414
Validation loss: 2.5162010756872033

Epoch: 6| Step: 3
Training loss: 2.109694480895996
Validation loss: 2.5182649038171254

Epoch: 6| Step: 4
Training loss: 2.898228406906128
Validation loss: 2.5164879778380036

Epoch: 6| Step: 5
Training loss: 2.4833109378814697
Validation loss: 2.5177923120478147

Epoch: 6| Step: 6
Training loss: 2.9456634521484375
Validation loss: 2.5131870521012174

Epoch: 6| Step: 7
Training loss: 2.215622901916504
Validation loss: 2.516470847591277

Epoch: 6| Step: 8
Training loss: 2.8488869667053223
Validation loss: 2.5125593036733647

Epoch: 6| Step: 9
Training loss: 2.9722278118133545
Validation loss: 2.5122295835966706

Epoch: 6| Step: 10
Training loss: 3.3647613525390625
Validation loss: 2.5166663751807263

Epoch: 6| Step: 11
Training loss: 3.2890243530273438
Validation loss: 2.5116119794948126

Epoch: 6| Step: 12
Training loss: 3.0802810192108154
Validation loss: 2.5101444362312235

Epoch: 6| Step: 13
Training loss: 2.5411813259124756
Validation loss: 2.5098349048245336

Epoch: 143| Step: 0
Training loss: 2.067574977874756
Validation loss: 2.511314868927002

Epoch: 6| Step: 1
Training loss: 2.6181273460388184
Validation loss: 2.5122035164986887

Epoch: 6| Step: 2
Training loss: 2.3412551879882812
Validation loss: 2.509913465028168

Epoch: 6| Step: 3
Training loss: 3.050097942352295
Validation loss: 2.5159004785681285

Epoch: 6| Step: 4
Training loss: 3.11403226852417
Validation loss: 2.507190527454499

Epoch: 6| Step: 5
Training loss: 2.2987730503082275
Validation loss: 2.5104502939408824

Epoch: 6| Step: 6
Training loss: 3.489607572555542
Validation loss: 2.513856182816208

Epoch: 6| Step: 7
Training loss: 3.1317367553710938
Validation loss: 2.509664591922555

Epoch: 6| Step: 8
Training loss: 2.183635711669922
Validation loss: 2.510988943038448

Epoch: 6| Step: 9
Training loss: 3.072742223739624
Validation loss: 2.5109021420119912

Epoch: 6| Step: 10
Training loss: 3.379298210144043
Validation loss: 2.5096130319820937

Epoch: 6| Step: 11
Training loss: 2.0609989166259766
Validation loss: 2.5054824916265344

Epoch: 6| Step: 12
Training loss: 2.195969820022583
Validation loss: 2.509439488892914

Epoch: 6| Step: 13
Training loss: 2.731609582901001
Validation loss: 2.5138247141274075

Epoch: 144| Step: 0
Training loss: 3.2724571228027344
Validation loss: 2.513172875168503

Epoch: 6| Step: 1
Training loss: 2.5553781986236572
Validation loss: 2.5083117356864353

Epoch: 6| Step: 2
Training loss: 2.558950424194336
Validation loss: 2.509701487838581

Epoch: 6| Step: 3
Training loss: 2.8848674297332764
Validation loss: 2.5098003841215566

Epoch: 6| Step: 4
Training loss: 2.053086280822754
Validation loss: 2.5052854527709303

Epoch: 6| Step: 5
Training loss: 3.2074074745178223
Validation loss: 2.505290436488326

Epoch: 6| Step: 6
Training loss: 3.3869152069091797
Validation loss: 2.5076467913966023

Epoch: 6| Step: 7
Training loss: 2.6090450286865234
Validation loss: 2.509994383781187

Epoch: 6| Step: 8
Training loss: 2.6646008491516113
Validation loss: 2.5097993830198884

Epoch: 6| Step: 9
Training loss: 2.076422929763794
Validation loss: 2.506795196123021

Epoch: 6| Step: 10
Training loss: 2.5076029300689697
Validation loss: 2.5095806737099924

Epoch: 6| Step: 11
Training loss: 2.1424198150634766
Validation loss: 2.5105608432523665

Epoch: 6| Step: 12
Training loss: 2.4293036460876465
Validation loss: 2.5058026826509865

Epoch: 6| Step: 13
Training loss: 3.7686376571655273
Validation loss: 2.506764801599646

Epoch: 145| Step: 0
Training loss: 2.6382737159729004
Validation loss: 2.5046600962197907

Epoch: 6| Step: 1
Training loss: 2.5869557857513428
Validation loss: 2.5094345103028

Epoch: 6| Step: 2
Training loss: 2.0764806270599365
Validation loss: 2.515553653881114

Epoch: 6| Step: 3
Training loss: 2.5728180408477783
Validation loss: 2.5217755879125288

Epoch: 6| Step: 4
Training loss: 3.145681858062744
Validation loss: 2.5153658184953915

Epoch: 6| Step: 5
Training loss: 2.95815372467041
Validation loss: 2.515500840320382

Epoch: 6| Step: 6
Training loss: 3.031737804412842
Validation loss: 2.512846739061417

Epoch: 6| Step: 7
Training loss: 2.8055789470672607
Validation loss: 2.519186830007902

Epoch: 6| Step: 8
Training loss: 2.6962571144104004
Validation loss: 2.5168498280227825

Epoch: 6| Step: 9
Training loss: 1.9528173208236694
Validation loss: 2.5190233979173886

Epoch: 6| Step: 10
Training loss: 3.000093936920166
Validation loss: 2.5134801121168238

Epoch: 6| Step: 11
Training loss: 2.033430337905884
Validation loss: 2.5154018914827736

Epoch: 6| Step: 12
Training loss: 3.101008415222168
Validation loss: 2.513828892861643

Epoch: 6| Step: 13
Training loss: 3.270937919616699
Validation loss: 2.5117184218539985

Epoch: 146| Step: 0
Training loss: 2.458508014678955
Validation loss: 2.5178972418590257

Epoch: 6| Step: 1
Training loss: 2.0737345218658447
Validation loss: 2.520546467073502

Epoch: 6| Step: 2
Training loss: 2.669497013092041
Validation loss: 2.5146360910066994

Epoch: 6| Step: 3
Training loss: 2.897218942642212
Validation loss: 2.5233051751249578

Epoch: 6| Step: 4
Training loss: 2.3824949264526367
Validation loss: 2.5205013828892864

Epoch: 6| Step: 5
Training loss: 2.0136845111846924
Validation loss: 2.518176835070374

Epoch: 6| Step: 6
Training loss: 2.869652509689331
Validation loss: 2.5168342410877185

Epoch: 6| Step: 7
Training loss: 2.342416286468506
Validation loss: 2.515658378601074

Epoch: 6| Step: 8
Training loss: 2.787642002105713
Validation loss: 2.5090733189736643

Epoch: 6| Step: 9
Training loss: 2.747920513153076
Validation loss: 2.507263191284672

Epoch: 6| Step: 10
Training loss: 3.000066041946411
Validation loss: 2.5086389895408385

Epoch: 6| Step: 11
Training loss: 3.645099401473999
Validation loss: 2.505508622815532

Epoch: 6| Step: 12
Training loss: 3.073089599609375
Validation loss: 2.5062449106606106

Epoch: 6| Step: 13
Training loss: 2.690762758255005
Validation loss: 2.5102344071993263

Epoch: 147| Step: 0
Training loss: 2.612542152404785
Validation loss: 2.5055523764702583

Epoch: 6| Step: 1
Training loss: 2.5238990783691406
Validation loss: 2.5063439210255942

Epoch: 6| Step: 2
Training loss: 2.2932052612304688
Validation loss: 2.5081381361971617

Epoch: 6| Step: 3
Training loss: 3.0618603229522705
Validation loss: 2.5042149430962017

Epoch: 6| Step: 4
Training loss: 3.0993824005126953
Validation loss: 2.5052302268243607

Epoch: 6| Step: 5
Training loss: 2.914306640625
Validation loss: 2.5121141274770102

Epoch: 6| Step: 6
Training loss: 3.3256006240844727
Validation loss: 2.5041719187972364

Epoch: 6| Step: 7
Training loss: 2.9649300575256348
Validation loss: 2.4996916991408153

Epoch: 6| Step: 8
Training loss: 1.6874120235443115
Validation loss: 2.5021795790682555

Epoch: 6| Step: 9
Training loss: 2.555676221847534
Validation loss: 2.497649573510693

Epoch: 6| Step: 10
Training loss: 2.8504738807678223
Validation loss: 2.504634016303606

Epoch: 6| Step: 11
Training loss: 2.05234956741333
Validation loss: 2.5026871183867097

Epoch: 6| Step: 12
Training loss: 2.423163414001465
Validation loss: 2.5121235206562984

Epoch: 6| Step: 13
Training loss: 3.654592275619507
Validation loss: 2.5140746024347123

Epoch: 148| Step: 0
Training loss: 2.6329400539398193
Validation loss: 2.5150401515345417

Epoch: 6| Step: 1
Training loss: 2.5138235092163086
Validation loss: 2.5156932056591077

Epoch: 6| Step: 2
Training loss: 3.1069087982177734
Validation loss: 2.5117605399059992

Epoch: 6| Step: 3
Training loss: 3.1816582679748535
Validation loss: 2.5184996897174465

Epoch: 6| Step: 4
Training loss: 2.4082016944885254
Validation loss: 2.5101267419835573

Epoch: 6| Step: 5
Training loss: 2.2043609619140625
Validation loss: 2.5212101346702984

Epoch: 6| Step: 6
Training loss: 2.891159772872925
Validation loss: 2.5206450082922496

Epoch: 6| Step: 7
Training loss: 1.990769386291504
Validation loss: 2.52819654762104

Epoch: 6| Step: 8
Training loss: 2.572706699371338
Validation loss: 2.5117176989073395

Epoch: 6| Step: 9
Training loss: 3.410395383834839
Validation loss: 2.5089865987018873

Epoch: 6| Step: 10
Training loss: 2.4808692932128906
Validation loss: 2.504808441285164

Epoch: 6| Step: 11
Training loss: 2.4704089164733887
Validation loss: 2.5016540660653064

Epoch: 6| Step: 12
Training loss: 3.125736713409424
Validation loss: 2.507026764654344

Epoch: 6| Step: 13
Training loss: 2.604870319366455
Validation loss: 2.504167015834521

Epoch: 149| Step: 0
Training loss: 1.552709937095642
Validation loss: 2.507481598084973

Epoch: 6| Step: 1
Training loss: 3.0779764652252197
Validation loss: 2.5034348862145537

Epoch: 6| Step: 2
Training loss: 2.763679265975952
Validation loss: 2.5039124283739316

Epoch: 6| Step: 3
Training loss: 3.3567819595336914
Validation loss: 2.5110597943746917

Epoch: 6| Step: 4
Training loss: 2.0089409351348877
Validation loss: 2.501471363088136

Epoch: 6| Step: 5
Training loss: 2.707531452178955
Validation loss: 2.502271995749525

Epoch: 6| Step: 6
Training loss: 2.117002248764038
Validation loss: 2.5071134823624805

Epoch: 6| Step: 7
Training loss: 3.0900299549102783
Validation loss: 2.501896027595766

Epoch: 6| Step: 8
Training loss: 2.9686121940612793
Validation loss: 2.4989068943967103

Epoch: 6| Step: 9
Training loss: 3.219674587249756
Validation loss: 2.5033639374599663

Epoch: 6| Step: 10
Training loss: 1.9837068319320679
Validation loss: 2.504474716801797

Epoch: 6| Step: 11
Training loss: 2.8668572902679443
Validation loss: 2.504188532470375

Epoch: 6| Step: 12
Training loss: 2.906508445739746
Validation loss: 2.51031821004806

Epoch: 6| Step: 13
Training loss: 3.366213083267212
Validation loss: 2.509478512630668

Epoch: 150| Step: 0
Training loss: 2.3473033905029297
Validation loss: 2.5103655861270044

Epoch: 6| Step: 1
Training loss: 2.2806620597839355
Validation loss: 2.5067694110255085

Epoch: 6| Step: 2
Training loss: 2.8212718963623047
Validation loss: 2.505207723186862

Epoch: 6| Step: 3
Training loss: 2.3009352684020996
Validation loss: 2.501337843556558

Epoch: 6| Step: 4
Training loss: 2.9082021713256836
Validation loss: 2.501099422413816

Epoch: 6| Step: 5
Training loss: 2.3763644695281982
Validation loss: 2.4967821669834915

Epoch: 6| Step: 6
Training loss: 3.24151611328125
Validation loss: 2.4986792456719185

Epoch: 6| Step: 7
Training loss: 2.5627031326293945
Validation loss: 2.4998554978319394

Epoch: 6| Step: 8
Training loss: 3.9829835891723633
Validation loss: 2.49833575371773

Epoch: 6| Step: 9
Training loss: 2.860150098800659
Validation loss: 2.5000065526654645

Epoch: 6| Step: 10
Training loss: 2.325809955596924
Validation loss: 2.5005836961089924

Epoch: 6| Step: 11
Training loss: 2.931893825531006
Validation loss: 2.5003042297978557

Epoch: 6| Step: 12
Training loss: 2.079237461090088
Validation loss: 2.500647367969636

Epoch: 6| Step: 13
Training loss: 2.311156988143921
Validation loss: 2.5042331987811672

Epoch: 151| Step: 0
Training loss: 2.820598602294922
Validation loss: 2.505024984318723

Epoch: 6| Step: 1
Training loss: 1.8329087495803833
Validation loss: 2.5140466202971754

Epoch: 6| Step: 2
Training loss: 2.5915110111236572
Validation loss: 2.508253517971244

Epoch: 6| Step: 3
Training loss: 2.6196489334106445
Validation loss: 2.509312960409349

Epoch: 6| Step: 4
Training loss: 3.4926326274871826
Validation loss: 2.508662639125701

Epoch: 6| Step: 5
Training loss: 2.9775049686431885
Validation loss: 2.4972578966489403

Epoch: 6| Step: 6
Training loss: 2.2903389930725098
Validation loss: 2.495834791532127

Epoch: 6| Step: 7
Training loss: 3.0450639724731445
Validation loss: 2.497583907137635

Epoch: 6| Step: 8
Training loss: 2.4206702709198
Validation loss: 2.4954261523421093

Epoch: 6| Step: 9
Training loss: 2.528325319290161
Validation loss: 2.4992569954164567

Epoch: 6| Step: 10
Training loss: 2.9825799465179443
Validation loss: 2.4981826146443686

Epoch: 6| Step: 11
Training loss: 2.4995574951171875
Validation loss: 2.497236023667038

Epoch: 6| Step: 12
Training loss: 2.967815637588501
Validation loss: 2.495397642094602

Epoch: 6| Step: 13
Training loss: 2.3692538738250732
Validation loss: 2.495832645764915

Epoch: 152| Step: 0
Training loss: 2.6992592811584473
Validation loss: 2.4973592091632146

Epoch: 6| Step: 1
Training loss: 2.2114715576171875
Validation loss: 2.4948739556856054

Epoch: 6| Step: 2
Training loss: 2.902433156967163
Validation loss: 2.496922372489847

Epoch: 6| Step: 3
Training loss: 2.364675521850586
Validation loss: 2.5000247878413044

Epoch: 6| Step: 4
Training loss: 2.3771917819976807
Validation loss: 2.5000039069883284

Epoch: 6| Step: 5
Training loss: 2.783792018890381
Validation loss: 2.4910209460925032

Epoch: 6| Step: 6
Training loss: 3.115368366241455
Validation loss: 2.4933693050056376

Epoch: 6| Step: 7
Training loss: 3.0544698238372803
Validation loss: 2.497266830936555

Epoch: 6| Step: 8
Training loss: 2.5083534717559814
Validation loss: 2.4918379911812405

Epoch: 6| Step: 9
Training loss: 2.645526885986328
Validation loss: 2.493594641326576

Epoch: 6| Step: 10
Training loss: 2.787631034851074
Validation loss: 2.4923472917208107

Epoch: 6| Step: 11
Training loss: 2.5363283157348633
Validation loss: 2.4953977369493052

Epoch: 6| Step: 12
Training loss: 2.8845643997192383
Validation loss: 2.49112953165526

Epoch: 6| Step: 13
Training loss: 2.608445644378662
Validation loss: 2.4908453905454246

Epoch: 153| Step: 0
Training loss: 2.2172131538391113
Validation loss: 2.4922068977868683

Epoch: 6| Step: 1
Training loss: 2.2621569633483887
Validation loss: 2.489471814965689

Epoch: 6| Step: 2
Training loss: 2.8872268199920654
Validation loss: 2.4913432239204325

Epoch: 6| Step: 3
Training loss: 3.0106170177459717
Validation loss: 2.490934084820491

Epoch: 6| Step: 4
Training loss: 2.41876220703125
Validation loss: 2.493472058285949

Epoch: 6| Step: 5
Training loss: 2.7052903175354004
Validation loss: 2.4892279794139247

Epoch: 6| Step: 6
Training loss: 3.2323946952819824
Validation loss: 2.485437911043885

Epoch: 6| Step: 7
Training loss: 2.45794939994812
Validation loss: 2.4902431529055358

Epoch: 6| Step: 8
Training loss: 2.9086174964904785
Validation loss: 2.492644256161105

Epoch: 6| Step: 9
Training loss: 2.9386253356933594
Validation loss: 2.4956109575046006

Epoch: 6| Step: 10
Training loss: 3.21309232711792
Validation loss: 2.4899279455984793

Epoch: 6| Step: 11
Training loss: 2.435633659362793
Validation loss: 2.490827939843619

Epoch: 6| Step: 12
Training loss: 2.43371319770813
Validation loss: 2.49096240023131

Epoch: 6| Step: 13
Training loss: 2.079258680343628
Validation loss: 2.4921082886316444

Epoch: 154| Step: 0
Training loss: 3.057980537414551
Validation loss: 2.4955039588353967

Epoch: 6| Step: 1
Training loss: 2.97636079788208
Validation loss: 2.4918806117068053

Epoch: 6| Step: 2
Training loss: 3.056459426879883
Validation loss: 2.4935551843335553

Epoch: 6| Step: 3
Training loss: 2.9132308959960938
Validation loss: 2.4965175890153453

Epoch: 6| Step: 4
Training loss: 1.8894495964050293
Validation loss: 2.4933950721576648

Epoch: 6| Step: 5
Training loss: 2.7075963020324707
Validation loss: 2.4942282451096403

Epoch: 6| Step: 6
Training loss: 2.7185466289520264
Validation loss: 2.493165011047035

Epoch: 6| Step: 7
Training loss: 2.814535140991211
Validation loss: 2.4989952656530563

Epoch: 6| Step: 8
Training loss: 3.3771371841430664
Validation loss: 2.4976191469418105

Epoch: 6| Step: 9
Training loss: 1.562957525253296
Validation loss: 2.491350980215175

Epoch: 6| Step: 10
Training loss: 2.6521639823913574
Validation loss: 2.494168568682927

Epoch: 6| Step: 11
Training loss: 3.212042808532715
Validation loss: 2.491836340196671

Epoch: 6| Step: 12
Training loss: 2.412900924682617
Validation loss: 2.496056813065724

Epoch: 6| Step: 13
Training loss: 1.5205166339874268
Validation loss: 2.4931502239678496

Epoch: 155| Step: 0
Training loss: 3.1896238327026367
Validation loss: 2.494180735721383

Epoch: 6| Step: 1
Training loss: 2.5957961082458496
Validation loss: 2.501667043214203

Epoch: 6| Step: 2
Training loss: 2.3418915271759033
Validation loss: 2.5122854119987896

Epoch: 6| Step: 3
Training loss: 2.5727198123931885
Validation loss: 2.4985905847241803

Epoch: 6| Step: 4
Training loss: 2.9498727321624756
Validation loss: 2.4927057925090996

Epoch: 6| Step: 5
Training loss: 2.214233160018921
Validation loss: 2.493502260536276

Epoch: 6| Step: 6
Training loss: 1.8179703950881958
Validation loss: 2.4958752355267926

Epoch: 6| Step: 7
Training loss: 2.8282036781311035
Validation loss: 2.4897380080274356

Epoch: 6| Step: 8
Training loss: 3.018075466156006
Validation loss: 2.4900521642418316

Epoch: 6| Step: 9
Training loss: 2.4368178844451904
Validation loss: 2.4960413158580823

Epoch: 6| Step: 10
Training loss: 2.8003807067871094
Validation loss: 2.4882131853411273

Epoch: 6| Step: 11
Training loss: 2.650026321411133
Validation loss: 2.4888263081991546

Epoch: 6| Step: 12
Training loss: 3.5796561241149902
Validation loss: 2.4915355110681183

Epoch: 6| Step: 13
Training loss: 2.3359670639038086
Validation loss: 2.488178568501626

Epoch: 156| Step: 0
Training loss: 2.9727907180786133
Validation loss: 2.49105804709978

Epoch: 6| Step: 1
Training loss: 2.34413480758667
Validation loss: 2.4921330662183863

Epoch: 6| Step: 2
Training loss: 2.5955898761749268
Validation loss: 2.4883933016048965

Epoch: 6| Step: 3
Training loss: 2.6359384059906006
Validation loss: 2.4893689437579085

Epoch: 6| Step: 4
Training loss: 3.2850282192230225
Validation loss: 2.490428114450106

Epoch: 6| Step: 5
Training loss: 2.6591057777404785
Validation loss: 2.487156932071973

Epoch: 6| Step: 6
Training loss: 2.596325159072876
Validation loss: 2.487288354545511

Epoch: 6| Step: 7
Training loss: 2.3812735080718994
Validation loss: 2.489792141863095

Epoch: 6| Step: 8
Training loss: 2.8421268463134766
Validation loss: 2.486522477160218

Epoch: 6| Step: 9
Training loss: 2.243906259536743
Validation loss: 2.485213510451778

Epoch: 6| Step: 10
Training loss: 2.8425323963165283
Validation loss: 2.4898336843777726

Epoch: 6| Step: 11
Training loss: 2.112771987915039
Validation loss: 2.486746588060933

Epoch: 6| Step: 12
Training loss: 3.129133939743042
Validation loss: 2.4928264669192735

Epoch: 6| Step: 13
Training loss: 2.8517401218414307
Validation loss: 2.4938470202107585

Epoch: 157| Step: 0
Training loss: 2.2581264972686768
Validation loss: 2.494648569373674

Epoch: 6| Step: 1
Training loss: 2.6639785766601562
Validation loss: 2.4945498692092074

Epoch: 6| Step: 2
Training loss: 3.267618417739868
Validation loss: 2.5003285920748146

Epoch: 6| Step: 3
Training loss: 2.606750011444092
Validation loss: 2.4969422842866633

Epoch: 6| Step: 4
Training loss: 3.141026496887207
Validation loss: 2.4995968982737553

Epoch: 6| Step: 5
Training loss: 2.00510835647583
Validation loss: 2.4955366093625306

Epoch: 6| Step: 6
Training loss: 3.5354037284851074
Validation loss: 2.4897073238126692

Epoch: 6| Step: 7
Training loss: 2.269779682159424
Validation loss: 2.48631380450341

Epoch: 6| Step: 8
Training loss: 2.960418701171875
Validation loss: 2.4861865197458575

Epoch: 6| Step: 9
Training loss: 2.3333027362823486
Validation loss: 2.4927288639929985

Epoch: 6| Step: 10
Training loss: 2.4055428504943848
Validation loss: 2.492077581344112

Epoch: 6| Step: 11
Training loss: 2.360536575317383
Validation loss: 2.490337097516624

Epoch: 6| Step: 12
Training loss: 3.3673548698425293
Validation loss: 2.4909190554772653

Epoch: 6| Step: 13
Training loss: 2.0862605571746826
Validation loss: 2.494054368747178

Epoch: 158| Step: 0
Training loss: 2.2861266136169434
Validation loss: 2.493522180024014

Epoch: 6| Step: 1
Training loss: 3.0756869316101074
Validation loss: 2.4932517851552656

Epoch: 6| Step: 2
Training loss: 2.6273915767669678
Validation loss: 2.4889890647703603

Epoch: 6| Step: 3
Training loss: 3.1752123832702637
Validation loss: 2.485746358030586

Epoch: 6| Step: 4
Training loss: 2.102719783782959
Validation loss: 2.4864777993130427

Epoch: 6| Step: 5
Training loss: 2.3528223037719727
Validation loss: 2.4874875724956556

Epoch: 6| Step: 6
Training loss: 2.7251639366149902
Validation loss: 2.4897693203341578

Epoch: 6| Step: 7
Training loss: 2.535773277282715
Validation loss: 2.495174736104986

Epoch: 6| Step: 8
Training loss: 3.127633571624756
Validation loss: 2.4934197959079536

Epoch: 6| Step: 9
Training loss: 2.755889892578125
Validation loss: 2.5114266013586395

Epoch: 6| Step: 10
Training loss: 2.6174020767211914
Validation loss: 2.514076384164954

Epoch: 6| Step: 11
Training loss: 2.5854387283325195
Validation loss: 2.5009583555242068

Epoch: 6| Step: 12
Training loss: 2.916132926940918
Validation loss: 2.5071434436305875

Epoch: 6| Step: 13
Training loss: 2.549133062362671
Validation loss: 2.5077620142249653

Epoch: 159| Step: 0
Training loss: 2.7441978454589844
Validation loss: 2.499920563031268

Epoch: 6| Step: 1
Training loss: 2.5605669021606445
Validation loss: 2.5075465863750828

Epoch: 6| Step: 2
Training loss: 2.691037893295288
Validation loss: 2.4942724525287585

Epoch: 6| Step: 3
Training loss: 2.9446334838867188
Validation loss: 2.483517913408177

Epoch: 6| Step: 4
Training loss: 2.698345899581909
Validation loss: 2.484252873287406

Epoch: 6| Step: 5
Training loss: 1.9572467803955078
Validation loss: 2.481148806951379

Epoch: 6| Step: 6
Training loss: 2.4814133644104004
Validation loss: 2.4842988880731727

Epoch: 6| Step: 7
Training loss: 2.369954824447632
Validation loss: 2.4889948111708446

Epoch: 6| Step: 8
Training loss: 2.4393422603607178
Validation loss: 2.484451901528143

Epoch: 6| Step: 9
Training loss: 2.8611295223236084
Validation loss: 2.487188116196663

Epoch: 6| Step: 10
Training loss: 2.7835373878479004
Validation loss: 2.491205617945681

Epoch: 6| Step: 11
Training loss: 2.8858437538146973
Validation loss: 2.4877962655918573

Epoch: 6| Step: 12
Training loss: 3.2726547718048096
Validation loss: 2.48974585276778

Epoch: 6| Step: 13
Training loss: 2.9333345890045166
Validation loss: 2.4813891944064888

Epoch: 160| Step: 0
Training loss: 3.3443143367767334
Validation loss: 2.4836607902280745

Epoch: 6| Step: 1
Training loss: 2.9792518615722656
Validation loss: 2.4787073289194415

Epoch: 6| Step: 2
Training loss: 2.0204858779907227
Validation loss: 2.4797738623875443

Epoch: 6| Step: 3
Training loss: 2.3840930461883545
Validation loss: 2.4863150324872745

Epoch: 6| Step: 4
Training loss: 1.9840891361236572
Validation loss: 2.4808043690137964

Epoch: 6| Step: 5
Training loss: 2.8922762870788574
Validation loss: 2.4842334998551237

Epoch: 6| Step: 6
Training loss: 2.582508087158203
Validation loss: 2.4833624568036807

Epoch: 6| Step: 7
Training loss: 1.7164040803909302
Validation loss: 2.492038780643094

Epoch: 6| Step: 8
Training loss: 3.027838945388794
Validation loss: 2.5000571050951557

Epoch: 6| Step: 9
Training loss: 2.46219539642334
Validation loss: 2.49600496087023

Epoch: 6| Step: 10
Training loss: 2.7497711181640625
Validation loss: 2.5115516967670892

Epoch: 6| Step: 11
Training loss: 2.5694613456726074
Validation loss: 2.519017582298607

Epoch: 6| Step: 12
Training loss: 3.3965396881103516
Validation loss: 2.509211012112197

Epoch: 6| Step: 13
Training loss: 3.7296266555786133
Validation loss: 2.5163557965268373

Epoch: 161| Step: 0
Training loss: 2.6582698822021484
Validation loss: 2.497307905586817

Epoch: 6| Step: 1
Training loss: 1.913774013519287
Validation loss: 2.495336955593478

Epoch: 6| Step: 2
Training loss: 2.6643643379211426
Validation loss: 2.4829791361285793

Epoch: 6| Step: 3
Training loss: 2.883657455444336
Validation loss: 2.483328337310463

Epoch: 6| Step: 4
Training loss: 3.4322757720947266
Validation loss: 2.4836716857007755

Epoch: 6| Step: 5
Training loss: 2.228771448135376
Validation loss: 2.4825682229893182

Epoch: 6| Step: 6
Training loss: 2.358330249786377
Validation loss: 2.480456618852513

Epoch: 6| Step: 7
Training loss: 2.7098865509033203
Validation loss: 2.486092241861487

Epoch: 6| Step: 8
Training loss: 2.3991451263427734
Validation loss: 2.4839473437237483

Epoch: 6| Step: 9
Training loss: 2.773803949356079
Validation loss: 2.481781610878565

Epoch: 6| Step: 10
Training loss: 2.941438913345337
Validation loss: 2.4801311262192263

Epoch: 6| Step: 11
Training loss: 3.075881242752075
Validation loss: 2.4816472389364757

Epoch: 6| Step: 12
Training loss: 2.529641628265381
Validation loss: 2.483534687308855

Epoch: 6| Step: 13
Training loss: 2.8902997970581055
Validation loss: 2.4826050189233597

Epoch: 162| Step: 0
Training loss: 2.531317949295044
Validation loss: 2.477584292811732

Epoch: 6| Step: 1
Training loss: 2.6078035831451416
Validation loss: 2.4785606963660127

Epoch: 6| Step: 2
Training loss: 2.636439323425293
Validation loss: 2.478107693374798

Epoch: 6| Step: 3
Training loss: 2.871598243713379
Validation loss: 2.483239922472226

Epoch: 6| Step: 4
Training loss: 2.6161530017852783
Validation loss: 2.482418337175923

Epoch: 6| Step: 5
Training loss: 2.724545478820801
Validation loss: 2.4859789879091325

Epoch: 6| Step: 6
Training loss: 2.57228422164917
Validation loss: 2.486957824358376

Epoch: 6| Step: 7
Training loss: 2.1986896991729736
Validation loss: 2.4876328847741567

Epoch: 6| Step: 8
Training loss: 3.1697535514831543
Validation loss: 2.4886194531635573

Epoch: 6| Step: 9
Training loss: 2.902050018310547
Validation loss: 2.4867224347206855

Epoch: 6| Step: 10
Training loss: 2.7500953674316406
Validation loss: 2.484164191830543

Epoch: 6| Step: 11
Training loss: 3.0504744052886963
Validation loss: 2.4905530496310164

Epoch: 6| Step: 12
Training loss: 2.5716323852539062
Validation loss: 2.4868787180992866

Epoch: 6| Step: 13
Training loss: 1.6981701850891113
Validation loss: 2.477340723878594

Epoch: 163| Step: 0
Training loss: 2.0710551738739014
Validation loss: 2.4818425229800645

Epoch: 6| Step: 1
Training loss: 3.6415765285491943
Validation loss: 2.4792425658113215

Epoch: 6| Step: 2
Training loss: 2.5523602962493896
Validation loss: 2.479417049756614

Epoch: 6| Step: 3
Training loss: 2.5252270698547363
Validation loss: 2.48314687385354

Epoch: 6| Step: 4
Training loss: 2.2989916801452637
Validation loss: 2.4830385536275883

Epoch: 6| Step: 5
Training loss: 2.615291118621826
Validation loss: 2.4814402749461513

Epoch: 6| Step: 6
Training loss: 3.0611205101013184
Validation loss: 2.4833960225505214

Epoch: 6| Step: 7
Training loss: 1.9591262340545654
Validation loss: 2.4880698675750406

Epoch: 6| Step: 8
Training loss: 2.539198875427246
Validation loss: 2.484360905103786

Epoch: 6| Step: 9
Training loss: 2.3029656410217285
Validation loss: 2.4847702595495407

Epoch: 6| Step: 10
Training loss: 3.8796803951263428
Validation loss: 2.487586832815601

Epoch: 6| Step: 11
Training loss: 2.6844170093536377
Validation loss: 2.486251700309015

Epoch: 6| Step: 12
Training loss: 2.7585301399230957
Validation loss: 2.483669650170111

Epoch: 6| Step: 13
Training loss: 2.1479756832122803
Validation loss: 2.478153387705485

Epoch: 164| Step: 0
Training loss: 2.981921434402466
Validation loss: 2.4818180043210267

Epoch: 6| Step: 1
Training loss: 3.174386978149414
Validation loss: 2.476486057363531

Epoch: 6| Step: 2
Training loss: 2.034554958343506
Validation loss: 2.4771468972647064

Epoch: 6| Step: 3
Training loss: 3.187807321548462
Validation loss: 2.4770563648593042

Epoch: 6| Step: 4
Training loss: 2.709784984588623
Validation loss: 2.477699866858862

Epoch: 6| Step: 5
Training loss: 2.9783647060394287
Validation loss: 2.4800676966226227

Epoch: 6| Step: 6
Training loss: 3.0745911598205566
Validation loss: 2.4770053996834704

Epoch: 6| Step: 7
Training loss: 1.8888603448867798
Validation loss: 2.48237044067793

Epoch: 6| Step: 8
Training loss: 2.0080742835998535
Validation loss: 2.4882126033947034

Epoch: 6| Step: 9
Training loss: 2.5719377994537354
Validation loss: 2.4810144106547036

Epoch: 6| Step: 10
Training loss: 2.762254238128662
Validation loss: 2.4823324423964306

Epoch: 6| Step: 11
Training loss: 2.495391845703125
Validation loss: 2.4826904189202095

Epoch: 6| Step: 12
Training loss: 2.4764721393585205
Validation loss: 2.4738514320824736

Epoch: 6| Step: 13
Training loss: 3.036829948425293
Validation loss: 2.4787873196345505

Epoch: 165| Step: 0
Training loss: 2.9935312271118164
Validation loss: 2.4739373883893414

Epoch: 6| Step: 1
Training loss: 3.074185609817505
Validation loss: 2.478570022890645

Epoch: 6| Step: 2
Training loss: 2.111351251602173
Validation loss: 2.4785231031397337

Epoch: 6| Step: 3
Training loss: 2.7682008743286133
Validation loss: 2.477098846948275

Epoch: 6| Step: 4
Training loss: 2.5718765258789062
Validation loss: 2.475704287969938

Epoch: 6| Step: 5
Training loss: 2.9341118335723877
Validation loss: 2.479598235058528

Epoch: 6| Step: 6
Training loss: 3.0383214950561523
Validation loss: 2.4756613803166214

Epoch: 6| Step: 7
Training loss: 2.0361785888671875
Validation loss: 2.4829063030981247

Epoch: 6| Step: 8
Training loss: 2.6924071311950684
Validation loss: 2.479787071545919

Epoch: 6| Step: 9
Training loss: 2.434711456298828
Validation loss: 2.4790037473042807

Epoch: 6| Step: 10
Training loss: 2.5954744815826416
Validation loss: 2.4792344262523036

Epoch: 6| Step: 11
Training loss: 2.6591579914093018
Validation loss: 2.4763238481296006

Epoch: 6| Step: 12
Training loss: 2.553446054458618
Validation loss: 2.481128038898591

Epoch: 6| Step: 13
Training loss: 2.8606934547424316
Validation loss: 2.485411177399338

Epoch: 166| Step: 0
Training loss: 1.9889256954193115
Validation loss: 2.4762493718054985

Epoch: 6| Step: 1
Training loss: 1.668886423110962
Validation loss: 2.4804531348648893

Epoch: 6| Step: 2
Training loss: 2.9690988063812256
Validation loss: 2.476870100985291

Epoch: 6| Step: 3
Training loss: 2.7662508487701416
Validation loss: 2.4778584485412924

Epoch: 6| Step: 4
Training loss: 2.5061771869659424
Validation loss: 2.47558436342465

Epoch: 6| Step: 5
Training loss: 2.4348390102386475
Validation loss: 2.478637710694344

Epoch: 6| Step: 6
Training loss: 3.2857983112335205
Validation loss: 2.472838815822396

Epoch: 6| Step: 7
Training loss: 3.1417064666748047
Validation loss: 2.4805392885720856

Epoch: 6| Step: 8
Training loss: 1.923950433731079
Validation loss: 2.485460922282229

Epoch: 6| Step: 9
Training loss: 2.4154162406921387
Validation loss: 2.4870572731059086

Epoch: 6| Step: 10
Training loss: 2.9775304794311523
Validation loss: 2.4805341843635804

Epoch: 6| Step: 11
Training loss: 3.2687554359436035
Validation loss: 2.4917845931104434

Epoch: 6| Step: 12
Training loss: 3.268537998199463
Validation loss: 2.4847387011333177

Epoch: 6| Step: 13
Training loss: 2.599546194076538
Validation loss: 2.4927132873124975

Epoch: 167| Step: 0
Training loss: 3.093323230743408
Validation loss: 2.504537756725024

Epoch: 6| Step: 1
Training loss: 2.081265449523926
Validation loss: 2.5089532226644535

Epoch: 6| Step: 2
Training loss: 2.4091262817382812
Validation loss: 2.514836521558864

Epoch: 6| Step: 3
Training loss: 2.70965838432312
Validation loss: 2.496914243185392

Epoch: 6| Step: 4
Training loss: 3.43125581741333
Validation loss: 2.496246099472046

Epoch: 6| Step: 5
Training loss: 2.477832555770874
Validation loss: 2.482104029706729

Epoch: 6| Step: 6
Training loss: 2.5163445472717285
Validation loss: 2.4801429933117283

Epoch: 6| Step: 7
Training loss: 2.5115981101989746
Validation loss: 2.477378260704779

Epoch: 6| Step: 8
Training loss: 2.5863280296325684
Validation loss: 2.476209138029365

Epoch: 6| Step: 9
Training loss: 2.8216772079467773
Validation loss: 2.4716899882080736

Epoch: 6| Step: 10
Training loss: 3.2669761180877686
Validation loss: 2.4673044502094226

Epoch: 6| Step: 11
Training loss: 2.320641040802002
Validation loss: 2.474638754321683

Epoch: 6| Step: 12
Training loss: 2.1610748767852783
Validation loss: 2.469699690418859

Epoch: 6| Step: 13
Training loss: 3.0056657791137695
Validation loss: 2.4768776380887596

Epoch: 168| Step: 0
Training loss: 3.0727648735046387
Validation loss: 2.4793205773958595

Epoch: 6| Step: 1
Training loss: 2.3829445838928223
Validation loss: 2.476014091122535

Epoch: 6| Step: 2
Training loss: 2.514634132385254
Validation loss: 2.48803888341432

Epoch: 6| Step: 3
Training loss: 2.6647191047668457
Validation loss: 2.4921005259278

Epoch: 6| Step: 4
Training loss: 2.2900867462158203
Validation loss: 2.489954666424823

Epoch: 6| Step: 5
Training loss: 2.8992621898651123
Validation loss: 2.493959516607305

Epoch: 6| Step: 6
Training loss: 2.903411865234375
Validation loss: 2.496268359563684

Epoch: 6| Step: 7
Training loss: 2.862973213195801
Validation loss: 2.4936551176091677

Epoch: 6| Step: 8
Training loss: 2.1099307537078857
Validation loss: 2.496258484419956

Epoch: 6| Step: 9
Training loss: 2.190732479095459
Validation loss: 2.49883609689692

Epoch: 6| Step: 10
Training loss: 2.824411392211914
Validation loss: 2.4912896233220256

Epoch: 6| Step: 11
Training loss: 3.2054219245910645
Validation loss: 2.4897619216672835

Epoch: 6| Step: 12
Training loss: 3.0220649242401123
Validation loss: 2.479308912830968

Epoch: 6| Step: 13
Training loss: 2.075467348098755
Validation loss: 2.474036752536733

Epoch: 169| Step: 0
Training loss: 2.762845993041992
Validation loss: 2.4739597894812144

Epoch: 6| Step: 1
Training loss: 2.3107759952545166
Validation loss: 2.4696051561704246

Epoch: 6| Step: 2
Training loss: 2.9670357704162598
Validation loss: 2.4714491290430867

Epoch: 6| Step: 3
Training loss: 2.8133673667907715
Validation loss: 2.4683650821767826

Epoch: 6| Step: 4
Training loss: 2.1920056343078613
Validation loss: 2.4698111318772837

Epoch: 6| Step: 5
Training loss: 2.829774856567383
Validation loss: 2.467460593869609

Epoch: 6| Step: 6
Training loss: 3.2033615112304688
Validation loss: 2.4669233804107993

Epoch: 6| Step: 7
Training loss: 2.1765661239624023
Validation loss: 2.466829945964198

Epoch: 6| Step: 8
Training loss: 3.022855043411255
Validation loss: 2.4631487361846434

Epoch: 6| Step: 9
Training loss: 3.2608232498168945
Validation loss: 2.470565193442888

Epoch: 6| Step: 10
Training loss: 2.3021082878112793
Validation loss: 2.470062791660268

Epoch: 6| Step: 11
Training loss: 2.198650360107422
Validation loss: 2.4621829960935857

Epoch: 6| Step: 12
Training loss: 2.5711758136749268
Validation loss: 2.474858635215349

Epoch: 6| Step: 13
Training loss: 2.4507384300231934
Validation loss: 2.4746933342308126

Epoch: 170| Step: 0
Training loss: 3.1556477546691895
Validation loss: 2.4752033474624797

Epoch: 6| Step: 1
Training loss: 2.903898000717163
Validation loss: 2.4719569657438543

Epoch: 6| Step: 2
Training loss: 3.036729335784912
Validation loss: 2.47396380542427

Epoch: 6| Step: 3
Training loss: 2.9211478233337402
Validation loss: 2.4696398473555043

Epoch: 6| Step: 4
Training loss: 2.7951714992523193
Validation loss: 2.461321012948149

Epoch: 6| Step: 5
Training loss: 3.485819101333618
Validation loss: 2.4626424645864837

Epoch: 6| Step: 6
Training loss: 2.7525603771209717
Validation loss: 2.461411609444567

Epoch: 6| Step: 7
Training loss: 2.1801977157592773
Validation loss: 2.4558694670277257

Epoch: 6| Step: 8
Training loss: 1.4578324556350708
Validation loss: 2.4631401364521315

Epoch: 6| Step: 9
Training loss: 2.403000831604004
Validation loss: 2.4571434784961004

Epoch: 6| Step: 10
Training loss: 2.0783603191375732
Validation loss: 2.4610943973705335

Epoch: 6| Step: 11
Training loss: 3.0000152587890625
Validation loss: 2.454966155431604

Epoch: 6| Step: 12
Training loss: 2.4384775161743164
Validation loss: 2.463072758848949

Epoch: 6| Step: 13
Training loss: 2.640019655227661
Validation loss: 2.4545635843789704

Epoch: 171| Step: 0
Training loss: 1.6460976600646973
Validation loss: 2.4626422030951387

Epoch: 6| Step: 1
Training loss: 3.297470808029175
Validation loss: 2.460570050824073

Epoch: 6| Step: 2
Training loss: 2.8987624645233154
Validation loss: 2.4604899498724166

Epoch: 6| Step: 3
Training loss: 2.85693359375
Validation loss: 2.4645506233297367

Epoch: 6| Step: 4
Training loss: 2.3732261657714844
Validation loss: 2.4678184063203874

Epoch: 6| Step: 5
Training loss: 2.364891290664673
Validation loss: 2.4739611623107747

Epoch: 6| Step: 6
Training loss: 2.3952550888061523
Validation loss: 2.4769952912484445

Epoch: 6| Step: 7
Training loss: 3.011794090270996
Validation loss: 2.489112195148263

Epoch: 6| Step: 8
Training loss: 3.090137243270874
Validation loss: 2.4889564924342658

Epoch: 6| Step: 9
Training loss: 2.579559564590454
Validation loss: 2.490577579826437

Epoch: 6| Step: 10
Training loss: 2.9215121269226074
Validation loss: 2.492600602488364

Epoch: 6| Step: 11
Training loss: 2.513023853302002
Validation loss: 2.483079630841491

Epoch: 6| Step: 12
Training loss: 2.385317325592041
Validation loss: 2.4849407288335983

Epoch: 6| Step: 13
Training loss: 2.939774990081787
Validation loss: 2.483640901504024

Epoch: 172| Step: 0
Training loss: 1.8840880393981934
Validation loss: 2.4848867693255023

Epoch: 6| Step: 1
Training loss: 2.8678464889526367
Validation loss: 2.475718682812106

Epoch: 6| Step: 2
Training loss: 3.124492645263672
Validation loss: 2.464270296917167

Epoch: 6| Step: 3
Training loss: 3.6870903968811035
Validation loss: 2.4560515239674556

Epoch: 6| Step: 4
Training loss: 2.735705852508545
Validation loss: 2.459117353603404

Epoch: 6| Step: 5
Training loss: 2.2064619064331055
Validation loss: 2.4577532250394105

Epoch: 6| Step: 6
Training loss: 3.2018487453460693
Validation loss: 2.4559505703628703

Epoch: 6| Step: 7
Training loss: 2.17744517326355
Validation loss: 2.453607156712522

Epoch: 6| Step: 8
Training loss: 2.1321632862091064
Validation loss: 2.4590492684354066

Epoch: 6| Step: 9
Training loss: 2.6500935554504395
Validation loss: 2.45457858936761

Epoch: 6| Step: 10
Training loss: 2.9494335651397705
Validation loss: 2.4562289355903544

Epoch: 6| Step: 11
Training loss: 3.0392837524414062
Validation loss: 2.4558467659898984

Epoch: 6| Step: 12
Training loss: 2.2454304695129395
Validation loss: 2.4560199834967174

Epoch: 6| Step: 13
Training loss: 2.134138345718384
Validation loss: 2.4582068304861746

Epoch: 173| Step: 0
Training loss: 3.0015339851379395
Validation loss: 2.46055095682862

Epoch: 6| Step: 1
Training loss: 2.5623486042022705
Validation loss: 2.4611852527946554

Epoch: 6| Step: 2
Training loss: 3.2047014236450195
Validation loss: 2.4618877672380015

Epoch: 6| Step: 3
Training loss: 2.896892786026001
Validation loss: 2.463460773550054

Epoch: 6| Step: 4
Training loss: 2.685814619064331
Validation loss: 2.4583374659220376

Epoch: 6| Step: 5
Training loss: 2.759657859802246
Validation loss: 2.4649790102435696

Epoch: 6| Step: 6
Training loss: 2.6144800186157227
Validation loss: 2.472050723209176

Epoch: 6| Step: 7
Training loss: 2.901289463043213
Validation loss: 2.471739443399573

Epoch: 6| Step: 8
Training loss: 2.0616776943206787
Validation loss: 2.4659737002465034

Epoch: 6| Step: 9
Training loss: 2.2093725204467773
Validation loss: 2.4632339092992965

Epoch: 6| Step: 10
Training loss: 3.0574746131896973
Validation loss: 2.463375142825547

Epoch: 6| Step: 11
Training loss: 2.7515206336975098
Validation loss: 2.461988377314742

Epoch: 6| Step: 12
Training loss: 2.6666736602783203
Validation loss: 2.4657519248224076

Epoch: 6| Step: 13
Training loss: 1.1448864936828613
Validation loss: 2.4562537849590345

Epoch: 174| Step: 0
Training loss: 2.5971105098724365
Validation loss: 2.4579128244871735

Epoch: 6| Step: 1
Training loss: 2.9470763206481934
Validation loss: 2.4630275657100063

Epoch: 6| Step: 2
Training loss: 1.8825957775115967
Validation loss: 2.458130944159723

Epoch: 6| Step: 3
Training loss: 2.861684799194336
Validation loss: 2.4611114173807125

Epoch: 6| Step: 4
Training loss: 3.0389466285705566
Validation loss: 2.461193223153391

Epoch: 6| Step: 5
Training loss: 2.9589884281158447
Validation loss: 2.460491411147579

Epoch: 6| Step: 6
Training loss: 3.2350621223449707
Validation loss: 2.4592804114023843

Epoch: 6| Step: 7
Training loss: 2.73360276222229
Validation loss: 2.461279135878368

Epoch: 6| Step: 8
Training loss: 2.5798914432525635
Validation loss: 2.4673840204874673

Epoch: 6| Step: 9
Training loss: 2.0563080310821533
Validation loss: 2.4768008006516324

Epoch: 6| Step: 10
Training loss: 2.366536855697632
Validation loss: 2.4742071141478834

Epoch: 6| Step: 11
Training loss: 3.004387855529785
Validation loss: 2.4744694514941146

Epoch: 6| Step: 12
Training loss: 2.142913818359375
Validation loss: 2.474344717558994

Epoch: 6| Step: 13
Training loss: 2.696084976196289
Validation loss: 2.47462728459348

Epoch: 175| Step: 0
Training loss: 2.309795618057251
Validation loss: 2.4720102561417447

Epoch: 6| Step: 1
Training loss: 2.9714746475219727
Validation loss: 2.474146863465668

Epoch: 6| Step: 2
Training loss: 3.04103946685791
Validation loss: 2.4686875343322754

Epoch: 6| Step: 3
Training loss: 2.1533279418945312
Validation loss: 2.4772285646007908

Epoch: 6| Step: 4
Training loss: 2.149362087249756
Validation loss: 2.470838233988772

Epoch: 6| Step: 5
Training loss: 3.1317873001098633
Validation loss: 2.4701708414221324

Epoch: 6| Step: 6
Training loss: 2.5711708068847656
Validation loss: 2.476259123894476

Epoch: 6| Step: 7
Training loss: 2.4539356231689453
Validation loss: 2.4685129657868417

Epoch: 6| Step: 8
Training loss: 1.800788164138794
Validation loss: 2.4766056511991765

Epoch: 6| Step: 9
Training loss: 2.8283984661102295
Validation loss: 2.464178974910449

Epoch: 6| Step: 10
Training loss: 3.1437525749206543
Validation loss: 2.4690603620262555

Epoch: 6| Step: 11
Training loss: 2.9743576049804688
Validation loss: 2.472373644510905

Epoch: 6| Step: 12
Training loss: 2.9953415393829346
Validation loss: 2.467124395473029

Epoch: 6| Step: 13
Training loss: 2.6017873287200928
Validation loss: 2.4628364142551216

Epoch: 176| Step: 0
Training loss: 1.9028544425964355
Validation loss: 2.4682363540895524

Epoch: 6| Step: 1
Training loss: 2.5128631591796875
Validation loss: 2.470754628540367

Epoch: 6| Step: 2
Training loss: 2.410884141921997
Validation loss: 2.465318992573728

Epoch: 6| Step: 3
Training loss: 2.949039936065674
Validation loss: 2.464039707696566

Epoch: 6| Step: 4
Training loss: 2.8609228134155273
Validation loss: 2.461485457676713

Epoch: 6| Step: 5
Training loss: 2.3147130012512207
Validation loss: 2.461511088955787

Epoch: 6| Step: 6
Training loss: 2.8917598724365234
Validation loss: 2.459316833044893

Epoch: 6| Step: 7
Training loss: 2.1909220218658447
Validation loss: 2.462743213099818

Epoch: 6| Step: 8
Training loss: 3.3118362426757812
Validation loss: 2.4592322290584607

Epoch: 6| Step: 9
Training loss: 3.220243215560913
Validation loss: 2.4648667535474225

Epoch: 6| Step: 10
Training loss: 1.67909574508667
Validation loss: 2.461047059746199

Epoch: 6| Step: 11
Training loss: 3.185490131378174
Validation loss: 2.4633439663917787

Epoch: 6| Step: 12
Training loss: 2.894148111343384
Validation loss: 2.4626430696056736

Epoch: 6| Step: 13
Training loss: 2.8322956562042236
Validation loss: 2.463562483428627

Epoch: 177| Step: 0
Training loss: 2.337674856185913
Validation loss: 2.4611256173861924

Epoch: 6| Step: 1
Training loss: 2.378359317779541
Validation loss: 2.468784293820781

Epoch: 6| Step: 2
Training loss: 2.2677698135375977
Validation loss: 2.4637752912377797

Epoch: 6| Step: 3
Training loss: 3.056121349334717
Validation loss: 2.469672294073207

Epoch: 6| Step: 4
Training loss: 2.786332607269287
Validation loss: 2.4708884121269308

Epoch: 6| Step: 5
Training loss: 3.0612454414367676
Validation loss: 2.473820132593955

Epoch: 6| Step: 6
Training loss: 2.1782283782958984
Validation loss: 2.494298770863523

Epoch: 6| Step: 7
Training loss: 2.70481538772583
Validation loss: 2.488504445681008

Epoch: 6| Step: 8
Training loss: 1.9908431768417358
Validation loss: 2.4890985386345976

Epoch: 6| Step: 9
Training loss: 2.936068534851074
Validation loss: 2.4891012535300305

Epoch: 6| Step: 10
Training loss: 2.762305498123169
Validation loss: 2.471817829275644

Epoch: 6| Step: 11
Training loss: 3.486954689025879
Validation loss: 2.4625246601720012

Epoch: 6| Step: 12
Training loss: 2.782233238220215
Validation loss: 2.4557315918707077

Epoch: 6| Step: 13
Training loss: 2.1787781715393066
Validation loss: 2.459745309686148

Epoch: 178| Step: 0
Training loss: 2.7144432067871094
Validation loss: 2.4541685504298054

Epoch: 6| Step: 1
Training loss: 3.0402190685272217
Validation loss: 2.455326680214174

Epoch: 6| Step: 2
Training loss: 2.8911027908325195
Validation loss: 2.4513098975663543

Epoch: 6| Step: 3
Training loss: 2.4033944606781006
Validation loss: 2.451497403524255

Epoch: 6| Step: 4
Training loss: 2.103377342224121
Validation loss: 2.451735342702558

Epoch: 6| Step: 5
Training loss: 2.333479404449463
Validation loss: 2.4518503630033104

Epoch: 6| Step: 6
Training loss: 1.9729586839675903
Validation loss: 2.4499785413024244

Epoch: 6| Step: 7
Training loss: 2.3711397647857666
Validation loss: 2.4579041516909035

Epoch: 6| Step: 8
Training loss: 2.784348726272583
Validation loss: 2.469659579697476

Epoch: 6| Step: 9
Training loss: 2.6051688194274902
Validation loss: 2.472588005886283

Epoch: 6| Step: 10
Training loss: 3.4201173782348633
Validation loss: 2.469505274167625

Epoch: 6| Step: 11
Training loss: 3.343518018722534
Validation loss: 2.4669622195664274

Epoch: 6| Step: 12
Training loss: 2.3739097118377686
Validation loss: 2.4637729993430515

Epoch: 6| Step: 13
Training loss: 2.9694015979766846
Validation loss: 2.4574279400610153

Epoch: 179| Step: 0
Training loss: 3.2283496856689453
Validation loss: 2.459576988732943

Epoch: 6| Step: 1
Training loss: 2.1997244358062744
Validation loss: 2.45133468925312

Epoch: 6| Step: 2
Training loss: 2.9022536277770996
Validation loss: 2.4486571640096684

Epoch: 6| Step: 3
Training loss: 2.3471407890319824
Validation loss: 2.4517064299634708

Epoch: 6| Step: 4
Training loss: 2.723094940185547
Validation loss: 2.4510920765579387

Epoch: 6| Step: 5
Training loss: 2.889564275741577
Validation loss: 2.4469763822452997

Epoch: 6| Step: 6
Training loss: 2.1214115619659424
Validation loss: 2.4490757270525862

Epoch: 6| Step: 7
Training loss: 2.325777053833008
Validation loss: 2.445132019699261

Epoch: 6| Step: 8
Training loss: 3.1188995838165283
Validation loss: 2.4492992560068765

Epoch: 6| Step: 9
Training loss: 2.7899789810180664
Validation loss: 2.445673486237885

Epoch: 6| Step: 10
Training loss: 3.061227798461914
Validation loss: 2.448481939172232

Epoch: 6| Step: 11
Training loss: 2.738192558288574
Validation loss: 2.4505189567483883

Epoch: 6| Step: 12
Training loss: 2.497920036315918
Validation loss: 2.454641347290367

Epoch: 6| Step: 13
Training loss: 1.9775052070617676
Validation loss: 2.4492072930899997

Epoch: 180| Step: 0
Training loss: 3.331895589828491
Validation loss: 2.449259691340949

Epoch: 6| Step: 1
Training loss: 2.3922414779663086
Validation loss: 2.4534316216745684

Epoch: 6| Step: 2
Training loss: 2.863133430480957
Validation loss: 2.459968307966827

Epoch: 6| Step: 3
Training loss: 2.495906352996826
Validation loss: 2.4672324939440657

Epoch: 6| Step: 4
Training loss: 2.567420482635498
Validation loss: 2.488357020962623

Epoch: 6| Step: 5
Training loss: 2.375709056854248
Validation loss: 2.516513652698968

Epoch: 6| Step: 6
Training loss: 1.7816388607025146
Validation loss: 2.536493993574573

Epoch: 6| Step: 7
Training loss: 2.516589879989624
Validation loss: 2.5267849686325237

Epoch: 6| Step: 8
Training loss: 2.096831798553467
Validation loss: 2.527835927983766

Epoch: 6| Step: 9
Training loss: 3.952697277069092
Validation loss: 2.5135089505103325

Epoch: 6| Step: 10
Training loss: 2.35943603515625
Validation loss: 2.48751042478828

Epoch: 6| Step: 11
Training loss: 3.38863205909729
Validation loss: 2.4678226440183577

Epoch: 6| Step: 12
Training loss: 2.56797194480896
Validation loss: 2.4617030261665263

Epoch: 6| Step: 13
Training loss: 2.6070199012756348
Validation loss: 2.451500279929048

Epoch: 181| Step: 0
Training loss: 1.8308358192443848
Validation loss: 2.450898483235349

Epoch: 6| Step: 1
Training loss: 2.761691093444824
Validation loss: 2.4493022682846233

Epoch: 6| Step: 2
Training loss: 2.326052188873291
Validation loss: 2.451688115314771

Epoch: 6| Step: 3
Training loss: 2.910257339477539
Validation loss: 2.4586022797451226

Epoch: 6| Step: 4
Training loss: 2.9268717765808105
Validation loss: 2.47203835492493

Epoch: 6| Step: 5
Training loss: 2.508171558380127
Validation loss: 2.5051000528438117

Epoch: 6| Step: 6
Training loss: 2.951491355895996
Validation loss: 2.4985791816506335

Epoch: 6| Step: 7
Training loss: 2.728700637817383
Validation loss: 2.4777286001431045

Epoch: 6| Step: 8
Training loss: 2.646704912185669
Validation loss: 2.4587754421336676

Epoch: 6| Step: 9
Training loss: 2.6634345054626465
Validation loss: 2.455234250714702

Epoch: 6| Step: 10
Training loss: 3.045166254043579
Validation loss: 2.4497560942044823

Epoch: 6| Step: 11
Training loss: 2.359370708465576
Validation loss: 2.4539390738292406

Epoch: 6| Step: 12
Training loss: 2.6522560119628906
Validation loss: 2.4697550291656167

Epoch: 6| Step: 13
Training loss: 3.0408663749694824
Validation loss: 2.465662856255808

Epoch: 182| Step: 0
Training loss: 2.421015977859497
Validation loss: 2.4747295712911956

Epoch: 6| Step: 1
Training loss: 2.711636543273926
Validation loss: 2.4958700236453804

Epoch: 6| Step: 2
Training loss: 2.5687103271484375
Validation loss: 2.5011226618161766

Epoch: 6| Step: 3
Training loss: 2.361872911453247
Validation loss: 2.5138862979027534

Epoch: 6| Step: 4
Training loss: 3.1087827682495117
Validation loss: 2.52223793409204

Epoch: 6| Step: 5
Training loss: 2.5013811588287354
Validation loss: 2.5130824555632887

Epoch: 6| Step: 6
Training loss: 3.3038744926452637
Validation loss: 2.503184544142856

Epoch: 6| Step: 7
Training loss: 1.9831995964050293
Validation loss: 2.4834379047475834

Epoch: 6| Step: 8
Training loss: 2.612428665161133
Validation loss: 2.4757566400753555

Epoch: 6| Step: 9
Training loss: 2.5164103507995605
Validation loss: 2.4631690722639843

Epoch: 6| Step: 10
Training loss: 2.4089314937591553
Validation loss: 2.454467278654857

Epoch: 6| Step: 11
Training loss: 3.184056282043457
Validation loss: 2.4448278437378588

Epoch: 6| Step: 12
Training loss: 2.500382423400879
Validation loss: 2.4461758649477394

Epoch: 6| Step: 13
Training loss: 3.472299337387085
Validation loss: 2.452255195186984

Epoch: 183| Step: 0
Training loss: 2.436995029449463
Validation loss: 2.4475482074163293

Epoch: 6| Step: 1
Training loss: 2.865609645843506
Validation loss: 2.446943049789757

Epoch: 6| Step: 2
Training loss: 3.159177303314209
Validation loss: 2.4483588869853685

Epoch: 6| Step: 3
Training loss: 2.3003087043762207
Validation loss: 2.4414276487083844

Epoch: 6| Step: 4
Training loss: 2.516005039215088
Validation loss: 2.447525553805854

Epoch: 6| Step: 5
Training loss: 3.1625118255615234
Validation loss: 2.454949550731208

Epoch: 6| Step: 6
Training loss: 2.2190356254577637
Validation loss: 2.448181926563222

Epoch: 6| Step: 7
Training loss: 2.684847354888916
Validation loss: 2.4529113718258437

Epoch: 6| Step: 8
Training loss: 3.1886982917785645
Validation loss: 2.4734823716584073

Epoch: 6| Step: 9
Training loss: 2.62664794921875
Validation loss: 2.4758056825207126

Epoch: 6| Step: 10
Training loss: 1.6934645175933838
Validation loss: 2.4757834711382465

Epoch: 6| Step: 11
Training loss: 2.936844825744629
Validation loss: 2.4811626685562955

Epoch: 6| Step: 12
Training loss: 3.275906562805176
Validation loss: 2.476859026057746

Epoch: 6| Step: 13
Training loss: 1.7543599605560303
Validation loss: 2.4674859892937446

Epoch: 184| Step: 0
Training loss: 2.6599981784820557
Validation loss: 2.4615187029684744

Epoch: 6| Step: 1
Training loss: 2.550875186920166
Validation loss: 2.4524458967229372

Epoch: 6| Step: 2
Training loss: 3.150803565979004
Validation loss: 2.4532190240839475

Epoch: 6| Step: 3
Training loss: 2.2995376586914062
Validation loss: 2.4462425080678796

Epoch: 6| Step: 4
Training loss: 2.275134801864624
Validation loss: 2.4478529166149836

Epoch: 6| Step: 5
Training loss: 2.2357568740844727
Validation loss: 2.4462507847816712

Epoch: 6| Step: 6
Training loss: 2.908311128616333
Validation loss: 2.445537910666517

Epoch: 6| Step: 7
Training loss: 2.278252124786377
Validation loss: 2.4515961190705657

Epoch: 6| Step: 8
Training loss: 2.6437511444091797
Validation loss: 2.450590190067086

Epoch: 6| Step: 9
Training loss: 2.7920377254486084
Validation loss: 2.449318334620486

Epoch: 6| Step: 10
Training loss: 2.8576693534851074
Validation loss: 2.4611255661133797

Epoch: 6| Step: 11
Training loss: 2.6097657680511475
Validation loss: 2.461251543414208

Epoch: 6| Step: 12
Training loss: 2.3816328048706055
Validation loss: 2.462358602913477

Epoch: 6| Step: 13
Training loss: 3.8828744888305664
Validation loss: 2.4598630653914584

Epoch: 185| Step: 0
Training loss: 2.921968936920166
Validation loss: 2.458908168218469

Epoch: 6| Step: 1
Training loss: 3.7587523460388184
Validation loss: 2.4493835664564565

Epoch: 6| Step: 2
Training loss: 3.1386756896972656
Validation loss: 2.4389426016038462

Epoch: 6| Step: 3
Training loss: 2.3229422569274902
Validation loss: 2.441787196743873

Epoch: 6| Step: 4
Training loss: 1.5508406162261963
Validation loss: 2.4419117537877892

Epoch: 6| Step: 5
Training loss: 2.161736011505127
Validation loss: 2.4419139328823296

Epoch: 6| Step: 6
Training loss: 2.774797201156616
Validation loss: 2.4421836535135903

Epoch: 6| Step: 7
Training loss: 2.3559513092041016
Validation loss: 2.4469591033074165

Epoch: 6| Step: 8
Training loss: 2.822777271270752
Validation loss: 2.4432598160159205

Epoch: 6| Step: 9
Training loss: 2.9290199279785156
Validation loss: 2.445901496436006

Epoch: 6| Step: 10
Training loss: 2.6224143505096436
Validation loss: 2.446653873689713

Epoch: 6| Step: 11
Training loss: 2.8452675342559814
Validation loss: 2.4456721223810667

Epoch: 6| Step: 12
Training loss: 2.521625518798828
Validation loss: 2.450147077601443

Epoch: 6| Step: 13
Training loss: 2.0028903484344482
Validation loss: 2.446338233127389

Epoch: 186| Step: 0
Training loss: 2.760061740875244
Validation loss: 2.445802903944446

Epoch: 6| Step: 1
Training loss: 2.1559979915618896
Validation loss: 2.4383147480667278

Epoch: 6| Step: 2
Training loss: 2.5712196826934814
Validation loss: 2.437995469698342

Epoch: 6| Step: 3
Training loss: 1.9457601308822632
Validation loss: 2.4395929639057448

Epoch: 6| Step: 4
Training loss: 2.9121592044830322
Validation loss: 2.440575671452348

Epoch: 6| Step: 5
Training loss: 1.8840386867523193
Validation loss: 2.442591254429151

Epoch: 6| Step: 6
Training loss: 2.7970633506774902
Validation loss: 2.4462222617159606

Epoch: 6| Step: 7
Training loss: 2.527892589569092
Validation loss: 2.4396125321747153

Epoch: 6| Step: 8
Training loss: 2.4825143814086914
Validation loss: 2.447177679307999

Epoch: 6| Step: 9
Training loss: 3.111217498779297
Validation loss: 2.4437654915676323

Epoch: 6| Step: 10
Training loss: 3.0496997833251953
Validation loss: 2.446460275239842

Epoch: 6| Step: 11
Training loss: 3.1932849884033203
Validation loss: 2.4434683579270557

Epoch: 6| Step: 12
Training loss: 2.9019570350646973
Validation loss: 2.448271559130761

Epoch: 6| Step: 13
Training loss: 2.5220000743865967
Validation loss: 2.4451594378358577

Epoch: 187| Step: 0
Training loss: 2.578564405441284
Validation loss: 2.4448682159505863

Epoch: 6| Step: 1
Training loss: 2.2275309562683105
Validation loss: 2.435947492558469

Epoch: 6| Step: 2
Training loss: 2.9601340293884277
Validation loss: 2.4395737648010254

Epoch: 6| Step: 3
Training loss: 2.6457223892211914
Validation loss: 2.438338161796652

Epoch: 6| Step: 4
Training loss: 1.9264910221099854
Validation loss: 2.442005334361907

Epoch: 6| Step: 5
Training loss: 2.509643316268921
Validation loss: 2.443394781440817

Epoch: 6| Step: 6
Training loss: 3.80003023147583
Validation loss: 2.444889808213839

Epoch: 6| Step: 7
Training loss: 2.4189000129699707
Validation loss: 2.4414041734510854

Epoch: 6| Step: 8
Training loss: 2.8382339477539062
Validation loss: 2.440288351428124

Epoch: 6| Step: 9
Training loss: 3.109144926071167
Validation loss: 2.442519695528092

Epoch: 6| Step: 10
Training loss: 2.3138322830200195
Validation loss: 2.4378123796114357

Epoch: 6| Step: 11
Training loss: 2.3068461418151855
Validation loss: 2.435526042856196

Epoch: 6| Step: 12
Training loss: 2.4856019020080566
Validation loss: 2.4429168419171403

Epoch: 6| Step: 13
Training loss: 2.8668947219848633
Validation loss: 2.4474460617188485

Epoch: 188| Step: 0
Training loss: 2.5154452323913574
Validation loss: 2.4511427853697088

Epoch: 6| Step: 1
Training loss: 2.688558578491211
Validation loss: 2.455760427700576

Epoch: 6| Step: 2
Training loss: 2.880502700805664
Validation loss: 2.4437161158489924

Epoch: 6| Step: 3
Training loss: 2.7034029960632324
Validation loss: 2.4598608222059024

Epoch: 6| Step: 4
Training loss: 2.5795722007751465
Validation loss: 2.4547530810038247

Epoch: 6| Step: 5
Training loss: 2.225119113922119
Validation loss: 2.452929437801402

Epoch: 6| Step: 6
Training loss: 2.8942298889160156
Validation loss: 2.4415441507934244

Epoch: 6| Step: 7
Training loss: 2.28867769241333
Validation loss: 2.4538497514622186

Epoch: 6| Step: 8
Training loss: 2.7702713012695312
Validation loss: 2.45615206995318

Epoch: 6| Step: 9
Training loss: 2.1547765731811523
Validation loss: 2.460916596074258

Epoch: 6| Step: 10
Training loss: 2.7966766357421875
Validation loss: 2.4538041468589538

Epoch: 6| Step: 11
Training loss: 3.483818292617798
Validation loss: 2.4705015279913463

Epoch: 6| Step: 12
Training loss: 2.3041911125183105
Validation loss: 2.4652842911340858

Epoch: 6| Step: 13
Training loss: 2.6074113845825195
Validation loss: 2.4566288071293987

Epoch: 189| Step: 0
Training loss: 2.319706916809082
Validation loss: 2.454592743227559

Epoch: 6| Step: 1
Training loss: 2.727301836013794
Validation loss: 2.461603272345758

Epoch: 6| Step: 2
Training loss: 2.9753215312957764
Validation loss: 2.4514861209418184

Epoch: 6| Step: 3
Training loss: 3.250419855117798
Validation loss: 2.4488962850263043

Epoch: 6| Step: 4
Training loss: 1.6445822715759277
Validation loss: 2.4463649911265217

Epoch: 6| Step: 5
Training loss: 3.2137274742126465
Validation loss: 2.447685815954721

Epoch: 6| Step: 6
Training loss: 2.6375932693481445
Validation loss: 2.4518458740685576

Epoch: 6| Step: 7
Training loss: 3.287567615509033
Validation loss: 2.4495357364736576

Epoch: 6| Step: 8
Training loss: 2.4415457248687744
Validation loss: 2.4471423715673466

Epoch: 6| Step: 9
Training loss: 2.1726315021514893
Validation loss: 2.438590213816653

Epoch: 6| Step: 10
Training loss: 2.695183277130127
Validation loss: 2.452797633345409

Epoch: 6| Step: 11
Training loss: 2.5099539756774902
Validation loss: 2.4392694247666227

Epoch: 6| Step: 12
Training loss: 2.3584160804748535
Validation loss: 2.4427456804501113

Epoch: 6| Step: 13
Training loss: 2.7565295696258545
Validation loss: 2.4353477288317937

Epoch: 190| Step: 0
Training loss: 2.4969305992126465
Validation loss: 2.439745622296487

Epoch: 6| Step: 1
Training loss: 2.9526915550231934
Validation loss: 2.434375460429858

Epoch: 6| Step: 2
Training loss: 2.355921983718872
Validation loss: 2.4424506720676216

Epoch: 6| Step: 3
Training loss: 2.2816200256347656
Validation loss: 2.437649724304035

Epoch: 6| Step: 4
Training loss: 2.6211540699005127
Validation loss: 2.451004643594065

Epoch: 6| Step: 5
Training loss: 3.070793628692627
Validation loss: 2.454758113430392

Epoch: 6| Step: 6
Training loss: 2.8695197105407715
Validation loss: 2.4585771791396605

Epoch: 6| Step: 7
Training loss: 2.6656453609466553
Validation loss: 2.4431654432768464

Epoch: 6| Step: 8
Training loss: 2.683516025543213
Validation loss: 2.4465347054184123

Epoch: 6| Step: 9
Training loss: 2.454893112182617
Validation loss: 2.4480161820688555

Epoch: 6| Step: 10
Training loss: 3.0045523643493652
Validation loss: 2.4500751213360856

Epoch: 6| Step: 11
Training loss: 2.168691635131836
Validation loss: 2.4388568068063385

Epoch: 6| Step: 12
Training loss: 2.6561574935913086
Validation loss: 2.438103522023847

Epoch: 6| Step: 13
Training loss: 2.485185384750366
Validation loss: 2.4386550072700746

Epoch: 191| Step: 0
Training loss: 1.6948579549789429
Validation loss: 2.4302140153864378

Epoch: 6| Step: 1
Training loss: 2.9525094032287598
Validation loss: 2.42859588643556

Epoch: 6| Step: 2
Training loss: 2.542656898498535
Validation loss: 2.4328497276511243

Epoch: 6| Step: 3
Training loss: 2.585623264312744
Validation loss: 2.4323103248432116

Epoch: 6| Step: 4
Training loss: 2.166668176651001
Validation loss: 2.436552001583961

Epoch: 6| Step: 5
Training loss: 3.1214635372161865
Validation loss: 2.439271201369583

Epoch: 6| Step: 6
Training loss: 2.8637630939483643
Validation loss: 2.4370314921102216

Epoch: 6| Step: 7
Training loss: 2.7238705158233643
Validation loss: 2.439099334901379

Epoch: 6| Step: 8
Training loss: 2.987518310546875
Validation loss: 2.441115827970607

Epoch: 6| Step: 9
Training loss: 2.300563335418701
Validation loss: 2.4417607925271474

Epoch: 6| Step: 10
Training loss: 2.5476958751678467
Validation loss: 2.4424503618671047

Epoch: 6| Step: 11
Training loss: 2.7461071014404297
Validation loss: 2.4390077872942855

Epoch: 6| Step: 12
Training loss: 3.0350332260131836
Validation loss: 2.4352675842982467

Epoch: 6| Step: 13
Training loss: 2.9500572681427
Validation loss: 2.4408211246613534

Epoch: 192| Step: 0
Training loss: 2.4181370735168457
Validation loss: 2.4366882667746594

Epoch: 6| Step: 1
Training loss: 2.770768642425537
Validation loss: 2.430437159794633

Epoch: 6| Step: 2
Training loss: 2.689692735671997
Validation loss: 2.4334400341074955

Epoch: 6| Step: 3
Training loss: 2.1683542728424072
Validation loss: 2.4326667247279996

Epoch: 6| Step: 4
Training loss: 1.9885345697402954
Validation loss: 2.4299955906406527

Epoch: 6| Step: 5
Training loss: 2.578975200653076
Validation loss: 2.4393403709575696

Epoch: 6| Step: 6
Training loss: 2.9639780521392822
Validation loss: 2.4449322941482707

Epoch: 6| Step: 7
Training loss: 2.807931900024414
Validation loss: 2.4499032856315694

Epoch: 6| Step: 8
Training loss: 3.209127902984619
Validation loss: 2.447852114195465

Epoch: 6| Step: 9
Training loss: 2.9723849296569824
Validation loss: 2.458012657780801

Epoch: 6| Step: 10
Training loss: 2.8219265937805176
Validation loss: 2.461670098766204

Epoch: 6| Step: 11
Training loss: 2.5023245811462402
Validation loss: 2.4609216490099506

Epoch: 6| Step: 12
Training loss: 2.1385345458984375
Validation loss: 2.4560188221675094

Epoch: 6| Step: 13
Training loss: 2.9035840034484863
Validation loss: 2.4565017428449405

Epoch: 193| Step: 0
Training loss: 2.6477394104003906
Validation loss: 2.446739714632752

Epoch: 6| Step: 1
Training loss: 1.7884103059768677
Validation loss: 2.445380990223218

Epoch: 6| Step: 2
Training loss: 2.765371799468994
Validation loss: 2.4471474668031097

Epoch: 6| Step: 3
Training loss: 2.5785083770751953
Validation loss: 2.4377036479211625

Epoch: 6| Step: 4
Training loss: 2.6447348594665527
Validation loss: 2.4359424934592298

Epoch: 6| Step: 5
Training loss: 2.8264150619506836
Validation loss: 2.437949816385905

Epoch: 6| Step: 6
Training loss: 2.926129102706909
Validation loss: 2.426816245561005

Epoch: 6| Step: 7
Training loss: 2.591935873031616
Validation loss: 2.430616209583898

Epoch: 6| Step: 8
Training loss: 2.623673915863037
Validation loss: 2.4301102622862785

Epoch: 6| Step: 9
Training loss: 2.981213092803955
Validation loss: 2.430068083988723

Epoch: 6| Step: 10
Training loss: 2.672663688659668
Validation loss: 2.4329527347318587

Epoch: 6| Step: 11
Training loss: 3.2242724895477295
Validation loss: 2.4309852661625033

Epoch: 6| Step: 12
Training loss: 2.414259672164917
Validation loss: 2.4313307064835743

Epoch: 6| Step: 13
Training loss: 1.7757116556167603
Validation loss: 2.432749563647855

Epoch: 194| Step: 0
Training loss: 2.281750202178955
Validation loss: 2.427575724099272

Epoch: 6| Step: 1
Training loss: 3.401681423187256
Validation loss: 2.4268100159142607

Epoch: 6| Step: 2
Training loss: 3.4976401329040527
Validation loss: 2.4299801113784953

Epoch: 6| Step: 3
Training loss: 2.3355369567871094
Validation loss: 2.4316309728930072

Epoch: 6| Step: 4
Training loss: 2.0616650581359863
Validation loss: 2.426701145787393

Epoch: 6| Step: 5
Training loss: 2.1152682304382324
Validation loss: 2.4251850164064797

Epoch: 6| Step: 6
Training loss: 2.6122939586639404
Validation loss: 2.4240920466761433

Epoch: 6| Step: 7
Training loss: 2.732809543609619
Validation loss: 2.432562953682356

Epoch: 6| Step: 8
Training loss: 2.914419651031494
Validation loss: 2.4341201064407185

Epoch: 6| Step: 9
Training loss: 2.8233470916748047
Validation loss: 2.4313114843060895

Epoch: 6| Step: 10
Training loss: 2.7722978591918945
Validation loss: 2.4410455713989916

Epoch: 6| Step: 11
Training loss: 2.0125107765197754
Validation loss: 2.4421167527475665

Epoch: 6| Step: 12
Training loss: 2.541386604309082
Validation loss: 2.4450335579533733

Epoch: 6| Step: 13
Training loss: 2.6589536666870117
Validation loss: 2.452111372383692

Epoch: 195| Step: 0
Training loss: 2.7999181747436523
Validation loss: 2.447495178509784

Epoch: 6| Step: 1
Training loss: 2.8390045166015625
Validation loss: 2.459045938266221

Epoch: 6| Step: 2
Training loss: 1.8870809078216553
Validation loss: 2.4551599974273355

Epoch: 6| Step: 3
Training loss: 2.7297816276550293
Validation loss: 2.4560128642666723

Epoch: 6| Step: 4
Training loss: 3.038071632385254
Validation loss: 2.4480757405680995

Epoch: 6| Step: 5
Training loss: 1.9948076009750366
Validation loss: 2.454503213205645

Epoch: 6| Step: 6
Training loss: 2.801509380340576
Validation loss: 2.4465199132119455

Epoch: 6| Step: 7
Training loss: 2.7618207931518555
Validation loss: 2.440481005176421

Epoch: 6| Step: 8
Training loss: 2.2925868034362793
Validation loss: 2.4395080176732873

Epoch: 6| Step: 9
Training loss: 3.3022608757019043
Validation loss: 2.439054589117727

Epoch: 6| Step: 10
Training loss: 2.8609633445739746
Validation loss: 2.4360842089499197

Epoch: 6| Step: 11
Training loss: 2.3576180934906006
Validation loss: 2.435116357700799

Epoch: 6| Step: 12
Training loss: 2.372335433959961
Validation loss: 2.4330681882878786

Epoch: 6| Step: 13
Training loss: 2.8056890964508057
Validation loss: 2.4359538990964174

Epoch: 196| Step: 0
Training loss: 2.784029006958008
Validation loss: 2.4365293056734147

Epoch: 6| Step: 1
Training loss: 3.005490303039551
Validation loss: 2.429560976643716

Epoch: 6| Step: 2
Training loss: 2.703463077545166
Validation loss: 2.4314479571516796

Epoch: 6| Step: 3
Training loss: 2.777223587036133
Validation loss: 2.429002474713069

Epoch: 6| Step: 4
Training loss: 2.4798572063446045
Validation loss: 2.4357789357503257

Epoch: 6| Step: 5
Training loss: 1.7097926139831543
Validation loss: 2.427203809061358

Epoch: 6| Step: 6
Training loss: 1.9640308618545532
Validation loss: 2.4231197475105204

Epoch: 6| Step: 7
Training loss: 2.7404069900512695
Validation loss: 2.428487503400413

Epoch: 6| Step: 8
Training loss: 2.3285608291625977
Validation loss: 2.431316637223767

Epoch: 6| Step: 9
Training loss: 3.249831199645996
Validation loss: 2.4384926929268786

Epoch: 6| Step: 10
Training loss: 3.373171091079712
Validation loss: 2.434969235492009

Epoch: 6| Step: 11
Training loss: 2.589376449584961
Validation loss: 2.436784403298491

Epoch: 6| Step: 12
Training loss: 2.3715689182281494
Validation loss: 2.4401074378721175

Epoch: 6| Step: 13
Training loss: 2.6413259506225586
Validation loss: 2.4408435231895855

Epoch: 197| Step: 0
Training loss: 2.518387794494629
Validation loss: 2.4342017558313187

Epoch: 6| Step: 1
Training loss: 2.628053665161133
Validation loss: 2.448703770996422

Epoch: 6| Step: 2
Training loss: 2.4708988666534424
Validation loss: 2.444647217309603

Epoch: 6| Step: 3
Training loss: 2.649409294128418
Validation loss: 2.4376807392284436

Epoch: 6| Step: 4
Training loss: 3.7863192558288574
Validation loss: 2.4295688034385763

Epoch: 6| Step: 5
Training loss: 2.1891486644744873
Validation loss: 2.424267602223222

Epoch: 6| Step: 6
Training loss: 2.3155949115753174
Validation loss: 2.424946984937114

Epoch: 6| Step: 7
Training loss: 3.52518892288208
Validation loss: 2.4250822759443715

Epoch: 6| Step: 8
Training loss: 2.2022814750671387
Validation loss: 2.42449850420798

Epoch: 6| Step: 9
Training loss: 2.2646589279174805
Validation loss: 2.440346356361143

Epoch: 6| Step: 10
Training loss: 2.8890461921691895
Validation loss: 2.4237624035086682

Epoch: 6| Step: 11
Training loss: 2.6894516944885254
Validation loss: 2.432068763240691

Epoch: 6| Step: 12
Training loss: 2.351496696472168
Validation loss: 2.4448960545242473

Epoch: 6| Step: 13
Training loss: 1.8355026245117188
Validation loss: 2.4433488204915035

Epoch: 198| Step: 0
Training loss: 3.2605652809143066
Validation loss: 2.4378370751616774

Epoch: 6| Step: 1
Training loss: 2.368516445159912
Validation loss: 2.44778976901885

Epoch: 6| Step: 2
Training loss: 3.277195930480957
Validation loss: 2.4328503749703847

Epoch: 6| Step: 3
Training loss: 2.6326918601989746
Validation loss: 2.427001517306092

Epoch: 6| Step: 4
Training loss: 2.589381217956543
Validation loss: 2.425547215246385

Epoch: 6| Step: 5
Training loss: 2.802847146987915
Validation loss: 2.4281226588833715

Epoch: 6| Step: 6
Training loss: 2.2758631706237793
Validation loss: 2.4212782280419463

Epoch: 6| Step: 7
Training loss: 2.2996459007263184
Validation loss: 2.4299498604189966

Epoch: 6| Step: 8
Training loss: 2.6940317153930664
Validation loss: 2.430617122239964

Epoch: 6| Step: 9
Training loss: 2.8549115657806396
Validation loss: 2.4250606054900796

Epoch: 6| Step: 10
Training loss: 2.245694398880005
Validation loss: 2.434811299847018

Epoch: 6| Step: 11
Training loss: 2.57741117477417
Validation loss: 2.4269392926205873

Epoch: 6| Step: 12
Training loss: 2.4129297733306885
Validation loss: 2.428719638496317

Epoch: 6| Step: 13
Training loss: 2.0810892581939697
Validation loss: 2.423895069347915

Epoch: 199| Step: 0
Training loss: 2.448525905609131
Validation loss: 2.4197447581957747

Epoch: 6| Step: 1
Training loss: 2.2827320098876953
Validation loss: 2.427824116522266

Epoch: 6| Step: 2
Training loss: 3.052604913711548
Validation loss: 2.4259973802874164

Epoch: 6| Step: 3
Training loss: 2.17668080329895
Validation loss: 2.426087179491597

Epoch: 6| Step: 4
Training loss: 2.9400553703308105
Validation loss: 2.4233285457857194

Epoch: 6| Step: 5
Training loss: 2.6568796634674072
Validation loss: 2.418509647410403

Epoch: 6| Step: 6
Training loss: 2.327816963195801
Validation loss: 2.428372747154646

Epoch: 6| Step: 7
Training loss: 3.0757880210876465
Validation loss: 2.4284774564927623

Epoch: 6| Step: 8
Training loss: 2.8786206245422363
Validation loss: 2.4256767867713847

Epoch: 6| Step: 9
Training loss: 2.3287415504455566
Validation loss: 2.428666881335679

Epoch: 6| Step: 10
Training loss: 2.022719621658325
Validation loss: 2.4235588735149753

Epoch: 6| Step: 11
Training loss: 3.0191431045532227
Validation loss: 2.421783188337921

Epoch: 6| Step: 12
Training loss: 2.4145312309265137
Validation loss: 2.4279679072800504

Epoch: 6| Step: 13
Training loss: 3.2644870281219482
Validation loss: 2.42919748316529

Epoch: 200| Step: 0
Training loss: 2.7691526412963867
Validation loss: 2.423491854821482

Epoch: 6| Step: 1
Training loss: 2.559833526611328
Validation loss: 2.4186397137180453

Epoch: 6| Step: 2
Training loss: 2.3927507400512695
Validation loss: 2.4191294844432543

Epoch: 6| Step: 3
Training loss: 2.50698184967041
Validation loss: 2.4218148018724177

Epoch: 6| Step: 4
Training loss: 2.4352688789367676
Validation loss: 2.428459328989829

Epoch: 6| Step: 5
Training loss: 2.1901493072509766
Validation loss: 2.4354221077375513

Epoch: 6| Step: 6
Training loss: 3.096541404724121
Validation loss: 2.4301627887192594

Epoch: 6| Step: 7
Training loss: 2.844404458999634
Validation loss: 2.428077863108727

Epoch: 6| Step: 8
Training loss: 3.2100019454956055
Validation loss: 2.4309793979890886

Epoch: 6| Step: 9
Training loss: 2.7239010334014893
Validation loss: 2.442210833231608

Epoch: 6| Step: 10
Training loss: 2.5299181938171387
Validation loss: 2.444992575594174

Epoch: 6| Step: 11
Training loss: 1.7842214107513428
Validation loss: 2.4477877104154198

Epoch: 6| Step: 12
Training loss: 2.668187141418457
Validation loss: 2.4451796444513465

Epoch: 6| Step: 13
Training loss: 3.2507412433624268
Validation loss: 2.4303277948851227

Epoch: 201| Step: 0
Training loss: 2.1425156593322754
Validation loss: 2.4318851476074546

Epoch: 6| Step: 1
Training loss: 2.7343738079071045
Validation loss: 2.4286685938476236

Epoch: 6| Step: 2
Training loss: 1.9361510276794434
Validation loss: 2.436932312544956

Epoch: 6| Step: 3
Training loss: 3.352323532104492
Validation loss: 2.435220797856649

Epoch: 6| Step: 4
Training loss: 2.7662370204925537
Validation loss: 2.4320271784259426

Epoch: 6| Step: 5
Training loss: 2.764882802963257
Validation loss: 2.4376143422178043

Epoch: 6| Step: 6
Training loss: 2.454120635986328
Validation loss: 2.4291786019520094

Epoch: 6| Step: 7
Training loss: 2.7477829456329346
Validation loss: 2.4320098225788405

Epoch: 6| Step: 8
Training loss: 3.112081289291382
Validation loss: 2.4192998588726087

Epoch: 6| Step: 9
Training loss: 2.7495203018188477
Validation loss: 2.427437531050815

Epoch: 6| Step: 10
Training loss: 3.263899087905884
Validation loss: 2.4183883026082027

Epoch: 6| Step: 11
Training loss: 1.9336930513381958
Validation loss: 2.4232782343382477

Epoch: 6| Step: 12
Training loss: 1.9897489547729492
Validation loss: 2.419063150241811

Epoch: 6| Step: 13
Training loss: 2.7020578384399414
Validation loss: 2.4160525798797607

Epoch: 202| Step: 0
Training loss: 2.963876724243164
Validation loss: 2.417106948873048

Epoch: 6| Step: 1
Training loss: 2.4479727745056152
Validation loss: 2.4186653962699314

Epoch: 6| Step: 2
Training loss: 3.5291736125946045
Validation loss: 2.420020282909434

Epoch: 6| Step: 3
Training loss: 2.273578643798828
Validation loss: 2.4120041196064284

Epoch: 6| Step: 4
Training loss: 2.3437039852142334
Validation loss: 2.405739397130987

Epoch: 6| Step: 5
Training loss: 3.150104284286499
Validation loss: 2.4040974827222925

Epoch: 6| Step: 6
Training loss: 2.2751033306121826
Validation loss: 2.4098111890977427

Epoch: 6| Step: 7
Training loss: 2.0043976306915283
Validation loss: 2.405666405154813

Epoch: 6| Step: 8
Training loss: 1.9914979934692383
Validation loss: 2.403567188529558

Epoch: 6| Step: 9
Training loss: 2.3069915771484375
Validation loss: 2.4105733286949897

Epoch: 6| Step: 10
Training loss: 2.6366190910339355
Validation loss: 2.401980415467293

Epoch: 6| Step: 11
Training loss: 3.0956366062164307
Validation loss: 2.41211853232435

Epoch: 6| Step: 12
Training loss: 2.9040980339050293
Validation loss: 2.4130835584414903

Epoch: 6| Step: 13
Training loss: 2.646639108657837
Validation loss: 2.419920849543746

Epoch: 203| Step: 0
Training loss: 1.8734347820281982
Validation loss: 2.420649218302901

Epoch: 6| Step: 1
Training loss: 2.904557943344116
Validation loss: 2.417232605718797

Epoch: 6| Step: 2
Training loss: 2.4711222648620605
Validation loss: 2.414556139258928

Epoch: 6| Step: 3
Training loss: 1.92622971534729
Validation loss: 2.418773087122107

Epoch: 6| Step: 4
Training loss: 2.3544445037841797
Validation loss: 2.4293442028825

Epoch: 6| Step: 5
Training loss: 3.3943920135498047
Validation loss: 2.427325535846013

Epoch: 6| Step: 6
Training loss: 2.6115736961364746
Validation loss: 2.422988981328985

Epoch: 6| Step: 7
Training loss: 2.1498591899871826
Validation loss: 2.4326065971005346

Epoch: 6| Step: 8
Training loss: 3.0336203575134277
Validation loss: 2.4259533651413454

Epoch: 6| Step: 9
Training loss: 3.470047950744629
Validation loss: 2.4245628285151657

Epoch: 6| Step: 10
Training loss: 2.435544967651367
Validation loss: 2.422217253715761

Epoch: 6| Step: 11
Training loss: 2.4803571701049805
Validation loss: 2.413609012480705

Epoch: 6| Step: 12
Training loss: 3.208625555038452
Validation loss: 2.4225759429316365

Epoch: 6| Step: 13
Training loss: 2.2234134674072266
Validation loss: 2.4171001539435437

Epoch: 204| Step: 0
Training loss: 2.39267635345459
Validation loss: 2.419300692055815

Epoch: 6| Step: 1
Training loss: 3.913431406021118
Validation loss: 2.420477480016729

Epoch: 6| Step: 2
Training loss: 2.6962890625
Validation loss: 2.4286117092255624

Epoch: 6| Step: 3
Training loss: 2.915626049041748
Validation loss: 2.4435132959837556

Epoch: 6| Step: 4
Training loss: 3.6367506980895996
Validation loss: 2.444425223976053

Epoch: 6| Step: 5
Training loss: 2.095348596572876
Validation loss: 2.4544479231680594

Epoch: 6| Step: 6
Training loss: 2.331855297088623
Validation loss: 2.4604139584366993

Epoch: 6| Step: 7
Training loss: 2.4599647521972656
Validation loss: 2.4748732325851277

Epoch: 6| Step: 8
Training loss: 2.365548610687256
Validation loss: 2.4588071607774302

Epoch: 6| Step: 9
Training loss: 2.183297872543335
Validation loss: 2.4368206454861547

Epoch: 6| Step: 10
Training loss: 2.327387809753418
Validation loss: 2.439261349298621

Epoch: 6| Step: 11
Training loss: 2.794198513031006
Validation loss: 2.4268501291992846

Epoch: 6| Step: 12
Training loss: 1.9450695514678955
Validation loss: 2.413533651700584

Epoch: 6| Step: 13
Training loss: 2.863018751144409
Validation loss: 2.401622377416139

Epoch: 205| Step: 0
Training loss: 2.992609977722168
Validation loss: 2.402622566428236

Epoch: 6| Step: 1
Training loss: 3.0831809043884277
Validation loss: 2.404864409918426

Epoch: 6| Step: 2
Training loss: 1.8995238542556763
Validation loss: 2.4159582455952964

Epoch: 6| Step: 3
Training loss: 3.2345967292785645
Validation loss: 2.4182237399521695

Epoch: 6| Step: 4
Training loss: 2.8670101165771484
Validation loss: 2.417678225424982

Epoch: 6| Step: 5
Training loss: 2.365241050720215
Validation loss: 2.414524988461566

Epoch: 6| Step: 6
Training loss: 1.8088933229446411
Validation loss: 2.414161600092406

Epoch: 6| Step: 7
Training loss: 2.2538301944732666
Validation loss: 2.4075515065141904

Epoch: 6| Step: 8
Training loss: 3.257113456726074
Validation loss: 2.4074181613101753

Epoch: 6| Step: 9
Training loss: 2.4739859104156494
Validation loss: 2.4075669973127303

Epoch: 6| Step: 10
Training loss: 2.5522422790527344
Validation loss: 2.3968410427852342

Epoch: 6| Step: 11
Training loss: 1.9531590938568115
Validation loss: 2.4055703814311693

Epoch: 6| Step: 12
Training loss: 3.3619883060455322
Validation loss: 2.4083327324159685

Epoch: 6| Step: 13
Training loss: 2.8741655349731445
Validation loss: 2.4212874930392028

Epoch: 206| Step: 0
Training loss: 2.791594982147217
Validation loss: 2.4187639195431947

Epoch: 6| Step: 1
Training loss: 2.634345054626465
Validation loss: 2.4337106750857447

Epoch: 6| Step: 2
Training loss: 2.3563365936279297
Validation loss: 2.429618227866388

Epoch: 6| Step: 3
Training loss: 3.621870517730713
Validation loss: 2.431572344995314

Epoch: 6| Step: 4
Training loss: 2.605490207672119
Validation loss: 2.432743926202097

Epoch: 6| Step: 5
Training loss: 2.3103156089782715
Validation loss: 2.42578262154774

Epoch: 6| Step: 6
Training loss: 2.931765079498291
Validation loss: 2.432475728373374

Epoch: 6| Step: 7
Training loss: 2.0176303386688232
Validation loss: 2.432877379079019

Epoch: 6| Step: 8
Training loss: 1.4195499420166016
Validation loss: 2.4396575035587436

Epoch: 6| Step: 9
Training loss: 2.6338257789611816
Validation loss: 2.442144904085385

Epoch: 6| Step: 10
Training loss: 2.2468814849853516
Validation loss: 2.42252435991841

Epoch: 6| Step: 11
Training loss: 3.7917144298553467
Validation loss: 2.4300536442828435

Epoch: 6| Step: 12
Training loss: 2.8467612266540527
Validation loss: 2.422418794324321

Epoch: 6| Step: 13
Training loss: 2.336071014404297
Validation loss: 2.419455046294838

Epoch: 207| Step: 0
Training loss: 2.5346250534057617
Validation loss: 2.4163878733111965

Epoch: 6| Step: 1
Training loss: 3.18255877494812
Validation loss: 2.413152189664943

Epoch: 6| Step: 2
Training loss: 2.3205268383026123
Validation loss: 2.417790723103349

Epoch: 6| Step: 3
Training loss: 3.1091697216033936
Validation loss: 2.4184355069232244

Epoch: 6| Step: 4
Training loss: 2.5314645767211914
Validation loss: 2.4221132237424134

Epoch: 6| Step: 5
Training loss: 2.7296128273010254
Validation loss: 2.4264227779962684

Epoch: 6| Step: 6
Training loss: 2.783165693283081
Validation loss: 2.4285804046097623

Epoch: 6| Step: 7
Training loss: 2.0576212406158447
Validation loss: 2.4142789815061834

Epoch: 6| Step: 8
Training loss: 2.629911184310913
Validation loss: 2.4089827870809906

Epoch: 6| Step: 9
Training loss: 2.6927847862243652
Validation loss: 2.4061583934291715

Epoch: 6| Step: 10
Training loss: 2.976238250732422
Validation loss: 2.4053659413450506

Epoch: 6| Step: 11
Training loss: 2.6767995357513428
Validation loss: 2.4023689736602125

Epoch: 6| Step: 12
Training loss: 2.1383719444274902
Validation loss: 2.4090521540693057

Epoch: 6| Step: 13
Training loss: 2.0796585083007812
Validation loss: 2.4147689880863314

Epoch: 208| Step: 0
Training loss: 2.8000712394714355
Validation loss: 2.416568991958454

Epoch: 6| Step: 1
Training loss: 3.0680482387542725
Validation loss: 2.432522940379317

Epoch: 6| Step: 2
Training loss: 3.14056396484375
Validation loss: 2.4279175548143286

Epoch: 6| Step: 3
Training loss: 2.929612636566162
Validation loss: 2.4255759357124247

Epoch: 6| Step: 4
Training loss: 2.1295835971832275
Validation loss: 2.4281542224268757

Epoch: 6| Step: 5
Training loss: 3.33551025390625
Validation loss: 2.4237533666754283

Epoch: 6| Step: 6
Training loss: 2.7402725219726562
Validation loss: 2.430133642688874

Epoch: 6| Step: 7
Training loss: 2.453369140625
Validation loss: 2.4131243869822514

Epoch: 6| Step: 8
Training loss: 1.8341858386993408
Validation loss: 2.414449337990053

Epoch: 6| Step: 9
Training loss: 2.5159125328063965
Validation loss: 2.411873527752456

Epoch: 6| Step: 10
Training loss: 1.8774676322937012
Validation loss: 2.408375181177611

Epoch: 6| Step: 11
Training loss: 3.163832664489746
Validation loss: 2.4113519448106007

Epoch: 6| Step: 12
Training loss: 2.5710926055908203
Validation loss: 2.413155937707552

Epoch: 6| Step: 13
Training loss: 1.6072218418121338
Validation loss: 2.412962846858527

Epoch: 209| Step: 0
Training loss: 2.5207035541534424
Validation loss: 2.4213220662968133

Epoch: 6| Step: 1
Training loss: 2.712282180786133
Validation loss: 2.4290389963375625

Epoch: 6| Step: 2
Training loss: 3.147366523742676
Validation loss: 2.4329548625535864

Epoch: 6| Step: 3
Training loss: 2.2783658504486084
Validation loss: 2.438381330941313

Epoch: 6| Step: 4
Training loss: 2.5152673721313477
Validation loss: 2.4664359067075994

Epoch: 6| Step: 5
Training loss: 2.839665651321411
Validation loss: 2.4486356832647838

Epoch: 6| Step: 6
Training loss: 3.2949724197387695
Validation loss: 2.4699178382914555

Epoch: 6| Step: 7
Training loss: 1.8678860664367676
Validation loss: 2.457669706754787

Epoch: 6| Step: 8
Training loss: 2.891791820526123
Validation loss: 2.4333739844701623

Epoch: 6| Step: 9
Training loss: 2.476931095123291
Validation loss: 2.4180373273869997

Epoch: 6| Step: 10
Training loss: 3.366931438446045
Validation loss: 2.423017019866615

Epoch: 6| Step: 11
Training loss: 1.5870022773742676
Validation loss: 2.4176240582619943

Epoch: 6| Step: 12
Training loss: 2.7996931076049805
Validation loss: 2.421170298771192

Epoch: 6| Step: 13
Training loss: 2.2318849563598633
Validation loss: 2.413102544764037

Epoch: 210| Step: 0
Training loss: 2.7001357078552246
Validation loss: 2.417111140425487

Epoch: 6| Step: 1
Training loss: 2.130056381225586
Validation loss: 2.411358807676582

Epoch: 6| Step: 2
Training loss: 3.022029161453247
Validation loss: 2.4128002889694704

Epoch: 6| Step: 3
Training loss: 1.9699946641921997
Validation loss: 2.410713570092314

Epoch: 6| Step: 4
Training loss: 3.0953855514526367
Validation loss: 2.4080205784049085

Epoch: 6| Step: 5
Training loss: 2.635244369506836
Validation loss: 2.41121385687141

Epoch: 6| Step: 6
Training loss: 2.580780267715454
Validation loss: 2.409317680584487

Epoch: 6| Step: 7
Training loss: 1.9547266960144043
Validation loss: 2.404158692206106

Epoch: 6| Step: 8
Training loss: 2.9219436645507812
Validation loss: 2.411032893324411

Epoch: 6| Step: 9
Training loss: 3.0235400199890137
Validation loss: 2.4127635186718357

Epoch: 6| Step: 10
Training loss: 2.2638020515441895
Validation loss: 2.398567743198846

Epoch: 6| Step: 11
Training loss: 2.7869069576263428
Validation loss: 2.3977001431167766

Epoch: 6| Step: 12
Training loss: 2.786771059036255
Validation loss: 2.4026910592150945

Epoch: 6| Step: 13
Training loss: 2.8574986457824707
Validation loss: 2.4054629751431045

Epoch: 211| Step: 0
Training loss: 1.1228232383728027
Validation loss: 2.39730941864752

Epoch: 6| Step: 1
Training loss: 3.3790621757507324
Validation loss: 2.398376682753204

Epoch: 6| Step: 2
Training loss: 2.402219295501709
Validation loss: 2.3954180056048977

Epoch: 6| Step: 3
Training loss: 2.8393168449401855
Validation loss: 2.4005509832853913

Epoch: 6| Step: 4
Training loss: 2.623525619506836
Validation loss: 2.4095272223154702

Epoch: 6| Step: 5
Training loss: 2.778090476989746
Validation loss: 2.4058578937284407

Epoch: 6| Step: 6
Training loss: 3.1235733032226562
Validation loss: 2.4083645702690206

Epoch: 6| Step: 7
Training loss: 2.541990280151367
Validation loss: 2.399521320096908

Epoch: 6| Step: 8
Training loss: 2.99326229095459
Validation loss: 2.396511165044641

Epoch: 6| Step: 9
Training loss: 2.6818299293518066
Validation loss: 2.4071056894076768

Epoch: 6| Step: 10
Training loss: 2.4639291763305664
Validation loss: 2.4001522987119612

Epoch: 6| Step: 11
Training loss: 3.3663573265075684
Validation loss: 2.3911396713667017

Epoch: 6| Step: 12
Training loss: 1.7549965381622314
Validation loss: 2.404064098993937

Epoch: 6| Step: 13
Training loss: 2.393022060394287
Validation loss: 2.410381906776018

Epoch: 212| Step: 0
Training loss: 2.7443008422851562
Validation loss: 2.407383288106611

Epoch: 6| Step: 1
Training loss: 2.9255013465881348
Validation loss: 2.4212537991103305

Epoch: 6| Step: 2
Training loss: 2.2244715690612793
Validation loss: 2.4191263234743507

Epoch: 6| Step: 3
Training loss: 2.816418409347534
Validation loss: 2.4281392328200804

Epoch: 6| Step: 4
Training loss: 2.452672004699707
Validation loss: 2.426834075681625

Epoch: 6| Step: 5
Training loss: 2.4076037406921387
Validation loss: 2.42785293568847

Epoch: 6| Step: 6
Training loss: 2.69466233253479
Validation loss: 2.4228320749857093

Epoch: 6| Step: 7
Training loss: 2.462770462036133
Validation loss: 2.420660852104105

Epoch: 6| Step: 8
Training loss: 3.0616354942321777
Validation loss: 2.432687895272368

Epoch: 6| Step: 9
Training loss: 2.690075159072876
Validation loss: 2.4375879354374383

Epoch: 6| Step: 10
Training loss: 2.7185561656951904
Validation loss: 2.435194492340088

Epoch: 6| Step: 11
Training loss: 2.1212713718414307
Validation loss: 2.439487862330611

Epoch: 6| Step: 12
Training loss: 2.643592357635498
Validation loss: 2.4475458693760697

Epoch: 6| Step: 13
Training loss: 2.396590232849121
Validation loss: 2.457236518142044

Epoch: 213| Step: 0
Training loss: 2.536276340484619
Validation loss: 2.460878356810539

Epoch: 6| Step: 1
Training loss: 2.9759793281555176
Validation loss: 2.4433977014275006

Epoch: 6| Step: 2
Training loss: 2.35330867767334
Validation loss: 2.4415042554178545

Epoch: 6| Step: 3
Training loss: 2.5392494201660156
Validation loss: 2.427505116308889

Epoch: 6| Step: 4
Training loss: 1.9097106456756592
Validation loss: 2.431558729499899

Epoch: 6| Step: 5
Training loss: 2.955672264099121
Validation loss: 2.4297958855987876

Epoch: 6| Step: 6
Training loss: 1.9979743957519531
Validation loss: 2.426110365057504

Epoch: 6| Step: 7
Training loss: 2.9638681411743164
Validation loss: 2.4179555036688365

Epoch: 6| Step: 8
Training loss: 2.5686123371124268
Validation loss: 2.41155598496878

Epoch: 6| Step: 9
Training loss: 2.3973171710968018
Validation loss: 2.406611363093058

Epoch: 6| Step: 10
Training loss: 2.9805703163146973
Validation loss: 2.401144161019274

Epoch: 6| Step: 11
Training loss: 2.6896495819091797
Validation loss: 2.3906931979681856

Epoch: 6| Step: 12
Training loss: 2.7128257751464844
Validation loss: 2.3877595855343725

Epoch: 6| Step: 13
Training loss: 3.0208003520965576
Validation loss: 2.388438270938012

Epoch: 214| Step: 0
Training loss: 2.305786609649658
Validation loss: 2.391507074397097

Epoch: 6| Step: 1
Training loss: 3.040499687194824
Validation loss: 2.3881414833889214

Epoch: 6| Step: 2
Training loss: 2.189495086669922
Validation loss: 2.3905936953842

Epoch: 6| Step: 3
Training loss: 2.8304176330566406
Validation loss: 2.38904300812752

Epoch: 6| Step: 4
Training loss: 1.7741435766220093
Validation loss: 2.387798258053359

Epoch: 6| Step: 5
Training loss: 2.9666314125061035
Validation loss: 2.3999328792736097

Epoch: 6| Step: 6
Training loss: 2.647109031677246
Validation loss: 2.4121744504538913

Epoch: 6| Step: 7
Training loss: 2.5467538833618164
Validation loss: 2.4108324922541136

Epoch: 6| Step: 8
Training loss: 2.737969160079956
Validation loss: 2.395898949715399

Epoch: 6| Step: 9
Training loss: 2.3806376457214355
Validation loss: 2.404902542791059

Epoch: 6| Step: 10
Training loss: 2.929471492767334
Validation loss: 2.409911081355105

Epoch: 6| Step: 11
Training loss: 2.0868377685546875
Validation loss: 2.400474545776203

Epoch: 6| Step: 12
Training loss: 3.5333330631256104
Validation loss: 2.3985250457640617

Epoch: 6| Step: 13
Training loss: 2.241581916809082
Validation loss: 2.407074743701566

Epoch: 215| Step: 0
Training loss: 2.96043062210083
Validation loss: 2.4055570889544744

Epoch: 6| Step: 1
Training loss: 2.6142258644104004
Validation loss: 2.4029680195675103

Epoch: 6| Step: 2
Training loss: 2.5237841606140137
Validation loss: 2.4019624366555163

Epoch: 6| Step: 3
Training loss: 2.1253292560577393
Validation loss: 2.406050564140402

Epoch: 6| Step: 4
Training loss: 3.13224196434021
Validation loss: 2.401509195245722

Epoch: 6| Step: 5
Training loss: 3.259554386138916
Validation loss: 2.401766123310212

Epoch: 6| Step: 6
Training loss: 2.673255681991577
Validation loss: 2.407085987829393

Epoch: 6| Step: 7
Training loss: 2.3793797492980957
Validation loss: 2.400657198762381

Epoch: 6| Step: 8
Training loss: 2.538433313369751
Validation loss: 2.39455860148194

Epoch: 6| Step: 9
Training loss: 2.358260154724121
Validation loss: 2.3992041682684295

Epoch: 6| Step: 10
Training loss: 2.3172788619995117
Validation loss: 2.3978253385072112

Epoch: 6| Step: 11
Training loss: 2.9238271713256836
Validation loss: 2.3883301160668813

Epoch: 6| Step: 12
Training loss: 2.184199094772339
Validation loss: 2.393773586519303

Epoch: 6| Step: 13
Training loss: 2.268514633178711
Validation loss: 2.3916224818075857

Epoch: 216| Step: 0
Training loss: 2.920827865600586
Validation loss: 2.3925621612097627

Epoch: 6| Step: 1
Training loss: 2.1095595359802246
Validation loss: 2.3980583619045954

Epoch: 6| Step: 2
Training loss: 2.9434380531311035
Validation loss: 2.391113399177469

Epoch: 6| Step: 3
Training loss: 2.5392394065856934
Validation loss: 2.393385748709402

Epoch: 6| Step: 4
Training loss: 2.5008277893066406
Validation loss: 2.402009194897067

Epoch: 6| Step: 5
Training loss: 2.2141189575195312
Validation loss: 2.3999421775981946

Epoch: 6| Step: 6
Training loss: 3.5838139057159424
Validation loss: 2.4027085560624317

Epoch: 6| Step: 7
Training loss: 2.360062837600708
Validation loss: 2.3953092123872493

Epoch: 6| Step: 8
Training loss: 1.7848787307739258
Validation loss: 2.4062039467596237

Epoch: 6| Step: 9
Training loss: 2.454946994781494
Validation loss: 2.396629512950938

Epoch: 6| Step: 10
Training loss: 2.8915553092956543
Validation loss: 2.4004456022734284

Epoch: 6| Step: 11
Training loss: 2.4746365547180176
Validation loss: 2.3917286883118334

Epoch: 6| Step: 12
Training loss: 3.0173816680908203
Validation loss: 2.4000197943820747

Epoch: 6| Step: 13
Training loss: 2.6738405227661133
Validation loss: 2.404696072301557

Epoch: 217| Step: 0
Training loss: 1.9326181411743164
Validation loss: 2.404291809246104

Epoch: 6| Step: 1
Training loss: 2.6464812755584717
Validation loss: 2.402469220981803

Epoch: 6| Step: 2
Training loss: 2.1896727085113525
Validation loss: 2.402026376416606

Epoch: 6| Step: 3
Training loss: 3.066572666168213
Validation loss: 2.397641835674163

Epoch: 6| Step: 4
Training loss: 2.5001003742218018
Validation loss: 2.403522281236546

Epoch: 6| Step: 5
Training loss: 2.861797571182251
Validation loss: 2.401613414928477

Epoch: 6| Step: 6
Training loss: 3.662733316421509
Validation loss: 2.4049586788300545

Epoch: 6| Step: 7
Training loss: 2.0532453060150146
Validation loss: 2.4030466079711914

Epoch: 6| Step: 8
Training loss: 1.7449907064437866
Validation loss: 2.4039509886054584

Epoch: 6| Step: 9
Training loss: 3.004079580307007
Validation loss: 2.3977402820382068

Epoch: 6| Step: 10
Training loss: 2.1549530029296875
Validation loss: 2.3872473009171022

Epoch: 6| Step: 11
Training loss: 3.0915071964263916
Validation loss: 2.389552903431718

Epoch: 6| Step: 12
Training loss: 2.999178886413574
Validation loss: 2.390435316229379

Epoch: 6| Step: 13
Training loss: 2.2308053970336914
Validation loss: 2.4004287155725623

Epoch: 218| Step: 0
Training loss: 3.1287262439727783
Validation loss: 2.3953995191922752

Epoch: 6| Step: 1
Training loss: 2.2746052742004395
Validation loss: 2.3922503814902356

Epoch: 6| Step: 2
Training loss: 2.1343774795532227
Validation loss: 2.39770245552063

Epoch: 6| Step: 3
Training loss: 3.1067748069763184
Validation loss: 2.3967008026697303

Epoch: 6| Step: 4
Training loss: 2.155958652496338
Validation loss: 2.403258559524372

Epoch: 6| Step: 5
Training loss: 2.6532464027404785
Validation loss: 2.4054290043410433

Epoch: 6| Step: 6
Training loss: 2.609151840209961
Validation loss: 2.422044583546218

Epoch: 6| Step: 7
Training loss: 3.066072463989258
Validation loss: 2.415454110791606

Epoch: 6| Step: 8
Training loss: 2.392249584197998
Validation loss: 2.42286693152561

Epoch: 6| Step: 9
Training loss: 2.4607954025268555
Validation loss: 2.419706677877775

Epoch: 6| Step: 10
Training loss: 2.6481575965881348
Validation loss: 2.406623360931232

Epoch: 6| Step: 11
Training loss: 2.154136896133423
Validation loss: 2.415914948268603

Epoch: 6| Step: 12
Training loss: 2.6280019283294678
Validation loss: 2.3978088568615656

Epoch: 6| Step: 13
Training loss: 3.0904951095581055
Validation loss: 2.388847909947877

Epoch: 219| Step: 0
Training loss: 2.0530290603637695
Validation loss: 2.3925801066942114

Epoch: 6| Step: 1
Training loss: 2.5413966178894043
Validation loss: 2.3934515419826714

Epoch: 6| Step: 2
Training loss: 3.0816192626953125
Validation loss: 2.397010031566825

Epoch: 6| Step: 3
Training loss: 1.9642407894134521
Validation loss: 2.3960718467671382

Epoch: 6| Step: 4
Training loss: 2.680803060531616
Validation loss: 2.3881566319414365

Epoch: 6| Step: 5
Training loss: 2.5037407875061035
Validation loss: 2.395457285706715

Epoch: 6| Step: 6
Training loss: 2.3736205101013184
Validation loss: 2.4000503939967

Epoch: 6| Step: 7
Training loss: 2.100848913192749
Validation loss: 2.402784232170351

Epoch: 6| Step: 8
Training loss: 3.200547218322754
Validation loss: 2.3961505043891167

Epoch: 6| Step: 9
Training loss: 2.8806724548339844
Validation loss: 2.4008078446952243

Epoch: 6| Step: 10
Training loss: 2.856443405151367
Validation loss: 2.396514243977044

Epoch: 6| Step: 11
Training loss: 2.5105762481689453
Validation loss: 2.388002936558057

Epoch: 6| Step: 12
Training loss: 3.404573917388916
Validation loss: 2.397717365654566

Epoch: 6| Step: 13
Training loss: 2.084378957748413
Validation loss: 2.395379058776363

Epoch: 220| Step: 0
Training loss: 2.293933868408203
Validation loss: 2.399471193231562

Epoch: 6| Step: 1
Training loss: 3.542477607727051
Validation loss: 2.4068439314442296

Epoch: 6| Step: 2
Training loss: 1.8680516481399536
Validation loss: 2.4131263007399855

Epoch: 6| Step: 3
Training loss: 1.6112267971038818
Validation loss: 2.406510014687815

Epoch: 6| Step: 4
Training loss: 2.774358034133911
Validation loss: 2.4107805605857604

Epoch: 6| Step: 5
Training loss: 2.0792860984802246
Validation loss: 2.412819677783597

Epoch: 6| Step: 6
Training loss: 3.205287456512451
Validation loss: 2.4026800253057994

Epoch: 6| Step: 7
Training loss: 2.6921677589416504
Validation loss: 2.4031895463184645

Epoch: 6| Step: 8
Training loss: 3.233452558517456
Validation loss: 2.4140030722464285

Epoch: 6| Step: 9
Training loss: 2.76358699798584
Validation loss: 2.4040649424317064

Epoch: 6| Step: 10
Training loss: 2.7033443450927734
Validation loss: 2.3942835125871884

Epoch: 6| Step: 11
Training loss: 1.898362398147583
Validation loss: 2.393137772878011

Epoch: 6| Step: 12
Training loss: 2.3845033645629883
Validation loss: 2.3940484575046006

Epoch: 6| Step: 13
Training loss: 3.7294082641601562
Validation loss: 2.38738093837615

Epoch: 221| Step: 0
Training loss: 2.332594871520996
Validation loss: 2.3902857021618913

Epoch: 6| Step: 1
Training loss: 3.2799978256225586
Validation loss: 2.396479016991072

Epoch: 6| Step: 2
Training loss: 3.3069047927856445
Validation loss: 2.3952581395385084

Epoch: 6| Step: 3
Training loss: 3.1831610202789307
Validation loss: 2.3954480822368334

Epoch: 6| Step: 4
Training loss: 2.3170385360717773
Validation loss: 2.38761156605136

Epoch: 6| Step: 5
Training loss: 2.620187759399414
Validation loss: 2.39921893483849

Epoch: 6| Step: 6
Training loss: 2.984452724456787
Validation loss: 2.3983491851437475

Epoch: 6| Step: 7
Training loss: 2.29610276222229
Validation loss: 2.4134380099593953

Epoch: 6| Step: 8
Training loss: 2.51353120803833
Validation loss: 2.403493599225116

Epoch: 6| Step: 9
Training loss: 1.7772114276885986
Validation loss: 2.411033804698657

Epoch: 6| Step: 10
Training loss: 2.379323959350586
Validation loss: 2.4010591750503867

Epoch: 6| Step: 11
Training loss: 2.1376726627349854
Validation loss: 2.423568241057857

Epoch: 6| Step: 12
Training loss: 3.093418836593628
Validation loss: 2.4137692143840175

Epoch: 6| Step: 13
Training loss: 1.5175344944000244
Validation loss: 2.4137447649432766

Epoch: 222| Step: 0
Training loss: 3.1385557651519775
Validation loss: 2.425798467410508

Epoch: 6| Step: 1
Training loss: 2.560112953186035
Validation loss: 2.424951345689835

Epoch: 6| Step: 2
Training loss: 2.439460039138794
Validation loss: 2.4118533262642483

Epoch: 6| Step: 3
Training loss: 1.2812143564224243
Validation loss: 2.41028614710736

Epoch: 6| Step: 4
Training loss: 2.386826992034912
Validation loss: 2.4125280457158245

Epoch: 6| Step: 5
Training loss: 2.9592785835266113
Validation loss: 2.4253776355456282

Epoch: 6| Step: 6
Training loss: 2.5560200214385986
Validation loss: 2.402859164822486

Epoch: 6| Step: 7
Training loss: 3.0700807571411133
Validation loss: 2.4124773292131323

Epoch: 6| Step: 8
Training loss: 2.4845712184906006
Validation loss: 2.3983453243009505

Epoch: 6| Step: 9
Training loss: 2.959292411804199
Validation loss: 2.4162997045824604

Epoch: 6| Step: 10
Training loss: 2.1823277473449707
Validation loss: 2.395877112624466

Epoch: 6| Step: 11
Training loss: 2.755936861038208
Validation loss: 2.3999113216195056

Epoch: 6| Step: 12
Training loss: 2.429032325744629
Validation loss: 2.3934716537434566

Epoch: 6| Step: 13
Training loss: 3.4160172939300537
Validation loss: 2.3794017017528577

Epoch: 223| Step: 0
Training loss: 2.1903085708618164
Validation loss: 2.3852727156813427

Epoch: 6| Step: 1
Training loss: 2.9412450790405273
Validation loss: 2.3760169988037436

Epoch: 6| Step: 2
Training loss: 2.5906715393066406
Validation loss: 2.377881932002242

Epoch: 6| Step: 3
Training loss: 2.4777565002441406
Validation loss: 2.3719105976884083

Epoch: 6| Step: 4
Training loss: 3.1862411499023438
Validation loss: 2.376574382987074

Epoch: 6| Step: 5
Training loss: 3.3123559951782227
Validation loss: 2.377762463784987

Epoch: 6| Step: 6
Training loss: 2.755067825317383
Validation loss: 2.3727595959940264

Epoch: 6| Step: 7
Training loss: 2.3707833290100098
Validation loss: 2.3806440061138523

Epoch: 6| Step: 8
Training loss: 2.784186363220215
Validation loss: 2.377957267145957

Epoch: 6| Step: 9
Training loss: 2.051501750946045
Validation loss: 2.376127609642603

Epoch: 6| Step: 10
Training loss: 2.089121103286743
Validation loss: 2.373241803979361

Epoch: 6| Step: 11
Training loss: 2.0970420837402344
Validation loss: 2.384770256216808

Epoch: 6| Step: 12
Training loss: 2.866210699081421
Validation loss: 2.377872145304116

Epoch: 6| Step: 13
Training loss: 2.4858903884887695
Validation loss: 2.381736898934969

Epoch: 224| Step: 0
Training loss: 1.9080873727798462
Validation loss: 2.387843026909777

Epoch: 6| Step: 1
Training loss: 2.9914357662200928
Validation loss: 2.4022308395754908

Epoch: 6| Step: 2
Training loss: 1.895763635635376
Validation loss: 2.40652798580867

Epoch: 6| Step: 3
Training loss: 2.5511250495910645
Validation loss: 2.405263908447758

Epoch: 6| Step: 4
Training loss: 2.276808738708496
Validation loss: 2.414315641567271

Epoch: 6| Step: 5
Training loss: 2.495910167694092
Validation loss: 2.412936141414027

Epoch: 6| Step: 6
Training loss: 2.478562831878662
Validation loss: 2.406458257347025

Epoch: 6| Step: 7
Training loss: 2.7496206760406494
Validation loss: 2.4120173838830765

Epoch: 6| Step: 8
Training loss: 2.5164358615875244
Validation loss: 2.4073903022273893

Epoch: 6| Step: 9
Training loss: 3.289287805557251
Validation loss: 2.3888386244414956

Epoch: 6| Step: 10
Training loss: 1.797644019126892
Validation loss: 2.391038079415598

Epoch: 6| Step: 11
Training loss: 2.9292590618133545
Validation loss: 2.3897204399108887

Epoch: 6| Step: 12
Training loss: 2.9016060829162598
Validation loss: 2.3903565201708066

Epoch: 6| Step: 13
Training loss: 4.032421588897705
Validation loss: 2.392143177729781

Epoch: 225| Step: 0
Training loss: 2.7777304649353027
Validation loss: 2.3809287932611283

Epoch: 6| Step: 1
Training loss: 2.6917967796325684
Validation loss: 2.3819281619082213

Epoch: 6| Step: 2
Training loss: 2.367492198944092
Validation loss: 2.3867243797548356

Epoch: 6| Step: 3
Training loss: 3.3227670192718506
Validation loss: 2.3821783091432307

Epoch: 6| Step: 4
Training loss: 2.594066858291626
Validation loss: 2.379681864092427

Epoch: 6| Step: 5
Training loss: 2.7959511280059814
Validation loss: 2.382355959184708

Epoch: 6| Step: 6
Training loss: 2.4398112297058105
Validation loss: 2.3933079165797078

Epoch: 6| Step: 7
Training loss: 2.0214245319366455
Validation loss: 2.396512113591676

Epoch: 6| Step: 8
Training loss: 1.8560686111450195
Validation loss: 2.386678541860273

Epoch: 6| Step: 9
Training loss: 3.3064985275268555
Validation loss: 2.3912525459002425

Epoch: 6| Step: 10
Training loss: 1.853498101234436
Validation loss: 2.383115344150092

Epoch: 6| Step: 11
Training loss: 2.1058292388916016
Validation loss: 2.3717596710369153

Epoch: 6| Step: 12
Training loss: 3.032942056655884
Validation loss: 2.3688347365266536

Epoch: 6| Step: 13
Training loss: 3.5575153827667236
Validation loss: 2.3758428276226087

Epoch: 226| Step: 0
Training loss: 2.891286611557007
Validation loss: 2.3686370798336562

Epoch: 6| Step: 1
Training loss: 2.3495240211486816
Validation loss: 2.3787818083199124

Epoch: 6| Step: 2
Training loss: 2.520153284072876
Validation loss: 2.3838484056534304

Epoch: 6| Step: 3
Training loss: 2.752347707748413
Validation loss: 2.380518965823676

Epoch: 6| Step: 4
Training loss: 2.614253044128418
Validation loss: 2.388018731148012

Epoch: 6| Step: 5
Training loss: 2.186979293823242
Validation loss: 2.3910422812226

Epoch: 6| Step: 6
Training loss: 3.4293253421783447
Validation loss: 2.3920938994294856

Epoch: 6| Step: 7
Training loss: 2.7520720958709717
Validation loss: 2.393173784338018

Epoch: 6| Step: 8
Training loss: 2.620551109313965
Validation loss: 2.3890616586131435

Epoch: 6| Step: 9
Training loss: 2.9508543014526367
Validation loss: 2.385381393535163

Epoch: 6| Step: 10
Training loss: 2.8697118759155273
Validation loss: 2.3777341868287776

Epoch: 6| Step: 11
Training loss: 1.8382773399353027
Validation loss: 2.3837193442929174

Epoch: 6| Step: 12
Training loss: 2.124483108520508
Validation loss: 2.3707053379345964

Epoch: 6| Step: 13
Training loss: 2.555117607116699
Validation loss: 2.3669533191188687

Epoch: 227| Step: 0
Training loss: 2.672860860824585
Validation loss: 2.3692137041399555

Epoch: 6| Step: 1
Training loss: 3.3331756591796875
Validation loss: 2.3757635547268774

Epoch: 6| Step: 2
Training loss: 2.636814594268799
Validation loss: 2.360163386150073

Epoch: 6| Step: 3
Training loss: 3.038092613220215
Validation loss: 2.373118805628951

Epoch: 6| Step: 4
Training loss: 2.5334572792053223
Validation loss: 2.3661928997244885

Epoch: 6| Step: 5
Training loss: 2.821213722229004
Validation loss: 2.3755546077605216

Epoch: 6| Step: 6
Training loss: 3.1817705631256104
Validation loss: 2.3860032635350383

Epoch: 6| Step: 7
Training loss: 2.3218040466308594
Validation loss: 2.385482936777094

Epoch: 6| Step: 8
Training loss: 2.302957534790039
Validation loss: 2.393269718334239

Epoch: 6| Step: 9
Training loss: 2.2206907272338867
Validation loss: 2.379100881597047

Epoch: 6| Step: 10
Training loss: 2.546238899230957
Validation loss: 2.385824259891305

Epoch: 6| Step: 11
Training loss: 1.9352538585662842
Validation loss: 2.3813260037411927

Epoch: 6| Step: 12
Training loss: 2.39092755317688
Validation loss: 2.3724852479914182

Epoch: 6| Step: 13
Training loss: 1.947877287864685
Validation loss: 2.3826689745790217

Epoch: 228| Step: 0
Training loss: 2.172011137008667
Validation loss: 2.392323624703192

Epoch: 6| Step: 1
Training loss: 2.811953544616699
Validation loss: 2.408775670554048

Epoch: 6| Step: 2
Training loss: 2.6414098739624023
Validation loss: 2.4163521541062223

Epoch: 6| Step: 3
Training loss: 1.7546007633209229
Validation loss: 2.3999005774016022

Epoch: 6| Step: 4
Training loss: 3.07470965385437
Validation loss: 2.401527871367752

Epoch: 6| Step: 5
Training loss: 2.1431756019592285
Validation loss: 2.3879372894123034

Epoch: 6| Step: 6
Training loss: 2.361772298812866
Validation loss: 2.396271321081346

Epoch: 6| Step: 7
Training loss: 3.4355199337005615
Validation loss: 2.3880639409506195

Epoch: 6| Step: 8
Training loss: 2.3946032524108887
Validation loss: 2.3884835140679472

Epoch: 6| Step: 9
Training loss: 2.7375824451446533
Validation loss: 2.3866154557915142

Epoch: 6| Step: 10
Training loss: 3.1229541301727295
Validation loss: 2.3870802951115433

Epoch: 6| Step: 11
Training loss: 2.946643829345703
Validation loss: 2.377412808838711

Epoch: 6| Step: 12
Training loss: 2.6992990970611572
Validation loss: 2.37877724119412

Epoch: 6| Step: 13
Training loss: 1.2950043678283691
Validation loss: 2.374164189061811

Epoch: 229| Step: 0
Training loss: 2.3649497032165527
Validation loss: 2.376005070183867

Epoch: 6| Step: 1
Training loss: 3.2653961181640625
Validation loss: 2.3744220964370237

Epoch: 6| Step: 2
Training loss: 2.873185634613037
Validation loss: 2.3692418195868052

Epoch: 6| Step: 3
Training loss: 3.666022777557373
Validation loss: 2.376389349660566

Epoch: 6| Step: 4
Training loss: 2.3503732681274414
Validation loss: 2.3629405485686434

Epoch: 6| Step: 5
Training loss: 2.8695740699768066
Validation loss: 2.3639602635496404

Epoch: 6| Step: 6
Training loss: 2.3883087635040283
Validation loss: 2.36619726304085

Epoch: 6| Step: 7
Training loss: 1.9011039733886719
Validation loss: 2.367997036185316

Epoch: 6| Step: 8
Training loss: 3.031355381011963
Validation loss: 2.3707684265669955

Epoch: 6| Step: 9
Training loss: 3.013010263442993
Validation loss: 2.392499049504598

Epoch: 6| Step: 10
Training loss: 2.2224931716918945
Validation loss: 2.3964783350626626

Epoch: 6| Step: 11
Training loss: 2.329400062561035
Validation loss: 2.4112128749970467

Epoch: 6| Step: 12
Training loss: 1.7843616008758545
Validation loss: 2.4235286071736324

Epoch: 6| Step: 13
Training loss: 1.6709942817687988
Validation loss: 2.4202064955106346

Epoch: 230| Step: 0
Training loss: 2.444593906402588
Validation loss: 2.4259531190318446

Epoch: 6| Step: 1
Training loss: 2.2452595233917236
Validation loss: 2.4534601908858105

Epoch: 6| Step: 2
Training loss: 2.22678804397583
Validation loss: 2.4455129920795398

Epoch: 6| Step: 3
Training loss: 2.431422710418701
Validation loss: 2.4395790484643753

Epoch: 6| Step: 4
Training loss: 2.3197810649871826
Validation loss: 2.4521331018017185

Epoch: 6| Step: 5
Training loss: 1.9424610137939453
Validation loss: 2.433699761667559

Epoch: 6| Step: 6
Training loss: 2.729517936706543
Validation loss: 2.4146734270998227

Epoch: 6| Step: 7
Training loss: 3.0490055084228516
Validation loss: 2.4291497763767036

Epoch: 6| Step: 8
Training loss: 2.70835018157959
Validation loss: 2.4109959192173456

Epoch: 6| Step: 9
Training loss: 3.2156319618225098
Validation loss: 2.400787761134486

Epoch: 6| Step: 10
Training loss: 2.5025720596313477
Validation loss: 2.394745566511667

Epoch: 6| Step: 11
Training loss: 3.1966805458068848
Validation loss: 2.393015246237478

Epoch: 6| Step: 12
Training loss: 2.912698745727539
Validation loss: 2.3767393532619683

Epoch: 6| Step: 13
Training loss: 2.0092952251434326
Validation loss: 2.374920339994533

Epoch: 231| Step: 0
Training loss: 3.254669666290283
Validation loss: 2.3753288048569874

Epoch: 6| Step: 1
Training loss: 2.3598711490631104
Validation loss: 2.3742154926382084

Epoch: 6| Step: 2
Training loss: 2.7136409282684326
Validation loss: 2.3640285973907798

Epoch: 6| Step: 3
Training loss: 2.0423479080200195
Validation loss: 2.3755558690717145

Epoch: 6| Step: 4
Training loss: 2.629861354827881
Validation loss: 2.374328615844891

Epoch: 6| Step: 5
Training loss: 2.3370697498321533
Validation loss: 2.362150402479274

Epoch: 6| Step: 6
Training loss: 2.9372305870056152
Validation loss: 2.3685352289548485

Epoch: 6| Step: 7
Training loss: 2.361654281616211
Validation loss: 2.3728676560104534

Epoch: 6| Step: 8
Training loss: 1.9453579187393188
Validation loss: 2.3781619200142483

Epoch: 6| Step: 9
Training loss: 3.055075168609619
Validation loss: 2.372719790345879

Epoch: 6| Step: 10
Training loss: 2.185055732727051
Validation loss: 2.3728840658741612

Epoch: 6| Step: 11
Training loss: 2.878037929534912
Validation loss: 2.3669302668622745

Epoch: 6| Step: 12
Training loss: 2.7257814407348633
Validation loss: 2.3610196549405336

Epoch: 6| Step: 13
Training loss: 3.2663521766662598
Validation loss: 2.353588209357313

Epoch: 232| Step: 0
Training loss: 2.6362521648406982
Validation loss: 2.3532133768963557

Epoch: 6| Step: 1
Training loss: 2.4637417793273926
Validation loss: 2.360712146246305

Epoch: 6| Step: 2
Training loss: 2.390895366668701
Validation loss: 2.3552563575006302

Epoch: 6| Step: 3
Training loss: 2.799971103668213
Validation loss: 2.367836603554346

Epoch: 6| Step: 4
Training loss: 2.637685775756836
Validation loss: 2.3603807751850416

Epoch: 6| Step: 5
Training loss: 2.0250473022460938
Validation loss: 2.3798052546798543

Epoch: 6| Step: 6
Training loss: 2.1754696369171143
Validation loss: 2.3722959615850963

Epoch: 6| Step: 7
Training loss: 2.71089506149292
Validation loss: 2.3786816648257676

Epoch: 6| Step: 8
Training loss: 2.9102795124053955
Validation loss: 2.3869087208983717

Epoch: 6| Step: 9
Training loss: 2.6473121643066406
Validation loss: 2.3943121305076023

Epoch: 6| Step: 10
Training loss: 2.6617860794067383
Validation loss: 2.398429721914312

Epoch: 6| Step: 11
Training loss: 2.436952829360962
Validation loss: 2.396584508239582

Epoch: 6| Step: 12
Training loss: 2.8612008094787598
Validation loss: 2.3894175790971324

Epoch: 6| Step: 13
Training loss: 3.037949562072754
Validation loss: 2.398492408055131

Epoch: 233| Step: 0
Training loss: 2.6628823280334473
Validation loss: 2.3955801404932493

Epoch: 6| Step: 1
Training loss: 2.968234062194824
Validation loss: 2.3954230867406374

Epoch: 6| Step: 2
Training loss: 2.217681407928467
Validation loss: 2.3895311740136917

Epoch: 6| Step: 3
Training loss: 3.0679140090942383
Validation loss: 2.3840841144643803

Epoch: 6| Step: 4
Training loss: 2.5564019680023193
Validation loss: 2.3789935445272796

Epoch: 6| Step: 5
Training loss: 2.092494010925293
Validation loss: 2.386690185916039

Epoch: 6| Step: 6
Training loss: 2.479367256164551
Validation loss: 2.398403234379266

Epoch: 6| Step: 7
Training loss: 3.535069227218628
Validation loss: 2.405204601185296

Epoch: 6| Step: 8
Training loss: 2.711761474609375
Validation loss: 2.406010240636846

Epoch: 6| Step: 9
Training loss: 2.1125051975250244
Validation loss: 2.419527769088745

Epoch: 6| Step: 10
Training loss: 2.2738964557647705
Validation loss: 2.4031185719274704

Epoch: 6| Step: 11
Training loss: 2.8264825344085693
Validation loss: 2.3954886082679994

Epoch: 6| Step: 12
Training loss: 2.189713478088379
Validation loss: 2.382502535338043

Epoch: 6| Step: 13
Training loss: 2.273427724838257
Validation loss: 2.3611099130363873

Epoch: 234| Step: 0
Training loss: 3.119565010070801
Validation loss: 2.3693720243310414

Epoch: 6| Step: 1
Training loss: 2.2332534790039062
Validation loss: 2.356669631055606

Epoch: 6| Step: 2
Training loss: 2.191565752029419
Validation loss: 2.360653651657925

Epoch: 6| Step: 3
Training loss: 3.2932658195495605
Validation loss: 2.3601235394836753

Epoch: 6| Step: 4
Training loss: 3.043084144592285
Validation loss: 2.365753842938331

Epoch: 6| Step: 5
Training loss: 2.308525323867798
Validation loss: 2.363699719470034

Epoch: 6| Step: 6
Training loss: 2.153040885925293
Validation loss: 2.3658007037255073

Epoch: 6| Step: 7
Training loss: 2.6050024032592773
Validation loss: 2.3726339160755114

Epoch: 6| Step: 8
Training loss: 1.6735491752624512
Validation loss: 2.384811547494704

Epoch: 6| Step: 9
Training loss: 3.310662269592285
Validation loss: 2.399818533210344

Epoch: 6| Step: 10
Training loss: 2.027991533279419
Validation loss: 2.388118710569156

Epoch: 6| Step: 11
Training loss: 2.7346653938293457
Validation loss: 2.3963337508581017

Epoch: 6| Step: 12
Training loss: 2.8289332389831543
Validation loss: 2.4050829641280638

Epoch: 6| Step: 13
Training loss: 2.4084432125091553
Validation loss: 2.406251594584475

Epoch: 235| Step: 0
Training loss: 2.6564548015594482
Validation loss: 2.4313771724700928

Epoch: 6| Step: 1
Training loss: 3.1027185916900635
Validation loss: 2.4227553285578245

Epoch: 6| Step: 2
Training loss: 1.8105307817459106
Validation loss: 2.411567993061517

Epoch: 6| Step: 3
Training loss: 2.063274383544922
Validation loss: 2.412358089159894

Epoch: 6| Step: 4
Training loss: 2.4070990085601807
Validation loss: 2.40570177314102

Epoch: 6| Step: 5
Training loss: 2.391643524169922
Validation loss: 2.4056220413536153

Epoch: 6| Step: 6
Training loss: 3.0569705963134766
Validation loss: 2.4059476467870895

Epoch: 6| Step: 7
Training loss: 2.71484375
Validation loss: 2.3877201157231487

Epoch: 6| Step: 8
Training loss: 2.488576889038086
Validation loss: 2.3850545934451524

Epoch: 6| Step: 9
Training loss: 3.211056709289551
Validation loss: 2.3894628760635213

Epoch: 6| Step: 10
Training loss: 2.517772912979126
Validation loss: 2.388901043963689

Epoch: 6| Step: 11
Training loss: 2.435507297515869
Validation loss: 2.3825266771419074

Epoch: 6| Step: 12
Training loss: 2.9852652549743652
Validation loss: 2.377719771477484

Epoch: 6| Step: 13
Training loss: 1.8787400722503662
Validation loss: 2.3660386787947787

Epoch: 236| Step: 0
Training loss: 2.365978479385376
Validation loss: 2.3759187703491538

Epoch: 6| Step: 1
Training loss: 2.3732433319091797
Validation loss: 2.369604085081367

Epoch: 6| Step: 2
Training loss: 1.9833067655563354
Validation loss: 2.3739273330216766

Epoch: 6| Step: 3
Training loss: 1.5646145343780518
Validation loss: 2.3701215303072365

Epoch: 6| Step: 4
Training loss: 2.5981903076171875
Validation loss: 2.3667655760242092

Epoch: 6| Step: 5
Training loss: 3.280761241912842
Validation loss: 2.3683622024392568

Epoch: 6| Step: 6
Training loss: 2.5596466064453125
Validation loss: 2.3648056573765253

Epoch: 6| Step: 7
Training loss: 2.1782381534576416
Validation loss: 2.3698449955191663

Epoch: 6| Step: 8
Training loss: 2.471050262451172
Validation loss: 2.3702804555175123

Epoch: 6| Step: 9
Training loss: 2.806550979614258
Validation loss: 2.366066127695063

Epoch: 6| Step: 10
Training loss: 3.0925066471099854
Validation loss: 2.3701882682820803

Epoch: 6| Step: 11
Training loss: 2.8382880687713623
Validation loss: 2.373654139939175

Epoch: 6| Step: 12
Training loss: 2.9181020259857178
Validation loss: 2.381646387038692

Epoch: 6| Step: 13
Training loss: 3.06294584274292
Validation loss: 2.372088283620855

Epoch: 237| Step: 0
Training loss: 2.3136348724365234
Validation loss: 2.3689553724822177

Epoch: 6| Step: 1
Training loss: 2.4399991035461426
Validation loss: 2.3812074071617535

Epoch: 6| Step: 2
Training loss: 2.614982843399048
Validation loss: 2.3917269706726074

Epoch: 6| Step: 3
Training loss: 3.388683319091797
Validation loss: 2.4143592465308403

Epoch: 6| Step: 4
Training loss: 2.6191704273223877
Validation loss: 2.40057619156376

Epoch: 6| Step: 5
Training loss: 2.2084474563598633
Validation loss: 2.3892733281658542

Epoch: 6| Step: 6
Training loss: 2.939655303955078
Validation loss: 2.4004126364184963

Epoch: 6| Step: 7
Training loss: 2.3816933631896973
Validation loss: 2.3939683565529446

Epoch: 6| Step: 8
Training loss: 2.4439258575439453
Validation loss: 2.384200665258592

Epoch: 6| Step: 9
Training loss: 2.03194522857666
Validation loss: 2.38149526042323

Epoch: 6| Step: 10
Training loss: 2.824643611907959
Validation loss: 2.3795778853918916

Epoch: 6| Step: 11
Training loss: 2.2744133472442627
Validation loss: 2.3698682656852146

Epoch: 6| Step: 12
Training loss: 3.22348952293396
Validation loss: 2.3763562581872426

Epoch: 6| Step: 13
Training loss: 1.9504280090332031
Validation loss: 2.370034025561425

Epoch: 238| Step: 0
Training loss: 2.9493885040283203
Validation loss: 2.3787880328393753

Epoch: 6| Step: 1
Training loss: 2.3963565826416016
Validation loss: 2.3662125525936

Epoch: 6| Step: 2
Training loss: 2.112523078918457
Validation loss: 2.3586368637700237

Epoch: 6| Step: 3
Training loss: 2.408552408218384
Validation loss: 2.3602026226699993

Epoch: 6| Step: 4
Training loss: 2.068081855773926
Validation loss: 2.3591794685650895

Epoch: 6| Step: 5
Training loss: 3.120908260345459
Validation loss: 2.355073264850083

Epoch: 6| Step: 6
Training loss: 2.3989830017089844
Validation loss: 2.356425669885451

Epoch: 6| Step: 7
Training loss: 2.7742912769317627
Validation loss: 2.3623719446120726

Epoch: 6| Step: 8
Training loss: 2.899177074432373
Validation loss: 2.36498365863677

Epoch: 6| Step: 9
Training loss: 2.65289306640625
Validation loss: 2.3652794438023723

Epoch: 6| Step: 10
Training loss: 2.6738548278808594
Validation loss: 2.388649635417487

Epoch: 6| Step: 11
Training loss: 2.538729667663574
Validation loss: 2.3774684795769314

Epoch: 6| Step: 12
Training loss: 2.455562114715576
Validation loss: 2.3925568314008814

Epoch: 6| Step: 13
Training loss: 2.1697587966918945
Validation loss: 2.3830062779047156

Epoch: 239| Step: 0
Training loss: 2.1360230445861816
Validation loss: 2.3994659634046656

Epoch: 6| Step: 1
Training loss: 3.0727756023406982
Validation loss: 2.3795179064555834

Epoch: 6| Step: 2
Training loss: 3.507603645324707
Validation loss: 2.378813046281056

Epoch: 6| Step: 3
Training loss: 3.4090452194213867
Validation loss: 2.362510409406436

Epoch: 6| Step: 4
Training loss: 2.177126884460449
Validation loss: 2.3588259066304853

Epoch: 6| Step: 5
Training loss: 2.2584586143493652
Validation loss: 2.369863433222617

Epoch: 6| Step: 6
Training loss: 2.390829563140869
Validation loss: 2.366324568307528

Epoch: 6| Step: 7
Training loss: 2.494443416595459
Validation loss: 2.3593202970361196

Epoch: 6| Step: 8
Training loss: 2.7975783348083496
Validation loss: 2.3696657637114167

Epoch: 6| Step: 9
Training loss: 2.5841195583343506
Validation loss: 2.3630683755361908

Epoch: 6| Step: 10
Training loss: 2.346039295196533
Validation loss: 2.3559797656151558

Epoch: 6| Step: 11
Training loss: 2.26080584526062
Validation loss: 2.354899573069747

Epoch: 6| Step: 12
Training loss: 1.6706039905548096
Validation loss: 2.3571232365023707

Epoch: 6| Step: 13
Training loss: 2.9514541625976562
Validation loss: 2.369524491730557

Epoch: 240| Step: 0
Training loss: 2.5167315006256104
Validation loss: 2.370268775570777

Epoch: 6| Step: 1
Training loss: 2.6460344791412354
Validation loss: 2.3787416655530214

Epoch: 6| Step: 2
Training loss: 3.364467144012451
Validation loss: 2.3717253028705554

Epoch: 6| Step: 3
Training loss: 2.1459310054779053
Validation loss: 2.3713307919040805

Epoch: 6| Step: 4
Training loss: 2.293832540512085
Validation loss: 2.367038147423857

Epoch: 6| Step: 5
Training loss: 2.3226895332336426
Validation loss: 2.3663137753804526

Epoch: 6| Step: 6
Training loss: 2.4331727027893066
Validation loss: 2.381919353238998

Epoch: 6| Step: 7
Training loss: 1.9244275093078613
Validation loss: 2.37123227888538

Epoch: 6| Step: 8
Training loss: 2.2651267051696777
Validation loss: 2.3912280836412982

Epoch: 6| Step: 9
Training loss: 2.5498106479644775
Validation loss: 2.402102521670762

Epoch: 6| Step: 10
Training loss: 2.445542812347412
Validation loss: 2.413508238330964

Epoch: 6| Step: 11
Training loss: 2.4473211765289307
Validation loss: 2.428530244417088

Epoch: 6| Step: 12
Training loss: 3.6242432594299316
Validation loss: 2.426677726930188

Epoch: 6| Step: 13
Training loss: 3.1913411617279053
Validation loss: 2.403668729207849

Epoch: 241| Step: 0
Training loss: 2.194429397583008
Validation loss: 2.3874082590944026

Epoch: 6| Step: 1
Training loss: 2.2229843139648438
Validation loss: 2.3786562130015385

Epoch: 6| Step: 2
Training loss: 3.12209153175354
Validation loss: 2.3723915135988625

Epoch: 6| Step: 3
Training loss: 2.2561638355255127
Validation loss: 2.3572439403944117

Epoch: 6| Step: 4
Training loss: 1.6680402755737305
Validation loss: 2.3513250735498246

Epoch: 6| Step: 5
Training loss: 2.6035470962524414
Validation loss: 2.351898257450391

Epoch: 6| Step: 6
Training loss: 2.4058847427368164
Validation loss: 2.346469251058435

Epoch: 6| Step: 7
Training loss: 2.561927318572998
Validation loss: 2.352834168300834

Epoch: 6| Step: 8
Training loss: 2.4912943840026855
Validation loss: 2.3539740039456274

Epoch: 6| Step: 9
Training loss: 2.920330047607422
Validation loss: 2.3538689767160723

Epoch: 6| Step: 10
Training loss: 3.0711307525634766
Validation loss: 2.364343225315053

Epoch: 6| Step: 11
Training loss: 2.164144515991211
Validation loss: 2.358090541696036

Epoch: 6| Step: 12
Training loss: 3.4584312438964844
Validation loss: 2.3642091802371445

Epoch: 6| Step: 13
Training loss: 2.8969717025756836
Validation loss: 2.371676380916308

Epoch: 242| Step: 0
Training loss: 3.2263851165771484
Validation loss: 2.374902512437554

Epoch: 6| Step: 1
Training loss: 2.2643065452575684
Validation loss: 2.391456873186173

Epoch: 6| Step: 2
Training loss: 1.905921459197998
Validation loss: 2.39153633066403

Epoch: 6| Step: 3
Training loss: 3.272888660430908
Validation loss: 2.3997539397208922

Epoch: 6| Step: 4
Training loss: 2.0961923599243164
Validation loss: 2.385652483150523

Epoch: 6| Step: 5
Training loss: 2.052992820739746
Validation loss: 2.3969360936072563

Epoch: 6| Step: 6
Training loss: 2.2090048789978027
Validation loss: 2.3987191684784426

Epoch: 6| Step: 7
Training loss: 2.954500198364258
Validation loss: 2.396377855731595

Epoch: 6| Step: 8
Training loss: 2.1334900856018066
Validation loss: 2.3866100080551638

Epoch: 6| Step: 9
Training loss: 2.0662312507629395
Validation loss: 2.3770879212246148

Epoch: 6| Step: 10
Training loss: 2.4089674949645996
Validation loss: 2.3907222029983357

Epoch: 6| Step: 11
Training loss: 3.050818920135498
Validation loss: 2.372563298030566

Epoch: 6| Step: 12
Training loss: 3.6811423301696777
Validation loss: 2.3802648462275022

Epoch: 6| Step: 13
Training loss: 2.538069009780884
Validation loss: 2.368330801686933

Epoch: 243| Step: 0
Training loss: 2.4517452716827393
Validation loss: 2.3780778146559194

Epoch: 6| Step: 1
Training loss: 2.803582191467285
Validation loss: 2.367227180029756

Epoch: 6| Step: 2
Training loss: 2.7882235050201416
Validation loss: 2.3678234213141987

Epoch: 6| Step: 3
Training loss: 2.1242587566375732
Validation loss: 2.36265673945027

Epoch: 6| Step: 4
Training loss: 3.5110297203063965
Validation loss: 2.3566697797467633

Epoch: 6| Step: 5
Training loss: 2.5035738945007324
Validation loss: 2.358572377953478

Epoch: 6| Step: 6
Training loss: 2.677551031112671
Validation loss: 2.350381753777945

Epoch: 6| Step: 7
Training loss: 1.8735418319702148
Validation loss: 2.3677266156801613

Epoch: 6| Step: 8
Training loss: 2.10441255569458
Validation loss: 2.3645057908950315

Epoch: 6| Step: 9
Training loss: 2.667677879333496
Validation loss: 2.372439556224372

Epoch: 6| Step: 10
Training loss: 2.665510892868042
Validation loss: 2.3799316601086686

Epoch: 6| Step: 11
Training loss: 2.3341777324676514
Validation loss: 2.368267556672455

Epoch: 6| Step: 12
Training loss: 2.8724308013916016
Validation loss: 2.372306139238419

Epoch: 6| Step: 13
Training loss: 2.243013858795166
Validation loss: 2.3954201513721096

Epoch: 244| Step: 0
Training loss: 3.0904412269592285
Validation loss: 2.394282440985403

Epoch: 6| Step: 1
Training loss: 1.7651994228363037
Validation loss: 2.3672464765528196

Epoch: 6| Step: 2
Training loss: 2.390005350112915
Validation loss: 2.3696766668750393

Epoch: 6| Step: 3
Training loss: 1.7372546195983887
Validation loss: 2.37908737890182

Epoch: 6| Step: 4
Training loss: 2.6215097904205322
Validation loss: 2.3828171376259095

Epoch: 6| Step: 5
Training loss: 2.2884302139282227
Validation loss: 2.3754245337619575

Epoch: 6| Step: 6
Training loss: 2.720785617828369
Validation loss: 2.373519359096404

Epoch: 6| Step: 7
Training loss: 3.0505433082580566
Validation loss: 2.3701845189576507

Epoch: 6| Step: 8
Training loss: 3.048712730407715
Validation loss: 2.3828146714036182

Epoch: 6| Step: 9
Training loss: 2.6507275104522705
Validation loss: 2.363910407148382

Epoch: 6| Step: 10
Training loss: 2.7816128730773926
Validation loss: 2.380610817222185

Epoch: 6| Step: 11
Training loss: 2.7985105514526367
Validation loss: 2.3675318969193326

Epoch: 6| Step: 12
Training loss: 2.5037176609039307
Validation loss: 2.3678770449853714

Epoch: 6| Step: 13
Training loss: 2.0414254665374756
Validation loss: 2.3829710252823366

Epoch: 245| Step: 0
Training loss: 2.4839963912963867
Validation loss: 2.3645679232894734

Epoch: 6| Step: 1
Training loss: 2.3533692359924316
Validation loss: 2.370361899816862

Epoch: 6| Step: 2
Training loss: 2.004436731338501
Validation loss: 2.364935598065776

Epoch: 6| Step: 3
Training loss: 2.8280372619628906
Validation loss: 2.3597424261031614

Epoch: 6| Step: 4
Training loss: 2.7336459159851074
Validation loss: 2.369743545850118

Epoch: 6| Step: 5
Training loss: 1.6758097410202026
Validation loss: 2.3654008450046664

Epoch: 6| Step: 6
Training loss: 2.6771936416625977
Validation loss: 2.3721279098141577

Epoch: 6| Step: 7
Training loss: 2.2927088737487793
Validation loss: 2.376323846078688

Epoch: 6| Step: 8
Training loss: 2.6477537155151367
Validation loss: 2.3852236014540478

Epoch: 6| Step: 9
Training loss: 2.8780531883239746
Validation loss: 2.378586087175595

Epoch: 6| Step: 10
Training loss: 2.95060396194458
Validation loss: 2.3815958858818136

Epoch: 6| Step: 11
Training loss: 2.253406047821045
Validation loss: 2.3793892783503376

Epoch: 6| Step: 12
Training loss: 2.77839994430542
Validation loss: 2.376956316732591

Epoch: 6| Step: 13
Training loss: 3.442699670791626
Validation loss: 2.374921275723365

Epoch: 246| Step: 0
Training loss: 2.414199113845825
Validation loss: 2.368674603841638

Epoch: 6| Step: 1
Training loss: 2.4960875511169434
Validation loss: 2.3684842112243816

Epoch: 6| Step: 2
Training loss: 2.259033203125
Validation loss: 2.36639190489246

Epoch: 6| Step: 3
Training loss: 2.392967462539673
Validation loss: 2.3712058246776624

Epoch: 6| Step: 4
Training loss: 2.8382015228271484
Validation loss: 2.3724171705143426

Epoch: 6| Step: 5
Training loss: 2.019249439239502
Validation loss: 2.359724647255354

Epoch: 6| Step: 6
Training loss: 2.2269017696380615
Validation loss: 2.3689581578777683

Epoch: 6| Step: 7
Training loss: 2.1969852447509766
Validation loss: 2.357551905416673

Epoch: 6| Step: 8
Training loss: 2.9979190826416016
Validation loss: 2.376566369046447

Epoch: 6| Step: 9
Training loss: 2.1873576641082764
Validation loss: 2.3626322131003104

Epoch: 6| Step: 10
Training loss: 2.8803248405456543
Validation loss: 2.365230970485236

Epoch: 6| Step: 11
Training loss: 2.7162299156188965
Validation loss: 2.366549804646482

Epoch: 6| Step: 12
Training loss: 2.7662856578826904
Validation loss: 2.367715971444243

Epoch: 6| Step: 13
Training loss: 3.757481813430786
Validation loss: 2.3718980512311383

Epoch: 247| Step: 0
Training loss: 2.918689727783203
Validation loss: 2.3618962213557255

Epoch: 6| Step: 1
Training loss: 2.222266674041748
Validation loss: 2.3461631908211658

Epoch: 6| Step: 2
Training loss: 2.881838321685791
Validation loss: 2.362699931667697

Epoch: 6| Step: 3
Training loss: 1.9632079601287842
Validation loss: 2.346793069634386

Epoch: 6| Step: 4
Training loss: 2.8955984115600586
Validation loss: 2.358890128392045

Epoch: 6| Step: 5
Training loss: 2.510110855102539
Validation loss: 2.358419854153869

Epoch: 6| Step: 6
Training loss: 2.695326566696167
Validation loss: 2.360665657187021

Epoch: 6| Step: 7
Training loss: 2.31294846534729
Validation loss: 2.367599397577265

Epoch: 6| Step: 8
Training loss: 2.4235055446624756
Validation loss: 2.3657490053484516

Epoch: 6| Step: 9
Training loss: 2.798178195953369
Validation loss: 2.36432191377045

Epoch: 6| Step: 10
Training loss: 2.9510602951049805
Validation loss: 2.387818459541567

Epoch: 6| Step: 11
Training loss: 2.2249348163604736
Validation loss: 2.3956909512960785

Epoch: 6| Step: 12
Training loss: 2.2660765647888184
Validation loss: 2.3797071313345306

Epoch: 6| Step: 13
Training loss: 2.5635499954223633
Validation loss: 2.3871149401510916

Epoch: 248| Step: 0
Training loss: 2.692552089691162
Validation loss: 2.374079037738103

Epoch: 6| Step: 1
Training loss: 2.614612102508545
Validation loss: 2.3780595897346415

Epoch: 6| Step: 2
Training loss: 2.7089335918426514
Validation loss: 2.3799481725180023

Epoch: 6| Step: 3
Training loss: 2.769887924194336
Validation loss: 2.3797464550182386

Epoch: 6| Step: 4
Training loss: 2.3846349716186523
Validation loss: 2.3792255232411046

Epoch: 6| Step: 5
Training loss: 2.259864091873169
Validation loss: 2.3820535162443757

Epoch: 6| Step: 6
Training loss: 2.7433083057403564
Validation loss: 2.369042255545175

Epoch: 6| Step: 7
Training loss: 2.639641761779785
Validation loss: 2.3643318196778655

Epoch: 6| Step: 8
Training loss: 2.1240482330322266
Validation loss: 2.3570043117769304

Epoch: 6| Step: 9
Training loss: 3.9377620220184326
Validation loss: 2.350019278064851

Epoch: 6| Step: 10
Training loss: 2.770169496536255
Validation loss: 2.334133804485362

Epoch: 6| Step: 11
Training loss: 1.7626311779022217
Validation loss: 2.3356544356192313

Epoch: 6| Step: 12
Training loss: 1.5898504257202148
Validation loss: 2.3385324452513006

Epoch: 6| Step: 13
Training loss: 2.744541645050049
Validation loss: 2.323919188591742

Epoch: 249| Step: 0
Training loss: 2.250784397125244
Validation loss: 2.329962315097932

Epoch: 6| Step: 1
Training loss: 2.2790377140045166
Validation loss: 2.333932920168805

Epoch: 6| Step: 2
Training loss: 1.9891183376312256
Validation loss: 2.3409233554717033

Epoch: 6| Step: 3
Training loss: 2.917757034301758
Validation loss: 2.336940675653437

Epoch: 6| Step: 4
Training loss: 2.558431625366211
Validation loss: 2.3501889885112806

Epoch: 6| Step: 5
Training loss: 2.6060333251953125
Validation loss: 2.3673915170854136

Epoch: 6| Step: 6
Training loss: 2.8335134983062744
Validation loss: 2.3709815984131186

Epoch: 6| Step: 7
Training loss: 2.2344727516174316
Validation loss: 2.3788998178256455

Epoch: 6| Step: 8
Training loss: 1.924460768699646
Validation loss: 2.3738022837587582

Epoch: 6| Step: 9
Training loss: 2.4952380657196045
Validation loss: 2.37982443840273

Epoch: 6| Step: 10
Training loss: 3.34653902053833
Validation loss: 2.3846263654770388

Epoch: 6| Step: 11
Training loss: 2.6041371822357178
Validation loss: 2.3810279997446204

Epoch: 6| Step: 12
Training loss: 3.0227959156036377
Validation loss: 2.3927317075831915

Epoch: 6| Step: 13
Training loss: 2.8257529735565186
Validation loss: 2.3896199439161565

Epoch: 250| Step: 0
Training loss: 2.262282371520996
Validation loss: 2.3827694257100425

Epoch: 6| Step: 1
Training loss: 2.9386515617370605
Validation loss: 2.3613581811228106

Epoch: 6| Step: 2
Training loss: 2.997429847717285
Validation loss: 2.3602011383220716

Epoch: 6| Step: 3
Training loss: 2.1751322746276855
Validation loss: 2.341586325758247

Epoch: 6| Step: 4
Training loss: 3.1706461906433105
Validation loss: 2.335565718271399

Epoch: 6| Step: 5
Training loss: 1.6750596761703491
Validation loss: 2.3335495148935625

Epoch: 6| Step: 6
Training loss: 2.187847137451172
Validation loss: 2.3364240482289302

Epoch: 6| Step: 7
Training loss: 2.4223432540893555
Validation loss: 2.3245200162292807

Epoch: 6| Step: 8
Training loss: 2.9536542892456055
Validation loss: 2.341772546050369

Epoch: 6| Step: 9
Training loss: 2.2306270599365234
Validation loss: 2.3332363636262956

Epoch: 6| Step: 10
Training loss: 3.065972328186035
Validation loss: 2.346968920000138

Epoch: 6| Step: 11
Training loss: 2.666764974594116
Validation loss: 2.3509264299946446

Epoch: 6| Step: 12
Training loss: 2.179056406021118
Validation loss: 2.3416774734374015

Epoch: 6| Step: 13
Training loss: 2.7703168392181396
Validation loss: 2.331868064018988

Epoch: 251| Step: 0
Training loss: 2.3567543029785156
Validation loss: 2.3381522496541343

Epoch: 6| Step: 1
Training loss: 3.3419065475463867
Validation loss: 2.35598055008919

Epoch: 6| Step: 2
Training loss: 1.6060848236083984
Validation loss: 2.35349412374599

Epoch: 6| Step: 3
Training loss: 2.7259950637817383
Validation loss: 2.358798537203061

Epoch: 6| Step: 4
Training loss: 2.5507588386535645
Validation loss: 2.35533320519232

Epoch: 6| Step: 5
Training loss: 1.7820788621902466
Validation loss: 2.359254483253725

Epoch: 6| Step: 6
Training loss: 2.7487618923187256
Validation loss: 2.3689754163065264

Epoch: 6| Step: 7
Training loss: 2.0487515926361084
Validation loss: 2.392667370457803

Epoch: 6| Step: 8
Training loss: 2.544041156768799
Validation loss: 2.389403978983561

Epoch: 6| Step: 9
Training loss: 2.6939549446105957
Validation loss: 2.4105018697759157

Epoch: 6| Step: 10
Training loss: 2.7748703956604004
Validation loss: 2.382095500987063

Epoch: 6| Step: 11
Training loss: 2.567091464996338
Validation loss: 2.391035167119836

Epoch: 6| Step: 12
Training loss: 3.042473793029785
Validation loss: 2.3891466279183664

Epoch: 6| Step: 13
Training loss: 2.884582042694092
Validation loss: 2.3734689322851037

Epoch: 252| Step: 0
Training loss: 2.720560073852539
Validation loss: 2.3766708220205

Epoch: 6| Step: 1
Training loss: 2.9702911376953125
Validation loss: 2.369969598708614

Epoch: 6| Step: 2
Training loss: 2.121006965637207
Validation loss: 2.361149086747118

Epoch: 6| Step: 3
Training loss: 2.381909132003784
Validation loss: 2.351798185738184

Epoch: 6| Step: 4
Training loss: 2.739297866821289
Validation loss: 2.3430995659161638

Epoch: 6| Step: 5
Training loss: 2.6718616485595703
Validation loss: 2.3356180665313557

Epoch: 6| Step: 6
Training loss: 2.9039306640625
Validation loss: 2.322276646091092

Epoch: 6| Step: 7
Training loss: 1.912933349609375
Validation loss: 2.3324048275588662

Epoch: 6| Step: 8
Training loss: 2.4826226234436035
Validation loss: 2.3400192312015

Epoch: 6| Step: 9
Training loss: 2.4897260665893555
Validation loss: 2.332439350825484

Epoch: 6| Step: 10
Training loss: 2.3433303833007812
Validation loss: 2.3410919840617845

Epoch: 6| Step: 11
Training loss: 2.516045570373535
Validation loss: 2.34172761055731

Epoch: 6| Step: 12
Training loss: 3.041623592376709
Validation loss: 2.347959077486428

Epoch: 6| Step: 13
Training loss: 2.469189167022705
Validation loss: 2.366053112091557

Epoch: 253| Step: 0
Training loss: 2.6524410247802734
Validation loss: 2.3708852773071616

Epoch: 6| Step: 1
Training loss: 1.908539891242981
Validation loss: 2.3760003171941286

Epoch: 6| Step: 2
Training loss: 2.3433821201324463
Validation loss: 2.36445890959873

Epoch: 6| Step: 3
Training loss: 2.8502726554870605
Validation loss: 2.37091475917447

Epoch: 6| Step: 4
Training loss: 2.322054862976074
Validation loss: 2.3647221262736986

Epoch: 6| Step: 5
Training loss: 2.6407737731933594
Validation loss: 2.371521726731331

Epoch: 6| Step: 6
Training loss: 3.1162261962890625
Validation loss: 2.3591984189966673

Epoch: 6| Step: 7
Training loss: 2.800800323486328
Validation loss: 2.3556604949376916

Epoch: 6| Step: 8
Training loss: 1.8075222969055176
Validation loss: 2.3547132476683585

Epoch: 6| Step: 9
Training loss: 2.082387685775757
Validation loss: 2.3618611597245738

Epoch: 6| Step: 10
Training loss: 2.812839984893799
Validation loss: 2.38427468269102

Epoch: 6| Step: 11
Training loss: 2.795950412750244
Validation loss: 2.3810602759802215

Epoch: 6| Step: 12
Training loss: 3.379793882369995
Validation loss: 2.3664682911288355

Epoch: 6| Step: 13
Training loss: 1.6817355155944824
Validation loss: 2.3633754740479174

Epoch: 254| Step: 0
Training loss: 3.700806140899658
Validation loss: 2.355602134940445

Epoch: 6| Step: 1
Training loss: 2.6404976844787598
Validation loss: 2.3616272762257564

Epoch: 6| Step: 2
Training loss: 2.4136147499084473
Validation loss: 2.3599538367281676

Epoch: 6| Step: 3
Training loss: 2.022207498550415
Validation loss: 2.3526074296684674

Epoch: 6| Step: 4
Training loss: 2.364896297454834
Validation loss: 2.368454417874736

Epoch: 6| Step: 5
Training loss: 2.286214828491211
Validation loss: 2.360493408736362

Epoch: 6| Step: 6
Training loss: 2.4623513221740723
Validation loss: 2.3689306371955463

Epoch: 6| Step: 7
Training loss: 2.283437490463257
Validation loss: 2.376055737977387

Epoch: 6| Step: 8
Training loss: 2.4067721366882324
Validation loss: 2.409772580669772

Epoch: 6| Step: 9
Training loss: 2.684164524078369
Validation loss: 2.390389597544106

Epoch: 6| Step: 10
Training loss: 2.516226291656494
Validation loss: 2.4102287959027033

Epoch: 6| Step: 11
Training loss: 2.761122226715088
Validation loss: 2.4141894822479575

Epoch: 6| Step: 12
Training loss: 1.9294490814208984
Validation loss: 2.407155800891179

Epoch: 6| Step: 13
Training loss: 3.5753979682922363
Validation loss: 2.3840393661170878

Epoch: 255| Step: 0
Training loss: 2.57435941696167
Validation loss: 2.3637893687012377

Epoch: 6| Step: 1
Training loss: 2.4237215518951416
Validation loss: 2.3533987486234276

Epoch: 6| Step: 2
Training loss: 1.4149748086929321
Validation loss: 2.344699146927044

Epoch: 6| Step: 3
Training loss: 2.819648265838623
Validation loss: 2.3187479152474353

Epoch: 6| Step: 4
Training loss: 2.571518898010254
Validation loss: 2.320342427940779

Epoch: 6| Step: 5
Training loss: 2.736752510070801
Validation loss: 2.335762514862963

Epoch: 6| Step: 6
Training loss: 2.651658535003662
Validation loss: 2.3337068711557696

Epoch: 6| Step: 7
Training loss: 2.521144151687622
Validation loss: 2.3547122401575886

Epoch: 6| Step: 8
Training loss: 2.3206467628479004
Validation loss: 2.363253393480855

Epoch: 6| Step: 9
Training loss: 2.8790149688720703
Validation loss: 2.3574196343780844

Epoch: 6| Step: 10
Training loss: 2.5603370666503906
Validation loss: 2.3429624675422587

Epoch: 6| Step: 11
Training loss: 2.929410934448242
Validation loss: 2.349797232176668

Epoch: 6| Step: 12
Training loss: 2.6150197982788086
Validation loss: 2.348277771344749

Epoch: 6| Step: 13
Training loss: 2.9647910594940186
Validation loss: 2.3360944255705802

Epoch: 256| Step: 0
Training loss: 2.5771827697753906
Validation loss: 2.327814132936539

Epoch: 6| Step: 1
Training loss: 2.6134791374206543
Validation loss: 2.3227574825286865

Epoch: 6| Step: 2
Training loss: 2.648402214050293
Validation loss: 2.333943651568505

Epoch: 6| Step: 3
Training loss: 2.840186595916748
Validation loss: 2.337047364122124

Epoch: 6| Step: 4
Training loss: 2.4855401515960693
Validation loss: 2.3362865242906796

Epoch: 6| Step: 5
Training loss: 2.6957387924194336
Validation loss: 2.3523682548153784

Epoch: 6| Step: 6
Training loss: 2.942274808883667
Validation loss: 2.3682641521576913

Epoch: 6| Step: 7
Training loss: 2.762460708618164
Validation loss: 2.392214411048479

Epoch: 6| Step: 8
Training loss: 1.6671864986419678
Validation loss: 2.3948840146423667

Epoch: 6| Step: 9
Training loss: 2.4047060012817383
Validation loss: 2.412605257444484

Epoch: 6| Step: 10
Training loss: 2.592374801635742
Validation loss: 2.3973293201897734

Epoch: 6| Step: 11
Training loss: 1.080410122871399
Validation loss: 2.4039208683916318

Epoch: 6| Step: 12
Training loss: 3.41807222366333
Validation loss: 2.392028982921313

Epoch: 6| Step: 13
Training loss: 2.9013264179229736
Validation loss: 2.367267377914921

Epoch: 257| Step: 0
Training loss: 2.599717617034912
Validation loss: 2.361926955561484

Epoch: 6| Step: 1
Training loss: 2.518782138824463
Validation loss: 2.3600049993043304

Epoch: 6| Step: 2
Training loss: 2.742424249649048
Validation loss: 2.336185161785413

Epoch: 6| Step: 3
Training loss: 2.2874443531036377
Validation loss: 2.3382079678197063

Epoch: 6| Step: 4
Training loss: 3.212719440460205
Validation loss: 2.318326534763459

Epoch: 6| Step: 5
Training loss: 2.626445770263672
Validation loss: 2.321597594086842

Epoch: 6| Step: 6
Training loss: 3.382211208343506
Validation loss: 2.318795786109022

Epoch: 6| Step: 7
Training loss: 2.498978614807129
Validation loss: 2.3163762989864556

Epoch: 6| Step: 8
Training loss: 1.96871817111969
Validation loss: 2.333491066450714

Epoch: 6| Step: 9
Training loss: 3.0431928634643555
Validation loss: 2.3286468675059657

Epoch: 6| Step: 10
Training loss: 2.0468482971191406
Validation loss: 2.3247583502082416

Epoch: 6| Step: 11
Training loss: 1.915006399154663
Validation loss: 2.3294195718662714

Epoch: 6| Step: 12
Training loss: 2.2747397422790527
Validation loss: 2.3298582287244898

Epoch: 6| Step: 13
Training loss: 2.274752378463745
Validation loss: 2.340481996536255

Epoch: 258| Step: 0
Training loss: 1.5696961879730225
Validation loss: 2.337878170833793

Epoch: 6| Step: 1
Training loss: 2.4802870750427246
Validation loss: 2.3432221835659397

Epoch: 6| Step: 2
Training loss: 2.46972393989563
Validation loss: 2.3498289969659623

Epoch: 6| Step: 3
Training loss: 2.31666898727417
Validation loss: 2.344521789140599

Epoch: 6| Step: 4
Training loss: 2.6384806632995605
Validation loss: 2.3454419951285086

Epoch: 6| Step: 5
Training loss: 3.1315698623657227
Validation loss: 2.3397724064447547

Epoch: 6| Step: 6
Training loss: 2.6262218952178955
Validation loss: 2.338551793047177

Epoch: 6| Step: 7
Training loss: 2.7986230850219727
Validation loss: 2.340103692905877

Epoch: 6| Step: 8
Training loss: 3.1793787479400635
Validation loss: 2.3343874677535026

Epoch: 6| Step: 9
Training loss: 2.3580124378204346
Validation loss: 2.3374509837037776

Epoch: 6| Step: 10
Training loss: 1.4106311798095703
Validation loss: 2.3482726209907123

Epoch: 6| Step: 11
Training loss: 2.8716068267822266
Validation loss: 2.348124498962074

Epoch: 6| Step: 12
Training loss: 3.081364631652832
Validation loss: 2.3629475242348126

Epoch: 6| Step: 13
Training loss: 2.302511692047119
Validation loss: 2.3740747538946008

Epoch: 259| Step: 0
Training loss: 2.737602949142456
Validation loss: 2.3603157510039625

Epoch: 6| Step: 1
Training loss: 2.40211820602417
Validation loss: 2.3775018953507945

Epoch: 6| Step: 2
Training loss: 2.636277675628662
Validation loss: 2.369976500029205

Epoch: 6| Step: 3
Training loss: 2.851705312728882
Validation loss: 2.3627239247804046

Epoch: 6| Step: 4
Training loss: 2.349976062774658
Validation loss: 2.35671805566357

Epoch: 6| Step: 5
Training loss: 2.2419002056121826
Validation loss: 2.351954483216809

Epoch: 6| Step: 6
Training loss: 2.5780081748962402
Validation loss: 2.3694370972212924

Epoch: 6| Step: 7
Training loss: 1.9673810005187988
Validation loss: 2.3597320279767438

Epoch: 6| Step: 8
Training loss: 2.855314016342163
Validation loss: 2.359794919208814

Epoch: 6| Step: 9
Training loss: 2.4574148654937744
Validation loss: 2.3562109675458682

Epoch: 6| Step: 10
Training loss: 2.7009973526000977
Validation loss: 2.360586938037667

Epoch: 6| Step: 11
Training loss: 3.0004706382751465
Validation loss: 2.3492586125609694

Epoch: 6| Step: 12
Training loss: 2.3552920818328857
Validation loss: 2.3355544972163376

Epoch: 6| Step: 13
Training loss: 2.262486457824707
Validation loss: 2.342392690720097

Epoch: 260| Step: 0
Training loss: 1.8274794816970825
Validation loss: 2.3304783093032015

Epoch: 6| Step: 1
Training loss: 2.9128334522247314
Validation loss: 2.3273311866227018

Epoch: 6| Step: 2
Training loss: 2.5836551189422607
Validation loss: 2.319138547425629

Epoch: 6| Step: 3
Training loss: 2.9307713508605957
Validation loss: 2.312654692639587

Epoch: 6| Step: 4
Training loss: 2.566291570663452
Validation loss: 2.3126257196549447

Epoch: 6| Step: 5
Training loss: 2.6752612590789795
Validation loss: 2.309327079403785

Epoch: 6| Step: 6
Training loss: 2.2541415691375732
Validation loss: 2.3090040478655087

Epoch: 6| Step: 7
Training loss: 3.0627946853637695
Validation loss: 2.3075892899626043

Epoch: 6| Step: 8
Training loss: 2.9786510467529297
Validation loss: 2.3101071696127615

Epoch: 6| Step: 9
Training loss: 2.244291067123413
Validation loss: 2.3163561205710135

Epoch: 6| Step: 10
Training loss: 2.3763792514801025
Validation loss: 2.3251378254223893

Epoch: 6| Step: 11
Training loss: 2.2485456466674805
Validation loss: 2.322561162774281

Epoch: 6| Step: 12
Training loss: 2.2238709926605225
Validation loss: 2.312371894877444

Epoch: 6| Step: 13
Training loss: 2.4373998641967773
Validation loss: 2.3289288320849018

Epoch: 261| Step: 0
Training loss: 2.66110897064209
Validation loss: 2.3267273928529475

Epoch: 6| Step: 1
Training loss: 2.3101940155029297
Validation loss: 2.346414089202881

Epoch: 6| Step: 2
Training loss: 2.835460662841797
Validation loss: 2.3398781540573284

Epoch: 6| Step: 3
Training loss: 2.354619026184082
Validation loss: 2.3491293127818773

Epoch: 6| Step: 4
Training loss: 1.8902475833892822
Validation loss: 2.341495111424436

Epoch: 6| Step: 5
Training loss: 2.1000025272369385
Validation loss: 2.3422819542628464

Epoch: 6| Step: 6
Training loss: 2.008784055709839
Validation loss: 2.35517656418585

Epoch: 6| Step: 7
Training loss: 2.532865524291992
Validation loss: 2.3665905203870548

Epoch: 6| Step: 8
Training loss: 3.155622959136963
Validation loss: 2.3623775641123452

Epoch: 6| Step: 9
Training loss: 2.333527088165283
Validation loss: 2.3750938061744935

Epoch: 6| Step: 10
Training loss: 2.5043981075286865
Validation loss: 2.359894549974831

Epoch: 6| Step: 11
Training loss: 2.589097499847412
Validation loss: 2.36821303572706

Epoch: 6| Step: 12
Training loss: 3.0098094940185547
Validation loss: 2.3681739889165407

Epoch: 6| Step: 13
Training loss: 3.4414777755737305
Validation loss: 2.376758196020639

Epoch: 262| Step: 0
Training loss: 2.958383083343506
Validation loss: 2.35337552203927

Epoch: 6| Step: 1
Training loss: 1.9855544567108154
Validation loss: 2.3448050227216495

Epoch: 6| Step: 2
Training loss: 2.126025915145874
Validation loss: 2.3554313170012606

Epoch: 6| Step: 3
Training loss: 2.6508727073669434
Validation loss: 2.359041449844196

Epoch: 6| Step: 4
Training loss: 2.4656286239624023
Validation loss: 2.349339346731863

Epoch: 6| Step: 5
Training loss: 2.7726449966430664
Validation loss: 2.3470747265764462

Epoch: 6| Step: 6
Training loss: 2.499884605407715
Validation loss: 2.345688863467145

Epoch: 6| Step: 7
Training loss: 2.887641429901123
Validation loss: 2.350328233934218

Epoch: 6| Step: 8
Training loss: 2.789008617401123
Validation loss: 2.3558701545961442

Epoch: 6| Step: 9
Training loss: 2.384976387023926
Validation loss: 2.3499351342519126

Epoch: 6| Step: 10
Training loss: 2.9607205390930176
Validation loss: 2.3630013004426034

Epoch: 6| Step: 11
Training loss: 1.728935956954956
Validation loss: 2.3583608391464397

Epoch: 6| Step: 12
Training loss: 2.402486801147461
Validation loss: 2.3431234539196057

Epoch: 6| Step: 13
Training loss: 2.5463638305664062
Validation loss: 2.3433421516931183

Epoch: 263| Step: 0
Training loss: 1.7662009000778198
Validation loss: 2.3267275940987373

Epoch: 6| Step: 1
Training loss: 2.2909793853759766
Validation loss: 2.3238211190828713

Epoch: 6| Step: 2
Training loss: 2.320723056793213
Validation loss: 2.3201345974399197

Epoch: 6| Step: 3
Training loss: 3.346684455871582
Validation loss: 2.322764491522184

Epoch: 6| Step: 4
Training loss: 2.1777563095092773
Validation loss: 2.314573005963397

Epoch: 6| Step: 5
Training loss: 2.2412352561950684
Validation loss: 2.30611051026211

Epoch: 6| Step: 6
Training loss: 2.6801867485046387
Validation loss: 2.302319198526362

Epoch: 6| Step: 7
Training loss: 2.2038440704345703
Validation loss: 2.29783647547486

Epoch: 6| Step: 8
Training loss: 3.217728853225708
Validation loss: 2.3180541325640935

Epoch: 6| Step: 9
Training loss: 2.9051942825317383
Validation loss: 2.310481584200295

Epoch: 6| Step: 10
Training loss: 3.574044942855835
Validation loss: 2.3262122087581183

Epoch: 6| Step: 11
Training loss: 2.32438588142395
Validation loss: 2.331181099337916

Epoch: 6| Step: 12
Training loss: 2.0527119636535645
Validation loss: 2.3438798304527038

Epoch: 6| Step: 13
Training loss: 2.0476763248443604
Validation loss: 2.342308041869953

Epoch: 264| Step: 0
Training loss: 2.5590286254882812
Validation loss: 2.3578122713232554

Epoch: 6| Step: 1
Training loss: 2.228806972503662
Validation loss: 2.362310181381882

Epoch: 6| Step: 2
Training loss: 2.7352726459503174
Validation loss: 2.3645534233380388

Epoch: 6| Step: 3
Training loss: 2.3005099296569824
Validation loss: 2.355714282681865

Epoch: 6| Step: 4
Training loss: 2.799816131591797
Validation loss: 2.3576425275494977

Epoch: 6| Step: 5
Training loss: 2.5600454807281494
Validation loss: 2.3556496840651318

Epoch: 6| Step: 6
Training loss: 1.814953088760376
Validation loss: 2.3508783232781196

Epoch: 6| Step: 7
Training loss: 2.9320871829986572
Validation loss: 2.3474500166472567

Epoch: 6| Step: 8
Training loss: 2.4321579933166504
Validation loss: 2.3469549481586744

Epoch: 6| Step: 9
Training loss: 2.011765956878662
Validation loss: 2.340672246871456

Epoch: 6| Step: 10
Training loss: 3.022716999053955
Validation loss: 2.3593102398739068

Epoch: 6| Step: 11
Training loss: 2.952936887741089
Validation loss: 2.3759402664758826

Epoch: 6| Step: 12
Training loss: 2.4449474811553955
Validation loss: 2.3782687546104513

Epoch: 6| Step: 13
Training loss: 2.2676777839660645
Validation loss: 2.374205517512496

Epoch: 265| Step: 0
Training loss: 2.579338788986206
Validation loss: 2.3821628862811672

Epoch: 6| Step: 1
Training loss: 2.3756752014160156
Validation loss: 2.398902803338984

Epoch: 6| Step: 2
Training loss: 2.7333836555480957
Validation loss: 2.378565824160012

Epoch: 6| Step: 3
Training loss: 2.093585252761841
Validation loss: 2.384231049527404

Epoch: 6| Step: 4
Training loss: 3.509467124938965
Validation loss: 2.368895407645933

Epoch: 6| Step: 5
Training loss: 2.6459550857543945
Validation loss: 2.3509732164362425

Epoch: 6| Step: 6
Training loss: 2.612651824951172
Validation loss: 2.326710277988065

Epoch: 6| Step: 7
Training loss: 1.9538753032684326
Validation loss: 2.3343644526696976

Epoch: 6| Step: 8
Training loss: 2.271131992340088
Validation loss: 2.3207324345906577

Epoch: 6| Step: 9
Training loss: 2.450752019882202
Validation loss: 2.3090155893756497

Epoch: 6| Step: 10
Training loss: 2.5211617946624756
Validation loss: 2.320781841072985

Epoch: 6| Step: 11
Training loss: 2.4632644653320312
Validation loss: 2.317191000907652

Epoch: 6| Step: 12
Training loss: 2.7038278579711914
Validation loss: 2.306025679393481

Epoch: 6| Step: 13
Training loss: 2.0491976737976074
Validation loss: 2.3101312447619695

Epoch: 266| Step: 0
Training loss: 2.634082794189453
Validation loss: 2.2942406387739283

Epoch: 6| Step: 1
Training loss: 2.3634376525878906
Validation loss: 2.3098392435299453

Epoch: 6| Step: 2
Training loss: 2.1730360984802246
Validation loss: 2.296152417377759

Epoch: 6| Step: 3
Training loss: 2.6759703159332275
Validation loss: 2.2936850350390197

Epoch: 6| Step: 4
Training loss: 2.4789934158325195
Validation loss: 2.293305584179458

Epoch: 6| Step: 5
Training loss: 3.2109124660491943
Validation loss: 2.288927052610664

Epoch: 6| Step: 6
Training loss: 2.724478244781494
Validation loss: 2.3076215021071897

Epoch: 6| Step: 7
Training loss: 2.79156494140625
Validation loss: 2.310576387630996

Epoch: 6| Step: 8
Training loss: 2.679142951965332
Validation loss: 2.310702534132106

Epoch: 6| Step: 9
Training loss: 2.216160297393799
Validation loss: 2.321264015730991

Epoch: 6| Step: 10
Training loss: 2.0433239936828613
Validation loss: 2.3275831002061085

Epoch: 6| Step: 11
Training loss: 2.054490089416504
Validation loss: 2.330901015189386

Epoch: 6| Step: 12
Training loss: 2.583538293838501
Validation loss: 2.356529090994148

Epoch: 6| Step: 13
Training loss: 2.5822131633758545
Validation loss: 2.3600313663482666

Epoch: 267| Step: 0
Training loss: 2.450103998184204
Validation loss: 2.3823957673964964

Epoch: 6| Step: 1
Training loss: 3.504737138748169
Validation loss: 2.3964091308655275

Epoch: 6| Step: 2
Training loss: 2.6940698623657227
Validation loss: 2.3979408279542

Epoch: 6| Step: 3
Training loss: 1.5989830493927002
Validation loss: 2.404996774529898

Epoch: 6| Step: 4
Training loss: 2.4591407775878906
Validation loss: 2.376722910070932

Epoch: 6| Step: 5
Training loss: 1.4705756902694702
Validation loss: 2.3902831923577095

Epoch: 6| Step: 6
Training loss: 2.687821626663208
Validation loss: 2.3623298547601186

Epoch: 6| Step: 7
Training loss: 2.58099627494812
Validation loss: 2.3462839562405824

Epoch: 6| Step: 8
Training loss: 3.3238606452941895
Validation loss: 2.3222851240506737

Epoch: 6| Step: 9
Training loss: 2.558656930923462
Validation loss: 2.3142825582975983

Epoch: 6| Step: 10
Training loss: 2.1195669174194336
Validation loss: 2.3205330077038018

Epoch: 6| Step: 11
Training loss: 2.5458779335021973
Validation loss: 2.312383782479071

Epoch: 6| Step: 12
Training loss: 1.7987395524978638
Validation loss: 2.3182504651367024

Epoch: 6| Step: 13
Training loss: 4.109897613525391
Validation loss: 2.312230679296678

Epoch: 268| Step: 0
Training loss: 2.4155421257019043
Validation loss: 2.317730470370221

Epoch: 6| Step: 1
Training loss: 2.507519483566284
Validation loss: 2.31408533998715

Epoch: 6| Step: 2
Training loss: 2.3243134021759033
Validation loss: 2.3333723160528366

Epoch: 6| Step: 3
Training loss: 2.7152013778686523
Validation loss: 2.32232085094657

Epoch: 6| Step: 4
Training loss: 2.492173671722412
Validation loss: 2.3319592616891347

Epoch: 6| Step: 5
Training loss: 2.444415807723999
Validation loss: 2.3332096568999754

Epoch: 6| Step: 6
Training loss: 1.900413990020752
Validation loss: 2.32407138680899

Epoch: 6| Step: 7
Training loss: 2.4704174995422363
Validation loss: 2.3460769653320312

Epoch: 6| Step: 8
Training loss: 2.5815110206604004
Validation loss: 2.3393787619888142

Epoch: 6| Step: 9
Training loss: 2.697890520095825
Validation loss: 2.328528501654184

Epoch: 6| Step: 10
Training loss: 2.5992650985717773
Validation loss: 2.3383271002000376

Epoch: 6| Step: 11
Training loss: 2.7885067462921143
Validation loss: 2.34755773954494

Epoch: 6| Step: 12
Training loss: 2.2945892810821533
Validation loss: 2.3558930094524095

Epoch: 6| Step: 13
Training loss: 3.0163862705230713
Validation loss: 2.373349197449223

Epoch: 269| Step: 0
Training loss: 2.3755102157592773
Validation loss: 2.3972686490704938

Epoch: 6| Step: 1
Training loss: 2.2464640140533447
Validation loss: 2.4095935078077417

Epoch: 6| Step: 2
Training loss: 2.6914520263671875
Validation loss: 2.397234078376524

Epoch: 6| Step: 3
Training loss: 2.7291512489318848
Validation loss: 2.383888049792218

Epoch: 6| Step: 4
Training loss: 2.547433614730835
Validation loss: 2.3643680041836155

Epoch: 6| Step: 5
Training loss: 2.6387555599212646
Validation loss: 2.3571919125895344

Epoch: 6| Step: 6
Training loss: 2.449037790298462
Validation loss: 2.3653047418081634

Epoch: 6| Step: 7
Training loss: 2.6004085540771484
Validation loss: 2.344129249613772

Epoch: 6| Step: 8
Training loss: 1.7106019258499146
Validation loss: 2.3461220469526065

Epoch: 6| Step: 9
Training loss: 2.976195812225342
Validation loss: 2.35049876090019

Epoch: 6| Step: 10
Training loss: 1.7678678035736084
Validation loss: 2.3395901828683834

Epoch: 6| Step: 11
Training loss: 2.6017606258392334
Validation loss: 2.337624596011254

Epoch: 6| Step: 12
Training loss: 2.9128572940826416
Validation loss: 2.3441807403359363

Epoch: 6| Step: 13
Training loss: 3.1404621601104736
Validation loss: 2.340591082008936

Epoch: 270| Step: 0
Training loss: 2.2570688724517822
Validation loss: 2.326349719878166

Epoch: 6| Step: 1
Training loss: 2.2198338508605957
Validation loss: 2.319433125116492

Epoch: 6| Step: 2
Training loss: 2.2255983352661133
Validation loss: 2.317148723909932

Epoch: 6| Step: 3
Training loss: 3.0505199432373047
Validation loss: 2.3105282360507595

Epoch: 6| Step: 4
Training loss: 2.347661018371582
Validation loss: 2.3086289462222847

Epoch: 6| Step: 5
Training loss: 2.73239803314209
Validation loss: 2.2995191556151195

Epoch: 6| Step: 6
Training loss: 1.5942158699035645
Validation loss: 2.305587250699279

Epoch: 6| Step: 7
Training loss: 2.4462106227874756
Validation loss: 2.321085724779355

Epoch: 6| Step: 8
Training loss: 2.759084701538086
Validation loss: 2.335794432188875

Epoch: 6| Step: 9
Training loss: 3.0960867404937744
Validation loss: 2.328605995383314

Epoch: 6| Step: 10
Training loss: 2.9201810359954834
Validation loss: 2.3168408563060146

Epoch: 6| Step: 11
Training loss: 2.563598155975342
Validation loss: 2.3116502890022854

Epoch: 6| Step: 12
Training loss: 2.4953513145446777
Validation loss: 2.310824683917466

Epoch: 6| Step: 13
Training loss: 2.380699872970581
Validation loss: 2.293872356414795

Epoch: 271| Step: 0
Training loss: 2.2854952812194824
Validation loss: 2.324434926432948

Epoch: 6| Step: 1
Training loss: 2.2130987644195557
Validation loss: 2.319149266007126

Epoch: 6| Step: 2
Training loss: 2.3574771881103516
Validation loss: 2.331507767400434

Epoch: 6| Step: 3
Training loss: 2.578822612762451
Validation loss: 2.3320050842018536

Epoch: 6| Step: 4
Training loss: 2.4630117416381836
Validation loss: 2.3364865241512174

Epoch: 6| Step: 5
Training loss: 2.7268946170806885
Validation loss: 2.3560924401847263

Epoch: 6| Step: 6
Training loss: 2.329254627227783
Validation loss: 2.345617430184477

Epoch: 6| Step: 7
Training loss: 2.5464417934417725
Validation loss: 2.3666024438796507

Epoch: 6| Step: 8
Training loss: 1.5375313758850098
Validation loss: 2.363467352364653

Epoch: 6| Step: 9
Training loss: 3.152588367462158
Validation loss: 2.3546018395372617

Epoch: 6| Step: 10
Training loss: 2.7341926097869873
Validation loss: 2.3560585232191187

Epoch: 6| Step: 11
Training loss: 2.523623466491699
Validation loss: 2.3400393967987387

Epoch: 6| Step: 12
Training loss: 2.7457733154296875
Validation loss: 2.341499895177862

Epoch: 6| Step: 13
Training loss: 3.204577684402466
Validation loss: 2.3177994117941907

Epoch: 272| Step: 0
Training loss: 2.252004384994507
Validation loss: 2.320432439927132

Epoch: 6| Step: 1
Training loss: 2.6797280311584473
Validation loss: 2.3150655710568993

Epoch: 6| Step: 2
Training loss: 2.473540782928467
Validation loss: 2.3071911975901616

Epoch: 6| Step: 3
Training loss: 2.4769067764282227
Validation loss: 2.3172197777737855

Epoch: 6| Step: 4
Training loss: 2.0056357383728027
Validation loss: 2.3076061279542985

Epoch: 6| Step: 5
Training loss: 2.1971940994262695
Validation loss: 2.3100663410720004

Epoch: 6| Step: 6
Training loss: 2.598237991333008
Validation loss: 2.315539795865295

Epoch: 6| Step: 7
Training loss: 2.346163272857666
Validation loss: 2.3384788190164874

Epoch: 6| Step: 8
Training loss: 2.52626633644104
Validation loss: 2.3494170583704466

Epoch: 6| Step: 9
Training loss: 3.340528964996338
Validation loss: 2.346274309260871

Epoch: 6| Step: 10
Training loss: 2.8040273189544678
Validation loss: 2.3519886873101674

Epoch: 6| Step: 11
Training loss: 1.9506280422210693
Validation loss: 2.3339852722742225

Epoch: 6| Step: 12
Training loss: 2.5003013610839844
Validation loss: 2.3349896656569613

Epoch: 6| Step: 13
Training loss: 2.937135934829712
Validation loss: 2.353096813283941

Epoch: 273| Step: 0
Training loss: 2.6075761318206787
Validation loss: 2.364855884223856

Epoch: 6| Step: 1
Training loss: 2.6883950233459473
Validation loss: 2.376211735510057

Epoch: 6| Step: 2
Training loss: 2.082967758178711
Validation loss: 2.377181994017734

Epoch: 6| Step: 3
Training loss: 2.2967336177825928
Validation loss: 2.396672551349927

Epoch: 6| Step: 4
Training loss: 1.9999377727508545
Validation loss: 2.412728912086897

Epoch: 6| Step: 5
Training loss: 2.683328866958618
Validation loss: 2.40698226036564

Epoch: 6| Step: 6
Training loss: 2.760979413986206
Validation loss: 2.40364807908253

Epoch: 6| Step: 7
Training loss: 2.224835157394409
Validation loss: 2.386061422286495

Epoch: 6| Step: 8
Training loss: 2.9133517742156982
Validation loss: 2.349878490612071

Epoch: 6| Step: 9
Training loss: 2.3161025047302246
Validation loss: 2.343390803183279

Epoch: 6| Step: 10
Training loss: 2.3819990158081055
Validation loss: 2.323505236256507

Epoch: 6| Step: 11
Training loss: 2.400585651397705
Validation loss: 2.3113972576715613

Epoch: 6| Step: 12
Training loss: 3.0889573097229004
Validation loss: 2.3037733416403494

Epoch: 6| Step: 13
Training loss: 2.7830650806427
Validation loss: 2.3041232593597902

Epoch: 274| Step: 0
Training loss: 2.553257703781128
Validation loss: 2.296000842125185

Epoch: 6| Step: 1
Training loss: 2.3697147369384766
Validation loss: 2.29147070710377

Epoch: 6| Step: 2
Training loss: 2.1189322471618652
Validation loss: 2.2894964115594023

Epoch: 6| Step: 3
Training loss: 2.2264065742492676
Validation loss: 2.2911739964638986

Epoch: 6| Step: 4
Training loss: 2.8278656005859375
Validation loss: 2.3013085472968315

Epoch: 6| Step: 5
Training loss: 2.079704523086548
Validation loss: 2.3034668840387815

Epoch: 6| Step: 6
Training loss: 3.0350656509399414
Validation loss: 2.3074969142995854

Epoch: 6| Step: 7
Training loss: 2.712174892425537
Validation loss: 2.3171934235480522

Epoch: 6| Step: 8
Training loss: 2.641097068786621
Validation loss: 2.32467064421664

Epoch: 6| Step: 9
Training loss: 2.626880407333374
Validation loss: 2.327282049322641

Epoch: 6| Step: 10
Training loss: 2.771132230758667
Validation loss: 2.3552767102436354

Epoch: 6| Step: 11
Training loss: 1.8882601261138916
Validation loss: 2.3850761882720457

Epoch: 6| Step: 12
Training loss: 2.9587724208831787
Validation loss: 2.4141076457115913

Epoch: 6| Step: 13
Training loss: 1.9004838466644287
Validation loss: 2.406130744564918

Epoch: 275| Step: 0
Training loss: 2.008946418762207
Validation loss: 2.398577954179497

Epoch: 6| Step: 1
Training loss: 2.209517002105713
Validation loss: 2.401726274080174

Epoch: 6| Step: 2
Training loss: 2.1317458152770996
Validation loss: 2.408434875549809

Epoch: 6| Step: 3
Training loss: 3.26450252532959
Validation loss: 2.3845728725515385

Epoch: 6| Step: 4
Training loss: 2.552452325820923
Validation loss: 2.361926204414778

Epoch: 6| Step: 5
Training loss: 2.8742551803588867
Validation loss: 2.3553146521250405

Epoch: 6| Step: 6
Training loss: 2.5595576763153076
Validation loss: 2.3244297094242548

Epoch: 6| Step: 7
Training loss: 2.24741792678833
Validation loss: 2.322322955695532

Epoch: 6| Step: 8
Training loss: 2.4024789333343506
Validation loss: 2.3198087446151243

Epoch: 6| Step: 9
Training loss: 2.450666666030884
Validation loss: 2.32327425864435

Epoch: 6| Step: 10
Training loss: 2.3257365226745605
Validation loss: 2.3277988690201954

Epoch: 6| Step: 11
Training loss: 2.9216015338897705
Validation loss: 2.328006608511812

Epoch: 6| Step: 12
Training loss: 2.235739231109619
Validation loss: 2.336657028044424

Epoch: 6| Step: 13
Training loss: 2.8285510540008545
Validation loss: 2.3346787806480163

Epoch: 276| Step: 0
Training loss: 2.4400312900543213
Validation loss: 2.333486295515491

Epoch: 6| Step: 1
Training loss: 2.1963164806365967
Validation loss: 2.3241838896146385

Epoch: 6| Step: 2
Training loss: 3.1059587001800537
Validation loss: 2.3304296052584084

Epoch: 6| Step: 3
Training loss: 2.6619441509246826
Validation loss: 2.3172407355359805

Epoch: 6| Step: 4
Training loss: 2.8450582027435303
Validation loss: 2.3225663656829507

Epoch: 6| Step: 5
Training loss: 2.178309679031372
Validation loss: 2.315005648520685

Epoch: 6| Step: 6
Training loss: 2.715615749359131
Validation loss: 2.3100289503733316

Epoch: 6| Step: 7
Training loss: 2.045459508895874
Validation loss: 2.3045854568481445

Epoch: 6| Step: 8
Training loss: 2.5998072624206543
Validation loss: 2.3132940312867523

Epoch: 6| Step: 9
Training loss: 2.5420937538146973
Validation loss: 2.3086656473016225

Epoch: 6| Step: 10
Training loss: 2.9390435218811035
Validation loss: 2.314369329842188

Epoch: 6| Step: 11
Training loss: 1.4773744344711304
Validation loss: 2.3259726955044653

Epoch: 6| Step: 12
Training loss: 2.8557653427124023
Validation loss: 2.3307498039737826

Epoch: 6| Step: 13
Training loss: 1.837888479232788
Validation loss: 2.3515525748652797

Epoch: 277| Step: 0
Training loss: 2.2571027278900146
Validation loss: 2.3586698168067524

Epoch: 6| Step: 1
Training loss: 2.9868927001953125
Validation loss: 2.3704190818212365

Epoch: 6| Step: 2
Training loss: 2.2048025131225586
Validation loss: 2.401271830322922

Epoch: 6| Step: 3
Training loss: 2.642713785171509
Validation loss: 2.425297288484471

Epoch: 6| Step: 4
Training loss: 2.1978492736816406
Validation loss: 2.403272677493352

Epoch: 6| Step: 5
Training loss: 2.27821683883667
Validation loss: 2.39852342554318

Epoch: 6| Step: 6
Training loss: 2.0142605304718018
Validation loss: 2.4134814098317134

Epoch: 6| Step: 7
Training loss: 2.192214250564575
Validation loss: 2.3739341946058374

Epoch: 6| Step: 8
Training loss: 2.7530064582824707
Validation loss: 2.3753309429332776

Epoch: 6| Step: 9
Training loss: 2.9708855152130127
Validation loss: 2.3480756308442805

Epoch: 6| Step: 10
Training loss: 2.0069637298583984
Validation loss: 2.330461945585025

Epoch: 6| Step: 11
Training loss: 2.873955249786377
Validation loss: 2.317302370584139

Epoch: 6| Step: 12
Training loss: 2.8250017166137695
Validation loss: 2.311530436238935

Epoch: 6| Step: 13
Training loss: 2.7998945713043213
Validation loss: 2.2877670359867874

Epoch: 278| Step: 0
Training loss: 2.5859241485595703
Validation loss: 2.2885131502664215

Epoch: 6| Step: 1
Training loss: 2.2305731773376465
Validation loss: 2.295255914811165

Epoch: 6| Step: 2
Training loss: 2.6362533569335938
Validation loss: 2.300592855740619

Epoch: 6| Step: 3
Training loss: 3.2091431617736816
Validation loss: 2.2951170731616277

Epoch: 6| Step: 4
Training loss: 2.8332927227020264
Validation loss: 2.294957532677599

Epoch: 6| Step: 5
Training loss: 2.012193202972412
Validation loss: 2.3044122008867163

Epoch: 6| Step: 6
Training loss: 3.0048747062683105
Validation loss: 2.2929125370517855

Epoch: 6| Step: 7
Training loss: 1.9984787702560425
Validation loss: 2.2906274667350193

Epoch: 6| Step: 8
Training loss: 1.8188574314117432
Validation loss: 2.2907344615587624

Epoch: 6| Step: 9
Training loss: 1.7392678260803223
Validation loss: 2.3019831924028296

Epoch: 6| Step: 10
Training loss: 2.18538236618042
Validation loss: 2.3052557335104993

Epoch: 6| Step: 11
Training loss: 2.8534021377563477
Validation loss: 2.3294328130701536

Epoch: 6| Step: 12
Training loss: 2.8432483673095703
Validation loss: 2.338298641225343

Epoch: 6| Step: 13
Training loss: 3.059011459350586
Validation loss: 2.342919575270786

Epoch: 279| Step: 0
Training loss: 2.1289637088775635
Validation loss: 2.3699191052426576

Epoch: 6| Step: 1
Training loss: 3.3931398391723633
Validation loss: 2.3922038027035293

Epoch: 6| Step: 2
Training loss: 2.017788887023926
Validation loss: 2.418712080165904

Epoch: 6| Step: 3
Training loss: 3.435030937194824
Validation loss: 2.4268073907462497

Epoch: 6| Step: 4
Training loss: 2.7169365882873535
Validation loss: 2.421801613223168

Epoch: 6| Step: 5
Training loss: 2.9706192016601562
Validation loss: 2.412070156425558

Epoch: 6| Step: 6
Training loss: 2.822807788848877
Validation loss: 2.404221201455721

Epoch: 6| Step: 7
Training loss: 1.982011318206787
Validation loss: 2.3816665757086968

Epoch: 6| Step: 8
Training loss: 2.420578718185425
Validation loss: 2.3634294156105287

Epoch: 6| Step: 9
Training loss: 2.378927707672119
Validation loss: 2.3434888855103524

Epoch: 6| Step: 10
Training loss: 2.22113299369812
Validation loss: 2.32498142796178

Epoch: 6| Step: 11
Training loss: 2.193896532058716
Validation loss: 2.3214047365291144

Epoch: 6| Step: 12
Training loss: 1.7349286079406738
Validation loss: 2.3175767108958256

Epoch: 6| Step: 13
Training loss: 2.944580316543579
Validation loss: 2.304263263620356

Epoch: 280| Step: 0
Training loss: 1.990755558013916
Validation loss: 2.3194454575097687

Epoch: 6| Step: 1
Training loss: 2.9985384941101074
Validation loss: 2.323547327390281

Epoch: 6| Step: 2
Training loss: 1.2698004245758057
Validation loss: 2.322506102182532

Epoch: 6| Step: 3
Training loss: 3.857501983642578
Validation loss: 2.3119503836477957

Epoch: 6| Step: 4
Training loss: 2.5573606491088867
Validation loss: 2.3270005103080504

Epoch: 6| Step: 5
Training loss: 2.0636441707611084
Validation loss: 2.3266360234188777

Epoch: 6| Step: 6
Training loss: 1.9869657754898071
Validation loss: 2.326578160767914

Epoch: 6| Step: 7
Training loss: 1.855233907699585
Validation loss: 2.3304530676975044

Epoch: 6| Step: 8
Training loss: 2.7071633338928223
Validation loss: 2.329675224519545

Epoch: 6| Step: 9
Training loss: 2.7603516578674316
Validation loss: 2.3322381947630193

Epoch: 6| Step: 10
Training loss: 2.333026885986328
Validation loss: 2.3270464097299883

Epoch: 6| Step: 11
Training loss: 3.066237688064575
Validation loss: 2.3287988785774476

Epoch: 6| Step: 12
Training loss: 2.554541826248169
Validation loss: 2.3288314598862843

Epoch: 6| Step: 13
Training loss: 2.804655075073242
Validation loss: 2.340682778307187

Epoch: 281| Step: 0
Training loss: 2.8179965019226074
Validation loss: 2.3430248460462018

Epoch: 6| Step: 1
Training loss: 2.018862724304199
Validation loss: 2.3275101287390596

Epoch: 6| Step: 2
Training loss: 2.7979161739349365
Validation loss: 2.3323661383762153

Epoch: 6| Step: 3
Training loss: 2.624344825744629
Validation loss: 2.328008028768724

Epoch: 6| Step: 4
Training loss: 2.578296661376953
Validation loss: 2.317918840275016

Epoch: 6| Step: 5
Training loss: 3.10245418548584
Validation loss: 2.3299127342880412

Epoch: 6| Step: 6
Training loss: 2.0490975379943848
Validation loss: 2.326942410520328

Epoch: 6| Step: 7
Training loss: 2.04219913482666
Validation loss: 2.335618024231285

Epoch: 6| Step: 8
Training loss: 2.2209439277648926
Validation loss: 2.3440340436914915

Epoch: 6| Step: 9
Training loss: 2.2065062522888184
Validation loss: 2.3473648204598376

Epoch: 6| Step: 10
Training loss: 2.3023664951324463
Validation loss: 2.3520649594645344

Epoch: 6| Step: 11
Training loss: 2.196627140045166
Validation loss: 2.361155456112277

Epoch: 6| Step: 12
Training loss: 2.5637805461883545
Validation loss: 2.368134173013831

Epoch: 6| Step: 13
Training loss: 3.3012208938598633
Validation loss: 2.3471925309909287

Epoch: 282| Step: 0
Training loss: 3.031283378601074
Validation loss: 2.337218334597926

Epoch: 6| Step: 1
Training loss: 2.0827267169952393
Validation loss: 2.3261003045625586

Epoch: 6| Step: 2
Training loss: 1.9603185653686523
Validation loss: 2.3168134843149493

Epoch: 6| Step: 3
Training loss: 2.87996768951416
Validation loss: 2.3088626220662105

Epoch: 6| Step: 4
Training loss: 2.6698925495147705
Validation loss: 2.3120097934558825

Epoch: 6| Step: 5
Training loss: 2.335998296737671
Validation loss: 2.296896549963182

Epoch: 6| Step: 6
Training loss: 3.2445037364959717
Validation loss: 2.303395535356255

Epoch: 6| Step: 7
Training loss: 1.9998753070831299
Validation loss: 2.29725549297948

Epoch: 6| Step: 8
Training loss: 2.414614200592041
Validation loss: 2.2918149732774302

Epoch: 6| Step: 9
Training loss: 2.7413625717163086
Validation loss: 2.2891686449768724

Epoch: 6| Step: 10
Training loss: 1.580975890159607
Validation loss: 2.296118205593478

Epoch: 6| Step: 11
Training loss: 2.7877416610717773
Validation loss: 2.295948756638394

Epoch: 6| Step: 12
Training loss: 2.2345809936523438
Validation loss: 2.3097378643610145

Epoch: 6| Step: 13
Training loss: 2.7920541763305664
Validation loss: 2.309835131450366

Epoch: 283| Step: 0
Training loss: 2.353179693222046
Validation loss: 2.3292407246046167

Epoch: 6| Step: 1
Training loss: 2.3657140731811523
Validation loss: 2.3486445001376572

Epoch: 6| Step: 2
Training loss: 2.1699135303497314
Validation loss: 2.3583105379535305

Epoch: 6| Step: 3
Training loss: 2.5984764099121094
Validation loss: 2.389778667880643

Epoch: 6| Step: 4
Training loss: 2.7219834327697754
Validation loss: 2.419015069161692

Epoch: 6| Step: 5
Training loss: 1.9034442901611328
Validation loss: 2.3919239121098674

Epoch: 6| Step: 6
Training loss: 2.1363110542297363
Validation loss: 2.3864374827313166

Epoch: 6| Step: 7
Training loss: 2.89029598236084
Validation loss: 2.3840599495877504

Epoch: 6| Step: 8
Training loss: 2.4181101322174072
Validation loss: 2.3796048805277836

Epoch: 6| Step: 9
Training loss: 3.007380247116089
Validation loss: 2.3575756626744426

Epoch: 6| Step: 10
Training loss: 2.6034908294677734
Validation loss: 2.336971516250282

Epoch: 6| Step: 11
Training loss: 2.904209852218628
Validation loss: 2.3423669722772416

Epoch: 6| Step: 12
Training loss: 2.251999855041504
Validation loss: 2.3292822645556543

Epoch: 6| Step: 13
Training loss: 1.9993964433670044
Validation loss: 2.321211511088956

Epoch: 284| Step: 0
Training loss: 1.851552963256836
Validation loss: 2.2995557477397304

Epoch: 6| Step: 1
Training loss: 2.2435734272003174
Validation loss: 2.29662642171306

Epoch: 6| Step: 2
Training loss: 3.229560375213623
Validation loss: 2.292430211138982

Epoch: 6| Step: 3
Training loss: 2.026934862136841
Validation loss: 2.291450910670783

Epoch: 6| Step: 4
Training loss: 2.9739110469818115
Validation loss: 2.2858373247167116

Epoch: 6| Step: 5
Training loss: 1.9893134832382202
Validation loss: 2.2903427154787126

Epoch: 6| Step: 6
Training loss: 2.0866332054138184
Validation loss: 2.3033867805234847

Epoch: 6| Step: 7
Training loss: 2.5409677028656006
Validation loss: 2.2979339079190324

Epoch: 6| Step: 8
Training loss: 2.983372688293457
Validation loss: 2.300624862793953

Epoch: 6| Step: 9
Training loss: 2.749950408935547
Validation loss: 2.301181739376437

Epoch: 6| Step: 10
Training loss: 2.9017114639282227
Validation loss: 2.308722129432104

Epoch: 6| Step: 11
Training loss: 2.000356435775757
Validation loss: 2.3103772363355084

Epoch: 6| Step: 12
Training loss: 2.868347644805908
Validation loss: 2.323221580956572

Epoch: 6| Step: 13
Training loss: 1.830474853515625
Validation loss: 2.32618990764823

Epoch: 285| Step: 0
Training loss: 2.730717182159424
Validation loss: 2.338909743934549

Epoch: 6| Step: 1
Training loss: 2.810577630996704
Validation loss: 2.3464509184642504

Epoch: 6| Step: 2
Training loss: 1.8947409391403198
Validation loss: 2.3484549522399902

Epoch: 6| Step: 3
Training loss: 1.9092459678649902
Validation loss: 2.374391773695587

Epoch: 6| Step: 4
Training loss: 2.6147775650024414
Validation loss: 2.408369687295729

Epoch: 6| Step: 5
Training loss: 2.759760856628418
Validation loss: 2.3767198824113414

Epoch: 6| Step: 6
Training loss: 2.3772268295288086
Validation loss: 2.3506996682895127

Epoch: 6| Step: 7
Training loss: 1.9398014545440674
Validation loss: 2.3724459883987263

Epoch: 6| Step: 8
Training loss: 2.7565619945526123
Validation loss: 2.354874436573316

Epoch: 6| Step: 9
Training loss: 1.9484330415725708
Validation loss: 2.3552383069069154

Epoch: 6| Step: 10
Training loss: 3.043015718460083
Validation loss: 2.3322171600916053

Epoch: 6| Step: 11
Training loss: 2.692831039428711
Validation loss: 2.3237535043429305

Epoch: 6| Step: 12
Training loss: 2.50719952583313
Validation loss: 2.3205940031236216

Epoch: 6| Step: 13
Training loss: 2.542025089263916
Validation loss: 2.3055125077565513

Epoch: 286| Step: 0
Training loss: 2.404686450958252
Validation loss: 2.3092457402137017

Epoch: 6| Step: 1
Training loss: 2.243368148803711
Validation loss: 2.2918426759781374

Epoch: 6| Step: 2
Training loss: 2.747868061065674
Validation loss: 2.295194648927258

Epoch: 6| Step: 3
Training loss: 2.756294012069702
Validation loss: 2.2827104983791227

Epoch: 6| Step: 4
Training loss: 2.19700288772583
Validation loss: 2.305542189587829

Epoch: 6| Step: 5
Training loss: 2.266730785369873
Validation loss: 2.3050074064603416

Epoch: 6| Step: 6
Training loss: 2.7749428749084473
Validation loss: 2.3111348921252834

Epoch: 6| Step: 7
Training loss: 2.9135358333587646
Validation loss: 2.298444176232943

Epoch: 6| Step: 8
Training loss: 2.521829128265381
Validation loss: 2.307869952212098

Epoch: 6| Step: 9
Training loss: 2.563154458999634
Validation loss: 2.3114668092420025

Epoch: 6| Step: 10
Training loss: 1.7825993299484253
Validation loss: 2.3128046015257477

Epoch: 6| Step: 11
Training loss: 2.5275583267211914
Validation loss: 2.3030736933472338

Epoch: 6| Step: 12
Training loss: 2.8691561222076416
Validation loss: 2.315652157670708

Epoch: 6| Step: 13
Training loss: 1.5309778451919556
Validation loss: 2.3365035185249905

Epoch: 287| Step: 0
Training loss: 2.5097172260284424
Validation loss: 2.354447777553271

Epoch: 6| Step: 1
Training loss: 2.445190191268921
Validation loss: 2.3710287988826795

Epoch: 6| Step: 2
Training loss: 1.7179187536239624
Validation loss: 2.3949454676720405

Epoch: 6| Step: 3
Training loss: 2.64096736907959
Validation loss: 2.39288765640669

Epoch: 6| Step: 4
Training loss: 2.9057774543762207
Validation loss: 2.3902742067972818

Epoch: 6| Step: 5
Training loss: 2.4188332557678223
Validation loss: 2.3626359572974582

Epoch: 6| Step: 6
Training loss: 2.25201416015625
Validation loss: 2.3707297950662594

Epoch: 6| Step: 7
Training loss: 2.3610033988952637
Validation loss: 2.350533449521629

Epoch: 6| Step: 8
Training loss: 2.3196239471435547
Validation loss: 2.3495663930011053

Epoch: 6| Step: 9
Training loss: 2.459559440612793
Validation loss: 2.329698557494789

Epoch: 6| Step: 10
Training loss: 2.431567668914795
Validation loss: 2.340891045908774

Epoch: 6| Step: 11
Training loss: 2.684626340866089
Validation loss: 2.3321440604425248

Epoch: 6| Step: 12
Training loss: 2.617400646209717
Validation loss: 2.3250481646548034

Epoch: 6| Step: 13
Training loss: 2.742525815963745
Validation loss: 2.316477503827823

Epoch: 288| Step: 0
Training loss: 2.635253429412842
Validation loss: 2.309526515263383

Epoch: 6| Step: 1
Training loss: 2.4662466049194336
Validation loss: 2.300584613635976

Epoch: 6| Step: 2
Training loss: 2.334177255630493
Validation loss: 2.2938306305998113

Epoch: 6| Step: 3
Training loss: 2.7575387954711914
Validation loss: 2.2898766251020533

Epoch: 6| Step: 4
Training loss: 2.429046154022217
Validation loss: 2.291383015212192

Epoch: 6| Step: 5
Training loss: 2.2739171981811523
Validation loss: 2.300590540773125

Epoch: 6| Step: 6
Training loss: 2.7963647842407227
Validation loss: 2.3022237670037056

Epoch: 6| Step: 7
Training loss: 2.402660369873047
Validation loss: 2.3063878782333864

Epoch: 6| Step: 8
Training loss: 2.2320961952209473
Validation loss: 2.3140308498054423

Epoch: 6| Step: 9
Training loss: 1.752187728881836
Validation loss: 2.310949907507948

Epoch: 6| Step: 10
Training loss: 1.905623197555542
Validation loss: 2.343258629563034

Epoch: 6| Step: 11
Training loss: 2.4419665336608887
Validation loss: 2.3486636582241265

Epoch: 6| Step: 12
Training loss: 3.227856397628784
Validation loss: 2.347027476115893

Epoch: 6| Step: 13
Training loss: 2.960146903991699
Validation loss: 2.3372105398485736

Epoch: 289| Step: 0
Training loss: 1.6612168550491333
Validation loss: 2.3201546463915097

Epoch: 6| Step: 1
Training loss: 2.7899131774902344
Validation loss: 2.2983435353925152

Epoch: 6| Step: 2
Training loss: 1.5128841400146484
Validation loss: 2.2845709682792745

Epoch: 6| Step: 3
Training loss: 2.4896187782287598
Validation loss: 2.287037023933985

Epoch: 6| Step: 4
Training loss: 2.444932222366333
Validation loss: 2.2970410521312425

Epoch: 6| Step: 5
Training loss: 2.142181396484375
Validation loss: 2.3179894390926568

Epoch: 6| Step: 6
Training loss: 2.6046700477600098
Validation loss: 2.3159295615329536

Epoch: 6| Step: 7
Training loss: 2.8691563606262207
Validation loss: 2.312601935478949

Epoch: 6| Step: 8
Training loss: 3.725191354751587
Validation loss: 2.3217138859533493

Epoch: 6| Step: 9
Training loss: 2.5099973678588867
Validation loss: 2.3289563296943583

Epoch: 6| Step: 10
Training loss: 2.9655728340148926
Validation loss: 2.336136071912704

Epoch: 6| Step: 11
Training loss: 2.258430242538452
Validation loss: 2.341411675176313

Epoch: 6| Step: 12
Training loss: 1.9426008462905884
Validation loss: 2.3392548432914158

Epoch: 6| Step: 13
Training loss: 2.4577558040618896
Validation loss: 2.3416243125033636

Epoch: 290| Step: 0
Training loss: 2.8470664024353027
Validation loss: 2.3257765282866774

Epoch: 6| Step: 1
Training loss: 2.49771785736084
Validation loss: 2.3366960812640447

Epoch: 6| Step: 2
Training loss: 1.8932740688323975
Validation loss: 2.3462241849591656

Epoch: 6| Step: 3
Training loss: 2.208540201187134
Validation loss: 2.3202350472891204

Epoch: 6| Step: 4
Training loss: 2.1662166118621826
Validation loss: 2.3236531416575112

Epoch: 6| Step: 5
Training loss: 2.077380418777466
Validation loss: 2.3123177200235348

Epoch: 6| Step: 6
Training loss: 2.8348593711853027
Validation loss: 2.2947254667999926

Epoch: 6| Step: 7
Training loss: 3.6000359058380127
Validation loss: 2.2979242955484698

Epoch: 6| Step: 8
Training loss: 2.4158730506896973
Validation loss: 2.2941564641973025

Epoch: 6| Step: 9
Training loss: 2.686460494995117
Validation loss: 2.279683405353177

Epoch: 6| Step: 10
Training loss: 2.570401191711426
Validation loss: 2.2822609896300943

Epoch: 6| Step: 11
Training loss: 1.9090290069580078
Validation loss: 2.2940889994303384

Epoch: 6| Step: 12
Training loss: 2.0804357528686523
Validation loss: 2.291417461569591

Epoch: 6| Step: 13
Training loss: 2.597059965133667
Validation loss: 2.3058104591984905

Epoch: 291| Step: 0
Training loss: 3.0700936317443848
Validation loss: 2.2963657686787267

Epoch: 6| Step: 1
Training loss: 1.9736542701721191
Validation loss: 2.3334421214237007

Epoch: 6| Step: 2
Training loss: 2.42158842086792
Validation loss: 2.3444070969858477

Epoch: 6| Step: 3
Training loss: 2.2834858894348145
Validation loss: 2.337292343057612

Epoch: 6| Step: 4
Training loss: 2.7105817794799805
Validation loss: 2.341496052280549

Epoch: 6| Step: 5
Training loss: 3.233747720718384
Validation loss: 2.3217493257214947

Epoch: 6| Step: 6
Training loss: 2.6771116256713867
Validation loss: 2.328129865789926

Epoch: 6| Step: 7
Training loss: 2.0354599952697754
Validation loss: 2.3198983900008665

Epoch: 6| Step: 8
Training loss: 1.5619616508483887
Validation loss: 2.3350075291049097

Epoch: 6| Step: 9
Training loss: 2.2996912002563477
Validation loss: 2.3270982478254583

Epoch: 6| Step: 10
Training loss: 2.4455230236053467
Validation loss: 2.3175206620206117

Epoch: 6| Step: 11
Training loss: 2.556687593460083
Validation loss: 2.3162419206352642

Epoch: 6| Step: 12
Training loss: 2.180860996246338
Validation loss: 2.3111093428827103

Epoch: 6| Step: 13
Training loss: 2.7952167987823486
Validation loss: 2.306090103682651

Epoch: 292| Step: 0
Training loss: 2.997032880783081
Validation loss: 2.2984468462646648

Epoch: 6| Step: 1
Training loss: 2.163003444671631
Validation loss: 2.300410856482803

Epoch: 6| Step: 2
Training loss: 2.6625537872314453
Validation loss: 2.285769663831239

Epoch: 6| Step: 3
Training loss: 1.9788355827331543
Validation loss: 2.29225633734016

Epoch: 6| Step: 4
Training loss: 2.2422916889190674
Validation loss: 2.302535472377654

Epoch: 6| Step: 5
Training loss: 2.9634172916412354
Validation loss: 2.3031243611407537

Epoch: 6| Step: 6
Training loss: 2.5218918323516846
Validation loss: 2.3085165331440587

Epoch: 6| Step: 7
Training loss: 2.340174436569214
Validation loss: 2.31340859013219

Epoch: 6| Step: 8
Training loss: 2.7915685176849365
Validation loss: 2.330412713430261

Epoch: 6| Step: 9
Training loss: 1.6820327043533325
Validation loss: 2.335185581637967

Epoch: 6| Step: 10
Training loss: 2.4806978702545166
Validation loss: 2.334426156936153

Epoch: 6| Step: 11
Training loss: 2.4109458923339844
Validation loss: 2.3292602646735405

Epoch: 6| Step: 12
Training loss: 2.4708070755004883
Validation loss: 2.3342426284666984

Epoch: 6| Step: 13
Training loss: 2.327313184738159
Validation loss: 2.333055488524898

Epoch: 293| Step: 0
Training loss: 2.0359866619110107
Validation loss: 2.3444839882594284

Epoch: 6| Step: 1
Training loss: 1.5594277381896973
Validation loss: 2.349533106691094

Epoch: 6| Step: 2
Training loss: 2.6362195014953613
Validation loss: 2.346023764661563

Epoch: 6| Step: 3
Training loss: 2.2817397117614746
Validation loss: 2.3369140201999294

Epoch: 6| Step: 4
Training loss: 1.7650798559188843
Validation loss: 2.3169785289354223

Epoch: 6| Step: 5
Training loss: 2.98636794090271
Validation loss: 2.3074471950531006

Epoch: 6| Step: 6
Training loss: 2.0386741161346436
Validation loss: 2.2983745503169235

Epoch: 6| Step: 7
Training loss: 2.811166763305664
Validation loss: 2.2903679109388784

Epoch: 6| Step: 8
Training loss: 2.566117286682129
Validation loss: 2.2928139086692565

Epoch: 6| Step: 9
Training loss: 3.334911584854126
Validation loss: 2.302636957937671

Epoch: 6| Step: 10
Training loss: 2.53957462310791
Validation loss: 2.3001351869234474

Epoch: 6| Step: 11
Training loss: 2.6494903564453125
Validation loss: 2.2896831497069328

Epoch: 6| Step: 12
Training loss: 2.544546127319336
Validation loss: 2.3044872348026564

Epoch: 6| Step: 13
Training loss: 2.4412033557891846
Validation loss: 2.3091045759057485

Epoch: 294| Step: 0
Training loss: 2.8997557163238525
Validation loss: 2.317043945353518

Epoch: 6| Step: 1
Training loss: 1.794020652770996
Validation loss: 2.31623250438321

Epoch: 6| Step: 2
Training loss: 2.172300338745117
Validation loss: 2.3107771412018807

Epoch: 6| Step: 3
Training loss: 2.573927402496338
Validation loss: 2.3264369631326325

Epoch: 6| Step: 4
Training loss: 2.7417593002319336
Validation loss: 2.310808043326101

Epoch: 6| Step: 5
Training loss: 2.1931424140930176
Validation loss: 2.301132925095097

Epoch: 6| Step: 6
Training loss: 2.68455171585083
Validation loss: 2.3200091136399137

Epoch: 6| Step: 7
Training loss: 2.1298604011535645
Validation loss: 2.3095310964891986

Epoch: 6| Step: 8
Training loss: 3.269896984100342
Validation loss: 2.297057610686107

Epoch: 6| Step: 9
Training loss: 2.1247191429138184
Validation loss: 2.3137773954740135

Epoch: 6| Step: 10
Training loss: 2.9359047412872314
Validation loss: 2.3014519009538876

Epoch: 6| Step: 11
Training loss: 2.8170933723449707
Validation loss: 2.3117932042767926

Epoch: 6| Step: 12
Training loss: 1.820944905281067
Validation loss: 2.302211582019765

Epoch: 6| Step: 13
Training loss: 1.3981279134750366
Validation loss: 2.305389817043017

Epoch: 295| Step: 0
Training loss: 2.6654343605041504
Validation loss: 2.3368899488961823

Epoch: 6| Step: 1
Training loss: 2.220304012298584
Validation loss: 2.3990793510149886

Epoch: 6| Step: 2
Training loss: 2.141911029815674
Validation loss: 2.4395359023924796

Epoch: 6| Step: 3
Training loss: 3.0471510887145996
Validation loss: 2.4471950723278906

Epoch: 6| Step: 4
Training loss: 1.7939356565475464
Validation loss: 2.4541786255375033

Epoch: 6| Step: 5
Training loss: 2.3526997566223145
Validation loss: 2.454742057349092

Epoch: 6| Step: 6
Training loss: 2.3580424785614014
Validation loss: 2.3982275121955463

Epoch: 6| Step: 7
Training loss: 1.9827399253845215
Validation loss: 2.3610880759454544

Epoch: 6| Step: 8
Training loss: 1.999649167060852
Validation loss: 2.3216275989368396

Epoch: 6| Step: 9
Training loss: 2.7944741249084473
Validation loss: 2.3031736855865805

Epoch: 6| Step: 10
Training loss: 2.848175048828125
Validation loss: 2.294562796110748

Epoch: 6| Step: 11
Training loss: 2.6954421997070312
Validation loss: 2.2917451243246756

Epoch: 6| Step: 12
Training loss: 2.980666399002075
Validation loss: 2.2705939610799155

Epoch: 6| Step: 13
Training loss: 2.4588329792022705
Validation loss: 2.266518264688471

Epoch: 296| Step: 0
Training loss: 2.316589593887329
Validation loss: 2.267273303001158

Epoch: 6| Step: 1
Training loss: 2.483541965484619
Validation loss: 2.2677515424707884

Epoch: 6| Step: 2
Training loss: 1.8740904331207275
Validation loss: 2.2697631236045592

Epoch: 6| Step: 3
Training loss: 1.961020827293396
Validation loss: 2.2752035920337965

Epoch: 6| Step: 4
Training loss: 3.072875499725342
Validation loss: 2.2656839970619447

Epoch: 6| Step: 5
Training loss: 2.6701738834381104
Validation loss: 2.2715873051715154

Epoch: 6| Step: 6
Training loss: 2.157620906829834
Validation loss: 2.2886212615556616

Epoch: 6| Step: 7
Training loss: 1.7580046653747559
Validation loss: 2.286093081197431

Epoch: 6| Step: 8
Training loss: 2.758970260620117
Validation loss: 2.292990320472307

Epoch: 6| Step: 9
Training loss: 2.8252334594726562
Validation loss: 2.3034635436150337

Epoch: 6| Step: 10
Training loss: 2.444920063018799
Validation loss: 2.3002035002554617

Epoch: 6| Step: 11
Training loss: 3.431001663208008
Validation loss: 2.30222394645855

Epoch: 6| Step: 12
Training loss: 1.7954676151275635
Validation loss: 2.3093656801408335

Epoch: 6| Step: 13
Training loss: 2.819626569747925
Validation loss: 2.3177832659854682

Epoch: 297| Step: 0
Training loss: 2.2825891971588135
Validation loss: 2.3368407500687467

Epoch: 6| Step: 1
Training loss: 3.3110837936401367
Validation loss: 2.3182055027254167

Epoch: 6| Step: 2
Training loss: 2.72055983543396
Validation loss: 2.3120915556466706

Epoch: 6| Step: 3
Training loss: 2.092115879058838
Validation loss: 2.316060645605928

Epoch: 6| Step: 4
Training loss: 2.5894060134887695
Validation loss: 2.305450418944

Epoch: 6| Step: 5
Training loss: 1.9523663520812988
Validation loss: 2.290806875433973

Epoch: 6| Step: 6
Training loss: 2.5365817546844482
Validation loss: 2.2862114060309624

Epoch: 6| Step: 7
Training loss: 2.514050006866455
Validation loss: 2.2799551743333057

Epoch: 6| Step: 8
Training loss: 2.0510034561157227
Validation loss: 2.2805679126452376

Epoch: 6| Step: 9
Training loss: 2.1141810417175293
Validation loss: 2.293682080443187

Epoch: 6| Step: 10
Training loss: 2.3744425773620605
Validation loss: 2.2897782197562595

Epoch: 6| Step: 11
Training loss: 2.5073401927948
Validation loss: 2.3020980870851906

Epoch: 6| Step: 12
Training loss: 2.9851582050323486
Validation loss: 2.302236214760811

Epoch: 6| Step: 13
Training loss: 2.0051257610321045
Validation loss: 2.300592517340055

Epoch: 298| Step: 0
Training loss: 2.1417582035064697
Validation loss: 2.305789486054451

Epoch: 6| Step: 1
Training loss: 2.087986469268799
Validation loss: 2.3024924955060406

Epoch: 6| Step: 2
Training loss: 2.1465301513671875
Validation loss: 2.3171743192980365

Epoch: 6| Step: 3
Training loss: 2.585078477859497
Validation loss: 2.315289428157191

Epoch: 6| Step: 4
Training loss: 1.6397395133972168
Validation loss: 2.3272982925497074

Epoch: 6| Step: 5
Training loss: 2.3977160453796387
Validation loss: 2.316673796664002

Epoch: 6| Step: 6
Training loss: 2.8851144313812256
Validation loss: 2.315068673062068

Epoch: 6| Step: 7
Training loss: 2.3633012771606445
Validation loss: 2.337055157589656

Epoch: 6| Step: 8
Training loss: 3.3504011631011963
Validation loss: 2.3316700125253327

Epoch: 6| Step: 9
Training loss: 1.9226114749908447
Validation loss: 2.3323526638810352

Epoch: 6| Step: 10
Training loss: 2.0867345333099365
Validation loss: 2.3187202228012906

Epoch: 6| Step: 11
Training loss: 2.826458215713501
Validation loss: 2.3322985531181417

Epoch: 6| Step: 12
Training loss: 2.9140167236328125
Validation loss: 2.3393615420146654

Epoch: 6| Step: 13
Training loss: 2.669372320175171
Validation loss: 2.321076585400489

Epoch: 299| Step: 0
Training loss: 2.205897331237793
Validation loss: 2.3069608237153743

Epoch: 6| Step: 1
Training loss: 2.78102970123291
Validation loss: 2.292072937052737

Epoch: 6| Step: 2
Training loss: 1.9727222919464111
Validation loss: 2.281378420450354

Epoch: 6| Step: 3
Training loss: 2.415907859802246
Validation loss: 2.295556083802254

Epoch: 6| Step: 4
Training loss: 2.6715824604034424
Validation loss: 2.286042413403911

Epoch: 6| Step: 5
Training loss: 2.4670777320861816
Validation loss: 2.266070927343061

Epoch: 6| Step: 6
Training loss: 3.0491034984588623
Validation loss: 2.25529739164537

Epoch: 6| Step: 7
Training loss: 2.197049856185913
Validation loss: 2.278790141946526

Epoch: 6| Step: 8
Training loss: 1.9517855644226074
Validation loss: 2.2837113975196757

Epoch: 6| Step: 9
Training loss: 1.8205231428146362
Validation loss: 2.30327602612075

Epoch: 6| Step: 10
Training loss: 2.171142816543579
Validation loss: 2.305000953776862

Epoch: 6| Step: 11
Training loss: 3.2367453575134277
Validation loss: 2.3205410459990143

Epoch: 6| Step: 12
Training loss: 2.530829429626465
Validation loss: 2.341467288232619

Epoch: 6| Step: 13
Training loss: 2.714284658432007
Validation loss: 2.334962001410864

Epoch: 300| Step: 0
Training loss: 2.7327327728271484
Validation loss: 2.3265931580656316

Epoch: 6| Step: 1
Training loss: 2.5711917877197266
Validation loss: 2.3176337044726134

Epoch: 6| Step: 2
Training loss: 2.1774778366088867
Validation loss: 2.297872071625084

Epoch: 6| Step: 3
Training loss: 2.3134543895721436
Validation loss: 2.2801411664614113

Epoch: 6| Step: 4
Training loss: 2.005061149597168
Validation loss: 2.2802567584540254

Epoch: 6| Step: 5
Training loss: 2.0373330116271973
Validation loss: 2.26758502375695

Epoch: 6| Step: 6
Training loss: 2.951181411743164
Validation loss: 2.2779316927797053

Epoch: 6| Step: 7
Training loss: 2.137113332748413
Validation loss: 2.2760201602853756

Epoch: 6| Step: 8
Training loss: 2.820003032684326
Validation loss: 2.2747467307634253

Epoch: 6| Step: 9
Training loss: 2.5778043270111084
Validation loss: 2.280596144737736

Epoch: 6| Step: 10
Training loss: 1.857630729675293
Validation loss: 2.2912357327758626

Epoch: 6| Step: 11
Training loss: 2.819389820098877
Validation loss: 2.2889529940902547

Epoch: 6| Step: 12
Training loss: 2.3215770721435547
Validation loss: 2.3017546669129403

Epoch: 6| Step: 13
Training loss: 2.8836934566497803
Validation loss: 2.3145176236347487

Epoch: 301| Step: 0
Training loss: 2.5887694358825684
Validation loss: 2.3075444365060456

Epoch: 6| Step: 1
Training loss: 2.248056411743164
Validation loss: 2.314215621640605

Epoch: 6| Step: 2
Training loss: 1.705094337463379
Validation loss: 2.303564712565432

Epoch: 6| Step: 3
Training loss: 1.8494879007339478
Validation loss: 2.3020734197349957

Epoch: 6| Step: 4
Training loss: 2.686680555343628
Validation loss: 2.2894647352157103

Epoch: 6| Step: 5
Training loss: 2.8301777839660645
Validation loss: 2.280733485375681

Epoch: 6| Step: 6
Training loss: 2.601713180541992
Validation loss: 2.2897545676077566

Epoch: 6| Step: 7
Training loss: 2.8438501358032227
Validation loss: 2.2845884189810803

Epoch: 6| Step: 8
Training loss: 1.8501086235046387
Validation loss: 2.299984211562782

Epoch: 6| Step: 9
Training loss: 1.7512000799179077
Validation loss: 2.2851251581663727

Epoch: 6| Step: 10
Training loss: 2.6171391010284424
Validation loss: 2.2793431153861423

Epoch: 6| Step: 11
Training loss: 3.0059564113616943
Validation loss: 2.273864084674466

Epoch: 6| Step: 12
Training loss: 2.7018232345581055
Validation loss: 2.2736458291289625

Epoch: 6| Step: 13
Training loss: 2.6717588901519775
Validation loss: 2.2774664637862996

Epoch: 302| Step: 0
Training loss: 1.9397046566009521
Validation loss: 2.2834930343012654

Epoch: 6| Step: 1
Training loss: 2.9636054039001465
Validation loss: 2.273185399270827

Epoch: 6| Step: 2
Training loss: 2.628556489944458
Validation loss: 2.2880883165585097

Epoch: 6| Step: 3
Training loss: 2.248713493347168
Validation loss: 2.2922293268224245

Epoch: 6| Step: 4
Training loss: 3.0187900066375732
Validation loss: 2.2923014933063137

Epoch: 6| Step: 5
Training loss: 2.797072410583496
Validation loss: 2.2977808470367105

Epoch: 6| Step: 6
Training loss: 2.7393226623535156
Validation loss: 2.3004631585972284

Epoch: 6| Step: 7
Training loss: 2.3225626945495605
Validation loss: 2.2935595461117324

Epoch: 6| Step: 8
Training loss: 2.3069355487823486
Validation loss: 2.3064014014377388

Epoch: 6| Step: 9
Training loss: 1.783949613571167
Validation loss: 2.3001499791299143

Epoch: 6| Step: 10
Training loss: 2.1554622650146484
Validation loss: 2.3165074343322427

Epoch: 6| Step: 11
Training loss: 1.6713640689849854
Validation loss: 2.3162982002381356

Epoch: 6| Step: 12
Training loss: 2.5931992530822754
Validation loss: 2.3324633759836995

Epoch: 6| Step: 13
Training loss: 2.830590009689331
Validation loss: 2.3483663425650647

Epoch: 303| Step: 0
Training loss: 2.3817286491394043
Validation loss: 2.335999714430942

Epoch: 6| Step: 1
Training loss: 2.293778896331787
Validation loss: 2.3262405241689375

Epoch: 6| Step: 2
Training loss: 3.2460200786590576
Validation loss: 2.309100607390045

Epoch: 6| Step: 3
Training loss: 2.233844757080078
Validation loss: 2.296399393389302

Epoch: 6| Step: 4
Training loss: 2.6590638160705566
Validation loss: 2.267531153976276

Epoch: 6| Step: 5
Training loss: 1.0379040241241455
Validation loss: 2.2697239204119612

Epoch: 6| Step: 6
Training loss: 2.416351795196533
Validation loss: 2.2633450979827554

Epoch: 6| Step: 7
Training loss: 3.1150689125061035
Validation loss: 2.2664370998259513

Epoch: 6| Step: 8
Training loss: 2.5762038230895996
Validation loss: 2.25874969010712

Epoch: 6| Step: 9
Training loss: 2.6671252250671387
Validation loss: 2.2649937547663206

Epoch: 6| Step: 10
Training loss: 2.5658979415893555
Validation loss: 2.262423333301339

Epoch: 6| Step: 11
Training loss: 2.3740410804748535
Validation loss: 2.261046189133839

Epoch: 6| Step: 12
Training loss: 2.2163524627685547
Validation loss: 2.27623079669091

Epoch: 6| Step: 13
Training loss: 1.9997062683105469
Validation loss: 2.2776452649024224

Epoch: 304| Step: 0
Training loss: 3.2817130088806152
Validation loss: 2.2895245141880487

Epoch: 6| Step: 1
Training loss: 3.5058646202087402
Validation loss: 2.302698619904057

Epoch: 6| Step: 2
Training loss: 2.7817699909210205
Validation loss: 2.327964723751109

Epoch: 6| Step: 3
Training loss: 1.652895450592041
Validation loss: 2.3321324086958364

Epoch: 6| Step: 4
Training loss: 1.8372924327850342
Validation loss: 2.308816917480961

Epoch: 6| Step: 5
Training loss: 2.566288471221924
Validation loss: 2.3143170623369116

Epoch: 6| Step: 6
Training loss: 2.0586485862731934
Validation loss: 2.3066885355980165

Epoch: 6| Step: 7
Training loss: 2.240781784057617
Validation loss: 2.299132177906652

Epoch: 6| Step: 8
Training loss: 2.243569850921631
Validation loss: 2.3053929754482803

Epoch: 6| Step: 9
Training loss: 2.221761703491211
Validation loss: 2.3191936144264798

Epoch: 6| Step: 10
Training loss: 2.8316335678100586
Validation loss: 2.3115868876057286

Epoch: 6| Step: 11
Training loss: 1.7978267669677734
Validation loss: 2.2879660078274306

Epoch: 6| Step: 12
Training loss: 1.775549054145813
Validation loss: 2.2953796848174064

Epoch: 6| Step: 13
Training loss: 3.2264654636383057
Validation loss: 2.2969796683198664

Epoch: 305| Step: 0
Training loss: 2.3923871517181396
Validation loss: 2.2844942205695697

Epoch: 6| Step: 1
Training loss: 2.9515674114227295
Validation loss: 2.2809115532905824

Epoch: 6| Step: 2
Training loss: 1.6862918138504028
Validation loss: 2.2867312764608734

Epoch: 6| Step: 3
Training loss: 2.5767788887023926
Validation loss: 2.2982611015278804

Epoch: 6| Step: 4
Training loss: 2.750527858734131
Validation loss: 2.2905518803545224

Epoch: 6| Step: 5
Training loss: 2.4657933712005615
Validation loss: 2.283788099083849

Epoch: 6| Step: 6
Training loss: 2.597805976867676
Validation loss: 2.289108345585485

Epoch: 6| Step: 7
Training loss: 1.5911715030670166
Validation loss: 2.2869993691803305

Epoch: 6| Step: 8
Training loss: 2.2768702507019043
Validation loss: 2.285670236874652

Epoch: 6| Step: 9
Training loss: 2.3331034183502197
Validation loss: 2.2909663056814544

Epoch: 6| Step: 10
Training loss: 3.3124237060546875
Validation loss: 2.279758343132593

Epoch: 6| Step: 11
Training loss: 2.7234604358673096
Validation loss: 2.2943141152781825

Epoch: 6| Step: 12
Training loss: 1.6669976711273193
Validation loss: 2.304029377557898

Epoch: 6| Step: 13
Training loss: 2.360154151916504
Validation loss: 2.30675628108363

Epoch: 306| Step: 0
Training loss: 2.270956516265869
Validation loss: 2.3123448971779115

Epoch: 6| Step: 1
Training loss: 3.4475529193878174
Validation loss: 2.313420323915379

Epoch: 6| Step: 2
Training loss: 2.751534938812256
Validation loss: 2.3333559407982776

Epoch: 6| Step: 3
Training loss: 2.302851438522339
Validation loss: 2.328155986724361

Epoch: 6| Step: 4
Training loss: 2.318505048751831
Validation loss: 2.310917810727191

Epoch: 6| Step: 5
Training loss: 3.1643669605255127
Validation loss: 2.287304239888345

Epoch: 6| Step: 6
Training loss: 2.6599440574645996
Validation loss: 2.2873894578667096

Epoch: 6| Step: 7
Training loss: 2.33693265914917
Validation loss: 2.288314614244687

Epoch: 6| Step: 8
Training loss: 2.2753026485443115
Validation loss: 2.2711129855084162

Epoch: 6| Step: 9
Training loss: 1.9311552047729492
Validation loss: 2.27501666930414

Epoch: 6| Step: 10
Training loss: 1.743947982788086
Validation loss: 2.2641536753664733

Epoch: 6| Step: 11
Training loss: 2.369952440261841
Validation loss: 2.25777466066422

Epoch: 6| Step: 12
Training loss: 2.334412097930908
Validation loss: 2.281125883902273

Epoch: 6| Step: 13
Training loss: 1.5465537309646606
Validation loss: 2.272268613179525

Epoch: 307| Step: 0
Training loss: 1.9739172458648682
Validation loss: 2.294483025868734

Epoch: 6| Step: 1
Training loss: 2.7248950004577637
Validation loss: 2.3049220820908904

Epoch: 6| Step: 2
Training loss: 1.942397952079773
Validation loss: 2.331714544244992

Epoch: 6| Step: 3
Training loss: 2.9313206672668457
Validation loss: 2.3534235313374507

Epoch: 6| Step: 4
Training loss: 2.693873882293701
Validation loss: 2.3722573505934847

Epoch: 6| Step: 5
Training loss: 2.0319650173187256
Validation loss: 2.3727429361753565

Epoch: 6| Step: 6
Training loss: 2.0605177879333496
Validation loss: 2.398824066244146

Epoch: 6| Step: 7
Training loss: 3.3110456466674805
Validation loss: 2.358782968213481

Epoch: 6| Step: 8
Training loss: 1.9270904064178467
Validation loss: 2.3387541553025604

Epoch: 6| Step: 9
Training loss: 2.473299026489258
Validation loss: 2.3117253318909676

Epoch: 6| Step: 10
Training loss: 2.6360180377960205
Validation loss: 2.288483665835473

Epoch: 6| Step: 11
Training loss: 2.2161929607391357
Validation loss: 2.274748530439151

Epoch: 6| Step: 12
Training loss: 2.242845296859741
Validation loss: 2.2711521528100453

Epoch: 6| Step: 13
Training loss: 2.493435859680176
Validation loss: 2.257464098673995

Epoch: 308| Step: 0
Training loss: 2.5943422317504883
Validation loss: 2.258271566001318

Epoch: 6| Step: 1
Training loss: 2.074383020401001
Validation loss: 2.2678526601483746

Epoch: 6| Step: 2
Training loss: 2.106489658355713
Validation loss: 2.2708673938628166

Epoch: 6| Step: 3
Training loss: 3.416424512863159
Validation loss: 2.256860984269009

Epoch: 6| Step: 4
Training loss: 2.774336338043213
Validation loss: 2.2747246039811

Epoch: 6| Step: 5
Training loss: 1.8799104690551758
Validation loss: 2.278319348571121

Epoch: 6| Step: 6
Training loss: 2.366438627243042
Validation loss: 2.2806799052863993

Epoch: 6| Step: 7
Training loss: 2.4813265800476074
Validation loss: 2.2935055225126204

Epoch: 6| Step: 8
Training loss: 1.7022311687469482
Validation loss: 2.295943852393858

Epoch: 6| Step: 9
Training loss: 2.8627679347991943
Validation loss: 2.2939838568369546

Epoch: 6| Step: 10
Training loss: 2.2253661155700684
Validation loss: 2.3144390121583016

Epoch: 6| Step: 11
Training loss: 2.6650307178497314
Validation loss: 2.3128798187419934

Epoch: 6| Step: 12
Training loss: 2.118882417678833
Validation loss: 2.3032300779896397

Epoch: 6| Step: 13
Training loss: 2.464146614074707
Validation loss: 2.2971111471934984

Epoch: 309| Step: 0
Training loss: 2.5796549320220947
Validation loss: 2.28241140355346

Epoch: 6| Step: 1
Training loss: 1.9474537372589111
Validation loss: 2.2908746478378132

Epoch: 6| Step: 2
Training loss: 2.4291422367095947
Validation loss: 2.289780027122908

Epoch: 6| Step: 3
Training loss: 2.6747024059295654
Validation loss: 2.287062921831685

Epoch: 6| Step: 4
Training loss: 1.9737818241119385
Validation loss: 2.2891811324704077

Epoch: 6| Step: 5
Training loss: 2.253251791000366
Validation loss: 2.29449325735851

Epoch: 6| Step: 6
Training loss: 2.623422384262085
Validation loss: 2.3050269798565934

Epoch: 6| Step: 7
Training loss: 2.7089931964874268
Validation loss: 2.3242833204166864

Epoch: 6| Step: 8
Training loss: 2.8650739192962646
Validation loss: 2.3370840139286493

Epoch: 6| Step: 9
Training loss: 2.516207456588745
Validation loss: 2.3168927674652426

Epoch: 6| Step: 10
Training loss: 1.8985087871551514
Validation loss: 2.2967455566570325

Epoch: 6| Step: 11
Training loss: 2.6038923263549805
Validation loss: 2.3041260139916533

Epoch: 6| Step: 12
Training loss: 2.651233196258545
Validation loss: 2.283150053793384

Epoch: 6| Step: 13
Training loss: 1.3856256008148193
Validation loss: 2.280692154361356

Epoch: 310| Step: 0
Training loss: 2.661080837249756
Validation loss: 2.2902107725861254

Epoch: 6| Step: 1
Training loss: 2.609264373779297
Validation loss: 2.283055069626019

Epoch: 6| Step: 2
Training loss: 1.9916317462921143
Validation loss: 2.297932178743424

Epoch: 6| Step: 3
Training loss: 1.8111474514007568
Validation loss: 2.295712688917755

Epoch: 6| Step: 4
Training loss: 2.3645029067993164
Validation loss: 2.32102100054423

Epoch: 6| Step: 5
Training loss: 2.221370220184326
Validation loss: 2.3535570201053413

Epoch: 6| Step: 6
Training loss: 2.1433606147766113
Validation loss: 2.3510381893445085

Epoch: 6| Step: 7
Training loss: 2.7466936111450195
Validation loss: 2.353710961598222

Epoch: 6| Step: 8
Training loss: 1.9421429634094238
Validation loss: 2.3558879936895063

Epoch: 6| Step: 9
Training loss: 2.387727737426758
Validation loss: 2.316922551842146

Epoch: 6| Step: 10
Training loss: 1.8979887962341309
Validation loss: 2.3160806317483225

Epoch: 6| Step: 11
Training loss: 3.3702378273010254
Validation loss: 2.2912809976967434

Epoch: 6| Step: 12
Training loss: 2.7469711303710938
Validation loss: 2.284036910662087

Epoch: 6| Step: 13
Training loss: 3.0359792709350586
Validation loss: 2.289132679662397

Epoch: 311| Step: 0
Training loss: 2.494530439376831
Validation loss: 2.2702887442804154

Epoch: 6| Step: 1
Training loss: 2.4125819206237793
Validation loss: 2.2609809957524782

Epoch: 6| Step: 2
Training loss: 2.821079969406128
Validation loss: 2.2552257276350454

Epoch: 6| Step: 3
Training loss: 1.7681143283843994
Validation loss: 2.2619019093052035

Epoch: 6| Step: 4
Training loss: 2.9718849658966064
Validation loss: 2.2673931660190707

Epoch: 6| Step: 5
Training loss: 2.2424497604370117
Validation loss: 2.272976940678012

Epoch: 6| Step: 6
Training loss: 2.503376007080078
Validation loss: 2.289934833844503

Epoch: 6| Step: 7
Training loss: 1.8974846601486206
Validation loss: 2.2893461642726773

Epoch: 6| Step: 8
Training loss: 2.4331483840942383
Validation loss: 2.2869709076419955

Epoch: 6| Step: 9
Training loss: 2.203437089920044
Validation loss: 2.282415877106369

Epoch: 6| Step: 10
Training loss: 2.6276392936706543
Validation loss: 2.2869851755839523

Epoch: 6| Step: 11
Training loss: 2.1033859252929688
Validation loss: 2.2819580621616815

Epoch: 6| Step: 12
Training loss: 1.9956367015838623
Validation loss: 2.2910111258106847

Epoch: 6| Step: 13
Training loss: 3.643826484680176
Validation loss: 2.286749037363196

Epoch: 312| Step: 0
Training loss: 2.6855103969573975
Validation loss: 2.2774916361736994

Epoch: 6| Step: 1
Training loss: 2.0293631553649902
Validation loss: 2.2766130842188352

Epoch: 6| Step: 2
Training loss: 2.213425397872925
Validation loss: 2.2910514416233188

Epoch: 6| Step: 3
Training loss: 2.249706745147705
Validation loss: 2.304484639116513

Epoch: 6| Step: 4
Training loss: 2.480337381362915
Validation loss: 2.3084469097916798

Epoch: 6| Step: 5
Training loss: 1.841843843460083
Validation loss: 2.306638381814444

Epoch: 6| Step: 6
Training loss: 2.4322152137756348
Validation loss: 2.305144850925733

Epoch: 6| Step: 7
Training loss: 2.085681676864624
Validation loss: 2.2899349581810737

Epoch: 6| Step: 8
Training loss: 3.408114433288574
Validation loss: 2.279375922295355

Epoch: 6| Step: 9
Training loss: 2.0912487506866455
Validation loss: 2.2769104588416313

Epoch: 6| Step: 10
Training loss: 2.7111544609069824
Validation loss: 2.2581026618198683

Epoch: 6| Step: 11
Training loss: 2.269268035888672
Validation loss: 2.2665720216689573

Epoch: 6| Step: 12
Training loss: 2.62581205368042
Validation loss: 2.266475622371961

Epoch: 6| Step: 13
Training loss: 2.253629207611084
Validation loss: 2.258768589265885

Epoch: 313| Step: 0
Training loss: 2.459228038787842
Validation loss: 2.2742012239271596

Epoch: 6| Step: 1
Training loss: 2.545628309249878
Validation loss: 2.3068988630848546

Epoch: 6| Step: 2
Training loss: 2.427597999572754
Validation loss: 2.2935403393160914

Epoch: 6| Step: 3
Training loss: 2.0053272247314453
Validation loss: 2.322910524183704

Epoch: 6| Step: 4
Training loss: 2.200551748275757
Validation loss: 2.296921237822502

Epoch: 6| Step: 5
Training loss: 2.422645092010498
Validation loss: 2.269039323253016

Epoch: 6| Step: 6
Training loss: 2.042996406555176
Validation loss: 2.277488359840967

Epoch: 6| Step: 7
Training loss: 2.2737526893615723
Validation loss: 2.2729342342704855

Epoch: 6| Step: 8
Training loss: 2.3282880783081055
Validation loss: 2.2847205797831216

Epoch: 6| Step: 9
Training loss: 3.139575242996216
Validation loss: 2.276465239063386

Epoch: 6| Step: 10
Training loss: 2.696193218231201
Validation loss: 2.2820796107733123

Epoch: 6| Step: 11
Training loss: 2.163877010345459
Validation loss: 2.300155137174873

Epoch: 6| Step: 12
Training loss: 2.637934684753418
Validation loss: 2.303486552289737

Epoch: 6| Step: 13
Training loss: 2.1729896068573
Validation loss: 2.2864748444608463

Epoch: 314| Step: 0
Training loss: 2.4221701622009277
Validation loss: 2.3020741324270926

Epoch: 6| Step: 1
Training loss: 2.0090272426605225
Validation loss: 2.2819996879946802

Epoch: 6| Step: 2
Training loss: 2.409020185470581
Validation loss: 2.292225327543033

Epoch: 6| Step: 3
Training loss: 2.294084310531616
Validation loss: 2.2843211748266734

Epoch: 6| Step: 4
Training loss: 2.6674928665161133
Validation loss: 2.2769041945857387

Epoch: 6| Step: 5
Training loss: 1.9841774702072144
Validation loss: 2.281134177279729

Epoch: 6| Step: 6
Training loss: 2.2970237731933594
Validation loss: 2.2733198929858465

Epoch: 6| Step: 7
Training loss: 2.7978947162628174
Validation loss: 2.2792122184589343

Epoch: 6| Step: 8
Training loss: 2.198758125305176
Validation loss: 2.2755368601891304

Epoch: 6| Step: 9
Training loss: 2.560264825820923
Validation loss: 2.293302312974007

Epoch: 6| Step: 10
Training loss: 2.4819910526275635
Validation loss: 2.2919903826969925

Epoch: 6| Step: 11
Training loss: 2.2099146842956543
Validation loss: 2.2870787856399373

Epoch: 6| Step: 12
Training loss: 2.958897352218628
Validation loss: 2.320349577934511

Epoch: 6| Step: 13
Training loss: 2.1391849517822266
Validation loss: 2.3223348586790022

Epoch: 315| Step: 0
Training loss: 2.152226686477661
Validation loss: 2.3292663174290813

Epoch: 6| Step: 1
Training loss: 2.3146438598632812
Validation loss: 2.321948233471122

Epoch: 6| Step: 2
Training loss: 2.5901122093200684
Validation loss: 2.3416223115818475

Epoch: 6| Step: 3
Training loss: 2.431684970855713
Validation loss: 2.3108445546960317

Epoch: 6| Step: 4
Training loss: 1.9321197271347046
Validation loss: 2.334296070119386

Epoch: 6| Step: 5
Training loss: 1.7251927852630615
Validation loss: 2.309149270416588

Epoch: 6| Step: 6
Training loss: 1.836134910583496
Validation loss: 2.2995173110756824

Epoch: 6| Step: 7
Training loss: 2.0755467414855957
Validation loss: 2.289758336159491

Epoch: 6| Step: 8
Training loss: 3.322368860244751
Validation loss: 2.2780829757772465

Epoch: 6| Step: 9
Training loss: 2.765716075897217
Validation loss: 2.2584010054988246

Epoch: 6| Step: 10
Training loss: 2.538888931274414
Validation loss: 2.2612719715282483

Epoch: 6| Step: 11
Training loss: 2.1846067905426025
Validation loss: 2.2572047787327922

Epoch: 6| Step: 12
Training loss: 2.485934019088745
Validation loss: 2.269111192354592

Epoch: 6| Step: 13
Training loss: 3.2808380126953125
Validation loss: 2.2502239878459642

Epoch: 316| Step: 0
Training loss: 2.5390377044677734
Validation loss: 2.254148560185586

Epoch: 6| Step: 1
Training loss: 2.3971738815307617
Validation loss: 2.2534262877638622

Epoch: 6| Step: 2
Training loss: 1.8161765336990356
Validation loss: 2.230186426511375

Epoch: 6| Step: 3
Training loss: 2.3331193923950195
Validation loss: 2.2352953739063715

Epoch: 6| Step: 4
Training loss: 1.9669126272201538
Validation loss: 2.2493539779416976

Epoch: 6| Step: 5
Training loss: 2.6995153427124023
Validation loss: 2.2516961584809008

Epoch: 6| Step: 6
Training loss: 2.097724199295044
Validation loss: 2.2654196164941274

Epoch: 6| Step: 7
Training loss: 2.80842924118042
Validation loss: 2.294911802455943

Epoch: 6| Step: 8
Training loss: 2.4488351345062256
Validation loss: 2.3038892604971446

Epoch: 6| Step: 9
Training loss: 2.601499557495117
Validation loss: 2.309701012026879

Epoch: 6| Step: 10
Training loss: 2.7071051597595215
Validation loss: 2.307253501748526

Epoch: 6| Step: 11
Training loss: 2.2229397296905518
Validation loss: 2.3163440381326983

Epoch: 6| Step: 12
Training loss: 2.5862536430358887
Validation loss: 2.328709771556239

Epoch: 6| Step: 13
Training loss: 2.1900136470794678
Validation loss: 2.33266516141994

Epoch: 317| Step: 0
Training loss: 2.480771541595459
Validation loss: 2.327464647190545

Epoch: 6| Step: 1
Training loss: 1.9867721796035767
Validation loss: 2.3222964143240326

Epoch: 6| Step: 2
Training loss: 2.2825114727020264
Validation loss: 2.3386256771702922

Epoch: 6| Step: 3
Training loss: 2.797529458999634
Validation loss: 2.335088917004165

Epoch: 6| Step: 4
Training loss: 3.051520586013794
Validation loss: 2.302058263491559

Epoch: 6| Step: 5
Training loss: 2.720425605773926
Validation loss: 2.2752268083633913

Epoch: 6| Step: 6
Training loss: 2.2981605529785156
Validation loss: 2.259425816997405

Epoch: 6| Step: 7
Training loss: 2.7233760356903076
Validation loss: 2.2458568567870767

Epoch: 6| Step: 8
Training loss: 1.5913418531417847
Validation loss: 2.246741525588497

Epoch: 6| Step: 9
Training loss: 2.4791460037231445
Validation loss: 2.24459901420019

Epoch: 6| Step: 10
Training loss: 2.401550769805908
Validation loss: 2.2445908951502975

Epoch: 6| Step: 11
Training loss: 1.8086351156234741
Validation loss: 2.2389723331697526

Epoch: 6| Step: 12
Training loss: 2.9673209190368652
Validation loss: 2.243231778503746

Epoch: 6| Step: 13
Training loss: 1.3585153818130493
Validation loss: 2.2456933272782194

Epoch: 318| Step: 0
Training loss: 3.1426949501037598
Validation loss: 2.240847042811814

Epoch: 6| Step: 1
Training loss: 1.7995719909667969
Validation loss: 2.2491467486145678

Epoch: 6| Step: 2
Training loss: 2.3195183277130127
Validation loss: 2.2662332968045305

Epoch: 6| Step: 3
Training loss: 2.6675610542297363
Validation loss: 2.2824257548137377

Epoch: 6| Step: 4
Training loss: 1.9922090768814087
Validation loss: 2.3211192597625074

Epoch: 6| Step: 5
Training loss: 2.8469810485839844
Validation loss: 2.3604302278129

Epoch: 6| Step: 6
Training loss: 2.329416275024414
Validation loss: 2.381073397974814

Epoch: 6| Step: 7
Training loss: 1.412074327468872
Validation loss: 2.3736249118722896

Epoch: 6| Step: 8
Training loss: 2.618659257888794
Validation loss: 2.384981252813852

Epoch: 6| Step: 9
Training loss: 2.515057325363159
Validation loss: 2.377367276017384

Epoch: 6| Step: 10
Training loss: 2.2698607444763184
Validation loss: 2.3734876289162585

Epoch: 6| Step: 11
Training loss: 2.3567252159118652
Validation loss: 2.364468615542176

Epoch: 6| Step: 12
Training loss: 2.8629088401794434
Validation loss: 2.3366954275356826

Epoch: 6| Step: 13
Training loss: 2.634024143218994
Validation loss: 2.317988008581182

Epoch: 319| Step: 0
Training loss: 1.7253001928329468
Validation loss: 2.291542991515129

Epoch: 6| Step: 1
Training loss: 2.6370162963867188
Validation loss: 2.3009240986198507

Epoch: 6| Step: 2
Training loss: 2.052902936935425
Validation loss: 2.277266284470917

Epoch: 6| Step: 3
Training loss: 1.8936340808868408
Validation loss: 2.2531888972046556

Epoch: 6| Step: 4
Training loss: 2.171187162399292
Validation loss: 2.256451560604957

Epoch: 6| Step: 5
Training loss: 2.6145834922790527
Validation loss: 2.248747420567338

Epoch: 6| Step: 6
Training loss: 2.2853639125823975
Validation loss: 2.2500411054139495

Epoch: 6| Step: 7
Training loss: 2.747161626815796
Validation loss: 2.247769204519128

Epoch: 6| Step: 8
Training loss: 2.518120288848877
Validation loss: 2.2518615389382965

Epoch: 6| Step: 9
Training loss: 2.6321403980255127
Validation loss: 2.237139562124847

Epoch: 6| Step: 10
Training loss: 2.552447557449341
Validation loss: 2.2567766661285074

Epoch: 6| Step: 11
Training loss: 2.602588415145874
Validation loss: 2.258087399185345

Epoch: 6| Step: 12
Training loss: 2.906597852706909
Validation loss: 2.264944843066636

Epoch: 6| Step: 13
Training loss: 1.5521950721740723
Validation loss: 2.2540136626971665

Epoch: 320| Step: 0
Training loss: 2.3968939781188965
Validation loss: 2.259212916897189

Epoch: 6| Step: 1
Training loss: 2.484928607940674
Validation loss: 2.263322327726631

Epoch: 6| Step: 2
Training loss: 2.3096354007720947
Validation loss: 2.265920321146647

Epoch: 6| Step: 3
Training loss: 2.6451706886291504
Validation loss: 2.258065660794576

Epoch: 6| Step: 4
Training loss: 2.32914137840271
Validation loss: 2.2877475523179576

Epoch: 6| Step: 5
Training loss: 2.8796329498291016
Validation loss: 2.293854446821315

Epoch: 6| Step: 6
Training loss: 2.0274229049682617
Validation loss: 2.295002316915861

Epoch: 6| Step: 7
Training loss: 2.1458020210266113
Validation loss: 2.2908796161733647

Epoch: 6| Step: 8
Training loss: 2.119628429412842
Validation loss: 2.279570723092684

Epoch: 6| Step: 9
Training loss: 2.6151881217956543
Validation loss: 2.2603240320759435

Epoch: 6| Step: 10
Training loss: 1.8877017498016357
Validation loss: 2.256967818865212

Epoch: 6| Step: 11
Training loss: 2.0490829944610596
Validation loss: 2.2591782154575473

Epoch: 6| Step: 12
Training loss: 2.9774329662323
Validation loss: 2.2571139361268733

Epoch: 6| Step: 13
Training loss: 2.18524169921875
Validation loss: 2.263001118936846

Epoch: 321| Step: 0
Training loss: 1.6878726482391357
Validation loss: 2.248087042121477

Epoch: 6| Step: 1
Training loss: 2.4377999305725098
Validation loss: 2.2542667350461407

Epoch: 6| Step: 2
Training loss: 2.0595810413360596
Validation loss: 2.2554872676890385

Epoch: 6| Step: 3
Training loss: 2.488213062286377
Validation loss: 2.2621576401495163

Epoch: 6| Step: 4
Training loss: 1.6842610836029053
Validation loss: 2.2384987287623908

Epoch: 6| Step: 5
Training loss: 2.854443073272705
Validation loss: 2.2456065608609106

Epoch: 6| Step: 6
Training loss: 2.36283016204834
Validation loss: 2.239549136930896

Epoch: 6| Step: 7
Training loss: 2.1901257038116455
Validation loss: 2.2550763468588553

Epoch: 6| Step: 8
Training loss: 3.3729450702667236
Validation loss: 2.2628327390199066

Epoch: 6| Step: 9
Training loss: 2.651930332183838
Validation loss: 2.2598464463346746

Epoch: 6| Step: 10
Training loss: 2.297877311706543
Validation loss: 2.2716160922922115

Epoch: 6| Step: 11
Training loss: 2.242138624191284
Validation loss: 2.2638446387424263

Epoch: 6| Step: 12
Training loss: 2.622418165206909
Validation loss: 2.262615547385267

Epoch: 6| Step: 13
Training loss: 1.957841396331787
Validation loss: 2.268017425332018

Epoch: 322| Step: 0
Training loss: 1.8930785655975342
Validation loss: 2.2848102161961217

Epoch: 6| Step: 1
Training loss: 2.2571139335632324
Validation loss: 2.2970638531510548

Epoch: 6| Step: 2
Training loss: 2.421433925628662
Validation loss: 2.307916215671006

Epoch: 6| Step: 3
Training loss: 2.318179130554199
Validation loss: 2.311916737146275

Epoch: 6| Step: 4
Training loss: 2.982947587966919
Validation loss: 2.3076666093641713

Epoch: 6| Step: 5
Training loss: 2.9235434532165527
Validation loss: 2.319063058463476

Epoch: 6| Step: 6
Training loss: 2.5752532482147217
Validation loss: 2.339364769638226

Epoch: 6| Step: 7
Training loss: 1.4689936637878418
Validation loss: 2.3238182093507502

Epoch: 6| Step: 8
Training loss: 2.545544147491455
Validation loss: 2.316223282967844

Epoch: 6| Step: 9
Training loss: 2.064620018005371
Validation loss: 2.3069889994077784

Epoch: 6| Step: 10
Training loss: 2.562221050262451
Validation loss: 2.3025819665642193

Epoch: 6| Step: 11
Training loss: 2.4134814739227295
Validation loss: 2.2935617816063667

Epoch: 6| Step: 12
Training loss: 2.3527212142944336
Validation loss: 2.2726933366508892

Epoch: 6| Step: 13
Training loss: 2.320997953414917
Validation loss: 2.2592090893817205

Epoch: 323| Step: 0
Training loss: 2.2665882110595703
Validation loss: 2.258487122033232

Epoch: 6| Step: 1
Training loss: 1.91486394405365
Validation loss: 2.2636391321818032

Epoch: 6| Step: 2
Training loss: 1.8777037858963013
Validation loss: 2.248727129351708

Epoch: 6| Step: 3
Training loss: 2.3472230434417725
Validation loss: 2.2441397559258247

Epoch: 6| Step: 4
Training loss: 2.229811668395996
Validation loss: 2.2488844574138684

Epoch: 6| Step: 5
Training loss: 2.1883721351623535
Validation loss: 2.2400841892406507

Epoch: 6| Step: 6
Training loss: 2.343073606491089
Validation loss: 2.242144342391722

Epoch: 6| Step: 7
Training loss: 2.6808581352233887
Validation loss: 2.2405975992961595

Epoch: 6| Step: 8
Training loss: 2.082200050354004
Validation loss: 2.2490807912682973

Epoch: 6| Step: 9
Training loss: 3.3373894691467285
Validation loss: 2.2546548676747147

Epoch: 6| Step: 10
Training loss: 3.3783535957336426
Validation loss: 2.2450452543074086

Epoch: 6| Step: 11
Training loss: 1.9023722410202026
Validation loss: 2.2505917036405174

Epoch: 6| Step: 12
Training loss: 2.264781951904297
Validation loss: 2.2834890555309992

Epoch: 6| Step: 13
Training loss: 1.8535600900650024
Validation loss: 2.298105496232228

Epoch: 324| Step: 0
Training loss: 2.635383129119873
Validation loss: 2.3540805334685952

Epoch: 6| Step: 1
Training loss: 2.0750765800476074
Validation loss: 2.44496004555815

Epoch: 6| Step: 2
Training loss: 1.7686364650726318
Validation loss: 2.432943554334743

Epoch: 6| Step: 3
Training loss: 2.534313201904297
Validation loss: 2.438553099991173

Epoch: 6| Step: 4
Training loss: 2.3177809715270996
Validation loss: 2.4045893530691824

Epoch: 6| Step: 5
Training loss: 2.223916530609131
Validation loss: 2.355896503694596

Epoch: 6| Step: 6
Training loss: 1.6071693897247314
Validation loss: 2.3173277403718684

Epoch: 6| Step: 7
Training loss: 3.0254716873168945
Validation loss: 2.251710686632382

Epoch: 6| Step: 8
Training loss: 2.061340808868408
Validation loss: 2.2410027826986005

Epoch: 6| Step: 9
Training loss: 2.5275702476501465
Validation loss: 2.2446528583444576

Epoch: 6| Step: 10
Training loss: 2.5151734352111816
Validation loss: 2.226834738126365

Epoch: 6| Step: 11
Training loss: 2.384451150894165
Validation loss: 2.2266955965308735

Epoch: 6| Step: 12
Training loss: 3.3076729774475098
Validation loss: 2.236032662853118

Epoch: 6| Step: 13
Training loss: 3.055527448654175
Validation loss: 2.233726409173781

Epoch: 325| Step: 0
Training loss: 3.348728895187378
Validation loss: 2.242638898152177

Epoch: 6| Step: 1
Training loss: 2.9289779663085938
Validation loss: 2.236713924715596

Epoch: 6| Step: 2
Training loss: 3.0893638134002686
Validation loss: 2.2371807764935236

Epoch: 6| Step: 3
Training loss: 2.0992348194122314
Validation loss: 2.2213334447594097

Epoch: 6| Step: 4
Training loss: 2.52223801612854
Validation loss: 2.22371353385269

Epoch: 6| Step: 5
Training loss: 1.728285312652588
Validation loss: 2.221309538810484

Epoch: 6| Step: 6
Training loss: 2.127562999725342
Validation loss: 2.228169612987067

Epoch: 6| Step: 7
Training loss: 2.3129916191101074
Validation loss: 2.2460522023580407

Epoch: 6| Step: 8
Training loss: 2.0766775608062744
Validation loss: 2.2480546659038914

Epoch: 6| Step: 9
Training loss: 2.2064104080200195
Validation loss: 2.254571273762693

Epoch: 6| Step: 10
Training loss: 1.9389337301254272
Validation loss: 2.276254471912179

Epoch: 6| Step: 11
Training loss: 1.8924860954284668
Validation loss: 2.308150919534827

Epoch: 6| Step: 12
Training loss: 2.578843593597412
Validation loss: 2.3012916657232467

Epoch: 6| Step: 13
Training loss: 2.7570817470550537
Validation loss: 2.298986852809947

Epoch: 326| Step: 0
Training loss: 2.0752017498016357
Validation loss: 2.280338553972142

Epoch: 6| Step: 1
Training loss: 2.975822925567627
Validation loss: 2.2743054795008835

Epoch: 6| Step: 2
Training loss: 2.856004238128662
Validation loss: 2.306064615967453

Epoch: 6| Step: 3
Training loss: 2.4381494522094727
Validation loss: 2.2888757464706257

Epoch: 6| Step: 4
Training loss: 3.154672861099243
Validation loss: 2.272750900637719

Epoch: 6| Step: 5
Training loss: 1.730358362197876
Validation loss: 2.26994429608827

Epoch: 6| Step: 6
Training loss: 1.5393214225769043
Validation loss: 2.267979900042216

Epoch: 6| Step: 7
Training loss: 1.635212779045105
Validation loss: 2.2822812859730055

Epoch: 6| Step: 8
Training loss: 2.130453586578369
Validation loss: 2.287049824191678

Epoch: 6| Step: 9
Training loss: 2.2958908081054688
Validation loss: 2.278515710625597

Epoch: 6| Step: 10
Training loss: 1.8993985652923584
Validation loss: 2.300709580862394

Epoch: 6| Step: 11
Training loss: 2.997427225112915
Validation loss: 2.2953033601084063

Epoch: 6| Step: 12
Training loss: 2.5744824409484863
Validation loss: 2.285807655703637

Epoch: 6| Step: 13
Training loss: 3.069186210632324
Validation loss: 2.276072099644651

Epoch: 327| Step: 0
Training loss: 2.3381195068359375
Validation loss: 2.271243741435389

Epoch: 6| Step: 1
Training loss: 2.8285789489746094
Validation loss: 2.2730904112580004

Epoch: 6| Step: 2
Training loss: 2.5627479553222656
Validation loss: 2.2498357629263275

Epoch: 6| Step: 3
Training loss: 1.8725483417510986
Validation loss: 2.2636586671234458

Epoch: 6| Step: 4
Training loss: 2.0644755363464355
Validation loss: 2.2540628217881724

Epoch: 6| Step: 5
Training loss: 2.343092441558838
Validation loss: 2.25537193975141

Epoch: 6| Step: 6
Training loss: 2.0238070487976074
Validation loss: 2.2587382472971433

Epoch: 6| Step: 7
Training loss: 2.7134246826171875
Validation loss: 2.2428091700359056

Epoch: 6| Step: 8
Training loss: 2.294632911682129
Validation loss: 2.246614594613352

Epoch: 6| Step: 9
Training loss: 2.691497802734375
Validation loss: 2.2379055407739457

Epoch: 6| Step: 10
Training loss: 2.3524036407470703
Validation loss: 2.2366478404691144

Epoch: 6| Step: 11
Training loss: 2.134734630584717
Validation loss: 2.24707600121857

Epoch: 6| Step: 12
Training loss: 2.3187882900238037
Validation loss: 2.2389584151647424

Epoch: 6| Step: 13
Training loss: 2.334585666656494
Validation loss: 2.2455882821031796

Epoch: 328| Step: 0
Training loss: 1.6828994750976562
Validation loss: 2.265013797308809

Epoch: 6| Step: 1
Training loss: 2.233675956726074
Validation loss: 2.2652975269543227

Epoch: 6| Step: 2
Training loss: 2.3684067726135254
Validation loss: 2.28939179835781

Epoch: 6| Step: 3
Training loss: 1.869072437286377
Validation loss: 2.3149647815253145

Epoch: 6| Step: 4
Training loss: 3.1679019927978516
Validation loss: 2.3396081155346287

Epoch: 6| Step: 5
Training loss: 2.531564950942993
Validation loss: 2.322814569678358

Epoch: 6| Step: 6
Training loss: 2.044645309448242
Validation loss: 2.30685588108596

Epoch: 6| Step: 7
Training loss: 2.597156047821045
Validation loss: 2.2859617561422367

Epoch: 6| Step: 8
Training loss: 2.9247007369995117
Validation loss: 2.273852291927543

Epoch: 6| Step: 9
Training loss: 2.8011512756347656
Validation loss: 2.2424195479321223

Epoch: 6| Step: 10
Training loss: 2.0224337577819824
Validation loss: 2.232913171091387

Epoch: 6| Step: 11
Training loss: 2.5500152111053467
Validation loss: 2.2192480333389772

Epoch: 6| Step: 12
Training loss: 1.6079819202423096
Validation loss: 2.2113210385845554

Epoch: 6| Step: 13
Training loss: 2.9147872924804688
Validation loss: 2.2089222092782297

Epoch: 329| Step: 0
Training loss: 2.111968517303467
Validation loss: 2.2137391772321475

Epoch: 6| Step: 1
Training loss: 1.8810248374938965
Validation loss: 2.2109013526670394

Epoch: 6| Step: 2
Training loss: 1.9072757959365845
Validation loss: 2.2216893472979145

Epoch: 6| Step: 3
Training loss: 2.072028160095215
Validation loss: 2.2340801992724018

Epoch: 6| Step: 4
Training loss: 2.4785375595092773
Validation loss: 2.254149270314042

Epoch: 6| Step: 5
Training loss: 2.3460640907287598
Validation loss: 2.266919312938567

Epoch: 6| Step: 6
Training loss: 2.0684661865234375
Validation loss: 2.2688242132945726

Epoch: 6| Step: 7
Training loss: 2.4918720722198486
Validation loss: 2.290420380971765

Epoch: 6| Step: 8
Training loss: 2.089616298675537
Validation loss: 2.3102463688901675

Epoch: 6| Step: 9
Training loss: 2.7303214073181152
Validation loss: 2.3313999842571955

Epoch: 6| Step: 10
Training loss: 2.0891406536102295
Validation loss: 2.3179383149711033

Epoch: 6| Step: 11
Training loss: 3.125454902648926
Validation loss: 2.331259153222525

Epoch: 6| Step: 12
Training loss: 2.6932122707366943
Validation loss: 2.3242303222738285

Epoch: 6| Step: 13
Training loss: 3.7056820392608643
Validation loss: 2.3198538211084183

Epoch: 330| Step: 0
Training loss: 2.481168031692505
Validation loss: 2.300665581098167

Epoch: 6| Step: 1
Training loss: 2.1948776245117188
Validation loss: 2.2941926371666694

Epoch: 6| Step: 2
Training loss: 2.451730251312256
Validation loss: 2.2829833876702095

Epoch: 6| Step: 3
Training loss: 1.6289691925048828
Validation loss: 2.2643446947938655

Epoch: 6| Step: 4
Training loss: 2.491082191467285
Validation loss: 2.26022998491923

Epoch: 6| Step: 5
Training loss: 2.397679328918457
Validation loss: 2.259110014925721

Epoch: 6| Step: 6
Training loss: 2.1409478187561035
Validation loss: 2.2389317584294144

Epoch: 6| Step: 7
Training loss: 2.5935845375061035
Validation loss: 2.2529753100487495

Epoch: 6| Step: 8
Training loss: 2.846930980682373
Validation loss: 2.231966046876805

Epoch: 6| Step: 9
Training loss: 2.761867046356201
Validation loss: 2.234223945166475

Epoch: 6| Step: 10
Training loss: 2.358153820037842
Validation loss: 2.2228696448828584

Epoch: 6| Step: 11
Training loss: 2.641055107116699
Validation loss: 2.2399150453588015

Epoch: 6| Step: 12
Training loss: 1.8327980041503906
Validation loss: 2.226651801857897

Epoch: 6| Step: 13
Training loss: 2.1533844470977783
Validation loss: 2.24287054615636

Epoch: 331| Step: 0
Training loss: 2.4394068717956543
Validation loss: 2.2509441427005235

Epoch: 6| Step: 1
Training loss: 2.0004005432128906
Validation loss: 2.2735937846604215

Epoch: 6| Step: 2
Training loss: 1.4599592685699463
Validation loss: 2.2741949186530164

Epoch: 6| Step: 3
Training loss: 2.1182665824890137
Validation loss: 2.282435135174823

Epoch: 6| Step: 4
Training loss: 2.1204068660736084
Validation loss: 2.2655373093902424

Epoch: 6| Step: 5
Training loss: 2.1531224250793457
Validation loss: 2.2670679092407227

Epoch: 6| Step: 6
Training loss: 2.823422431945801
Validation loss: 2.25746585476783

Epoch: 6| Step: 7
Training loss: 2.689192533493042
Validation loss: 2.229476700546921

Epoch: 6| Step: 8
Training loss: 3.068366050720215
Validation loss: 2.2336322492168796

Epoch: 6| Step: 9
Training loss: 2.13457989692688
Validation loss: 2.2335781025630173

Epoch: 6| Step: 10
Training loss: 2.298748016357422
Validation loss: 2.2273808320363364

Epoch: 6| Step: 11
Training loss: 2.2667646408081055
Validation loss: 2.239293558623201

Epoch: 6| Step: 12
Training loss: 2.307461738586426
Validation loss: 2.2509136943406958

Epoch: 6| Step: 13
Training loss: 3.458950996398926
Validation loss: 2.259742611198015

Epoch: 332| Step: 0
Training loss: 2.2472214698791504
Validation loss: 2.2653899295355684

Epoch: 6| Step: 1
Training loss: 2.1491732597351074
Validation loss: 2.2764342241389777

Epoch: 6| Step: 2
Training loss: 2.5438804626464844
Validation loss: 2.286333235361243

Epoch: 6| Step: 3
Training loss: 2.786670684814453
Validation loss: 2.2895870849650395

Epoch: 6| Step: 4
Training loss: 3.5138893127441406
Validation loss: 2.3034663866924983

Epoch: 6| Step: 5
Training loss: 1.6313867568969727
Validation loss: 2.2945079136920232

Epoch: 6| Step: 6
Training loss: 2.479471206665039
Validation loss: 2.293329561910322

Epoch: 6| Step: 7
Training loss: 1.8988975286483765
Validation loss: 2.284153997257192

Epoch: 6| Step: 8
Training loss: 2.149315595626831
Validation loss: 2.2744643611292683

Epoch: 6| Step: 9
Training loss: 1.7089650630950928
Validation loss: 2.280991783706091

Epoch: 6| Step: 10
Training loss: 2.6469640731811523
Validation loss: 2.2775143602842927

Epoch: 6| Step: 11
Training loss: 2.604890823364258
Validation loss: 2.2812538941701255

Epoch: 6| Step: 12
Training loss: 2.4777705669403076
Validation loss: 2.27319759450933

Epoch: 6| Step: 13
Training loss: 1.5748851299285889
Validation loss: 2.269686109276228

Epoch: 333| Step: 0
Training loss: 1.974851369857788
Validation loss: 2.2491224299194994

Epoch: 6| Step: 1
Training loss: 3.1239044666290283
Validation loss: 2.2612410527403637

Epoch: 6| Step: 2
Training loss: 2.0412063598632812
Validation loss: 2.247808407711726

Epoch: 6| Step: 3
Training loss: 2.1390724182128906
Validation loss: 2.2428235828235583

Epoch: 6| Step: 4
Training loss: 2.1839611530303955
Validation loss: 2.2356638600749354

Epoch: 6| Step: 5
Training loss: 1.9763236045837402
Validation loss: 2.2431595992016535

Epoch: 6| Step: 6
Training loss: 2.2978405952453613
Validation loss: 2.241235069049302

Epoch: 6| Step: 7
Training loss: 2.63084077835083
Validation loss: 2.242955561607115

Epoch: 6| Step: 8
Training loss: 2.1518378257751465
Validation loss: 2.2437157118192284

Epoch: 6| Step: 9
Training loss: 2.4900224208831787
Validation loss: 2.2487611386083786

Epoch: 6| Step: 10
Training loss: 1.8881044387817383
Validation loss: 2.272560663120721

Epoch: 6| Step: 11
Training loss: 2.7079219818115234
Validation loss: 2.2820785148169405

Epoch: 6| Step: 12
Training loss: 2.725371837615967
Validation loss: 2.3162391339578936

Epoch: 6| Step: 13
Training loss: 2.4408936500549316
Validation loss: 2.3316142379596667

Epoch: 334| Step: 0
Training loss: 2.5555922985076904
Validation loss: 2.315517335809687

Epoch: 6| Step: 1
Training loss: 2.111086845397949
Validation loss: 2.315900161702146

Epoch: 6| Step: 2
Training loss: 2.3100109100341797
Validation loss: 2.2903753557512836

Epoch: 6| Step: 3
Training loss: 2.161160707473755
Validation loss: 2.270402285360521

Epoch: 6| Step: 4
Training loss: 2.3679990768432617
Validation loss: 2.261501646810962

Epoch: 6| Step: 5
Training loss: 2.3704745769500732
Validation loss: 2.237227011752385

Epoch: 6| Step: 6
Training loss: 2.6399152278900146
Validation loss: 2.247571760608304

Epoch: 6| Step: 7
Training loss: 1.9225623607635498
Validation loss: 2.2359100900670534

Epoch: 6| Step: 8
Training loss: 2.4987380504608154
Validation loss: 2.239604401332076

Epoch: 6| Step: 9
Training loss: 2.3104453086853027
Validation loss: 2.225384278963971

Epoch: 6| Step: 10
Training loss: 2.6641077995300293
Validation loss: 2.225920405439151

Epoch: 6| Step: 11
Training loss: 2.422137975692749
Validation loss: 2.2291425171718804

Epoch: 6| Step: 12
Training loss: 2.3145227432250977
Validation loss: 2.2264355062156596

Epoch: 6| Step: 13
Training loss: 1.9493263959884644
Validation loss: 2.2364112279748403

Epoch: 335| Step: 0
Training loss: 2.526764392852783
Validation loss: 2.2409045696258545

Epoch: 6| Step: 1
Training loss: 2.34916353225708
Validation loss: 2.2689527773088023

Epoch: 6| Step: 2
Training loss: 2.0543618202209473
Validation loss: 2.282976504295103

Epoch: 6| Step: 3
Training loss: 2.310818672180176
Validation loss: 2.2973508142655894

Epoch: 6| Step: 4
Training loss: 2.416256904602051
Validation loss: 2.29187677496223

Epoch: 6| Step: 5
Training loss: 2.7050909996032715
Validation loss: 2.2920042955747215

Epoch: 6| Step: 6
Training loss: 1.7977819442749023
Validation loss: 2.2814677530719387

Epoch: 6| Step: 7
Training loss: 1.9376862049102783
Validation loss: 2.2733810665786907

Epoch: 6| Step: 8
Training loss: 2.352935314178467
Validation loss: 2.257093883329822

Epoch: 6| Step: 9
Training loss: 2.0820698738098145
Validation loss: 2.2556473234648347

Epoch: 6| Step: 10
Training loss: 3.0405468940734863
Validation loss: 2.253234568462577

Epoch: 6| Step: 11
Training loss: 2.31817626953125
Validation loss: 2.2591787410038773

Epoch: 6| Step: 12
Training loss: 1.8165409564971924
Validation loss: 2.2775742776932253

Epoch: 6| Step: 13
Training loss: 3.3746776580810547
Validation loss: 2.2895358198432514

Epoch: 336| Step: 0
Training loss: 1.7255877256393433
Validation loss: 2.283621279142236

Epoch: 6| Step: 1
Training loss: 2.867602825164795
Validation loss: 2.262022561924432

Epoch: 6| Step: 2
Training loss: 2.193417549133301
Validation loss: 2.264950054948048

Epoch: 6| Step: 3
Training loss: 3.0301637649536133
Validation loss: 2.270734125568021

Epoch: 6| Step: 4
Training loss: 3.030158042907715
Validation loss: 2.288272867920578

Epoch: 6| Step: 5
Training loss: 2.0685720443725586
Validation loss: 2.2683335119678127

Epoch: 6| Step: 6
Training loss: 2.3694424629211426
Validation loss: 2.254772311897688

Epoch: 6| Step: 7
Training loss: 1.092813491821289
Validation loss: 2.255355872133727

Epoch: 6| Step: 8
Training loss: 3.0470685958862305
Validation loss: 2.2389161791852725

Epoch: 6| Step: 9
Training loss: 1.9157565832138062
Validation loss: 2.2408154805501304

Epoch: 6| Step: 10
Training loss: 2.1338107585906982
Validation loss: 2.2355910424263246

Epoch: 6| Step: 11
Training loss: 2.100522994995117
Validation loss: 2.219088315963745

Epoch: 6| Step: 12
Training loss: 3.087399482727051
Validation loss: 2.22882350285848

Epoch: 6| Step: 13
Training loss: 1.612137794494629
Validation loss: 2.2267533220270628

Epoch: 337| Step: 0
Training loss: 1.7837797403335571
Validation loss: 2.2251266587165093

Epoch: 6| Step: 1
Training loss: 2.368175983428955
Validation loss: 2.2241597406325804

Epoch: 6| Step: 2
Training loss: 2.3082847595214844
Validation loss: 2.217823975829668

Epoch: 6| Step: 3
Training loss: 2.9048266410827637
Validation loss: 2.228082195405037

Epoch: 6| Step: 4
Training loss: 2.133822441101074
Validation loss: 2.2220146168944654

Epoch: 6| Step: 5
Training loss: 1.7852922677993774
Validation loss: 2.2388689928157355

Epoch: 6| Step: 6
Training loss: 1.7694138288497925
Validation loss: 2.249729379530876

Epoch: 6| Step: 7
Training loss: 2.79726505279541
Validation loss: 2.261003981354416

Epoch: 6| Step: 8
Training loss: 2.9138031005859375
Validation loss: 2.266206297823178

Epoch: 6| Step: 9
Training loss: 2.4344663619995117
Validation loss: 2.265680193901062

Epoch: 6| Step: 10
Training loss: 2.4173779487609863
Validation loss: 2.2822335727753176

Epoch: 6| Step: 11
Training loss: 2.1341681480407715
Validation loss: 2.2797759489346574

Epoch: 6| Step: 12
Training loss: 1.9899861812591553
Validation loss: 2.2797124565288587

Epoch: 6| Step: 13
Training loss: 3.3580336570739746
Validation loss: 2.2632041541478967

Epoch: 338| Step: 0
Training loss: 1.8187243938446045
Validation loss: 2.2614780856717016

Epoch: 6| Step: 1
Training loss: 2.0394794940948486
Validation loss: 2.2447176159069104

Epoch: 6| Step: 2
Training loss: 2.3543572425842285
Validation loss: 2.252503366880519

Epoch: 6| Step: 3
Training loss: 2.1807541847229004
Validation loss: 2.270292907632807

Epoch: 6| Step: 4
Training loss: 1.8277862071990967
Validation loss: 2.2608193300103627

Epoch: 6| Step: 5
Training loss: 2.8637912273406982
Validation loss: 2.249638451042996

Epoch: 6| Step: 6
Training loss: 1.5176639556884766
Validation loss: 2.256348445851316

Epoch: 6| Step: 7
Training loss: 2.2909626960754395
Validation loss: 2.2602012157440186

Epoch: 6| Step: 8
Training loss: 2.4813199043273926
Validation loss: 2.255089523971722

Epoch: 6| Step: 9
Training loss: 2.825356960296631
Validation loss: 2.2508736400194067

Epoch: 6| Step: 10
Training loss: 2.694239616394043
Validation loss: 2.240529647437475

Epoch: 6| Step: 11
Training loss: 2.4960498809814453
Validation loss: 2.243414681444886

Epoch: 6| Step: 12
Training loss: 3.0781331062316895
Validation loss: 2.2445698579152427

Epoch: 6| Step: 13
Training loss: 1.888183832168579
Validation loss: 2.2261287396953953

Epoch: 339| Step: 0
Training loss: 1.9698058366775513
Validation loss: 2.2244839309364237

Epoch: 6| Step: 1
Training loss: 2.807832717895508
Validation loss: 2.22098522545189

Epoch: 6| Step: 2
Training loss: 2.465303421020508
Validation loss: 2.2290255792679323

Epoch: 6| Step: 3
Training loss: 2.453850030899048
Validation loss: 2.2144761777693227

Epoch: 6| Step: 4
Training loss: 2.5337953567504883
Validation loss: 2.221069510265063

Epoch: 6| Step: 5
Training loss: 2.168201446533203
Validation loss: 2.2394054589733

Epoch: 6| Step: 6
Training loss: 2.181044578552246
Validation loss: 2.2362169655420447

Epoch: 6| Step: 7
Training loss: 2.2310917377471924
Validation loss: 2.234109901612805

Epoch: 6| Step: 8
Training loss: 1.904876470565796
Validation loss: 2.2633729057927288

Epoch: 6| Step: 9
Training loss: 1.129175066947937
Validation loss: 2.2708153058123846

Epoch: 6| Step: 10
Training loss: 2.703589916229248
Validation loss: 2.314109763791484

Epoch: 6| Step: 11
Training loss: 2.7831902503967285
Validation loss: 2.3383738238324403

Epoch: 6| Step: 12
Training loss: 2.8424370288848877
Validation loss: 2.3589233326655563

Epoch: 6| Step: 13
Training loss: 2.597691535949707
Validation loss: 2.3254411092368503

Epoch: 340| Step: 0
Training loss: 2.1440811157226562
Validation loss: 2.305745288889895

Epoch: 6| Step: 1
Training loss: 2.623988151550293
Validation loss: 2.2650740582455873

Epoch: 6| Step: 2
Training loss: 2.2348105907440186
Validation loss: 2.2454859697690575

Epoch: 6| Step: 3
Training loss: 2.3342103958129883
Validation loss: 2.2292190520994124

Epoch: 6| Step: 4
Training loss: 2.6349778175354004
Validation loss: 2.2065089607751496

Epoch: 6| Step: 5
Training loss: 2.8091630935668945
Validation loss: 2.2148176316292054

Epoch: 6| Step: 6
Training loss: 2.0207090377807617
Validation loss: 2.2217302476206133

Epoch: 6| Step: 7
Training loss: 2.0976624488830566
Validation loss: 2.2034870809124363

Epoch: 6| Step: 8
Training loss: 2.4634313583374023
Validation loss: 2.1968168302248885

Epoch: 6| Step: 9
Training loss: 2.540245771408081
Validation loss: 2.206034311684229

Epoch: 6| Step: 10
Training loss: 2.5499234199523926
Validation loss: 2.195615268522693

Epoch: 6| Step: 11
Training loss: 1.7650697231292725
Validation loss: 2.202545935107816

Epoch: 6| Step: 12
Training loss: 2.4589929580688477
Validation loss: 2.20812887530173

Epoch: 6| Step: 13
Training loss: 1.8156263828277588
Validation loss: 2.219010471015848

Epoch: 341| Step: 0
Training loss: 1.1692219972610474
Validation loss: 2.2276765095290316

Epoch: 6| Step: 1
Training loss: 1.8415707349777222
Validation loss: 2.2521076356211016

Epoch: 6| Step: 2
Training loss: 3.2164554595947266
Validation loss: 2.273618262301209

Epoch: 6| Step: 3
Training loss: 1.8606979846954346
Validation loss: 2.2756934473591466

Epoch: 6| Step: 4
Training loss: 3.037670373916626
Validation loss: 2.2561922816820044

Epoch: 6| Step: 5
Training loss: 2.2227251529693604
Validation loss: 2.239941668766801

Epoch: 6| Step: 6
Training loss: 2.8434014320373535
Validation loss: 2.2458034881981472

Epoch: 6| Step: 7
Training loss: 2.8750834465026855
Validation loss: 2.2454983624078895

Epoch: 6| Step: 8
Training loss: 2.211026668548584
Validation loss: 2.2531806525363716

Epoch: 6| Step: 9
Training loss: 2.1361567974090576
Validation loss: 2.2454728157289567

Epoch: 6| Step: 10
Training loss: 1.5648856163024902
Validation loss: 2.250944586210353

Epoch: 6| Step: 11
Training loss: 2.432490587234497
Validation loss: 2.2542393515186925

Epoch: 6| Step: 12
Training loss: 2.3952465057373047
Validation loss: 2.2469602361802132

Epoch: 6| Step: 13
Training loss: 3.1196353435516357
Validation loss: 2.2536555362004105

Epoch: 342| Step: 0
Training loss: 2.3657612800598145
Validation loss: 2.2474634724278606

Epoch: 6| Step: 1
Training loss: 2.0295770168304443
Validation loss: 2.2607022575152818

Epoch: 6| Step: 2
Training loss: 3.1644644737243652
Validation loss: 2.2381631687123287

Epoch: 6| Step: 3
Training loss: 2.432281732559204
Validation loss: 2.227543710380472

Epoch: 6| Step: 4
Training loss: 2.4565298557281494
Validation loss: 2.224111339097382

Epoch: 6| Step: 5
Training loss: 2.229278087615967
Validation loss: 2.198632355659239

Epoch: 6| Step: 6
Training loss: 1.919298529624939
Validation loss: 2.1909212348281697

Epoch: 6| Step: 7
Training loss: 2.2202274799346924
Validation loss: 2.1903044485276744

Epoch: 6| Step: 8
Training loss: 2.481940269470215
Validation loss: 2.189160475166895

Epoch: 6| Step: 9
Training loss: 2.2979369163513184
Validation loss: 2.1880209266498523

Epoch: 6| Step: 10
Training loss: 2.4351978302001953
Validation loss: 2.204064681965818

Epoch: 6| Step: 11
Training loss: 1.9658751487731934
Validation loss: 2.2020392533271544

Epoch: 6| Step: 12
Training loss: 2.6373252868652344
Validation loss: 2.219636001894551

Epoch: 6| Step: 13
Training loss: 1.3566536903381348
Validation loss: 2.2391577343786917

Epoch: 343| Step: 0
Training loss: 2.305659770965576
Validation loss: 2.2723915692298644

Epoch: 6| Step: 1
Training loss: 2.8010945320129395
Validation loss: 2.3017175915420696

Epoch: 6| Step: 2
Training loss: 1.876107096672058
Validation loss: 2.331640535785306

Epoch: 6| Step: 3
Training loss: 2.7435433864593506
Validation loss: 2.334246043236025

Epoch: 6| Step: 4
Training loss: 1.9321221113204956
Validation loss: 2.323547024880686

Epoch: 6| Step: 5
Training loss: 2.1220250129699707
Validation loss: 2.276420872698548

Epoch: 6| Step: 6
Training loss: 2.646411418914795
Validation loss: 2.257901721103217

Epoch: 6| Step: 7
Training loss: 2.9010848999023438
Validation loss: 2.261390442489296

Epoch: 6| Step: 8
Training loss: 2.68906831741333
Validation loss: 2.24425248945913

Epoch: 6| Step: 9
Training loss: 2.050762414932251
Validation loss: 2.2383489839492308

Epoch: 6| Step: 10
Training loss: 1.65736985206604
Validation loss: 2.227971410238615

Epoch: 6| Step: 11
Training loss: 2.6003096103668213
Validation loss: 2.234851224448091

Epoch: 6| Step: 12
Training loss: 1.895615816116333
Validation loss: 2.2250458937819286

Epoch: 6| Step: 13
Training loss: 2.240050792694092
Validation loss: 2.205858656155166

Epoch: 344| Step: 0
Training loss: 3.0822324752807617
Validation loss: 2.208973207781392

Epoch: 6| Step: 1
Training loss: 1.8736729621887207
Validation loss: 2.2034812768300376

Epoch: 6| Step: 2
Training loss: 2.7537145614624023
Validation loss: 2.219624247602237

Epoch: 6| Step: 3
Training loss: 2.452509880065918
Validation loss: 2.2076516997429634

Epoch: 6| Step: 4
Training loss: 1.9115443229675293
Validation loss: 2.2367679842056765

Epoch: 6| Step: 5
Training loss: 1.5643532276153564
Validation loss: 2.224878167593351

Epoch: 6| Step: 6
Training loss: 2.4162724018096924
Validation loss: 2.2417998621540685

Epoch: 6| Step: 7
Training loss: 3.190000295639038
Validation loss: 2.269589900970459

Epoch: 6| Step: 8
Training loss: 2.582643508911133
Validation loss: 2.267697658590091

Epoch: 6| Step: 9
Training loss: 2.100205183029175
Validation loss: 2.2658551046925206

Epoch: 6| Step: 10
Training loss: 2.133237838745117
Validation loss: 2.2772996528174287

Epoch: 6| Step: 11
Training loss: 1.4106621742248535
Validation loss: 2.2644010384877524

Epoch: 6| Step: 12
Training loss: 2.331218957901001
Validation loss: 2.2577483410476358

Epoch: 6| Step: 13
Training loss: 2.764882802963257
Validation loss: 2.259270401411159

Epoch: 345| Step: 0
Training loss: 2.1277973651885986
Validation loss: 2.245705002097673

Epoch: 6| Step: 1
Training loss: 2.7764644622802734
Validation loss: 2.239530001917193

Epoch: 6| Step: 2
Training loss: 2.5119175910949707
Validation loss: 2.2228193693263556

Epoch: 6| Step: 3
Training loss: 2.711205005645752
Validation loss: 2.2236101627349854

Epoch: 6| Step: 4
Training loss: 2.111513614654541
Validation loss: 2.2286545153587096

Epoch: 6| Step: 5
Training loss: 2.5115280151367188
Validation loss: 2.232851166878977

Epoch: 6| Step: 6
Training loss: 1.8807048797607422
Validation loss: 2.2423958445108063

Epoch: 6| Step: 7
Training loss: 2.279360771179199
Validation loss: 2.2487617897731003

Epoch: 6| Step: 8
Training loss: 2.196659803390503
Validation loss: 2.274566440172093

Epoch: 6| Step: 9
Training loss: 2.002480983734131
Validation loss: 2.265513800805615

Epoch: 6| Step: 10
Training loss: 3.0165700912475586
Validation loss: 2.2467051552188013

Epoch: 6| Step: 11
Training loss: 2.2911324501037598
Validation loss: 2.2492792478171726

Epoch: 6| Step: 12
Training loss: 1.8357789516448975
Validation loss: 2.2242765247180896

Epoch: 6| Step: 13
Training loss: 1.578265905380249
Validation loss: 2.2313375293567614

Epoch: 346| Step: 0
Training loss: 1.6857550144195557
Validation loss: 2.2216839175070486

Epoch: 6| Step: 1
Training loss: 2.344203472137451
Validation loss: 2.2180824202875935

Epoch: 6| Step: 2
Training loss: 2.3892202377319336
Validation loss: 2.2296478748321533

Epoch: 6| Step: 3
Training loss: 2.307803153991699
Validation loss: 2.227993305011462

Epoch: 6| Step: 4
Training loss: 2.639477252960205
Validation loss: 2.2363080529756445

Epoch: 6| Step: 5
Training loss: 2.2380166053771973
Validation loss: 2.2469945992192915

Epoch: 6| Step: 6
Training loss: 2.3807153701782227
Validation loss: 2.2442720961827103

Epoch: 6| Step: 7
Training loss: 2.097139835357666
Validation loss: 2.2508636264390844

Epoch: 6| Step: 8
Training loss: 2.5555169582366943
Validation loss: 2.2400882269746516

Epoch: 6| Step: 9
Training loss: 2.3257453441619873
Validation loss: 2.2254130071209324

Epoch: 6| Step: 10
Training loss: 2.8449933528900146
Validation loss: 2.2285593145637104

Epoch: 6| Step: 11
Training loss: 2.251616954803467
Validation loss: 2.2224628220322313

Epoch: 6| Step: 12
Training loss: 2.150195837020874
Validation loss: 2.2299983475797918

Epoch: 6| Step: 13
Training loss: 1.760029673576355
Validation loss: 2.239104704190326

Epoch: 347| Step: 0
Training loss: 2.722144603729248
Validation loss: 2.241962745625486

Epoch: 6| Step: 1
Training loss: 1.5133984088897705
Validation loss: 2.238980890602194

Epoch: 6| Step: 2
Training loss: 2.3606550693511963
Validation loss: 2.2330700107800063

Epoch: 6| Step: 3
Training loss: 2.122011184692383
Validation loss: 2.2652287303760485

Epoch: 6| Step: 4
Training loss: 2.3794901371002197
Validation loss: 2.2831387545472834

Epoch: 6| Step: 5
Training loss: 2.2549097537994385
Validation loss: 2.2703307238958215

Epoch: 6| Step: 6
Training loss: 2.3383889198303223
Validation loss: 2.277032116407989

Epoch: 6| Step: 7
Training loss: 1.9767236709594727
Validation loss: 2.2902365602472776

Epoch: 6| Step: 8
Training loss: 2.7948286533355713
Validation loss: 2.2603740025592107

Epoch: 6| Step: 9
Training loss: 3.2478179931640625
Validation loss: 2.2280602275684314

Epoch: 6| Step: 10
Training loss: 1.866037368774414
Validation loss: 2.226855754852295

Epoch: 6| Step: 11
Training loss: 1.9670555591583252
Validation loss: 2.2225854178910613

Epoch: 6| Step: 12
Training loss: 2.2578043937683105
Validation loss: 2.214784586301414

Epoch: 6| Step: 13
Training loss: 2.6160430908203125
Validation loss: 2.212902299819454

Epoch: 348| Step: 0
Training loss: 2.0434131622314453
Validation loss: 2.1998937104337957

Epoch: 6| Step: 1
Training loss: 2.0061566829681396
Validation loss: 2.184730993804111

Epoch: 6| Step: 2
Training loss: 2.8378372192382812
Validation loss: 2.18798594192792

Epoch: 6| Step: 3
Training loss: 3.0800886154174805
Validation loss: 2.191370940977527

Epoch: 6| Step: 4
Training loss: 2.162757635116577
Validation loss: 2.199142284290765

Epoch: 6| Step: 5
Training loss: 2.448859214782715
Validation loss: 2.1877927139241207

Epoch: 6| Step: 6
Training loss: 2.447389602661133
Validation loss: 2.206531575931016

Epoch: 6| Step: 7
Training loss: 1.5318204164505005
Validation loss: 2.2211694589225193

Epoch: 6| Step: 8
Training loss: 2.448442220687866
Validation loss: 2.265757688912012

Epoch: 6| Step: 9
Training loss: 1.8403496742248535
Validation loss: 2.2682346977213377

Epoch: 6| Step: 10
Training loss: 2.1730170249938965
Validation loss: 2.2799574713553152

Epoch: 6| Step: 11
Training loss: 2.6002540588378906
Validation loss: 2.314849733024515

Epoch: 6| Step: 12
Training loss: 2.38942813873291
Validation loss: 2.3352114667174635

Epoch: 6| Step: 13
Training loss: 2.596013069152832
Validation loss: 2.3084712272049277

Epoch: 349| Step: 0
Training loss: 2.204127311706543
Validation loss: 2.2670623615223873

Epoch: 6| Step: 1
Training loss: 2.339204788208008
Validation loss: 2.2357257412325953

Epoch: 6| Step: 2
Training loss: 1.8339999914169312
Validation loss: 2.219317360590863

Epoch: 6| Step: 3
Training loss: 2.902569532394409
Validation loss: 2.2034947013342254

Epoch: 6| Step: 4
Training loss: 2.3546338081359863
Validation loss: 2.1920600552712717

Epoch: 6| Step: 5
Training loss: 2.1849803924560547
Validation loss: 2.203504862323884

Epoch: 6| Step: 6
Training loss: 1.9525635242462158
Validation loss: 2.1935208920509583

Epoch: 6| Step: 7
Training loss: 2.09675931930542
Validation loss: 2.187438298297185

Epoch: 6| Step: 8
Training loss: 2.10831356048584
Validation loss: 2.188091749786049

Epoch: 6| Step: 9
Training loss: 2.8047268390655518
Validation loss: 2.191329417690154

Epoch: 6| Step: 10
Training loss: 2.069357395172119
Validation loss: 2.201382044822939

Epoch: 6| Step: 11
Training loss: 2.1094753742218018
Validation loss: 2.201617992052468

Epoch: 6| Step: 12
Training loss: 2.632613182067871
Validation loss: 2.2029956617662982

Epoch: 6| Step: 13
Training loss: 2.699174404144287
Validation loss: 2.2185659113750664

Epoch: 350| Step: 0
Training loss: 2.508796215057373
Validation loss: 2.201490922640729

Epoch: 6| Step: 1
Training loss: 2.448366165161133
Validation loss: 2.2054348299580235

Epoch: 6| Step: 2
Training loss: 2.3408560752868652
Validation loss: 2.2049432621207288

Epoch: 6| Step: 3
Training loss: 1.6412478685379028
Validation loss: 2.214247008805634

Epoch: 6| Step: 4
Training loss: 2.191481590270996
Validation loss: 2.2183753521211687

Epoch: 6| Step: 5
Training loss: 2.2288060188293457
Validation loss: 2.223704085555128

Epoch: 6| Step: 6
Training loss: 2.0654938220977783
Validation loss: 2.2315335068651425

Epoch: 6| Step: 7
Training loss: 2.0085718631744385
Validation loss: 2.2467241441049883

Epoch: 6| Step: 8
Training loss: 2.085151195526123
Validation loss: 2.258824790677717

Epoch: 6| Step: 9
Training loss: 2.2837729454040527
Validation loss: 2.27285034425797

Epoch: 6| Step: 10
Training loss: 2.7896909713745117
Validation loss: 2.2724788547844015

Epoch: 6| Step: 11
Training loss: 2.479389190673828
Validation loss: 2.278439293625534

Epoch: 6| Step: 12
Training loss: 2.332444190979004
Validation loss: 2.256930301907242

Epoch: 6| Step: 13
Training loss: 3.0170559883117676
Validation loss: 2.2579135151319605

Epoch: 351| Step: 0
Training loss: 2.8598480224609375
Validation loss: 2.25487772880062

Epoch: 6| Step: 1
Training loss: 2.8134193420410156
Validation loss: 2.239078028227693

Epoch: 6| Step: 2
Training loss: 2.047286033630371
Validation loss: 2.220724603181244

Epoch: 6| Step: 3
Training loss: 2.342386245727539
Validation loss: 2.2009424970995997

Epoch: 6| Step: 4
Training loss: 2.1797194480895996
Validation loss: 2.1976416200719853

Epoch: 6| Step: 5
Training loss: 2.5617566108703613
Validation loss: 2.184514109806348

Epoch: 6| Step: 6
Training loss: 2.6249160766601562
Validation loss: 2.187436903676679

Epoch: 6| Step: 7
Training loss: 2.7333645820617676
Validation loss: 2.1958077210252003

Epoch: 6| Step: 8
Training loss: 2.030315399169922
Validation loss: 2.1885547330302577

Epoch: 6| Step: 9
Training loss: 2.344731092453003
Validation loss: 2.2000589498909573

Epoch: 6| Step: 10
Training loss: 1.988558053970337
Validation loss: 2.218835864015805

Epoch: 6| Step: 11
Training loss: 1.7500097751617432
Validation loss: 2.217784015081262

Epoch: 6| Step: 12
Training loss: 1.411039113998413
Validation loss: 2.242734944948586

Epoch: 6| Step: 13
Training loss: 2.4406750202178955
Validation loss: 2.245307453217045

Epoch: 352| Step: 0
Training loss: 2.4243855476379395
Validation loss: 2.2362873861866612

Epoch: 6| Step: 1
Training loss: 2.223351001739502
Validation loss: 2.2409341002023346

Epoch: 6| Step: 2
Training loss: 1.8646353483200073
Validation loss: 2.2618058804542787

Epoch: 6| Step: 3
Training loss: 2.304643154144287
Validation loss: 2.256947933986623

Epoch: 6| Step: 4
Training loss: 2.892033815383911
Validation loss: 2.27510711198212

Epoch: 6| Step: 5
Training loss: 2.7305822372436523
Validation loss: 2.257280654804681

Epoch: 6| Step: 6
Training loss: 2.0775396823883057
Validation loss: 2.2398817462305867

Epoch: 6| Step: 7
Training loss: 2.477525234222412
Validation loss: 2.2227182862579182

Epoch: 6| Step: 8
Training loss: 2.422243595123291
Validation loss: 2.2225747364823536

Epoch: 6| Step: 9
Training loss: 1.3754303455352783
Validation loss: 2.209854313122329

Epoch: 6| Step: 10
Training loss: 2.0624563694000244
Validation loss: 2.1941181869917017

Epoch: 6| Step: 11
Training loss: 2.8480770587921143
Validation loss: 2.1965076205550984

Epoch: 6| Step: 12
Training loss: 1.5784175395965576
Validation loss: 2.187714775403341

Epoch: 6| Step: 13
Training loss: 2.8357722759246826
Validation loss: 2.1971359432384534

Epoch: 353| Step: 0
Training loss: 2.578800678253174
Validation loss: 2.1881486203080867

Epoch: 6| Step: 1
Training loss: 1.9740829467773438
Validation loss: 2.1837012306336434

Epoch: 6| Step: 2
Training loss: 1.8162786960601807
Validation loss: 2.1901489893595376

Epoch: 6| Step: 3
Training loss: 2.307504653930664
Validation loss: 2.196591951513803

Epoch: 6| Step: 4
Training loss: 2.329916000366211
Validation loss: 2.2020210681423062

Epoch: 6| Step: 5
Training loss: 2.4078116416931152
Validation loss: 2.2110050262943393

Epoch: 6| Step: 6
Training loss: 2.742420196533203
Validation loss: 2.2175028349763606

Epoch: 6| Step: 7
Training loss: 2.5548200607299805
Validation loss: 2.217306629303963

Epoch: 6| Step: 8
Training loss: 1.6491954326629639
Validation loss: 2.221506372574837

Epoch: 6| Step: 9
Training loss: 1.9469019174575806
Validation loss: 2.2176821244660245

Epoch: 6| Step: 10
Training loss: 2.686143636703491
Validation loss: 2.2189440881052325

Epoch: 6| Step: 11
Training loss: 2.075357437133789
Validation loss: 2.2140915022101453

Epoch: 6| Step: 12
Training loss: 1.7985504865646362
Validation loss: 2.2013628611000637

Epoch: 6| Step: 13
Training loss: 3.915449857711792
Validation loss: 2.195448881836348

Epoch: 354| Step: 0
Training loss: 2.077570915222168
Validation loss: 2.19208667355199

Epoch: 6| Step: 1
Training loss: 2.4095304012298584
Validation loss: 2.19787871965798

Epoch: 6| Step: 2
Training loss: 2.0032339096069336
Validation loss: 2.191434188555646

Epoch: 6| Step: 3
Training loss: 2.4628803730010986
Validation loss: 2.2016285388700423

Epoch: 6| Step: 4
Training loss: 2.7901885509490967
Validation loss: 2.1925344236435427

Epoch: 6| Step: 5
Training loss: 2.5576725006103516
Validation loss: 2.2061587431097545

Epoch: 6| Step: 6
Training loss: 1.700639247894287
Validation loss: 2.2140733285616805

Epoch: 6| Step: 7
Training loss: 2.337799310684204
Validation loss: 2.2013208148300007

Epoch: 6| Step: 8
Training loss: 1.3944451808929443
Validation loss: 2.218217367767006

Epoch: 6| Step: 9
Training loss: 3.025172233581543
Validation loss: 2.205763304105369

Epoch: 6| Step: 10
Training loss: 3.0071802139282227
Validation loss: 2.2276988157662014

Epoch: 6| Step: 11
Training loss: 2.3065645694732666
Validation loss: 2.2342168374728133

Epoch: 6| Step: 12
Training loss: 1.3246508836746216
Validation loss: 2.2389151921836277

Epoch: 6| Step: 13
Training loss: 2.5252926349639893
Validation loss: 2.2141286121901644

Epoch: 355| Step: 0
Training loss: 2.317647933959961
Validation loss: 2.2206107724097466

Epoch: 6| Step: 1
Training loss: 2.5680060386657715
Validation loss: 2.2373338873668382

Epoch: 6| Step: 2
Training loss: 1.8812437057495117
Validation loss: 2.2693080773917575

Epoch: 6| Step: 3
Training loss: 2.8987953662872314
Validation loss: 2.2619621189691688

Epoch: 6| Step: 4
Training loss: 1.8109827041625977
Validation loss: 2.2630774308276433

Epoch: 6| Step: 5
Training loss: 1.9259114265441895
Validation loss: 2.2500877957190237

Epoch: 6| Step: 6
Training loss: 2.176912307739258
Validation loss: 2.231905811576433

Epoch: 6| Step: 7
Training loss: 2.413209915161133
Validation loss: 2.2280871791224324

Epoch: 6| Step: 8
Training loss: 2.836648941040039
Validation loss: 2.229328029899187

Epoch: 6| Step: 9
Training loss: 2.402982473373413
Validation loss: 2.233870878014513

Epoch: 6| Step: 10
Training loss: 1.641956090927124
Validation loss: 2.2074788270458097

Epoch: 6| Step: 11
Training loss: 2.7763843536376953
Validation loss: 2.2136486294449016

Epoch: 6| Step: 12
Training loss: 2.092853546142578
Validation loss: 2.213542374231482

Epoch: 6| Step: 13
Training loss: 1.9147915840148926
Validation loss: 2.2240471814268377

Epoch: 356| Step: 0
Training loss: 3.007673740386963
Validation loss: 2.2144363900666595

Epoch: 6| Step: 1
Training loss: 2.1626551151275635
Validation loss: 2.213362670713855

Epoch: 6| Step: 2
Training loss: 1.5962507724761963
Validation loss: 2.20287412212741

Epoch: 6| Step: 3
Training loss: 2.20101261138916
Validation loss: 2.214709089648339

Epoch: 6| Step: 4
Training loss: 2.3459036350250244
Validation loss: 2.2042500947111394

Epoch: 6| Step: 5
Training loss: 2.3891761302948
Validation loss: 2.209639681282864

Epoch: 6| Step: 6
Training loss: 1.996561884880066
Validation loss: 2.2235294260004514

Epoch: 6| Step: 7
Training loss: 3.2661092281341553
Validation loss: 2.233831197984757

Epoch: 6| Step: 8
Training loss: 2.398221492767334
Validation loss: 2.2584472189667406

Epoch: 6| Step: 9
Training loss: 1.3572914600372314
Validation loss: 2.231003153708673

Epoch: 6| Step: 10
Training loss: 2.9029505252838135
Validation loss: 2.2391423307439333

Epoch: 6| Step: 11
Training loss: 2.6728811264038086
Validation loss: 2.205127146936232

Epoch: 6| Step: 12
Training loss: 1.5948493480682373
Validation loss: 2.1974722390533774

Epoch: 6| Step: 13
Training loss: 1.9456857442855835
Validation loss: 2.1891666561044674

Epoch: 357| Step: 0
Training loss: 1.8170139789581299
Validation loss: 2.185278936098981

Epoch: 6| Step: 1
Training loss: 1.6788749694824219
Validation loss: 2.1868683343292563

Epoch: 6| Step: 2
Training loss: 2.434748649597168
Validation loss: 2.178090815903038

Epoch: 6| Step: 3
Training loss: 2.368502140045166
Validation loss: 2.1943623506894676

Epoch: 6| Step: 4
Training loss: 2.467524528503418
Validation loss: 2.1820943996470463

Epoch: 6| Step: 5
Training loss: 2.7384276390075684
Validation loss: 2.182077402709633

Epoch: 6| Step: 6
Training loss: 2.5657057762145996
Validation loss: 2.1878634165692072

Epoch: 6| Step: 7
Training loss: 2.337651252746582
Validation loss: 2.1956486112327984

Epoch: 6| Step: 8
Training loss: 2.501204252243042
Validation loss: 2.2021065040301253

Epoch: 6| Step: 9
Training loss: 1.9963024854660034
Validation loss: 2.193836368540282

Epoch: 6| Step: 10
Training loss: 2.609823226928711
Validation loss: 2.2045460516406643

Epoch: 6| Step: 11
Training loss: 2.033480167388916
Validation loss: 2.182367597856829

Epoch: 6| Step: 12
Training loss: 2.2834157943725586
Validation loss: 2.194741934858343

Epoch: 6| Step: 13
Training loss: 1.7633476257324219
Validation loss: 2.18397787309462

Epoch: 358| Step: 0
Training loss: 2.05708646774292
Validation loss: 2.1969049322989678

Epoch: 6| Step: 1
Training loss: 1.794846534729004
Validation loss: 2.2014374425334315

Epoch: 6| Step: 2
Training loss: 2.4329943656921387
Validation loss: 2.188351438891503

Epoch: 6| Step: 3
Training loss: 2.539214611053467
Validation loss: 2.189800202205617

Epoch: 6| Step: 4
Training loss: 1.9905424118041992
Validation loss: 2.2112918925541702

Epoch: 6| Step: 5
Training loss: 2.622267961502075
Validation loss: 2.1965616531269525

Epoch: 6| Step: 6
Training loss: 2.716423511505127
Validation loss: 2.1948774783842024

Epoch: 6| Step: 7
Training loss: 2.7057533264160156
Validation loss: 2.1944762276064966

Epoch: 6| Step: 8
Training loss: 1.8366260528564453
Validation loss: 2.1936820040466967

Epoch: 6| Step: 9
Training loss: 2.459228038787842
Validation loss: 2.2124229964389595

Epoch: 6| Step: 10
Training loss: 1.5984313488006592
Validation loss: 2.2038871985609814

Epoch: 6| Step: 11
Training loss: 2.7080323696136475
Validation loss: 2.204453186322284

Epoch: 6| Step: 12
Training loss: 1.8192986249923706
Validation loss: 2.2030710020372943

Epoch: 6| Step: 13
Training loss: 2.7166388034820557
Validation loss: 2.210627786574825

Epoch: 359| Step: 0
Training loss: 2.1846179962158203
Validation loss: 2.2222481722472818

Epoch: 6| Step: 1
Training loss: 2.628171443939209
Validation loss: 2.244117577870687

Epoch: 6| Step: 2
Training loss: 1.9077696800231934
Validation loss: 2.260963911651283

Epoch: 6| Step: 3
Training loss: 1.9325257539749146
Validation loss: 2.2610594072649555

Epoch: 6| Step: 4
Training loss: 2.4040067195892334
Validation loss: 2.2494768019645446

Epoch: 6| Step: 5
Training loss: 1.8313581943511963
Validation loss: 2.2535771246879333

Epoch: 6| Step: 6
Training loss: 2.2237958908081055
Validation loss: 2.2487617538821314

Epoch: 6| Step: 7
Training loss: 2.3750851154327393
Validation loss: 2.226948171533564

Epoch: 6| Step: 8
Training loss: 2.4051713943481445
Validation loss: 2.213493844514252

Epoch: 6| Step: 9
Training loss: 1.710538387298584
Validation loss: 2.198398126068936

Epoch: 6| Step: 10
Training loss: 2.4625496864318848
Validation loss: 2.202410310827276

Epoch: 6| Step: 11
Training loss: 2.4980807304382324
Validation loss: 2.2029768472076743

Epoch: 6| Step: 12
Training loss: 2.735006809234619
Validation loss: 2.2038792025658394

Epoch: 6| Step: 13
Training loss: 2.627755880355835
Validation loss: 2.215344809716748

Epoch: 360| Step: 0
Training loss: 2.0172414779663086
Validation loss: 2.1957121741387153

Epoch: 6| Step: 1
Training loss: 1.4325718879699707
Validation loss: 2.1989176427164385

Epoch: 6| Step: 2
Training loss: 2.8821239471435547
Validation loss: 2.215204777256135

Epoch: 6| Step: 3
Training loss: 2.4747023582458496
Validation loss: 2.2033039780073267

Epoch: 6| Step: 4
Training loss: 2.052165985107422
Validation loss: 2.2280734431359077

Epoch: 6| Step: 5
Training loss: 1.9223482608795166
Validation loss: 2.2204875958863126

Epoch: 6| Step: 6
Training loss: 2.4479713439941406
Validation loss: 2.230885842795013

Epoch: 6| Step: 7
Training loss: 2.3335556983947754
Validation loss: 2.2405422631130425

Epoch: 6| Step: 8
Training loss: 1.2554861307144165
Validation loss: 2.2557889133371334

Epoch: 6| Step: 9
Training loss: 3.0822207927703857
Validation loss: 2.2775459840733516

Epoch: 6| Step: 10
Training loss: 2.1191816329956055
Validation loss: 2.270829512226966

Epoch: 6| Step: 11
Training loss: 2.6842873096466064
Validation loss: 2.2558265373271

Epoch: 6| Step: 12
Training loss: 2.601778507232666
Validation loss: 2.2543140547249907

Epoch: 6| Step: 13
Training loss: 2.5709593296051025
Validation loss: 2.2504216932481333

Epoch: 361| Step: 0
Training loss: 1.8778369426727295
Validation loss: 2.2356791778277327

Epoch: 6| Step: 1
Training loss: 2.525603771209717
Validation loss: 2.2222865063657045

Epoch: 6| Step: 2
Training loss: 1.97029447555542
Validation loss: 2.2142598910998275

Epoch: 6| Step: 3
Training loss: 2.666862726211548
Validation loss: 2.221435236674483

Epoch: 6| Step: 4
Training loss: 2.773134231567383
Validation loss: 2.1990488626623668

Epoch: 6| Step: 5
Training loss: 2.9130234718322754
Validation loss: 2.1826545897350518

Epoch: 6| Step: 6
Training loss: 2.078275680541992
Validation loss: 2.1824820810748684

Epoch: 6| Step: 7
Training loss: 1.8829737901687622
Validation loss: 2.1757902394058886

Epoch: 6| Step: 8
Training loss: 2.628357172012329
Validation loss: 2.1874143026208364

Epoch: 6| Step: 9
Training loss: 1.3972773551940918
Validation loss: 2.1842223341746996

Epoch: 6| Step: 10
Training loss: 2.3137707710266113
Validation loss: 2.178331959632135

Epoch: 6| Step: 11
Training loss: 2.297029972076416
Validation loss: 2.1994963512625745

Epoch: 6| Step: 12
Training loss: 1.918067455291748
Validation loss: 2.205011990762526

Epoch: 6| Step: 13
Training loss: 2.4175310134887695
Validation loss: 2.2081251400773243

Epoch: 362| Step: 0
Training loss: 1.8565998077392578
Validation loss: 2.2394367289799515

Epoch: 6| Step: 1
Training loss: 2.2377724647521973
Validation loss: 2.229454436609822

Epoch: 6| Step: 2
Training loss: 2.3531265258789062
Validation loss: 2.2250545819600425

Epoch: 6| Step: 3
Training loss: 3.1464812755584717
Validation loss: 2.233564684467931

Epoch: 6| Step: 4
Training loss: 2.3468570709228516
Validation loss: 2.2231167670219176

Epoch: 6| Step: 5
Training loss: 1.600376009941101
Validation loss: 2.2048529091701714

Epoch: 6| Step: 6
Training loss: 1.4760760068893433
Validation loss: 2.1892624350004297

Epoch: 6| Step: 7
Training loss: 2.044506549835205
Validation loss: 2.193456442125382

Epoch: 6| Step: 8
Training loss: 2.995147228240967
Validation loss: 2.1972644495707687

Epoch: 6| Step: 9
Training loss: 2.502638816833496
Validation loss: 2.19216162671325

Epoch: 6| Step: 10
Training loss: 2.4656152725219727
Validation loss: 2.19098412606024

Epoch: 6| Step: 11
Training loss: 2.575479507446289
Validation loss: 2.1912475965356313

Epoch: 6| Step: 12
Training loss: 2.320077419281006
Validation loss: 2.208219789689587

Epoch: 6| Step: 13
Training loss: 1.369545340538025
Validation loss: 2.1977352993462675

Epoch: 363| Step: 0
Training loss: 2.7643275260925293
Validation loss: 2.220045207649149

Epoch: 6| Step: 1
Training loss: 2.39835786819458
Validation loss: 2.236288837207261

Epoch: 6| Step: 2
Training loss: 2.957420587539673
Validation loss: 2.2635947427442

Epoch: 6| Step: 3
Training loss: 2.1043128967285156
Validation loss: 2.268629415060884

Epoch: 6| Step: 4
Training loss: 2.5966901779174805
Validation loss: 2.2591644205072874

Epoch: 6| Step: 5
Training loss: 1.3296139240264893
Validation loss: 2.2576104287178285

Epoch: 6| Step: 6
Training loss: 2.655233144760132
Validation loss: 2.2448124295921734

Epoch: 6| Step: 7
Training loss: 2.0475053787231445
Validation loss: 2.226896967939151

Epoch: 6| Step: 8
Training loss: 1.9648463726043701
Validation loss: 2.20488239360112

Epoch: 6| Step: 9
Training loss: 2.7998600006103516
Validation loss: 2.182099183400472

Epoch: 6| Step: 10
Training loss: 2.1885101795196533
Validation loss: 2.1841043579962944

Epoch: 6| Step: 11
Training loss: 2.0724806785583496
Validation loss: 2.18169665849337

Epoch: 6| Step: 12
Training loss: 1.9375537633895874
Validation loss: 2.181138594945272

Epoch: 6| Step: 13
Training loss: 1.3414075374603271
Validation loss: 2.1873406440980974

Epoch: 364| Step: 0
Training loss: 2.7383980751037598
Validation loss: 2.200168748055735

Epoch: 6| Step: 1
Training loss: 1.8237252235412598
Validation loss: 2.2102307888769333

Epoch: 6| Step: 2
Training loss: 2.1370601654052734
Validation loss: 2.2145588833798646

Epoch: 6| Step: 3
Training loss: 1.3936944007873535
Validation loss: 2.2436814269711896

Epoch: 6| Step: 4
Training loss: 3.185006618499756
Validation loss: 2.26891279092399

Epoch: 6| Step: 5
Training loss: 2.092285633087158
Validation loss: 2.282993821687596

Epoch: 6| Step: 6
Training loss: 2.8000597953796387
Validation loss: 2.2767675512580463

Epoch: 6| Step: 7
Training loss: 2.6835474967956543
Validation loss: 2.2809454882016746

Epoch: 6| Step: 8
Training loss: 2.098658800125122
Validation loss: 2.282376535477177

Epoch: 6| Step: 9
Training loss: 1.8818671703338623
Validation loss: 2.272466641600414

Epoch: 6| Step: 10
Training loss: 2.185737133026123
Validation loss: 2.2588243817770355

Epoch: 6| Step: 11
Training loss: 2.3493218421936035
Validation loss: 2.254246042620751

Epoch: 6| Step: 12
Training loss: 2.1248419284820557
Validation loss: 2.2160196893958637

Epoch: 6| Step: 13
Training loss: 2.0863919258117676
Validation loss: 2.205474325405654

Epoch: 365| Step: 0
Training loss: 2.3436713218688965
Validation loss: 2.2046254014456146

Epoch: 6| Step: 1
Training loss: 2.8748624324798584
Validation loss: 2.19930124539201

Epoch: 6| Step: 2
Training loss: 2.462921142578125
Validation loss: 2.1803590943736415

Epoch: 6| Step: 3
Training loss: 2.310695171356201
Validation loss: 2.168345102699854

Epoch: 6| Step: 4
Training loss: 1.806031584739685
Validation loss: 2.175540647199077

Epoch: 6| Step: 5
Training loss: 2.535392999649048
Validation loss: 2.1813572324732298

Epoch: 6| Step: 6
Training loss: 1.6835781335830688
Validation loss: 2.1966506768298406

Epoch: 6| Step: 7
Training loss: 2.7036619186401367
Validation loss: 2.1883502621804514

Epoch: 6| Step: 8
Training loss: 2.5665125846862793
Validation loss: 2.1977877411791074

Epoch: 6| Step: 9
Training loss: 1.957800030708313
Validation loss: 2.1997561288136307

Epoch: 6| Step: 10
Training loss: 2.469599962234497
Validation loss: 2.247053769326979

Epoch: 6| Step: 11
Training loss: 2.5656182765960693
Validation loss: 2.2304639098464802

Epoch: 6| Step: 12
Training loss: 1.1293699741363525
Validation loss: 2.2229348357005785

Epoch: 6| Step: 13
Training loss: 1.7835832834243774
Validation loss: 2.249670079959336

Epoch: 366| Step: 0
Training loss: 2.8512935638427734
Validation loss: 2.223100495594804

Epoch: 6| Step: 1
Training loss: 2.7664523124694824
Validation loss: 2.239080282949632

Epoch: 6| Step: 2
Training loss: 3.0541043281555176
Validation loss: 2.2005239455930647

Epoch: 6| Step: 3
Training loss: 2.237720012664795
Validation loss: 2.1991229185494046

Epoch: 6| Step: 4
Training loss: 2.251972198486328
Validation loss: 2.197016171229783

Epoch: 6| Step: 5
Training loss: 2.215439796447754
Validation loss: 2.2072701556708223

Epoch: 6| Step: 6
Training loss: 1.8887097835540771
Validation loss: 2.2107573375906995

Epoch: 6| Step: 7
Training loss: 1.6986827850341797
Validation loss: 2.2099349575658

Epoch: 6| Step: 8
Training loss: 2.014293909072876
Validation loss: 2.2199620995470273

Epoch: 6| Step: 9
Training loss: 1.7865320444107056
Validation loss: 2.215403283796003

Epoch: 6| Step: 10
Training loss: 2.1380748748779297
Validation loss: 2.206647737051851

Epoch: 6| Step: 11
Training loss: 2.1960878372192383
Validation loss: 2.1908496323452202

Epoch: 6| Step: 12
Training loss: 2.196880340576172
Validation loss: 2.202998113888566

Epoch: 6| Step: 13
Training loss: 1.9883122444152832
Validation loss: 2.2276969289266937

Epoch: 367| Step: 0
Training loss: 2.184157371520996
Validation loss: 2.2495918684108283

Epoch: 6| Step: 1
Training loss: 2.349777936935425
Validation loss: 2.2598675630425893

Epoch: 6| Step: 2
Training loss: 1.917548418045044
Validation loss: 2.2463599994618404

Epoch: 6| Step: 3
Training loss: 2.320286750793457
Validation loss: 2.2423208554585776

Epoch: 6| Step: 4
Training loss: 2.52079439163208
Validation loss: 2.2433199792779903

Epoch: 6| Step: 5
Training loss: 2.7439279556274414
Validation loss: 2.217353981028321

Epoch: 6| Step: 6
Training loss: 1.2463350296020508
Validation loss: 2.2253922211226596

Epoch: 6| Step: 7
Training loss: 2.5936269760131836
Validation loss: 2.19437018645707

Epoch: 6| Step: 8
Training loss: 2.4771628379821777
Validation loss: 2.19509736440515

Epoch: 6| Step: 9
Training loss: 3.0481786727905273
Validation loss: 2.1886185830639255

Epoch: 6| Step: 10
Training loss: 2.027228593826294
Validation loss: 2.1736943670498428

Epoch: 6| Step: 11
Training loss: 1.9408916234970093
Validation loss: 2.189123494650728

Epoch: 6| Step: 12
Training loss: 2.021280288696289
Validation loss: 2.18584022983428

Epoch: 6| Step: 13
Training loss: 1.9123464822769165
Validation loss: 2.19024561297509

Epoch: 368| Step: 0
Training loss: 2.3500583171844482
Validation loss: 2.223935293894942

Epoch: 6| Step: 1
Training loss: 3.018049716949463
Validation loss: 2.2078040261422434

Epoch: 6| Step: 2
Training loss: 1.9462764263153076
Validation loss: 2.1977410393376506

Epoch: 6| Step: 3
Training loss: 2.2692742347717285
Validation loss: 2.191784458775674

Epoch: 6| Step: 4
Training loss: 2.0560193061828613
Validation loss: 2.204498960125831

Epoch: 6| Step: 5
Training loss: 3.426647186279297
Validation loss: 2.1941911610223914

Epoch: 6| Step: 6
Training loss: 2.2679736614227295
Validation loss: 2.2103406280599613

Epoch: 6| Step: 7
Training loss: 2.499943971633911
Validation loss: 2.191339651743571

Epoch: 6| Step: 8
Training loss: 2.05171537399292
Validation loss: 2.1967834516238143

Epoch: 6| Step: 9
Training loss: 2.138252019882202
Validation loss: 2.197475935823174

Epoch: 6| Step: 10
Training loss: 1.6192281246185303
Validation loss: 2.1788351638342744

Epoch: 6| Step: 11
Training loss: 1.5474226474761963
Validation loss: 2.201282552493516

Epoch: 6| Step: 12
Training loss: 1.8474140167236328
Validation loss: 2.190570891544383

Epoch: 6| Step: 13
Training loss: 2.089752674102783
Validation loss: 2.18600046250128

Epoch: 369| Step: 0
Training loss: 2.273752450942993
Validation loss: 2.18724920416391

Epoch: 6| Step: 1
Training loss: 2.6606945991516113
Validation loss: 2.184346681000084

Epoch: 6| Step: 2
Training loss: 2.1365365982055664
Validation loss: 2.2009556190941924

Epoch: 6| Step: 3
Training loss: 2.717881202697754
Validation loss: 2.1921368286173832

Epoch: 6| Step: 4
Training loss: 2.3020832538604736
Validation loss: 2.177882135555308

Epoch: 6| Step: 5
Training loss: 1.6492030620574951
Validation loss: 2.1788551858676377

Epoch: 6| Step: 6
Training loss: 1.8839138746261597
Validation loss: 2.1870504963782524

Epoch: 6| Step: 7
Training loss: 1.722243309020996
Validation loss: 2.194588392011581

Epoch: 6| Step: 8
Training loss: 2.4409379959106445
Validation loss: 2.202379616357947

Epoch: 6| Step: 9
Training loss: 2.414928913116455
Validation loss: 2.2069166937182025

Epoch: 6| Step: 10
Training loss: 2.6210074424743652
Validation loss: 2.217340564215055

Epoch: 6| Step: 11
Training loss: 1.794981837272644
Validation loss: 2.2452555933306293

Epoch: 6| Step: 12
Training loss: 1.8612651824951172
Validation loss: 2.212873311452968

Epoch: 6| Step: 13
Training loss: 3.039900779724121
Validation loss: 2.2134908860729587

Epoch: 370| Step: 0
Training loss: 2.4950461387634277
Validation loss: 2.2171461082273916

Epoch: 6| Step: 1
Training loss: 1.806136965751648
Validation loss: 2.2021312739259455

Epoch: 6| Step: 2
Training loss: 2.3198962211608887
Validation loss: 2.1874266055322464

Epoch: 6| Step: 3
Training loss: 2.127857208251953
Validation loss: 2.200560733836184

Epoch: 6| Step: 4
Training loss: 2.399322986602783
Validation loss: 2.191562351360116

Epoch: 6| Step: 5
Training loss: 1.9589405059814453
Validation loss: 2.185539669887994

Epoch: 6| Step: 6
Training loss: 2.5201501846313477
Validation loss: 2.1985916424823064

Epoch: 6| Step: 7
Training loss: 1.0241539478302002
Validation loss: 2.1918984023473596

Epoch: 6| Step: 8
Training loss: 2.9580836296081543
Validation loss: 2.2291333983021397

Epoch: 6| Step: 9
Training loss: 2.8400001525878906
Validation loss: 2.2469664209632465

Epoch: 6| Step: 10
Training loss: 1.7567214965820312
Validation loss: 2.2467513956049436

Epoch: 6| Step: 11
Training loss: 2.1149024963378906
Validation loss: 2.2760302584658385

Epoch: 6| Step: 12
Training loss: 2.846952199935913
Validation loss: 2.2678542726783344

Epoch: 6| Step: 13
Training loss: 2.1733896732330322
Validation loss: 2.23376271288882

Epoch: 371| Step: 0
Training loss: 2.593160629272461
Validation loss: 2.2185973723729453

Epoch: 6| Step: 1
Training loss: 2.5897281169891357
Validation loss: 2.213876567861085

Epoch: 6| Step: 2
Training loss: 2.3509368896484375
Validation loss: 2.1833046149182063

Epoch: 6| Step: 3
Training loss: 2.1004066467285156
Validation loss: 2.175117100438764

Epoch: 6| Step: 4
Training loss: 2.109623908996582
Validation loss: 2.1503305435180664

Epoch: 6| Step: 5
Training loss: 2.0613925457000732
Validation loss: 2.157777400426967

Epoch: 6| Step: 6
Training loss: 1.911059021949768
Validation loss: 2.1578020331680134

Epoch: 6| Step: 7
Training loss: 1.860526204109192
Validation loss: 2.1635438601175943

Epoch: 6| Step: 8
Training loss: 2.2240471839904785
Validation loss: 2.1713430689227198

Epoch: 6| Step: 9
Training loss: 3.0176615715026855
Validation loss: 2.1737898472816712

Epoch: 6| Step: 10
Training loss: 2.385812759399414
Validation loss: 2.173732142294607

Epoch: 6| Step: 11
Training loss: 1.9305403232574463
Validation loss: 2.1911576537675757

Epoch: 6| Step: 12
Training loss: 1.945020318031311
Validation loss: 2.1938595617971113

Epoch: 6| Step: 13
Training loss: 2.1810402870178223
Validation loss: 2.200200726909022

Epoch: 372| Step: 0
Training loss: 2.079233169555664
Validation loss: 2.1962445474440053

Epoch: 6| Step: 1
Training loss: 1.6140897274017334
Validation loss: 2.1907100831308672

Epoch: 6| Step: 2
Training loss: 1.9561042785644531
Validation loss: 2.217872042809763

Epoch: 6| Step: 3
Training loss: 2.225620985031128
Validation loss: 2.2146707068207445

Epoch: 6| Step: 4
Training loss: 2.5187249183654785
Validation loss: 2.2238966829033306

Epoch: 6| Step: 5
Training loss: 2.507768154144287
Validation loss: 2.243614081413515

Epoch: 6| Step: 6
Training loss: 1.8095676898956299
Validation loss: 2.245652985829179

Epoch: 6| Step: 7
Training loss: 2.636381149291992
Validation loss: 2.2380774841513684

Epoch: 6| Step: 8
Training loss: 2.893372058868408
Validation loss: 2.229992474279096

Epoch: 6| Step: 9
Training loss: 1.850647211074829
Validation loss: 2.2014784761654433

Epoch: 6| Step: 10
Training loss: 1.61173415184021
Validation loss: 2.1853703914150113

Epoch: 6| Step: 11
Training loss: 2.6956427097320557
Validation loss: 2.1769559742302023

Epoch: 6| Step: 12
Training loss: 1.9169049263000488
Validation loss: 2.1769590941808556

Epoch: 6| Step: 13
Training loss: 3.311624765396118
Validation loss: 2.1619011766167096

Epoch: 373| Step: 0
Training loss: 1.6442389488220215
Validation loss: 2.169862461346452

Epoch: 6| Step: 1
Training loss: 2.6682863235473633
Validation loss: 2.1644095682328746

Epoch: 6| Step: 2
Training loss: 1.6509528160095215
Validation loss: 2.1681338458932857

Epoch: 6| Step: 3
Training loss: 1.7183504104614258
Validation loss: 2.172773556042743

Epoch: 6| Step: 4
Training loss: 2.8590898513793945
Validation loss: 2.174721658870738

Epoch: 6| Step: 5
Training loss: 2.6508071422576904
Validation loss: 2.173523251728345

Epoch: 6| Step: 6
Training loss: 2.904695749282837
Validation loss: 2.157041895774103

Epoch: 6| Step: 7
Training loss: 2.6914520263671875
Validation loss: 2.166389546086711

Epoch: 6| Step: 8
Training loss: 2.8373637199401855
Validation loss: 2.179961105828644

Epoch: 6| Step: 9
Training loss: 2.240356922149658
Validation loss: 2.192477157039027

Epoch: 6| Step: 10
Training loss: 1.9236403703689575
Validation loss: 2.188451579822007

Epoch: 6| Step: 11
Training loss: 1.0292878150939941
Validation loss: 2.2187371741059008

Epoch: 6| Step: 12
Training loss: 2.0866410732269287
Validation loss: 2.2814826542331326

Epoch: 6| Step: 13
Training loss: 2.5267739295959473
Validation loss: 2.253996536295901

Epoch: 374| Step: 0
Training loss: 1.9365613460540771
Validation loss: 2.2487780048001196

Epoch: 6| Step: 1
Training loss: 2.199781894683838
Validation loss: 2.2119791302629697

Epoch: 6| Step: 2
Training loss: 2.4782378673553467
Validation loss: 2.1765513958469516

Epoch: 6| Step: 3
Training loss: 1.9373294115066528
Validation loss: 2.149440957653907

Epoch: 6| Step: 4
Training loss: 2.3636436462402344
Validation loss: 2.1443391871708695

Epoch: 6| Step: 5
Training loss: 3.053471088409424
Validation loss: 2.1566772768574376

Epoch: 6| Step: 6
Training loss: 1.6905972957611084
Validation loss: 2.140271004810128

Epoch: 6| Step: 7
Training loss: 2.2748911380767822
Validation loss: 2.144301358089652

Epoch: 6| Step: 8
Training loss: 2.173426866531372
Validation loss: 2.144666317970522

Epoch: 6| Step: 9
Training loss: 1.8663606643676758
Validation loss: 2.1576656269770798

Epoch: 6| Step: 10
Training loss: 2.37515926361084
Validation loss: 2.1493725328035254

Epoch: 6| Step: 11
Training loss: 2.3866236209869385
Validation loss: 2.1680839369373937

Epoch: 6| Step: 12
Training loss: 2.1615965366363525
Validation loss: 2.1969681196315314

Epoch: 6| Step: 13
Training loss: 2.5184168815612793
Validation loss: 2.204823527284848

Epoch: 375| Step: 0
Training loss: 1.4877070188522339
Validation loss: 2.219082263208205

Epoch: 6| Step: 1
Training loss: 2.085480213165283
Validation loss: 2.2190009496545278

Epoch: 6| Step: 2
Training loss: 2.164844036102295
Validation loss: 2.2410009958410777

Epoch: 6| Step: 3
Training loss: 2.090355396270752
Validation loss: 2.241410170831988

Epoch: 6| Step: 4
Training loss: 1.921613097190857
Validation loss: 2.259999144461847

Epoch: 6| Step: 5
Training loss: 2.769975185394287
Validation loss: 2.2542276561901136

Epoch: 6| Step: 6
Training loss: 2.3241257667541504
Validation loss: 2.2381656733892297

Epoch: 6| Step: 7
Training loss: 2.745454788208008
Validation loss: 2.2231846676077893

Epoch: 6| Step: 8
Training loss: 1.8308238983154297
Validation loss: 2.203422369495515

Epoch: 6| Step: 9
Training loss: 3.406735897064209
Validation loss: 2.198449429645333

Epoch: 6| Step: 10
Training loss: 2.427727699279785
Validation loss: 2.1803353140431065

Epoch: 6| Step: 11
Training loss: 2.41523814201355
Validation loss: 2.176928953457904

Epoch: 6| Step: 12
Training loss: 1.6887658834457397
Validation loss: 2.1620745479419665

Epoch: 6| Step: 13
Training loss: 1.2046658992767334
Validation loss: 2.1750154085056757

Epoch: 376| Step: 0
Training loss: 1.857201099395752
Validation loss: 2.1562616312375633

Epoch: 6| Step: 1
Training loss: 1.6684017181396484
Validation loss: 2.1523118096013225

Epoch: 6| Step: 2
Training loss: 1.873578429222107
Validation loss: 2.1616972518223587

Epoch: 6| Step: 3
Training loss: 3.0030994415283203
Validation loss: 2.1548294277601343

Epoch: 6| Step: 4
Training loss: 1.6292864084243774
Validation loss: 2.164182586054648

Epoch: 6| Step: 5
Training loss: 1.9695303440093994
Validation loss: 2.1507353359653103

Epoch: 6| Step: 6
Training loss: 2.872408390045166
Validation loss: 2.1656397260645384

Epoch: 6| Step: 7
Training loss: 2.590071678161621
Validation loss: 2.1739383256563576

Epoch: 6| Step: 8
Training loss: 1.9107180833816528
Validation loss: 2.1969743159509476

Epoch: 6| Step: 9
Training loss: 2.510181427001953
Validation loss: 2.213546847784391

Epoch: 6| Step: 10
Training loss: 2.0145931243896484
Validation loss: 2.2025620706619753

Epoch: 6| Step: 11
Training loss: 1.4247435331344604
Validation loss: 2.219768703624766

Epoch: 6| Step: 12
Training loss: 3.223175048828125
Validation loss: 2.2131464173716884

Epoch: 6| Step: 13
Training loss: 2.661579132080078
Validation loss: 2.195085875449642

Epoch: 377| Step: 0
Training loss: 2.750115394592285
Validation loss: 2.1956574122111

Epoch: 6| Step: 1
Training loss: 1.9603105783462524
Validation loss: 2.199444627249113

Epoch: 6| Step: 2
Training loss: 2.2483348846435547
Validation loss: 2.2098344243982786

Epoch: 6| Step: 3
Training loss: 1.9354217052459717
Validation loss: 2.204478550982732

Epoch: 6| Step: 4
Training loss: 2.0016965866088867
Validation loss: 2.196387065354214

Epoch: 6| Step: 5
Training loss: 2.312937021255493
Validation loss: 2.1879635498087895

Epoch: 6| Step: 6
Training loss: 2.21458101272583
Validation loss: 2.1667554429782334

Epoch: 6| Step: 7
Training loss: 2.3605518341064453
Validation loss: 2.1558631568826656

Epoch: 6| Step: 8
Training loss: 2.3910675048828125
Validation loss: 2.1552371132758354

Epoch: 6| Step: 9
Training loss: 1.6108170747756958
Validation loss: 2.1499364696523195

Epoch: 6| Step: 10
Training loss: 2.3596878051757812
Validation loss: 2.152378333512173

Epoch: 6| Step: 11
Training loss: 2.0637600421905518
Validation loss: 2.163850668937929

Epoch: 6| Step: 12
Training loss: 2.383681535720825
Validation loss: 2.1595920478144

Epoch: 6| Step: 13
Training loss: 2.4150047302246094
Validation loss: 2.1618171661130843

Epoch: 378| Step: 0
Training loss: 2.1099557876586914
Validation loss: 2.176423047178535

Epoch: 6| Step: 1
Training loss: 1.9677306413650513
Validation loss: 2.169095067567723

Epoch: 6| Step: 2
Training loss: 2.6351795196533203
Validation loss: 2.2014445335634294

Epoch: 6| Step: 3
Training loss: 2.3713669776916504
Validation loss: 2.184021429349017

Epoch: 6| Step: 4
Training loss: 1.9909917116165161
Validation loss: 2.210759065484488

Epoch: 6| Step: 5
Training loss: 2.036306619644165
Validation loss: 2.195530750418222

Epoch: 6| Step: 6
Training loss: 3.1233227252960205
Validation loss: 2.2223821583614556

Epoch: 6| Step: 7
Training loss: 1.2916803359985352
Validation loss: 2.2394867379178285

Epoch: 6| Step: 8
Training loss: 2.253840446472168
Validation loss: 2.2382633327155985

Epoch: 6| Step: 9
Training loss: 2.6102757453918457
Validation loss: 2.2510390281677246

Epoch: 6| Step: 10
Training loss: 1.6936039924621582
Validation loss: 2.241782375561294

Epoch: 6| Step: 11
Training loss: 2.0909299850463867
Validation loss: 2.233454829903059

Epoch: 6| Step: 12
Training loss: 2.1906790733337402
Validation loss: 2.2147726166632866

Epoch: 6| Step: 13
Training loss: 3.0329103469848633
Validation loss: 2.2166969135243404

Epoch: 379| Step: 0
Training loss: 2.4245898723602295
Validation loss: 2.187307470588274

Epoch: 6| Step: 1
Training loss: 2.5338315963745117
Validation loss: 2.2015713325110813

Epoch: 6| Step: 2
Training loss: 2.4303743839263916
Validation loss: 2.1912909297532934

Epoch: 6| Step: 3
Training loss: 2.5464859008789062
Validation loss: 2.193917653893912

Epoch: 6| Step: 4
Training loss: 2.2066807746887207
Validation loss: 2.190314772308514

Epoch: 6| Step: 5
Training loss: 2.0676984786987305
Validation loss: 2.1826734414664646

Epoch: 6| Step: 6
Training loss: 1.7683767080307007
Validation loss: 2.182692297043339

Epoch: 6| Step: 7
Training loss: 2.1170706748962402
Validation loss: 2.1875378547176236

Epoch: 6| Step: 8
Training loss: 2.870965003967285
Validation loss: 2.184200789338799

Epoch: 6| Step: 9
Training loss: 1.9989030361175537
Validation loss: 2.190429932327681

Epoch: 6| Step: 10
Training loss: 1.7298951148986816
Validation loss: 2.188491234215357

Epoch: 6| Step: 11
Training loss: 1.9531279802322388
Validation loss: 2.19470275345669

Epoch: 6| Step: 12
Training loss: 2.2814464569091797
Validation loss: 2.1871574501837454

Epoch: 6| Step: 13
Training loss: 1.9222158193588257
Validation loss: 2.1688677572434947

Epoch: 380| Step: 0
Training loss: 2.2833213806152344
Validation loss: 2.1728843527455486

Epoch: 6| Step: 1
Training loss: 2.159994125366211
Validation loss: 2.1549594889404955

Epoch: 6| Step: 2
Training loss: 1.9510746002197266
Validation loss: 2.1691984720127557

Epoch: 6| Step: 3
Training loss: 2.4501373767852783
Validation loss: 2.162180589091393

Epoch: 6| Step: 4
Training loss: 2.4037058353424072
Validation loss: 2.1565485282610823

Epoch: 6| Step: 5
Training loss: 1.3003737926483154
Validation loss: 2.167230550960828

Epoch: 6| Step: 6
Training loss: 2.048065662384033
Validation loss: 2.1676557089692805

Epoch: 6| Step: 7
Training loss: 2.3412086963653564
Validation loss: 2.160692602075556

Epoch: 6| Step: 8
Training loss: 2.211110830307007
Validation loss: 2.1745481029633553

Epoch: 6| Step: 9
Training loss: 2.3161792755126953
Validation loss: 2.1630269545380787

Epoch: 6| Step: 10
Training loss: 2.8437623977661133
Validation loss: 2.19080364319586

Epoch: 6| Step: 11
Training loss: 2.250248432159424
Validation loss: 2.199646383203486

Epoch: 6| Step: 12
Training loss: 2.6984446048736572
Validation loss: 2.2031961307730725

Epoch: 6| Step: 13
Training loss: 1.1691737174987793
Validation loss: 2.194743856306999

Epoch: 381| Step: 0
Training loss: 3.140822172164917
Validation loss: 2.1995569095816663

Epoch: 6| Step: 1
Training loss: 1.5789124965667725
Validation loss: 2.1989462888368996

Epoch: 6| Step: 2
Training loss: 2.039738893508911
Validation loss: 2.204225504270164

Epoch: 6| Step: 3
Training loss: 2.050502300262451
Validation loss: 2.220312339003368

Epoch: 6| Step: 4
Training loss: 1.8707726001739502
Validation loss: 2.2238973930317867

Epoch: 6| Step: 5
Training loss: 2.4210681915283203
Validation loss: 2.225520233954153

Epoch: 6| Step: 6
Training loss: 2.168363094329834
Validation loss: 2.2284368622687554

Epoch: 6| Step: 7
Training loss: 2.2608821392059326
Validation loss: 2.2324833075205484

Epoch: 6| Step: 8
Training loss: 1.6108064651489258
Validation loss: 2.220456466879896

Epoch: 6| Step: 9
Training loss: 2.22287654876709
Validation loss: 2.1928470929463706

Epoch: 6| Step: 10
Training loss: 2.9286608695983887
Validation loss: 2.2196654965800624

Epoch: 6| Step: 11
Training loss: 2.4358716011047363
Validation loss: 2.194596408515848

Epoch: 6| Step: 12
Training loss: 1.8148131370544434
Validation loss: 2.187644991823422

Epoch: 6| Step: 13
Training loss: 2.1352245807647705
Validation loss: 2.162032550381076

Epoch: 382| Step: 0
Training loss: 2.3793375492095947
Validation loss: 2.1660600657104165

Epoch: 6| Step: 1
Training loss: 1.906756043434143
Validation loss: 2.157040813917755

Epoch: 6| Step: 2
Training loss: 1.6685893535614014
Validation loss: 2.1554796413708757

Epoch: 6| Step: 3
Training loss: 2.181562900543213
Validation loss: 2.1487096586535053

Epoch: 6| Step: 4
Training loss: 1.8651211261749268
Validation loss: 2.152560187924293

Epoch: 6| Step: 5
Training loss: 1.938045620918274
Validation loss: 2.155385027649582

Epoch: 6| Step: 6
Training loss: 3.1825010776519775
Validation loss: 2.15064138238148

Epoch: 6| Step: 7
Training loss: 1.577326774597168
Validation loss: 2.16050817889552

Epoch: 6| Step: 8
Training loss: 2.116161823272705
Validation loss: 2.1667495901866625

Epoch: 6| Step: 9
Training loss: 2.263000726699829
Validation loss: 2.1842103953002603

Epoch: 6| Step: 10
Training loss: 2.6644444465637207
Validation loss: 2.186658546488772

Epoch: 6| Step: 11
Training loss: 2.2804970741271973
Validation loss: 2.189165396075095

Epoch: 6| Step: 12
Training loss: 2.5186009407043457
Validation loss: 2.202714394497615

Epoch: 6| Step: 13
Training loss: 2.4575746059417725
Validation loss: 2.203242119922433

Epoch: 383| Step: 0
Training loss: 3.28372859954834
Validation loss: 2.1986258388847433

Epoch: 6| Step: 1
Training loss: 2.260037899017334
Validation loss: 2.182546466909429

Epoch: 6| Step: 2
Training loss: 1.8071088790893555
Validation loss: 2.2057768247460805

Epoch: 6| Step: 3
Training loss: 1.8451894521713257
Validation loss: 2.1858970042197936

Epoch: 6| Step: 4
Training loss: 1.7167909145355225
Validation loss: 2.1706830045228362

Epoch: 6| Step: 5
Training loss: 1.769654631614685
Validation loss: 2.166640544450411

Epoch: 6| Step: 6
Training loss: 2.4424312114715576
Validation loss: 2.1525018266452256

Epoch: 6| Step: 7
Training loss: 2.559432029724121
Validation loss: 2.1526786383762153

Epoch: 6| Step: 8
Training loss: 1.5697739124298096
Validation loss: 2.150957154971297

Epoch: 6| Step: 9
Training loss: 1.6542689800262451
Validation loss: 2.1604623909919494

Epoch: 6| Step: 10
Training loss: 2.519756555557251
Validation loss: 2.1603526633272887

Epoch: 6| Step: 11
Training loss: 2.052860736846924
Validation loss: 2.17715887613194

Epoch: 6| Step: 12
Training loss: 2.9393415451049805
Validation loss: 2.18745865104019

Epoch: 6| Step: 13
Training loss: 2.3021152019500732
Validation loss: 2.1918269383010043

Epoch: 384| Step: 0
Training loss: 2.2773118019104004
Validation loss: 2.1958563840517433

Epoch: 6| Step: 1
Training loss: 2.329821825027466
Validation loss: 2.206170530729396

Epoch: 6| Step: 2
Training loss: 2.1625566482543945
Validation loss: 2.197026756501967

Epoch: 6| Step: 3
Training loss: 2.578583240509033
Validation loss: 2.2121197280063423

Epoch: 6| Step: 4
Training loss: 1.7862595319747925
Validation loss: 2.215366030252108

Epoch: 6| Step: 5
Training loss: 2.6703929901123047
Validation loss: 2.217474094001196

Epoch: 6| Step: 6
Training loss: 2.629295587539673
Validation loss: 2.2105811821517123

Epoch: 6| Step: 7
Training loss: 2.3704113960266113
Validation loss: 2.2045964002609253

Epoch: 6| Step: 8
Training loss: 1.3466899394989014
Validation loss: 2.2116591930389404

Epoch: 6| Step: 9
Training loss: 2.5290141105651855
Validation loss: 2.2081721777557046

Epoch: 6| Step: 10
Training loss: 3.0669288635253906
Validation loss: 2.2147017089269494

Epoch: 6| Step: 11
Training loss: 1.4010601043701172
Validation loss: 2.202610672161143

Epoch: 6| Step: 12
Training loss: 1.2694693803787231
Validation loss: 2.184731339895597

Epoch: 6| Step: 13
Training loss: 2.3472135066986084
Validation loss: 2.1842371955994637

Epoch: 385| Step: 0
Training loss: 2.7058191299438477
Validation loss: 2.1669464342055784

Epoch: 6| Step: 1
Training loss: 2.280485153198242
Validation loss: 2.167352444382124

Epoch: 6| Step: 2
Training loss: 2.9136931896209717
Validation loss: 2.1733653673561673

Epoch: 6| Step: 3
Training loss: 2.3167896270751953
Validation loss: 2.1800903748440486

Epoch: 6| Step: 4
Training loss: 2.0398406982421875
Validation loss: 2.1825951555723786

Epoch: 6| Step: 5
Training loss: 1.887642502784729
Validation loss: 2.1961964497002224

Epoch: 6| Step: 6
Training loss: 2.320936918258667
Validation loss: 2.1994117280488372

Epoch: 6| Step: 7
Training loss: 1.902789831161499
Validation loss: 2.2323854046483196

Epoch: 6| Step: 8
Training loss: 2.0236103534698486
Validation loss: 2.217215722607028

Epoch: 6| Step: 9
Training loss: 2.247234344482422
Validation loss: 2.2389114185046126

Epoch: 6| Step: 10
Training loss: 2.4300055503845215
Validation loss: 2.2466415794946815

Epoch: 6| Step: 11
Training loss: 1.950968623161316
Validation loss: 2.2225263272562334

Epoch: 6| Step: 12
Training loss: 1.6435160636901855
Validation loss: 2.190848901707639

Epoch: 6| Step: 13
Training loss: 1.9906349182128906
Validation loss: 2.157567629250147

Epoch: 386| Step: 0
Training loss: 2.3185248374938965
Validation loss: 2.145040326221015

Epoch: 6| Step: 1
Training loss: 2.0990326404571533
Validation loss: 2.136349913894489

Epoch: 6| Step: 2
Training loss: 2.0226616859436035
Validation loss: 2.1342971914558

Epoch: 6| Step: 3
Training loss: 2.6240296363830566
Validation loss: 2.1284647116097073

Epoch: 6| Step: 4
Training loss: 2.6007461547851562
Validation loss: 2.111828298978908

Epoch: 6| Step: 5
Training loss: 2.271711826324463
Validation loss: 2.1053327642461306

Epoch: 6| Step: 6
Training loss: 2.2633938789367676
Validation loss: 2.1043062081900974

Epoch: 6| Step: 7
Training loss: 2.3998165130615234
Validation loss: 2.1216656161892797

Epoch: 6| Step: 8
Training loss: 2.375218391418457
Validation loss: 2.1170594743503037

Epoch: 6| Step: 9
Training loss: 1.7595564126968384
Validation loss: 2.119547809323957

Epoch: 6| Step: 10
Training loss: 2.052520751953125
Validation loss: 2.129026623182399

Epoch: 6| Step: 11
Training loss: 1.5212922096252441
Validation loss: 2.1355826675250964

Epoch: 6| Step: 12
Training loss: 2.474701166152954
Validation loss: 2.1713090660751506

Epoch: 6| Step: 13
Training loss: 2.2540392875671387
Validation loss: 2.182107053777223

Epoch: 387| Step: 0
Training loss: 2.7651681900024414
Validation loss: 2.2068268740048973

Epoch: 6| Step: 1
Training loss: 2.3713855743408203
Validation loss: 2.2305865415962796

Epoch: 6| Step: 2
Training loss: 2.61051082611084
Validation loss: 2.258193403161982

Epoch: 6| Step: 3
Training loss: 2.51334810256958
Validation loss: 2.221947882765083

Epoch: 6| Step: 4
Training loss: 2.2007811069488525
Validation loss: 2.20976851319754

Epoch: 6| Step: 5
Training loss: 2.375357151031494
Validation loss: 2.1951759169178624

Epoch: 6| Step: 6
Training loss: 1.12945556640625
Validation loss: 2.2013132572174072

Epoch: 6| Step: 7
Training loss: 2.5631446838378906
Validation loss: 2.19950351407451

Epoch: 6| Step: 8
Training loss: 1.8822698593139648
Validation loss: 2.178959737541855

Epoch: 6| Step: 9
Training loss: 3.116011619567871
Validation loss: 2.179148377910737

Epoch: 6| Step: 10
Training loss: 1.881818175315857
Validation loss: 2.162089183766355

Epoch: 6| Step: 11
Training loss: 1.3432950973510742
Validation loss: 2.1615561926236717

Epoch: 6| Step: 12
Training loss: 2.384519338607788
Validation loss: 2.1497452361609346

Epoch: 6| Step: 13
Training loss: 1.1351840496063232
Validation loss: 2.13416640732878

Epoch: 388| Step: 0
Training loss: 2.3481931686401367
Validation loss: 2.1348440442033993

Epoch: 6| Step: 1
Training loss: 2.154985189437866
Validation loss: 2.1301294039654475

Epoch: 6| Step: 2
Training loss: 1.8798339366912842
Validation loss: 2.124343608015327

Epoch: 6| Step: 3
Training loss: 1.9204365015029907
Validation loss: 2.13593751512548

Epoch: 6| Step: 4
Training loss: 3.245849132537842
Validation loss: 2.144064111094321

Epoch: 6| Step: 5
Training loss: 1.9704159498214722
Validation loss: 2.1283924195074264

Epoch: 6| Step: 6
Training loss: 2.514265537261963
Validation loss: 2.1591107640215146

Epoch: 6| Step: 7
Training loss: 2.1392250061035156
Validation loss: 2.1659825027629895

Epoch: 6| Step: 8
Training loss: 1.9156396389007568
Validation loss: 2.160310645257273

Epoch: 6| Step: 9
Training loss: 1.6896133422851562
Validation loss: 2.1782351001616447

Epoch: 6| Step: 10
Training loss: 2.3246545791625977
Validation loss: 2.192876265894982

Epoch: 6| Step: 11
Training loss: 1.9906513690948486
Validation loss: 2.241229584140162

Epoch: 6| Step: 12
Training loss: 2.3515172004699707
Validation loss: 2.2475203006498274

Epoch: 6| Step: 13
Training loss: 2.8008294105529785
Validation loss: 2.232858250218053

Epoch: 389| Step: 0
Training loss: 2.233004331588745
Validation loss: 2.228337444284911

Epoch: 6| Step: 1
Training loss: 1.9984347820281982
Validation loss: 2.221425833240632

Epoch: 6| Step: 2
Training loss: 1.6027195453643799
Validation loss: 2.2134170583499375

Epoch: 6| Step: 3
Training loss: 2.535053253173828
Validation loss: 2.1783007806347263

Epoch: 6| Step: 4
Training loss: 1.9374146461486816
Validation loss: 2.203720518337783

Epoch: 6| Step: 5
Training loss: 1.8128430843353271
Validation loss: 2.1695961721481813

Epoch: 6| Step: 6
Training loss: 2.240814208984375
Validation loss: 2.163086854001527

Epoch: 6| Step: 7
Training loss: 2.993744373321533
Validation loss: 2.1598500410715737

Epoch: 6| Step: 8
Training loss: 2.05082631111145
Validation loss: 2.1433566129335793

Epoch: 6| Step: 9
Training loss: 2.3686776161193848
Validation loss: 2.146165053049723

Epoch: 6| Step: 10
Training loss: 1.7970024347305298
Validation loss: 2.1451814687380226

Epoch: 6| Step: 11
Training loss: 2.562490463256836
Validation loss: 2.1406066840694797

Epoch: 6| Step: 12
Training loss: 1.9154868125915527
Validation loss: 2.1587541257181475

Epoch: 6| Step: 13
Training loss: 2.663212299346924
Validation loss: 2.1525041172581334

Epoch: 390| Step: 0
Training loss: 1.8629562854766846
Validation loss: 2.1497493354223107

Epoch: 6| Step: 1
Training loss: 2.7310941219329834
Validation loss: 2.1618704693291777

Epoch: 6| Step: 2
Training loss: 2.0269951820373535
Validation loss: 2.1746119760697886

Epoch: 6| Step: 3
Training loss: 2.3416919708251953
Validation loss: 2.1691654882123395

Epoch: 6| Step: 4
Training loss: 2.2822279930114746
Validation loss: 2.1765409874659714

Epoch: 6| Step: 5
Training loss: 2.500084400177002
Validation loss: 2.194698969523112

Epoch: 6| Step: 6
Training loss: 2.012259006500244
Validation loss: 2.2063897220037316

Epoch: 6| Step: 7
Training loss: 1.7305643558502197
Validation loss: 2.2053491787243913

Epoch: 6| Step: 8
Training loss: 2.019105911254883
Validation loss: 2.1861501163051975

Epoch: 6| Step: 9
Training loss: 1.7571961879730225
Validation loss: 2.1776593885114117

Epoch: 6| Step: 10
Training loss: 2.3963894844055176
Validation loss: 2.1865137751384447

Epoch: 6| Step: 11
Training loss: 2.540740728378296
Validation loss: 2.182873523363503

Epoch: 6| Step: 12
Training loss: 2.2189621925354004
Validation loss: 2.162480217154308

Epoch: 6| Step: 13
Training loss: 1.743782639503479
Validation loss: 2.1584144664067093

Epoch: 391| Step: 0
Training loss: 2.404136896133423
Validation loss: 2.1594177292239283

Epoch: 6| Step: 1
Training loss: 2.387300491333008
Validation loss: 2.157638862568845

Epoch: 6| Step: 2
Training loss: 2.022158145904541
Validation loss: 2.1492706498792096

Epoch: 6| Step: 3
Training loss: 2.331218719482422
Validation loss: 2.150979083071473

Epoch: 6| Step: 4
Training loss: 2.157501220703125
Validation loss: 2.144328449362068

Epoch: 6| Step: 5
Training loss: 2.052574634552002
Validation loss: 2.146308870725734

Epoch: 6| Step: 6
Training loss: 2.0508995056152344
Validation loss: 2.14711893117556

Epoch: 6| Step: 7
Training loss: 2.0916450023651123
Validation loss: 2.151210549057171

Epoch: 6| Step: 8
Training loss: 2.3679561614990234
Validation loss: 2.1319337326993226

Epoch: 6| Step: 9
Training loss: 2.4171974658966064
Validation loss: 2.158615578887283

Epoch: 6| Step: 10
Training loss: 2.1727354526519775
Validation loss: 2.1600496743315007

Epoch: 6| Step: 11
Training loss: 1.9434762001037598
Validation loss: 2.1734352675817346

Epoch: 6| Step: 12
Training loss: 2.066514253616333
Validation loss: 2.1815666742222284

Epoch: 6| Step: 13
Training loss: 1.7034108638763428
Validation loss: 2.1702978944265716

Epoch: 392| Step: 0
Training loss: 2.3052730560302734
Validation loss: 2.1748211486365205

Epoch: 6| Step: 1
Training loss: 1.7909331321716309
Validation loss: 2.175960963772189

Epoch: 6| Step: 2
Training loss: 2.3849244117736816
Validation loss: 2.162216664642416

Epoch: 6| Step: 3
Training loss: 2.4293861389160156
Validation loss: 2.1649333559056765

Epoch: 6| Step: 4
Training loss: 2.48117995262146
Validation loss: 2.1757959806790916

Epoch: 6| Step: 5
Training loss: 2.063852310180664
Validation loss: 2.192610881661856

Epoch: 6| Step: 6
Training loss: 1.8682265281677246
Validation loss: 2.1937312310741794

Epoch: 6| Step: 7
Training loss: 1.811586856842041
Validation loss: 2.2050850724661224

Epoch: 6| Step: 8
Training loss: 1.9286056756973267
Validation loss: 2.181907284644342

Epoch: 6| Step: 9
Training loss: 1.5048874616622925
Validation loss: 2.1824806864543627

Epoch: 6| Step: 10
Training loss: 2.5148043632507324
Validation loss: 2.179955674755958

Epoch: 6| Step: 11
Training loss: 2.209890842437744
Validation loss: 2.1843033298369376

Epoch: 6| Step: 12
Training loss: 2.8382296562194824
Validation loss: 2.173434780490014

Epoch: 6| Step: 13
Training loss: 2.446528196334839
Validation loss: 2.18331342358743

Epoch: 393| Step: 0
Training loss: 2.9983997344970703
Validation loss: 2.1793138160500476

Epoch: 6| Step: 1
Training loss: 1.6094779968261719
Validation loss: 2.1550130664661364

Epoch: 6| Step: 2
Training loss: 2.1220273971557617
Validation loss: 2.182029370338686

Epoch: 6| Step: 3
Training loss: 1.4502707719802856
Validation loss: 2.184278534304711

Epoch: 6| Step: 4
Training loss: 2.3490989208221436
Validation loss: 2.178100529537406

Epoch: 6| Step: 5
Training loss: 1.9017438888549805
Validation loss: 2.1717186102303128

Epoch: 6| Step: 6
Training loss: 2.611538887023926
Validation loss: 2.1822071972713677

Epoch: 6| Step: 7
Training loss: 2.192034959793091
Validation loss: 2.1779143194998465

Epoch: 6| Step: 8
Training loss: 2.1990537643432617
Validation loss: 2.196709971274099

Epoch: 6| Step: 9
Training loss: 2.0713839530944824
Validation loss: 2.1996275096811275

Epoch: 6| Step: 10
Training loss: 2.3764214515686035
Validation loss: 2.179744666622531

Epoch: 6| Step: 11
Training loss: 2.5946431159973145
Validation loss: 2.185746746678506

Epoch: 6| Step: 12
Training loss: 1.5211198329925537
Validation loss: 2.1800818776571624

Epoch: 6| Step: 13
Training loss: 2.482907295227051
Validation loss: 2.1682770329137004

Epoch: 394| Step: 0
Training loss: 3.254763603210449
Validation loss: 2.1810478395031345

Epoch: 6| Step: 1
Training loss: 1.6426892280578613
Validation loss: 2.19445284464026

Epoch: 6| Step: 2
Training loss: 2.4763002395629883
Validation loss: 2.1938282264176237

Epoch: 6| Step: 3
Training loss: 2.0370523929595947
Validation loss: 2.200973362051031

Epoch: 6| Step: 4
Training loss: 1.8490746021270752
Validation loss: 2.189567400563148

Epoch: 6| Step: 5
Training loss: 1.048696756362915
Validation loss: 2.2005046900882514

Epoch: 6| Step: 6
Training loss: 2.9036765098571777
Validation loss: 2.2089863131123204

Epoch: 6| Step: 7
Training loss: 3.1310884952545166
Validation loss: 2.2135209191230034

Epoch: 6| Step: 8
Training loss: 1.3743736743927002
Validation loss: 2.1968817069966304

Epoch: 6| Step: 9
Training loss: 1.7161654233932495
Validation loss: 2.190773476836502

Epoch: 6| Step: 10
Training loss: 2.0149636268615723
Validation loss: 2.186081081308344

Epoch: 6| Step: 11
Training loss: 2.05594801902771
Validation loss: 2.153293578855453

Epoch: 6| Step: 12
Training loss: 2.0117363929748535
Validation loss: 2.1555125251893075

Epoch: 6| Step: 13
Training loss: 3.0497145652770996
Validation loss: 2.1397947854893182

Epoch: 395| Step: 0
Training loss: 2.469637870788574
Validation loss: 2.1471073678744736

Epoch: 6| Step: 1
Training loss: 1.8218271732330322
Validation loss: 2.1422418496942006

Epoch: 6| Step: 2
Training loss: 2.256032943725586
Validation loss: 2.148573613935901

Epoch: 6| Step: 3
Training loss: 2.5970866680145264
Validation loss: 2.1462213608526413

Epoch: 6| Step: 4
Training loss: 1.7378777265548706
Validation loss: 2.1317258342619865

Epoch: 6| Step: 5
Training loss: 2.6681602001190186
Validation loss: 2.14708024455655

Epoch: 6| Step: 6
Training loss: 2.40683650970459
Validation loss: 2.157615505239015

Epoch: 6| Step: 7
Training loss: 2.582753896713257
Validation loss: 2.199974847096269

Epoch: 6| Step: 8
Training loss: 1.9420620203018188
Validation loss: 2.2076520266071444

Epoch: 6| Step: 9
Training loss: 2.251513957977295
Validation loss: 2.198967486299494

Epoch: 6| Step: 10
Training loss: 2.2205100059509277
Validation loss: 2.1763418387341242

Epoch: 6| Step: 11
Training loss: 1.655313491821289
Validation loss: 2.176179010380981

Epoch: 6| Step: 12
Training loss: 2.4604711532592773
Validation loss: 2.1873189787710867

Epoch: 6| Step: 13
Training loss: 0.8260246515274048
Validation loss: 2.162289865555302

Epoch: 396| Step: 0
Training loss: 1.8353866338729858
Validation loss: 2.1618826825131654

Epoch: 6| Step: 1
Training loss: 2.338956832885742
Validation loss: 2.1807936314613587

Epoch: 6| Step: 2
Training loss: 2.0072758197784424
Validation loss: 2.1495101272418933

Epoch: 6| Step: 3
Training loss: 1.5679130554199219
Validation loss: 2.1603874955126035

Epoch: 6| Step: 4
Training loss: 1.9303112030029297
Validation loss: 2.139693110219894

Epoch: 6| Step: 5
Training loss: 2.8005051612854004
Validation loss: 2.1328526978851645

Epoch: 6| Step: 6
Training loss: 2.3143162727355957
Validation loss: 2.120809167943975

Epoch: 6| Step: 7
Training loss: 1.7748582363128662
Validation loss: 2.146511311172157

Epoch: 6| Step: 8
Training loss: 2.692628860473633
Validation loss: 2.137507466859715

Epoch: 6| Step: 9
Training loss: 2.503260612487793
Validation loss: 2.145731767018636

Epoch: 6| Step: 10
Training loss: 1.7178137302398682
Validation loss: 2.155827586368848

Epoch: 6| Step: 11
Training loss: 2.0573692321777344
Validation loss: 2.179320122606011

Epoch: 6| Step: 12
Training loss: 2.474118232727051
Validation loss: 2.1937893744437926

Epoch: 6| Step: 13
Training loss: 2.3492465019226074
Validation loss: 2.2092465867278395

Epoch: 397| Step: 0
Training loss: 2.6432530879974365
Validation loss: 2.1868761201058664

Epoch: 6| Step: 1
Training loss: 2.261115550994873
Validation loss: 2.153820144232883

Epoch: 6| Step: 2
Training loss: 1.8904327154159546
Validation loss: 2.1519734705648115

Epoch: 6| Step: 3
Training loss: 2.057250738143921
Validation loss: 2.1317037715706775

Epoch: 6| Step: 4
Training loss: 2.4165470600128174
Validation loss: 2.117920998604067

Epoch: 6| Step: 5
Training loss: 2.2923030853271484
Validation loss: 2.1280756124886135

Epoch: 6| Step: 6
Training loss: 1.891836404800415
Validation loss: 2.118500368569487

Epoch: 6| Step: 7
Training loss: 2.017181873321533
Validation loss: 2.1399339245211695

Epoch: 6| Step: 8
Training loss: 1.9403882026672363
Validation loss: 2.1617185864397275

Epoch: 6| Step: 9
Training loss: 2.0790982246398926
Validation loss: 2.1534602872786985

Epoch: 6| Step: 10
Training loss: 2.4504871368408203
Validation loss: 2.168193860720563

Epoch: 6| Step: 11
Training loss: 2.0841774940490723
Validation loss: 2.172484887543545

Epoch: 6| Step: 12
Training loss: 2.113952398300171
Validation loss: 2.1692259721858527

Epoch: 6| Step: 13
Training loss: 1.9566179513931274
Validation loss: 2.158502059598123

Epoch: 398| Step: 0
Training loss: 2.601754665374756
Validation loss: 2.1728941086799867

Epoch: 6| Step: 1
Training loss: 1.967099666595459
Validation loss: 2.1917012865825365

Epoch: 6| Step: 2
Training loss: 2.5971758365631104
Validation loss: 2.2020721486819688

Epoch: 6| Step: 3
Training loss: 2.0941381454467773
Validation loss: 2.1812010965039654

Epoch: 6| Step: 4
Training loss: 1.9475574493408203
Validation loss: 2.1997651976923787

Epoch: 6| Step: 5
Training loss: 2.869293689727783
Validation loss: 2.1709435678297475

Epoch: 6| Step: 6
Training loss: 2.261148452758789
Validation loss: 2.2008315593965593

Epoch: 6| Step: 7
Training loss: 1.6675169467926025
Validation loss: 2.1664316256841025

Epoch: 6| Step: 8
Training loss: 2.9196081161499023
Validation loss: 2.1590630572329284

Epoch: 6| Step: 9
Training loss: 2.6276416778564453
Validation loss: 2.137317269079147

Epoch: 6| Step: 10
Training loss: 1.6036927700042725
Validation loss: 2.143590601541663

Epoch: 6| Step: 11
Training loss: 1.652206540107727
Validation loss: 2.130849343474193

Epoch: 6| Step: 12
Training loss: 1.5262120962142944
Validation loss: 2.1456634767593874

Epoch: 6| Step: 13
Training loss: 1.7342766523361206
Validation loss: 2.1409454832794848

Epoch: 399| Step: 0
Training loss: 2.5085196495056152
Validation loss: 2.1407228285266506

Epoch: 6| Step: 1
Training loss: 1.9050869941711426
Validation loss: 2.1636212589920207

Epoch: 6| Step: 2
Training loss: 2.411846399307251
Validation loss: 2.155623077064432

Epoch: 6| Step: 3
Training loss: 2.496103525161743
Validation loss: 2.1955811887659054

Epoch: 6| Step: 4
Training loss: 2.121999740600586
Validation loss: 2.239753659053515

Epoch: 6| Step: 5
Training loss: 1.6451365947723389
Validation loss: 2.2663139681662283

Epoch: 6| Step: 6
Training loss: 2.316009283065796
Validation loss: 2.319318425270819

Epoch: 6| Step: 7
Training loss: 2.2294750213623047
Validation loss: 2.3439287575342322

Epoch: 6| Step: 8
Training loss: 1.6486842632293701
Validation loss: 2.3495615528475855

Epoch: 6| Step: 9
Training loss: 2.8534529209136963
Validation loss: 2.3148130037451304

Epoch: 6| Step: 10
Training loss: 2.5398635864257812
Validation loss: 2.268645081468808

Epoch: 6| Step: 11
Training loss: 2.097172260284424
Validation loss: 2.222969680704096

Epoch: 6| Step: 12
Training loss: 2.059988498687744
Validation loss: 2.1705196544688237

Epoch: 6| Step: 13
Training loss: 1.5866494178771973
Validation loss: 2.1598853449667654

Epoch: 400| Step: 0
Training loss: 2.7328951358795166
Validation loss: 2.1392706260886243

Epoch: 6| Step: 1
Training loss: 1.4900999069213867
Validation loss: 2.138812006160777

Epoch: 6| Step: 2
Training loss: 2.183135747909546
Validation loss: 2.1217427010177285

Epoch: 6| Step: 3
Training loss: 2.1277215480804443
Validation loss: 2.126487639642531

Epoch: 6| Step: 4
Training loss: 2.0836434364318848
Validation loss: 2.1146484113508657

Epoch: 6| Step: 5
Training loss: 2.3716611862182617
Validation loss: 2.120830132115272

Epoch: 6| Step: 6
Training loss: 2.6585869789123535
Validation loss: 2.134196101978261

Epoch: 6| Step: 7
Training loss: 2.311250686645508
Validation loss: 2.151644314489057

Epoch: 6| Step: 8
Training loss: 1.7865715026855469
Validation loss: 2.1456704780619633

Epoch: 6| Step: 9
Training loss: 1.792731523513794
Validation loss: 2.157778868111231

Epoch: 6| Step: 10
Training loss: 1.9471595287322998
Validation loss: 2.1816693582842426

Epoch: 6| Step: 11
Training loss: 1.8695365190505981
Validation loss: 2.2112899570054907

Epoch: 6| Step: 12
Training loss: 2.7978005409240723
Validation loss: 2.2119974077388806

Epoch: 6| Step: 13
Training loss: 1.8390146493911743
Validation loss: 2.1975649813170075

Testing loss: 2.3297991222805448

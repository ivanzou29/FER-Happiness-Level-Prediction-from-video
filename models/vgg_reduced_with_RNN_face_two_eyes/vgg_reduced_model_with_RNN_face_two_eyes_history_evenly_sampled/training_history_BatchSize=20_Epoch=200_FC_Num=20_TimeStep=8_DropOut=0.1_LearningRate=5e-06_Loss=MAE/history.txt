Epoch: 1| Step: 0
Training loss: 5.143773555755615
Validation loss: 5.1411194391148065

Epoch: 5| Step: 1
Training loss: 4.275387763977051
Validation loss: 5.13663508302422

Epoch: 5| Step: 2
Training loss: 4.4143195152282715
Validation loss: 5.132432701767132

Epoch: 5| Step: 3
Training loss: 4.990645408630371
Validation loss: 5.1283835031652965

Epoch: 5| Step: 4
Training loss: 5.606590747833252
Validation loss: 5.124548532629526

Epoch: 5| Step: 5
Training loss: 4.890445232391357
Validation loss: 5.120409186168383

Epoch: 5| Step: 6
Training loss: 5.0949602127075195
Validation loss: 5.116217838820591

Epoch: 5| Step: 7
Training loss: 4.699346542358398
Validation loss: 5.112146259636007

Epoch: 5| Step: 8
Training loss: 5.434300899505615
Validation loss: 5.108160372703306

Epoch: 5| Step: 9
Training loss: 4.461564540863037
Validation loss: 5.103350070215041

Epoch: 5| Step: 10
Training loss: 5.098678112030029
Validation loss: 5.098792132510934

Epoch: 2| Step: 0
Training loss: 5.042172908782959
Validation loss: 5.094161684795092

Epoch: 5| Step: 1
Training loss: 3.526297092437744
Validation loss: 5.08963522859799

Epoch: 5| Step: 2
Training loss: 5.848782062530518
Validation loss: 5.083932702259351

Epoch: 5| Step: 3
Training loss: 5.308257102966309
Validation loss: 5.078707018206196

Epoch: 5| Step: 4
Training loss: 4.90697717666626
Validation loss: 5.072997139346215

Epoch: 5| Step: 5
Training loss: 4.620145320892334
Validation loss: 5.067003142449163

Epoch: 5| Step: 6
Training loss: 6.045492649078369
Validation loss: 5.060866530223559

Epoch: 5| Step: 7
Training loss: 4.070527076721191
Validation loss: 5.054285533966556

Epoch: 5| Step: 8
Training loss: 4.34757137298584
Validation loss: 5.0476897967759

Epoch: 5| Step: 9
Training loss: 5.1524176597595215
Validation loss: 5.041126210202453

Epoch: 5| Step: 10
Training loss: 4.576208591461182
Validation loss: 5.033270015511461

Epoch: 3| Step: 0
Training loss: 5.012442588806152
Validation loss: 5.025939269732404

Epoch: 5| Step: 1
Training loss: 3.45341157913208
Validation loss: 5.01832853850498

Epoch: 5| Step: 2
Training loss: 4.481478214263916
Validation loss: 5.009505200129683

Epoch: 5| Step: 3
Training loss: 4.840720176696777
Validation loss: 5.000856773827666

Epoch: 5| Step: 4
Training loss: 4.519198417663574
Validation loss: 4.990945359712006

Epoch: 5| Step: 5
Training loss: 3.1371359825134277
Validation loss: 4.981858107351488

Epoch: 5| Step: 6
Training loss: 5.0725321769714355
Validation loss: 4.971972670606387

Epoch: 5| Step: 7
Training loss: 6.420157432556152
Validation loss: 4.961755306490006

Epoch: 5| Step: 8
Training loss: 5.415182113647461
Validation loss: 4.95043884810581

Epoch: 5| Step: 9
Training loss: 4.949299335479736
Validation loss: 4.938290590881019

Epoch: 5| Step: 10
Training loss: 5.274098873138428
Validation loss: 4.925249771405292

Epoch: 4| Step: 0
Training loss: 4.845465183258057
Validation loss: 4.912691116333008

Epoch: 5| Step: 1
Training loss: 4.609299182891846
Validation loss: 4.899210550451792

Epoch: 5| Step: 2
Training loss: 4.792508125305176
Validation loss: 4.884564322810019

Epoch: 5| Step: 3
Training loss: 3.850554943084717
Validation loss: 4.869873236584407

Epoch: 5| Step: 4
Training loss: 4.890045166015625
Validation loss: 4.854345008891116

Epoch: 5| Step: 5
Training loss: 4.455635070800781
Validation loss: 4.83830318656019

Epoch: 5| Step: 6
Training loss: 4.396732330322266
Validation loss: 4.821983762966689

Epoch: 5| Step: 7
Training loss: 5.239364147186279
Validation loss: 4.802918157269878

Epoch: 5| Step: 8
Training loss: 4.455928325653076
Validation loss: 4.785530818405972

Epoch: 5| Step: 9
Training loss: 4.538140296936035
Validation loss: 4.7666600493974585

Epoch: 5| Step: 10
Training loss: 4.798493385314941
Validation loss: 4.7485497587470595

Epoch: 5| Step: 0
Training loss: 4.394062042236328
Validation loss: 4.7277291359439975

Epoch: 5| Step: 1
Training loss: 4.008559703826904
Validation loss: 4.70993548054849

Epoch: 5| Step: 2
Training loss: 4.793443202972412
Validation loss: 4.688887965294622

Epoch: 5| Step: 3
Training loss: 5.021458625793457
Validation loss: 4.668329269655289

Epoch: 5| Step: 4
Training loss: 4.0237531661987305
Validation loss: 4.647159463615828

Epoch: 5| Step: 5
Training loss: 4.20698356628418
Validation loss: 4.625888065625262

Epoch: 5| Step: 6
Training loss: 3.775531053543091
Validation loss: 4.60431783430038

Epoch: 5| Step: 7
Training loss: 4.643232822418213
Validation loss: 4.582449748951902

Epoch: 5| Step: 8
Training loss: 4.200293064117432
Validation loss: 4.561428344377908

Epoch: 5| Step: 9
Training loss: 4.125899791717529
Validation loss: 4.5396157439037035

Epoch: 5| Step: 10
Training loss: 5.5094146728515625
Validation loss: 4.516723530266875

Epoch: 6| Step: 0
Training loss: 3.825857162475586
Validation loss: 4.49465750622493

Epoch: 5| Step: 1
Training loss: 4.135889530181885
Validation loss: 4.472020559413458

Epoch: 5| Step: 2
Training loss: 4.426416873931885
Validation loss: 4.450248005569622

Epoch: 5| Step: 3
Training loss: 4.10210657119751
Validation loss: 4.428213339979931

Epoch: 5| Step: 4
Training loss: 3.2618095874786377
Validation loss: 4.407593083638017

Epoch: 5| Step: 5
Training loss: 4.627355575561523
Validation loss: 4.383902857380528

Epoch: 5| Step: 6
Training loss: 4.029153823852539
Validation loss: 4.360557858661939

Epoch: 5| Step: 7
Training loss: 3.8494160175323486
Validation loss: 4.335740540617255

Epoch: 5| Step: 8
Training loss: 4.68875789642334
Validation loss: 4.314113406724827

Epoch: 5| Step: 9
Training loss: 4.296871185302734
Validation loss: 4.291130970883113

Epoch: 5| Step: 10
Training loss: 4.778677463531494
Validation loss: 4.266790859160885

Epoch: 7| Step: 0
Training loss: 4.382814407348633
Validation loss: 4.242062655828333

Epoch: 5| Step: 1
Training loss: 3.9528870582580566
Validation loss: 4.217742914794593

Epoch: 5| Step: 2
Training loss: 3.9263179302215576
Validation loss: 4.191890544788812

Epoch: 5| Step: 3
Training loss: 2.699833869934082
Validation loss: 4.1667065364058296

Epoch: 5| Step: 4
Training loss: 3.6475539207458496
Validation loss: 4.140029538062311

Epoch: 5| Step: 5
Training loss: 4.483406066894531
Validation loss: 4.113853100807436

Epoch: 5| Step: 6
Training loss: 4.258447170257568
Validation loss: 4.08134707327812

Epoch: 5| Step: 7
Training loss: 3.576775074005127
Validation loss: 4.053045108754148

Epoch: 5| Step: 8
Training loss: 4.317718505859375
Validation loss: 4.021655072448074

Epoch: 5| Step: 9
Training loss: 4.363391399383545
Validation loss: 3.9845546086629233

Epoch: 5| Step: 10
Training loss: 3.642984390258789
Validation loss: 3.946292966924688

Epoch: 8| Step: 0
Training loss: 3.9284214973449707
Validation loss: 3.9052344906714653

Epoch: 5| Step: 1
Training loss: 3.5892882347106934
Validation loss: 3.875243633024154

Epoch: 5| Step: 2
Training loss: 3.22149658203125
Validation loss: 3.8514436316746536

Epoch: 5| Step: 3
Training loss: 4.532812595367432
Validation loss: 3.827698128197783

Epoch: 5| Step: 4
Training loss: 4.501494884490967
Validation loss: 3.798447749947989

Epoch: 5| Step: 5
Training loss: 3.0251317024230957
Validation loss: 3.7767939311201855

Epoch: 5| Step: 6
Training loss: 3.95343017578125
Validation loss: 3.7564325589005665

Epoch: 5| Step: 7
Training loss: 2.884274482727051
Validation loss: 3.737113204053653

Epoch: 5| Step: 8
Training loss: 3.836986541748047
Validation loss: 3.714088809105658

Epoch: 5| Step: 9
Training loss: 3.8040390014648438
Validation loss: 3.6898526991567304

Epoch: 5| Step: 10
Training loss: 2.9259488582611084
Validation loss: 3.6686932912436863

Epoch: 9| Step: 0
Training loss: 2.984987735748291
Validation loss: 3.6488101713119017

Epoch: 5| Step: 1
Training loss: 3.6623387336730957
Validation loss: 3.6232159624817553

Epoch: 5| Step: 2
Training loss: 3.760286808013916
Validation loss: 3.60071474506009

Epoch: 5| Step: 3
Training loss: 2.822016477584839
Validation loss: 3.5761026285027944

Epoch: 5| Step: 4
Training loss: 4.3281331062316895
Validation loss: 3.5554059423426145

Epoch: 5| Step: 5
Training loss: 3.554797410964966
Validation loss: 3.5303683050217165

Epoch: 5| Step: 6
Training loss: 3.04852294921875
Validation loss: 3.5000134104041645

Epoch: 5| Step: 7
Training loss: 4.055352210998535
Validation loss: 3.4744579817659114

Epoch: 5| Step: 8
Training loss: 3.283842086791992
Validation loss: 3.455360110088061

Epoch: 5| Step: 9
Training loss: 3.4826788902282715
Validation loss: 3.433842376996112

Epoch: 5| Step: 10
Training loss: 2.9450595378875732
Validation loss: 3.416916634446831

Epoch: 10| Step: 0
Training loss: 4.9403462409973145
Validation loss: 3.4001788682835077

Epoch: 5| Step: 1
Training loss: 3.2988038063049316
Validation loss: 3.386623195422593

Epoch: 5| Step: 2
Training loss: 3.685267686843872
Validation loss: 3.37204481709388

Epoch: 5| Step: 3
Training loss: 2.754972219467163
Validation loss: 3.360224708434074

Epoch: 5| Step: 4
Training loss: 3.561331272125244
Validation loss: 3.345229741065733

Epoch: 5| Step: 5
Training loss: 4.4052534103393555
Validation loss: 3.336943975058935

Epoch: 5| Step: 6
Training loss: 1.8249181509017944
Validation loss: 3.325190782546997

Epoch: 5| Step: 7
Training loss: 3.576662540435791
Validation loss: 3.3213144656150573

Epoch: 5| Step: 8
Training loss: 2.2138657569885254
Validation loss: 3.3124411849565405

Epoch: 5| Step: 9
Training loss: 3.1919960975646973
Validation loss: 3.3049478402701755

Epoch: 5| Step: 10
Training loss: 2.8016440868377686
Validation loss: 3.293572897552162

Epoch: 11| Step: 0
Training loss: 4.285251140594482
Validation loss: 3.2876746321237214

Epoch: 5| Step: 1
Training loss: 1.8211326599121094
Validation loss: 3.2802293428810696

Epoch: 5| Step: 2
Training loss: 3.682384490966797
Validation loss: 3.273361426527782

Epoch: 5| Step: 3
Training loss: 3.572995662689209
Validation loss: 3.2641993568789576

Epoch: 5| Step: 4
Training loss: 3.592008113861084
Validation loss: 3.25684102119938

Epoch: 5| Step: 5
Training loss: 2.8240270614624023
Validation loss: 3.2501393902686333

Epoch: 5| Step: 6
Training loss: 2.493922710418701
Validation loss: 3.2461361577433925

Epoch: 5| Step: 7
Training loss: 2.7468647956848145
Validation loss: 3.2394609041111444

Epoch: 5| Step: 8
Training loss: 3.9676296710968018
Validation loss: 3.232577852023545

Epoch: 5| Step: 9
Training loss: 3.209585666656494
Validation loss: 3.228148570624731

Epoch: 5| Step: 10
Training loss: 3.405313491821289
Validation loss: 3.2238512167366604

Epoch: 12| Step: 0
Training loss: 2.9954240322113037
Validation loss: 3.221216524800947

Epoch: 5| Step: 1
Training loss: 3.0484752655029297
Validation loss: 3.2142341598387687

Epoch: 5| Step: 2
Training loss: 2.9403867721557617
Validation loss: 3.2083749412208475

Epoch: 5| Step: 3
Training loss: 3.7169735431671143
Validation loss: 3.2043673069246355

Epoch: 5| Step: 4
Training loss: 3.421597957611084
Validation loss: 3.2017100395694857

Epoch: 5| Step: 5
Training loss: 3.4521446228027344
Validation loss: 3.196156722243114

Epoch: 5| Step: 6
Training loss: 3.4772114753723145
Validation loss: 3.190305999530259

Epoch: 5| Step: 7
Training loss: 4.2234272956848145
Validation loss: 3.188317829562772

Epoch: 5| Step: 8
Training loss: 2.595438003540039
Validation loss: 3.183356464550059

Epoch: 5| Step: 9
Training loss: 2.469205856323242
Validation loss: 3.181706233691144

Epoch: 5| Step: 10
Training loss: 2.736992120742798
Validation loss: 3.175075133641561

Epoch: 13| Step: 0
Training loss: 3.057459831237793
Validation loss: 3.177396487164241

Epoch: 5| Step: 1
Training loss: 2.9209651947021484
Validation loss: 3.1744324366251626

Epoch: 5| Step: 2
Training loss: 3.6707091331481934
Validation loss: 3.172947842587707

Epoch: 5| Step: 3
Training loss: 3.5951054096221924
Validation loss: 3.1648881948122414

Epoch: 5| Step: 4
Training loss: 3.640291929244995
Validation loss: 3.16153048956266

Epoch: 5| Step: 5
Training loss: 2.8452508449554443
Validation loss: 3.160441316584105

Epoch: 5| Step: 6
Training loss: 3.269158124923706
Validation loss: 3.156252066294352

Epoch: 5| Step: 7
Training loss: 3.35660982131958
Validation loss: 3.152915405970748

Epoch: 5| Step: 8
Training loss: 3.055267572402954
Validation loss: 3.150017497360065

Epoch: 5| Step: 9
Training loss: 2.58626127243042
Validation loss: 3.149503287448678

Epoch: 5| Step: 10
Training loss: 2.8092100620269775
Validation loss: 3.1469987387298257

Epoch: 14| Step: 0
Training loss: 2.7334179878234863
Validation loss: 3.150749980762441

Epoch: 5| Step: 1
Training loss: 3.748793840408325
Validation loss: 3.149800649253271

Epoch: 5| Step: 2
Training loss: 3.735621690750122
Validation loss: 3.1410190366929576

Epoch: 5| Step: 3
Training loss: 2.6419589519500732
Validation loss: 3.141492730827742

Epoch: 5| Step: 4
Training loss: 3.9218108654022217
Validation loss: 3.1350026874132055

Epoch: 5| Step: 5
Training loss: 3.092862367630005
Validation loss: 3.1288477913025887

Epoch: 5| Step: 6
Training loss: 3.375941514968872
Validation loss: 3.1246113059341267

Epoch: 5| Step: 7
Training loss: 2.7093589305877686
Validation loss: 3.124451393722206

Epoch: 5| Step: 8
Training loss: 2.7530202865600586
Validation loss: 3.1235727187125915

Epoch: 5| Step: 9
Training loss: 3.302302598953247
Validation loss: 3.118064949589391

Epoch: 5| Step: 10
Training loss: 2.5084705352783203
Validation loss: 3.115166284704721

Epoch: 15| Step: 0
Training loss: 3.0317821502685547
Validation loss: 3.1111357494067122

Epoch: 5| Step: 1
Training loss: 3.244708299636841
Validation loss: 3.111358191377373

Epoch: 5| Step: 2
Training loss: 3.526395082473755
Validation loss: 3.111847713429441

Epoch: 5| Step: 3
Training loss: 2.7935686111450195
Validation loss: 3.1081837069603706

Epoch: 5| Step: 4
Training loss: 3.5038833618164062
Validation loss: 3.103098935978387

Epoch: 5| Step: 5
Training loss: 3.0478858947753906
Validation loss: 3.0982094580127346

Epoch: 5| Step: 6
Training loss: 2.1103527545928955
Validation loss: 3.0939477130930912

Epoch: 5| Step: 7
Training loss: 3.32108998298645
Validation loss: 3.095231730450866

Epoch: 5| Step: 8
Training loss: 3.112226963043213
Validation loss: 3.0922163712081088

Epoch: 5| Step: 9
Training loss: 3.0158677101135254
Validation loss: 3.091544258979059

Epoch: 5| Step: 10
Training loss: 3.753175735473633
Validation loss: 3.0923810799916587

Epoch: 16| Step: 0
Training loss: 2.5197882652282715
Validation loss: 3.08271304509973

Epoch: 5| Step: 1
Training loss: 2.8354434967041016
Validation loss: 3.082208382186069

Epoch: 5| Step: 2
Training loss: 3.5560028553009033
Validation loss: 3.0772070346340055

Epoch: 5| Step: 3
Training loss: 3.493492841720581
Validation loss: 3.076376735523183

Epoch: 5| Step: 4
Training loss: 3.793994426727295
Validation loss: 3.0738917884006294

Epoch: 5| Step: 5
Training loss: 2.7089297771453857
Validation loss: 3.081465895457934

Epoch: 5| Step: 6
Training loss: 3.3339691162109375
Validation loss: 3.0804402725670927

Epoch: 5| Step: 7
Training loss: 2.204601526260376
Validation loss: 3.065482270333075

Epoch: 5| Step: 8
Training loss: 3.124375820159912
Validation loss: 3.0680837349225114

Epoch: 5| Step: 9
Training loss: 3.3760452270507812
Validation loss: 3.0645171314157467

Epoch: 5| Step: 10
Training loss: 3.2999391555786133
Validation loss: 3.060799052638392

Epoch: 17| Step: 0
Training loss: 2.9375298023223877
Validation loss: 3.0622753327892673

Epoch: 5| Step: 1
Training loss: 4.445456504821777
Validation loss: 3.0558006737821843

Epoch: 5| Step: 2
Training loss: 2.9273524284362793
Validation loss: 3.0581842519903697

Epoch: 5| Step: 3
Training loss: 2.80210542678833
Validation loss: 3.053627239760532

Epoch: 5| Step: 4
Training loss: 3.677729845046997
Validation loss: 3.0459247865984516

Epoch: 5| Step: 5
Training loss: 2.9525809288024902
Validation loss: 3.050576607386271

Epoch: 5| Step: 6
Training loss: 2.723602771759033
Validation loss: 3.0491682252576275

Epoch: 5| Step: 7
Training loss: 2.805013656616211
Validation loss: 3.0408691949741815

Epoch: 5| Step: 8
Training loss: 2.878448247909546
Validation loss: 3.0391009135912825

Epoch: 5| Step: 9
Training loss: 2.990779161453247
Validation loss: 3.0344960356271393

Epoch: 5| Step: 10
Training loss: 2.82226824760437
Validation loss: 3.0360206968040875

Epoch: 18| Step: 0
Training loss: 3.3692283630371094
Validation loss: 3.0367580639418734

Epoch: 5| Step: 1
Training loss: 2.88924241065979
Validation loss: 3.034617695757138

Epoch: 5| Step: 2
Training loss: 3.421534299850464
Validation loss: 3.0263045680138374

Epoch: 5| Step: 3
Training loss: 2.2569406032562256
Validation loss: 3.0231612010668685

Epoch: 5| Step: 4
Training loss: 3.0099709033966064
Validation loss: 3.0241086867547806

Epoch: 5| Step: 5
Training loss: 3.7026684284210205
Validation loss: 3.0200128196388163

Epoch: 5| Step: 6
Training loss: 2.597818374633789
Validation loss: 3.017713410879976

Epoch: 5| Step: 7
Training loss: 2.7088332176208496
Validation loss: 3.0144215194127892

Epoch: 5| Step: 8
Training loss: 3.18672251701355
Validation loss: 3.011894618311236

Epoch: 5| Step: 9
Training loss: 3.500347852706909
Validation loss: 3.011430994156868

Epoch: 5| Step: 10
Training loss: 3.1878926753997803
Validation loss: 3.0086656667852916

Epoch: 19| Step: 0
Training loss: 2.9906744956970215
Validation loss: 3.0071140899453113

Epoch: 5| Step: 1
Training loss: 3.292583465576172
Validation loss: 3.003578716708768

Epoch: 5| Step: 2
Training loss: 2.9174416065216064
Validation loss: 3.004651731060397

Epoch: 5| Step: 3
Training loss: 3.735544204711914
Validation loss: 3.0010877629762054

Epoch: 5| Step: 4
Training loss: 2.182401180267334
Validation loss: 3.001438376724079

Epoch: 5| Step: 5
Training loss: 3.143915891647339
Validation loss: 2.995023186488818

Epoch: 5| Step: 6
Training loss: 2.8019776344299316
Validation loss: 2.9930096339153986

Epoch: 5| Step: 7
Training loss: 3.824669599533081
Validation loss: 2.9922851183081187

Epoch: 5| Step: 8
Training loss: 2.663088798522949
Validation loss: 2.9884035407855944

Epoch: 5| Step: 9
Training loss: 3.4052085876464844
Validation loss: 2.9885776658211984

Epoch: 5| Step: 10
Training loss: 2.6063320636749268
Validation loss: 2.9867387894661195

Epoch: 20| Step: 0
Training loss: 3.518789768218994
Validation loss: 2.9814123415177867

Epoch: 5| Step: 1
Training loss: 2.8761911392211914
Validation loss: 2.9768085146463044

Epoch: 5| Step: 2
Training loss: 2.459197998046875
Validation loss: 2.9824288481025287

Epoch: 5| Step: 3
Training loss: 3.75748872756958
Validation loss: 2.981518550585675

Epoch: 5| Step: 4
Training loss: 2.3569531440734863
Validation loss: 2.9972528180768414

Epoch: 5| Step: 5
Training loss: 2.342024564743042
Validation loss: 2.9857659775723695

Epoch: 5| Step: 6
Training loss: 2.611567974090576
Validation loss: 2.988263878771054

Epoch: 5| Step: 7
Training loss: 2.99491548538208
Validation loss: 2.9724868856450564

Epoch: 5| Step: 8
Training loss: 4.103336811065674
Validation loss: 2.969694840010776

Epoch: 5| Step: 9
Training loss: 3.3273956775665283
Validation loss: 2.972504049219111

Epoch: 5| Step: 10
Training loss: 3.1567156314849854
Validation loss: 2.9669481297974944

Epoch: 21| Step: 0
Training loss: 3.2830779552459717
Validation loss: 2.964105272805819

Epoch: 5| Step: 1
Training loss: 3.255497455596924
Validation loss: 2.959626328560614

Epoch: 5| Step: 2
Training loss: 2.1879823207855225
Validation loss: 2.961399011714484

Epoch: 5| Step: 3
Training loss: 3.253497362136841
Validation loss: 2.9562072241178123

Epoch: 5| Step: 4
Training loss: 3.382190704345703
Validation loss: 2.962318766501642

Epoch: 5| Step: 5
Training loss: 3.6381707191467285
Validation loss: 2.9611592779877367

Epoch: 5| Step: 6
Training loss: 2.721327781677246
Validation loss: 2.954129970201882

Epoch: 5| Step: 7
Training loss: 3.227381944656372
Validation loss: 2.9528915805201374

Epoch: 5| Step: 8
Training loss: 2.4311671257019043
Validation loss: 2.9480373885041926

Epoch: 5| Step: 9
Training loss: 2.628582000732422
Validation loss: 2.943705138339791

Epoch: 5| Step: 10
Training loss: 3.394505262374878
Validation loss: 2.945842742919922

Epoch: 22| Step: 0
Training loss: 2.018113613128662
Validation loss: 2.9442926786279164

Epoch: 5| Step: 1
Training loss: 3.0149054527282715
Validation loss: 2.941673324954125

Epoch: 5| Step: 2
Training loss: 2.6611948013305664
Validation loss: 2.9393526969417447

Epoch: 5| Step: 3
Training loss: 1.7721790075302124
Validation loss: 2.9365901024110856

Epoch: 5| Step: 4
Training loss: 3.488785982131958
Validation loss: 2.9333531241263113

Epoch: 5| Step: 5
Training loss: 3.3656165599823
Validation loss: 2.9316202517478698

Epoch: 5| Step: 6
Training loss: 3.570347547531128
Validation loss: 2.928649725452546

Epoch: 5| Step: 7
Training loss: 3.901038408279419
Validation loss: 2.929840516018611

Epoch: 5| Step: 8
Training loss: 3.018131971359253
Validation loss: 2.9356326646702264

Epoch: 5| Step: 9
Training loss: 2.5565099716186523
Validation loss: 2.9378805493795745

Epoch: 5| Step: 10
Training loss: 3.9538915157318115
Validation loss: 2.947709537321521

Epoch: 23| Step: 0
Training loss: 2.5268797874450684
Validation loss: 2.928280738092238

Epoch: 5| Step: 1
Training loss: 3.456740617752075
Validation loss: 2.9212087790171304

Epoch: 5| Step: 2
Training loss: 2.5382771492004395
Validation loss: 2.9218674090600785

Epoch: 5| Step: 3
Training loss: 3.0211703777313232
Validation loss: 2.922592798868815

Epoch: 5| Step: 4
Training loss: 3.368894100189209
Validation loss: 2.9259567004378124

Epoch: 5| Step: 5
Training loss: 3.1308465003967285
Validation loss: 2.924123523055866

Epoch: 5| Step: 6
Training loss: 3.2504990100860596
Validation loss: 2.9162527156132523

Epoch: 5| Step: 7
Training loss: 2.8778483867645264
Validation loss: 2.9170304934183755

Epoch: 5| Step: 8
Training loss: 2.717601776123047
Validation loss: 2.9122758296228226

Epoch: 5| Step: 9
Training loss: 3.4494476318359375
Validation loss: 2.9110283031258533

Epoch: 5| Step: 10
Training loss: 2.71924090385437
Validation loss: 2.9140429368583103

Epoch: 24| Step: 0
Training loss: 2.6656293869018555
Validation loss: 2.9188345273335776

Epoch: 5| Step: 1
Training loss: 2.622225284576416
Validation loss: 2.9311685741588636

Epoch: 5| Step: 2
Training loss: 2.9630959033966064
Validation loss: 2.917178177064465

Epoch: 5| Step: 3
Training loss: 3.1500093936920166
Validation loss: 2.9141948043659167

Epoch: 5| Step: 4
Training loss: 3.0449976921081543
Validation loss: 2.9060595702099543

Epoch: 5| Step: 5
Training loss: 2.5161945819854736
Validation loss: 2.9073201789650867

Epoch: 5| Step: 6
Training loss: 3.48957896232605
Validation loss: 2.9055596910497195

Epoch: 5| Step: 7
Training loss: 3.332270383834839
Validation loss: 2.899016567455825

Epoch: 5| Step: 8
Training loss: 3.384230375289917
Validation loss: 2.901097120777253

Epoch: 5| Step: 9
Training loss: 3.48298716545105
Validation loss: 2.9018792798442226

Epoch: 5| Step: 10
Training loss: 2.2355823516845703
Validation loss: 2.8942906805264053

Epoch: 25| Step: 0
Training loss: 3.81178617477417
Validation loss: 2.8896886917852584

Epoch: 5| Step: 1
Training loss: 3.450382709503174
Validation loss: 2.8916177852179414

Epoch: 5| Step: 2
Training loss: 2.2177493572235107
Validation loss: 2.8866875607480287

Epoch: 5| Step: 3
Training loss: 3.0178349018096924
Validation loss: 2.8930513833158757

Epoch: 5| Step: 4
Training loss: 2.566819906234741
Validation loss: 2.8893371064175843

Epoch: 5| Step: 5
Training loss: 2.784583806991577
Validation loss: 2.8882468105644308

Epoch: 5| Step: 6
Training loss: 2.915257453918457
Validation loss: 2.887304218866492

Epoch: 5| Step: 7
Training loss: 3.3766028881073
Validation loss: 2.8859473864237466

Epoch: 5| Step: 8
Training loss: 3.2579638957977295
Validation loss: 2.8832565738308813

Epoch: 5| Step: 9
Training loss: 1.9680497646331787
Validation loss: 2.8760214313384025

Epoch: 5| Step: 10
Training loss: 3.4811973571777344
Validation loss: 2.872781430521319

Epoch: 26| Step: 0
Training loss: 3.370265483856201
Validation loss: 2.8690504092042164

Epoch: 5| Step: 1
Training loss: 3.313783645629883
Validation loss: 2.864145594258462

Epoch: 5| Step: 2
Training loss: 3.6470234394073486
Validation loss: 2.863417210117463

Epoch: 5| Step: 3
Training loss: 3.5436336994171143
Validation loss: 2.864064860087569

Epoch: 5| Step: 4
Training loss: 3.168616771697998
Validation loss: 2.8648281046139297

Epoch: 5| Step: 5
Training loss: 2.182088613510132
Validation loss: 2.863145779537898

Epoch: 5| Step: 6
Training loss: 3.2426834106445312
Validation loss: 2.8632594129090667

Epoch: 5| Step: 7
Training loss: 2.824855327606201
Validation loss: 2.8564207912773214

Epoch: 5| Step: 8
Training loss: 2.451828956604004
Validation loss: 2.853814455770677

Epoch: 5| Step: 9
Training loss: 2.3389530181884766
Validation loss: 2.8540030884486374

Epoch: 5| Step: 10
Training loss: 2.4778499603271484
Validation loss: 2.851123550886749

Epoch: 27| Step: 0
Training loss: 3.6887335777282715
Validation loss: 2.8516406782211794

Epoch: 5| Step: 1
Training loss: 3.526811122894287
Validation loss: 2.8496670364051737

Epoch: 5| Step: 2
Training loss: 2.6814706325531006
Validation loss: 2.848391640570856

Epoch: 5| Step: 3
Training loss: 3.1001060009002686
Validation loss: 2.8491606404704433

Epoch: 5| Step: 4
Training loss: 2.4279398918151855
Validation loss: 2.8504368284697175

Epoch: 5| Step: 5
Training loss: 3.475487470626831
Validation loss: 2.8507432886349258

Epoch: 5| Step: 6
Training loss: 3.389665126800537
Validation loss: 2.844633666417932

Epoch: 5| Step: 7
Training loss: 2.886504888534546
Validation loss: 2.842560334872174

Epoch: 5| Step: 8
Training loss: 2.8801002502441406
Validation loss: 2.8416947459661834

Epoch: 5| Step: 9
Training loss: 2.4221911430358887
Validation loss: 2.840526657719766

Epoch: 5| Step: 10
Training loss: 1.847342610359192
Validation loss: 2.839737307640814

Epoch: 28| Step: 0
Training loss: 2.6945347785949707
Validation loss: 2.841224188445717

Epoch: 5| Step: 1
Training loss: 2.4381136894226074
Validation loss: 2.839231567998086

Epoch: 5| Step: 2
Training loss: 2.4102869033813477
Validation loss: 2.8351185398717083

Epoch: 5| Step: 3
Training loss: 3.0067741870880127
Validation loss: 2.835123339006978

Epoch: 5| Step: 4
Training loss: 2.549269676208496
Validation loss: 2.8386616501756894

Epoch: 5| Step: 5
Training loss: 3.4940223693847656
Validation loss: 2.833299870132118

Epoch: 5| Step: 6
Training loss: 3.3038735389709473
Validation loss: 2.8247995350950506

Epoch: 5| Step: 7
Training loss: 3.481396198272705
Validation loss: 2.828933154383013

Epoch: 5| Step: 8
Training loss: 3.0479226112365723
Validation loss: 2.8274738762968328

Epoch: 5| Step: 9
Training loss: 2.9934535026550293
Validation loss: 2.82336107120719

Epoch: 5| Step: 10
Training loss: 2.971220016479492
Validation loss: 2.820701076138404

Epoch: 29| Step: 0
Training loss: 3.7890071868896484
Validation loss: 2.818957236505324

Epoch: 5| Step: 1
Training loss: 2.5677216053009033
Validation loss: 2.8175702018122517

Epoch: 5| Step: 2
Training loss: 3.080885648727417
Validation loss: 2.8101459985138266

Epoch: 5| Step: 3
Training loss: 2.723764181137085
Validation loss: 2.8188739438210764

Epoch: 5| Step: 4
Training loss: 3.3550941944122314
Validation loss: 2.8182212793698875

Epoch: 5| Step: 5
Training loss: 3.184091091156006
Validation loss: 2.8276840897016626

Epoch: 5| Step: 6
Training loss: 2.968467950820923
Validation loss: 2.8246093437235844

Epoch: 5| Step: 7
Training loss: 2.7897119522094727
Validation loss: 2.8205427380018335

Epoch: 5| Step: 8
Training loss: 3.3279716968536377
Validation loss: 2.809667479607367

Epoch: 5| Step: 9
Training loss: 2.1725287437438965
Validation loss: 2.8048203965669036

Epoch: 5| Step: 10
Training loss: 2.206120729446411
Validation loss: 2.8058721455194617

Epoch: 30| Step: 0
Training loss: 2.1614437103271484
Validation loss: 2.8049776195197977

Epoch: 5| Step: 1
Training loss: 2.8905844688415527
Validation loss: 2.7983577277070735

Epoch: 5| Step: 2
Training loss: 3.4130706787109375
Validation loss: 2.801481508439587

Epoch: 5| Step: 3
Training loss: 2.899777889251709
Validation loss: 2.801317661039291

Epoch: 5| Step: 4
Training loss: 2.6111719608306885
Validation loss: 2.80433718363444

Epoch: 5| Step: 5
Training loss: 3.0996875762939453
Validation loss: 2.8082788528934604

Epoch: 5| Step: 6
Training loss: 3.3314876556396484
Validation loss: 2.804356357102753

Epoch: 5| Step: 7
Training loss: 3.6571972370147705
Validation loss: 2.8096488163035405

Epoch: 5| Step: 8
Training loss: 2.9945647716522217
Validation loss: 2.8126035556998303

Epoch: 5| Step: 9
Training loss: 2.2731125354766846
Validation loss: 2.8074366507991666

Epoch: 5| Step: 10
Training loss: 2.861555576324463
Validation loss: 2.803150656402752

Epoch: 31| Step: 0
Training loss: 3.0527865886688232
Validation loss: 2.79725972811381

Epoch: 5| Step: 1
Training loss: 3.097679615020752
Validation loss: 2.7899882203789166

Epoch: 5| Step: 2
Training loss: 2.5812957286834717
Validation loss: 2.7903249161217802

Epoch: 5| Step: 3
Training loss: 2.904242753982544
Validation loss: 2.7882365103690856

Epoch: 5| Step: 4
Training loss: 2.795943260192871
Validation loss: 2.7848227049714778

Epoch: 5| Step: 5
Training loss: 2.5380053520202637
Validation loss: 2.786294285969068

Epoch: 5| Step: 6
Training loss: 3.259648084640503
Validation loss: 2.781342770463677

Epoch: 5| Step: 7
Training loss: 3.7162811756134033
Validation loss: 2.7796665647978425

Epoch: 5| Step: 8
Training loss: 2.3915696144104004
Validation loss: 2.7891894976298013

Epoch: 5| Step: 9
Training loss: 2.755859375
Validation loss: 2.7914898139174267

Epoch: 5| Step: 10
Training loss: 2.9741225242614746
Validation loss: 2.8057375928407073

Epoch: 32| Step: 0
Training loss: 3.1766791343688965
Validation loss: 2.8134251333052114

Epoch: 5| Step: 1
Training loss: 2.6626522541046143
Validation loss: 2.798966984595022

Epoch: 5| Step: 2
Training loss: 2.9672493934631348
Validation loss: 2.773973185528991

Epoch: 5| Step: 3
Training loss: 2.9865634441375732
Validation loss: 2.768922608385804

Epoch: 5| Step: 4
Training loss: 2.817990779876709
Validation loss: 2.768374276417558

Epoch: 5| Step: 5
Training loss: 2.7854604721069336
Validation loss: 2.77526871619686

Epoch: 5| Step: 6
Training loss: 2.8611857891082764
Validation loss: 2.772433380926809

Epoch: 5| Step: 7
Training loss: 3.4022140502929688
Validation loss: 2.7799684104099067

Epoch: 5| Step: 8
Training loss: 3.19828462600708
Validation loss: 2.7773588831706713

Epoch: 5| Step: 9
Training loss: 2.374873638153076
Validation loss: 2.773835038626066

Epoch: 5| Step: 10
Training loss: 2.816826343536377
Validation loss: 2.772528491994386

Epoch: 33| Step: 0
Training loss: 2.6335151195526123
Validation loss: 2.7684066116168933

Epoch: 5| Step: 1
Training loss: 2.069042682647705
Validation loss: 2.7650465760179745

Epoch: 5| Step: 2
Training loss: 3.0421435832977295
Validation loss: 2.765264262435257

Epoch: 5| Step: 3
Training loss: 3.119035005569458
Validation loss: 2.7679023717039373

Epoch: 5| Step: 4
Training loss: 2.8473448753356934
Validation loss: 2.7630536325516237

Epoch: 5| Step: 5
Training loss: 3.046119213104248
Validation loss: 2.765477847027522

Epoch: 5| Step: 6
Training loss: 3.2802319526672363
Validation loss: 2.7680425746466524

Epoch: 5| Step: 7
Training loss: 2.288442850112915
Validation loss: 2.7661836531854447

Epoch: 5| Step: 8
Training loss: 3.039797306060791
Validation loss: 2.7692674129239974

Epoch: 5| Step: 9
Training loss: 3.3568878173828125
Validation loss: 2.7654862788415726

Epoch: 5| Step: 10
Training loss: 3.286048412322998
Validation loss: 2.7612966901512555

Epoch: 34| Step: 0
Training loss: 2.3137707710266113
Validation loss: 2.753275684131089

Epoch: 5| Step: 1
Training loss: 2.532846689224243
Validation loss: 2.7502543080237603

Epoch: 5| Step: 2
Training loss: 2.432582378387451
Validation loss: 2.7396403589556293

Epoch: 5| Step: 3
Training loss: 3.4557623863220215
Validation loss: 2.7440692173537387

Epoch: 5| Step: 4
Training loss: 3.1130738258361816
Validation loss: 2.745441880277408

Epoch: 5| Step: 5
Training loss: 3.5136325359344482
Validation loss: 2.7394088211879937

Epoch: 5| Step: 6
Training loss: 3.140575647354126
Validation loss: 2.7373362407889417

Epoch: 5| Step: 7
Training loss: 2.9722723960876465
Validation loss: 2.7372952584297425

Epoch: 5| Step: 8
Training loss: 2.846897602081299
Validation loss: 2.7491928967096473

Epoch: 5| Step: 9
Training loss: 2.692025661468506
Validation loss: 2.761454979578654

Epoch: 5| Step: 10
Training loss: 2.741401433944702
Validation loss: 2.7662838633342455

Epoch: 35| Step: 0
Training loss: 3.5173065662384033
Validation loss: 2.7690203394941104

Epoch: 5| Step: 1
Training loss: 2.4102368354797363
Validation loss: 2.7615438891995336

Epoch: 5| Step: 2
Training loss: 2.580104351043701
Validation loss: 2.754566149045062

Epoch: 5| Step: 3
Training loss: 3.0174331665039062
Validation loss: 2.7465482552846274

Epoch: 5| Step: 4
Training loss: 2.894221067428589
Validation loss: 2.741836283796577

Epoch: 5| Step: 5
Training loss: 3.1130223274230957
Validation loss: 2.733986864807785

Epoch: 5| Step: 6
Training loss: 2.868417739868164
Validation loss: 2.733639891429614

Epoch: 5| Step: 7
Training loss: 2.765273332595825
Validation loss: 2.732773996168567

Epoch: 5| Step: 8
Training loss: 2.7135894298553467
Validation loss: 2.727368841889084

Epoch: 5| Step: 9
Training loss: 2.9409899711608887
Validation loss: 2.7286620909167874

Epoch: 5| Step: 10
Training loss: 2.867537498474121
Validation loss: 2.726326286151845

Epoch: 36| Step: 0
Training loss: 2.1268117427825928
Validation loss: 2.7238559786991408

Epoch: 5| Step: 1
Training loss: 2.9485771656036377
Validation loss: 2.7214925955700617

Epoch: 5| Step: 2
Training loss: 3.2210402488708496
Validation loss: 2.7213203804467314

Epoch: 5| Step: 3
Training loss: 2.9232242107391357
Validation loss: 2.7194001802834133

Epoch: 5| Step: 4
Training loss: 3.288119077682495
Validation loss: 2.7130014460573912

Epoch: 5| Step: 5
Training loss: 2.503293514251709
Validation loss: 2.715404136206514

Epoch: 5| Step: 6
Training loss: 2.8840935230255127
Validation loss: 2.7183804191568846

Epoch: 5| Step: 7
Training loss: 2.7707695960998535
Validation loss: 2.7163816395626275

Epoch: 5| Step: 8
Training loss: 3.021625518798828
Validation loss: 2.716906691110262

Epoch: 5| Step: 9
Training loss: 3.1547083854675293
Validation loss: 2.717152444265222

Epoch: 5| Step: 10
Training loss: 2.648571252822876
Validation loss: 2.7230147084882184

Epoch: 37| Step: 0
Training loss: 2.3939011096954346
Validation loss: 2.716181311556088

Epoch: 5| Step: 1
Training loss: 2.1350581645965576
Validation loss: 2.7197563673860286

Epoch: 5| Step: 2
Training loss: 3.566904067993164
Validation loss: 2.711860174773842

Epoch: 5| Step: 3
Training loss: 2.60394024848938
Validation loss: 2.7093066989734607

Epoch: 5| Step: 4
Training loss: 3.663151502609253
Validation loss: 2.709004822597709

Epoch: 5| Step: 5
Training loss: 2.8716256618499756
Validation loss: 2.7079236379233738

Epoch: 5| Step: 6
Training loss: 2.4882125854492188
Validation loss: 2.704145536627821

Epoch: 5| Step: 7
Training loss: 2.899454116821289
Validation loss: 2.70034501629491

Epoch: 5| Step: 8
Training loss: 3.049219846725464
Validation loss: 2.7031412816816762

Epoch: 5| Step: 9
Training loss: 3.0216851234436035
Validation loss: 2.703621223408689

Epoch: 5| Step: 10
Training loss: 2.684337615966797
Validation loss: 2.7040221537313154

Epoch: 38| Step: 0
Training loss: 2.610262632369995
Validation loss: 2.6969356947047736

Epoch: 5| Step: 1
Training loss: 3.0048482418060303
Validation loss: 2.7023997973370295

Epoch: 5| Step: 2
Training loss: 3.3151888847351074
Validation loss: 2.704447736022293

Epoch: 5| Step: 3
Training loss: 2.869062900543213
Validation loss: 2.704983631769816

Epoch: 5| Step: 4
Training loss: 2.6959195137023926
Validation loss: 2.6993113204997075

Epoch: 5| Step: 5
Training loss: 2.918309211730957
Validation loss: 2.7041617157638713

Epoch: 5| Step: 6
Training loss: 2.4632201194763184
Validation loss: 2.705676140323762

Epoch: 5| Step: 7
Training loss: 3.3783316612243652
Validation loss: 2.704765260860484

Epoch: 5| Step: 8
Training loss: 3.2101962566375732
Validation loss: 2.697309322254632

Epoch: 5| Step: 9
Training loss: 2.659302234649658
Validation loss: 2.691901858134936

Epoch: 5| Step: 10
Training loss: 2.093266248703003
Validation loss: 2.6884409766043387

Epoch: 39| Step: 0
Training loss: 3.3334896564483643
Validation loss: 2.682923288755519

Epoch: 5| Step: 1
Training loss: 3.535097599029541
Validation loss: 2.685613380965366

Epoch: 5| Step: 2
Training loss: 2.732175588607788
Validation loss: 2.683364429781514

Epoch: 5| Step: 3
Training loss: 2.7045207023620605
Validation loss: 2.6835112443534275

Epoch: 5| Step: 4
Training loss: 2.8042473793029785
Validation loss: 2.683262901921426

Epoch: 5| Step: 5
Training loss: 2.3988423347473145
Validation loss: 2.6841233391915598

Epoch: 5| Step: 6
Training loss: 3.2955925464630127
Validation loss: 2.680764977649976

Epoch: 5| Step: 7
Training loss: 3.0546557903289795
Validation loss: 2.6808083647040912

Epoch: 5| Step: 8
Training loss: 3.1939187049865723
Validation loss: 2.6789462053647606

Epoch: 5| Step: 9
Training loss: 2.0404839515686035
Validation loss: 2.6816741702377156

Epoch: 5| Step: 10
Training loss: 2.123915672302246
Validation loss: 2.67180513053812

Epoch: 40| Step: 0
Training loss: 2.8487844467163086
Validation loss: 2.6746593675305768

Epoch: 5| Step: 1
Training loss: 2.9441044330596924
Validation loss: 2.674955855133713

Epoch: 5| Step: 2
Training loss: 2.3516147136688232
Validation loss: 2.6679923918939408

Epoch: 5| Step: 3
Training loss: 2.8081767559051514
Validation loss: 2.677718713719358

Epoch: 5| Step: 4
Training loss: 2.5487112998962402
Validation loss: 2.6748627590876755

Epoch: 5| Step: 5
Training loss: 2.754716396331787
Validation loss: 2.677642865847516

Epoch: 5| Step: 6
Training loss: 2.9096901416778564
Validation loss: 2.6754482087268623

Epoch: 5| Step: 7
Training loss: 2.4452035427093506
Validation loss: 2.676758227809783

Epoch: 5| Step: 8
Training loss: 3.413482189178467
Validation loss: 2.6692659085796726

Epoch: 5| Step: 9
Training loss: 2.89445161819458
Validation loss: 2.666891328750118

Epoch: 5| Step: 10
Training loss: 3.3694908618927
Validation loss: 2.66359620453209

Epoch: 41| Step: 0
Training loss: 2.415587902069092
Validation loss: 2.6657746607257473

Epoch: 5| Step: 1
Training loss: 2.4995646476745605
Validation loss: 2.6605031003234205

Epoch: 5| Step: 2
Training loss: 3.2939159870147705
Validation loss: 2.6649984005958802

Epoch: 5| Step: 3
Training loss: 3.2099366188049316
Validation loss: 2.6667960536095405

Epoch: 5| Step: 4
Training loss: 3.5318686962127686
Validation loss: 2.666966543402723

Epoch: 5| Step: 5
Training loss: 3.003477096557617
Validation loss: 2.663136315602128

Epoch: 5| Step: 6
Training loss: 2.094778537750244
Validation loss: 2.6599131655949417

Epoch: 5| Step: 7
Training loss: 2.4573864936828613
Validation loss: 2.6604597337784304

Epoch: 5| Step: 8
Training loss: 3.003662586212158
Validation loss: 2.6529627410314416

Epoch: 5| Step: 9
Training loss: 2.8562424182891846
Validation loss: 2.652253573940646

Epoch: 5| Step: 10
Training loss: 2.713186740875244
Validation loss: 2.651753297416113

Epoch: 42| Step: 0
Training loss: 3.1957290172576904
Validation loss: 2.650041595582039

Epoch: 5| Step: 1
Training loss: 2.7601795196533203
Validation loss: 2.656365684283677

Epoch: 5| Step: 2
Training loss: 2.812441825866699
Validation loss: 2.6527949968973794

Epoch: 5| Step: 3
Training loss: 2.1963584423065186
Validation loss: 2.6630255022356586

Epoch: 5| Step: 4
Training loss: 2.9158740043640137
Validation loss: 2.6578205170169955

Epoch: 5| Step: 5
Training loss: 3.3048553466796875
Validation loss: 2.657899074656989

Epoch: 5| Step: 6
Training loss: 3.1423561573028564
Validation loss: 2.659964494807746

Epoch: 5| Step: 7
Training loss: 3.2229740619659424
Validation loss: 2.6550249950860136

Epoch: 5| Step: 8
Training loss: 2.0843288898468018
Validation loss: 2.6452325108230754

Epoch: 5| Step: 9
Training loss: 2.3635616302490234
Validation loss: 2.6391700211391655

Epoch: 5| Step: 10
Training loss: 3.0674164295196533
Validation loss: 2.6437002125606743

Epoch: 43| Step: 0
Training loss: 1.920567512512207
Validation loss: 2.6471278449540496

Epoch: 5| Step: 1
Training loss: 2.040517568588257
Validation loss: 2.650566283092704

Epoch: 5| Step: 2
Training loss: 2.996797561645508
Validation loss: 2.6496185871862594

Epoch: 5| Step: 3
Training loss: 3.015691041946411
Validation loss: 2.6529118527648268

Epoch: 5| Step: 4
Training loss: 3.065761089324951
Validation loss: 2.6486446498542704

Epoch: 5| Step: 5
Training loss: 2.5385189056396484
Validation loss: 2.640548067708169

Epoch: 5| Step: 6
Training loss: 2.3931756019592285
Validation loss: 2.641519272199241

Epoch: 5| Step: 7
Training loss: 4.036411762237549
Validation loss: 2.6367212085313696

Epoch: 5| Step: 8
Training loss: 3.3857340812683105
Validation loss: 2.6379783102261123

Epoch: 5| Step: 9
Training loss: 2.7457220554351807
Validation loss: 2.6338849580416115

Epoch: 5| Step: 10
Training loss: 2.755725860595703
Validation loss: 2.638939611373409

Epoch: 44| Step: 0
Training loss: 2.8589046001434326
Validation loss: 2.6352574004921863

Epoch: 5| Step: 1
Training loss: 3.469149112701416
Validation loss: 2.6383148367686937

Epoch: 5| Step: 2
Training loss: 2.5233864784240723
Validation loss: 2.6345531453368483

Epoch: 5| Step: 3
Training loss: 2.5894789695739746
Validation loss: 2.631113308732228

Epoch: 5| Step: 4
Training loss: 2.5053069591522217
Validation loss: 2.6298578349492883

Epoch: 5| Step: 5
Training loss: 2.878959894180298
Validation loss: 2.6292765909625637

Epoch: 5| Step: 6
Training loss: 2.67084002494812
Validation loss: 2.629158704511581

Epoch: 5| Step: 7
Training loss: 2.7373039722442627
Validation loss: 2.6362710857904084

Epoch: 5| Step: 8
Training loss: 3.249274492263794
Validation loss: 2.6363024711608887

Epoch: 5| Step: 9
Training loss: 2.795821189880371
Validation loss: 2.6306791972088557

Epoch: 5| Step: 10
Training loss: 2.5285043716430664
Validation loss: 2.6276702880859375

Epoch: 45| Step: 0
Training loss: 3.011072874069214
Validation loss: 2.625118940107284

Epoch: 5| Step: 1
Training loss: 2.4722957611083984
Validation loss: 2.6217417127342633

Epoch: 5| Step: 2
Training loss: 3.1963162422180176
Validation loss: 2.6189649438345306

Epoch: 5| Step: 3
Training loss: 3.281604766845703
Validation loss: 2.6169130340699227

Epoch: 5| Step: 4
Training loss: 2.7307236194610596
Validation loss: 2.6177091854874805

Epoch: 5| Step: 5
Training loss: 2.492711067199707
Validation loss: 2.6171284670470865

Epoch: 5| Step: 6
Training loss: 2.5806777477264404
Validation loss: 2.617889265860281

Epoch: 5| Step: 7
Training loss: 3.794577121734619
Validation loss: 2.6299207287449993

Epoch: 5| Step: 8
Training loss: 2.124011754989624
Validation loss: 2.6271448930104575

Epoch: 5| Step: 9
Training loss: 2.389463424682617
Validation loss: 2.6174885431925454

Epoch: 5| Step: 10
Training loss: 2.6462886333465576
Validation loss: 2.6149471652123237

Epoch: 46| Step: 0
Training loss: 2.9220266342163086
Validation loss: 2.6158546478517595

Epoch: 5| Step: 1
Training loss: 2.9644112586975098
Validation loss: 2.6104314493876632

Epoch: 5| Step: 2
Training loss: 2.7992873191833496
Validation loss: 2.6099270902654177

Epoch: 5| Step: 3
Training loss: 2.9599523544311523
Validation loss: 2.6061175100265013

Epoch: 5| Step: 4
Training loss: 2.5615127086639404
Validation loss: 2.6055768817983647

Epoch: 5| Step: 5
Training loss: 2.293762683868408
Validation loss: 2.608441029825518

Epoch: 5| Step: 6
Training loss: 3.0585975646972656
Validation loss: 2.609842215814898

Epoch: 5| Step: 7
Training loss: 2.963041305541992
Validation loss: 2.6213137667666198

Epoch: 5| Step: 8
Training loss: 2.6029915809631348
Validation loss: 2.610664636858048

Epoch: 5| Step: 9
Training loss: 2.8865997791290283
Validation loss: 2.607645275772259

Epoch: 5| Step: 10
Training loss: 2.7123897075653076
Validation loss: 2.6021474228110364

Epoch: 47| Step: 0
Training loss: 2.427410840988159
Validation loss: 2.6028084216579312

Epoch: 5| Step: 1
Training loss: 2.43493390083313
Validation loss: 2.60124796436679

Epoch: 5| Step: 2
Training loss: 2.7965569496154785
Validation loss: 2.600644119324223

Epoch: 5| Step: 3
Training loss: 3.4751555919647217
Validation loss: 2.6005724450593353

Epoch: 5| Step: 4
Training loss: 2.6251461505889893
Validation loss: 2.5956067500575895

Epoch: 5| Step: 5
Training loss: 3.311894178390503
Validation loss: 2.6058186818194646

Epoch: 5| Step: 6
Training loss: 2.7082228660583496
Validation loss: 2.5996038862453994

Epoch: 5| Step: 7
Training loss: 2.401390552520752
Validation loss: 2.5995602658999863

Epoch: 5| Step: 8
Training loss: 2.9119091033935547
Validation loss: 2.599397087609896

Epoch: 5| Step: 9
Training loss: 2.587813377380371
Validation loss: 2.6008140476801063

Epoch: 5| Step: 10
Training loss: 2.907358169555664
Validation loss: 2.6014697782454954

Epoch: 48| Step: 0
Training loss: 2.8363420963287354
Validation loss: 2.598097034679946

Epoch: 5| Step: 1
Training loss: 3.1654326915740967
Validation loss: 2.5986469689235894

Epoch: 5| Step: 2
Training loss: 3.499864101409912
Validation loss: 2.601334564147457

Epoch: 5| Step: 3
Training loss: 2.5251617431640625
Validation loss: 2.596688611533052

Epoch: 5| Step: 4
Training loss: 2.2728054523468018
Validation loss: 2.5888748297127346

Epoch: 5| Step: 5
Training loss: 2.5441601276397705
Validation loss: 2.5853833075492614

Epoch: 5| Step: 6
Training loss: 2.238232135772705
Validation loss: 2.586816498028335

Epoch: 5| Step: 7
Training loss: 2.9595961570739746
Validation loss: 2.5802497402314217

Epoch: 5| Step: 8
Training loss: 2.9498660564422607
Validation loss: 2.5796325616939093

Epoch: 5| Step: 9
Training loss: 2.529041290283203
Validation loss: 2.575762125753587

Epoch: 5| Step: 10
Training loss: 2.95597243309021
Validation loss: 2.575774669647217

Epoch: 49| Step: 0
Training loss: 2.211268901824951
Validation loss: 2.5753238226777766

Epoch: 5| Step: 1
Training loss: 2.7890820503234863
Validation loss: 2.581471173994003

Epoch: 5| Step: 2
Training loss: 3.261578321456909
Validation loss: 2.575936699426302

Epoch: 5| Step: 3
Training loss: 2.518599033355713
Validation loss: 2.5712624237101567

Epoch: 5| Step: 4
Training loss: 3.3323283195495605
Validation loss: 2.5722803300426853

Epoch: 5| Step: 5
Training loss: 2.3349993228912354
Validation loss: 2.5713975916626635

Epoch: 5| Step: 6
Training loss: 3.5373973846435547
Validation loss: 2.5756215049374487

Epoch: 5| Step: 7
Training loss: 2.9388999938964844
Validation loss: 2.5757710702957644

Epoch: 5| Step: 8
Training loss: 2.8715617656707764
Validation loss: 2.572906430049609

Epoch: 5| Step: 9
Training loss: 2.552048444747925
Validation loss: 2.5687840523258334

Epoch: 5| Step: 10
Training loss: 1.9534533023834229
Validation loss: 2.566772791647142

Epoch: 50| Step: 0
Training loss: 3.555180788040161
Validation loss: 2.5649739234678206

Epoch: 5| Step: 1
Training loss: 2.4991135597229004
Validation loss: 2.569568762215235

Epoch: 5| Step: 2
Training loss: 2.5467066764831543
Validation loss: 2.571011991911037

Epoch: 5| Step: 3
Training loss: 2.4285216331481934
Validation loss: 2.5756843987331597

Epoch: 5| Step: 4
Training loss: 2.777324914932251
Validation loss: 2.5801784376944266

Epoch: 5| Step: 5
Training loss: 2.979318857192993
Validation loss: 2.5807447612926526

Epoch: 5| Step: 6
Training loss: 3.209756374359131
Validation loss: 2.574735426133679

Epoch: 5| Step: 7
Training loss: 1.897566795349121
Validation loss: 2.572513593140469

Epoch: 5| Step: 8
Training loss: 2.4617791175842285
Validation loss: 2.5745462525275444

Epoch: 5| Step: 9
Training loss: 2.7745823860168457
Validation loss: 2.5596987432049167

Epoch: 5| Step: 10
Training loss: 3.242130994796753
Validation loss: 2.561681403908678

Epoch: 51| Step: 0
Training loss: 2.9670732021331787
Validation loss: 2.5634199291147213

Epoch: 5| Step: 1
Training loss: 2.4548873901367188
Validation loss: 2.567730542152159

Epoch: 5| Step: 2
Training loss: 2.499070167541504
Validation loss: 2.5638996401140766

Epoch: 5| Step: 3
Training loss: 2.8898403644561768
Validation loss: 2.5593976820668867

Epoch: 5| Step: 4
Training loss: 2.3559703826904297
Validation loss: 2.5590729123802594

Epoch: 5| Step: 5
Training loss: 3.6991279125213623
Validation loss: 2.5589136513330604

Epoch: 5| Step: 6
Training loss: 2.664792060852051
Validation loss: 2.562256238793814

Epoch: 5| Step: 7
Training loss: 2.4264893531799316
Validation loss: 2.5581609946425243

Epoch: 5| Step: 8
Training loss: 2.5139963626861572
Validation loss: 2.5544286004958616

Epoch: 5| Step: 9
Training loss: 2.74202299118042
Validation loss: 2.560527519513202

Epoch: 5| Step: 10
Training loss: 3.163470983505249
Validation loss: 2.5564883703826577

Epoch: 52| Step: 0
Training loss: 2.6086158752441406
Validation loss: 2.5632083364712295

Epoch: 5| Step: 1
Training loss: 2.9605650901794434
Validation loss: 2.5551497269702215

Epoch: 5| Step: 2
Training loss: 2.2111430168151855
Validation loss: 2.550343108433549

Epoch: 5| Step: 3
Training loss: 3.0876660346984863
Validation loss: 2.548642404617802

Epoch: 5| Step: 4
Training loss: 2.820185661315918
Validation loss: 2.550453498799314

Epoch: 5| Step: 5
Training loss: 3.12221097946167
Validation loss: 2.5481079727090816

Epoch: 5| Step: 6
Training loss: 2.438948631286621
Validation loss: 2.5503700164056595

Epoch: 5| Step: 7
Training loss: 2.719811201095581
Validation loss: 2.5476260697969826

Epoch: 5| Step: 8
Training loss: 2.305473804473877
Validation loss: 2.546947625375563

Epoch: 5| Step: 9
Training loss: 2.8263919353485107
Validation loss: 2.5462377686654367

Epoch: 5| Step: 10
Training loss: 3.183793544769287
Validation loss: 2.5452696892522995

Epoch: 53| Step: 0
Training loss: 2.882972478866577
Validation loss: 2.543424362777382

Epoch: 5| Step: 1
Training loss: 2.8395752906799316
Validation loss: 2.5423053900400796

Epoch: 5| Step: 2
Training loss: 2.793513536453247
Validation loss: 2.54310247975011

Epoch: 5| Step: 3
Training loss: 2.9176266193389893
Validation loss: 2.5580727361863658

Epoch: 5| Step: 4
Training loss: 2.319103240966797
Validation loss: 2.5708433248663463

Epoch: 5| Step: 5
Training loss: 3.400803327560425
Validation loss: 2.5922348114752

Epoch: 5| Step: 6
Training loss: 2.2079691886901855
Validation loss: 2.5581263880575857

Epoch: 5| Step: 7
Training loss: 3.1053757667541504
Validation loss: 2.5399356478004047

Epoch: 5| Step: 8
Training loss: 2.7202506065368652
Validation loss: 2.542421192251226

Epoch: 5| Step: 9
Training loss: 2.518550157546997
Validation loss: 2.552874593324559

Epoch: 5| Step: 10
Training loss: 2.504732131958008
Validation loss: 2.5666260796208538

Epoch: 54| Step: 0
Training loss: 2.3209102153778076
Validation loss: 2.586239445594049

Epoch: 5| Step: 1
Training loss: 3.3329367637634277
Validation loss: 2.5749785195114794

Epoch: 5| Step: 2
Training loss: 3.3046703338623047
Validation loss: 2.557974894841512

Epoch: 5| Step: 3
Training loss: 2.4257302284240723
Validation loss: 2.5486651723102858

Epoch: 5| Step: 4
Training loss: 2.702681303024292
Validation loss: 2.5483559664859565

Epoch: 5| Step: 5
Training loss: 2.2408294677734375
Validation loss: 2.5528130505674627

Epoch: 5| Step: 6
Training loss: 3.023209810256958
Validation loss: 2.568321371591219

Epoch: 5| Step: 7
Training loss: 2.8948159217834473
Validation loss: 2.6073560099447928

Epoch: 5| Step: 8
Training loss: 2.8934950828552246
Validation loss: 2.667421720361197

Epoch: 5| Step: 9
Training loss: 2.636707067489624
Validation loss: 2.6693871098179973

Epoch: 5| Step: 10
Training loss: 2.7489638328552246
Validation loss: 2.612329257431851

Epoch: 55| Step: 0
Training loss: 2.4843437671661377
Validation loss: 2.564033980010658

Epoch: 5| Step: 1
Training loss: 2.9820761680603027
Validation loss: 2.5422488797095513

Epoch: 5| Step: 2
Training loss: 2.9603569507598877
Validation loss: 2.538396486672022

Epoch: 5| Step: 3
Training loss: 2.9086034297943115
Validation loss: 2.5390188540181806

Epoch: 5| Step: 4
Training loss: 2.9850544929504395
Validation loss: 2.5393415087012836

Epoch: 5| Step: 5
Training loss: 2.8595666885375977
Validation loss: 2.541787885850476

Epoch: 5| Step: 6
Training loss: 2.825929880142212
Validation loss: 2.5377603679574947

Epoch: 5| Step: 7
Training loss: 2.178575038909912
Validation loss: 2.532752747176796

Epoch: 5| Step: 8
Training loss: 2.394282579421997
Validation loss: 2.529490460631668

Epoch: 5| Step: 9
Training loss: 2.1213395595550537
Validation loss: 2.532895544523834

Epoch: 5| Step: 10
Training loss: 3.478217363357544
Validation loss: 2.5332552079231507

Epoch: 56| Step: 0
Training loss: 2.324054479598999
Validation loss: 2.5495606366024224

Epoch: 5| Step: 1
Training loss: 2.6331095695495605
Validation loss: 2.566536788017519

Epoch: 5| Step: 2
Training loss: 2.851226329803467
Validation loss: 2.56886391229527

Epoch: 5| Step: 3
Training loss: 2.9252610206604004
Validation loss: 2.546286215064346

Epoch: 5| Step: 4
Training loss: 2.6320033073425293
Validation loss: 2.526290039862356

Epoch: 5| Step: 5
Training loss: 2.6852474212646484
Validation loss: 2.5249116036199752

Epoch: 5| Step: 6
Training loss: 2.8555216789245605
Validation loss: 2.530040171838576

Epoch: 5| Step: 7
Training loss: 2.2537243366241455
Validation loss: 2.5375435659962315

Epoch: 5| Step: 8
Training loss: 3.33469820022583
Validation loss: 2.5374055216389317

Epoch: 5| Step: 9
Training loss: 3.366368055343628
Validation loss: 2.5391961630954536

Epoch: 5| Step: 10
Training loss: 2.192671298980713
Validation loss: 2.5388587802969

Epoch: 57| Step: 0
Training loss: 2.8343822956085205
Validation loss: 2.5446450812842256

Epoch: 5| Step: 1
Training loss: 3.219599962234497
Validation loss: 2.539393940279561

Epoch: 5| Step: 2
Training loss: 2.617522716522217
Validation loss: 2.538476808096773

Epoch: 5| Step: 3
Training loss: 2.781907320022583
Validation loss: 2.538878761312013

Epoch: 5| Step: 4
Training loss: 3.1128835678100586
Validation loss: 2.541149657259705

Epoch: 5| Step: 5
Training loss: 2.34694766998291
Validation loss: 2.5382437321447555

Epoch: 5| Step: 6
Training loss: 3.0755722522735596
Validation loss: 2.534349436401039

Epoch: 5| Step: 7
Training loss: 2.284245014190674
Validation loss: 2.5347899698442027

Epoch: 5| Step: 8
Training loss: 2.4302287101745605
Validation loss: 2.5302650133768716

Epoch: 5| Step: 9
Training loss: 2.192628860473633
Validation loss: 2.532421132569672

Epoch: 5| Step: 10
Training loss: 3.4228577613830566
Validation loss: 2.5274206284553773

Epoch: 58| Step: 0
Training loss: 2.2803127765655518
Validation loss: 2.525529484595022

Epoch: 5| Step: 1
Training loss: 2.761275291442871
Validation loss: 2.52555747442348

Epoch: 5| Step: 2
Training loss: 3.141085386276245
Validation loss: 2.52134608709684

Epoch: 5| Step: 3
Training loss: 2.1126084327697754
Validation loss: 2.5199219744692565

Epoch: 5| Step: 4
Training loss: 2.7716948986053467
Validation loss: 2.520368855486634

Epoch: 5| Step: 5
Training loss: 3.0595908164978027
Validation loss: 2.5191848867683

Epoch: 5| Step: 6
Training loss: 2.462498188018799
Validation loss: 2.5226955670182423

Epoch: 5| Step: 7
Training loss: 2.628237247467041
Validation loss: 2.521233412527269

Epoch: 5| Step: 8
Training loss: 2.8938817977905273
Validation loss: 2.5216090961169173

Epoch: 5| Step: 9
Training loss: 3.1131861209869385
Validation loss: 2.5145360628763833

Epoch: 5| Step: 10
Training loss: 2.8125319480895996
Validation loss: 2.5129544991318897

Epoch: 59| Step: 0
Training loss: 2.4930524826049805
Validation loss: 2.517607911940544

Epoch: 5| Step: 1
Training loss: 2.6006431579589844
Validation loss: 2.5234803153622534

Epoch: 5| Step: 2
Training loss: 3.0769190788269043
Validation loss: 2.5248902074752317

Epoch: 5| Step: 3
Training loss: 2.4471802711486816
Validation loss: 2.5277260503461285

Epoch: 5| Step: 4
Training loss: 2.8725619316101074
Validation loss: 2.51860640382254

Epoch: 5| Step: 5
Training loss: 3.1464617252349854
Validation loss: 2.5125602855477283

Epoch: 5| Step: 6
Training loss: 3.100027322769165
Validation loss: 2.5023136523462113

Epoch: 5| Step: 7
Training loss: 1.9426733255386353
Validation loss: 2.5087308858030584

Epoch: 5| Step: 8
Training loss: 2.79215145111084
Validation loss: 2.505426691424462

Epoch: 5| Step: 9
Training loss: 2.5797088146209717
Validation loss: 2.5046997890677503

Epoch: 5| Step: 10
Training loss: 2.9056615829467773
Validation loss: 2.504653469208748

Epoch: 60| Step: 0
Training loss: 3.009383201599121
Validation loss: 2.502664742931243

Epoch: 5| Step: 1
Training loss: 3.074995517730713
Validation loss: 2.504746160199565

Epoch: 5| Step: 2
Training loss: 2.718433380126953
Validation loss: 2.502178261356969

Epoch: 5| Step: 3
Training loss: 2.484616279602051
Validation loss: 2.502147738651563

Epoch: 5| Step: 4
Training loss: 2.583055019378662
Validation loss: 2.505045944644559

Epoch: 5| Step: 5
Training loss: 2.90988826751709
Validation loss: 2.4981196080484698

Epoch: 5| Step: 6
Training loss: 2.6482205390930176
Validation loss: 2.495038245313911

Epoch: 5| Step: 7
Training loss: 2.3587937355041504
Validation loss: 2.502454665399367

Epoch: 5| Step: 8
Training loss: 2.6575851440429688
Validation loss: 2.5029216133138186

Epoch: 5| Step: 9
Training loss: 2.4644622802734375
Validation loss: 2.5210726209866103

Epoch: 5| Step: 10
Training loss: 3.0155093669891357
Validation loss: 2.53774191230856

Epoch: 61| Step: 0
Training loss: 2.94667649269104
Validation loss: 2.5582707928073023

Epoch: 5| Step: 1
Training loss: 3.0426037311553955
Validation loss: 2.5407381160284883

Epoch: 5| Step: 2
Training loss: 2.9134814739227295
Validation loss: 2.531961307730726

Epoch: 5| Step: 3
Training loss: 2.77372407913208
Validation loss: 2.505135420830019

Epoch: 5| Step: 4
Training loss: 2.7829577922821045
Validation loss: 2.499437488535399

Epoch: 5| Step: 5
Training loss: 2.7664103507995605
Validation loss: 2.49645302372594

Epoch: 5| Step: 6
Training loss: 2.4623425006866455
Validation loss: 2.5009423276429534

Epoch: 5| Step: 7
Training loss: 2.5884809494018555
Validation loss: 2.4968273024405203

Epoch: 5| Step: 8
Training loss: 3.2600014209747314
Validation loss: 2.5074887455150647

Epoch: 5| Step: 9
Training loss: 1.9236894845962524
Validation loss: 2.5062058048863567

Epoch: 5| Step: 10
Training loss: 2.2515993118286133
Validation loss: 2.5154633650215725

Epoch: 62| Step: 0
Training loss: 2.513028621673584
Validation loss: 2.5132981115771877

Epoch: 5| Step: 1
Training loss: 2.719463348388672
Validation loss: 2.5054046953878095

Epoch: 5| Step: 2
Training loss: 2.8628413677215576
Validation loss: 2.5066022872924805

Epoch: 5| Step: 3
Training loss: 2.787848949432373
Validation loss: 2.4960399007284515

Epoch: 5| Step: 4
Training loss: 3.3873207569122314
Validation loss: 2.491187910879812

Epoch: 5| Step: 5
Training loss: 2.6466970443725586
Validation loss: 2.4850423951302805

Epoch: 5| Step: 6
Training loss: 2.2229504585266113
Validation loss: 2.4866117123634583

Epoch: 5| Step: 7
Training loss: 2.432596206665039
Validation loss: 2.4936293555844213

Epoch: 5| Step: 8
Training loss: 2.8057899475097656
Validation loss: 2.4946036313169744

Epoch: 5| Step: 9
Training loss: 2.404188632965088
Validation loss: 2.5082476831251577

Epoch: 5| Step: 10
Training loss: 3.056143283843994
Validation loss: 2.508923510069488

Epoch: 63| Step: 0
Training loss: 2.2650017738342285
Validation loss: 2.49817673108911

Epoch: 5| Step: 1
Training loss: 2.274221420288086
Validation loss: 2.494918454077936

Epoch: 5| Step: 2
Training loss: 2.9076945781707764
Validation loss: 2.4823950490643902

Epoch: 5| Step: 3
Training loss: 2.8380117416381836
Validation loss: 2.4819215677117787

Epoch: 5| Step: 4
Training loss: 2.517575740814209
Validation loss: 2.4782013226580877

Epoch: 5| Step: 5
Training loss: 2.787423849105835
Validation loss: 2.4858994458311345

Epoch: 5| Step: 6
Training loss: 3.3383071422576904
Validation loss: 2.4829928080240884

Epoch: 5| Step: 7
Training loss: 2.6991496086120605
Validation loss: 2.4846302642617175

Epoch: 5| Step: 8
Training loss: 2.7427613735198975
Validation loss: 2.481510090571578

Epoch: 5| Step: 9
Training loss: 2.743544816970825
Validation loss: 2.4828492672212663

Epoch: 5| Step: 10
Training loss: 2.530211925506592
Validation loss: 2.4809409495322936

Epoch: 64| Step: 0
Training loss: 1.951923131942749
Validation loss: 2.4899326370608423

Epoch: 5| Step: 1
Training loss: 2.6756248474121094
Validation loss: 2.509956200917562

Epoch: 5| Step: 2
Training loss: 3.0213584899902344
Validation loss: 2.5202956648283106

Epoch: 5| Step: 3
Training loss: 3.4354724884033203
Validation loss: 2.541976474946545

Epoch: 5| Step: 4
Training loss: 2.7297699451446533
Validation loss: 2.5697326967793126

Epoch: 5| Step: 5
Training loss: 2.661411762237549
Validation loss: 2.58015045555689

Epoch: 5| Step: 6
Training loss: 2.4162774085998535
Validation loss: 2.569049054576505

Epoch: 5| Step: 7
Training loss: 2.6343836784362793
Validation loss: 2.5067191123962402

Epoch: 5| Step: 8
Training loss: 3.0488057136535645
Validation loss: 2.4839782022660777

Epoch: 5| Step: 9
Training loss: 2.6803715229034424
Validation loss: 2.4749302428255797

Epoch: 5| Step: 10
Training loss: 2.626828908920288
Validation loss: 2.4897259178981987

Epoch: 65| Step: 0
Training loss: 2.3502063751220703
Validation loss: 2.5064756716451337

Epoch: 5| Step: 1
Training loss: 2.9661762714385986
Validation loss: 2.5222944316043647

Epoch: 5| Step: 2
Training loss: 2.952296018600464
Validation loss: 2.519679977047828

Epoch: 5| Step: 3
Training loss: 2.75007700920105
Validation loss: 2.51226669485851

Epoch: 5| Step: 4
Training loss: 3.0748085975646973
Validation loss: 2.5017608468250563

Epoch: 5| Step: 5
Training loss: 3.711264133453369
Validation loss: 2.494622997058335

Epoch: 5| Step: 6
Training loss: 2.616638660430908
Validation loss: 2.486439771549676

Epoch: 5| Step: 7
Training loss: 2.712653636932373
Validation loss: 2.478376055276522

Epoch: 5| Step: 8
Training loss: 2.0583624839782715
Validation loss: 2.473203561639273

Epoch: 5| Step: 9
Training loss: 2.1532764434814453
Validation loss: 2.484726910950035

Epoch: 5| Step: 10
Training loss: 2.570979118347168
Validation loss: 2.50225136228787

Epoch: 66| Step: 0
Training loss: 2.7164759635925293
Validation loss: 2.5476625888578353

Epoch: 5| Step: 1
Training loss: 3.092010021209717
Validation loss: 2.5604985042284896

Epoch: 5| Step: 2
Training loss: 2.8407928943634033
Validation loss: 2.5740650905075895

Epoch: 5| Step: 3
Training loss: 2.4727368354797363
Validation loss: 2.5388224150544856

Epoch: 5| Step: 4
Training loss: 2.5561153888702393
Validation loss: 2.5006385772458968

Epoch: 5| Step: 5
Training loss: 2.312579393386841
Validation loss: 2.4832846913286435

Epoch: 5| Step: 6
Training loss: 2.621138095855713
Validation loss: 2.474533111818375

Epoch: 5| Step: 7
Training loss: 2.6917219161987305
Validation loss: 2.4692675144441667

Epoch: 5| Step: 8
Training loss: 2.9522271156311035
Validation loss: 2.469930212984803

Epoch: 5| Step: 9
Training loss: 2.9050474166870117
Validation loss: 2.458749299408287

Epoch: 5| Step: 10
Training loss: 2.623516082763672
Validation loss: 2.4605698841874317

Epoch: 67| Step: 0
Training loss: 3.2164924144744873
Validation loss: 2.4548888180845525

Epoch: 5| Step: 1
Training loss: 2.916522741317749
Validation loss: 2.4557486580264185

Epoch: 5| Step: 2
Training loss: 2.559173107147217
Validation loss: 2.4528769728957966

Epoch: 5| Step: 3
Training loss: 2.438925266265869
Validation loss: 2.4561217472117436

Epoch: 5| Step: 4
Training loss: 2.283341884613037
Validation loss: 2.4526592018783733

Epoch: 5| Step: 5
Training loss: 3.268155574798584
Validation loss: 2.4572543021171325

Epoch: 5| Step: 6
Training loss: 2.5013365745544434
Validation loss: 2.465188390465193

Epoch: 5| Step: 7
Training loss: 2.4750053882598877
Validation loss: 2.4801726725793656

Epoch: 5| Step: 8
Training loss: 2.38517165184021
Validation loss: 2.498766332544306

Epoch: 5| Step: 9
Training loss: 2.734534502029419
Validation loss: 2.494926480836766

Epoch: 5| Step: 10
Training loss: 2.89335036277771
Validation loss: 2.4814096086768695

Epoch: 68| Step: 0
Training loss: 1.6974315643310547
Validation loss: 2.4677446196156163

Epoch: 5| Step: 1
Training loss: 2.9535951614379883
Validation loss: 2.4637049885206324

Epoch: 5| Step: 2
Training loss: 2.865973472595215
Validation loss: 2.4622655107129003

Epoch: 5| Step: 3
Training loss: 2.6605396270751953
Validation loss: 2.463014230933241

Epoch: 5| Step: 4
Training loss: 3.082381010055542
Validation loss: 2.4624609408840055

Epoch: 5| Step: 5
Training loss: 2.3429617881774902
Validation loss: 2.460769996848158

Epoch: 5| Step: 6
Training loss: 2.694099187850952
Validation loss: 2.459249832296884

Epoch: 5| Step: 7
Training loss: 2.241597890853882
Validation loss: 2.464525358651274

Epoch: 5| Step: 8
Training loss: 3.073404312133789
Validation loss: 2.466482731603807

Epoch: 5| Step: 9
Training loss: 2.8845620155334473
Validation loss: 2.470614125651698

Epoch: 5| Step: 10
Training loss: 2.995515823364258
Validation loss: 2.467707654481293

Epoch: 69| Step: 0
Training loss: 2.426025152206421
Validation loss: 2.46787848523868

Epoch: 5| Step: 1
Training loss: 3.109297752380371
Validation loss: 2.461968139935565

Epoch: 5| Step: 2
Training loss: 2.5253713130950928
Validation loss: 2.455621196377662

Epoch: 5| Step: 3
Training loss: 2.5668342113494873
Validation loss: 2.4530026041051394

Epoch: 5| Step: 4
Training loss: 2.363957166671753
Validation loss: 2.4496993352008123

Epoch: 5| Step: 5
Training loss: 2.0648014545440674
Validation loss: 2.4503120799218454

Epoch: 5| Step: 6
Training loss: 2.9732375144958496
Validation loss: 2.4465528713759555

Epoch: 5| Step: 7
Training loss: 3.291140079498291
Validation loss: 2.446501765199887

Epoch: 5| Step: 8
Training loss: 2.637643814086914
Validation loss: 2.4512196843342116

Epoch: 5| Step: 9
Training loss: 2.3226277828216553
Validation loss: 2.4478488327354513

Epoch: 5| Step: 10
Training loss: 3.197436809539795
Validation loss: 2.451807745041386

Epoch: 70| Step: 0
Training loss: 1.997496247291565
Validation loss: 2.4481758097166657

Epoch: 5| Step: 1
Training loss: 2.6211628913879395
Validation loss: 2.457803169886271

Epoch: 5| Step: 2
Training loss: 2.6305794715881348
Validation loss: 2.467194587953629

Epoch: 5| Step: 3
Training loss: 3.051851749420166
Validation loss: 2.4752331420939457

Epoch: 5| Step: 4
Training loss: 2.0735599994659424
Validation loss: 2.4706556130481023

Epoch: 5| Step: 5
Training loss: 3.478950023651123
Validation loss: 2.4740067630685787

Epoch: 5| Step: 6
Training loss: 2.1498405933380127
Validation loss: 2.4623513555014007

Epoch: 5| Step: 7
Training loss: 2.634976863861084
Validation loss: 2.4528429841482513

Epoch: 5| Step: 8
Training loss: 2.592416763305664
Validation loss: 2.4538312958132837

Epoch: 5| Step: 9
Training loss: 3.2961056232452393
Validation loss: 2.4617172953903035

Epoch: 5| Step: 10
Training loss: 2.9715137481689453
Validation loss: 2.4441440387438704

Epoch: 71| Step: 0
Training loss: 3.1751718521118164
Validation loss: 2.4486714409243677

Epoch: 5| Step: 1
Training loss: 2.4159023761749268
Validation loss: 2.4476246500527985

Epoch: 5| Step: 2
Training loss: 3.6553077697753906
Validation loss: 2.4455977870571997

Epoch: 5| Step: 3
Training loss: 2.09476900100708
Validation loss: 2.4544581290214293

Epoch: 5| Step: 4
Training loss: 2.4697558879852295
Validation loss: 2.4538032624029342

Epoch: 5| Step: 5
Training loss: 1.7470033168792725
Validation loss: 2.460156745808099

Epoch: 5| Step: 6
Training loss: 2.372079372406006
Validation loss: 2.4708384493345856

Epoch: 5| Step: 7
Training loss: 2.864022731781006
Validation loss: 2.4855254773170716

Epoch: 5| Step: 8
Training loss: 2.9999961853027344
Validation loss: 2.4922055352118706

Epoch: 5| Step: 9
Training loss: 2.521388530731201
Validation loss: 2.491371147094234

Epoch: 5| Step: 10
Training loss: 3.222316265106201
Validation loss: 2.5032843236000306

Epoch: 72| Step: 0
Training loss: 3.674044132232666
Validation loss: 2.48335297389697

Epoch: 5| Step: 1
Training loss: 2.3655378818511963
Validation loss: 2.4644319165137505

Epoch: 5| Step: 2
Training loss: 1.9706852436065674
Validation loss: 2.4642038268427693

Epoch: 5| Step: 3
Training loss: 2.481658935546875
Validation loss: 2.4571114765700472

Epoch: 5| Step: 4
Training loss: 2.1959073543548584
Validation loss: 2.45512342837549

Epoch: 5| Step: 5
Training loss: 2.830253839492798
Validation loss: 2.4631709334670857

Epoch: 5| Step: 6
Training loss: 3.2882494926452637
Validation loss: 2.466111419021442

Epoch: 5| Step: 7
Training loss: 2.8237862586975098
Validation loss: 2.461071365623064

Epoch: 5| Step: 8
Training loss: 2.4349820613861084
Validation loss: 2.45819596064988

Epoch: 5| Step: 9
Training loss: 2.5205588340759277
Validation loss: 2.450716213513446

Epoch: 5| Step: 10
Training loss: 2.7725250720977783
Validation loss: 2.450389903078797

Epoch: 73| Step: 0
Training loss: 2.165894031524658
Validation loss: 2.439438504557456

Epoch: 5| Step: 1
Training loss: 3.0188968181610107
Validation loss: 2.4413159008949035

Epoch: 5| Step: 2
Training loss: 2.8603320121765137
Validation loss: 2.4361289585790327

Epoch: 5| Step: 3
Training loss: 2.627333641052246
Validation loss: 2.4341954620935584

Epoch: 5| Step: 4
Training loss: 1.9615644216537476
Validation loss: 2.435705825846682

Epoch: 5| Step: 5
Training loss: 2.7584285736083984
Validation loss: 2.434059530176142

Epoch: 5| Step: 6
Training loss: 2.807842493057251
Validation loss: 2.4344198242310555

Epoch: 5| Step: 7
Training loss: 3.077810287475586
Validation loss: 2.436400028967088

Epoch: 5| Step: 8
Training loss: 2.55657958984375
Validation loss: 2.433669259471278

Epoch: 5| Step: 9
Training loss: 2.7549078464508057
Validation loss: 2.441313733336746

Epoch: 5| Step: 10
Training loss: 2.734363555908203
Validation loss: 2.441326266975813

Epoch: 74| Step: 0
Training loss: 2.6325886249542236
Validation loss: 2.4520281924996326

Epoch: 5| Step: 1
Training loss: 2.7427515983581543
Validation loss: 2.4661300771979877

Epoch: 5| Step: 2
Training loss: 2.9978508949279785
Validation loss: 2.482096879712997

Epoch: 5| Step: 3
Training loss: 2.208414077758789
Validation loss: 2.4822534002283567

Epoch: 5| Step: 4
Training loss: 2.540266752243042
Validation loss: 2.4680522436736734

Epoch: 5| Step: 5
Training loss: 2.9928805828094482
Validation loss: 2.460537500278924

Epoch: 5| Step: 6
Training loss: 2.8928542137145996
Validation loss: 2.454241998734013

Epoch: 5| Step: 7
Training loss: 2.6005959510803223
Validation loss: 2.4481258828152894

Epoch: 5| Step: 8
Training loss: 2.7219555377960205
Validation loss: 2.433893495990384

Epoch: 5| Step: 9
Training loss: 2.4896466732025146
Validation loss: 2.435859262302358

Epoch: 5| Step: 10
Training loss: 2.573579788208008
Validation loss: 2.43151076891089

Epoch: 75| Step: 0
Training loss: 2.7360763549804688
Validation loss: 2.435069412313482

Epoch: 5| Step: 1
Training loss: 2.3980095386505127
Validation loss: 2.4319889186530985

Epoch: 5| Step: 2
Training loss: 2.9121437072753906
Validation loss: 2.43873772569882

Epoch: 5| Step: 3
Training loss: 1.8858410120010376
Validation loss: 2.4369331047099125

Epoch: 5| Step: 4
Training loss: 3.136467456817627
Validation loss: 2.4492222826967955

Epoch: 5| Step: 5
Training loss: 2.5084166526794434
Validation loss: 2.4476516887705815

Epoch: 5| Step: 6
Training loss: 2.908843994140625
Validation loss: 2.455651508864536

Epoch: 5| Step: 7
Training loss: 2.5241878032684326
Validation loss: 2.4511725261647213

Epoch: 5| Step: 8
Training loss: 2.618900775909424
Validation loss: 2.4494654337565103

Epoch: 5| Step: 9
Training loss: 2.7580013275146484
Validation loss: 2.4431753184205744

Epoch: 5| Step: 10
Training loss: 2.8673043251037598
Validation loss: 2.4484301690132386

Epoch: 76| Step: 0
Training loss: 3.050960063934326
Validation loss: 2.4473794326987317

Epoch: 5| Step: 1
Training loss: 2.7899327278137207
Validation loss: 2.4394760362563597

Epoch: 5| Step: 2
Training loss: 2.654115915298462
Validation loss: 2.4377975899686097

Epoch: 5| Step: 3
Training loss: 1.7507305145263672
Validation loss: 2.4399712726634037

Epoch: 5| Step: 4
Training loss: 2.0903313159942627
Validation loss: 2.4385987404854066

Epoch: 5| Step: 5
Training loss: 2.1221396923065186
Validation loss: 2.4415439790295017

Epoch: 5| Step: 6
Training loss: 2.8965306282043457
Validation loss: 2.469272029015326

Epoch: 5| Step: 7
Training loss: 3.1005616188049316
Validation loss: 2.462643446460847

Epoch: 5| Step: 8
Training loss: 3.0998406410217285
Validation loss: 2.455914484557285

Epoch: 5| Step: 9
Training loss: 3.172790050506592
Validation loss: 2.438133660183158

Epoch: 5| Step: 10
Training loss: 2.4695873260498047
Validation loss: 2.4382820180667344

Epoch: 77| Step: 0
Training loss: 2.1757729053497314
Validation loss: 2.4399141650046072

Epoch: 5| Step: 1
Training loss: 1.9402673244476318
Validation loss: 2.443984890496859

Epoch: 5| Step: 2
Training loss: 2.968590259552002
Validation loss: 2.437688091749786

Epoch: 5| Step: 3
Training loss: 2.7982113361358643
Validation loss: 2.4659237118177515

Epoch: 5| Step: 4
Training loss: 2.865628719329834
Validation loss: 2.450821958562379

Epoch: 5| Step: 5
Training loss: 2.7187232971191406
Validation loss: 2.4672050629892657

Epoch: 5| Step: 6
Training loss: 2.501737594604492
Validation loss: 2.465418028575118

Epoch: 5| Step: 7
Training loss: 2.48218035697937
Validation loss: 2.437891362815775

Epoch: 5| Step: 8
Training loss: 2.3470895290374756
Validation loss: 2.422935414057906

Epoch: 5| Step: 9
Training loss: 3.348511219024658
Validation loss: 2.419346832459973

Epoch: 5| Step: 10
Training loss: 3.192021369934082
Validation loss: 2.418759845918225

Epoch: 78| Step: 0
Training loss: 2.8389878273010254
Validation loss: 2.4231235083713325

Epoch: 5| Step: 1
Training loss: 2.4238173961639404
Validation loss: 2.4249055436862412

Epoch: 5| Step: 2
Training loss: 2.260164260864258
Validation loss: 2.4253843984296246

Epoch: 5| Step: 3
Training loss: 2.29203724861145
Validation loss: 2.4307157275497273

Epoch: 5| Step: 4
Training loss: 2.3530972003936768
Validation loss: 2.431720454205749

Epoch: 5| Step: 5
Training loss: 2.576127529144287
Validation loss: 2.4334433591493996

Epoch: 5| Step: 6
Training loss: 2.8712515830993652
Validation loss: 2.439586798350016

Epoch: 5| Step: 7
Training loss: 3.1885762214660645
Validation loss: 2.4461068184145036

Epoch: 5| Step: 8
Training loss: 2.5164003372192383
Validation loss: 2.451219056242256

Epoch: 5| Step: 9
Training loss: 2.8430352210998535
Validation loss: 2.445336298276019

Epoch: 5| Step: 10
Training loss: 3.15777850151062
Validation loss: 2.4348173782389653

Epoch: 79| Step: 0
Training loss: 2.9028637409210205
Validation loss: 2.4387783235119236

Epoch: 5| Step: 1
Training loss: 3.067692279815674
Validation loss: 2.4334567516080794

Epoch: 5| Step: 2
Training loss: 2.8691399097442627
Validation loss: 2.456447970482611

Epoch: 5| Step: 3
Training loss: 1.925731897354126
Validation loss: 2.450415616394371

Epoch: 5| Step: 4
Training loss: 2.2898037433624268
Validation loss: 2.4597884788308093

Epoch: 5| Step: 5
Training loss: 2.4975674152374268
Validation loss: 2.4679299580153597

Epoch: 5| Step: 6
Training loss: 2.3448739051818848
Validation loss: 2.4525963516645533

Epoch: 5| Step: 7
Training loss: 2.6817445755004883
Validation loss: 2.4321015214407318

Epoch: 5| Step: 8
Training loss: 2.817045211791992
Validation loss: 2.423633888203611

Epoch: 5| Step: 9
Training loss: 2.907458782196045
Validation loss: 2.42107633877826

Epoch: 5| Step: 10
Training loss: 3.076164484024048
Validation loss: 2.420600729603921

Epoch: 80| Step: 0
Training loss: 2.8642311096191406
Validation loss: 2.4202443835555867

Epoch: 5| Step: 1
Training loss: 1.9864654541015625
Validation loss: 2.414516072119436

Epoch: 5| Step: 2
Training loss: 2.1548666954040527
Validation loss: 2.408873414480558

Epoch: 5| Step: 3
Training loss: 3.0311717987060547
Validation loss: 2.412899935117332

Epoch: 5| Step: 4
Training loss: 2.9022696018218994
Validation loss: 2.413301270495179

Epoch: 5| Step: 5
Training loss: 3.3715789318084717
Validation loss: 2.4225484991586335

Epoch: 5| Step: 6
Training loss: 2.7152621746063232
Validation loss: 2.4220049637620167

Epoch: 5| Step: 7
Training loss: 2.188616991043091
Validation loss: 2.4292200406392417

Epoch: 5| Step: 8
Training loss: 2.7234437465667725
Validation loss: 2.4433932535109983

Epoch: 5| Step: 9
Training loss: 2.8036134243011475
Validation loss: 2.4586845623549594

Epoch: 5| Step: 10
Training loss: 2.2996256351470947
Validation loss: 2.470211608435518

Epoch: 81| Step: 0
Training loss: 2.856358051300049
Validation loss: 2.473185998137279

Epoch: 5| Step: 1
Training loss: 3.3172714710235596
Validation loss: 2.4702390291357554

Epoch: 5| Step: 2
Training loss: 2.2482450008392334
Validation loss: 2.4703556978574364

Epoch: 5| Step: 3
Training loss: 2.39363694190979
Validation loss: 2.451192222615724

Epoch: 5| Step: 4
Training loss: 3.331282138824463
Validation loss: 2.431298396920645

Epoch: 5| Step: 5
Training loss: 2.801570415496826
Validation loss: 2.4118879238764444

Epoch: 5| Step: 6
Training loss: 2.7906978130340576
Validation loss: 2.4016940670628704

Epoch: 5| Step: 7
Training loss: 2.9026598930358887
Validation loss: 2.403960991931218

Epoch: 5| Step: 8
Training loss: 2.63966965675354
Validation loss: 2.4076179227521344

Epoch: 5| Step: 9
Training loss: 2.0175247192382812
Validation loss: 2.414589653732956

Epoch: 5| Step: 10
Training loss: 1.9241642951965332
Validation loss: 2.414952496046661

Epoch: 82| Step: 0
Training loss: 2.3878042697906494
Validation loss: 2.412012756511729

Epoch: 5| Step: 1
Training loss: 2.7464511394500732
Validation loss: 2.4040307921748005

Epoch: 5| Step: 2
Training loss: 2.6953988075256348
Validation loss: 2.3979066007880756

Epoch: 5| Step: 3
Training loss: 2.994480848312378
Validation loss: 2.4058793821642475

Epoch: 5| Step: 4
Training loss: 2.4684455394744873
Validation loss: 2.411297618701894

Epoch: 5| Step: 5
Training loss: 2.2565014362335205
Validation loss: 2.4214282215282483

Epoch: 5| Step: 6
Training loss: 2.932633399963379
Validation loss: 2.434162888475644

Epoch: 5| Step: 7
Training loss: 2.8778347969055176
Validation loss: 2.4369069581390708

Epoch: 5| Step: 8
Training loss: 2.8451545238494873
Validation loss: 2.438508579807897

Epoch: 5| Step: 9
Training loss: 2.975179672241211
Validation loss: 2.431795766276698

Epoch: 5| Step: 10
Training loss: 1.8816041946411133
Validation loss: 2.422172813005345

Epoch: 83| Step: 0
Training loss: 2.6017937660217285
Validation loss: 2.424802703242148

Epoch: 5| Step: 1
Training loss: 1.8488824367523193
Validation loss: 2.424333087859615

Epoch: 5| Step: 2
Training loss: 2.802159547805786
Validation loss: 2.434006183378158

Epoch: 5| Step: 3
Training loss: 2.8613247871398926
Validation loss: 2.429159713047807

Epoch: 5| Step: 4
Training loss: 2.3774735927581787
Validation loss: 2.4248097878630444

Epoch: 5| Step: 5
Training loss: 2.613023042678833
Validation loss: 2.4225079449274207

Epoch: 5| Step: 6
Training loss: 3.6916701793670654
Validation loss: 2.4148704774918093

Epoch: 5| Step: 7
Training loss: 2.4516994953155518
Validation loss: 2.4023384509548062

Epoch: 5| Step: 8
Training loss: 2.683527708053589
Validation loss: 2.3982464805726083

Epoch: 5| Step: 9
Training loss: 2.7910056114196777
Validation loss: 2.3952065026888283

Epoch: 5| Step: 10
Training loss: 2.2686562538146973
Validation loss: 2.3974131512385544

Epoch: 84| Step: 0
Training loss: 2.5545010566711426
Validation loss: 2.400743502442555

Epoch: 5| Step: 1
Training loss: 2.2256462574005127
Validation loss: 2.4035350712396766

Epoch: 5| Step: 2
Training loss: 2.533987522125244
Validation loss: 2.40051838403107

Epoch: 5| Step: 3
Training loss: 2.6068522930145264
Validation loss: 2.4010614861724195

Epoch: 5| Step: 4
Training loss: 2.6431713104248047
Validation loss: 2.4034196125563754

Epoch: 5| Step: 5
Training loss: 2.2404658794403076
Validation loss: 2.4039493324936076

Epoch: 5| Step: 6
Training loss: 2.682725429534912
Validation loss: 2.4091039575556272

Epoch: 5| Step: 7
Training loss: 2.8353524208068848
Validation loss: 2.414007806008862

Epoch: 5| Step: 8
Training loss: 3.296604871749878
Validation loss: 2.4205746343058925

Epoch: 5| Step: 9
Training loss: 3.128279209136963
Validation loss: 2.421608022464219

Epoch: 5| Step: 10
Training loss: 2.209399461746216
Validation loss: 2.425168378378755

Epoch: 85| Step: 0
Training loss: 2.0442488193511963
Validation loss: 2.4148140491977816

Epoch: 5| Step: 1
Training loss: 2.145789623260498
Validation loss: 2.4173322467393774

Epoch: 5| Step: 2
Training loss: 3.5828890800476074
Validation loss: 2.4169986812017297

Epoch: 5| Step: 3
Training loss: 3.206319808959961
Validation loss: 2.415451036986484

Epoch: 5| Step: 4
Training loss: 1.803272008895874
Validation loss: 2.4052725889349498

Epoch: 5| Step: 5
Training loss: 2.0161218643188477
Validation loss: 2.401166146801364

Epoch: 5| Step: 6
Training loss: 2.796121597290039
Validation loss: 2.3990957762605403

Epoch: 5| Step: 7
Training loss: 3.159358501434326
Validation loss: 2.388112985959617

Epoch: 5| Step: 8
Training loss: 3.22261381149292
Validation loss: 2.391706766620759

Epoch: 5| Step: 9
Training loss: 2.348360061645508
Validation loss: 2.388720596990278

Epoch: 5| Step: 10
Training loss: 2.5532209873199463
Validation loss: 2.390261980795091

Epoch: 86| Step: 0
Training loss: 2.2286553382873535
Validation loss: 2.4030627973618044

Epoch: 5| Step: 1
Training loss: 2.776214122772217
Validation loss: 2.3964967420024257

Epoch: 5| Step: 2
Training loss: 2.4147982597351074
Validation loss: 2.3982629212000037

Epoch: 5| Step: 3
Training loss: 2.9926815032958984
Validation loss: 2.4019785952824417

Epoch: 5| Step: 4
Training loss: 2.6664235591888428
Validation loss: 2.3993911025344685

Epoch: 5| Step: 5
Training loss: 2.866173267364502
Validation loss: 2.4019799411937757

Epoch: 5| Step: 6
Training loss: 2.7483506202697754
Validation loss: 2.399368273314609

Epoch: 5| Step: 7
Training loss: 2.2413976192474365
Validation loss: 2.4069437980651855

Epoch: 5| Step: 8
Training loss: 2.7077999114990234
Validation loss: 2.412578228981264

Epoch: 5| Step: 9
Training loss: 2.5825445652008057
Validation loss: 2.412611720382526

Epoch: 5| Step: 10
Training loss: 2.6037487983703613
Validation loss: 2.4107592695502826

Epoch: 87| Step: 0
Training loss: 2.6555418968200684
Validation loss: 2.409092785209738

Epoch: 5| Step: 1
Training loss: 3.035978078842163
Validation loss: 2.4077104522335913

Epoch: 5| Step: 2
Training loss: 2.2973685264587402
Validation loss: 2.404163904087518

Epoch: 5| Step: 3
Training loss: 2.92889142036438
Validation loss: 2.400485784776749

Epoch: 5| Step: 4
Training loss: 2.4057908058166504
Validation loss: 2.4062367190596876

Epoch: 5| Step: 5
Training loss: 2.9115774631500244
Validation loss: 2.4088670797245477

Epoch: 5| Step: 6
Training loss: 2.216188907623291
Validation loss: 2.410083945079516

Epoch: 5| Step: 7
Training loss: 2.8570525646209717
Validation loss: 2.408453231216759

Epoch: 5| Step: 8
Training loss: 2.43761944770813
Validation loss: 2.412445891288019

Epoch: 5| Step: 9
Training loss: 2.726393461227417
Validation loss: 2.416055526784671

Epoch: 5| Step: 10
Training loss: 2.2485885620117188
Validation loss: 2.4266208423081266

Epoch: 88| Step: 0
Training loss: 2.2371115684509277
Validation loss: 2.430324649298063

Epoch: 5| Step: 1
Training loss: 2.786355495452881
Validation loss: 2.436865334869713

Epoch: 5| Step: 2
Training loss: 2.3841097354888916
Validation loss: 2.423860634526899

Epoch: 5| Step: 3
Training loss: 2.6369519233703613
Validation loss: 2.4071869337430565

Epoch: 5| Step: 4
Training loss: 2.9591996669769287
Validation loss: 2.392904363652711

Epoch: 5| Step: 5
Training loss: 2.545391321182251
Validation loss: 2.383109143985215

Epoch: 5| Step: 6
Training loss: 2.669128894805908
Validation loss: 2.379269528132613

Epoch: 5| Step: 7
Training loss: 2.6510818004608154
Validation loss: 2.380122641081451

Epoch: 5| Step: 8
Training loss: 2.552708148956299
Validation loss: 2.3745171844318347

Epoch: 5| Step: 9
Training loss: 2.622925043106079
Validation loss: 2.3821911581100954

Epoch: 5| Step: 10
Training loss: 2.8411240577697754
Validation loss: 2.3807209358420423

Epoch: 89| Step: 0
Training loss: 3.0946311950683594
Validation loss: 2.384396458184847

Epoch: 5| Step: 1
Training loss: 2.359078884124756
Validation loss: 2.3828267692237772

Epoch: 5| Step: 2
Training loss: 2.0830078125
Validation loss: 2.3933649524565666

Epoch: 5| Step: 3
Training loss: 2.5752625465393066
Validation loss: 2.397815583854593

Epoch: 5| Step: 4
Training loss: 2.2586519718170166
Validation loss: 2.4025339977715605

Epoch: 5| Step: 5
Training loss: 1.9980138540267944
Validation loss: 2.4146830651067916

Epoch: 5| Step: 6
Training loss: 2.6801693439483643
Validation loss: 2.407098549668507

Epoch: 5| Step: 7
Training loss: 2.6740403175354004
Validation loss: 2.3980233874372257

Epoch: 5| Step: 8
Training loss: 2.743945360183716
Validation loss: 2.4089766561344104

Epoch: 5| Step: 9
Training loss: 3.261733293533325
Validation loss: 2.413952442907518

Epoch: 5| Step: 10
Training loss: 3.134261131286621
Validation loss: 2.4057764417381695

Epoch: 90| Step: 0
Training loss: 2.5016074180603027
Validation loss: 2.4029365354968655

Epoch: 5| Step: 1
Training loss: 2.526585578918457
Validation loss: 2.3962684498038342

Epoch: 5| Step: 2
Training loss: 2.4536116123199463
Validation loss: 2.385323214274581

Epoch: 5| Step: 3
Training loss: 2.728461742401123
Validation loss: 2.378887414932251

Epoch: 5| Step: 4
Training loss: 2.4597256183624268
Validation loss: 2.3705743333344818

Epoch: 5| Step: 5
Training loss: 2.9030261039733887
Validation loss: 2.3665741284688315

Epoch: 5| Step: 6
Training loss: 2.7091829776763916
Validation loss: 2.370795529375794

Epoch: 5| Step: 7
Training loss: 2.6764979362487793
Validation loss: 2.3653347415308796

Epoch: 5| Step: 8
Training loss: 2.5896897315979004
Validation loss: 2.3693683275612454

Epoch: 5| Step: 9
Training loss: 2.4644196033477783
Validation loss: 2.365735287307411

Epoch: 5| Step: 10
Training loss: 2.9061360359191895
Validation loss: 2.3688413609740553

Epoch: 91| Step: 0
Training loss: 2.562875747680664
Validation loss: 2.3710634810950166

Epoch: 5| Step: 1
Training loss: 2.602621555328369
Validation loss: 2.379390283297467

Epoch: 5| Step: 2
Training loss: 2.4117884635925293
Validation loss: 2.3970007998968965

Epoch: 5| Step: 3
Training loss: 2.133166551589966
Validation loss: 2.395592943314583

Epoch: 5| Step: 4
Training loss: 3.3149399757385254
Validation loss: 2.410713641874252

Epoch: 5| Step: 5
Training loss: 2.2065858840942383
Validation loss: 2.403074691372533

Epoch: 5| Step: 6
Training loss: 2.68864107131958
Validation loss: 2.4057719143488074

Epoch: 5| Step: 7
Training loss: 2.683652877807617
Validation loss: 2.4075315460082023

Epoch: 5| Step: 8
Training loss: 2.553887128829956
Validation loss: 2.407579711688462

Epoch: 5| Step: 9
Training loss: 2.8221240043640137
Validation loss: 2.409034957167923

Epoch: 5| Step: 10
Training loss: 2.8240132331848145
Validation loss: 2.409520267158426

Epoch: 92| Step: 0
Training loss: 2.2248916625976562
Validation loss: 2.4112146695454917

Epoch: 5| Step: 1
Training loss: 2.5055484771728516
Validation loss: 2.40760309978198

Epoch: 5| Step: 2
Training loss: 2.3226284980773926
Validation loss: 2.402072319420435

Epoch: 5| Step: 3
Training loss: 2.2385799884796143
Validation loss: 2.385040811313096

Epoch: 5| Step: 4
Training loss: 2.496211290359497
Validation loss: 2.374055511207991

Epoch: 5| Step: 5
Training loss: 2.9069228172302246
Validation loss: 2.371911892326929

Epoch: 5| Step: 6
Training loss: 2.0887861251831055
Validation loss: 2.3650149658162105

Epoch: 5| Step: 7
Training loss: 3.1415340900421143
Validation loss: 2.3651960075542493

Epoch: 5| Step: 8
Training loss: 3.6978302001953125
Validation loss: 2.364276393767326

Epoch: 5| Step: 9
Training loss: 2.1003928184509277
Validation loss: 2.361052998932459

Epoch: 5| Step: 10
Training loss: 3.103835344314575
Validation loss: 2.363305789168163

Epoch: 93| Step: 0
Training loss: 2.254314422607422
Validation loss: 2.3594864799130346

Epoch: 5| Step: 1
Training loss: 2.041581392288208
Validation loss: 2.3630283314694642

Epoch: 5| Step: 2
Training loss: 2.7390427589416504
Validation loss: 2.3659655894002607

Epoch: 5| Step: 3
Training loss: 2.355775833129883
Validation loss: 2.3682289097898748

Epoch: 5| Step: 4
Training loss: 2.956164836883545
Validation loss: 2.375723256859728

Epoch: 5| Step: 5
Training loss: 2.4631805419921875
Validation loss: 2.387308636019307

Epoch: 5| Step: 6
Training loss: 2.7969913482666016
Validation loss: 2.403030956945112

Epoch: 5| Step: 7
Training loss: 2.9723591804504395
Validation loss: 2.4139200025989163

Epoch: 5| Step: 8
Training loss: 2.448615789413452
Validation loss: 2.4278691302063646

Epoch: 5| Step: 9
Training loss: 2.564779758453369
Validation loss: 2.42082074124326

Epoch: 5| Step: 10
Training loss: 3.262722969055176
Validation loss: 2.407223747622582

Epoch: 94| Step: 0
Training loss: 1.733838438987732
Validation loss: 2.4067965656198482

Epoch: 5| Step: 1
Training loss: 2.4124698638916016
Validation loss: 2.4015602116943686

Epoch: 5| Step: 2
Training loss: 2.647585868835449
Validation loss: 2.3879332337328183

Epoch: 5| Step: 3
Training loss: 3.171111583709717
Validation loss: 2.3794997071707122

Epoch: 5| Step: 4
Training loss: 2.6985442638397217
Validation loss: 2.383878623285601

Epoch: 5| Step: 5
Training loss: 2.975947856903076
Validation loss: 2.3792327719350017

Epoch: 5| Step: 6
Training loss: 2.4524288177490234
Validation loss: 2.3663983524486585

Epoch: 5| Step: 7
Training loss: 2.384687900543213
Validation loss: 2.366342718883227

Epoch: 5| Step: 8
Training loss: 3.1880481243133545
Validation loss: 2.3606163583776003

Epoch: 5| Step: 9
Training loss: 2.5628762245178223
Validation loss: 2.3569595390750515

Epoch: 5| Step: 10
Training loss: 2.3501811027526855
Validation loss: 2.3613383795625422

Epoch: 95| Step: 0
Training loss: 2.438436985015869
Validation loss: 2.3645385490950717

Epoch: 5| Step: 1
Training loss: 2.5279736518859863
Validation loss: 2.358353760934645

Epoch: 5| Step: 2
Training loss: 2.9418044090270996
Validation loss: 2.360167339283933

Epoch: 5| Step: 3
Training loss: 2.4804587364196777
Validation loss: 2.3542553250507643

Epoch: 5| Step: 4
Training loss: 2.660027027130127
Validation loss: 2.3653335058560936

Epoch: 5| Step: 5
Training loss: 2.298506259918213
Validation loss: 2.361466056557112

Epoch: 5| Step: 6
Training loss: 2.92897891998291
Validation loss: 2.3587315185095674

Epoch: 5| Step: 7
Training loss: 2.5566298961639404
Validation loss: 2.3704613049825034

Epoch: 5| Step: 8
Training loss: 2.5870633125305176
Validation loss: 2.3895314073049896

Epoch: 5| Step: 9
Training loss: 2.659088611602783
Validation loss: 2.410240827068206

Epoch: 5| Step: 10
Training loss: 2.530829906463623
Validation loss: 2.425495993706488

Epoch: 96| Step: 0
Training loss: 3.4424519538879395
Validation loss: 2.4463946947487454

Epoch: 5| Step: 1
Training loss: 1.869879961013794
Validation loss: 2.4380507981905373

Epoch: 5| Step: 2
Training loss: 2.587172746658325
Validation loss: 2.414929016943901

Epoch: 5| Step: 3
Training loss: 2.188931941986084
Validation loss: 2.404248837501772

Epoch: 5| Step: 4
Training loss: 3.12170672416687
Validation loss: 2.3968802062414025

Epoch: 5| Step: 5
Training loss: 2.332942485809326
Validation loss: 2.379244044262876

Epoch: 5| Step: 6
Training loss: 3.153087854385376
Validation loss: 2.3763024499339442

Epoch: 5| Step: 7
Training loss: 2.08663272857666
Validation loss: 2.3666224595039123

Epoch: 5| Step: 8
Training loss: 2.547089099884033
Validation loss: 2.365708504953692

Epoch: 5| Step: 9
Training loss: 3.15525221824646
Validation loss: 2.3629416265795307

Epoch: 5| Step: 10
Training loss: 2.185307502746582
Validation loss: 2.3624923972673315

Epoch: 97| Step: 0
Training loss: 2.389885187149048
Validation loss: 2.3613538203700895

Epoch: 5| Step: 1
Training loss: 2.231205463409424
Validation loss: 2.3684154582279984

Epoch: 5| Step: 2
Training loss: 2.827066421508789
Validation loss: 2.364823361878754

Epoch: 5| Step: 3
Training loss: 2.3137776851654053
Validation loss: 2.3718513622078845

Epoch: 5| Step: 4
Training loss: 2.863119602203369
Validation loss: 2.374649009396953

Epoch: 5| Step: 5
Training loss: 2.4191079139709473
Validation loss: 2.376934596287307

Epoch: 5| Step: 6
Training loss: 3.00215220451355
Validation loss: 2.373949678995276

Epoch: 5| Step: 7
Training loss: 2.6327407360076904
Validation loss: 2.3683001610540573

Epoch: 5| Step: 8
Training loss: 3.2400848865509033
Validation loss: 2.37151716473282

Epoch: 5| Step: 9
Training loss: 2.1848747730255127
Validation loss: 2.367992795923705

Epoch: 5| Step: 10
Training loss: 2.571591377258301
Validation loss: 2.38002154391299

Epoch: 98| Step: 0
Training loss: 2.749903440475464
Validation loss: 2.3673162973055275

Epoch: 5| Step: 1
Training loss: 2.360020875930786
Validation loss: 2.376590521104874

Epoch: 5| Step: 2
Training loss: 2.3648996353149414
Validation loss: 2.3756538616713656

Epoch: 5| Step: 3
Training loss: 3.476905107498169
Validation loss: 2.3819110778070267

Epoch: 5| Step: 4
Training loss: 2.624615430831909
Validation loss: 2.3850927839996996

Epoch: 5| Step: 5
Training loss: 2.3287248611450195
Validation loss: 2.3836327393849692

Epoch: 5| Step: 6
Training loss: 3.026768922805786
Validation loss: 2.407779401348483

Epoch: 5| Step: 7
Training loss: 2.2172341346740723
Validation loss: 2.383509885880255

Epoch: 5| Step: 8
Training loss: 2.8372464179992676
Validation loss: 2.366382993677611

Epoch: 5| Step: 9
Training loss: 2.1483349800109863
Validation loss: 2.3506288374623945

Epoch: 5| Step: 10
Training loss: 2.4651103019714355
Validation loss: 2.3441603619565248

Epoch: 99| Step: 0
Training loss: 2.8007304668426514
Validation loss: 2.338199015586607

Epoch: 5| Step: 1
Training loss: 2.579047441482544
Validation loss: 2.340758139087308

Epoch: 5| Step: 2
Training loss: 3.138476610183716
Validation loss: 2.3406797352657525

Epoch: 5| Step: 3
Training loss: 2.0743887424468994
Validation loss: 2.3364490078341578

Epoch: 5| Step: 4
Training loss: 2.27837872505188
Validation loss: 2.338608641778269

Epoch: 5| Step: 5
Training loss: 2.7324845790863037
Validation loss: 2.336216572792299

Epoch: 5| Step: 6
Training loss: 2.893446445465088
Validation loss: 2.3416074706662084

Epoch: 5| Step: 7
Training loss: 2.7288055419921875
Validation loss: 2.3358034062129196

Epoch: 5| Step: 8
Training loss: 2.6407108306884766
Validation loss: 2.337690399539086

Epoch: 5| Step: 9
Training loss: 2.171205520629883
Validation loss: 2.340837258164601

Epoch: 5| Step: 10
Training loss: 2.7022323608398438
Validation loss: 2.345238898390083

Epoch: 100| Step: 0
Training loss: 2.7439124584198
Validation loss: 2.3451799718282555

Epoch: 5| Step: 1
Training loss: 2.486799716949463
Validation loss: 2.350113768731394

Epoch: 5| Step: 2
Training loss: 3.1710152626037598
Validation loss: 2.352834468246788

Epoch: 5| Step: 3
Training loss: 2.7827014923095703
Validation loss: 2.352764250129782

Epoch: 5| Step: 4
Training loss: 2.060777187347412
Validation loss: 2.3625917255237536

Epoch: 5| Step: 5
Training loss: 3.191816806793213
Validation loss: 2.374172741366971

Epoch: 5| Step: 6
Training loss: 2.210585117340088
Validation loss: 2.3833100513745378

Epoch: 5| Step: 7
Training loss: 2.2397420406341553
Validation loss: 2.388668562776299

Epoch: 5| Step: 8
Training loss: 2.547130584716797
Validation loss: 2.371649880563059

Epoch: 5| Step: 9
Training loss: 2.8437209129333496
Validation loss: 2.361818057234569

Epoch: 5| Step: 10
Training loss: 2.205467939376831
Validation loss: 2.364811151258407

Epoch: 101| Step: 0
Training loss: 3.0759940147399902
Validation loss: 2.355688907766855

Epoch: 5| Step: 1
Training loss: 2.3364717960357666
Validation loss: 2.3450977789458407

Epoch: 5| Step: 2
Training loss: 2.314434289932251
Validation loss: 2.3398584447881228

Epoch: 5| Step: 3
Training loss: 2.9313273429870605
Validation loss: 2.3481412728627524

Epoch: 5| Step: 4
Training loss: 2.858314037322998
Validation loss: 2.351409540381483

Epoch: 5| Step: 5
Training loss: 3.173964262008667
Validation loss: 2.353300743205573

Epoch: 5| Step: 6
Training loss: 2.5361504554748535
Validation loss: 2.3582272016873924

Epoch: 5| Step: 7
Training loss: 2.5663630962371826
Validation loss: 2.3586665840559107

Epoch: 5| Step: 8
Training loss: 2.130859851837158
Validation loss: 2.3578225028130317

Epoch: 5| Step: 9
Training loss: 2.6211509704589844
Validation loss: 2.3513640152510775

Epoch: 5| Step: 10
Training loss: 2.0139355659484863
Validation loss: 2.3581037931544806

Epoch: 102| Step: 0
Training loss: 1.9785245656967163
Validation loss: 2.3746671804817776

Epoch: 5| Step: 1
Training loss: 2.3442723751068115
Validation loss: 2.377513470188264

Epoch: 5| Step: 2
Training loss: 2.7718286514282227
Validation loss: 2.381977391499345

Epoch: 5| Step: 3
Training loss: 2.586313247680664
Validation loss: 2.3788632372374177

Epoch: 5| Step: 4
Training loss: 2.737657070159912
Validation loss: 2.3692974223885486

Epoch: 5| Step: 5
Training loss: 2.848743200302124
Validation loss: 2.363358343801191

Epoch: 5| Step: 6
Training loss: 2.8534185886383057
Validation loss: 2.3632829112391316

Epoch: 5| Step: 7
Training loss: 2.836362838745117
Validation loss: 2.3527027650546004

Epoch: 5| Step: 8
Training loss: 2.471407413482666
Validation loss: 2.3574001378910516

Epoch: 5| Step: 9
Training loss: 2.2658743858337402
Validation loss: 2.3571004406098397

Epoch: 5| Step: 10
Training loss: 2.8935251235961914
Validation loss: 2.35855874963986

Epoch: 103| Step: 0
Training loss: 2.6609694957733154
Validation loss: 2.379831270505023

Epoch: 5| Step: 1
Training loss: 2.9591691493988037
Validation loss: 2.394272312041252

Epoch: 5| Step: 2
Training loss: 2.5074431896209717
Validation loss: 2.409144880951092

Epoch: 5| Step: 3
Training loss: 2.8649988174438477
Validation loss: 2.4164471933918614

Epoch: 5| Step: 4
Training loss: 2.345365285873413
Validation loss: 2.4323671017923663

Epoch: 5| Step: 5
Training loss: 2.111132860183716
Validation loss: 2.442445937023368

Epoch: 5| Step: 6
Training loss: 2.257685899734497
Validation loss: 2.4567876477395334

Epoch: 5| Step: 7
Training loss: 3.3093276023864746
Validation loss: 2.4700875948834162

Epoch: 5| Step: 8
Training loss: 3.173889398574829
Validation loss: 2.4515745485982587

Epoch: 5| Step: 9
Training loss: 2.519441843032837
Validation loss: 2.4123069676019813

Epoch: 5| Step: 10
Training loss: 1.889038324356079
Validation loss: 2.3819314382409535

Epoch: 104| Step: 0
Training loss: 2.9244227409362793
Validation loss: 2.3578266379653767

Epoch: 5| Step: 1
Training loss: 1.628077745437622
Validation loss: 2.330924177682528

Epoch: 5| Step: 2
Training loss: 2.453840494155884
Validation loss: 2.332749366760254

Epoch: 5| Step: 3
Training loss: 2.6435976028442383
Validation loss: 2.3376350018285934

Epoch: 5| Step: 4
Training loss: 2.231504440307617
Validation loss: 2.3331403655390583

Epoch: 5| Step: 5
Training loss: 2.8725051879882812
Validation loss: 2.3359765570650817

Epoch: 5| Step: 6
Training loss: 2.724260091781616
Validation loss: 2.3453320585271364

Epoch: 5| Step: 7
Training loss: 2.198275566101074
Validation loss: 2.335991323635142

Epoch: 5| Step: 8
Training loss: 2.7607808113098145
Validation loss: 2.335943422009868

Epoch: 5| Step: 9
Training loss: 3.5144569873809814
Validation loss: 2.3407122371017293

Epoch: 5| Step: 10
Training loss: 2.6121103763580322
Validation loss: 2.3524405853722685

Epoch: 105| Step: 0
Training loss: 2.2405877113342285
Validation loss: 2.3510863037519556

Epoch: 5| Step: 1
Training loss: 2.439265012741089
Validation loss: 2.3469669613786923

Epoch: 5| Step: 2
Training loss: 2.6947622299194336
Validation loss: 2.354478200276693

Epoch: 5| Step: 3
Training loss: 3.632380723953247
Validation loss: 2.3581862680373655

Epoch: 5| Step: 4
Training loss: 2.492530107498169
Validation loss: 2.3634113675804547

Epoch: 5| Step: 5
Training loss: 2.677527904510498
Validation loss: 2.3770834963808776

Epoch: 5| Step: 6
Training loss: 2.2584166526794434
Validation loss: 2.382057738560502

Epoch: 5| Step: 7
Training loss: 2.570960521697998
Validation loss: 2.375339933620986

Epoch: 5| Step: 8
Training loss: 2.9082064628601074
Validation loss: 2.369214329668271

Epoch: 5| Step: 9
Training loss: 2.2376608848571777
Validation loss: 2.3629672745222687

Epoch: 5| Step: 10
Training loss: 2.2647109031677246
Validation loss: 2.364372855873518

Epoch: 106| Step: 0
Training loss: 3.410696029663086
Validation loss: 2.3488180086176884

Epoch: 5| Step: 1
Training loss: 2.2993853092193604
Validation loss: 2.354720784771827

Epoch: 5| Step: 2
Training loss: 2.231562376022339
Validation loss: 2.349177855317311

Epoch: 5| Step: 3
Training loss: 2.4705607891082764
Validation loss: 2.3584760671020835

Epoch: 5| Step: 4
Training loss: 2.1529858112335205
Validation loss: 2.3563875831583494

Epoch: 5| Step: 5
Training loss: 2.7441368103027344
Validation loss: 2.3633165218496837

Epoch: 5| Step: 6
Training loss: 2.9511661529541016
Validation loss: 2.3695871829986572

Epoch: 5| Step: 7
Training loss: 2.506387710571289
Validation loss: 2.3619979632798063

Epoch: 5| Step: 8
Training loss: 2.8103084564208984
Validation loss: 2.372381748691682

Epoch: 5| Step: 9
Training loss: 2.253230333328247
Validation loss: 2.361715914100729

Epoch: 5| Step: 10
Training loss: 2.560580015182495
Validation loss: 2.373311170967676

Epoch: 107| Step: 0
Training loss: 3.311722993850708
Validation loss: 2.365244612898878

Epoch: 5| Step: 1
Training loss: 2.1763272285461426
Validation loss: 2.3632291286222395

Epoch: 5| Step: 2
Training loss: 2.457181692123413
Validation loss: 2.353345735098726

Epoch: 5| Step: 3
Training loss: 2.406014919281006
Validation loss: 2.376902746897872

Epoch: 5| Step: 4
Training loss: 2.4618184566497803
Validation loss: 2.3795085953127955

Epoch: 5| Step: 5
Training loss: 1.9633677005767822
Validation loss: 2.388086918861635

Epoch: 5| Step: 6
Training loss: 2.088855504989624
Validation loss: 2.394680610267065

Epoch: 5| Step: 7
Training loss: 2.5054879188537598
Validation loss: 2.3777842342212634

Epoch: 5| Step: 8
Training loss: 3.660499095916748
Validation loss: 2.37836056499071

Epoch: 5| Step: 9
Training loss: 2.6240062713623047
Validation loss: 2.361893928179177

Epoch: 5| Step: 10
Training loss: 2.7564504146575928
Validation loss: 2.343823186812862

Epoch: 108| Step: 0
Training loss: 2.5712320804595947
Validation loss: 2.333246772007276

Epoch: 5| Step: 1
Training loss: 2.8330233097076416
Validation loss: 2.323186259115896

Epoch: 5| Step: 2
Training loss: 2.6874923706054688
Validation loss: 2.322309224836288

Epoch: 5| Step: 3
Training loss: 2.0225768089294434
Validation loss: 2.323084233909525

Epoch: 5| Step: 4
Training loss: 2.6193833351135254
Validation loss: 2.3199683414992465

Epoch: 5| Step: 5
Training loss: 2.7805237770080566
Validation loss: 2.318230764840239

Epoch: 5| Step: 6
Training loss: 2.826962471008301
Validation loss: 2.316159248352051

Epoch: 5| Step: 7
Training loss: 2.437072992324829
Validation loss: 2.3132890860239663

Epoch: 5| Step: 8
Training loss: 2.3957505226135254
Validation loss: 2.31139515548624

Epoch: 5| Step: 9
Training loss: 2.5765299797058105
Validation loss: 2.313375803732103

Epoch: 5| Step: 10
Training loss: 2.846421957015991
Validation loss: 2.3178040699292253

Epoch: 109| Step: 0
Training loss: 2.290097236633301
Validation loss: 2.3174564774318407

Epoch: 5| Step: 1
Training loss: 2.4756178855895996
Validation loss: 2.3181132078170776

Epoch: 5| Step: 2
Training loss: 2.6642441749572754
Validation loss: 2.3203486268238356

Epoch: 5| Step: 3
Training loss: 2.8453736305236816
Validation loss: 2.323109975425146

Epoch: 5| Step: 4
Training loss: 2.4792466163635254
Validation loss: 2.322343757075648

Epoch: 5| Step: 5
Training loss: 2.291419267654419
Validation loss: 2.321345877903764

Epoch: 5| Step: 6
Training loss: 2.779006242752075
Validation loss: 2.328865820361722

Epoch: 5| Step: 7
Training loss: 3.176135540008545
Validation loss: 2.333094243080385

Epoch: 5| Step: 8
Training loss: 2.722724199295044
Validation loss: 2.3284756137478735

Epoch: 5| Step: 9
Training loss: 2.017921209335327
Validation loss: 2.331267882418889

Epoch: 5| Step: 10
Training loss: 2.6808979511260986
Validation loss: 2.341588899653445

Epoch: 110| Step: 0
Training loss: 3.2618355751037598
Validation loss: 2.352016338738062

Epoch: 5| Step: 1
Training loss: 2.0447771549224854
Validation loss: 2.3493971799009588

Epoch: 5| Step: 2
Training loss: 2.641390323638916
Validation loss: 2.3491774553893716

Epoch: 5| Step: 3
Training loss: 2.204132556915283
Validation loss: 2.3470669254179923

Epoch: 5| Step: 4
Training loss: 2.4344375133514404
Validation loss: 2.355800936298986

Epoch: 5| Step: 5
Training loss: 3.032170057296753
Validation loss: 2.3556343214486235

Epoch: 5| Step: 6
Training loss: 2.895162343978882
Validation loss: 2.357422290309783

Epoch: 5| Step: 7
Training loss: 2.419287919998169
Validation loss: 2.3659770450284405

Epoch: 5| Step: 8
Training loss: 2.5659031867980957
Validation loss: 2.357474693688013

Epoch: 5| Step: 9
Training loss: 2.8252789974212646
Validation loss: 2.3592350277849423

Epoch: 5| Step: 10
Training loss: 1.8530057668685913
Validation loss: 2.351008807459185

Epoch: 111| Step: 0
Training loss: 2.5246071815490723
Validation loss: 2.346924751035629

Epoch: 5| Step: 1
Training loss: 2.5410468578338623
Validation loss: 2.3535158531640166

Epoch: 5| Step: 2
Training loss: 2.465256690979004
Validation loss: 2.3451742920824277

Epoch: 5| Step: 3
Training loss: 2.3387794494628906
Validation loss: 2.3602431820284937

Epoch: 5| Step: 4
Training loss: 2.8303582668304443
Validation loss: 2.356824703114007

Epoch: 5| Step: 5
Training loss: 2.6226272583007812
Validation loss: 2.3568056552640853

Epoch: 5| Step: 6
Training loss: 3.067023754119873
Validation loss: 2.3595841597485285

Epoch: 5| Step: 7
Training loss: 2.3637936115264893
Validation loss: 2.34901927363488

Epoch: 5| Step: 8
Training loss: 2.1864476203918457
Validation loss: 2.3424248721009944

Epoch: 5| Step: 9
Training loss: 2.750549793243408
Validation loss: 2.3350395182127595

Epoch: 5| Step: 10
Training loss: 2.5084283351898193
Validation loss: 2.3258013379189277

Epoch: 112| Step: 0
Training loss: 2.7995941638946533
Validation loss: 2.3132025810980026

Epoch: 5| Step: 1
Training loss: 2.792799472808838
Validation loss: 2.3148916511125464

Epoch: 5| Step: 2
Training loss: 2.3142411708831787
Validation loss: 2.316822518584549

Epoch: 5| Step: 3
Training loss: 1.7665331363677979
Validation loss: 2.3209673076547603

Epoch: 5| Step: 4
Training loss: 2.4339680671691895
Validation loss: 2.3288660254529727

Epoch: 5| Step: 5
Training loss: 2.304080009460449
Validation loss: 2.3311389569313294

Epoch: 5| Step: 6
Training loss: 3.2243199348449707
Validation loss: 2.326702804975612

Epoch: 5| Step: 7
Training loss: 2.8773913383483887
Validation loss: 2.3207274201095744

Epoch: 5| Step: 8
Training loss: 2.8535714149475098
Validation loss: 2.319246594623853

Epoch: 5| Step: 9
Training loss: 2.261946201324463
Validation loss: 2.3091861278780046

Epoch: 5| Step: 10
Training loss: 2.751230001449585
Validation loss: 2.3135648953017367

Epoch: 113| Step: 0
Training loss: 2.2564730644226074
Validation loss: 2.3116845059138473

Epoch: 5| Step: 1
Training loss: 2.7613332271575928
Validation loss: 2.317321074906216

Epoch: 5| Step: 2
Training loss: 2.664517641067505
Validation loss: 2.321826416959045

Epoch: 5| Step: 3
Training loss: 3.226433515548706
Validation loss: 2.3281903651452835

Epoch: 5| Step: 4
Training loss: 2.495913028717041
Validation loss: 2.33986823789535

Epoch: 5| Step: 5
Training loss: 2.9128243923187256
Validation loss: 2.3562167972646733

Epoch: 5| Step: 6
Training loss: 2.197875738143921
Validation loss: 2.3634046123873804

Epoch: 5| Step: 7
Training loss: 2.288151264190674
Validation loss: 2.3452716181355138

Epoch: 5| Step: 8
Training loss: 2.7534842491149902
Validation loss: 2.3285295886378132

Epoch: 5| Step: 9
Training loss: 2.8187053203582764
Validation loss: 2.314769060380997

Epoch: 5| Step: 10
Training loss: 1.906556248664856
Validation loss: 2.2990830867521224

Epoch: 114| Step: 0
Training loss: 2.482682466506958
Validation loss: 2.303734399939096

Epoch: 5| Step: 1
Training loss: 2.312385320663452
Validation loss: 2.3036962324573147

Epoch: 5| Step: 2
Training loss: 2.7468857765197754
Validation loss: 2.300978442674042

Epoch: 5| Step: 3
Training loss: 2.7645230293273926
Validation loss: 2.3032750878282773

Epoch: 5| Step: 4
Training loss: 2.638551950454712
Validation loss: 2.306711371226977

Epoch: 5| Step: 5
Training loss: 2.431486129760742
Validation loss: 2.311435358498686

Epoch: 5| Step: 6
Training loss: 2.3220245838165283
Validation loss: 2.3078832882706837

Epoch: 5| Step: 7
Training loss: 2.1690526008605957
Validation loss: 2.3150990701490834

Epoch: 5| Step: 8
Training loss: 2.918165922164917
Validation loss: 2.316762613993819

Epoch: 5| Step: 9
Training loss: 2.7313570976257324
Validation loss: 2.3297304645661385

Epoch: 5| Step: 10
Training loss: 2.697284460067749
Validation loss: 2.3201929984554166

Epoch: 115| Step: 0
Training loss: 2.53570294380188
Validation loss: 2.3409041666215464

Epoch: 5| Step: 1
Training loss: 2.535975694656372
Validation loss: 2.3574913227429954

Epoch: 5| Step: 2
Training loss: 3.059192657470703
Validation loss: 2.3695577908587713

Epoch: 5| Step: 3
Training loss: 3.0372061729431152
Validation loss: 2.3663578802539456

Epoch: 5| Step: 4
Training loss: 2.407230854034424
Validation loss: 2.368484458615703

Epoch: 5| Step: 5
Training loss: 2.705113649368286
Validation loss: 2.360788540173602

Epoch: 5| Step: 6
Training loss: 2.149573802947998
Validation loss: 2.3409663067069104

Epoch: 5| Step: 7
Training loss: 2.6226329803466797
Validation loss: 2.3359816382008214

Epoch: 5| Step: 8
Training loss: 2.4250130653381348
Validation loss: 2.323701491919897

Epoch: 5| Step: 9
Training loss: 2.478627920150757
Validation loss: 2.330013326419297

Epoch: 5| Step: 10
Training loss: 2.3731391429901123
Validation loss: 2.3255638858323455

Epoch: 116| Step: 0
Training loss: 2.729868173599243
Validation loss: 2.3296347741157777

Epoch: 5| Step: 1
Training loss: 2.5130856037139893
Validation loss: 2.3172759420128277

Epoch: 5| Step: 2
Training loss: 2.485819101333618
Validation loss: 2.3158978621164956

Epoch: 5| Step: 3
Training loss: 2.606163501739502
Validation loss: 2.3109384095796974

Epoch: 5| Step: 4
Training loss: 2.143738269805908
Validation loss: 2.3078963474560807

Epoch: 5| Step: 5
Training loss: 2.631699800491333
Validation loss: 2.3040119242924515

Epoch: 5| Step: 6
Training loss: 2.8002963066101074
Validation loss: 2.2979968491420952

Epoch: 5| Step: 7
Training loss: 2.937450647354126
Validation loss: 2.3049793474135862

Epoch: 5| Step: 8
Training loss: 2.470710039138794
Validation loss: 2.3013894609225694

Epoch: 5| Step: 9
Training loss: 2.4913368225097656
Validation loss: 2.3050905427625104

Epoch: 5| Step: 10
Training loss: 2.3993687629699707
Validation loss: 2.3125615248116116

Epoch: 117| Step: 0
Training loss: 2.781404495239258
Validation loss: 2.31733521082068

Epoch: 5| Step: 1
Training loss: 2.805471897125244
Validation loss: 2.3182485641971713

Epoch: 5| Step: 2
Training loss: 3.157325267791748
Validation loss: 2.325083681332168

Epoch: 5| Step: 3
Training loss: 2.38901948928833
Validation loss: 2.3447212557638846

Epoch: 5| Step: 4
Training loss: 2.3863654136657715
Validation loss: 2.3548954276628393

Epoch: 5| Step: 5
Training loss: 2.4909603595733643
Validation loss: 2.3718464323269424

Epoch: 5| Step: 6
Training loss: 2.0676722526550293
Validation loss: 2.3807302623666744

Epoch: 5| Step: 7
Training loss: 3.1217403411865234
Validation loss: 2.395355946274214

Epoch: 5| Step: 8
Training loss: 2.1582651138305664
Validation loss: 2.3705016156678558

Epoch: 5| Step: 9
Training loss: 2.717991590499878
Validation loss: 2.349501963584654

Epoch: 5| Step: 10
Training loss: 2.142949342727661
Validation loss: 2.3114559112056607

Epoch: 118| Step: 0
Training loss: 2.937936305999756
Validation loss: 2.295876810627599

Epoch: 5| Step: 1
Training loss: 2.9715588092803955
Validation loss: 2.288964525345833

Epoch: 5| Step: 2
Training loss: 2.5529046058654785
Validation loss: 2.300684536657026

Epoch: 5| Step: 3
Training loss: 2.9234747886657715
Validation loss: 2.3121875280975015

Epoch: 5| Step: 4
Training loss: 2.13944935798645
Validation loss: 2.3171869119008384

Epoch: 5| Step: 5
Training loss: 1.9757530689239502
Validation loss: 2.3156696878453737

Epoch: 5| Step: 6
Training loss: 2.6033072471618652
Validation loss: 2.3188147878134124

Epoch: 5| Step: 7
Training loss: 2.6375932693481445
Validation loss: 2.3180293754864763

Epoch: 5| Step: 8
Training loss: 2.2586095333099365
Validation loss: 2.3152935992005053

Epoch: 5| Step: 9
Training loss: 2.6629130840301514
Validation loss: 2.31140624835927

Epoch: 5| Step: 10
Training loss: 3.0317907333374023
Validation loss: 2.2993398738163773

Epoch: 119| Step: 0
Training loss: 2.2491886615753174
Validation loss: 2.296226332264562

Epoch: 5| Step: 1
Training loss: 3.1700072288513184
Validation loss: 2.3037163288362565

Epoch: 5| Step: 2
Training loss: 2.479928731918335
Validation loss: 2.3161946471019457

Epoch: 5| Step: 3
Training loss: 2.4563794136047363
Validation loss: 2.3275327400494645

Epoch: 5| Step: 4
Training loss: 2.3286900520324707
Validation loss: 2.3520461974605436

Epoch: 5| Step: 5
Training loss: 1.8035799264907837
Validation loss: 2.3699166749113347

Epoch: 5| Step: 6
Training loss: 2.780996799468994
Validation loss: 2.3836923004478536

Epoch: 5| Step: 7
Training loss: 2.7899117469787598
Validation loss: 2.386861485819663

Epoch: 5| Step: 8
Training loss: 2.7151479721069336
Validation loss: 2.357083425726942

Epoch: 5| Step: 9
Training loss: 2.960885524749756
Validation loss: 2.3481268805842244

Epoch: 5| Step: 10
Training loss: 2.7225468158721924
Validation loss: 2.3401755132982807

Epoch: 120| Step: 0
Training loss: 2.2353312969207764
Validation loss: 2.3188989111172256

Epoch: 5| Step: 1
Training loss: 2.0590999126434326
Validation loss: 2.3191441130894486

Epoch: 5| Step: 2
Training loss: 2.807853937149048
Validation loss: 2.307947410050259

Epoch: 5| Step: 3
Training loss: 3.0121312141418457
Validation loss: 2.309848887946016

Epoch: 5| Step: 4
Training loss: 2.7948851585388184
Validation loss: 2.303509968583302

Epoch: 5| Step: 5
Training loss: 2.3308956623077393
Validation loss: 2.303469334879229

Epoch: 5| Step: 6
Training loss: 3.0219454765319824
Validation loss: 2.30063622484925

Epoch: 5| Step: 7
Training loss: 2.452094316482544
Validation loss: 2.2959036955269436

Epoch: 5| Step: 8
Training loss: 2.134700059890747
Validation loss: 2.2971538420646422

Epoch: 5| Step: 9
Training loss: 2.9231512546539307
Validation loss: 2.2910021082047494

Epoch: 5| Step: 10
Training loss: 2.3931779861450195
Validation loss: 2.3030756724778043

Epoch: 121| Step: 0
Training loss: 1.731313705444336
Validation loss: 2.310099163363057

Epoch: 5| Step: 1
Training loss: 3.421700954437256
Validation loss: 2.3181300137632634

Epoch: 5| Step: 2
Training loss: 2.5580050945281982
Validation loss: 2.3304460689585698

Epoch: 5| Step: 3
Training loss: 2.321423053741455
Validation loss: 2.332489787891347

Epoch: 5| Step: 4
Training loss: 2.7975573539733887
Validation loss: 2.335500874826985

Epoch: 5| Step: 5
Training loss: 2.576230764389038
Validation loss: 2.3450300488420712

Epoch: 5| Step: 6
Training loss: 1.9601142406463623
Validation loss: 2.337523978243592

Epoch: 5| Step: 7
Training loss: 3.0295159816741943
Validation loss: 2.3321571234733827

Epoch: 5| Step: 8
Training loss: 2.2086853981018066
Validation loss: 2.32203398981402

Epoch: 5| Step: 9
Training loss: 2.5579161643981934
Validation loss: 2.326503466534358

Epoch: 5| Step: 10
Training loss: 3.0025997161865234
Validation loss: 2.3080579888436104

Epoch: 122| Step: 0
Training loss: 2.492527484893799
Validation loss: 2.3157189635820288

Epoch: 5| Step: 1
Training loss: 2.887732982635498
Validation loss: 2.3211380973938973

Epoch: 5| Step: 2
Training loss: 2.0221917629241943
Validation loss: 2.327979862049062

Epoch: 5| Step: 3
Training loss: 2.136996030807495
Validation loss: 2.330445376775598

Epoch: 5| Step: 4
Training loss: 2.0001730918884277
Validation loss: 2.32946507136027

Epoch: 5| Step: 5
Training loss: 2.9803919792175293
Validation loss: 2.3387444019317627

Epoch: 5| Step: 6
Training loss: 2.9403867721557617
Validation loss: 2.3299461539073656

Epoch: 5| Step: 7
Training loss: 2.900175094604492
Validation loss: 2.3327655689690703

Epoch: 5| Step: 8
Training loss: 2.143429756164551
Validation loss: 2.3123135771802676

Epoch: 5| Step: 9
Training loss: 2.7993364334106445
Validation loss: 2.3041208431284916

Epoch: 5| Step: 10
Training loss: 2.712759494781494
Validation loss: 2.3055666492831324

Epoch: 123| Step: 0
Training loss: 2.1970341205596924
Validation loss: 2.306323648780905

Epoch: 5| Step: 1
Training loss: 2.842751979827881
Validation loss: 2.327256102715769

Epoch: 5| Step: 2
Training loss: 3.1317999362945557
Validation loss: 2.3470919516778763

Epoch: 5| Step: 3
Training loss: 2.159733533859253
Validation loss: 2.360661788653302

Epoch: 5| Step: 4
Training loss: 2.444471836090088
Validation loss: 2.3673823623247046

Epoch: 5| Step: 5
Training loss: 3.2513928413391113
Validation loss: 2.371958714659496

Epoch: 5| Step: 6
Training loss: 3.085214614868164
Validation loss: 2.378572694716915

Epoch: 5| Step: 7
Training loss: 2.6569509506225586
Validation loss: 2.3648621395070064

Epoch: 5| Step: 8
Training loss: 1.3295648097991943
Validation loss: 2.3600571335002942

Epoch: 5| Step: 9
Training loss: 2.1331489086151123
Validation loss: 2.341087192617437

Epoch: 5| Step: 10
Training loss: 2.922037363052368
Validation loss: 2.3256909103803736

Epoch: 124| Step: 0
Training loss: 2.6854147911071777
Validation loss: 2.3041000930211877

Epoch: 5| Step: 1
Training loss: 2.4162325859069824
Validation loss: 2.2828825981386247

Epoch: 5| Step: 2
Training loss: 2.3773956298828125
Validation loss: 2.287708964399112

Epoch: 5| Step: 3
Training loss: 2.0832958221435547
Validation loss: 2.2847136733352498

Epoch: 5| Step: 4
Training loss: 2.5109589099884033
Validation loss: 2.27891085737495

Epoch: 5| Step: 5
Training loss: 2.7811837196350098
Validation loss: 2.279403446823038

Epoch: 5| Step: 6
Training loss: 2.907301187515259
Validation loss: 2.2790403801907777

Epoch: 5| Step: 7
Training loss: 2.7816920280456543
Validation loss: 2.279982988552381

Epoch: 5| Step: 8
Training loss: 2.5924034118652344
Validation loss: 2.267608683596375

Epoch: 5| Step: 9
Training loss: 2.0995094776153564
Validation loss: 2.2769453294815554

Epoch: 5| Step: 10
Training loss: 2.8775949478149414
Validation loss: 2.28246981610534

Epoch: 125| Step: 0
Training loss: 2.433372974395752
Validation loss: 2.298065839275237

Epoch: 5| Step: 1
Training loss: 2.383836507797241
Validation loss: 2.321603585315007

Epoch: 5| Step: 2
Training loss: 3.226393461227417
Validation loss: 2.331914940188008

Epoch: 5| Step: 3
Training loss: 2.7347447872161865
Validation loss: 2.3421849230284333

Epoch: 5| Step: 4
Training loss: 2.787801504135132
Validation loss: 2.337741210896482

Epoch: 5| Step: 5
Training loss: 2.314669370651245
Validation loss: 2.333070831914102

Epoch: 5| Step: 6
Training loss: 2.856280565261841
Validation loss: 2.3403207050856722

Epoch: 5| Step: 7
Training loss: 2.1165130138397217
Validation loss: 2.3307908709331224

Epoch: 5| Step: 8
Training loss: 2.330690860748291
Validation loss: 2.3333403628359557

Epoch: 5| Step: 9
Training loss: 2.3991379737854004
Validation loss: 2.3327402171268257

Epoch: 5| Step: 10
Training loss: 2.4099280834198
Validation loss: 2.3240007687640447

Epoch: 126| Step: 0
Training loss: 2.455141067504883
Validation loss: 2.3197913631316154

Epoch: 5| Step: 1
Training loss: 2.644928455352783
Validation loss: 2.323945983763664

Epoch: 5| Step: 2
Training loss: 2.2963314056396484
Validation loss: 2.3024752255408996

Epoch: 5| Step: 3
Training loss: 2.842233657836914
Validation loss: 2.297194911587623

Epoch: 5| Step: 4
Training loss: 2.4451863765716553
Validation loss: 2.2907929266652753

Epoch: 5| Step: 5
Training loss: 2.984377384185791
Validation loss: 2.2873875864090456

Epoch: 5| Step: 6
Training loss: 3.111937999725342
Validation loss: 2.2805840379448346

Epoch: 5| Step: 7
Training loss: 1.8417994976043701
Validation loss: 2.2844001336764266

Epoch: 5| Step: 8
Training loss: 2.581575870513916
Validation loss: 2.2820481997664257

Epoch: 5| Step: 9
Training loss: 2.3808090686798096
Validation loss: 2.2862080630435737

Epoch: 5| Step: 10
Training loss: 2.3860347270965576
Validation loss: 2.2881025396367556

Epoch: 127| Step: 0
Training loss: 2.2537856101989746
Validation loss: 2.296452333850245

Epoch: 5| Step: 1
Training loss: 3.298375368118286
Validation loss: 2.2932647376932125

Epoch: 5| Step: 2
Training loss: 2.4447121620178223
Validation loss: 2.3001621051501204

Epoch: 5| Step: 3
Training loss: 2.729799747467041
Validation loss: 2.294570081977434

Epoch: 5| Step: 4
Training loss: 1.958036184310913
Validation loss: 2.302397656184371

Epoch: 5| Step: 5
Training loss: 2.6049976348876953
Validation loss: 2.2979731393116776

Epoch: 5| Step: 6
Training loss: 2.5700173377990723
Validation loss: 2.311703189726799

Epoch: 5| Step: 7
Training loss: 2.608477830886841
Validation loss: 2.316270466773741

Epoch: 5| Step: 8
Training loss: 2.205831527709961
Validation loss: 2.3252428564974057

Epoch: 5| Step: 9
Training loss: 2.214190721511841
Validation loss: 2.324290842138311

Epoch: 5| Step: 10
Training loss: 3.016566038131714
Validation loss: 2.337419597051477

Epoch: 128| Step: 0
Training loss: 2.3431811332702637
Validation loss: 2.3396560197235434

Epoch: 5| Step: 1
Training loss: 1.8983322381973267
Validation loss: 2.3338913199722127

Epoch: 5| Step: 2
Training loss: 2.975743293762207
Validation loss: 2.342890462567729

Epoch: 5| Step: 3
Training loss: 2.510573387145996
Validation loss: 2.339201823357613

Epoch: 5| Step: 4
Training loss: 1.8665294647216797
Validation loss: 2.3211324548208587

Epoch: 5| Step: 5
Training loss: 2.500636577606201
Validation loss: 2.316354992569134

Epoch: 5| Step: 6
Training loss: 2.898585796356201
Validation loss: 2.308897249160274

Epoch: 5| Step: 7
Training loss: 2.4799535274505615
Validation loss: 2.2985494393174366

Epoch: 5| Step: 8
Training loss: 3.451876401901245
Validation loss: 2.29021050853114

Epoch: 5| Step: 9
Training loss: 2.8884212970733643
Validation loss: 2.28683538334344

Epoch: 5| Step: 10
Training loss: 1.9660931825637817
Validation loss: 2.280574098710091

Epoch: 129| Step: 0
Training loss: 2.569080114364624
Validation loss: 2.273864755066492

Epoch: 5| Step: 1
Training loss: 3.074511766433716
Validation loss: 2.27745214457153

Epoch: 5| Step: 2
Training loss: 2.7063770294189453
Validation loss: 2.2779289163568968

Epoch: 5| Step: 3
Training loss: 1.6953299045562744
Validation loss: 2.2708069303984284

Epoch: 5| Step: 4
Training loss: 2.6206231117248535
Validation loss: 2.2691010941741285

Epoch: 5| Step: 5
Training loss: 2.1095919609069824
Validation loss: 2.2816088302161104

Epoch: 5| Step: 6
Training loss: 2.962409734725952
Validation loss: 2.2970744973869732

Epoch: 5| Step: 7
Training loss: 2.7703375816345215
Validation loss: 2.31429261033253

Epoch: 5| Step: 8
Training loss: 2.3541691303253174
Validation loss: 2.323306688698389

Epoch: 5| Step: 9
Training loss: 2.684213638305664
Validation loss: 2.327043520506992

Epoch: 5| Step: 10
Training loss: 2.2069060802459717
Validation loss: 2.3450306794976674

Epoch: 130| Step: 0
Training loss: 2.862206220626831
Validation loss: 2.3673818624147804

Epoch: 5| Step: 1
Training loss: 1.6194692850112915
Validation loss: 2.371843981486495

Epoch: 5| Step: 2
Training loss: 2.855820894241333
Validation loss: 2.3687864580462055

Epoch: 5| Step: 3
Training loss: 2.705503463745117
Validation loss: 2.3705403932961087

Epoch: 5| Step: 4
Training loss: 2.4997305870056152
Validation loss: 2.3597396676258375

Epoch: 5| Step: 5
Training loss: 2.721693515777588
Validation loss: 2.338903619397071

Epoch: 5| Step: 6
Training loss: 2.8038132190704346
Validation loss: 2.337758707743819

Epoch: 5| Step: 7
Training loss: 2.886796236038208
Validation loss: 2.3068857833903325

Epoch: 5| Step: 8
Training loss: 2.3866539001464844
Validation loss: 2.2923556245783323

Epoch: 5| Step: 9
Training loss: 2.037498950958252
Validation loss: 2.2850862241560415

Epoch: 5| Step: 10
Training loss: 2.669832468032837
Validation loss: 2.286857574216781

Epoch: 131| Step: 0
Training loss: 2.654341459274292
Validation loss: 2.2863007950526413

Epoch: 5| Step: 1
Training loss: 2.8691189289093018
Validation loss: 2.2840730297950005

Epoch: 5| Step: 2
Training loss: 2.514559268951416
Validation loss: 2.289242764954926

Epoch: 5| Step: 3
Training loss: 2.1232528686523438
Validation loss: 2.2893942017709055

Epoch: 5| Step: 4
Training loss: 3.342695951461792
Validation loss: 2.2931635866882982

Epoch: 5| Step: 5
Training loss: 2.721350908279419
Validation loss: 2.2805743948105843

Epoch: 5| Step: 6
Training loss: 2.5466463565826416
Validation loss: 2.2631642408268426

Epoch: 5| Step: 7
Training loss: 3.0516791343688965
Validation loss: 2.258733598134851

Epoch: 5| Step: 8
Training loss: 2.4730629920959473
Validation loss: 2.265305015348619

Epoch: 5| Step: 9
Training loss: 1.5865532159805298
Validation loss: 2.2688808979526645

Epoch: 5| Step: 10
Training loss: 2.0391321182250977
Validation loss: 2.278245488802592

Epoch: 132| Step: 0
Training loss: 2.9237005710601807
Validation loss: 2.300816253949237

Epoch: 5| Step: 1
Training loss: 2.562819242477417
Validation loss: 2.3243035398503786

Epoch: 5| Step: 2
Training loss: 2.673985004425049
Validation loss: 2.301896923331804

Epoch: 5| Step: 3
Training loss: 1.932580590248108
Validation loss: 2.3042421981852543

Epoch: 5| Step: 4
Training loss: 2.636021852493286
Validation loss: 2.325847145049803

Epoch: 5| Step: 5
Training loss: 2.0123355388641357
Validation loss: 2.3179498436630412

Epoch: 5| Step: 6
Training loss: 2.352177143096924
Validation loss: 2.308924413496448

Epoch: 5| Step: 7
Training loss: 2.823056697845459
Validation loss: 2.320358431467446

Epoch: 5| Step: 8
Training loss: 2.3521294593811035
Validation loss: 2.3052649562076857

Epoch: 5| Step: 9
Training loss: 2.8774375915527344
Validation loss: 2.298974703716975

Epoch: 5| Step: 10
Training loss: 2.771186590194702
Validation loss: 2.2879948308390956

Epoch: 133| Step: 0
Training loss: 2.1365609169006348
Validation loss: 2.2805478060117332

Epoch: 5| Step: 1
Training loss: 2.5867061614990234
Validation loss: 2.2837932161105576

Epoch: 5| Step: 2
Training loss: 1.8403260707855225
Validation loss: 2.2865089088357906

Epoch: 5| Step: 3
Training loss: 2.3991265296936035
Validation loss: 2.300621278824345

Epoch: 5| Step: 4
Training loss: 3.3438045978546143
Validation loss: 2.3041213827748455

Epoch: 5| Step: 5
Training loss: 3.2813072204589844
Validation loss: 2.312536511369931

Epoch: 5| Step: 6
Training loss: 2.5346527099609375
Validation loss: 2.323000810479605

Epoch: 5| Step: 7
Training loss: 2.8908042907714844
Validation loss: 2.305445706972512

Epoch: 5| Step: 8
Training loss: 2.228285312652588
Validation loss: 2.2913638776348484

Epoch: 5| Step: 9
Training loss: 1.816083312034607
Validation loss: 2.281629595705258

Epoch: 5| Step: 10
Training loss: 2.8107738494873047
Validation loss: 2.2773499258102907

Epoch: 134| Step: 0
Training loss: 2.9882194995880127
Validation loss: 2.2808797820921867

Epoch: 5| Step: 1
Training loss: 2.3136277198791504
Validation loss: 2.283261299133301

Epoch: 5| Step: 2
Training loss: 2.508274555206299
Validation loss: 2.2818537168605353

Epoch: 5| Step: 3
Training loss: 1.847699522972107
Validation loss: 2.2836717790172947

Epoch: 5| Step: 4
Training loss: 2.4473328590393066
Validation loss: 2.283837290220363

Epoch: 5| Step: 5
Training loss: 2.922015905380249
Validation loss: 2.2894813040251374

Epoch: 5| Step: 6
Training loss: 2.780679225921631
Validation loss: 2.3144192131616736

Epoch: 5| Step: 7
Training loss: 2.476022243499756
Validation loss: 2.3102850503818964

Epoch: 5| Step: 8
Training loss: 2.976046085357666
Validation loss: 2.3199279077591433

Epoch: 5| Step: 9
Training loss: 2.2026138305664062
Validation loss: 2.307719865152913

Epoch: 5| Step: 10
Training loss: 2.3146657943725586
Validation loss: 2.3206235567728677

Epoch: 135| Step: 0
Training loss: 2.294010877609253
Validation loss: 2.301759041765685

Epoch: 5| Step: 1
Training loss: 2.7258267402648926
Validation loss: 2.2923577190727316

Epoch: 5| Step: 2
Training loss: 2.5575788021087646
Validation loss: 2.2920656896406606

Epoch: 5| Step: 3
Training loss: 2.3018298149108887
Validation loss: 2.2772915260766142

Epoch: 5| Step: 4
Training loss: 2.7755300998687744
Validation loss: 2.2803936055911485

Epoch: 5| Step: 5
Training loss: 2.979407548904419
Validation loss: 2.273617547045472

Epoch: 5| Step: 6
Training loss: 2.213270902633667
Validation loss: 2.284775539111066

Epoch: 5| Step: 7
Training loss: 2.3126871585845947
Validation loss: 2.2734593396545737

Epoch: 5| Step: 8
Training loss: 2.7259881496429443
Validation loss: 2.2704227509037143

Epoch: 5| Step: 9
Training loss: 2.549492597579956
Validation loss: 2.2753705773302304

Epoch: 5| Step: 10
Training loss: 2.1854138374328613
Validation loss: 2.266030632039552

Epoch: 136| Step: 0
Training loss: 2.7802696228027344
Validation loss: 2.2677238038791123

Epoch: 5| Step: 1
Training loss: 2.541330337524414
Validation loss: 2.2705100505582747

Epoch: 5| Step: 2
Training loss: 2.565906047821045
Validation loss: 2.262795650830833

Epoch: 5| Step: 3
Training loss: 3.2175498008728027
Validation loss: 2.2762885247507403

Epoch: 5| Step: 4
Training loss: 2.3024210929870605
Validation loss: 2.286957149864525

Epoch: 5| Step: 5
Training loss: 2.439711332321167
Validation loss: 2.3014626964446037

Epoch: 5| Step: 6
Training loss: 2.581134796142578
Validation loss: 2.3095353726417787

Epoch: 5| Step: 7
Training loss: 2.365833282470703
Validation loss: 2.310900601007605

Epoch: 5| Step: 8
Training loss: 2.7039005756378174
Validation loss: 2.289609673202679

Epoch: 5| Step: 9
Training loss: 2.208895206451416
Validation loss: 2.297558751157535

Epoch: 5| Step: 10
Training loss: 1.9237033128738403
Validation loss: 2.2910293507319626

Epoch: 137| Step: 0
Training loss: 2.6059255599975586
Validation loss: 2.30791913565769

Epoch: 5| Step: 1
Training loss: 1.88382887840271
Validation loss: 2.3116235271576913

Epoch: 5| Step: 2
Training loss: 2.8868401050567627
Validation loss: 2.309421447015578

Epoch: 5| Step: 3
Training loss: 2.4367942810058594
Validation loss: 2.3136238282726658

Epoch: 5| Step: 4
Training loss: 2.517742156982422
Validation loss: 2.3057196781199467

Epoch: 5| Step: 5
Training loss: 2.207071542739868
Validation loss: 2.317567474098616

Epoch: 5| Step: 6
Training loss: 2.7745280265808105
Validation loss: 2.317428627321797

Epoch: 5| Step: 7
Training loss: 3.0366950035095215
Validation loss: 2.305890767805038

Epoch: 5| Step: 8
Training loss: 2.4609484672546387
Validation loss: 2.295150661981234

Epoch: 5| Step: 9
Training loss: 2.58454966545105
Validation loss: 2.289620958348756

Epoch: 5| Step: 10
Training loss: 2.3575551509857178
Validation loss: 2.278324181033719

Epoch: 138| Step: 0
Training loss: 2.4815165996551514
Validation loss: 2.2811407427633963

Epoch: 5| Step: 1
Training loss: 2.265082359313965
Validation loss: 2.278416318278159

Epoch: 5| Step: 2
Training loss: 2.6108250617980957
Validation loss: 2.273623179363948

Epoch: 5| Step: 3
Training loss: 2.7256479263305664
Validation loss: 2.2766601936791533

Epoch: 5| Step: 4
Training loss: 2.870133638381958
Validation loss: 2.2889074971598964

Epoch: 5| Step: 5
Training loss: 2.2182650566101074
Validation loss: 2.302614788855276

Epoch: 5| Step: 6
Training loss: 2.93741512298584
Validation loss: 2.3197020535827964

Epoch: 5| Step: 7
Training loss: 2.229905605316162
Validation loss: 2.322702405273273

Epoch: 5| Step: 8
Training loss: 2.360539197921753
Validation loss: 2.331478952079691

Epoch: 5| Step: 9
Training loss: 3.0340206623077393
Validation loss: 2.3218993038259526

Epoch: 5| Step: 10
Training loss: 1.900175929069519
Validation loss: 2.292472106154247

Epoch: 139| Step: 0
Training loss: 2.5088436603546143
Validation loss: 2.2858905048780542

Epoch: 5| Step: 1
Training loss: 2.402604103088379
Validation loss: 2.2702010318797123

Epoch: 5| Step: 2
Training loss: 2.289309501647949
Validation loss: 2.2705432573954263

Epoch: 5| Step: 3
Training loss: 2.955742120742798
Validation loss: 2.2610220986027874

Epoch: 5| Step: 4
Training loss: 2.8677356243133545
Validation loss: 2.258993625640869

Epoch: 5| Step: 5
Training loss: 2.009202480316162
Validation loss: 2.2603147952787337

Epoch: 5| Step: 6
Training loss: 2.654005289077759
Validation loss: 2.2587538355140278

Epoch: 5| Step: 7
Training loss: 2.115931749343872
Validation loss: 2.2576995485572406

Epoch: 5| Step: 8
Training loss: 2.4654390811920166
Validation loss: 2.2571192864448792

Epoch: 5| Step: 9
Training loss: 2.6217074394226074
Validation loss: 2.2594223894098753

Epoch: 5| Step: 10
Training loss: 2.8667032718658447
Validation loss: 2.2552317316814134

Epoch: 140| Step: 0
Training loss: 2.565652370452881
Validation loss: 2.265803398624543

Epoch: 5| Step: 1
Training loss: 1.8647010326385498
Validation loss: 2.2697301013495332

Epoch: 5| Step: 2
Training loss: 2.6473965644836426
Validation loss: 2.2831446970662763

Epoch: 5| Step: 3
Training loss: 2.5231947898864746
Validation loss: 2.2882187571576846

Epoch: 5| Step: 4
Training loss: 2.2164664268493652
Validation loss: 2.3146900259038454

Epoch: 5| Step: 5
Training loss: 2.405606746673584
Validation loss: 2.3211322612659906

Epoch: 5| Step: 6
Training loss: 2.701467990875244
Validation loss: 2.333823597559365

Epoch: 5| Step: 7
Training loss: 2.7712783813476562
Validation loss: 2.3422099057064263

Epoch: 5| Step: 8
Training loss: 2.788956880569458
Validation loss: 2.3289238611857095

Epoch: 5| Step: 9
Training loss: 2.2966580390930176
Validation loss: 2.3204344421304683

Epoch: 5| Step: 10
Training loss: 2.8151073455810547
Validation loss: 2.3006078325292116

Epoch: 141| Step: 0
Training loss: 2.7927448749542236
Validation loss: 2.2814039620020057

Epoch: 5| Step: 1
Training loss: 2.222498655319214
Validation loss: 2.271420431393449

Epoch: 5| Step: 2
Training loss: 2.273974657058716
Validation loss: 2.2587316420770462

Epoch: 5| Step: 3
Training loss: 2.229290723800659
Validation loss: 2.262607543699203

Epoch: 5| Step: 4
Training loss: 2.789159059524536
Validation loss: 2.261838618145194

Epoch: 5| Step: 5
Training loss: 1.993507742881775
Validation loss: 2.2563983701890513

Epoch: 5| Step: 6
Training loss: 2.4347081184387207
Validation loss: 2.260356459566342

Epoch: 5| Step: 7
Training loss: 2.4716007709503174
Validation loss: 2.2589107431391233

Epoch: 5| Step: 8
Training loss: 2.5713915824890137
Validation loss: 2.255013732499974

Epoch: 5| Step: 9
Training loss: 3.596172332763672
Validation loss: 2.277495802089732

Epoch: 5| Step: 10
Training loss: 2.163032054901123
Validation loss: 2.2915026526297293

Epoch: 142| Step: 0
Training loss: 2.2097866535186768
Validation loss: 2.3133450861900084

Epoch: 5| Step: 1
Training loss: 2.1358542442321777
Validation loss: 2.3164735199302755

Epoch: 5| Step: 2
Training loss: 2.3808608055114746
Validation loss: 2.313295684834962

Epoch: 5| Step: 3
Training loss: 2.5504164695739746
Validation loss: 2.320907305645686

Epoch: 5| Step: 4
Training loss: 2.3576135635375977
Validation loss: 2.305527538381597

Epoch: 5| Step: 5
Training loss: 2.6351945400238037
Validation loss: 2.3105445395233812

Epoch: 5| Step: 6
Training loss: 3.434331178665161
Validation loss: 2.298492908477783

Epoch: 5| Step: 7
Training loss: 2.8137879371643066
Validation loss: 2.28817944629218

Epoch: 5| Step: 8
Training loss: 2.4204907417297363
Validation loss: 2.2711920815129436

Epoch: 5| Step: 9
Training loss: 2.2312400341033936
Validation loss: 2.2589401814245407

Epoch: 5| Step: 10
Training loss: 2.3560307025909424
Validation loss: 2.252671690397365

Epoch: 143| Step: 0
Training loss: 2.66686749458313
Validation loss: 2.2493060122254076

Epoch: 5| Step: 1
Training loss: 2.328356981277466
Validation loss: 2.247311343428909

Epoch: 5| Step: 2
Training loss: 3.1163289546966553
Validation loss: 2.241047982246645

Epoch: 5| Step: 3
Training loss: 2.2674717903137207
Validation loss: 2.240446880299558

Epoch: 5| Step: 4
Training loss: 2.0137672424316406
Validation loss: 2.240021228790283

Epoch: 5| Step: 5
Training loss: 2.0618863105773926
Validation loss: 2.242169908297959

Epoch: 5| Step: 6
Training loss: 2.739980697631836
Validation loss: 2.2513125814417356

Epoch: 5| Step: 7
Training loss: 2.600905656814575
Validation loss: 2.2587450242811635

Epoch: 5| Step: 8
Training loss: 2.200698137283325
Validation loss: 2.27450494868781

Epoch: 5| Step: 9
Training loss: 2.7881553173065186
Validation loss: 2.2880281889310448

Epoch: 5| Step: 10
Training loss: 2.8372576236724854
Validation loss: 2.2991269121887865

Epoch: 144| Step: 0
Training loss: 2.6253714561462402
Validation loss: 2.321424079197709

Epoch: 5| Step: 1
Training loss: 2.1662135124206543
Validation loss: 2.3282246871661116

Epoch: 5| Step: 2
Training loss: 2.2104740142822266
Validation loss: 2.3241957874708277

Epoch: 5| Step: 3
Training loss: 2.3478379249572754
Validation loss: 2.305699427922567

Epoch: 5| Step: 4
Training loss: 3.047785758972168
Validation loss: 2.304704528982921

Epoch: 5| Step: 5
Training loss: 2.924548625946045
Validation loss: 2.304732661093435

Epoch: 5| Step: 6
Training loss: 2.361936569213867
Validation loss: 2.291296907650527

Epoch: 5| Step: 7
Training loss: 2.2742817401885986
Validation loss: 2.272474137685632

Epoch: 5| Step: 8
Training loss: 2.090080499649048
Validation loss: 2.2629505741980767

Epoch: 5| Step: 9
Training loss: 2.522960901260376
Validation loss: 2.2638277007687475

Epoch: 5| Step: 10
Training loss: 3.1723077297210693
Validation loss: 2.250476433384803

Epoch: 145| Step: 0
Training loss: 2.166728973388672
Validation loss: 2.2498257262732393

Epoch: 5| Step: 1
Training loss: 2.5006866455078125
Validation loss: 2.241445743909446

Epoch: 5| Step: 2
Training loss: 2.5921757221221924
Validation loss: 2.250641021677243

Epoch: 5| Step: 3
Training loss: 2.4527032375335693
Validation loss: 2.2695586924911826

Epoch: 5| Step: 4
Training loss: 2.695352077484131
Validation loss: 2.2704084893708587

Epoch: 5| Step: 5
Training loss: 2.2281205654144287
Validation loss: 2.2711561213257494

Epoch: 5| Step: 6
Training loss: 1.8305485248565674
Validation loss: 2.288678789651522

Epoch: 5| Step: 7
Training loss: 2.7068393230438232
Validation loss: 2.299923099497313

Epoch: 5| Step: 8
Training loss: 3.080139398574829
Validation loss: 2.3064568363210207

Epoch: 5| Step: 9
Training loss: 2.850069999694824
Validation loss: 2.31182756475223

Epoch: 5| Step: 10
Training loss: 2.5585098266601562
Validation loss: 2.294702927271525

Epoch: 146| Step: 0
Training loss: 2.311554431915283
Validation loss: 2.2719658113295034

Epoch: 5| Step: 1
Training loss: 2.8089494705200195
Validation loss: 2.277298455597252

Epoch: 5| Step: 2
Training loss: 2.2017340660095215
Validation loss: 2.260850326989287

Epoch: 5| Step: 3
Training loss: 2.7157936096191406
Validation loss: 2.2584817371060772

Epoch: 5| Step: 4
Training loss: 2.5569615364074707
Validation loss: 2.250573294137114

Epoch: 5| Step: 5
Training loss: 2.903973340988159
Validation loss: 2.251713313082213

Epoch: 5| Step: 6
Training loss: 2.283720016479492
Validation loss: 2.2590312214307886

Epoch: 5| Step: 7
Training loss: 2.6356472969055176
Validation loss: 2.259920535549041

Epoch: 5| Step: 8
Training loss: 2.6975700855255127
Validation loss: 2.2566583207858506

Epoch: 5| Step: 9
Training loss: 1.993628740310669
Validation loss: 2.2717359476192023

Epoch: 5| Step: 10
Training loss: 2.334908962249756
Validation loss: 2.277977515292424

Epoch: 147| Step: 0
Training loss: 2.279703378677368
Validation loss: 2.2807070773134948

Epoch: 5| Step: 1
Training loss: 2.9426381587982178
Validation loss: 2.2951652772964968

Epoch: 5| Step: 2
Training loss: 2.010946273803711
Validation loss: 2.3023825794137935

Epoch: 5| Step: 3
Training loss: 2.7799015045166016
Validation loss: 2.296471849564583

Epoch: 5| Step: 4
Training loss: 2.0841081142425537
Validation loss: 2.295533090509394

Epoch: 5| Step: 5
Training loss: 2.736288547515869
Validation loss: 2.298715788830993

Epoch: 5| Step: 6
Training loss: 2.0934481620788574
Validation loss: 2.3059197395078597

Epoch: 5| Step: 7
Training loss: 2.6689579486846924
Validation loss: 2.3238299892794703

Epoch: 5| Step: 8
Training loss: 2.3602561950683594
Validation loss: 2.317542770857452

Epoch: 5| Step: 9
Training loss: 3.187110424041748
Validation loss: 2.310446805851434

Epoch: 5| Step: 10
Training loss: 2.349409818649292
Validation loss: 2.298256194719704

Epoch: 148| Step: 0
Training loss: 2.7835946083068848
Validation loss: 2.2958868267715618

Epoch: 5| Step: 1
Training loss: 2.7052974700927734
Validation loss: 2.2755048223721084

Epoch: 5| Step: 2
Training loss: 2.293447971343994
Validation loss: 2.2575894273737425

Epoch: 5| Step: 3
Training loss: 2.632023811340332
Validation loss: 2.258684838971784

Epoch: 5| Step: 4
Training loss: 2.659900188446045
Validation loss: 2.2522479641822075

Epoch: 5| Step: 5
Training loss: 2.9782142639160156
Validation loss: 2.2544450606069257

Epoch: 5| Step: 6
Training loss: 2.1644859313964844
Validation loss: 2.253286797513244

Epoch: 5| Step: 7
Training loss: 2.5783920288085938
Validation loss: 2.2569536291142946

Epoch: 5| Step: 8
Training loss: 1.9580061435699463
Validation loss: 2.265373896527034

Epoch: 5| Step: 9
Training loss: 1.8981926441192627
Validation loss: 2.2717614225161973

Epoch: 5| Step: 10
Training loss: 2.8983356952667236
Validation loss: 2.281175608276039

Epoch: 149| Step: 0
Training loss: 2.656496286392212
Validation loss: 2.2795513035148702

Epoch: 5| Step: 1
Training loss: 2.427783966064453
Validation loss: 2.290611902872721

Epoch: 5| Step: 2
Training loss: 2.080300807952881
Validation loss: 2.296919434301315

Epoch: 5| Step: 3
Training loss: 2.690016269683838
Validation loss: 2.2885315969426143

Epoch: 5| Step: 4
Training loss: 2.4097354412078857
Validation loss: 2.2867231574109805

Epoch: 5| Step: 5
Training loss: 2.288381576538086
Validation loss: 2.2976443331728698

Epoch: 5| Step: 6
Training loss: 3.4520580768585205
Validation loss: 2.3164876020082863

Epoch: 5| Step: 7
Training loss: 2.339174747467041
Validation loss: 2.325362995106687

Epoch: 5| Step: 8
Training loss: 2.313452959060669
Validation loss: 2.3386030479144027

Epoch: 5| Step: 9
Training loss: 2.062563419342041
Validation loss: 2.350171873646398

Epoch: 5| Step: 10
Training loss: 2.770893096923828
Validation loss: 2.331081226307859

Epoch: 150| Step: 0
Training loss: 2.2274131774902344
Validation loss: 2.311982290719145

Epoch: 5| Step: 1
Training loss: 2.952143907546997
Validation loss: 2.2910643392993557

Epoch: 5| Step: 2
Training loss: 2.215924024581909
Validation loss: 2.2826527536556287

Epoch: 5| Step: 3
Training loss: 2.9147567749023438
Validation loss: 2.2783895102880334

Epoch: 5| Step: 4
Training loss: 2.316350221633911
Validation loss: 2.279062331363719

Epoch: 5| Step: 5
Training loss: 2.6225297451019287
Validation loss: 2.2730718889544086

Epoch: 5| Step: 6
Training loss: 2.725968837738037
Validation loss: 2.2658554302748812

Epoch: 5| Step: 7
Training loss: 1.7440516948699951
Validation loss: 2.261671758467151

Epoch: 5| Step: 8
Training loss: 2.9126360416412354
Validation loss: 2.25766077861991

Epoch: 5| Step: 9
Training loss: 2.4048848152160645
Validation loss: 2.2619624727515766

Epoch: 5| Step: 10
Training loss: 2.3863651752471924
Validation loss: 2.2607203004180745

Epoch: 151| Step: 0
Training loss: 2.528587818145752
Validation loss: 2.259709292842496

Epoch: 5| Step: 1
Training loss: 1.7327064275741577
Validation loss: 2.2734494696381273

Epoch: 5| Step: 2
Training loss: 2.2946815490722656
Validation loss: 2.2800978165800854

Epoch: 5| Step: 3
Training loss: 2.852919816970825
Validation loss: 2.294632621990737

Epoch: 5| Step: 4
Training loss: 2.3227310180664062
Validation loss: 2.297793339657527

Epoch: 5| Step: 5
Training loss: 2.147498369216919
Validation loss: 2.296868006388346

Epoch: 5| Step: 6
Training loss: 2.87717866897583
Validation loss: 2.315571892646051

Epoch: 5| Step: 7
Training loss: 2.340869665145874
Validation loss: 2.324694948811685

Epoch: 5| Step: 8
Training loss: 2.8892555236816406
Validation loss: 2.293105899646718

Epoch: 5| Step: 9
Training loss: 2.675370931625366
Validation loss: 2.2646636142525622

Epoch: 5| Step: 10
Training loss: 2.79646897315979
Validation loss: 2.250994133692916

Epoch: 152| Step: 0
Training loss: 2.402329921722412
Validation loss: 2.2285534092175063

Epoch: 5| Step: 1
Training loss: 2.0749590396881104
Validation loss: 2.233530693156745

Epoch: 5| Step: 2
Training loss: 2.496674060821533
Validation loss: 2.2379941863398396

Epoch: 5| Step: 3
Training loss: 2.7384259700775146
Validation loss: 2.237462748763382

Epoch: 5| Step: 4
Training loss: 2.6810801029205322
Validation loss: 2.2341302723012944

Epoch: 5| Step: 5
Training loss: 2.8017990589141846
Validation loss: 2.2367512461959675

Epoch: 5| Step: 6
Training loss: 1.978844404220581
Validation loss: 2.243624561576433

Epoch: 5| Step: 7
Training loss: 2.7311184406280518
Validation loss: 2.2389350552712717

Epoch: 5| Step: 8
Training loss: 2.879615068435669
Validation loss: 2.24571870475687

Epoch: 5| Step: 9
Training loss: 2.1486215591430664
Validation loss: 2.2512612163379626

Epoch: 5| Step: 10
Training loss: 2.6607232093811035
Validation loss: 2.2464262413722214

Epoch: 153| Step: 0
Training loss: 2.331033706665039
Validation loss: 2.251812481111096

Epoch: 5| Step: 1
Training loss: 2.6511833667755127
Validation loss: 2.255593522902458

Epoch: 5| Step: 2
Training loss: 2.9935028553009033
Validation loss: 2.2583533781830982

Epoch: 5| Step: 3
Training loss: 2.6654276847839355
Validation loss: 2.278452152846962

Epoch: 5| Step: 4
Training loss: 2.9168825149536133
Validation loss: 2.3093806056566137

Epoch: 5| Step: 5
Training loss: 1.6373279094696045
Validation loss: 2.3259709573561147

Epoch: 5| Step: 6
Training loss: 2.2369935512542725
Validation loss: 2.3432278812572522

Epoch: 5| Step: 7
Training loss: 1.9338794946670532
Validation loss: 2.3284949897437968

Epoch: 5| Step: 8
Training loss: 2.505937099456787
Validation loss: 2.3158378703619844

Epoch: 5| Step: 9
Training loss: 2.5135409832000732
Validation loss: 2.3105277143498903

Epoch: 5| Step: 10
Training loss: 3.114253044128418
Validation loss: 2.2753426951746785

Epoch: 154| Step: 0
Training loss: 2.1147072315216064
Validation loss: 2.2679695237067437

Epoch: 5| Step: 1
Training loss: 2.891955614089966
Validation loss: 2.2581096746588267

Epoch: 5| Step: 2
Training loss: 2.581646680831909
Validation loss: 2.2547164040227092

Epoch: 5| Step: 3
Training loss: 2.500699520111084
Validation loss: 2.248646015762001

Epoch: 5| Step: 4
Training loss: 2.2951247692108154
Validation loss: 2.2506375748624086

Epoch: 5| Step: 5
Training loss: 2.75177264213562
Validation loss: 2.2503805827069026

Epoch: 5| Step: 6
Training loss: 2.455385684967041
Validation loss: 2.25004405103704

Epoch: 5| Step: 7
Training loss: 1.961155652999878
Validation loss: 2.2525435288747153

Epoch: 5| Step: 8
Training loss: 1.867415428161621
Validation loss: 2.2624814010435537

Epoch: 5| Step: 9
Training loss: 3.3180885314941406
Validation loss: 2.264691093916534

Epoch: 5| Step: 10
Training loss: 2.508668899536133
Validation loss: 2.266900811144101

Epoch: 155| Step: 0
Training loss: 2.7009730339050293
Validation loss: 2.257030448605937

Epoch: 5| Step: 1
Training loss: 1.9806302785873413
Validation loss: 2.26396531187078

Epoch: 5| Step: 2
Training loss: 2.548619270324707
Validation loss: 2.271540070092806

Epoch: 5| Step: 3
Training loss: 2.5258724689483643
Validation loss: 2.28073473130503

Epoch: 5| Step: 4
Training loss: 2.8673768043518066
Validation loss: 2.271036814617854

Epoch: 5| Step: 5
Training loss: 2.085831880569458
Validation loss: 2.27259280989247

Epoch: 5| Step: 6
Training loss: 2.5770227909088135
Validation loss: 2.2917344467614287

Epoch: 5| Step: 7
Training loss: 2.4446187019348145
Validation loss: 2.293627828680059

Epoch: 5| Step: 8
Training loss: 3.0451560020446777
Validation loss: 2.2958906491597495

Epoch: 5| Step: 9
Training loss: 2.4728798866271973
Validation loss: 2.2933602230523222

Epoch: 5| Step: 10
Training loss: 1.843001365661621
Validation loss: 2.3127872969514582

Epoch: 156| Step: 0
Training loss: 2.972311019897461
Validation loss: 2.2960517457736436

Epoch: 5| Step: 1
Training loss: 2.1519672870635986
Validation loss: 2.3030673278275358

Epoch: 5| Step: 2
Training loss: 2.2360382080078125
Validation loss: 2.30734569539306

Epoch: 5| Step: 3
Training loss: 2.0219058990478516
Validation loss: 2.305278262784404

Epoch: 5| Step: 4
Training loss: 2.9354379177093506
Validation loss: 2.310260634268484

Epoch: 5| Step: 5
Training loss: 2.2963500022888184
Validation loss: 2.3141817431296072

Epoch: 5| Step: 6
Training loss: 2.2059922218322754
Validation loss: 2.299032921432167

Epoch: 5| Step: 7
Training loss: 2.640409231185913
Validation loss: 2.2921814264789706

Epoch: 5| Step: 8
Training loss: 3.2458672523498535
Validation loss: 2.2850584804370837

Epoch: 5| Step: 9
Training loss: 2.49853253364563
Validation loss: 2.2555570358871133

Epoch: 5| Step: 10
Training loss: 1.9616907835006714
Validation loss: 2.248256634640437

Epoch: 157| Step: 0
Training loss: 3.06706166267395
Validation loss: 2.2399152607046147

Epoch: 5| Step: 1
Training loss: 2.1154189109802246
Validation loss: 2.2325236592241513

Epoch: 5| Step: 2
Training loss: 2.411329984664917
Validation loss: 2.231950108722974

Epoch: 5| Step: 3
Training loss: 2.7078943252563477
Validation loss: 2.2313466943720335

Epoch: 5| Step: 4
Training loss: 2.393004894256592
Validation loss: 2.234303817954115

Epoch: 5| Step: 5
Training loss: 1.7817745208740234
Validation loss: 2.237899309845381

Epoch: 5| Step: 6
Training loss: 2.2907931804656982
Validation loss: 2.244191429948294

Epoch: 5| Step: 7
Training loss: 2.4548442363739014
Validation loss: 2.264557284693564

Epoch: 5| Step: 8
Training loss: 2.62300181388855
Validation loss: 2.2851649804781844

Epoch: 5| Step: 9
Training loss: 2.770423412322998
Validation loss: 2.3202978923756588

Epoch: 5| Step: 10
Training loss: 2.9260599613189697
Validation loss: 2.3268740023336103

Epoch: 158| Step: 0
Training loss: 2.1858372688293457
Validation loss: 2.3173978405614055

Epoch: 5| Step: 1
Training loss: 2.545839786529541
Validation loss: 2.3044723015959545

Epoch: 5| Step: 2
Training loss: 2.6612350940704346
Validation loss: 2.2822061559205413

Epoch: 5| Step: 3
Training loss: 2.0013303756713867
Validation loss: 2.265386889057775

Epoch: 5| Step: 4
Training loss: 2.341658592224121
Validation loss: 2.265666251541466

Epoch: 5| Step: 5
Training loss: 2.656513214111328
Validation loss: 2.253595798246322

Epoch: 5| Step: 6
Training loss: 2.9888720512390137
Validation loss: 2.2603245832586802

Epoch: 5| Step: 7
Training loss: 2.587907552719116
Validation loss: 2.2534261518909084

Epoch: 5| Step: 8
Training loss: 2.2895889282226562
Validation loss: 2.2463432050520376

Epoch: 5| Step: 9
Training loss: 2.482132911682129
Validation loss: 2.2522082713342484

Epoch: 5| Step: 10
Training loss: 2.3598406314849854
Validation loss: 2.245788220436342

Epoch: 159| Step: 0
Training loss: 2.6532058715820312
Validation loss: 2.2435550484606015

Epoch: 5| Step: 1
Training loss: 2.5383944511413574
Validation loss: 2.2376413345336914

Epoch: 5| Step: 2
Training loss: 2.2268426418304443
Validation loss: 2.244431890467162

Epoch: 5| Step: 3
Training loss: 2.0291364192962646
Validation loss: 2.240595208701267

Epoch: 5| Step: 4
Training loss: 2.832014799118042
Validation loss: 2.248616933822632

Epoch: 5| Step: 5
Training loss: 1.98164963722229
Validation loss: 2.2529801476386284

Epoch: 5| Step: 6
Training loss: 2.069725513458252
Validation loss: 2.2508041243399344

Epoch: 5| Step: 7
Training loss: 2.7671146392822266
Validation loss: 2.2494231244569183

Epoch: 5| Step: 8
Training loss: 2.669884204864502
Validation loss: 2.264591860514815

Epoch: 5| Step: 9
Training loss: 3.501885175704956
Validation loss: 2.2635569854449202

Epoch: 5| Step: 10
Training loss: 1.745862364768982
Validation loss: 2.2556602544682

Epoch: 160| Step: 0
Training loss: 2.6053833961486816
Validation loss: 2.251531506097445

Epoch: 5| Step: 1
Training loss: 1.9676024913787842
Validation loss: 2.2546629316063336

Epoch: 5| Step: 2
Training loss: 2.100527048110962
Validation loss: 2.24914050614962

Epoch: 5| Step: 3
Training loss: 3.0498852729797363
Validation loss: 2.2500889814028175

Epoch: 5| Step: 4
Training loss: 2.5782980918884277
Validation loss: 2.239995200146911

Epoch: 5| Step: 5
Training loss: 2.511871337890625
Validation loss: 2.2432651455684374

Epoch: 5| Step: 6
Training loss: 2.6647305488586426
Validation loss: 2.2550056647228938

Epoch: 5| Step: 7
Training loss: 2.480846405029297
Validation loss: 2.263445628586636

Epoch: 5| Step: 8
Training loss: 2.2000577449798584
Validation loss: 2.2450772434152584

Epoch: 5| Step: 9
Training loss: 2.5200400352478027
Validation loss: 2.2456660783419045

Epoch: 5| Step: 10
Training loss: 2.553651809692383
Validation loss: 2.236110364237139

Epoch: 161| Step: 0
Training loss: 2.444105863571167
Validation loss: 2.2224314828072824

Epoch: 5| Step: 1
Training loss: 2.7577786445617676
Validation loss: 2.2236398381571614

Epoch: 5| Step: 2
Training loss: 2.530080795288086
Validation loss: 2.221000644468492

Epoch: 5| Step: 3
Training loss: 2.393096923828125
Validation loss: 2.2271654029046335

Epoch: 5| Step: 4
Training loss: 2.1291632652282715
Validation loss: 2.2253055521236953

Epoch: 5| Step: 5
Training loss: 2.5601985454559326
Validation loss: 2.2279376240186792

Epoch: 5| Step: 6
Training loss: 2.010585308074951
Validation loss: 2.2285005995022353

Epoch: 5| Step: 7
Training loss: 3.0010483264923096
Validation loss: 2.2279476401626424

Epoch: 5| Step: 8
Training loss: 2.460526943206787
Validation loss: 2.2392011637328775

Epoch: 5| Step: 9
Training loss: 2.127368211746216
Validation loss: 2.249103335924046

Epoch: 5| Step: 10
Training loss: 2.7434098720550537
Validation loss: 2.254047493780813

Epoch: 162| Step: 0
Training loss: 2.3243279457092285
Validation loss: 2.2588174163654284

Epoch: 5| Step: 1
Training loss: 2.493016004562378
Validation loss: 2.2589936717864005

Epoch: 5| Step: 2
Training loss: 2.85398530960083
Validation loss: 2.2690233568991385

Epoch: 5| Step: 3
Training loss: 2.999828577041626
Validation loss: 2.2855112603915635

Epoch: 5| Step: 4
Training loss: 3.0429089069366455
Validation loss: 2.2753549186132287

Epoch: 5| Step: 5
Training loss: 2.7259089946746826
Validation loss: 2.2702978785319994

Epoch: 5| Step: 6
Training loss: 2.120694637298584
Validation loss: 2.2475965689587336

Epoch: 5| Step: 7
Training loss: 1.426120638847351
Validation loss: 2.2451300595396306

Epoch: 5| Step: 8
Training loss: 2.3179869651794434
Validation loss: 2.2452578736889746

Epoch: 5| Step: 9
Training loss: 2.3579983711242676
Validation loss: 2.244537548352313

Epoch: 5| Step: 10
Training loss: 2.358955144882202
Validation loss: 2.243596928094023

Epoch: 163| Step: 0
Training loss: 2.079871416091919
Validation loss: 2.246964157268565

Epoch: 5| Step: 1
Training loss: 2.248533010482788
Validation loss: 2.248265456127864

Epoch: 5| Step: 2
Training loss: 2.416395902633667
Validation loss: 2.2387251700124433

Epoch: 5| Step: 3
Training loss: 2.477491855621338
Validation loss: 2.234395832143804

Epoch: 5| Step: 4
Training loss: 2.4886183738708496
Validation loss: 2.232963349229546

Epoch: 5| Step: 5
Training loss: 2.732590436935425
Validation loss: 2.2382474022526897

Epoch: 5| Step: 6
Training loss: 2.727045774459839
Validation loss: 2.2284660493173907

Epoch: 5| Step: 7
Training loss: 2.5811901092529297
Validation loss: 2.2290227746450775

Epoch: 5| Step: 8
Training loss: 2.2402167320251465
Validation loss: 2.228661790970833

Epoch: 5| Step: 9
Training loss: 2.803964614868164
Validation loss: 2.2288870901189823

Epoch: 5| Step: 10
Training loss: 2.3271472454071045
Validation loss: 2.2264823041936403

Epoch: 164| Step: 0
Training loss: 2.407839775085449
Validation loss: 2.232772005501614

Epoch: 5| Step: 1
Training loss: 2.180863857269287
Validation loss: 2.232503749990976

Epoch: 5| Step: 2
Training loss: 2.2458090782165527
Validation loss: 2.2424253981600524

Epoch: 5| Step: 3
Training loss: 2.4869577884674072
Validation loss: 2.2666777321087417

Epoch: 5| Step: 4
Training loss: 2.679579496383667
Validation loss: 2.289046411873192

Epoch: 5| Step: 5
Training loss: 2.1749351024627686
Validation loss: 2.321733590095274

Epoch: 5| Step: 6
Training loss: 3.19598126411438
Validation loss: 2.3326803304815806

Epoch: 5| Step: 7
Training loss: 2.6094815731048584
Validation loss: 2.3139056749241327

Epoch: 5| Step: 8
Training loss: 2.4330921173095703
Validation loss: 2.3101783696041314

Epoch: 5| Step: 9
Training loss: 2.072300910949707
Validation loss: 2.2726156045031805

Epoch: 5| Step: 10
Training loss: 2.830700159072876
Validation loss: 2.233052078113761

Epoch: 165| Step: 0
Training loss: 2.0638856887817383
Validation loss: 2.2197945297405286

Epoch: 5| Step: 1
Training loss: 2.3189477920532227
Validation loss: 2.2160727131751274

Epoch: 5| Step: 2
Training loss: 2.765833616256714
Validation loss: 2.211098893996208

Epoch: 5| Step: 3
Training loss: 2.271413564682007
Validation loss: 2.208727587935745

Epoch: 5| Step: 4
Training loss: 2.0677857398986816
Validation loss: 2.215184801368303

Epoch: 5| Step: 5
Training loss: 2.635075092315674
Validation loss: 2.210435482763475

Epoch: 5| Step: 6
Training loss: 1.7199872732162476
Validation loss: 2.2164728385145946

Epoch: 5| Step: 7
Training loss: 2.3973159790039062
Validation loss: 2.2174257539933726

Epoch: 5| Step: 8
Training loss: 2.9612889289855957
Validation loss: 2.2269146109140046

Epoch: 5| Step: 9
Training loss: 2.9281420707702637
Validation loss: 2.2250475832211074

Epoch: 5| Step: 10
Training loss: 3.303232192993164
Validation loss: 2.2264005061118834

Epoch: 166| Step: 0
Training loss: 2.475724697113037
Validation loss: 2.246744325084071

Epoch: 5| Step: 1
Training loss: 2.1266751289367676
Validation loss: 2.2569350888652187

Epoch: 5| Step: 2
Training loss: 2.311494827270508
Validation loss: 2.2847578845998293

Epoch: 5| Step: 3
Training loss: 2.2035269737243652
Validation loss: 2.302394428560811

Epoch: 5| Step: 4
Training loss: 2.930886745452881
Validation loss: 2.328317108974662

Epoch: 5| Step: 5
Training loss: 2.7797324657440186
Validation loss: 2.333054478450488

Epoch: 5| Step: 6
Training loss: 2.7090721130371094
Validation loss: 2.311156757416264

Epoch: 5| Step: 7
Training loss: 1.8470573425292969
Validation loss: 2.2981742992196033

Epoch: 5| Step: 8
Training loss: 2.281543731689453
Validation loss: 2.2879741012409167

Epoch: 5| Step: 9
Training loss: 2.9382376670837402
Validation loss: 2.2441629773827008

Epoch: 5| Step: 10
Training loss: 2.5511112213134766
Validation loss: 2.2233755152712584

Epoch: 167| Step: 0
Training loss: 2.4628207683563232
Validation loss: 2.211866876130463

Epoch: 5| Step: 1
Training loss: 2.8360044956207275
Validation loss: 2.2021148230439875

Epoch: 5| Step: 2
Training loss: 2.7273402214050293
Validation loss: 2.2109218694830455

Epoch: 5| Step: 3
Training loss: 1.9685672521591187
Validation loss: 2.2071727911631265

Epoch: 5| Step: 4
Training loss: 2.2564005851745605
Validation loss: 2.2003714166661745

Epoch: 5| Step: 5
Training loss: 2.161569356918335
Validation loss: 2.2043359074541318

Epoch: 5| Step: 6
Training loss: 3.0153896808624268
Validation loss: 2.201293242874966

Epoch: 5| Step: 7
Training loss: 2.5307674407958984
Validation loss: 2.211089148316332

Epoch: 5| Step: 8
Training loss: 1.9247373342514038
Validation loss: 2.224170854014735

Epoch: 5| Step: 9
Training loss: 2.9722437858581543
Validation loss: 2.2446220997841126

Epoch: 5| Step: 10
Training loss: 2.2005653381347656
Validation loss: 2.2601309976270123

Epoch: 168| Step: 0
Training loss: 2.3020970821380615
Validation loss: 2.2862507527874363

Epoch: 5| Step: 1
Training loss: 2.630765438079834
Validation loss: 2.2827378396065003

Epoch: 5| Step: 2
Training loss: 2.6683244705200195
Validation loss: 2.2884622850725727

Epoch: 5| Step: 3
Training loss: 3.2670581340789795
Validation loss: 2.2915688919764694

Epoch: 5| Step: 4
Training loss: 2.134761333465576
Validation loss: 2.259896262999504

Epoch: 5| Step: 5
Training loss: 2.2714436054229736
Validation loss: 2.24232381133623

Epoch: 5| Step: 6
Training loss: 1.867368459701538
Validation loss: 2.2475137813116914

Epoch: 5| Step: 7
Training loss: 1.6904842853546143
Validation loss: 2.2365154758576424

Epoch: 5| Step: 8
Training loss: 2.6774559020996094
Validation loss: 2.234485549311484

Epoch: 5| Step: 9
Training loss: 2.4081356525421143
Validation loss: 2.2318489154179892

Epoch: 5| Step: 10
Training loss: 3.136159896850586
Validation loss: 2.2311113188343663

Epoch: 169| Step: 0
Training loss: 3.290107250213623
Validation loss: 2.2274488223496305

Epoch: 5| Step: 1
Training loss: 2.326026439666748
Validation loss: 2.226929485156972

Epoch: 5| Step: 2
Training loss: 2.6573989391326904
Validation loss: 2.210205252452563

Epoch: 5| Step: 3
Training loss: 2.6563403606414795
Validation loss: 2.2117249311939364

Epoch: 5| Step: 4
Training loss: 2.0483765602111816
Validation loss: 2.211807830359346

Epoch: 5| Step: 5
Training loss: 2.1783447265625
Validation loss: 2.221563845552424

Epoch: 5| Step: 6
Training loss: 3.0848941802978516
Validation loss: 2.2184199107590543

Epoch: 5| Step: 7
Training loss: 1.8989086151123047
Validation loss: 2.2232310361759637

Epoch: 5| Step: 8
Training loss: 2.4535930156707764
Validation loss: 2.2380621228166806

Epoch: 5| Step: 9
Training loss: 2.4929356575012207
Validation loss: 2.237367687686797

Epoch: 5| Step: 10
Training loss: 1.5896189212799072
Validation loss: 2.253625192949849

Epoch: 170| Step: 0
Training loss: 2.2339253425598145
Validation loss: 2.265707662028651

Epoch: 5| Step: 1
Training loss: 2.9794578552246094
Validation loss: 2.2748112165799705

Epoch: 5| Step: 2
Training loss: 3.014378309249878
Validation loss: 2.2916123431216002

Epoch: 5| Step: 3
Training loss: 2.16159987449646
Validation loss: 2.280080823488133

Epoch: 5| Step: 4
Training loss: 2.6241939067840576
Validation loss: 2.290457974198044

Epoch: 5| Step: 5
Training loss: 1.8012139797210693
Validation loss: 2.2644913376018567

Epoch: 5| Step: 6
Training loss: 2.080354690551758
Validation loss: 2.245681983168407

Epoch: 5| Step: 7
Training loss: 2.782365322113037
Validation loss: 2.235462068229593

Epoch: 5| Step: 8
Training loss: 1.9529218673706055
Validation loss: 2.220386334644851

Epoch: 5| Step: 9
Training loss: 2.8161888122558594
Validation loss: 2.223181316929479

Epoch: 5| Step: 10
Training loss: 2.5045952796936035
Validation loss: 2.220889029964324

Epoch: 171| Step: 0
Training loss: 2.002403736114502
Validation loss: 2.222855926841818

Epoch: 5| Step: 1
Training loss: 2.128800868988037
Validation loss: 2.2185283181487874

Epoch: 5| Step: 2
Training loss: 2.224318027496338
Validation loss: 2.218367579162762

Epoch: 5| Step: 3
Training loss: 2.4860634803771973
Validation loss: 2.219800538914178

Epoch: 5| Step: 4
Training loss: 2.4109816551208496
Validation loss: 2.2162595461773615

Epoch: 5| Step: 5
Training loss: 2.907001495361328
Validation loss: 2.206214763784921

Epoch: 5| Step: 6
Training loss: 2.5570361614227295
Validation loss: 2.2222747828370784

Epoch: 5| Step: 7
Training loss: 1.9466688632965088
Validation loss: 2.2165728666449107

Epoch: 5| Step: 8
Training loss: 2.681122064590454
Validation loss: 2.2282729789774907

Epoch: 5| Step: 9
Training loss: 3.22517728805542
Validation loss: 2.2249314554276003

Epoch: 5| Step: 10
Training loss: 2.248821973800659
Validation loss: 2.2426254467297624

Epoch: 172| Step: 0
Training loss: 1.9918873310089111
Validation loss: 2.2453464820820797

Epoch: 5| Step: 1
Training loss: 2.7930562496185303
Validation loss: 2.2693293684272358

Epoch: 5| Step: 2
Training loss: 1.9385111331939697
Validation loss: 2.273206054523427

Epoch: 5| Step: 3
Training loss: 2.412822723388672
Validation loss: 2.2844962176456245

Epoch: 5| Step: 4
Training loss: 2.482895851135254
Validation loss: 2.2751047303599696

Epoch: 5| Step: 5
Training loss: 2.2907092571258545
Validation loss: 2.2607070374232467

Epoch: 5| Step: 6
Training loss: 3.1826884746551514
Validation loss: 2.2486008085230345

Epoch: 5| Step: 7
Training loss: 2.201503276824951
Validation loss: 2.236039824383233

Epoch: 5| Step: 8
Training loss: 2.681800127029419
Validation loss: 2.211114127148864

Epoch: 5| Step: 9
Training loss: 2.423891067504883
Validation loss: 2.2038546005884805

Epoch: 5| Step: 10
Training loss: 2.4859085083007812
Validation loss: 2.1970454082694104

Epoch: 173| Step: 0
Training loss: 2.885535717010498
Validation loss: 2.1984207040520123

Epoch: 5| Step: 1
Training loss: 2.347358226776123
Validation loss: 2.2039274861735683

Epoch: 5| Step: 2
Training loss: 2.6184017658233643
Validation loss: 2.217373319851455

Epoch: 5| Step: 3
Training loss: 2.1696600914001465
Validation loss: 2.2291551405383694

Epoch: 5| Step: 4
Training loss: 2.903926372528076
Validation loss: 2.2533080782941592

Epoch: 5| Step: 5
Training loss: 2.7496461868286133
Validation loss: 2.247177964897566

Epoch: 5| Step: 6
Training loss: 2.137946367263794
Validation loss: 2.2301045592113207

Epoch: 5| Step: 7
Training loss: 1.8018887042999268
Validation loss: 2.2190557500367523

Epoch: 5| Step: 8
Training loss: 2.622013807296753
Validation loss: 2.2111937653633857

Epoch: 5| Step: 9
Training loss: 2.533365488052368
Validation loss: 2.2085983163567

Epoch: 5| Step: 10
Training loss: 2.0629653930664062
Validation loss: 2.1998789131000476

Epoch: 174| Step: 0
Training loss: 2.3667829036712646
Validation loss: 2.1997065851765294

Epoch: 5| Step: 1
Training loss: 2.922861337661743
Validation loss: 2.2278326813892653

Epoch: 5| Step: 2
Training loss: 2.816873073577881
Validation loss: 2.232009897949875

Epoch: 5| Step: 3
Training loss: 2.220839023590088
Validation loss: 2.2379163439555834

Epoch: 5| Step: 4
Training loss: 2.6993041038513184
Validation loss: 2.2316321147385465

Epoch: 5| Step: 5
Training loss: 2.14249324798584
Validation loss: 2.218609071546985

Epoch: 5| Step: 6
Training loss: 2.5750114917755127
Validation loss: 2.2234768867492676

Epoch: 5| Step: 7
Training loss: 2.400829315185547
Validation loss: 2.230440383316368

Epoch: 5| Step: 8
Training loss: 2.6698086261749268
Validation loss: 2.24623603205527

Epoch: 5| Step: 9
Training loss: 2.6688733100891113
Validation loss: 2.2547526333921697

Epoch: 5| Step: 10
Training loss: 1.0280849933624268
Validation loss: 2.2770555147560696

Epoch: 175| Step: 0
Training loss: 2.1596689224243164
Validation loss: 2.267249266306559

Epoch: 5| Step: 1
Training loss: 2.3671164512634277
Validation loss: 2.2638297721903813

Epoch: 5| Step: 2
Training loss: 2.4385335445404053
Validation loss: 2.2372814686067644

Epoch: 5| Step: 3
Training loss: 3.0874555110931396
Validation loss: 2.2342861980520268

Epoch: 5| Step: 4
Training loss: 2.2577667236328125
Validation loss: 2.2237401777698147

Epoch: 5| Step: 5
Training loss: 2.399763584136963
Validation loss: 2.2154463952587498

Epoch: 5| Step: 6
Training loss: 2.2359509468078613
Validation loss: 2.2041080985018002

Epoch: 5| Step: 7
Training loss: 1.9422357082366943
Validation loss: 2.2074616237353255

Epoch: 5| Step: 8
Training loss: 2.8785927295684814
Validation loss: 2.2155710394664476

Epoch: 5| Step: 9
Training loss: 2.6938893795013428
Validation loss: 2.2107681279541342

Epoch: 5| Step: 10
Training loss: 2.239439010620117
Validation loss: 2.224456707636515

Epoch: 176| Step: 0
Training loss: 2.5199055671691895
Validation loss: 2.215060980089249

Epoch: 5| Step: 1
Training loss: 2.1304001808166504
Validation loss: 2.229990692548854

Epoch: 5| Step: 2
Training loss: 2.1575562953948975
Validation loss: 2.2329408250829226

Epoch: 5| Step: 3
Training loss: 2.0011069774627686
Validation loss: 2.270679368767687

Epoch: 5| Step: 4
Training loss: 2.590590000152588
Validation loss: 2.2777179056598293

Epoch: 5| Step: 5
Training loss: 2.4557995796203613
Validation loss: 2.2614950672272713

Epoch: 5| Step: 6
Training loss: 2.559972047805786
Validation loss: 2.273729972941901

Epoch: 5| Step: 7
Training loss: 2.435896158218384
Validation loss: 2.2617314502757084

Epoch: 5| Step: 8
Training loss: 3.4232165813446045
Validation loss: 2.2306034449608094

Epoch: 5| Step: 9
Training loss: 2.0734477043151855
Validation loss: 2.2152601108756116

Epoch: 5| Step: 10
Training loss: 2.2737584114074707
Validation loss: 2.1939268291637464

Epoch: 177| Step: 0
Training loss: 2.456446647644043
Validation loss: 2.2032203712771015

Epoch: 5| Step: 1
Training loss: 1.6669857501983643
Validation loss: 2.190648178900442

Epoch: 5| Step: 2
Training loss: 2.2563393115997314
Validation loss: 2.1984346041115383

Epoch: 5| Step: 3
Training loss: 2.5897088050842285
Validation loss: 2.1969282832196964

Epoch: 5| Step: 4
Training loss: 2.3874869346618652
Validation loss: 2.196501108907884

Epoch: 5| Step: 5
Training loss: 3.164839267730713
Validation loss: 2.2107784004621607

Epoch: 5| Step: 6
Training loss: 2.30511736869812
Validation loss: 2.217864482633529

Epoch: 5| Step: 7
Training loss: 2.2173917293548584
Validation loss: 2.2173738159159178

Epoch: 5| Step: 8
Training loss: 2.461801052093506
Validation loss: 2.22993803024292

Epoch: 5| Step: 9
Training loss: 2.0637521743774414
Validation loss: 2.2343032526713547

Epoch: 5| Step: 10
Training loss: 3.159104585647583
Validation loss: 2.234044554413006

Epoch: 178| Step: 0
Training loss: 2.13881254196167
Validation loss: 2.241083957815683

Epoch: 5| Step: 1
Training loss: 2.090811252593994
Validation loss: 2.2649269539822816

Epoch: 5| Step: 2
Training loss: 2.0533175468444824
Validation loss: 2.2747522707908385

Epoch: 5| Step: 3
Training loss: 2.6170248985290527
Validation loss: 2.2616319912736134

Epoch: 5| Step: 4
Training loss: 1.8626127243041992
Validation loss: 2.245891655645063

Epoch: 5| Step: 5
Training loss: 2.199033737182617
Validation loss: 2.231979275262484

Epoch: 5| Step: 6
Training loss: 2.9006881713867188
Validation loss: 2.2199861311143443

Epoch: 5| Step: 7
Training loss: 2.958630084991455
Validation loss: 2.210726943067325

Epoch: 5| Step: 8
Training loss: 2.151134490966797
Validation loss: 2.19665749226847

Epoch: 5| Step: 9
Training loss: 2.532857656478882
Validation loss: 2.194714948695193

Epoch: 5| Step: 10
Training loss: 3.2956883907318115
Validation loss: 2.184634739352811

Epoch: 179| Step: 0
Training loss: 2.835149049758911
Validation loss: 2.1756504453638548

Epoch: 5| Step: 1
Training loss: 2.134519100189209
Validation loss: 2.186135438180739

Epoch: 5| Step: 2
Training loss: 2.090047836303711
Validation loss: 2.184162232183641

Epoch: 5| Step: 3
Training loss: 2.5304946899414062
Validation loss: 2.187722370188723

Epoch: 5| Step: 4
Training loss: 2.2244677543640137
Validation loss: 2.1946035149276897

Epoch: 5| Step: 5
Training loss: 2.123284339904785
Validation loss: 2.2033703762997865

Epoch: 5| Step: 6
Training loss: 2.283468246459961
Validation loss: 2.2064740016896236

Epoch: 5| Step: 7
Training loss: 3.047354221343994
Validation loss: 2.1965481645317486

Epoch: 5| Step: 8
Training loss: 2.647706985473633
Validation loss: 2.1873574077442126

Epoch: 5| Step: 9
Training loss: 2.1678073406219482
Validation loss: 2.182952412994959

Epoch: 5| Step: 10
Training loss: 2.6293890476226807
Validation loss: 2.1833572208240466

Epoch: 180| Step: 0
Training loss: 2.1660513877868652
Validation loss: 2.18901482448783

Epoch: 5| Step: 1
Training loss: 2.719611644744873
Validation loss: 2.1916775934157835

Epoch: 5| Step: 2
Training loss: 2.0225625038146973
Validation loss: 2.2136153482621714

Epoch: 5| Step: 3
Training loss: 2.4878487586975098
Validation loss: 2.2342698702248196

Epoch: 5| Step: 4
Training loss: 2.2146220207214355
Validation loss: 2.2306202098887455

Epoch: 5| Step: 5
Training loss: 2.531714916229248
Validation loss: 2.228155771891276

Epoch: 5| Step: 6
Training loss: 2.2404308319091797
Validation loss: 2.231031995947643

Epoch: 5| Step: 7
Training loss: 2.3265602588653564
Validation loss: 2.2223280783622497

Epoch: 5| Step: 8
Training loss: 2.7656877040863037
Validation loss: 2.233137092282695

Epoch: 5| Step: 9
Training loss: 2.2844104766845703
Validation loss: 2.2448282831458637

Epoch: 5| Step: 10
Training loss: 2.8701391220092773
Validation loss: 2.25030957370676

Epoch: 181| Step: 0
Training loss: 2.764951229095459
Validation loss: 2.2426955674284246

Epoch: 5| Step: 1
Training loss: 2.3868696689605713
Validation loss: 2.241302926053283

Epoch: 5| Step: 2
Training loss: 2.6466917991638184
Validation loss: 2.2455878616661153

Epoch: 5| Step: 3
Training loss: 2.382920503616333
Validation loss: 2.233846551628523

Epoch: 5| Step: 4
Training loss: 2.4621493816375732
Validation loss: 2.24515599589194

Epoch: 5| Step: 5
Training loss: 1.424695372581482
Validation loss: 2.220023460285638

Epoch: 5| Step: 6
Training loss: 2.0482876300811768
Validation loss: 2.2113664509147726

Epoch: 5| Step: 7
Training loss: 2.9675021171569824
Validation loss: 2.204610937385149

Epoch: 5| Step: 8
Training loss: 2.0771255493164062
Validation loss: 2.208516700293428

Epoch: 5| Step: 9
Training loss: 2.6858444213867188
Validation loss: 2.2033341661576302

Epoch: 5| Step: 10
Training loss: 2.6469240188598633
Validation loss: 2.2167564771508657

Epoch: 182| Step: 0
Training loss: 1.8613163232803345
Validation loss: 2.2316901376170497

Epoch: 5| Step: 1
Training loss: 2.1475989818573
Validation loss: 2.236914115567361

Epoch: 5| Step: 2
Training loss: 2.678591251373291
Validation loss: 2.2335957352833082

Epoch: 5| Step: 3
Training loss: 2.501774787902832
Validation loss: 2.2299076793014363

Epoch: 5| Step: 4
Training loss: 2.4725327491760254
Validation loss: 2.2181543791165916

Epoch: 5| Step: 5
Training loss: 2.8688406944274902
Validation loss: 2.1993416406775035

Epoch: 5| Step: 6
Training loss: 2.2554187774658203
Validation loss: 2.190087967021491

Epoch: 5| Step: 7
Training loss: 2.221780300140381
Validation loss: 2.1895527429478143

Epoch: 5| Step: 8
Training loss: 2.707604169845581
Validation loss: 2.186351695368367

Epoch: 5| Step: 9
Training loss: 2.5971081256866455
Validation loss: 2.179783627551089

Epoch: 5| Step: 10
Training loss: 2.1530814170837402
Validation loss: 2.1933263450540523

Epoch: 183| Step: 0
Training loss: 2.285217761993408
Validation loss: 2.21756999979737

Epoch: 5| Step: 1
Training loss: 2.641052722930908
Validation loss: 2.2397212828359296

Epoch: 5| Step: 2
Training loss: 2.6900429725646973
Validation loss: 2.264102235917122

Epoch: 5| Step: 3
Training loss: 2.6749892234802246
Validation loss: 2.3137552968917356

Epoch: 5| Step: 4
Training loss: 2.0212578773498535
Validation loss: 2.3362707579007713

Epoch: 5| Step: 5
Training loss: 2.5516610145568848
Validation loss: 2.3475605403223345

Epoch: 5| Step: 6
Training loss: 2.0802862644195557
Validation loss: 2.342479936538204

Epoch: 5| Step: 7
Training loss: 2.5478737354278564
Validation loss: 2.31532363225055

Epoch: 5| Step: 8
Training loss: 2.2881433963775635
Validation loss: 2.292922830068937

Epoch: 5| Step: 9
Training loss: 1.841691255569458
Validation loss: 2.2459818035043697

Epoch: 5| Step: 10
Training loss: 3.2401046752929688
Validation loss: 2.231369815846925

Epoch: 184| Step: 0
Training loss: 2.1102347373962402
Validation loss: 2.2142917007528324

Epoch: 5| Step: 1
Training loss: 3.4066672325134277
Validation loss: 2.199552938502322

Epoch: 5| Step: 2
Training loss: 2.4082272052764893
Validation loss: 2.1869589436438774

Epoch: 5| Step: 3
Training loss: 2.2347264289855957
Validation loss: 2.1938392680178405

Epoch: 5| Step: 4
Training loss: 2.5668013095855713
Validation loss: 2.1893628899769118

Epoch: 5| Step: 5
Training loss: 2.4463603496551514
Validation loss: 2.1930521483062417

Epoch: 5| Step: 6
Training loss: 2.0525240898132324
Validation loss: 2.1958484854749454

Epoch: 5| Step: 7
Training loss: 2.8769936561584473
Validation loss: 2.2029352444474415

Epoch: 5| Step: 8
Training loss: 2.4099478721618652
Validation loss: 2.2088265137005876

Epoch: 5| Step: 9
Training loss: 2.115917921066284
Validation loss: 2.211153586705526

Epoch: 5| Step: 10
Training loss: 1.813506841659546
Validation loss: 2.2175378722529255

Epoch: 185| Step: 0
Training loss: 1.9908103942871094
Validation loss: 2.2309376411540534

Epoch: 5| Step: 1
Training loss: 2.337322950363159
Validation loss: 2.2181314729875132

Epoch: 5| Step: 2
Training loss: 2.055147647857666
Validation loss: 2.218954131167422

Epoch: 5| Step: 3
Training loss: 2.6029162406921387
Validation loss: 2.218785544877411

Epoch: 5| Step: 4
Training loss: 3.04905366897583
Validation loss: 2.2185697811906055

Epoch: 5| Step: 5
Training loss: 2.3834023475646973
Validation loss: 2.2148840888853996

Epoch: 5| Step: 6
Training loss: 2.2846198081970215
Validation loss: 2.209629146001672

Epoch: 5| Step: 7
Training loss: 1.9972903728485107
Validation loss: 2.215554892375905

Epoch: 5| Step: 8
Training loss: 2.0749502182006836
Validation loss: 2.2134804007827595

Epoch: 5| Step: 9
Training loss: 2.363516330718994
Validation loss: 2.2343178679866176

Epoch: 5| Step: 10
Training loss: 3.374286413192749
Validation loss: 2.225554532902215

Epoch: 186| Step: 0
Training loss: 1.9864637851715088
Validation loss: 2.22625700120003

Epoch: 5| Step: 1
Training loss: 2.5354056358337402
Validation loss: 2.2202511628468833

Epoch: 5| Step: 2
Training loss: 2.82944917678833
Validation loss: 2.2188808982090285

Epoch: 5| Step: 3
Training loss: 2.1901001930236816
Validation loss: 2.224286194770567

Epoch: 5| Step: 4
Training loss: 2.8263638019561768
Validation loss: 2.208002369890931

Epoch: 5| Step: 5
Training loss: 2.603422164916992
Validation loss: 2.1974877311337377

Epoch: 5| Step: 6
Training loss: 2.023155927658081
Validation loss: 2.180282054408904

Epoch: 5| Step: 7
Training loss: 2.925909996032715
Validation loss: 2.1910238727446525

Epoch: 5| Step: 8
Training loss: 2.1824584007263184
Validation loss: 2.18973042631662

Epoch: 5| Step: 9
Training loss: 1.9086997509002686
Validation loss: 2.181624643264278

Epoch: 5| Step: 10
Training loss: 2.346768379211426
Validation loss: 2.2014564519287436

Epoch: 187| Step: 0
Training loss: 2.3973774909973145
Validation loss: 2.2076108968386086

Epoch: 5| Step: 1
Training loss: 2.3280649185180664
Validation loss: 2.206825407602454

Epoch: 5| Step: 2
Training loss: 2.3345062732696533
Validation loss: 2.212849583677066

Epoch: 5| Step: 3
Training loss: 2.7599549293518066
Validation loss: 2.214933159530804

Epoch: 5| Step: 4
Training loss: 2.1624763011932373
Validation loss: 2.206462985725813

Epoch: 5| Step: 5
Training loss: 2.400472402572632
Validation loss: 2.209856520416916

Epoch: 5| Step: 6
Training loss: 2.728001832962036
Validation loss: 2.2130545826368433

Epoch: 5| Step: 7
Training loss: 2.8321616649627686
Validation loss: 2.209835765182331

Epoch: 5| Step: 8
Training loss: 2.1163108348846436
Validation loss: 2.19571231385713

Epoch: 5| Step: 9
Training loss: 2.010861873626709
Validation loss: 2.2277295166446316

Epoch: 5| Step: 10
Training loss: 2.168358087539673
Validation loss: 2.2428994178771973

Epoch: 188| Step: 0
Training loss: 2.2623000144958496
Validation loss: 2.2569507604004233

Epoch: 5| Step: 1
Training loss: 3.0311455726623535
Validation loss: 2.2927875057343514

Epoch: 5| Step: 2
Training loss: 2.360028028488159
Validation loss: 2.2699059183879564

Epoch: 5| Step: 3
Training loss: 2.153083086013794
Validation loss: 2.2213462001533917

Epoch: 5| Step: 4
Training loss: 3.0725436210632324
Validation loss: 2.1985705027016262

Epoch: 5| Step: 5
Training loss: 2.6754202842712402
Validation loss: 2.1968017560179516

Epoch: 5| Step: 6
Training loss: 2.415437936782837
Validation loss: 2.18195540161543

Epoch: 5| Step: 7
Training loss: 1.8583011627197266
Validation loss: 2.1776144632729153

Epoch: 5| Step: 8
Training loss: 2.197279691696167
Validation loss: 2.180286997108049

Epoch: 5| Step: 9
Training loss: 2.2820236682891846
Validation loss: 2.1827660811844694

Epoch: 5| Step: 10
Training loss: 1.953169822692871
Validation loss: 2.1845206932354997

Epoch: 189| Step: 0
Training loss: 2.6766180992126465
Validation loss: 2.199542386557466

Epoch: 5| Step: 1
Training loss: 2.065040111541748
Validation loss: 2.223346907605407

Epoch: 5| Step: 2
Training loss: 2.3097567558288574
Validation loss: 2.2475111792164464

Epoch: 5| Step: 3
Training loss: 2.1073317527770996
Validation loss: 2.283913878984349

Epoch: 5| Step: 4
Training loss: 2.4091451168060303
Validation loss: 2.2924322928151777

Epoch: 5| Step: 5
Training loss: 2.7405264377593994
Validation loss: 2.2876241053304365

Epoch: 5| Step: 6
Training loss: 2.6311633586883545
Validation loss: 2.2712486636254097

Epoch: 5| Step: 7
Training loss: 2.436537742614746
Validation loss: 2.2490742834665443

Epoch: 5| Step: 8
Training loss: 2.0690133571624756
Validation loss: 2.2310515501165904

Epoch: 5| Step: 9
Training loss: 2.4021217823028564
Validation loss: 2.2031487213668

Epoch: 5| Step: 10
Training loss: 2.6938669681549072
Validation loss: 2.1898014571077082

Epoch: 190| Step: 0
Training loss: 2.80692458152771
Validation loss: 2.190012280659009

Epoch: 5| Step: 1
Training loss: 1.8696529865264893
Validation loss: 2.1700102078017367

Epoch: 5| Step: 2
Training loss: 2.650710105895996
Validation loss: 2.1644735310667302

Epoch: 5| Step: 3
Training loss: 1.8771095275878906
Validation loss: 2.161739198110437

Epoch: 5| Step: 4
Training loss: 2.583631992340088
Validation loss: 2.1629892472297914

Epoch: 5| Step: 5
Training loss: 2.1215527057647705
Validation loss: 2.167657018989645

Epoch: 5| Step: 6
Training loss: 2.414212703704834
Validation loss: 2.1796514782854306

Epoch: 5| Step: 7
Training loss: 2.1435060501098633
Validation loss: 2.197205916527779

Epoch: 5| Step: 8
Training loss: 2.3172450065612793
Validation loss: 2.2316133540163756

Epoch: 5| Step: 9
Training loss: 2.4738669395446777
Validation loss: 2.2830701451147757

Epoch: 5| Step: 10
Training loss: 3.062878131866455
Validation loss: 2.3181183427892704

Epoch: 191| Step: 0
Training loss: 2.491044282913208
Validation loss: 2.35697671931277

Epoch: 5| Step: 1
Training loss: 2.7470505237579346
Validation loss: 2.3042514093460573

Epoch: 5| Step: 2
Training loss: 2.129868984222412
Validation loss: 2.2743294226225985

Epoch: 5| Step: 3
Training loss: 2.376835346221924
Validation loss: 2.2415798171874015

Epoch: 5| Step: 4
Training loss: 2.8588614463806152
Validation loss: 2.229778374395063

Epoch: 5| Step: 5
Training loss: 2.516928195953369
Validation loss: 2.213915155779931

Epoch: 5| Step: 6
Training loss: 2.3653457164764404
Validation loss: 2.1873282283864994

Epoch: 5| Step: 7
Training loss: 1.6687018871307373
Validation loss: 2.1764299715718916

Epoch: 5| Step: 8
Training loss: 2.105976104736328
Validation loss: 2.184994125878939

Epoch: 5| Step: 9
Training loss: 2.4446539878845215
Validation loss: 2.185137641045355

Epoch: 5| Step: 10
Training loss: 2.6823205947875977
Validation loss: 2.175109927372266

Epoch: 192| Step: 0
Training loss: 2.1986231803894043
Validation loss: 2.185604347977587

Epoch: 5| Step: 1
Training loss: 2.629863977432251
Validation loss: 2.19386129225454

Epoch: 5| Step: 2
Training loss: 1.860226035118103
Validation loss: 2.195488670820831

Epoch: 5| Step: 3
Training loss: 1.8125648498535156
Validation loss: 2.195187718637528

Epoch: 5| Step: 4
Training loss: 2.65995192527771
Validation loss: 2.19471489485874

Epoch: 5| Step: 5
Training loss: 3.0284581184387207
Validation loss: 2.2143783312971874

Epoch: 5| Step: 6
Training loss: 2.2192435264587402
Validation loss: 2.228724742448458

Epoch: 5| Step: 7
Training loss: 2.559885025024414
Validation loss: 2.2232715775889735

Epoch: 5| Step: 8
Training loss: 3.11326265335083
Validation loss: 2.2244313814306773

Epoch: 5| Step: 9
Training loss: 1.9861772060394287
Validation loss: 2.2240322136109874

Epoch: 5| Step: 10
Training loss: 1.956397294998169
Validation loss: 2.2363280019452496

Epoch: 193| Step: 0
Training loss: 2.068049907684326
Validation loss: 2.2614098236125004

Epoch: 5| Step: 1
Training loss: 2.7145607471466064
Validation loss: 2.2509668027201006

Epoch: 5| Step: 2
Training loss: 2.965810775756836
Validation loss: 2.2464836335951284

Epoch: 5| Step: 3
Training loss: 2.6359918117523193
Validation loss: 2.235353259630101

Epoch: 5| Step: 4
Training loss: 2.3912081718444824
Validation loss: 2.2338764590601765

Epoch: 5| Step: 5
Training loss: 2.910686492919922
Validation loss: 2.222061364881454

Epoch: 5| Step: 6
Training loss: 1.4634828567504883
Validation loss: 2.235711013117144

Epoch: 5| Step: 7
Training loss: 1.9343475103378296
Validation loss: 2.2383258035106044

Epoch: 5| Step: 8
Training loss: 1.8756803274154663
Validation loss: 2.2316989309044293

Epoch: 5| Step: 9
Training loss: 2.1105942726135254
Validation loss: 2.214472181053572

Epoch: 5| Step: 10
Training loss: 3.048845052719116
Validation loss: 2.198583641359883

Epoch: 194| Step: 0
Training loss: 2.1004538536071777
Validation loss: 2.1902404395482873

Epoch: 5| Step: 1
Training loss: 1.8563339710235596
Validation loss: 2.1847524027670584

Epoch: 5| Step: 2
Training loss: 3.2002105712890625
Validation loss: 2.171622637779482

Epoch: 5| Step: 3
Training loss: 2.3033926486968994
Validation loss: 2.186378284167218

Epoch: 5| Step: 4
Training loss: 2.381964921951294
Validation loss: 2.1746223459961596

Epoch: 5| Step: 5
Training loss: 2.6279540061950684
Validation loss: 2.1775705865634385

Epoch: 5| Step: 6
Training loss: 2.5545125007629395
Validation loss: 2.174633019713945

Epoch: 5| Step: 7
Training loss: 1.9133555889129639
Validation loss: 2.1754678795414586

Epoch: 5| Step: 8
Training loss: 2.6051106452941895
Validation loss: 2.190478865818311

Epoch: 5| Step: 9
Training loss: 2.7553372383117676
Validation loss: 2.1975624535673406

Epoch: 5| Step: 10
Training loss: 1.7227907180786133
Validation loss: 2.2216101461841213

Epoch: 195| Step: 0
Training loss: 2.589864492416382
Validation loss: 2.2411010431986984

Epoch: 5| Step: 1
Training loss: 2.610827684402466
Validation loss: 2.242283046886485

Epoch: 5| Step: 2
Training loss: 2.365917682647705
Validation loss: 2.2394443852927095

Epoch: 5| Step: 3
Training loss: 1.9816452264785767
Validation loss: 2.2354118157458562

Epoch: 5| Step: 4
Training loss: 2.870530366897583
Validation loss: 2.2238839339184504

Epoch: 5| Step: 5
Training loss: 2.5893898010253906
Validation loss: 2.223925223914526

Epoch: 5| Step: 6
Training loss: 1.974492073059082
Validation loss: 2.217729540281398

Epoch: 5| Step: 7
Training loss: 2.479060649871826
Validation loss: 2.223568247210595

Epoch: 5| Step: 8
Training loss: 2.106473207473755
Validation loss: 2.2256396893532044

Epoch: 5| Step: 9
Training loss: 2.7216498851776123
Validation loss: 2.2379561880583405

Epoch: 5| Step: 10
Training loss: 1.7476718425750732
Validation loss: 2.223786231010191

Epoch: 196| Step: 0
Training loss: 1.877763032913208
Validation loss: 2.2115921640908844

Epoch: 5| Step: 1
Training loss: 2.9192659854888916
Validation loss: 2.2081333527001004

Epoch: 5| Step: 2
Training loss: 1.3606176376342773
Validation loss: 2.193527106315859

Epoch: 5| Step: 3
Training loss: 2.9725537300109863
Validation loss: 2.195871933814018

Epoch: 5| Step: 4
Training loss: 2.3602771759033203
Validation loss: 2.187225790433986

Epoch: 5| Step: 5
Training loss: 2.4961295127868652
Validation loss: 2.1912309277442192

Epoch: 5| Step: 6
Training loss: 2.503011465072632
Validation loss: 2.202540551462481

Epoch: 5| Step: 7
Training loss: 2.51540207862854
Validation loss: 2.190451737373106

Epoch: 5| Step: 8
Training loss: 2.033935546875
Validation loss: 2.1920114076265724

Epoch: 5| Step: 9
Training loss: 2.6779465675354004
Validation loss: 2.1870718591956684

Epoch: 5| Step: 10
Training loss: 2.185060501098633
Validation loss: 2.196358988361974

Epoch: 197| Step: 0
Training loss: 2.7925491333007812
Validation loss: 2.2099862970331663

Epoch: 5| Step: 1
Training loss: 2.109395742416382
Validation loss: 2.2158177321957004

Epoch: 5| Step: 2
Training loss: 2.773796558380127
Validation loss: 2.236261313961398

Epoch: 5| Step: 3
Training loss: 2.1490895748138428
Validation loss: 2.260355803274339

Epoch: 5| Step: 4
Training loss: 2.316628932952881
Validation loss: 2.274665273645873

Epoch: 5| Step: 5
Training loss: 2.5281717777252197
Validation loss: 2.243572409434985

Epoch: 5| Step: 6
Training loss: 3.0469095706939697
Validation loss: 2.2180795592646443

Epoch: 5| Step: 7
Training loss: 2.029651165008545
Validation loss: 2.1966554862196728

Epoch: 5| Step: 8
Training loss: 2.1129138469696045
Validation loss: 2.173933876458035

Epoch: 5| Step: 9
Training loss: 1.9773578643798828
Validation loss: 2.1796172767557125

Epoch: 5| Step: 10
Training loss: 2.0372283458709717
Validation loss: 2.1819460135634228

Epoch: 198| Step: 0
Training loss: 2.0596303939819336
Validation loss: 2.186949171045775

Epoch: 5| Step: 1
Training loss: 2.6589794158935547
Validation loss: 2.2032617933006695

Epoch: 5| Step: 2
Training loss: 2.786245346069336
Validation loss: 2.2481781410914596

Epoch: 5| Step: 3
Training loss: 2.589007616043091
Validation loss: 2.241206817729499

Epoch: 5| Step: 4
Training loss: 1.652397871017456
Validation loss: 2.2237524806812243

Epoch: 5| Step: 5
Training loss: 2.1598916053771973
Validation loss: 2.191861298776442

Epoch: 5| Step: 6
Training loss: 2.1711738109588623
Validation loss: 2.179341249568488

Epoch: 5| Step: 7
Training loss: 3.153458833694458
Validation loss: 2.1822085688191075

Epoch: 5| Step: 8
Training loss: 2.537990093231201
Validation loss: 2.1720995621014665

Epoch: 5| Step: 9
Training loss: 1.8653287887573242
Validation loss: 2.1845787622595347

Epoch: 5| Step: 10
Training loss: 2.3625755310058594
Validation loss: 2.1870780324423187

Epoch: 199| Step: 0
Training loss: 2.8017516136169434
Validation loss: 2.1846978074760846

Epoch: 5| Step: 1
Training loss: 2.8136868476867676
Validation loss: 2.1843415896097818

Epoch: 5| Step: 2
Training loss: 1.7578579187393188
Validation loss: 2.185178828495805

Epoch: 5| Step: 3
Training loss: 2.6777868270874023
Validation loss: 2.1708095483882452

Epoch: 5| Step: 4
Training loss: 2.297532320022583
Validation loss: 2.1726619966568483

Epoch: 5| Step: 5
Training loss: 2.8168156147003174
Validation loss: 2.1867659784132436

Epoch: 5| Step: 6
Training loss: 1.9915616512298584
Validation loss: 2.2281737609576155

Epoch: 5| Step: 7
Training loss: 1.503000020980835
Validation loss: 2.267488710341915

Epoch: 5| Step: 8
Training loss: 2.6139614582061768
Validation loss: 2.277538894325174

Epoch: 5| Step: 9
Training loss: 3.1115365028381348
Validation loss: 2.274290047666078

Epoch: 5| Step: 10
Training loss: 1.4624557495117188
Validation loss: 2.251035790289602

Epoch: 200| Step: 0
Training loss: 2.3190791606903076
Validation loss: 2.213455689850674

Epoch: 5| Step: 1
Training loss: 2.1747193336486816
Validation loss: 2.195324702929425

Epoch: 5| Step: 2
Training loss: 2.420442581176758
Validation loss: 2.1887856157877112

Epoch: 5| Step: 3
Training loss: 2.87205171585083
Validation loss: 2.1842071356311923

Epoch: 5| Step: 4
Training loss: 2.773486614227295
Validation loss: 2.182806395715283

Epoch: 5| Step: 5
Training loss: 2.2481751441955566
Validation loss: 2.179961459611052

Epoch: 5| Step: 6
Training loss: 2.356764793395996
Validation loss: 2.190123934899607

Epoch: 5| Step: 7
Training loss: 2.830308198928833
Validation loss: 2.1921876963748725

Epoch: 5| Step: 8
Training loss: 1.987666368484497
Validation loss: 2.1968439086791007

Epoch: 5| Step: 9
Training loss: 1.8090683221817017
Validation loss: 2.213820885586482

Epoch: 5| Step: 10
Training loss: 2.149211883544922
Validation loss: 2.2076608134854223

Testing loss: 2.35364658302731

Epoch: 1| Step: 0
Training loss: 4.594103813171387
Validation loss: 5.151406754729568

Epoch: 5| Step: 1
Training loss: 3.9002177715301514
Validation loss: 5.141684650092997

Epoch: 5| Step: 2
Training loss: 4.453519344329834
Validation loss: 5.132229681937925

Epoch: 5| Step: 3
Training loss: 4.988852500915527
Validation loss: 5.123539857966925

Epoch: 5| Step: 4
Training loss: 5.161756992340088
Validation loss: 5.115015655435542

Epoch: 5| Step: 5
Training loss: 5.239340305328369
Validation loss: 5.106172279645038

Epoch: 5| Step: 6
Training loss: 4.943456172943115
Validation loss: 5.09655253605176

Epoch: 5| Step: 7
Training loss: 5.6506547927856445
Validation loss: 5.085882976490964

Epoch: 5| Step: 8
Training loss: 4.7460174560546875
Validation loss: 5.074251528709166

Epoch: 5| Step: 9
Training loss: 4.93057107925415
Validation loss: 5.061735286507555

Epoch: 5| Step: 10
Training loss: 5.368293285369873
Validation loss: 5.048152313437513

Epoch: 2| Step: 0
Training loss: 5.490609645843506
Validation loss: 5.033055372135614

Epoch: 5| Step: 1
Training loss: 4.924677848815918
Validation loss: 5.017873835820024

Epoch: 5| Step: 2
Training loss: 4.726207256317139
Validation loss: 5.000838654015654

Epoch: 5| Step: 3
Training loss: 4.293446063995361
Validation loss: 4.982270507402317

Epoch: 5| Step: 4
Training loss: 4.7201642990112305
Validation loss: 4.9625299822899605

Epoch: 5| Step: 5
Training loss: 4.6443657875061035
Validation loss: 4.942394800083612

Epoch: 5| Step: 6
Training loss: 5.460078239440918
Validation loss: 4.920479118183095

Epoch: 5| Step: 7
Training loss: 4.900672912597656
Validation loss: 4.895759808119907

Epoch: 5| Step: 8
Training loss: 4.715510845184326
Validation loss: 4.87024050886913

Epoch: 5| Step: 9
Training loss: 4.113177299499512
Validation loss: 4.842785445592737

Epoch: 5| Step: 10
Training loss: 3.9275853633880615
Validation loss: 4.813335305900984

Epoch: 3| Step: 0
Training loss: 4.477372169494629
Validation loss: 4.782434914701728

Epoch: 5| Step: 1
Training loss: 4.788877010345459
Validation loss: 4.749337221986504

Epoch: 5| Step: 2
Training loss: 4.059964179992676
Validation loss: 4.715958769603442

Epoch: 5| Step: 3
Training loss: 4.602569103240967
Validation loss: 4.681404103514969

Epoch: 5| Step: 4
Training loss: 4.131214141845703
Validation loss: 4.643008439771591

Epoch: 5| Step: 5
Training loss: 4.854963779449463
Validation loss: 4.603184807685114

Epoch: 5| Step: 6
Training loss: 4.702579498291016
Validation loss: 4.561746986963415

Epoch: 5| Step: 7
Training loss: 3.955475330352783
Validation loss: 4.520391602669993

Epoch: 5| Step: 8
Training loss: 4.586567401885986
Validation loss: 4.477257297885034

Epoch: 5| Step: 9
Training loss: 3.7306792736053467
Validation loss: 4.435005064933531

Epoch: 5| Step: 10
Training loss: 4.357186794281006
Validation loss: 4.395043962745256

Epoch: 4| Step: 0
Training loss: 4.297210693359375
Validation loss: 4.357426722844441

Epoch: 5| Step: 1
Training loss: 3.193161964416504
Validation loss: 4.3208918417653726

Epoch: 5| Step: 2
Training loss: 3.815197706222534
Validation loss: 4.2870349627669135

Epoch: 5| Step: 3
Training loss: 4.66397762298584
Validation loss: 4.2528594975830405

Epoch: 5| Step: 4
Training loss: 3.7958977222442627
Validation loss: 4.2206465198147685

Epoch: 5| Step: 5
Training loss: 4.516205310821533
Validation loss: 4.1880813875506

Epoch: 5| Step: 6
Training loss: 4.624724388122559
Validation loss: 4.155163703426238

Epoch: 5| Step: 7
Training loss: 3.803178071975708
Validation loss: 4.121090273703298

Epoch: 5| Step: 8
Training loss: 3.6096832752227783
Validation loss: 4.0896254329271216

Epoch: 5| Step: 9
Training loss: 3.9380905628204346
Validation loss: 4.055799473998367

Epoch: 5| Step: 10
Training loss: 3.7202343940734863
Validation loss: 4.024662981751145

Epoch: 5| Step: 0
Training loss: 3.2535176277160645
Validation loss: 3.9951902102398615

Epoch: 5| Step: 1
Training loss: 4.649218559265137
Validation loss: 3.9668948111995572

Epoch: 5| Step: 2
Training loss: 4.212540626525879
Validation loss: 3.9400675527511106

Epoch: 5| Step: 3
Training loss: 3.64143705368042
Validation loss: 3.9149425773210424

Epoch: 5| Step: 4
Training loss: 3.6989753246307373
Validation loss: 3.890319962655344

Epoch: 5| Step: 5
Training loss: 4.438514232635498
Validation loss: 3.869102952300861

Epoch: 5| Step: 6
Training loss: 2.952918529510498
Validation loss: 3.8476541221782727

Epoch: 5| Step: 7
Training loss: 3.523242235183716
Validation loss: 3.827840123125302

Epoch: 5| Step: 8
Training loss: 4.603598117828369
Validation loss: 3.8126418436727216

Epoch: 5| Step: 9
Training loss: 3.006243944168091
Validation loss: 3.792718005436723

Epoch: 5| Step: 10
Training loss: 3.1370060443878174
Validation loss: 3.7761485089537916

Epoch: 6| Step: 0
Training loss: 3.394103527069092
Validation loss: 3.7585039061884724

Epoch: 5| Step: 1
Training loss: 3.766533613204956
Validation loss: 3.744121515622703

Epoch: 5| Step: 2
Training loss: 3.0960915088653564
Validation loss: 3.729536461573775

Epoch: 5| Step: 3
Training loss: 3.29716157913208
Validation loss: 3.7143864836744083

Epoch: 5| Step: 4
Training loss: 2.991360902786255
Validation loss: 3.6995091438293457

Epoch: 5| Step: 5
Training loss: 4.0168352127075195
Validation loss: 3.6842707818554294

Epoch: 5| Step: 6
Training loss: 4.053601264953613
Validation loss: 3.6690898582499516

Epoch: 5| Step: 7
Training loss: 3.341768980026245
Validation loss: 3.6505568053132746

Epoch: 5| Step: 8
Training loss: 3.803884983062744
Validation loss: 3.6353690239690963

Epoch: 5| Step: 9
Training loss: 4.04683780670166
Validation loss: 3.6187905239802536

Epoch: 5| Step: 10
Training loss: 3.581545114517212
Validation loss: 3.596150013708299

Epoch: 7| Step: 0
Training loss: 3.6257553100585938
Validation loss: 3.5778725377974974

Epoch: 5| Step: 1
Training loss: 3.453711748123169
Validation loss: 3.561799218577723

Epoch: 5| Step: 2
Training loss: 3.338709592819214
Validation loss: 3.5465987215759935

Epoch: 5| Step: 3
Training loss: 3.3996684551239014
Validation loss: 3.5335431406574864

Epoch: 5| Step: 4
Training loss: 3.9948318004608154
Validation loss: 3.5206867315435924

Epoch: 5| Step: 5
Training loss: 3.2235655784606934
Validation loss: 3.5102191637921076

Epoch: 5| Step: 6
Training loss: 3.0014219284057617
Validation loss: 3.4972988226080455

Epoch: 5| Step: 7
Training loss: 2.949805736541748
Validation loss: 3.4870955636424403

Epoch: 5| Step: 8
Training loss: 4.210768699645996
Validation loss: 3.474554797654511

Epoch: 5| Step: 9
Training loss: 3.2193856239318848
Validation loss: 3.4599409103393555

Epoch: 5| Step: 10
Training loss: 3.3975086212158203
Validation loss: 3.4445786835044943

Epoch: 8| Step: 0
Training loss: 3.034219741821289
Validation loss: 3.4335012794822775

Epoch: 5| Step: 1
Training loss: 2.741957187652588
Validation loss: 3.419294516245524

Epoch: 5| Step: 2
Training loss: 2.8350367546081543
Validation loss: 3.4086125666095364

Epoch: 5| Step: 3
Training loss: 4.054256439208984
Validation loss: 3.399824639802338

Epoch: 5| Step: 4
Training loss: 3.3225841522216797
Validation loss: 3.39454589351531

Epoch: 5| Step: 5
Training loss: 3.1105198860168457
Validation loss: 3.385164676174041

Epoch: 5| Step: 6
Training loss: 2.5715854167938232
Validation loss: 3.3728396277273855

Epoch: 5| Step: 7
Training loss: 4.1410040855407715
Validation loss: 3.363826472272155

Epoch: 5| Step: 8
Training loss: 3.6438660621643066
Validation loss: 3.359038504221106

Epoch: 5| Step: 9
Training loss: 4.080435276031494
Validation loss: 3.3497986511517595

Epoch: 5| Step: 10
Training loss: 3.117335557937622
Validation loss: 3.345601794540241

Epoch: 9| Step: 0
Training loss: 3.7121551036834717
Validation loss: 3.3353900422332106

Epoch: 5| Step: 1
Training loss: 1.9794700145721436
Validation loss: 3.329781565614926

Epoch: 5| Step: 2
Training loss: 3.3544723987579346
Validation loss: 3.321313511940741

Epoch: 5| Step: 3
Training loss: 3.753877639770508
Validation loss: 3.314778197196222

Epoch: 5| Step: 4
Training loss: 3.32673978805542
Validation loss: 3.305927353520547

Epoch: 5| Step: 5
Training loss: 2.4183480739593506
Validation loss: 3.30045751858783

Epoch: 5| Step: 6
Training loss: 3.863919734954834
Validation loss: 3.2936003925979778

Epoch: 5| Step: 7
Training loss: 4.158064365386963
Validation loss: 3.2902777400068057

Epoch: 5| Step: 8
Training loss: 3.13969349861145
Validation loss: 3.2902140002096854

Epoch: 5| Step: 9
Training loss: 3.5258536338806152
Validation loss: 3.279782384954473

Epoch: 5| Step: 10
Training loss: 2.6737959384918213
Validation loss: 3.2680041174734793

Epoch: 10| Step: 0
Training loss: 3.0494494438171387
Validation loss: 3.2727403384383007

Epoch: 5| Step: 1
Training loss: 2.7734196186065674
Validation loss: 3.2693162989872757

Epoch: 5| Step: 2
Training loss: 3.9780571460723877
Validation loss: 3.2666146985946165

Epoch: 5| Step: 3
Training loss: 3.7158379554748535
Validation loss: 3.2582418662245556

Epoch: 5| Step: 4
Training loss: 2.672492504119873
Validation loss: 3.2504508956786125

Epoch: 5| Step: 5
Training loss: 3.8145134449005127
Validation loss: 3.2477938052146667

Epoch: 5| Step: 6
Training loss: 2.9913382530212402
Validation loss: 3.2427005408912577

Epoch: 5| Step: 7
Training loss: 3.105710506439209
Validation loss: 3.236773990815686

Epoch: 5| Step: 8
Training loss: 3.3233437538146973
Validation loss: 3.227253242205548

Epoch: 5| Step: 9
Training loss: 2.7165443897247314
Validation loss: 3.2226653252878497

Epoch: 5| Step: 10
Training loss: 3.4396045207977295
Validation loss: 3.22118362047339

Epoch: 11| Step: 0
Training loss: 3.4778695106506348
Validation loss: 3.2143047343018236

Epoch: 5| Step: 1
Training loss: 2.8918747901916504
Validation loss: 3.2087341611103346

Epoch: 5| Step: 2
Training loss: 2.574425458908081
Validation loss: 3.2037166472404235

Epoch: 5| Step: 3
Training loss: 3.4439384937286377
Validation loss: 3.195033901481218

Epoch: 5| Step: 4
Training loss: 2.8382930755615234
Validation loss: 3.188258053154074

Epoch: 5| Step: 5
Training loss: 2.95622181892395
Validation loss: 3.1833310268258534

Epoch: 5| Step: 6
Training loss: 3.9554286003112793
Validation loss: 3.1785167442855013

Epoch: 5| Step: 7
Training loss: 4.116276741027832
Validation loss: 3.1740593500034784

Epoch: 5| Step: 8
Training loss: 3.0252647399902344
Validation loss: 3.1648168358751523

Epoch: 5| Step: 9
Training loss: 2.4812095165252686
Validation loss: 3.1586977974061043

Epoch: 5| Step: 10
Training loss: 3.311887741088867
Validation loss: 3.1505483273536927

Epoch: 12| Step: 0
Training loss: 3.373889207839966
Validation loss: 3.151267502897529

Epoch: 5| Step: 1
Training loss: 2.9315288066864014
Validation loss: 3.14460111946188

Epoch: 5| Step: 2
Training loss: 2.812340259552002
Validation loss: 3.1388381963135092

Epoch: 5| Step: 3
Training loss: 2.8522167205810547
Validation loss: 3.1314644403355096

Epoch: 5| Step: 4
Training loss: 2.545405626296997
Validation loss: 3.1299995196762906

Epoch: 5| Step: 5
Training loss: 3.533447742462158
Validation loss: 3.134264246109993

Epoch: 5| Step: 6
Training loss: 3.480072021484375
Validation loss: 3.117088046125186

Epoch: 5| Step: 7
Training loss: 3.4266655445098877
Validation loss: 3.1159348513490412

Epoch: 5| Step: 8
Training loss: 3.4076359272003174
Validation loss: 3.1121671251071397

Epoch: 5| Step: 9
Training loss: 3.401937484741211
Validation loss: 3.1108010251034974

Epoch: 5| Step: 10
Training loss: 2.7635152339935303
Validation loss: 3.10347641411648

Epoch: 13| Step: 0
Training loss: 4.564536094665527
Validation loss: 3.099019350544099

Epoch: 5| Step: 1
Training loss: 2.847437620162964
Validation loss: 3.086903679755426

Epoch: 5| Step: 2
Training loss: 3.3732542991638184
Validation loss: 3.088363283423967

Epoch: 5| Step: 3
Training loss: 2.6803712844848633
Validation loss: 3.106840597685947

Epoch: 5| Step: 4
Training loss: 2.4015626907348633
Validation loss: 3.083558882436445

Epoch: 5| Step: 5
Training loss: 2.5650107860565186
Validation loss: 3.0739792957100818

Epoch: 5| Step: 6
Training loss: 3.600262403488159
Validation loss: 3.0829985193026963

Epoch: 5| Step: 7
Training loss: 2.9996752738952637
Validation loss: 3.087538852486559

Epoch: 5| Step: 8
Training loss: 3.075599431991577
Validation loss: 3.085356225249588

Epoch: 5| Step: 9
Training loss: 3.2727439403533936
Validation loss: 3.076246469251571

Epoch: 5| Step: 10
Training loss: 2.8896658420562744
Validation loss: 3.0753352411331667

Epoch: 14| Step: 0
Training loss: 3.0873332023620605
Validation loss: 3.0705983997673116

Epoch: 5| Step: 1
Training loss: 3.0159237384796143
Validation loss: 3.0606005319985012

Epoch: 5| Step: 2
Training loss: 3.231398820877075
Validation loss: 3.0523224235862814

Epoch: 5| Step: 3
Training loss: 2.4760310649871826
Validation loss: 3.038534318247149

Epoch: 5| Step: 4
Training loss: 2.931143283843994
Validation loss: 3.0397133596481813

Epoch: 5| Step: 5
Training loss: 3.3868179321289062
Validation loss: 3.064493904831589

Epoch: 5| Step: 6
Training loss: 3.3687636852264404
Validation loss: 3.037983863584457

Epoch: 5| Step: 7
Training loss: 3.3486549854278564
Validation loss: 3.027749774276569

Epoch: 5| Step: 8
Training loss: 2.9915900230407715
Validation loss: 3.0189618628512145

Epoch: 5| Step: 9
Training loss: 3.1127848625183105
Validation loss: 3.020508122700517

Epoch: 5| Step: 10
Training loss: 3.0478627681732178
Validation loss: 3.021327259720013

Epoch: 15| Step: 0
Training loss: 3.491109848022461
Validation loss: 3.0180168177491877

Epoch: 5| Step: 1
Training loss: 3.165013313293457
Validation loss: 3.007173204934725

Epoch: 5| Step: 2
Training loss: 2.7388739585876465
Validation loss: 2.9994694161158737

Epoch: 5| Step: 3
Training loss: 2.67000150680542
Validation loss: 2.994873600621377

Epoch: 5| Step: 4
Training loss: 2.9288830757141113
Validation loss: 2.9935524514926377

Epoch: 5| Step: 5
Training loss: 2.5886917114257812
Validation loss: 2.9961995693945114

Epoch: 5| Step: 6
Training loss: 3.5874781608581543
Validation loss: 2.9871492334591445

Epoch: 5| Step: 7
Training loss: 3.5773873329162598
Validation loss: 2.990512591536327

Epoch: 5| Step: 8
Training loss: 2.9716262817382812
Validation loss: 2.994911181029453

Epoch: 5| Step: 9
Training loss: 2.4613864421844482
Validation loss: 2.99888704669091

Epoch: 5| Step: 10
Training loss: 3.6152706146240234
Validation loss: 3.0006733273947113

Epoch: 16| Step: 0
Training loss: 2.2358665466308594
Validation loss: 2.9907283706049763

Epoch: 5| Step: 1
Training loss: 3.217097520828247
Validation loss: 2.9907621029884583

Epoch: 5| Step: 2
Training loss: 2.5150156021118164
Validation loss: 2.9867257123352378

Epoch: 5| Step: 3
Training loss: 3.2959563732147217
Validation loss: 2.9724550093373945

Epoch: 5| Step: 4
Training loss: 3.294616222381592
Validation loss: 2.961882855302544

Epoch: 5| Step: 5
Training loss: 3.423938274383545
Validation loss: 2.9505291728563208

Epoch: 5| Step: 6
Training loss: 2.8855533599853516
Validation loss: 2.9505633615678355

Epoch: 5| Step: 7
Training loss: 3.3572399616241455
Validation loss: 2.957334364614179

Epoch: 5| Step: 8
Training loss: 2.3229687213897705
Validation loss: 2.9724278757649083

Epoch: 5| Step: 9
Training loss: 2.904470443725586
Validation loss: 3.002670811068627

Epoch: 5| Step: 10
Training loss: 4.259357929229736
Validation loss: 3.0090344875089583

Epoch: 17| Step: 0
Training loss: 2.584477424621582
Validation loss: 2.9317109687353975

Epoch: 5| Step: 1
Training loss: 2.9965710639953613
Validation loss: 2.9336421489715576

Epoch: 5| Step: 2
Training loss: 2.469980239868164
Validation loss: 2.942643157897457

Epoch: 5| Step: 3
Training loss: 2.610724925994873
Validation loss: 2.9546035335909937

Epoch: 5| Step: 4
Training loss: 3.6889045238494873
Validation loss: 2.9451988025378157

Epoch: 5| Step: 5
Training loss: 3.6100144386291504
Validation loss: 2.940252222040648

Epoch: 5| Step: 6
Training loss: 3.614530563354492
Validation loss: 2.9279635901092202

Epoch: 5| Step: 7
Training loss: 2.8147952556610107
Validation loss: 2.9169091332343315

Epoch: 5| Step: 8
Training loss: 3.0143496990203857
Validation loss: 2.913405733723794

Epoch: 5| Step: 9
Training loss: 3.3270485401153564
Validation loss: 2.909418394488673

Epoch: 5| Step: 10
Training loss: 2.3877322673797607
Validation loss: 2.9110646760591896

Epoch: 18| Step: 0
Training loss: 2.392220973968506
Validation loss: 2.9281295550766813

Epoch: 5| Step: 1
Training loss: 2.436570882797241
Validation loss: 2.90628630627868

Epoch: 5| Step: 2
Training loss: 2.717236280441284
Validation loss: 2.8932091446333033

Epoch: 5| Step: 3
Training loss: 3.6293716430664062
Validation loss: 2.89151410389972

Epoch: 5| Step: 4
Training loss: 2.7998013496398926
Validation loss: 2.8920451082209104

Epoch: 5| Step: 5
Training loss: 3.3360679149627686
Validation loss: 2.8934501473621657

Epoch: 5| Step: 6
Training loss: 2.954643726348877
Validation loss: 2.89718391818385

Epoch: 5| Step: 7
Training loss: 3.0973715782165527
Validation loss: 2.8964050123768468

Epoch: 5| Step: 8
Training loss: 3.2102882862091064
Validation loss: 2.8973747709746003

Epoch: 5| Step: 9
Training loss: 3.359715223312378
Validation loss: 2.89167280350962

Epoch: 5| Step: 10
Training loss: 3.0224106311798096
Validation loss: 2.883840989041072

Epoch: 19| Step: 0
Training loss: 3.142683506011963
Validation loss: 2.8708407853239324

Epoch: 5| Step: 1
Training loss: 3.3087291717529297
Validation loss: 2.8725487724427254

Epoch: 5| Step: 2
Training loss: 3.064148426055908
Validation loss: 2.8681577482531146

Epoch: 5| Step: 3
Training loss: 2.3921539783477783
Validation loss: 2.865681109889861

Epoch: 5| Step: 4
Training loss: 2.9831702709198
Validation loss: 2.862285978050642

Epoch: 5| Step: 5
Training loss: 2.660637617111206
Validation loss: 2.8652754419593403

Epoch: 5| Step: 6
Training loss: 2.4585533142089844
Validation loss: 2.857353118158156

Epoch: 5| Step: 7
Training loss: 3.207669734954834
Validation loss: 2.8534241055929535

Epoch: 5| Step: 8
Training loss: 2.8610854148864746
Validation loss: 2.8492377188897904

Epoch: 5| Step: 9
Training loss: 3.429084062576294
Validation loss: 2.845965503364481

Epoch: 5| Step: 10
Training loss: 3.1536269187927246
Validation loss: 2.840020997549898

Epoch: 20| Step: 0
Training loss: 2.8406739234924316
Validation loss: 2.844170690864645

Epoch: 5| Step: 1
Training loss: 2.5627970695495605
Validation loss: 2.8440135960937827

Epoch: 5| Step: 2
Training loss: 3.7885582447052
Validation loss: 2.840529141887542

Epoch: 5| Step: 3
Training loss: 2.932898998260498
Validation loss: 2.8407662427553566

Epoch: 5| Step: 4
Training loss: 3.2526767253875732
Validation loss: 2.830309337185275

Epoch: 5| Step: 5
Training loss: 3.1771903038024902
Validation loss: 2.82679606253101

Epoch: 5| Step: 6
Training loss: 2.521232843399048
Validation loss: 2.822123327562886

Epoch: 5| Step: 7
Training loss: 2.7147278785705566
Validation loss: 2.8256107889195925

Epoch: 5| Step: 8
Training loss: 2.325009822845459
Validation loss: 2.8247489185743433

Epoch: 5| Step: 9
Training loss: 3.150965452194214
Validation loss: 2.831040374694332

Epoch: 5| Step: 10
Training loss: 3.2106220722198486
Validation loss: 2.824053036269321

Epoch: 21| Step: 0
Training loss: 3.1599280834198
Validation loss: 2.8192540881454304

Epoch: 5| Step: 1
Training loss: 2.7876124382019043
Validation loss: 2.8135008453040995

Epoch: 5| Step: 2
Training loss: 2.182375192642212
Validation loss: 2.8102014808244604

Epoch: 5| Step: 3
Training loss: 3.349494218826294
Validation loss: 2.8101553968203965

Epoch: 5| Step: 4
Training loss: 2.859487533569336
Validation loss: 2.8066719296158

Epoch: 5| Step: 5
Training loss: 3.060497760772705
Validation loss: 2.8051649524319555

Epoch: 5| Step: 6
Training loss: 3.1997056007385254
Validation loss: 2.80397532832238

Epoch: 5| Step: 7
Training loss: 3.356732130050659
Validation loss: 2.8139718604344193

Epoch: 5| Step: 8
Training loss: 2.509836435317993
Validation loss: 2.8120444923318844

Epoch: 5| Step: 9
Training loss: 2.749343156814575
Validation loss: 2.798627448338334

Epoch: 5| Step: 10
Training loss: 3.0750772953033447
Validation loss: 2.799143845035184

Epoch: 22| Step: 0
Training loss: 2.5490033626556396
Validation loss: 2.797032315243957

Epoch: 5| Step: 1
Training loss: 3.149998426437378
Validation loss: 2.796842416127523

Epoch: 5| Step: 2
Training loss: 2.551313877105713
Validation loss: 2.7972393497343986

Epoch: 5| Step: 3
Training loss: 3.3156070709228516
Validation loss: 2.79455909421367

Epoch: 5| Step: 4
Training loss: 2.3125851154327393
Validation loss: 2.792844254483459

Epoch: 5| Step: 5
Training loss: 2.8469395637512207
Validation loss: 2.7930968730680403

Epoch: 5| Step: 6
Training loss: 2.860344409942627
Validation loss: 2.79219159003227

Epoch: 5| Step: 7
Training loss: 2.627546787261963
Validation loss: 2.7895009543306086

Epoch: 5| Step: 8
Training loss: 2.9689242839813232
Validation loss: 2.787052433977845

Epoch: 5| Step: 9
Training loss: 3.9112319946289062
Validation loss: 2.7868780282235917

Epoch: 5| Step: 10
Training loss: 3.0944576263427734
Validation loss: 2.783834659925071

Epoch: 23| Step: 0
Training loss: 2.546220302581787
Validation loss: 2.7864833775387017

Epoch: 5| Step: 1
Training loss: 2.532559633255005
Validation loss: 2.784219003492786

Epoch: 5| Step: 2
Training loss: 2.943535327911377
Validation loss: 2.7804110024565007

Epoch: 5| Step: 3
Training loss: 3.207848072052002
Validation loss: 2.778237353089035

Epoch: 5| Step: 4
Training loss: 3.3316001892089844
Validation loss: 2.775315261656238

Epoch: 5| Step: 5
Training loss: 2.998347282409668
Validation loss: 2.7770433246448474

Epoch: 5| Step: 6
Training loss: 2.5220329761505127
Validation loss: 2.776651228627851

Epoch: 5| Step: 7
Training loss: 2.764892578125
Validation loss: 2.7739691657404744

Epoch: 5| Step: 8
Training loss: 3.5102531909942627
Validation loss: 2.7740800688343663

Epoch: 5| Step: 9
Training loss: 2.474494457244873
Validation loss: 2.7727907626859603

Epoch: 5| Step: 10
Training loss: 3.2706172466278076
Validation loss: 2.769897955720143

Epoch: 24| Step: 0
Training loss: 2.821424961090088
Validation loss: 2.76768224470077

Epoch: 5| Step: 1
Training loss: 2.2456321716308594
Validation loss: 2.773525466201126

Epoch: 5| Step: 2
Training loss: 3.6844820976257324
Validation loss: 2.7669603132432505

Epoch: 5| Step: 3
Training loss: 2.5828781127929688
Validation loss: 2.7650984743589997

Epoch: 5| Step: 4
Training loss: 3.18217396736145
Validation loss: 2.759627849824967

Epoch: 5| Step: 5
Training loss: 2.7283828258514404
Validation loss: 2.7587815330874537

Epoch: 5| Step: 6
Training loss: 2.779442071914673
Validation loss: 2.7569411954572125

Epoch: 5| Step: 7
Training loss: 3.042022228240967
Validation loss: 2.7553480543116087

Epoch: 5| Step: 8
Training loss: 2.8911643028259277
Validation loss: 2.754231547796598

Epoch: 5| Step: 9
Training loss: 2.3210203647613525
Validation loss: 2.757706980551443

Epoch: 5| Step: 10
Training loss: 3.7612407207489014
Validation loss: 2.7619398255502023

Epoch: 25| Step: 0
Training loss: 3.002974271774292
Validation loss: 2.7522493716209167

Epoch: 5| Step: 1
Training loss: 3.0119829177856445
Validation loss: 2.747410971631286

Epoch: 5| Step: 2
Training loss: 3.018561840057373
Validation loss: 2.7460461021751486

Epoch: 5| Step: 3
Training loss: 2.5840201377868652
Validation loss: 2.74773504400766

Epoch: 5| Step: 4
Training loss: 3.3188107013702393
Validation loss: 2.7472007838628625

Epoch: 5| Step: 5
Training loss: 2.9939560890197754
Validation loss: 2.7517179955718336

Epoch: 5| Step: 6
Training loss: 2.450634479522705
Validation loss: 2.744476790069252

Epoch: 5| Step: 7
Training loss: 2.7158749103546143
Validation loss: 2.7435697817033335

Epoch: 5| Step: 8
Training loss: 3.3023273944854736
Validation loss: 2.7433804132605113

Epoch: 5| Step: 9
Training loss: 3.1591007709503174
Validation loss: 2.7402222951253257

Epoch: 5| Step: 10
Training loss: 2.1694419384002686
Validation loss: 2.739127012991136

Epoch: 26| Step: 0
Training loss: 2.5249264240264893
Validation loss: 2.742036832276211

Epoch: 5| Step: 1
Training loss: 2.2807090282440186
Validation loss: 2.7390940086815947

Epoch: 5| Step: 2
Training loss: 3.0521607398986816
Validation loss: 2.7400939438932683

Epoch: 5| Step: 3
Training loss: 2.669875144958496
Validation loss: 2.7367029843791837

Epoch: 5| Step: 4
Training loss: 3.3257193565368652
Validation loss: 2.739027584752729

Epoch: 5| Step: 5
Training loss: 3.03629994392395
Validation loss: 2.766390382602651

Epoch: 5| Step: 6
Training loss: 2.287505626678467
Validation loss: 2.7813800150348293

Epoch: 5| Step: 7
Training loss: 2.6546685695648193
Validation loss: 2.7406207694802234

Epoch: 5| Step: 8
Training loss: 3.2900967597961426
Validation loss: 2.726477869095341

Epoch: 5| Step: 9
Training loss: 3.064495086669922
Validation loss: 2.7258450420953895

Epoch: 5| Step: 10
Training loss: 3.7120118141174316
Validation loss: 2.7261860524454424

Epoch: 27| Step: 0
Training loss: 2.8455214500427246
Validation loss: 2.730414495673231

Epoch: 5| Step: 1
Training loss: 2.2408690452575684
Validation loss: 2.725784945231612

Epoch: 5| Step: 2
Training loss: 2.861563205718994
Validation loss: 2.7353113569239134

Epoch: 5| Step: 3
Training loss: 3.269753932952881
Validation loss: 2.729823540615779

Epoch: 5| Step: 4
Training loss: 2.4214816093444824
Validation loss: 2.722174411178917

Epoch: 5| Step: 5
Training loss: 3.3038108348846436
Validation loss: 2.719804697139289

Epoch: 5| Step: 6
Training loss: 3.097093105316162
Validation loss: 2.721101863409883

Epoch: 5| Step: 7
Training loss: 2.9967727661132812
Validation loss: 2.7162085040923087

Epoch: 5| Step: 8
Training loss: 3.5109493732452393
Validation loss: 2.7202764018889396

Epoch: 5| Step: 9
Training loss: 2.4142024517059326
Validation loss: 2.722009233249131

Epoch: 5| Step: 10
Training loss: 2.7323358058929443
Validation loss: 2.7218526614609586

Epoch: 28| Step: 0
Training loss: 3.4854636192321777
Validation loss: 2.7297174597299225

Epoch: 5| Step: 1
Training loss: 2.1537530422210693
Validation loss: 2.7176909395443496

Epoch: 5| Step: 2
Training loss: 2.690328598022461
Validation loss: 2.720023867904499

Epoch: 5| Step: 3
Training loss: 3.585747241973877
Validation loss: 2.715418500284995

Epoch: 5| Step: 4
Training loss: 2.55299711227417
Validation loss: 2.708917804943618

Epoch: 5| Step: 5
Training loss: 3.887960910797119
Validation loss: 2.7092663318880144

Epoch: 5| Step: 6
Training loss: 2.8282837867736816
Validation loss: 2.704446377292756

Epoch: 5| Step: 7
Training loss: 2.6785225868225098
Validation loss: 2.706042851171186

Epoch: 5| Step: 8
Training loss: 2.8301405906677246
Validation loss: 2.7072149143424085

Epoch: 5| Step: 9
Training loss: 2.783151149749756
Validation loss: 2.705883054323094

Epoch: 5| Step: 10
Training loss: 1.9333715438842773
Validation loss: 2.7015888921676146

Epoch: 29| Step: 0
Training loss: 3.5449764728546143
Validation loss: 2.704738768198157

Epoch: 5| Step: 1
Training loss: 3.1726067066192627
Validation loss: 2.70367766452092

Epoch: 5| Step: 2
Training loss: 2.983229160308838
Validation loss: 2.716386077224567

Epoch: 5| Step: 3
Training loss: 2.6471784114837646
Validation loss: 2.709663365476875

Epoch: 5| Step: 4
Training loss: 2.962561845779419
Validation loss: 2.702898807423089

Epoch: 5| Step: 5
Training loss: 2.97229266166687
Validation loss: 2.6979316767825874

Epoch: 5| Step: 6
Training loss: 2.0932374000549316
Validation loss: 2.7046864519837084

Epoch: 5| Step: 7
Training loss: 2.4570231437683105
Validation loss: 2.7076283680495394

Epoch: 5| Step: 8
Training loss: 3.2014472484588623
Validation loss: 2.7099273153530654

Epoch: 5| Step: 9
Training loss: 2.3698461055755615
Validation loss: 2.7021708488464355

Epoch: 5| Step: 10
Training loss: 3.1858208179473877
Validation loss: 2.6944928348705335

Epoch: 30| Step: 0
Training loss: 3.60351824760437
Validation loss: 2.69533868246181

Epoch: 5| Step: 1
Training loss: 2.5790271759033203
Validation loss: 2.704891079215593

Epoch: 5| Step: 2
Training loss: 2.1718640327453613
Validation loss: 2.719701602894773

Epoch: 5| Step: 3
Training loss: 3.055933713912964
Validation loss: 2.7206191708964687

Epoch: 5| Step: 4
Training loss: 2.673292875289917
Validation loss: 2.720468977446197

Epoch: 5| Step: 5
Training loss: 3.4039034843444824
Validation loss: 2.710459509203511

Epoch: 5| Step: 6
Training loss: 2.9412307739257812
Validation loss: 2.707755309279247

Epoch: 5| Step: 7
Training loss: 2.986682176589966
Validation loss: 2.700962479396533

Epoch: 5| Step: 8
Training loss: 2.5235273838043213
Validation loss: 2.705759161262102

Epoch: 5| Step: 9
Training loss: 2.8474528789520264
Validation loss: 2.6949926755761586

Epoch: 5| Step: 10
Training loss: 2.637214422225952
Validation loss: 2.6904238526539137

Epoch: 31| Step: 0
Training loss: 2.4378249645233154
Validation loss: 2.7192868417309177

Epoch: 5| Step: 1
Training loss: 2.714627742767334
Validation loss: 2.7499403902279433

Epoch: 5| Step: 2
Training loss: 2.436630964279175
Validation loss: 2.727748191484841

Epoch: 5| Step: 3
Training loss: 2.537160873413086
Validation loss: 2.7284524427947177

Epoch: 5| Step: 4
Training loss: 3.0978500843048096
Validation loss: 2.720836785531813

Epoch: 5| Step: 5
Training loss: 2.687037467956543
Validation loss: 2.686053750335529

Epoch: 5| Step: 6
Training loss: 3.790637254714966
Validation loss: 2.6912661060210197

Epoch: 5| Step: 7
Training loss: 2.886582136154175
Validation loss: 2.692807630826068

Epoch: 5| Step: 8
Training loss: 3.1467597484588623
Validation loss: 2.6950126014729983

Epoch: 5| Step: 9
Training loss: 3.3341548442840576
Validation loss: 2.6907920324674217

Epoch: 5| Step: 10
Training loss: 2.3555045127868652
Validation loss: 2.6960265969717376

Epoch: 32| Step: 0
Training loss: 2.6714837551116943
Validation loss: 2.6931271809403614

Epoch: 5| Step: 1
Training loss: 3.5747954845428467
Validation loss: 2.7019534777569514

Epoch: 5| Step: 2
Training loss: 3.002995491027832
Validation loss: 2.702226277320616

Epoch: 5| Step: 3
Training loss: 3.048933267593384
Validation loss: 2.7005962069316576

Epoch: 5| Step: 4
Training loss: 2.7776336669921875
Validation loss: 2.6894606749216714

Epoch: 5| Step: 5
Training loss: 2.1327803134918213
Validation loss: 2.679938115099425

Epoch: 5| Step: 6
Training loss: 2.927685499191284
Validation loss: 2.6786718983804025

Epoch: 5| Step: 7
Training loss: 2.7155849933624268
Validation loss: 2.679746345807147

Epoch: 5| Step: 8
Training loss: 3.1827151775360107
Validation loss: 2.7071270122322986

Epoch: 5| Step: 9
Training loss: 2.359361171722412
Validation loss: 2.7173458504420456

Epoch: 5| Step: 10
Training loss: 3.0118660926818848
Validation loss: 2.69481909659601

Epoch: 33| Step: 0
Training loss: 2.735980749130249
Validation loss: 2.6710641563579602

Epoch: 5| Step: 1
Training loss: 3.2579376697540283
Validation loss: 2.6748939816669752

Epoch: 5| Step: 2
Training loss: 3.1779091358184814
Validation loss: 2.678601913554694

Epoch: 5| Step: 3
Training loss: 3.0250449180603027
Validation loss: 2.679781260028962

Epoch: 5| Step: 4
Training loss: 3.0229690074920654
Validation loss: 2.691730753068001

Epoch: 5| Step: 5
Training loss: 3.4089362621307373
Validation loss: 2.6938405344563146

Epoch: 5| Step: 6
Training loss: 2.0588958263397217
Validation loss: 2.6832451589645876

Epoch: 5| Step: 7
Training loss: 2.3970329761505127
Validation loss: 2.677694889806932

Epoch: 5| Step: 8
Training loss: 2.1761627197265625
Validation loss: 2.676680705880606

Epoch: 5| Step: 9
Training loss: 3.5181503295898438
Validation loss: 2.677927719649448

Epoch: 5| Step: 10
Training loss: 2.4663827419281006
Validation loss: 2.6769151943986134

Epoch: 34| Step: 0
Training loss: 3.238412380218506
Validation loss: 2.675568906209802

Epoch: 5| Step: 1
Training loss: 2.661024570465088
Validation loss: 2.667919256353891

Epoch: 5| Step: 2
Training loss: 2.6864547729492188
Validation loss: 2.669897343522759

Epoch: 5| Step: 3
Training loss: 2.693796157836914
Validation loss: 2.6646622816721597

Epoch: 5| Step: 4
Training loss: 2.733708381652832
Validation loss: 2.665379149939424

Epoch: 5| Step: 5
Training loss: 3.1310019493103027
Validation loss: 2.666424914072919

Epoch: 5| Step: 6
Training loss: 2.510221481323242
Validation loss: 2.668190551060502

Epoch: 5| Step: 7
Training loss: 2.8085412979125977
Validation loss: 2.664090315500895

Epoch: 5| Step: 8
Training loss: 2.4164535999298096
Validation loss: 2.663700190923547

Epoch: 5| Step: 9
Training loss: 3.5125980377197266
Validation loss: 2.6611518270225933

Epoch: 5| Step: 10
Training loss: 2.8266830444335938
Validation loss: 2.6644024028572986

Epoch: 35| Step: 0
Training loss: 2.882427930831909
Validation loss: 2.6600944560061217

Epoch: 5| Step: 1
Training loss: 2.6147408485412598
Validation loss: 2.662697635671144

Epoch: 5| Step: 2
Training loss: 3.829467296600342
Validation loss: 2.667461851591705

Epoch: 5| Step: 3
Training loss: 2.4453296661376953
Validation loss: 2.6861379813122492

Epoch: 5| Step: 4
Training loss: 2.1985795497894287
Validation loss: 2.6708805484156453

Epoch: 5| Step: 5
Training loss: 2.6068553924560547
Validation loss: 2.669489524697745

Epoch: 5| Step: 6
Training loss: 3.290546417236328
Validation loss: 2.66194672225624

Epoch: 5| Step: 7
Training loss: 2.8529343605041504
Validation loss: 2.6576453716524187

Epoch: 5| Step: 8
Training loss: 2.9313087463378906
Validation loss: 2.6664619702164845

Epoch: 5| Step: 9
Training loss: 2.7596821784973145
Validation loss: 2.671844418330859

Epoch: 5| Step: 10
Training loss: 2.7045676708221436
Validation loss: 2.6751074919136624

Epoch: 36| Step: 0
Training loss: 1.7643020153045654
Validation loss: 2.6742703991551555

Epoch: 5| Step: 1
Training loss: 2.8058114051818848
Validation loss: 2.66862642124135

Epoch: 5| Step: 2
Training loss: 3.0674660205841064
Validation loss: 2.658309572486467

Epoch: 5| Step: 3
Training loss: 3.1830272674560547
Validation loss: 2.6597817097940752

Epoch: 5| Step: 4
Training loss: 2.931936025619507
Validation loss: 2.654966031351397

Epoch: 5| Step: 5
Training loss: 3.1153080463409424
Validation loss: 2.64806475434252

Epoch: 5| Step: 6
Training loss: 2.7860515117645264
Validation loss: 2.6509206295013428

Epoch: 5| Step: 7
Training loss: 2.7144999504089355
Validation loss: 2.6474640010505595

Epoch: 5| Step: 8
Training loss: 3.20231294631958
Validation loss: 2.6460246475793983

Epoch: 5| Step: 9
Training loss: 2.64428973197937
Validation loss: 2.6424378015661754

Epoch: 5| Step: 10
Training loss: 2.8839211463928223
Validation loss: 2.6482030012274302

Epoch: 37| Step: 0
Training loss: 2.5647621154785156
Validation loss: 2.646460610051309

Epoch: 5| Step: 1
Training loss: 1.9913527965545654
Validation loss: 2.653341747099353

Epoch: 5| Step: 2
Training loss: 3.392179489135742
Validation loss: 2.650601230641847

Epoch: 5| Step: 3
Training loss: 3.242374897003174
Validation loss: 2.648116957756781

Epoch: 5| Step: 4
Training loss: 3.080447196960449
Validation loss: 2.6455743543563353

Epoch: 5| Step: 5
Training loss: 3.379011631011963
Validation loss: 2.642738096175655

Epoch: 5| Step: 6
Training loss: 2.1935482025146484
Validation loss: 2.640965994968209

Epoch: 5| Step: 7
Training loss: 2.8256945610046387
Validation loss: 2.6427559955145723

Epoch: 5| Step: 8
Training loss: 2.6232237815856934
Validation loss: 2.6389074171743085

Epoch: 5| Step: 9
Training loss: 2.4595649242401123
Validation loss: 2.6393167895655476

Epoch: 5| Step: 10
Training loss: 3.350283622741699
Validation loss: 2.6357250521259923

Epoch: 38| Step: 0
Training loss: 2.4547152519226074
Validation loss: 2.6374678663028184

Epoch: 5| Step: 1
Training loss: 2.8638052940368652
Validation loss: 2.6343391223620345

Epoch: 5| Step: 2
Training loss: 2.422826051712036
Validation loss: 2.6415977170390468

Epoch: 5| Step: 3
Training loss: 2.90409779548645
Validation loss: 2.652048208380258

Epoch: 5| Step: 4
Training loss: 3.6817240715026855
Validation loss: 2.641717800530054

Epoch: 5| Step: 5
Training loss: 2.6796116828918457
Validation loss: 2.632141887500722

Epoch: 5| Step: 6
Training loss: 2.990260601043701
Validation loss: 2.631778227385654

Epoch: 5| Step: 7
Training loss: 2.107203483581543
Validation loss: 2.6412891828885643

Epoch: 5| Step: 8
Training loss: 2.6041910648345947
Validation loss: 2.6417562628305085

Epoch: 5| Step: 9
Training loss: 3.5019516944885254
Validation loss: 2.6498747000130276

Epoch: 5| Step: 10
Training loss: 2.7987191677093506
Validation loss: 2.645661420719598

Epoch: 39| Step: 0
Training loss: 3.11143159866333
Validation loss: 2.63922635457849

Epoch: 5| Step: 1
Training loss: 2.550294876098633
Validation loss: 2.634286739492929

Epoch: 5| Step: 2
Training loss: 2.7526001930236816
Validation loss: 2.629122426432948

Epoch: 5| Step: 3
Training loss: 3.18705677986145
Validation loss: 2.6312853444007134

Epoch: 5| Step: 4
Training loss: 2.839651346206665
Validation loss: 2.6352348019999843

Epoch: 5| Step: 5
Training loss: 2.509814739227295
Validation loss: 2.6395781373464935

Epoch: 5| Step: 6
Training loss: 2.6462697982788086
Validation loss: 2.6367003404965965

Epoch: 5| Step: 7
Training loss: 2.630406618118286
Validation loss: 2.6427150387917795

Epoch: 5| Step: 8
Training loss: 2.480663299560547
Validation loss: 2.636453151702881

Epoch: 5| Step: 9
Training loss: 2.99359130859375
Validation loss: 2.630333339014361

Epoch: 5| Step: 10
Training loss: 3.2364418506622314
Validation loss: 2.6276102322404102

Epoch: 40| Step: 0
Training loss: 3.122713565826416
Validation loss: 2.625925281996368

Epoch: 5| Step: 1
Training loss: 2.7738730907440186
Validation loss: 2.6246723718540643

Epoch: 5| Step: 2
Training loss: 3.4376683235168457
Validation loss: 2.629875780433737

Epoch: 5| Step: 3
Training loss: 2.3389229774475098
Validation loss: 2.6330494367948143

Epoch: 5| Step: 4
Training loss: 3.922337293624878
Validation loss: 2.6401371648234706

Epoch: 5| Step: 5
Training loss: 2.0614519119262695
Validation loss: 2.6397363498646724

Epoch: 5| Step: 6
Training loss: 2.6802420616149902
Validation loss: 2.63378567336708

Epoch: 5| Step: 7
Training loss: 3.2572200298309326
Validation loss: 2.630118154710339

Epoch: 5| Step: 8
Training loss: 2.3330867290496826
Validation loss: 2.6342708321027857

Epoch: 5| Step: 9
Training loss: 2.753911256790161
Validation loss: 2.629605867529428

Epoch: 5| Step: 10
Training loss: 1.990264654159546
Validation loss: 2.623785703412948

Epoch: 41| Step: 0
Training loss: 2.254786252975464
Validation loss: 2.6270307956203336

Epoch: 5| Step: 1
Training loss: 3.1260435581207275
Validation loss: 2.63326157036648

Epoch: 5| Step: 2
Training loss: 2.5247902870178223
Validation loss: 2.6323572922778387

Epoch: 5| Step: 3
Training loss: 2.7429118156433105
Validation loss: 2.6315291543160715

Epoch: 5| Step: 4
Training loss: 3.2154147624969482
Validation loss: 2.6254564331423853

Epoch: 5| Step: 5
Training loss: 3.0654244422912598
Validation loss: 2.626849587245654

Epoch: 5| Step: 6
Training loss: 3.116945743560791
Validation loss: 2.6266612391318045

Epoch: 5| Step: 7
Training loss: 2.649042844772339
Validation loss: 2.635122932413573

Epoch: 5| Step: 8
Training loss: 3.2682533264160156
Validation loss: 2.630258729380946

Epoch: 5| Step: 9
Training loss: 2.3750405311584473
Validation loss: 2.6327266898206485

Epoch: 5| Step: 10
Training loss: 2.4568047523498535
Validation loss: 2.6308921819092124

Epoch: 42| Step: 0
Training loss: 2.299079656600952
Validation loss: 2.6363182836963284

Epoch: 5| Step: 1
Training loss: 3.2861199378967285
Validation loss: 2.661545597096925

Epoch: 5| Step: 2
Training loss: 2.7469069957733154
Validation loss: 2.6670145501372633

Epoch: 5| Step: 3
Training loss: 2.480480909347534
Validation loss: 2.6547900322944886

Epoch: 5| Step: 4
Training loss: 2.8898463249206543
Validation loss: 2.656637514791181

Epoch: 5| Step: 5
Training loss: 3.199490547180176
Validation loss: 2.637412278882919

Epoch: 5| Step: 6
Training loss: 1.7795358896255493
Validation loss: 2.6368034142319874

Epoch: 5| Step: 7
Training loss: 3.048389434814453
Validation loss: 2.63278950670714

Epoch: 5| Step: 8
Training loss: 3.1196200847625732
Validation loss: 2.624258633582823

Epoch: 5| Step: 9
Training loss: 2.8902587890625
Validation loss: 2.6214732867415234

Epoch: 5| Step: 10
Training loss: 3.0783374309539795
Validation loss: 2.6274064663917787

Epoch: 43| Step: 0
Training loss: 2.0272302627563477
Validation loss: 2.623610532411965

Epoch: 5| Step: 1
Training loss: 3.0166306495666504
Validation loss: 2.638905817462552

Epoch: 5| Step: 2
Training loss: 2.3841328620910645
Validation loss: 2.651970055795485

Epoch: 5| Step: 3
Training loss: 2.9083542823791504
Validation loss: 2.6320545340097077

Epoch: 5| Step: 4
Training loss: 2.6211400032043457
Validation loss: 2.6220601374103176

Epoch: 5| Step: 5
Training loss: 3.2742209434509277
Validation loss: 2.6232185709861016

Epoch: 5| Step: 6
Training loss: 2.4633736610412598
Validation loss: 2.6181577841440835

Epoch: 5| Step: 7
Training loss: 2.9953770637512207
Validation loss: 2.6156232664662022

Epoch: 5| Step: 8
Training loss: 3.288905382156372
Validation loss: 2.6201252783498457

Epoch: 5| Step: 9
Training loss: 2.5910866260528564
Validation loss: 2.624540777616603

Epoch: 5| Step: 10
Training loss: 3.2581286430358887
Validation loss: 2.6247343196663806

Epoch: 44| Step: 0
Training loss: 2.4230146408081055
Validation loss: 2.629637349036432

Epoch: 5| Step: 1
Training loss: 2.5220069885253906
Validation loss: 2.624687663970455

Epoch: 5| Step: 2
Training loss: 3.7829747200012207
Validation loss: 2.6233401554887013

Epoch: 5| Step: 3
Training loss: 2.79943585395813
Validation loss: 2.622490559854815

Epoch: 5| Step: 4
Training loss: 2.971846103668213
Validation loss: 2.614472976294897

Epoch: 5| Step: 5
Training loss: 2.408564805984497
Validation loss: 2.6182641777940976

Epoch: 5| Step: 6
Training loss: 2.664590358734131
Validation loss: 2.616420620231218

Epoch: 5| Step: 7
Training loss: 2.569511890411377
Validation loss: 2.6127505328065608

Epoch: 5| Step: 8
Training loss: 2.568571090698242
Validation loss: 2.616978096705611

Epoch: 5| Step: 9
Training loss: 3.387209415435791
Validation loss: 2.6144921523268505

Epoch: 5| Step: 10
Training loss: 2.5356686115264893
Validation loss: 2.6144344063215357

Epoch: 45| Step: 0
Training loss: 2.6854355335235596
Validation loss: 2.615714401327154

Epoch: 5| Step: 1
Training loss: 2.9785754680633545
Validation loss: 2.6157054849850234

Epoch: 5| Step: 2
Training loss: 2.505253553390503
Validation loss: 2.6120108173739527

Epoch: 5| Step: 3
Training loss: 2.3927650451660156
Validation loss: 2.6113771648817163

Epoch: 5| Step: 4
Training loss: 3.7709076404571533
Validation loss: 2.6145556434508292

Epoch: 5| Step: 5
Training loss: 3.081800937652588
Validation loss: 2.611275070457048

Epoch: 5| Step: 6
Training loss: 2.5389702320098877
Validation loss: 2.607445416911956

Epoch: 5| Step: 7
Training loss: 2.69958758354187
Validation loss: 2.6073125305996148

Epoch: 5| Step: 8
Training loss: 1.960137128829956
Validation loss: 2.608187078147806

Epoch: 5| Step: 9
Training loss: 3.478666305541992
Validation loss: 2.604824202035063

Epoch: 5| Step: 10
Training loss: 2.3999736309051514
Validation loss: 2.60215449589555

Epoch: 46| Step: 0
Training loss: 2.1310181617736816
Validation loss: 2.6014053924109346

Epoch: 5| Step: 1
Training loss: 2.4600727558135986
Validation loss: 2.60018591983344

Epoch: 5| Step: 2
Training loss: 2.708244800567627
Validation loss: 2.6050252042790896

Epoch: 5| Step: 3
Training loss: 3.189183235168457
Validation loss: 2.6102276745662896

Epoch: 5| Step: 4
Training loss: 2.8525211811065674
Validation loss: 2.6207300821940103

Epoch: 5| Step: 5
Training loss: 2.731980562210083
Validation loss: 2.6485172881874988

Epoch: 5| Step: 6
Training loss: 3.3482322692871094
Validation loss: 2.6668751367958645

Epoch: 5| Step: 7
Training loss: 2.631887435913086
Validation loss: 2.633631947220013

Epoch: 5| Step: 8
Training loss: 2.678849697113037
Validation loss: 2.6037624728295112

Epoch: 5| Step: 9
Training loss: 2.953144073486328
Validation loss: 2.599567451784688

Epoch: 5| Step: 10
Training loss: 2.97169828414917
Validation loss: 2.6006097665397068

Epoch: 47| Step: 0
Training loss: 2.8402392864227295
Validation loss: 2.6090850189167965

Epoch: 5| Step: 1
Training loss: 3.4157238006591797
Validation loss: 2.6134832007910616

Epoch: 5| Step: 2
Training loss: 2.842332124710083
Validation loss: 2.607614712048602

Epoch: 5| Step: 3
Training loss: 2.5730695724487305
Validation loss: 2.6091983036328386

Epoch: 5| Step: 4
Training loss: 2.5378594398498535
Validation loss: 2.609724329363915

Epoch: 5| Step: 5
Training loss: 2.360607862472534
Validation loss: 2.607057340683476

Epoch: 5| Step: 6
Training loss: 3.263744831085205
Validation loss: 2.598794683333366

Epoch: 5| Step: 7
Training loss: 2.1738476753234863
Validation loss: 2.6015652738591677

Epoch: 5| Step: 8
Training loss: 2.2266812324523926
Validation loss: 2.5994708230418544

Epoch: 5| Step: 9
Training loss: 2.960050582885742
Validation loss: 2.6024815472223426

Epoch: 5| Step: 10
Training loss: 3.413348913192749
Validation loss: 2.6034511340561735

Epoch: 48| Step: 0
Training loss: 2.608915328979492
Validation loss: 2.6084054862299273

Epoch: 5| Step: 1
Training loss: 3.1033194065093994
Validation loss: 2.6106387927968013

Epoch: 5| Step: 2
Training loss: 3.279362440109253
Validation loss: 2.6098803320238666

Epoch: 5| Step: 3
Training loss: 2.3886375427246094
Validation loss: 2.611207513399022

Epoch: 5| Step: 4
Training loss: 2.8031177520751953
Validation loss: 2.6080519589044715

Epoch: 5| Step: 5
Training loss: 2.4719398021698
Validation loss: 2.6066895992525163

Epoch: 5| Step: 6
Training loss: 2.767575263977051
Validation loss: 2.5955333427716325

Epoch: 5| Step: 7
Training loss: 3.129373550415039
Validation loss: 2.5906573546830045

Epoch: 5| Step: 8
Training loss: 2.6287479400634766
Validation loss: 2.592216996736424

Epoch: 5| Step: 9
Training loss: 2.530160903930664
Validation loss: 2.5889740400416876

Epoch: 5| Step: 10
Training loss: 2.64493989944458
Validation loss: 2.589066264449909

Epoch: 49| Step: 0
Training loss: 2.2121357917785645
Validation loss: 2.5845356141367266

Epoch: 5| Step: 1
Training loss: 3.3136050701141357
Validation loss: 2.5874603384284565

Epoch: 5| Step: 2
Training loss: 2.680816173553467
Validation loss: 2.5861439243439706

Epoch: 5| Step: 3
Training loss: 2.6236979961395264
Validation loss: 2.588793455913503

Epoch: 5| Step: 4
Training loss: 3.593726396560669
Validation loss: 2.5945799709648214

Epoch: 5| Step: 5
Training loss: 2.2872891426086426
Validation loss: 2.602915189599478

Epoch: 5| Step: 6
Training loss: 2.5999622344970703
Validation loss: 2.6072617089876564

Epoch: 5| Step: 7
Training loss: 2.5271198749542236
Validation loss: 2.6121458776535524

Epoch: 5| Step: 8
Training loss: 2.434077262878418
Validation loss: 2.6087594134833223

Epoch: 5| Step: 9
Training loss: 2.916485071182251
Validation loss: 2.6086739929773475

Epoch: 5| Step: 10
Training loss: 3.193115234375
Validation loss: 2.590436799551851

Epoch: 50| Step: 0
Training loss: 2.407825469970703
Validation loss: 2.5780868325182187

Epoch: 5| Step: 1
Training loss: 3.75190806388855
Validation loss: 2.5841918760730374

Epoch: 5| Step: 2
Training loss: 2.8693366050720215
Validation loss: 2.5872522912999636

Epoch: 5| Step: 3
Training loss: 3.0212924480438232
Validation loss: 2.593952061027609

Epoch: 5| Step: 4
Training loss: 2.3738181591033936
Validation loss: 2.595843965007413

Epoch: 5| Step: 5
Training loss: 2.4932072162628174
Validation loss: 2.6007805460242817

Epoch: 5| Step: 6
Training loss: 2.7436561584472656
Validation loss: 2.610429166465677

Epoch: 5| Step: 7
Training loss: 3.1641581058502197
Validation loss: 2.596131001749346

Epoch: 5| Step: 8
Training loss: 2.255025625228882
Validation loss: 2.591642713034025

Epoch: 5| Step: 9
Training loss: 2.191418409347534
Validation loss: 2.5830279883518013

Epoch: 5| Step: 10
Training loss: 3.112844228744507
Validation loss: 2.5770687749308925

Epoch: 51| Step: 0
Training loss: 3.0744757652282715
Validation loss: 2.5760998636163692

Epoch: 5| Step: 1
Training loss: 2.732339382171631
Validation loss: 2.5691588283866964

Epoch: 5| Step: 2
Training loss: 2.056506872177124
Validation loss: 2.5700182299460135

Epoch: 5| Step: 3
Training loss: 2.7103395462036133
Validation loss: 2.5702822490405013

Epoch: 5| Step: 4
Training loss: 3.136439800262451
Validation loss: 2.5674773698211997

Epoch: 5| Step: 5
Training loss: 3.468410015106201
Validation loss: 2.5684771742872012

Epoch: 5| Step: 6
Training loss: 2.2107837200164795
Validation loss: 2.571852966021466

Epoch: 5| Step: 7
Training loss: 2.8158512115478516
Validation loss: 2.57292192469361

Epoch: 5| Step: 8
Training loss: 2.6636836528778076
Validation loss: 2.5976060923709663

Epoch: 5| Step: 9
Training loss: 3.213141679763794
Validation loss: 2.6222785877925094

Epoch: 5| Step: 10
Training loss: 2.047224998474121
Validation loss: 2.59484435665992

Epoch: 52| Step: 0
Training loss: 2.3580403327941895
Validation loss: 2.5805371628012708

Epoch: 5| Step: 1
Training loss: 2.067460536956787
Validation loss: 2.5697032200392855

Epoch: 5| Step: 2
Training loss: 2.7278194427490234
Validation loss: 2.5660868434495825

Epoch: 5| Step: 3
Training loss: 3.3444225788116455
Validation loss: 2.5617400958973873

Epoch: 5| Step: 4
Training loss: 3.158695697784424
Validation loss: 2.5618718670260523

Epoch: 5| Step: 5
Training loss: 3.2457337379455566
Validation loss: 2.5639187930732645

Epoch: 5| Step: 6
Training loss: 2.751401424407959
Validation loss: 2.55951589410023

Epoch: 5| Step: 7
Training loss: 2.2972869873046875
Validation loss: 2.5590441329504854

Epoch: 5| Step: 8
Training loss: 3.135394334793091
Validation loss: 2.5600151195321033

Epoch: 5| Step: 9
Training loss: 3.3142738342285156
Validation loss: 2.554003384805495

Epoch: 5| Step: 10
Training loss: 1.7054758071899414
Validation loss: 2.561260369516188

Epoch: 53| Step: 0
Training loss: 2.513890027999878
Validation loss: 2.554562102081955

Epoch: 5| Step: 1
Training loss: 2.624485492706299
Validation loss: 2.5556770896398895

Epoch: 5| Step: 2
Training loss: 3.1176562309265137
Validation loss: 2.5535215921299432

Epoch: 5| Step: 3
Training loss: 3.4071364402770996
Validation loss: 2.5539202177396385

Epoch: 5| Step: 4
Training loss: 2.9552183151245117
Validation loss: 2.554926936344434

Epoch: 5| Step: 5
Training loss: 2.2745959758758545
Validation loss: 2.5590274257044636

Epoch: 5| Step: 6
Training loss: 2.9640095233917236
Validation loss: 2.574521480068084

Epoch: 5| Step: 7
Training loss: 2.7963130474090576
Validation loss: 2.576801420539938

Epoch: 5| Step: 8
Training loss: 2.513665199279785
Validation loss: 2.569723249763571

Epoch: 5| Step: 9
Training loss: 2.000671863555908
Validation loss: 2.5660588408029206

Epoch: 5| Step: 10
Training loss: 2.966041088104248
Validation loss: 2.5587228011059504

Epoch: 54| Step: 0
Training loss: 2.8281147480010986
Validation loss: 2.5631588658978863

Epoch: 5| Step: 1
Training loss: 2.2954723834991455
Validation loss: 2.5554811005951255

Epoch: 5| Step: 2
Training loss: 2.4794235229492188
Validation loss: 2.5548391906164025

Epoch: 5| Step: 3
Training loss: 2.8988242149353027
Validation loss: 2.5530806587588404

Epoch: 5| Step: 4
Training loss: 2.9104504585266113
Validation loss: 2.548401804380519

Epoch: 5| Step: 5
Training loss: 2.7874691486358643
Validation loss: 2.54646574553623

Epoch: 5| Step: 6
Training loss: 2.850809335708618
Validation loss: 2.548980833381735

Epoch: 5| Step: 7
Training loss: 2.5102925300598145
Validation loss: 2.5473036945507093

Epoch: 5| Step: 8
Training loss: 2.45432710647583
Validation loss: 2.544200033269903

Epoch: 5| Step: 9
Training loss: 2.6050219535827637
Validation loss: 2.546107074265839

Epoch: 5| Step: 10
Training loss: 3.5007591247558594
Validation loss: 2.5540921893171085

Epoch: 55| Step: 0
Training loss: 2.0392868518829346
Validation loss: 2.566026637631078

Epoch: 5| Step: 1
Training loss: 3.2476654052734375
Validation loss: 2.5643245814948954

Epoch: 5| Step: 2
Training loss: 2.72257661819458
Validation loss: 2.5585004719354774

Epoch: 5| Step: 3
Training loss: 2.799215316772461
Validation loss: 2.551105853049986

Epoch: 5| Step: 4
Training loss: 2.8248207569122314
Validation loss: 2.547446317570184

Epoch: 5| Step: 5
Training loss: 1.807997465133667
Validation loss: 2.5521709611338954

Epoch: 5| Step: 6
Training loss: 2.4253134727478027
Validation loss: 2.5512357604119087

Epoch: 5| Step: 7
Training loss: 3.2357795238494873
Validation loss: 2.5564090231413483

Epoch: 5| Step: 8
Training loss: 2.7800793647766113
Validation loss: 2.5613625331591536

Epoch: 5| Step: 9
Training loss: 3.304917097091675
Validation loss: 2.5684824835869575

Epoch: 5| Step: 10
Training loss: 2.9716687202453613
Validation loss: 2.5642031418379916

Epoch: 56| Step: 0
Training loss: 2.3505642414093018
Validation loss: 2.565563968432847

Epoch: 5| Step: 1
Training loss: 2.6027863025665283
Validation loss: 2.5575366327839513

Epoch: 5| Step: 2
Training loss: 2.3431215286254883
Validation loss: 2.551232403324496

Epoch: 5| Step: 3
Training loss: 2.976191759109497
Validation loss: 2.5551013587623514

Epoch: 5| Step: 4
Training loss: 2.6673896312713623
Validation loss: 2.5528332853829987

Epoch: 5| Step: 5
Training loss: 3.541576862335205
Validation loss: 2.5537474591244935

Epoch: 5| Step: 6
Training loss: 2.6993865966796875
Validation loss: 2.5456151039369646

Epoch: 5| Step: 7
Training loss: 3.1873116493225098
Validation loss: 2.543903386721047

Epoch: 5| Step: 8
Training loss: 2.047281503677368
Validation loss: 2.5497715370629424

Epoch: 5| Step: 9
Training loss: 2.553417682647705
Validation loss: 2.5552738276861047

Epoch: 5| Step: 10
Training loss: 3.0739269256591797
Validation loss: 2.553953896286667

Epoch: 57| Step: 0
Training loss: 2.367224931716919
Validation loss: 2.5587772605239705

Epoch: 5| Step: 1
Training loss: 2.4699695110321045
Validation loss: 2.561056988213652

Epoch: 5| Step: 2
Training loss: 2.192560911178589
Validation loss: 2.5777777728214057

Epoch: 5| Step: 3
Training loss: 2.9554507732391357
Validation loss: 2.585530250303207

Epoch: 5| Step: 4
Training loss: 3.605203628540039
Validation loss: 2.5793426985381753

Epoch: 5| Step: 5
Training loss: 2.5317559242248535
Validation loss: 2.5669968012840516

Epoch: 5| Step: 6
Training loss: 2.8624377250671387
Validation loss: 2.5531625773317073

Epoch: 5| Step: 7
Training loss: 2.4568192958831787
Validation loss: 2.5424450725637455

Epoch: 5| Step: 8
Training loss: 2.726733446121216
Validation loss: 2.536407498903172

Epoch: 5| Step: 9
Training loss: 2.9213881492614746
Validation loss: 2.5409074291106193

Epoch: 5| Step: 10
Training loss: 2.928558111190796
Validation loss: 2.543917966145341

Epoch: 58| Step: 0
Training loss: 2.4946060180664062
Validation loss: 2.5422991116841636

Epoch: 5| Step: 1
Training loss: 2.838310480117798
Validation loss: 2.5439435282061176

Epoch: 5| Step: 2
Training loss: 2.225254535675049
Validation loss: 2.545396186972177

Epoch: 5| Step: 3
Training loss: 3.3094704151153564
Validation loss: 2.541670112199681

Epoch: 5| Step: 4
Training loss: 3.434256076812744
Validation loss: 2.54316722449436

Epoch: 5| Step: 5
Training loss: 2.1012282371520996
Validation loss: 2.540865008549024

Epoch: 5| Step: 6
Training loss: 3.1260986328125
Validation loss: 2.538398773439469

Epoch: 5| Step: 7
Training loss: 2.5926990509033203
Validation loss: 2.5399560159252537

Epoch: 5| Step: 8
Training loss: 2.758105754852295
Validation loss: 2.5403986105354885

Epoch: 5| Step: 9
Training loss: 2.6751620769500732
Validation loss: 2.541944529420586

Epoch: 5| Step: 10
Training loss: 2.4037132263183594
Validation loss: 2.5366302049288185

Epoch: 59| Step: 0
Training loss: 2.203723430633545
Validation loss: 2.5308112175233903

Epoch: 5| Step: 1
Training loss: 3.4870829582214355
Validation loss: 2.526284181943504

Epoch: 5| Step: 2
Training loss: 2.883171558380127
Validation loss: 2.524736758201353

Epoch: 5| Step: 3
Training loss: 3.1554489135742188
Validation loss: 2.523809676529259

Epoch: 5| Step: 4
Training loss: 2.520129442214966
Validation loss: 2.5234404699776762

Epoch: 5| Step: 5
Training loss: 2.035515546798706
Validation loss: 2.5267097257798716

Epoch: 5| Step: 6
Training loss: 3.478564739227295
Validation loss: 2.528991727418797

Epoch: 5| Step: 7
Training loss: 2.154456615447998
Validation loss: 2.536022368297782

Epoch: 5| Step: 8
Training loss: 2.5248749256134033
Validation loss: 2.5498640973080873

Epoch: 5| Step: 9
Training loss: 2.409618854522705
Validation loss: 2.574368731949919

Epoch: 5| Step: 10
Training loss: 3.162923812866211
Validation loss: 2.5947658400381766

Epoch: 60| Step: 0
Training loss: 2.6633009910583496
Validation loss: 2.5870327590614237

Epoch: 5| Step: 1
Training loss: 3.0987331867218018
Validation loss: 2.55003789419769

Epoch: 5| Step: 2
Training loss: 2.9189233779907227
Validation loss: 2.5329513549804688

Epoch: 5| Step: 3
Training loss: 2.5918240547180176
Validation loss: 2.525903409527194

Epoch: 5| Step: 4
Training loss: 2.441765546798706
Validation loss: 2.523175116508238

Epoch: 5| Step: 5
Training loss: 3.109269380569458
Validation loss: 2.5286700905010266

Epoch: 5| Step: 6
Training loss: 2.619797706604004
Validation loss: 2.538316665157195

Epoch: 5| Step: 7
Training loss: 2.9850845336914062
Validation loss: 2.56344529890245

Epoch: 5| Step: 8
Training loss: 2.4491240978240967
Validation loss: 2.5817931339304936

Epoch: 5| Step: 9
Training loss: 2.5065078735351562
Validation loss: 2.5691289158277613

Epoch: 5| Step: 10
Training loss: 2.8137831687927246
Validation loss: 2.5513654883189867

Epoch: 61| Step: 0
Training loss: 3.076730251312256
Validation loss: 2.528824983104583

Epoch: 5| Step: 1
Training loss: 2.0371601581573486
Validation loss: 2.5272460470917406

Epoch: 5| Step: 2
Training loss: 2.962249994277954
Validation loss: 2.5272354028558217

Epoch: 5| Step: 3
Training loss: 2.611900806427002
Validation loss: 2.5259845743897142

Epoch: 5| Step: 4
Training loss: 3.3897979259490967
Validation loss: 2.5353618873062955

Epoch: 5| Step: 5
Training loss: 2.9935741424560547
Validation loss: 2.5411072648981565

Epoch: 5| Step: 6
Training loss: 2.592872142791748
Validation loss: 2.5402824981238252

Epoch: 5| Step: 7
Training loss: 2.6072263717651367
Validation loss: 2.5367738328954226

Epoch: 5| Step: 8
Training loss: 2.3367042541503906
Validation loss: 2.5281681424827984

Epoch: 5| Step: 9
Training loss: 2.6104917526245117
Validation loss: 2.5194268303532756

Epoch: 5| Step: 10
Training loss: 2.7129833698272705
Validation loss: 2.5172354021380023

Epoch: 62| Step: 0
Training loss: 3.123370409011841
Validation loss: 2.5155853712430565

Epoch: 5| Step: 1
Training loss: 2.3849315643310547
Validation loss: 2.5189442121854393

Epoch: 5| Step: 2
Training loss: 3.2788987159729004
Validation loss: 2.528444492688743

Epoch: 5| Step: 3
Training loss: 2.8467907905578613
Validation loss: 2.530806913170763

Epoch: 5| Step: 4
Training loss: 2.8020758628845215
Validation loss: 2.5335494702862156

Epoch: 5| Step: 5
Training loss: 2.701568603515625
Validation loss: 2.528803156268212

Epoch: 5| Step: 6
Training loss: 2.7712435722351074
Validation loss: 2.528739816399031

Epoch: 5| Step: 7
Training loss: 2.2357311248779297
Validation loss: 2.520718123323174

Epoch: 5| Step: 8
Training loss: 2.520787000656128
Validation loss: 2.5147249442274853

Epoch: 5| Step: 9
Training loss: 2.859581470489502
Validation loss: 2.5139391165907665

Epoch: 5| Step: 10
Training loss: 2.3288660049438477
Validation loss: 2.511187896933607

Epoch: 63| Step: 0
Training loss: 3.856210231781006
Validation loss: 2.5157639954679754

Epoch: 5| Step: 1
Training loss: 2.6931118965148926
Validation loss: 2.518158979313348

Epoch: 5| Step: 2
Training loss: 2.643289089202881
Validation loss: 2.527525048102102

Epoch: 5| Step: 3
Training loss: 2.3191781044006348
Validation loss: 2.5577272702288885

Epoch: 5| Step: 4
Training loss: 2.033663272857666
Validation loss: 2.576017481024547

Epoch: 5| Step: 5
Training loss: 2.8630452156066895
Validation loss: 2.560217918888215

Epoch: 5| Step: 6
Training loss: 2.87831449508667
Validation loss: 2.5422890147855206

Epoch: 5| Step: 7
Training loss: 2.8963356018066406
Validation loss: 2.5337134868867937

Epoch: 5| Step: 8
Training loss: 3.0111403465270996
Validation loss: 2.528210557917113

Epoch: 5| Step: 9
Training loss: 2.8126425743103027
Validation loss: 2.5216420568445677

Epoch: 5| Step: 10
Training loss: 1.7564270496368408
Validation loss: 2.511320150026711

Epoch: 64| Step: 0
Training loss: 2.702375650405884
Validation loss: 2.508441273884107

Epoch: 5| Step: 1
Training loss: 1.8717594146728516
Validation loss: 2.5117575199373308

Epoch: 5| Step: 2
Training loss: 2.803760051727295
Validation loss: 2.5188920497894287

Epoch: 5| Step: 3
Training loss: 2.4050142765045166
Validation loss: 2.5201129298056326

Epoch: 5| Step: 4
Training loss: 2.470432758331299
Validation loss: 2.521069881736591

Epoch: 5| Step: 5
Training loss: 2.289656162261963
Validation loss: 2.526014199820898

Epoch: 5| Step: 6
Training loss: 2.715989589691162
Validation loss: 2.523651638338643

Epoch: 5| Step: 7
Training loss: 3.289356231689453
Validation loss: 2.5233555173361175

Epoch: 5| Step: 8
Training loss: 2.9129960536956787
Validation loss: 2.5195759214380735

Epoch: 5| Step: 9
Training loss: 3.1352524757385254
Validation loss: 2.5159950615257345

Epoch: 5| Step: 10
Training loss: 3.3917815685272217
Validation loss: 2.512129529829948

Epoch: 65| Step: 0
Training loss: 2.2968649864196777
Validation loss: 2.508227330382152

Epoch: 5| Step: 1
Training loss: 3.107978343963623
Validation loss: 2.5050942538886942

Epoch: 5| Step: 2
Training loss: 2.762505292892456
Validation loss: 2.5119313834815897

Epoch: 5| Step: 3
Training loss: 2.221837043762207
Validation loss: 2.558846022493096

Epoch: 5| Step: 4
Training loss: 3.193934679031372
Validation loss: 2.6000029707467682

Epoch: 5| Step: 5
Training loss: 2.6609435081481934
Validation loss: 2.6090089377536567

Epoch: 5| Step: 6
Training loss: 2.622361660003662
Validation loss: 2.5745683588007444

Epoch: 5| Step: 7
Training loss: 2.906569719314575
Validation loss: 2.5463251811201855

Epoch: 5| Step: 8
Training loss: 2.667285203933716
Validation loss: 2.531787118604106

Epoch: 5| Step: 9
Training loss: 2.7698540687561035
Validation loss: 2.5193977253411406

Epoch: 5| Step: 10
Training loss: 2.885620355606079
Validation loss: 2.505598665565573

Epoch: 66| Step: 0
Training loss: 2.0919389724731445
Validation loss: 2.507778829143893

Epoch: 5| Step: 1
Training loss: 2.7177882194519043
Validation loss: 2.5158672153308825

Epoch: 5| Step: 2
Training loss: 2.757103204727173
Validation loss: 2.5181632093203965

Epoch: 5| Step: 3
Training loss: 3.0547704696655273
Validation loss: 2.5328641142896426

Epoch: 5| Step: 4
Training loss: 2.6456587314605713
Validation loss: 2.5367693439606698

Epoch: 5| Step: 5
Training loss: 2.371532917022705
Validation loss: 2.5620460382071872

Epoch: 5| Step: 6
Training loss: 2.780073642730713
Validation loss: 2.5634636032965874

Epoch: 5| Step: 7
Training loss: 2.3920884132385254
Validation loss: 2.548582092408211

Epoch: 5| Step: 8
Training loss: 3.104846954345703
Validation loss: 2.5180846465531217

Epoch: 5| Step: 9
Training loss: 3.0862865447998047
Validation loss: 2.5062739003089165

Epoch: 5| Step: 10
Training loss: 2.9397642612457275
Validation loss: 2.503291360793575

Epoch: 67| Step: 0
Training loss: 2.8968868255615234
Validation loss: 2.505218812214431

Epoch: 5| Step: 1
Training loss: 2.441556692123413
Validation loss: 2.509912283189835

Epoch: 5| Step: 2
Training loss: 2.7781639099121094
Validation loss: 2.514697023617324

Epoch: 5| Step: 3
Training loss: 3.156379222869873
Validation loss: 2.5179359630871843

Epoch: 5| Step: 4
Training loss: 2.6859681606292725
Validation loss: 2.52053681753015

Epoch: 5| Step: 5
Training loss: 1.982150673866272
Validation loss: 2.5192177859685754

Epoch: 5| Step: 6
Training loss: 2.9618911743164062
Validation loss: 2.5279725341386694

Epoch: 5| Step: 7
Training loss: 2.3551313877105713
Validation loss: 2.5249701648630123

Epoch: 5| Step: 8
Training loss: 2.7990760803222656
Validation loss: 2.527722220267019

Epoch: 5| Step: 9
Training loss: 2.8781096935272217
Validation loss: 2.5187653444146596

Epoch: 5| Step: 10
Training loss: 2.755937337875366
Validation loss: 2.516111648210915

Epoch: 68| Step: 0
Training loss: 2.3106682300567627
Validation loss: 2.5156058188407653

Epoch: 5| Step: 1
Training loss: 2.851815938949585
Validation loss: 2.515439848746023

Epoch: 5| Step: 2
Training loss: 2.6333487033843994
Validation loss: 2.5111046273221254

Epoch: 5| Step: 3
Training loss: 2.665271282196045
Validation loss: 2.509606912571897

Epoch: 5| Step: 4
Training loss: 3.2586874961853027
Validation loss: 2.5069843928019204

Epoch: 5| Step: 5
Training loss: 3.6493988037109375
Validation loss: 2.5096556755804245

Epoch: 5| Step: 6
Training loss: 2.1396498680114746
Validation loss: 2.5058079458052114

Epoch: 5| Step: 7
Training loss: 1.5840938091278076
Validation loss: 2.511338649257537

Epoch: 5| Step: 8
Training loss: 3.2263312339782715
Validation loss: 2.50177498273952

Epoch: 5| Step: 9
Training loss: 2.36801815032959
Validation loss: 2.4953479843754924

Epoch: 5| Step: 10
Training loss: 2.99727201461792
Validation loss: 2.4917571980466127

Epoch: 69| Step: 0
Training loss: 2.6896181106567383
Validation loss: 2.486506292896886

Epoch: 5| Step: 1
Training loss: 3.0509755611419678
Validation loss: 2.49226991591915

Epoch: 5| Step: 2
Training loss: 2.8665080070495605
Validation loss: 2.4909799252786944

Epoch: 5| Step: 3
Training loss: 2.27744197845459
Validation loss: 2.492735501258604

Epoch: 5| Step: 4
Training loss: 2.2375152111053467
Validation loss: 2.5007783161696566

Epoch: 5| Step: 5
Training loss: 2.4715936183929443
Validation loss: 2.5131912513445784

Epoch: 5| Step: 6
Training loss: 3.235560178756714
Validation loss: 2.516684386038011

Epoch: 5| Step: 7
Training loss: 2.7060251235961914
Validation loss: 2.5245100144417054

Epoch: 5| Step: 8
Training loss: 3.0691421031951904
Validation loss: 2.5029139775101856

Epoch: 5| Step: 9
Training loss: 2.5840673446655273
Validation loss: 2.4950607771514566

Epoch: 5| Step: 10
Training loss: 2.363591432571411
Validation loss: 2.488519068687193

Epoch: 70| Step: 0
Training loss: 2.35097074508667
Validation loss: 2.4826714761795534

Epoch: 5| Step: 1
Training loss: 2.801879405975342
Validation loss: 2.483590090146629

Epoch: 5| Step: 2
Training loss: 2.920767307281494
Validation loss: 2.4821066523110993

Epoch: 5| Step: 3
Training loss: 2.1107676029205322
Validation loss: 2.481891152679279

Epoch: 5| Step: 4
Training loss: 3.078669786453247
Validation loss: 2.4852403210055445

Epoch: 5| Step: 5
Training loss: 2.3894317150115967
Validation loss: 2.4879107321462324

Epoch: 5| Step: 6
Training loss: 2.794053792953491
Validation loss: 2.4894852535699004

Epoch: 5| Step: 7
Training loss: 2.684729814529419
Validation loss: 2.4984406758380193

Epoch: 5| Step: 8
Training loss: 2.845838785171509
Validation loss: 2.5146743469340826

Epoch: 5| Step: 9
Training loss: 3.2953267097473145
Validation loss: 2.504894684719783

Epoch: 5| Step: 10
Training loss: 2.346822738647461
Validation loss: 2.5077025736531904

Epoch: 71| Step: 0
Training loss: 2.539402484893799
Validation loss: 2.487979708179351

Epoch: 5| Step: 1
Training loss: 3.5333664417266846
Validation loss: 2.4755056006934053

Epoch: 5| Step: 2
Training loss: 3.0663890838623047
Validation loss: 2.47839016555458

Epoch: 5| Step: 3
Training loss: 2.283379077911377
Validation loss: 2.4819519545442317

Epoch: 5| Step: 4
Training loss: 2.2715671062469482
Validation loss: 2.4805420316675657

Epoch: 5| Step: 5
Training loss: 2.2494747638702393
Validation loss: 2.4847788067274195

Epoch: 5| Step: 6
Training loss: 3.287097215652466
Validation loss: 2.486600791254351

Epoch: 5| Step: 7
Training loss: 2.242720127105713
Validation loss: 2.492571851258637

Epoch: 5| Step: 8
Training loss: 2.6084089279174805
Validation loss: 2.486506821006857

Epoch: 5| Step: 9
Training loss: 2.9649415016174316
Validation loss: 2.4835433524142028

Epoch: 5| Step: 10
Training loss: 2.6918880939483643
Validation loss: 2.4891251543516755

Epoch: 72| Step: 0
Training loss: 2.324676990509033
Validation loss: 2.4851920271432526

Epoch: 5| Step: 1
Training loss: 2.4570670127868652
Validation loss: 2.4974765598133044

Epoch: 5| Step: 2
Training loss: 3.130997657775879
Validation loss: 2.518623923742643

Epoch: 5| Step: 3
Training loss: 2.5612809658050537
Validation loss: 2.4976505669214393

Epoch: 5| Step: 4
Training loss: 2.965268135070801
Validation loss: 2.4874808775481356

Epoch: 5| Step: 5
Training loss: 3.4286842346191406
Validation loss: 2.4801140164816253

Epoch: 5| Step: 6
Training loss: 2.6480965614318848
Validation loss: 2.4725186183888423

Epoch: 5| Step: 7
Training loss: 2.432448387145996
Validation loss: 2.472752727488036

Epoch: 5| Step: 8
Training loss: 2.1965060234069824
Validation loss: 2.4765781830715876

Epoch: 5| Step: 9
Training loss: 2.409479856491089
Validation loss: 2.476318661884595

Epoch: 5| Step: 10
Training loss: 2.986982822418213
Validation loss: 2.4758957150161907

Epoch: 73| Step: 0
Training loss: 3.037569522857666
Validation loss: 2.490177595487205

Epoch: 5| Step: 1
Training loss: 2.9098353385925293
Validation loss: 2.4886217501855667

Epoch: 5| Step: 2
Training loss: 2.595367908477783
Validation loss: 2.5062163978494625

Epoch: 5| Step: 3
Training loss: 2.411996364593506
Validation loss: 2.513957877312937

Epoch: 5| Step: 4
Training loss: 2.63531494140625
Validation loss: 2.5064783839769262

Epoch: 5| Step: 5
Training loss: 2.6163432598114014
Validation loss: 2.50014687866293

Epoch: 5| Step: 6
Training loss: 2.6214239597320557
Validation loss: 2.4919350788157475

Epoch: 5| Step: 7
Training loss: 2.4992358684539795
Validation loss: 2.4841375248406523

Epoch: 5| Step: 8
Training loss: 2.3775858879089355
Validation loss: 2.48493097418098

Epoch: 5| Step: 9
Training loss: 2.8606982231140137
Validation loss: 2.478676308867752

Epoch: 5| Step: 10
Training loss: 2.980654716491699
Validation loss: 2.482473001685194

Epoch: 74| Step: 0
Training loss: 2.6459851264953613
Validation loss: 2.4809202917160524

Epoch: 5| Step: 1
Training loss: 2.4909987449645996
Validation loss: 2.477191250811341

Epoch: 5| Step: 2
Training loss: 2.833714723587036
Validation loss: 2.484433530479349

Epoch: 5| Step: 3
Training loss: 2.9365315437316895
Validation loss: 2.471902093579692

Epoch: 5| Step: 4
Training loss: 3.0196313858032227
Validation loss: 2.466711075075211

Epoch: 5| Step: 5
Training loss: 2.212719678878784
Validation loss: 2.465340942464849

Epoch: 5| Step: 6
Training loss: 3.2084717750549316
Validation loss: 2.4662452231171312

Epoch: 5| Step: 7
Training loss: 2.4432735443115234
Validation loss: 2.466564868086128

Epoch: 5| Step: 8
Training loss: 1.9648736715316772
Validation loss: 2.4686135348453315

Epoch: 5| Step: 9
Training loss: 2.8834738731384277
Validation loss: 2.463746711771975

Epoch: 5| Step: 10
Training loss: 2.968153476715088
Validation loss: 2.471493644099082

Epoch: 75| Step: 0
Training loss: 2.4444427490234375
Validation loss: 2.4786613295155187

Epoch: 5| Step: 1
Training loss: 2.6851375102996826
Validation loss: 2.4736682445772233

Epoch: 5| Step: 2
Training loss: 2.398507833480835
Validation loss: 2.489002048328359

Epoch: 5| Step: 3
Training loss: 3.0813090801239014
Validation loss: 2.52868624143703

Epoch: 5| Step: 4
Training loss: 2.270951747894287
Validation loss: 2.5542973805499334

Epoch: 5| Step: 5
Training loss: 1.9890037775039673
Validation loss: 2.5952366090589956

Epoch: 5| Step: 6
Training loss: 3.1587421894073486
Validation loss: 2.5885068037176646

Epoch: 5| Step: 7
Training loss: 2.479279041290283
Validation loss: 2.5880092805431736

Epoch: 5| Step: 8
Training loss: 2.6460893154144287
Validation loss: 2.54717497159076

Epoch: 5| Step: 9
Training loss: 3.6322848796844482
Validation loss: 2.5110811546284664

Epoch: 5| Step: 10
Training loss: 3.041635274887085
Validation loss: 2.482955987735461

Epoch: 76| Step: 0
Training loss: 2.6397721767425537
Validation loss: 2.4643408201074086

Epoch: 5| Step: 1
Training loss: 2.2525014877319336
Validation loss: 2.474339049349549

Epoch: 5| Step: 2
Training loss: 2.867687225341797
Validation loss: 2.4796407068929365

Epoch: 5| Step: 3
Training loss: 2.5552453994750977
Validation loss: 2.4961292077136297

Epoch: 5| Step: 4
Training loss: 2.2653117179870605
Validation loss: 2.503384223548315

Epoch: 5| Step: 5
Training loss: 2.839505910873413
Validation loss: 2.503471005347467

Epoch: 5| Step: 6
Training loss: 3.020966053009033
Validation loss: 2.502713998158773

Epoch: 5| Step: 7
Training loss: 2.578406572341919
Validation loss: 2.4911774486623783

Epoch: 5| Step: 8
Training loss: 3.302184581756592
Validation loss: 2.4814359757208053

Epoch: 5| Step: 9
Training loss: 2.627340793609619
Validation loss: 2.479194889786423

Epoch: 5| Step: 10
Training loss: 2.715344190597534
Validation loss: 2.4672801853508077

Epoch: 77| Step: 0
Training loss: 2.8128509521484375
Validation loss: 2.4678152966242966

Epoch: 5| Step: 1
Training loss: 2.7119956016540527
Validation loss: 2.464029468515868

Epoch: 5| Step: 2
Training loss: 2.6087217330932617
Validation loss: 2.4700605664201962

Epoch: 5| Step: 3
Training loss: 3.0010876655578613
Validation loss: 2.474319581062563

Epoch: 5| Step: 4
Training loss: 2.3697400093078613
Validation loss: 2.4740230293684107

Epoch: 5| Step: 5
Training loss: 2.2770564556121826
Validation loss: 2.4753404714727916

Epoch: 5| Step: 6
Training loss: 2.62787127494812
Validation loss: 2.4939728757386566

Epoch: 5| Step: 7
Training loss: 2.948314666748047
Validation loss: 2.494180363993491

Epoch: 5| Step: 8
Training loss: 3.434537172317505
Validation loss: 2.5114362829474994

Epoch: 5| Step: 9
Training loss: 2.1844730377197266
Validation loss: 2.499920304103564

Epoch: 5| Step: 10
Training loss: 2.4190351963043213
Validation loss: 2.49083532825593

Epoch: 78| Step: 0
Training loss: 1.9092092514038086
Validation loss: 2.4788016401311403

Epoch: 5| Step: 1
Training loss: 3.087327480316162
Validation loss: 2.471370614984984

Epoch: 5| Step: 2
Training loss: 2.093721866607666
Validation loss: 2.4650660919886764

Epoch: 5| Step: 3
Training loss: 2.8239898681640625
Validation loss: 2.464451587328347

Epoch: 5| Step: 4
Training loss: 3.2701194286346436
Validation loss: 2.4603689255252963

Epoch: 5| Step: 5
Training loss: 3.008514165878296
Validation loss: 2.4629015358545447

Epoch: 5| Step: 6
Training loss: 2.648164749145508
Validation loss: 2.4599420973049697

Epoch: 5| Step: 7
Training loss: 3.1147494316101074
Validation loss: 2.465723999084965

Epoch: 5| Step: 8
Training loss: 2.6034271717071533
Validation loss: 2.4616241044895624

Epoch: 5| Step: 9
Training loss: 2.13486909866333
Validation loss: 2.465722894155851

Epoch: 5| Step: 10
Training loss: 2.730316162109375
Validation loss: 2.470004330399216

Epoch: 79| Step: 0
Training loss: 2.4828438758850098
Validation loss: 2.4747546949694232

Epoch: 5| Step: 1
Training loss: 2.652085781097412
Validation loss: 2.465978835218696

Epoch: 5| Step: 2
Training loss: 2.8151214122772217
Validation loss: 2.4656656096058507

Epoch: 5| Step: 3
Training loss: 2.9344937801361084
Validation loss: 2.457241801805394

Epoch: 5| Step: 4
Training loss: 2.7083001136779785
Validation loss: 2.4650509639452864

Epoch: 5| Step: 5
Training loss: 2.8415441513061523
Validation loss: 2.4638521825113604

Epoch: 5| Step: 6
Training loss: 1.71932852268219
Validation loss: 2.4582132652241695

Epoch: 5| Step: 7
Training loss: 2.6049282550811768
Validation loss: 2.4642318474349154

Epoch: 5| Step: 8
Training loss: 2.7181248664855957
Validation loss: 2.463113031079692

Epoch: 5| Step: 9
Training loss: 3.2646281719207764
Validation loss: 2.461705894880397

Epoch: 5| Step: 10
Training loss: 2.677156686782837
Validation loss: 2.460488445015364

Epoch: 80| Step: 0
Training loss: 2.6561813354492188
Validation loss: 2.453187111885317

Epoch: 5| Step: 1
Training loss: 2.9828453063964844
Validation loss: 2.4494183806962866

Epoch: 5| Step: 2
Training loss: 2.793761730194092
Validation loss: 2.4538926360427693

Epoch: 5| Step: 3
Training loss: 3.0891342163085938
Validation loss: 2.450211881309427

Epoch: 5| Step: 4
Training loss: 2.582489252090454
Validation loss: 2.4558823224036925

Epoch: 5| Step: 5
Training loss: 2.7917258739471436
Validation loss: 2.4524343603400776

Epoch: 5| Step: 6
Training loss: 3.075193166732788
Validation loss: 2.45267500159561

Epoch: 5| Step: 7
Training loss: 2.194636821746826
Validation loss: 2.4512887359947286

Epoch: 5| Step: 8
Training loss: 2.725090742111206
Validation loss: 2.4518447537576

Epoch: 5| Step: 9
Training loss: 2.0749709606170654
Validation loss: 2.4482577693077827

Epoch: 5| Step: 10
Training loss: 2.377901077270508
Validation loss: 2.4499177317465506

Epoch: 81| Step: 0
Training loss: 2.074934482574463
Validation loss: 2.4497666564039005

Epoch: 5| Step: 1
Training loss: 2.362126350402832
Validation loss: 2.4527853509431243

Epoch: 5| Step: 2
Training loss: 2.126795530319214
Validation loss: 2.462373912975352

Epoch: 5| Step: 3
Training loss: 3.1973273754119873
Validation loss: 2.4676111872478197

Epoch: 5| Step: 4
Training loss: 3.2634987831115723
Validation loss: 2.4813946831610894

Epoch: 5| Step: 5
Training loss: 2.68833589553833
Validation loss: 2.4750250078016713

Epoch: 5| Step: 6
Training loss: 3.1262402534484863
Validation loss: 2.4694392040211666

Epoch: 5| Step: 7
Training loss: 3.127081871032715
Validation loss: 2.4696351174385316

Epoch: 5| Step: 8
Training loss: 2.1784303188323975
Validation loss: 2.4664560030865412

Epoch: 5| Step: 9
Training loss: 2.9189579486846924
Validation loss: 2.4521820981015443

Epoch: 5| Step: 10
Training loss: 2.2507569789886475
Validation loss: 2.4521784654227634

Epoch: 82| Step: 0
Training loss: 2.5603690147399902
Validation loss: 2.4475341073928343

Epoch: 5| Step: 1
Training loss: 3.446446180343628
Validation loss: 2.4482441179213987

Epoch: 5| Step: 2
Training loss: 2.396851062774658
Validation loss: 2.4484883226374143

Epoch: 5| Step: 3
Training loss: 2.0202085971832275
Validation loss: 2.4502965609232583

Epoch: 5| Step: 4
Training loss: 2.723475694656372
Validation loss: 2.4572310422056463

Epoch: 5| Step: 5
Training loss: 2.406320810317993
Validation loss: 2.4583472026291715

Epoch: 5| Step: 6
Training loss: 2.800682544708252
Validation loss: 2.462545000096803

Epoch: 5| Step: 7
Training loss: 2.3835620880126953
Validation loss: 2.4666811368798696

Epoch: 5| Step: 8
Training loss: 2.197758674621582
Validation loss: 2.4647095869946223

Epoch: 5| Step: 9
Training loss: 3.519860029220581
Validation loss: 2.4638799851940525

Epoch: 5| Step: 10
Training loss: 2.8494646549224854
Validation loss: 2.457947151635283

Epoch: 83| Step: 0
Training loss: 2.543687105178833
Validation loss: 2.453201932291831

Epoch: 5| Step: 1
Training loss: 3.8361239433288574
Validation loss: 2.448296921227568

Epoch: 5| Step: 2
Training loss: 2.9331352710723877
Validation loss: 2.446731792983188

Epoch: 5| Step: 3
Training loss: 2.5937509536743164
Validation loss: 2.447978328633052

Epoch: 5| Step: 4
Training loss: 2.890261173248291
Validation loss: 2.442407715705133

Epoch: 5| Step: 5
Training loss: 2.2996082305908203
Validation loss: 2.443957431342012

Epoch: 5| Step: 6
Training loss: 2.053223133087158
Validation loss: 2.4429699144055768

Epoch: 5| Step: 7
Training loss: 2.9399750232696533
Validation loss: 2.443042770508797

Epoch: 5| Step: 8
Training loss: 2.482907772064209
Validation loss: 2.4424091641620924

Epoch: 5| Step: 9
Training loss: 2.454381227493286
Validation loss: 2.441815032753893

Epoch: 5| Step: 10
Training loss: 2.286447525024414
Validation loss: 2.439956175383701

Epoch: 84| Step: 0
Training loss: 2.8788113594055176
Validation loss: 2.44430979733826

Epoch: 5| Step: 1
Training loss: 2.932138442993164
Validation loss: 2.4516678189718597

Epoch: 5| Step: 2
Training loss: 1.4773849248886108
Validation loss: 2.4526594249151086

Epoch: 5| Step: 3
Training loss: 2.3531100749969482
Validation loss: 2.459785463989422

Epoch: 5| Step: 4
Training loss: 2.775826930999756
Validation loss: 2.460374293788787

Epoch: 5| Step: 5
Training loss: 2.8454794883728027
Validation loss: 2.455263440326978

Epoch: 5| Step: 6
Training loss: 3.132483959197998
Validation loss: 2.454651540325534

Epoch: 5| Step: 7
Training loss: 3.1826062202453613
Validation loss: 2.450172047461233

Epoch: 5| Step: 8
Training loss: 2.6331570148468018
Validation loss: 2.4476780558145173

Epoch: 5| Step: 9
Training loss: 2.373189926147461
Validation loss: 2.44382619473242

Epoch: 5| Step: 10
Training loss: 2.739635944366455
Validation loss: 2.4424741216885146

Epoch: 85| Step: 0
Training loss: 2.7227444648742676
Validation loss: 2.439828098461192

Epoch: 5| Step: 1
Training loss: 2.702918529510498
Validation loss: 2.4378445186922626

Epoch: 5| Step: 2
Training loss: 2.434521198272705
Validation loss: 2.437931876028738

Epoch: 5| Step: 3
Training loss: 3.0014586448669434
Validation loss: 2.4387838558484147

Epoch: 5| Step: 4
Training loss: 2.957017421722412
Validation loss: 2.437368805690478

Epoch: 5| Step: 5
Training loss: 2.359084367752075
Validation loss: 2.433864934470064

Epoch: 5| Step: 6
Training loss: 1.7340500354766846
Validation loss: 2.4384976792079147

Epoch: 5| Step: 7
Training loss: 2.9518072605133057
Validation loss: 2.4356591163143033

Epoch: 5| Step: 8
Training loss: 2.803382158279419
Validation loss: 2.439191533673194

Epoch: 5| Step: 9
Training loss: 2.8509411811828613
Validation loss: 2.4365343996273574

Epoch: 5| Step: 10
Training loss: 2.7382705211639404
Validation loss: 2.4362423061042704

Epoch: 86| Step: 0
Training loss: 2.895688772201538
Validation loss: 2.4389454267358266

Epoch: 5| Step: 1
Training loss: 2.677156925201416
Validation loss: 2.4428314329475485

Epoch: 5| Step: 2
Training loss: 2.8539226055145264
Validation loss: 2.444457342547755

Epoch: 5| Step: 3
Training loss: 2.4727048873901367
Validation loss: 2.4464790436529342

Epoch: 5| Step: 4
Training loss: 2.7837436199188232
Validation loss: 2.453180452828766

Epoch: 5| Step: 5
Training loss: 2.6403517723083496
Validation loss: 2.4402283212190032

Epoch: 5| Step: 6
Training loss: 2.7059621810913086
Validation loss: 2.4421552394026067

Epoch: 5| Step: 7
Training loss: 2.1074352264404297
Validation loss: 2.435761108193346

Epoch: 5| Step: 8
Training loss: 3.1021578311920166
Validation loss: 2.4346661465142363

Epoch: 5| Step: 9
Training loss: 2.557555675506592
Validation loss: 2.4412026533516507

Epoch: 5| Step: 10
Training loss: 2.351872682571411
Validation loss: 2.43592639892332

Epoch: 87| Step: 0
Training loss: 2.58565616607666
Validation loss: 2.444280162934334

Epoch: 5| Step: 1
Training loss: 2.385612726211548
Validation loss: 2.4566372491980113

Epoch: 5| Step: 2
Training loss: 2.8906750679016113
Validation loss: 2.4617550462804814

Epoch: 5| Step: 3
Training loss: 2.489729642868042
Validation loss: 2.483388267537599

Epoch: 5| Step: 4
Training loss: 2.659287691116333
Validation loss: 2.5053278502597602

Epoch: 5| Step: 5
Training loss: 2.7814242839813232
Validation loss: 2.5068274595404185

Epoch: 5| Step: 6
Training loss: 2.508540630340576
Validation loss: 2.4994141991420458

Epoch: 5| Step: 7
Training loss: 2.564049482345581
Validation loss: 2.4918849596413235

Epoch: 5| Step: 8
Training loss: 2.968026638031006
Validation loss: 2.4500877831571843

Epoch: 5| Step: 9
Training loss: 3.024625301361084
Validation loss: 2.434013018044092

Epoch: 5| Step: 10
Training loss: 2.392564058303833
Validation loss: 2.4294701391650784

Epoch: 88| Step: 0
Training loss: 2.6493988037109375
Validation loss: 2.433257287548434

Epoch: 5| Step: 1
Training loss: 2.5668163299560547
Validation loss: 2.4355194773725284

Epoch: 5| Step: 2
Training loss: 2.573880434036255
Validation loss: 2.4361595287117908

Epoch: 5| Step: 3
Training loss: 3.137748956680298
Validation loss: 2.4404063070974042

Epoch: 5| Step: 4
Training loss: 2.5191309452056885
Validation loss: 2.4524687900338122

Epoch: 5| Step: 5
Training loss: 2.786991834640503
Validation loss: 2.450785526665308

Epoch: 5| Step: 6
Training loss: 2.8695883750915527
Validation loss: 2.4461367463552826

Epoch: 5| Step: 7
Training loss: 2.9920945167541504
Validation loss: 2.450424086663031

Epoch: 5| Step: 8
Training loss: 2.1350319385528564
Validation loss: 2.450132539195399

Epoch: 5| Step: 9
Training loss: 2.3992772102355957
Validation loss: 2.4514541395248903

Epoch: 5| Step: 10
Training loss: 2.6382644176483154
Validation loss: 2.453488631915021

Epoch: 89| Step: 0
Training loss: 3.265193462371826
Validation loss: 2.447404407685803

Epoch: 5| Step: 1
Training loss: 2.3550186157226562
Validation loss: 2.444443666806785

Epoch: 5| Step: 2
Training loss: 2.516500473022461
Validation loss: 2.440829292420418

Epoch: 5| Step: 3
Training loss: 3.1223933696746826
Validation loss: 2.435266653696696

Epoch: 5| Step: 4
Training loss: 2.5875816345214844
Validation loss: 2.4518288258583314

Epoch: 5| Step: 5
Training loss: 2.9310600757598877
Validation loss: 2.4490315324516705

Epoch: 5| Step: 6
Training loss: 2.912008762359619
Validation loss: 2.4465445805621404

Epoch: 5| Step: 7
Training loss: 2.4726855754852295
Validation loss: 2.4372608328378327

Epoch: 5| Step: 8
Training loss: 2.1294105052948
Validation loss: 2.42834722867576

Epoch: 5| Step: 9
Training loss: 2.646209716796875
Validation loss: 2.4312138326706423

Epoch: 5| Step: 10
Training loss: 2.1900460720062256
Validation loss: 2.429122068548715

Epoch: 90| Step: 0
Training loss: 2.9959638118743896
Validation loss: 2.4231725303075646

Epoch: 5| Step: 1
Training loss: 2.6951441764831543
Validation loss: 2.427045786252586

Epoch: 5| Step: 2
Training loss: 2.54323410987854
Validation loss: 2.4257969035897204

Epoch: 5| Step: 3
Training loss: 2.693180561065674
Validation loss: 2.4273623804892264

Epoch: 5| Step: 4
Training loss: 2.497480869293213
Validation loss: 2.4209625926069034

Epoch: 5| Step: 5
Training loss: 3.1597304344177246
Validation loss: 2.426363173351493

Epoch: 5| Step: 6
Training loss: 2.6171152591705322
Validation loss: 2.4239092924261607

Epoch: 5| Step: 7
Training loss: 2.2633492946624756
Validation loss: 2.4387848518228017

Epoch: 5| Step: 8
Training loss: 3.231402635574341
Validation loss: 2.459682887600314

Epoch: 5| Step: 9
Training loss: 2.1475818157196045
Validation loss: 2.4764950288239347

Epoch: 5| Step: 10
Training loss: 2.340977907180786
Validation loss: 2.4698622457442747

Epoch: 91| Step: 0
Training loss: 2.280571222305298
Validation loss: 2.462234402215609

Epoch: 5| Step: 1
Training loss: 2.316169261932373
Validation loss: 2.4600458042595976

Epoch: 5| Step: 2
Training loss: 3.0981972217559814
Validation loss: 2.4589205890573482

Epoch: 5| Step: 3
Training loss: 3.3506178855895996
Validation loss: 2.449409930936752

Epoch: 5| Step: 4
Training loss: 2.584130048751831
Validation loss: 2.438204549974011

Epoch: 5| Step: 5
Training loss: 2.1652939319610596
Validation loss: 2.4218390295582433

Epoch: 5| Step: 6
Training loss: 2.4032645225524902
Validation loss: 2.418351568201537

Epoch: 5| Step: 7
Training loss: 3.0801002979278564
Validation loss: 2.416753715084445

Epoch: 5| Step: 8
Training loss: 3.091552257537842
Validation loss: 2.419792047110937

Epoch: 5| Step: 9
Training loss: 1.683119535446167
Validation loss: 2.42007456287261

Epoch: 5| Step: 10
Training loss: 3.287630558013916
Validation loss: 2.4224362270806425

Epoch: 92| Step: 0
Training loss: 2.1009857654571533
Validation loss: 2.421946753737747

Epoch: 5| Step: 1
Training loss: 2.2345755100250244
Validation loss: 2.4261470994641705

Epoch: 5| Step: 2
Training loss: 3.4382147789001465
Validation loss: 2.4262414491304787

Epoch: 5| Step: 3
Training loss: 2.939413547515869
Validation loss: 2.4292462589920207

Epoch: 5| Step: 4
Training loss: 2.498358964920044
Validation loss: 2.444828633339174

Epoch: 5| Step: 5
Training loss: 2.8709475994110107
Validation loss: 2.447540219112109

Epoch: 5| Step: 6
Training loss: 2.359443187713623
Validation loss: 2.4387453807297574

Epoch: 5| Step: 7
Training loss: 2.2988858222961426
Validation loss: 2.4484420668694282

Epoch: 5| Step: 8
Training loss: 2.6481385231018066
Validation loss: 2.4514166514078775

Epoch: 5| Step: 9
Training loss: 2.870082378387451
Validation loss: 2.4409433859650806

Epoch: 5| Step: 10
Training loss: 2.873739719390869
Validation loss: 2.4301029148922173

Epoch: 93| Step: 0
Training loss: 2.5665409564971924
Validation loss: 2.4366309155700026

Epoch: 5| Step: 1
Training loss: 2.203062057495117
Validation loss: 2.430248961653761

Epoch: 5| Step: 2
Training loss: 2.2755911350250244
Validation loss: 2.4297968315821823

Epoch: 5| Step: 3
Training loss: 2.54814076423645
Validation loss: 2.4339843155235372

Epoch: 5| Step: 4
Training loss: 3.296388626098633
Validation loss: 2.4258946244434645

Epoch: 5| Step: 5
Training loss: 2.577927350997925
Validation loss: 2.418093583917105

Epoch: 5| Step: 6
Training loss: 2.7874419689178467
Validation loss: 2.41767184452344

Epoch: 5| Step: 7
Training loss: 2.586690902709961
Validation loss: 2.4133063593218402

Epoch: 5| Step: 8
Training loss: 2.5437941551208496
Validation loss: 2.4097641052738314

Epoch: 5| Step: 9
Training loss: 2.646214008331299
Validation loss: 2.4112056968032674

Epoch: 5| Step: 10
Training loss: 3.145768404006958
Validation loss: 2.409988933993924

Epoch: 94| Step: 0
Training loss: 2.825735569000244
Validation loss: 2.4084894759680635

Epoch: 5| Step: 1
Training loss: 2.591942310333252
Validation loss: 2.410070585948165

Epoch: 5| Step: 2
Training loss: 2.380713939666748
Validation loss: 2.406826456387838

Epoch: 5| Step: 3
Training loss: 2.3188412189483643
Validation loss: 2.407855554293561

Epoch: 5| Step: 4
Training loss: 1.8828575611114502
Validation loss: 2.4092979713152816

Epoch: 5| Step: 5
Training loss: 2.6264452934265137
Validation loss: 2.4082821825499177

Epoch: 5| Step: 6
Training loss: 3.4599242210388184
Validation loss: 2.4094232525876773

Epoch: 5| Step: 7
Training loss: 3.1456260681152344
Validation loss: 2.412754063965172

Epoch: 5| Step: 8
Training loss: 2.8010616302490234
Validation loss: 2.4164523950187107

Epoch: 5| Step: 9
Training loss: 2.9523630142211914
Validation loss: 2.424859090517926

Epoch: 5| Step: 10
Training loss: 2.0601062774658203
Validation loss: 2.4192460967648413

Epoch: 95| Step: 0
Training loss: 2.4272732734680176
Validation loss: 2.42678411545292

Epoch: 5| Step: 1
Training loss: 2.931349039077759
Validation loss: 2.4268962080760668

Epoch: 5| Step: 2
Training loss: 3.1094446182250977
Validation loss: 2.419392893391271

Epoch: 5| Step: 3
Training loss: 2.263780117034912
Validation loss: 2.405814714329217

Epoch: 5| Step: 4
Training loss: 3.213648557662964
Validation loss: 2.4099111069915113

Epoch: 5| Step: 5
Training loss: 2.64896821975708
Validation loss: 2.406933702448363

Epoch: 5| Step: 6
Training loss: 2.408480644226074
Validation loss: 2.403951729497602

Epoch: 5| Step: 7
Training loss: 2.7813973426818848
Validation loss: 2.400325629018968

Epoch: 5| Step: 8
Training loss: 2.6981308460235596
Validation loss: 2.4039685213437645

Epoch: 5| Step: 9
Training loss: 2.3798985481262207
Validation loss: 2.40397084272036

Epoch: 5| Step: 10
Training loss: 2.168562650680542
Validation loss: 2.4103564011153353

Epoch: 96| Step: 0
Training loss: 3.121803045272827
Validation loss: 2.4132246842948337

Epoch: 5| Step: 1
Training loss: 3.303861618041992
Validation loss: 2.4189109494609218

Epoch: 5| Step: 2
Training loss: 2.381972551345825
Validation loss: 2.4214714598912064

Epoch: 5| Step: 3
Training loss: 2.4681124687194824
Validation loss: 2.4175612644482682

Epoch: 5| Step: 4
Training loss: 3.3709616661071777
Validation loss: 2.419606408765239

Epoch: 5| Step: 5
Training loss: 1.809002161026001
Validation loss: 2.4252882747239966

Epoch: 5| Step: 6
Training loss: 2.3231289386749268
Validation loss: 2.423990975144089

Epoch: 5| Step: 7
Training loss: 3.452255964279175
Validation loss: 2.4226455534658125

Epoch: 5| Step: 8
Training loss: 2.474316120147705
Validation loss: 2.426736001045473

Epoch: 5| Step: 9
Training loss: 2.3131089210510254
Validation loss: 2.4175016187852427

Epoch: 5| Step: 10
Training loss: 1.8193771839141846
Validation loss: 2.412181385101811

Epoch: 97| Step: 0
Training loss: 2.776712417602539
Validation loss: 2.42389815597124

Epoch: 5| Step: 1
Training loss: 2.8000648021698
Validation loss: 2.450965401946857

Epoch: 5| Step: 2
Training loss: 2.997248649597168
Validation loss: 2.447959662765585

Epoch: 5| Step: 3
Training loss: 2.27445125579834
Validation loss: 2.434801047848117

Epoch: 5| Step: 4
Training loss: 3.107482433319092
Validation loss: 2.424400714135939

Epoch: 5| Step: 5
Training loss: 2.942563533782959
Validation loss: 2.412367187520509

Epoch: 5| Step: 6
Training loss: 2.247877597808838
Validation loss: 2.405771609275572

Epoch: 5| Step: 7
Training loss: 2.3536648750305176
Validation loss: 2.4020780735118414

Epoch: 5| Step: 8
Training loss: 2.4860920906066895
Validation loss: 2.4047484397888184

Epoch: 5| Step: 9
Training loss: 2.64670991897583
Validation loss: 2.4012580071726153

Epoch: 5| Step: 10
Training loss: 2.375969648361206
Validation loss: 2.403154137314007

Epoch: 98| Step: 0
Training loss: 2.8977608680725098
Validation loss: 2.401379749339114

Epoch: 5| Step: 1
Training loss: 2.3319220542907715
Validation loss: 2.4074471894130913

Epoch: 5| Step: 2
Training loss: 2.9383797645568848
Validation loss: 2.4075308794616372

Epoch: 5| Step: 3
Training loss: 3.2804653644561768
Validation loss: 2.4195149457582863

Epoch: 5| Step: 4
Training loss: 2.956098794937134
Validation loss: 2.41585438482223

Epoch: 5| Step: 5
Training loss: 2.131305456161499
Validation loss: 2.4142116167212047

Epoch: 5| Step: 6
Training loss: 2.520948886871338
Validation loss: 2.4198986612340456

Epoch: 5| Step: 7
Training loss: 2.047907590866089
Validation loss: 2.417955699787345

Epoch: 5| Step: 8
Training loss: 2.0083096027374268
Validation loss: 2.428744302001051

Epoch: 5| Step: 9
Training loss: 2.8943724632263184
Validation loss: 2.43411192329981

Epoch: 5| Step: 10
Training loss: 3.071633815765381
Validation loss: 2.4395839501452703

Epoch: 99| Step: 0
Training loss: 3.082392454147339
Validation loss: 2.44790974227331

Epoch: 5| Step: 1
Training loss: 2.9007630348205566
Validation loss: 2.437816486563734

Epoch: 5| Step: 2
Training loss: 2.0359320640563965
Validation loss: 2.4334482403211695

Epoch: 5| Step: 3
Training loss: 2.115309238433838
Validation loss: 2.41905540291981

Epoch: 5| Step: 4
Training loss: 2.874850034713745
Validation loss: 2.402869052784417

Epoch: 5| Step: 5
Training loss: 2.4312238693237305
Validation loss: 2.4086105669698408

Epoch: 5| Step: 6
Training loss: 2.5951590538024902
Validation loss: 2.4000035473095473

Epoch: 5| Step: 7
Training loss: 2.978130578994751
Validation loss: 2.3942149659638763

Epoch: 5| Step: 8
Training loss: 2.530961275100708
Validation loss: 2.3954688451623403

Epoch: 5| Step: 9
Training loss: 2.365485668182373
Validation loss: 2.3922123524450485

Epoch: 5| Step: 10
Training loss: 3.116851568222046
Validation loss: 2.390984845417802

Epoch: 100| Step: 0
Training loss: 2.745227813720703
Validation loss: 2.392583700918382

Epoch: 5| Step: 1
Training loss: 2.8091039657592773
Validation loss: 2.394717154964324

Epoch: 5| Step: 2
Training loss: 2.3223531246185303
Validation loss: 2.3948259328001287

Epoch: 5| Step: 3
Training loss: 2.0386757850646973
Validation loss: 2.3933003922944427

Epoch: 5| Step: 4
Training loss: 2.9212894439697266
Validation loss: 2.398057455657631

Epoch: 5| Step: 5
Training loss: 2.6524322032928467
Validation loss: 2.40233789977207

Epoch: 5| Step: 6
Training loss: 2.7250428199768066
Validation loss: 2.4082033377821728

Epoch: 5| Step: 7
Training loss: 2.465524196624756
Validation loss: 2.4125890526720273

Epoch: 5| Step: 8
Training loss: 3.527419328689575
Validation loss: 2.418827118412141

Epoch: 5| Step: 9
Training loss: 2.229294776916504
Validation loss: 2.416757719491118

Epoch: 5| Step: 10
Training loss: 2.5529489517211914
Validation loss: 2.4030130088970227

Epoch: 101| Step: 0
Training loss: 2.5393683910369873
Validation loss: 2.3986965892135457

Epoch: 5| Step: 1
Training loss: 3.499103546142578
Validation loss: 2.392446014188951

Epoch: 5| Step: 2
Training loss: 2.976283311843872
Validation loss: 2.3885626767271306

Epoch: 5| Step: 3
Training loss: 2.3077280521392822
Validation loss: 2.3899459172320623

Epoch: 5| Step: 4
Training loss: 2.927595615386963
Validation loss: 2.3879260222117105

Epoch: 5| Step: 5
Training loss: 1.9288103580474854
Validation loss: 2.392335086740473

Epoch: 5| Step: 6
Training loss: 2.9794223308563232
Validation loss: 2.3911991760294926

Epoch: 5| Step: 7
Training loss: 2.664252519607544
Validation loss: 2.3907363824946906

Epoch: 5| Step: 8
Training loss: 2.228707790374756
Validation loss: 2.3927990313499206

Epoch: 5| Step: 9
Training loss: 1.5255337953567505
Validation loss: 2.395539960553569

Epoch: 5| Step: 10
Training loss: 3.5267584323883057
Validation loss: 2.387960257068757

Epoch: 102| Step: 0
Training loss: 2.471569776535034
Validation loss: 2.3967036226744294

Epoch: 5| Step: 1
Training loss: 2.4020113945007324
Validation loss: 2.4014746604427213

Epoch: 5| Step: 2
Training loss: 2.5456135272979736
Validation loss: 2.421366978717107

Epoch: 5| Step: 3
Training loss: 2.8046088218688965
Validation loss: 2.442485692680523

Epoch: 5| Step: 4
Training loss: 2.6912193298339844
Validation loss: 2.4570499863675845

Epoch: 5| Step: 5
Training loss: 2.618224620819092
Validation loss: 2.453292082714778

Epoch: 5| Step: 6
Training loss: 2.8568503856658936
Validation loss: 2.428168093004534

Epoch: 5| Step: 7
Training loss: 2.3845152854919434
Validation loss: 2.4248879571114816

Epoch: 5| Step: 8
Training loss: 2.972837209701538
Validation loss: 2.406607413804659

Epoch: 5| Step: 9
Training loss: 2.5925586223602295
Validation loss: 2.3972459121416976

Epoch: 5| Step: 10
Training loss: 2.583951950073242
Validation loss: 2.3874924221346454

Epoch: 103| Step: 0
Training loss: 2.3745439052581787
Validation loss: 2.3845355895257767

Epoch: 5| Step: 1
Training loss: 2.458688735961914
Validation loss: 2.3857560516685568

Epoch: 5| Step: 2
Training loss: 2.5558743476867676
Validation loss: 2.3830417202365015

Epoch: 5| Step: 3
Training loss: 3.2978172302246094
Validation loss: 2.3867429405130367

Epoch: 5| Step: 4
Training loss: 2.375474214553833
Validation loss: 2.39265618273007

Epoch: 5| Step: 5
Training loss: 2.1769378185272217
Validation loss: 2.3879071461257113

Epoch: 5| Step: 6
Training loss: 2.870401620864868
Validation loss: 2.384329870182981

Epoch: 5| Step: 7
Training loss: 2.9537224769592285
Validation loss: 2.3814765714829966

Epoch: 5| Step: 8
Training loss: 2.164773464202881
Validation loss: 2.387921133349019

Epoch: 5| Step: 9
Training loss: 2.619746446609497
Validation loss: 2.387620049138223

Epoch: 5| Step: 10
Training loss: 3.2084717750549316
Validation loss: 2.396981918683616

Epoch: 104| Step: 0
Training loss: 2.117992877960205
Validation loss: 2.3996264524357294

Epoch: 5| Step: 1
Training loss: 2.9387314319610596
Validation loss: 2.4022073284272225

Epoch: 5| Step: 2
Training loss: 2.4693427085876465
Validation loss: 2.411337470495573

Epoch: 5| Step: 3
Training loss: 2.9427261352539062
Validation loss: 2.4235356059125674

Epoch: 5| Step: 4
Training loss: 2.7787022590637207
Validation loss: 2.430685043334961

Epoch: 5| Step: 5
Training loss: 2.766977310180664
Validation loss: 2.428279789545203

Epoch: 5| Step: 6
Training loss: 2.706352710723877
Validation loss: 2.4138984526357343

Epoch: 5| Step: 7
Training loss: 2.190871477127075
Validation loss: 2.4088814796939975

Epoch: 5| Step: 8
Training loss: 2.7228732109069824
Validation loss: 2.4061045954304356

Epoch: 5| Step: 9
Training loss: 2.5925936698913574
Validation loss: 2.3994068843062206

Epoch: 5| Step: 10
Training loss: 2.556081771850586
Validation loss: 2.398264040229141

Epoch: 105| Step: 0
Training loss: 2.6011013984680176
Validation loss: 2.404139680247153

Epoch: 5| Step: 1
Training loss: 2.0237298011779785
Validation loss: 2.4089166092616257

Epoch: 5| Step: 2
Training loss: 3.227668046951294
Validation loss: 2.413375977546938

Epoch: 5| Step: 3
Training loss: 2.145064353942871
Validation loss: 2.412915616907099

Epoch: 5| Step: 4
Training loss: 3.3951103687286377
Validation loss: 2.404773086629888

Epoch: 5| Step: 5
Training loss: 2.8547706604003906
Validation loss: 2.402355388928485

Epoch: 5| Step: 6
Training loss: 2.787513017654419
Validation loss: 2.3924831536508377

Epoch: 5| Step: 7
Training loss: 3.103783130645752
Validation loss: 2.386198887261011

Epoch: 5| Step: 8
Training loss: 2.342689037322998
Validation loss: 2.3794747475654847

Epoch: 5| Step: 9
Training loss: 2.3748056888580322
Validation loss: 2.381319930476527

Epoch: 5| Step: 10
Training loss: 1.8579790592193604
Validation loss: 2.374146681959911

Epoch: 106| Step: 0
Training loss: 2.396960735321045
Validation loss: 2.3801923926158617

Epoch: 5| Step: 1
Training loss: 2.644503355026245
Validation loss: 2.3842666636231127

Epoch: 5| Step: 2
Training loss: 2.126258134841919
Validation loss: 2.3882232173796623

Epoch: 5| Step: 3
Training loss: 2.9586181640625
Validation loss: 2.3852565185998076

Epoch: 5| Step: 4
Training loss: 3.126739740371704
Validation loss: 2.386875201297063

Epoch: 5| Step: 5
Training loss: 2.337768077850342
Validation loss: 2.4045995589225524

Epoch: 5| Step: 6
Training loss: 2.847125291824341
Validation loss: 2.411831393036791

Epoch: 5| Step: 7
Training loss: 3.0224392414093018
Validation loss: 2.433164417102773

Epoch: 5| Step: 8
Training loss: 2.173496961593628
Validation loss: 2.444335824699812

Epoch: 5| Step: 9
Training loss: 2.711566686630249
Validation loss: 2.4219142903563795

Epoch: 5| Step: 10
Training loss: 2.405179977416992
Validation loss: 2.400486699996456

Epoch: 107| Step: 0
Training loss: 2.0646042823791504
Validation loss: 2.382295911030103

Epoch: 5| Step: 1
Training loss: 2.5384116172790527
Validation loss: 2.3776210764402985

Epoch: 5| Step: 2
Training loss: 3.285583972930908
Validation loss: 2.3758900985922864

Epoch: 5| Step: 3
Training loss: 2.3541386127471924
Validation loss: 2.371860142677061

Epoch: 5| Step: 4
Training loss: 2.6268856525421143
Validation loss: 2.3705528705350813

Epoch: 5| Step: 5
Training loss: 2.812274694442749
Validation loss: 2.3753961286237164

Epoch: 5| Step: 6
Training loss: 2.3985610008239746
Validation loss: 2.378057213239772

Epoch: 5| Step: 7
Training loss: 2.48996639251709
Validation loss: 2.3783762211440713

Epoch: 5| Step: 8
Training loss: 2.471984386444092
Validation loss: 2.3795278995267806

Epoch: 5| Step: 9
Training loss: 2.8455235958099365
Validation loss: 2.377376050077459

Epoch: 5| Step: 10
Training loss: 3.091249465942383
Validation loss: 2.3755300634650776

Epoch: 108| Step: 0
Training loss: 2.236349582672119
Validation loss: 2.3751303098535024

Epoch: 5| Step: 1
Training loss: 2.0647976398468018
Validation loss: 2.382160376476985

Epoch: 5| Step: 2
Training loss: 2.8219542503356934
Validation loss: 2.3899538055542977

Epoch: 5| Step: 3
Training loss: 2.7490153312683105
Validation loss: 2.398668319948258

Epoch: 5| Step: 4
Training loss: 3.146254539489746
Validation loss: 2.41342403042701

Epoch: 5| Step: 5
Training loss: 2.8805384635925293
Validation loss: 2.4132406839760403

Epoch: 5| Step: 6
Training loss: 2.7036774158477783
Validation loss: 2.4103051847027195

Epoch: 5| Step: 7
Training loss: 3.1736159324645996
Validation loss: 2.406134905353669

Epoch: 5| Step: 8
Training loss: 2.587423801422119
Validation loss: 2.4085309377280613

Epoch: 5| Step: 9
Training loss: 1.5283710956573486
Validation loss: 2.4071573775301696

Epoch: 5| Step: 10
Training loss: 3.0278751850128174
Validation loss: 2.391994140481436

Epoch: 109| Step: 0
Training loss: 3.1968865394592285
Validation loss: 2.3888567493807886

Epoch: 5| Step: 1
Training loss: 1.9880367517471313
Validation loss: 2.3800946461257113

Epoch: 5| Step: 2
Training loss: 2.13909912109375
Validation loss: 2.3754101799380396

Epoch: 5| Step: 3
Training loss: 3.0490458011627197
Validation loss: 2.3747494707825365

Epoch: 5| Step: 4
Training loss: 2.7567481994628906
Validation loss: 2.3770109786782214

Epoch: 5| Step: 5
Training loss: 1.7477047443389893
Validation loss: 2.3729732062226985

Epoch: 5| Step: 6
Training loss: 2.63069486618042
Validation loss: 2.3748167150764057

Epoch: 5| Step: 7
Training loss: 3.4985733032226562
Validation loss: 2.3750993462019068

Epoch: 5| Step: 8
Training loss: 2.8301284313201904
Validation loss: 2.382155969578733

Epoch: 5| Step: 9
Training loss: 2.149660587310791
Validation loss: 2.3920596158632668

Epoch: 5| Step: 10
Training loss: 2.760504961013794
Validation loss: 2.4051679770151773

Epoch: 110| Step: 0
Training loss: 2.3317573070526123
Validation loss: 2.433614512925507

Epoch: 5| Step: 1
Training loss: 2.8294692039489746
Validation loss: 2.4486558334801787

Epoch: 5| Step: 2
Training loss: 2.4435603618621826
Validation loss: 2.4564486319018948

Epoch: 5| Step: 3
Training loss: 2.0281341075897217
Validation loss: 2.47265524248923

Epoch: 5| Step: 4
Training loss: 3.214508056640625
Validation loss: 2.4694694677988687

Epoch: 5| Step: 5
Training loss: 2.8118724822998047
Validation loss: 2.4397049616741877

Epoch: 5| Step: 6
Training loss: 2.2615342140197754
Validation loss: 2.401075909214635

Epoch: 5| Step: 7
Training loss: 3.098825693130493
Validation loss: 2.3862910219418105

Epoch: 5| Step: 8
Training loss: 2.534536838531494
Validation loss: 2.3942092592998216

Epoch: 5| Step: 9
Training loss: 2.552511692047119
Validation loss: 2.4095383228794223

Epoch: 5| Step: 10
Training loss: 2.888406991958618
Validation loss: 2.426648760354647

Epoch: 111| Step: 0
Training loss: 2.82456111907959
Validation loss: 2.4540391455414476

Epoch: 5| Step: 1
Training loss: 2.733154773712158
Validation loss: 2.4539758107995473

Epoch: 5| Step: 2
Training loss: 2.4728288650512695
Validation loss: 2.4255823986504668

Epoch: 5| Step: 3
Training loss: 2.4257564544677734
Validation loss: 2.3997338228328253

Epoch: 5| Step: 4
Training loss: 2.7361645698547363
Validation loss: 2.394040646091584

Epoch: 5| Step: 5
Training loss: 2.7082412242889404
Validation loss: 2.3864207652307328

Epoch: 5| Step: 6
Training loss: 2.445570468902588
Validation loss: 2.3796968126809723

Epoch: 5| Step: 7
Training loss: 2.5137977600097656
Validation loss: 2.3767951483367593

Epoch: 5| Step: 8
Training loss: 2.385962963104248
Validation loss: 2.3770341104076755

Epoch: 5| Step: 9
Training loss: 2.7065939903259277
Validation loss: 2.3851089041720153

Epoch: 5| Step: 10
Training loss: 2.837722063064575
Validation loss: 2.402804784877326

Epoch: 112| Step: 0
Training loss: 3.1348071098327637
Validation loss: 2.4452977359935804

Epoch: 5| Step: 1
Training loss: 2.68481707572937
Validation loss: 2.4758590370096187

Epoch: 5| Step: 2
Training loss: 2.2641470432281494
Validation loss: 2.486767333040955

Epoch: 5| Step: 3
Training loss: 2.027322292327881
Validation loss: 2.484492599323232

Epoch: 5| Step: 4
Training loss: 3.150862216949463
Validation loss: 2.4938662462337042

Epoch: 5| Step: 5
Training loss: 2.9379467964172363
Validation loss: 2.4673131435148177

Epoch: 5| Step: 6
Training loss: 2.8920249938964844
Validation loss: 2.4272825025743052

Epoch: 5| Step: 7
Training loss: 2.2759149074554443
Validation loss: 2.3808565524316605

Epoch: 5| Step: 8
Training loss: 2.83982253074646
Validation loss: 2.3615642721934984

Epoch: 5| Step: 9
Training loss: 2.437377452850342
Validation loss: 2.3573020017275246

Epoch: 5| Step: 10
Training loss: 2.1352250576019287
Validation loss: 2.3565650037539903

Epoch: 113| Step: 0
Training loss: 3.059126615524292
Validation loss: 2.358725660590715

Epoch: 5| Step: 1
Training loss: 2.7085065841674805
Validation loss: 2.3595085246588594

Epoch: 5| Step: 2
Training loss: 3.0557351112365723
Validation loss: 2.3671293553485664

Epoch: 5| Step: 3
Training loss: 2.8311526775360107
Validation loss: 2.3754414794265584

Epoch: 5| Step: 4
Training loss: 2.924191474914551
Validation loss: 2.368827304532451

Epoch: 5| Step: 5
Training loss: 2.043114185333252
Validation loss: 2.3695838861568

Epoch: 5| Step: 6
Training loss: 2.112510919570923
Validation loss: 2.364440882077781

Epoch: 5| Step: 7
Training loss: 2.442140579223633
Validation loss: 2.3637667573908323

Epoch: 5| Step: 8
Training loss: 2.485640048980713
Validation loss: 2.3606334963152484

Epoch: 5| Step: 9
Training loss: 2.8889389038085938
Validation loss: 2.363535970769903

Epoch: 5| Step: 10
Training loss: 2.2558939456939697
Validation loss: 2.3755949594641246

Epoch: 114| Step: 0
Training loss: 2.431497097015381
Validation loss: 2.388232966904999

Epoch: 5| Step: 1
Training loss: 2.1268627643585205
Validation loss: 2.3958645123307423

Epoch: 5| Step: 2
Training loss: 2.8417632579803467
Validation loss: 2.4283292934458744

Epoch: 5| Step: 3
Training loss: 2.606830358505249
Validation loss: 2.446542263031006

Epoch: 5| Step: 4
Training loss: 2.840064525604248
Validation loss: 2.448023565353886

Epoch: 5| Step: 5
Training loss: 1.9762985706329346
Validation loss: 2.4171364025403093

Epoch: 5| Step: 6
Training loss: 3.4035465717315674
Validation loss: 2.3769193849255963

Epoch: 5| Step: 7
Training loss: 2.341986656188965
Validation loss: 2.3584808380373063

Epoch: 5| Step: 8
Training loss: 2.060512065887451
Validation loss: 2.3567212473961616

Epoch: 5| Step: 9
Training loss: 2.9384255409240723
Validation loss: 2.365676659409718

Epoch: 5| Step: 10
Training loss: 3.2588372230529785
Validation loss: 2.3776413445831626

Epoch: 115| Step: 0
Training loss: 3.0537800788879395
Validation loss: 2.3893491016921176

Epoch: 5| Step: 1
Training loss: 2.447652578353882
Validation loss: 2.390904793175318

Epoch: 5| Step: 2
Training loss: 3.070812702178955
Validation loss: 2.3914092048521964

Epoch: 5| Step: 3
Training loss: 2.7611889839172363
Validation loss: 2.37169506472926

Epoch: 5| Step: 4
Training loss: 2.4051952362060547
Validation loss: 2.3759386783005088

Epoch: 5| Step: 5
Training loss: 2.779770612716675
Validation loss: 2.3650066955115205

Epoch: 5| Step: 6
Training loss: 3.126723051071167
Validation loss: 2.360649329359813

Epoch: 5| Step: 7
Training loss: 2.4918441772460938
Validation loss: 2.354835961454658

Epoch: 5| Step: 8
Training loss: 1.7820287942886353
Validation loss: 2.341608206431071

Epoch: 5| Step: 9
Training loss: 2.5240120887756348
Validation loss: 2.3486581002512286

Epoch: 5| Step: 10
Training loss: 2.22124981880188
Validation loss: 2.3467377590876755

Epoch: 116| Step: 0
Training loss: 2.693669319152832
Validation loss: 2.351468011897097

Epoch: 5| Step: 1
Training loss: 2.630338191986084
Validation loss: 2.373475428550474

Epoch: 5| Step: 2
Training loss: 3.196436643600464
Validation loss: 2.3695657714720695

Epoch: 5| Step: 3
Training loss: 2.67903470993042
Validation loss: 2.383775685423164

Epoch: 5| Step: 4
Training loss: 2.5088911056518555
Validation loss: 2.3993699640356083

Epoch: 5| Step: 5
Training loss: 1.9694334268569946
Validation loss: 2.402927009008264

Epoch: 5| Step: 6
Training loss: 2.859435558319092
Validation loss: 2.409792192520634

Epoch: 5| Step: 7
Training loss: 2.7194604873657227
Validation loss: 2.4176704191392466

Epoch: 5| Step: 8
Training loss: 2.0079989433288574
Validation loss: 2.443349156328427

Epoch: 5| Step: 9
Training loss: 3.007725715637207
Validation loss: 2.456814891548567

Epoch: 5| Step: 10
Training loss: 2.277310848236084
Validation loss: 2.4641792107653875

Epoch: 117| Step: 0
Training loss: 2.481741428375244
Validation loss: 2.447466701589605

Epoch: 5| Step: 1
Training loss: 3.149585247039795
Validation loss: 2.414002703082177

Epoch: 5| Step: 2
Training loss: 2.5985817909240723
Validation loss: 2.3966891406684794

Epoch: 5| Step: 3
Training loss: 2.624818801879883
Validation loss: 2.3813133316655315

Epoch: 5| Step: 4
Training loss: 2.590871810913086
Validation loss: 2.371086871752175

Epoch: 5| Step: 5
Training loss: 3.004080057144165
Validation loss: 2.358883983345442

Epoch: 5| Step: 6
Training loss: 2.070361614227295
Validation loss: 2.3617166473019506

Epoch: 5| Step: 7
Training loss: 1.8323907852172852
Validation loss: 2.3558505529998452

Epoch: 5| Step: 8
Training loss: 3.5171310901641846
Validation loss: 2.3583449240653747

Epoch: 5| Step: 9
Training loss: 2.5579655170440674
Validation loss: 2.3602895595694102

Epoch: 5| Step: 10
Training loss: 2.1226439476013184
Validation loss: 2.3602434717198855

Epoch: 118| Step: 0
Training loss: 2.346975803375244
Validation loss: 2.3628975165787565

Epoch: 5| Step: 1
Training loss: 2.291123867034912
Validation loss: 2.355293212398406

Epoch: 5| Step: 2
Training loss: 1.9992625713348389
Validation loss: 2.344384249820504

Epoch: 5| Step: 3
Training loss: 3.330662965774536
Validation loss: 2.3459805339895268

Epoch: 5| Step: 4
Training loss: 2.974566698074341
Validation loss: 2.3555788160652242

Epoch: 5| Step: 5
Training loss: 2.874370574951172
Validation loss: 2.3473255967581146

Epoch: 5| Step: 6
Training loss: 2.2787528038024902
Validation loss: 2.351653652806436

Epoch: 5| Step: 7
Training loss: 2.922045946121216
Validation loss: 2.344748725173294

Epoch: 5| Step: 8
Training loss: 2.5629353523254395
Validation loss: 2.352769692738851

Epoch: 5| Step: 9
Training loss: 2.3310370445251465
Validation loss: 2.3553292302675146

Epoch: 5| Step: 10
Training loss: 2.594689130783081
Validation loss: 2.361207236525833

Epoch: 119| Step: 0
Training loss: 3.2132344245910645
Validation loss: 2.363259479563723

Epoch: 5| Step: 1
Training loss: 1.9438810348510742
Validation loss: 2.3647195600694224

Epoch: 5| Step: 2
Training loss: 2.187781810760498
Validation loss: 2.3729910055796304

Epoch: 5| Step: 3
Training loss: 2.9274985790252686
Validation loss: 2.3746422106219875

Epoch: 5| Step: 4
Training loss: 2.454725980758667
Validation loss: 2.3873177446344847

Epoch: 5| Step: 5
Training loss: 2.1423232555389404
Validation loss: 2.3945601653027278

Epoch: 5| Step: 6
Training loss: 2.9006614685058594
Validation loss: 2.390303252845682

Epoch: 5| Step: 7
Training loss: 2.812302589416504
Validation loss: 2.3875806177816083

Epoch: 5| Step: 8
Training loss: 3.0619378089904785
Validation loss: 2.3813325230793287

Epoch: 5| Step: 9
Training loss: 2.7984659671783447
Validation loss: 2.3513364356051207

Epoch: 5| Step: 10
Training loss: 1.844659686088562
Validation loss: 2.340324540292063

Epoch: 120| Step: 0
Training loss: 2.3862521648406982
Validation loss: 2.334197700664561

Epoch: 5| Step: 1
Training loss: 2.6558890342712402
Validation loss: 2.3357485930124917

Epoch: 5| Step: 2
Training loss: 2.9300880432128906
Validation loss: 2.3424492677052817

Epoch: 5| Step: 3
Training loss: 2.9281461238861084
Validation loss: 2.3466323960211968

Epoch: 5| Step: 4
Training loss: 2.562572956085205
Validation loss: 2.371849307449915

Epoch: 5| Step: 5
Training loss: 2.8172714710235596
Validation loss: 2.3959449414283998

Epoch: 5| Step: 6
Training loss: 2.80991792678833
Validation loss: 2.4143031233100483

Epoch: 5| Step: 7
Training loss: 2.1479365825653076
Validation loss: 2.4028227867618686

Epoch: 5| Step: 8
Training loss: 2.1700422763824463
Validation loss: 2.402143511720883

Epoch: 5| Step: 9
Training loss: 2.7330403327941895
Validation loss: 2.3835305629238004

Epoch: 5| Step: 10
Training loss: 2.5135462284088135
Validation loss: 2.373156862874185

Epoch: 121| Step: 0
Training loss: 2.6712822914123535
Validation loss: 2.3650777673208587

Epoch: 5| Step: 1
Training loss: 2.317689895629883
Validation loss: 2.3728995579545216

Epoch: 5| Step: 2
Training loss: 2.5838851928710938
Validation loss: 2.373694166060417

Epoch: 5| Step: 3
Training loss: 2.4717535972595215
Validation loss: 2.3626210638271865

Epoch: 5| Step: 4
Training loss: 2.8559813499450684
Validation loss: 2.3620272759468324

Epoch: 5| Step: 5
Training loss: 2.9261398315429688
Validation loss: 2.3679234930264053

Epoch: 5| Step: 6
Training loss: 2.204049825668335
Validation loss: 2.365877113034648

Epoch: 5| Step: 7
Training loss: 2.7389774322509766
Validation loss: 2.3736206434106313

Epoch: 5| Step: 8
Training loss: 2.554041624069214
Validation loss: 2.3893618122223885

Epoch: 5| Step: 9
Training loss: 2.51059627532959
Validation loss: 2.4133196876895044

Epoch: 5| Step: 10
Training loss: 2.5640130043029785
Validation loss: 2.419663288260019

Epoch: 122| Step: 0
Training loss: 2.661776304244995
Validation loss: 2.4509241375871884

Epoch: 5| Step: 1
Training loss: 3.3986942768096924
Validation loss: 2.4501859552116803

Epoch: 5| Step: 2
Training loss: 2.770751953125
Validation loss: 2.4329129367746334

Epoch: 5| Step: 3
Training loss: 2.836470127105713
Validation loss: 2.4120371316068914

Epoch: 5| Step: 4
Training loss: 2.344700336456299
Validation loss: 2.4188090498729418

Epoch: 5| Step: 5
Training loss: 2.9057626724243164
Validation loss: 2.3960899486336658

Epoch: 5| Step: 6
Training loss: 2.0957839488983154
Validation loss: 2.3778413111163723

Epoch: 5| Step: 7
Training loss: 2.7419214248657227
Validation loss: 2.352114908156856

Epoch: 5| Step: 8
Training loss: 2.035107135772705
Validation loss: 2.3412698648309194

Epoch: 5| Step: 9
Training loss: 2.50758695602417
Validation loss: 2.344828446706136

Epoch: 5| Step: 10
Training loss: 2.1379287242889404
Validation loss: 2.333506045802947

Epoch: 123| Step: 0
Training loss: 3.055910587310791
Validation loss: 2.3262802964897564

Epoch: 5| Step: 1
Training loss: 2.4091567993164062
Validation loss: 2.332862202839185

Epoch: 5| Step: 2
Training loss: 2.749788761138916
Validation loss: 2.34963765323803

Epoch: 5| Step: 3
Training loss: 2.598503828048706
Validation loss: 2.3725283120268132

Epoch: 5| Step: 4
Training loss: 2.3823952674865723
Validation loss: 2.3861986949879634

Epoch: 5| Step: 5
Training loss: 2.137186050415039
Validation loss: 2.4049971436941497

Epoch: 5| Step: 6
Training loss: 2.559507131576538
Validation loss: 2.4095862321956183

Epoch: 5| Step: 7
Training loss: 3.416494846343994
Validation loss: 2.413277772165114

Epoch: 5| Step: 8
Training loss: 2.4380526542663574
Validation loss: 2.4157967362352597

Epoch: 5| Step: 9
Training loss: 2.2883124351501465
Validation loss: 2.414817939522446

Epoch: 5| Step: 10
Training loss: 2.687559127807617
Validation loss: 2.4220621585845947

Epoch: 124| Step: 0
Training loss: 1.9618743658065796
Validation loss: 2.420667027914396

Epoch: 5| Step: 1
Training loss: 3.1081385612487793
Validation loss: 2.424125461168187

Epoch: 5| Step: 2
Training loss: 2.986025333404541
Validation loss: 2.424267899605536

Epoch: 5| Step: 3
Training loss: 1.8483978509902954
Validation loss: 2.418506660769063

Epoch: 5| Step: 4
Training loss: 2.608259916305542
Validation loss: 2.4144844393576346

Epoch: 5| Step: 5
Training loss: 3.188420057296753
Validation loss: 2.416039877040412

Epoch: 5| Step: 6
Training loss: 2.5366806983947754
Validation loss: 2.429892696360106

Epoch: 5| Step: 7
Training loss: 2.5659639835357666
Validation loss: 2.4361417857549523

Epoch: 5| Step: 8
Training loss: 2.947955369949341
Validation loss: 2.414361843498804

Epoch: 5| Step: 9
Training loss: 2.341317892074585
Validation loss: 2.406435274308728

Epoch: 5| Step: 10
Training loss: 2.543421983718872
Validation loss: 2.4138253837503414

Epoch: 125| Step: 0
Training loss: 3.180157423019409
Validation loss: 2.408102358541181

Epoch: 5| Step: 1
Training loss: 2.595673084259033
Validation loss: 2.399007520368022

Epoch: 5| Step: 2
Training loss: 2.924844264984131
Validation loss: 2.3998557008722776

Epoch: 5| Step: 3
Training loss: 2.5672168731689453
Validation loss: 2.3985666715970604

Epoch: 5| Step: 4
Training loss: 2.1212658882141113
Validation loss: 2.3827725533516175

Epoch: 5| Step: 5
Training loss: 2.7296864986419678
Validation loss: 2.382310264854021

Epoch: 5| Step: 6
Training loss: 2.4896631240844727
Validation loss: 2.3639713436044674

Epoch: 5| Step: 7
Training loss: 2.7818427085876465
Validation loss: 2.3507669792380383

Epoch: 5| Step: 8
Training loss: 2.1957404613494873
Validation loss: 2.329810575772357

Epoch: 5| Step: 9
Training loss: 2.3638458251953125
Validation loss: 2.32473563378857

Epoch: 5| Step: 10
Training loss: 2.4424850940704346
Validation loss: 2.3126528814274776

Epoch: 126| Step: 0
Training loss: 2.835014820098877
Validation loss: 2.3117183126429075

Epoch: 5| Step: 1
Training loss: 2.2641761302948
Validation loss: 2.3195954984234226

Epoch: 5| Step: 2
Training loss: 2.3830277919769287
Validation loss: 2.338121275747976

Epoch: 5| Step: 3
Training loss: 2.7885642051696777
Validation loss: 2.3900903142908567

Epoch: 5| Step: 4
Training loss: 2.6283071041107178
Validation loss: 2.4155670109615532

Epoch: 5| Step: 5
Training loss: 3.129460573196411
Validation loss: 2.4098417656396025

Epoch: 5| Step: 6
Training loss: 2.637392044067383
Validation loss: 2.4091085131450365

Epoch: 5| Step: 7
Training loss: 2.575511932373047
Validation loss: 2.396899256654965

Epoch: 5| Step: 8
Training loss: 2.8560802936553955
Validation loss: 2.358349184836111

Epoch: 5| Step: 9
Training loss: 2.3394622802734375
Validation loss: 2.3254064821427867

Epoch: 5| Step: 10
Training loss: 2.085899591445923
Validation loss: 2.3094403372016004

Epoch: 127| Step: 0
Training loss: 2.961989641189575
Validation loss: 2.306138805163804

Epoch: 5| Step: 1
Training loss: 2.1559929847717285
Validation loss: 2.305427582033219

Epoch: 5| Step: 2
Training loss: 2.5052762031555176
Validation loss: 2.301919009095879

Epoch: 5| Step: 3
Training loss: 2.493586301803589
Validation loss: 2.302367592370638

Epoch: 5| Step: 4
Training loss: 2.835204601287842
Validation loss: 2.297327559481385

Epoch: 5| Step: 5
Training loss: 2.081108570098877
Validation loss: 2.293251560580346

Epoch: 5| Step: 6
Training loss: 2.7761921882629395
Validation loss: 2.2916403124409337

Epoch: 5| Step: 7
Training loss: 2.5941762924194336
Validation loss: 2.292496904250114

Epoch: 5| Step: 8
Training loss: 2.1232776641845703
Validation loss: 2.295883822184737

Epoch: 5| Step: 9
Training loss: 2.6122207641601562
Validation loss: 2.298517712982752

Epoch: 5| Step: 10
Training loss: 3.453909158706665
Validation loss: 2.3107782768946823

Epoch: 128| Step: 0
Training loss: 2.502150297164917
Validation loss: 2.3130300378286712

Epoch: 5| Step: 1
Training loss: 2.4940781593322754
Validation loss: 2.3121233024904804

Epoch: 5| Step: 2
Training loss: 2.5276479721069336
Validation loss: 2.31107154328336

Epoch: 5| Step: 3
Training loss: 2.4737486839294434
Validation loss: 2.305857916032114

Epoch: 5| Step: 4
Training loss: 2.363877534866333
Validation loss: 2.3118103601599254

Epoch: 5| Step: 5
Training loss: 2.8770689964294434
Validation loss: 2.310080587223012

Epoch: 5| Step: 6
Training loss: 2.5155320167541504
Validation loss: 2.3093789110901537

Epoch: 5| Step: 7
Training loss: 2.569730043411255
Validation loss: 2.317429604068879

Epoch: 5| Step: 8
Training loss: 2.1024093627929688
Validation loss: 2.323111246990901

Epoch: 5| Step: 9
Training loss: 2.7696595191955566
Validation loss: 2.342460657960625

Epoch: 5| Step: 10
Training loss: 3.159900665283203
Validation loss: 2.3445525553918656

Epoch: 129| Step: 0
Training loss: 2.2773854732513428
Validation loss: 2.3687188394608034

Epoch: 5| Step: 1
Training loss: 2.5555872917175293
Validation loss: 2.3680051988170994

Epoch: 5| Step: 2
Training loss: 2.612778425216675
Validation loss: 2.3595255292871946

Epoch: 5| Step: 3
Training loss: 3.02368426322937
Validation loss: 2.3527111289321736

Epoch: 5| Step: 4
Training loss: 2.1215710639953613
Validation loss: 2.3340713080539497

Epoch: 5| Step: 5
Training loss: 2.530648708343506
Validation loss: 2.3217327928030365

Epoch: 5| Step: 6
Training loss: 2.889777898788452
Validation loss: 2.3209886653448946

Epoch: 5| Step: 7
Training loss: 3.2730319499969482
Validation loss: 2.3171900421060543

Epoch: 5| Step: 8
Training loss: 1.7869888544082642
Validation loss: 2.3170515824389715

Epoch: 5| Step: 9
Training loss: 2.5615220069885254
Validation loss: 2.3178812611487603

Epoch: 5| Step: 10
Training loss: 2.616640567779541
Validation loss: 2.3238628115705264

Epoch: 130| Step: 0
Training loss: 2.9293808937072754
Validation loss: 2.323517161030923

Epoch: 5| Step: 1
Training loss: 2.221165180206299
Validation loss: 2.327280439356322

Epoch: 5| Step: 2
Training loss: 3.0890331268310547
Validation loss: 2.3315858917851604

Epoch: 5| Step: 3
Training loss: 2.7204670906066895
Validation loss: 2.336351904817807

Epoch: 5| Step: 4
Training loss: 2.7559831142425537
Validation loss: 2.337719832697222

Epoch: 5| Step: 5
Training loss: 2.4842121601104736
Validation loss: 2.3468620495129655

Epoch: 5| Step: 6
Training loss: 2.130868673324585
Validation loss: 2.352150999089723

Epoch: 5| Step: 7
Training loss: 2.773308515548706
Validation loss: 2.3605098160364295

Epoch: 5| Step: 8
Training loss: 1.6041758060455322
Validation loss: 2.3753141459598335

Epoch: 5| Step: 9
Training loss: 2.61444091796875
Validation loss: 2.3794350265174784

Epoch: 5| Step: 10
Training loss: 2.8394999504089355
Validation loss: 2.386626402537028

Epoch: 131| Step: 0
Training loss: 2.442237377166748
Validation loss: 2.3822456662372877

Epoch: 5| Step: 1
Training loss: 2.973550796508789
Validation loss: 2.3902869557821624

Epoch: 5| Step: 2
Training loss: 2.6306166648864746
Validation loss: 2.3828088237393286

Epoch: 5| Step: 3
Training loss: 3.0645763874053955
Validation loss: 2.3803762312858336

Epoch: 5| Step: 4
Training loss: 2.194124698638916
Validation loss: 2.359421499313847

Epoch: 5| Step: 5
Training loss: 2.3703227043151855
Validation loss: 2.340991691876483

Epoch: 5| Step: 6
Training loss: 3.1430182456970215
Validation loss: 2.3375856491827194

Epoch: 5| Step: 7
Training loss: 2.2193472385406494
Validation loss: 2.3252760235981276

Epoch: 5| Step: 8
Training loss: 2.60764741897583
Validation loss: 2.337100580174436

Epoch: 5| Step: 9
Training loss: 2.0951411724090576
Validation loss: 2.3257389991514144

Epoch: 5| Step: 10
Training loss: 2.4990904331207275
Validation loss: 2.3221073945363364

Epoch: 132| Step: 0
Training loss: 2.663940906524658
Validation loss: 2.311695444968439

Epoch: 5| Step: 1
Training loss: 2.6268677711486816
Validation loss: 2.3096963064644926

Epoch: 5| Step: 2
Training loss: 2.540031671524048
Validation loss: 2.3036641484947613

Epoch: 5| Step: 3
Training loss: 2.256040096282959
Validation loss: 2.303548207847021

Epoch: 5| Step: 4
Training loss: 2.8757095336914062
Validation loss: 2.310967594064692

Epoch: 5| Step: 5
Training loss: 2.7111918926239014
Validation loss: 2.310605879752867

Epoch: 5| Step: 6
Training loss: 3.00040602684021
Validation loss: 2.309719229257235

Epoch: 5| Step: 7
Training loss: 2.0450186729431152
Validation loss: 2.309103076175977

Epoch: 5| Step: 8
Training loss: 2.666883945465088
Validation loss: 2.3113691934975247

Epoch: 5| Step: 9
Training loss: 2.5158514976501465
Validation loss: 2.3232095856820383

Epoch: 5| Step: 10
Training loss: 2.1852664947509766
Validation loss: 2.3271113826382543

Epoch: 133| Step: 0
Training loss: 2.336946964263916
Validation loss: 2.338891395958521

Epoch: 5| Step: 1
Training loss: 2.5106053352355957
Validation loss: 2.3684231440226235

Epoch: 5| Step: 2
Training loss: 2.2599103450775146
Validation loss: 2.388520658657115

Epoch: 5| Step: 3
Training loss: 2.3098864555358887
Validation loss: 2.4060593599914224

Epoch: 5| Step: 4
Training loss: 2.675011157989502
Validation loss: 2.3809108247039137

Epoch: 5| Step: 5
Training loss: 2.984584331512451
Validation loss: 2.3443684193395797

Epoch: 5| Step: 6
Training loss: 2.9972891807556152
Validation loss: 2.3168289071770123

Epoch: 5| Step: 7
Training loss: 2.643231153488159
Validation loss: 2.2923251326366136

Epoch: 5| Step: 8
Training loss: 2.5132408142089844
Validation loss: 2.293320284094862

Epoch: 5| Step: 9
Training loss: 2.1413869857788086
Validation loss: 2.2946648828444944

Epoch: 5| Step: 10
Training loss: 3.041753053665161
Validation loss: 2.295391901846855

Epoch: 134| Step: 0
Training loss: 2.521019220352173
Validation loss: 2.2958281578556186

Epoch: 5| Step: 1
Training loss: 2.7246439456939697
Validation loss: 2.298990623925322

Epoch: 5| Step: 2
Training loss: 3.3171329498291016
Validation loss: 2.2970614792198263

Epoch: 5| Step: 3
Training loss: 2.307807683944702
Validation loss: 2.294147329945718

Epoch: 5| Step: 4
Training loss: 2.811732769012451
Validation loss: 2.2908995805248136

Epoch: 5| Step: 5
Training loss: 2.342001438140869
Validation loss: 2.28766808971282

Epoch: 5| Step: 6
Training loss: 2.318415403366089
Validation loss: 2.2890987242421796

Epoch: 5| Step: 7
Training loss: 2.12004017829895
Validation loss: 2.290386576806345

Epoch: 5| Step: 8
Training loss: 3.450744152069092
Validation loss: 2.309725688349816

Epoch: 5| Step: 9
Training loss: 2.1070685386657715
Validation loss: 2.3141046416374946

Epoch: 5| Step: 10
Training loss: 2.0608341693878174
Validation loss: 2.3285062800171556

Epoch: 135| Step: 0
Training loss: 2.391160488128662
Validation loss: 2.34105864391532

Epoch: 5| Step: 1
Training loss: 2.4046430587768555
Validation loss: 2.3369582365917903

Epoch: 5| Step: 2
Training loss: 2.191807508468628
Validation loss: 2.3337078914847424

Epoch: 5| Step: 3
Training loss: 2.634920120239258
Validation loss: 2.3490921399926625

Epoch: 5| Step: 4
Training loss: 2.5778720378875732
Validation loss: 2.3496568484972884

Epoch: 5| Step: 5
Training loss: 2.8643994331359863
Validation loss: 2.3574868940537974

Epoch: 5| Step: 6
Training loss: 2.3438003063201904
Validation loss: 2.341856628335932

Epoch: 5| Step: 7
Training loss: 2.448314666748047
Validation loss: 2.3374489802186207

Epoch: 5| Step: 8
Training loss: 2.530128240585327
Validation loss: 2.3363431038395053

Epoch: 5| Step: 9
Training loss: 2.7954134941101074
Validation loss: 2.3303449794810307

Epoch: 5| Step: 10
Training loss: 2.9467523097991943
Validation loss: 2.3223072841603267

Epoch: 136| Step: 0
Training loss: 1.6362022161483765
Validation loss: 2.312185541276009

Epoch: 5| Step: 1
Training loss: 2.452897548675537
Validation loss: 2.319504694272113

Epoch: 5| Step: 2
Training loss: 3.685804843902588
Validation loss: 2.311911695746965

Epoch: 5| Step: 3
Training loss: 2.2127132415771484
Validation loss: 2.319729789610832

Epoch: 5| Step: 4
Training loss: 2.8487730026245117
Validation loss: 2.307326732143279

Epoch: 5| Step: 5
Training loss: 2.3036816120147705
Validation loss: 2.302358786265055

Epoch: 5| Step: 6
Training loss: 2.48125958442688
Validation loss: 2.306515265536565

Epoch: 5| Step: 7
Training loss: 2.587484359741211
Validation loss: 2.293370795506303

Epoch: 5| Step: 8
Training loss: 2.8045430183410645
Validation loss: 2.3082053738255657

Epoch: 5| Step: 9
Training loss: 2.723332643508911
Validation loss: 2.306422428418231

Epoch: 5| Step: 10
Training loss: 2.145421266555786
Validation loss: 2.3236121490437496

Epoch: 137| Step: 0
Training loss: 3.008815050125122
Validation loss: 2.325902460723795

Epoch: 5| Step: 1
Training loss: 2.2384674549102783
Validation loss: 2.3207315373164352

Epoch: 5| Step: 2
Training loss: 2.853985548019409
Validation loss: 2.3177322828641502

Epoch: 5| Step: 3
Training loss: 2.434173822402954
Validation loss: 2.3169919188304613

Epoch: 5| Step: 4
Training loss: 2.6704506874084473
Validation loss: 2.320009275149274

Epoch: 5| Step: 5
Training loss: 2.957037925720215
Validation loss: 2.3110686809785905

Epoch: 5| Step: 6
Training loss: 2.183755397796631
Validation loss: 2.3148149316028883

Epoch: 5| Step: 7
Training loss: 2.8414053916931152
Validation loss: 2.316084602827667

Epoch: 5| Step: 8
Training loss: 2.3938753604888916
Validation loss: 2.3170342855556036

Epoch: 5| Step: 9
Training loss: 1.6635410785675049
Validation loss: 2.317191949454687

Epoch: 5| Step: 10
Training loss: 2.7274858951568604
Validation loss: 2.322937073246125

Epoch: 138| Step: 0
Training loss: 2.081930637359619
Validation loss: 2.3443616410737396

Epoch: 5| Step: 1
Training loss: 2.6417291164398193
Validation loss: 2.347191310697986

Epoch: 5| Step: 2
Training loss: 3.04164457321167
Validation loss: 2.342212938493298

Epoch: 5| Step: 3
Training loss: 2.422060251235962
Validation loss: 2.317260898569579

Epoch: 5| Step: 4
Training loss: 2.1473000049591064
Validation loss: 2.3131845587043354

Epoch: 5| Step: 5
Training loss: 2.492788791656494
Validation loss: 2.3014078832441762

Epoch: 5| Step: 6
Training loss: 2.5193841457366943
Validation loss: 2.2941829389141453

Epoch: 5| Step: 7
Training loss: 2.625685214996338
Validation loss: 2.295079021043675

Epoch: 5| Step: 8
Training loss: 3.232903242111206
Validation loss: 2.289600508187407

Epoch: 5| Step: 9
Training loss: 2.338554859161377
Validation loss: 2.302192329078592

Epoch: 5| Step: 10
Training loss: 2.2510933876037598
Validation loss: 2.306227708375582

Epoch: 139| Step: 0
Training loss: 2.349416971206665
Validation loss: 2.312130246111142

Epoch: 5| Step: 1
Training loss: 2.5165152549743652
Validation loss: 2.312834275666104

Epoch: 5| Step: 2
Training loss: 2.3226871490478516
Validation loss: 2.323889631097035

Epoch: 5| Step: 3
Training loss: 2.9074416160583496
Validation loss: 2.3277825514475503

Epoch: 5| Step: 4
Training loss: 3.0717649459838867
Validation loss: 2.318439381096953

Epoch: 5| Step: 5
Training loss: 1.990624189376831
Validation loss: 2.3151203586209204

Epoch: 5| Step: 6
Training loss: 3.4777328968048096
Validation loss: 2.3149304210498767

Epoch: 5| Step: 7
Training loss: 2.433105945587158
Validation loss: 2.3043100474983134

Epoch: 5| Step: 8
Training loss: 2.1058781147003174
Validation loss: 2.301304937690817

Epoch: 5| Step: 9
Training loss: 2.167454481124878
Validation loss: 2.294028810275498

Epoch: 5| Step: 10
Training loss: 2.480964183807373
Validation loss: 2.2989345289045766

Epoch: 140| Step: 0
Training loss: 2.8312971591949463
Validation loss: 2.306581030609787

Epoch: 5| Step: 1
Training loss: 2.2904839515686035
Validation loss: 2.3013995667939544

Epoch: 5| Step: 2
Training loss: 2.8816370964050293
Validation loss: 2.3086576871974493

Epoch: 5| Step: 3
Training loss: 2.429926633834839
Validation loss: 2.3105303574633855

Epoch: 5| Step: 4
Training loss: 2.646968364715576
Validation loss: 2.3023320949205788

Epoch: 5| Step: 5
Training loss: 2.489051342010498
Validation loss: 2.29064764002318

Epoch: 5| Step: 6
Training loss: 2.4652163982391357
Validation loss: 2.292091704184009

Epoch: 5| Step: 7
Training loss: 2.204467296600342
Validation loss: 2.2979108287442114

Epoch: 5| Step: 8
Training loss: 2.9940314292907715
Validation loss: 2.3110555013020835

Epoch: 5| Step: 9
Training loss: 2.0277299880981445
Validation loss: 2.3195705759909844

Epoch: 5| Step: 10
Training loss: 2.5855965614318848
Validation loss: 2.314819492319579

Epoch: 141| Step: 0
Training loss: 3.3602242469787598
Validation loss: 2.3303711645064817

Epoch: 5| Step: 1
Training loss: 2.6466879844665527
Validation loss: 2.3369130652437926

Epoch: 5| Step: 2
Training loss: 2.7527031898498535
Validation loss: 2.3372571750353743

Epoch: 5| Step: 3
Training loss: 2.323525905609131
Validation loss: 2.3047259751186577

Epoch: 5| Step: 4
Training loss: 2.821882724761963
Validation loss: 2.290638264789376

Epoch: 5| Step: 5
Training loss: 2.3118412494659424
Validation loss: 2.2711281699519

Epoch: 5| Step: 6
Training loss: 2.049731731414795
Validation loss: 2.262828375703545

Epoch: 5| Step: 7
Training loss: 2.2648043632507324
Validation loss: 2.261727991924491

Epoch: 5| Step: 8
Training loss: 2.192584276199341
Validation loss: 2.2806579092497468

Epoch: 5| Step: 9
Training loss: 2.4322705268859863
Validation loss: 2.288866191781977

Epoch: 5| Step: 10
Training loss: 2.800978183746338
Validation loss: 2.281642903563797

Epoch: 142| Step: 0
Training loss: 2.5178074836730957
Validation loss: 2.2987526180923625

Epoch: 5| Step: 1
Training loss: 1.983036994934082
Validation loss: 2.307814859574841

Epoch: 5| Step: 2
Training loss: 2.091675281524658
Validation loss: 2.310146031841155

Epoch: 5| Step: 3
Training loss: 2.702371835708618
Validation loss: 2.303943636596844

Epoch: 5| Step: 4
Training loss: 2.607635021209717
Validation loss: 2.294557785475126

Epoch: 5| Step: 5
Training loss: 2.9137516021728516
Validation loss: 2.287451914561692

Epoch: 5| Step: 6
Training loss: 3.2002944946289062
Validation loss: 2.285731530958606

Epoch: 5| Step: 7
Training loss: 2.6028552055358887
Validation loss: 2.269678887500558

Epoch: 5| Step: 8
Training loss: 2.7859740257263184
Validation loss: 2.2781164338511806

Epoch: 5| Step: 9
Training loss: 2.2753005027770996
Validation loss: 2.2711433185044156

Epoch: 5| Step: 10
Training loss: 2.0221288204193115
Validation loss: 2.2663443524350404

Epoch: 143| Step: 0
Training loss: 2.1356046199798584
Validation loss: 2.276844242567657

Epoch: 5| Step: 1
Training loss: 2.946967363357544
Validation loss: 2.2823333535143124

Epoch: 5| Step: 2
Training loss: 2.1277058124542236
Validation loss: 2.281687887766028

Epoch: 5| Step: 3
Training loss: 2.5342888832092285
Validation loss: 2.291540199710477

Epoch: 5| Step: 4
Training loss: 2.726240634918213
Validation loss: 2.3091128180103917

Epoch: 5| Step: 5
Training loss: 2.259546995162964
Validation loss: 2.329757990375642

Epoch: 5| Step: 6
Training loss: 2.6085047721862793
Validation loss: 2.3591748822119927

Epoch: 5| Step: 7
Training loss: 2.5980007648468018
Validation loss: 2.347271998723348

Epoch: 5| Step: 8
Training loss: 2.6492760181427
Validation loss: 2.3160867357766755

Epoch: 5| Step: 9
Training loss: 2.5661439895629883
Validation loss: 2.2934330817191833

Epoch: 5| Step: 10
Training loss: 2.6470510959625244
Validation loss: 2.267104515465357

Epoch: 144| Step: 0
Training loss: 3.5116724967956543
Validation loss: 2.2585515642678864

Epoch: 5| Step: 1
Training loss: 2.867818832397461
Validation loss: 2.2609624196124334

Epoch: 5| Step: 2
Training loss: 2.2180817127227783
Validation loss: 2.2685136231043006

Epoch: 5| Step: 3
Training loss: 1.7276045083999634
Validation loss: 2.277868793856713

Epoch: 5| Step: 4
Training loss: 2.656157970428467
Validation loss: 2.304228654471777

Epoch: 5| Step: 5
Training loss: 2.4917802810668945
Validation loss: 2.3065583603356474

Epoch: 5| Step: 6
Training loss: 2.56844425201416
Validation loss: 2.308778239834693

Epoch: 5| Step: 7
Training loss: 2.271526336669922
Validation loss: 2.296970098249374

Epoch: 5| Step: 8
Training loss: 2.6502223014831543
Validation loss: 2.282222073565247

Epoch: 5| Step: 9
Training loss: 3.0513222217559814
Validation loss: 2.2744724699245986

Epoch: 5| Step: 10
Training loss: 2.153712034225464
Validation loss: 2.264208221948275

Epoch: 145| Step: 0
Training loss: 3.0772628784179688
Validation loss: 2.253169613499795

Epoch: 5| Step: 1
Training loss: 1.9207252264022827
Validation loss: 2.2465798598463818

Epoch: 5| Step: 2
Training loss: 2.8378264904022217
Validation loss: 2.254481533522247

Epoch: 5| Step: 3
Training loss: 2.1415140628814697
Validation loss: 2.266382089225195

Epoch: 5| Step: 4
Training loss: 2.4317357540130615
Validation loss: 2.283077032335343

Epoch: 5| Step: 5
Training loss: 2.6793901920318604
Validation loss: 2.303840624388828

Epoch: 5| Step: 6
Training loss: 2.535964012145996
Validation loss: 2.3304585308156986

Epoch: 5| Step: 7
Training loss: 2.642658233642578
Validation loss: 2.334065616771739

Epoch: 5| Step: 8
Training loss: 2.540156841278076
Validation loss: 2.3276530414499264

Epoch: 5| Step: 9
Training loss: 2.3774325847625732
Validation loss: 2.3259968475628923

Epoch: 5| Step: 10
Training loss: 2.494875907897949
Validation loss: 2.311356880331552

Epoch: 146| Step: 0
Training loss: 2.2265467643737793
Validation loss: 2.2946965438063427

Epoch: 5| Step: 1
Training loss: 2.8190038204193115
Validation loss: 2.2773227948014454

Epoch: 5| Step: 2
Training loss: 2.2789149284362793
Validation loss: 2.27775760363507

Epoch: 5| Step: 3
Training loss: 2.347982406616211
Validation loss: 2.2727997867009972

Epoch: 5| Step: 4
Training loss: 2.5008883476257324
Validation loss: 2.258848515889978

Epoch: 5| Step: 5
Training loss: 2.3291430473327637
Validation loss: 2.263528254724318

Epoch: 5| Step: 6
Training loss: 2.7273807525634766
Validation loss: 2.254910715164677

Epoch: 5| Step: 7
Training loss: 2.4210689067840576
Validation loss: 2.2570629632601173

Epoch: 5| Step: 8
Training loss: 2.4693920612335205
Validation loss: 2.2589899839893466

Epoch: 5| Step: 9
Training loss: 2.867427349090576
Validation loss: 2.274325592543489

Epoch: 5| Step: 10
Training loss: 2.67435359954834
Validation loss: 2.2860350890826155

Epoch: 147| Step: 0
Training loss: 2.990992307662964
Validation loss: 2.289296793681319

Epoch: 5| Step: 1
Training loss: 2.4087672233581543
Validation loss: 2.2937399289941274

Epoch: 5| Step: 2
Training loss: 1.9517621994018555
Validation loss: 2.277717718514063

Epoch: 5| Step: 3
Training loss: 2.6438331604003906
Validation loss: 2.252012700162908

Epoch: 5| Step: 4
Training loss: 2.9179863929748535
Validation loss: 2.2388554388476956

Epoch: 5| Step: 5
Training loss: 2.252784252166748
Validation loss: 2.239461978276571

Epoch: 5| Step: 6
Training loss: 2.273991823196411
Validation loss: 2.23552341358636

Epoch: 5| Step: 7
Training loss: 2.782015085220337
Validation loss: 2.2385351862958682

Epoch: 5| Step: 8
Training loss: 2.8740041255950928
Validation loss: 2.238471510589764

Epoch: 5| Step: 9
Training loss: 2.043198823928833
Validation loss: 2.2378386271897184

Epoch: 5| Step: 10
Training loss: 2.6131370067596436
Validation loss: 2.236569135419784

Epoch: 148| Step: 0
Training loss: 2.6095809936523438
Validation loss: 2.2414691140574794

Epoch: 5| Step: 1
Training loss: 2.836594343185425
Validation loss: 2.249753213697864

Epoch: 5| Step: 2
Training loss: 2.3160221576690674
Validation loss: 2.2483957659813667

Epoch: 5| Step: 3
Training loss: 2.8321571350097656
Validation loss: 2.2533302076401247

Epoch: 5| Step: 4
Training loss: 1.8127323389053345
Validation loss: 2.2620353826912503

Epoch: 5| Step: 5
Training loss: 2.6145896911621094
Validation loss: 2.2633245862940305

Epoch: 5| Step: 6
Training loss: 2.584240436553955
Validation loss: 2.2654194626756894

Epoch: 5| Step: 7
Training loss: 1.5952281951904297
Validation loss: 2.2630175095732494

Epoch: 5| Step: 8
Training loss: 3.445383071899414
Validation loss: 2.2814698142390095

Epoch: 5| Step: 9
Training loss: 2.3860535621643066
Validation loss: 2.2851434523059475

Epoch: 5| Step: 10
Training loss: 2.4901556968688965
Validation loss: 2.293737144880397

Epoch: 149| Step: 0
Training loss: 2.248645305633545
Validation loss: 2.3159131311601207

Epoch: 5| Step: 1
Training loss: 2.851635456085205
Validation loss: 2.3270691697315504

Epoch: 5| Step: 2
Training loss: 2.4580070972442627
Validation loss: 2.33978174578759

Epoch: 5| Step: 3
Training loss: 2.6467220783233643
Validation loss: 2.325306015629922

Epoch: 5| Step: 4
Training loss: 2.9181904792785645
Validation loss: 2.3125313353794876

Epoch: 5| Step: 5
Training loss: 2.199166774749756
Validation loss: 2.287168610480524

Epoch: 5| Step: 6
Training loss: 2.140720844268799
Validation loss: 2.2782925123809488

Epoch: 5| Step: 7
Training loss: 2.139906406402588
Validation loss: 2.274271518953385

Epoch: 5| Step: 8
Training loss: 2.444319248199463
Validation loss: 2.269935210545858

Epoch: 5| Step: 9
Training loss: 2.7953925132751465
Validation loss: 2.2823987135323147

Epoch: 5| Step: 10
Training loss: 2.9114999771118164
Validation loss: 2.275963209008658

Epoch: 150| Step: 0
Training loss: 2.6799042224884033
Validation loss: 2.276953384440432

Epoch: 5| Step: 1
Training loss: 2.114542245864868
Validation loss: 2.267859402523246

Epoch: 5| Step: 2
Training loss: 2.6613752841949463
Validation loss: 2.2603765713271273

Epoch: 5| Step: 3
Training loss: 2.765822172164917
Validation loss: 2.270629293175154

Epoch: 5| Step: 4
Training loss: 2.4045493602752686
Validation loss: 2.265990044481011

Epoch: 5| Step: 5
Training loss: 3.101871967315674
Validation loss: 2.2879122534105854

Epoch: 5| Step: 6
Training loss: 1.8520698547363281
Validation loss: 2.2926919716660694

Epoch: 5| Step: 7
Training loss: 2.7003886699676514
Validation loss: 2.308893649808822

Epoch: 5| Step: 8
Training loss: 2.5765509605407715
Validation loss: 2.3215283629714802

Epoch: 5| Step: 9
Training loss: 1.9990304708480835
Validation loss: 2.3227484700500325

Epoch: 5| Step: 10
Training loss: 2.8615565299987793
Validation loss: 2.2997117042541504

Epoch: 151| Step: 0
Training loss: 2.462272882461548
Validation loss: 2.278144089124536

Epoch: 5| Step: 1
Training loss: 2.32838773727417
Validation loss: 2.2597819810272544

Epoch: 5| Step: 2
Training loss: 3.030576229095459
Validation loss: 2.238597875000328

Epoch: 5| Step: 3
Training loss: 1.6882864236831665
Validation loss: 2.24203937028044

Epoch: 5| Step: 4
Training loss: 2.8318748474121094
Validation loss: 2.238915851039271

Epoch: 5| Step: 5
Training loss: 2.280665636062622
Validation loss: 2.23697837193807

Epoch: 5| Step: 6
Training loss: 2.9167561531066895
Validation loss: 2.2328179805509505

Epoch: 5| Step: 7
Training loss: 2.411130428314209
Validation loss: 2.235962901064145

Epoch: 5| Step: 8
Training loss: 2.6911380290985107
Validation loss: 2.2381477766139533

Epoch: 5| Step: 9
Training loss: 2.561054229736328
Validation loss: 2.2409538722807363

Epoch: 5| Step: 10
Training loss: 2.4102015495300293
Validation loss: 2.250835648147009

Epoch: 152| Step: 0
Training loss: 2.793820858001709
Validation loss: 2.2621686612406084

Epoch: 5| Step: 1
Training loss: 1.7726976871490479
Validation loss: 2.293944461371309

Epoch: 5| Step: 2
Training loss: 2.649149179458618
Validation loss: 2.3371590414354877

Epoch: 5| Step: 3
Training loss: 2.1851260662078857
Validation loss: 2.338933819083757

Epoch: 5| Step: 4
Training loss: 2.8763973712921143
Validation loss: 2.320401753148725

Epoch: 5| Step: 5
Training loss: 2.4503397941589355
Validation loss: 2.293094486318609

Epoch: 5| Step: 6
Training loss: 2.846313953399658
Validation loss: 2.257104722402429

Epoch: 5| Step: 7
Training loss: 2.27571702003479
Validation loss: 2.2518527200145106

Epoch: 5| Step: 8
Training loss: 2.6210527420043945
Validation loss: 2.2377420317742134

Epoch: 5| Step: 9
Training loss: 2.540898561477661
Validation loss: 2.2421816600266324

Epoch: 5| Step: 10
Training loss: 2.7009902000427246
Validation loss: 2.2362206879482476

Epoch: 153| Step: 0
Training loss: 2.0140585899353027
Validation loss: 2.23453438410195

Epoch: 5| Step: 1
Training loss: 2.596358060836792
Validation loss: 2.2499829492261334

Epoch: 5| Step: 2
Training loss: 2.851802349090576
Validation loss: 2.247998551655841

Epoch: 5| Step: 3
Training loss: 2.3423054218292236
Validation loss: 2.24546133318255

Epoch: 5| Step: 4
Training loss: 2.2067322731018066
Validation loss: 2.252886164572931

Epoch: 5| Step: 5
Training loss: 2.589151382446289
Validation loss: 2.257338890465357

Epoch: 5| Step: 6
Training loss: 2.376497745513916
Validation loss: 2.2567338174389255

Epoch: 5| Step: 7
Training loss: 2.2853424549102783
Validation loss: 2.273763064415224

Epoch: 5| Step: 8
Training loss: 2.8150033950805664
Validation loss: 2.274819027992987

Epoch: 5| Step: 9
Training loss: 2.814997434616089
Validation loss: 2.2817310774198143

Epoch: 5| Step: 10
Training loss: 2.4927151203155518
Validation loss: 2.2958069027111097

Epoch: 154| Step: 0
Training loss: 2.3599352836608887
Validation loss: 2.308778812808375

Epoch: 5| Step: 1
Training loss: 2.6623523235321045
Validation loss: 2.3142757492680706

Epoch: 5| Step: 2
Training loss: 2.2058982849121094
Validation loss: 2.2918699377326557

Epoch: 5| Step: 3
Training loss: 2.813804864883423
Validation loss: 2.267923652484853

Epoch: 5| Step: 4
Training loss: 2.5950920581817627
Validation loss: 2.2604672280691003

Epoch: 5| Step: 5
Training loss: 2.58363676071167
Validation loss: 2.2542385567900953

Epoch: 5| Step: 6
Training loss: 1.7437772750854492
Validation loss: 2.2472113768259683

Epoch: 5| Step: 7
Training loss: 2.647742509841919
Validation loss: 2.249418707304103

Epoch: 5| Step: 8
Training loss: 2.544238567352295
Validation loss: 2.236742724654495

Epoch: 5| Step: 9
Training loss: 2.4163296222686768
Validation loss: 2.2512545816359983

Epoch: 5| Step: 10
Training loss: 2.917935848236084
Validation loss: 2.2534347247051936

Epoch: 155| Step: 0
Training loss: 2.2017292976379395
Validation loss: 2.2748267009694088

Epoch: 5| Step: 1
Training loss: 2.7905898094177246
Validation loss: 2.2944906526996243

Epoch: 5| Step: 2
Training loss: 2.3837881088256836
Validation loss: 2.2874398385324786

Epoch: 5| Step: 3
Training loss: 3.1433019638061523
Validation loss: 2.2914433581854707

Epoch: 5| Step: 4
Training loss: 2.5032947063446045
Validation loss: 2.289627021358859

Epoch: 5| Step: 5
Training loss: 1.8055126667022705
Validation loss: 2.28124378701692

Epoch: 5| Step: 6
Training loss: 2.375967025756836
Validation loss: 2.2776948021304224

Epoch: 5| Step: 7
Training loss: 2.5796334743499756
Validation loss: 2.2522765692844184

Epoch: 5| Step: 8
Training loss: 2.2420575618743896
Validation loss: 2.244734741026355

Epoch: 5| Step: 9
Training loss: 2.9248898029327393
Validation loss: 2.2474991326691

Epoch: 5| Step: 10
Training loss: 2.6153154373168945
Validation loss: 2.259799358665302

Epoch: 156| Step: 0
Training loss: 2.6587445735931396
Validation loss: 2.2663524894304174

Epoch: 5| Step: 1
Training loss: 2.4475150108337402
Validation loss: 2.294200251179357

Epoch: 5| Step: 2
Training loss: 2.6523454189300537
Validation loss: 2.2783458181606826

Epoch: 5| Step: 3
Training loss: 2.4556002616882324
Validation loss: 2.2860986186612036

Epoch: 5| Step: 4
Training loss: 2.6582672595977783
Validation loss: 2.272599481767224

Epoch: 5| Step: 5
Training loss: 1.8403100967407227
Validation loss: 2.26465130108659

Epoch: 5| Step: 6
Training loss: 2.691748857498169
Validation loss: 2.2380725734977314

Epoch: 5| Step: 7
Training loss: 1.8554750680923462
Validation loss: 2.237049982111941

Epoch: 5| Step: 8
Training loss: 2.5874829292297363
Validation loss: 2.2358307094984156

Epoch: 5| Step: 9
Training loss: 2.887601375579834
Validation loss: 2.2402948423098494

Epoch: 5| Step: 10
Training loss: 2.799168348312378
Validation loss: 2.252683501089773

Epoch: 157| Step: 0
Training loss: 2.0192911624908447
Validation loss: 2.23387861123649

Epoch: 5| Step: 1
Training loss: 2.592440128326416
Validation loss: 2.2441841504907094

Epoch: 5| Step: 2
Training loss: 2.329322338104248
Validation loss: 2.2371099661755305

Epoch: 5| Step: 3
Training loss: 2.804640531539917
Validation loss: 2.2264016200137395

Epoch: 5| Step: 4
Training loss: 2.8051371574401855
Validation loss: 2.218209628135927

Epoch: 5| Step: 5
Training loss: 2.8351757526397705
Validation loss: 2.2267637086170975

Epoch: 5| Step: 6
Training loss: 2.6782355308532715
Validation loss: 2.223618463803363

Epoch: 5| Step: 7
Training loss: 2.6887080669403076
Validation loss: 2.2200583642528904

Epoch: 5| Step: 8
Training loss: 2.322843074798584
Validation loss: 2.227425662420129

Epoch: 5| Step: 9
Training loss: 1.834845781326294
Validation loss: 2.224981259274226

Epoch: 5| Step: 10
Training loss: 2.581732749938965
Validation loss: 2.233039394501717

Epoch: 158| Step: 0
Training loss: 2.582188367843628
Validation loss: 2.2325561213236984

Epoch: 5| Step: 1
Training loss: 1.9705390930175781
Validation loss: 2.2432399770264984

Epoch: 5| Step: 2
Training loss: 3.027588129043579
Validation loss: 2.2337510406330066

Epoch: 5| Step: 3
Training loss: 3.3053627014160156
Validation loss: 2.2501811647927887

Epoch: 5| Step: 4
Training loss: 2.8300578594207764
Validation loss: 2.2572078551015546

Epoch: 5| Step: 5
Training loss: 2.417928457260132
Validation loss: 2.270034913093813

Epoch: 5| Step: 6
Training loss: 2.4200565814971924
Validation loss: 2.269028422653034

Epoch: 5| Step: 7
Training loss: 1.9485054016113281
Validation loss: 2.2866523086383777

Epoch: 5| Step: 8
Training loss: 2.4639370441436768
Validation loss: 2.294114815291538

Epoch: 5| Step: 9
Training loss: 1.953993558883667
Validation loss: 2.279496785133116

Epoch: 5| Step: 10
Training loss: 2.485596179962158
Validation loss: 2.2706842435303556

Epoch: 159| Step: 0
Training loss: 2.68534517288208
Validation loss: 2.280638333289854

Epoch: 5| Step: 1
Training loss: 1.953944206237793
Validation loss: 2.261568756513698

Epoch: 5| Step: 2
Training loss: 2.1782872676849365
Validation loss: 2.2501391569773355

Epoch: 5| Step: 3
Training loss: 2.641249179840088
Validation loss: 2.2629072640531804

Epoch: 5| Step: 4
Training loss: 2.906935214996338
Validation loss: 2.2593473926667245

Epoch: 5| Step: 5
Training loss: 2.5078747272491455
Validation loss: 2.256182147610572

Epoch: 5| Step: 6
Training loss: 2.8235554695129395
Validation loss: 2.2629788396179036

Epoch: 5| Step: 7
Training loss: 2.7607102394104004
Validation loss: 2.2659294989801224

Epoch: 5| Step: 8
Training loss: 2.215357542037964
Validation loss: 2.2615184963390393

Epoch: 5| Step: 9
Training loss: 1.9851477146148682
Validation loss: 2.257100979487101

Epoch: 5| Step: 10
Training loss: 2.353710412979126
Validation loss: 2.2481474594403337

Epoch: 160| Step: 0
Training loss: 2.2994282245635986
Validation loss: 2.2478561939731723

Epoch: 5| Step: 1
Training loss: 2.419743299484253
Validation loss: 2.246697300223894

Epoch: 5| Step: 2
Training loss: 2.246171474456787
Validation loss: 2.252842932619074

Epoch: 5| Step: 3
Training loss: 2.5747122764587402
Validation loss: 2.2632456107806136

Epoch: 5| Step: 4
Training loss: 2.4396402835845947
Validation loss: 2.2537052913378646

Epoch: 5| Step: 5
Training loss: 3.038541078567505
Validation loss: 2.231562358076854

Epoch: 5| Step: 6
Training loss: 2.338829755783081
Validation loss: 2.2366601972169775

Epoch: 5| Step: 7
Training loss: 3.027301788330078
Validation loss: 2.2348128903296685

Epoch: 5| Step: 8
Training loss: 2.411719799041748
Validation loss: 2.230935601777928

Epoch: 5| Step: 9
Training loss: 1.5476105213165283
Validation loss: 2.243339241191905

Epoch: 5| Step: 10
Training loss: 3.018627166748047
Validation loss: 2.2790774940162577

Epoch: 161| Step: 0
Training loss: 1.7882426977157593
Validation loss: 2.313727314754199

Epoch: 5| Step: 1
Training loss: 2.5569796562194824
Validation loss: 2.3276973770510767

Epoch: 5| Step: 2
Training loss: 3.3868095874786377
Validation loss: 2.3182149138501895

Epoch: 5| Step: 3
Training loss: 2.476139545440674
Validation loss: 2.297844030523813

Epoch: 5| Step: 4
Training loss: 2.4426820278167725
Validation loss: 2.2796975361403597

Epoch: 5| Step: 5
Training loss: 2.749847888946533
Validation loss: 2.262920125838249

Epoch: 5| Step: 6
Training loss: 1.9635210037231445
Validation loss: 2.2499226780347925

Epoch: 5| Step: 7
Training loss: 2.835239887237549
Validation loss: 2.2566231732727378

Epoch: 5| Step: 8
Training loss: 2.736558675765991
Validation loss: 2.2579429713628625

Epoch: 5| Step: 9
Training loss: 2.0812695026397705
Validation loss: 2.263744669575845

Epoch: 5| Step: 10
Training loss: 2.1469595432281494
Validation loss: 2.2571230549966135

Epoch: 162| Step: 0
Training loss: 2.308830976486206
Validation loss: 2.262441537713492

Epoch: 5| Step: 1
Training loss: 2.8046298027038574
Validation loss: 2.2628018471502487

Epoch: 5| Step: 2
Training loss: 2.0590808391571045
Validation loss: 2.2707911511903167

Epoch: 5| Step: 3
Training loss: 2.3312220573425293
Validation loss: 2.269704011178786

Epoch: 5| Step: 4
Training loss: 3.202997922897339
Validation loss: 2.288164543849166

Epoch: 5| Step: 5
Training loss: 2.565488338470459
Validation loss: 2.2760491101972518

Epoch: 5| Step: 6
Training loss: 2.600834369659424
Validation loss: 2.259158029351183

Epoch: 5| Step: 7
Training loss: 2.2878737449645996
Validation loss: 2.237983149866904

Epoch: 5| Step: 8
Training loss: 2.128387928009033
Validation loss: 2.2312371576986005

Epoch: 5| Step: 9
Training loss: 2.613358974456787
Validation loss: 2.226976366453273

Epoch: 5| Step: 10
Training loss: 2.119718551635742
Validation loss: 2.224091472164277

Epoch: 163| Step: 0
Training loss: 2.486353874206543
Validation loss: 2.2184124121101956

Epoch: 5| Step: 1
Training loss: 2.1441450119018555
Validation loss: 2.2277400365439792

Epoch: 5| Step: 2
Training loss: 2.109238386154175
Validation loss: 2.2341844548461256

Epoch: 5| Step: 3
Training loss: 3.1634764671325684
Validation loss: 2.243472017267699

Epoch: 5| Step: 4
Training loss: 2.993791341781616
Validation loss: 2.2579827142018143

Epoch: 5| Step: 5
Training loss: 1.9104083776474
Validation loss: 2.2604623712519163

Epoch: 5| Step: 6
Training loss: 1.750636339187622
Validation loss: 2.2651885812000563

Epoch: 5| Step: 7
Training loss: 1.9426708221435547
Validation loss: 2.23716898118296

Epoch: 5| Step: 8
Training loss: 2.826627016067505
Validation loss: 2.2238894970186296

Epoch: 5| Step: 9
Training loss: 2.534170627593994
Validation loss: 2.2131478260922175

Epoch: 5| Step: 10
Training loss: 3.185765504837036
Validation loss: 2.21666379128733

Epoch: 164| Step: 0
Training loss: 2.11033296585083
Validation loss: 2.2112842734142015

Epoch: 5| Step: 1
Training loss: 2.734606981277466
Validation loss: 2.212814202872656

Epoch: 5| Step: 2
Training loss: 1.8542991876602173
Validation loss: 2.2201974033027567

Epoch: 5| Step: 3
Training loss: 2.436039447784424
Validation loss: 2.2265074573537356

Epoch: 5| Step: 4
Training loss: 2.859105348587036
Validation loss: 2.216382083072457

Epoch: 5| Step: 5
Training loss: 2.9982805252075195
Validation loss: 2.214803740542422

Epoch: 5| Step: 6
Training loss: 1.9792922735214233
Validation loss: 2.2218002068099154

Epoch: 5| Step: 7
Training loss: 2.1290552616119385
Validation loss: 2.2274825085875807

Epoch: 5| Step: 8
Training loss: 2.8690543174743652
Validation loss: 2.232837850047696

Epoch: 5| Step: 9
Training loss: 2.2903294563293457
Validation loss: 2.247378533886325

Epoch: 5| Step: 10
Training loss: 2.6435985565185547
Validation loss: 2.2389315648745467

Epoch: 165| Step: 0
Training loss: 1.9220813512802124
Validation loss: 2.2343364530994045

Epoch: 5| Step: 1
Training loss: 2.313119411468506
Validation loss: 2.2357952056392545

Epoch: 5| Step: 2
Training loss: 2.6522533893585205
Validation loss: 2.221275037334811

Epoch: 5| Step: 3
Training loss: 2.6324639320373535
Validation loss: 2.225145552747993

Epoch: 5| Step: 4
Training loss: 1.8570787906646729
Validation loss: 2.2228006521860757

Epoch: 5| Step: 5
Training loss: 2.9094085693359375
Validation loss: 2.22732372437754

Epoch: 5| Step: 6
Training loss: 2.7479324340820312
Validation loss: 2.2297835452582246

Epoch: 5| Step: 7
Training loss: 2.746145009994507
Validation loss: 2.2240823238126692

Epoch: 5| Step: 8
Training loss: 1.6752465963363647
Validation loss: 2.219450076421102

Epoch: 5| Step: 9
Training loss: 2.7495951652526855
Validation loss: 2.226035584685623

Epoch: 5| Step: 10
Training loss: 2.571631669998169
Validation loss: 2.232830714153987

Epoch: 166| Step: 0
Training loss: 3.129507541656494
Validation loss: 2.2551512154199744

Epoch: 5| Step: 1
Training loss: 2.731207847595215
Validation loss: 2.278938688257689

Epoch: 5| Step: 2
Training loss: 2.394674777984619
Validation loss: 2.2754438730978195

Epoch: 5| Step: 3
Training loss: 2.3371894359588623
Validation loss: 2.26186377258711

Epoch: 5| Step: 4
Training loss: 2.4588963985443115
Validation loss: 2.2669486384237967

Epoch: 5| Step: 5
Training loss: 2.586491107940674
Validation loss: 2.2445107019075783

Epoch: 5| Step: 6
Training loss: 1.9288440942764282
Validation loss: 2.2308689291759203

Epoch: 5| Step: 7
Training loss: 2.1355810165405273
Validation loss: 2.235812576868201

Epoch: 5| Step: 8
Training loss: 2.1758227348327637
Validation loss: 2.222151828068559

Epoch: 5| Step: 9
Training loss: 2.2949044704437256
Validation loss: 2.228627702241303

Epoch: 5| Step: 10
Training loss: 2.5380859375
Validation loss: 2.2304499867141887

Epoch: 167| Step: 0
Training loss: 2.597221851348877
Validation loss: 2.2354759003526423

Epoch: 5| Step: 1
Training loss: 2.4587273597717285
Validation loss: 2.243824630655268

Epoch: 5| Step: 2
Training loss: 2.384819269180298
Validation loss: 2.243406170157976

Epoch: 5| Step: 3
Training loss: 1.779470443725586
Validation loss: 2.2229704164689585

Epoch: 5| Step: 4
Training loss: 2.6662158966064453
Validation loss: 2.218195107675368

Epoch: 5| Step: 5
Training loss: 2.3614439964294434
Validation loss: 2.2239188891585155

Epoch: 5| Step: 6
Training loss: 2.469773530960083
Validation loss: 2.238339821497599

Epoch: 5| Step: 7
Training loss: 2.3560993671417236
Validation loss: 2.258318357570197

Epoch: 5| Step: 8
Training loss: 3.1585958003997803
Validation loss: 2.256326808724352

Epoch: 5| Step: 9
Training loss: 2.796679735183716
Validation loss: 2.2556543298946914

Epoch: 5| Step: 10
Training loss: 1.517342448234558
Validation loss: 2.254482328250844

Epoch: 168| Step: 0
Training loss: 2.328730344772339
Validation loss: 2.240798527194608

Epoch: 5| Step: 1
Training loss: 1.761204481124878
Validation loss: 2.23254463236819

Epoch: 5| Step: 2
Training loss: 1.8452796936035156
Validation loss: 2.2198951833991596

Epoch: 5| Step: 3
Training loss: 2.7385573387145996
Validation loss: 2.2123805220409105

Epoch: 5| Step: 4
Training loss: 2.386927604675293
Validation loss: 2.205025892103872

Epoch: 5| Step: 5
Training loss: 2.1300933361053467
Validation loss: 2.1988902912344983

Epoch: 5| Step: 6
Training loss: 3.0909056663513184
Validation loss: 2.1930834452311196

Epoch: 5| Step: 7
Training loss: 2.538017749786377
Validation loss: 2.1944742189940585

Epoch: 5| Step: 8
Training loss: 3.2847797870635986
Validation loss: 2.230213285774313

Epoch: 5| Step: 9
Training loss: 2.4989912509918213
Validation loss: 2.2443435358744797

Epoch: 5| Step: 10
Training loss: 2.026254653930664
Validation loss: 2.261001422841062

Epoch: 169| Step: 0
Training loss: 2.49717378616333
Validation loss: 2.2686541900839856

Epoch: 5| Step: 1
Training loss: 2.31491756439209
Validation loss: 2.2828785732228267

Epoch: 5| Step: 2
Training loss: 2.3816051483154297
Validation loss: 2.2702755594766266

Epoch: 5| Step: 3
Training loss: 2.7169442176818848
Validation loss: 2.250795884798932

Epoch: 5| Step: 4
Training loss: 1.633504867553711
Validation loss: 2.235795300493958

Epoch: 5| Step: 5
Training loss: 2.371145486831665
Validation loss: 2.2295426732750347

Epoch: 5| Step: 6
Training loss: 2.374363660812378
Validation loss: 2.222536457482205

Epoch: 5| Step: 7
Training loss: 2.1837847232818604
Validation loss: 2.2197425467993623

Epoch: 5| Step: 8
Training loss: 2.9547934532165527
Validation loss: 2.204423014835645

Epoch: 5| Step: 9
Training loss: 2.3770461082458496
Validation loss: 2.221329682616777

Epoch: 5| Step: 10
Training loss: 3.044981002807617
Validation loss: 2.221523654076361

Epoch: 170| Step: 0
Training loss: 2.359337568283081
Validation loss: 2.226547861611971

Epoch: 5| Step: 1
Training loss: 2.579777240753174
Validation loss: 2.2245075946213095

Epoch: 5| Step: 2
Training loss: 2.357210636138916
Validation loss: 2.227965453619598

Epoch: 5| Step: 3
Training loss: 2.76123309135437
Validation loss: 2.2570027407779487

Epoch: 5| Step: 4
Training loss: 2.4885189533233643
Validation loss: 2.273740965832946

Epoch: 5| Step: 5
Training loss: 2.2796506881713867
Validation loss: 2.2550213157489734

Epoch: 5| Step: 6
Training loss: 2.2681005001068115
Validation loss: 2.2301446609599616

Epoch: 5| Step: 7
Training loss: 2.479255199432373
Validation loss: 2.2114293216377177

Epoch: 5| Step: 8
Training loss: 1.8882824182510376
Validation loss: 2.2035827418809295

Epoch: 5| Step: 9
Training loss: 2.7881247997283936
Validation loss: 2.203456419770436

Epoch: 5| Step: 10
Training loss: 2.3788504600524902
Validation loss: 2.1941243320383053

Epoch: 171| Step: 0
Training loss: 2.3014657497406006
Validation loss: 2.197494001798732

Epoch: 5| Step: 1
Training loss: 2.494129180908203
Validation loss: 2.2144441758432696

Epoch: 5| Step: 2
Training loss: 2.78338623046875
Validation loss: 2.24472874979819

Epoch: 5| Step: 3
Training loss: 2.2600204944610596
Validation loss: 2.2635861622389926

Epoch: 5| Step: 4
Training loss: 2.0067780017852783
Validation loss: 2.270128598777197

Epoch: 5| Step: 5
Training loss: 2.618905544281006
Validation loss: 2.249983613209058

Epoch: 5| Step: 6
Training loss: 2.049224615097046
Validation loss: 2.227043168519133

Epoch: 5| Step: 7
Training loss: 2.4846558570861816
Validation loss: 2.205583523678523

Epoch: 5| Step: 8
Training loss: 2.1445724964141846
Validation loss: 2.202973755457068

Epoch: 5| Step: 9
Training loss: 2.5750956535339355
Validation loss: 2.188172560866161

Epoch: 5| Step: 10
Training loss: 3.0454375743865967
Validation loss: 2.1950235546276136

Epoch: 172| Step: 0
Training loss: 2.708284854888916
Validation loss: 2.1968348231366885

Epoch: 5| Step: 1
Training loss: 2.3138484954833984
Validation loss: 2.1960074183761433

Epoch: 5| Step: 2
Training loss: 2.6428918838500977
Validation loss: 2.197194814682007

Epoch: 5| Step: 3
Training loss: 2.532952308654785
Validation loss: 2.1970444648496565

Epoch: 5| Step: 4
Training loss: 2.1572203636169434
Validation loss: 2.1933905975792998

Epoch: 5| Step: 5
Training loss: 2.5884151458740234
Validation loss: 2.202011143007586

Epoch: 5| Step: 6
Training loss: 2.6563572883605957
Validation loss: 2.2280345142528577

Epoch: 5| Step: 7
Training loss: 1.8885574340820312
Validation loss: 2.2205872048613844

Epoch: 5| Step: 8
Training loss: 3.0000576972961426
Validation loss: 2.19961186890961

Epoch: 5| Step: 9
Training loss: 1.9642314910888672
Validation loss: 2.185607938356297

Epoch: 5| Step: 10
Training loss: 2.421947479248047
Validation loss: 2.1672190402143743

Epoch: 173| Step: 0
Training loss: 2.0018584728240967
Validation loss: 2.168427223800331

Epoch: 5| Step: 1
Training loss: 2.297983169555664
Validation loss: 2.167504677208521

Epoch: 5| Step: 2
Training loss: 2.270643472671509
Validation loss: 2.168894653679222

Epoch: 5| Step: 3
Training loss: 2.5304226875305176
Validation loss: 2.1717678526396393

Epoch: 5| Step: 4
Training loss: 1.8563944101333618
Validation loss: 2.1656347487562444

Epoch: 5| Step: 5
Training loss: 2.2853386402130127
Validation loss: 2.168889166206442

Epoch: 5| Step: 6
Training loss: 2.7629995346069336
Validation loss: 2.1753927200071272

Epoch: 5| Step: 7
Training loss: 2.6143367290496826
Validation loss: 2.1718291262144684

Epoch: 5| Step: 8
Training loss: 2.6888463497161865
Validation loss: 2.1917097696693997

Epoch: 5| Step: 9
Training loss: 2.9177298545837402
Validation loss: 2.2016696007021013

Epoch: 5| Step: 10
Training loss: 2.410670042037964
Validation loss: 2.2151129245758057

Epoch: 174| Step: 0
Training loss: 2.5606610774993896
Validation loss: 2.2062009816528647

Epoch: 5| Step: 1
Training loss: 1.9723002910614014
Validation loss: 2.213931437461607

Epoch: 5| Step: 2
Training loss: 2.5057740211486816
Validation loss: 2.21066370830741

Epoch: 5| Step: 3
Training loss: 2.2795658111572266
Validation loss: 2.210259956698264

Epoch: 5| Step: 4
Training loss: 2.181145191192627
Validation loss: 2.1957882258199874

Epoch: 5| Step: 5
Training loss: 2.40177321434021
Validation loss: 2.1937380093400196

Epoch: 5| Step: 6
Training loss: 2.955810308456421
Validation loss: 2.19636042400073

Epoch: 5| Step: 7
Training loss: 2.516770124435425
Validation loss: 2.1902013978650494

Epoch: 5| Step: 8
Training loss: 2.0781993865966797
Validation loss: 2.196708074180029

Epoch: 5| Step: 9
Training loss: 2.483875274658203
Validation loss: 2.192803273918808

Epoch: 5| Step: 10
Training loss: 2.823694944381714
Validation loss: 2.193453928475739

Epoch: 175| Step: 0
Training loss: 2.063500165939331
Validation loss: 2.205691550367622

Epoch: 5| Step: 1
Training loss: 2.3422675132751465
Validation loss: 2.2161634263171943

Epoch: 5| Step: 2
Training loss: 2.085603713989258
Validation loss: 2.2230307825150026

Epoch: 5| Step: 3
Training loss: 2.8690879344940186
Validation loss: 2.226047782487767

Epoch: 5| Step: 4
Training loss: 2.7058138847351074
Validation loss: 2.2214491085339616

Epoch: 5| Step: 5
Training loss: 2.4604153633117676
Validation loss: 2.2127252035243536

Epoch: 5| Step: 6
Training loss: 1.9809986352920532
Validation loss: 2.2109740946882512

Epoch: 5| Step: 7
Training loss: 2.745368242263794
Validation loss: 2.190899843810707

Epoch: 5| Step: 8
Training loss: 2.446568727493286
Validation loss: 2.201471944009104

Epoch: 5| Step: 9
Training loss: 2.410759449005127
Validation loss: 2.184054228567308

Epoch: 5| Step: 10
Training loss: 2.3268587589263916
Validation loss: 2.1888022217699277

Epoch: 176| Step: 0
Training loss: 2.4913127422332764
Validation loss: 2.187362483752671

Epoch: 5| Step: 1
Training loss: 3.0422720909118652
Validation loss: 2.18727804512106

Epoch: 5| Step: 2
Training loss: 2.4675469398498535
Validation loss: 2.195172248348113

Epoch: 5| Step: 3
Training loss: 2.27958607673645
Validation loss: 2.1977103397410405

Epoch: 5| Step: 4
Training loss: 2.8511714935302734
Validation loss: 2.216188153912944

Epoch: 5| Step: 5
Training loss: 1.7359431982040405
Validation loss: 2.22453147621565

Epoch: 5| Step: 6
Training loss: 1.9208732843399048
Validation loss: 2.23489152103342

Epoch: 5| Step: 7
Training loss: 2.4623372554779053
Validation loss: 2.2297262619900446

Epoch: 5| Step: 8
Training loss: 1.9763438701629639
Validation loss: 2.2254761290806595

Epoch: 5| Step: 9
Training loss: 2.1492443084716797
Validation loss: 2.208393448142595

Epoch: 5| Step: 10
Training loss: 3.1467418670654297
Validation loss: 2.202832104057394

Epoch: 177| Step: 0
Training loss: 2.404942274093628
Validation loss: 2.2045979525453303

Epoch: 5| Step: 1
Training loss: 2.87654185295105
Validation loss: 2.2037167087677987

Epoch: 5| Step: 2
Training loss: 2.2528598308563232
Validation loss: 2.2119426496567263

Epoch: 5| Step: 3
Training loss: 2.0335853099823
Validation loss: 2.2102139547307003

Epoch: 5| Step: 4
Training loss: 3.0072078704833984
Validation loss: 2.2152537453559136

Epoch: 5| Step: 5
Training loss: 1.8599109649658203
Validation loss: 2.2055929489033197

Epoch: 5| Step: 6
Training loss: 2.2057831287384033
Validation loss: 2.224329540806432

Epoch: 5| Step: 7
Training loss: 2.3736748695373535
Validation loss: 2.206596932103557

Epoch: 5| Step: 8
Training loss: 2.6314377784729004
Validation loss: 2.199429342823644

Epoch: 5| Step: 9
Training loss: 2.1487600803375244
Validation loss: 2.1954346895217896

Epoch: 5| Step: 10
Training loss: 2.4840683937072754
Validation loss: 2.190751060362785

Epoch: 178| Step: 0
Training loss: 2.572436571121216
Validation loss: 2.1913034480105162

Epoch: 5| Step: 1
Training loss: 2.444733142852783
Validation loss: 2.192483191849083

Epoch: 5| Step: 2
Training loss: 2.3601675033569336
Validation loss: 2.1831408444271294

Epoch: 5| Step: 3
Training loss: 2.723071336746216
Validation loss: 2.1801631655744327

Epoch: 5| Step: 4
Training loss: 2.5382590293884277
Validation loss: 2.1799475659606276

Epoch: 5| Step: 5
Training loss: 2.385202646255493
Validation loss: 2.179303947315421

Epoch: 5| Step: 6
Training loss: 2.588592529296875
Validation loss: 2.1987572536673596

Epoch: 5| Step: 7
Training loss: 2.371533155441284
Validation loss: 2.2051846211956394

Epoch: 5| Step: 8
Training loss: 2.260535717010498
Validation loss: 2.203820046558175

Epoch: 5| Step: 9
Training loss: 1.8445589542388916
Validation loss: 2.1937605642503306

Epoch: 5| Step: 10
Training loss: 2.2377171516418457
Validation loss: 2.1973320720016316

Epoch: 179| Step: 0
Training loss: 1.849517583847046
Validation loss: 2.1813194726103093

Epoch: 5| Step: 1
Training loss: 2.5239977836608887
Validation loss: 2.185816950695489

Epoch: 5| Step: 2
Training loss: 2.7763302326202393
Validation loss: 2.196587929161646

Epoch: 5| Step: 3
Training loss: 2.51379132270813
Validation loss: 2.194867701940639

Epoch: 5| Step: 4
Training loss: 2.570927143096924
Validation loss: 2.2023408515478975

Epoch: 5| Step: 5
Training loss: 2.017165422439575
Validation loss: 2.1951932112375894

Epoch: 5| Step: 6
Training loss: 2.3546934127807617
Validation loss: 2.1939164759010397

Epoch: 5| Step: 7
Training loss: 2.2927236557006836
Validation loss: 2.198528943523284

Epoch: 5| Step: 8
Training loss: 2.7514617443084717
Validation loss: 2.202098766962687

Epoch: 5| Step: 9
Training loss: 2.351172685623169
Validation loss: 2.1918623575600247

Epoch: 5| Step: 10
Training loss: 2.3279261589050293
Validation loss: 2.1929997782553396

Epoch: 180| Step: 0
Training loss: 1.9683592319488525
Validation loss: 2.202104801772743

Epoch: 5| Step: 1
Training loss: 2.3200347423553467
Validation loss: 2.2015019078408518

Epoch: 5| Step: 2
Training loss: 2.1981253623962402
Validation loss: 2.2065243823553926

Epoch: 5| Step: 3
Training loss: 2.2378883361816406
Validation loss: 2.21795255009846

Epoch: 5| Step: 4
Training loss: 2.310746908187866
Validation loss: 2.208289433551091

Epoch: 5| Step: 5
Training loss: 2.445122480392456
Validation loss: 2.193710455330469

Epoch: 5| Step: 6
Training loss: 2.9039206504821777
Validation loss: 2.183232750943912

Epoch: 5| Step: 7
Training loss: 2.329288959503174
Validation loss: 2.182380094323107

Epoch: 5| Step: 8
Training loss: 2.5301783084869385
Validation loss: 2.1758422185015935

Epoch: 5| Step: 9
Training loss: 2.4190633296966553
Validation loss: 2.1920648069791895

Epoch: 5| Step: 10
Training loss: 2.530592918395996
Validation loss: 2.197481570705291

Epoch: 181| Step: 0
Training loss: 1.9950954914093018
Validation loss: 2.211159339515112

Epoch: 5| Step: 1
Training loss: 2.8101742267608643
Validation loss: 2.1922144582194667

Epoch: 5| Step: 2
Training loss: 1.8169664144515991
Validation loss: 2.203803559785248

Epoch: 5| Step: 3
Training loss: 2.0386784076690674
Validation loss: 2.20114403898998

Epoch: 5| Step: 4
Training loss: 2.4176266193389893
Validation loss: 2.20774588277263

Epoch: 5| Step: 5
Training loss: 2.9442741870880127
Validation loss: 2.1941153900597685

Epoch: 5| Step: 6
Training loss: 2.150773525238037
Validation loss: 2.206170510220271

Epoch: 5| Step: 7
Training loss: 3.0224955081939697
Validation loss: 2.2116688220731673

Epoch: 5| Step: 8
Training loss: 2.743914842605591
Validation loss: 2.1934180451977636

Epoch: 5| Step: 9
Training loss: 1.9033432006835938
Validation loss: 2.1948122068118026

Epoch: 5| Step: 10
Training loss: 2.249955654144287
Validation loss: 2.189060236818047

Epoch: 182| Step: 0
Training loss: 2.677525758743286
Validation loss: 2.175748904546102

Epoch: 5| Step: 1
Training loss: 2.560443162918091
Validation loss: 2.181892669329079

Epoch: 5| Step: 2
Training loss: 2.4949212074279785
Validation loss: 2.177878120894073

Epoch: 5| Step: 3
Training loss: 2.1291210651397705
Validation loss: 2.184069557856488

Epoch: 5| Step: 4
Training loss: 2.6273999214172363
Validation loss: 2.1849844071172897

Epoch: 5| Step: 5
Training loss: 2.1575891971588135
Validation loss: 2.194534678612986

Epoch: 5| Step: 6
Training loss: 2.054931879043579
Validation loss: 2.199666289873021

Epoch: 5| Step: 7
Training loss: 2.328178882598877
Validation loss: 2.2133315045346498

Epoch: 5| Step: 8
Training loss: 1.7822620868682861
Validation loss: 2.199518636990619

Epoch: 5| Step: 9
Training loss: 2.882720708847046
Validation loss: 2.21501572157747

Epoch: 5| Step: 10
Training loss: 2.3949778079986572
Validation loss: 2.1966776963203185

Epoch: 183| Step: 0
Training loss: 1.9526042938232422
Validation loss: 2.2038327519611647

Epoch: 5| Step: 1
Training loss: 2.380793809890747
Validation loss: 2.207173325682199

Epoch: 5| Step: 2
Training loss: 2.483708143234253
Validation loss: 2.218586826837191

Epoch: 5| Step: 3
Training loss: 2.20156192779541
Validation loss: 2.1969127898575156

Epoch: 5| Step: 4
Training loss: 2.462646245956421
Validation loss: 2.189368801732217

Epoch: 5| Step: 5
Training loss: 2.280616283416748
Validation loss: 2.183286537406265

Epoch: 5| Step: 6
Training loss: 2.264502763748169
Validation loss: 2.1859245979657738

Epoch: 5| Step: 7
Training loss: 2.4032466411590576
Validation loss: 2.187151009036649

Epoch: 5| Step: 8
Training loss: 2.6963422298431396
Validation loss: 2.2008563010923323

Epoch: 5| Step: 9
Training loss: 2.1126961708068848
Validation loss: 2.2125002158585416

Epoch: 5| Step: 10
Training loss: 2.8006649017333984
Validation loss: 2.2203070514945575

Epoch: 184| Step: 0
Training loss: 2.762394666671753
Validation loss: 2.2236681240861134

Epoch: 5| Step: 1
Training loss: 2.0485129356384277
Validation loss: 2.2179285967221825

Epoch: 5| Step: 2
Training loss: 2.2448296546936035
Validation loss: 2.2079910975630566

Epoch: 5| Step: 3
Training loss: 1.5522651672363281
Validation loss: 2.2014217786891486

Epoch: 5| Step: 4
Training loss: 2.668123245239258
Validation loss: 2.195750060901847

Epoch: 5| Step: 5
Training loss: 2.905116081237793
Validation loss: 2.197022038121377

Epoch: 5| Step: 6
Training loss: 2.6717517375946045
Validation loss: 2.208151768612605

Epoch: 5| Step: 7
Training loss: 2.3367984294891357
Validation loss: 2.1994725914411646

Epoch: 5| Step: 8
Training loss: 2.3072099685668945
Validation loss: 2.209812900071503

Epoch: 5| Step: 9
Training loss: 2.1434054374694824
Validation loss: 2.2179818537927445

Epoch: 5| Step: 10
Training loss: 2.5026912689208984
Validation loss: 2.2252232028592016

Epoch: 185| Step: 0
Training loss: 2.5549609661102295
Validation loss: 2.2441556684432493

Epoch: 5| Step: 1
Training loss: 2.7792835235595703
Validation loss: 2.260266186088644

Epoch: 5| Step: 2
Training loss: 2.0563111305236816
Validation loss: 2.2465161405583864

Epoch: 5| Step: 3
Training loss: 1.8423455953598022
Validation loss: 2.2416303029624363

Epoch: 5| Step: 4
Training loss: 1.8495594263076782
Validation loss: 2.2282367624262327

Epoch: 5| Step: 5
Training loss: 2.6344363689422607
Validation loss: 2.247774249763899

Epoch: 5| Step: 6
Training loss: 2.29239821434021
Validation loss: 2.235649775433284

Epoch: 5| Step: 7
Training loss: 2.5365166664123535
Validation loss: 2.2430953812855545

Epoch: 5| Step: 8
Training loss: 2.5705223083496094
Validation loss: 2.227999605158324

Epoch: 5| Step: 9
Training loss: 2.2295312881469727
Validation loss: 2.215433410418931

Epoch: 5| Step: 10
Training loss: 2.873507499694824
Validation loss: 2.204525375878939

Epoch: 186| Step: 0
Training loss: 3.123617172241211
Validation loss: 2.1877364330394293

Epoch: 5| Step: 1
Training loss: 2.065870761871338
Validation loss: 2.1633808151368172

Epoch: 5| Step: 2
Training loss: 2.7453513145446777
Validation loss: 2.1576299846813245

Epoch: 5| Step: 3
Training loss: 2.619882106781006
Validation loss: 2.1623914523791243

Epoch: 5| Step: 4
Training loss: 2.272219181060791
Validation loss: 2.203991033697641

Epoch: 5| Step: 5
Training loss: 1.8493635654449463
Validation loss: 2.2073960945170414

Epoch: 5| Step: 6
Training loss: 2.12247896194458
Validation loss: 2.208066012269707

Epoch: 5| Step: 7
Training loss: 2.8065128326416016
Validation loss: 2.20707619062034

Epoch: 5| Step: 8
Training loss: 2.285559892654419
Validation loss: 2.185709455961822

Epoch: 5| Step: 9
Training loss: 2.0571680068969727
Validation loss: 2.1636925000016407

Epoch: 5| Step: 10
Training loss: 1.9992307424545288
Validation loss: 2.1503003374222787

Epoch: 187| Step: 0
Training loss: 3.0416038036346436
Validation loss: 2.147853139908083

Epoch: 5| Step: 1
Training loss: 1.841538667678833
Validation loss: 2.151754403627047

Epoch: 5| Step: 2
Training loss: 2.3440003395080566
Validation loss: 2.1621835641963507

Epoch: 5| Step: 3
Training loss: 2.966965913772583
Validation loss: 2.18929140029415

Epoch: 5| Step: 4
Training loss: 2.0447938442230225
Validation loss: 2.2192867199579873

Epoch: 5| Step: 5
Training loss: 2.492731809616089
Validation loss: 2.216724552134032

Epoch: 5| Step: 6
Training loss: 2.4867660999298096
Validation loss: 2.2092335326697237

Epoch: 5| Step: 7
Training loss: 1.8326683044433594
Validation loss: 2.184880495071411

Epoch: 5| Step: 8
Training loss: 2.660762071609497
Validation loss: 2.1643246348186205

Epoch: 5| Step: 9
Training loss: 2.0601813793182373
Validation loss: 2.1590188908320602

Epoch: 5| Step: 10
Training loss: 2.210282802581787
Validation loss: 2.1547131102572203

Epoch: 188| Step: 0
Training loss: 2.4289534091949463
Validation loss: 2.148408878234125

Epoch: 5| Step: 1
Training loss: 2.2835464477539062
Validation loss: 2.153275277024956

Epoch: 5| Step: 2
Training loss: 2.2304177284240723
Validation loss: 2.1692473196214244

Epoch: 5| Step: 3
Training loss: 2.747981071472168
Validation loss: 2.185226091774561

Epoch: 5| Step: 4
Training loss: 2.222008228302002
Validation loss: 2.182455228221032

Epoch: 5| Step: 5
Training loss: 1.8722426891326904
Validation loss: 2.1708441536913634

Epoch: 5| Step: 6
Training loss: 2.4200167655944824
Validation loss: 2.1854237330857145

Epoch: 5| Step: 7
Training loss: 2.4945132732391357
Validation loss: 2.1762672137188654

Epoch: 5| Step: 8
Training loss: 2.5014984607696533
Validation loss: 2.201039465524817

Epoch: 5| Step: 9
Training loss: 2.037872314453125
Validation loss: 2.219534532998198

Epoch: 5| Step: 10
Training loss: 3.073211669921875
Validation loss: 2.213062076158421

Epoch: 189| Step: 0
Training loss: 1.6552379131317139
Validation loss: 2.212644402698804

Epoch: 5| Step: 1
Training loss: 2.816322088241577
Validation loss: 2.2034564428432013

Epoch: 5| Step: 2
Training loss: 2.0079891681671143
Validation loss: 2.1884820756091865

Epoch: 5| Step: 3
Training loss: 1.6101897954940796
Validation loss: 2.181075729349608

Epoch: 5| Step: 4
Training loss: 3.0132126808166504
Validation loss: 2.1752351714718725

Epoch: 5| Step: 5
Training loss: 2.4803268909454346
Validation loss: 2.170093849141111

Epoch: 5| Step: 6
Training loss: 1.9047319889068604
Validation loss: 2.1750900514664187

Epoch: 5| Step: 7
Training loss: 2.9697697162628174
Validation loss: 2.1749129910622873

Epoch: 5| Step: 8
Training loss: 2.6576690673828125
Validation loss: 2.182105379719888

Epoch: 5| Step: 9
Training loss: 2.632171154022217
Validation loss: 2.1806366879452943

Epoch: 5| Step: 10
Training loss: 1.985533595085144
Validation loss: 2.1859316492593415

Epoch: 190| Step: 0
Training loss: 2.299142837524414
Validation loss: 2.1849113407955376

Epoch: 5| Step: 1
Training loss: 2.207688808441162
Validation loss: 2.1911753762152886

Epoch: 5| Step: 2
Training loss: 1.4325056076049805
Validation loss: 2.196396117569298

Epoch: 5| Step: 3
Training loss: 3.055861234664917
Validation loss: 2.1859591186687513

Epoch: 5| Step: 4
Training loss: 2.0358424186706543
Validation loss: 2.1764090086824153

Epoch: 5| Step: 5
Training loss: 2.506042003631592
Validation loss: 2.1611042971252115

Epoch: 5| Step: 6
Training loss: 1.8850677013397217
Validation loss: 2.1654869202644593

Epoch: 5| Step: 7
Training loss: 2.44588303565979
Validation loss: 2.1715509404418287

Epoch: 5| Step: 8
Training loss: 2.2794244289398193
Validation loss: 2.1735790160394486

Epoch: 5| Step: 9
Training loss: 2.6757240295410156
Validation loss: 2.179123070932204

Epoch: 5| Step: 10
Training loss: 2.984285593032837
Validation loss: 2.1844557113544916

Epoch: 191| Step: 0
Training loss: 2.7350118160247803
Validation loss: 2.1813868245770855

Epoch: 5| Step: 1
Training loss: 1.8896013498306274
Validation loss: 2.1729743121772684

Epoch: 5| Step: 2
Training loss: 2.9001991748809814
Validation loss: 2.181241222607192

Epoch: 5| Step: 3
Training loss: 2.160090923309326
Validation loss: 2.179573243664157

Epoch: 5| Step: 4
Training loss: 2.418210744857788
Validation loss: 2.1910535827759774

Epoch: 5| Step: 5
Training loss: 2.177049160003662
Validation loss: 2.17972804141301

Epoch: 5| Step: 6
Training loss: 2.431076765060425
Validation loss: 2.1961334315679406

Epoch: 5| Step: 7
Training loss: 1.9849052429199219
Validation loss: 2.1968884288623767

Epoch: 5| Step: 8
Training loss: 2.1340765953063965
Validation loss: 2.1961727578152894

Epoch: 5| Step: 9
Training loss: 2.206456422805786
Validation loss: 2.214717012579723

Epoch: 5| Step: 10
Training loss: 2.5356485843658447
Validation loss: 2.1942966112526516

Epoch: 192| Step: 0
Training loss: 2.5370898246765137
Validation loss: 2.2190527403226463

Epoch: 5| Step: 1
Training loss: 2.349186420440674
Validation loss: 2.1863051127361994

Epoch: 5| Step: 2
Training loss: 2.2478692531585693
Validation loss: 2.1646082324366414

Epoch: 5| Step: 3
Training loss: 2.3992960453033447
Validation loss: 2.153667660169704

Epoch: 5| Step: 4
Training loss: 2.4492926597595215
Validation loss: 2.1616275374607374

Epoch: 5| Step: 5
Training loss: 2.438401460647583
Validation loss: 2.162053936271257

Epoch: 5| Step: 6
Training loss: 2.259446859359741
Validation loss: 2.1556498209635415

Epoch: 5| Step: 7
Training loss: 2.1618270874023438
Validation loss: 2.1571060149900374

Epoch: 5| Step: 8
Training loss: 2.550079345703125
Validation loss: 2.1514573302320255

Epoch: 5| Step: 9
Training loss: 2.3930435180664062
Validation loss: 2.158956285445921

Epoch: 5| Step: 10
Training loss: 1.758336067199707
Validation loss: 2.157348443103093

Epoch: 193| Step: 0
Training loss: 2.095517635345459
Validation loss: 2.1909125927955873

Epoch: 5| Step: 1
Training loss: 2.3005449771881104
Validation loss: 2.2342220455087642

Epoch: 5| Step: 2
Training loss: 2.821296215057373
Validation loss: 2.2706001291992846

Epoch: 5| Step: 3
Training loss: 2.2003493309020996
Validation loss: 2.285979332462434

Epoch: 5| Step: 4
Training loss: 3.1755528450012207
Validation loss: 2.2654414202577327

Epoch: 5| Step: 5
Training loss: 1.576319694519043
Validation loss: 2.212504215137933

Epoch: 5| Step: 6
Training loss: 2.4313948154449463
Validation loss: 2.1924872424012873

Epoch: 5| Step: 7
Training loss: 2.0330445766448975
Validation loss: 2.1782138347625732

Epoch: 5| Step: 8
Training loss: 2.876208543777466
Validation loss: 2.1717406857398247

Epoch: 5| Step: 9
Training loss: 2.2661736011505127
Validation loss: 2.1901672758081907

Epoch: 5| Step: 10
Training loss: 1.9670852422714233
Validation loss: 2.186762027843024

Epoch: 194| Step: 0
Training loss: 2.382209062576294
Validation loss: 2.1731839590175177

Epoch: 5| Step: 1
Training loss: 2.1125588417053223
Validation loss: 2.1701245923196115

Epoch: 5| Step: 2
Training loss: 1.9074195623397827
Validation loss: 2.160701063371474

Epoch: 5| Step: 3
Training loss: 1.8800386190414429
Validation loss: 2.1491097199019564

Epoch: 5| Step: 4
Training loss: 3.006258487701416
Validation loss: 2.150630138253653

Epoch: 5| Step: 5
Training loss: 2.534971237182617
Validation loss: 2.1464591513397875

Epoch: 5| Step: 6
Training loss: 1.8673031330108643
Validation loss: 2.1650708516438804

Epoch: 5| Step: 7
Training loss: 2.566253423690796
Validation loss: 2.179210908951298

Epoch: 5| Step: 8
Training loss: 2.3311374187469482
Validation loss: 2.2163645118795414

Epoch: 5| Step: 9
Training loss: 2.527891159057617
Validation loss: 2.2255816152018886

Epoch: 5| Step: 10
Training loss: 2.533092737197876
Validation loss: 2.2184818713895735

Epoch: 195| Step: 0
Training loss: 2.3105573654174805
Validation loss: 2.2219302372265886

Epoch: 5| Step: 1
Training loss: 1.8864376544952393
Validation loss: 2.2209113233832904

Epoch: 5| Step: 2
Training loss: 2.21278977394104
Validation loss: 2.2334411810803156

Epoch: 5| Step: 3
Training loss: 2.0486364364624023
Validation loss: 2.2410886185143584

Epoch: 5| Step: 4
Training loss: 2.278998851776123
Validation loss: 2.2126285786269815

Epoch: 5| Step: 5
Training loss: 2.2462637424468994
Validation loss: 2.202688245363133

Epoch: 5| Step: 6
Training loss: 3.2362780570983887
Validation loss: 2.187736847067392

Epoch: 5| Step: 7
Training loss: 2.3493306636810303
Validation loss: 2.200350897286528

Epoch: 5| Step: 8
Training loss: 2.2596940994262695
Validation loss: 2.2157487574444024

Epoch: 5| Step: 9
Training loss: 2.93229341506958
Validation loss: 2.225295953853156

Epoch: 5| Step: 10
Training loss: 1.9821805953979492
Validation loss: 2.2122873926675446

Epoch: 196| Step: 0
Training loss: 2.955118417739868
Validation loss: 2.2058283257228073

Epoch: 5| Step: 1
Training loss: 2.2896499633789062
Validation loss: 2.202294406070504

Epoch: 5| Step: 2
Training loss: 2.1810314655303955
Validation loss: 2.1796393753379903

Epoch: 5| Step: 3
Training loss: 1.9319655895233154
Validation loss: 2.1665051547429894

Epoch: 5| Step: 4
Training loss: 2.321877956390381
Validation loss: 2.1441562714115268

Epoch: 5| Step: 5
Training loss: 2.3626790046691895
Validation loss: 2.166410292348554

Epoch: 5| Step: 6
Training loss: 2.5646541118621826
Validation loss: 2.208672683726075

Epoch: 5| Step: 7
Training loss: 2.1696159839630127
Validation loss: 2.225187666954533

Epoch: 5| Step: 8
Training loss: 2.3304553031921387
Validation loss: 2.2584656361610658

Epoch: 5| Step: 9
Training loss: 2.145369052886963
Validation loss: 2.2491790504865747

Epoch: 5| Step: 10
Training loss: 2.866631269454956
Validation loss: 2.2265339051523516

Epoch: 197| Step: 0
Training loss: 2.0044665336608887
Validation loss: 2.2002741521404636

Epoch: 5| Step: 1
Training loss: 2.0965960025787354
Validation loss: 2.1899815220986643

Epoch: 5| Step: 2
Training loss: 2.559518814086914
Validation loss: 2.175323463255359

Epoch: 5| Step: 3
Training loss: 2.4466869831085205
Validation loss: 2.185444413974721

Epoch: 5| Step: 4
Training loss: 2.3598573207855225
Validation loss: 2.18831681307926

Epoch: 5| Step: 5
Training loss: 2.0394318103790283
Validation loss: 2.1922919391303934

Epoch: 5| Step: 6
Training loss: 2.364763021469116
Validation loss: 2.2053683368108605

Epoch: 5| Step: 7
Training loss: 2.543492555618286
Validation loss: 2.1987942931472615

Epoch: 5| Step: 8
Training loss: 2.4512953758239746
Validation loss: 2.2062656469242548

Epoch: 5| Step: 9
Training loss: 2.399364471435547
Validation loss: 2.2019650807944675

Epoch: 5| Step: 10
Training loss: 2.284273862838745
Validation loss: 2.1895789279732654

Epoch: 198| Step: 0
Training loss: 1.709633231163025
Validation loss: 2.2048041897435344

Epoch: 5| Step: 1
Training loss: 2.206555128097534
Validation loss: 2.1796489351539203

Epoch: 5| Step: 2
Training loss: 2.4639525413513184
Validation loss: 2.203690416069441

Epoch: 5| Step: 3
Training loss: 1.7286837100982666
Validation loss: 2.2098915551298406

Epoch: 5| Step: 4
Training loss: 2.5924367904663086
Validation loss: 2.193211455498972

Epoch: 5| Step: 5
Training loss: 2.557007312774658
Validation loss: 2.1948550362740793

Epoch: 5| Step: 6
Training loss: 2.2275853157043457
Validation loss: 2.192821530885594

Epoch: 5| Step: 7
Training loss: 2.679534435272217
Validation loss: 2.194475120113742

Epoch: 5| Step: 8
Training loss: 2.2292065620422363
Validation loss: 2.1816094370298487

Epoch: 5| Step: 9
Training loss: 2.1416611671447754
Validation loss: 2.172101464322818

Epoch: 5| Step: 10
Training loss: 2.930288314819336
Validation loss: 2.1684794989965295

Epoch: 199| Step: 0
Training loss: 2.543435573577881
Validation loss: 2.151123264784454

Epoch: 5| Step: 1
Training loss: 2.1915640830993652
Validation loss: 2.1481240603231613

Epoch: 5| Step: 2
Training loss: 2.48001766204834
Validation loss: 2.14951943197558

Epoch: 5| Step: 3
Training loss: 2.6345114707946777
Validation loss: 2.137060378187446

Epoch: 5| Step: 4
Training loss: 2.4032013416290283
Validation loss: 2.136623077495124

Epoch: 5| Step: 5
Training loss: 2.26971173286438
Validation loss: 2.1363406053153415

Epoch: 5| Step: 6
Training loss: 2.009326696395874
Validation loss: 2.1525461443008913

Epoch: 5| Step: 7
Training loss: 2.0805282592773438
Validation loss: 2.1585867328028523

Epoch: 5| Step: 8
Training loss: 2.3305678367614746
Validation loss: 2.1572804348443144

Epoch: 5| Step: 9
Training loss: 2.138577938079834
Validation loss: 2.174816159791844

Epoch: 5| Step: 10
Training loss: 2.307137966156006
Validation loss: 2.1810378592501403

Epoch: 200| Step: 0
Training loss: 2.6842758655548096
Validation loss: 2.175641682840163

Epoch: 5| Step: 1
Training loss: 2.079288959503174
Validation loss: 2.175245100452054

Epoch: 5| Step: 2
Training loss: 1.9659903049468994
Validation loss: 2.167327061776192

Epoch: 5| Step: 3
Training loss: 2.4591033458709717
Validation loss: 2.1737000711502565

Epoch: 5| Step: 4
Training loss: 2.115257978439331
Validation loss: 2.15268232745509

Epoch: 5| Step: 5
Training loss: 2.2109832763671875
Validation loss: 2.16289290817835

Epoch: 5| Step: 6
Training loss: 1.9534454345703125
Validation loss: 2.1462594950070946

Epoch: 5| Step: 7
Training loss: 2.2746124267578125
Validation loss: 2.147574552925684

Epoch: 5| Step: 8
Training loss: 2.70717191696167
Validation loss: 2.156599208872805

Epoch: 5| Step: 9
Training loss: 2.4965155124664307
Validation loss: 2.155230268355339

Epoch: 5| Step: 10
Training loss: 2.2256906032562256
Validation loss: 2.164041596074258

Epoch: 201| Step: 0
Training loss: 2.3824515342712402
Validation loss: 2.15658115571545

Epoch: 5| Step: 1
Training loss: 2.705467939376831
Validation loss: 2.168899110568467

Epoch: 5| Step: 2
Training loss: 2.3855273723602295
Validation loss: 2.1501322138694023

Epoch: 5| Step: 3
Training loss: 2.2987265586853027
Validation loss: 2.1552157850675684

Epoch: 5| Step: 4
Training loss: 2.1701478958129883
Validation loss: 2.1423216865908716

Epoch: 5| Step: 5
Training loss: 1.7555030584335327
Validation loss: 2.1509221805039274

Epoch: 5| Step: 6
Training loss: 2.4640488624572754
Validation loss: 2.1608911714246197

Epoch: 5| Step: 7
Training loss: 2.455106735229492
Validation loss: 2.181652548492596

Epoch: 5| Step: 8
Training loss: 2.345078945159912
Validation loss: 2.1712457967060868

Epoch: 5| Step: 9
Training loss: 2.298964738845825
Validation loss: 2.1700883450046664

Epoch: 5| Step: 10
Training loss: 1.6474262475967407
Validation loss: 2.1765026302747827

Epoch: 202| Step: 0
Training loss: 1.806703805923462
Validation loss: 2.1578415952703005

Epoch: 5| Step: 1
Training loss: 1.7031126022338867
Validation loss: 2.1789485280231764

Epoch: 5| Step: 2
Training loss: 2.234370708465576
Validation loss: 2.180849946955199

Epoch: 5| Step: 3
Training loss: 2.523731231689453
Validation loss: 2.2163269314714658

Epoch: 5| Step: 4
Training loss: 3.368690013885498
Validation loss: 2.2500032968418573

Epoch: 5| Step: 5
Training loss: 2.3908402919769287
Validation loss: 2.273388503700174

Epoch: 5| Step: 6
Training loss: 1.5989201068878174
Validation loss: 2.2409543016905427

Epoch: 5| Step: 7
Training loss: 2.320016384124756
Validation loss: 2.2104192472273305

Epoch: 5| Step: 8
Training loss: 2.0388755798339844
Validation loss: 2.182112870677825

Epoch: 5| Step: 9
Training loss: 3.252005100250244
Validation loss: 2.1767233853699057

Epoch: 5| Step: 10
Training loss: 2.2346813678741455
Validation loss: 2.1810509876538346

Epoch: 203| Step: 0
Training loss: 2.683651924133301
Validation loss: 2.1911794780403056

Epoch: 5| Step: 1
Training loss: 1.6760295629501343
Validation loss: 2.220591498959449

Epoch: 5| Step: 2
Training loss: 2.490347385406494
Validation loss: 2.245300646751158

Epoch: 5| Step: 3
Training loss: 2.1449079513549805
Validation loss: 2.2252429890376266

Epoch: 5| Step: 4
Training loss: 2.3045120239257812
Validation loss: 2.2183026498363865

Epoch: 5| Step: 5
Training loss: 2.6582720279693604
Validation loss: 2.191607657299247

Epoch: 5| Step: 6
Training loss: 2.5519213676452637
Validation loss: 2.1538007208096084

Epoch: 5| Step: 7
Training loss: 2.042081117630005
Validation loss: 2.152099665775094

Epoch: 5| Step: 8
Training loss: 2.8148839473724365
Validation loss: 2.1662519977938746

Epoch: 5| Step: 9
Training loss: 1.593583345413208
Validation loss: 2.177693005531065

Epoch: 5| Step: 10
Training loss: 2.702798843383789
Validation loss: 2.179538430706147

Epoch: 204| Step: 0
Training loss: 2.200735569000244
Validation loss: 2.181120672533589

Epoch: 5| Step: 1
Training loss: 2.5764212608337402
Validation loss: 2.1589414996485554

Epoch: 5| Step: 2
Training loss: 2.982609510421753
Validation loss: 2.136642676527782

Epoch: 5| Step: 3
Training loss: 1.6117477416992188
Validation loss: 2.132161522424349

Epoch: 5| Step: 4
Training loss: 1.6243135929107666
Validation loss: 2.1362061295458066

Epoch: 5| Step: 5
Training loss: 2.464686632156372
Validation loss: 2.153131810567712

Epoch: 5| Step: 6
Training loss: 2.3886170387268066
Validation loss: 2.161687650988179

Epoch: 5| Step: 7
Training loss: 1.9378623962402344
Validation loss: 2.176493754950903

Epoch: 5| Step: 8
Training loss: 2.619443655014038
Validation loss: 2.179292832651446

Epoch: 5| Step: 9
Training loss: 2.8015918731689453
Validation loss: 2.2024923819367603

Epoch: 5| Step: 10
Training loss: 1.863035798072815
Validation loss: 2.193699718803488

Epoch: 205| Step: 0
Training loss: 1.948937177658081
Validation loss: 2.2038089498396842

Epoch: 5| Step: 1
Training loss: 2.6609292030334473
Validation loss: 2.2042560526119765

Epoch: 5| Step: 2
Training loss: 2.2444968223571777
Validation loss: 2.2034394125784598

Epoch: 5| Step: 3
Training loss: 1.8499542474746704
Validation loss: 2.174936500928735

Epoch: 5| Step: 4
Training loss: 2.8442130088806152
Validation loss: 2.1739751062085553

Epoch: 5| Step: 5
Training loss: 2.322868824005127
Validation loss: 2.1576763045403267

Epoch: 5| Step: 6
Training loss: 2.084929943084717
Validation loss: 2.156769043655806

Epoch: 5| Step: 7
Training loss: 1.99334716796875
Validation loss: 2.1669142348791963

Epoch: 5| Step: 8
Training loss: 2.600198984146118
Validation loss: 2.1684319255172566

Epoch: 5| Step: 9
Training loss: 2.218010663986206
Validation loss: 2.167239204529793

Epoch: 5| Step: 10
Training loss: 2.3333468437194824
Validation loss: 2.1684594820904475

Epoch: 206| Step: 0
Training loss: 2.350696563720703
Validation loss: 2.1669239690226894

Epoch: 5| Step: 1
Training loss: 1.711025595664978
Validation loss: 2.1545707359108874

Epoch: 5| Step: 2
Training loss: 2.114366054534912
Validation loss: 2.153884087839434

Epoch: 5| Step: 3
Training loss: 2.5804953575134277
Validation loss: 2.137042184029856

Epoch: 5| Step: 4
Training loss: 1.7644790410995483
Validation loss: 2.146619983898696

Epoch: 5| Step: 5
Training loss: 2.6980082988739014
Validation loss: 2.1521677060793807

Epoch: 5| Step: 6
Training loss: 2.2213499546051025
Validation loss: 2.1707456214453584

Epoch: 5| Step: 7
Training loss: 1.6601130962371826
Validation loss: 2.182324774803654

Epoch: 5| Step: 8
Training loss: 2.9683876037597656
Validation loss: 2.2091327892836703

Epoch: 5| Step: 9
Training loss: 2.086568832397461
Validation loss: 2.2285876222836074

Epoch: 5| Step: 10
Training loss: 3.1054916381835938
Validation loss: 2.196492718112084

Epoch: 207| Step: 0
Training loss: 2.9686129093170166
Validation loss: 2.199892977232574

Epoch: 5| Step: 1
Training loss: 2.2778587341308594
Validation loss: 2.205792575754145

Epoch: 5| Step: 2
Training loss: 2.117311716079712
Validation loss: 2.1939440799015824

Epoch: 5| Step: 3
Training loss: 2.2891736030578613
Validation loss: 2.1880553691617903

Epoch: 5| Step: 4
Training loss: 2.0231494903564453
Validation loss: 2.1725075655086066

Epoch: 5| Step: 5
Training loss: 2.2499351501464844
Validation loss: 2.1517360992329095

Epoch: 5| Step: 6
Training loss: 1.9379055500030518
Validation loss: 2.1444826818281606

Epoch: 5| Step: 7
Training loss: 2.1127724647521973
Validation loss: 2.1306205282929125

Epoch: 5| Step: 8
Training loss: 2.1407082080841064
Validation loss: 2.1152096768861175

Epoch: 5| Step: 9
Training loss: 2.1947169303894043
Validation loss: 2.114624218274188

Epoch: 5| Step: 10
Training loss: 2.679410934448242
Validation loss: 2.123063915519304

Epoch: 208| Step: 0
Training loss: 2.08382511138916
Validation loss: 2.1312727210342244

Epoch: 5| Step: 1
Training loss: 2.581179141998291
Validation loss: 2.136832150079871

Epoch: 5| Step: 2
Training loss: 2.231421947479248
Validation loss: 2.1183081403855355

Epoch: 5| Step: 3
Training loss: 1.9617431163787842
Validation loss: 2.125439202913674

Epoch: 5| Step: 4
Training loss: 2.030719757080078
Validation loss: 2.125563390793339

Epoch: 5| Step: 5
Training loss: 2.1525776386260986
Validation loss: 2.133989216178976

Epoch: 5| Step: 6
Training loss: 2.152660369873047
Validation loss: 2.140033798833047

Epoch: 5| Step: 7
Training loss: 2.348428726196289
Validation loss: 2.138978588965631

Epoch: 5| Step: 8
Training loss: 2.2146763801574707
Validation loss: 2.13904030476847

Epoch: 5| Step: 9
Training loss: 2.582117795944214
Validation loss: 2.15308315010481

Epoch: 5| Step: 10
Training loss: 2.6373367309570312
Validation loss: 2.162259458213724

Epoch: 209| Step: 0
Training loss: 1.9550155401229858
Validation loss: 2.1598690966124177

Epoch: 5| Step: 1
Training loss: 2.5084493160247803
Validation loss: 2.1741133505298245

Epoch: 5| Step: 2
Training loss: 2.443965196609497
Validation loss: 2.15371940212865

Epoch: 5| Step: 3
Training loss: 2.2673885822296143
Validation loss: 2.1606396846873785

Epoch: 5| Step: 4
Training loss: 2.2592458724975586
Validation loss: 2.170285878642913

Epoch: 5| Step: 5
Training loss: 2.2278285026550293
Validation loss: 2.16783171315347

Epoch: 5| Step: 6
Training loss: 1.712585210800171
Validation loss: 2.1691994590144

Epoch: 5| Step: 7
Training loss: 2.053292989730835
Validation loss: 2.18213181213666

Epoch: 5| Step: 8
Training loss: 2.6332736015319824
Validation loss: 2.185017611390801

Epoch: 5| Step: 9
Training loss: 2.114840269088745
Validation loss: 2.1870089936000046

Epoch: 5| Step: 10
Training loss: 2.3777337074279785
Validation loss: 2.177984776035432

Epoch: 210| Step: 0
Training loss: 1.9984502792358398
Validation loss: 2.1881737247590096

Epoch: 5| Step: 1
Training loss: 2.792165756225586
Validation loss: 2.1745908068072413

Epoch: 5| Step: 2
Training loss: 2.4651217460632324
Validation loss: 2.179883800527101

Epoch: 5| Step: 3
Training loss: 2.343015670776367
Validation loss: 2.187408411374656

Epoch: 5| Step: 4
Training loss: 1.720337152481079
Validation loss: 2.1800104930836666

Epoch: 5| Step: 5
Training loss: 2.7193198204040527
Validation loss: 2.1744594830338673

Epoch: 5| Step: 6
Training loss: 1.8051992654800415
Validation loss: 2.17631620489141

Epoch: 5| Step: 7
Training loss: 1.8663524389266968
Validation loss: 2.1658333732235815

Epoch: 5| Step: 8
Training loss: 2.172969341278076
Validation loss: 2.153787471914804

Epoch: 5| Step: 9
Training loss: 2.4030163288116455
Validation loss: 2.1538977494803806

Epoch: 5| Step: 10
Training loss: 2.2542927265167236
Validation loss: 2.151019832139374

Epoch: 211| Step: 0
Training loss: 1.5539239645004272
Validation loss: 2.143203704587875

Epoch: 5| Step: 1
Training loss: 1.9201698303222656
Validation loss: 2.136211049172186

Epoch: 5| Step: 2
Training loss: 2.079712390899658
Validation loss: 2.132314514088374

Epoch: 5| Step: 3
Training loss: 2.069031238555908
Validation loss: 2.1276987252696866

Epoch: 5| Step: 4
Training loss: 1.999066948890686
Validation loss: 2.1258115101886053

Epoch: 5| Step: 5
Training loss: 2.364206314086914
Validation loss: 2.1276600899234897

Epoch: 5| Step: 6
Training loss: 2.1175379753112793
Validation loss: 2.139534076054891

Epoch: 5| Step: 7
Training loss: 2.9344964027404785
Validation loss: 2.156280848287767

Epoch: 5| Step: 8
Training loss: 2.6630427837371826
Validation loss: 2.1583121873999156

Epoch: 5| Step: 9
Training loss: 2.445375919342041
Validation loss: 2.1716472436023015

Epoch: 5| Step: 10
Training loss: 2.401421308517456
Validation loss: 2.1598219897157405

Epoch: 212| Step: 0
Training loss: 1.9629766941070557
Validation loss: 2.1475299019967355

Epoch: 5| Step: 1
Training loss: 2.0231339931488037
Validation loss: 2.169054021117508

Epoch: 5| Step: 2
Training loss: 2.194477081298828
Validation loss: 2.1730885890222367

Epoch: 5| Step: 3
Training loss: 1.9620939493179321
Validation loss: 2.1894002550391742

Epoch: 5| Step: 4
Training loss: 2.2334096431732178
Validation loss: 2.169913770050131

Epoch: 5| Step: 5
Training loss: 2.5906524658203125
Validation loss: 2.161740285094066

Epoch: 5| Step: 6
Training loss: 1.6935666799545288
Validation loss: 2.1553596809346187

Epoch: 5| Step: 7
Training loss: 2.227525472640991
Validation loss: 2.134187730409766

Epoch: 5| Step: 8
Training loss: 2.8022029399871826
Validation loss: 2.140095403117518

Epoch: 5| Step: 9
Training loss: 2.3589539527893066
Validation loss: 2.143002522889004

Epoch: 5| Step: 10
Training loss: 2.4936912059783936
Validation loss: 2.134706797138337

Epoch: 213| Step: 0
Training loss: 2.3553881645202637
Validation loss: 2.132048676090856

Epoch: 5| Step: 1
Training loss: 2.4362308979034424
Validation loss: 2.1190037650446736

Epoch: 5| Step: 2
Training loss: 2.1235697269439697
Validation loss: 2.1169787850431216

Epoch: 5| Step: 3
Training loss: 2.1726090908050537
Validation loss: 2.108982652746221

Epoch: 5| Step: 4
Training loss: 2.054002285003662
Validation loss: 2.112741167827319

Epoch: 5| Step: 5
Training loss: 1.4903819561004639
Validation loss: 2.1184677154787126

Epoch: 5| Step: 6
Training loss: 2.4459736347198486
Validation loss: 2.1186679896487983

Epoch: 5| Step: 7
Training loss: 2.516556739807129
Validation loss: 2.120462648330196

Epoch: 5| Step: 8
Training loss: 1.9520959854125977
Validation loss: 2.1292874402897333

Epoch: 5| Step: 9
Training loss: 2.430921792984009
Validation loss: 2.13173153579876

Epoch: 5| Step: 10
Training loss: 2.450587511062622
Validation loss: 2.1370226811337214

Epoch: 214| Step: 0
Training loss: 1.965571641921997
Validation loss: 2.150374867582834

Epoch: 5| Step: 1
Training loss: 2.4593658447265625
Validation loss: 2.1313662362355057

Epoch: 5| Step: 2
Training loss: 1.9253956079483032
Validation loss: 2.1267321866045714

Epoch: 5| Step: 3
Training loss: 2.2217941284179688
Validation loss: 2.135711193084717

Epoch: 5| Step: 4
Training loss: 2.05124831199646
Validation loss: 2.1314582363251717

Epoch: 5| Step: 5
Training loss: 2.302396059036255
Validation loss: 2.126205587899813

Epoch: 5| Step: 6
Training loss: 1.928816556930542
Validation loss: 2.1321415747365644

Epoch: 5| Step: 7
Training loss: 2.5047945976257324
Validation loss: 2.1346301083923667

Epoch: 5| Step: 8
Training loss: 1.833080530166626
Validation loss: 2.142993357873732

Epoch: 5| Step: 9
Training loss: 2.5253255367279053
Validation loss: 2.1481800489528204

Epoch: 5| Step: 10
Training loss: 2.7350687980651855
Validation loss: 2.1515596182115617

Epoch: 215| Step: 0
Training loss: 3.043412446975708
Validation loss: 2.1708682634497203

Epoch: 5| Step: 1
Training loss: 2.6804778575897217
Validation loss: 2.16658716817056

Epoch: 5| Step: 2
Training loss: 1.639566421508789
Validation loss: 2.167357490908715

Epoch: 5| Step: 3
Training loss: 2.432145357131958
Validation loss: 2.1577087140852407

Epoch: 5| Step: 4
Training loss: 1.5088565349578857
Validation loss: 2.1641329001354914

Epoch: 5| Step: 5
Training loss: 2.3596601486206055
Validation loss: 2.154074489429433

Epoch: 5| Step: 6
Training loss: 1.9278093576431274
Validation loss: 2.159479811627378

Epoch: 5| Step: 7
Training loss: 2.5966567993164062
Validation loss: 2.1677627460930937

Epoch: 5| Step: 8
Training loss: 1.7651067972183228
Validation loss: 2.172212996790486

Epoch: 5| Step: 9
Training loss: 2.37097430229187
Validation loss: 2.1626875836362123

Epoch: 5| Step: 10
Training loss: 2.0081405639648438
Validation loss: 2.1461369094028266

Epoch: 216| Step: 0
Training loss: 2.345332384109497
Validation loss: 2.1554420391718545

Epoch: 5| Step: 1
Training loss: 1.8281577825546265
Validation loss: 2.134791862580084

Epoch: 5| Step: 2
Training loss: 1.876023292541504
Validation loss: 2.153909352517897

Epoch: 5| Step: 3
Training loss: 2.0228447914123535
Validation loss: 2.151697648468838

Epoch: 5| Step: 4
Training loss: 2.061863899230957
Validation loss: 2.1736616165407243

Epoch: 5| Step: 5
Training loss: 2.8350563049316406
Validation loss: 2.168857423208093

Epoch: 5| Step: 6
Training loss: 2.119614362716675
Validation loss: 2.1500183856615456

Epoch: 5| Step: 7
Training loss: 2.1192493438720703
Validation loss: 2.147429773884435

Epoch: 5| Step: 8
Training loss: 2.1364760398864746
Validation loss: 2.119447236420006

Epoch: 5| Step: 9
Training loss: 2.533357620239258
Validation loss: 2.114995648784022

Epoch: 5| Step: 10
Training loss: 2.4906556606292725
Validation loss: 2.1202931788659867

Epoch: 217| Step: 0
Training loss: 2.607140302658081
Validation loss: 2.131913504292888

Epoch: 5| Step: 1
Training loss: 2.4134631156921387
Validation loss: 2.1419216176514984

Epoch: 5| Step: 2
Training loss: 2.4581003189086914
Validation loss: 2.132509717377283

Epoch: 5| Step: 3
Training loss: 2.067836046218872
Validation loss: 2.112588272299818

Epoch: 5| Step: 4
Training loss: 2.5796923637390137
Validation loss: 2.091074110359274

Epoch: 5| Step: 5
Training loss: 1.3980709314346313
Validation loss: 2.080593280894782

Epoch: 5| Step: 6
Training loss: 2.3110461235046387
Validation loss: 2.0881013895875666

Epoch: 5| Step: 7
Training loss: 2.212825059890747
Validation loss: 2.087476384255194

Epoch: 5| Step: 8
Training loss: 2.234589099884033
Validation loss: 2.0805011731322094

Epoch: 5| Step: 9
Training loss: 2.0048184394836426
Validation loss: 2.100648510840631

Epoch: 5| Step: 10
Training loss: 2.0199592113494873
Validation loss: 2.097982921907979

Epoch: 218| Step: 0
Training loss: 2.841397762298584
Validation loss: 2.0994832361898115

Epoch: 5| Step: 1
Training loss: 2.2152509689331055
Validation loss: 2.0889877875645957

Epoch: 5| Step: 2
Training loss: 2.1647706031799316
Validation loss: 2.1078882576316915

Epoch: 5| Step: 3
Training loss: 1.725832223892212
Validation loss: 2.1306176083062285

Epoch: 5| Step: 4
Training loss: 1.9579708576202393
Validation loss: 2.1457875518388647

Epoch: 5| Step: 5
Training loss: 2.638054847717285
Validation loss: 2.1465842877664874

Epoch: 5| Step: 6
Training loss: 2.4882664680480957
Validation loss: 2.156937499200144

Epoch: 5| Step: 7
Training loss: 1.6167351007461548
Validation loss: 2.143866741529075

Epoch: 5| Step: 8
Training loss: 2.0374796390533447
Validation loss: 2.1385053691043647

Epoch: 5| Step: 9
Training loss: 1.9795706272125244
Validation loss: 2.138137884037469

Epoch: 5| Step: 10
Training loss: 2.60962176322937
Validation loss: 2.1293988086844005

Epoch: 219| Step: 0
Training loss: 1.8349926471710205
Validation loss: 2.1285040968207904

Epoch: 5| Step: 1
Training loss: 2.3735134601593018
Validation loss: 2.1281661141303276

Epoch: 5| Step: 2
Training loss: 2.0156362056732178
Validation loss: 2.1428165512700237

Epoch: 5| Step: 3
Training loss: 2.0899012088775635
Validation loss: 2.132621516463577

Epoch: 5| Step: 4
Training loss: 2.589580535888672
Validation loss: 2.1390726438132663

Epoch: 5| Step: 5
Training loss: 2.9096438884735107
Validation loss: 2.1315691445463445

Epoch: 5| Step: 6
Training loss: 2.281200885772705
Validation loss: 2.1142970131289576

Epoch: 5| Step: 7
Training loss: 2.084582805633545
Validation loss: 2.13068449881769

Epoch: 5| Step: 8
Training loss: 1.8516242504119873
Validation loss: 2.1300460036082933

Epoch: 5| Step: 9
Training loss: 2.5202670097351074
Validation loss: 2.1056969909257788

Epoch: 5| Step: 10
Training loss: 1.3615388870239258
Validation loss: 2.1073778495993665

Epoch: 220| Step: 0
Training loss: 2.1518938541412354
Validation loss: 2.113532334245661

Epoch: 5| Step: 1
Training loss: 2.092421054840088
Validation loss: 2.133719518620481

Epoch: 5| Step: 2
Training loss: 2.0936074256896973
Validation loss: 2.1438691269966865

Epoch: 5| Step: 3
Training loss: 2.470693826675415
Validation loss: 2.153282488546064

Epoch: 5| Step: 4
Training loss: 2.5509963035583496
Validation loss: 2.154060656024564

Epoch: 5| Step: 5
Training loss: 2.863647937774658
Validation loss: 2.1501896612105833

Epoch: 5| Step: 6
Training loss: 1.7422997951507568
Validation loss: 2.1557406507512575

Epoch: 5| Step: 7
Training loss: 1.739580512046814
Validation loss: 2.1533248770621514

Epoch: 5| Step: 8
Training loss: 2.479642391204834
Validation loss: 2.1530255553542927

Epoch: 5| Step: 9
Training loss: 1.924644112586975
Validation loss: 2.1407791594023347

Epoch: 5| Step: 10
Training loss: 2.0352115631103516
Validation loss: 2.144230620835417

Epoch: 221| Step: 0
Training loss: 2.315002679824829
Validation loss: 2.1292153609696256

Epoch: 5| Step: 1
Training loss: 2.3041441440582275
Validation loss: 2.1290003356113227

Epoch: 5| Step: 2
Training loss: 1.9462248086929321
Validation loss: 2.1313597771429245

Epoch: 5| Step: 3
Training loss: 2.264906883239746
Validation loss: 2.1425798810938352

Epoch: 5| Step: 4
Training loss: 2.5280497074127197
Validation loss: 2.1492260861140426

Epoch: 5| Step: 5
Training loss: 2.747413158416748
Validation loss: 2.147399663925171

Epoch: 5| Step: 6
Training loss: 1.551784634590149
Validation loss: 2.144577903132285

Epoch: 5| Step: 7
Training loss: 1.938085913658142
Validation loss: 2.1468387393541235

Epoch: 5| Step: 8
Training loss: 2.2138493061065674
Validation loss: 2.1480557290456628

Epoch: 5| Step: 9
Training loss: 2.161806583404541
Validation loss: 2.1445507682779783

Epoch: 5| Step: 10
Training loss: 1.9751192331314087
Validation loss: 2.1436189323343258

Epoch: 222| Step: 0
Training loss: 1.9399068355560303
Validation loss: 2.1327704434753745

Epoch: 5| Step: 1
Training loss: 1.8812592029571533
Validation loss: 2.1182065227980256

Epoch: 5| Step: 2
Training loss: 2.6862220764160156
Validation loss: 2.13155786965483

Epoch: 5| Step: 3
Training loss: 1.5090012550354004
Validation loss: 2.1422612282537643

Epoch: 5| Step: 4
Training loss: 2.110813856124878
Validation loss: 2.1460873388474986

Epoch: 5| Step: 5
Training loss: 2.823854923248291
Validation loss: 2.161108656596112

Epoch: 5| Step: 6
Training loss: 2.5559043884277344
Validation loss: 2.1215481271025953

Epoch: 5| Step: 7
Training loss: 1.7529083490371704
Validation loss: 2.112915445399541

Epoch: 5| Step: 8
Training loss: 2.2575690746307373
Validation loss: 2.1269811648194508

Epoch: 5| Step: 9
Training loss: 1.7731071710586548
Validation loss: 2.126388415213554

Epoch: 5| Step: 10
Training loss: 2.672150135040283
Validation loss: 2.1304875471258677

Epoch: 223| Step: 0
Training loss: 2.072436809539795
Validation loss: 2.1215807468660417

Epoch: 5| Step: 1
Training loss: 1.3555315732955933
Validation loss: 2.1040987455716698

Epoch: 5| Step: 2
Training loss: 2.274092197418213
Validation loss: 2.11356448101741

Epoch: 5| Step: 3
Training loss: 2.6653943061828613
Validation loss: 2.116935727416828

Epoch: 5| Step: 4
Training loss: 1.9806007146835327
Validation loss: 2.114681456678657

Epoch: 5| Step: 5
Training loss: 2.656324863433838
Validation loss: 2.1258599578693347

Epoch: 5| Step: 6
Training loss: 2.0723299980163574
Validation loss: 2.1422340152084187

Epoch: 5| Step: 7
Training loss: 2.3901970386505127
Validation loss: 2.137221086409784

Epoch: 5| Step: 8
Training loss: 2.3967833518981934
Validation loss: 2.1405614652941303

Epoch: 5| Step: 9
Training loss: 2.119913101196289
Validation loss: 2.1420553217652025

Epoch: 5| Step: 10
Training loss: 1.8133667707443237
Validation loss: 2.1377115839271137

Epoch: 224| Step: 0
Training loss: 2.635239839553833
Validation loss: 2.1434433460235596

Epoch: 5| Step: 1
Training loss: 2.4301133155822754
Validation loss: 2.179384413585868

Epoch: 5| Step: 2
Training loss: 2.1012251377105713
Validation loss: 2.1796791784224974

Epoch: 5| Step: 3
Training loss: 2.1076724529266357
Validation loss: 2.178372031898909

Epoch: 5| Step: 4
Training loss: 1.7202990055084229
Validation loss: 2.1835469661220426

Epoch: 5| Step: 5
Training loss: 1.8765952587127686
Validation loss: 2.1576823854959137

Epoch: 5| Step: 6
Training loss: 1.8078864812850952
Validation loss: 2.1427892202972085

Epoch: 5| Step: 7
Training loss: 2.760042667388916
Validation loss: 2.135781200983191

Epoch: 5| Step: 8
Training loss: 2.0891740322113037
Validation loss: 2.127134261592742

Epoch: 5| Step: 9
Training loss: 1.8531262874603271
Validation loss: 2.133050625042249

Epoch: 5| Step: 10
Training loss: 2.7927052974700928
Validation loss: 2.1418115233862274

Epoch: 225| Step: 0
Training loss: 1.761060118675232
Validation loss: 2.1556569581390708

Epoch: 5| Step: 1
Training loss: 1.9450676441192627
Validation loss: 2.150281713854882

Epoch: 5| Step: 2
Training loss: 1.8920577764511108
Validation loss: 2.130689355634874

Epoch: 5| Step: 3
Training loss: 2.0656027793884277
Validation loss: 2.1138253006883847

Epoch: 5| Step: 4
Training loss: 3.285412311553955
Validation loss: 2.1144612553299114

Epoch: 5| Step: 5
Training loss: 2.324105978012085
Validation loss: 2.120174641250282

Epoch: 5| Step: 6
Training loss: 2.057927370071411
Validation loss: 2.1220403909683228

Epoch: 5| Step: 7
Training loss: 1.6211366653442383
Validation loss: 2.144777579974103

Epoch: 5| Step: 8
Training loss: 2.289069414138794
Validation loss: 2.157043368585648

Epoch: 5| Step: 9
Training loss: 2.284661054611206
Validation loss: 2.14526121077999

Epoch: 5| Step: 10
Training loss: 2.269782066345215
Validation loss: 2.1361387903972338

Epoch: 226| Step: 0
Training loss: 1.8825623989105225
Validation loss: 2.1350635085054623

Epoch: 5| Step: 1
Training loss: 2.106854200363159
Validation loss: 2.1263079053612164

Epoch: 5| Step: 2
Training loss: 1.9197769165039062
Validation loss: 2.1291405411176783

Epoch: 5| Step: 3
Training loss: 3.481957197189331
Validation loss: 2.128197390546081

Epoch: 5| Step: 4
Training loss: 1.837110161781311
Validation loss: 2.1383326412529073

Epoch: 5| Step: 5
Training loss: 1.872255563735962
Validation loss: 2.1543410157644622

Epoch: 5| Step: 6
Training loss: 2.6868908405303955
Validation loss: 2.1486923848429034

Epoch: 5| Step: 7
Training loss: 2.0147743225097656
Validation loss: 2.1484722783488612

Epoch: 5| Step: 8
Training loss: 1.8754281997680664
Validation loss: 2.1269104634561846

Epoch: 5| Step: 9
Training loss: 2.2233872413635254
Validation loss: 2.128642774397327

Epoch: 5| Step: 10
Training loss: 1.7154980897903442
Validation loss: 2.126296038268715

Epoch: 227| Step: 0
Training loss: 2.1853513717651367
Validation loss: 2.1327189591623124

Epoch: 5| Step: 1
Training loss: 2.0490245819091797
Validation loss: 2.125866907899098

Epoch: 5| Step: 2
Training loss: 1.798151969909668
Validation loss: 2.1314269765730827

Epoch: 5| Step: 3
Training loss: 2.267995834350586
Validation loss: 2.1141476156891033

Epoch: 5| Step: 4
Training loss: 2.2348194122314453
Validation loss: 2.121262337571831

Epoch: 5| Step: 5
Training loss: 2.4954428672790527
Validation loss: 2.1151484058749292

Epoch: 5| Step: 6
Training loss: 1.996246099472046
Validation loss: 2.1247781707394506

Epoch: 5| Step: 7
Training loss: 2.4218056201934814
Validation loss: 2.1356759250804944

Epoch: 5| Step: 8
Training loss: 1.5753148794174194
Validation loss: 2.1595812395054805

Epoch: 5| Step: 9
Training loss: 2.4927804470062256
Validation loss: 2.137547940336248

Epoch: 5| Step: 10
Training loss: 2.1699864864349365
Validation loss: 2.1594973456475044

Epoch: 228| Step: 0
Training loss: 2.0768003463745117
Validation loss: 2.1539093345724125

Epoch: 5| Step: 1
Training loss: 2.2127859592437744
Validation loss: 2.170545421620851

Epoch: 5| Step: 2
Training loss: 2.067882537841797
Validation loss: 2.147747532013924

Epoch: 5| Step: 3
Training loss: 2.4822115898132324
Validation loss: 2.165070955471326

Epoch: 5| Step: 4
Training loss: 2.5167341232299805
Validation loss: 2.1787128640759374

Epoch: 5| Step: 5
Training loss: 1.7999566793441772
Validation loss: 2.1376207413211947

Epoch: 5| Step: 6
Training loss: 1.9980449676513672
Validation loss: 2.1353984878909205

Epoch: 5| Step: 7
Training loss: 1.645785927772522
Validation loss: 2.1189916979882026

Epoch: 5| Step: 8
Training loss: 2.2919347286224365
Validation loss: 2.127228352331346

Epoch: 5| Step: 9
Training loss: 2.7325680255889893
Validation loss: 2.114412589739728

Epoch: 5| Step: 10
Training loss: 1.7822884321212769
Validation loss: 2.1172167280668854

Epoch: 229| Step: 0
Training loss: 2.025327444076538
Validation loss: 2.1038329806379092

Epoch: 5| Step: 1
Training loss: 2.3496127128601074
Validation loss: 2.096313281725812

Epoch: 5| Step: 2
Training loss: 1.4969085454940796
Validation loss: 2.1051720752510974

Epoch: 5| Step: 3
Training loss: 2.0566935539245605
Validation loss: 2.112157388400006

Epoch: 5| Step: 4
Training loss: 2.549337148666382
Validation loss: 2.1333302759355113

Epoch: 5| Step: 5
Training loss: 2.1862378120422363
Validation loss: 2.138623858010897

Epoch: 5| Step: 6
Training loss: 2.0494861602783203
Validation loss: 2.1277629944585983

Epoch: 5| Step: 7
Training loss: 2.6004128456115723
Validation loss: 2.112426342502717

Epoch: 5| Step: 8
Training loss: 1.9907371997833252
Validation loss: 2.1050889056216002

Epoch: 5| Step: 9
Training loss: 2.0128085613250732
Validation loss: 2.1064953086196736

Epoch: 5| Step: 10
Training loss: 2.1090617179870605
Validation loss: 2.112303364661432

Epoch: 230| Step: 0
Training loss: 2.064700126647949
Validation loss: 2.105989215194538

Epoch: 5| Step: 1
Training loss: 2.4285004138946533
Validation loss: 2.135619527550154

Epoch: 5| Step: 2
Training loss: 1.2520501613616943
Validation loss: 2.134285783254972

Epoch: 5| Step: 3
Training loss: 2.5437991619110107
Validation loss: 2.141283486479072

Epoch: 5| Step: 4
Training loss: 2.319427967071533
Validation loss: 2.140416811871272

Epoch: 5| Step: 5
Training loss: 2.2727162837982178
Validation loss: 2.1364340333528418

Epoch: 5| Step: 6
Training loss: 1.3835560083389282
Validation loss: 2.150169926304971

Epoch: 5| Step: 7
Training loss: 2.327267646789551
Validation loss: 2.150631753347253

Epoch: 5| Step: 8
Training loss: 1.6911487579345703
Validation loss: 2.138062415584441

Epoch: 5| Step: 9
Training loss: 2.3662524223327637
Validation loss: 2.128610413561585

Epoch: 5| Step: 10
Training loss: 2.8744423389434814
Validation loss: 2.1118885829884517

Epoch: 231| Step: 0
Training loss: 2.1584019660949707
Validation loss: 2.0864717460447744

Epoch: 5| Step: 1
Training loss: 2.163301944732666
Validation loss: 2.086001270560808

Epoch: 5| Step: 2
Training loss: 2.380873680114746
Validation loss: 2.0648955478463122

Epoch: 5| Step: 3
Training loss: 1.4830267429351807
Validation loss: 2.069047793265312

Epoch: 5| Step: 4
Training loss: 2.101362943649292
Validation loss: 2.0667715034177228

Epoch: 5| Step: 5
Training loss: 2.662405490875244
Validation loss: 2.0713390009377592

Epoch: 5| Step: 6
Training loss: 1.950688123703003
Validation loss: 2.066567628614364

Epoch: 5| Step: 7
Training loss: 1.8258039951324463
Validation loss: 2.072708334974063

Epoch: 5| Step: 8
Training loss: 2.4964599609375
Validation loss: 2.0703236518367643

Epoch: 5| Step: 9
Training loss: 2.2858498096466064
Validation loss: 2.0779282803176553

Epoch: 5| Step: 10
Training loss: 1.9403963088989258
Validation loss: 2.0840993260824554

Epoch: 232| Step: 0
Training loss: 2.185664415359497
Validation loss: 2.085548309869664

Epoch: 5| Step: 1
Training loss: 2.4629597663879395
Validation loss: 2.0922006637819353

Epoch: 5| Step: 2
Training loss: 1.7122278213500977
Validation loss: 2.0786612328662666

Epoch: 5| Step: 3
Training loss: 1.6519893407821655
Validation loss: 2.0882331812253563

Epoch: 5| Step: 4
Training loss: 2.3951854705810547
Validation loss: 2.076773488393394

Epoch: 5| Step: 5
Training loss: 2.0444071292877197
Validation loss: 2.0857661680508683

Epoch: 5| Step: 6
Training loss: 2.6757562160491943
Validation loss: 2.0948090399465253

Epoch: 5| Step: 7
Training loss: 1.8463928699493408
Validation loss: 2.114184130904495

Epoch: 5| Step: 8
Training loss: 1.7821851968765259
Validation loss: 2.1432447587290118

Epoch: 5| Step: 9
Training loss: 2.629699230194092
Validation loss: 2.1388881103966826

Epoch: 5| Step: 10
Training loss: 1.988844871520996
Validation loss: 2.123565181609123

Epoch: 233| Step: 0
Training loss: 2.326082944869995
Validation loss: 2.1138990412476244

Epoch: 5| Step: 1
Training loss: 1.9826936721801758
Validation loss: 2.1256984651729627

Epoch: 5| Step: 2
Training loss: 1.9010932445526123
Validation loss: 2.129617429548694

Epoch: 5| Step: 3
Training loss: 1.466296672821045
Validation loss: 2.132399933312529

Epoch: 5| Step: 4
Training loss: 1.772120714187622
Validation loss: 2.146498962115216

Epoch: 5| Step: 5
Training loss: 2.724308967590332
Validation loss: 2.1641469335043304

Epoch: 5| Step: 6
Training loss: 2.16863751411438
Validation loss: 2.1697235697059223

Epoch: 5| Step: 7
Training loss: 2.0665087699890137
Validation loss: 2.1544533365516254

Epoch: 5| Step: 8
Training loss: 2.929265022277832
Validation loss: 2.1343123425719557

Epoch: 5| Step: 9
Training loss: 1.667764663696289
Validation loss: 2.1315425108837824

Epoch: 5| Step: 10
Training loss: 2.253432273864746
Validation loss: 2.111825030337098

Epoch: 234| Step: 0
Training loss: 2.368917942047119
Validation loss: 2.1181418126629246

Epoch: 5| Step: 1
Training loss: 1.335545539855957
Validation loss: 2.0973635040303713

Epoch: 5| Step: 2
Training loss: 2.15279483795166
Validation loss: 2.0970891009094896

Epoch: 5| Step: 3
Training loss: 1.8487859964370728
Validation loss: 2.0867207909143097

Epoch: 5| Step: 4
Training loss: 2.689192295074463
Validation loss: 2.090988382216423

Epoch: 5| Step: 5
Training loss: 1.9004881381988525
Validation loss: 2.096206485584218

Epoch: 5| Step: 6
Training loss: 2.438201904296875
Validation loss: 2.0946777687277844

Epoch: 5| Step: 7
Training loss: 1.5166696310043335
Validation loss: 2.1153122930116552

Epoch: 5| Step: 8
Training loss: 2.7443416118621826
Validation loss: 2.127225632308632

Epoch: 5| Step: 9
Training loss: 2.167172908782959
Validation loss: 2.1198151970422394

Epoch: 5| Step: 10
Training loss: 1.9489315748214722
Validation loss: 2.1115564095076693

Epoch: 235| Step: 0
Training loss: 2.0525219440460205
Validation loss: 2.115789705707181

Epoch: 5| Step: 1
Training loss: 2.088613986968994
Validation loss: 2.1196522815253145

Epoch: 5| Step: 2
Training loss: 1.715555191040039
Validation loss: 2.1186429787707586

Epoch: 5| Step: 3
Training loss: 2.35689115524292
Validation loss: 2.108536785648715

Epoch: 5| Step: 4
Training loss: 1.6501966714859009
Validation loss: 2.1224463062901653

Epoch: 5| Step: 5
Training loss: 2.024149179458618
Validation loss: 2.1136840722894155

Epoch: 5| Step: 6
Training loss: 2.056589126586914
Validation loss: 2.1307243480477283

Epoch: 5| Step: 7
Training loss: 2.7477974891662598
Validation loss: 2.125988598792784

Epoch: 5| Step: 8
Training loss: 2.161933183670044
Validation loss: 2.108143186056486

Epoch: 5| Step: 9
Training loss: 1.9689571857452393
Validation loss: 2.1065935857834353

Epoch: 5| Step: 10
Training loss: 2.1792774200439453
Validation loss: 2.11684537190263

Epoch: 236| Step: 0
Training loss: 2.7228543758392334
Validation loss: 2.10461748543606

Epoch: 5| Step: 1
Training loss: 1.9873180389404297
Validation loss: 2.0812161148235364

Epoch: 5| Step: 2
Training loss: 2.0234174728393555
Validation loss: 2.0764305668492473

Epoch: 5| Step: 3
Training loss: 1.7289283275604248
Validation loss: 2.0585381215618503

Epoch: 5| Step: 4
Training loss: 1.9685395956039429
Validation loss: 2.0796767998767156

Epoch: 5| Step: 5
Training loss: 2.4527587890625
Validation loss: 2.0771737701149395

Epoch: 5| Step: 6
Training loss: 2.245518207550049
Validation loss: 2.0820965920725176

Epoch: 5| Step: 7
Training loss: 2.2943332195281982
Validation loss: 2.0680452649311354

Epoch: 5| Step: 8
Training loss: 1.2618051767349243
Validation loss: 2.0918397262532222

Epoch: 5| Step: 9
Training loss: 2.2328875064849854
Validation loss: 2.1010037211961645

Epoch: 5| Step: 10
Training loss: 2.111226797103882
Validation loss: 2.104155291793167

Epoch: 237| Step: 0
Training loss: 1.8674371242523193
Validation loss: 2.1120978837372153

Epoch: 5| Step: 1
Training loss: 2.20837664604187
Validation loss: 2.133862821004724

Epoch: 5| Step: 2
Training loss: 2.329423427581787
Validation loss: 2.149027742365355

Epoch: 5| Step: 3
Training loss: 2.7266738414764404
Validation loss: 2.1681443004197973

Epoch: 5| Step: 4
Training loss: 1.786987066268921
Validation loss: 2.137229647687686

Epoch: 5| Step: 5
Training loss: 1.550168752670288
Validation loss: 2.133879469287011

Epoch: 5| Step: 6
Training loss: 2.27693510055542
Validation loss: 2.116504201325037

Epoch: 5| Step: 7
Training loss: 1.9521678686141968
Validation loss: 2.100759554934758

Epoch: 5| Step: 8
Training loss: 2.020648956298828
Validation loss: 2.099446371037473

Epoch: 5| Step: 9
Training loss: 2.087462902069092
Validation loss: 2.104980076512983

Epoch: 5| Step: 10
Training loss: 2.2000250816345215
Validation loss: 2.1125266077697917

Epoch: 238| Step: 0
Training loss: 2.2286200523376465
Validation loss: 2.102283646983485

Epoch: 5| Step: 1
Training loss: 1.6659311056137085
Validation loss: 2.1037529053226596

Epoch: 5| Step: 2
Training loss: 2.414973497390747
Validation loss: 2.0880533238892913

Epoch: 5| Step: 3
Training loss: 1.8259687423706055
Validation loss: 2.094918393319653

Epoch: 5| Step: 4
Training loss: 1.62233567237854
Validation loss: 2.1038264664270545

Epoch: 5| Step: 5
Training loss: 2.589256763458252
Validation loss: 2.10582527934864

Epoch: 5| Step: 6
Training loss: 2.2929656505584717
Validation loss: 2.108695601904264

Epoch: 5| Step: 7
Training loss: 2.7812118530273438
Validation loss: 2.1122404708657214

Epoch: 5| Step: 8
Training loss: 1.5498846769332886
Validation loss: 2.0944223775658557

Epoch: 5| Step: 9
Training loss: 1.8321268558502197
Validation loss: 2.090658433975712

Epoch: 5| Step: 10
Training loss: 2.107076406478882
Validation loss: 2.102787851005472

Epoch: 239| Step: 0
Training loss: 2.1838979721069336
Validation loss: 2.1056694535798925

Epoch: 5| Step: 1
Training loss: 2.27639102935791
Validation loss: 2.1002232797684206

Epoch: 5| Step: 2
Training loss: 1.884121298789978
Validation loss: 2.0941650021460747

Epoch: 5| Step: 3
Training loss: 2.579826831817627
Validation loss: 2.0918050107135566

Epoch: 5| Step: 4
Training loss: 1.7723028659820557
Validation loss: 2.088739131086616

Epoch: 5| Step: 5
Training loss: 2.2039589881896973
Validation loss: 2.095494653588982

Epoch: 5| Step: 6
Training loss: 1.1717246770858765
Validation loss: 2.105918679186093

Epoch: 5| Step: 7
Training loss: 2.2556099891662598
Validation loss: 2.110689309335524

Epoch: 5| Step: 8
Training loss: 1.9988397359848022
Validation loss: 2.1147577454966884

Epoch: 5| Step: 9
Training loss: 2.155789375305176
Validation loss: 2.107929006699593

Epoch: 5| Step: 10
Training loss: 2.281330108642578
Validation loss: 2.090224145561136

Epoch: 240| Step: 0
Training loss: 2.172489881515503
Validation loss: 2.1002899010976157

Epoch: 5| Step: 1
Training loss: 1.7881511449813843
Validation loss: 2.1004220439541723

Epoch: 5| Step: 2
Training loss: 1.83502995967865
Validation loss: 2.1142431061754943

Epoch: 5| Step: 3
Training loss: 2.0783486366271973
Validation loss: 2.123281228926874

Epoch: 5| Step: 4
Training loss: 2.4841020107269287
Validation loss: 2.103365123912852

Epoch: 5| Step: 5
Training loss: 2.799454927444458
Validation loss: 2.1051674722343363

Epoch: 5| Step: 6
Training loss: 1.7527328729629517
Validation loss: 2.108107013087119

Epoch: 5| Step: 7
Training loss: 2.5153844356536865
Validation loss: 2.10282273702724

Epoch: 5| Step: 8
Training loss: 2.57051157951355
Validation loss: 2.096369949720239

Epoch: 5| Step: 9
Training loss: 0.9184505343437195
Validation loss: 2.0865244801326464

Epoch: 5| Step: 10
Training loss: 1.8954607248306274
Validation loss: 2.072701818199568

Epoch: 241| Step: 0
Training loss: 2.055020570755005
Validation loss: 2.0664680286120345

Epoch: 5| Step: 1
Training loss: 2.194119453430176
Validation loss: 2.0626537210197857

Epoch: 5| Step: 2
Training loss: 2.0107717514038086
Validation loss: 2.077737203208349

Epoch: 5| Step: 3
Training loss: 2.100132465362549
Validation loss: 2.094689579420192

Epoch: 5| Step: 4
Training loss: 1.8801076412200928
Validation loss: 2.0858922209790958

Epoch: 5| Step: 5
Training loss: 1.9872760772705078
Validation loss: 2.0943867775701706

Epoch: 5| Step: 6
Training loss: 2.235069990158081
Validation loss: 2.105359131290067

Epoch: 5| Step: 7
Training loss: 1.9086602926254272
Validation loss: 2.117807662615212

Epoch: 5| Step: 8
Training loss: 2.0605225563049316
Validation loss: 2.1250108134362007

Epoch: 5| Step: 9
Training loss: 2.1715638637542725
Validation loss: 2.1379987834602274

Epoch: 5| Step: 10
Training loss: 2.123709201812744
Validation loss: 2.1453313699332615

Epoch: 242| Step: 0
Training loss: 2.6997427940368652
Validation loss: 2.118394431247506

Epoch: 5| Step: 1
Training loss: 2.1153435707092285
Validation loss: 2.099355738650086

Epoch: 5| Step: 2
Training loss: 2.048825740814209
Validation loss: 2.0817629855166198

Epoch: 5| Step: 3
Training loss: 1.7695763111114502
Validation loss: 2.0842968417752172

Epoch: 5| Step: 4
Training loss: 1.3733155727386475
Validation loss: 2.0944442133749686

Epoch: 5| Step: 5
Training loss: 1.9453668594360352
Validation loss: 2.0826559579500588

Epoch: 5| Step: 6
Training loss: 2.2245819568634033
Validation loss: 2.0769645962663876

Epoch: 5| Step: 7
Training loss: 2.7021615505218506
Validation loss: 2.081301353311026

Epoch: 5| Step: 8
Training loss: 2.186302661895752
Validation loss: 2.074235136790942

Epoch: 5| Step: 9
Training loss: 2.1004109382629395
Validation loss: 2.069531285634605

Epoch: 5| Step: 10
Training loss: 1.3790820837020874
Validation loss: 2.069758863859279

Epoch: 243| Step: 0
Training loss: 2.167421817779541
Validation loss: 2.066203535244029

Epoch: 5| Step: 1
Training loss: 2.283212184906006
Validation loss: 2.0691578554850754

Epoch: 5| Step: 2
Training loss: 2.383579730987549
Validation loss: 2.0692038894981466

Epoch: 5| Step: 3
Training loss: 1.8376877307891846
Validation loss: 2.05060734287385

Epoch: 5| Step: 4
Training loss: 2.1985559463500977
Validation loss: 2.075094155085984

Epoch: 5| Step: 5
Training loss: 1.6640071868896484
Validation loss: 2.0600693225860596

Epoch: 5| Step: 6
Training loss: 2.4584879875183105
Validation loss: 2.0667546615805676

Epoch: 5| Step: 7
Training loss: 1.6134326457977295
Validation loss: 2.0693006669321368

Epoch: 5| Step: 8
Training loss: 1.8827606439590454
Validation loss: 2.0721656096878873

Epoch: 5| Step: 9
Training loss: 1.924030065536499
Validation loss: 2.0874470562063236

Epoch: 5| Step: 10
Training loss: 2.3354101181030273
Validation loss: 2.108957485486102

Epoch: 244| Step: 0
Training loss: 2.467083215713501
Validation loss: 2.1078198878995833

Epoch: 5| Step: 1
Training loss: 1.4542080163955688
Validation loss: 2.1026214386827204

Epoch: 5| Step: 2
Training loss: 1.687233328819275
Validation loss: 2.102239119109287

Epoch: 5| Step: 3
Training loss: 2.5626537799835205
Validation loss: 2.102991152835149

Epoch: 5| Step: 4
Training loss: 2.1590185165405273
Validation loss: 2.0881694132281887

Epoch: 5| Step: 5
Training loss: 1.4843987226486206
Validation loss: 2.0858204621140675

Epoch: 5| Step: 6
Training loss: 2.182803153991699
Validation loss: 2.099598933291692

Epoch: 5| Step: 7
Training loss: 1.942809820175171
Validation loss: 2.1038147095711

Epoch: 5| Step: 8
Training loss: 1.9348270893096924
Validation loss: 2.0930204647843555

Epoch: 5| Step: 9
Training loss: 2.3193535804748535
Validation loss: 2.083564519882202

Epoch: 5| Step: 10
Training loss: 2.4824631214141846
Validation loss: 2.0976749056129047

Epoch: 245| Step: 0
Training loss: 1.538853406906128
Validation loss: 2.092010967193111

Epoch: 5| Step: 1
Training loss: 2.4934821128845215
Validation loss: 2.1087543323475826

Epoch: 5| Step: 2
Training loss: 1.5569674968719482
Validation loss: 2.0921371367669876

Epoch: 5| Step: 3
Training loss: 1.754992127418518
Validation loss: 2.1213800522588913

Epoch: 5| Step: 4
Training loss: 2.1376543045043945
Validation loss: 2.1226079803641125

Epoch: 5| Step: 5
Training loss: 2.2326300144195557
Validation loss: 2.1243362272939375

Epoch: 5| Step: 6
Training loss: 2.1143383979797363
Validation loss: 2.127067845354798

Epoch: 5| Step: 7
Training loss: 1.7325204610824585
Validation loss: 2.136046886444092

Epoch: 5| Step: 8
Training loss: 1.8648042678833008
Validation loss: 2.1234048207600913

Epoch: 5| Step: 9
Training loss: 2.266352653503418
Validation loss: 2.1072315144282516

Epoch: 5| Step: 10
Training loss: 2.7287471294403076
Validation loss: 2.0972244406259186

Epoch: 246| Step: 0
Training loss: 1.349069356918335
Validation loss: 2.0876421543859665

Epoch: 5| Step: 1
Training loss: 1.8801476955413818
Validation loss: 2.092313015332786

Epoch: 5| Step: 2
Training loss: 2.4146876335144043
Validation loss: 2.091769790136686

Epoch: 5| Step: 3
Training loss: 1.3320906162261963
Validation loss: 2.0831841550847536

Epoch: 5| Step: 4
Training loss: 2.338459014892578
Validation loss: 2.0793982526307464

Epoch: 5| Step: 5
Training loss: 1.184757113456726
Validation loss: 2.0683203025530745

Epoch: 5| Step: 6
Training loss: 2.3360114097595215
Validation loss: 2.0612135318017777

Epoch: 5| Step: 7
Training loss: 2.6065144538879395
Validation loss: 2.0630639137760287

Epoch: 5| Step: 8
Training loss: 2.234111785888672
Validation loss: 2.0706511691052425

Epoch: 5| Step: 9
Training loss: 2.618485927581787
Validation loss: 2.0766015680887366

Epoch: 5| Step: 10
Training loss: 2.1671977043151855
Validation loss: 2.0934529586504866

Epoch: 247| Step: 0
Training loss: 2.452244520187378
Validation loss: 2.1032575894427556

Epoch: 5| Step: 1
Training loss: 1.8841241598129272
Validation loss: 2.101505284668297

Epoch: 5| Step: 2
Training loss: 2.1019320487976074
Validation loss: 2.0737908091596378

Epoch: 5| Step: 3
Training loss: 1.4715687036514282
Validation loss: 2.0761704701249317

Epoch: 5| Step: 4
Training loss: 2.490638256072998
Validation loss: 2.066080839403214

Epoch: 5| Step: 5
Training loss: 2.3749852180480957
Validation loss: 2.0749684379946802

Epoch: 5| Step: 6
Training loss: 2.294801950454712
Validation loss: 2.0754450905707573

Epoch: 5| Step: 7
Training loss: 1.4804136753082275
Validation loss: 2.068865127460931

Epoch: 5| Step: 8
Training loss: 2.3660483360290527
Validation loss: 2.068479836628001

Epoch: 5| Step: 9
Training loss: 1.8674907684326172
Validation loss: 2.076252116951891

Epoch: 5| Step: 10
Training loss: 1.6044071912765503
Validation loss: 2.088409553291977

Epoch: 248| Step: 0
Training loss: 1.512502670288086
Validation loss: 2.099186792168566

Epoch: 5| Step: 1
Training loss: 2.310143232345581
Validation loss: 2.0992457059121903

Epoch: 5| Step: 2
Training loss: 2.434960126876831
Validation loss: 2.1031042888600338

Epoch: 5| Step: 3
Training loss: 1.7611669301986694
Validation loss: 2.108319690150599

Epoch: 5| Step: 4
Training loss: 2.192385673522949
Validation loss: 2.0996592583194857

Epoch: 5| Step: 5
Training loss: 2.406432628631592
Validation loss: 2.0973548402068434

Epoch: 5| Step: 6
Training loss: 1.503147840499878
Validation loss: 2.103907541562152

Epoch: 5| Step: 7
Training loss: 2.1089954376220703
Validation loss: 2.0969015936697684

Epoch: 5| Step: 8
Training loss: 2.4202303886413574
Validation loss: 2.109639379285997

Epoch: 5| Step: 9
Training loss: 1.7248523235321045
Validation loss: 2.1012043799123457

Epoch: 5| Step: 10
Training loss: 1.964095115661621
Validation loss: 2.097562884771696

Epoch: 249| Step: 0
Training loss: 2.7487952709198
Validation loss: 2.09977674740617

Epoch: 5| Step: 1
Training loss: 2.255021572113037
Validation loss: 2.089949413012433

Epoch: 5| Step: 2
Training loss: 2.020806074142456
Validation loss: 2.0863612146787744

Epoch: 5| Step: 3
Training loss: 1.7995376586914062
Validation loss: 2.075860074771348

Epoch: 5| Step: 4
Training loss: 1.9150501489639282
Validation loss: 2.084775686264038

Epoch: 5| Step: 5
Training loss: 2.351778507232666
Validation loss: 2.0645347948997252

Epoch: 5| Step: 6
Training loss: 2.0870883464813232
Validation loss: 2.0559731503968597

Epoch: 5| Step: 7
Training loss: 1.7005964517593384
Validation loss: 2.0454930695154334

Epoch: 5| Step: 8
Training loss: 2.08858585357666
Validation loss: 2.0657410185824157

Epoch: 5| Step: 9
Training loss: 1.5825459957122803
Validation loss: 2.055836510914628

Epoch: 5| Step: 10
Training loss: 1.6536613702774048
Validation loss: 2.0715862384406467

Epoch: 250| Step: 0
Training loss: 1.906124472618103
Validation loss: 2.067641140312277

Epoch: 5| Step: 1
Training loss: 2.0426278114318848
Validation loss: 2.0786251560334237

Epoch: 5| Step: 2
Training loss: 1.430983304977417
Validation loss: 2.0693938321964715

Epoch: 5| Step: 3
Training loss: 2.4352521896362305
Validation loss: 2.0792381891640286

Epoch: 5| Step: 4
Training loss: 2.2134554386138916
Validation loss: 2.0774149407622633

Epoch: 5| Step: 5
Training loss: 2.226134777069092
Validation loss: 2.067911604399322

Epoch: 5| Step: 6
Training loss: 1.9166514873504639
Validation loss: 2.0787940922603814

Epoch: 5| Step: 7
Training loss: 2.1638944149017334
Validation loss: 2.0634513542216313

Epoch: 5| Step: 8
Training loss: 1.7159106731414795
Validation loss: 2.0613904230056272

Epoch: 5| Step: 9
Training loss: 1.991456389427185
Validation loss: 2.080869677246258

Epoch: 5| Step: 10
Training loss: 1.9845255613327026
Validation loss: 2.070622726153302

Testing loss: 2.270344442791409

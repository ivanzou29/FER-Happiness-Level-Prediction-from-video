Epoch: 1| Step: 0
Training loss: 4.996646404266357
Validation loss: 5.2040775975873395

Epoch: 6| Step: 1
Training loss: 7.011192321777344
Validation loss: 5.198287061465684

Epoch: 6| Step: 2
Training loss: 4.585912704467773
Validation loss: 5.1920436069529545

Epoch: 6| Step: 3
Training loss: 2.773858070373535
Validation loss: 5.185742470525926

Epoch: 6| Step: 4
Training loss: 5.911861419677734
Validation loss: 5.180266082927745

Epoch: 6| Step: 5
Training loss: 4.976386070251465
Validation loss: 5.173623423422536

Epoch: 6| Step: 6
Training loss: 3.789832592010498
Validation loss: 5.167568791297175

Epoch: 6| Step: 7
Training loss: 4.195753574371338
Validation loss: 5.161240029078658

Epoch: 6| Step: 8
Training loss: 5.104106426239014
Validation loss: 5.15441321813932

Epoch: 6| Step: 9
Training loss: 4.545845031738281
Validation loss: 5.1473456813443095

Epoch: 6| Step: 10
Training loss: 6.058802127838135
Validation loss: 5.1401751015775945

Epoch: 6| Step: 11
Training loss: 5.582098007202148
Validation loss: 5.133066654205322

Epoch: 6| Step: 12
Training loss: 5.369050979614258
Validation loss: 5.12489152211015

Epoch: 6| Step: 13
Training loss: 4.1655049324035645
Validation loss: 5.117008188719391

Epoch: 2| Step: 0
Training loss: 4.8161516189575195
Validation loss: 5.108437266401065

Epoch: 6| Step: 1
Training loss: 4.171204566955566
Validation loss: 5.099563280741374

Epoch: 6| Step: 2
Training loss: 6.330681800842285
Validation loss: 5.09028616259175

Epoch: 6| Step: 3
Training loss: 4.903697490692139
Validation loss: 5.080466460156185

Epoch: 6| Step: 4
Training loss: 4.239419937133789
Validation loss: 5.0713098997710855

Epoch: 6| Step: 5
Training loss: 5.010817050933838
Validation loss: 5.060709045779321

Epoch: 6| Step: 6
Training loss: 5.037561416625977
Validation loss: 5.050272895443824

Epoch: 6| Step: 7
Training loss: 4.2295427322387695
Validation loss: 5.038793599733743

Epoch: 6| Step: 8
Training loss: 4.938173294067383
Validation loss: 5.028014244571809

Epoch: 6| Step: 9
Training loss: 4.4155073165893555
Validation loss: 5.016035438865743

Epoch: 6| Step: 10
Training loss: 5.2594218254089355
Validation loss: 5.0035599585502375

Epoch: 6| Step: 11
Training loss: 4.7972636222839355
Validation loss: 4.990393782174715

Epoch: 6| Step: 12
Training loss: 4.794386386871338
Validation loss: 4.976454934766216

Epoch: 6| Step: 13
Training loss: 4.65631103515625
Validation loss: 4.9622879182138755

Epoch: 3| Step: 0
Training loss: 3.891875743865967
Validation loss: 4.947547799797468

Epoch: 6| Step: 1
Training loss: 3.6163668632507324
Validation loss: 4.9322136448275655

Epoch: 6| Step: 2
Training loss: 4.765568733215332
Validation loss: 4.9169666690211145

Epoch: 6| Step: 3
Training loss: 6.116776466369629
Validation loss: 4.89971455707345

Epoch: 6| Step: 4
Training loss: 4.062258243560791
Validation loss: 4.883215668380902

Epoch: 6| Step: 5
Training loss: 3.95556902885437
Validation loss: 4.864498261482485

Epoch: 6| Step: 6
Training loss: 4.861277103424072
Validation loss: 4.845747511873963

Epoch: 6| Step: 7
Training loss: 5.8082427978515625
Validation loss: 4.826752073021345

Epoch: 6| Step: 8
Training loss: 3.8753066062927246
Validation loss: 4.806994279225667

Epoch: 6| Step: 9
Training loss: 5.1070990562438965
Validation loss: 4.785775779395975

Epoch: 6| Step: 10
Training loss: 4.843159198760986
Validation loss: 4.763424478551393

Epoch: 6| Step: 11
Training loss: 4.862661838531494
Validation loss: 4.741658503009427

Epoch: 6| Step: 12
Training loss: 4.276980400085449
Validation loss: 4.719055262945032

Epoch: 6| Step: 13
Training loss: 4.645290374755859
Validation loss: 4.694982872214369

Epoch: 4| Step: 0
Training loss: 5.227425575256348
Validation loss: 4.6719301849283195

Epoch: 6| Step: 1
Training loss: 5.078507900238037
Validation loss: 4.646429436181181

Epoch: 6| Step: 2
Training loss: 5.129159450531006
Validation loss: 4.620379770955732

Epoch: 6| Step: 3
Training loss: 5.109642028808594
Validation loss: 4.594006271772487

Epoch: 6| Step: 4
Training loss: 4.411395072937012
Validation loss: 4.568548817788401

Epoch: 6| Step: 5
Training loss: 3.633608818054199
Validation loss: 4.540761422085506

Epoch: 6| Step: 6
Training loss: 3.4551877975463867
Validation loss: 4.510668159813009

Epoch: 6| Step: 7
Training loss: 4.523486137390137
Validation loss: 4.482776580318328

Epoch: 6| Step: 8
Training loss: 3.447612762451172
Validation loss: 4.4531988020866144

Epoch: 6| Step: 9
Training loss: 3.8282766342163086
Validation loss: 4.424250428394605

Epoch: 6| Step: 10
Training loss: 3.2965540885925293
Validation loss: 4.395257006409348

Epoch: 6| Step: 11
Training loss: 5.448721885681152
Validation loss: 4.367023688490673

Epoch: 6| Step: 12
Training loss: 4.454365253448486
Validation loss: 4.337772336057437

Epoch: 6| Step: 13
Training loss: 1.938099980354309
Validation loss: 4.310080179604151

Epoch: 5| Step: 0
Training loss: 3.2946994304656982
Validation loss: 4.281255542591054

Epoch: 6| Step: 1
Training loss: 4.7734880447387695
Validation loss: 4.253953269732896

Epoch: 6| Step: 2
Training loss: 3.3063199520111084
Validation loss: 4.22674814603662

Epoch: 6| Step: 3
Training loss: 4.313506126403809
Validation loss: 4.201207648041428

Epoch: 6| Step: 4
Training loss: 3.7703933715820312
Validation loss: 4.173857117211947

Epoch: 6| Step: 5
Training loss: 3.9319849014282227
Validation loss: 4.151160419628185

Epoch: 6| Step: 6
Training loss: 3.259342908859253
Validation loss: 4.12745048153785

Epoch: 6| Step: 7
Training loss: 2.6821351051330566
Validation loss: 4.101901259473575

Epoch: 6| Step: 8
Training loss: 4.779103755950928
Validation loss: 4.080821383383967

Epoch: 6| Step: 9
Training loss: 3.8272078037261963
Validation loss: 4.056209917991392

Epoch: 6| Step: 10
Training loss: 5.8931379318237305
Validation loss: 4.034444170613443

Epoch: 6| Step: 11
Training loss: 3.260909080505371
Validation loss: 4.011336854709092

Epoch: 6| Step: 12
Training loss: 4.727485179901123
Validation loss: 3.987358513698783

Epoch: 6| Step: 13
Training loss: 2.8288426399230957
Validation loss: 3.964144404216479

Epoch: 6| Step: 0
Training loss: 4.223649978637695
Validation loss: 3.941071858970068

Epoch: 6| Step: 1
Training loss: 3.545525550842285
Validation loss: 3.9194275179216937

Epoch: 6| Step: 2
Training loss: 3.4352498054504395
Validation loss: 3.898642452814246

Epoch: 6| Step: 3
Training loss: 3.766624927520752
Validation loss: 3.8796506825313775

Epoch: 6| Step: 4
Training loss: 3.736830949783325
Validation loss: 3.861712396785777

Epoch: 6| Step: 5
Training loss: 4.593727111816406
Validation loss: 3.846118034855012

Epoch: 6| Step: 6
Training loss: 3.1218271255493164
Validation loss: 3.8298469999785065

Epoch: 6| Step: 7
Training loss: 2.744772434234619
Validation loss: 3.8127358395566224

Epoch: 6| Step: 8
Training loss: 2.7944979667663574
Validation loss: 3.7964046411616827

Epoch: 6| Step: 9
Training loss: 3.9679148197174072
Validation loss: 3.782608914118941

Epoch: 6| Step: 10
Training loss: 2.6690027713775635
Validation loss: 3.767679314459524

Epoch: 6| Step: 11
Training loss: 4.655653953552246
Validation loss: 3.7520745390204975

Epoch: 6| Step: 12
Training loss: 4.56982421875
Validation loss: 3.7385108188916276

Epoch: 6| Step: 13
Training loss: 4.135031700134277
Validation loss: 3.721987239776119

Epoch: 7| Step: 0
Training loss: 2.8191535472869873
Validation loss: 3.7081699678974767

Epoch: 6| Step: 1
Training loss: 4.648776054382324
Validation loss: 3.6946956162811606

Epoch: 6| Step: 2
Training loss: 3.271726131439209
Validation loss: 3.6781648307718258

Epoch: 6| Step: 3
Training loss: 4.6415934562683105
Validation loss: 3.663728621698195

Epoch: 6| Step: 4
Training loss: 3.6121904850006104
Validation loss: 3.6495510685828423

Epoch: 6| Step: 5
Training loss: 3.3722877502441406
Validation loss: 3.636238887745847

Epoch: 6| Step: 6
Training loss: 2.1995787620544434
Validation loss: 3.621978211146529

Epoch: 6| Step: 7
Training loss: 2.9707906246185303
Validation loss: 3.6083183391119844

Epoch: 6| Step: 8
Training loss: 2.5154716968536377
Validation loss: 3.5975379046573432

Epoch: 6| Step: 9
Training loss: 4.349010467529297
Validation loss: 3.5852898987390662

Epoch: 6| Step: 10
Training loss: 4.069993495941162
Validation loss: 3.5734352193852907

Epoch: 6| Step: 11
Training loss: 4.482295989990234
Validation loss: 3.5642735547916864

Epoch: 6| Step: 12
Training loss: 3.003310203552246
Validation loss: 3.550888210214594

Epoch: 6| Step: 13
Training loss: 3.55168080329895
Validation loss: 3.541246052711241

Epoch: 8| Step: 0
Training loss: 3.4322216510772705
Validation loss: 3.5291929039903867

Epoch: 6| Step: 1
Training loss: 4.593109130859375
Validation loss: 3.5167594930177093

Epoch: 6| Step: 2
Training loss: 3.401496648788452
Validation loss: 3.507986981381652

Epoch: 6| Step: 3
Training loss: 3.1083381175994873
Validation loss: 3.4971317680933143

Epoch: 6| Step: 4
Training loss: 2.917032241821289
Validation loss: 3.487344398293444

Epoch: 6| Step: 5
Training loss: 3.3045294284820557
Validation loss: 3.477545856147684

Epoch: 6| Step: 6
Training loss: 3.3701493740081787
Validation loss: 3.4698741974369174

Epoch: 6| Step: 7
Training loss: 2.7062203884124756
Validation loss: 3.4553348069549887

Epoch: 6| Step: 8
Training loss: 3.532371997833252
Validation loss: 3.4464082256440194

Epoch: 6| Step: 9
Training loss: 2.92526912689209
Validation loss: 3.4410267132584766

Epoch: 6| Step: 10
Training loss: 4.405590534210205
Validation loss: 3.4326437621988277

Epoch: 6| Step: 11
Training loss: 2.4263415336608887
Validation loss: 3.4248839629593717

Epoch: 6| Step: 12
Training loss: 3.9894256591796875
Validation loss: 3.4143585646024315

Epoch: 6| Step: 13
Training loss: 3.600252389907837
Validation loss: 3.4058361771286174

Epoch: 9| Step: 0
Training loss: 2.800055503845215
Validation loss: 3.3975971129632767

Epoch: 6| Step: 1
Training loss: 3.7894439697265625
Validation loss: 3.389940023422241

Epoch: 6| Step: 2
Training loss: 3.8052704334259033
Validation loss: 3.3817278108289166

Epoch: 6| Step: 3
Training loss: 3.349668025970459
Validation loss: 3.374277901905839

Epoch: 6| Step: 4
Training loss: 4.225189208984375
Validation loss: 3.368410530910697

Epoch: 6| Step: 5
Training loss: 3.621792793273926
Validation loss: 3.36173152923584

Epoch: 6| Step: 6
Training loss: 3.0475516319274902
Validation loss: 3.3550064743206067

Epoch: 6| Step: 7
Training loss: 4.30632209777832
Validation loss: 3.3498425611885647

Epoch: 6| Step: 8
Training loss: 2.427000045776367
Validation loss: 3.339663541445168

Epoch: 6| Step: 9
Training loss: 3.2013192176818848
Validation loss: 3.3329488615835867

Epoch: 6| Step: 10
Training loss: 3.1493148803710938
Validation loss: 3.328240707356443

Epoch: 6| Step: 11
Training loss: 2.2825112342834473
Validation loss: 3.3191246653115876

Epoch: 6| Step: 12
Training loss: 3.169154167175293
Validation loss: 3.314240768391599

Epoch: 6| Step: 13
Training loss: 3.1522936820983887
Validation loss: 3.3085873408984114

Epoch: 10| Step: 0
Training loss: 4.2262187004089355
Validation loss: 3.301835698466147

Epoch: 6| Step: 1
Training loss: 2.598527193069458
Validation loss: 3.296531269627233

Epoch: 6| Step: 2
Training loss: 3.1194815635681152
Validation loss: 3.2916386127471924

Epoch: 6| Step: 3
Training loss: 2.978233814239502
Validation loss: 3.283643299533475

Epoch: 6| Step: 4
Training loss: 3.5914881229400635
Validation loss: 3.2803537948157198

Epoch: 6| Step: 5
Training loss: 3.5519278049468994
Validation loss: 3.2737414862519953

Epoch: 6| Step: 6
Training loss: 3.039185047149658
Validation loss: 3.268609598118772

Epoch: 6| Step: 7
Training loss: 3.9803028106689453
Validation loss: 3.264445509961856

Epoch: 6| Step: 8
Training loss: 2.3327817916870117
Validation loss: 3.2580881323865665

Epoch: 6| Step: 9
Training loss: 2.517462730407715
Validation loss: 3.250350254838185

Epoch: 6| Step: 10
Training loss: 3.185791015625
Validation loss: 3.2470884861484652

Epoch: 6| Step: 11
Training loss: 3.361311912536621
Validation loss: 3.2390122003452753

Epoch: 6| Step: 12
Training loss: 3.51832914352417
Validation loss: 3.2329521717563754

Epoch: 6| Step: 13
Training loss: 3.44166898727417
Validation loss: 3.2275779862557687

Epoch: 11| Step: 0
Training loss: 2.8505561351776123
Validation loss: 3.2244654855420514

Epoch: 6| Step: 1
Training loss: 2.9832165241241455
Validation loss: 3.2170677749059533

Epoch: 6| Step: 2
Training loss: 3.4602458477020264
Validation loss: 3.2141431300870833

Epoch: 6| Step: 3
Training loss: 3.678441047668457
Validation loss: 3.2069912674606487

Epoch: 6| Step: 4
Training loss: 3.2223682403564453
Validation loss: 3.2017349735383065

Epoch: 6| Step: 5
Training loss: 3.257181406021118
Validation loss: 3.200580253395983

Epoch: 6| Step: 6
Training loss: 3.7699508666992188
Validation loss: 3.192509392256378

Epoch: 6| Step: 7
Training loss: 2.038975238800049
Validation loss: 3.1884985739184963

Epoch: 6| Step: 8
Training loss: 2.781694173812866
Validation loss: 3.181893733239943

Epoch: 6| Step: 9
Training loss: 2.6995317935943604
Validation loss: 3.1783202489217124

Epoch: 6| Step: 10
Training loss: 3.376948356628418
Validation loss: 3.1738962460589666

Epoch: 6| Step: 11
Training loss: 3.4775795936584473
Validation loss: 3.17135202500128

Epoch: 6| Step: 12
Training loss: 3.7975289821624756
Validation loss: 3.164821042809435

Epoch: 6| Step: 13
Training loss: 3.3257246017456055
Validation loss: 3.157687376904231

Epoch: 12| Step: 0
Training loss: 3.168855905532837
Validation loss: 3.154499166755266

Epoch: 6| Step: 1
Training loss: 2.1367905139923096
Validation loss: 3.1496448747573362

Epoch: 6| Step: 2
Training loss: 2.8893675804138184
Validation loss: 3.1501037869402158

Epoch: 6| Step: 3
Training loss: 2.939826488494873
Validation loss: 3.143740351482104

Epoch: 6| Step: 4
Training loss: 2.9699153900146484
Validation loss: 3.137841952744351

Epoch: 6| Step: 5
Training loss: 3.1771559715270996
Validation loss: 3.1316131802015406

Epoch: 6| Step: 6
Training loss: 3.7431559562683105
Validation loss: 3.130411696690385

Epoch: 6| Step: 7
Training loss: 2.389592170715332
Validation loss: 3.12349598638473

Epoch: 6| Step: 8
Training loss: 3.9462697505950928
Validation loss: 3.120721291470271

Epoch: 6| Step: 9
Training loss: 2.673574447631836
Validation loss: 3.114018006991315

Epoch: 6| Step: 10
Training loss: 3.354146957397461
Validation loss: 3.112035530869679

Epoch: 6| Step: 11
Training loss: 4.075076103210449
Validation loss: 3.104690464594031

Epoch: 6| Step: 12
Training loss: 3.638538360595703
Validation loss: 3.1027101162941224

Epoch: 6| Step: 13
Training loss: 2.7602834701538086
Validation loss: 3.099437472640827

Epoch: 13| Step: 0
Training loss: 2.5773892402648926
Validation loss: 3.098441380326466

Epoch: 6| Step: 1
Training loss: 2.486271381378174
Validation loss: 3.093955006650699

Epoch: 6| Step: 2
Training loss: 2.561385154724121
Validation loss: 3.0872497661139375

Epoch: 6| Step: 3
Training loss: 2.04681658744812
Validation loss: 3.0843935679363947

Epoch: 6| Step: 4
Training loss: 3.26979398727417
Validation loss: 3.0816785673941336

Epoch: 6| Step: 5
Training loss: 3.594252824783325
Validation loss: 3.075631146789879

Epoch: 6| Step: 6
Training loss: 2.715287685394287
Validation loss: 3.0750888316862044

Epoch: 6| Step: 7
Training loss: 3.1330084800720215
Validation loss: 3.0712518333106913

Epoch: 6| Step: 8
Training loss: 3.4449217319488525
Validation loss: 3.069255949348532

Epoch: 6| Step: 9
Training loss: 3.3473868370056152
Validation loss: 3.062009332000568

Epoch: 6| Step: 10
Training loss: 3.109337329864502
Validation loss: 3.059520298434842

Epoch: 6| Step: 11
Training loss: 3.9253242015838623
Validation loss: 3.0579373387880224

Epoch: 6| Step: 12
Training loss: 3.5652949810028076
Validation loss: 3.0532808970379572

Epoch: 6| Step: 13
Training loss: 4.232654094696045
Validation loss: 3.0489548406293316

Epoch: 14| Step: 0
Training loss: 3.57029390335083
Validation loss: 3.0446172042559554

Epoch: 6| Step: 1
Training loss: 2.824080228805542
Validation loss: 3.045039217959168

Epoch: 6| Step: 2
Training loss: 2.706308603286743
Validation loss: 3.039068555319181

Epoch: 6| Step: 3
Training loss: 3.1569671630859375
Validation loss: 3.037442725191834

Epoch: 6| Step: 4
Training loss: 3.756279945373535
Validation loss: 3.0353898181710193

Epoch: 6| Step: 5
Training loss: 3.966679096221924
Validation loss: 3.0322939170304166

Epoch: 6| Step: 6
Training loss: 3.8412365913391113
Validation loss: 3.0243848549422396

Epoch: 6| Step: 7
Training loss: 2.630319356918335
Validation loss: 3.0244677938440794

Epoch: 6| Step: 8
Training loss: 2.6896166801452637
Validation loss: 3.022149552581131

Epoch: 6| Step: 9
Training loss: 2.4253358840942383
Validation loss: 3.0184197656569944

Epoch: 6| Step: 10
Training loss: 1.9420621395111084
Validation loss: 3.0164609057928926

Epoch: 6| Step: 11
Training loss: 3.98270845413208
Validation loss: 3.0149865868271037

Epoch: 6| Step: 12
Training loss: 2.7769289016723633
Validation loss: 3.0109944420476116

Epoch: 6| Step: 13
Training loss: 2.7123026847839355
Validation loss: 3.0110008383309967

Epoch: 15| Step: 0
Training loss: 2.533179521560669
Validation loss: 3.011930159343186

Epoch: 6| Step: 1
Training loss: 3.992379665374756
Validation loss: 3.0080808747199272

Epoch: 6| Step: 2
Training loss: 2.9714913368225098
Validation loss: 3.006696726686211

Epoch: 6| Step: 3
Training loss: 3.5620052814483643
Validation loss: 3.0038555898974018

Epoch: 6| Step: 4
Training loss: 3.821770668029785
Validation loss: 3.003812713007773

Epoch: 6| Step: 5
Training loss: 3.908245325088501
Validation loss: 3.002305035950035

Epoch: 6| Step: 6
Training loss: 2.19952654838562
Validation loss: 2.9961783296318463

Epoch: 6| Step: 7
Training loss: 2.890275001525879
Validation loss: 2.996919103848037

Epoch: 6| Step: 8
Training loss: 2.986504554748535
Validation loss: 2.9926733483550367

Epoch: 6| Step: 9
Training loss: 3.091653823852539
Validation loss: 2.990695802114343

Epoch: 6| Step: 10
Training loss: 2.243596315383911
Validation loss: 2.9909749646340646

Epoch: 6| Step: 11
Training loss: 3.5819931030273438
Validation loss: 2.9852020586690595

Epoch: 6| Step: 12
Training loss: 2.7798564434051514
Validation loss: 2.984036830163771

Epoch: 6| Step: 13
Training loss: 1.7030224800109863
Validation loss: 2.979206259532641

Epoch: 16| Step: 0
Training loss: 3.415686845779419
Validation loss: 2.9800681990961873

Epoch: 6| Step: 1
Training loss: 3.2675464153289795
Validation loss: 2.9799533890139673

Epoch: 6| Step: 2
Training loss: 3.276608943939209
Validation loss: 2.97946229545019

Epoch: 6| Step: 3
Training loss: 2.161771535873413
Validation loss: 2.970924215932046

Epoch: 6| Step: 4
Training loss: 2.223477840423584
Validation loss: 2.970313387532388

Epoch: 6| Step: 5
Training loss: 2.6304874420166016
Validation loss: 2.9689933484600437

Epoch: 6| Step: 6
Training loss: 3.8377585411071777
Validation loss: 2.966513331218432

Epoch: 6| Step: 7
Training loss: 2.694981098175049
Validation loss: 2.9651269066718315

Epoch: 6| Step: 8
Training loss: 3.24582576751709
Validation loss: 2.967012174667851

Epoch: 6| Step: 9
Training loss: 3.190969705581665
Validation loss: 2.9638158813599618

Epoch: 6| Step: 10
Training loss: 3.2998030185699463
Validation loss: 2.9675841408391155

Epoch: 6| Step: 11
Training loss: 3.332911491394043
Validation loss: 2.963943140481108

Epoch: 6| Step: 12
Training loss: 3.2944793701171875
Validation loss: 2.9655374019376692

Epoch: 6| Step: 13
Training loss: 2.5504744052886963
Validation loss: 2.9678728221565165

Epoch: 17| Step: 0
Training loss: 2.2699217796325684
Validation loss: 2.961521651155205

Epoch: 6| Step: 1
Training loss: 3.912116050720215
Validation loss: 2.9600951748509563

Epoch: 6| Step: 2
Training loss: 3.168999671936035
Validation loss: 2.957427388878279

Epoch: 6| Step: 3
Training loss: 3.4729881286621094
Validation loss: 2.957852312313613

Epoch: 6| Step: 4
Training loss: 2.671713352203369
Validation loss: 2.952888527224141

Epoch: 6| Step: 5
Training loss: 3.4382824897766113
Validation loss: 2.9516756714031263

Epoch: 6| Step: 6
Training loss: 2.9945554733276367
Validation loss: 2.9498066927797053

Epoch: 6| Step: 7
Training loss: 3.1849682331085205
Validation loss: 2.948263399062618

Epoch: 6| Step: 8
Training loss: 3.6125402450561523
Validation loss: 2.9470655020847114

Epoch: 6| Step: 9
Training loss: 2.256291389465332
Validation loss: 2.945424372150052

Epoch: 6| Step: 10
Training loss: 2.320675849914551
Validation loss: 2.9434560652702086

Epoch: 6| Step: 11
Training loss: 2.6536951065063477
Validation loss: 2.9415797905255388

Epoch: 6| Step: 12
Training loss: 3.1432671546936035
Validation loss: 2.9410299972821305

Epoch: 6| Step: 13
Training loss: 3.5611817836761475
Validation loss: 2.9358623309801986

Epoch: 18| Step: 0
Training loss: 3.273064374923706
Validation loss: 2.9343946056981243

Epoch: 6| Step: 1
Training loss: 3.0326085090637207
Validation loss: 2.9334580744466474

Epoch: 6| Step: 2
Training loss: 4.330528259277344
Validation loss: 2.9309121947134695

Epoch: 6| Step: 3
Training loss: 3.1332366466522217
Validation loss: 2.9294637428816928

Epoch: 6| Step: 4
Training loss: 3.2251973152160645
Validation loss: 2.926956222903344

Epoch: 6| Step: 5
Training loss: 2.698342800140381
Validation loss: 2.9274815795242146

Epoch: 6| Step: 6
Training loss: 3.4745514392852783
Validation loss: 2.9246115043599117

Epoch: 6| Step: 7
Training loss: 2.6033883094787598
Validation loss: 2.9208658433729604

Epoch: 6| Step: 8
Training loss: 3.056682586669922
Validation loss: 2.9217147493875153

Epoch: 6| Step: 9
Training loss: 2.782693386077881
Validation loss: 2.9193645728531705

Epoch: 6| Step: 10
Training loss: 2.742690086364746
Validation loss: 2.917118236582766

Epoch: 6| Step: 11
Training loss: 2.241882562637329
Validation loss: 2.918436368306478

Epoch: 6| Step: 12
Training loss: 3.262402057647705
Validation loss: 2.91662690460041

Epoch: 6| Step: 13
Training loss: 1.8761162757873535
Validation loss: 2.9219131956818285

Epoch: 19| Step: 0
Training loss: 3.077206611633301
Validation loss: 2.924620364301948

Epoch: 6| Step: 1
Training loss: 3.577014446258545
Validation loss: 2.9267888325516895

Epoch: 6| Step: 2
Training loss: 2.9915859699249268
Validation loss: 2.926347122397474

Epoch: 6| Step: 3
Training loss: 3.0839614868164062
Validation loss: 2.9244924053069083

Epoch: 6| Step: 4
Training loss: 2.128798007965088
Validation loss: 2.9148655245381017

Epoch: 6| Step: 5
Training loss: 2.3473060131073
Validation loss: 2.9141881953003588

Epoch: 6| Step: 6
Training loss: 2.417177200317383
Validation loss: 2.911201605232813

Epoch: 6| Step: 7
Training loss: 4.118290901184082
Validation loss: 2.91042499901146

Epoch: 6| Step: 8
Training loss: 3.9518513679504395
Validation loss: 2.9051182577686925

Epoch: 6| Step: 9
Training loss: 2.928948402404785
Validation loss: 2.9036031820440806

Epoch: 6| Step: 10
Training loss: 2.4655723571777344
Validation loss: 2.905377621291786

Epoch: 6| Step: 11
Training loss: 3.1824440956115723
Validation loss: 2.9032188820582565

Epoch: 6| Step: 12
Training loss: 2.9922890663146973
Validation loss: 2.902041512150918

Epoch: 6| Step: 13
Training loss: 2.7548646926879883
Validation loss: 2.9024557221320366

Epoch: 20| Step: 0
Training loss: 2.3324170112609863
Validation loss: 2.898618193082912

Epoch: 6| Step: 1
Training loss: 2.1736395359039307
Validation loss: 2.89773400111865

Epoch: 6| Step: 2
Training loss: 2.4763622283935547
Validation loss: 2.8980867837065007

Epoch: 6| Step: 3
Training loss: 3.75495982170105
Validation loss: 2.8947650206986295

Epoch: 6| Step: 4
Training loss: 2.9421958923339844
Validation loss: 2.8948435270658104

Epoch: 6| Step: 5
Training loss: 2.1493020057678223
Validation loss: 2.8951300779978433

Epoch: 6| Step: 6
Training loss: 3.2819385528564453
Validation loss: 2.8947031959410636

Epoch: 6| Step: 7
Training loss: 2.3946268558502197
Validation loss: 2.891493671683855

Epoch: 6| Step: 8
Training loss: 3.1362459659576416
Validation loss: 2.8866711380661174

Epoch: 6| Step: 9
Training loss: 2.635786771774292
Validation loss: 2.8911087205333095

Epoch: 6| Step: 10
Training loss: 3.62839674949646
Validation loss: 2.8882669377070602

Epoch: 6| Step: 11
Training loss: 3.9887466430664062
Validation loss: 2.886215573997908

Epoch: 6| Step: 12
Training loss: 3.5340921878814697
Validation loss: 2.8875601266020086

Epoch: 6| Step: 13
Training loss: 3.8204030990600586
Validation loss: 2.882781567112092

Epoch: 21| Step: 0
Training loss: 3.681964635848999
Validation loss: 2.883151526092201

Epoch: 6| Step: 1
Training loss: 3.4695351123809814
Validation loss: 2.8825249953936507

Epoch: 6| Step: 2
Training loss: 2.9841184616088867
Validation loss: 2.879877454491072

Epoch: 6| Step: 3
Training loss: 3.2982053756713867
Validation loss: 2.87642526882951

Epoch: 6| Step: 4
Training loss: 2.953871965408325
Validation loss: 2.8758184063819145

Epoch: 6| Step: 5
Training loss: 1.9538819789886475
Validation loss: 2.8734443751714562

Epoch: 6| Step: 6
Training loss: 3.1175293922424316
Validation loss: 2.874028423781036

Epoch: 6| Step: 7
Training loss: 2.923684597015381
Validation loss: 2.8726259841713855

Epoch: 6| Step: 8
Training loss: 3.098148822784424
Validation loss: 2.8693030726525093

Epoch: 6| Step: 9
Training loss: 2.4717745780944824
Validation loss: 2.8678242801338114

Epoch: 6| Step: 10
Training loss: 3.053352117538452
Validation loss: 2.8687397997866393

Epoch: 6| Step: 11
Training loss: 3.139991283416748
Validation loss: 2.8680291586024786

Epoch: 6| Step: 12
Training loss: 2.541248321533203
Validation loss: 2.865357586132583

Epoch: 6| Step: 13
Training loss: 3.0766491889953613
Validation loss: 2.864805826576807

Epoch: 22| Step: 0
Training loss: 2.3310461044311523
Validation loss: 2.8624379557947957

Epoch: 6| Step: 1
Training loss: 3.777693033218384
Validation loss: 2.8635671318218274

Epoch: 6| Step: 2
Training loss: 3.2913408279418945
Validation loss: 2.8613179499103176

Epoch: 6| Step: 3
Training loss: 3.0307517051696777
Validation loss: 2.8567197245936238

Epoch: 6| Step: 4
Training loss: 2.748847484588623
Validation loss: 2.8566472914911087

Epoch: 6| Step: 5
Training loss: 3.0847487449645996
Validation loss: 2.8558268905967794

Epoch: 6| Step: 6
Training loss: 2.2543325424194336
Validation loss: 2.8553657993193595

Epoch: 6| Step: 7
Training loss: 3.528073787689209
Validation loss: 2.8521232912617345

Epoch: 6| Step: 8
Training loss: 3.2430167198181152
Validation loss: 2.8495504727927585

Epoch: 6| Step: 9
Training loss: 3.049229145050049
Validation loss: 2.8503788799367924

Epoch: 6| Step: 10
Training loss: 3.477623462677002
Validation loss: 2.8476946969186105

Epoch: 6| Step: 11
Training loss: 2.1210086345672607
Validation loss: 2.8453811317361812

Epoch: 6| Step: 12
Training loss: 2.4823966026306152
Validation loss: 2.8452618019555205

Epoch: 6| Step: 13
Training loss: 3.2071197032928467
Validation loss: 2.8431215260618474

Epoch: 23| Step: 0
Training loss: 3.195002555847168
Validation loss: 2.8425959540951635

Epoch: 6| Step: 1
Training loss: 2.8957722187042236
Validation loss: 2.8363432576579433

Epoch: 6| Step: 2
Training loss: 3.243684768676758
Validation loss: 2.8323048494195424

Epoch: 6| Step: 3
Training loss: 3.2426083087921143
Validation loss: 2.8339928401413785

Epoch: 6| Step: 4
Training loss: 3.3206663131713867
Validation loss: 2.830046166655838

Epoch: 6| Step: 5
Training loss: 2.3935842514038086
Validation loss: 2.832478272017612

Epoch: 6| Step: 6
Training loss: 3.147068977355957
Validation loss: 2.82977210321734

Epoch: 6| Step: 7
Training loss: 2.964132785797119
Validation loss: 2.8278840075257006

Epoch: 6| Step: 8
Training loss: 3.243478536605835
Validation loss: 2.8237991294553204

Epoch: 6| Step: 9
Training loss: 3.219116449356079
Validation loss: 2.8233244803643998

Epoch: 6| Step: 10
Training loss: 3.290323495864868
Validation loss: 2.8225277444367767

Epoch: 6| Step: 11
Training loss: 1.6865711212158203
Validation loss: 2.822371828940607

Epoch: 6| Step: 12
Training loss: 2.8872337341308594
Validation loss: 2.819501087229739

Epoch: 6| Step: 13
Training loss: 2.350130558013916
Validation loss: 2.8212805255766837

Epoch: 24| Step: 0
Training loss: 3.617678642272949
Validation loss: 2.8183967016076528

Epoch: 6| Step: 1
Training loss: 2.9026527404785156
Validation loss: 2.8178043724388204

Epoch: 6| Step: 2
Training loss: 3.5052170753479004
Validation loss: 2.8125258568794496

Epoch: 6| Step: 3
Training loss: 2.972017765045166
Validation loss: 2.8129073522424184

Epoch: 6| Step: 4
Training loss: 2.3133726119995117
Validation loss: 2.81225331880713

Epoch: 6| Step: 5
Training loss: 3.3193259239196777
Validation loss: 2.814111837776758

Epoch: 6| Step: 6
Training loss: 2.3840813636779785
Validation loss: 2.8165868123372397

Epoch: 6| Step: 7
Training loss: 3.2760348320007324
Validation loss: 2.814835984219787

Epoch: 6| Step: 8
Training loss: 3.4026098251342773
Validation loss: 2.8264104320156958

Epoch: 6| Step: 9
Training loss: 2.2860732078552246
Validation loss: 2.8109331336072696

Epoch: 6| Step: 10
Training loss: 3.2870922088623047
Validation loss: 2.8077835703408844

Epoch: 6| Step: 11
Training loss: 3.5206351280212402
Validation loss: 2.806838245802028

Epoch: 6| Step: 12
Training loss: 1.3881335258483887
Validation loss: 2.803187667682607

Epoch: 6| Step: 13
Training loss: 3.02034592628479
Validation loss: 2.804492704329952

Epoch: 25| Step: 0
Training loss: 2.2210965156555176
Validation loss: 2.8015391493356354

Epoch: 6| Step: 1
Training loss: 2.851633071899414
Validation loss: 2.804620653070429

Epoch: 6| Step: 2
Training loss: 3.6942977905273438
Validation loss: 2.804799238840739

Epoch: 6| Step: 3
Training loss: 2.2637956142425537
Validation loss: 2.8024639339857202

Epoch: 6| Step: 4
Training loss: 2.895246982574463
Validation loss: 2.8017493627404653

Epoch: 6| Step: 5
Training loss: 2.8674097061157227
Validation loss: 2.8032071205877487

Epoch: 6| Step: 6
Training loss: 2.738281726837158
Validation loss: 2.7991546277076966

Epoch: 6| Step: 7
Training loss: 2.4672017097473145
Validation loss: 2.7989239000505015

Epoch: 6| Step: 8
Training loss: 3.2389516830444336
Validation loss: 2.7958330518455914

Epoch: 6| Step: 9
Training loss: 3.14312744140625
Validation loss: 2.792354496576453

Epoch: 6| Step: 10
Training loss: 2.886812686920166
Validation loss: 2.792311999105638

Epoch: 6| Step: 11
Training loss: 3.329310655593872
Validation loss: 2.7900668523644887

Epoch: 6| Step: 12
Training loss: 3.270156145095825
Validation loss: 2.7926549732044177

Epoch: 6| Step: 13
Training loss: 3.398847818374634
Validation loss: 2.787909161659979

Epoch: 26| Step: 0
Training loss: 1.9843828678131104
Validation loss: 2.786707321802775

Epoch: 6| Step: 1
Training loss: 2.597830295562744
Validation loss: 2.7888510252839778

Epoch: 6| Step: 2
Training loss: 3.2922539710998535
Validation loss: 2.786478304093884

Epoch: 6| Step: 3
Training loss: 2.7575411796569824
Validation loss: 2.786799689774872

Epoch: 6| Step: 4
Training loss: 3.071943998336792
Validation loss: 2.7881070644624772

Epoch: 6| Step: 5
Training loss: 3.480221748352051
Validation loss: 2.7867140744322088

Epoch: 6| Step: 6
Training loss: 3.305891275405884
Validation loss: 2.7810878266570387

Epoch: 6| Step: 7
Training loss: 3.1082897186279297
Validation loss: 2.775884641114102

Epoch: 6| Step: 8
Training loss: 2.9532699584960938
Validation loss: 2.780985347686275

Epoch: 6| Step: 9
Training loss: 3.312593698501587
Validation loss: 2.7770826970377276

Epoch: 6| Step: 10
Training loss: 2.9663009643554688
Validation loss: 2.777898262905818

Epoch: 6| Step: 11
Training loss: 2.881847381591797
Validation loss: 2.7757414771664526

Epoch: 6| Step: 12
Training loss: 2.386911392211914
Validation loss: 2.7790616481534895

Epoch: 6| Step: 13
Training loss: 2.6814584732055664
Validation loss: 2.7751950781832457

Epoch: 27| Step: 0
Training loss: 2.736072301864624
Validation loss: 2.7764703919810634

Epoch: 6| Step: 1
Training loss: 2.674731731414795
Validation loss: 2.7762518595623713

Epoch: 6| Step: 2
Training loss: 3.864292621612549
Validation loss: 2.784511553343906

Epoch: 6| Step: 3
Training loss: 2.54805326461792
Validation loss: 2.7777573882892566

Epoch: 6| Step: 4
Training loss: 1.634382963180542
Validation loss: 2.7781382094147387

Epoch: 6| Step: 5
Training loss: 3.274949312210083
Validation loss: 2.780366292563818

Epoch: 6| Step: 6
Training loss: 2.5390591621398926
Validation loss: 2.7784864005222114

Epoch: 6| Step: 7
Training loss: 3.1294479370117188
Validation loss: 2.7778221945608816

Epoch: 6| Step: 8
Training loss: 2.51570987701416
Validation loss: 2.7744702267390426

Epoch: 6| Step: 9
Training loss: 3.1490273475646973
Validation loss: 2.7708455054990706

Epoch: 6| Step: 10
Training loss: 3.5980067253112793
Validation loss: 2.77097443098663

Epoch: 6| Step: 11
Training loss: 3.2953946590423584
Validation loss: 2.766229849989696

Epoch: 6| Step: 12
Training loss: 3.4753565788269043
Validation loss: 2.7669459158374416

Epoch: 6| Step: 13
Training loss: 1.945340633392334
Validation loss: 2.765844545056743

Epoch: 28| Step: 0
Training loss: 3.1483991146087646
Validation loss: 2.7686007317676338

Epoch: 6| Step: 1
Training loss: 2.2845618724823
Validation loss: 2.7686466863078456

Epoch: 6| Step: 2
Training loss: 3.141892671585083
Validation loss: 2.7607374857830744

Epoch: 6| Step: 3
Training loss: 2.900454044342041
Validation loss: 2.764209706296203

Epoch: 6| Step: 4
Training loss: 3.006809711456299
Validation loss: 2.763682185962636

Epoch: 6| Step: 5
Training loss: 2.645569324493408
Validation loss: 2.7628048594279955

Epoch: 6| Step: 6
Training loss: 3.2753372192382812
Validation loss: 2.7633157032792286

Epoch: 6| Step: 7
Training loss: 2.329770565032959
Validation loss: 2.7572616736094155

Epoch: 6| Step: 8
Training loss: 3.659878730773926
Validation loss: 2.7563767048620407

Epoch: 6| Step: 9
Training loss: 2.243222713470459
Validation loss: 2.7565771277232836

Epoch: 6| Step: 10
Training loss: 3.4705851078033447
Validation loss: 2.756907255418839

Epoch: 6| Step: 11
Training loss: 3.017094373703003
Validation loss: 2.7575553078805246

Epoch: 6| Step: 12
Training loss: 2.852053642272949
Validation loss: 2.755213306796166

Epoch: 6| Step: 13
Training loss: 2.6090855598449707
Validation loss: 2.7564381168734644

Epoch: 29| Step: 0
Training loss: 2.9643666744232178
Validation loss: 2.7534412286614858

Epoch: 6| Step: 1
Training loss: 2.927912712097168
Validation loss: 2.755162436475036

Epoch: 6| Step: 2
Training loss: 2.5301270484924316
Validation loss: 2.751756347635741

Epoch: 6| Step: 3
Training loss: 3.217672824859619
Validation loss: 2.753737247118386

Epoch: 6| Step: 4
Training loss: 1.9618182182312012
Validation loss: 2.7534282233125422

Epoch: 6| Step: 5
Training loss: 2.6811509132385254
Validation loss: 2.7522244543157597

Epoch: 6| Step: 6
Training loss: 1.9527645111083984
Validation loss: 2.749563568381853

Epoch: 6| Step: 7
Training loss: 3.134944200515747
Validation loss: 2.7462075807714976

Epoch: 6| Step: 8
Training loss: 3.840886354446411
Validation loss: 2.745149479117445

Epoch: 6| Step: 9
Training loss: 3.1570563316345215
Validation loss: 2.747214768522529

Epoch: 6| Step: 10
Training loss: 2.4911305904388428
Validation loss: 2.750499379250311

Epoch: 6| Step: 11
Training loss: 3.3970344066619873
Validation loss: 2.7489120985872004

Epoch: 6| Step: 12
Training loss: 2.8149056434631348
Validation loss: 2.7471615934884674

Epoch: 6| Step: 13
Training loss: 4.050729751586914
Validation loss: 2.748648614011785

Epoch: 30| Step: 0
Training loss: 2.664858341217041
Validation loss: 2.750295941547681

Epoch: 6| Step: 1
Training loss: 3.8422083854675293
Validation loss: 2.751695256079397

Epoch: 6| Step: 2
Training loss: 1.9365582466125488
Validation loss: 2.7522124962140153

Epoch: 6| Step: 3
Training loss: 2.243194818496704
Validation loss: 2.7553868857763146

Epoch: 6| Step: 4
Training loss: 2.8089535236358643
Validation loss: 2.7546078620418424

Epoch: 6| Step: 5
Training loss: 3.189213275909424
Validation loss: 2.752706394400648

Epoch: 6| Step: 6
Training loss: 2.994469165802002
Validation loss: 2.7479008167020735

Epoch: 6| Step: 7
Training loss: 2.85294771194458
Validation loss: 2.747830255057222

Epoch: 6| Step: 8
Training loss: 2.8131110668182373
Validation loss: 2.7432615474988054

Epoch: 6| Step: 9
Training loss: 3.493623971939087
Validation loss: 2.738710723897462

Epoch: 6| Step: 10
Training loss: 2.4752373695373535
Validation loss: 2.7358245490699686

Epoch: 6| Step: 11
Training loss: 2.839142322540283
Validation loss: 2.734141149828511

Epoch: 6| Step: 12
Training loss: 3.297173500061035
Validation loss: 2.736180172171644

Epoch: 6| Step: 13
Training loss: 3.326725959777832
Validation loss: 2.7320201140578075

Epoch: 31| Step: 0
Training loss: 3.0076873302459717
Validation loss: 2.735066170333534

Epoch: 6| Step: 1
Training loss: 3.175875663757324
Validation loss: 2.7285129921410674

Epoch: 6| Step: 2
Training loss: 2.212958812713623
Validation loss: 2.7235818601423696

Epoch: 6| Step: 3
Training loss: 2.5260283946990967
Validation loss: 2.7285880914298435

Epoch: 6| Step: 4
Training loss: 2.4666197299957275
Validation loss: 2.728524746433381

Epoch: 6| Step: 5
Training loss: 2.8863096237182617
Validation loss: 2.729036215812929

Epoch: 6| Step: 6
Training loss: 4.259669780731201
Validation loss: 2.7260062181821434

Epoch: 6| Step: 7
Training loss: 2.1684203147888184
Validation loss: 2.7254669948290755

Epoch: 6| Step: 8
Training loss: 3.090111255645752
Validation loss: 2.722628193516885

Epoch: 6| Step: 9
Training loss: 2.60001802444458
Validation loss: 2.716112764932776

Epoch: 6| Step: 10
Training loss: 2.8652312755584717
Validation loss: 2.721042566401984

Epoch: 6| Step: 11
Training loss: 3.03542423248291
Validation loss: 2.7169491731992332

Epoch: 6| Step: 12
Training loss: 2.8177199363708496
Validation loss: 2.736362167583999

Epoch: 6| Step: 13
Training loss: 3.6628239154815674
Validation loss: 2.7503887837932957

Epoch: 32| Step: 0
Training loss: 2.674384593963623
Validation loss: 2.720711220977127

Epoch: 6| Step: 1
Training loss: 2.7922568321228027
Validation loss: 2.7151752979524675

Epoch: 6| Step: 2
Training loss: 4.051651954650879
Validation loss: 2.7111379818249772

Epoch: 6| Step: 3
Training loss: 3.1827173233032227
Validation loss: 2.708454288462157

Epoch: 6| Step: 4
Training loss: 2.4659688472747803
Validation loss: 2.711266891930693

Epoch: 6| Step: 5
Training loss: 3.045807361602783
Validation loss: 2.7120079173836658

Epoch: 6| Step: 6
Training loss: 2.7744853496551514
Validation loss: 2.7111600368253645

Epoch: 6| Step: 7
Training loss: 2.78524112701416
Validation loss: 2.7116364612374255

Epoch: 6| Step: 8
Training loss: 2.9931745529174805
Validation loss: 2.7113419809649066

Epoch: 6| Step: 9
Training loss: 3.419548273086548
Validation loss: 2.711432885098201

Epoch: 6| Step: 10
Training loss: 2.2794885635375977
Validation loss: 2.7017455972651

Epoch: 6| Step: 11
Training loss: 3.156190872192383
Validation loss: 2.705229687434371

Epoch: 6| Step: 12
Training loss: 2.081498622894287
Validation loss: 2.701210896174113

Epoch: 6| Step: 13
Training loss: 2.2788445949554443
Validation loss: 2.702749667629119

Epoch: 33| Step: 0
Training loss: 1.9660040140151978
Validation loss: 2.7003156549187115

Epoch: 6| Step: 1
Training loss: 2.4510793685913086
Validation loss: 2.7004152510755803

Epoch: 6| Step: 2
Training loss: 3.080495834350586
Validation loss: 2.6982585563454577

Epoch: 6| Step: 3
Training loss: 3.4212474822998047
Validation loss: 2.7002126529652584

Epoch: 6| Step: 4
Training loss: 3.073753833770752
Validation loss: 2.6983237830541467

Epoch: 6| Step: 5
Training loss: 2.2064051628112793
Validation loss: 2.697741216228854

Epoch: 6| Step: 6
Training loss: 2.3621630668640137
Validation loss: 2.6972835653571674

Epoch: 6| Step: 7
Training loss: 2.4848122596740723
Validation loss: 2.695900132579188

Epoch: 6| Step: 8
Training loss: 3.7238874435424805
Validation loss: 2.694632053375244

Epoch: 6| Step: 9
Training loss: 2.922739028930664
Validation loss: 2.6929911644228044

Epoch: 6| Step: 10
Training loss: 2.4760093688964844
Validation loss: 2.6927620339137253

Epoch: 6| Step: 11
Training loss: 3.7293975353240967
Validation loss: 2.69374321609415

Epoch: 6| Step: 12
Training loss: 3.5573172569274902
Validation loss: 2.6943401316160798

Epoch: 6| Step: 13
Training loss: 2.5336172580718994
Validation loss: 2.6927053415647118

Epoch: 34| Step: 0
Training loss: 3.8279333114624023
Validation loss: 2.6922877475779545

Epoch: 6| Step: 1
Training loss: 3.0069961547851562
Validation loss: 2.6905495761543192

Epoch: 6| Step: 2
Training loss: 2.1102418899536133
Validation loss: 2.6875574204229538

Epoch: 6| Step: 3
Training loss: 2.367854595184326
Validation loss: 2.689577853807839

Epoch: 6| Step: 4
Training loss: 3.3809609413146973
Validation loss: 2.6920121049368255

Epoch: 6| Step: 5
Training loss: 2.461317539215088
Validation loss: 2.6893599879357124

Epoch: 6| Step: 6
Training loss: 2.6208620071411133
Validation loss: 2.688271507140129

Epoch: 6| Step: 7
Training loss: 3.2030985355377197
Validation loss: 2.6850555507085656

Epoch: 6| Step: 8
Training loss: 2.075188159942627
Validation loss: 2.6835663549361692

Epoch: 6| Step: 9
Training loss: 2.3697328567504883
Validation loss: 2.685577351559875

Epoch: 6| Step: 10
Training loss: 3.5187275409698486
Validation loss: 2.6894639666362474

Epoch: 6| Step: 11
Training loss: 3.0068631172180176
Validation loss: 2.6853945332188762

Epoch: 6| Step: 12
Training loss: 3.213284969329834
Validation loss: 2.6817021267388457

Epoch: 6| Step: 13
Training loss: 2.9017796516418457
Validation loss: 2.683765031958139

Epoch: 35| Step: 0
Training loss: 3.3296074867248535
Validation loss: 2.6815832584134993

Epoch: 6| Step: 1
Training loss: 3.121212959289551
Validation loss: 2.6838976003790416

Epoch: 6| Step: 2
Training loss: 2.5195953845977783
Validation loss: 2.6835529676047702

Epoch: 6| Step: 3
Training loss: 2.860024929046631
Validation loss: 2.6793107627540507

Epoch: 6| Step: 4
Training loss: 2.4539313316345215
Validation loss: 2.6801992154890493

Epoch: 6| Step: 5
Training loss: 2.6637110710144043
Validation loss: 2.6741044188058503

Epoch: 6| Step: 6
Training loss: 2.4034955501556396
Validation loss: 2.6733892425414054

Epoch: 6| Step: 7
Training loss: 3.4642486572265625
Validation loss: 2.6761730255619174

Epoch: 6| Step: 8
Training loss: 3.2184038162231445
Validation loss: 2.673319306424869

Epoch: 6| Step: 9
Training loss: 3.07206392288208
Validation loss: 2.6703351620704896

Epoch: 6| Step: 10
Training loss: 2.3373911380767822
Validation loss: 2.669837495332123

Epoch: 6| Step: 11
Training loss: 2.7115869522094727
Validation loss: 2.6744452112464496

Epoch: 6| Step: 12
Training loss: 2.938286542892456
Validation loss: 2.667807312421901

Epoch: 6| Step: 13
Training loss: 2.7795863151550293
Validation loss: 2.6643216404863583

Epoch: 36| Step: 0
Training loss: 2.8949623107910156
Validation loss: 2.6680149621860956

Epoch: 6| Step: 1
Training loss: 2.6196341514587402
Validation loss: 2.6587324296274493

Epoch: 6| Step: 2
Training loss: 2.822150230407715
Validation loss: 2.661066691080729

Epoch: 6| Step: 3
Training loss: 3.127089023590088
Validation loss: 2.670363623608825

Epoch: 6| Step: 4
Training loss: 2.1742825508117676
Validation loss: 2.693241524439986

Epoch: 6| Step: 5
Training loss: 3.697720527648926
Validation loss: 2.698176622390747

Epoch: 6| Step: 6
Training loss: 2.5474042892456055
Validation loss: 2.6913141537738103

Epoch: 6| Step: 7
Training loss: 3.29665470123291
Validation loss: 2.6844458759471936

Epoch: 6| Step: 8
Training loss: 2.8029611110687256
Validation loss: 2.6693077138675156

Epoch: 6| Step: 9
Training loss: 2.81304669380188
Validation loss: 2.6629360952684955

Epoch: 6| Step: 10
Training loss: 2.259458541870117
Validation loss: 2.6532740669865764

Epoch: 6| Step: 11
Training loss: 3.0237479209899902
Validation loss: 2.655451564378636

Epoch: 6| Step: 12
Training loss: 3.20158052444458
Validation loss: 2.6638993037644254

Epoch: 6| Step: 13
Training loss: 2.3145570755004883
Validation loss: 2.6525573986832813

Epoch: 37| Step: 0
Training loss: 2.513582706451416
Validation loss: 2.6469611660126717

Epoch: 6| Step: 1
Training loss: 3.243875503540039
Validation loss: 2.650164327313823

Epoch: 6| Step: 2
Training loss: 3.6655538082122803
Validation loss: 2.6478020196319907

Epoch: 6| Step: 3
Training loss: 2.4953393936157227
Validation loss: 2.643255013291554

Epoch: 6| Step: 4
Training loss: 3.06644868850708
Validation loss: 2.642875345804358

Epoch: 6| Step: 5
Training loss: 2.5783774852752686
Validation loss: 2.6475417331982682

Epoch: 6| Step: 6
Training loss: 2.9965622425079346
Validation loss: 2.640265549382856

Epoch: 6| Step: 7
Training loss: 2.687265396118164
Validation loss: 2.6408826663929927

Epoch: 6| Step: 8
Training loss: 2.678394317626953
Validation loss: 2.6458755718764437

Epoch: 6| Step: 9
Training loss: 2.794735908508301
Validation loss: 2.646166996289325

Epoch: 6| Step: 10
Training loss: 2.6636996269226074
Validation loss: 2.6427528781275593

Epoch: 6| Step: 11
Training loss: 3.003401279449463
Validation loss: 2.6530832347049507

Epoch: 6| Step: 12
Training loss: 2.2469680309295654
Validation loss: 2.6555419737292874

Epoch: 6| Step: 13
Training loss: 3.18062686920166
Validation loss: 2.666195700245519

Epoch: 38| Step: 0
Training loss: 2.939295768737793
Validation loss: 2.680060161057339

Epoch: 6| Step: 1
Training loss: 2.736204147338867
Validation loss: 2.6852804973561275

Epoch: 6| Step: 2
Training loss: 2.8351032733917236
Validation loss: 2.654807298414169

Epoch: 6| Step: 3
Training loss: 2.7223336696624756
Validation loss: 2.656702838918214

Epoch: 6| Step: 4
Training loss: 2.6767592430114746
Validation loss: 2.6500393421419206

Epoch: 6| Step: 5
Training loss: 3.1513619422912598
Validation loss: 2.640927827486428

Epoch: 6| Step: 6
Training loss: 2.5130155086517334
Validation loss: 2.6379702603945168

Epoch: 6| Step: 7
Training loss: 3.0518898963928223
Validation loss: 2.6376103278129333

Epoch: 6| Step: 8
Training loss: 2.176133632659912
Validation loss: 2.633522682292487

Epoch: 6| Step: 9
Training loss: 3.2111897468566895
Validation loss: 2.6331911727946293

Epoch: 6| Step: 10
Training loss: 3.0544424057006836
Validation loss: 2.6354484660651094

Epoch: 6| Step: 11
Training loss: 3.2734932899475098
Validation loss: 2.636319791117022

Epoch: 6| Step: 12
Training loss: 2.695432424545288
Validation loss: 2.638612944592712

Epoch: 6| Step: 13
Training loss: 2.365818500518799
Validation loss: 2.637368658537506

Epoch: 39| Step: 0
Training loss: 2.4070611000061035
Validation loss: 2.632477573169175

Epoch: 6| Step: 1
Training loss: 3.201456069946289
Validation loss: 2.6298998043101323

Epoch: 6| Step: 2
Training loss: 2.651672840118408
Validation loss: 2.6285667560433827

Epoch: 6| Step: 3
Training loss: 2.5529844760894775
Validation loss: 2.632218027627596

Epoch: 6| Step: 4
Training loss: 2.355800151824951
Validation loss: 2.633607115796817

Epoch: 6| Step: 5
Training loss: 1.5632600784301758
Validation loss: 2.632608413696289

Epoch: 6| Step: 6
Training loss: 2.1631343364715576
Validation loss: 2.638564427693685

Epoch: 6| Step: 7
Training loss: 3.8477771282196045
Validation loss: 2.63746823546707

Epoch: 6| Step: 8
Training loss: 2.597837448120117
Validation loss: 2.6425744718120945

Epoch: 6| Step: 9
Training loss: 3.852086305618286
Validation loss: 2.6430248624535015

Epoch: 6| Step: 10
Training loss: 3.0331122875213623
Validation loss: 2.6516324012510237

Epoch: 6| Step: 11
Training loss: 3.9841904640197754
Validation loss: 2.6347053127904094

Epoch: 6| Step: 12
Training loss: 2.9641242027282715
Validation loss: 2.630180722923689

Epoch: 6| Step: 13
Training loss: 1.945737361907959
Validation loss: 2.628523208761728

Epoch: 40| Step: 0
Training loss: 3.2823033332824707
Validation loss: 2.6276769689334336

Epoch: 6| Step: 1
Training loss: 1.7047038078308105
Validation loss: 2.6314636814978813

Epoch: 6| Step: 2
Training loss: 2.900186538696289
Validation loss: 2.627880747600268

Epoch: 6| Step: 3
Training loss: 3.2778613567352295
Validation loss: 2.628477704140448

Epoch: 6| Step: 4
Training loss: 2.210906982421875
Validation loss: 2.62575618169641

Epoch: 6| Step: 5
Training loss: 2.7181148529052734
Validation loss: 2.6224966638831684

Epoch: 6| Step: 6
Training loss: 3.3135499954223633
Validation loss: 2.6280772583459013

Epoch: 6| Step: 7
Training loss: 2.7781410217285156
Validation loss: 2.6253189732951503

Epoch: 6| Step: 8
Training loss: 3.8878016471862793
Validation loss: 2.632669336052351

Epoch: 6| Step: 9
Training loss: 3.0063300132751465
Validation loss: 2.629121406103975

Epoch: 6| Step: 10
Training loss: 2.390352249145508
Validation loss: 2.624978311600224

Epoch: 6| Step: 11
Training loss: 2.171215057373047
Validation loss: 2.6237406935743106

Epoch: 6| Step: 12
Training loss: 2.8270554542541504
Validation loss: 2.617484343949185

Epoch: 6| Step: 13
Training loss: 3.0714826583862305
Validation loss: 2.621651459765691

Epoch: 41| Step: 0
Training loss: 3.406348705291748
Validation loss: 2.6201188359209286

Epoch: 6| Step: 1
Training loss: 2.5839076042175293
Validation loss: 2.6209296795629684

Epoch: 6| Step: 2
Training loss: 2.718393564224243
Validation loss: 2.618633770173596

Epoch: 6| Step: 3
Training loss: 3.7363791465759277
Validation loss: 2.6172390855768675

Epoch: 6| Step: 4
Training loss: 2.8410377502441406
Validation loss: 2.6163188129343014

Epoch: 6| Step: 5
Training loss: 2.7986230850219727
Validation loss: 2.6169927299663587

Epoch: 6| Step: 6
Training loss: 3.1201095581054688
Validation loss: 2.6164996675265733

Epoch: 6| Step: 7
Training loss: 1.6218211650848389
Validation loss: 2.612883867756013

Epoch: 6| Step: 8
Training loss: 2.6344685554504395
Validation loss: 2.6130160003580074

Epoch: 6| Step: 9
Training loss: 2.5314176082611084
Validation loss: 2.6144281228383384

Epoch: 6| Step: 10
Training loss: 3.1375346183776855
Validation loss: 2.6110564893291843

Epoch: 6| Step: 11
Training loss: 2.7237772941589355
Validation loss: 2.6123157470457015

Epoch: 6| Step: 12
Training loss: 2.551816701889038
Validation loss: 2.6138552440110074

Epoch: 6| Step: 13
Training loss: 3.0237176418304443
Validation loss: 2.618515819631597

Epoch: 42| Step: 0
Training loss: 2.2493247985839844
Validation loss: 2.6226808999174382

Epoch: 6| Step: 1
Training loss: 2.655834197998047
Validation loss: 2.618753730609853

Epoch: 6| Step: 2
Training loss: 2.862248659133911
Validation loss: 2.6197119784611527

Epoch: 6| Step: 3
Training loss: 3.686750888824463
Validation loss: 2.62095187299995

Epoch: 6| Step: 4
Training loss: 1.8061749935150146
Validation loss: 2.614855771423668

Epoch: 6| Step: 5
Training loss: 3.0431952476501465
Validation loss: 2.613022986278739

Epoch: 6| Step: 6
Training loss: 2.2763171195983887
Validation loss: 2.6109073110806045

Epoch: 6| Step: 7
Training loss: 2.969601631164551
Validation loss: 2.6118393303245626

Epoch: 6| Step: 8
Training loss: 2.74560546875
Validation loss: 2.613812300466722

Epoch: 6| Step: 9
Training loss: 2.612793445587158
Validation loss: 2.6120071488042034

Epoch: 6| Step: 10
Training loss: 3.7572684288024902
Validation loss: 2.612916243973599

Epoch: 6| Step: 11
Training loss: 2.4035024642944336
Validation loss: 2.612533043789607

Epoch: 6| Step: 12
Training loss: 3.440457820892334
Validation loss: 2.6144745785702943

Epoch: 6| Step: 13
Training loss: 2.7861499786376953
Validation loss: 2.6107185579115346

Epoch: 43| Step: 0
Training loss: 2.9544591903686523
Validation loss: 2.612287529053227

Epoch: 6| Step: 1
Training loss: 3.043351173400879
Validation loss: 2.6242345533063336

Epoch: 6| Step: 2
Training loss: 2.970980405807495
Validation loss: 2.6255497394069547

Epoch: 6| Step: 3
Training loss: 2.933505058288574
Validation loss: 2.6155100304593324

Epoch: 6| Step: 4
Training loss: 2.569614887237549
Validation loss: 2.6094531500211327

Epoch: 6| Step: 5
Training loss: 2.229574680328369
Validation loss: 2.6115295553720124

Epoch: 6| Step: 6
Training loss: 2.7804746627807617
Validation loss: 2.614095334083803

Epoch: 6| Step: 7
Training loss: 2.4993395805358887
Validation loss: 2.6144379902911443

Epoch: 6| Step: 8
Training loss: 2.8683042526245117
Validation loss: 2.609943425783547

Epoch: 6| Step: 9
Training loss: 3.0748114585876465
Validation loss: 2.6106288048528854

Epoch: 6| Step: 10
Training loss: 2.2950656414031982
Validation loss: 2.611292444249635

Epoch: 6| Step: 11
Training loss: 2.989558696746826
Validation loss: 2.613617668869675

Epoch: 6| Step: 12
Training loss: 3.529665946960449
Validation loss: 2.6088455851360033

Epoch: 6| Step: 13
Training loss: 2.3175501823425293
Validation loss: 2.609421153222361

Epoch: 44| Step: 0
Training loss: 2.5987954139709473
Validation loss: 2.6115584194019275

Epoch: 6| Step: 1
Training loss: 3.4265198707580566
Validation loss: 2.6099908608262257

Epoch: 6| Step: 2
Training loss: 2.4268174171447754
Validation loss: 2.606273886977985

Epoch: 6| Step: 3
Training loss: 2.2603328227996826
Validation loss: 2.6076865298773653

Epoch: 6| Step: 4
Training loss: 2.9329800605773926
Validation loss: 2.60751744239561

Epoch: 6| Step: 5
Training loss: 2.8595712184906006
Validation loss: 2.6030587996205976

Epoch: 6| Step: 6
Training loss: 2.7930548191070557
Validation loss: 2.606182831589894

Epoch: 6| Step: 7
Training loss: 2.7644710540771484
Validation loss: 2.6047628387328117

Epoch: 6| Step: 8
Training loss: 2.8728928565979004
Validation loss: 2.6023965984262447

Epoch: 6| Step: 9
Training loss: 3.4734559059143066
Validation loss: 2.6027493912686586

Epoch: 6| Step: 10
Training loss: 2.8017454147338867
Validation loss: 2.6050198078155518

Epoch: 6| Step: 11
Training loss: 3.0851998329162598
Validation loss: 2.6020314001267955

Epoch: 6| Step: 12
Training loss: 2.422025203704834
Validation loss: 2.6027608276695333

Epoch: 6| Step: 13
Training loss: 2.273763418197632
Validation loss: 2.605678343003796

Epoch: 45| Step: 0
Training loss: 3.1821603775024414
Validation loss: 2.600130832323464

Epoch: 6| Step: 1
Training loss: 2.453887462615967
Validation loss: 2.605445420870217

Epoch: 6| Step: 2
Training loss: 3.540116548538208
Validation loss: 2.604175829118298

Epoch: 6| Step: 3
Training loss: 3.220601797103882
Validation loss: 2.601016629126764

Epoch: 6| Step: 4
Training loss: 2.475766658782959
Validation loss: 2.6004539740982877

Epoch: 6| Step: 5
Training loss: 2.654979705810547
Validation loss: 2.60053692325469

Epoch: 6| Step: 6
Training loss: 3.2908518314361572
Validation loss: 2.6004113945909726

Epoch: 6| Step: 7
Training loss: 2.4968414306640625
Validation loss: 2.603449736872027

Epoch: 6| Step: 8
Training loss: 3.177145481109619
Validation loss: 2.6010003525723695

Epoch: 6| Step: 9
Training loss: 2.6913251876831055
Validation loss: 2.6020377092463995

Epoch: 6| Step: 10
Training loss: 2.0652425289154053
Validation loss: 2.602712436388898

Epoch: 6| Step: 11
Training loss: 1.9993665218353271
Validation loss: 2.609798187850624

Epoch: 6| Step: 12
Training loss: 2.692986488342285
Validation loss: 2.6034201011862805

Epoch: 6| Step: 13
Training loss: 3.533552408218384
Validation loss: 2.599798317878477

Epoch: 46| Step: 0
Training loss: 2.5496110916137695
Validation loss: 2.596040956435665

Epoch: 6| Step: 1
Training loss: 2.9695210456848145
Validation loss: 2.597049346534155

Epoch: 6| Step: 2
Training loss: 2.8894660472869873
Validation loss: 2.596033401386712

Epoch: 6| Step: 3
Training loss: 3.05104923248291
Validation loss: 2.599781092777047

Epoch: 6| Step: 4
Training loss: 2.861185073852539
Validation loss: 2.5980223019917807

Epoch: 6| Step: 5
Training loss: 2.7159533500671387
Validation loss: 2.6004633852230605

Epoch: 6| Step: 6
Training loss: 2.473339319229126
Validation loss: 2.5991507371266684

Epoch: 6| Step: 7
Training loss: 2.8296563625335693
Validation loss: 2.597964591877435

Epoch: 6| Step: 8
Training loss: 2.9424142837524414
Validation loss: 2.6018542089769916

Epoch: 6| Step: 9
Training loss: 2.115586996078491
Validation loss: 2.596641045744701

Epoch: 6| Step: 10
Training loss: 2.435210943222046
Validation loss: 2.5964385309526996

Epoch: 6| Step: 11
Training loss: 2.6078639030456543
Validation loss: 2.5957066243694675

Epoch: 6| Step: 12
Training loss: 3.837858200073242
Validation loss: 2.5984116933679067

Epoch: 6| Step: 13
Training loss: 2.937544822692871
Validation loss: 2.599986327591763

Epoch: 47| Step: 0
Training loss: 3.1243820190429688
Validation loss: 2.5920142307076404

Epoch: 6| Step: 1
Training loss: 2.5739693641662598
Validation loss: 2.595449347649851

Epoch: 6| Step: 2
Training loss: 2.5455474853515625
Validation loss: 2.595164898903139

Epoch: 6| Step: 3
Training loss: 2.914348840713501
Validation loss: 2.594902161628969

Epoch: 6| Step: 4
Training loss: 2.7972302436828613
Validation loss: 2.5966701379386325

Epoch: 6| Step: 5
Training loss: 2.793023109436035
Validation loss: 2.5953611609756306

Epoch: 6| Step: 6
Training loss: 1.6162502765655518
Validation loss: 2.5960118847508586

Epoch: 6| Step: 7
Training loss: 2.7964470386505127
Validation loss: 2.5926209393367974

Epoch: 6| Step: 8
Training loss: 2.2107439041137695
Validation loss: 2.594115875100577

Epoch: 6| Step: 9
Training loss: 3.12788462638855
Validation loss: 2.592615153199883

Epoch: 6| Step: 10
Training loss: 2.9605956077575684
Validation loss: 2.5962307888974427

Epoch: 6| Step: 11
Training loss: 3.3803908824920654
Validation loss: 2.596302109379922

Epoch: 6| Step: 12
Training loss: 3.530449390411377
Validation loss: 2.5963963898279334

Epoch: 6| Step: 13
Training loss: 2.6424973011016846
Validation loss: 2.5910397575747584

Epoch: 48| Step: 0
Training loss: 2.3389689922332764
Validation loss: 2.6004508951658845

Epoch: 6| Step: 1
Training loss: 2.2684884071350098
Validation loss: 2.6066384571854786

Epoch: 6| Step: 2
Training loss: 3.0543737411499023
Validation loss: 2.6172181073055474

Epoch: 6| Step: 3
Training loss: 2.6130542755126953
Validation loss: 2.6255322912687897

Epoch: 6| Step: 4
Training loss: 2.820343017578125
Validation loss: 2.6482774314060005

Epoch: 6| Step: 5
Training loss: 3.4581146240234375
Validation loss: 2.6302355233059136

Epoch: 6| Step: 6
Training loss: 2.56105899810791
Validation loss: 2.5950356093786096

Epoch: 6| Step: 7
Training loss: 2.9836347103118896
Validation loss: 2.596728846591006

Epoch: 6| Step: 8
Training loss: 2.8377487659454346
Validation loss: 2.5969817356396745

Epoch: 6| Step: 9
Training loss: 2.0281410217285156
Validation loss: 2.602206309636434

Epoch: 6| Step: 10
Training loss: 2.7153005599975586
Validation loss: 2.597182309755715

Epoch: 6| Step: 11
Training loss: 2.280669689178467
Validation loss: 2.6046502872179915

Epoch: 6| Step: 12
Training loss: 3.619091510772705
Validation loss: 2.616930066898305

Epoch: 6| Step: 13
Training loss: 4.088455677032471
Validation loss: 2.610948529294742

Epoch: 49| Step: 0
Training loss: 2.2682204246520996
Validation loss: 2.6069548309490247

Epoch: 6| Step: 1
Training loss: 2.8861804008483887
Validation loss: 2.6064600431790916

Epoch: 6| Step: 2
Training loss: 1.961165189743042
Validation loss: 2.61570139597821

Epoch: 6| Step: 3
Training loss: 2.530836820602417
Validation loss: 2.6193620850962978

Epoch: 6| Step: 4
Training loss: 2.8941922187805176
Validation loss: 2.6192441524997836

Epoch: 6| Step: 5
Training loss: 3.235846757888794
Validation loss: 2.6178018585328133

Epoch: 6| Step: 6
Training loss: 2.9664876461029053
Validation loss: 2.6092247219495874

Epoch: 6| Step: 7
Training loss: 3.1563804149627686
Validation loss: 2.603185838268649

Epoch: 6| Step: 8
Training loss: 2.3707470893859863
Validation loss: 2.5954544595492783

Epoch: 6| Step: 9
Training loss: 3.3019423484802246
Validation loss: 2.6033856458561395

Epoch: 6| Step: 10
Training loss: 3.1348001956939697
Validation loss: 2.5994368778761996

Epoch: 6| Step: 11
Training loss: 2.9761552810668945
Validation loss: 2.598403822991156

Epoch: 6| Step: 12
Training loss: 2.181122064590454
Validation loss: 2.5932094281719578

Epoch: 6| Step: 13
Training loss: 3.7272469997406006
Validation loss: 2.5923080367426716

Epoch: 50| Step: 0
Training loss: 2.885124444961548
Validation loss: 2.5927775854705484

Epoch: 6| Step: 1
Training loss: 2.9791297912597656
Validation loss: 2.588777272931991

Epoch: 6| Step: 2
Training loss: 2.5005853176116943
Validation loss: 2.5911682831343783

Epoch: 6| Step: 3
Training loss: 3.4026060104370117
Validation loss: 2.5899594932474117

Epoch: 6| Step: 4
Training loss: 3.0325212478637695
Validation loss: 2.5928246359671316

Epoch: 6| Step: 5
Training loss: 2.706137180328369
Validation loss: 2.5910500531555503

Epoch: 6| Step: 6
Training loss: 3.0419464111328125
Validation loss: 2.5916634067412345

Epoch: 6| Step: 7
Training loss: 1.707777976989746
Validation loss: 2.593166212881765

Epoch: 6| Step: 8
Training loss: 2.3010451793670654
Validation loss: 2.5882987104436403

Epoch: 6| Step: 9
Training loss: 2.388582229614258
Validation loss: 2.592997699655512

Epoch: 6| Step: 10
Training loss: 2.8517050743103027
Validation loss: 2.5861804921139955

Epoch: 6| Step: 11
Training loss: 3.8136301040649414
Validation loss: 2.5899534738191994

Epoch: 6| Step: 12
Training loss: 3.1508424282073975
Validation loss: 2.590610163186186

Epoch: 6| Step: 13
Training loss: 1.868032693862915
Validation loss: 2.5868249285605645

Epoch: 51| Step: 0
Training loss: 3.4926657676696777
Validation loss: 2.585725522810413

Epoch: 6| Step: 1
Training loss: 2.6067092418670654
Validation loss: 2.5838845904155443

Epoch: 6| Step: 2
Training loss: 3.051150321960449
Validation loss: 2.5884411411900676

Epoch: 6| Step: 3
Training loss: 3.353754997253418
Validation loss: 2.5854247257273686

Epoch: 6| Step: 4
Training loss: 2.194366455078125
Validation loss: 2.5807281130103656

Epoch: 6| Step: 5
Training loss: 2.123444080352783
Validation loss: 2.584986622615527

Epoch: 6| Step: 6
Training loss: 3.1931333541870117
Validation loss: 2.5857688996099655

Epoch: 6| Step: 7
Training loss: 2.538109540939331
Validation loss: 2.5883036582700667

Epoch: 6| Step: 8
Training loss: 2.4428176879882812
Validation loss: 2.5859053750191965

Epoch: 6| Step: 9
Training loss: 2.410310983657837
Validation loss: 2.587626882778701

Epoch: 6| Step: 10
Training loss: 2.5136189460754395
Validation loss: 2.585894610292168

Epoch: 6| Step: 11
Training loss: 3.663574457168579
Validation loss: 2.5851172580513904

Epoch: 6| Step: 12
Training loss: 2.3538620471954346
Validation loss: 2.5851281381422475

Epoch: 6| Step: 13
Training loss: 3.216830253601074
Validation loss: 2.5805009462500132

Epoch: 52| Step: 0
Training loss: 3.6358871459960938
Validation loss: 2.582758693284886

Epoch: 6| Step: 1
Training loss: 2.5647644996643066
Validation loss: 2.580375768805063

Epoch: 6| Step: 2
Training loss: 2.7580771446228027
Validation loss: 2.5799444798500306

Epoch: 6| Step: 3
Training loss: 2.202965259552002
Validation loss: 2.5788737855931765

Epoch: 6| Step: 4
Training loss: 3.0988643169403076
Validation loss: 2.580554444302795

Epoch: 6| Step: 5
Training loss: 3.0560967922210693
Validation loss: 2.5806331198702575

Epoch: 6| Step: 6
Training loss: 2.606107473373413
Validation loss: 2.5793358177267094

Epoch: 6| Step: 7
Training loss: 2.8023529052734375
Validation loss: 2.5788694684223463

Epoch: 6| Step: 8
Training loss: 2.5564162731170654
Validation loss: 2.5816373876346055

Epoch: 6| Step: 9
Training loss: 2.3940207958221436
Validation loss: 2.5757793277822514

Epoch: 6| Step: 10
Training loss: 3.129723310470581
Validation loss: 2.5779814412516933

Epoch: 6| Step: 11
Training loss: 2.529026508331299
Validation loss: 2.5808377060838925

Epoch: 6| Step: 12
Training loss: 2.6348280906677246
Validation loss: 2.5751861577392905

Epoch: 6| Step: 13
Training loss: 3.1153366565704346
Validation loss: 2.579496222157632

Epoch: 53| Step: 0
Training loss: 2.6683597564697266
Validation loss: 2.576001554407099

Epoch: 6| Step: 1
Training loss: 2.1607120037078857
Validation loss: 2.5784756163115143

Epoch: 6| Step: 2
Training loss: 2.8435492515563965
Validation loss: 2.577716335173576

Epoch: 6| Step: 3
Training loss: 3.0292675495147705
Validation loss: 2.581363644651187

Epoch: 6| Step: 4
Training loss: 3.3155970573425293
Validation loss: 2.576597054799398

Epoch: 6| Step: 5
Training loss: 2.3391077518463135
Validation loss: 2.574752192343435

Epoch: 6| Step: 6
Training loss: 2.1481735706329346
Validation loss: 2.5750374819642756

Epoch: 6| Step: 7
Training loss: 2.8871750831604004
Validation loss: 2.5745057136781755

Epoch: 6| Step: 8
Training loss: 2.9533843994140625
Validation loss: 2.573646968410861

Epoch: 6| Step: 9
Training loss: 3.2234435081481934
Validation loss: 2.5738583995449926

Epoch: 6| Step: 10
Training loss: 2.8078367710113525
Validation loss: 2.5768867513184905

Epoch: 6| Step: 11
Training loss: 2.873974561691284
Validation loss: 2.572657374925511

Epoch: 6| Step: 12
Training loss: 2.9191489219665527
Validation loss: 2.5784489518852642

Epoch: 6| Step: 13
Training loss: 2.6775801181793213
Validation loss: 2.583561758841238

Epoch: 54| Step: 0
Training loss: 2.829909086227417
Validation loss: 2.585358517144316

Epoch: 6| Step: 1
Training loss: 2.6081430912017822
Validation loss: 2.5787724269333707

Epoch: 6| Step: 2
Training loss: 2.3672025203704834
Validation loss: 2.577754651346514

Epoch: 6| Step: 3
Training loss: 2.6987011432647705
Validation loss: 2.580080229748962

Epoch: 6| Step: 4
Training loss: 2.2647337913513184
Validation loss: 2.5763648633033998

Epoch: 6| Step: 5
Training loss: 2.248385429382324
Validation loss: 2.5801734360315467

Epoch: 6| Step: 6
Training loss: 2.7884273529052734
Validation loss: 2.580904237685665

Epoch: 6| Step: 7
Training loss: 3.851224184036255
Validation loss: 2.5764817473708943

Epoch: 6| Step: 8
Training loss: 2.559948444366455
Validation loss: 2.5756587905268513

Epoch: 6| Step: 9
Training loss: 3.876432418823242
Validation loss: 2.5770222627988426

Epoch: 6| Step: 10
Training loss: 2.210676431655884
Validation loss: 2.577993926181588

Epoch: 6| Step: 11
Training loss: 2.7460334300994873
Validation loss: 2.5771739149606354

Epoch: 6| Step: 12
Training loss: 2.7746944427490234
Validation loss: 2.580059333514142

Epoch: 6| Step: 13
Training loss: 3.2725579738616943
Validation loss: 2.586173185738184

Epoch: 55| Step: 0
Training loss: 3.9666123390197754
Validation loss: 2.5860823123685774

Epoch: 6| Step: 1
Training loss: 3.0459861755371094
Validation loss: 2.5825380407353884

Epoch: 6| Step: 2
Training loss: 2.4187707901000977
Validation loss: 2.5737873713175454

Epoch: 6| Step: 3
Training loss: 2.615344762802124
Validation loss: 2.571282563670989

Epoch: 6| Step: 4
Training loss: 2.014479160308838
Validation loss: 2.582459875332412

Epoch: 6| Step: 5
Training loss: 2.9006423950195312
Validation loss: 2.5998463348675798

Epoch: 6| Step: 6
Training loss: 2.7552194595336914
Validation loss: 2.611262382999543

Epoch: 6| Step: 7
Training loss: 2.3326663970947266
Validation loss: 2.6198659891723306

Epoch: 6| Step: 8
Training loss: 1.7619754076004028
Validation loss: 2.621249468095841

Epoch: 6| Step: 9
Training loss: 4.008246421813965
Validation loss: 2.6203965474200506

Epoch: 6| Step: 10
Training loss: 3.1052427291870117
Validation loss: 2.614884853363037

Epoch: 6| Step: 11
Training loss: 2.662658452987671
Validation loss: 2.6181851330623833

Epoch: 6| Step: 12
Training loss: 3.3043160438537598
Validation loss: 2.6068264463896393

Epoch: 6| Step: 13
Training loss: 1.657726526260376
Validation loss: 2.6033888375887306

Epoch: 56| Step: 0
Training loss: 2.8844571113586426
Validation loss: 2.590949832752187

Epoch: 6| Step: 1
Training loss: 3.1249194145202637
Validation loss: 2.587161740949077

Epoch: 6| Step: 2
Training loss: 1.8344836235046387
Validation loss: 2.5711937207047657

Epoch: 6| Step: 3
Training loss: 3.029531955718994
Validation loss: 2.574083266719695

Epoch: 6| Step: 4
Training loss: 2.4666965007781982
Validation loss: 2.573723793029785

Epoch: 6| Step: 5
Training loss: 2.5582315921783447
Validation loss: 2.5785735191837436

Epoch: 6| Step: 6
Training loss: 2.9971814155578613
Validation loss: 2.584584733491303

Epoch: 6| Step: 7
Training loss: 2.9864134788513184
Validation loss: 2.5808143205540155

Epoch: 6| Step: 8
Training loss: 2.9090099334716797
Validation loss: 2.5816183987484185

Epoch: 6| Step: 9
Training loss: 2.735398292541504
Validation loss: 2.571343027135377

Epoch: 6| Step: 10
Training loss: 2.915097236633301
Validation loss: 2.5682468311761015

Epoch: 6| Step: 11
Training loss: 2.171440601348877
Validation loss: 2.5670881296998713

Epoch: 6| Step: 12
Training loss: 3.465865135192871
Validation loss: 2.568887859262446

Epoch: 6| Step: 13
Training loss: 2.8981502056121826
Validation loss: 2.5718085740202214

Epoch: 57| Step: 0
Training loss: 2.861003875732422
Validation loss: 2.575383283758676

Epoch: 6| Step: 1
Training loss: 4.21600866317749
Validation loss: 2.5757043746209916

Epoch: 6| Step: 2
Training loss: 2.2514681816101074
Validation loss: 2.5818675666727047

Epoch: 6| Step: 3
Training loss: 2.4726970195770264
Validation loss: 2.5845685594825336

Epoch: 6| Step: 4
Training loss: 3.1719958782196045
Validation loss: 2.587660163961431

Epoch: 6| Step: 5
Training loss: 2.514810562133789
Validation loss: 2.5813327809815765

Epoch: 6| Step: 6
Training loss: 1.7648898363113403
Validation loss: 2.5793821580948366

Epoch: 6| Step: 7
Training loss: 2.9842467308044434
Validation loss: 2.5730036509934293

Epoch: 6| Step: 8
Training loss: 3.093798875808716
Validation loss: 2.5661536467972623

Epoch: 6| Step: 9
Training loss: 2.7511508464813232
Validation loss: 2.5633621587548205

Epoch: 6| Step: 10
Training loss: 2.5469157695770264
Validation loss: 2.56604835038544

Epoch: 6| Step: 11
Training loss: 2.911853075027466
Validation loss: 2.5634257152516353

Epoch: 6| Step: 12
Training loss: 2.6090121269226074
Validation loss: 2.564015616652786

Epoch: 6| Step: 13
Training loss: 2.4964802265167236
Validation loss: 2.5634920673985637

Epoch: 58| Step: 0
Training loss: 2.8456950187683105
Validation loss: 2.5616942938937934

Epoch: 6| Step: 1
Training loss: 3.8638601303100586
Validation loss: 2.5606013600544264

Epoch: 6| Step: 2
Training loss: 3.0348336696624756
Validation loss: 2.5622746611154206

Epoch: 6| Step: 3
Training loss: 2.540672779083252
Validation loss: 2.5614019286247993

Epoch: 6| Step: 4
Training loss: 2.009925127029419
Validation loss: 2.559560966748063

Epoch: 6| Step: 5
Training loss: 2.8591980934143066
Validation loss: 2.560890641263736

Epoch: 6| Step: 6
Training loss: 2.8372421264648438
Validation loss: 2.5611511866251626

Epoch: 6| Step: 7
Training loss: 3.178609848022461
Validation loss: 2.564310566071541

Epoch: 6| Step: 8
Training loss: 2.2207846641540527
Validation loss: 2.5628231776657926

Epoch: 6| Step: 9
Training loss: 2.607759475708008
Validation loss: 2.564318685121434

Epoch: 6| Step: 10
Training loss: 2.8604378700256348
Validation loss: 2.561728790242185

Epoch: 6| Step: 11
Training loss: 2.323763847351074
Validation loss: 2.5695847106236283

Epoch: 6| Step: 12
Training loss: 2.293900489807129
Validation loss: 2.565705742887271

Epoch: 6| Step: 13
Training loss: 3.6209514141082764
Validation loss: 2.5640431219531643

Epoch: 59| Step: 0
Training loss: 2.980839729309082
Validation loss: 2.561848707096551

Epoch: 6| Step: 1
Training loss: 2.512737989425659
Validation loss: 2.5590075446713354

Epoch: 6| Step: 2
Training loss: 2.9317238330841064
Validation loss: 2.5596589196112847

Epoch: 6| Step: 3
Training loss: 2.534976005554199
Validation loss: 2.557857362172937

Epoch: 6| Step: 4
Training loss: 2.8964672088623047
Validation loss: 2.5613136368413127

Epoch: 6| Step: 5
Training loss: 2.5414743423461914
Validation loss: 2.5560517567460255

Epoch: 6| Step: 6
Training loss: 2.2992217540740967
Validation loss: 2.5555825874369633

Epoch: 6| Step: 7
Training loss: 3.8787786960601807
Validation loss: 2.5600292964648177

Epoch: 6| Step: 8
Training loss: 3.1413795948028564
Validation loss: 2.555893123790782

Epoch: 6| Step: 9
Training loss: 2.7856032848358154
Validation loss: 2.5554484167406635

Epoch: 6| Step: 10
Training loss: 2.0083882808685303
Validation loss: 2.5569772643427693

Epoch: 6| Step: 11
Training loss: 3.171276092529297
Validation loss: 2.5571970401271695

Epoch: 6| Step: 12
Training loss: 1.6907767057418823
Validation loss: 2.5564985916178715

Epoch: 6| Step: 13
Training loss: 3.7238147258758545
Validation loss: 2.5565030190252487

Epoch: 60| Step: 0
Training loss: 3.714850902557373
Validation loss: 2.5628372443619596

Epoch: 6| Step: 1
Training loss: 2.475008487701416
Validation loss: 2.559217570930399

Epoch: 6| Step: 2
Training loss: 2.751178741455078
Validation loss: 2.5714090895909134

Epoch: 6| Step: 3
Training loss: 2.8193607330322266
Validation loss: 2.566526448854836

Epoch: 6| Step: 4
Training loss: 3.0478672981262207
Validation loss: 2.568719889528008

Epoch: 6| Step: 5
Training loss: 2.184828281402588
Validation loss: 2.577469971872145

Epoch: 6| Step: 6
Training loss: 1.7813518047332764
Validation loss: 2.569693673041559

Epoch: 6| Step: 7
Training loss: 1.5966352224349976
Validation loss: 2.574596405029297

Epoch: 6| Step: 8
Training loss: 3.1838581562042236
Validation loss: 2.5705110590945006

Epoch: 6| Step: 9
Training loss: 3.275290012359619
Validation loss: 2.569131084667739

Epoch: 6| Step: 10
Training loss: 2.9616880416870117
Validation loss: 2.564785070316766

Epoch: 6| Step: 11
Training loss: 2.7236461639404297
Validation loss: 2.5629971053010676

Epoch: 6| Step: 12
Training loss: 3.127495765686035
Validation loss: 2.5551560642898723

Epoch: 6| Step: 13
Training loss: 3.1839497089385986
Validation loss: 2.5552706154443885

Epoch: 61| Step: 0
Training loss: 3.8135507106781006
Validation loss: 2.5546000183269544

Epoch: 6| Step: 1
Training loss: 2.396195411682129
Validation loss: 2.5527339007264827

Epoch: 6| Step: 2
Training loss: 2.87064266204834
Validation loss: 2.5569847194097375

Epoch: 6| Step: 3
Training loss: 2.591193199157715
Validation loss: 2.5555597069442912

Epoch: 6| Step: 4
Training loss: 2.3454179763793945
Validation loss: 2.554956838648806

Epoch: 6| Step: 5
Training loss: 2.9922451972961426
Validation loss: 2.563360833352612

Epoch: 6| Step: 6
Training loss: 3.2201027870178223
Validation loss: 2.5654022898725284

Epoch: 6| Step: 7
Training loss: 2.6607329845428467
Validation loss: 2.5613922893360095

Epoch: 6| Step: 8
Training loss: 3.3882017135620117
Validation loss: 2.555040592788368

Epoch: 6| Step: 9
Training loss: 2.805598258972168
Validation loss: 2.5536286523265224

Epoch: 6| Step: 10
Training loss: 2.152470350265503
Validation loss: 2.551102287025862

Epoch: 6| Step: 11
Training loss: 2.662090301513672
Validation loss: 2.5524213493511243

Epoch: 6| Step: 12
Training loss: 2.1500442028045654
Validation loss: 2.5531895288857083

Epoch: 6| Step: 13
Training loss: 2.3988828659057617
Validation loss: 2.558051093932121

Epoch: 62| Step: 0
Training loss: 2.745664119720459
Validation loss: 2.5520581737641366

Epoch: 6| Step: 1
Training loss: 2.3212504386901855
Validation loss: 2.560967955538022

Epoch: 6| Step: 2
Training loss: 2.6946277618408203
Validation loss: 2.554823593426776

Epoch: 6| Step: 3
Training loss: 2.495654821395874
Validation loss: 2.5525233643029326

Epoch: 6| Step: 4
Training loss: 2.717622756958008
Validation loss: 2.5499987986779984

Epoch: 6| Step: 5
Training loss: 2.836789608001709
Validation loss: 2.548758586247762

Epoch: 6| Step: 6
Training loss: 3.11138916015625
Validation loss: 2.551987337809737

Epoch: 6| Step: 7
Training loss: 2.7693300247192383
Validation loss: 2.5472939501526537

Epoch: 6| Step: 8
Training loss: 2.518566608428955
Validation loss: 2.5439765414884015

Epoch: 6| Step: 9
Training loss: 3.1586050987243652
Validation loss: 2.545720508021693

Epoch: 6| Step: 10
Training loss: 2.8369898796081543
Validation loss: 2.5512962905309533

Epoch: 6| Step: 11
Training loss: 2.7456154823303223
Validation loss: 2.5453355850711947

Epoch: 6| Step: 12
Training loss: 2.9232308864593506
Validation loss: 2.544823500417894

Epoch: 6| Step: 13
Training loss: 2.6636624336242676
Validation loss: 2.546226055391373

Epoch: 63| Step: 0
Training loss: 2.9918079376220703
Validation loss: 2.5423568525621967

Epoch: 6| Step: 1
Training loss: 1.8609395027160645
Validation loss: 2.543338050124466

Epoch: 6| Step: 2
Training loss: 3.396878957748413
Validation loss: 2.542614531773393

Epoch: 6| Step: 3
Training loss: 2.54489803314209
Validation loss: 2.5463900822465138

Epoch: 6| Step: 4
Training loss: 2.511965751647949
Validation loss: 2.542954155193862

Epoch: 6| Step: 5
Training loss: 3.068955421447754
Validation loss: 2.5432173180323776

Epoch: 6| Step: 6
Training loss: 2.640883684158325
Validation loss: 2.544612856321437

Epoch: 6| Step: 7
Training loss: 2.7237067222595215
Validation loss: 2.5417388023868686

Epoch: 6| Step: 8
Training loss: 2.476801872253418
Validation loss: 2.5440953675136773

Epoch: 6| Step: 9
Training loss: 2.082596778869629
Validation loss: 2.5431195202694146

Epoch: 6| Step: 10
Training loss: 3.395538806915283
Validation loss: 2.5462981244569183

Epoch: 6| Step: 11
Training loss: 2.963345766067505
Validation loss: 2.5491407994301087

Epoch: 6| Step: 12
Training loss: 3.0263500213623047
Validation loss: 2.5532295011704966

Epoch: 6| Step: 13
Training loss: 2.8356523513793945
Validation loss: 2.554946437958748

Epoch: 64| Step: 0
Training loss: 3.394531726837158
Validation loss: 2.5584329430774977

Epoch: 6| Step: 1
Training loss: 1.8965568542480469
Validation loss: 2.5624461917467016

Epoch: 6| Step: 2
Training loss: 3.2245311737060547
Validation loss: 2.5596652389854513

Epoch: 6| Step: 3
Training loss: 1.9844365119934082
Validation loss: 2.553863581790719

Epoch: 6| Step: 4
Training loss: 2.372833013534546
Validation loss: 2.552454730515839

Epoch: 6| Step: 5
Training loss: 2.342968463897705
Validation loss: 2.542862622968612

Epoch: 6| Step: 6
Training loss: 2.612255573272705
Validation loss: 2.542362577171736

Epoch: 6| Step: 7
Training loss: 2.0704455375671387
Validation loss: 2.540694723847092

Epoch: 6| Step: 8
Training loss: 2.829829454421997
Validation loss: 2.5419852067065496

Epoch: 6| Step: 9
Training loss: 3.4681882858276367
Validation loss: 2.542155723417959

Epoch: 6| Step: 10
Training loss: 3.3373279571533203
Validation loss: 2.542260028982675

Epoch: 6| Step: 11
Training loss: 3.176945209503174
Validation loss: 2.541952117796867

Epoch: 6| Step: 12
Training loss: 2.787848949432373
Validation loss: 2.5442799906576834

Epoch: 6| Step: 13
Training loss: 3.194477081298828
Validation loss: 2.543912490208944

Epoch: 65| Step: 0
Training loss: 2.6772003173828125
Validation loss: 2.5434525064242783

Epoch: 6| Step: 1
Training loss: 2.6166152954101562
Validation loss: 2.53700674990172

Epoch: 6| Step: 2
Training loss: 2.276341676712036
Validation loss: 2.5380584552723873

Epoch: 6| Step: 3
Training loss: 2.2673473358154297
Validation loss: 2.538174893266411

Epoch: 6| Step: 4
Training loss: 3.100311279296875
Validation loss: 2.536770023325438

Epoch: 6| Step: 5
Training loss: 3.4886012077331543
Validation loss: 2.5364001720182356

Epoch: 6| Step: 6
Training loss: 3.434847831726074
Validation loss: 2.536273620461905

Epoch: 6| Step: 7
Training loss: 2.760955810546875
Validation loss: 2.5388130167479157

Epoch: 6| Step: 8
Training loss: 2.5399563312530518
Validation loss: 2.5398200506805093

Epoch: 6| Step: 9
Training loss: 2.8596203327178955
Validation loss: 2.547689204574913

Epoch: 6| Step: 10
Training loss: 2.989030599594116
Validation loss: 2.5632076545428206

Epoch: 6| Step: 11
Training loss: 2.1429286003112793
Validation loss: 2.5799919918019283

Epoch: 6| Step: 12
Training loss: 2.8675832748413086
Validation loss: 2.589754404560212

Epoch: 6| Step: 13
Training loss: 2.245467185974121
Validation loss: 2.5775311300831456

Epoch: 66| Step: 0
Training loss: 2.9248530864715576
Validation loss: 2.571029970722814

Epoch: 6| Step: 1
Training loss: 1.877718210220337
Validation loss: 2.552948280047345

Epoch: 6| Step: 2
Training loss: 3.035466194152832
Validation loss: 2.5366447638439875

Epoch: 6| Step: 3
Training loss: 2.62382173538208
Validation loss: 2.5338168349317325

Epoch: 6| Step: 4
Training loss: 2.6030893325805664
Validation loss: 2.5358667091656755

Epoch: 6| Step: 5
Training loss: 3.1452457904815674
Validation loss: 2.5400001848897626

Epoch: 6| Step: 6
Training loss: 2.938889741897583
Validation loss: 2.548963769789665

Epoch: 6| Step: 7
Training loss: 2.415149450302124
Validation loss: 2.5481387594694733

Epoch: 6| Step: 8
Training loss: 3.862952947616577
Validation loss: 2.603789003946448

Epoch: 6| Step: 9
Training loss: 2.851095199584961
Validation loss: 2.6228551018622612

Epoch: 6| Step: 10
Training loss: 2.5976650714874268
Validation loss: 2.6169668551414245

Epoch: 6| Step: 11
Training loss: 2.563685655593872
Validation loss: 2.592895971831455

Epoch: 6| Step: 12
Training loss: 2.722179889678955
Validation loss: 2.561685303206085

Epoch: 6| Step: 13
Training loss: 2.4064629077911377
Validation loss: 2.5410702305455364

Epoch: 67| Step: 0
Training loss: 3.446725368499756
Validation loss: 2.537743191565237

Epoch: 6| Step: 1
Training loss: 2.9331088066101074
Validation loss: 2.5484580224560154

Epoch: 6| Step: 2
Training loss: 3.3009464740753174
Validation loss: 2.561579253083916

Epoch: 6| Step: 3
Training loss: 3.2320854663848877
Validation loss: 2.566931378456854

Epoch: 6| Step: 4
Training loss: 2.027637481689453
Validation loss: 2.571207733564479

Epoch: 6| Step: 5
Training loss: 3.048771858215332
Validation loss: 2.577514353618827

Epoch: 6| Step: 6
Training loss: 2.6347198486328125
Validation loss: 2.580214500427246

Epoch: 6| Step: 7
Training loss: 2.4004335403442383
Validation loss: 2.568119807909894

Epoch: 6| Step: 8
Training loss: 1.9737505912780762
Validation loss: 2.558884213047643

Epoch: 6| Step: 9
Training loss: 3.302715301513672
Validation loss: 2.5611696525286605

Epoch: 6| Step: 10
Training loss: 2.478532552719116
Validation loss: 2.558738306004514

Epoch: 6| Step: 11
Training loss: 2.705751657485962
Validation loss: 2.5505791223177345

Epoch: 6| Step: 12
Training loss: 2.3818483352661133
Validation loss: 2.5429477358377106

Epoch: 6| Step: 13
Training loss: 2.49228572845459
Validation loss: 2.5367866357167563

Epoch: 68| Step: 0
Training loss: 2.9627180099487305
Validation loss: 2.53407694062879

Epoch: 6| Step: 1
Training loss: 2.3408620357513428
Validation loss: 2.527830075192195

Epoch: 6| Step: 2
Training loss: 2.418839931488037
Validation loss: 2.5315768539264636

Epoch: 6| Step: 3
Training loss: 2.695547342300415
Validation loss: 2.531566696782266

Epoch: 6| Step: 4
Training loss: 2.569962501525879
Validation loss: 2.5360392165440384

Epoch: 6| Step: 5
Training loss: 2.8022284507751465
Validation loss: 2.537904752198086

Epoch: 6| Step: 6
Training loss: 2.4140641689300537
Validation loss: 2.537120937019266

Epoch: 6| Step: 7
Training loss: 3.365823984146118
Validation loss: 2.538942898473432

Epoch: 6| Step: 8
Training loss: 2.620206356048584
Validation loss: 2.539763283985917

Epoch: 6| Step: 9
Training loss: 2.662057876586914
Validation loss: 2.5315414859402563

Epoch: 6| Step: 10
Training loss: 2.729438543319702
Validation loss: 2.5270364566515853

Epoch: 6| Step: 11
Training loss: 2.9599852561950684
Validation loss: 2.5267495032279723

Epoch: 6| Step: 12
Training loss: 3.145339012145996
Validation loss: 2.529039295770789

Epoch: 6| Step: 13
Training loss: 2.614912986755371
Validation loss: 2.5298271102289998

Epoch: 69| Step: 0
Training loss: 2.673135757446289
Validation loss: 2.5267916289708947

Epoch: 6| Step: 1
Training loss: 3.1941750049591064
Validation loss: 2.529179488458941

Epoch: 6| Step: 2
Training loss: 3.4055685997009277
Validation loss: 2.524893827335809

Epoch: 6| Step: 3
Training loss: 3.490140438079834
Validation loss: 2.5257205681134294

Epoch: 6| Step: 4
Training loss: 3.060453414916992
Validation loss: 2.5273519562136744

Epoch: 6| Step: 5
Training loss: 2.707920551300049
Validation loss: 2.5263481165773127

Epoch: 6| Step: 6
Training loss: 3.57912540435791
Validation loss: 2.5230953385753017

Epoch: 6| Step: 7
Training loss: 1.5701922178268433
Validation loss: 2.5238542851581367

Epoch: 6| Step: 8
Training loss: 2.5141282081604004
Validation loss: 2.524653596262778

Epoch: 6| Step: 9
Training loss: 2.4774398803710938
Validation loss: 2.5322254114253546

Epoch: 6| Step: 10
Training loss: 2.429075241088867
Validation loss: 2.528394422223491

Epoch: 6| Step: 11
Training loss: 1.871758222579956
Validation loss: 2.534727604158463

Epoch: 6| Step: 12
Training loss: 2.759525775909424
Validation loss: 2.530578028771185

Epoch: 6| Step: 13
Training loss: 2.460972309112549
Validation loss: 2.530705528874551

Epoch: 70| Step: 0
Training loss: 2.511031150817871
Validation loss: 2.52716056505839

Epoch: 6| Step: 1
Training loss: 2.5715672969818115
Validation loss: 2.5271358643808672

Epoch: 6| Step: 2
Training loss: 2.859281063079834
Validation loss: 2.5260273846246863

Epoch: 6| Step: 3
Training loss: 1.8931329250335693
Validation loss: 2.5272959278475855

Epoch: 6| Step: 4
Training loss: 3.298952579498291
Validation loss: 2.5270450115203857

Epoch: 6| Step: 5
Training loss: 2.3016037940979004
Validation loss: 2.5243471104611634

Epoch: 6| Step: 6
Training loss: 2.7909653186798096
Validation loss: 2.5258527468609553

Epoch: 6| Step: 7
Training loss: 3.2360129356384277
Validation loss: 2.522800494265813

Epoch: 6| Step: 8
Training loss: 3.409905195236206
Validation loss: 2.5259369188739407

Epoch: 6| Step: 9
Training loss: 2.358412981033325
Validation loss: 2.51877163815242

Epoch: 6| Step: 10
Training loss: 2.0409905910491943
Validation loss: 2.5214289285803355

Epoch: 6| Step: 11
Training loss: 2.2196383476257324
Validation loss: 2.520273793128229

Epoch: 6| Step: 12
Training loss: 3.525908946990967
Validation loss: 2.521590248230965

Epoch: 6| Step: 13
Training loss: 3.519289970397949
Validation loss: 2.523505626186248

Epoch: 71| Step: 0
Training loss: 2.9344167709350586
Validation loss: 2.521809757396739

Epoch: 6| Step: 1
Training loss: 3.003732681274414
Validation loss: 2.5232143658463673

Epoch: 6| Step: 2
Training loss: 2.3511087894439697
Validation loss: 2.521215820825228

Epoch: 6| Step: 3
Training loss: 2.687391519546509
Validation loss: 2.5222853101709837

Epoch: 6| Step: 4
Training loss: 2.692924976348877
Validation loss: 2.5242092276132233

Epoch: 6| Step: 5
Training loss: 2.53885555267334
Validation loss: 2.523048331660609

Epoch: 6| Step: 6
Training loss: 2.248595952987671
Validation loss: 2.518641064243932

Epoch: 6| Step: 7
Training loss: 2.8390841484069824
Validation loss: 2.523422933393909

Epoch: 6| Step: 8
Training loss: 3.174788475036621
Validation loss: 2.5222171224573606

Epoch: 6| Step: 9
Training loss: 2.5685391426086426
Validation loss: 2.524600257155716

Epoch: 6| Step: 10
Training loss: 3.0165457725524902
Validation loss: 2.526864379964849

Epoch: 6| Step: 11
Training loss: 3.4267849922180176
Validation loss: 2.531839680928056

Epoch: 6| Step: 12
Training loss: 2.5143492221832275
Validation loss: 2.52765223287767

Epoch: 6| Step: 13
Training loss: 1.907982349395752
Validation loss: 2.526718049921015

Epoch: 72| Step: 0
Training loss: 2.7777886390686035
Validation loss: 2.5217834736711238

Epoch: 6| Step: 1
Training loss: 2.4837546348571777
Validation loss: 2.5176912456430416

Epoch: 6| Step: 2
Training loss: 2.2805259227752686
Validation loss: 2.5212005338361188

Epoch: 6| Step: 3
Training loss: 3.323748826980591
Validation loss: 2.5244281599598546

Epoch: 6| Step: 4
Training loss: 2.768481731414795
Validation loss: 2.5316691834439515

Epoch: 6| Step: 5
Training loss: 1.9310493469238281
Validation loss: 2.532132843489288

Epoch: 6| Step: 6
Training loss: 2.8264193534851074
Validation loss: 2.5362213093747377

Epoch: 6| Step: 7
Training loss: 3.4129223823547363
Validation loss: 2.5421756262420327

Epoch: 6| Step: 8
Training loss: 1.8255469799041748
Validation loss: 2.5409576303215435

Epoch: 6| Step: 9
Training loss: 3.1070408821105957
Validation loss: 2.540158828099569

Epoch: 6| Step: 10
Training loss: 3.0892038345336914
Validation loss: 2.5386719908765567

Epoch: 6| Step: 11
Training loss: 2.9810030460357666
Validation loss: 2.535531097842801

Epoch: 6| Step: 12
Training loss: 2.567415714263916
Validation loss: 2.5311284731793147

Epoch: 6| Step: 13
Training loss: 2.91982364654541
Validation loss: 2.5342325882245134

Epoch: 73| Step: 0
Training loss: 2.6795177459716797
Validation loss: 2.5240037774526947

Epoch: 6| Step: 1
Training loss: 1.7811644077301025
Validation loss: 2.5258893043764177

Epoch: 6| Step: 2
Training loss: 2.5740880966186523
Validation loss: 2.5241854716372747

Epoch: 6| Step: 3
Training loss: 2.349945545196533
Validation loss: 2.525671789723058

Epoch: 6| Step: 4
Training loss: 2.7005207538604736
Validation loss: 2.525812333629977

Epoch: 6| Step: 5
Training loss: 2.497635841369629
Validation loss: 2.5420513050530547

Epoch: 6| Step: 6
Training loss: 2.6312766075134277
Validation loss: 2.5415745037858204

Epoch: 6| Step: 7
Training loss: 3.221585988998413
Validation loss: 2.5437866103264595

Epoch: 6| Step: 8
Training loss: 2.885422706604004
Validation loss: 2.5275169546886156

Epoch: 6| Step: 9
Training loss: 3.599299907684326
Validation loss: 2.525076919986356

Epoch: 6| Step: 10
Training loss: 2.7531275749206543
Validation loss: 2.5166357332660305

Epoch: 6| Step: 11
Training loss: 2.336484909057617
Validation loss: 2.519249029057

Epoch: 6| Step: 12
Training loss: 3.3313894271850586
Validation loss: 2.5253525344274377

Epoch: 6| Step: 13
Training loss: 2.926302909851074
Validation loss: 2.5376262126430387

Epoch: 74| Step: 0
Training loss: 3.233335494995117
Validation loss: 2.5536873494425127

Epoch: 6| Step: 1
Training loss: 3.3841662406921387
Validation loss: 2.556513870916059

Epoch: 6| Step: 2
Training loss: 2.3678829669952393
Validation loss: 2.5564293733207126

Epoch: 6| Step: 3
Training loss: 3.2770984172821045
Validation loss: 2.5578223274600123

Epoch: 6| Step: 4
Training loss: 2.4548301696777344
Validation loss: 2.552855553165559

Epoch: 6| Step: 5
Training loss: 2.466616630554199
Validation loss: 2.550463050924322

Epoch: 6| Step: 6
Training loss: 3.350238084793091
Validation loss: 2.539697101039271

Epoch: 6| Step: 7
Training loss: 2.862027168273926
Validation loss: 2.5313039492535334

Epoch: 6| Step: 8
Training loss: 2.1210615634918213
Validation loss: 2.5309645078515493

Epoch: 6| Step: 9
Training loss: 2.8341195583343506
Validation loss: 2.532267698677637

Epoch: 6| Step: 10
Training loss: 3.2967491149902344
Validation loss: 2.525221888737012

Epoch: 6| Step: 11
Training loss: 2.1801998615264893
Validation loss: 2.51884219466999

Epoch: 6| Step: 12
Training loss: 2.4804770946502686
Validation loss: 2.5173669374117287

Epoch: 6| Step: 13
Training loss: 1.6681424379348755
Validation loss: 2.5138836240255706

Epoch: 75| Step: 0
Training loss: 2.207423448562622
Validation loss: 2.5134621102322816

Epoch: 6| Step: 1
Training loss: 2.6894092559814453
Validation loss: 2.5182029842048563

Epoch: 6| Step: 2
Training loss: 3.1296656131744385
Validation loss: 2.5175200969942155

Epoch: 6| Step: 3
Training loss: 2.388282060623169
Validation loss: 2.5244290649249987

Epoch: 6| Step: 4
Training loss: 2.5842084884643555
Validation loss: 2.5245434648247174

Epoch: 6| Step: 5
Training loss: 2.8461709022521973
Validation loss: 2.525802361067905

Epoch: 6| Step: 6
Training loss: 2.652975082397461
Validation loss: 2.521677258194134

Epoch: 6| Step: 7
Training loss: 2.4716687202453613
Validation loss: 2.5265700791471746

Epoch: 6| Step: 8
Training loss: 3.704503059387207
Validation loss: 2.525175645787229

Epoch: 6| Step: 9
Training loss: 2.4093594551086426
Validation loss: 2.5229505851704586

Epoch: 6| Step: 10
Training loss: 2.061782121658325
Validation loss: 2.5178622609825543

Epoch: 6| Step: 11
Training loss: 2.7179951667785645
Validation loss: 2.518600712540329

Epoch: 6| Step: 12
Training loss: 3.260688066482544
Validation loss: 2.5224377955159833

Epoch: 6| Step: 13
Training loss: 3.2651031017303467
Validation loss: 2.5211981009411555

Epoch: 76| Step: 0
Training loss: 2.097783088684082
Validation loss: 2.522717821982599

Epoch: 6| Step: 1
Training loss: 2.308554172515869
Validation loss: 2.5195026064431794

Epoch: 6| Step: 2
Training loss: 3.217031240463257
Validation loss: 2.51880677541097

Epoch: 6| Step: 3
Training loss: 2.6460728645324707
Validation loss: 2.518359837993499

Epoch: 6| Step: 4
Training loss: 2.3161983489990234
Validation loss: 2.510411872658678

Epoch: 6| Step: 5
Training loss: 2.6614627838134766
Validation loss: 2.5111460942094044

Epoch: 6| Step: 6
Training loss: 2.903726100921631
Validation loss: 2.512875500545707

Epoch: 6| Step: 7
Training loss: 2.8180298805236816
Validation loss: 2.5107352989976124

Epoch: 6| Step: 8
Training loss: 2.516817331314087
Validation loss: 2.5143114469384633

Epoch: 6| Step: 9
Training loss: 3.2730677127838135
Validation loss: 2.5107106162655737

Epoch: 6| Step: 10
Training loss: 3.0117788314819336
Validation loss: 2.511318337532782

Epoch: 6| Step: 11
Training loss: 2.7514381408691406
Validation loss: 2.5147334657689577

Epoch: 6| Step: 12
Training loss: 2.40635347366333
Validation loss: 2.514283267400598

Epoch: 6| Step: 13
Training loss: 3.508896827697754
Validation loss: 2.513223548089304

Epoch: 77| Step: 0
Training loss: 2.375800609588623
Validation loss: 2.512745236837736

Epoch: 6| Step: 1
Training loss: 3.0330052375793457
Validation loss: 2.5093806251402824

Epoch: 6| Step: 2
Training loss: 2.827176332473755
Validation loss: 2.5109729177208355

Epoch: 6| Step: 3
Training loss: 2.201404333114624
Validation loss: 2.5108351707458496

Epoch: 6| Step: 4
Training loss: 2.4051589965820312
Validation loss: 2.509638032605571

Epoch: 6| Step: 5
Training loss: 3.447077512741089
Validation loss: 2.5113347268873647

Epoch: 6| Step: 6
Training loss: 3.3463335037231445
Validation loss: 2.508275967772289

Epoch: 6| Step: 7
Training loss: 2.859973192214966
Validation loss: 2.50807519753774

Epoch: 6| Step: 8
Training loss: 2.4653310775756836
Validation loss: 2.5089278298039592

Epoch: 6| Step: 9
Training loss: 3.340263843536377
Validation loss: 2.510594798672584

Epoch: 6| Step: 10
Training loss: 2.0472898483276367
Validation loss: 2.5093616567632204

Epoch: 6| Step: 11
Training loss: 2.3254549503326416
Validation loss: 2.510675799462103

Epoch: 6| Step: 12
Training loss: 2.7922754287719727
Validation loss: 2.5162739984450804

Epoch: 6| Step: 13
Training loss: 2.3952062129974365
Validation loss: 2.519423505311371

Epoch: 78| Step: 0
Training loss: 2.3751940727233887
Validation loss: 2.518408718929496

Epoch: 6| Step: 1
Training loss: 2.9626808166503906
Validation loss: 2.510287041305214

Epoch: 6| Step: 2
Training loss: 2.6015524864196777
Validation loss: 2.5103502786287697

Epoch: 6| Step: 3
Training loss: 2.9594664573669434
Validation loss: 2.5101350045973256

Epoch: 6| Step: 4
Training loss: 2.655071258544922
Validation loss: 2.505754093970022

Epoch: 6| Step: 5
Training loss: 2.7772879600524902
Validation loss: 2.5064692087070917

Epoch: 6| Step: 6
Training loss: 2.716454029083252
Validation loss: 2.5050550558233775

Epoch: 6| Step: 7
Training loss: 2.6056134700775146
Validation loss: 2.510243190232144

Epoch: 6| Step: 8
Training loss: 2.848705291748047
Validation loss: 2.5091564860395206

Epoch: 6| Step: 9
Training loss: 2.947161912918091
Validation loss: 2.5091783013395084

Epoch: 6| Step: 10
Training loss: 2.511237621307373
Validation loss: 2.5136150493416736

Epoch: 6| Step: 11
Training loss: 2.922898769378662
Validation loss: 2.5156137584358134

Epoch: 6| Step: 12
Training loss: 2.4458999633789062
Validation loss: 2.5171154775927143

Epoch: 6| Step: 13
Training loss: 2.646207094192505
Validation loss: 2.5193675051453295

Epoch: 79| Step: 0
Training loss: 3.5848824977874756
Validation loss: 2.516681430160358

Epoch: 6| Step: 1
Training loss: 2.813098430633545
Validation loss: 2.515955553259901

Epoch: 6| Step: 2
Training loss: 3.5610179901123047
Validation loss: 2.5109940395560315

Epoch: 6| Step: 3
Training loss: 2.193787097930908
Validation loss: 2.504674446198248

Epoch: 6| Step: 4
Training loss: 1.9974184036254883
Validation loss: 2.5084110459973736

Epoch: 6| Step: 5
Training loss: 2.838528871536255
Validation loss: 2.503682792827647

Epoch: 6| Step: 6
Training loss: 2.044773817062378
Validation loss: 2.5040610349306496

Epoch: 6| Step: 7
Training loss: 2.766622543334961
Validation loss: 2.5012078515944944

Epoch: 6| Step: 8
Training loss: 2.5988330841064453
Validation loss: 2.4997322251719813

Epoch: 6| Step: 9
Training loss: 2.584268808364868
Validation loss: 2.5004442635402886

Epoch: 6| Step: 10
Training loss: 2.9348859786987305
Validation loss: 2.504479556955317

Epoch: 6| Step: 11
Training loss: 3.4809420108795166
Validation loss: 2.5042994048005793

Epoch: 6| Step: 12
Training loss: 2.473700523376465
Validation loss: 2.5015501053102556

Epoch: 6| Step: 13
Training loss: 1.7001343965530396
Validation loss: 2.503182613721458

Epoch: 80| Step: 0
Training loss: 2.645373821258545
Validation loss: 2.5016590574736237

Epoch: 6| Step: 1
Training loss: 2.2320408821105957
Validation loss: 2.502200911121984

Epoch: 6| Step: 2
Training loss: 2.658172607421875
Validation loss: 2.5009992917378745

Epoch: 6| Step: 3
Training loss: 2.9229462146759033
Validation loss: 2.5056722599972963

Epoch: 6| Step: 4
Training loss: 2.7438478469848633
Validation loss: 2.507811979580951

Epoch: 6| Step: 5
Training loss: 3.1213560104370117
Validation loss: 2.5080252693545435

Epoch: 6| Step: 6
Training loss: 2.9472789764404297
Validation loss: 2.512915777903731

Epoch: 6| Step: 7
Training loss: 3.197246789932251
Validation loss: 2.5070911222888577

Epoch: 6| Step: 8
Training loss: 2.262817144393921
Validation loss: 2.5077056679674374

Epoch: 6| Step: 9
Training loss: 2.9128758907318115
Validation loss: 2.5089935307861655

Epoch: 6| Step: 10
Training loss: 2.612582206726074
Validation loss: 2.5075414667847338

Epoch: 6| Step: 11
Training loss: 2.7519710063934326
Validation loss: 2.511806013763592

Epoch: 6| Step: 12
Training loss: 1.9213030338287354
Validation loss: 2.5104377397926907

Epoch: 6| Step: 13
Training loss: 3.334412097930908
Validation loss: 2.50810343475752

Epoch: 81| Step: 0
Training loss: 2.509045124053955
Validation loss: 2.511118665818245

Epoch: 6| Step: 1
Training loss: 2.9625775814056396
Validation loss: 2.510987680445435

Epoch: 6| Step: 2
Training loss: 2.9331443309783936
Validation loss: 2.5136555548637145

Epoch: 6| Step: 3
Training loss: 3.136385440826416
Validation loss: 2.517484223970803

Epoch: 6| Step: 4
Training loss: 2.513021945953369
Validation loss: 2.508303460254464

Epoch: 6| Step: 5
Training loss: 2.9532032012939453
Validation loss: 2.513189349123227

Epoch: 6| Step: 6
Training loss: 2.1136109828948975
Validation loss: 2.5137568750689105

Epoch: 6| Step: 7
Training loss: 3.064824342727661
Validation loss: 2.504459627213017

Epoch: 6| Step: 8
Training loss: 3.262831449508667
Validation loss: 2.502422812164471

Epoch: 6| Step: 9
Training loss: 1.9485193490982056
Validation loss: 2.4993202737582627

Epoch: 6| Step: 10
Training loss: 2.639741897583008
Validation loss: 2.499193847820323

Epoch: 6| Step: 11
Training loss: 2.5929737091064453
Validation loss: 2.498655037213397

Epoch: 6| Step: 12
Training loss: 2.024829864501953
Validation loss: 2.4959084808185534

Epoch: 6| Step: 13
Training loss: 3.718061685562134
Validation loss: 2.496604565651186

Epoch: 82| Step: 0
Training loss: 2.644235610961914
Validation loss: 2.4965364445922194

Epoch: 6| Step: 1
Training loss: 3.516111373901367
Validation loss: 2.49700669575763

Epoch: 6| Step: 2
Training loss: 2.386125087738037
Validation loss: 2.4943550735391598

Epoch: 6| Step: 3
Training loss: 1.9035367965698242
Validation loss: 2.4966412564759612

Epoch: 6| Step: 4
Training loss: 2.7283217906951904
Validation loss: 2.494107561726724

Epoch: 6| Step: 5
Training loss: 2.854641914367676
Validation loss: 2.4942283117642967

Epoch: 6| Step: 6
Training loss: 2.498966693878174
Validation loss: 2.4945510741203063

Epoch: 6| Step: 7
Training loss: 3.2049131393432617
Validation loss: 2.497152825837494

Epoch: 6| Step: 8
Training loss: 3.192506790161133
Validation loss: 2.4973327805919032

Epoch: 6| Step: 9
Training loss: 2.33500337600708
Validation loss: 2.5032698851759716

Epoch: 6| Step: 10
Training loss: 2.273897647857666
Validation loss: 2.5045356596669843

Epoch: 6| Step: 11
Training loss: 2.868420124053955
Validation loss: 2.504856937675066

Epoch: 6| Step: 12
Training loss: 2.3801937103271484
Validation loss: 2.51234701371962

Epoch: 6| Step: 13
Training loss: 3.4103269577026367
Validation loss: 2.5105627172736713

Epoch: 83| Step: 0
Training loss: 2.3713254928588867
Validation loss: 2.5123310473657425

Epoch: 6| Step: 1
Training loss: 2.809778928756714
Validation loss: 2.5151700947874334

Epoch: 6| Step: 2
Training loss: 3.0715320110321045
Validation loss: 2.5171687320996354

Epoch: 6| Step: 3
Training loss: 2.7196836471557617
Validation loss: 2.512174208958944

Epoch: 6| Step: 4
Training loss: 3.0432815551757812
Validation loss: 2.513193617584885

Epoch: 6| Step: 5
Training loss: 2.989515781402588
Validation loss: 2.5058858471532024

Epoch: 6| Step: 6
Training loss: 2.784409999847412
Validation loss: 2.505761223454629

Epoch: 6| Step: 7
Training loss: 2.270052909851074
Validation loss: 2.5049936873938448

Epoch: 6| Step: 8
Training loss: 2.2549359798431396
Validation loss: 2.499076320279029

Epoch: 6| Step: 9
Training loss: 3.447786331176758
Validation loss: 2.498592148544968

Epoch: 6| Step: 10
Training loss: 2.4428396224975586
Validation loss: 2.502749079017229

Epoch: 6| Step: 11
Training loss: 2.908457040786743
Validation loss: 2.4973377514910955

Epoch: 6| Step: 12
Training loss: 2.6256790161132812
Validation loss: 2.4912518839682303

Epoch: 6| Step: 13
Training loss: 1.6789777278900146
Validation loss: 2.4946871906198482

Epoch: 84| Step: 0
Training loss: 2.9712276458740234
Validation loss: 2.4964991179845666

Epoch: 6| Step: 1
Training loss: 2.98024845123291
Validation loss: 2.4948712856538835

Epoch: 6| Step: 2
Training loss: 2.822172164916992
Validation loss: 2.4900062007288777

Epoch: 6| Step: 3
Training loss: 3.0974960327148438
Validation loss: 2.4977176907241985

Epoch: 6| Step: 4
Training loss: 3.1309614181518555
Validation loss: 2.5020380071414414

Epoch: 6| Step: 5
Training loss: 2.9343442916870117
Validation loss: 2.4972654440069713

Epoch: 6| Step: 6
Training loss: 2.2644855976104736
Validation loss: 2.5041368058932725

Epoch: 6| Step: 7
Training loss: 2.0261237621307373
Validation loss: 2.4948599543622745

Epoch: 6| Step: 8
Training loss: 2.053492307662964
Validation loss: 2.495415251742127

Epoch: 6| Step: 9
Training loss: 2.4633214473724365
Validation loss: 2.4997013897024174

Epoch: 6| Step: 10
Training loss: 3.359806537628174
Validation loss: 2.493700696576026

Epoch: 6| Step: 11
Training loss: 2.5936241149902344
Validation loss: 2.4919597474477624

Epoch: 6| Step: 12
Training loss: 2.8209381103515625
Validation loss: 2.495890566097793

Epoch: 6| Step: 13
Training loss: 2.026895046234131
Validation loss: 2.4939065441008537

Epoch: 85| Step: 0
Training loss: 1.8399556875228882
Validation loss: 2.4949021313780095

Epoch: 6| Step: 1
Training loss: 2.7200403213500977
Validation loss: 2.491224035140007

Epoch: 6| Step: 2
Training loss: 3.073657989501953
Validation loss: 2.491816069490166

Epoch: 6| Step: 3
Training loss: 3.667635440826416
Validation loss: 2.492218389306017

Epoch: 6| Step: 4
Training loss: 2.8130033016204834
Validation loss: 2.493154587284211

Epoch: 6| Step: 5
Training loss: 3.3043265342712402
Validation loss: 2.49097103200933

Epoch: 6| Step: 6
Training loss: 2.5524954795837402
Validation loss: 2.4911248453201784

Epoch: 6| Step: 7
Training loss: 2.7394919395446777
Validation loss: 2.4896932084073304

Epoch: 6| Step: 8
Training loss: 3.1895413398742676
Validation loss: 2.487027196473973

Epoch: 6| Step: 9
Training loss: 2.2268662452697754
Validation loss: 2.4964948572138304

Epoch: 6| Step: 10
Training loss: 2.5957119464874268
Validation loss: 2.4966842487294185

Epoch: 6| Step: 11
Training loss: 2.2107934951782227
Validation loss: 2.5073021099131596

Epoch: 6| Step: 12
Training loss: 2.101238250732422
Validation loss: 2.5011437516058646

Epoch: 6| Step: 13
Training loss: 2.788823366165161
Validation loss: 2.510562289145685

Epoch: 86| Step: 0
Training loss: 2.315943717956543
Validation loss: 2.5015729165846303

Epoch: 6| Step: 1
Training loss: 3.968980550765991
Validation loss: 2.503679580585931

Epoch: 6| Step: 2
Training loss: 3.034590721130371
Validation loss: 2.5099564829180316

Epoch: 6| Step: 3
Training loss: 3.322366952896118
Validation loss: 2.518746410646746

Epoch: 6| Step: 4
Training loss: 2.2336413860321045
Validation loss: 2.525642946202268

Epoch: 6| Step: 5
Training loss: 2.569744110107422
Validation loss: 2.522518680941674

Epoch: 6| Step: 6
Training loss: 2.5461409091949463
Validation loss: 2.5249385987558672

Epoch: 6| Step: 7
Training loss: 2.3559012413024902
Validation loss: 2.5214476277751308

Epoch: 6| Step: 8
Training loss: 2.298299551010132
Validation loss: 2.5179467021778064

Epoch: 6| Step: 9
Training loss: 2.9449639320373535
Validation loss: 2.5096541861052155

Epoch: 6| Step: 10
Training loss: 2.654209613800049
Validation loss: 2.5039675287021104

Epoch: 6| Step: 11
Training loss: 2.510615587234497
Validation loss: 2.499651183364212

Epoch: 6| Step: 12
Training loss: 2.250783920288086
Validation loss: 2.4942939409645657

Epoch: 6| Step: 13
Training loss: 3.0627567768096924
Validation loss: 2.494151617891045

Epoch: 87| Step: 0
Training loss: 2.952755928039551
Validation loss: 2.4957819715622933

Epoch: 6| Step: 1
Training loss: 2.5331954956054688
Validation loss: 2.495860938102968

Epoch: 6| Step: 2
Training loss: 2.9757800102233887
Validation loss: 2.4906581627425326

Epoch: 6| Step: 3
Training loss: 2.601949691772461
Validation loss: 2.4925707232567573

Epoch: 6| Step: 4
Training loss: 2.918292999267578
Validation loss: 2.4896429892509215

Epoch: 6| Step: 5
Training loss: 2.919466495513916
Validation loss: 2.488895539314516

Epoch: 6| Step: 6
Training loss: 1.2488871812820435
Validation loss: 2.492555095303443

Epoch: 6| Step: 7
Training loss: 2.9432215690612793
Validation loss: 2.51081879420947

Epoch: 6| Step: 8
Training loss: 1.9354019165039062
Validation loss: 2.5025272113020702

Epoch: 6| Step: 9
Training loss: 2.991177558898926
Validation loss: 2.5026323410772506

Epoch: 6| Step: 10
Training loss: 3.157036066055298
Validation loss: 2.4937319089007635

Epoch: 6| Step: 11
Training loss: 2.930757522583008
Validation loss: 2.4919918788376676

Epoch: 6| Step: 12
Training loss: 3.04116153717041
Validation loss: 2.4944198439198155

Epoch: 6| Step: 13
Training loss: 2.621001720428467
Validation loss: 2.4965935419964533

Epoch: 88| Step: 0
Training loss: 2.313812255859375
Validation loss: 2.493399807201919

Epoch: 6| Step: 1
Training loss: 2.0032052993774414
Validation loss: 2.492788350710305

Epoch: 6| Step: 2
Training loss: 2.625739336013794
Validation loss: 2.4909838668761717

Epoch: 6| Step: 3
Training loss: 3.071200370788574
Validation loss: 2.4912324208085255

Epoch: 6| Step: 4
Training loss: 4.040482997894287
Validation loss: 2.49017676999492

Epoch: 6| Step: 5
Training loss: 2.5219645500183105
Validation loss: 2.4867905493705504

Epoch: 6| Step: 6
Training loss: 2.2882280349731445
Validation loss: 2.485163983478341

Epoch: 6| Step: 7
Training loss: 2.1145458221435547
Validation loss: 2.4860889014377388

Epoch: 6| Step: 8
Training loss: 2.7035813331604004
Validation loss: 2.4844584875209357

Epoch: 6| Step: 9
Training loss: 3.210381031036377
Validation loss: 2.484692105682947

Epoch: 6| Step: 10
Training loss: 2.8245558738708496
Validation loss: 2.4866614854463966

Epoch: 6| Step: 11
Training loss: 2.5999391078948975
Validation loss: 2.4826365696486605

Epoch: 6| Step: 12
Training loss: 2.070899248123169
Validation loss: 2.484610593447121

Epoch: 6| Step: 13
Training loss: 3.8516883850097656
Validation loss: 2.4867193698883057

Epoch: 89| Step: 0
Training loss: 2.6007518768310547
Validation loss: 2.494083414795578

Epoch: 6| Step: 1
Training loss: 3.120964527130127
Validation loss: 2.4933293352844896

Epoch: 6| Step: 2
Training loss: 2.761293649673462
Validation loss: 2.496631994042345

Epoch: 6| Step: 3
Training loss: 1.9611226320266724
Validation loss: 2.5026660273152013

Epoch: 6| Step: 4
Training loss: 2.0697264671325684
Validation loss: 2.504261637246737

Epoch: 6| Step: 5
Training loss: 2.864856004714966
Validation loss: 2.511348403910155

Epoch: 6| Step: 6
Training loss: 3.07743239402771
Validation loss: 2.5098507019781295

Epoch: 6| Step: 7
Training loss: 2.6252799034118652
Validation loss: 2.5079980614364787

Epoch: 6| Step: 8
Training loss: 3.4793577194213867
Validation loss: 2.5104714439761255

Epoch: 6| Step: 9
Training loss: 2.703538417816162
Validation loss: 2.5065671756703365

Epoch: 6| Step: 10
Training loss: 2.951083183288574
Validation loss: 2.5038789215908257

Epoch: 6| Step: 11
Training loss: 2.533991813659668
Validation loss: 2.4934576711347027

Epoch: 6| Step: 12
Training loss: 2.575003147125244
Validation loss: 2.490176749485795

Epoch: 6| Step: 13
Training loss: 2.2188260555267334
Validation loss: 2.4851038532872356

Epoch: 90| Step: 0
Training loss: 2.4970459938049316
Validation loss: 2.4838047283951954

Epoch: 6| Step: 1
Training loss: 3.0734848976135254
Validation loss: 2.481301289732738

Epoch: 6| Step: 2
Training loss: 2.9767796993255615
Validation loss: 2.4819039119187223

Epoch: 6| Step: 3
Training loss: 2.3125834465026855
Validation loss: 2.4817967286673923

Epoch: 6| Step: 4
Training loss: 2.5021133422851562
Validation loss: 2.4826308604209655

Epoch: 6| Step: 5
Training loss: 3.17234468460083
Validation loss: 2.4835232534716205

Epoch: 6| Step: 6
Training loss: 2.18516206741333
Validation loss: 2.482530393908101

Epoch: 6| Step: 7
Training loss: 2.553053140640259
Validation loss: 2.4818187785404984

Epoch: 6| Step: 8
Training loss: 2.596055507659912
Validation loss: 2.490391428752612

Epoch: 6| Step: 9
Training loss: 1.9135805368423462
Validation loss: 2.485721779125993

Epoch: 6| Step: 10
Training loss: 3.3126468658447266
Validation loss: 2.484604275354775

Epoch: 6| Step: 11
Training loss: 2.5292553901672363
Validation loss: 2.4894083007689445

Epoch: 6| Step: 12
Training loss: 2.9710028171539307
Validation loss: 2.4876100299178914

Epoch: 6| Step: 13
Training loss: 3.325678825378418
Validation loss: 2.4886554133507515

Epoch: 91| Step: 0
Training loss: 3.078143835067749
Validation loss: 2.4891410207235687

Epoch: 6| Step: 1
Training loss: 2.7371950149536133
Validation loss: 2.4949876518659693

Epoch: 6| Step: 2
Training loss: 2.0991783142089844
Validation loss: 2.4885037483707553

Epoch: 6| Step: 3
Training loss: 1.9729949235916138
Validation loss: 2.491691617555516

Epoch: 6| Step: 4
Training loss: 3.0162465572357178
Validation loss: 2.4915735644678914

Epoch: 6| Step: 5
Training loss: 2.7896721363067627
Validation loss: 2.486943288515973

Epoch: 6| Step: 6
Training loss: 3.3356571197509766
Validation loss: 2.4823551536888204

Epoch: 6| Step: 7
Training loss: 2.746792793273926
Validation loss: 2.486090108912478

Epoch: 6| Step: 8
Training loss: 2.582045555114746
Validation loss: 2.478535367596534

Epoch: 6| Step: 9
Training loss: 3.2644803524017334
Validation loss: 2.4795891854070846

Epoch: 6| Step: 10
Training loss: 3.054901123046875
Validation loss: 2.4786977357761835

Epoch: 6| Step: 11
Training loss: 2.0919265747070312
Validation loss: 2.482060932344006

Epoch: 6| Step: 12
Training loss: 2.1492960453033447
Validation loss: 2.478086986849385

Epoch: 6| Step: 13
Training loss: 2.8930890560150146
Validation loss: 2.477947496598767

Epoch: 92| Step: 0
Training loss: 3.1095798015594482
Validation loss: 2.4770216941833496

Epoch: 6| Step: 1
Training loss: 2.933737277984619
Validation loss: 2.4780624066629717

Epoch: 6| Step: 2
Training loss: 2.317901849746704
Validation loss: 2.476215588149204

Epoch: 6| Step: 3
Training loss: 2.1271939277648926
Validation loss: 2.4767750719542145

Epoch: 6| Step: 4
Training loss: 2.3301143646240234
Validation loss: 2.4763049848618044

Epoch: 6| Step: 5
Training loss: 2.9760208129882812
Validation loss: 2.477430110336632

Epoch: 6| Step: 6
Training loss: 2.719666004180908
Validation loss: 2.480286390550675

Epoch: 6| Step: 7
Training loss: 2.500002861022949
Validation loss: 2.48774371352247

Epoch: 6| Step: 8
Training loss: 3.1519649028778076
Validation loss: 2.4927202386240803

Epoch: 6| Step: 9
Training loss: 2.2872185707092285
Validation loss: 2.4986697448197233

Epoch: 6| Step: 10
Training loss: 3.5841989517211914
Validation loss: 2.4866799231498473

Epoch: 6| Step: 11
Training loss: 2.9217536449432373
Validation loss: 2.4807422955830893

Epoch: 6| Step: 12
Training loss: 2.1014156341552734
Validation loss: 2.482872739914925

Epoch: 6| Step: 13
Training loss: 2.5848121643066406
Validation loss: 2.4811562389455815

Epoch: 93| Step: 0
Training loss: 2.7145752906799316
Validation loss: 2.4908460724738335

Epoch: 6| Step: 1
Training loss: 2.1271603107452393
Validation loss: 2.4990302055112776

Epoch: 6| Step: 2
Training loss: 2.338994026184082
Validation loss: 2.5134906922617266

Epoch: 6| Step: 3
Training loss: 2.8157196044921875
Validation loss: 2.517233730644308

Epoch: 6| Step: 4
Training loss: 3.0803422927856445
Validation loss: 2.520990438358758

Epoch: 6| Step: 5
Training loss: 4.129185676574707
Validation loss: 2.521931181671799

Epoch: 6| Step: 6
Training loss: 2.589095115661621
Validation loss: 2.5167687913422943

Epoch: 6| Step: 7
Training loss: 2.698991298675537
Validation loss: 2.520430548216707

Epoch: 6| Step: 8
Training loss: 2.7607901096343994
Validation loss: 2.511874114313433

Epoch: 6| Step: 9
Training loss: 2.261209726333618
Validation loss: 2.505288267648348

Epoch: 6| Step: 10
Training loss: 2.1438326835632324
Validation loss: 2.5015952100035963

Epoch: 6| Step: 11
Training loss: 2.151721477508545
Validation loss: 2.484558077268703

Epoch: 6| Step: 12
Training loss: 3.506631374359131
Validation loss: 2.4812380088272916

Epoch: 6| Step: 13
Training loss: 2.255671739578247
Validation loss: 2.478371189486596

Epoch: 94| Step: 0
Training loss: 2.8551039695739746
Validation loss: 2.4759578909925235

Epoch: 6| Step: 1
Training loss: 3.2482364177703857
Validation loss: 2.4754668179378716

Epoch: 6| Step: 2
Training loss: 2.413362979888916
Validation loss: 2.4786345061435493

Epoch: 6| Step: 3
Training loss: 2.09700870513916
Validation loss: 2.4800868598363732

Epoch: 6| Step: 4
Training loss: 3.5868914127349854
Validation loss: 2.4804222071042625

Epoch: 6| Step: 5
Training loss: 3.093229055404663
Validation loss: 2.4873754593633834

Epoch: 6| Step: 6
Training loss: 2.912526845932007
Validation loss: 2.4820927548152145

Epoch: 6| Step: 7
Training loss: 2.1952757835388184
Validation loss: 2.4794683969149025

Epoch: 6| Step: 8
Training loss: 3.085956573486328
Validation loss: 2.481594704812573

Epoch: 6| Step: 9
Training loss: 1.8797190189361572
Validation loss: 2.4825359749537643

Epoch: 6| Step: 10
Training loss: 2.441361904144287
Validation loss: 2.483036753951862

Epoch: 6| Step: 11
Training loss: 2.1043267250061035
Validation loss: 2.4848859899787494

Epoch: 6| Step: 12
Training loss: 3.0437264442443848
Validation loss: 2.4937146761084117

Epoch: 6| Step: 13
Training loss: 2.6684165000915527
Validation loss: 2.494352474007555

Epoch: 95| Step: 0
Training loss: 2.7053709030151367
Validation loss: 2.4978067259634695

Epoch: 6| Step: 1
Training loss: 2.8313283920288086
Validation loss: 2.501722228142523

Epoch: 6| Step: 2
Training loss: 3.100801467895508
Validation loss: 2.505555424638974

Epoch: 6| Step: 3
Training loss: 2.4944067001342773
Validation loss: 2.5069042636502172

Epoch: 6| Step: 4
Training loss: 2.8823423385620117
Validation loss: 2.5077384133492746

Epoch: 6| Step: 5
Training loss: 1.7657039165496826
Validation loss: 2.508255204846782

Epoch: 6| Step: 6
Training loss: 2.413172960281372
Validation loss: 2.5052401993864324

Epoch: 6| Step: 7
Training loss: 3.438016653060913
Validation loss: 2.501872408774591

Epoch: 6| Step: 8
Training loss: 3.7113592624664307
Validation loss: 2.4962894198715047

Epoch: 6| Step: 9
Training loss: 2.4168808460235596
Validation loss: 2.4906292628216486

Epoch: 6| Step: 10
Training loss: 2.2019152641296387
Validation loss: 2.4808813653966433

Epoch: 6| Step: 11
Training loss: 2.5641427040100098
Validation loss: 2.481024275543869

Epoch: 6| Step: 12
Training loss: 2.14159893989563
Validation loss: 2.4796481209416545

Epoch: 6| Step: 13
Training loss: 3.3836758136749268
Validation loss: 2.4805502814631306

Epoch: 96| Step: 0
Training loss: 3.1080985069274902
Validation loss: 2.4777313201658187

Epoch: 6| Step: 1
Training loss: 2.7007288932800293
Validation loss: 2.4823690793847524

Epoch: 6| Step: 2
Training loss: 2.3991589546203613
Validation loss: 2.4780378546766055

Epoch: 6| Step: 3
Training loss: 2.7494893074035645
Validation loss: 2.4824852071782595

Epoch: 6| Step: 4
Training loss: 2.3927581310272217
Validation loss: 2.4826212518958637

Epoch: 6| Step: 5
Training loss: 3.2425670623779297
Validation loss: 2.483867532463484

Epoch: 6| Step: 6
Training loss: 2.5999717712402344
Validation loss: 2.4829057544790287

Epoch: 6| Step: 7
Training loss: 2.483546733856201
Validation loss: 2.4804572700172343

Epoch: 6| Step: 8
Training loss: 2.735579013824463
Validation loss: 2.4816601891671457

Epoch: 6| Step: 9
Training loss: 2.834082841873169
Validation loss: 2.482082420779813

Epoch: 6| Step: 10
Training loss: 1.986634373664856
Validation loss: 2.4832996629899546

Epoch: 6| Step: 11
Training loss: 2.6408843994140625
Validation loss: 2.478640451226183

Epoch: 6| Step: 12
Training loss: 2.895890712738037
Validation loss: 2.481145433200303

Epoch: 6| Step: 13
Training loss: 2.898066759109497
Validation loss: 2.4819829284503894

Epoch: 97| Step: 0
Training loss: 2.666900157928467
Validation loss: 2.478040910536243

Epoch: 6| Step: 1
Training loss: 2.232491970062256
Validation loss: 2.481093252858808

Epoch: 6| Step: 2
Training loss: 2.6670961380004883
Validation loss: 2.490343319472446

Epoch: 6| Step: 3
Training loss: 2.9668478965759277
Validation loss: 2.4877720186787267

Epoch: 6| Step: 4
Training loss: 2.889765739440918
Validation loss: 2.4868489439769457

Epoch: 6| Step: 5
Training loss: 3.5976133346557617
Validation loss: 2.488386264411352

Epoch: 6| Step: 6
Training loss: 2.901930809020996
Validation loss: 2.4859113372782224

Epoch: 6| Step: 7
Training loss: 1.9954569339752197
Validation loss: 2.4841232658714376

Epoch: 6| Step: 8
Training loss: 3.075664520263672
Validation loss: 2.4780301253000894

Epoch: 6| Step: 9
Training loss: 2.4843344688415527
Validation loss: 2.4789436837678314

Epoch: 6| Step: 10
Training loss: 3.0899059772491455
Validation loss: 2.47964140676683

Epoch: 6| Step: 11
Training loss: 1.532680630683899
Validation loss: 2.4766124192104546

Epoch: 6| Step: 12
Training loss: 2.6890792846679688
Validation loss: 2.478358622520201

Epoch: 6| Step: 13
Training loss: 2.8995518684387207
Validation loss: 2.475609517866565

Epoch: 98| Step: 0
Training loss: 2.073845386505127
Validation loss: 2.479241619827927

Epoch: 6| Step: 1
Training loss: 2.8638315200805664
Validation loss: 2.4724290037667878

Epoch: 6| Step: 2
Training loss: 2.241509199142456
Validation loss: 2.4791030781243437

Epoch: 6| Step: 3
Training loss: 3.226320505142212
Validation loss: 2.475907520581317

Epoch: 6| Step: 4
Training loss: 3.2179477214813232
Validation loss: 2.4720573553475003

Epoch: 6| Step: 5
Training loss: 3.224503993988037
Validation loss: 2.474575765671269

Epoch: 6| Step: 6
Training loss: 1.4093561172485352
Validation loss: 2.473212319035684

Epoch: 6| Step: 7
Training loss: 2.566586494445801
Validation loss: 2.472802354443458

Epoch: 6| Step: 8
Training loss: 2.1065545082092285
Validation loss: 2.473781907430259

Epoch: 6| Step: 9
Training loss: 3.63118577003479
Validation loss: 2.473315382516512

Epoch: 6| Step: 10
Training loss: 2.0142016410827637
Validation loss: 2.476292651186707

Epoch: 6| Step: 11
Training loss: 3.098747491836548
Validation loss: 2.477692416919175

Epoch: 6| Step: 12
Training loss: 3.5797250270843506
Validation loss: 2.4797995782667592

Epoch: 6| Step: 13
Training loss: 1.9361252784729004
Validation loss: 2.4800700808084137

Epoch: 99| Step: 0
Training loss: 2.4435906410217285
Validation loss: 2.494593922809888

Epoch: 6| Step: 1
Training loss: 2.295741081237793
Validation loss: 2.50237081127782

Epoch: 6| Step: 2
Training loss: 1.9662632942199707
Validation loss: 2.511378462596606

Epoch: 6| Step: 3
Training loss: 2.5206613540649414
Validation loss: 2.512410609952865

Epoch: 6| Step: 4
Training loss: 2.6217055320739746
Validation loss: 2.5132233429980535

Epoch: 6| Step: 5
Training loss: 2.424561023712158
Validation loss: 2.5053014396339335

Epoch: 6| Step: 6
Training loss: 2.424043655395508
Validation loss: 2.5216191096972396

Epoch: 6| Step: 7
Training loss: 3.6867752075195312
Validation loss: 2.519839796968686

Epoch: 6| Step: 8
Training loss: 2.9999446868896484
Validation loss: 2.524843474870087

Epoch: 6| Step: 9
Training loss: 2.5492172241210938
Validation loss: 2.520485196062314

Epoch: 6| Step: 10
Training loss: 2.767030715942383
Validation loss: 2.515366344041722

Epoch: 6| Step: 11
Training loss: 4.2191877365112305
Validation loss: 2.5199168779516734

Epoch: 6| Step: 12
Training loss: 2.4358677864074707
Validation loss: 2.5142638965319564

Epoch: 6| Step: 13
Training loss: 2.322683811187744
Validation loss: 2.5120655823779363

Epoch: 100| Step: 0
Training loss: 2.510016918182373
Validation loss: 2.5136248116852133

Epoch: 6| Step: 1
Training loss: 2.284722328186035
Validation loss: 2.5078576508388726

Epoch: 6| Step: 2
Training loss: 2.3418991565704346
Validation loss: 2.509025994167533

Epoch: 6| Step: 3
Training loss: 3.4619998931884766
Validation loss: 2.4992165334763063

Epoch: 6| Step: 4
Training loss: 3.4311630725860596
Validation loss: 2.493947518769131

Epoch: 6| Step: 5
Training loss: 2.39990234375
Validation loss: 2.4911794406111523

Epoch: 6| Step: 6
Training loss: 2.8070006370544434
Validation loss: 2.4853264311308503

Epoch: 6| Step: 7
Training loss: 3.264594554901123
Validation loss: 2.480576710034442

Epoch: 6| Step: 8
Training loss: 1.835099220275879
Validation loss: 2.4782170582843084

Epoch: 6| Step: 9
Training loss: 2.676882743835449
Validation loss: 2.47715651348073

Epoch: 6| Step: 10
Training loss: 2.349778652191162
Validation loss: 2.477512272455359

Epoch: 6| Step: 11
Training loss: 3.091564178466797
Validation loss: 2.483093923138034

Epoch: 6| Step: 12
Training loss: 2.3724515438079834
Validation loss: 2.47892334640667

Epoch: 6| Step: 13
Training loss: 2.8477578163146973
Validation loss: 2.476320274414555

Epoch: 101| Step: 0
Training loss: 2.3716259002685547
Validation loss: 2.477290020194105

Epoch: 6| Step: 1
Training loss: 2.5798351764678955
Validation loss: 2.4756071054807274

Epoch: 6| Step: 2
Training loss: 2.4543380737304688
Validation loss: 2.479646175138412

Epoch: 6| Step: 3
Training loss: 3.1020400524139404
Validation loss: 2.4835815160505232

Epoch: 6| Step: 4
Training loss: 2.540207862854004
Validation loss: 2.4903205517799623

Epoch: 6| Step: 5
Training loss: 2.102473258972168
Validation loss: 2.4898343701516428

Epoch: 6| Step: 6
Training loss: 2.5919189453125
Validation loss: 2.4888539096360565

Epoch: 6| Step: 7
Training loss: 3.3708202838897705
Validation loss: 2.492559499638055

Epoch: 6| Step: 8
Training loss: 2.540526866912842
Validation loss: 2.491974889591176

Epoch: 6| Step: 9
Training loss: 2.498288154602051
Validation loss: 2.4857667594827633

Epoch: 6| Step: 10
Training loss: 2.593890428543091
Validation loss: 2.483680607170187

Epoch: 6| Step: 11
Training loss: 2.6257412433624268
Validation loss: 2.4828281261587657

Epoch: 6| Step: 12
Training loss: 3.265151023864746
Validation loss: 2.487016477892476

Epoch: 6| Step: 13
Training loss: 3.039938449859619
Validation loss: 2.4878550370534263

Epoch: 102| Step: 0
Training loss: 2.364708662033081
Validation loss: 2.481576645246116

Epoch: 6| Step: 1
Training loss: 3.4452643394470215
Validation loss: 2.4756007450883106

Epoch: 6| Step: 2
Training loss: 2.4252591133117676
Validation loss: 2.4734533422736713

Epoch: 6| Step: 3
Training loss: 2.8645403385162354
Validation loss: 2.470166571678654

Epoch: 6| Step: 4
Training loss: 2.9391355514526367
Validation loss: 2.4675615269650697

Epoch: 6| Step: 5
Training loss: 2.4455461502075195
Validation loss: 2.4662596307775027

Epoch: 6| Step: 6
Training loss: 3.00445556640625
Validation loss: 2.461532103118076

Epoch: 6| Step: 7
Training loss: 2.662217855453491
Validation loss: 2.463058292224843

Epoch: 6| Step: 8
Training loss: 2.2093138694763184
Validation loss: 2.465274703118109

Epoch: 6| Step: 9
Training loss: 2.9304065704345703
Validation loss: 2.463988609211419

Epoch: 6| Step: 10
Training loss: 3.209207773208618
Validation loss: 2.4709790355415753

Epoch: 6| Step: 11
Training loss: 2.472289800643921
Validation loss: 2.471543686364287

Epoch: 6| Step: 12
Training loss: 2.30275297164917
Validation loss: 2.470569541377406

Epoch: 6| Step: 13
Training loss: 1.7799001932144165
Validation loss: 2.476205925787649

Epoch: 103| Step: 0
Training loss: 2.4910831451416016
Validation loss: 2.4768203022659465

Epoch: 6| Step: 1
Training loss: 1.822014570236206
Validation loss: 2.479418859686903

Epoch: 6| Step: 2
Training loss: 3.021998643875122
Validation loss: 2.483302439412763

Epoch: 6| Step: 3
Training loss: 2.5924630165100098
Validation loss: 2.478533560229886

Epoch: 6| Step: 4
Training loss: 2.111300468444824
Validation loss: 2.4761887032498597

Epoch: 6| Step: 5
Training loss: 3.14703106880188
Validation loss: 2.473585162111508

Epoch: 6| Step: 6
Training loss: 3.0939579010009766
Validation loss: 2.471100022715907

Epoch: 6| Step: 7
Training loss: 3.112311363220215
Validation loss: 2.466835570591752

Epoch: 6| Step: 8
Training loss: 2.169475555419922
Validation loss: 2.4588905098617717

Epoch: 6| Step: 9
Training loss: 3.052457094192505
Validation loss: 2.458461043655231

Epoch: 6| Step: 10
Training loss: 3.3123035430908203
Validation loss: 2.4564597811750186

Epoch: 6| Step: 11
Training loss: 2.7787177562713623
Validation loss: 2.459097739188902

Epoch: 6| Step: 12
Training loss: 2.3908801078796387
Validation loss: 2.4609232333398636

Epoch: 6| Step: 13
Training loss: 2.298121452331543
Validation loss: 2.4630245854777675

Epoch: 104| Step: 0
Training loss: 3.3687992095947266
Validation loss: 2.4575735010126585

Epoch: 6| Step: 1
Training loss: 3.0551609992980957
Validation loss: 2.463620762671194

Epoch: 6| Step: 2
Training loss: 2.077289581298828
Validation loss: 2.457485075919859

Epoch: 6| Step: 3
Training loss: 2.881568431854248
Validation loss: 2.4563890631480882

Epoch: 6| Step: 4
Training loss: 3.198518753051758
Validation loss: 2.457852263604441

Epoch: 6| Step: 5
Training loss: 2.714629888534546
Validation loss: 2.460720310929001

Epoch: 6| Step: 6
Training loss: 2.784759044647217
Validation loss: 2.4623541626878964

Epoch: 6| Step: 7
Training loss: 3.221348762512207
Validation loss: 2.458838365411246

Epoch: 6| Step: 8
Training loss: 2.6770339012145996
Validation loss: 2.457972316331761

Epoch: 6| Step: 9
Training loss: 3.103849172592163
Validation loss: 2.4580271385049306

Epoch: 6| Step: 10
Training loss: 1.6264954805374146
Validation loss: 2.461500685702088

Epoch: 6| Step: 11
Training loss: 2.072519302368164
Validation loss: 2.460994892222907

Epoch: 6| Step: 12
Training loss: 2.0500571727752686
Validation loss: 2.4660789223127466

Epoch: 6| Step: 13
Training loss: 2.5703020095825195
Validation loss: 2.483398227281468

Epoch: 105| Step: 0
Training loss: 2.817903757095337
Validation loss: 2.4979538968814317

Epoch: 6| Step: 1
Training loss: 2.767345428466797
Validation loss: 2.479427086409702

Epoch: 6| Step: 2
Training loss: 2.864591598510742
Validation loss: 2.469277963843397

Epoch: 6| Step: 3
Training loss: 3.03584623336792
Validation loss: 2.4629289104092504

Epoch: 6| Step: 4
Training loss: 2.5603647232055664
Validation loss: 2.462491035461426

Epoch: 6| Step: 5
Training loss: 2.299839496612549
Validation loss: 2.4656276164516324

Epoch: 6| Step: 6
Training loss: 2.820965051651001
Validation loss: 2.4714681410020396

Epoch: 6| Step: 7
Training loss: 2.0495409965515137
Validation loss: 2.4716335035139516

Epoch: 6| Step: 8
Training loss: 2.9096121788024902
Validation loss: 2.4840971936461744

Epoch: 6| Step: 9
Training loss: 3.150796890258789
Validation loss: 2.4873915538992932

Epoch: 6| Step: 10
Training loss: 3.090071201324463
Validation loss: 2.481741684739308

Epoch: 6| Step: 11
Training loss: 2.2805469036102295
Validation loss: 2.4960713873627367

Epoch: 6| Step: 12
Training loss: 2.266491413116455
Validation loss: 2.511006445013067

Epoch: 6| Step: 13
Training loss: 2.581818103790283
Validation loss: 2.5251555596628497

Epoch: 106| Step: 0
Training loss: 3.0550808906555176
Validation loss: 2.5116164530477216

Epoch: 6| Step: 1
Training loss: 2.068511724472046
Validation loss: 2.5376130445029146

Epoch: 6| Step: 2
Training loss: 3.7890145778656006
Validation loss: 2.5490283812246015

Epoch: 6| Step: 3
Training loss: 2.724738359451294
Validation loss: 2.529984748491677

Epoch: 6| Step: 4
Training loss: 2.368267297744751
Validation loss: 2.483766549377031

Epoch: 6| Step: 5
Training loss: 2.5284619331359863
Validation loss: 2.490687426700387

Epoch: 6| Step: 6
Training loss: 2.7566919326782227
Validation loss: 2.499148202198808

Epoch: 6| Step: 7
Training loss: 2.6166200637817383
Validation loss: 2.512602298490463

Epoch: 6| Step: 8
Training loss: 3.3731367588043213
Validation loss: 2.536395239573653

Epoch: 6| Step: 9
Training loss: 2.461860179901123
Validation loss: 2.5368786986156175

Epoch: 6| Step: 10
Training loss: 2.206800937652588
Validation loss: 2.5236426989237466

Epoch: 6| Step: 11
Training loss: 2.330972194671631
Validation loss: 2.516913701129216

Epoch: 6| Step: 12
Training loss: 3.1720612049102783
Validation loss: 2.5092905952084448

Epoch: 6| Step: 13
Training loss: 2.245511770248413
Validation loss: 2.5078780907456593

Epoch: 107| Step: 0
Training loss: 2.3603873252868652
Validation loss: 2.5076964542429936

Epoch: 6| Step: 1
Training loss: 3.6892919540405273
Validation loss: 2.4999451560358845

Epoch: 6| Step: 2
Training loss: 2.248901844024658
Validation loss: 2.498915574883902

Epoch: 6| Step: 3
Training loss: 2.2198104858398438
Validation loss: 2.4856192347823933

Epoch: 6| Step: 4
Training loss: 2.2172048091888428
Validation loss: 2.4883642760656213

Epoch: 6| Step: 5
Training loss: 2.971921443939209
Validation loss: 2.4843418290538173

Epoch: 6| Step: 6
Training loss: 3.404294013977051
Validation loss: 2.484330528525896

Epoch: 6| Step: 7
Training loss: 1.3319579362869263
Validation loss: 2.4831314163823284

Epoch: 6| Step: 8
Training loss: 3.1558680534362793
Validation loss: 2.4829321933049027

Epoch: 6| Step: 9
Training loss: 2.819674015045166
Validation loss: 2.487302736569476

Epoch: 6| Step: 10
Training loss: 2.442425012588501
Validation loss: 2.4848943448835805

Epoch: 6| Step: 11
Training loss: 3.0551013946533203
Validation loss: 2.4832637745846986

Epoch: 6| Step: 12
Training loss: 2.4688374996185303
Validation loss: 2.4817996653177405

Epoch: 6| Step: 13
Training loss: 3.4844272136688232
Validation loss: 2.480596593631211

Epoch: 108| Step: 0
Training loss: 2.48260498046875
Validation loss: 2.4767515864423526

Epoch: 6| Step: 1
Training loss: 2.1683971881866455
Validation loss: 2.478623508125223

Epoch: 6| Step: 2
Training loss: 2.840012788772583
Validation loss: 2.472794084138768

Epoch: 6| Step: 3
Training loss: 2.6255671977996826
Validation loss: 2.4741996308808685

Epoch: 6| Step: 4
Training loss: 2.065694808959961
Validation loss: 2.472697327213903

Epoch: 6| Step: 5
Training loss: 3.638319253921509
Validation loss: 2.4747609271798083

Epoch: 6| Step: 6
Training loss: 2.993140935897827
Validation loss: 2.471329181425033

Epoch: 6| Step: 7
Training loss: 2.6821022033691406
Validation loss: 2.4706452149216847

Epoch: 6| Step: 8
Training loss: 2.9751322269439697
Validation loss: 2.469112314203734

Epoch: 6| Step: 9
Training loss: 1.7438223361968994
Validation loss: 2.4676088184438725

Epoch: 6| Step: 10
Training loss: 3.003478527069092
Validation loss: 2.4659257242756505

Epoch: 6| Step: 11
Training loss: 2.2744526863098145
Validation loss: 2.4620528400585218

Epoch: 6| Step: 12
Training loss: 2.3909695148468018
Validation loss: 2.4645080950952347

Epoch: 6| Step: 13
Training loss: 4.217755317687988
Validation loss: 2.4556448510898057

Epoch: 109| Step: 0
Training loss: 2.0959174633026123
Validation loss: 2.4526207575234036

Epoch: 6| Step: 1
Training loss: 2.5695314407348633
Validation loss: 2.450915557081981

Epoch: 6| Step: 2
Training loss: 2.8417465686798096
Validation loss: 2.4501988810877644

Epoch: 6| Step: 3
Training loss: 2.812680721282959
Validation loss: 2.448486074324577

Epoch: 6| Step: 4
Training loss: 2.924300193786621
Validation loss: 2.4440117753962034

Epoch: 6| Step: 5
Training loss: 3.681957721710205
Validation loss: 2.4455759345844226

Epoch: 6| Step: 6
Training loss: 2.9966750144958496
Validation loss: 2.4453695435677805

Epoch: 6| Step: 7
Training loss: 1.5275301933288574
Validation loss: 2.448171338727397

Epoch: 6| Step: 8
Training loss: 2.538461446762085
Validation loss: 2.4470795713445193

Epoch: 6| Step: 9
Training loss: 2.63870906829834
Validation loss: 2.446256342754569

Epoch: 6| Step: 10
Training loss: 3.110748291015625
Validation loss: 2.4482469738170667

Epoch: 6| Step: 11
Training loss: 2.4379873275756836
Validation loss: 2.4454753168167604

Epoch: 6| Step: 12
Training loss: 2.5609731674194336
Validation loss: 2.452591444856377

Epoch: 6| Step: 13
Training loss: 2.6949856281280518
Validation loss: 2.4617925510611585

Epoch: 110| Step: 0
Training loss: 2.5289807319641113
Validation loss: 2.4604946208256546

Epoch: 6| Step: 1
Training loss: 3.4523391723632812
Validation loss: 2.4675463220124603

Epoch: 6| Step: 2
Training loss: 2.168978214263916
Validation loss: 2.4836388723824614

Epoch: 6| Step: 3
Training loss: 3.3028922080993652
Validation loss: 2.4861716096119215

Epoch: 6| Step: 4
Training loss: 2.7623677253723145
Validation loss: 2.4864385384385304

Epoch: 6| Step: 5
Training loss: 3.167727470397949
Validation loss: 2.494991376835813

Epoch: 6| Step: 6
Training loss: 2.553023338317871
Validation loss: 2.482147794897838

Epoch: 6| Step: 7
Training loss: 2.916691303253174
Validation loss: 2.4668474171751287

Epoch: 6| Step: 8
Training loss: 2.7374091148376465
Validation loss: 2.4636748785613687

Epoch: 6| Step: 9
Training loss: 1.8067723512649536
Validation loss: 2.4483927552418043

Epoch: 6| Step: 10
Training loss: 2.7414724826812744
Validation loss: 2.446047516279323

Epoch: 6| Step: 11
Training loss: 1.9775607585906982
Validation loss: 2.4409813034919

Epoch: 6| Step: 12
Training loss: 2.8529624938964844
Validation loss: 2.441362693745603

Epoch: 6| Step: 13
Training loss: 2.55855655670166
Validation loss: 2.442210602503951

Epoch: 111| Step: 0
Training loss: 2.317441701889038
Validation loss: 2.4434049539668585

Epoch: 6| Step: 1
Training loss: 3.4619126319885254
Validation loss: 2.445420308779645

Epoch: 6| Step: 2
Training loss: 3.014730930328369
Validation loss: 2.446091982626146

Epoch: 6| Step: 3
Training loss: 2.581491470336914
Validation loss: 2.4433656379740727

Epoch: 6| Step: 4
Training loss: 2.8453845977783203
Validation loss: 2.4446843644624114

Epoch: 6| Step: 5
Training loss: 2.6285173892974854
Validation loss: 2.4440222606864026

Epoch: 6| Step: 6
Training loss: 2.396024703979492
Validation loss: 2.4464403762612292

Epoch: 6| Step: 7
Training loss: 1.597578763961792
Validation loss: 2.4472175054652716

Epoch: 6| Step: 8
Training loss: 2.5799338817596436
Validation loss: 2.444209357743622

Epoch: 6| Step: 9
Training loss: 3.2014496326446533
Validation loss: 2.4468516611283824

Epoch: 6| Step: 10
Training loss: 2.49886417388916
Validation loss: 2.446134067350818

Epoch: 6| Step: 11
Training loss: 2.5371198654174805
Validation loss: 2.4428126017252603

Epoch: 6| Step: 12
Training loss: 2.6776299476623535
Validation loss: 2.4420016247739076

Epoch: 6| Step: 13
Training loss: 3.519803762435913
Validation loss: 2.442552549864656

Epoch: 112| Step: 0
Training loss: 2.1321096420288086
Validation loss: 2.4413862638576056

Epoch: 6| Step: 1
Training loss: 2.7922749519348145
Validation loss: 2.449434498304962

Epoch: 6| Step: 2
Training loss: 2.583780288696289
Validation loss: 2.4493648980253484

Epoch: 6| Step: 3
Training loss: 3.0821080207824707
Validation loss: 2.4475653530448995

Epoch: 6| Step: 4
Training loss: 3.018181800842285
Validation loss: 2.4550232656540407

Epoch: 6| Step: 5
Training loss: 3.1076431274414062
Validation loss: 2.4527161659732943

Epoch: 6| Step: 6
Training loss: 2.9861745834350586
Validation loss: 2.4609874320286576

Epoch: 6| Step: 7
Training loss: 2.5053043365478516
Validation loss: 2.4668275566511255

Epoch: 6| Step: 8
Training loss: 2.845085859298706
Validation loss: 2.4673146227354645

Epoch: 6| Step: 9
Training loss: 2.861100673675537
Validation loss: 2.4660465153314735

Epoch: 6| Step: 10
Training loss: 2.1365532875061035
Validation loss: 2.467509743987873

Epoch: 6| Step: 11
Training loss: 2.015672206878662
Validation loss: 2.4775682700577604

Epoch: 6| Step: 12
Training loss: 2.524681568145752
Validation loss: 2.4756737780827347

Epoch: 6| Step: 13
Training loss: 2.8088200092315674
Validation loss: 2.4750369005305792

Epoch: 113| Step: 0
Training loss: 3.4617462158203125
Validation loss: 2.4843330280755156

Epoch: 6| Step: 1
Training loss: 2.8221304416656494
Validation loss: 2.47691334191189

Epoch: 6| Step: 2
Training loss: 3.341036319732666
Validation loss: 2.4785520543334303

Epoch: 6| Step: 3
Training loss: 2.214003324508667
Validation loss: 2.47502992486441

Epoch: 6| Step: 4
Training loss: 2.9467644691467285
Validation loss: 2.477339462567401

Epoch: 6| Step: 5
Training loss: 3.4993789196014404
Validation loss: 2.471047124555034

Epoch: 6| Step: 6
Training loss: 2.6180825233459473
Validation loss: 2.4677742591468235

Epoch: 6| Step: 7
Training loss: 2.4284701347351074
Validation loss: 2.4677151505665114

Epoch: 6| Step: 8
Training loss: 3.01877498626709
Validation loss: 2.46682579799365

Epoch: 6| Step: 9
Training loss: 1.4103126525878906
Validation loss: 2.4579135628156763

Epoch: 6| Step: 10
Training loss: 1.7571778297424316
Validation loss: 2.455760814810312

Epoch: 6| Step: 11
Training loss: 2.1114189624786377
Validation loss: 2.4544339795266428

Epoch: 6| Step: 12
Training loss: 3.0401041507720947
Validation loss: 2.4529693306133313

Epoch: 6| Step: 13
Training loss: 2.6075143814086914
Validation loss: 2.4566491957633727

Epoch: 114| Step: 0
Training loss: 3.0628483295440674
Validation loss: 2.4505143293770413

Epoch: 6| Step: 1
Training loss: 2.06364369392395
Validation loss: 2.4509922125006236

Epoch: 6| Step: 2
Training loss: 2.1228842735290527
Validation loss: 2.4527118103478545

Epoch: 6| Step: 3
Training loss: 2.135115623474121
Validation loss: 2.4523653778978574

Epoch: 6| Step: 4
Training loss: 2.836845874786377
Validation loss: 2.4544165890703917

Epoch: 6| Step: 5
Training loss: 2.5954225063323975
Validation loss: 2.4542354845231578

Epoch: 6| Step: 6
Training loss: 2.2947678565979004
Validation loss: 2.4495204059026574

Epoch: 6| Step: 7
Training loss: 3.348268508911133
Validation loss: 2.4489586994212162

Epoch: 6| Step: 8
Training loss: 3.0450339317321777
Validation loss: 2.448279993508452

Epoch: 6| Step: 9
Training loss: 2.278397560119629
Validation loss: 2.451326160020726

Epoch: 6| Step: 10
Training loss: 3.043952465057373
Validation loss: 2.4517089577131372

Epoch: 6| Step: 11
Training loss: 2.454087257385254
Validation loss: 2.4545467258781515

Epoch: 6| Step: 12
Training loss: 2.6496877670288086
Validation loss: 2.4476488251839914

Epoch: 6| Step: 13
Training loss: 4.002773761749268
Validation loss: 2.4524296009412376

Epoch: 115| Step: 0
Training loss: 3.0927112102508545
Validation loss: 2.448393593552292

Epoch: 6| Step: 1
Training loss: 3.380354404449463
Validation loss: 2.457236877051733

Epoch: 6| Step: 2
Training loss: 1.8472318649291992
Validation loss: 2.459363809195898

Epoch: 6| Step: 3
Training loss: 2.5862483978271484
Validation loss: 2.464021493029851

Epoch: 6| Step: 4
Training loss: 2.668889045715332
Validation loss: 2.4691871263647593

Epoch: 6| Step: 5
Training loss: 2.3672165870666504
Validation loss: 2.473419266362344

Epoch: 6| Step: 6
Training loss: 3.0627708435058594
Validation loss: 2.470320870799403

Epoch: 6| Step: 7
Training loss: 3.1075611114501953
Validation loss: 2.473583008653374

Epoch: 6| Step: 8
Training loss: 2.675692081451416
Validation loss: 2.4745754913617204

Epoch: 6| Step: 9
Training loss: 2.63289737701416
Validation loss: 2.474369115726922

Epoch: 6| Step: 10
Training loss: 2.1818108558654785
Validation loss: 2.468515434572774

Epoch: 6| Step: 11
Training loss: 2.6519360542297363
Validation loss: 2.469529605680896

Epoch: 6| Step: 12
Training loss: 2.36120343208313
Validation loss: 2.468881291727866

Epoch: 6| Step: 13
Training loss: 2.6578869819641113
Validation loss: 2.46104537158884

Epoch: 116| Step: 0
Training loss: 3.2621517181396484
Validation loss: 2.4631399749427714

Epoch: 6| Step: 1
Training loss: 3.0890142917633057
Validation loss: 2.456611312845702

Epoch: 6| Step: 2
Training loss: 3.0106329917907715
Validation loss: 2.4589343993894515

Epoch: 6| Step: 3
Training loss: 2.4643163681030273
Validation loss: 2.4571058660425167

Epoch: 6| Step: 4
Training loss: 1.9063166379928589
Validation loss: 2.4656396373625724

Epoch: 6| Step: 5
Training loss: 2.9971203804016113
Validation loss: 2.463534475654684

Epoch: 6| Step: 6
Training loss: 2.262498617172241
Validation loss: 2.455156445503235

Epoch: 6| Step: 7
Training loss: 2.033686637878418
Validation loss: 2.457249500418222

Epoch: 6| Step: 8
Training loss: 3.199225902557373
Validation loss: 2.4512891384863083

Epoch: 6| Step: 9
Training loss: 2.56355619430542
Validation loss: 2.454655452441144

Epoch: 6| Step: 10
Training loss: 1.9908623695373535
Validation loss: 2.450986031563051

Epoch: 6| Step: 11
Training loss: 2.9312703609466553
Validation loss: 2.4433524147156747

Epoch: 6| Step: 12
Training loss: 2.3546433448791504
Validation loss: 2.446878333245554

Epoch: 6| Step: 13
Training loss: 3.538353204727173
Validation loss: 2.446105695539905

Epoch: 117| Step: 0
Training loss: 2.619358539581299
Validation loss: 2.444838934047248

Epoch: 6| Step: 1
Training loss: 3.492326498031616
Validation loss: 2.4494206572091706

Epoch: 6| Step: 2
Training loss: 3.547928810119629
Validation loss: 2.4446513011891353

Epoch: 6| Step: 3
Training loss: 2.272918701171875
Validation loss: 2.4424788490418465

Epoch: 6| Step: 4
Training loss: 1.6478092670440674
Validation loss: 2.4397944019686792

Epoch: 6| Step: 5
Training loss: 2.3804001808166504
Validation loss: 2.4437889924613376

Epoch: 6| Step: 6
Training loss: 2.6163618564605713
Validation loss: 2.4502904645858274

Epoch: 6| Step: 7
Training loss: 2.9415040016174316
Validation loss: 2.449503747365808

Epoch: 6| Step: 8
Training loss: 2.3095057010650635
Validation loss: 2.450116148558996

Epoch: 6| Step: 9
Training loss: 2.1034493446350098
Validation loss: 2.4553187175463607

Epoch: 6| Step: 10
Training loss: 2.8895254135131836
Validation loss: 2.4604776931065384

Epoch: 6| Step: 11
Training loss: 2.6323769092559814
Validation loss: 2.4660315462338027

Epoch: 6| Step: 12
Training loss: 3.107988119125366
Validation loss: 2.470018812405166

Epoch: 6| Step: 13
Training loss: 2.630397081375122
Validation loss: 2.473110096428984

Epoch: 118| Step: 0
Training loss: 3.449963092803955
Validation loss: 2.4675576866313977

Epoch: 6| Step: 1
Training loss: 3.4254655838012695
Validation loss: 2.4707245057629

Epoch: 6| Step: 2
Training loss: 2.2535290718078613
Validation loss: 2.469884841672836

Epoch: 6| Step: 3
Training loss: 2.9388084411621094
Validation loss: 2.4676173374217045

Epoch: 6| Step: 4
Training loss: 2.167114496231079
Validation loss: 2.478562762660365

Epoch: 6| Step: 5
Training loss: 2.418672561645508
Validation loss: 2.475106913556335

Epoch: 6| Step: 6
Training loss: 2.762807846069336
Validation loss: 2.47733610932545

Epoch: 6| Step: 7
Training loss: 2.211594820022583
Validation loss: 2.4712606091653146

Epoch: 6| Step: 8
Training loss: 1.879349708557129
Validation loss: 2.4779309534257457

Epoch: 6| Step: 9
Training loss: 3.3677732944488525
Validation loss: 2.47430694231423

Epoch: 6| Step: 10
Training loss: 2.524564266204834
Validation loss: 2.4656054614692606

Epoch: 6| Step: 11
Training loss: 2.835120677947998
Validation loss: 2.4683906083465903

Epoch: 6| Step: 12
Training loss: 3.036914825439453
Validation loss: 2.465256160305392

Epoch: 6| Step: 13
Training loss: 1.5428922176361084
Validation loss: 2.4604668822339786

Epoch: 119| Step: 0
Training loss: 2.8001489639282227
Validation loss: 2.4618751413078717

Epoch: 6| Step: 1
Training loss: 3.1682419776916504
Validation loss: 2.4528348932984056

Epoch: 6| Step: 2
Training loss: 2.7059733867645264
Validation loss: 2.463404575983683

Epoch: 6| Step: 3
Training loss: 2.3491854667663574
Validation loss: 2.462575553565897

Epoch: 6| Step: 4
Training loss: 2.071392059326172
Validation loss: 2.4530527360977663

Epoch: 6| Step: 5
Training loss: 2.88850736618042
Validation loss: 2.4481464175767798

Epoch: 6| Step: 6
Training loss: 2.4501917362213135
Validation loss: 2.4494230798495713

Epoch: 6| Step: 7
Training loss: 3.100860118865967
Validation loss: 2.4455279663044918

Epoch: 6| Step: 8
Training loss: 2.5447330474853516
Validation loss: 2.449724752415893

Epoch: 6| Step: 9
Training loss: 3.042060375213623
Validation loss: 2.4424946077408327

Epoch: 6| Step: 10
Training loss: 2.0062832832336426
Validation loss: 2.4451111439735658

Epoch: 6| Step: 11
Training loss: 1.8597450256347656
Validation loss: 2.4466611544291177

Epoch: 6| Step: 12
Training loss: 2.785980224609375
Validation loss: 2.4499159371981056

Epoch: 6| Step: 13
Training loss: 3.8987061977386475
Validation loss: 2.455080355367353

Epoch: 120| Step: 0
Training loss: 2.491079568862915
Validation loss: 2.4502704835707143

Epoch: 6| Step: 1
Training loss: 3.157597541809082
Validation loss: 2.454972290223645

Epoch: 6| Step: 2
Training loss: 2.528916597366333
Validation loss: 2.4554661576465895

Epoch: 6| Step: 3
Training loss: 3.2606170177459717
Validation loss: 2.4701336276146675

Epoch: 6| Step: 4
Training loss: 3.4177441596984863
Validation loss: 2.472521020520118

Epoch: 6| Step: 5
Training loss: 2.340290069580078
Validation loss: 2.464495994711435

Epoch: 6| Step: 6
Training loss: 2.2964816093444824
Validation loss: 2.464088022067983

Epoch: 6| Step: 7
Training loss: 2.081003189086914
Validation loss: 2.4596091919047858

Epoch: 6| Step: 8
Training loss: 2.968477249145508
Validation loss: 2.455211900895642

Epoch: 6| Step: 9
Training loss: 1.88575279712677
Validation loss: 2.450459608467676

Epoch: 6| Step: 10
Training loss: 1.9871041774749756
Validation loss: 2.44286318748228

Epoch: 6| Step: 11
Training loss: 2.889948844909668
Validation loss: 2.450052204952445

Epoch: 6| Step: 12
Training loss: 2.856480836868286
Validation loss: 2.445716555400561

Epoch: 6| Step: 13
Training loss: 3.416213035583496
Validation loss: 2.4494217826474096

Epoch: 121| Step: 0
Training loss: 2.2910962104797363
Validation loss: 2.446323679339501

Epoch: 6| Step: 1
Training loss: 2.555103302001953
Validation loss: 2.4425923029581704

Epoch: 6| Step: 2
Training loss: 2.9316468238830566
Validation loss: 2.4401315335304505

Epoch: 6| Step: 3
Training loss: 2.54811429977417
Validation loss: 2.43354465628183

Epoch: 6| Step: 4
Training loss: 3.355947971343994
Validation loss: 2.437447553039879

Epoch: 6| Step: 5
Training loss: 2.1156814098358154
Validation loss: 2.4312150273271786

Epoch: 6| Step: 6
Training loss: 3.243589401245117
Validation loss: 2.4306702152375252

Epoch: 6| Step: 7
Training loss: 2.2957515716552734
Validation loss: 2.4332044534785773

Epoch: 6| Step: 8
Training loss: 1.5491865873336792
Validation loss: 2.436368150095786

Epoch: 6| Step: 9
Training loss: 3.0153682231903076
Validation loss: 2.4407584154477684

Epoch: 6| Step: 10
Training loss: 2.7046940326690674
Validation loss: 2.4372942293843916

Epoch: 6| Step: 11
Training loss: 2.815573215484619
Validation loss: 2.4470176748050156

Epoch: 6| Step: 12
Training loss: 2.954801321029663
Validation loss: 2.4501456547808904

Epoch: 6| Step: 13
Training loss: 2.9145395755767822
Validation loss: 2.4542921845630934

Epoch: 122| Step: 0
Training loss: 1.7564013004302979
Validation loss: 2.4534857196192585

Epoch: 6| Step: 1
Training loss: 2.049021005630493
Validation loss: 2.4571008887342227

Epoch: 6| Step: 2
Training loss: 1.8275623321533203
Validation loss: 2.4563618270299767

Epoch: 6| Step: 3
Training loss: 2.4946510791778564
Validation loss: 2.455856905188612

Epoch: 6| Step: 4
Training loss: 2.6675100326538086
Validation loss: 2.456055438646706

Epoch: 6| Step: 5
Training loss: 2.309223175048828
Validation loss: 2.468250859168268

Epoch: 6| Step: 6
Training loss: 2.0688424110412598
Validation loss: 2.463998520246116

Epoch: 6| Step: 7
Training loss: 3.9245526790618896
Validation loss: 2.4750072904812392

Epoch: 6| Step: 8
Training loss: 2.4526498317718506
Validation loss: 2.4722251174270466

Epoch: 6| Step: 9
Training loss: 2.826427459716797
Validation loss: 2.480543513451853

Epoch: 6| Step: 10
Training loss: 3.452637195587158
Validation loss: 2.4817142281481015

Epoch: 6| Step: 11
Training loss: 3.7839534282684326
Validation loss: 2.4882719696208997

Epoch: 6| Step: 12
Training loss: 2.771263837814331
Validation loss: 2.46187908675081

Epoch: 6| Step: 13
Training loss: 3.151245355606079
Validation loss: 2.462497857309157

Epoch: 123| Step: 0
Training loss: 3.4776644706726074
Validation loss: 2.4580121642799786

Epoch: 6| Step: 1
Training loss: 3.037522315979004
Validation loss: 2.461596535098168

Epoch: 6| Step: 2
Training loss: 2.368957042694092
Validation loss: 2.453932016126571

Epoch: 6| Step: 3
Training loss: 2.718736171722412
Validation loss: 2.462143765982761

Epoch: 6| Step: 4
Training loss: 2.876904249191284
Validation loss: 2.459729399732364

Epoch: 6| Step: 5
Training loss: 2.6913576126098633
Validation loss: 2.4577233534987255

Epoch: 6| Step: 6
Training loss: 2.675584316253662
Validation loss: 2.4648085742868404

Epoch: 6| Step: 7
Training loss: 2.380791187286377
Validation loss: 2.4525337501238753

Epoch: 6| Step: 8
Training loss: 2.184037923812866
Validation loss: 2.4529764831707044

Epoch: 6| Step: 9
Training loss: 2.3358922004699707
Validation loss: 2.4462171293074086

Epoch: 6| Step: 10
Training loss: 2.6712217330932617
Validation loss: 2.441826771664363

Epoch: 6| Step: 11
Training loss: 2.7950546741485596
Validation loss: 2.4484500321008826

Epoch: 6| Step: 12
Training loss: 2.616121292114258
Validation loss: 2.4412989462575605

Epoch: 6| Step: 13
Training loss: 1.9681146144866943
Validation loss: 2.4387131173123597

Epoch: 124| Step: 0
Training loss: 3.035658836364746
Validation loss: 2.445282681013948

Epoch: 6| Step: 1
Training loss: 2.7924728393554688
Validation loss: 2.4413243442453365

Epoch: 6| Step: 2
Training loss: 2.2732698917388916
Validation loss: 2.4422433094311784

Epoch: 6| Step: 3
Training loss: 2.4541783332824707
Validation loss: 2.445201817379203

Epoch: 6| Step: 4
Training loss: 2.6132559776306152
Validation loss: 2.4508664300364833

Epoch: 6| Step: 5
Training loss: 3.1031556129455566
Validation loss: 2.4471296277097476

Epoch: 6| Step: 6
Training loss: 2.4827980995178223
Validation loss: 2.4549133136708248

Epoch: 6| Step: 7
Training loss: 2.732396125793457
Validation loss: 2.4550656221246205

Epoch: 6| Step: 8
Training loss: 2.3883004188537598
Validation loss: 2.4528835563249487

Epoch: 6| Step: 9
Training loss: 2.713702440261841
Validation loss: 2.4517103266972367

Epoch: 6| Step: 10
Training loss: 2.5838892459869385
Validation loss: 2.446678758949362

Epoch: 6| Step: 11
Training loss: 3.235854387283325
Validation loss: 2.4541892774643435

Epoch: 6| Step: 12
Training loss: 2.632873058319092
Validation loss: 2.4504369228116927

Epoch: 6| Step: 13
Training loss: 1.5131410360336304
Validation loss: 2.4524837283677954

Epoch: 125| Step: 0
Training loss: 2.9382336139678955
Validation loss: 2.4535418197672856

Epoch: 6| Step: 1
Training loss: 2.8172664642333984
Validation loss: 2.4491878991485923

Epoch: 6| Step: 2
Training loss: 2.8863492012023926
Validation loss: 2.456163457644883

Epoch: 6| Step: 3
Training loss: 2.1287224292755127
Validation loss: 2.458518853751562

Epoch: 6| Step: 4
Training loss: 2.0772786140441895
Validation loss: 2.463027374718779

Epoch: 6| Step: 5
Training loss: 3.284923553466797
Validation loss: 2.4612250610064437

Epoch: 6| Step: 6
Training loss: 1.9145349264144897
Validation loss: 2.4601274433956353

Epoch: 6| Step: 7
Training loss: 2.1065385341644287
Validation loss: 2.454381032656598

Epoch: 6| Step: 8
Training loss: 3.276477813720703
Validation loss: 2.4487180709838867

Epoch: 6| Step: 9
Training loss: 3.5429725646972656
Validation loss: 2.4462769185343096

Epoch: 6| Step: 10
Training loss: 2.0309650897979736
Validation loss: 2.4428320777031685

Epoch: 6| Step: 11
Training loss: 3.1079938411712646
Validation loss: 2.43679214549321

Epoch: 6| Step: 12
Training loss: 2.0914993286132812
Validation loss: 2.4408113879542195

Epoch: 6| Step: 13
Training loss: 3.00566029548645
Validation loss: 2.4360533298984652

Epoch: 126| Step: 0
Training loss: 2.549461603164673
Validation loss: 2.4371105394055768

Epoch: 6| Step: 1
Training loss: 2.6657094955444336
Validation loss: 2.438900457915439

Epoch: 6| Step: 2
Training loss: 2.877387285232544
Validation loss: 2.435047508567892

Epoch: 6| Step: 3
Training loss: 2.9106829166412354
Validation loss: 2.4329408214938257

Epoch: 6| Step: 4
Training loss: 2.5317816734313965
Validation loss: 2.434595448996431

Epoch: 6| Step: 5
Training loss: 2.4981236457824707
Validation loss: 2.435824268607683

Epoch: 6| Step: 6
Training loss: 2.4880380630493164
Validation loss: 2.434339613042852

Epoch: 6| Step: 7
Training loss: 2.3717095851898193
Validation loss: 2.4374866331777265

Epoch: 6| Step: 8
Training loss: 3.0672950744628906
Validation loss: 2.4420427404424196

Epoch: 6| Step: 9
Training loss: 2.8324809074401855
Validation loss: 2.4487811057798323

Epoch: 6| Step: 10
Training loss: 2.380387306213379
Validation loss: 2.445086422786918

Epoch: 6| Step: 11
Training loss: 2.6544952392578125
Validation loss: 2.4523755658057427

Epoch: 6| Step: 12
Training loss: 2.551680088043213
Validation loss: 2.4484849027408067

Epoch: 6| Step: 13
Training loss: 2.928086280822754
Validation loss: 2.4426606150083643

Epoch: 127| Step: 0
Training loss: 2.6059932708740234
Validation loss: 2.4422831894249044

Epoch: 6| Step: 1
Training loss: 2.377582550048828
Validation loss: 2.4403595616740565

Epoch: 6| Step: 2
Training loss: 2.1320407390594482
Validation loss: 2.441091363148023

Epoch: 6| Step: 3
Training loss: 2.0170741081237793
Validation loss: 2.4420081594938874

Epoch: 6| Step: 4
Training loss: 2.714214324951172
Validation loss: 2.44508635100498

Epoch: 6| Step: 5
Training loss: 2.7774741649627686
Validation loss: 2.448601761171895

Epoch: 6| Step: 6
Training loss: 2.3622078895568848
Validation loss: 2.4444379601427304

Epoch: 6| Step: 7
Training loss: 3.061520576477051
Validation loss: 2.4462567273006646

Epoch: 6| Step: 8
Training loss: 3.216691493988037
Validation loss: 2.4441448462906705

Epoch: 6| Step: 9
Training loss: 2.5042035579681396
Validation loss: 2.4391195286986647

Epoch: 6| Step: 10
Training loss: 3.47062349319458
Validation loss: 2.4435215611611643

Epoch: 6| Step: 11
Training loss: 2.3303475379943848
Validation loss: 2.4445606688017487

Epoch: 6| Step: 12
Training loss: 2.905357837677002
Validation loss: 2.435244773023872

Epoch: 6| Step: 13
Training loss: 2.4407734870910645
Validation loss: 2.4425786669536302

Epoch: 128| Step: 0
Training loss: 3.323106050491333
Validation loss: 2.4430564193315405

Epoch: 6| Step: 1
Training loss: 3.2883899211883545
Validation loss: 2.4481207196430494

Epoch: 6| Step: 2
Training loss: 2.0571696758270264
Validation loss: 2.4426824995266494

Epoch: 6| Step: 3
Training loss: 2.818526268005371
Validation loss: 2.4445299256232476

Epoch: 6| Step: 4
Training loss: 2.7887802124023438
Validation loss: 2.446350271983813

Epoch: 6| Step: 5
Training loss: 2.5302281379699707
Validation loss: 2.443695475978236

Epoch: 6| Step: 6
Training loss: 2.384230852127075
Validation loss: 2.448429871630925

Epoch: 6| Step: 7
Training loss: 2.438765525817871
Validation loss: 2.453673598586872

Epoch: 6| Step: 8
Training loss: 2.3994131088256836
Validation loss: 2.454473469846992

Epoch: 6| Step: 9
Training loss: 2.399782180786133
Validation loss: 2.446514583403064

Epoch: 6| Step: 10
Training loss: 1.835378885269165
Validation loss: 2.4509464771516862

Epoch: 6| Step: 11
Training loss: 3.277606964111328
Validation loss: 2.447887105326499

Epoch: 6| Step: 12
Training loss: 2.5471858978271484
Validation loss: 2.4497636646352787

Epoch: 6| Step: 13
Training loss: 3.0795793533325195
Validation loss: 2.4502225204180648

Epoch: 129| Step: 0
Training loss: 2.6785616874694824
Validation loss: 2.4420338369184926

Epoch: 6| Step: 1
Training loss: 2.331224203109741
Validation loss: 2.4432652509340675

Epoch: 6| Step: 2
Training loss: 2.8202333450317383
Validation loss: 2.4443851183819514

Epoch: 6| Step: 3
Training loss: 3.1129891872406006
Validation loss: 2.4388560582232732

Epoch: 6| Step: 4
Training loss: 3.017892837524414
Validation loss: 2.4472835448480423

Epoch: 6| Step: 5
Training loss: 2.2725062370300293
Validation loss: 2.4453450402905865

Epoch: 6| Step: 6
Training loss: 2.9644837379455566
Validation loss: 2.444354006039199

Epoch: 6| Step: 7
Training loss: 2.494251251220703
Validation loss: 2.436784634026148

Epoch: 6| Step: 8
Training loss: 2.9904024600982666
Validation loss: 2.443314136997346

Epoch: 6| Step: 9
Training loss: 2.281716823577881
Validation loss: 2.441673776154877

Epoch: 6| Step: 10
Training loss: 2.1809897422790527
Validation loss: 2.438921123422602

Epoch: 6| Step: 11
Training loss: 2.9742431640625
Validation loss: 2.4462810203593266

Epoch: 6| Step: 12
Training loss: 2.7285056114196777
Validation loss: 2.446110351111299

Epoch: 6| Step: 13
Training loss: 1.8276689052581787
Validation loss: 2.4493947618751117

Epoch: 130| Step: 0
Training loss: 2.560849666595459
Validation loss: 2.447619835535685

Epoch: 6| Step: 1
Training loss: 2.6036696434020996
Validation loss: 2.4571337699890137

Epoch: 6| Step: 2
Training loss: 3.0027105808258057
Validation loss: 2.463701596824072

Epoch: 6| Step: 3
Training loss: 1.8776907920837402
Validation loss: 2.4633143999243297

Epoch: 6| Step: 4
Training loss: 2.9411869049072266
Validation loss: 2.459221419467721

Epoch: 6| Step: 5
Training loss: 2.361039876937866
Validation loss: 2.461147892859674

Epoch: 6| Step: 6
Training loss: 2.9579386711120605
Validation loss: 2.461389938990275

Epoch: 6| Step: 7
Training loss: 2.228358030319214
Validation loss: 2.4563151764613327

Epoch: 6| Step: 8
Training loss: 3.104154348373413
Validation loss: 2.457009675682232

Epoch: 6| Step: 9
Training loss: 2.8114850521087646
Validation loss: 2.459611238971833

Epoch: 6| Step: 10
Training loss: 2.7080748081207275
Validation loss: 2.4441031717484996

Epoch: 6| Step: 11
Training loss: 2.834289312362671
Validation loss: 2.455432762381851

Epoch: 6| Step: 12
Training loss: 2.3690195083618164
Validation loss: 2.456063647424021

Epoch: 6| Step: 13
Training loss: 2.777876377105713
Validation loss: 2.460247201304282

Epoch: 131| Step: 0
Training loss: 2.7573318481445312
Validation loss: 2.4499285451827513

Epoch: 6| Step: 1
Training loss: 2.123663902282715
Validation loss: 2.44576709757569

Epoch: 6| Step: 2
Training loss: 2.737908363342285
Validation loss: 2.437881632517743

Epoch: 6| Step: 3
Training loss: 2.414236307144165
Validation loss: 2.432894799017137

Epoch: 6| Step: 4
Training loss: 1.3291749954223633
Validation loss: 2.4255518605632167

Epoch: 6| Step: 5
Training loss: 3.381728410720825
Validation loss: 2.4242833173403175

Epoch: 6| Step: 6
Training loss: 2.4043893814086914
Validation loss: 2.4312218568658315

Epoch: 6| Step: 7
Training loss: 3.203282117843628
Validation loss: 2.4398402219177573

Epoch: 6| Step: 8
Training loss: 2.938572406768799
Validation loss: 2.4568172577888734

Epoch: 6| Step: 9
Training loss: 2.9551405906677246
Validation loss: 2.461496630022603

Epoch: 6| Step: 10
Training loss: 3.165351390838623
Validation loss: 2.4671285203708115

Epoch: 6| Step: 11
Training loss: 1.754421591758728
Validation loss: 2.461130875413136

Epoch: 6| Step: 12
Training loss: 3.293259859085083
Validation loss: 2.4652961248992593

Epoch: 6| Step: 13
Training loss: 2.669680595397949
Validation loss: 2.4660824063003703

Epoch: 132| Step: 0
Training loss: 2.683192491531372
Validation loss: 2.452373384147562

Epoch: 6| Step: 1
Training loss: 1.7273141145706177
Validation loss: 2.447261728266234

Epoch: 6| Step: 2
Training loss: 2.4907631874084473
Validation loss: 2.444974399382068

Epoch: 6| Step: 3
Training loss: 2.198458671569824
Validation loss: 2.4412439946205384

Epoch: 6| Step: 4
Training loss: 3.1683201789855957
Validation loss: 2.446606123319236

Epoch: 6| Step: 5
Training loss: 2.9891538619995117
Validation loss: 2.443566817109303

Epoch: 6| Step: 6
Training loss: 2.3520913124084473
Validation loss: 2.440405512368807

Epoch: 6| Step: 7
Training loss: 3.025724411010742
Validation loss: 2.4415500164031982

Epoch: 6| Step: 8
Training loss: 1.9917044639587402
Validation loss: 2.4337385495503745

Epoch: 6| Step: 9
Training loss: 2.8137898445129395
Validation loss: 2.4357378713546263

Epoch: 6| Step: 10
Training loss: 2.9617910385131836
Validation loss: 2.4285046669744674

Epoch: 6| Step: 11
Training loss: 2.900069236755371
Validation loss: 2.4284589213709675

Epoch: 6| Step: 12
Training loss: 2.8651700019836426
Validation loss: 2.4285619258880615

Epoch: 6| Step: 13
Training loss: 2.9852380752563477
Validation loss: 2.42670262757168

Epoch: 133| Step: 0
Training loss: 2.9546022415161133
Validation loss: 2.424623150979319

Epoch: 6| Step: 1
Training loss: 2.923659563064575
Validation loss: 2.429989842958348

Epoch: 6| Step: 2
Training loss: 2.6420867443084717
Validation loss: 2.4205782439119075

Epoch: 6| Step: 3
Training loss: 2.4462878704071045
Validation loss: 2.4221757535011537

Epoch: 6| Step: 4
Training loss: 2.2087230682373047
Validation loss: 2.4161182731710453

Epoch: 6| Step: 5
Training loss: 2.3996641635894775
Validation loss: 2.4166996504670832

Epoch: 6| Step: 6
Training loss: 2.0473241806030273
Validation loss: 2.414323978526618

Epoch: 6| Step: 7
Training loss: 2.869809627532959
Validation loss: 2.4220403368755052

Epoch: 6| Step: 8
Training loss: 2.987316608428955
Validation loss: 2.4188089947546683

Epoch: 6| Step: 9
Training loss: 3.203458786010742
Validation loss: 2.4255694702107418

Epoch: 6| Step: 10
Training loss: 2.1725220680236816
Validation loss: 2.4219108114960375

Epoch: 6| Step: 11
Training loss: 3.1361379623413086
Validation loss: 2.420408518083634

Epoch: 6| Step: 12
Training loss: 2.661895751953125
Validation loss: 2.4204603343881588

Epoch: 6| Step: 13
Training loss: 2.1152970790863037
Validation loss: 2.4193654239818616

Epoch: 134| Step: 0
Training loss: 3.508030652999878
Validation loss: 2.4172307111883677

Epoch: 6| Step: 1
Training loss: 3.0819759368896484
Validation loss: 2.4166911084164857

Epoch: 6| Step: 2
Training loss: 2.8393406867980957
Validation loss: 2.411956294890373

Epoch: 6| Step: 3
Training loss: 2.873419761657715
Validation loss: 2.4087822027103876

Epoch: 6| Step: 4
Training loss: 2.036313533782959
Validation loss: 2.4070424238840737

Epoch: 6| Step: 5
Training loss: 2.130953073501587
Validation loss: 2.408840763953424

Epoch: 6| Step: 6
Training loss: 2.4589366912841797
Validation loss: 2.4070489893677416

Epoch: 6| Step: 7
Training loss: 2.8028485774993896
Validation loss: 2.411983210553405

Epoch: 6| Step: 8
Training loss: 2.697694778442383
Validation loss: 2.404737949371338

Epoch: 6| Step: 9
Training loss: 2.262770652770996
Validation loss: 2.407325324191842

Epoch: 6| Step: 10
Training loss: 2.039107084274292
Validation loss: 2.4043718127794165

Epoch: 6| Step: 11
Training loss: 3.109182834625244
Validation loss: 2.4034968729942077

Epoch: 6| Step: 12
Training loss: 2.7290453910827637
Validation loss: 2.406033164711409

Epoch: 6| Step: 13
Training loss: 2.2654833793640137
Validation loss: 2.402529230681799

Epoch: 135| Step: 0
Training loss: 2.045309066772461
Validation loss: 2.4081720049663256

Epoch: 6| Step: 1
Training loss: 3.028512477874756
Validation loss: 2.4084201512798185

Epoch: 6| Step: 2
Training loss: 2.4488167762756348
Validation loss: 2.4109422545279227

Epoch: 6| Step: 3
Training loss: 3.29675030708313
Validation loss: 2.40479871021804

Epoch: 6| Step: 4
Training loss: 1.683831810951233
Validation loss: 2.407213641751197

Epoch: 6| Step: 5
Training loss: 2.3094308376312256
Validation loss: 2.406569855187529

Epoch: 6| Step: 6
Training loss: 2.539473533630371
Validation loss: 2.4075699108903126

Epoch: 6| Step: 7
Training loss: 3.323153018951416
Validation loss: 2.408284928209038

Epoch: 6| Step: 8
Training loss: 2.986645460128784
Validation loss: 2.4117840515669955

Epoch: 6| Step: 9
Training loss: 2.5773890018463135
Validation loss: 2.41286737175398

Epoch: 6| Step: 10
Training loss: 2.265862464904785
Validation loss: 2.4147221785719677

Epoch: 6| Step: 11
Training loss: 2.4688639640808105
Validation loss: 2.4158016456070768

Epoch: 6| Step: 12
Training loss: 3.166973829269409
Validation loss: 2.4099352744317826

Epoch: 6| Step: 13
Training loss: 2.947981357574463
Validation loss: 2.412022973901482

Epoch: 136| Step: 0
Training loss: 3.0441017150878906
Validation loss: 2.415101238476333

Epoch: 6| Step: 1
Training loss: 3.0701327323913574
Validation loss: 2.4157261797176894

Epoch: 6| Step: 2
Training loss: 2.967789649963379
Validation loss: 2.4118639294819166

Epoch: 6| Step: 3
Training loss: 2.6456351280212402
Validation loss: 2.4132402507207726

Epoch: 6| Step: 4
Training loss: 1.9640687704086304
Validation loss: 2.414424414275795

Epoch: 6| Step: 5
Training loss: 2.739349842071533
Validation loss: 2.4160800390346076

Epoch: 6| Step: 6
Training loss: 2.111032247543335
Validation loss: 2.418025811513265

Epoch: 6| Step: 7
Training loss: 2.922356605529785
Validation loss: 2.4261961162731214

Epoch: 6| Step: 8
Training loss: 3.2105774879455566
Validation loss: 2.429979073104038

Epoch: 6| Step: 9
Training loss: 2.4493637084960938
Validation loss: 2.430691942091911

Epoch: 6| Step: 10
Training loss: 2.2131009101867676
Validation loss: 2.431670868268577

Epoch: 6| Step: 11
Training loss: 2.3414182662963867
Validation loss: 2.432715677445935

Epoch: 6| Step: 12
Training loss: 2.5308008193969727
Validation loss: 2.440370685310774

Epoch: 6| Step: 13
Training loss: 2.710204601287842
Validation loss: 2.440434989108834

Epoch: 137| Step: 0
Training loss: 2.558354377746582
Validation loss: 2.44092833611273

Epoch: 6| Step: 1
Training loss: 3.005155086517334
Validation loss: 2.4528877119864188

Epoch: 6| Step: 2
Training loss: 2.309959888458252
Validation loss: 2.446725073681083

Epoch: 6| Step: 3
Training loss: 2.7071704864501953
Validation loss: 2.4464573091076267

Epoch: 6| Step: 4
Training loss: 2.320744037628174
Validation loss: 2.4421164335743075

Epoch: 6| Step: 5
Training loss: 2.781463384628296
Validation loss: 2.43399211155471

Epoch: 6| Step: 6
Training loss: 2.7335400581359863
Validation loss: 2.4384125996661443

Epoch: 6| Step: 7
Training loss: 2.5827932357788086
Validation loss: 2.438748921117475

Epoch: 6| Step: 8
Training loss: 2.482034206390381
Validation loss: 2.4391478800004527

Epoch: 6| Step: 9
Training loss: 2.9070522785186768
Validation loss: 2.43875285374221

Epoch: 6| Step: 10
Training loss: 2.9328136444091797
Validation loss: 2.434479018693329

Epoch: 6| Step: 11
Training loss: 2.5678458213806152
Validation loss: 2.437897769353723

Epoch: 6| Step: 12
Training loss: 2.342771053314209
Validation loss: 2.4386914596762708

Epoch: 6| Step: 13
Training loss: 2.643131732940674
Validation loss: 2.4339925691645634

Epoch: 138| Step: 0
Training loss: 1.7322907447814941
Validation loss: 2.4292872746785483

Epoch: 6| Step: 1
Training loss: 2.2587127685546875
Validation loss: 2.4276968202283307

Epoch: 6| Step: 2
Training loss: 2.2972915172576904
Validation loss: 2.433630597206854

Epoch: 6| Step: 3
Training loss: 2.5382795333862305
Validation loss: 2.4336044711451374

Epoch: 6| Step: 4
Training loss: 2.655590057373047
Validation loss: 2.429977842556533

Epoch: 6| Step: 5
Training loss: 2.544658660888672
Validation loss: 2.431393605406566

Epoch: 6| Step: 6
Training loss: 2.3983404636383057
Validation loss: 2.437176812079645

Epoch: 6| Step: 7
Training loss: 3.156801462173462
Validation loss: 2.4347554740085395

Epoch: 6| Step: 8
Training loss: 3.521825075149536
Validation loss: 2.4326858597417034

Epoch: 6| Step: 9
Training loss: 2.8839354515075684
Validation loss: 2.4336414106430544

Epoch: 6| Step: 10
Training loss: 2.3555026054382324
Validation loss: 2.4280197056390906

Epoch: 6| Step: 11
Training loss: 3.213768482208252
Validation loss: 2.4341872046070714

Epoch: 6| Step: 12
Training loss: 2.7633750438690186
Validation loss: 2.430969456190704

Epoch: 6| Step: 13
Training loss: 2.3949756622314453
Validation loss: 2.4312827984491983

Epoch: 139| Step: 0
Training loss: 2.979188919067383
Validation loss: 2.4260430720544632

Epoch: 6| Step: 1
Training loss: 2.915092706680298
Validation loss: 2.4258714414411977

Epoch: 6| Step: 2
Training loss: 2.70827054977417
Validation loss: 2.4285553040043

Epoch: 6| Step: 3
Training loss: 2.8405561447143555
Validation loss: 2.429252414293187

Epoch: 6| Step: 4
Training loss: 3.039259433746338
Validation loss: 2.431672369280169

Epoch: 6| Step: 5
Training loss: 2.5139477252960205
Validation loss: 2.4335357758306686

Epoch: 6| Step: 6
Training loss: 3.3201069831848145
Validation loss: 2.4332194610308577

Epoch: 6| Step: 7
Training loss: 2.5002026557922363
Validation loss: 2.439111835213118

Epoch: 6| Step: 8
Training loss: 2.1593620777130127
Validation loss: 2.434117712000365

Epoch: 6| Step: 9
Training loss: 1.970445156097412
Validation loss: 2.431769924779092

Epoch: 6| Step: 10
Training loss: 2.5240447521209717
Validation loss: 2.435596489137219

Epoch: 6| Step: 11
Training loss: 2.1287636756896973
Validation loss: 2.4251995343033985

Epoch: 6| Step: 12
Training loss: 3.0376243591308594
Validation loss: 2.4268455351552656

Epoch: 6| Step: 13
Training loss: 2.0237531661987305
Validation loss: 2.417712883282733

Epoch: 140| Step: 0
Training loss: 2.467902183532715
Validation loss: 2.4153036609772713

Epoch: 6| Step: 1
Training loss: 2.211970329284668
Validation loss: 2.4089111102524625

Epoch: 6| Step: 2
Training loss: 2.1546859741210938
Validation loss: 2.4083224060714885

Epoch: 6| Step: 3
Training loss: 2.89752197265625
Validation loss: 2.406070047809232

Epoch: 6| Step: 4
Training loss: 3.2270588874816895
Validation loss: 2.4093227719747894

Epoch: 6| Step: 5
Training loss: 2.6300525665283203
Validation loss: 2.4094894355343235

Epoch: 6| Step: 6
Training loss: 2.455047369003296
Validation loss: 2.410089159524569

Epoch: 6| Step: 7
Training loss: 2.893740177154541
Validation loss: 2.4114362193692114

Epoch: 6| Step: 8
Training loss: 2.428222894668579
Validation loss: 2.415869443647323

Epoch: 6| Step: 9
Training loss: 1.9230587482452393
Validation loss: 2.4196940416930826

Epoch: 6| Step: 10
Training loss: 3.142524003982544
Validation loss: 2.4164882526602796

Epoch: 6| Step: 11
Training loss: 2.9137487411499023
Validation loss: 2.419448350065498

Epoch: 6| Step: 12
Training loss: 2.7172205448150635
Validation loss: 2.4146048830401514

Epoch: 6| Step: 13
Training loss: 2.8988723754882812
Validation loss: 2.413926097654527

Epoch: 141| Step: 0
Training loss: 2.6736972332000732
Validation loss: 2.4091737552355696

Epoch: 6| Step: 1
Training loss: 3.049926280975342
Validation loss: 2.4100906515634186

Epoch: 6| Step: 2
Training loss: 2.474031925201416
Validation loss: 2.412038492900069

Epoch: 6| Step: 3
Training loss: 2.62874698638916
Validation loss: 2.4045524520258748

Epoch: 6| Step: 4
Training loss: 1.70805025100708
Validation loss: 2.4095402276644142

Epoch: 6| Step: 5
Training loss: 2.8740878105163574
Validation loss: 2.416544096444243

Epoch: 6| Step: 6
Training loss: 2.1898245811462402
Validation loss: 2.41258301273469

Epoch: 6| Step: 7
Training loss: 2.8417439460754395
Validation loss: 2.4191429102292625

Epoch: 6| Step: 8
Training loss: 2.470438003540039
Validation loss: 2.426018473922565

Epoch: 6| Step: 9
Training loss: 2.6161108016967773
Validation loss: 2.4250788150295133

Epoch: 6| Step: 10
Training loss: 2.9187912940979004
Validation loss: 2.421724896277151

Epoch: 6| Step: 11
Training loss: 2.128504991531372
Validation loss: 2.417706492126629

Epoch: 6| Step: 12
Training loss: 3.0669403076171875
Validation loss: 2.423140223308276

Epoch: 6| Step: 13
Training loss: 3.4900758266448975
Validation loss: 2.419066274037925

Epoch: 142| Step: 0
Training loss: 2.5655388832092285
Validation loss: 2.4252387092959498

Epoch: 6| Step: 1
Training loss: 2.2820627689361572
Validation loss: 2.426452749518938

Epoch: 6| Step: 2
Training loss: 2.1226658821105957
Validation loss: 2.4227369523817495

Epoch: 6| Step: 3
Training loss: 2.7549681663513184
Validation loss: 2.4222799347292994

Epoch: 6| Step: 4
Training loss: 2.6580586433410645
Validation loss: 2.4283127015636814

Epoch: 6| Step: 5
Training loss: 2.5545589923858643
Validation loss: 2.418479706651421

Epoch: 6| Step: 6
Training loss: 2.3260176181793213
Validation loss: 2.426978506067748

Epoch: 6| Step: 7
Training loss: 2.249980926513672
Validation loss: 2.4132344158746863

Epoch: 6| Step: 8
Training loss: 2.6530256271362305
Validation loss: 2.409199225005283

Epoch: 6| Step: 9
Training loss: 1.979801893234253
Validation loss: 2.4082657829407723

Epoch: 6| Step: 10
Training loss: 3.1558308601379395
Validation loss: 2.408151252295381

Epoch: 6| Step: 11
Training loss: 2.7915868759155273
Validation loss: 2.4073571953722226

Epoch: 6| Step: 12
Training loss: 3.1692357063293457
Validation loss: 2.407270690446259

Epoch: 6| Step: 13
Training loss: 4.1363043785095215
Validation loss: 2.403484934119768

Epoch: 143| Step: 0
Training loss: 2.1098837852478027
Validation loss: 2.4034174898619294

Epoch: 6| Step: 1
Training loss: 1.975584626197815
Validation loss: 2.3977257500412645

Epoch: 6| Step: 2
Training loss: 2.749847888946533
Validation loss: 2.399431308110555

Epoch: 6| Step: 3
Training loss: 3.285215377807617
Validation loss: 2.3942919162011917

Epoch: 6| Step: 4
Training loss: 2.8431520462036133
Validation loss: 2.396946858334285

Epoch: 6| Step: 5
Training loss: 2.0743134021759033
Validation loss: 2.3988423552564395

Epoch: 6| Step: 6
Training loss: 3.3860788345336914
Validation loss: 2.396034468886673

Epoch: 6| Step: 7
Training loss: 2.6831555366516113
Validation loss: 2.392425496091125

Epoch: 6| Step: 8
Training loss: 2.9674320220947266
Validation loss: 2.3859644448885353

Epoch: 6| Step: 9
Training loss: 2.732377052307129
Validation loss: 2.3920090429244505

Epoch: 6| Step: 10
Training loss: 2.81577730178833
Validation loss: 2.391184660696214

Epoch: 6| Step: 11
Training loss: 1.9925028085708618
Validation loss: 2.3889577696400304

Epoch: 6| Step: 12
Training loss: 2.921753406524658
Validation loss: 2.3919867802691717

Epoch: 6| Step: 13
Training loss: 1.8031196594238281
Validation loss: 2.3959082941855154

Epoch: 144| Step: 0
Training loss: 3.416349172592163
Validation loss: 2.392623752676031

Epoch: 6| Step: 1
Training loss: 2.5751662254333496
Validation loss: 2.4038940706560687

Epoch: 6| Step: 2
Training loss: 2.686784267425537
Validation loss: 2.397133622118222

Epoch: 6| Step: 3
Training loss: 2.8750791549682617
Validation loss: 2.3974553154360865

Epoch: 6| Step: 4
Training loss: 2.0097951889038086
Validation loss: 2.3906890217975905

Epoch: 6| Step: 5
Training loss: 2.2646639347076416
Validation loss: 2.3911108227186304

Epoch: 6| Step: 6
Training loss: 2.7529659271240234
Validation loss: 2.395590784729168

Epoch: 6| Step: 7
Training loss: 2.853102922439575
Validation loss: 2.401175988617764

Epoch: 6| Step: 8
Training loss: 2.936814069747925
Validation loss: 2.3993263962448284

Epoch: 6| Step: 9
Training loss: 2.1373419761657715
Validation loss: 2.390755759772434

Epoch: 6| Step: 10
Training loss: 2.3511545658111572
Validation loss: 2.395167048259448

Epoch: 6| Step: 11
Training loss: 2.6622533798217773
Validation loss: 2.3927449872416835

Epoch: 6| Step: 12
Training loss: 2.9059159755706787
Validation loss: 2.4023290270118305

Epoch: 6| Step: 13
Training loss: 2.0015125274658203
Validation loss: 2.394434044438024

Epoch: 145| Step: 0
Training loss: 2.309908866882324
Validation loss: 2.4021795821446243

Epoch: 6| Step: 1
Training loss: 2.40421462059021
Validation loss: 2.3975413537794545

Epoch: 6| Step: 2
Training loss: 2.8722386360168457
Validation loss: 2.4040546647963987

Epoch: 6| Step: 3
Training loss: 2.6895275115966797
Validation loss: 2.4072574441150953

Epoch: 6| Step: 4
Training loss: 1.847800612449646
Validation loss: 2.4247112171624297

Epoch: 6| Step: 5
Training loss: 2.806629180908203
Validation loss: 2.450590466940275

Epoch: 6| Step: 6
Training loss: 2.96897029876709
Validation loss: 2.457304569982713

Epoch: 6| Step: 7
Training loss: 3.0953614711761475
Validation loss: 2.445163619133734

Epoch: 6| Step: 8
Training loss: 2.6638028621673584
Validation loss: 2.4141639099326184

Epoch: 6| Step: 9
Training loss: 2.0461997985839844
Validation loss: 2.410505105090398

Epoch: 6| Step: 10
Training loss: 3.694788932800293
Validation loss: 2.4138820248265422

Epoch: 6| Step: 11
Training loss: 2.4471476078033447
Validation loss: 2.423672637631816

Epoch: 6| Step: 12
Training loss: 2.449423313140869
Validation loss: 2.4277695660950034

Epoch: 6| Step: 13
Training loss: 2.5896244049072266
Validation loss: 2.427085086863528

Epoch: 146| Step: 0
Training loss: 2.6514892578125
Validation loss: 2.417744669862973

Epoch: 6| Step: 1
Training loss: 2.3783047199249268
Validation loss: 2.419153295537477

Epoch: 6| Step: 2
Training loss: 3.3905301094055176
Validation loss: 2.4115464148982877

Epoch: 6| Step: 3
Training loss: 2.2150394916534424
Validation loss: 2.4183924198150635

Epoch: 6| Step: 4
Training loss: 3.0616865158081055
Validation loss: 2.4164335573873212

Epoch: 6| Step: 5
Training loss: 2.4191393852233887
Validation loss: 2.4226602892721854

Epoch: 6| Step: 6
Training loss: 2.0648982524871826
Validation loss: 2.4211696399155485

Epoch: 6| Step: 7
Training loss: 2.41021728515625
Validation loss: 2.4073429748576176

Epoch: 6| Step: 8
Training loss: 2.9754676818847656
Validation loss: 2.4277891600003807

Epoch: 6| Step: 9
Training loss: 2.685682535171509
Validation loss: 2.4147143838226155

Epoch: 6| Step: 10
Training loss: 2.0637588500976562
Validation loss: 2.410580277442932

Epoch: 6| Step: 11
Training loss: 3.0177178382873535
Validation loss: 2.410059146983649

Epoch: 6| Step: 12
Training loss: 2.4940030574798584
Validation loss: 2.3960096861726496

Epoch: 6| Step: 13
Training loss: 2.878904104232788
Validation loss: 2.4108342419388475

Epoch: 147| Step: 0
Training loss: 2.6144752502441406
Validation loss: 2.4033044256189817

Epoch: 6| Step: 1
Training loss: 2.8378653526306152
Validation loss: 2.3928988569526264

Epoch: 6| Step: 2
Training loss: 3.3847076892852783
Validation loss: 2.394923053761964

Epoch: 6| Step: 3
Training loss: 2.054478645324707
Validation loss: 2.397980325965471

Epoch: 6| Step: 4
Training loss: 2.1350691318511963
Validation loss: 2.4042818546295166

Epoch: 6| Step: 5
Training loss: 2.4523823261260986
Validation loss: 2.3968826134999595

Epoch: 6| Step: 6
Training loss: 2.8766372203826904
Validation loss: 2.3954398503867527

Epoch: 6| Step: 7
Training loss: 2.7413229942321777
Validation loss: 2.392600118473012

Epoch: 6| Step: 8
Training loss: 2.073134422302246
Validation loss: 2.3884147354351577

Epoch: 6| Step: 9
Training loss: 2.147649049758911
Validation loss: 2.3901086520123225

Epoch: 6| Step: 10
Training loss: 2.270104169845581
Validation loss: 2.3872092026536182

Epoch: 6| Step: 11
Training loss: 2.607759952545166
Validation loss: 2.3970976926947154

Epoch: 6| Step: 12
Training loss: 3.2809901237487793
Validation loss: 2.381562417553317

Epoch: 6| Step: 13
Training loss: 3.4333202838897705
Validation loss: 2.389140639253842

Epoch: 148| Step: 0
Training loss: 2.188690185546875
Validation loss: 2.3820750021165416

Epoch: 6| Step: 1
Training loss: 2.7236013412475586
Validation loss: 2.3947732730578353

Epoch: 6| Step: 2
Training loss: 2.0412209033966064
Validation loss: 2.3955058948968047

Epoch: 6| Step: 3
Training loss: 3.1499831676483154
Validation loss: 2.407843182163854

Epoch: 6| Step: 4
Training loss: 2.9634172916412354
Validation loss: 2.409280912850493

Epoch: 6| Step: 5
Training loss: 3.1013636589050293
Validation loss: 2.406143831950362

Epoch: 6| Step: 6
Training loss: 2.6170196533203125
Validation loss: 2.404898210238385

Epoch: 6| Step: 7
Training loss: 2.2030153274536133
Validation loss: 2.397859791273712

Epoch: 6| Step: 8
Training loss: 2.385438919067383
Validation loss: 2.403220425369919

Epoch: 6| Step: 9
Training loss: 2.8854002952575684
Validation loss: 2.3967962290651057

Epoch: 6| Step: 10
Training loss: 2.503640651702881
Validation loss: 2.394329649145885

Epoch: 6| Step: 11
Training loss: 2.6822633743286133
Validation loss: 2.3925345431091967

Epoch: 6| Step: 12
Training loss: 2.82049560546875
Validation loss: 2.394262424079321

Epoch: 6| Step: 13
Training loss: 2.052128314971924
Validation loss: 2.3890524013068086

Epoch: 149| Step: 0
Training loss: 2.861288547515869
Validation loss: 2.3893045584360757

Epoch: 6| Step: 1
Training loss: 2.541929244995117
Validation loss: 2.3845019135423886

Epoch: 6| Step: 2
Training loss: 3.198915719985962
Validation loss: 2.3900689924916914

Epoch: 6| Step: 3
Training loss: 2.5024356842041016
Validation loss: 2.389821006405738

Epoch: 6| Step: 4
Training loss: 3.2731614112854004
Validation loss: 2.3879550451873452

Epoch: 6| Step: 5
Training loss: 3.2216200828552246
Validation loss: 2.3897008895874023

Epoch: 6| Step: 6
Training loss: 2.669355869293213
Validation loss: 2.394343770960326

Epoch: 6| Step: 7
Training loss: 1.9984912872314453
Validation loss: 2.39509065176851

Epoch: 6| Step: 8
Training loss: 2.5801963806152344
Validation loss: 2.3927590462469284

Epoch: 6| Step: 9
Training loss: 2.7163453102111816
Validation loss: 2.394586214455225

Epoch: 6| Step: 10
Training loss: 2.4414749145507812
Validation loss: 2.3901534054868963

Epoch: 6| Step: 11
Training loss: 2.148456335067749
Validation loss: 2.3940515236188005

Epoch: 6| Step: 12
Training loss: 2.026066780090332
Validation loss: 2.398861203142392

Epoch: 6| Step: 13
Training loss: 2.021566867828369
Validation loss: 2.3969382150198824

Epoch: 150| Step: 0
Training loss: 1.8662903308868408
Validation loss: 2.3955284933890066

Epoch: 6| Step: 1
Training loss: 2.5530929565429688
Validation loss: 2.405608912949921

Epoch: 6| Step: 2
Training loss: 2.1379289627075195
Validation loss: 2.3997046101477837

Epoch: 6| Step: 3
Training loss: 2.6434922218322754
Validation loss: 2.4034279264429563

Epoch: 6| Step: 4
Training loss: 3.3831775188446045
Validation loss: 2.4054419789262997

Epoch: 6| Step: 5
Training loss: 2.9470348358154297
Validation loss: 2.4077239933834282

Epoch: 6| Step: 6
Training loss: 3.1656055450439453
Validation loss: 2.396456887645106

Epoch: 6| Step: 7
Training loss: 2.4354405403137207
Validation loss: 2.3974136819121656

Epoch: 6| Step: 8
Training loss: 2.9364452362060547
Validation loss: 2.3979902062364804

Epoch: 6| Step: 9
Training loss: 2.446941375732422
Validation loss: 2.3998348841103176

Epoch: 6| Step: 10
Training loss: 2.399893283843994
Validation loss: 2.4085721585058395

Epoch: 6| Step: 11
Training loss: 2.4576354026794434
Validation loss: 2.412672363301759

Epoch: 6| Step: 12
Training loss: 2.9331936836242676
Validation loss: 2.3988593803939

Epoch: 6| Step: 13
Training loss: 1.9001508951187134
Validation loss: 2.4072711775379796

Epoch: 151| Step: 0
Training loss: 1.7106350660324097
Validation loss: 2.3972986205931632

Epoch: 6| Step: 1
Training loss: 3.3458175659179688
Validation loss: 2.4082409950994674

Epoch: 6| Step: 2
Training loss: 3.061678171157837
Validation loss: 2.406194398480077

Epoch: 6| Step: 3
Training loss: 2.9115779399871826
Validation loss: 2.4042400621598765

Epoch: 6| Step: 4
Training loss: 2.3455331325531006
Validation loss: 2.407394483525266

Epoch: 6| Step: 5
Training loss: 2.5833723545074463
Validation loss: 2.4018820024305776

Epoch: 6| Step: 6
Training loss: 1.7828701734542847
Validation loss: 2.402203852130521

Epoch: 6| Step: 7
Training loss: 2.660832643508911
Validation loss: 2.3981634980888775

Epoch: 6| Step: 8
Training loss: 3.0163440704345703
Validation loss: 2.396401753989599

Epoch: 6| Step: 9
Training loss: 2.2891182899475098
Validation loss: 2.3942338625590005

Epoch: 6| Step: 10
Training loss: 2.047529697418213
Validation loss: 2.400368816109114

Epoch: 6| Step: 11
Training loss: 2.952932596206665
Validation loss: 2.3950379997171383

Epoch: 6| Step: 12
Training loss: 3.029217481613159
Validation loss: 2.3914673841127785

Epoch: 6| Step: 13
Training loss: 2.8250651359558105
Validation loss: 2.39352253688279

Epoch: 152| Step: 0
Training loss: 3.4703264236450195
Validation loss: 2.3907520822299424

Epoch: 6| Step: 1
Training loss: 2.141359806060791
Validation loss: 2.398587590904646

Epoch: 6| Step: 2
Training loss: 2.6372036933898926
Validation loss: 2.4002627890597106

Epoch: 6| Step: 3
Training loss: 2.537181854248047
Validation loss: 2.398822881842172

Epoch: 6| Step: 4
Training loss: 2.6169910430908203
Validation loss: 2.404817229957991

Epoch: 6| Step: 5
Training loss: 2.281200885772705
Validation loss: 2.4179932430226314

Epoch: 6| Step: 6
Training loss: 2.841506004333496
Validation loss: 2.416011934639305

Epoch: 6| Step: 7
Training loss: 3.404472827911377
Validation loss: 2.4257486379274757

Epoch: 6| Step: 8
Training loss: 2.7181296348571777
Validation loss: 2.420192690305812

Epoch: 6| Step: 9
Training loss: 1.9399852752685547
Validation loss: 2.427291090770434

Epoch: 6| Step: 10
Training loss: 2.0577032566070557
Validation loss: 2.414268378288515

Epoch: 6| Step: 11
Training loss: 2.5289978981018066
Validation loss: 2.4138643254515944

Epoch: 6| Step: 12
Training loss: 3.1405630111694336
Validation loss: 2.423804711270076

Epoch: 6| Step: 13
Training loss: 1.9413236379623413
Validation loss: 2.4112735871345765

Epoch: 153| Step: 0
Training loss: 1.8546476364135742
Validation loss: 2.4165030576849498

Epoch: 6| Step: 1
Training loss: 2.810617446899414
Validation loss: 2.4178482870901785

Epoch: 6| Step: 2
Training loss: 2.6166574954986572
Validation loss: 2.409084609759751

Epoch: 6| Step: 3
Training loss: 2.908175230026245
Validation loss: 2.4167083258269937

Epoch: 6| Step: 4
Training loss: 2.1430516242980957
Validation loss: 2.41374046059065

Epoch: 6| Step: 5
Training loss: 0.9950985908508301
Validation loss: 2.419564026658253

Epoch: 6| Step: 6
Training loss: 3.1673240661621094
Validation loss: 2.4159300532392276

Epoch: 6| Step: 7
Training loss: 2.987076997756958
Validation loss: 2.422054583026517

Epoch: 6| Step: 8
Training loss: 2.8360912799835205
Validation loss: 2.4135794331950526

Epoch: 6| Step: 9
Training loss: 3.2902023792266846
Validation loss: 2.4128328830965105

Epoch: 6| Step: 10
Training loss: 2.246824264526367
Validation loss: 2.4112913852096884

Epoch: 6| Step: 11
Training loss: 2.9909071922302246
Validation loss: 2.4091122201693955

Epoch: 6| Step: 12
Training loss: 3.0837440490722656
Validation loss: 2.4078696876443844

Epoch: 6| Step: 13
Training loss: 2.249220848083496
Validation loss: 2.4072671141675723

Epoch: 154| Step: 0
Training loss: 2.9670417308807373
Validation loss: 2.419426889829738

Epoch: 6| Step: 1
Training loss: 3.4341485500335693
Validation loss: 2.3910982724158996

Epoch: 6| Step: 2
Training loss: 2.477257251739502
Validation loss: 2.4067649123489216

Epoch: 6| Step: 3
Training loss: 2.2181997299194336
Validation loss: 2.395519966720253

Epoch: 6| Step: 4
Training loss: 2.355408191680908
Validation loss: 2.400652176590376

Epoch: 6| Step: 5
Training loss: 2.1644749641418457
Validation loss: 2.394102050412086

Epoch: 6| Step: 6
Training loss: 2.451624631881714
Validation loss: 2.3968261672604467

Epoch: 6| Step: 7
Training loss: 2.0814428329467773
Validation loss: 2.411241977445541

Epoch: 6| Step: 8
Training loss: 2.887869358062744
Validation loss: 2.4155939240609445

Epoch: 6| Step: 9
Training loss: 2.8738131523132324
Validation loss: 2.415913810012161

Epoch: 6| Step: 10
Training loss: 2.531658172607422
Validation loss: 2.409534172345233

Epoch: 6| Step: 11
Training loss: 2.513324737548828
Validation loss: 2.4144723646102415

Epoch: 6| Step: 12
Training loss: 2.7105565071105957
Validation loss: 2.4000227271869616

Epoch: 6| Step: 13
Training loss: 2.8449196815490723
Validation loss: 2.4034408574463217

Epoch: 155| Step: 0
Training loss: 2.4136574268341064
Validation loss: 2.3975133742055585

Epoch: 6| Step: 1
Training loss: 2.0098471641540527
Validation loss: 2.408981866734002

Epoch: 6| Step: 2
Training loss: 3.1701533794403076
Validation loss: 2.407286823436778

Epoch: 6| Step: 3
Training loss: 3.4435853958129883
Validation loss: 2.4075596435095674

Epoch: 6| Step: 4
Training loss: 2.252138614654541
Validation loss: 2.395628418973697

Epoch: 6| Step: 5
Training loss: 2.611440658569336
Validation loss: 2.398788075293264

Epoch: 6| Step: 6
Training loss: 2.3524861335754395
Validation loss: 2.391992611269797

Epoch: 6| Step: 7
Training loss: 2.8624372482299805
Validation loss: 2.3925342405996015

Epoch: 6| Step: 8
Training loss: 2.7268636226654053
Validation loss: 2.394461880448044

Epoch: 6| Step: 9
Training loss: 2.4818315505981445
Validation loss: 2.388093066471879

Epoch: 6| Step: 10
Training loss: 2.277045249938965
Validation loss: 2.3905831075483754

Epoch: 6| Step: 11
Training loss: 2.0560758113861084
Validation loss: 2.3896737893422446

Epoch: 6| Step: 12
Training loss: 2.676814556121826
Validation loss: 2.3827865110930575

Epoch: 6| Step: 13
Training loss: 3.265653371810913
Validation loss: 2.3940188987280733

Epoch: 156| Step: 0
Training loss: 2.2330031394958496
Validation loss: 2.399598844589726

Epoch: 6| Step: 1
Training loss: 2.9163694381713867
Validation loss: 2.410807917194982

Epoch: 6| Step: 2
Training loss: 2.9630179405212402
Validation loss: 2.4285249069172847

Epoch: 6| Step: 3
Training loss: 1.815025806427002
Validation loss: 2.4536394970391386

Epoch: 6| Step: 4
Training loss: 2.3183984756469727
Validation loss: 2.475914870539019

Epoch: 6| Step: 5
Training loss: 2.883780002593994
Validation loss: 2.4806932018649195

Epoch: 6| Step: 6
Training loss: 2.7869560718536377
Validation loss: 2.494774754329394

Epoch: 6| Step: 7
Training loss: 2.795851945877075
Validation loss: 2.492281226701634

Epoch: 6| Step: 8
Training loss: 2.7668864727020264
Validation loss: 2.468349133768389

Epoch: 6| Step: 9
Training loss: 2.9613265991210938
Validation loss: 2.4503484797734085

Epoch: 6| Step: 10
Training loss: 2.874272346496582
Validation loss: 2.4336323661188923

Epoch: 6| Step: 11
Training loss: 2.671607494354248
Validation loss: 2.404856228059338

Epoch: 6| Step: 12
Training loss: 2.6193056106567383
Validation loss: 2.378413354196856

Epoch: 6| Step: 13
Training loss: 1.9679194688796997
Validation loss: 2.3745971982197096

Epoch: 157| Step: 0
Training loss: 3.031182289123535
Validation loss: 2.3759810181074243

Epoch: 6| Step: 1
Training loss: 2.443131446838379
Validation loss: 2.3815590027839906

Epoch: 6| Step: 2
Training loss: 3.5129752159118652
Validation loss: 2.385359566698792

Epoch: 6| Step: 3
Training loss: 2.998166561126709
Validation loss: 2.394470881390315

Epoch: 6| Step: 4
Training loss: 2.3101983070373535
Validation loss: 2.3995940608362996

Epoch: 6| Step: 5
Training loss: 1.823991298675537
Validation loss: 2.4060280169210126

Epoch: 6| Step: 6
Training loss: 2.9666056632995605
Validation loss: 2.4136227125762613

Epoch: 6| Step: 7
Training loss: 2.6676278114318848
Validation loss: 2.4264023688531693

Epoch: 6| Step: 8
Training loss: 2.4454920291900635
Validation loss: 2.421963325110815

Epoch: 6| Step: 9
Training loss: 2.7276506423950195
Validation loss: 2.427344073531448

Epoch: 6| Step: 10
Training loss: 3.0444605350494385
Validation loss: 2.4089057086616434

Epoch: 6| Step: 11
Training loss: 1.6967658996582031
Validation loss: 2.405739950877364

Epoch: 6| Step: 12
Training loss: 2.2575011253356934
Validation loss: 2.401186958436043

Epoch: 6| Step: 13
Training loss: 2.5821595191955566
Validation loss: 2.39450559308452

Epoch: 158| Step: 0
Training loss: 2.645847797393799
Validation loss: 2.4017541639266478

Epoch: 6| Step: 1
Training loss: 2.7311291694641113
Validation loss: 2.3864555743432816

Epoch: 6| Step: 2
Training loss: 2.5483436584472656
Validation loss: 2.3757076494155394

Epoch: 6| Step: 3
Training loss: 2.678114175796509
Validation loss: 2.378644889400851

Epoch: 6| Step: 4
Training loss: 1.842842936515808
Validation loss: 2.3758166349062355

Epoch: 6| Step: 5
Training loss: 2.2561185359954834
Validation loss: 2.3788676184992634

Epoch: 6| Step: 6
Training loss: 1.9806455373764038
Validation loss: 2.3806353410085044

Epoch: 6| Step: 7
Training loss: 2.4934146404266357
Validation loss: 2.3727780413883988

Epoch: 6| Step: 8
Training loss: 2.7931623458862305
Validation loss: 2.377725447377851

Epoch: 6| Step: 9
Training loss: 2.553253412246704
Validation loss: 2.382324359750235

Epoch: 6| Step: 10
Training loss: 3.903838872909546
Validation loss: 2.3823572102413384

Epoch: 6| Step: 11
Training loss: 2.480483055114746
Validation loss: 2.377858961782148

Epoch: 6| Step: 12
Training loss: 2.785468578338623
Validation loss: 2.3768790896220873

Epoch: 6| Step: 13
Training loss: 2.7564001083374023
Validation loss: 2.3857733562428463

Epoch: 159| Step: 0
Training loss: 2.270352602005005
Validation loss: 2.391091926123506

Epoch: 6| Step: 1
Training loss: 3.0253028869628906
Validation loss: 2.389401440979332

Epoch: 6| Step: 2
Training loss: 3.162644863128662
Validation loss: 2.4079971774931876

Epoch: 6| Step: 3
Training loss: 2.391326427459717
Validation loss: 2.412401081413351

Epoch: 6| Step: 4
Training loss: 3.028229236602783
Validation loss: 2.423852320640318

Epoch: 6| Step: 5
Training loss: 2.2060530185699463
Validation loss: 2.414356941817909

Epoch: 6| Step: 6
Training loss: 2.302136182785034
Validation loss: 2.4024994988595285

Epoch: 6| Step: 7
Training loss: 2.7182273864746094
Validation loss: 2.39341127744285

Epoch: 6| Step: 8
Training loss: 3.1702826023101807
Validation loss: 2.387451228275094

Epoch: 6| Step: 9
Training loss: 2.2281107902526855
Validation loss: 2.3930290052967687

Epoch: 6| Step: 10
Training loss: 2.3708839416503906
Validation loss: 2.391495123986275

Epoch: 6| Step: 11
Training loss: 2.2384018898010254
Validation loss: 2.3951363486628376

Epoch: 6| Step: 12
Training loss: 3.0353310108184814
Validation loss: 2.3890664423665693

Epoch: 6| Step: 13
Training loss: 2.066096305847168
Validation loss: 2.386685695699466

Epoch: 160| Step: 0
Training loss: 3.1736433506011963
Validation loss: 2.3819574694479666

Epoch: 6| Step: 1
Training loss: 1.690097451210022
Validation loss: 2.380930164808868

Epoch: 6| Step: 2
Training loss: 2.782467842102051
Validation loss: 2.377227050001903

Epoch: 6| Step: 3
Training loss: 2.3829851150512695
Validation loss: 2.3890821959382746

Epoch: 6| Step: 4
Training loss: 2.941987991333008
Validation loss: 2.3874270018710884

Epoch: 6| Step: 5
Training loss: 3.036120891571045
Validation loss: 2.389660276392455

Epoch: 6| Step: 6
Training loss: 3.0709571838378906
Validation loss: 2.392888751081241

Epoch: 6| Step: 7
Training loss: 2.4692304134368896
Validation loss: 2.395875915404289

Epoch: 6| Step: 8
Training loss: 1.7886395454406738
Validation loss: 2.398781422645815

Epoch: 6| Step: 9
Training loss: 2.0486326217651367
Validation loss: 2.4063607877300632

Epoch: 6| Step: 10
Training loss: 2.4146323204040527
Validation loss: 2.4095216207606818

Epoch: 6| Step: 11
Training loss: 2.8771812915802
Validation loss: 2.414821542719359

Epoch: 6| Step: 12
Training loss: 2.95558500289917
Validation loss: 2.4149050584403415

Epoch: 6| Step: 13
Training loss: 2.647674560546875
Validation loss: 2.4198402845731346

Epoch: 161| Step: 0
Training loss: 3.1034297943115234
Validation loss: 2.419887378651609

Epoch: 6| Step: 1
Training loss: 2.4137661457061768
Validation loss: 2.4053199906503

Epoch: 6| Step: 2
Training loss: 2.1151609420776367
Validation loss: 2.40708944489879

Epoch: 6| Step: 3
Training loss: 2.8558268547058105
Validation loss: 2.408364301086754

Epoch: 6| Step: 4
Training loss: 2.5639047622680664
Validation loss: 2.408586771257462

Epoch: 6| Step: 5
Training loss: 3.035048484802246
Validation loss: 2.411983830954439

Epoch: 6| Step: 6
Training loss: 2.6821177005767822
Validation loss: 2.397685343219388

Epoch: 6| Step: 7
Training loss: 2.4786863327026367
Validation loss: 2.394083030762211

Epoch: 6| Step: 8
Training loss: 2.420363426208496
Validation loss: 2.383435244201332

Epoch: 6| Step: 9
Training loss: 2.775102138519287
Validation loss: 2.3763332084942888

Epoch: 6| Step: 10
Training loss: 2.651158332824707
Validation loss: 2.384029498664282

Epoch: 6| Step: 11
Training loss: 2.5331268310546875
Validation loss: 2.38032873727942

Epoch: 6| Step: 12
Training loss: 2.6466777324676514
Validation loss: 2.373823078729773

Epoch: 6| Step: 13
Training loss: 1.5190308094024658
Validation loss: 2.375383141220257

Epoch: 162| Step: 0
Training loss: 2.651083469390869
Validation loss: 2.3727947601708035

Epoch: 6| Step: 1
Training loss: 2.7255821228027344
Validation loss: 2.3782275081962667

Epoch: 6| Step: 2
Training loss: 2.9917752742767334
Validation loss: 2.3753129231032504

Epoch: 6| Step: 3
Training loss: 2.152548313140869
Validation loss: 2.372741073690435

Epoch: 6| Step: 4
Training loss: 2.1533584594726562
Validation loss: 2.366883365056848

Epoch: 6| Step: 5
Training loss: 2.4956741333007812
Validation loss: 2.3736689834184546

Epoch: 6| Step: 6
Training loss: 2.8553824424743652
Validation loss: 2.3744236423123266

Epoch: 6| Step: 7
Training loss: 2.0986475944519043
Validation loss: 2.3771491460902716

Epoch: 6| Step: 8
Training loss: 2.7790489196777344
Validation loss: 2.3684009198219544

Epoch: 6| Step: 9
Training loss: 2.5867486000061035
Validation loss: 2.3797041857114403

Epoch: 6| Step: 10
Training loss: 2.5140485763549805
Validation loss: 2.3805283961757535

Epoch: 6| Step: 11
Training loss: 2.4870126247406006
Validation loss: 2.372310161590576

Epoch: 6| Step: 12
Training loss: 2.7051143646240234
Validation loss: 2.3762323394898446

Epoch: 6| Step: 13
Training loss: 3.2720999717712402
Validation loss: 2.381289157816159

Epoch: 163| Step: 0
Training loss: 3.3678319454193115
Validation loss: 2.3849017671359483

Epoch: 6| Step: 1
Training loss: 1.9287461042404175
Validation loss: 2.3810071688826366

Epoch: 6| Step: 2
Training loss: 3.1569442749023438
Validation loss: 2.3837528433851016

Epoch: 6| Step: 3
Training loss: 2.6900129318237305
Validation loss: 2.3801222808899416

Epoch: 6| Step: 4
Training loss: 2.053629159927368
Validation loss: 2.388412760150048

Epoch: 6| Step: 5
Training loss: 3.254312515258789
Validation loss: 2.4009327145032984

Epoch: 6| Step: 6
Training loss: 2.1387720108032227
Validation loss: 2.409094456703432

Epoch: 6| Step: 7
Training loss: 2.671602725982666
Validation loss: 2.4214334077732538

Epoch: 6| Step: 8
Training loss: 2.5380566120147705
Validation loss: 2.445888611578172

Epoch: 6| Step: 9
Training loss: 2.5039637088775635
Validation loss: 2.4338439228714153

Epoch: 6| Step: 10
Training loss: 1.975814938545227
Validation loss: 2.441926343466646

Epoch: 6| Step: 11
Training loss: 2.470393657684326
Validation loss: 2.4416569561086674

Epoch: 6| Step: 12
Training loss: 2.432558059692383
Validation loss: 2.4439046664904525

Epoch: 6| Step: 13
Training loss: 3.25052809715271
Validation loss: 2.4279282528867006

Epoch: 164| Step: 0
Training loss: 2.1795694828033447
Validation loss: 2.4006906324817288

Epoch: 6| Step: 1
Training loss: 2.3946821689605713
Validation loss: 2.4000460793895106

Epoch: 6| Step: 2
Training loss: 2.9255290031433105
Validation loss: 2.407252647543466

Epoch: 6| Step: 3
Training loss: 3.0973262786865234
Validation loss: 2.4068206740963842

Epoch: 6| Step: 4
Training loss: 3.4909443855285645
Validation loss: 2.3938663313465733

Epoch: 6| Step: 5
Training loss: 1.928261399269104
Validation loss: 2.4047170633910806

Epoch: 6| Step: 6
Training loss: 2.7092976570129395
Validation loss: 2.401229237997404

Epoch: 6| Step: 7
Training loss: 2.3562235832214355
Validation loss: 2.4070878541597756

Epoch: 6| Step: 8
Training loss: 2.3385698795318604
Validation loss: 2.407179811949371

Epoch: 6| Step: 9
Training loss: 2.3723440170288086
Validation loss: 2.3974259950781382

Epoch: 6| Step: 10
Training loss: 3.401946544647217
Validation loss: 2.3918592237657115

Epoch: 6| Step: 11
Training loss: 2.199406147003174
Validation loss: 2.3965387216178318

Epoch: 6| Step: 12
Training loss: 1.6983919143676758
Validation loss: 2.3885434981315368

Epoch: 6| Step: 13
Training loss: 3.3168997764587402
Validation loss: 2.3804754928875993

Epoch: 165| Step: 0
Training loss: 2.54990816116333
Validation loss: 2.3753389748193885

Epoch: 6| Step: 1
Training loss: 2.7614541053771973
Validation loss: 2.3826985641192366

Epoch: 6| Step: 2
Training loss: 2.5626845359802246
Validation loss: 2.3798164295893844

Epoch: 6| Step: 3
Training loss: 2.9216065406799316
Validation loss: 2.367028969590382

Epoch: 6| Step: 4
Training loss: 1.7712403535842896
Validation loss: 2.3730679096714145

Epoch: 6| Step: 5
Training loss: 3.0971381664276123
Validation loss: 2.3647463065321728

Epoch: 6| Step: 6
Training loss: 2.2372121810913086
Validation loss: 2.3692649641344623

Epoch: 6| Step: 7
Training loss: 2.4930362701416016
Validation loss: 2.3658802842581146

Epoch: 6| Step: 8
Training loss: 2.521597146987915
Validation loss: 2.363814635943341

Epoch: 6| Step: 9
Training loss: 2.1836447715759277
Validation loss: 2.3631190151296635

Epoch: 6| Step: 10
Training loss: 2.5259227752685547
Validation loss: 2.3619179623101347

Epoch: 6| Step: 11
Training loss: 3.548093795776367
Validation loss: 2.3606424588029102

Epoch: 6| Step: 12
Training loss: 2.0189661979675293
Validation loss: 2.3620327185559016

Epoch: 6| Step: 13
Training loss: 3.181103229522705
Validation loss: 2.361386799043225

Epoch: 166| Step: 0
Training loss: 2.3197264671325684
Validation loss: 2.3672780426599647

Epoch: 6| Step: 1
Training loss: 2.335052251815796
Validation loss: 2.372765791031622

Epoch: 6| Step: 2
Training loss: 3.214421272277832
Validation loss: 2.382547752831572

Epoch: 6| Step: 3
Training loss: 2.736403465270996
Validation loss: 2.3859588356428247

Epoch: 6| Step: 4
Training loss: 2.3427884578704834
Validation loss: 2.3913841093740156

Epoch: 6| Step: 5
Training loss: 2.6557421684265137
Validation loss: 2.3905826537839827

Epoch: 6| Step: 6
Training loss: 2.1364645957946777
Validation loss: 2.3893343838312293

Epoch: 6| Step: 7
Training loss: 2.8002800941467285
Validation loss: 2.3878078473511564

Epoch: 6| Step: 8
Training loss: 2.921940326690674
Validation loss: 2.384276300348261

Epoch: 6| Step: 9
Training loss: 2.648178815841675
Validation loss: 2.3809787406716296

Epoch: 6| Step: 10
Training loss: 2.906905174255371
Validation loss: 2.3771355177766536

Epoch: 6| Step: 11
Training loss: 2.1389594078063965
Validation loss: 2.36956540999874

Epoch: 6| Step: 12
Training loss: 2.148643970489502
Validation loss: 2.368272045607208

Epoch: 6| Step: 13
Training loss: 2.8377323150634766
Validation loss: 2.372293218489616

Epoch: 167| Step: 0
Training loss: 3.008523464202881
Validation loss: 2.3730846835720922

Epoch: 6| Step: 1
Training loss: 2.190861225128174
Validation loss: 2.3747898968317176

Epoch: 6| Step: 2
Training loss: 1.6997116804122925
Validation loss: 2.3764333955703245

Epoch: 6| Step: 3
Training loss: 3.2794506549835205
Validation loss: 2.3862984436814503

Epoch: 6| Step: 4
Training loss: 3.233931541442871
Validation loss: 2.3855265096951555

Epoch: 6| Step: 5
Training loss: 2.933140277862549
Validation loss: 2.3870988686879477

Epoch: 6| Step: 6
Training loss: 2.252207040786743
Validation loss: 2.3895292923014653

Epoch: 6| Step: 7
Training loss: 2.1503233909606934
Validation loss: 2.388620730369322

Epoch: 6| Step: 8
Training loss: 2.2018606662750244
Validation loss: 2.3818213811484714

Epoch: 6| Step: 9
Training loss: 2.655949831008911
Validation loss: 2.3910965214493456

Epoch: 6| Step: 10
Training loss: 1.2930794954299927
Validation loss: 2.376032954903059

Epoch: 6| Step: 11
Training loss: 3.2500200271606445
Validation loss: 2.3826239749949467

Epoch: 6| Step: 12
Training loss: 2.773129463195801
Validation loss: 2.37877978304381

Epoch: 6| Step: 13
Training loss: 3.2913806438446045
Validation loss: 2.3792815669890373

Epoch: 168| Step: 0
Training loss: 2.299773693084717
Validation loss: 2.3803817995132937

Epoch: 6| Step: 1
Training loss: 2.730189323425293
Validation loss: 2.384394112453666

Epoch: 6| Step: 2
Training loss: 2.9142956733703613
Validation loss: 2.389919409187891

Epoch: 6| Step: 3
Training loss: 2.094062328338623
Validation loss: 2.394643255459365

Epoch: 6| Step: 4
Training loss: 1.8531099557876587
Validation loss: 2.392779116989464

Epoch: 6| Step: 5
Training loss: 1.9095444679260254
Validation loss: 2.3938957516865065

Epoch: 6| Step: 6
Training loss: 2.4426980018615723
Validation loss: 2.3747614814389135

Epoch: 6| Step: 7
Training loss: 3.321619987487793
Validation loss: 2.3796453988680275

Epoch: 6| Step: 8
Training loss: 3.151822328567505
Validation loss: 2.3776235170261835

Epoch: 6| Step: 9
Training loss: 2.808328628540039
Validation loss: 2.375750152013635

Epoch: 6| Step: 10
Training loss: 2.9142656326293945
Validation loss: 2.3725754573781

Epoch: 6| Step: 11
Training loss: 2.527880907058716
Validation loss: 2.3668010004105104

Epoch: 6| Step: 12
Training loss: 2.9945836067199707
Validation loss: 2.3659185107036302

Epoch: 6| Step: 13
Training loss: 1.5589556694030762
Validation loss: 2.3776975575313775

Epoch: 169| Step: 0
Training loss: 2.4871156215667725
Validation loss: 2.3781957626342773

Epoch: 6| Step: 1
Training loss: 2.6379005908966064
Validation loss: 2.3772246786343154

Epoch: 6| Step: 2
Training loss: 2.921304225921631
Validation loss: 2.3724576529636177

Epoch: 6| Step: 3
Training loss: 2.866178274154663
Validation loss: 2.3768011523831274

Epoch: 6| Step: 4
Training loss: 2.075684070587158
Validation loss: 2.3697152881212133

Epoch: 6| Step: 5
Training loss: 3.6263608932495117
Validation loss: 2.369662038741573

Epoch: 6| Step: 6
Training loss: 2.2491347789764404
Validation loss: 2.3628118909815305

Epoch: 6| Step: 7
Training loss: 1.9253184795379639
Validation loss: 2.367089617636896

Epoch: 6| Step: 8
Training loss: 2.034425735473633
Validation loss: 2.3652254048214165

Epoch: 6| Step: 9
Training loss: 2.110762119293213
Validation loss: 2.3767706988960184

Epoch: 6| Step: 10
Training loss: 2.5517802238464355
Validation loss: 2.3703280802695983

Epoch: 6| Step: 11
Training loss: 2.8514068126678467
Validation loss: 2.3797220773594354

Epoch: 6| Step: 12
Training loss: 3.0868451595306396
Validation loss: 2.37867905247596

Epoch: 6| Step: 13
Training loss: 2.5254979133605957
Validation loss: 2.378343597535164

Epoch: 170| Step: 0
Training loss: 3.362107276916504
Validation loss: 2.380018449598743

Epoch: 6| Step: 1
Training loss: 2.469114065170288
Validation loss: 2.373101285708848

Epoch: 6| Step: 2
Training loss: 1.6973321437835693
Validation loss: 2.3730014652334233

Epoch: 6| Step: 3
Training loss: 2.9464964866638184
Validation loss: 2.3873240127358386

Epoch: 6| Step: 4
Training loss: 3.258091449737549
Validation loss: 2.3861492885056363

Epoch: 6| Step: 5
Training loss: 1.7711873054504395
Validation loss: 2.400291553107641

Epoch: 6| Step: 6
Training loss: 2.160968065261841
Validation loss: 2.3880992986822642

Epoch: 6| Step: 7
Training loss: 2.4171323776245117
Validation loss: 2.3990756042541994

Epoch: 6| Step: 8
Training loss: 2.0840718746185303
Validation loss: 2.3794376696309736

Epoch: 6| Step: 9
Training loss: 3.1131932735443115
Validation loss: 2.3876245893457884

Epoch: 6| Step: 10
Training loss: 2.6089372634887695
Validation loss: 2.378055541746078

Epoch: 6| Step: 11
Training loss: 3.0306830406188965
Validation loss: 2.3692786027026433

Epoch: 6| Step: 12
Training loss: 2.587019443511963
Validation loss: 2.3631930120529665

Epoch: 6| Step: 13
Training loss: 2.5030202865600586
Validation loss: 2.362663725371002

Epoch: 171| Step: 0
Training loss: 2.5952303409576416
Validation loss: 2.3617995451855403

Epoch: 6| Step: 1
Training loss: 2.6249289512634277
Validation loss: 2.361231332184166

Epoch: 6| Step: 2
Training loss: 2.6892454624176025
Validation loss: 2.3607129384112615

Epoch: 6| Step: 3
Training loss: 1.8645607233047485
Validation loss: 2.3613540228976997

Epoch: 6| Step: 4
Training loss: 2.7560248374938965
Validation loss: 2.3638320738269436

Epoch: 6| Step: 5
Training loss: 2.050553560256958
Validation loss: 2.368534718790362

Epoch: 6| Step: 6
Training loss: 2.039794445037842
Validation loss: 2.3694316917850125

Epoch: 6| Step: 7
Training loss: 2.5479865074157715
Validation loss: 2.3670339212622693

Epoch: 6| Step: 8
Training loss: 2.22928524017334
Validation loss: 2.3702360045525337

Epoch: 6| Step: 9
Training loss: 3.337740421295166
Validation loss: 2.3625337872453915

Epoch: 6| Step: 10
Training loss: 3.144505023956299
Validation loss: 2.362750335406232

Epoch: 6| Step: 11
Training loss: 2.276822328567505
Validation loss: 2.3540466946940266

Epoch: 6| Step: 12
Training loss: 3.162208080291748
Validation loss: 2.350490641850297

Epoch: 6| Step: 13
Training loss: 3.0599238872528076
Validation loss: 2.350187768218338

Epoch: 172| Step: 0
Training loss: 2.8131704330444336
Validation loss: 2.351639014418407

Epoch: 6| Step: 1
Training loss: 2.6060116291046143
Validation loss: 2.349349655130858

Epoch: 6| Step: 2
Training loss: 2.483808994293213
Validation loss: 2.3500253282567507

Epoch: 6| Step: 3
Training loss: 3.2959272861480713
Validation loss: 2.343097726504008

Epoch: 6| Step: 4
Training loss: 2.372269868850708
Validation loss: 2.3431983122261624

Epoch: 6| Step: 5
Training loss: 2.2899129390716553
Validation loss: 2.3469590961292224

Epoch: 6| Step: 6
Training loss: 1.81489896774292
Validation loss: 2.346330934955228

Epoch: 6| Step: 7
Training loss: 2.1812939643859863
Validation loss: 2.3549553219990065

Epoch: 6| Step: 8
Training loss: 2.8486433029174805
Validation loss: 2.358382950546921

Epoch: 6| Step: 9
Training loss: 2.424002170562744
Validation loss: 2.354160221674109

Epoch: 6| Step: 10
Training loss: 2.6680543422698975
Validation loss: 2.3485234552814114

Epoch: 6| Step: 11
Training loss: 3.195305824279785
Validation loss: 2.354235574763308

Epoch: 6| Step: 12
Training loss: 2.4663939476013184
Validation loss: 2.3634022051288235

Epoch: 6| Step: 13
Training loss: 2.7839558124542236
Validation loss: 2.367464606479932

Epoch: 173| Step: 0
Training loss: 2.542217254638672
Validation loss: 2.352549606753934

Epoch: 6| Step: 1
Training loss: 2.166200637817383
Validation loss: 2.351000855045934

Epoch: 6| Step: 2
Training loss: 2.8756511211395264
Validation loss: 2.3618076744899956

Epoch: 6| Step: 3
Training loss: 2.836761474609375
Validation loss: 2.361085163649692

Epoch: 6| Step: 4
Training loss: 2.5838232040405273
Validation loss: 2.366001162477719

Epoch: 6| Step: 5
Training loss: 2.138763189315796
Validation loss: 2.372196182127922

Epoch: 6| Step: 6
Training loss: 2.944899559020996
Validation loss: 2.3710848516033542

Epoch: 6| Step: 7
Training loss: 2.2650914192199707
Validation loss: 2.3750884250927995

Epoch: 6| Step: 8
Training loss: 2.825617551803589
Validation loss: 2.3717640266623548

Epoch: 6| Step: 9
Training loss: 2.3843026161193848
Validation loss: 2.3727360335729455

Epoch: 6| Step: 10
Training loss: 2.134152889251709
Validation loss: 2.3745544443848314

Epoch: 6| Step: 11
Training loss: 3.0442986488342285
Validation loss: 2.3684364723902878

Epoch: 6| Step: 12
Training loss: 2.714383602142334
Validation loss: 2.373219402887488

Epoch: 6| Step: 13
Training loss: 2.4335529804229736
Validation loss: 2.375583182099045

Epoch: 174| Step: 0
Training loss: 2.4380085468292236
Validation loss: 2.370707073519307

Epoch: 6| Step: 1
Training loss: 2.1409754753112793
Validation loss: 2.373162997666226

Epoch: 6| Step: 2
Training loss: 2.1636452674865723
Validation loss: 2.3694876932328746

Epoch: 6| Step: 3
Training loss: 2.2318193912506104
Validation loss: 2.3814104244273198

Epoch: 6| Step: 4
Training loss: 2.077785015106201
Validation loss: 2.375246778611214

Epoch: 6| Step: 5
Training loss: 3.105714797973633
Validation loss: 2.368166164685321

Epoch: 6| Step: 6
Training loss: 3.3307385444641113
Validation loss: 2.3558514220740205

Epoch: 6| Step: 7
Training loss: 2.1067895889282227
Validation loss: 2.355994192502832

Epoch: 6| Step: 8
Training loss: 3.2081053256988525
Validation loss: 2.356398062039447

Epoch: 6| Step: 9
Training loss: 2.329237222671509
Validation loss: 2.3518478255118094

Epoch: 6| Step: 10
Training loss: 2.080000400543213
Validation loss: 2.3568980847635577

Epoch: 6| Step: 11
Training loss: 2.958656072616577
Validation loss: 2.3532629705244497

Epoch: 6| Step: 12
Training loss: 2.714162826538086
Validation loss: 2.349398671939809

Epoch: 6| Step: 13
Training loss: 3.4731264114379883
Validation loss: 2.347996988604146

Epoch: 175| Step: 0
Training loss: 2.5234580039978027
Validation loss: 2.34797239816317

Epoch: 6| Step: 1
Training loss: 2.0039727687835693
Validation loss: 2.3557104231208883

Epoch: 6| Step: 2
Training loss: 2.9493072032928467
Validation loss: 2.349599351165115

Epoch: 6| Step: 3
Training loss: 2.7444183826446533
Validation loss: 2.34593467814948

Epoch: 6| Step: 4
Training loss: 2.680300235748291
Validation loss: 2.3396576963445193

Epoch: 6| Step: 5
Training loss: 2.3925464153289795
Validation loss: 2.3396026088345434

Epoch: 6| Step: 6
Training loss: 3.115164279937744
Validation loss: 2.336011835323867

Epoch: 6| Step: 7
Training loss: 2.3489692211151123
Validation loss: 2.3382514164011967

Epoch: 6| Step: 8
Training loss: 2.331376791000366
Validation loss: 2.3394900060469106

Epoch: 6| Step: 9
Training loss: 3.1628212928771973
Validation loss: 2.3416122185286654

Epoch: 6| Step: 10
Training loss: 2.646739959716797
Validation loss: 2.3439452135434715

Epoch: 6| Step: 11
Training loss: 2.9830474853515625
Validation loss: 2.354477446566346

Epoch: 6| Step: 12
Training loss: 1.9132559299468994
Validation loss: 2.347542670465285

Epoch: 6| Step: 13
Training loss: 1.7570174932479858
Validation loss: 2.364418602758838

Epoch: 176| Step: 0
Training loss: 3.624340057373047
Validation loss: 2.359395001524238

Epoch: 6| Step: 1
Training loss: 2.457216501235962
Validation loss: 2.3614857504444737

Epoch: 6| Step: 2
Training loss: 2.9449374675750732
Validation loss: 2.365045439812445

Epoch: 6| Step: 3
Training loss: 2.496155261993408
Validation loss: 2.3595031487044467

Epoch: 6| Step: 4
Training loss: 2.531001091003418
Validation loss: 2.3695758773434545

Epoch: 6| Step: 5
Training loss: 2.212405204772949
Validation loss: 2.3543877037622596

Epoch: 6| Step: 6
Training loss: 2.698237895965576
Validation loss: 2.351685990569412

Epoch: 6| Step: 7
Training loss: 1.3701424598693848
Validation loss: 2.353670553494525

Epoch: 6| Step: 8
Training loss: 2.639937400817871
Validation loss: 2.3542198186279624

Epoch: 6| Step: 9
Training loss: 2.2027409076690674
Validation loss: 2.3662329412275747

Epoch: 6| Step: 10
Training loss: 2.6693050861358643
Validation loss: 2.35661211962341

Epoch: 6| Step: 11
Training loss: 3.080249309539795
Validation loss: 2.37108592320514

Epoch: 6| Step: 12
Training loss: 2.285259246826172
Validation loss: 2.368561328098338

Epoch: 6| Step: 13
Training loss: 3.017549753189087
Validation loss: 2.366373454370806

Epoch: 177| Step: 0
Training loss: 3.4951508045196533
Validation loss: 2.3500887578533542

Epoch: 6| Step: 1
Training loss: 2.552385091781616
Validation loss: 2.3471315112165225

Epoch: 6| Step: 2
Training loss: 2.2702720165252686
Validation loss: 2.35348069026906

Epoch: 6| Step: 3
Training loss: 2.4142401218414307
Validation loss: 2.35038742198739

Epoch: 6| Step: 4
Training loss: 2.6802616119384766
Validation loss: 2.348365932382563

Epoch: 6| Step: 5
Training loss: 2.2353949546813965
Validation loss: 2.345203556040282

Epoch: 6| Step: 6
Training loss: 3.09614896774292
Validation loss: 2.347681563387635

Epoch: 6| Step: 7
Training loss: 3.030367851257324
Validation loss: 2.354520297819568

Epoch: 6| Step: 8
Training loss: 2.483304023742676
Validation loss: 2.348332574290614

Epoch: 6| Step: 9
Training loss: 1.7945326566696167
Validation loss: 2.349100269297118

Epoch: 6| Step: 10
Training loss: 2.0537476539611816
Validation loss: 2.348325752442883

Epoch: 6| Step: 11
Training loss: 2.6346421241760254
Validation loss: 2.3484318461469424

Epoch: 6| Step: 12
Training loss: 3.052367925643921
Validation loss: 2.343816388037897

Epoch: 6| Step: 13
Training loss: 1.95148766040802
Validation loss: 2.3499025273066696

Epoch: 178| Step: 0
Training loss: 3.1560795307159424
Validation loss: 2.348620581370528

Epoch: 6| Step: 1
Training loss: 2.423433780670166
Validation loss: 2.3447830574486845

Epoch: 6| Step: 2
Training loss: 2.1300647258758545
Validation loss: 2.3420996486499743

Epoch: 6| Step: 3
Training loss: 2.0632519721984863
Validation loss: 2.343607287253103

Epoch: 6| Step: 4
Training loss: 2.6305289268493652
Validation loss: 2.3341981146925237

Epoch: 6| Step: 5
Training loss: 2.9606213569641113
Validation loss: 2.3402561808145173

Epoch: 6| Step: 6
Training loss: 2.0817830562591553
Validation loss: 2.339500588755454

Epoch: 6| Step: 7
Training loss: 2.431090831756592
Validation loss: 2.3434336185455322

Epoch: 6| Step: 8
Training loss: 2.8336069583892822
Validation loss: 2.334038911327239

Epoch: 6| Step: 9
Training loss: 2.6799111366271973
Validation loss: 2.3359700172178206

Epoch: 6| Step: 10
Training loss: 2.589757204055786
Validation loss: 2.3409045691131265

Epoch: 6| Step: 11
Training loss: 2.9188218116760254
Validation loss: 2.3419269784804313

Epoch: 6| Step: 12
Training loss: 2.3548526763916016
Validation loss: 2.3471341645845802

Epoch: 6| Step: 13
Training loss: 2.5528039932250977
Validation loss: 2.349929924934141

Epoch: 179| Step: 0
Training loss: 1.849030613899231
Validation loss: 2.344870421194261

Epoch: 6| Step: 1
Training loss: 3.703159809112549
Validation loss: 2.3545984260497557

Epoch: 6| Step: 2
Training loss: 1.922544240951538
Validation loss: 2.3572500495500464

Epoch: 6| Step: 3
Training loss: 2.9585819244384766
Validation loss: 2.3710071886739423

Epoch: 6| Step: 4
Training loss: 2.3589580059051514
Validation loss: 2.364361516890987

Epoch: 6| Step: 5
Training loss: 2.3078832626342773
Validation loss: 2.3733174570145144

Epoch: 6| Step: 6
Training loss: 2.850745439529419
Validation loss: 2.3797104794492006

Epoch: 6| Step: 7
Training loss: 3.041471242904663
Validation loss: 2.3728036111400974

Epoch: 6| Step: 8
Training loss: 2.087428331375122
Validation loss: 2.3591492727238643

Epoch: 6| Step: 9
Training loss: 2.636669158935547
Validation loss: 2.3656604777100267

Epoch: 6| Step: 10
Training loss: 3.132516622543335
Validation loss: 2.3610885476553314

Epoch: 6| Step: 11
Training loss: 2.233156442642212
Validation loss: 2.3495627321222776

Epoch: 6| Step: 12
Training loss: 2.5291643142700195
Validation loss: 2.3509264094855196

Epoch: 6| Step: 13
Training loss: 1.951048493385315
Validation loss: 2.346262196058868

Epoch: 180| Step: 0
Training loss: 2.8092880249023438
Validation loss: 2.3515007034424813

Epoch: 6| Step: 1
Training loss: 2.5698342323303223
Validation loss: 2.3484775481685514

Epoch: 6| Step: 2
Training loss: 2.128871440887451
Validation loss: 2.3603550951967955

Epoch: 6| Step: 3
Training loss: 1.9630365371704102
Validation loss: 2.3619145154953003

Epoch: 6| Step: 4
Training loss: 2.3862762451171875
Validation loss: 2.350235708298222

Epoch: 6| Step: 5
Training loss: 3.368989944458008
Validation loss: 2.3443809965605378

Epoch: 6| Step: 6
Training loss: 2.6457161903381348
Validation loss: 2.3506089948838755

Epoch: 6| Step: 7
Training loss: 1.6141865253448486
Validation loss: 2.341194086177375

Epoch: 6| Step: 8
Training loss: 2.284322500228882
Validation loss: 2.3469757033932592

Epoch: 6| Step: 9
Training loss: 3.298323154449463
Validation loss: 2.344311998736474

Epoch: 6| Step: 10
Training loss: 3.1024250984191895
Validation loss: 2.3479867930053384

Epoch: 6| Step: 11
Training loss: 2.2320709228515625
Validation loss: 2.3442383479046565

Epoch: 6| Step: 12
Training loss: 2.2880780696868896
Validation loss: 2.3543951716474307

Epoch: 6| Step: 13
Training loss: 3.2594032287597656
Validation loss: 2.352714115573514

Epoch: 181| Step: 0
Training loss: 2.4941861629486084
Validation loss: 2.363429573274428

Epoch: 6| Step: 1
Training loss: 3.2336766719818115
Validation loss: 2.3577290170936176

Epoch: 6| Step: 2
Training loss: 2.8037071228027344
Validation loss: 2.36280180305563

Epoch: 6| Step: 3
Training loss: 2.610576629638672
Validation loss: 2.369143129676901

Epoch: 6| Step: 4
Training loss: 2.996127128601074
Validation loss: 2.366832128135107

Epoch: 6| Step: 5
Training loss: 2.281414747238159
Validation loss: 2.3507040034058275

Epoch: 6| Step: 6
Training loss: 2.5938010215759277
Validation loss: 2.3716552552356513

Epoch: 6| Step: 7
Training loss: 1.8037281036376953
Validation loss: 2.36022763611168

Epoch: 6| Step: 8
Training loss: 2.6974921226501465
Validation loss: 2.3730116275049027

Epoch: 6| Step: 9
Training loss: 2.520571708679199
Validation loss: 2.3805806072809363

Epoch: 6| Step: 10
Training loss: 2.5986328125
Validation loss: 2.3723405074047785

Epoch: 6| Step: 11
Training loss: 2.1725058555603027
Validation loss: 2.3682537822313208

Epoch: 6| Step: 12
Training loss: 2.362457752227783
Validation loss: 2.3558521552752425

Epoch: 6| Step: 13
Training loss: 2.361722946166992
Validation loss: 2.3610449324371996

Epoch: 182| Step: 0
Training loss: 2.5166361331939697
Validation loss: 2.3562024408771145

Epoch: 6| Step: 1
Training loss: 2.8550124168395996
Validation loss: 2.343432216234105

Epoch: 6| Step: 2
Training loss: 1.828867793083191
Validation loss: 2.3496893118786555

Epoch: 6| Step: 3
Training loss: 2.3587827682495117
Validation loss: 2.3419478375424623

Epoch: 6| Step: 4
Training loss: 2.6102898120880127
Validation loss: 2.3438218998652633

Epoch: 6| Step: 5
Training loss: 2.8907418251037598
Validation loss: 2.3541427940450688

Epoch: 6| Step: 6
Training loss: 2.2144882678985596
Validation loss: 2.353991236737979

Epoch: 6| Step: 7
Training loss: 2.4054975509643555
Validation loss: 2.349130553583945

Epoch: 6| Step: 8
Training loss: 2.445889949798584
Validation loss: 2.3620315777358187

Epoch: 6| Step: 9
Training loss: 2.7947802543640137
Validation loss: 2.356857204949984

Epoch: 6| Step: 10
Training loss: 2.613830089569092
Validation loss: 2.369496319883613

Epoch: 6| Step: 11
Training loss: 2.8481688499450684
Validation loss: 2.3612401895625617

Epoch: 6| Step: 12
Training loss: 2.4921953678131104
Validation loss: 2.367244594840593

Epoch: 6| Step: 13
Training loss: 2.877025604248047
Validation loss: 2.3534456914471042

Epoch: 183| Step: 0
Training loss: 2.7156553268432617
Validation loss: 2.3625407424024356

Epoch: 6| Step: 1
Training loss: 3.0226001739501953
Validation loss: 2.3531046708424888

Epoch: 6| Step: 2
Training loss: 2.5490622520446777
Validation loss: 2.357392439278223

Epoch: 6| Step: 3
Training loss: 1.8919072151184082
Validation loss: 2.3520288826316915

Epoch: 6| Step: 4
Training loss: 2.3428707122802734
Validation loss: 2.3479203254945817

Epoch: 6| Step: 5
Training loss: 2.3053481578826904
Validation loss: 2.356284713232389

Epoch: 6| Step: 6
Training loss: 2.702360153198242
Validation loss: 2.364142994726858

Epoch: 6| Step: 7
Training loss: 2.3912787437438965
Validation loss: 2.3531161431343324

Epoch: 6| Step: 8
Training loss: 2.9086484909057617
Validation loss: 2.361392169870356

Epoch: 6| Step: 9
Training loss: 2.4429948329925537
Validation loss: 2.3595953295307774

Epoch: 6| Step: 10
Training loss: 2.4562439918518066
Validation loss: 2.3435433423647316

Epoch: 6| Step: 11
Training loss: 2.8621556758880615
Validation loss: 2.349131235512354

Epoch: 6| Step: 12
Training loss: 2.4480438232421875
Validation loss: 2.360591455172467

Epoch: 6| Step: 13
Training loss: 2.308713436126709
Validation loss: 2.350083089643909

Epoch: 184| Step: 0
Training loss: 2.715409278869629
Validation loss: 2.3485011862170313

Epoch: 6| Step: 1
Training loss: 2.3609061241149902
Validation loss: 2.352504225187404

Epoch: 6| Step: 2
Training loss: 2.470257043838501
Validation loss: 2.3570753528225805

Epoch: 6| Step: 3
Training loss: 2.6185131072998047
Validation loss: 2.3514487153740338

Epoch: 6| Step: 4
Training loss: 2.469088077545166
Validation loss: 2.3448295106169996

Epoch: 6| Step: 5
Training loss: 2.1941850185394287
Validation loss: 2.358249646361156

Epoch: 6| Step: 6
Training loss: 2.7077884674072266
Validation loss: 2.3480532912797827

Epoch: 6| Step: 7
Training loss: 2.979952573776245
Validation loss: 2.3503041780123146

Epoch: 6| Step: 8
Training loss: 2.7360939979553223
Validation loss: 2.3662825425465903

Epoch: 6| Step: 9
Training loss: 2.8166513442993164
Validation loss: 2.3588547322057907

Epoch: 6| Step: 10
Training loss: 2.7461299896240234
Validation loss: 2.3450560569763184

Epoch: 6| Step: 11
Training loss: 1.7994266748428345
Validation loss: 2.356868164513701

Epoch: 6| Step: 12
Training loss: 3.0703999996185303
Validation loss: 2.3514480590820312

Epoch: 6| Step: 13
Training loss: 1.1948095560073853
Validation loss: 2.351065210116807

Epoch: 185| Step: 0
Training loss: 2.5695149898529053
Validation loss: 2.3565364806882796

Epoch: 6| Step: 1
Training loss: 2.2495760917663574
Validation loss: 2.3601557413736978

Epoch: 6| Step: 2
Training loss: 2.9257373809814453
Validation loss: 2.3683512774846887

Epoch: 6| Step: 3
Training loss: 2.94726300239563
Validation loss: 2.3566816929847962

Epoch: 6| Step: 4
Training loss: 2.244597911834717
Validation loss: 2.3597169486425256

Epoch: 6| Step: 5
Training loss: 2.6161062717437744
Validation loss: 2.367754723436089

Epoch: 6| Step: 6
Training loss: 2.6141433715820312
Validation loss: 2.354781391800091

Epoch: 6| Step: 7
Training loss: 2.352027416229248
Validation loss: 2.344945423064693

Epoch: 6| Step: 8
Training loss: 2.325038194656372
Validation loss: 2.3397284118078088

Epoch: 6| Step: 9
Training loss: 2.467463493347168
Validation loss: 2.325081322782783

Epoch: 6| Step: 10
Training loss: 2.4174513816833496
Validation loss: 2.3298716724559827

Epoch: 6| Step: 11
Training loss: 2.690809488296509
Validation loss: 2.3268766249379804

Epoch: 6| Step: 12
Training loss: 2.861515998840332
Validation loss: 2.316469761633104

Epoch: 6| Step: 13
Training loss: 2.245455503463745
Validation loss: 2.3251538327945176

Epoch: 186| Step: 0
Training loss: 2.528156042098999
Validation loss: 2.343123197555542

Epoch: 6| Step: 1
Training loss: 2.830042839050293
Validation loss: 2.349271374364053

Epoch: 6| Step: 2
Training loss: 2.4862046241760254
Validation loss: 2.345189325271114

Epoch: 6| Step: 3
Training loss: 2.925171375274658
Validation loss: 2.362032180191368

Epoch: 6| Step: 4
Training loss: 2.4930124282836914
Validation loss: 2.3569340141870643

Epoch: 6| Step: 5
Training loss: 2.3716158866882324
Validation loss: 2.36076283454895

Epoch: 6| Step: 6
Training loss: 2.6289126873016357
Validation loss: 2.3424022223359797

Epoch: 6| Step: 7
Training loss: 2.290604591369629
Validation loss: 2.352807862784273

Epoch: 6| Step: 8
Training loss: 2.27718186378479
Validation loss: 2.33793657313111

Epoch: 6| Step: 9
Training loss: 2.4832851886749268
Validation loss: 2.3348495703871532

Epoch: 6| Step: 10
Training loss: 2.134634256362915
Validation loss: 2.333577029166683

Epoch: 6| Step: 11
Training loss: 2.945787191390991
Validation loss: 2.3442250477370394

Epoch: 6| Step: 12
Training loss: 2.761012554168701
Validation loss: 2.3366408066083024

Epoch: 6| Step: 13
Training loss: 2.141779661178589
Validation loss: 2.3381548132947696

Epoch: 187| Step: 0
Training loss: 1.5757843255996704
Validation loss: 2.343569876045309

Epoch: 6| Step: 1
Training loss: 2.559251070022583
Validation loss: 2.3400998807722524

Epoch: 6| Step: 2
Training loss: 2.7664551734924316
Validation loss: 2.340221563975016

Epoch: 6| Step: 3
Training loss: 2.2640504837036133
Validation loss: 2.3415547211964927

Epoch: 6| Step: 4
Training loss: 2.1182610988616943
Validation loss: 2.339093992787023

Epoch: 6| Step: 5
Training loss: 2.9493870735168457
Validation loss: 2.3413524345685075

Epoch: 6| Step: 6
Training loss: 2.1727399826049805
Validation loss: 2.334993131699101

Epoch: 6| Step: 7
Training loss: 2.402790069580078
Validation loss: 2.342254836072204

Epoch: 6| Step: 8
Training loss: 2.756444215774536
Validation loss: 2.3336742872832925

Epoch: 6| Step: 9
Training loss: 2.443911075592041
Validation loss: 2.3379271389335714

Epoch: 6| Step: 10
Training loss: 2.378018379211426
Validation loss: 2.338973770859421

Epoch: 6| Step: 11
Training loss: 3.2326319217681885
Validation loss: 2.335867968938684

Epoch: 6| Step: 12
Training loss: 2.93801212310791
Validation loss: 2.339774118956699

Epoch: 6| Step: 13
Training loss: 3.100275993347168
Validation loss: 2.339804818553309

Epoch: 188| Step: 0
Training loss: 1.804516077041626
Validation loss: 2.33775318566189

Epoch: 6| Step: 1
Training loss: 2.4532241821289062
Validation loss: 2.348418866434405

Epoch: 6| Step: 2
Training loss: 2.1423399448394775
Validation loss: 2.3540103153515886

Epoch: 6| Step: 3
Training loss: 2.60142183303833
Validation loss: 2.346347652455812

Epoch: 6| Step: 4
Training loss: 2.2767481803894043
Validation loss: 2.3496332912034887

Epoch: 6| Step: 5
Training loss: 3.217949628829956
Validation loss: 2.3418837875448246

Epoch: 6| Step: 6
Training loss: 2.1203551292419434
Validation loss: 2.3408464424071775

Epoch: 6| Step: 7
Training loss: 2.4545817375183105
Validation loss: 2.3383314442890946

Epoch: 6| Step: 8
Training loss: 2.9457082748413086
Validation loss: 2.3465120818025325

Epoch: 6| Step: 9
Training loss: 2.6065833568573
Validation loss: 2.3433576142916115

Epoch: 6| Step: 10
Training loss: 2.6697378158569336
Validation loss: 2.338611441273843

Epoch: 6| Step: 11
Training loss: 2.7982468605041504
Validation loss: 2.3355339970639957

Epoch: 6| Step: 12
Training loss: 2.8424997329711914
Validation loss: 2.349788652953281

Epoch: 6| Step: 13
Training loss: 2.3233089447021484
Validation loss: 2.337729525822465

Epoch: 189| Step: 0
Training loss: 2.89011287689209
Validation loss: 2.340763102295578

Epoch: 6| Step: 1
Training loss: 2.4068737030029297
Validation loss: 2.3411800476812545

Epoch: 6| Step: 2
Training loss: 2.482760429382324
Validation loss: 2.3278964129827355

Epoch: 6| Step: 3
Training loss: 2.1172375679016113
Validation loss: 2.3301182818669144

Epoch: 6| Step: 4
Training loss: 3.3632524013519287
Validation loss: 2.319790904239942

Epoch: 6| Step: 5
Training loss: 2.3165957927703857
Validation loss: 2.3298490867819837

Epoch: 6| Step: 6
Training loss: 2.483114242553711
Validation loss: 2.3245229695432927

Epoch: 6| Step: 7
Training loss: 2.934293031692505
Validation loss: 2.3326444446399646

Epoch: 6| Step: 8
Training loss: 1.9367958307266235
Validation loss: 2.3405671452963226

Epoch: 6| Step: 9
Training loss: 2.53194522857666
Validation loss: 2.337163679061397

Epoch: 6| Step: 10
Training loss: 2.0728960037231445
Validation loss: 2.331756040614138

Epoch: 6| Step: 11
Training loss: 2.656841278076172
Validation loss: 2.3247012451130855

Epoch: 6| Step: 12
Training loss: 2.5376806259155273
Validation loss: 2.344602323347522

Epoch: 6| Step: 13
Training loss: 2.744187355041504
Validation loss: 2.3396661076494443

Epoch: 190| Step: 0
Training loss: 2.3561205863952637
Validation loss: 2.3386205909072713

Epoch: 6| Step: 1
Training loss: 1.5033040046691895
Validation loss: 2.3515621705721785

Epoch: 6| Step: 2
Training loss: 1.7135035991668701
Validation loss: 2.3447728182679866

Epoch: 6| Step: 3
Training loss: 2.3923180103302
Validation loss: 2.3544586063713155

Epoch: 6| Step: 4
Training loss: 2.301583766937256
Validation loss: 2.3532690822437243

Epoch: 6| Step: 5
Training loss: 2.954357624053955
Validation loss: 2.3651283325687533

Epoch: 6| Step: 6
Training loss: 2.6985526084899902
Validation loss: 2.364184541086997

Epoch: 6| Step: 7
Training loss: 2.4400899410247803
Validation loss: 2.3422980898170063

Epoch: 6| Step: 8
Training loss: 3.1551475524902344
Validation loss: 2.3398628978319067

Epoch: 6| Step: 9
Training loss: 2.7426271438598633
Validation loss: 2.324340944649071

Epoch: 6| Step: 10
Training loss: 2.550360918045044
Validation loss: 2.3238189451156126

Epoch: 6| Step: 11
Training loss: 2.8343911170959473
Validation loss: 2.3359411198605775

Epoch: 6| Step: 12
Training loss: 3.0408401489257812
Validation loss: 2.354927198861235

Epoch: 6| Step: 13
Training loss: 3.1257739067077637
Validation loss: 2.354323858855873

Epoch: 191| Step: 0
Training loss: 2.319711208343506
Validation loss: 2.3680325861900084

Epoch: 6| Step: 1
Training loss: 2.3425545692443848
Validation loss: 2.3805396864491124

Epoch: 6| Step: 2
Training loss: 2.2843079566955566
Validation loss: 2.3810160903520483

Epoch: 6| Step: 3
Training loss: 2.9010841846466064
Validation loss: 2.3660221330581175

Epoch: 6| Step: 4
Training loss: 2.3098397254943848
Validation loss: 2.3617278375933246

Epoch: 6| Step: 5
Training loss: 3.3684911727905273
Validation loss: 2.361844757551788

Epoch: 6| Step: 6
Training loss: 2.631789445877075
Validation loss: 2.35533308470121

Epoch: 6| Step: 7
Training loss: 2.634394645690918
Validation loss: 2.351047783769587

Epoch: 6| Step: 8
Training loss: 2.4990570545196533
Validation loss: 2.354071230016729

Epoch: 6| Step: 9
Training loss: 2.695858955383301
Validation loss: 2.3399162446298907

Epoch: 6| Step: 10
Training loss: 2.6462454795837402
Validation loss: 2.328680715253276

Epoch: 6| Step: 11
Training loss: 2.078878879547119
Validation loss: 2.3364794792667514

Epoch: 6| Step: 12
Training loss: 2.1767630577087402
Validation loss: 2.3360393944606987

Epoch: 6| Step: 13
Training loss: 3.0539236068725586
Validation loss: 2.3467985122434554

Epoch: 192| Step: 0
Training loss: 2.5420122146606445
Validation loss: 2.3544540789819535

Epoch: 6| Step: 1
Training loss: 2.7489733695983887
Validation loss: 2.368740350969376

Epoch: 6| Step: 2
Training loss: 2.8114519119262695
Validation loss: 2.381978355428224

Epoch: 6| Step: 3
Training loss: 2.3659725189208984
Validation loss: 2.38226604718034

Epoch: 6| Step: 4
Training loss: 2.379885673522949
Validation loss: 2.376746221255231

Epoch: 6| Step: 5
Training loss: 2.4329614639282227
Validation loss: 2.3614157784369683

Epoch: 6| Step: 6
Training loss: 2.9775595664978027
Validation loss: 2.3567857229581444

Epoch: 6| Step: 7
Training loss: 2.7968220710754395
Validation loss: 2.3454417618372108

Epoch: 6| Step: 8
Training loss: 2.189253807067871
Validation loss: 2.331459294083298

Epoch: 6| Step: 9
Training loss: 2.5912365913391113
Validation loss: 2.326428782555365

Epoch: 6| Step: 10
Training loss: 2.438549518585205
Validation loss: 2.328432952204058

Epoch: 6| Step: 11
Training loss: 2.7731659412384033
Validation loss: 2.3210214081630913

Epoch: 6| Step: 12
Training loss: 2.057586669921875
Validation loss: 2.317765553792318

Epoch: 6| Step: 13
Training loss: 2.277414083480835
Validation loss: 2.314536997067031

Epoch: 193| Step: 0
Training loss: 2.811986207962036
Validation loss: 2.3259807248269357

Epoch: 6| Step: 1
Training loss: 2.8466877937316895
Validation loss: 2.3195743201881327

Epoch: 6| Step: 2
Training loss: 1.5159598588943481
Validation loss: 2.3150445902219383

Epoch: 6| Step: 3
Training loss: 2.6868433952331543
Validation loss: 2.320026489996141

Epoch: 6| Step: 4
Training loss: 2.4826807975769043
Validation loss: 2.316860883466659

Epoch: 6| Step: 5
Training loss: 3.425715446472168
Validation loss: 2.322327665103379

Epoch: 6| Step: 6
Training loss: 2.914137840270996
Validation loss: 2.3188794787212084

Epoch: 6| Step: 7
Training loss: 2.8171684741973877
Validation loss: 2.327269002955447

Epoch: 6| Step: 8
Training loss: 2.4455060958862305
Validation loss: 2.3171304643795056

Epoch: 6| Step: 9
Training loss: 1.944128155708313
Validation loss: 2.3281815539124193

Epoch: 6| Step: 10
Training loss: 2.8688254356384277
Validation loss: 2.325967924569243

Epoch: 6| Step: 11
Training loss: 2.036176919937134
Validation loss: 2.33841121837657

Epoch: 6| Step: 12
Training loss: 2.5694899559020996
Validation loss: 2.3374696854622132

Epoch: 6| Step: 13
Training loss: 1.7752108573913574
Validation loss: 2.34089901626751

Epoch: 194| Step: 0
Training loss: 2.7447502613067627
Validation loss: 2.3312992459984234

Epoch: 6| Step: 1
Training loss: 3.020082473754883
Validation loss: 2.3407926636357463

Epoch: 6| Step: 2
Training loss: 3.483516216278076
Validation loss: 2.3326849091437554

Epoch: 6| Step: 3
Training loss: 2.618082046508789
Validation loss: 2.3402020033969673

Epoch: 6| Step: 4
Training loss: 2.1700947284698486
Validation loss: 2.338553213304089

Epoch: 6| Step: 5
Training loss: 2.7283506393432617
Validation loss: 2.3356606806478193

Epoch: 6| Step: 6
Training loss: 2.006504535675049
Validation loss: 2.3401109633907193

Epoch: 6| Step: 7
Training loss: 2.5936801433563232
Validation loss: 2.341898323387228

Epoch: 6| Step: 8
Training loss: 1.7564213275909424
Validation loss: 2.3263312309019026

Epoch: 6| Step: 9
Training loss: 3.285332202911377
Validation loss: 2.333769308623447

Epoch: 6| Step: 10
Training loss: 2.068281412124634
Validation loss: 2.326464354351003

Epoch: 6| Step: 11
Training loss: 1.894266128540039
Validation loss: 2.3387506213239444

Epoch: 6| Step: 12
Training loss: 2.3653321266174316
Validation loss: 2.325405623323174

Epoch: 6| Step: 13
Training loss: 2.4274075031280518
Validation loss: 2.3264058238716534

Epoch: 195| Step: 0
Training loss: 2.868706464767456
Validation loss: 2.330021814633441

Epoch: 6| Step: 1
Training loss: 2.7660887241363525
Validation loss: 2.3310361869873537

Epoch: 6| Step: 2
Training loss: 3.182809829711914
Validation loss: 2.336307305161671

Epoch: 6| Step: 3
Training loss: 2.5591535568237305
Validation loss: 2.3224970243310414

Epoch: 6| Step: 4
Training loss: 2.79826021194458
Validation loss: 2.3341440436660603

Epoch: 6| Step: 5
Training loss: 2.1244771480560303
Validation loss: 2.331602483667353

Epoch: 6| Step: 6
Training loss: 3.1747066974639893
Validation loss: 2.326467337146882

Epoch: 6| Step: 7
Training loss: 2.8349876403808594
Validation loss: 2.3159582589262273

Epoch: 6| Step: 8
Training loss: 1.5451589822769165
Validation loss: 2.3144513689061648

Epoch: 6| Step: 9
Training loss: 1.7548353672027588
Validation loss: 2.3179441549444713

Epoch: 6| Step: 10
Training loss: 2.955709457397461
Validation loss: 2.3137623853580926

Epoch: 6| Step: 11
Training loss: 2.570897102355957
Validation loss: 2.3187629638179654

Epoch: 6| Step: 12
Training loss: 2.1518819332122803
Validation loss: 2.3221032798931165

Epoch: 6| Step: 13
Training loss: 1.5955841541290283
Validation loss: 2.3242693152478946

Epoch: 196| Step: 0
Training loss: 3.120901584625244
Validation loss: 2.3091615579461537

Epoch: 6| Step: 1
Training loss: 2.6220932006835938
Validation loss: 2.3436599905772875

Epoch: 6| Step: 2
Training loss: 1.6589305400848389
Validation loss: 2.3405847549438477

Epoch: 6| Step: 3
Training loss: 2.556955337524414
Validation loss: 2.333419599840718

Epoch: 6| Step: 4
Training loss: 3.0915684700012207
Validation loss: 2.3359471469797115

Epoch: 6| Step: 5
Training loss: 2.134288787841797
Validation loss: 2.332938978748937

Epoch: 6| Step: 6
Training loss: 1.775228500366211
Validation loss: 2.321481840584868

Epoch: 6| Step: 7
Training loss: 3.2031545639038086
Validation loss: 2.335286791606616

Epoch: 6| Step: 8
Training loss: 3.013824462890625
Validation loss: 2.3417743841807046

Epoch: 6| Step: 9
Training loss: 2.172846794128418
Validation loss: 2.3414805576365483

Epoch: 6| Step: 10
Training loss: 2.408918857574463
Validation loss: 2.346104973105974

Epoch: 6| Step: 11
Training loss: 2.079085350036621
Validation loss: 2.342031442990867

Epoch: 6| Step: 12
Training loss: 2.7072689533233643
Validation loss: 2.331294023862449

Epoch: 6| Step: 13
Training loss: 2.7610158920288086
Validation loss: 2.3385333117618354

Epoch: 197| Step: 0
Training loss: 2.76248836517334
Validation loss: 2.328110533375894

Epoch: 6| Step: 1
Training loss: 3.0273027420043945
Validation loss: 2.315348853347122

Epoch: 6| Step: 2
Training loss: 2.2902207374572754
Validation loss: 2.3252535173969884

Epoch: 6| Step: 3
Training loss: 2.711142063140869
Validation loss: 2.3323395713683097

Epoch: 6| Step: 4
Training loss: 1.9174290895462036
Validation loss: 2.334926087369201

Epoch: 6| Step: 5
Training loss: 2.8872628211975098
Validation loss: 2.3370951478199293

Epoch: 6| Step: 6
Training loss: 2.593294143676758
Validation loss: 2.3444148494351293

Epoch: 6| Step: 7
Training loss: 2.305860757827759
Validation loss: 2.343309235829179

Epoch: 6| Step: 8
Training loss: 2.4401137828826904
Validation loss: 2.341962645130773

Epoch: 6| Step: 9
Training loss: 1.761265516281128
Validation loss: 2.3347578279433714

Epoch: 6| Step: 10
Training loss: 3.1989893913269043
Validation loss: 2.338385958825388

Epoch: 6| Step: 11
Training loss: 2.7691900730133057
Validation loss: 2.3173280736451507

Epoch: 6| Step: 12
Training loss: 2.649412155151367
Validation loss: 2.3167615218829085

Epoch: 6| Step: 13
Training loss: 2.135366201400757
Validation loss: 2.299275771264107

Epoch: 198| Step: 0
Training loss: 2.779567241668701
Validation loss: 2.298328927768174

Epoch: 6| Step: 1
Training loss: 3.446178436279297
Validation loss: 2.302368425553845

Epoch: 6| Step: 2
Training loss: 3.6352698802948
Validation loss: 2.298147193847164

Epoch: 6| Step: 3
Training loss: 2.021920680999756
Validation loss: 2.300126398763349

Epoch: 6| Step: 4
Training loss: 1.6630010604858398
Validation loss: 2.296866455385762

Epoch: 6| Step: 5
Training loss: 2.9435324668884277
Validation loss: 2.305961439686437

Epoch: 6| Step: 6
Training loss: 2.660632610321045
Validation loss: 2.307374564550256

Epoch: 6| Step: 7
Training loss: 2.3810720443725586
Validation loss: 2.30776672978555

Epoch: 6| Step: 8
Training loss: 2.4368233680725098
Validation loss: 2.3108166904859644

Epoch: 6| Step: 9
Training loss: 2.4105381965637207
Validation loss: 2.307236122828658

Epoch: 6| Step: 10
Training loss: 2.2409920692443848
Validation loss: 2.3268498964207147

Epoch: 6| Step: 11
Training loss: 2.16087007522583
Validation loss: 2.3158093652417584

Epoch: 6| Step: 12
Training loss: 1.8951904773712158
Validation loss: 2.310871330640649

Epoch: 6| Step: 13
Training loss: 2.970860481262207
Validation loss: 2.3033697630769465

Epoch: 199| Step: 0
Training loss: 3.1352109909057617
Validation loss: 2.3064250202589136

Epoch: 6| Step: 1
Training loss: 3.419840097427368
Validation loss: 2.296079222873975

Epoch: 6| Step: 2
Training loss: 2.125819683074951
Validation loss: 2.3012564618100404

Epoch: 6| Step: 3
Training loss: 2.888132333755493
Validation loss: 2.3096639328105475

Epoch: 6| Step: 4
Training loss: 2.9341278076171875
Validation loss: 2.3086501629121843

Epoch: 6| Step: 5
Training loss: 2.3991432189941406
Validation loss: 2.317112535558721

Epoch: 6| Step: 6
Training loss: 2.4816577434539795
Validation loss: 2.3245236745444675

Epoch: 6| Step: 7
Training loss: 2.2495269775390625
Validation loss: 2.3227746589209444

Epoch: 6| Step: 8
Training loss: 2.372664213180542
Validation loss: 2.3276534618869906

Epoch: 6| Step: 9
Training loss: 1.6075202226638794
Validation loss: 2.3244825255486274

Epoch: 6| Step: 10
Training loss: 2.806342840194702
Validation loss: 2.32163986339364

Epoch: 6| Step: 11
Training loss: 1.9302849769592285
Validation loss: 2.3309834798177085

Epoch: 6| Step: 12
Training loss: 2.681950569152832
Validation loss: 2.3268497144022295

Epoch: 6| Step: 13
Training loss: 2.1728732585906982
Validation loss: 2.333228482994982

Epoch: 200| Step: 0
Training loss: 1.6019103527069092
Validation loss: 2.327573045607536

Epoch: 6| Step: 1
Training loss: 3.009343147277832
Validation loss: 2.3261720954730944

Epoch: 6| Step: 2
Training loss: 2.8193411827087402
Validation loss: 2.3265860926720405

Epoch: 6| Step: 3
Training loss: 3.04636812210083
Validation loss: 2.32585939925204

Epoch: 6| Step: 4
Training loss: 2.921152114868164
Validation loss: 2.3090152048295542

Epoch: 6| Step: 5
Training loss: 1.775091528892517
Validation loss: 2.3108969990925123

Epoch: 6| Step: 6
Training loss: 2.8218185901641846
Validation loss: 2.295026451028803

Epoch: 6| Step: 7
Training loss: 2.062333106994629
Validation loss: 2.303505277120939

Epoch: 6| Step: 8
Training loss: 2.099184036254883
Validation loss: 2.3026558327418503

Epoch: 6| Step: 9
Training loss: 2.724020004272461
Validation loss: 2.3054408745099138

Epoch: 6| Step: 10
Training loss: 2.9098052978515625
Validation loss: 2.3136325241417013

Epoch: 6| Step: 11
Training loss: 2.22403621673584
Validation loss: 2.322945830642536

Epoch: 6| Step: 12
Training loss: 2.478987216949463
Validation loss: 2.29976478956079

Epoch: 6| Step: 13
Training loss: 2.806882381439209
Validation loss: 2.3058805670789493

Epoch: 201| Step: 0
Training loss: 2.426319122314453
Validation loss: 2.310600755035236

Epoch: 6| Step: 1
Training loss: 2.0977656841278076
Validation loss: 2.296296811872913

Epoch: 6| Step: 2
Training loss: 3.210296154022217
Validation loss: 2.2929400013339136

Epoch: 6| Step: 3
Training loss: 3.395956516265869
Validation loss: 2.2875315245761665

Epoch: 6| Step: 4
Training loss: 2.7674615383148193
Validation loss: 2.2882464008946575

Epoch: 6| Step: 5
Training loss: 2.310262680053711
Validation loss: 2.2870102236347813

Epoch: 6| Step: 6
Training loss: 2.3682188987731934
Validation loss: 2.284720318291777

Epoch: 6| Step: 7
Training loss: 1.7965210676193237
Validation loss: 2.2871896630974224

Epoch: 6| Step: 8
Training loss: 2.7512173652648926
Validation loss: 2.296971918434225

Epoch: 6| Step: 9
Training loss: 2.4952244758605957
Validation loss: 2.2876923699532785

Epoch: 6| Step: 10
Training loss: 2.4021151065826416
Validation loss: 2.287712515041392

Epoch: 6| Step: 11
Training loss: 2.017376184463501
Validation loss: 2.3000678990476873

Epoch: 6| Step: 12
Training loss: 3.0568408966064453
Validation loss: 2.292995527226438

Epoch: 6| Step: 13
Training loss: 2.037313461303711
Validation loss: 2.3100263969872588

Epoch: 202| Step: 0
Training loss: 2.014042377471924
Validation loss: 2.304208001782817

Epoch: 6| Step: 1
Training loss: 2.4591684341430664
Validation loss: 2.3132973230013283

Epoch: 6| Step: 2
Training loss: 3.152987480163574
Validation loss: 2.330827038775208

Epoch: 6| Step: 3
Training loss: 2.1682186126708984
Validation loss: 2.332424627837314

Epoch: 6| Step: 4
Training loss: 2.072734832763672
Validation loss: 2.3606496164875646

Epoch: 6| Step: 5
Training loss: 3.262503147125244
Validation loss: 2.370150271282401

Epoch: 6| Step: 6
Training loss: 2.092503786087036
Validation loss: 2.377670854650518

Epoch: 6| Step: 7
Training loss: 2.920433521270752
Validation loss: 2.3907699123505624

Epoch: 6| Step: 8
Training loss: 2.866887092590332
Validation loss: 2.377044762334516

Epoch: 6| Step: 9
Training loss: 2.883028507232666
Validation loss: 2.364822300531531

Epoch: 6| Step: 10
Training loss: 1.8545132875442505
Validation loss: 2.368932654780726

Epoch: 6| Step: 11
Training loss: 2.763522148132324
Validation loss: 2.3671315254703647

Epoch: 6| Step: 12
Training loss: 2.4748806953430176
Validation loss: 2.341342756825109

Epoch: 6| Step: 13
Training loss: 2.1897480487823486
Validation loss: 2.3354972972664783

Epoch: 203| Step: 0
Training loss: 2.4209506511688232
Validation loss: 2.317643283515848

Epoch: 6| Step: 1
Training loss: 3.128751754760742
Validation loss: 2.3151918995764946

Epoch: 6| Step: 2
Training loss: 2.63836669921875
Validation loss: 2.321505072296307

Epoch: 6| Step: 3
Training loss: 2.921321392059326
Validation loss: 2.32423508808177

Epoch: 6| Step: 4
Training loss: 1.729051113128662
Validation loss: 2.3125456469033354

Epoch: 6| Step: 5
Training loss: 2.3850584030151367
Validation loss: 2.318166696897117

Epoch: 6| Step: 6
Training loss: 2.700953960418701
Validation loss: 2.3196363910551994

Epoch: 6| Step: 7
Training loss: 3.2721476554870605
Validation loss: 2.3208236937881797

Epoch: 6| Step: 8
Training loss: 2.4193975925445557
Validation loss: 2.3250129274142686

Epoch: 6| Step: 9
Training loss: 1.5671041011810303
Validation loss: 2.316783592265139

Epoch: 6| Step: 10
Training loss: 3.1724565029144287
Validation loss: 2.309138636435232

Epoch: 6| Step: 11
Training loss: 1.9997508525848389
Validation loss: 2.3089747762167327

Epoch: 6| Step: 12
Training loss: 2.849398612976074
Validation loss: 2.315822673100297

Epoch: 6| Step: 13
Training loss: 1.5362401008605957
Validation loss: 2.317272452897923

Epoch: 204| Step: 0
Training loss: 2.5304737091064453
Validation loss: 2.321602747004519

Epoch: 6| Step: 1
Training loss: 2.937918186187744
Validation loss: 2.3063445475793656

Epoch: 6| Step: 2
Training loss: 2.907688856124878
Validation loss: 2.3078907330830893

Epoch: 6| Step: 3
Training loss: 2.4874212741851807
Validation loss: 2.3147277088575464

Epoch: 6| Step: 4
Training loss: 2.8907594680786133
Validation loss: 2.3153340380678893

Epoch: 6| Step: 5
Training loss: 2.8867547512054443
Validation loss: 2.314518023562688

Epoch: 6| Step: 6
Training loss: 1.6757125854492188
Validation loss: 2.3015828901721584

Epoch: 6| Step: 7
Training loss: 1.8396661281585693
Validation loss: 2.3129766243760304

Epoch: 6| Step: 8
Training loss: 2.492079019546509
Validation loss: 2.3158598433258715

Epoch: 6| Step: 9
Training loss: 2.5795986652374268
Validation loss: 2.3239679105820192

Epoch: 6| Step: 10
Training loss: 1.8946741819381714
Validation loss: 2.3221057691881732

Epoch: 6| Step: 11
Training loss: 2.5791704654693604
Validation loss: 2.3137085976139193

Epoch: 6| Step: 12
Training loss: 2.674529552459717
Validation loss: 2.3136817729601296

Epoch: 6| Step: 13
Training loss: 2.620177745819092
Validation loss: 2.3026858837373796

Epoch: 205| Step: 0
Training loss: 1.890587568283081
Validation loss: 2.301338198364422

Epoch: 6| Step: 1
Training loss: 2.6680240631103516
Validation loss: 2.3061971638792302

Epoch: 6| Step: 2
Training loss: 2.721346855163574
Validation loss: 2.301162137780138

Epoch: 6| Step: 3
Training loss: 3.0670907497406006
Validation loss: 2.29493889885564

Epoch: 6| Step: 4
Training loss: 2.6493289470672607
Validation loss: 2.2970096424061763

Epoch: 6| Step: 5
Training loss: 2.7152321338653564
Validation loss: 2.30377709609206

Epoch: 6| Step: 6
Training loss: 2.6331074237823486
Validation loss: 2.2859861850738525

Epoch: 6| Step: 7
Training loss: 2.472574472427368
Validation loss: 2.290872320052116

Epoch: 6| Step: 8
Training loss: 2.5611515045166016
Validation loss: 2.28827844127532

Epoch: 6| Step: 9
Training loss: 2.631997585296631
Validation loss: 2.2883800050263763

Epoch: 6| Step: 10
Training loss: 2.0293917655944824
Validation loss: 2.30255215655091

Epoch: 6| Step: 11
Training loss: 2.6232082843780518
Validation loss: 2.31654199477165

Epoch: 6| Step: 12
Training loss: 1.4542109966278076
Validation loss: 2.338522623944026

Epoch: 6| Step: 13
Training loss: 3.0595927238464355
Validation loss: 2.3430007785879154

Epoch: 206| Step: 0
Training loss: 2.3884291648864746
Validation loss: 2.365795530298705

Epoch: 6| Step: 1
Training loss: 2.8776276111602783
Validation loss: 2.3696216434560795

Epoch: 6| Step: 2
Training loss: 2.9182796478271484
Validation loss: 2.3676657856151624

Epoch: 6| Step: 3
Training loss: 2.9556589126586914
Validation loss: 2.3388801428579513

Epoch: 6| Step: 4
Training loss: 1.9800947904586792
Validation loss: 2.310260513777374

Epoch: 6| Step: 5
Training loss: 1.781132459640503
Validation loss: 2.286507914143224

Epoch: 6| Step: 6
Training loss: 2.436460018157959
Validation loss: 2.277897798886863

Epoch: 6| Step: 7
Training loss: 2.8986663818359375
Validation loss: 2.2636539307973718

Epoch: 6| Step: 8
Training loss: 2.3208439350128174
Validation loss: 2.2779193334682013

Epoch: 6| Step: 9
Training loss: 2.9141125679016113
Validation loss: 2.2970168795636905

Epoch: 6| Step: 10
Training loss: 2.634584665298462
Validation loss: 2.3107894825679

Epoch: 6| Step: 11
Training loss: 2.8301401138305664
Validation loss: 2.315614628535445

Epoch: 6| Step: 12
Training loss: 2.6315088272094727
Validation loss: 2.3206384976704917

Epoch: 6| Step: 13
Training loss: 1.868836522102356
Validation loss: 2.3140269094897854

Epoch: 207| Step: 0
Training loss: 2.7509329319000244
Validation loss: 2.306653786731023

Epoch: 6| Step: 1
Training loss: 2.143472671508789
Validation loss: 2.304069777970673

Epoch: 6| Step: 2
Training loss: 2.6351852416992188
Validation loss: 2.2983557485765025

Epoch: 6| Step: 3
Training loss: 1.8280975818634033
Validation loss: 2.288448305540187

Epoch: 6| Step: 4
Training loss: 2.6900243759155273
Validation loss: 2.289596193580217

Epoch: 6| Step: 5
Training loss: 1.9347381591796875
Validation loss: 2.296239870850758

Epoch: 6| Step: 6
Training loss: 2.2213480472564697
Validation loss: 2.2857087683934036

Epoch: 6| Step: 7
Training loss: 2.061375856399536
Validation loss: 2.281292630780128

Epoch: 6| Step: 8
Training loss: 2.7944140434265137
Validation loss: 2.2874640880092496

Epoch: 6| Step: 9
Training loss: 2.9483349323272705
Validation loss: 2.290873227580901

Epoch: 6| Step: 10
Training loss: 3.315279960632324
Validation loss: 2.2878871220414356

Epoch: 6| Step: 11
Training loss: 2.375889778137207
Validation loss: 2.284628483556932

Epoch: 6| Step: 12
Training loss: 3.0200753211975098
Validation loss: 2.287364878962117

Epoch: 6| Step: 13
Training loss: 2.741508960723877
Validation loss: 2.3029896597708426

Epoch: 208| Step: 0
Training loss: 2.6331491470336914
Validation loss: 2.303172557584701

Epoch: 6| Step: 1
Training loss: 2.218611240386963
Validation loss: 2.2985060086814304

Epoch: 6| Step: 2
Training loss: 2.7266435623168945
Validation loss: 2.319712056908556

Epoch: 6| Step: 3
Training loss: 1.9121885299682617
Validation loss: 2.3108587444469495

Epoch: 6| Step: 4
Training loss: 2.5712075233459473
Validation loss: 2.3169172079332414

Epoch: 6| Step: 5
Training loss: 2.6405956745147705
Validation loss: 2.332555663201117

Epoch: 6| Step: 6
Training loss: 3.2345757484436035
Validation loss: 2.319531194625362

Epoch: 6| Step: 7
Training loss: 2.3318302631378174
Validation loss: 2.3099534152656473

Epoch: 6| Step: 8
Training loss: 2.046118974685669
Validation loss: 2.3025748601523777

Epoch: 6| Step: 9
Training loss: 2.770949363708496
Validation loss: 2.294963562360374

Epoch: 6| Step: 10
Training loss: 2.3735861778259277
Validation loss: 2.288720028374785

Epoch: 6| Step: 11
Training loss: 2.239574432373047
Validation loss: 2.2829894237620856

Epoch: 6| Step: 12
Training loss: 2.911278486251831
Validation loss: 2.2851324799240276

Epoch: 6| Step: 13
Training loss: 2.37241530418396
Validation loss: 2.290267505953389

Epoch: 209| Step: 0
Training loss: 2.2888545989990234
Validation loss: 2.2878775365890993

Epoch: 6| Step: 1
Training loss: 2.91031813621521
Validation loss: 2.2832691246463406

Epoch: 6| Step: 2
Training loss: 2.8702754974365234
Validation loss: 2.2927983012250674

Epoch: 6| Step: 3
Training loss: 2.1137592792510986
Validation loss: 2.299804518299718

Epoch: 6| Step: 4
Training loss: 2.946704864501953
Validation loss: 2.2937721206295874

Epoch: 6| Step: 5
Training loss: 2.4333910942077637
Validation loss: 2.2944885530779437

Epoch: 6| Step: 6
Training loss: 2.751222610473633
Validation loss: 2.289665640041392

Epoch: 6| Step: 7
Training loss: 1.9567553997039795
Validation loss: 2.2938528381368166

Epoch: 6| Step: 8
Training loss: 1.5429813861846924
Validation loss: 2.2989963959622126

Epoch: 6| Step: 9
Training loss: 2.996027946472168
Validation loss: 2.3000315850780857

Epoch: 6| Step: 10
Training loss: 2.280101776123047
Validation loss: 2.2918207696689072

Epoch: 6| Step: 11
Training loss: 2.243757724761963
Validation loss: 2.3018025993019022

Epoch: 6| Step: 12
Training loss: 2.7537431716918945
Validation loss: 2.294172692042525

Epoch: 6| Step: 13
Training loss: 3.1600356101989746
Validation loss: 2.3169806272752824

Epoch: 210| Step: 0
Training loss: 2.5033578872680664
Validation loss: 2.313893877049928

Epoch: 6| Step: 1
Training loss: 2.4314777851104736
Validation loss: 2.3254394838886876

Epoch: 6| Step: 2
Training loss: 2.251577377319336
Validation loss: 2.333778496711485

Epoch: 6| Step: 3
Training loss: 2.5300629138946533
Validation loss: 2.325957267515121

Epoch: 6| Step: 4
Training loss: 2.3534059524536133
Validation loss: 2.349105341460115

Epoch: 6| Step: 5
Training loss: 2.5804951190948486
Validation loss: 2.353337824985545

Epoch: 6| Step: 6
Training loss: 2.376701831817627
Validation loss: 2.329360051821637

Epoch: 6| Step: 7
Training loss: 2.5081686973571777
Validation loss: 2.3379282489899667

Epoch: 6| Step: 8
Training loss: 2.9291012287139893
Validation loss: 2.3116125150393416

Epoch: 6| Step: 9
Training loss: 2.564008951187134
Validation loss: 2.3066061389061714

Epoch: 6| Step: 10
Training loss: 2.557948589324951
Validation loss: 2.3020950542983187

Epoch: 6| Step: 11
Training loss: 1.9436454772949219
Validation loss: 2.2907003689837713

Epoch: 6| Step: 12
Training loss: 2.593224287033081
Validation loss: 2.3040754359255553

Epoch: 6| Step: 13
Training loss: 3.1397550106048584
Validation loss: 2.288063754317581

Epoch: 211| Step: 0
Training loss: 2.346843719482422
Validation loss: 2.2985617473561275

Epoch: 6| Step: 1
Training loss: 1.9742408990859985
Validation loss: 2.2902118518788326

Epoch: 6| Step: 2
Training loss: 2.476593017578125
Validation loss: 2.294400599695021

Epoch: 6| Step: 3
Training loss: 2.461981773376465
Validation loss: 2.2934713620011524

Epoch: 6| Step: 4
Training loss: 2.8794398307800293
Validation loss: 2.2962340770229215

Epoch: 6| Step: 5
Training loss: 1.990006685256958
Validation loss: 2.2912243976387927

Epoch: 6| Step: 6
Training loss: 2.72609543800354
Validation loss: 2.2813163675287718

Epoch: 6| Step: 7
Training loss: 2.5225353240966797
Validation loss: 2.2748414188302974

Epoch: 6| Step: 8
Training loss: 2.5974388122558594
Validation loss: 2.2755747815614105

Epoch: 6| Step: 9
Training loss: 2.934866428375244
Validation loss: 2.2808787694541355

Epoch: 6| Step: 10
Training loss: 2.2735090255737305
Validation loss: 2.269576370075185

Epoch: 6| Step: 11
Training loss: 2.029550313949585
Validation loss: 2.2755286616663777

Epoch: 6| Step: 12
Training loss: 2.753221273422241
Validation loss: 2.279783546283681

Epoch: 6| Step: 13
Training loss: 3.2900238037109375
Validation loss: 2.2898250549070296

Epoch: 212| Step: 0
Training loss: 2.163613796234131
Validation loss: 2.2921592625238563

Epoch: 6| Step: 1
Training loss: 2.8467016220092773
Validation loss: 2.290207109143657

Epoch: 6| Step: 2
Training loss: 2.130997896194458
Validation loss: 2.3052664764465822

Epoch: 6| Step: 3
Training loss: 2.240140914916992
Validation loss: 2.300851333525873

Epoch: 6| Step: 4
Training loss: 3.3541674613952637
Validation loss: 2.2965433956474386

Epoch: 6| Step: 5
Training loss: 3.188101291656494
Validation loss: 2.3151680654095066

Epoch: 6| Step: 6
Training loss: 2.6291451454162598
Validation loss: 2.3178360205824657

Epoch: 6| Step: 7
Training loss: 1.8629536628723145
Validation loss: 2.3214115147949546

Epoch: 6| Step: 8
Training loss: 2.091766357421875
Validation loss: 2.3341512539053477

Epoch: 6| Step: 9
Training loss: 2.5721817016601562
Validation loss: 2.3178807894388833

Epoch: 6| Step: 10
Training loss: 2.979092597961426
Validation loss: 2.307295947946528

Epoch: 6| Step: 11
Training loss: 2.283219337463379
Validation loss: 2.2959501922771497

Epoch: 6| Step: 12
Training loss: 2.6195011138916016
Validation loss: 2.2819035873618176

Epoch: 6| Step: 13
Training loss: 1.6119998693466187
Validation loss: 2.2739630617121214

Epoch: 213| Step: 0
Training loss: 2.7242860794067383
Validation loss: 2.2872936238524733

Epoch: 6| Step: 1
Training loss: 1.915452241897583
Validation loss: 2.2847236497427827

Epoch: 6| Step: 2
Training loss: 2.67531156539917
Validation loss: 2.288507823021181

Epoch: 6| Step: 3
Training loss: 2.4404940605163574
Validation loss: 2.2718002719263874

Epoch: 6| Step: 4
Training loss: 2.651658535003662
Validation loss: 2.281512547564763

Epoch: 6| Step: 5
Training loss: 2.5094027519226074
Validation loss: 2.271691817109303

Epoch: 6| Step: 6
Training loss: 2.506781816482544
Validation loss: 2.2686359061989734

Epoch: 6| Step: 7
Training loss: 2.458798408508301
Validation loss: 2.2657874245797434

Epoch: 6| Step: 8
Training loss: 2.661983013153076
Validation loss: 2.265867958786667

Epoch: 6| Step: 9
Training loss: 2.377427577972412
Validation loss: 2.2604125725325717

Epoch: 6| Step: 10
Training loss: 3.0369467735290527
Validation loss: 2.261564068896796

Epoch: 6| Step: 11
Training loss: 2.6319401264190674
Validation loss: 2.263651273583853

Epoch: 6| Step: 12
Training loss: 2.158798933029175
Validation loss: 2.259175423652895

Epoch: 6| Step: 13
Training loss: 1.9718375205993652
Validation loss: 2.2684436267422092

Epoch: 214| Step: 0
Training loss: 2.348113775253296
Validation loss: 2.2723328144319597

Epoch: 6| Step: 1
Training loss: 2.272913694381714
Validation loss: 2.293372224735957

Epoch: 6| Step: 2
Training loss: 2.5238089561462402
Validation loss: 2.288372944760066

Epoch: 6| Step: 3
Training loss: 2.076188802719116
Validation loss: 2.278974087007584

Epoch: 6| Step: 4
Training loss: 2.569833755493164
Validation loss: 2.294515214940553

Epoch: 6| Step: 5
Training loss: 2.0708694458007812
Validation loss: 2.2979511035385953

Epoch: 6| Step: 6
Training loss: 1.7757519483566284
Validation loss: 2.310845184069808

Epoch: 6| Step: 7
Training loss: 2.8759772777557373
Validation loss: 2.3123574692715883

Epoch: 6| Step: 8
Training loss: 2.657008171081543
Validation loss: 2.3181381943405315

Epoch: 6| Step: 9
Training loss: 3.5635452270507812
Validation loss: 2.310581435439407

Epoch: 6| Step: 10
Training loss: 2.790949821472168
Validation loss: 2.322265430163312

Epoch: 6| Step: 11
Training loss: 2.3379502296447754
Validation loss: 2.307619519131158

Epoch: 6| Step: 12
Training loss: 1.8455381393432617
Validation loss: 2.290180716463315

Epoch: 6| Step: 13
Training loss: 3.697127103805542
Validation loss: 2.2859634532723376

Epoch: 215| Step: 0
Training loss: 3.587813138961792
Validation loss: 2.2805911725567234

Epoch: 6| Step: 1
Training loss: 2.8489928245544434
Validation loss: 2.279959355631182

Epoch: 6| Step: 2
Training loss: 2.6472623348236084
Validation loss: 2.276039574735908

Epoch: 6| Step: 3
Training loss: 2.167234420776367
Validation loss: 2.2737176649032103

Epoch: 6| Step: 4
Training loss: 1.8637816905975342
Validation loss: 2.268544284246301

Epoch: 6| Step: 5
Training loss: 2.926708698272705
Validation loss: 2.2630264041244343

Epoch: 6| Step: 6
Training loss: 2.3556413650512695
Validation loss: 2.270202895646454

Epoch: 6| Step: 7
Training loss: 2.683227062225342
Validation loss: 2.2890963913292013

Epoch: 6| Step: 8
Training loss: 1.687570571899414
Validation loss: 2.271573028256816

Epoch: 6| Step: 9
Training loss: 1.9740564823150635
Validation loss: 2.2826105651035102

Epoch: 6| Step: 10
Training loss: 2.1674461364746094
Validation loss: 2.2907416179615963

Epoch: 6| Step: 11
Training loss: 2.958730459213257
Validation loss: 2.2927398066366873

Epoch: 6| Step: 12
Training loss: 2.339043617248535
Validation loss: 2.303687451988138

Epoch: 6| Step: 13
Training loss: 2.5916638374328613
Validation loss: 2.3030413478933354

Epoch: 216| Step: 0
Training loss: 2.9164013862609863
Validation loss: 2.3044771430312947

Epoch: 6| Step: 1
Training loss: 2.6671056747436523
Validation loss: 2.3001784842501403

Epoch: 6| Step: 2
Training loss: 2.1053547859191895
Validation loss: 2.3132475037728586

Epoch: 6| Step: 3
Training loss: 1.9309556484222412
Validation loss: 2.3025354416139665

Epoch: 6| Step: 4
Training loss: 2.63088321685791
Validation loss: 2.316909892584688

Epoch: 6| Step: 5
Training loss: 2.507052421569824
Validation loss: 2.28972444739393

Epoch: 6| Step: 6
Training loss: 2.745405673980713
Validation loss: 2.2880981583749094

Epoch: 6| Step: 7
Training loss: 2.1725926399230957
Validation loss: 2.287357017558108

Epoch: 6| Step: 8
Training loss: 2.7213237285614014
Validation loss: 2.291014773871309

Epoch: 6| Step: 9
Training loss: 2.7434144020080566
Validation loss: 2.28275470067096

Epoch: 6| Step: 10
Training loss: 2.573479175567627
Validation loss: 2.281672421322074

Epoch: 6| Step: 11
Training loss: 2.5102243423461914
Validation loss: 2.2730593809517483

Epoch: 6| Step: 12
Training loss: 2.3546338081359863
Validation loss: 2.2833319581964964

Epoch: 6| Step: 13
Training loss: 1.8990472555160522
Validation loss: 2.271717999571113

Epoch: 217| Step: 0
Training loss: 1.936368465423584
Validation loss: 2.2779035158054803

Epoch: 6| Step: 1
Training loss: 2.9193828105926514
Validation loss: 2.277282412334155

Epoch: 6| Step: 2
Training loss: 2.4111108779907227
Validation loss: 2.271251796394266

Epoch: 6| Step: 3
Training loss: 3.082528591156006
Validation loss: 2.2799414588559057

Epoch: 6| Step: 4
Training loss: 2.700627326965332
Validation loss: 2.2831955314964376

Epoch: 6| Step: 5
Training loss: 2.08133602142334
Validation loss: 2.28004543242916

Epoch: 6| Step: 6
Training loss: 2.5796279907226562
Validation loss: 2.282491381450366

Epoch: 6| Step: 7
Training loss: 2.708148717880249
Validation loss: 2.28594329280238

Epoch: 6| Step: 8
Training loss: 2.3314170837402344
Validation loss: 2.2742830835362917

Epoch: 6| Step: 9
Training loss: 2.913566827774048
Validation loss: 2.264485684774255

Epoch: 6| Step: 10
Training loss: 2.2852251529693604
Validation loss: 2.2805652259498514

Epoch: 6| Step: 11
Training loss: 2.070890426635742
Validation loss: 2.2791884535102436

Epoch: 6| Step: 12
Training loss: 2.3334813117980957
Validation loss: 2.270642799715842

Epoch: 6| Step: 13
Training loss: 2.423297166824341
Validation loss: 2.2807611957673104

Epoch: 218| Step: 0
Training loss: 2.822162628173828
Validation loss: 2.286782944074241

Epoch: 6| Step: 1
Training loss: 2.2771177291870117
Validation loss: 2.279995018436063

Epoch: 6| Step: 2
Training loss: 2.3520073890686035
Validation loss: 2.296511509085214

Epoch: 6| Step: 3
Training loss: 2.659614086151123
Validation loss: 2.2953201340090845

Epoch: 6| Step: 4
Training loss: 2.99471116065979
Validation loss: 2.2827255161859656

Epoch: 6| Step: 5
Training loss: 2.745570182800293
Validation loss: 2.2837074623313

Epoch: 6| Step: 6
Training loss: 2.52854061126709
Validation loss: 2.273994889310611

Epoch: 6| Step: 7
Training loss: 2.886782169342041
Validation loss: 2.27011304004218

Epoch: 6| Step: 8
Training loss: 2.3628344535827637
Validation loss: 2.2959001987211165

Epoch: 6| Step: 9
Training loss: 1.4965906143188477
Validation loss: 2.288042999082996

Epoch: 6| Step: 10
Training loss: 2.6224117279052734
Validation loss: 2.2910889502494567

Epoch: 6| Step: 11
Training loss: 2.1733837127685547
Validation loss: 2.2685666827745337

Epoch: 6| Step: 12
Training loss: 2.4514284133911133
Validation loss: 2.2713432504284765

Epoch: 6| Step: 13
Training loss: 2.1964876651763916
Validation loss: 2.276875052400815

Epoch: 219| Step: 0
Training loss: 3.0618953704833984
Validation loss: 2.284322746338383

Epoch: 6| Step: 1
Training loss: 2.3313348293304443
Validation loss: 2.289537924592213

Epoch: 6| Step: 2
Training loss: 2.1279335021972656
Validation loss: 2.2748349764013804

Epoch: 6| Step: 3
Training loss: 3.1944193840026855
Validation loss: 2.278746461355558

Epoch: 6| Step: 4
Training loss: 2.679739475250244
Validation loss: 2.2776466313228814

Epoch: 6| Step: 5
Training loss: 2.3376624584198
Validation loss: 2.28021756551599

Epoch: 6| Step: 6
Training loss: 2.3066039085388184
Validation loss: 2.2715250471586823

Epoch: 6| Step: 7
Training loss: 1.5669742822647095
Validation loss: 2.270943728826379

Epoch: 6| Step: 8
Training loss: 2.775470733642578
Validation loss: 2.259465768773069

Epoch: 6| Step: 9
Training loss: 2.6270060539245605
Validation loss: 2.2637297645691903

Epoch: 6| Step: 10
Training loss: 2.0509462356567383
Validation loss: 2.267197488456644

Epoch: 6| Step: 11
Training loss: 2.319791793823242
Validation loss: 2.2611680671732914

Epoch: 6| Step: 12
Training loss: 2.81424617767334
Validation loss: 2.264336043788541

Epoch: 6| Step: 13
Training loss: 2.438588857650757
Validation loss: 2.263462971615535

Epoch: 220| Step: 0
Training loss: 2.3202943801879883
Validation loss: 2.271383954632667

Epoch: 6| Step: 1
Training loss: 2.4946324825286865
Validation loss: 2.2725634164707635

Epoch: 6| Step: 2
Training loss: 2.2957639694213867
Validation loss: 2.2706553807822605

Epoch: 6| Step: 3
Training loss: 2.9896388053894043
Validation loss: 2.274071513965566

Epoch: 6| Step: 4
Training loss: 2.5448174476623535
Validation loss: 2.273655088998938

Epoch: 6| Step: 5
Training loss: 2.5901083946228027
Validation loss: 2.2816566318594

Epoch: 6| Step: 6
Training loss: 2.040346622467041
Validation loss: 2.262309753766624

Epoch: 6| Step: 7
Training loss: 2.6370701789855957
Validation loss: 2.278511893364691

Epoch: 6| Step: 8
Training loss: 1.8104722499847412
Validation loss: 2.2701385995393157

Epoch: 6| Step: 9
Training loss: 2.7539987564086914
Validation loss: 2.26861499714595

Epoch: 6| Step: 10
Training loss: 2.4219298362731934
Validation loss: 2.2704349845968266

Epoch: 6| Step: 11
Training loss: 3.080552101135254
Validation loss: 2.275742087312924

Epoch: 6| Step: 12
Training loss: 2.4065003395080566
Validation loss: 2.2714332662602907

Epoch: 6| Step: 13
Training loss: 1.7853434085845947
Validation loss: 2.2694286736108924

Epoch: 221| Step: 0
Training loss: 2.299936294555664
Validation loss: 2.2759817236213276

Epoch: 6| Step: 1
Training loss: 2.292748212814331
Validation loss: 2.2823844596903813

Epoch: 6| Step: 2
Training loss: 2.4304232597351074
Validation loss: 2.2884927898324947

Epoch: 6| Step: 3
Training loss: 2.2415642738342285
Validation loss: 2.2897399804925405

Epoch: 6| Step: 4
Training loss: 2.798952579498291
Validation loss: 2.2859566314246065

Epoch: 6| Step: 5
Training loss: 1.7970938682556152
Validation loss: 2.2900452152375252

Epoch: 6| Step: 6
Training loss: 2.569483518600464
Validation loss: 2.2677507374876287

Epoch: 6| Step: 7
Training loss: 2.9994935989379883
Validation loss: 2.2765746039728962

Epoch: 6| Step: 8
Training loss: 2.416292905807495
Validation loss: 2.277719538698914

Epoch: 6| Step: 9
Training loss: 2.688369035720825
Validation loss: 2.2774109866029475

Epoch: 6| Step: 10
Training loss: 2.573296308517456
Validation loss: 2.2731722426670853

Epoch: 6| Step: 11
Training loss: 2.616720199584961
Validation loss: 2.271491760848671

Epoch: 6| Step: 12
Training loss: 2.7890944480895996
Validation loss: 2.2794130989300307

Epoch: 6| Step: 13
Training loss: 1.6079459190368652
Validation loss: 2.277065989791706

Epoch: 222| Step: 0
Training loss: 1.7816851139068604
Validation loss: 2.2716273633382653

Epoch: 6| Step: 1
Training loss: 3.4929518699645996
Validation loss: 2.3085666766730686

Epoch: 6| Step: 2
Training loss: 2.4966683387756348
Validation loss: 2.2866421989215318

Epoch: 6| Step: 3
Training loss: 2.5691936016082764
Validation loss: 2.284843348687695

Epoch: 6| Step: 4
Training loss: 2.4382357597351074
Validation loss: 2.2927180836277623

Epoch: 6| Step: 5
Training loss: 2.1518781185150146
Validation loss: 2.2957058542518207

Epoch: 6| Step: 6
Training loss: 2.1140899658203125
Validation loss: 2.289866740985583

Epoch: 6| Step: 7
Training loss: 2.73762845993042
Validation loss: 2.2882940423104072

Epoch: 6| Step: 8
Training loss: 2.24294114112854
Validation loss: 2.29284139602415

Epoch: 6| Step: 9
Training loss: 2.604336738586426
Validation loss: 2.2942900170562086

Epoch: 6| Step: 10
Training loss: 2.9943950176239014
Validation loss: 2.2901837748865925

Epoch: 6| Step: 11
Training loss: 2.0808050632476807
Validation loss: 2.2888032185134066

Epoch: 6| Step: 12
Training loss: 2.0965709686279297
Validation loss: 2.2768549047490603

Epoch: 6| Step: 13
Training loss: 3.0359127521514893
Validation loss: 2.2701976760741203

Epoch: 223| Step: 0
Training loss: 1.7876918315887451
Validation loss: 2.277720061681604

Epoch: 6| Step: 1
Training loss: 2.697734832763672
Validation loss: 2.275509931707895

Epoch: 6| Step: 2
Training loss: 2.4402947425842285
Validation loss: 2.278614080080422

Epoch: 6| Step: 3
Training loss: 2.0105676651000977
Validation loss: 2.2732547149863294

Epoch: 6| Step: 4
Training loss: 3.0582056045532227
Validation loss: 2.2748862569050123

Epoch: 6| Step: 5
Training loss: 2.5557098388671875
Validation loss: 2.2713242141149377

Epoch: 6| Step: 6
Training loss: 2.9522814750671387
Validation loss: 2.25835858878269

Epoch: 6| Step: 7
Training loss: 2.8760342597961426
Validation loss: 2.2682427847257225

Epoch: 6| Step: 8
Training loss: 1.9217917919158936
Validation loss: 2.2703228201917423

Epoch: 6| Step: 9
Training loss: 2.1106691360473633
Validation loss: 2.266914011329733

Epoch: 6| Step: 10
Training loss: 2.273507595062256
Validation loss: 2.270769665318151

Epoch: 6| Step: 11
Training loss: 2.8177618980407715
Validation loss: 2.271623896014306

Epoch: 6| Step: 12
Training loss: 1.8624544143676758
Validation loss: 2.274887720743815

Epoch: 6| Step: 13
Training loss: 3.539501428604126
Validation loss: 2.271768780164821

Epoch: 224| Step: 0
Training loss: 2.7264113426208496
Validation loss: 2.2686189220797632

Epoch: 6| Step: 1
Training loss: 2.6147594451904297
Validation loss: 2.267583062571864

Epoch: 6| Step: 2
Training loss: 2.234262466430664
Validation loss: 2.2667279602378927

Epoch: 6| Step: 3
Training loss: 2.623566150665283
Validation loss: 2.2781658377698673

Epoch: 6| Step: 4
Training loss: 1.711791753768921
Validation loss: 2.2786606024670344

Epoch: 6| Step: 5
Training loss: 1.8995667695999146
Validation loss: 2.276623897655036

Epoch: 6| Step: 6
Training loss: 1.9899057149887085
Validation loss: 2.280651036129203

Epoch: 6| Step: 7
Training loss: 2.2046735286712646
Validation loss: 2.292286031989641

Epoch: 6| Step: 8
Training loss: 2.8693153858184814
Validation loss: 2.300842938884612

Epoch: 6| Step: 9
Training loss: 2.648221015930176
Validation loss: 2.2850661457225843

Epoch: 6| Step: 10
Training loss: 2.7982428073883057
Validation loss: 2.2735812946032454

Epoch: 6| Step: 11
Training loss: 2.5342788696289062
Validation loss: 2.2752040765618764

Epoch: 6| Step: 12
Training loss: 2.9571568965911865
Validation loss: 2.2723906911829466

Epoch: 6| Step: 13
Training loss: 2.6786656379699707
Validation loss: 2.268993659686017

Epoch: 225| Step: 0
Training loss: 2.1844892501831055
Validation loss: 2.266379576857372

Epoch: 6| Step: 1
Training loss: 2.313039779663086
Validation loss: 2.272561396321943

Epoch: 6| Step: 2
Training loss: 2.354254722595215
Validation loss: 2.2631736673334593

Epoch: 6| Step: 3
Training loss: 2.710742473602295
Validation loss: 2.2639754023603214

Epoch: 6| Step: 4
Training loss: 2.7715835571289062
Validation loss: 2.2613693591087096

Epoch: 6| Step: 5
Training loss: 2.361149311065674
Validation loss: 2.269159365725774

Epoch: 6| Step: 6
Training loss: 2.7053236961364746
Validation loss: 2.2752112778284217

Epoch: 6| Step: 7
Training loss: 2.217285633087158
Validation loss: 2.26647856414959

Epoch: 6| Step: 8
Training loss: 2.1941871643066406
Validation loss: 2.263172657259049

Epoch: 6| Step: 9
Training loss: 2.8376305103302
Validation loss: 2.2747731465165333

Epoch: 6| Step: 10
Training loss: 2.322265148162842
Validation loss: 2.261496672066309

Epoch: 6| Step: 11
Training loss: 2.693638324737549
Validation loss: 2.264318540532102

Epoch: 6| Step: 12
Training loss: 2.854163646697998
Validation loss: 2.264130739755528

Epoch: 6| Step: 13
Training loss: 1.8946737051010132
Validation loss: 2.2631559346311834

Epoch: 226| Step: 0
Training loss: 3.1123571395874023
Validation loss: 2.2637589144450363

Epoch: 6| Step: 1
Training loss: 2.0901646614074707
Validation loss: 2.2738237765527542

Epoch: 6| Step: 2
Training loss: 3.173849582672119
Validation loss: 2.2621032576407156

Epoch: 6| Step: 3
Training loss: 2.685183048248291
Validation loss: 2.2769963920757337

Epoch: 6| Step: 4
Training loss: 2.110079526901245
Validation loss: 2.274473636381088

Epoch: 6| Step: 5
Training loss: 2.764962673187256
Validation loss: 2.280012215337446

Epoch: 6| Step: 6
Training loss: 2.176640033721924
Validation loss: 2.2769917288134174

Epoch: 6| Step: 7
Training loss: 2.0259292125701904
Validation loss: 2.2791564362023466

Epoch: 6| Step: 8
Training loss: 2.3169922828674316
Validation loss: 2.2764682821048203

Epoch: 6| Step: 9
Training loss: 2.0466532707214355
Validation loss: 2.2595224201038318

Epoch: 6| Step: 10
Training loss: 1.9996575117111206
Validation loss: 2.274100430550114

Epoch: 6| Step: 11
Training loss: 2.3999009132385254
Validation loss: 2.283779216069047

Epoch: 6| Step: 12
Training loss: 3.4541306495666504
Validation loss: 2.2921666381179646

Epoch: 6| Step: 13
Training loss: 1.6935482025146484
Validation loss: 2.291918977614372

Epoch: 227| Step: 0
Training loss: 2.7595391273498535
Validation loss: 2.2870068780837522

Epoch: 6| Step: 1
Training loss: 2.565870761871338
Validation loss: 2.288232743099172

Epoch: 6| Step: 2
Training loss: 2.8384971618652344
Validation loss: 2.2804009350397254

Epoch: 6| Step: 3
Training loss: 2.468132972717285
Validation loss: 2.275544625456615

Epoch: 6| Step: 4
Training loss: 2.2592337131500244
Validation loss: 2.272468251566733

Epoch: 6| Step: 5
Training loss: 2.6975138187408447
Validation loss: 2.2788912660332135

Epoch: 6| Step: 6
Training loss: 2.0689094066619873
Validation loss: 2.2647394992971934

Epoch: 6| Step: 7
Training loss: 1.782417893409729
Validation loss: 2.280484163632957

Epoch: 6| Step: 8
Training loss: 1.9314545392990112
Validation loss: 2.2821967729958157

Epoch: 6| Step: 9
Training loss: 2.716850757598877
Validation loss: 2.2662862423927552

Epoch: 6| Step: 10
Training loss: 2.0294480323791504
Validation loss: 2.261395554388723

Epoch: 6| Step: 11
Training loss: 2.1040658950805664
Validation loss: 2.281941088297034

Epoch: 6| Step: 12
Training loss: 2.8655221462249756
Validation loss: 2.2665933537226852

Epoch: 6| Step: 13
Training loss: 3.715705394744873
Validation loss: 2.287705382993144

Epoch: 228| Step: 0
Training loss: 2.0794529914855957
Validation loss: 2.2959092663180445

Epoch: 6| Step: 1
Training loss: 2.1846704483032227
Validation loss: 2.291322062092443

Epoch: 6| Step: 2
Training loss: 2.2844412326812744
Validation loss: 2.288718118462511

Epoch: 6| Step: 3
Training loss: 2.946014404296875
Validation loss: 2.270971926309729

Epoch: 6| Step: 4
Training loss: 2.7404699325561523
Validation loss: 2.264010011508901

Epoch: 6| Step: 5
Training loss: 2.63789701461792
Validation loss: 2.25321259549869

Epoch: 6| Step: 6
Training loss: 2.5587449073791504
Validation loss: 2.265984255780456

Epoch: 6| Step: 7
Training loss: 2.4396047592163086
Validation loss: 2.251670874575133

Epoch: 6| Step: 8
Training loss: 2.4478816986083984
Validation loss: 2.247666835784912

Epoch: 6| Step: 9
Training loss: 3.007390022277832
Validation loss: 2.249857653853714

Epoch: 6| Step: 10
Training loss: 2.0752549171447754
Validation loss: 2.237252399485598

Epoch: 6| Step: 11
Training loss: 2.0907158851623535
Validation loss: 2.2396068367906796

Epoch: 6| Step: 12
Training loss: 2.761659622192383
Validation loss: 2.2430834975293887

Epoch: 6| Step: 13
Training loss: 2.2423620223999023
Validation loss: 2.237323089312482

Epoch: 229| Step: 0
Training loss: 2.741764545440674
Validation loss: 2.237626858936843

Epoch: 6| Step: 1
Training loss: 2.388761281967163
Validation loss: 2.245224242569298

Epoch: 6| Step: 2
Training loss: 1.9980518817901611
Validation loss: 2.2471554445964035

Epoch: 6| Step: 3
Training loss: 2.3428163528442383
Validation loss: 2.2371927333134476

Epoch: 6| Step: 4
Training loss: 2.5609536170959473
Validation loss: 2.251164372249316

Epoch: 6| Step: 5
Training loss: 2.497340202331543
Validation loss: 2.2555916770812003

Epoch: 6| Step: 6
Training loss: 2.9126362800598145
Validation loss: 2.264497110920568

Epoch: 6| Step: 7
Training loss: 3.3527917861938477
Validation loss: 2.278160782270534

Epoch: 6| Step: 8
Training loss: 2.195138454437256
Validation loss: 2.2998931946293

Epoch: 6| Step: 9
Training loss: 2.6108388900756836
Validation loss: 2.2956099587102092

Epoch: 6| Step: 10
Training loss: 2.1431174278259277
Validation loss: 2.2683029713169223

Epoch: 6| Step: 11
Training loss: 2.314700126647949
Validation loss: 2.263699980192287

Epoch: 6| Step: 12
Training loss: 2.4582700729370117
Validation loss: 2.274838532170942

Epoch: 6| Step: 13
Training loss: 1.5465137958526611
Validation loss: 2.2718898480938328

Epoch: 230| Step: 0
Training loss: 3.1653361320495605
Validation loss: 2.261188289170624

Epoch: 6| Step: 1
Training loss: 3.3422770500183105
Validation loss: 2.2612486808530745

Epoch: 6| Step: 2
Training loss: 2.0188825130462646
Validation loss: 2.262644960034278

Epoch: 6| Step: 3
Training loss: 2.5783634185791016
Validation loss: 2.271995472651656

Epoch: 6| Step: 4
Training loss: 2.2875680923461914
Validation loss: 2.284199022477673

Epoch: 6| Step: 5
Training loss: 2.253110885620117
Validation loss: 2.276629668410106

Epoch: 6| Step: 6
Training loss: 2.0341410636901855
Validation loss: 2.277750030640633

Epoch: 6| Step: 7
Training loss: 2.806142568588257
Validation loss: 2.2769836918000252

Epoch: 6| Step: 8
Training loss: 3.340353488922119
Validation loss: 2.283637797960671

Epoch: 6| Step: 9
Training loss: 2.4824514389038086
Validation loss: 2.2785163746085217

Epoch: 6| Step: 10
Training loss: 2.1006250381469727
Validation loss: 2.273909165013221

Epoch: 6| Step: 11
Training loss: 1.979038953781128
Validation loss: 2.2650856023193686

Epoch: 6| Step: 12
Training loss: 1.5380563735961914
Validation loss: 2.2780620949242705

Epoch: 6| Step: 13
Training loss: 2.531172752380371
Validation loss: 2.272860796220841

Epoch: 231| Step: 0
Training loss: 2.7121143341064453
Validation loss: 2.265705447043142

Epoch: 6| Step: 1
Training loss: 1.9126646518707275
Validation loss: 2.2823334214507893

Epoch: 6| Step: 2
Training loss: 2.6569836139678955
Validation loss: 2.282674579210179

Epoch: 6| Step: 3
Training loss: 2.3484444618225098
Validation loss: 2.2723735096634075

Epoch: 6| Step: 4
Training loss: 1.5899332761764526
Validation loss: 2.289578794151224

Epoch: 6| Step: 5
Training loss: 2.879753589630127
Validation loss: 2.3053785703515493

Epoch: 6| Step: 6
Training loss: 3.106199026107788
Validation loss: 2.2966962424657678

Epoch: 6| Step: 7
Training loss: 2.322131633758545
Validation loss: 2.303790238595778

Epoch: 6| Step: 8
Training loss: 2.3875932693481445
Validation loss: 2.2841954256898616

Epoch: 6| Step: 9
Training loss: 2.6694984436035156
Validation loss: 2.2747087453001287

Epoch: 6| Step: 10
Training loss: 1.7868510484695435
Validation loss: 2.2703521046587216

Epoch: 6| Step: 11
Training loss: 2.2375383377075195
Validation loss: 2.2680262980922574

Epoch: 6| Step: 12
Training loss: 3.1447014808654785
Validation loss: 2.2630314647510485

Epoch: 6| Step: 13
Training loss: 2.802217721939087
Validation loss: 2.2685277846551712

Epoch: 232| Step: 0
Training loss: 2.8117990493774414
Validation loss: 2.266007597728442

Epoch: 6| Step: 1
Training loss: 3.31668758392334
Validation loss: 2.2610383495207755

Epoch: 6| Step: 2
Training loss: 2.50122332572937
Validation loss: 2.2480482029658493

Epoch: 6| Step: 3
Training loss: 2.1325254440307617
Validation loss: 2.241549212445495

Epoch: 6| Step: 4
Training loss: 2.4958853721618652
Validation loss: 2.2484942661818637

Epoch: 6| Step: 5
Training loss: 2.265286922454834
Validation loss: 2.2483091585097776

Epoch: 6| Step: 6
Training loss: 2.193481206893921
Validation loss: 2.249064773641607

Epoch: 6| Step: 7
Training loss: 2.399430751800537
Validation loss: 2.238156267391738

Epoch: 6| Step: 8
Training loss: 2.402360677719116
Validation loss: 2.2466834411826184

Epoch: 6| Step: 9
Training loss: 2.1220197677612305
Validation loss: 2.2594599185451383

Epoch: 6| Step: 10
Training loss: 2.092341423034668
Validation loss: 2.260364786271126

Epoch: 6| Step: 11
Training loss: 3.0400121212005615
Validation loss: 2.254077342248732

Epoch: 6| Step: 12
Training loss: 2.377800464630127
Validation loss: 2.2633204588326077

Epoch: 6| Step: 13
Training loss: 1.9045230150222778
Validation loss: 2.274870578960706

Epoch: 233| Step: 0
Training loss: 2.916768789291382
Validation loss: 2.25170555422383

Epoch: 6| Step: 1
Training loss: 1.8181350231170654
Validation loss: 2.2576045554171325

Epoch: 6| Step: 2
Training loss: 2.4201250076293945
Validation loss: 2.2438869809591644

Epoch: 6| Step: 3
Training loss: 2.5771870613098145
Validation loss: 2.2308398164728636

Epoch: 6| Step: 4
Training loss: 2.3561668395996094
Validation loss: 2.2380664271693074

Epoch: 6| Step: 5
Training loss: 1.770654559135437
Validation loss: 2.238088374496788

Epoch: 6| Step: 6
Training loss: 2.650348663330078
Validation loss: 2.246844676233107

Epoch: 6| Step: 7
Training loss: 2.941329002380371
Validation loss: 2.235109706078806

Epoch: 6| Step: 8
Training loss: 2.422384262084961
Validation loss: 2.2376788687962357

Epoch: 6| Step: 9
Training loss: 2.4291181564331055
Validation loss: 2.245070880459201

Epoch: 6| Step: 10
Training loss: 2.387018918991089
Validation loss: 2.2477158038846907

Epoch: 6| Step: 11
Training loss: 2.880730628967285
Validation loss: 2.25113413410802

Epoch: 6| Step: 12
Training loss: 2.1419620513916016
Validation loss: 2.2567069274122997

Epoch: 6| Step: 13
Training loss: 2.346665382385254
Validation loss: 2.249335745329498

Epoch: 234| Step: 0
Training loss: 2.1186912059783936
Validation loss: 2.270552460865308

Epoch: 6| Step: 1
Training loss: 2.6020779609680176
Validation loss: 2.2587991222258537

Epoch: 6| Step: 2
Training loss: 2.5344676971435547
Validation loss: 2.2712732873937136

Epoch: 6| Step: 3
Training loss: 1.991898536682129
Validation loss: 2.2684735931375974

Epoch: 6| Step: 4
Training loss: 2.4833600521087646
Validation loss: 2.2751824496894755

Epoch: 6| Step: 5
Training loss: 2.3241076469421387
Validation loss: 2.265456220155121

Epoch: 6| Step: 6
Training loss: 2.488898277282715
Validation loss: 2.2601996865323795

Epoch: 6| Step: 7
Training loss: 2.2238731384277344
Validation loss: 2.271972663940922

Epoch: 6| Step: 8
Training loss: 2.4816551208496094
Validation loss: 2.2686159623566495

Epoch: 6| Step: 9
Training loss: 2.9436757564544678
Validation loss: 2.2636764587894564

Epoch: 6| Step: 10
Training loss: 2.3806910514831543
Validation loss: 2.257063833616113

Epoch: 6| Step: 11
Training loss: 1.8500736951828003
Validation loss: 2.259146067403978

Epoch: 6| Step: 12
Training loss: 3.145979642868042
Validation loss: 2.2579535771441717

Epoch: 6| Step: 13
Training loss: 2.5905349254608154
Validation loss: 2.2628462109514462

Epoch: 235| Step: 0
Training loss: 3.108886241912842
Validation loss: 2.2609411695952057

Epoch: 6| Step: 1
Training loss: 2.3453166484832764
Validation loss: 2.2519728906692995

Epoch: 6| Step: 2
Training loss: 2.4483087062835693
Validation loss: 2.2515426797251545

Epoch: 6| Step: 3
Training loss: 1.9817075729370117
Validation loss: 2.2506724378114105

Epoch: 6| Step: 4
Training loss: 2.101391315460205
Validation loss: 2.26683985289707

Epoch: 6| Step: 5
Training loss: 2.6913084983825684
Validation loss: 2.2383016283794115

Epoch: 6| Step: 6
Training loss: 2.7500832080841064
Validation loss: 2.2532251342650382

Epoch: 6| Step: 7
Training loss: 1.7879451513290405
Validation loss: 2.245988203633216

Epoch: 6| Step: 8
Training loss: 2.4460954666137695
Validation loss: 2.257262993884343

Epoch: 6| Step: 9
Training loss: 2.9031548500061035
Validation loss: 2.242640026154057

Epoch: 6| Step: 10
Training loss: 2.0522358417510986
Validation loss: 2.2667312468251875

Epoch: 6| Step: 11
Training loss: 2.1619949340820312
Validation loss: 2.2512447449468795

Epoch: 6| Step: 12
Training loss: 2.752838611602783
Validation loss: 2.264822208753196

Epoch: 6| Step: 13
Training loss: 2.642575979232788
Validation loss: 2.2610443843308317

Epoch: 236| Step: 0
Training loss: 1.651175856590271
Validation loss: 2.265360047740321

Epoch: 6| Step: 1
Training loss: 2.5718069076538086
Validation loss: 2.2602293209363054

Epoch: 6| Step: 2
Training loss: 2.553556203842163
Validation loss: 2.2557000524254254

Epoch: 6| Step: 3
Training loss: 2.1179351806640625
Validation loss: 2.2535684365098194

Epoch: 6| Step: 4
Training loss: 2.647953987121582
Validation loss: 2.2575280115168583

Epoch: 6| Step: 5
Training loss: 2.5316948890686035
Validation loss: 2.2521864444978776

Epoch: 6| Step: 6
Training loss: 2.6203861236572266
Validation loss: 2.2496326533696984

Epoch: 6| Step: 7
Training loss: 1.8518404960632324
Validation loss: 2.2528986674483105

Epoch: 6| Step: 8
Training loss: 3.033383369445801
Validation loss: 2.247253361568656

Epoch: 6| Step: 9
Training loss: 2.256467580795288
Validation loss: 2.252423970930038

Epoch: 6| Step: 10
Training loss: 1.8715605735778809
Validation loss: 2.2525987676394883

Epoch: 6| Step: 11
Training loss: 2.4924445152282715
Validation loss: 2.2478544558248212

Epoch: 6| Step: 12
Training loss: 3.145930767059326
Validation loss: 2.2459485402671238

Epoch: 6| Step: 13
Training loss: 2.900960683822632
Validation loss: 2.2617811541403494

Epoch: 237| Step: 0
Training loss: 2.6644716262817383
Validation loss: 2.2655379720913467

Epoch: 6| Step: 1
Training loss: 2.290761709213257
Validation loss: 2.268820039687618

Epoch: 6| Step: 2
Training loss: 2.080134391784668
Validation loss: 2.2703926947809037

Epoch: 6| Step: 3
Training loss: 2.436164379119873
Validation loss: 2.280732318919192

Epoch: 6| Step: 4
Training loss: 1.8695181608200073
Validation loss: 2.2867047350893737

Epoch: 6| Step: 5
Training loss: 2.267958164215088
Validation loss: 2.300556534080095

Epoch: 6| Step: 6
Training loss: 3.258986234664917
Validation loss: 2.2898941706585627

Epoch: 6| Step: 7
Training loss: 2.32265043258667
Validation loss: 2.3072413116373043

Epoch: 6| Step: 8
Training loss: 2.7339768409729004
Validation loss: 2.289713257102556

Epoch: 6| Step: 9
Training loss: 2.2997872829437256
Validation loss: 2.2821377220974175

Epoch: 6| Step: 10
Training loss: 2.187722682952881
Validation loss: 2.2727088646222184

Epoch: 6| Step: 11
Training loss: 2.5688767433166504
Validation loss: 2.267695347468058

Epoch: 6| Step: 12
Training loss: 3.01287841796875
Validation loss: 2.247813352974512

Epoch: 6| Step: 13
Training loss: 2.2814769744873047
Validation loss: 2.263413449769379

Epoch: 238| Step: 0
Training loss: 3.671590805053711
Validation loss: 2.241447297475671

Epoch: 6| Step: 1
Training loss: 2.296698570251465
Validation loss: 2.247075980709445

Epoch: 6| Step: 2
Training loss: 2.5602030754089355
Validation loss: 2.2492062225136706

Epoch: 6| Step: 3
Training loss: 2.565031051635742
Validation loss: 2.2426794831470778

Epoch: 6| Step: 4
Training loss: 1.94148588180542
Validation loss: 2.2368222334051646

Epoch: 6| Step: 5
Training loss: 1.9601879119873047
Validation loss: 2.2391090751976095

Epoch: 6| Step: 6
Training loss: 3.0305094718933105
Validation loss: 2.23021690563489

Epoch: 6| Step: 7
Training loss: 2.271055221557617
Validation loss: 2.2294120173300467

Epoch: 6| Step: 8
Training loss: 2.5362863540649414
Validation loss: 2.2259592676675446

Epoch: 6| Step: 9
Training loss: 1.9197789430618286
Validation loss: 2.237748699803506

Epoch: 6| Step: 10
Training loss: 2.592071056365967
Validation loss: 2.2410376982022355

Epoch: 6| Step: 11
Training loss: 2.600490093231201
Validation loss: 2.2700268632622174

Epoch: 6| Step: 12
Training loss: 2.0584592819213867
Validation loss: 2.2702217409687657

Epoch: 6| Step: 13
Training loss: 1.6018364429473877
Validation loss: 2.2849193157688266

Epoch: 239| Step: 0
Training loss: 1.5795634984970093
Validation loss: 2.302880141042894

Epoch: 6| Step: 1
Training loss: 2.3098223209381104
Validation loss: 2.325718793817746

Epoch: 6| Step: 2
Training loss: 2.0845489501953125
Validation loss: 2.3398619467212307

Epoch: 6| Step: 3
Training loss: 3.2933731079101562
Validation loss: 2.322975153564125

Epoch: 6| Step: 4
Training loss: 2.2688241004943848
Validation loss: 2.327909679823024

Epoch: 6| Step: 5
Training loss: 2.5751938819885254
Validation loss: 2.3150810785191034

Epoch: 6| Step: 6
Training loss: 2.7077980041503906
Validation loss: 2.262852886671661

Epoch: 6| Step: 7
Training loss: 2.386524200439453
Validation loss: 2.257522670171594

Epoch: 6| Step: 8
Training loss: 2.548274040222168
Validation loss: 2.2577453300517094

Epoch: 6| Step: 9
Training loss: 1.8106775283813477
Validation loss: 2.2433407973217707

Epoch: 6| Step: 10
Training loss: 3.3937597274780273
Validation loss: 2.2431821823120117

Epoch: 6| Step: 11
Training loss: 1.9717085361480713
Validation loss: 2.2523983088872765

Epoch: 6| Step: 12
Training loss: 2.885878562927246
Validation loss: 2.2407025316710114

Epoch: 6| Step: 13
Training loss: 2.3542490005493164
Validation loss: 2.251650376986432

Epoch: 240| Step: 0
Training loss: 2.405381202697754
Validation loss: 2.2495853490726923

Epoch: 6| Step: 1
Training loss: 2.190971851348877
Validation loss: 2.2491054637457735

Epoch: 6| Step: 2
Training loss: 2.331937551498413
Validation loss: 2.24699176896003

Epoch: 6| Step: 3
Training loss: 2.4408376216888428
Validation loss: 2.240779020453012

Epoch: 6| Step: 4
Training loss: 2.7398314476013184
Validation loss: 2.2344280904339207

Epoch: 6| Step: 5
Training loss: 2.5469810962677
Validation loss: 2.2327242487220356

Epoch: 6| Step: 6
Training loss: 2.5409557819366455
Validation loss: 2.234817381828062

Epoch: 6| Step: 7
Training loss: 2.113516092300415
Validation loss: 2.2403061800105597

Epoch: 6| Step: 8
Training loss: 2.420562267303467
Validation loss: 2.230642662253431

Epoch: 6| Step: 9
Training loss: 2.7843010425567627
Validation loss: 2.2382027667055846

Epoch: 6| Step: 10
Training loss: 2.4199259281158447
Validation loss: 2.250968110176825

Epoch: 6| Step: 11
Training loss: 2.1339051723480225
Validation loss: 2.2453971203937324

Epoch: 6| Step: 12
Training loss: 2.0045225620269775
Validation loss: 2.2559638613013813

Epoch: 6| Step: 13
Training loss: 3.1390061378479004
Validation loss: 2.244685560144404

Epoch: 241| Step: 0
Training loss: 2.9275968074798584
Validation loss: 2.247485194154965

Epoch: 6| Step: 1
Training loss: 2.391132354736328
Validation loss: 2.2526847111281527

Epoch: 6| Step: 2
Training loss: 2.904188632965088
Validation loss: 2.2492803963281776

Epoch: 6| Step: 3
Training loss: 2.4565846920013428
Validation loss: 2.2538327145320114

Epoch: 6| Step: 4
Training loss: 2.3679678440093994
Validation loss: 2.244379610143682

Epoch: 6| Step: 5
Training loss: 1.980567216873169
Validation loss: 2.2472391679722774

Epoch: 6| Step: 6
Training loss: 2.935840606689453
Validation loss: 2.2410216792937248

Epoch: 6| Step: 7
Training loss: 2.6627001762390137
Validation loss: 2.2629939151066605

Epoch: 6| Step: 8
Training loss: 2.9987716674804688
Validation loss: 2.247960013727988

Epoch: 6| Step: 9
Training loss: 2.6227424144744873
Validation loss: 2.2497703849628405

Epoch: 6| Step: 10
Training loss: 1.62428617477417
Validation loss: 2.2379689960069555

Epoch: 6| Step: 11
Training loss: 2.0321784019470215
Validation loss: 2.248997998493974

Epoch: 6| Step: 12
Training loss: 2.0572891235351562
Validation loss: 2.235690724465155

Epoch: 6| Step: 13
Training loss: 1.6472049951553345
Validation loss: 2.2446282140670286

Epoch: 242| Step: 0
Training loss: 3.2541141510009766
Validation loss: 2.240933028600549

Epoch: 6| Step: 1
Training loss: 2.5633487701416016
Validation loss: 2.256794250139626

Epoch: 6| Step: 2
Training loss: 2.3559529781341553
Validation loss: 2.2790518986281527

Epoch: 6| Step: 3
Training loss: 2.1161863803863525
Validation loss: 2.264980052107124

Epoch: 6| Step: 4
Training loss: 2.4580001831054688
Validation loss: 2.287233401370305

Epoch: 6| Step: 5
Training loss: 2.4933228492736816
Validation loss: 2.2872962592750468

Epoch: 6| Step: 6
Training loss: 2.994974136352539
Validation loss: 2.2553717449147213

Epoch: 6| Step: 7
Training loss: 2.5297346115112305
Validation loss: 2.2516071181143484

Epoch: 6| Step: 8
Training loss: 1.6473557949066162
Validation loss: 2.2478878959532707

Epoch: 6| Step: 9
Training loss: 2.485276460647583
Validation loss: 2.234030310825635

Epoch: 6| Step: 10
Training loss: 2.1575968265533447
Validation loss: 2.2295609622873287

Epoch: 6| Step: 11
Training loss: 2.200242042541504
Validation loss: 2.239874444982057

Epoch: 6| Step: 12
Training loss: 2.2605104446411133
Validation loss: 2.241795596256051

Epoch: 6| Step: 13
Training loss: 2.536766529083252
Validation loss: 2.2481266426783737

Epoch: 243| Step: 0
Training loss: 2.5848357677459717
Validation loss: 2.243123100649926

Epoch: 6| Step: 1
Training loss: 2.194159984588623
Validation loss: 2.258191849595757

Epoch: 6| Step: 2
Training loss: 2.5193142890930176
Validation loss: 2.24042373318826

Epoch: 6| Step: 3
Training loss: 2.3342843055725098
Validation loss: 2.2506288930933964

Epoch: 6| Step: 4
Training loss: 2.1992597579956055
Validation loss: 2.251804990153159

Epoch: 6| Step: 5
Training loss: 2.5631823539733887
Validation loss: 2.243966402546052

Epoch: 6| Step: 6
Training loss: 2.3469200134277344
Validation loss: 2.238160171816426

Epoch: 6| Step: 7
Training loss: 2.7254388332366943
Validation loss: 2.2399765009521158

Epoch: 6| Step: 8
Training loss: 3.204862594604492
Validation loss: 2.244402991828098

Epoch: 6| Step: 9
Training loss: 1.645613431930542
Validation loss: 2.244125396974625

Epoch: 6| Step: 10
Training loss: 1.5250132083892822
Validation loss: 2.24471265269864

Epoch: 6| Step: 11
Training loss: 2.6431069374084473
Validation loss: 2.237611924448321

Epoch: 6| Step: 12
Training loss: 2.8707852363586426
Validation loss: 2.2538489526317966

Epoch: 6| Step: 13
Training loss: 2.7721447944641113
Validation loss: 2.247866843336372

Epoch: 244| Step: 0
Training loss: 1.9761168956756592
Validation loss: 2.2418782198300926

Epoch: 6| Step: 1
Training loss: 2.9421141147613525
Validation loss: 2.242155408346525

Epoch: 6| Step: 2
Training loss: 1.5180329084396362
Validation loss: 2.263531969439599

Epoch: 6| Step: 3
Training loss: 2.1508913040161133
Validation loss: 2.287018727230769

Epoch: 6| Step: 4
Training loss: 2.8834972381591797
Validation loss: 2.283292865240446

Epoch: 6| Step: 5
Training loss: 2.5308332443237305
Validation loss: 2.2964965399875434

Epoch: 6| Step: 6
Training loss: 2.5729990005493164
Validation loss: 2.300869572547174

Epoch: 6| Step: 7
Training loss: 2.616450786590576
Validation loss: 2.2970417058596047

Epoch: 6| Step: 8
Training loss: 2.1677019596099854
Validation loss: 2.2800858200237317

Epoch: 6| Step: 9
Training loss: 2.500786304473877
Validation loss: 2.244691559063491

Epoch: 6| Step: 10
Training loss: 1.911505103111267
Validation loss: 2.2364987301570114

Epoch: 6| Step: 11
Training loss: 3.45896577835083
Validation loss: 2.235224441815448

Epoch: 6| Step: 12
Training loss: 2.663457155227661
Validation loss: 2.215294966133692

Epoch: 6| Step: 13
Training loss: 2.131270408630371
Validation loss: 2.224866715810632

Epoch: 245| Step: 0
Training loss: 2.477738380432129
Validation loss: 2.2303452286669003

Epoch: 6| Step: 1
Training loss: 1.6416213512420654
Validation loss: 2.2197729669591433

Epoch: 6| Step: 2
Training loss: 2.0743179321289062
Validation loss: 2.219458859453919

Epoch: 6| Step: 3
Training loss: 2.9616057872772217
Validation loss: 2.2190541785250426

Epoch: 6| Step: 4
Training loss: 3.004211902618408
Validation loss: 2.2218748369524555

Epoch: 6| Step: 5
Training loss: 2.255326747894287
Validation loss: 2.229537848503359

Epoch: 6| Step: 6
Training loss: 2.2570624351501465
Validation loss: 2.216783474850398

Epoch: 6| Step: 7
Training loss: 2.972935676574707
Validation loss: 2.2104928608863585

Epoch: 6| Step: 8
Training loss: 2.665395498275757
Validation loss: 2.218694227997975

Epoch: 6| Step: 9
Training loss: 2.132138967514038
Validation loss: 2.219947479104483

Epoch: 6| Step: 10
Training loss: 2.305704355239868
Validation loss: 2.2405179495452554

Epoch: 6| Step: 11
Training loss: 2.4139952659606934
Validation loss: 2.2233678576766804

Epoch: 6| Step: 12
Training loss: 1.9284236431121826
Validation loss: 2.226693258490614

Epoch: 6| Step: 13
Training loss: 3.2176060676574707
Validation loss: 2.210698150819348

Epoch: 246| Step: 0
Training loss: 2.3983192443847656
Validation loss: 2.2202654730889106

Epoch: 6| Step: 1
Training loss: 2.6387534141540527
Validation loss: 2.2224762452545987

Epoch: 6| Step: 2
Training loss: 2.024536371231079
Validation loss: 2.2071870219322944

Epoch: 6| Step: 3
Training loss: 2.5239195823669434
Validation loss: 2.225258877200465

Epoch: 6| Step: 4
Training loss: 2.7427310943603516
Validation loss: 2.2104373670393422

Epoch: 6| Step: 5
Training loss: 1.7837605476379395
Validation loss: 2.213344015100951

Epoch: 6| Step: 6
Training loss: 1.9910229444503784
Validation loss: 2.226697575661444

Epoch: 6| Step: 7
Training loss: 2.635481834411621
Validation loss: 2.2287227338360203

Epoch: 6| Step: 8
Training loss: 2.2873730659484863
Validation loss: 2.2199857722046556

Epoch: 6| Step: 9
Training loss: 2.086061477661133
Validation loss: 2.2319590378833074

Epoch: 6| Step: 10
Training loss: 3.397400379180908
Validation loss: 2.2292904700002363

Epoch: 6| Step: 11
Training loss: 2.2839841842651367
Validation loss: 2.2428705564109226

Epoch: 6| Step: 12
Training loss: 2.8468642234802246
Validation loss: 2.227703171391641

Epoch: 6| Step: 13
Training loss: 2.0617260932922363
Validation loss: 2.2297687761245237

Epoch: 247| Step: 0
Training loss: 2.563382148742676
Validation loss: 2.2434363903537875

Epoch: 6| Step: 1
Training loss: 2.3188085556030273
Validation loss: 2.2314299332198275

Epoch: 6| Step: 2
Training loss: 2.3105413913726807
Validation loss: 2.234820745324576

Epoch: 6| Step: 3
Training loss: 1.9012315273284912
Validation loss: 2.2382557520302395

Epoch: 6| Step: 4
Training loss: 2.2597551345825195
Validation loss: 2.2317512086642686

Epoch: 6| Step: 5
Training loss: 1.9503536224365234
Validation loss: 2.2356716868697957

Epoch: 6| Step: 6
Training loss: 2.2443532943725586
Validation loss: 2.2334278475853706

Epoch: 6| Step: 7
Training loss: 2.5032663345336914
Validation loss: 2.237344523911835

Epoch: 6| Step: 8
Training loss: 2.605975866317749
Validation loss: 2.2216964511461157

Epoch: 6| Step: 9
Training loss: 2.858488082885742
Validation loss: 2.226740506387526

Epoch: 6| Step: 10
Training loss: 2.2107656002044678
Validation loss: 2.231925174754153

Epoch: 6| Step: 11
Training loss: 2.581916332244873
Validation loss: 2.2384954255114318

Epoch: 6| Step: 12
Training loss: 3.1343066692352295
Validation loss: 2.2383186919714815

Epoch: 6| Step: 13
Training loss: 2.14510440826416
Validation loss: 2.2494023487132084

Epoch: 248| Step: 0
Training loss: 2.6231637001037598
Validation loss: 2.2697594217074815

Epoch: 6| Step: 1
Training loss: 2.8499717712402344
Validation loss: 2.262175339524464

Epoch: 6| Step: 2
Training loss: 3.409543037414551
Validation loss: 2.2824130878653577

Epoch: 6| Step: 3
Training loss: 2.5221500396728516
Validation loss: 2.280021402143663

Epoch: 6| Step: 4
Training loss: 1.8189384937286377
Validation loss: 2.274319512869722

Epoch: 6| Step: 5
Training loss: 2.672330379486084
Validation loss: 2.291304014062369

Epoch: 6| Step: 6
Training loss: 2.5217270851135254
Validation loss: 2.274325406679543

Epoch: 6| Step: 7
Training loss: 2.343160629272461
Validation loss: 2.2808837762442966

Epoch: 6| Step: 8
Training loss: 2.598541498184204
Validation loss: 2.2818130370109313

Epoch: 6| Step: 9
Training loss: 1.830308198928833
Validation loss: 2.2681060632069907

Epoch: 6| Step: 10
Training loss: 1.8665947914123535
Validation loss: 2.263458208371234

Epoch: 6| Step: 11
Training loss: 2.1819908618927
Validation loss: 2.25598927210736

Epoch: 6| Step: 12
Training loss: 1.8641438484191895
Validation loss: 2.2430741812593196

Epoch: 6| Step: 13
Training loss: 3.1458826065063477
Validation loss: 2.2247765397512786

Epoch: 249| Step: 0
Training loss: 2.5524120330810547
Validation loss: 2.250069087551486

Epoch: 6| Step: 1
Training loss: 1.5588994026184082
Validation loss: 2.229477467075471

Epoch: 6| Step: 2
Training loss: 2.2927000522613525
Validation loss: 2.240650817912112

Epoch: 6| Step: 3
Training loss: 2.342420816421509
Validation loss: 2.2274311422019877

Epoch: 6| Step: 4
Training loss: 3.0346951484680176
Validation loss: 2.2411410911108858

Epoch: 6| Step: 5
Training loss: 2.5366454124450684
Validation loss: 2.2332243432280836

Epoch: 6| Step: 6
Training loss: 2.1818652153015137
Validation loss: 2.2332013524988645

Epoch: 6| Step: 7
Training loss: 3.0128138065338135
Validation loss: 2.224380916164767

Epoch: 6| Step: 8
Training loss: 2.43532133102417
Validation loss: 2.2261978400650846

Epoch: 6| Step: 9
Training loss: 2.667947292327881
Validation loss: 2.218508930616481

Epoch: 6| Step: 10
Training loss: 2.0320024490356445
Validation loss: 2.2227066204112065

Epoch: 6| Step: 11
Training loss: 3.0747811794281006
Validation loss: 2.2285897962508665

Epoch: 6| Step: 12
Training loss: 1.7106399536132812
Validation loss: 2.229453827745171

Epoch: 6| Step: 13
Training loss: 2.677849292755127
Validation loss: 2.2197848443062074

Epoch: 250| Step: 0
Training loss: 2.7339301109313965
Validation loss: 2.2349443153668473

Epoch: 6| Step: 1
Training loss: 2.37811279296875
Validation loss: 2.2426988258156726

Epoch: 6| Step: 2
Training loss: 2.607097864151001
Validation loss: 2.2444696682755665

Epoch: 6| Step: 3
Training loss: 2.416585922241211
Validation loss: 2.241575417980071

Epoch: 6| Step: 4
Training loss: 2.07761812210083
Validation loss: 2.2456147414381786

Epoch: 6| Step: 5
Training loss: 2.588740587234497
Validation loss: 2.26367417330383

Epoch: 6| Step: 6
Training loss: 2.5084052085876465
Validation loss: 2.2563630406574537

Epoch: 6| Step: 7
Training loss: 2.931318998336792
Validation loss: 2.2572025868200485

Epoch: 6| Step: 8
Training loss: 2.8613715171813965
Validation loss: 2.266863435827276

Epoch: 6| Step: 9
Training loss: 2.6463418006896973
Validation loss: 2.259865432657221

Epoch: 6| Step: 10
Training loss: 2.2886228561401367
Validation loss: 2.233244228106673

Epoch: 6| Step: 11
Training loss: 2.089775323867798
Validation loss: 2.2275009898729223

Epoch: 6| Step: 12
Training loss: 2.3117854595184326
Validation loss: 2.217229417575303

Epoch: 6| Step: 13
Training loss: 0.619220495223999
Validation loss: 2.2177409536095074

Epoch: 251| Step: 0
Training loss: 3.0730109214782715
Validation loss: 2.2023752786779918

Epoch: 6| Step: 1
Training loss: 1.8817760944366455
Validation loss: 2.216710244455645

Epoch: 6| Step: 2
Training loss: 2.1568899154663086
Validation loss: 2.2143366823914232

Epoch: 6| Step: 3
Training loss: 2.494013786315918
Validation loss: 2.2094654934380644

Epoch: 6| Step: 4
Training loss: 2.1137940883636475
Validation loss: 2.203703370145572

Epoch: 6| Step: 5
Training loss: 1.9452910423278809
Validation loss: 2.2137366110278713

Epoch: 6| Step: 6
Training loss: 2.4070191383361816
Validation loss: 2.2166580205322592

Epoch: 6| Step: 7
Training loss: 2.140303134918213
Validation loss: 2.223263250884189

Epoch: 6| Step: 8
Training loss: 2.2890501022338867
Validation loss: 2.227843730680404

Epoch: 6| Step: 9
Training loss: 2.4323906898498535
Validation loss: 2.2137144778364446

Epoch: 6| Step: 10
Training loss: 3.1464667320251465
Validation loss: 2.2219021269070205

Epoch: 6| Step: 11
Training loss: 2.3972859382629395
Validation loss: 2.228310138948502

Epoch: 6| Step: 12
Training loss: 2.390481948852539
Validation loss: 2.2235425902951147

Epoch: 6| Step: 13
Training loss: 3.1347439289093018
Validation loss: 2.2259896057908253

Epoch: 252| Step: 0
Training loss: 2.559410333633423
Validation loss: 2.2417923942688973

Epoch: 6| Step: 1
Training loss: 2.233366012573242
Validation loss: 2.2349725179774786

Epoch: 6| Step: 2
Training loss: 2.14574933052063
Validation loss: 2.218605678568604

Epoch: 6| Step: 3
Training loss: 2.2533984184265137
Validation loss: 2.2050005133434007

Epoch: 6| Step: 4
Training loss: 1.8662000894546509
Validation loss: 2.210525525513516

Epoch: 6| Step: 5
Training loss: 2.3335111141204834
Validation loss: 2.2072215208443264

Epoch: 6| Step: 6
Training loss: 3.0841164588928223
Validation loss: 2.209112100703742

Epoch: 6| Step: 7
Training loss: 2.028116464614868
Validation loss: 2.2217874552613948

Epoch: 6| Step: 8
Training loss: 2.4295647144317627
Validation loss: 2.2170142230167182

Epoch: 6| Step: 9
Training loss: 2.1969597339630127
Validation loss: 2.2156722930169876

Epoch: 6| Step: 10
Training loss: 2.867258071899414
Validation loss: 2.2348199377777758

Epoch: 6| Step: 11
Training loss: 2.8311147689819336
Validation loss: 2.2229374083139564

Epoch: 6| Step: 12
Training loss: 2.5245590209960938
Validation loss: 2.2118736877236316

Epoch: 6| Step: 13
Training loss: 2.3921875953674316
Validation loss: 2.2123963602127565

Epoch: 253| Step: 0
Training loss: 3.1845738887786865
Validation loss: 2.2123516016109015

Epoch: 6| Step: 1
Training loss: 2.2554385662078857
Validation loss: 2.2315417976789576

Epoch: 6| Step: 2
Training loss: 1.8757917881011963
Validation loss: 2.25233954768027

Epoch: 6| Step: 3
Training loss: 1.9923508167266846
Validation loss: 2.2714718157245266

Epoch: 6| Step: 4
Training loss: 2.2014107704162598
Validation loss: 2.2854226686621226

Epoch: 6| Step: 5
Training loss: 2.0453569889068604
Validation loss: 2.25412997891826

Epoch: 6| Step: 6
Training loss: 2.478043556213379
Validation loss: 2.259852091471354

Epoch: 6| Step: 7
Training loss: 2.632200002670288
Validation loss: 2.2493309346578454

Epoch: 6| Step: 8
Training loss: 2.8397316932678223
Validation loss: 2.2213378567849436

Epoch: 6| Step: 9
Training loss: 2.704986095428467
Validation loss: 2.229534851607456

Epoch: 6| Step: 10
Training loss: 2.5488786697387695
Validation loss: 2.212718722640827

Epoch: 6| Step: 11
Training loss: 2.1024067401885986
Validation loss: 2.21852687610093

Epoch: 6| Step: 12
Training loss: 2.3666231632232666
Validation loss: 2.2041781948458765

Epoch: 6| Step: 13
Training loss: 2.3184633255004883
Validation loss: 2.2103608218572472

Epoch: 254| Step: 0
Training loss: 2.463331937789917
Validation loss: 2.2231219430123605

Epoch: 6| Step: 1
Training loss: 2.232776403427124
Validation loss: 2.2024568152684036

Epoch: 6| Step: 2
Training loss: 1.8099277019500732
Validation loss: 2.2327867887353383

Epoch: 6| Step: 3
Training loss: 3.2217655181884766
Validation loss: 2.219272321270358

Epoch: 6| Step: 4
Training loss: 2.731778144836426
Validation loss: 2.209166685740153

Epoch: 6| Step: 5
Training loss: 1.991604208946228
Validation loss: 2.2172956056492303

Epoch: 6| Step: 6
Training loss: 2.0626490116119385
Validation loss: 2.1907469098285963

Epoch: 6| Step: 7
Training loss: 2.661057949066162
Validation loss: 2.210831942096833

Epoch: 6| Step: 8
Training loss: 1.5900455713272095
Validation loss: 2.2146913210550943

Epoch: 6| Step: 9
Training loss: 2.9805355072021484
Validation loss: 2.2192609233240925

Epoch: 6| Step: 10
Training loss: 2.474966049194336
Validation loss: 2.237087070301015

Epoch: 6| Step: 11
Training loss: 2.0528054237365723
Validation loss: 2.233568751683799

Epoch: 6| Step: 12
Training loss: 2.476198673248291
Validation loss: 2.235454979763236

Epoch: 6| Step: 13
Training loss: 3.029193878173828
Validation loss: 2.225173424648982

Epoch: 255| Step: 0
Training loss: 2.1085453033447266
Validation loss: 2.231026969930177

Epoch: 6| Step: 1
Training loss: 2.301270008087158
Validation loss: 2.212387807907597

Epoch: 6| Step: 2
Training loss: 2.3762106895446777
Validation loss: 2.2121739951513146

Epoch: 6| Step: 3
Training loss: 2.8215503692626953
Validation loss: 2.2088946655232418

Epoch: 6| Step: 4
Training loss: 2.046933650970459
Validation loss: 2.197982721431281

Epoch: 6| Step: 5
Training loss: 2.0835840702056885
Validation loss: 2.2055803832187446

Epoch: 6| Step: 6
Training loss: 2.531053066253662
Validation loss: 2.193119974546535

Epoch: 6| Step: 7
Training loss: 2.926888942718506
Validation loss: 2.1932692758498655

Epoch: 6| Step: 8
Training loss: 2.4910693168640137
Validation loss: 2.1903304207709526

Epoch: 6| Step: 9
Training loss: 2.7333664894104004
Validation loss: 2.1981302128043225

Epoch: 6| Step: 10
Training loss: 2.039271116256714
Validation loss: 2.192075057696271

Epoch: 6| Step: 11
Training loss: 2.263716220855713
Validation loss: 2.19303063936131

Epoch: 6| Step: 12
Training loss: 2.208515167236328
Validation loss: 2.192931448259661

Epoch: 6| Step: 13
Training loss: 2.531461477279663
Validation loss: 2.1893192132314048

Epoch: 256| Step: 0
Training loss: 1.4021399021148682
Validation loss: 2.2072350132849907

Epoch: 6| Step: 1
Training loss: 2.010549306869507
Validation loss: 2.2047579083391415

Epoch: 6| Step: 2
Training loss: 2.057650089263916
Validation loss: 2.2221899532502696

Epoch: 6| Step: 3
Training loss: 2.6653647422790527
Validation loss: 2.2244138461287304

Epoch: 6| Step: 4
Training loss: 2.896885871887207
Validation loss: 2.229059145014773

Epoch: 6| Step: 5
Training loss: 2.9992544651031494
Validation loss: 2.211069687720268

Epoch: 6| Step: 6
Training loss: 2.908461570739746
Validation loss: 2.218963223118936

Epoch: 6| Step: 7
Training loss: 2.924194812774658
Validation loss: 2.2130561233848653

Epoch: 6| Step: 8
Training loss: 2.320314884185791
Validation loss: 2.217433853815961

Epoch: 6| Step: 9
Training loss: 2.171908378601074
Validation loss: 2.226548846049975

Epoch: 6| Step: 10
Training loss: 2.1724679470062256
Validation loss: 2.217188572370878

Epoch: 6| Step: 11
Training loss: 2.3689815998077393
Validation loss: 2.2189851537827523

Epoch: 6| Step: 12
Training loss: 2.3559350967407227
Validation loss: 2.2194341562127553

Epoch: 6| Step: 13
Training loss: 2.237879514694214
Validation loss: 2.2156899667555288

Epoch: 257| Step: 0
Training loss: 2.519266366958618
Validation loss: 2.235170587416618

Epoch: 6| Step: 1
Training loss: 2.2012674808502197
Validation loss: 2.2179597475195445

Epoch: 6| Step: 2
Training loss: 2.8396153450012207
Validation loss: 2.221402205446715

Epoch: 6| Step: 3
Training loss: 2.4500083923339844
Validation loss: 2.2166734997944166

Epoch: 6| Step: 4
Training loss: 2.551091194152832
Validation loss: 2.229062580293225

Epoch: 6| Step: 5
Training loss: 2.436197280883789
Validation loss: 2.21246160486693

Epoch: 6| Step: 6
Training loss: 2.8176488876342773
Validation loss: 2.2199564441557853

Epoch: 6| Step: 7
Training loss: 2.57135009765625
Validation loss: 2.214307378697139

Epoch: 6| Step: 8
Training loss: 2.588895320892334
Validation loss: 2.199648459752401

Epoch: 6| Step: 9
Training loss: 3.2953505516052246
Validation loss: 2.199642276251188

Epoch: 6| Step: 10
Training loss: 1.8162202835083008
Validation loss: 2.2063036503330355

Epoch: 6| Step: 11
Training loss: 1.966787338256836
Validation loss: 2.1971198358843402

Epoch: 6| Step: 12
Training loss: 1.5130724906921387
Validation loss: 2.213789620707112

Epoch: 6| Step: 13
Training loss: 1.6987299919128418
Validation loss: 2.1943644118565384

Epoch: 258| Step: 0
Training loss: 2.226620674133301
Validation loss: 2.1966073538667414

Epoch: 6| Step: 1
Training loss: 2.983875274658203
Validation loss: 2.2001361564923356

Epoch: 6| Step: 2
Training loss: 2.7273550033569336
Validation loss: 2.1949755940386044

Epoch: 6| Step: 3
Training loss: 2.0796735286712646
Validation loss: 2.203184930227136

Epoch: 6| Step: 4
Training loss: 1.8399786949157715
Validation loss: 2.1935862520689606

Epoch: 6| Step: 5
Training loss: 2.386580467224121
Validation loss: 2.200338687948001

Epoch: 6| Step: 6
Training loss: 2.4434337615966797
Validation loss: 2.213396672279604

Epoch: 6| Step: 7
Training loss: 2.084278106689453
Validation loss: 2.2279773681394515

Epoch: 6| Step: 8
Training loss: 2.191540479660034
Validation loss: 2.2154915319975985

Epoch: 6| Step: 9
Training loss: 2.2498769760131836
Validation loss: 2.241822087636558

Epoch: 6| Step: 10
Training loss: 3.2635505199432373
Validation loss: 2.2463579562402542

Epoch: 6| Step: 11
Training loss: 2.073667049407959
Validation loss: 2.2314269517057683

Epoch: 6| Step: 12
Training loss: 2.730961561203003
Validation loss: 2.2389093086283696

Epoch: 6| Step: 13
Training loss: 1.989289402961731
Validation loss: 2.210381571964551

Epoch: 259| Step: 0
Training loss: 2.3069424629211426
Validation loss: 2.2234446925501667

Epoch: 6| Step: 1
Training loss: 1.901451587677002
Validation loss: 2.2215981816732757

Epoch: 6| Step: 2
Training loss: 2.2673377990722656
Validation loss: 2.2092684738097654

Epoch: 6| Step: 3
Training loss: 1.9415448904037476
Validation loss: 2.202345895510848

Epoch: 6| Step: 4
Training loss: 2.9045042991638184
Validation loss: 2.1991078558788506

Epoch: 6| Step: 5
Training loss: 3.0101308822631836
Validation loss: 2.2084792865219938

Epoch: 6| Step: 6
Training loss: 1.808355689048767
Validation loss: 2.226565079022479

Epoch: 6| Step: 7
Training loss: 2.2858550548553467
Validation loss: 2.246449265428769

Epoch: 6| Step: 8
Training loss: 2.926513195037842
Validation loss: 2.2536817417349866

Epoch: 6| Step: 9
Training loss: 2.513122797012329
Validation loss: 2.259277792387111

Epoch: 6| Step: 10
Training loss: 2.5282082557678223
Validation loss: 2.2666061027075655

Epoch: 6| Step: 11
Training loss: 2.0519089698791504
Validation loss: 2.259499060210361

Epoch: 6| Step: 12
Training loss: 2.3824734687805176
Validation loss: 2.2673050306176625

Epoch: 6| Step: 13
Training loss: 2.968770742416382
Validation loss: 2.254124520927347

Epoch: 260| Step: 0
Training loss: 2.65283465385437
Validation loss: 2.2483474695554344

Epoch: 6| Step: 1
Training loss: 1.7311336994171143
Validation loss: 2.2436484367616716

Epoch: 6| Step: 2
Training loss: 2.465876579284668
Validation loss: 2.225903880211615

Epoch: 6| Step: 3
Training loss: 2.01655912399292
Validation loss: 2.2195749103382068

Epoch: 6| Step: 4
Training loss: 2.1265017986297607
Validation loss: 2.190920017098868

Epoch: 6| Step: 5
Training loss: 2.850142478942871
Validation loss: 2.1947797293304117

Epoch: 6| Step: 6
Training loss: 2.437596321105957
Validation loss: 2.2018165562742498

Epoch: 6| Step: 7
Training loss: 2.8086647987365723
Validation loss: 2.202483041312105

Epoch: 6| Step: 8
Training loss: 1.9044196605682373
Validation loss: 2.207676301720322

Epoch: 6| Step: 9
Training loss: 3.287229537963867
Validation loss: 2.213063293887723

Epoch: 6| Step: 10
Training loss: 2.138997793197632
Validation loss: 2.2253564224448255

Epoch: 6| Step: 11
Training loss: 2.629843235015869
Validation loss: 2.2149978606931624

Epoch: 6| Step: 12
Training loss: 2.3889200687408447
Validation loss: 2.2080806442486343

Epoch: 6| Step: 13
Training loss: 1.7192668914794922
Validation loss: 2.211108789649061

Epoch: 261| Step: 0
Training loss: 2.0861315727233887
Validation loss: 2.2130732395315684

Epoch: 6| Step: 1
Training loss: 1.5450372695922852
Validation loss: 2.1965400480454966

Epoch: 6| Step: 2
Training loss: 2.575190544128418
Validation loss: 2.1806773677948983

Epoch: 6| Step: 3
Training loss: 2.7893152236938477
Validation loss: 2.1975422469518517

Epoch: 6| Step: 4
Training loss: 2.5633091926574707
Validation loss: 2.1865723799633723

Epoch: 6| Step: 5
Training loss: 2.0030481815338135
Validation loss: 2.1893157036073747

Epoch: 6| Step: 6
Training loss: 2.76794171333313
Validation loss: 2.193681888682868

Epoch: 6| Step: 7
Training loss: 3.312434673309326
Validation loss: 2.201571790120935

Epoch: 6| Step: 8
Training loss: 2.804293632507324
Validation loss: 2.216332745808427

Epoch: 6| Step: 9
Training loss: 2.2463693618774414
Validation loss: 2.2092689314196186

Epoch: 6| Step: 10
Training loss: 1.465835452079773
Validation loss: 2.211528050002231

Epoch: 6| Step: 11
Training loss: 2.5305469036102295
Validation loss: 2.2021424565263974

Epoch: 6| Step: 12
Training loss: 2.2192628383636475
Validation loss: 2.202508993046258

Epoch: 6| Step: 13
Training loss: 2.386420249938965
Validation loss: 2.208500385284424

Epoch: 262| Step: 0
Training loss: 2.3326635360717773
Validation loss: 2.1837126926709245

Epoch: 6| Step: 1
Training loss: 2.3156049251556396
Validation loss: 2.1825239671173917

Epoch: 6| Step: 2
Training loss: 2.487523078918457
Validation loss: 2.1877401644183743

Epoch: 6| Step: 3
Training loss: 2.3256564140319824
Validation loss: 2.1913702436672744

Epoch: 6| Step: 4
Training loss: 2.490450859069824
Validation loss: 2.192217726861277

Epoch: 6| Step: 5
Training loss: 2.2156858444213867
Validation loss: 2.197928555550114

Epoch: 6| Step: 6
Training loss: 2.3047235012054443
Validation loss: 2.204119159329322

Epoch: 6| Step: 7
Training loss: 2.871128559112549
Validation loss: 2.215888269485966

Epoch: 6| Step: 8
Training loss: 2.395017385482788
Validation loss: 2.2086650427951606

Epoch: 6| Step: 9
Training loss: 1.8996589183807373
Validation loss: 2.1989892272539038

Epoch: 6| Step: 10
Training loss: 2.1268503665924072
Validation loss: 2.21431851643388

Epoch: 6| Step: 11
Training loss: 2.716224193572998
Validation loss: 2.2265587417028283

Epoch: 6| Step: 12
Training loss: 2.8447017669677734
Validation loss: 2.2097035120892268

Epoch: 6| Step: 13
Training loss: 2.2399532794952393
Validation loss: 2.2022222857321463

Epoch: 263| Step: 0
Training loss: 1.7248824834823608
Validation loss: 2.2028383747223885

Epoch: 6| Step: 1
Training loss: 2.3272743225097656
Validation loss: 2.192580261538106

Epoch: 6| Step: 2
Training loss: 2.327643394470215
Validation loss: 2.1875907682603404

Epoch: 6| Step: 3
Training loss: 2.56173038482666
Validation loss: 2.1876644703649704

Epoch: 6| Step: 4
Training loss: 2.7427291870117188
Validation loss: 2.180163453984004

Epoch: 6| Step: 5
Training loss: 2.3720474243164062
Validation loss: 2.1844683411300823

Epoch: 6| Step: 6
Training loss: 1.9141407012939453
Validation loss: 2.195885758246145

Epoch: 6| Step: 7
Training loss: 2.6755924224853516
Validation loss: 2.1959189035559215

Epoch: 6| Step: 8
Training loss: 2.4695448875427246
Validation loss: 2.2137334846681163

Epoch: 6| Step: 9
Training loss: 2.0116119384765625
Validation loss: 2.2164020769057737

Epoch: 6| Step: 10
Training loss: 2.579317569732666
Validation loss: 2.2235395908355713

Epoch: 6| Step: 11
Training loss: 2.4439005851745605
Validation loss: 2.2093654742804905

Epoch: 6| Step: 12
Training loss: 2.1469595432281494
Validation loss: 2.2046901282443794

Epoch: 6| Step: 13
Training loss: 3.527346611022949
Validation loss: 2.197949206957253

Epoch: 264| Step: 0
Training loss: 2.4067585468292236
Validation loss: 2.2101872198043333

Epoch: 6| Step: 1
Training loss: 2.3418991565704346
Validation loss: 2.1954737478686916

Epoch: 6| Step: 2
Training loss: 1.739556908607483
Validation loss: 2.1930411118333057

Epoch: 6| Step: 3
Training loss: 2.2656848430633545
Validation loss: 2.198098326242098

Epoch: 6| Step: 4
Training loss: 1.9802345037460327
Validation loss: 2.2151845526951615

Epoch: 6| Step: 5
Training loss: 3.2520968914031982
Validation loss: 2.201145823283862

Epoch: 6| Step: 6
Training loss: 3.290464162826538
Validation loss: 2.21063127440791

Epoch: 6| Step: 7
Training loss: 2.0846710205078125
Validation loss: 2.2152196899537118

Epoch: 6| Step: 8
Training loss: 2.3210043907165527
Validation loss: 2.20947907047887

Epoch: 6| Step: 9
Training loss: 2.3839380741119385
Validation loss: 2.205794124193089

Epoch: 6| Step: 10
Training loss: 2.1856446266174316
Validation loss: 2.20835006108848

Epoch: 6| Step: 11
Training loss: 2.0681934356689453
Validation loss: 2.2009785482960362

Epoch: 6| Step: 12
Training loss: 2.4935266971588135
Validation loss: 2.2030226748476744

Epoch: 6| Step: 13
Training loss: 2.7896389961242676
Validation loss: 2.194895157250025

Epoch: 265| Step: 0
Training loss: 1.794711709022522
Validation loss: 2.1947832056271133

Epoch: 6| Step: 1
Training loss: 3.136277914047241
Validation loss: 2.2117832809366207

Epoch: 6| Step: 2
Training loss: 2.2989559173583984
Validation loss: 2.2396915907500894

Epoch: 6| Step: 3
Training loss: 2.8074870109558105
Validation loss: 2.224388814741565

Epoch: 6| Step: 4
Training loss: 2.5825822353363037
Validation loss: 2.23621488899313

Epoch: 6| Step: 5
Training loss: 2.141883134841919
Validation loss: 2.210744504005678

Epoch: 6| Step: 6
Training loss: 2.376720428466797
Validation loss: 2.2084052255076747

Epoch: 6| Step: 7
Training loss: 1.814266562461853
Validation loss: 2.2027987152017574

Epoch: 6| Step: 8
Training loss: 2.9819388389587402
Validation loss: 2.195007339600594

Epoch: 6| Step: 9
Training loss: 2.4332878589630127
Validation loss: 2.1795690828754055

Epoch: 6| Step: 10
Training loss: 2.666855573654175
Validation loss: 2.1732014738103396

Epoch: 6| Step: 11
Training loss: 1.3025002479553223
Validation loss: 2.172372856447774

Epoch: 6| Step: 12
Training loss: 2.3117947578430176
Validation loss: 2.1782549632492887

Epoch: 6| Step: 13
Training loss: 2.7840988636016846
Validation loss: 2.1800936780950075

Epoch: 266| Step: 0
Training loss: 2.001098155975342
Validation loss: 2.1756200585314023

Epoch: 6| Step: 1
Training loss: 2.88500714302063
Validation loss: 2.1763634297155563

Epoch: 6| Step: 2
Training loss: 2.4762651920318604
Validation loss: 2.1792305772022535

Epoch: 6| Step: 3
Training loss: 2.3084237575531006
Validation loss: 2.1984730048846175

Epoch: 6| Step: 4
Training loss: 1.6947154998779297
Validation loss: 2.203768407144854

Epoch: 6| Step: 5
Training loss: 2.0590550899505615
Validation loss: 2.1807631369559997

Epoch: 6| Step: 6
Training loss: 2.1913928985595703
Validation loss: 2.1925483339576313

Epoch: 6| Step: 7
Training loss: 2.425626754760742
Validation loss: 2.196511043015347

Epoch: 6| Step: 8
Training loss: 1.9790780544281006
Validation loss: 2.1840638883652224

Epoch: 6| Step: 9
Training loss: 2.9210433959960938
Validation loss: 2.19611600137526

Epoch: 6| Step: 10
Training loss: 2.5380983352661133
Validation loss: 2.1858094661466536

Epoch: 6| Step: 11
Training loss: 3.047945737838745
Validation loss: 2.1865893192188715

Epoch: 6| Step: 12
Training loss: 1.9803011417388916
Validation loss: 2.188885276035596

Epoch: 6| Step: 13
Training loss: 2.561436414718628
Validation loss: 2.186224458038166

Epoch: 267| Step: 0
Training loss: 2.0881261825561523
Validation loss: 2.1879399309876146

Epoch: 6| Step: 1
Training loss: 2.1687264442443848
Validation loss: 2.194922708695935

Epoch: 6| Step: 2
Training loss: 2.0002353191375732
Validation loss: 2.19686447933156

Epoch: 6| Step: 3
Training loss: 1.6983318328857422
Validation loss: 2.2161225631672847

Epoch: 6| Step: 4
Training loss: 2.710724353790283
Validation loss: 2.218832908138152

Epoch: 6| Step: 5
Training loss: 2.780566692352295
Validation loss: 2.227662783797069

Epoch: 6| Step: 6
Training loss: 2.011702060699463
Validation loss: 2.2150347002090944

Epoch: 6| Step: 7
Training loss: 2.4996938705444336
Validation loss: 2.239997956060594

Epoch: 6| Step: 8
Training loss: 2.7351908683776855
Validation loss: 2.232542298173392

Epoch: 6| Step: 9
Training loss: 2.537041187286377
Validation loss: 2.2156540142592562

Epoch: 6| Step: 10
Training loss: 3.0406851768493652
Validation loss: 2.2187414092402302

Epoch: 6| Step: 11
Training loss: 2.2781238555908203
Validation loss: 2.19470456851426

Epoch: 6| Step: 12
Training loss: 2.3955483436584473
Validation loss: 2.1761652654217136

Epoch: 6| Step: 13
Training loss: 2.3515384197235107
Validation loss: 2.179293199252057

Epoch: 268| Step: 0
Training loss: 2.323941946029663
Validation loss: 2.1920090721499537

Epoch: 6| Step: 1
Training loss: 3.0655555725097656
Validation loss: 2.2031480650747977

Epoch: 6| Step: 2
Training loss: 2.968926429748535
Validation loss: 2.1980165614876697

Epoch: 6| Step: 3
Training loss: 2.396449327468872
Validation loss: 2.2072412352408133

Epoch: 6| Step: 4
Training loss: 2.387096405029297
Validation loss: 2.2151520713683097

Epoch: 6| Step: 5
Training loss: 2.1252293586730957
Validation loss: 2.2120384785436813

Epoch: 6| Step: 6
Training loss: 1.8542665243148804
Validation loss: 2.200778308735099

Epoch: 6| Step: 7
Training loss: 2.2726166248321533
Validation loss: 2.1775553290561964

Epoch: 6| Step: 8
Training loss: 2.080824136734009
Validation loss: 2.1698186935917025

Epoch: 6| Step: 9
Training loss: 2.3146142959594727
Validation loss: 2.175374041321457

Epoch: 6| Step: 10
Training loss: 2.4401323795318604
Validation loss: 2.1637948148994037

Epoch: 6| Step: 11
Training loss: 1.9741127490997314
Validation loss: 2.1812239129056215

Epoch: 6| Step: 12
Training loss: 2.6080713272094727
Validation loss: 2.1682481227382535

Epoch: 6| Step: 13
Training loss: 2.5880706310272217
Validation loss: 2.181388499916241

Epoch: 269| Step: 0
Training loss: 2.2288687229156494
Validation loss: 2.1840575510455715

Epoch: 6| Step: 1
Training loss: 2.1797990798950195
Validation loss: 2.1803904579531763

Epoch: 6| Step: 2
Training loss: 2.3739824295043945
Validation loss: 2.181448416043353

Epoch: 6| Step: 3
Training loss: 2.0684919357299805
Validation loss: 2.1834720437244703

Epoch: 6| Step: 4
Training loss: 2.017068386077881
Validation loss: 2.1643162824774302

Epoch: 6| Step: 5
Training loss: 2.528616428375244
Validation loss: 2.175037714742845

Epoch: 6| Step: 6
Training loss: 2.167787790298462
Validation loss: 2.185504754384359

Epoch: 6| Step: 7
Training loss: 3.2520527839660645
Validation loss: 2.191405952617686

Epoch: 6| Step: 8
Training loss: 2.713366985321045
Validation loss: 2.178917457980494

Epoch: 6| Step: 9
Training loss: 1.990387201309204
Validation loss: 2.1805487037986837

Epoch: 6| Step: 10
Training loss: 2.5959551334381104
Validation loss: 2.181454586726363

Epoch: 6| Step: 11
Training loss: 2.3814103603363037
Validation loss: 2.180488165988717

Epoch: 6| Step: 12
Training loss: 2.830604076385498
Validation loss: 2.185335179810883

Epoch: 6| Step: 13
Training loss: 1.7480621337890625
Validation loss: 2.181686626967563

Epoch: 270| Step: 0
Training loss: 2.5520055294036865
Validation loss: 2.1840280461054977

Epoch: 6| Step: 1
Training loss: 2.5491175651550293
Validation loss: 2.1970306596448346

Epoch: 6| Step: 2
Training loss: 2.1109092235565186
Validation loss: 2.1963235601302116

Epoch: 6| Step: 3
Training loss: 3.1017420291900635
Validation loss: 2.2182659077387985

Epoch: 6| Step: 4
Training loss: 1.7613455057144165
Validation loss: 2.2294081975055

Epoch: 6| Step: 5
Training loss: 3.006004571914673
Validation loss: 2.2332521330925728

Epoch: 6| Step: 6
Training loss: 2.809218645095825
Validation loss: 2.2338956709830993

Epoch: 6| Step: 7
Training loss: 2.456026554107666
Validation loss: 2.215666755553215

Epoch: 6| Step: 8
Training loss: 1.53386390209198
Validation loss: 2.2025581200917563

Epoch: 6| Step: 9
Training loss: 2.8065052032470703
Validation loss: 2.169278190981957

Epoch: 6| Step: 10
Training loss: 1.9198992252349854
Validation loss: 2.175908370684552

Epoch: 6| Step: 11
Training loss: 1.6967439651489258
Validation loss: 2.1808972589431272

Epoch: 6| Step: 12
Training loss: 2.2603156566619873
Validation loss: 2.1913397747983216

Epoch: 6| Step: 13
Training loss: 2.9953088760375977
Validation loss: 2.1906788836243334

Epoch: 271| Step: 0
Training loss: 2.7101144790649414
Validation loss: 2.194800889620217

Epoch: 6| Step: 1
Training loss: 2.50051212310791
Validation loss: 2.1954388413377988

Epoch: 6| Step: 2
Training loss: 2.3731942176818848
Validation loss: 2.198361240407472

Epoch: 6| Step: 3
Training loss: 2.714360237121582
Validation loss: 2.2027720943573983

Epoch: 6| Step: 4
Training loss: 1.6084434986114502
Validation loss: 2.2152256683636735

Epoch: 6| Step: 5
Training loss: 2.8094241619110107
Validation loss: 2.2149047390107186

Epoch: 6| Step: 6
Training loss: 2.547220230102539
Validation loss: 2.202897430748068

Epoch: 6| Step: 7
Training loss: 2.2659082412719727
Validation loss: 2.19819809544471

Epoch: 6| Step: 8
Training loss: 1.9008104801177979
Validation loss: 2.209957209966516

Epoch: 6| Step: 9
Training loss: 2.0140295028686523
Validation loss: 2.201904732693908

Epoch: 6| Step: 10
Training loss: 2.3065719604492188
Validation loss: 2.20741097645093

Epoch: 6| Step: 11
Training loss: 2.5744853019714355
Validation loss: 2.219811460023285

Epoch: 6| Step: 12
Training loss: 3.056702136993408
Validation loss: 2.185574121372674

Epoch: 6| Step: 13
Training loss: 1.2308955192565918
Validation loss: 2.172605219707694

Epoch: 272| Step: 0
Training loss: 2.3344125747680664
Validation loss: 2.1755136020721926

Epoch: 6| Step: 1
Training loss: 2.3943581581115723
Validation loss: 2.1639218458565335

Epoch: 6| Step: 2
Training loss: 2.602656602859497
Validation loss: 2.167465249697367

Epoch: 6| Step: 3
Training loss: 2.343181848526001
Validation loss: 2.1630274531661824

Epoch: 6| Step: 4
Training loss: 1.5591816902160645
Validation loss: 2.1646554982790382

Epoch: 6| Step: 5
Training loss: 1.599706768989563
Validation loss: 2.1578358809153237

Epoch: 6| Step: 6
Training loss: 2.4925527572631836
Validation loss: 2.1712543400385047

Epoch: 6| Step: 7
Training loss: 2.4511466026306152
Validation loss: 2.170907243605583

Epoch: 6| Step: 8
Training loss: 2.6812736988067627
Validation loss: 2.166267979529596

Epoch: 6| Step: 9
Training loss: 3.205188512802124
Validation loss: 2.168310055168726

Epoch: 6| Step: 10
Training loss: 2.6924641132354736
Validation loss: 2.1624105104836087

Epoch: 6| Step: 11
Training loss: 1.9553139209747314
Validation loss: 2.163446803246775

Epoch: 6| Step: 12
Training loss: 2.608980655670166
Validation loss: 2.1700194317807435

Epoch: 6| Step: 13
Training loss: 2.1079981327056885
Validation loss: 2.1739861080723424

Epoch: 273| Step: 0
Training loss: 2.0946710109710693
Validation loss: 2.1821840091418196

Epoch: 6| Step: 1
Training loss: 2.7849249839782715
Validation loss: 2.208167160710981

Epoch: 6| Step: 2
Training loss: 2.3505759239196777
Validation loss: 2.2138465348110405

Epoch: 6| Step: 3
Training loss: 2.396923303604126
Validation loss: 2.2463380213706725

Epoch: 6| Step: 4
Training loss: 2.3796591758728027
Validation loss: 2.2377021005076747

Epoch: 6| Step: 5
Training loss: 2.370075225830078
Validation loss: 2.239879708136282

Epoch: 6| Step: 6
Training loss: 2.450183868408203
Validation loss: 2.231069052091209

Epoch: 6| Step: 7
Training loss: 2.112452268600464
Validation loss: 2.2100422125990673

Epoch: 6| Step: 8
Training loss: 2.05975604057312
Validation loss: 2.197444790153093

Epoch: 6| Step: 9
Training loss: 2.5766067504882812
Validation loss: 2.1912221652205273

Epoch: 6| Step: 10
Training loss: 2.214240550994873
Validation loss: 2.1667938591331564

Epoch: 6| Step: 11
Training loss: 2.538482904434204
Validation loss: 2.171617912989791

Epoch: 6| Step: 12
Training loss: 2.819448947906494
Validation loss: 2.158242884502616

Epoch: 6| Step: 13
Training loss: 1.93330979347229
Validation loss: 2.159672273102627

Epoch: 274| Step: 0
Training loss: 2.354182720184326
Validation loss: 2.175527985377978

Epoch: 6| Step: 1
Training loss: 2.5993621349334717
Validation loss: 2.160731638631513

Epoch: 6| Step: 2
Training loss: 2.5883641242980957
Validation loss: 2.173307841823947

Epoch: 6| Step: 3
Training loss: 2.5054664611816406
Validation loss: 2.1852235678703553

Epoch: 6| Step: 4
Training loss: 2.350259304046631
Validation loss: 2.2196960628673597

Epoch: 6| Step: 5
Training loss: 2.404351234436035
Validation loss: 2.2026549026530278

Epoch: 6| Step: 6
Training loss: 2.272324800491333
Validation loss: 2.2330457343850085

Epoch: 6| Step: 7
Training loss: 1.8934643268585205
Validation loss: 2.2151527122784684

Epoch: 6| Step: 8
Training loss: 2.210114002227783
Validation loss: 2.2137779471694783

Epoch: 6| Step: 9
Training loss: 2.540527105331421
Validation loss: 2.211134515782838

Epoch: 6| Step: 10
Training loss: 1.7911407947540283
Validation loss: 2.2102104527975923

Epoch: 6| Step: 11
Training loss: 2.7422304153442383
Validation loss: 2.1960855632699947

Epoch: 6| Step: 12
Training loss: 2.8680546283721924
Validation loss: 2.2026915832232405

Epoch: 6| Step: 13
Training loss: 2.023178815841675
Validation loss: 2.2065799979753393

Epoch: 275| Step: 0
Training loss: 2.455808401107788
Validation loss: 2.1881998200570383

Epoch: 6| Step: 1
Training loss: 2.6249446868896484
Validation loss: 2.1857171673928537

Epoch: 6| Step: 2
Training loss: 1.5817139148712158
Validation loss: 2.1830781121407785

Epoch: 6| Step: 3
Training loss: 2.199570655822754
Validation loss: 2.181083130580123

Epoch: 6| Step: 4
Training loss: 2.6024792194366455
Validation loss: 2.1612969393371255

Epoch: 6| Step: 5
Training loss: 2.818319797515869
Validation loss: 2.165740389977732

Epoch: 6| Step: 6
Training loss: 2.133035659790039
Validation loss: 2.182618864120976

Epoch: 6| Step: 7
Training loss: 2.7850382328033447
Validation loss: 2.1754892397952337

Epoch: 6| Step: 8
Training loss: 2.7923665046691895
Validation loss: 2.183357169551234

Epoch: 6| Step: 9
Training loss: 1.7401854991912842
Validation loss: 2.1777840199009066

Epoch: 6| Step: 10
Training loss: 2.15187406539917
Validation loss: 2.1861702537023895

Epoch: 6| Step: 11
Training loss: 2.346226453781128
Validation loss: 2.187082288085773

Epoch: 6| Step: 12
Training loss: 2.4467601776123047
Validation loss: 2.190138236168892

Epoch: 6| Step: 13
Training loss: 2.720672130584717
Validation loss: 2.18579521230472

Epoch: 276| Step: 0
Training loss: 2.052743911743164
Validation loss: 2.1940708698764926

Epoch: 6| Step: 1
Training loss: 2.552311897277832
Validation loss: 2.174131985633604

Epoch: 6| Step: 2
Training loss: 2.213134765625
Validation loss: 2.1862193281932543

Epoch: 6| Step: 3
Training loss: 2.0664942264556885
Validation loss: 2.1724325559472524

Epoch: 6| Step: 4
Training loss: 2.107750415802002
Validation loss: 2.1852833314608504

Epoch: 6| Step: 5
Training loss: 2.1055800914764404
Validation loss: 2.173827414871544

Epoch: 6| Step: 6
Training loss: 2.3851656913757324
Validation loss: 2.160968371616897

Epoch: 6| Step: 7
Training loss: 2.6410346031188965
Validation loss: 2.17277971647119

Epoch: 6| Step: 8
Training loss: 2.4674346446990967
Validation loss: 2.1664918250935052

Epoch: 6| Step: 9
Training loss: 2.8981878757476807
Validation loss: 2.1766397491578133

Epoch: 6| Step: 10
Training loss: 2.1135973930358887
Validation loss: 2.1805551282821165

Epoch: 6| Step: 11
Training loss: 2.8921539783477783
Validation loss: 2.1882007096403386

Epoch: 6| Step: 12
Training loss: 2.0441789627075195
Validation loss: 2.186906063428489

Epoch: 6| Step: 13
Training loss: 2.454683542251587
Validation loss: 2.1864749590555825

Epoch: 277| Step: 0
Training loss: 2.4895174503326416
Validation loss: 2.183507516819944

Epoch: 6| Step: 1
Training loss: 2.225757598876953
Validation loss: 2.177394638779343

Epoch: 6| Step: 2
Training loss: 2.6773264408111572
Validation loss: 2.1665120291453537

Epoch: 6| Step: 3
Training loss: 2.4050347805023193
Validation loss: 2.177158127548874

Epoch: 6| Step: 4
Training loss: 2.132941722869873
Validation loss: 2.183571248926142

Epoch: 6| Step: 5
Training loss: 1.905653715133667
Validation loss: 2.1729773424004994

Epoch: 6| Step: 6
Training loss: 2.6910741329193115
Validation loss: 2.173071998421864

Epoch: 6| Step: 7
Training loss: 2.6137375831604004
Validation loss: 2.183046858797791

Epoch: 6| Step: 8
Training loss: 1.8305559158325195
Validation loss: 2.1819792973097933

Epoch: 6| Step: 9
Training loss: 2.5012145042419434
Validation loss: 2.178868309144051

Epoch: 6| Step: 10
Training loss: 2.582216262817383
Validation loss: 2.1865060483255694

Epoch: 6| Step: 11
Training loss: 2.2253897190093994
Validation loss: 2.1731716817425144

Epoch: 6| Step: 12
Training loss: 2.059513807296753
Validation loss: 2.1766909424976637

Epoch: 6| Step: 13
Training loss: 2.8876442909240723
Validation loss: 2.163610722429009

Epoch: 278| Step: 0
Training loss: 2.489912509918213
Validation loss: 2.160965459321135

Epoch: 6| Step: 1
Training loss: 2.378845691680908
Validation loss: 2.1635001833720873

Epoch: 6| Step: 2
Training loss: 3.1102893352508545
Validation loss: 2.1687489619819065

Epoch: 6| Step: 3
Training loss: 2.290356159210205
Validation loss: 2.1528619668817006

Epoch: 6| Step: 4
Training loss: 1.7384378910064697
Validation loss: 2.1640856368567354

Epoch: 6| Step: 5
Training loss: 2.494633197784424
Validation loss: 2.1758873039676296

Epoch: 6| Step: 6
Training loss: 2.507845401763916
Validation loss: 2.1696791084863807

Epoch: 6| Step: 7
Training loss: 2.4985790252685547
Validation loss: 2.1751355022512455

Epoch: 6| Step: 8
Training loss: 1.9001927375793457
Validation loss: 2.166742235101679

Epoch: 6| Step: 9
Training loss: 1.7273354530334473
Validation loss: 2.1750195795489895

Epoch: 6| Step: 10
Training loss: 2.613532543182373
Validation loss: 2.1750616617100214

Epoch: 6| Step: 11
Training loss: 1.8590306043624878
Validation loss: 2.164871674711986

Epoch: 6| Step: 12
Training loss: 2.762862205505371
Validation loss: 2.1607716211708645

Epoch: 6| Step: 13
Training loss: 2.5352375507354736
Validation loss: 2.193406387041974

Epoch: 279| Step: 0
Training loss: 1.2346227169036865
Validation loss: 2.1966353347224574

Epoch: 6| Step: 1
Training loss: 2.4037859439849854
Validation loss: 2.196870101395474

Epoch: 6| Step: 2
Training loss: 2.444887161254883
Validation loss: 2.1874888917451263

Epoch: 6| Step: 3
Training loss: 1.7628862857818604
Validation loss: 2.196740699070756

Epoch: 6| Step: 4
Training loss: 2.217358112335205
Validation loss: 2.1889604009607786

Epoch: 6| Step: 5
Training loss: 2.2706410884857178
Validation loss: 2.185443029608778

Epoch: 6| Step: 6
Training loss: 2.698519706726074
Validation loss: 2.1850472573311097

Epoch: 6| Step: 7
Training loss: 2.3544158935546875
Validation loss: 2.1822985654236167

Epoch: 6| Step: 8
Training loss: 2.872926950454712
Validation loss: 2.175310939870855

Epoch: 6| Step: 9
Training loss: 2.7623562812805176
Validation loss: 2.1779848837083384

Epoch: 6| Step: 10
Training loss: 2.1661860942840576
Validation loss: 2.172251791082403

Epoch: 6| Step: 11
Training loss: 2.527500629425049
Validation loss: 2.1721484712375108

Epoch: 6| Step: 12
Training loss: 2.923373222351074
Validation loss: 2.173831852533484

Epoch: 6| Step: 13
Training loss: 1.9239981174468994
Validation loss: 2.192571561823609

Epoch: 280| Step: 0
Training loss: 2.6613855361938477
Validation loss: 2.1857026687232395

Epoch: 6| Step: 1
Training loss: 2.7954392433166504
Validation loss: 2.1790980754360074

Epoch: 6| Step: 2
Training loss: 2.264765739440918
Validation loss: 2.1756463666116037

Epoch: 6| Step: 3
Training loss: 3.1055731773376465
Validation loss: 2.156006106766321

Epoch: 6| Step: 4
Training loss: 2.3495383262634277
Validation loss: 2.1664378335399013

Epoch: 6| Step: 5
Training loss: 1.8769259452819824
Validation loss: 2.1427682279258646

Epoch: 6| Step: 6
Training loss: 2.3615102767944336
Validation loss: 2.159136672173777

Epoch: 6| Step: 7
Training loss: 2.7789833545684814
Validation loss: 2.155053045160027

Epoch: 6| Step: 8
Training loss: 2.0490479469299316
Validation loss: 2.1505052992092666

Epoch: 6| Step: 9
Training loss: 2.0952606201171875
Validation loss: 2.167234738667806

Epoch: 6| Step: 10
Training loss: 1.910595178604126
Validation loss: 2.1838813122882637

Epoch: 6| Step: 11
Training loss: 2.3893091678619385
Validation loss: 2.163535338576122

Epoch: 6| Step: 12
Training loss: 2.5447916984558105
Validation loss: 2.1623266461074993

Epoch: 6| Step: 13
Training loss: 1.3029303550720215
Validation loss: 2.173088942804644

Epoch: 281| Step: 0
Training loss: 2.037602424621582
Validation loss: 2.174367058661676

Epoch: 6| Step: 1
Training loss: 2.398353099822998
Validation loss: 2.16849596013305

Epoch: 6| Step: 2
Training loss: 2.2374441623687744
Validation loss: 2.179631822852678

Epoch: 6| Step: 3
Training loss: 2.75631046295166
Validation loss: 2.169695408113541

Epoch: 6| Step: 4
Training loss: 2.099654197692871
Validation loss: 2.1636689811624508

Epoch: 6| Step: 5
Training loss: 2.5573556423187256
Validation loss: 2.1549313350390364

Epoch: 6| Step: 6
Training loss: 2.629518985748291
Validation loss: 2.1622083725467807

Epoch: 6| Step: 7
Training loss: 2.794400215148926
Validation loss: 2.1637773283066286

Epoch: 6| Step: 8
Training loss: 2.628681182861328
Validation loss: 2.1479579146190355

Epoch: 6| Step: 9
Training loss: 1.5403387546539307
Validation loss: 2.159525066293696

Epoch: 6| Step: 10
Training loss: 2.504472255706787
Validation loss: 2.1597137169171403

Epoch: 6| Step: 11
Training loss: 1.2341759204864502
Validation loss: 2.1559460086207234

Epoch: 6| Step: 12
Training loss: 3.2628774642944336
Validation loss: 2.1673729201798797

Epoch: 6| Step: 13
Training loss: 1.95233154296875
Validation loss: 2.1663894191865

Epoch: 282| Step: 0
Training loss: 2.505201816558838
Validation loss: 2.164355888161608

Epoch: 6| Step: 1
Training loss: 1.9311280250549316
Validation loss: 2.1749823811233684

Epoch: 6| Step: 2
Training loss: 2.7645137310028076
Validation loss: 2.1809510646327848

Epoch: 6| Step: 3
Training loss: 2.470764398574829
Validation loss: 2.197603487199353

Epoch: 6| Step: 4
Training loss: 1.997172236442566
Validation loss: 2.1953723635724796

Epoch: 6| Step: 5
Training loss: 1.9388453960418701
Validation loss: 2.1949356294447377

Epoch: 6| Step: 6
Training loss: 3.051520347595215
Validation loss: 2.2081818990809943

Epoch: 6| Step: 7
Training loss: 1.9220577478408813
Validation loss: 2.1709356256710586

Epoch: 6| Step: 8
Training loss: 2.2561662197113037
Validation loss: 2.177081652866897

Epoch: 6| Step: 9
Training loss: 2.9422500133514404
Validation loss: 2.1643177155525453

Epoch: 6| Step: 10
Training loss: 2.5890045166015625
Validation loss: 2.1752714655732595

Epoch: 6| Step: 11
Training loss: 2.691802501678467
Validation loss: 2.1615066669320546

Epoch: 6| Step: 12
Training loss: 1.8750004768371582
Validation loss: 2.1698148865853586

Epoch: 6| Step: 13
Training loss: 1.5295674800872803
Validation loss: 2.156240731157282

Epoch: 283| Step: 0
Training loss: 1.8991111516952515
Validation loss: 2.147121867825908

Epoch: 6| Step: 1
Training loss: 1.6198272705078125
Validation loss: 2.1659294264290923

Epoch: 6| Step: 2
Training loss: 2.8576669692993164
Validation loss: 2.169037926581598

Epoch: 6| Step: 3
Training loss: 2.044160842895508
Validation loss: 2.1653039968141945

Epoch: 6| Step: 4
Training loss: 1.9292542934417725
Validation loss: 2.173850223582278

Epoch: 6| Step: 5
Training loss: 3.247896194458008
Validation loss: 2.16493724238488

Epoch: 6| Step: 6
Training loss: 2.53908109664917
Validation loss: 2.1685964112640708

Epoch: 6| Step: 7
Training loss: 1.8493680953979492
Validation loss: 2.1838119132544405

Epoch: 6| Step: 8
Training loss: 3.0610623359680176
Validation loss: 2.182087480380971

Epoch: 6| Step: 9
Training loss: 1.953037977218628
Validation loss: 2.1847159119062525

Epoch: 6| Step: 10
Training loss: 2.9785852432250977
Validation loss: 2.18661392119623

Epoch: 6| Step: 11
Training loss: 2.6382410526275635
Validation loss: 2.182264768949119

Epoch: 6| Step: 12
Training loss: 1.9044893980026245
Validation loss: 2.1795235167267504

Epoch: 6| Step: 13
Training loss: 2.2604150772094727
Validation loss: 2.1720558904832408

Epoch: 284| Step: 0
Training loss: 2.2496237754821777
Validation loss: 2.1634581614566106

Epoch: 6| Step: 1
Training loss: 2.367500066757202
Validation loss: 2.1741032010765484

Epoch: 6| Step: 2
Training loss: 1.5993914604187012
Validation loss: 2.175663478912846

Epoch: 6| Step: 3
Training loss: 2.579533100128174
Validation loss: 2.190863165804135

Epoch: 6| Step: 4
Training loss: 3.199489116668701
Validation loss: 2.1788805633462887

Epoch: 6| Step: 5
Training loss: 2.8815600872039795
Validation loss: 2.1899353252944125

Epoch: 6| Step: 6
Training loss: 2.1145882606506348
Validation loss: 2.186859194950391

Epoch: 6| Step: 7
Training loss: 1.981175422668457
Validation loss: 2.1872801755064275

Epoch: 6| Step: 8
Training loss: 2.5791168212890625
Validation loss: 2.1591688304819088

Epoch: 6| Step: 9
Training loss: 2.403844118118286
Validation loss: 2.1492010906178463

Epoch: 6| Step: 10
Training loss: 2.9670188426971436
Validation loss: 2.145107730742424

Epoch: 6| Step: 11
Training loss: 1.9205687046051025
Validation loss: 2.1424159183297107

Epoch: 6| Step: 12
Training loss: 1.839526891708374
Validation loss: 2.1482385640503256

Epoch: 6| Step: 13
Training loss: 1.8900206089019775
Validation loss: 2.1434129668820288

Epoch: 285| Step: 0
Training loss: 1.9029687643051147
Validation loss: 2.1572883423938545

Epoch: 6| Step: 1
Training loss: 2.198117971420288
Validation loss: 2.1442053523114932

Epoch: 6| Step: 2
Training loss: 2.5786495208740234
Validation loss: 2.1544460570940407

Epoch: 6| Step: 3
Training loss: 2.5526113510131836
Validation loss: 2.146169590693648

Epoch: 6| Step: 4
Training loss: 2.1691832542419434
Validation loss: 2.1444514964216497

Epoch: 6| Step: 5
Training loss: 2.902827262878418
Validation loss: 2.1529403348122873

Epoch: 6| Step: 6
Training loss: 2.224447727203369
Validation loss: 2.1425368016765964

Epoch: 6| Step: 7
Training loss: 2.320125102996826
Validation loss: 2.134184980905184

Epoch: 6| Step: 8
Training loss: 2.402465343475342
Validation loss: 2.139539131554224

Epoch: 6| Step: 9
Training loss: 1.8130944967269897
Validation loss: 2.1442615062959733

Epoch: 6| Step: 10
Training loss: 2.665513038635254
Validation loss: 2.1659308018222934

Epoch: 6| Step: 11
Training loss: 2.0538406372070312
Validation loss: 2.1783581702939925

Epoch: 6| Step: 12
Training loss: 2.9174537658691406
Validation loss: 2.18137026345858

Epoch: 6| Step: 13
Training loss: 1.8198846578598022
Validation loss: 2.230709429710142

Epoch: 286| Step: 0
Training loss: 2.726193428039551
Validation loss: 2.2323135714377127

Epoch: 6| Step: 1
Training loss: 2.3523662090301514
Validation loss: 2.239316250688286

Epoch: 6| Step: 2
Training loss: 2.331404685974121
Validation loss: 2.23940674976636

Epoch: 6| Step: 3
Training loss: 2.3937482833862305
Validation loss: 2.223113518889232

Epoch: 6| Step: 4
Training loss: 2.6405441761016846
Validation loss: 2.201777986300889

Epoch: 6| Step: 5
Training loss: 2.976515293121338
Validation loss: 2.1775438888098604

Epoch: 6| Step: 6
Training loss: 1.8396755456924438
Validation loss: 2.1591274763948176

Epoch: 6| Step: 7
Training loss: 2.234492778778076
Validation loss: 2.1493729135041595

Epoch: 6| Step: 8
Training loss: 2.196249008178711
Validation loss: 2.138421430382677

Epoch: 6| Step: 9
Training loss: 1.918358325958252
Validation loss: 2.141970037132181

Epoch: 6| Step: 10
Training loss: 2.273660659790039
Validation loss: 2.142101877479143

Epoch: 6| Step: 11
Training loss: 2.436737060546875
Validation loss: 2.143804265606788

Epoch: 6| Step: 12
Training loss: 2.3178043365478516
Validation loss: 2.1565013034369356

Epoch: 6| Step: 13
Training loss: 2.1889331340789795
Validation loss: 2.1606858340642785

Epoch: 287| Step: 0
Training loss: 2.9453604221343994
Validation loss: 2.151342233022054

Epoch: 6| Step: 1
Training loss: 3.2034993171691895
Validation loss: 2.147779712113001

Epoch: 6| Step: 2
Training loss: 1.6182454824447632
Validation loss: 2.152910071034585

Epoch: 6| Step: 3
Training loss: 2.3322179317474365
Validation loss: 2.145717482413015

Epoch: 6| Step: 4
Training loss: 2.0569770336151123
Validation loss: 2.1451913772090787

Epoch: 6| Step: 5
Training loss: 2.1149790287017822
Validation loss: 2.1439503982502925

Epoch: 6| Step: 6
Training loss: 1.9505972862243652
Validation loss: 2.1640098479486283

Epoch: 6| Step: 7
Training loss: 2.752652168273926
Validation loss: 2.1612464253620436

Epoch: 6| Step: 8
Training loss: 3.0931923389434814
Validation loss: 2.163661236404091

Epoch: 6| Step: 9
Training loss: 1.8897740840911865
Validation loss: 2.185113642805366

Epoch: 6| Step: 10
Training loss: 2.224837303161621
Validation loss: 2.183622237174742

Epoch: 6| Step: 11
Training loss: 1.6794230937957764
Validation loss: 2.181749241326445

Epoch: 6| Step: 12
Training loss: 2.52925443649292
Validation loss: 2.1823415551134335

Epoch: 6| Step: 13
Training loss: 1.9444506168365479
Validation loss: 2.1756986546260055

Epoch: 288| Step: 0
Training loss: 2.9899258613586426
Validation loss: 2.1670758019211473

Epoch: 6| Step: 1
Training loss: 2.3021223545074463
Validation loss: 2.167582978484451

Epoch: 6| Step: 2
Training loss: 2.177624464035034
Validation loss: 2.146645240886237

Epoch: 6| Step: 3
Training loss: 2.1643149852752686
Validation loss: 2.1353482610435894

Epoch: 6| Step: 4
Training loss: 2.567370891571045
Validation loss: 2.1451274925662625

Epoch: 6| Step: 5
Training loss: 2.005922794342041
Validation loss: 2.1425012619264665

Epoch: 6| Step: 6
Training loss: 1.905179738998413
Validation loss: 2.132546281301847

Epoch: 6| Step: 7
Training loss: 2.4473211765289307
Validation loss: 2.1472955519153225

Epoch: 6| Step: 8
Training loss: 2.3929004669189453
Validation loss: 2.1498949066285165

Epoch: 6| Step: 9
Training loss: 2.695143222808838
Validation loss: 2.153694978324316

Epoch: 6| Step: 10
Training loss: 1.3685197830200195
Validation loss: 2.151028540826613

Epoch: 6| Step: 11
Training loss: 2.038579225540161
Validation loss: 2.1643221096325944

Epoch: 6| Step: 12
Training loss: 2.901886224746704
Validation loss: 2.1679823552408526

Epoch: 6| Step: 13
Training loss: 2.834064483642578
Validation loss: 2.1564052194677372

Epoch: 289| Step: 0
Training loss: 2.501950263977051
Validation loss: 2.165627935881256

Epoch: 6| Step: 1
Training loss: 1.7241063117980957
Validation loss: 2.143450311435166

Epoch: 6| Step: 2
Training loss: 2.2788069248199463
Validation loss: 2.157897282672185

Epoch: 6| Step: 3
Training loss: 2.7239885330200195
Validation loss: 2.149899899318654

Epoch: 6| Step: 4
Training loss: 2.3010740280151367
Validation loss: 2.1513150891950055

Epoch: 6| Step: 5
Training loss: 2.7110049724578857
Validation loss: 2.154711433636245

Epoch: 6| Step: 6
Training loss: 2.9034981727600098
Validation loss: 2.1512647674929712

Epoch: 6| Step: 7
Training loss: 2.0154712200164795
Validation loss: 2.1375764262291694

Epoch: 6| Step: 8
Training loss: 1.68285071849823
Validation loss: 2.146673657560861

Epoch: 6| Step: 9
Training loss: 2.512319326400757
Validation loss: 2.146293350445327

Epoch: 6| Step: 10
Training loss: 2.3160722255706787
Validation loss: 2.1389944194465556

Epoch: 6| Step: 11
Training loss: 2.780550718307495
Validation loss: 2.146035161069644

Epoch: 6| Step: 12
Training loss: 2.187155246734619
Validation loss: 2.147770879089191

Epoch: 6| Step: 13
Training loss: 1.4516549110412598
Validation loss: 2.149135130707936

Epoch: 290| Step: 0
Training loss: 2.509453296661377
Validation loss: 2.1643738618461033

Epoch: 6| Step: 1
Training loss: 1.628263235092163
Validation loss: 2.1749425498388146

Epoch: 6| Step: 2
Training loss: 1.7736833095550537
Validation loss: 2.166755496814687

Epoch: 6| Step: 3
Training loss: 2.8721847534179688
Validation loss: 2.1867996825966785

Epoch: 6| Step: 4
Training loss: 2.526109457015991
Validation loss: 2.187908236698438

Epoch: 6| Step: 5
Training loss: 3.2641329765319824
Validation loss: 2.171853478236865

Epoch: 6| Step: 6
Training loss: 2.3066067695617676
Validation loss: 2.172675981316515

Epoch: 6| Step: 7
Training loss: 2.25653338432312
Validation loss: 2.164153431051521

Epoch: 6| Step: 8
Training loss: 2.490187168121338
Validation loss: 2.1591508273155458

Epoch: 6| Step: 9
Training loss: 2.511075973510742
Validation loss: 2.1666723989671275

Epoch: 6| Step: 10
Training loss: 2.7283692359924316
Validation loss: 2.171526360255416

Epoch: 6| Step: 11
Training loss: 1.7263010740280151
Validation loss: 2.1647480431423394

Epoch: 6| Step: 12
Training loss: 2.4025039672851562
Validation loss: 2.174828371694011

Epoch: 6| Step: 13
Training loss: 1.039589762687683
Validation loss: 2.1593696250710437

Epoch: 291| Step: 0
Training loss: 1.8300538063049316
Validation loss: 2.1591190163807203

Epoch: 6| Step: 1
Training loss: 2.431035041809082
Validation loss: 2.1753392373361895

Epoch: 6| Step: 2
Training loss: 1.515343427658081
Validation loss: 2.1683685395025436

Epoch: 6| Step: 3
Training loss: 2.2490336894989014
Validation loss: 2.166289965311686

Epoch: 6| Step: 4
Training loss: 2.3788740634918213
Validation loss: 2.176458312619117

Epoch: 6| Step: 5
Training loss: 2.739640951156616
Validation loss: 2.171561405222903

Epoch: 6| Step: 6
Training loss: 1.7528936862945557
Validation loss: 2.154765967399843

Epoch: 6| Step: 7
Training loss: 2.1457581520080566
Validation loss: 2.168475212589387

Epoch: 6| Step: 8
Training loss: 2.444890260696411
Validation loss: 2.1395650832883772

Epoch: 6| Step: 9
Training loss: 2.446101665496826
Validation loss: 2.140481866816039

Epoch: 6| Step: 10
Training loss: 2.625030279159546
Validation loss: 2.1281731513238724

Epoch: 6| Step: 11
Training loss: 2.9749562740325928
Validation loss: 2.1342588381100724

Epoch: 6| Step: 12
Training loss: 2.828845500946045
Validation loss: 2.1341023675857054

Epoch: 6| Step: 13
Training loss: 2.2272140979766846
Validation loss: 2.127936106856151

Epoch: 292| Step: 0
Training loss: 2.0539608001708984
Validation loss: 2.1346131729823288

Epoch: 6| Step: 1
Training loss: 2.762913703918457
Validation loss: 2.1338952613133255

Epoch: 6| Step: 2
Training loss: 2.008579730987549
Validation loss: 2.1381022340507916

Epoch: 6| Step: 3
Training loss: 2.0999865531921387
Validation loss: 2.141601129244733

Epoch: 6| Step: 4
Training loss: 3.2981674671173096
Validation loss: 2.139569874732725

Epoch: 6| Step: 5
Training loss: 1.803337812423706
Validation loss: 2.143743357350749

Epoch: 6| Step: 6
Training loss: 2.156670570373535
Validation loss: 2.130921635576474

Epoch: 6| Step: 7
Training loss: 1.4849984645843506
Validation loss: 2.140484376620221

Epoch: 6| Step: 8
Training loss: 3.1975162029266357
Validation loss: 2.1461030026917816

Epoch: 6| Step: 9
Training loss: 2.3789961338043213
Validation loss: 2.149996994644083

Epoch: 6| Step: 10
Training loss: 2.253230571746826
Validation loss: 2.1351027757890764

Epoch: 6| Step: 11
Training loss: 2.0309104919433594
Validation loss: 2.1353587976066013

Epoch: 6| Step: 12
Training loss: 2.361642837524414
Validation loss: 2.150994690515662

Epoch: 6| Step: 13
Training loss: 3.062812089920044
Validation loss: 2.151326451250302

Epoch: 293| Step: 0
Training loss: 2.380557060241699
Validation loss: 2.1590883552387194

Epoch: 6| Step: 1
Training loss: 2.2136285305023193
Validation loss: 2.1713348332271782

Epoch: 6| Step: 2
Training loss: 2.4372806549072266
Validation loss: 2.1727401082233717

Epoch: 6| Step: 3
Training loss: 2.4894728660583496
Validation loss: 2.1886699122767292

Epoch: 6| Step: 4
Training loss: 2.854628086090088
Validation loss: 2.1796054506814606

Epoch: 6| Step: 5
Training loss: 2.0318965911865234
Validation loss: 2.1710199463752007

Epoch: 6| Step: 6
Training loss: 2.8295583724975586
Validation loss: 2.1546887684893865

Epoch: 6| Step: 7
Training loss: 2.1036829948425293
Validation loss: 2.1473887812706733

Epoch: 6| Step: 8
Training loss: 2.4195737838745117
Validation loss: 2.1518223670221146

Epoch: 6| Step: 9
Training loss: 1.4583708047866821
Validation loss: 2.145765443002024

Epoch: 6| Step: 10
Training loss: 2.6889452934265137
Validation loss: 2.1446258867940595

Epoch: 6| Step: 11
Training loss: 2.101015329360962
Validation loss: 2.1448448550316597

Epoch: 6| Step: 12
Training loss: 2.3284268379211426
Validation loss: 2.1637205693029586

Epoch: 6| Step: 13
Training loss: 2.274648666381836
Validation loss: 2.1474174684093845

Epoch: 294| Step: 0
Training loss: 2.8503713607788086
Validation loss: 2.146174589792887

Epoch: 6| Step: 1
Training loss: 2.6229774951934814
Validation loss: 2.1548434470289495

Epoch: 6| Step: 2
Training loss: 2.321810007095337
Validation loss: 2.153325198799051

Epoch: 6| Step: 3
Training loss: 1.6282339096069336
Validation loss: 2.1357477198364916

Epoch: 6| Step: 4
Training loss: 2.3983914852142334
Validation loss: 2.1461860056846374

Epoch: 6| Step: 5
Training loss: 2.5299458503723145
Validation loss: 2.1373595883769374

Epoch: 6| Step: 6
Training loss: 1.979574203491211
Validation loss: 2.1426557148656538

Epoch: 6| Step: 7
Training loss: 2.252790927886963
Validation loss: 2.145722871185631

Epoch: 6| Step: 8
Training loss: 1.9840238094329834
Validation loss: 2.1398151254141204

Epoch: 6| Step: 9
Training loss: 2.213312864303589
Validation loss: 2.149799600724251

Epoch: 6| Step: 10
Training loss: 3.0069990158081055
Validation loss: 2.1642497790757047

Epoch: 6| Step: 11
Training loss: 2.1496644020080566
Validation loss: 2.151698886707265

Epoch: 6| Step: 12
Training loss: 2.2330007553100586
Validation loss: 2.1421997444604033

Epoch: 6| Step: 13
Training loss: 2.1797091960906982
Validation loss: 2.136019583671324

Epoch: 295| Step: 0
Training loss: 2.4736976623535156
Validation loss: 2.1309607721144155

Epoch: 6| Step: 1
Training loss: 2.418224573135376
Validation loss: 2.1273171876066472

Epoch: 6| Step: 2
Training loss: 2.257460355758667
Validation loss: 2.1354599037478046

Epoch: 6| Step: 3
Training loss: 2.3392574787139893
Validation loss: 2.1496920470268495

Epoch: 6| Step: 4
Training loss: 2.5846927165985107
Validation loss: 2.142469431764336

Epoch: 6| Step: 5
Training loss: 1.691504955291748
Validation loss: 2.1517065161017963

Epoch: 6| Step: 6
Training loss: 2.2347817420959473
Validation loss: 2.1517509004121185

Epoch: 6| Step: 7
Training loss: 2.289433240890503
Validation loss: 2.1514391488926385

Epoch: 6| Step: 8
Training loss: 2.4022903442382812
Validation loss: 2.1547491793991416

Epoch: 6| Step: 9
Training loss: 1.897354245185852
Validation loss: 2.143185034874947

Epoch: 6| Step: 10
Training loss: 2.809123992919922
Validation loss: 2.1378726933592107

Epoch: 6| Step: 11
Training loss: 2.595181941986084
Validation loss: 2.1371829894281205

Epoch: 6| Step: 12
Training loss: 2.049271821975708
Validation loss: 2.1277865286796325

Epoch: 6| Step: 13
Training loss: 2.3547821044921875
Validation loss: 2.1244667089113625

Epoch: 296| Step: 0
Training loss: 2.0150365829467773
Validation loss: 2.1326713946557816

Epoch: 6| Step: 1
Training loss: 2.8933610916137695
Validation loss: 2.1479335600329983

Epoch: 6| Step: 2
Training loss: 2.2172422409057617
Validation loss: 2.1622323143866753

Epoch: 6| Step: 3
Training loss: 2.2593421936035156
Validation loss: 2.162544952925815

Epoch: 6| Step: 4
Training loss: 2.2423787117004395
Validation loss: 2.180552808187341

Epoch: 6| Step: 5
Training loss: 2.487804889678955
Validation loss: 2.1759326252886044

Epoch: 6| Step: 6
Training loss: 2.339226007461548
Validation loss: 2.1923728104560607

Epoch: 6| Step: 7
Training loss: 2.9008917808532715
Validation loss: 2.1973219097301526

Epoch: 6| Step: 8
Training loss: 1.9040151834487915
Validation loss: 2.1750761642250964

Epoch: 6| Step: 9
Training loss: 2.770313262939453
Validation loss: 2.172091189251151

Epoch: 6| Step: 10
Training loss: 2.8435165882110596
Validation loss: 2.1550250591770297

Epoch: 6| Step: 11
Training loss: 2.1593010425567627
Validation loss: 2.152075755980707

Epoch: 6| Step: 12
Training loss: 1.8181002140045166
Validation loss: 2.137430206421883

Epoch: 6| Step: 13
Training loss: 1.6454510688781738
Validation loss: 2.1349235555177093

Epoch: 297| Step: 0
Training loss: 2.751960277557373
Validation loss: 2.144313176472982

Epoch: 6| Step: 1
Training loss: 2.5278892517089844
Validation loss: 2.1225174178359327

Epoch: 6| Step: 2
Training loss: 2.6170029640197754
Validation loss: 2.129284556194018

Epoch: 6| Step: 3
Training loss: 2.2062745094299316
Validation loss: 2.131075538614745

Epoch: 6| Step: 4
Training loss: 2.6665968894958496
Validation loss: 2.1302454997134466

Epoch: 6| Step: 5
Training loss: 1.4302786588668823
Validation loss: 2.1382444366332023

Epoch: 6| Step: 6
Training loss: 1.8340253829956055
Validation loss: 2.1340859782311226

Epoch: 6| Step: 7
Training loss: 2.033803939819336
Validation loss: 2.1503686981816448

Epoch: 6| Step: 8
Training loss: 2.5113861560821533
Validation loss: 2.1206549444506244

Epoch: 6| Step: 9
Training loss: 2.237776517868042
Validation loss: 2.134974620675528

Epoch: 6| Step: 10
Training loss: 1.9671014547348022
Validation loss: 2.131001716019005

Epoch: 6| Step: 11
Training loss: 2.629328727722168
Validation loss: 2.1287852397529026

Epoch: 6| Step: 12
Training loss: 2.6102678775787354
Validation loss: 2.1342987834766345

Epoch: 6| Step: 13
Training loss: 2.457871675491333
Validation loss: 2.1383625102299515

Epoch: 298| Step: 0
Training loss: 2.467927932739258
Validation loss: 2.142088741384527

Epoch: 6| Step: 1
Training loss: 3.577014446258545
Validation loss: 2.1816599343412664

Epoch: 6| Step: 2
Training loss: 1.3608629703521729
Validation loss: 2.1580507832188762

Epoch: 6| Step: 3
Training loss: 2.3926024436950684
Validation loss: 2.1626278328639206

Epoch: 6| Step: 4
Training loss: 2.6455724239349365
Validation loss: 2.151374263148154

Epoch: 6| Step: 5
Training loss: 2.6729202270507812
Validation loss: 2.1535780583658526

Epoch: 6| Step: 6
Training loss: 1.6686784029006958
Validation loss: 2.1602627461956394

Epoch: 6| Step: 7
Training loss: 2.2130417823791504
Validation loss: 2.1383114091811644

Epoch: 6| Step: 8
Training loss: 2.0861964225769043
Validation loss: 2.1473273308046403

Epoch: 6| Step: 9
Training loss: 2.3454782962799072
Validation loss: 2.144598258438931

Epoch: 6| Step: 10
Training loss: 2.102357864379883
Validation loss: 2.1409878807683147

Epoch: 6| Step: 11
Training loss: 2.575916290283203
Validation loss: 2.1341673328030493

Epoch: 6| Step: 12
Training loss: 1.8706023693084717
Validation loss: 2.138587115913309

Epoch: 6| Step: 13
Training loss: 2.279914379119873
Validation loss: 2.1389260984236196

Epoch: 299| Step: 0
Training loss: 1.8259632587432861
Validation loss: 2.1469035712621545

Epoch: 6| Step: 1
Training loss: 2.920347213745117
Validation loss: 2.1527393787137923

Epoch: 6| Step: 2
Training loss: 2.6737565994262695
Validation loss: 2.1471404465295936

Epoch: 6| Step: 3
Training loss: 2.9636764526367188
Validation loss: 2.1595724680090465

Epoch: 6| Step: 4
Training loss: 2.051098346710205
Validation loss: 2.133744475662067

Epoch: 6| Step: 5
Training loss: 2.009021282196045
Validation loss: 2.1312637970011723

Epoch: 6| Step: 6
Training loss: 2.28407621383667
Validation loss: 2.13260240708628

Epoch: 6| Step: 7
Training loss: 2.153601884841919
Validation loss: 2.140075379802335

Epoch: 6| Step: 8
Training loss: 2.327972173690796
Validation loss: 2.1304013472731396

Epoch: 6| Step: 9
Training loss: 2.3571975231170654
Validation loss: 2.1317278954290573

Epoch: 6| Step: 10
Training loss: 2.331145763397217
Validation loss: 2.1334622829191145

Epoch: 6| Step: 11
Training loss: 1.811919927597046
Validation loss: 2.1426293567944596

Epoch: 6| Step: 12
Training loss: 2.2832705974578857
Validation loss: 2.148576828741258

Epoch: 6| Step: 13
Training loss: 2.3438920974731445
Validation loss: 2.146172311998183

Epoch: 300| Step: 0
Training loss: 2.499175548553467
Validation loss: 2.159498760777135

Epoch: 6| Step: 1
Training loss: 1.7838501930236816
Validation loss: 2.1531895822094334

Epoch: 6| Step: 2
Training loss: 2.9984564781188965
Validation loss: 2.150436388548984

Epoch: 6| Step: 3
Training loss: 2.406606674194336
Validation loss: 2.1524993693956764

Epoch: 6| Step: 4
Training loss: 2.781132459640503
Validation loss: 2.150593044937298

Epoch: 6| Step: 5
Training loss: 1.9537001848220825
Validation loss: 2.143252830351553

Epoch: 6| Step: 6
Training loss: 1.6798354387283325
Validation loss: 2.147750714773773

Epoch: 6| Step: 7
Training loss: 2.1214497089385986
Validation loss: 2.125482206703514

Epoch: 6| Step: 8
Training loss: 2.7619800567626953
Validation loss: 2.115373965232603

Epoch: 6| Step: 9
Training loss: 2.2063045501708984
Validation loss: 2.1281279722849527

Epoch: 6| Step: 10
Training loss: 2.18365478515625
Validation loss: 2.1395348092561126

Epoch: 6| Step: 11
Training loss: 2.0139365196228027
Validation loss: 2.1233560910788913

Epoch: 6| Step: 12
Training loss: 2.055750846862793
Validation loss: 2.1188470932745163

Epoch: 6| Step: 13
Training loss: 3.3684492111206055
Validation loss: 2.1404921982877996

Testing loss: 2.45562318166097

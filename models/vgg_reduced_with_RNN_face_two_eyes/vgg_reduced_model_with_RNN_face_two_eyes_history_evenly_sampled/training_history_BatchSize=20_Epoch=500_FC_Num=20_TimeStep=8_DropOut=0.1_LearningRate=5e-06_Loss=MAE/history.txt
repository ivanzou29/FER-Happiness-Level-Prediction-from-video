Epoch: 1| Step: 0
Training loss: 5.7319746017456055
Validation loss: 5.145595960719611

Epoch: 5| Step: 1
Training loss: 3.8783867359161377
Validation loss: 5.1411385382375405

Epoch: 5| Step: 2
Training loss: 4.532443046569824
Validation loss: 5.13730582370553

Epoch: 5| Step: 3
Training loss: 5.021827697753906
Validation loss: 5.133627788994902

Epoch: 5| Step: 4
Training loss: 5.868738174438477
Validation loss: 5.130026863467309

Epoch: 5| Step: 5
Training loss: 5.223122596740723
Validation loss: 5.126363954236431

Epoch: 5| Step: 6
Training loss: 5.3053717613220215
Validation loss: 5.1227023319531515

Epoch: 5| Step: 7
Training loss: 4.174263954162598
Validation loss: 5.118938933136643

Epoch: 5| Step: 8
Training loss: 5.017787933349609
Validation loss: 5.114992285287508

Epoch: 5| Step: 9
Training loss: 4.735111236572266
Validation loss: 5.110750762365198

Epoch: 5| Step: 10
Training loss: 4.6158270835876465
Validation loss: 5.106287776782948

Epoch: 2| Step: 0
Training loss: 4.723555088043213
Validation loss: 5.101910375779675

Epoch: 5| Step: 1
Training loss: 5.603649616241455
Validation loss: 5.096501924658335

Epoch: 5| Step: 2
Training loss: 6.221314907073975
Validation loss: 5.091533507070234

Epoch: 5| Step: 3
Training loss: 5.011961936950684
Validation loss: 5.085981702291837

Epoch: 5| Step: 4
Training loss: 3.458033800125122
Validation loss: 5.080315225867815

Epoch: 5| Step: 5
Training loss: 5.336993217468262
Validation loss: 5.0740094236148305

Epoch: 5| Step: 6
Training loss: 4.116230010986328
Validation loss: 5.0674941514128

Epoch: 5| Step: 7
Training loss: 5.782679557800293
Validation loss: 5.061161630897112

Epoch: 5| Step: 8
Training loss: 4.2454352378845215
Validation loss: 5.053669985904489

Epoch: 5| Step: 9
Training loss: 3.919522523880005
Validation loss: 5.046429131620673

Epoch: 5| Step: 10
Training loss: 5.1846137046813965
Validation loss: 5.038650728041126

Epoch: 3| Step: 0
Training loss: 5.157902717590332
Validation loss: 5.030859449858307

Epoch: 5| Step: 1
Training loss: 5.032408714294434
Validation loss: 5.021973809888286

Epoch: 5| Step: 2
Training loss: 4.050158500671387
Validation loss: 5.013334556292462

Epoch: 5| Step: 3
Training loss: 4.516131401062012
Validation loss: 5.00435891202701

Epoch: 5| Step: 4
Training loss: 6.03125
Validation loss: 4.994160682924332

Epoch: 5| Step: 5
Training loss: 3.9528450965881348
Validation loss: 4.984471387760614

Epoch: 5| Step: 6
Training loss: 5.114988803863525
Validation loss: 4.973791804364932

Epoch: 5| Step: 7
Training loss: 4.5099897384643555
Validation loss: 4.963174025217692

Epoch: 5| Step: 8
Training loss: 5.5376996994018555
Validation loss: 4.952086576851466

Epoch: 5| Step: 9
Training loss: 4.328486442565918
Validation loss: 4.941474781241468

Epoch: 5| Step: 10
Training loss: 4.242594242095947
Validation loss: 4.929259618123372

Epoch: 4| Step: 0
Training loss: 5.1352338790893555
Validation loss: 4.917029534616778

Epoch: 5| Step: 1
Training loss: 5.463808536529541
Validation loss: 4.904073704955398

Epoch: 5| Step: 2
Training loss: 5.009467124938965
Validation loss: 4.892186031546644

Epoch: 5| Step: 3
Training loss: 5.205201148986816
Validation loss: 4.877990338110155

Epoch: 5| Step: 4
Training loss: 3.4799046516418457
Validation loss: 4.864068349202474

Epoch: 5| Step: 5
Training loss: 4.53134822845459
Validation loss: 4.850354171568347

Epoch: 5| Step: 6
Training loss: 4.354829788208008
Validation loss: 4.835093246993198

Epoch: 5| Step: 7
Training loss: 4.931385040283203
Validation loss: 4.819988009750202

Epoch: 5| Step: 8
Training loss: 4.147573471069336
Validation loss: 4.804211872880177

Epoch: 5| Step: 9
Training loss: 5.003968238830566
Validation loss: 4.787408023752192

Epoch: 5| Step: 10
Training loss: 3.598801374435425
Validation loss: 4.769799252992035

Epoch: 5| Step: 0
Training loss: 5.002643585205078
Validation loss: 4.752526149954847

Epoch: 5| Step: 1
Training loss: 4.178205966949463
Validation loss: 4.733271029687697

Epoch: 5| Step: 2
Training loss: 4.299173831939697
Validation loss: 4.71419612310266

Epoch: 5| Step: 3
Training loss: 4.772564888000488
Validation loss: 4.69521568154776

Epoch: 5| Step: 4
Training loss: 3.032106876373291
Validation loss: 4.6735539590158774

Epoch: 5| Step: 5
Training loss: 3.9186313152313232
Validation loss: 4.654081667623212

Epoch: 5| Step: 6
Training loss: 5.446280479431152
Validation loss: 4.631234763770975

Epoch: 5| Step: 7
Training loss: 5.234097003936768
Validation loss: 4.6080384715910885

Epoch: 5| Step: 8
Training loss: 3.7770423889160156
Validation loss: 4.585276998499388

Epoch: 5| Step: 9
Training loss: 4.579517841339111
Validation loss: 4.561078756086288

Epoch: 5| Step: 10
Training loss: 4.624472141265869
Validation loss: 4.536348235222601

Epoch: 6| Step: 0
Training loss: 3.4072585105895996
Validation loss: 4.51112170885968

Epoch: 5| Step: 1
Training loss: 4.869791030883789
Validation loss: 4.488171013452673

Epoch: 5| Step: 2
Training loss: 4.0880937576293945
Validation loss: 4.463468702890539

Epoch: 5| Step: 3
Training loss: 5.171977519989014
Validation loss: 4.438639435716855

Epoch: 5| Step: 4
Training loss: 5.553945541381836
Validation loss: 4.41108444429213

Epoch: 5| Step: 5
Training loss: 3.9787228107452393
Validation loss: 4.38629291903588

Epoch: 5| Step: 6
Training loss: 4.523990631103516
Validation loss: 4.360733575718378

Epoch: 5| Step: 7
Training loss: 3.4888088703155518
Validation loss: 4.333272467377365

Epoch: 5| Step: 8
Training loss: 3.2172324657440186
Validation loss: 4.307213034681094

Epoch: 5| Step: 9
Training loss: 3.8095908164978027
Validation loss: 4.279136139859435

Epoch: 5| Step: 10
Training loss: 3.8173141479492188
Validation loss: 4.253857617737145

Epoch: 7| Step: 0
Training loss: 4.008980751037598
Validation loss: 4.226175585100727

Epoch: 5| Step: 1
Training loss: 4.029640197753906
Validation loss: 4.201581985719742

Epoch: 5| Step: 2
Training loss: 4.592620372772217
Validation loss: 4.17711494814965

Epoch: 5| Step: 3
Training loss: 4.320948600769043
Validation loss: 4.150953118519117

Epoch: 5| Step: 4
Training loss: 3.6384029388427734
Validation loss: 4.127189902849095

Epoch: 5| Step: 5
Training loss: 4.855268955230713
Validation loss: 4.104702513705018

Epoch: 5| Step: 6
Training loss: 3.5222525596618652
Validation loss: 4.080331694695257

Epoch: 5| Step: 7
Training loss: 2.7629568576812744
Validation loss: 4.056922215287403

Epoch: 5| Step: 8
Training loss: 3.36214017868042
Validation loss: 4.035742567431543

Epoch: 5| Step: 9
Training loss: 4.0448408126831055
Validation loss: 4.014036909226449

Epoch: 5| Step: 10
Training loss: 4.165986061096191
Validation loss: 3.991828464692639

Epoch: 8| Step: 0
Training loss: 5.01682186126709
Validation loss: 3.9719705274028163

Epoch: 5| Step: 1
Training loss: 3.6176598072052
Validation loss: 3.9517019230832338

Epoch: 5| Step: 2
Training loss: 3.7671875953674316
Validation loss: 3.933416617813931

Epoch: 5| Step: 3
Training loss: 2.9021096229553223
Validation loss: 3.9164266201757614

Epoch: 5| Step: 4
Training loss: 3.7689623832702637
Validation loss: 3.899613388123051

Epoch: 5| Step: 5
Training loss: 3.586033582687378
Validation loss: 3.883895476659139

Epoch: 5| Step: 6
Training loss: 4.464861869812012
Validation loss: 3.870474297513244

Epoch: 5| Step: 7
Training loss: 3.7399604320526123
Validation loss: 3.856165696215886

Epoch: 5| Step: 8
Training loss: 4.184004306793213
Validation loss: 3.841459107655351

Epoch: 5| Step: 9
Training loss: 2.522123336791992
Validation loss: 3.827938764326034

Epoch: 5| Step: 10
Training loss: 3.7137372493743896
Validation loss: 3.81697577814902

Epoch: 9| Step: 0
Training loss: 2.816314697265625
Validation loss: 3.804093263482535

Epoch: 5| Step: 1
Training loss: 4.7745819091796875
Validation loss: 3.7921692940496627

Epoch: 5| Step: 2
Training loss: 3.9463438987731934
Validation loss: 3.7809496797541136

Epoch: 5| Step: 3
Training loss: 3.7660059928894043
Validation loss: 3.767357095595329

Epoch: 5| Step: 4
Training loss: 3.3614342212677
Validation loss: 3.758773519146827

Epoch: 5| Step: 5
Training loss: 4.1883955001831055
Validation loss: 3.746293952388148

Epoch: 5| Step: 6
Training loss: 2.7751011848449707
Validation loss: 3.7343324743291384

Epoch: 5| Step: 7
Training loss: 4.279732704162598
Validation loss: 3.7225305880269697

Epoch: 5| Step: 8
Training loss: 3.2087600231170654
Validation loss: 3.7112253276250695

Epoch: 5| Step: 9
Training loss: 3.3425514698028564
Validation loss: 3.700963097233926

Epoch: 5| Step: 10
Training loss: 3.499598979949951
Validation loss: 3.688933698079919

Epoch: 10| Step: 0
Training loss: 3.1484789848327637
Validation loss: 3.6785547323124383

Epoch: 5| Step: 1
Training loss: 3.625950574874878
Validation loss: 3.6689131747009935

Epoch: 5| Step: 2
Training loss: 3.1152074337005615
Validation loss: 3.658632819370557

Epoch: 5| Step: 3
Training loss: 2.947526216506958
Validation loss: 3.648965681752851

Epoch: 5| Step: 4
Training loss: 4.5325117111206055
Validation loss: 3.6405862428808726

Epoch: 5| Step: 5
Training loss: 2.6828384399414062
Validation loss: 3.6270804815394904

Epoch: 5| Step: 6
Training loss: 4.045870304107666
Validation loss: 3.6190501028491604

Epoch: 5| Step: 7
Training loss: 4.3551926612854
Validation loss: 3.6090504225864204

Epoch: 5| Step: 8
Training loss: 3.3727927207946777
Validation loss: 3.5956698386899886

Epoch: 5| Step: 9
Training loss: 2.9807629585266113
Validation loss: 3.5882896018284622

Epoch: 5| Step: 10
Training loss: 4.184757232666016
Validation loss: 3.578997632508637

Epoch: 11| Step: 0
Training loss: 3.3875670433044434
Validation loss: 3.5688703649787494

Epoch: 5| Step: 1
Training loss: 3.0231308937072754
Validation loss: 3.5590273436679634

Epoch: 5| Step: 2
Training loss: 4.002079963684082
Validation loss: 3.551232635333974

Epoch: 5| Step: 3
Training loss: 3.734412431716919
Validation loss: 3.5443655085820023

Epoch: 5| Step: 4
Training loss: 5.002494812011719
Validation loss: 3.534455155813566

Epoch: 5| Step: 5
Training loss: 3.060011863708496
Validation loss: 3.5250583951191237

Epoch: 5| Step: 6
Training loss: 3.7732081413269043
Validation loss: 3.515588629630304

Epoch: 5| Step: 7
Training loss: 3.6733908653259277
Validation loss: 3.508768032955867

Epoch: 5| Step: 8
Training loss: 2.9051356315612793
Validation loss: 3.499906060516193

Epoch: 5| Step: 9
Training loss: 2.190563678741455
Validation loss: 3.4913227993954896

Epoch: 5| Step: 10
Training loss: 3.163539409637451
Validation loss: 3.481745240508869

Epoch: 12| Step: 0
Training loss: 3.2874808311462402
Validation loss: 3.475746044548609

Epoch: 5| Step: 1
Training loss: 2.9630792140960693
Validation loss: 3.4686574705185427

Epoch: 5| Step: 2
Training loss: 3.085635185241699
Validation loss: 3.4609315882446947

Epoch: 5| Step: 3
Training loss: 3.7269787788391113
Validation loss: 3.4516427106754755

Epoch: 5| Step: 4
Training loss: 4.465420722961426
Validation loss: 3.4439264830722602

Epoch: 5| Step: 5
Training loss: 2.313629150390625
Validation loss: 3.4367600846034225

Epoch: 5| Step: 6
Training loss: 3.0985913276672363
Validation loss: 3.428915177622149

Epoch: 5| Step: 7
Training loss: 4.2828474044799805
Validation loss: 3.4233833769316315

Epoch: 5| Step: 8
Training loss: 3.6719608306884766
Validation loss: 3.4143295159903904

Epoch: 5| Step: 9
Training loss: 2.7139010429382324
Validation loss: 3.404537526510095

Epoch: 5| Step: 10
Training loss: 3.5637035369873047
Validation loss: 3.3960068866770756

Epoch: 13| Step: 0
Training loss: 3.22084379196167
Validation loss: 3.385695739458966

Epoch: 5| Step: 1
Training loss: 2.887996196746826
Validation loss: 3.3799395766309512

Epoch: 5| Step: 2
Training loss: 3.581514835357666
Validation loss: 3.369883829547513

Epoch: 5| Step: 3
Training loss: 1.945471167564392
Validation loss: 3.3659929306276384

Epoch: 5| Step: 4
Training loss: 3.6324448585510254
Validation loss: 3.3559119650112685

Epoch: 5| Step: 5
Training loss: 3.5644888877868652
Validation loss: 3.342482446342386

Epoch: 5| Step: 6
Training loss: 2.7755508422851562
Validation loss: 3.3339175921614452

Epoch: 5| Step: 7
Training loss: 3.1974070072174072
Validation loss: 3.327509451937932

Epoch: 5| Step: 8
Training loss: 4.1631059646606445
Validation loss: 3.318345090394379

Epoch: 5| Step: 9
Training loss: 3.9600539207458496
Validation loss: 3.311420458619313

Epoch: 5| Step: 10
Training loss: 3.3953239917755127
Validation loss: 3.3050204246274886

Epoch: 14| Step: 0
Training loss: 2.936157464981079
Validation loss: 3.294899214980423

Epoch: 5| Step: 1
Training loss: 2.4269676208496094
Validation loss: 3.290963003712316

Epoch: 5| Step: 2
Training loss: 2.9226205348968506
Validation loss: 3.2856149647825506

Epoch: 5| Step: 3
Training loss: 3.789881944656372
Validation loss: 3.2834886504757788

Epoch: 5| Step: 4
Training loss: 2.5088391304016113
Validation loss: 3.2732206108749553

Epoch: 5| Step: 5
Training loss: 4.739884376525879
Validation loss: 3.2658476573164745

Epoch: 5| Step: 6
Training loss: 3.233396053314209
Validation loss: 3.259467719703592

Epoch: 5| Step: 7
Training loss: 2.513885736465454
Validation loss: 3.251444103897259

Epoch: 5| Step: 8
Training loss: 3.5810813903808594
Validation loss: 3.250957791523267

Epoch: 5| Step: 9
Training loss: 3.586613893508911
Validation loss: 3.2444112557236866

Epoch: 5| Step: 10
Training loss: 3.437283754348755
Validation loss: 3.237471262613932

Epoch: 15| Step: 0
Training loss: 4.125941276550293
Validation loss: 3.231181785624514

Epoch: 5| Step: 1
Training loss: 2.6475844383239746
Validation loss: 3.226480209699241

Epoch: 5| Step: 2
Training loss: 3.579087018966675
Validation loss: 3.2206311918074086

Epoch: 5| Step: 3
Training loss: 3.394782543182373
Validation loss: 3.2171836642808813

Epoch: 5| Step: 4
Training loss: 4.60496711730957
Validation loss: 3.212523878261607

Epoch: 5| Step: 5
Training loss: 2.274211883544922
Validation loss: 3.207111830352455

Epoch: 5| Step: 6
Training loss: 2.6495025157928467
Validation loss: 3.2056037431122153

Epoch: 5| Step: 7
Training loss: 2.899425506591797
Validation loss: 3.2014778762735348

Epoch: 5| Step: 8
Training loss: 2.855895519256592
Validation loss: 3.1961495466129755

Epoch: 5| Step: 9
Training loss: 2.8099822998046875
Validation loss: 3.196878643446071

Epoch: 5| Step: 10
Training loss: 3.391772508621216
Validation loss: 3.189461456832065

Epoch: 16| Step: 0
Training loss: 3.04642653465271
Validation loss: 3.18203390541897

Epoch: 5| Step: 1
Training loss: 3.702725648880005
Validation loss: 3.1771317374321724

Epoch: 5| Step: 2
Training loss: 2.637220621109009
Validation loss: 3.1759951217200166

Epoch: 5| Step: 3
Training loss: 3.7511496543884277
Validation loss: 3.173740079326014

Epoch: 5| Step: 4
Training loss: 2.9444198608398438
Validation loss: 3.1699692459516626

Epoch: 5| Step: 5
Training loss: 2.943854331970215
Validation loss: 3.161585689872824

Epoch: 5| Step: 6
Training loss: 2.7465033531188965
Validation loss: 3.154160996919037

Epoch: 5| Step: 7
Training loss: 3.286896228790283
Validation loss: 3.152420982237785

Epoch: 5| Step: 8
Training loss: 3.8150672912597656
Validation loss: 3.1518821947036253

Epoch: 5| Step: 9
Training loss: 3.6015231609344482
Validation loss: 3.1459111757175897

Epoch: 5| Step: 10
Training loss: 2.1587624549865723
Validation loss: 3.1462546138353247

Epoch: 17| Step: 0
Training loss: 3.2370314598083496
Validation loss: 3.1405526284248597

Epoch: 5| Step: 1
Training loss: 2.7126731872558594
Validation loss: 3.1356552134278

Epoch: 5| Step: 2
Training loss: 3.730177402496338
Validation loss: 3.1341481900984243

Epoch: 5| Step: 3
Training loss: 2.9468092918395996
Validation loss: 3.1295968127507034

Epoch: 5| Step: 4
Training loss: 2.9047434329986572
Validation loss: 3.1245260213011052

Epoch: 5| Step: 5
Training loss: 3.4066224098205566
Validation loss: 3.1224809974752445

Epoch: 5| Step: 6
Training loss: 3.3472037315368652
Validation loss: 3.1170565287272134

Epoch: 5| Step: 7
Training loss: 3.0259158611297607
Validation loss: 3.11548779344046

Epoch: 5| Step: 8
Training loss: 3.327667713165283
Validation loss: 3.1163253527815624

Epoch: 5| Step: 9
Training loss: 2.95585298538208
Validation loss: 3.1140504062816663

Epoch: 5| Step: 10
Training loss: 2.8317465782165527
Validation loss: 3.109147758894069

Epoch: 18| Step: 0
Training loss: 2.980891466140747
Validation loss: 3.0999007763401156

Epoch: 5| Step: 1
Training loss: 2.616205930709839
Validation loss: 3.097393687053393

Epoch: 5| Step: 2
Training loss: 2.864288806915283
Validation loss: 3.0952709387707453

Epoch: 5| Step: 3
Training loss: 3.6671550273895264
Validation loss: 3.0967994069540374

Epoch: 5| Step: 4
Training loss: 3.2379088401794434
Validation loss: 3.095872084299723

Epoch: 5| Step: 5
Training loss: 3.164034128189087
Validation loss: 3.086037366620956

Epoch: 5| Step: 6
Training loss: 3.9781670570373535
Validation loss: 3.0780365082525436

Epoch: 5| Step: 7
Training loss: 2.8883447647094727
Validation loss: 3.08038451081963

Epoch: 5| Step: 8
Training loss: 2.4289565086364746
Validation loss: 3.093768617158295

Epoch: 5| Step: 9
Training loss: 3.428601026535034
Validation loss: 3.124002569465227

Epoch: 5| Step: 10
Training loss: 3.0343611240386963
Validation loss: 3.10515853153762

Epoch: 19| Step: 0
Training loss: 3.546416759490967
Validation loss: 3.0978446827139905

Epoch: 5| Step: 1
Training loss: 3.548682689666748
Validation loss: 3.067996727522983

Epoch: 5| Step: 2
Training loss: 3.3343002796173096
Validation loss: 3.0661894275296118

Epoch: 5| Step: 3
Training loss: 2.7606680393218994
Validation loss: 3.066622411051104

Epoch: 5| Step: 4
Training loss: 3.1972134113311768
Validation loss: 3.0691573748024563

Epoch: 5| Step: 5
Training loss: 2.9970343112945557
Validation loss: 3.0677174137484644

Epoch: 5| Step: 6
Training loss: 2.859867572784424
Validation loss: 3.0678621184441353

Epoch: 5| Step: 7
Training loss: 2.4697461128234863
Validation loss: 3.0667676976931992

Epoch: 5| Step: 8
Training loss: 2.995495557785034
Validation loss: 3.0637210799801733

Epoch: 5| Step: 9
Training loss: 3.3967151641845703
Validation loss: 3.0624728561729513

Epoch: 5| Step: 10
Training loss: 3.0109596252441406
Validation loss: 3.0580669628676547

Epoch: 20| Step: 0
Training loss: 2.3606510162353516
Validation loss: 3.053922407088741

Epoch: 5| Step: 1
Training loss: 2.798048734664917
Validation loss: 3.0518461042834866

Epoch: 5| Step: 2
Training loss: 4.228405475616455
Validation loss: 3.0530544173332954

Epoch: 5| Step: 3
Training loss: 3.1430516242980957
Validation loss: 3.045697714692803

Epoch: 5| Step: 4
Training loss: 3.596527576446533
Validation loss: 3.044589978392406

Epoch: 5| Step: 5
Training loss: 2.9172661304473877
Validation loss: 3.0381166986239854

Epoch: 5| Step: 6
Training loss: 3.096647262573242
Validation loss: 3.043022496725923

Epoch: 5| Step: 7
Training loss: 3.317911148071289
Validation loss: 3.051787086712417

Epoch: 5| Step: 8
Training loss: 2.868259906768799
Validation loss: 3.0340048343904558

Epoch: 5| Step: 9
Training loss: 2.268022298812866
Validation loss: 3.0282952042036158

Epoch: 5| Step: 10
Training loss: 3.3620223999023438
Validation loss: 3.027081604926817

Epoch: 21| Step: 0
Training loss: 2.344486951828003
Validation loss: 3.0294983130629345

Epoch: 5| Step: 1
Training loss: 3.398221492767334
Validation loss: 3.0309853451226347

Epoch: 5| Step: 2
Training loss: 3.2395243644714355
Validation loss: 3.0267202264519146

Epoch: 5| Step: 3
Training loss: 3.796941041946411
Validation loss: 3.028380194017964

Epoch: 5| Step: 4
Training loss: 2.9827187061309814
Validation loss: 3.0222009894668416

Epoch: 5| Step: 5
Training loss: 2.68037748336792
Validation loss: 3.015738348807058

Epoch: 5| Step: 6
Training loss: 2.7798171043395996
Validation loss: 3.017128739305722

Epoch: 5| Step: 7
Training loss: 3.384876251220703
Validation loss: 3.01339114609585

Epoch: 5| Step: 8
Training loss: 3.8334031105041504
Validation loss: 3.0154592196146646

Epoch: 5| Step: 9
Training loss: 3.13995623588562
Validation loss: 3.0214690239198747

Epoch: 5| Step: 10
Training loss: 1.991398811340332
Validation loss: 3.009267455788069

Epoch: 22| Step: 0
Training loss: 3.7827022075653076
Validation loss: 3.0123911544840825

Epoch: 5| Step: 1
Training loss: 2.3952269554138184
Validation loss: 3.010728828368648

Epoch: 5| Step: 2
Training loss: 3.3401012420654297
Validation loss: 3.0026515324910483

Epoch: 5| Step: 3
Training loss: 2.5896825790405273
Validation loss: 2.9948763411532164

Epoch: 5| Step: 4
Training loss: 3.0415568351745605
Validation loss: 2.9960093318775134

Epoch: 5| Step: 5
Training loss: 3.108295440673828
Validation loss: 3.0032858720389743

Epoch: 5| Step: 6
Training loss: 3.80401611328125
Validation loss: 2.998703419521291

Epoch: 5| Step: 7
Training loss: 2.944258689880371
Validation loss: 2.9901696635830786

Epoch: 5| Step: 8
Training loss: 3.4988505840301514
Validation loss: 2.986724868897469

Epoch: 5| Step: 9
Training loss: 2.383666515350342
Validation loss: 2.9856704845223376

Epoch: 5| Step: 10
Training loss: 2.6388213634490967
Validation loss: 2.985579700880153

Epoch: 23| Step: 0
Training loss: 2.531752109527588
Validation loss: 2.985976449904903

Epoch: 5| Step: 1
Training loss: 3.1385629177093506
Validation loss: 2.9856630653463383

Epoch: 5| Step: 2
Training loss: 2.7787389755249023
Validation loss: 2.9815791883776264

Epoch: 5| Step: 3
Training loss: 3.5555214881896973
Validation loss: 2.9855059167390228

Epoch: 5| Step: 4
Training loss: 3.079375743865967
Validation loss: 2.9800044746809107

Epoch: 5| Step: 5
Training loss: 3.3307442665100098
Validation loss: 2.976155291321457

Epoch: 5| Step: 6
Training loss: 2.9818501472473145
Validation loss: 2.9740907017902662

Epoch: 5| Step: 7
Training loss: 3.365464687347412
Validation loss: 2.972144280710528

Epoch: 5| Step: 8
Training loss: 3.078747272491455
Validation loss: 2.9684539969249437

Epoch: 5| Step: 9
Training loss: 2.4996936321258545
Validation loss: 2.966466232012677

Epoch: 5| Step: 10
Training loss: 3.1348578929901123
Validation loss: 2.969875115220265

Epoch: 24| Step: 0
Training loss: 2.358581781387329
Validation loss: 2.9691469361705165

Epoch: 5| Step: 1
Training loss: 2.669306516647339
Validation loss: 2.964167574400543

Epoch: 5| Step: 2
Training loss: 3.7125916481018066
Validation loss: 2.9608133351931007

Epoch: 5| Step: 3
Training loss: 3.3238518238067627
Validation loss: 2.9586484586038897

Epoch: 5| Step: 4
Training loss: 3.1120800971984863
Validation loss: 2.9554843518041793

Epoch: 5| Step: 5
Training loss: 2.881941318511963
Validation loss: 2.9511139726126068

Epoch: 5| Step: 6
Training loss: 3.65558123588562
Validation loss: 2.950893255972093

Epoch: 5| Step: 7
Training loss: 2.301431655883789
Validation loss: 2.9496985302176526

Epoch: 5| Step: 8
Training loss: 3.5600619316101074
Validation loss: 2.9476781096509708

Epoch: 5| Step: 9
Training loss: 3.188865900039673
Validation loss: 2.9470730981519146

Epoch: 5| Step: 10
Training loss: 2.457974433898926
Validation loss: 2.9491518543612574

Epoch: 25| Step: 0
Training loss: 2.7282283306121826
Validation loss: 2.9481831545470865

Epoch: 5| Step: 1
Training loss: 3.4972903728485107
Validation loss: 2.9387834431022726

Epoch: 5| Step: 2
Training loss: 2.7134017944335938
Validation loss: 2.938283351159865

Epoch: 5| Step: 3
Training loss: 3.065446615219116
Validation loss: 2.943809529786469

Epoch: 5| Step: 4
Training loss: 3.0031211376190186
Validation loss: 2.9482224372125443

Epoch: 5| Step: 5
Training loss: 2.5668530464172363
Validation loss: 2.9526382825707875

Epoch: 5| Step: 6
Training loss: 3.2525906562805176
Validation loss: 2.951613956882108

Epoch: 5| Step: 7
Training loss: 3.6539623737335205
Validation loss: 2.944971053831039

Epoch: 5| Step: 8
Training loss: 2.9275736808776855
Validation loss: 2.939887172432356

Epoch: 5| Step: 9
Training loss: 2.6406009197235107
Validation loss: 2.9373133669617357

Epoch: 5| Step: 10
Training loss: 3.2170751094818115
Validation loss: 2.9382008737133396

Epoch: 26| Step: 0
Training loss: 2.687117099761963
Validation loss: 2.9388164858664236

Epoch: 5| Step: 1
Training loss: 2.8379592895507812
Validation loss: 2.9374521701566634

Epoch: 5| Step: 2
Training loss: 3.042948007583618
Validation loss: 2.934017981252363

Epoch: 5| Step: 3
Training loss: 2.327641248703003
Validation loss: 2.936186598193261

Epoch: 5| Step: 4
Training loss: 3.763918399810791
Validation loss: 2.9333803935717513

Epoch: 5| Step: 5
Training loss: 3.3561599254608154
Validation loss: 2.9303061116126274

Epoch: 5| Step: 6
Training loss: 2.858973264694214
Validation loss: 2.92862101780471

Epoch: 5| Step: 7
Training loss: 3.9129462242126465
Validation loss: 2.926343174390895

Epoch: 5| Step: 8
Training loss: 3.259835720062256
Validation loss: 2.9285974271835817

Epoch: 5| Step: 9
Training loss: 2.7535948753356934
Validation loss: 2.9310922058679725

Epoch: 5| Step: 10
Training loss: 2.1948633193969727
Validation loss: 2.925113308814264

Epoch: 27| Step: 0
Training loss: 3.0577430725097656
Validation loss: 2.924630302254872

Epoch: 5| Step: 1
Training loss: 3.265618085861206
Validation loss: 2.924518508295859

Epoch: 5| Step: 2
Training loss: 3.273777723312378
Validation loss: 2.924664097447549

Epoch: 5| Step: 3
Training loss: 3.108124256134033
Validation loss: 2.924112455819243

Epoch: 5| Step: 4
Training loss: 2.3943450450897217
Validation loss: 2.921672777463031

Epoch: 5| Step: 5
Training loss: 3.2243354320526123
Validation loss: 2.9179510480614117

Epoch: 5| Step: 6
Training loss: 3.7329418659210205
Validation loss: 2.915296587892758

Epoch: 5| Step: 7
Training loss: 2.119316339492798
Validation loss: 2.914119182094451

Epoch: 5| Step: 8
Training loss: 3.2791080474853516
Validation loss: 2.9130315242275113

Epoch: 5| Step: 9
Training loss: 2.7651565074920654
Validation loss: 2.906772918598626

Epoch: 5| Step: 10
Training loss: 2.7308640480041504
Validation loss: 2.910628080368042

Epoch: 28| Step: 0
Training loss: 3.828737258911133
Validation loss: 2.907892632228072

Epoch: 5| Step: 1
Training loss: 2.903064250946045
Validation loss: 2.9048764526203112

Epoch: 5| Step: 2
Training loss: 3.07277512550354
Validation loss: 2.903284770186229

Epoch: 5| Step: 3
Training loss: 3.5769362449645996
Validation loss: 2.9086935776536182

Epoch: 5| Step: 4
Training loss: 2.8875045776367188
Validation loss: 2.906018393014067

Epoch: 5| Step: 5
Training loss: 3.2438693046569824
Validation loss: 2.9056609343456965

Epoch: 5| Step: 6
Training loss: 3.1124215126037598
Validation loss: 2.9045648882465978

Epoch: 5| Step: 7
Training loss: 2.386857748031616
Validation loss: 2.9003348735071

Epoch: 5| Step: 8
Training loss: 2.569896697998047
Validation loss: 2.9011068754298712

Epoch: 5| Step: 9
Training loss: 2.6964216232299805
Validation loss: 2.9016233797996276

Epoch: 5| Step: 10
Training loss: 2.5293567180633545
Validation loss: 2.8947295783668436

Epoch: 29| Step: 0
Training loss: 3.108351230621338
Validation loss: 2.89222288644442

Epoch: 5| Step: 1
Training loss: 3.0145905017852783
Validation loss: 2.893660186439432

Epoch: 5| Step: 2
Training loss: 2.8173460960388184
Validation loss: 2.8939383106846965

Epoch: 5| Step: 3
Training loss: 3.170440673828125
Validation loss: 2.8942729350059264

Epoch: 5| Step: 4
Training loss: 3.6683437824249268
Validation loss: 2.8883333077994724

Epoch: 5| Step: 5
Training loss: 3.0096635818481445
Validation loss: 2.8895790807662474

Epoch: 5| Step: 6
Training loss: 2.2165207862854004
Validation loss: 2.88759429993168

Epoch: 5| Step: 7
Training loss: 3.054216146469116
Validation loss: 2.8882990139786915

Epoch: 5| Step: 8
Training loss: 2.9970059394836426
Validation loss: 2.8873614854710077

Epoch: 5| Step: 9
Training loss: 2.5313801765441895
Validation loss: 2.8881908693621234

Epoch: 5| Step: 10
Training loss: 3.2361364364624023
Validation loss: 2.886540236011628

Epoch: 30| Step: 0
Training loss: 2.818682909011841
Validation loss: 2.8853974624346663

Epoch: 5| Step: 1
Training loss: 2.9551196098327637
Validation loss: 2.8832645570078204

Epoch: 5| Step: 2
Training loss: 3.1500422954559326
Validation loss: 2.8824292305977113

Epoch: 5| Step: 3
Training loss: 3.4773566722869873
Validation loss: 2.8862351089395504

Epoch: 5| Step: 4
Training loss: 3.266584873199463
Validation loss: 2.8827936136594383

Epoch: 5| Step: 5
Training loss: 2.673253297805786
Validation loss: 2.8804554811087986

Epoch: 5| Step: 6
Training loss: 3.709280014038086
Validation loss: 2.8825285255268054

Epoch: 5| Step: 7
Training loss: 2.403949737548828
Validation loss: 2.8741820294369935

Epoch: 5| Step: 8
Training loss: 2.6540255546569824
Validation loss: 2.8815770661959084

Epoch: 5| Step: 9
Training loss: 2.7449231147766113
Validation loss: 2.8806019854801956

Epoch: 5| Step: 10
Training loss: 2.86031436920166
Validation loss: 2.8805989616660663

Epoch: 31| Step: 0
Training loss: 3.515700578689575
Validation loss: 2.876767812236663

Epoch: 5| Step: 1
Training loss: 3.5952014923095703
Validation loss: 2.8769772591129428

Epoch: 5| Step: 2
Training loss: 2.404205083847046
Validation loss: 2.875681677172261

Epoch: 5| Step: 3
Training loss: 3.2746939659118652
Validation loss: 2.8729405428773616

Epoch: 5| Step: 4
Training loss: 2.6602821350097656
Validation loss: 2.875182710668092

Epoch: 5| Step: 5
Training loss: 2.3785312175750732
Validation loss: 2.8752815236327467

Epoch: 5| Step: 6
Training loss: 2.432436466217041
Validation loss: 2.8719604117895967

Epoch: 5| Step: 7
Training loss: 4.016973495483398
Validation loss: 2.868212038470853

Epoch: 5| Step: 8
Training loss: 3.4426448345184326
Validation loss: 2.8710804113777737

Epoch: 5| Step: 9
Training loss: 2.5018844604492188
Validation loss: 2.8690787899878716

Epoch: 5| Step: 10
Training loss: 2.34395432472229
Validation loss: 2.864120888453658

Epoch: 32| Step: 0
Training loss: 3.5895168781280518
Validation loss: 2.867562073533253

Epoch: 5| Step: 1
Training loss: 2.8008294105529785
Validation loss: 2.8681831488045315

Epoch: 5| Step: 2
Training loss: 3.1361594200134277
Validation loss: 2.8683338729284142

Epoch: 5| Step: 3
Training loss: 2.4076688289642334
Validation loss: 2.86642054588564

Epoch: 5| Step: 4
Training loss: 3.294994354248047
Validation loss: 2.8669921659654185

Epoch: 5| Step: 5
Training loss: 3.2980332374572754
Validation loss: 2.870741149430634

Epoch: 5| Step: 6
Training loss: 2.7140727043151855
Validation loss: 2.868392759753812

Epoch: 5| Step: 7
Training loss: 2.4731411933898926
Validation loss: 2.8707078887570288

Epoch: 5| Step: 8
Training loss: 3.1983895301818848
Validation loss: 2.873147056948754

Epoch: 5| Step: 9
Training loss: 3.240077257156372
Validation loss: 2.8901736479933544

Epoch: 5| Step: 10
Training loss: 2.421506881713867
Validation loss: 2.869360331566103

Epoch: 33| Step: 0
Training loss: 2.861344575881958
Validation loss: 2.8661887568812214

Epoch: 5| Step: 1
Training loss: 3.417100429534912
Validation loss: 2.8644979256455616

Epoch: 5| Step: 2
Training loss: 2.888542652130127
Validation loss: 2.8645000944855394

Epoch: 5| Step: 3
Training loss: 2.91814923286438
Validation loss: 2.864540571807533

Epoch: 5| Step: 4
Training loss: 3.2901177406311035
Validation loss: 2.8652903726024013

Epoch: 5| Step: 5
Training loss: 2.976912021636963
Validation loss: 2.868255720343641

Epoch: 5| Step: 6
Training loss: 2.0414416790008545
Validation loss: 2.867710139161797

Epoch: 5| Step: 7
Training loss: 2.6779747009277344
Validation loss: 2.870995436945269

Epoch: 5| Step: 8
Training loss: 3.577711582183838
Validation loss: 2.8671564645664667

Epoch: 5| Step: 9
Training loss: 3.2372047901153564
Validation loss: 2.867142951616677

Epoch: 5| Step: 10
Training loss: 2.63716721534729
Validation loss: 2.8611653415105676

Epoch: 34| Step: 0
Training loss: 3.8160786628723145
Validation loss: 2.8615972585575555

Epoch: 5| Step: 1
Training loss: 2.7140052318573
Validation loss: 2.860834272958899

Epoch: 5| Step: 2
Training loss: 2.5917396545410156
Validation loss: 2.858958923688499

Epoch: 5| Step: 3
Training loss: 3.1548569202423096
Validation loss: 2.8578177062414025

Epoch: 5| Step: 4
Training loss: 2.8486742973327637
Validation loss: 2.8605032044072307

Epoch: 5| Step: 5
Training loss: 2.9175524711608887
Validation loss: 2.858685624214911

Epoch: 5| Step: 6
Training loss: 3.441047191619873
Validation loss: 2.859457762010636

Epoch: 5| Step: 7
Training loss: 2.687272310256958
Validation loss: 2.858841383329002

Epoch: 5| Step: 8
Training loss: 3.21087384223938
Validation loss: 2.861431993463988

Epoch: 5| Step: 9
Training loss: 2.7710728645324707
Validation loss: 2.8550992191478772

Epoch: 5| Step: 10
Training loss: 2.3181588649749756
Validation loss: 2.854099186517859

Epoch: 35| Step: 0
Training loss: 3.0399677753448486
Validation loss: 2.8512465518007994

Epoch: 5| Step: 1
Training loss: 2.545191526412964
Validation loss: 2.847378025772751

Epoch: 5| Step: 2
Training loss: 2.2875564098358154
Validation loss: 2.8491913939035065

Epoch: 5| Step: 3
Training loss: 3.047147750854492
Validation loss: 2.849183331253708

Epoch: 5| Step: 4
Training loss: 3.2940731048583984
Validation loss: 2.8507861116881013

Epoch: 5| Step: 5
Training loss: 3.1862261295318604
Validation loss: 2.850386283730948

Epoch: 5| Step: 6
Training loss: 2.877929210662842
Validation loss: 2.8433326546863844

Epoch: 5| Step: 7
Training loss: 3.231245517730713
Validation loss: 2.8412880179702595

Epoch: 5| Step: 8
Training loss: 2.7689452171325684
Validation loss: 2.842230699395621

Epoch: 5| Step: 9
Training loss: 3.349846363067627
Validation loss: 2.8387661108406643

Epoch: 5| Step: 10
Training loss: 2.853137969970703
Validation loss: 2.838093424356112

Epoch: 36| Step: 0
Training loss: 2.129838228225708
Validation loss: 2.840152209804904

Epoch: 5| Step: 1
Training loss: 3.087921619415283
Validation loss: 2.838320496261761

Epoch: 5| Step: 2
Training loss: 3.749809980392456
Validation loss: 2.8344830671946206

Epoch: 5| Step: 3
Training loss: 2.7787528038024902
Validation loss: 2.8370468693394817

Epoch: 5| Step: 4
Training loss: 3.0448811054229736
Validation loss: 2.8335678192877

Epoch: 5| Step: 5
Training loss: 3.665266752243042
Validation loss: 2.8367071485006683

Epoch: 5| Step: 6
Training loss: 2.8117403984069824
Validation loss: 2.8351379261221936

Epoch: 5| Step: 7
Training loss: 3.0540547370910645
Validation loss: 2.8320797412626204

Epoch: 5| Step: 8
Training loss: 3.17063570022583
Validation loss: 2.830153388361777

Epoch: 5| Step: 9
Training loss: 2.081705093383789
Validation loss: 2.827568474636283

Epoch: 5| Step: 10
Training loss: 2.8098385334014893
Validation loss: 2.832423445998981

Epoch: 37| Step: 0
Training loss: 2.669654607772827
Validation loss: 2.8267238114469793

Epoch: 5| Step: 1
Training loss: 2.715311288833618
Validation loss: 2.8283411149055726

Epoch: 5| Step: 2
Training loss: 2.802490711212158
Validation loss: 2.828060398819626

Epoch: 5| Step: 3
Training loss: 2.7305655479431152
Validation loss: 2.825304287736134

Epoch: 5| Step: 4
Training loss: 3.589883327484131
Validation loss: 2.826228964713312

Epoch: 5| Step: 5
Training loss: 2.9039652347564697
Validation loss: 2.8210782850942304

Epoch: 5| Step: 6
Training loss: 2.8692562580108643
Validation loss: 2.8153809911461285

Epoch: 5| Step: 7
Training loss: 2.7442126274108887
Validation loss: 2.8190246564085766

Epoch: 5| Step: 8
Training loss: 3.1149678230285645
Validation loss: 2.814163516926509

Epoch: 5| Step: 9
Training loss: 3.42042875289917
Validation loss: 2.8130979076508553

Epoch: 5| Step: 10
Training loss: 2.692472457885742
Validation loss: 2.8117932170949955

Epoch: 38| Step: 0
Training loss: 3.4445343017578125
Validation loss: 2.8147151624002764

Epoch: 5| Step: 1
Training loss: 2.677255153656006
Validation loss: 2.813474121914115

Epoch: 5| Step: 2
Training loss: 2.7620670795440674
Validation loss: 2.81303301934273

Epoch: 5| Step: 3
Training loss: 2.902479410171509
Validation loss: 2.8098549535197597

Epoch: 5| Step: 4
Training loss: 3.3824257850646973
Validation loss: 2.8089941804127028

Epoch: 5| Step: 5
Training loss: 2.4105708599090576
Validation loss: 2.811170503657351

Epoch: 5| Step: 6
Training loss: 2.6889736652374268
Validation loss: 2.809312005196848

Epoch: 5| Step: 7
Training loss: 2.683506488800049
Validation loss: 2.813175939744519

Epoch: 5| Step: 8
Training loss: 3.867023468017578
Validation loss: 2.811684831496208

Epoch: 5| Step: 9
Training loss: 2.929047107696533
Validation loss: 2.8155689854775705

Epoch: 5| Step: 10
Training loss: 2.365488290786743
Validation loss: 2.8097646954239055

Epoch: 39| Step: 0
Training loss: 3.2614128589630127
Validation loss: 2.807809845093758

Epoch: 5| Step: 1
Training loss: 1.5817314386367798
Validation loss: 2.809798732880623

Epoch: 5| Step: 2
Training loss: 2.9637579917907715
Validation loss: 2.806956301453293

Epoch: 5| Step: 3
Training loss: 2.81662654876709
Validation loss: 2.8047877511670514

Epoch: 5| Step: 4
Training loss: 3.005951404571533
Validation loss: 2.8055911935785764

Epoch: 5| Step: 5
Training loss: 3.279242753982544
Validation loss: 2.8049771862645305

Epoch: 5| Step: 6
Training loss: 2.551577091217041
Validation loss: 2.805329627888177

Epoch: 5| Step: 7
Training loss: 3.4123973846435547
Validation loss: 2.805846486040341

Epoch: 5| Step: 8
Training loss: 3.1328279972076416
Validation loss: 2.8034492179911625

Epoch: 5| Step: 9
Training loss: 3.690427303314209
Validation loss: 2.8034544760181057

Epoch: 5| Step: 10
Training loss: 2.437882900238037
Validation loss: 2.8030740317477973

Epoch: 40| Step: 0
Training loss: 2.2965705394744873
Validation loss: 2.79967430586456

Epoch: 5| Step: 1
Training loss: 2.8681061267852783
Validation loss: 2.803871011221281

Epoch: 5| Step: 2
Training loss: 3.588752031326294
Validation loss: 2.8025127380124983

Epoch: 5| Step: 3
Training loss: 3.195150136947632
Validation loss: 2.8010655269827893

Epoch: 5| Step: 4
Training loss: 2.494614362716675
Validation loss: 2.8015273770978375

Epoch: 5| Step: 5
Training loss: 3.6323981285095215
Validation loss: 2.7970036434870895

Epoch: 5| Step: 6
Training loss: 2.964276075363159
Validation loss: 2.796090707984022

Epoch: 5| Step: 7
Training loss: 2.3548057079315186
Validation loss: 2.795713932283463

Epoch: 5| Step: 8
Training loss: 3.1057610511779785
Validation loss: 2.796560784821869

Epoch: 5| Step: 9
Training loss: 3.1456499099731445
Validation loss: 2.792065102566955

Epoch: 5| Step: 10
Training loss: 2.387958288192749
Validation loss: 2.8001924253279165

Epoch: 41| Step: 0
Training loss: 2.3647873401641846
Validation loss: 2.798176811587426

Epoch: 5| Step: 1
Training loss: 2.7313342094421387
Validation loss: 2.7995241867598666

Epoch: 5| Step: 2
Training loss: 3.326586961746216
Validation loss: 2.804143836421351

Epoch: 5| Step: 3
Training loss: 2.847085475921631
Validation loss: 2.802900109239804

Epoch: 5| Step: 4
Training loss: 2.9389095306396484
Validation loss: 2.798343471301499

Epoch: 5| Step: 5
Training loss: 3.294154405593872
Validation loss: 2.80212305438134

Epoch: 5| Step: 6
Training loss: 2.9071624279022217
Validation loss: 2.7875407383006108

Epoch: 5| Step: 7
Training loss: 3.0094106197357178
Validation loss: 2.7914005556414203

Epoch: 5| Step: 8
Training loss: 2.913116931915283
Validation loss: 2.7908441482051725

Epoch: 5| Step: 9
Training loss: 2.9416871070861816
Validation loss: 2.794212977091471

Epoch: 5| Step: 10
Training loss: 2.8082332611083984
Validation loss: 2.795977279704104

Epoch: 42| Step: 0
Training loss: 2.8625030517578125
Validation loss: 2.8094308786494757

Epoch: 5| Step: 1
Training loss: 2.5598461627960205
Validation loss: 2.8139768338972524

Epoch: 5| Step: 2
Training loss: 2.545105457305908
Validation loss: 2.8151109295506633

Epoch: 5| Step: 3
Training loss: 3.187758207321167
Validation loss: 2.802748449387089

Epoch: 5| Step: 4
Training loss: 2.9921162128448486
Validation loss: 2.7999079099265476

Epoch: 5| Step: 5
Training loss: 2.3479697704315186
Validation loss: 2.794154251775434

Epoch: 5| Step: 6
Training loss: 3.2772536277770996
Validation loss: 2.791887285888836

Epoch: 5| Step: 7
Training loss: 2.870396375656128
Validation loss: 2.792164182150236

Epoch: 5| Step: 8
Training loss: 3.093698024749756
Validation loss: 2.7901689211527505

Epoch: 5| Step: 9
Training loss: 2.9781041145324707
Validation loss: 2.790241390146235

Epoch: 5| Step: 10
Training loss: 3.611715078353882
Validation loss: 2.789850732331635

Epoch: 43| Step: 0
Training loss: 2.840329170227051
Validation loss: 2.7885682557218816

Epoch: 5| Step: 1
Training loss: 2.8874926567077637
Validation loss: 2.787880346339236

Epoch: 5| Step: 2
Training loss: 3.4897594451904297
Validation loss: 2.7861448154654553

Epoch: 5| Step: 3
Training loss: 2.3042349815368652
Validation loss: 2.787860846006742

Epoch: 5| Step: 4
Training loss: 3.0394458770751953
Validation loss: 2.7881596524228334

Epoch: 5| Step: 5
Training loss: 2.8582119941711426
Validation loss: 2.787481205437773

Epoch: 5| Step: 6
Training loss: 2.2444875240325928
Validation loss: 2.789550099321591

Epoch: 5| Step: 7
Training loss: 2.4588093757629395
Validation loss: 2.787136406026861

Epoch: 5| Step: 8
Training loss: 3.047865629196167
Validation loss: 2.7877283045040664

Epoch: 5| Step: 9
Training loss: 2.9908218383789062
Validation loss: 2.7894546062715593

Epoch: 5| Step: 10
Training loss: 4.094754695892334
Validation loss: 2.7868696592187368

Epoch: 44| Step: 0
Training loss: 2.9753975868225098
Validation loss: 2.7862157232017926

Epoch: 5| Step: 1
Training loss: 2.8203601837158203
Validation loss: 2.7830320301876275

Epoch: 5| Step: 2
Training loss: 3.2033915519714355
Validation loss: 2.785241191105176

Epoch: 5| Step: 3
Training loss: 2.2564563751220703
Validation loss: 2.781832174588275

Epoch: 5| Step: 4
Training loss: 3.0732667446136475
Validation loss: 2.7791117134914605

Epoch: 5| Step: 5
Training loss: 3.054121732711792
Validation loss: 2.7790437654782365

Epoch: 5| Step: 6
Training loss: 2.766103744506836
Validation loss: 2.7766655183607534

Epoch: 5| Step: 7
Training loss: 2.5277864933013916
Validation loss: 2.775047840610627

Epoch: 5| Step: 8
Training loss: 2.4074203968048096
Validation loss: 2.7759533723195395

Epoch: 5| Step: 9
Training loss: 3.0731632709503174
Validation loss: 2.7755120595296225

Epoch: 5| Step: 10
Training loss: 4.040431022644043
Validation loss: 2.7783022772881294

Epoch: 45| Step: 0
Training loss: 2.7943625450134277
Validation loss: 2.7778848755744194

Epoch: 5| Step: 1
Training loss: 2.7692949771881104
Validation loss: 2.7770996080931796

Epoch: 5| Step: 2
Training loss: 3.0339195728302
Validation loss: 2.774613647050755

Epoch: 5| Step: 3
Training loss: 2.5121281147003174
Validation loss: 2.769053441221996

Epoch: 5| Step: 4
Training loss: 2.748671054840088
Validation loss: 2.770570642204695

Epoch: 5| Step: 5
Training loss: 3.5934345722198486
Validation loss: 2.7735560171065794

Epoch: 5| Step: 6
Training loss: 2.90828800201416
Validation loss: 2.7669220175794376

Epoch: 5| Step: 7
Training loss: 3.078235149383545
Validation loss: 2.765205639664845

Epoch: 5| Step: 8
Training loss: 3.1959640979766846
Validation loss: 2.762559260091474

Epoch: 5| Step: 9
Training loss: 3.2092642784118652
Validation loss: 2.7618567661572526

Epoch: 5| Step: 10
Training loss: 1.9243271350860596
Validation loss: 2.763371088171518

Epoch: 46| Step: 0
Training loss: 2.4522695541381836
Validation loss: 2.7598999123419485

Epoch: 5| Step: 1
Training loss: 2.9028449058532715
Validation loss: 2.7586177754145798

Epoch: 5| Step: 2
Training loss: 2.2221131324768066
Validation loss: 2.766363331066665

Epoch: 5| Step: 3
Training loss: 3.4887115955352783
Validation loss: 2.7689851432718258

Epoch: 5| Step: 4
Training loss: 3.132341146469116
Validation loss: 2.762409346078032

Epoch: 5| Step: 5
Training loss: 3.312533140182495
Validation loss: 2.768529094675536

Epoch: 5| Step: 6
Training loss: 2.555678129196167
Validation loss: 2.7664300023868518

Epoch: 5| Step: 7
Training loss: 3.3316376209259033
Validation loss: 2.762369950612386

Epoch: 5| Step: 8
Training loss: 3.1412601470947266
Validation loss: 2.7562728466526156

Epoch: 5| Step: 9
Training loss: 2.6443862915039062
Validation loss: 2.754707239007437

Epoch: 5| Step: 10
Training loss: 2.6530988216400146
Validation loss: 2.7509705161535614

Epoch: 47| Step: 0
Training loss: 4.043868064880371
Validation loss: 2.7516650410108667

Epoch: 5| Step: 1
Training loss: 2.978849172592163
Validation loss: 2.7552439115380727

Epoch: 5| Step: 2
Training loss: 2.8708655834198
Validation loss: 2.7560009033449235

Epoch: 5| Step: 3
Training loss: 2.899695873260498
Validation loss: 2.759555524395358

Epoch: 5| Step: 4
Training loss: 2.9469707012176514
Validation loss: 2.7588465854685795

Epoch: 5| Step: 5
Training loss: 1.5106772184371948
Validation loss: 2.7589482184379333

Epoch: 5| Step: 6
Training loss: 2.840095043182373
Validation loss: 2.7592228228046047

Epoch: 5| Step: 7
Training loss: 2.8938896656036377
Validation loss: 2.7569024357744443

Epoch: 5| Step: 8
Training loss: 3.0922210216522217
Validation loss: 2.7546197675889537

Epoch: 5| Step: 9
Training loss: 2.2533955574035645
Validation loss: 2.7533664575187107

Epoch: 5| Step: 10
Training loss: 3.6123504638671875
Validation loss: 2.752882952331215

Epoch: 48| Step: 0
Training loss: 2.494189739227295
Validation loss: 2.749003582103278

Epoch: 5| Step: 1
Training loss: 2.270427942276001
Validation loss: 2.744903169652467

Epoch: 5| Step: 2
Training loss: 3.681896209716797
Validation loss: 2.746517624906314

Epoch: 5| Step: 3
Training loss: 2.5943965911865234
Validation loss: 2.743862303354407

Epoch: 5| Step: 4
Training loss: 2.87959361076355
Validation loss: 2.741624411716256

Epoch: 5| Step: 5
Training loss: 2.855912923812866
Validation loss: 2.7407852757361626

Epoch: 5| Step: 6
Training loss: 2.453524112701416
Validation loss: 2.7407883521049254

Epoch: 5| Step: 7
Training loss: 2.8537027835845947
Validation loss: 2.738436947586716

Epoch: 5| Step: 8
Training loss: 2.9114840030670166
Validation loss: 2.7385052916824177

Epoch: 5| Step: 9
Training loss: 3.47806978225708
Validation loss: 2.7428150305183987

Epoch: 5| Step: 10
Training loss: 3.366084575653076
Validation loss: 2.7399458141737085

Epoch: 49| Step: 0
Training loss: 3.069094181060791
Validation loss: 2.736636318186278

Epoch: 5| Step: 1
Training loss: 2.8375954627990723
Validation loss: 2.741171234397478

Epoch: 5| Step: 2
Training loss: 2.6907904148101807
Validation loss: 2.740759482947729

Epoch: 5| Step: 3
Training loss: 3.0261764526367188
Validation loss: 2.74283266580233

Epoch: 5| Step: 4
Training loss: 2.522657871246338
Validation loss: 2.7418945502209406

Epoch: 5| Step: 5
Training loss: 2.829055070877075
Validation loss: 2.7403876422553934

Epoch: 5| Step: 6
Training loss: 2.924652099609375
Validation loss: 2.740356824731314

Epoch: 5| Step: 7
Training loss: 2.849058151245117
Validation loss: 2.733283619726858

Epoch: 5| Step: 8
Training loss: 2.8572752475738525
Validation loss: 2.7318509906850834

Epoch: 5| Step: 9
Training loss: 3.537015914916992
Validation loss: 2.7313807036287043

Epoch: 5| Step: 10
Training loss: 2.5180044174194336
Validation loss: 2.7315370293073755

Epoch: 50| Step: 0
Training loss: 3.090977668762207
Validation loss: 2.732668266501478

Epoch: 5| Step: 1
Training loss: 2.7264437675476074
Validation loss: 2.730189464425528

Epoch: 5| Step: 2
Training loss: 2.9195973873138428
Validation loss: 2.729558201246364

Epoch: 5| Step: 3
Training loss: 3.354005813598633
Validation loss: 2.7291990839025027

Epoch: 5| Step: 4
Training loss: 2.6936373710632324
Validation loss: 2.7307119728416525

Epoch: 5| Step: 5
Training loss: 2.4116995334625244
Validation loss: 2.7284007610813266

Epoch: 5| Step: 6
Training loss: 2.087862968444824
Validation loss: 2.7297995987758843

Epoch: 5| Step: 7
Training loss: 3.1802024841308594
Validation loss: 2.728082646605789

Epoch: 5| Step: 8
Training loss: 3.0804409980773926
Validation loss: 2.728532229700396

Epoch: 5| Step: 9
Training loss: 3.023491621017456
Validation loss: 2.7248058114000546

Epoch: 5| Step: 10
Training loss: 3.131026029586792
Validation loss: 2.7232452336178032

Epoch: 51| Step: 0
Training loss: 2.6847832202911377
Validation loss: 2.725672314243932

Epoch: 5| Step: 1
Training loss: 2.166712999343872
Validation loss: 2.723304566516671

Epoch: 5| Step: 2
Training loss: 3.0214455127716064
Validation loss: 2.7379602821924354

Epoch: 5| Step: 3
Training loss: 3.109811782836914
Validation loss: 2.7360049498978483

Epoch: 5| Step: 4
Training loss: 2.525653600692749
Validation loss: 2.7417577620475524

Epoch: 5| Step: 5
Training loss: 3.254498243331909
Validation loss: 2.7260551170636247

Epoch: 5| Step: 6
Training loss: 2.632007122039795
Validation loss: 2.7276252879891345

Epoch: 5| Step: 7
Training loss: 3.032226085662842
Validation loss: 2.724698784530804

Epoch: 5| Step: 8
Training loss: 3.2571513652801514
Validation loss: 2.7264721034675516

Epoch: 5| Step: 9
Training loss: 3.2999420166015625
Validation loss: 2.7294776311484714

Epoch: 5| Step: 10
Training loss: 2.7201850414276123
Validation loss: 2.72308502915085

Epoch: 52| Step: 0
Training loss: 2.8850152492523193
Validation loss: 2.723506437834873

Epoch: 5| Step: 1
Training loss: 2.900611400604248
Validation loss: 2.726071426945348

Epoch: 5| Step: 2
Training loss: 3.212453842163086
Validation loss: 2.730195983763664

Epoch: 5| Step: 3
Training loss: 2.7462832927703857
Validation loss: 2.7252540767833753

Epoch: 5| Step: 4
Training loss: 3.0723204612731934
Validation loss: 2.7223665662991103

Epoch: 5| Step: 5
Training loss: 3.4231133460998535
Validation loss: 2.719063622977144

Epoch: 5| Step: 6
Training loss: 2.3929009437561035
Validation loss: 2.722615921369163

Epoch: 5| Step: 7
Training loss: 2.823748826980591
Validation loss: 2.719935868376045

Epoch: 5| Step: 8
Training loss: 3.115079164505005
Validation loss: 2.725312484207974

Epoch: 5| Step: 9
Training loss: 2.619434356689453
Validation loss: 2.71874814392418

Epoch: 5| Step: 10
Training loss: 2.37841796875
Validation loss: 2.719515792785152

Epoch: 53| Step: 0
Training loss: 2.881509780883789
Validation loss: 2.719789989532963

Epoch: 5| Step: 1
Training loss: 3.137310028076172
Validation loss: 2.716148535410563

Epoch: 5| Step: 2
Training loss: 3.037598133087158
Validation loss: 2.719278291989398

Epoch: 5| Step: 3
Training loss: 2.661184787750244
Validation loss: 2.7206236316311743

Epoch: 5| Step: 4
Training loss: 2.823866605758667
Validation loss: 2.71918313477629

Epoch: 5| Step: 5
Training loss: 2.8402912616729736
Validation loss: 2.721169061558221

Epoch: 5| Step: 6
Training loss: 2.4493050575256348
Validation loss: 2.7224284166930826

Epoch: 5| Step: 7
Training loss: 2.954177141189575
Validation loss: 2.7216650952575026

Epoch: 5| Step: 8
Training loss: 3.0976510047912598
Validation loss: 2.725097608822648

Epoch: 5| Step: 9
Training loss: 3.100341796875
Validation loss: 2.722241706745599

Epoch: 5| Step: 10
Training loss: 2.5474398136138916
Validation loss: 2.7194465360333844

Epoch: 54| Step: 0
Training loss: 3.626328229904175
Validation loss: 2.722491628380232

Epoch: 5| Step: 1
Training loss: 3.0615556240081787
Validation loss: 2.719632866562054

Epoch: 5| Step: 2
Training loss: 2.8223609924316406
Validation loss: 2.7172137870583484

Epoch: 5| Step: 3
Training loss: 2.9537758827209473
Validation loss: 2.7166831211377214

Epoch: 5| Step: 4
Training loss: 2.355741024017334
Validation loss: 2.718936397183326

Epoch: 5| Step: 5
Training loss: 2.727724075317383
Validation loss: 2.7187062719816804

Epoch: 5| Step: 6
Training loss: 2.80104398727417
Validation loss: 2.7201128082890667

Epoch: 5| Step: 7
Training loss: 2.830211639404297
Validation loss: 2.723218646100772

Epoch: 5| Step: 8
Training loss: 3.289133071899414
Validation loss: 2.721135316356536

Epoch: 5| Step: 9
Training loss: 2.6339797973632812
Validation loss: 2.7201885843789704

Epoch: 5| Step: 10
Training loss: 2.3661341667175293
Validation loss: 2.7136290227213213

Epoch: 55| Step: 0
Training loss: 3.1196842193603516
Validation loss: 2.7134193425537436

Epoch: 5| Step: 1
Training loss: 2.7001144886016846
Validation loss: 2.713929760840631

Epoch: 5| Step: 2
Training loss: 2.0590415000915527
Validation loss: 2.711762128337737

Epoch: 5| Step: 3
Training loss: 2.7116341590881348
Validation loss: 2.7152216383205947

Epoch: 5| Step: 4
Training loss: 3.294970989227295
Validation loss: 2.713097508235644

Epoch: 5| Step: 5
Training loss: 3.205595016479492
Validation loss: 2.7127193866237516

Epoch: 5| Step: 6
Training loss: 3.2109827995300293
Validation loss: 2.711645790325698

Epoch: 5| Step: 7
Training loss: 2.7731335163116455
Validation loss: 2.710560506389987

Epoch: 5| Step: 8
Training loss: 2.319965124130249
Validation loss: 2.7110744137917795

Epoch: 5| Step: 9
Training loss: 3.4130349159240723
Validation loss: 2.710677577603248

Epoch: 5| Step: 10
Training loss: 2.6669483184814453
Validation loss: 2.7104643365388275

Epoch: 56| Step: 0
Training loss: 3.3179614543914795
Validation loss: 2.7088644889093216

Epoch: 5| Step: 1
Training loss: 1.8926970958709717
Validation loss: 2.708720645596904

Epoch: 5| Step: 2
Training loss: 2.2312870025634766
Validation loss: 2.715747615342499

Epoch: 5| Step: 3
Training loss: 2.973999500274658
Validation loss: 2.7141420123397664

Epoch: 5| Step: 4
Training loss: 2.9900779724121094
Validation loss: 2.710906587621217

Epoch: 5| Step: 5
Training loss: 3.3196864128112793
Validation loss: 2.7118600312099663

Epoch: 5| Step: 6
Training loss: 3.996307373046875
Validation loss: 2.705675219976774

Epoch: 5| Step: 7
Training loss: 2.7426059246063232
Validation loss: 2.7086839727176133

Epoch: 5| Step: 8
Training loss: 1.8196277618408203
Validation loss: 2.707058019535516

Epoch: 5| Step: 9
Training loss: 2.7890372276306152
Validation loss: 2.7073640849000666

Epoch: 5| Step: 10
Training loss: 3.509803533554077
Validation loss: 2.706957999096122

Epoch: 57| Step: 0
Training loss: 3.2998015880584717
Validation loss: 2.707977830722768

Epoch: 5| Step: 1
Training loss: 2.5411906242370605
Validation loss: 2.707625999245592

Epoch: 5| Step: 2
Training loss: 3.067857265472412
Validation loss: 2.7059917142314296

Epoch: 5| Step: 3
Training loss: 2.575369358062744
Validation loss: 2.708908175909391

Epoch: 5| Step: 4
Training loss: 2.422311305999756
Validation loss: 2.7069275045907624

Epoch: 5| Step: 5
Training loss: 2.7001700401306152
Validation loss: 2.707074224307973

Epoch: 5| Step: 6
Training loss: 2.038097620010376
Validation loss: 2.7058021868428876

Epoch: 5| Step: 7
Training loss: 3.1731293201446533
Validation loss: 2.706164154955136

Epoch: 5| Step: 8
Training loss: 2.5862529277801514
Validation loss: 2.7065071162357124

Epoch: 5| Step: 9
Training loss: 3.3551278114318848
Validation loss: 2.7016561364614837

Epoch: 5| Step: 10
Training loss: 3.8550333976745605
Validation loss: 2.7035965304220877

Epoch: 58| Step: 0
Training loss: 3.3268589973449707
Validation loss: 2.701723419209962

Epoch: 5| Step: 1
Training loss: 2.776150703430176
Validation loss: 2.7016839519623788

Epoch: 5| Step: 2
Training loss: 1.8546371459960938
Validation loss: 2.7037378639303227

Epoch: 5| Step: 3
Training loss: 3.0935490131378174
Validation loss: 2.703982009682604

Epoch: 5| Step: 4
Training loss: 3.1171817779541016
Validation loss: 2.70465878517397

Epoch: 5| Step: 5
Training loss: 2.333221197128296
Validation loss: 2.706242146030549

Epoch: 5| Step: 6
Training loss: 2.595828056335449
Validation loss: 2.7016141030096237

Epoch: 5| Step: 7
Training loss: 3.460660934448242
Validation loss: 2.70096278959705

Epoch: 5| Step: 8
Training loss: 2.9241318702697754
Validation loss: 2.7023913296320106

Epoch: 5| Step: 9
Training loss: 3.108520984649658
Validation loss: 2.702084236247565

Epoch: 5| Step: 10
Training loss: 2.8362061977386475
Validation loss: 2.700855988328175

Epoch: 59| Step: 0
Training loss: 2.9062163829803467
Validation loss: 2.699821082494592

Epoch: 5| Step: 1
Training loss: 2.3092334270477295
Validation loss: 2.7020611096453924

Epoch: 5| Step: 2
Training loss: 1.9601646661758423
Validation loss: 2.704816372163834

Epoch: 5| Step: 3
Training loss: 2.8889260292053223
Validation loss: 2.7075408402309624

Epoch: 5| Step: 4
Training loss: 3.5790207386016846
Validation loss: 2.706743678738994

Epoch: 5| Step: 5
Training loss: 3.444124937057495
Validation loss: 2.7045532452162875

Epoch: 5| Step: 6
Training loss: 3.3035647869110107
Validation loss: 2.6990672208929576

Epoch: 5| Step: 7
Training loss: 2.845207691192627
Validation loss: 2.6979290182872484

Epoch: 5| Step: 8
Training loss: 2.2859973907470703
Validation loss: 2.696866271316364

Epoch: 5| Step: 9
Training loss: 2.8259871006011963
Validation loss: 2.700960038810648

Epoch: 5| Step: 10
Training loss: 3.09739089012146
Validation loss: 2.70089413017355

Epoch: 60| Step: 0
Training loss: 3.3637471199035645
Validation loss: 2.7047358994842856

Epoch: 5| Step: 1
Training loss: 2.7286758422851562
Validation loss: 2.7048613050932526

Epoch: 5| Step: 2
Training loss: 3.334942579269409
Validation loss: 2.7071516129278366

Epoch: 5| Step: 3
Training loss: 2.4187378883361816
Validation loss: 2.711008774336948

Epoch: 5| Step: 4
Training loss: 3.001392364501953
Validation loss: 2.7132182275095293

Epoch: 5| Step: 5
Training loss: 2.8281145095825195
Validation loss: 2.7165229730708624

Epoch: 5| Step: 6
Training loss: 2.2491044998168945
Validation loss: 2.7046984831492105

Epoch: 5| Step: 7
Training loss: 2.65289568901062
Validation loss: 2.7068803951304448

Epoch: 5| Step: 8
Training loss: 3.143613815307617
Validation loss: 2.6992640751664356

Epoch: 5| Step: 9
Training loss: 2.9724037647247314
Validation loss: 2.697040157933389

Epoch: 5| Step: 10
Training loss: 2.81601881980896
Validation loss: 2.697239068246657

Epoch: 61| Step: 0
Training loss: 2.6224350929260254
Validation loss: 2.696013378840621

Epoch: 5| Step: 1
Training loss: 3.2999279499053955
Validation loss: 2.69588577875527

Epoch: 5| Step: 2
Training loss: 2.444844961166382
Validation loss: 2.6949554489504908

Epoch: 5| Step: 3
Training loss: 3.421428680419922
Validation loss: 2.696843577969459

Epoch: 5| Step: 4
Training loss: 2.0671982765197754
Validation loss: 2.697405038341399

Epoch: 5| Step: 5
Training loss: 3.1709861755371094
Validation loss: 2.695419019268405

Epoch: 5| Step: 6
Training loss: 3.0319182872772217
Validation loss: 2.69614423987686

Epoch: 5| Step: 7
Training loss: 2.771338701248169
Validation loss: 2.6940055662585842

Epoch: 5| Step: 8
Training loss: 3.075653076171875
Validation loss: 2.6924234205676663

Epoch: 5| Step: 9
Training loss: 2.982827663421631
Validation loss: 2.695584576617005

Epoch: 5| Step: 10
Training loss: 2.4647910594940186
Validation loss: 2.69581921638981

Epoch: 62| Step: 0
Training loss: 3.1106960773468018
Validation loss: 2.693063264252037

Epoch: 5| Step: 1
Training loss: 2.9905014038085938
Validation loss: 2.694483208399947

Epoch: 5| Step: 2
Training loss: 1.6257126331329346
Validation loss: 2.6942540368726178

Epoch: 5| Step: 3
Training loss: 3.006777286529541
Validation loss: 2.6968597442873063

Epoch: 5| Step: 4
Training loss: 2.764676570892334
Validation loss: 2.6947234010183685

Epoch: 5| Step: 5
Training loss: 2.8143343925476074
Validation loss: 2.6917770626724407

Epoch: 5| Step: 6
Training loss: 2.574737548828125
Validation loss: 2.693542839378439

Epoch: 5| Step: 7
Training loss: 3.4963479042053223
Validation loss: 2.692172383749357

Epoch: 5| Step: 8
Training loss: 3.4266746044158936
Validation loss: 2.691842948236773

Epoch: 5| Step: 9
Training loss: 2.0203583240509033
Validation loss: 2.6938386065985567

Epoch: 5| Step: 10
Training loss: 3.701857566833496
Validation loss: 2.6924152246085544

Epoch: 63| Step: 0
Training loss: 3.480304002761841
Validation loss: 2.692108110714984

Epoch: 5| Step: 1
Training loss: 3.212256908416748
Validation loss: 2.6909702926553707

Epoch: 5| Step: 2
Training loss: 3.21187162399292
Validation loss: 2.6925044444299515

Epoch: 5| Step: 3
Training loss: 2.8765439987182617
Validation loss: 2.6888941641776793

Epoch: 5| Step: 4
Training loss: 2.494171619415283
Validation loss: 2.6902685216678086

Epoch: 5| Step: 5
Training loss: 2.85255765914917
Validation loss: 2.6887345980572444

Epoch: 5| Step: 6
Training loss: 2.9459900856018066
Validation loss: 2.686893370843703

Epoch: 5| Step: 7
Training loss: 2.2352421283721924
Validation loss: 2.6910891020169823

Epoch: 5| Step: 8
Training loss: 3.008981227874756
Validation loss: 2.68844630641322

Epoch: 5| Step: 9
Training loss: 1.7986948490142822
Validation loss: 2.6883022810823176

Epoch: 5| Step: 10
Training loss: 3.303954601287842
Validation loss: 2.6852119045872844

Epoch: 64| Step: 0
Training loss: 2.85951566696167
Validation loss: 2.688616265532791

Epoch: 5| Step: 1
Training loss: 2.419365406036377
Validation loss: 2.6901073199446484

Epoch: 5| Step: 2
Training loss: 2.48248028755188
Validation loss: 2.6914511598566526

Epoch: 5| Step: 3
Training loss: 3.2668166160583496
Validation loss: 2.6864022260071128

Epoch: 5| Step: 4
Training loss: 2.9195058345794678
Validation loss: 2.6903284185676166

Epoch: 5| Step: 5
Training loss: 2.931260824203491
Validation loss: 2.6892400608267835

Epoch: 5| Step: 6
Training loss: 3.300340175628662
Validation loss: 2.689795147988104

Epoch: 5| Step: 7
Training loss: 3.254809617996216
Validation loss: 2.690946291851741

Epoch: 5| Step: 8
Training loss: 2.922701120376587
Validation loss: 2.6886545586329635

Epoch: 5| Step: 9
Training loss: 2.554891347885132
Validation loss: 2.6963994169747956

Epoch: 5| Step: 10
Training loss: 2.3443572521209717
Validation loss: 2.6933258118168

Epoch: 65| Step: 0
Training loss: 3.1060855388641357
Validation loss: 2.6969163545998196

Epoch: 5| Step: 1
Training loss: 2.69092059135437
Validation loss: 2.6978885563470985

Epoch: 5| Step: 2
Training loss: 2.966094732284546
Validation loss: 2.6947072987915366

Epoch: 5| Step: 3
Training loss: 2.693053960800171
Validation loss: 2.696280138466948

Epoch: 5| Step: 4
Training loss: 2.5395078659057617
Validation loss: 2.696129898871145

Epoch: 5| Step: 5
Training loss: 2.9848015308380127
Validation loss: 2.6902100886068037

Epoch: 5| Step: 6
Training loss: 2.540226697921753
Validation loss: 2.6897970502094557

Epoch: 5| Step: 7
Training loss: 3.4205334186553955
Validation loss: 2.6900849419255413

Epoch: 5| Step: 8
Training loss: 2.7067458629608154
Validation loss: 2.685716796946782

Epoch: 5| Step: 9
Training loss: 2.7838802337646484
Validation loss: 2.6886424018490698

Epoch: 5| Step: 10
Training loss: 2.876049757003784
Validation loss: 2.6895892363722607

Epoch: 66| Step: 0
Training loss: 2.7509610652923584
Validation loss: 2.688451861822477

Epoch: 5| Step: 1
Training loss: 2.4285826683044434
Validation loss: 2.691126726006949

Epoch: 5| Step: 2
Training loss: 3.463528871536255
Validation loss: 2.688648739168721

Epoch: 5| Step: 3
Training loss: 2.658562660217285
Validation loss: 2.689965171198691

Epoch: 5| Step: 4
Training loss: 3.0651378631591797
Validation loss: 2.6828832472524335

Epoch: 5| Step: 5
Training loss: 2.0988383293151855
Validation loss: 2.6869412391416487

Epoch: 5| Step: 6
Training loss: 3.045818567276001
Validation loss: 2.6822823683420816

Epoch: 5| Step: 7
Training loss: 2.7121715545654297
Validation loss: 2.6863769100558375

Epoch: 5| Step: 8
Training loss: 3.374223232269287
Validation loss: 2.693465368722075

Epoch: 5| Step: 9
Training loss: 2.597484588623047
Validation loss: 2.6859531274405857

Epoch: 5| Step: 10
Training loss: 3.127096652984619
Validation loss: 2.687480060003137

Epoch: 67| Step: 0
Training loss: 2.5224709510803223
Validation loss: 2.682688815619356

Epoch: 5| Step: 1
Training loss: 2.5655760765075684
Validation loss: 2.685033275235084

Epoch: 5| Step: 2
Training loss: 1.8950822353363037
Validation loss: 2.6836836389316026

Epoch: 5| Step: 3
Training loss: 2.8356573581695557
Validation loss: 2.682679307076239

Epoch: 5| Step: 4
Training loss: 3.668945789337158
Validation loss: 2.6810154222672984

Epoch: 5| Step: 5
Training loss: 3.0299694538116455
Validation loss: 2.6838869228157947

Epoch: 5| Step: 6
Training loss: 2.865656614303589
Validation loss: 2.6833185329232165

Epoch: 5| Step: 7
Training loss: 2.9822442531585693
Validation loss: 2.6823884261551725

Epoch: 5| Step: 8
Training loss: 3.146152973175049
Validation loss: 2.688378967264647

Epoch: 5| Step: 9
Training loss: 2.7464795112609863
Validation loss: 2.6811937875645135

Epoch: 5| Step: 10
Training loss: 3.013272523880005
Validation loss: 2.6837575948366554

Epoch: 68| Step: 0
Training loss: 2.6797335147857666
Validation loss: 2.6843619346618652

Epoch: 5| Step: 1
Training loss: 2.167776107788086
Validation loss: 2.6850128096918904

Epoch: 5| Step: 2
Training loss: 3.1069324016571045
Validation loss: 2.683156305743802

Epoch: 5| Step: 3
Training loss: 2.124497652053833
Validation loss: 2.6848246974329792

Epoch: 5| Step: 4
Training loss: 2.5321035385131836
Validation loss: 2.6958120228141866

Epoch: 5| Step: 5
Training loss: 3.5723013877868652
Validation loss: 2.70242827938449

Epoch: 5| Step: 6
Training loss: 2.8133161067962646
Validation loss: 2.703109477155952

Epoch: 5| Step: 7
Training loss: 3.2042148113250732
Validation loss: 2.7216653772579726

Epoch: 5| Step: 8
Training loss: 3.2434723377227783
Validation loss: 2.7180486007403304

Epoch: 5| Step: 9
Training loss: 2.368454933166504
Validation loss: 2.72435700637038

Epoch: 5| Step: 10
Training loss: 3.6836659908294678
Validation loss: 2.715704943544121

Epoch: 69| Step: 0
Training loss: 2.0138728618621826
Validation loss: 2.7129883843083538

Epoch: 5| Step: 1
Training loss: 2.5007009506225586
Validation loss: 2.7061936188769597

Epoch: 5| Step: 2
Training loss: 4.047605991363525
Validation loss: 2.6843277151866625

Epoch: 5| Step: 3
Training loss: 2.353867292404175
Validation loss: 2.683582111071515

Epoch: 5| Step: 4
Training loss: 2.7270965576171875
Validation loss: 2.682078505075106

Epoch: 5| Step: 5
Training loss: 2.982973098754883
Validation loss: 2.681967653254027

Epoch: 5| Step: 6
Training loss: 2.9228696823120117
Validation loss: 2.6846487881034933

Epoch: 5| Step: 7
Training loss: 3.172067880630493
Validation loss: 2.6848095129894953

Epoch: 5| Step: 8
Training loss: 2.7014501094818115
Validation loss: 2.685437866436538

Epoch: 5| Step: 9
Training loss: 2.7067389488220215
Validation loss: 2.6862225071076424

Epoch: 5| Step: 10
Training loss: 3.2247233390808105
Validation loss: 2.689100524430634

Epoch: 70| Step: 0
Training loss: 2.2460687160491943
Validation loss: 2.682796314198484

Epoch: 5| Step: 1
Training loss: 2.5629239082336426
Validation loss: 2.683795154735606

Epoch: 5| Step: 2
Training loss: 3.298886775970459
Validation loss: 2.6805087468957387

Epoch: 5| Step: 3
Training loss: 3.3715693950653076
Validation loss: 2.676129356507332

Epoch: 5| Step: 4
Training loss: 3.0622293949127197
Validation loss: 2.676819593675675

Epoch: 5| Step: 5
Training loss: 1.9436357021331787
Validation loss: 2.6752740029365785

Epoch: 5| Step: 6
Training loss: 2.780210494995117
Validation loss: 2.6764397339154313

Epoch: 5| Step: 7
Training loss: 3.009880542755127
Validation loss: 2.6810616472715973

Epoch: 5| Step: 8
Training loss: 2.8472468852996826
Validation loss: 2.680175919686594

Epoch: 5| Step: 9
Training loss: 3.0414538383483887
Validation loss: 2.674420490059801

Epoch: 5| Step: 10
Training loss: 2.9781198501586914
Validation loss: 2.6765946162644254

Epoch: 71| Step: 0
Training loss: 3.6395676136016846
Validation loss: 2.6751004137018675

Epoch: 5| Step: 1
Training loss: 2.4360134601593018
Validation loss: 2.681149823691255

Epoch: 5| Step: 2
Training loss: 2.239346742630005
Validation loss: 2.6772439941283195

Epoch: 5| Step: 3
Training loss: 3.2009754180908203
Validation loss: 2.678708140568067

Epoch: 5| Step: 4
Training loss: 2.7448925971984863
Validation loss: 2.683370894001376

Epoch: 5| Step: 5
Training loss: 2.5289034843444824
Validation loss: 2.682196742744856

Epoch: 5| Step: 6
Training loss: 3.119689464569092
Validation loss: 2.679103607772499

Epoch: 5| Step: 7
Training loss: 2.984546661376953
Validation loss: 2.6815215003105903

Epoch: 5| Step: 8
Training loss: 3.0299744606018066
Validation loss: 2.681525650844779

Epoch: 5| Step: 9
Training loss: 2.241513252258301
Validation loss: 2.6793176692019225

Epoch: 5| Step: 10
Training loss: 2.923882246017456
Validation loss: 2.6789083301380114

Epoch: 72| Step: 0
Training loss: 3.249769687652588
Validation loss: 2.681782650691207

Epoch: 5| Step: 1
Training loss: 2.653341054916382
Validation loss: 2.6826768152175413

Epoch: 5| Step: 2
Training loss: 2.875361919403076
Validation loss: 2.6813924389500774

Epoch: 5| Step: 3
Training loss: 2.4181551933288574
Validation loss: 2.6837759581945275

Epoch: 5| Step: 4
Training loss: 3.3600757122039795
Validation loss: 2.681544324403168

Epoch: 5| Step: 5
Training loss: 2.170524835586548
Validation loss: 2.6715426880826234

Epoch: 5| Step: 6
Training loss: 2.88137149810791
Validation loss: 2.673466420942737

Epoch: 5| Step: 7
Training loss: 2.9902701377868652
Validation loss: 2.674374157382596

Epoch: 5| Step: 8
Training loss: 2.8687119483947754
Validation loss: 2.6721394369679112

Epoch: 5| Step: 9
Training loss: 2.9365932941436768
Validation loss: 2.6757350493502874

Epoch: 5| Step: 10
Training loss: 2.7331619262695312
Validation loss: 2.678079723030008

Epoch: 73| Step: 0
Training loss: 3.3024566173553467
Validation loss: 2.676634004039149

Epoch: 5| Step: 1
Training loss: 2.7753376960754395
Validation loss: 2.677728817027102

Epoch: 5| Step: 2
Training loss: 2.9407360553741455
Validation loss: 2.6759993132724555

Epoch: 5| Step: 3
Training loss: 2.769026279449463
Validation loss: 2.675300851944954

Epoch: 5| Step: 4
Training loss: 3.250108242034912
Validation loss: 2.67653973128206

Epoch: 5| Step: 5
Training loss: 2.60398268699646
Validation loss: 2.672348245497673

Epoch: 5| Step: 6
Training loss: 2.7902133464813232
Validation loss: 2.672844453524518

Epoch: 5| Step: 7
Training loss: 2.7176311016082764
Validation loss: 2.66778984121097

Epoch: 5| Step: 8
Training loss: 2.682039260864258
Validation loss: 2.67211079084745

Epoch: 5| Step: 9
Training loss: 2.3582096099853516
Validation loss: 2.670163280220442

Epoch: 5| Step: 10
Training loss: 2.9841198921203613
Validation loss: 2.66622345934632

Epoch: 74| Step: 0
Training loss: 3.101433277130127
Validation loss: 2.6684712492009646

Epoch: 5| Step: 1
Training loss: 2.734717845916748
Validation loss: 2.665698112980012

Epoch: 5| Step: 2
Training loss: 2.8546595573425293
Validation loss: 2.66574336636451

Epoch: 5| Step: 3
Training loss: 2.8140599727630615
Validation loss: 2.670363862027404

Epoch: 5| Step: 4
Training loss: 2.775395154953003
Validation loss: 2.6707134939009145

Epoch: 5| Step: 5
Training loss: 2.7918028831481934
Validation loss: 2.6727655010838665

Epoch: 5| Step: 6
Training loss: 2.9826412200927734
Validation loss: 2.672597236530755

Epoch: 5| Step: 7
Training loss: 2.170567035675049
Validation loss: 2.670709781749274

Epoch: 5| Step: 8
Training loss: 2.856635093688965
Validation loss: 2.6686032510572866

Epoch: 5| Step: 9
Training loss: 3.014038562774658
Validation loss: 2.6677777690272175

Epoch: 5| Step: 10
Training loss: 2.958955764770508
Validation loss: 2.6673835400612123

Epoch: 75| Step: 0
Training loss: 2.6893022060394287
Validation loss: 2.6685120623598815

Epoch: 5| Step: 1
Training loss: 2.3322091102600098
Validation loss: 2.6674348820922194

Epoch: 5| Step: 2
Training loss: 2.929504156112671
Validation loss: 2.670436482275686

Epoch: 5| Step: 3
Training loss: 2.864319324493408
Validation loss: 2.66844093415045

Epoch: 5| Step: 4
Training loss: 2.854843854904175
Validation loss: 2.669634001229399

Epoch: 5| Step: 5
Training loss: 3.174079418182373
Validation loss: 2.6700908624997703

Epoch: 5| Step: 6
Training loss: 2.208000659942627
Validation loss: 2.6642037873627036

Epoch: 5| Step: 7
Training loss: 3.47119140625
Validation loss: 2.6672883777208227

Epoch: 5| Step: 8
Training loss: 3.251911163330078
Validation loss: 2.667090813318888

Epoch: 5| Step: 9
Training loss: 2.0987329483032227
Validation loss: 2.6661435147767425

Epoch: 5| Step: 10
Training loss: 3.200556755065918
Validation loss: 2.665395321384553

Epoch: 76| Step: 0
Training loss: 2.9165754318237305
Validation loss: 2.6701416354025564

Epoch: 5| Step: 1
Training loss: 2.312819004058838
Validation loss: 2.6772784904767106

Epoch: 5| Step: 2
Training loss: 2.6446151733398438
Validation loss: 2.6805350242122525

Epoch: 5| Step: 3
Training loss: 3.059154510498047
Validation loss: 2.689515421467443

Epoch: 5| Step: 4
Training loss: 2.1896421909332275
Validation loss: 2.687209462606779

Epoch: 5| Step: 5
Training loss: 3.057459592819214
Validation loss: 2.686724329507479

Epoch: 5| Step: 6
Training loss: 3.168034791946411
Validation loss: 2.6814640132329797

Epoch: 5| Step: 7
Training loss: 3.240142822265625
Validation loss: 2.6717699804613666

Epoch: 5| Step: 8
Training loss: 2.1263842582702637
Validation loss: 2.667191243940784

Epoch: 5| Step: 9
Training loss: 3.325568675994873
Validation loss: 2.6694916089375815

Epoch: 5| Step: 10
Training loss: 3.0235283374786377
Validation loss: 2.664344902961485

Epoch: 77| Step: 0
Training loss: 2.8892340660095215
Validation loss: 2.662258945485597

Epoch: 5| Step: 1
Training loss: 2.8288540840148926
Validation loss: 2.666902301132038

Epoch: 5| Step: 2
Training loss: 2.6521353721618652
Validation loss: 2.661660612270396

Epoch: 5| Step: 3
Training loss: 2.4321484565734863
Validation loss: 2.662363936824183

Epoch: 5| Step: 4
Training loss: 2.322502374649048
Validation loss: 2.6628886294621292

Epoch: 5| Step: 5
Training loss: 3.5182571411132812
Validation loss: 2.6663666053484847

Epoch: 5| Step: 6
Training loss: 2.4031119346618652
Validation loss: 2.66236755412112

Epoch: 5| Step: 7
Training loss: 2.5386996269226074
Validation loss: 2.663485598820512

Epoch: 5| Step: 8
Training loss: 2.875336170196533
Validation loss: 2.6609897023888043

Epoch: 5| Step: 9
Training loss: 3.631732225418091
Validation loss: 2.662001379074589

Epoch: 5| Step: 10
Training loss: 2.8819360733032227
Validation loss: 2.659348293017316

Epoch: 78| Step: 0
Training loss: 2.6262547969818115
Validation loss: 2.6597732113253687

Epoch: 5| Step: 1
Training loss: 2.5791664123535156
Validation loss: 2.665926697433636

Epoch: 5| Step: 2
Training loss: 3.310460329055786
Validation loss: 2.6710245032464304

Epoch: 5| Step: 3
Training loss: 2.8730075359344482
Validation loss: 2.6737887500434794

Epoch: 5| Step: 4
Training loss: 1.9863970279693604
Validation loss: 2.6719741513652187

Epoch: 5| Step: 5
Training loss: 2.56322979927063
Validation loss: 2.6787724905116583

Epoch: 5| Step: 6
Training loss: 2.905071973800659
Validation loss: 2.6663744987980014

Epoch: 5| Step: 7
Training loss: 3.127986192703247
Validation loss: 2.669541997294272

Epoch: 5| Step: 8
Training loss: 2.8576157093048096
Validation loss: 2.6666457422317995

Epoch: 5| Step: 9
Training loss: 3.4063572883605957
Validation loss: 2.661812223413939

Epoch: 5| Step: 10
Training loss: 2.7150886058807373
Validation loss: 2.6593483365992063

Epoch: 79| Step: 0
Training loss: 3.5101208686828613
Validation loss: 2.6534013389259257

Epoch: 5| Step: 1
Training loss: 2.6258952617645264
Validation loss: 2.658323990401401

Epoch: 5| Step: 2
Training loss: 2.8690433502197266
Validation loss: 2.659340259849384

Epoch: 5| Step: 3
Training loss: 2.05745267868042
Validation loss: 2.6560952535239597

Epoch: 5| Step: 4
Training loss: 3.1802244186401367
Validation loss: 2.6610078324553785

Epoch: 5| Step: 5
Training loss: 2.965057373046875
Validation loss: 2.6575764097193235

Epoch: 5| Step: 6
Training loss: 3.662458896636963
Validation loss: 2.65829369073273

Epoch: 5| Step: 7
Training loss: 3.0190269947052
Validation loss: 2.6587090774249007

Epoch: 5| Step: 8
Training loss: 2.4311976432800293
Validation loss: 2.652950397101782

Epoch: 5| Step: 9
Training loss: 2.1376399993896484
Validation loss: 2.657016646477484

Epoch: 5| Step: 10
Training loss: 2.3831470012664795
Validation loss: 2.6566082226332797

Epoch: 80| Step: 0
Training loss: 2.9465975761413574
Validation loss: 2.661062950729042

Epoch: 5| Step: 1
Training loss: 2.6646933555603027
Validation loss: 2.6561988861330095

Epoch: 5| Step: 2
Training loss: 3.1531054973602295
Validation loss: 2.6618733764976583

Epoch: 5| Step: 3
Training loss: 2.5992016792297363
Validation loss: 2.664250248221941

Epoch: 5| Step: 4
Training loss: 3.0679690837860107
Validation loss: 2.667778471464752

Epoch: 5| Step: 5
Training loss: 2.454030990600586
Validation loss: 2.6665506183460193

Epoch: 5| Step: 6
Training loss: 2.5198326110839844
Validation loss: 2.6633467289709274

Epoch: 5| Step: 7
Training loss: 2.923243522644043
Validation loss: 2.6720534191336682

Epoch: 5| Step: 8
Training loss: 3.2715156078338623
Validation loss: 2.6654117850847143

Epoch: 5| Step: 9
Training loss: 2.957050085067749
Validation loss: 2.66519441399523

Epoch: 5| Step: 10
Training loss: 2.193791151046753
Validation loss: 2.666339159011841

Epoch: 81| Step: 0
Training loss: 2.645569324493408
Validation loss: 2.662856909536546

Epoch: 5| Step: 1
Training loss: 2.949836015701294
Validation loss: 2.658991306058822

Epoch: 5| Step: 2
Training loss: 2.625932455062866
Validation loss: 2.6661720455333753

Epoch: 5| Step: 3
Training loss: 3.262147903442383
Validation loss: 2.6603645406743532

Epoch: 5| Step: 4
Training loss: 2.724400281906128
Validation loss: 2.654763380686442

Epoch: 5| Step: 5
Training loss: 3.3103270530700684
Validation loss: 2.659392503000075

Epoch: 5| Step: 6
Training loss: 2.1450066566467285
Validation loss: 2.670388578086771

Epoch: 5| Step: 7
Training loss: 2.47712779045105
Validation loss: 2.6971004778339016

Epoch: 5| Step: 8
Training loss: 2.887765884399414
Validation loss: 2.708450855747346

Epoch: 5| Step: 9
Training loss: 3.139918088912964
Validation loss: 2.7115890569584344

Epoch: 5| Step: 10
Training loss: 3.008059501647949
Validation loss: 2.6834360655917915

Epoch: 82| Step: 0
Training loss: 3.1608951091766357
Validation loss: 2.652941357704901

Epoch: 5| Step: 1
Training loss: 3.4320950508117676
Validation loss: 2.6514315835891233

Epoch: 5| Step: 2
Training loss: 2.4224259853363037
Validation loss: 2.654498771954608

Epoch: 5| Step: 3
Training loss: 2.906588315963745
Validation loss: 2.654672989281275

Epoch: 5| Step: 4
Training loss: 3.0811355113983154
Validation loss: 2.6608757818898847

Epoch: 5| Step: 5
Training loss: 2.6880719661712646
Validation loss: 2.661433276309762

Epoch: 5| Step: 6
Training loss: 2.7410247325897217
Validation loss: 2.67476216952006

Epoch: 5| Step: 7
Training loss: 2.486468553543091
Validation loss: 2.6851159552092194

Epoch: 5| Step: 8
Training loss: 3.086453914642334
Validation loss: 2.6889502899621123

Epoch: 5| Step: 9
Training loss: 2.317263126373291
Validation loss: 2.6738136122303624

Epoch: 5| Step: 10
Training loss: 2.600064992904663
Validation loss: 2.6731628679460093

Epoch: 83| Step: 0
Training loss: 3.193861484527588
Validation loss: 2.664843031155166

Epoch: 5| Step: 1
Training loss: 2.6090660095214844
Validation loss: 2.66044875883287

Epoch: 5| Step: 2
Training loss: 2.4150004386901855
Validation loss: 2.6575326535009567

Epoch: 5| Step: 3
Training loss: 2.742227792739868
Validation loss: 2.6546498011517268

Epoch: 5| Step: 4
Training loss: 3.255725860595703
Validation loss: 2.6558584782385055

Epoch: 5| Step: 5
Training loss: 2.503664255142212
Validation loss: 2.652198137775544

Epoch: 5| Step: 6
Training loss: 2.3471720218658447
Validation loss: 2.649682862784273

Epoch: 5| Step: 7
Training loss: 2.8353970050811768
Validation loss: 2.650054483003514

Epoch: 5| Step: 8
Training loss: 2.8799688816070557
Validation loss: 2.651116837737381

Epoch: 5| Step: 9
Training loss: 2.4363319873809814
Validation loss: 2.6516322141052573

Epoch: 5| Step: 10
Training loss: 3.7401256561279297
Validation loss: 2.6503084398085073

Epoch: 84| Step: 0
Training loss: 3.005768299102783
Validation loss: 2.64973004915381

Epoch: 5| Step: 1
Training loss: 3.1635329723358154
Validation loss: 2.6569294288594234

Epoch: 5| Step: 2
Training loss: 2.867198944091797
Validation loss: 2.653153952731881

Epoch: 5| Step: 3
Training loss: 2.3962106704711914
Validation loss: 2.6516990507802656

Epoch: 5| Step: 4
Training loss: 3.1730871200561523
Validation loss: 2.652359095952844

Epoch: 5| Step: 5
Training loss: 3.4029412269592285
Validation loss: 2.6525047799592376

Epoch: 5| Step: 6
Training loss: 2.5701355934143066
Validation loss: 2.6505379061545096

Epoch: 5| Step: 7
Training loss: 2.9458494186401367
Validation loss: 2.6478734247146116

Epoch: 5| Step: 8
Training loss: 2.0481066703796387
Validation loss: 2.646111544742379

Epoch: 5| Step: 9
Training loss: 2.6943612098693848
Validation loss: 2.651197243762273

Epoch: 5| Step: 10
Training loss: 2.5253548622131348
Validation loss: 2.649444269877608

Epoch: 85| Step: 0
Training loss: 2.8476223945617676
Validation loss: 2.6458032361922728

Epoch: 5| Step: 1
Training loss: 2.7191526889801025
Validation loss: 2.643672594460108

Epoch: 5| Step: 2
Training loss: 2.2809090614318848
Validation loss: 2.6414130477495092

Epoch: 5| Step: 3
Training loss: 3.0300514698028564
Validation loss: 2.6400473810011342

Epoch: 5| Step: 4
Training loss: 3.440302610397339
Validation loss: 2.6433192581258793

Epoch: 5| Step: 5
Training loss: 3.1207382678985596
Validation loss: 2.6400748657923874

Epoch: 5| Step: 6
Training loss: 2.577083110809326
Validation loss: 2.6369216929199877

Epoch: 5| Step: 7
Training loss: 2.840348720550537
Validation loss: 2.635802097218011

Epoch: 5| Step: 8
Training loss: 2.195490837097168
Validation loss: 2.6374231051373225

Epoch: 5| Step: 9
Training loss: 2.9884419441223145
Validation loss: 2.633657519535352

Epoch: 5| Step: 10
Training loss: 2.73242449760437
Validation loss: 2.63488103753777

Epoch: 86| Step: 0
Training loss: 2.375218629837036
Validation loss: 2.6353641222882014

Epoch: 5| Step: 1
Training loss: 2.5722689628601074
Validation loss: 2.636501914711409

Epoch: 5| Step: 2
Training loss: 3.0414233207702637
Validation loss: 2.6379292241988646

Epoch: 5| Step: 3
Training loss: 2.742732524871826
Validation loss: 2.63938392618651

Epoch: 5| Step: 4
Training loss: 2.7237541675567627
Validation loss: 2.640045250615766

Epoch: 5| Step: 5
Training loss: 2.8416054248809814
Validation loss: 2.6379494154325096

Epoch: 5| Step: 6
Training loss: 2.7292535305023193
Validation loss: 2.639249322234943

Epoch: 5| Step: 7
Training loss: 2.685917377471924
Validation loss: 2.6385156313578286

Epoch: 5| Step: 8
Training loss: 3.587920665740967
Validation loss: 2.6421274421035603

Epoch: 5| Step: 9
Training loss: 3.2221157550811768
Validation loss: 2.641735179449922

Epoch: 5| Step: 10
Training loss: 2.0850586891174316
Validation loss: 2.6433270490297707

Epoch: 87| Step: 0
Training loss: 3.138662815093994
Validation loss: 2.6381651111828384

Epoch: 5| Step: 1
Training loss: 2.2572007179260254
Validation loss: 2.6330855533640873

Epoch: 5| Step: 2
Training loss: 2.8442251682281494
Validation loss: 2.6341424347251974

Epoch: 5| Step: 3
Training loss: 3.380932569503784
Validation loss: 2.631636522149527

Epoch: 5| Step: 4
Training loss: 2.822798013687134
Validation loss: 2.6355249087015786

Epoch: 5| Step: 5
Training loss: 2.8473243713378906
Validation loss: 2.6364934059881393

Epoch: 5| Step: 6
Training loss: 2.6697845458984375
Validation loss: 2.6297778570523827

Epoch: 5| Step: 7
Training loss: 3.2029547691345215
Validation loss: 2.632523854573568

Epoch: 5| Step: 8
Training loss: 2.3818717002868652
Validation loss: 2.631492340436546

Epoch: 5| Step: 9
Training loss: 2.528080940246582
Validation loss: 2.6330194293811755

Epoch: 5| Step: 10
Training loss: 2.598417282104492
Validation loss: 2.642577048270933

Epoch: 88| Step: 0
Training loss: 2.4161314964294434
Validation loss: 2.6408309449431715

Epoch: 5| Step: 1
Training loss: 2.659799337387085
Validation loss: 2.640777864763814

Epoch: 5| Step: 2
Training loss: 2.6298184394836426
Validation loss: 2.6455436650142876

Epoch: 5| Step: 3
Training loss: 3.1519649028778076
Validation loss: 2.6490677377229095

Epoch: 5| Step: 4
Training loss: 2.7804160118103027
Validation loss: 2.641582827414236

Epoch: 5| Step: 5
Training loss: 3.123487949371338
Validation loss: 2.641859416038759

Epoch: 5| Step: 6
Training loss: 2.528022289276123
Validation loss: 2.639232381697624

Epoch: 5| Step: 7
Training loss: 2.863457202911377
Validation loss: 2.633632080529326

Epoch: 5| Step: 8
Training loss: 3.2011284828186035
Validation loss: 2.6307720445817515

Epoch: 5| Step: 9
Training loss: 3.038695812225342
Validation loss: 2.6321243009259625

Epoch: 5| Step: 10
Training loss: 2.2196924686431885
Validation loss: 2.6278189177154214

Epoch: 89| Step: 0
Training loss: 3.2556190490722656
Validation loss: 2.6272091352811424

Epoch: 5| Step: 1
Training loss: 2.7356433868408203
Validation loss: 2.6263717271948375

Epoch: 5| Step: 2
Training loss: 3.314544200897217
Validation loss: 2.625937631053309

Epoch: 5| Step: 3
Training loss: 2.1067264080047607
Validation loss: 2.6254925445843766

Epoch: 5| Step: 4
Training loss: 3.298983335494995
Validation loss: 2.627811049902311

Epoch: 5| Step: 5
Training loss: 2.7860796451568604
Validation loss: 2.6244434977090485

Epoch: 5| Step: 6
Training loss: 2.1945948600769043
Validation loss: 2.634486195861652

Epoch: 5| Step: 7
Training loss: 2.738770008087158
Validation loss: 2.6286698977152505

Epoch: 5| Step: 8
Training loss: 2.5514957904815674
Validation loss: 2.6339250328720256

Epoch: 5| Step: 9
Training loss: 2.639907121658325
Validation loss: 2.637750164155037

Epoch: 5| Step: 10
Training loss: 3.0755789279937744
Validation loss: 2.6390790772694412

Epoch: 90| Step: 0
Training loss: 2.7032392024993896
Validation loss: 2.639035678678943

Epoch: 5| Step: 1
Training loss: 2.6901819705963135
Validation loss: 2.639051834742228

Epoch: 5| Step: 2
Training loss: 3.202857494354248
Validation loss: 2.640590529288015

Epoch: 5| Step: 3
Training loss: 3.13942289352417
Validation loss: 2.6361631706196773

Epoch: 5| Step: 4
Training loss: 3.245651960372925
Validation loss: 2.627500025174951

Epoch: 5| Step: 5
Training loss: 1.9397695064544678
Validation loss: 2.625589601455196

Epoch: 5| Step: 6
Training loss: 3.104050874710083
Validation loss: 2.627139240182856

Epoch: 5| Step: 7
Training loss: 2.3938992023468018
Validation loss: 2.623767993783438

Epoch: 5| Step: 8
Training loss: 3.0975663661956787
Validation loss: 2.6267051542958906

Epoch: 5| Step: 9
Training loss: 2.330735445022583
Validation loss: 2.6277964653507357

Epoch: 5| Step: 10
Training loss: 2.7865757942199707
Validation loss: 2.632594111145184

Epoch: 91| Step: 0
Training loss: 2.809847593307495
Validation loss: 2.635075612734723

Epoch: 5| Step: 1
Training loss: 3.4506912231445312
Validation loss: 2.6368221134267826

Epoch: 5| Step: 2
Training loss: 3.2047247886657715
Validation loss: 2.6344070972934848

Epoch: 5| Step: 3
Training loss: 2.518049955368042
Validation loss: 2.63694179186257

Epoch: 5| Step: 4
Training loss: 2.778735637664795
Validation loss: 2.631571562059464

Epoch: 5| Step: 5
Training loss: 2.5852210521698
Validation loss: 2.6319401187281453

Epoch: 5| Step: 6
Training loss: 2.6925673484802246
Validation loss: 2.626514883451564

Epoch: 5| Step: 7
Training loss: 3.1520256996154785
Validation loss: 2.6242046997111332

Epoch: 5| Step: 8
Training loss: 2.6520819664001465
Validation loss: 2.619513791094544

Epoch: 5| Step: 9
Training loss: 2.334467649459839
Validation loss: 2.6245390676682994

Epoch: 5| Step: 10
Training loss: 2.429880142211914
Validation loss: 2.619175649458362

Epoch: 92| Step: 0
Training loss: 3.0606117248535156
Validation loss: 2.6205584105624946

Epoch: 5| Step: 1
Training loss: 2.156492233276367
Validation loss: 2.6260497980220343

Epoch: 5| Step: 2
Training loss: 2.0917983055114746
Validation loss: 2.6268446881283998

Epoch: 5| Step: 3
Training loss: 2.827183485031128
Validation loss: 2.6284332634300314

Epoch: 5| Step: 4
Training loss: 3.1826682090759277
Validation loss: 2.6306756696393414

Epoch: 5| Step: 5
Training loss: 2.342535972595215
Validation loss: 2.6218985460137807

Epoch: 5| Step: 6
Training loss: 3.3509368896484375
Validation loss: 2.620664934958181

Epoch: 5| Step: 7
Training loss: 2.1290669441223145
Validation loss: 2.620853172835483

Epoch: 5| Step: 8
Training loss: 3.372565507888794
Validation loss: 2.621383341409827

Epoch: 5| Step: 9
Training loss: 3.7115108966827393
Validation loss: 2.619604472191103

Epoch: 5| Step: 10
Training loss: 2.354851722717285
Validation loss: 2.6206007337057464

Epoch: 93| Step: 0
Training loss: 2.51784610748291
Validation loss: 2.6174317893161567

Epoch: 5| Step: 1
Training loss: 2.8957715034484863
Validation loss: 2.6202683807701193

Epoch: 5| Step: 2
Training loss: 3.3136532306671143
Validation loss: 2.618381249007358

Epoch: 5| Step: 3
Training loss: 1.9563026428222656
Validation loss: 2.618670937835529

Epoch: 5| Step: 4
Training loss: 3.5527732372283936
Validation loss: 2.6242489789121892

Epoch: 5| Step: 5
Training loss: 2.449767589569092
Validation loss: 2.6287338733673096

Epoch: 5| Step: 6
Training loss: 2.346006393432617
Validation loss: 2.6287019355322725

Epoch: 5| Step: 7
Training loss: 3.678016185760498
Validation loss: 2.628525987748177

Epoch: 5| Step: 8
Training loss: 2.1653342247009277
Validation loss: 2.6296090079892065

Epoch: 5| Step: 9
Training loss: 2.907256603240967
Validation loss: 2.623124714820616

Epoch: 5| Step: 10
Training loss: 2.8243086338043213
Validation loss: 2.6218077187897055

Epoch: 94| Step: 0
Training loss: 3.5255532264709473
Validation loss: 2.6212721383699806

Epoch: 5| Step: 1
Training loss: 2.760561227798462
Validation loss: 2.614653600159512

Epoch: 5| Step: 2
Training loss: 2.5440075397491455
Validation loss: 2.6177184274119716

Epoch: 5| Step: 3
Training loss: 2.9753053188323975
Validation loss: 2.617103809951454

Epoch: 5| Step: 4
Training loss: 3.0380330085754395
Validation loss: 2.613453636887253

Epoch: 5| Step: 5
Training loss: 2.2623579502105713
Validation loss: 2.6155215335148636

Epoch: 5| Step: 6
Training loss: 2.013174057006836
Validation loss: 2.6182434276867936

Epoch: 5| Step: 7
Training loss: 3.0600552558898926
Validation loss: 2.6143276076163016

Epoch: 5| Step: 8
Training loss: 3.054722309112549
Validation loss: 2.615216278260754

Epoch: 5| Step: 9
Training loss: 2.5263302326202393
Validation loss: 2.61729553181638

Epoch: 5| Step: 10
Training loss: 2.839446544647217
Validation loss: 2.62275198454498

Epoch: 95| Step: 0
Training loss: 2.9125850200653076
Validation loss: 2.635221671032649

Epoch: 5| Step: 1
Training loss: 3.2254841327667236
Validation loss: 2.6520166986732074

Epoch: 5| Step: 2
Training loss: 3.0997111797332764
Validation loss: 2.6631799641475884

Epoch: 5| Step: 3
Training loss: 2.5221195220947266
Validation loss: 2.674163282558482

Epoch: 5| Step: 4
Training loss: 2.4371676445007324
Validation loss: 2.6696015609207975

Epoch: 5| Step: 5
Training loss: 2.8604445457458496
Validation loss: 2.654924233754476

Epoch: 5| Step: 6
Training loss: 2.3996944427490234
Validation loss: 2.635580252575618

Epoch: 5| Step: 7
Training loss: 1.9373624324798584
Validation loss: 2.638985703068395

Epoch: 5| Step: 8
Training loss: 2.5314579010009766
Validation loss: 2.6348768382944088

Epoch: 5| Step: 9
Training loss: 3.493042469024658
Validation loss: 2.6281650168921358

Epoch: 5| Step: 10
Training loss: 3.38777232170105
Validation loss: 2.6195846834490375

Epoch: 96| Step: 0
Training loss: 2.9167819023132324
Validation loss: 2.617211772549537

Epoch: 5| Step: 1
Training loss: 2.5049874782562256
Validation loss: 2.6138359910698346

Epoch: 5| Step: 2
Training loss: 2.161172389984131
Validation loss: 2.615638594473562

Epoch: 5| Step: 3
Training loss: 2.5642380714416504
Validation loss: 2.6176992770164245

Epoch: 5| Step: 4
Training loss: 2.928389310836792
Validation loss: 2.6195567500206733

Epoch: 5| Step: 5
Training loss: 3.06333327293396
Validation loss: 2.621071389926377

Epoch: 5| Step: 6
Training loss: 3.5437591075897217
Validation loss: 2.6220405076139714

Epoch: 5| Step: 7
Training loss: 2.682813882827759
Validation loss: 2.6165330384367254

Epoch: 5| Step: 8
Training loss: 2.892488479614258
Validation loss: 2.624583254578293

Epoch: 5| Step: 9
Training loss: 2.723905086517334
Validation loss: 2.6212439639593965

Epoch: 5| Step: 10
Training loss: 2.51958966255188
Validation loss: 2.614183904022299

Epoch: 97| Step: 0
Training loss: 2.7136528491973877
Validation loss: 2.615282427880072

Epoch: 5| Step: 1
Training loss: 3.580085039138794
Validation loss: 2.6145493548403502

Epoch: 5| Step: 2
Training loss: 2.3340096473693848
Validation loss: 2.615709609882806

Epoch: 5| Step: 3
Training loss: 2.6086790561676025
Validation loss: 2.618858955239737

Epoch: 5| Step: 4
Training loss: 2.5864548683166504
Validation loss: 2.6252894017004196

Epoch: 5| Step: 5
Training loss: 3.276104688644409
Validation loss: 2.6185675423632384

Epoch: 5| Step: 6
Training loss: 2.585059642791748
Validation loss: 2.6158514381736837

Epoch: 5| Step: 7
Training loss: 2.8838953971862793
Validation loss: 2.622900119391821

Epoch: 5| Step: 8
Training loss: 2.5232675075531006
Validation loss: 2.616509932343678

Epoch: 5| Step: 9
Training loss: 2.105412721633911
Validation loss: 2.6118050467583442

Epoch: 5| Step: 10
Training loss: 3.3895864486694336
Validation loss: 2.60985916660678

Epoch: 98| Step: 0
Training loss: 2.30965518951416
Validation loss: 2.6079105818143455

Epoch: 5| Step: 1
Training loss: 3.1711840629577637
Validation loss: 2.6065719512201126

Epoch: 5| Step: 2
Training loss: 3.083341121673584
Validation loss: 2.6043321983788603

Epoch: 5| Step: 3
Training loss: 2.846935749053955
Validation loss: 2.607459032407371

Epoch: 5| Step: 4
Training loss: 1.9967868328094482
Validation loss: 2.60767803653594

Epoch: 5| Step: 5
Training loss: 2.6476147174835205
Validation loss: 2.612015393472487

Epoch: 5| Step: 6
Training loss: 3.321241855621338
Validation loss: 2.6130602577681183

Epoch: 5| Step: 7
Training loss: 2.172363042831421
Validation loss: 2.610390745183473

Epoch: 5| Step: 8
Training loss: 2.4043354988098145
Validation loss: 2.611480748781594

Epoch: 5| Step: 9
Training loss: 3.5282421112060547
Validation loss: 2.6084409490708382

Epoch: 5| Step: 10
Training loss: 3.002389907836914
Validation loss: 2.6085565551634757

Epoch: 99| Step: 0
Training loss: 2.6330714225769043
Validation loss: 2.6087521840167303

Epoch: 5| Step: 1
Training loss: 3.1278398036956787
Validation loss: 2.609007453405729

Epoch: 5| Step: 2
Training loss: 3.078981637954712
Validation loss: 2.6057678448256625

Epoch: 5| Step: 3
Training loss: 2.6583662033081055
Validation loss: 2.6040149888684674

Epoch: 5| Step: 4
Training loss: 2.875394105911255
Validation loss: 2.6060318818656345

Epoch: 5| Step: 5
Training loss: 3.045293092727661
Validation loss: 2.603904257538498

Epoch: 5| Step: 6
Training loss: 2.0532734394073486
Validation loss: 2.602193809324695

Epoch: 5| Step: 7
Training loss: 2.6051907539367676
Validation loss: 2.6022177703918947

Epoch: 5| Step: 8
Training loss: 3.1235642433166504
Validation loss: 2.6021057405779437

Epoch: 5| Step: 9
Training loss: 2.41805362701416
Validation loss: 2.6050679504230456

Epoch: 5| Step: 10
Training loss: 2.7649736404418945
Validation loss: 2.605344949230071

Epoch: 100| Step: 0
Training loss: 2.688692092895508
Validation loss: 2.6043995016364643

Epoch: 5| Step: 1
Training loss: 2.6980690956115723
Validation loss: 2.6032816287009948

Epoch: 5| Step: 2
Training loss: 2.7793235778808594
Validation loss: 2.607716001490111

Epoch: 5| Step: 3
Training loss: 2.251375198364258
Validation loss: 2.6111882630214898

Epoch: 5| Step: 4
Training loss: 2.3344597816467285
Validation loss: 2.6062501989385134

Epoch: 5| Step: 5
Training loss: 2.825618267059326
Validation loss: 2.606775353031774

Epoch: 5| Step: 6
Training loss: 2.9454269409179688
Validation loss: 2.6092670143291516

Epoch: 5| Step: 7
Training loss: 2.865494966506958
Validation loss: 2.6086037415330128

Epoch: 5| Step: 8
Training loss: 2.4111673831939697
Validation loss: 2.6082407287372056

Epoch: 5| Step: 9
Training loss: 3.6188552379608154
Validation loss: 2.6119970557510213

Epoch: 5| Step: 10
Training loss: 3.056220054626465
Validation loss: 2.607045594082084

Epoch: 101| Step: 0
Training loss: 2.966074228286743
Validation loss: 2.60588474042954

Epoch: 5| Step: 1
Training loss: 1.8109452724456787
Validation loss: 2.6023777428493706

Epoch: 5| Step: 2
Training loss: 3.2272708415985107
Validation loss: 2.6092396192653204

Epoch: 5| Step: 3
Training loss: 2.750253915786743
Validation loss: 2.6059033921969834

Epoch: 5| Step: 4
Training loss: 3.08221697807312
Validation loss: 2.602712792734946

Epoch: 5| Step: 5
Training loss: 2.568345546722412
Validation loss: 2.6033662339692474

Epoch: 5| Step: 6
Training loss: 2.8565609455108643
Validation loss: 2.6002178243411485

Epoch: 5| Step: 7
Training loss: 2.268122911453247
Validation loss: 2.604434879877234

Epoch: 5| Step: 8
Training loss: 3.077803134918213
Validation loss: 2.6014525557077057

Epoch: 5| Step: 9
Training loss: 3.0296733379364014
Validation loss: 2.6016382222534506

Epoch: 5| Step: 10
Training loss: 2.717531681060791
Validation loss: 2.6007475032601306

Epoch: 102| Step: 0
Training loss: 2.7466371059417725
Validation loss: 2.60483209548458

Epoch: 5| Step: 1
Training loss: 2.62786602973938
Validation loss: 2.6059951013134373

Epoch: 5| Step: 2
Training loss: 2.9322848320007324
Validation loss: 2.602789289207869

Epoch: 5| Step: 3
Training loss: 2.2945351600646973
Validation loss: 2.6023258214355796

Epoch: 5| Step: 4
Training loss: 2.9552834033966064
Validation loss: 2.6047688325246177

Epoch: 5| Step: 5
Training loss: 2.799973964691162
Validation loss: 2.6030261516571045

Epoch: 5| Step: 6
Training loss: 2.6782634258270264
Validation loss: 2.6062991926746983

Epoch: 5| Step: 7
Training loss: 3.2556400299072266
Validation loss: 2.6054198716276433

Epoch: 5| Step: 8
Training loss: 2.301143169403076
Validation loss: 2.600988426516133

Epoch: 5| Step: 9
Training loss: 3.1439778804779053
Validation loss: 2.6010886879377466

Epoch: 5| Step: 10
Training loss: 2.636896848678589
Validation loss: 2.601881960386871

Epoch: 103| Step: 0
Training loss: 3.07804799079895
Validation loss: 2.6015183771810224

Epoch: 5| Step: 1
Training loss: 2.830292224884033
Validation loss: 2.6029204758264686

Epoch: 5| Step: 2
Training loss: 2.524991750717163
Validation loss: 2.6002173757040374

Epoch: 5| Step: 3
Training loss: 2.77888822555542
Validation loss: 2.5999718071312032

Epoch: 5| Step: 4
Training loss: 3.3326401710510254
Validation loss: 2.600461385583365

Epoch: 5| Step: 5
Training loss: 2.2052502632141113
Validation loss: 2.5963002661223054

Epoch: 5| Step: 6
Training loss: 2.1851096153259277
Validation loss: 2.598226175513319

Epoch: 5| Step: 7
Training loss: 3.119241237640381
Validation loss: 2.599583584775207

Epoch: 5| Step: 8
Training loss: 2.1375133991241455
Validation loss: 2.5991787346460486

Epoch: 5| Step: 9
Training loss: 3.0468153953552246
Validation loss: 2.6005686688166794

Epoch: 5| Step: 10
Training loss: 3.157888174057007
Validation loss: 2.597801726351502

Epoch: 104| Step: 0
Training loss: 3.5957653522491455
Validation loss: 2.5936054619409705

Epoch: 5| Step: 1
Training loss: 2.712048053741455
Validation loss: 2.596350246860135

Epoch: 5| Step: 2
Training loss: 2.4937119483947754
Validation loss: 2.597554022266019

Epoch: 5| Step: 3
Training loss: 3.004307270050049
Validation loss: 2.601224858273742

Epoch: 5| Step: 4
Training loss: 2.049191951751709
Validation loss: 2.606239354738625

Epoch: 5| Step: 5
Training loss: 3.043994665145874
Validation loss: 2.611862310799219

Epoch: 5| Step: 6
Training loss: 2.7190964221954346
Validation loss: 2.614548073020033

Epoch: 5| Step: 7
Training loss: 2.6353442668914795
Validation loss: 2.6126738466242307

Epoch: 5| Step: 8
Training loss: 2.5663208961486816
Validation loss: 2.607886357973981

Epoch: 5| Step: 9
Training loss: 2.5077686309814453
Validation loss: 2.6047240303408716

Epoch: 5| Step: 10
Training loss: 3.219791889190674
Validation loss: 2.594344574918029

Epoch: 105| Step: 0
Training loss: 3.135470151901245
Validation loss: 2.5922832591559297

Epoch: 5| Step: 1
Training loss: 3.241713285446167
Validation loss: 2.5928101257611345

Epoch: 5| Step: 2
Training loss: 2.9573230743408203
Validation loss: 2.5983652401995916

Epoch: 5| Step: 3
Training loss: 3.0780537128448486
Validation loss: 2.6023199712076495

Epoch: 5| Step: 4
Training loss: 2.725450038909912
Validation loss: 2.6092535039430023

Epoch: 5| Step: 5
Training loss: 2.613417863845825
Validation loss: 2.60714409043712

Epoch: 5| Step: 6
Training loss: 2.287050724029541
Validation loss: 2.608018554666991

Epoch: 5| Step: 7
Training loss: 2.770338535308838
Validation loss: 2.6142155637023268

Epoch: 5| Step: 8
Training loss: 2.3104889392852783
Validation loss: 2.608396053314209

Epoch: 5| Step: 9
Training loss: 2.737136125564575
Validation loss: 2.6043797282762426

Epoch: 5| Step: 10
Training loss: 2.6966865062713623
Validation loss: 2.5986376680353636

Epoch: 106| Step: 0
Training loss: 2.165278673171997
Validation loss: 2.5984664091499905

Epoch: 5| Step: 1
Training loss: 2.9003026485443115
Validation loss: 2.592277808855939

Epoch: 5| Step: 2
Training loss: 2.858755588531494
Validation loss: 2.5911616663778982

Epoch: 5| Step: 3
Training loss: 1.462833046913147
Validation loss: 2.5910537242889404

Epoch: 5| Step: 4
Training loss: 2.974120616912842
Validation loss: 2.592726407512542

Epoch: 5| Step: 5
Training loss: 2.51186203956604
Validation loss: 2.6003050368319274

Epoch: 5| Step: 6
Training loss: 3.7257652282714844
Validation loss: 2.6139598841308267

Epoch: 5| Step: 7
Training loss: 3.160060405731201
Validation loss: 2.608867606809062

Epoch: 5| Step: 8
Training loss: 2.8238415718078613
Validation loss: 2.6113128431381716

Epoch: 5| Step: 9
Training loss: 2.6649489402770996
Validation loss: 2.6194514510452107

Epoch: 5| Step: 10
Training loss: 3.2377984523773193
Validation loss: 2.6156729754581245

Epoch: 107| Step: 0
Training loss: 3.441584825515747
Validation loss: 2.611533587978732

Epoch: 5| Step: 1
Training loss: 3.1804399490356445
Validation loss: 2.6064068501995457

Epoch: 5| Step: 2
Training loss: 2.6023316383361816
Validation loss: 2.5992194119320122

Epoch: 5| Step: 3
Training loss: 2.3998265266418457
Validation loss: 2.592756217525851

Epoch: 5| Step: 4
Training loss: 2.290215015411377
Validation loss: 2.5941556012758644

Epoch: 5| Step: 5
Training loss: 2.4294607639312744
Validation loss: 2.5947646351270777

Epoch: 5| Step: 6
Training loss: 3.013701915740967
Validation loss: 2.5896249535263225

Epoch: 5| Step: 7
Training loss: 2.111276388168335
Validation loss: 2.5885005740709204

Epoch: 5| Step: 8
Training loss: 2.832947015762329
Validation loss: 2.5928489085166686

Epoch: 5| Step: 9
Training loss: 3.6187355518341064
Validation loss: 2.59555564131788

Epoch: 5| Step: 10
Training loss: 2.332425355911255
Validation loss: 2.5965583888433312

Epoch: 108| Step: 0
Training loss: 3.06252121925354
Validation loss: 2.597205920885968

Epoch: 5| Step: 1
Training loss: 2.929269790649414
Validation loss: 2.5963395692968882

Epoch: 5| Step: 2
Training loss: 3.016345977783203
Validation loss: 2.5919298125851538

Epoch: 5| Step: 3
Training loss: 3.1687543392181396
Validation loss: 2.585871183744041

Epoch: 5| Step: 4
Training loss: 2.7565436363220215
Validation loss: 2.5867319927420667

Epoch: 5| Step: 5
Training loss: 1.8265607357025146
Validation loss: 2.586046916182323

Epoch: 5| Step: 6
Training loss: 2.907736301422119
Validation loss: 2.586835991951727

Epoch: 5| Step: 7
Training loss: 2.4535439014434814
Validation loss: 2.5845901786640124

Epoch: 5| Step: 8
Training loss: 2.8047330379486084
Validation loss: 2.5867400707737094

Epoch: 5| Step: 9
Training loss: 2.815422534942627
Validation loss: 2.5901672737572783

Epoch: 5| Step: 10
Training loss: 2.5761306285858154
Validation loss: 2.5840479199604323

Epoch: 109| Step: 0
Training loss: 2.555011510848999
Validation loss: 2.590349853679698

Epoch: 5| Step: 1
Training loss: 2.7744855880737305
Validation loss: 2.592331281272314

Epoch: 5| Step: 2
Training loss: 3.4877631664276123
Validation loss: 2.5935276246839956

Epoch: 5| Step: 3
Training loss: 2.570042133331299
Validation loss: 2.5899125760601414

Epoch: 5| Step: 4
Training loss: 3.4410502910614014
Validation loss: 2.590966575889177

Epoch: 5| Step: 5
Training loss: 2.3759989738464355
Validation loss: 2.59237822153235

Epoch: 5| Step: 6
Training loss: 3.1165523529052734
Validation loss: 2.5904029261681343

Epoch: 5| Step: 7
Training loss: 2.7065789699554443
Validation loss: 2.5886150008888653

Epoch: 5| Step: 8
Training loss: 2.007781505584717
Validation loss: 2.587985861685968

Epoch: 5| Step: 9
Training loss: 2.861666202545166
Validation loss: 2.5823093998816704

Epoch: 5| Step: 10
Training loss: 2.3026320934295654
Validation loss: 2.585293500654159

Epoch: 110| Step: 0
Training loss: 2.8842597007751465
Validation loss: 2.5881795139722925

Epoch: 5| Step: 1
Training loss: 2.804936170578003
Validation loss: 2.5921104543952533

Epoch: 5| Step: 2
Training loss: 2.934918165206909
Validation loss: 2.592397669310211

Epoch: 5| Step: 3
Training loss: 2.293670415878296
Validation loss: 2.5865240020136677

Epoch: 5| Step: 4
Training loss: 2.3996188640594482
Validation loss: 2.5883242237952446

Epoch: 5| Step: 5
Training loss: 2.4450957775115967
Validation loss: 2.588034506767027

Epoch: 5| Step: 6
Training loss: 3.389080047607422
Validation loss: 2.582528252755442

Epoch: 5| Step: 7
Training loss: 2.00834584236145
Validation loss: 2.588993013546031

Epoch: 5| Step: 8
Training loss: 2.8327488899230957
Validation loss: 2.5902572575435845

Epoch: 5| Step: 9
Training loss: 3.183912992477417
Validation loss: 2.5918504294528755

Epoch: 5| Step: 10
Training loss: 3.2046549320220947
Validation loss: 2.595025190743067

Epoch: 111| Step: 0
Training loss: 2.8028042316436768
Validation loss: 2.6033437354590303

Epoch: 5| Step: 1
Training loss: 2.223905563354492
Validation loss: 2.603168310657624

Epoch: 5| Step: 2
Training loss: 2.620476245880127
Validation loss: 2.60016010397224

Epoch: 5| Step: 3
Training loss: 2.6749112606048584
Validation loss: 2.6017761743196877

Epoch: 5| Step: 4
Training loss: 2.676642656326294
Validation loss: 2.596464398086712

Epoch: 5| Step: 5
Training loss: 2.1524269580841064
Validation loss: 2.5932198057892504

Epoch: 5| Step: 6
Training loss: 2.9060864448547363
Validation loss: 2.591070580226119

Epoch: 5| Step: 7
Training loss: 3.2737326622009277
Validation loss: 2.58570675183368

Epoch: 5| Step: 8
Training loss: 2.891526699066162
Validation loss: 2.582689610860681

Epoch: 5| Step: 9
Training loss: 2.7831969261169434
Validation loss: 2.579863340623917

Epoch: 5| Step: 10
Training loss: 3.3095107078552246
Validation loss: 2.5791432447330926

Epoch: 112| Step: 0
Training loss: 2.7988333702087402
Validation loss: 2.5809564462272068

Epoch: 5| Step: 1
Training loss: 2.252640724182129
Validation loss: 2.578619316060056

Epoch: 5| Step: 2
Training loss: 3.2754242420196533
Validation loss: 2.5790298420895814

Epoch: 5| Step: 3
Training loss: 2.3291211128234863
Validation loss: 2.5812092955394457

Epoch: 5| Step: 4
Training loss: 2.458630323410034
Validation loss: 2.5818003377606793

Epoch: 5| Step: 5
Training loss: 2.1953125
Validation loss: 2.5806723692083873

Epoch: 5| Step: 6
Training loss: 2.9012436866760254
Validation loss: 2.5784462292989097

Epoch: 5| Step: 7
Training loss: 2.4856817722320557
Validation loss: 2.578867563637354

Epoch: 5| Step: 8
Training loss: 3.1682093143463135
Validation loss: 2.5838178870498494

Epoch: 5| Step: 9
Training loss: 3.412886381149292
Validation loss: 2.580325144593434

Epoch: 5| Step: 10
Training loss: 2.947657346725464
Validation loss: 2.5789067411935456

Epoch: 113| Step: 0
Training loss: 2.715308666229248
Validation loss: 2.57929757589935

Epoch: 5| Step: 1
Training loss: 3.2559974193573
Validation loss: 2.5788018139459754

Epoch: 5| Step: 2
Training loss: 2.5345065593719482
Validation loss: 2.5797925841423774

Epoch: 5| Step: 3
Training loss: 3.0513007640838623
Validation loss: 2.5789313034344743

Epoch: 5| Step: 4
Training loss: 2.5893607139587402
Validation loss: 2.5812539926139255

Epoch: 5| Step: 5
Training loss: 2.420422077178955
Validation loss: 2.58231157641257

Epoch: 5| Step: 6
Training loss: 2.397059917449951
Validation loss: 2.5906086762746177

Epoch: 5| Step: 7
Training loss: 2.674363613128662
Validation loss: 2.5959572689507597

Epoch: 5| Step: 8
Training loss: 3.394366502761841
Validation loss: 2.5936192133093394

Epoch: 5| Step: 9
Training loss: 2.856314182281494
Validation loss: 2.5883378623634257

Epoch: 5| Step: 10
Training loss: 2.3205416202545166
Validation loss: 2.58138124404415

Epoch: 114| Step: 0
Training loss: 2.50170636177063
Validation loss: 2.5817208033736034

Epoch: 5| Step: 1
Training loss: 3.151667356491089
Validation loss: 2.5788124197272846

Epoch: 5| Step: 2
Training loss: 2.844296932220459
Validation loss: 2.580772884430424

Epoch: 5| Step: 3
Training loss: 2.8217825889587402
Validation loss: 2.5839611048339517

Epoch: 5| Step: 4
Training loss: 2.1805479526519775
Validation loss: 2.5844560861587524

Epoch: 5| Step: 5
Training loss: 3.0278196334838867
Validation loss: 2.587595162853118

Epoch: 5| Step: 6
Training loss: 2.6610851287841797
Validation loss: 2.5865229791210544

Epoch: 5| Step: 7
Training loss: 3.6627304553985596
Validation loss: 2.583669615048234

Epoch: 5| Step: 8
Training loss: 2.1485354900360107
Validation loss: 2.5831256194781234

Epoch: 5| Step: 9
Training loss: 2.7004501819610596
Validation loss: 2.5788515947198354

Epoch: 5| Step: 10
Training loss: 2.525468587875366
Validation loss: 2.577801460860878

Epoch: 115| Step: 0
Training loss: 2.680741786956787
Validation loss: 2.574310464243735

Epoch: 5| Step: 1
Training loss: 2.8244740962982178
Validation loss: 2.569817345629456

Epoch: 5| Step: 2
Training loss: 2.810107469558716
Validation loss: 2.5693552058230162

Epoch: 5| Step: 3
Training loss: 2.6932148933410645
Validation loss: 2.57246333296581

Epoch: 5| Step: 4
Training loss: 2.6288552284240723
Validation loss: 2.576186710788358

Epoch: 5| Step: 5
Training loss: 3.05517840385437
Validation loss: 2.57100986665295

Epoch: 5| Step: 6
Training loss: 2.704612970352173
Validation loss: 2.576929392353181

Epoch: 5| Step: 7
Training loss: 3.1897389888763428
Validation loss: 2.581917134664392

Epoch: 5| Step: 8
Training loss: 2.3791744709014893
Validation loss: 2.5810398722207673

Epoch: 5| Step: 9
Training loss: 2.53775954246521
Validation loss: 2.581899783944571

Epoch: 5| Step: 10
Training loss: 2.6683452129364014
Validation loss: 2.5731226244280414

Epoch: 116| Step: 0
Training loss: 2.9682559967041016
Validation loss: 2.5691813294605543

Epoch: 5| Step: 1
Training loss: 3.15461802482605
Validation loss: 2.567355430254372

Epoch: 5| Step: 2
Training loss: 2.709096908569336
Validation loss: 2.5671133559237242

Epoch: 5| Step: 3
Training loss: 2.3179917335510254
Validation loss: 2.5635919442740818

Epoch: 5| Step: 4
Training loss: 2.5643069744110107
Validation loss: 2.5624957059019353

Epoch: 5| Step: 5
Training loss: 3.0164008140563965
Validation loss: 2.5676576937398603

Epoch: 5| Step: 6
Training loss: 1.923301339149475
Validation loss: 2.564636638087611

Epoch: 5| Step: 7
Training loss: 3.1113221645355225
Validation loss: 2.5660454073259906

Epoch: 5| Step: 8
Training loss: 2.80971360206604
Validation loss: 2.56796335404919

Epoch: 5| Step: 9
Training loss: 2.4355852603912354
Validation loss: 2.5653219889569026

Epoch: 5| Step: 10
Training loss: 3.1800522804260254
Validation loss: 2.5610316363714074

Epoch: 117| Step: 0
Training loss: 2.4659676551818848
Validation loss: 2.5685173106449906

Epoch: 5| Step: 1
Training loss: 2.7650344371795654
Validation loss: 2.564501788026543

Epoch: 5| Step: 2
Training loss: 2.915879964828491
Validation loss: 2.5691397959186184

Epoch: 5| Step: 3
Training loss: 2.9467666149139404
Validation loss: 2.565711727706335

Epoch: 5| Step: 4
Training loss: 2.7721798419952393
Validation loss: 2.5639190391827653

Epoch: 5| Step: 5
Training loss: 2.348553419113159
Validation loss: 2.56450504385015

Epoch: 5| Step: 6
Training loss: 2.748816728591919
Validation loss: 2.5633589708676903

Epoch: 5| Step: 7
Training loss: 2.6507434844970703
Validation loss: 2.5602577732455347

Epoch: 5| Step: 8
Training loss: 3.0125229358673096
Validation loss: 2.5629403616792414

Epoch: 5| Step: 9
Training loss: 3.145972728729248
Validation loss: 2.562089027897004

Epoch: 5| Step: 10
Training loss: 2.2879908084869385
Validation loss: 2.566238377683906

Epoch: 118| Step: 0
Training loss: 2.9323201179504395
Validation loss: 2.5671619292228454

Epoch: 5| Step: 1
Training loss: 2.4899168014526367
Validation loss: 2.566364562639626

Epoch: 5| Step: 2
Training loss: 2.8302581310272217
Validation loss: 2.5659440255934194

Epoch: 5| Step: 3
Training loss: 2.379279375076294
Validation loss: 2.562007837398078

Epoch: 5| Step: 4
Training loss: 3.0728631019592285
Validation loss: 2.5679909644588346

Epoch: 5| Step: 5
Training loss: 3.06364107131958
Validation loss: 2.565831207459973

Epoch: 5| Step: 6
Training loss: 2.1377007961273193
Validation loss: 2.5578276136870026

Epoch: 5| Step: 7
Training loss: 1.9796091318130493
Validation loss: 2.5567810099612

Epoch: 5| Step: 8
Training loss: 3.7327067852020264
Validation loss: 2.5577695882448586

Epoch: 5| Step: 9
Training loss: 2.2294762134552
Validation loss: 2.557713805988271

Epoch: 5| Step: 10
Training loss: 3.3174538612365723
Validation loss: 2.5519266487449728

Epoch: 119| Step: 0
Training loss: 2.766648769378662
Validation loss: 2.5570625643576346

Epoch: 5| Step: 1
Training loss: 2.294675350189209
Validation loss: 2.5547574156074115

Epoch: 5| Step: 2
Training loss: 3.5577378273010254
Validation loss: 2.55646578983594

Epoch: 5| Step: 3
Training loss: 3.498243808746338
Validation loss: 2.5540153082980903

Epoch: 5| Step: 4
Training loss: 2.646141529083252
Validation loss: 2.551654990001391

Epoch: 5| Step: 5
Training loss: 2.4684927463531494
Validation loss: 2.5514533212107997

Epoch: 5| Step: 6
Training loss: 1.9120893478393555
Validation loss: 2.553276385030439

Epoch: 5| Step: 7
Training loss: 3.6120810508728027
Validation loss: 2.552180900368639

Epoch: 5| Step: 8
Training loss: 2.2655906677246094
Validation loss: 2.554048581789899

Epoch: 5| Step: 9
Training loss: 2.3638205528259277
Validation loss: 2.5550221371394333

Epoch: 5| Step: 10
Training loss: 2.6222589015960693
Validation loss: 2.5523814719210387

Epoch: 120| Step: 0
Training loss: 2.5611045360565186
Validation loss: 2.5549658934275308

Epoch: 5| Step: 1
Training loss: 2.9882798194885254
Validation loss: 2.5538063408226095

Epoch: 5| Step: 2
Training loss: 3.1181540489196777
Validation loss: 2.5620445102773686

Epoch: 5| Step: 3
Training loss: 2.476764678955078
Validation loss: 2.557102803261049

Epoch: 5| Step: 4
Training loss: 2.9302096366882324
Validation loss: 2.5590545336405435

Epoch: 5| Step: 5
Training loss: 2.746339797973633
Validation loss: 2.56004673434842

Epoch: 5| Step: 6
Training loss: 2.1963658332824707
Validation loss: 2.5556215419564197

Epoch: 5| Step: 7
Training loss: 2.4933485984802246
Validation loss: 2.5582576900400142

Epoch: 5| Step: 8
Training loss: 2.837404727935791
Validation loss: 2.5623066809869584

Epoch: 5| Step: 9
Training loss: 3.022298574447632
Validation loss: 2.559366085196054

Epoch: 5| Step: 10
Training loss: 2.6354002952575684
Validation loss: 2.5533387891707884

Epoch: 121| Step: 0
Training loss: 2.4526028633117676
Validation loss: 2.549218931505757

Epoch: 5| Step: 1
Training loss: 2.8722305297851562
Validation loss: 2.553792122871645

Epoch: 5| Step: 2
Training loss: 3.0068697929382324
Validation loss: 2.5491107484345794

Epoch: 5| Step: 3
Training loss: 2.8062727451324463
Validation loss: 2.547525203356179

Epoch: 5| Step: 4
Training loss: 2.8796849250793457
Validation loss: 2.5477945189322195

Epoch: 5| Step: 5
Training loss: 2.672938585281372
Validation loss: 2.5509960600124892

Epoch: 5| Step: 6
Training loss: 3.0590755939483643
Validation loss: 2.549491802851359

Epoch: 5| Step: 7
Training loss: 2.2319693565368652
Validation loss: 2.5491112765445503

Epoch: 5| Step: 8
Training loss: 2.859835147857666
Validation loss: 2.548639456431071

Epoch: 5| Step: 9
Training loss: 2.34393310546875
Validation loss: 2.5482179785287506

Epoch: 5| Step: 10
Training loss: 2.829498767852783
Validation loss: 2.548638214347183

Epoch: 122| Step: 0
Training loss: 2.717698574066162
Validation loss: 2.5509431618516163

Epoch: 5| Step: 1
Training loss: 2.795025587081909
Validation loss: 2.549912137369956

Epoch: 5| Step: 2
Training loss: 2.165012836456299
Validation loss: 2.5459492078391452

Epoch: 5| Step: 3
Training loss: 3.478350877761841
Validation loss: 2.547371531045565

Epoch: 5| Step: 4
Training loss: 2.7948391437530518
Validation loss: 2.553151769022788

Epoch: 5| Step: 5
Training loss: 2.3191466331481934
Validation loss: 2.5533814789146505

Epoch: 5| Step: 6
Training loss: 3.330484390258789
Validation loss: 2.5549937909649265

Epoch: 5| Step: 7
Training loss: 2.373150587081909
Validation loss: 2.55245392809632

Epoch: 5| Step: 8
Training loss: 2.3889479637145996
Validation loss: 2.5515034198760986

Epoch: 5| Step: 9
Training loss: 2.677234649658203
Validation loss: 2.5570524559226087

Epoch: 5| Step: 10
Training loss: 2.989753007888794
Validation loss: 2.547218894445768

Epoch: 123| Step: 0
Training loss: 2.3495066165924072
Validation loss: 2.549590608125092

Epoch: 5| Step: 1
Training loss: 2.8132193088531494
Validation loss: 2.547801927853656

Epoch: 5| Step: 2
Training loss: 3.3331191539764404
Validation loss: 2.547293281042448

Epoch: 5| Step: 3
Training loss: 2.8559727668762207
Validation loss: 2.5494249482308664

Epoch: 5| Step: 4
Training loss: 2.522765636444092
Validation loss: 2.548949118583433

Epoch: 5| Step: 5
Training loss: 2.880082607269287
Validation loss: 2.5530120967536845

Epoch: 5| Step: 6
Training loss: 2.897296190261841
Validation loss: 2.552950821897035

Epoch: 5| Step: 7
Training loss: 2.887360095977783
Validation loss: 2.553336799785655

Epoch: 5| Step: 8
Training loss: 1.6945555210113525
Validation loss: 2.553746251649754

Epoch: 5| Step: 9
Training loss: 2.940821886062622
Validation loss: 2.5551124926536315

Epoch: 5| Step: 10
Training loss: 2.81097412109375
Validation loss: 2.554106978959935

Epoch: 124| Step: 0
Training loss: 3.2715892791748047
Validation loss: 2.552037590293474

Epoch: 5| Step: 1
Training loss: 2.6143784523010254
Validation loss: 2.5517418999825754

Epoch: 5| Step: 2
Training loss: 2.1313371658325195
Validation loss: 2.549410894352903

Epoch: 5| Step: 3
Training loss: 2.3981571197509766
Validation loss: 2.548635205914897

Epoch: 5| Step: 4
Training loss: 2.996846914291382
Validation loss: 2.544473842907977

Epoch: 5| Step: 5
Training loss: 2.9219727516174316
Validation loss: 2.5516052066638903

Epoch: 5| Step: 6
Training loss: 2.3693313598632812
Validation loss: 2.5529678226799093

Epoch: 5| Step: 7
Training loss: 3.607783555984497
Validation loss: 2.548111877133769

Epoch: 5| Step: 8
Training loss: 2.6329362392425537
Validation loss: 2.5499039926836566

Epoch: 5| Step: 9
Training loss: 2.691863536834717
Validation loss: 2.550039250363586

Epoch: 5| Step: 10
Training loss: 2.2408926486968994
Validation loss: 2.5478357704736854

Epoch: 125| Step: 0
Training loss: 2.462268114089966
Validation loss: 2.5523274252491612

Epoch: 5| Step: 1
Training loss: 2.921496629714966
Validation loss: 2.5463958888925533

Epoch: 5| Step: 2
Training loss: 2.5900673866271973
Validation loss: 2.551889265737226

Epoch: 5| Step: 3
Training loss: 2.431622266769409
Validation loss: 2.5500939020546536

Epoch: 5| Step: 4
Training loss: 2.704939365386963
Validation loss: 2.552584502004808

Epoch: 5| Step: 5
Training loss: 3.026458501815796
Validation loss: 2.553167494394446

Epoch: 5| Step: 6
Training loss: 2.865574359893799
Validation loss: 2.5536608721620295

Epoch: 5| Step: 7
Training loss: 2.413362741470337
Validation loss: 2.550102772251252

Epoch: 5| Step: 8
Training loss: 3.1108078956604004
Validation loss: 2.5486369927724204

Epoch: 5| Step: 9
Training loss: 2.974562168121338
Validation loss: 2.5475839645631853

Epoch: 5| Step: 10
Training loss: 2.329561948776245
Validation loss: 2.542452883976762

Epoch: 126| Step: 0
Training loss: 2.384594440460205
Validation loss: 2.5444097211284022

Epoch: 5| Step: 1
Training loss: 2.0632996559143066
Validation loss: 2.549678312834873

Epoch: 5| Step: 2
Training loss: 2.5832271575927734
Validation loss: 2.5540212661989274

Epoch: 5| Step: 3
Training loss: 3.4444260597229004
Validation loss: 2.550075605351438

Epoch: 5| Step: 4
Training loss: 3.4472713470458984
Validation loss: 2.5446622294764363

Epoch: 5| Step: 5
Training loss: 2.447786808013916
Validation loss: 2.543369457285891

Epoch: 5| Step: 6
Training loss: 2.970012903213501
Validation loss: 2.5418217592341925

Epoch: 5| Step: 7
Training loss: 2.6773228645324707
Validation loss: 2.5472115470517065

Epoch: 5| Step: 8
Training loss: 2.264974355697632
Validation loss: 2.542738301779634

Epoch: 5| Step: 9
Training loss: 2.7746171951293945
Validation loss: 2.5444047374110066

Epoch: 5| Step: 10
Training loss: 2.851700782775879
Validation loss: 2.5388617566836778

Epoch: 127| Step: 0
Training loss: 3.032639265060425
Validation loss: 2.538746239036642

Epoch: 5| Step: 1
Training loss: 1.9901111125946045
Validation loss: 2.539133048826648

Epoch: 5| Step: 2
Training loss: 2.5427472591400146
Validation loss: 2.543664550268522

Epoch: 5| Step: 3
Training loss: 3.433319568634033
Validation loss: 2.535120379540228

Epoch: 5| Step: 4
Training loss: 2.8764801025390625
Validation loss: 2.5404273053651214

Epoch: 5| Step: 5
Training loss: 2.676689624786377
Validation loss: 2.5390005701331684

Epoch: 5| Step: 6
Training loss: 3.0542426109313965
Validation loss: 2.540280557447864

Epoch: 5| Step: 7
Training loss: 2.8816006183624268
Validation loss: 2.536776063262775

Epoch: 5| Step: 8
Training loss: 2.2374520301818848
Validation loss: 2.5348815020694526

Epoch: 5| Step: 9
Training loss: 3.0825467109680176
Validation loss: 2.5357710084607525

Epoch: 5| Step: 10
Training loss: 1.9243801832199097
Validation loss: 2.5375134842370146

Epoch: 128| Step: 0
Training loss: 1.899084448814392
Validation loss: 2.5356068393235565

Epoch: 5| Step: 1
Training loss: 2.651618480682373
Validation loss: 2.540420106662217

Epoch: 5| Step: 2
Training loss: 3.4942727088928223
Validation loss: 2.5404598610375517

Epoch: 5| Step: 3
Training loss: 2.693375825881958
Validation loss: 2.53841096098705

Epoch: 5| Step: 4
Training loss: 2.521907329559326
Validation loss: 2.5380077567151798

Epoch: 5| Step: 5
Training loss: 3.754383087158203
Validation loss: 2.5398841032417874

Epoch: 5| Step: 6
Training loss: 3.2144827842712402
Validation loss: 2.539051112308297

Epoch: 5| Step: 7
Training loss: 2.6506545543670654
Validation loss: 2.5403480401603122

Epoch: 5| Step: 8
Training loss: 1.7305033206939697
Validation loss: 2.540422772848478

Epoch: 5| Step: 9
Training loss: 2.862865686416626
Validation loss: 2.5415610113451557

Epoch: 5| Step: 10
Training loss: 2.323052167892456
Validation loss: 2.547265870596773

Epoch: 129| Step: 0
Training loss: 2.786891460418701
Validation loss: 2.5461709460904522

Epoch: 5| Step: 1
Training loss: 2.805924415588379
Validation loss: 2.5430771227805846

Epoch: 5| Step: 2
Training loss: 2.3210723400115967
Validation loss: 2.5501077790414133

Epoch: 5| Step: 3
Training loss: 2.127782106399536
Validation loss: 2.5412155248785533

Epoch: 5| Step: 4
Training loss: 2.263327121734619
Validation loss: 2.54252556831606

Epoch: 5| Step: 5
Training loss: 2.813406467437744
Validation loss: 2.5418729218103553

Epoch: 5| Step: 6
Training loss: 3.003998279571533
Validation loss: 2.5372474193573

Epoch: 5| Step: 7
Training loss: 3.4037318229675293
Validation loss: 2.5398793758884555

Epoch: 5| Step: 8
Training loss: 2.783687114715576
Validation loss: 2.538508794640982

Epoch: 5| Step: 9
Training loss: 2.9706220626831055
Validation loss: 2.5406969901054137

Epoch: 5| Step: 10
Training loss: 2.5629665851593018
Validation loss: 2.5371363983359387

Epoch: 130| Step: 0
Training loss: 3.1750712394714355
Validation loss: 2.5379439079633324

Epoch: 5| Step: 1
Training loss: 2.625762462615967
Validation loss: 2.538103388201806

Epoch: 5| Step: 2
Training loss: 2.1795687675476074
Validation loss: 2.5419094101075204

Epoch: 5| Step: 3
Training loss: 3.3591341972351074
Validation loss: 2.5358974600350983

Epoch: 5| Step: 4
Training loss: 2.4605345726013184
Validation loss: 2.5387249864557737

Epoch: 5| Step: 5
Training loss: 2.646692991256714
Validation loss: 2.539559489937239

Epoch: 5| Step: 6
Training loss: 3.021639585494995
Validation loss: 2.5369690284934094

Epoch: 5| Step: 7
Training loss: 2.8562963008880615
Validation loss: 2.536834478378296

Epoch: 5| Step: 8
Training loss: 2.8073534965515137
Validation loss: 2.541052233788275

Epoch: 5| Step: 9
Training loss: 2.5282723903656006
Validation loss: 2.5396293388899935

Epoch: 5| Step: 10
Training loss: 2.0716793537139893
Validation loss: 2.532855123601934

Epoch: 131| Step: 0
Training loss: 2.9965500831604004
Validation loss: 2.530966317781838

Epoch: 5| Step: 1
Training loss: 3.161531925201416
Validation loss: 2.535151945647373

Epoch: 5| Step: 2
Training loss: 2.3014791011810303
Validation loss: 2.5335492369949177

Epoch: 5| Step: 3
Training loss: 2.765995502471924
Validation loss: 2.5351371278044996

Epoch: 5| Step: 4
Training loss: 2.5193164348602295
Validation loss: 2.533210100666169

Epoch: 5| Step: 5
Training loss: 2.5002334117889404
Validation loss: 2.538118113753616

Epoch: 5| Step: 6
Training loss: 2.451066732406616
Validation loss: 2.5408359419914985

Epoch: 5| Step: 7
Training loss: 2.0787320137023926
Validation loss: 2.5387977041223997

Epoch: 5| Step: 8
Training loss: 2.8065085411071777
Validation loss: 2.543747978825723

Epoch: 5| Step: 9
Training loss: 3.0078771114349365
Validation loss: 2.543731642025773

Epoch: 5| Step: 10
Training loss: 3.3463923931121826
Validation loss: 2.5447134971618652

Epoch: 132| Step: 0
Training loss: 2.245546579360962
Validation loss: 2.5452605678189184

Epoch: 5| Step: 1
Training loss: 3.604008197784424
Validation loss: 2.5380520769344863

Epoch: 5| Step: 2
Training loss: 2.744049549102783
Validation loss: 2.5345653205789547

Epoch: 5| Step: 3
Training loss: 2.410208225250244
Validation loss: 2.5332247262359946

Epoch: 5| Step: 4
Training loss: 1.7204233407974243
Validation loss: 2.5298598940654466

Epoch: 5| Step: 5
Training loss: 2.528015613555908
Validation loss: 2.5280036208450154

Epoch: 5| Step: 6
Training loss: 3.3026890754699707
Validation loss: 2.5276184184576875

Epoch: 5| Step: 7
Training loss: 2.7782886028289795
Validation loss: 2.530415411918394

Epoch: 5| Step: 8
Training loss: 2.531909704208374
Validation loss: 2.526723174638646

Epoch: 5| Step: 9
Training loss: 3.039273500442505
Validation loss: 2.5282948324757237

Epoch: 5| Step: 10
Training loss: 2.93377947807312
Validation loss: 2.525370931112638

Epoch: 133| Step: 0
Training loss: 2.120866060256958
Validation loss: 2.5323673089345298

Epoch: 5| Step: 1
Training loss: 3.5338294506073
Validation loss: 2.5255494117736816

Epoch: 5| Step: 2
Training loss: 2.9965953826904297
Validation loss: 2.5284700726950042

Epoch: 5| Step: 3
Training loss: 2.6887452602386475
Validation loss: 2.53025084669872

Epoch: 5| Step: 4
Training loss: 1.9554322957992554
Validation loss: 2.5293688287017164

Epoch: 5| Step: 5
Training loss: 2.5298526287078857
Validation loss: 2.5328960521246797

Epoch: 5| Step: 6
Training loss: 2.731813669204712
Validation loss: 2.5324077657473985

Epoch: 5| Step: 7
Training loss: 2.295243740081787
Validation loss: 2.5330724767459336

Epoch: 5| Step: 8
Training loss: 3.4522106647491455
Validation loss: 2.531164041129492

Epoch: 5| Step: 9
Training loss: 2.5627028942108154
Validation loss: 2.531372703531737

Epoch: 5| Step: 10
Training loss: 2.975348711013794
Validation loss: 2.5331854666433027

Epoch: 134| Step: 0
Training loss: 3.581965684890747
Validation loss: 2.5368037787816857

Epoch: 5| Step: 1
Training loss: 2.208613872528076
Validation loss: 2.538348182555168

Epoch: 5| Step: 2
Training loss: 2.7613818645477295
Validation loss: 2.5368622067154094

Epoch: 5| Step: 3
Training loss: 2.6741764545440674
Validation loss: 2.541201759410161

Epoch: 5| Step: 4
Training loss: 2.886578321456909
Validation loss: 2.538459183067404

Epoch: 5| Step: 5
Training loss: 3.2953052520751953
Validation loss: 2.539638880760439

Epoch: 5| Step: 6
Training loss: 2.4728264808654785
Validation loss: 2.5419130325317383

Epoch: 5| Step: 7
Training loss: 2.015227794647217
Validation loss: 2.5409044117055912

Epoch: 5| Step: 8
Training loss: 2.5256991386413574
Validation loss: 2.540804306666056

Epoch: 5| Step: 9
Training loss: 2.957934617996216
Validation loss: 2.538251530739569

Epoch: 5| Step: 10
Training loss: 2.3563883304595947
Validation loss: 2.5362254393998014

Epoch: 135| Step: 0
Training loss: 2.1345181465148926
Validation loss: 2.5380824432578137

Epoch: 5| Step: 1
Training loss: 2.5276215076446533
Validation loss: 2.5382338595646683

Epoch: 5| Step: 2
Training loss: 3.2057979106903076
Validation loss: 2.5391627434761292

Epoch: 5| Step: 3
Training loss: 3.1207339763641357
Validation loss: 2.5383879933305966

Epoch: 5| Step: 4
Training loss: 3.03395414352417
Validation loss: 2.535454296296643

Epoch: 5| Step: 5
Training loss: 2.290238618850708
Validation loss: 2.5347633515634844

Epoch: 5| Step: 6
Training loss: 2.7389304637908936
Validation loss: 2.5344272300761235

Epoch: 5| Step: 7
Training loss: 3.2379143238067627
Validation loss: 2.527665735572897

Epoch: 5| Step: 8
Training loss: 2.0915164947509766
Validation loss: 2.5283026105614117

Epoch: 5| Step: 9
Training loss: 3.2135040760040283
Validation loss: 2.5238203207651773

Epoch: 5| Step: 10
Training loss: 2.127256155014038
Validation loss: 2.530059365816014

Epoch: 136| Step: 0
Training loss: 3.0278422832489014
Validation loss: 2.5281326104235906

Epoch: 5| Step: 1
Training loss: 2.426781177520752
Validation loss: 2.5279519737407727

Epoch: 5| Step: 2
Training loss: 2.7910571098327637
Validation loss: 2.5275093509304907

Epoch: 5| Step: 3
Training loss: 2.7250068187713623
Validation loss: 2.524264033122729

Epoch: 5| Step: 4
Training loss: 2.9632296562194824
Validation loss: 2.5313604621477026

Epoch: 5| Step: 5
Training loss: 2.827559232711792
Validation loss: 2.526815378537742

Epoch: 5| Step: 6
Training loss: 3.02647066116333
Validation loss: 2.529928053579023

Epoch: 5| Step: 7
Training loss: 2.8717200756073
Validation loss: 2.5253722001147527

Epoch: 5| Step: 8
Training loss: 2.7551865577697754
Validation loss: 2.5234297321688746

Epoch: 5| Step: 9
Training loss: 1.4354349374771118
Validation loss: 2.525235693941834

Epoch: 5| Step: 10
Training loss: 2.9628889560699463
Validation loss: 2.527774477517733

Epoch: 137| Step: 0
Training loss: 1.5005906820297241
Validation loss: 2.5255640142707416

Epoch: 5| Step: 1
Training loss: 3.1662306785583496
Validation loss: 2.5279433163263465

Epoch: 5| Step: 2
Training loss: 3.052100658416748
Validation loss: 2.529511959322037

Epoch: 5| Step: 3
Training loss: 2.172407388687134
Validation loss: 2.529772432901526

Epoch: 5| Step: 4
Training loss: 2.6470272541046143
Validation loss: 2.5314773282697125

Epoch: 5| Step: 5
Training loss: 2.8672611713409424
Validation loss: 2.532828829621756

Epoch: 5| Step: 6
Training loss: 2.147083282470703
Validation loss: 2.531813503593527

Epoch: 5| Step: 7
Training loss: 2.2609291076660156
Validation loss: 2.533938797571326

Epoch: 5| Step: 8
Training loss: 3.301473617553711
Validation loss: 2.534561764809393

Epoch: 5| Step: 9
Training loss: 3.495211124420166
Validation loss: 2.5338323500848587

Epoch: 5| Step: 10
Training loss: 3.295555830001831
Validation loss: 2.5324597999613774

Epoch: 138| Step: 0
Training loss: 2.2522635459899902
Validation loss: 2.5312408042210404

Epoch: 5| Step: 1
Training loss: 2.2924060821533203
Validation loss: 2.5329710129768617

Epoch: 5| Step: 2
Training loss: 2.6984450817108154
Validation loss: 2.538731380175519

Epoch: 5| Step: 3
Training loss: 3.2729110717773438
Validation loss: 2.528974440789992

Epoch: 5| Step: 4
Training loss: 2.5936739444732666
Validation loss: 2.5300680950123775

Epoch: 5| Step: 5
Training loss: 2.4210596084594727
Validation loss: 2.525781318705569

Epoch: 5| Step: 6
Training loss: 3.1466472148895264
Validation loss: 2.525029828471522

Epoch: 5| Step: 7
Training loss: 2.666022539138794
Validation loss: 2.5272169061886367

Epoch: 5| Step: 8
Training loss: 2.873575210571289
Validation loss: 2.5245780970460627

Epoch: 5| Step: 9
Training loss: 2.9464659690856934
Validation loss: 2.5320782815256426

Epoch: 5| Step: 10
Training loss: 2.5401692390441895
Validation loss: 2.5270471726694415

Epoch: 139| Step: 0
Training loss: 2.6395301818847656
Validation loss: 2.529336101265364

Epoch: 5| Step: 1
Training loss: 2.354360580444336
Validation loss: 2.5279556653832875

Epoch: 5| Step: 2
Training loss: 2.8262994289398193
Validation loss: 2.53265877180202

Epoch: 5| Step: 3
Training loss: 2.3771042823791504
Validation loss: 2.5274571603344334

Epoch: 5| Step: 4
Training loss: 3.0579099655151367
Validation loss: 2.52631720419853

Epoch: 5| Step: 5
Training loss: 3.154296398162842
Validation loss: 2.5298050295922065

Epoch: 5| Step: 6
Training loss: 2.8836395740509033
Validation loss: 2.5256963647821897

Epoch: 5| Step: 7
Training loss: 2.920238494873047
Validation loss: 2.529178229711389

Epoch: 5| Step: 8
Training loss: 2.5135109424591064
Validation loss: 2.5212633917408604

Epoch: 5| Step: 9
Training loss: 2.4378178119659424
Validation loss: 2.5267600039000153

Epoch: 5| Step: 10
Training loss: 2.575847864151001
Validation loss: 2.522845806614045

Epoch: 140| Step: 0
Training loss: 2.7903285026550293
Validation loss: 2.534405764713082

Epoch: 5| Step: 1
Training loss: 3.2171835899353027
Validation loss: 2.5374651544837543

Epoch: 5| Step: 2
Training loss: 3.5288634300231934
Validation loss: 2.5309968404872443

Epoch: 5| Step: 3
Training loss: 2.879018783569336
Validation loss: 2.5275389353434243

Epoch: 5| Step: 4
Training loss: 2.196315288543701
Validation loss: 2.532201074784802

Epoch: 5| Step: 5
Training loss: 2.392951250076294
Validation loss: 2.5244795660818777

Epoch: 5| Step: 6
Training loss: 2.610433578491211
Validation loss: 2.52604962677084

Epoch: 5| Step: 7
Training loss: 2.3207569122314453
Validation loss: 2.5301117474032986

Epoch: 5| Step: 8
Training loss: 3.125389814376831
Validation loss: 2.5293339093526206

Epoch: 5| Step: 9
Training loss: 1.8641586303710938
Validation loss: 2.5291765453994914

Epoch: 5| Step: 10
Training loss: 2.8222506046295166
Validation loss: 2.5294201784236456

Epoch: 141| Step: 0
Training loss: 2.846771240234375
Validation loss: 2.5279515891946773

Epoch: 5| Step: 1
Training loss: 3.1171720027923584
Validation loss: 2.5252324688819145

Epoch: 5| Step: 2
Training loss: 2.5550026893615723
Validation loss: 2.5241360638731267

Epoch: 5| Step: 3
Training loss: 2.7841222286224365
Validation loss: 2.525751765056323

Epoch: 5| Step: 4
Training loss: 2.4587836265563965
Validation loss: 2.5228217007011495

Epoch: 5| Step: 5
Training loss: 2.3632125854492188
Validation loss: 2.5241090174644225

Epoch: 5| Step: 6
Training loss: 2.916560411453247
Validation loss: 2.519575967583605

Epoch: 5| Step: 7
Training loss: 2.927990436553955
Validation loss: 2.5187093109212895

Epoch: 5| Step: 8
Training loss: 2.692692518234253
Validation loss: 2.515789939511207

Epoch: 5| Step: 9
Training loss: 2.348431348800659
Validation loss: 2.5138286698249077

Epoch: 5| Step: 10
Training loss: 2.6559884548187256
Validation loss: 2.5160272249611477

Epoch: 142| Step: 0
Training loss: 2.2544004917144775
Validation loss: 2.519114519960137

Epoch: 5| Step: 1
Training loss: 2.5357472896575928
Validation loss: 2.5152356906603743

Epoch: 5| Step: 2
Training loss: 2.769371747970581
Validation loss: 2.515853576762702

Epoch: 5| Step: 3
Training loss: 3.0858490467071533
Validation loss: 2.519835777180169

Epoch: 5| Step: 4
Training loss: 3.1619839668273926
Validation loss: 2.517747156081661

Epoch: 5| Step: 5
Training loss: 3.3035411834716797
Validation loss: 2.5201491437932497

Epoch: 5| Step: 6
Training loss: 2.3268566131591797
Validation loss: 2.521925039188836

Epoch: 5| Step: 7
Training loss: 2.062427043914795
Validation loss: 2.519643595141749

Epoch: 5| Step: 8
Training loss: 3.036222457885742
Validation loss: 2.524966022019745

Epoch: 5| Step: 9
Training loss: 2.6918671131134033
Validation loss: 2.5241068076061945

Epoch: 5| Step: 10
Training loss: 2.3829307556152344
Validation loss: 2.5303231311100784

Epoch: 143| Step: 0
Training loss: 2.486494541168213
Validation loss: 2.5222481348181285

Epoch: 5| Step: 1
Training loss: 2.099764585494995
Validation loss: 2.5253041123831146

Epoch: 5| Step: 2
Training loss: 3.3845314979553223
Validation loss: 2.5269379667056504

Epoch: 5| Step: 3
Training loss: 2.9319002628326416
Validation loss: 2.527332482799407

Epoch: 5| Step: 4
Training loss: 3.003819227218628
Validation loss: 2.526906692853538

Epoch: 5| Step: 5
Training loss: 3.3959221839904785
Validation loss: 2.5275484977229947

Epoch: 5| Step: 6
Training loss: 1.7449138164520264
Validation loss: 2.531376543865409

Epoch: 5| Step: 7
Training loss: 2.929474353790283
Validation loss: 2.5286802271360993

Epoch: 5| Step: 8
Training loss: 2.906529188156128
Validation loss: 2.5291858488513577

Epoch: 5| Step: 9
Training loss: 2.1786856651306152
Validation loss: 2.522652949056318

Epoch: 5| Step: 10
Training loss: 2.631028652191162
Validation loss: 2.5221212884431243

Epoch: 144| Step: 0
Training loss: 3.0352702140808105
Validation loss: 2.5184783576637186

Epoch: 5| Step: 1
Training loss: 1.5174640417099
Validation loss: 2.526087996780231

Epoch: 5| Step: 2
Training loss: 2.723194122314453
Validation loss: 2.533616932489539

Epoch: 5| Step: 3
Training loss: 2.8158206939697266
Validation loss: 2.5365717513586885

Epoch: 5| Step: 4
Training loss: 3.2347500324249268
Validation loss: 2.5259634935727684

Epoch: 5| Step: 5
Training loss: 2.2797389030456543
Validation loss: 2.526468558978009

Epoch: 5| Step: 6
Training loss: 2.454091787338257
Validation loss: 2.5232845096177954

Epoch: 5| Step: 7
Training loss: 3.1969692707061768
Validation loss: 2.516934828091693

Epoch: 5| Step: 8
Training loss: 3.3302643299102783
Validation loss: 2.5149320863908335

Epoch: 5| Step: 9
Training loss: 2.304175853729248
Validation loss: 2.510659617762412

Epoch: 5| Step: 10
Training loss: 2.8282577991485596
Validation loss: 2.5186385531579294

Epoch: 145| Step: 0
Training loss: 2.0177409648895264
Validation loss: 2.511709492693665

Epoch: 5| Step: 1
Training loss: 2.7569026947021484
Validation loss: 2.5160640388406734

Epoch: 5| Step: 2
Training loss: 3.009864330291748
Validation loss: 2.517083480793943

Epoch: 5| Step: 3
Training loss: 3.0888164043426514
Validation loss: 2.5165242456620738

Epoch: 5| Step: 4
Training loss: 2.957854747772217
Validation loss: 2.5223055321683168

Epoch: 5| Step: 5
Training loss: 2.548698902130127
Validation loss: 2.5185036556695097

Epoch: 5| Step: 6
Training loss: 2.2505807876586914
Validation loss: 2.516782855474821

Epoch: 5| Step: 7
Training loss: 2.659771203994751
Validation loss: 2.5150743351187757

Epoch: 5| Step: 8
Training loss: 2.6589174270629883
Validation loss: 2.5103720670105307

Epoch: 5| Step: 9
Training loss: 2.930662155151367
Validation loss: 2.5129149575387277

Epoch: 5| Step: 10
Training loss: 2.8174080848693848
Validation loss: 2.514138057667722

Epoch: 146| Step: 0
Training loss: 2.6186182498931885
Validation loss: 2.51045209874389

Epoch: 5| Step: 1
Training loss: 3.389230251312256
Validation loss: 2.5136869774069837

Epoch: 5| Step: 2
Training loss: 2.9550669193267822
Validation loss: 2.512717373909489

Epoch: 5| Step: 3
Training loss: 3.016784191131592
Validation loss: 2.521908744688957

Epoch: 5| Step: 4
Training loss: 2.098132610321045
Validation loss: 2.5212525757410194

Epoch: 5| Step: 5
Training loss: 2.4095330238342285
Validation loss: 2.517411498613255

Epoch: 5| Step: 6
Training loss: 1.9360120296478271
Validation loss: 2.518352949491111

Epoch: 5| Step: 7
Training loss: 2.8214097023010254
Validation loss: 2.5127251584042787

Epoch: 5| Step: 8
Training loss: 2.3160595893859863
Validation loss: 2.5133906231131604

Epoch: 5| Step: 9
Training loss: 2.8551931381225586
Validation loss: 2.5117490317231868

Epoch: 5| Step: 10
Training loss: 3.332334280014038
Validation loss: 2.522137831616145

Epoch: 147| Step: 0
Training loss: 3.166100025177002
Validation loss: 2.525389802071356

Epoch: 5| Step: 1
Training loss: 3.213400363922119
Validation loss: 2.533289001834008

Epoch: 5| Step: 2
Training loss: 2.406451463699341
Validation loss: 2.5345357233478176

Epoch: 5| Step: 3
Training loss: 2.514683485031128
Validation loss: 2.5307567657962924

Epoch: 5| Step: 4
Training loss: 2.9063708782196045
Validation loss: 2.5333752709050334

Epoch: 5| Step: 5
Training loss: 2.5032637119293213
Validation loss: 2.522232417137392

Epoch: 5| Step: 6
Training loss: 3.012234926223755
Validation loss: 2.5178591769228698

Epoch: 5| Step: 7
Training loss: 1.6229711771011353
Validation loss: 2.5159231667877524

Epoch: 5| Step: 8
Training loss: 2.6625723838806152
Validation loss: 2.512289870169855

Epoch: 5| Step: 9
Training loss: 3.361614942550659
Validation loss: 2.515054161830615

Epoch: 5| Step: 10
Training loss: 2.335233211517334
Validation loss: 2.5133449954371296

Epoch: 148| Step: 0
Training loss: 2.806457042694092
Validation loss: 2.5132078355358494

Epoch: 5| Step: 1
Training loss: 3.4958808422088623
Validation loss: 2.5168366714190413

Epoch: 5| Step: 2
Training loss: 2.5586206912994385
Validation loss: 2.519884660679807

Epoch: 5| Step: 3
Training loss: 2.77384614944458
Validation loss: 2.521877319582047

Epoch: 5| Step: 4
Training loss: 2.919299602508545
Validation loss: 2.530471404393514

Epoch: 5| Step: 5
Training loss: 3.1433937549591064
Validation loss: 2.539162992149271

Epoch: 5| Step: 6
Training loss: 2.5273470878601074
Validation loss: 2.5284602718968547

Epoch: 5| Step: 7
Training loss: 2.8142380714416504
Validation loss: 2.5382187404940204

Epoch: 5| Step: 8
Training loss: 2.463271379470825
Validation loss: 2.518142415631202

Epoch: 5| Step: 9
Training loss: 2.4347550868988037
Validation loss: 2.52087414392861

Epoch: 5| Step: 10
Training loss: 1.5633026361465454
Validation loss: 2.5161085949149182

Epoch: 149| Step: 0
Training loss: 2.6030850410461426
Validation loss: 2.508831993226082

Epoch: 5| Step: 1
Training loss: 3.1085803508758545
Validation loss: 2.505457483312135

Epoch: 5| Step: 2
Training loss: 2.9459712505340576
Validation loss: 2.506514767164825

Epoch: 5| Step: 3
Training loss: 3.3129050731658936
Validation loss: 2.5061887207851616

Epoch: 5| Step: 4
Training loss: 2.9180409908294678
Validation loss: 2.505058883338846

Epoch: 5| Step: 5
Training loss: 2.0934996604919434
Validation loss: 2.506994119254492

Epoch: 5| Step: 6
Training loss: 3.299938917160034
Validation loss: 2.5075787446832143

Epoch: 5| Step: 7
Training loss: 2.795525074005127
Validation loss: 2.5063065354542067

Epoch: 5| Step: 8
Training loss: 2.647667407989502
Validation loss: 2.5092932844674714

Epoch: 5| Step: 9
Training loss: 2.0166521072387695
Validation loss: 2.5124754828791462

Epoch: 5| Step: 10
Training loss: 1.7125693559646606
Validation loss: 2.5135440185505855

Epoch: 150| Step: 0
Training loss: 2.874826669692993
Validation loss: 2.513676489553144

Epoch: 5| Step: 1
Training loss: 2.0940191745758057
Validation loss: 2.5172465385929232

Epoch: 5| Step: 2
Training loss: 3.5870022773742676
Validation loss: 2.5248085324482252

Epoch: 5| Step: 3
Training loss: 2.9800643920898438
Validation loss: 2.519793297654839

Epoch: 5| Step: 4
Training loss: 2.7608320713043213
Validation loss: 2.5215469944861626

Epoch: 5| Step: 5
Training loss: 2.991438627243042
Validation loss: 2.515986914275795

Epoch: 5| Step: 6
Training loss: 2.686849355697632
Validation loss: 2.5157446733085056

Epoch: 5| Step: 7
Training loss: 2.2870373725891113
Validation loss: 2.5108826826977473

Epoch: 5| Step: 8
Training loss: 2.416494369506836
Validation loss: 2.510602105048395

Epoch: 5| Step: 9
Training loss: 2.36617374420166
Validation loss: 2.5114556512525006

Epoch: 5| Step: 10
Training loss: 2.597522497177124
Validation loss: 2.510273318136892

Epoch: 151| Step: 0
Training loss: 2.472778081893921
Validation loss: 2.505932697685816

Epoch: 5| Step: 1
Training loss: 2.5952563285827637
Validation loss: 2.507669212997601

Epoch: 5| Step: 2
Training loss: 2.866234540939331
Validation loss: 2.504538884726904

Epoch: 5| Step: 3
Training loss: 2.701756477355957
Validation loss: 2.5057630846577306

Epoch: 5| Step: 4
Training loss: 1.9953540563583374
Validation loss: 2.5089967840461322

Epoch: 5| Step: 5
Training loss: 2.883603811264038
Validation loss: 2.504949362047257

Epoch: 5| Step: 6
Training loss: 3.5693752765655518
Validation loss: 2.508104085922241

Epoch: 5| Step: 7
Training loss: 2.0003981590270996
Validation loss: 2.5066768200166765

Epoch: 5| Step: 8
Training loss: 2.6111338138580322
Validation loss: 2.511174048146894

Epoch: 5| Step: 9
Training loss: 2.5970466136932373
Validation loss: 2.520181204683037

Epoch: 5| Step: 10
Training loss: 3.3770651817321777
Validation loss: 2.515297146253688

Epoch: 152| Step: 0
Training loss: 2.065403461456299
Validation loss: 2.506970582469817

Epoch: 5| Step: 1
Training loss: 3.3608994483947754
Validation loss: 2.507682741329234

Epoch: 5| Step: 2
Training loss: 2.189748764038086
Validation loss: 2.505166830555085

Epoch: 5| Step: 3
Training loss: 2.6321113109588623
Validation loss: 2.5047666334336802

Epoch: 5| Step: 4
Training loss: 2.898559808731079
Validation loss: 2.5025795070073937

Epoch: 5| Step: 5
Training loss: 2.867757558822632
Validation loss: 2.503950188236852

Epoch: 5| Step: 6
Training loss: 2.4830918312072754
Validation loss: 2.500924451376802

Epoch: 5| Step: 7
Training loss: 2.3144690990448
Validation loss: 2.5026129548267653

Epoch: 5| Step: 8
Training loss: 2.5050582885742188
Validation loss: 2.501768112182617

Epoch: 5| Step: 9
Training loss: 2.6328344345092773
Validation loss: 2.5070879228653444

Epoch: 5| Step: 10
Training loss: 3.795469045639038
Validation loss: 2.5046200572803454

Epoch: 153| Step: 0
Training loss: 2.1336240768432617
Validation loss: 2.506868136826382

Epoch: 5| Step: 1
Training loss: 2.546125888824463
Validation loss: 2.5115554948006906

Epoch: 5| Step: 2
Training loss: 3.500258684158325
Validation loss: 2.5105012104075444

Epoch: 5| Step: 3
Training loss: 2.7480552196502686
Validation loss: 2.508201814466907

Epoch: 5| Step: 4
Training loss: 3.2189993858337402
Validation loss: 2.5086765161124607

Epoch: 5| Step: 5
Training loss: 2.2766714096069336
Validation loss: 2.508025002735917

Epoch: 5| Step: 6
Training loss: 3.017197370529175
Validation loss: 2.512135439021613

Epoch: 5| Step: 7
Training loss: 2.04996919631958
Validation loss: 2.507143794849355

Epoch: 5| Step: 8
Training loss: 2.276567220687866
Validation loss: 2.5089656499124344

Epoch: 5| Step: 9
Training loss: 3.0923380851745605
Validation loss: 2.5076973566444973

Epoch: 5| Step: 10
Training loss: 2.7680904865264893
Validation loss: 2.5029176588981383

Epoch: 154| Step: 0
Training loss: 2.1904795169830322
Validation loss: 2.5020504074711956

Epoch: 5| Step: 1
Training loss: 3.3092637062072754
Validation loss: 2.5021502253829793

Epoch: 5| Step: 2
Training loss: 2.59787917137146
Validation loss: 2.4996849516386628

Epoch: 5| Step: 3
Training loss: 3.2949612140655518
Validation loss: 2.5002547566608717

Epoch: 5| Step: 4
Training loss: 2.1028695106506348
Validation loss: 2.5045182653652724

Epoch: 5| Step: 5
Training loss: 2.5225162506103516
Validation loss: 2.508234320148345

Epoch: 5| Step: 6
Training loss: 2.693362236022949
Validation loss: 2.5092956173804497

Epoch: 5| Step: 7
Training loss: 2.798175096511841
Validation loss: 2.5089734087708178

Epoch: 5| Step: 8
Training loss: 2.9194607734680176
Validation loss: 2.5075856306219615

Epoch: 5| Step: 9
Training loss: 2.613896369934082
Validation loss: 2.5075458711193455

Epoch: 5| Step: 10
Training loss: 2.4240241050720215
Validation loss: 2.5066114574350338

Epoch: 155| Step: 0
Training loss: 3.026376485824585
Validation loss: 2.502178148556781

Epoch: 5| Step: 1
Training loss: 2.4451496601104736
Validation loss: 2.5022839833331365

Epoch: 5| Step: 2
Training loss: 2.588367462158203
Validation loss: 2.5033050121799594

Epoch: 5| Step: 3
Training loss: 2.1862852573394775
Validation loss: 2.504874029467183

Epoch: 5| Step: 4
Training loss: 2.4127516746520996
Validation loss: 2.498594614767259

Epoch: 5| Step: 5
Training loss: 2.4596896171569824
Validation loss: 2.498245526385564

Epoch: 5| Step: 6
Training loss: 2.5659167766571045
Validation loss: 2.4985429394629692

Epoch: 5| Step: 7
Training loss: 2.444549083709717
Validation loss: 2.50142301026211

Epoch: 5| Step: 8
Training loss: 3.1722865104675293
Validation loss: 2.5034154538185365

Epoch: 5| Step: 9
Training loss: 2.942213773727417
Validation loss: 2.49955718235303

Epoch: 5| Step: 10
Training loss: 3.3154520988464355
Validation loss: 2.4985987960651355

Epoch: 156| Step: 0
Training loss: 2.9390931129455566
Validation loss: 2.5036620017020934

Epoch: 5| Step: 1
Training loss: 2.523609161376953
Validation loss: 2.506640967502389

Epoch: 5| Step: 2
Training loss: 3.121389389038086
Validation loss: 2.500423203232468

Epoch: 5| Step: 3
Training loss: 2.4017646312713623
Validation loss: 2.5024367609331684

Epoch: 5| Step: 4
Training loss: 2.2489891052246094
Validation loss: 2.50360510938911

Epoch: 5| Step: 5
Training loss: 2.5761308670043945
Validation loss: 2.506584641753986

Epoch: 5| Step: 6
Training loss: 2.81931734085083
Validation loss: 2.5065410214085735

Epoch: 5| Step: 7
Training loss: 3.220576047897339
Validation loss: 2.5011109703330585

Epoch: 5| Step: 8
Training loss: 2.4503958225250244
Validation loss: 2.5030585412056214

Epoch: 5| Step: 9
Training loss: 2.6944704055786133
Validation loss: 2.504327330538022

Epoch: 5| Step: 10
Training loss: 2.45676589012146
Validation loss: 2.5045833203100387

Epoch: 157| Step: 0
Training loss: 2.780074119567871
Validation loss: 2.5089701042380383

Epoch: 5| Step: 1
Training loss: 2.1456758975982666
Validation loss: 2.509827975303896

Epoch: 5| Step: 2
Training loss: 1.9745975732803345
Validation loss: 2.508034513842675

Epoch: 5| Step: 3
Training loss: 2.524383544921875
Validation loss: 2.50094937252742

Epoch: 5| Step: 4
Training loss: 2.823486804962158
Validation loss: 2.502601162079842

Epoch: 5| Step: 5
Training loss: 2.4635424613952637
Validation loss: 2.500524277328163

Epoch: 5| Step: 6
Training loss: 3.0295379161834717
Validation loss: 2.4945680915668444

Epoch: 5| Step: 7
Training loss: 2.2102198600769043
Validation loss: 2.4982256427887948

Epoch: 5| Step: 8
Training loss: 3.092485189437866
Validation loss: 2.495861884086363

Epoch: 5| Step: 9
Training loss: 3.0261356830596924
Validation loss: 2.4994346300760903

Epoch: 5| Step: 10
Training loss: 3.45501708984375
Validation loss: 2.5056497435415945

Epoch: 158| Step: 0
Training loss: 3.37943696975708
Validation loss: 2.500602019730435

Epoch: 5| Step: 1
Training loss: 2.330469846725464
Validation loss: 2.498471562580396

Epoch: 5| Step: 2
Training loss: 2.173447370529175
Validation loss: 2.500377097437459

Epoch: 5| Step: 3
Training loss: 2.408872604370117
Validation loss: 2.501674149626045

Epoch: 5| Step: 4
Training loss: 2.229435443878174
Validation loss: 2.494201232028264

Epoch: 5| Step: 5
Training loss: 2.9028773307800293
Validation loss: 2.5089717295862015

Epoch: 5| Step: 6
Training loss: 2.713970899581909
Validation loss: 2.4963396569733978

Epoch: 5| Step: 7
Training loss: 2.427849769592285
Validation loss: 2.4989771637865292

Epoch: 5| Step: 8
Training loss: 2.8682608604431152
Validation loss: 2.500674242614418

Epoch: 5| Step: 9
Training loss: 3.0333385467529297
Validation loss: 2.500353782407699

Epoch: 5| Step: 10
Training loss: 3.0421886444091797
Validation loss: 2.504821308197514

Epoch: 159| Step: 0
Training loss: 2.382157802581787
Validation loss: 2.5068337225144908

Epoch: 5| Step: 1
Training loss: 2.358186721801758
Validation loss: 2.504103873365669

Epoch: 5| Step: 2
Training loss: 2.615687370300293
Validation loss: 2.507576838616402

Epoch: 5| Step: 3
Training loss: 2.702227830886841
Validation loss: 2.5025181334505797

Epoch: 5| Step: 4
Training loss: 2.484846830368042
Validation loss: 2.508558065660538

Epoch: 5| Step: 5
Training loss: 2.8697104454040527
Validation loss: 2.495483613783313

Epoch: 5| Step: 6
Training loss: 2.3602261543273926
Validation loss: 2.489011956799415

Epoch: 5| Step: 7
Training loss: 2.879552125930786
Validation loss: 2.491153060749013

Epoch: 5| Step: 8
Training loss: 3.3034889698028564
Validation loss: 2.49435765768892

Epoch: 5| Step: 9
Training loss: 2.682446241378784
Validation loss: 2.4958405827963226

Epoch: 5| Step: 10
Training loss: 2.836416721343994
Validation loss: 2.5006144277511106

Epoch: 160| Step: 0
Training loss: 2.9740748405456543
Validation loss: 2.5060894796925206

Epoch: 5| Step: 1
Training loss: 2.764807939529419
Validation loss: 2.512524079251033

Epoch: 5| Step: 2
Training loss: 2.5517382621765137
Validation loss: 2.508668182998575

Epoch: 5| Step: 3
Training loss: 2.7653467655181885
Validation loss: 2.5202364844660603

Epoch: 5| Step: 4
Training loss: 2.233384609222412
Validation loss: 2.5192168707488687

Epoch: 5| Step: 5
Training loss: 2.436527729034424
Validation loss: 2.51240933966893

Epoch: 5| Step: 6
Training loss: 2.4068312644958496
Validation loss: 2.4988166593736216

Epoch: 5| Step: 7
Training loss: 2.9784374237060547
Validation loss: 2.498358726501465

Epoch: 5| Step: 8
Training loss: 2.2652010917663574
Validation loss: 2.486870837467973

Epoch: 5| Step: 9
Training loss: 3.191953659057617
Validation loss: 2.4850493118327153

Epoch: 5| Step: 10
Training loss: 3.05550479888916
Validation loss: 2.493365628744966

Epoch: 161| Step: 0
Training loss: 1.8954635858535767
Validation loss: 2.4961217757194274

Epoch: 5| Step: 1
Training loss: 2.765254497528076
Validation loss: 2.4957585437323457

Epoch: 5| Step: 2
Training loss: 2.676649570465088
Validation loss: 2.502221530483615

Epoch: 5| Step: 3
Training loss: 2.8453121185302734
Validation loss: 2.5131342154677196

Epoch: 5| Step: 4
Training loss: 2.317239284515381
Validation loss: 2.503410921301893

Epoch: 5| Step: 5
Training loss: 2.7168774604797363
Validation loss: 2.498944333804551

Epoch: 5| Step: 6
Training loss: 3.154198408126831
Validation loss: 2.4950105708132506

Epoch: 5| Step: 7
Training loss: 3.32118558883667
Validation loss: 2.4895542360121206

Epoch: 5| Step: 8
Training loss: 2.5244088172912598
Validation loss: 2.4908392454988215

Epoch: 5| Step: 9
Training loss: 2.70573353767395
Validation loss: 2.4807590874292518

Epoch: 5| Step: 10
Training loss: 2.6680476665496826
Validation loss: 2.4866753931968444

Epoch: 162| Step: 0
Training loss: 2.7490782737731934
Validation loss: 2.489058212567401

Epoch: 5| Step: 1
Training loss: 3.028717041015625
Validation loss: 2.4904121711689937

Epoch: 5| Step: 2
Training loss: 3.172718048095703
Validation loss: 2.495571490257017

Epoch: 5| Step: 3
Training loss: 2.3694958686828613
Validation loss: 2.5004330655579925

Epoch: 5| Step: 4
Training loss: 2.6363964080810547
Validation loss: 2.499739436693089

Epoch: 5| Step: 5
Training loss: 2.64153790473938
Validation loss: 2.4990520579840547

Epoch: 5| Step: 6
Training loss: 2.9183359146118164
Validation loss: 2.500162115661047

Epoch: 5| Step: 7
Training loss: 2.4628140926361084
Validation loss: 2.505291069707563

Epoch: 5| Step: 8
Training loss: 2.5540878772735596
Validation loss: 2.501604394246173

Epoch: 5| Step: 9
Training loss: 2.5723519325256348
Validation loss: 2.503389737939322

Epoch: 5| Step: 10
Training loss: 2.3651328086853027
Validation loss: 2.4969062343720467

Epoch: 163| Step: 0
Training loss: 2.8073811531066895
Validation loss: 2.495864739982031

Epoch: 5| Step: 1
Training loss: 2.0028674602508545
Validation loss: 2.4943855090807845

Epoch: 5| Step: 2
Training loss: 2.99674391746521
Validation loss: 2.4862763599682878

Epoch: 5| Step: 3
Training loss: 2.1075520515441895
Validation loss: 2.492424967468426

Epoch: 5| Step: 4
Training loss: 2.576510190963745
Validation loss: 2.4919696700188423

Epoch: 5| Step: 5
Training loss: 2.604687213897705
Validation loss: 2.4891001152735885

Epoch: 5| Step: 6
Training loss: 2.5002732276916504
Validation loss: 2.4890780038731073

Epoch: 5| Step: 7
Training loss: 3.1693778038024902
Validation loss: 2.4899999146820395

Epoch: 5| Step: 8
Training loss: 3.062502145767212
Validation loss: 2.4897018504399124

Epoch: 5| Step: 9
Training loss: 3.2982630729675293
Validation loss: 2.488288200029763

Epoch: 5| Step: 10
Training loss: 2.1928324699401855
Validation loss: 2.4873723291581675

Epoch: 164| Step: 0
Training loss: 2.724881172180176
Validation loss: 2.493671514654672

Epoch: 5| Step: 1
Training loss: 1.6236231327056885
Validation loss: 2.4914502559169645

Epoch: 5| Step: 2
Training loss: 2.9193859100341797
Validation loss: 2.4930301943132953

Epoch: 5| Step: 3
Training loss: 2.315624713897705
Validation loss: 2.4951381990986485

Epoch: 5| Step: 4
Training loss: 3.008181095123291
Validation loss: 2.5019597109927925

Epoch: 5| Step: 5
Training loss: 3.0562827587127686
Validation loss: 2.5011024834007345

Epoch: 5| Step: 6
Training loss: 3.1307129859924316
Validation loss: 2.502199057609804

Epoch: 5| Step: 7
Training loss: 2.2752082347869873
Validation loss: 2.5132058666598414

Epoch: 5| Step: 8
Training loss: 2.6317367553710938
Validation loss: 2.506245169588315

Epoch: 5| Step: 9
Training loss: 2.5541300773620605
Validation loss: 2.5086115816588044

Epoch: 5| Step: 10
Training loss: 3.2622811794281006
Validation loss: 2.51140917757506

Epoch: 165| Step: 0
Training loss: 2.000990390777588
Validation loss: 2.508284238076979

Epoch: 5| Step: 1
Training loss: 2.7604947090148926
Validation loss: 2.5037118581033524

Epoch: 5| Step: 2
Training loss: 2.3649086952209473
Validation loss: 2.5011359876202

Epoch: 5| Step: 3
Training loss: 2.5366721153259277
Validation loss: 2.5036811162066717

Epoch: 5| Step: 4
Training loss: 2.905524969100952
Validation loss: 2.506161089866392

Epoch: 5| Step: 5
Training loss: 2.6834170818328857
Validation loss: 2.500934877703267

Epoch: 5| Step: 6
Training loss: 2.5408949851989746
Validation loss: 2.498338855722899

Epoch: 5| Step: 7
Training loss: 3.0131561756134033
Validation loss: 2.504040066913892

Epoch: 5| Step: 8
Training loss: 3.1537041664123535
Validation loss: 2.50232333008961

Epoch: 5| Step: 9
Training loss: 3.006314516067505
Validation loss: 2.4942359514133905

Epoch: 5| Step: 10
Training loss: 2.375495195388794
Validation loss: 2.5060223763988865

Epoch: 166| Step: 0
Training loss: 2.4080560207366943
Validation loss: 2.4985008444837344

Epoch: 5| Step: 1
Training loss: 3.526911973953247
Validation loss: 2.5068275159405125

Epoch: 5| Step: 2
Training loss: 2.005155324935913
Validation loss: 2.4944656741234565

Epoch: 5| Step: 3
Training loss: 3.6038193702697754
Validation loss: 2.4980259813288206

Epoch: 5| Step: 4
Training loss: 3.347710371017456
Validation loss: 2.4947071524076563

Epoch: 5| Step: 5
Training loss: 2.4614851474761963
Validation loss: 2.4852850642255557

Epoch: 5| Step: 6
Training loss: 2.5038981437683105
Validation loss: 2.4858007405393865

Epoch: 5| Step: 7
Training loss: 2.0559616088867188
Validation loss: 2.480114347191267

Epoch: 5| Step: 8
Training loss: 3.2674567699432373
Validation loss: 2.4851490400170766

Epoch: 5| Step: 9
Training loss: 1.9372084140777588
Validation loss: 2.483664246015651

Epoch: 5| Step: 10
Training loss: 2.2527122497558594
Validation loss: 2.485966364542643

Epoch: 167| Step: 0
Training loss: 1.741628885269165
Validation loss: 2.4906237843216106

Epoch: 5| Step: 1
Training loss: 2.2477622032165527
Validation loss: 2.489934495700303

Epoch: 5| Step: 2
Training loss: 3.238924741744995
Validation loss: 2.490736981873871

Epoch: 5| Step: 3
Training loss: 2.3984692096710205
Validation loss: 2.500042038579141

Epoch: 5| Step: 4
Training loss: 3.0531859397888184
Validation loss: 2.4988055524005683

Epoch: 5| Step: 5
Training loss: 3.265953779220581
Validation loss: 2.499724083049323

Epoch: 5| Step: 6
Training loss: 2.8607265949249268
Validation loss: 2.500451821152882

Epoch: 5| Step: 7
Training loss: 2.499504804611206
Validation loss: 2.4909177339205177

Epoch: 5| Step: 8
Training loss: 1.9751899242401123
Validation loss: 2.4892205474197224

Epoch: 5| Step: 9
Training loss: 3.0788655281066895
Validation loss: 2.4900269021270094

Epoch: 5| Step: 10
Training loss: 3.0323033332824707
Validation loss: 2.4886494221225863

Epoch: 168| Step: 0
Training loss: 2.8649916648864746
Validation loss: 2.489344660953809

Epoch: 5| Step: 1
Training loss: 2.670637607574463
Validation loss: 2.490166976887693

Epoch: 5| Step: 2
Training loss: 2.996835231781006
Validation loss: 2.4841966295755036

Epoch: 5| Step: 3
Training loss: 2.485769510269165
Validation loss: 2.4891476451709704

Epoch: 5| Step: 4
Training loss: 3.0047333240509033
Validation loss: 2.48957384017206

Epoch: 5| Step: 5
Training loss: 2.319491386413574
Validation loss: 2.4878795813488703

Epoch: 5| Step: 6
Training loss: 2.944944381713867
Validation loss: 2.496409687944638

Epoch: 5| Step: 7
Training loss: 2.5212056636810303
Validation loss: 2.49038952396762

Epoch: 5| Step: 8
Training loss: 2.4680404663085938
Validation loss: 2.4879782943315405

Epoch: 5| Step: 9
Training loss: 2.7158749103546143
Validation loss: 2.4826080055646997

Epoch: 5| Step: 10
Training loss: 2.2448890209198
Validation loss: 2.4904864039472354

Epoch: 169| Step: 0
Training loss: 3.044562816619873
Validation loss: 2.488563058196857

Epoch: 5| Step: 1
Training loss: 2.605498790740967
Validation loss: 2.491296927134196

Epoch: 5| Step: 2
Training loss: 3.3534653186798096
Validation loss: 2.4928543670203096

Epoch: 5| Step: 3
Training loss: 2.3542752265930176
Validation loss: 2.493409620818271

Epoch: 5| Step: 4
Training loss: 3.0199859142303467
Validation loss: 2.5008999942451395

Epoch: 5| Step: 5
Training loss: 2.7883243560791016
Validation loss: 2.5145717333721858

Epoch: 5| Step: 6
Training loss: 2.1766300201416016
Validation loss: 2.5243311902528167

Epoch: 5| Step: 7
Training loss: 2.208892345428467
Validation loss: 2.5165406914167505

Epoch: 5| Step: 8
Training loss: 2.7241389751434326
Validation loss: 2.5192365005452144

Epoch: 5| Step: 9
Training loss: 2.8176751136779785
Validation loss: 2.5154991944630942

Epoch: 5| Step: 10
Training loss: 2.258880853652954
Validation loss: 2.4978930386163856

Epoch: 170| Step: 0
Training loss: 2.889967441558838
Validation loss: 2.4924502629105763

Epoch: 5| Step: 1
Training loss: 2.5969207286834717
Validation loss: 2.4920518808467413

Epoch: 5| Step: 2
Training loss: 3.1932835578918457
Validation loss: 2.4877406781719578

Epoch: 5| Step: 3
Training loss: 2.6796340942382812
Validation loss: 2.481246458586826

Epoch: 5| Step: 4
Training loss: 1.7425121068954468
Validation loss: 2.4880802759560208

Epoch: 5| Step: 5
Training loss: 2.563964366912842
Validation loss: 2.4876601721650813

Epoch: 5| Step: 6
Training loss: 2.360330581665039
Validation loss: 2.4868732806174987

Epoch: 5| Step: 7
Training loss: 3.4769020080566406
Validation loss: 2.4860427200153308

Epoch: 5| Step: 8
Training loss: 2.701590061187744
Validation loss: 2.4939876551269204

Epoch: 5| Step: 9
Training loss: 2.3749783039093018
Validation loss: 2.4830687840779624

Epoch: 5| Step: 10
Training loss: 2.7698915004730225
Validation loss: 2.4860552075088664

Epoch: 171| Step: 0
Training loss: 3.17734694480896
Validation loss: 2.479513309335196

Epoch: 5| Step: 1
Training loss: 2.7014317512512207
Validation loss: 2.4827857684063654

Epoch: 5| Step: 2
Training loss: 2.2974467277526855
Validation loss: 2.477181488467801

Epoch: 5| Step: 3
Training loss: 2.263458251953125
Validation loss: 2.472383112035772

Epoch: 5| Step: 4
Training loss: 2.0158324241638184
Validation loss: 2.478975162711195

Epoch: 5| Step: 5
Training loss: 2.5339395999908447
Validation loss: 2.4848610816463346

Epoch: 5| Step: 6
Training loss: 2.3387844562530518
Validation loss: 2.4830932873551563

Epoch: 5| Step: 7
Training loss: 2.7122926712036133
Validation loss: 2.491416661970077

Epoch: 5| Step: 8
Training loss: 3.264866352081299
Validation loss: 2.502695099000008

Epoch: 5| Step: 9
Training loss: 3.2689082622528076
Validation loss: 2.5051223334445747

Epoch: 5| Step: 10
Training loss: 2.806500196456909
Validation loss: 2.498291254043579

Epoch: 172| Step: 0
Training loss: 2.5728039741516113
Validation loss: 2.501844136945663

Epoch: 5| Step: 1
Training loss: 2.3785223960876465
Validation loss: 2.4978273350705384

Epoch: 5| Step: 2
Training loss: 2.8411104679107666
Validation loss: 2.4929887863897506

Epoch: 5| Step: 3
Training loss: 2.6050219535827637
Validation loss: 2.5000243674042406

Epoch: 5| Step: 4
Training loss: 2.4165616035461426
Validation loss: 2.495293145538658

Epoch: 5| Step: 5
Training loss: 3.2531402111053467
Validation loss: 2.490827798843384

Epoch: 5| Step: 6
Training loss: 2.8171353340148926
Validation loss: 2.4928362343900945

Epoch: 5| Step: 7
Training loss: 2.422227621078491
Validation loss: 2.485095031799809

Epoch: 5| Step: 8
Training loss: 3.0332202911376953
Validation loss: 2.4905001348064792

Epoch: 5| Step: 9
Training loss: 2.4466800689697266
Validation loss: 2.4835448188166462

Epoch: 5| Step: 10
Training loss: 2.6219911575317383
Validation loss: 2.4840915305640108

Epoch: 173| Step: 0
Training loss: 2.848583221435547
Validation loss: 2.4846881281945015

Epoch: 5| Step: 1
Training loss: 2.334646701812744
Validation loss: 2.490449018375848

Epoch: 5| Step: 2
Training loss: 2.6536800861358643
Validation loss: 2.4893063729809177

Epoch: 5| Step: 3
Training loss: 2.469860315322876
Validation loss: 2.4740435256752917

Epoch: 5| Step: 4
Training loss: 2.8821191787719727
Validation loss: 2.4717534972775366

Epoch: 5| Step: 5
Training loss: 2.4875049591064453
Validation loss: 2.4706429230269564

Epoch: 5| Step: 6
Training loss: 2.7612555027008057
Validation loss: 2.48061817179444

Epoch: 5| Step: 7
Training loss: 3.2981998920440674
Validation loss: 2.4757994298012025

Epoch: 5| Step: 8
Training loss: 2.498886823654175
Validation loss: 2.477407629771899

Epoch: 5| Step: 9
Training loss: 2.401261806488037
Validation loss: 2.478209169962073

Epoch: 5| Step: 10
Training loss: 2.769387722015381
Validation loss: 2.4775753815968833

Epoch: 174| Step: 0
Training loss: 2.2198121547698975
Validation loss: 2.4768693075385144

Epoch: 5| Step: 1
Training loss: 2.2642345428466797
Validation loss: 2.4728795033629223

Epoch: 5| Step: 2
Training loss: 2.4341280460357666
Validation loss: 2.474606583195348

Epoch: 5| Step: 3
Training loss: 2.237349033355713
Validation loss: 2.4710475706285044

Epoch: 5| Step: 4
Training loss: 2.643141508102417
Validation loss: 2.476916872045045

Epoch: 5| Step: 5
Training loss: 2.911576509475708
Validation loss: 2.475223268232038

Epoch: 5| Step: 6
Training loss: 3.132612705230713
Validation loss: 2.476432405492311

Epoch: 5| Step: 7
Training loss: 3.0614330768585205
Validation loss: 2.472051123137115

Epoch: 5| Step: 8
Training loss: 2.4823365211486816
Validation loss: 2.477165505450259

Epoch: 5| Step: 9
Training loss: 2.7638721466064453
Validation loss: 2.476606781764697

Epoch: 5| Step: 10
Training loss: 3.210650682449341
Validation loss: 2.478111569599439

Epoch: 175| Step: 0
Training loss: 2.3165736198425293
Validation loss: 2.4840187718791347

Epoch: 5| Step: 1
Training loss: 3.186654806137085
Validation loss: 2.4865841468175254

Epoch: 5| Step: 2
Training loss: 2.152284622192383
Validation loss: 2.4802087327485443

Epoch: 5| Step: 3
Training loss: 2.9871249198913574
Validation loss: 2.4794480903174287

Epoch: 5| Step: 4
Training loss: 2.5870211124420166
Validation loss: 2.478347927011469

Epoch: 5| Step: 5
Training loss: 2.7419769763946533
Validation loss: 2.4758958944710354

Epoch: 5| Step: 6
Training loss: 2.8391265869140625
Validation loss: 2.4757010603463776

Epoch: 5| Step: 7
Training loss: 2.577197790145874
Validation loss: 2.467844699018745

Epoch: 5| Step: 8
Training loss: 3.193005084991455
Validation loss: 2.468831149480676

Epoch: 5| Step: 9
Training loss: 2.415903091430664
Validation loss: 2.4700266699637137

Epoch: 5| Step: 10
Training loss: 2.307713031768799
Validation loss: 2.469764368508452

Epoch: 176| Step: 0
Training loss: 3.0423271656036377
Validation loss: 2.469007376701601

Epoch: 5| Step: 1
Training loss: 3.4152932167053223
Validation loss: 2.4652897773250455

Epoch: 5| Step: 2
Training loss: 2.2880988121032715
Validation loss: 2.473042180461268

Epoch: 5| Step: 3
Training loss: 2.0878615379333496
Validation loss: 2.474387184266121

Epoch: 5| Step: 4
Training loss: 2.689842939376831
Validation loss: 2.4763127373110865

Epoch: 5| Step: 5
Training loss: 2.046252727508545
Validation loss: 2.469751109359085

Epoch: 5| Step: 6
Training loss: 2.897667407989502
Validation loss: 2.4705299587659937

Epoch: 5| Step: 7
Training loss: 2.8125901222229004
Validation loss: 2.4729846651836107

Epoch: 5| Step: 8
Training loss: 2.6531362533569336
Validation loss: 2.4797124375579176

Epoch: 5| Step: 9
Training loss: 2.7442879676818848
Validation loss: 2.477828792346421

Epoch: 5| Step: 10
Training loss: 2.602790355682373
Validation loss: 2.4832023164277435

Epoch: 177| Step: 0
Training loss: 3.1508469581604004
Validation loss: 2.4816391032229186

Epoch: 5| Step: 1
Training loss: 2.538433790206909
Validation loss: 2.484212290856146

Epoch: 5| Step: 2
Training loss: 2.512291669845581
Validation loss: 2.4817830824082896

Epoch: 5| Step: 3
Training loss: 2.973574161529541
Validation loss: 2.4840436776479087

Epoch: 5| Step: 4
Training loss: 2.5801291465759277
Validation loss: 2.4797212795544694

Epoch: 5| Step: 5
Training loss: 2.470690965652466
Validation loss: 2.4727234430210565

Epoch: 5| Step: 6
Training loss: 2.388659715652466
Validation loss: 2.465214885691161

Epoch: 5| Step: 7
Training loss: 2.9097981452941895
Validation loss: 2.468574413689234

Epoch: 5| Step: 8
Training loss: 2.5911529064178467
Validation loss: 2.470559891834054

Epoch: 5| Step: 9
Training loss: 2.8364264965057373
Validation loss: 2.4733581876242035

Epoch: 5| Step: 10
Training loss: 2.3274943828582764
Validation loss: 2.4784811158334055

Epoch: 178| Step: 0
Training loss: 1.7086464166641235
Validation loss: 2.481475455786592

Epoch: 5| Step: 1
Training loss: 3.166470766067505
Validation loss: 2.48166044809485

Epoch: 5| Step: 2
Training loss: 2.2687041759490967
Validation loss: 2.491167473536666

Epoch: 5| Step: 3
Training loss: 2.617168426513672
Validation loss: 2.4939718502824024

Epoch: 5| Step: 4
Training loss: 3.0144200325012207
Validation loss: 2.500056302675637

Epoch: 5| Step: 5
Training loss: 3.380798816680908
Validation loss: 2.4888768708834084

Epoch: 5| Step: 6
Training loss: 3.211333751678467
Validation loss: 2.4841551216699744

Epoch: 5| Step: 7
Training loss: 3.0970988273620605
Validation loss: 2.4770298337423675

Epoch: 5| Step: 8
Training loss: 2.3658149242401123
Validation loss: 2.4704219705315045

Epoch: 5| Step: 9
Training loss: 2.1682965755462646
Validation loss: 2.4760762363351803

Epoch: 5| Step: 10
Training loss: 2.2255260944366455
Validation loss: 2.470734207860885

Epoch: 179| Step: 0
Training loss: 2.9834461212158203
Validation loss: 2.4796470647217124

Epoch: 5| Step: 1
Training loss: 2.5996499061584473
Validation loss: 2.4794203312166276

Epoch: 5| Step: 2
Training loss: 3.1717591285705566
Validation loss: 2.486451182314145

Epoch: 5| Step: 3
Training loss: 2.917482376098633
Validation loss: 2.4792423966110393

Epoch: 5| Step: 4
Training loss: 2.9109930992126465
Validation loss: 2.48091523878036

Epoch: 5| Step: 5
Training loss: 1.818730115890503
Validation loss: 2.48278025914264

Epoch: 5| Step: 6
Training loss: 2.9194748401641846
Validation loss: 2.485950372552359

Epoch: 5| Step: 7
Training loss: 3.3097891807556152
Validation loss: 2.4805257294767644

Epoch: 5| Step: 8
Training loss: 2.528303861618042
Validation loss: 2.4722896622073267

Epoch: 5| Step: 9
Training loss: 1.8910558223724365
Validation loss: 2.471346245017103

Epoch: 5| Step: 10
Training loss: 2.1857380867004395
Validation loss: 2.467902188659996

Epoch: 180| Step: 0
Training loss: 1.9398285150527954
Validation loss: 2.4737158078019337

Epoch: 5| Step: 1
Training loss: 3.1691155433654785
Validation loss: 2.477009347690049

Epoch: 5| Step: 2
Training loss: 2.8491873741149902
Validation loss: 2.4901907264545398

Epoch: 5| Step: 3
Training loss: 2.0634076595306396
Validation loss: 2.492578255232944

Epoch: 5| Step: 4
Training loss: 2.4356350898742676
Validation loss: 2.500138782685803

Epoch: 5| Step: 5
Training loss: 3.0663344860076904
Validation loss: 2.48065399354504

Epoch: 5| Step: 6
Training loss: 2.8819963932037354
Validation loss: 2.470751941844981

Epoch: 5| Step: 7
Training loss: 2.802457094192505
Validation loss: 2.4704485144666446

Epoch: 5| Step: 8
Training loss: 2.7738378047943115
Validation loss: 2.464655242940431

Epoch: 5| Step: 9
Training loss: 2.535367250442505
Validation loss: 2.4602311195865756

Epoch: 5| Step: 10
Training loss: 2.842393159866333
Validation loss: 2.4642727605758177

Epoch: 181| Step: 0
Training loss: 2.8743510246276855
Validation loss: 2.4586105218497654

Epoch: 5| Step: 1
Training loss: 2.6342129707336426
Validation loss: 2.4668920681040776

Epoch: 5| Step: 2
Training loss: 2.2279865741729736
Validation loss: 2.4710662108595653

Epoch: 5| Step: 3
Training loss: 2.71744966506958
Validation loss: 2.481039103641305

Epoch: 5| Step: 4
Training loss: 2.416679859161377
Validation loss: 2.4832016678266626

Epoch: 5| Step: 5
Training loss: 2.4272067546844482
Validation loss: 2.4993553853804067

Epoch: 5| Step: 6
Training loss: 2.6881213188171387
Validation loss: 2.4883833700610745

Epoch: 5| Step: 7
Training loss: 3.245507001876831
Validation loss: 2.4827619957667526

Epoch: 5| Step: 8
Training loss: 2.5775933265686035
Validation loss: 2.4836615285565777

Epoch: 5| Step: 9
Training loss: 2.9016597270965576
Validation loss: 2.48404275473728

Epoch: 5| Step: 10
Training loss: 2.729229688644409
Validation loss: 2.4688113043385167

Epoch: 182| Step: 0
Training loss: 2.9581704139709473
Validation loss: 2.4626382807249665

Epoch: 5| Step: 1
Training loss: 2.6460063457489014
Validation loss: 2.4600102914276945

Epoch: 5| Step: 2
Training loss: 2.8721752166748047
Validation loss: 2.457312627505231

Epoch: 5| Step: 3
Training loss: 2.5821945667266846
Validation loss: 2.460339756422145

Epoch: 5| Step: 4
Training loss: 2.3487606048583984
Validation loss: 2.4645363976878505

Epoch: 5| Step: 5
Training loss: 2.9530601501464844
Validation loss: 2.463195939217844

Epoch: 5| Step: 6
Training loss: 1.8335139751434326
Validation loss: 2.4697751588718866

Epoch: 5| Step: 7
Training loss: 2.8034236431121826
Validation loss: 2.467019106752129

Epoch: 5| Step: 8
Training loss: 2.729278326034546
Validation loss: 2.4619156109389437

Epoch: 5| Step: 9
Training loss: 3.0320942401885986
Validation loss: 2.4596759042432232

Epoch: 5| Step: 10
Training loss: 2.4581856727600098
Validation loss: 2.4563278203369467

Epoch: 183| Step: 0
Training loss: 3.096735715866089
Validation loss: 2.4613569526262182

Epoch: 5| Step: 1
Training loss: 1.7806545495986938
Validation loss: 2.4564082571255264

Epoch: 5| Step: 2
Training loss: 2.882521629333496
Validation loss: 2.453625984089349

Epoch: 5| Step: 3
Training loss: 2.563685894012451
Validation loss: 2.4577908028838453

Epoch: 5| Step: 4
Training loss: 2.549952268600464
Validation loss: 2.4553972777499946

Epoch: 5| Step: 5
Training loss: 3.0318703651428223
Validation loss: 2.45665830181491

Epoch: 5| Step: 6
Training loss: 2.817915439605713
Validation loss: 2.458582267966322

Epoch: 5| Step: 7
Training loss: 3.3676514625549316
Validation loss: 2.4590356362763273

Epoch: 5| Step: 8
Training loss: 2.108666181564331
Validation loss: 2.4600904039157334

Epoch: 5| Step: 9
Training loss: 2.6647980213165283
Validation loss: 2.4612535199811383

Epoch: 5| Step: 10
Training loss: 2.2060787677764893
Validation loss: 2.4620356585389827

Epoch: 184| Step: 0
Training loss: 3.385458469390869
Validation loss: 2.461502262341079

Epoch: 5| Step: 1
Training loss: 3.364997386932373
Validation loss: 2.4589924376497985

Epoch: 5| Step: 2
Training loss: 2.2219021320343018
Validation loss: 2.462832743121732

Epoch: 5| Step: 3
Training loss: 2.666391611099243
Validation loss: 2.459100184902068

Epoch: 5| Step: 4
Training loss: 2.3771164417266846
Validation loss: 2.4672819158082366

Epoch: 5| Step: 5
Training loss: 2.5849907398223877
Validation loss: 2.466886362721843

Epoch: 5| Step: 6
Training loss: 2.5904898643493652
Validation loss: 2.4807182024883967

Epoch: 5| Step: 7
Training loss: 2.62844181060791
Validation loss: 2.498060090567476

Epoch: 5| Step: 8
Training loss: 2.3043220043182373
Validation loss: 2.501843834436068

Epoch: 5| Step: 9
Training loss: 2.0195491313934326
Validation loss: 2.4940781413867907

Epoch: 5| Step: 10
Training loss: 3.0480711460113525
Validation loss: 2.485024639355239

Epoch: 185| Step: 0
Training loss: 3.0648694038391113
Validation loss: 2.4841017928174747

Epoch: 5| Step: 1
Training loss: 2.7920784950256348
Validation loss: 2.4806835164305983

Epoch: 5| Step: 2
Training loss: 2.6613922119140625
Validation loss: 2.4731494483127388

Epoch: 5| Step: 3
Training loss: 2.5792605876922607
Validation loss: 2.4680387717421337

Epoch: 5| Step: 4
Training loss: 2.8869941234588623
Validation loss: 2.472120031233757

Epoch: 5| Step: 5
Training loss: 2.090045928955078
Validation loss: 2.4674176810890116

Epoch: 5| Step: 6
Training loss: 2.1877408027648926
Validation loss: 2.501580966416226

Epoch: 5| Step: 7
Training loss: 3.1372714042663574
Validation loss: 2.5127804356236614

Epoch: 5| Step: 8
Training loss: 3.105884552001953
Validation loss: 2.5143505398945143

Epoch: 5| Step: 9
Training loss: 2.2487740516662598
Validation loss: 2.5073306586152766

Epoch: 5| Step: 10
Training loss: 2.5075955390930176
Validation loss: 2.500672312193019

Epoch: 186| Step: 0
Training loss: 3.0211994647979736
Validation loss: 2.4803729185494046

Epoch: 5| Step: 1
Training loss: 2.2913315296173096
Validation loss: 2.474042812983195

Epoch: 5| Step: 2
Training loss: 2.3936991691589355
Validation loss: 2.471773414201634

Epoch: 5| Step: 3
Training loss: 2.3269705772399902
Validation loss: 2.4635312429038425

Epoch: 5| Step: 4
Training loss: 2.6454124450683594
Validation loss: 2.471912699361001

Epoch: 5| Step: 5
Training loss: 2.7523224353790283
Validation loss: 2.4656559882625455

Epoch: 5| Step: 6
Training loss: 3.3042426109313965
Validation loss: 2.4734656349305184

Epoch: 5| Step: 7
Training loss: 2.3060266971588135
Validation loss: 2.472662828301871

Epoch: 5| Step: 8
Training loss: 2.8072521686553955
Validation loss: 2.467338187720186

Epoch: 5| Step: 9
Training loss: 2.933974504470825
Validation loss: 2.46172934450129

Epoch: 5| Step: 10
Training loss: 2.3976683616638184
Validation loss: 2.4634003126493065

Epoch: 187| Step: 0
Training loss: 2.8002593517303467
Validation loss: 2.457693553739978

Epoch: 5| Step: 1
Training loss: 2.4928717613220215
Validation loss: 2.460337141508697

Epoch: 5| Step: 2
Training loss: 1.7848598957061768
Validation loss: 2.458668162745814

Epoch: 5| Step: 3
Training loss: 2.4976959228515625
Validation loss: 2.4683014859435377

Epoch: 5| Step: 4
Training loss: 3.34826922416687
Validation loss: 2.491315651965398

Epoch: 5| Step: 5
Training loss: 2.299226999282837
Validation loss: 2.479021692788729

Epoch: 5| Step: 6
Training loss: 3.3050034046173096
Validation loss: 2.469569283147012

Epoch: 5| Step: 7
Training loss: 2.305966854095459
Validation loss: 2.467403914338799

Epoch: 5| Step: 8
Training loss: 1.958346962928772
Validation loss: 2.4567495430669477

Epoch: 5| Step: 9
Training loss: 3.3891403675079346
Validation loss: 2.457997093918503

Epoch: 5| Step: 10
Training loss: 3.0914134979248047
Validation loss: 2.459588658425116

Epoch: 188| Step: 0
Training loss: 2.381429672241211
Validation loss: 2.4624767175284763

Epoch: 5| Step: 1
Training loss: 3.552233934402466
Validation loss: 2.460639524203475

Epoch: 5| Step: 2
Training loss: 2.6723694801330566
Validation loss: 2.4693782098831667

Epoch: 5| Step: 3
Training loss: 2.4373128414154053
Validation loss: 2.467106173115392

Epoch: 5| Step: 4
Training loss: 2.4602324962615967
Validation loss: 2.4683569887632966

Epoch: 5| Step: 5
Training loss: 1.9989604949951172
Validation loss: 2.4651972119526198

Epoch: 5| Step: 6
Training loss: 3.023437023162842
Validation loss: 2.4697598654736757

Epoch: 5| Step: 7
Training loss: 2.206259250640869
Validation loss: 2.4620313464954333

Epoch: 5| Step: 8
Training loss: 2.820162296295166
Validation loss: 2.4639984407732562

Epoch: 5| Step: 9
Training loss: 3.1394054889678955
Validation loss: 2.468809937918058

Epoch: 5| Step: 10
Training loss: 2.3675825595855713
Validation loss: 2.4643156810473372

Epoch: 189| Step: 0
Training loss: 2.9064769744873047
Validation loss: 2.4568413124289563

Epoch: 5| Step: 1
Training loss: 2.129128932952881
Validation loss: 2.4568828344345093

Epoch: 5| Step: 2
Training loss: 2.708972215652466
Validation loss: 2.4502935358273086

Epoch: 5| Step: 3
Training loss: 2.858015537261963
Validation loss: 2.455414487469581

Epoch: 5| Step: 4
Training loss: 2.6757636070251465
Validation loss: 2.448388235543364

Epoch: 5| Step: 5
Training loss: 2.973754644393921
Validation loss: 2.4515282236119753

Epoch: 5| Step: 6
Training loss: 2.6509201526641846
Validation loss: 2.451329251771332

Epoch: 5| Step: 7
Training loss: 2.5613255500793457
Validation loss: 2.454397355356524

Epoch: 5| Step: 8
Training loss: 3.0287182331085205
Validation loss: 2.4642906419692503

Epoch: 5| Step: 9
Training loss: 2.3630824089050293
Validation loss: 2.4710995663878736

Epoch: 5| Step: 10
Training loss: 2.113959789276123
Validation loss: 2.4713702483843734

Epoch: 190| Step: 0
Training loss: 2.45155668258667
Validation loss: 2.495330492655436

Epoch: 5| Step: 1
Training loss: 2.685941219329834
Validation loss: 2.487430300763858

Epoch: 5| Step: 2
Training loss: 3.2254295349121094
Validation loss: 2.478962423980877

Epoch: 5| Step: 3
Training loss: 2.3825366497039795
Validation loss: 2.464062149806689

Epoch: 5| Step: 4
Training loss: 2.6314241886138916
Validation loss: 2.451967885417323

Epoch: 5| Step: 5
Training loss: 2.1622674465179443
Validation loss: 2.4528983331495717

Epoch: 5| Step: 6
Training loss: 2.9733481407165527
Validation loss: 2.4542990217926683

Epoch: 5| Step: 7
Training loss: 2.095053195953369
Validation loss: 2.449388878319853

Epoch: 5| Step: 8
Training loss: 2.498029947280884
Validation loss: 2.45623839798794

Epoch: 5| Step: 9
Training loss: 2.7750084400177
Validation loss: 2.456509400439519

Epoch: 5| Step: 10
Training loss: 3.356497049331665
Validation loss: 2.454615572447418

Epoch: 191| Step: 0
Training loss: 3.388248920440674
Validation loss: 2.450899959892355

Epoch: 5| Step: 1
Training loss: 2.3242831230163574
Validation loss: 2.4539749109616844

Epoch: 5| Step: 2
Training loss: 2.6157126426696777
Validation loss: 2.455432694445374

Epoch: 5| Step: 3
Training loss: 2.318850517272949
Validation loss: 2.4596103801522204

Epoch: 5| Step: 4
Training loss: 2.9281585216522217
Validation loss: 2.4577958763286634

Epoch: 5| Step: 5
Training loss: 2.392712354660034
Validation loss: 2.454505926819258

Epoch: 5| Step: 6
Training loss: 2.569838285446167
Validation loss: 2.4589966240749566

Epoch: 5| Step: 7
Training loss: 2.6581156253814697
Validation loss: 2.458212549968432

Epoch: 5| Step: 8
Training loss: 2.611870288848877
Validation loss: 2.4611217565433954

Epoch: 5| Step: 9
Training loss: 2.646332263946533
Validation loss: 2.464696361172584

Epoch: 5| Step: 10
Training loss: 2.55967116355896
Validation loss: 2.4627464663597847

Epoch: 192| Step: 0
Training loss: 2.6544785499572754
Validation loss: 2.4603602014562136

Epoch: 5| Step: 1
Training loss: 2.6575000286102295
Validation loss: 2.4671126668171217

Epoch: 5| Step: 2
Training loss: 3.0848469734191895
Validation loss: 2.469094932720225

Epoch: 5| Step: 3
Training loss: 2.8211746215820312
Validation loss: 2.4749831230409685

Epoch: 5| Step: 4
Training loss: 3.104766845703125
Validation loss: 2.4731579749814925

Epoch: 5| Step: 5
Training loss: 2.403071165084839
Validation loss: 2.4608191879846717

Epoch: 5| Step: 6
Training loss: 2.0878281593322754
Validation loss: 2.4541608748897428

Epoch: 5| Step: 7
Training loss: 2.6333792209625244
Validation loss: 2.455737554898826

Epoch: 5| Step: 8
Training loss: 2.131842851638794
Validation loss: 2.4547768920980473

Epoch: 5| Step: 9
Training loss: 3.0894272327423096
Validation loss: 2.4545423728163525

Epoch: 5| Step: 10
Training loss: 2.388049602508545
Validation loss: 2.454441847339753

Epoch: 193| Step: 0
Training loss: 2.747471570968628
Validation loss: 2.459595739200551

Epoch: 5| Step: 1
Training loss: 2.4435482025146484
Validation loss: 2.458558224862622

Epoch: 5| Step: 2
Training loss: 2.1853420734405518
Validation loss: 2.4496748575600247

Epoch: 5| Step: 3
Training loss: 2.3841192722320557
Validation loss: 2.4474748052576536

Epoch: 5| Step: 4
Training loss: 2.5081942081451416
Validation loss: 2.4461491723214426

Epoch: 5| Step: 5
Training loss: 2.9705677032470703
Validation loss: 2.4377574061834686

Epoch: 5| Step: 6
Training loss: 2.1559031009674072
Validation loss: 2.4417523953222458

Epoch: 5| Step: 7
Training loss: 3.057497262954712
Validation loss: 2.4357016240396807

Epoch: 5| Step: 8
Training loss: 3.6494979858398438
Validation loss: 2.440819558276925

Epoch: 5| Step: 9
Training loss: 1.7162950038909912
Validation loss: 2.4461459395706013

Epoch: 5| Step: 10
Training loss: 3.2758357524871826
Validation loss: 2.4483746892662457

Epoch: 194| Step: 0
Training loss: 2.4246020317077637
Validation loss: 2.455131635870985

Epoch: 5| Step: 1
Training loss: 2.152559757232666
Validation loss: 2.4546059280313473

Epoch: 5| Step: 2
Training loss: 2.579789638519287
Validation loss: 2.4629755891779417

Epoch: 5| Step: 3
Training loss: 2.7773025035858154
Validation loss: 2.464139292317052

Epoch: 5| Step: 4
Training loss: 3.4453670978546143
Validation loss: 2.4655484435378865

Epoch: 5| Step: 5
Training loss: 2.1279892921447754
Validation loss: 2.4877332615595993

Epoch: 5| Step: 6
Training loss: 3.3689751625061035
Validation loss: 2.4634960056633077

Epoch: 5| Step: 7
Training loss: 2.598038673400879
Validation loss: 2.46208591358636

Epoch: 5| Step: 8
Training loss: 2.494224786758423
Validation loss: 2.457178273508626

Epoch: 5| Step: 9
Training loss: 2.574824094772339
Validation loss: 2.4561291048603673

Epoch: 5| Step: 10
Training loss: 2.4188807010650635
Validation loss: 2.4503727805229927

Epoch: 195| Step: 0
Training loss: 2.6443705558776855
Validation loss: 2.445067333918746

Epoch: 5| Step: 1
Training loss: 2.786088228225708
Validation loss: 2.4543464440171436

Epoch: 5| Step: 2
Training loss: 2.43729567527771
Validation loss: 2.4449748839101484

Epoch: 5| Step: 3
Training loss: 2.859205722808838
Validation loss: 2.4409417516441754

Epoch: 5| Step: 4
Training loss: 2.6839511394500732
Validation loss: 2.4430997653674056

Epoch: 5| Step: 5
Training loss: 2.4755048751831055
Validation loss: 2.443966788630332

Epoch: 5| Step: 6
Training loss: 2.291813611984253
Validation loss: 2.444826267098868

Epoch: 5| Step: 7
Training loss: 2.592205047607422
Validation loss: 2.4388829021043676

Epoch: 5| Step: 8
Training loss: 3.0143425464630127
Validation loss: 2.440555895528486

Epoch: 5| Step: 9
Training loss: 2.959233283996582
Validation loss: 2.4486244032459874

Epoch: 5| Step: 10
Training loss: 2.189588785171509
Validation loss: 2.45767903584306

Epoch: 196| Step: 0
Training loss: 2.4071991443634033
Validation loss: 2.46766540568362

Epoch: 5| Step: 1
Training loss: 3.0576579570770264
Validation loss: 2.4755040855817896

Epoch: 5| Step: 2
Training loss: 2.7269179821014404
Validation loss: 2.465562717888945

Epoch: 5| Step: 3
Training loss: 2.9792866706848145
Validation loss: 2.449765546347505

Epoch: 5| Step: 4
Training loss: 2.824159622192383
Validation loss: 2.4496621547206754

Epoch: 5| Step: 5
Training loss: 2.7948391437530518
Validation loss: 2.4422194445005028

Epoch: 5| Step: 6
Training loss: 2.7675673961639404
Validation loss: 2.4406048815737487

Epoch: 5| Step: 7
Training loss: 1.8690048456192017
Validation loss: 2.444259302590483

Epoch: 5| Step: 8
Training loss: 2.4998764991760254
Validation loss: 2.4438481433417207

Epoch: 5| Step: 9
Training loss: 2.371995210647583
Validation loss: 2.449167054186585

Epoch: 5| Step: 10
Training loss: 2.712291717529297
Validation loss: 2.4499723372920865

Epoch: 197| Step: 0
Training loss: 2.9892468452453613
Validation loss: 2.4546326232212845

Epoch: 5| Step: 1
Training loss: 2.870129346847534
Validation loss: 2.451595519178657

Epoch: 5| Step: 2
Training loss: 2.141906976699829
Validation loss: 2.446659331680626

Epoch: 5| Step: 3
Training loss: 3.0151922702789307
Validation loss: 2.4481658858637654

Epoch: 5| Step: 4
Training loss: 1.3479632139205933
Validation loss: 2.448077242861512

Epoch: 5| Step: 5
Training loss: 2.9170889854431152
Validation loss: 2.449790644389327

Epoch: 5| Step: 6
Training loss: 2.4068949222564697
Validation loss: 2.44224472712445

Epoch: 5| Step: 7
Training loss: 3.2331154346466064
Validation loss: 2.4425122225156395

Epoch: 5| Step: 8
Training loss: 2.169955253601074
Validation loss: 2.452664872651459

Epoch: 5| Step: 9
Training loss: 3.117013931274414
Validation loss: 2.4543486461844495

Epoch: 5| Step: 10
Training loss: 2.841053009033203
Validation loss: 2.4546585788008985

Epoch: 198| Step: 0
Training loss: 1.8374723196029663
Validation loss: 2.4441965267222416

Epoch: 5| Step: 1
Training loss: 3.0586986541748047
Validation loss: 2.4527254771160822

Epoch: 5| Step: 2
Training loss: 2.753702163696289
Validation loss: 2.4535262328322216

Epoch: 5| Step: 3
Training loss: 3.4431090354919434
Validation loss: 2.457444878034694

Epoch: 5| Step: 4
Training loss: 2.07169771194458
Validation loss: 2.4542712216736167

Epoch: 5| Step: 5
Training loss: 2.070849657058716
Validation loss: 2.4580507124623945

Epoch: 5| Step: 6
Training loss: 2.7099621295928955
Validation loss: 2.45024206689609

Epoch: 5| Step: 7
Training loss: 2.5404160022735596
Validation loss: 2.4447068501544256

Epoch: 5| Step: 8
Training loss: 3.1631104946136475
Validation loss: 2.4407433130407847

Epoch: 5| Step: 9
Training loss: 2.7406649589538574
Validation loss: 2.44347587836686

Epoch: 5| Step: 10
Training loss: 2.687649965286255
Validation loss: 2.44138168263179

Epoch: 199| Step: 0
Training loss: 3.187959671020508
Validation loss: 2.4398461977640786

Epoch: 5| Step: 1
Training loss: 2.3421568870544434
Validation loss: 2.438103437423706

Epoch: 5| Step: 2
Training loss: 3.2505269050598145
Validation loss: 2.4440860312472106

Epoch: 5| Step: 3
Training loss: 2.770099401473999
Validation loss: 2.445063844803841

Epoch: 5| Step: 4
Training loss: 2.7940473556518555
Validation loss: 2.439886398212884

Epoch: 5| Step: 5
Training loss: 2.885214328765869
Validation loss: 2.4410089215924664

Epoch: 5| Step: 6
Training loss: 2.09883189201355
Validation loss: 2.4489664621250604

Epoch: 5| Step: 7
Training loss: 2.760300636291504
Validation loss: 2.443386385517736

Epoch: 5| Step: 8
Training loss: 2.5193610191345215
Validation loss: 2.438075342485982

Epoch: 5| Step: 9
Training loss: 1.896352767944336
Validation loss: 2.4392040314212924

Epoch: 5| Step: 10
Training loss: 2.3688912391662598
Validation loss: 2.4456023195738434

Epoch: 200| Step: 0
Training loss: 2.041628837585449
Validation loss: 2.4405227938006

Epoch: 5| Step: 1
Training loss: 2.692673683166504
Validation loss: 2.4385052393841486

Epoch: 5| Step: 2
Training loss: 2.8475639820098877
Validation loss: 2.437773125146025

Epoch: 5| Step: 3
Training loss: 2.7346785068511963
Validation loss: 2.437320011918263

Epoch: 5| Step: 4
Training loss: 2.8342933654785156
Validation loss: 2.4445437949190856

Epoch: 5| Step: 5
Training loss: 2.3285751342773438
Validation loss: 2.4445584204889115

Epoch: 5| Step: 6
Training loss: 2.9601399898529053
Validation loss: 2.452718723204828

Epoch: 5| Step: 7
Training loss: 2.56423282623291
Validation loss: 2.4509148187534784

Epoch: 5| Step: 8
Training loss: 2.7377679347991943
Validation loss: 2.4474916791403167

Epoch: 5| Step: 9
Training loss: 1.9574460983276367
Validation loss: 2.4427407403146066

Epoch: 5| Step: 10
Training loss: 3.4584262371063232
Validation loss: 2.442430588506883

Epoch: 201| Step: 0
Training loss: 2.046855926513672
Validation loss: 2.433441177491219

Epoch: 5| Step: 1
Training loss: 3.046128749847412
Validation loss: 2.437882684892224

Epoch: 5| Step: 2
Training loss: 2.8434505462646484
Validation loss: 2.4435283624997703

Epoch: 5| Step: 3
Training loss: 2.330695629119873
Validation loss: 2.4446276900588826

Epoch: 5| Step: 4
Training loss: 3.1368441581726074
Validation loss: 2.4590569209027033

Epoch: 5| Step: 5
Training loss: 2.787933826446533
Validation loss: 2.4721060927196215

Epoch: 5| Step: 6
Training loss: 1.7116857767105103
Validation loss: 2.476906032972438

Epoch: 5| Step: 7
Training loss: 3.027991533279419
Validation loss: 2.480357764869608

Epoch: 5| Step: 8
Training loss: 2.8263142108917236
Validation loss: 2.488636460355533

Epoch: 5| Step: 9
Training loss: 2.472470283508301
Validation loss: 2.4758302268161567

Epoch: 5| Step: 10
Training loss: 2.9485161304473877
Validation loss: 2.459668415848927

Epoch: 202| Step: 0
Training loss: 2.720484495162964
Validation loss: 2.442244755324497

Epoch: 5| Step: 1
Training loss: 2.7779669761657715
Validation loss: 2.442518057361726

Epoch: 5| Step: 2
Training loss: 2.3216640949249268
Validation loss: 2.435603041802683

Epoch: 5| Step: 3
Training loss: 3.288104295730591
Validation loss: 2.4517874922803653

Epoch: 5| Step: 4
Training loss: 2.305396318435669
Validation loss: 2.44157269436826

Epoch: 5| Step: 5
Training loss: 3.321885347366333
Validation loss: 2.439926532007033

Epoch: 5| Step: 6
Training loss: 2.801488161087036
Validation loss: 2.439428601213681

Epoch: 5| Step: 7
Training loss: 2.1309027671813965
Validation loss: 2.4330261240723314

Epoch: 5| Step: 8
Training loss: 2.181398391723633
Validation loss: 2.4321703859554824

Epoch: 5| Step: 9
Training loss: 2.5290889739990234
Validation loss: 2.4306694538362565

Epoch: 5| Step: 10
Training loss: 2.671271324157715
Validation loss: 2.4291688934449227

Epoch: 203| Step: 0
Training loss: 1.9273483753204346
Validation loss: 2.4244325314798663

Epoch: 5| Step: 1
Training loss: 2.3784313201904297
Validation loss: 2.4331358991643435

Epoch: 5| Step: 2
Training loss: 3.2718474864959717
Validation loss: 2.429229264618248

Epoch: 5| Step: 3
Training loss: 2.5664989948272705
Validation loss: 2.4288395938052925

Epoch: 5| Step: 4
Training loss: 2.6698803901672363
Validation loss: 2.431812870887018

Epoch: 5| Step: 5
Training loss: 2.78456974029541
Validation loss: 2.4282684967082035

Epoch: 5| Step: 6
Training loss: 2.434940814971924
Validation loss: 2.433481982959214

Epoch: 5| Step: 7
Training loss: 2.0663490295410156
Validation loss: 2.431651182072137

Epoch: 5| Step: 8
Training loss: 3.08744478225708
Validation loss: 2.4309398922868954

Epoch: 5| Step: 9
Training loss: 2.566157817840576
Validation loss: 2.432687697872039

Epoch: 5| Step: 10
Training loss: 3.277435302734375
Validation loss: 2.4334715976509997

Epoch: 204| Step: 0
Training loss: 3.0340662002563477
Validation loss: 2.440780057702013

Epoch: 5| Step: 1
Training loss: 2.2111449241638184
Validation loss: 2.435389421319449

Epoch: 5| Step: 2
Training loss: 3.275096893310547
Validation loss: 2.444563542642901

Epoch: 5| Step: 3
Training loss: 3.067908763885498
Validation loss: 2.4493446093733593

Epoch: 5| Step: 4
Training loss: 2.9235520362854004
Validation loss: 2.4534786837075346

Epoch: 5| Step: 5
Training loss: 2.261492967605591
Validation loss: 2.45477472325807

Epoch: 5| Step: 6
Training loss: 2.4592604637145996
Validation loss: 2.455645256145026

Epoch: 5| Step: 7
Training loss: 2.575528383255005
Validation loss: 2.450093435984786

Epoch: 5| Step: 8
Training loss: 2.1593635082244873
Validation loss: 2.449753133199548

Epoch: 5| Step: 9
Training loss: 2.3868229389190674
Validation loss: 2.4517714746536745

Epoch: 5| Step: 10
Training loss: 2.5107240676879883
Validation loss: 2.4557334966557

Epoch: 205| Step: 0
Training loss: 2.4031856060028076
Validation loss: 2.438989270117975

Epoch: 5| Step: 1
Training loss: 2.3914871215820312
Validation loss: 2.4404018463626986

Epoch: 5| Step: 2
Training loss: 2.724881649017334
Validation loss: 2.447378440569806

Epoch: 5| Step: 3
Training loss: 2.870522975921631
Validation loss: 2.4391515972793743

Epoch: 5| Step: 4
Training loss: 2.3255162239074707
Validation loss: 2.4473406191795104

Epoch: 5| Step: 5
Training loss: 2.8607051372528076
Validation loss: 2.442341319976314

Epoch: 5| Step: 6
Training loss: 1.9803693294525146
Validation loss: 2.436990873787993

Epoch: 5| Step: 7
Training loss: 2.7026522159576416
Validation loss: 2.4384162989995812

Epoch: 5| Step: 8
Training loss: 3.340147018432617
Validation loss: 2.4323400733291463

Epoch: 5| Step: 9
Training loss: 2.7437448501586914
Validation loss: 2.4312811769464964

Epoch: 5| Step: 10
Training loss: 2.5521650314331055
Validation loss: 2.4266577792424027

Epoch: 206| Step: 0
Training loss: 2.8268489837646484
Validation loss: 2.4306018070508073

Epoch: 5| Step: 1
Training loss: 3.4204070568084717
Validation loss: 2.4343547154498357

Epoch: 5| Step: 2
Training loss: 2.3228774070739746
Validation loss: 2.4362598234607327

Epoch: 5| Step: 3
Training loss: 2.5028059482574463
Validation loss: 2.437470333550566

Epoch: 5| Step: 4
Training loss: 3.043217897415161
Validation loss: 2.4315571144062984

Epoch: 5| Step: 5
Training loss: 2.419590473175049
Validation loss: 2.4290161312267347

Epoch: 5| Step: 6
Training loss: 2.1510214805603027
Validation loss: 2.427476941898305

Epoch: 5| Step: 7
Training loss: 1.747541069984436
Validation loss: 2.4377054347786853

Epoch: 5| Step: 8
Training loss: 3.781019687652588
Validation loss: 2.43543363899313

Epoch: 5| Step: 9
Training loss: 2.154245138168335
Validation loss: 2.4354236715583393

Epoch: 5| Step: 10
Training loss: 2.533519744873047
Validation loss: 2.4325472770198697

Epoch: 207| Step: 0
Training loss: 2.6676528453826904
Validation loss: 2.432865991387316

Epoch: 5| Step: 1
Training loss: 2.915372848510742
Validation loss: 2.4332928836986585

Epoch: 5| Step: 2
Training loss: 3.245651960372925
Validation loss: 2.4396888030472623

Epoch: 5| Step: 3
Training loss: 1.4942126274108887
Validation loss: 2.4425407891632407

Epoch: 5| Step: 4
Training loss: 2.610159397125244
Validation loss: 2.4456110872248167

Epoch: 5| Step: 5
Training loss: 2.2198688983917236
Validation loss: 2.4447463712384625

Epoch: 5| Step: 6
Training loss: 2.7998790740966797
Validation loss: 2.450982629611928

Epoch: 5| Step: 7
Training loss: 2.3203372955322266
Validation loss: 2.453211171652681

Epoch: 5| Step: 8
Training loss: 2.7548325061798096
Validation loss: 2.4546355111624605

Epoch: 5| Step: 9
Training loss: 3.56939697265625
Validation loss: 2.4660068327380764

Epoch: 5| Step: 10
Training loss: 2.127061367034912
Validation loss: 2.4666837005205053

Epoch: 208| Step: 0
Training loss: 2.6190645694732666
Validation loss: 2.4796023061198573

Epoch: 5| Step: 1
Training loss: 2.2903263568878174
Validation loss: 2.4925761658658265

Epoch: 5| Step: 2
Training loss: 3.0282745361328125
Validation loss: 2.515638138658257

Epoch: 5| Step: 3
Training loss: 2.0650784969329834
Validation loss: 2.5163993143266246

Epoch: 5| Step: 4
Training loss: 2.3901610374450684
Validation loss: 2.529069189102419

Epoch: 5| Step: 5
Training loss: 2.307723045349121
Validation loss: 2.5184871278783327

Epoch: 5| Step: 6
Training loss: 2.8339345455169678
Validation loss: 2.503935992076833

Epoch: 5| Step: 7
Training loss: 2.6673755645751953
Validation loss: 2.4998626093710623

Epoch: 5| Step: 8
Training loss: 2.550943374633789
Validation loss: 2.482148749853975

Epoch: 5| Step: 9
Training loss: 3.457296848297119
Validation loss: 2.4680172166516705

Epoch: 5| Step: 10
Training loss: 2.9033634662628174
Validation loss: 2.4507672350893737

Epoch: 209| Step: 0
Training loss: 2.9897570610046387
Validation loss: 2.4298083064376668

Epoch: 5| Step: 1
Training loss: 2.895947217941284
Validation loss: 2.4267707024851153

Epoch: 5| Step: 2
Training loss: 2.8691983222961426
Validation loss: 2.4378930317458285

Epoch: 5| Step: 3
Training loss: 2.5936989784240723
Validation loss: 2.4344746963952177

Epoch: 5| Step: 4
Training loss: 2.9105935096740723
Validation loss: 2.4346106180580716

Epoch: 5| Step: 5
Training loss: 2.344574451446533
Validation loss: 2.431754007134386

Epoch: 5| Step: 6
Training loss: 2.1497890949249268
Validation loss: 2.4347954155296407

Epoch: 5| Step: 7
Training loss: 2.6331493854522705
Validation loss: 2.434786488932948

Epoch: 5| Step: 8
Training loss: 2.885838031768799
Validation loss: 2.432513516436341

Epoch: 5| Step: 9
Training loss: 2.292846202850342
Validation loss: 2.4325370891119844

Epoch: 5| Step: 10
Training loss: 2.4543237686157227
Validation loss: 2.43385749478494

Epoch: 210| Step: 0
Training loss: 2.183382749557495
Validation loss: 2.436875189504316

Epoch: 5| Step: 1
Training loss: 2.1427161693573
Validation loss: 2.4327075071232294

Epoch: 5| Step: 2
Training loss: 2.5898325443267822
Validation loss: 2.4372931424007622

Epoch: 5| Step: 3
Training loss: 2.4780287742614746
Validation loss: 2.4418767113839426

Epoch: 5| Step: 4
Training loss: 2.913196086883545
Validation loss: 2.4445707618549304

Epoch: 5| Step: 5
Training loss: 3.0933032035827637
Validation loss: 2.4507490447772446

Epoch: 5| Step: 6
Training loss: 2.866995096206665
Validation loss: 2.437884799895748

Epoch: 5| Step: 7
Training loss: 3.0222411155700684
Validation loss: 2.4409099625002955

Epoch: 5| Step: 8
Training loss: 2.833094596862793
Validation loss: 2.4468571678284676

Epoch: 5| Step: 9
Training loss: 2.1017403602600098
Validation loss: 2.4456797492119575

Epoch: 5| Step: 10
Training loss: 2.847005844116211
Validation loss: 2.4380004380338933

Epoch: 211| Step: 0
Training loss: 2.9440054893493652
Validation loss: 2.4451662596835884

Epoch: 5| Step: 1
Training loss: 2.7271060943603516
Validation loss: 2.4476955424072924

Epoch: 5| Step: 2
Training loss: 2.452662229537964
Validation loss: 2.4512465153971026

Epoch: 5| Step: 3
Training loss: 2.383068561553955
Validation loss: 2.453353871581375

Epoch: 5| Step: 4
Training loss: 2.6392016410827637
Validation loss: 2.4513229939245407

Epoch: 5| Step: 5
Training loss: 2.9570932388305664
Validation loss: 2.4540687837908344

Epoch: 5| Step: 6
Training loss: 2.742518901824951
Validation loss: 2.44502467109311

Epoch: 5| Step: 7
Training loss: 2.501661777496338
Validation loss: 2.440563068594984

Epoch: 5| Step: 8
Training loss: 2.7053353786468506
Validation loss: 2.439797668046849

Epoch: 5| Step: 9
Training loss: 2.527916431427002
Validation loss: 2.43061428172614

Epoch: 5| Step: 10
Training loss: 2.37420916557312
Validation loss: 2.436927077590778

Epoch: 212| Step: 0
Training loss: 3.0189850330352783
Validation loss: 2.4345182141950055

Epoch: 5| Step: 1
Training loss: 3.016853094100952
Validation loss: 2.4297636606359996

Epoch: 5| Step: 2
Training loss: 1.8936843872070312
Validation loss: 2.425515772193991

Epoch: 5| Step: 3
Training loss: 2.5961761474609375
Validation loss: 2.4211122861472507

Epoch: 5| Step: 4
Training loss: 2.531463623046875
Validation loss: 2.4234410357731644

Epoch: 5| Step: 5
Training loss: 2.6299679279327393
Validation loss: 2.424691810402819

Epoch: 5| Step: 6
Training loss: 2.210991621017456
Validation loss: 2.4247145575861775

Epoch: 5| Step: 7
Training loss: 2.5083909034729004
Validation loss: 2.424322453878259

Epoch: 5| Step: 8
Training loss: 3.099137544631958
Validation loss: 2.4218481176642963

Epoch: 5| Step: 9
Training loss: 3.0659384727478027
Validation loss: 2.4218398806869343

Epoch: 5| Step: 10
Training loss: 2.2392938137054443
Validation loss: 2.418969385085567

Epoch: 213| Step: 0
Training loss: 2.173229932785034
Validation loss: 2.4156812596064743

Epoch: 5| Step: 1
Training loss: 2.511928081512451
Validation loss: 2.415534116888559

Epoch: 5| Step: 2
Training loss: 2.041883707046509
Validation loss: 2.416597627824353

Epoch: 5| Step: 3
Training loss: 2.3550591468811035
Validation loss: 2.4134207617851997

Epoch: 5| Step: 4
Training loss: 3.309525728225708
Validation loss: 2.4171425834778817

Epoch: 5| Step: 5
Training loss: 2.946812629699707
Validation loss: 2.4254319744725383

Epoch: 5| Step: 6
Training loss: 2.5982861518859863
Validation loss: 2.425226357675368

Epoch: 5| Step: 7
Training loss: 2.5969696044921875
Validation loss: 2.43207872298456

Epoch: 5| Step: 8
Training loss: 3.1010401248931885
Validation loss: 2.4228952366818666

Epoch: 5| Step: 9
Training loss: 2.344468832015991
Validation loss: 2.4278523281056392

Epoch: 5| Step: 10
Training loss: 2.920656442642212
Validation loss: 2.4320086817587576

Epoch: 214| Step: 0
Training loss: 2.521679401397705
Validation loss: 2.4215013365591727

Epoch: 5| Step: 1
Training loss: 2.7314791679382324
Validation loss: 2.4231280793425856

Epoch: 5| Step: 2
Training loss: 2.8734285831451416
Validation loss: 2.424265394928635

Epoch: 5| Step: 3
Training loss: 1.9502757787704468
Validation loss: 2.424216547319966

Epoch: 5| Step: 4
Training loss: 3.1323838233947754
Validation loss: 2.4247512099563435

Epoch: 5| Step: 5
Training loss: 3.2100601196289062
Validation loss: 2.427554226690723

Epoch: 5| Step: 6
Training loss: 2.122563362121582
Validation loss: 2.420051779798282

Epoch: 5| Step: 7
Training loss: 2.273393154144287
Validation loss: 2.426497685011997

Epoch: 5| Step: 8
Training loss: 2.3488574028015137
Validation loss: 2.427577118719778

Epoch: 5| Step: 9
Training loss: 2.529104709625244
Validation loss: 2.4256480637417046

Epoch: 5| Step: 10
Training loss: 3.2068259716033936
Validation loss: 2.4179601489856677

Epoch: 215| Step: 0
Training loss: 2.8799996376037598
Validation loss: 2.4270453222336306

Epoch: 5| Step: 1
Training loss: 2.9999735355377197
Validation loss: 2.428999447053479

Epoch: 5| Step: 2
Training loss: 2.4630520343780518
Validation loss: 2.429228715999152

Epoch: 5| Step: 3
Training loss: 2.7925310134887695
Validation loss: 2.4249675555895736

Epoch: 5| Step: 4
Training loss: 2.4223439693450928
Validation loss: 2.422819665683213

Epoch: 5| Step: 5
Training loss: 2.2962353229522705
Validation loss: 2.415105979929688

Epoch: 5| Step: 6
Training loss: 2.272846221923828
Validation loss: 2.4172284603118896

Epoch: 5| Step: 7
Training loss: 3.5250842571258545
Validation loss: 2.4208097714249805

Epoch: 5| Step: 8
Training loss: 2.015538454055786
Validation loss: 2.4223050686620895

Epoch: 5| Step: 9
Training loss: 2.559718608856201
Validation loss: 2.420975951738255

Epoch: 5| Step: 10
Training loss: 2.546105146408081
Validation loss: 2.420551794831471

Epoch: 216| Step: 0
Training loss: 1.9959875345230103
Validation loss: 2.4270654186125724

Epoch: 5| Step: 1
Training loss: 2.63519024848938
Validation loss: 2.4283733060283046

Epoch: 5| Step: 2
Training loss: 2.404372215270996
Validation loss: 2.4230443841667584

Epoch: 5| Step: 3
Training loss: 2.502345561981201
Validation loss: 2.4237660233692457

Epoch: 5| Step: 4
Training loss: 2.492753744125366
Validation loss: 2.424242683636245

Epoch: 5| Step: 5
Training loss: 3.1830718517303467
Validation loss: 2.417603110754362

Epoch: 5| Step: 6
Training loss: 2.493877410888672
Validation loss: 2.4174913334590133

Epoch: 5| Step: 7
Training loss: 3.3603744506835938
Validation loss: 2.4235500520275486

Epoch: 5| Step: 8
Training loss: 2.463759422302246
Validation loss: 2.4238333599541777

Epoch: 5| Step: 9
Training loss: 2.4428038597106934
Validation loss: 2.4216380862779516

Epoch: 5| Step: 10
Training loss: 2.801867961883545
Validation loss: 2.4285063666682087

Epoch: 217| Step: 0
Training loss: 3.296124219894409
Validation loss: 2.418680480731431

Epoch: 5| Step: 1
Training loss: 2.502671718597412
Validation loss: 2.422920847451815

Epoch: 5| Step: 2
Training loss: 3.3049263954162598
Validation loss: 2.420315341282916

Epoch: 5| Step: 3
Training loss: 3.437589168548584
Validation loss: 2.418134161221084

Epoch: 5| Step: 4
Training loss: 1.5988237857818604
Validation loss: 2.4171339850271902

Epoch: 5| Step: 5
Training loss: 2.2241644859313965
Validation loss: 2.4192299740288847

Epoch: 5| Step: 6
Training loss: 2.7477872371673584
Validation loss: 2.4173227433235414

Epoch: 5| Step: 7
Training loss: 2.552398204803467
Validation loss: 2.415554490140689

Epoch: 5| Step: 8
Training loss: 2.3904075622558594
Validation loss: 2.4154285474490096

Epoch: 5| Step: 9
Training loss: 2.2378780841827393
Validation loss: 2.4186524370665192

Epoch: 5| Step: 10
Training loss: 2.3762896060943604
Validation loss: 2.4177581289763093

Epoch: 218| Step: 0
Training loss: 2.862736463546753
Validation loss: 2.4205432297081075

Epoch: 5| Step: 1
Training loss: 2.0240190029144287
Validation loss: 2.424717982610067

Epoch: 5| Step: 2
Training loss: 2.6925690174102783
Validation loss: 2.4288938071138118

Epoch: 5| Step: 3
Training loss: 2.4997916221618652
Validation loss: 2.4305889221929733

Epoch: 5| Step: 4
Training loss: 2.6084399223327637
Validation loss: 2.421783419065578

Epoch: 5| Step: 5
Training loss: 2.344888687133789
Validation loss: 2.4199987457644556

Epoch: 5| Step: 6
Training loss: 3.119148015975952
Validation loss: 2.428733277064498

Epoch: 5| Step: 7
Training loss: 3.0933663845062256
Validation loss: 2.424456855302216

Epoch: 5| Step: 8
Training loss: 2.8960299491882324
Validation loss: 2.425068332302955

Epoch: 5| Step: 9
Training loss: 2.4501025676727295
Validation loss: 2.424571644875311

Epoch: 5| Step: 10
Training loss: 1.9249051809310913
Validation loss: 2.4206332827127106

Epoch: 219| Step: 0
Training loss: 2.5704498291015625
Validation loss: 2.4243879754056215

Epoch: 5| Step: 1
Training loss: 2.984382152557373
Validation loss: 2.4175444828566683

Epoch: 5| Step: 2
Training loss: 1.9987512826919556
Validation loss: 2.4259277005349436

Epoch: 5| Step: 3
Training loss: 2.206829786300659
Validation loss: 2.427526912381572

Epoch: 5| Step: 4
Training loss: 3.093684434890747
Validation loss: 2.430188560998568

Epoch: 5| Step: 5
Training loss: 2.371464729309082
Validation loss: 2.4225051838864564

Epoch: 5| Step: 6
Training loss: 2.626221179962158
Validation loss: 2.421762256212132

Epoch: 5| Step: 7
Training loss: 2.863708257675171
Validation loss: 2.4275228438838834

Epoch: 5| Step: 8
Training loss: 2.401981830596924
Validation loss: 2.4153798767315444

Epoch: 5| Step: 9
Training loss: 2.8729255199432373
Validation loss: 2.412426907529113

Epoch: 5| Step: 10
Training loss: 2.6193556785583496
Validation loss: 2.4195739120565434

Epoch: 220| Step: 0
Training loss: 2.0930278301239014
Validation loss: 2.4180170310440885

Epoch: 5| Step: 1
Training loss: 2.0540690422058105
Validation loss: 2.4203477969733616

Epoch: 5| Step: 2
Training loss: 2.6527292728424072
Validation loss: 2.411060994671237

Epoch: 5| Step: 3
Training loss: 3.2573585510253906
Validation loss: 2.4213642433125484

Epoch: 5| Step: 4
Training loss: 2.5656306743621826
Validation loss: 2.427383456178891

Epoch: 5| Step: 5
Training loss: 2.629594326019287
Validation loss: 2.441796923196444

Epoch: 5| Step: 6
Training loss: 2.374452590942383
Validation loss: 2.4327622818690475

Epoch: 5| Step: 7
Training loss: 2.3292038440704346
Validation loss: 2.4439280315112044

Epoch: 5| Step: 8
Training loss: 2.9555630683898926
Validation loss: 2.425298531850179

Epoch: 5| Step: 9
Training loss: 2.953782081604004
Validation loss: 2.4253509352284093

Epoch: 5| Step: 10
Training loss: 2.864420175552368
Validation loss: 2.428804354001117

Epoch: 221| Step: 0
Training loss: 2.383650302886963
Validation loss: 2.423330740262103

Epoch: 5| Step: 1
Training loss: 2.9366772174835205
Validation loss: 2.4223863578611806

Epoch: 5| Step: 2
Training loss: 2.575434446334839
Validation loss: 2.4140340820435555

Epoch: 5| Step: 3
Training loss: 2.7150685787200928
Validation loss: 2.415253003438314

Epoch: 5| Step: 4
Training loss: 2.4887099266052246
Validation loss: 2.420120144403109

Epoch: 5| Step: 5
Training loss: 2.334512233734131
Validation loss: 2.4131154962765273

Epoch: 5| Step: 6
Training loss: 2.145700454711914
Validation loss: 2.417467765910651

Epoch: 5| Step: 7
Training loss: 3.583339214324951
Validation loss: 2.4218761510746454

Epoch: 5| Step: 8
Training loss: 2.3773398399353027
Validation loss: 2.432515977531351

Epoch: 5| Step: 9
Training loss: 2.3088221549987793
Validation loss: 2.423829406820318

Epoch: 5| Step: 10
Training loss: 2.734300374984741
Validation loss: 2.4262120928815616

Epoch: 222| Step: 0
Training loss: 2.5910544395446777
Validation loss: 2.4387394869199364

Epoch: 5| Step: 1
Training loss: 2.890411376953125
Validation loss: 2.4318916515637468

Epoch: 5| Step: 2
Training loss: 2.8950698375701904
Validation loss: 2.433061594604164

Epoch: 5| Step: 3
Training loss: 3.007028579711914
Validation loss: 2.4162182410558066

Epoch: 5| Step: 4
Training loss: 2.4906222820281982
Validation loss: 2.4222678010181715

Epoch: 5| Step: 5
Training loss: 2.67350435256958
Validation loss: 2.414496855069232

Epoch: 5| Step: 6
Training loss: 2.193474531173706
Validation loss: 2.4134940434527654

Epoch: 5| Step: 7
Training loss: 2.4654788970947266
Validation loss: 2.4134415734198784

Epoch: 5| Step: 8
Training loss: 2.6817002296447754
Validation loss: 2.4075023679323095

Epoch: 5| Step: 9
Training loss: 2.483311176300049
Validation loss: 2.409959687981554

Epoch: 5| Step: 10
Training loss: 2.188079357147217
Validation loss: 2.4068051717614614

Epoch: 223| Step: 0
Training loss: 2.6711316108703613
Validation loss: 2.4073441977142007

Epoch: 5| Step: 1
Training loss: 2.38386607170105
Validation loss: 2.419227102751373

Epoch: 5| Step: 2
Training loss: 3.372140407562256
Validation loss: 2.4124130638696815

Epoch: 5| Step: 3
Training loss: 2.116989850997925
Validation loss: 2.421697457631429

Epoch: 5| Step: 4
Training loss: 2.49300217628479
Validation loss: 2.4268702178873043

Epoch: 5| Step: 5
Training loss: 2.0328762531280518
Validation loss: 2.424072160515734

Epoch: 5| Step: 6
Training loss: 2.9197089672088623
Validation loss: 2.432230769947011

Epoch: 5| Step: 7
Training loss: 2.7822937965393066
Validation loss: 2.4454541283269084

Epoch: 5| Step: 8
Training loss: 2.3900065422058105
Validation loss: 2.4321460134239605

Epoch: 5| Step: 9
Training loss: 2.7192771434783936
Validation loss: 2.4314998631836264

Epoch: 5| Step: 10
Training loss: 2.7496800422668457
Validation loss: 2.4424185317049742

Epoch: 224| Step: 0
Training loss: 2.351712226867676
Validation loss: 2.4471756976137877

Epoch: 5| Step: 1
Training loss: 3.2191824913024902
Validation loss: 2.446276803170481

Epoch: 5| Step: 2
Training loss: 2.828136920928955
Validation loss: 2.4233997688498548

Epoch: 5| Step: 3
Training loss: 2.409557819366455
Validation loss: 2.4093750420437066

Epoch: 5| Step: 4
Training loss: 2.6182470321655273
Validation loss: 2.3920951825316235

Epoch: 5| Step: 5
Training loss: 2.2637581825256348
Validation loss: 2.4030021570062123

Epoch: 5| Step: 6
Training loss: 3.6982250213623047
Validation loss: 2.4105234069208943

Epoch: 5| Step: 7
Training loss: 2.2107291221618652
Validation loss: 2.409261765018586

Epoch: 5| Step: 8
Training loss: 2.6126904487609863
Validation loss: 2.4033856417543147

Epoch: 5| Step: 9
Training loss: 2.529125690460205
Validation loss: 2.40174445029228

Epoch: 5| Step: 10
Training loss: 1.8629701137542725
Validation loss: 2.4028719061164447

Epoch: 225| Step: 0
Training loss: 2.599618911743164
Validation loss: 2.3990085458242767

Epoch: 5| Step: 1
Training loss: 2.1398110389709473
Validation loss: 2.40442031557842

Epoch: 5| Step: 2
Training loss: 3.0839219093322754
Validation loss: 2.4009858921010006

Epoch: 5| Step: 3
Training loss: 2.9424736499786377
Validation loss: 2.394277575195477

Epoch: 5| Step: 4
Training loss: 2.7367236614227295
Validation loss: 2.393138588115733

Epoch: 5| Step: 5
Training loss: 2.165748119354248
Validation loss: 2.3940524388385076

Epoch: 5| Step: 6
Training loss: 2.4165759086608887
Validation loss: 2.392624560222831

Epoch: 5| Step: 7
Training loss: 2.451892375946045
Validation loss: 2.395299529516569

Epoch: 5| Step: 8
Training loss: 3.1329805850982666
Validation loss: 2.3998149953862673

Epoch: 5| Step: 9
Training loss: 2.336610794067383
Validation loss: 2.4023444550011748

Epoch: 5| Step: 10
Training loss: 2.669151544570923
Validation loss: 2.4064172006422475

Epoch: 226| Step: 0
Training loss: 2.715348482131958
Validation loss: 2.4131537047765588

Epoch: 5| Step: 1
Training loss: 2.2276926040649414
Validation loss: 2.412895643582908

Epoch: 5| Step: 2
Training loss: 2.971907138824463
Validation loss: 2.419843319923647

Epoch: 5| Step: 3
Training loss: 2.1722774505615234
Validation loss: 2.4155138179820073

Epoch: 5| Step: 4
Training loss: 2.3532567024230957
Validation loss: 2.4181574326689526

Epoch: 5| Step: 5
Training loss: 3.009761095046997
Validation loss: 2.410310305574889

Epoch: 5| Step: 6
Training loss: 2.345238208770752
Validation loss: 2.398427168528239

Epoch: 5| Step: 7
Training loss: 3.1643567085266113
Validation loss: 2.396383364995321

Epoch: 5| Step: 8
Training loss: 2.253458023071289
Validation loss: 2.4042636399628012

Epoch: 5| Step: 9
Training loss: 2.2551770210266113
Validation loss: 2.4004378446968655

Epoch: 5| Step: 10
Training loss: 3.286318302154541
Validation loss: 2.3990446957208778

Epoch: 227| Step: 0
Training loss: 1.790897011756897
Validation loss: 2.407983467143069

Epoch: 5| Step: 1
Training loss: 2.671057939529419
Validation loss: 2.403573982177242

Epoch: 5| Step: 2
Training loss: 2.2888457775115967
Validation loss: 2.39831603470669

Epoch: 5| Step: 3
Training loss: 2.7897896766662598
Validation loss: 2.398290567500617

Epoch: 5| Step: 4
Training loss: 2.783182382583618
Validation loss: 2.4010057295522382

Epoch: 5| Step: 5
Training loss: 2.737727642059326
Validation loss: 2.4000341020604616

Epoch: 5| Step: 6
Training loss: 3.0652661323547363
Validation loss: 2.398512701834402

Epoch: 5| Step: 7
Training loss: 2.0607218742370605
Validation loss: 2.4015298915165726

Epoch: 5| Step: 8
Training loss: 2.957465887069702
Validation loss: 2.400891820589701

Epoch: 5| Step: 9
Training loss: 3.0486550331115723
Validation loss: 2.3998327332158245

Epoch: 5| Step: 10
Training loss: 2.357358694076538
Validation loss: 2.4098234586818243

Epoch: 228| Step: 0
Training loss: 2.3674988746643066
Validation loss: 2.411460227863763

Epoch: 5| Step: 1
Training loss: 2.2614493370056152
Validation loss: 2.4078898276052167

Epoch: 5| Step: 2
Training loss: 2.5799546241760254
Validation loss: 2.4137088483379734

Epoch: 5| Step: 3
Training loss: 2.1916251182556152
Validation loss: 2.413666248321533

Epoch: 5| Step: 4
Training loss: 2.8030858039855957
Validation loss: 2.418328035262323

Epoch: 5| Step: 5
Training loss: 2.446044445037842
Validation loss: 2.41276051152137

Epoch: 5| Step: 6
Training loss: 2.545806407928467
Validation loss: 2.410105659115699

Epoch: 5| Step: 7
Training loss: 2.962742328643799
Validation loss: 2.4115887944416334

Epoch: 5| Step: 8
Training loss: 2.8781700134277344
Validation loss: 2.402508251128658

Epoch: 5| Step: 9
Training loss: 2.7511372566223145
Validation loss: 2.4023327237816265

Epoch: 5| Step: 10
Training loss: 2.8552582263946533
Validation loss: 2.3970038660110964

Epoch: 229| Step: 0
Training loss: 2.7824268341064453
Validation loss: 2.4015808797651723

Epoch: 5| Step: 1
Training loss: 2.7973663806915283
Validation loss: 2.3991670634156916

Epoch: 5| Step: 2
Training loss: 2.318528652191162
Validation loss: 2.402750410059447

Epoch: 5| Step: 3
Training loss: 2.250577211380005
Validation loss: 2.4046399362625612

Epoch: 5| Step: 4
Training loss: 2.540714740753174
Validation loss: 2.4060007654210573

Epoch: 5| Step: 5
Training loss: 2.417752742767334
Validation loss: 2.418561663678897

Epoch: 5| Step: 6
Training loss: 2.518690586090088
Validation loss: 2.4090203444163003

Epoch: 5| Step: 7
Training loss: 3.0750832557678223
Validation loss: 2.418893990978118

Epoch: 5| Step: 8
Training loss: 2.1995410919189453
Validation loss: 2.419173361152731

Epoch: 5| Step: 9
Training loss: 3.194411277770996
Validation loss: 2.417473719966027

Epoch: 5| Step: 10
Training loss: 2.4551312923431396
Validation loss: 2.413239332937425

Epoch: 230| Step: 0
Training loss: 2.5945892333984375
Validation loss: 2.406625634880476

Epoch: 5| Step: 1
Training loss: 2.864138603210449
Validation loss: 2.4138965375961794

Epoch: 5| Step: 2
Training loss: 2.584135055541992
Validation loss: 2.404643443322951

Epoch: 5| Step: 3
Training loss: 2.1232821941375732
Validation loss: 2.403866106464017

Epoch: 5| Step: 4
Training loss: 2.367241382598877
Validation loss: 2.3959062663457726

Epoch: 5| Step: 5
Training loss: 2.9806816577911377
Validation loss: 2.39835609415526

Epoch: 5| Step: 6
Training loss: 2.8473944664001465
Validation loss: 2.3971448021550334

Epoch: 5| Step: 7
Training loss: 3.09177827835083
Validation loss: 2.399273782648066

Epoch: 5| Step: 8
Training loss: 2.397087335586548
Validation loss: 2.4002689110335482

Epoch: 5| Step: 9
Training loss: 2.511880874633789
Validation loss: 2.40020889107899

Epoch: 5| Step: 10
Training loss: 2.023574113845825
Validation loss: 2.3998048356784287

Epoch: 231| Step: 0
Training loss: 2.3232839107513428
Validation loss: 2.400768554338845

Epoch: 5| Step: 1
Training loss: 2.177367687225342
Validation loss: 2.405545134698191

Epoch: 5| Step: 2
Training loss: 3.1678943634033203
Validation loss: 2.399130518718432

Epoch: 5| Step: 3
Training loss: 2.9269819259643555
Validation loss: 2.4002393548206618

Epoch: 5| Step: 4
Training loss: 2.727095365524292
Validation loss: 2.407297136963055

Epoch: 5| Step: 5
Training loss: 2.701906204223633
Validation loss: 2.4006441024041947

Epoch: 5| Step: 6
Training loss: 2.5042083263397217
Validation loss: 2.402743015238034

Epoch: 5| Step: 7
Training loss: 2.3878426551818848
Validation loss: 2.4045981386656403

Epoch: 5| Step: 8
Training loss: 2.7146804332733154
Validation loss: 2.411925931130686

Epoch: 5| Step: 9
Training loss: 2.3945024013519287
Validation loss: 2.4060727421955397

Epoch: 5| Step: 10
Training loss: 2.4768595695495605
Validation loss: 2.4106762178482546

Epoch: 232| Step: 0
Training loss: 1.9447190761566162
Validation loss: 2.3977545179346555

Epoch: 5| Step: 1
Training loss: 2.8461194038391113
Validation loss: 2.402011445773545

Epoch: 5| Step: 2
Training loss: 2.559825897216797
Validation loss: 2.4078704157183246

Epoch: 5| Step: 3
Training loss: 2.5384109020233154
Validation loss: 2.3961084683736167

Epoch: 5| Step: 4
Training loss: 2.35931658744812
Validation loss: 2.3988656049133628

Epoch: 5| Step: 5
Training loss: 2.897045135498047
Validation loss: 2.3930627863894225

Epoch: 5| Step: 6
Training loss: 2.575681686401367
Validation loss: 2.39785203882443

Epoch: 5| Step: 7
Training loss: 2.7865970134735107
Validation loss: 2.399729013442993

Epoch: 5| Step: 8
Training loss: 2.9522669315338135
Validation loss: 2.398662926048361

Epoch: 5| Step: 9
Training loss: 2.2292613983154297
Validation loss: 2.3891486378126245

Epoch: 5| Step: 10
Training loss: 2.8254213333129883
Validation loss: 2.386901601668327

Epoch: 233| Step: 0
Training loss: 2.733180522918701
Validation loss: 2.3930992387956187

Epoch: 5| Step: 1
Training loss: 2.781980037689209
Validation loss: 2.393152383065993

Epoch: 5| Step: 2
Training loss: 2.4981396198272705
Validation loss: 2.3835301399230957

Epoch: 5| Step: 3
Training loss: 2.2482099533081055
Validation loss: 2.3903902730634137

Epoch: 5| Step: 4
Training loss: 2.413567066192627
Validation loss: 2.392360746219594

Epoch: 5| Step: 5
Training loss: 3.2718429565429688
Validation loss: 2.3879483515216458

Epoch: 5| Step: 6
Training loss: 2.9079995155334473
Validation loss: 2.3908058238285843

Epoch: 5| Step: 7
Training loss: 2.723994016647339
Validation loss: 2.38976954131998

Epoch: 5| Step: 8
Training loss: 2.375561475753784
Validation loss: 2.3877330005809827

Epoch: 5| Step: 9
Training loss: 2.2900681495666504
Validation loss: 2.389438675295922

Epoch: 5| Step: 10
Training loss: 2.1934878826141357
Validation loss: 2.3822075243919127

Epoch: 234| Step: 0
Training loss: 2.32797908782959
Validation loss: 2.391146786751286

Epoch: 5| Step: 1
Training loss: 2.2867252826690674
Validation loss: 2.385139396113734

Epoch: 5| Step: 2
Training loss: 2.3347880840301514
Validation loss: 2.3898032737034622

Epoch: 5| Step: 3
Training loss: 2.9789047241210938
Validation loss: 2.394793733473747

Epoch: 5| Step: 4
Training loss: 2.8951427936553955
Validation loss: 2.39298947652181

Epoch: 5| Step: 5
Training loss: 2.90385365486145
Validation loss: 2.399405728104294

Epoch: 5| Step: 6
Training loss: 2.0933966636657715
Validation loss: 2.3972368676175355

Epoch: 5| Step: 7
Training loss: 2.5086917877197266
Validation loss: 2.4047379801350255

Epoch: 5| Step: 8
Training loss: 3.193155288696289
Validation loss: 2.4109765893669537

Epoch: 5| Step: 9
Training loss: 2.5708954334259033
Validation loss: 2.4088989509049283

Epoch: 5| Step: 10
Training loss: 2.3209760189056396
Validation loss: 2.4180820885524956

Epoch: 235| Step: 0
Training loss: 2.4330742359161377
Validation loss: 2.411992016658988

Epoch: 5| Step: 1
Training loss: 2.9633328914642334
Validation loss: 2.4162216494160313

Epoch: 5| Step: 2
Training loss: 2.723543643951416
Validation loss: 2.4154180685679116

Epoch: 5| Step: 3
Training loss: 3.116520881652832
Validation loss: 2.4140973629490023

Epoch: 5| Step: 4
Training loss: 2.1697802543640137
Validation loss: 2.411123380866102

Epoch: 5| Step: 5
Training loss: 2.0676732063293457
Validation loss: 2.393630317462388

Epoch: 5| Step: 6
Training loss: 2.7948784828186035
Validation loss: 2.391875828466108

Epoch: 5| Step: 7
Training loss: 3.000478982925415
Validation loss: 2.3891047585395073

Epoch: 5| Step: 8
Training loss: 2.490719795227051
Validation loss: 2.388310552925192

Epoch: 5| Step: 9
Training loss: 2.669731378555298
Validation loss: 2.3922181539638068

Epoch: 5| Step: 10
Training loss: 1.887395977973938
Validation loss: 2.381410778209727

Epoch: 236| Step: 0
Training loss: 3.2671897411346436
Validation loss: 2.3867302453646095

Epoch: 5| Step: 1
Training loss: 2.4872078895568848
Validation loss: 2.381744623184204

Epoch: 5| Step: 2
Training loss: 2.372835636138916
Validation loss: 2.3865161300987325

Epoch: 5| Step: 3
Training loss: 2.4010329246520996
Validation loss: 2.3961876156509563

Epoch: 5| Step: 4
Training loss: 2.942410707473755
Validation loss: 2.4016708891878844

Epoch: 5| Step: 5
Training loss: 1.734130620956421
Validation loss: 2.4028750568307857

Epoch: 5| Step: 6
Training loss: 3.2147490978240967
Validation loss: 2.4097453291698168

Epoch: 5| Step: 7
Training loss: 2.807114601135254
Validation loss: 2.4106462309437413

Epoch: 5| Step: 8
Training loss: 2.3827919960021973
Validation loss: 2.4052442940332557

Epoch: 5| Step: 9
Training loss: 2.252725124359131
Validation loss: 2.3884715469934608

Epoch: 5| Step: 10
Training loss: 2.6211986541748047
Validation loss: 2.392030046832177

Epoch: 237| Step: 0
Training loss: 2.3260622024536133
Validation loss: 2.389808826549079

Epoch: 5| Step: 1
Training loss: 2.450374126434326
Validation loss: 2.3881198872802076

Epoch: 5| Step: 2
Training loss: 2.752495527267456
Validation loss: 2.3877534904787616

Epoch: 5| Step: 3
Training loss: 2.7715487480163574
Validation loss: 2.3840392404986965

Epoch: 5| Step: 4
Training loss: 2.616455554962158
Validation loss: 2.3835183599943757

Epoch: 5| Step: 5
Training loss: 2.4221649169921875
Validation loss: 2.3882929137957993

Epoch: 5| Step: 6
Training loss: 2.719416618347168
Validation loss: 2.3927665013138966

Epoch: 5| Step: 7
Training loss: 2.5179238319396973
Validation loss: 2.3931649243959816

Epoch: 5| Step: 8
Training loss: 2.0844051837921143
Validation loss: 2.3973151419752385

Epoch: 5| Step: 9
Training loss: 2.895826816558838
Validation loss: 2.402173314043271

Epoch: 5| Step: 10
Training loss: 2.916773796081543
Validation loss: 2.4010227957079486

Epoch: 238| Step: 0
Training loss: 2.768430233001709
Validation loss: 2.4098310393671833

Epoch: 5| Step: 1
Training loss: 2.4724678993225098
Validation loss: 2.4110800989212526

Epoch: 5| Step: 2
Training loss: 1.84286367893219
Validation loss: 2.4061237612078266

Epoch: 5| Step: 3
Training loss: 2.3861751556396484
Validation loss: 2.4050049012707126

Epoch: 5| Step: 4
Training loss: 2.91892671585083
Validation loss: 2.4150274492079213

Epoch: 5| Step: 5
Training loss: 2.4028830528259277
Validation loss: 2.4052824153695056

Epoch: 5| Step: 6
Training loss: 2.3966822624206543
Validation loss: 2.41540664498524

Epoch: 5| Step: 7
Training loss: 2.540419101715088
Validation loss: 2.402012945503317

Epoch: 5| Step: 8
Training loss: 2.951392412185669
Validation loss: 2.400230089823405

Epoch: 5| Step: 9
Training loss: 2.664090633392334
Validation loss: 2.4036902919892342

Epoch: 5| Step: 10
Training loss: 3.118015766143799
Validation loss: 2.4044629835313365

Epoch: 239| Step: 0
Training loss: 3.005727767944336
Validation loss: 2.3980389692450084

Epoch: 5| Step: 1
Training loss: 2.610403537750244
Validation loss: 2.3984216079917005

Epoch: 5| Step: 2
Training loss: 3.207742214202881
Validation loss: 2.4012314119646625

Epoch: 5| Step: 3
Training loss: 2.186443328857422
Validation loss: 2.395850996817312

Epoch: 5| Step: 4
Training loss: 2.206590414047241
Validation loss: 2.3955686964014524

Epoch: 5| Step: 5
Training loss: 2.68154239654541
Validation loss: 2.404056078644209

Epoch: 5| Step: 6
Training loss: 2.4128594398498535
Validation loss: 2.4062827428181968

Epoch: 5| Step: 7
Training loss: 2.20331072807312
Validation loss: 2.3981559558581282

Epoch: 5| Step: 8
Training loss: 2.309584856033325
Validation loss: 2.3986171342993297

Epoch: 5| Step: 9
Training loss: 3.052645206451416
Validation loss: 2.4005047275174047

Epoch: 5| Step: 10
Training loss: 2.4544389247894287
Validation loss: 2.3976247297820223

Epoch: 240| Step: 0
Training loss: 3.2147209644317627
Validation loss: 2.3847455542574645

Epoch: 5| Step: 1
Training loss: 2.815397262573242
Validation loss: 2.388608535130819

Epoch: 5| Step: 2
Training loss: 2.5799717903137207
Validation loss: 2.39685470058072

Epoch: 5| Step: 3
Training loss: 1.8169605731964111
Validation loss: 2.396423860262799

Epoch: 5| Step: 4
Training loss: 3.0536441802978516
Validation loss: 2.394399345562022

Epoch: 5| Step: 5
Training loss: 2.334014415740967
Validation loss: 2.4016171706620084

Epoch: 5| Step: 6
Training loss: 2.5566296577453613
Validation loss: 2.3965311768234416

Epoch: 5| Step: 7
Training loss: 2.693816661834717
Validation loss: 2.399888451381396

Epoch: 5| Step: 8
Training loss: 2.2415413856506348
Validation loss: 2.406366435430383

Epoch: 5| Step: 9
Training loss: 2.78080153465271
Validation loss: 2.3972260900723037

Epoch: 5| Step: 10
Training loss: 2.1823554039001465
Validation loss: 2.3930981159210205

Epoch: 241| Step: 0
Training loss: 3.160144090652466
Validation loss: 2.3927939963597122

Epoch: 5| Step: 1
Training loss: 2.8865249156951904
Validation loss: 2.3931213040505686

Epoch: 5| Step: 2
Training loss: 2.6443424224853516
Validation loss: 2.3878932819571546

Epoch: 5| Step: 3
Training loss: 2.079699993133545
Validation loss: 2.3737422343223327

Epoch: 5| Step: 4
Training loss: 2.580005645751953
Validation loss: 2.379714826101898

Epoch: 5| Step: 5
Training loss: 2.7615694999694824
Validation loss: 2.375105848876379

Epoch: 5| Step: 6
Training loss: 2.1250813007354736
Validation loss: 2.3809782048707366

Epoch: 5| Step: 7
Training loss: 2.6430375576019287
Validation loss: 2.3900119361057075

Epoch: 5| Step: 8
Training loss: 2.128174304962158
Validation loss: 2.3866371236821657

Epoch: 5| Step: 9
Training loss: 2.6415514945983887
Validation loss: 2.3997553625414447

Epoch: 5| Step: 10
Training loss: 2.799633264541626
Validation loss: 2.407744515326715

Epoch: 242| Step: 0
Training loss: 2.544449806213379
Validation loss: 2.4074183356377388

Epoch: 5| Step: 1
Training loss: 3.0341687202453613
Validation loss: 2.400425005984563

Epoch: 5| Step: 2
Training loss: 2.4631800651550293
Validation loss: 2.4021834032509917

Epoch: 5| Step: 3
Training loss: 2.166041612625122
Validation loss: 2.4176183131433304

Epoch: 5| Step: 4
Training loss: 2.862027883529663
Validation loss: 2.4041755481432845

Epoch: 5| Step: 5
Training loss: 1.936396837234497
Validation loss: 2.398526671112225

Epoch: 5| Step: 6
Training loss: 2.983952283859253
Validation loss: 2.4141750515148206

Epoch: 5| Step: 7
Training loss: 2.4626617431640625
Validation loss: 2.408170377054522

Epoch: 5| Step: 8
Training loss: 2.5407309532165527
Validation loss: 2.4038188662580264

Epoch: 5| Step: 9
Training loss: 2.8138465881347656
Validation loss: 2.406001626804311

Epoch: 5| Step: 10
Training loss: 2.5849952697753906
Validation loss: 2.3964507759258313

Epoch: 243| Step: 0
Training loss: 2.6042699813842773
Validation loss: 2.395019062103764

Epoch: 5| Step: 1
Training loss: 2.1814427375793457
Validation loss: 2.385068255086099

Epoch: 5| Step: 2
Training loss: 2.6685051918029785
Validation loss: 2.3814493968922603

Epoch: 5| Step: 3
Training loss: 2.1520814895629883
Validation loss: 2.3743829368263163

Epoch: 5| Step: 4
Training loss: 1.9691963195800781
Validation loss: 2.3803902902910785

Epoch: 5| Step: 5
Training loss: 2.5703959465026855
Validation loss: 2.383242509698355

Epoch: 5| Step: 6
Training loss: 3.0992462635040283
Validation loss: 2.392141008889803

Epoch: 5| Step: 7
Training loss: 2.927917718887329
Validation loss: 2.406794564698332

Epoch: 5| Step: 8
Training loss: 2.775526523590088
Validation loss: 2.3938883299468667

Epoch: 5| Step: 9
Training loss: 2.653547525405884
Validation loss: 2.391133928811678

Epoch: 5| Step: 10
Training loss: 2.7355263233184814
Validation loss: 2.3834246486745854

Epoch: 244| Step: 0
Training loss: 1.8592822551727295
Validation loss: 2.399812577873148

Epoch: 5| Step: 1
Training loss: 2.7573370933532715
Validation loss: 2.399289959220476

Epoch: 5| Step: 2
Training loss: 2.956528663635254
Validation loss: 2.4049077162178616

Epoch: 5| Step: 3
Training loss: 3.1066246032714844
Validation loss: 2.4201418738211355

Epoch: 5| Step: 4
Training loss: 2.5358221530914307
Validation loss: 2.414159442788811

Epoch: 5| Step: 5
Training loss: 2.4400129318237305
Validation loss: 2.4210952481915875

Epoch: 5| Step: 6
Training loss: 1.8380340337753296
Validation loss: 2.414035092117966

Epoch: 5| Step: 7
Training loss: 2.489776611328125
Validation loss: 2.416692728637367

Epoch: 5| Step: 8
Training loss: 2.9232289791107178
Validation loss: 2.4066053334102837

Epoch: 5| Step: 9
Training loss: 2.443481683731079
Validation loss: 2.398814332100653

Epoch: 5| Step: 10
Training loss: 3.095045804977417
Validation loss: 2.4018239769884335

Epoch: 245| Step: 0
Training loss: 2.8595595359802246
Validation loss: 2.398652171575895

Epoch: 5| Step: 1
Training loss: 2.537031412124634
Validation loss: 2.4001933425985356

Epoch: 5| Step: 2
Training loss: 2.661795139312744
Validation loss: 2.383697678965907

Epoch: 5| Step: 3
Training loss: 2.3503801822662354
Validation loss: 2.381153529690158

Epoch: 5| Step: 4
Training loss: 2.4075679779052734
Validation loss: 2.371891344747236

Epoch: 5| Step: 5
Training loss: 2.5282516479492188
Validation loss: 2.382111385304441

Epoch: 5| Step: 6
Training loss: 2.852940082550049
Validation loss: 2.3684694907998525

Epoch: 5| Step: 7
Training loss: 2.411278486251831
Validation loss: 2.373024312398767

Epoch: 5| Step: 8
Training loss: 2.481271266937256
Validation loss: 2.37164395342591

Epoch: 5| Step: 9
Training loss: 2.7785143852233887
Validation loss: 2.3807728777649584

Epoch: 5| Step: 10
Training loss: 2.4355216026306152
Validation loss: 2.373089426307268

Epoch: 246| Step: 0
Training loss: 2.934631824493408
Validation loss: 2.371188150939121

Epoch: 5| Step: 1
Training loss: 2.212170124053955
Validation loss: 2.3819061556170062

Epoch: 5| Step: 2
Training loss: 2.7413458824157715
Validation loss: 2.3819292591464136

Epoch: 5| Step: 3
Training loss: 2.884377956390381
Validation loss: 2.371140859460318

Epoch: 5| Step: 4
Training loss: 2.385918140411377
Validation loss: 2.3776438518237044

Epoch: 5| Step: 5
Training loss: 1.8136417865753174
Validation loss: 2.3848360302627727

Epoch: 5| Step: 6
Training loss: 2.9716906547546387
Validation loss: 2.3905494572013937

Epoch: 5| Step: 7
Training loss: 2.7089571952819824
Validation loss: 2.386539682265251

Epoch: 5| Step: 8
Training loss: 2.4190945625305176
Validation loss: 2.3897955661178916

Epoch: 5| Step: 9
Training loss: 3.5251946449279785
Validation loss: 2.399824680820588

Epoch: 5| Step: 10
Training loss: 1.5136442184448242
Validation loss: 2.3865557114283242

Epoch: 247| Step: 0
Training loss: 2.381432294845581
Validation loss: 2.3897081036721506

Epoch: 5| Step: 1
Training loss: 2.299661874771118
Validation loss: 2.3816379475337204

Epoch: 5| Step: 2
Training loss: 2.162816047668457
Validation loss: 2.3950334056731193

Epoch: 5| Step: 3
Training loss: 2.2512805461883545
Validation loss: 2.393056390106037

Epoch: 5| Step: 4
Training loss: 3.7764573097229004
Validation loss: 2.4002268006724696

Epoch: 5| Step: 5
Training loss: 3.2466483116149902
Validation loss: 2.4055536229123353

Epoch: 5| Step: 6
Training loss: 2.2912802696228027
Validation loss: 2.4101544810879614

Epoch: 5| Step: 7
Training loss: 3.18329119682312
Validation loss: 2.422974463432066

Epoch: 5| Step: 8
Training loss: 1.872370958328247
Validation loss: 2.4198421714126424

Epoch: 5| Step: 9
Training loss: 2.4456727504730225
Validation loss: 2.4085827950508363

Epoch: 5| Step: 10
Training loss: 2.3744304180145264
Validation loss: 2.401399068934943

Epoch: 248| Step: 0
Training loss: 2.2860052585601807
Validation loss: 2.395689550266471

Epoch: 5| Step: 1
Training loss: 2.5578410625457764
Validation loss: 2.39620905025031

Epoch: 5| Step: 2
Training loss: 3.3942813873291016
Validation loss: 2.398200140204481

Epoch: 5| Step: 3
Training loss: 2.414576530456543
Validation loss: 2.38344233523133

Epoch: 5| Step: 4
Training loss: 2.4607391357421875
Validation loss: 2.3871623777574107

Epoch: 5| Step: 5
Training loss: 2.1942138671875
Validation loss: 2.3814086247515935

Epoch: 5| Step: 6
Training loss: 2.3825600147247314
Validation loss: 2.3799227386392574

Epoch: 5| Step: 7
Training loss: 2.620575428009033
Validation loss: 2.3705718748031126

Epoch: 5| Step: 8
Training loss: 2.6693003177642822
Validation loss: 2.379741843028735

Epoch: 5| Step: 9
Training loss: 2.619870662689209
Validation loss: 2.3772273294387327

Epoch: 5| Step: 10
Training loss: 2.563882350921631
Validation loss: 2.3723418097342215

Epoch: 249| Step: 0
Training loss: 2.9308295249938965
Validation loss: 2.3703006621330016

Epoch: 5| Step: 1
Training loss: 2.0454773902893066
Validation loss: 2.385415636083131

Epoch: 5| Step: 2
Training loss: 3.1580843925476074
Validation loss: 2.3829934007378033

Epoch: 5| Step: 3
Training loss: 2.4551515579223633
Validation loss: 2.3994438648223877

Epoch: 5| Step: 4
Training loss: 1.7220786809921265
Validation loss: 2.4152460995540825

Epoch: 5| Step: 5
Training loss: 3.1137752532958984
Validation loss: 2.4326314131418862

Epoch: 5| Step: 6
Training loss: 2.7888407707214355
Validation loss: 2.4320106480711248

Epoch: 5| Step: 7
Training loss: 2.237302780151367
Validation loss: 2.4434288906794723

Epoch: 5| Step: 8
Training loss: 2.4042677879333496
Validation loss: 2.432330740395413

Epoch: 5| Step: 9
Training loss: 2.7221717834472656
Validation loss: 2.4420371055603027

Epoch: 5| Step: 10
Training loss: 2.80499529838562
Validation loss: 2.454165981661889

Epoch: 250| Step: 0
Training loss: 2.681227445602417
Validation loss: 2.4530901755056074

Epoch: 5| Step: 1
Training loss: 2.636643648147583
Validation loss: 2.450295417539535

Epoch: 5| Step: 2
Training loss: 3.0600147247314453
Validation loss: 2.4302248083135134

Epoch: 5| Step: 3
Training loss: 2.9739348888397217
Validation loss: 2.4108922558446086

Epoch: 5| Step: 4
Training loss: 2.015618085861206
Validation loss: 2.3903877863319973

Epoch: 5| Step: 5
Training loss: 2.2918808460235596
Validation loss: 2.382771740677536

Epoch: 5| Step: 6
Training loss: 2.662978410720825
Validation loss: 2.3802130171047744

Epoch: 5| Step: 7
Training loss: 2.9386589527130127
Validation loss: 2.3812208316659413

Epoch: 5| Step: 8
Training loss: 2.330479145050049
Validation loss: 2.371562277117083

Epoch: 5| Step: 9
Training loss: 2.628264904022217
Validation loss: 2.369432782614103

Epoch: 5| Step: 10
Training loss: 2.176037073135376
Validation loss: 2.372123092733404

Epoch: 251| Step: 0
Training loss: 2.3501739501953125
Validation loss: 2.367024003818471

Epoch: 5| Step: 1
Training loss: 2.418696641921997
Validation loss: 2.3697969862209853

Epoch: 5| Step: 2
Training loss: 3.3839638233184814
Validation loss: 2.3604797291499313

Epoch: 5| Step: 3
Training loss: 2.445690393447876
Validation loss: 2.3709582769742577

Epoch: 5| Step: 4
Training loss: 2.681525945663452
Validation loss: 2.3613250024857058

Epoch: 5| Step: 5
Training loss: 2.7739765644073486
Validation loss: 2.3627538245211364

Epoch: 5| Step: 6
Training loss: 2.9116530418395996
Validation loss: 2.361653830415459

Epoch: 5| Step: 7
Training loss: 2.993048906326294
Validation loss: 2.3745882306047665

Epoch: 5| Step: 8
Training loss: 2.2748751640319824
Validation loss: 2.3565780834485124

Epoch: 5| Step: 9
Training loss: 1.9888858795166016
Validation loss: 2.3662121577929427

Epoch: 5| Step: 10
Training loss: 1.9191312789916992
Validation loss: 2.3553757718814317

Epoch: 252| Step: 0
Training loss: 1.9457086324691772
Validation loss: 2.376637271655503

Epoch: 5| Step: 1
Training loss: 2.398498058319092
Validation loss: 2.3754242953433784

Epoch: 5| Step: 2
Training loss: 2.844102144241333
Validation loss: 2.3962669141830935

Epoch: 5| Step: 3
Training loss: 2.489635944366455
Validation loss: 2.392673400140578

Epoch: 5| Step: 4
Training loss: 2.945530891418457
Validation loss: 2.415996725841235

Epoch: 5| Step: 5
Training loss: 2.270930528640747
Validation loss: 2.4062296780206824

Epoch: 5| Step: 6
Training loss: 2.638535737991333
Validation loss: 2.4045617759868665

Epoch: 5| Step: 7
Training loss: 3.3680686950683594
Validation loss: 2.395237607340659

Epoch: 5| Step: 8
Training loss: 2.3032469749450684
Validation loss: 2.382919637105798

Epoch: 5| Step: 9
Training loss: 2.4028327465057373
Validation loss: 2.3704450207371868

Epoch: 5| Step: 10
Training loss: 2.7357828617095947
Validation loss: 2.372490116344985

Epoch: 253| Step: 0
Training loss: 2.9681637287139893
Validation loss: 2.373380717410836

Epoch: 5| Step: 1
Training loss: 2.468693494796753
Validation loss: 2.3846180720995833

Epoch: 5| Step: 2
Training loss: 2.3675622940063477
Validation loss: 2.385538080687164

Epoch: 5| Step: 3
Training loss: 2.6018424034118652
Validation loss: 2.380444147253549

Epoch: 5| Step: 4
Training loss: 2.618060350418091
Validation loss: 2.364224749226724

Epoch: 5| Step: 5
Training loss: 2.8383002281188965
Validation loss: 2.3649449220267673

Epoch: 5| Step: 6
Training loss: 2.1065146923065186
Validation loss: 2.368596246165614

Epoch: 5| Step: 7
Training loss: 2.746626615524292
Validation loss: 2.388796701226183

Epoch: 5| Step: 8
Training loss: 2.7362277507781982
Validation loss: 2.3943068699170182

Epoch: 5| Step: 9
Training loss: 2.508077621459961
Validation loss: 2.396670836274342

Epoch: 5| Step: 10
Training loss: 2.3029112815856934
Validation loss: 2.3932773605469735

Epoch: 254| Step: 0
Training loss: 2.342862367630005
Validation loss: 2.3927199686727216

Epoch: 5| Step: 1
Training loss: 2.3357434272766113
Validation loss: 2.382225158394024

Epoch: 5| Step: 2
Training loss: 2.019731044769287
Validation loss: 2.3917956864962013

Epoch: 5| Step: 3
Training loss: 2.086219310760498
Validation loss: 2.385500177260368

Epoch: 5| Step: 4
Training loss: 2.2795395851135254
Validation loss: 2.378505640132453

Epoch: 5| Step: 5
Training loss: 2.6523447036743164
Validation loss: 2.3843599391239945

Epoch: 5| Step: 6
Training loss: 3.1046640872955322
Validation loss: 2.376948100264354

Epoch: 5| Step: 7
Training loss: 2.704812526702881
Validation loss: 2.3747954112227245

Epoch: 5| Step: 8
Training loss: 3.429877519607544
Validation loss: 2.373413767865909

Epoch: 5| Step: 9
Training loss: 2.9738528728485107
Validation loss: 2.36711637948149

Epoch: 5| Step: 10
Training loss: 2.135300874710083
Validation loss: 2.3715106800038326

Epoch: 255| Step: 0
Training loss: 2.520310163497925
Validation loss: 2.366307945661647

Epoch: 5| Step: 1
Training loss: 3.0334877967834473
Validation loss: 2.3692438858811573

Epoch: 5| Step: 2
Training loss: 2.5391082763671875
Validation loss: 2.365137700111635

Epoch: 5| Step: 3
Training loss: 2.542987823486328
Validation loss: 2.3679068319259153

Epoch: 5| Step: 4
Training loss: 2.5015482902526855
Validation loss: 2.3622190157572427

Epoch: 5| Step: 5
Training loss: 1.5230425596237183
Validation loss: 2.356612156796199

Epoch: 5| Step: 6
Training loss: 2.866201400756836
Validation loss: 2.3674693415241856

Epoch: 5| Step: 7
Training loss: 2.624558687210083
Validation loss: 2.372093495502267

Epoch: 5| Step: 8
Training loss: 2.159133195877075
Validation loss: 2.383741560802665

Epoch: 5| Step: 9
Training loss: 2.7533867359161377
Validation loss: 2.3883490075347242

Epoch: 5| Step: 10
Training loss: 3.1190993785858154
Validation loss: 2.4074286953095467

Epoch: 256| Step: 0
Training loss: 1.746023416519165
Validation loss: 2.397335483181861

Epoch: 5| Step: 1
Training loss: 2.555847644805908
Validation loss: 2.4041075680845525

Epoch: 5| Step: 2
Training loss: 2.785851240158081
Validation loss: 2.3890617855133547

Epoch: 5| Step: 3
Training loss: 3.067145824432373
Validation loss: 2.394881615074732

Epoch: 5| Step: 4
Training loss: 2.679853916168213
Validation loss: 2.3780659552543395

Epoch: 5| Step: 5
Training loss: 2.9175643920898438
Validation loss: 2.374804676219981

Epoch: 5| Step: 6
Training loss: 2.95743465423584
Validation loss: 2.373971200758411

Epoch: 5| Step: 7
Training loss: 2.692335844039917
Validation loss: 2.3734417294943206

Epoch: 5| Step: 8
Training loss: 2.231710910797119
Validation loss: 2.381699300581409

Epoch: 5| Step: 9
Training loss: 2.4516043663024902
Validation loss: 2.382595000728484

Epoch: 5| Step: 10
Training loss: 1.8615061044692993
Validation loss: 2.365197104792441

Epoch: 257| Step: 0
Training loss: 2.2715086936950684
Validation loss: 2.3757363647542973

Epoch: 5| Step: 1
Training loss: 3.3631293773651123
Validation loss: 2.368991464696905

Epoch: 5| Step: 2
Training loss: 2.366758346557617
Validation loss: 2.3727289297247447

Epoch: 5| Step: 3
Training loss: 2.255554676055908
Validation loss: 2.375482287458194

Epoch: 5| Step: 4
Training loss: 2.472965717315674
Validation loss: 2.372940545441002

Epoch: 5| Step: 5
Training loss: 2.9437904357910156
Validation loss: 2.3773437187235844

Epoch: 5| Step: 6
Training loss: 2.4028446674346924
Validation loss: 2.390630870737055

Epoch: 5| Step: 7
Training loss: 2.4864373207092285
Validation loss: 2.3967698543302474

Epoch: 5| Step: 8
Training loss: 2.559276819229126
Validation loss: 2.386403445274599

Epoch: 5| Step: 9
Training loss: 2.578017473220825
Validation loss: 2.4040973776130268

Epoch: 5| Step: 10
Training loss: 2.224950075149536
Validation loss: 2.393793805952995

Epoch: 258| Step: 0
Training loss: 2.6916866302490234
Validation loss: 2.392223778591361

Epoch: 5| Step: 1
Training loss: 2.9018502235412598
Validation loss: 2.385332653599401

Epoch: 5| Step: 2
Training loss: 1.8586845397949219
Validation loss: 2.388082911891322

Epoch: 5| Step: 3
Training loss: 2.5643630027770996
Validation loss: 2.3744078220859652

Epoch: 5| Step: 4
Training loss: 2.1500821113586426
Validation loss: 2.378979270176221

Epoch: 5| Step: 5
Training loss: 2.462376594543457
Validation loss: 2.3784838876416607

Epoch: 5| Step: 6
Training loss: 2.540377140045166
Validation loss: 2.381847099591327

Epoch: 5| Step: 7
Training loss: 2.9363205432891846
Validation loss: 2.366898085481377

Epoch: 5| Step: 8
Training loss: 3.134467363357544
Validation loss: 2.3805369433536323

Epoch: 5| Step: 9
Training loss: 2.239598512649536
Validation loss: 2.382398441273679

Epoch: 5| Step: 10
Training loss: 2.474329710006714
Validation loss: 2.378460466220815

Epoch: 259| Step: 0
Training loss: 3.3085341453552246
Validation loss: 2.366015952120545

Epoch: 5| Step: 1
Training loss: 2.366900682449341
Validation loss: 2.387154907308599

Epoch: 5| Step: 2
Training loss: 3.5705032348632812
Validation loss: 2.379534222746408

Epoch: 5| Step: 3
Training loss: 2.506223678588867
Validation loss: 2.3883003368172595

Epoch: 5| Step: 4
Training loss: 2.1961135864257812
Validation loss: 2.3760231207775813

Epoch: 5| Step: 5
Training loss: 1.6659576892852783
Validation loss: 2.3769386147940033

Epoch: 5| Step: 6
Training loss: 2.7045724391937256
Validation loss: 2.3875925310196413

Epoch: 5| Step: 7
Training loss: 2.7772650718688965
Validation loss: 2.3701006109996507

Epoch: 5| Step: 8
Training loss: 2.106250524520874
Validation loss: 2.3693260428726033

Epoch: 5| Step: 9
Training loss: 2.4501445293426514
Validation loss: 2.3619658895718154

Epoch: 5| Step: 10
Training loss: 2.2895548343658447
Validation loss: 2.3670108190146824

Epoch: 260| Step: 0
Training loss: 3.1879467964172363
Validation loss: 2.3656821353461153

Epoch: 5| Step: 1
Training loss: 2.581233263015747
Validation loss: 2.368030660895891

Epoch: 5| Step: 2
Training loss: 2.846686840057373
Validation loss: 2.3650971433167816

Epoch: 5| Step: 3
Training loss: 1.969056487083435
Validation loss: 2.3745490351030902

Epoch: 5| Step: 4
Training loss: 2.3195273876190186
Validation loss: 2.3716507214371876

Epoch: 5| Step: 5
Training loss: 2.5410068035125732
Validation loss: 2.374524203679895

Epoch: 5| Step: 6
Training loss: 2.148834705352783
Validation loss: 2.372133754914807

Epoch: 5| Step: 7
Training loss: 3.073267936706543
Validation loss: 2.3830721583417667

Epoch: 5| Step: 8
Training loss: 2.479926824569702
Validation loss: 2.3882954479545675

Epoch: 5| Step: 9
Training loss: 1.7313745021820068
Validation loss: 2.3820087294424734

Epoch: 5| Step: 10
Training loss: 3.1677887439727783
Validation loss: 2.388395009502288

Epoch: 261| Step: 0
Training loss: 2.2139105796813965
Validation loss: 2.390749041752149

Epoch: 5| Step: 1
Training loss: 2.9754862785339355
Validation loss: 2.3873608548154115

Epoch: 5| Step: 2
Training loss: 3.1286749839782715
Validation loss: 2.384982442343107

Epoch: 5| Step: 3
Training loss: 1.9498732089996338
Validation loss: 2.385970469444029

Epoch: 5| Step: 4
Training loss: 2.1207292079925537
Validation loss: 2.37205921962697

Epoch: 5| Step: 5
Training loss: 3.3178038597106934
Validation loss: 2.358559534113894

Epoch: 5| Step: 6
Training loss: 2.926666259765625
Validation loss: 2.36119338773912

Epoch: 5| Step: 7
Training loss: 2.2818961143493652
Validation loss: 2.3497923779231247

Epoch: 5| Step: 8
Training loss: 2.2178382873535156
Validation loss: 2.3539536922208724

Epoch: 5| Step: 9
Training loss: 2.2078659534454346
Validation loss: 2.368957675913329

Epoch: 5| Step: 10
Training loss: 2.7020370960235596
Validation loss: 2.360899673995151

Epoch: 262| Step: 0
Training loss: 3.1299314498901367
Validation loss: 2.3664774279440604

Epoch: 5| Step: 1
Training loss: 1.8895232677459717
Validation loss: 2.362552353130874

Epoch: 5| Step: 2
Training loss: 2.349714517593384
Validation loss: 2.358838194160051

Epoch: 5| Step: 3
Training loss: 2.361983060836792
Validation loss: 2.366022676549932

Epoch: 5| Step: 4
Training loss: 2.5790834426879883
Validation loss: 2.368458858100317

Epoch: 5| Step: 5
Training loss: 3.0286858081817627
Validation loss: 2.366317264495357

Epoch: 5| Step: 6
Training loss: 2.534183979034424
Validation loss: 2.363208015759786

Epoch: 5| Step: 7
Training loss: 2.573868751525879
Validation loss: 2.376770945005519

Epoch: 5| Step: 8
Training loss: 2.459618091583252
Validation loss: 2.3870313270117647

Epoch: 5| Step: 9
Training loss: 2.8948614597320557
Validation loss: 2.392983687821255

Epoch: 5| Step: 10
Training loss: 2.1249358654022217
Validation loss: 2.405939752055753

Epoch: 263| Step: 0
Training loss: 2.531226634979248
Validation loss: 2.391083243072674

Epoch: 5| Step: 1
Training loss: 2.3256444931030273
Validation loss: 2.3923898896863385

Epoch: 5| Step: 2
Training loss: 2.091548442840576
Validation loss: 2.3810739363393476

Epoch: 5| Step: 3
Training loss: 2.1012754440307617
Validation loss: 2.370681542222218

Epoch: 5| Step: 4
Training loss: 2.801391124725342
Validation loss: 2.36549227212065

Epoch: 5| Step: 5
Training loss: 2.4231648445129395
Validation loss: 2.350121928799537

Epoch: 5| Step: 6
Training loss: 2.8251538276672363
Validation loss: 2.3550572395324707

Epoch: 5| Step: 7
Training loss: 2.731004238128662
Validation loss: 2.349840102657195

Epoch: 5| Step: 8
Training loss: 3.0851502418518066
Validation loss: 2.356165301415228

Epoch: 5| Step: 9
Training loss: 2.8950586318969727
Validation loss: 2.3487001824122604

Epoch: 5| Step: 10
Training loss: 2.0429537296295166
Validation loss: 2.3584204412275747

Epoch: 264| Step: 0
Training loss: 2.070760726928711
Validation loss: 2.361747593008062

Epoch: 5| Step: 1
Training loss: 1.6105235815048218
Validation loss: 2.3550558026118944

Epoch: 5| Step: 2
Training loss: 2.8091013431549072
Validation loss: 2.362682242547312

Epoch: 5| Step: 3
Training loss: 2.631214141845703
Validation loss: 2.3678803982273227

Epoch: 5| Step: 4
Training loss: 2.864386558532715
Validation loss: 2.385902977758838

Epoch: 5| Step: 5
Training loss: 2.545003652572632
Validation loss: 2.3820547134645524

Epoch: 5| Step: 6
Training loss: 2.750593662261963
Validation loss: 2.389082888121246

Epoch: 5| Step: 7
Training loss: 2.625669002532959
Validation loss: 2.3933863614195134

Epoch: 5| Step: 8
Training loss: 2.710629940032959
Validation loss: 2.3708237704410347

Epoch: 5| Step: 9
Training loss: 2.943002700805664
Validation loss: 2.3751556283684185

Epoch: 5| Step: 10
Training loss: 2.3767166137695312
Validation loss: 2.3590681399068525

Epoch: 265| Step: 0
Training loss: 2.216358184814453
Validation loss: 2.358847725775934

Epoch: 5| Step: 1
Training loss: 3.4676730632781982
Validation loss: 2.3479478436131633

Epoch: 5| Step: 2
Training loss: 2.2639389038085938
Validation loss: 2.3545387778230893

Epoch: 5| Step: 3
Training loss: 2.656397581100464
Validation loss: 2.3430936951791086

Epoch: 5| Step: 4
Training loss: 2.202279567718506
Validation loss: 2.3541247870332453

Epoch: 5| Step: 5
Training loss: 2.164206027984619
Validation loss: 2.3506722680984007

Epoch: 5| Step: 6
Training loss: 3.230313777923584
Validation loss: 2.3441858035261913

Epoch: 5| Step: 7
Training loss: 2.7405521869659424
Validation loss: 2.3442592556758592

Epoch: 5| Step: 8
Training loss: 2.369926929473877
Validation loss: 2.345965805874076

Epoch: 5| Step: 9
Training loss: 2.6255531311035156
Validation loss: 2.363366796124366

Epoch: 5| Step: 10
Training loss: 1.9816246032714844
Validation loss: 2.367536175635553

Epoch: 266| Step: 0
Training loss: 2.6488752365112305
Validation loss: 2.381350613409473

Epoch: 5| Step: 1
Training loss: 2.312899112701416
Validation loss: 2.3659206821072485

Epoch: 5| Step: 2
Training loss: 2.0910747051239014
Validation loss: 2.3749580255118747

Epoch: 5| Step: 3
Training loss: 2.8609237670898438
Validation loss: 2.366988497395669

Epoch: 5| Step: 4
Training loss: 2.4272823333740234
Validation loss: 2.3624761232765774

Epoch: 5| Step: 5
Training loss: 2.9959876537323
Validation loss: 2.3715031262367003

Epoch: 5| Step: 6
Training loss: 1.975057601928711
Validation loss: 2.3703731183082826

Epoch: 5| Step: 7
Training loss: 2.870154857635498
Validation loss: 2.360506747358589

Epoch: 5| Step: 8
Training loss: 2.34808087348938
Validation loss: 2.3708603971747944

Epoch: 5| Step: 9
Training loss: 2.6686205863952637
Validation loss: 2.367284615834554

Epoch: 5| Step: 10
Training loss: 2.9599878787994385
Validation loss: 2.36390858568171

Epoch: 267| Step: 0
Training loss: 1.726190209388733
Validation loss: 2.3626165018286756

Epoch: 5| Step: 1
Training loss: 2.5907974243164062
Validation loss: 2.3708845184695337

Epoch: 5| Step: 2
Training loss: 2.469231128692627
Validation loss: 2.3681962413172566

Epoch: 5| Step: 3
Training loss: 2.2939510345458984
Validation loss: 2.3679821414332234

Epoch: 5| Step: 4
Training loss: 3.1674485206604004
Validation loss: 2.36852886087151

Epoch: 5| Step: 5
Training loss: 2.5687921047210693
Validation loss: 2.377146118430681

Epoch: 5| Step: 6
Training loss: 2.5483603477478027
Validation loss: 2.371280342020014

Epoch: 5| Step: 7
Training loss: 2.2622997760772705
Validation loss: 2.3651387691497803

Epoch: 5| Step: 8
Training loss: 2.592085599899292
Validation loss: 2.369222815318774

Epoch: 5| Step: 9
Training loss: 2.783613920211792
Validation loss: 2.363400466980473

Epoch: 5| Step: 10
Training loss: 3.112051010131836
Validation loss: 2.3577398612935054

Epoch: 268| Step: 0
Training loss: 2.0156259536743164
Validation loss: 2.3660044157376854

Epoch: 5| Step: 1
Training loss: 2.956031560897827
Validation loss: 2.358096371414841

Epoch: 5| Step: 2
Training loss: 3.1138112545013428
Validation loss: 2.363667716262161

Epoch: 5| Step: 3
Training loss: 3.4748923778533936
Validation loss: 2.361224948718984

Epoch: 5| Step: 4
Training loss: 2.792600154876709
Validation loss: 2.3514540644102198

Epoch: 5| Step: 5
Training loss: 2.0391221046447754
Validation loss: 2.36185424558578

Epoch: 5| Step: 6
Training loss: 1.8828233480453491
Validation loss: 2.3487770916313253

Epoch: 5| Step: 7
Training loss: 2.157496213912964
Validation loss: 2.3508247431888374

Epoch: 5| Step: 8
Training loss: 2.3838133811950684
Validation loss: 2.357991236512379

Epoch: 5| Step: 9
Training loss: 2.812793016433716
Validation loss: 2.3636539520755893

Epoch: 5| Step: 10
Training loss: 2.346609115600586
Validation loss: 2.379282433499572

Epoch: 269| Step: 0
Training loss: 2.9468801021575928
Validation loss: 2.377520627872918

Epoch: 5| Step: 1
Training loss: 2.107381820678711
Validation loss: 2.3699163416380524

Epoch: 5| Step: 2
Training loss: 2.292431592941284
Validation loss: 2.3837753777862876

Epoch: 5| Step: 3
Training loss: 2.540313720703125
Validation loss: 2.3730823070772233

Epoch: 5| Step: 4
Training loss: 2.5992660522460938
Validation loss: 2.372077921385406

Epoch: 5| Step: 5
Training loss: 2.582817554473877
Validation loss: 2.3721899268447713

Epoch: 5| Step: 6
Training loss: 1.7557761669158936
Validation loss: 2.366130828857422

Epoch: 5| Step: 7
Training loss: 2.819481372833252
Validation loss: 2.367032353596021

Epoch: 5| Step: 8
Training loss: 2.527012348175049
Validation loss: 2.3669523577536307

Epoch: 5| Step: 9
Training loss: 2.888659954071045
Validation loss: 2.3612552278785297

Epoch: 5| Step: 10
Training loss: 2.8343698978424072
Validation loss: 2.3618539892217165

Epoch: 270| Step: 0
Training loss: 2.057440757751465
Validation loss: 2.353019465682327

Epoch: 5| Step: 1
Training loss: 3.406255006790161
Validation loss: 2.3504121201012724

Epoch: 5| Step: 2
Training loss: 2.151902914047241
Validation loss: 2.3466784595161356

Epoch: 5| Step: 3
Training loss: 2.1432948112487793
Validation loss: 2.340588544004707

Epoch: 5| Step: 4
Training loss: 2.6284031867980957
Validation loss: 2.347328355235438

Epoch: 5| Step: 5
Training loss: 2.8112196922302246
Validation loss: 2.3439292305259296

Epoch: 5| Step: 6
Training loss: 2.7389183044433594
Validation loss: 2.3509461290092877

Epoch: 5| Step: 7
Training loss: 2.6387128829956055
Validation loss: 2.3435305421070387

Epoch: 5| Step: 8
Training loss: 2.66776180267334
Validation loss: 2.3448695828837733

Epoch: 5| Step: 9
Training loss: 1.573512315750122
Validation loss: 2.352451998700378

Epoch: 5| Step: 10
Training loss: 3.1100025177001953
Validation loss: 2.34797429525724

Epoch: 271| Step: 0
Training loss: 2.389894962310791
Validation loss: 2.360661311816144

Epoch: 5| Step: 1
Training loss: 3.2288284301757812
Validation loss: 2.3666911573820215

Epoch: 5| Step: 2
Training loss: 2.5176987648010254
Validation loss: 2.372731098564722

Epoch: 5| Step: 3
Training loss: 1.8446401357650757
Validation loss: 2.3670250113292406

Epoch: 5| Step: 4
Training loss: 2.431969404220581
Validation loss: 2.3793763934925036

Epoch: 5| Step: 5
Training loss: 2.456683397293091
Validation loss: 2.383448403368714

Epoch: 5| Step: 6
Training loss: 3.112983226776123
Validation loss: 2.3798470497131348

Epoch: 5| Step: 7
Training loss: 2.2376630306243896
Validation loss: 2.3762946051935994

Epoch: 5| Step: 8
Training loss: 2.259438991546631
Validation loss: 2.363364302983848

Epoch: 5| Step: 9
Training loss: 2.5452628135681152
Validation loss: 2.362546484957459

Epoch: 5| Step: 10
Training loss: 2.8726625442504883
Validation loss: 2.3592293800846225

Epoch: 272| Step: 0
Training loss: 2.5141146183013916
Validation loss: 2.3694382598323207

Epoch: 5| Step: 1
Training loss: 2.9643774032592773
Validation loss: 2.3837971046406734

Epoch: 5| Step: 2
Training loss: 2.1207237243652344
Validation loss: 2.4038355427403606

Epoch: 5| Step: 3
Training loss: 2.9711291790008545
Validation loss: 2.403403256529121

Epoch: 5| Step: 4
Training loss: 2.304903507232666
Validation loss: 2.3946473829207884

Epoch: 5| Step: 5
Training loss: 2.282512664794922
Validation loss: 2.3960187204422487

Epoch: 5| Step: 6
Training loss: 3.133915424346924
Validation loss: 2.397993787642448

Epoch: 5| Step: 7
Training loss: 2.0369458198547363
Validation loss: 2.388215587985131

Epoch: 5| Step: 8
Training loss: 2.589622974395752
Validation loss: 2.374863919391427

Epoch: 5| Step: 9
Training loss: 2.6406071186065674
Validation loss: 2.375703952645743

Epoch: 5| Step: 10
Training loss: 2.486812114715576
Validation loss: 2.366682134648805

Epoch: 273| Step: 0
Training loss: 2.923105239868164
Validation loss: 2.3679398490536596

Epoch: 5| Step: 1
Training loss: 2.585549831390381
Validation loss: 2.3626495689474125

Epoch: 5| Step: 2
Training loss: 2.3707804679870605
Validation loss: 2.360784189675444

Epoch: 5| Step: 3
Training loss: 2.5854623317718506
Validation loss: 2.3730591445840816

Epoch: 5| Step: 4
Training loss: 2.7725892066955566
Validation loss: 2.3679726098173406

Epoch: 5| Step: 5
Training loss: 2.8957695960998535
Validation loss: 2.371631791514735

Epoch: 5| Step: 6
Training loss: 1.7407581806182861
Validation loss: 2.3611708020651214

Epoch: 5| Step: 7
Training loss: 2.3680148124694824
Validation loss: 2.344763873725809

Epoch: 5| Step: 8
Training loss: 2.4267988204956055
Validation loss: 2.359463755802442

Epoch: 5| Step: 9
Training loss: 2.9951891899108887
Validation loss: 2.34848339070556

Epoch: 5| Step: 10
Training loss: 2.319025993347168
Validation loss: 2.347694727682298

Epoch: 274| Step: 0
Training loss: 2.4618561267852783
Validation loss: 2.345148981258433

Epoch: 5| Step: 1
Training loss: 2.0950965881347656
Validation loss: 2.3416599945355485

Epoch: 5| Step: 2
Training loss: 2.659804105758667
Validation loss: 2.3471904006055606

Epoch: 5| Step: 3
Training loss: 3.3178069591522217
Validation loss: 2.3563564105700423

Epoch: 5| Step: 4
Training loss: 1.7673485279083252
Validation loss: 2.358890320665093

Epoch: 5| Step: 5
Training loss: 2.377898693084717
Validation loss: 2.3638184608951693

Epoch: 5| Step: 6
Training loss: 2.9269213676452637
Validation loss: 2.3725165936254684

Epoch: 5| Step: 7
Training loss: 1.915154218673706
Validation loss: 2.37294521639424

Epoch: 5| Step: 8
Training loss: 2.754297971725464
Validation loss: 2.408748380599483

Epoch: 5| Step: 9
Training loss: 2.9760048389434814
Validation loss: 2.411474479142056

Epoch: 5| Step: 10
Training loss: 2.645308494567871
Validation loss: 2.4052270279135755

Epoch: 275| Step: 0
Training loss: 2.51155948638916
Validation loss: 2.410829231303225

Epoch: 5| Step: 1
Training loss: 2.7484264373779297
Validation loss: 2.39946811942644

Epoch: 5| Step: 2
Training loss: 2.9921274185180664
Validation loss: 2.3845826425860004

Epoch: 5| Step: 3
Training loss: 2.7720189094543457
Validation loss: 2.369828247254895

Epoch: 5| Step: 4
Training loss: 2.4126791954040527
Validation loss: 2.341974316104766

Epoch: 5| Step: 5
Training loss: 2.756366014480591
Validation loss: 2.3443927021436792

Epoch: 5| Step: 6
Training loss: 1.9963575601577759
Validation loss: 2.339855155637187

Epoch: 5| Step: 7
Training loss: 2.947786808013916
Validation loss: 2.3447699521177556

Epoch: 5| Step: 8
Training loss: 2.4470133781433105
Validation loss: 2.332718877382176

Epoch: 5| Step: 9
Training loss: 2.1448922157287598
Validation loss: 2.3406577917837326

Epoch: 5| Step: 10
Training loss: 2.1444613933563232
Validation loss: 2.333777350764121

Epoch: 276| Step: 0
Training loss: 2.6900458335876465
Validation loss: 2.331277390962006

Epoch: 5| Step: 1
Training loss: 2.532698392868042
Validation loss: 2.3285848927754227

Epoch: 5| Step: 2
Training loss: 3.0144405364990234
Validation loss: 2.3328860139334076

Epoch: 5| Step: 3
Training loss: 2.874634265899658
Validation loss: 2.3489006078371437

Epoch: 5| Step: 4
Training loss: 2.7131662368774414
Validation loss: 2.342990372770576

Epoch: 5| Step: 5
Training loss: 1.835696816444397
Validation loss: 2.3414841980062504

Epoch: 5| Step: 6
Training loss: 2.012410879135132
Validation loss: 2.3508241663696947

Epoch: 5| Step: 7
Training loss: 2.396857738494873
Validation loss: 2.3626418831527873

Epoch: 5| Step: 8
Training loss: 2.5563108921051025
Validation loss: 2.364354525842974

Epoch: 5| Step: 9
Training loss: 2.356502056121826
Validation loss: 2.3720834229582097

Epoch: 5| Step: 10
Training loss: 2.860459804534912
Validation loss: 2.37323946081182

Epoch: 277| Step: 0
Training loss: 2.484956741333008
Validation loss: 2.365003260233069

Epoch: 5| Step: 1
Training loss: 3.1421377658843994
Validation loss: 2.3704582157955376

Epoch: 5| Step: 2
Training loss: 2.812157154083252
Validation loss: 2.375788950151013

Epoch: 5| Step: 3
Training loss: 2.3284573554992676
Validation loss: 2.379770863440729

Epoch: 5| Step: 4
Training loss: 2.7877767086029053
Validation loss: 2.361348553370404

Epoch: 5| Step: 5
Training loss: 1.925474762916565
Validation loss: 2.358275713459138

Epoch: 5| Step: 6
Training loss: 1.8339149951934814
Validation loss: 2.362174285355435

Epoch: 5| Step: 7
Training loss: 2.6161179542541504
Validation loss: 2.354566248514319

Epoch: 5| Step: 8
Training loss: 2.690985918045044
Validation loss: 2.3478541425479356

Epoch: 5| Step: 9
Training loss: 2.038208484649658
Validation loss: 2.343669306847357

Epoch: 5| Step: 10
Training loss: 3.1296546459198
Validation loss: 2.3350277600749845

Epoch: 278| Step: 0
Training loss: 2.8555798530578613
Validation loss: 2.341367011429161

Epoch: 5| Step: 1
Training loss: 2.2206034660339355
Validation loss: 2.3326723755046888

Epoch: 5| Step: 2
Training loss: 2.635902166366577
Validation loss: 2.332627055465534

Epoch: 5| Step: 3
Training loss: 2.5846474170684814
Validation loss: 2.3254336669880855

Epoch: 5| Step: 4
Training loss: 2.742485523223877
Validation loss: 2.333651860555013

Epoch: 5| Step: 5
Training loss: 2.7286787033081055
Validation loss: 2.3163270386316444

Epoch: 5| Step: 6
Training loss: 2.749748945236206
Validation loss: 2.320357532911403

Epoch: 5| Step: 7
Training loss: 2.390568971633911
Validation loss: 2.3188348431741037

Epoch: 5| Step: 8
Training loss: 2.044736385345459
Validation loss: 2.3265847852153163

Epoch: 5| Step: 9
Training loss: 2.001121997833252
Validation loss: 2.3377769070286907

Epoch: 5| Step: 10
Training loss: 2.7018022537231445
Validation loss: 2.359914372044225

Epoch: 279| Step: 0
Training loss: 2.4106740951538086
Validation loss: 2.405079513467768

Epoch: 5| Step: 1
Training loss: 2.4637985229492188
Validation loss: 2.4289615948994956

Epoch: 5| Step: 2
Training loss: 2.2779157161712646
Validation loss: 2.4564907268811296

Epoch: 5| Step: 3
Training loss: 2.412675380706787
Validation loss: 2.474315748419813

Epoch: 5| Step: 4
Training loss: 2.878661632537842
Validation loss: 2.4554814266902145

Epoch: 5| Step: 5
Training loss: 1.7880544662475586
Validation loss: 2.4356287756273822

Epoch: 5| Step: 6
Training loss: 2.552530288696289
Validation loss: 2.4159987536809777

Epoch: 5| Step: 7
Training loss: 3.741978168487549
Validation loss: 2.3957596850651566

Epoch: 5| Step: 8
Training loss: 2.3558385372161865
Validation loss: 2.3571105285357405

Epoch: 5| Step: 9
Training loss: 2.510258197784424
Validation loss: 2.325395973779822

Epoch: 5| Step: 10
Training loss: 2.863266944885254
Validation loss: 2.3202948570251465

Epoch: 280| Step: 0
Training loss: 2.4905476570129395
Validation loss: 2.3173508721013225

Epoch: 5| Step: 1
Training loss: 2.9518513679504395
Validation loss: 2.3232598253475722

Epoch: 5| Step: 2
Training loss: 2.4168171882629395
Validation loss: 2.3221969194309686

Epoch: 5| Step: 3
Training loss: 2.3786582946777344
Validation loss: 2.324965807699388

Epoch: 5| Step: 4
Training loss: 2.9931094646453857
Validation loss: 2.328320316089097

Epoch: 5| Step: 5
Training loss: 2.8516042232513428
Validation loss: 2.331135265288814

Epoch: 5| Step: 6
Training loss: 2.604063034057617
Validation loss: 2.3414567542332474

Epoch: 5| Step: 7
Training loss: 2.5087902545928955
Validation loss: 2.3364769592080066

Epoch: 5| Step: 8
Training loss: 2.432614803314209
Validation loss: 2.3236036249386367

Epoch: 5| Step: 9
Training loss: 2.0561623573303223
Validation loss: 2.3359864001633017

Epoch: 5| Step: 10
Training loss: 2.232123374938965
Validation loss: 2.3304034638148483

Epoch: 281| Step: 0
Training loss: 2.202126979827881
Validation loss: 2.3531947828108266

Epoch: 5| Step: 1
Training loss: 2.7381043434143066
Validation loss: 2.3569336604046565

Epoch: 5| Step: 2
Training loss: 2.6448495388031006
Validation loss: 2.3681498727490826

Epoch: 5| Step: 3
Training loss: 2.4210541248321533
Validation loss: 2.384036651221655

Epoch: 5| Step: 4
Training loss: 2.315636396408081
Validation loss: 2.401940438055223

Epoch: 5| Step: 5
Training loss: 2.91884708404541
Validation loss: 2.389341668416095

Epoch: 5| Step: 6
Training loss: 2.326918840408325
Validation loss: 2.371701978868054

Epoch: 5| Step: 7
Training loss: 3.068087577819824
Validation loss: 2.3551602004676737

Epoch: 5| Step: 8
Training loss: 2.4050514698028564
Validation loss: 2.362205672007735

Epoch: 5| Step: 9
Training loss: 2.386199951171875
Validation loss: 2.3401641691884687

Epoch: 5| Step: 10
Training loss: 2.4776060581207275
Validation loss: 2.3322132813033236

Epoch: 282| Step: 0
Training loss: 2.9762673377990723
Validation loss: 2.3365212332817817

Epoch: 5| Step: 1
Training loss: 2.5849318504333496
Validation loss: 2.3325434602716917

Epoch: 5| Step: 2
Training loss: 2.682743549346924
Validation loss: 2.3262417239527546

Epoch: 5| Step: 3
Training loss: 2.3431248664855957
Validation loss: 2.3328738084403415

Epoch: 5| Step: 4
Training loss: 3.183316707611084
Validation loss: 2.33485988391343

Epoch: 5| Step: 5
Training loss: 1.8126115798950195
Validation loss: 2.338147117245582

Epoch: 5| Step: 6
Training loss: 2.4647669792175293
Validation loss: 2.333408781277236

Epoch: 5| Step: 7
Training loss: 2.7145304679870605
Validation loss: 2.3405058973579

Epoch: 5| Step: 8
Training loss: 2.392094135284424
Validation loss: 2.338339503093432

Epoch: 5| Step: 9
Training loss: 2.3709983825683594
Validation loss: 2.3408932814034085

Epoch: 5| Step: 10
Training loss: 2.1237924098968506
Validation loss: 2.3405868712291924

Epoch: 283| Step: 0
Training loss: 2.437673568725586
Validation loss: 2.345901043184342

Epoch: 5| Step: 1
Training loss: 2.685065746307373
Validation loss: 2.3478195487812

Epoch: 5| Step: 2
Training loss: 2.879723072052002
Validation loss: 2.350136956860942

Epoch: 5| Step: 3
Training loss: 2.5135650634765625
Validation loss: 2.3470437167793192

Epoch: 5| Step: 4
Training loss: 2.169588565826416
Validation loss: 2.348874358720677

Epoch: 5| Step: 5
Training loss: 2.401965618133545
Validation loss: 2.334354228870843

Epoch: 5| Step: 6
Training loss: 2.347449779510498
Validation loss: 2.326350647916076

Epoch: 5| Step: 7
Training loss: 2.3177380561828613
Validation loss: 2.333492389289282

Epoch: 5| Step: 8
Training loss: 2.469538688659668
Validation loss: 2.3388789866560247

Epoch: 5| Step: 9
Training loss: 2.7559549808502197
Validation loss: 2.3399587574825493

Epoch: 5| Step: 10
Training loss: 2.7651681900024414
Validation loss: 2.3287232281059347

Epoch: 284| Step: 0
Training loss: 2.131739854812622
Validation loss: 2.338151357507193

Epoch: 5| Step: 1
Training loss: 2.168550968170166
Validation loss: 2.336860365765069

Epoch: 5| Step: 2
Training loss: 2.062544584274292
Validation loss: 2.349082754504296

Epoch: 5| Step: 3
Training loss: 2.078110933303833
Validation loss: 2.3552144086489113

Epoch: 5| Step: 4
Training loss: 3.0746047496795654
Validation loss: 2.3747146821791127

Epoch: 5| Step: 5
Training loss: 2.417402744293213
Validation loss: 2.36239125908062

Epoch: 5| Step: 6
Training loss: 2.4336190223693848
Validation loss: 2.3756309247786

Epoch: 5| Step: 7
Training loss: 2.7345526218414307
Validation loss: 2.37097599685833

Epoch: 5| Step: 8
Training loss: 2.908360004425049
Validation loss: 2.374677665771977

Epoch: 5| Step: 9
Training loss: 2.6136999130249023
Validation loss: 2.3725333316351778

Epoch: 5| Step: 10
Training loss: 3.2242825031280518
Validation loss: 2.3533869097309728

Epoch: 285| Step: 0
Training loss: 2.6152477264404297
Validation loss: 2.3481381221484114

Epoch: 5| Step: 1
Training loss: 3.290673017501831
Validation loss: 2.3447572108237975

Epoch: 5| Step: 2
Training loss: 2.510831832885742
Validation loss: 2.3530192349546697

Epoch: 5| Step: 3
Training loss: 2.1380484104156494
Validation loss: 2.354515960139613

Epoch: 5| Step: 4
Training loss: 2.8782260417938232
Validation loss: 2.354103203742735

Epoch: 5| Step: 5
Training loss: 2.5664620399475098
Validation loss: 2.3386530414704354

Epoch: 5| Step: 6
Training loss: 2.773106098175049
Validation loss: 2.3386906731513237

Epoch: 5| Step: 7
Training loss: 2.6409430503845215
Validation loss: 2.3390176911507883

Epoch: 5| Step: 8
Training loss: 1.8694528341293335
Validation loss: 2.3252337107094387

Epoch: 5| Step: 9
Training loss: 2.1798529624938965
Validation loss: 2.324062639667142

Epoch: 5| Step: 10
Training loss: 2.1752753257751465
Validation loss: 2.328539558636245

Epoch: 286| Step: 0
Training loss: 2.5156168937683105
Validation loss: 2.349316691839567

Epoch: 5| Step: 1
Training loss: 2.997579574584961
Validation loss: 2.3689103229071504

Epoch: 5| Step: 2
Training loss: 2.8400790691375732
Validation loss: 2.3560331752223354

Epoch: 5| Step: 3
Training loss: 2.809548854827881
Validation loss: 2.379417147687686

Epoch: 5| Step: 4
Training loss: 2.95155668258667
Validation loss: 2.3817795425333004

Epoch: 5| Step: 5
Training loss: 2.2144522666931152
Validation loss: 2.37164613252045

Epoch: 5| Step: 6
Training loss: 2.2117843627929688
Validation loss: 2.375370156380438

Epoch: 5| Step: 7
Training loss: 2.5713400840759277
Validation loss: 2.381604672760092

Epoch: 5| Step: 8
Training loss: 2.068922996520996
Validation loss: 2.367943584278066

Epoch: 5| Step: 9
Training loss: 2.4024810791015625
Validation loss: 2.350144806728568

Epoch: 5| Step: 10
Training loss: 2.218708038330078
Validation loss: 2.3576084388199674

Epoch: 287| Step: 0
Training loss: 2.991367816925049
Validation loss: 2.3299920764020694

Epoch: 5| Step: 1
Training loss: 1.9515254497528076
Validation loss: 2.3263062559148318

Epoch: 5| Step: 2
Training loss: 3.092733383178711
Validation loss: 2.3258325976710164

Epoch: 5| Step: 3
Training loss: 3.087435245513916
Validation loss: 2.3259574597881687

Epoch: 5| Step: 4
Training loss: 2.2484188079833984
Validation loss: 2.329091815538304

Epoch: 5| Step: 5
Training loss: 2.4386987686157227
Validation loss: 2.3227796810929493

Epoch: 5| Step: 6
Training loss: 2.5211074352264404
Validation loss: 2.30611918305838

Epoch: 5| Step: 7
Training loss: 2.4873340129852295
Validation loss: 2.3101284375754734

Epoch: 5| Step: 8
Training loss: 2.434253692626953
Validation loss: 2.3049209822890577

Epoch: 5| Step: 9
Training loss: 2.4220287799835205
Validation loss: 2.31037639546138

Epoch: 5| Step: 10
Training loss: 2.121304750442505
Validation loss: 2.3150178950320006

Epoch: 288| Step: 0
Training loss: 2.625276565551758
Validation loss: 2.326762383983981

Epoch: 5| Step: 1
Training loss: 2.590177536010742
Validation loss: 2.329871569910357

Epoch: 5| Step: 2
Training loss: 1.776576042175293
Validation loss: 2.362096642935148

Epoch: 5| Step: 3
Training loss: 2.041285753250122
Validation loss: 2.380977843397407

Epoch: 5| Step: 4
Training loss: 2.212585926055908
Validation loss: 2.384981596341697

Epoch: 5| Step: 5
Training loss: 2.542449474334717
Validation loss: 2.3788029250278266

Epoch: 5| Step: 6
Training loss: 2.1858160495758057
Validation loss: 2.371445281531221

Epoch: 5| Step: 7
Training loss: 2.7530014514923096
Validation loss: 2.3825064628354964

Epoch: 5| Step: 8
Training loss: 2.9481570720672607
Validation loss: 2.3721005250048894

Epoch: 5| Step: 9
Training loss: 2.8908493518829346
Validation loss: 2.3515135447184243

Epoch: 5| Step: 10
Training loss: 3.290093421936035
Validation loss: 2.3394402227094098

Epoch: 289| Step: 0
Training loss: 2.5350849628448486
Validation loss: 2.339651543606994

Epoch: 5| Step: 1
Training loss: 2.9583067893981934
Validation loss: 2.3352543615525767

Epoch: 5| Step: 2
Training loss: 1.937535285949707
Validation loss: 2.3169631727280153

Epoch: 5| Step: 3
Training loss: 2.4870758056640625
Validation loss: 2.328020553435049

Epoch: 5| Step: 4
Training loss: 2.8291568756103516
Validation loss: 2.3297124883180023

Epoch: 5| Step: 5
Training loss: 2.9848380088806152
Validation loss: 2.33630112935138

Epoch: 5| Step: 6
Training loss: 2.6562933921813965
Validation loss: 2.3319194239954792

Epoch: 5| Step: 7
Training loss: 2.2191691398620605
Validation loss: 2.3185943044641966

Epoch: 5| Step: 8
Training loss: 2.469865322113037
Validation loss: 2.324394605493033

Epoch: 5| Step: 9
Training loss: 2.0776143074035645
Validation loss: 2.3305041405462448

Epoch: 5| Step: 10
Training loss: 2.6879360675811768
Validation loss: 2.3286177496756277

Epoch: 290| Step: 0
Training loss: 2.255502700805664
Validation loss: 2.3333341383164927

Epoch: 5| Step: 1
Training loss: 2.6186444759368896
Validation loss: 2.3471925463727725

Epoch: 5| Step: 2
Training loss: 2.556417465209961
Validation loss: 2.3652272480790333

Epoch: 5| Step: 3
Training loss: 2.7627041339874268
Validation loss: 2.3775643123093473

Epoch: 5| Step: 4
Training loss: 2.8539974689483643
Validation loss: 2.376775772340836

Epoch: 5| Step: 5
Training loss: 2.2209410667419434
Validation loss: 2.3545972660023677

Epoch: 5| Step: 6
Training loss: 2.771867275238037
Validation loss: 2.34444583359585

Epoch: 5| Step: 7
Training loss: 2.4654457569122314
Validation loss: 2.3308876099125033

Epoch: 5| Step: 8
Training loss: 2.2826294898986816
Validation loss: 2.32521725726384

Epoch: 5| Step: 9
Training loss: 2.545788288116455
Validation loss: 2.3181345796072357

Epoch: 5| Step: 10
Training loss: 2.4552552700042725
Validation loss: 2.311196175954675

Epoch: 291| Step: 0
Training loss: 2.590216875076294
Validation loss: 2.3185611360816547

Epoch: 5| Step: 1
Training loss: 2.2019200325012207
Validation loss: 2.3172421352837675

Epoch: 5| Step: 2
Training loss: 2.993403911590576
Validation loss: 2.3154979008500294

Epoch: 5| Step: 3
Training loss: 2.304399013519287
Validation loss: 2.316401481628418

Epoch: 5| Step: 4
Training loss: 2.711550235748291
Validation loss: 2.3274587918353338

Epoch: 5| Step: 5
Training loss: 2.7289867401123047
Validation loss: 2.32729100155574

Epoch: 5| Step: 6
Training loss: 2.230107307434082
Validation loss: 2.3262997929767897

Epoch: 5| Step: 7
Training loss: 2.65673828125
Validation loss: 2.3190248858544136

Epoch: 5| Step: 8
Training loss: 2.6375458240509033
Validation loss: 2.319657233453566

Epoch: 5| Step: 9
Training loss: 2.4988436698913574
Validation loss: 2.3298021208855415

Epoch: 5| Step: 10
Training loss: 2.0728261470794678
Validation loss: 2.3251433962134906

Epoch: 292| Step: 0
Training loss: 2.5871338844299316
Validation loss: 2.3303929605791645

Epoch: 5| Step: 1
Training loss: 2.7861123085021973
Validation loss: 2.3486250164688274

Epoch: 5| Step: 2
Training loss: 2.0382192134857178
Validation loss: 2.36349142495022

Epoch: 5| Step: 3
Training loss: 2.431887626647949
Validation loss: 2.3542919953664145

Epoch: 5| Step: 4
Training loss: 2.4514453411102295
Validation loss: 2.3590281471129386

Epoch: 5| Step: 5
Training loss: 2.9576754570007324
Validation loss: 2.350874226580384

Epoch: 5| Step: 6
Training loss: 3.016531467437744
Validation loss: 2.343709489350678

Epoch: 5| Step: 7
Training loss: 2.2055459022521973
Validation loss: 2.342557645613147

Epoch: 5| Step: 8
Training loss: 2.608715772628784
Validation loss: 2.3366123155881

Epoch: 5| Step: 9
Training loss: 2.4343700408935547
Validation loss: 2.3493737943710817

Epoch: 5| Step: 10
Training loss: 2.059206247329712
Validation loss: 2.3435960713253228

Epoch: 293| Step: 0
Training loss: 2.2427642345428467
Validation loss: 2.357303801403251

Epoch: 5| Step: 1
Training loss: 2.503237247467041
Validation loss: 2.3572350727614535

Epoch: 5| Step: 2
Training loss: 1.972429633140564
Validation loss: 2.3561937398807977

Epoch: 5| Step: 3
Training loss: 2.1032989025115967
Validation loss: 2.3605361497530373

Epoch: 5| Step: 4
Training loss: 3.2566215991973877
Validation loss: 2.363681153584552

Epoch: 5| Step: 5
Training loss: 3.1791465282440186
Validation loss: 2.350019085791803

Epoch: 5| Step: 6
Training loss: 3.0494742393493652
Validation loss: 2.3335051254559587

Epoch: 5| Step: 7
Training loss: 2.3122806549072266
Validation loss: 2.3457362831279798

Epoch: 5| Step: 8
Training loss: 1.9656178951263428
Validation loss: 2.3381143513546196

Epoch: 5| Step: 9
Training loss: 1.977968454360962
Validation loss: 2.335177142132995

Epoch: 5| Step: 10
Training loss: 3.141244411468506
Validation loss: 2.344341811313424

Epoch: 294| Step: 0
Training loss: 2.1610491275787354
Validation loss: 2.352733619751469

Epoch: 5| Step: 1
Training loss: 2.7325100898742676
Validation loss: 2.335334244594779

Epoch: 5| Step: 2
Training loss: 3.509814739227295
Validation loss: 2.34878626690116

Epoch: 5| Step: 3
Training loss: 2.439344644546509
Validation loss: 2.3410398370476178

Epoch: 5| Step: 4
Training loss: 1.8023771047592163
Validation loss: 2.338731545273976

Epoch: 5| Step: 5
Training loss: 2.989018678665161
Validation loss: 2.3353554689756004

Epoch: 5| Step: 6
Training loss: 1.7636387348175049
Validation loss: 2.3251295422994964

Epoch: 5| Step: 7
Training loss: 2.321916341781616
Validation loss: 2.320314209948304

Epoch: 5| Step: 8
Training loss: 2.9084324836730957
Validation loss: 2.3052070512566516

Epoch: 5| Step: 9
Training loss: 1.8376209735870361
Validation loss: 2.321417552168651

Epoch: 5| Step: 10
Training loss: 3.1816322803497314
Validation loss: 2.316044061414657

Epoch: 295| Step: 0
Training loss: 2.310295820236206
Validation loss: 2.313154019335265

Epoch: 5| Step: 1
Training loss: 2.1067888736724854
Validation loss: 2.3181637128194175

Epoch: 5| Step: 2
Training loss: 2.376053810119629
Validation loss: 2.327467464631604

Epoch: 5| Step: 3
Training loss: 3.0066492557525635
Validation loss: 2.328664138752927

Epoch: 5| Step: 4
Training loss: 2.492511749267578
Validation loss: 2.3237073729115147

Epoch: 5| Step: 5
Training loss: 2.2710981369018555
Validation loss: 2.3393400074333273

Epoch: 5| Step: 6
Training loss: 2.2413439750671387
Validation loss: 2.348455067603819

Epoch: 5| Step: 7
Training loss: 2.680269241333008
Validation loss: 2.3510959994408394

Epoch: 5| Step: 8
Training loss: 2.356306791305542
Validation loss: 2.3525774709640013

Epoch: 5| Step: 9
Training loss: 3.29646635055542
Validation loss: 2.3576819768515964

Epoch: 5| Step: 10
Training loss: 2.3090298175811768
Validation loss: 2.332141912111672

Epoch: 296| Step: 0
Training loss: 2.981560468673706
Validation loss: 2.3363531379289526

Epoch: 5| Step: 1
Training loss: 3.177306652069092
Validation loss: 2.3294381659518004

Epoch: 5| Step: 2
Training loss: 2.4215216636657715
Validation loss: 2.3325020472208657

Epoch: 5| Step: 3
Training loss: 2.2988131046295166
Validation loss: 2.322706876262542

Epoch: 5| Step: 4
Training loss: 2.53633451461792
Validation loss: 2.319166644926994

Epoch: 5| Step: 5
Training loss: 1.9845021963119507
Validation loss: 2.313981863760179

Epoch: 5| Step: 6
Training loss: 2.249741792678833
Validation loss: 2.3237912526694675

Epoch: 5| Step: 7
Training loss: 2.4650444984436035
Validation loss: 2.3096754653479463

Epoch: 5| Step: 8
Training loss: 2.1739699840545654
Validation loss: 2.325514563950159

Epoch: 5| Step: 9
Training loss: 2.9141509532928467
Validation loss: 2.3316466859591904

Epoch: 5| Step: 10
Training loss: 2.1884260177612305
Validation loss: 2.330833024876092

Epoch: 297| Step: 0
Training loss: 2.090463876724243
Validation loss: 2.34151884817308

Epoch: 5| Step: 1
Training loss: 2.8763163089752197
Validation loss: 2.342822072326496

Epoch: 5| Step: 2
Training loss: 3.324444532394409
Validation loss: 2.3460470835367837

Epoch: 5| Step: 3
Training loss: 2.240025281906128
Validation loss: 2.3449334431720037

Epoch: 5| Step: 4
Training loss: 2.0309605598449707
Validation loss: 2.33836147990278

Epoch: 5| Step: 5
Training loss: 3.5341975688934326
Validation loss: 2.345454567222185

Epoch: 5| Step: 6
Training loss: 2.024613380432129
Validation loss: 2.335152074854861

Epoch: 5| Step: 7
Training loss: 2.2348792552948
Validation loss: 2.344703589716265

Epoch: 5| Step: 8
Training loss: 2.376533031463623
Validation loss: 2.325010488110204

Epoch: 5| Step: 9
Training loss: 2.2572827339172363
Validation loss: 2.321199973424276

Epoch: 5| Step: 10
Training loss: 2.4225826263427734
Validation loss: 2.3100706813155965

Epoch: 298| Step: 0
Training loss: 2.867957353591919
Validation loss: 2.3118549136705298

Epoch: 5| Step: 1
Training loss: 3.1552987098693848
Validation loss: 2.307158457335605

Epoch: 5| Step: 2
Training loss: 2.0751304626464844
Validation loss: 2.2959675071060017

Epoch: 5| Step: 3
Training loss: 3.2030436992645264
Validation loss: 2.3034609543379916

Epoch: 5| Step: 4
Training loss: 1.9336860179901123
Validation loss: 2.3226859056821434

Epoch: 5| Step: 5
Training loss: 1.6711918115615845
Validation loss: 2.313508665689858

Epoch: 5| Step: 6
Training loss: 2.7594146728515625
Validation loss: 2.3216782949304067

Epoch: 5| Step: 7
Training loss: 2.482557773590088
Validation loss: 2.3355254357860935

Epoch: 5| Step: 8
Training loss: 2.995239734649658
Validation loss: 2.3245254203837407

Epoch: 5| Step: 9
Training loss: 2.1187996864318848
Validation loss: 2.330871755076993

Epoch: 5| Step: 10
Training loss: 2.169666290283203
Validation loss: 2.343567327786517

Epoch: 299| Step: 0
Training loss: 1.9683916568756104
Validation loss: 2.346554345982049

Epoch: 5| Step: 1
Training loss: 3.0879580974578857
Validation loss: 2.3412599563598633

Epoch: 5| Step: 2
Training loss: 2.0134148597717285
Validation loss: 2.356322060349167

Epoch: 5| Step: 3
Training loss: 2.6073989868164062
Validation loss: 2.3463736349536526

Epoch: 5| Step: 4
Training loss: 1.9586082696914673
Validation loss: 2.3450164205284527

Epoch: 5| Step: 5
Training loss: 2.3568732738494873
Validation loss: 2.338622147037137

Epoch: 5| Step: 6
Training loss: 2.2567837238311768
Validation loss: 2.3303398163087907

Epoch: 5| Step: 7
Training loss: 3.1178290843963623
Validation loss: 2.323576304220384

Epoch: 5| Step: 8
Training loss: 2.607957363128662
Validation loss: 2.31843235159433

Epoch: 5| Step: 9
Training loss: 2.7367053031921387
Validation loss: 2.3272469556459816

Epoch: 5| Step: 10
Training loss: 2.6432809829711914
Validation loss: 2.310125663716306

Epoch: 300| Step: 0
Training loss: 2.073137044906616
Validation loss: 2.3041400935060237

Epoch: 5| Step: 1
Training loss: 2.620560884475708
Validation loss: 2.3122538007715696

Epoch: 5| Step: 2
Training loss: 2.38208270072937
Validation loss: 2.313008595538396

Epoch: 5| Step: 3
Training loss: 2.8209075927734375
Validation loss: 2.3164486859434392

Epoch: 5| Step: 4
Training loss: 1.9658712148666382
Validation loss: 2.3114883156232935

Epoch: 5| Step: 5
Training loss: 2.1828086376190186
Validation loss: 2.3072825503605667

Epoch: 5| Step: 6
Training loss: 2.843999147415161
Validation loss: 2.296682568006618

Epoch: 5| Step: 7
Training loss: 2.8592281341552734
Validation loss: 2.2989789849968365

Epoch: 5| Step: 8
Training loss: 2.7150275707244873
Validation loss: 2.2978620298447145

Epoch: 5| Step: 9
Training loss: 2.2686424255371094
Validation loss: 2.3005585773016817

Epoch: 5| Step: 10
Training loss: 2.8176002502441406
Validation loss: 2.2974913761179936

Epoch: 301| Step: 0
Training loss: 2.0624840259552
Validation loss: 2.29799154240598

Epoch: 5| Step: 1
Training loss: 2.4889111518859863
Validation loss: 2.2926596672304216

Epoch: 5| Step: 2
Training loss: 2.2212138175964355
Validation loss: 2.2880989684853503

Epoch: 5| Step: 3
Training loss: 2.200136184692383
Validation loss: 2.3052096136154665

Epoch: 5| Step: 4
Training loss: 2.889444351196289
Validation loss: 2.300844807778635

Epoch: 5| Step: 5
Training loss: 2.7094197273254395
Validation loss: 2.2998605569203696

Epoch: 5| Step: 6
Training loss: 3.1501660346984863
Validation loss: 2.324003945114792

Epoch: 5| Step: 7
Training loss: 2.0668461322784424
Validation loss: 2.32494951832679

Epoch: 5| Step: 8
Training loss: 2.868229389190674
Validation loss: 2.327000715399301

Epoch: 5| Step: 9
Training loss: 2.572192668914795
Validation loss: 2.322633235685287

Epoch: 5| Step: 10
Training loss: 2.140385389328003
Validation loss: 2.3130947595001548

Epoch: 302| Step: 0
Training loss: 3.3334712982177734
Validation loss: 2.3224109052329935

Epoch: 5| Step: 1
Training loss: 2.3201358318328857
Validation loss: 2.3285378781698083

Epoch: 5| Step: 2
Training loss: 1.9634206295013428
Validation loss: 2.340833774176977

Epoch: 5| Step: 3
Training loss: 2.396543264389038
Validation loss: 2.320091119376562

Epoch: 5| Step: 4
Training loss: 1.9884628057479858
Validation loss: 2.324871245250907

Epoch: 5| Step: 5
Training loss: 2.4369664192199707
Validation loss: 2.3145927331780873

Epoch: 5| Step: 6
Training loss: 2.729921579360962
Validation loss: 2.320818434479416

Epoch: 5| Step: 7
Training loss: 2.5739052295684814
Validation loss: 2.320399184380808

Epoch: 5| Step: 8
Training loss: 2.456224203109741
Validation loss: 2.3156361246621735

Epoch: 5| Step: 9
Training loss: 2.6977975368499756
Validation loss: 2.311844356598393

Epoch: 5| Step: 10
Training loss: 2.400956869125366
Validation loss: 2.320521357238934

Epoch: 303| Step: 0
Training loss: 2.9687252044677734
Validation loss: 2.3217422090550905

Epoch: 5| Step: 1
Training loss: 2.583517074584961
Validation loss: 2.3243887347559773

Epoch: 5| Step: 2
Training loss: 2.371767997741699
Validation loss: 2.331938615409277

Epoch: 5| Step: 3
Training loss: 2.7952356338500977
Validation loss: 2.333570090673303

Epoch: 5| Step: 4
Training loss: 2.933744430541992
Validation loss: 2.327298714268592

Epoch: 5| Step: 5
Training loss: 2.66145396232605
Validation loss: 2.333357123918431

Epoch: 5| Step: 6
Training loss: 2.2612240314483643
Validation loss: 2.3054474797300113

Epoch: 5| Step: 7
Training loss: 2.212723731994629
Validation loss: 2.308870074569538

Epoch: 5| Step: 8
Training loss: 1.7297544479370117
Validation loss: 2.3047442077308573

Epoch: 5| Step: 9
Training loss: 2.3345274925231934
Validation loss: 2.2998218664558987

Epoch: 5| Step: 10
Training loss: 2.407816171646118
Validation loss: 2.3192063941750476

Epoch: 304| Step: 0
Training loss: 2.148249864578247
Validation loss: 2.3534600747528898

Epoch: 5| Step: 1
Training loss: 2.2677483558654785
Validation loss: 2.3467013810270574

Epoch: 5| Step: 2
Training loss: 2.351107358932495
Validation loss: 2.3654274338035175

Epoch: 5| Step: 3
Training loss: 3.06097674369812
Validation loss: 2.3706556904700493

Epoch: 5| Step: 4
Training loss: 2.6411194801330566
Validation loss: 2.3703308566924064

Epoch: 5| Step: 5
Training loss: 3.1609790325164795
Validation loss: 2.342765918342016

Epoch: 5| Step: 6
Training loss: 1.7901675701141357
Validation loss: 2.340070891123946

Epoch: 5| Step: 7
Training loss: 1.9528898000717163
Validation loss: 2.3202735659896687

Epoch: 5| Step: 8
Training loss: 2.3761532306671143
Validation loss: 2.308920154007532

Epoch: 5| Step: 9
Training loss: 2.354846239089966
Validation loss: 2.3124323916691605

Epoch: 5| Step: 10
Training loss: 3.3601481914520264
Validation loss: 2.309591575335431

Epoch: 305| Step: 0
Training loss: 1.6006097793579102
Validation loss: 2.3008859234471477

Epoch: 5| Step: 1
Training loss: 2.3384640216827393
Validation loss: 2.297126862310594

Epoch: 5| Step: 2
Training loss: 2.7394673824310303
Validation loss: 2.3090410873454106

Epoch: 5| Step: 3
Training loss: 3.1557841300964355
Validation loss: 2.3085761890616467

Epoch: 5| Step: 4
Training loss: 2.353609800338745
Validation loss: 2.3204300685595443

Epoch: 5| Step: 5
Training loss: 2.5217175483703613
Validation loss: 2.314717613240724

Epoch: 5| Step: 6
Training loss: 2.511491537094116
Validation loss: 2.3141363897631244

Epoch: 5| Step: 7
Training loss: 2.8343255519866943
Validation loss: 2.297981213497859

Epoch: 5| Step: 8
Training loss: 2.3927323818206787
Validation loss: 2.304054770418393

Epoch: 5| Step: 9
Training loss: 2.378352642059326
Validation loss: 2.307308845622565

Epoch: 5| Step: 10
Training loss: 2.4106616973876953
Validation loss: 2.3168573687153478

Epoch: 306| Step: 0
Training loss: 2.709697723388672
Validation loss: 2.3040712648822415

Epoch: 5| Step: 1
Training loss: 2.7152373790740967
Validation loss: 2.3243950874574724

Epoch: 5| Step: 2
Training loss: 2.107985019683838
Validation loss: 2.3096686332456526

Epoch: 5| Step: 3
Training loss: 2.344679594039917
Validation loss: 2.3206078954922256

Epoch: 5| Step: 4
Training loss: 2.7571842670440674
Validation loss: 2.3160334863970355

Epoch: 5| Step: 5
Training loss: 2.2288336753845215
Validation loss: 2.3142491053509455

Epoch: 5| Step: 6
Training loss: 2.230001449584961
Validation loss: 2.316705631953414

Epoch: 5| Step: 7
Training loss: 2.0243077278137207
Validation loss: 2.305392557574857

Epoch: 5| Step: 8
Training loss: 2.6402015686035156
Validation loss: 2.3056287098956365

Epoch: 5| Step: 9
Training loss: 2.480654716491699
Validation loss: 2.3217626412709556

Epoch: 5| Step: 10
Training loss: 3.088045120239258
Validation loss: 2.30700021918102

Epoch: 307| Step: 0
Training loss: 2.557631015777588
Validation loss: 2.3227351455278296

Epoch: 5| Step: 1
Training loss: 1.8366458415985107
Validation loss: 2.3244517951883297

Epoch: 5| Step: 2
Training loss: 2.644324541091919
Validation loss: 2.331871553133893

Epoch: 5| Step: 3
Training loss: 2.4308018684387207
Validation loss: 2.331936715751566

Epoch: 5| Step: 4
Training loss: 2.5033583641052246
Validation loss: 2.3482197638480895

Epoch: 5| Step: 5
Training loss: 2.690883159637451
Validation loss: 2.340199601265692

Epoch: 5| Step: 6
Training loss: 3.452847957611084
Validation loss: 2.3373668783454487

Epoch: 5| Step: 7
Training loss: 2.05422043800354
Validation loss: 2.3180873676012923

Epoch: 5| Step: 8
Training loss: 2.3296432495117188
Validation loss: 2.311429733871132

Epoch: 5| Step: 9
Training loss: 2.2378976345062256
Validation loss: 2.3010691981161795

Epoch: 5| Step: 10
Training loss: 2.606358528137207
Validation loss: 2.3028171754652456

Epoch: 308| Step: 0
Training loss: 2.0172746181488037
Validation loss: 2.3043458948853197

Epoch: 5| Step: 1
Training loss: 2.7254364490509033
Validation loss: 2.3069860217391804

Epoch: 5| Step: 2
Training loss: 2.0371603965759277
Validation loss: 2.3121301512564383

Epoch: 5| Step: 3
Training loss: 2.4875171184539795
Validation loss: 2.3202903193812214

Epoch: 5| Step: 4
Training loss: 2.84716534614563
Validation loss: 2.3373528744584773

Epoch: 5| Step: 5
Training loss: 2.156949043273926
Validation loss: 2.3411199636356805

Epoch: 5| Step: 6
Training loss: 2.5619711875915527
Validation loss: 2.3598684956950526

Epoch: 5| Step: 7
Training loss: 2.662968397140503
Validation loss: 2.3653684559688775

Epoch: 5| Step: 8
Training loss: 2.926652669906616
Validation loss: 2.3487274287849345

Epoch: 5| Step: 9
Training loss: 2.174372911453247
Validation loss: 2.346732326733169

Epoch: 5| Step: 10
Training loss: 2.7560057640075684
Validation loss: 2.3351355162999963

Epoch: 309| Step: 0
Training loss: 2.1071438789367676
Validation loss: 2.3289854321428525

Epoch: 5| Step: 1
Training loss: 2.4007530212402344
Validation loss: 2.3273388980537333

Epoch: 5| Step: 2
Training loss: 3.108719825744629
Validation loss: 2.312125885358421

Epoch: 5| Step: 3
Training loss: 3.1295523643493652
Validation loss: 2.3113210303809053

Epoch: 5| Step: 4
Training loss: 2.790698289871216
Validation loss: 2.30307259098176

Epoch: 5| Step: 5
Training loss: 2.01446270942688
Validation loss: 2.3124454213726904

Epoch: 5| Step: 6
Training loss: 1.820281982421875
Validation loss: 2.3108757080570346

Epoch: 5| Step: 7
Training loss: 2.2444674968719482
Validation loss: 2.3120599997940885

Epoch: 5| Step: 8
Training loss: 3.3772997856140137
Validation loss: 2.308628559112549

Epoch: 5| Step: 9
Training loss: 2.3175487518310547
Validation loss: 2.309693587723599

Epoch: 5| Step: 10
Training loss: 1.7130292654037476
Validation loss: 2.304420640391688

Epoch: 310| Step: 0
Training loss: 2.5959599018096924
Validation loss: 2.324490995817287

Epoch: 5| Step: 1
Training loss: 2.7088723182678223
Validation loss: 2.319997746457336

Epoch: 5| Step: 2
Training loss: 2.6012325286865234
Validation loss: 2.307320069241267

Epoch: 5| Step: 3
Training loss: 2.4151859283447266
Validation loss: 2.3341747432626705

Epoch: 5| Step: 4
Training loss: 2.637194871902466
Validation loss: 2.3292185234767135

Epoch: 5| Step: 5
Training loss: 2.5139527320861816
Validation loss: 2.3292853524607997

Epoch: 5| Step: 6
Training loss: 2.488968849182129
Validation loss: 2.3358267827700545

Epoch: 5| Step: 7
Training loss: 2.508349657058716
Validation loss: 2.334832851604749

Epoch: 5| Step: 8
Training loss: 1.9941591024398804
Validation loss: 2.3174758726550686

Epoch: 5| Step: 9
Training loss: 2.104569435119629
Validation loss: 2.3087911580198552

Epoch: 5| Step: 10
Training loss: 2.5829601287841797
Validation loss: 2.3054370085398355

Epoch: 311| Step: 0
Training loss: 2.705911159515381
Validation loss: 2.3103442281805058

Epoch: 5| Step: 1
Training loss: 1.9357064962387085
Validation loss: 2.2972991261430966

Epoch: 5| Step: 2
Training loss: 2.6797711849212646
Validation loss: 2.2962222612032326

Epoch: 5| Step: 3
Training loss: 2.50738525390625
Validation loss: 2.3025395613844677

Epoch: 5| Step: 4
Training loss: 2.7355287075042725
Validation loss: 2.307112261813174

Epoch: 5| Step: 5
Training loss: 1.7557404041290283
Validation loss: 2.3113368685527513

Epoch: 5| Step: 6
Training loss: 2.653548002243042
Validation loss: 2.31764329120677

Epoch: 5| Step: 7
Training loss: 2.0989978313446045
Validation loss: 2.3030996502086682

Epoch: 5| Step: 8
Training loss: 2.821967601776123
Validation loss: 2.2975280387427217

Epoch: 5| Step: 9
Training loss: 2.8436648845672607
Validation loss: 2.306547527672142

Epoch: 5| Step: 10
Training loss: 2.3512625694274902
Validation loss: 2.2869838283907984

Epoch: 312| Step: 0
Training loss: 1.9442622661590576
Validation loss: 2.2894997289103847

Epoch: 5| Step: 1
Training loss: 2.4356842041015625
Validation loss: 2.293099746909193

Epoch: 5| Step: 2
Training loss: 2.1992626190185547
Validation loss: 2.2964113348273822

Epoch: 5| Step: 3
Training loss: 2.399778127670288
Validation loss: 2.302240671650056

Epoch: 5| Step: 4
Training loss: 2.319946765899658
Validation loss: 2.2838945234975507

Epoch: 5| Step: 5
Training loss: 2.7064948081970215
Validation loss: 2.3032558015597764

Epoch: 5| Step: 6
Training loss: 2.5387163162231445
Validation loss: 2.3066735165093535

Epoch: 5| Step: 7
Training loss: 2.116934061050415
Validation loss: 2.312306282340839

Epoch: 5| Step: 8
Training loss: 2.7010791301727295
Validation loss: 2.318778617407686

Epoch: 5| Step: 9
Training loss: 3.3697714805603027
Validation loss: 2.3148620410632064

Epoch: 5| Step: 10
Training loss: 2.3236165046691895
Validation loss: 2.3141520587346887

Epoch: 313| Step: 0
Training loss: 2.5320448875427246
Validation loss: 2.309245042903449

Epoch: 5| Step: 1
Training loss: 3.015794038772583
Validation loss: 2.303941718993648

Epoch: 5| Step: 2
Training loss: 2.5467376708984375
Validation loss: 2.2999746825105403

Epoch: 5| Step: 3
Training loss: 2.6804401874542236
Validation loss: 2.289097411658174

Epoch: 5| Step: 4
Training loss: 2.3257505893707275
Validation loss: 2.3083743818344606

Epoch: 5| Step: 5
Training loss: 2.8387351036071777
Validation loss: 2.2886241174513295

Epoch: 5| Step: 6
Training loss: 2.777453660964966
Validation loss: 2.292596829834805

Epoch: 5| Step: 7
Training loss: 1.4423049688339233
Validation loss: 2.293234981516356

Epoch: 5| Step: 8
Training loss: 1.5670204162597656
Validation loss: 2.2928617000579834

Epoch: 5| Step: 9
Training loss: 2.647279977798462
Validation loss: 2.2974953741155644

Epoch: 5| Step: 10
Training loss: 2.795732259750366
Validation loss: 2.296981626941312

Epoch: 314| Step: 0
Training loss: 2.385589838027954
Validation loss: 2.311748593084274

Epoch: 5| Step: 1
Training loss: 2.4669747352600098
Validation loss: 2.3109940303269254

Epoch: 5| Step: 2
Training loss: 2.9822604656219482
Validation loss: 2.2799532798028763

Epoch: 5| Step: 3
Training loss: 2.535094738006592
Validation loss: 2.282321230057747

Epoch: 5| Step: 4
Training loss: 2.1873016357421875
Validation loss: 2.2975757327131046

Epoch: 5| Step: 5
Training loss: 2.3335514068603516
Validation loss: 2.3018433393970614

Epoch: 5| Step: 6
Training loss: 2.392751455307007
Validation loss: 2.285806712283883

Epoch: 5| Step: 7
Training loss: 2.249363660812378
Validation loss: 2.289522727330526

Epoch: 5| Step: 8
Training loss: 2.7868142127990723
Validation loss: 2.299204682791105

Epoch: 5| Step: 9
Training loss: 2.548069477081299
Validation loss: 2.3050830889773626

Epoch: 5| Step: 10
Training loss: 2.267077922821045
Validation loss: 2.31269101173647

Epoch: 315| Step: 0
Training loss: 2.715106725692749
Validation loss: 2.3110367149435063

Epoch: 5| Step: 1
Training loss: 2.4110794067382812
Validation loss: 2.3198364857704408

Epoch: 5| Step: 2
Training loss: 2.958458662033081
Validation loss: 2.3187544474037747

Epoch: 5| Step: 3
Training loss: 1.9932053089141846
Validation loss: 2.3105127862704697

Epoch: 5| Step: 4
Training loss: 2.33161997795105
Validation loss: 2.3076248450945784

Epoch: 5| Step: 5
Training loss: 2.430471420288086
Validation loss: 2.3099888422155894

Epoch: 5| Step: 6
Training loss: 2.2095744609832764
Validation loss: 2.304823613935901

Epoch: 5| Step: 7
Training loss: 2.6307156085968018
Validation loss: 2.3045712286426174

Epoch: 5| Step: 8
Training loss: 2.1125850677490234
Validation loss: 2.30861936589723

Epoch: 5| Step: 9
Training loss: 2.798194408416748
Validation loss: 2.303510207001881

Epoch: 5| Step: 10
Training loss: 2.419403553009033
Validation loss: 2.2975560285711802

Epoch: 316| Step: 0
Training loss: 2.4883344173431396
Validation loss: 2.304100872367941

Epoch: 5| Step: 1
Training loss: 2.8602538108825684
Validation loss: 2.293043956961683

Epoch: 5| Step: 2
Training loss: 2.1047415733337402
Validation loss: 2.3001879017840148

Epoch: 5| Step: 3
Training loss: 2.8121109008789062
Validation loss: 2.3120760892027166

Epoch: 5| Step: 4
Training loss: 2.695901393890381
Validation loss: 2.3201880147380214

Epoch: 5| Step: 5
Training loss: 2.270714521408081
Validation loss: 2.313777874874812

Epoch: 5| Step: 6
Training loss: 2.9290215969085693
Validation loss: 2.2989930106747534

Epoch: 5| Step: 7
Training loss: 2.418135643005371
Validation loss: 2.299298551774794

Epoch: 5| Step: 8
Training loss: 1.7770706415176392
Validation loss: 2.289272121203843

Epoch: 5| Step: 9
Training loss: 1.933948278427124
Validation loss: 2.2957732472368466

Epoch: 5| Step: 10
Training loss: 2.8057358264923096
Validation loss: 2.289644202878398

Epoch: 317| Step: 0
Training loss: 2.906135082244873
Validation loss: 2.287097459198326

Epoch: 5| Step: 1
Training loss: 2.241138458251953
Validation loss: 2.2926361855640205

Epoch: 5| Step: 2
Training loss: 1.6025667190551758
Validation loss: 2.2879069210380636

Epoch: 5| Step: 3
Training loss: 3.035277843475342
Validation loss: 2.2813911745625157

Epoch: 5| Step: 4
Training loss: 2.650526523590088
Validation loss: 2.2825013309396724

Epoch: 5| Step: 5
Training loss: 1.9069312810897827
Validation loss: 2.2797296700939054

Epoch: 5| Step: 6
Training loss: 2.5425734519958496
Validation loss: 2.2844662538138767

Epoch: 5| Step: 7
Training loss: 2.8229904174804688
Validation loss: 2.2820504250064975

Epoch: 5| Step: 8
Training loss: 2.5522308349609375
Validation loss: 2.2857536410772674

Epoch: 5| Step: 9
Training loss: 2.8436455726623535
Validation loss: 2.2931755537627847

Epoch: 5| Step: 10
Training loss: 1.7834402322769165
Validation loss: 2.307547187292448

Epoch: 318| Step: 0
Training loss: 1.7281957864761353
Validation loss: 2.3146222419636224

Epoch: 5| Step: 1
Training loss: 2.810849666595459
Validation loss: 2.323193480891566

Epoch: 5| Step: 2
Training loss: 2.2135157585144043
Validation loss: 2.3319402971575336

Epoch: 5| Step: 3
Training loss: 2.3039710521698
Validation loss: 2.3371800863614647

Epoch: 5| Step: 4
Training loss: 2.8854260444641113
Validation loss: 2.3321188931824057

Epoch: 5| Step: 5
Training loss: 2.389535427093506
Validation loss: 2.3237194271497827

Epoch: 5| Step: 6
Training loss: 2.03254771232605
Validation loss: 2.3295024082224858

Epoch: 5| Step: 7
Training loss: 3.224060535430908
Validation loss: 2.312818555421727

Epoch: 5| Step: 8
Training loss: 2.6535658836364746
Validation loss: 2.3066494695601927

Epoch: 5| Step: 9
Training loss: 2.506169080734253
Validation loss: 2.2969046561948714

Epoch: 5| Step: 10
Training loss: 2.3218746185302734
Validation loss: 2.286444861401794

Epoch: 319| Step: 0
Training loss: 2.589890956878662
Validation loss: 2.280304642133815

Epoch: 5| Step: 1
Training loss: 2.5134894847869873
Validation loss: 2.282335017317085

Epoch: 5| Step: 2
Training loss: 2.8844621181488037
Validation loss: 2.291197428139307

Epoch: 5| Step: 3
Training loss: 2.398319721221924
Validation loss: 2.2825041022351993

Epoch: 5| Step: 4
Training loss: 2.1266281604766846
Validation loss: 2.2902045455030215

Epoch: 5| Step: 5
Training loss: 2.462867498397827
Validation loss: 2.2838622062436995

Epoch: 5| Step: 6
Training loss: 2.5018019676208496
Validation loss: 2.286260263894194

Epoch: 5| Step: 7
Training loss: 1.722184419631958
Validation loss: 2.281748030775337

Epoch: 5| Step: 8
Training loss: 2.1861982345581055
Validation loss: 2.286925741421279

Epoch: 5| Step: 9
Training loss: 2.546812057495117
Validation loss: 2.3116324588816655

Epoch: 5| Step: 10
Training loss: 3.2232935428619385
Validation loss: 2.3207616216392926

Epoch: 320| Step: 0
Training loss: 2.8418986797332764
Validation loss: 2.317022118517148

Epoch: 5| Step: 1
Training loss: 2.8486227989196777
Validation loss: 2.3213664857290124

Epoch: 5| Step: 2
Training loss: 2.509032726287842
Validation loss: 2.3220598133661414

Epoch: 5| Step: 3
Training loss: 2.4149672985076904
Validation loss: 2.3109196027119956

Epoch: 5| Step: 4
Training loss: 2.8539843559265137
Validation loss: 2.314681440271357

Epoch: 5| Step: 5
Training loss: 2.129429340362549
Validation loss: 2.3012325763702393

Epoch: 5| Step: 6
Training loss: 2.249183177947998
Validation loss: 2.297807478135632

Epoch: 5| Step: 7
Training loss: 2.4416210651397705
Validation loss: 2.295002042606313

Epoch: 5| Step: 8
Training loss: 2.1497786045074463
Validation loss: 2.2850281628229285

Epoch: 5| Step: 9
Training loss: 2.5116209983825684
Validation loss: 2.284437979421308

Epoch: 5| Step: 10
Training loss: 2.118375778198242
Validation loss: 2.281418146625642

Epoch: 321| Step: 0
Training loss: 2.261963367462158
Validation loss: 2.2858848366686093

Epoch: 5| Step: 1
Training loss: 2.0745553970336914
Validation loss: 2.289712862301898

Epoch: 5| Step: 2
Training loss: 1.6668822765350342
Validation loss: 2.2797769910545758

Epoch: 5| Step: 3
Training loss: 3.06640625
Validation loss: 2.3002548397228284

Epoch: 5| Step: 4
Training loss: 1.7868210077285767
Validation loss: 2.2847995937511487

Epoch: 5| Step: 5
Training loss: 2.8914566040039062
Validation loss: 2.304224888483683

Epoch: 5| Step: 6
Training loss: 2.0300028324127197
Validation loss: 2.2970358530680337

Epoch: 5| Step: 7
Training loss: 3.1976044178009033
Validation loss: 2.2958549094456497

Epoch: 5| Step: 8
Training loss: 3.182386875152588
Validation loss: 2.322917056340043

Epoch: 5| Step: 9
Training loss: 2.192434549331665
Validation loss: 2.306196424268907

Epoch: 5| Step: 10
Training loss: 2.4887447357177734
Validation loss: 2.310588864869969

Epoch: 322| Step: 0
Training loss: 2.5364928245544434
Validation loss: 2.3036530633126535

Epoch: 5| Step: 1
Training loss: 1.421628713607788
Validation loss: 2.2918771466901227

Epoch: 5| Step: 2
Training loss: 1.8870694637298584
Validation loss: 2.3126607761588147

Epoch: 5| Step: 3
Training loss: 2.6344165802001953
Validation loss: 2.3030726807091826

Epoch: 5| Step: 4
Training loss: 2.4798362255096436
Validation loss: 2.289313554763794

Epoch: 5| Step: 5
Training loss: 3.3744006156921387
Validation loss: 2.283043324306447

Epoch: 5| Step: 6
Training loss: 2.3765506744384766
Validation loss: 2.2898747741535144

Epoch: 5| Step: 7
Training loss: 2.797950029373169
Validation loss: 2.279327546396563

Epoch: 5| Step: 8
Training loss: 2.1444664001464844
Validation loss: 2.290733947548815

Epoch: 5| Step: 9
Training loss: 2.6312167644500732
Validation loss: 2.2902359654826503

Epoch: 5| Step: 10
Training loss: 2.6586573123931885
Validation loss: 2.2834296149592244

Epoch: 323| Step: 0
Training loss: 2.8498706817626953
Validation loss: 2.28571980614816

Epoch: 5| Step: 1
Training loss: 2.008533000946045
Validation loss: 2.324504913822297

Epoch: 5| Step: 2
Training loss: 2.1356616020202637
Validation loss: 2.3177507026221162

Epoch: 5| Step: 3
Training loss: 2.3926920890808105
Validation loss: 2.3453105777822514

Epoch: 5| Step: 4
Training loss: 2.3053781986236572
Validation loss: 2.3681472142537436

Epoch: 5| Step: 5
Training loss: 3.244335889816284
Validation loss: 2.372198994441699

Epoch: 5| Step: 6
Training loss: 2.6626169681549072
Validation loss: 2.350894481905045

Epoch: 5| Step: 7
Training loss: 2.5135741233825684
Validation loss: 2.355003518442954

Epoch: 5| Step: 8
Training loss: 2.511679172515869
Validation loss: 2.339162713737898

Epoch: 5| Step: 9
Training loss: 2.3812918663024902
Validation loss: 2.322702556528071

Epoch: 5| Step: 10
Training loss: 2.168212413787842
Validation loss: 2.3056669119865663

Epoch: 324| Step: 0
Training loss: 2.1537601947784424
Validation loss: 2.299833397711477

Epoch: 5| Step: 1
Training loss: 2.0202555656433105
Validation loss: 2.2980756964734805

Epoch: 5| Step: 2
Training loss: 2.1696276664733887
Validation loss: 2.28143290422296

Epoch: 5| Step: 3
Training loss: 2.4264721870422363
Validation loss: 2.2733271455252044

Epoch: 5| Step: 4
Training loss: 3.2325949668884277
Validation loss: 2.2753232627786617

Epoch: 5| Step: 5
Training loss: 2.511570930480957
Validation loss: 2.278944319294345

Epoch: 5| Step: 6
Training loss: 2.241563558578491
Validation loss: 2.2869302559924383

Epoch: 5| Step: 7
Training loss: 2.7081851959228516
Validation loss: 2.2942920897596624

Epoch: 5| Step: 8
Training loss: 2.266040563583374
Validation loss: 2.3007167475197905

Epoch: 5| Step: 9
Training loss: 2.616530418395996
Validation loss: 2.3075065881975236

Epoch: 5| Step: 10
Training loss: 2.5960726737976074
Validation loss: 2.2938485940297446

Epoch: 325| Step: 0
Training loss: 2.960223436355591
Validation loss: 2.275771807598811

Epoch: 5| Step: 1
Training loss: 2.897308826446533
Validation loss: 2.262685675774851

Epoch: 5| Step: 2
Training loss: 3.03485369682312
Validation loss: 2.2618229030280985

Epoch: 5| Step: 3
Training loss: 2.3239669799804688
Validation loss: 2.2607641271365586

Epoch: 5| Step: 4
Training loss: 2.0576910972595215
Validation loss: 2.250796095017464

Epoch: 5| Step: 5
Training loss: 2.92036509513855
Validation loss: 2.2601063059222315

Epoch: 5| Step: 6
Training loss: 2.4574522972106934
Validation loss: 2.2708704753588607

Epoch: 5| Step: 7
Training loss: 2.16343092918396
Validation loss: 2.2623978199497348

Epoch: 5| Step: 8
Training loss: 2.026738166809082
Validation loss: 2.263175509309256

Epoch: 5| Step: 9
Training loss: 2.106571912765503
Validation loss: 2.274034395012804

Epoch: 5| Step: 10
Training loss: 2.01967191696167
Validation loss: 2.2750600486673336

Epoch: 326| Step: 0
Training loss: 2.452998638153076
Validation loss: 2.290449342420024

Epoch: 5| Step: 1
Training loss: 2.0937540531158447
Validation loss: 2.2977648806828324

Epoch: 5| Step: 2
Training loss: 2.4317333698272705
Validation loss: 2.2980024455696024

Epoch: 5| Step: 3
Training loss: 2.4588866233825684
Validation loss: 2.294483300178282

Epoch: 5| Step: 4
Training loss: 2.8160178661346436
Validation loss: 2.2975816675411758

Epoch: 5| Step: 5
Training loss: 2.451460599899292
Validation loss: 2.307788545085538

Epoch: 5| Step: 6
Training loss: 2.4440934658050537
Validation loss: 2.302053379756148

Epoch: 5| Step: 7
Training loss: 1.9428455829620361
Validation loss: 2.312243930755123

Epoch: 5| Step: 8
Training loss: 3.036771297454834
Validation loss: 2.3130348318366596

Epoch: 5| Step: 9
Training loss: 2.041875123977661
Validation loss: 2.316841058833625

Epoch: 5| Step: 10
Training loss: 2.7417073249816895
Validation loss: 2.323419218422264

Epoch: 327| Step: 0
Training loss: 2.4294419288635254
Validation loss: 2.3191088066306165

Epoch: 5| Step: 1
Training loss: 3.0517661571502686
Validation loss: 2.318316464783043

Epoch: 5| Step: 2
Training loss: 2.212815761566162
Validation loss: 2.31297218671409

Epoch: 5| Step: 3
Training loss: 2.8050084114074707
Validation loss: 2.3134007453918457

Epoch: 5| Step: 4
Training loss: 2.8963403701782227
Validation loss: 2.3043984879729567

Epoch: 5| Step: 5
Training loss: 2.1335549354553223
Validation loss: 2.311699769830191

Epoch: 5| Step: 6
Training loss: 2.020087480545044
Validation loss: 2.3223055844665854

Epoch: 5| Step: 7
Training loss: 2.3060991764068604
Validation loss: 2.2886687171074653

Epoch: 5| Step: 8
Training loss: 2.0268142223358154
Validation loss: 2.303294684297295

Epoch: 5| Step: 9
Training loss: 2.302733898162842
Validation loss: 2.299001929580524

Epoch: 5| Step: 10
Training loss: 2.6417903900146484
Validation loss: 2.2743972988538843

Epoch: 328| Step: 0
Training loss: 1.6841089725494385
Validation loss: 2.271240134393015

Epoch: 5| Step: 1
Training loss: 3.1859328746795654
Validation loss: 2.270174418726275

Epoch: 5| Step: 2
Training loss: 2.501521348953247
Validation loss: 2.269095972020139

Epoch: 5| Step: 3
Training loss: 2.357586622238159
Validation loss: 2.277810509486865

Epoch: 5| Step: 4
Training loss: 2.0756733417510986
Validation loss: 2.2843022884861117

Epoch: 5| Step: 5
Training loss: 1.8382461071014404
Validation loss: 2.287107544560586

Epoch: 5| Step: 6
Training loss: 2.925208330154419
Validation loss: 2.287662274094038

Epoch: 5| Step: 7
Training loss: 2.9157629013061523
Validation loss: 2.2889371636093303

Epoch: 5| Step: 8
Training loss: 2.6322455406188965
Validation loss: 2.295604257173436

Epoch: 5| Step: 9
Training loss: 2.96173357963562
Validation loss: 2.307090510604202

Epoch: 5| Step: 10
Training loss: 1.6183619499206543
Validation loss: 2.313283121714028

Epoch: 329| Step: 0
Training loss: 2.822908401489258
Validation loss: 2.322797013867286

Epoch: 5| Step: 1
Training loss: 2.2887063026428223
Validation loss: 2.305441766656855

Epoch: 5| Step: 2
Training loss: 1.2426255941390991
Validation loss: 2.3069124657620668

Epoch: 5| Step: 3
Training loss: 2.627197265625
Validation loss: 2.302145492646002

Epoch: 5| Step: 4
Training loss: 2.586799383163452
Validation loss: 2.297031456424344

Epoch: 5| Step: 5
Training loss: 2.558417797088623
Validation loss: 2.284872765182167

Epoch: 5| Step: 6
Training loss: 2.118083953857422
Validation loss: 2.2916013425396335

Epoch: 5| Step: 7
Training loss: 3.1383073329925537
Validation loss: 2.284888421335528

Epoch: 5| Step: 8
Training loss: 2.48498272895813
Validation loss: 2.286973158518473

Epoch: 5| Step: 9
Training loss: 2.441208600997925
Validation loss: 2.274117728715302

Epoch: 5| Step: 10
Training loss: 2.5101511478424072
Validation loss: 2.2702116209973573

Epoch: 330| Step: 0
Training loss: 2.677152633666992
Validation loss: 2.269810717592957

Epoch: 5| Step: 1
Training loss: 3.2209084033966064
Validation loss: 2.2551432578794417

Epoch: 5| Step: 2
Training loss: 1.7893816232681274
Validation loss: 2.2453970037480837

Epoch: 5| Step: 3
Training loss: 2.403625965118408
Validation loss: 2.2465000665316017

Epoch: 5| Step: 4
Training loss: 3.223707675933838
Validation loss: 2.2490743821667087

Epoch: 5| Step: 5
Training loss: 1.8249435424804688
Validation loss: 2.2762996714602233

Epoch: 5| Step: 6
Training loss: 2.3863883018493652
Validation loss: 2.276556348287931

Epoch: 5| Step: 7
Training loss: 2.1880087852478027
Validation loss: 2.291930878034202

Epoch: 5| Step: 8
Training loss: 2.173809051513672
Validation loss: 2.2860900868651686

Epoch: 5| Step: 9
Training loss: 2.5977489948272705
Validation loss: 2.2960032570746636

Epoch: 5| Step: 10
Training loss: 2.3489649295806885
Validation loss: 2.300260341295632

Epoch: 331| Step: 0
Training loss: 3.0520691871643066
Validation loss: 2.314604405433901

Epoch: 5| Step: 1
Training loss: 2.738023042678833
Validation loss: 2.2816373430272585

Epoch: 5| Step: 2
Training loss: 2.750269889831543
Validation loss: 2.279507024313814

Epoch: 5| Step: 3
Training loss: 1.84909987449646
Validation loss: 2.266203026617727

Epoch: 5| Step: 4
Training loss: 2.498493194580078
Validation loss: 2.272779039157334

Epoch: 5| Step: 5
Training loss: 2.297947645187378
Validation loss: 2.2565741590274278

Epoch: 5| Step: 6
Training loss: 2.53825306892395
Validation loss: 2.2617576327375186

Epoch: 5| Step: 7
Training loss: 2.3904995918273926
Validation loss: 2.258992100274691

Epoch: 5| Step: 8
Training loss: 2.7566978931427
Validation loss: 2.267861408572043

Epoch: 5| Step: 9
Training loss: 2.4971203804016113
Validation loss: 2.2620452603986188

Epoch: 5| Step: 10
Training loss: 1.1880338191986084
Validation loss: 2.268290668405512

Epoch: 332| Step: 0
Training loss: 2.6018130779266357
Validation loss: 2.266888549250941

Epoch: 5| Step: 1
Training loss: 2.1160929203033447
Validation loss: 2.2684102019956036

Epoch: 5| Step: 2
Training loss: 2.699456214904785
Validation loss: 2.2880746369720786

Epoch: 5| Step: 3
Training loss: 2.4387269020080566
Validation loss: 2.2913649876912436

Epoch: 5| Step: 4
Training loss: 1.8257182836532593
Validation loss: 2.2883085358527397

Epoch: 5| Step: 5
Training loss: 2.5370841026306152
Validation loss: 2.300424942406275

Epoch: 5| Step: 6
Training loss: 2.507272243499756
Validation loss: 2.3030899058106127

Epoch: 5| Step: 7
Training loss: 2.601764678955078
Validation loss: 2.321047480388354

Epoch: 5| Step: 8
Training loss: 1.9825782775878906
Validation loss: 2.31956083672021

Epoch: 5| Step: 9
Training loss: 2.6413731575012207
Validation loss: 2.3221634934025426

Epoch: 5| Step: 10
Training loss: 2.8688161373138428
Validation loss: 2.292982111694992

Epoch: 333| Step: 0
Training loss: 2.410393714904785
Validation loss: 2.283917034825971

Epoch: 5| Step: 1
Training loss: 2.0502727031707764
Validation loss: 2.26777914134405

Epoch: 5| Step: 2
Training loss: 2.4007294178009033
Validation loss: 2.254748941749655

Epoch: 5| Step: 3
Training loss: 2.4476261138916016
Validation loss: 2.266674126348188

Epoch: 5| Step: 4
Training loss: 2.5619170665740967
Validation loss: 2.2452260935178368

Epoch: 5| Step: 5
Training loss: 2.6166627407073975
Validation loss: 2.2633918805788924

Epoch: 5| Step: 6
Training loss: 1.9693330526351929
Validation loss: 2.2688219252453057

Epoch: 5| Step: 7
Training loss: 2.4709832668304443
Validation loss: 2.2813369740722

Epoch: 5| Step: 8
Training loss: 3.202157974243164
Validation loss: 2.291751359098701

Epoch: 5| Step: 9
Training loss: 2.1446995735168457
Validation loss: 2.2958913157063146

Epoch: 5| Step: 10
Training loss: 2.4655303955078125
Validation loss: 2.2911149276200162

Epoch: 334| Step: 0
Training loss: 2.1783053874969482
Validation loss: 2.2848553580622517

Epoch: 5| Step: 1
Training loss: 2.8233609199523926
Validation loss: 2.2646375522818616

Epoch: 5| Step: 2
Training loss: 2.9781248569488525
Validation loss: 2.258644829514206

Epoch: 5| Step: 3
Training loss: 1.8305251598358154
Validation loss: 2.249720670843637

Epoch: 5| Step: 4
Training loss: 2.7880892753601074
Validation loss: 2.2461606635842273

Epoch: 5| Step: 5
Training loss: 2.125800609588623
Validation loss: 2.2380164925770094

Epoch: 5| Step: 6
Training loss: 2.608788013458252
Validation loss: 2.245438260416831

Epoch: 5| Step: 7
Training loss: 2.086509943008423
Validation loss: 2.237718295025569

Epoch: 5| Step: 8
Training loss: 2.46575665473938
Validation loss: 2.2468242106899137

Epoch: 5| Step: 9
Training loss: 2.289806842803955
Validation loss: 2.2452132214782057

Epoch: 5| Step: 10
Training loss: 2.55639910697937
Validation loss: 2.2609002526088426

Epoch: 335| Step: 0
Training loss: 2.206740140914917
Validation loss: 2.2652409076690674

Epoch: 5| Step: 1
Training loss: 2.373049736022949
Validation loss: 2.284156435279436

Epoch: 5| Step: 2
Training loss: 2.1407597064971924
Validation loss: 2.290876915377955

Epoch: 5| Step: 3
Training loss: 3.2172629833221436
Validation loss: 2.3153640557360906

Epoch: 5| Step: 4
Training loss: 2.212705612182617
Validation loss: 2.3313241748399633

Epoch: 5| Step: 5
Training loss: 1.5587972402572632
Validation loss: 2.3449938399817354

Epoch: 5| Step: 6
Training loss: 2.5918006896972656
Validation loss: 2.339323159187071

Epoch: 5| Step: 7
Training loss: 2.1516530513763428
Validation loss: 2.338988504102153

Epoch: 5| Step: 8
Training loss: 3.017500400543213
Validation loss: 2.330991680904101

Epoch: 5| Step: 9
Training loss: 2.135469436645508
Validation loss: 2.3149931892271964

Epoch: 5| Step: 10
Training loss: 3.3741657733917236
Validation loss: 2.3026177857511785

Epoch: 336| Step: 0
Training loss: 1.9910290241241455
Validation loss: 2.2843270532546507

Epoch: 5| Step: 1
Training loss: 2.593015193939209
Validation loss: 2.2677384089398127

Epoch: 5| Step: 2
Training loss: 2.1206133365631104
Validation loss: 2.264811932399709

Epoch: 5| Step: 3
Training loss: 2.426510810852051
Validation loss: 2.252397808977353

Epoch: 5| Step: 4
Training loss: 2.7080395221710205
Validation loss: 2.262927334795716

Epoch: 5| Step: 5
Training loss: 2.950636625289917
Validation loss: 2.250662090957806

Epoch: 5| Step: 6
Training loss: 1.9297205209732056
Validation loss: 2.244654224764916

Epoch: 5| Step: 7
Training loss: 2.3967623710632324
Validation loss: 2.2513882524223736

Epoch: 5| Step: 8
Training loss: 2.662274122238159
Validation loss: 2.2605854157478578

Epoch: 5| Step: 9
Training loss: 1.927838683128357
Validation loss: 2.264876955298967

Epoch: 5| Step: 10
Training loss: 3.1033291816711426
Validation loss: 2.2779121732199066

Epoch: 337| Step: 0
Training loss: 2.380948066711426
Validation loss: 2.2893660401785247

Epoch: 5| Step: 1
Training loss: 3.1785788536071777
Validation loss: 2.2861685188867713

Epoch: 5| Step: 2
Training loss: 2.4838461875915527
Validation loss: 2.2947288097873813

Epoch: 5| Step: 3
Training loss: 2.757507801055908
Validation loss: 2.286033340679702

Epoch: 5| Step: 4
Training loss: 2.1110997200012207
Validation loss: 2.2698676381059872

Epoch: 5| Step: 5
Training loss: 1.771484375
Validation loss: 2.2683728561606458

Epoch: 5| Step: 6
Training loss: 2.641688108444214
Validation loss: 2.267001921130765

Epoch: 5| Step: 7
Training loss: 1.7361513376235962
Validation loss: 2.255225125179496

Epoch: 5| Step: 8
Training loss: 2.615496873855591
Validation loss: 2.2601874643756497

Epoch: 5| Step: 9
Training loss: 2.654231309890747
Validation loss: 2.2541351587541643

Epoch: 5| Step: 10
Training loss: 2.378187417984009
Validation loss: 2.255652650710075

Epoch: 338| Step: 0
Training loss: 2.5917530059814453
Validation loss: 2.2565694855105494

Epoch: 5| Step: 1
Training loss: 1.7234728336334229
Validation loss: 2.2724113387446248

Epoch: 5| Step: 2
Training loss: 2.2581005096435547
Validation loss: 2.269832126555904

Epoch: 5| Step: 3
Training loss: 2.667086124420166
Validation loss: 2.261586900680296

Epoch: 5| Step: 4
Training loss: 2.1010050773620605
Validation loss: 2.2718887790556876

Epoch: 5| Step: 5
Training loss: 2.7783732414245605
Validation loss: 2.2690747502029582

Epoch: 5| Step: 6
Training loss: 2.5170881748199463
Validation loss: 2.289707642729564

Epoch: 5| Step: 7
Training loss: 2.559110164642334
Validation loss: 2.2803687280224216

Epoch: 5| Step: 8
Training loss: 2.7720186710357666
Validation loss: 2.2882442397456013

Epoch: 5| Step: 9
Training loss: 2.3444361686706543
Validation loss: 2.296252005843706

Epoch: 5| Step: 10
Training loss: 2.2982139587402344
Validation loss: 2.299946329926932

Epoch: 339| Step: 0
Training loss: 1.9717069864273071
Validation loss: 2.2939815162330546

Epoch: 5| Step: 1
Training loss: 2.616605758666992
Validation loss: 2.3002353560539985

Epoch: 5| Step: 2
Training loss: 2.1706652641296387
Validation loss: 2.30270480596891

Epoch: 5| Step: 3
Training loss: 1.8995949029922485
Validation loss: 2.2993282361697127

Epoch: 5| Step: 4
Training loss: 2.456559419631958
Validation loss: 2.289826049599596

Epoch: 5| Step: 5
Training loss: 2.7683441638946533
Validation loss: 2.2820288071068386

Epoch: 5| Step: 6
Training loss: 2.924842357635498
Validation loss: 2.2742214664336173

Epoch: 5| Step: 7
Training loss: 2.869424343109131
Validation loss: 2.277096434306073

Epoch: 5| Step: 8
Training loss: 2.40777587890625
Validation loss: 2.2758947495491273

Epoch: 5| Step: 9
Training loss: 2.2564921379089355
Validation loss: 2.254644937412713

Epoch: 5| Step: 10
Training loss: 2.4970829486846924
Validation loss: 2.2583694227280153

Epoch: 340| Step: 0
Training loss: 2.008234977722168
Validation loss: 2.2647598405038156

Epoch: 5| Step: 1
Training loss: 2.1091744899749756
Validation loss: 2.278504366515785

Epoch: 5| Step: 2
Training loss: 2.6851630210876465
Validation loss: 2.2835195577272804

Epoch: 5| Step: 3
Training loss: 2.534480571746826
Validation loss: 2.307749871284731

Epoch: 5| Step: 4
Training loss: 3.1877024173736572
Validation loss: 2.3165410052063646

Epoch: 5| Step: 5
Training loss: 1.9586502313613892
Validation loss: 2.3258959606129634

Epoch: 5| Step: 6
Training loss: 2.064089298248291
Validation loss: 2.3421060372424383

Epoch: 5| Step: 7
Training loss: 2.5297515392303467
Validation loss: 2.358328106582806

Epoch: 5| Step: 8
Training loss: 2.252476215362549
Validation loss: 2.330810635320602

Epoch: 5| Step: 9
Training loss: 2.793231964111328
Validation loss: 2.3256594801461823

Epoch: 5| Step: 10
Training loss: 2.591470956802368
Validation loss: 2.294577311444026

Epoch: 341| Step: 0
Training loss: 2.6069092750549316
Validation loss: 2.2837338024570095

Epoch: 5| Step: 1
Training loss: 1.986908197402954
Validation loss: 2.25947973292361

Epoch: 5| Step: 2
Training loss: 2.908937692642212
Validation loss: 2.2611842924548733

Epoch: 5| Step: 3
Training loss: 2.5403201580047607
Validation loss: 2.273500978305776

Epoch: 5| Step: 4
Training loss: 2.6807861328125
Validation loss: 2.270567722218011

Epoch: 5| Step: 5
Training loss: 2.211355686187744
Validation loss: 2.2757678262649046

Epoch: 5| Step: 6
Training loss: 2.656045436859131
Validation loss: 2.2644886457791893

Epoch: 5| Step: 7
Training loss: 2.4008917808532715
Validation loss: 2.2658348327041953

Epoch: 5| Step: 8
Training loss: 2.647960662841797
Validation loss: 2.266755255319739

Epoch: 5| Step: 9
Training loss: 1.955748200416565
Validation loss: 2.257855103861901

Epoch: 5| Step: 10
Training loss: 2.0466530323028564
Validation loss: 2.267356288048529

Epoch: 342| Step: 0
Training loss: 2.3193717002868652
Validation loss: 2.2835900578447568

Epoch: 5| Step: 1
Training loss: 2.294067859649658
Validation loss: 2.275547945371238

Epoch: 5| Step: 2
Training loss: 2.938485860824585
Validation loss: 2.282138145098122

Epoch: 5| Step: 3
Training loss: 2.8386597633361816
Validation loss: 2.2973244267125286

Epoch: 5| Step: 4
Training loss: 2.537205219268799
Validation loss: 2.286186718171643

Epoch: 5| Step: 5
Training loss: 2.2841808795928955
Validation loss: 2.278521114780057

Epoch: 5| Step: 6
Training loss: 1.5462310314178467
Validation loss: 2.2727891501560005

Epoch: 5| Step: 7
Training loss: 2.4199726581573486
Validation loss: 2.2842802104129585

Epoch: 5| Step: 8
Training loss: 3.4322028160095215
Validation loss: 2.275206232583651

Epoch: 5| Step: 9
Training loss: 1.4412963390350342
Validation loss: 2.272593387993433

Epoch: 5| Step: 10
Training loss: 2.5623350143432617
Validation loss: 2.2706467002950688

Epoch: 343| Step: 0
Training loss: 2.529597520828247
Validation loss: 2.2812949278021373

Epoch: 5| Step: 1
Training loss: 2.8536205291748047
Validation loss: 2.287985122332009

Epoch: 5| Step: 2
Training loss: 1.9604103565216064
Validation loss: 2.2822994878215175

Epoch: 5| Step: 3
Training loss: 2.0456578731536865
Validation loss: 2.269958929349017

Epoch: 5| Step: 4
Training loss: 2.8539841175079346
Validation loss: 2.270978835321242

Epoch: 5| Step: 5
Training loss: 2.2635819911956787
Validation loss: 2.268864712407512

Epoch: 5| Step: 6
Training loss: 2.6004979610443115
Validation loss: 2.269366987289921

Epoch: 5| Step: 7
Training loss: 2.4972712993621826
Validation loss: 2.2540041708177134

Epoch: 5| Step: 8
Training loss: 2.7966055870056152
Validation loss: 2.2550949255625405

Epoch: 5| Step: 9
Training loss: 1.945182204246521
Validation loss: 2.242920793512816

Epoch: 5| Step: 10
Training loss: 2.1855316162109375
Validation loss: 2.258458214421426

Epoch: 344| Step: 0
Training loss: 1.9610240459442139
Validation loss: 2.260803780248088

Epoch: 5| Step: 1
Training loss: 2.499117612838745
Validation loss: 2.2754231088904926

Epoch: 5| Step: 2
Training loss: 2.425741672515869
Validation loss: 2.2596322528777586

Epoch: 5| Step: 3
Training loss: 3.283320665359497
Validation loss: 2.259409330224478

Epoch: 5| Step: 4
Training loss: 2.1106534004211426
Validation loss: 2.264782110850016

Epoch: 5| Step: 5
Training loss: 2.7150211334228516
Validation loss: 2.2546667514308805

Epoch: 5| Step: 6
Training loss: 2.7382612228393555
Validation loss: 2.263163088470377

Epoch: 5| Step: 7
Training loss: 2.2320687770843506
Validation loss: 2.2548365413501696

Epoch: 5| Step: 8
Training loss: 2.42787504196167
Validation loss: 2.248250533175725

Epoch: 5| Step: 9
Training loss: 2.120506763458252
Validation loss: 2.26751773459937

Epoch: 5| Step: 10
Training loss: 1.9026339054107666
Validation loss: 2.2525954425975843

Epoch: 345| Step: 0
Training loss: 1.8853042125701904
Validation loss: 2.2583847802172423

Epoch: 5| Step: 1
Training loss: 1.958706259727478
Validation loss: 2.2494783709126134

Epoch: 5| Step: 2
Training loss: 2.788393974304199
Validation loss: 2.2568394701967955

Epoch: 5| Step: 3
Training loss: 1.7126152515411377
Validation loss: 2.25874262984081

Epoch: 5| Step: 4
Training loss: 2.8845832347869873
Validation loss: 2.245348599649245

Epoch: 5| Step: 5
Training loss: 2.172597646713257
Validation loss: 2.275408434611495

Epoch: 5| Step: 6
Training loss: 2.120110034942627
Validation loss: 2.2853477642100346

Epoch: 5| Step: 7
Training loss: 2.4693942070007324
Validation loss: 2.304936574351403

Epoch: 5| Step: 8
Training loss: 2.923015594482422
Validation loss: 2.3184405911353325

Epoch: 5| Step: 9
Training loss: 2.2466652393341064
Validation loss: 2.308154957268828

Epoch: 5| Step: 10
Training loss: 3.4919285774230957
Validation loss: 2.3050855052086616

Epoch: 346| Step: 0
Training loss: 2.3066279888153076
Validation loss: 2.303321235923357

Epoch: 5| Step: 1
Training loss: 2.6987013816833496
Validation loss: 2.3025170167287192

Epoch: 5| Step: 2
Training loss: 2.6195106506347656
Validation loss: 2.274348875527741

Epoch: 5| Step: 3
Training loss: 1.7440265417099
Validation loss: 2.271735901473671

Epoch: 5| Step: 4
Training loss: 3.0561203956604004
Validation loss: 2.2565978573214625

Epoch: 5| Step: 5
Training loss: 2.330256700515747
Validation loss: 2.2415496662098873

Epoch: 5| Step: 6
Training loss: 2.414280652999878
Validation loss: 2.2288214596368934

Epoch: 5| Step: 7
Training loss: 2.1174769401550293
Validation loss: 2.223385633960847

Epoch: 5| Step: 8
Training loss: 2.2472658157348633
Validation loss: 2.2321280676831483

Epoch: 5| Step: 9
Training loss: 2.018251419067383
Validation loss: 2.219956209582667

Epoch: 5| Step: 10
Training loss: 2.92494797706604
Validation loss: 2.25433672371731

Epoch: 347| Step: 0
Training loss: 2.4389426708221436
Validation loss: 2.2553584370561826

Epoch: 5| Step: 1
Training loss: 2.976710557937622
Validation loss: 2.2923532480834634

Epoch: 5| Step: 2
Training loss: 2.119697093963623
Validation loss: 2.3056204806091967

Epoch: 5| Step: 3
Training loss: 2.5475914478302
Validation loss: 2.303305100369197

Epoch: 5| Step: 4
Training loss: 2.071643590927124
Validation loss: 2.2969764586417907

Epoch: 5| Step: 5
Training loss: 1.8623679876327515
Validation loss: 2.2900513243931595

Epoch: 5| Step: 6
Training loss: 2.281013011932373
Validation loss: 2.2831329068829938

Epoch: 5| Step: 7
Training loss: 2.6759018898010254
Validation loss: 2.268490670829691

Epoch: 5| Step: 8
Training loss: 2.5792737007141113
Validation loss: 2.2659593448844007

Epoch: 5| Step: 9
Training loss: 2.669464588165283
Validation loss: 2.2726293379260647

Epoch: 5| Step: 10
Training loss: 2.3511035442352295
Validation loss: 2.2699841786456365

Epoch: 348| Step: 0
Training loss: 2.7138705253601074
Validation loss: 2.263279304709486

Epoch: 5| Step: 1
Training loss: 2.4697365760803223
Validation loss: 2.2701969864547893

Epoch: 5| Step: 2
Training loss: 2.6141438484191895
Validation loss: 2.2739889493552585

Epoch: 5| Step: 3
Training loss: 2.7553372383117676
Validation loss: 2.2700061734004686

Epoch: 5| Step: 4
Training loss: 2.1878700256347656
Validation loss: 2.2744964630373063

Epoch: 5| Step: 5
Training loss: 2.0868759155273438
Validation loss: 2.277972031665105

Epoch: 5| Step: 6
Training loss: 2.2901835441589355
Validation loss: 2.2898736205152286

Epoch: 5| Step: 7
Training loss: 3.02353572845459
Validation loss: 2.2692342778687835

Epoch: 5| Step: 8
Training loss: 1.850188970565796
Validation loss: 2.279375127566758

Epoch: 5| Step: 9
Training loss: 2.3451178073883057
Validation loss: 2.287768692098638

Epoch: 5| Step: 10
Training loss: 2.1801323890686035
Validation loss: 2.287802924392044

Epoch: 349| Step: 0
Training loss: 2.71891450881958
Validation loss: 2.274582073252688

Epoch: 5| Step: 1
Training loss: 3.4374001026153564
Validation loss: 2.2787381141416487

Epoch: 5| Step: 2
Training loss: 2.148592233657837
Validation loss: 2.2931785224586405

Epoch: 5| Step: 3
Training loss: 1.7382326126098633
Validation loss: 2.265339333523986

Epoch: 5| Step: 4
Training loss: 1.5969302654266357
Validation loss: 2.279654989960373

Epoch: 5| Step: 5
Training loss: 2.4313220977783203
Validation loss: 2.2715644503152497

Epoch: 5| Step: 6
Training loss: 2.620209217071533
Validation loss: 2.2679859002431235

Epoch: 5| Step: 7
Training loss: 3.101933002471924
Validation loss: 2.255262194141265

Epoch: 5| Step: 8
Training loss: 2.1010589599609375
Validation loss: 2.258809740825366

Epoch: 5| Step: 9
Training loss: 1.9613037109375
Validation loss: 2.2475047598602953

Epoch: 5| Step: 10
Training loss: 2.4896113872528076
Validation loss: 2.2545580607588573

Epoch: 350| Step: 0
Training loss: 2.454254627227783
Validation loss: 2.2449836064410467

Epoch: 5| Step: 1
Training loss: 2.2233898639678955
Validation loss: 2.237580189140894

Epoch: 5| Step: 2
Training loss: 2.4149742126464844
Validation loss: 2.2419867874473653

Epoch: 5| Step: 3
Training loss: 1.6458923816680908
Validation loss: 2.2326188677100727

Epoch: 5| Step: 4
Training loss: 2.7495646476745605
Validation loss: 2.248233699029492

Epoch: 5| Step: 5
Training loss: 3.3227012157440186
Validation loss: 2.2357725174196306

Epoch: 5| Step: 6
Training loss: 1.6327667236328125
Validation loss: 2.2177695664026404

Epoch: 5| Step: 7
Training loss: 2.9880614280700684
Validation loss: 2.220225462349512

Epoch: 5| Step: 8
Training loss: 2.3285274505615234
Validation loss: 2.2213989355230845

Epoch: 5| Step: 9
Training loss: 2.662628173828125
Validation loss: 2.225172358174478

Epoch: 5| Step: 10
Training loss: 1.760157823562622
Validation loss: 2.235044297351632

Epoch: 351| Step: 0
Training loss: 2.034055471420288
Validation loss: 2.241819489386774

Epoch: 5| Step: 1
Training loss: 1.921252965927124
Validation loss: 2.249007435255153

Epoch: 5| Step: 2
Training loss: 2.1310019493103027
Validation loss: 2.2549760085280224

Epoch: 5| Step: 3
Training loss: 3.0352683067321777
Validation loss: 2.2573230266571045

Epoch: 5| Step: 4
Training loss: 2.6055757999420166
Validation loss: 2.2598854751997095

Epoch: 5| Step: 5
Training loss: 2.3279411792755127
Validation loss: 2.2731642223173574

Epoch: 5| Step: 6
Training loss: 2.572993755340576
Validation loss: 2.28338966061992

Epoch: 5| Step: 7
Training loss: 2.300917148590088
Validation loss: 2.2839871606519146

Epoch: 5| Step: 8
Training loss: 2.7748470306396484
Validation loss: 2.2867289691843014

Epoch: 5| Step: 9
Training loss: 1.7784993648529053
Validation loss: 2.2950072698695685

Epoch: 5| Step: 10
Training loss: 2.858273506164551
Validation loss: 2.282864470635691

Epoch: 352| Step: 0
Training loss: 2.0914719104766846
Validation loss: 2.259995545110395

Epoch: 5| Step: 1
Training loss: 2.1213996410369873
Validation loss: 2.256342577677901

Epoch: 5| Step: 2
Training loss: 1.9671592712402344
Validation loss: 2.253526959367978

Epoch: 5| Step: 3
Training loss: 2.3476314544677734
Validation loss: 2.2716347543142175

Epoch: 5| Step: 4
Training loss: 2.2220585346221924
Validation loss: 2.2585248331869803

Epoch: 5| Step: 5
Training loss: 2.2834231853485107
Validation loss: 2.2623871705865346

Epoch: 5| Step: 6
Training loss: 2.9794764518737793
Validation loss: 2.262861277467461

Epoch: 5| Step: 7
Training loss: 2.9882125854492188
Validation loss: 2.272660208004777

Epoch: 5| Step: 8
Training loss: 2.2881195545196533
Validation loss: 2.270892504722841

Epoch: 5| Step: 9
Training loss: 2.0908374786376953
Validation loss: 2.2499025073102725

Epoch: 5| Step: 10
Training loss: 2.931338310241699
Validation loss: 2.247966215174685

Epoch: 353| Step: 0
Training loss: 2.759915828704834
Validation loss: 2.244406338660948

Epoch: 5| Step: 1
Training loss: 2.768751859664917
Validation loss: 2.2431225469035487

Epoch: 5| Step: 2
Training loss: 1.9461950063705444
Validation loss: 2.229258427055933

Epoch: 5| Step: 3
Training loss: 2.320826768875122
Validation loss: 2.224174968657955

Epoch: 5| Step: 4
Training loss: 1.7306209802627563
Validation loss: 2.2338357817742134

Epoch: 5| Step: 5
Training loss: 2.0287868976593018
Validation loss: 2.231607698625134

Epoch: 5| Step: 6
Training loss: 2.8793609142303467
Validation loss: 2.2494195635600756

Epoch: 5| Step: 7
Training loss: 2.703059196472168
Validation loss: 2.2614987511788645

Epoch: 5| Step: 8
Training loss: 2.5056240558624268
Validation loss: 2.2711454027442524

Epoch: 5| Step: 9
Training loss: 2.2112793922424316
Validation loss: 2.2733135325934297

Epoch: 5| Step: 10
Training loss: 2.5997109413146973
Validation loss: 2.292423056017968

Epoch: 354| Step: 0
Training loss: 2.59224009513855
Validation loss: 2.292399767906435

Epoch: 5| Step: 1
Training loss: 1.8255046606063843
Validation loss: 2.276723369475334

Epoch: 5| Step: 2
Training loss: 2.2035319805145264
Validation loss: 2.2700476159331617

Epoch: 5| Step: 3
Training loss: 3.1129891872406006
Validation loss: 2.2584640159401843

Epoch: 5| Step: 4
Training loss: 1.6181272268295288
Validation loss: 2.2408499717712402

Epoch: 5| Step: 5
Training loss: 2.693167209625244
Validation loss: 2.226560961815619

Epoch: 5| Step: 6
Training loss: 2.31378173828125
Validation loss: 2.2376968476080124

Epoch: 5| Step: 7
Training loss: 2.5516517162323
Validation loss: 2.2202246035298994

Epoch: 5| Step: 8
Training loss: 2.7539267539978027
Validation loss: 2.2173238274871663

Epoch: 5| Step: 9
Training loss: 2.853268623352051
Validation loss: 2.217895318103093

Epoch: 5| Step: 10
Training loss: 1.6529896259307861
Validation loss: 2.221378354616063

Epoch: 355| Step: 0
Training loss: 2.3462588787078857
Validation loss: 2.232348560005106

Epoch: 5| Step: 1
Training loss: 1.9413938522338867
Validation loss: 2.229709909808251

Epoch: 5| Step: 2
Training loss: 2.4694318771362305
Validation loss: 2.2322461733254055

Epoch: 5| Step: 3
Training loss: 2.5706586837768555
Validation loss: 2.2321929829095

Epoch: 5| Step: 4
Training loss: 2.3152928352355957
Validation loss: 2.2290436734435377

Epoch: 5| Step: 5
Training loss: 2.19885516166687
Validation loss: 2.222759980027394

Epoch: 5| Step: 6
Training loss: 2.6742496490478516
Validation loss: 2.2361813270917503

Epoch: 5| Step: 7
Training loss: 2.798677921295166
Validation loss: 2.2358132357238443

Epoch: 5| Step: 8
Training loss: 2.2530667781829834
Validation loss: 2.261696269435267

Epoch: 5| Step: 9
Training loss: 2.4924752712249756
Validation loss: 2.260253934450047

Epoch: 5| Step: 10
Training loss: 2.145875930786133
Validation loss: 2.2814082125181794

Epoch: 356| Step: 0
Training loss: 2.501915693283081
Validation loss: 2.282053250138478

Epoch: 5| Step: 1
Training loss: 2.6553335189819336
Validation loss: 2.2589169599676646

Epoch: 5| Step: 2
Training loss: 2.1627349853515625
Validation loss: 2.2586196981450564

Epoch: 5| Step: 3
Training loss: 2.378041982650757
Validation loss: 2.2573208373080016

Epoch: 5| Step: 4
Training loss: 2.5886878967285156
Validation loss: 2.2551527971862466

Epoch: 5| Step: 5
Training loss: 2.635078191757202
Validation loss: 2.242942415257936

Epoch: 5| Step: 6
Training loss: 1.8721349239349365
Validation loss: 2.2361469948163597

Epoch: 5| Step: 7
Training loss: 2.259782075881958
Validation loss: 2.241610170692526

Epoch: 5| Step: 8
Training loss: 2.1902308464050293
Validation loss: 2.2394397104940107

Epoch: 5| Step: 9
Training loss: 2.3866374492645264
Validation loss: 2.2363056367443455

Epoch: 5| Step: 10
Training loss: 2.670027256011963
Validation loss: 2.2460310741137435

Epoch: 357| Step: 0
Training loss: 2.8532392978668213
Validation loss: 2.2475730065376527

Epoch: 5| Step: 1
Training loss: 2.272352933883667
Validation loss: 2.2448646560792

Epoch: 5| Step: 2
Training loss: 2.5219974517822266
Validation loss: 2.252819835498769

Epoch: 5| Step: 3
Training loss: 2.1290221214294434
Validation loss: 2.263314188167613

Epoch: 5| Step: 4
Training loss: 1.8468739986419678
Validation loss: 2.258294418293943

Epoch: 5| Step: 5
Training loss: 2.3947315216064453
Validation loss: 2.249873322825278

Epoch: 5| Step: 6
Training loss: 2.3869221210479736
Validation loss: 2.268022980741275

Epoch: 5| Step: 7
Training loss: 2.625492811203003
Validation loss: 2.2645205067050074

Epoch: 5| Step: 8
Training loss: 2.0446460247039795
Validation loss: 2.2494365989520984

Epoch: 5| Step: 9
Training loss: 2.059959888458252
Validation loss: 2.240794235660184

Epoch: 5| Step: 10
Training loss: 3.111541986465454
Validation loss: 2.226879144227633

Epoch: 358| Step: 0
Training loss: 2.1703102588653564
Validation loss: 2.2319568536614858

Epoch: 5| Step: 1
Training loss: 2.7851929664611816
Validation loss: 2.2228161622119207

Epoch: 5| Step: 2
Training loss: 2.353494644165039
Validation loss: 2.228907915853685

Epoch: 5| Step: 3
Training loss: 2.4931271076202393
Validation loss: 2.220756992217033

Epoch: 5| Step: 4
Training loss: 2.250154972076416
Validation loss: 2.2363581888137327

Epoch: 5| Step: 5
Training loss: 3.227757215499878
Validation loss: 2.2448905898678686

Epoch: 5| Step: 6
Training loss: 2.1166982650756836
Validation loss: 2.2330335673465522

Epoch: 5| Step: 7
Training loss: 2.043592929840088
Validation loss: 2.2518892467662854

Epoch: 5| Step: 8
Training loss: 2.1621196269989014
Validation loss: 2.2526547165327173

Epoch: 5| Step: 9
Training loss: 2.3020076751708984
Validation loss: 2.248793045679728

Epoch: 5| Step: 10
Training loss: 2.1932785511016846
Validation loss: 2.253981136506604

Epoch: 359| Step: 0
Training loss: 1.9359099864959717
Validation loss: 2.2500067308384883

Epoch: 5| Step: 1
Training loss: 2.166473627090454
Validation loss: 2.2694659207456853

Epoch: 5| Step: 2
Training loss: 2.522850751876831
Validation loss: 2.2553371614025486

Epoch: 5| Step: 3
Training loss: 2.560070514678955
Validation loss: 2.2872077367639028

Epoch: 5| Step: 4
Training loss: 2.711148262023926
Validation loss: 2.273453156153361

Epoch: 5| Step: 5
Training loss: 2.4547085762023926
Validation loss: 2.280937833170737

Epoch: 5| Step: 6
Training loss: 2.328779697418213
Validation loss: 2.2602175281893824

Epoch: 5| Step: 7
Training loss: 2.5728917121887207
Validation loss: 2.263673292693271

Epoch: 5| Step: 8
Training loss: 2.0890755653381348
Validation loss: 2.242316310123731

Epoch: 5| Step: 9
Training loss: 2.9260852336883545
Validation loss: 2.2386561209155666

Epoch: 5| Step: 10
Training loss: 1.7544565200805664
Validation loss: 2.2200494658562446

Epoch: 360| Step: 0
Training loss: 2.53126859664917
Validation loss: 2.2266435469350507

Epoch: 5| Step: 1
Training loss: 2.1903626918792725
Validation loss: 2.2343251935897337

Epoch: 5| Step: 2
Training loss: 1.8569133281707764
Validation loss: 2.2421626198676323

Epoch: 5| Step: 3
Training loss: 2.347017765045166
Validation loss: 2.2554689786767446

Epoch: 5| Step: 4
Training loss: 1.7488054037094116
Validation loss: 2.236075044960104

Epoch: 5| Step: 5
Training loss: 2.813340187072754
Validation loss: 2.2622020116416355

Epoch: 5| Step: 6
Training loss: 2.6333818435668945
Validation loss: 2.253207524617513

Epoch: 5| Step: 7
Training loss: 2.984330177307129
Validation loss: 2.2377733940719278

Epoch: 5| Step: 8
Training loss: 2.6440258026123047
Validation loss: 2.241957687562512

Epoch: 5| Step: 9
Training loss: 2.1313552856445312
Validation loss: 2.2310110163945023

Epoch: 5| Step: 10
Training loss: 2.1430227756500244
Validation loss: 2.2273484199277815

Epoch: 361| Step: 0
Training loss: 1.7795326709747314
Validation loss: 2.229698047843031

Epoch: 5| Step: 1
Training loss: 2.39860463142395
Validation loss: 2.2351241521937872

Epoch: 5| Step: 2
Training loss: 2.6246626377105713
Validation loss: 2.2375767333533174

Epoch: 5| Step: 3
Training loss: 3.0972812175750732
Validation loss: 2.24391346849421

Epoch: 5| Step: 4
Training loss: 2.882660388946533
Validation loss: 2.2326335881346013

Epoch: 5| Step: 5
Training loss: 2.9482109546661377
Validation loss: 2.246993595553983

Epoch: 5| Step: 6
Training loss: 1.6270087957382202
Validation loss: 2.240769342709613

Epoch: 5| Step: 7
Training loss: 1.9882673025131226
Validation loss: 2.2384230654726744

Epoch: 5| Step: 8
Training loss: 1.4867832660675049
Validation loss: 2.2454305502676193

Epoch: 5| Step: 9
Training loss: 2.307018756866455
Validation loss: 2.2483150010467856

Epoch: 5| Step: 10
Training loss: 3.128436803817749
Validation loss: 2.2536993783007384

Epoch: 362| Step: 0
Training loss: 2.4754254817962646
Validation loss: 2.268255272219258

Epoch: 5| Step: 1
Training loss: 2.4489352703094482
Validation loss: 2.266125573906847

Epoch: 5| Step: 2
Training loss: 1.7767976522445679
Validation loss: 2.2893845817094207

Epoch: 5| Step: 3
Training loss: 1.9784702062606812
Validation loss: 2.287155641022549

Epoch: 5| Step: 4
Training loss: 2.4234566688537598
Validation loss: 2.2847042904105237

Epoch: 5| Step: 5
Training loss: 3.0874369144439697
Validation loss: 2.2769004016794185

Epoch: 5| Step: 6
Training loss: 2.6128246784210205
Validation loss: 2.264771653759864

Epoch: 5| Step: 7
Training loss: 2.2614009380340576
Validation loss: 2.266072170708769

Epoch: 5| Step: 8
Training loss: 2.6845545768737793
Validation loss: 2.2213769100045644

Epoch: 5| Step: 9
Training loss: 2.5115065574645996
Validation loss: 2.2331299166525564

Epoch: 5| Step: 10
Training loss: 1.8476186990737915
Validation loss: 2.221537105498775

Epoch: 363| Step: 0
Training loss: 2.3893275260925293
Validation loss: 2.213360701837847

Epoch: 5| Step: 1
Training loss: 2.0074656009674072
Validation loss: 2.2279578844706216

Epoch: 5| Step: 2
Training loss: 2.016273021697998
Validation loss: 2.222400180755123

Epoch: 5| Step: 3
Training loss: 2.114487886428833
Validation loss: 2.2263474105506815

Epoch: 5| Step: 4
Training loss: 2.59574556350708
Validation loss: 2.2291696994535384

Epoch: 5| Step: 5
Training loss: 2.21669340133667
Validation loss: 2.2412028953593266

Epoch: 5| Step: 6
Training loss: 2.5807721614837646
Validation loss: 2.223855636453116

Epoch: 5| Step: 7
Training loss: 3.0771281719207764
Validation loss: 2.23936737737348

Epoch: 5| Step: 8
Training loss: 2.2833454608917236
Validation loss: 2.2363748729869886

Epoch: 5| Step: 9
Training loss: 2.922003984451294
Validation loss: 2.229219922455408

Epoch: 5| Step: 10
Training loss: 1.6862748861312866
Validation loss: 2.2403139401507635

Epoch: 364| Step: 0
Training loss: 2.456059455871582
Validation loss: 2.2298130989074707

Epoch: 5| Step: 1
Training loss: 2.323713779449463
Validation loss: 2.240073460404591

Epoch: 5| Step: 2
Training loss: 2.2390148639678955
Validation loss: 2.218040499635922

Epoch: 5| Step: 3
Training loss: 1.788527250289917
Validation loss: 2.232390658829802

Epoch: 5| Step: 4
Training loss: 2.512212038040161
Validation loss: 2.2367396764857794

Epoch: 5| Step: 5
Training loss: 2.1028521060943604
Validation loss: 2.248620761338101

Epoch: 5| Step: 6
Training loss: 2.2615716457366943
Validation loss: 2.233849815143052

Epoch: 5| Step: 7
Training loss: 1.8423213958740234
Validation loss: 2.235111162226687

Epoch: 5| Step: 8
Training loss: 2.679983615875244
Validation loss: 2.238888854621559

Epoch: 5| Step: 9
Training loss: 2.8494420051574707
Validation loss: 2.2450455363078783

Epoch: 5| Step: 10
Training loss: 3.0558865070343018
Validation loss: 2.2465217087858464

Epoch: 365| Step: 0
Training loss: 2.487945079803467
Validation loss: 2.268357943463069

Epoch: 5| Step: 1
Training loss: 2.222160577774048
Validation loss: 2.2640500735211115

Epoch: 5| Step: 2
Training loss: 2.986443281173706
Validation loss: 2.287400743012787

Epoch: 5| Step: 3
Training loss: 2.4317398071289062
Validation loss: 2.2706407090669036

Epoch: 5| Step: 4
Training loss: 1.850250005722046
Validation loss: 2.259342437149376

Epoch: 5| Step: 5
Training loss: 2.315683364868164
Validation loss: 2.2427502421922583

Epoch: 5| Step: 6
Training loss: 2.251098394393921
Validation loss: 2.2419147670909925

Epoch: 5| Step: 7
Training loss: 2.2723796367645264
Validation loss: 2.2395675118251512

Epoch: 5| Step: 8
Training loss: 2.7563438415527344
Validation loss: 2.234030455671331

Epoch: 5| Step: 9
Training loss: 2.174546718597412
Validation loss: 2.2271448848068074

Epoch: 5| Step: 10
Training loss: 2.361928701400757
Validation loss: 2.224581515917214

Epoch: 366| Step: 0
Training loss: 2.2505850791931152
Validation loss: 2.232431624525337

Epoch: 5| Step: 1
Training loss: 2.3815760612487793
Validation loss: 2.2144092257304857

Epoch: 5| Step: 2
Training loss: 2.348712205886841
Validation loss: 2.2262395838255524

Epoch: 5| Step: 3
Training loss: 2.5762715339660645
Validation loss: 2.2287433634522142

Epoch: 5| Step: 4
Training loss: 1.9950602054595947
Validation loss: 2.2342009723827405

Epoch: 5| Step: 5
Training loss: 3.0659680366516113
Validation loss: 2.2328294707882788

Epoch: 5| Step: 6
Training loss: 2.10162353515625
Validation loss: 2.2397501981386574

Epoch: 5| Step: 7
Training loss: 2.243741989135742
Validation loss: 2.2499763606697

Epoch: 5| Step: 8
Training loss: 2.3522753715515137
Validation loss: 2.249264540210847

Epoch: 5| Step: 9
Training loss: 2.3003289699554443
Validation loss: 2.2627011909279773

Epoch: 5| Step: 10
Training loss: 2.3947269916534424
Validation loss: 2.2595733763069235

Epoch: 367| Step: 0
Training loss: 2.654198408126831
Validation loss: 2.257634773049303

Epoch: 5| Step: 1
Training loss: 2.3507285118103027
Validation loss: 2.250262870583483

Epoch: 5| Step: 2
Training loss: 1.7702834606170654
Validation loss: 2.236000730145362

Epoch: 5| Step: 3
Training loss: 2.241413116455078
Validation loss: 2.2318987128555134

Epoch: 5| Step: 4
Training loss: 2.469909191131592
Validation loss: 2.2349600804749357

Epoch: 5| Step: 5
Training loss: 2.169736862182617
Validation loss: 2.216954679899318

Epoch: 5| Step: 6
Training loss: 2.8049213886260986
Validation loss: 2.2266278215633926

Epoch: 5| Step: 7
Training loss: 2.0621564388275146
Validation loss: 2.2360444940546507

Epoch: 5| Step: 8
Training loss: 2.158703565597534
Validation loss: 2.2169003768633773

Epoch: 5| Step: 9
Training loss: 2.3834426403045654
Validation loss: 2.21063575436992

Epoch: 5| Step: 10
Training loss: 2.9038729667663574
Validation loss: 2.218019099645717

Epoch: 368| Step: 0
Training loss: 2.3722071647644043
Validation loss: 2.221334226669804

Epoch: 5| Step: 1
Training loss: 2.3103976249694824
Validation loss: 2.231094665424798

Epoch: 5| Step: 2
Training loss: 1.8961251974105835
Validation loss: 2.223211016706241

Epoch: 5| Step: 3
Training loss: 2.737776041030884
Validation loss: 2.241173815983598

Epoch: 5| Step: 4
Training loss: 1.5082570314407349
Validation loss: 2.243751688670087

Epoch: 5| Step: 5
Training loss: 2.673362970352173
Validation loss: 2.249330961576072

Epoch: 5| Step: 6
Training loss: 2.881439685821533
Validation loss: 2.257488432750907

Epoch: 5| Step: 7
Training loss: 2.2420318126678467
Validation loss: 2.2631259451630297

Epoch: 5| Step: 8
Training loss: 2.1304190158843994
Validation loss: 2.2541343127527544

Epoch: 5| Step: 9
Training loss: 2.7769882678985596
Validation loss: 2.2589685122172036

Epoch: 5| Step: 10
Training loss: 2.3782927989959717
Validation loss: 2.253904605424532

Epoch: 369| Step: 0
Training loss: 2.110806465148926
Validation loss: 2.2571364679644184

Epoch: 5| Step: 1
Training loss: 3.0015127658843994
Validation loss: 2.2330304627777426

Epoch: 5| Step: 2
Training loss: 2.1581778526306152
Validation loss: 2.245336265974147

Epoch: 5| Step: 3
Training loss: 2.8385231494903564
Validation loss: 2.236358646423586

Epoch: 5| Step: 4
Training loss: 2.4686384201049805
Validation loss: 2.236397156151392

Epoch: 5| Step: 5
Training loss: 2.200974464416504
Validation loss: 2.2377541321580128

Epoch: 5| Step: 6
Training loss: 2.4670169353485107
Validation loss: 2.2169076319663756

Epoch: 5| Step: 7
Training loss: 2.0241270065307617
Validation loss: 2.2102857135957286

Epoch: 5| Step: 8
Training loss: 2.4295761585235596
Validation loss: 2.1988025634519515

Epoch: 5| Step: 9
Training loss: 1.9636808633804321
Validation loss: 2.1939653606824976

Epoch: 5| Step: 10
Training loss: 2.2910232543945312
Validation loss: 2.2172552052364556

Epoch: 370| Step: 0
Training loss: 2.3834261894226074
Validation loss: 2.235653585003268

Epoch: 5| Step: 1
Training loss: 1.934602975845337
Validation loss: 2.248561987312891

Epoch: 5| Step: 2
Training loss: 2.0951170921325684
Validation loss: 2.2377171542054866

Epoch: 5| Step: 3
Training loss: 3.036250114440918
Validation loss: 2.2471999609342186

Epoch: 5| Step: 4
Training loss: 2.371046781539917
Validation loss: 2.2552029317425144

Epoch: 5| Step: 5
Training loss: 2.601381301879883
Validation loss: 2.236220713584654

Epoch: 5| Step: 6
Training loss: 2.6725707054138184
Validation loss: 2.2391307379609797

Epoch: 5| Step: 7
Training loss: 2.484532117843628
Validation loss: 2.2487980242698424

Epoch: 5| Step: 8
Training loss: 2.3879294395446777
Validation loss: 2.2409642909162786

Epoch: 5| Step: 9
Training loss: 2.0314643383026123
Validation loss: 2.238384444226501

Epoch: 5| Step: 10
Training loss: 2.008918523788452
Validation loss: 2.2372669058461345

Epoch: 371| Step: 0
Training loss: 1.837514877319336
Validation loss: 2.2398221749131397

Epoch: 5| Step: 1
Training loss: 2.5786638259887695
Validation loss: 2.23507498925732

Epoch: 5| Step: 2
Training loss: 2.311250686645508
Validation loss: 2.237587439116611

Epoch: 5| Step: 3
Training loss: 2.832658052444458
Validation loss: 2.2339062536916425

Epoch: 5| Step: 4
Training loss: 2.181612253189087
Validation loss: 2.248138521307258

Epoch: 5| Step: 5
Training loss: 2.7647223472595215
Validation loss: 2.260513823519471

Epoch: 5| Step: 6
Training loss: 1.7749989032745361
Validation loss: 2.270616026334865

Epoch: 5| Step: 7
Training loss: 2.5254573822021484
Validation loss: 2.2708616769441994

Epoch: 5| Step: 8
Training loss: 2.6913044452667236
Validation loss: 2.2743805813533005

Epoch: 5| Step: 9
Training loss: 2.167003870010376
Validation loss: 2.2505438276516494

Epoch: 5| Step: 10
Training loss: 2.300016403198242
Validation loss: 2.244006064630324

Epoch: 372| Step: 0
Training loss: 1.8647387027740479
Validation loss: 2.2321689680058467

Epoch: 5| Step: 1
Training loss: 2.6084561347961426
Validation loss: 2.2123922840241463

Epoch: 5| Step: 2
Training loss: 2.1325650215148926
Validation loss: 2.218108164366855

Epoch: 5| Step: 3
Training loss: 2.4102020263671875
Validation loss: 2.2088729694325435

Epoch: 5| Step: 4
Training loss: 1.5017030239105225
Validation loss: 2.2207576664545203

Epoch: 5| Step: 5
Training loss: 2.614809036254883
Validation loss: 2.2048698573984127

Epoch: 5| Step: 6
Training loss: 2.1262972354888916
Validation loss: 2.204376571921892

Epoch: 5| Step: 7
Training loss: 2.7163009643554688
Validation loss: 2.2036541277362454

Epoch: 5| Step: 8
Training loss: 2.7006237506866455
Validation loss: 2.2136735070136284

Epoch: 5| Step: 9
Training loss: 2.545698642730713
Validation loss: 2.2062534721948768

Epoch: 5| Step: 10
Training loss: 2.633784055709839
Validation loss: 2.206088799302296

Epoch: 373| Step: 0
Training loss: 1.8758697509765625
Validation loss: 2.2027530670166016

Epoch: 5| Step: 1
Training loss: 2.590087413787842
Validation loss: 2.2210248336997083

Epoch: 5| Step: 2
Training loss: 2.5402233600616455
Validation loss: 2.2306401550128894

Epoch: 5| Step: 3
Training loss: 2.259866714477539
Validation loss: 2.2388694209437214

Epoch: 5| Step: 4
Training loss: 1.85575270652771
Validation loss: 2.227257115866548

Epoch: 5| Step: 5
Training loss: 2.4594027996063232
Validation loss: 2.231679031925817

Epoch: 5| Step: 6
Training loss: 2.6149730682373047
Validation loss: 2.223392963409424

Epoch: 5| Step: 7
Training loss: 2.1860721111297607
Validation loss: 2.2147853553936048

Epoch: 5| Step: 8
Training loss: 2.7005603313446045
Validation loss: 2.22186476953568

Epoch: 5| Step: 9
Training loss: 2.4340999126434326
Validation loss: 2.2238412134109007

Epoch: 5| Step: 10
Training loss: 2.288755178451538
Validation loss: 2.225446172939834

Epoch: 374| Step: 0
Training loss: 2.0856215953826904
Validation loss: 2.228320026910433

Epoch: 5| Step: 1
Training loss: 1.8705542087554932
Validation loss: 2.224876796045611

Epoch: 5| Step: 2
Training loss: 1.955446481704712
Validation loss: 2.24134551325152

Epoch: 5| Step: 3
Training loss: 3.063718318939209
Validation loss: 2.223016313327256

Epoch: 5| Step: 4
Training loss: 2.6306185722351074
Validation loss: 2.242654195395849

Epoch: 5| Step: 5
Training loss: 2.6414618492126465
Validation loss: 2.240371952774704

Epoch: 5| Step: 6
Training loss: 1.8345386981964111
Validation loss: 2.2248605733276694

Epoch: 5| Step: 7
Training loss: 2.5445380210876465
Validation loss: 2.2486344768155004

Epoch: 5| Step: 8
Training loss: 2.8496434688568115
Validation loss: 2.222456024539086

Epoch: 5| Step: 9
Training loss: 2.283499002456665
Validation loss: 2.217839397409911

Epoch: 5| Step: 10
Training loss: 1.9650408029556274
Validation loss: 2.2029122870455504

Epoch: 375| Step: 0
Training loss: 1.8172452449798584
Validation loss: 2.210997235390448

Epoch: 5| Step: 1
Training loss: 2.7389931678771973
Validation loss: 2.201919992764791

Epoch: 5| Step: 2
Training loss: 2.5514168739318848
Validation loss: 2.222328693636002

Epoch: 5| Step: 3
Training loss: 2.3749470710754395
Validation loss: 2.2021487169368292

Epoch: 5| Step: 4
Training loss: 2.9875903129577637
Validation loss: 2.1908356323037097

Epoch: 5| Step: 5
Training loss: 2.1769323348999023
Validation loss: 2.1899616987474504

Epoch: 5| Step: 6
Training loss: 2.081606388092041
Validation loss: 2.1900315579547676

Epoch: 5| Step: 7
Training loss: 2.5150060653686523
Validation loss: 2.1976522937897713

Epoch: 5| Step: 8
Training loss: 2.1116325855255127
Validation loss: 2.1891751315004084

Epoch: 5| Step: 9
Training loss: 1.826867699623108
Validation loss: 2.1963439961915374

Epoch: 5| Step: 10
Training loss: 2.6659445762634277
Validation loss: 2.217081351946759

Epoch: 376| Step: 0
Training loss: 3.3059113025665283
Validation loss: 2.2184294769840855

Epoch: 5| Step: 1
Training loss: 2.379287004470825
Validation loss: 2.2184454036015335

Epoch: 5| Step: 2
Training loss: 2.3730194568634033
Validation loss: 2.207335869471232

Epoch: 5| Step: 3
Training loss: 2.5300464630126953
Validation loss: 2.211400024352535

Epoch: 5| Step: 4
Training loss: 2.1474976539611816
Validation loss: 2.2082465207704933

Epoch: 5| Step: 5
Training loss: 2.189732313156128
Validation loss: 2.201996079055212

Epoch: 5| Step: 6
Training loss: 1.8676131963729858
Validation loss: 2.2222253276455786

Epoch: 5| Step: 7
Training loss: 2.0556297302246094
Validation loss: 2.2106041190444783

Epoch: 5| Step: 8
Training loss: 2.695948839187622
Validation loss: 2.205786076925134

Epoch: 5| Step: 9
Training loss: 2.488102436065674
Validation loss: 2.2131620222522366

Epoch: 5| Step: 10
Training loss: 1.5725358724594116
Validation loss: 2.232103378542008

Epoch: 377| Step: 0
Training loss: 2.670187473297119
Validation loss: 2.228779805603848

Epoch: 5| Step: 1
Training loss: 2.242830753326416
Validation loss: 2.2224171379561066

Epoch: 5| Step: 2
Training loss: 2.2396178245544434
Validation loss: 2.2377553088690645

Epoch: 5| Step: 3
Training loss: 2.2989211082458496
Validation loss: 2.2505212547958537

Epoch: 5| Step: 4
Training loss: 2.070859432220459
Validation loss: 2.251865485663055

Epoch: 5| Step: 5
Training loss: 1.8847999572753906
Validation loss: 2.259177090019308

Epoch: 5| Step: 6
Training loss: 1.9589086771011353
Validation loss: 2.2451225634544127

Epoch: 5| Step: 7
Training loss: 2.608588695526123
Validation loss: 2.252407414938814

Epoch: 5| Step: 8
Training loss: 2.7220919132232666
Validation loss: 2.263245808180942

Epoch: 5| Step: 9
Training loss: 2.6824450492858887
Validation loss: 2.2492034178908153

Epoch: 5| Step: 10
Training loss: 2.3438334465026855
Validation loss: 2.244623094476679

Epoch: 378| Step: 0
Training loss: 2.438336133956909
Validation loss: 2.2078841552939465

Epoch: 5| Step: 1
Training loss: 1.9058929681777954
Validation loss: 2.2094789717787053

Epoch: 5| Step: 2
Training loss: 2.206183671951294
Validation loss: 2.1890304396229405

Epoch: 5| Step: 3
Training loss: 3.235988140106201
Validation loss: 2.192917634082097

Epoch: 5| Step: 4
Training loss: 2.3099780082702637
Validation loss: 2.1884047215984714

Epoch: 5| Step: 5
Training loss: 1.9445594549179077
Validation loss: 2.1850207210868917

Epoch: 5| Step: 6
Training loss: 1.4477696418762207
Validation loss: 2.195505016593523

Epoch: 5| Step: 7
Training loss: 2.907653331756592
Validation loss: 2.1803620887059036

Epoch: 5| Step: 8
Training loss: 2.065941333770752
Validation loss: 2.1934143497097875

Epoch: 5| Step: 9
Training loss: 2.6202330589294434
Validation loss: 2.192934436182822

Epoch: 5| Step: 10
Training loss: 2.6920008659362793
Validation loss: 2.2005895991479196

Epoch: 379| Step: 0
Training loss: 2.2232213020324707
Validation loss: 2.1984529649057696

Epoch: 5| Step: 1
Training loss: 2.2025234699249268
Validation loss: 2.210668476678992

Epoch: 5| Step: 2
Training loss: 2.0837600231170654
Validation loss: 2.218833900267078

Epoch: 5| Step: 3
Training loss: 2.134835720062256
Validation loss: 2.2150186877096854

Epoch: 5| Step: 4
Training loss: 2.821377992630005
Validation loss: 2.216722862694853

Epoch: 5| Step: 5
Training loss: 2.3824777603149414
Validation loss: 2.213593795735349

Epoch: 5| Step: 6
Training loss: 2.541299343109131
Validation loss: 2.2123387731531614

Epoch: 5| Step: 7
Training loss: 1.9822384119033813
Validation loss: 2.199532695995864

Epoch: 5| Step: 8
Training loss: 2.839433193206787
Validation loss: 2.2027130742226877

Epoch: 5| Step: 9
Training loss: 2.276613712310791
Validation loss: 2.2054951985677085

Epoch: 5| Step: 10
Training loss: 2.2539618015289307
Validation loss: 2.1937032745730494

Epoch: 380| Step: 0
Training loss: 2.07731294631958
Validation loss: 2.199304216651506

Epoch: 5| Step: 1
Training loss: 1.9065545797348022
Validation loss: 2.2114547683346655

Epoch: 5| Step: 2
Training loss: 1.6257539987564087
Validation loss: 2.2077808149399294

Epoch: 5| Step: 3
Training loss: 2.949629545211792
Validation loss: 2.229944593162947

Epoch: 5| Step: 4
Training loss: 2.218933582305908
Validation loss: 2.2248303377500145

Epoch: 5| Step: 5
Training loss: 3.019249439239502
Validation loss: 2.2352740072434947

Epoch: 5| Step: 6
Training loss: 2.040947437286377
Validation loss: 2.2313394700327227

Epoch: 5| Step: 7
Training loss: 2.386232614517212
Validation loss: 2.2345514400030977

Epoch: 5| Step: 8
Training loss: 2.041790723800659
Validation loss: 2.2354844949578725

Epoch: 5| Step: 9
Training loss: 3.062089204788208
Validation loss: 2.2140349726523123

Epoch: 5| Step: 10
Training loss: 2.3397216796875
Validation loss: 2.210630762961603

Epoch: 381| Step: 0
Training loss: 2.6643757820129395
Validation loss: 2.209790232361004

Epoch: 5| Step: 1
Training loss: 2.1558997631073
Validation loss: 2.2258689839352845

Epoch: 5| Step: 2
Training loss: 2.255532741546631
Validation loss: 2.193145836553266

Epoch: 5| Step: 3
Training loss: 2.190577268600464
Validation loss: 2.2019911632742932

Epoch: 5| Step: 4
Training loss: 1.8584991693496704
Validation loss: 2.1955478678467455

Epoch: 5| Step: 5
Training loss: 2.19144868850708
Validation loss: 2.2079047208191245

Epoch: 5| Step: 6
Training loss: 2.1188113689422607
Validation loss: 2.204953332101145

Epoch: 5| Step: 7
Training loss: 2.6595466136932373
Validation loss: 2.196148221210767

Epoch: 5| Step: 8
Training loss: 2.2300238609313965
Validation loss: 2.2040052080667145

Epoch: 5| Step: 9
Training loss: 2.7094783782958984
Validation loss: 2.201931186901626

Epoch: 5| Step: 10
Training loss: 2.634516477584839
Validation loss: 2.1996230476646015

Epoch: 382| Step: 0
Training loss: 2.310211420059204
Validation loss: 2.196902544267716

Epoch: 5| Step: 1
Training loss: 3.0201003551483154
Validation loss: 2.2105389512995237

Epoch: 5| Step: 2
Training loss: 2.038449287414551
Validation loss: 2.2132584279583347

Epoch: 5| Step: 3
Training loss: 2.5502095222473145
Validation loss: 2.2148757211623655

Epoch: 5| Step: 4
Training loss: 2.5937812328338623
Validation loss: 2.234737503913141

Epoch: 5| Step: 5
Training loss: 2.035432815551758
Validation loss: 2.206745001577562

Epoch: 5| Step: 6
Training loss: 1.989061951637268
Validation loss: 2.200352063743017

Epoch: 5| Step: 7
Training loss: 1.9699461460113525
Validation loss: 2.196204857159686

Epoch: 5| Step: 8
Training loss: 2.641679286956787
Validation loss: 2.194820434816422

Epoch: 5| Step: 9
Training loss: 2.444650650024414
Validation loss: 2.1854886701030116

Epoch: 5| Step: 10
Training loss: 1.9975666999816895
Validation loss: 2.1967240495066487

Epoch: 383| Step: 0
Training loss: 1.9340221881866455
Validation loss: 2.174385932184035

Epoch: 5| Step: 1
Training loss: 2.5804333686828613
Validation loss: 2.202820900947817

Epoch: 5| Step: 2
Training loss: 2.335495710372925
Validation loss: 2.2088555366762224

Epoch: 5| Step: 3
Training loss: 2.733497142791748
Validation loss: 2.211849743320096

Epoch: 5| Step: 4
Training loss: 2.5073695182800293
Validation loss: 2.223522336252274

Epoch: 5| Step: 5
Training loss: 1.8958766460418701
Validation loss: 2.223695247404037

Epoch: 5| Step: 6
Training loss: 2.313683032989502
Validation loss: 2.232625663921397

Epoch: 5| Step: 7
Training loss: 2.556511402130127
Validation loss: 2.2279256595078336

Epoch: 5| Step: 8
Training loss: 2.1441102027893066
Validation loss: 2.2342840061392835

Epoch: 5| Step: 9
Training loss: 2.4744155406951904
Validation loss: 2.25522191550142

Epoch: 5| Step: 10
Training loss: 2.111988067626953
Validation loss: 2.251810822435605

Epoch: 384| Step: 0
Training loss: 1.8887859582901
Validation loss: 2.2317696438040784

Epoch: 5| Step: 1
Training loss: 2.4243667125701904
Validation loss: 2.244083096904139

Epoch: 5| Step: 2
Training loss: 3.296414852142334
Validation loss: 2.218144163008659

Epoch: 5| Step: 3
Training loss: 2.082045078277588
Validation loss: 2.217236776505747

Epoch: 5| Step: 4
Training loss: 2.4561214447021484
Validation loss: 2.205868160852822

Epoch: 5| Step: 5
Training loss: 2.464757204055786
Validation loss: 2.1913370727210917

Epoch: 5| Step: 6
Training loss: 1.9956800937652588
Validation loss: 2.189593720179732

Epoch: 5| Step: 7
Training loss: 2.3784682750701904
Validation loss: 2.1758383025405226

Epoch: 5| Step: 8
Training loss: 2.1328227519989014
Validation loss: 2.1847240899198797

Epoch: 5| Step: 9
Training loss: 2.3279242515563965
Validation loss: 2.1792843752009894

Epoch: 5| Step: 10
Training loss: 2.1502649784088135
Validation loss: 2.1709967172274025

Epoch: 385| Step: 0
Training loss: 2.3812568187713623
Validation loss: 2.187537912399538

Epoch: 5| Step: 1
Training loss: 2.7509872913360596
Validation loss: 2.1973772202768633

Epoch: 5| Step: 2
Training loss: 2.309738874435425
Validation loss: 2.228069366947297

Epoch: 5| Step: 3
Training loss: 2.9339284896850586
Validation loss: 2.2052112548582015

Epoch: 5| Step: 4
Training loss: 2.183459520339966
Validation loss: 2.218500991021433

Epoch: 5| Step: 5
Training loss: 1.8436591625213623
Validation loss: 2.2290544548342304

Epoch: 5| Step: 6
Training loss: 2.269059658050537
Validation loss: 2.213518763101229

Epoch: 5| Step: 7
Training loss: 2.547292709350586
Validation loss: 2.21634312598936

Epoch: 5| Step: 8
Training loss: 2.601375102996826
Validation loss: 2.226465473892868

Epoch: 5| Step: 9
Training loss: 1.7905470132827759
Validation loss: 2.219715263253899

Epoch: 5| Step: 10
Training loss: 1.8914861679077148
Validation loss: 2.212710876618662

Epoch: 386| Step: 0
Training loss: 2.7419440746307373
Validation loss: 2.1995861966122865

Epoch: 5| Step: 1
Training loss: 2.0980734825134277
Validation loss: 2.196830290620045

Epoch: 5| Step: 2
Training loss: 1.808394193649292
Validation loss: 2.188005813988306

Epoch: 5| Step: 3
Training loss: 2.2196197509765625
Validation loss: 2.18871965331416

Epoch: 5| Step: 4
Training loss: 2.517411708831787
Validation loss: 2.170678807843116

Epoch: 5| Step: 5
Training loss: 2.456634044647217
Validation loss: 2.1800321481561147

Epoch: 5| Step: 6
Training loss: 2.0666520595550537
Validation loss: 2.1740965330472557

Epoch: 5| Step: 7
Training loss: 2.629014253616333
Validation loss: 2.177275570490027

Epoch: 5| Step: 8
Training loss: 2.0841729640960693
Validation loss: 2.1710084433196695

Epoch: 5| Step: 9
Training loss: 3.0856385231018066
Validation loss: 2.1830568762235742

Epoch: 5| Step: 10
Training loss: 1.742079734802246
Validation loss: 2.20509684214028

Epoch: 387| Step: 0
Training loss: 2.127044200897217
Validation loss: 2.206219252719674

Epoch: 5| Step: 1
Training loss: 2.9628491401672363
Validation loss: 2.2108073157648884

Epoch: 5| Step: 2
Training loss: 2.037381649017334
Validation loss: 2.2243923884566112

Epoch: 5| Step: 3
Training loss: 2.8507633209228516
Validation loss: 2.224280095869495

Epoch: 5| Step: 4
Training loss: 2.5032036304473877
Validation loss: 2.211918395052674

Epoch: 5| Step: 5
Training loss: 2.5192067623138428
Validation loss: 2.1902813808892363

Epoch: 5| Step: 6
Training loss: 1.6265853643417358
Validation loss: 2.1909054684382614

Epoch: 5| Step: 7
Training loss: 2.871657609939575
Validation loss: 2.1793233245931645

Epoch: 5| Step: 8
Training loss: 1.908308744430542
Validation loss: 2.17719970210906

Epoch: 5| Step: 9
Training loss: 2.4175405502319336
Validation loss: 2.1759336148538897

Epoch: 5| Step: 10
Training loss: 1.720666527748108
Validation loss: 2.1899748489420903

Epoch: 388| Step: 0
Training loss: 3.305936336517334
Validation loss: 2.1900721109041603

Epoch: 5| Step: 1
Training loss: 2.3891379833221436
Validation loss: 2.2097165302563737

Epoch: 5| Step: 2
Training loss: 2.375441312789917
Validation loss: 2.225095974501743

Epoch: 5| Step: 3
Training loss: 2.286911725997925
Validation loss: 2.2306788403500795

Epoch: 5| Step: 4
Training loss: 1.8613420724868774
Validation loss: 2.2329006348886797

Epoch: 5| Step: 5
Training loss: 2.6094417572021484
Validation loss: 2.225426335488596

Epoch: 5| Step: 6
Training loss: 2.449673891067505
Validation loss: 2.2244736584283973

Epoch: 5| Step: 7
Training loss: 2.249040126800537
Validation loss: 2.2390817570429977

Epoch: 5| Step: 8
Training loss: 2.1762726306915283
Validation loss: 2.236186078799668

Epoch: 5| Step: 9
Training loss: 2.240253210067749
Validation loss: 2.243228199661419

Epoch: 5| Step: 10
Training loss: 1.4824635982513428
Validation loss: 2.236503642092469

Epoch: 389| Step: 0
Training loss: 2.587378978729248
Validation loss: 2.212177627830095

Epoch: 5| Step: 1
Training loss: 1.4216487407684326
Validation loss: 2.2099631165945404

Epoch: 5| Step: 2
Training loss: 1.9353700876235962
Validation loss: 2.1939160362366708

Epoch: 5| Step: 3
Training loss: 1.802098274230957
Validation loss: 2.1903469459984892

Epoch: 5| Step: 4
Training loss: 2.5898897647857666
Validation loss: 2.1863924175180416

Epoch: 5| Step: 5
Training loss: 2.8007774353027344
Validation loss: 2.1939295902047107

Epoch: 5| Step: 6
Training loss: 1.8237473964691162
Validation loss: 2.1865605551709413

Epoch: 5| Step: 7
Training loss: 2.5932040214538574
Validation loss: 2.204865704300583

Epoch: 5| Step: 8
Training loss: 2.6004815101623535
Validation loss: 2.2060298406949608

Epoch: 5| Step: 9
Training loss: 2.527426242828369
Validation loss: 2.20586569334871

Epoch: 5| Step: 10
Training loss: 2.989640235900879
Validation loss: 2.2377874646135556

Epoch: 390| Step: 0
Training loss: 1.992845892906189
Validation loss: 2.2326984149153515

Epoch: 5| Step: 1
Training loss: 2.488093852996826
Validation loss: 2.2272591385790097

Epoch: 5| Step: 2
Training loss: 2.332118034362793
Validation loss: 2.2059750685127835

Epoch: 5| Step: 3
Training loss: 2.3215770721435547
Validation loss: 2.1917422868872203

Epoch: 5| Step: 4
Training loss: 2.6070351600646973
Validation loss: 2.1869137492231143

Epoch: 5| Step: 5
Training loss: 3.004004716873169
Validation loss: 2.1817600188716764

Epoch: 5| Step: 6
Training loss: 1.4227168560028076
Validation loss: 2.176792649812596

Epoch: 5| Step: 7
Training loss: 2.3332290649414062
Validation loss: 2.176965759646508

Epoch: 5| Step: 8
Training loss: 2.0594286918640137
Validation loss: 2.173628509685557

Epoch: 5| Step: 9
Training loss: 2.6740100383758545
Validation loss: 2.1717397987201648

Epoch: 5| Step: 10
Training loss: 2.275963068008423
Validation loss: 2.183487635786815

Epoch: 391| Step: 0
Training loss: 3.040146589279175
Validation loss: 2.1917352548209568

Epoch: 5| Step: 1
Training loss: 2.6417369842529297
Validation loss: 2.2014197867403746

Epoch: 5| Step: 2
Training loss: 2.08379864692688
Validation loss: 2.203976881119513

Epoch: 5| Step: 3
Training loss: 2.664074420928955
Validation loss: 2.200018209795798

Epoch: 5| Step: 4
Training loss: 2.4158666133880615
Validation loss: 2.1986724317714734

Epoch: 5| Step: 5
Training loss: 1.8352787494659424
Validation loss: 2.1980424234944005

Epoch: 5| Step: 6
Training loss: 2.0762200355529785
Validation loss: 2.2103511569320515

Epoch: 5| Step: 7
Training loss: 2.156877040863037
Validation loss: 2.2087801400051323

Epoch: 5| Step: 8
Training loss: 2.4978833198547363
Validation loss: 2.2149699452102825

Epoch: 5| Step: 9
Training loss: 1.805625319480896
Validation loss: 2.2183688968740483

Epoch: 5| Step: 10
Training loss: 2.216486692428589
Validation loss: 2.2091732794238674

Epoch: 392| Step: 0
Training loss: 2.3963100910186768
Validation loss: 2.2297150268349597

Epoch: 5| Step: 1
Training loss: 2.149085283279419
Validation loss: 2.231349622049639

Epoch: 5| Step: 2
Training loss: 2.5173637866973877
Validation loss: 2.227331312753821

Epoch: 5| Step: 3
Training loss: 2.409358263015747
Validation loss: 2.2194934968025453

Epoch: 5| Step: 4
Training loss: 2.676401138305664
Validation loss: 2.2255566273966143

Epoch: 5| Step: 5
Training loss: 2.4442784786224365
Validation loss: 2.2240573744620047

Epoch: 5| Step: 6
Training loss: 2.493762254714966
Validation loss: 2.215113760322653

Epoch: 5| Step: 7
Training loss: 2.54681396484375
Validation loss: 2.2071077900548137

Epoch: 5| Step: 8
Training loss: 1.8651901483535767
Validation loss: 2.204214003778273

Epoch: 5| Step: 9
Training loss: 2.182612657546997
Validation loss: 2.1952864072656118

Epoch: 5| Step: 10
Training loss: 1.6006512641906738
Validation loss: 2.1981064350374284

Epoch: 393| Step: 0
Training loss: 2.8083395957946777
Validation loss: 2.2007675529808126

Epoch: 5| Step: 1
Training loss: 1.7554489374160767
Validation loss: 2.1969457057214554

Epoch: 5| Step: 2
Training loss: 2.740779399871826
Validation loss: 2.192232362685665

Epoch: 5| Step: 3
Training loss: 1.9812183380126953
Validation loss: 2.196751735543692

Epoch: 5| Step: 4
Training loss: 2.127274751663208
Validation loss: 2.202939374472505

Epoch: 5| Step: 5
Training loss: 2.1073944568634033
Validation loss: 2.1926411390304565

Epoch: 5| Step: 6
Training loss: 1.9387428760528564
Validation loss: 2.1959541125964095

Epoch: 5| Step: 7
Training loss: 2.5058255195617676
Validation loss: 2.210245725929096

Epoch: 5| Step: 8
Training loss: 2.921506881713867
Validation loss: 2.2037620787979453

Epoch: 5| Step: 9
Training loss: 2.0537452697753906
Validation loss: 2.2200345326495428

Epoch: 5| Step: 10
Training loss: 2.4210383892059326
Validation loss: 2.200987826111496

Epoch: 394| Step: 0
Training loss: 2.1438944339752197
Validation loss: 2.205364093985609

Epoch: 5| Step: 1
Training loss: 2.89367938041687
Validation loss: 2.21873660754132

Epoch: 5| Step: 2
Training loss: 1.8881094455718994
Validation loss: 2.220923034093713

Epoch: 5| Step: 3
Training loss: 2.4308223724365234
Validation loss: 2.2184893597838697

Epoch: 5| Step: 4
Training loss: 2.0642454624176025
Validation loss: 2.209641500185895

Epoch: 5| Step: 5
Training loss: 2.3650944232940674
Validation loss: 2.2085235272684405

Epoch: 5| Step: 6
Training loss: 2.276567220687866
Validation loss: 2.195656984083114

Epoch: 5| Step: 7
Training loss: 2.377490520477295
Validation loss: 2.197235143312844

Epoch: 5| Step: 8
Training loss: 2.2831637859344482
Validation loss: 2.1894894440968833

Epoch: 5| Step: 9
Training loss: 2.2061774730682373
Validation loss: 2.1823545245714087

Epoch: 5| Step: 10
Training loss: 2.391911029815674
Validation loss: 2.181462767303631

Epoch: 395| Step: 0
Training loss: 2.2525713443756104
Validation loss: 2.182365227771062

Epoch: 5| Step: 1
Training loss: 3.0577220916748047
Validation loss: 2.1844954708571076

Epoch: 5| Step: 2
Training loss: 2.306851863861084
Validation loss: 2.181852117661507

Epoch: 5| Step: 3
Training loss: 2.568751096725464
Validation loss: 2.1917109386895293

Epoch: 5| Step: 4
Training loss: 2.5620908737182617
Validation loss: 2.1890713527638423

Epoch: 5| Step: 5
Training loss: 2.4980921745300293
Validation loss: 2.1784569524949595

Epoch: 5| Step: 6
Training loss: 2.060927629470825
Validation loss: 2.1644896102207962

Epoch: 5| Step: 7
Training loss: 1.8028045892715454
Validation loss: 2.173795048908521

Epoch: 5| Step: 8
Training loss: 1.8867599964141846
Validation loss: 2.165918124619351

Epoch: 5| Step: 9
Training loss: 2.410606622695923
Validation loss: 2.170080672028244

Epoch: 5| Step: 10
Training loss: 1.940893530845642
Validation loss: 2.184743135206161

Epoch: 396| Step: 0
Training loss: 1.6501086950302124
Validation loss: 2.217480746648645

Epoch: 5| Step: 1
Training loss: 2.8532474040985107
Validation loss: 2.22476860015623

Epoch: 5| Step: 2
Training loss: 2.226588487625122
Validation loss: 2.241796188457038

Epoch: 5| Step: 3
Training loss: 2.790804386138916
Validation loss: 2.241009704528316

Epoch: 5| Step: 4
Training loss: 2.6512997150421143
Validation loss: 2.249501261659848

Epoch: 5| Step: 5
Training loss: 2.606560230255127
Validation loss: 2.2282286446581603

Epoch: 5| Step: 6
Training loss: 2.334689140319824
Validation loss: 2.2209199910522788

Epoch: 5| Step: 7
Training loss: 2.511277198791504
Validation loss: 2.2164227731766237

Epoch: 5| Step: 8
Training loss: 2.1325972080230713
Validation loss: 2.2099261488965762

Epoch: 5| Step: 9
Training loss: 1.755679726600647
Validation loss: 2.2032051752972346

Epoch: 5| Step: 10
Training loss: 1.7594146728515625
Validation loss: 2.1986501934707805

Epoch: 397| Step: 0
Training loss: 2.0463128089904785
Validation loss: 2.19512842547509

Epoch: 5| Step: 1
Training loss: 2.582756519317627
Validation loss: 2.1920099591696136

Epoch: 5| Step: 2
Training loss: 2.6999809741973877
Validation loss: 2.19368831060266

Epoch: 5| Step: 3
Training loss: 1.9731452465057373
Validation loss: 2.1845317604721233

Epoch: 5| Step: 4
Training loss: 2.583858013153076
Validation loss: 2.180322600949195

Epoch: 5| Step: 5
Training loss: 1.9344818592071533
Validation loss: 2.1696435687362507

Epoch: 5| Step: 6
Training loss: 1.9090967178344727
Validation loss: 2.1850909930403515

Epoch: 5| Step: 7
Training loss: 2.293114423751831
Validation loss: 2.1830960191706175

Epoch: 5| Step: 8
Training loss: 1.89922297000885
Validation loss: 2.1988362137989332

Epoch: 5| Step: 9
Training loss: 3.109450578689575
Validation loss: 2.219617574445663

Epoch: 5| Step: 10
Training loss: 2.3780689239501953
Validation loss: 2.218229975751651

Epoch: 398| Step: 0
Training loss: 2.708228826522827
Validation loss: 2.2289118792421077

Epoch: 5| Step: 1
Training loss: 2.558598041534424
Validation loss: 2.220920701180735

Epoch: 5| Step: 2
Training loss: 3.082209825515747
Validation loss: 2.2049360352177776

Epoch: 5| Step: 3
Training loss: 2.4155495166778564
Validation loss: 2.193791620192989

Epoch: 5| Step: 4
Training loss: 2.0265185832977295
Validation loss: 2.18968532418692

Epoch: 5| Step: 5
Training loss: 2.3496718406677246
Validation loss: 2.1827020081140662

Epoch: 5| Step: 6
Training loss: 1.6805181503295898
Validation loss: 2.1835450856916365

Epoch: 5| Step: 7
Training loss: 1.9188458919525146
Validation loss: 2.1794057802487443

Epoch: 5| Step: 8
Training loss: 1.9279091358184814
Validation loss: 2.1800587049094577

Epoch: 5| Step: 9
Training loss: 2.0271546840667725
Validation loss: 2.1763728126402824

Epoch: 5| Step: 10
Training loss: 2.7030704021453857
Validation loss: 2.212736277170079

Epoch: 399| Step: 0
Training loss: 2.2996726036071777
Validation loss: 2.184797538224087

Epoch: 5| Step: 1
Training loss: 1.7843797206878662
Validation loss: 2.214047360163863

Epoch: 5| Step: 2
Training loss: 3.162273645401001
Validation loss: 2.2229362021210375

Epoch: 5| Step: 3
Training loss: 2.0245308876037598
Validation loss: 2.2068772290342595

Epoch: 5| Step: 4
Training loss: 2.140930414199829
Validation loss: 2.1967497410312777

Epoch: 5| Step: 5
Training loss: 2.051640272140503
Validation loss: 2.1860760437544955

Epoch: 5| Step: 6
Training loss: 2.732905149459839
Validation loss: 2.1800805996823054

Epoch: 5| Step: 7
Training loss: 2.8947548866271973
Validation loss: 2.1801485220591226

Epoch: 5| Step: 8
Training loss: 1.5790343284606934
Validation loss: 2.1869789746499833

Epoch: 5| Step: 9
Training loss: 2.1688015460968018
Validation loss: 2.180421990732993

Epoch: 5| Step: 10
Training loss: 2.53786039352417
Validation loss: 2.1922007670966526

Epoch: 400| Step: 0
Training loss: 2.3041093349456787
Validation loss: 2.188531098827239

Epoch: 5| Step: 1
Training loss: 2.4337778091430664
Validation loss: 2.219840967527

Epoch: 5| Step: 2
Training loss: 3.051093578338623
Validation loss: 2.2180060032875306

Epoch: 5| Step: 3
Training loss: 2.176499843597412
Validation loss: 2.2342455105115007

Epoch: 5| Step: 4
Training loss: 2.220456600189209
Validation loss: 2.2158941581685054

Epoch: 5| Step: 5
Training loss: 2.7951598167419434
Validation loss: 2.2209542874367005

Epoch: 5| Step: 6
Training loss: 2.1345489025115967
Validation loss: 2.2157804184062506

Epoch: 5| Step: 7
Training loss: 2.4570984840393066
Validation loss: 2.2167153845551195

Epoch: 5| Step: 8
Training loss: 1.5594967603683472
Validation loss: 2.213687842892062

Epoch: 5| Step: 9
Training loss: 1.6813091039657593
Validation loss: 2.2047866723870717

Epoch: 5| Step: 10
Training loss: 2.3613553047180176
Validation loss: 2.193808965785529

Epoch: 401| Step: 0
Training loss: 2.1971864700317383
Validation loss: 2.18474365690703

Epoch: 5| Step: 1
Training loss: 2.259622097015381
Validation loss: 2.1786391376167216

Epoch: 5| Step: 2
Training loss: 2.3494088649749756
Validation loss: 2.1918680334603913

Epoch: 5| Step: 3
Training loss: 2.8952317237854004
Validation loss: 2.1769251823425293

Epoch: 5| Step: 4
Training loss: 2.046764373779297
Validation loss: 2.180863936742147

Epoch: 5| Step: 5
Training loss: 2.2366843223571777
Validation loss: 2.1790205150522213

Epoch: 5| Step: 6
Training loss: 2.2393581867218018
Validation loss: 2.1948375419903825

Epoch: 5| Step: 7
Training loss: 2.4044189453125
Validation loss: 2.184707289100975

Epoch: 5| Step: 8
Training loss: 2.3379485607147217
Validation loss: 2.183083406058691

Epoch: 5| Step: 9
Training loss: 2.054771900177002
Validation loss: 2.1693295227584017

Epoch: 5| Step: 10
Training loss: 2.1956515312194824
Validation loss: 2.1801142731020526

Epoch: 402| Step: 0
Training loss: 2.478111743927002
Validation loss: 2.1726133656758133

Epoch: 5| Step: 1
Training loss: 1.6387417316436768
Validation loss: 2.1582599788583736

Epoch: 5| Step: 2
Training loss: 1.9934260845184326
Validation loss: 2.159253099913238

Epoch: 5| Step: 3
Training loss: 2.8703670501708984
Validation loss: 2.169379166377488

Epoch: 5| Step: 4
Training loss: 2.2510218620300293
Validation loss: 2.1568724955281904

Epoch: 5| Step: 5
Training loss: 2.1626195907592773
Validation loss: 2.1638476848602295

Epoch: 5| Step: 6
Training loss: 2.6459031105041504
Validation loss: 2.1740219029047156

Epoch: 5| Step: 7
Training loss: 1.7563819885253906
Validation loss: 2.1772337139293714

Epoch: 5| Step: 8
Training loss: 2.530496120452881
Validation loss: 2.1924866220002532

Epoch: 5| Step: 9
Training loss: 2.236318588256836
Validation loss: 2.189727867803266

Epoch: 5| Step: 10
Training loss: 2.724745035171509
Validation loss: 2.1986721138800345

Epoch: 403| Step: 0
Training loss: 2.7114741802215576
Validation loss: 2.2021128259679323

Epoch: 5| Step: 1
Training loss: 2.0113699436187744
Validation loss: 2.2055921452019804

Epoch: 5| Step: 2
Training loss: 2.4600112438201904
Validation loss: 2.2173377672831216

Epoch: 5| Step: 3
Training loss: 2.036064624786377
Validation loss: 2.202499661394345

Epoch: 5| Step: 4
Training loss: 1.9137691259384155
Validation loss: 2.2081034619321107

Epoch: 5| Step: 5
Training loss: 3.046552896499634
Validation loss: 2.2203166484832764

Epoch: 5| Step: 6
Training loss: 1.9696696996688843
Validation loss: 2.234893404027467

Epoch: 5| Step: 7
Training loss: 1.9974524974822998
Validation loss: 2.237329488159508

Epoch: 5| Step: 8
Training loss: 2.3113749027252197
Validation loss: 2.222417159747052

Epoch: 5| Step: 9
Training loss: 3.139883518218994
Validation loss: 2.207098150766024

Epoch: 5| Step: 10
Training loss: 1.4536466598510742
Validation loss: 2.1934991857056976

Epoch: 404| Step: 0
Training loss: 2.4074196815490723
Validation loss: 2.1832926145163913

Epoch: 5| Step: 1
Training loss: 2.0156948566436768
Validation loss: 2.171468916759696

Epoch: 5| Step: 2
Training loss: 2.484224796295166
Validation loss: 2.171096263393279

Epoch: 5| Step: 3
Training loss: 2.1527247428894043
Validation loss: 2.176074899652953

Epoch: 5| Step: 4
Training loss: 1.8979320526123047
Validation loss: 2.1796463381859565

Epoch: 5| Step: 5
Training loss: 2.0363457202911377
Validation loss: 2.1773189395986576

Epoch: 5| Step: 6
Training loss: 2.174685001373291
Validation loss: 2.1766585662800777

Epoch: 5| Step: 7
Training loss: 2.3752331733703613
Validation loss: 2.180761880772088

Epoch: 5| Step: 8
Training loss: 2.7252163887023926
Validation loss: 2.185877764096824

Epoch: 5| Step: 9
Training loss: 2.642298936843872
Validation loss: 2.177952922800536

Epoch: 5| Step: 10
Training loss: 2.404395818710327
Validation loss: 2.1758988634232552

Epoch: 405| Step: 0
Training loss: 2.109060525894165
Validation loss: 2.2020795652943272

Epoch: 5| Step: 1
Training loss: 2.7706656455993652
Validation loss: 2.217601014721778

Epoch: 5| Step: 2
Training loss: 2.4441654682159424
Validation loss: 2.198721114025321

Epoch: 5| Step: 3
Training loss: 2.567802667617798
Validation loss: 2.1968711499244935

Epoch: 5| Step: 4
Training loss: 2.1140828132629395
Validation loss: 2.183271578563157

Epoch: 5| Step: 5
Training loss: 2.076859474182129
Validation loss: 2.1853758365877214

Epoch: 5| Step: 6
Training loss: 1.7718238830566406
Validation loss: 2.1690594329628894

Epoch: 5| Step: 7
Training loss: 2.4278385639190674
Validation loss: 2.1731115271968227

Epoch: 5| Step: 8
Training loss: 2.2096312046051025
Validation loss: 2.1988993203768166

Epoch: 5| Step: 9
Training loss: 2.1845250129699707
Validation loss: 2.1812926107837307

Epoch: 5| Step: 10
Training loss: 2.500486135482788
Validation loss: 2.179423265559699

Epoch: 406| Step: 0
Training loss: 1.678023338317871
Validation loss: 2.197831997307398

Epoch: 5| Step: 1
Training loss: 2.525824785232544
Validation loss: 2.1903641710999193

Epoch: 5| Step: 2
Training loss: 1.3301739692687988
Validation loss: 2.201328530106493

Epoch: 5| Step: 3
Training loss: 2.907958507537842
Validation loss: 2.2100392362122894

Epoch: 5| Step: 4
Training loss: 2.2879226207733154
Validation loss: 2.2154388017551874

Epoch: 5| Step: 5
Training loss: 2.68709397315979
Validation loss: 2.2122040076922347

Epoch: 5| Step: 6
Training loss: 2.688605308532715
Validation loss: 2.214574134478005

Epoch: 5| Step: 7
Training loss: 2.2391316890716553
Validation loss: 2.2092942166072067

Epoch: 5| Step: 8
Training loss: 2.4591166973114014
Validation loss: 2.1849781928523893

Epoch: 5| Step: 9
Training loss: 2.353877544403076
Validation loss: 2.19067322310581

Epoch: 5| Step: 10
Training loss: 1.898766279220581
Validation loss: 2.182958731087305

Epoch: 407| Step: 0
Training loss: 1.9824930429458618
Validation loss: 2.1877254081028763

Epoch: 5| Step: 1
Training loss: 2.5112318992614746
Validation loss: 2.1688017947699434

Epoch: 5| Step: 2
Training loss: 2.541639804840088
Validation loss: 2.174186339942358

Epoch: 5| Step: 3
Training loss: 2.065934658050537
Validation loss: 2.174112814728932

Epoch: 5| Step: 4
Training loss: 2.5290346145629883
Validation loss: 2.1785034928270566

Epoch: 5| Step: 5
Training loss: 2.546668291091919
Validation loss: 2.187294388330111

Epoch: 5| Step: 6
Training loss: 2.5692410469055176
Validation loss: 2.182335948431364

Epoch: 5| Step: 7
Training loss: 1.7077796459197998
Validation loss: 2.18325150141152

Epoch: 5| Step: 8
Training loss: 2.2346951961517334
Validation loss: 2.1875280436649116

Epoch: 5| Step: 9
Training loss: 2.697214126586914
Validation loss: 2.179352852606004

Epoch: 5| Step: 10
Training loss: 1.5110055208206177
Validation loss: 2.1623424894066265

Epoch: 408| Step: 0
Training loss: 1.9016292095184326
Validation loss: 2.1623887092836442

Epoch: 5| Step: 1
Training loss: 2.317514419555664
Validation loss: 2.1816255802749307

Epoch: 5| Step: 2
Training loss: 2.346926212310791
Validation loss: 2.1787205896069928

Epoch: 5| Step: 3
Training loss: 2.6470117568969727
Validation loss: 2.1821500562852427

Epoch: 5| Step: 4
Training loss: 2.672858715057373
Validation loss: 2.176586074213828

Epoch: 5| Step: 5
Training loss: 1.7242257595062256
Validation loss: 2.179680649952222

Epoch: 5| Step: 6
Training loss: 2.4338791370391846
Validation loss: 2.1906229680584324

Epoch: 5| Step: 7
Training loss: 1.6805931329727173
Validation loss: 2.1958981085849065

Epoch: 5| Step: 8
Training loss: 2.7938218116760254
Validation loss: 2.186215957005819

Epoch: 5| Step: 9
Training loss: 2.5028793811798096
Validation loss: 2.176457756309099

Epoch: 5| Step: 10
Training loss: 2.009328842163086
Validation loss: 2.1844983216254943

Epoch: 409| Step: 0
Training loss: 1.9996265172958374
Validation loss: 2.173737727185731

Epoch: 5| Step: 1
Training loss: 2.7477974891662598
Validation loss: 2.1769306377698014

Epoch: 5| Step: 2
Training loss: 1.7962934970855713
Validation loss: 2.1716082737010014

Epoch: 5| Step: 3
Training loss: 2.597529649734497
Validation loss: 2.1655417232103247

Epoch: 5| Step: 4
Training loss: 2.491346597671509
Validation loss: 2.1726996719196277

Epoch: 5| Step: 5
Training loss: 2.1949710845947266
Validation loss: 2.184183333509712

Epoch: 5| Step: 6
Training loss: 2.809736728668213
Validation loss: 2.1874448483990085

Epoch: 5| Step: 7
Training loss: 2.3135030269622803
Validation loss: 2.1762948702740412

Epoch: 5| Step: 8
Training loss: 1.8920704126358032
Validation loss: 2.1736557842582784

Epoch: 5| Step: 9
Training loss: 1.8986717462539673
Validation loss: 2.1659624012567664

Epoch: 5| Step: 10
Training loss: 2.2901265621185303
Validation loss: 2.1625698022944952

Epoch: 410| Step: 0
Training loss: 2.1622891426086426
Validation loss: 2.169885766121649

Epoch: 5| Step: 1
Training loss: 1.6972932815551758
Validation loss: 2.191155819482701

Epoch: 5| Step: 2
Training loss: 2.680677890777588
Validation loss: 2.1710711576605357

Epoch: 5| Step: 3
Training loss: 2.3603339195251465
Validation loss: 2.1805058910000708

Epoch: 5| Step: 4
Training loss: 2.787532329559326
Validation loss: 2.1763842285320325

Epoch: 5| Step: 5
Training loss: 2.3547122478485107
Validation loss: 2.1743370409934752

Epoch: 5| Step: 6
Training loss: 2.7680928707122803
Validation loss: 2.1773611140507523

Epoch: 5| Step: 7
Training loss: 2.1190736293792725
Validation loss: 2.1847956488209386

Epoch: 5| Step: 8
Training loss: 2.4444420337677
Validation loss: 2.1764076909711285

Epoch: 5| Step: 9
Training loss: 2.2669529914855957
Validation loss: 2.1802894133393482

Epoch: 5| Step: 10
Training loss: 1.1837979555130005
Validation loss: 2.187895703059371

Epoch: 411| Step: 0
Training loss: 2.605546236038208
Validation loss: 2.186275295031968

Epoch: 5| Step: 1
Training loss: 2.291832685470581
Validation loss: 2.196816663588247

Epoch: 5| Step: 2
Training loss: 2.139739513397217
Validation loss: 2.1811042652335217

Epoch: 5| Step: 3
Training loss: 2.44264554977417
Validation loss: 2.144508328489078

Epoch: 5| Step: 4
Training loss: 1.670747995376587
Validation loss: 2.1662282687361523

Epoch: 5| Step: 5
Training loss: 2.1924431324005127
Validation loss: 2.160372808415403

Epoch: 5| Step: 6
Training loss: 2.2908647060394287
Validation loss: 2.164706548055013

Epoch: 5| Step: 7
Training loss: 2.622622489929199
Validation loss: 2.156813616393715

Epoch: 5| Step: 8
Training loss: 2.032668352127075
Validation loss: 2.163588559755715

Epoch: 5| Step: 9
Training loss: 2.5270302295684814
Validation loss: 2.1640108708412416

Epoch: 5| Step: 10
Training loss: 2.1808555126190186
Validation loss: 2.170099892923909

Epoch: 412| Step: 0
Training loss: 2.543549060821533
Validation loss: 2.182363884423369

Epoch: 5| Step: 1
Training loss: 2.0082802772521973
Validation loss: 2.179144021003477

Epoch: 5| Step: 2
Training loss: 1.611820936203003
Validation loss: 2.1860929919827368

Epoch: 5| Step: 3
Training loss: 2.8769564628601074
Validation loss: 2.178115121779903

Epoch: 5| Step: 4
Training loss: 2.4200828075408936
Validation loss: 2.180536982833698

Epoch: 5| Step: 5
Training loss: 1.9973042011260986
Validation loss: 2.1910371485576836

Epoch: 5| Step: 6
Training loss: 2.800550699234009
Validation loss: 2.169756616315534

Epoch: 5| Step: 7
Training loss: 1.490627646446228
Validation loss: 2.1633448011131695

Epoch: 5| Step: 8
Training loss: 2.5411713123321533
Validation loss: 2.16683179332364

Epoch: 5| Step: 9
Training loss: 2.2322051525115967
Validation loss: 2.1643974729763564

Epoch: 5| Step: 10
Training loss: 2.565239429473877
Validation loss: 2.152456880897604

Epoch: 413| Step: 0
Training loss: 2.2809576988220215
Validation loss: 2.1574154259056173

Epoch: 5| Step: 1
Training loss: 2.4235644340515137
Validation loss: 2.169718611624933

Epoch: 5| Step: 2
Training loss: 1.5721286535263062
Validation loss: 2.1643837126352454

Epoch: 5| Step: 3
Training loss: 1.9999539852142334
Validation loss: 2.1551394975313576

Epoch: 5| Step: 4
Training loss: 2.125349998474121
Validation loss: 2.175106179329657

Epoch: 5| Step: 5
Training loss: 2.308642625808716
Validation loss: 2.174819984743672

Epoch: 5| Step: 6
Training loss: 2.616405963897705
Validation loss: 2.197535717359153

Epoch: 5| Step: 7
Training loss: 2.6049251556396484
Validation loss: 2.2068721363621373

Epoch: 5| Step: 8
Training loss: 2.2014918327331543
Validation loss: 2.1914461171755226

Epoch: 5| Step: 9
Training loss: 2.048123598098755
Validation loss: 2.1919618857804166

Epoch: 5| Step: 10
Training loss: 2.713114023208618
Validation loss: 2.1940538267935477

Epoch: 414| Step: 0
Training loss: 2.5816664695739746
Validation loss: 2.1802616068111953

Epoch: 5| Step: 1
Training loss: 2.8599541187286377
Validation loss: 2.1710412527925227

Epoch: 5| Step: 2
Training loss: 2.457735300064087
Validation loss: 2.1755245911177767

Epoch: 5| Step: 3
Training loss: 1.3314844369888306
Validation loss: 2.168560589513471

Epoch: 5| Step: 4
Training loss: 1.9810184240341187
Validation loss: 2.1697783162516933

Epoch: 5| Step: 5
Training loss: 1.978420615196228
Validation loss: 2.1829666681187128

Epoch: 5| Step: 6
Training loss: 2.2211921215057373
Validation loss: 2.1755550587049095

Epoch: 5| Step: 7
Training loss: 2.3693737983703613
Validation loss: 2.17397206060348

Epoch: 5| Step: 8
Training loss: 2.2120747566223145
Validation loss: 2.188068861602455

Epoch: 5| Step: 9
Training loss: 2.355130195617676
Validation loss: 2.2062824669704644

Epoch: 5| Step: 10
Training loss: 2.7823240756988525
Validation loss: 2.1977415443748556

Epoch: 415| Step: 0
Training loss: 2.5021705627441406
Validation loss: 2.220038062782698

Epoch: 5| Step: 1
Training loss: 2.147200584411621
Validation loss: 2.203795853481498

Epoch: 5| Step: 2
Training loss: 2.2864365577697754
Validation loss: 2.207450364225654

Epoch: 5| Step: 3
Training loss: 2.0548319816589355
Validation loss: 2.188469563761065

Epoch: 5| Step: 4
Training loss: 2.84692645072937
Validation loss: 2.181225715144988

Epoch: 5| Step: 5
Training loss: 2.0123982429504395
Validation loss: 2.1658209523847027

Epoch: 5| Step: 6
Training loss: 1.6601804494857788
Validation loss: 2.150873068840273

Epoch: 5| Step: 7
Training loss: 2.8264222145080566
Validation loss: 2.167098133794723

Epoch: 5| Step: 8
Training loss: 2.3438754081726074
Validation loss: 2.159276711043491

Epoch: 5| Step: 9
Training loss: 2.3650879859924316
Validation loss: 2.157016950268899

Epoch: 5| Step: 10
Training loss: 1.9693377017974854
Validation loss: 2.1601875494885188

Epoch: 416| Step: 0
Training loss: 2.6623072624206543
Validation loss: 2.1600050926208496

Epoch: 5| Step: 1
Training loss: 2.632401704788208
Validation loss: 2.175970290296821

Epoch: 5| Step: 2
Training loss: 1.502162218093872
Validation loss: 2.1552254089745144

Epoch: 5| Step: 3
Training loss: 2.763871431350708
Validation loss: 2.168220830220048

Epoch: 5| Step: 4
Training loss: 2.600698471069336
Validation loss: 2.177060737404772

Epoch: 5| Step: 5
Training loss: 2.144714117050171
Validation loss: 2.1827126728591097

Epoch: 5| Step: 6
Training loss: 1.516400933265686
Validation loss: 2.1604254040666806

Epoch: 5| Step: 7
Training loss: 2.2090578079223633
Validation loss: 2.1848102102997484

Epoch: 5| Step: 8
Training loss: 2.391049861907959
Validation loss: 2.178340842646937

Epoch: 5| Step: 9
Training loss: 2.3246288299560547
Validation loss: 2.1898643560307

Epoch: 5| Step: 10
Training loss: 2.123298168182373
Validation loss: 2.2023345949829265

Epoch: 417| Step: 0
Training loss: 2.383223056793213
Validation loss: 2.2040532122376146

Epoch: 5| Step: 1
Training loss: 2.4072766304016113
Validation loss: 2.2010034181738414

Epoch: 5| Step: 2
Training loss: 2.639705181121826
Validation loss: 2.1995302861736667

Epoch: 5| Step: 3
Training loss: 2.8557534217834473
Validation loss: 2.1718653350748043

Epoch: 5| Step: 4
Training loss: 2.01566219329834
Validation loss: 2.157773447293107

Epoch: 5| Step: 5
Training loss: 2.4607396125793457
Validation loss: 2.155243226276931

Epoch: 5| Step: 6
Training loss: 2.025918483734131
Validation loss: 2.158216872522908

Epoch: 5| Step: 7
Training loss: 1.8106428384780884
Validation loss: 2.1548053449200046

Epoch: 5| Step: 8
Training loss: 2.271028518676758
Validation loss: 2.1605578289237073

Epoch: 5| Step: 9
Training loss: 1.5915462970733643
Validation loss: 2.16068430613446

Epoch: 5| Step: 10
Training loss: 2.4862658977508545
Validation loss: 2.1715326437386135

Epoch: 418| Step: 0
Training loss: 2.4033074378967285
Validation loss: 2.1637706346409296

Epoch: 5| Step: 1
Training loss: 1.7915565967559814
Validation loss: 2.158888445105604

Epoch: 5| Step: 2
Training loss: 2.553637742996216
Validation loss: 2.1698227108165784

Epoch: 5| Step: 3
Training loss: 1.5200343132019043
Validation loss: 2.1846779674612065

Epoch: 5| Step: 4
Training loss: 2.3051674365997314
Validation loss: 2.1927447895849905

Epoch: 5| Step: 5
Training loss: 2.4594507217407227
Validation loss: 2.210934308267409

Epoch: 5| Step: 6
Training loss: 2.3332934379577637
Validation loss: 2.206866151543074

Epoch: 5| Step: 7
Training loss: 1.5636537075042725
Validation loss: 2.1952812056387625

Epoch: 5| Step: 8
Training loss: 2.835045576095581
Validation loss: 2.1953635164486465

Epoch: 5| Step: 9
Training loss: 2.8365538120269775
Validation loss: 2.1925979057947793

Epoch: 5| Step: 10
Training loss: 2.124246835708618
Validation loss: 2.1927094715897755

Epoch: 419| Step: 0
Training loss: 2.3540399074554443
Validation loss: 2.179226254904142

Epoch: 5| Step: 1
Training loss: 1.0076309442520142
Validation loss: 2.17979601634446

Epoch: 5| Step: 2
Training loss: 2.2664990425109863
Validation loss: 2.180912768968972

Epoch: 5| Step: 3
Training loss: 2.3626842498779297
Validation loss: 2.1766656932010444

Epoch: 5| Step: 4
Training loss: 2.3933615684509277
Validation loss: 2.1731318017487884

Epoch: 5| Step: 5
Training loss: 1.930172324180603
Validation loss: 2.1696197512329265

Epoch: 5| Step: 6
Training loss: 2.4834437370300293
Validation loss: 2.1674880801990466

Epoch: 5| Step: 7
Training loss: 2.2939724922180176
Validation loss: 2.1678413575695408

Epoch: 5| Step: 8
Training loss: 2.8026480674743652
Validation loss: 2.1611359452688568

Epoch: 5| Step: 9
Training loss: 2.428201198577881
Validation loss: 2.1546536709672663

Epoch: 5| Step: 10
Training loss: 2.5046393871307373
Validation loss: 2.16992175450889

Epoch: 420| Step: 0
Training loss: 2.468532085418701
Validation loss: 2.143721224159323

Epoch: 5| Step: 1
Training loss: 1.9564847946166992
Validation loss: 2.1412929027311263

Epoch: 5| Step: 2
Training loss: 1.5705012083053589
Validation loss: 2.1356479275611138

Epoch: 5| Step: 3
Training loss: 2.846451759338379
Validation loss: 2.127326694867944

Epoch: 5| Step: 4
Training loss: 2.5336081981658936
Validation loss: 2.141339437935942

Epoch: 5| Step: 5
Training loss: 2.7202024459838867
Validation loss: 2.151261379641871

Epoch: 5| Step: 6
Training loss: 2.6321730613708496
Validation loss: 2.1459137444855063

Epoch: 5| Step: 7
Training loss: 2.1575374603271484
Validation loss: 2.1532124396293395

Epoch: 5| Step: 8
Training loss: 2.248401165008545
Validation loss: 2.157141758549598

Epoch: 5| Step: 9
Training loss: 1.996275544166565
Validation loss: 2.155038801572656

Epoch: 5| Step: 10
Training loss: 1.7540637254714966
Validation loss: 2.1611791938863774

Epoch: 421| Step: 0
Training loss: 1.712484359741211
Validation loss: 2.1816426400215394

Epoch: 5| Step: 1
Training loss: 1.8793329000473022
Validation loss: 2.2010098862391647

Epoch: 5| Step: 2
Training loss: 2.4893901348114014
Validation loss: 2.2094443716028684

Epoch: 5| Step: 3
Training loss: 2.497067451477051
Validation loss: 2.2119249361817555

Epoch: 5| Step: 4
Training loss: 2.091362476348877
Validation loss: 2.192379759204003

Epoch: 5| Step: 5
Training loss: 2.5918755531311035
Validation loss: 2.1769645726808937

Epoch: 5| Step: 6
Training loss: 2.563951015472412
Validation loss: 2.156735593272794

Epoch: 5| Step: 7
Training loss: 2.8097777366638184
Validation loss: 2.149942826199275

Epoch: 5| Step: 8
Training loss: 1.9388208389282227
Validation loss: 2.1337149553401495

Epoch: 5| Step: 9
Training loss: 2.0206618309020996
Validation loss: 2.1385919663213913

Epoch: 5| Step: 10
Training loss: 2.417818069458008
Validation loss: 2.1359000539266937

Epoch: 422| Step: 0
Training loss: 2.473172903060913
Validation loss: 2.13627532733384

Epoch: 5| Step: 1
Training loss: 2.4408652782440186
Validation loss: 2.1404765280344153

Epoch: 5| Step: 2
Training loss: 2.4064769744873047
Validation loss: 2.154212918332828

Epoch: 5| Step: 3
Training loss: 2.1610069274902344
Validation loss: 2.1694139998446227

Epoch: 5| Step: 4
Training loss: 1.6147654056549072
Validation loss: 2.1797599664298435

Epoch: 5| Step: 5
Training loss: 2.176684856414795
Validation loss: 2.195215430310977

Epoch: 5| Step: 6
Training loss: 2.2393009662628174
Validation loss: 2.203362359795519

Epoch: 5| Step: 7
Training loss: 2.841151475906372
Validation loss: 2.1993556689190608

Epoch: 5| Step: 8
Training loss: 2.3673601150512695
Validation loss: 2.203339909994474

Epoch: 5| Step: 9
Training loss: 2.3908355236053467
Validation loss: 2.205185359524142

Epoch: 5| Step: 10
Training loss: 1.6842727661132812
Validation loss: 2.2060092854243454

Epoch: 423| Step: 0
Training loss: 2.0843029022216797
Validation loss: 2.2076480542459795

Epoch: 5| Step: 1
Training loss: 2.529500961303711
Validation loss: 2.186616938601258

Epoch: 5| Step: 2
Training loss: 2.2576000690460205
Validation loss: 2.1811998351927726

Epoch: 5| Step: 3
Training loss: 2.219475269317627
Validation loss: 2.176684377013996

Epoch: 5| Step: 4
Training loss: 3.0078587532043457
Validation loss: 2.157801838331325

Epoch: 5| Step: 5
Training loss: 2.2634968757629395
Validation loss: 2.166793256677607

Epoch: 5| Step: 6
Training loss: 2.366623640060425
Validation loss: 2.1706238177514847

Epoch: 5| Step: 7
Training loss: 1.7057822942733765
Validation loss: 2.172138858866948

Epoch: 5| Step: 8
Training loss: 1.9952226877212524
Validation loss: 2.15499851780553

Epoch: 5| Step: 9
Training loss: 2.2007811069488525
Validation loss: 2.1827595182644424

Epoch: 5| Step: 10
Training loss: 2.0298798084259033
Validation loss: 2.174524825106385

Epoch: 424| Step: 0
Training loss: 2.056988000869751
Validation loss: 2.1740885678158013

Epoch: 5| Step: 1
Training loss: 2.5487403869628906
Validation loss: 2.173547419168616

Epoch: 5| Step: 2
Training loss: 2.4381091594696045
Validation loss: 2.1801757761227187

Epoch: 5| Step: 3
Training loss: 2.405609607696533
Validation loss: 2.176541705285349

Epoch: 5| Step: 4
Training loss: 2.0971426963806152
Validation loss: 2.1789893129820466

Epoch: 5| Step: 5
Training loss: 1.6811538934707642
Validation loss: 2.1665159502337055

Epoch: 5| Step: 6
Training loss: 2.199579954147339
Validation loss: 2.1406881527234147

Epoch: 5| Step: 7
Training loss: 2.7685470581054688
Validation loss: 2.139722979196938

Epoch: 5| Step: 8
Training loss: 2.4750003814697266
Validation loss: 2.1664941990247337

Epoch: 5| Step: 9
Training loss: 2.0232558250427246
Validation loss: 2.181210116673541

Epoch: 5| Step: 10
Training loss: 2.035661458969116
Validation loss: 2.1647704801251813

Epoch: 425| Step: 0
Training loss: 2.5249080657958984
Validation loss: 2.178943395614624

Epoch: 5| Step: 1
Training loss: 2.056659698486328
Validation loss: 2.1844713316168836

Epoch: 5| Step: 2
Training loss: 2.371753215789795
Validation loss: 2.17311865796325

Epoch: 5| Step: 3
Training loss: 1.642539381980896
Validation loss: 2.1540272543507237

Epoch: 5| Step: 4
Training loss: 1.7288286685943604
Validation loss: 2.1556702557430474

Epoch: 5| Step: 5
Training loss: 1.7015310525894165
Validation loss: 2.145318167183989

Epoch: 5| Step: 6
Training loss: 2.362807035446167
Validation loss: 2.1636930460570962

Epoch: 5| Step: 7
Training loss: 2.3637771606445312
Validation loss: 2.1383408551575034

Epoch: 5| Step: 8
Training loss: 3.0852019786834717
Validation loss: 2.1408790798597437

Epoch: 5| Step: 9
Training loss: 2.4450511932373047
Validation loss: 2.146604762282423

Epoch: 5| Step: 10
Training loss: 2.6279525756835938
Validation loss: 2.1454910283447592

Epoch: 426| Step: 0
Training loss: 2.1951847076416016
Validation loss: 2.14886293103618

Epoch: 5| Step: 1
Training loss: 1.3646527528762817
Validation loss: 2.1519480802679576

Epoch: 5| Step: 2
Training loss: 1.9288209676742554
Validation loss: 2.1567609540877806

Epoch: 5| Step: 3
Training loss: 2.3759238719940186
Validation loss: 2.1518465549715105

Epoch: 5| Step: 4
Training loss: 2.6930625438690186
Validation loss: 2.1516503287899877

Epoch: 5| Step: 5
Training loss: 2.5925440788269043
Validation loss: 2.1706260532461186

Epoch: 5| Step: 6
Training loss: 2.11004376411438
Validation loss: 2.170874657169465

Epoch: 5| Step: 7
Training loss: 2.3915905952453613
Validation loss: 2.1721348916330645

Epoch: 5| Step: 8
Training loss: 2.8777847290039062
Validation loss: 2.1664033436006114

Epoch: 5| Step: 9
Training loss: 1.7916622161865234
Validation loss: 2.180685045898602

Epoch: 5| Step: 10
Training loss: 2.3626708984375
Validation loss: 2.1821541299102125

Epoch: 427| Step: 0
Training loss: 1.7934116125106812
Validation loss: 2.180221197425678

Epoch: 5| Step: 1
Training loss: 2.415217638015747
Validation loss: 2.174245362640709

Epoch: 5| Step: 2
Training loss: 2.3722991943359375
Validation loss: 2.156777563915458

Epoch: 5| Step: 3
Training loss: 2.2357330322265625
Validation loss: 2.1538316331883913

Epoch: 5| Step: 4
Training loss: 1.976231336593628
Validation loss: 2.1438833359749085

Epoch: 5| Step: 5
Training loss: 1.9991400241851807
Validation loss: 2.135431315309258

Epoch: 5| Step: 6
Training loss: 2.2287096977233887
Validation loss: 2.1321217039579987

Epoch: 5| Step: 7
Training loss: 2.091165781021118
Validation loss: 2.1311706599368843

Epoch: 5| Step: 8
Training loss: 2.389273166656494
Validation loss: 2.1360467480074976

Epoch: 5| Step: 9
Training loss: 2.1942479610443115
Validation loss: 2.143265588309175

Epoch: 5| Step: 10
Training loss: 3.090402364730835
Validation loss: 2.172653159787578

Epoch: 428| Step: 0
Training loss: 2.6402220726013184
Validation loss: 2.1749000933862503

Epoch: 5| Step: 1
Training loss: 2.3920397758483887
Validation loss: 2.1912609095214517

Epoch: 5| Step: 2
Training loss: 1.7643473148345947
Validation loss: 2.186078635595178

Epoch: 5| Step: 3
Training loss: 2.040666103363037
Validation loss: 2.1705990863102738

Epoch: 5| Step: 4
Training loss: 2.7986435890197754
Validation loss: 2.160210019798689

Epoch: 5| Step: 5
Training loss: 1.9041109085083008
Validation loss: 2.1462109037624892

Epoch: 5| Step: 6
Training loss: 2.546015501022339
Validation loss: 2.1469501603034233

Epoch: 5| Step: 7
Training loss: 1.906381607055664
Validation loss: 2.159421249102521

Epoch: 5| Step: 8
Training loss: 2.2562694549560547
Validation loss: 2.1418826144228698

Epoch: 5| Step: 9
Training loss: 2.3324153423309326
Validation loss: 2.143820526779339

Epoch: 5| Step: 10
Training loss: 1.8825838565826416
Validation loss: 2.144259806602232

Epoch: 429| Step: 0
Training loss: 2.731950521469116
Validation loss: 2.1601402041732625

Epoch: 5| Step: 1
Training loss: 2.8965370655059814
Validation loss: 2.1625409587737052

Epoch: 5| Step: 2
Training loss: 1.7067798376083374
Validation loss: 2.1547517289397535

Epoch: 5| Step: 3
Training loss: 2.2140095233917236
Validation loss: 2.1672116889748523

Epoch: 5| Step: 4
Training loss: 1.8051433563232422
Validation loss: 2.1531295725094375

Epoch: 5| Step: 5
Training loss: 2.278700828552246
Validation loss: 2.1522218988787745

Epoch: 5| Step: 6
Training loss: 2.702244758605957
Validation loss: 2.161183823821365

Epoch: 5| Step: 7
Training loss: 1.353266716003418
Validation loss: 2.150105604561426

Epoch: 5| Step: 8
Training loss: 2.893345355987549
Validation loss: 2.138326285987772

Epoch: 5| Step: 9
Training loss: 2.0537524223327637
Validation loss: 2.1559124685102895

Epoch: 5| Step: 10
Training loss: 1.7704390287399292
Validation loss: 2.137978305098831

Epoch: 430| Step: 0
Training loss: 2.20770001411438
Validation loss: 2.139942146116687

Epoch: 5| Step: 1
Training loss: 2.586106777191162
Validation loss: 2.142202995156729

Epoch: 5| Step: 2
Training loss: 1.7501968145370483
Validation loss: 2.150938877495386

Epoch: 5| Step: 3
Training loss: 2.850903034210205
Validation loss: 2.161735186012842

Epoch: 5| Step: 4
Training loss: 2.3971943855285645
Validation loss: 2.16860709780006

Epoch: 5| Step: 5
Training loss: 1.1700350046157837
Validation loss: 2.170191018812118

Epoch: 5| Step: 6
Training loss: 1.9706718921661377
Validation loss: 2.1590990584383727

Epoch: 5| Step: 7
Training loss: 2.18280291557312
Validation loss: 2.1876088009085706

Epoch: 5| Step: 8
Training loss: 2.7369532585144043
Validation loss: 2.2162785991545646

Epoch: 5| Step: 9
Training loss: 2.2901787757873535
Validation loss: 2.204764530222903

Epoch: 5| Step: 10
Training loss: 2.5663745403289795
Validation loss: 2.2007893157261673

Epoch: 431| Step: 0
Training loss: 1.6385751962661743
Validation loss: 2.174614974247512

Epoch: 5| Step: 1
Training loss: 2.063215732574463
Validation loss: 2.188070639487236

Epoch: 5| Step: 2
Training loss: 3.3407158851623535
Validation loss: 2.16609311872913

Epoch: 5| Step: 3
Training loss: 2.102102756500244
Validation loss: 2.1651672086408063

Epoch: 5| Step: 4
Training loss: 2.2767205238342285
Validation loss: 2.1678803428526847

Epoch: 5| Step: 5
Training loss: 2.412205934524536
Validation loss: 2.1667362887372255

Epoch: 5| Step: 6
Training loss: 2.4562771320343018
Validation loss: 2.1453469440501225

Epoch: 5| Step: 7
Training loss: 1.4784921407699585
Validation loss: 2.1413478223226403

Epoch: 5| Step: 8
Training loss: 2.0075016021728516
Validation loss: 2.154410590407669

Epoch: 5| Step: 9
Training loss: 2.032599449157715
Validation loss: 2.139257997594854

Epoch: 5| Step: 10
Training loss: 2.6838483810424805
Validation loss: 2.1602571523317726

Epoch: 432| Step: 0
Training loss: 2.062847852706909
Validation loss: 2.147899563594531

Epoch: 5| Step: 1
Training loss: 2.240856885910034
Validation loss: 2.1577842466292845

Epoch: 5| Step: 2
Training loss: 1.8327823877334595
Validation loss: 2.1724426413095124

Epoch: 5| Step: 3
Training loss: 2.419711112976074
Validation loss: 2.1579310227465887

Epoch: 5| Step: 4
Training loss: 2.830073118209839
Validation loss: 2.143477319389261

Epoch: 5| Step: 5
Training loss: 2.4520785808563232
Validation loss: 2.1338654128454064

Epoch: 5| Step: 6
Training loss: 2.002075433731079
Validation loss: 2.1248601867306616

Epoch: 5| Step: 7
Training loss: 2.3392786979675293
Validation loss: 2.124936144839051

Epoch: 5| Step: 8
Training loss: 2.5068089962005615
Validation loss: 2.1421323360935336

Epoch: 5| Step: 9
Training loss: 1.6776148080825806
Validation loss: 2.155696315150107

Epoch: 5| Step: 10
Training loss: 2.0528037548065186
Validation loss: 2.1584934291019233

Epoch: 433| Step: 0
Training loss: 2.6508336067199707
Validation loss: 2.164692553140784

Epoch: 5| Step: 1
Training loss: 1.6253162622451782
Validation loss: 2.184343635395009

Epoch: 5| Step: 2
Training loss: 1.9687442779541016
Validation loss: 2.1934275498954197

Epoch: 5| Step: 3
Training loss: 1.7623412609100342
Validation loss: 2.1924170217206402

Epoch: 5| Step: 4
Training loss: 2.366084098815918
Validation loss: 2.1857263042080786

Epoch: 5| Step: 5
Training loss: 2.279374599456787
Validation loss: 2.2072179932748117

Epoch: 5| Step: 6
Training loss: 3.301457166671753
Validation loss: 2.1854473852342173

Epoch: 5| Step: 7
Training loss: 2.022970676422119
Validation loss: 2.1827071969227125

Epoch: 5| Step: 8
Training loss: 1.7934659719467163
Validation loss: 2.1641611360734507

Epoch: 5| Step: 9
Training loss: 2.2524497509002686
Validation loss: 2.1636353564518753

Epoch: 5| Step: 10
Training loss: 2.366184711456299
Validation loss: 2.1604124935724403

Epoch: 434| Step: 0
Training loss: 1.9760065078735352
Validation loss: 2.1843644739479147

Epoch: 5| Step: 1
Training loss: 2.6214516162872314
Validation loss: 2.1616402518364692

Epoch: 5| Step: 2
Training loss: 2.1847715377807617
Validation loss: 2.161832571029663

Epoch: 5| Step: 3
Training loss: 2.675851345062256
Validation loss: 2.144272509441581

Epoch: 5| Step: 4
Training loss: 2.069368362426758
Validation loss: 2.147939048787599

Epoch: 5| Step: 5
Training loss: 2.21136212348938
Validation loss: 2.141209843338177

Epoch: 5| Step: 6
Training loss: 2.392685651779175
Validation loss: 2.1400664391056186

Epoch: 5| Step: 7
Training loss: 2.1690826416015625
Validation loss: 2.1510802520218717

Epoch: 5| Step: 8
Training loss: 2.1882057189941406
Validation loss: 2.1485472443283244

Epoch: 5| Step: 9
Training loss: 1.5415384769439697
Validation loss: 2.1425349686735418

Epoch: 5| Step: 10
Training loss: 2.4314990043640137
Validation loss: 2.123972796624707

Epoch: 435| Step: 0
Training loss: 1.8664108514785767
Validation loss: 2.142499235368544

Epoch: 5| Step: 1
Training loss: 2.6589510440826416
Validation loss: 2.1388461641086045

Epoch: 5| Step: 2
Training loss: 2.1506030559539795
Validation loss: 2.133858278233518

Epoch: 5| Step: 3
Training loss: 1.878259301185608
Validation loss: 2.135262704664661

Epoch: 5| Step: 4
Training loss: 2.1985976696014404
Validation loss: 2.1333876758493404

Epoch: 5| Step: 5
Training loss: 2.1885933876037598
Validation loss: 2.136888970610916

Epoch: 5| Step: 6
Training loss: 2.242048740386963
Validation loss: 2.137961741416685

Epoch: 5| Step: 7
Training loss: 2.363295555114746
Validation loss: 2.1325798021849764

Epoch: 5| Step: 8
Training loss: 2.3700897693634033
Validation loss: 2.128763783362604

Epoch: 5| Step: 9
Training loss: 1.8681453466415405
Validation loss: 2.1276206534395934

Epoch: 5| Step: 10
Training loss: 2.564805746078491
Validation loss: 2.1278954282883675

Epoch: 436| Step: 0
Training loss: 2.2531566619873047
Validation loss: 2.1465883831824026

Epoch: 5| Step: 1
Training loss: 1.8222774267196655
Validation loss: 2.124219294517271

Epoch: 5| Step: 2
Training loss: 2.1246132850646973
Validation loss: 2.156952804134738

Epoch: 5| Step: 3
Training loss: 2.5463767051696777
Validation loss: 2.17779198000508

Epoch: 5| Step: 4
Training loss: 1.9915764331817627
Validation loss: 2.1739171628029115

Epoch: 5| Step: 5
Training loss: 2.4593520164489746
Validation loss: 2.1747410194848174

Epoch: 5| Step: 6
Training loss: 2.477306842803955
Validation loss: 2.183452139618576

Epoch: 5| Step: 7
Training loss: 1.6684049367904663
Validation loss: 2.1617106596628823

Epoch: 5| Step: 8
Training loss: 2.4477646350860596
Validation loss: 2.1444680024218816

Epoch: 5| Step: 9
Training loss: 2.1374783515930176
Validation loss: 2.1528886236170286

Epoch: 5| Step: 10
Training loss: 2.1832427978515625
Validation loss: 2.1402543667824037

Epoch: 437| Step: 0
Training loss: 1.9466902017593384
Validation loss: 2.145202157317951

Epoch: 5| Step: 1
Training loss: 1.9469391107559204
Validation loss: 2.146557977122645

Epoch: 5| Step: 2
Training loss: 2.030062437057495
Validation loss: 2.1296780852861303

Epoch: 5| Step: 3
Training loss: 2.000619411468506
Validation loss: 2.1313230709363054

Epoch: 5| Step: 4
Training loss: 2.3686609268188477
Validation loss: 2.1350424956249934

Epoch: 5| Step: 5
Training loss: 1.850066900253296
Validation loss: 2.1390243307236703

Epoch: 5| Step: 6
Training loss: 2.639617681503296
Validation loss: 2.1195677929027106

Epoch: 5| Step: 7
Training loss: 2.881990909576416
Validation loss: 2.1452739328466435

Epoch: 5| Step: 8
Training loss: 2.269991636276245
Validation loss: 2.1604503546991656

Epoch: 5| Step: 9
Training loss: 2.090456008911133
Validation loss: 2.159600007918573

Epoch: 5| Step: 10
Training loss: 2.213102340698242
Validation loss: 2.175114740607559

Epoch: 438| Step: 0
Training loss: 2.1884355545043945
Validation loss: 2.1975883232649935

Epoch: 5| Step: 1
Training loss: 2.2342023849487305
Validation loss: 2.1806285099316667

Epoch: 5| Step: 2
Training loss: 1.7886463403701782
Validation loss: 2.1813710287053096

Epoch: 5| Step: 3
Training loss: 2.36559796333313
Validation loss: 2.1673540761393886

Epoch: 5| Step: 4
Training loss: 2.4556307792663574
Validation loss: 2.1751114091565533

Epoch: 5| Step: 5
Training loss: 2.3226864337921143
Validation loss: 2.15228630522246

Epoch: 5| Step: 6
Training loss: 1.4322307109832764
Validation loss: 2.1351344393145655

Epoch: 5| Step: 7
Training loss: 2.406747817993164
Validation loss: 2.140501827322027

Epoch: 5| Step: 8
Training loss: 2.4474995136260986
Validation loss: 2.127597616564843

Epoch: 5| Step: 9
Training loss: 2.3975610733032227
Validation loss: 2.1462177589375484

Epoch: 5| Step: 10
Training loss: 2.2128078937530518
Validation loss: 2.1388387282689414

Epoch: 439| Step: 0
Training loss: 2.1497533321380615
Validation loss: 2.1416398735456568

Epoch: 5| Step: 1
Training loss: 1.8080326318740845
Validation loss: 2.131417179620394

Epoch: 5| Step: 2
Training loss: 2.157498598098755
Validation loss: 2.1167745449209727

Epoch: 5| Step: 3
Training loss: 2.2958340644836426
Validation loss: 2.139893968900045

Epoch: 5| Step: 4
Training loss: 2.451770305633545
Validation loss: 2.1307000870345743

Epoch: 5| Step: 5
Training loss: 2.2776553630828857
Validation loss: 2.1360437946934856

Epoch: 5| Step: 6
Training loss: 2.797069549560547
Validation loss: 2.1167799042117212

Epoch: 5| Step: 7
Training loss: 1.9717353582382202
Validation loss: 2.1180858355696484

Epoch: 5| Step: 8
Training loss: 2.1543309688568115
Validation loss: 2.1131504428002144

Epoch: 5| Step: 9
Training loss: 2.060516834259033
Validation loss: 2.1308759694458335

Epoch: 5| Step: 10
Training loss: 2.0597243309020996
Validation loss: 2.1446104998229654

Epoch: 440| Step: 0
Training loss: 2.040050506591797
Validation loss: 2.146059195200602

Epoch: 5| Step: 1
Training loss: 1.6202728748321533
Validation loss: 2.16584805006622

Epoch: 5| Step: 2
Training loss: 2.26023530960083
Validation loss: 2.159483944216082

Epoch: 5| Step: 3
Training loss: 2.1398167610168457
Validation loss: 2.1748204590171896

Epoch: 5| Step: 4
Training loss: 2.0295422077178955
Validation loss: 2.191121426961755

Epoch: 5| Step: 5
Training loss: 2.410203456878662
Validation loss: 2.169673099312731

Epoch: 5| Step: 6
Training loss: 2.2025160789489746
Validation loss: 2.149552135057347

Epoch: 5| Step: 7
Training loss: 2.293398380279541
Validation loss: 2.152094861512543

Epoch: 5| Step: 8
Training loss: 2.9687342643737793
Validation loss: 2.156751089198615

Epoch: 5| Step: 9
Training loss: 2.1418826580047607
Validation loss: 2.1531003059879428

Epoch: 5| Step: 10
Training loss: 2.140653133392334
Validation loss: 2.1535970882702897

Epoch: 441| Step: 0
Training loss: 1.9000087976455688
Validation loss: 2.1476278612690587

Epoch: 5| Step: 1
Training loss: 2.6034927368164062
Validation loss: 2.1447337289010324

Epoch: 5| Step: 2
Training loss: 2.47865629196167
Validation loss: 2.156457777946226

Epoch: 5| Step: 3
Training loss: 1.9814612865447998
Validation loss: 2.1557652104285454

Epoch: 5| Step: 4
Training loss: 1.960919976234436
Validation loss: 2.1727509549869004

Epoch: 5| Step: 5
Training loss: 1.9708000421524048
Validation loss: 2.1680292596099195

Epoch: 5| Step: 6
Training loss: 1.813746452331543
Validation loss: 2.1684189637502036

Epoch: 5| Step: 7
Training loss: 2.877854108810425
Validation loss: 2.171741949614658

Epoch: 5| Step: 8
Training loss: 2.255481719970703
Validation loss: 2.167045042078982

Epoch: 5| Step: 9
Training loss: 2.3426687717437744
Validation loss: 2.159137054156232

Epoch: 5| Step: 10
Training loss: 1.9581305980682373
Validation loss: 2.129415540285008

Epoch: 442| Step: 0
Training loss: 1.8850351572036743
Validation loss: 2.1386860160417456

Epoch: 5| Step: 1
Training loss: 2.1804189682006836
Validation loss: 2.129187935142107

Epoch: 5| Step: 2
Training loss: 1.7893813848495483
Validation loss: 2.133819910787767

Epoch: 5| Step: 3
Training loss: 3.082834005355835
Validation loss: 2.147916450295397

Epoch: 5| Step: 4
Training loss: 2.5211150646209717
Validation loss: 2.1559948126475015

Epoch: 5| Step: 5
Training loss: 1.9942715167999268
Validation loss: 2.155030717131912

Epoch: 5| Step: 6
Training loss: 1.7716821432113647
Validation loss: 2.1815715553939983

Epoch: 5| Step: 7
Training loss: 2.9663946628570557
Validation loss: 2.1871216630422943

Epoch: 5| Step: 8
Training loss: 1.8718525171279907
Validation loss: 2.1872651474450224

Epoch: 5| Step: 9
Training loss: 1.9215492010116577
Validation loss: 2.164269075598768

Epoch: 5| Step: 10
Training loss: 2.339845657348633
Validation loss: 2.154177899001747

Epoch: 443| Step: 0
Training loss: 2.4110336303710938
Validation loss: 2.1454625809064476

Epoch: 5| Step: 1
Training loss: 2.5343589782714844
Validation loss: 2.141857631744877

Epoch: 5| Step: 2
Training loss: 2.517683506011963
Validation loss: 2.127313724128149

Epoch: 5| Step: 3
Training loss: 1.8208019733428955
Validation loss: 2.1209837518712527

Epoch: 5| Step: 4
Training loss: 2.1379246711730957
Validation loss: 2.12690286097988

Epoch: 5| Step: 5
Training loss: 2.5350801944732666
Validation loss: 2.1215447866788475

Epoch: 5| Step: 6
Training loss: 2.110224485397339
Validation loss: 2.1345772922679944

Epoch: 5| Step: 7
Training loss: 1.6802055835723877
Validation loss: 2.1343857806216002

Epoch: 5| Step: 8
Training loss: 2.4814467430114746
Validation loss: 2.12967820577724

Epoch: 5| Step: 9
Training loss: 1.943887710571289
Validation loss: 2.148460218983312

Epoch: 5| Step: 10
Training loss: 1.8184585571289062
Validation loss: 2.159620746489494

Epoch: 444| Step: 0
Training loss: 2.0765161514282227
Validation loss: 2.159399902948769

Epoch: 5| Step: 1
Training loss: 1.921099305152893
Validation loss: 2.1639509047231367

Epoch: 5| Step: 2
Training loss: 2.7885079383850098
Validation loss: 2.1584931394105316

Epoch: 5| Step: 3
Training loss: 2.053335666656494
Validation loss: 2.164098301241475

Epoch: 5| Step: 4
Training loss: 2.6068403720855713
Validation loss: 2.134735676550096

Epoch: 5| Step: 5
Training loss: 2.2132344245910645
Validation loss: 2.1388389474602154

Epoch: 5| Step: 6
Training loss: 2.321547746658325
Validation loss: 2.12905534877572

Epoch: 5| Step: 7
Training loss: 1.6775567531585693
Validation loss: 2.1168356762137464

Epoch: 5| Step: 8
Training loss: 2.4621927738189697
Validation loss: 2.126377256967688

Epoch: 5| Step: 9
Training loss: 2.062091827392578
Validation loss: 2.1163114270856305

Epoch: 5| Step: 10
Training loss: 2.036844491958618
Validation loss: 2.104587462640578

Epoch: 445| Step: 0
Training loss: 2.4638123512268066
Validation loss: 2.1131719414905836

Epoch: 5| Step: 1
Training loss: 2.2432141304016113
Validation loss: 2.1266841593609063

Epoch: 5| Step: 2
Training loss: 2.398576021194458
Validation loss: 2.1094816410413353

Epoch: 5| Step: 3
Training loss: 1.945741057395935
Validation loss: 2.0967133980925365

Epoch: 5| Step: 4
Training loss: 1.8598966598510742
Validation loss: 2.100463846678375

Epoch: 5| Step: 5
Training loss: 2.6324172019958496
Validation loss: 2.1106133422543927

Epoch: 5| Step: 6
Training loss: 2.5842483043670654
Validation loss: 2.107436369824153

Epoch: 5| Step: 7
Training loss: 1.6687259674072266
Validation loss: 2.106618873534664

Epoch: 5| Step: 8
Training loss: 2.2772340774536133
Validation loss: 2.132473789235597

Epoch: 5| Step: 9
Training loss: 2.018925189971924
Validation loss: 2.1458184539630847

Epoch: 5| Step: 10
Training loss: 1.9243810176849365
Validation loss: 2.1739846480790006

Epoch: 446| Step: 0
Training loss: 2.0663695335388184
Validation loss: 2.1927007449570524

Epoch: 5| Step: 1
Training loss: 2.2302842140197754
Validation loss: 2.2184011090186333

Epoch: 5| Step: 2
Training loss: 2.185288906097412
Validation loss: 2.207709825167092

Epoch: 5| Step: 3
Training loss: 2.010715961456299
Validation loss: 2.1980817317962646

Epoch: 5| Step: 4
Training loss: 2.1350924968719482
Validation loss: 2.1785777486780638

Epoch: 5| Step: 5
Training loss: 2.1507785320281982
Validation loss: 2.165834944735291

Epoch: 5| Step: 6
Training loss: 2.4370341300964355
Validation loss: 2.1683928146157214

Epoch: 5| Step: 7
Training loss: 2.3879406452178955
Validation loss: 2.1568189872208463

Epoch: 5| Step: 8
Training loss: 2.361032724380493
Validation loss: 2.150101487354566

Epoch: 5| Step: 9
Training loss: 2.0847134590148926
Validation loss: 2.1641618846565165

Epoch: 5| Step: 10
Training loss: 1.9398374557495117
Validation loss: 2.159475993084651

Epoch: 447| Step: 0
Training loss: 2.222844123840332
Validation loss: 2.1511322221448346

Epoch: 5| Step: 1
Training loss: 2.4954874515533447
Validation loss: 2.1480727221376155

Epoch: 5| Step: 2
Training loss: 2.535881519317627
Validation loss: 2.1463041638815277

Epoch: 5| Step: 3
Training loss: 1.6103601455688477
Validation loss: 2.1538682214675413

Epoch: 5| Step: 4
Training loss: 2.6199681758880615
Validation loss: 2.1708411914046093

Epoch: 5| Step: 5
Training loss: 1.743285894393921
Validation loss: 2.1547441918362855

Epoch: 5| Step: 6
Training loss: 2.365018367767334
Validation loss: 2.1328935059168006

Epoch: 5| Step: 7
Training loss: 2.5676071643829346
Validation loss: 2.1421749002190045

Epoch: 5| Step: 8
Training loss: 2.148322582244873
Validation loss: 2.1321001693766606

Epoch: 5| Step: 9
Training loss: 1.5323835611343384
Validation loss: 2.118634887920913

Epoch: 5| Step: 10
Training loss: 2.1431071758270264
Validation loss: 2.1188928158052507

Epoch: 448| Step: 0
Training loss: 2.4448461532592773
Validation loss: 2.114837622129789

Epoch: 5| Step: 1
Training loss: 2.5102648735046387
Validation loss: 2.122291916160173

Epoch: 5| Step: 2
Training loss: 1.904254674911499
Validation loss: 2.120593132511262

Epoch: 5| Step: 3
Training loss: 2.6832242012023926
Validation loss: 2.112821573852211

Epoch: 5| Step: 4
Training loss: 1.9284954071044922
Validation loss: 2.130041286509524

Epoch: 5| Step: 5
Training loss: 2.3689072132110596
Validation loss: 2.1383718893092167

Epoch: 5| Step: 6
Training loss: 2.3137435913085938
Validation loss: 2.1237985933980634

Epoch: 5| Step: 7
Training loss: 1.8912080526351929
Validation loss: 2.129989357404811

Epoch: 5| Step: 8
Training loss: 2.1190428733825684
Validation loss: 2.152208302610664

Epoch: 5| Step: 9
Training loss: 1.6203655004501343
Validation loss: 2.1713539759318032

Epoch: 5| Step: 10
Training loss: 2.1244068145751953
Validation loss: 2.1770875659040225

Epoch: 449| Step: 0
Training loss: 1.9821163415908813
Validation loss: 2.1982407800612913

Epoch: 5| Step: 1
Training loss: 2.1931254863739014
Validation loss: 2.18362162446463

Epoch: 5| Step: 2
Training loss: 1.893738031387329
Validation loss: 2.185251687162666

Epoch: 5| Step: 3
Training loss: 1.890562653541565
Validation loss: 2.180128361589165

Epoch: 5| Step: 4
Training loss: 2.335324287414551
Validation loss: 2.163747377293084

Epoch: 5| Step: 5
Training loss: 2.3413853645324707
Validation loss: 2.1649623032539123

Epoch: 5| Step: 6
Training loss: 3.009960174560547
Validation loss: 2.1452660176061813

Epoch: 5| Step: 7
Training loss: 2.272629976272583
Validation loss: 2.1407065494086153

Epoch: 5| Step: 8
Training loss: 2.226624011993408
Validation loss: 2.13511025392881

Epoch: 5| Step: 9
Training loss: 1.8085565567016602
Validation loss: 2.1242792106443837

Epoch: 5| Step: 10
Training loss: 2.1887242794036865
Validation loss: 2.1155798524938603

Epoch: 450| Step: 0
Training loss: 2.523723602294922
Validation loss: 2.1162504726840603

Epoch: 5| Step: 1
Training loss: 1.623720407485962
Validation loss: 2.1321375216207197

Epoch: 5| Step: 2
Training loss: 2.995448589324951
Validation loss: 2.142835114591865

Epoch: 5| Step: 3
Training loss: 2.165351152420044
Validation loss: 2.1587430507906022

Epoch: 5| Step: 4
Training loss: 1.7411062717437744
Validation loss: 2.1835629888760146

Epoch: 5| Step: 5
Training loss: 2.7231199741363525
Validation loss: 2.176274540603802

Epoch: 5| Step: 6
Training loss: 2.411186456680298
Validation loss: 2.2064464874165033

Epoch: 5| Step: 7
Training loss: 1.4215646982192993
Validation loss: 2.1986572075915594

Epoch: 5| Step: 8
Training loss: 1.796656608581543
Validation loss: 2.177292139299454

Epoch: 5| Step: 9
Training loss: 1.8623040914535522
Validation loss: 2.1628726784900953

Epoch: 5| Step: 10
Training loss: 3.033080577850342
Validation loss: 2.1452819544781923

Epoch: 451| Step: 0
Training loss: 2.154679775238037
Validation loss: 2.1243378936603503

Epoch: 5| Step: 1
Training loss: 2.089259386062622
Validation loss: 2.113528702848701

Epoch: 5| Step: 2
Training loss: 2.2955126762390137
Validation loss: 2.125487978740405

Epoch: 5| Step: 3
Training loss: 2.2662644386291504
Validation loss: 2.1204412444945304

Epoch: 5| Step: 4
Training loss: 2.2099273204803467
Validation loss: 2.121599192260414

Epoch: 5| Step: 5
Training loss: 2.224538564682007
Validation loss: 2.121172158948837

Epoch: 5| Step: 6
Training loss: 2.7132484912872314
Validation loss: 2.143292987218467

Epoch: 5| Step: 7
Training loss: 1.241333246231079
Validation loss: 2.149017528821063

Epoch: 5| Step: 8
Training loss: 2.702355146408081
Validation loss: 2.170919918244885

Epoch: 5| Step: 9
Training loss: 2.1114461421966553
Validation loss: 2.1528840206002675

Epoch: 5| Step: 10
Training loss: 1.8406848907470703
Validation loss: 2.167695491544662

Epoch: 452| Step: 0
Training loss: 1.7484607696533203
Validation loss: 2.1617380137084634

Epoch: 5| Step: 1
Training loss: 2.312530994415283
Validation loss: 2.1784278423555437

Epoch: 5| Step: 2
Training loss: 2.628222703933716
Validation loss: 2.1671649153514574

Epoch: 5| Step: 3
Training loss: 2.377324342727661
Validation loss: 2.164677402024628

Epoch: 5| Step: 4
Training loss: 2.0918822288513184
Validation loss: 2.119851804548694

Epoch: 5| Step: 5
Training loss: 2.3288044929504395
Validation loss: 2.1285032867103495

Epoch: 5| Step: 6
Training loss: 2.0353798866271973
Validation loss: 2.0965056778282247

Epoch: 5| Step: 7
Training loss: 2.468557357788086
Validation loss: 2.1074294800399453

Epoch: 5| Step: 8
Training loss: 1.9474687576293945
Validation loss: 2.090179003695006

Epoch: 5| Step: 9
Training loss: 2.0509426593780518
Validation loss: 2.0930081362365396

Epoch: 5| Step: 10
Training loss: 2.0892601013183594
Validation loss: 2.09243756212214

Epoch: 453| Step: 0
Training loss: 1.7843395471572876
Validation loss: 2.087308801630492

Epoch: 5| Step: 1
Training loss: 2.1054677963256836
Validation loss: 2.0975627886351718

Epoch: 5| Step: 2
Training loss: 1.6032803058624268
Validation loss: 2.1108215726831907

Epoch: 5| Step: 3
Training loss: 1.3850716352462769
Validation loss: 2.1274711803723405

Epoch: 5| Step: 4
Training loss: 3.1369049549102783
Validation loss: 2.150724413574383

Epoch: 5| Step: 5
Training loss: 2.6625630855560303
Validation loss: 2.14087857994982

Epoch: 5| Step: 6
Training loss: 2.143275022506714
Validation loss: 2.178574482599894

Epoch: 5| Step: 7
Training loss: 2.5135509967803955
Validation loss: 2.2110061466052966

Epoch: 5| Step: 8
Training loss: 3.1542911529541016
Validation loss: 2.213404070946478

Epoch: 5| Step: 9
Training loss: 2.4485254287719727
Validation loss: 2.2185486183371594

Epoch: 5| Step: 10
Training loss: 1.1605267524719238
Validation loss: 2.1991112693663566

Epoch: 454| Step: 0
Training loss: 1.928200364112854
Validation loss: 2.1737837253078336

Epoch: 5| Step: 1
Training loss: 2.8004939556121826
Validation loss: 2.1712966042180217

Epoch: 5| Step: 2
Training loss: 2.0403549671173096
Validation loss: 2.156929208386329

Epoch: 5| Step: 3
Training loss: 2.0736658573150635
Validation loss: 2.140725951040945

Epoch: 5| Step: 4
Training loss: 2.286107301712036
Validation loss: 2.1453795561226467

Epoch: 5| Step: 5
Training loss: 2.0131030082702637
Validation loss: 2.1357194582621255

Epoch: 5| Step: 6
Training loss: 2.3091812133789062
Validation loss: 2.1397087240731842

Epoch: 5| Step: 7
Training loss: 2.207693576812744
Validation loss: 2.127993388842511

Epoch: 5| Step: 8
Training loss: 1.9192540645599365
Validation loss: 2.1338772209741736

Epoch: 5| Step: 9
Training loss: 1.9290157556533813
Validation loss: 2.1173308152024464

Epoch: 5| Step: 10
Training loss: 2.3599748611450195
Validation loss: 2.1213126849102717

Epoch: 455| Step: 0
Training loss: 2.1999480724334717
Validation loss: 2.121013623411937

Epoch: 5| Step: 1
Training loss: 2.126770496368408
Validation loss: 2.121053182950584

Epoch: 5| Step: 2
Training loss: 2.0055489540100098
Validation loss: 2.125361566902489

Epoch: 5| Step: 3
Training loss: 2.748683214187622
Validation loss: 2.132800590607428

Epoch: 5| Step: 4
Training loss: 2.0305376052856445
Validation loss: 2.1374836532018517

Epoch: 5| Step: 5
Training loss: 2.2547972202301025
Validation loss: 2.1490069691852858

Epoch: 5| Step: 6
Training loss: 2.118997097015381
Validation loss: 2.147484246120658

Epoch: 5| Step: 7
Training loss: 1.9778705835342407
Validation loss: 2.1466790373607347

Epoch: 5| Step: 8
Training loss: 2.270162582397461
Validation loss: 2.126340562297452

Epoch: 5| Step: 9
Training loss: 1.7330089807510376
Validation loss: 2.1143996433545182

Epoch: 5| Step: 10
Training loss: 2.331120014190674
Validation loss: 2.116897016443232

Epoch: 456| Step: 0
Training loss: 1.6286163330078125
Validation loss: 2.1129804657351587

Epoch: 5| Step: 1
Training loss: 1.7619152069091797
Validation loss: 2.1020966191445627

Epoch: 5| Step: 2
Training loss: 1.8280601501464844
Validation loss: 2.1200174926429667

Epoch: 5| Step: 3
Training loss: 1.4235457181930542
Validation loss: 2.1079123353445404

Epoch: 5| Step: 4
Training loss: 2.667250871658325
Validation loss: 2.126324975362388

Epoch: 5| Step: 5
Training loss: 2.221681594848633
Validation loss: 2.136123385480655

Epoch: 5| Step: 6
Training loss: 2.3919360637664795
Validation loss: 2.14623696573319

Epoch: 5| Step: 7
Training loss: 2.667778491973877
Validation loss: 2.122604549572032

Epoch: 5| Step: 8
Training loss: 2.32039213180542
Validation loss: 2.1511148278431227

Epoch: 5| Step: 9
Training loss: 1.8652057647705078
Validation loss: 2.158529240597961

Epoch: 5| Step: 10
Training loss: 3.033320188522339
Validation loss: 2.1645578953527633

Epoch: 457| Step: 0
Training loss: 2.3999767303466797
Validation loss: 2.170309933282996

Epoch: 5| Step: 1
Training loss: 2.041703701019287
Validation loss: 2.1551663875579834

Epoch: 5| Step: 2
Training loss: 1.9294719696044922
Validation loss: 2.1458061766880814

Epoch: 5| Step: 3
Training loss: 1.5049179792404175
Validation loss: 2.129330535088816

Epoch: 5| Step: 4
Training loss: 2.3213491439819336
Validation loss: 2.1448868859198784

Epoch: 5| Step: 5
Training loss: 2.5802791118621826
Validation loss: 2.1321599650126632

Epoch: 5| Step: 6
Training loss: 2.016662120819092
Validation loss: 2.126111694561538

Epoch: 5| Step: 7
Training loss: 2.6953461170196533
Validation loss: 2.1300787925720215

Epoch: 5| Step: 8
Training loss: 2.0077767372131348
Validation loss: 2.134773850440979

Epoch: 5| Step: 9
Training loss: 2.1464591026306152
Validation loss: 2.1072514877524426

Epoch: 5| Step: 10
Training loss: 2.0307140350341797
Validation loss: 2.1219598349704536

Epoch: 458| Step: 0
Training loss: 2.5882420539855957
Validation loss: 2.111738516438392

Epoch: 5| Step: 1
Training loss: 2.5467326641082764
Validation loss: 2.1209519499091694

Epoch: 5| Step: 2
Training loss: 1.8652286529541016
Validation loss: 2.111859908667944

Epoch: 5| Step: 3
Training loss: 2.141449451446533
Validation loss: 2.1157323365570395

Epoch: 5| Step: 4
Training loss: 2.0577750205993652
Validation loss: 2.1110586940601306

Epoch: 5| Step: 5
Training loss: 1.9527069330215454
Validation loss: 2.126470163304319

Epoch: 5| Step: 6
Training loss: 2.492832660675049
Validation loss: 2.1385178796706663

Epoch: 5| Step: 7
Training loss: 2.143263816833496
Validation loss: 2.1335927799183834

Epoch: 5| Step: 8
Training loss: 1.7688496112823486
Validation loss: 2.1377617800107567

Epoch: 5| Step: 9
Training loss: 2.016500473022461
Validation loss: 2.1298222208535798

Epoch: 5| Step: 10
Training loss: 2.2254889011383057
Validation loss: 2.1241586131434285

Epoch: 459| Step: 0
Training loss: 2.579531192779541
Validation loss: 2.1337245484834075

Epoch: 5| Step: 1
Training loss: 1.8050403594970703
Validation loss: 2.1304836401375393

Epoch: 5| Step: 2
Training loss: 2.0062787532806396
Validation loss: 2.1370564468445314

Epoch: 5| Step: 3
Training loss: 1.9557254314422607
Validation loss: 2.137887800893476

Epoch: 5| Step: 4
Training loss: 1.7683522701263428
Validation loss: 2.136342053772301

Epoch: 5| Step: 5
Training loss: 1.214656114578247
Validation loss: 2.130718732392916

Epoch: 5| Step: 6
Training loss: 2.555934190750122
Validation loss: 2.126877474528487

Epoch: 5| Step: 7
Training loss: 2.0555269718170166
Validation loss: 2.10680878034202

Epoch: 5| Step: 8
Training loss: 2.7306265830993652
Validation loss: 2.1068284973021476

Epoch: 5| Step: 9
Training loss: 2.6801857948303223
Validation loss: 2.1139109185946885

Epoch: 5| Step: 10
Training loss: 2.3040125370025635
Validation loss: 2.1091272548962663

Epoch: 460| Step: 0
Training loss: 1.8882509469985962
Validation loss: 2.104653840423912

Epoch: 5| Step: 1
Training loss: 2.081670045852661
Validation loss: 2.1156624619678785

Epoch: 5| Step: 2
Training loss: 1.8024322986602783
Validation loss: 2.109374592381139

Epoch: 5| Step: 3
Training loss: 2.4705846309661865
Validation loss: 2.0971977710723877

Epoch: 5| Step: 4
Training loss: 1.9092214107513428
Validation loss: 2.114674800185747

Epoch: 5| Step: 5
Training loss: 2.5114805698394775
Validation loss: 2.123713013946369

Epoch: 5| Step: 6
Training loss: 1.568569302558899
Validation loss: 2.1199623743693032

Epoch: 5| Step: 7
Training loss: 2.5930025577545166
Validation loss: 2.1254093262457077

Epoch: 5| Step: 8
Training loss: 2.425891399383545
Validation loss: 2.1460391385580904

Epoch: 5| Step: 9
Training loss: 2.684898853302002
Validation loss: 2.1665089873857397

Epoch: 5| Step: 10
Training loss: 1.7147247791290283
Validation loss: 2.183988978785853

Epoch: 461| Step: 0
Training loss: 1.6357746124267578
Validation loss: 2.173482759024507

Epoch: 5| Step: 1
Training loss: 2.244311571121216
Validation loss: 2.173147888593776

Epoch: 5| Step: 2
Training loss: 2.101067066192627
Validation loss: 2.1681510222855436

Epoch: 5| Step: 3
Training loss: 2.076627254486084
Validation loss: 2.152077838938723

Epoch: 5| Step: 4
Training loss: 2.046027183532715
Validation loss: 2.146128054588072

Epoch: 5| Step: 5
Training loss: 2.513477325439453
Validation loss: 2.1208789797239405

Epoch: 5| Step: 6
Training loss: 2.2526137828826904
Validation loss: 2.142923597366579

Epoch: 5| Step: 7
Training loss: 2.0049471855163574
Validation loss: 2.1217482013087117

Epoch: 5| Step: 8
Training loss: 2.2393782138824463
Validation loss: 2.1207364566864504

Epoch: 5| Step: 9
Training loss: 2.403994560241699
Validation loss: 2.1231494924073577

Epoch: 5| Step: 10
Training loss: 2.1872780323028564
Validation loss: 2.133357722272155

Epoch: 462| Step: 0
Training loss: 2.2158665657043457
Validation loss: 2.1262513283760316

Epoch: 5| Step: 1
Training loss: 2.2012715339660645
Validation loss: 2.1259401690575386

Epoch: 5| Step: 2
Training loss: 2.458789110183716
Validation loss: 2.1236440289405083

Epoch: 5| Step: 3
Training loss: 2.432255268096924
Validation loss: 2.118611329345293

Epoch: 5| Step: 4
Training loss: 2.4178969860076904
Validation loss: 2.1192829121825514

Epoch: 5| Step: 5
Training loss: 2.1995596885681152
Validation loss: 2.100738433099562

Epoch: 5| Step: 6
Training loss: 2.5508182048797607
Validation loss: 2.1012742801379134

Epoch: 5| Step: 7
Training loss: 1.552527904510498
Validation loss: 2.0975878866769935

Epoch: 5| Step: 8
Training loss: 1.623399019241333
Validation loss: 2.1226051968912922

Epoch: 5| Step: 9
Training loss: 1.4848077297210693
Validation loss: 2.12107083618

Epoch: 5| Step: 10
Training loss: 2.4649250507354736
Validation loss: 2.1323208552534862

Epoch: 463| Step: 0
Training loss: 2.384669780731201
Validation loss: 2.128551552372594

Epoch: 5| Step: 1
Training loss: 1.9615955352783203
Validation loss: 2.149313916442215

Epoch: 5| Step: 2
Training loss: 2.260484218597412
Validation loss: 2.17266502688008

Epoch: 5| Step: 3
Training loss: 2.4932522773742676
Validation loss: 2.148716708665253

Epoch: 5| Step: 4
Training loss: 1.978039026260376
Validation loss: 2.1387248269973265

Epoch: 5| Step: 5
Training loss: 2.44217848777771
Validation loss: 2.135346830532115

Epoch: 5| Step: 6
Training loss: 2.038417339324951
Validation loss: 2.1178996293775496

Epoch: 5| Step: 7
Training loss: 1.587437391281128
Validation loss: 2.105741967437088

Epoch: 5| Step: 8
Training loss: 2.1854634284973145
Validation loss: 2.102008277370084

Epoch: 5| Step: 9
Training loss: 2.3449409008026123
Validation loss: 2.09562333168522

Epoch: 5| Step: 10
Training loss: 1.762776255607605
Validation loss: 2.098592617178476

Epoch: 464| Step: 0
Training loss: 2.6954102516174316
Validation loss: 2.091942246242236

Epoch: 5| Step: 1
Training loss: 2.5070109367370605
Validation loss: 2.111556200570958

Epoch: 5| Step: 2
Training loss: 2.1201024055480957
Validation loss: 2.1047668713395313

Epoch: 5| Step: 3
Training loss: 1.8398253917694092
Validation loss: 2.1085770796704035

Epoch: 5| Step: 4
Training loss: 2.064565658569336
Validation loss: 2.111852950947259

Epoch: 5| Step: 5
Training loss: 1.671007752418518
Validation loss: 2.1170412801927134

Epoch: 5| Step: 6
Training loss: 2.5290236473083496
Validation loss: 2.129817106390512

Epoch: 5| Step: 7
Training loss: 1.6621767282485962
Validation loss: 2.1299314293810117

Epoch: 5| Step: 8
Training loss: 2.782592296600342
Validation loss: 2.129423761880526

Epoch: 5| Step: 9
Training loss: 1.840049386024475
Validation loss: 2.145762489688012

Epoch: 5| Step: 10
Training loss: 1.48923921585083
Validation loss: 2.1551744450805006

Epoch: 465| Step: 0
Training loss: 2.220923900604248
Validation loss: 2.1740028653093564

Epoch: 5| Step: 1
Training loss: 1.6412932872772217
Validation loss: 2.1820038159688315

Epoch: 5| Step: 2
Training loss: 2.181331157684326
Validation loss: 2.1593501311476513

Epoch: 5| Step: 3
Training loss: 2.6827573776245117
Validation loss: 2.1379644383666334

Epoch: 5| Step: 4
Training loss: 2.177236795425415
Validation loss: 2.1160871598028366

Epoch: 5| Step: 5
Training loss: 2.4299232959747314
Validation loss: 2.1040173333178283

Epoch: 5| Step: 6
Training loss: 2.041475534439087
Validation loss: 2.092082836294687

Epoch: 5| Step: 7
Training loss: 2.480870246887207
Validation loss: 2.088917742493332

Epoch: 5| Step: 8
Training loss: 1.9943110942840576
Validation loss: 2.0922589135426346

Epoch: 5| Step: 9
Training loss: 2.0823216438293457
Validation loss: 2.0942493254138577

Epoch: 5| Step: 10
Training loss: 1.5314617156982422
Validation loss: 2.092341234607081

Epoch: 466| Step: 0
Training loss: 2.5697834491729736
Validation loss: 2.0911512272332304

Epoch: 5| Step: 1
Training loss: 2.2042667865753174
Validation loss: 2.1027377061946417

Epoch: 5| Step: 2
Training loss: 2.0486061573028564
Validation loss: 2.122528273572204

Epoch: 5| Step: 3
Training loss: 2.110591411590576
Validation loss: 2.1182389361884004

Epoch: 5| Step: 4
Training loss: 2.4229660034179688
Validation loss: 2.137914971638751

Epoch: 5| Step: 5
Training loss: 1.870826005935669
Validation loss: 2.1713719521799395

Epoch: 5| Step: 6
Training loss: 2.1891534328460693
Validation loss: 2.1779175061051563

Epoch: 5| Step: 7
Training loss: 2.3352551460266113
Validation loss: 2.1738926229938382

Epoch: 5| Step: 8
Training loss: 2.2529358863830566
Validation loss: 2.1570882463967926

Epoch: 5| Step: 9
Training loss: 2.20491886138916
Validation loss: 2.144706554310296

Epoch: 5| Step: 10
Training loss: 1.1608837842941284
Validation loss: 2.1248022446068386

Epoch: 467| Step: 0
Training loss: 2.524418354034424
Validation loss: 2.139063226279392

Epoch: 5| Step: 1
Training loss: 2.897371530532837
Validation loss: 2.1119291628560712

Epoch: 5| Step: 2
Training loss: 2.4643657207489014
Validation loss: 2.1097965330205937

Epoch: 5| Step: 3
Training loss: 2.0080482959747314
Validation loss: 2.103018732481105

Epoch: 5| Step: 4
Training loss: 1.861864447593689
Validation loss: 2.0994222625609367

Epoch: 5| Step: 5
Training loss: 2.11186146736145
Validation loss: 2.094592462303818

Epoch: 5| Step: 6
Training loss: 1.6166632175445557
Validation loss: 2.07962461697158

Epoch: 5| Step: 7
Training loss: 2.1096608638763428
Validation loss: 2.087497515063132

Epoch: 5| Step: 8
Training loss: 2.1382479667663574
Validation loss: 2.093399073487969

Epoch: 5| Step: 9
Training loss: 2.11407732963562
Validation loss: 2.1270654355326006

Epoch: 5| Step: 10
Training loss: 1.4670559167861938
Validation loss: 2.1127385400956675

Epoch: 468| Step: 0
Training loss: 1.4894514083862305
Validation loss: 2.127322220033215

Epoch: 5| Step: 1
Training loss: 2.610948324203491
Validation loss: 2.147178731938844

Epoch: 5| Step: 2
Training loss: 1.5881905555725098
Validation loss: 2.1393607021659933

Epoch: 5| Step: 3
Training loss: 2.1500306129455566
Validation loss: 2.1354357914258073

Epoch: 5| Step: 4
Training loss: 2.4799458980560303
Validation loss: 2.1383500253000567

Epoch: 5| Step: 5
Training loss: 2.5985233783721924
Validation loss: 2.136519259022128

Epoch: 5| Step: 6
Training loss: 2.300095796585083
Validation loss: 2.133506203210482

Epoch: 5| Step: 7
Training loss: 1.6045055389404297
Validation loss: 2.1258070494538996

Epoch: 5| Step: 8
Training loss: 1.871350884437561
Validation loss: 2.1180842063760243

Epoch: 5| Step: 9
Training loss: 2.271909713745117
Validation loss: 2.106185059393606

Epoch: 5| Step: 10
Training loss: 2.310985803604126
Validation loss: 2.1225097230685654

Epoch: 469| Step: 0
Training loss: 2.2298057079315186
Validation loss: 2.095122964151444

Epoch: 5| Step: 1
Training loss: 2.7215442657470703
Validation loss: 2.1030069781887915

Epoch: 5| Step: 2
Training loss: 2.431884765625
Validation loss: 2.088749583049487

Epoch: 5| Step: 3
Training loss: 2.396606922149658
Validation loss: 2.111858106428577

Epoch: 5| Step: 4
Training loss: 1.7790796756744385
Validation loss: 2.1044280247021745

Epoch: 5| Step: 5
Training loss: 1.8314628601074219
Validation loss: 2.1085349385456373

Epoch: 5| Step: 6
Training loss: 1.819791555404663
Validation loss: 2.113305860950101

Epoch: 5| Step: 7
Training loss: 1.9030357599258423
Validation loss: 2.1126851522794334

Epoch: 5| Step: 8
Training loss: 2.1982293128967285
Validation loss: 2.1197688528286514

Epoch: 5| Step: 9
Training loss: 2.0746002197265625
Validation loss: 2.1385805683751262

Epoch: 5| Step: 10
Training loss: 1.9449092149734497
Validation loss: 2.1249504845629454

Epoch: 470| Step: 0
Training loss: 1.9153892993927002
Validation loss: 2.1099792744523738

Epoch: 5| Step: 1
Training loss: 2.11250638961792
Validation loss: 2.1012562885079333

Epoch: 5| Step: 2
Training loss: 1.9733574390411377
Validation loss: 2.1100593997586157

Epoch: 5| Step: 3
Training loss: 2.147968053817749
Validation loss: 2.125095457159063

Epoch: 5| Step: 4
Training loss: 2.434734344482422
Validation loss: 2.1073641571947324

Epoch: 5| Step: 5
Training loss: 2.5334787368774414
Validation loss: 2.116319417953491

Epoch: 5| Step: 6
Training loss: 1.821245789527893
Validation loss: 2.1069318325288835

Epoch: 5| Step: 7
Training loss: 2.012787342071533
Validation loss: 2.101236358765633

Epoch: 5| Step: 8
Training loss: 2.082277774810791
Validation loss: 2.101734294686266

Epoch: 5| Step: 9
Training loss: 1.5914318561553955
Validation loss: 2.1182974871768745

Epoch: 5| Step: 10
Training loss: 2.7289390563964844
Validation loss: 2.1343417988028577

Epoch: 471| Step: 0
Training loss: 2.3456339836120605
Validation loss: 2.14521590868632

Epoch: 5| Step: 1
Training loss: 1.6997015476226807
Validation loss: 2.141172798730994

Epoch: 5| Step: 2
Training loss: 1.9748256206512451
Validation loss: 2.1221046806663595

Epoch: 5| Step: 3
Training loss: 2.410137176513672
Validation loss: 2.1152227617079213

Epoch: 5| Step: 4
Training loss: 2.2889418601989746
Validation loss: 2.111710976528865

Epoch: 5| Step: 5
Training loss: 2.136841297149658
Validation loss: 2.115332168917502

Epoch: 5| Step: 6
Training loss: 2.191257953643799
Validation loss: 2.105608663251323

Epoch: 5| Step: 7
Training loss: 2.545156478881836
Validation loss: 2.0956455571677095

Epoch: 5| Step: 8
Training loss: 2.1156017780303955
Validation loss: 2.1129595566821355

Epoch: 5| Step: 9
Training loss: 1.5088189840316772
Validation loss: 2.095414336009692

Epoch: 5| Step: 10
Training loss: 2.0329158306121826
Validation loss: 2.095449580941149

Epoch: 472| Step: 0
Training loss: 1.8136541843414307
Validation loss: 2.0876260213954474

Epoch: 5| Step: 1
Training loss: 1.8840335607528687
Validation loss: 2.0950202134347733

Epoch: 5| Step: 2
Training loss: 2.430215358734131
Validation loss: 2.1106464888459895

Epoch: 5| Step: 3
Training loss: 2.1243338584899902
Validation loss: 2.118736861854471

Epoch: 5| Step: 4
Training loss: 2.264286994934082
Validation loss: 2.1096417442444833

Epoch: 5| Step: 5
Training loss: 2.579843521118164
Validation loss: 2.1226733115411576

Epoch: 5| Step: 6
Training loss: 1.624193549156189
Validation loss: 2.1042883562785324

Epoch: 5| Step: 7
Training loss: 2.5285897254943848
Validation loss: 2.1090855303631035

Epoch: 5| Step: 8
Training loss: 1.5896260738372803
Validation loss: 2.118540143453947

Epoch: 5| Step: 9
Training loss: 2.462904453277588
Validation loss: 2.1089132126941474

Epoch: 5| Step: 10
Training loss: 1.8782682418823242
Validation loss: 2.127753652552123

Epoch: 473| Step: 0
Training loss: 2.5522913932800293
Validation loss: 2.1154206696377007

Epoch: 5| Step: 1
Training loss: 2.3342156410217285
Validation loss: 2.1151992377414497

Epoch: 5| Step: 2
Training loss: 2.295363426208496
Validation loss: 2.1215552155689528

Epoch: 5| Step: 3
Training loss: 1.7519925832748413
Validation loss: 2.1122196643583235

Epoch: 5| Step: 4
Training loss: 1.946963906288147
Validation loss: 2.0933087564283803

Epoch: 5| Step: 5
Training loss: 2.089055299758911
Validation loss: 2.0985626828285957

Epoch: 5| Step: 6
Training loss: 2.060640811920166
Validation loss: 2.0957577869456303

Epoch: 5| Step: 7
Training loss: 2.357632875442505
Validation loss: 2.091118606187964

Epoch: 5| Step: 8
Training loss: 1.8275728225708008
Validation loss: 2.0914821804210706

Epoch: 5| Step: 9
Training loss: 2.069545030593872
Validation loss: 2.1025771556362027

Epoch: 5| Step: 10
Training loss: 1.7175875902175903
Validation loss: 2.088332786354967

Epoch: 474| Step: 0
Training loss: 2.3377041816711426
Validation loss: 2.1210071297102076

Epoch: 5| Step: 1
Training loss: 2.0351336002349854
Validation loss: 2.1303649089669667

Epoch: 5| Step: 2
Training loss: 1.9458353519439697
Validation loss: 2.14475509940937

Epoch: 5| Step: 3
Training loss: 1.4792068004608154
Validation loss: 2.184106230735779

Epoch: 5| Step: 4
Training loss: 1.9901844263076782
Validation loss: 2.147057079499768

Epoch: 5| Step: 5
Training loss: 2.2182633876800537
Validation loss: 2.130693120341147

Epoch: 5| Step: 6
Training loss: 1.9901864528656006
Validation loss: 2.1175802125725696

Epoch: 5| Step: 7
Training loss: 2.071115016937256
Validation loss: 2.106220071033765

Epoch: 5| Step: 8
Training loss: 2.8417487144470215
Validation loss: 2.1093710686570857

Epoch: 5| Step: 9
Training loss: 2.2871110439300537
Validation loss: 2.0885139767841627

Epoch: 5| Step: 10
Training loss: 1.9121426343917847
Validation loss: 2.0822930541089786

Epoch: 475| Step: 0
Training loss: 1.7575699090957642
Validation loss: 2.086037364057315

Epoch: 5| Step: 1
Training loss: 1.7759491205215454
Validation loss: 2.0785469355121737

Epoch: 5| Step: 2
Training loss: 2.1600334644317627
Validation loss: 2.0857635582647016

Epoch: 5| Step: 3
Training loss: 2.5792689323425293
Validation loss: 2.0846543542800413

Epoch: 5| Step: 4
Training loss: 2.5089409351348877
Validation loss: 2.102131769221316

Epoch: 5| Step: 5
Training loss: 1.933842420578003
Validation loss: 2.0969074810704877

Epoch: 5| Step: 6
Training loss: 2.0087904930114746
Validation loss: 2.115536456467003

Epoch: 5| Step: 7
Training loss: 2.3154754638671875
Validation loss: 2.1136876793317896

Epoch: 5| Step: 8
Training loss: 2.757904052734375
Validation loss: 2.1031952955389537

Epoch: 5| Step: 9
Training loss: 1.9904544353485107
Validation loss: 2.1342842912161224

Epoch: 5| Step: 10
Training loss: 1.1821919679641724
Validation loss: 2.1126636023162515

Epoch: 476| Step: 0
Training loss: 1.8728761672973633
Validation loss: 2.129605257382957

Epoch: 5| Step: 1
Training loss: 1.557716727256775
Validation loss: 2.1427742383813344

Epoch: 5| Step: 2
Training loss: 2.2153000831604004
Validation loss: 2.1397110031497095

Epoch: 5| Step: 3
Training loss: 2.1669065952301025
Validation loss: 2.1430962393360753

Epoch: 5| Step: 4
Training loss: 2.0163474082946777
Validation loss: 2.1453764541174776

Epoch: 5| Step: 5
Training loss: 2.021787405014038
Validation loss: 2.1369385232207594

Epoch: 5| Step: 6
Training loss: 2.428776264190674
Validation loss: 2.1348911100818264

Epoch: 5| Step: 7
Training loss: 2.606450319290161
Validation loss: 2.1024378499677105

Epoch: 5| Step: 8
Training loss: 1.700274109840393
Validation loss: 2.0741429636555333

Epoch: 5| Step: 9
Training loss: 2.6031501293182373
Validation loss: 2.0753908772622385

Epoch: 5| Step: 10
Training loss: 1.8955144882202148
Validation loss: 2.0584883766789592

Epoch: 477| Step: 0
Training loss: 1.8342682123184204
Validation loss: 2.063102552967687

Epoch: 5| Step: 1
Training loss: 1.6598165035247803
Validation loss: 2.0767484070152364

Epoch: 5| Step: 2
Training loss: 2.097135543823242
Validation loss: 2.07246204345457

Epoch: 5| Step: 3
Training loss: 2.5679078102111816
Validation loss: 2.0842514255995392

Epoch: 5| Step: 4
Training loss: 1.9678207635879517
Validation loss: 2.105692767327832

Epoch: 5| Step: 5
Training loss: 1.9028558731079102
Validation loss: 2.11981858745698

Epoch: 5| Step: 6
Training loss: 2.2956762313842773
Validation loss: 2.1194550247602564

Epoch: 5| Step: 7
Training loss: 2.7154452800750732
Validation loss: 2.1186314321333364

Epoch: 5| Step: 8
Training loss: 1.8645260334014893
Validation loss: 2.130328914170624

Epoch: 5| Step: 9
Training loss: 2.332239866256714
Validation loss: 2.144323241326117

Epoch: 5| Step: 10
Training loss: 1.8656415939331055
Validation loss: 2.137097212576097

Epoch: 478| Step: 0
Training loss: 2.461787700653076
Validation loss: 2.143250834557318

Epoch: 5| Step: 1
Training loss: 1.834778070449829
Validation loss: 2.1298182856652046

Epoch: 5| Step: 2
Training loss: 2.552968740463257
Validation loss: 2.1009465622645553

Epoch: 5| Step: 3
Training loss: 2.049898624420166
Validation loss: 2.1069156764655985

Epoch: 5| Step: 4
Training loss: 1.5931036472320557
Validation loss: 2.0982979600147535

Epoch: 5| Step: 5
Training loss: 2.202850818634033
Validation loss: 2.0870854162400767

Epoch: 5| Step: 6
Training loss: 1.9096475839614868
Validation loss: 2.0884545900488414

Epoch: 5| Step: 7
Training loss: 2.1942241191864014
Validation loss: 2.0898933308098906

Epoch: 5| Step: 8
Training loss: 1.3621432781219482
Validation loss: 2.0840418313139226

Epoch: 5| Step: 9
Training loss: 2.6855950355529785
Validation loss: 2.0873405471924813

Epoch: 5| Step: 10
Training loss: 2.187364101409912
Validation loss: 2.083617083487972

Epoch: 479| Step: 0
Training loss: 1.3922569751739502
Validation loss: 2.0967445783717658

Epoch: 5| Step: 1
Training loss: 2.0360045433044434
Validation loss: 2.123852591360769

Epoch: 5| Step: 2
Training loss: 2.2041094303131104
Validation loss: 2.1264761622234056

Epoch: 5| Step: 3
Training loss: 2.0007824897766113
Validation loss: 2.1198478719239593

Epoch: 5| Step: 4
Training loss: 1.4354619979858398
Validation loss: 2.1231503102087204

Epoch: 5| Step: 5
Training loss: 2.1807444095611572
Validation loss: 2.0948121791244834

Epoch: 5| Step: 6
Training loss: 2.406339168548584
Validation loss: 2.1035049756368003

Epoch: 5| Step: 7
Training loss: 2.1878440380096436
Validation loss: 2.111530062972858

Epoch: 5| Step: 8
Training loss: 2.4559144973754883
Validation loss: 2.104555017204695

Epoch: 5| Step: 9
Training loss: 2.1622681617736816
Validation loss: 2.1062502130385368

Epoch: 5| Step: 10
Training loss: 2.6593704223632812
Validation loss: 2.1114970291814497

Epoch: 480| Step: 0
Training loss: 1.6902488470077515
Validation loss: 2.091564416885376

Epoch: 5| Step: 1
Training loss: 2.1731033325195312
Validation loss: 2.0874937477932183

Epoch: 5| Step: 2
Training loss: 2.092503070831299
Validation loss: 2.087475986890895

Epoch: 5| Step: 3
Training loss: 2.2801826000213623
Validation loss: 2.0833770792971373

Epoch: 5| Step: 4
Training loss: 2.5215351581573486
Validation loss: 2.086893763593448

Epoch: 5| Step: 5
Training loss: 2.9116368293762207
Validation loss: 2.102752407391866

Epoch: 5| Step: 6
Training loss: 1.5368258953094482
Validation loss: 2.1080130018213743

Epoch: 5| Step: 7
Training loss: 2.079009532928467
Validation loss: 2.0977547348186536

Epoch: 5| Step: 8
Training loss: 2.531705379486084
Validation loss: 2.1230431628483597

Epoch: 5| Step: 9
Training loss: 1.339590311050415
Validation loss: 2.107386668523153

Epoch: 5| Step: 10
Training loss: 1.8073612451553345
Validation loss: 2.1315969344108336

Epoch: 481| Step: 0
Training loss: 1.8862053155899048
Validation loss: 2.1252663494438253

Epoch: 5| Step: 1
Training loss: 2.5139946937561035
Validation loss: 2.128839031342537

Epoch: 5| Step: 2
Training loss: 1.8452297449111938
Validation loss: 2.1147881630928285

Epoch: 5| Step: 3
Training loss: 0.9375972747802734
Validation loss: 2.115930067595615

Epoch: 5| Step: 4
Training loss: 2.1073784828186035
Validation loss: 2.1207723027916363

Epoch: 5| Step: 5
Training loss: 2.196793794631958
Validation loss: 2.1091639277755574

Epoch: 5| Step: 6
Training loss: 2.6720242500305176
Validation loss: 2.1068616233846194

Epoch: 5| Step: 7
Training loss: 3.027984142303467
Validation loss: 2.100933692788565

Epoch: 5| Step: 8
Training loss: 2.2072603702545166
Validation loss: 2.0930524820922525

Epoch: 5| Step: 9
Training loss: 1.7099082469940186
Validation loss: 2.106668015962006

Epoch: 5| Step: 10
Training loss: 1.8383396863937378
Validation loss: 2.1202802452989804

Epoch: 482| Step: 0
Training loss: 2.109740972518921
Validation loss: 2.103085712719989

Epoch: 5| Step: 1
Training loss: 2.133911609649658
Validation loss: 2.086148945234155

Epoch: 5| Step: 2
Training loss: 1.7635421752929688
Validation loss: 2.0879405839468843

Epoch: 5| Step: 3
Training loss: 2.3979973793029785
Validation loss: 2.1003411264829737

Epoch: 5| Step: 4
Training loss: 2.373866558074951
Validation loss: 2.0878282439324165

Epoch: 5| Step: 5
Training loss: 2.622689723968506
Validation loss: 2.082861674729214

Epoch: 5| Step: 6
Training loss: 1.1377835273742676
Validation loss: 2.093844361202691

Epoch: 5| Step: 7
Training loss: 1.359850287437439
Validation loss: 2.0938246685971498

Epoch: 5| Step: 8
Training loss: 2.4545857906341553
Validation loss: 2.1136812881756852

Epoch: 5| Step: 9
Training loss: 2.4237008094787598
Validation loss: 2.0981728992154522

Epoch: 5| Step: 10
Training loss: 2.1445000171661377
Validation loss: 2.098845298572253

Epoch: 483| Step: 0
Training loss: 2.1053714752197266
Validation loss: 2.1008055569023214

Epoch: 5| Step: 1
Training loss: 2.072234630584717
Validation loss: 2.1067218652335544

Epoch: 5| Step: 2
Training loss: 1.8567256927490234
Validation loss: 2.1104236033654984

Epoch: 5| Step: 3
Training loss: 1.9166736602783203
Validation loss: 2.118318660284883

Epoch: 5| Step: 4
Training loss: 1.5377380847930908
Validation loss: 2.1290443815210813

Epoch: 5| Step: 5
Training loss: 2.394824981689453
Validation loss: 2.122183815125496

Epoch: 5| Step: 6
Training loss: 2.2637932300567627
Validation loss: 2.1101065528008247

Epoch: 5| Step: 7
Training loss: 1.595177412033081
Validation loss: 2.0930960383466495

Epoch: 5| Step: 8
Training loss: 2.5031888484954834
Validation loss: 2.0976227291168703

Epoch: 5| Step: 9
Training loss: 2.0070571899414062
Validation loss: 2.08629213353639

Epoch: 5| Step: 10
Training loss: 2.6342761516571045
Validation loss: 2.07821161003523

Epoch: 484| Step: 0
Training loss: 1.8633785247802734
Validation loss: 2.0765316973450365

Epoch: 5| Step: 1
Training loss: 1.5398179292678833
Validation loss: 2.0741671772413355

Epoch: 5| Step: 2
Training loss: 1.6076338291168213
Validation loss: 2.072631605209843

Epoch: 5| Step: 3
Training loss: 2.275433301925659
Validation loss: 2.0774258029076362

Epoch: 5| Step: 4
Training loss: 2.4319815635681152
Validation loss: 2.0847850153523106

Epoch: 5| Step: 5
Training loss: 2.5841617584228516
Validation loss: 2.0984138032441497

Epoch: 5| Step: 6
Training loss: 1.082554817199707
Validation loss: 2.0780539256270214

Epoch: 5| Step: 7
Training loss: 2.8408851623535156
Validation loss: 2.0897769645978044

Epoch: 5| Step: 8
Training loss: 2.217548131942749
Validation loss: 2.084496064852643

Epoch: 5| Step: 9
Training loss: 2.3436920642852783
Validation loss: 2.0840771993001304

Epoch: 5| Step: 10
Training loss: 1.9794378280639648
Validation loss: 2.098403448699623

Epoch: 485| Step: 0
Training loss: 1.6686201095581055
Validation loss: 2.090550467532168

Epoch: 5| Step: 1
Training loss: 2.2682206630706787
Validation loss: 2.1166943811601207

Epoch: 5| Step: 2
Training loss: 2.1271567344665527
Validation loss: 2.135928446246732

Epoch: 5| Step: 3
Training loss: 2.090705156326294
Validation loss: 2.1631207619943926

Epoch: 5| Step: 4
Training loss: 2.1734166145324707
Validation loss: 2.1512850522994995

Epoch: 5| Step: 5
Training loss: 2.168931484222412
Validation loss: 2.1462251524771414

Epoch: 5| Step: 6
Training loss: 2.1967897415161133
Validation loss: 2.1341161932996524

Epoch: 5| Step: 7
Training loss: 2.298859119415283
Validation loss: 2.131457336487309

Epoch: 5| Step: 8
Training loss: 2.226620674133301
Validation loss: 2.116298947283017

Epoch: 5| Step: 9
Training loss: 1.8474905490875244
Validation loss: 2.1068109286728727

Epoch: 5| Step: 10
Training loss: 1.7303476333618164
Validation loss: 2.0848986615416822

Epoch: 486| Step: 0
Training loss: 1.6860138177871704
Validation loss: 2.075547395213958

Epoch: 5| Step: 1
Training loss: 2.325207233428955
Validation loss: 2.0754373573487803

Epoch: 5| Step: 2
Training loss: 2.059124231338501
Validation loss: 2.066533220711575

Epoch: 5| Step: 3
Training loss: 2.3440616130828857
Validation loss: 2.081686750535042

Epoch: 5| Step: 4
Training loss: 1.5403083562850952
Validation loss: 2.0731047327800463

Epoch: 5| Step: 5
Training loss: 1.881365180015564
Validation loss: 2.085138136340726

Epoch: 5| Step: 6
Training loss: 2.0529866218566895
Validation loss: 2.1016358380676596

Epoch: 5| Step: 7
Training loss: 2.1655449867248535
Validation loss: 2.1097970252395957

Epoch: 5| Step: 8
Training loss: 2.165591239929199
Validation loss: 2.096003552918793

Epoch: 5| Step: 9
Training loss: 2.470385789871216
Validation loss: 2.104882135186144

Epoch: 5| Step: 10
Training loss: 2.2158310413360596
Validation loss: 2.115403949573476

Epoch: 487| Step: 0
Training loss: 2.1520628929138184
Validation loss: 2.1065893788491525

Epoch: 5| Step: 1
Training loss: 2.4525656700134277
Validation loss: 2.132855337153199

Epoch: 5| Step: 2
Training loss: 1.30936598777771
Validation loss: 2.1146377414785404

Epoch: 5| Step: 3
Training loss: 2.4603939056396484
Validation loss: 2.1170867437957437

Epoch: 5| Step: 4
Training loss: 2.1097609996795654
Validation loss: 2.138401521149502

Epoch: 5| Step: 5
Training loss: 1.8602840900421143
Validation loss: 2.1242953013348322

Epoch: 5| Step: 6
Training loss: 1.4009748697280884
Validation loss: 2.132694516130673

Epoch: 5| Step: 7
Training loss: 2.5955448150634766
Validation loss: 2.126339804741644

Epoch: 5| Step: 8
Training loss: 2.423823595046997
Validation loss: 2.097190281396271

Epoch: 5| Step: 9
Training loss: 2.3620564937591553
Validation loss: 2.095317861085297

Epoch: 5| Step: 10
Training loss: 1.5014475584030151
Validation loss: 2.0905563677510908

Epoch: 488| Step: 0
Training loss: 2.40108585357666
Validation loss: 2.1022532832237983

Epoch: 5| Step: 1
Training loss: 1.961742639541626
Validation loss: 2.1093784711694203

Epoch: 5| Step: 2
Training loss: 1.4808032512664795
Validation loss: 2.110728092091058

Epoch: 5| Step: 3
Training loss: 2.1388306617736816
Validation loss: 2.1110945183743715

Epoch: 5| Step: 4
Training loss: 2.3608508110046387
Validation loss: 2.108970683108094

Epoch: 5| Step: 5
Training loss: 2.4292943477630615
Validation loss: 2.1077194521504063

Epoch: 5| Step: 6
Training loss: 2.2057042121887207
Validation loss: 2.0982487932328255

Epoch: 5| Step: 7
Training loss: 1.8934189081192017
Validation loss: 2.1013187118755874

Epoch: 5| Step: 8
Training loss: 2.19472074508667
Validation loss: 2.1022114369177047

Epoch: 5| Step: 9
Training loss: 2.1906135082244873
Validation loss: 2.1262844788130892

Epoch: 5| Step: 10
Training loss: 1.6849853992462158
Validation loss: 2.1228765954253492

Epoch: 489| Step: 0
Training loss: 2.0693435668945312
Validation loss: 2.1126160621643066

Epoch: 5| Step: 1
Training loss: 2.4340806007385254
Validation loss: 2.0978369251374276

Epoch: 5| Step: 2
Training loss: 1.4293615818023682
Validation loss: 2.0858725142735306

Epoch: 5| Step: 3
Training loss: 2.431056499481201
Validation loss: 2.10444127872426

Epoch: 5| Step: 4
Training loss: 2.424506425857544
Validation loss: 2.0683126500857774

Epoch: 5| Step: 5
Training loss: 1.9003651142120361
Validation loss: 2.0796479486650035

Epoch: 5| Step: 6
Training loss: 2.4675240516662598
Validation loss: 2.08084475481382

Epoch: 5| Step: 7
Training loss: 1.5034222602844238
Validation loss: 2.0634688510689685

Epoch: 5| Step: 8
Training loss: 2.2968363761901855
Validation loss: 2.072651101696876

Epoch: 5| Step: 9
Training loss: 1.6818126440048218
Validation loss: 2.0823308742174538

Epoch: 5| Step: 10
Training loss: 2.066688299179077
Validation loss: 2.0900429038591284

Epoch: 490| Step: 0
Training loss: 1.9923746585845947
Validation loss: 2.078023633649272

Epoch: 5| Step: 1
Training loss: 1.618151068687439
Validation loss: 2.08879868958586

Epoch: 5| Step: 2
Training loss: 1.8149744272232056
Validation loss: 2.115218170227543

Epoch: 5| Step: 3
Training loss: 2.112905263900757
Validation loss: 2.138067255737961

Epoch: 5| Step: 4
Training loss: 2.0834527015686035
Validation loss: 2.1303316239387757

Epoch: 5| Step: 5
Training loss: 2.046541929244995
Validation loss: 2.1441279688189105

Epoch: 5| Step: 6
Training loss: 1.9704334735870361
Validation loss: 2.116109367339842

Epoch: 5| Step: 7
Training loss: 2.503715991973877
Validation loss: 2.1048064744600685

Epoch: 5| Step: 8
Training loss: 2.3592867851257324
Validation loss: 2.121651985312021

Epoch: 5| Step: 9
Training loss: 2.023930072784424
Validation loss: 2.113701858828145

Epoch: 5| Step: 10
Training loss: 2.124774932861328
Validation loss: 2.0945599027859267

Epoch: 491| Step: 0
Training loss: 1.54106867313385
Validation loss: 2.102581972716957

Epoch: 5| Step: 1
Training loss: 3.007204532623291
Validation loss: 2.0935037097623272

Epoch: 5| Step: 2
Training loss: 1.9190101623535156
Validation loss: 2.087763694024855

Epoch: 5| Step: 3
Training loss: 2.5548856258392334
Validation loss: 2.078613909341956

Epoch: 5| Step: 4
Training loss: 1.7182508707046509
Validation loss: 2.0757751951935473

Epoch: 5| Step: 5
Training loss: 2.501577854156494
Validation loss: 2.0921974707675237

Epoch: 5| Step: 6
Training loss: 2.056316614151001
Validation loss: 2.0813729352848505

Epoch: 5| Step: 7
Training loss: 1.74932062625885
Validation loss: 2.072926208537112

Epoch: 5| Step: 8
Training loss: 2.7367472648620605
Validation loss: 2.0740894399663454

Epoch: 5| Step: 9
Training loss: 1.4320378303527832
Validation loss: 2.108710606892904

Epoch: 5| Step: 10
Training loss: 1.425915241241455
Validation loss: 2.120797267524145

Epoch: 492| Step: 0
Training loss: 1.8606895208358765
Validation loss: 2.107152351769068

Epoch: 5| Step: 1
Training loss: 1.9296051263809204
Validation loss: 2.1660975102455384

Epoch: 5| Step: 2
Training loss: 2.2458696365356445
Validation loss: 2.174737230423958

Epoch: 5| Step: 3
Training loss: 2.2708821296691895
Validation loss: 2.1760107240369244

Epoch: 5| Step: 4
Training loss: 1.4444105625152588
Validation loss: 2.184005037430794

Epoch: 5| Step: 5
Training loss: 2.6817007064819336
Validation loss: 2.160531482388896

Epoch: 5| Step: 6
Training loss: 2.148616075515747
Validation loss: 2.1558175471521195

Epoch: 5| Step: 7
Training loss: 1.9311012029647827
Validation loss: 2.1175235586781658

Epoch: 5| Step: 8
Training loss: 2.0713958740234375
Validation loss: 2.1030766502503426

Epoch: 5| Step: 9
Training loss: 1.8619499206542969
Validation loss: 2.0914109137750443

Epoch: 5| Step: 10
Training loss: 2.3416643142700195
Validation loss: 2.095713732063129

Epoch: 493| Step: 0
Training loss: 2.859518527984619
Validation loss: 2.098065196826894

Epoch: 5| Step: 1
Training loss: 1.7290832996368408
Validation loss: 2.097497881099742

Epoch: 5| Step: 2
Training loss: 1.650107741355896
Validation loss: 2.075931023525935

Epoch: 5| Step: 3
Training loss: 1.7805039882659912
Validation loss: 2.0652690664414437

Epoch: 5| Step: 4
Training loss: 2.3833959102630615
Validation loss: 2.0884819376853203

Epoch: 5| Step: 5
Training loss: 2.121703863143921
Validation loss: 2.0892926646817114

Epoch: 5| Step: 6
Training loss: 2.0768895149230957
Validation loss: 2.101734863814487

Epoch: 5| Step: 7
Training loss: 2.432448148727417
Validation loss: 2.1468399775925504

Epoch: 5| Step: 8
Training loss: 2.5642054080963135
Validation loss: 2.137954028703833

Epoch: 5| Step: 9
Training loss: 1.32428777217865
Validation loss: 2.120681614004156

Epoch: 5| Step: 10
Training loss: 1.7687162160873413
Validation loss: 2.086504465790205

Epoch: 494| Step: 0
Training loss: 1.9106162786483765
Validation loss: 2.101418960478998

Epoch: 5| Step: 1
Training loss: 1.8378827571868896
Validation loss: 2.100812037785848

Epoch: 5| Step: 2
Training loss: 1.8674023151397705
Validation loss: 2.091463806808636

Epoch: 5| Step: 3
Training loss: 1.6339820623397827
Validation loss: 2.092430142946141

Epoch: 5| Step: 4
Training loss: 2.44889497756958
Validation loss: 2.0817012504864763

Epoch: 5| Step: 5
Training loss: 1.912766695022583
Validation loss: 2.083495106748355

Epoch: 5| Step: 6
Training loss: 2.6686699390411377
Validation loss: 2.090941318901636

Epoch: 5| Step: 7
Training loss: 2.232250213623047
Validation loss: 2.0831255169324976

Epoch: 5| Step: 8
Training loss: 2.2240848541259766
Validation loss: 2.097549920441002

Epoch: 5| Step: 9
Training loss: 2.239220142364502
Validation loss: 2.0581348980626752

Epoch: 5| Step: 10
Training loss: 1.5951896905899048
Validation loss: 2.0838384346295427

Epoch: 495| Step: 0
Training loss: 1.1260513067245483
Validation loss: 2.09273204752194

Epoch: 5| Step: 1
Training loss: 2.1664509773254395
Validation loss: 2.071752440544867

Epoch: 5| Step: 2
Training loss: 1.9932893514633179
Validation loss: 2.0919075960754068

Epoch: 5| Step: 3
Training loss: 2.1673800945281982
Validation loss: 2.092654476883591

Epoch: 5| Step: 4
Training loss: 2.565199375152588
Validation loss: 2.104383061009069

Epoch: 5| Step: 5
Training loss: 2.428131580352783
Validation loss: 2.1172378293929563

Epoch: 5| Step: 6
Training loss: 1.5783987045288086
Validation loss: 2.1018233991438344

Epoch: 5| Step: 7
Training loss: 1.8062654733657837
Validation loss: 2.111467769069056

Epoch: 5| Step: 8
Training loss: 2.1598153114318848
Validation loss: 2.1148334395500923

Epoch: 5| Step: 9
Training loss: 2.6631100177764893
Validation loss: 2.1097756970313286

Epoch: 5| Step: 10
Training loss: 1.7619857788085938
Validation loss: 2.0842738997551704

Epoch: 496| Step: 0
Training loss: 2.5740342140197754
Validation loss: 2.1006056134418776

Epoch: 5| Step: 1
Training loss: 2.633368968963623
Validation loss: 2.1021647607126543

Epoch: 5| Step: 2
Training loss: 1.850184440612793
Validation loss: 2.0942695397202686

Epoch: 5| Step: 3
Training loss: 2.080592393875122
Validation loss: 2.089244586165233

Epoch: 5| Step: 4
Training loss: 1.7791054248809814
Validation loss: 2.085559014351137

Epoch: 5| Step: 5
Training loss: 1.4937950372695923
Validation loss: 2.095344130710889

Epoch: 5| Step: 6
Training loss: 1.6494070291519165
Validation loss: 2.1094057636876262

Epoch: 5| Step: 7
Training loss: 2.3190224170684814
Validation loss: 2.1027097932754026

Epoch: 5| Step: 8
Training loss: 2.0448415279388428
Validation loss: 2.0929410739611556

Epoch: 5| Step: 9
Training loss: 2.0645782947540283
Validation loss: 2.0872175424329695

Epoch: 5| Step: 10
Training loss: 1.8444373607635498
Validation loss: 2.093561532676861

Epoch: 497| Step: 0
Training loss: 1.8803913593292236
Validation loss: 2.0772639141287854

Epoch: 5| Step: 1
Training loss: 2.2859411239624023
Validation loss: 2.0553750889275664

Epoch: 5| Step: 2
Training loss: 1.664002776145935
Validation loss: 2.054757523280318

Epoch: 5| Step: 3
Training loss: 2.178454875946045
Validation loss: 2.0711519436169694

Epoch: 5| Step: 4
Training loss: 1.9697277545928955
Validation loss: 2.0693620533071537

Epoch: 5| Step: 5
Training loss: 2.2342898845672607
Validation loss: 2.076185905805198

Epoch: 5| Step: 6
Training loss: 2.3991355895996094
Validation loss: 2.0790084779903455

Epoch: 5| Step: 7
Training loss: 2.1695244312286377
Validation loss: 2.078029130094795

Epoch: 5| Step: 8
Training loss: 2.037684917449951
Validation loss: 2.1056567827860513

Epoch: 5| Step: 9
Training loss: 1.6056268215179443
Validation loss: 2.1259118100648284

Epoch: 5| Step: 10
Training loss: 1.9595038890838623
Validation loss: 2.1405332755017024

Epoch: 498| Step: 0
Training loss: 2.3661551475524902
Validation loss: 2.1465496196541736

Epoch: 5| Step: 1
Training loss: 1.5806821584701538
Validation loss: 2.13370785661923

Epoch: 5| Step: 2
Training loss: 2.49043869972229
Validation loss: 2.1243271596970095

Epoch: 5| Step: 3
Training loss: 2.5230422019958496
Validation loss: 2.1340280732800885

Epoch: 5| Step: 4
Training loss: 1.7328917980194092
Validation loss: 2.1126269858370543

Epoch: 5| Step: 5
Training loss: 2.2063140869140625
Validation loss: 2.0918095688666067

Epoch: 5| Step: 6
Training loss: 1.6167995929718018
Validation loss: 2.099038288157473

Epoch: 5| Step: 7
Training loss: 1.5400221347808838
Validation loss: 2.088599064016855

Epoch: 5| Step: 8
Training loss: 1.833261489868164
Validation loss: 2.0882777424268824

Epoch: 5| Step: 9
Training loss: 2.398292064666748
Validation loss: 2.0672654182680192

Epoch: 5| Step: 10
Training loss: 2.0354669094085693
Validation loss: 2.0808415823085333

Epoch: 499| Step: 0
Training loss: 1.71584153175354
Validation loss: 2.085685126243099

Epoch: 5| Step: 1
Training loss: 2.7575981616973877
Validation loss: 2.0830635768111034

Epoch: 5| Step: 2
Training loss: 2.2427010536193848
Validation loss: 2.089181166823192

Epoch: 5| Step: 3
Training loss: 2.1967504024505615
Validation loss: 2.094583898462275

Epoch: 5| Step: 4
Training loss: 2.0298843383789062
Validation loss: 2.0882254454397384

Epoch: 5| Step: 5
Training loss: 1.6982558965682983
Validation loss: 2.0994781447995092

Epoch: 5| Step: 6
Training loss: 2.2874999046325684
Validation loss: 2.113418053555232

Epoch: 5| Step: 7
Training loss: 1.434651255607605
Validation loss: 2.1012095315482027

Epoch: 5| Step: 8
Training loss: 1.855984091758728
Validation loss: 2.089147630558219

Epoch: 5| Step: 9
Training loss: 2.2248692512512207
Validation loss: 2.071922994429065

Epoch: 5| Step: 10
Training loss: 1.759808897972107
Validation loss: 2.0678698042387604

Epoch: 500| Step: 0
Training loss: 1.6714502573013306
Validation loss: 2.0639367231758694

Epoch: 5| Step: 1
Training loss: 2.588012218475342
Validation loss: 2.0554392953072824

Epoch: 5| Step: 2
Training loss: 2.0245025157928467
Validation loss: 2.0604479210351103

Epoch: 5| Step: 3
Training loss: 1.4537298679351807
Validation loss: 2.0549300947496967

Epoch: 5| Step: 4
Training loss: 2.3029427528381348
Validation loss: 2.041780816611423

Epoch: 5| Step: 5
Training loss: 2.6457557678222656
Validation loss: 2.0818203226212533

Epoch: 5| Step: 6
Training loss: 2.422919750213623
Validation loss: 2.0959510675040622

Epoch: 5| Step: 7
Training loss: 1.897862195968628
Validation loss: 2.0979361021390526

Epoch: 5| Step: 8
Training loss: 1.7168270349502563
Validation loss: 2.1015764923505884

Epoch: 5| Step: 9
Training loss: 2.0336835384368896
Validation loss: 2.104414070806196

Epoch: 5| Step: 10
Training loss: 1.4897011518478394
Validation loss: 2.098829646264353

Testing loss: 2.2696978648503623

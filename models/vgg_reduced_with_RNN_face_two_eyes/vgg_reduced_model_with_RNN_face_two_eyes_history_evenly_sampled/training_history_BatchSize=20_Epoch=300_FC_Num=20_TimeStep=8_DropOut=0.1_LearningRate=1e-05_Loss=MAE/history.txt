Epoch: 1| Step: 0
Training loss: 5.199960231781006
Validation loss: 5.202322154916743

Epoch: 5| Step: 1
Training loss: 4.704424858093262
Validation loss: 5.195989875383274

Epoch: 5| Step: 2
Training loss: 3.7289955615997314
Validation loss: 5.19033286904776

Epoch: 5| Step: 3
Training loss: 4.961395740509033
Validation loss: 5.184320260119694

Epoch: 5| Step: 4
Training loss: 4.716458797454834
Validation loss: 5.178447895152594

Epoch: 5| Step: 5
Training loss: 4.741961479187012
Validation loss: 5.17184643591604

Epoch: 5| Step: 6
Training loss: 4.6998701095581055
Validation loss: 5.164724734521681

Epoch: 5| Step: 7
Training loss: 5.4141058921813965
Validation loss: 5.157056080397739

Epoch: 5| Step: 8
Training loss: 4.598044395446777
Validation loss: 5.14854625476304

Epoch: 5| Step: 9
Training loss: 6.141299247741699
Validation loss: 5.139131956203009

Epoch: 5| Step: 10
Training loss: 5.839106559753418
Validation loss: 5.129102476181522

Epoch: 2| Step: 0
Training loss: 5.144811630249023
Validation loss: 5.118286281503657

Epoch: 5| Step: 1
Training loss: 5.179490089416504
Validation loss: 5.106510234135453

Epoch: 5| Step: 2
Training loss: 4.632230281829834
Validation loss: 5.094127132046607

Epoch: 5| Step: 3
Training loss: 5.454489707946777
Validation loss: 5.080110324326382

Epoch: 5| Step: 4
Training loss: 5.100325107574463
Validation loss: 5.065575051051314

Epoch: 5| Step: 5
Training loss: 4.711755275726318
Validation loss: 5.048970996692616

Epoch: 5| Step: 6
Training loss: 4.600888252258301
Validation loss: 5.0317805813204854

Epoch: 5| Step: 7
Training loss: 4.59704065322876
Validation loss: 5.01335346570579

Epoch: 5| Step: 8
Training loss: 5.3140130043029785
Validation loss: 4.992942446021623

Epoch: 5| Step: 9
Training loss: 3.7501213550567627
Validation loss: 4.971643565803446

Epoch: 5| Step: 10
Training loss: 4.692953586578369
Validation loss: 4.9483993591800814

Epoch: 3| Step: 0
Training loss: 3.6881637573242188
Validation loss: 4.923883017673288

Epoch: 5| Step: 1
Training loss: 4.901657581329346
Validation loss: 4.897619150018179

Epoch: 5| Step: 2
Training loss: 4.249217510223389
Validation loss: 4.870084911264399

Epoch: 5| Step: 3
Training loss: 5.8441596031188965
Validation loss: 4.840724750231671

Epoch: 5| Step: 4
Training loss: 4.155421733856201
Validation loss: 4.810308066747522

Epoch: 5| Step: 5
Training loss: 3.911329746246338
Validation loss: 4.779328346252441

Epoch: 5| Step: 6
Training loss: 5.072836399078369
Validation loss: 4.745411324244674

Epoch: 5| Step: 7
Training loss: 4.205889701843262
Validation loss: 4.711064528393489

Epoch: 5| Step: 8
Training loss: 4.1104607582092285
Validation loss: 4.677658819383191

Epoch: 5| Step: 9
Training loss: 4.492000579833984
Validation loss: 4.64315318035823

Epoch: 5| Step: 10
Training loss: 5.741298198699951
Validation loss: 4.607843455447946

Epoch: 4| Step: 0
Training loss: 4.567273139953613
Validation loss: 4.571417234277212

Epoch: 5| Step: 1
Training loss: 4.949975490570068
Validation loss: 4.535266312219763

Epoch: 5| Step: 2
Training loss: 4.748396396636963
Validation loss: 4.498129711356214

Epoch: 5| Step: 3
Training loss: 4.365115165710449
Validation loss: 4.461616331531156

Epoch: 5| Step: 4
Training loss: 5.5833916664123535
Validation loss: 4.4237617625985095

Epoch: 5| Step: 5
Training loss: 3.608741044998169
Validation loss: 4.389368857106855

Epoch: 5| Step: 6
Training loss: 3.688253402709961
Validation loss: 4.35668259282266

Epoch: 5| Step: 7
Training loss: 3.658576488494873
Validation loss: 4.324214658429546

Epoch: 5| Step: 8
Training loss: 3.369478940963745
Validation loss: 4.292999439342047

Epoch: 5| Step: 9
Training loss: 3.7837653160095215
Validation loss: 4.26486026599843

Epoch: 5| Step: 10
Training loss: 3.8232061862945557
Validation loss: 4.2365466138368015

Epoch: 5| Step: 0
Training loss: 4.660542011260986
Validation loss: 4.209670579561624

Epoch: 5| Step: 1
Training loss: 3.3022780418395996
Validation loss: 4.184612838170862

Epoch: 5| Step: 2
Training loss: 3.8623290061950684
Validation loss: 4.159794617724675

Epoch: 5| Step: 3
Training loss: 4.3663811683654785
Validation loss: 4.1363720432404545

Epoch: 5| Step: 4
Training loss: 4.696768760681152
Validation loss: 4.1108909371078655

Epoch: 5| Step: 5
Training loss: 3.8033478260040283
Validation loss: 4.089319875163417

Epoch: 5| Step: 6
Training loss: 3.7430663108825684
Validation loss: 4.070782282019175

Epoch: 5| Step: 7
Training loss: 3.9713902473449707
Validation loss: 4.047383662193052

Epoch: 5| Step: 8
Training loss: 3.0871734619140625
Validation loss: 4.030411233184158

Epoch: 5| Step: 9
Training loss: 3.973154067993164
Validation loss: 4.010847696693995

Epoch: 5| Step: 10
Training loss: 3.6949384212493896
Validation loss: 3.9914849932475756

Epoch: 6| Step: 0
Training loss: 4.1329851150512695
Validation loss: 3.970398400419502

Epoch: 5| Step: 1
Training loss: 4.891923427581787
Validation loss: 3.948256492614746

Epoch: 5| Step: 2
Training loss: 4.196176052093506
Validation loss: 3.9257511708044235

Epoch: 5| Step: 3
Training loss: 3.6455636024475098
Validation loss: 3.9032117833373365

Epoch: 5| Step: 4
Training loss: 3.135307788848877
Validation loss: 3.8795874375168995

Epoch: 5| Step: 5
Training loss: 3.4976234436035156
Validation loss: 3.8597155847857074

Epoch: 5| Step: 6
Training loss: 4.621721267700195
Validation loss: 3.838332486409013

Epoch: 5| Step: 7
Training loss: 3.2171473503112793
Validation loss: 3.8140961457324285

Epoch: 5| Step: 8
Training loss: 3.4221091270446777
Validation loss: 3.7882692993328138

Epoch: 5| Step: 9
Training loss: 3.375871181488037
Validation loss: 3.7650481654751684

Epoch: 5| Step: 10
Training loss: 2.840963125228882
Validation loss: 3.7430406796034945

Epoch: 7| Step: 0
Training loss: 3.2472572326660156
Validation loss: 3.7266476615782707

Epoch: 5| Step: 1
Training loss: 3.316556453704834
Validation loss: 3.7087871848895984

Epoch: 5| Step: 2
Training loss: 4.478377342224121
Validation loss: 3.6925147887199157

Epoch: 5| Step: 3
Training loss: 2.9463539123535156
Validation loss: 3.6785614695600284

Epoch: 5| Step: 4
Training loss: 3.7925453186035156
Validation loss: 3.66345977270475

Epoch: 5| Step: 5
Training loss: 3.4013335704803467
Validation loss: 3.652749102602723

Epoch: 5| Step: 6
Training loss: 3.9955203533172607
Validation loss: 3.639961547749017

Epoch: 5| Step: 7
Training loss: 3.3460311889648438
Validation loss: 3.6268367792970393

Epoch: 5| Step: 8
Training loss: 3.7972359657287598
Validation loss: 3.609215105733564

Epoch: 5| Step: 9
Training loss: 3.809767246246338
Validation loss: 3.5955971364052064

Epoch: 5| Step: 10
Training loss: 2.978435754776001
Validation loss: 3.578876628670641

Epoch: 8| Step: 0
Training loss: 3.3592522144317627
Validation loss: 3.563534757142426

Epoch: 5| Step: 1
Training loss: 3.325653553009033
Validation loss: 3.5525828176929104

Epoch: 5| Step: 2
Training loss: 3.1677730083465576
Validation loss: 3.5412119152725383

Epoch: 5| Step: 3
Training loss: 3.707322597503662
Validation loss: 3.530237179930492

Epoch: 5| Step: 4
Training loss: 3.706392765045166
Validation loss: 3.5225131486051824

Epoch: 5| Step: 5
Training loss: 2.9607439041137695
Validation loss: 3.511050457595497

Epoch: 5| Step: 6
Training loss: 3.4958465099334717
Validation loss: 3.50244628742177

Epoch: 5| Step: 7
Training loss: 3.349562406539917
Validation loss: 3.4931997483776462

Epoch: 5| Step: 8
Training loss: 4.927179336547852
Validation loss: 3.489405280800276

Epoch: 5| Step: 9
Training loss: 3.1563339233398438
Validation loss: 3.4783555846060477

Epoch: 5| Step: 10
Training loss: 2.6286168098449707
Validation loss: 3.4700274236740603

Epoch: 9| Step: 0
Training loss: 3.3604495525360107
Validation loss: 3.4668142000834146

Epoch: 5| Step: 1
Training loss: 2.7052159309387207
Validation loss: 3.4580108555414344

Epoch: 5| Step: 2
Training loss: 2.6067893505096436
Validation loss: 3.4489278331879647

Epoch: 5| Step: 3
Training loss: 4.463839530944824
Validation loss: 3.4439366376528175

Epoch: 5| Step: 4
Training loss: 3.0371596813201904
Validation loss: 3.434712356136691

Epoch: 5| Step: 5
Training loss: 3.56758451461792
Validation loss: 3.4298717385979107

Epoch: 5| Step: 6
Training loss: 3.3349502086639404
Validation loss: 3.4233094748630317

Epoch: 5| Step: 7
Training loss: 3.733293056488037
Validation loss: 3.4185121059417725

Epoch: 5| Step: 8
Training loss: 3.5107626914978027
Validation loss: 3.413112255834764

Epoch: 5| Step: 9
Training loss: 3.3527188301086426
Validation loss: 3.4033771535401702

Epoch: 5| Step: 10
Training loss: 3.474409818649292
Validation loss: 3.3985022293624056

Epoch: 10| Step: 0
Training loss: 3.5738348960876465
Validation loss: 3.3938477372610443

Epoch: 5| Step: 1
Training loss: 4.0255022048950195
Validation loss: 3.3834350801283315

Epoch: 5| Step: 2
Training loss: 3.4074301719665527
Validation loss: 3.3754480192738194

Epoch: 5| Step: 3
Training loss: 3.4157211780548096
Validation loss: 3.3773024261638684

Epoch: 5| Step: 4
Training loss: 2.036745071411133
Validation loss: 3.378834088643392

Epoch: 5| Step: 5
Training loss: 4.361125946044922
Validation loss: 3.3670561595629622

Epoch: 5| Step: 6
Training loss: 3.1447408199310303
Validation loss: 3.347901908300256

Epoch: 5| Step: 7
Training loss: 3.445817470550537
Validation loss: 3.348734289087275

Epoch: 5| Step: 8
Training loss: 2.6478283405303955
Validation loss: 3.3608247797976256

Epoch: 5| Step: 9
Training loss: 2.942840099334717
Validation loss: 3.359246248840004

Epoch: 5| Step: 10
Training loss: 3.661611557006836
Validation loss: 3.3537512030652774

Epoch: 11| Step: 0
Training loss: 3.1810665130615234
Validation loss: 3.337066158171623

Epoch: 5| Step: 1
Training loss: 3.2234795093536377
Validation loss: 3.327090532548966

Epoch: 5| Step: 2
Training loss: 2.579822540283203
Validation loss: 3.3231068298380864

Epoch: 5| Step: 3
Training loss: 3.77189564704895
Validation loss: 3.321152840891192

Epoch: 5| Step: 4
Training loss: 3.935553789138794
Validation loss: 3.326161894747006

Epoch: 5| Step: 5
Training loss: 3.1287953853607178
Validation loss: 3.31441742374051

Epoch: 5| Step: 6
Training loss: 3.539755344390869
Validation loss: 3.2999137088816655

Epoch: 5| Step: 7
Training loss: 3.3408515453338623
Validation loss: 3.2946837589304936

Epoch: 5| Step: 8
Training loss: 2.785719633102417
Validation loss: 3.2924188337018414

Epoch: 5| Step: 9
Training loss: 3.280275344848633
Validation loss: 3.2882168292999268

Epoch: 5| Step: 10
Training loss: 3.283217191696167
Validation loss: 3.2862237422697005

Epoch: 12| Step: 0
Training loss: 2.226228713989258
Validation loss: 3.2809549813629477

Epoch: 5| Step: 1
Training loss: 2.724809169769287
Validation loss: 3.271369380335654

Epoch: 5| Step: 2
Training loss: 3.5076756477355957
Validation loss: 3.266565445930727

Epoch: 5| Step: 3
Training loss: 3.29967999458313
Validation loss: 3.265847370188723

Epoch: 5| Step: 4
Training loss: 3.4980673789978027
Validation loss: 3.267260641180059

Epoch: 5| Step: 5
Training loss: 2.6834774017333984
Validation loss: 3.2612908065959973

Epoch: 5| Step: 6
Training loss: 2.888474225997925
Validation loss: 3.2621063288821968

Epoch: 5| Step: 7
Training loss: 3.345909595489502
Validation loss: 3.252596006598524

Epoch: 5| Step: 8
Training loss: 3.9234118461608887
Validation loss: 3.2432037527843187

Epoch: 5| Step: 9
Training loss: 3.7740066051483154
Validation loss: 3.2436322576256207

Epoch: 5| Step: 10
Training loss: 3.855921506881714
Validation loss: 3.2449311030808317

Epoch: 13| Step: 0
Training loss: 3.32197642326355
Validation loss: 3.2408689683483494

Epoch: 5| Step: 1
Training loss: 2.7964205741882324
Validation loss: 3.2315815802543395

Epoch: 5| Step: 2
Training loss: 3.4621996879577637
Validation loss: 3.2260869933712866

Epoch: 5| Step: 3
Training loss: 2.5186586380004883
Validation loss: 3.2262991782157653

Epoch: 5| Step: 4
Training loss: 3.034620761871338
Validation loss: 3.2261449572860554

Epoch: 5| Step: 5
Training loss: 3.865563154220581
Validation loss: 3.2279783602683776

Epoch: 5| Step: 6
Training loss: 2.483118772506714
Validation loss: 3.2180903214280323

Epoch: 5| Step: 7
Training loss: 3.14475417137146
Validation loss: 3.206691829107141

Epoch: 5| Step: 8
Training loss: 3.663804531097412
Validation loss: 3.1995777263436267

Epoch: 5| Step: 9
Training loss: 2.9396278858184814
Validation loss: 3.193847889541298

Epoch: 5| Step: 10
Training loss: 4.221958160400391
Validation loss: 3.1915429945914977

Epoch: 14| Step: 0
Training loss: 2.8936333656311035
Validation loss: 3.1847098309506654

Epoch: 5| Step: 1
Training loss: 3.8758652210235596
Validation loss: 3.18051589176219

Epoch: 5| Step: 2
Training loss: 2.7238216400146484
Validation loss: 3.1765176352634223

Epoch: 5| Step: 3
Training loss: 3.1086411476135254
Validation loss: 3.1694682567350325

Epoch: 5| Step: 4
Training loss: 3.3551247119903564
Validation loss: 3.1657534414722073

Epoch: 5| Step: 5
Training loss: 3.507966995239258
Validation loss: 3.1613747919759443

Epoch: 5| Step: 6
Training loss: 3.3293938636779785
Validation loss: 3.155249987879107

Epoch: 5| Step: 7
Training loss: 2.8637630939483643
Validation loss: 3.147249262820008

Epoch: 5| Step: 8
Training loss: 2.801628351211548
Validation loss: 3.142369075488019

Epoch: 5| Step: 9
Training loss: 3.214289426803589
Validation loss: 3.136566028800062

Epoch: 5| Step: 10
Training loss: 3.195648670196533
Validation loss: 3.1319155154689664

Epoch: 15| Step: 0
Training loss: 2.528165817260742
Validation loss: 3.130692256394253

Epoch: 5| Step: 1
Training loss: 2.6631624698638916
Validation loss: 3.1223541434093187

Epoch: 5| Step: 2
Training loss: 3.264160633087158
Validation loss: 3.1186588169426046

Epoch: 5| Step: 3
Training loss: 3.8368046283721924
Validation loss: 3.116208222604567

Epoch: 5| Step: 4
Training loss: 3.7122700214385986
Validation loss: 3.1099665908403296

Epoch: 5| Step: 5
Training loss: 2.7989211082458496
Validation loss: 3.103983607343448

Epoch: 5| Step: 6
Training loss: 3.2404541969299316
Validation loss: 3.1008877779847834

Epoch: 5| Step: 7
Training loss: 2.706324577331543
Validation loss: 3.0974440318281933

Epoch: 5| Step: 8
Training loss: 3.0662035942077637
Validation loss: 3.0980659377190376

Epoch: 5| Step: 9
Training loss: 4.0724334716796875
Validation loss: 3.091445856196906

Epoch: 5| Step: 10
Training loss: 2.4361565113067627
Validation loss: 3.0851784136987503

Epoch: 16| Step: 0
Training loss: 3.394763231277466
Validation loss: 3.0812119617257068

Epoch: 5| Step: 1
Training loss: 3.3944873809814453
Validation loss: 3.080946430083244

Epoch: 5| Step: 2
Training loss: 3.6398959159851074
Validation loss: 3.0729889946599163

Epoch: 5| Step: 3
Training loss: 3.478283405303955
Validation loss: 3.0684062768054265

Epoch: 5| Step: 4
Training loss: 3.1423563957214355
Validation loss: 3.0638329700757096

Epoch: 5| Step: 5
Training loss: 3.2645092010498047
Validation loss: 3.061788994778869

Epoch: 5| Step: 6
Training loss: 3.5769877433776855
Validation loss: 3.058496839256697

Epoch: 5| Step: 7
Training loss: 2.7392771244049072
Validation loss: 3.054807960346181

Epoch: 5| Step: 8
Training loss: 1.984008550643921
Validation loss: 3.05121946847567

Epoch: 5| Step: 9
Training loss: 2.5303425788879395
Validation loss: 3.04724702014718

Epoch: 5| Step: 10
Training loss: 2.9967429637908936
Validation loss: 3.045589047093545

Epoch: 17| Step: 0
Training loss: 2.0209765434265137
Validation loss: 3.043843315493676

Epoch: 5| Step: 1
Training loss: 2.741008758544922
Validation loss: 3.0497769950538554

Epoch: 5| Step: 2
Training loss: 3.699124813079834
Validation loss: 3.046967183389971

Epoch: 5| Step: 3
Training loss: 3.1753435134887695
Validation loss: 3.0294899376489783

Epoch: 5| Step: 4
Training loss: 3.7895305156707764
Validation loss: 3.03218347795548

Epoch: 5| Step: 5
Training loss: 3.562976360321045
Validation loss: 3.0247061175684773

Epoch: 5| Step: 6
Training loss: 2.85956072807312
Validation loss: 3.018884692140805

Epoch: 5| Step: 7
Training loss: 3.3188958168029785
Validation loss: 3.020492899802423

Epoch: 5| Step: 8
Training loss: 3.396139621734619
Validation loss: 3.0176928556093605

Epoch: 5| Step: 9
Training loss: 2.6296138763427734
Validation loss: 3.0278453057812107

Epoch: 5| Step: 10
Training loss: 2.668679714202881
Validation loss: 3.027444208821943

Epoch: 18| Step: 0
Training loss: 3.2284488677978516
Validation loss: 3.012784696394397

Epoch: 5| Step: 1
Training loss: 3.727109909057617
Validation loss: 3.0064234707945134

Epoch: 5| Step: 2
Training loss: 2.9644389152526855
Validation loss: 3.002776297189856

Epoch: 5| Step: 3
Training loss: 3.1655113697052
Validation loss: 3.0009214032080864

Epoch: 5| Step: 4
Training loss: 2.4773693084716797
Validation loss: 2.997815111631988

Epoch: 5| Step: 5
Training loss: 3.196673631668091
Validation loss: 2.9926141641473256

Epoch: 5| Step: 6
Training loss: 2.4784064292907715
Validation loss: 2.991703450038869

Epoch: 5| Step: 7
Training loss: 2.8537631034851074
Validation loss: 2.990750615314771

Epoch: 5| Step: 8
Training loss: 3.0038657188415527
Validation loss: 2.986197974092217

Epoch: 5| Step: 9
Training loss: 3.1719069480895996
Validation loss: 2.988553762435913

Epoch: 5| Step: 10
Training loss: 3.4906651973724365
Validation loss: 2.9846686957984843

Epoch: 19| Step: 0
Training loss: 3.311328887939453
Validation loss: 2.9809965959159275

Epoch: 5| Step: 1
Training loss: 2.896150827407837
Validation loss: 2.9791769725020214

Epoch: 5| Step: 2
Training loss: 3.7137374877929688
Validation loss: 2.9747933751793316

Epoch: 5| Step: 3
Training loss: 2.3906023502349854
Validation loss: 2.972029916701778

Epoch: 5| Step: 4
Training loss: 3.227524995803833
Validation loss: 2.969564086647444

Epoch: 5| Step: 5
Training loss: 3.9492690563201904
Validation loss: 2.96421831141236

Epoch: 5| Step: 6
Training loss: 3.3605780601501465
Validation loss: 2.962216515694895

Epoch: 5| Step: 7
Training loss: 2.409266710281372
Validation loss: 2.9671970413577173

Epoch: 5| Step: 8
Training loss: 2.1640660762786865
Validation loss: 2.9726653560515373

Epoch: 5| Step: 9
Training loss: 3.288769483566284
Validation loss: 2.9692027876454015

Epoch: 5| Step: 10
Training loss: 2.7177934646606445
Validation loss: 2.9573000605388353

Epoch: 20| Step: 0
Training loss: 3.3467509746551514
Validation loss: 2.9562841589732836

Epoch: 5| Step: 1
Training loss: 3.3827717304229736
Validation loss: 2.957256963176112

Epoch: 5| Step: 2
Training loss: 2.6022677421569824
Validation loss: 2.9506477899448846

Epoch: 5| Step: 3
Training loss: 2.880479574203491
Validation loss: 2.948399051543205

Epoch: 5| Step: 4
Training loss: 2.71769118309021
Validation loss: 2.9480313511304956

Epoch: 5| Step: 5
Training loss: 3.145554542541504
Validation loss: 2.9443533625653995

Epoch: 5| Step: 6
Training loss: 2.754040479660034
Validation loss: 2.94533025064776

Epoch: 5| Step: 7
Training loss: 3.5323195457458496
Validation loss: 2.9428010602151193

Epoch: 5| Step: 8
Training loss: 2.934067487716675
Validation loss: 2.9409231062858336

Epoch: 5| Step: 9
Training loss: 3.481163740158081
Validation loss: 2.936474533491237

Epoch: 5| Step: 10
Training loss: 2.4162025451660156
Validation loss: 2.9340214216580955

Epoch: 21| Step: 0
Training loss: 2.5462284088134766
Validation loss: 2.932435474088115

Epoch: 5| Step: 1
Training loss: 3.3409316539764404
Validation loss: 2.934065306058494

Epoch: 5| Step: 2
Training loss: 2.3563055992126465
Validation loss: 2.929764037491173

Epoch: 5| Step: 3
Training loss: 3.4334168434143066
Validation loss: 2.930589537466726

Epoch: 5| Step: 4
Training loss: 2.640336275100708
Validation loss: 2.926613861514676

Epoch: 5| Step: 5
Training loss: 3.0041286945343018
Validation loss: 2.9228203886298725

Epoch: 5| Step: 6
Training loss: 3.0487122535705566
Validation loss: 2.925240760208458

Epoch: 5| Step: 7
Training loss: 3.9545998573303223
Validation loss: 2.9210925614962013

Epoch: 5| Step: 8
Training loss: 2.8066577911376953
Validation loss: 2.919038895637758

Epoch: 5| Step: 9
Training loss: 3.419644594192505
Validation loss: 2.920128971017817

Epoch: 5| Step: 10
Training loss: 2.542304754257202
Validation loss: 2.9275499236199165

Epoch: 22| Step: 0
Training loss: 3.4634289741516113
Validation loss: 2.9136165342023297

Epoch: 5| Step: 1
Training loss: 2.865769863128662
Validation loss: 2.908962893229659

Epoch: 5| Step: 2
Training loss: 2.8524396419525146
Validation loss: 2.910781355314357

Epoch: 5| Step: 3
Training loss: 3.229776382446289
Validation loss: 2.917995365717078

Epoch: 5| Step: 4
Training loss: 3.4417786598205566
Validation loss: 2.912138967103856

Epoch: 5| Step: 5
Training loss: 2.7531094551086426
Validation loss: 2.906738219722625

Epoch: 5| Step: 6
Training loss: 3.721176862716675
Validation loss: 2.9136060258393646

Epoch: 5| Step: 7
Training loss: 2.8276095390319824
Validation loss: 2.9127535102187947

Epoch: 5| Step: 8
Training loss: 3.009218454360962
Validation loss: 2.9118107159932456

Epoch: 5| Step: 9
Training loss: 2.458533763885498
Validation loss: 2.9008959031874135

Epoch: 5| Step: 10
Training loss: 2.3217785358428955
Validation loss: 2.896509321787024

Epoch: 23| Step: 0
Training loss: 2.9400391578674316
Validation loss: 2.8948150629638345

Epoch: 5| Step: 1
Training loss: 2.321655750274658
Validation loss: 2.8930152718738844

Epoch: 5| Step: 2
Training loss: 3.051126003265381
Validation loss: 2.896764555285054

Epoch: 5| Step: 3
Training loss: 3.4542133808135986
Validation loss: 2.900388471541866

Epoch: 5| Step: 4
Training loss: 2.8607406616210938
Validation loss: 2.9007019586460565

Epoch: 5| Step: 5
Training loss: 2.7640559673309326
Validation loss: 2.9008853614971204

Epoch: 5| Step: 6
Training loss: 2.863241195678711
Validation loss: 2.895185832054384

Epoch: 5| Step: 7
Training loss: 3.1940953731536865
Validation loss: 2.9230374136278705

Epoch: 5| Step: 8
Training loss: 3.128425121307373
Validation loss: 2.8891184996533137

Epoch: 5| Step: 9
Training loss: 3.215178966522217
Validation loss: 2.885872822935863

Epoch: 5| Step: 10
Training loss: 3.1578357219696045
Validation loss: 2.886547229623282

Epoch: 24| Step: 0
Training loss: 4.575521945953369
Validation loss: 2.9011590762804915

Epoch: 5| Step: 1
Training loss: 2.4673819541931152
Validation loss: 2.8951543992565525

Epoch: 5| Step: 2
Training loss: 2.630471706390381
Validation loss: 2.9051592042369228

Epoch: 5| Step: 3
Training loss: 2.6525967121124268
Validation loss: 2.8940388566704205

Epoch: 5| Step: 4
Training loss: 3.9669876098632812
Validation loss: 2.8932252007146038

Epoch: 5| Step: 5
Training loss: 2.2649168968200684
Validation loss: 2.9032311567696194

Epoch: 5| Step: 6
Training loss: 3.3499464988708496
Validation loss: 2.894699499171267

Epoch: 5| Step: 7
Training loss: 2.8524134159088135
Validation loss: 2.890350913488737

Epoch: 5| Step: 8
Training loss: 2.5172641277313232
Validation loss: 2.8869469217074815

Epoch: 5| Step: 9
Training loss: 2.94657301902771
Validation loss: 2.885288200070781

Epoch: 5| Step: 10
Training loss: 2.5786046981811523
Validation loss: 2.882807203518447

Epoch: 25| Step: 0
Training loss: 2.4513614177703857
Validation loss: 2.8810818759343957

Epoch: 5| Step: 1
Training loss: 3.7354559898376465
Validation loss: 2.8835535485257386

Epoch: 5| Step: 2
Training loss: 2.636727809906006
Validation loss: 2.879099251121603

Epoch: 5| Step: 3
Training loss: 2.298496961593628
Validation loss: 2.8750116158557195

Epoch: 5| Step: 4
Training loss: 3.008739948272705
Validation loss: 2.8790405052964405

Epoch: 5| Step: 5
Training loss: 2.9240174293518066
Validation loss: 2.8819255111038045

Epoch: 5| Step: 6
Training loss: 3.225494384765625
Validation loss: 2.876485691275648

Epoch: 5| Step: 7
Training loss: 3.1725802421569824
Validation loss: 2.8769700937373663

Epoch: 5| Step: 8
Training loss: 3.1003775596618652
Validation loss: 2.876308789817236

Epoch: 5| Step: 9
Training loss: 2.4475550651550293
Validation loss: 2.878383723638391

Epoch: 5| Step: 10
Training loss: 3.8796961307525635
Validation loss: 2.880190533976401

Epoch: 26| Step: 0
Training loss: 3.8059933185577393
Validation loss: 2.876515183397519

Epoch: 5| Step: 1
Training loss: 2.4621033668518066
Validation loss: 2.879521736534693

Epoch: 5| Step: 2
Training loss: 2.821828603744507
Validation loss: 2.883681466502528

Epoch: 5| Step: 3
Training loss: 3.249105930328369
Validation loss: 2.8731585112951135

Epoch: 5| Step: 4
Training loss: 2.7821736335754395
Validation loss: 2.8711614634401057

Epoch: 5| Step: 5
Training loss: 2.66373872756958
Validation loss: 2.87109471649252

Epoch: 5| Step: 6
Training loss: 3.1635258197784424
Validation loss: 2.870738096134637

Epoch: 5| Step: 7
Training loss: 2.874110698699951
Validation loss: 2.863101415736701

Epoch: 5| Step: 8
Training loss: 2.4076294898986816
Validation loss: 2.8680826822916665

Epoch: 5| Step: 9
Training loss: 3.0199625492095947
Validation loss: 2.86636681454156

Epoch: 5| Step: 10
Training loss: 3.548189878463745
Validation loss: 2.868599896789879

Epoch: 27| Step: 0
Training loss: 2.1772372722625732
Validation loss: 2.868554138368176

Epoch: 5| Step: 1
Training loss: 3.481316089630127
Validation loss: 2.8734775589358423

Epoch: 5| Step: 2
Training loss: 3.291489362716675
Validation loss: 2.8835831739569224

Epoch: 5| Step: 3
Training loss: 2.7896652221679688
Validation loss: 2.8712146128377607

Epoch: 5| Step: 4
Training loss: 2.3730568885803223
Validation loss: 2.866035117897936

Epoch: 5| Step: 5
Training loss: 3.4063668251037598
Validation loss: 2.8700328385958107

Epoch: 5| Step: 6
Training loss: 3.153858184814453
Validation loss: 2.8703802977838824

Epoch: 5| Step: 7
Training loss: 1.979042410850525
Validation loss: 2.8629090632161787

Epoch: 5| Step: 8
Training loss: 4.175166130065918
Validation loss: 2.862101037015197

Epoch: 5| Step: 9
Training loss: 3.554609775543213
Validation loss: 2.859135180391291

Epoch: 5| Step: 10
Training loss: 2.1329429149627686
Validation loss: 2.860399333379602

Epoch: 28| Step: 0
Training loss: 2.4704158306121826
Validation loss: 2.8630060098504506

Epoch: 5| Step: 1
Training loss: 2.769610643386841
Validation loss: 2.8575982124574724

Epoch: 5| Step: 2
Training loss: 3.071568012237549
Validation loss: 2.8705250165795766

Epoch: 5| Step: 3
Training loss: 2.1878371238708496
Validation loss: 2.8739566956796954

Epoch: 5| Step: 4
Training loss: 3.4381275177001953
Validation loss: 2.8640067038997525

Epoch: 5| Step: 5
Training loss: 3.0625216960906982
Validation loss: 2.858830064855596

Epoch: 5| Step: 6
Training loss: 3.5952377319335938
Validation loss: 2.873319323344897

Epoch: 5| Step: 7
Training loss: 3.150291681289673
Validation loss: 2.881400910756921

Epoch: 5| Step: 8
Training loss: 3.0462300777435303
Validation loss: 2.8628161338067826

Epoch: 5| Step: 9
Training loss: 2.640166759490967
Validation loss: 2.864001263854324

Epoch: 5| Step: 10
Training loss: 3.2017924785614014
Validation loss: 2.882959565808696

Epoch: 29| Step: 0
Training loss: 3.164095401763916
Validation loss: 2.9007898812652915

Epoch: 5| Step: 1
Training loss: 3.378976821899414
Validation loss: 2.9243194057095434

Epoch: 5| Step: 2
Training loss: 2.958591938018799
Validation loss: 2.929632807290682

Epoch: 5| Step: 3
Training loss: 2.822460651397705
Validation loss: 2.969508665864186

Epoch: 5| Step: 4
Training loss: 2.371593952178955
Validation loss: 2.9656928354693997

Epoch: 5| Step: 5
Training loss: 2.6895642280578613
Validation loss: 2.926649001336867

Epoch: 5| Step: 6
Training loss: 2.8868155479431152
Validation loss: 2.8839260250009517

Epoch: 5| Step: 7
Training loss: 3.318784236907959
Validation loss: 2.8593868106924076

Epoch: 5| Step: 8
Training loss: 2.934534788131714
Validation loss: 2.866827436672744

Epoch: 5| Step: 9
Training loss: 3.748347759246826
Validation loss: 2.872011597438525

Epoch: 5| Step: 10
Training loss: 2.4861207008361816
Validation loss: 2.8545844606173936

Epoch: 30| Step: 0
Training loss: 2.6690614223480225
Validation loss: 2.8543527690313195

Epoch: 5| Step: 1
Training loss: 2.7813994884490967
Validation loss: 2.849610492747317

Epoch: 5| Step: 2
Training loss: 2.2902839183807373
Validation loss: 2.8485126213360856

Epoch: 5| Step: 3
Training loss: 3.2990775108337402
Validation loss: 2.853372889180337

Epoch: 5| Step: 4
Training loss: 3.4020886421203613
Validation loss: 2.8635433899459017

Epoch: 5| Step: 5
Training loss: 2.1101138591766357
Validation loss: 2.8702593054822696

Epoch: 5| Step: 6
Training loss: 3.2692787647247314
Validation loss: 2.8549761054336384

Epoch: 5| Step: 7
Training loss: 3.442063093185425
Validation loss: 2.8497657519514843

Epoch: 5| Step: 8
Training loss: 3.4197134971618652
Validation loss: 2.845488166296354

Epoch: 5| Step: 9
Training loss: 2.8776752948760986
Validation loss: 2.839115370986282

Epoch: 5| Step: 10
Training loss: 2.8992321491241455
Validation loss: 2.837907506573585

Epoch: 31| Step: 0
Training loss: 3.151602268218994
Validation loss: 2.8342058479145007

Epoch: 5| Step: 1
Training loss: 3.162238836288452
Validation loss: 2.8337810116429485

Epoch: 5| Step: 2
Training loss: 2.4737486839294434
Validation loss: 2.8300943630997852

Epoch: 5| Step: 3
Training loss: 3.5066077709198
Validation loss: 2.8327542453683834

Epoch: 5| Step: 4
Training loss: 2.585750102996826
Validation loss: 2.830246502353299

Epoch: 5| Step: 5
Training loss: 3.1121015548706055
Validation loss: 2.8248290913079375

Epoch: 5| Step: 6
Training loss: 2.7799456119537354
Validation loss: 2.822490435774608

Epoch: 5| Step: 7
Training loss: 2.31270694732666
Validation loss: 2.819823052293511

Epoch: 5| Step: 8
Training loss: 2.916038990020752
Validation loss: 2.8197821340253277

Epoch: 5| Step: 9
Training loss: 2.8285982608795166
Validation loss: 2.8167518287576656

Epoch: 5| Step: 10
Training loss: 3.6102824211120605
Validation loss: 2.8169909984834733

Epoch: 32| Step: 0
Training loss: 3.2423806190490723
Validation loss: 2.8141907235627532

Epoch: 5| Step: 1
Training loss: 2.139991283416748
Validation loss: 2.8168461322784424

Epoch: 5| Step: 2
Training loss: 2.477184295654297
Validation loss: 2.8187970679293395

Epoch: 5| Step: 3
Training loss: 3.735734462738037
Validation loss: 2.8159611455855833

Epoch: 5| Step: 4
Training loss: 2.8777804374694824
Validation loss: 2.8105016677610335

Epoch: 5| Step: 5
Training loss: 2.843632221221924
Validation loss: 2.81344856498062

Epoch: 5| Step: 6
Training loss: 2.6483235359191895
Validation loss: 2.8102357413179133

Epoch: 5| Step: 7
Training loss: 3.011317729949951
Validation loss: 2.818499393360589

Epoch: 5| Step: 8
Training loss: 3.2186050415039062
Validation loss: 2.8130679848373576

Epoch: 5| Step: 9
Training loss: 3.3406918048858643
Validation loss: 2.8087791935090096

Epoch: 5| Step: 10
Training loss: 2.598355531692505
Validation loss: 2.8075905282010316

Epoch: 33| Step: 0
Training loss: 2.7595620155334473
Validation loss: 2.811015570035545

Epoch: 5| Step: 1
Training loss: 2.3139584064483643
Validation loss: 2.811366529874904

Epoch: 5| Step: 2
Training loss: 2.176299571990967
Validation loss: 2.8135320653197584

Epoch: 5| Step: 3
Training loss: 3.1649956703186035
Validation loss: 2.8148569958184355

Epoch: 5| Step: 4
Training loss: 2.8958675861358643
Validation loss: 2.8323321419377483

Epoch: 5| Step: 5
Training loss: 3.636653184890747
Validation loss: 2.8277215162913003

Epoch: 5| Step: 6
Training loss: 4.089158058166504
Validation loss: 2.8265617675678705

Epoch: 5| Step: 7
Training loss: 3.088670253753662
Validation loss: 2.8160108468865834

Epoch: 5| Step: 8
Training loss: 2.1240921020507812
Validation loss: 2.804363675014947

Epoch: 5| Step: 9
Training loss: 2.7600109577178955
Validation loss: 2.8023578838635514

Epoch: 5| Step: 10
Training loss: 3.2460570335388184
Validation loss: 2.8012414286213536

Epoch: 34| Step: 0
Training loss: 2.614109754562378
Validation loss: 2.8066971404578096

Epoch: 5| Step: 1
Training loss: 3.1870594024658203
Validation loss: 2.807440583423902

Epoch: 5| Step: 2
Training loss: 3.171416759490967
Validation loss: 2.798803937050604

Epoch: 5| Step: 3
Training loss: 2.9702024459838867
Validation loss: 2.7974205299090316

Epoch: 5| Step: 4
Training loss: 2.850191831588745
Validation loss: 2.7971240679423013

Epoch: 5| Step: 5
Training loss: 2.9150567054748535
Validation loss: 2.8037848703322874

Epoch: 5| Step: 6
Training loss: 3.114053726196289
Validation loss: 2.810987046969834

Epoch: 5| Step: 7
Training loss: 2.839322566986084
Validation loss: 2.8145689656657558

Epoch: 5| Step: 8
Training loss: 2.8147025108337402
Validation loss: 2.8231457907666444

Epoch: 5| Step: 9
Training loss: 2.9841058254241943
Validation loss: 2.8332557678222656

Epoch: 5| Step: 10
Training loss: 2.6567487716674805
Validation loss: 2.8543974661057994

Epoch: 35| Step: 0
Training loss: 2.810375928878784
Validation loss: 2.8640082113204466

Epoch: 5| Step: 1
Training loss: 2.532825469970703
Validation loss: 2.8493165046938005

Epoch: 5| Step: 2
Training loss: 2.8288650512695312
Validation loss: 2.8285074182735976

Epoch: 5| Step: 3
Training loss: 2.506347179412842
Validation loss: 2.8114287853240967

Epoch: 5| Step: 4
Training loss: 2.768359422683716
Validation loss: 2.802764654159546

Epoch: 5| Step: 5
Training loss: 3.3659374713897705
Validation loss: 2.7921498155081146

Epoch: 5| Step: 6
Training loss: 2.6623313426971436
Validation loss: 2.7868830106591664

Epoch: 5| Step: 7
Training loss: 2.9059512615203857
Validation loss: 2.7883950971787974

Epoch: 5| Step: 8
Training loss: 3.339398145675659
Validation loss: 2.7875972922130297

Epoch: 5| Step: 9
Training loss: 3.666339874267578
Validation loss: 2.7954886805626655

Epoch: 5| Step: 10
Training loss: 2.6414711475372314
Validation loss: 2.7985724121011715

Epoch: 36| Step: 0
Training loss: 2.306824207305908
Validation loss: 2.7921128067919003

Epoch: 5| Step: 1
Training loss: 2.7916932106018066
Validation loss: 2.786852741754183

Epoch: 5| Step: 2
Training loss: 3.271103620529175
Validation loss: 2.78691545865869

Epoch: 5| Step: 3
Training loss: 2.7909131050109863
Validation loss: 2.7878457448815785

Epoch: 5| Step: 4
Training loss: 2.803645610809326
Validation loss: 2.787618501212007

Epoch: 5| Step: 5
Training loss: 2.240906238555908
Validation loss: 2.7957156012135167

Epoch: 5| Step: 6
Training loss: 3.5510246753692627
Validation loss: 2.8021989458350727

Epoch: 5| Step: 7
Training loss: 3.092433214187622
Validation loss: 2.8037105350084204

Epoch: 5| Step: 8
Training loss: 2.867211103439331
Validation loss: 2.808029720860143

Epoch: 5| Step: 9
Training loss: 3.0718672275543213
Validation loss: 2.800646771666824

Epoch: 5| Step: 10
Training loss: 3.244123935699463
Validation loss: 2.7967758127438125

Epoch: 37| Step: 0
Training loss: 2.8253676891326904
Validation loss: 2.790276681223223

Epoch: 5| Step: 1
Training loss: 2.548384666442871
Validation loss: 2.783722480138143

Epoch: 5| Step: 2
Training loss: 2.7773988246917725
Validation loss: 2.7824940835275958

Epoch: 5| Step: 3
Training loss: 3.730675458908081
Validation loss: 2.7852558038567983

Epoch: 5| Step: 4
Training loss: 2.322305202484131
Validation loss: 2.7889912077175674

Epoch: 5| Step: 5
Training loss: 2.402015209197998
Validation loss: 2.79419723890161

Epoch: 5| Step: 6
Training loss: 3.4939627647399902
Validation loss: 2.805206083482312

Epoch: 5| Step: 7
Training loss: 2.673485517501831
Validation loss: 2.794163147608439

Epoch: 5| Step: 8
Training loss: 2.6442744731903076
Validation loss: 2.782057349399854

Epoch: 5| Step: 9
Training loss: 3.5591094493865967
Validation loss: 2.7757930371069137

Epoch: 5| Step: 10
Training loss: 2.996899366378784
Validation loss: 2.7839362185488463

Epoch: 38| Step: 0
Training loss: 2.6433539390563965
Validation loss: 2.787784420033937

Epoch: 5| Step: 1
Training loss: 3.6027958393096924
Validation loss: 2.8005328460406234

Epoch: 5| Step: 2
Training loss: 3.9740042686462402
Validation loss: 2.80586160895645

Epoch: 5| Step: 3
Training loss: 2.2562613487243652
Validation loss: 2.8204917061713433

Epoch: 5| Step: 4
Training loss: 3.0491714477539062
Validation loss: 2.8409856596300678

Epoch: 5| Step: 5
Training loss: 2.5490739345550537
Validation loss: 2.8150981677475797

Epoch: 5| Step: 6
Training loss: 3.277191162109375
Validation loss: 2.8010126134400726

Epoch: 5| Step: 7
Training loss: 2.293713331222534
Validation loss: 2.7814673044348277

Epoch: 5| Step: 8
Training loss: 2.6199517250061035
Validation loss: 2.7840782288582093

Epoch: 5| Step: 9
Training loss: 2.7850160598754883
Validation loss: 2.7766746167213685

Epoch: 5| Step: 10
Training loss: 2.9840176105499268
Validation loss: 2.7812930614717546

Epoch: 39| Step: 0
Training loss: 2.580998182296753
Validation loss: 2.776643637687929

Epoch: 5| Step: 1
Training loss: 2.949392080307007
Validation loss: 2.776770181553338

Epoch: 5| Step: 2
Training loss: 2.879539728164673
Validation loss: 2.774150263878607

Epoch: 5| Step: 3
Training loss: 2.364039421081543
Validation loss: 2.774642167552825

Epoch: 5| Step: 4
Training loss: 3.567476272583008
Validation loss: 2.7784700137312695

Epoch: 5| Step: 5
Training loss: 2.600259780883789
Validation loss: 2.771435670955207

Epoch: 5| Step: 6
Training loss: 2.685046911239624
Validation loss: 2.77252515926156

Epoch: 5| Step: 7
Training loss: 3.3201587200164795
Validation loss: 2.767559128422891

Epoch: 5| Step: 8
Training loss: 3.099125385284424
Validation loss: 2.768958471154654

Epoch: 5| Step: 9
Training loss: 2.512852191925049
Validation loss: 2.7681635195209133

Epoch: 5| Step: 10
Training loss: 3.3523473739624023
Validation loss: 2.772725938468851

Epoch: 40| Step: 0
Training loss: 3.006366729736328
Validation loss: 2.772547132225447

Epoch: 5| Step: 1
Training loss: 3.340380907058716
Validation loss: 2.780773642242596

Epoch: 5| Step: 2
Training loss: 2.4814629554748535
Validation loss: 2.7866485862321753

Epoch: 5| Step: 3
Training loss: 2.908845901489258
Validation loss: 2.799805018209642

Epoch: 5| Step: 4
Training loss: 2.5139288902282715
Validation loss: 2.8003843163931244

Epoch: 5| Step: 5
Training loss: 2.7301619052886963
Validation loss: 2.800018997602565

Epoch: 5| Step: 6
Training loss: 3.2575366497039795
Validation loss: 2.807325452886602

Epoch: 5| Step: 7
Training loss: 3.6802475452423096
Validation loss: 2.787522962016444

Epoch: 5| Step: 8
Training loss: 2.887319326400757
Validation loss: 2.7854766922612346

Epoch: 5| Step: 9
Training loss: 2.4991230964660645
Validation loss: 2.7821156850425144

Epoch: 5| Step: 10
Training loss: 2.5476272106170654
Validation loss: 2.7800352547758367

Epoch: 41| Step: 0
Training loss: 3.2739334106445312
Validation loss: 2.781861138600175

Epoch: 5| Step: 1
Training loss: 3.419293165206909
Validation loss: 2.779246876316686

Epoch: 5| Step: 2
Training loss: 2.6404008865356445
Validation loss: 2.778938913858065

Epoch: 5| Step: 3
Training loss: 3.718113660812378
Validation loss: 2.772330322573262

Epoch: 5| Step: 4
Training loss: 2.7133865356445312
Validation loss: 2.7749188612866145

Epoch: 5| Step: 5
Training loss: 2.753030300140381
Validation loss: 2.7808372641122467

Epoch: 5| Step: 6
Training loss: 2.4849750995635986
Validation loss: 2.780672014400523

Epoch: 5| Step: 7
Training loss: 2.3195853233337402
Validation loss: 2.774647133324736

Epoch: 5| Step: 8
Training loss: 2.3616859912872314
Validation loss: 2.7857654735606205

Epoch: 5| Step: 9
Training loss: 3.053342819213867
Validation loss: 2.7838718609143327

Epoch: 5| Step: 10
Training loss: 3.059966564178467
Validation loss: 2.7987921507127824

Epoch: 42| Step: 0
Training loss: 3.7496542930603027
Validation loss: 2.803923991418654

Epoch: 5| Step: 1
Training loss: 2.744441509246826
Validation loss: 2.7739553092628397

Epoch: 5| Step: 2
Training loss: 2.361543893814087
Validation loss: 2.767840567455497

Epoch: 5| Step: 3
Training loss: 3.2441093921661377
Validation loss: 2.76618073063512

Epoch: 5| Step: 4
Training loss: 2.293424606323242
Validation loss: 2.7682430872353176

Epoch: 5| Step: 5
Training loss: 3.504474639892578
Validation loss: 2.76906390600307

Epoch: 5| Step: 6
Training loss: 3.7273826599121094
Validation loss: 2.7701684749254616

Epoch: 5| Step: 7
Training loss: 3.031345844268799
Validation loss: 2.7640137877515567

Epoch: 5| Step: 8
Training loss: 1.8791038990020752
Validation loss: 2.7597210509802705

Epoch: 5| Step: 9
Training loss: 3.118098735809326
Validation loss: 2.7632066588247977

Epoch: 5| Step: 10
Training loss: 2.052633285522461
Validation loss: 2.762640663372573

Epoch: 43| Step: 0
Training loss: 2.3742518424987793
Validation loss: 2.764374630425566

Epoch: 5| Step: 1
Training loss: 2.90832257270813
Validation loss: 2.7664763645459245

Epoch: 5| Step: 2
Training loss: 3.0567784309387207
Validation loss: 2.7740152958900697

Epoch: 5| Step: 3
Training loss: 3.000757932662964
Validation loss: 2.775084057161885

Epoch: 5| Step: 4
Training loss: 3.489943027496338
Validation loss: 2.773468699506534

Epoch: 5| Step: 5
Training loss: 2.4962220191955566
Validation loss: 2.7744093889831216

Epoch: 5| Step: 6
Training loss: 2.421597719192505
Validation loss: 2.78535028683242

Epoch: 5| Step: 7
Training loss: 3.397704601287842
Validation loss: 2.7871719252678657

Epoch: 5| Step: 8
Training loss: 2.7001099586486816
Validation loss: 2.781371416584138

Epoch: 5| Step: 9
Training loss: 2.9171669483184814
Validation loss: 2.781389977342339

Epoch: 5| Step: 10
Training loss: 2.919968366622925
Validation loss: 2.7700140860772904

Epoch: 44| Step: 0
Training loss: 2.0778019428253174
Validation loss: 2.7610560617139264

Epoch: 5| Step: 1
Training loss: 2.5418717861175537
Validation loss: 2.7593596673780874

Epoch: 5| Step: 2
Training loss: 3.271219253540039
Validation loss: 2.756313262447234

Epoch: 5| Step: 3
Training loss: 2.8184731006622314
Validation loss: 2.7569038483404342

Epoch: 5| Step: 4
Training loss: 3.716994524002075
Validation loss: 2.757373648305093

Epoch: 5| Step: 5
Training loss: 2.343693971633911
Validation loss: 2.75353507585423

Epoch: 5| Step: 6
Training loss: 2.9187073707580566
Validation loss: 2.751054758666664

Epoch: 5| Step: 7
Training loss: 3.2724528312683105
Validation loss: 2.7519488411564983

Epoch: 5| Step: 8
Training loss: 2.967769145965576
Validation loss: 2.752710775662494

Epoch: 5| Step: 9
Training loss: 2.9062297344207764
Validation loss: 2.751879225495041

Epoch: 5| Step: 10
Training loss: 2.7854154109954834
Validation loss: 2.7520730956908195

Epoch: 45| Step: 0
Training loss: 2.755830764770508
Validation loss: 2.7503392452834756

Epoch: 5| Step: 1
Training loss: 2.832327365875244
Validation loss: 2.749546904717722

Epoch: 5| Step: 2
Training loss: 3.2976531982421875
Validation loss: 2.7495757764385593

Epoch: 5| Step: 3
Training loss: 2.7747504711151123
Validation loss: 2.75075513316739

Epoch: 5| Step: 4
Training loss: 1.6461498737335205
Validation loss: 2.7471645339842765

Epoch: 5| Step: 5
Training loss: 2.8681952953338623
Validation loss: 2.749246661381055

Epoch: 5| Step: 6
Training loss: 2.6817781925201416
Validation loss: 2.747817665018061

Epoch: 5| Step: 7
Training loss: 3.799307346343994
Validation loss: 2.748096578864641

Epoch: 5| Step: 8
Training loss: 2.79235577583313
Validation loss: 2.7446315109088855

Epoch: 5| Step: 9
Training loss: 3.5416228771209717
Validation loss: 2.744932759192682

Epoch: 5| Step: 10
Training loss: 2.515981435775757
Validation loss: 2.7429308404204664

Epoch: 46| Step: 0
Training loss: 2.9883289337158203
Validation loss: 2.7458242831691617

Epoch: 5| Step: 1
Training loss: 2.5344505310058594
Validation loss: 2.7476825867929766

Epoch: 5| Step: 2
Training loss: 2.784536361694336
Validation loss: 2.751536535960372

Epoch: 5| Step: 3
Training loss: 3.3098435401916504
Validation loss: 2.7470491265737884

Epoch: 5| Step: 4
Training loss: 2.7157254219055176
Validation loss: 2.7385426541810394

Epoch: 5| Step: 5
Training loss: 3.325812578201294
Validation loss: 2.7411548194064888

Epoch: 5| Step: 6
Training loss: 3.175708293914795
Validation loss: 2.7453203560203634

Epoch: 5| Step: 7
Training loss: 2.563291549682617
Validation loss: 2.7399168578527306

Epoch: 5| Step: 8
Training loss: 2.3093154430389404
Validation loss: 2.753669511887335

Epoch: 5| Step: 9
Training loss: 3.2051596641540527
Validation loss: 2.751043128710921

Epoch: 5| Step: 10
Training loss: 2.559927463531494
Validation loss: 2.755216219091928

Epoch: 47| Step: 0
Training loss: 2.463901996612549
Validation loss: 2.748503674742996

Epoch: 5| Step: 1
Training loss: 2.3739922046661377
Validation loss: 2.7558381506191787

Epoch: 5| Step: 2
Training loss: 2.7035348415374756
Validation loss: 2.7576099980262017

Epoch: 5| Step: 3
Training loss: 2.609590530395508
Validation loss: 2.752774028367894

Epoch: 5| Step: 4
Training loss: 2.924415111541748
Validation loss: 2.7581328525338122

Epoch: 5| Step: 5
Training loss: 2.813960552215576
Validation loss: 2.762284509597286

Epoch: 5| Step: 6
Training loss: 3.0899899005889893
Validation loss: 2.753030479595225

Epoch: 5| Step: 7
Training loss: 3.6811649799346924
Validation loss: 2.755454347979638

Epoch: 5| Step: 8
Training loss: 2.3571581840515137
Validation loss: 2.7457108882165726

Epoch: 5| Step: 9
Training loss: 3.448622465133667
Validation loss: 2.735987255650182

Epoch: 5| Step: 10
Training loss: 2.991292715072632
Validation loss: 2.7335412015197096

Epoch: 48| Step: 0
Training loss: 2.4417526721954346
Validation loss: 2.7404356156626055

Epoch: 5| Step: 1
Training loss: 3.078368663787842
Validation loss: 2.742821983111802

Epoch: 5| Step: 2
Training loss: 3.044969081878662
Validation loss: 2.751283517447851

Epoch: 5| Step: 3
Training loss: 1.9182727336883545
Validation loss: 2.757422242113339

Epoch: 5| Step: 4
Training loss: 2.3738620281219482
Validation loss: 2.772912650979975

Epoch: 5| Step: 5
Training loss: 3.2822933197021484
Validation loss: 2.758665853931058

Epoch: 5| Step: 6
Training loss: 2.281999111175537
Validation loss: 2.7497142822511735

Epoch: 5| Step: 7
Training loss: 3.1558730602264404
Validation loss: 2.7422906608991724

Epoch: 5| Step: 8
Training loss: 2.9906675815582275
Validation loss: 2.734695339715609

Epoch: 5| Step: 9
Training loss: 3.37994647026062
Validation loss: 2.7329619546090402

Epoch: 5| Step: 10
Training loss: 3.668297052383423
Validation loss: 2.731287940855949

Epoch: 49| Step: 0
Training loss: 2.637228488922119
Validation loss: 2.7334964864997455

Epoch: 5| Step: 1
Training loss: 2.7690441608428955
Validation loss: 2.7359517620455835

Epoch: 5| Step: 2
Training loss: 3.1652140617370605
Validation loss: 2.7345030666679464

Epoch: 5| Step: 3
Training loss: 2.4038257598876953
Validation loss: 2.737361784904234

Epoch: 5| Step: 4
Training loss: 3.099768877029419
Validation loss: 2.7343550933304654

Epoch: 5| Step: 5
Training loss: 2.390913248062134
Validation loss: 2.745690397036973

Epoch: 5| Step: 6
Training loss: 3.179556369781494
Validation loss: 2.739350744473037

Epoch: 5| Step: 7
Training loss: 3.3810741901397705
Validation loss: 2.7352557156675603

Epoch: 5| Step: 8
Training loss: 3.3252015113830566
Validation loss: 2.738457392620784

Epoch: 5| Step: 9
Training loss: 3.0712294578552246
Validation loss: 2.7401156220384824

Epoch: 5| Step: 10
Training loss: 1.774113655090332
Validation loss: 2.737841421558011

Epoch: 50| Step: 0
Training loss: 2.5492477416992188
Validation loss: 2.7315258825978925

Epoch: 5| Step: 1
Training loss: 2.754249334335327
Validation loss: 2.7286953644085954

Epoch: 5| Step: 2
Training loss: 2.862574815750122
Validation loss: 2.7263437035263225

Epoch: 5| Step: 3
Training loss: 2.806854724884033
Validation loss: 2.724043494911604

Epoch: 5| Step: 4
Training loss: 2.851935386657715
Validation loss: 2.7187427372060795

Epoch: 5| Step: 5
Training loss: 3.207162380218506
Validation loss: 2.72177799799109

Epoch: 5| Step: 6
Training loss: 2.596961498260498
Validation loss: 2.7182227796123875

Epoch: 5| Step: 7
Training loss: 2.4088780879974365
Validation loss: 2.7208746633222027

Epoch: 5| Step: 8
Training loss: 3.5034542083740234
Validation loss: 2.720564050059165

Epoch: 5| Step: 9
Training loss: 3.106409788131714
Validation loss: 2.722280240827991

Epoch: 5| Step: 10
Training loss: 2.6405904293060303
Validation loss: 2.72232803990764

Epoch: 51| Step: 0
Training loss: 2.4974961280822754
Validation loss: 2.7222411324900966

Epoch: 5| Step: 1
Training loss: 2.6243557929992676
Validation loss: 2.7183850529373332

Epoch: 5| Step: 2
Training loss: 3.056990385055542
Validation loss: 2.717748188203381

Epoch: 5| Step: 3
Training loss: 3.1168646812438965
Validation loss: 2.7181858888236423

Epoch: 5| Step: 4
Training loss: 3.061826229095459
Validation loss: 2.724229099929974

Epoch: 5| Step: 5
Training loss: 2.138702154159546
Validation loss: 2.731139031789636

Epoch: 5| Step: 6
Training loss: 3.5122268199920654
Validation loss: 2.753275853331371

Epoch: 5| Step: 7
Training loss: 2.8550076484680176
Validation loss: 2.7455122547764934

Epoch: 5| Step: 8
Training loss: 2.3365261554718018
Validation loss: 2.7340738927164385

Epoch: 5| Step: 9
Training loss: 3.4239463806152344
Validation loss: 2.733888969626478

Epoch: 5| Step: 10
Training loss: 2.750687599182129
Validation loss: 2.732909676849201

Epoch: 52| Step: 0
Training loss: 2.0740184783935547
Validation loss: 2.7245900554041707

Epoch: 5| Step: 1
Training loss: 3.6613032817840576
Validation loss: 2.7221286194298857

Epoch: 5| Step: 2
Training loss: 3.2211966514587402
Validation loss: 2.7242609198375414

Epoch: 5| Step: 3
Training loss: 2.523167371749878
Validation loss: 2.7295096612745717

Epoch: 5| Step: 4
Training loss: 2.613290548324585
Validation loss: 2.7420981263601654

Epoch: 5| Step: 5
Training loss: 2.7212226390838623
Validation loss: 2.761427851133449

Epoch: 5| Step: 6
Training loss: 3.1798198223114014
Validation loss: 2.772406936973654

Epoch: 5| Step: 7
Training loss: 2.809962749481201
Validation loss: 2.7592170879405034

Epoch: 5| Step: 8
Training loss: 2.811305284500122
Validation loss: 2.7336189259764967

Epoch: 5| Step: 9
Training loss: 2.840831995010376
Validation loss: 2.7193281342906337

Epoch: 5| Step: 10
Training loss: 2.918205499649048
Validation loss: 2.7191029774245394

Epoch: 53| Step: 0
Training loss: 2.5166587829589844
Validation loss: 2.7203993515301774

Epoch: 5| Step: 1
Training loss: 2.316005229949951
Validation loss: 2.720243694961712

Epoch: 5| Step: 2
Training loss: 2.791604995727539
Validation loss: 2.725083358826176

Epoch: 5| Step: 3
Training loss: 2.633368968963623
Validation loss: 2.731352734309371

Epoch: 5| Step: 4
Training loss: 3.1608364582061768
Validation loss: 2.7249019017783542

Epoch: 5| Step: 5
Training loss: 3.1615078449249268
Validation loss: 2.726962927849062

Epoch: 5| Step: 6
Training loss: 3.649980068206787
Validation loss: 2.7268370351483746

Epoch: 5| Step: 7
Training loss: 2.285557746887207
Validation loss: 2.730093233046993

Epoch: 5| Step: 8
Training loss: 3.134716033935547
Validation loss: 2.721765682261477

Epoch: 5| Step: 9
Training loss: 2.6564619541168213
Validation loss: 2.712338614207442

Epoch: 5| Step: 10
Training loss: 3.0390100479125977
Validation loss: 2.7169042582152994

Epoch: 54| Step: 0
Training loss: 1.9935710430145264
Validation loss: 2.711911227113457

Epoch: 5| Step: 1
Training loss: 2.563692092895508
Validation loss: 2.7089551700058805

Epoch: 5| Step: 2
Training loss: 2.759286642074585
Validation loss: 2.711570747437016

Epoch: 5| Step: 3
Training loss: 2.207855701446533
Validation loss: 2.7095110057502665

Epoch: 5| Step: 4
Training loss: 2.642390727996826
Validation loss: 2.709546189154348

Epoch: 5| Step: 5
Training loss: 2.2965564727783203
Validation loss: 2.705480606325211

Epoch: 5| Step: 6
Training loss: 3.597226619720459
Validation loss: 2.7079173723856607

Epoch: 5| Step: 7
Training loss: 3.2274887561798096
Validation loss: 2.7136067651933238

Epoch: 5| Step: 8
Training loss: 3.1483705043792725
Validation loss: 2.714525520160634

Epoch: 5| Step: 9
Training loss: 3.43076753616333
Validation loss: 2.7090853337318666

Epoch: 5| Step: 10
Training loss: 3.4121155738830566
Validation loss: 2.702012377400552

Epoch: 55| Step: 0
Training loss: 2.967266798019409
Validation loss: 2.707793148615027

Epoch: 5| Step: 1
Training loss: 3.233701229095459
Validation loss: 2.711750094608594

Epoch: 5| Step: 2
Training loss: 2.6363720893859863
Validation loss: 2.7109288656583397

Epoch: 5| Step: 3
Training loss: 3.2283005714416504
Validation loss: 2.7141845559561126

Epoch: 5| Step: 4
Training loss: 2.7230823040008545
Validation loss: 2.711865181564003

Epoch: 5| Step: 5
Training loss: 2.367281436920166
Validation loss: 2.710918572641188

Epoch: 5| Step: 6
Training loss: 2.9812092781066895
Validation loss: 2.714285050669024

Epoch: 5| Step: 7
Training loss: 2.6488280296325684
Validation loss: 2.711628024296094

Epoch: 5| Step: 8
Training loss: 2.831076145172119
Validation loss: 2.708437506870557

Epoch: 5| Step: 9
Training loss: 2.5732131004333496
Validation loss: 2.705546071452479

Epoch: 5| Step: 10
Training loss: 2.9594461917877197
Validation loss: 2.703858837004631

Epoch: 56| Step: 0
Training loss: 2.8036839962005615
Validation loss: 2.695274335081859

Epoch: 5| Step: 1
Training loss: 3.398932695388794
Validation loss: 2.696343380917785

Epoch: 5| Step: 2
Training loss: 3.097672939300537
Validation loss: 2.6993469345954155

Epoch: 5| Step: 3
Training loss: 2.2554409503936768
Validation loss: 2.6973933558310232

Epoch: 5| Step: 4
Training loss: 2.5011417865753174
Validation loss: 2.700401782989502

Epoch: 5| Step: 5
Training loss: 3.6994266510009766
Validation loss: 2.7022601430134108

Epoch: 5| Step: 6
Training loss: 2.4263134002685547
Validation loss: 2.7037211361751763

Epoch: 5| Step: 7
Training loss: 2.574517011642456
Validation loss: 2.706331314579133

Epoch: 5| Step: 8
Training loss: 2.9044888019561768
Validation loss: 2.708811221584197

Epoch: 5| Step: 9
Training loss: 2.8487582206726074
Validation loss: 2.709888389033656

Epoch: 5| Step: 10
Training loss: 2.5722813606262207
Validation loss: 2.7054505425114788

Epoch: 57| Step: 0
Training loss: 2.6858582496643066
Validation loss: 2.6991344959505144

Epoch: 5| Step: 1
Training loss: 3.3473236560821533
Validation loss: 2.698963221683297

Epoch: 5| Step: 2
Training loss: 2.6323773860931396
Validation loss: 2.6983761838687363

Epoch: 5| Step: 3
Training loss: 3.623699903488159
Validation loss: 2.6976289338963007

Epoch: 5| Step: 4
Training loss: 2.4892826080322266
Validation loss: 2.6941155028599564

Epoch: 5| Step: 5
Training loss: 2.443413496017456
Validation loss: 2.696018816322409

Epoch: 5| Step: 6
Training loss: 3.207308292388916
Validation loss: 2.696601560038905

Epoch: 5| Step: 7
Training loss: 2.966991662979126
Validation loss: 2.6995762983957925

Epoch: 5| Step: 8
Training loss: 2.424370527267456
Validation loss: 2.6986656240237656

Epoch: 5| Step: 9
Training loss: 2.666365146636963
Validation loss: 2.6951339937025502

Epoch: 5| Step: 10
Training loss: 2.6155543327331543
Validation loss: 2.695380928695843

Epoch: 58| Step: 0
Training loss: 2.7119076251983643
Validation loss: 2.692877059341759

Epoch: 5| Step: 1
Training loss: 2.7934610843658447
Validation loss: 2.6892978593867314

Epoch: 5| Step: 2
Training loss: 2.737677812576294
Validation loss: 2.6888239204242663

Epoch: 5| Step: 3
Training loss: 2.7092463970184326
Validation loss: 2.6943378397213515

Epoch: 5| Step: 4
Training loss: 3.20072865486145
Validation loss: 2.6994517131518294

Epoch: 5| Step: 5
Training loss: 2.2279067039489746
Validation loss: 2.7033593321359284

Epoch: 5| Step: 6
Training loss: 2.5886950492858887
Validation loss: 2.7034365515555105

Epoch: 5| Step: 7
Training loss: 3.275524854660034
Validation loss: 2.7029544512430825

Epoch: 5| Step: 8
Training loss: 2.58394455909729
Validation loss: 2.7001645513760146

Epoch: 5| Step: 9
Training loss: 3.4108211994171143
Validation loss: 2.697938798576273

Epoch: 5| Step: 10
Training loss: 2.9463634490966797
Validation loss: 2.7012187152780514

Epoch: 59| Step: 0
Training loss: 2.6023659706115723
Validation loss: 2.695210038974721

Epoch: 5| Step: 1
Training loss: 2.2683374881744385
Validation loss: 2.6888715836309616

Epoch: 5| Step: 2
Training loss: 2.5976834297180176
Validation loss: 2.6884040935065157

Epoch: 5| Step: 3
Training loss: 3.0377426147460938
Validation loss: 2.6880495676430325

Epoch: 5| Step: 4
Training loss: 2.9859960079193115
Validation loss: 2.694489896938365

Epoch: 5| Step: 5
Training loss: 4.216245651245117
Validation loss: 2.692576157149448

Epoch: 5| Step: 6
Training loss: 3.0705809593200684
Validation loss: 2.6892938690800823

Epoch: 5| Step: 7
Training loss: 2.4890177249908447
Validation loss: 2.6925422888930126

Epoch: 5| Step: 8
Training loss: 2.682068347930908
Validation loss: 2.6930669610218336

Epoch: 5| Step: 9
Training loss: 2.5256190299987793
Validation loss: 2.696187344930505

Epoch: 5| Step: 10
Training loss: 2.495574951171875
Validation loss: 2.6950239391737085

Epoch: 60| Step: 0
Training loss: 2.925199270248413
Validation loss: 2.6926534227145615

Epoch: 5| Step: 1
Training loss: 2.929211139678955
Validation loss: 2.6931156573757047

Epoch: 5| Step: 2
Training loss: 1.965092420578003
Validation loss: 2.6915009585759972

Epoch: 5| Step: 3
Training loss: 2.580099582672119
Validation loss: 2.6926256277227916

Epoch: 5| Step: 4
Training loss: 2.849544048309326
Validation loss: 2.6948890122034217

Epoch: 5| Step: 5
Training loss: 3.0962328910827637
Validation loss: 2.688689454909294

Epoch: 5| Step: 6
Training loss: 2.7556381225585938
Validation loss: 2.682888953916488

Epoch: 5| Step: 7
Training loss: 3.2265877723693848
Validation loss: 2.6834236806438816

Epoch: 5| Step: 8
Training loss: 2.935666561126709
Validation loss: 2.6824326797198226

Epoch: 5| Step: 9
Training loss: 3.2070679664611816
Validation loss: 2.681986939522528

Epoch: 5| Step: 10
Training loss: 2.501099109649658
Validation loss: 2.6861156494386735

Epoch: 61| Step: 0
Training loss: 3.1710972785949707
Validation loss: 2.6849267200757096

Epoch: 5| Step: 1
Training loss: 2.34979248046875
Validation loss: 2.6848439913924023

Epoch: 5| Step: 2
Training loss: 3.1531288623809814
Validation loss: 2.682052166231217

Epoch: 5| Step: 3
Training loss: 2.392836570739746
Validation loss: 2.6776695661647345

Epoch: 5| Step: 4
Training loss: 2.459495782852173
Validation loss: 2.681901513889272

Epoch: 5| Step: 5
Training loss: 3.0014610290527344
Validation loss: 2.6789634945572063

Epoch: 5| Step: 6
Training loss: 2.7879109382629395
Validation loss: 2.6822964427291707

Epoch: 5| Step: 7
Training loss: 3.146474838256836
Validation loss: 2.6807598862596738

Epoch: 5| Step: 8
Training loss: 3.6545352935791016
Validation loss: 2.681181376980197

Epoch: 5| Step: 9
Training loss: 2.1083908081054688
Validation loss: 2.6785944559240855

Epoch: 5| Step: 10
Training loss: 2.666429042816162
Validation loss: 2.677111982017435

Epoch: 62| Step: 0
Training loss: 2.6618294715881348
Validation loss: 2.674002855054794

Epoch: 5| Step: 1
Training loss: 3.140249729156494
Validation loss: 2.6763709975827124

Epoch: 5| Step: 2
Training loss: 2.4670825004577637
Validation loss: 2.674682540278281

Epoch: 5| Step: 3
Training loss: 2.6514878273010254
Validation loss: 2.6784975503080632

Epoch: 5| Step: 4
Training loss: 3.7047362327575684
Validation loss: 2.682821153312601

Epoch: 5| Step: 5
Training loss: 2.9197678565979004
Validation loss: 2.680445178862541

Epoch: 5| Step: 6
Training loss: 2.2095863819122314
Validation loss: 2.6824165313474593

Epoch: 5| Step: 7
Training loss: 2.3877015113830566
Validation loss: 2.677257542969078

Epoch: 5| Step: 8
Training loss: 2.731001615524292
Validation loss: 2.676593562608124

Epoch: 5| Step: 9
Training loss: 3.158750295639038
Validation loss: 2.6748573933878252

Epoch: 5| Step: 10
Training loss: 2.9415431022644043
Validation loss: 2.6761101061298

Epoch: 63| Step: 0
Training loss: 2.129094362258911
Validation loss: 2.670633997968448

Epoch: 5| Step: 1
Training loss: 3.569950819015503
Validation loss: 2.6698548357973815

Epoch: 5| Step: 2
Training loss: 2.7270283699035645
Validation loss: 2.6725156230311238

Epoch: 5| Step: 3
Training loss: 2.3069944381713867
Validation loss: 2.6751636407708608

Epoch: 5| Step: 4
Training loss: 3.436211109161377
Validation loss: 2.6735294019022295

Epoch: 5| Step: 5
Training loss: 3.044496774673462
Validation loss: 2.6750505201278196

Epoch: 5| Step: 6
Training loss: 2.9564967155456543
Validation loss: 2.6753143033673688

Epoch: 5| Step: 7
Training loss: 2.2184090614318848
Validation loss: 2.677937143592424

Epoch: 5| Step: 8
Training loss: 1.9749168157577515
Validation loss: 2.6811551740092616

Epoch: 5| Step: 9
Training loss: 3.2575440406799316
Validation loss: 2.6776088488999235

Epoch: 5| Step: 10
Training loss: 3.353757619857788
Validation loss: 2.672748117036717

Epoch: 64| Step: 0
Training loss: 3.205854892730713
Validation loss: 2.6710489821690384

Epoch: 5| Step: 1
Training loss: 3.305929183959961
Validation loss: 2.6724578616439656

Epoch: 5| Step: 2
Training loss: 2.297851085662842
Validation loss: 2.6712702935741794

Epoch: 5| Step: 3
Training loss: 2.967904567718506
Validation loss: 2.673555420291039

Epoch: 5| Step: 4
Training loss: 2.073812961578369
Validation loss: 2.673924312796644

Epoch: 5| Step: 5
Training loss: 3.1347134113311768
Validation loss: 2.6681902690600325

Epoch: 5| Step: 6
Training loss: 3.005953073501587
Validation loss: 2.662959562834873

Epoch: 5| Step: 7
Training loss: 2.9307684898376465
Validation loss: 2.6637481720216813

Epoch: 5| Step: 8
Training loss: 2.918300151824951
Validation loss: 2.6659865148605837

Epoch: 5| Step: 9
Training loss: 2.203666925430298
Validation loss: 2.661876465684624

Epoch: 5| Step: 10
Training loss: 2.833873987197876
Validation loss: 2.6644634123771422

Epoch: 65| Step: 0
Training loss: 2.9115240573883057
Validation loss: 2.6623831154197775

Epoch: 5| Step: 1
Training loss: 2.3461666107177734
Validation loss: 2.662904508652226

Epoch: 5| Step: 2
Training loss: 3.2061474323272705
Validation loss: 2.661423255038518

Epoch: 5| Step: 3
Training loss: 3.082526445388794
Validation loss: 2.6612613739505893

Epoch: 5| Step: 4
Training loss: 3.164368152618408
Validation loss: 2.66093954219613

Epoch: 5| Step: 5
Training loss: 2.1627602577209473
Validation loss: 2.658985335339782

Epoch: 5| Step: 6
Training loss: 2.1605072021484375
Validation loss: 2.6572266291546565

Epoch: 5| Step: 7
Training loss: 2.859649658203125
Validation loss: 2.6594560710332726

Epoch: 5| Step: 8
Training loss: 3.0282020568847656
Validation loss: 2.657769472368302

Epoch: 5| Step: 9
Training loss: 2.672266721725464
Validation loss: 2.658181990346601

Epoch: 5| Step: 10
Training loss: 3.329025983810425
Validation loss: 2.654192337425806

Epoch: 66| Step: 0
Training loss: 3.359905242919922
Validation loss: 2.658115174180718

Epoch: 5| Step: 1
Training loss: 2.3685364723205566
Validation loss: 2.655961303300755

Epoch: 5| Step: 2
Training loss: 2.4666264057159424
Validation loss: 2.6563633539343394

Epoch: 5| Step: 3
Training loss: 2.7549502849578857
Validation loss: 2.6552290531896774

Epoch: 5| Step: 4
Training loss: 2.9992291927337646
Validation loss: 2.6538844544400453

Epoch: 5| Step: 5
Training loss: 2.704876661300659
Validation loss: 2.652423656114968

Epoch: 5| Step: 6
Training loss: 3.111457586288452
Validation loss: 2.653430582374655

Epoch: 5| Step: 7
Training loss: 2.056324005126953
Validation loss: 2.6515966615369244

Epoch: 5| Step: 8
Training loss: 2.5659005641937256
Validation loss: 2.6591726990156275

Epoch: 5| Step: 9
Training loss: 3.1654858589172363
Validation loss: 2.6574109959345993

Epoch: 5| Step: 10
Training loss: 3.289520263671875
Validation loss: 2.657902456098987

Epoch: 67| Step: 0
Training loss: 3.375628709793091
Validation loss: 2.6519647259866037

Epoch: 5| Step: 1
Training loss: 2.3063197135925293
Validation loss: 2.6555742499648884

Epoch: 5| Step: 2
Training loss: 2.6851067543029785
Validation loss: 2.65217258596933

Epoch: 5| Step: 3
Training loss: 2.59486722946167
Validation loss: 2.6501422748770764

Epoch: 5| Step: 4
Training loss: 2.7701504230499268
Validation loss: 2.6494915357200046

Epoch: 5| Step: 5
Training loss: 3.1231541633605957
Validation loss: 2.651872076014037

Epoch: 5| Step: 6
Training loss: 3.103489398956299
Validation loss: 2.6513106130784556

Epoch: 5| Step: 7
Training loss: 2.0837204456329346
Validation loss: 2.6514914420343216

Epoch: 5| Step: 8
Training loss: 2.843780040740967
Validation loss: 2.6521353798527874

Epoch: 5| Step: 9
Training loss: 2.8710713386535645
Validation loss: 2.649328362557196

Epoch: 5| Step: 10
Training loss: 3.03633713722229
Validation loss: 2.6562273476713445

Epoch: 68| Step: 0
Training loss: 3.2826087474823
Validation loss: 2.6531532733671126

Epoch: 5| Step: 1
Training loss: 3.5013835430145264
Validation loss: 2.655580735975696

Epoch: 5| Step: 2
Training loss: 2.465839385986328
Validation loss: 2.651385373966668

Epoch: 5| Step: 3
Training loss: 2.3257343769073486
Validation loss: 2.6550170478000434

Epoch: 5| Step: 4
Training loss: 2.4892354011535645
Validation loss: 2.6499663809294343

Epoch: 5| Step: 5
Training loss: 3.6413967609405518
Validation loss: 2.658336793222735

Epoch: 5| Step: 6
Training loss: 2.716015338897705
Validation loss: 2.6543120773889686

Epoch: 5| Step: 7
Training loss: 2.6365342140197754
Validation loss: 2.6547573022944952

Epoch: 5| Step: 8
Training loss: 2.8025383949279785
Validation loss: 2.65173614153298

Epoch: 5| Step: 9
Training loss: 2.684908390045166
Validation loss: 2.648543211721605

Epoch: 5| Step: 10
Training loss: 2.1006858348846436
Validation loss: 2.645818136071646

Epoch: 69| Step: 0
Training loss: 2.627779006958008
Validation loss: 2.6451571218429075

Epoch: 5| Step: 1
Training loss: 2.7512412071228027
Validation loss: 2.6488453034431703

Epoch: 5| Step: 2
Training loss: 2.547398567199707
Validation loss: 2.6544445227551203

Epoch: 5| Step: 3
Training loss: 2.9303536415100098
Validation loss: 2.654942471493957

Epoch: 5| Step: 4
Training loss: 3.0900604724884033
Validation loss: 2.6573054149586666

Epoch: 5| Step: 5
Training loss: 2.9569621086120605
Validation loss: 2.658160455765263

Epoch: 5| Step: 6
Training loss: 3.230379104614258
Validation loss: 2.650636693482758

Epoch: 5| Step: 7
Training loss: 2.7004106044769287
Validation loss: 2.6457038566630375

Epoch: 5| Step: 8
Training loss: 2.586796283721924
Validation loss: 2.6495921765604327

Epoch: 5| Step: 9
Training loss: 2.832061529159546
Validation loss: 2.6496107732096026

Epoch: 5| Step: 10
Training loss: 2.4032719135284424
Validation loss: 2.642859758869294

Epoch: 70| Step: 0
Training loss: 2.5617356300354004
Validation loss: 2.649676133227605

Epoch: 5| Step: 1
Training loss: 3.0970702171325684
Validation loss: 2.6484665281029156

Epoch: 5| Step: 2
Training loss: 3.121995210647583
Validation loss: 2.643581203235093

Epoch: 5| Step: 3
Training loss: 3.186758518218994
Validation loss: 2.642074792615829

Epoch: 5| Step: 4
Training loss: 2.8857595920562744
Validation loss: 2.636230860987017

Epoch: 5| Step: 5
Training loss: 2.6656482219696045
Validation loss: 2.6409915544653453

Epoch: 5| Step: 6
Training loss: 3.06575608253479
Validation loss: 2.641059257650888

Epoch: 5| Step: 7
Training loss: 2.4408679008483887
Validation loss: 2.647844719630416

Epoch: 5| Step: 8
Training loss: 2.915072441101074
Validation loss: 2.6511399117849206

Epoch: 5| Step: 9
Training loss: 2.514618396759033
Validation loss: 2.6710868855958343

Epoch: 5| Step: 10
Training loss: 2.1332554817199707
Validation loss: 2.6702717786194174

Epoch: 71| Step: 0
Training loss: 2.6942152976989746
Validation loss: 2.6621459530245875

Epoch: 5| Step: 1
Training loss: 2.6697921752929688
Validation loss: 2.6473779063070975

Epoch: 5| Step: 2
Training loss: 2.526026964187622
Validation loss: 2.6332834177119757

Epoch: 5| Step: 3
Training loss: 2.0126309394836426
Validation loss: 2.6386647096244236

Epoch: 5| Step: 4
Training loss: 3.0120606422424316
Validation loss: 2.64107584696944

Epoch: 5| Step: 5
Training loss: 2.3567962646484375
Validation loss: 2.644346608910509

Epoch: 5| Step: 6
Training loss: 3.509333372116089
Validation loss: 2.640088132632676

Epoch: 5| Step: 7
Training loss: 2.2628073692321777
Validation loss: 2.6521899315618698

Epoch: 5| Step: 8
Training loss: 3.632312297821045
Validation loss: 2.6530579008081907

Epoch: 5| Step: 9
Training loss: 2.884397506713867
Validation loss: 2.647263665353098

Epoch: 5| Step: 10
Training loss: 3.2141618728637695
Validation loss: 2.650652734182214

Epoch: 72| Step: 0
Training loss: 2.8484115600585938
Validation loss: 2.646888212491107

Epoch: 5| Step: 1
Training loss: 3.4792327880859375
Validation loss: 2.634312673281598

Epoch: 5| Step: 2
Training loss: 3.2591705322265625
Validation loss: 2.6245093473824124

Epoch: 5| Step: 3
Training loss: 2.600271463394165
Validation loss: 2.629517429618425

Epoch: 5| Step: 4
Training loss: 2.812920331954956
Validation loss: 2.6290003151021977

Epoch: 5| Step: 5
Training loss: 3.064432382583618
Validation loss: 2.632499974261048

Epoch: 5| Step: 6
Training loss: 2.301178455352783
Validation loss: 2.6338762160270446

Epoch: 5| Step: 7
Training loss: 2.414525270462036
Validation loss: 2.633038951504615

Epoch: 5| Step: 8
Training loss: 1.7685292959213257
Validation loss: 2.6373413096192064

Epoch: 5| Step: 9
Training loss: 3.0639584064483643
Validation loss: 2.6368787134847333

Epoch: 5| Step: 10
Training loss: 3.0478689670562744
Validation loss: 2.6347313209246566

Epoch: 73| Step: 0
Training loss: 3.046175956726074
Validation loss: 2.6333548689401276

Epoch: 5| Step: 1
Training loss: 3.481318712234497
Validation loss: 2.6329177784663376

Epoch: 5| Step: 2
Training loss: 2.7264065742492676
Validation loss: 2.6309487153125066

Epoch: 5| Step: 3
Training loss: 3.0660600662231445
Validation loss: 2.6265787463034354

Epoch: 5| Step: 4
Training loss: 3.383188247680664
Validation loss: 2.626413937537901

Epoch: 5| Step: 5
Training loss: 2.780186176300049
Validation loss: 2.6251016483511975

Epoch: 5| Step: 6
Training loss: 2.8722667694091797
Validation loss: 2.6319751303683043

Epoch: 5| Step: 7
Training loss: 2.0184924602508545
Validation loss: 2.6344673120847313

Epoch: 5| Step: 8
Training loss: 2.1179256439208984
Validation loss: 2.640940850780856

Epoch: 5| Step: 9
Training loss: 2.486391544342041
Validation loss: 2.6546312942299792

Epoch: 5| Step: 10
Training loss: 2.6509203910827637
Validation loss: 2.633014340554514

Epoch: 74| Step: 0
Training loss: 1.5832641124725342
Validation loss: 2.624468372714135

Epoch: 5| Step: 1
Training loss: 3.5857861042022705
Validation loss: 2.6228324905518563

Epoch: 5| Step: 2
Training loss: 2.936713218688965
Validation loss: 2.628343577026039

Epoch: 5| Step: 3
Training loss: 2.5093982219696045
Validation loss: 2.6268519893769295

Epoch: 5| Step: 4
Training loss: 3.02347993850708
Validation loss: 2.627233966704338

Epoch: 5| Step: 5
Training loss: 2.345374584197998
Validation loss: 2.628540773545542

Epoch: 5| Step: 6
Training loss: 3.498065948486328
Validation loss: 2.6288073729443293

Epoch: 5| Step: 7
Training loss: 3.108795404434204
Validation loss: 2.633488319253409

Epoch: 5| Step: 8
Training loss: 2.351020336151123
Validation loss: 2.62701262709915

Epoch: 5| Step: 9
Training loss: 3.026991844177246
Validation loss: 2.6291788675451793

Epoch: 5| Step: 10
Training loss: 2.575075626373291
Validation loss: 2.6243310307943695

Epoch: 75| Step: 0
Training loss: 2.929349422454834
Validation loss: 2.625045571275937

Epoch: 5| Step: 1
Training loss: 3.267268419265747
Validation loss: 2.6370027757460073

Epoch: 5| Step: 2
Training loss: 2.224478006362915
Validation loss: 2.629226294896936

Epoch: 5| Step: 3
Training loss: 3.159074544906616
Validation loss: 2.6246100574411373

Epoch: 5| Step: 4
Training loss: 2.7119057178497314
Validation loss: 2.622225038466915

Epoch: 5| Step: 5
Training loss: 3.7331314086914062
Validation loss: 2.626005821330573

Epoch: 5| Step: 6
Training loss: 3.2217719554901123
Validation loss: 2.6262742652687976

Epoch: 5| Step: 7
Training loss: 2.2255146503448486
Validation loss: 2.6207775505640174

Epoch: 5| Step: 8
Training loss: 2.5164408683776855
Validation loss: 2.618729204259893

Epoch: 5| Step: 9
Training loss: 2.3190853595733643
Validation loss: 2.6254876223943566

Epoch: 5| Step: 10
Training loss: 2.1241538524627686
Validation loss: 2.6254761731752785

Epoch: 76| Step: 0
Training loss: 2.6799099445343018
Validation loss: 2.6206369323115193

Epoch: 5| Step: 1
Training loss: 2.734039783477783
Validation loss: 2.6157810072745047

Epoch: 5| Step: 2
Training loss: 2.9147634506225586
Validation loss: 2.6181243183792278

Epoch: 5| Step: 3
Training loss: 2.9183802604675293
Validation loss: 2.620929238616779

Epoch: 5| Step: 4
Training loss: 3.122873306274414
Validation loss: 2.6215597455219557

Epoch: 5| Step: 5
Training loss: 2.813830614089966
Validation loss: 2.6230235638157016

Epoch: 5| Step: 6
Training loss: 2.407838821411133
Validation loss: 2.623891374116303

Epoch: 5| Step: 7
Training loss: 2.902596950531006
Validation loss: 2.6217449326669016

Epoch: 5| Step: 8
Training loss: 2.546377420425415
Validation loss: 2.6221118127146075

Epoch: 5| Step: 9
Training loss: 2.4140796661376953
Validation loss: 2.6239424751650904

Epoch: 5| Step: 10
Training loss: 3.110264539718628
Validation loss: 2.6195876008720806

Epoch: 77| Step: 0
Training loss: 2.674396514892578
Validation loss: 2.619892140870453

Epoch: 5| Step: 1
Training loss: 2.574916362762451
Validation loss: 2.6247111161549888

Epoch: 5| Step: 2
Training loss: 2.645127058029175
Validation loss: 2.6268162906810804

Epoch: 5| Step: 3
Training loss: 2.717101573944092
Validation loss: 2.627796047477312

Epoch: 5| Step: 4
Training loss: 2.4756104946136475
Validation loss: 2.628841710347001

Epoch: 5| Step: 5
Training loss: 2.8618390560150146
Validation loss: 2.6297601422955914

Epoch: 5| Step: 6
Training loss: 2.5454976558685303
Validation loss: 2.6281376936102427

Epoch: 5| Step: 7
Training loss: 3.1909677982330322
Validation loss: 2.6284387983301634

Epoch: 5| Step: 8
Training loss: 2.5640645027160645
Validation loss: 2.624009434894849

Epoch: 5| Step: 9
Training loss: 3.376255750656128
Validation loss: 2.6209510039257746

Epoch: 5| Step: 10
Training loss: 2.895521640777588
Validation loss: 2.6173988696067565

Epoch: 78| Step: 0
Training loss: 2.8244309425354004
Validation loss: 2.6229941075847996

Epoch: 5| Step: 1
Training loss: 3.169785737991333
Validation loss: 2.614566000558997

Epoch: 5| Step: 2
Training loss: 3.1107430458068848
Validation loss: 2.611479572070542

Epoch: 5| Step: 3
Training loss: 2.5816009044647217
Validation loss: 2.610271615366782

Epoch: 5| Step: 4
Training loss: 2.8518288135528564
Validation loss: 2.60807474454244

Epoch: 5| Step: 5
Training loss: 2.8476157188415527
Validation loss: 2.609468262682679

Epoch: 5| Step: 6
Training loss: 2.186509370803833
Validation loss: 2.609645874269547

Epoch: 5| Step: 7
Training loss: 2.6403822898864746
Validation loss: 2.6104153099880425

Epoch: 5| Step: 8
Training loss: 2.569065570831299
Validation loss: 2.6081025651706162

Epoch: 5| Step: 9
Training loss: 2.603646993637085
Validation loss: 2.6048891672524075

Epoch: 5| Step: 10
Training loss: 3.124405860900879
Validation loss: 2.6105510496324107

Epoch: 79| Step: 0
Training loss: 3.149167060852051
Validation loss: 2.6055851751758206

Epoch: 5| Step: 1
Training loss: 2.6044907569885254
Validation loss: 2.605300857174781

Epoch: 5| Step: 2
Training loss: 2.9989564418792725
Validation loss: 2.6036547409590853

Epoch: 5| Step: 3
Training loss: 2.3143486976623535
Validation loss: 2.6027247316093853

Epoch: 5| Step: 4
Training loss: 2.8889236450195312
Validation loss: 2.601102562360866

Epoch: 5| Step: 5
Training loss: 2.4410250186920166
Validation loss: 2.6013639998692337

Epoch: 5| Step: 6
Training loss: 2.3383052349090576
Validation loss: 2.60363910787849

Epoch: 5| Step: 7
Training loss: 2.651738405227661
Validation loss: 2.604053017913654

Epoch: 5| Step: 8
Training loss: 2.941164255142212
Validation loss: 2.606913443534605

Epoch: 5| Step: 9
Training loss: 2.5682790279388428
Validation loss: 2.6093555470948577

Epoch: 5| Step: 10
Training loss: 3.619542121887207
Validation loss: 2.6113766393353863

Epoch: 80| Step: 0
Training loss: 2.9270880222320557
Validation loss: 2.6093791530978296

Epoch: 5| Step: 1
Training loss: 2.260120153427124
Validation loss: 2.6064553670985724

Epoch: 5| Step: 2
Training loss: 2.7607955932617188
Validation loss: 2.6066208449743127

Epoch: 5| Step: 3
Training loss: 2.8719871044158936
Validation loss: 2.600683061025476

Epoch: 5| Step: 4
Training loss: 2.6437060832977295
Validation loss: 2.5974470902514715

Epoch: 5| Step: 5
Training loss: 2.926215410232544
Validation loss: 2.6018723416072067

Epoch: 5| Step: 6
Training loss: 2.3221256732940674
Validation loss: 2.6006903930376937

Epoch: 5| Step: 7
Training loss: 2.3870983123779297
Validation loss: 2.6008617288322857

Epoch: 5| Step: 8
Training loss: 3.2243258953094482
Validation loss: 2.6030723946068877

Epoch: 5| Step: 9
Training loss: 2.97979474067688
Validation loss: 2.6034481935603644

Epoch: 5| Step: 10
Training loss: 3.08601450920105
Validation loss: 2.601038614908854

Epoch: 81| Step: 0
Training loss: 2.5202219486236572
Validation loss: 2.602841754113474

Epoch: 5| Step: 1
Training loss: 2.7952399253845215
Validation loss: 2.6067097263951458

Epoch: 5| Step: 2
Training loss: 2.8855819702148438
Validation loss: 2.5958431920697613

Epoch: 5| Step: 3
Training loss: 2.687187671661377
Validation loss: 2.594349222798501

Epoch: 5| Step: 4
Training loss: 3.1219398975372314
Validation loss: 2.5933650565403763

Epoch: 5| Step: 5
Training loss: 2.942822217941284
Validation loss: 2.5955208373326126

Epoch: 5| Step: 6
Training loss: 2.3169116973876953
Validation loss: 2.5952910941134215

Epoch: 5| Step: 7
Training loss: 2.325416326522827
Validation loss: 2.5980515480041504

Epoch: 5| Step: 8
Training loss: 3.581745147705078
Validation loss: 2.600682299624207

Epoch: 5| Step: 9
Training loss: 2.7745368480682373
Validation loss: 2.5981546268668225

Epoch: 5| Step: 10
Training loss: 2.359131097793579
Validation loss: 2.596781325596635

Epoch: 82| Step: 0
Training loss: 2.118420124053955
Validation loss: 2.59608442296264

Epoch: 5| Step: 1
Training loss: 2.6514225006103516
Validation loss: 2.5953500629753194

Epoch: 5| Step: 2
Training loss: 2.6420137882232666
Validation loss: 2.593850202457879

Epoch: 5| Step: 3
Training loss: 2.273250102996826
Validation loss: 2.5948491942497993

Epoch: 5| Step: 4
Training loss: 2.3400111198425293
Validation loss: 2.596216450455368

Epoch: 5| Step: 5
Training loss: 3.5101845264434814
Validation loss: 2.602952872553179

Epoch: 5| Step: 6
Training loss: 2.9998953342437744
Validation loss: 2.6088599466508433

Epoch: 5| Step: 7
Training loss: 3.5758635997772217
Validation loss: 2.597516998167961

Epoch: 5| Step: 8
Training loss: 3.0660243034362793
Validation loss: 2.6005370693822063

Epoch: 5| Step: 9
Training loss: 2.4617233276367188
Validation loss: 2.601343280525618

Epoch: 5| Step: 10
Training loss: 2.667539119720459
Validation loss: 2.6006221386694137

Epoch: 83| Step: 0
Training loss: 2.9852476119995117
Validation loss: 2.596999365796325

Epoch: 5| Step: 1
Training loss: 3.0489768981933594
Validation loss: 2.5951494042591383

Epoch: 5| Step: 2
Training loss: 3.2608585357666016
Validation loss: 2.5949444565721738

Epoch: 5| Step: 3
Training loss: 2.5354199409484863
Validation loss: 2.5964760011242283

Epoch: 5| Step: 4
Training loss: 2.4754409790039062
Validation loss: 2.595460045722223

Epoch: 5| Step: 5
Training loss: 2.456360340118408
Validation loss: 2.5974475183794574

Epoch: 5| Step: 6
Training loss: 2.19588041305542
Validation loss: 2.592826940680063

Epoch: 5| Step: 7
Training loss: 2.8627281188964844
Validation loss: 2.5919738379857873

Epoch: 5| Step: 8
Training loss: 2.3135383129119873
Validation loss: 2.583837127172819

Epoch: 5| Step: 9
Training loss: 2.8324809074401855
Validation loss: 2.580354672606273

Epoch: 5| Step: 10
Training loss: 3.4153831005096436
Validation loss: 2.5827635642020934

Epoch: 84| Step: 0
Training loss: 2.8377628326416016
Validation loss: 2.58390788878164

Epoch: 5| Step: 1
Training loss: 2.8014800548553467
Validation loss: 2.579544872365972

Epoch: 5| Step: 2
Training loss: 2.623666286468506
Validation loss: 2.5808570692616124

Epoch: 5| Step: 3
Training loss: 2.467292547225952
Validation loss: 2.5801863952349593

Epoch: 5| Step: 4
Training loss: 2.825253486633301
Validation loss: 2.5797707444878033

Epoch: 5| Step: 5
Training loss: 3.323732376098633
Validation loss: 2.577073891957601

Epoch: 5| Step: 6
Training loss: 3.930128574371338
Validation loss: 2.5754343284073697

Epoch: 5| Step: 7
Training loss: 1.8980356454849243
Validation loss: 2.5779272638341433

Epoch: 5| Step: 8
Training loss: 2.7424042224884033
Validation loss: 2.576074554074195

Epoch: 5| Step: 9
Training loss: 2.0562171936035156
Validation loss: 2.5775563101614676

Epoch: 5| Step: 10
Training loss: 2.696981430053711
Validation loss: 2.5765683394606396

Epoch: 85| Step: 0
Training loss: 2.7213211059570312
Validation loss: 2.581428466304656

Epoch: 5| Step: 1
Training loss: 2.3861896991729736
Validation loss: 2.589187434924546

Epoch: 5| Step: 2
Training loss: 3.0223898887634277
Validation loss: 2.588304458125945

Epoch: 5| Step: 3
Training loss: 2.5915229320526123
Validation loss: 2.585346177060117

Epoch: 5| Step: 4
Training loss: 3.460324764251709
Validation loss: 2.585438661677863

Epoch: 5| Step: 5
Training loss: 2.3142151832580566
Validation loss: 2.581344458364671

Epoch: 5| Step: 6
Training loss: 2.3083858489990234
Validation loss: 2.5778642085290726

Epoch: 5| Step: 7
Training loss: 2.1832666397094727
Validation loss: 2.579164158913397

Epoch: 5| Step: 8
Training loss: 3.1919686794281006
Validation loss: 2.5705000815852994

Epoch: 5| Step: 9
Training loss: 2.5266237258911133
Validation loss: 2.577848470339211

Epoch: 5| Step: 10
Training loss: 3.6730473041534424
Validation loss: 2.5835191101156254

Epoch: 86| Step: 0
Training loss: 2.6863045692443848
Validation loss: 2.5882313443768408

Epoch: 5| Step: 1
Training loss: 2.7783164978027344
Validation loss: 2.5822209055705736

Epoch: 5| Step: 2
Training loss: 2.902940034866333
Validation loss: 2.5785977430241083

Epoch: 5| Step: 3
Training loss: 3.601649761199951
Validation loss: 2.5742266152494695

Epoch: 5| Step: 4
Training loss: 2.299790143966675
Validation loss: 2.5701991281201764

Epoch: 5| Step: 5
Training loss: 3.238461971282959
Validation loss: 2.57593108248967

Epoch: 5| Step: 6
Training loss: 3.5325427055358887
Validation loss: 2.584358274295766

Epoch: 5| Step: 7
Training loss: 1.9816547632217407
Validation loss: 2.584980692914737

Epoch: 5| Step: 8
Training loss: 2.2916598320007324
Validation loss: 2.584042203041815

Epoch: 5| Step: 9
Training loss: 2.925466537475586
Validation loss: 2.5873318872144146

Epoch: 5| Step: 10
Training loss: 1.856048822402954
Validation loss: 2.5769080423539683

Epoch: 87| Step: 0
Training loss: 3.4942550659179688
Validation loss: 2.5715610237531763

Epoch: 5| Step: 1
Training loss: 2.6666016578674316
Validation loss: 2.569745591891709

Epoch: 5| Step: 2
Training loss: 2.9049696922302246
Validation loss: 2.57102309760227

Epoch: 5| Step: 3
Training loss: 2.5378060340881348
Validation loss: 2.57631927920926

Epoch: 5| Step: 4
Training loss: 2.2776453495025635
Validation loss: 2.577061196809174

Epoch: 5| Step: 5
Training loss: 3.0929112434387207
Validation loss: 2.575522509954309

Epoch: 5| Step: 6
Training loss: 2.166738510131836
Validation loss: 2.571519074901458

Epoch: 5| Step: 7
Training loss: 2.6636791229248047
Validation loss: 2.5727535601585143

Epoch: 5| Step: 8
Training loss: 3.0780136585235596
Validation loss: 2.5806384471154984

Epoch: 5| Step: 9
Training loss: 2.6680800914764404
Validation loss: 2.5800450437812397

Epoch: 5| Step: 10
Training loss: 2.5676376819610596
Validation loss: 2.5906816451780257

Epoch: 88| Step: 0
Training loss: 3.296546459197998
Validation loss: 2.601703619444242

Epoch: 5| Step: 1
Training loss: 2.98535418510437
Validation loss: 2.577599751052036

Epoch: 5| Step: 2
Training loss: 3.0408236980438232
Validation loss: 2.5658498400001117

Epoch: 5| Step: 3
Training loss: 2.5506138801574707
Validation loss: 2.5684707241673626

Epoch: 5| Step: 4
Training loss: 2.467148780822754
Validation loss: 2.565291994361467

Epoch: 5| Step: 5
Training loss: 2.933227062225342
Validation loss: 2.568348059090235

Epoch: 5| Step: 6
Training loss: 2.2516722679138184
Validation loss: 2.567434703150103

Epoch: 5| Step: 7
Training loss: 2.515281915664673
Validation loss: 2.5767339455184115

Epoch: 5| Step: 8
Training loss: 2.761037826538086
Validation loss: 2.579394107223839

Epoch: 5| Step: 9
Training loss: 2.2261931896209717
Validation loss: 2.5821212927500405

Epoch: 5| Step: 10
Training loss: 3.185504198074341
Validation loss: 2.5733089088111796

Epoch: 89| Step: 0
Training loss: 2.7697384357452393
Validation loss: 2.568794112051687

Epoch: 5| Step: 1
Training loss: 2.6174628734588623
Validation loss: 2.5706627740654895

Epoch: 5| Step: 2
Training loss: 3.405129909515381
Validation loss: 2.56466156436551

Epoch: 5| Step: 3
Training loss: 2.8358967304229736
Validation loss: 2.568541537048996

Epoch: 5| Step: 4
Training loss: 3.2874042987823486
Validation loss: 2.568586516123946

Epoch: 5| Step: 5
Training loss: 2.179551839828491
Validation loss: 2.5670913932144

Epoch: 5| Step: 6
Training loss: 2.4024949073791504
Validation loss: 2.5661894941842682

Epoch: 5| Step: 7
Training loss: 2.1982245445251465
Validation loss: 2.5647720957315094

Epoch: 5| Step: 8
Training loss: 2.571040391921997
Validation loss: 2.5633196266748572

Epoch: 5| Step: 9
Training loss: 2.464871406555176
Validation loss: 2.561469106264012

Epoch: 5| Step: 10
Training loss: 3.476372480392456
Validation loss: 2.560364543750722

Epoch: 90| Step: 0
Training loss: 2.8828697204589844
Validation loss: 2.5628033017599456

Epoch: 5| Step: 1
Training loss: 2.962113857269287
Validation loss: 2.5605251686547392

Epoch: 5| Step: 2
Training loss: 3.2042136192321777
Validation loss: 2.5625600994274182

Epoch: 5| Step: 3
Training loss: 2.7424674034118652
Validation loss: 2.560726258062547

Epoch: 5| Step: 4
Training loss: 2.708556890487671
Validation loss: 2.56884019349211

Epoch: 5| Step: 5
Training loss: 2.414745330810547
Validation loss: 2.566857309751613

Epoch: 5| Step: 6
Training loss: 2.931823968887329
Validation loss: 2.563603179429167

Epoch: 5| Step: 7
Training loss: 2.878972291946411
Validation loss: 2.5540925764268443

Epoch: 5| Step: 8
Training loss: 1.733629584312439
Validation loss: 2.556786439752066

Epoch: 5| Step: 9
Training loss: 2.5302538871765137
Validation loss: 2.5559457502057477

Epoch: 5| Step: 10
Training loss: 3.054891586303711
Validation loss: 2.5504354174419115

Epoch: 91| Step: 0
Training loss: 2.983633041381836
Validation loss: 2.5511007565324024

Epoch: 5| Step: 1
Training loss: 3.1067357063293457
Validation loss: 2.5518624756925847

Epoch: 5| Step: 2
Training loss: 2.665287971496582
Validation loss: 2.555845493911415

Epoch: 5| Step: 3
Training loss: 1.9606059789657593
Validation loss: 2.5569767439237205

Epoch: 5| Step: 4
Training loss: 3.0269951820373535
Validation loss: 2.5572604876692577

Epoch: 5| Step: 5
Training loss: 3.028658151626587
Validation loss: 2.5518477988499466

Epoch: 5| Step: 6
Training loss: 3.272799015045166
Validation loss: 2.5565413044344996

Epoch: 5| Step: 7
Training loss: 2.5860962867736816
Validation loss: 2.551993716147638

Epoch: 5| Step: 8
Training loss: 2.312847137451172
Validation loss: 2.548489496272097

Epoch: 5| Step: 9
Training loss: 2.4326798915863037
Validation loss: 2.553992199641402

Epoch: 5| Step: 10
Training loss: 2.536285877227783
Validation loss: 2.551029407849876

Epoch: 92| Step: 0
Training loss: 2.859161376953125
Validation loss: 2.5471045483825026

Epoch: 5| Step: 1
Training loss: 2.7630152702331543
Validation loss: 2.5472658270148822

Epoch: 5| Step: 2
Training loss: 3.3112995624542236
Validation loss: 2.5513883790662213

Epoch: 5| Step: 3
Training loss: 3.5354676246643066
Validation loss: 2.554669803188693

Epoch: 5| Step: 4
Training loss: 2.1982696056365967
Validation loss: 2.554814769375709

Epoch: 5| Step: 5
Training loss: 2.637756586074829
Validation loss: 2.546294076468355

Epoch: 5| Step: 6
Training loss: 2.4255385398864746
Validation loss: 2.5436820932613906

Epoch: 5| Step: 7
Training loss: 2.714468479156494
Validation loss: 2.5405744967922086

Epoch: 5| Step: 8
Training loss: 2.5186638832092285
Validation loss: 2.540202207462762

Epoch: 5| Step: 9
Training loss: 2.533496379852295
Validation loss: 2.5449278649463447

Epoch: 5| Step: 10
Training loss: 2.3538811206817627
Validation loss: 2.544205368206065

Epoch: 93| Step: 0
Training loss: 2.544376850128174
Validation loss: 2.544763908591322

Epoch: 5| Step: 1
Training loss: 2.6413652896881104
Validation loss: 2.5433622765284714

Epoch: 5| Step: 2
Training loss: 2.799996852874756
Validation loss: 2.5399099421757523

Epoch: 5| Step: 3
Training loss: 2.9134507179260254
Validation loss: 2.5416983891558904

Epoch: 5| Step: 4
Training loss: 3.1555099487304688
Validation loss: 2.538187301287087

Epoch: 5| Step: 5
Training loss: 3.0474014282226562
Validation loss: 2.533596482328189

Epoch: 5| Step: 6
Training loss: 2.8393423557281494
Validation loss: 2.5398798527256137

Epoch: 5| Step: 7
Training loss: 2.2586874961853027
Validation loss: 2.544023921412806

Epoch: 5| Step: 8
Training loss: 2.892604112625122
Validation loss: 2.548051436742147

Epoch: 5| Step: 9
Training loss: 2.763373851776123
Validation loss: 2.554268665211175

Epoch: 5| Step: 10
Training loss: 1.9573270082473755
Validation loss: 2.54366704469086

Epoch: 94| Step: 0
Training loss: 2.849120855331421
Validation loss: 2.5365541545293664

Epoch: 5| Step: 1
Training loss: 2.680346727371216
Validation loss: 2.5362382037665254

Epoch: 5| Step: 2
Training loss: 2.9281630516052246
Validation loss: 2.538637204836774

Epoch: 5| Step: 3
Training loss: 2.4885852336883545
Validation loss: 2.5424858165043656

Epoch: 5| Step: 4
Training loss: 2.05741548538208
Validation loss: 2.5387303983011553

Epoch: 5| Step: 5
Training loss: 3.333634853363037
Validation loss: 2.5388897875303864

Epoch: 5| Step: 6
Training loss: 2.804896354675293
Validation loss: 2.541224084874635

Epoch: 5| Step: 7
Training loss: 2.2372047901153564
Validation loss: 2.5316929201925955

Epoch: 5| Step: 8
Training loss: 2.920321464538574
Validation loss: 2.532421976007441

Epoch: 5| Step: 9
Training loss: 2.3540148735046387
Validation loss: 2.5298552256758495

Epoch: 5| Step: 10
Training loss: 3.2946362495422363
Validation loss: 2.5297569190302203

Epoch: 95| Step: 0
Training loss: 2.509028196334839
Validation loss: 2.529056877218267

Epoch: 5| Step: 1
Training loss: 3.29248309135437
Validation loss: 2.530598173859299

Epoch: 5| Step: 2
Training loss: 2.206109046936035
Validation loss: 2.530432883129325

Epoch: 5| Step: 3
Training loss: 2.6228744983673096
Validation loss: 2.532724262565695

Epoch: 5| Step: 4
Training loss: 2.515109062194824
Validation loss: 2.533328958736953

Epoch: 5| Step: 5
Training loss: 3.0278096199035645
Validation loss: 2.5281950735276744

Epoch: 5| Step: 6
Training loss: 2.557046413421631
Validation loss: 2.5288853388960644

Epoch: 5| Step: 7
Training loss: 2.5597782135009766
Validation loss: 2.5297946468476327

Epoch: 5| Step: 8
Training loss: 2.832914113998413
Validation loss: 2.5272844068465696

Epoch: 5| Step: 9
Training loss: 3.023850440979004
Validation loss: 2.531382253093104

Epoch: 5| Step: 10
Training loss: 2.658935070037842
Validation loss: 2.5314659610871346

Epoch: 96| Step: 0
Training loss: 3.5896029472351074
Validation loss: 2.527235823292886

Epoch: 5| Step: 1
Training loss: 3.3020095825195312
Validation loss: 2.5267513772492767

Epoch: 5| Step: 2
Training loss: 2.313824415206909
Validation loss: 2.5266254281484954

Epoch: 5| Step: 3
Training loss: 2.457481861114502
Validation loss: 2.5315727187741186

Epoch: 5| Step: 4
Training loss: 2.3267884254455566
Validation loss: 2.525659397084226

Epoch: 5| Step: 5
Training loss: 2.881190776824951
Validation loss: 2.5303190241577806

Epoch: 5| Step: 6
Training loss: 2.398782253265381
Validation loss: 2.5268301938169744

Epoch: 5| Step: 7
Training loss: 2.886418104171753
Validation loss: 2.525261741812511

Epoch: 5| Step: 8
Training loss: 2.363837957382202
Validation loss: 2.524822458144157

Epoch: 5| Step: 9
Training loss: 2.8156752586364746
Validation loss: 2.5228974152636785

Epoch: 5| Step: 10
Training loss: 2.424677610397339
Validation loss: 2.523736415370818

Epoch: 97| Step: 0
Training loss: 2.6948275566101074
Validation loss: 2.523551046207387

Epoch: 5| Step: 1
Training loss: 2.7728347778320312
Validation loss: 2.524075572208692

Epoch: 5| Step: 2
Training loss: 2.7702014446258545
Validation loss: 2.522105332343809

Epoch: 5| Step: 3
Training loss: 3.07706880569458
Validation loss: 2.520048705480432

Epoch: 5| Step: 4
Training loss: 2.4812631607055664
Validation loss: 2.5260354652199695

Epoch: 5| Step: 5
Training loss: 2.231816530227661
Validation loss: 2.523074121885402

Epoch: 5| Step: 6
Training loss: 2.581930637359619
Validation loss: 2.5198464085978847

Epoch: 5| Step: 7
Training loss: 3.1350340843200684
Validation loss: 2.5190912421031664

Epoch: 5| Step: 8
Training loss: 2.9715065956115723
Validation loss: 2.522508011069349

Epoch: 5| Step: 9
Training loss: 2.41536808013916
Validation loss: 2.517716227039214

Epoch: 5| Step: 10
Training loss: 2.5999317169189453
Validation loss: 2.525563073414628

Epoch: 98| Step: 0
Training loss: 2.6420536041259766
Validation loss: 2.5221895197386384

Epoch: 5| Step: 1
Training loss: 3.042954921722412
Validation loss: 2.523236219600965

Epoch: 5| Step: 2
Training loss: 2.3552377223968506
Validation loss: 2.5236038623317594

Epoch: 5| Step: 3
Training loss: 2.748321056365967
Validation loss: 2.523814583337435

Epoch: 5| Step: 4
Training loss: 2.90812611579895
Validation loss: 2.520602423657653

Epoch: 5| Step: 5
Training loss: 3.0140485763549805
Validation loss: 2.517241642039309

Epoch: 5| Step: 6
Training loss: 2.9197421073913574
Validation loss: 2.518597041406939

Epoch: 5| Step: 7
Training loss: 2.436332941055298
Validation loss: 2.5214181484714633

Epoch: 5| Step: 8
Training loss: 1.7648531198501587
Validation loss: 2.525247568725258

Epoch: 5| Step: 9
Training loss: 2.926718235015869
Validation loss: 2.5411467859821935

Epoch: 5| Step: 10
Training loss: 2.957880735397339
Validation loss: 2.56076559969174

Epoch: 99| Step: 0
Training loss: 3.310873508453369
Validation loss: 2.5764753818511963

Epoch: 5| Step: 1
Training loss: 2.578261375427246
Validation loss: 2.583774787123485

Epoch: 5| Step: 2
Training loss: 2.113490581512451
Validation loss: 2.564496486417709

Epoch: 5| Step: 3
Training loss: 2.9056801795959473
Validation loss: 2.5450635802361274

Epoch: 5| Step: 4
Training loss: 2.650996446609497
Validation loss: 2.531174587947066

Epoch: 5| Step: 5
Training loss: 2.609563112258911
Validation loss: 2.524895421920284

Epoch: 5| Step: 6
Training loss: 2.6002769470214844
Validation loss: 2.5184558309534544

Epoch: 5| Step: 7
Training loss: 2.7449216842651367
Validation loss: 2.513816974496329

Epoch: 5| Step: 8
Training loss: 2.5544676780700684
Validation loss: 2.513100277992987

Epoch: 5| Step: 9
Training loss: 2.632209062576294
Validation loss: 2.519331027102727

Epoch: 5| Step: 10
Training loss: 3.236978054046631
Validation loss: 2.522143692098638

Epoch: 100| Step: 0
Training loss: 2.7662200927734375
Validation loss: 2.523197522727392

Epoch: 5| Step: 1
Training loss: 2.350243330001831
Validation loss: 2.52317799034939

Epoch: 5| Step: 2
Training loss: 3.3178584575653076
Validation loss: 2.5169443135620444

Epoch: 5| Step: 3
Training loss: 2.7950122356414795
Validation loss: 2.513274597865279

Epoch: 5| Step: 4
Training loss: 2.5822794437408447
Validation loss: 2.5152965284162954

Epoch: 5| Step: 5
Training loss: 2.7951393127441406
Validation loss: 2.5157631751029723

Epoch: 5| Step: 6
Training loss: 3.1819069385528564
Validation loss: 2.513253153011363

Epoch: 5| Step: 7
Training loss: 2.756256103515625
Validation loss: 2.5150303866273616

Epoch: 5| Step: 8
Training loss: 2.6683545112609863
Validation loss: 2.515300594350343

Epoch: 5| Step: 9
Training loss: 2.254678726196289
Validation loss: 2.511297587425478

Epoch: 5| Step: 10
Training loss: 2.214200973510742
Validation loss: 2.516763943497853

Epoch: 101| Step: 0
Training loss: 2.763535737991333
Validation loss: 2.5250174409599713

Epoch: 5| Step: 1
Training loss: 2.205880641937256
Validation loss: 2.525141028947728

Epoch: 5| Step: 2
Training loss: 2.8879010677337646
Validation loss: 2.5163646410870295

Epoch: 5| Step: 3
Training loss: 2.4017932415008545
Validation loss: 2.5184103442776586

Epoch: 5| Step: 4
Training loss: 3.601059675216675
Validation loss: 2.5205725392987652

Epoch: 5| Step: 5
Training loss: 3.0851962566375732
Validation loss: 2.5152720353936635

Epoch: 5| Step: 6
Training loss: 3.261965274810791
Validation loss: 2.514726000447427

Epoch: 5| Step: 7
Training loss: 1.9547218084335327
Validation loss: 2.5074688567910144

Epoch: 5| Step: 8
Training loss: 2.9460699558258057
Validation loss: 2.5061414318699993

Epoch: 5| Step: 9
Training loss: 2.480800151824951
Validation loss: 2.506367568046816

Epoch: 5| Step: 10
Training loss: 2.022517681121826
Validation loss: 2.5030279287727932

Epoch: 102| Step: 0
Training loss: 2.1783769130706787
Validation loss: 2.504681860246966

Epoch: 5| Step: 1
Training loss: 3.2374377250671387
Validation loss: 2.506017384990569

Epoch: 5| Step: 2
Training loss: 2.8514320850372314
Validation loss: 2.5045456937564317

Epoch: 5| Step: 3
Training loss: 3.1538796424865723
Validation loss: 2.506977806809128

Epoch: 5| Step: 4
Training loss: 2.559847116470337
Validation loss: 2.5068134851353143

Epoch: 5| Step: 5
Training loss: 2.0077731609344482
Validation loss: 2.5109375343527844

Epoch: 5| Step: 6
Training loss: 2.9733164310455322
Validation loss: 2.5145327224526355

Epoch: 5| Step: 7
Training loss: 3.4707648754119873
Validation loss: 2.5154347547920803

Epoch: 5| Step: 8
Training loss: 2.537235975265503
Validation loss: 2.5095806608917894

Epoch: 5| Step: 9
Training loss: 1.8045133352279663
Validation loss: 2.5110591406463296

Epoch: 5| Step: 10
Training loss: 2.9023232460021973
Validation loss: 2.5025125126684866

Epoch: 103| Step: 0
Training loss: 2.2962498664855957
Validation loss: 2.5045615139827935

Epoch: 5| Step: 1
Training loss: 1.6641660928726196
Validation loss: 2.5073346681492303

Epoch: 5| Step: 2
Training loss: 2.9255502223968506
Validation loss: 2.5019784537694787

Epoch: 5| Step: 3
Training loss: 3.0708298683166504
Validation loss: 2.500484594734766

Epoch: 5| Step: 4
Training loss: 2.7120094299316406
Validation loss: 2.5091970505252963

Epoch: 5| Step: 5
Training loss: 2.296433210372925
Validation loss: 2.508612145659744

Epoch: 5| Step: 6
Training loss: 3.2970213890075684
Validation loss: 2.5045011120457805

Epoch: 5| Step: 7
Training loss: 2.4520068168640137
Validation loss: 2.506350555727559

Epoch: 5| Step: 8
Training loss: 2.957869052886963
Validation loss: 2.503761934977706

Epoch: 5| Step: 9
Training loss: 3.1568379402160645
Validation loss: 2.505347708220123

Epoch: 5| Step: 10
Training loss: 2.859818935394287
Validation loss: 2.5046115203570296

Epoch: 104| Step: 0
Training loss: 1.9958938360214233
Validation loss: 2.504227779244864

Epoch: 5| Step: 1
Training loss: 2.879838466644287
Validation loss: 2.5029198097926315

Epoch: 5| Step: 2
Training loss: 2.7811639308929443
Validation loss: 2.498831266997963

Epoch: 5| Step: 3
Training loss: 1.9068536758422852
Validation loss: 2.497443574731068

Epoch: 5| Step: 4
Training loss: 2.9678351879119873
Validation loss: 2.495777124999672

Epoch: 5| Step: 5
Training loss: 3.5189731121063232
Validation loss: 2.501114296656783

Epoch: 5| Step: 6
Training loss: 2.644686222076416
Validation loss: 2.514768810682399

Epoch: 5| Step: 7
Training loss: 2.6592824459075928
Validation loss: 2.526953428022323

Epoch: 5| Step: 8
Training loss: 2.3658971786499023
Validation loss: 2.541791780020601

Epoch: 5| Step: 9
Training loss: 3.151245355606079
Validation loss: 2.5410057960018033

Epoch: 5| Step: 10
Training loss: 2.7663376331329346
Validation loss: 2.5298410205430883

Epoch: 105| Step: 0
Training loss: 2.1438918113708496
Validation loss: 2.5242247017480994

Epoch: 5| Step: 1
Training loss: 3.309607744216919
Validation loss: 2.5107254289811656

Epoch: 5| Step: 2
Training loss: 3.0249345302581787
Validation loss: 2.502909121974822

Epoch: 5| Step: 3
Training loss: 2.6582446098327637
Validation loss: 2.510735851462169

Epoch: 5| Step: 4
Training loss: 2.866915225982666
Validation loss: 2.5061703292272424

Epoch: 5| Step: 5
Training loss: 1.9642479419708252
Validation loss: 2.507618834895472

Epoch: 5| Step: 6
Training loss: 2.674255847930908
Validation loss: 2.505708122766146

Epoch: 5| Step: 7
Training loss: 2.1430277824401855
Validation loss: 2.4985819324370353

Epoch: 5| Step: 8
Training loss: 2.419055223464966
Validation loss: 2.502377886925974

Epoch: 5| Step: 9
Training loss: 3.043311595916748
Validation loss: 2.502381311949863

Epoch: 5| Step: 10
Training loss: 3.475229501724243
Validation loss: 2.509214829373103

Epoch: 106| Step: 0
Training loss: 2.7046194076538086
Validation loss: 2.5114538874677432

Epoch: 5| Step: 1
Training loss: 3.26544451713562
Validation loss: 2.511647601281443

Epoch: 5| Step: 2
Training loss: 2.987387180328369
Validation loss: 2.5006521696685464

Epoch: 5| Step: 3
Training loss: 2.1623330116271973
Validation loss: 2.5019510458874445

Epoch: 5| Step: 4
Training loss: 3.061710834503174
Validation loss: 2.5018094739606305

Epoch: 5| Step: 5
Training loss: 2.2819275856018066
Validation loss: 2.5046272585468907

Epoch: 5| Step: 6
Training loss: 2.0790581703186035
Validation loss: 2.5100380477084907

Epoch: 5| Step: 7
Training loss: 2.4824116230010986
Validation loss: 2.5137017080860753

Epoch: 5| Step: 8
Training loss: 3.3704001903533936
Validation loss: 2.511739582143804

Epoch: 5| Step: 9
Training loss: 2.4133973121643066
Validation loss: 2.507983074393324

Epoch: 5| Step: 10
Training loss: 2.823761224746704
Validation loss: 2.490633700483589

Epoch: 107| Step: 0
Training loss: 2.032073736190796
Validation loss: 2.4898746885279173

Epoch: 5| Step: 1
Training loss: 2.1380341053009033
Validation loss: 2.4933067316650064

Epoch: 5| Step: 2
Training loss: 2.2686245441436768
Validation loss: 2.496449339774347

Epoch: 5| Step: 3
Training loss: 2.4483299255371094
Validation loss: 2.4938474214205177

Epoch: 5| Step: 4
Training loss: 2.4850211143493652
Validation loss: 2.49307579891656

Epoch: 5| Step: 5
Training loss: 3.5207343101501465
Validation loss: 2.490987629018804

Epoch: 5| Step: 6
Training loss: 2.6333603858947754
Validation loss: 2.4907580908908638

Epoch: 5| Step: 7
Training loss: 2.7606921195983887
Validation loss: 2.4874107453130905

Epoch: 5| Step: 8
Training loss: 3.0367679595947266
Validation loss: 2.4902743908666793

Epoch: 5| Step: 9
Training loss: 3.3121941089630127
Validation loss: 2.4916088504175984

Epoch: 5| Step: 10
Training loss: 2.971405267715454
Validation loss: 2.490491574810397

Epoch: 108| Step: 0
Training loss: 3.0329644680023193
Validation loss: 2.4939887010922996

Epoch: 5| Step: 1
Training loss: 2.4374146461486816
Validation loss: 2.4955143979800645

Epoch: 5| Step: 2
Training loss: 3.129293918609619
Validation loss: 2.4910700808289232

Epoch: 5| Step: 3
Training loss: 3.4304580688476562
Validation loss: 2.490982581210393

Epoch: 5| Step: 4
Training loss: 3.1095473766326904
Validation loss: 2.4950995611888107

Epoch: 5| Step: 5
Training loss: 2.7589566707611084
Validation loss: 2.4938987852424703

Epoch: 5| Step: 6
Training loss: 2.1218228340148926
Validation loss: 2.4955344917953655

Epoch: 5| Step: 7
Training loss: 2.14268159866333
Validation loss: 2.4929419050934496

Epoch: 5| Step: 8
Training loss: 2.250948429107666
Validation loss: 2.499037024795368

Epoch: 5| Step: 9
Training loss: 2.372255325317383
Validation loss: 2.5040880095574165

Epoch: 5| Step: 10
Training loss: 2.8070714473724365
Validation loss: 2.5003324324084866

Epoch: 109| Step: 0
Training loss: 2.97123384475708
Validation loss: 2.4964364677347164

Epoch: 5| Step: 1
Training loss: 2.8526535034179688
Validation loss: 2.486145942441879

Epoch: 5| Step: 2
Training loss: 2.7812933921813965
Validation loss: 2.484064389300603

Epoch: 5| Step: 3
Training loss: 2.2003989219665527
Validation loss: 2.486462585387691

Epoch: 5| Step: 4
Training loss: 2.7043817043304443
Validation loss: 2.4930267257075154

Epoch: 5| Step: 5
Training loss: 2.1861817836761475
Validation loss: 2.4926776398894606

Epoch: 5| Step: 6
Training loss: 2.7592296600341797
Validation loss: 2.489615450623215

Epoch: 5| Step: 7
Training loss: 2.3606019020080566
Validation loss: 2.486857586009528

Epoch: 5| Step: 8
Training loss: 3.7429325580596924
Validation loss: 2.486554202213082

Epoch: 5| Step: 9
Training loss: 2.293266773223877
Validation loss: 2.487083158185405

Epoch: 5| Step: 10
Training loss: 2.685049057006836
Validation loss: 2.492232738002654

Epoch: 110| Step: 0
Training loss: 2.5529191493988037
Validation loss: 2.4949008675031763

Epoch: 5| Step: 1
Training loss: 2.9758074283599854
Validation loss: 2.5091538993261193

Epoch: 5| Step: 2
Training loss: 2.1834042072296143
Validation loss: 2.5171677527889127

Epoch: 5| Step: 3
Training loss: 3.652329921722412
Validation loss: 2.5114843896640244

Epoch: 5| Step: 4
Training loss: 2.0618715286254883
Validation loss: 2.5108095945850497

Epoch: 5| Step: 5
Training loss: 3.437976121902466
Validation loss: 2.4993356786748415

Epoch: 5| Step: 6
Training loss: 2.9194273948669434
Validation loss: 2.48856540905532

Epoch: 5| Step: 7
Training loss: 2.1610898971557617
Validation loss: 2.4792628057541384

Epoch: 5| Step: 8
Training loss: 2.6793622970581055
Validation loss: 2.4820738966746996

Epoch: 5| Step: 9
Training loss: 2.075676441192627
Validation loss: 2.4865835994802494

Epoch: 5| Step: 10
Training loss: 2.9936537742614746
Validation loss: 2.486615393751411

Epoch: 111| Step: 0
Training loss: 2.4014651775360107
Validation loss: 2.4918775379016833

Epoch: 5| Step: 1
Training loss: 3.0401203632354736
Validation loss: 2.493814173565116

Epoch: 5| Step: 2
Training loss: 2.8601021766662598
Validation loss: 2.501001024758944

Epoch: 5| Step: 3
Training loss: 2.208085298538208
Validation loss: 2.4931529106632357

Epoch: 5| Step: 4
Training loss: 2.8673999309539795
Validation loss: 2.486554258613176

Epoch: 5| Step: 5
Training loss: 2.6592459678649902
Validation loss: 2.480941508405952

Epoch: 5| Step: 6
Training loss: 2.42474102973938
Validation loss: 2.4846849544073946

Epoch: 5| Step: 7
Training loss: 2.715073585510254
Validation loss: 2.484425083283455

Epoch: 5| Step: 8
Training loss: 2.313292980194092
Validation loss: 2.4921271160084713

Epoch: 5| Step: 9
Training loss: 3.031200647354126
Validation loss: 2.4997787783222813

Epoch: 5| Step: 10
Training loss: 3.0650346279144287
Validation loss: 2.5013959587261243

Epoch: 112| Step: 0
Training loss: 2.4882521629333496
Validation loss: 2.5051198723495647

Epoch: 5| Step: 1
Training loss: 2.5296385288238525
Validation loss: 2.51008653384383

Epoch: 5| Step: 2
Training loss: 2.3650717735290527
Validation loss: 2.5210264600733274

Epoch: 5| Step: 3
Training loss: 2.6934876441955566
Validation loss: 2.5199993271981516

Epoch: 5| Step: 4
Training loss: 3.0447378158569336
Validation loss: 2.5169268654238794

Epoch: 5| Step: 5
Training loss: 2.7370104789733887
Validation loss: 2.496198574701945

Epoch: 5| Step: 6
Training loss: 2.4701626300811768
Validation loss: 2.490700965286583

Epoch: 5| Step: 7
Training loss: 2.675812005996704
Validation loss: 2.486742127326227

Epoch: 5| Step: 8
Training loss: 2.467808485031128
Validation loss: 2.478863726380051

Epoch: 5| Step: 9
Training loss: 3.041827440261841
Validation loss: 2.483036518096924

Epoch: 5| Step: 10
Training loss: 2.997215747833252
Validation loss: 2.490599593808574

Epoch: 113| Step: 0
Training loss: 3.662766695022583
Validation loss: 2.489379629012077

Epoch: 5| Step: 1
Training loss: 2.7273964881896973
Validation loss: 2.492955030933503

Epoch: 5| Step: 2
Training loss: 2.6695258617401123
Validation loss: 2.4875527581860943

Epoch: 5| Step: 3
Training loss: 2.9824512004852295
Validation loss: 2.4852461097060994

Epoch: 5| Step: 4
Training loss: 2.402805805206299
Validation loss: 2.4818163533364572

Epoch: 5| Step: 5
Training loss: 3.147547960281372
Validation loss: 2.4758914465545327

Epoch: 5| Step: 6
Training loss: 2.294800043106079
Validation loss: 2.473900966746833

Epoch: 5| Step: 7
Training loss: 1.8249642848968506
Validation loss: 2.4759096586576073

Epoch: 5| Step: 8
Training loss: 2.997600555419922
Validation loss: 2.479044154126157

Epoch: 5| Step: 9
Training loss: 2.244293689727783
Validation loss: 2.4837257605727

Epoch: 5| Step: 10
Training loss: 2.587716579437256
Validation loss: 2.4828614445142847

Epoch: 114| Step: 0
Training loss: 2.884979248046875
Validation loss: 2.4855452173499653

Epoch: 5| Step: 1
Training loss: 3.13653564453125
Validation loss: 2.48264959678855

Epoch: 5| Step: 2
Training loss: 3.1218271255493164
Validation loss: 2.4788515388324694

Epoch: 5| Step: 3
Training loss: 2.5866103172302246
Validation loss: 2.4754001145721762

Epoch: 5| Step: 4
Training loss: 3.2578113079071045
Validation loss: 2.4742835542207122

Epoch: 5| Step: 5
Training loss: 1.7728668451309204
Validation loss: 2.469958664268576

Epoch: 5| Step: 6
Training loss: 2.643007755279541
Validation loss: 2.4684108354712047

Epoch: 5| Step: 7
Training loss: 2.180626153945923
Validation loss: 2.469792240409441

Epoch: 5| Step: 8
Training loss: 2.804952383041382
Validation loss: 2.469866603933355

Epoch: 5| Step: 9
Training loss: 2.5207715034484863
Validation loss: 2.4753942335805585

Epoch: 5| Step: 10
Training loss: 2.4059743881225586
Validation loss: 2.473075718008062

Epoch: 115| Step: 0
Training loss: 2.5573601722717285
Validation loss: 2.477348904455862

Epoch: 5| Step: 1
Training loss: 2.971102476119995
Validation loss: 2.4894697102167274

Epoch: 5| Step: 2
Training loss: 2.5500285625457764
Validation loss: 2.4930705460168983

Epoch: 5| Step: 3
Training loss: 2.2365527153015137
Validation loss: 2.4799754568325576

Epoch: 5| Step: 4
Training loss: 2.562483549118042
Validation loss: 2.473997146852555

Epoch: 5| Step: 5
Training loss: 2.875973701477051
Validation loss: 2.4700279158930623

Epoch: 5| Step: 6
Training loss: 3.022212266921997
Validation loss: 2.4676484190007693

Epoch: 5| Step: 7
Training loss: 2.073802947998047
Validation loss: 2.470669787417176

Epoch: 5| Step: 8
Training loss: 3.0966458320617676
Validation loss: 2.4806898947684997

Epoch: 5| Step: 9
Training loss: 2.7678418159484863
Validation loss: 2.4837303212893906

Epoch: 5| Step: 10
Training loss: 2.7216501235961914
Validation loss: 2.4779312866990284

Epoch: 116| Step: 0
Training loss: 3.4035181999206543
Validation loss: 2.475314368483841

Epoch: 5| Step: 1
Training loss: 2.0178987979888916
Validation loss: 2.4780967158655964

Epoch: 5| Step: 2
Training loss: 3.1517608165740967
Validation loss: 2.474979272452734

Epoch: 5| Step: 3
Training loss: 2.3276500701904297
Validation loss: 2.47742155034055

Epoch: 5| Step: 4
Training loss: 2.4590561389923096
Validation loss: 2.480404364165439

Epoch: 5| Step: 5
Training loss: 2.8624114990234375
Validation loss: 2.497899055480957

Epoch: 5| Step: 6
Training loss: 2.819185256958008
Validation loss: 2.5049847146516204

Epoch: 5| Step: 7
Training loss: 2.418574571609497
Validation loss: 2.5060357150211128

Epoch: 5| Step: 8
Training loss: 2.288407564163208
Validation loss: 2.51388983316319

Epoch: 5| Step: 9
Training loss: 2.9708542823791504
Validation loss: 2.5037350116237516

Epoch: 5| Step: 10
Training loss: 2.686155319213867
Validation loss: 2.5051637285499164

Epoch: 117| Step: 0
Training loss: 2.9364025592803955
Validation loss: 2.492062473809847

Epoch: 5| Step: 1
Training loss: 2.8145413398742676
Validation loss: 2.482867504960747

Epoch: 5| Step: 2
Training loss: 2.87652587890625
Validation loss: 2.48010604868653

Epoch: 5| Step: 3
Training loss: 3.4289753437042236
Validation loss: 2.4773183330412833

Epoch: 5| Step: 4
Training loss: 2.470811367034912
Validation loss: 2.472163846415858

Epoch: 5| Step: 5
Training loss: 2.673067569732666
Validation loss: 2.4703325225460913

Epoch: 5| Step: 6
Training loss: 2.272468090057373
Validation loss: 2.4728108529121644

Epoch: 5| Step: 7
Training loss: 3.0845253467559814
Validation loss: 2.4697227272936093

Epoch: 5| Step: 8
Training loss: 2.8124632835388184
Validation loss: 2.4704489938674437

Epoch: 5| Step: 9
Training loss: 1.9053850173950195
Validation loss: 2.475006934135191

Epoch: 5| Step: 10
Training loss: 1.978367567062378
Validation loss: 2.480362481968377

Epoch: 118| Step: 0
Training loss: 2.8324408531188965
Validation loss: 2.487701931307393

Epoch: 5| Step: 1
Training loss: 1.8416812419891357
Validation loss: 2.4945493359719553

Epoch: 5| Step: 2
Training loss: 3.147855281829834
Validation loss: 2.4915486510081957

Epoch: 5| Step: 3
Training loss: 2.223982572555542
Validation loss: 2.4875086071670696

Epoch: 5| Step: 4
Training loss: 2.3827877044677734
Validation loss: 2.486599983707551

Epoch: 5| Step: 5
Training loss: 3.2757067680358887
Validation loss: 2.48363422834745

Epoch: 5| Step: 6
Training loss: 2.737642288208008
Validation loss: 2.4733036154059955

Epoch: 5| Step: 7
Training loss: 2.5733113288879395
Validation loss: 2.4733836599575576

Epoch: 5| Step: 8
Training loss: 3.340977907180786
Validation loss: 2.4626533318591375

Epoch: 5| Step: 9
Training loss: 2.6553685665130615
Validation loss: 2.4651671814662155

Epoch: 5| Step: 10
Training loss: 2.349125623703003
Validation loss: 2.464584453131563

Epoch: 119| Step: 0
Training loss: 2.8435325622558594
Validation loss: 2.4671127155262935

Epoch: 5| Step: 1
Training loss: 3.4073116779327393
Validation loss: 2.465617702853295

Epoch: 5| Step: 2
Training loss: 2.6866354942321777
Validation loss: 2.4635527621033373

Epoch: 5| Step: 3
Training loss: 2.6157596111297607
Validation loss: 2.4674596273770897

Epoch: 5| Step: 4
Training loss: 2.2157177925109863
Validation loss: 2.4693140586217246

Epoch: 5| Step: 5
Training loss: 2.4264543056488037
Validation loss: 2.463693085537162

Epoch: 5| Step: 6
Training loss: 2.3634328842163086
Validation loss: 2.4733945067210863

Epoch: 5| Step: 7
Training loss: 3.39727783203125
Validation loss: 2.487935743024272

Epoch: 5| Step: 8
Training loss: 2.9466514587402344
Validation loss: 2.492927348741921

Epoch: 5| Step: 9
Training loss: 1.9414288997650146
Validation loss: 2.5086696109464093

Epoch: 5| Step: 10
Training loss: 2.4757866859436035
Validation loss: 2.507036978198636

Epoch: 120| Step: 0
Training loss: 2.604843854904175
Validation loss: 2.5096349998186995

Epoch: 5| Step: 1
Training loss: 2.8238685131073
Validation loss: 2.511879753041011

Epoch: 5| Step: 2
Training loss: 2.1695525646209717
Validation loss: 2.506260753959738

Epoch: 5| Step: 3
Training loss: 2.9234602451324463
Validation loss: 2.5039994921735538

Epoch: 5| Step: 4
Training loss: 2.0868589878082275
Validation loss: 2.4824463218771

Epoch: 5| Step: 5
Training loss: 2.3142852783203125
Validation loss: 2.469908163111697

Epoch: 5| Step: 6
Training loss: 2.9505722522735596
Validation loss: 2.4544664454716507

Epoch: 5| Step: 7
Training loss: 2.611536741256714
Validation loss: 2.454087357367239

Epoch: 5| Step: 8
Training loss: 2.7288057804107666
Validation loss: 2.45641634028445

Epoch: 5| Step: 9
Training loss: 3.4709343910217285
Validation loss: 2.463906445810872

Epoch: 5| Step: 10
Training loss: 2.721869945526123
Validation loss: 2.467859375861383

Epoch: 121| Step: 0
Training loss: 2.504974842071533
Validation loss: 2.4741491374149116

Epoch: 5| Step: 1
Training loss: 2.710843563079834
Validation loss: 2.482818308696952

Epoch: 5| Step: 2
Training loss: 3.510685682296753
Validation loss: 2.482448721444735

Epoch: 5| Step: 3
Training loss: 2.491720199584961
Validation loss: 2.471186578914683

Epoch: 5| Step: 4
Training loss: 2.6801116466522217
Validation loss: 2.4623969729228685

Epoch: 5| Step: 5
Training loss: 3.0459704399108887
Validation loss: 2.454580776153072

Epoch: 5| Step: 6
Training loss: 2.8895630836486816
Validation loss: 2.45161868679908

Epoch: 5| Step: 7
Training loss: 2.456747531890869
Validation loss: 2.4598482244758197

Epoch: 5| Step: 8
Training loss: 1.6821746826171875
Validation loss: 2.459784182169104

Epoch: 5| Step: 9
Training loss: 2.8481287956237793
Validation loss: 2.470616638019521

Epoch: 5| Step: 10
Training loss: 2.5423784255981445
Validation loss: 2.4785096440263974

Epoch: 122| Step: 0
Training loss: 2.6029858589172363
Validation loss: 2.4794764275191934

Epoch: 5| Step: 1
Training loss: 3.1093478202819824
Validation loss: 2.490577669553859

Epoch: 5| Step: 2
Training loss: 2.562756299972534
Validation loss: 2.479705277309623

Epoch: 5| Step: 3
Training loss: 2.61885404586792
Validation loss: 2.4755641927001295

Epoch: 5| Step: 4
Training loss: 2.995635509490967
Validation loss: 2.4622283930419595

Epoch: 5| Step: 5
Training loss: 2.3875555992126465
Validation loss: 2.4591307870803343

Epoch: 5| Step: 6
Training loss: 3.072178363800049
Validation loss: 2.456096754279188

Epoch: 5| Step: 7
Training loss: 2.242143392562866
Validation loss: 2.455346361283333

Epoch: 5| Step: 8
Training loss: 2.049259901046753
Validation loss: 2.457279171994937

Epoch: 5| Step: 9
Training loss: 2.8772549629211426
Validation loss: 2.4599717945180912

Epoch: 5| Step: 10
Training loss: 2.9345390796661377
Validation loss: 2.465115817644263

Epoch: 123| Step: 0
Training loss: 2.817180633544922
Validation loss: 2.464599178683373

Epoch: 5| Step: 1
Training loss: 2.8929340839385986
Validation loss: 2.4688610594759703

Epoch: 5| Step: 2
Training loss: 2.3717849254608154
Validation loss: 2.4710170325412544

Epoch: 5| Step: 3
Training loss: 2.6870763301849365
Validation loss: 2.4721672970761537

Epoch: 5| Step: 4
Training loss: 2.466157913208008
Validation loss: 2.4709756194904284

Epoch: 5| Step: 5
Training loss: 2.4215927124023438
Validation loss: 2.475152951414867

Epoch: 5| Step: 6
Training loss: 2.7337357997894287
Validation loss: 2.474951708188621

Epoch: 5| Step: 7
Training loss: 2.772249937057495
Validation loss: 2.482763223750617

Epoch: 5| Step: 8
Training loss: 2.761989116668701
Validation loss: 2.48495069883203

Epoch: 5| Step: 9
Training loss: 2.663926601409912
Validation loss: 2.4855557564766175

Epoch: 5| Step: 10
Training loss: 2.782294988632202
Validation loss: 2.4871139885276876

Epoch: 124| Step: 0
Training loss: 2.263136863708496
Validation loss: 2.472613832002045

Epoch: 5| Step: 1
Training loss: 2.2119126319885254
Validation loss: 2.4661573799707557

Epoch: 5| Step: 2
Training loss: 2.3176236152648926
Validation loss: 2.459749156428922

Epoch: 5| Step: 3
Training loss: 2.591724157333374
Validation loss: 2.463291939868722

Epoch: 5| Step: 4
Training loss: 3.413585662841797
Validation loss: 2.459581290521929

Epoch: 5| Step: 5
Training loss: 2.4977800846099854
Validation loss: 2.462034915083198

Epoch: 5| Step: 6
Training loss: 2.6873481273651123
Validation loss: 2.457320743991483

Epoch: 5| Step: 7
Training loss: 2.2650504112243652
Validation loss: 2.460725333101006

Epoch: 5| Step: 8
Training loss: 2.441816568374634
Validation loss: 2.4599724738828597

Epoch: 5| Step: 9
Training loss: 3.071981906890869
Validation loss: 2.4523628245117846

Epoch: 5| Step: 10
Training loss: 3.5775046348571777
Validation loss: 2.454596947598201

Epoch: 125| Step: 0
Training loss: 2.6462044715881348
Validation loss: 2.4491361059168333

Epoch: 5| Step: 1
Training loss: 2.604750156402588
Validation loss: 2.4450915321227042

Epoch: 5| Step: 2
Training loss: 3.0620999336242676
Validation loss: 2.445297702666252

Epoch: 5| Step: 3
Training loss: 2.272127151489258
Validation loss: 2.4497469573892574

Epoch: 5| Step: 4
Training loss: 2.840217113494873
Validation loss: 2.4453694384585143

Epoch: 5| Step: 5
Training loss: 2.6257095336914062
Validation loss: 2.4458670052148963

Epoch: 5| Step: 6
Training loss: 2.785975217819214
Validation loss: 2.4415661417027956

Epoch: 5| Step: 7
Training loss: 2.7328336238861084
Validation loss: 2.4440602820406676

Epoch: 5| Step: 8
Training loss: 2.6053898334503174
Validation loss: 2.4374627913198164

Epoch: 5| Step: 9
Training loss: 3.028873920440674
Validation loss: 2.444273205213649

Epoch: 5| Step: 10
Training loss: 1.979166865348816
Validation loss: 2.4360301648416827

Epoch: 126| Step: 0
Training loss: 3.432647705078125
Validation loss: 2.435640419683149

Epoch: 5| Step: 1
Training loss: 2.3775038719177246
Validation loss: 2.43649975715145

Epoch: 5| Step: 2
Training loss: 1.8676170110702515
Validation loss: 2.4369593025535665

Epoch: 5| Step: 3
Training loss: 2.256171941757202
Validation loss: 2.4421503441308134

Epoch: 5| Step: 4
Training loss: 2.610445499420166
Validation loss: 2.4534076516346266

Epoch: 5| Step: 5
Training loss: 2.2641525268554688
Validation loss: 2.464058745291925

Epoch: 5| Step: 6
Training loss: 3.0500881671905518
Validation loss: 2.475012110125634

Epoch: 5| Step: 7
Training loss: 2.595338821411133
Validation loss: 2.4908734880467898

Epoch: 5| Step: 8
Training loss: 2.4709625244140625
Validation loss: 2.505306374642157

Epoch: 5| Step: 9
Training loss: 3.2004897594451904
Validation loss: 2.5025283957040436

Epoch: 5| Step: 10
Training loss: 3.26678466796875
Validation loss: 2.488350442660752

Epoch: 127| Step: 0
Training loss: 2.385042428970337
Validation loss: 2.471642053255471

Epoch: 5| Step: 1
Training loss: 2.262357711791992
Validation loss: 2.455982618434455

Epoch: 5| Step: 2
Training loss: 2.562162160873413
Validation loss: 2.4532697713503273

Epoch: 5| Step: 3
Training loss: 2.305438756942749
Validation loss: 2.4530293915861394

Epoch: 5| Step: 4
Training loss: 3.0827364921569824
Validation loss: 2.453698919665429

Epoch: 5| Step: 5
Training loss: 2.777864933013916
Validation loss: 2.455808424180554

Epoch: 5| Step: 6
Training loss: 1.926736831665039
Validation loss: 2.458611314014722

Epoch: 5| Step: 7
Training loss: 3.203341007232666
Validation loss: 2.4660523040320284

Epoch: 5| Step: 8
Training loss: 3.476421356201172
Validation loss: 2.461600165213308

Epoch: 5| Step: 9
Training loss: 2.905545711517334
Validation loss: 2.456729176223919

Epoch: 5| Step: 10
Training loss: 2.3371615409851074
Validation loss: 2.4571444629341044

Epoch: 128| Step: 0
Training loss: 2.8891212940216064
Validation loss: 2.448382480170137

Epoch: 5| Step: 1
Training loss: 2.6942849159240723
Validation loss: 2.449456061086347

Epoch: 5| Step: 2
Training loss: 2.4490256309509277
Validation loss: 2.4433058897654214

Epoch: 5| Step: 3
Training loss: 2.610448122024536
Validation loss: 2.44553368835039

Epoch: 5| Step: 4
Training loss: 2.938032627105713
Validation loss: 2.4492033707198275

Epoch: 5| Step: 5
Training loss: 3.2529330253601074
Validation loss: 2.4495812590404222

Epoch: 5| Step: 6
Training loss: 2.25520658493042
Validation loss: 2.4578957557678223

Epoch: 5| Step: 7
Training loss: 2.294099807739258
Validation loss: 2.4596876123900056

Epoch: 5| Step: 8
Training loss: 2.157501220703125
Validation loss: 2.4643034396633023

Epoch: 5| Step: 9
Training loss: 3.077697277069092
Validation loss: 2.4639159402539654

Epoch: 5| Step: 10
Training loss: 2.6169815063476562
Validation loss: 2.4539779898940877

Epoch: 129| Step: 0
Training loss: 2.1632983684539795
Validation loss: 2.459723516177106

Epoch: 5| Step: 1
Training loss: 2.7680137157440186
Validation loss: 2.4627455024309057

Epoch: 5| Step: 2
Training loss: 2.3584389686584473
Validation loss: 2.461241311924432

Epoch: 5| Step: 3
Training loss: 2.9134202003479004
Validation loss: 2.4702214528155584

Epoch: 5| Step: 4
Training loss: 2.5784976482391357
Validation loss: 2.4619401924071775

Epoch: 5| Step: 5
Training loss: 2.9521870613098145
Validation loss: 2.4557555362742436

Epoch: 5| Step: 6
Training loss: 2.2081096172332764
Validation loss: 2.459049695281572

Epoch: 5| Step: 7
Training loss: 3.704657793045044
Validation loss: 2.4468572037194365

Epoch: 5| Step: 8
Training loss: 2.581798553466797
Validation loss: 2.43998896691107

Epoch: 5| Step: 9
Training loss: 2.105787754058838
Validation loss: 2.435146988079112

Epoch: 5| Step: 10
Training loss: 2.9039268493652344
Validation loss: 2.43321729219088

Epoch: 130| Step: 0
Training loss: 2.5154354572296143
Validation loss: 2.426187453731414

Epoch: 5| Step: 1
Training loss: 2.423691987991333
Validation loss: 2.426462181152836

Epoch: 5| Step: 2
Training loss: 2.811692714691162
Validation loss: 2.428831741374026

Epoch: 5| Step: 3
Training loss: 2.928622007369995
Validation loss: 2.4412600917200886

Epoch: 5| Step: 4
Training loss: 2.8814423084259033
Validation loss: 2.473556000699279

Epoch: 5| Step: 5
Training loss: 2.451340913772583
Validation loss: 2.505850685540066

Epoch: 5| Step: 6
Training loss: 2.7074074745178223
Validation loss: 2.4875254323405604

Epoch: 5| Step: 7
Training loss: 2.382800817489624
Validation loss: 2.445812059986976

Epoch: 5| Step: 8
Training loss: 2.8038887977600098
Validation loss: 2.436898257142754

Epoch: 5| Step: 9
Training loss: 2.4962706565856934
Validation loss: 2.424957303590672

Epoch: 5| Step: 10
Training loss: 3.0571773052215576
Validation loss: 2.432752041406529

Epoch: 131| Step: 0
Training loss: 3.0782628059387207
Validation loss: 2.438805023829142

Epoch: 5| Step: 1
Training loss: 2.4488911628723145
Validation loss: 2.4455329833492154

Epoch: 5| Step: 2
Training loss: 2.531968593597412
Validation loss: 2.4452815286574827

Epoch: 5| Step: 3
Training loss: 2.386291265487671
Validation loss: 2.44380174913714

Epoch: 5| Step: 4
Training loss: 3.1893627643585205
Validation loss: 2.4473440288215556

Epoch: 5| Step: 5
Training loss: 2.4601292610168457
Validation loss: 2.4481548442635486

Epoch: 5| Step: 6
Training loss: 2.747175931930542
Validation loss: 2.441398610350906

Epoch: 5| Step: 7
Training loss: 2.451449155807495
Validation loss: 2.4329487969798427

Epoch: 5| Step: 8
Training loss: 2.3034682273864746
Validation loss: 2.4248005138930453

Epoch: 5| Step: 9
Training loss: 2.8537049293518066
Validation loss: 2.4244056414532404

Epoch: 5| Step: 10
Training loss: 2.969179153442383
Validation loss: 2.422887832887711

Epoch: 132| Step: 0
Training loss: 3.3109450340270996
Validation loss: 2.4306256565996396

Epoch: 5| Step: 1
Training loss: 3.1135170459747314
Validation loss: 2.4362392938265236

Epoch: 5| Step: 2
Training loss: 2.434079647064209
Validation loss: 2.4435123064184703

Epoch: 5| Step: 3
Training loss: 2.68587327003479
Validation loss: 2.4534696802016227

Epoch: 5| Step: 4
Training loss: 2.8212406635284424
Validation loss: 2.4565097875492548

Epoch: 5| Step: 5
Training loss: 2.7421271800994873
Validation loss: 2.467907469759705

Epoch: 5| Step: 6
Training loss: 2.2541720867156982
Validation loss: 2.4629915709136636

Epoch: 5| Step: 7
Training loss: 2.4455454349517822
Validation loss: 2.4617911987407233

Epoch: 5| Step: 8
Training loss: 2.5837454795837402
Validation loss: 2.456841074010377

Epoch: 5| Step: 9
Training loss: 2.348832607269287
Validation loss: 2.4551027590228665

Epoch: 5| Step: 10
Training loss: 2.596738815307617
Validation loss: 2.432594633871509

Epoch: 133| Step: 0
Training loss: 2.638134002685547
Validation loss: 2.4276138967083347

Epoch: 5| Step: 1
Training loss: 2.1629695892333984
Validation loss: 2.429310260280486

Epoch: 5| Step: 2
Training loss: 2.4397053718566895
Validation loss: 2.4271852585577194

Epoch: 5| Step: 3
Training loss: 2.3503425121307373
Validation loss: 2.433661148112307

Epoch: 5| Step: 4
Training loss: 2.115027904510498
Validation loss: 2.433065511847055

Epoch: 5| Step: 5
Training loss: 2.6647305488586426
Validation loss: 2.433085713335263

Epoch: 5| Step: 6
Training loss: 3.284440517425537
Validation loss: 2.4261772145507154

Epoch: 5| Step: 7
Training loss: 3.1411337852478027
Validation loss: 2.421735732786117

Epoch: 5| Step: 8
Training loss: 2.62195086479187
Validation loss: 2.4194782805699173

Epoch: 5| Step: 9
Training loss: 2.6776251792907715
Validation loss: 2.416088791303737

Epoch: 5| Step: 10
Training loss: 3.190004348754883
Validation loss: 2.4150907403679303

Epoch: 134| Step: 0
Training loss: 2.411444664001465
Validation loss: 2.413570988562799

Epoch: 5| Step: 1
Training loss: 2.7025418281555176
Validation loss: 2.416738292222382

Epoch: 5| Step: 2
Training loss: 2.7506682872772217
Validation loss: 2.4149906891648487

Epoch: 5| Step: 3
Training loss: 2.904796600341797
Validation loss: 2.420023118295977

Epoch: 5| Step: 4
Training loss: 3.1833457946777344
Validation loss: 2.4238133738117833

Epoch: 5| Step: 5
Training loss: 2.158942461013794
Validation loss: 2.4447720819903958

Epoch: 5| Step: 6
Training loss: 2.688255548477173
Validation loss: 2.4616530326104935

Epoch: 5| Step: 7
Training loss: 2.8452000617980957
Validation loss: 2.5017582011479202

Epoch: 5| Step: 8
Training loss: 2.4122986793518066
Validation loss: 2.5136533860237367

Epoch: 5| Step: 9
Training loss: 2.6422247886657715
Validation loss: 2.5137378990009265

Epoch: 5| Step: 10
Training loss: 2.6368045806884766
Validation loss: 2.499472551448371

Epoch: 135| Step: 0
Training loss: 2.606470823287964
Validation loss: 2.4773871475650417

Epoch: 5| Step: 1
Training loss: 2.5234603881835938
Validation loss: 2.44152670778254

Epoch: 5| Step: 2
Training loss: 1.9253971576690674
Validation loss: 2.4310763394960793

Epoch: 5| Step: 3
Training loss: 3.2215423583984375
Validation loss: 2.422859763586393

Epoch: 5| Step: 4
Training loss: 2.46980357170105
Validation loss: 2.4393066949741815

Epoch: 5| Step: 5
Training loss: 2.5481369495391846
Validation loss: 2.468392405458676

Epoch: 5| Step: 6
Training loss: 3.0227198600769043
Validation loss: 2.492345466408678

Epoch: 5| Step: 7
Training loss: 3.2432193756103516
Validation loss: 2.459639923546904

Epoch: 5| Step: 8
Training loss: 3.1661641597747803
Validation loss: 2.439346600604314

Epoch: 5| Step: 9
Training loss: 1.8100839853286743
Validation loss: 2.42225020675249

Epoch: 5| Step: 10
Training loss: 2.820918321609497
Validation loss: 2.4150513090113157

Epoch: 136| Step: 0
Training loss: 2.686049699783325
Validation loss: 2.4123910345057005

Epoch: 5| Step: 1
Training loss: 2.2854864597320557
Validation loss: 2.4215933763852684

Epoch: 5| Step: 2
Training loss: 2.652949094772339
Validation loss: 2.4429273759165118

Epoch: 5| Step: 3
Training loss: 2.9343578815460205
Validation loss: 2.470737013765561

Epoch: 5| Step: 4
Training loss: 2.8639893531799316
Validation loss: 2.4906175264748196

Epoch: 5| Step: 5
Training loss: 2.6770331859588623
Validation loss: 2.520657290694534

Epoch: 5| Step: 6
Training loss: 2.8325772285461426
Validation loss: 2.5279339769835114

Epoch: 5| Step: 7
Training loss: 3.35564923286438
Validation loss: 2.5202075922360985

Epoch: 5| Step: 8
Training loss: 2.818819522857666
Validation loss: 2.490684037567467

Epoch: 5| Step: 9
Training loss: 2.174712896347046
Validation loss: 2.4578543555351997

Epoch: 5| Step: 10
Training loss: 2.0536441802978516
Validation loss: 2.434965988641144

Epoch: 137| Step: 0
Training loss: 2.3644752502441406
Validation loss: 2.4157163507194928

Epoch: 5| Step: 1
Training loss: 2.6162657737731934
Validation loss: 2.4157005074203655

Epoch: 5| Step: 2
Training loss: 2.9019854068756104
Validation loss: 2.430031709773566

Epoch: 5| Step: 3
Training loss: 2.3582615852355957
Validation loss: 2.4300258031455417

Epoch: 5| Step: 4
Training loss: 3.2016005516052246
Validation loss: 2.435956665264663

Epoch: 5| Step: 5
Training loss: 2.878816843032837
Validation loss: 2.439653524788477

Epoch: 5| Step: 6
Training loss: 3.146099805831909
Validation loss: 2.4333903071700886

Epoch: 5| Step: 7
Training loss: 2.960232734680176
Validation loss: 2.4341000100617767

Epoch: 5| Step: 8
Training loss: 2.4963791370391846
Validation loss: 2.433297816143241

Epoch: 5| Step: 9
Training loss: 2.308624744415283
Validation loss: 2.424880578953733

Epoch: 5| Step: 10
Training loss: 1.9815925359725952
Validation loss: 2.420138600052044

Epoch: 138| Step: 0
Training loss: 2.8476436138153076
Validation loss: 2.4151214835464314

Epoch: 5| Step: 1
Training loss: 2.428121328353882
Validation loss: 2.414825854762908

Epoch: 5| Step: 2
Training loss: 2.7657358646392822
Validation loss: 2.4168017243826263

Epoch: 5| Step: 3
Training loss: 2.100037097930908
Validation loss: 2.4228856717386553

Epoch: 5| Step: 4
Training loss: 2.1481001377105713
Validation loss: 2.4395661405337754

Epoch: 5| Step: 5
Training loss: 2.896872043609619
Validation loss: 2.4472878338188253

Epoch: 5| Step: 6
Training loss: 2.7269134521484375
Validation loss: 2.455278427370133

Epoch: 5| Step: 7
Training loss: 3.009396553039551
Validation loss: 2.4447228062537407

Epoch: 5| Step: 8
Training loss: 2.322162628173828
Validation loss: 2.4228771476335424

Epoch: 5| Step: 9
Training loss: 2.6833672523498535
Validation loss: 2.4191868125751452

Epoch: 5| Step: 10
Training loss: 3.3179807662963867
Validation loss: 2.4200922186656664

Epoch: 139| Step: 0
Training loss: 2.9840564727783203
Validation loss: 2.420471219606297

Epoch: 5| Step: 1
Training loss: 2.8515090942382812
Validation loss: 2.4146134878999446

Epoch: 5| Step: 2
Training loss: 2.5595669746398926
Validation loss: 2.4247665559091875

Epoch: 5| Step: 3
Training loss: 2.3352913856506348
Validation loss: 2.4113754072496967

Epoch: 5| Step: 4
Training loss: 2.798940658569336
Validation loss: 2.4117251109051447

Epoch: 5| Step: 5
Training loss: 2.3063292503356934
Validation loss: 2.406452489155595

Epoch: 5| Step: 6
Training loss: 2.4757511615753174
Validation loss: 2.3993179541762157

Epoch: 5| Step: 7
Training loss: 2.209273099899292
Validation loss: 2.401984860820155

Epoch: 5| Step: 8
Training loss: 3.1259446144104004
Validation loss: 2.4002886972119732

Epoch: 5| Step: 9
Training loss: 3.0651967525482178
Validation loss: 2.4057733679330475

Epoch: 5| Step: 10
Training loss: 2.4215614795684814
Validation loss: 2.4107013299900997

Epoch: 140| Step: 0
Training loss: 3.152439594268799
Validation loss: 2.4112328662667224

Epoch: 5| Step: 1
Training loss: 2.76102352142334
Validation loss: 2.40466901563829

Epoch: 5| Step: 2
Training loss: 2.5044703483581543
Validation loss: 2.4034240348364717

Epoch: 5| Step: 3
Training loss: 2.462528705596924
Validation loss: 2.403403669275263

Epoch: 5| Step: 4
Training loss: 2.685469388961792
Validation loss: 2.4004370474046275

Epoch: 5| Step: 5
Training loss: 2.575559139251709
Validation loss: 2.402681471199118

Epoch: 5| Step: 6
Training loss: 2.550961971282959
Validation loss: 2.401831675601262

Epoch: 5| Step: 7
Training loss: 2.7985095977783203
Validation loss: 2.401535144416235

Epoch: 5| Step: 8
Training loss: 2.200627326965332
Validation loss: 2.4114421695791264

Epoch: 5| Step: 9
Training loss: 2.664761781692505
Validation loss: 2.409696917380056

Epoch: 5| Step: 10
Training loss: 2.7573091983795166
Validation loss: 2.4161548383774294

Epoch: 141| Step: 0
Training loss: 2.229430675506592
Validation loss: 2.420730290874358

Epoch: 5| Step: 1
Training loss: 3.2031784057617188
Validation loss: 2.4154377573279926

Epoch: 5| Step: 2
Training loss: 2.415224075317383
Validation loss: 2.4125628291919665

Epoch: 5| Step: 3
Training loss: 2.7094457149505615
Validation loss: 2.419623895358014

Epoch: 5| Step: 4
Training loss: 2.4866394996643066
Validation loss: 2.4206028804984143

Epoch: 5| Step: 5
Training loss: 2.446873426437378
Validation loss: 2.4144570160937566

Epoch: 5| Step: 6
Training loss: 2.5317044258117676
Validation loss: 2.420707574454687

Epoch: 5| Step: 7
Training loss: 2.6501965522766113
Validation loss: 2.4146380475772324

Epoch: 5| Step: 8
Training loss: 2.9718759059906006
Validation loss: 2.418305112469581

Epoch: 5| Step: 9
Training loss: 2.953352451324463
Validation loss: 2.4124001508118003

Epoch: 5| Step: 10
Training loss: 2.342956781387329
Validation loss: 2.4080922475425144

Epoch: 142| Step: 0
Training loss: 1.6883573532104492
Validation loss: 2.4126074621754308

Epoch: 5| Step: 1
Training loss: 2.642565965652466
Validation loss: 2.4082178274790444

Epoch: 5| Step: 2
Training loss: 2.232783794403076
Validation loss: 2.406140399235551

Epoch: 5| Step: 3
Training loss: 2.496194839477539
Validation loss: 2.404248127373316

Epoch: 5| Step: 4
Training loss: 3.430091142654419
Validation loss: 2.4048471014986754

Epoch: 5| Step: 5
Training loss: 2.1286256313323975
Validation loss: 2.399947238224809

Epoch: 5| Step: 6
Training loss: 3.0699527263641357
Validation loss: 2.4000538920843475

Epoch: 5| Step: 7
Training loss: 2.5650253295898438
Validation loss: 2.395933246099821

Epoch: 5| Step: 8
Training loss: 2.6995396614074707
Validation loss: 2.398485783607729

Epoch: 5| Step: 9
Training loss: 2.738126277923584
Validation loss: 2.4015740604810816

Epoch: 5| Step: 10
Training loss: 3.4024806022644043
Validation loss: 2.3959602053447435

Epoch: 143| Step: 0
Training loss: 2.3563742637634277
Validation loss: 2.3967855668837026

Epoch: 5| Step: 1
Training loss: 3.1318047046661377
Validation loss: 2.395946074557561

Epoch: 5| Step: 2
Training loss: 2.3914966583251953
Validation loss: 2.3975021608414187

Epoch: 5| Step: 3
Training loss: 2.5663743019104004
Validation loss: 2.4010818453245264

Epoch: 5| Step: 4
Training loss: 2.197948932647705
Validation loss: 2.4055204442752305

Epoch: 5| Step: 5
Training loss: 3.504168748855591
Validation loss: 2.4114255418059645

Epoch: 5| Step: 6
Training loss: 2.0692317485809326
Validation loss: 2.4005544518911712

Epoch: 5| Step: 7
Training loss: 2.5352563858032227
Validation loss: 2.3999229964389595

Epoch: 5| Step: 8
Training loss: 3.7414703369140625
Validation loss: 2.3910389433624926

Epoch: 5| Step: 9
Training loss: 2.0064148902893066
Validation loss: 2.3938419870150986

Epoch: 5| Step: 10
Training loss: 2.4120631217956543
Validation loss: 2.3954148997542677

Epoch: 144| Step: 0
Training loss: 2.760056734085083
Validation loss: 2.399097950227799

Epoch: 5| Step: 1
Training loss: 2.582444190979004
Validation loss: 2.395102685497653

Epoch: 5| Step: 2
Training loss: 2.031456232070923
Validation loss: 2.397403465804233

Epoch: 5| Step: 3
Training loss: 2.194331645965576
Validation loss: 2.3965383729627057

Epoch: 5| Step: 4
Training loss: 2.393629550933838
Validation loss: 2.3978719877940353

Epoch: 5| Step: 5
Training loss: 2.583683729171753
Validation loss: 2.404658253474902

Epoch: 5| Step: 6
Training loss: 2.8287222385406494
Validation loss: 2.4125107795961442

Epoch: 5| Step: 7
Training loss: 2.8971991539001465
Validation loss: 2.420561667411558

Epoch: 5| Step: 8
Training loss: 2.8540172576904297
Validation loss: 2.4244436217892553

Epoch: 5| Step: 9
Training loss: 2.6292061805725098
Validation loss: 2.4263946356311923

Epoch: 5| Step: 10
Training loss: 3.204094409942627
Validation loss: 2.433061276712725

Epoch: 145| Step: 0
Training loss: 2.63458251953125
Validation loss: 2.429690673787107

Epoch: 5| Step: 1
Training loss: 2.6653404235839844
Validation loss: 2.41973509839786

Epoch: 5| Step: 2
Training loss: 2.348937511444092
Validation loss: 2.4114113059095157

Epoch: 5| Step: 3
Training loss: 2.7240169048309326
Validation loss: 2.4064698629481818

Epoch: 5| Step: 4
Training loss: 2.8994579315185547
Validation loss: 2.4001220041705715

Epoch: 5| Step: 5
Training loss: 2.1441893577575684
Validation loss: 2.4006689569001556

Epoch: 5| Step: 6
Training loss: 3.0347228050231934
Validation loss: 2.397680959393901

Epoch: 5| Step: 7
Training loss: 2.3109047412872314
Validation loss: 2.4012637087093887

Epoch: 5| Step: 8
Training loss: 3.02897310256958
Validation loss: 2.3978316091722056

Epoch: 5| Step: 9
Training loss: 2.5161619186401367
Validation loss: 2.3985607649690364

Epoch: 5| Step: 10
Training loss: 2.5428459644317627
Validation loss: 2.397628456033686

Epoch: 146| Step: 0
Training loss: 2.4816067218780518
Validation loss: 2.3933016946238856

Epoch: 5| Step: 1
Training loss: 2.5723214149475098
Validation loss: 2.3991109222494145

Epoch: 5| Step: 2
Training loss: 2.7208876609802246
Validation loss: 2.403138565760787

Epoch: 5| Step: 3
Training loss: 1.8222014904022217
Validation loss: 2.4084903604240826

Epoch: 5| Step: 4
Training loss: 3.2474193572998047
Validation loss: 2.412235524064751

Epoch: 5| Step: 5
Training loss: 3.2888972759246826
Validation loss: 2.412288301734514

Epoch: 5| Step: 6
Training loss: 2.0229332447052
Validation loss: 2.405861848144121

Epoch: 5| Step: 7
Training loss: 2.4944300651550293
Validation loss: 2.3993768179288475

Epoch: 5| Step: 8
Training loss: 2.717301607131958
Validation loss: 2.4012317862561954

Epoch: 5| Step: 9
Training loss: 2.6698553562164307
Validation loss: 2.4051236516685894

Epoch: 5| Step: 10
Training loss: 2.7840423583984375
Validation loss: 2.3927763328757337

Epoch: 147| Step: 0
Training loss: 2.162233829498291
Validation loss: 2.3987601008466495

Epoch: 5| Step: 1
Training loss: 2.966587781906128
Validation loss: 2.3948472187083256

Epoch: 5| Step: 2
Training loss: 1.609551191329956
Validation loss: 2.393647506672849

Epoch: 5| Step: 3
Training loss: 2.371156692504883
Validation loss: 2.3862284460375385

Epoch: 5| Step: 4
Training loss: 3.416335344314575
Validation loss: 2.3879000127956433

Epoch: 5| Step: 5
Training loss: 3.063291311264038
Validation loss: 2.382955845966134

Epoch: 5| Step: 6
Training loss: 2.6667377948760986
Validation loss: 2.3802832916218746

Epoch: 5| Step: 7
Training loss: 2.8767826557159424
Validation loss: 2.381346061665525

Epoch: 5| Step: 8
Training loss: 2.8703789710998535
Validation loss: 2.380324936682178

Epoch: 5| Step: 9
Training loss: 2.0324740409851074
Validation loss: 2.3796149658900436

Epoch: 5| Step: 10
Training loss: 2.8468539714813232
Validation loss: 2.3826139062963505

Epoch: 148| Step: 0
Training loss: 3.1396028995513916
Validation loss: 2.3804048133152786

Epoch: 5| Step: 1
Training loss: 3.034740447998047
Validation loss: 2.3826594532177015

Epoch: 5| Step: 2
Training loss: 2.7213377952575684
Validation loss: 2.3933131310247604

Epoch: 5| Step: 3
Training loss: 2.7046005725860596
Validation loss: 2.3903963642735637

Epoch: 5| Step: 4
Training loss: 2.725264072418213
Validation loss: 2.3987644180174796

Epoch: 5| Step: 5
Training loss: 1.9778369665145874
Validation loss: 2.390898057209548

Epoch: 5| Step: 6
Training loss: 2.520228862762451
Validation loss: 2.38974957055943

Epoch: 5| Step: 7
Training loss: 2.390998601913452
Validation loss: 2.3891844390540995

Epoch: 5| Step: 8
Training loss: 2.6147475242614746
Validation loss: 2.3917126476123767

Epoch: 5| Step: 9
Training loss: 2.3694863319396973
Validation loss: 2.398560344531972

Epoch: 5| Step: 10
Training loss: 2.634382963180542
Validation loss: 2.4009565948158182

Epoch: 149| Step: 0
Training loss: 2.7076001167297363
Validation loss: 2.4026737982226956

Epoch: 5| Step: 1
Training loss: 2.618481159210205
Validation loss: 2.407033512669225

Epoch: 5| Step: 2
Training loss: 3.377365827560425
Validation loss: 2.418375858696558

Epoch: 5| Step: 3
Training loss: 2.7195000648498535
Validation loss: 2.410032010847522

Epoch: 5| Step: 4
Training loss: 2.2406694889068604
Validation loss: 2.4108863825439126

Epoch: 5| Step: 5
Training loss: 3.3193650245666504
Validation loss: 2.4042578589531685

Epoch: 5| Step: 6
Training loss: 2.307708263397217
Validation loss: 2.398751525468724

Epoch: 5| Step: 7
Training loss: 2.0697290897369385
Validation loss: 2.3936030069986978

Epoch: 5| Step: 8
Training loss: 2.583926200866699
Validation loss: 2.3937006817069104

Epoch: 5| Step: 9
Training loss: 2.934513807296753
Validation loss: 2.4021549788854455

Epoch: 5| Step: 10
Training loss: 1.8513050079345703
Validation loss: 2.4080852641854236

Epoch: 150| Step: 0
Training loss: 2.162781238555908
Validation loss: 2.419261463226811

Epoch: 5| Step: 1
Training loss: 2.277575731277466
Validation loss: 2.42276253238801

Epoch: 5| Step: 2
Training loss: 2.556786060333252
Validation loss: 2.408374701776812

Epoch: 5| Step: 3
Training loss: 2.9172940254211426
Validation loss: 2.4155136987727177

Epoch: 5| Step: 4
Training loss: 2.8970730304718018
Validation loss: 2.4082286229697605

Epoch: 5| Step: 5
Training loss: 2.2318904399871826
Validation loss: 2.4038748305330992

Epoch: 5| Step: 6
Training loss: 2.3859245777130127
Validation loss: 2.3976704279581704

Epoch: 5| Step: 7
Training loss: 2.643148183822632
Validation loss: 2.3946285811803674

Epoch: 5| Step: 8
Training loss: 2.726248264312744
Validation loss: 2.396961168576312

Epoch: 5| Step: 9
Training loss: 3.342254638671875
Validation loss: 2.393473796947028

Epoch: 5| Step: 10
Training loss: 2.662266254425049
Validation loss: 2.3898065987453667

Epoch: 151| Step: 0
Training loss: 2.4628403186798096
Validation loss: 2.390859609009117

Epoch: 5| Step: 1
Training loss: 2.321476697921753
Validation loss: 2.3934187222552556

Epoch: 5| Step: 2
Training loss: 3.149898052215576
Validation loss: 2.3810929662437847

Epoch: 5| Step: 3
Training loss: 2.4366085529327393
Validation loss: 2.3849861365492626

Epoch: 5| Step: 4
Training loss: 2.5870654582977295
Validation loss: 2.382007009239607

Epoch: 5| Step: 5
Training loss: 2.806295394897461
Validation loss: 2.378195478070167

Epoch: 5| Step: 6
Training loss: 2.956078052520752
Validation loss: 2.3776990341883835

Epoch: 5| Step: 7
Training loss: 2.9123387336730957
Validation loss: 2.383557092758917

Epoch: 5| Step: 8
Training loss: 2.245079517364502
Validation loss: 2.387340748181907

Epoch: 5| Step: 9
Training loss: 2.1448020935058594
Validation loss: 2.3942271612023793

Epoch: 5| Step: 10
Training loss: 2.7585535049438477
Validation loss: 2.393846114476522

Epoch: 152| Step: 0
Training loss: 2.267638683319092
Validation loss: 2.39391496873671

Epoch: 5| Step: 1
Training loss: 2.61202335357666
Validation loss: 2.40329288026338

Epoch: 5| Step: 2
Training loss: 2.421719551086426
Validation loss: 2.3923518862775577

Epoch: 5| Step: 3
Training loss: 2.5705103874206543
Validation loss: 2.3935434074812036

Epoch: 5| Step: 4
Training loss: 2.5161190032958984
Validation loss: 2.3969863819819626

Epoch: 5| Step: 5
Training loss: 2.7736968994140625
Validation loss: 2.389986768845589

Epoch: 5| Step: 6
Training loss: 2.9804277420043945
Validation loss: 2.383467161527244

Epoch: 5| Step: 7
Training loss: 2.6845314502716064
Validation loss: 2.37705090994476

Epoch: 5| Step: 8
Training loss: 3.2031428813934326
Validation loss: 2.3836497004314134

Epoch: 5| Step: 9
Training loss: 2.6345465183258057
Validation loss: 2.3747505116206344

Epoch: 5| Step: 10
Training loss: 1.934918999671936
Validation loss: 2.375006841075036

Epoch: 153| Step: 0
Training loss: 2.739830255508423
Validation loss: 2.372560598516977

Epoch: 5| Step: 1
Training loss: 2.711010694503784
Validation loss: 2.378067793384675

Epoch: 5| Step: 2
Training loss: 2.380955696105957
Validation loss: 2.377103187704599

Epoch: 5| Step: 3
Training loss: 1.8367725610733032
Validation loss: 2.3832717736562095

Epoch: 5| Step: 4
Training loss: 2.7350454330444336
Validation loss: 2.3900825669688563

Epoch: 5| Step: 5
Training loss: 2.4010424613952637
Validation loss: 2.3984144785070933

Epoch: 5| Step: 6
Training loss: 2.94221568107605
Validation loss: 2.401372468599709

Epoch: 5| Step: 7
Training loss: 2.555079221725464
Validation loss: 2.407690827564527

Epoch: 5| Step: 8
Training loss: 2.0425946712493896
Validation loss: 2.3921139522265364

Epoch: 5| Step: 9
Training loss: 3.0553014278411865
Validation loss: 2.3985425785023677

Epoch: 5| Step: 10
Training loss: 3.4187331199645996
Validation loss: 2.3901083623209307

Epoch: 154| Step: 0
Training loss: 2.100043773651123
Validation loss: 2.3953001524812434

Epoch: 5| Step: 1
Training loss: 2.3884191513061523
Validation loss: 2.3890078003688524

Epoch: 5| Step: 2
Training loss: 2.735356092453003
Validation loss: 2.3832226184106644

Epoch: 5| Step: 3
Training loss: 2.937389850616455
Validation loss: 2.376168081837316

Epoch: 5| Step: 4
Training loss: 2.545701503753662
Validation loss: 2.370919192990949

Epoch: 5| Step: 5
Training loss: 2.8496549129486084
Validation loss: 2.3653466829689602

Epoch: 5| Step: 6
Training loss: 2.509287118911743
Validation loss: 2.3669081887891217

Epoch: 5| Step: 7
Training loss: 2.8215861320495605
Validation loss: 2.3684882810038905

Epoch: 5| Step: 8
Training loss: 2.3857293128967285
Validation loss: 2.366816620672903

Epoch: 5| Step: 9
Training loss: 3.225123643875122
Validation loss: 2.3686500544189126

Epoch: 5| Step: 10
Training loss: 2.094597101211548
Validation loss: 2.3803206310477307

Epoch: 155| Step: 0
Training loss: 3.1618754863739014
Validation loss: 2.3816255677130913

Epoch: 5| Step: 1
Training loss: 2.3992443084716797
Validation loss: 2.3861516983278337

Epoch: 5| Step: 2
Training loss: 2.8699729442596436
Validation loss: 2.38660962863635

Epoch: 5| Step: 3
Training loss: 2.3456509113311768
Validation loss: 2.3791074265715895

Epoch: 5| Step: 4
Training loss: 2.990947961807251
Validation loss: 2.394013356137019

Epoch: 5| Step: 5
Training loss: 2.6332850456237793
Validation loss: 2.3912154013110745

Epoch: 5| Step: 6
Training loss: 2.7730228900909424
Validation loss: 2.391176910810573

Epoch: 5| Step: 7
Training loss: 2.3858790397644043
Validation loss: 2.3931286565719114

Epoch: 5| Step: 8
Training loss: 2.4454550743103027
Validation loss: 2.389300351501793

Epoch: 5| Step: 9
Training loss: 1.9199546575546265
Validation loss: 2.386444967280152

Epoch: 5| Step: 10
Training loss: 2.659370183944702
Validation loss: 2.3852405112276793

Epoch: 156| Step: 0
Training loss: 3.09116792678833
Validation loss: 2.386984445715463

Epoch: 5| Step: 1
Training loss: 2.9281797409057617
Validation loss: 2.3880065282185874

Epoch: 5| Step: 2
Training loss: 2.3963711261749268
Validation loss: 2.3831088876211517

Epoch: 5| Step: 3
Training loss: 2.3092963695526123
Validation loss: 2.393321511565998

Epoch: 5| Step: 4
Training loss: 3.206552505493164
Validation loss: 2.391319474866313

Epoch: 5| Step: 5
Training loss: 2.5475990772247314
Validation loss: 2.3878786666418916

Epoch: 5| Step: 6
Training loss: 2.316384792327881
Validation loss: 2.38461273459978

Epoch: 5| Step: 7
Training loss: 2.1288483142852783
Validation loss: 2.3780500453005553

Epoch: 5| Step: 8
Training loss: 2.4342877864837646
Validation loss: 2.3756898244222007

Epoch: 5| Step: 9
Training loss: 2.618384599685669
Validation loss: 2.3731759876333256

Epoch: 5| Step: 10
Training loss: 2.6682536602020264
Validation loss: 2.3752335322800504

Epoch: 157| Step: 0
Training loss: 2.5486176013946533
Validation loss: 2.3862265950889996

Epoch: 5| Step: 1
Training loss: 2.5905394554138184
Validation loss: 2.3952829889071885

Epoch: 5| Step: 2
Training loss: 2.4370243549346924
Validation loss: 2.399720420119583

Epoch: 5| Step: 3
Training loss: 2.60335111618042
Validation loss: 2.402395784213979

Epoch: 5| Step: 4
Training loss: 2.095913887023926
Validation loss: 2.395889377081266

Epoch: 5| Step: 5
Training loss: 3.1675286293029785
Validation loss: 2.382240036482452

Epoch: 5| Step: 6
Training loss: 2.257181406021118
Validation loss: 2.3749461391920685

Epoch: 5| Step: 7
Training loss: 2.4216253757476807
Validation loss: 2.3656338440474642

Epoch: 5| Step: 8
Training loss: 2.6737823486328125
Validation loss: 2.3645311042826664

Epoch: 5| Step: 9
Training loss: 3.2257182598114014
Validation loss: 2.365398291618593

Epoch: 5| Step: 10
Training loss: 2.5814077854156494
Validation loss: 2.368917488282727

Epoch: 158| Step: 0
Training loss: 2.3598179817199707
Validation loss: 2.369848233397289

Epoch: 5| Step: 1
Training loss: 2.09383225440979
Validation loss: 2.3753107491359917

Epoch: 5| Step: 2
Training loss: 2.1330952644348145
Validation loss: 2.3723267739818943

Epoch: 5| Step: 3
Training loss: 2.7649967670440674
Validation loss: 2.373780858132147

Epoch: 5| Step: 4
Training loss: 3.088261365890503
Validation loss: 2.39179237299068

Epoch: 5| Step: 5
Training loss: 2.7907159328460693
Validation loss: 2.401672194080968

Epoch: 5| Step: 6
Training loss: 2.862889528274536
Validation loss: 2.4185211863569034

Epoch: 5| Step: 7
Training loss: 2.6657891273498535
Validation loss: 2.436382988447784

Epoch: 5| Step: 8
Training loss: 3.1328091621398926
Validation loss: 2.444763519430673

Epoch: 5| Step: 9
Training loss: 2.342900276184082
Validation loss: 2.4486812237770326

Epoch: 5| Step: 10
Training loss: 2.3833982944488525
Validation loss: 2.422665442189863

Epoch: 159| Step: 0
Training loss: 2.404127836227417
Validation loss: 2.411802076524304

Epoch: 5| Step: 1
Training loss: 2.8376801013946533
Validation loss: 2.3934609710529284

Epoch: 5| Step: 2
Training loss: 2.902705430984497
Validation loss: 2.38359321061001

Epoch: 5| Step: 3
Training loss: 2.583163261413574
Validation loss: 2.381448031753622

Epoch: 5| Step: 4
Training loss: 1.8673374652862549
Validation loss: 2.377532743638562

Epoch: 5| Step: 5
Training loss: 2.230103015899658
Validation loss: 2.378626802916168

Epoch: 5| Step: 6
Training loss: 2.1267447471618652
Validation loss: 2.37217221208798

Epoch: 5| Step: 7
Training loss: 2.5339415073394775
Validation loss: 2.3684249257528656

Epoch: 5| Step: 8
Training loss: 2.751373767852783
Validation loss: 2.3594080094368226

Epoch: 5| Step: 9
Training loss: 3.4977989196777344
Validation loss: 2.3606576919555664

Epoch: 5| Step: 10
Training loss: 3.0376384258270264
Validation loss: 2.3545272170856433

Epoch: 160| Step: 0
Training loss: 2.9027395248413086
Validation loss: 2.3556483842993297

Epoch: 5| Step: 1
Training loss: 2.5628719329833984
Validation loss: 2.3590409460888115

Epoch: 5| Step: 2
Training loss: 2.256103515625
Validation loss: 2.366136876485681

Epoch: 5| Step: 3
Training loss: 2.3545305728912354
Validation loss: 2.380342173319991

Epoch: 5| Step: 4
Training loss: 2.9419922828674316
Validation loss: 2.3879939189521213

Epoch: 5| Step: 5
Training loss: 2.2529263496398926
Validation loss: 2.4235773624912387

Epoch: 5| Step: 6
Training loss: 2.8120758533477783
Validation loss: 2.447425834594234

Epoch: 5| Step: 7
Training loss: 2.517171859741211
Validation loss: 2.4951572290030857

Epoch: 5| Step: 8
Training loss: 2.425485134124756
Validation loss: 2.4894588224349485

Epoch: 5| Step: 9
Training loss: 3.308133602142334
Validation loss: 2.4779133796691895

Epoch: 5| Step: 10
Training loss: 2.6665561199188232
Validation loss: 2.4288951914797545

Epoch: 161| Step: 0
Training loss: 2.7253222465515137
Validation loss: 2.3896467454971804

Epoch: 5| Step: 1
Training loss: 2.465137481689453
Validation loss: 2.369233710791475

Epoch: 5| Step: 2
Training loss: 2.465585708618164
Validation loss: 2.3661935919074604

Epoch: 5| Step: 3
Training loss: 1.8912851810455322
Validation loss: 2.368183200077344

Epoch: 5| Step: 4
Training loss: 3.175170660018921
Validation loss: 2.379252609386239

Epoch: 5| Step: 5
Training loss: 3.1786437034606934
Validation loss: 2.3751035018633773

Epoch: 5| Step: 6
Training loss: 2.352599859237671
Validation loss: 2.36437717945345

Epoch: 5| Step: 7
Training loss: 2.6151444911956787
Validation loss: 2.365434938861478

Epoch: 5| Step: 8
Training loss: 2.1308255195617676
Validation loss: 2.374333281670847

Epoch: 5| Step: 9
Training loss: 2.75766921043396
Validation loss: 2.365248869824153

Epoch: 5| Step: 10
Training loss: 3.026604652404785
Validation loss: 2.362071996094078

Epoch: 162| Step: 0
Training loss: 2.26678204536438
Validation loss: 2.375601162192642

Epoch: 5| Step: 1
Training loss: 3.2010204792022705
Validation loss: 2.3704984264989055

Epoch: 5| Step: 2
Training loss: 2.2676753997802734
Validation loss: 2.3694072538806545

Epoch: 5| Step: 3
Training loss: 2.2372405529022217
Validation loss: 2.371656915192963

Epoch: 5| Step: 4
Training loss: 2.1433067321777344
Validation loss: 2.3811500098115657

Epoch: 5| Step: 5
Training loss: 2.7789413928985596
Validation loss: 2.3794577147371028

Epoch: 5| Step: 6
Training loss: 2.9726758003234863
Validation loss: 2.379729696499404

Epoch: 5| Step: 7
Training loss: 2.2242751121520996
Validation loss: 2.3804414733763664

Epoch: 5| Step: 8
Training loss: 2.711940050125122
Validation loss: 2.378019053448913

Epoch: 5| Step: 9
Training loss: 3.1084463596343994
Validation loss: 2.3847986088004163

Epoch: 5| Step: 10
Training loss: 2.639241933822632
Validation loss: 2.3911748137525333

Epoch: 163| Step: 0
Training loss: 3.132439374923706
Validation loss: 2.39629550133982

Epoch: 5| Step: 1
Training loss: 2.079463481903076
Validation loss: 2.387010781995712

Epoch: 5| Step: 2
Training loss: 2.702700138092041
Validation loss: 2.379318016831593

Epoch: 5| Step: 3
Training loss: 2.6013901233673096
Validation loss: 2.3799958049610095

Epoch: 5| Step: 4
Training loss: 2.9003660678863525
Validation loss: 2.383542626134811

Epoch: 5| Step: 5
Training loss: 2.9522652626037598
Validation loss: 2.391497306926276

Epoch: 5| Step: 6
Training loss: 2.4583559036254883
Validation loss: 2.377400331599738

Epoch: 5| Step: 7
Training loss: 2.997217893600464
Validation loss: 2.352553541942309

Epoch: 5| Step: 8
Training loss: 1.966651201248169
Validation loss: 2.3473219871520996

Epoch: 5| Step: 9
Training loss: 2.3562915325164795
Validation loss: 2.3427208085213937

Epoch: 5| Step: 10
Training loss: 2.343261241912842
Validation loss: 2.344451222368466

Epoch: 164| Step: 0
Training loss: 2.352508544921875
Validation loss: 2.341123560423492

Epoch: 5| Step: 1
Training loss: 2.6920669078826904
Validation loss: 2.3509042416849444

Epoch: 5| Step: 2
Training loss: 1.9764680862426758
Validation loss: 2.362769324292419

Epoch: 5| Step: 3
Training loss: 2.9251046180725098
Validation loss: 2.3648868632572952

Epoch: 5| Step: 4
Training loss: 2.9158685207366943
Validation loss: 2.3691471571563394

Epoch: 5| Step: 5
Training loss: 2.2846593856811523
Validation loss: 2.370643090176326

Epoch: 5| Step: 6
Training loss: 2.3508403301239014
Validation loss: 2.370182283463017

Epoch: 5| Step: 7
Training loss: 2.813568592071533
Validation loss: 2.357223379996515

Epoch: 5| Step: 8
Training loss: 3.1000962257385254
Validation loss: 2.3621366254744993

Epoch: 5| Step: 9
Training loss: 2.731339931488037
Validation loss: 2.3528747686775784

Epoch: 5| Step: 10
Training loss: 2.338951349258423
Validation loss: 2.359975061108989

Epoch: 165| Step: 0
Training loss: 2.4291155338287354
Validation loss: 2.361326209960445

Epoch: 5| Step: 1
Training loss: 2.9466261863708496
Validation loss: 2.3655898622287217

Epoch: 5| Step: 2
Training loss: 2.448270320892334
Validation loss: 2.3694997859257523

Epoch: 5| Step: 3
Training loss: 2.403231620788574
Validation loss: 2.375083631084811

Epoch: 5| Step: 4
Training loss: 2.841261625289917
Validation loss: 2.3962776481464343

Epoch: 5| Step: 5
Training loss: 2.6621832847595215
Validation loss: 2.4191465172716367

Epoch: 5| Step: 6
Training loss: 2.3930180072784424
Validation loss: 2.421620879122006

Epoch: 5| Step: 7
Training loss: 2.692659854888916
Validation loss: 2.39764396349589

Epoch: 5| Step: 8
Training loss: 2.5451889038085938
Validation loss: 2.36023574618883

Epoch: 5| Step: 9
Training loss: 2.6615147590637207
Validation loss: 2.3473113839344313

Epoch: 5| Step: 10
Training loss: 2.5703506469726562
Validation loss: 2.3393401689426874

Epoch: 166| Step: 0
Training loss: 2.86445951461792
Validation loss: 2.339436895103865

Epoch: 5| Step: 1
Training loss: 2.3834075927734375
Validation loss: 2.3451048046030025

Epoch: 5| Step: 2
Training loss: 2.0028343200683594
Validation loss: 2.3482374939867245

Epoch: 5| Step: 3
Training loss: 2.5914154052734375
Validation loss: 2.3482723851357736

Epoch: 5| Step: 4
Training loss: 3.111539363861084
Validation loss: 2.354967724892401

Epoch: 5| Step: 5
Training loss: 1.9683635234832764
Validation loss: 2.3430633301376016

Epoch: 5| Step: 6
Training loss: 2.537707805633545
Validation loss: 2.3499691896541144

Epoch: 5| Step: 7
Training loss: 2.587125301361084
Validation loss: 2.35779082903298

Epoch: 5| Step: 8
Training loss: 2.9088966846466064
Validation loss: 2.3642800546461538

Epoch: 5| Step: 9
Training loss: 2.8877339363098145
Validation loss: 2.382351303613314

Epoch: 5| Step: 10
Training loss: 2.691153049468994
Validation loss: 2.3831084441113215

Epoch: 167| Step: 0
Training loss: 2.6794636249542236
Validation loss: 2.390577754666728

Epoch: 5| Step: 1
Training loss: 2.2247724533081055
Validation loss: 2.3943527770298783

Epoch: 5| Step: 2
Training loss: 2.8686442375183105
Validation loss: 2.38830045987201

Epoch: 5| Step: 3
Training loss: 2.647773027420044
Validation loss: 2.3931427283953597

Epoch: 5| Step: 4
Training loss: 2.2445785999298096
Validation loss: 2.387962132371882

Epoch: 5| Step: 5
Training loss: 2.1115341186523438
Validation loss: 2.390287709492509

Epoch: 5| Step: 6
Training loss: 2.512622833251953
Validation loss: 2.3750699168892315

Epoch: 5| Step: 7
Training loss: 2.043910503387451
Validation loss: 2.365218316355059

Epoch: 5| Step: 8
Training loss: 2.876084566116333
Validation loss: 2.3685572634461107

Epoch: 5| Step: 9
Training loss: 2.611423969268799
Validation loss: 2.3784102188643588

Epoch: 5| Step: 10
Training loss: 3.7335731983184814
Validation loss: 2.3888437747955322

Epoch: 168| Step: 0
Training loss: 2.3161368370056152
Validation loss: 2.376216237263013

Epoch: 5| Step: 1
Training loss: 2.630398988723755
Validation loss: 2.3752334963890815

Epoch: 5| Step: 2
Training loss: 2.2656686305999756
Validation loss: 2.3640927371158393

Epoch: 5| Step: 3
Training loss: 3.0396676063537598
Validation loss: 2.3564565848278742

Epoch: 5| Step: 4
Training loss: 2.434814929962158
Validation loss: 2.347019572411814

Epoch: 5| Step: 5
Training loss: 2.5149447917938232
Validation loss: 2.355581888588526

Epoch: 5| Step: 6
Training loss: 2.5546276569366455
Validation loss: 2.3544047917089155

Epoch: 5| Step: 7
Training loss: 3.024782180786133
Validation loss: 2.367905186068627

Epoch: 5| Step: 8
Training loss: 2.3464372158050537
Validation loss: 2.367616914933728

Epoch: 5| Step: 9
Training loss: 2.275733709335327
Validation loss: 2.3690364232627292

Epoch: 5| Step: 10
Training loss: 3.042097806930542
Validation loss: 2.3601133054302585

Epoch: 169| Step: 0
Training loss: 2.969128370285034
Validation loss: 2.348731435755248

Epoch: 5| Step: 1
Training loss: 2.629779100418091
Validation loss: 2.3416386214635705

Epoch: 5| Step: 2
Training loss: 2.4877429008483887
Validation loss: 2.3351869070401756

Epoch: 5| Step: 3
Training loss: 2.1624393463134766
Validation loss: 2.338377518038596

Epoch: 5| Step: 4
Training loss: 2.954819440841675
Validation loss: 2.3402181850966586

Epoch: 5| Step: 5
Training loss: 2.2529406547546387
Validation loss: 2.3425677309754076

Epoch: 5| Step: 6
Training loss: 2.2022852897644043
Validation loss: 2.339463556966474

Epoch: 5| Step: 7
Training loss: 3.087421417236328
Validation loss: 2.3332542450197282

Epoch: 5| Step: 8
Training loss: 2.0266201496124268
Validation loss: 2.329178971628989

Epoch: 5| Step: 9
Training loss: 2.99705171585083
Validation loss: 2.3357685150638705

Epoch: 5| Step: 10
Training loss: 2.6234869956970215
Validation loss: 2.3359202043984526

Epoch: 170| Step: 0
Training loss: 2.5997672080993652
Validation loss: 2.3419564154840287

Epoch: 5| Step: 1
Training loss: 2.764591693878174
Validation loss: 2.338112546551612

Epoch: 5| Step: 2
Training loss: 2.4606783390045166
Validation loss: 2.3338595705647625

Epoch: 5| Step: 3
Training loss: 2.690124034881592
Validation loss: 2.3408774355406403

Epoch: 5| Step: 4
Training loss: 1.6114017963409424
Validation loss: 2.3400320212046304

Epoch: 5| Step: 5
Training loss: 2.964810848236084
Validation loss: 2.3498167402000836

Epoch: 5| Step: 6
Training loss: 3.170778751373291
Validation loss: 2.3656368665797736

Epoch: 5| Step: 7
Training loss: 2.1075453758239746
Validation loss: 2.365749412967313

Epoch: 5| Step: 8
Training loss: 2.9064056873321533
Validation loss: 2.363215143962573

Epoch: 5| Step: 9
Training loss: 2.0780911445617676
Validation loss: 2.364774973161759

Epoch: 5| Step: 10
Training loss: 2.9726059436798096
Validation loss: 2.3833185626614477

Epoch: 171| Step: 0
Training loss: 2.2281837463378906
Validation loss: 2.3686686638862855

Epoch: 5| Step: 1
Training loss: 2.2433862686157227
Validation loss: 2.351925633286917

Epoch: 5| Step: 2
Training loss: 2.7018380165100098
Validation loss: 2.3583418066783617

Epoch: 5| Step: 3
Training loss: 2.5743839740753174
Validation loss: 2.358949020344724

Epoch: 5| Step: 4
Training loss: 3.500666856765747
Validation loss: 2.3608091774807183

Epoch: 5| Step: 5
Training loss: 2.867797374725342
Validation loss: 2.3648826486320904

Epoch: 5| Step: 6
Training loss: 2.4456233978271484
Validation loss: 2.3568341937116397

Epoch: 5| Step: 7
Training loss: 1.8541920185089111
Validation loss: 2.352463960647583

Epoch: 5| Step: 8
Training loss: 2.0212082862854004
Validation loss: 2.3452895559290403

Epoch: 5| Step: 9
Training loss: 2.817657947540283
Validation loss: 2.338035291241061

Epoch: 5| Step: 10
Training loss: 2.958828926086426
Validation loss: 2.340004221085579

Epoch: 172| Step: 0
Training loss: 1.8867905139923096
Validation loss: 2.361503347273796

Epoch: 5| Step: 1
Training loss: 2.898308753967285
Validation loss: 2.3828330962888655

Epoch: 5| Step: 2
Training loss: 2.6090941429138184
Validation loss: 2.402380310079103

Epoch: 5| Step: 3
Training loss: 2.5262527465820312
Validation loss: 2.3840199849938832

Epoch: 5| Step: 4
Training loss: 2.6297693252563477
Validation loss: 2.3586237686936573

Epoch: 5| Step: 5
Training loss: 2.420488119125366
Validation loss: 2.342898263726183

Epoch: 5| Step: 6
Training loss: 2.753479480743408
Validation loss: 2.3391316731770835

Epoch: 5| Step: 7
Training loss: 2.627951145172119
Validation loss: 2.3390224646496516

Epoch: 5| Step: 8
Training loss: 2.5548996925354004
Validation loss: 2.3407998905386975

Epoch: 5| Step: 9
Training loss: 2.8228976726531982
Validation loss: 2.3533182810711604

Epoch: 5| Step: 10
Training loss: 2.6946218013763428
Validation loss: 2.3563062093591176

Epoch: 173| Step: 0
Training loss: 2.6234512329101562
Validation loss: 2.361784058232461

Epoch: 5| Step: 1
Training loss: 2.414562702178955
Validation loss: 2.373352404563658

Epoch: 5| Step: 2
Training loss: 2.572554111480713
Validation loss: 2.37146072746605

Epoch: 5| Step: 3
Training loss: 2.6940219402313232
Validation loss: 2.3773552986883346

Epoch: 5| Step: 4
Training loss: 2.5316202640533447
Validation loss: 2.3820159435272217

Epoch: 5| Step: 5
Training loss: 2.4053282737731934
Validation loss: 2.3900354575085383

Epoch: 5| Step: 6
Training loss: 2.7767398357391357
Validation loss: 2.3926658220188592

Epoch: 5| Step: 7
Training loss: 2.4378011226654053
Validation loss: 2.3864758065951768

Epoch: 5| Step: 8
Training loss: 2.16056489944458
Validation loss: 2.361745898441602

Epoch: 5| Step: 9
Training loss: 2.9709606170654297
Validation loss: 2.34579651330107

Epoch: 5| Step: 10
Training loss: 2.8638691902160645
Validation loss: 2.3325375741527927

Epoch: 174| Step: 0
Training loss: 2.4546380043029785
Validation loss: 2.3403461722917456

Epoch: 5| Step: 1
Training loss: 2.123095750808716
Validation loss: 2.347719891096956

Epoch: 5| Step: 2
Training loss: 2.2639594078063965
Validation loss: 2.35554269308685

Epoch: 5| Step: 3
Training loss: 2.898981809616089
Validation loss: 2.348994770357686

Epoch: 5| Step: 4
Training loss: 2.312304973602295
Validation loss: 2.336415221614222

Epoch: 5| Step: 5
Training loss: 2.0802130699157715
Validation loss: 2.3248639670751428

Epoch: 5| Step: 6
Training loss: 2.8426551818847656
Validation loss: 2.315686930892288

Epoch: 5| Step: 7
Training loss: 2.77249813079834
Validation loss: 2.3210949949038926

Epoch: 5| Step: 8
Training loss: 3.0958714485168457
Validation loss: 2.329467961865087

Epoch: 5| Step: 9
Training loss: 2.847595453262329
Validation loss: 2.3356369131354877

Epoch: 5| Step: 10
Training loss: 2.6472761631011963
Validation loss: 2.343995799300491

Epoch: 175| Step: 0
Training loss: 2.4129154682159424
Validation loss: 2.3579115944523967

Epoch: 5| Step: 1
Training loss: 3.2185699939727783
Validation loss: 2.3793762140376593

Epoch: 5| Step: 2
Training loss: 2.432816982269287
Validation loss: 2.401326302559145

Epoch: 5| Step: 3
Training loss: 2.8288755416870117
Validation loss: 2.3894761608492945

Epoch: 5| Step: 4
Training loss: 2.1158030033111572
Validation loss: 2.3856637759875228

Epoch: 5| Step: 5
Training loss: 2.350043535232544
Validation loss: 2.384692412550731

Epoch: 5| Step: 6
Training loss: 3.031278610229492
Validation loss: 2.3734356921206237

Epoch: 5| Step: 7
Training loss: 1.4037201404571533
Validation loss: 2.3755303787928757

Epoch: 5| Step: 8
Training loss: 3.1865477561950684
Validation loss: 2.382674194151355

Epoch: 5| Step: 9
Training loss: 2.4473154544830322
Validation loss: 2.379386871091781

Epoch: 5| Step: 10
Training loss: 3.0158920288085938
Validation loss: 2.3738769280013217

Epoch: 176| Step: 0
Training loss: 2.9421916007995605
Validation loss: 2.363573264050227

Epoch: 5| Step: 1
Training loss: 2.345383405685425
Validation loss: 2.3582877036063903

Epoch: 5| Step: 2
Training loss: 2.409045696258545
Validation loss: 2.3628174130634596

Epoch: 5| Step: 3
Training loss: 2.059566020965576
Validation loss: 2.3582138374287593

Epoch: 5| Step: 4
Training loss: 2.2362053394317627
Validation loss: 2.3468285632389847

Epoch: 5| Step: 5
Training loss: 2.6027281284332275
Validation loss: 2.3443180053464827

Epoch: 5| Step: 6
Training loss: 3.6319222450256348
Validation loss: 2.3558744486942085

Epoch: 5| Step: 7
Training loss: 2.833646535873413
Validation loss: 2.3644638061523438

Epoch: 5| Step: 8
Training loss: 3.0002942085266113
Validation loss: 2.3927005388403453

Epoch: 5| Step: 9
Training loss: 2.239561080932617
Validation loss: 2.406779771210045

Epoch: 5| Step: 10
Training loss: 2.1202967166900635
Validation loss: 2.413051994897986

Epoch: 177| Step: 0
Training loss: 2.578582763671875
Validation loss: 2.4196770960284817

Epoch: 5| Step: 1
Training loss: 2.70207142829895
Validation loss: 2.410400790552939

Epoch: 5| Step: 2
Training loss: 2.3437743186950684
Validation loss: 2.4013471526484333

Epoch: 5| Step: 3
Training loss: 2.738720178604126
Validation loss: 2.38846956786289

Epoch: 5| Step: 4
Training loss: 2.752790689468384
Validation loss: 2.378207852763514

Epoch: 5| Step: 5
Training loss: 2.2820968627929688
Validation loss: 2.3671172357374624

Epoch: 5| Step: 6
Training loss: 2.1116509437561035
Validation loss: 2.3727010475691928

Epoch: 5| Step: 7
Training loss: 2.5682756900787354
Validation loss: 2.3731372305141982

Epoch: 5| Step: 8
Training loss: 2.9276561737060547
Validation loss: 2.375062334922052

Epoch: 5| Step: 9
Training loss: 3.0562124252319336
Validation loss: 2.371767044067383

Epoch: 5| Step: 10
Training loss: 2.1540820598602295
Validation loss: 2.365529732037616

Epoch: 178| Step: 0
Training loss: 2.4981627464294434
Validation loss: 2.373815664681055

Epoch: 5| Step: 1
Training loss: 2.8494889736175537
Validation loss: 2.3563866999841507

Epoch: 5| Step: 2
Training loss: 2.440056800842285
Validation loss: 2.343761795310564

Epoch: 5| Step: 3
Training loss: 2.744633674621582
Validation loss: 2.3437755441152923

Epoch: 5| Step: 4
Training loss: 2.3308348655700684
Validation loss: 2.326623949953305

Epoch: 5| Step: 5
Training loss: 3.062513828277588
Validation loss: 2.333351286508704

Epoch: 5| Step: 6
Training loss: 2.804452896118164
Validation loss: 2.3401341233202206

Epoch: 5| Step: 7
Training loss: 1.706221342086792
Validation loss: 2.3433234307073776

Epoch: 5| Step: 8
Training loss: 2.5472609996795654
Validation loss: 2.3454225037687566

Epoch: 5| Step: 9
Training loss: 2.637955904006958
Validation loss: 2.3424764499869397

Epoch: 5| Step: 10
Training loss: 2.6864118576049805
Validation loss: 2.34047302123039

Epoch: 179| Step: 0
Training loss: 2.907013416290283
Validation loss: 2.326471628681306

Epoch: 5| Step: 1
Training loss: 2.675057888031006
Validation loss: 2.3252643154513453

Epoch: 5| Step: 2
Training loss: 2.2542564868927
Validation loss: 2.315904184054303

Epoch: 5| Step: 3
Training loss: 2.785061836242676
Validation loss: 2.3255340283916843

Epoch: 5| Step: 4
Training loss: 2.792663812637329
Validation loss: 2.3367031492212766

Epoch: 5| Step: 5
Training loss: 2.2325572967529297
Validation loss: 2.3443750886506933

Epoch: 5| Step: 6
Training loss: 2.434817314147949
Validation loss: 2.350023854163385

Epoch: 5| Step: 7
Training loss: 2.3442513942718506
Validation loss: 2.352178721017735

Epoch: 5| Step: 8
Training loss: 2.3240866661071777
Validation loss: 2.3528751532236734

Epoch: 5| Step: 9
Training loss: 1.9994224309921265
Validation loss: 2.3488026998376332

Epoch: 5| Step: 10
Training loss: 3.419938802719116
Validation loss: 2.341683846648021

Epoch: 180| Step: 0
Training loss: 3.3219172954559326
Validation loss: 2.3410372810979045

Epoch: 5| Step: 1
Training loss: 2.573188543319702
Validation loss: 2.3519331306539555

Epoch: 5| Step: 2
Training loss: 2.7940051555633545
Validation loss: 2.349598196245009

Epoch: 5| Step: 3
Training loss: 1.9576390981674194
Validation loss: 2.33896013229124

Epoch: 5| Step: 4
Training loss: 2.6850643157958984
Validation loss: 2.336481136660422

Epoch: 5| Step: 5
Training loss: 2.5401196479797363
Validation loss: 2.3236957852558424

Epoch: 5| Step: 6
Training loss: 2.427621364593506
Validation loss: 2.317011449926643

Epoch: 5| Step: 7
Training loss: 1.7136766910552979
Validation loss: 2.3141632156987346

Epoch: 5| Step: 8
Training loss: 2.885491132736206
Validation loss: 2.3164296893663305

Epoch: 5| Step: 9
Training loss: 2.555816173553467
Validation loss: 2.319238301246397

Epoch: 5| Step: 10
Training loss: 2.5610859394073486
Validation loss: 2.3213149603976997

Epoch: 181| Step: 0
Training loss: 1.8949470520019531
Validation loss: 2.333012303998393

Epoch: 5| Step: 1
Training loss: 3.012223243713379
Validation loss: 2.3393087117902693

Epoch: 5| Step: 2
Training loss: 2.172778844833374
Validation loss: 2.3409482791859615

Epoch: 5| Step: 3
Training loss: 2.2314865589141846
Validation loss: 2.3336759562133462

Epoch: 5| Step: 4
Training loss: 3.021899461746216
Validation loss: 2.3531314916508173

Epoch: 5| Step: 5
Training loss: 2.2223849296569824
Validation loss: 2.3448321870578233

Epoch: 5| Step: 6
Training loss: 2.8443076610565186
Validation loss: 2.3402391890043854

Epoch: 5| Step: 7
Training loss: 2.853055715560913
Validation loss: 2.3449624610203568

Epoch: 5| Step: 8
Training loss: 2.3543496131896973
Validation loss: 2.3477794893326296

Epoch: 5| Step: 9
Training loss: 2.473217248916626
Validation loss: 2.3459697538806545

Epoch: 5| Step: 10
Training loss: 2.940671682357788
Validation loss: 2.3359811485454602

Epoch: 182| Step: 0
Training loss: 2.364011287689209
Validation loss: 2.3306940242808354

Epoch: 5| Step: 1
Training loss: 2.658048152923584
Validation loss: 2.329284450059296

Epoch: 5| Step: 2
Training loss: 2.233572483062744
Validation loss: 2.334920472996209

Epoch: 5| Step: 3
Training loss: 2.558868408203125
Validation loss: 2.330588307431949

Epoch: 5| Step: 4
Training loss: 2.256765842437744
Validation loss: 2.3302636043999785

Epoch: 5| Step: 5
Training loss: 2.523702383041382
Validation loss: 2.328701114141813

Epoch: 5| Step: 6
Training loss: 2.3899478912353516
Validation loss: 2.334359368970317

Epoch: 5| Step: 7
Training loss: 2.848562717437744
Validation loss: 2.341595116481986

Epoch: 5| Step: 8
Training loss: 3.6247305870056152
Validation loss: 2.344596783320109

Epoch: 5| Step: 9
Training loss: 2.0748679637908936
Validation loss: 2.3430711146323913

Epoch: 5| Step: 10
Training loss: 2.310845375061035
Validation loss: 2.351589684845299

Epoch: 183| Step: 0
Training loss: 2.4406497478485107
Validation loss: 2.3378813753845873

Epoch: 5| Step: 1
Training loss: 2.6869711875915527
Validation loss: 2.338919044822775

Epoch: 5| Step: 2
Training loss: 2.8288698196411133
Validation loss: 2.337622163116291

Epoch: 5| Step: 3
Training loss: 2.5277669429779053
Validation loss: 2.330246922790363

Epoch: 5| Step: 4
Training loss: 2.4633255004882812
Validation loss: 2.336897132217243

Epoch: 5| Step: 5
Training loss: 2.5706255435943604
Validation loss: 2.3591295314091507

Epoch: 5| Step: 6
Training loss: 2.511265754699707
Validation loss: 2.3795815795980473

Epoch: 5| Step: 7
Training loss: 2.252473831176758
Validation loss: 2.3844140114322787

Epoch: 5| Step: 8
Training loss: 2.4718005657196045
Validation loss: 2.3833181627335085

Epoch: 5| Step: 9
Training loss: 2.8989410400390625
Validation loss: 2.369163441401656

Epoch: 5| Step: 10
Training loss: 2.422286033630371
Validation loss: 2.3469071606154084

Epoch: 184| Step: 0
Training loss: 2.448159694671631
Validation loss: 2.345013697942098

Epoch: 5| Step: 1
Training loss: 2.3721792697906494
Validation loss: 2.346426717696651

Epoch: 5| Step: 2
Training loss: 2.808765172958374
Validation loss: 2.35889958822599

Epoch: 5| Step: 3
Training loss: 2.3990986347198486
Validation loss: 2.360109782988025

Epoch: 5| Step: 4
Training loss: 2.6536402702331543
Validation loss: 2.3573312733763006

Epoch: 5| Step: 5
Training loss: 2.3711724281311035
Validation loss: 2.346225743652672

Epoch: 5| Step: 6
Training loss: 2.7903618812561035
Validation loss: 2.3363432268942557

Epoch: 5| Step: 7
Training loss: 2.8323094844818115
Validation loss: 2.3183465926877913

Epoch: 5| Step: 8
Training loss: 2.6155996322631836
Validation loss: 2.3023400998884633

Epoch: 5| Step: 9
Training loss: 2.3087844848632812
Validation loss: 2.3059022272786787

Epoch: 5| Step: 10
Training loss: 2.2648050785064697
Validation loss: 2.2941827440774567

Epoch: 185| Step: 0
Training loss: 2.9180996417999268
Validation loss: 2.292171811544767

Epoch: 5| Step: 1
Training loss: 2.487105131149292
Validation loss: 2.296054087659364

Epoch: 5| Step: 2
Training loss: 2.81512188911438
Validation loss: 2.305086780619878

Epoch: 5| Step: 3
Training loss: 2.2001850605010986
Validation loss: 2.3167563984470982

Epoch: 5| Step: 4
Training loss: 2.460167169570923
Validation loss: 2.321345058820581

Epoch: 5| Step: 5
Training loss: 2.8237383365631104
Validation loss: 2.3111111117947485

Epoch: 5| Step: 6
Training loss: 2.87772274017334
Validation loss: 2.3061907983595327

Epoch: 5| Step: 7
Training loss: 2.509235382080078
Validation loss: 2.308353042089811

Epoch: 5| Step: 8
Training loss: 2.4435927867889404
Validation loss: 2.3083349274050806

Epoch: 5| Step: 9
Training loss: 2.411468029022217
Validation loss: 2.30884858100645

Epoch: 5| Step: 10
Training loss: 2.0767149925231934
Validation loss: 2.3055367777424474

Epoch: 186| Step: 0
Training loss: 3.0217461585998535
Validation loss: 2.3061691176506782

Epoch: 5| Step: 1
Training loss: 2.338637590408325
Validation loss: 2.2984948850447133

Epoch: 5| Step: 2
Training loss: 1.9666640758514404
Validation loss: 2.2896441310964604

Epoch: 5| Step: 3
Training loss: 2.7215030193328857
Validation loss: 2.2880383512025237

Epoch: 5| Step: 4
Training loss: 2.792677879333496
Validation loss: 2.298877992937642

Epoch: 5| Step: 5
Training loss: 2.476438522338867
Validation loss: 2.297707808915005

Epoch: 5| Step: 6
Training loss: 2.300929546356201
Validation loss: 2.299222138620192

Epoch: 5| Step: 7
Training loss: 2.7793326377868652
Validation loss: 2.3005461385173183

Epoch: 5| Step: 8
Training loss: 2.0469179153442383
Validation loss: 2.311462543343985

Epoch: 5| Step: 9
Training loss: 2.9608547687530518
Validation loss: 2.3249780849743913

Epoch: 5| Step: 10
Training loss: 2.539050817489624
Validation loss: 2.3287856963373

Epoch: 187| Step: 0
Training loss: 2.1270241737365723
Validation loss: 2.338355366901685

Epoch: 5| Step: 1
Training loss: 2.9553260803222656
Validation loss: 2.3319775673650924

Epoch: 5| Step: 2
Training loss: 2.179858446121216
Validation loss: 2.3284159655212076

Epoch: 5| Step: 3
Training loss: 2.9054813385009766
Validation loss: 2.334516212504397

Epoch: 5| Step: 4
Training loss: 2.1764144897460938
Validation loss: 2.323169133996451

Epoch: 5| Step: 5
Training loss: 2.1273090839385986
Validation loss: 2.326578294077227

Epoch: 5| Step: 6
Training loss: 3.1643424034118652
Validation loss: 2.3325110045812463

Epoch: 5| Step: 7
Training loss: 2.3336148262023926
Validation loss: 2.325303522489404

Epoch: 5| Step: 8
Training loss: 2.416844129562378
Validation loss: 2.3203531926678074

Epoch: 5| Step: 9
Training loss: 2.552020788192749
Validation loss: 2.324685399250318

Epoch: 5| Step: 10
Training loss: 2.9614694118499756
Validation loss: 2.312399806514863

Epoch: 188| Step: 0
Training loss: 2.4914207458496094
Validation loss: 2.318797598602951

Epoch: 5| Step: 1
Training loss: 2.2803261280059814
Validation loss: 2.3245071185532438

Epoch: 5| Step: 2
Training loss: 2.954155921936035
Validation loss: 2.3295665402566232

Epoch: 5| Step: 3
Training loss: 2.27671217918396
Validation loss: 2.3363459417896886

Epoch: 5| Step: 4
Training loss: 2.3854305744171143
Validation loss: 2.32291421838986

Epoch: 5| Step: 5
Training loss: 2.495784282684326
Validation loss: 2.3288098304502425

Epoch: 5| Step: 6
Training loss: 2.821413040161133
Validation loss: 2.3327769002606793

Epoch: 5| Step: 7
Training loss: 2.0639445781707764
Validation loss: 2.3251290423895723

Epoch: 5| Step: 8
Training loss: 2.5847437381744385
Validation loss: 2.3318273277692896

Epoch: 5| Step: 9
Training loss: 2.739696741104126
Validation loss: 2.3360787540353756

Epoch: 5| Step: 10
Training loss: 2.566377878189087
Validation loss: 2.3498984972635903

Epoch: 189| Step: 0
Training loss: 2.404790163040161
Validation loss: 2.349884803577136

Epoch: 5| Step: 1
Training loss: 2.98288631439209
Validation loss: 2.350227778957736

Epoch: 5| Step: 2
Training loss: 2.798229694366455
Validation loss: 2.3484756356926373

Epoch: 5| Step: 3
Training loss: 2.5992679595947266
Validation loss: 2.31694023711707

Epoch: 5| Step: 4
Training loss: 2.3404719829559326
Validation loss: 2.2998131923778082

Epoch: 5| Step: 5
Training loss: 2.3594470024108887
Validation loss: 2.2990559262614094

Epoch: 5| Step: 6
Training loss: 2.6755385398864746
Validation loss: 2.3036610105986237

Epoch: 5| Step: 7
Training loss: 2.7998242378234863
Validation loss: 2.3106353667474564

Epoch: 5| Step: 8
Training loss: 2.1570236682891846
Validation loss: 2.31558169088056

Epoch: 5| Step: 9
Training loss: 2.124929904937744
Validation loss: 2.3088483887334026

Epoch: 5| Step: 10
Training loss: 2.4162256717681885
Validation loss: 2.3062141172347532

Epoch: 190| Step: 0
Training loss: 2.245356559753418
Validation loss: 2.3071868086373932

Epoch: 5| Step: 1
Training loss: 2.3968684673309326
Validation loss: 2.308284021192981

Epoch: 5| Step: 2
Training loss: 2.3550450801849365
Validation loss: 2.306347946966848

Epoch: 5| Step: 3
Training loss: 2.9402737617492676
Validation loss: 2.3057142855018697

Epoch: 5| Step: 4
Training loss: 2.3353183269500732
Validation loss: 2.29959455356803

Epoch: 5| Step: 5
Training loss: 2.427189588546753
Validation loss: 2.3116430338992866

Epoch: 5| Step: 6
Training loss: 3.0603713989257812
Validation loss: 2.314462815561602

Epoch: 5| Step: 7
Training loss: 2.0951104164123535
Validation loss: 2.333027316677955

Epoch: 5| Step: 8
Training loss: 3.195885419845581
Validation loss: 2.342286984125773

Epoch: 5| Step: 9
Training loss: 2.1995017528533936
Validation loss: 2.3607090570593394

Epoch: 5| Step: 10
Training loss: 2.37619686126709
Validation loss: 2.3744046688079834

Epoch: 191| Step: 0
Training loss: 1.707467794418335
Validation loss: 2.362223171418713

Epoch: 5| Step: 1
Training loss: 2.6890244483947754
Validation loss: 2.3634598383339505

Epoch: 5| Step: 2
Training loss: 2.5454583168029785
Validation loss: 2.3505071773323962

Epoch: 5| Step: 3
Training loss: 2.2830214500427246
Validation loss: 2.340681488795947

Epoch: 5| Step: 4
Training loss: 2.4757704734802246
Validation loss: 2.331855015088153

Epoch: 5| Step: 5
Training loss: 2.8491928577423096
Validation loss: 2.3222832243929625

Epoch: 5| Step: 6
Training loss: 2.9994757175445557
Validation loss: 2.3166443250512563

Epoch: 5| Step: 7
Training loss: 2.7175774574279785
Validation loss: 2.3123661395042174

Epoch: 5| Step: 8
Training loss: 2.9401116371154785
Validation loss: 2.3053295048334266

Epoch: 5| Step: 9
Training loss: 2.3584251403808594
Validation loss: 2.301223660028109

Epoch: 5| Step: 10
Training loss: 1.976141095161438
Validation loss: 2.30746357158948

Epoch: 192| Step: 0
Training loss: 2.4287657737731934
Validation loss: 2.3244873503203034

Epoch: 5| Step: 1
Training loss: 2.6069624423980713
Validation loss: 2.331188009631249

Epoch: 5| Step: 2
Training loss: 2.524951457977295
Validation loss: 2.338554861725018

Epoch: 5| Step: 3
Training loss: 2.365938663482666
Validation loss: 2.3368385197013937

Epoch: 5| Step: 4
Training loss: 2.266003370285034
Validation loss: 2.3167482781153854

Epoch: 5| Step: 5
Training loss: 2.673401355743408
Validation loss: 2.3184397630794074

Epoch: 5| Step: 6
Training loss: 2.6304945945739746
Validation loss: 2.3204002021461405

Epoch: 5| Step: 7
Training loss: 2.14750599861145
Validation loss: 2.321432823775917

Epoch: 5| Step: 8
Training loss: 2.354706287384033
Validation loss: 2.3315682718830724

Epoch: 5| Step: 9
Training loss: 2.236687183380127
Validation loss: 2.342615763346354

Epoch: 5| Step: 10
Training loss: 3.414801836013794
Validation loss: 2.3256482360183552

Epoch: 193| Step: 0
Training loss: 2.4020111560821533
Validation loss: 2.3102275453588015

Epoch: 5| Step: 1
Training loss: 2.628091812133789
Validation loss: 2.2814716139147357

Epoch: 5| Step: 2
Training loss: 2.2517545223236084
Validation loss: 2.2775751518946823

Epoch: 5| Step: 3
Training loss: 2.927905559539795
Validation loss: 2.2763023068827968

Epoch: 5| Step: 4
Training loss: 2.538212299346924
Validation loss: 2.2776450662202734

Epoch: 5| Step: 5
Training loss: 2.7667665481567383
Validation loss: 2.269255353558448

Epoch: 5| Step: 6
Training loss: 2.491506338119507
Validation loss: 2.2709247501947547

Epoch: 5| Step: 7
Training loss: 2.139047145843506
Validation loss: 2.268144356307163

Epoch: 5| Step: 8
Training loss: 2.849325656890869
Validation loss: 2.2771957869170816

Epoch: 5| Step: 9
Training loss: 2.002455234527588
Validation loss: 2.2903750763144544

Epoch: 5| Step: 10
Training loss: 2.6986663341522217
Validation loss: 2.2927481538505963

Epoch: 194| Step: 0
Training loss: 2.7462704181671143
Validation loss: 2.310088267890356

Epoch: 5| Step: 1
Training loss: 2.610867977142334
Validation loss: 2.3067703298343125

Epoch: 5| Step: 2
Training loss: 2.2433810234069824
Validation loss: 2.3240517467580815

Epoch: 5| Step: 3
Training loss: 2.8764100074768066
Validation loss: 2.3274073805860294

Epoch: 5| Step: 4
Training loss: 1.8858604431152344
Validation loss: 2.3221015058537966

Epoch: 5| Step: 5
Training loss: 2.827643871307373
Validation loss: 2.3334496277634815

Epoch: 5| Step: 6
Training loss: 2.3845908641815186
Validation loss: 2.323975116975846

Epoch: 5| Step: 7
Training loss: 2.617908477783203
Validation loss: 2.3309058297064995

Epoch: 5| Step: 8
Training loss: 2.5215423107147217
Validation loss: 2.326798395443988

Epoch: 5| Step: 9
Training loss: 2.5794286727905273
Validation loss: 2.3157440898238972

Epoch: 5| Step: 10
Training loss: 2.2303497791290283
Validation loss: 2.305160912134314

Epoch: 195| Step: 0
Training loss: 2.5629451274871826
Validation loss: 2.296265479057066

Epoch: 5| Step: 1
Training loss: 2.3894526958465576
Validation loss: 2.2864521946958316

Epoch: 5| Step: 2
Training loss: 2.193143367767334
Validation loss: 2.2900423234508884

Epoch: 5| Step: 3
Training loss: 2.362588882446289
Validation loss: 2.280092639307822

Epoch: 5| Step: 4
Training loss: 2.4435830116271973
Validation loss: 2.2791581487142913

Epoch: 5| Step: 5
Training loss: 1.8897984027862549
Validation loss: 2.2826412493182766

Epoch: 5| Step: 6
Training loss: 1.9196516275405884
Validation loss: 2.276377898390575

Epoch: 5| Step: 7
Training loss: 3.374687910079956
Validation loss: 2.2772006475797264

Epoch: 5| Step: 8
Training loss: 2.602552652359009
Validation loss: 2.278776614896713

Epoch: 5| Step: 9
Training loss: 2.9726366996765137
Validation loss: 2.2857627561015468

Epoch: 5| Step: 10
Training loss: 2.804877758026123
Validation loss: 2.3055216676445416

Epoch: 196| Step: 0
Training loss: 2.257474422454834
Validation loss: 2.3013984618648404

Epoch: 5| Step: 1
Training loss: 2.298598051071167
Validation loss: 2.3130302095925934

Epoch: 5| Step: 2
Training loss: 2.348254680633545
Validation loss: 2.319173320647209

Epoch: 5| Step: 3
Training loss: 2.690062999725342
Validation loss: 2.3311264514923096

Epoch: 5| Step: 4
Training loss: 2.379861831665039
Validation loss: 2.347134494012402

Epoch: 5| Step: 5
Training loss: 2.3662478923797607
Validation loss: 2.3638570641958587

Epoch: 5| Step: 6
Training loss: 2.490286350250244
Validation loss: 2.395871188050957

Epoch: 5| Step: 7
Training loss: 2.594301223754883
Validation loss: 2.3701503815189486

Epoch: 5| Step: 8
Training loss: 2.538649559020996
Validation loss: 2.338006314410958

Epoch: 5| Step: 9
Training loss: 2.4747533798217773
Validation loss: 2.3265961959797847

Epoch: 5| Step: 10
Training loss: 3.093238353729248
Validation loss: 2.3036118092075473

Epoch: 197| Step: 0
Training loss: 2.1891064643859863
Validation loss: 2.28329046310917

Epoch: 5| Step: 1
Training loss: 2.1392481327056885
Validation loss: 2.2738334799325592

Epoch: 5| Step: 2
Training loss: 2.2144155502319336
Validation loss: 2.259764486743558

Epoch: 5| Step: 3
Training loss: 2.6041312217712402
Validation loss: 2.259727026826592

Epoch: 5| Step: 4
Training loss: 3.3428707122802734
Validation loss: 2.2667711691189836

Epoch: 5| Step: 5
Training loss: 2.134084463119507
Validation loss: 2.270373229057558

Epoch: 5| Step: 6
Training loss: 2.066007137298584
Validation loss: 2.277729841970628

Epoch: 5| Step: 7
Training loss: 3.223567485809326
Validation loss: 2.28828513750466

Epoch: 5| Step: 8
Training loss: 2.6978886127471924
Validation loss: 2.300630454094179

Epoch: 5| Step: 9
Training loss: 2.265669345855713
Validation loss: 2.314161564714165

Epoch: 5| Step: 10
Training loss: 2.810544967651367
Validation loss: 2.3311560641052904

Epoch: 198| Step: 0
Training loss: 2.2565178871154785
Validation loss: 2.3201071652032996

Epoch: 5| Step: 1
Training loss: 2.3667778968811035
Validation loss: 2.324576513741606

Epoch: 5| Step: 2
Training loss: 2.3519058227539062
Validation loss: 2.3252578179041543

Epoch: 5| Step: 3
Training loss: 1.8956124782562256
Validation loss: 2.3246876142358266

Epoch: 5| Step: 4
Training loss: 2.7070116996765137
Validation loss: 2.3433018627987114

Epoch: 5| Step: 5
Training loss: 2.5216732025146484
Validation loss: 2.34880728619073

Epoch: 5| Step: 6
Training loss: 2.2699177265167236
Validation loss: 2.343266566594442

Epoch: 5| Step: 7
Training loss: 3.2349002361297607
Validation loss: 2.350331252621066

Epoch: 5| Step: 8
Training loss: 2.6400809288024902
Validation loss: 2.308502120356406

Epoch: 5| Step: 9
Training loss: 2.72622013092041
Validation loss: 2.2865261031735327

Epoch: 5| Step: 10
Training loss: 2.4630281925201416
Validation loss: 2.2812745981318976

Epoch: 199| Step: 0
Training loss: 2.453294277191162
Validation loss: 2.2751655706795315

Epoch: 5| Step: 1
Training loss: 2.881664991378784
Validation loss: 2.276616447715349

Epoch: 5| Step: 2
Training loss: 2.3473894596099854
Validation loss: 2.2762797481270245

Epoch: 5| Step: 3
Training loss: 3.1595659255981445
Validation loss: 2.272920130401529

Epoch: 5| Step: 4
Training loss: 2.4472057819366455
Validation loss: 2.2788459357394966

Epoch: 5| Step: 5
Training loss: 2.2721896171569824
Validation loss: 2.275431333049651

Epoch: 5| Step: 6
Training loss: 2.6836652755737305
Validation loss: 2.278327934203609

Epoch: 5| Step: 7
Training loss: 2.169132709503174
Validation loss: 2.284363308260518

Epoch: 5| Step: 8
Training loss: 2.1314940452575684
Validation loss: 2.2817692833562053

Epoch: 5| Step: 9
Training loss: 2.391075611114502
Validation loss: 2.280324638530772

Epoch: 5| Step: 10
Training loss: 2.5625271797180176
Validation loss: 2.2838566123798327

Epoch: 200| Step: 0
Training loss: 2.9188618659973145
Validation loss: 2.2744844831446165

Epoch: 5| Step: 1
Training loss: 2.493492603302002
Validation loss: 2.271466629479521

Epoch: 5| Step: 2
Training loss: 2.5586459636688232
Validation loss: 2.276798773837346

Epoch: 5| Step: 3
Training loss: 2.0602359771728516
Validation loss: 2.267977060810212

Epoch: 5| Step: 4
Training loss: 2.6499392986297607
Validation loss: 2.273748920809838

Epoch: 5| Step: 5
Training loss: 2.72355318069458
Validation loss: 2.276610697469404

Epoch: 5| Step: 6
Training loss: 2.231125593185425
Validation loss: 2.2764814412722023

Epoch: 5| Step: 7
Training loss: 3.015291690826416
Validation loss: 2.289991424929711

Epoch: 5| Step: 8
Training loss: 1.678224802017212
Validation loss: 2.298366041593654

Epoch: 5| Step: 9
Training loss: 2.5865912437438965
Validation loss: 2.3114369761559272

Epoch: 5| Step: 10
Training loss: 2.3421521186828613
Validation loss: 2.3360117250873196

Epoch: 201| Step: 0
Training loss: 2.69769549369812
Validation loss: 2.337220658538162

Epoch: 5| Step: 1
Training loss: 2.239161491394043
Validation loss: 2.342873204138971

Epoch: 5| Step: 2
Training loss: 2.4825191497802734
Validation loss: 2.3502296042698685

Epoch: 5| Step: 3
Training loss: 2.6120388507843018
Validation loss: 2.349234511775355

Epoch: 5| Step: 4
Training loss: 2.321652412414551
Validation loss: 2.3545123761700046

Epoch: 5| Step: 5
Training loss: 2.362340211868286
Validation loss: 2.3382270182332685

Epoch: 5| Step: 6
Training loss: 2.502483606338501
Validation loss: 2.323973709537137

Epoch: 5| Step: 7
Training loss: 2.9575603008270264
Validation loss: 2.3063810128037647

Epoch: 5| Step: 8
Training loss: 1.9318339824676514
Validation loss: 2.2891386529450775

Epoch: 5| Step: 9
Training loss: 2.416167974472046
Validation loss: 2.27809823712995

Epoch: 5| Step: 10
Training loss: 2.73293399810791
Validation loss: 2.2790086551379134

Epoch: 202| Step: 0
Training loss: 2.804107189178467
Validation loss: 2.273191031589303

Epoch: 5| Step: 1
Training loss: 3.0528366565704346
Validation loss: 2.2619700931733653

Epoch: 5| Step: 2
Training loss: 2.14616322517395
Validation loss: 2.2724530850687334

Epoch: 5| Step: 3
Training loss: 2.4511284828186035
Validation loss: 2.2633202716868412

Epoch: 5| Step: 4
Training loss: 2.138324499130249
Validation loss: 2.2718798524589947

Epoch: 5| Step: 5
Training loss: 2.242368221282959
Validation loss: 2.276434201066212

Epoch: 5| Step: 6
Training loss: 2.097945213317871
Validation loss: 2.2724313992325977

Epoch: 5| Step: 7
Training loss: 2.351820468902588
Validation loss: 2.278781521704889

Epoch: 5| Step: 8
Training loss: 2.8962137699127197
Validation loss: 2.287552264428908

Epoch: 5| Step: 9
Training loss: 2.60922908782959
Validation loss: 2.289039937398767

Epoch: 5| Step: 10
Training loss: 2.3038792610168457
Validation loss: 2.2952462960315008

Epoch: 203| Step: 0
Training loss: 2.474560260772705
Validation loss: 2.297676588899346

Epoch: 5| Step: 1
Training loss: 2.9450583457946777
Validation loss: 2.2949493649185344

Epoch: 5| Step: 2
Training loss: 2.6405396461486816
Validation loss: 2.2967090581053045

Epoch: 5| Step: 3
Training loss: 2.3498587608337402
Validation loss: 2.298174632492886

Epoch: 5| Step: 4
Training loss: 2.465489387512207
Validation loss: 2.2936514193011868

Epoch: 5| Step: 5
Training loss: 1.9070472717285156
Validation loss: 2.3027295758647304

Epoch: 5| Step: 6
Training loss: 1.9245513677597046
Validation loss: 2.3085614430007113

Epoch: 5| Step: 7
Training loss: 2.7348172664642334
Validation loss: 2.3092324349188034

Epoch: 5| Step: 8
Training loss: 2.7516632080078125
Validation loss: 2.31063574616627

Epoch: 5| Step: 9
Training loss: 2.66528058052063
Validation loss: 2.3100811435330297

Epoch: 5| Step: 10
Training loss: 2.1866164207458496
Validation loss: 2.2939949368917816

Epoch: 204| Step: 0
Training loss: 2.5356879234313965
Validation loss: 2.274669154997795

Epoch: 5| Step: 1
Training loss: 2.8946030139923096
Validation loss: 2.2769492364698842

Epoch: 5| Step: 2
Training loss: 2.5086207389831543
Validation loss: 2.2763387400616883

Epoch: 5| Step: 3
Training loss: 2.585015058517456
Validation loss: 2.2588506616571897

Epoch: 5| Step: 4
Training loss: 1.9964135885238647
Validation loss: 2.262392933650683

Epoch: 5| Step: 5
Training loss: 2.0981783866882324
Validation loss: 2.261723554262551

Epoch: 5| Step: 6
Training loss: 2.1487269401550293
Validation loss: 2.2541689257467947

Epoch: 5| Step: 7
Training loss: 2.508568525314331
Validation loss: 2.2610651241835726

Epoch: 5| Step: 8
Training loss: 2.846611499786377
Validation loss: 2.265919248263041

Epoch: 5| Step: 9
Training loss: 2.574871301651001
Validation loss: 2.2854306826027493

Epoch: 5| Step: 10
Training loss: 2.4866135120391846
Validation loss: 2.2778844884646836

Epoch: 205| Step: 0
Training loss: 3.078211545944214
Validation loss: 2.279966551770446

Epoch: 5| Step: 1
Training loss: 2.1762242317199707
Validation loss: 2.2838332063408306

Epoch: 5| Step: 2
Training loss: 1.6807292699813843
Validation loss: 2.2912601552983767

Epoch: 5| Step: 3
Training loss: 3.249152660369873
Validation loss: 2.2974734203789824

Epoch: 5| Step: 4
Training loss: 3.1006736755371094
Validation loss: 2.3048303588744132

Epoch: 5| Step: 5
Training loss: 2.4737918376922607
Validation loss: 2.3093015788703837

Epoch: 5| Step: 6
Training loss: 2.113344669342041
Validation loss: 2.2953154271648777

Epoch: 5| Step: 7
Training loss: 2.2975211143493652
Validation loss: 2.3118112805069133

Epoch: 5| Step: 8
Training loss: 2.116788864135742
Validation loss: 2.2918496183169785

Epoch: 5| Step: 9
Training loss: 2.7873997688293457
Validation loss: 2.2900387215357956

Epoch: 5| Step: 10
Training loss: 1.822521448135376
Validation loss: 2.2955649591261342

Epoch: 206| Step: 0
Training loss: 1.9207847118377686
Validation loss: 2.29103910282094

Epoch: 5| Step: 1
Training loss: 2.807788610458374
Validation loss: 2.2885855731143745

Epoch: 5| Step: 2
Training loss: 2.6521716117858887
Validation loss: 2.2835853253641436

Epoch: 5| Step: 3
Training loss: 2.9815518856048584
Validation loss: 2.2735088897007767

Epoch: 5| Step: 4
Training loss: 3.025829792022705
Validation loss: 2.2690947645454

Epoch: 5| Step: 5
Training loss: 1.8301851749420166
Validation loss: 2.2661734678411998

Epoch: 5| Step: 6
Training loss: 2.3282532691955566
Validation loss: 2.2779821606092554

Epoch: 5| Step: 7
Training loss: 2.5857977867126465
Validation loss: 2.2795352012880388

Epoch: 5| Step: 8
Training loss: 2.202130079269409
Validation loss: 2.2896841572177027

Epoch: 5| Step: 9
Training loss: 2.974775552749634
Validation loss: 2.300158400689402

Epoch: 5| Step: 10
Training loss: 1.4027252197265625
Validation loss: 2.314555334788497

Epoch: 207| Step: 0
Training loss: 2.1135592460632324
Validation loss: 2.3262805502901793

Epoch: 5| Step: 1
Training loss: 2.4733920097351074
Validation loss: 2.3458048489785965

Epoch: 5| Step: 2
Training loss: 2.528007745742798
Validation loss: 2.326703438194849

Epoch: 5| Step: 3
Training loss: 2.372922897338867
Validation loss: 2.3231366167786303

Epoch: 5| Step: 4
Training loss: 2.3764069080352783
Validation loss: 2.313501093977241

Epoch: 5| Step: 5
Training loss: 2.324502944946289
Validation loss: 2.298267808011783

Epoch: 5| Step: 6
Training loss: 2.8704071044921875
Validation loss: 2.2957686608837498

Epoch: 5| Step: 7
Training loss: 2.215911865234375
Validation loss: 2.292437477778363

Epoch: 5| Step: 8
Training loss: 2.1134531497955322
Validation loss: 2.281328660185619

Epoch: 5| Step: 9
Training loss: 2.7097151279449463
Validation loss: 2.291659819182529

Epoch: 5| Step: 10
Training loss: 2.683743953704834
Validation loss: 2.2775239662457536

Epoch: 208| Step: 0
Training loss: 2.3938310146331787
Validation loss: 2.280693164435766

Epoch: 5| Step: 1
Training loss: 2.4862217903137207
Validation loss: 2.2856686692084036

Epoch: 5| Step: 2
Training loss: 2.5419273376464844
Validation loss: 2.2802364903111614

Epoch: 5| Step: 3
Training loss: 2.590118169784546
Validation loss: 2.295279315722886

Epoch: 5| Step: 4
Training loss: 2.9082000255584717
Validation loss: 2.2890739389645156

Epoch: 5| Step: 5
Training loss: 2.61739182472229
Validation loss: 2.2798673004232426

Epoch: 5| Step: 6
Training loss: 2.509277820587158
Validation loss: 2.2734078950779413

Epoch: 5| Step: 7
Training loss: 2.1308979988098145
Validation loss: 2.270973211975508

Epoch: 5| Step: 8
Training loss: 2.3471620082855225
Validation loss: 2.2971580002897527

Epoch: 5| Step: 9
Training loss: 1.7504507303237915
Validation loss: 2.309973955154419

Epoch: 5| Step: 10
Training loss: 2.601087808609009
Validation loss: 2.317040207565472

Epoch: 209| Step: 0
Training loss: 2.4672577381134033
Validation loss: 2.3270092933408675

Epoch: 5| Step: 1
Training loss: 2.8254079818725586
Validation loss: 2.3150374633009716

Epoch: 5| Step: 2
Training loss: 2.130646228790283
Validation loss: 2.2932396191422657

Epoch: 5| Step: 3
Training loss: 2.6059162616729736
Validation loss: 2.282715925606348

Epoch: 5| Step: 4
Training loss: 2.5004334449768066
Validation loss: 2.278712723844795

Epoch: 5| Step: 5
Training loss: 2.084138870239258
Validation loss: 2.2775910874848724

Epoch: 5| Step: 6
Training loss: 2.5684704780578613
Validation loss: 2.2643891201224378

Epoch: 5| Step: 7
Training loss: 2.6196460723876953
Validation loss: 2.2733360336672876

Epoch: 5| Step: 8
Training loss: 2.5246634483337402
Validation loss: 2.2860217299512637

Epoch: 5| Step: 9
Training loss: 2.059504508972168
Validation loss: 2.2927587621955463

Epoch: 5| Step: 10
Training loss: 2.327240467071533
Validation loss: 2.2935366938191075

Epoch: 210| Step: 0
Training loss: 3.0388646125793457
Validation loss: 2.313854089347265

Epoch: 5| Step: 1
Training loss: 2.457900285720825
Validation loss: 2.3051335824433195

Epoch: 5| Step: 2
Training loss: 2.357448101043701
Validation loss: 2.30239773309359

Epoch: 5| Step: 3
Training loss: 2.895388126373291
Validation loss: 2.2867079704038558

Epoch: 5| Step: 4
Training loss: 1.7226473093032837
Validation loss: 2.2742446596904466

Epoch: 5| Step: 5
Training loss: 2.558137893676758
Validation loss: 2.2827615327732538

Epoch: 5| Step: 6
Training loss: 2.0607471466064453
Validation loss: 2.298737692576583

Epoch: 5| Step: 7
Training loss: 1.9685823917388916
Validation loss: 2.310773741814398

Epoch: 5| Step: 8
Training loss: 2.8631696701049805
Validation loss: 2.3209861401588685

Epoch: 5| Step: 9
Training loss: 2.724987030029297
Validation loss: 2.3324826173884894

Epoch: 5| Step: 10
Training loss: 2.0521438121795654
Validation loss: 2.3472851604543705

Epoch: 211| Step: 0
Training loss: 2.7387146949768066
Validation loss: 2.333608337627944

Epoch: 5| Step: 1
Training loss: 2.6923282146453857
Validation loss: 2.353882389683877

Epoch: 5| Step: 2
Training loss: 2.428014039993286
Validation loss: 2.3219871649178128

Epoch: 5| Step: 3
Training loss: 1.8955729007720947
Validation loss: 2.3177650641369563

Epoch: 5| Step: 4
Training loss: 2.057130813598633
Validation loss: 2.3251411299551688

Epoch: 5| Step: 5
Training loss: 1.8953831195831299
Validation loss: 2.305837805553149

Epoch: 5| Step: 6
Training loss: 2.5040829181671143
Validation loss: 2.299935663900068

Epoch: 5| Step: 7
Training loss: 2.014589548110962
Validation loss: 2.3033688888754895

Epoch: 5| Step: 8
Training loss: 2.541154146194458
Validation loss: 2.2916890908313055

Epoch: 5| Step: 9
Training loss: 3.0445570945739746
Validation loss: 2.2915244692115375

Epoch: 5| Step: 10
Training loss: 2.8097071647644043
Validation loss: 2.2863699851497525

Epoch: 212| Step: 0
Training loss: 2.4340410232543945
Validation loss: 2.2877468216803765

Epoch: 5| Step: 1
Training loss: 2.050673007965088
Validation loss: 2.294230890530412

Epoch: 5| Step: 2
Training loss: 3.013629198074341
Validation loss: 2.3051386571699575

Epoch: 5| Step: 3
Training loss: 2.795659303665161
Validation loss: 2.3151569699728363

Epoch: 5| Step: 4
Training loss: 2.46931529045105
Validation loss: 2.3360196185368363

Epoch: 5| Step: 5
Training loss: 2.2018160820007324
Validation loss: 2.3482386399340887

Epoch: 5| Step: 6
Training loss: 2.4971323013305664
Validation loss: 2.3775170836397397

Epoch: 5| Step: 7
Training loss: 2.2527432441711426
Validation loss: 2.401070938315443

Epoch: 5| Step: 8
Training loss: 2.6954379081726074
Validation loss: 2.4046600377687843

Epoch: 5| Step: 9
Training loss: 2.1951355934143066
Validation loss: 2.3908587348076606

Epoch: 5| Step: 10
Training loss: 2.2726852893829346
Validation loss: 2.376723894508936

Epoch: 213| Step: 0
Training loss: 2.2504162788391113
Validation loss: 2.35942510379258

Epoch: 5| Step: 1
Training loss: 2.729461669921875
Validation loss: 2.339472298981041

Epoch: 5| Step: 2
Training loss: 2.2694497108459473
Validation loss: 2.310046777930311

Epoch: 5| Step: 3
Training loss: 2.1853420734405518
Validation loss: 2.297700200029599

Epoch: 5| Step: 4
Training loss: 2.5747580528259277
Validation loss: 2.285046433889738

Epoch: 5| Step: 5
Training loss: 2.8546035289764404
Validation loss: 2.273055409872404

Epoch: 5| Step: 6
Training loss: 2.4679596424102783
Validation loss: 2.2695901368253972

Epoch: 5| Step: 7
Training loss: 1.8549728393554688
Validation loss: 2.2543767844477007

Epoch: 5| Step: 8
Training loss: 2.4004921913146973
Validation loss: 2.2581744296576387

Epoch: 5| Step: 9
Training loss: 2.511862277984619
Validation loss: 2.2547532819932505

Epoch: 5| Step: 10
Training loss: 2.578603506088257
Validation loss: 2.2526241297362954

Epoch: 214| Step: 0
Training loss: 2.431295156478882
Validation loss: 2.254030837807604

Epoch: 5| Step: 1
Training loss: 2.4823250770568848
Validation loss: 2.267179666026946

Epoch: 5| Step: 2
Training loss: 2.09228777885437
Validation loss: 2.272260360820319

Epoch: 5| Step: 3
Training loss: 2.516392946243286
Validation loss: 2.2817400860530075

Epoch: 5| Step: 4
Training loss: 2.669593334197998
Validation loss: 2.2810775874763407

Epoch: 5| Step: 5
Training loss: 2.262662172317505
Validation loss: 2.298722856788225

Epoch: 5| Step: 6
Training loss: 2.648967981338501
Validation loss: 2.310817200650451

Epoch: 5| Step: 7
Training loss: 1.967084288597107
Validation loss: 2.311664045497935

Epoch: 5| Step: 8
Training loss: 3.00203800201416
Validation loss: 2.3075210586670907

Epoch: 5| Step: 9
Training loss: 2.1201603412628174
Validation loss: 2.299965544413495

Epoch: 5| Step: 10
Training loss: 2.431835651397705
Validation loss: 2.291495887182092

Epoch: 215| Step: 0
Training loss: 2.5296473503112793
Validation loss: 2.2862730897882932

Epoch: 5| Step: 1
Training loss: 2.618316173553467
Validation loss: 2.282512061057552

Epoch: 5| Step: 2
Training loss: 1.8624136447906494
Validation loss: 2.2866073346907094

Epoch: 5| Step: 3
Training loss: 2.6460766792297363
Validation loss: 2.2783384630756993

Epoch: 5| Step: 4
Training loss: 2.367978572845459
Validation loss: 2.2768063519590642

Epoch: 5| Step: 5
Training loss: 2.0813300609588623
Validation loss: 2.280576426495788

Epoch: 5| Step: 6
Training loss: 2.1445953845977783
Validation loss: 2.2898941245130313

Epoch: 5| Step: 7
Training loss: 2.395318031311035
Validation loss: 2.287951497621434

Epoch: 5| Step: 8
Training loss: 3.0448899269104004
Validation loss: 2.2934571671229538

Epoch: 5| Step: 9
Training loss: 2.0670948028564453
Validation loss: 2.2787028281919417

Epoch: 5| Step: 10
Training loss: 2.6628546714782715
Validation loss: 2.285307932925481

Epoch: 216| Step: 0
Training loss: 2.6265358924865723
Validation loss: 2.2784939119892735

Epoch: 5| Step: 1
Training loss: 2.0013134479522705
Validation loss: 2.2748026565838884

Epoch: 5| Step: 2
Training loss: 2.074183702468872
Validation loss: 2.2834386646106677

Epoch: 5| Step: 3
Training loss: 2.0462048053741455
Validation loss: 2.2767806899163032

Epoch: 5| Step: 4
Training loss: 2.26792049407959
Validation loss: 2.2797555974734727

Epoch: 5| Step: 5
Training loss: 2.798941135406494
Validation loss: 2.2982699153243855

Epoch: 5| Step: 6
Training loss: 2.1744868755340576
Validation loss: 2.2978469223104496

Epoch: 5| Step: 7
Training loss: 2.5324203968048096
Validation loss: 2.294293080606768

Epoch: 5| Step: 8
Training loss: 2.7183303833007812
Validation loss: 2.304954316026421

Epoch: 5| Step: 9
Training loss: 2.5757861137390137
Validation loss: 2.303307743482692

Epoch: 5| Step: 10
Training loss: 2.5182666778564453
Validation loss: 2.2929738593357865

Epoch: 217| Step: 0
Training loss: 2.6440682411193848
Validation loss: 2.2932209942930486

Epoch: 5| Step: 1
Training loss: 1.8648998737335205
Validation loss: 2.283838259276523

Epoch: 5| Step: 2
Training loss: 1.8733680248260498
Validation loss: 2.2688629704137004

Epoch: 5| Step: 3
Training loss: 1.732182502746582
Validation loss: 2.2737218180010395

Epoch: 5| Step: 4
Training loss: 3.1507933139801025
Validation loss: 2.289163330549835

Epoch: 5| Step: 5
Training loss: 2.4129910469055176
Validation loss: 2.289108354558227

Epoch: 5| Step: 6
Training loss: 2.578399181365967
Validation loss: 2.2858183717214935

Epoch: 5| Step: 7
Training loss: 1.747690200805664
Validation loss: 2.303442173106696

Epoch: 5| Step: 8
Training loss: 3.380969524383545
Validation loss: 2.3161387058996383

Epoch: 5| Step: 9
Training loss: 2.9455201625823975
Validation loss: 2.33261751872237

Epoch: 5| Step: 10
Training loss: 1.7960772514343262
Validation loss: 2.328965679291756

Epoch: 218| Step: 0
Training loss: 2.357389211654663
Validation loss: 2.3234594714257026

Epoch: 5| Step: 1
Training loss: 1.753553032875061
Validation loss: 2.341215149048836

Epoch: 5| Step: 2
Training loss: 2.825976848602295
Validation loss: 2.3514886671496975

Epoch: 5| Step: 3
Training loss: 3.085752010345459
Validation loss: 2.339489136972735

Epoch: 5| Step: 4
Training loss: 2.093320846557617
Validation loss: 2.3129609964227162

Epoch: 5| Step: 5
Training loss: 2.3563714027404785
Validation loss: 2.3035059000856135

Epoch: 5| Step: 6
Training loss: 2.783982753753662
Validation loss: 2.305194167680638

Epoch: 5| Step: 7
Training loss: 2.808013439178467
Validation loss: 2.2961482232616794

Epoch: 5| Step: 8
Training loss: 2.1898598670959473
Validation loss: 2.2907271359556463

Epoch: 5| Step: 9
Training loss: 1.9314308166503906
Validation loss: 2.2910462733237975

Epoch: 5| Step: 10
Training loss: 2.0949461460113525
Validation loss: 2.2969159797955583

Epoch: 219| Step: 0
Training loss: 3.1197752952575684
Validation loss: 2.2941790755077074

Epoch: 5| Step: 1
Training loss: 2.353055238723755
Validation loss: 2.290482654366442

Epoch: 5| Step: 2
Training loss: 2.902428150177002
Validation loss: 2.2963292034723426

Epoch: 5| Step: 3
Training loss: 2.4666194915771484
Validation loss: 2.304925585305819

Epoch: 5| Step: 4
Training loss: 2.203092575073242
Validation loss: 2.3067725127743137

Epoch: 5| Step: 5
Training loss: 2.49481463432312
Validation loss: 2.3125767900097753

Epoch: 5| Step: 6
Training loss: 2.0610191822052
Validation loss: 2.302448667505736

Epoch: 5| Step: 7
Training loss: 2.1035561561584473
Validation loss: 2.318429171398122

Epoch: 5| Step: 8
Training loss: 2.4958481788635254
Validation loss: 2.3207382027820875

Epoch: 5| Step: 9
Training loss: 1.8773181438446045
Validation loss: 2.329051113897754

Epoch: 5| Step: 10
Training loss: 2.1383135318756104
Validation loss: 2.3231566388119935

Epoch: 220| Step: 0
Training loss: 2.54229474067688
Validation loss: 2.317289621599259

Epoch: 5| Step: 1
Training loss: 3.209089994430542
Validation loss: 2.304950248810553

Epoch: 5| Step: 2
Training loss: 2.5279603004455566
Validation loss: 2.292889784741145

Epoch: 5| Step: 3
Training loss: 2.1744115352630615
Validation loss: 2.2876708174264557

Epoch: 5| Step: 4
Training loss: 1.8869632482528687
Validation loss: 2.2860050496234687

Epoch: 5| Step: 5
Training loss: 2.295928955078125
Validation loss: 2.2897996646101757

Epoch: 5| Step: 6
Training loss: 2.130075216293335
Validation loss: 2.297559827886602

Epoch: 5| Step: 7
Training loss: 2.8924155235290527
Validation loss: 2.3051444868887625

Epoch: 5| Step: 8
Training loss: 2.358262300491333
Validation loss: 2.2882683584767003

Epoch: 5| Step: 9
Training loss: 1.8349506855010986
Validation loss: 2.2686798380267237

Epoch: 5| Step: 10
Training loss: 2.5603551864624023
Validation loss: 2.26902606666729

Epoch: 221| Step: 0
Training loss: 2.1521997451782227
Validation loss: 2.2726807235389628

Epoch: 5| Step: 1
Training loss: 2.4764628410339355
Validation loss: 2.2762051346481487

Epoch: 5| Step: 2
Training loss: 2.238524913787842
Validation loss: 2.2745991291538363

Epoch: 5| Step: 3
Training loss: 2.6546006202697754
Validation loss: 2.285985900509742

Epoch: 5| Step: 4
Training loss: 2.372464179992676
Validation loss: 2.2961176697925856

Epoch: 5| Step: 5
Training loss: 2.1438136100769043
Validation loss: 2.3124299305741505

Epoch: 5| Step: 6
Training loss: 2.42077898979187
Validation loss: 2.3271573179511615

Epoch: 5| Step: 7
Training loss: 2.5271549224853516
Validation loss: 2.338967820649506

Epoch: 5| Step: 8
Training loss: 2.659454822540283
Validation loss: 2.328243834998018

Epoch: 5| Step: 9
Training loss: 2.741220474243164
Validation loss: 2.3206815565786054

Epoch: 5| Step: 10
Training loss: 1.797616720199585
Validation loss: 2.3028378614815335

Epoch: 222| Step: 0
Training loss: 2.1030287742614746
Validation loss: 2.2964810735435894

Epoch: 5| Step: 1
Training loss: 1.7636833190917969
Validation loss: 2.286890745162964

Epoch: 5| Step: 2
Training loss: 2.085249662399292
Validation loss: 2.2910503469487673

Epoch: 5| Step: 3
Training loss: 2.505049467086792
Validation loss: 2.2759965530005832

Epoch: 5| Step: 4
Training loss: 2.639270544052124
Validation loss: 2.2759705820391254

Epoch: 5| Step: 5
Training loss: 1.969434380531311
Validation loss: 2.2786565083329395

Epoch: 5| Step: 6
Training loss: 2.7846662998199463
Validation loss: 2.27967639892332

Epoch: 5| Step: 7
Training loss: 2.8104684352874756
Validation loss: 2.275117499853975

Epoch: 5| Step: 8
Training loss: 1.9000227451324463
Validation loss: 2.2767350340402253

Epoch: 5| Step: 9
Training loss: 2.989194393157959
Validation loss: 2.28183981808283

Epoch: 5| Step: 10
Training loss: 2.4172794818878174
Validation loss: 2.278363420117286

Epoch: 223| Step: 0
Training loss: 2.3749289512634277
Validation loss: 2.269939561044016

Epoch: 5| Step: 1
Training loss: 2.5877599716186523
Validation loss: 2.2755186275769304

Epoch: 5| Step: 2
Training loss: 2.4607648849487305
Validation loss: 2.272182713272751

Epoch: 5| Step: 3
Training loss: 2.5526607036590576
Validation loss: 2.2623572772549045

Epoch: 5| Step: 4
Training loss: 2.1734585762023926
Validation loss: 2.2903084421670563

Epoch: 5| Step: 5
Training loss: 2.3606090545654297
Validation loss: 2.271601043721681

Epoch: 5| Step: 6
Training loss: 1.6327072381973267
Validation loss: 2.278845925484934

Epoch: 5| Step: 7
Training loss: 2.133199691772461
Validation loss: 2.2652297814687095

Epoch: 5| Step: 8
Training loss: 2.3810393810272217
Validation loss: 2.2680405429614487

Epoch: 5| Step: 9
Training loss: 2.180696964263916
Validation loss: 2.276310042668414

Epoch: 5| Step: 10
Training loss: 3.3795039653778076
Validation loss: 2.294699686829762

Epoch: 224| Step: 0
Training loss: 1.982351303100586
Validation loss: 2.323067965046052

Epoch: 5| Step: 1
Training loss: 2.4966635704040527
Validation loss: 2.3584196259898524

Epoch: 5| Step: 2
Training loss: 2.4505202770233154
Validation loss: 2.361947951778289

Epoch: 5| Step: 3
Training loss: 2.581066846847534
Validation loss: 2.345930378924134

Epoch: 5| Step: 4
Training loss: 2.7405903339385986
Validation loss: 2.3395830239019086

Epoch: 5| Step: 5
Training loss: 1.9951318502426147
Validation loss: 2.31504124979819

Epoch: 5| Step: 6
Training loss: 2.4775402545928955
Validation loss: 2.2943394363567395

Epoch: 5| Step: 7
Training loss: 2.2446510791778564
Validation loss: 2.2857804811129006

Epoch: 5| Step: 8
Training loss: 2.036231517791748
Validation loss: 2.284829778055991

Epoch: 5| Step: 9
Training loss: 2.516942262649536
Validation loss: 2.3001745106071554

Epoch: 5| Step: 10
Training loss: 2.805434226989746
Validation loss: 2.3002441339595343

Epoch: 225| Step: 0
Training loss: 1.830439805984497
Validation loss: 2.2843674126491753

Epoch: 5| Step: 1
Training loss: 2.42541766166687
Validation loss: 2.2710174206764466

Epoch: 5| Step: 2
Training loss: 2.658775568008423
Validation loss: 2.2679963214423067

Epoch: 5| Step: 3
Training loss: 2.4155399799346924
Validation loss: 2.262273156514732

Epoch: 5| Step: 4
Training loss: 1.8945105075836182
Validation loss: 2.2653901884632726

Epoch: 5| Step: 5
Training loss: 2.631958484649658
Validation loss: 2.267766765368882

Epoch: 5| Step: 6
Training loss: 2.420504093170166
Validation loss: 2.2695690483175297

Epoch: 5| Step: 7
Training loss: 2.3266735076904297
Validation loss: 2.277767586451705

Epoch: 5| Step: 8
Training loss: 2.712613582611084
Validation loss: 2.2920092767284763

Epoch: 5| Step: 9
Training loss: 2.4320521354675293
Validation loss: 2.311905849364496

Epoch: 5| Step: 10
Training loss: 2.3598597049713135
Validation loss: 2.311950651548242

Epoch: 226| Step: 0
Training loss: 2.0461087226867676
Validation loss: 2.3641677415499123

Epoch: 5| Step: 1
Training loss: 2.7742481231689453
Validation loss: 2.380855042447326

Epoch: 5| Step: 2
Training loss: 3.0712382793426514
Validation loss: 2.421302059645294

Epoch: 5| Step: 3
Training loss: 1.829594612121582
Validation loss: 2.4228542543226674

Epoch: 5| Step: 4
Training loss: 1.9930801391601562
Validation loss: 2.4101954711380826

Epoch: 5| Step: 5
Training loss: 2.099472999572754
Validation loss: 2.396961204467281

Epoch: 5| Step: 6
Training loss: 2.204367160797119
Validation loss: 2.3828040425495436

Epoch: 5| Step: 7
Training loss: 2.499668836593628
Validation loss: 2.355390538451492

Epoch: 5| Step: 8
Training loss: 2.230898380279541
Validation loss: 2.34133582217719

Epoch: 5| Step: 9
Training loss: 2.755697727203369
Validation loss: 2.3383881225380847

Epoch: 5| Step: 10
Training loss: 2.7961087226867676
Validation loss: 2.340158554815477

Epoch: 227| Step: 0
Training loss: 2.8951175212860107
Validation loss: 2.3051130130726802

Epoch: 5| Step: 1
Training loss: 2.0258705615997314
Validation loss: 2.2892296416785127

Epoch: 5| Step: 2
Training loss: 2.3452236652374268
Validation loss: 2.279198743963754

Epoch: 5| Step: 3
Training loss: 2.3093042373657227
Validation loss: 2.2803694573781823

Epoch: 5| Step: 4
Training loss: 2.292614459991455
Validation loss: 2.280197940846925

Epoch: 5| Step: 5
Training loss: 2.036902904510498
Validation loss: 2.2803609896731634

Epoch: 5| Step: 6
Training loss: 2.8645541667938232
Validation loss: 2.2793993834526307

Epoch: 5| Step: 7
Training loss: 1.8895556926727295
Validation loss: 2.2899124032707623

Epoch: 5| Step: 8
Training loss: 2.271799325942993
Validation loss: 2.2968241219879477

Epoch: 5| Step: 9
Training loss: 2.2736611366271973
Validation loss: 2.2749231605119604

Epoch: 5| Step: 10
Training loss: 2.726804256439209
Validation loss: 2.2883691057082145

Epoch: 228| Step: 0
Training loss: 2.75382399559021
Validation loss: 2.27431155789283

Epoch: 5| Step: 1
Training loss: 2.4276552200317383
Validation loss: 2.279513600052044

Epoch: 5| Step: 2
Training loss: 1.5154409408569336
Validation loss: 2.2764990304106023

Epoch: 5| Step: 3
Training loss: 2.4469521045684814
Validation loss: 2.285524965614401

Epoch: 5| Step: 4
Training loss: 2.1114213466644287
Validation loss: 2.2838522977726434

Epoch: 5| Step: 5
Training loss: 1.9494050741195679
Validation loss: 2.2750043894654963

Epoch: 5| Step: 6
Training loss: 2.7749557495117188
Validation loss: 2.2890814837589057

Epoch: 5| Step: 7
Training loss: 2.2754082679748535
Validation loss: 2.294624423467985

Epoch: 5| Step: 8
Training loss: 3.0206003189086914
Validation loss: 2.2848991527352283

Epoch: 5| Step: 9
Training loss: 2.071807861328125
Validation loss: 2.285954095984018

Epoch: 5| Step: 10
Training loss: 2.462059497833252
Validation loss: 2.282997997858191

Epoch: 229| Step: 0
Training loss: 2.4704337120056152
Validation loss: 2.294471994523079

Epoch: 5| Step: 1
Training loss: 2.4913387298583984
Validation loss: 2.2830310867678736

Epoch: 5| Step: 2
Training loss: 2.2265915870666504
Validation loss: 2.285935289116316

Epoch: 5| Step: 3
Training loss: 2.2144575119018555
Validation loss: 2.29398597696776

Epoch: 5| Step: 4
Training loss: 2.426056385040283
Validation loss: 2.2791116494004444

Epoch: 5| Step: 5
Training loss: 2.555725574493408
Validation loss: 2.26457836679233

Epoch: 5| Step: 6
Training loss: 2.2966866493225098
Validation loss: 2.280594177143548

Epoch: 5| Step: 7
Training loss: 2.6650962829589844
Validation loss: 2.2676273751002487

Epoch: 5| Step: 8
Training loss: 2.1121630668640137
Validation loss: 2.2581872863154255

Epoch: 5| Step: 9
Training loss: 2.0667831897735596
Validation loss: 2.2522854881901897

Epoch: 5| Step: 10
Training loss: 2.2055165767669678
Validation loss: 2.2644448100879626

Epoch: 230| Step: 0
Training loss: 2.98997163772583
Validation loss: 2.2660913467407227

Epoch: 5| Step: 1
Training loss: 1.577699899673462
Validation loss: 2.28504345750296

Epoch: 5| Step: 2
Training loss: 2.269906997680664
Validation loss: 2.2875492342056765

Epoch: 5| Step: 3
Training loss: 2.3081345558166504
Validation loss: 2.287202832519367

Epoch: 5| Step: 4
Training loss: 2.7384068965911865
Validation loss: 2.2707544795928465

Epoch: 5| Step: 5
Training loss: 2.3853652477264404
Validation loss: 2.2672051281057377

Epoch: 5| Step: 6
Training loss: 2.8071508407592773
Validation loss: 2.2598885208047848

Epoch: 5| Step: 7
Training loss: 2.277496814727783
Validation loss: 2.2514885522985972

Epoch: 5| Step: 8
Training loss: 2.0572686195373535
Validation loss: 2.249854544157623

Epoch: 5| Step: 9
Training loss: 2.096294641494751
Validation loss: 2.25282347074119

Epoch: 5| Step: 10
Training loss: 2.0294008255004883
Validation loss: 2.254171020241194

Epoch: 231| Step: 0
Training loss: 2.1842665672302246
Validation loss: 2.254174296573926

Epoch: 5| Step: 1
Training loss: 2.0878634452819824
Validation loss: 2.244863780595923

Epoch: 5| Step: 2
Training loss: 2.6219089031219482
Validation loss: 2.2554271605706986

Epoch: 5| Step: 3
Training loss: 2.204237461090088
Validation loss: 2.260832095658907

Epoch: 5| Step: 4
Training loss: 2.7769041061401367
Validation loss: 2.2727716789450696

Epoch: 5| Step: 5
Training loss: 2.056640625
Validation loss: 2.2697071208748767

Epoch: 5| Step: 6
Training loss: 2.382643222808838
Validation loss: 2.2629398427983767

Epoch: 5| Step: 7
Training loss: 2.366741418838501
Validation loss: 2.2533805857422533

Epoch: 5| Step: 8
Training loss: 2.4183688163757324
Validation loss: 2.2409089252512944

Epoch: 5| Step: 9
Training loss: 1.979702353477478
Validation loss: 2.2365361670012116

Epoch: 5| Step: 10
Training loss: 2.5360493659973145
Validation loss: 2.2369150423234507

Epoch: 232| Step: 0
Training loss: 2.556809902191162
Validation loss: 2.245114199576839

Epoch: 5| Step: 1
Training loss: 2.432340383529663
Validation loss: 2.2559601465861

Epoch: 5| Step: 2
Training loss: 2.805284023284912
Validation loss: 2.253317258691275

Epoch: 5| Step: 3
Training loss: 2.0338563919067383
Validation loss: 2.267814051720404

Epoch: 5| Step: 4
Training loss: 1.7479597330093384
Validation loss: 2.260469328972601

Epoch: 5| Step: 5
Training loss: 2.498176336288452
Validation loss: 2.2559603901319605

Epoch: 5| Step: 6
Training loss: 1.9828414916992188
Validation loss: 2.264001400240006

Epoch: 5| Step: 7
Training loss: 2.764948606491089
Validation loss: 2.2761715586467455

Epoch: 5| Step: 8
Training loss: 2.357407331466675
Validation loss: 2.2597085609230945

Epoch: 5| Step: 9
Training loss: 2.2278709411621094
Validation loss: 2.2705569600546234

Epoch: 5| Step: 10
Training loss: 2.324025869369507
Validation loss: 2.247633675093292

Epoch: 233| Step: 0
Training loss: 2.7167258262634277
Validation loss: 2.2449527094441075

Epoch: 5| Step: 1
Training loss: 2.2268259525299072
Validation loss: 2.233297009621897

Epoch: 5| Step: 2
Training loss: 2.4438469409942627
Validation loss: 2.238607568125571

Epoch: 5| Step: 3
Training loss: 1.8522170782089233
Validation loss: 2.225925058446905

Epoch: 5| Step: 4
Training loss: 2.4925646781921387
Validation loss: 2.236749666993336

Epoch: 5| Step: 5
Training loss: 2.4043407440185547
Validation loss: 2.2639776891277683

Epoch: 5| Step: 6
Training loss: 2.6400952339172363
Validation loss: 2.2685008433557328

Epoch: 5| Step: 7
Training loss: 2.493532419204712
Validation loss: 2.2886512382056123

Epoch: 5| Step: 8
Training loss: 1.9790124893188477
Validation loss: 2.3025251947423464

Epoch: 5| Step: 9
Training loss: 2.2519679069519043
Validation loss: 2.285307556070307

Epoch: 5| Step: 10
Training loss: 2.2251758575439453
Validation loss: 2.279846954089339

Epoch: 234| Step: 0
Training loss: 1.7135207653045654
Validation loss: 2.2663712809162755

Epoch: 5| Step: 1
Training loss: 2.635101318359375
Validation loss: 2.255170235069849

Epoch: 5| Step: 2
Training loss: 2.1757054328918457
Validation loss: 2.259891830464845

Epoch: 5| Step: 3
Training loss: 2.6092724800109863
Validation loss: 2.238937767603064

Epoch: 5| Step: 4
Training loss: 1.9382143020629883
Validation loss: 2.2324034219147055

Epoch: 5| Step: 5
Training loss: 2.6694588661193848
Validation loss: 2.242105386590445

Epoch: 5| Step: 6
Training loss: 2.435776710510254
Validation loss: 2.223441013725855

Epoch: 5| Step: 7
Training loss: 1.7679359912872314
Validation loss: 2.2236739089412074

Epoch: 5| Step: 8
Training loss: 2.2168052196502686
Validation loss: 2.2196529706319175

Epoch: 5| Step: 9
Training loss: 3.030118465423584
Validation loss: 2.2111314624868412

Epoch: 5| Step: 10
Training loss: 2.4796390533447266
Validation loss: 2.228354489931496

Epoch: 235| Step: 0
Training loss: 2.9032716751098633
Validation loss: 2.235096726366269

Epoch: 5| Step: 1
Training loss: 2.4052975177764893
Validation loss: 2.2511689791115383

Epoch: 5| Step: 2
Training loss: 2.240893840789795
Validation loss: 2.262025845948086

Epoch: 5| Step: 3
Training loss: 1.6862999200820923
Validation loss: 2.2719314354722218

Epoch: 5| Step: 4
Training loss: 2.378700017929077
Validation loss: 2.2719022125326176

Epoch: 5| Step: 5
Training loss: 2.7266316413879395
Validation loss: 2.2592131245520806

Epoch: 5| Step: 6
Training loss: 2.3030498027801514
Validation loss: 2.240576567188386

Epoch: 5| Step: 7
Training loss: 2.1707682609558105
Validation loss: 2.250977441828738

Epoch: 5| Step: 8
Training loss: 2.009176731109619
Validation loss: 2.26300230333882

Epoch: 5| Step: 9
Training loss: 2.22700834274292
Validation loss: 2.259831455446059

Epoch: 5| Step: 10
Training loss: 2.561643600463867
Validation loss: 2.2653180681249148

Epoch: 236| Step: 0
Training loss: 3.0472874641418457
Validation loss: 2.244717523615847

Epoch: 5| Step: 1
Training loss: 2.0113959312438965
Validation loss: 2.2409045311712448

Epoch: 5| Step: 2
Training loss: 2.255156993865967
Validation loss: 2.242910082622241

Epoch: 5| Step: 3
Training loss: 1.740393042564392
Validation loss: 2.223863240211241

Epoch: 5| Step: 4
Training loss: 2.0098767280578613
Validation loss: 2.2308894908556374

Epoch: 5| Step: 5
Training loss: 2.5258476734161377
Validation loss: 2.2288000865649154

Epoch: 5| Step: 6
Training loss: 2.061666250228882
Validation loss: 2.243247744857624

Epoch: 5| Step: 7
Training loss: 2.1291615962982178
Validation loss: 2.233605541208739

Epoch: 5| Step: 8
Training loss: 2.527780055999756
Validation loss: 2.2309184433311544

Epoch: 5| Step: 9
Training loss: 2.246074914932251
Validation loss: 2.2335907592568347

Epoch: 5| Step: 10
Training loss: 2.8777897357940674
Validation loss: 2.245946514991022

Epoch: 237| Step: 0
Training loss: 2.4682366847991943
Validation loss: 2.245467338510739

Epoch: 5| Step: 1
Training loss: 2.211635112762451
Validation loss: 2.2605598306143158

Epoch: 5| Step: 2
Training loss: 1.6724998950958252
Validation loss: 2.272255471957627

Epoch: 5| Step: 3
Training loss: 2.8513550758361816
Validation loss: 2.2765780828332387

Epoch: 5| Step: 4
Training loss: 2.41599178314209
Validation loss: 2.2740042914626417

Epoch: 5| Step: 5
Training loss: 2.7144627571105957
Validation loss: 2.271945704695999

Epoch: 5| Step: 6
Training loss: 2.4332025051116943
Validation loss: 2.2863873025422454

Epoch: 5| Step: 7
Training loss: 1.9321168661117554
Validation loss: 2.273453535572175

Epoch: 5| Step: 8
Training loss: 2.1604905128479004
Validation loss: 2.265325528319164

Epoch: 5| Step: 9
Training loss: 2.4116714000701904
Validation loss: 2.251299987557114

Epoch: 5| Step: 10
Training loss: 1.9936721324920654
Validation loss: 2.2521185349392634

Epoch: 238| Step: 0
Training loss: 2.092350482940674
Validation loss: 2.241555413892192

Epoch: 5| Step: 1
Training loss: 1.8875255584716797
Validation loss: 2.247973048558799

Epoch: 5| Step: 2
Training loss: 2.9964139461517334
Validation loss: 2.269105467745053

Epoch: 5| Step: 3
Training loss: 2.5323641300201416
Validation loss: 2.262973854618688

Epoch: 5| Step: 4
Training loss: 2.237287998199463
Validation loss: 2.266487051081914

Epoch: 5| Step: 5
Training loss: 2.000086545944214
Validation loss: 2.254239051572738

Epoch: 5| Step: 6
Training loss: 2.310070753097534
Validation loss: 2.243005152671568

Epoch: 5| Step: 7
Training loss: 2.3680057525634766
Validation loss: 2.2556939535243536

Epoch: 5| Step: 8
Training loss: 3.0440430641174316
Validation loss: 2.2555983015286025

Epoch: 5| Step: 9
Training loss: 1.70309317111969
Validation loss: 2.278216346617668

Epoch: 5| Step: 10
Training loss: 2.250952959060669
Validation loss: 2.2927811761056223

Epoch: 239| Step: 0
Training loss: 2.000279426574707
Validation loss: 2.314323938021096

Epoch: 5| Step: 1
Training loss: 2.2388126850128174
Validation loss: 2.3062849134527226

Epoch: 5| Step: 2
Training loss: 2.011733055114746
Validation loss: 2.2919125454400175

Epoch: 5| Step: 3
Training loss: 2.3419394493103027
Validation loss: 2.277198096757294

Epoch: 5| Step: 4
Training loss: 1.9147803783416748
Validation loss: 2.281809663259855

Epoch: 5| Step: 5
Training loss: 2.8716752529144287
Validation loss: 2.2925208153263217

Epoch: 5| Step: 6
Training loss: 2.9438984394073486
Validation loss: 2.291540244574188

Epoch: 5| Step: 7
Training loss: 2.2012925148010254
Validation loss: 2.291444093950333

Epoch: 5| Step: 8
Training loss: 2.354984998703003
Validation loss: 2.2624475571417038

Epoch: 5| Step: 9
Training loss: 2.3996100425720215
Validation loss: 2.2431415357897357

Epoch: 5| Step: 10
Training loss: 2.2601234912872314
Validation loss: 2.228627210022301

Epoch: 240| Step: 0
Training loss: 2.0748796463012695
Validation loss: 2.217174135228639

Epoch: 5| Step: 1
Training loss: 2.130497455596924
Validation loss: 2.2091926297833844

Epoch: 5| Step: 2
Training loss: 2.248504638671875
Validation loss: 2.2082330796026413

Epoch: 5| Step: 3
Training loss: 2.4789466857910156
Validation loss: 2.2050648235505625

Epoch: 5| Step: 4
Training loss: 3.017364978790283
Validation loss: 2.221376483158399

Epoch: 5| Step: 5
Training loss: 2.236236572265625
Validation loss: 2.2347713721695768

Epoch: 5| Step: 6
Training loss: 2.1990363597869873
Validation loss: 2.2314489503060617

Epoch: 5| Step: 7
Training loss: 2.5830044746398926
Validation loss: 2.2433630484406666

Epoch: 5| Step: 8
Training loss: 1.9955822229385376
Validation loss: 2.2494047777627104

Epoch: 5| Step: 9
Training loss: 2.0527071952819824
Validation loss: 2.2490096194769746

Epoch: 5| Step: 10
Training loss: 2.301244020462036
Validation loss: 2.2441853695018317

Epoch: 241| Step: 0
Training loss: 2.9128966331481934
Validation loss: 2.2515914299154796

Epoch: 5| Step: 1
Training loss: 1.6176811456680298
Validation loss: 2.242482090509066

Epoch: 5| Step: 2
Training loss: 1.6817020177841187
Validation loss: 2.2359470680195797

Epoch: 5| Step: 3
Training loss: 2.198421001434326
Validation loss: 2.240999328192844

Epoch: 5| Step: 4
Training loss: 2.076110363006592
Validation loss: 2.242061553462859

Epoch: 5| Step: 5
Training loss: 2.372997283935547
Validation loss: 2.2304593414388676

Epoch: 5| Step: 6
Training loss: 2.4430489540100098
Validation loss: 2.208146326003536

Epoch: 5| Step: 7
Training loss: 2.6798431873321533
Validation loss: 2.2304644635928574

Epoch: 5| Step: 8
Training loss: 2.358386516571045
Validation loss: 2.2235150029582362

Epoch: 5| Step: 9
Training loss: 2.2542428970336914
Validation loss: 2.236427330201672

Epoch: 5| Step: 10
Training loss: 2.5471324920654297
Validation loss: 2.219767560241043

Epoch: 242| Step: 0
Training loss: 1.773553490638733
Validation loss: 2.2277964853471324

Epoch: 5| Step: 1
Training loss: 1.9306080341339111
Validation loss: 2.221302273452923

Epoch: 5| Step: 2
Training loss: 1.7992961406707764
Validation loss: 2.2149975069107546

Epoch: 5| Step: 3
Training loss: 2.2047009468078613
Validation loss: 2.241088487768686

Epoch: 5| Step: 4
Training loss: 2.7471213340759277
Validation loss: 2.235787025061987

Epoch: 5| Step: 5
Training loss: 1.857672095298767
Validation loss: 2.2358503059674333

Epoch: 5| Step: 6
Training loss: 2.224518299102783
Validation loss: 2.257786420083815

Epoch: 5| Step: 7
Training loss: 2.5481343269348145
Validation loss: 2.26308262219993

Epoch: 5| Step: 8
Training loss: 2.958366632461548
Validation loss: 2.240038446200791

Epoch: 5| Step: 9
Training loss: 2.743776321411133
Validation loss: 2.2415043551434755

Epoch: 5| Step: 10
Training loss: 2.4093029499053955
Validation loss: 2.236277070096744

Epoch: 243| Step: 0
Training loss: 2.663827419281006
Validation loss: 2.2319291535244195

Epoch: 5| Step: 1
Training loss: 2.1558468341827393
Validation loss: 2.2318217882546048

Epoch: 5| Step: 2
Training loss: 2.6832361221313477
Validation loss: 2.214810140671269

Epoch: 5| Step: 3
Training loss: 1.6076583862304688
Validation loss: 2.2234233694691814

Epoch: 5| Step: 4
Training loss: 2.6077332496643066
Validation loss: 2.2299601313888386

Epoch: 5| Step: 5
Training loss: 2.135812282562256
Validation loss: 2.221415232586604

Epoch: 5| Step: 6
Training loss: 1.9676001071929932
Validation loss: 2.2303725570760746

Epoch: 5| Step: 7
Training loss: 1.849043846130371
Validation loss: 2.2423910235845916

Epoch: 5| Step: 8
Training loss: 2.1727283000946045
Validation loss: 2.258369053563764

Epoch: 5| Step: 9
Training loss: 2.727588176727295
Validation loss: 2.2462039506563576

Epoch: 5| Step: 10
Training loss: 2.4339780807495117
Validation loss: 2.250704280791744

Epoch: 244| Step: 0
Training loss: 2.4040145874023438
Validation loss: 2.250141662936057

Epoch: 5| Step: 1
Training loss: 2.4779152870178223
Validation loss: 2.241332059265465

Epoch: 5| Step: 2
Training loss: 2.530845880508423
Validation loss: 2.25330090010038

Epoch: 5| Step: 3
Training loss: 2.0403010845184326
Validation loss: 2.2474960011820637

Epoch: 5| Step: 4
Training loss: 2.5561037063598633
Validation loss: 2.2475484507058257

Epoch: 5| Step: 5
Training loss: 2.438234806060791
Validation loss: 2.24520650089428

Epoch: 5| Step: 6
Training loss: 2.024502992630005
Validation loss: 2.2417175154532156

Epoch: 5| Step: 7
Training loss: 1.8098644018173218
Validation loss: 2.2461972339178926

Epoch: 5| Step: 8
Training loss: 2.2661914825439453
Validation loss: 2.2433189089580248

Epoch: 5| Step: 9
Training loss: 2.2888662815093994
Validation loss: 2.2482452418214534

Epoch: 5| Step: 10
Training loss: 2.273515462875366
Validation loss: 2.255649076995029

Epoch: 245| Step: 0
Training loss: 2.3237462043762207
Validation loss: 2.244831436423845

Epoch: 5| Step: 1
Training loss: 2.427779197692871
Validation loss: 2.2331439602759575

Epoch: 5| Step: 2
Training loss: 2.3453235626220703
Validation loss: 2.226438540284352

Epoch: 5| Step: 3
Training loss: 2.195483446121216
Validation loss: 2.2253738577647875

Epoch: 5| Step: 4
Training loss: 2.6221327781677246
Validation loss: 2.2059085676746983

Epoch: 5| Step: 5
Training loss: 2.263848066329956
Validation loss: 2.226012813147678

Epoch: 5| Step: 6
Training loss: 2.183206081390381
Validation loss: 2.216851508745583

Epoch: 5| Step: 7
Training loss: 2.0763869285583496
Validation loss: 2.229463997707572

Epoch: 5| Step: 8
Training loss: 2.0556647777557373
Validation loss: 2.227361813668282

Epoch: 5| Step: 9
Training loss: 2.268256664276123
Validation loss: 2.241814726142473

Epoch: 5| Step: 10
Training loss: 2.212155818939209
Validation loss: 2.2506831717747513

Epoch: 246| Step: 0
Training loss: 2.3317646980285645
Validation loss: 2.2559891477707894

Epoch: 5| Step: 1
Training loss: 3.0848910808563232
Validation loss: 2.2569670536184825

Epoch: 5| Step: 2
Training loss: 2.1880619525909424
Validation loss: 2.2495604638130433

Epoch: 5| Step: 3
Training loss: 1.2975614070892334
Validation loss: 2.252561415395429

Epoch: 5| Step: 4
Training loss: 2.077664852142334
Validation loss: 2.240344227001231

Epoch: 5| Step: 5
Training loss: 2.283583402633667
Validation loss: 2.2296433294973066

Epoch: 5| Step: 6
Training loss: 2.002851963043213
Validation loss: 2.2257269864441245

Epoch: 5| Step: 7
Training loss: 2.8034188747406006
Validation loss: 2.2299029724572295

Epoch: 5| Step: 8
Training loss: 2.312126398086548
Validation loss: 2.230399097165754

Epoch: 5| Step: 9
Training loss: 2.4795150756835938
Validation loss: 2.239954433133525

Epoch: 5| Step: 10
Training loss: 1.8853472471237183
Validation loss: 2.2339020211209535

Epoch: 247| Step: 0
Training loss: 1.9209632873535156
Validation loss: 2.2405922412872314

Epoch: 5| Step: 1
Training loss: 2.4408483505249023
Validation loss: 2.243798766084897

Epoch: 5| Step: 2
Training loss: 2.6650948524475098
Validation loss: 2.2606095331971363

Epoch: 5| Step: 3
Training loss: 1.775761604309082
Validation loss: 2.271684879897743

Epoch: 5| Step: 4
Training loss: 2.5291409492492676
Validation loss: 2.2585862298165598

Epoch: 5| Step: 5
Training loss: 2.511875867843628
Validation loss: 2.2587075669278383

Epoch: 5| Step: 6
Training loss: 1.8353526592254639
Validation loss: 2.2560949248652302

Epoch: 5| Step: 7
Training loss: 2.487191915512085
Validation loss: 2.250343712427283

Epoch: 5| Step: 8
Training loss: 2.5219569206237793
Validation loss: 2.2597305518324657

Epoch: 5| Step: 9
Training loss: 1.8055576086044312
Validation loss: 2.257650354857086

Epoch: 5| Step: 10
Training loss: 2.338747024536133
Validation loss: 2.2557490461616108

Epoch: 248| Step: 0
Training loss: 2.3660030364990234
Validation loss: 2.243550797944428

Epoch: 5| Step: 1
Training loss: 2.8649911880493164
Validation loss: 2.256954280279016

Epoch: 5| Step: 2
Training loss: 1.8329565525054932
Validation loss: 2.2379254500071206

Epoch: 5| Step: 3
Training loss: 2.537080764770508
Validation loss: 2.252824103960427

Epoch: 5| Step: 4
Training loss: 2.084789752960205
Validation loss: 2.2532321124948482

Epoch: 5| Step: 5
Training loss: 2.097815752029419
Validation loss: 2.251651951061782

Epoch: 5| Step: 6
Training loss: 1.6894495487213135
Validation loss: 2.2383028832815026

Epoch: 5| Step: 7
Training loss: 2.3755955696105957
Validation loss: 2.241243468817844

Epoch: 5| Step: 8
Training loss: 2.5920376777648926
Validation loss: 2.2293609098721574

Epoch: 5| Step: 9
Training loss: 2.4051575660705566
Validation loss: 2.240170585211887

Epoch: 5| Step: 10
Training loss: 1.8006559610366821
Validation loss: 2.2368127005074614

Epoch: 249| Step: 0
Training loss: 2.051860809326172
Validation loss: 2.2318692796973774

Epoch: 5| Step: 1
Training loss: 1.8505977392196655
Validation loss: 2.2359514620996292

Epoch: 5| Step: 2
Training loss: 2.3781683444976807
Validation loss: 2.2422295078154533

Epoch: 5| Step: 3
Training loss: 2.7365753650665283
Validation loss: 2.242069495621548

Epoch: 5| Step: 4
Training loss: 2.9757697582244873
Validation loss: 2.248536081724269

Epoch: 5| Step: 5
Training loss: 2.2367870807647705
Validation loss: 2.2581645365684264

Epoch: 5| Step: 6
Training loss: 1.992958664894104
Validation loss: 2.2560004226623045

Epoch: 5| Step: 7
Training loss: 2.4852662086486816
Validation loss: 2.2371624208265737

Epoch: 5| Step: 8
Training loss: 1.9460811614990234
Validation loss: 2.237454766868263

Epoch: 5| Step: 9
Training loss: 1.6903297901153564
Validation loss: 2.2348208811975296

Epoch: 5| Step: 10
Training loss: 2.3063228130340576
Validation loss: 2.2288876374562583

Epoch: 250| Step: 0
Training loss: 2.133800506591797
Validation loss: 2.2403379024997836

Epoch: 5| Step: 1
Training loss: 2.5753254890441895
Validation loss: 2.2359047858945784

Epoch: 5| Step: 2
Training loss: 2.202329397201538
Validation loss: 2.2628747211989535

Epoch: 5| Step: 3
Training loss: 2.068103075027466
Validation loss: 2.2802878707967777

Epoch: 5| Step: 4
Training loss: 2.7577359676361084
Validation loss: 2.271326605991651

Epoch: 5| Step: 5
Training loss: 2.2390284538269043
Validation loss: 2.267568393420148

Epoch: 5| Step: 6
Training loss: 2.326125383377075
Validation loss: 2.2736359078397035

Epoch: 5| Step: 7
Training loss: 2.035717010498047
Validation loss: 2.251683231322996

Epoch: 5| Step: 8
Training loss: 2.1192784309387207
Validation loss: 2.2491866926993094

Epoch: 5| Step: 9
Training loss: 2.002739429473877
Validation loss: 2.22725433944374

Epoch: 5| Step: 10
Training loss: 2.177379608154297
Validation loss: 2.216022647837157

Epoch: 251| Step: 0
Training loss: 2.7097671031951904
Validation loss: 2.2105021066563104

Epoch: 5| Step: 1
Training loss: 2.431461811065674
Validation loss: 2.20578775354611

Epoch: 5| Step: 2
Training loss: 2.872384548187256
Validation loss: 2.202738179955431

Epoch: 5| Step: 3
Training loss: 2.0112998485565186
Validation loss: 2.212235027743924

Epoch: 5| Step: 4
Training loss: 2.170001268386841
Validation loss: 2.2016263161936114

Epoch: 5| Step: 5
Training loss: 2.0097508430480957
Validation loss: 2.216231974222327

Epoch: 5| Step: 6
Training loss: 2.359266996383667
Validation loss: 2.2267148520356868

Epoch: 5| Step: 7
Training loss: 1.547222375869751
Validation loss: 2.2358762538561257

Epoch: 5| Step: 8
Training loss: 2.2878711223602295
Validation loss: 2.252690779265537

Epoch: 5| Step: 9
Training loss: 2.2198967933654785
Validation loss: 2.239437053280492

Epoch: 5| Step: 10
Training loss: 2.0211682319641113
Validation loss: 2.2442096125695015

Epoch: 252| Step: 0
Training loss: 3.021317958831787
Validation loss: 2.2405115481345885

Epoch: 5| Step: 1
Training loss: 2.342928409576416
Validation loss: 2.2310251497453257

Epoch: 5| Step: 2
Training loss: 2.449843168258667
Validation loss: 2.217404470648817

Epoch: 5| Step: 3
Training loss: 1.378048062324524
Validation loss: 2.2020953598842827

Epoch: 5| Step: 4
Training loss: 2.2289347648620605
Validation loss: 2.207400089950972

Epoch: 5| Step: 5
Training loss: 2.796515703201294
Validation loss: 2.20256055555036

Epoch: 5| Step: 6
Training loss: 2.1726043224334717
Validation loss: 2.2008357791490454

Epoch: 5| Step: 7
Training loss: 2.2506303787231445
Validation loss: 2.200332869765579

Epoch: 5| Step: 8
Training loss: 2.1143391132354736
Validation loss: 2.204892066217238

Epoch: 5| Step: 9
Training loss: 1.812586784362793
Validation loss: 2.2177385181509037

Epoch: 5| Step: 10
Training loss: 1.9785070419311523
Validation loss: 2.2302789021563787

Epoch: 253| Step: 0
Training loss: 2.5947954654693604
Validation loss: 2.247880192213161

Epoch: 5| Step: 1
Training loss: 1.8631480932235718
Validation loss: 2.2418439157547487

Epoch: 5| Step: 2
Training loss: 2.0173306465148926
Validation loss: 2.2427398979022937

Epoch: 5| Step: 3
Training loss: 2.12296724319458
Validation loss: 2.2389051939851496

Epoch: 5| Step: 4
Training loss: 1.359511375427246
Validation loss: 2.216094324665685

Epoch: 5| Step: 5
Training loss: 2.819577932357788
Validation loss: 2.2085452669410297

Epoch: 5| Step: 6
Training loss: 1.9624621868133545
Validation loss: 2.2010390732877996

Epoch: 5| Step: 7
Training loss: 2.6007256507873535
Validation loss: 2.2161639582726265

Epoch: 5| Step: 8
Training loss: 2.520383358001709
Validation loss: 2.2092933603512344

Epoch: 5| Step: 9
Training loss: 2.504152774810791
Validation loss: 2.2129361988395773

Epoch: 5| Step: 10
Training loss: 2.2103660106658936
Validation loss: 2.2195657658320602

Epoch: 254| Step: 0
Training loss: 1.8711929321289062
Validation loss: 2.224841093504301

Epoch: 5| Step: 1
Training loss: 1.7700388431549072
Validation loss: 2.2405486235054592

Epoch: 5| Step: 2
Training loss: 2.1084606647491455
Validation loss: 2.2427303560318483

Epoch: 5| Step: 3
Training loss: 2.3958160877227783
Validation loss: 2.23478941763601

Epoch: 5| Step: 4
Training loss: 2.2250990867614746
Validation loss: 2.2232707085147982

Epoch: 5| Step: 5
Training loss: 2.124532699584961
Validation loss: 2.2186713372507403

Epoch: 5| Step: 6
Training loss: 1.9588515758514404
Validation loss: 2.1968603980156685

Epoch: 5| Step: 7
Training loss: 2.5820579528808594
Validation loss: 2.2061482039831017

Epoch: 5| Step: 8
Training loss: 2.7079696655273438
Validation loss: 2.2136757989083566

Epoch: 5| Step: 9
Training loss: 1.9093444347381592
Validation loss: 2.205936389584695

Epoch: 5| Step: 10
Training loss: 2.8571364879608154
Validation loss: 2.2184858040143083

Epoch: 255| Step: 0
Training loss: 2.669891357421875
Validation loss: 2.2070048393741732

Epoch: 5| Step: 1
Training loss: 2.1433727741241455
Validation loss: 2.195907792737407

Epoch: 5| Step: 2
Training loss: 2.4147956371307373
Validation loss: 2.190182608942832

Epoch: 5| Step: 3
Training loss: 2.2198424339294434
Validation loss: 2.190707543844818

Epoch: 5| Step: 4
Training loss: 1.6942230463027954
Validation loss: 2.1786822811249764

Epoch: 5| Step: 5
Training loss: 2.1393673419952393
Validation loss: 2.193922645302229

Epoch: 5| Step: 6
Training loss: 2.7602362632751465
Validation loss: 2.190004147509093

Epoch: 5| Step: 7
Training loss: 1.6923723220825195
Validation loss: 2.1869015360391266

Epoch: 5| Step: 8
Training loss: 3.001807689666748
Validation loss: 2.1991401974872877

Epoch: 5| Step: 9
Training loss: 1.6971979141235352
Validation loss: 2.191238710957189

Epoch: 5| Step: 10
Training loss: 1.990875005722046
Validation loss: 2.203104685711604

Epoch: 256| Step: 0
Training loss: 2.3611562252044678
Validation loss: 2.198856812651439

Epoch: 5| Step: 1
Training loss: 2.487114906311035
Validation loss: 2.2111459214200258

Epoch: 5| Step: 2
Training loss: 2.598706007003784
Validation loss: 2.2091868769737983

Epoch: 5| Step: 3
Training loss: 2.2475409507751465
Validation loss: 2.2210858816741617

Epoch: 5| Step: 4
Training loss: 1.4746239185333252
Validation loss: 2.2112371947175715

Epoch: 5| Step: 5
Training loss: 2.304527759552002
Validation loss: 2.2198222888413297

Epoch: 5| Step: 6
Training loss: 2.2199437618255615
Validation loss: 2.2215101872721026

Epoch: 5| Step: 7
Training loss: 2.0409653186798096
Validation loss: 2.2281280871360534

Epoch: 5| Step: 8
Training loss: 2.215445041656494
Validation loss: 2.2178644518698416

Epoch: 5| Step: 9
Training loss: 2.1587417125701904
Validation loss: 2.1947883380356656

Epoch: 5| Step: 10
Training loss: 2.2663722038269043
Validation loss: 2.2054709029454056

Epoch: 257| Step: 0
Training loss: 2.730510711669922
Validation loss: 2.2034551405137583

Epoch: 5| Step: 1
Training loss: 2.256005048751831
Validation loss: 2.196926921926519

Epoch: 5| Step: 2
Training loss: 2.366074323654175
Validation loss: 2.1773863800110353

Epoch: 5| Step: 3
Training loss: 2.2187857627868652
Validation loss: 2.1764318584113993

Epoch: 5| Step: 4
Training loss: 2.1507468223571777
Validation loss: 2.1647865503065047

Epoch: 5| Step: 5
Training loss: 1.6434218883514404
Validation loss: 2.166145104233937

Epoch: 5| Step: 6
Training loss: 2.858968496322632
Validation loss: 2.179414180017287

Epoch: 5| Step: 7
Training loss: 1.6741631031036377
Validation loss: 2.170218624094481

Epoch: 5| Step: 8
Training loss: 2.0779943466186523
Validation loss: 2.1816134939911547

Epoch: 5| Step: 9
Training loss: 2.088867664337158
Validation loss: 2.181801806214035

Epoch: 5| Step: 10
Training loss: 2.4479176998138428
Validation loss: 2.2060235700299664

Epoch: 258| Step: 0
Training loss: 2.3511688709259033
Validation loss: 2.2187941817827124

Epoch: 5| Step: 1
Training loss: 1.656590461730957
Validation loss: 2.249150465893489

Epoch: 5| Step: 2
Training loss: 2.482025146484375
Validation loss: 2.239086986869894

Epoch: 5| Step: 3
Training loss: 2.342453718185425
Validation loss: 2.258792472142045

Epoch: 5| Step: 4
Training loss: 2.9009642601013184
Validation loss: 2.222403100741807

Epoch: 5| Step: 5
Training loss: 1.8527511358261108
Validation loss: 2.1980406161277526

Epoch: 5| Step: 6
Training loss: 1.8909991979599
Validation loss: 2.19202398484753

Epoch: 5| Step: 7
Training loss: 1.7627887725830078
Validation loss: 2.195737709281265

Epoch: 5| Step: 8
Training loss: 1.966742753982544
Validation loss: 2.192344001544419

Epoch: 5| Step: 9
Training loss: 2.614150285720825
Validation loss: 2.182863394419352

Epoch: 5| Step: 10
Training loss: 2.7770638465881348
Validation loss: 2.183745006079315

Epoch: 259| Step: 0
Training loss: 1.9662902355194092
Validation loss: 2.1892889097172725

Epoch: 5| Step: 1
Training loss: 2.5038249492645264
Validation loss: 2.1873931654037966

Epoch: 5| Step: 2
Training loss: 1.8406970500946045
Validation loss: 2.185461208384524

Epoch: 5| Step: 3
Training loss: 2.6592016220092773
Validation loss: 2.2096949341476604

Epoch: 5| Step: 4
Training loss: 2.2418007850646973
Validation loss: 2.2281435663982103

Epoch: 5| Step: 5
Training loss: 2.3815226554870605
Validation loss: 2.237912990713632

Epoch: 5| Step: 6
Training loss: 2.496121883392334
Validation loss: 2.2269896717481714

Epoch: 5| Step: 7
Training loss: 1.3565561771392822
Validation loss: 2.21602967990342

Epoch: 5| Step: 8
Training loss: 2.2701776027679443
Validation loss: 2.218607815363074

Epoch: 5| Step: 9
Training loss: 2.329206943511963
Validation loss: 2.2243722959231307

Epoch: 5| Step: 10
Training loss: 2.566354513168335
Validation loss: 2.241956413433116

Epoch: 260| Step: 0
Training loss: 2.3814494609832764
Validation loss: 2.240870254014128

Epoch: 5| Step: 1
Training loss: 2.405830144882202
Validation loss: 2.261230607186594

Epoch: 5| Step: 2
Training loss: 1.9852873086929321
Validation loss: 2.2743526248521704

Epoch: 5| Step: 3
Training loss: 2.856666088104248
Validation loss: 2.284464145219454

Epoch: 5| Step: 4
Training loss: 2.6468091011047363
Validation loss: 2.2586672049696728

Epoch: 5| Step: 5
Training loss: 1.9605289697647095
Validation loss: 2.2421943115931686

Epoch: 5| Step: 6
Training loss: 1.8399989604949951
Validation loss: 2.2402813050054733

Epoch: 5| Step: 7
Training loss: 1.637164831161499
Validation loss: 2.216202171899939

Epoch: 5| Step: 8
Training loss: 2.480731248855591
Validation loss: 2.213242143713018

Epoch: 5| Step: 9
Training loss: 2.1656479835510254
Validation loss: 2.2079583457721177

Epoch: 5| Step: 10
Training loss: 1.9944862127304077
Validation loss: 2.203775035437717

Epoch: 261| Step: 0
Training loss: 2.5231001377105713
Validation loss: 2.2100407449148034

Epoch: 5| Step: 1
Training loss: 2.4090423583984375
Validation loss: 2.2125388627411215

Epoch: 5| Step: 2
Training loss: 1.7968791723251343
Validation loss: 2.189769173181185

Epoch: 5| Step: 3
Training loss: 2.235607624053955
Validation loss: 2.218640286435363

Epoch: 5| Step: 4
Training loss: 2.4988584518432617
Validation loss: 2.2029009839539886

Epoch: 5| Step: 5
Training loss: 2.2551028728485107
Validation loss: 2.219458185216432

Epoch: 5| Step: 6
Training loss: 2.0427188873291016
Validation loss: 2.2147408582830943

Epoch: 5| Step: 7
Training loss: 2.3212623596191406
Validation loss: 2.223339444847517

Epoch: 5| Step: 8
Training loss: 2.066326856613159
Validation loss: 2.228858511934998

Epoch: 5| Step: 9
Training loss: 2.181042194366455
Validation loss: 2.2340959759168726

Epoch: 5| Step: 10
Training loss: 1.8382738828659058
Validation loss: 2.235126356924734

Epoch: 262| Step: 0
Training loss: 1.7745898962020874
Validation loss: 2.2380012876244

Epoch: 5| Step: 1
Training loss: 2.266634464263916
Validation loss: 2.2505451838175454

Epoch: 5| Step: 2
Training loss: 2.388728618621826
Validation loss: 2.2300804635529876

Epoch: 5| Step: 3
Training loss: 2.287839651107788
Validation loss: 2.2240548877305883

Epoch: 5| Step: 4
Training loss: 2.302783489227295
Validation loss: 2.2006469747071624

Epoch: 5| Step: 5
Training loss: 2.927032947540283
Validation loss: 2.199537051621304

Epoch: 5| Step: 6
Training loss: 2.3291614055633545
Validation loss: 2.2010502584518923

Epoch: 5| Step: 7
Training loss: 2.331428289413452
Validation loss: 2.182203087755429

Epoch: 5| Step: 8
Training loss: 2.059598207473755
Validation loss: 2.184744927190965

Epoch: 5| Step: 9
Training loss: 1.9350019693374634
Validation loss: 2.184311039986149

Epoch: 5| Step: 10
Training loss: 1.4697484970092773
Validation loss: 2.1747475439502346

Epoch: 263| Step: 0
Training loss: 2.255467653274536
Validation loss: 2.1929425308781285

Epoch: 5| Step: 1
Training loss: 2.8168082237243652
Validation loss: 2.1934878826141357

Epoch: 5| Step: 2
Training loss: 3.2660937309265137
Validation loss: 2.2150781551996865

Epoch: 5| Step: 3
Training loss: 1.3882992267608643
Validation loss: 2.2196446926363054

Epoch: 5| Step: 4
Training loss: 2.7476720809936523
Validation loss: 2.219997634169876

Epoch: 5| Step: 5
Training loss: 1.6062088012695312
Validation loss: 2.2368348567716536

Epoch: 5| Step: 6
Training loss: 2.138913869857788
Validation loss: 2.2410828413501864

Epoch: 5| Step: 7
Training loss: 1.7782719135284424
Validation loss: 2.2587066286353656

Epoch: 5| Step: 8
Training loss: 2.2384696006774902
Validation loss: 2.2562003417681624

Epoch: 5| Step: 9
Training loss: 2.3152694702148438
Validation loss: 2.2716641502995647

Epoch: 5| Step: 10
Training loss: 1.614153265953064
Validation loss: 2.2559635075189735

Epoch: 264| Step: 0
Training loss: 1.7527936697006226
Validation loss: 2.261763629092965

Epoch: 5| Step: 1
Training loss: 1.8266099691390991
Validation loss: 2.274649356001167

Epoch: 5| Step: 2
Training loss: 1.8469547033309937
Validation loss: 2.301196216255106

Epoch: 5| Step: 3
Training loss: 2.1531360149383545
Validation loss: 2.302359293865901

Epoch: 5| Step: 4
Training loss: 2.5016586780548096
Validation loss: 2.273623497255387

Epoch: 5| Step: 5
Training loss: 2.8663485050201416
Validation loss: 2.2309963805701143

Epoch: 5| Step: 6
Training loss: 2.107292413711548
Validation loss: 2.2091119904671945

Epoch: 5| Step: 7
Training loss: 2.7696762084960938
Validation loss: 2.1947071885549896

Epoch: 5| Step: 8
Training loss: 1.9395965337753296
Validation loss: 2.198777670501381

Epoch: 5| Step: 9
Training loss: 2.143376588821411
Validation loss: 2.19245316648996

Epoch: 5| Step: 10
Training loss: 2.4190256595611572
Validation loss: 2.193120776966054

Epoch: 265| Step: 0
Training loss: 2.204622983932495
Validation loss: 2.1994840380966023

Epoch: 5| Step: 1
Training loss: 2.4482734203338623
Validation loss: 2.182754919093142

Epoch: 5| Step: 2
Training loss: 2.9212183952331543
Validation loss: 2.1790879695646224

Epoch: 5| Step: 3
Training loss: 1.7215001583099365
Validation loss: 2.187939533623316

Epoch: 5| Step: 4
Training loss: 1.7881816625595093
Validation loss: 2.194821616654755

Epoch: 5| Step: 5
Training loss: 2.1432902812957764
Validation loss: 2.216943035843552

Epoch: 5| Step: 6
Training loss: 1.6008596420288086
Validation loss: 2.2281568460567023

Epoch: 5| Step: 7
Training loss: 2.584927558898926
Validation loss: 2.2295673765161985

Epoch: 5| Step: 8
Training loss: 2.440619707107544
Validation loss: 2.213465893140403

Epoch: 5| Step: 9
Training loss: 2.526318073272705
Validation loss: 2.2201439437045845

Epoch: 5| Step: 10
Training loss: 1.893433928489685
Validation loss: 2.1921301259789416

Epoch: 266| Step: 0
Training loss: 2.9496140480041504
Validation loss: 2.1954228185838267

Epoch: 5| Step: 1
Training loss: 2.3962996006011963
Validation loss: 2.178264520501578

Epoch: 5| Step: 2
Training loss: 2.206145763397217
Validation loss: 2.1651672099226262

Epoch: 5| Step: 3
Training loss: 2.617419719696045
Validation loss: 2.167363448809552

Epoch: 5| Step: 4
Training loss: 1.4594470262527466
Validation loss: 2.162233885898385

Epoch: 5| Step: 5
Training loss: 2.295039415359497
Validation loss: 2.166532849752775

Epoch: 5| Step: 6
Training loss: 2.350468873977661
Validation loss: 2.16507121311721

Epoch: 5| Step: 7
Training loss: 2.5433530807495117
Validation loss: 2.1736859031902847

Epoch: 5| Step: 8
Training loss: 1.8911828994750977
Validation loss: 2.1853273414796397

Epoch: 5| Step: 9
Training loss: 1.5042400360107422
Validation loss: 2.200044831921977

Epoch: 5| Step: 10
Training loss: 1.8388041257858276
Validation loss: 2.212016587616295

Epoch: 267| Step: 0
Training loss: 1.9287395477294922
Validation loss: 2.2016827239785144

Epoch: 5| Step: 1
Training loss: 1.99604070186615
Validation loss: 2.2082773946946666

Epoch: 5| Step: 2
Training loss: 2.0353240966796875
Validation loss: 2.2007469028554936

Epoch: 5| Step: 3
Training loss: 2.4134292602539062
Validation loss: 2.2048971678621028

Epoch: 5| Step: 4
Training loss: 2.6545355319976807
Validation loss: 2.209628353836716

Epoch: 5| Step: 5
Training loss: 2.2382495403289795
Validation loss: 2.214427143014887

Epoch: 5| Step: 6
Training loss: 2.5456790924072266
Validation loss: 2.2197636173617457

Epoch: 5| Step: 7
Training loss: 2.239837884902954
Validation loss: 2.2096701283608713

Epoch: 5| Step: 8
Training loss: 2.1864445209503174
Validation loss: 2.2065332974157026

Epoch: 5| Step: 9
Training loss: 2.0351905822753906
Validation loss: 2.199663112240453

Epoch: 5| Step: 10
Training loss: 1.4921178817749023
Validation loss: 2.199996922605781

Epoch: 268| Step: 0
Training loss: 2.2109787464141846
Validation loss: 2.1911349501661075

Epoch: 5| Step: 1
Training loss: 2.509758472442627
Validation loss: 2.200163615647183

Epoch: 5| Step: 2
Training loss: 1.5220258235931396
Validation loss: 2.192515024574854

Epoch: 5| Step: 3
Training loss: 2.2570228576660156
Validation loss: 2.1989850972288396

Epoch: 5| Step: 4
Training loss: 1.8896324634552002
Validation loss: 2.1896851857503257

Epoch: 5| Step: 5
Training loss: 1.6463005542755127
Validation loss: 2.191737864607124

Epoch: 5| Step: 6
Training loss: 2.4291017055511475
Validation loss: 2.187734362899616

Epoch: 5| Step: 7
Training loss: 2.614476203918457
Validation loss: 2.196734846279185

Epoch: 5| Step: 8
Training loss: 2.051729679107666
Validation loss: 2.184088937697872

Epoch: 5| Step: 9
Training loss: 2.6361660957336426
Validation loss: 2.1868887024541057

Epoch: 5| Step: 10
Training loss: 2.096475124359131
Validation loss: 2.186653524316767

Epoch: 269| Step: 0
Training loss: 2.3551149368286133
Validation loss: 2.1926729038197506

Epoch: 5| Step: 1
Training loss: 2.1970794200897217
Validation loss: 2.20929915161543

Epoch: 5| Step: 2
Training loss: 2.1524767875671387
Validation loss: 2.2217725143637708

Epoch: 5| Step: 3
Training loss: 2.5223371982574463
Validation loss: 2.2319369957011235

Epoch: 5| Step: 4
Training loss: 1.9861971139907837
Validation loss: 2.237954488364599

Epoch: 5| Step: 5
Training loss: 1.275706171989441
Validation loss: 2.2404088704816756

Epoch: 5| Step: 6
Training loss: 2.11521577835083
Validation loss: 2.225797191742928

Epoch: 5| Step: 7
Training loss: 2.6140553951263428
Validation loss: 2.213640579613306

Epoch: 5| Step: 8
Training loss: 1.810054063796997
Validation loss: 2.206400348294166

Epoch: 5| Step: 9
Training loss: 2.4009900093078613
Validation loss: 2.2110995015790387

Epoch: 5| Step: 10
Training loss: 2.2914304733276367
Validation loss: 2.2020845977208947

Epoch: 270| Step: 0
Training loss: 2.4847514629364014
Validation loss: 2.191936759538548

Epoch: 5| Step: 1
Training loss: 2.0466344356536865
Validation loss: 2.1852005861138784

Epoch: 5| Step: 2
Training loss: 2.501095771789551
Validation loss: 2.194664823111667

Epoch: 5| Step: 3
Training loss: 2.183797836303711
Validation loss: 2.1892991271070255

Epoch: 5| Step: 4
Training loss: 2.214071273803711
Validation loss: 2.1924315242357153

Epoch: 5| Step: 5
Training loss: 1.986379623413086
Validation loss: 2.190201905465895

Epoch: 5| Step: 6
Training loss: 1.7387393712997437
Validation loss: 2.1775045894807383

Epoch: 5| Step: 7
Training loss: 2.515469789505005
Validation loss: 2.198254636538926

Epoch: 5| Step: 8
Training loss: 2.095357894897461
Validation loss: 2.198864395900439

Epoch: 5| Step: 9
Training loss: 1.6915614604949951
Validation loss: 2.2105922237519295

Epoch: 5| Step: 10
Training loss: 2.2986366748809814
Validation loss: 2.199379362085814

Epoch: 271| Step: 0
Training loss: 1.9595973491668701
Validation loss: 2.1954618333488383

Epoch: 5| Step: 1
Training loss: 2.481567621231079
Validation loss: 2.221980558928623

Epoch: 5| Step: 2
Training loss: 1.8032739162445068
Validation loss: 2.2323005865978938

Epoch: 5| Step: 3
Training loss: 1.9387050867080688
Validation loss: 2.219558226164951

Epoch: 5| Step: 4
Training loss: 2.133761167526245
Validation loss: 2.202644245598906

Epoch: 5| Step: 5
Training loss: 2.702653646469116
Validation loss: 2.1969856754426034

Epoch: 5| Step: 6
Training loss: 2.1926631927490234
Validation loss: 2.194099795433783

Epoch: 5| Step: 7
Training loss: 2.355886697769165
Validation loss: 2.1935982729799006

Epoch: 5| Step: 8
Training loss: 1.724471092224121
Validation loss: 2.1903303848799838

Epoch: 5| Step: 9
Training loss: 2.2869713306427
Validation loss: 2.194396839346937

Epoch: 5| Step: 10
Training loss: 2.0171892642974854
Validation loss: 2.1889109790966077

Epoch: 272| Step: 0
Training loss: 2.2355740070343018
Validation loss: 2.1966701246077016

Epoch: 5| Step: 1
Training loss: 2.8098487854003906
Validation loss: 2.196244957626507

Epoch: 5| Step: 2
Training loss: 2.4125816822052
Validation loss: 2.1843740901639386

Epoch: 5| Step: 3
Training loss: 1.9439125061035156
Validation loss: 2.181999155270156

Epoch: 5| Step: 4
Training loss: 1.8976129293441772
Validation loss: 2.202935573875263

Epoch: 5| Step: 5
Training loss: 1.9384543895721436
Validation loss: 2.2028013993335027

Epoch: 5| Step: 6
Training loss: 1.9698247909545898
Validation loss: 2.1846204829472367

Epoch: 5| Step: 7
Training loss: 2.2638254165649414
Validation loss: 2.1811778699198077

Epoch: 5| Step: 8
Training loss: 1.7950090169906616
Validation loss: 2.17071855965481

Epoch: 5| Step: 9
Training loss: 2.4988579750061035
Validation loss: 2.185185114542643

Epoch: 5| Step: 10
Training loss: 1.654557228088379
Validation loss: 2.1992618230081376

Epoch: 273| Step: 0
Training loss: 1.993298888206482
Validation loss: 2.2056444793619137

Epoch: 5| Step: 1
Training loss: 2.586885690689087
Validation loss: 2.2038638591766357

Epoch: 5| Step: 2
Training loss: 1.9329255819320679
Validation loss: 2.206185604936333

Epoch: 5| Step: 3
Training loss: 2.1024374961853027
Validation loss: 2.205754105762769

Epoch: 5| Step: 4
Training loss: 1.6814014911651611
Validation loss: 2.1888204184911584

Epoch: 5| Step: 5
Training loss: 1.4843858480453491
Validation loss: 2.1778739139597905

Epoch: 5| Step: 6
Training loss: 1.7814264297485352
Validation loss: 2.178583111814273

Epoch: 5| Step: 7
Training loss: 1.9917113780975342
Validation loss: 2.183699671940137

Epoch: 5| Step: 8
Training loss: 2.1811020374298096
Validation loss: 2.203452261545325

Epoch: 5| Step: 9
Training loss: 3.028916597366333
Validation loss: 2.213330812351678

Epoch: 5| Step: 10
Training loss: 3.1009552478790283
Validation loss: 2.209532856941223

Epoch: 274| Step: 0
Training loss: 2.1768994331359863
Validation loss: 2.2266688423772014

Epoch: 5| Step: 1
Training loss: 2.565080165863037
Validation loss: 2.1968710807061966

Epoch: 5| Step: 2
Training loss: 2.130643844604492
Validation loss: 2.180867284856817

Epoch: 5| Step: 3
Training loss: 2.1295840740203857
Validation loss: 2.1573352954720937

Epoch: 5| Step: 4
Training loss: 2.513549327850342
Validation loss: 2.173840598393512

Epoch: 5| Step: 5
Training loss: 2.0577690601348877
Validation loss: 2.162433506340109

Epoch: 5| Step: 6
Training loss: 1.6208889484405518
Validation loss: 2.1535498557552213

Epoch: 5| Step: 7
Training loss: 2.016343355178833
Validation loss: 2.161591418327824

Epoch: 5| Step: 8
Training loss: 1.60024094581604
Validation loss: 2.1624018210236744

Epoch: 5| Step: 9
Training loss: 2.6975290775299072
Validation loss: 2.1596696530618975

Epoch: 5| Step: 10
Training loss: 2.1555206775665283
Validation loss: 2.167171770526517

Epoch: 275| Step: 0
Training loss: 2.03945255279541
Validation loss: 2.158950092971966

Epoch: 5| Step: 1
Training loss: 2.5941901206970215
Validation loss: 2.1713473873753704

Epoch: 5| Step: 2
Training loss: 2.376451015472412
Validation loss: 2.1775443887197845

Epoch: 5| Step: 3
Training loss: 1.9387333393096924
Validation loss: 2.185864674147739

Epoch: 5| Step: 4
Training loss: 1.9202525615692139
Validation loss: 2.1890804177971295

Epoch: 5| Step: 5
Training loss: 2.215043544769287
Validation loss: 2.1942059865561863

Epoch: 5| Step: 6
Training loss: 3.085082530975342
Validation loss: 2.2029490599068264

Epoch: 5| Step: 7
Training loss: 2.235905170440674
Validation loss: 2.2096443842816096

Epoch: 5| Step: 8
Training loss: 1.6142871379852295
Validation loss: 2.174128051727049

Epoch: 5| Step: 9
Training loss: 2.0851056575775146
Validation loss: 2.161351206482098

Epoch: 5| Step: 10
Training loss: 1.3676724433898926
Validation loss: 2.162734439296107

Epoch: 276| Step: 0
Training loss: 2.3695175647735596
Validation loss: 2.162996192132273

Epoch: 5| Step: 1
Training loss: 2.97102689743042
Validation loss: 2.1714571881037887

Epoch: 5| Step: 2
Training loss: 1.6451116800308228
Validation loss: 2.158491219243696

Epoch: 5| Step: 3
Training loss: 2.378636121749878
Validation loss: 2.1695564370001517

Epoch: 5| Step: 4
Training loss: 1.8668628931045532
Validation loss: 2.1582685875636276

Epoch: 5| Step: 5
Training loss: 2.2671589851379395
Validation loss: 2.1749253247373845

Epoch: 5| Step: 6
Training loss: 1.8672816753387451
Validation loss: 2.1891761928476314

Epoch: 5| Step: 7
Training loss: 1.9354406595230103
Validation loss: 2.1900282085582776

Epoch: 5| Step: 8
Training loss: 2.1837990283966064
Validation loss: 2.174014477319615

Epoch: 5| Step: 9
Training loss: 1.5551769733428955
Validation loss: 2.1749067101427304

Epoch: 5| Step: 10
Training loss: 2.4053614139556885
Validation loss: 2.1797488402294856

Epoch: 277| Step: 0
Training loss: 2.082491874694824
Validation loss: 2.1735772189273628

Epoch: 5| Step: 1
Training loss: 1.549717903137207
Validation loss: 2.168302925684119

Epoch: 5| Step: 2
Training loss: 2.1174330711364746
Validation loss: 2.168966304871344

Epoch: 5| Step: 3
Training loss: 2.6692137718200684
Validation loss: 2.1649605740783033

Epoch: 5| Step: 4
Training loss: 2.5425045490264893
Validation loss: 2.1714081866766817

Epoch: 5| Step: 5
Training loss: 1.793391466140747
Validation loss: 2.1544750403332453

Epoch: 5| Step: 6
Training loss: 1.9579941034317017
Validation loss: 2.159951648404521

Epoch: 5| Step: 7
Training loss: 1.255341649055481
Validation loss: 2.1495323668244066

Epoch: 5| Step: 8
Training loss: 2.3603525161743164
Validation loss: 2.163251115429786

Epoch: 5| Step: 9
Training loss: 2.466291904449463
Validation loss: 2.1656798598586873

Epoch: 5| Step: 10
Training loss: 2.6109471321105957
Validation loss: 2.189880017311342

Epoch: 278| Step: 0
Training loss: 1.7911230325698853
Validation loss: 2.1854449574665358

Epoch: 5| Step: 1
Training loss: 2.139864444732666
Validation loss: 2.189101847269202

Epoch: 5| Step: 2
Training loss: 2.216096878051758
Validation loss: 2.2107631878186296

Epoch: 5| Step: 3
Training loss: 2.4304018020629883
Validation loss: 2.20371913397184

Epoch: 5| Step: 4
Training loss: 1.9443668127059937
Validation loss: 2.188766428219375

Epoch: 5| Step: 5
Training loss: 2.7012219429016113
Validation loss: 2.178303732666918

Epoch: 5| Step: 6
Training loss: 1.9778629541397095
Validation loss: 2.156090021133423

Epoch: 5| Step: 7
Training loss: 1.4929813146591187
Validation loss: 2.147423948011091

Epoch: 5| Step: 8
Training loss: 2.168119430541992
Validation loss: 2.1430506193509666

Epoch: 5| Step: 9
Training loss: 3.1025211811065674
Validation loss: 2.1595807613865023

Epoch: 5| Step: 10
Training loss: 1.2905396223068237
Validation loss: 2.1485384997501167

Epoch: 279| Step: 0
Training loss: 1.7978051900863647
Validation loss: 2.1655735520906347

Epoch: 5| Step: 1
Training loss: 1.9231364727020264
Validation loss: 2.165775127308343

Epoch: 5| Step: 2
Training loss: 2.2515921592712402
Validation loss: 2.1642452414317797

Epoch: 5| Step: 3
Training loss: 1.621851921081543
Validation loss: 2.1761880587506037

Epoch: 5| Step: 4
Training loss: 2.0158801078796387
Validation loss: 2.2204244828993276

Epoch: 5| Step: 5
Training loss: 2.755171298980713
Validation loss: 2.2391147228979293

Epoch: 5| Step: 6
Training loss: 2.0087521076202393
Validation loss: 2.2170644678095335

Epoch: 5| Step: 7
Training loss: 2.3304996490478516
Validation loss: 2.2117168646986767

Epoch: 5| Step: 8
Training loss: 2.0797531604766846
Validation loss: 2.1936058511016188

Epoch: 5| Step: 9
Training loss: 2.632006883621216
Validation loss: 2.1926347081379225

Epoch: 5| Step: 10
Training loss: 2.2538249492645264
Validation loss: 2.181286278591361

Epoch: 280| Step: 0
Training loss: 1.7704670429229736
Validation loss: 2.1683870118151427

Epoch: 5| Step: 1
Training loss: 2.295349597930908
Validation loss: 2.174963112800352

Epoch: 5| Step: 2
Training loss: 2.020336389541626
Validation loss: 2.171878327605545

Epoch: 5| Step: 3
Training loss: 2.838364839553833
Validation loss: 2.1598987912618988

Epoch: 5| Step: 4
Training loss: 2.273536205291748
Validation loss: 2.1523877164368987

Epoch: 5| Step: 5
Training loss: 2.200745105743408
Validation loss: 2.1588667156875774

Epoch: 5| Step: 6
Training loss: 1.8137340545654297
Validation loss: 2.153843902772473

Epoch: 5| Step: 7
Training loss: 2.525024890899658
Validation loss: 2.166633241920061

Epoch: 5| Step: 8
Training loss: 1.641187310218811
Validation loss: 2.1924569235053113

Epoch: 5| Step: 9
Training loss: 2.0382046699523926
Validation loss: 2.169715109691825

Epoch: 5| Step: 10
Training loss: 1.8899221420288086
Validation loss: 2.172242388930372

Epoch: 281| Step: 0
Training loss: 2.0629050731658936
Validation loss: 2.170484922265494

Epoch: 5| Step: 1
Training loss: 2.016815662384033
Validation loss: 2.167565520091723

Epoch: 5| Step: 2
Training loss: 1.820660948753357
Validation loss: 2.167365761213405

Epoch: 5| Step: 3
Training loss: 2.05963134765625
Validation loss: 2.150969705274028

Epoch: 5| Step: 4
Training loss: 2.418898105621338
Validation loss: 2.157135855767035

Epoch: 5| Step: 5
Training loss: 2.3386151790618896
Validation loss: 2.165560236541174

Epoch: 5| Step: 6
Training loss: 1.9389235973358154
Validation loss: 2.1842150201079664

Epoch: 5| Step: 7
Training loss: 1.7691015005111694
Validation loss: 2.1734490984229633

Epoch: 5| Step: 8
Training loss: 1.9924986362457275
Validation loss: 2.182319087366904

Epoch: 5| Step: 9
Training loss: 2.2727346420288086
Validation loss: 2.1681522246330016

Epoch: 5| Step: 10
Training loss: 2.4347047805786133
Validation loss: 2.1570591106209704

Epoch: 282| Step: 0
Training loss: 1.8418956995010376
Validation loss: 2.153902094851258

Epoch: 5| Step: 1
Training loss: 2.177696466445923
Validation loss: 2.144356067462634

Epoch: 5| Step: 2
Training loss: 1.787377953529358
Validation loss: 2.146093532603274

Epoch: 5| Step: 3
Training loss: 2.609509229660034
Validation loss: 2.141592764085339

Epoch: 5| Step: 4
Training loss: 2.141146421432495
Validation loss: 2.128411828830678

Epoch: 5| Step: 5
Training loss: 2.49800705909729
Validation loss: 2.134197058216218

Epoch: 5| Step: 6
Training loss: 1.6416938304901123
Validation loss: 2.1274750668515443

Epoch: 5| Step: 7
Training loss: 2.3443102836608887
Validation loss: 2.130027263395248

Epoch: 5| Step: 8
Training loss: 1.8938210010528564
Validation loss: 2.1255230801079863

Epoch: 5| Step: 9
Training loss: 2.5093345642089844
Validation loss: 2.13917141191421

Epoch: 5| Step: 10
Training loss: 1.641078233718872
Validation loss: 2.1511648175536946

Epoch: 283| Step: 0
Training loss: 2.0369668006896973
Validation loss: 2.1526895005215883

Epoch: 5| Step: 1
Training loss: 2.090930461883545
Validation loss: 2.176847714249806

Epoch: 5| Step: 2
Training loss: 1.9252216815948486
Validation loss: 2.175368519239528

Epoch: 5| Step: 3
Training loss: 2.865495204925537
Validation loss: 2.17925827733932

Epoch: 5| Step: 4
Training loss: 2.3753561973571777
Validation loss: 2.1635776258284047

Epoch: 5| Step: 5
Training loss: 2.032170534133911
Validation loss: 2.1517556354563725

Epoch: 5| Step: 6
Training loss: 2.6372413635253906
Validation loss: 2.143534291175104

Epoch: 5| Step: 7
Training loss: 1.3681820631027222
Validation loss: 2.1392288259280625

Epoch: 5| Step: 8
Training loss: 1.836957335472107
Validation loss: 2.1521185277610697

Epoch: 5| Step: 9
Training loss: 1.9428688287734985
Validation loss: 2.143612036141016

Epoch: 5| Step: 10
Training loss: 2.0387721061706543
Validation loss: 2.149313014040711

Epoch: 284| Step: 0
Training loss: 1.466170072555542
Validation loss: 2.1499611459752566

Epoch: 5| Step: 1
Training loss: 2.212519407272339
Validation loss: 2.1350734131310576

Epoch: 5| Step: 2
Training loss: 1.7389278411865234
Validation loss: 2.147043230713055

Epoch: 5| Step: 3
Training loss: 2.535308837890625
Validation loss: 2.1412177572968187

Epoch: 5| Step: 4
Training loss: 1.9920870065689087
Validation loss: 2.1532685269591627

Epoch: 5| Step: 5
Training loss: 2.6656246185302734
Validation loss: 2.1666326189553864

Epoch: 5| Step: 6
Training loss: 1.699761986732483
Validation loss: 2.168883257014777

Epoch: 5| Step: 7
Training loss: 2.545426845550537
Validation loss: 2.144803762435913

Epoch: 5| Step: 8
Training loss: 1.9364449977874756
Validation loss: 2.1466962727167274

Epoch: 5| Step: 9
Training loss: 1.9877550601959229
Validation loss: 2.140047986020324

Epoch: 5| Step: 10
Training loss: 2.148322105407715
Validation loss: 2.1278405702242287

Epoch: 285| Step: 0
Training loss: 1.7751747369766235
Validation loss: 2.138180268708096

Epoch: 5| Step: 1
Training loss: 1.5203160047531128
Validation loss: 2.131577278978081

Epoch: 5| Step: 2
Training loss: 2.129108428955078
Validation loss: 2.1502893355584916

Epoch: 5| Step: 3
Training loss: 2.5533719062805176
Validation loss: 2.137604918531192

Epoch: 5| Step: 4
Training loss: 2.0700430870056152
Validation loss: 2.1285963686563636

Epoch: 5| Step: 5
Training loss: 2.830199718475342
Validation loss: 2.134298329712242

Epoch: 5| Step: 6
Training loss: 2.6166281700134277
Validation loss: 2.134268496626167

Epoch: 5| Step: 7
Training loss: 1.6063182353973389
Validation loss: 2.1233245121535433

Epoch: 5| Step: 8
Training loss: 2.188946485519409
Validation loss: 2.15284288314081

Epoch: 5| Step: 9
Training loss: 1.8196475505828857
Validation loss: 2.1725981517504622

Epoch: 5| Step: 10
Training loss: 2.2883856296539307
Validation loss: 2.1711115144914195

Epoch: 286| Step: 0
Training loss: 1.9772617816925049
Validation loss: 2.1591211467660885

Epoch: 5| Step: 1
Training loss: 2.0781493186950684
Validation loss: 2.1443745859207644

Epoch: 5| Step: 2
Training loss: 1.3948789834976196
Validation loss: 2.1500454871885237

Epoch: 5| Step: 3
Training loss: 2.3564212322235107
Validation loss: 2.1415716807047525

Epoch: 5| Step: 4
Training loss: 1.9489448070526123
Validation loss: 2.1338337826472458

Epoch: 5| Step: 5
Training loss: 2.1544017791748047
Validation loss: 2.1359644320703324

Epoch: 5| Step: 6
Training loss: 2.060615301132202
Validation loss: 2.1504333121802217

Epoch: 5| Step: 7
Training loss: 2.1178195476531982
Validation loss: 2.1584933214290167

Epoch: 5| Step: 8
Training loss: 2.4081573486328125
Validation loss: 2.1546776833072787

Epoch: 5| Step: 9
Training loss: 2.1782963275909424
Validation loss: 2.1604447390443537

Epoch: 5| Step: 10
Training loss: 2.1646625995635986
Validation loss: 2.173422626269761

Epoch: 287| Step: 0
Training loss: 1.5468535423278809
Validation loss: 2.1715437494298464

Epoch: 5| Step: 1
Training loss: 1.8592361211776733
Validation loss: 2.1645939632128646

Epoch: 5| Step: 2
Training loss: 2.536843776702881
Validation loss: 2.2012721518034577

Epoch: 5| Step: 3
Training loss: 2.0339951515197754
Validation loss: 2.2197626136964366

Epoch: 5| Step: 4
Training loss: 1.4873323440551758
Validation loss: 2.208341788220149

Epoch: 5| Step: 5
Training loss: 2.3655173778533936
Validation loss: 2.1707390790344565

Epoch: 5| Step: 6
Training loss: 1.7122703790664673
Validation loss: 2.160041588608937

Epoch: 5| Step: 7
Training loss: 2.6951377391815186
Validation loss: 2.1578547454649404

Epoch: 5| Step: 8
Training loss: 2.8214354515075684
Validation loss: 2.145660851591377

Epoch: 5| Step: 9
Training loss: 1.9013230800628662
Validation loss: 2.1477397206009075

Epoch: 5| Step: 10
Training loss: 2.327256679534912
Validation loss: 2.1424719390048774

Epoch: 288| Step: 0
Training loss: 1.9764690399169922
Validation loss: 2.134286677965554

Epoch: 5| Step: 1
Training loss: 1.8714349269866943
Validation loss: 2.1390228758576098

Epoch: 5| Step: 2
Training loss: 1.900604248046875
Validation loss: 2.1366564458416355

Epoch: 5| Step: 3
Training loss: 2.581571578979492
Validation loss: 2.1430447973230833

Epoch: 5| Step: 4
Training loss: 2.014207363128662
Validation loss: 2.1680694087859123

Epoch: 5| Step: 5
Training loss: 1.9165817499160767
Validation loss: 2.1917478422964773

Epoch: 5| Step: 6
Training loss: 1.8453242778778076
Validation loss: 2.1843266499939786

Epoch: 5| Step: 7
Training loss: 1.9288151264190674
Validation loss: 2.175241284472968

Epoch: 5| Step: 8
Training loss: 2.3694865703582764
Validation loss: 2.1583293279012046

Epoch: 5| Step: 9
Training loss: 2.4562950134277344
Validation loss: 2.143743820087884

Epoch: 5| Step: 10
Training loss: 2.0193614959716797
Validation loss: 2.149148616739499

Epoch: 289| Step: 0
Training loss: 2.6131675243377686
Validation loss: 2.138646976922148

Epoch: 5| Step: 1
Training loss: 1.7256271839141846
Validation loss: 2.1339132324341805

Epoch: 5| Step: 2
Training loss: 2.509371042251587
Validation loss: 2.128263000519045

Epoch: 5| Step: 3
Training loss: 2.8115227222442627
Validation loss: 2.1237026171017717

Epoch: 5| Step: 4
Training loss: 2.748236894607544
Validation loss: 2.1228283887268393

Epoch: 5| Step: 5
Training loss: 1.8431282043457031
Validation loss: 2.1320548057556152

Epoch: 5| Step: 6
Training loss: 1.9342418909072876
Validation loss: 2.1387700547454176

Epoch: 5| Step: 7
Training loss: 2.0392332077026367
Validation loss: 2.138227134622553

Epoch: 5| Step: 8
Training loss: 1.6724039316177368
Validation loss: 2.1545061757487636

Epoch: 5| Step: 9
Training loss: 1.2947824001312256
Validation loss: 2.162630986141902

Epoch: 5| Step: 10
Training loss: 1.5610181093215942
Validation loss: 2.1889612085075787

Epoch: 290| Step: 0
Training loss: 2.263947010040283
Validation loss: 2.209587718850823

Epoch: 5| Step: 1
Training loss: 1.9496186971664429
Validation loss: 2.257244843308644

Epoch: 5| Step: 2
Training loss: 1.7143360376358032
Validation loss: 2.2737984682924006

Epoch: 5| Step: 3
Training loss: 1.9351803064346313
Validation loss: 2.293217735905801

Epoch: 5| Step: 4
Training loss: 1.7809953689575195
Validation loss: 2.3102584077465917

Epoch: 5| Step: 5
Training loss: 2.651538848876953
Validation loss: 2.320710761572725

Epoch: 5| Step: 6
Training loss: 2.306973934173584
Validation loss: 2.2944066729596866

Epoch: 5| Step: 7
Training loss: 1.9653816223144531
Validation loss: 2.2110894174985987

Epoch: 5| Step: 8
Training loss: 2.9185547828674316
Validation loss: 2.1719237527539654

Epoch: 5| Step: 9
Training loss: 1.6792538166046143
Validation loss: 2.152861136262135

Epoch: 5| Step: 10
Training loss: 2.054576873779297
Validation loss: 2.1447148451241116

Epoch: 291| Step: 0
Training loss: 2.0292820930480957
Validation loss: 2.151228722705636

Epoch: 5| Step: 1
Training loss: 2.4323763847351074
Validation loss: 2.1405325705005276

Epoch: 5| Step: 2
Training loss: 1.6067161560058594
Validation loss: 2.163319062161189

Epoch: 5| Step: 3
Training loss: 2.5823323726654053
Validation loss: 2.169687840246385

Epoch: 5| Step: 4
Training loss: 1.182848334312439
Validation loss: 2.18043194278594

Epoch: 5| Step: 5
Training loss: 2.9163565635681152
Validation loss: 2.1822421396932294

Epoch: 5| Step: 6
Training loss: 1.7514934539794922
Validation loss: 2.1382535426847395

Epoch: 5| Step: 7
Training loss: 2.174126148223877
Validation loss: 2.142557438983712

Epoch: 5| Step: 8
Training loss: 1.8609561920166016
Validation loss: 2.1337803999582925

Epoch: 5| Step: 9
Training loss: 2.255561351776123
Validation loss: 2.1450925950081117

Epoch: 5| Step: 10
Training loss: 2.392941474914551
Validation loss: 2.1374272556715113

Epoch: 292| Step: 0
Training loss: 2.318665027618408
Validation loss: 2.124179037668372

Epoch: 5| Step: 1
Training loss: 2.7397243976593018
Validation loss: 2.140410238696683

Epoch: 5| Step: 2
Training loss: 2.012174606323242
Validation loss: 2.1512141266176776

Epoch: 5| Step: 3
Training loss: 2.0342679023742676
Validation loss: 2.161938113550986

Epoch: 5| Step: 4
Training loss: 2.1688294410705566
Validation loss: 2.1875182646577076

Epoch: 5| Step: 5
Training loss: 2.8200981616973877
Validation loss: 2.1668718220085226

Epoch: 5| Step: 6
Training loss: 1.6752979755401611
Validation loss: 2.172172725841563

Epoch: 5| Step: 7
Training loss: 1.357643485069275
Validation loss: 2.1630381896931636

Epoch: 5| Step: 8
Training loss: 1.4142348766326904
Validation loss: 2.144293164694181

Epoch: 5| Step: 9
Training loss: 1.996216058731079
Validation loss: 2.153911546994281

Epoch: 5| Step: 10
Training loss: 2.2248640060424805
Validation loss: 2.156774713147071

Epoch: 293| Step: 0
Training loss: 1.674917459487915
Validation loss: 2.166586224750806

Epoch: 5| Step: 1
Training loss: 2.0179123878479004
Validation loss: 2.150115689923686

Epoch: 5| Step: 2
Training loss: 2.214280128479004
Validation loss: 2.1192601803810365

Epoch: 5| Step: 3
Training loss: 1.791587471961975
Validation loss: 2.1379667507704867

Epoch: 5| Step: 4
Training loss: 2.3327717781066895
Validation loss: 2.1283382497807986

Epoch: 5| Step: 5
Training loss: 1.8834164142608643
Validation loss: 2.1210445127179547

Epoch: 5| Step: 6
Training loss: 2.34920334815979
Validation loss: 2.109635747889037

Epoch: 5| Step: 7
Training loss: 2.324716329574585
Validation loss: 2.121699289609027

Epoch: 5| Step: 8
Training loss: 2.023200750350952
Validation loss: 2.111591787748439

Epoch: 5| Step: 9
Training loss: 2.076932907104492
Validation loss: 2.1128973396875526

Epoch: 5| Step: 10
Training loss: 1.9759689569473267
Validation loss: 2.1262218336905203

Epoch: 294| Step: 0
Training loss: 2.043468475341797
Validation loss: 2.1173729999091035

Epoch: 5| Step: 1
Training loss: 1.7108852863311768
Validation loss: 2.1228554184718798

Epoch: 5| Step: 2
Training loss: 1.8156607151031494
Validation loss: 2.1260643236098753

Epoch: 5| Step: 3
Training loss: 2.339871644973755
Validation loss: 2.1161663147710983

Epoch: 5| Step: 4
Training loss: 2.4787025451660156
Validation loss: 2.13162729817052

Epoch: 5| Step: 5
Training loss: 2.2707531452178955
Validation loss: 2.1291848754370086

Epoch: 5| Step: 6
Training loss: 2.0652225017547607
Validation loss: 2.1419477142313474

Epoch: 5| Step: 7
Training loss: 1.5641000270843506
Validation loss: 2.157446212666009

Epoch: 5| Step: 8
Training loss: 1.8669350147247314
Validation loss: 2.1590624317046134

Epoch: 5| Step: 9
Training loss: 2.00004243850708
Validation loss: 2.162261878290484

Epoch: 5| Step: 10
Training loss: 2.4488823413848877
Validation loss: 2.1356307780870827

Epoch: 295| Step: 0
Training loss: 2.3338887691497803
Validation loss: 2.1357670830142115

Epoch: 5| Step: 1
Training loss: 1.6022770404815674
Validation loss: 2.145469360454108

Epoch: 5| Step: 2
Training loss: 2.274501323699951
Validation loss: 2.132068180268811

Epoch: 5| Step: 3
Training loss: 1.4253947734832764
Validation loss: 2.1305744289070048

Epoch: 5| Step: 4
Training loss: 1.9191606044769287
Validation loss: 2.139401246142644

Epoch: 5| Step: 5
Training loss: 2.4064507484436035
Validation loss: 2.152689964540543

Epoch: 5| Step: 6
Training loss: 2.199270486831665
Validation loss: 2.1381906911890995

Epoch: 5| Step: 7
Training loss: 1.6954841613769531
Validation loss: 2.155357958168112

Epoch: 5| Step: 8
Training loss: 1.750833511352539
Validation loss: 2.155059722162062

Epoch: 5| Step: 9
Training loss: 2.2995285987854004
Validation loss: 2.170223871866862

Epoch: 5| Step: 10
Training loss: 2.599364757537842
Validation loss: 2.1773322833481656

Epoch: 296| Step: 0
Training loss: 2.6176095008850098
Validation loss: 2.151534549651607

Epoch: 5| Step: 1
Training loss: 1.578471302986145
Validation loss: 2.1401232596366637

Epoch: 5| Step: 2
Training loss: 2.2899630069732666
Validation loss: 2.1473826259695072

Epoch: 5| Step: 3
Training loss: 1.5706560611724854
Validation loss: 2.1286828466641006

Epoch: 5| Step: 4
Training loss: 1.9804608821868896
Validation loss: 2.1333692150731243

Epoch: 5| Step: 5
Training loss: 1.6361430883407593
Validation loss: 2.1356762198991674

Epoch: 5| Step: 6
Training loss: 1.921447992324829
Validation loss: 2.1283756379158265

Epoch: 5| Step: 7
Training loss: 2.1039795875549316
Validation loss: 2.1336954665440384

Epoch: 5| Step: 8
Training loss: 2.2463736534118652
Validation loss: 2.133498227724465

Epoch: 5| Step: 9
Training loss: 2.179737091064453
Validation loss: 2.1729340681465725

Epoch: 5| Step: 10
Training loss: 2.2828125953674316
Validation loss: 2.179124124588505

Epoch: 297| Step: 0
Training loss: 1.4641520977020264
Validation loss: 2.2173360368256927

Epoch: 5| Step: 1
Training loss: 1.7443287372589111
Validation loss: 2.221089801480693

Epoch: 5| Step: 2
Training loss: 2.6983444690704346
Validation loss: 2.231179516802552

Epoch: 5| Step: 3
Training loss: 1.6681480407714844
Validation loss: 2.1951860663711384

Epoch: 5| Step: 4
Training loss: 2.0496482849121094
Validation loss: 2.171574433644613

Epoch: 5| Step: 5
Training loss: 2.0899693965911865
Validation loss: 2.1338592857442875

Epoch: 5| Step: 6
Training loss: 1.9498800039291382
Validation loss: 2.118198340938937

Epoch: 5| Step: 7
Training loss: 1.9604530334472656
Validation loss: 2.1164476653581024

Epoch: 5| Step: 8
Training loss: 2.238675594329834
Validation loss: 2.1236651353938605

Epoch: 5| Step: 9
Training loss: 2.2570834159851074
Validation loss: 2.1054877696498746

Epoch: 5| Step: 10
Training loss: 2.1946861743927
Validation loss: 2.1160045695561234

Epoch: 298| Step: 0
Training loss: 1.4085434675216675
Validation loss: 2.1043709990798787

Epoch: 5| Step: 1
Training loss: 2.154188632965088
Validation loss: 2.1267426334401613

Epoch: 5| Step: 2
Training loss: 2.548204183578491
Validation loss: 2.116551944004592

Epoch: 5| Step: 3
Training loss: 2.02107834815979
Validation loss: 2.1306769412050963

Epoch: 5| Step: 4
Training loss: 1.6182199716567993
Validation loss: 2.1429076810036936

Epoch: 5| Step: 5
Training loss: 2.021496295928955
Validation loss: 2.1555157810129146

Epoch: 5| Step: 6
Training loss: 1.8385164737701416
Validation loss: 2.1510923267692648

Epoch: 5| Step: 7
Training loss: 1.9668766260147095
Validation loss: 2.173350639240716

Epoch: 5| Step: 8
Training loss: 2.4497742652893066
Validation loss: 2.1609155644652662

Epoch: 5| Step: 9
Training loss: 1.8322045803070068
Validation loss: 2.1523933218371485

Epoch: 5| Step: 10
Training loss: 2.3938469886779785
Validation loss: 2.1503270774759273

Epoch: 299| Step: 0
Training loss: 2.184556722640991
Validation loss: 2.133910522666029

Epoch: 5| Step: 1
Training loss: 2.1787238121032715
Validation loss: 2.1192501911553006

Epoch: 5| Step: 2
Training loss: 2.6542434692382812
Validation loss: 2.1209866359669673

Epoch: 5| Step: 3
Training loss: 2.230581521987915
Validation loss: 2.136121965223743

Epoch: 5| Step: 4
Training loss: 1.9547929763793945
Validation loss: 2.128042867106776

Epoch: 5| Step: 5
Training loss: 1.5659325122833252
Validation loss: 2.1283215527893393

Epoch: 5| Step: 6
Training loss: 1.6289352178573608
Validation loss: 2.127931051356818

Epoch: 5| Step: 7
Training loss: 1.9083995819091797
Validation loss: 2.1393429271636473

Epoch: 5| Step: 8
Training loss: 2.0020995140075684
Validation loss: 2.142232120677989

Epoch: 5| Step: 9
Training loss: 1.755923867225647
Validation loss: 2.1709538557196177

Epoch: 5| Step: 10
Training loss: 2.041330337524414
Validation loss: 2.1820116094363633

Epoch: 300| Step: 0
Training loss: 1.663435935974121
Validation loss: 2.1868012887175365

Epoch: 5| Step: 1
Training loss: 1.969366431236267
Validation loss: 2.19156301406122

Epoch: 5| Step: 2
Training loss: 2.7463388442993164
Validation loss: 2.1805399079476633

Epoch: 5| Step: 3
Training loss: 2.186380386352539
Validation loss: 2.135293896480273

Epoch: 5| Step: 4
Training loss: 2.1098265647888184
Validation loss: 2.128804919540241

Epoch: 5| Step: 5
Training loss: 2.101217269897461
Validation loss: 2.1411209388445784

Epoch: 5| Step: 6
Training loss: 1.8478931188583374
Validation loss: 2.1417810275990474

Epoch: 5| Step: 7
Training loss: 1.743534803390503
Validation loss: 2.1510440123978483

Epoch: 5| Step: 8
Training loss: 1.2025272846221924
Validation loss: 2.1414183339764996

Epoch: 5| Step: 9
Training loss: 2.5311453342437744
Validation loss: 2.1219239888652677

Epoch: 5| Step: 10
Training loss: 2.1767306327819824
Validation loss: 2.122021141872611

Testing loss: 2.3545235660341053

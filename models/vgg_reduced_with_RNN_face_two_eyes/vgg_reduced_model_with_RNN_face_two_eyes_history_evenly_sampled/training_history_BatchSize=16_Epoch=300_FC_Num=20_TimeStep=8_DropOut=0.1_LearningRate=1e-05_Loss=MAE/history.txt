Epoch: 1| Step: 0
Training loss: 5.828533172607422
Validation loss: 5.108731239072738

Epoch: 6| Step: 1
Training loss: 4.089815139770508
Validation loss: 5.1013938227007465

Epoch: 6| Step: 2
Training loss: 5.10106086730957
Validation loss: 5.095333489038611

Epoch: 6| Step: 3
Training loss: 4.009428024291992
Validation loss: 5.090080174066687

Epoch: 6| Step: 4
Training loss: 3.1657650470733643
Validation loss: 5.0841919940005065

Epoch: 6| Step: 5
Training loss: 5.5598978996276855
Validation loss: 5.078198350885863

Epoch: 6| Step: 6
Training loss: 5.456376075744629
Validation loss: 5.071199791405791

Epoch: 6| Step: 7
Training loss: 3.838439464569092
Validation loss: 5.063830652544575

Epoch: 6| Step: 8
Training loss: 4.2520904541015625
Validation loss: 5.055619019334034

Epoch: 6| Step: 9
Training loss: 5.441409111022949
Validation loss: 5.046772510774674

Epoch: 6| Step: 10
Training loss: 5.36673641204834
Validation loss: 5.037194687833068

Epoch: 6| Step: 11
Training loss: 5.332817077636719
Validation loss: 5.027169863382976

Epoch: 6| Step: 12
Training loss: 6.1346821784973145
Validation loss: 5.016022036152501

Epoch: 6| Step: 13
Training loss: 4.175073623657227
Validation loss: 5.003767054568055

Epoch: 2| Step: 0
Training loss: 4.282188892364502
Validation loss: 4.991166796735538

Epoch: 6| Step: 1
Training loss: 3.8169913291931152
Validation loss: 4.976977558546169

Epoch: 6| Step: 2
Training loss: 5.29168701171875
Validation loss: 4.96277658400997

Epoch: 6| Step: 3
Training loss: 5.307689666748047
Validation loss: 4.946897506713867

Epoch: 6| Step: 4
Training loss: 5.528676509857178
Validation loss: 4.930251588103592

Epoch: 6| Step: 5
Training loss: 4.852331161499023
Validation loss: 4.9121578431898545

Epoch: 6| Step: 6
Training loss: 5.4185943603515625
Validation loss: 4.8941887219746905

Epoch: 6| Step: 7
Training loss: 4.636074066162109
Validation loss: 4.8747943960210325

Epoch: 6| Step: 8
Training loss: 4.539559841156006
Validation loss: 4.855037238008233

Epoch: 6| Step: 9
Training loss: 5.096696853637695
Validation loss: 4.834416430483582

Epoch: 6| Step: 10
Training loss: 2.9232382774353027
Validation loss: 4.81305605365384

Epoch: 6| Step: 11
Training loss: 4.717624664306641
Validation loss: 4.791388065584244

Epoch: 6| Step: 12
Training loss: 4.533084392547607
Validation loss: 4.768871327882172

Epoch: 6| Step: 13
Training loss: 4.31195592880249
Validation loss: 4.746230417682279

Epoch: 3| Step: 0
Training loss: 5.238594055175781
Validation loss: 4.723528041634508

Epoch: 6| Step: 1
Training loss: 5.222223281860352
Validation loss: 4.700116649750741

Epoch: 6| Step: 2
Training loss: 3.579746961593628
Validation loss: 4.676608416341966

Epoch: 6| Step: 3
Training loss: 4.113783836364746
Validation loss: 4.653585695451306

Epoch: 6| Step: 4
Training loss: 5.0763258934021
Validation loss: 4.629752641083092

Epoch: 6| Step: 5
Training loss: 3.6400508880615234
Validation loss: 4.604514306591403

Epoch: 6| Step: 6
Training loss: 5.256757736206055
Validation loss: 4.577310141696725

Epoch: 6| Step: 7
Training loss: 3.597062587738037
Validation loss: 4.550655980263987

Epoch: 6| Step: 8
Training loss: 4.5929107666015625
Validation loss: 4.520336279305079

Epoch: 6| Step: 9
Training loss: 3.384537935256958
Validation loss: 4.4911412064747145

Epoch: 6| Step: 10
Training loss: 4.900269031524658
Validation loss: 4.459579375482375

Epoch: 6| Step: 11
Training loss: 4.824986457824707
Validation loss: 4.426883479600312

Epoch: 6| Step: 12
Training loss: 3.18137788772583
Validation loss: 4.394143806990757

Epoch: 6| Step: 13
Training loss: 4.156139373779297
Validation loss: 4.357300399452128

Epoch: 4| Step: 0
Training loss: 2.8269529342651367
Validation loss: 4.3201724585666454

Epoch: 6| Step: 1
Training loss: 4.619842529296875
Validation loss: 4.280839027897004

Epoch: 6| Step: 2
Training loss: 4.255199432373047
Validation loss: 4.244717095487861

Epoch: 6| Step: 3
Training loss: 4.074939727783203
Validation loss: 4.20606699041141

Epoch: 6| Step: 4
Training loss: 3.6322596073150635
Validation loss: 4.1675374687358895

Epoch: 6| Step: 5
Training loss: 3.533414602279663
Validation loss: 4.1333947284247285

Epoch: 6| Step: 6
Training loss: 4.427733421325684
Validation loss: 4.098386810671899

Epoch: 6| Step: 7
Training loss: 3.9642720222473145
Validation loss: 4.066041828483663

Epoch: 6| Step: 8
Training loss: 5.117347717285156
Validation loss: 4.035311875804778

Epoch: 6| Step: 9
Training loss: 4.355652809143066
Validation loss: 3.9976747010343816

Epoch: 6| Step: 10
Training loss: 2.893002986907959
Validation loss: 3.9557086011414886

Epoch: 6| Step: 11
Training loss: 3.835885524749756
Validation loss: 3.918869859428816

Epoch: 6| Step: 12
Training loss: 3.893272638320923
Validation loss: 3.890230937670636

Epoch: 6| Step: 13
Training loss: 3.136597156524658
Validation loss: 3.867041003319525

Epoch: 5| Step: 0
Training loss: 3.7148842811584473
Validation loss: 3.848193230167512

Epoch: 6| Step: 1
Training loss: 3.1371898651123047
Validation loss: 3.8296830679780696

Epoch: 6| Step: 2
Training loss: 4.376518249511719
Validation loss: 3.8139388817612843

Epoch: 6| Step: 3
Training loss: 3.3974859714508057
Validation loss: 3.7980741582890993

Epoch: 6| Step: 4
Training loss: 3.0277321338653564
Validation loss: 3.7803990610184206

Epoch: 6| Step: 5
Training loss: 4.024260520935059
Validation loss: 3.768030120480445

Epoch: 6| Step: 6
Training loss: 3.570883274078369
Validation loss: 3.7526672219717376

Epoch: 6| Step: 7
Training loss: 3.924818992614746
Validation loss: 3.7380352148445706

Epoch: 6| Step: 8
Training loss: 3.8984549045562744
Validation loss: 3.723864678413637

Epoch: 6| Step: 9
Training loss: 3.829122543334961
Validation loss: 3.7072234307565997

Epoch: 6| Step: 10
Training loss: 3.6881051063537598
Validation loss: 3.692183627877184

Epoch: 6| Step: 11
Training loss: 3.120279312133789
Validation loss: 3.6766087009060766

Epoch: 6| Step: 12
Training loss: 3.6060452461242676
Validation loss: 3.6591891216975387

Epoch: 6| Step: 13
Training loss: 3.617691993713379
Validation loss: 3.640872804067468

Epoch: 6| Step: 0
Training loss: 3.3212032318115234
Validation loss: 3.6232096354166665

Epoch: 6| Step: 1
Training loss: 3.863797903060913
Validation loss: 3.6028260184872534

Epoch: 6| Step: 2
Training loss: 4.319146156311035
Validation loss: 3.5842536572487123

Epoch: 6| Step: 3
Training loss: 3.3736133575439453
Validation loss: 3.5612069893908758

Epoch: 6| Step: 4
Training loss: 2.746215581893921
Validation loss: 3.5417868116850495

Epoch: 6| Step: 5
Training loss: 3.5819787979125977
Validation loss: 3.5189143047537854

Epoch: 6| Step: 6
Training loss: 3.8002994060516357
Validation loss: 3.495286633891444

Epoch: 6| Step: 7
Training loss: 3.692619562149048
Validation loss: 3.4824818590635895

Epoch: 6| Step: 8
Training loss: 3.9783012866973877
Validation loss: 3.455117210265129

Epoch: 6| Step: 9
Training loss: 2.619457244873047
Validation loss: 3.443206546127155

Epoch: 6| Step: 10
Training loss: 3.215322732925415
Validation loss: 3.4201159836143575

Epoch: 6| Step: 11
Training loss: 3.063523769378662
Validation loss: 3.4047568536573842

Epoch: 6| Step: 12
Training loss: 2.4502594470977783
Validation loss: 3.3804099585420344

Epoch: 6| Step: 13
Training loss: 4.570002555847168
Validation loss: 3.3607199550956808

Epoch: 7| Step: 0
Training loss: 2.8823041915893555
Validation loss: 3.354883614406791

Epoch: 6| Step: 1
Training loss: 2.814934730529785
Validation loss: 3.3456353346506753

Epoch: 6| Step: 2
Training loss: 2.276700735092163
Validation loss: 3.341617081754951

Epoch: 6| Step: 3
Training loss: 3.3921403884887695
Validation loss: 3.3141968173365437

Epoch: 6| Step: 4
Training loss: 2.995152711868286
Validation loss: 3.299721961380333

Epoch: 6| Step: 5
Training loss: 2.840325355529785
Validation loss: 3.2916162116553194

Epoch: 6| Step: 6
Training loss: 3.645956516265869
Validation loss: 3.287718270414619

Epoch: 6| Step: 7
Training loss: 3.4959235191345215
Validation loss: 3.279150591101698

Epoch: 6| Step: 8
Training loss: 2.8951199054718018
Validation loss: 3.2677368117916967

Epoch: 6| Step: 9
Training loss: 3.6909029483795166
Validation loss: 3.255199552864157

Epoch: 6| Step: 10
Training loss: 2.997722864151001
Validation loss: 3.2420239217819704

Epoch: 6| Step: 11
Training loss: 4.218136787414551
Validation loss: 3.2315949137492845

Epoch: 6| Step: 12
Training loss: 3.609560012817383
Validation loss: 3.2201084321545017

Epoch: 6| Step: 13
Training loss: 4.171541213989258
Validation loss: 3.213043823037096

Epoch: 8| Step: 0
Training loss: 2.6017351150512695
Validation loss: 3.201526931537095

Epoch: 6| Step: 1
Training loss: 2.776120662689209
Validation loss: 3.196969214306083

Epoch: 6| Step: 2
Training loss: 3.093432903289795
Validation loss: 3.1870694365552676

Epoch: 6| Step: 3
Training loss: 3.832296371459961
Validation loss: 3.1680375760601414

Epoch: 6| Step: 4
Training loss: 3.1024632453918457
Validation loss: 3.1541145360598

Epoch: 6| Step: 5
Training loss: 3.183046579360962
Validation loss: 3.1441935954555387

Epoch: 6| Step: 6
Training loss: 3.672543525695801
Validation loss: 3.138495978488717

Epoch: 6| Step: 7
Training loss: 3.214646577835083
Validation loss: 3.133616498721543

Epoch: 6| Step: 8
Training loss: 3.8107142448425293
Validation loss: 3.132252203520908

Epoch: 6| Step: 9
Training loss: 2.819833278656006
Validation loss: 3.132124347071494

Epoch: 6| Step: 10
Training loss: 2.6054773330688477
Validation loss: 3.1185122561711136

Epoch: 6| Step: 11
Training loss: 3.176712989807129
Validation loss: 3.114127979483656

Epoch: 6| Step: 12
Training loss: 2.911500930786133
Validation loss: 3.104803841601136

Epoch: 6| Step: 13
Training loss: 3.58461856842041
Validation loss: 3.0971300268685944

Epoch: 9| Step: 0
Training loss: 3.7148516178131104
Validation loss: 3.089048211292554

Epoch: 6| Step: 1
Training loss: 3.2499656677246094
Validation loss: 3.0829709012021302

Epoch: 6| Step: 2
Training loss: 3.7945542335510254
Validation loss: 3.079841375350952

Epoch: 6| Step: 3
Training loss: 3.4879567623138428
Validation loss: 3.07214194215754

Epoch: 6| Step: 4
Training loss: 2.263111114501953
Validation loss: 3.063926765995641

Epoch: 6| Step: 5
Training loss: 3.27428936958313
Validation loss: 3.0586355450332805

Epoch: 6| Step: 6
Training loss: 3.2319834232330322
Validation loss: 3.0570163060260076

Epoch: 6| Step: 7
Training loss: 2.7147083282470703
Validation loss: 3.054914774433259

Epoch: 6| Step: 8
Training loss: 3.5735931396484375
Validation loss: 3.045495307573708

Epoch: 6| Step: 9
Training loss: 3.7939295768737793
Validation loss: 3.054916907382268

Epoch: 6| Step: 10
Training loss: 2.3816280364990234
Validation loss: 3.0470412162042435

Epoch: 6| Step: 11
Training loss: 2.3219621181488037
Validation loss: 3.0457367999579317

Epoch: 6| Step: 12
Training loss: 2.3237361907958984
Validation loss: 3.0353819990670807

Epoch: 6| Step: 13
Training loss: 3.437659978866577
Validation loss: 3.0294851128773024

Epoch: 10| Step: 0
Training loss: 3.4888432025909424
Validation loss: 3.025774730149136

Epoch: 6| Step: 1
Training loss: 3.6893298625946045
Validation loss: 3.024377399875272

Epoch: 6| Step: 2
Training loss: 2.961444616317749
Validation loss: 3.018819098831505

Epoch: 6| Step: 3
Training loss: 1.905409336090088
Validation loss: 3.019932190577189

Epoch: 6| Step: 4
Training loss: 3.3963332176208496
Validation loss: 3.0161324085727816

Epoch: 6| Step: 5
Training loss: 2.4891552925109863
Validation loss: 3.013103905544486

Epoch: 6| Step: 6
Training loss: 2.9686405658721924
Validation loss: 3.0156009556144796

Epoch: 6| Step: 7
Training loss: 4.294168472290039
Validation loss: 3.008191482995146

Epoch: 6| Step: 8
Training loss: 3.823171377182007
Validation loss: 3.0061995316577215

Epoch: 6| Step: 9
Training loss: 3.663465976715088
Validation loss: 3.002679988902102

Epoch: 6| Step: 10
Training loss: 2.4279215335845947
Validation loss: 2.995012537125618

Epoch: 6| Step: 11
Training loss: 1.7301576137542725
Validation loss: 2.990251064300537

Epoch: 6| Step: 12
Training loss: 3.172524929046631
Validation loss: 2.9837600287570747

Epoch: 6| Step: 13
Training loss: 2.900304079055786
Validation loss: 2.9831009731497815

Epoch: 11| Step: 0
Training loss: 3.8072667121887207
Validation loss: 2.9781047144243793

Epoch: 6| Step: 1
Training loss: 2.9722399711608887
Validation loss: 2.9773525345710015

Epoch: 6| Step: 2
Training loss: 2.9426920413970947
Validation loss: 2.971819311059931

Epoch: 6| Step: 3
Training loss: 4.093585014343262
Validation loss: 2.9720410608476207

Epoch: 6| Step: 4
Training loss: 3.963637351989746
Validation loss: 2.971346060434977

Epoch: 6| Step: 5
Training loss: 2.693049192428589
Validation loss: 2.9632136180836666

Epoch: 6| Step: 6
Training loss: 2.745222568511963
Validation loss: 2.95619321638538

Epoch: 6| Step: 7
Training loss: 2.535661458969116
Validation loss: 2.955331997204852

Epoch: 6| Step: 8
Training loss: 2.826934337615967
Validation loss: 2.964488496062576

Epoch: 6| Step: 9
Training loss: 2.644930601119995
Validation loss: 2.9617118937994844

Epoch: 6| Step: 10
Training loss: 2.985525131225586
Validation loss: 2.9481685110317764

Epoch: 6| Step: 11
Training loss: 2.435199499130249
Validation loss: 2.943568955185593

Epoch: 6| Step: 12
Training loss: 2.7537643909454346
Validation loss: 2.9446650935757543

Epoch: 6| Step: 13
Training loss: 3.238086700439453
Validation loss: 2.9522543620037776

Epoch: 12| Step: 0
Training loss: 2.722294807434082
Validation loss: 2.9489950313363025

Epoch: 6| Step: 1
Training loss: 3.6327693462371826
Validation loss: 2.948220737518803

Epoch: 6| Step: 2
Training loss: 2.1254653930664062
Validation loss: 2.9363721955207085

Epoch: 6| Step: 3
Training loss: 2.8437743186950684
Validation loss: 2.926807249746015

Epoch: 6| Step: 4
Training loss: 2.8643417358398438
Validation loss: 2.926107019506475

Epoch: 6| Step: 5
Training loss: 2.965121269226074
Validation loss: 2.9235368851692445

Epoch: 6| Step: 6
Training loss: 2.7546439170837402
Validation loss: 2.9261046583934496

Epoch: 6| Step: 7
Training loss: 2.894923448562622
Validation loss: 2.9349928081676526

Epoch: 6| Step: 8
Training loss: 2.9988865852355957
Validation loss: 2.9659136418373353

Epoch: 6| Step: 9
Training loss: 3.1704769134521484
Validation loss: 2.946936438160558

Epoch: 6| Step: 10
Training loss: 3.5603866577148438
Validation loss: 2.917300260195168

Epoch: 6| Step: 11
Training loss: 2.9954543113708496
Validation loss: 2.90495180314587

Epoch: 6| Step: 12
Training loss: 3.7666006088256836
Validation loss: 2.899266258362801

Epoch: 6| Step: 13
Training loss: 2.9232771396636963
Validation loss: 2.9011104901631675

Epoch: 13| Step: 0
Training loss: 3.7595112323760986
Validation loss: 2.9078687826792398

Epoch: 6| Step: 1
Training loss: 2.7757182121276855
Validation loss: 2.9077430309787875

Epoch: 6| Step: 2
Training loss: 2.5693628787994385
Validation loss: 2.9094149758738856

Epoch: 6| Step: 3
Training loss: 2.774566650390625
Validation loss: 2.907413718520954

Epoch: 6| Step: 4
Training loss: 3.073812961578369
Validation loss: 2.9003434693941506

Epoch: 6| Step: 5
Training loss: 3.790005683898926
Validation loss: 2.888378538111205

Epoch: 6| Step: 6
Training loss: 2.9920873641967773
Validation loss: 2.885477283949493

Epoch: 6| Step: 7
Training loss: 3.227102756500244
Validation loss: 2.884328567853538

Epoch: 6| Step: 8
Training loss: 3.290518283843994
Validation loss: 2.885333207345778

Epoch: 6| Step: 9
Training loss: 2.0925989151000977
Validation loss: 2.883182333361718

Epoch: 6| Step: 10
Training loss: 3.096160411834717
Validation loss: 2.8798598371526247

Epoch: 6| Step: 11
Training loss: 2.6818950176239014
Validation loss: 2.88102731140711

Epoch: 6| Step: 12
Training loss: 2.642921209335327
Validation loss: 2.8787854768896617

Epoch: 6| Step: 13
Training loss: 3.25582218170166
Validation loss: 2.8741606820014214

Epoch: 14| Step: 0
Training loss: 2.500143527984619
Validation loss: 2.8754320811199885

Epoch: 6| Step: 1
Training loss: 2.336536407470703
Validation loss: 2.871787037900699

Epoch: 6| Step: 2
Training loss: 2.431391716003418
Validation loss: 2.877068045318768

Epoch: 6| Step: 3
Training loss: 3.5488266944885254
Validation loss: 2.8748967160460768

Epoch: 6| Step: 4
Training loss: 2.1792874336242676
Validation loss: 2.8720711303013626

Epoch: 6| Step: 5
Training loss: 3.3672220706939697
Validation loss: 2.8759834433114655

Epoch: 6| Step: 6
Training loss: 4.360136985778809
Validation loss: 2.8764502540711434

Epoch: 6| Step: 7
Training loss: 4.002200126647949
Validation loss: 2.874260425567627

Epoch: 6| Step: 8
Training loss: 3.1280665397644043
Validation loss: 2.8661821273065384

Epoch: 6| Step: 9
Training loss: 2.6286211013793945
Validation loss: 2.8622751928144887

Epoch: 6| Step: 10
Training loss: 2.8270020484924316
Validation loss: 2.85858339904457

Epoch: 6| Step: 11
Training loss: 2.515331268310547
Validation loss: 2.8567049221325944

Epoch: 6| Step: 12
Training loss: 3.0745770931243896
Validation loss: 2.8580136606770177

Epoch: 6| Step: 13
Training loss: 2.537768602371216
Validation loss: 2.8582355001921296

Epoch: 15| Step: 0
Training loss: 2.7694778442382812
Validation loss: 2.8534421254229803

Epoch: 6| Step: 1
Training loss: 2.5382351875305176
Validation loss: 2.854050208163518

Epoch: 6| Step: 2
Training loss: 3.201064348220825
Validation loss: 2.8529006691389185

Epoch: 6| Step: 3
Training loss: 3.4439117908477783
Validation loss: 2.8491950112004436

Epoch: 6| Step: 4
Training loss: 3.1514198780059814
Validation loss: 2.8463962847186672

Epoch: 6| Step: 5
Training loss: 2.3540544509887695
Validation loss: 2.849920367681852

Epoch: 6| Step: 6
Training loss: 3.232966899871826
Validation loss: 2.849567767112486

Epoch: 6| Step: 7
Training loss: 4.03398323059082
Validation loss: 2.8434988760179087

Epoch: 6| Step: 8
Training loss: 2.165724039077759
Validation loss: 2.8412050918866227

Epoch: 6| Step: 9
Training loss: 3.309720039367676
Validation loss: 2.839899193856024

Epoch: 6| Step: 10
Training loss: 2.4509007930755615
Validation loss: 2.8396688840722524

Epoch: 6| Step: 11
Training loss: 3.312115430831909
Validation loss: 2.8409308515569216

Epoch: 6| Step: 12
Training loss: 2.650580406188965
Validation loss: 2.836632856758692

Epoch: 6| Step: 13
Training loss: 2.7579054832458496
Validation loss: 2.8383748608250774

Epoch: 16| Step: 0
Training loss: 3.2137818336486816
Validation loss: 2.837762799314273

Epoch: 6| Step: 1
Training loss: 2.8613202571868896
Validation loss: 2.849535670331729

Epoch: 6| Step: 2
Training loss: 3.269653558731079
Validation loss: 2.8470418863399054

Epoch: 6| Step: 3
Training loss: 3.6005101203918457
Validation loss: 2.830763122086884

Epoch: 6| Step: 4
Training loss: 2.6742758750915527
Validation loss: 2.8268520370606454

Epoch: 6| Step: 5
Training loss: 2.3811724185943604
Validation loss: 2.8279960540033158

Epoch: 6| Step: 6
Training loss: 3.7242794036865234
Validation loss: 2.827199602639803

Epoch: 6| Step: 7
Training loss: 3.476134777069092
Validation loss: 2.8288604444073093

Epoch: 6| Step: 8
Training loss: 2.0271105766296387
Validation loss: 2.8318999146902435

Epoch: 6| Step: 9
Training loss: 3.3535521030426025
Validation loss: 2.8365155317450084

Epoch: 6| Step: 10
Training loss: 2.8477282524108887
Validation loss: 2.8312624859553512

Epoch: 6| Step: 11
Training loss: 2.600949764251709
Validation loss: 2.8297635047666487

Epoch: 6| Step: 12
Training loss: 2.6127772331237793
Validation loss: 2.829607084233274

Epoch: 6| Step: 13
Training loss: 2.540565013885498
Validation loss: 2.841251701437017

Epoch: 17| Step: 0
Training loss: 2.578798294067383
Validation loss: 2.8490486901293517

Epoch: 6| Step: 1
Training loss: 3.103151798248291
Validation loss: 2.851751394169305

Epoch: 6| Step: 2
Training loss: 2.8299245834350586
Validation loss: 2.872572773246355

Epoch: 6| Step: 3
Training loss: 2.572153091430664
Validation loss: 2.8293549065948813

Epoch: 6| Step: 4
Training loss: 2.722958564758301
Validation loss: 2.816120260505266

Epoch: 6| Step: 5
Training loss: 3.1744940280914307
Validation loss: 2.824553543521512

Epoch: 6| Step: 6
Training loss: 3.053877830505371
Validation loss: 2.839989067405783

Epoch: 6| Step: 7
Training loss: 3.1788811683654785
Validation loss: 2.8441553705482074

Epoch: 6| Step: 8
Training loss: 3.0553603172302246
Validation loss: 2.8391805284766742

Epoch: 6| Step: 9
Training loss: 3.2835030555725098
Validation loss: 2.821824078918785

Epoch: 6| Step: 10
Training loss: 3.185246229171753
Validation loss: 2.8088531929959535

Epoch: 6| Step: 11
Training loss: 3.338512897491455
Validation loss: 2.810516947059221

Epoch: 6| Step: 12
Training loss: 2.5583300590515137
Validation loss: 2.8101728398312806

Epoch: 6| Step: 13
Training loss: 2.598257303237915
Validation loss: 2.8179537891059794

Epoch: 18| Step: 0
Training loss: 2.658416986465454
Validation loss: 2.8183883774665093

Epoch: 6| Step: 1
Training loss: 2.371826171875
Validation loss: 2.8240339525284304

Epoch: 6| Step: 2
Training loss: 2.8685524463653564
Validation loss: 2.831458850573468

Epoch: 6| Step: 3
Training loss: 2.956404209136963
Validation loss: 2.8258578623494794

Epoch: 6| Step: 4
Training loss: 3.006080150604248
Validation loss: 2.8296286059964086

Epoch: 6| Step: 5
Training loss: 3.0197856426239014
Validation loss: 2.8075578443465696

Epoch: 6| Step: 6
Training loss: 2.988178253173828
Validation loss: 2.80621842158738

Epoch: 6| Step: 7
Training loss: 3.258141040802002
Validation loss: 2.7995770874843804

Epoch: 6| Step: 8
Training loss: 3.5791680812835693
Validation loss: 2.797511454551451

Epoch: 6| Step: 9
Training loss: 2.7577295303344727
Validation loss: 2.7990120149427846

Epoch: 6| Step: 10
Training loss: 3.16115140914917
Validation loss: 2.8004907484977477

Epoch: 6| Step: 11
Training loss: 2.7937238216400146
Validation loss: 2.7976076269662506

Epoch: 6| Step: 12
Training loss: 3.2722620964050293
Validation loss: 2.805463621693273

Epoch: 6| Step: 13
Training loss: 2.0887603759765625
Validation loss: 2.808327172392158

Epoch: 19| Step: 0
Training loss: 3.272308588027954
Validation loss: 2.8102653718763784

Epoch: 6| Step: 1
Training loss: 3.220273494720459
Validation loss: 2.8002224891416487

Epoch: 6| Step: 2
Training loss: 3.7704484462738037
Validation loss: 2.796934102171211

Epoch: 6| Step: 3
Training loss: 3.3139476776123047
Validation loss: 2.7902008461695846

Epoch: 6| Step: 4
Training loss: 3.2238807678222656
Validation loss: 2.7880350338515414

Epoch: 6| Step: 5
Training loss: 3.106177806854248
Validation loss: 2.7852672402576735

Epoch: 6| Step: 6
Training loss: 2.625383138656616
Validation loss: 2.7898814985829015

Epoch: 6| Step: 7
Training loss: 2.51233172416687
Validation loss: 2.7860875232245332

Epoch: 6| Step: 8
Training loss: 3.116090774536133
Validation loss: 2.784631424052741

Epoch: 6| Step: 9
Training loss: 1.9366918802261353
Validation loss: 2.7833958415574926

Epoch: 6| Step: 10
Training loss: 2.3434720039367676
Validation loss: 2.785780427276447

Epoch: 6| Step: 11
Training loss: 1.9432519674301147
Validation loss: 2.7865300845074397

Epoch: 6| Step: 12
Training loss: 2.628760576248169
Validation loss: 2.78948551095942

Epoch: 6| Step: 13
Training loss: 4.6707763671875
Validation loss: 2.795470037767964

Epoch: 20| Step: 0
Training loss: 3.009322166442871
Validation loss: 2.7951285377625497

Epoch: 6| Step: 1
Training loss: 2.54585599899292
Validation loss: 2.816414879214379

Epoch: 6| Step: 2
Training loss: 3.628774404525757
Validation loss: 2.8003262191690426

Epoch: 6| Step: 3
Training loss: 3.4941091537475586
Validation loss: 2.7951285249443463

Epoch: 6| Step: 4
Training loss: 2.230762481689453
Validation loss: 2.7789694827090026

Epoch: 6| Step: 5
Training loss: 2.6974964141845703
Validation loss: 2.7709732901665474

Epoch: 6| Step: 6
Training loss: 2.5761098861694336
Validation loss: 2.7678910404123287

Epoch: 6| Step: 7
Training loss: 2.2133846282958984
Validation loss: 2.773383773783202

Epoch: 6| Step: 8
Training loss: 3.841602087020874
Validation loss: 2.7756050325209096

Epoch: 6| Step: 9
Training loss: 2.312817096710205
Validation loss: 2.77516847015709

Epoch: 6| Step: 10
Training loss: 3.084864616394043
Validation loss: 2.802088619560324

Epoch: 6| Step: 11
Training loss: 2.54744029045105
Validation loss: 2.804381016762026

Epoch: 6| Step: 12
Training loss: 3.4545435905456543
Validation loss: 2.850882460994105

Epoch: 6| Step: 13
Training loss: 3.5012528896331787
Validation loss: 2.798104275939285

Epoch: 21| Step: 0
Training loss: 2.618525266647339
Validation loss: 2.7683128438970095

Epoch: 6| Step: 1
Training loss: 1.9696054458618164
Validation loss: 2.7636178616554505

Epoch: 6| Step: 2
Training loss: 3.3548431396484375
Validation loss: 2.771230951432259

Epoch: 6| Step: 3
Training loss: 2.994844913482666
Validation loss: 2.7824180074917373

Epoch: 6| Step: 4
Training loss: 2.9107179641723633
Validation loss: 2.789637137484807

Epoch: 6| Step: 5
Training loss: 2.637507200241089
Validation loss: 2.785668514108145

Epoch: 6| Step: 6
Training loss: 2.7418036460876465
Validation loss: 2.779131531715393

Epoch: 6| Step: 7
Training loss: 2.6704623699188232
Validation loss: 2.773593246295888

Epoch: 6| Step: 8
Training loss: 3.4478445053100586
Validation loss: 2.769871452803253

Epoch: 6| Step: 9
Training loss: 2.8071999549865723
Validation loss: 2.767437845148066

Epoch: 6| Step: 10
Training loss: 2.969837188720703
Validation loss: 2.76591327369854

Epoch: 6| Step: 11
Training loss: 3.7846288681030273
Validation loss: 2.7671499021591677

Epoch: 6| Step: 12
Training loss: 2.3222482204437256
Validation loss: 2.76819610082975

Epoch: 6| Step: 13
Training loss: 4.223251819610596
Validation loss: 2.769941855502385

Epoch: 22| Step: 0
Training loss: 2.8018240928649902
Validation loss: 2.7660395817090104

Epoch: 6| Step: 1
Training loss: 3.4572196006774902
Validation loss: 2.75940954044301

Epoch: 6| Step: 2
Training loss: 3.147186279296875
Validation loss: 2.763078312720022

Epoch: 6| Step: 3
Training loss: 2.337608575820923
Validation loss: 2.760979849805114

Epoch: 6| Step: 4
Training loss: 2.5127882957458496
Validation loss: 2.756021676524993

Epoch: 6| Step: 5
Training loss: 2.661351203918457
Validation loss: 2.763425883426461

Epoch: 6| Step: 6
Training loss: 3.5292420387268066
Validation loss: 2.759196727506576

Epoch: 6| Step: 7
Training loss: 3.272531032562256
Validation loss: 2.763371362481066

Epoch: 6| Step: 8
Training loss: 3.7918436527252197
Validation loss: 2.7581951105466453

Epoch: 6| Step: 9
Training loss: 2.523106575012207
Validation loss: 2.759364592131748

Epoch: 6| Step: 10
Training loss: 2.2414050102233887
Validation loss: 2.755142181150375

Epoch: 6| Step: 11
Training loss: 2.75970196723938
Validation loss: 2.7567819882464666

Epoch: 6| Step: 12
Training loss: 3.0076165199279785
Validation loss: 2.755063288955278

Epoch: 6| Step: 13
Training loss: 2.432176351547241
Validation loss: 2.756111378310829

Epoch: 23| Step: 0
Training loss: 2.464042901992798
Validation loss: 2.7551796231218564

Epoch: 6| Step: 1
Training loss: 3.5719871520996094
Validation loss: 2.7625501412217335

Epoch: 6| Step: 2
Training loss: 2.622971773147583
Validation loss: 2.7739765285163798

Epoch: 6| Step: 3
Training loss: 2.8892898559570312
Validation loss: 2.8577705249991467

Epoch: 6| Step: 4
Training loss: 2.4132959842681885
Validation loss: 2.9044070141289824

Epoch: 6| Step: 5
Training loss: 2.9602198600769043
Validation loss: 2.8569856382185415

Epoch: 6| Step: 6
Training loss: 3.7505645751953125
Validation loss: 2.7845238870190037

Epoch: 6| Step: 7
Training loss: 2.7015609741210938
Validation loss: 2.7517690222750426

Epoch: 6| Step: 8
Training loss: 3.1862454414367676
Validation loss: 2.7449004214297057

Epoch: 6| Step: 9
Training loss: 2.566864013671875
Validation loss: 2.7539009227547595

Epoch: 6| Step: 10
Training loss: 2.801973342895508
Validation loss: 2.780458027316678

Epoch: 6| Step: 11
Training loss: 2.3412880897521973
Validation loss: 2.816930058181927

Epoch: 6| Step: 12
Training loss: 3.2817091941833496
Validation loss: 2.813253577037524

Epoch: 6| Step: 13
Training loss: 3.6685192584991455
Validation loss: 2.7808977096311507

Epoch: 24| Step: 0
Training loss: 3.419358491897583
Validation loss: 2.7698068849502073

Epoch: 6| Step: 1
Training loss: 3.0502495765686035
Validation loss: 2.7594220561365925

Epoch: 6| Step: 2
Training loss: 3.1290245056152344
Validation loss: 2.755942847139092

Epoch: 6| Step: 3
Training loss: 2.5640974044799805
Validation loss: 2.757709600592172

Epoch: 6| Step: 4
Training loss: 3.2302587032318115
Validation loss: 2.7631797354708434

Epoch: 6| Step: 5
Training loss: 2.679853916168213
Validation loss: 2.761960542330178

Epoch: 6| Step: 6
Training loss: 3.183964252471924
Validation loss: 2.7645682750209684

Epoch: 6| Step: 7
Training loss: 2.7428479194641113
Validation loss: 2.7699633106108634

Epoch: 6| Step: 8
Training loss: 2.6722092628479004
Validation loss: 2.763218782281363

Epoch: 6| Step: 9
Training loss: 3.253662586212158
Validation loss: 2.7594693450517553

Epoch: 6| Step: 10
Training loss: 2.4119458198547363
Validation loss: 2.7570877049558904

Epoch: 6| Step: 11
Training loss: 3.5156970024108887
Validation loss: 2.7598195768171743

Epoch: 6| Step: 12
Training loss: 2.6561028957366943
Validation loss: 2.7631499126393306

Epoch: 6| Step: 13
Training loss: 1.6519109010696411
Validation loss: 2.7531024281696608

Epoch: 25| Step: 0
Training loss: 3.188380241394043
Validation loss: 2.745313980246103

Epoch: 6| Step: 1
Training loss: 3.180791139602661
Validation loss: 2.7402762187424528

Epoch: 6| Step: 2
Training loss: 2.5163915157318115
Validation loss: 2.737954383255333

Epoch: 6| Step: 3
Training loss: 3.2010912895202637
Validation loss: 2.735798566572128

Epoch: 6| Step: 4
Training loss: 2.538365125656128
Validation loss: 2.733228252780053

Epoch: 6| Step: 5
Training loss: 3.091554880142212
Validation loss: 2.7313048326840965

Epoch: 6| Step: 6
Training loss: 2.5002899169921875
Validation loss: 2.730830566857451

Epoch: 6| Step: 7
Training loss: 3.0504252910614014
Validation loss: 2.732477677765713

Epoch: 6| Step: 8
Training loss: 2.538623332977295
Validation loss: 2.732116524891187

Epoch: 6| Step: 9
Training loss: 2.7333450317382812
Validation loss: 2.7298660175774687

Epoch: 6| Step: 10
Training loss: 2.879220724105835
Validation loss: 2.731564352589269

Epoch: 6| Step: 11
Training loss: 2.643526554107666
Validation loss: 2.731226877499652

Epoch: 6| Step: 12
Training loss: 3.2102980613708496
Validation loss: 2.7310999003789758

Epoch: 6| Step: 13
Training loss: 3.408163547515869
Validation loss: 2.7255133659608903

Epoch: 26| Step: 0
Training loss: 4.059126853942871
Validation loss: 2.726152717426259

Epoch: 6| Step: 1
Training loss: 2.3318376541137695
Validation loss: 2.7287341125549807

Epoch: 6| Step: 2
Training loss: 2.6097583770751953
Validation loss: 2.726765012228361

Epoch: 6| Step: 3
Training loss: 2.8865010738372803
Validation loss: 2.7288533590173207

Epoch: 6| Step: 4
Training loss: 2.7872390747070312
Validation loss: 2.7271654272592194

Epoch: 6| Step: 5
Training loss: 2.71164608001709
Validation loss: 2.725416168089836

Epoch: 6| Step: 6
Training loss: 2.7905466556549072
Validation loss: 2.724360819785826

Epoch: 6| Step: 7
Training loss: 3.408341407775879
Validation loss: 2.7225451674512637

Epoch: 6| Step: 8
Training loss: 2.4098525047302246
Validation loss: 2.7259074282902542

Epoch: 6| Step: 9
Training loss: 2.767460346221924
Validation loss: 2.728948229102678

Epoch: 6| Step: 10
Training loss: 2.011983871459961
Validation loss: 2.728683540897985

Epoch: 6| Step: 11
Training loss: 3.2369067668914795
Validation loss: 2.731582736456266

Epoch: 6| Step: 12
Training loss: 3.1072988510131836
Validation loss: 2.727446389454667

Epoch: 6| Step: 13
Training loss: 3.5038034915924072
Validation loss: 2.734763471029138

Epoch: 27| Step: 0
Training loss: 2.579483985900879
Validation loss: 2.7284283791818926

Epoch: 6| Step: 1
Training loss: 2.9923315048217773
Validation loss: 2.7228263449925247

Epoch: 6| Step: 2
Training loss: 2.2160556316375732
Validation loss: 2.718736579341273

Epoch: 6| Step: 3
Training loss: 3.2292747497558594
Validation loss: 2.718142245405464

Epoch: 6| Step: 4
Training loss: 3.0295448303222656
Validation loss: 2.7203166895015265

Epoch: 6| Step: 5
Training loss: 3.893693447113037
Validation loss: 2.7172073471930718

Epoch: 6| Step: 6
Training loss: 3.151536226272583
Validation loss: 2.7164178817502913

Epoch: 6| Step: 7
Training loss: 2.7680230140686035
Validation loss: 2.7175596555074057

Epoch: 6| Step: 8
Training loss: 3.1366264820098877
Validation loss: 2.715515987847441

Epoch: 6| Step: 9
Training loss: 2.891939640045166
Validation loss: 2.7162082195281982

Epoch: 6| Step: 10
Training loss: 2.439620018005371
Validation loss: 2.7137879120406283

Epoch: 6| Step: 11
Training loss: 3.5161871910095215
Validation loss: 2.71100127825173

Epoch: 6| Step: 12
Training loss: 2.201843738555908
Validation loss: 2.7146875576306413

Epoch: 6| Step: 13
Training loss: 1.779460072517395
Validation loss: 2.7114740264031196

Epoch: 28| Step: 0
Training loss: 3.2938835620880127
Validation loss: 2.7132552874985563

Epoch: 6| Step: 1
Training loss: 2.5637106895446777
Validation loss: 2.7116101377753803

Epoch: 6| Step: 2
Training loss: 2.7427611351013184
Validation loss: 2.710816575634864

Epoch: 6| Step: 3
Training loss: 1.9442925453186035
Validation loss: 2.7114783845922

Epoch: 6| Step: 4
Training loss: 3.0312490463256836
Validation loss: 2.710381902674193

Epoch: 6| Step: 5
Training loss: 2.559166431427002
Validation loss: 2.709530538128268

Epoch: 6| Step: 6
Training loss: 3.1042237281799316
Validation loss: 2.706910414080466

Epoch: 6| Step: 7
Training loss: 2.4527597427368164
Validation loss: 2.7061449840504634

Epoch: 6| Step: 8
Training loss: 3.0869102478027344
Validation loss: 2.7074712502059115

Epoch: 6| Step: 9
Training loss: 3.469971179962158
Validation loss: 2.707150443907707

Epoch: 6| Step: 10
Training loss: 2.7460503578186035
Validation loss: 2.707860856927851

Epoch: 6| Step: 11
Training loss: 2.354127883911133
Validation loss: 2.708336853211926

Epoch: 6| Step: 12
Training loss: 3.884488105773926
Validation loss: 2.7111983376164592

Epoch: 6| Step: 13
Training loss: 3.016587734222412
Validation loss: 2.709298095395488

Epoch: 29| Step: 0
Training loss: 3.01241135597229
Validation loss: 2.71163002649943

Epoch: 6| Step: 1
Training loss: 3.152719736099243
Validation loss: 2.7131513113616617

Epoch: 6| Step: 2
Training loss: 3.4820756912231445
Validation loss: 2.7162516040186726

Epoch: 6| Step: 3
Training loss: 3.6495792865753174
Validation loss: 2.7164075656603743

Epoch: 6| Step: 4
Training loss: 2.531792640686035
Validation loss: 2.71772180065032

Epoch: 6| Step: 5
Training loss: 2.225003242492676
Validation loss: 2.716847781212099

Epoch: 6| Step: 6
Training loss: 2.3122568130493164
Validation loss: 2.720659222654117

Epoch: 6| Step: 7
Training loss: 3.430837869644165
Validation loss: 2.7133110723187848

Epoch: 6| Step: 8
Training loss: 2.710300922393799
Validation loss: 2.708782736973096

Epoch: 6| Step: 9
Training loss: 1.9476847648620605
Validation loss: 2.7046940762509584

Epoch: 6| Step: 10
Training loss: 2.8832876682281494
Validation loss: 2.705083429172475

Epoch: 6| Step: 11
Training loss: 2.637237787246704
Validation loss: 2.7017932297081075

Epoch: 6| Step: 12
Training loss: 4.155639171600342
Validation loss: 2.708395024781586

Epoch: 6| Step: 13
Training loss: 1.4216238260269165
Validation loss: 2.7055524138994116

Epoch: 30| Step: 0
Training loss: 3.5296101570129395
Validation loss: 2.706310749053955

Epoch: 6| Step: 1
Training loss: 2.4641847610473633
Validation loss: 2.705777193910332

Epoch: 6| Step: 2
Training loss: 2.954756259918213
Validation loss: 2.7058072243967364

Epoch: 6| Step: 3
Training loss: 3.2219059467315674
Validation loss: 2.7056450190082675

Epoch: 6| Step: 4
Training loss: 2.696206569671631
Validation loss: 2.7033430991634244

Epoch: 6| Step: 5
Training loss: 2.388777732849121
Validation loss: 2.7032649927241827

Epoch: 6| Step: 6
Training loss: 3.06973934173584
Validation loss: 2.7040892442067466

Epoch: 6| Step: 7
Training loss: 2.882153034210205
Validation loss: 2.7042175313477874

Epoch: 6| Step: 8
Training loss: 2.52994441986084
Validation loss: 2.706309464669997

Epoch: 6| Step: 9
Training loss: 2.7283406257629395
Validation loss: 2.7066850303321757

Epoch: 6| Step: 10
Training loss: 2.8157262802124023
Validation loss: 2.712757936087988

Epoch: 6| Step: 11
Training loss: 2.6684985160827637
Validation loss: 2.7068595219683904

Epoch: 6| Step: 12
Training loss: 3.3385424613952637
Validation loss: 2.704237384180869

Epoch: 6| Step: 13
Training loss: 2.834517478942871
Validation loss: 2.7017501092726186

Epoch: 31| Step: 0
Training loss: 2.7919697761535645
Validation loss: 2.6996754395064486

Epoch: 6| Step: 1
Training loss: 2.962350845336914
Validation loss: 2.6996988660545758

Epoch: 6| Step: 2
Training loss: 2.6573591232299805
Validation loss: 2.696741429708337

Epoch: 6| Step: 3
Training loss: 2.3457319736480713
Validation loss: 2.695812422742126

Epoch: 6| Step: 4
Training loss: 2.740774154663086
Validation loss: 2.700112214652441

Epoch: 6| Step: 5
Training loss: 3.4891552925109863
Validation loss: 2.698989406708748

Epoch: 6| Step: 6
Training loss: 3.15769100189209
Validation loss: 2.698719286149548

Epoch: 6| Step: 7
Training loss: 3.1691503524780273
Validation loss: 2.7013107217768186

Epoch: 6| Step: 8
Training loss: 2.895528554916382
Validation loss: 2.6980041047578216

Epoch: 6| Step: 9
Training loss: 2.9463350772857666
Validation loss: 2.6938948605650213

Epoch: 6| Step: 10
Training loss: 3.425279140472412
Validation loss: 2.6961566991703485

Epoch: 6| Step: 11
Training loss: 2.542720317840576
Validation loss: 2.6945658242830666

Epoch: 6| Step: 12
Training loss: 2.3211545944213867
Validation loss: 2.695070766633557

Epoch: 6| Step: 13
Training loss: 2.439570903778076
Validation loss: 2.7014422519232637

Epoch: 32| Step: 0
Training loss: 2.3670687675476074
Validation loss: 2.703882171261695

Epoch: 6| Step: 1
Training loss: 3.413241386413574
Validation loss: 2.7111345824374946

Epoch: 6| Step: 2
Training loss: 2.7437005043029785
Validation loss: 2.7117386300076722

Epoch: 6| Step: 3
Training loss: 2.249246835708618
Validation loss: 2.7172592378431752

Epoch: 6| Step: 4
Training loss: 2.658101797103882
Validation loss: 2.7366884446913198

Epoch: 6| Step: 5
Training loss: 3.84177827835083
Validation loss: 2.746101943395471

Epoch: 6| Step: 6
Training loss: 2.50565242767334
Validation loss: 2.718942621702789

Epoch: 6| Step: 7
Training loss: 2.2357535362243652
Validation loss: 2.7034852530366633

Epoch: 6| Step: 8
Training loss: 2.539581775665283
Validation loss: 2.6988040965090514

Epoch: 6| Step: 9
Training loss: 3.2816786766052246
Validation loss: 2.699286881313529

Epoch: 6| Step: 10
Training loss: 3.1041531562805176
Validation loss: 2.6973213329110095

Epoch: 6| Step: 11
Training loss: 2.710391044616699
Validation loss: 2.699107182923184

Epoch: 6| Step: 12
Training loss: 3.498382568359375
Validation loss: 2.698967238908173

Epoch: 6| Step: 13
Training loss: 2.9630823135375977
Validation loss: 2.6967271861209663

Epoch: 33| Step: 0
Training loss: 3.2315990924835205
Validation loss: 2.6930036160253708

Epoch: 6| Step: 1
Training loss: 2.994607448577881
Validation loss: 2.6896428395343084

Epoch: 6| Step: 2
Training loss: 2.791731834411621
Validation loss: 2.6967156112834973

Epoch: 6| Step: 3
Training loss: 2.6734187602996826
Validation loss: 2.6908536136791272

Epoch: 6| Step: 4
Training loss: 2.659594774246216
Validation loss: 2.69901934746773

Epoch: 6| Step: 5
Training loss: 3.0230541229248047
Validation loss: 2.7207904067090762

Epoch: 6| Step: 6
Training loss: 2.3899881839752197
Validation loss: 2.7392997639153593

Epoch: 6| Step: 7
Training loss: 1.9378881454467773
Validation loss: 2.7375433752613683

Epoch: 6| Step: 8
Training loss: 2.891051769256592
Validation loss: 2.743928773428804

Epoch: 6| Step: 9
Training loss: 3.535146951675415
Validation loss: 2.7389355090356644

Epoch: 6| Step: 10
Training loss: 3.099858283996582
Validation loss: 2.710743206803517

Epoch: 6| Step: 11
Training loss: 2.6631364822387695
Validation loss: 2.69767157236735

Epoch: 6| Step: 12
Training loss: 3.319056510925293
Validation loss: 2.692823020360803

Epoch: 6| Step: 13
Training loss: 3.0881052017211914
Validation loss: 2.7052632044720393

Epoch: 34| Step: 0
Training loss: 2.087670087814331
Validation loss: 2.7161393037406345

Epoch: 6| Step: 1
Training loss: 3.374434471130371
Validation loss: 2.704092912776496

Epoch: 6| Step: 2
Training loss: 3.1780738830566406
Validation loss: 2.6896486102893786

Epoch: 6| Step: 3
Training loss: 2.8078114986419678
Validation loss: 2.6874714256614767

Epoch: 6| Step: 4
Training loss: 2.6687817573547363
Validation loss: 2.686053273498371

Epoch: 6| Step: 5
Training loss: 1.9616148471832275
Validation loss: 2.688683545717629

Epoch: 6| Step: 6
Training loss: 2.40126371383667
Validation loss: 2.6915705357828448

Epoch: 6| Step: 7
Training loss: 3.488459348678589
Validation loss: 2.6977440772518033

Epoch: 6| Step: 8
Training loss: 3.2382543087005615
Validation loss: 2.7002929384990404

Epoch: 6| Step: 9
Training loss: 3.5417027473449707
Validation loss: 2.6998351799544467

Epoch: 6| Step: 10
Training loss: 2.7459349632263184
Validation loss: 2.708052312174151

Epoch: 6| Step: 11
Training loss: 3.064964771270752
Validation loss: 2.7030368107621388

Epoch: 6| Step: 12
Training loss: 2.694166898727417
Validation loss: 2.701589492059523

Epoch: 6| Step: 13
Training loss: 2.496760368347168
Validation loss: 2.6979054558661675

Epoch: 35| Step: 0
Training loss: 2.847846508026123
Validation loss: 2.699580546348326

Epoch: 6| Step: 1
Training loss: 1.977183222770691
Validation loss: 2.699614655586981

Epoch: 6| Step: 2
Training loss: 3.0154240131378174
Validation loss: 2.698543979275611

Epoch: 6| Step: 3
Training loss: 2.9455082416534424
Validation loss: 2.6979380679386917

Epoch: 6| Step: 4
Training loss: 3.049330711364746
Validation loss: 2.693128414051507

Epoch: 6| Step: 5
Training loss: 3.554842948913574
Validation loss: 2.694218220249299

Epoch: 6| Step: 6
Training loss: 3.037046432495117
Validation loss: 2.692360434480893

Epoch: 6| Step: 7
Training loss: 3.2698450088500977
Validation loss: 2.6952065472961753

Epoch: 6| Step: 8
Training loss: 2.033271551132202
Validation loss: 2.701263145733905

Epoch: 6| Step: 9
Training loss: 2.5251522064208984
Validation loss: 2.7036213644089235

Epoch: 6| Step: 10
Training loss: 2.5115621089935303
Validation loss: 2.6958119792322957

Epoch: 6| Step: 11
Training loss: 2.676978826522827
Validation loss: 2.68478589160468

Epoch: 6| Step: 12
Training loss: 3.1112279891967773
Validation loss: 2.68227147030574

Epoch: 6| Step: 13
Training loss: 3.5950045585632324
Validation loss: 2.682658613369029

Epoch: 36| Step: 0
Training loss: 2.330530881881714
Validation loss: 2.6827609231395106

Epoch: 6| Step: 1
Training loss: 2.5659825801849365
Validation loss: 2.684353108047157

Epoch: 6| Step: 2
Training loss: 3.5397543907165527
Validation loss: 2.6826662401999197

Epoch: 6| Step: 3
Training loss: 2.6094565391540527
Validation loss: 2.6801566180362495

Epoch: 6| Step: 4
Training loss: 2.966360092163086
Validation loss: 2.685139035665861

Epoch: 6| Step: 5
Training loss: 3.4930474758148193
Validation loss: 2.6757445104660524

Epoch: 6| Step: 6
Training loss: 2.435793876647949
Validation loss: 2.6744355129939255

Epoch: 6| Step: 7
Training loss: 2.681525707244873
Validation loss: 2.6752750078837075

Epoch: 6| Step: 8
Training loss: 2.7608208656311035
Validation loss: 2.673523320946642

Epoch: 6| Step: 9
Training loss: 3.4382739067077637
Validation loss: 2.671704976789413

Epoch: 6| Step: 10
Training loss: 2.603257656097412
Validation loss: 2.6757543676642963

Epoch: 6| Step: 11
Training loss: 3.407456636428833
Validation loss: 2.6722237499811317

Epoch: 6| Step: 12
Training loss: 2.2525553703308105
Validation loss: 2.672861494043822

Epoch: 6| Step: 13
Training loss: 2.3983192443847656
Validation loss: 2.6743517178361134

Epoch: 37| Step: 0
Training loss: 2.1773085594177246
Validation loss: 2.672589963482272

Epoch: 6| Step: 1
Training loss: 3.081151247024536
Validation loss: 2.6715908999084146

Epoch: 6| Step: 2
Training loss: 3.770930290222168
Validation loss: 2.6653819827623266

Epoch: 6| Step: 3
Training loss: 2.189221143722534
Validation loss: 2.6744992656092488

Epoch: 6| Step: 4
Training loss: 3.8419671058654785
Validation loss: 2.670606618286461

Epoch: 6| Step: 5
Training loss: 2.4465417861938477
Validation loss: 2.671851852888702

Epoch: 6| Step: 6
Training loss: 2.455378532409668
Validation loss: 2.669360124936668

Epoch: 6| Step: 7
Training loss: 2.886399269104004
Validation loss: 2.669089392949176

Epoch: 6| Step: 8
Training loss: 3.1249709129333496
Validation loss: 2.6727589945639334

Epoch: 6| Step: 9
Training loss: 2.471162796020508
Validation loss: 2.673337080145395

Epoch: 6| Step: 10
Training loss: 3.026585817337036
Validation loss: 2.674472665274015

Epoch: 6| Step: 11
Training loss: 3.151078701019287
Validation loss: 2.6728005255422285

Epoch: 6| Step: 12
Training loss: 2.6559603214263916
Validation loss: 2.6769756219720326

Epoch: 6| Step: 13
Training loss: 1.9561337232589722
Validation loss: 2.6770638855554725

Epoch: 38| Step: 0
Training loss: 2.8610920906066895
Validation loss: 2.677109287631127

Epoch: 6| Step: 1
Training loss: 2.134862184524536
Validation loss: 2.67898129904142

Epoch: 6| Step: 2
Training loss: 2.920055389404297
Validation loss: 2.6780436269698606

Epoch: 6| Step: 3
Training loss: 3.1160457134246826
Validation loss: 2.6816802614478656

Epoch: 6| Step: 4
Training loss: 2.4912238121032715
Validation loss: 2.6817976761889715

Epoch: 6| Step: 5
Training loss: 3.4936397075653076
Validation loss: 2.6800077038426555

Epoch: 6| Step: 6
Training loss: 3.3162803649902344
Validation loss: 2.6828755178759174

Epoch: 6| Step: 7
Training loss: 2.4689388275146484
Validation loss: 2.675985182485273

Epoch: 6| Step: 8
Training loss: 2.817351818084717
Validation loss: 2.68234339580741

Epoch: 6| Step: 9
Training loss: 2.841841697692871
Validation loss: 2.679742028636317

Epoch: 6| Step: 10
Training loss: 2.835019588470459
Validation loss: 2.6852960432729414

Epoch: 6| Step: 11
Training loss: 2.4954280853271484
Validation loss: 2.6892895314001266

Epoch: 6| Step: 12
Training loss: 2.7433199882507324
Validation loss: 2.6955587248648367

Epoch: 6| Step: 13
Training loss: 3.2654924392700195
Validation loss: 2.6842808518358456

Epoch: 39| Step: 0
Training loss: 3.4600582122802734
Validation loss: 2.6788885337050243

Epoch: 6| Step: 1
Training loss: 2.512216091156006
Validation loss: 2.6736806105541926

Epoch: 6| Step: 2
Training loss: 2.928175687789917
Validation loss: 2.671398403824017

Epoch: 6| Step: 3
Training loss: 2.6018190383911133
Validation loss: 2.6678875877011206

Epoch: 6| Step: 4
Training loss: 3.0941264629364014
Validation loss: 2.6677162288337626

Epoch: 6| Step: 5
Training loss: 3.0363101959228516
Validation loss: 2.6675647304904078

Epoch: 6| Step: 6
Training loss: 2.7265565395355225
Validation loss: 2.6664201085285475

Epoch: 6| Step: 7
Training loss: 2.608586549758911
Validation loss: 2.66317154002446

Epoch: 6| Step: 8
Training loss: 2.338951349258423
Validation loss: 2.666760693314255

Epoch: 6| Step: 9
Training loss: 2.5767455101013184
Validation loss: 2.672741841244441

Epoch: 6| Step: 10
Training loss: 3.2734527587890625
Validation loss: 2.678143178263018

Epoch: 6| Step: 11
Training loss: 3.0629334449768066
Validation loss: 2.6721110754115607

Epoch: 6| Step: 12
Training loss: 2.7313754558563232
Validation loss: 2.662083943684896

Epoch: 6| Step: 13
Training loss: 2.4830894470214844
Validation loss: 2.65980072944395

Epoch: 40| Step: 0
Training loss: 2.023061752319336
Validation loss: 2.6612768583400275

Epoch: 6| Step: 1
Training loss: 3.617063522338867
Validation loss: 2.6609778583690686

Epoch: 6| Step: 2
Training loss: 2.2652955055236816
Validation loss: 2.6708271067629576

Epoch: 6| Step: 3
Training loss: 3.246086835861206
Validation loss: 2.6834063376149824

Epoch: 6| Step: 4
Training loss: 3.4989492893218994
Validation loss: 2.7061130462154264

Epoch: 6| Step: 5
Training loss: 2.3093974590301514
Validation loss: 2.7272038280322985

Epoch: 6| Step: 6
Training loss: 2.9954042434692383
Validation loss: 2.755330603609803

Epoch: 6| Step: 7
Training loss: 3.369723081588745
Validation loss: 2.7480636258279123

Epoch: 6| Step: 8
Training loss: 2.9240589141845703
Validation loss: 2.7426486835684827

Epoch: 6| Step: 9
Training loss: 2.03082275390625
Validation loss: 2.7526005262969644

Epoch: 6| Step: 10
Training loss: 2.255303382873535
Validation loss: 2.7275303948310112

Epoch: 6| Step: 11
Training loss: 3.1348764896392822
Validation loss: 2.703358316934237

Epoch: 6| Step: 12
Training loss: 3.2348179817199707
Validation loss: 2.6919737938911683

Epoch: 6| Step: 13
Training loss: 2.5910444259643555
Validation loss: 2.6843596478944183

Epoch: 41| Step: 0
Training loss: 2.8658246994018555
Validation loss: 2.6732062139818744

Epoch: 6| Step: 1
Training loss: 3.6793813705444336
Validation loss: 2.6563254479439027

Epoch: 6| Step: 2
Training loss: 3.070270538330078
Validation loss: 2.655357200612304

Epoch: 6| Step: 3
Training loss: 2.361870050430298
Validation loss: 2.676053718854022

Epoch: 6| Step: 4
Training loss: 2.8878819942474365
Validation loss: 2.6898794328012774

Epoch: 6| Step: 5
Training loss: 2.3226046562194824
Validation loss: 2.6998520000006563

Epoch: 6| Step: 6
Training loss: 3.4264633655548096
Validation loss: 2.7089843416726715

Epoch: 6| Step: 7
Training loss: 1.7482621669769287
Validation loss: 2.701803094597273

Epoch: 6| Step: 8
Training loss: 2.6364521980285645
Validation loss: 2.688274029762514

Epoch: 6| Step: 9
Training loss: 2.481088638305664
Validation loss: 2.683943630546652

Epoch: 6| Step: 10
Training loss: 3.1589293479919434
Validation loss: 2.6738899856485348

Epoch: 6| Step: 11
Training loss: 3.1375155448913574
Validation loss: 2.660276436036633

Epoch: 6| Step: 12
Training loss: 2.6556708812713623
Validation loss: 2.6510950032100884

Epoch: 6| Step: 13
Training loss: 3.712773323059082
Validation loss: 2.6560750084538616

Epoch: 42| Step: 0
Training loss: 2.641331911087036
Validation loss: 2.6701960666205293

Epoch: 6| Step: 1
Training loss: 2.4602527618408203
Validation loss: 2.666963469597601

Epoch: 6| Step: 2
Training loss: 2.1212732791900635
Validation loss: 2.6727219063748597

Epoch: 6| Step: 3
Training loss: 1.839709758758545
Validation loss: 2.6729166507720947

Epoch: 6| Step: 4
Training loss: 2.676417350769043
Validation loss: 2.6853752495140157

Epoch: 6| Step: 5
Training loss: 3.18241286277771
Validation loss: 2.683927136082803

Epoch: 6| Step: 6
Training loss: 3.0846338272094727
Validation loss: 2.6708663202101186

Epoch: 6| Step: 7
Training loss: 2.87807035446167
Validation loss: 2.657325583119546

Epoch: 6| Step: 8
Training loss: 2.229280710220337
Validation loss: 2.6514649621901976

Epoch: 6| Step: 9
Training loss: 2.3067140579223633
Validation loss: 2.64788410996878

Epoch: 6| Step: 10
Training loss: 3.177591323852539
Validation loss: 2.6444972240796654

Epoch: 6| Step: 11
Training loss: 4.499941349029541
Validation loss: 2.636640159032678

Epoch: 6| Step: 12
Training loss: 3.663306713104248
Validation loss: 2.6417085868056103

Epoch: 6| Step: 13
Training loss: 2.4634666442871094
Validation loss: 2.6376263531305457

Epoch: 43| Step: 0
Training loss: 2.442519426345825
Validation loss: 2.6341162138087775

Epoch: 6| Step: 1
Training loss: 3.0117132663726807
Validation loss: 2.634470103889383

Epoch: 6| Step: 2
Training loss: 2.842210292816162
Validation loss: 2.637387793551209

Epoch: 6| Step: 3
Training loss: 2.662463426589966
Validation loss: 2.633555184128464

Epoch: 6| Step: 4
Training loss: 2.5632107257843018
Validation loss: 2.6339507436239593

Epoch: 6| Step: 5
Training loss: 2.761794090270996
Validation loss: 2.633091457428471

Epoch: 6| Step: 6
Training loss: 2.5064537525177
Validation loss: 2.6330830461235455

Epoch: 6| Step: 7
Training loss: 3.320934534072876
Validation loss: 2.633789582919049

Epoch: 6| Step: 8
Training loss: 3.064523696899414
Validation loss: 2.630900429141137

Epoch: 6| Step: 9
Training loss: 3.2859911918640137
Validation loss: 2.6312289648158576

Epoch: 6| Step: 10
Training loss: 3.1610124111175537
Validation loss: 2.633889962268132

Epoch: 6| Step: 11
Training loss: 2.409130811691284
Validation loss: 2.6315395421879266

Epoch: 6| Step: 12
Training loss: 2.509814977645874
Validation loss: 2.6361710743237565

Epoch: 6| Step: 13
Training loss: 2.479250907897949
Validation loss: 2.6390837571954213

Epoch: 44| Step: 0
Training loss: 2.6077582836151123
Validation loss: 2.633102532356016

Epoch: 6| Step: 1
Training loss: 2.857193946838379
Validation loss: 2.6396362807161067

Epoch: 6| Step: 2
Training loss: 2.9622750282287598
Validation loss: 2.636798684315015

Epoch: 6| Step: 3
Training loss: 3.0481810569763184
Validation loss: 2.627374167083412

Epoch: 6| Step: 4
Training loss: 2.5813963413238525
Validation loss: 2.6250696951343166

Epoch: 6| Step: 5
Training loss: 3.09100079536438
Validation loss: 2.62549663102755

Epoch: 6| Step: 6
Training loss: 3.3797693252563477
Validation loss: 2.622948210726502

Epoch: 6| Step: 7
Training loss: 2.434446096420288
Validation loss: 2.6213158792065037

Epoch: 6| Step: 8
Training loss: 2.940297842025757
Validation loss: 2.6195551656907603

Epoch: 6| Step: 9
Training loss: 2.0790505409240723
Validation loss: 2.6209985492050007

Epoch: 6| Step: 10
Training loss: 3.125771999359131
Validation loss: 2.6179839590544343

Epoch: 6| Step: 11
Training loss: 3.3290939331054688
Validation loss: 2.623923355533231

Epoch: 6| Step: 12
Training loss: 2.0638906955718994
Validation loss: 2.6209174330516527

Epoch: 6| Step: 13
Training loss: 2.47434139251709
Validation loss: 2.6216891170829855

Epoch: 45| Step: 0
Training loss: 3.4361495971679688
Validation loss: 2.6189890318019415

Epoch: 6| Step: 1
Training loss: 2.5177993774414062
Validation loss: 2.61770938545145

Epoch: 6| Step: 2
Training loss: 2.5010433197021484
Validation loss: 2.616437699205132

Epoch: 6| Step: 3
Training loss: 2.1647696495056152
Validation loss: 2.6206901970730034

Epoch: 6| Step: 4
Training loss: 2.128039598464966
Validation loss: 2.6253114464462444

Epoch: 6| Step: 5
Training loss: 2.3005447387695312
Validation loss: 2.625554851306382

Epoch: 6| Step: 6
Training loss: 2.5323293209075928
Validation loss: 2.630598319474087

Epoch: 6| Step: 7
Training loss: 2.7885379791259766
Validation loss: 2.637551164114347

Epoch: 6| Step: 8
Training loss: 2.966362476348877
Validation loss: 2.6293008699212024

Epoch: 6| Step: 9
Training loss: 3.1226019859313965
Validation loss: 2.626364682310371

Epoch: 6| Step: 10
Training loss: 3.680860757827759
Validation loss: 2.6233861036198114

Epoch: 6| Step: 11
Training loss: 2.812868356704712
Validation loss: 2.620939588034025

Epoch: 6| Step: 12
Training loss: 3.3571159839630127
Validation loss: 2.615702341961604

Epoch: 6| Step: 13
Training loss: 2.6534318923950195
Validation loss: 2.616569244733421

Epoch: 46| Step: 0
Training loss: 3.538285255432129
Validation loss: 2.615313455622683

Epoch: 6| Step: 1
Training loss: 4.004435062408447
Validation loss: 2.6123108363920644

Epoch: 6| Step: 2
Training loss: 2.4478681087493896
Validation loss: 2.6148509979248047

Epoch: 6| Step: 3
Training loss: 2.151089668273926
Validation loss: 2.6154082513624624

Epoch: 6| Step: 4
Training loss: 2.4782016277313232
Validation loss: 2.612732559122065

Epoch: 6| Step: 5
Training loss: 2.910071849822998
Validation loss: 2.610535906207177

Epoch: 6| Step: 6
Training loss: 2.537062644958496
Validation loss: 2.611433570102979

Epoch: 6| Step: 7
Training loss: 3.694365978240967
Validation loss: 2.610058940866942

Epoch: 6| Step: 8
Training loss: 2.239779472351074
Validation loss: 2.6120253275799494

Epoch: 6| Step: 9
Training loss: 2.5572822093963623
Validation loss: 2.610405691208378

Epoch: 6| Step: 10
Training loss: 2.5806357860565186
Validation loss: 2.609410934550788

Epoch: 6| Step: 11
Training loss: 2.1790475845336914
Validation loss: 2.60858985429169

Epoch: 6| Step: 12
Training loss: 2.934385299682617
Validation loss: 2.6119855347500054

Epoch: 6| Step: 13
Training loss: 2.6310083866119385
Validation loss: 2.6092246527312906

Epoch: 47| Step: 0
Training loss: 2.453915596008301
Validation loss: 2.6126150854172243

Epoch: 6| Step: 1
Training loss: 2.2285358905792236
Validation loss: 2.608968183558474

Epoch: 6| Step: 2
Training loss: 2.445185899734497
Validation loss: 2.6139752659746396

Epoch: 6| Step: 3
Training loss: 3.135260581970215
Validation loss: 2.6132886050849833

Epoch: 6| Step: 4
Training loss: 2.5743885040283203
Validation loss: 2.6190932361028527

Epoch: 6| Step: 5
Training loss: 2.5048599243164062
Validation loss: 2.6204674090108564

Epoch: 6| Step: 6
Training loss: 3.115837574005127
Validation loss: 2.62292161808219

Epoch: 6| Step: 7
Training loss: 3.289167642593384
Validation loss: 2.6246321098778838

Epoch: 6| Step: 8
Training loss: 2.4213857650756836
Validation loss: 2.621424118677775

Epoch: 6| Step: 9
Training loss: 2.299283504486084
Validation loss: 2.613453334377658

Epoch: 6| Step: 10
Training loss: 3.2515597343444824
Validation loss: 2.610301681744155

Epoch: 6| Step: 11
Training loss: 2.646700143814087
Validation loss: 2.6054263179020216

Epoch: 6| Step: 12
Training loss: 3.2810192108154297
Validation loss: 2.6032124719312115

Epoch: 6| Step: 13
Training loss: 3.5280685424804688
Validation loss: 2.604953917123938

Epoch: 48| Step: 0
Training loss: 2.811117649078369
Validation loss: 2.605870290469098

Epoch: 6| Step: 1
Training loss: 1.822816014289856
Validation loss: 2.6037754448511268

Epoch: 6| Step: 2
Training loss: 2.801187515258789
Validation loss: 2.6134612355180966

Epoch: 6| Step: 3
Training loss: 3.4590022563934326
Validation loss: 2.614647473058393

Epoch: 6| Step: 4
Training loss: 2.3894715309143066
Validation loss: 2.619765402168356

Epoch: 6| Step: 5
Training loss: 2.586951494216919
Validation loss: 2.626883145301573

Epoch: 6| Step: 6
Training loss: 2.3835926055908203
Validation loss: 2.6267868754684285

Epoch: 6| Step: 7
Training loss: 2.9766368865966797
Validation loss: 2.6168061866555163

Epoch: 6| Step: 8
Training loss: 2.4331464767456055
Validation loss: 2.604468176441808

Epoch: 6| Step: 9
Training loss: 3.9129252433776855
Validation loss: 2.59962208296663

Epoch: 6| Step: 10
Training loss: 2.5649800300598145
Validation loss: 2.5996764065116964

Epoch: 6| Step: 11
Training loss: 2.468173027038574
Validation loss: 2.595111347013904

Epoch: 6| Step: 12
Training loss: 2.9515464305877686
Validation loss: 2.5985863875317317

Epoch: 6| Step: 13
Training loss: 3.484879493713379
Validation loss: 2.598213682892502

Epoch: 49| Step: 0
Training loss: 2.565309524536133
Validation loss: 2.6007822072634132

Epoch: 6| Step: 1
Training loss: 1.9719877243041992
Validation loss: 2.6008996796864334

Epoch: 6| Step: 2
Training loss: 2.703503370285034
Validation loss: 2.601236107528851

Epoch: 6| Step: 3
Training loss: 3.4614789485931396
Validation loss: 2.6004209313341367

Epoch: 6| Step: 4
Training loss: 2.103618621826172
Validation loss: 2.6006040368028867

Epoch: 6| Step: 5
Training loss: 3.482849597930908
Validation loss: 2.5988757123229322

Epoch: 6| Step: 6
Training loss: 2.713700771331787
Validation loss: 2.5991144487934728

Epoch: 6| Step: 7
Training loss: 2.9023327827453613
Validation loss: 2.59792431195577

Epoch: 6| Step: 8
Training loss: 2.8750863075256348
Validation loss: 2.595926361699258

Epoch: 6| Step: 9
Training loss: 2.4021801948547363
Validation loss: 2.594222858387937

Epoch: 6| Step: 10
Training loss: 3.189042091369629
Validation loss: 2.5976533018132693

Epoch: 6| Step: 11
Training loss: 3.5346016883850098
Validation loss: 2.594220035819597

Epoch: 6| Step: 12
Training loss: 2.4978890419006348
Validation loss: 2.5990489067569857

Epoch: 6| Step: 13
Training loss: 2.0455641746520996
Validation loss: 2.601268991347282

Epoch: 50| Step: 0
Training loss: 2.864027500152588
Validation loss: 2.6074766587185603

Epoch: 6| Step: 1
Training loss: 2.9909284114837646
Validation loss: 2.607954881524527

Epoch: 6| Step: 2
Training loss: 2.545100450515747
Validation loss: 2.609860920136975

Epoch: 6| Step: 3
Training loss: 2.5766053199768066
Validation loss: 2.609043990412066

Epoch: 6| Step: 4
Training loss: 2.6154468059539795
Validation loss: 2.610178020692641

Epoch: 6| Step: 5
Training loss: 3.2652153968811035
Validation loss: 2.6059482148898545

Epoch: 6| Step: 6
Training loss: 2.6595754623413086
Validation loss: 2.5976444316166702

Epoch: 6| Step: 7
Training loss: 3.0086259841918945
Validation loss: 2.592262127066171

Epoch: 6| Step: 8
Training loss: 2.658830165863037
Validation loss: 2.5931726809470885

Epoch: 6| Step: 9
Training loss: 3.091015100479126
Validation loss: 2.5920675262328117

Epoch: 6| Step: 10
Training loss: 1.708977222442627
Validation loss: 2.595352549706736

Epoch: 6| Step: 11
Training loss: 2.6613805294036865
Validation loss: 2.5973843733469644

Epoch: 6| Step: 12
Training loss: 3.309339761734009
Validation loss: 2.5995970515794653

Epoch: 6| Step: 13
Training loss: 2.695631980895996
Validation loss: 2.602607291231873

Epoch: 51| Step: 0
Training loss: 2.7163867950439453
Validation loss: 2.6027713488507014

Epoch: 6| Step: 1
Training loss: 2.5521936416625977
Validation loss: 2.6110946132290747

Epoch: 6| Step: 2
Training loss: 2.522148370742798
Validation loss: 2.601861571752897

Epoch: 6| Step: 3
Training loss: 3.281050205230713
Validation loss: 2.596707054363784

Epoch: 6| Step: 4
Training loss: 2.750217914581299
Validation loss: 2.5918443254245225

Epoch: 6| Step: 5
Training loss: 2.8507866859436035
Validation loss: 2.5951070118975896

Epoch: 6| Step: 6
Training loss: 3.7011771202087402
Validation loss: 2.5994644165039062

Epoch: 6| Step: 7
Training loss: 2.3505361080169678
Validation loss: 2.5882095393314155

Epoch: 6| Step: 8
Training loss: 3.074450969696045
Validation loss: 2.587004353923182

Epoch: 6| Step: 9
Training loss: 2.0275282859802246
Validation loss: 2.591857389737201

Epoch: 6| Step: 10
Training loss: 2.912883758544922
Validation loss: 2.5977034543150213

Epoch: 6| Step: 11
Training loss: 3.1876699924468994
Validation loss: 2.588433993759976

Epoch: 6| Step: 12
Training loss: 2.2194676399230957
Validation loss: 2.5831431881073983

Epoch: 6| Step: 13
Training loss: 2.3831117153167725
Validation loss: 2.584469049207626

Epoch: 52| Step: 0
Training loss: 3.1710050106048584
Validation loss: 2.5878016410335416

Epoch: 6| Step: 1
Training loss: 2.4863338470458984
Validation loss: 2.590026229940435

Epoch: 6| Step: 2
Training loss: 2.900038957595825
Validation loss: 2.5953316996174474

Epoch: 6| Step: 3
Training loss: 2.7489025592803955
Validation loss: 2.6004230412103797

Epoch: 6| Step: 4
Training loss: 3.0276126861572266
Validation loss: 2.61034801185772

Epoch: 6| Step: 5
Training loss: 2.499927282333374
Validation loss: 2.6130167848320416

Epoch: 6| Step: 6
Training loss: 3.2062907218933105
Validation loss: 2.6084598392568608

Epoch: 6| Step: 7
Training loss: 2.729860544204712
Validation loss: 2.601512009097684

Epoch: 6| Step: 8
Training loss: 2.565471649169922
Validation loss: 2.5900845707103772

Epoch: 6| Step: 9
Training loss: 1.930665373802185
Validation loss: 2.5873049151512886

Epoch: 6| Step: 10
Training loss: 3.028066873550415
Validation loss: 2.5880575872236684

Epoch: 6| Step: 11
Training loss: 3.3151912689208984
Validation loss: 2.577951467165383

Epoch: 6| Step: 12
Training loss: 2.180102586746216
Validation loss: 2.5793552449954453

Epoch: 6| Step: 13
Training loss: 2.7966525554656982
Validation loss: 2.57583716864227

Epoch: 53| Step: 0
Training loss: 2.639265537261963
Validation loss: 2.5758405270115023

Epoch: 6| Step: 1
Training loss: 2.914910316467285
Validation loss: 2.575603110815889

Epoch: 6| Step: 2
Training loss: 2.381962776184082
Validation loss: 2.5803551725161973

Epoch: 6| Step: 3
Training loss: 3.04226016998291
Validation loss: 2.585573414320587

Epoch: 6| Step: 4
Training loss: 3.1036629676818848
Validation loss: 2.5803086962751163

Epoch: 6| Step: 5
Training loss: 2.0951199531555176
Validation loss: 2.585414719837968

Epoch: 6| Step: 6
Training loss: 2.5956194400787354
Validation loss: 2.5835317642458024

Epoch: 6| Step: 7
Training loss: 2.353332996368408
Validation loss: 2.581543496859971

Epoch: 6| Step: 8
Training loss: 2.7879018783569336
Validation loss: 2.57996783461622

Epoch: 6| Step: 9
Training loss: 3.159132480621338
Validation loss: 2.5784705915758686

Epoch: 6| Step: 10
Training loss: 3.3484370708465576
Validation loss: 2.577913661156931

Epoch: 6| Step: 11
Training loss: 3.744734525680542
Validation loss: 2.5772387263595418

Epoch: 6| Step: 12
Training loss: 2.6224937438964844
Validation loss: 2.57352098085547

Epoch: 6| Step: 13
Training loss: 1.1233129501342773
Validation loss: 2.57380675244075

Epoch: 54| Step: 0
Training loss: 3.5228943824768066
Validation loss: 2.574406031639345

Epoch: 6| Step: 1
Training loss: 3.9667036533355713
Validation loss: 2.581203370965937

Epoch: 6| Step: 2
Training loss: 2.4148879051208496
Validation loss: 2.590005087596114

Epoch: 6| Step: 3
Training loss: 2.386709213256836
Validation loss: 2.588490237471878

Epoch: 6| Step: 4
Training loss: 2.648653030395508
Validation loss: 2.5950103959729596

Epoch: 6| Step: 5
Training loss: 2.8990819454193115
Validation loss: 2.607994866627519

Epoch: 6| Step: 6
Training loss: 1.9807243347167969
Validation loss: 2.606249342682541

Epoch: 6| Step: 7
Training loss: 2.069708824157715
Validation loss: 2.6146161581880305

Epoch: 6| Step: 8
Training loss: 3.075988531112671
Validation loss: 2.61514800594699

Epoch: 6| Step: 9
Training loss: 2.1531713008880615
Validation loss: 2.600935700119183

Epoch: 6| Step: 10
Training loss: 2.5364363193511963
Validation loss: 2.589478003081455

Epoch: 6| Step: 11
Training loss: 3.0710394382476807
Validation loss: 2.583855544367144

Epoch: 6| Step: 12
Training loss: 2.9871788024902344
Validation loss: 2.575526855325186

Epoch: 6| Step: 13
Training loss: 2.7892355918884277
Validation loss: 2.5671015195949103

Epoch: 55| Step: 0
Training loss: 2.088397741317749
Validation loss: 2.5657458869359826

Epoch: 6| Step: 1
Training loss: 2.872300386428833
Validation loss: 2.568403661891978

Epoch: 6| Step: 2
Training loss: 2.749350070953369
Validation loss: 2.5705095978193384

Epoch: 6| Step: 3
Training loss: 2.507777690887451
Validation loss: 2.5709168295706473

Epoch: 6| Step: 4
Training loss: 3.0336380004882812
Validation loss: 2.5715498180799585

Epoch: 6| Step: 5
Training loss: 2.757786989212036
Validation loss: 2.5728589591159614

Epoch: 6| Step: 6
Training loss: 3.2179348468780518
Validation loss: 2.5704018121124594

Epoch: 6| Step: 7
Training loss: 2.963569402694702
Validation loss: 2.5745689920199815

Epoch: 6| Step: 8
Training loss: 2.9008145332336426
Validation loss: 2.5719051925084924

Epoch: 6| Step: 9
Training loss: 3.1460981369018555
Validation loss: 2.571351258985458

Epoch: 6| Step: 10
Training loss: 3.3319082260131836
Validation loss: 2.569102574420232

Epoch: 6| Step: 11
Training loss: 2.3429582118988037
Validation loss: 2.5750667125948015

Epoch: 6| Step: 12
Training loss: 2.042302370071411
Validation loss: 2.5719990730285645

Epoch: 6| Step: 13
Training loss: 2.439992666244507
Validation loss: 2.568704620484383

Epoch: 56| Step: 0
Training loss: 3.6104865074157715
Validation loss: 2.5658027869398876

Epoch: 6| Step: 1
Training loss: 2.383030652999878
Validation loss: 2.562797889914564

Epoch: 6| Step: 2
Training loss: 2.272855043411255
Validation loss: 2.565534737802321

Epoch: 6| Step: 3
Training loss: 2.7010347843170166
Validation loss: 2.5618228630353044

Epoch: 6| Step: 4
Training loss: 3.3680787086486816
Validation loss: 2.5625596302811817

Epoch: 6| Step: 5
Training loss: 2.9572031497955322
Validation loss: 2.562097516111148

Epoch: 6| Step: 6
Training loss: 2.3169522285461426
Validation loss: 2.559234542231406

Epoch: 6| Step: 7
Training loss: 2.8016715049743652
Validation loss: 2.560455414556688

Epoch: 6| Step: 8
Training loss: 2.575937032699585
Validation loss: 2.5587442049416165

Epoch: 6| Step: 9
Training loss: 3.352292776107788
Validation loss: 2.559821585173248

Epoch: 6| Step: 10
Training loss: 2.445993185043335
Validation loss: 2.558816735462476

Epoch: 6| Step: 11
Training loss: 2.327029228210449
Validation loss: 2.5576241682934504

Epoch: 6| Step: 12
Training loss: 2.2160699367523193
Validation loss: 2.5561291299840456

Epoch: 6| Step: 13
Training loss: 3.2803618907928467
Validation loss: 2.555837764534899

Epoch: 57| Step: 0
Training loss: 3.3151750564575195
Validation loss: 2.5575276420962427

Epoch: 6| Step: 1
Training loss: 3.0129098892211914
Validation loss: 2.5603407326564995

Epoch: 6| Step: 2
Training loss: 2.362064838409424
Validation loss: 2.566172927938482

Epoch: 6| Step: 3
Training loss: 2.966277599334717
Validation loss: 2.5675048135942027

Epoch: 6| Step: 4
Training loss: 2.429565906524658
Validation loss: 2.5713471469058784

Epoch: 6| Step: 5
Training loss: 2.80173397064209
Validation loss: 2.573766459700882

Epoch: 6| Step: 6
Training loss: 2.4719653129577637
Validation loss: 2.581754799812071

Epoch: 6| Step: 7
Training loss: 2.4795334339141846
Validation loss: 2.583325319392707

Epoch: 6| Step: 8
Training loss: 3.072924852371216
Validation loss: 2.5896838864972516

Epoch: 6| Step: 9
Training loss: 2.5221102237701416
Validation loss: 2.5894922056505756

Epoch: 6| Step: 10
Training loss: 2.610591173171997
Validation loss: 2.5812306301568144

Epoch: 6| Step: 11
Training loss: 3.080659866333008
Validation loss: 2.5704214803634153

Epoch: 6| Step: 12
Training loss: 2.774664878845215
Validation loss: 2.567665994808238

Epoch: 6| Step: 13
Training loss: 2.114957094192505
Validation loss: 2.5638963278903755

Epoch: 58| Step: 0
Training loss: 1.6914478540420532
Validation loss: 2.5660445510700183

Epoch: 6| Step: 1
Training loss: 2.7906229496002197
Validation loss: 2.5610431419905795

Epoch: 6| Step: 2
Training loss: 3.1870055198669434
Validation loss: 2.561286109749989

Epoch: 6| Step: 3
Training loss: 3.0901246070861816
Validation loss: 2.5587879585963424

Epoch: 6| Step: 4
Training loss: 3.3971383571624756
Validation loss: 2.561626019016389

Epoch: 6| Step: 5
Training loss: 2.5127131938934326
Validation loss: 2.5579103603157947

Epoch: 6| Step: 6
Training loss: 2.7974860668182373
Validation loss: 2.563614945257864

Epoch: 6| Step: 7
Training loss: 3.0223278999328613
Validation loss: 2.55847841693509

Epoch: 6| Step: 8
Training loss: 2.857896327972412
Validation loss: 2.5583453306587796

Epoch: 6| Step: 9
Training loss: 2.2544257640838623
Validation loss: 2.553883865315427

Epoch: 6| Step: 10
Training loss: 2.400866746902466
Validation loss: 2.5551202938120854

Epoch: 6| Step: 11
Training loss: 2.046917676925659
Validation loss: 2.553923386399464

Epoch: 6| Step: 12
Training loss: 3.4181466102600098
Validation loss: 2.5576962604317615

Epoch: 6| Step: 13
Training loss: 2.856121301651001
Validation loss: 2.5528396637209

Epoch: 59| Step: 0
Training loss: 3.4894399642944336
Validation loss: 2.5567720320916947

Epoch: 6| Step: 1
Training loss: 2.210191488265991
Validation loss: 2.5613430366721204

Epoch: 6| Step: 2
Training loss: 2.27671480178833
Validation loss: 2.5597603628712315

Epoch: 6| Step: 3
Training loss: 2.3618381023406982
Validation loss: 2.564366125291394

Epoch: 6| Step: 4
Training loss: 3.43098783493042
Validation loss: 2.5621451921360467

Epoch: 6| Step: 5
Training loss: 3.46730899810791
Validation loss: 2.5561838970389417

Epoch: 6| Step: 6
Training loss: 3.1801629066467285
Validation loss: 2.558777391269643

Epoch: 6| Step: 7
Training loss: 2.1999387741088867
Validation loss: 2.554835058027698

Epoch: 6| Step: 8
Training loss: 3.2002439498901367
Validation loss: 2.5543439747184835

Epoch: 6| Step: 9
Training loss: 2.15238618850708
Validation loss: 2.5504929301559285

Epoch: 6| Step: 10
Training loss: 2.605415105819702
Validation loss: 2.5505973959481842

Epoch: 6| Step: 11
Training loss: 2.540449380874634
Validation loss: 2.5512323225698164

Epoch: 6| Step: 12
Training loss: 2.264591693878174
Validation loss: 2.553346190401303

Epoch: 6| Step: 13
Training loss: 2.7722349166870117
Validation loss: 2.5508247395997405

Epoch: 60| Step: 0
Training loss: 3.0137524604797363
Validation loss: 2.5474826417943484

Epoch: 6| Step: 1
Training loss: 3.1173391342163086
Validation loss: 2.5459671840872815

Epoch: 6| Step: 2
Training loss: 2.7715489864349365
Validation loss: 2.5474476711724394

Epoch: 6| Step: 3
Training loss: 3.0850796699523926
Validation loss: 2.5463397451626357

Epoch: 6| Step: 4
Training loss: 2.8845391273498535
Validation loss: 2.545245414139122

Epoch: 6| Step: 5
Training loss: 3.0759100914001465
Validation loss: 2.54624572876961

Epoch: 6| Step: 6
Training loss: 2.481931209564209
Validation loss: 2.5453836712785947

Epoch: 6| Step: 7
Training loss: 2.312798261642456
Validation loss: 2.548021593401509

Epoch: 6| Step: 8
Training loss: 2.6690893173217773
Validation loss: 2.554580127039263

Epoch: 6| Step: 9
Training loss: 2.0270652770996094
Validation loss: 2.559630124799667

Epoch: 6| Step: 10
Training loss: 2.1450870037078857
Validation loss: 2.5677795153792187

Epoch: 6| Step: 11
Training loss: 3.0706212520599365
Validation loss: 2.5807312970520346

Epoch: 6| Step: 12
Training loss: 3.1272988319396973
Validation loss: 2.5898674713668

Epoch: 6| Step: 13
Training loss: 2.0584897994995117
Validation loss: 2.602480944766793

Epoch: 61| Step: 0
Training loss: 2.9673969745635986
Validation loss: 2.5824589626763457

Epoch: 6| Step: 1
Training loss: 2.8010082244873047
Validation loss: 2.5645164597419

Epoch: 6| Step: 2
Training loss: 1.8540940284729004
Validation loss: 2.55271928797486

Epoch: 6| Step: 3
Training loss: 3.112262487411499
Validation loss: 2.5409263205784622

Epoch: 6| Step: 4
Training loss: 3.3305327892303467
Validation loss: 2.5395728413776686

Epoch: 6| Step: 5
Training loss: 2.2190632820129395
Validation loss: 2.5401283797397407

Epoch: 6| Step: 6
Training loss: 2.682690143585205
Validation loss: 2.5402951855813303

Epoch: 6| Step: 7
Training loss: 1.5550028085708618
Validation loss: 2.5369870175597486

Epoch: 6| Step: 8
Training loss: 2.594723701477051
Validation loss: 2.5399588282390306

Epoch: 6| Step: 9
Training loss: 2.337754249572754
Validation loss: 2.539097668022238

Epoch: 6| Step: 10
Training loss: 2.8229689598083496
Validation loss: 2.5373900526313373

Epoch: 6| Step: 11
Training loss: 3.2057759761810303
Validation loss: 2.539416041425479

Epoch: 6| Step: 12
Training loss: 3.2873337268829346
Validation loss: 2.533867081006368

Epoch: 6| Step: 13
Training loss: 3.744602680206299
Validation loss: 2.5388535555972847

Epoch: 62| Step: 0
Training loss: 4.044528961181641
Validation loss: 2.5395862338363484

Epoch: 6| Step: 1
Training loss: 2.4789328575134277
Validation loss: 2.541260039934548

Epoch: 6| Step: 2
Training loss: 3.0283291339874268
Validation loss: 2.543436163215227

Epoch: 6| Step: 3
Training loss: 2.7601711750030518
Validation loss: 2.5379413558590795

Epoch: 6| Step: 4
Training loss: 2.240051507949829
Validation loss: 2.5454820048424507

Epoch: 6| Step: 5
Training loss: 2.16982364654541
Validation loss: 2.5444750350008727

Epoch: 6| Step: 6
Training loss: 2.084017038345337
Validation loss: 2.535228001174106

Epoch: 6| Step: 7
Training loss: 2.393096446990967
Validation loss: 2.542703651612805

Epoch: 6| Step: 8
Training loss: 3.0238499641418457
Validation loss: 2.543898423512777

Epoch: 6| Step: 9
Training loss: 2.7158496379852295
Validation loss: 2.538282850737213

Epoch: 6| Step: 10
Training loss: 3.074336051940918
Validation loss: 2.5471708184929303

Epoch: 6| Step: 11
Training loss: 2.3154525756835938
Validation loss: 2.561418525634273

Epoch: 6| Step: 12
Training loss: 3.0011730194091797
Validation loss: 2.577332709425239

Epoch: 6| Step: 13
Training loss: 2.480654716491699
Validation loss: 2.5762179410585793

Epoch: 63| Step: 0
Training loss: 2.3513317108154297
Validation loss: 2.6188675947086786

Epoch: 6| Step: 1
Training loss: 2.0004873275756836
Validation loss: 2.6250820903367895

Epoch: 6| Step: 2
Training loss: 2.464691638946533
Validation loss: 2.605553411668347

Epoch: 6| Step: 3
Training loss: 2.7741432189941406
Validation loss: 2.551194924180226

Epoch: 6| Step: 4
Training loss: 2.6560707092285156
Validation loss: 2.5255027714596

Epoch: 6| Step: 5
Training loss: 2.8978676795959473
Validation loss: 2.5231529999804754

Epoch: 6| Step: 6
Training loss: 2.8670759201049805
Validation loss: 2.520404190145513

Epoch: 6| Step: 7
Training loss: 2.5629773139953613
Validation loss: 2.5236717270266626

Epoch: 6| Step: 8
Training loss: 2.5094406604766846
Validation loss: 2.5238284039241012

Epoch: 6| Step: 9
Training loss: 3.0622823238372803
Validation loss: 2.5270354901590655

Epoch: 6| Step: 10
Training loss: 3.671971559524536
Validation loss: 2.5285602359361548

Epoch: 6| Step: 11
Training loss: 2.7762045860290527
Validation loss: 2.528991163417857

Epoch: 6| Step: 12
Training loss: 2.75691819190979
Validation loss: 2.5316162301648046

Epoch: 6| Step: 13
Training loss: 3.06748104095459
Validation loss: 2.563750279847012

Epoch: 64| Step: 0
Training loss: 3.475175380706787
Validation loss: 2.5426598928307973

Epoch: 6| Step: 1
Training loss: 3.274064064025879
Validation loss: 2.5260930189522366

Epoch: 6| Step: 2
Training loss: 2.2156190872192383
Validation loss: 2.5233134710660545

Epoch: 6| Step: 3
Training loss: 2.1716716289520264
Validation loss: 2.5208828679976927

Epoch: 6| Step: 4
Training loss: 1.9310787916183472
Validation loss: 2.514643297400526

Epoch: 6| Step: 5
Training loss: 3.119180202484131
Validation loss: 2.5139220811987437

Epoch: 6| Step: 6
Training loss: 2.8385543823242188
Validation loss: 2.5158040574801865

Epoch: 6| Step: 7
Training loss: 2.664017915725708
Validation loss: 2.5206554884551675

Epoch: 6| Step: 8
Training loss: 2.1918845176696777
Validation loss: 2.526078567709974

Epoch: 6| Step: 9
Training loss: 2.6697440147399902
Validation loss: 2.5423626079354236

Epoch: 6| Step: 10
Training loss: 2.6438403129577637
Validation loss: 2.5377183011783067

Epoch: 6| Step: 11
Training loss: 2.7315006256103516
Validation loss: 2.534719087744272

Epoch: 6| Step: 12
Training loss: 2.8943123817443848
Validation loss: 2.521044167139197

Epoch: 6| Step: 13
Training loss: 3.3468339443206787
Validation loss: 2.5156523719910653

Epoch: 65| Step: 0
Training loss: 2.279222249984741
Validation loss: 2.517868580356721

Epoch: 6| Step: 1
Training loss: 2.689533233642578
Validation loss: 2.518685056317237

Epoch: 6| Step: 2
Training loss: 2.7446351051330566
Validation loss: 2.5143130133228917

Epoch: 6| Step: 3
Training loss: 3.5958640575408936
Validation loss: 2.5147096367292505

Epoch: 6| Step: 4
Training loss: 2.3889987468719482
Validation loss: 2.5111691849206084

Epoch: 6| Step: 5
Training loss: 2.6633894443511963
Validation loss: 2.5090522304657967

Epoch: 6| Step: 6
Training loss: 2.7457923889160156
Validation loss: 2.51105175992494

Epoch: 6| Step: 7
Training loss: 2.406463146209717
Validation loss: 2.509201275405063

Epoch: 6| Step: 8
Training loss: 2.7192931175231934
Validation loss: 2.5083248615264893

Epoch: 6| Step: 9
Training loss: 3.094905376434326
Validation loss: 2.5057246454300417

Epoch: 6| Step: 10
Training loss: 2.9364328384399414
Validation loss: 2.5062457669165825

Epoch: 6| Step: 11
Training loss: 2.9910364151000977
Validation loss: 2.507139826333651

Epoch: 6| Step: 12
Training loss: 1.7768527269363403
Validation loss: 2.508647995610391

Epoch: 6| Step: 13
Training loss: 2.9349584579467773
Validation loss: 2.510574245965609

Epoch: 66| Step: 0
Training loss: 2.6197454929351807
Validation loss: 2.5235573091814594

Epoch: 6| Step: 1
Training loss: 2.678246021270752
Validation loss: 2.524917179538358

Epoch: 6| Step: 2
Training loss: 3.0610976219177246
Validation loss: 2.5238328005677912

Epoch: 6| Step: 3
Training loss: 2.666700839996338
Validation loss: 2.5311762850771666

Epoch: 6| Step: 4
Training loss: 3.4331653118133545
Validation loss: 2.5286829907407045

Epoch: 6| Step: 5
Training loss: 2.5192790031433105
Validation loss: 2.5290883253979426

Epoch: 6| Step: 6
Training loss: 3.0270557403564453
Validation loss: 2.5241650663396364

Epoch: 6| Step: 7
Training loss: 2.2126705646514893
Validation loss: 2.5187195936838784

Epoch: 6| Step: 8
Training loss: 2.9934182167053223
Validation loss: 2.5188425099977882

Epoch: 6| Step: 9
Training loss: 3.219944715499878
Validation loss: 2.51425616715544

Epoch: 6| Step: 10
Training loss: 2.366821527481079
Validation loss: 2.510617297182801

Epoch: 6| Step: 11
Training loss: 1.374393105506897
Validation loss: 2.5054741854308755

Epoch: 6| Step: 12
Training loss: 2.727691888809204
Validation loss: 2.5029066711343746

Epoch: 6| Step: 13
Training loss: 3.152660846710205
Validation loss: 2.502827198274674

Epoch: 67| Step: 0
Training loss: 2.338831901550293
Validation loss: 2.4980588138744397

Epoch: 6| Step: 1
Training loss: 2.618265151977539
Validation loss: 2.5046761753738567

Epoch: 6| Step: 2
Training loss: 2.665607452392578
Validation loss: 2.502867785833215

Epoch: 6| Step: 3
Training loss: 2.052248477935791
Validation loss: 2.503981344161495

Epoch: 6| Step: 4
Training loss: 2.3911006450653076
Validation loss: 2.501103616529895

Epoch: 6| Step: 5
Training loss: 2.2886319160461426
Validation loss: 2.490937525226224

Epoch: 6| Step: 6
Training loss: 2.564887523651123
Validation loss: 2.4930148124694824

Epoch: 6| Step: 7
Training loss: 3.6806187629699707
Validation loss: 2.4929531492212766

Epoch: 6| Step: 8
Training loss: 2.5354766845703125
Validation loss: 2.492205589048324

Epoch: 6| Step: 9
Training loss: 2.747957229614258
Validation loss: 2.4967895400139595

Epoch: 6| Step: 10
Training loss: 3.289102077484131
Validation loss: 2.4990046895960325

Epoch: 6| Step: 11
Training loss: 2.8127012252807617
Validation loss: 2.4983906515182985

Epoch: 6| Step: 12
Training loss: 3.006380319595337
Validation loss: 2.4993398420272337

Epoch: 6| Step: 13
Training loss: 2.796414613723755
Validation loss: 2.505990405236521

Epoch: 68| Step: 0
Training loss: 2.763227939605713
Validation loss: 2.528649655721521

Epoch: 6| Step: 1
Training loss: 2.3435261249542236
Validation loss: 2.5436097498862975

Epoch: 6| Step: 2
Training loss: 2.4146251678466797
Validation loss: 2.5514806188562864

Epoch: 6| Step: 3
Training loss: 2.808616876602173
Validation loss: 2.55309566374748

Epoch: 6| Step: 4
Training loss: 2.5279178619384766
Validation loss: 2.547476419838526

Epoch: 6| Step: 5
Training loss: 2.0206828117370605
Validation loss: 2.5446566048488823

Epoch: 6| Step: 6
Training loss: 2.7040717601776123
Validation loss: 2.539765250298285

Epoch: 6| Step: 7
Training loss: 2.835197687149048
Validation loss: 2.534348154580721

Epoch: 6| Step: 8
Training loss: 2.4120230674743652
Validation loss: 2.535436963522306

Epoch: 6| Step: 9
Training loss: 3.1694936752319336
Validation loss: 2.5421475825771207

Epoch: 6| Step: 10
Training loss: 2.8976101875305176
Validation loss: 2.5339490136792584

Epoch: 6| Step: 11
Training loss: 2.5327091217041016
Validation loss: 2.527727093747867

Epoch: 6| Step: 12
Training loss: 3.3095502853393555
Validation loss: 2.547591978503812

Epoch: 6| Step: 13
Training loss: 3.941838502883911
Validation loss: 2.5366136002284225

Epoch: 69| Step: 0
Training loss: 2.7858850955963135
Validation loss: 2.532157596721444

Epoch: 6| Step: 1
Training loss: 3.254668712615967
Validation loss: 2.526658696513022

Epoch: 6| Step: 2
Training loss: 2.710953712463379
Validation loss: 2.525432276469405

Epoch: 6| Step: 3
Training loss: 2.271915912628174
Validation loss: 2.5178513552552912

Epoch: 6| Step: 4
Training loss: 2.330035448074341
Validation loss: 2.511261142710204

Epoch: 6| Step: 5
Training loss: 2.151914596557617
Validation loss: 2.508206752038771

Epoch: 6| Step: 6
Training loss: 3.249525547027588
Validation loss: 2.509766722238192

Epoch: 6| Step: 7
Training loss: 2.827385187149048
Validation loss: 2.503521652631862

Epoch: 6| Step: 8
Training loss: 2.6830999851226807
Validation loss: 2.506534284160983

Epoch: 6| Step: 9
Training loss: 2.667407751083374
Validation loss: 2.4944983297778713

Epoch: 6| Step: 10
Training loss: 2.495025157928467
Validation loss: 2.493094805748232

Epoch: 6| Step: 11
Training loss: 2.557506561279297
Validation loss: 2.4960280951633247

Epoch: 6| Step: 12
Training loss: 3.1075406074523926
Validation loss: 2.489552784991521

Epoch: 6| Step: 13
Training loss: 2.5241971015930176
Validation loss: 2.491297173243697

Epoch: 70| Step: 0
Training loss: 2.378594398498535
Validation loss: 2.487652653007097

Epoch: 6| Step: 1
Training loss: 3.2333054542541504
Validation loss: 2.4905494515613844

Epoch: 6| Step: 2
Training loss: 2.608290910720825
Validation loss: 2.4919972086465485

Epoch: 6| Step: 3
Training loss: 3.3170225620269775
Validation loss: 2.4907140706175115

Epoch: 6| Step: 4
Training loss: 2.6353464126586914
Validation loss: 2.4960495835991314

Epoch: 6| Step: 5
Training loss: 2.3824644088745117
Validation loss: 2.5062925789945867

Epoch: 6| Step: 6
Training loss: 2.9283041954040527
Validation loss: 2.526230389072049

Epoch: 6| Step: 7
Training loss: 2.8691012859344482
Validation loss: 2.5264117845924954

Epoch: 6| Step: 8
Training loss: 2.3405675888061523
Validation loss: 2.5009453860662316

Epoch: 6| Step: 9
Training loss: 2.402179718017578
Validation loss: 2.486800939806046

Epoch: 6| Step: 10
Training loss: 3.051454544067383
Validation loss: 2.488608234672136

Epoch: 6| Step: 11
Training loss: 2.2065117359161377
Validation loss: 2.49506281011848

Epoch: 6| Step: 12
Training loss: 3.1653225421905518
Validation loss: 2.4895937622234388

Epoch: 6| Step: 13
Training loss: 1.9295344352722168
Validation loss: 2.4916476511186167

Epoch: 71| Step: 0
Training loss: 3.1867713928222656
Validation loss: 2.507248129895938

Epoch: 6| Step: 1
Training loss: 2.777686595916748
Validation loss: 2.522706359945318

Epoch: 6| Step: 2
Training loss: 2.274359703063965
Validation loss: 2.534083835540279

Epoch: 6| Step: 3
Training loss: 2.933670997619629
Validation loss: 2.5408114925507577

Epoch: 6| Step: 4
Training loss: 3.248439311981201
Validation loss: 2.548309936318346

Epoch: 6| Step: 5
Training loss: 2.2097702026367188
Validation loss: 2.537278647063881

Epoch: 6| Step: 6
Training loss: 2.8348493576049805
Validation loss: 2.5525184267310688

Epoch: 6| Step: 7
Training loss: 3.1387345790863037
Validation loss: 2.5469155388493694

Epoch: 6| Step: 8
Training loss: 2.6490979194641113
Validation loss: 2.5374621960424606

Epoch: 6| Step: 9
Training loss: 2.3646416664123535
Validation loss: 2.500507611100392

Epoch: 6| Step: 10
Training loss: 3.352658271789551
Validation loss: 2.489525120745423

Epoch: 6| Step: 11
Training loss: 2.177093982696533
Validation loss: 2.4843812732286352

Epoch: 6| Step: 12
Training loss: 2.5870168209075928
Validation loss: 2.4847872500778525

Epoch: 6| Step: 13
Training loss: 1.82335364818573
Validation loss: 2.4812927451185

Epoch: 72| Step: 0
Training loss: 2.963470458984375
Validation loss: 2.4865960510828162

Epoch: 6| Step: 1
Training loss: 3.3177778720855713
Validation loss: 2.4827900496862267

Epoch: 6| Step: 2
Training loss: 3.3419363498687744
Validation loss: 2.481245897149527

Epoch: 6| Step: 3
Training loss: 1.810028076171875
Validation loss: 2.480462138370801

Epoch: 6| Step: 4
Training loss: 1.7335853576660156
Validation loss: 2.4820436816061697

Epoch: 6| Step: 5
Training loss: 2.733458995819092
Validation loss: 2.4840414626623994

Epoch: 6| Step: 6
Training loss: 2.3361730575561523
Validation loss: 2.4860518645214778

Epoch: 6| Step: 7
Training loss: 2.6180591583251953
Validation loss: 2.480773648908061

Epoch: 6| Step: 8
Training loss: 3.25837779045105
Validation loss: 2.4850062862519295

Epoch: 6| Step: 9
Training loss: 2.0939478874206543
Validation loss: 2.4801946916887836

Epoch: 6| Step: 10
Training loss: 2.9539730548858643
Validation loss: 2.4878655223436255

Epoch: 6| Step: 11
Training loss: 1.8087406158447266
Validation loss: 2.482143186753796

Epoch: 6| Step: 12
Training loss: 3.7591822147369385
Validation loss: 2.476096619841873

Epoch: 6| Step: 13
Training loss: 3.1396143436431885
Validation loss: 2.4768305875921763

Epoch: 73| Step: 0
Training loss: 3.0797014236450195
Validation loss: 2.4760752288244103

Epoch: 6| Step: 1
Training loss: 2.3344485759735107
Validation loss: 2.4809891536671627

Epoch: 6| Step: 2
Training loss: 2.654712438583374
Validation loss: 2.4804233684334704

Epoch: 6| Step: 3
Training loss: 2.660646438598633
Validation loss: 2.483977348573746

Epoch: 6| Step: 4
Training loss: 2.505913734436035
Validation loss: 2.477567442001835

Epoch: 6| Step: 5
Training loss: 1.9065505266189575
Validation loss: 2.4807605589589765

Epoch: 6| Step: 6
Training loss: 3.523894786834717
Validation loss: 2.484265076216831

Epoch: 6| Step: 7
Training loss: 2.391808032989502
Validation loss: 2.483752994127171

Epoch: 6| Step: 8
Training loss: 2.2464325428009033
Validation loss: 2.483765479057066

Epoch: 6| Step: 9
Training loss: 2.3167550563812256
Validation loss: 2.483398101663077

Epoch: 6| Step: 10
Training loss: 3.046727180480957
Validation loss: 2.4818650573812504

Epoch: 6| Step: 11
Training loss: 3.644913911819458
Validation loss: 2.4820330630066576

Epoch: 6| Step: 12
Training loss: 2.7541487216949463
Validation loss: 2.4851939806374173

Epoch: 6| Step: 13
Training loss: 2.5462331771850586
Validation loss: 2.4748975948620866

Epoch: 74| Step: 0
Training loss: 3.0788991451263428
Validation loss: 2.478795072083832

Epoch: 6| Step: 1
Training loss: 3.2627475261688232
Validation loss: 2.4832057619607575

Epoch: 6| Step: 2
Training loss: 2.998684883117676
Validation loss: 2.483479465207746

Epoch: 6| Step: 3
Training loss: 3.0615735054016113
Validation loss: 2.4838734878006803

Epoch: 6| Step: 4
Training loss: 2.731560230255127
Validation loss: 2.4841813118227067

Epoch: 6| Step: 5
Training loss: 3.332502603530884
Validation loss: 2.4859364724928334

Epoch: 6| Step: 6
Training loss: 2.544376850128174
Validation loss: 2.49012174401232

Epoch: 6| Step: 7
Training loss: 2.112724542617798
Validation loss: 2.4897422354708434

Epoch: 6| Step: 8
Training loss: 2.694690227508545
Validation loss: 2.4892687592455136

Epoch: 6| Step: 9
Training loss: 2.23616361618042
Validation loss: 2.491646597462316

Epoch: 6| Step: 10
Training loss: 2.58327054977417
Validation loss: 2.5030117957822737

Epoch: 6| Step: 11
Training loss: 2.6769704818725586
Validation loss: 2.5070547314100367

Epoch: 6| Step: 12
Training loss: 2.330261707305908
Validation loss: 2.4945926743168987

Epoch: 6| Step: 13
Training loss: 1.4215017557144165
Validation loss: 2.4854785806389263

Epoch: 75| Step: 0
Training loss: 2.63543701171875
Validation loss: 2.4793447679088962

Epoch: 6| Step: 1
Training loss: 2.017925262451172
Validation loss: 2.4763887825832573

Epoch: 6| Step: 2
Training loss: 2.190035820007324
Validation loss: 2.471783168854252

Epoch: 6| Step: 3
Training loss: 2.5518198013305664
Validation loss: 2.474713489573489

Epoch: 6| Step: 4
Training loss: 2.6341552734375
Validation loss: 2.465547809036829

Epoch: 6| Step: 5
Training loss: 2.697167158126831
Validation loss: 2.469900861863167

Epoch: 6| Step: 6
Training loss: 2.5102877616882324
Validation loss: 2.4663370322155695

Epoch: 6| Step: 7
Training loss: 3.5027236938476562
Validation loss: 2.464331352582542

Epoch: 6| Step: 8
Training loss: 2.230133295059204
Validation loss: 2.4635905886209137

Epoch: 6| Step: 9
Training loss: 2.9580416679382324
Validation loss: 2.463481767203218

Epoch: 6| Step: 10
Training loss: 3.2253899574279785
Validation loss: 2.464620779919368

Epoch: 6| Step: 11
Training loss: 3.330655574798584
Validation loss: 2.4610101843392975

Epoch: 6| Step: 12
Training loss: 2.1891696453094482
Validation loss: 2.460384661151517

Epoch: 6| Step: 13
Training loss: 2.7978196144104004
Validation loss: 2.46098179330108

Epoch: 76| Step: 0
Training loss: 2.7745518684387207
Validation loss: 2.4615899209053285

Epoch: 6| Step: 1
Training loss: 1.9888279438018799
Validation loss: 2.4587151286422566

Epoch: 6| Step: 2
Training loss: 2.760632276535034
Validation loss: 2.462506604451005

Epoch: 6| Step: 3
Training loss: 2.3683512210845947
Validation loss: 2.459691959042703

Epoch: 6| Step: 4
Training loss: 2.4186418056488037
Validation loss: 2.457997642537599

Epoch: 6| Step: 5
Training loss: 3.1286463737487793
Validation loss: 2.458120543469665

Epoch: 6| Step: 6
Training loss: 2.354541778564453
Validation loss: 2.462282226931664

Epoch: 6| Step: 7
Training loss: 2.7597460746765137
Validation loss: 2.471818270221833

Epoch: 6| Step: 8
Training loss: 1.714811086654663
Validation loss: 2.4790594782880557

Epoch: 6| Step: 9
Training loss: 2.912243127822876
Validation loss: 2.492496928861064

Epoch: 6| Step: 10
Training loss: 3.1459646224975586
Validation loss: 2.5045648236428537

Epoch: 6| Step: 11
Training loss: 3.055368423461914
Validation loss: 2.511657694334625

Epoch: 6| Step: 12
Training loss: 3.946100950241089
Validation loss: 2.5085606369920956

Epoch: 6| Step: 13
Training loss: 1.7939143180847168
Validation loss: 2.489860011685279

Epoch: 77| Step: 0
Training loss: 2.398686170578003
Validation loss: 2.480571331516389

Epoch: 6| Step: 1
Training loss: 2.8232994079589844
Validation loss: 2.4698103243304836

Epoch: 6| Step: 2
Training loss: 2.515857458114624
Validation loss: 2.4609105792096866

Epoch: 6| Step: 3
Training loss: 3.159993886947632
Validation loss: 2.46197941482708

Epoch: 6| Step: 4
Training loss: 3.0437426567077637
Validation loss: 2.4584695626330633

Epoch: 6| Step: 5
Training loss: 2.2875423431396484
Validation loss: 2.4609234589402393

Epoch: 6| Step: 6
Training loss: 3.0666275024414062
Validation loss: 2.465661584690053

Epoch: 6| Step: 7
Training loss: 2.956826686859131
Validation loss: 2.460893884781868

Epoch: 6| Step: 8
Training loss: 2.1628668308258057
Validation loss: 2.4602305017491823

Epoch: 6| Step: 9
Training loss: 2.2209632396698
Validation loss: 2.460685163415888

Epoch: 6| Step: 10
Training loss: 2.9534711837768555
Validation loss: 2.463114964064731

Epoch: 6| Step: 11
Training loss: 2.585723638534546
Validation loss: 2.462939367499403

Epoch: 6| Step: 12
Training loss: 3.2493908405303955
Validation loss: 2.4615803764712427

Epoch: 6| Step: 13
Training loss: 1.639172077178955
Validation loss: 2.4660672833842616

Epoch: 78| Step: 0
Training loss: 2.9218595027923584
Validation loss: 2.480372287893808

Epoch: 6| Step: 1
Training loss: 1.6946407556533813
Validation loss: 2.4865291708259174

Epoch: 6| Step: 2
Training loss: 3.1937332153320312
Validation loss: 2.4852824057302167

Epoch: 6| Step: 3
Training loss: 3.262653350830078
Validation loss: 2.4739180470025666

Epoch: 6| Step: 4
Training loss: 2.3997654914855957
Validation loss: 2.4677907446379304

Epoch: 6| Step: 5
Training loss: 2.0903964042663574
Validation loss: 2.455837934247909

Epoch: 6| Step: 6
Training loss: 3.270895004272461
Validation loss: 2.4643038959913355

Epoch: 6| Step: 7
Training loss: 3.001055955886841
Validation loss: 2.466988742992442

Epoch: 6| Step: 8
Training loss: 3.3274154663085938
Validation loss: 2.4794665664754887

Epoch: 6| Step: 9
Training loss: 1.6972646713256836
Validation loss: 2.4866627236848236

Epoch: 6| Step: 10
Training loss: 3.0066285133361816
Validation loss: 2.506569065073485

Epoch: 6| Step: 11
Training loss: 2.559048652648926
Validation loss: 2.515287163437054

Epoch: 6| Step: 12
Training loss: 2.2589550018310547
Validation loss: 2.535047833637525

Epoch: 6| Step: 13
Training loss: 2.7675912380218506
Validation loss: 2.5589717177934546

Epoch: 79| Step: 0
Training loss: 2.6485910415649414
Validation loss: 2.533954861343548

Epoch: 6| Step: 1
Training loss: 3.44360613822937
Validation loss: 2.503307706566267

Epoch: 6| Step: 2
Training loss: 3.0499207973480225
Validation loss: 2.4738132107642388

Epoch: 6| Step: 3
Training loss: 2.099893093109131
Validation loss: 2.4617906411488852

Epoch: 6| Step: 4
Training loss: 2.728749990463257
Validation loss: 2.450942431726763

Epoch: 6| Step: 5
Training loss: 2.747239589691162
Validation loss: 2.4490746067416285

Epoch: 6| Step: 6
Training loss: 2.9316649436950684
Validation loss: 2.4488745299718713

Epoch: 6| Step: 7
Training loss: 2.512950897216797
Validation loss: 2.449803013955393

Epoch: 6| Step: 8
Training loss: 2.1847681999206543
Validation loss: 2.447442408530943

Epoch: 6| Step: 9
Training loss: 1.5085771083831787
Validation loss: 2.4456200497124785

Epoch: 6| Step: 10
Training loss: 2.9068222045898438
Validation loss: 2.4471424753947923

Epoch: 6| Step: 11
Training loss: 3.315767288208008
Validation loss: 2.4467475311730498

Epoch: 6| Step: 12
Training loss: 2.773899555206299
Validation loss: 2.4455900884443715

Epoch: 6| Step: 13
Training loss: 2.4981393814086914
Validation loss: 2.447268291186261

Epoch: 80| Step: 0
Training loss: 3.450655221939087
Validation loss: 2.445709938644081

Epoch: 6| Step: 1
Training loss: 3.1193766593933105
Validation loss: 2.448372397371518

Epoch: 6| Step: 2
Training loss: 2.0388216972351074
Validation loss: 2.4458258536554154

Epoch: 6| Step: 3
Training loss: 3.17366886138916
Validation loss: 2.445764039152412

Epoch: 6| Step: 4
Training loss: 2.2749342918395996
Validation loss: 2.4465258711127826

Epoch: 6| Step: 5
Training loss: 2.4134936332702637
Validation loss: 2.447196296466294

Epoch: 6| Step: 6
Training loss: 2.877894163131714
Validation loss: 2.4461422607462895

Epoch: 6| Step: 7
Training loss: 2.2050845623016357
Validation loss: 2.44901442527771

Epoch: 6| Step: 8
Training loss: 3.1882987022399902
Validation loss: 2.4516452076614543

Epoch: 6| Step: 9
Training loss: 3.2490267753601074
Validation loss: 2.4535844274746474

Epoch: 6| Step: 10
Training loss: 2.2188634872436523
Validation loss: 2.465218054350986

Epoch: 6| Step: 11
Training loss: 2.2690823078155518
Validation loss: 2.471780405249647

Epoch: 6| Step: 12
Training loss: 2.514962673187256
Validation loss: 2.4633530262977845

Epoch: 6| Step: 13
Training loss: 2.1238791942596436
Validation loss: 2.459621544807188

Epoch: 81| Step: 0
Training loss: 2.2561161518096924
Validation loss: 2.4638354932108233

Epoch: 6| Step: 1
Training loss: 2.4195408821105957
Validation loss: 2.455413136430966

Epoch: 6| Step: 2
Training loss: 3.169196844100952
Validation loss: 2.451256795596051

Epoch: 6| Step: 3
Training loss: 2.404446601867676
Validation loss: 2.4587814436163953

Epoch: 6| Step: 4
Training loss: 2.4272541999816895
Validation loss: 2.4673668466588503

Epoch: 6| Step: 5
Training loss: 2.8692257404327393
Validation loss: 2.461058485892511

Epoch: 6| Step: 6
Training loss: 3.1272101402282715
Validation loss: 2.468294851241573

Epoch: 6| Step: 7
Training loss: 2.249817132949829
Validation loss: 2.463481926148938

Epoch: 6| Step: 8
Training loss: 3.2041406631469727
Validation loss: 2.4745627500677623

Epoch: 6| Step: 9
Training loss: 2.5171947479248047
Validation loss: 2.463028730884675

Epoch: 6| Step: 10
Training loss: 2.6252005100250244
Validation loss: 2.4612740906335975

Epoch: 6| Step: 11
Training loss: 2.770669937133789
Validation loss: 2.451410307679125

Epoch: 6| Step: 12
Training loss: 3.1805367469787598
Validation loss: 2.447542485370431

Epoch: 6| Step: 13
Training loss: 1.6547659635543823
Validation loss: 2.4434785868531916

Epoch: 82| Step: 0
Training loss: 4.043818950653076
Validation loss: 2.445882620350007

Epoch: 6| Step: 1
Training loss: 2.618492364883423
Validation loss: 2.441743894289899

Epoch: 6| Step: 2
Training loss: 2.687051296234131
Validation loss: 2.4436690679160495

Epoch: 6| Step: 3
Training loss: 2.196040630340576
Validation loss: 2.44060569296601

Epoch: 6| Step: 4
Training loss: 2.4369781017303467
Validation loss: 2.442037126069428

Epoch: 6| Step: 5
Training loss: 2.582794189453125
Validation loss: 2.4413423461298787

Epoch: 6| Step: 6
Training loss: 2.6515324115753174
Validation loss: 2.4428009089603218

Epoch: 6| Step: 7
Training loss: 2.827524185180664
Validation loss: 2.443391846072289

Epoch: 6| Step: 8
Training loss: 2.6712255477905273
Validation loss: 2.4472183847940094

Epoch: 6| Step: 9
Training loss: 2.4715054035186768
Validation loss: 2.446424506043875

Epoch: 6| Step: 10
Training loss: 2.7451257705688477
Validation loss: 2.462782541910807

Epoch: 6| Step: 11
Training loss: 2.392214298248291
Validation loss: 2.4591805242723033

Epoch: 6| Step: 12
Training loss: 2.564570426940918
Validation loss: 2.455469382706509

Epoch: 6| Step: 13
Training loss: 2.1613683700561523
Validation loss: 2.4512181884499005

Epoch: 83| Step: 0
Training loss: 2.41581654548645
Validation loss: 2.4434216740310832

Epoch: 6| Step: 1
Training loss: 2.748185157775879
Validation loss: 2.4452794059630363

Epoch: 6| Step: 2
Training loss: 3.322460174560547
Validation loss: 2.4381561074205624

Epoch: 6| Step: 3
Training loss: 3.076279640197754
Validation loss: 2.4419098720755628

Epoch: 6| Step: 4
Training loss: 2.7118868827819824
Validation loss: 2.440445569253737

Epoch: 6| Step: 5
Training loss: 2.3957648277282715
Validation loss: 2.4426013269732074

Epoch: 6| Step: 6
Training loss: 2.3207015991210938
Validation loss: 2.4479015027323077

Epoch: 6| Step: 7
Training loss: 2.9841156005859375
Validation loss: 2.4514527320861816

Epoch: 6| Step: 8
Training loss: 1.8450877666473389
Validation loss: 2.451202525887438

Epoch: 6| Step: 9
Training loss: 2.4718198776245117
Validation loss: 2.4521338401302213

Epoch: 6| Step: 10
Training loss: 2.2582974433898926
Validation loss: 2.4519363654557096

Epoch: 6| Step: 11
Training loss: 2.659409999847412
Validation loss: 2.446715677938154

Epoch: 6| Step: 12
Training loss: 3.195002317428589
Validation loss: 2.4417935520090084

Epoch: 6| Step: 13
Training loss: 3.378872871398926
Validation loss: 2.437537080498152

Epoch: 84| Step: 0
Training loss: 2.6144895553588867
Validation loss: 2.4358388377774145

Epoch: 6| Step: 1
Training loss: 2.7315926551818848
Validation loss: 2.439218459590789

Epoch: 6| Step: 2
Training loss: 2.7005624771118164
Validation loss: 2.4517806037779777

Epoch: 6| Step: 3
Training loss: 2.637619972229004
Validation loss: 2.481458633176742

Epoch: 6| Step: 4
Training loss: 2.8755288124084473
Validation loss: 2.4970643725446475

Epoch: 6| Step: 5
Training loss: 2.0021185874938965
Validation loss: 2.5052308241526284

Epoch: 6| Step: 6
Training loss: 2.6884207725524902
Validation loss: 2.499664209222281

Epoch: 6| Step: 7
Training loss: 2.3981151580810547
Validation loss: 2.488249227564822

Epoch: 6| Step: 8
Training loss: 1.9415863752365112
Validation loss: 2.490606866857057

Epoch: 6| Step: 9
Training loss: 2.6249327659606934
Validation loss: 2.4815685697781142

Epoch: 6| Step: 10
Training loss: 3.3971292972564697
Validation loss: 2.4720845504473616

Epoch: 6| Step: 11
Training loss: 2.821303606033325
Validation loss: 2.466035671131585

Epoch: 6| Step: 12
Training loss: 2.895235776901245
Validation loss: 2.44143315797211

Epoch: 6| Step: 13
Training loss: 3.15128231048584
Validation loss: 2.439475372273435

Epoch: 85| Step: 0
Training loss: 2.8606479167938232
Validation loss: 2.4291241989340833

Epoch: 6| Step: 1
Training loss: 2.7632687091827393
Validation loss: 2.4342667800123974

Epoch: 6| Step: 2
Training loss: 2.2997238636016846
Validation loss: 2.441652387701055

Epoch: 6| Step: 3
Training loss: 3.0599074363708496
Validation loss: 2.4433326464827343

Epoch: 6| Step: 4
Training loss: 2.667512893676758
Validation loss: 2.4490983588721162

Epoch: 6| Step: 5
Training loss: 2.706920623779297
Validation loss: 2.467878823639244

Epoch: 6| Step: 6
Training loss: 2.2544312477111816
Validation loss: 2.4751089695961244

Epoch: 6| Step: 7
Training loss: 2.5780534744262695
Validation loss: 2.4667299588521323

Epoch: 6| Step: 8
Training loss: 2.5055267810821533
Validation loss: 2.4559032635022233

Epoch: 6| Step: 9
Training loss: 2.104762554168701
Validation loss: 2.451057021335889

Epoch: 6| Step: 10
Training loss: 3.2536096572875977
Validation loss: 2.445411838510985

Epoch: 6| Step: 11
Training loss: 3.8244590759277344
Validation loss: 2.4340461710447907

Epoch: 6| Step: 12
Training loss: 1.9352495670318604
Validation loss: 2.4279190724895847

Epoch: 6| Step: 13
Training loss: 2.465376615524292
Validation loss: 2.4291043794283302

Epoch: 86| Step: 0
Training loss: 3.0243091583251953
Validation loss: 2.4401074173629924

Epoch: 6| Step: 1
Training loss: 2.6444218158721924
Validation loss: 2.448353582812894

Epoch: 6| Step: 2
Training loss: 2.905381917953491
Validation loss: 2.46936894488591

Epoch: 6| Step: 3
Training loss: 2.5280370712280273
Validation loss: 2.485670712686354

Epoch: 6| Step: 4
Training loss: 2.791376829147339
Validation loss: 2.4925047018194713

Epoch: 6| Step: 5
Training loss: 2.5903396606445312
Validation loss: 2.487678558595719

Epoch: 6| Step: 6
Training loss: 2.5665721893310547
Validation loss: 2.495107258519819

Epoch: 6| Step: 7
Training loss: 2.880554676055908
Validation loss: 2.4840330231574272

Epoch: 6| Step: 8
Training loss: 2.565094470977783
Validation loss: 2.4758896263696815

Epoch: 6| Step: 9
Training loss: 2.3765454292297363
Validation loss: 2.4862555380790465

Epoch: 6| Step: 10
Training loss: 2.4869937896728516
Validation loss: 2.476983683083647

Epoch: 6| Step: 11
Training loss: 2.6848995685577393
Validation loss: 2.458363048491939

Epoch: 6| Step: 12
Training loss: 2.3669281005859375
Validation loss: 2.441887342801658

Epoch: 6| Step: 13
Training loss: 3.160438060760498
Validation loss: 2.4321277295389483

Epoch: 87| Step: 0
Training loss: 2.513916492462158
Validation loss: 2.4309880220761864

Epoch: 6| Step: 1
Training loss: 2.790757656097412
Validation loss: 2.4307162864233858

Epoch: 6| Step: 2
Training loss: 2.235431432723999
Validation loss: 2.4251624153506373

Epoch: 6| Step: 3
Training loss: 2.8396341800689697
Validation loss: 2.4280968609676568

Epoch: 6| Step: 4
Training loss: 2.786698818206787
Validation loss: 2.430508795604911

Epoch: 6| Step: 5
Training loss: 2.7987563610076904
Validation loss: 2.4333290669225875

Epoch: 6| Step: 6
Training loss: 2.489655017852783
Validation loss: 2.4308175720194334

Epoch: 6| Step: 7
Training loss: 2.707538604736328
Validation loss: 2.42710623433513

Epoch: 6| Step: 8
Training loss: 2.770507335662842
Validation loss: 2.428045831700807

Epoch: 6| Step: 9
Training loss: 2.690032958984375
Validation loss: 2.428293030749085

Epoch: 6| Step: 10
Training loss: 2.118229389190674
Validation loss: 2.4265535980142574

Epoch: 6| Step: 11
Training loss: 2.5883822441101074
Validation loss: 2.4258246344904744

Epoch: 6| Step: 12
Training loss: 2.8967247009277344
Validation loss: 2.4236773867760935

Epoch: 6| Step: 13
Training loss: 3.207571268081665
Validation loss: 2.4238886217917166

Epoch: 88| Step: 0
Training loss: 3.1013593673706055
Validation loss: 2.4203001017211587

Epoch: 6| Step: 1
Training loss: 2.5030932426452637
Validation loss: 2.4205254226602535

Epoch: 6| Step: 2
Training loss: 2.599487781524658
Validation loss: 2.4214464259404007

Epoch: 6| Step: 3
Training loss: 2.944438934326172
Validation loss: 2.4225442704334053

Epoch: 6| Step: 4
Training loss: 2.9083404541015625
Validation loss: 2.4231978718952467

Epoch: 6| Step: 5
Training loss: 1.9642800092697144
Validation loss: 2.425369139640562

Epoch: 6| Step: 6
Training loss: 2.2018749713897705
Validation loss: 2.4228227241064912

Epoch: 6| Step: 7
Training loss: 2.767599105834961
Validation loss: 2.4249793355182936

Epoch: 6| Step: 8
Training loss: 3.084198474884033
Validation loss: 2.431222087593489

Epoch: 6| Step: 9
Training loss: 2.904817581176758
Validation loss: 2.4315655334021455

Epoch: 6| Step: 10
Training loss: 2.7656679153442383
Validation loss: 2.434013464117563

Epoch: 6| Step: 11
Training loss: 2.9651873111724854
Validation loss: 2.436082640001851

Epoch: 6| Step: 12
Training loss: 2.2860286235809326
Validation loss: 2.4286673915001655

Epoch: 6| Step: 13
Training loss: 1.8846982717514038
Validation loss: 2.4230823157936014

Epoch: 89| Step: 0
Training loss: 2.8101704120635986
Validation loss: 2.4185325907122706

Epoch: 6| Step: 1
Training loss: 3.2431235313415527
Validation loss: 2.42035609932356

Epoch: 6| Step: 2
Training loss: 2.276061534881592
Validation loss: 2.419367846622262

Epoch: 6| Step: 3
Training loss: 2.866936683654785
Validation loss: 2.4175780255307435

Epoch: 6| Step: 4
Training loss: 3.1385679244995117
Validation loss: 2.4196902577595045

Epoch: 6| Step: 5
Training loss: 2.0483131408691406
Validation loss: 2.4162235542010237

Epoch: 6| Step: 6
Training loss: 2.439955711364746
Validation loss: 2.418136112151607

Epoch: 6| Step: 7
Training loss: 2.052058696746826
Validation loss: 2.41674836989372

Epoch: 6| Step: 8
Training loss: 2.487588882446289
Validation loss: 2.417470250078427

Epoch: 6| Step: 9
Training loss: 2.1369035243988037
Validation loss: 2.420404822595658

Epoch: 6| Step: 10
Training loss: 2.6986780166625977
Validation loss: 2.420326655910861

Epoch: 6| Step: 11
Training loss: 3.0098342895507812
Validation loss: 2.4238167655083442

Epoch: 6| Step: 12
Training loss: 2.9820590019226074
Validation loss: 2.4325946223351265

Epoch: 6| Step: 13
Training loss: 3.1466495990753174
Validation loss: 2.431853404609106

Epoch: 90| Step: 0
Training loss: 2.072688102722168
Validation loss: 2.4444484018510386

Epoch: 6| Step: 1
Training loss: 3.3662216663360596
Validation loss: 2.4434042387111212

Epoch: 6| Step: 2
Training loss: 3.273167610168457
Validation loss: 2.444956999953075

Epoch: 6| Step: 3
Training loss: 2.851579189300537
Validation loss: 2.4391891751238095

Epoch: 6| Step: 4
Training loss: 2.49908447265625
Validation loss: 2.437183592909126

Epoch: 6| Step: 5
Training loss: 2.511241912841797
Validation loss: 2.431098215041622

Epoch: 6| Step: 6
Training loss: 2.334629535675049
Validation loss: 2.4274820461068103

Epoch: 6| Step: 7
Training loss: 2.803534746170044
Validation loss: 2.4349469818094724

Epoch: 6| Step: 8
Training loss: 2.3174071311950684
Validation loss: 2.4273921648661294

Epoch: 6| Step: 9
Training loss: 2.4677042961120605
Validation loss: 2.431477572328301

Epoch: 6| Step: 10
Training loss: 2.1432299613952637
Validation loss: 2.4331229220154467

Epoch: 6| Step: 11
Training loss: 2.357100248336792
Validation loss: 2.435911291389055

Epoch: 6| Step: 12
Training loss: 3.1260223388671875
Validation loss: 2.4353887624638055

Epoch: 6| Step: 13
Training loss: 3.1720030307769775
Validation loss: 2.421897921510922

Epoch: 91| Step: 0
Training loss: 1.0203672647476196
Validation loss: 2.417110476442563

Epoch: 6| Step: 1
Training loss: 2.3421995639801025
Validation loss: 2.4106001661669825

Epoch: 6| Step: 2
Training loss: 3.2238998413085938
Validation loss: 2.4160971820995374

Epoch: 6| Step: 3
Training loss: 2.9876246452331543
Validation loss: 2.4146207712029897

Epoch: 6| Step: 4
Training loss: 2.2949647903442383
Validation loss: 2.413110622795679

Epoch: 6| Step: 5
Training loss: 2.5302634239196777
Validation loss: 2.411431356142926

Epoch: 6| Step: 6
Training loss: 2.6245388984680176
Validation loss: 2.4101380737878944

Epoch: 6| Step: 7
Training loss: 3.1566524505615234
Validation loss: 2.411220919701361

Epoch: 6| Step: 8
Training loss: 2.6477913856506348
Validation loss: 2.413128199115876

Epoch: 6| Step: 9
Training loss: 2.2412357330322266
Validation loss: 2.413220710651849

Epoch: 6| Step: 10
Training loss: 2.509023427963257
Validation loss: 2.4119277974610687

Epoch: 6| Step: 11
Training loss: 2.948200225830078
Validation loss: 2.4168963739948888

Epoch: 6| Step: 12
Training loss: 3.097935914993286
Validation loss: 2.414012611553233

Epoch: 6| Step: 13
Training loss: 4.072320938110352
Validation loss: 2.4144216814348773

Epoch: 92| Step: 0
Training loss: 2.0336155891418457
Validation loss: 2.4194777242598997

Epoch: 6| Step: 1
Training loss: 2.385371208190918
Validation loss: 2.4106421342460056

Epoch: 6| Step: 2
Training loss: 3.135852813720703
Validation loss: 2.4140932380512194

Epoch: 6| Step: 3
Training loss: 3.298693895339966
Validation loss: 2.4093306731152278

Epoch: 6| Step: 4
Training loss: 2.4498422145843506
Validation loss: 2.4065613131369314

Epoch: 6| Step: 5
Training loss: 2.821080207824707
Validation loss: 2.4067159339945805

Epoch: 6| Step: 6
Training loss: 2.4006574153900146
Validation loss: 2.4064840578263804

Epoch: 6| Step: 7
Training loss: 2.460036516189575
Validation loss: 2.40211723953165

Epoch: 6| Step: 8
Training loss: 2.3524303436279297
Validation loss: 2.4075940706396617

Epoch: 6| Step: 9
Training loss: 2.4899392127990723
Validation loss: 2.408889893562563

Epoch: 6| Step: 10
Training loss: 2.571434497833252
Validation loss: 2.4042039968634166

Epoch: 6| Step: 11
Training loss: 2.9423983097076416
Validation loss: 2.4028898669827368

Epoch: 6| Step: 12
Training loss: 2.757310390472412
Validation loss: 2.403661448468444

Epoch: 6| Step: 13
Training loss: 3.1554346084594727
Validation loss: 2.4053190446669057

Epoch: 93| Step: 0
Training loss: 2.120708703994751
Validation loss: 2.4090501646841727

Epoch: 6| Step: 1
Training loss: 2.6741583347320557
Validation loss: 2.4170239689529582

Epoch: 6| Step: 2
Training loss: 2.6451058387756348
Validation loss: 2.4282681557439987

Epoch: 6| Step: 3
Training loss: 3.0745177268981934
Validation loss: 2.436168555290468

Epoch: 6| Step: 4
Training loss: 2.1878020763397217
Validation loss: 2.457372514150476

Epoch: 6| Step: 5
Training loss: 3.714212417602539
Validation loss: 2.4764322798739196

Epoch: 6| Step: 6
Training loss: 2.2623655796051025
Validation loss: 2.486531857521303

Epoch: 6| Step: 7
Training loss: 2.7270498275756836
Validation loss: 2.484032798838872

Epoch: 6| Step: 8
Training loss: 2.6079483032226562
Validation loss: 2.48360881241419

Epoch: 6| Step: 9
Training loss: 1.8191566467285156
Validation loss: 2.4720020371098674

Epoch: 6| Step: 10
Training loss: 3.251718044281006
Validation loss: 2.43418619196902

Epoch: 6| Step: 11
Training loss: 2.836390733718872
Validation loss: 2.4189018075184157

Epoch: 6| Step: 12
Training loss: 2.602614402770996
Validation loss: 2.4105241298675537

Epoch: 6| Step: 13
Training loss: 2.596421957015991
Validation loss: 2.4060785308960946

Epoch: 94| Step: 0
Training loss: 2.218280553817749
Validation loss: 2.4051157941100416

Epoch: 6| Step: 1
Training loss: 2.7077488899230957
Validation loss: 2.4041923348621657

Epoch: 6| Step: 2
Training loss: 2.4808223247528076
Validation loss: 2.4044359217407885

Epoch: 6| Step: 3
Training loss: 3.260415554046631
Validation loss: 2.4054913418267363

Epoch: 6| Step: 4
Training loss: 2.988373041152954
Validation loss: 2.403118054072062

Epoch: 6| Step: 5
Training loss: 2.956615686416626
Validation loss: 2.399668783269903

Epoch: 6| Step: 6
Training loss: 2.5558578968048096
Validation loss: 2.4037195508198073

Epoch: 6| Step: 7
Training loss: 2.9432406425476074
Validation loss: 2.412550933899418

Epoch: 6| Step: 8
Training loss: 2.1378014087677
Validation loss: 2.412630670814104

Epoch: 6| Step: 9
Training loss: 2.762375831604004
Validation loss: 2.4192370573679605

Epoch: 6| Step: 10
Training loss: 1.8238126039505005
Validation loss: 2.424010456249278

Epoch: 6| Step: 11
Training loss: 2.7441840171813965
Validation loss: 2.4237558405886412

Epoch: 6| Step: 12
Training loss: 2.7303614616394043
Validation loss: 2.427780971732191

Epoch: 6| Step: 13
Training loss: 2.977813482284546
Validation loss: 2.426376376100766

Epoch: 95| Step: 0
Training loss: 2.2231063842773438
Validation loss: 2.4246250250006236

Epoch: 6| Step: 1
Training loss: 2.5994133949279785
Validation loss: 2.429283025444195

Epoch: 6| Step: 2
Training loss: 3.5210771560668945
Validation loss: 2.423331552936185

Epoch: 6| Step: 3
Training loss: 2.0468053817749023
Validation loss: 2.4273692638643327

Epoch: 6| Step: 4
Training loss: 3.284811496734619
Validation loss: 2.423140054107994

Epoch: 6| Step: 5
Training loss: 3.6318178176879883
Validation loss: 2.419575491259175

Epoch: 6| Step: 6
Training loss: 1.898075819015503
Validation loss: 2.4204613418989283

Epoch: 6| Step: 7
Training loss: 2.5685606002807617
Validation loss: 2.4180325923427457

Epoch: 6| Step: 8
Training loss: 2.4607086181640625
Validation loss: 2.420660895685996

Epoch: 6| Step: 9
Training loss: 3.1638009548187256
Validation loss: 2.4307791289462837

Epoch: 6| Step: 10
Training loss: 2.030937433242798
Validation loss: 2.4318407991881013

Epoch: 6| Step: 11
Training loss: 2.6397924423217773
Validation loss: 2.4258072427524033

Epoch: 6| Step: 12
Training loss: 2.5095391273498535
Validation loss: 2.4298290847450175

Epoch: 6| Step: 13
Training loss: 2.344571113586426
Validation loss: 2.4323464388488443

Epoch: 96| Step: 0
Training loss: 2.4762039184570312
Validation loss: 2.4258170768778813

Epoch: 6| Step: 1
Training loss: 2.9971203804016113
Validation loss: 2.4362727313913326

Epoch: 6| Step: 2
Training loss: 2.9471077919006348
Validation loss: 2.4244629157486783

Epoch: 6| Step: 3
Training loss: 2.5449583530426025
Validation loss: 2.4186849773571057

Epoch: 6| Step: 4
Training loss: 2.702620029449463
Validation loss: 2.414176356407904

Epoch: 6| Step: 5
Training loss: 2.481139898300171
Validation loss: 2.4113724923902944

Epoch: 6| Step: 6
Training loss: 2.191850185394287
Validation loss: 2.4059377793342835

Epoch: 6| Step: 7
Training loss: 2.1006505489349365
Validation loss: 2.404567144250357

Epoch: 6| Step: 8
Training loss: 2.8590359687805176
Validation loss: 2.4022699222769788

Epoch: 6| Step: 9
Training loss: 2.94919753074646
Validation loss: 2.4057204877176592

Epoch: 6| Step: 10
Training loss: 2.6808698177337646
Validation loss: 2.4102945507213636

Epoch: 6| Step: 11
Training loss: 3.1956405639648438
Validation loss: 2.4068553140086513

Epoch: 6| Step: 12
Training loss: 2.132841110229492
Validation loss: 2.4076327764859764

Epoch: 6| Step: 13
Training loss: 2.5509326457977295
Validation loss: 2.40570314468876

Epoch: 97| Step: 0
Training loss: 3.0870866775512695
Validation loss: 2.403572374774564

Epoch: 6| Step: 1
Training loss: 3.155534505844116
Validation loss: 2.407199162308888

Epoch: 6| Step: 2
Training loss: 2.020829916000366
Validation loss: 2.4051846381156676

Epoch: 6| Step: 3
Training loss: 2.744210720062256
Validation loss: 2.413896063322662

Epoch: 6| Step: 4
Training loss: 2.700472593307495
Validation loss: 2.415551757299772

Epoch: 6| Step: 5
Training loss: 2.27443265914917
Validation loss: 2.4182183563068347

Epoch: 6| Step: 6
Training loss: 3.163681983947754
Validation loss: 2.4220800040870585

Epoch: 6| Step: 7
Training loss: 3.0389938354492188
Validation loss: 2.4374848745202504

Epoch: 6| Step: 8
Training loss: 3.4078140258789062
Validation loss: 2.4253802145681074

Epoch: 6| Step: 9
Training loss: 2.42329740524292
Validation loss: 2.4219664835160777

Epoch: 6| Step: 10
Training loss: 2.225327968597412
Validation loss: 2.4159356804304224

Epoch: 6| Step: 11
Training loss: 2.1005635261535645
Validation loss: 2.408446423469051

Epoch: 6| Step: 12
Training loss: 2.20719575881958
Validation loss: 2.4215289649143013

Epoch: 6| Step: 13
Training loss: 2.0160720348358154
Validation loss: 2.423495351627309

Epoch: 98| Step: 0
Training loss: 2.610198497772217
Validation loss: 2.4273552984319706

Epoch: 6| Step: 1
Training loss: 2.312643527984619
Validation loss: 2.439477059148973

Epoch: 6| Step: 2
Training loss: 2.5892984867095947
Validation loss: 2.460585653140981

Epoch: 6| Step: 3
Training loss: 2.042477607727051
Validation loss: 2.4654682964406986

Epoch: 6| Step: 4
Training loss: 3.4467010498046875
Validation loss: 2.4706145999252156

Epoch: 6| Step: 5
Training loss: 2.720643997192383
Validation loss: 2.440035066296977

Epoch: 6| Step: 6
Training loss: 2.4481449127197266
Validation loss: 2.4273733990166777

Epoch: 6| Step: 7
Training loss: 3.2856907844543457
Validation loss: 2.40983150851342

Epoch: 6| Step: 8
Training loss: 3.2078933715820312
Validation loss: 2.405634072519118

Epoch: 6| Step: 9
Training loss: 2.844789505004883
Validation loss: 2.398421210627402

Epoch: 6| Step: 10
Training loss: 2.4376440048217773
Validation loss: 2.392366332392539

Epoch: 6| Step: 11
Training loss: 2.288778305053711
Validation loss: 2.3898602083165157

Epoch: 6| Step: 12
Training loss: 1.75475013256073
Validation loss: 2.3874286272192515

Epoch: 6| Step: 13
Training loss: 3.04394793510437
Validation loss: 2.3838413620507843

Epoch: 99| Step: 0
Training loss: 2.3797237873077393
Validation loss: 2.3844748927700903

Epoch: 6| Step: 1
Training loss: 2.3917431831359863
Validation loss: 2.3860783833329395

Epoch: 6| Step: 2
Training loss: 3.01999831199646
Validation loss: 2.3848522042715423

Epoch: 6| Step: 3
Training loss: 2.7189226150512695
Validation loss: 2.380095990755225

Epoch: 6| Step: 4
Training loss: 1.7680184841156006
Validation loss: 2.378850395961474

Epoch: 6| Step: 5
Training loss: 3.2297286987304688
Validation loss: 2.381142803417739

Epoch: 6| Step: 6
Training loss: 2.4777283668518066
Validation loss: 2.381231889929823

Epoch: 6| Step: 7
Training loss: 2.4459681510925293
Validation loss: 2.389112505861508

Epoch: 6| Step: 8
Training loss: 2.399106740951538
Validation loss: 2.401355405007639

Epoch: 6| Step: 9
Training loss: 2.8437747955322266
Validation loss: 2.4283062386256393

Epoch: 6| Step: 10
Training loss: 2.645871162414551
Validation loss: 2.4474400627997612

Epoch: 6| Step: 11
Training loss: 2.6650571823120117
Validation loss: 2.4767115090482976

Epoch: 6| Step: 12
Training loss: 3.0913004875183105
Validation loss: 2.4366119651384253

Epoch: 6| Step: 13
Training loss: 2.9999170303344727
Validation loss: 2.4016692356396745

Epoch: 100| Step: 0
Training loss: 2.60294246673584
Validation loss: 2.3897660445141535

Epoch: 6| Step: 1
Training loss: 2.5455946922302246
Validation loss: 2.381343936407438

Epoch: 6| Step: 2
Training loss: 2.066847801208496
Validation loss: 2.3800139324639433

Epoch: 6| Step: 3
Training loss: 2.866758346557617
Validation loss: 2.3812742258912776

Epoch: 6| Step: 4
Training loss: 2.6511757373809814
Validation loss: 2.3817424210168983

Epoch: 6| Step: 5
Training loss: 2.9607582092285156
Validation loss: 2.3782677137723534

Epoch: 6| Step: 6
Training loss: 1.6902697086334229
Validation loss: 2.3833549919948784

Epoch: 6| Step: 7
Training loss: 3.2661287784576416
Validation loss: 2.3813105693427463

Epoch: 6| Step: 8
Training loss: 2.307090997695923
Validation loss: 2.3798315807055404

Epoch: 6| Step: 9
Training loss: 2.860710859298706
Validation loss: 2.382329974123227

Epoch: 6| Step: 10
Training loss: 2.919034957885742
Validation loss: 2.383777005698091

Epoch: 6| Step: 11
Training loss: 2.954118013381958
Validation loss: 2.38591790968372

Epoch: 6| Step: 12
Training loss: 2.6197762489318848
Validation loss: 2.3898641806776806

Epoch: 6| Step: 13
Training loss: 2.53035831451416
Validation loss: 2.392732999658072

Epoch: 101| Step: 0
Training loss: 2.7851722240448
Validation loss: 2.395228267997824

Epoch: 6| Step: 1
Training loss: 2.551196813583374
Validation loss: 2.420752756057247

Epoch: 6| Step: 2
Training loss: 3.4812841415405273
Validation loss: 2.438599258340815

Epoch: 6| Step: 3
Training loss: 3.230029821395874
Validation loss: 2.4585718185670915

Epoch: 6| Step: 4
Training loss: 2.776815891265869
Validation loss: 2.4419480446846253

Epoch: 6| Step: 5
Training loss: 2.17691707611084
Validation loss: 2.4270763986854145

Epoch: 6| Step: 6
Training loss: 2.553689479827881
Validation loss: 2.420658834518925

Epoch: 6| Step: 7
Training loss: 2.6209921836853027
Validation loss: 2.402661104356089

Epoch: 6| Step: 8
Training loss: 2.3093762397766113
Validation loss: 2.391325678876651

Epoch: 6| Step: 9
Training loss: 2.6058127880096436
Validation loss: 2.3772828809676634

Epoch: 6| Step: 10
Training loss: 2.1077070236206055
Validation loss: 2.3724398689885295

Epoch: 6| Step: 11
Training loss: 2.217276096343994
Validation loss: 2.3691060953242804

Epoch: 6| Step: 12
Training loss: 2.918084144592285
Validation loss: 2.3735644496897215

Epoch: 6| Step: 13
Training loss: 2.3040618896484375
Validation loss: 2.370723065509591

Epoch: 102| Step: 0
Training loss: 3.111149787902832
Validation loss: 2.36944309870402

Epoch: 6| Step: 1
Training loss: 2.694338321685791
Validation loss: 2.3748147308185534

Epoch: 6| Step: 2
Training loss: 3.280482292175293
Validation loss: 2.371168739052229

Epoch: 6| Step: 3
Training loss: 2.312697410583496
Validation loss: 2.3691675868085635

Epoch: 6| Step: 4
Training loss: 3.0364937782287598
Validation loss: 2.3684693895360476

Epoch: 6| Step: 5
Training loss: 2.535191059112549
Validation loss: 2.3647592708628666

Epoch: 6| Step: 6
Training loss: 2.6231327056884766
Validation loss: 2.362714804628844

Epoch: 6| Step: 7
Training loss: 1.77608323097229
Validation loss: 2.3675341965049825

Epoch: 6| Step: 8
Training loss: 2.7475428581237793
Validation loss: 2.3675211860287573

Epoch: 6| Step: 9
Training loss: 2.3477721214294434
Validation loss: 2.369224545776203

Epoch: 6| Step: 10
Training loss: 3.245180606842041
Validation loss: 2.3695675275659047

Epoch: 6| Step: 11
Training loss: 2.1403396129608154
Validation loss: 2.3706856850654847

Epoch: 6| Step: 12
Training loss: 2.039571523666382
Validation loss: 2.3766808561099473

Epoch: 6| Step: 13
Training loss: 2.780858278274536
Validation loss: 2.4011199192334245

Epoch: 103| Step: 0
Training loss: 2.2112622261047363
Validation loss: 2.42643738818425

Epoch: 6| Step: 1
Training loss: 2.266817569732666
Validation loss: 2.442702626669279

Epoch: 6| Step: 2
Training loss: 2.835172176361084
Validation loss: 2.44895585121647

Epoch: 6| Step: 3
Training loss: 3.4139819145202637
Validation loss: 2.4459270008148684

Epoch: 6| Step: 4
Training loss: 2.2832016944885254
Validation loss: 2.4369911993703535

Epoch: 6| Step: 5
Training loss: 2.8448803424835205
Validation loss: 2.4295911737667617

Epoch: 6| Step: 6
Training loss: 2.6837081909179688
Validation loss: 2.4088844663353375

Epoch: 6| Step: 7
Training loss: 2.8475286960601807
Validation loss: 2.3899410001693235

Epoch: 6| Step: 8
Training loss: 1.9457805156707764
Validation loss: 2.3860014510411087

Epoch: 6| Step: 9
Training loss: 2.6893105506896973
Validation loss: 2.378268969956265

Epoch: 6| Step: 10
Training loss: 2.773627758026123
Validation loss: 2.3745837211608887

Epoch: 6| Step: 11
Training loss: 2.5533080101013184
Validation loss: 2.3758915393583235

Epoch: 6| Step: 12
Training loss: 2.8542327880859375
Validation loss: 2.3679775909710954

Epoch: 6| Step: 13
Training loss: 2.607795476913452
Validation loss: 2.3678603262029667

Epoch: 104| Step: 0
Training loss: 2.947556734085083
Validation loss: 2.362321927983274

Epoch: 6| Step: 1
Training loss: 2.9839751720428467
Validation loss: 2.360045071571104

Epoch: 6| Step: 2
Training loss: 1.9362229108810425
Validation loss: 2.3613550637357976

Epoch: 6| Step: 3
Training loss: 2.8942277431488037
Validation loss: 2.3574298427950953

Epoch: 6| Step: 4
Training loss: 2.7072415351867676
Validation loss: 2.3604618810838267

Epoch: 6| Step: 5
Training loss: 2.4464187622070312
Validation loss: 2.371256136125134

Epoch: 6| Step: 6
Training loss: 2.426671028137207
Validation loss: 2.377817289803618

Epoch: 6| Step: 7
Training loss: 2.348984718322754
Validation loss: 2.3808090045887935

Epoch: 6| Step: 8
Training loss: 2.771644115447998
Validation loss: 2.3838002707368586

Epoch: 6| Step: 9
Training loss: 1.9998364448547363
Validation loss: 2.3868153556700675

Epoch: 6| Step: 10
Training loss: 2.790444850921631
Validation loss: 2.3942250949080273

Epoch: 6| Step: 11
Training loss: 2.8773581981658936
Validation loss: 2.394450374828872

Epoch: 6| Step: 12
Training loss: 2.8243556022644043
Validation loss: 2.38989947688195

Epoch: 6| Step: 13
Training loss: 2.8473496437072754
Validation loss: 2.3836173319047496

Epoch: 105| Step: 0
Training loss: 2.734527111053467
Validation loss: 2.372045934841197

Epoch: 6| Step: 1
Training loss: 2.578453779220581
Validation loss: 2.3650998812849804

Epoch: 6| Step: 2
Training loss: 2.694786787033081
Validation loss: 2.3552488844881774

Epoch: 6| Step: 3
Training loss: 2.7058253288269043
Validation loss: 2.359512477792719

Epoch: 6| Step: 4
Training loss: 2.2557175159454346
Validation loss: 2.356839902939335

Epoch: 6| Step: 5
Training loss: 2.375124454498291
Validation loss: 2.3615339186883744

Epoch: 6| Step: 6
Training loss: 2.3356876373291016
Validation loss: 2.3597182919902187

Epoch: 6| Step: 7
Training loss: 2.9312984943389893
Validation loss: 2.359210891108359

Epoch: 6| Step: 8
Training loss: 2.548002243041992
Validation loss: 2.355958474579678

Epoch: 6| Step: 9
Training loss: 3.262655019760132
Validation loss: 2.3578411071531233

Epoch: 6| Step: 10
Training loss: 3.26564359664917
Validation loss: 2.354392892570906

Epoch: 6| Step: 11
Training loss: 2.19024920463562
Validation loss: 2.351850942898822

Epoch: 6| Step: 12
Training loss: 2.0199317932128906
Validation loss: 2.3529595764734412

Epoch: 6| Step: 13
Training loss: 2.9819133281707764
Validation loss: 2.351895655355146

Epoch: 106| Step: 0
Training loss: 3.2277326583862305
Validation loss: 2.3522646170790478

Epoch: 6| Step: 1
Training loss: 2.1708364486694336
Validation loss: 2.3504546829449233

Epoch: 6| Step: 2
Training loss: 2.7094974517822266
Validation loss: 2.3517736516973025

Epoch: 6| Step: 3
Training loss: 2.1010587215423584
Validation loss: 2.355148260311414

Epoch: 6| Step: 4
Training loss: 2.768789291381836
Validation loss: 2.359511585645778

Epoch: 6| Step: 5
Training loss: 2.9165198802948
Validation loss: 2.3648140174086376

Epoch: 6| Step: 6
Training loss: 2.2393546104431152
Validation loss: 2.3677852717779015

Epoch: 6| Step: 7
Training loss: 2.730440855026245
Validation loss: 2.387571116929413

Epoch: 6| Step: 8
Training loss: 2.3094515800476074
Validation loss: 2.413024686997937

Epoch: 6| Step: 9
Training loss: 1.9484261274337769
Validation loss: 2.4217252423686366

Epoch: 6| Step: 10
Training loss: 2.571338176727295
Validation loss: 2.4206959380898425

Epoch: 6| Step: 11
Training loss: 3.0760316848754883
Validation loss: 2.4265383340979136

Epoch: 6| Step: 12
Training loss: 3.104969024658203
Validation loss: 2.4154316507359987

Epoch: 6| Step: 13
Training loss: 2.827136754989624
Validation loss: 2.3887532731538177

Epoch: 107| Step: 0
Training loss: 2.4597020149230957
Validation loss: 2.3679621322180635

Epoch: 6| Step: 1
Training loss: 3.153207778930664
Validation loss: 2.3556781840580765

Epoch: 6| Step: 2
Training loss: 2.74190616607666
Validation loss: 2.3542651617398827

Epoch: 6| Step: 3
Training loss: 2.989168167114258
Validation loss: 2.349866874756352

Epoch: 6| Step: 4
Training loss: 3.232632637023926
Validation loss: 2.3511951943879486

Epoch: 6| Step: 5
Training loss: 2.6362509727478027
Validation loss: 2.357941319865565

Epoch: 6| Step: 6
Training loss: 2.948821544647217
Validation loss: 2.3620632002430577

Epoch: 6| Step: 7
Training loss: 1.916495680809021
Validation loss: 2.3565875791734263

Epoch: 6| Step: 8
Training loss: 3.0383176803588867
Validation loss: 2.3545275683044107

Epoch: 6| Step: 9
Training loss: 2.638234853744507
Validation loss: 2.355524896293558

Epoch: 6| Step: 10
Training loss: 1.7105426788330078
Validation loss: 2.3538127919679046

Epoch: 6| Step: 11
Training loss: 2.0742545127868652
Validation loss: 2.353221047309137

Epoch: 6| Step: 12
Training loss: 2.692843437194824
Validation loss: 2.3516968398965816

Epoch: 6| Step: 13
Training loss: 2.3441405296325684
Validation loss: 2.351782306548088

Epoch: 108| Step: 0
Training loss: 2.617433786392212
Validation loss: 2.3556043230077273

Epoch: 6| Step: 1
Training loss: 2.6283774375915527
Validation loss: 2.3565795716419013

Epoch: 6| Step: 2
Training loss: 2.6878294944763184
Validation loss: 2.3571457837217595

Epoch: 6| Step: 3
Training loss: 2.787386894226074
Validation loss: 2.3625447519363894

Epoch: 6| Step: 4
Training loss: 2.546966075897217
Validation loss: 2.3597562595080306

Epoch: 6| Step: 5
Training loss: 2.9602737426757812
Validation loss: 2.3663612078594904

Epoch: 6| Step: 6
Training loss: 3.050701141357422
Validation loss: 2.3808745004797496

Epoch: 6| Step: 7
Training loss: 3.060594320297241
Validation loss: 2.3841712192822526

Epoch: 6| Step: 8
Training loss: 2.386505603790283
Validation loss: 2.4126362031505955

Epoch: 6| Step: 9
Training loss: 2.342888116836548
Validation loss: 2.4160433174461446

Epoch: 6| Step: 10
Training loss: 2.0809226036071777
Validation loss: 2.4160957874790316

Epoch: 6| Step: 11
Training loss: 2.673679828643799
Validation loss: 2.428159336889944

Epoch: 6| Step: 12
Training loss: 2.5439257621765137
Validation loss: 2.4063104737189507

Epoch: 6| Step: 13
Training loss: 2.025538921356201
Validation loss: 2.404467785230247

Epoch: 109| Step: 0
Training loss: 2.7601394653320312
Validation loss: 2.3808662635023876

Epoch: 6| Step: 1
Training loss: 2.5228488445281982
Validation loss: 2.3711139386700046

Epoch: 6| Step: 2
Training loss: 2.639880895614624
Validation loss: 2.3621721947064964

Epoch: 6| Step: 3
Training loss: 3.130013942718506
Validation loss: 2.359359295137467

Epoch: 6| Step: 4
Training loss: 2.583509683609009
Validation loss: 2.3565200323699624

Epoch: 6| Step: 5
Training loss: 3.598456382751465
Validation loss: 2.3468272557822605

Epoch: 6| Step: 6
Training loss: 2.5694260597229004
Validation loss: 2.3516863802427888

Epoch: 6| Step: 7
Training loss: 2.4697375297546387
Validation loss: 2.350442373624412

Epoch: 6| Step: 8
Training loss: 1.757728099822998
Validation loss: 2.3512825376244

Epoch: 6| Step: 9
Training loss: 2.831854820251465
Validation loss: 2.356951149561072

Epoch: 6| Step: 10
Training loss: 2.3150429725646973
Validation loss: 2.3627017774889545

Epoch: 6| Step: 11
Training loss: 2.849379539489746
Validation loss: 2.359796470211398

Epoch: 6| Step: 12
Training loss: 2.086130380630493
Validation loss: 2.3588003727697555

Epoch: 6| Step: 13
Training loss: 2.1109414100646973
Validation loss: 2.357591650819266

Epoch: 110| Step: 0
Training loss: 3.2985806465148926
Validation loss: 2.3541629468241045

Epoch: 6| Step: 1
Training loss: 2.346184253692627
Validation loss: 2.3497412102196806

Epoch: 6| Step: 2
Training loss: 2.341973304748535
Validation loss: 2.3501903190407702

Epoch: 6| Step: 3
Training loss: 2.1741390228271484
Validation loss: 2.354256171052174

Epoch: 6| Step: 4
Training loss: 2.2886710166931152
Validation loss: 2.3489936026193763

Epoch: 6| Step: 5
Training loss: 2.7680678367614746
Validation loss: 2.348523796245616

Epoch: 6| Step: 6
Training loss: 1.9661468267440796
Validation loss: 2.350085981430546

Epoch: 6| Step: 7
Training loss: 2.8394155502319336
Validation loss: 2.354509994547854

Epoch: 6| Step: 8
Training loss: 2.7434840202331543
Validation loss: 2.3578856965546966

Epoch: 6| Step: 9
Training loss: 3.4994449615478516
Validation loss: 2.36974641840945

Epoch: 6| Step: 10
Training loss: 2.134409189224243
Validation loss: 2.3787653061651413

Epoch: 6| Step: 11
Training loss: 2.2907261848449707
Validation loss: 2.38357445245148

Epoch: 6| Step: 12
Training loss: 2.8766446113586426
Validation loss: 2.406880706869146

Epoch: 6| Step: 13
Training loss: 3.0065455436706543
Validation loss: 2.4340340040063344

Epoch: 111| Step: 0
Training loss: 3.316105842590332
Validation loss: 2.4658229684316986

Epoch: 6| Step: 1
Training loss: 2.215322732925415
Validation loss: 2.4874863701481975

Epoch: 6| Step: 2
Training loss: 3.1595616340637207
Validation loss: 2.4735609100710962

Epoch: 6| Step: 3
Training loss: 2.576054334640503
Validation loss: 2.4332124417827976

Epoch: 6| Step: 4
Training loss: 3.072287082672119
Validation loss: 2.3987580601887037

Epoch: 6| Step: 5
Training loss: 1.97965407371521
Validation loss: 2.3731638539221978

Epoch: 6| Step: 6
Training loss: 2.8673806190490723
Validation loss: 2.3697169032148135

Epoch: 6| Step: 7
Training loss: 1.9690487384796143
Validation loss: 2.353328207487701

Epoch: 6| Step: 8
Training loss: 3.272702932357788
Validation loss: 2.3456405183320403

Epoch: 6| Step: 9
Training loss: 2.873769521713257
Validation loss: 2.348248315113847

Epoch: 6| Step: 10
Training loss: 2.034428119659424
Validation loss: 2.3535567945049656

Epoch: 6| Step: 11
Training loss: 2.377063035964966
Validation loss: 2.3619277323445966

Epoch: 6| Step: 12
Training loss: 2.566962242126465
Validation loss: 2.3815775814876763

Epoch: 6| Step: 13
Training loss: 2.313000202178955
Validation loss: 2.3682818694781234

Epoch: 112| Step: 0
Training loss: 3.1893811225891113
Validation loss: 2.361708741034231

Epoch: 6| Step: 1
Training loss: 1.8489559888839722
Validation loss: 2.3569673517698884

Epoch: 6| Step: 2
Training loss: 2.7675845623016357
Validation loss: 2.3529290973499255

Epoch: 6| Step: 3
Training loss: 2.900639533996582
Validation loss: 2.3558258292495564

Epoch: 6| Step: 4
Training loss: 2.8010332584381104
Validation loss: 2.3648655811945596

Epoch: 6| Step: 5
Training loss: 3.298630714416504
Validation loss: 2.371014141267346

Epoch: 6| Step: 6
Training loss: 2.7010393142700195
Validation loss: 2.365152007790022

Epoch: 6| Step: 7
Training loss: 1.7606934309005737
Validation loss: 2.3663405218432025

Epoch: 6| Step: 8
Training loss: 1.9284157752990723
Validation loss: 2.363289620286675

Epoch: 6| Step: 9
Training loss: 2.8279430866241455
Validation loss: 2.3535185706230903

Epoch: 6| Step: 10
Training loss: 2.1989336013793945
Validation loss: 2.349963229189637

Epoch: 6| Step: 11
Training loss: 2.6422674655914307
Validation loss: 2.3422477911877375

Epoch: 6| Step: 12
Training loss: 2.857006311416626
Validation loss: 2.3364639564227034

Epoch: 6| Step: 13
Training loss: 2.6973178386688232
Validation loss: 2.3342102548127532

Epoch: 113| Step: 0
Training loss: 2.201667308807373
Validation loss: 2.336431937832986

Epoch: 6| Step: 1
Training loss: 2.542649745941162
Validation loss: 2.3332922740649154

Epoch: 6| Step: 2
Training loss: 2.8292183876037598
Validation loss: 2.337479019677767

Epoch: 6| Step: 3
Training loss: 2.955331802368164
Validation loss: 2.337022235316615

Epoch: 6| Step: 4
Training loss: 2.537904739379883
Validation loss: 2.3389444658833165

Epoch: 6| Step: 5
Training loss: 3.4735898971557617
Validation loss: 2.3402136935982654

Epoch: 6| Step: 6
Training loss: 2.6685338020324707
Validation loss: 2.338702481280091

Epoch: 6| Step: 7
Training loss: 2.183995246887207
Validation loss: 2.340159321344027

Epoch: 6| Step: 8
Training loss: 1.9109654426574707
Validation loss: 2.3426466705978557

Epoch: 6| Step: 9
Training loss: 2.3650622367858887
Validation loss: 2.3434919067608413

Epoch: 6| Step: 10
Training loss: 3.2126305103302
Validation loss: 2.3448655451497724

Epoch: 6| Step: 11
Training loss: 2.251162528991699
Validation loss: 2.342127646169355

Epoch: 6| Step: 12
Training loss: 2.669783115386963
Validation loss: 2.341641946505475

Epoch: 6| Step: 13
Training loss: 2.6654367446899414
Validation loss: 2.347258456291691

Epoch: 114| Step: 0
Training loss: 1.893052339553833
Validation loss: 2.3392365299245363

Epoch: 6| Step: 1
Training loss: 2.871150493621826
Validation loss: 2.339601088595647

Epoch: 6| Step: 2
Training loss: 3.3488881587982178
Validation loss: 2.3505459498333674

Epoch: 6| Step: 3
Training loss: 3.1077613830566406
Validation loss: 2.3554082557719243

Epoch: 6| Step: 4
Training loss: 2.3347415924072266
Validation loss: 2.3685328216962915

Epoch: 6| Step: 5
Training loss: 3.099560260772705
Validation loss: 2.3739361916818926

Epoch: 6| Step: 6
Training loss: 1.700768232345581
Validation loss: 2.376556875885174

Epoch: 6| Step: 7
Training loss: 2.07905912399292
Validation loss: 2.3779453000714703

Epoch: 6| Step: 8
Training loss: 2.7849626541137695
Validation loss: 2.3639943984247025

Epoch: 6| Step: 9
Training loss: 2.706144332885742
Validation loss: 2.355077669184695

Epoch: 6| Step: 10
Training loss: 2.9394593238830566
Validation loss: 2.339597912244899

Epoch: 6| Step: 11
Training loss: 2.6909677982330322
Validation loss: 2.33418357500466

Epoch: 6| Step: 12
Training loss: 2.647459030151367
Validation loss: 2.332232757281232

Epoch: 6| Step: 13
Training loss: 1.9948793649673462
Validation loss: 2.327456805013841

Epoch: 115| Step: 0
Training loss: 2.7551655769348145
Validation loss: 2.3258484794247534

Epoch: 6| Step: 1
Training loss: 2.3720827102661133
Validation loss: 2.330222318249364

Epoch: 6| Step: 2
Training loss: 2.429190158843994
Validation loss: 2.3317862531190277

Epoch: 6| Step: 3
Training loss: 2.1759915351867676
Validation loss: 2.338647278406287

Epoch: 6| Step: 4
Training loss: 2.5035336017608643
Validation loss: 2.343343001539989

Epoch: 6| Step: 5
Training loss: 2.662480354309082
Validation loss: 2.3517533374089066

Epoch: 6| Step: 6
Training loss: 2.452296257019043
Validation loss: 2.342228727955972

Epoch: 6| Step: 7
Training loss: 2.4589879512786865
Validation loss: 2.3496176555592525

Epoch: 6| Step: 8
Training loss: 3.2958273887634277
Validation loss: 2.3562147771158526

Epoch: 6| Step: 9
Training loss: 2.351454019546509
Validation loss: 2.3496191475981023

Epoch: 6| Step: 10
Training loss: 2.7247514724731445
Validation loss: 2.353539615549067

Epoch: 6| Step: 11
Training loss: 3.3040919303894043
Validation loss: 2.3491031739019577

Epoch: 6| Step: 12
Training loss: 2.3649137020111084
Validation loss: 2.35306518308578

Epoch: 6| Step: 13
Training loss: 2.44803524017334
Validation loss: 2.354724463596139

Epoch: 116| Step: 0
Training loss: 2.594245433807373
Validation loss: 2.362756908580821

Epoch: 6| Step: 1
Training loss: 2.369260787963867
Validation loss: 2.3510759466437885

Epoch: 6| Step: 2
Training loss: 2.1117184162139893
Validation loss: 2.3443344793012066

Epoch: 6| Step: 3
Training loss: 2.148627519607544
Validation loss: 2.345197089256779

Epoch: 6| Step: 4
Training loss: 2.417668581008911
Validation loss: 2.341194696323846

Epoch: 6| Step: 5
Training loss: 2.1949639320373535
Validation loss: 2.3401253684874503

Epoch: 6| Step: 6
Training loss: 2.595893383026123
Validation loss: 2.330334637754707

Epoch: 6| Step: 7
Training loss: 2.8597497940063477
Validation loss: 2.334065111734534

Epoch: 6| Step: 8
Training loss: 2.54449462890625
Validation loss: 2.335336192961662

Epoch: 6| Step: 9
Training loss: 2.6220552921295166
Validation loss: 2.3325004603273127

Epoch: 6| Step: 10
Training loss: 2.0495362281799316
Validation loss: 2.3371205637531896

Epoch: 6| Step: 11
Training loss: 2.740959405899048
Validation loss: 2.3317203162818827

Epoch: 6| Step: 12
Training loss: 4.044003486633301
Validation loss: 2.335602160423033

Epoch: 6| Step: 13
Training loss: 3.2596116065979004
Validation loss: 2.3348061910239597

Epoch: 117| Step: 0
Training loss: 2.2020580768585205
Validation loss: 2.336940401343889

Epoch: 6| Step: 1
Training loss: 2.286351203918457
Validation loss: 2.3364216037975845

Epoch: 6| Step: 2
Training loss: 3.318793296813965
Validation loss: 2.333861152331034

Epoch: 6| Step: 3
Training loss: 2.18034029006958
Validation loss: 2.33787493167385

Epoch: 6| Step: 4
Training loss: 2.617286205291748
Validation loss: 2.3416194685043825

Epoch: 6| Step: 5
Training loss: 2.2282941341400146
Validation loss: 2.3375726463974162

Epoch: 6| Step: 6
Training loss: 2.999375104904175
Validation loss: 2.347068243129279

Epoch: 6| Step: 7
Training loss: 3.284090995788574
Validation loss: 2.369349695021106

Epoch: 6| Step: 8
Training loss: 2.6998953819274902
Validation loss: 2.3941413843503563

Epoch: 6| Step: 9
Training loss: 2.0102949142456055
Validation loss: 2.3994019980071695

Epoch: 6| Step: 10
Training loss: 2.4570698738098145
Validation loss: 2.3972506497495916

Epoch: 6| Step: 11
Training loss: 2.465378522872925
Validation loss: 2.40451632776568

Epoch: 6| Step: 12
Training loss: 3.0579447746276855
Validation loss: 2.3876150397844214

Epoch: 6| Step: 13
Training loss: 2.6722710132598877
Validation loss: 2.371348709188482

Epoch: 118| Step: 0
Training loss: 2.82487154006958
Validation loss: 2.3538557316667292

Epoch: 6| Step: 1
Training loss: 2.2216620445251465
Validation loss: 2.3375133378531343

Epoch: 6| Step: 2
Training loss: 2.8581080436706543
Validation loss: 2.3285907571033766

Epoch: 6| Step: 3
Training loss: 3.1706724166870117
Validation loss: 2.326078179062054

Epoch: 6| Step: 4
Training loss: 2.7228031158447266
Validation loss: 2.3223616025781118

Epoch: 6| Step: 5
Training loss: 2.3921890258789062
Validation loss: 2.324092898317563

Epoch: 6| Step: 6
Training loss: 1.6835095882415771
Validation loss: 2.3234288461746706

Epoch: 6| Step: 7
Training loss: 2.6506593227386475
Validation loss: 2.3277676502863565

Epoch: 6| Step: 8
Training loss: 2.9729385375976562
Validation loss: 2.331302627440422

Epoch: 6| Step: 9
Training loss: 2.8618686199188232
Validation loss: 2.32955874166181

Epoch: 6| Step: 10
Training loss: 1.8072047233581543
Validation loss: 2.3299634943726244

Epoch: 6| Step: 11
Training loss: 1.9400578737258911
Validation loss: 2.328539881654965

Epoch: 6| Step: 12
Training loss: 3.012734889984131
Validation loss: 2.3338500351034184

Epoch: 6| Step: 13
Training loss: 3.4984467029571533
Validation loss: 2.3325754660432056

Epoch: 119| Step: 0
Training loss: 2.700266122817993
Validation loss: 2.330822208876251

Epoch: 6| Step: 1
Training loss: 2.3334057331085205
Validation loss: 2.3334565265204317

Epoch: 6| Step: 2
Training loss: 2.249917507171631
Validation loss: 2.333182504100184

Epoch: 6| Step: 3
Training loss: 2.701087236404419
Validation loss: 2.3363159753943004

Epoch: 6| Step: 4
Training loss: 2.450723648071289
Validation loss: 2.334906449881933

Epoch: 6| Step: 5
Training loss: 2.2352139949798584
Validation loss: 2.329400393270677

Epoch: 6| Step: 6
Training loss: 2.834290027618408
Validation loss: 2.334930737813314

Epoch: 6| Step: 7
Training loss: 3.301055669784546
Validation loss: 2.3425295481117825

Epoch: 6| Step: 8
Training loss: 3.009521961212158
Validation loss: 2.339668345707719

Epoch: 6| Step: 9
Training loss: 2.7336206436157227
Validation loss: 2.3405383094664542

Epoch: 6| Step: 10
Training loss: 1.9346280097961426
Validation loss: 2.3368680041323424

Epoch: 6| Step: 11
Training loss: 1.6619006395339966
Validation loss: 2.3406413806382047

Epoch: 6| Step: 12
Training loss: 2.9451496601104736
Validation loss: 2.3383632270238732

Epoch: 6| Step: 13
Training loss: 3.336747646331787
Validation loss: 2.3453594817910144

Epoch: 120| Step: 0
Training loss: 2.6007840633392334
Validation loss: 2.344193935394287

Epoch: 6| Step: 1
Training loss: 1.9326283931732178
Validation loss: 2.3577097513342418

Epoch: 6| Step: 2
Training loss: 2.832801342010498
Validation loss: 2.365830321465769

Epoch: 6| Step: 3
Training loss: 2.4072680473327637
Validation loss: 2.3832734246407785

Epoch: 6| Step: 4
Training loss: 3.2831013202667236
Validation loss: 2.3717940827851653

Epoch: 6| Step: 5
Training loss: 2.929607391357422
Validation loss: 2.34639230082112

Epoch: 6| Step: 6
Training loss: 2.4914181232452393
Validation loss: 2.343942678102883

Epoch: 6| Step: 7
Training loss: 2.2027182579040527
Validation loss: 2.328472552760955

Epoch: 6| Step: 8
Training loss: 2.6251254081726074
Validation loss: 2.323906845943902

Epoch: 6| Step: 9
Training loss: 2.826436996459961
Validation loss: 2.321986101006949

Epoch: 6| Step: 10
Training loss: 2.610978126525879
Validation loss: 2.3204850663420973

Epoch: 6| Step: 11
Training loss: 2.4697773456573486
Validation loss: 2.321372006529121

Epoch: 6| Step: 12
Training loss: 2.2796382904052734
Validation loss: 2.320050311344926

Epoch: 6| Step: 13
Training loss: 2.956475257873535
Validation loss: 2.3209842276829544

Epoch: 121| Step: 0
Training loss: 1.9768810272216797
Validation loss: 2.3259182924865396

Epoch: 6| Step: 1
Training loss: 2.1292505264282227
Validation loss: 2.318695422141783

Epoch: 6| Step: 2
Training loss: 2.577939987182617
Validation loss: 2.321420923356087

Epoch: 6| Step: 3
Training loss: 2.2833995819091797
Validation loss: 2.3236907682111188

Epoch: 6| Step: 4
Training loss: 2.0287482738494873
Validation loss: 2.3343679392209618

Epoch: 6| Step: 5
Training loss: 3.1371395587921143
Validation loss: 2.3419399440929456

Epoch: 6| Step: 6
Training loss: 2.916550397872925
Validation loss: 2.3479557396263204

Epoch: 6| Step: 7
Training loss: 2.948836326599121
Validation loss: 2.341632604598999

Epoch: 6| Step: 8
Training loss: 3.0357842445373535
Validation loss: 2.331580954213296

Epoch: 6| Step: 9
Training loss: 2.5949413776397705
Validation loss: 2.337746827833114

Epoch: 6| Step: 10
Training loss: 2.7865381240844727
Validation loss: 2.3576386667067006

Epoch: 6| Step: 11
Training loss: 2.241142749786377
Validation loss: 2.353516786329208

Epoch: 6| Step: 12
Training loss: 2.977874517440796
Validation loss: 2.3364742955853863

Epoch: 6| Step: 13
Training loss: 2.3855230808258057
Validation loss: 2.3189024527867637

Epoch: 122| Step: 0
Training loss: 3.2636499404907227
Validation loss: 2.309364270138484

Epoch: 6| Step: 1
Training loss: 2.378509044647217
Validation loss: 2.304427659639748

Epoch: 6| Step: 2
Training loss: 2.2755770683288574
Validation loss: 2.3007210352087535

Epoch: 6| Step: 3
Training loss: 2.4712114334106445
Validation loss: 2.3053805392275573

Epoch: 6| Step: 4
Training loss: 2.3146660327911377
Validation loss: 2.304993324382331

Epoch: 6| Step: 5
Training loss: 2.293684482574463
Validation loss: 2.305835936659126

Epoch: 6| Step: 6
Training loss: 2.1633048057556152
Validation loss: 2.3089905502975627

Epoch: 6| Step: 7
Training loss: 2.4927291870117188
Validation loss: 2.3029843427801646

Epoch: 6| Step: 8
Training loss: 2.9374947547912598
Validation loss: 2.298479228891352

Epoch: 6| Step: 9
Training loss: 3.2593555450439453
Validation loss: 2.300099668964263

Epoch: 6| Step: 10
Training loss: 2.997797727584839
Validation loss: 2.313160784782902

Epoch: 6| Step: 11
Training loss: 3.2444896697998047
Validation loss: 2.3149192487039874

Epoch: 6| Step: 12
Training loss: 2.2458128929138184
Validation loss: 2.330593245003813

Epoch: 6| Step: 13
Training loss: 1.265725016593933
Validation loss: 2.346023682625063

Epoch: 123| Step: 0
Training loss: 2.3593621253967285
Validation loss: 2.354523217806252

Epoch: 6| Step: 1
Training loss: 3.0028624534606934
Validation loss: 2.3811949017227336

Epoch: 6| Step: 2
Training loss: 1.836961269378662
Validation loss: 2.410595509313768

Epoch: 6| Step: 3
Training loss: 2.152327299118042
Validation loss: 2.3933528341272825

Epoch: 6| Step: 4
Training loss: 3.36934757232666
Validation loss: 2.3683017069293606

Epoch: 6| Step: 5
Training loss: 2.139770269393921
Validation loss: 2.352453348457172

Epoch: 6| Step: 6
Training loss: 3.6723756790161133
Validation loss: 2.3310443919192076

Epoch: 6| Step: 7
Training loss: 2.851985454559326
Validation loss: 2.306921210340274

Epoch: 6| Step: 8
Training loss: 2.360785961151123
Validation loss: 2.3054208165855816

Epoch: 6| Step: 9
Training loss: 2.777714252471924
Validation loss: 2.2948494777884534

Epoch: 6| Step: 10
Training loss: 3.519260883331299
Validation loss: 2.296187166244753

Epoch: 6| Step: 11
Training loss: 1.8895928859710693
Validation loss: 2.299791354005055

Epoch: 6| Step: 12
Training loss: 1.8589324951171875
Validation loss: 2.2952000248816704

Epoch: 6| Step: 13
Training loss: 2.352726459503174
Validation loss: 2.2964180413112847

Epoch: 124| Step: 0
Training loss: 2.271113872528076
Validation loss: 2.295343477238891

Epoch: 6| Step: 1
Training loss: 2.3022875785827637
Validation loss: 2.2926171556595834

Epoch: 6| Step: 2
Training loss: 2.376286268234253
Validation loss: 2.293053703923379

Epoch: 6| Step: 3
Training loss: 2.7857322692871094
Validation loss: 2.296668556428725

Epoch: 6| Step: 4
Training loss: 3.0999181270599365
Validation loss: 2.292181073978383

Epoch: 6| Step: 5
Training loss: 2.7633914947509766
Validation loss: 2.295974859627344

Epoch: 6| Step: 6
Training loss: 2.5857033729553223
Validation loss: 2.298320654899843

Epoch: 6| Step: 7
Training loss: 3.1594619750976562
Validation loss: 2.3016203065072336

Epoch: 6| Step: 8
Training loss: 2.7415218353271484
Validation loss: 2.3005391320874615

Epoch: 6| Step: 9
Training loss: 2.3901281356811523
Validation loss: 2.3028311549976306

Epoch: 6| Step: 10
Training loss: 1.7323225736618042
Validation loss: 2.307366865937428

Epoch: 6| Step: 11
Training loss: 2.282402753829956
Validation loss: 2.3208311732097338

Epoch: 6| Step: 12
Training loss: 2.673504114151001
Validation loss: 2.323445373965848

Epoch: 6| Step: 13
Training loss: 3.0799074172973633
Validation loss: 2.336971905923659

Epoch: 125| Step: 0
Training loss: 2.455174446105957
Validation loss: 2.3323522126802834

Epoch: 6| Step: 1
Training loss: 2.7833971977233887
Validation loss: 2.330586359065066

Epoch: 6| Step: 2
Training loss: 1.6395509243011475
Validation loss: 2.3218497050705778

Epoch: 6| Step: 3
Training loss: 2.6132380962371826
Validation loss: 2.3043400420937488

Epoch: 6| Step: 4
Training loss: 2.699986696243286
Validation loss: 2.299506231020856

Epoch: 6| Step: 5
Training loss: 2.524289131164551
Validation loss: 2.2989971894089893

Epoch: 6| Step: 6
Training loss: 2.5566933155059814
Validation loss: 2.2949248616413405

Epoch: 6| Step: 7
Training loss: 2.659799575805664
Validation loss: 2.2927415781123663

Epoch: 6| Step: 8
Training loss: 2.3388748168945312
Validation loss: 2.299847128570721

Epoch: 6| Step: 9
Training loss: 2.9517664909362793
Validation loss: 2.2942700180956113

Epoch: 6| Step: 10
Training loss: 2.0100865364074707
Validation loss: 2.29016782904184

Epoch: 6| Step: 11
Training loss: 2.915367841720581
Validation loss: 2.288857765095208

Epoch: 6| Step: 12
Training loss: 2.842557907104492
Validation loss: 2.2904444586846138

Epoch: 6| Step: 13
Training loss: 3.149061441421509
Validation loss: 2.3000958683670207

Epoch: 126| Step: 0
Training loss: 2.7087295055389404
Validation loss: 2.306881768729097

Epoch: 6| Step: 1
Training loss: 2.8470025062561035
Validation loss: 2.330337816669095

Epoch: 6| Step: 2
Training loss: 2.6287307739257812
Validation loss: 2.3584895313427015

Epoch: 6| Step: 3
Training loss: 2.0628085136413574
Validation loss: 2.3750928653183805

Epoch: 6| Step: 4
Training loss: 2.7039051055908203
Validation loss: 2.3630606230869087

Epoch: 6| Step: 5
Training loss: 2.3010594844818115
Validation loss: 2.340170644944714

Epoch: 6| Step: 6
Training loss: 2.2589969635009766
Validation loss: 2.3189181089401245

Epoch: 6| Step: 7
Training loss: 2.6350111961364746
Validation loss: 2.3131865121984996

Epoch: 6| Step: 8
Training loss: 1.801868200302124
Validation loss: 2.2936793860568794

Epoch: 6| Step: 9
Training loss: 3.2066798210144043
Validation loss: 2.29772420852415

Epoch: 6| Step: 10
Training loss: 3.022167682647705
Validation loss: 2.293220430292109

Epoch: 6| Step: 11
Training loss: 2.4770936965942383
Validation loss: 2.2977131105238393

Epoch: 6| Step: 12
Training loss: 2.959459066390991
Validation loss: 2.294888614326395

Epoch: 6| Step: 13
Training loss: 1.9912400245666504
Validation loss: 2.29889900325447

Epoch: 127| Step: 0
Training loss: 2.489252805709839
Validation loss: 2.301208467893703

Epoch: 6| Step: 1
Training loss: 2.604233503341675
Validation loss: 2.298490265364288

Epoch: 6| Step: 2
Training loss: 2.3672077655792236
Validation loss: 2.2997877982354935

Epoch: 6| Step: 3
Training loss: 2.845335006713867
Validation loss: 2.307056147565124

Epoch: 6| Step: 4
Training loss: 2.8087596893310547
Validation loss: 2.28667276905429

Epoch: 6| Step: 5
Training loss: 2.1682748794555664
Validation loss: 2.285080058600313

Epoch: 6| Step: 6
Training loss: 1.8553500175476074
Validation loss: 2.2800672951564995

Epoch: 6| Step: 7
Training loss: 3.007176637649536
Validation loss: 2.284490300763038

Epoch: 6| Step: 8
Training loss: 2.767918586730957
Validation loss: 2.286363873430478

Epoch: 6| Step: 9
Training loss: 2.3898167610168457
Validation loss: 2.290510105830367

Epoch: 6| Step: 10
Training loss: 2.508376359939575
Validation loss: 2.296648988159754

Epoch: 6| Step: 11
Training loss: 2.7820024490356445
Validation loss: 2.298002217405586

Epoch: 6| Step: 12
Training loss: 2.871896505355835
Validation loss: 2.305529494439402

Epoch: 6| Step: 13
Training loss: 2.471085548400879
Validation loss: 2.32394455581583

Epoch: 128| Step: 0
Training loss: 3.412297010421753
Validation loss: 2.3262299824786443

Epoch: 6| Step: 1
Training loss: 2.0304605960845947
Validation loss: 2.336483661846448

Epoch: 6| Step: 2
Training loss: 2.567521572113037
Validation loss: 2.3445747180651595

Epoch: 6| Step: 3
Training loss: 2.4762091636657715
Validation loss: 2.3267312165229552

Epoch: 6| Step: 4
Training loss: 1.8716938495635986
Validation loss: 2.323779421467935

Epoch: 6| Step: 5
Training loss: 2.193668842315674
Validation loss: 2.3276158199515393

Epoch: 6| Step: 6
Training loss: 2.6530990600585938
Validation loss: 2.336501529139857

Epoch: 6| Step: 7
Training loss: 2.633185386657715
Validation loss: 2.3591708470416326

Epoch: 6| Step: 8
Training loss: 2.9535787105560303
Validation loss: 2.389470102966473

Epoch: 6| Step: 9
Training loss: 1.6846377849578857
Validation loss: 2.4182523065997708

Epoch: 6| Step: 10
Training loss: 2.8580174446105957
Validation loss: 2.4241535535422702

Epoch: 6| Step: 11
Training loss: 3.245422124862671
Validation loss: 2.393506162910051

Epoch: 6| Step: 12
Training loss: 3.233415126800537
Validation loss: 2.3732696092256935

Epoch: 6| Step: 13
Training loss: 2.164078950881958
Validation loss: 2.348517249989253

Epoch: 129| Step: 0
Training loss: 2.7025094032287598
Validation loss: 2.335173911945794

Epoch: 6| Step: 1
Training loss: 2.3808577060699463
Validation loss: 2.3170024810298795

Epoch: 6| Step: 2
Training loss: 2.6536736488342285
Validation loss: 2.3139985992062475

Epoch: 6| Step: 3
Training loss: 1.7667231559753418
Validation loss: 2.3144395120682253

Epoch: 6| Step: 4
Training loss: 2.408266544342041
Validation loss: 2.320213322998375

Epoch: 6| Step: 5
Training loss: 2.44600772857666
Validation loss: 2.3348874020320114

Epoch: 6| Step: 6
Training loss: 1.791573405265808
Validation loss: 2.3391104154689337

Epoch: 6| Step: 7
Training loss: 3.2497453689575195
Validation loss: 2.343832662028651

Epoch: 6| Step: 8
Training loss: 2.8309311866760254
Validation loss: 2.339913845062256

Epoch: 6| Step: 9
Training loss: 3.1236460208892822
Validation loss: 2.3331130884026967

Epoch: 6| Step: 10
Training loss: 2.6396546363830566
Validation loss: 2.3417219397842244

Epoch: 6| Step: 11
Training loss: 3.068737030029297
Validation loss: 2.3338060840483634

Epoch: 6| Step: 12
Training loss: 1.978793978691101
Validation loss: 2.3312709305876043

Epoch: 6| Step: 13
Training loss: 3.134809732437134
Validation loss: 2.3372281879507084

Epoch: 130| Step: 0
Training loss: 2.7719902992248535
Validation loss: 2.3302445437318537

Epoch: 6| Step: 1
Training loss: 1.8183889389038086
Validation loss: 2.3108810609386814

Epoch: 6| Step: 2
Training loss: 2.9444546699523926
Validation loss: 2.311825447185065

Epoch: 6| Step: 3
Training loss: 2.129237413406372
Validation loss: 2.2961518072312876

Epoch: 6| Step: 4
Training loss: 2.5705552101135254
Validation loss: 2.2970814602349394

Epoch: 6| Step: 5
Training loss: 2.853449821472168
Validation loss: 2.282163384140179

Epoch: 6| Step: 6
Training loss: 3.445868730545044
Validation loss: 2.2779809941527662

Epoch: 6| Step: 7
Training loss: 2.6461141109466553
Validation loss: 2.278689102459979

Epoch: 6| Step: 8
Training loss: 3.167788505554199
Validation loss: 2.2761581482425814

Epoch: 6| Step: 9
Training loss: 1.911849021911621
Validation loss: 2.2770193212775776

Epoch: 6| Step: 10
Training loss: 2.4493494033813477
Validation loss: 2.276793664501559

Epoch: 6| Step: 11
Training loss: 2.5240626335144043
Validation loss: 2.284498958177464

Epoch: 6| Step: 12
Training loss: 2.0072789192199707
Validation loss: 2.278298093426612

Epoch: 6| Step: 13
Training loss: 2.5050554275512695
Validation loss: 2.281942072735038

Epoch: 131| Step: 0
Training loss: 2.654676914215088
Validation loss: 2.2859375784474034

Epoch: 6| Step: 1
Training loss: 2.18241548538208
Validation loss: 2.2977819494021836

Epoch: 6| Step: 2
Training loss: 2.538848400115967
Validation loss: 2.309063321800642

Epoch: 6| Step: 3
Training loss: 1.8104257583618164
Validation loss: 2.317463198015767

Epoch: 6| Step: 4
Training loss: 2.6292476654052734
Validation loss: 2.3288356565660044

Epoch: 6| Step: 5
Training loss: 2.7182743549346924
Validation loss: 2.3362814252094557

Epoch: 6| Step: 6
Training loss: 3.338754653930664
Validation loss: 2.3301463844955608

Epoch: 6| Step: 7
Training loss: 1.9291999340057373
Validation loss: 2.313140730704031

Epoch: 6| Step: 8
Training loss: 1.7635245323181152
Validation loss: 2.341689230293356

Epoch: 6| Step: 9
Training loss: 3.1360154151916504
Validation loss: 2.3411725823597243

Epoch: 6| Step: 10
Training loss: 3.5627098083496094
Validation loss: 2.33741751281164

Epoch: 6| Step: 11
Training loss: 2.2559092044830322
Validation loss: 2.3115764792247484

Epoch: 6| Step: 12
Training loss: 3.0509305000305176
Validation loss: 2.3057795032378166

Epoch: 6| Step: 13
Training loss: 2.0477635860443115
Validation loss: 2.286280635864504

Epoch: 132| Step: 0
Training loss: 3.0401124954223633
Validation loss: 2.2786861773460143

Epoch: 6| Step: 1
Training loss: 1.855989933013916
Validation loss: 2.2734327008647304

Epoch: 6| Step: 2
Training loss: 1.7063528299331665
Validation loss: 2.2737357436969714

Epoch: 6| Step: 3
Training loss: 2.4359030723571777
Validation loss: 2.276867174333142

Epoch: 6| Step: 4
Training loss: 2.4661402702331543
Validation loss: 2.2815218305075042

Epoch: 6| Step: 5
Training loss: 2.4174563884735107
Validation loss: 2.2831953417870308

Epoch: 6| Step: 6
Training loss: 2.473839282989502
Validation loss: 2.293102664332236

Epoch: 6| Step: 7
Training loss: 2.805523157119751
Validation loss: 2.302299876366892

Epoch: 6| Step: 8
Training loss: 2.639313220977783
Validation loss: 2.301930924897553

Epoch: 6| Step: 9
Training loss: 2.8489904403686523
Validation loss: 2.3007363170705815

Epoch: 6| Step: 10
Training loss: 2.8454110622406006
Validation loss: 2.2917685739455687

Epoch: 6| Step: 11
Training loss: 2.665593147277832
Validation loss: 2.295175503658992

Epoch: 6| Step: 12
Training loss: 3.0772008895874023
Validation loss: 2.2842732552559144

Epoch: 6| Step: 13
Training loss: 2.4483213424682617
Validation loss: 2.2864087038142706

Epoch: 133| Step: 0
Training loss: 2.116671562194824
Validation loss: 2.292933246140839

Epoch: 6| Step: 1
Training loss: 2.802507162094116
Validation loss: 2.2924019931465067

Epoch: 6| Step: 2
Training loss: 2.152784824371338
Validation loss: 2.2890340666617117

Epoch: 6| Step: 3
Training loss: 2.3735697269439697
Validation loss: 2.2850504664964575

Epoch: 6| Step: 4
Training loss: 2.1181483268737793
Validation loss: 2.285374028708345

Epoch: 6| Step: 5
Training loss: 2.7886972427368164
Validation loss: 2.283520274264838

Epoch: 6| Step: 6
Training loss: 2.6378087997436523
Validation loss: 2.285270062826013

Epoch: 6| Step: 7
Training loss: 2.5392837524414062
Validation loss: 2.279990583337763

Epoch: 6| Step: 8
Training loss: 2.3899524211883545
Validation loss: 2.278241144713535

Epoch: 6| Step: 9
Training loss: 2.439467191696167
Validation loss: 2.285973423270769

Epoch: 6| Step: 10
Training loss: 2.7703428268432617
Validation loss: 2.2837645981901433

Epoch: 6| Step: 11
Training loss: 3.034177303314209
Validation loss: 2.2814518379908737

Epoch: 6| Step: 12
Training loss: 2.623701810836792
Validation loss: 2.27322982716304

Epoch: 6| Step: 13
Training loss: 3.017765998840332
Validation loss: 2.285219387341571

Epoch: 134| Step: 0
Training loss: 2.5348310470581055
Validation loss: 2.2885582793143486

Epoch: 6| Step: 1
Training loss: 3.0429720878601074
Validation loss: 2.284578997601745

Epoch: 6| Step: 2
Training loss: 2.5628881454467773
Validation loss: 2.2902929757231023

Epoch: 6| Step: 3
Training loss: 2.275022268295288
Validation loss: 2.2859504915052846

Epoch: 6| Step: 4
Training loss: 2.0946507453918457
Validation loss: 2.291690839234219

Epoch: 6| Step: 5
Training loss: 2.1746718883514404
Validation loss: 2.3004766125832834

Epoch: 6| Step: 6
Training loss: 2.977930784225464
Validation loss: 2.284302839668848

Epoch: 6| Step: 7
Training loss: 2.1475343704223633
Validation loss: 2.272766859300675

Epoch: 6| Step: 8
Training loss: 2.074676036834717
Validation loss: 2.273353836869681

Epoch: 6| Step: 9
Training loss: 2.6175899505615234
Validation loss: 2.27494078810497

Epoch: 6| Step: 10
Training loss: 2.528071880340576
Validation loss: 2.2738699733570056

Epoch: 6| Step: 11
Training loss: 2.5028507709503174
Validation loss: 2.271651970442905

Epoch: 6| Step: 12
Training loss: 2.985586404800415
Validation loss: 2.2805452500620196

Epoch: 6| Step: 13
Training loss: 3.381545066833496
Validation loss: 2.288649623112012

Epoch: 135| Step: 0
Training loss: 2.0772318840026855
Validation loss: 2.3144359947532736

Epoch: 6| Step: 1
Training loss: 2.3642826080322266
Validation loss: 2.313282915340957

Epoch: 6| Step: 2
Training loss: 2.578507661819458
Validation loss: 2.2934805885437997

Epoch: 6| Step: 3
Training loss: 2.4480514526367188
Validation loss: 2.2796547259053876

Epoch: 6| Step: 4
Training loss: 2.005248546600342
Validation loss: 2.270920020277782

Epoch: 6| Step: 5
Training loss: 2.821448802947998
Validation loss: 2.2630449559098933

Epoch: 6| Step: 6
Training loss: 2.9559574127197266
Validation loss: 2.2629595007947696

Epoch: 6| Step: 7
Training loss: 2.427894115447998
Validation loss: 2.2589447959776847

Epoch: 6| Step: 8
Training loss: 2.6057538986206055
Validation loss: 2.264525795495638

Epoch: 6| Step: 9
Training loss: 2.511413097381592
Validation loss: 2.260620109496578

Epoch: 6| Step: 10
Training loss: 2.287398099899292
Validation loss: 2.2655447529208277

Epoch: 6| Step: 11
Training loss: 2.467973232269287
Validation loss: 2.260886505085935

Epoch: 6| Step: 12
Training loss: 3.2604000568389893
Validation loss: 2.2663224256166847

Epoch: 6| Step: 13
Training loss: 2.9759843349456787
Validation loss: 2.2749979444729385

Epoch: 136| Step: 0
Training loss: 2.368837833404541
Validation loss: 2.2797645573974936

Epoch: 6| Step: 1
Training loss: 2.466099500656128
Validation loss: 2.3040154723710913

Epoch: 6| Step: 2
Training loss: 2.6847925186157227
Validation loss: 2.3079210173699165

Epoch: 6| Step: 3
Training loss: 3.115678310394287
Validation loss: 2.2986699688819145

Epoch: 6| Step: 4
Training loss: 1.85178804397583
Validation loss: 2.3007166231832197

Epoch: 6| Step: 5
Training loss: 3.606884002685547
Validation loss: 2.29142625101151

Epoch: 6| Step: 6
Training loss: 2.5853934288024902
Validation loss: 2.2758322300449496

Epoch: 6| Step: 7
Training loss: 3.1527857780456543
Validation loss: 2.265022590596189

Epoch: 6| Step: 8
Training loss: 2.306129217147827
Validation loss: 2.2634113424567768

Epoch: 6| Step: 9
Training loss: 2.166898727416992
Validation loss: 2.260231528230893

Epoch: 6| Step: 10
Training loss: 1.8562572002410889
Validation loss: 2.255655163077898

Epoch: 6| Step: 11
Training loss: 2.7789366245269775
Validation loss: 2.2579086775420816

Epoch: 6| Step: 12
Training loss: 2.593907594680786
Validation loss: 2.2641860003112466

Epoch: 6| Step: 13
Training loss: 1.808849811553955
Validation loss: 2.266604245349925

Epoch: 137| Step: 0
Training loss: 2.2568392753601074
Validation loss: 2.2748135161656204

Epoch: 6| Step: 1
Training loss: 2.8801350593566895
Validation loss: 2.2872043143036547

Epoch: 6| Step: 2
Training loss: 2.474353551864624
Validation loss: 2.291848446733208

Epoch: 6| Step: 3
Training loss: 1.920138955116272
Validation loss: 2.311498377912788

Epoch: 6| Step: 4
Training loss: 2.5220983028411865
Validation loss: 2.3222227455467306

Epoch: 6| Step: 5
Training loss: 2.4196486473083496
Validation loss: 2.3298500096926125

Epoch: 6| Step: 6
Training loss: 2.4212634563446045
Validation loss: 2.3433738344459125

Epoch: 6| Step: 7
Training loss: 2.7473673820495605
Validation loss: 2.332928119167205

Epoch: 6| Step: 8
Training loss: 2.368175745010376
Validation loss: 2.3049329942272556

Epoch: 6| Step: 9
Training loss: 2.883498191833496
Validation loss: 2.2837974230448403

Epoch: 6| Step: 10
Training loss: 2.696125030517578
Validation loss: 2.267766332113615

Epoch: 6| Step: 11
Training loss: 2.9864907264709473
Validation loss: 2.2581710046337498

Epoch: 6| Step: 12
Training loss: 2.586286783218384
Validation loss: 2.251030837335894

Epoch: 6| Step: 13
Training loss: 2.448164939880371
Validation loss: 2.2533609982459777

Epoch: 138| Step: 0
Training loss: 2.3729965686798096
Validation loss: 2.25256859871649

Epoch: 6| Step: 1
Training loss: 2.196401596069336
Validation loss: 2.2520346949177403

Epoch: 6| Step: 2
Training loss: 2.934260368347168
Validation loss: 2.2561590697175715

Epoch: 6| Step: 3
Training loss: 1.9126665592193604
Validation loss: 2.2556654355859243

Epoch: 6| Step: 4
Training loss: 2.497178077697754
Validation loss: 2.261689970570226

Epoch: 6| Step: 5
Training loss: 2.2529795169830322
Validation loss: 2.275213231322586

Epoch: 6| Step: 6
Training loss: 2.6953940391540527
Validation loss: 2.282828789885326

Epoch: 6| Step: 7
Training loss: 1.9293752908706665
Validation loss: 2.3011336839327248

Epoch: 6| Step: 8
Training loss: 3.285115957260132
Validation loss: 2.298068379843107

Epoch: 6| Step: 9
Training loss: 2.509882926940918
Validation loss: 2.2904976080822688

Epoch: 6| Step: 10
Training loss: 2.607339382171631
Validation loss: 2.2730504312822895

Epoch: 6| Step: 11
Training loss: 2.8619251251220703
Validation loss: 2.266096948295511

Epoch: 6| Step: 12
Training loss: 2.7100765705108643
Validation loss: 2.25381153116944

Epoch: 6| Step: 13
Training loss: 2.8682687282562256
Validation loss: 2.2541921907855618

Epoch: 139| Step: 0
Training loss: 1.9987304210662842
Validation loss: 2.2468158019486295

Epoch: 6| Step: 1
Training loss: 3.378620147705078
Validation loss: 2.250102132879278

Epoch: 6| Step: 2
Training loss: 2.746227264404297
Validation loss: 2.2477997092790503

Epoch: 6| Step: 3
Training loss: 2.701941967010498
Validation loss: 2.2472991533176874

Epoch: 6| Step: 4
Training loss: 2.093684196472168
Validation loss: 2.2436825280548423

Epoch: 6| Step: 5
Training loss: 2.1890223026275635
Validation loss: 2.2489277150041316

Epoch: 6| Step: 6
Training loss: 2.859802722930908
Validation loss: 2.243513345718384

Epoch: 6| Step: 7
Training loss: 2.0875039100646973
Validation loss: 2.253511433960289

Epoch: 6| Step: 8
Training loss: 2.5600032806396484
Validation loss: 2.2744223379319712

Epoch: 6| Step: 9
Training loss: 2.500568151473999
Validation loss: 2.281828862364574

Epoch: 6| Step: 10
Training loss: 1.8204363584518433
Validation loss: 2.2864841389399704

Epoch: 6| Step: 11
Training loss: 3.025622844696045
Validation loss: 2.2780956299074235

Epoch: 6| Step: 12
Training loss: 3.073741912841797
Validation loss: 2.2877707994112404

Epoch: 6| Step: 13
Training loss: 2.6147983074188232
Validation loss: 2.2887377982498496

Epoch: 140| Step: 0
Training loss: 2.5880558490753174
Validation loss: 2.285471729052964

Epoch: 6| Step: 1
Training loss: 2.7391371726989746
Validation loss: 2.296162418139878

Epoch: 6| Step: 2
Training loss: 2.7296195030212402
Validation loss: 2.3107597751002156

Epoch: 6| Step: 3
Training loss: 2.7027604579925537
Validation loss: 2.3227670551628194

Epoch: 6| Step: 4
Training loss: 1.328597903251648
Validation loss: 2.311023594230734

Epoch: 6| Step: 5
Training loss: 2.6606571674346924
Validation loss: 2.316026185148506

Epoch: 6| Step: 6
Training loss: 2.635763645172119
Validation loss: 2.3097820948528986

Epoch: 6| Step: 7
Training loss: 2.381380796432495
Validation loss: 2.309173266092936

Epoch: 6| Step: 8
Training loss: 2.538562297821045
Validation loss: 2.3039505661174817

Epoch: 6| Step: 9
Training loss: 2.314260959625244
Validation loss: 2.3012936935629895

Epoch: 6| Step: 10
Training loss: 2.4121060371398926
Validation loss: 2.301259656106272

Epoch: 6| Step: 11
Training loss: 2.9853756427764893
Validation loss: 2.2787653912780104

Epoch: 6| Step: 12
Training loss: 2.3283634185791016
Validation loss: 2.2543470808254775

Epoch: 6| Step: 13
Training loss: 3.619194984436035
Validation loss: 2.2415357251321115

Epoch: 141| Step: 0
Training loss: 2.1220693588256836
Validation loss: 2.239785191833332

Epoch: 6| Step: 1
Training loss: 3.0787296295166016
Validation loss: 2.242355167224843

Epoch: 6| Step: 2
Training loss: 2.067701578140259
Validation loss: 2.24772858747872

Epoch: 6| Step: 3
Training loss: 2.2710187435150146
Validation loss: 2.25507535729357

Epoch: 6| Step: 4
Training loss: 2.1406075954437256
Validation loss: 2.2576012073024625

Epoch: 6| Step: 5
Training loss: 2.374514102935791
Validation loss: 2.249968872275404

Epoch: 6| Step: 6
Training loss: 2.159221649169922
Validation loss: 2.2509188139310448

Epoch: 6| Step: 7
Training loss: 3.005549669265747
Validation loss: 2.250896018038514

Epoch: 6| Step: 8
Training loss: 2.5711565017700195
Validation loss: 2.2547804873476744

Epoch: 6| Step: 9
Training loss: 2.0357325077056885
Validation loss: 2.2460004693718365

Epoch: 6| Step: 10
Training loss: 3.2736196517944336
Validation loss: 2.2393512443829606

Epoch: 6| Step: 11
Training loss: 3.3024511337280273
Validation loss: 2.244106187615343

Epoch: 6| Step: 12
Training loss: 2.6820249557495117
Validation loss: 2.2456822190233456

Epoch: 6| Step: 13
Training loss: 2.418109178543091
Validation loss: 2.2665671738245154

Epoch: 142| Step: 0
Training loss: 2.7254562377929688
Validation loss: 2.2787527268932712

Epoch: 6| Step: 1
Training loss: 2.6073317527770996
Validation loss: 2.3119115791013165

Epoch: 6| Step: 2
Training loss: 2.18721342086792
Validation loss: 2.3077425931089666

Epoch: 6| Step: 3
Training loss: 2.079821825027466
Validation loss: 2.309688168187295

Epoch: 6| Step: 4
Training loss: 3.0622901916503906
Validation loss: 2.307844897752167

Epoch: 6| Step: 5
Training loss: 2.0690717697143555
Validation loss: 2.302016794040639

Epoch: 6| Step: 6
Training loss: 2.1471118927001953
Validation loss: 2.276965484824232

Epoch: 6| Step: 7
Training loss: 2.9257125854492188
Validation loss: 2.2644870281219482

Epoch: 6| Step: 8
Training loss: 2.9003682136535645
Validation loss: 2.253450096294444

Epoch: 6| Step: 9
Training loss: 2.5700483322143555
Validation loss: 2.243525693493505

Epoch: 6| Step: 10
Training loss: 2.7284960746765137
Validation loss: 2.241063053889941

Epoch: 6| Step: 11
Training loss: 2.9985032081604004
Validation loss: 2.237158852238809

Epoch: 6| Step: 12
Training loss: 1.8877735137939453
Validation loss: 2.2345855748781593

Epoch: 6| Step: 13
Training loss: 2.301525115966797
Validation loss: 2.233494665033074

Epoch: 143| Step: 0
Training loss: 2.7076687812805176
Validation loss: 2.2418597949448453

Epoch: 6| Step: 1
Training loss: 3.0129647254943848
Validation loss: 2.2432834768808014

Epoch: 6| Step: 2
Training loss: 2.377823829650879
Validation loss: 2.2495922093750327

Epoch: 6| Step: 3
Training loss: 2.586703300476074
Validation loss: 2.259907578909269

Epoch: 6| Step: 4
Training loss: 2.6325745582580566
Validation loss: 2.260109962955598

Epoch: 6| Step: 5
Training loss: 2.468931198120117
Validation loss: 2.2534354668791576

Epoch: 6| Step: 6
Training loss: 2.1091649532318115
Validation loss: 2.2507422816368843

Epoch: 6| Step: 7
Training loss: 2.251814126968384
Validation loss: 2.248827362573275

Epoch: 6| Step: 8
Training loss: 1.5866833925247192
Validation loss: 2.239423187830115

Epoch: 6| Step: 9
Training loss: 2.106156587600708
Validation loss: 2.2454221556263585

Epoch: 6| Step: 10
Training loss: 3.2558722496032715
Validation loss: 2.24320714704452

Epoch: 6| Step: 11
Training loss: 2.3925867080688477
Validation loss: 2.2429185554545414

Epoch: 6| Step: 12
Training loss: 2.7615973949432373
Validation loss: 2.2486562844245666

Epoch: 6| Step: 13
Training loss: 3.3392763137817383
Validation loss: 2.2555576716699908

Epoch: 144| Step: 0
Training loss: 2.4419803619384766
Validation loss: 2.259715287916122

Epoch: 6| Step: 1
Training loss: 2.978516101837158
Validation loss: 2.260823316471551

Epoch: 6| Step: 2
Training loss: 2.5404229164123535
Validation loss: 2.2678606766526417

Epoch: 6| Step: 3
Training loss: 2.496178150177002
Validation loss: 2.261465646887338

Epoch: 6| Step: 4
Training loss: 2.5562639236450195
Validation loss: 2.2749255575159544

Epoch: 6| Step: 5
Training loss: 3.1665549278259277
Validation loss: 2.2778658354154198

Epoch: 6| Step: 6
Training loss: 1.5149118900299072
Validation loss: 2.278647679154591

Epoch: 6| Step: 7
Training loss: 1.6989986896514893
Validation loss: 2.2700106700261435

Epoch: 6| Step: 8
Training loss: 3.076561450958252
Validation loss: 2.260629096338826

Epoch: 6| Step: 9
Training loss: 3.0520849227905273
Validation loss: 2.254805285443542

Epoch: 6| Step: 10
Training loss: 2.468654155731201
Validation loss: 2.2463059040807907

Epoch: 6| Step: 11
Training loss: 1.983974814414978
Validation loss: 2.237960369356217

Epoch: 6| Step: 12
Training loss: 2.4689927101135254
Validation loss: 2.240183795652082

Epoch: 6| Step: 13
Training loss: 2.8420071601867676
Validation loss: 2.2323126587816464

Epoch: 145| Step: 0
Training loss: 2.7617762088775635
Validation loss: 2.241495422137681

Epoch: 6| Step: 1
Training loss: 2.37862229347229
Validation loss: 2.2377453439979145

Epoch: 6| Step: 2
Training loss: 2.7918014526367188
Validation loss: 2.2364367233809603

Epoch: 6| Step: 3
Training loss: 2.0526554584503174
Validation loss: 2.2361597655921854

Epoch: 6| Step: 4
Training loss: 2.717607021331787
Validation loss: 2.2528331946301203

Epoch: 6| Step: 5
Training loss: 2.417229652404785
Validation loss: 2.2479721397481938

Epoch: 6| Step: 6
Training loss: 2.5524392127990723
Validation loss: 2.2579173272655857

Epoch: 6| Step: 7
Training loss: 2.802182197570801
Validation loss: 2.2699409454099593

Epoch: 6| Step: 8
Training loss: 2.7994606494903564
Validation loss: 2.267004488616861

Epoch: 6| Step: 9
Training loss: 1.8054156303405762
Validation loss: 2.2658078119318974

Epoch: 6| Step: 10
Training loss: 2.9775683879852295
Validation loss: 2.254681169345815

Epoch: 6| Step: 11
Training loss: 3.111435651779175
Validation loss: 2.256881626703406

Epoch: 6| Step: 12
Training loss: 1.9407505989074707
Validation loss: 2.24092798848306

Epoch: 6| Step: 13
Training loss: 1.7514429092407227
Validation loss: 2.228045460998371

Epoch: 146| Step: 0
Training loss: 2.70583438873291
Validation loss: 2.230343839173676

Epoch: 6| Step: 1
Training loss: 2.461300849914551
Validation loss: 2.231913971644576

Epoch: 6| Step: 2
Training loss: 1.969531536102295
Validation loss: 2.226398165507983

Epoch: 6| Step: 3
Training loss: 2.4086034297943115
Validation loss: 2.234701692417104

Epoch: 6| Step: 4
Training loss: 1.4901081323623657
Validation loss: 2.233658702142777

Epoch: 6| Step: 5
Training loss: 2.380681037902832
Validation loss: 2.2444897056907736

Epoch: 6| Step: 6
Training loss: 2.735656261444092
Validation loss: 2.264841579621838

Epoch: 6| Step: 7
Training loss: 3.010857105255127
Validation loss: 2.28031418144062

Epoch: 6| Step: 8
Training loss: 3.4887828826904297
Validation loss: 2.2874127818692114

Epoch: 6| Step: 9
Training loss: 2.300523519515991
Validation loss: 2.2832824491685435

Epoch: 6| Step: 10
Training loss: 2.9596080780029297
Validation loss: 2.2985034296589513

Epoch: 6| Step: 11
Training loss: 2.142986536026001
Validation loss: 2.2885360499863983

Epoch: 6| Step: 12
Training loss: 2.8471593856811523
Validation loss: 2.2700483568253054

Epoch: 6| Step: 13
Training loss: 2.2602217197418213
Validation loss: 2.272102266229609

Epoch: 147| Step: 0
Training loss: 2.6261911392211914
Validation loss: 2.278143172623009

Epoch: 6| Step: 1
Training loss: 3.266890048980713
Validation loss: 2.28544533124534

Epoch: 6| Step: 2
Training loss: 1.6215351819992065
Validation loss: 2.265658619583294

Epoch: 6| Step: 3
Training loss: 2.5446276664733887
Validation loss: 2.265054596367703

Epoch: 6| Step: 4
Training loss: 2.7060062885284424
Validation loss: 2.256545348833966

Epoch: 6| Step: 5
Training loss: 2.5374910831451416
Validation loss: 2.249773848441339

Epoch: 6| Step: 6
Training loss: 2.7818522453308105
Validation loss: 2.245445975693323

Epoch: 6| Step: 7
Training loss: 2.542386293411255
Validation loss: 2.238572520594443

Epoch: 6| Step: 8
Training loss: 1.4984900951385498
Validation loss: 2.2303757693177912

Epoch: 6| Step: 9
Training loss: 3.027556896209717
Validation loss: 2.2444172982246644

Epoch: 6| Step: 10
Training loss: 1.8486838340759277
Validation loss: 2.243151186614908

Epoch: 6| Step: 11
Training loss: 2.467458724975586
Validation loss: 2.2557795970670638

Epoch: 6| Step: 12
Training loss: 2.7170650959014893
Validation loss: 2.2649826759933145

Epoch: 6| Step: 13
Training loss: 3.122117042541504
Validation loss: 2.261549380517775

Epoch: 148| Step: 0
Training loss: 2.7651548385620117
Validation loss: 2.271859771461897

Epoch: 6| Step: 1
Training loss: 1.718033790588379
Validation loss: 2.249150609457365

Epoch: 6| Step: 2
Training loss: 2.1292872428894043
Validation loss: 2.2451975076429305

Epoch: 6| Step: 3
Training loss: 2.894324779510498
Validation loss: 2.2394267641088015

Epoch: 6| Step: 4
Training loss: 2.5775675773620605
Validation loss: 2.241215995562974

Epoch: 6| Step: 5
Training loss: 2.444395065307617
Validation loss: 2.2370829287395684

Epoch: 6| Step: 6
Training loss: 2.5415215492248535
Validation loss: 2.238847881235102

Epoch: 6| Step: 7
Training loss: 2.636439085006714
Validation loss: 2.241570880336146

Epoch: 6| Step: 8
Training loss: 3.128776788711548
Validation loss: 2.2393666646813832

Epoch: 6| Step: 9
Training loss: 2.3945674896240234
Validation loss: 2.2363007350634505

Epoch: 6| Step: 10
Training loss: 2.570875644683838
Validation loss: 2.236865766586796

Epoch: 6| Step: 11
Training loss: 2.4800987243652344
Validation loss: 2.2406001834459204

Epoch: 6| Step: 12
Training loss: 2.3413145542144775
Validation loss: 2.2469978435065157

Epoch: 6| Step: 13
Training loss: 2.591912269592285
Validation loss: 2.2491138391597296

Epoch: 149| Step: 0
Training loss: 1.7549707889556885
Validation loss: 2.258060088721655

Epoch: 6| Step: 1
Training loss: 2.620810031890869
Validation loss: 2.2471632252457323

Epoch: 6| Step: 2
Training loss: 2.772995710372925
Validation loss: 2.254868427912394

Epoch: 6| Step: 3
Training loss: 2.8844339847564697
Validation loss: 2.2594012573201168

Epoch: 6| Step: 4
Training loss: 2.6901469230651855
Validation loss: 2.2909101196514663

Epoch: 6| Step: 5
Training loss: 2.7555909156799316
Validation loss: 2.2864999258390037

Epoch: 6| Step: 6
Training loss: 2.2880518436431885
Validation loss: 2.2934463998322845

Epoch: 6| Step: 7
Training loss: 2.995243549346924
Validation loss: 2.2930913663679555

Epoch: 6| Step: 8
Training loss: 2.842805862426758
Validation loss: 2.270853850149339

Epoch: 6| Step: 9
Training loss: 2.717984199523926
Validation loss: 2.2402797001664356

Epoch: 6| Step: 10
Training loss: 2.3525216579437256
Validation loss: 2.2291942514399046

Epoch: 6| Step: 11
Training loss: 1.7466764450073242
Validation loss: 2.2254995787015526

Epoch: 6| Step: 12
Training loss: 2.182891368865967
Validation loss: 2.222968625765975

Epoch: 6| Step: 13
Training loss: 2.508035659790039
Validation loss: 2.231183332781638

Epoch: 150| Step: 0
Training loss: 2.527420997619629
Validation loss: 2.2223714115799114

Epoch: 6| Step: 1
Training loss: 2.453014373779297
Validation loss: 2.2287802286045526

Epoch: 6| Step: 2
Training loss: 2.682041645050049
Validation loss: 2.2292676946168304

Epoch: 6| Step: 3
Training loss: 2.7396674156188965
Validation loss: 2.2336040991608814

Epoch: 6| Step: 4
Training loss: 2.4423952102661133
Validation loss: 2.240202614056167

Epoch: 6| Step: 5
Training loss: 2.3715310096740723
Validation loss: 2.25580160848556

Epoch: 6| Step: 6
Training loss: 2.1831374168395996
Validation loss: 2.2504418062907394

Epoch: 6| Step: 7
Training loss: 3.117452621459961
Validation loss: 2.2613684156889557

Epoch: 6| Step: 8
Training loss: 2.1203200817108154
Validation loss: 2.2571188608805337

Epoch: 6| Step: 9
Training loss: 2.4504644870758057
Validation loss: 2.254958518089787

Epoch: 6| Step: 10
Training loss: 2.0828795433044434
Validation loss: 2.249107914586221

Epoch: 6| Step: 11
Training loss: 3.012810230255127
Validation loss: 2.234390443371188

Epoch: 6| Step: 12
Training loss: 2.0048601627349854
Validation loss: 2.234429185108472

Epoch: 6| Step: 13
Training loss: 2.904414176940918
Validation loss: 2.221861844421715

Epoch: 151| Step: 0
Training loss: 2.2388222217559814
Validation loss: 2.217278826621271

Epoch: 6| Step: 1
Training loss: 3.0118913650512695
Validation loss: 2.2136718201380905

Epoch: 6| Step: 2
Training loss: 2.6500942707061768
Validation loss: 2.217831778269942

Epoch: 6| Step: 3
Training loss: 2.427440881729126
Validation loss: 2.2179264483913297

Epoch: 6| Step: 4
Training loss: 2.2768852710723877
Validation loss: 2.2236698801799486

Epoch: 6| Step: 5
Training loss: 2.961296558380127
Validation loss: 2.223035661123132

Epoch: 6| Step: 6
Training loss: 2.6441726684570312
Validation loss: 2.2342104399076073

Epoch: 6| Step: 7
Training loss: 2.3174524307250977
Validation loss: 2.233134429941895

Epoch: 6| Step: 8
Training loss: 2.322230339050293
Validation loss: 2.252375623231293

Epoch: 6| Step: 9
Training loss: 1.5524555444717407
Validation loss: 2.246490929716377

Epoch: 6| Step: 10
Training loss: 2.7698163986206055
Validation loss: 2.2549712222109557

Epoch: 6| Step: 11
Training loss: 2.705568313598633
Validation loss: 2.250314002395958

Epoch: 6| Step: 12
Training loss: 2.9087419509887695
Validation loss: 2.2442262698245306

Epoch: 6| Step: 13
Training loss: 2.047926187515259
Validation loss: 2.2279144692164596

Epoch: 152| Step: 0
Training loss: 2.48466157913208
Validation loss: 2.2315710795822965

Epoch: 6| Step: 1
Training loss: 3.2291293144226074
Validation loss: 2.2261673788870535

Epoch: 6| Step: 2
Training loss: 2.435065269470215
Validation loss: 2.2247018224449566

Epoch: 6| Step: 3
Training loss: 1.6192975044250488
Validation loss: 2.2291547252285864

Epoch: 6| Step: 4
Training loss: 1.8942707777023315
Validation loss: 2.2390949521013486

Epoch: 6| Step: 5
Training loss: 2.581946849822998
Validation loss: 2.238715907578827

Epoch: 6| Step: 6
Training loss: 2.4680380821228027
Validation loss: 2.235457661331341

Epoch: 6| Step: 7
Training loss: 2.359175205230713
Validation loss: 2.223349530209777

Epoch: 6| Step: 8
Training loss: 2.621548891067505
Validation loss: 2.2366408263483355

Epoch: 6| Step: 9
Training loss: 2.590636730194092
Validation loss: 2.2309503606570664

Epoch: 6| Step: 10
Training loss: 2.863091468811035
Validation loss: 2.2217886537633915

Epoch: 6| Step: 11
Training loss: 2.673607349395752
Validation loss: 2.225652148646693

Epoch: 6| Step: 12
Training loss: 2.778217315673828
Validation loss: 2.228002945582072

Epoch: 6| Step: 13
Training loss: 2.2692441940307617
Validation loss: 2.2250284648710683

Epoch: 153| Step: 0
Training loss: 2.364839792251587
Validation loss: 2.226729416078137

Epoch: 6| Step: 1
Training loss: 2.1779255867004395
Validation loss: 2.2370639667716077

Epoch: 6| Step: 2
Training loss: 3.0210344791412354
Validation loss: 2.244551661194012

Epoch: 6| Step: 3
Training loss: 2.4785337448120117
Validation loss: 2.247899245190364

Epoch: 6| Step: 4
Training loss: 3.0198750495910645
Validation loss: 2.2473140301242953

Epoch: 6| Step: 5
Training loss: 1.6412993669509888
Validation loss: 2.2622166167023363

Epoch: 6| Step: 6
Training loss: 3.5043785572052
Validation loss: 2.2487345280185824

Epoch: 6| Step: 7
Training loss: 2.534564733505249
Validation loss: 2.237441534637123

Epoch: 6| Step: 8
Training loss: 2.340043783187866
Validation loss: 2.2270041332449964

Epoch: 6| Step: 9
Training loss: 2.7175371646881104
Validation loss: 2.2326642954221336

Epoch: 6| Step: 10
Training loss: 1.9303643703460693
Validation loss: 2.236320913478892

Epoch: 6| Step: 11
Training loss: 2.8701725006103516
Validation loss: 2.246468238933112

Epoch: 6| Step: 12
Training loss: 1.9714936017990112
Validation loss: 2.2525692780812583

Epoch: 6| Step: 13
Training loss: 2.2166032791137695
Validation loss: 2.247837112795922

Epoch: 154| Step: 0
Training loss: 2.4485480785369873
Validation loss: 2.24659715929339

Epoch: 6| Step: 1
Training loss: 2.4771742820739746
Validation loss: 2.2294243279323784

Epoch: 6| Step: 2
Training loss: 2.707139015197754
Validation loss: 2.235309132965662

Epoch: 6| Step: 3
Training loss: 2.346798896789551
Validation loss: 2.2208640344681276

Epoch: 6| Step: 4
Training loss: 2.5541772842407227
Validation loss: 2.2204115929142123

Epoch: 6| Step: 5
Training loss: 2.2635183334350586
Validation loss: 2.2231886335598525

Epoch: 6| Step: 6
Training loss: 1.8380998373031616
Validation loss: 2.223066383792508

Epoch: 6| Step: 7
Training loss: 2.706789970397949
Validation loss: 2.2237628480439544

Epoch: 6| Step: 8
Training loss: 2.211968183517456
Validation loss: 2.2213882374507126

Epoch: 6| Step: 9
Training loss: 2.660289764404297
Validation loss: 2.222206046504359

Epoch: 6| Step: 10
Training loss: 2.87637996673584
Validation loss: 2.2317482322774906

Epoch: 6| Step: 11
Training loss: 2.7723917961120605
Validation loss: 2.2324936107922624

Epoch: 6| Step: 12
Training loss: 2.7301647663116455
Validation loss: 2.2368621877444688

Epoch: 6| Step: 13
Training loss: 1.9125069379806519
Validation loss: 2.236230101636661

Epoch: 155| Step: 0
Training loss: 2.5560379028320312
Validation loss: 2.2358385157841507

Epoch: 6| Step: 1
Training loss: 2.5944347381591797
Validation loss: 2.233667007056616

Epoch: 6| Step: 2
Training loss: 2.2341394424438477
Validation loss: 2.232808087461738

Epoch: 6| Step: 3
Training loss: 2.7539069652557373
Validation loss: 2.226852532356016

Epoch: 6| Step: 4
Training loss: 2.6603775024414062
Validation loss: 2.2263956531401603

Epoch: 6| Step: 5
Training loss: 2.1119656562805176
Validation loss: 2.2200338289301884

Epoch: 6| Step: 6
Training loss: 2.69143009185791
Validation loss: 2.2228936790138163

Epoch: 6| Step: 7
Training loss: 2.5141444206237793
Validation loss: 2.2070825612673195

Epoch: 6| Step: 8
Training loss: 2.6209044456481934
Validation loss: 2.1994645621186946

Epoch: 6| Step: 9
Training loss: 2.0380442142486572
Validation loss: 2.2079057975481917

Epoch: 6| Step: 10
Training loss: 2.5060408115386963
Validation loss: 2.213516260987969

Epoch: 6| Step: 11
Training loss: 2.677124500274658
Validation loss: 2.214916859903643

Epoch: 6| Step: 12
Training loss: 2.680893898010254
Validation loss: 2.2256464342917166

Epoch: 6| Step: 13
Training loss: 1.8896280527114868
Validation loss: 2.23104271324732

Epoch: 156| Step: 0
Training loss: 2.2278082370758057
Validation loss: 2.2344774699980214

Epoch: 6| Step: 1
Training loss: 2.4195306301116943
Validation loss: 2.2268251116557787

Epoch: 6| Step: 2
Training loss: 2.0348496437072754
Validation loss: 2.232774390969225

Epoch: 6| Step: 3
Training loss: 2.306976795196533
Validation loss: 2.2319584482459613

Epoch: 6| Step: 4
Training loss: 2.500079393386841
Validation loss: 2.2319065319594515

Epoch: 6| Step: 5
Training loss: 2.0293312072753906
Validation loss: 2.2289487136307584

Epoch: 6| Step: 6
Training loss: 1.9096317291259766
Validation loss: 2.237576771807927

Epoch: 6| Step: 7
Training loss: 3.0511231422424316
Validation loss: 2.2520123348441174

Epoch: 6| Step: 8
Training loss: 2.686527967453003
Validation loss: 2.2455638711170485

Epoch: 6| Step: 9
Training loss: 2.710332155227661
Validation loss: 2.2376784650228356

Epoch: 6| Step: 10
Training loss: 2.4148411750793457
Validation loss: 2.232451505558465

Epoch: 6| Step: 11
Training loss: 2.943037986755371
Validation loss: 2.2552287194036666

Epoch: 6| Step: 12
Training loss: 2.7349348068237305
Validation loss: 2.2610221114209903

Epoch: 6| Step: 13
Training loss: 3.1542696952819824
Validation loss: 2.2781875492424093

Epoch: 157| Step: 0
Training loss: 2.803652286529541
Validation loss: 2.2768129225700133

Epoch: 6| Step: 1
Training loss: 2.3454959392547607
Validation loss: 2.2679206812253563

Epoch: 6| Step: 2
Training loss: 3.217784881591797
Validation loss: 2.271576061043688

Epoch: 6| Step: 3
Training loss: 2.517122983932495
Validation loss: 2.2791534700701312

Epoch: 6| Step: 4
Training loss: 3.1095454692840576
Validation loss: 2.2889955530884447

Epoch: 6| Step: 5
Training loss: 2.4444479942321777
Validation loss: 2.294746724508142

Epoch: 6| Step: 6
Training loss: 2.510645627975464
Validation loss: 2.299422965254835

Epoch: 6| Step: 7
Training loss: 2.508471965789795
Validation loss: 2.31460710494749

Epoch: 6| Step: 8
Training loss: 2.739312171936035
Validation loss: 2.325558329141268

Epoch: 6| Step: 9
Training loss: 2.326563835144043
Validation loss: 2.317590036699849

Epoch: 6| Step: 10
Training loss: 1.8114197254180908
Validation loss: 2.3134594194350706

Epoch: 6| Step: 11
Training loss: 2.4700732231140137
Validation loss: 2.3013056555101947

Epoch: 6| Step: 12
Training loss: 2.3924505710601807
Validation loss: 2.288621388455873

Epoch: 6| Step: 13
Training loss: 1.9793052673339844
Validation loss: 2.2896982777503228

Epoch: 158| Step: 0
Training loss: 2.3139357566833496
Validation loss: 2.2786272648842103

Epoch: 6| Step: 1
Training loss: 2.4163267612457275
Validation loss: 2.2570122390665035

Epoch: 6| Step: 2
Training loss: 2.735180616378784
Validation loss: 2.263763404661609

Epoch: 6| Step: 3
Training loss: 2.589721441268921
Validation loss: 2.2620292760992564

Epoch: 6| Step: 4
Training loss: 3.312556028366089
Validation loss: 2.27175844869306

Epoch: 6| Step: 5
Training loss: 1.9006880521774292
Validation loss: 2.256759061608263

Epoch: 6| Step: 6
Training loss: 2.387193441390991
Validation loss: 2.2440267429556897

Epoch: 6| Step: 7
Training loss: 2.5144355297088623
Validation loss: 2.235162188929896

Epoch: 6| Step: 8
Training loss: 2.388787269592285
Validation loss: 2.2320555563895934

Epoch: 6| Step: 9
Training loss: 2.495110034942627
Validation loss: 2.221721895279423

Epoch: 6| Step: 10
Training loss: 2.45839786529541
Validation loss: 2.2282403797231694

Epoch: 6| Step: 11
Training loss: 2.705517530441284
Validation loss: 2.234925006025581

Epoch: 6| Step: 12
Training loss: 1.9156339168548584
Validation loss: 2.2260751698606756

Epoch: 6| Step: 13
Training loss: 2.8889718055725098
Validation loss: 2.2120937775540095

Epoch: 159| Step: 0
Training loss: 2.3690593242645264
Validation loss: 2.2017892842651694

Epoch: 6| Step: 1
Training loss: 2.129945993423462
Validation loss: 2.2018297410780385

Epoch: 6| Step: 2
Training loss: 2.8090567588806152
Validation loss: 2.1978974970438148

Epoch: 6| Step: 3
Training loss: 2.585946559906006
Validation loss: 2.2025551078140095

Epoch: 6| Step: 4
Training loss: 2.625969886779785
Validation loss: 2.2022165508680445

Epoch: 6| Step: 5
Training loss: 2.302708148956299
Validation loss: 2.22022497013051

Epoch: 6| Step: 6
Training loss: 2.393137216567993
Validation loss: 2.22804771443849

Epoch: 6| Step: 7
Training loss: 1.9407470226287842
Validation loss: 2.2218531690618044

Epoch: 6| Step: 8
Training loss: 2.432243824005127
Validation loss: 2.206308409731875

Epoch: 6| Step: 9
Training loss: 2.2137928009033203
Validation loss: 2.2037398122972056

Epoch: 6| Step: 10
Training loss: 3.108919143676758
Validation loss: 2.211504127389641

Epoch: 6| Step: 11
Training loss: 2.444733142852783
Validation loss: 2.1979636376903904

Epoch: 6| Step: 12
Training loss: 3.0430634021759033
Validation loss: 2.2101403872172036

Epoch: 6| Step: 13
Training loss: 1.8184661865234375
Validation loss: 2.2332982222239175

Epoch: 160| Step: 0
Training loss: 2.9807090759277344
Validation loss: 2.2797936342095815

Epoch: 6| Step: 1
Training loss: 2.2581987380981445
Validation loss: 2.347480791871266

Epoch: 6| Step: 2
Training loss: 2.0962958335876465
Validation loss: 2.2914228464967463

Epoch: 6| Step: 3
Training loss: 2.6668808460235596
Validation loss: 2.323791265487671

Epoch: 6| Step: 4
Training loss: 2.595353364944458
Validation loss: 2.248066920106129

Epoch: 6| Step: 5
Training loss: 3.012324333190918
Validation loss: 2.230950891330678

Epoch: 6| Step: 6
Training loss: 2.2195253372192383
Validation loss: 2.2171566806813723

Epoch: 6| Step: 7
Training loss: 2.25813364982605
Validation loss: 2.2234992083682807

Epoch: 6| Step: 8
Training loss: 2.2330212593078613
Validation loss: 2.2507294788155505

Epoch: 6| Step: 9
Training loss: 3.246239423751831
Validation loss: 2.2818740978035876

Epoch: 6| Step: 10
Training loss: 2.4596283435821533
Validation loss: 2.2910485959822133

Epoch: 6| Step: 11
Training loss: 2.1464033126831055
Validation loss: 2.2919685340696767

Epoch: 6| Step: 12
Training loss: 2.63088321685791
Validation loss: 2.254235854712866

Epoch: 6| Step: 13
Training loss: 2.6538712978363037
Validation loss: 2.2512476969790716

Epoch: 161| Step: 0
Training loss: 2.7912020683288574
Validation loss: 2.229914278112432

Epoch: 6| Step: 1
Training loss: 2.7215278148651123
Validation loss: 2.215992909605785

Epoch: 6| Step: 2
Training loss: 2.373232841491699
Validation loss: 2.2014845314846245

Epoch: 6| Step: 3
Training loss: 2.4412636756896973
Validation loss: 2.207461457098684

Epoch: 6| Step: 4
Training loss: 2.336320161819458
Validation loss: 2.215285780609295

Epoch: 6| Step: 5
Training loss: 3.180133581161499
Validation loss: 2.2302095813135945

Epoch: 6| Step: 6
Training loss: 2.6737565994262695
Validation loss: 2.2373875443653395

Epoch: 6| Step: 7
Training loss: 2.3597774505615234
Validation loss: 2.221334377924601

Epoch: 6| Step: 8
Training loss: 2.28786039352417
Validation loss: 2.2160228349829234

Epoch: 6| Step: 9
Training loss: 2.6869750022888184
Validation loss: 2.222721452354103

Epoch: 6| Step: 10
Training loss: 2.2880444526672363
Validation loss: 2.2055804626916045

Epoch: 6| Step: 11
Training loss: 2.395702362060547
Validation loss: 2.1924935002480783

Epoch: 6| Step: 12
Training loss: 2.2339417934417725
Validation loss: 2.186157485490204

Epoch: 6| Step: 13
Training loss: 1.3913860321044922
Validation loss: 2.186839234444403

Epoch: 162| Step: 0
Training loss: 2.0786869525909424
Validation loss: 2.1917520389761975

Epoch: 6| Step: 1
Training loss: 2.920128583908081
Validation loss: 2.184814864589322

Epoch: 6| Step: 2
Training loss: 2.4093241691589355
Validation loss: 2.177577021301434

Epoch: 6| Step: 3
Training loss: 2.484311103820801
Validation loss: 2.179505476387598

Epoch: 6| Step: 4
Training loss: 2.5460338592529297
Validation loss: 2.1829609255636893

Epoch: 6| Step: 5
Training loss: 2.347696542739868
Validation loss: 2.1955774214959916

Epoch: 6| Step: 6
Training loss: 2.521879196166992
Validation loss: 2.2111274708983717

Epoch: 6| Step: 7
Training loss: 2.2270619869232178
Validation loss: 2.227662335159958

Epoch: 6| Step: 8
Training loss: 2.1309316158294678
Validation loss: 2.240809484194684

Epoch: 6| Step: 9
Training loss: 2.5365023612976074
Validation loss: 2.234488241134151

Epoch: 6| Step: 10
Training loss: 2.3797335624694824
Validation loss: 2.2344893742633123

Epoch: 6| Step: 11
Training loss: 2.496791124343872
Validation loss: 2.2175353163032123

Epoch: 6| Step: 12
Training loss: 3.0554630756378174
Validation loss: 2.1969414910962506

Epoch: 6| Step: 13
Training loss: 2.3357205390930176
Validation loss: 2.188057117564704

Epoch: 163| Step: 0
Training loss: 2.629573345184326
Validation loss: 2.179166575913788

Epoch: 6| Step: 1
Training loss: 2.190214157104492
Validation loss: 2.1978819395906184

Epoch: 6| Step: 2
Training loss: 2.2034072875976562
Validation loss: 2.2345700238340642

Epoch: 6| Step: 3
Training loss: 2.489997148513794
Validation loss: 2.257148386329733

Epoch: 6| Step: 4
Training loss: 2.4400057792663574
Validation loss: 2.229584370889971

Epoch: 6| Step: 5
Training loss: 2.1290507316589355
Validation loss: 2.274013739760204

Epoch: 6| Step: 6
Training loss: 2.414923906326294
Validation loss: 2.24111444206648

Epoch: 6| Step: 7
Training loss: 2.4965033531188965
Validation loss: 2.19314064774462

Epoch: 6| Step: 8
Training loss: 3.3468780517578125
Validation loss: 2.1882037244817263

Epoch: 6| Step: 9
Training loss: 3.0567708015441895
Validation loss: 2.1786864162773214

Epoch: 6| Step: 10
Training loss: 2.424797534942627
Validation loss: 2.1758882922510945

Epoch: 6| Step: 11
Training loss: 1.7601373195648193
Validation loss: 2.1721614765864548

Epoch: 6| Step: 12
Training loss: 2.521670341491699
Validation loss: 2.1775230964024863

Epoch: 6| Step: 13
Training loss: 2.4716978073120117
Validation loss: 2.1812059302483835

Epoch: 164| Step: 0
Training loss: 2.4794418811798096
Validation loss: 2.1908291462929017

Epoch: 6| Step: 1
Training loss: 2.4963130950927734
Validation loss: 2.20070985568467

Epoch: 6| Step: 2
Training loss: 2.403916120529175
Validation loss: 2.196608660041645

Epoch: 6| Step: 3
Training loss: 2.4571542739868164
Validation loss: 2.204729626255651

Epoch: 6| Step: 4
Training loss: 2.7063331604003906
Validation loss: 2.2138729864551174

Epoch: 6| Step: 5
Training loss: 2.5068745613098145
Validation loss: 2.217532450152982

Epoch: 6| Step: 6
Training loss: 2.300281286239624
Validation loss: 2.2020592176786034

Epoch: 6| Step: 7
Training loss: 1.887662649154663
Validation loss: 2.207756678263346

Epoch: 6| Step: 8
Training loss: 1.9669549465179443
Validation loss: 2.2093382522624028

Epoch: 6| Step: 9
Training loss: 2.8881821632385254
Validation loss: 2.1895228021888324

Epoch: 6| Step: 10
Training loss: 2.0995712280273438
Validation loss: 2.176105301867249

Epoch: 6| Step: 11
Training loss: 2.698672294616699
Validation loss: 2.1760089987067768

Epoch: 6| Step: 12
Training loss: 2.645358085632324
Validation loss: 2.1831188650541407

Epoch: 6| Step: 13
Training loss: 3.2445507049560547
Validation loss: 2.172232263831682

Epoch: 165| Step: 0
Training loss: 2.2217137813568115
Validation loss: 2.1969746415333082

Epoch: 6| Step: 1
Training loss: 2.8880553245544434
Validation loss: 2.193043221709549

Epoch: 6| Step: 2
Training loss: 2.39923095703125
Validation loss: 2.1921475651443645

Epoch: 6| Step: 3
Training loss: 1.991963505744934
Validation loss: 2.1874285808173557

Epoch: 6| Step: 4
Training loss: 1.9046902656555176
Validation loss: 2.1946637835553897

Epoch: 6| Step: 5
Training loss: 2.7358970642089844
Validation loss: 2.2031769649956816

Epoch: 6| Step: 6
Training loss: 2.908446788787842
Validation loss: 2.2093390521182807

Epoch: 6| Step: 7
Training loss: 1.9180707931518555
Validation loss: 2.221563610979306

Epoch: 6| Step: 8
Training loss: 2.7036521434783936
Validation loss: 2.2268779047073854

Epoch: 6| Step: 9
Training loss: 2.9233341217041016
Validation loss: 2.2288113717109925

Epoch: 6| Step: 10
Training loss: 2.1930556297302246
Validation loss: 2.2339604311091925

Epoch: 6| Step: 11
Training loss: 2.678035020828247
Validation loss: 2.243853258830245

Epoch: 6| Step: 12
Training loss: 2.8586068153381348
Validation loss: 2.2577207447380148

Epoch: 6| Step: 13
Training loss: 1.5657298564910889
Validation loss: 2.2429502471800773

Epoch: 166| Step: 0
Training loss: 2.315554618835449
Validation loss: 2.24612275503015

Epoch: 6| Step: 1
Training loss: 2.3414268493652344
Validation loss: 2.2342684948316185

Epoch: 6| Step: 2
Training loss: 2.3650670051574707
Validation loss: 2.236419008624169

Epoch: 6| Step: 3
Training loss: 2.163543701171875
Validation loss: 2.2434619421600015

Epoch: 6| Step: 4
Training loss: 1.7891712188720703
Validation loss: 2.2385528677253315

Epoch: 6| Step: 5
Training loss: 2.6244921684265137
Validation loss: 2.2237332854219662

Epoch: 6| Step: 6
Training loss: 3.191439151763916
Validation loss: 2.219134908850475

Epoch: 6| Step: 7
Training loss: 2.237403392791748
Validation loss: 2.2109409711694203

Epoch: 6| Step: 8
Training loss: 1.9906237125396729
Validation loss: 2.2105886500368834

Epoch: 6| Step: 9
Training loss: 2.666360855102539
Validation loss: 2.205062520119452

Epoch: 6| Step: 10
Training loss: 2.333268404006958
Validation loss: 2.192882263532249

Epoch: 6| Step: 11
Training loss: 3.042358875274658
Validation loss: 2.191286804855511

Epoch: 6| Step: 12
Training loss: 2.7789669036865234
Validation loss: 2.18271045530996

Epoch: 6| Step: 13
Training loss: 2.5293350219726562
Validation loss: 2.1832412032670874

Epoch: 167| Step: 0
Training loss: 2.4584484100341797
Validation loss: 2.1748887877310477

Epoch: 6| Step: 1
Training loss: 2.4321448802948
Validation loss: 2.18082082656122

Epoch: 6| Step: 2
Training loss: 2.3803606033325195
Validation loss: 2.181190562504594

Epoch: 6| Step: 3
Training loss: 1.6676464080810547
Validation loss: 2.192711020028719

Epoch: 6| Step: 4
Training loss: 2.283935070037842
Validation loss: 2.1946661651775403

Epoch: 6| Step: 5
Training loss: 3.183969020843506
Validation loss: 2.217608536443403

Epoch: 6| Step: 6
Training loss: 2.4160614013671875
Validation loss: 2.20545809243315

Epoch: 6| Step: 7
Training loss: 1.8646740913391113
Validation loss: 2.193374367170436

Epoch: 6| Step: 8
Training loss: 1.976739764213562
Validation loss: 2.1849690970554145

Epoch: 6| Step: 9
Training loss: 2.6550121307373047
Validation loss: 2.166253166814004

Epoch: 6| Step: 10
Training loss: 2.843996286392212
Validation loss: 2.168343610661004

Epoch: 6| Step: 11
Training loss: 2.619913339614868
Validation loss: 2.1540763249961277

Epoch: 6| Step: 12
Training loss: 3.2367515563964844
Validation loss: 2.1577509500647105

Epoch: 6| Step: 13
Training loss: 2.0523643493652344
Validation loss: 2.1653479876056796

Epoch: 168| Step: 0
Training loss: 2.0494232177734375
Validation loss: 2.1704986890157065

Epoch: 6| Step: 1
Training loss: 2.5357413291931152
Validation loss: 2.1684359696603592

Epoch: 6| Step: 2
Training loss: 1.9574649333953857
Validation loss: 2.189660538909256

Epoch: 6| Step: 3
Training loss: 2.440882682800293
Validation loss: 2.2133445227017967

Epoch: 6| Step: 4
Training loss: 1.8056591749191284
Validation loss: 2.2337061846128075

Epoch: 6| Step: 5
Training loss: 2.2370541095733643
Validation loss: 2.2264118348398516

Epoch: 6| Step: 6
Training loss: 2.3860466480255127
Validation loss: 2.2336376841350267

Epoch: 6| Step: 7
Training loss: 2.4249532222747803
Validation loss: 2.235517506958336

Epoch: 6| Step: 8
Training loss: 2.966582775115967
Validation loss: 2.2415039898246847

Epoch: 6| Step: 9
Training loss: 2.4897913932800293
Validation loss: 2.2474784799801406

Epoch: 6| Step: 10
Training loss: 2.2198891639709473
Validation loss: 2.2629629130004556

Epoch: 6| Step: 11
Training loss: 2.5735747814178467
Validation loss: 2.2540987563389603

Epoch: 6| Step: 12
Training loss: 3.2265756130218506
Validation loss: 2.2698782285054526

Epoch: 6| Step: 13
Training loss: 3.097633123397827
Validation loss: 2.2221947613582818

Epoch: 169| Step: 0
Training loss: 2.9254801273345947
Validation loss: 2.194707521828272

Epoch: 6| Step: 1
Training loss: 2.745718002319336
Validation loss: 2.177144214671145

Epoch: 6| Step: 2
Training loss: 2.377258777618408
Validation loss: 2.1590568404043875

Epoch: 6| Step: 3
Training loss: 2.5574862957000732
Validation loss: 2.158400984220607

Epoch: 6| Step: 4
Training loss: 2.9681153297424316
Validation loss: 2.1634812380677912

Epoch: 6| Step: 5
Training loss: 2.360454559326172
Validation loss: 2.187711415752288

Epoch: 6| Step: 6
Training loss: 2.3556363582611084
Validation loss: 2.1908519421854327

Epoch: 6| Step: 7
Training loss: 2.2973358631134033
Validation loss: 2.1965514716281684

Epoch: 6| Step: 8
Training loss: 1.9314736127853394
Validation loss: 2.1830151798904582

Epoch: 6| Step: 9
Training loss: 2.288093090057373
Validation loss: 2.205832435238746

Epoch: 6| Step: 10
Training loss: 2.3467588424682617
Validation loss: 2.267894447490733

Epoch: 6| Step: 11
Training loss: 1.8975796699523926
Validation loss: 2.3060536487128145

Epoch: 6| Step: 12
Training loss: 2.480051279067993
Validation loss: 2.321220185167046

Epoch: 6| Step: 13
Training loss: 2.818018913269043
Validation loss: 2.345113661981398

Epoch: 170| Step: 0
Training loss: 2.405749559402466
Validation loss: 2.3631015067459433

Epoch: 6| Step: 1
Training loss: 2.396538257598877
Validation loss: 2.3469054904035342

Epoch: 6| Step: 2
Training loss: 3.342731475830078
Validation loss: 2.3340585411235852

Epoch: 6| Step: 3
Training loss: 2.120738983154297
Validation loss: 2.309077101369058

Epoch: 6| Step: 4
Training loss: 1.9816341400146484
Validation loss: 2.2767146172062045

Epoch: 6| Step: 5
Training loss: 2.59521484375
Validation loss: 2.2602304425290836

Epoch: 6| Step: 6
Training loss: 2.59213924407959
Validation loss: 2.225338687178909

Epoch: 6| Step: 7
Training loss: 2.053197145462036
Validation loss: 2.2058216884572017

Epoch: 6| Step: 8
Training loss: 2.72074556350708
Validation loss: 2.171419520531931

Epoch: 6| Step: 9
Training loss: 1.9301605224609375
Validation loss: 2.156248192633352

Epoch: 6| Step: 10
Training loss: 3.189164638519287
Validation loss: 2.151852097562564

Epoch: 6| Step: 11
Training loss: 2.7204651832580566
Validation loss: 2.1514440557008148

Epoch: 6| Step: 12
Training loss: 2.474191188812256
Validation loss: 2.1494061305958736

Epoch: 6| Step: 13
Training loss: 2.1895744800567627
Validation loss: 2.1690351604133524

Epoch: 171| Step: 0
Training loss: 2.5494494438171387
Validation loss: 2.1625015838171846

Epoch: 6| Step: 1
Training loss: 2.9478378295898438
Validation loss: 2.1564850704644316

Epoch: 6| Step: 2
Training loss: 2.154787540435791
Validation loss: 2.14161543948676

Epoch: 6| Step: 3
Training loss: 2.928615093231201
Validation loss: 2.142579068419754

Epoch: 6| Step: 4
Training loss: 2.6857707500457764
Validation loss: 2.1540918991129887

Epoch: 6| Step: 5
Training loss: 2.2433090209960938
Validation loss: 2.1687254598063808

Epoch: 6| Step: 6
Training loss: 2.8943347930908203
Validation loss: 2.2237704646202827

Epoch: 6| Step: 7
Training loss: 2.377208709716797
Validation loss: 2.2616053909383793

Epoch: 6| Step: 8
Training loss: 2.5544731616973877
Validation loss: 2.297084759640437

Epoch: 6| Step: 9
Training loss: 2.1578969955444336
Validation loss: 2.266233326286398

Epoch: 6| Step: 10
Training loss: 1.726414680480957
Validation loss: 2.262447977578768

Epoch: 6| Step: 11
Training loss: 1.9568480253219604
Validation loss: 2.2330795641868346

Epoch: 6| Step: 12
Training loss: 2.7728517055511475
Validation loss: 2.2051823459645754

Epoch: 6| Step: 13
Training loss: 3.1214139461517334
Validation loss: 2.2000217091652656

Epoch: 172| Step: 0
Training loss: 2.6403679847717285
Validation loss: 2.169040331276514

Epoch: 6| Step: 1
Training loss: 2.252596855163574
Validation loss: 2.14727218433093

Epoch: 6| Step: 2
Training loss: 2.4413976669311523
Validation loss: 2.140363267672959

Epoch: 6| Step: 3
Training loss: 2.596818685531616
Validation loss: 2.140453618059876

Epoch: 6| Step: 4
Training loss: 1.9256446361541748
Validation loss: 2.157382288286763

Epoch: 6| Step: 5
Training loss: 2.2139410972595215
Validation loss: 2.1607594310596423

Epoch: 6| Step: 6
Training loss: 2.535329580307007
Validation loss: 2.1699851841054936

Epoch: 6| Step: 7
Training loss: 2.435626983642578
Validation loss: 2.17864720026652

Epoch: 6| Step: 8
Training loss: 2.526616096496582
Validation loss: 2.1742835352497716

Epoch: 6| Step: 9
Training loss: 2.491391181945801
Validation loss: 2.1780916772862917

Epoch: 6| Step: 10
Training loss: 2.25587797164917
Validation loss: 2.1680982087248113

Epoch: 6| Step: 11
Training loss: 3.2730600833892822
Validation loss: 2.1497144391459804

Epoch: 6| Step: 12
Training loss: 2.4976906776428223
Validation loss: 2.1498473344310636

Epoch: 6| Step: 13
Training loss: 3.1008496284484863
Validation loss: 2.1349166695789625

Epoch: 173| Step: 0
Training loss: 2.5149784088134766
Validation loss: 2.1371059289542575

Epoch: 6| Step: 1
Training loss: 2.3919811248779297
Validation loss: 2.1292537899427515

Epoch: 6| Step: 2
Training loss: 1.9702541828155518
Validation loss: 2.1331577518934846

Epoch: 6| Step: 3
Training loss: 2.2571539878845215
Validation loss: 2.143371028284873

Epoch: 6| Step: 4
Training loss: 1.8089044094085693
Validation loss: 2.158965568388662

Epoch: 6| Step: 5
Training loss: 2.694706439971924
Validation loss: 2.194080432256063

Epoch: 6| Step: 6
Training loss: 2.8254730701446533
Validation loss: 2.229809676447222

Epoch: 6| Step: 7
Training loss: 2.5302999019622803
Validation loss: 2.2473720812028453

Epoch: 6| Step: 8
Training loss: 2.5483334064483643
Validation loss: 2.262320846639654

Epoch: 6| Step: 9
Training loss: 2.659489154815674
Validation loss: 2.280935133657148

Epoch: 6| Step: 10
Training loss: 2.7252886295318604
Validation loss: 2.3150169580213484

Epoch: 6| Step: 11
Training loss: 2.3092076778411865
Validation loss: 2.3378897841258715

Epoch: 6| Step: 12
Training loss: 2.728975772857666
Validation loss: 2.338226233759234

Epoch: 6| Step: 13
Training loss: 2.5091943740844727
Validation loss: 2.315105176741077

Epoch: 174| Step: 0
Training loss: 1.9551372528076172
Validation loss: 2.2629648459854947

Epoch: 6| Step: 1
Training loss: 2.456531524658203
Validation loss: 2.2383354248539096

Epoch: 6| Step: 2
Training loss: 2.340491533279419
Validation loss: 2.214217283392465

Epoch: 6| Step: 3
Training loss: 2.150571584701538
Validation loss: 2.2050052560785764

Epoch: 6| Step: 4
Training loss: 2.2484030723571777
Validation loss: 2.210499063614876

Epoch: 6| Step: 5
Training loss: 2.2749478816986084
Validation loss: 2.205797487689603

Epoch: 6| Step: 6
Training loss: 2.122439384460449
Validation loss: 2.185213731181237

Epoch: 6| Step: 7
Training loss: 2.3953938484191895
Validation loss: 2.1742592832093597

Epoch: 6| Step: 8
Training loss: 3.339694023132324
Validation loss: 2.1757466485423427

Epoch: 6| Step: 9
Training loss: 2.2100515365600586
Validation loss: 2.1668671126006753

Epoch: 6| Step: 10
Training loss: 2.3087594509124756
Validation loss: 2.1554846891792874

Epoch: 6| Step: 11
Training loss: 2.4229180812835693
Validation loss: 2.165725423443702

Epoch: 6| Step: 12
Training loss: 2.8872389793395996
Validation loss: 2.156391025871359

Epoch: 6| Step: 13
Training loss: 2.963912010192871
Validation loss: 2.1587098516443723

Epoch: 175| Step: 0
Training loss: 1.9620957374572754
Validation loss: 2.1426526064513833

Epoch: 6| Step: 1
Training loss: 2.482667922973633
Validation loss: 2.1411509295945526

Epoch: 6| Step: 2
Training loss: 3.101888656616211
Validation loss: 2.1325848820388957

Epoch: 6| Step: 3
Training loss: 1.903273344039917
Validation loss: 2.1512080700166765

Epoch: 6| Step: 4
Training loss: 2.3940670490264893
Validation loss: 2.1510011393536805

Epoch: 6| Step: 5
Training loss: 3.273904800415039
Validation loss: 2.153479442801527

Epoch: 6| Step: 6
Training loss: 2.075303792953491
Validation loss: 2.1523113071277575

Epoch: 6| Step: 7
Training loss: 2.52117919921875
Validation loss: 2.160262523158904

Epoch: 6| Step: 8
Training loss: 2.824023723602295
Validation loss: 2.142200252061249

Epoch: 6| Step: 9
Training loss: 2.401104688644409
Validation loss: 2.140704733069225

Epoch: 6| Step: 10
Training loss: 3.285306930541992
Validation loss: 2.1431081410377257

Epoch: 6| Step: 11
Training loss: 2.0330471992492676
Validation loss: 2.134148141389252

Epoch: 6| Step: 12
Training loss: 1.7519567012786865
Validation loss: 2.1271861894156343

Epoch: 6| Step: 13
Training loss: 1.9695703983306885
Validation loss: 2.1361697617397515

Epoch: 176| Step: 0
Training loss: 2.0300025939941406
Validation loss: 2.14214385965819

Epoch: 6| Step: 1
Training loss: 2.102456569671631
Validation loss: 2.1453202847511537

Epoch: 6| Step: 2
Training loss: 1.8271657228469849
Validation loss: 2.148806487360308

Epoch: 6| Step: 3
Training loss: 2.778134822845459
Validation loss: 2.1558328905413227

Epoch: 6| Step: 4
Training loss: 2.3874034881591797
Validation loss: 2.150537401117304

Epoch: 6| Step: 5
Training loss: 3.3082008361816406
Validation loss: 2.1437491678422496

Epoch: 6| Step: 6
Training loss: 2.7938895225524902
Validation loss: 2.1517955462137857

Epoch: 6| Step: 7
Training loss: 2.0793256759643555
Validation loss: 2.1509690720547914

Epoch: 6| Step: 8
Training loss: 2.229544162750244
Validation loss: 2.1485179496067826

Epoch: 6| Step: 9
Training loss: 1.8985722064971924
Validation loss: 2.1465085014220207

Epoch: 6| Step: 10
Training loss: 2.665234088897705
Validation loss: 2.1448446448131273

Epoch: 6| Step: 11
Training loss: 2.259225845336914
Validation loss: 2.150536193642565

Epoch: 6| Step: 12
Training loss: 2.8374321460723877
Validation loss: 2.1514873197001796

Epoch: 6| Step: 13
Training loss: 2.8387508392333984
Validation loss: 2.1552644852669007

Epoch: 177| Step: 0
Training loss: 1.6410396099090576
Validation loss: 2.173038698011829

Epoch: 6| Step: 1
Training loss: 1.7727267742156982
Validation loss: 2.1922667680248136

Epoch: 6| Step: 2
Training loss: 2.4332990646362305
Validation loss: 2.191070556640625

Epoch: 6| Step: 3
Training loss: 2.920614242553711
Validation loss: 2.1994666079039216

Epoch: 6| Step: 4
Training loss: 3.0101375579833984
Validation loss: 2.22265749977481

Epoch: 6| Step: 5
Training loss: 2.706202507019043
Validation loss: 2.21402883273299

Epoch: 6| Step: 6
Training loss: 1.8387916088104248
Validation loss: 2.2094039224809214

Epoch: 6| Step: 7
Training loss: 2.9659063816070557
Validation loss: 2.1831443104692685

Epoch: 6| Step: 8
Training loss: 2.0270042419433594
Validation loss: 2.1790275932640157

Epoch: 6| Step: 9
Training loss: 2.2812340259552
Validation loss: 2.1798359514564596

Epoch: 6| Step: 10
Training loss: 2.2592685222625732
Validation loss: 2.1890730524575837

Epoch: 6| Step: 11
Training loss: 2.4548873901367188
Validation loss: 2.18724944642795

Epoch: 6| Step: 12
Training loss: 2.643012046813965
Validation loss: 2.1848411636967815

Epoch: 6| Step: 13
Training loss: 3.3133535385131836
Validation loss: 2.1418913948920464

Epoch: 178| Step: 0
Training loss: 2.256404161453247
Validation loss: 2.1271139755043933

Epoch: 6| Step: 1
Training loss: 2.886981248855591
Validation loss: 2.1229223051378803

Epoch: 6| Step: 2
Training loss: 2.4190104007720947
Validation loss: 2.116051281652143

Epoch: 6| Step: 3
Training loss: 2.8932089805603027
Validation loss: 2.1204207020421184

Epoch: 6| Step: 4
Training loss: 1.8106606006622314
Validation loss: 2.128770793637922

Epoch: 6| Step: 5
Training loss: 3.457064390182495
Validation loss: 2.1294073776532243

Epoch: 6| Step: 6
Training loss: 3.2548270225524902
Validation loss: 2.133989441779352

Epoch: 6| Step: 7
Training loss: 2.419473886489868
Validation loss: 2.1441310836422827

Epoch: 6| Step: 8
Training loss: 1.5428366661071777
Validation loss: 2.1457913678179503

Epoch: 6| Step: 9
Training loss: 2.3414580821990967
Validation loss: 2.1416672916822534

Epoch: 6| Step: 10
Training loss: 2.542407512664795
Validation loss: 2.139972730349469

Epoch: 6| Step: 11
Training loss: 2.149890899658203
Validation loss: 2.1417974066990677

Epoch: 6| Step: 12
Training loss: 1.5500810146331787
Validation loss: 2.156060590538927

Epoch: 6| Step: 13
Training loss: 2.58754825592041
Validation loss: 2.160847197296799

Epoch: 179| Step: 0
Training loss: 2.1678686141967773
Validation loss: 2.1888778671141593

Epoch: 6| Step: 1
Training loss: 1.822056531906128
Validation loss: 2.222047236657912

Epoch: 6| Step: 2
Training loss: 2.400404930114746
Validation loss: 2.2270763074198077

Epoch: 6| Step: 3
Training loss: 2.526686668395996
Validation loss: 2.2275003258899977

Epoch: 6| Step: 4
Training loss: 2.460899829864502
Validation loss: 2.236822525660197

Epoch: 6| Step: 5
Training loss: 2.4281766414642334
Validation loss: 2.2482439523102133

Epoch: 6| Step: 6
Training loss: 2.450814962387085
Validation loss: 2.247208103056877

Epoch: 6| Step: 7
Training loss: 1.907944679260254
Validation loss: 2.2219366181281304

Epoch: 6| Step: 8
Training loss: 2.9422686100006104
Validation loss: 2.210845019227715

Epoch: 6| Step: 9
Training loss: 2.560281276702881
Validation loss: 2.1897557935407086

Epoch: 6| Step: 10
Training loss: 2.204881191253662
Validation loss: 2.194395185798727

Epoch: 6| Step: 11
Training loss: 1.9594335556030273
Validation loss: 2.16124209537301

Epoch: 6| Step: 12
Training loss: 3.0473716259002686
Validation loss: 2.155589157535184

Epoch: 6| Step: 13
Training loss: 3.3610658645629883
Validation loss: 2.142045415857787

Epoch: 180| Step: 0
Training loss: 2.7891860008239746
Validation loss: 2.1401528261041127

Epoch: 6| Step: 1
Training loss: 2.5621676445007324
Validation loss: 2.1268089996871127

Epoch: 6| Step: 2
Training loss: 2.397744655609131
Validation loss: 2.125216454587957

Epoch: 6| Step: 3
Training loss: 2.1345677375793457
Validation loss: 2.1182481550401255

Epoch: 6| Step: 4
Training loss: 2.7040481567382812
Validation loss: 2.1157943202603247

Epoch: 6| Step: 5
Training loss: 3.0441701412200928
Validation loss: 2.116474010611093

Epoch: 6| Step: 6
Training loss: 2.1520090103149414
Validation loss: 2.117071569606822

Epoch: 6| Step: 7
Training loss: 2.614441394805908
Validation loss: 2.123065815177015

Epoch: 6| Step: 8
Training loss: 2.2006983757019043
Validation loss: 2.1380830298187914

Epoch: 6| Step: 9
Training loss: 2.3172128200531006
Validation loss: 2.147271374220489

Epoch: 6| Step: 10
Training loss: 1.7749743461608887
Validation loss: 2.1414260441257107

Epoch: 6| Step: 11
Training loss: 2.3597023487091064
Validation loss: 2.144706154382357

Epoch: 6| Step: 12
Training loss: 1.8926522731781006
Validation loss: 2.158300151107132

Epoch: 6| Step: 13
Training loss: 3.2593324184417725
Validation loss: 2.1610822728885117

Epoch: 181| Step: 0
Training loss: 1.9967255592346191
Validation loss: 2.1543636809113207

Epoch: 6| Step: 1
Training loss: 1.131373405456543
Validation loss: 2.1539961804625807

Epoch: 6| Step: 2
Training loss: 2.9899673461914062
Validation loss: 2.136892105943413

Epoch: 6| Step: 3
Training loss: 2.569999933242798
Validation loss: 2.1234398862367034

Epoch: 6| Step: 4
Training loss: 2.939481258392334
Validation loss: 2.1349254679936234

Epoch: 6| Step: 5
Training loss: 2.6019983291625977
Validation loss: 2.1407093283950642

Epoch: 6| Step: 6
Training loss: 3.125521183013916
Validation loss: 2.1477444556451615

Epoch: 6| Step: 7
Training loss: 2.0012996196746826
Validation loss: 2.1503135363260903

Epoch: 6| Step: 8
Training loss: 2.5201358795166016
Validation loss: 2.1612845761801607

Epoch: 6| Step: 9
Training loss: 2.290194511413574
Validation loss: 2.1485241061897686

Epoch: 6| Step: 10
Training loss: 2.133746385574341
Validation loss: 2.140203873316447

Epoch: 6| Step: 11
Training loss: 2.480666160583496
Validation loss: 2.142718889380014

Epoch: 6| Step: 12
Training loss: 2.3456735610961914
Validation loss: 2.125883453635759

Epoch: 6| Step: 13
Training loss: 2.8477389812469482
Validation loss: 2.1117260532994426

Epoch: 182| Step: 0
Training loss: 2.4980740547180176
Validation loss: 2.1150781467396724

Epoch: 6| Step: 1
Training loss: 2.3455650806427
Validation loss: 2.1118167113232356

Epoch: 6| Step: 2
Training loss: 2.8901796340942383
Validation loss: 2.111251026071528

Epoch: 6| Step: 3
Training loss: 1.8965542316436768
Validation loss: 2.1215791881725354

Epoch: 6| Step: 4
Training loss: 1.579600214958191
Validation loss: 2.109400506942503

Epoch: 6| Step: 5
Training loss: 2.1129212379455566
Validation loss: 2.108042118369892

Epoch: 6| Step: 6
Training loss: 2.4942445755004883
Validation loss: 2.1176865216224425

Epoch: 6| Step: 7
Training loss: 2.243191719055176
Validation loss: 2.1283400481747043

Epoch: 6| Step: 8
Training loss: 1.8208147287368774
Validation loss: 2.1303916400478733

Epoch: 6| Step: 9
Training loss: 2.768826961517334
Validation loss: 2.155151682515298

Epoch: 6| Step: 10
Training loss: 3.070042133331299
Validation loss: 2.1544692106144403

Epoch: 6| Step: 11
Training loss: 2.4852890968322754
Validation loss: 2.1874795036931194

Epoch: 6| Step: 12
Training loss: 2.8271005153656006
Validation loss: 2.191143358907392

Epoch: 6| Step: 13
Training loss: 2.6971757411956787
Validation loss: 2.200925091261505

Epoch: 183| Step: 0
Training loss: 2.2541229724884033
Validation loss: 2.174579812634376

Epoch: 6| Step: 1
Training loss: 2.2619235515594482
Validation loss: 2.1856798959034744

Epoch: 6| Step: 2
Training loss: 2.5829339027404785
Validation loss: 2.2067219775210143

Epoch: 6| Step: 3
Training loss: 2.579205274581909
Validation loss: 2.25863468006093

Epoch: 6| Step: 4
Training loss: 2.519235849380493
Validation loss: 2.27362536614941

Epoch: 6| Step: 5
Training loss: 2.876038074493408
Validation loss: 2.2912325013068413

Epoch: 6| Step: 6
Training loss: 2.205312728881836
Validation loss: 2.311965598854967

Epoch: 6| Step: 7
Training loss: 2.920942783355713
Validation loss: 2.294129720298193

Epoch: 6| Step: 8
Training loss: 2.0339770317077637
Validation loss: 2.2290985866259505

Epoch: 6| Step: 9
Training loss: 2.07865834236145
Validation loss: 2.1642167452842958

Epoch: 6| Step: 10
Training loss: 2.554159164428711
Validation loss: 2.1531902884924286

Epoch: 6| Step: 11
Training loss: 2.4015376567840576
Validation loss: 2.1758641760836364

Epoch: 6| Step: 12
Training loss: 2.1851372718811035
Validation loss: 2.2159193869559997

Epoch: 6| Step: 13
Training loss: 2.9124794006347656
Validation loss: 2.2661010757569344

Epoch: 184| Step: 0
Training loss: 3.034738779067993
Validation loss: 2.2777805174550703

Epoch: 6| Step: 1
Training loss: 2.1879889965057373
Validation loss: 2.25852067880733

Epoch: 6| Step: 2
Training loss: 2.3279473781585693
Validation loss: 2.2185849887068554

Epoch: 6| Step: 3
Training loss: 2.5493156909942627
Validation loss: 2.166238097734349

Epoch: 6| Step: 4
Training loss: 1.349992275238037
Validation loss: 2.1187643363911617

Epoch: 6| Step: 5
Training loss: 2.195645332336426
Validation loss: 2.1076785223458403

Epoch: 6| Step: 6
Training loss: 2.7771220207214355
Validation loss: 2.1043633901944725

Epoch: 6| Step: 7
Training loss: 2.9959359169006348
Validation loss: 2.118123444177771

Epoch: 6| Step: 8
Training loss: 2.6272308826446533
Validation loss: 2.138684167656847

Epoch: 6| Step: 9
Training loss: 2.522888660430908
Validation loss: 2.15757578931829

Epoch: 6| Step: 10
Training loss: 2.5991897583007812
Validation loss: 2.191569402653684

Epoch: 6| Step: 11
Training loss: 2.4089832305908203
Validation loss: 2.2099018968561643

Epoch: 6| Step: 12
Training loss: 2.632941722869873
Validation loss: 2.1915481564819173

Epoch: 6| Step: 13
Training loss: 2.2523295879364014
Validation loss: 2.1977305079019196

Epoch: 185| Step: 0
Training loss: 2.154975652694702
Validation loss: 2.2414289110450336

Epoch: 6| Step: 1
Training loss: 2.0091564655303955
Validation loss: 2.258792056832262

Epoch: 6| Step: 2
Training loss: 2.3213326930999756
Validation loss: 2.2735576296365387

Epoch: 6| Step: 3
Training loss: 2.5221750736236572
Validation loss: 2.3284511027797574

Epoch: 6| Step: 4
Training loss: 2.7238729000091553
Validation loss: 2.4290740054140807

Epoch: 6| Step: 5
Training loss: 3.0478274822235107
Validation loss: 2.4844102192950506

Epoch: 6| Step: 6
Training loss: 2.290219306945801
Validation loss: 2.507847173239595

Epoch: 6| Step: 7
Training loss: 2.840752601623535
Validation loss: 2.4713738964449976

Epoch: 6| Step: 8
Training loss: 2.64713716506958
Validation loss: 2.42042778640665

Epoch: 6| Step: 9
Training loss: 2.8362908363342285
Validation loss: 2.367169658342997

Epoch: 6| Step: 10
Training loss: 2.2224738597869873
Validation loss: 2.333541126661403

Epoch: 6| Step: 11
Training loss: 2.4444055557250977
Validation loss: 2.2988611370004635

Epoch: 6| Step: 12
Training loss: 2.7442855834960938
Validation loss: 2.258708820548109

Epoch: 6| Step: 13
Training loss: 2.426724433898926
Validation loss: 2.226995648876313

Epoch: 186| Step: 0
Training loss: 2.569835662841797
Validation loss: 2.186077566557033

Epoch: 6| Step: 1
Training loss: 2.5238513946533203
Validation loss: 2.148406404320912

Epoch: 6| Step: 2
Training loss: 2.0415401458740234
Validation loss: 2.150635319371377

Epoch: 6| Step: 3
Training loss: 2.6365718841552734
Validation loss: 2.174727415525785

Epoch: 6| Step: 4
Training loss: 2.702993392944336
Validation loss: 2.168797449399066

Epoch: 6| Step: 5
Training loss: 2.9924826622009277
Validation loss: 2.1531383824604813

Epoch: 6| Step: 6
Training loss: 2.3119120597839355
Validation loss: 2.1658913345747095

Epoch: 6| Step: 7
Training loss: 2.6615514755249023
Validation loss: 2.15030583002234

Epoch: 6| Step: 8
Training loss: 0.966187596321106
Validation loss: 2.1362385595998457

Epoch: 6| Step: 9
Training loss: 2.732679843902588
Validation loss: 2.127454492353624

Epoch: 6| Step: 10
Training loss: 2.2996673583984375
Validation loss: 2.120681885750063

Epoch: 6| Step: 11
Training loss: 2.257047414779663
Validation loss: 2.12726850663462

Epoch: 6| Step: 12
Training loss: 2.4972009658813477
Validation loss: 2.1302990477572203

Epoch: 6| Step: 13
Training loss: 3.6524085998535156
Validation loss: 2.126981989029915

Epoch: 187| Step: 0
Training loss: 2.216578483581543
Validation loss: 2.129720434065788

Epoch: 6| Step: 1
Training loss: 2.493157148361206
Validation loss: 2.1366879606759674

Epoch: 6| Step: 2
Training loss: 2.3968210220336914
Validation loss: 2.136943817138672

Epoch: 6| Step: 3
Training loss: 2.133305549621582
Validation loss: 2.1403168657774567

Epoch: 6| Step: 4
Training loss: 2.343841552734375
Validation loss: 2.1325039863586426

Epoch: 6| Step: 5
Training loss: 2.385662794113159
Validation loss: 2.131998723553073

Epoch: 6| Step: 6
Training loss: 2.804365634918213
Validation loss: 2.131600906771998

Epoch: 6| Step: 7
Training loss: 2.408128261566162
Validation loss: 2.1412390534595778

Epoch: 6| Step: 8
Training loss: 2.6671135425567627
Validation loss: 2.138727767493135

Epoch: 6| Step: 9
Training loss: 2.2563133239746094
Validation loss: 2.153249398354561

Epoch: 6| Step: 10
Training loss: 2.693204879760742
Validation loss: 2.1674961018305954

Epoch: 6| Step: 11
Training loss: 2.8175148963928223
Validation loss: 2.162986545152562

Epoch: 6| Step: 12
Training loss: 1.9418233633041382
Validation loss: 2.1816674304264847

Epoch: 6| Step: 13
Training loss: 1.7753912210464478
Validation loss: 2.182685839232578

Epoch: 188| Step: 0
Training loss: 2.284362316131592
Validation loss: 2.1930224049475884

Epoch: 6| Step: 1
Training loss: 2.5312700271606445
Validation loss: 2.1942408007960164

Epoch: 6| Step: 2
Training loss: 1.8204598426818848
Validation loss: 2.1884111845365135

Epoch: 6| Step: 3
Training loss: 2.7278013229370117
Validation loss: 2.188275724328974

Epoch: 6| Step: 4
Training loss: 2.715289831161499
Validation loss: 2.1944280901262836

Epoch: 6| Step: 5
Training loss: 2.6708929538726807
Validation loss: 2.2086295389359996

Epoch: 6| Step: 6
Training loss: 2.271512508392334
Validation loss: 2.212294988734748

Epoch: 6| Step: 7
Training loss: 1.8601844310760498
Validation loss: 2.2010289622891333

Epoch: 6| Step: 8
Training loss: 2.0085906982421875
Validation loss: 2.1850382563888386

Epoch: 6| Step: 9
Training loss: 2.2242043018341064
Validation loss: 2.2730884603274766

Epoch: 6| Step: 10
Training loss: 2.721181869506836
Validation loss: 2.3390559432327107

Epoch: 6| Step: 11
Training loss: 2.813410997390747
Validation loss: 2.3537086697034937

Epoch: 6| Step: 12
Training loss: 2.4176206588745117
Validation loss: 2.3432885869856803

Epoch: 6| Step: 13
Training loss: 3.423935890197754
Validation loss: 2.292987028757731

Epoch: 189| Step: 0
Training loss: 2.932284355163574
Validation loss: 2.2143625866982246

Epoch: 6| Step: 1
Training loss: 2.2981209754943848
Validation loss: 2.168958748540571

Epoch: 6| Step: 2
Training loss: 2.074664354324341
Validation loss: 2.1707577167018766

Epoch: 6| Step: 3
Training loss: 1.9388186931610107
Validation loss: 2.1475030504247195

Epoch: 6| Step: 4
Training loss: 2.8147027492523193
Validation loss: 2.1374535176061813

Epoch: 6| Step: 5
Training loss: 1.6214096546173096
Validation loss: 2.123175972251482

Epoch: 6| Step: 6
Training loss: 2.6257081031799316
Validation loss: 2.123529503422399

Epoch: 6| Step: 7
Training loss: 1.9775699377059937
Validation loss: 2.1105865419551892

Epoch: 6| Step: 8
Training loss: 2.8022284507751465
Validation loss: 2.1150676742676766

Epoch: 6| Step: 9
Training loss: 2.359285354614258
Validation loss: 2.1103630104372577

Epoch: 6| Step: 10
Training loss: 1.8606626987457275
Validation loss: 2.100242648073422

Epoch: 6| Step: 11
Training loss: 2.7974586486816406
Validation loss: 2.1011720652221353

Epoch: 6| Step: 12
Training loss: 2.9924280643463135
Validation loss: 2.0973436127426806

Epoch: 6| Step: 13
Training loss: 2.7526512145996094
Validation loss: 2.090885813518237

Epoch: 190| Step: 0
Training loss: 2.3653581142425537
Validation loss: 2.091166942350326

Epoch: 6| Step: 1
Training loss: 2.2233023643493652
Validation loss: 2.0959327118371123

Epoch: 6| Step: 2
Training loss: 3.6538777351379395
Validation loss: 2.0983902869686

Epoch: 6| Step: 3
Training loss: 2.9160094261169434
Validation loss: 2.094484808624432

Epoch: 6| Step: 4
Training loss: 2.3967337608337402
Validation loss: 2.095062471205188

Epoch: 6| Step: 5
Training loss: 2.4986391067504883
Validation loss: 2.1037345112011

Epoch: 6| Step: 6
Training loss: 2.4592747688293457
Validation loss: 2.0949774711362776

Epoch: 6| Step: 7
Training loss: 1.7964367866516113
Validation loss: 2.1048945201340543

Epoch: 6| Step: 8
Training loss: 2.2082457542419434
Validation loss: 2.105319356405607

Epoch: 6| Step: 9
Training loss: 1.5861626863479614
Validation loss: 2.1213848283213954

Epoch: 6| Step: 10
Training loss: 2.450418472290039
Validation loss: 2.1286638090687413

Epoch: 6| Step: 11
Training loss: 2.572624921798706
Validation loss: 2.1389141954401487

Epoch: 6| Step: 12
Training loss: 2.2460832595825195
Validation loss: 2.136228766492618

Epoch: 6| Step: 13
Training loss: 2.555753469467163
Validation loss: 2.13083384113927

Epoch: 191| Step: 0
Training loss: 2.148895740509033
Validation loss: 2.1417599852367113

Epoch: 6| Step: 1
Training loss: 2.6738076210021973
Validation loss: 2.1329448300023235

Epoch: 6| Step: 2
Training loss: 2.3280420303344727
Validation loss: 2.1252508073724727

Epoch: 6| Step: 3
Training loss: 2.4455184936523438
Validation loss: 2.116155280861803

Epoch: 6| Step: 4
Training loss: 2.221097469329834
Validation loss: 2.110020637512207

Epoch: 6| Step: 5
Training loss: 2.768127918243408
Validation loss: 2.116155880753712

Epoch: 6| Step: 6
Training loss: 2.1301255226135254
Validation loss: 2.14358332849318

Epoch: 6| Step: 7
Training loss: 2.176095485687256
Validation loss: 2.15263488728513

Epoch: 6| Step: 8
Training loss: 3.184133529663086
Validation loss: 2.151598204848587

Epoch: 6| Step: 9
Training loss: 2.4377336502075195
Validation loss: 2.1508407631228046

Epoch: 6| Step: 10
Training loss: 2.411233425140381
Validation loss: 2.1467413953555528

Epoch: 6| Step: 11
Training loss: 2.09346079826355
Validation loss: 2.1270446764525546

Epoch: 6| Step: 12
Training loss: 2.086287021636963
Validation loss: 2.115240462364689

Epoch: 6| Step: 13
Training loss: 3.005415916442871
Validation loss: 2.1002009299493607

Epoch: 192| Step: 0
Training loss: 2.3830597400665283
Validation loss: 2.1074421380155828

Epoch: 6| Step: 1
Training loss: 2.1729722023010254
Validation loss: 2.112367706914102

Epoch: 6| Step: 2
Training loss: 3.4000167846679688
Validation loss: 2.1197002626234487

Epoch: 6| Step: 3
Training loss: 2.0516605377197266
Validation loss: 2.137111897109657

Epoch: 6| Step: 4
Training loss: 3.3902857303619385
Validation loss: 2.140013958818169

Epoch: 6| Step: 5
Training loss: 2.2182579040527344
Validation loss: 2.14058623262631

Epoch: 6| Step: 6
Training loss: 2.371553897857666
Validation loss: 2.163931646654683

Epoch: 6| Step: 7
Training loss: 2.3465676307678223
Validation loss: 2.1654461122328237

Epoch: 6| Step: 8
Training loss: 2.27585768699646
Validation loss: 2.147268223506148

Epoch: 6| Step: 9
Training loss: 2.4586052894592285
Validation loss: 2.1216903245577248

Epoch: 6| Step: 10
Training loss: 2.0823869705200195
Validation loss: 2.1161527774667226

Epoch: 6| Step: 11
Training loss: 1.9584847688674927
Validation loss: 2.1072734709708922

Epoch: 6| Step: 12
Training loss: 2.4305601119995117
Validation loss: 2.0904652944175144

Epoch: 6| Step: 13
Training loss: 1.6360925436019897
Validation loss: 2.091586261667231

Epoch: 193| Step: 0
Training loss: 2.5303854942321777
Validation loss: 2.09926930550606

Epoch: 6| Step: 1
Training loss: 1.9386423826217651
Validation loss: 2.098610693408597

Epoch: 6| Step: 2
Training loss: 1.8606441020965576
Validation loss: 2.104743290972966

Epoch: 6| Step: 3
Training loss: 1.9614434242248535
Validation loss: 2.097659416096185

Epoch: 6| Step: 4
Training loss: 2.300100326538086
Validation loss: 2.1064768632253013

Epoch: 6| Step: 5
Training loss: 1.9967514276504517
Validation loss: 2.1035438814470844

Epoch: 6| Step: 6
Training loss: 2.744877338409424
Validation loss: 2.1114034909074024

Epoch: 6| Step: 7
Training loss: 2.2599706649780273
Validation loss: 2.115263754321683

Epoch: 6| Step: 8
Training loss: 2.576760768890381
Validation loss: 2.1180836180204987

Epoch: 6| Step: 9
Training loss: 2.1831870079040527
Validation loss: 2.1273843780640633

Epoch: 6| Step: 10
Training loss: 2.8964967727661133
Validation loss: 2.1225619239191853

Epoch: 6| Step: 11
Training loss: 2.4968886375427246
Validation loss: 2.1088944250537502

Epoch: 6| Step: 12
Training loss: 3.2526707649230957
Validation loss: 2.1077969638250207

Epoch: 6| Step: 13
Training loss: 2.462153196334839
Validation loss: 2.1080548148001395

Epoch: 194| Step: 0
Training loss: 2.6115729808807373
Validation loss: 2.1139848975725073

Epoch: 6| Step: 1
Training loss: 2.087435722351074
Validation loss: 2.118106480567686

Epoch: 6| Step: 2
Training loss: 2.470048427581787
Validation loss: 2.1164285726444696

Epoch: 6| Step: 3
Training loss: 1.869879961013794
Validation loss: 2.126042504464426

Epoch: 6| Step: 4
Training loss: 2.1524224281311035
Validation loss: 2.1255106233781382

Epoch: 6| Step: 5
Training loss: 2.742541790008545
Validation loss: 2.105824283374253

Epoch: 6| Step: 6
Training loss: 3.3023016452789307
Validation loss: 2.115175261292406

Epoch: 6| Step: 7
Training loss: 2.1652700901031494
Validation loss: 2.125906918638496

Epoch: 6| Step: 8
Training loss: 2.5792086124420166
Validation loss: 2.11316192278298

Epoch: 6| Step: 9
Training loss: 2.6915283203125
Validation loss: 2.1124344333525626

Epoch: 6| Step: 10
Training loss: 2.426527261734009
Validation loss: 2.1379500076334965

Epoch: 6| Step: 11
Training loss: 1.7403762340545654
Validation loss: 2.1239819962491273

Epoch: 6| Step: 12
Training loss: 2.333191156387329
Validation loss: 2.1329569880680372

Epoch: 6| Step: 13
Training loss: 1.615273356437683
Validation loss: 2.1359277438091975

Epoch: 195| Step: 0
Training loss: 2.0388660430908203
Validation loss: 2.1239767048948552

Epoch: 6| Step: 1
Training loss: 2.0662426948547363
Validation loss: 2.1332765830460416

Epoch: 6| Step: 2
Training loss: 2.1045961380004883
Validation loss: 2.13876840888813

Epoch: 6| Step: 3
Training loss: 2.581448554992676
Validation loss: 2.1538119675010763

Epoch: 6| Step: 4
Training loss: 2.186654806137085
Validation loss: 2.176387886206309

Epoch: 6| Step: 5
Training loss: 2.2934813499450684
Validation loss: 2.2202803575864403

Epoch: 6| Step: 6
Training loss: 2.3096492290496826
Validation loss: 2.218501265330981

Epoch: 6| Step: 7
Training loss: 2.726097583770752
Validation loss: 2.2422417312540035

Epoch: 6| Step: 8
Training loss: 2.787733554840088
Validation loss: 2.2501836463969243

Epoch: 6| Step: 9
Training loss: 2.195916175842285
Validation loss: 2.242931053202639

Epoch: 6| Step: 10
Training loss: 2.8630659580230713
Validation loss: 2.2307164643400457

Epoch: 6| Step: 11
Training loss: 2.4693362712860107
Validation loss: 2.211143729507282

Epoch: 6| Step: 12
Training loss: 2.435920238494873
Validation loss: 2.211836100906454

Epoch: 6| Step: 13
Training loss: 2.2923426628112793
Validation loss: 2.17371388148236

Epoch: 196| Step: 0
Training loss: 1.7096335887908936
Validation loss: 2.181729434638895

Epoch: 6| Step: 1
Training loss: 2.190837860107422
Validation loss: 2.169013592504686

Epoch: 6| Step: 2
Training loss: 2.822354555130005
Validation loss: 2.1727690286533807

Epoch: 6| Step: 3
Training loss: 2.9081060886383057
Validation loss: 2.1641191846580914

Epoch: 6| Step: 4
Training loss: 2.1352715492248535
Validation loss: 2.1470341323524393

Epoch: 6| Step: 5
Training loss: 1.8646962642669678
Validation loss: 2.132653642726201

Epoch: 6| Step: 6
Training loss: 1.8552680015563965
Validation loss: 2.1281551391847673

Epoch: 6| Step: 7
Training loss: 2.865828275680542
Validation loss: 2.1100101317128828

Epoch: 6| Step: 8
Training loss: 2.5704801082611084
Validation loss: 2.104096161421909

Epoch: 6| Step: 9
Training loss: 2.0788567066192627
Validation loss: 2.076360114159123

Epoch: 6| Step: 10
Training loss: 2.227428913116455
Validation loss: 2.079450729072735

Epoch: 6| Step: 11
Training loss: 2.459348201751709
Validation loss: 2.0735628233161023

Epoch: 6| Step: 12
Training loss: 2.766242742538452
Validation loss: 2.071243770660893

Epoch: 6| Step: 13
Training loss: 2.9262514114379883
Validation loss: 2.0644899363158853

Epoch: 197| Step: 0
Training loss: 2.312964916229248
Validation loss: 2.0675621237806094

Epoch: 6| Step: 1
Training loss: 2.064862012863159
Validation loss: 2.0701869328816733

Epoch: 6| Step: 2
Training loss: 2.6383140087127686
Validation loss: 2.082615752373972

Epoch: 6| Step: 3
Training loss: 2.365330696105957
Validation loss: 2.0768530650805404

Epoch: 6| Step: 4
Training loss: 2.1115946769714355
Validation loss: 2.0772391903784966

Epoch: 6| Step: 5
Training loss: 2.5972118377685547
Validation loss: 2.0769165023680656

Epoch: 6| Step: 6
Training loss: 2.815703868865967
Validation loss: 2.077106084874881

Epoch: 6| Step: 7
Training loss: 2.9029178619384766
Validation loss: 2.0841442667027956

Epoch: 6| Step: 8
Training loss: 1.7976813316345215
Validation loss: 2.087793682211189

Epoch: 6| Step: 9
Training loss: 2.4933362007141113
Validation loss: 2.1021787069177114

Epoch: 6| Step: 10
Training loss: 2.5383667945861816
Validation loss: 2.117083316208214

Epoch: 6| Step: 11
Training loss: 2.252610206604004
Validation loss: 2.121672248327604

Epoch: 6| Step: 12
Training loss: 2.1257617473602295
Validation loss: 2.140140820575017

Epoch: 6| Step: 13
Training loss: 1.9123384952545166
Validation loss: 2.1527530352274575

Epoch: 198| Step: 0
Training loss: 1.836307168006897
Validation loss: 2.1829965409412178

Epoch: 6| Step: 1
Training loss: 2.1255812644958496
Validation loss: 2.224431648049303

Epoch: 6| Step: 2
Training loss: 2.6460604667663574
Validation loss: 2.2337972041099303

Epoch: 6| Step: 3
Training loss: 2.187562942504883
Validation loss: 2.2413069214872134

Epoch: 6| Step: 4
Training loss: 1.9451351165771484
Validation loss: 2.260331871689007

Epoch: 6| Step: 5
Training loss: 2.225069999694824
Validation loss: 2.2435006326244724

Epoch: 6| Step: 6
Training loss: 2.182788610458374
Validation loss: 2.2226275731158514

Epoch: 6| Step: 7
Training loss: 2.4341514110565186
Validation loss: 2.1961987608222553

Epoch: 6| Step: 8
Training loss: 2.5438222885131836
Validation loss: 2.1822594750312065

Epoch: 6| Step: 9
Training loss: 2.714779853820801
Validation loss: 2.1933971194810766

Epoch: 6| Step: 10
Training loss: 2.1057145595550537
Validation loss: 2.244183394216722

Epoch: 6| Step: 11
Training loss: 2.6862714290618896
Validation loss: 2.2725569868600495

Epoch: 6| Step: 12
Training loss: 3.5115561485290527
Validation loss: 2.2186975363762147

Epoch: 6| Step: 13
Training loss: 1.7061009407043457
Validation loss: 2.1661273253861295

Epoch: 199| Step: 0
Training loss: 2.123910903930664
Validation loss: 2.1348873466573735

Epoch: 6| Step: 1
Training loss: 2.1863863468170166
Validation loss: 2.1330806004103793

Epoch: 6| Step: 2
Training loss: 1.537916898727417
Validation loss: 2.1145938878418296

Epoch: 6| Step: 3
Training loss: 2.1708147525787354
Validation loss: 2.0961782957917903

Epoch: 6| Step: 4
Training loss: 1.9971305131912231
Validation loss: 2.080318707291798

Epoch: 6| Step: 5
Training loss: 2.6186680793762207
Validation loss: 2.0900539582775486

Epoch: 6| Step: 6
Training loss: 2.5610337257385254
Validation loss: 2.0745980944684757

Epoch: 6| Step: 7
Training loss: 2.8821988105773926
Validation loss: 2.088177869396825

Epoch: 6| Step: 8
Training loss: 2.2489943504333496
Validation loss: 2.109031988728431

Epoch: 6| Step: 9
Training loss: 2.1188101768493652
Validation loss: 2.1177137000586397

Epoch: 6| Step: 10
Training loss: 3.320096015930176
Validation loss: 2.1276278098424277

Epoch: 6| Step: 11
Training loss: 2.3641419410705566
Validation loss: 2.1237135830745903

Epoch: 6| Step: 12
Training loss: 2.7764439582824707
Validation loss: 2.1202001802382933

Epoch: 6| Step: 13
Training loss: 2.3124778270721436
Validation loss: 2.100290988081245

Epoch: 200| Step: 0
Training loss: 2.2009851932525635
Validation loss: 2.0953172458115445

Epoch: 6| Step: 1
Training loss: 2.303274631500244
Validation loss: 2.101929062156267

Epoch: 6| Step: 2
Training loss: 1.9479385614395142
Validation loss: 2.0850346344773487

Epoch: 6| Step: 3
Training loss: 2.9946508407592773
Validation loss: 2.081785464799532

Epoch: 6| Step: 4
Training loss: 2.303727149963379
Validation loss: 2.071637189516457

Epoch: 6| Step: 5
Training loss: 1.9519739151000977
Validation loss: 2.0753787640602357

Epoch: 6| Step: 6
Training loss: 2.5094470977783203
Validation loss: 2.0742262140397103

Epoch: 6| Step: 7
Training loss: 2.2124075889587402
Validation loss: 2.0733444716340754

Epoch: 6| Step: 8
Training loss: 2.711705207824707
Validation loss: 2.076185313604211

Epoch: 6| Step: 9
Training loss: 2.6852877140045166
Validation loss: 2.0912143850839264

Epoch: 6| Step: 10
Training loss: 2.138813018798828
Validation loss: 2.08798788568025

Epoch: 6| Step: 11
Training loss: 2.1605372428894043
Validation loss: 2.0984800887364212

Epoch: 6| Step: 12
Training loss: 2.6686511039733887
Validation loss: 2.0899234612782798

Epoch: 6| Step: 13
Training loss: 2.269583225250244
Validation loss: 2.0953225192203315

Epoch: 201| Step: 0
Training loss: 2.6641807556152344
Validation loss: 2.109003266980571

Epoch: 6| Step: 1
Training loss: 2.802109718322754
Validation loss: 2.1347043770615772

Epoch: 6| Step: 2
Training loss: 2.1781864166259766
Validation loss: 2.1326428972264773

Epoch: 6| Step: 3
Training loss: 2.231890916824341
Validation loss: 2.1526346040028397

Epoch: 6| Step: 4
Training loss: 1.4759702682495117
Validation loss: 2.168719765960529

Epoch: 6| Step: 5
Training loss: 2.1194024085998535
Validation loss: 2.2050074146639917

Epoch: 6| Step: 6
Training loss: 2.8417205810546875
Validation loss: 2.1935658352349394

Epoch: 6| Step: 7
Training loss: 1.9276220798492432
Validation loss: 2.1943333892412085

Epoch: 6| Step: 8
Training loss: 2.7729580402374268
Validation loss: 2.1988310557539745

Epoch: 6| Step: 9
Training loss: 2.247408151626587
Validation loss: 2.2077451329077444

Epoch: 6| Step: 10
Training loss: 2.5066099166870117
Validation loss: 2.156917828385548

Epoch: 6| Step: 11
Training loss: 1.9019862413406372
Validation loss: 2.1423355443503267

Epoch: 6| Step: 12
Training loss: 2.5098929405212402
Validation loss: 2.135101964396815

Epoch: 6| Step: 13
Training loss: 2.8843672275543213
Validation loss: 2.139898743680728

Epoch: 202| Step: 0
Training loss: 2.2814691066741943
Validation loss: 2.1196304662253267

Epoch: 6| Step: 1
Training loss: 2.0954642295837402
Validation loss: 2.1076605973705167

Epoch: 6| Step: 2
Training loss: 2.0564117431640625
Validation loss: 2.087505517467376

Epoch: 6| Step: 3
Training loss: 2.5638561248779297
Validation loss: 2.0877958190056587

Epoch: 6| Step: 4
Training loss: 2.3340179920196533
Validation loss: 2.0885939034082557

Epoch: 6| Step: 5
Training loss: 1.632999300956726
Validation loss: 2.0798604180735927

Epoch: 6| Step: 6
Training loss: 2.104887008666992
Validation loss: 2.0677655461013957

Epoch: 6| Step: 7
Training loss: 3.1622605323791504
Validation loss: 2.0811379955660914

Epoch: 6| Step: 8
Training loss: 2.567868709564209
Validation loss: 2.0767024460659234

Epoch: 6| Step: 9
Training loss: 2.0275535583496094
Validation loss: 2.074214321310802

Epoch: 6| Step: 10
Training loss: 2.1577796936035156
Validation loss: 2.0732636797812676

Epoch: 6| Step: 11
Training loss: 2.9778826236724854
Validation loss: 2.0743078083120365

Epoch: 6| Step: 12
Training loss: 2.7376155853271484
Validation loss: 2.067206774988482

Epoch: 6| Step: 13
Training loss: 2.196248769760132
Validation loss: 2.067057878740372

Epoch: 203| Step: 0
Training loss: 2.5352354049682617
Validation loss: 2.079332951576479

Epoch: 6| Step: 1
Training loss: 2.333704948425293
Validation loss: 2.0894733141827326

Epoch: 6| Step: 2
Training loss: 1.8290235996246338
Validation loss: 2.093473567757555

Epoch: 6| Step: 3
Training loss: 2.1825618743896484
Validation loss: 2.0619531421251196

Epoch: 6| Step: 4
Training loss: 2.1623241901397705
Validation loss: 2.0729391831223682

Epoch: 6| Step: 5
Training loss: 2.756519317626953
Validation loss: 2.0626662726043374

Epoch: 6| Step: 6
Training loss: 2.5405263900756836
Validation loss: 2.0610721470207296

Epoch: 6| Step: 7
Training loss: 1.6227734088897705
Validation loss: 2.063193746792373

Epoch: 6| Step: 8
Training loss: 2.6099748611450195
Validation loss: 2.078151085043466

Epoch: 6| Step: 9
Training loss: 2.3077328205108643
Validation loss: 2.0803194968931136

Epoch: 6| Step: 10
Training loss: 2.3487048149108887
Validation loss: 2.0879496733347573

Epoch: 6| Step: 11
Training loss: 1.699267029762268
Validation loss: 2.103427463962186

Epoch: 6| Step: 12
Training loss: 3.3289387226104736
Validation loss: 2.121866128777945

Epoch: 6| Step: 13
Training loss: 2.662355661392212
Validation loss: 2.1272566933785715

Epoch: 204| Step: 0
Training loss: 2.1422996520996094
Validation loss: 2.1450659562182683

Epoch: 6| Step: 1
Training loss: 1.9330230951309204
Validation loss: 2.146191430348222

Epoch: 6| Step: 2
Training loss: 2.610440731048584
Validation loss: 2.1311545525827715

Epoch: 6| Step: 3
Training loss: 2.019678831100464
Validation loss: 2.1297098641754477

Epoch: 6| Step: 4
Training loss: 2.7519237995147705
Validation loss: 2.1226425939990627

Epoch: 6| Step: 5
Training loss: 2.286092758178711
Validation loss: 2.1197384941962456

Epoch: 6| Step: 6
Training loss: 2.160883903503418
Validation loss: 2.118049693363969

Epoch: 6| Step: 7
Training loss: 2.236830711364746
Validation loss: 2.1153536406896447

Epoch: 6| Step: 8
Training loss: 2.5179648399353027
Validation loss: 2.1366518133430072

Epoch: 6| Step: 9
Training loss: 1.8981467485427856
Validation loss: 2.1439932815490232

Epoch: 6| Step: 10
Training loss: 2.7477035522460938
Validation loss: 2.1450752109609623

Epoch: 6| Step: 11
Training loss: 1.7310705184936523
Validation loss: 2.1584687053516345

Epoch: 6| Step: 12
Training loss: 2.373258113861084
Validation loss: 2.177720169867239

Epoch: 6| Step: 13
Training loss: 3.750359058380127
Validation loss: 2.163726637440343

Epoch: 205| Step: 0
Training loss: 1.7177547216415405
Validation loss: 2.1448414658987396

Epoch: 6| Step: 1
Training loss: 2.667776584625244
Validation loss: 2.1321179507881083

Epoch: 6| Step: 2
Training loss: 2.6476471424102783
Validation loss: 2.11832986211264

Epoch: 6| Step: 3
Training loss: 1.982372760772705
Validation loss: 2.129726363766578

Epoch: 6| Step: 4
Training loss: 2.0715692043304443
Validation loss: 2.142325591015559

Epoch: 6| Step: 5
Training loss: 2.8633086681365967
Validation loss: 2.153722297760748

Epoch: 6| Step: 6
Training loss: 2.3888254165649414
Validation loss: 2.1236508789882866

Epoch: 6| Step: 7
Training loss: 2.134413242340088
Validation loss: 2.1282576643010622

Epoch: 6| Step: 8
Training loss: 1.762495756149292
Validation loss: 2.146134712362802

Epoch: 6| Step: 9
Training loss: 2.2243924140930176
Validation loss: 2.165700658675163

Epoch: 6| Step: 10
Training loss: 3.1204018592834473
Validation loss: 2.219763922434981

Epoch: 6| Step: 11
Training loss: 2.2123422622680664
Validation loss: 2.1954056063006

Epoch: 6| Step: 12
Training loss: 2.343045711517334
Validation loss: 2.1691504575872935

Epoch: 6| Step: 13
Training loss: 2.719855308532715
Validation loss: 2.1286030148947113

Epoch: 206| Step: 0
Training loss: 2.061845302581787
Validation loss: 2.0850923753553823

Epoch: 6| Step: 1
Training loss: 2.188422441482544
Validation loss: 2.0715348105276785

Epoch: 6| Step: 2
Training loss: 2.5192997455596924
Validation loss: 2.0713540251537035

Epoch: 6| Step: 3
Training loss: 1.983930230140686
Validation loss: 2.0766207582207135

Epoch: 6| Step: 4
Training loss: 2.312947988510132
Validation loss: 2.0780636918160225

Epoch: 6| Step: 5
Training loss: 2.5475826263427734
Validation loss: 2.071490054489464

Epoch: 6| Step: 6
Training loss: 2.760861873626709
Validation loss: 2.0743127176838536

Epoch: 6| Step: 7
Training loss: 2.4151082038879395
Validation loss: 2.069860771138181

Epoch: 6| Step: 8
Training loss: 2.6463494300842285
Validation loss: 2.0710925825180544

Epoch: 6| Step: 9
Training loss: 1.6340405941009521
Validation loss: 2.0723916023008284

Epoch: 6| Step: 10
Training loss: 2.1349897384643555
Validation loss: 2.076868890434183

Epoch: 6| Step: 11
Training loss: 3.027273654937744
Validation loss: 2.073142054260418

Epoch: 6| Step: 12
Training loss: 2.074434518814087
Validation loss: 2.0842833724073184

Epoch: 6| Step: 13
Training loss: 2.1824684143066406
Validation loss: 2.0965293966313845

Epoch: 207| Step: 0
Training loss: 1.963549017906189
Validation loss: 2.090313970401723

Epoch: 6| Step: 1
Training loss: 3.010347366333008
Validation loss: 2.103527548492596

Epoch: 6| Step: 2
Training loss: 2.625499725341797
Validation loss: 2.106085629873378

Epoch: 6| Step: 3
Training loss: 2.8693487644195557
Validation loss: 2.100178369911768

Epoch: 6| Step: 4
Training loss: 2.337733745574951
Validation loss: 2.083464114896713

Epoch: 6| Step: 5
Training loss: 2.4955739974975586
Validation loss: 2.086534100194131

Epoch: 6| Step: 6
Training loss: 2.350769281387329
Validation loss: 2.0808430384564143

Epoch: 6| Step: 7
Training loss: 2.952440023422241
Validation loss: 2.0808266773018786

Epoch: 6| Step: 8
Training loss: 1.6684229373931885
Validation loss: 2.0804149540521766

Epoch: 6| Step: 9
Training loss: 1.6541451215744019
Validation loss: 2.078219603466731

Epoch: 6| Step: 10
Training loss: 2.6182310581207275
Validation loss: 2.0766613534701768

Epoch: 6| Step: 11
Training loss: 1.9900538921356201
Validation loss: 2.0962008558293825

Epoch: 6| Step: 12
Training loss: 2.392275810241699
Validation loss: 2.0886270333361883

Epoch: 6| Step: 13
Training loss: 1.126587152481079
Validation loss: 2.0896782195696266

Epoch: 208| Step: 0
Training loss: 2.635409355163574
Validation loss: 2.0921430882587226

Epoch: 6| Step: 1
Training loss: 1.8259142637252808
Validation loss: 2.0924559588073404

Epoch: 6| Step: 2
Training loss: 1.5119467973709106
Validation loss: 2.0924338909887497

Epoch: 6| Step: 3
Training loss: 2.3176121711730957
Validation loss: 2.0889139688143166

Epoch: 6| Step: 4
Training loss: 2.2305421829223633
Validation loss: 2.0901059848006054

Epoch: 6| Step: 5
Training loss: 2.512202024459839
Validation loss: 2.085197288502929

Epoch: 6| Step: 6
Training loss: 2.248537540435791
Validation loss: 2.098401564423756

Epoch: 6| Step: 7
Training loss: 3.262467384338379
Validation loss: 2.1225001273616666

Epoch: 6| Step: 8
Training loss: 2.4462437629699707
Validation loss: 2.1172738511075258

Epoch: 6| Step: 9
Training loss: 2.4310858249664307
Validation loss: 2.147226018290366

Epoch: 6| Step: 10
Training loss: 1.7694191932678223
Validation loss: 2.211395529008681

Epoch: 6| Step: 11
Training loss: 3.1126813888549805
Validation loss: 2.2166877613272717

Epoch: 6| Step: 12
Training loss: 1.678736925125122
Validation loss: 2.192449910666353

Epoch: 6| Step: 13
Training loss: 2.514279365539551
Validation loss: 2.161056390372656

Epoch: 209| Step: 0
Training loss: 2.7826757431030273
Validation loss: 2.126113772392273

Epoch: 6| Step: 1
Training loss: 2.8473730087280273
Validation loss: 2.105607650613272

Epoch: 6| Step: 2
Training loss: 2.4158177375793457
Validation loss: 2.115062053485583

Epoch: 6| Step: 3
Training loss: 1.5301693677902222
Validation loss: 2.1045741368365545

Epoch: 6| Step: 4
Training loss: 2.1165108680725098
Validation loss: 2.1081195569807485

Epoch: 6| Step: 5
Training loss: 2.583268642425537
Validation loss: 2.0960385261043424

Epoch: 6| Step: 6
Training loss: 1.3707189559936523
Validation loss: 2.1071560767389115

Epoch: 6| Step: 7
Training loss: 3.085087776184082
Validation loss: 2.1147652300455237

Epoch: 6| Step: 8
Training loss: 2.1783969402313232
Validation loss: 2.1091555741525467

Epoch: 6| Step: 9
Training loss: 2.535284996032715
Validation loss: 2.1350512068758727

Epoch: 6| Step: 10
Training loss: 2.361217498779297
Validation loss: 2.140910162720629

Epoch: 6| Step: 11
Training loss: 2.4403610229492188
Validation loss: 2.1421189949076664

Epoch: 6| Step: 12
Training loss: 2.477691650390625
Validation loss: 2.136958022271433

Epoch: 6| Step: 13
Training loss: 1.3300849199295044
Validation loss: 2.1231213410695395

Epoch: 210| Step: 0
Training loss: 2.1646127700805664
Validation loss: 2.103968717718637

Epoch: 6| Step: 1
Training loss: 2.2752366065979004
Validation loss: 2.083464437915433

Epoch: 6| Step: 2
Training loss: 2.8852639198303223
Validation loss: 2.081437587738037

Epoch: 6| Step: 3
Training loss: 2.5099782943725586
Validation loss: 2.0604298089140203

Epoch: 6| Step: 4
Training loss: 2.964634895324707
Validation loss: 2.060952783912741

Epoch: 6| Step: 5
Training loss: 2.594599962234497
Validation loss: 2.07162634916203

Epoch: 6| Step: 6
Training loss: 2.4348483085632324
Validation loss: 2.060006103207988

Epoch: 6| Step: 7
Training loss: 2.2723660469055176
Validation loss: 2.0555059063819145

Epoch: 6| Step: 8
Training loss: 1.8784259557724
Validation loss: 2.055480921140281

Epoch: 6| Step: 9
Training loss: 1.6216151714324951
Validation loss: 2.060765633019068

Epoch: 6| Step: 10
Training loss: 2.5344722270965576
Validation loss: 2.06390699519906

Epoch: 6| Step: 11
Training loss: 2.2363295555114746
Validation loss: 2.0867717522446827

Epoch: 6| Step: 12
Training loss: 1.847485065460205
Validation loss: 2.09654022929489

Epoch: 6| Step: 13
Training loss: 2.188786506652832
Validation loss: 2.0870033259032876

Epoch: 211| Step: 0
Training loss: 2.3504951000213623
Validation loss: 2.0911711108299995

Epoch: 6| Step: 1
Training loss: 2.72752046585083
Validation loss: 2.0923881633307344

Epoch: 6| Step: 2
Training loss: 2.526520252227783
Validation loss: 2.0917561079866145

Epoch: 6| Step: 3
Training loss: 2.525097370147705
Validation loss: 2.0783580041700795

Epoch: 6| Step: 4
Training loss: 2.6887686252593994
Validation loss: 2.073945786363335

Epoch: 6| Step: 5
Training loss: 2.2563319206237793
Validation loss: 2.070495065822396

Epoch: 6| Step: 6
Training loss: 1.8563554286956787
Validation loss: 2.065885095186131

Epoch: 6| Step: 7
Training loss: 1.5281842947006226
Validation loss: 2.073325708348264

Epoch: 6| Step: 8
Training loss: 2.44771671295166
Validation loss: 2.0651990470065864

Epoch: 6| Step: 9
Training loss: 3.1471269130706787
Validation loss: 2.062743310005434

Epoch: 6| Step: 10
Training loss: 2.393359661102295
Validation loss: 2.0631813490262596

Epoch: 6| Step: 11
Training loss: 2.2508111000061035
Validation loss: 2.068318740014107

Epoch: 6| Step: 12
Training loss: 2.003537654876709
Validation loss: 2.0588079280750726

Epoch: 6| Step: 13
Training loss: 1.4324095249176025
Validation loss: 2.074413004741874

Epoch: 212| Step: 0
Training loss: 2.063526153564453
Validation loss: 2.066437772525254

Epoch: 6| Step: 1
Training loss: 2.604003667831421
Validation loss: 2.080883779833394

Epoch: 6| Step: 2
Training loss: 2.161853790283203
Validation loss: 2.0935205285267164

Epoch: 6| Step: 3
Training loss: 3.1254618167877197
Validation loss: 2.086098283849737

Epoch: 6| Step: 4
Training loss: 2.1276869773864746
Validation loss: 2.0916914606607087

Epoch: 6| Step: 5
Training loss: 2.1278984546661377
Validation loss: 2.0978328386942544

Epoch: 6| Step: 6
Training loss: 2.0780258178710938
Validation loss: 2.1217331476109003

Epoch: 6| Step: 7
Training loss: 2.379000663757324
Validation loss: 2.122851200001214

Epoch: 6| Step: 8
Training loss: 2.501899242401123
Validation loss: 2.118986988580355

Epoch: 6| Step: 9
Training loss: 2.0268619060516357
Validation loss: 2.1211228191211657

Epoch: 6| Step: 10
Training loss: 2.5826501846313477
Validation loss: 2.1003598256777694

Epoch: 6| Step: 11
Training loss: 2.597672939300537
Validation loss: 2.1039639596016175

Epoch: 6| Step: 12
Training loss: 1.5287647247314453
Validation loss: 2.1034668594278316

Epoch: 6| Step: 13
Training loss: 2.2656545639038086
Validation loss: 2.0946156324878817

Epoch: 213| Step: 0
Training loss: 2.2244105339050293
Validation loss: 2.0984710416486188

Epoch: 6| Step: 1
Training loss: 2.5668771266937256
Validation loss: 2.1117300269424275

Epoch: 6| Step: 2
Training loss: 2.2593941688537598
Validation loss: 2.1532497790551957

Epoch: 6| Step: 3
Training loss: 2.2575387954711914
Validation loss: 2.1447164730359147

Epoch: 6| Step: 4
Training loss: 1.7872655391693115
Validation loss: 2.1593226309745543

Epoch: 6| Step: 5
Training loss: 2.034179449081421
Validation loss: 2.176382387838056

Epoch: 6| Step: 6
Training loss: 2.5050487518310547
Validation loss: 2.1450495053363103

Epoch: 6| Step: 7
Training loss: 2.3749446868896484
Validation loss: 2.1413787872560563

Epoch: 6| Step: 8
Training loss: 2.7372002601623535
Validation loss: 2.132658576452604

Epoch: 6| Step: 9
Training loss: 2.5635275840759277
Validation loss: 2.1175881560130785

Epoch: 6| Step: 10
Training loss: 1.8750476837158203
Validation loss: 2.1228028035932973

Epoch: 6| Step: 11
Training loss: 2.0169899463653564
Validation loss: 2.1399949930047475

Epoch: 6| Step: 12
Training loss: 2.740469455718994
Validation loss: 2.1430637169909734

Epoch: 6| Step: 13
Training loss: 2.5325613021850586
Validation loss: 2.1336450089690504

Epoch: 214| Step: 0
Training loss: 2.104252338409424
Validation loss: 2.1239579005907943

Epoch: 6| Step: 1
Training loss: 2.272960662841797
Validation loss: 2.1387694087079776

Epoch: 6| Step: 2
Training loss: 2.635098934173584
Validation loss: 2.143944150658064

Epoch: 6| Step: 3
Training loss: 2.2584333419799805
Validation loss: 2.1367188217819377

Epoch: 6| Step: 4
Training loss: 2.0968894958496094
Validation loss: 2.1515724043692313

Epoch: 6| Step: 5
Training loss: 3.0752367973327637
Validation loss: 2.1375867615463915

Epoch: 6| Step: 6
Training loss: 2.900442123413086
Validation loss: 2.1302806715811453

Epoch: 6| Step: 7
Training loss: 2.381237030029297
Validation loss: 2.118399635437996

Epoch: 6| Step: 8
Training loss: 2.035698890686035
Validation loss: 2.107689224263673

Epoch: 6| Step: 9
Training loss: 2.282344341278076
Validation loss: 2.1024087731556227

Epoch: 6| Step: 10
Training loss: 1.9644834995269775
Validation loss: 2.098618838094896

Epoch: 6| Step: 11
Training loss: 2.3904337882995605
Validation loss: 2.1309592364936747

Epoch: 6| Step: 12
Training loss: 2.380431652069092
Validation loss: 2.1532933506914365

Epoch: 6| Step: 13
Training loss: 2.3006091117858887
Validation loss: 2.1717127112932104

Epoch: 215| Step: 0
Training loss: 1.601517915725708
Validation loss: 2.1683842264195925

Epoch: 6| Step: 1
Training loss: 2.9089794158935547
Validation loss: 2.1173680290099113

Epoch: 6| Step: 2
Training loss: 2.123471975326538
Validation loss: 2.110567251841227

Epoch: 6| Step: 3
Training loss: 2.7349820137023926
Validation loss: 2.091638724009196

Epoch: 6| Step: 4
Training loss: 2.809663772583008
Validation loss: 2.077936323740149

Epoch: 6| Step: 5
Training loss: 2.1355392932891846
Validation loss: 2.068996221788468

Epoch: 6| Step: 6
Training loss: 2.6837470531463623
Validation loss: 2.078069475389296

Epoch: 6| Step: 7
Training loss: 1.5765526294708252
Validation loss: 2.0883489565182756

Epoch: 6| Step: 8
Training loss: 2.1498332023620605
Validation loss: 2.0915114930880967

Epoch: 6| Step: 9
Training loss: 2.719799518585205
Validation loss: 2.1032591045543714

Epoch: 6| Step: 10
Training loss: 2.81758975982666
Validation loss: 2.111302098920268

Epoch: 6| Step: 11
Training loss: 2.0367517471313477
Validation loss: 2.1335106152360157

Epoch: 6| Step: 12
Training loss: 1.8707143068313599
Validation loss: 2.1313086325122463

Epoch: 6| Step: 13
Training loss: 2.047996997833252
Validation loss: 2.145579291928199

Epoch: 216| Step: 0
Training loss: 1.7282330989837646
Validation loss: 2.133792584942233

Epoch: 6| Step: 1
Training loss: 2.173715353012085
Validation loss: 2.1246786861009497

Epoch: 6| Step: 2
Training loss: 2.110541820526123
Validation loss: 2.1242605127314085

Epoch: 6| Step: 3
Training loss: 2.732815980911255
Validation loss: 2.111055912510041

Epoch: 6| Step: 4
Training loss: 2.425497531890869
Validation loss: 2.113462563483946

Epoch: 6| Step: 5
Training loss: 2.6708436012268066
Validation loss: 2.1207920940973426

Epoch: 6| Step: 6
Training loss: 1.9952541589736938
Validation loss: 2.1100704644315984

Epoch: 6| Step: 7
Training loss: 2.172333240509033
Validation loss: 2.102932794119722

Epoch: 6| Step: 8
Training loss: 1.9218082427978516
Validation loss: 2.101282758097495

Epoch: 6| Step: 9
Training loss: 2.829597234725952
Validation loss: 2.1030754184210174

Epoch: 6| Step: 10
Training loss: 2.308558940887451
Validation loss: 2.100349890288486

Epoch: 6| Step: 11
Training loss: 1.7802377939224243
Validation loss: 2.111219154891147

Epoch: 6| Step: 12
Training loss: 2.7287917137145996
Validation loss: 2.1092594977348083

Epoch: 6| Step: 13
Training loss: 2.4437975883483887
Validation loss: 2.1227447781511533

Epoch: 217| Step: 0
Training loss: 2.60031795501709
Validation loss: 2.094427654820104

Epoch: 6| Step: 1
Training loss: 2.3699934482574463
Validation loss: 2.0730689712750014

Epoch: 6| Step: 2
Training loss: 2.806856632232666
Validation loss: 2.069732548088156

Epoch: 6| Step: 3
Training loss: 1.7712798118591309
Validation loss: 2.056915360112344

Epoch: 6| Step: 4
Training loss: 1.8510715961456299
Validation loss: 2.0476477364058137

Epoch: 6| Step: 5
Training loss: 2.498828887939453
Validation loss: 2.0367377573443997

Epoch: 6| Step: 6
Training loss: 2.223385810852051
Validation loss: 2.0445261770679104

Epoch: 6| Step: 7
Training loss: 2.1878745555877686
Validation loss: 2.0424106761973393

Epoch: 6| Step: 8
Training loss: 2.197946548461914
Validation loss: 2.0388256503689672

Epoch: 6| Step: 9
Training loss: 2.4901928901672363
Validation loss: 2.0410070534675353

Epoch: 6| Step: 10
Training loss: 1.9842220544815063
Validation loss: 2.045509453742735

Epoch: 6| Step: 11
Training loss: 2.532132625579834
Validation loss: 2.046245357041718

Epoch: 6| Step: 12
Training loss: 2.3957948684692383
Validation loss: 2.049527455401677

Epoch: 6| Step: 13
Training loss: 2.1632561683654785
Validation loss: 2.0677500565846763

Epoch: 218| Step: 0
Training loss: 2.1419506072998047
Validation loss: 2.0856760394188667

Epoch: 6| Step: 1
Training loss: 2.888148069381714
Validation loss: 2.0876875538979807

Epoch: 6| Step: 2
Training loss: 2.541259288787842
Validation loss: 2.0845714922874206

Epoch: 6| Step: 3
Training loss: 2.368643283843994
Validation loss: 2.0851738170910905

Epoch: 6| Step: 4
Training loss: 2.0524473190307617
Validation loss: 2.080195567941153

Epoch: 6| Step: 5
Training loss: 2.859221935272217
Validation loss: 2.076239591003746

Epoch: 6| Step: 6
Training loss: 1.6737223863601685
Validation loss: 2.0749985094993346

Epoch: 6| Step: 7
Training loss: 2.045217990875244
Validation loss: 2.0852777906643447

Epoch: 6| Step: 8
Training loss: 2.3260931968688965
Validation loss: 2.081872465789959

Epoch: 6| Step: 9
Training loss: 1.565760850906372
Validation loss: 2.079276814255663

Epoch: 6| Step: 10
Training loss: 1.9755208492279053
Validation loss: 2.0813020660031225

Epoch: 6| Step: 11
Training loss: 2.282923698425293
Validation loss: 2.104973454629221

Epoch: 6| Step: 12
Training loss: 2.4692471027374268
Validation loss: 2.1241207891894924

Epoch: 6| Step: 13
Training loss: 3.503387689590454
Validation loss: 2.131479017196163

Epoch: 219| Step: 0
Training loss: 1.8920691013336182
Validation loss: 2.152078882340462

Epoch: 6| Step: 1
Training loss: 2.1759235858917236
Validation loss: 2.1609592424925936

Epoch: 6| Step: 2
Training loss: 2.7183966636657715
Validation loss: 2.1556776198007728

Epoch: 6| Step: 3
Training loss: 1.8930473327636719
Validation loss: 2.1650801320229807

Epoch: 6| Step: 4
Training loss: 2.3944454193115234
Validation loss: 2.1647287248283305

Epoch: 6| Step: 5
Training loss: 2.4516308307647705
Validation loss: 2.1515608833682154

Epoch: 6| Step: 6
Training loss: 2.540156841278076
Validation loss: 2.175063761331702

Epoch: 6| Step: 7
Training loss: 2.630091667175293
Validation loss: 2.1800875022847164

Epoch: 6| Step: 8
Training loss: 2.078903913497925
Validation loss: 2.174467317519649

Epoch: 6| Step: 9
Training loss: 2.1779046058654785
Validation loss: 2.1797910762089554

Epoch: 6| Step: 10
Training loss: 2.216604471206665
Validation loss: 2.181846131560623

Epoch: 6| Step: 11
Training loss: 2.4899237155914307
Validation loss: 2.166936580852796

Epoch: 6| Step: 12
Training loss: 2.185950756072998
Validation loss: 2.1684438951553835

Epoch: 6| Step: 13
Training loss: 1.8702178001403809
Validation loss: 2.119212204410184

Epoch: 220| Step: 0
Training loss: 2.1028194427490234
Validation loss: 2.116442190703525

Epoch: 6| Step: 1
Training loss: 2.2710912227630615
Validation loss: 2.1305233124763734

Epoch: 6| Step: 2
Training loss: 3.1067612171173096
Validation loss: 2.122150223742249

Epoch: 6| Step: 3
Training loss: 2.1986002922058105
Validation loss: 2.099638209548048

Epoch: 6| Step: 4
Training loss: 2.2623376846313477
Validation loss: 2.0727801169118574

Epoch: 6| Step: 5
Training loss: 2.470101833343506
Validation loss: 2.076539783067601

Epoch: 6| Step: 6
Training loss: 2.403240919113159
Validation loss: 2.086018644353395

Epoch: 6| Step: 7
Training loss: 2.0342676639556885
Validation loss: 2.107344719671434

Epoch: 6| Step: 8
Training loss: 1.1710312366485596
Validation loss: 2.0965225209472

Epoch: 6| Step: 9
Training loss: 2.0808916091918945
Validation loss: 2.089441171256445

Epoch: 6| Step: 10
Training loss: 2.7643213272094727
Validation loss: 2.0806929757518153

Epoch: 6| Step: 11
Training loss: 2.9447617530822754
Validation loss: 2.073452623941565

Epoch: 6| Step: 12
Training loss: 1.8656880855560303
Validation loss: 2.0570116094363633

Epoch: 6| Step: 13
Training loss: 2.157061815261841
Validation loss: 2.0356180257694696

Epoch: 221| Step: 0
Training loss: 1.5381429195404053
Validation loss: 2.040582364605319

Epoch: 6| Step: 1
Training loss: 2.001997709274292
Validation loss: 2.0384333466970794

Epoch: 6| Step: 2
Training loss: 2.3949337005615234
Validation loss: 2.049594945805047

Epoch: 6| Step: 3
Training loss: 2.303217887878418
Validation loss: 2.0526934874955045

Epoch: 6| Step: 4
Training loss: 2.0979576110839844
Validation loss: 2.0730436873692337

Epoch: 6| Step: 5
Training loss: 1.6216400861740112
Validation loss: 2.080229957898458

Epoch: 6| Step: 6
Training loss: 2.2265260219573975
Validation loss: 2.0808758402383454

Epoch: 6| Step: 7
Training loss: 3.3114097118377686
Validation loss: 2.090384761492411

Epoch: 6| Step: 8
Training loss: 2.30869722366333
Validation loss: 2.087167875741118

Epoch: 6| Step: 9
Training loss: 1.9935307502746582
Validation loss: 2.1038562713130826

Epoch: 6| Step: 10
Training loss: 1.3631839752197266
Validation loss: 2.1322844900110716

Epoch: 6| Step: 11
Training loss: 2.679093360900879
Validation loss: 2.1587405076590915

Epoch: 6| Step: 12
Training loss: 2.8573803901672363
Validation loss: 2.1435140794323337

Epoch: 6| Step: 13
Training loss: 3.3083486557006836
Validation loss: 2.1331524720755954

Epoch: 222| Step: 0
Training loss: 2.4819390773773193
Validation loss: 2.1381334489391697

Epoch: 6| Step: 1
Training loss: 1.7144858837127686
Validation loss: 2.1374551032179143

Epoch: 6| Step: 2
Training loss: 2.0507118701934814
Validation loss: 2.1397733829354726

Epoch: 6| Step: 3
Training loss: 2.2982208728790283
Validation loss: 2.1220593247362363

Epoch: 6| Step: 4
Training loss: 2.4504876136779785
Validation loss: 2.103173396920645

Epoch: 6| Step: 5
Training loss: 2.7845358848571777
Validation loss: 2.0847704641280638

Epoch: 6| Step: 6
Training loss: 2.4219765663146973
Validation loss: 2.0597369286321823

Epoch: 6| Step: 7
Training loss: 1.7248787879943848
Validation loss: 2.0490180343709965

Epoch: 6| Step: 8
Training loss: 2.4195656776428223
Validation loss: 2.056085043056037

Epoch: 6| Step: 9
Training loss: 2.2056210041046143
Validation loss: 2.06050120886936

Epoch: 6| Step: 10
Training loss: 2.6726596355438232
Validation loss: 2.063472896493891

Epoch: 6| Step: 11
Training loss: 2.4810099601745605
Validation loss: 2.0591286741277224

Epoch: 6| Step: 12
Training loss: 1.8343725204467773
Validation loss: 2.058785941011162

Epoch: 6| Step: 13
Training loss: 1.7910151481628418
Validation loss: 2.0642359026016726

Epoch: 223| Step: 0
Training loss: 2.063554286956787
Validation loss: 2.0651862903307845

Epoch: 6| Step: 1
Training loss: 2.004042387008667
Validation loss: 2.0635525667539207

Epoch: 6| Step: 2
Training loss: 1.7744053602218628
Validation loss: 2.059977276350862

Epoch: 6| Step: 3
Training loss: 1.8592572212219238
Validation loss: 2.077020965596681

Epoch: 6| Step: 4
Training loss: 2.363450765609741
Validation loss: 2.0974234586120932

Epoch: 6| Step: 5
Training loss: 2.104796886444092
Validation loss: 2.101794381295481

Epoch: 6| Step: 6
Training loss: 1.7912344932556152
Validation loss: 2.1079725398812243

Epoch: 6| Step: 7
Training loss: 2.33786678314209
Validation loss: 2.093188613973638

Epoch: 6| Step: 8
Training loss: 1.9496946334838867
Validation loss: 2.0827041441394436

Epoch: 6| Step: 9
Training loss: 2.5096359252929688
Validation loss: 2.1057287185422835

Epoch: 6| Step: 10
Training loss: 1.8790242671966553
Validation loss: 2.098496831873412

Epoch: 6| Step: 11
Training loss: 3.2809102535247803
Validation loss: 2.089763827221368

Epoch: 6| Step: 12
Training loss: 2.5197410583496094
Validation loss: 2.0862516985144666

Epoch: 6| Step: 13
Training loss: 3.5894229412078857
Validation loss: 2.0893326497847036

Epoch: 224| Step: 0
Training loss: 2.393639087677002
Validation loss: 2.069720329776887

Epoch: 6| Step: 1
Training loss: 2.3886313438415527
Validation loss: 2.0652876169450822

Epoch: 6| Step: 2
Training loss: 2.1022260189056396
Validation loss: 2.0768642938265236

Epoch: 6| Step: 3
Training loss: 2.62843656539917
Validation loss: 2.0831403117026053

Epoch: 6| Step: 4
Training loss: 2.724766731262207
Validation loss: 2.1207836302377845

Epoch: 6| Step: 5
Training loss: 2.6674187183380127
Validation loss: 2.155514581229097

Epoch: 6| Step: 6
Training loss: 2.0720951557159424
Validation loss: 2.1622033785748225

Epoch: 6| Step: 7
Training loss: 1.8980505466461182
Validation loss: 2.1628196034380185

Epoch: 6| Step: 8
Training loss: 2.4052047729492188
Validation loss: 2.142344122291893

Epoch: 6| Step: 9
Training loss: 2.9463918209075928
Validation loss: 2.1143843845654557

Epoch: 6| Step: 10
Training loss: 1.65187406539917
Validation loss: 2.0986592385076706

Epoch: 6| Step: 11
Training loss: 1.338849663734436
Validation loss: 2.11744519459304

Epoch: 6| Step: 12
Training loss: 1.962483286857605
Validation loss: 2.1181789418702484

Epoch: 6| Step: 13
Training loss: 2.176556348800659
Validation loss: 2.1351257011454594

Epoch: 225| Step: 0
Training loss: 1.9351489543914795
Validation loss: 2.1245035330454507

Epoch: 6| Step: 1
Training loss: 2.8516223430633545
Validation loss: 2.1135891176039174

Epoch: 6| Step: 2
Training loss: 2.6342804431915283
Validation loss: 2.1135339275483163

Epoch: 6| Step: 3
Training loss: 1.7673591375350952
Validation loss: 2.103452205657959

Epoch: 6| Step: 4
Training loss: 2.0706331729888916
Validation loss: 2.0873914790409867

Epoch: 6| Step: 5
Training loss: 1.4637160301208496
Validation loss: 2.080691405521926

Epoch: 6| Step: 6
Training loss: 3.169034004211426
Validation loss: 2.0890114743222474

Epoch: 6| Step: 7
Training loss: 1.878972053527832
Validation loss: 2.065034597150741

Epoch: 6| Step: 8
Training loss: 2.238682985305786
Validation loss: 2.069251629614061

Epoch: 6| Step: 9
Training loss: 1.8060849905014038
Validation loss: 2.073497336397889

Epoch: 6| Step: 10
Training loss: 2.4046995639801025
Validation loss: 2.071194186005541

Epoch: 6| Step: 11
Training loss: 2.773280143737793
Validation loss: 2.0743798312320503

Epoch: 6| Step: 12
Training loss: 2.2208518981933594
Validation loss: 2.0544506875417565

Epoch: 6| Step: 13
Training loss: 1.7776799201965332
Validation loss: 2.033741153696532

Epoch: 226| Step: 0
Training loss: 2.757542610168457
Validation loss: 2.039028003651609

Epoch: 6| Step: 1
Training loss: 1.8784822225570679
Validation loss: 2.040692353761324

Epoch: 6| Step: 2
Training loss: 2.1249823570251465
Validation loss: 2.031526500178922

Epoch: 6| Step: 3
Training loss: 2.8683717250823975
Validation loss: 2.0289552903944448

Epoch: 6| Step: 4
Training loss: 2.3497281074523926
Validation loss: 2.0329841503532986

Epoch: 6| Step: 5
Training loss: 2.4184694290161133
Validation loss: 2.030359519425259

Epoch: 6| Step: 6
Training loss: 2.4260177612304688
Validation loss: 2.032399444169896

Epoch: 6| Step: 7
Training loss: 2.0594451427459717
Validation loss: 2.053261228787002

Epoch: 6| Step: 8
Training loss: 1.5629569292068481
Validation loss: 2.064822126460332

Epoch: 6| Step: 9
Training loss: 1.6421507596969604
Validation loss: 2.083908410482509

Epoch: 6| Step: 10
Training loss: 2.733614444732666
Validation loss: 2.1170855222209806

Epoch: 6| Step: 11
Training loss: 2.3739006519317627
Validation loss: 2.1312057766863095

Epoch: 6| Step: 12
Training loss: 2.0974905490875244
Validation loss: 2.1369181576595513

Epoch: 6| Step: 13
Training loss: 2.27974796295166
Validation loss: 2.118543517205023

Epoch: 227| Step: 0
Training loss: 1.8135700225830078
Validation loss: 2.1095469613229074

Epoch: 6| Step: 1
Training loss: 2.0084753036499023
Validation loss: 2.103890031896612

Epoch: 6| Step: 2
Training loss: 2.6326234340667725
Validation loss: 2.09749420227543

Epoch: 6| Step: 3
Training loss: 2.4986045360565186
Validation loss: 2.1021777711888796

Epoch: 6| Step: 4
Training loss: 1.888405680656433
Validation loss: 2.0943597132159817

Epoch: 6| Step: 5
Training loss: 2.686591148376465
Validation loss: 2.1085236508359193

Epoch: 6| Step: 6
Training loss: 2.880749464035034
Validation loss: 2.138093615090975

Epoch: 6| Step: 7
Training loss: 2.1490235328674316
Validation loss: 2.1568110117348294

Epoch: 6| Step: 8
Training loss: 1.9536571502685547
Validation loss: 2.1689899095924954

Epoch: 6| Step: 9
Training loss: 2.083507537841797
Validation loss: 2.190227318835515

Epoch: 6| Step: 10
Training loss: 1.9353063106536865
Validation loss: 2.1713257323029223

Epoch: 6| Step: 11
Training loss: 2.346290111541748
Validation loss: 2.16313232914094

Epoch: 6| Step: 12
Training loss: 2.5601916313171387
Validation loss: 2.111519448218807

Epoch: 6| Step: 13
Training loss: 1.3862677812576294
Validation loss: 2.0752753673061246

Epoch: 228| Step: 0
Training loss: 2.581407308578491
Validation loss: 2.0584076694262925

Epoch: 6| Step: 1
Training loss: 2.4408950805664062
Validation loss: 2.040005276280065

Epoch: 6| Step: 2
Training loss: 2.084822416305542
Validation loss: 2.052802801132202

Epoch: 6| Step: 3
Training loss: 2.848970890045166
Validation loss: 2.052756327454762

Epoch: 6| Step: 4
Training loss: 1.9048924446105957
Validation loss: 2.062545285430006

Epoch: 6| Step: 5
Training loss: 2.287019968032837
Validation loss: 2.0646362766142814

Epoch: 6| Step: 6
Training loss: 1.9434866905212402
Validation loss: 2.0598089079703055

Epoch: 6| Step: 7
Training loss: 2.775078773498535
Validation loss: 2.043136114715248

Epoch: 6| Step: 8
Training loss: 2.1974711418151855
Validation loss: 2.0331083882239556

Epoch: 6| Step: 9
Training loss: 2.167961359024048
Validation loss: 2.02973751355243

Epoch: 6| Step: 10
Training loss: 1.8165770769119263
Validation loss: 2.0218236856563117

Epoch: 6| Step: 11
Training loss: 1.5879948139190674
Validation loss: 2.0381144285202026

Epoch: 6| Step: 12
Training loss: 2.7099008560180664
Validation loss: 2.056566551167478

Epoch: 6| Step: 13
Training loss: 2.6998720169067383
Validation loss: 2.0652550035907375

Epoch: 229| Step: 0
Training loss: 2.1383395195007324
Validation loss: 2.0823353157248548

Epoch: 6| Step: 1
Training loss: 1.7862104177474976
Validation loss: 2.089011225649106

Epoch: 6| Step: 2
Training loss: 2.2614455223083496
Validation loss: 2.1139551606229556

Epoch: 6| Step: 3
Training loss: 1.716862440109253
Validation loss: 2.1234745466580955

Epoch: 6| Step: 4
Training loss: 2.324702739715576
Validation loss: 2.1273886465257212

Epoch: 6| Step: 5
Training loss: 2.1352784633636475
Validation loss: 2.1502302282599994

Epoch: 6| Step: 6
Training loss: 2.8437981605529785
Validation loss: 2.1741885985097578

Epoch: 6| Step: 7
Training loss: 2.564919948577881
Validation loss: 2.213282505671183

Epoch: 6| Step: 8
Training loss: 2.345513343811035
Validation loss: 2.2086772739246325

Epoch: 6| Step: 9
Training loss: 1.7377336025238037
Validation loss: 2.188442926253042

Epoch: 6| Step: 10
Training loss: 2.9227170944213867
Validation loss: 2.166113761163527

Epoch: 6| Step: 11
Training loss: 2.331104040145874
Validation loss: 2.13673323200595

Epoch: 6| Step: 12
Training loss: 2.042477607727051
Validation loss: 2.1093690472264446

Epoch: 6| Step: 13
Training loss: 2.3386576175689697
Validation loss: 2.103809670735431

Epoch: 230| Step: 0
Training loss: 2.687256336212158
Validation loss: 2.106648455383957

Epoch: 6| Step: 1
Training loss: 2.017299175262451
Validation loss: 2.1004410225857972

Epoch: 6| Step: 2
Training loss: 2.406498432159424
Validation loss: 2.1156685890689975

Epoch: 6| Step: 3
Training loss: 2.419278144836426
Validation loss: 2.0996002484393377

Epoch: 6| Step: 4
Training loss: 2.4857144355773926
Validation loss: 2.1012799304018737

Epoch: 6| Step: 5
Training loss: 1.5177321434020996
Validation loss: 2.084267193271268

Epoch: 6| Step: 6
Training loss: 2.213038444519043
Validation loss: 2.097552627645513

Epoch: 6| Step: 7
Training loss: 2.0389420986175537
Validation loss: 2.085375890936903

Epoch: 6| Step: 8
Training loss: 2.1221232414245605
Validation loss: 2.0811586841460197

Epoch: 6| Step: 9
Training loss: 2.686412811279297
Validation loss: 2.0598587887261504

Epoch: 6| Step: 10
Training loss: 2.1000752449035645
Validation loss: 2.0604700939629668

Epoch: 6| Step: 11
Training loss: 1.9284288883209229
Validation loss: 2.071560662279847

Epoch: 6| Step: 12
Training loss: 2.4826765060424805
Validation loss: 2.0960049372847362

Epoch: 6| Step: 13
Training loss: 1.1383206844329834
Validation loss: 2.099175278858472

Epoch: 231| Step: 0
Training loss: 2.3031368255615234
Validation loss: 2.099107708982242

Epoch: 6| Step: 1
Training loss: 2.0355005264282227
Validation loss: 2.0917299973067416

Epoch: 6| Step: 2
Training loss: 1.8573583364486694
Validation loss: 2.0639920029588925

Epoch: 6| Step: 3
Training loss: 2.2418909072875977
Validation loss: 2.0493859398749565

Epoch: 6| Step: 4
Training loss: 2.414094924926758
Validation loss: 2.046765081344112

Epoch: 6| Step: 5
Training loss: 2.774948835372925
Validation loss: 2.0584676034988894

Epoch: 6| Step: 6
Training loss: 2.380875587463379
Validation loss: 2.0599159502214

Epoch: 6| Step: 7
Training loss: 2.0582165718078613
Validation loss: 2.057010322488764

Epoch: 6| Step: 8
Training loss: 2.7619788646698
Validation loss: 2.0658635016410583

Epoch: 6| Step: 9
Training loss: 1.8285777568817139
Validation loss: 2.0812302584289224

Epoch: 6| Step: 10
Training loss: 1.835416316986084
Validation loss: 2.0946074519106137

Epoch: 6| Step: 11
Training loss: 2.4378275871276855
Validation loss: 2.130889736196046

Epoch: 6| Step: 12
Training loss: 2.2354538440704346
Validation loss: 2.1213075371198755

Epoch: 6| Step: 13
Training loss: 1.621508240699768
Validation loss: 2.1019147544778805

Epoch: 232| Step: 0
Training loss: 2.421412467956543
Validation loss: 2.1001851327957644

Epoch: 6| Step: 1
Training loss: 1.7418303489685059
Validation loss: 2.094542252120151

Epoch: 6| Step: 2
Training loss: 2.4551992416381836
Validation loss: 2.0845833132343907

Epoch: 6| Step: 3
Training loss: 2.0942318439483643
Validation loss: 2.069246592060212

Epoch: 6| Step: 4
Training loss: 2.3743185997009277
Validation loss: 2.0867255451858684

Epoch: 6| Step: 5
Training loss: 1.7842662334442139
Validation loss: 2.0735310918541363

Epoch: 6| Step: 6
Training loss: 2.531752109527588
Validation loss: 2.0605840580437773

Epoch: 6| Step: 7
Training loss: 1.97119140625
Validation loss: 2.0569256531294955

Epoch: 6| Step: 8
Training loss: 2.302077054977417
Validation loss: 2.0678788103083128

Epoch: 6| Step: 9
Training loss: 2.106966257095337
Validation loss: 2.078686709045082

Epoch: 6| Step: 10
Training loss: 2.129734754562378
Validation loss: 2.087212908652521

Epoch: 6| Step: 11
Training loss: 1.9369820356369019
Validation loss: 2.0806084422655005

Epoch: 6| Step: 12
Training loss: 2.627810001373291
Validation loss: 2.087686432305203

Epoch: 6| Step: 13
Training loss: 2.12528657913208
Validation loss: 2.0947643479993268

Epoch: 233| Step: 0
Training loss: 2.043793201446533
Validation loss: 2.0985146043121174

Epoch: 6| Step: 1
Training loss: 2.1668291091918945
Validation loss: 2.084170146655011

Epoch: 6| Step: 2
Training loss: 2.971477508544922
Validation loss: 2.0844656267473773

Epoch: 6| Step: 3
Training loss: 2.1364927291870117
Validation loss: 2.090075618477278

Epoch: 6| Step: 4
Training loss: 2.2906224727630615
Validation loss: 2.0792176774753037

Epoch: 6| Step: 5
Training loss: 1.4983152151107788
Validation loss: 2.069871021855262

Epoch: 6| Step: 6
Training loss: 2.3047873973846436
Validation loss: 2.0610132871135587

Epoch: 6| Step: 7
Training loss: 2.359184741973877
Validation loss: 2.084335309202953

Epoch: 6| Step: 8
Training loss: 2.3313777446746826
Validation loss: 2.094273777418239

Epoch: 6| Step: 9
Training loss: 2.6755361557006836
Validation loss: 2.0942929124319427

Epoch: 6| Step: 10
Training loss: 1.7215794324874878
Validation loss: 2.0874686625696

Epoch: 6| Step: 11
Training loss: 2.37829327583313
Validation loss: 2.0830014597985054

Epoch: 6| Step: 12
Training loss: 1.5383293628692627
Validation loss: 2.0784406687623713

Epoch: 6| Step: 13
Training loss: 1.8170021772384644
Validation loss: 2.074368121803448

Epoch: 234| Step: 0
Training loss: 2.093388795852661
Validation loss: 2.082562792685724

Epoch: 6| Step: 1
Training loss: 2.563812255859375
Validation loss: 2.0825109404902302

Epoch: 6| Step: 2
Training loss: 1.67380690574646
Validation loss: 2.0880296691771476

Epoch: 6| Step: 3
Training loss: 2.095379114151001
Validation loss: 2.119754206749701

Epoch: 6| Step: 4
Training loss: 2.4865329265594482
Validation loss: 2.1398548951712986

Epoch: 6| Step: 5
Training loss: 2.7339491844177246
Validation loss: 2.1538385242544194

Epoch: 6| Step: 6
Training loss: 2.0746164321899414
Validation loss: 2.147127197634789

Epoch: 6| Step: 7
Training loss: 2.0634825229644775
Validation loss: 2.1138808881082842

Epoch: 6| Step: 8
Training loss: 2.2386474609375
Validation loss: 2.1060188431893625

Epoch: 6| Step: 9
Training loss: 2.268421173095703
Validation loss: 2.1034841742566837

Epoch: 6| Step: 10
Training loss: 2.2274811267852783
Validation loss: 2.087005969016783

Epoch: 6| Step: 11
Training loss: 2.3036322593688965
Validation loss: 2.076300779978434

Epoch: 6| Step: 12
Training loss: 1.3699121475219727
Validation loss: 2.0782360389668453

Epoch: 6| Step: 13
Training loss: 2.3497984409332275
Validation loss: 2.053040099400346

Epoch: 235| Step: 0
Training loss: 2.2985808849334717
Validation loss: 2.036011617670777

Epoch: 6| Step: 1
Training loss: 1.9178059101104736
Validation loss: 2.0219814315918954

Epoch: 6| Step: 2
Training loss: 2.0196738243103027
Validation loss: 2.0185507164206555

Epoch: 6| Step: 3
Training loss: 1.4604709148406982
Validation loss: 2.006237781176003

Epoch: 6| Step: 4
Training loss: 2.438650131225586
Validation loss: 1.994750907344203

Epoch: 6| Step: 5
Training loss: 1.9854283332824707
Validation loss: 1.993042786916097

Epoch: 6| Step: 6
Training loss: 2.085172176361084
Validation loss: 2.0140897971327587

Epoch: 6| Step: 7
Training loss: 2.458751678466797
Validation loss: 2.0083519181897564

Epoch: 6| Step: 8
Training loss: 2.3431715965270996
Validation loss: 2.012279215679374

Epoch: 6| Step: 9
Training loss: 3.088500499725342
Validation loss: 2.013426447427401

Epoch: 6| Step: 10
Training loss: 1.7639877796173096
Validation loss: 2.009829649361231

Epoch: 6| Step: 11
Training loss: 2.588898181915283
Validation loss: 2.0159428363205283

Epoch: 6| Step: 12
Training loss: 1.7119832038879395
Validation loss: 2.0235786604624924

Epoch: 6| Step: 13
Training loss: 3.0942018032073975
Validation loss: 2.0505397422339326

Epoch: 236| Step: 0
Training loss: 2.4781055450439453
Validation loss: 2.0864015189550256

Epoch: 6| Step: 1
Training loss: 2.5585100650787354
Validation loss: 2.112322222801947

Epoch: 6| Step: 2
Training loss: 2.2241995334625244
Validation loss: 2.142140457707067

Epoch: 6| Step: 3
Training loss: 2.1665234565734863
Validation loss: 2.1231069539182927

Epoch: 6| Step: 4
Training loss: 2.238731861114502
Validation loss: 2.0964371517140377

Epoch: 6| Step: 5
Training loss: 2.607563018798828
Validation loss: 2.0579524988769204

Epoch: 6| Step: 6
Training loss: 2.017155170440674
Validation loss: 2.0419592844542636

Epoch: 6| Step: 7
Training loss: 2.4405620098114014
Validation loss: 2.0385978837167062

Epoch: 6| Step: 8
Training loss: 2.0036520957946777
Validation loss: 2.0323822729049192

Epoch: 6| Step: 9
Training loss: 1.7594873905181885
Validation loss: 2.0349339490295737

Epoch: 6| Step: 10
Training loss: 1.29434072971344
Validation loss: 2.039754875244633

Epoch: 6| Step: 11
Training loss: 2.2564921379089355
Validation loss: 2.0476137156127603

Epoch: 6| Step: 12
Training loss: 2.4307773113250732
Validation loss: 2.044318734958608

Epoch: 6| Step: 13
Training loss: 1.9202204942703247
Validation loss: 2.0613715905015186

Epoch: 237| Step: 0
Training loss: 2.0739710330963135
Validation loss: 2.073469646515385

Epoch: 6| Step: 1
Training loss: 2.0847344398498535
Validation loss: 2.0676340031367477

Epoch: 6| Step: 2
Training loss: 3.0777969360351562
Validation loss: 2.0677490900921565

Epoch: 6| Step: 3
Training loss: 2.656745433807373
Validation loss: 2.066757148311984

Epoch: 6| Step: 4
Training loss: 2.4402003288269043
Validation loss: 2.06800018331056

Epoch: 6| Step: 5
Training loss: 2.2332310676574707
Validation loss: 2.0939781652983798

Epoch: 6| Step: 6
Training loss: 1.606593132019043
Validation loss: 2.088361975967243

Epoch: 6| Step: 7
Training loss: 1.7244776487350464
Validation loss: 2.0789310547613327

Epoch: 6| Step: 8
Training loss: 2.1149895191192627
Validation loss: 2.0976945584820164

Epoch: 6| Step: 9
Training loss: 2.1256356239318848
Validation loss: 2.0952555210359636

Epoch: 6| Step: 10
Training loss: 1.2426286935806274
Validation loss: 2.088082023846206

Epoch: 6| Step: 11
Training loss: 2.688920497894287
Validation loss: 2.0905082507800032

Epoch: 6| Step: 12
Training loss: 2.3755483627319336
Validation loss: 2.0891485368051836

Epoch: 6| Step: 13
Training loss: 1.4016908407211304
Validation loss: 2.1036640649200766

Epoch: 238| Step: 0
Training loss: 2.3497378826141357
Validation loss: 2.116118118327151

Epoch: 6| Step: 1
Training loss: 1.296464204788208
Validation loss: 2.1258484317410375

Epoch: 6| Step: 2
Training loss: 1.5132150650024414
Validation loss: 2.131219810055148

Epoch: 6| Step: 3
Training loss: 2.144730806350708
Validation loss: 2.160558872325446

Epoch: 6| Step: 4
Training loss: 1.8522608280181885
Validation loss: 2.1710795753745624

Epoch: 6| Step: 5
Training loss: 2.2670328617095947
Validation loss: 2.1980454383357877

Epoch: 6| Step: 6
Training loss: 1.997559905052185
Validation loss: 2.1853696043773363

Epoch: 6| Step: 7
Training loss: 2.7636237144470215
Validation loss: 2.140459005550672

Epoch: 6| Step: 8
Training loss: 2.161458730697632
Validation loss: 2.113485490122149

Epoch: 6| Step: 9
Training loss: 2.046408176422119
Validation loss: 2.069555885048323

Epoch: 6| Step: 10
Training loss: 2.230686664581299
Validation loss: 2.0672600166772

Epoch: 6| Step: 11
Training loss: 2.3047192096710205
Validation loss: 2.0569192722279537

Epoch: 6| Step: 12
Training loss: 2.6529574394226074
Validation loss: 2.035875512707618

Epoch: 6| Step: 13
Training loss: 2.963779926300049
Validation loss: 2.0336432905607325

Epoch: 239| Step: 0
Training loss: 2.115283966064453
Validation loss: 2.029469359305597

Epoch: 6| Step: 1
Training loss: 2.099639415740967
Validation loss: 2.0381479301760272

Epoch: 6| Step: 2
Training loss: 1.5340286493301392
Validation loss: 2.0205927997507076

Epoch: 6| Step: 3
Training loss: 2.317869186401367
Validation loss: 2.0322518771694553

Epoch: 6| Step: 4
Training loss: 2.8090403079986572
Validation loss: 2.030693725873065

Epoch: 6| Step: 5
Training loss: 2.1973278522491455
Validation loss: 2.0329582588647

Epoch: 6| Step: 6
Training loss: 2.2036359310150146
Validation loss: 2.0451792850289294

Epoch: 6| Step: 7
Training loss: 2.3218727111816406
Validation loss: 2.0381744497565815

Epoch: 6| Step: 8
Training loss: 1.4497383832931519
Validation loss: 2.0402370191389516

Epoch: 6| Step: 9
Training loss: 2.3981587886810303
Validation loss: 2.0555322093348347

Epoch: 6| Step: 10
Training loss: 2.6013946533203125
Validation loss: 2.0269705326326433

Epoch: 6| Step: 11
Training loss: 2.025646209716797
Validation loss: 2.044089783904373

Epoch: 6| Step: 12
Training loss: 2.6109399795532227
Validation loss: 2.0759911729443457

Epoch: 6| Step: 13
Training loss: 2.472177743911743
Validation loss: 2.0890898896801855

Epoch: 240| Step: 0
Training loss: 2.35402774810791
Validation loss: 2.1148273303944576

Epoch: 6| Step: 1
Training loss: 1.864630937576294
Validation loss: 2.1252960889570174

Epoch: 6| Step: 2
Training loss: 2.577099323272705
Validation loss: 2.1628467703378327

Epoch: 6| Step: 3
Training loss: 1.7259407043457031
Validation loss: 2.1794325254296743

Epoch: 6| Step: 4
Training loss: 2.020559787750244
Validation loss: 2.1625945632175734

Epoch: 6| Step: 5
Training loss: 1.7730494737625122
Validation loss: 2.183564603969615

Epoch: 6| Step: 6
Training loss: 2.4308576583862305
Validation loss: 2.1656950045657415

Epoch: 6| Step: 7
Training loss: 1.8693528175354004
Validation loss: 2.1683774327719085

Epoch: 6| Step: 8
Training loss: 2.2237861156463623
Validation loss: 2.165727415392476

Epoch: 6| Step: 9
Training loss: 2.8428473472595215
Validation loss: 2.142885378611985

Epoch: 6| Step: 10
Training loss: 1.6435019969940186
Validation loss: 2.161106251901196

Epoch: 6| Step: 11
Training loss: 2.664645195007324
Validation loss: 2.1504038328765542

Epoch: 6| Step: 12
Training loss: 2.1343884468078613
Validation loss: 2.1311674938406995

Epoch: 6| Step: 13
Training loss: 1.944347620010376
Validation loss: 2.092290339931365

Epoch: 241| Step: 0
Training loss: 1.8077466487884521
Validation loss: 2.0652267638073174

Epoch: 6| Step: 1
Training loss: 2.8448081016540527
Validation loss: 2.0654729553448257

Epoch: 6| Step: 2
Training loss: 2.0235633850097656
Validation loss: 2.047193096530053

Epoch: 6| Step: 3
Training loss: 2.0819149017333984
Validation loss: 2.0578570647906234

Epoch: 6| Step: 4
Training loss: 1.861863136291504
Validation loss: 2.0459540095380557

Epoch: 6| Step: 5
Training loss: 2.054060459136963
Validation loss: 2.059994759098176

Epoch: 6| Step: 6
Training loss: 1.9444950819015503
Validation loss: 2.055740564100204

Epoch: 6| Step: 7
Training loss: 2.160548686981201
Validation loss: 2.075623949368795

Epoch: 6| Step: 8
Training loss: 1.836854338645935
Validation loss: 2.051983764094691

Epoch: 6| Step: 9
Training loss: 2.0049564838409424
Validation loss: 2.061590072929218

Epoch: 6| Step: 10
Training loss: 2.0082149505615234
Validation loss: 2.069057035189803

Epoch: 6| Step: 11
Training loss: 2.743166923522949
Validation loss: 2.0896784438881824

Epoch: 6| Step: 12
Training loss: 2.5785770416259766
Validation loss: 2.077460842747842

Epoch: 6| Step: 13
Training loss: 2.1109728813171387
Validation loss: 2.064113747689032

Epoch: 242| Step: 0
Training loss: 1.846787691116333
Validation loss: 2.0570889826743834

Epoch: 6| Step: 1
Training loss: 2.1214756965637207
Validation loss: 2.0870014852093113

Epoch: 6| Step: 2
Training loss: 2.699397087097168
Validation loss: 2.0993724587143108

Epoch: 6| Step: 3
Training loss: 2.622715950012207
Validation loss: 2.1170271237691245

Epoch: 6| Step: 4
Training loss: 2.238128900527954
Validation loss: 2.0899991745589883

Epoch: 6| Step: 5
Training loss: 2.7608461380004883
Validation loss: 2.0937238752201037

Epoch: 6| Step: 6
Training loss: 1.6653602123260498
Validation loss: 2.104693740926763

Epoch: 6| Step: 7
Training loss: 1.4880465269088745
Validation loss: 2.0856916878813054

Epoch: 6| Step: 8
Training loss: 2.348644256591797
Validation loss: 2.0728366785151984

Epoch: 6| Step: 9
Training loss: 2.1500861644744873
Validation loss: 2.0871235888491393

Epoch: 6| Step: 10
Training loss: 1.7169523239135742
Validation loss: 2.0849515686752977

Epoch: 6| Step: 11
Training loss: 1.5567021369934082
Validation loss: 2.0875961498547624

Epoch: 6| Step: 12
Training loss: 2.403496503829956
Validation loss: 2.090802502888505

Epoch: 6| Step: 13
Training loss: 1.9703831672668457
Validation loss: 2.0824306203472998

Epoch: 243| Step: 0
Training loss: 2.1100339889526367
Validation loss: 2.08368742850519

Epoch: 6| Step: 1
Training loss: 2.6302146911621094
Validation loss: 2.0638469906263452

Epoch: 6| Step: 2
Training loss: 1.48494291305542
Validation loss: 2.0663299509274062

Epoch: 6| Step: 3
Training loss: 1.7706565856933594
Validation loss: 2.066987047913254

Epoch: 6| Step: 4
Training loss: 1.8807984590530396
Validation loss: 2.0935820123200775

Epoch: 6| Step: 5
Training loss: 2.6415300369262695
Validation loss: 2.0879183123188634

Epoch: 6| Step: 6
Training loss: 2.2443056106567383
Validation loss: 2.1001346547116517

Epoch: 6| Step: 7
Training loss: 2.4860610961914062
Validation loss: 2.1013889184562107

Epoch: 6| Step: 8
Training loss: 2.34714412689209
Validation loss: 2.0732289616779616

Epoch: 6| Step: 9
Training loss: 2.6585886478424072
Validation loss: 2.0757192001547864

Epoch: 6| Step: 10
Training loss: 2.0612053871154785
Validation loss: 2.0714478492736816

Epoch: 6| Step: 11
Training loss: 1.4768824577331543
Validation loss: 2.0634706302355696

Epoch: 6| Step: 12
Training loss: 1.5845787525177002
Validation loss: 2.07072542559716

Epoch: 6| Step: 13
Training loss: 2.1458675861358643
Validation loss: 2.076787858880976

Epoch: 244| Step: 0
Training loss: 2.5088319778442383
Validation loss: 2.077739856576407

Epoch: 6| Step: 1
Training loss: 2.191678047180176
Validation loss: 2.0936829108063892

Epoch: 6| Step: 2
Training loss: 2.363893985748291
Validation loss: 2.0975783025064776

Epoch: 6| Step: 3
Training loss: 2.236577033996582
Validation loss: 2.132331827635406

Epoch: 6| Step: 4
Training loss: 2.351114273071289
Validation loss: 2.142269261421696

Epoch: 6| Step: 5
Training loss: 1.9539103507995605
Validation loss: 2.15232253074646

Epoch: 6| Step: 6
Training loss: 1.5907979011535645
Validation loss: 2.101375000451201

Epoch: 6| Step: 7
Training loss: 1.8641870021820068
Validation loss: 2.0647474540177213

Epoch: 6| Step: 8
Training loss: 2.297435998916626
Validation loss: 2.052254689637051

Epoch: 6| Step: 9
Training loss: 1.8454582691192627
Validation loss: 2.025860701837847

Epoch: 6| Step: 10
Training loss: 2.7777488231658936
Validation loss: 2.040876720541267

Epoch: 6| Step: 11
Training loss: 1.550154209136963
Validation loss: 2.0579987930995163

Epoch: 6| Step: 12
Training loss: 1.7980835437774658
Validation loss: 2.051108326963199

Epoch: 6| Step: 13
Training loss: 3.1090478897094727
Validation loss: 2.0607268374453307

Epoch: 245| Step: 0
Training loss: 2.230353832244873
Validation loss: 2.05388464338036

Epoch: 6| Step: 1
Training loss: 1.8490502834320068
Validation loss: 2.0714723512690556

Epoch: 6| Step: 2
Training loss: 1.8717063665390015
Validation loss: 2.076546140896377

Epoch: 6| Step: 3
Training loss: 2.406003475189209
Validation loss: 2.1030978413038355

Epoch: 6| Step: 4
Training loss: 2.130188465118408
Validation loss: 2.1154435552576536

Epoch: 6| Step: 5
Training loss: 2.367931365966797
Validation loss: 2.135663132513723

Epoch: 6| Step: 6
Training loss: 1.983518362045288
Validation loss: 2.130802674960065

Epoch: 6| Step: 7
Training loss: 1.687133550643921
Validation loss: 2.1282417671654814

Epoch: 6| Step: 8
Training loss: 2.107926368713379
Validation loss: 2.108151364070113

Epoch: 6| Step: 9
Training loss: 2.2965199947357178
Validation loss: 2.093573039577853

Epoch: 6| Step: 10
Training loss: 1.8591361045837402
Validation loss: 2.081843918369662

Epoch: 6| Step: 11
Training loss: 2.426173686981201
Validation loss: 2.05892244462044

Epoch: 6| Step: 12
Training loss: 2.0290772914886475
Validation loss: 2.070363434412146

Epoch: 6| Step: 13
Training loss: 2.3268704414367676
Validation loss: 2.069707106518489

Epoch: 246| Step: 0
Training loss: 1.605933427810669
Validation loss: 2.0633032219384306

Epoch: 6| Step: 1
Training loss: 1.7148802280426025
Validation loss: 2.087072169908913

Epoch: 6| Step: 2
Training loss: 2.232844829559326
Validation loss: 2.091976950245519

Epoch: 6| Step: 3
Training loss: 1.6123658418655396
Validation loss: 2.1147568507861068

Epoch: 6| Step: 4
Training loss: 2.3304460048675537
Validation loss: 2.1524552529858005

Epoch: 6| Step: 5
Training loss: 1.6165647506713867
Validation loss: 2.160068088962186

Epoch: 6| Step: 6
Training loss: 2.4178214073181152
Validation loss: 2.181176665008709

Epoch: 6| Step: 7
Training loss: 2.340926170349121
Validation loss: 2.1542945856689126

Epoch: 6| Step: 8
Training loss: 2.3872475624084473
Validation loss: 2.119101714062434

Epoch: 6| Step: 9
Training loss: 2.093763828277588
Validation loss: 2.079933008840007

Epoch: 6| Step: 10
Training loss: 1.6018168926239014
Validation loss: 2.056349762024418

Epoch: 6| Step: 11
Training loss: 2.3887245655059814
Validation loss: 2.0312901825033207

Epoch: 6| Step: 12
Training loss: 3.028522253036499
Validation loss: 2.025867413449031

Epoch: 6| Step: 13
Training loss: 2.507739543914795
Validation loss: 2.025875045407203

Epoch: 247| Step: 0
Training loss: 2.4480185508728027
Validation loss: 2.00418960150852

Epoch: 6| Step: 1
Training loss: 2.064056396484375
Validation loss: 2.012067482035647

Epoch: 6| Step: 2
Training loss: 2.0129075050354004
Validation loss: 2.009217336613645

Epoch: 6| Step: 3
Training loss: 2.741919994354248
Validation loss: 2.0179338839746292

Epoch: 6| Step: 4
Training loss: 2.4415130615234375
Validation loss: 2.007826760251035

Epoch: 6| Step: 5
Training loss: 1.5191271305084229
Validation loss: 2.017317332247252

Epoch: 6| Step: 6
Training loss: 2.6141104698181152
Validation loss: 2.0365638809819377

Epoch: 6| Step: 7
Training loss: 2.4178824424743652
Validation loss: 2.038392084901051

Epoch: 6| Step: 8
Training loss: 2.1760523319244385
Validation loss: 2.0317066766882457

Epoch: 6| Step: 9
Training loss: 1.7252304553985596
Validation loss: 2.0489011836308304

Epoch: 6| Step: 10
Training loss: 1.578797698020935
Validation loss: 2.0536533209585373

Epoch: 6| Step: 11
Training loss: 1.5089492797851562
Validation loss: 2.080095297546797

Epoch: 6| Step: 12
Training loss: 2.5295252799987793
Validation loss: 2.099114671830208

Epoch: 6| Step: 13
Training loss: 2.00203800201416
Validation loss: 2.128805414322884

Epoch: 248| Step: 0
Training loss: 2.0905849933624268
Validation loss: 2.1450930667179886

Epoch: 6| Step: 1
Training loss: 3.0340261459350586
Validation loss: 2.1677753028049263

Epoch: 6| Step: 2
Training loss: 2.2684569358825684
Validation loss: 2.143188671399188

Epoch: 6| Step: 3
Training loss: 1.8661024570465088
Validation loss: 2.1242404983889673

Epoch: 6| Step: 4
Training loss: 1.7250027656555176
Validation loss: 2.082689536515103

Epoch: 6| Step: 5
Training loss: 1.8116497993469238
Validation loss: 2.091081360334991

Epoch: 6| Step: 6
Training loss: 2.2157418727874756
Validation loss: 2.082145174344381

Epoch: 6| Step: 7
Training loss: 2.144580125808716
Validation loss: 2.0698191747870496

Epoch: 6| Step: 8
Training loss: 1.6726807355880737
Validation loss: 2.0631044077616867

Epoch: 6| Step: 9
Training loss: 1.4097392559051514
Validation loss: 2.0585778400462162

Epoch: 6| Step: 10
Training loss: 2.140968084335327
Validation loss: 2.0563765866782076

Epoch: 6| Step: 11
Training loss: 2.4587419033050537
Validation loss: 2.064480250881564

Epoch: 6| Step: 12
Training loss: 2.2308764457702637
Validation loss: 2.092741617592432

Epoch: 6| Step: 13
Training loss: 2.8789756298065186
Validation loss: 2.1158459263463176

Epoch: 249| Step: 0
Training loss: 2.254772663116455
Validation loss: 2.0903619540634977

Epoch: 6| Step: 1
Training loss: 2.404280185699463
Validation loss: 2.0903483539499264

Epoch: 6| Step: 2
Training loss: 1.7598145008087158
Validation loss: 2.08463930314587

Epoch: 6| Step: 3
Training loss: 1.748241662979126
Validation loss: 2.0740544372989285

Epoch: 6| Step: 4
Training loss: 2.4368550777435303
Validation loss: 2.081977303310107

Epoch: 6| Step: 5
Training loss: 1.7344510555267334
Validation loss: 2.0829231431407313

Epoch: 6| Step: 6
Training loss: 2.423959732055664
Validation loss: 2.0876988069985503

Epoch: 6| Step: 7
Training loss: 2.5519590377807617
Validation loss: 2.071856188517745

Epoch: 6| Step: 8
Training loss: 2.2394728660583496
Validation loss: 2.087602948629728

Epoch: 6| Step: 9
Training loss: 1.4288119077682495
Validation loss: 2.0808961916995306

Epoch: 6| Step: 10
Training loss: 1.7088481187820435
Validation loss: 2.0863458930805163

Epoch: 6| Step: 11
Training loss: 2.581815242767334
Validation loss: 2.1065487797542284

Epoch: 6| Step: 12
Training loss: 1.5766465663909912
Validation loss: 2.1522617519542737

Epoch: 6| Step: 13
Training loss: 2.2902133464813232
Validation loss: 2.1645217121288343

Epoch: 250| Step: 0
Training loss: 2.466416120529175
Validation loss: 2.1618503703865954

Epoch: 6| Step: 1
Training loss: 1.9793809652328491
Validation loss: 2.143231471379598

Epoch: 6| Step: 2
Training loss: 1.7027000188827515
Validation loss: 2.1281570849880094

Epoch: 6| Step: 3
Training loss: 2.29024338722229
Validation loss: 2.123772777536864

Epoch: 6| Step: 4
Training loss: 2.2247400283813477
Validation loss: 2.13360523152095

Epoch: 6| Step: 5
Training loss: 2.7770400047302246
Validation loss: 2.1433443792404665

Epoch: 6| Step: 6
Training loss: 2.122835636138916
Validation loss: 2.1568201459864134

Epoch: 6| Step: 7
Training loss: 1.6789960861206055
Validation loss: 2.148256350589055

Epoch: 6| Step: 8
Training loss: 2.0445609092712402
Validation loss: 2.124873993217304

Epoch: 6| Step: 9
Training loss: 2.353790760040283
Validation loss: 2.123762761392901

Epoch: 6| Step: 10
Training loss: 1.6221531629562378
Validation loss: 2.119553058378158

Epoch: 6| Step: 11
Training loss: 2.436805248260498
Validation loss: 2.103247525871441

Epoch: 6| Step: 12
Training loss: 2.1890146732330322
Validation loss: 2.082532790399367

Epoch: 6| Step: 13
Training loss: 2.555401563644409
Validation loss: 2.0574071676500383

Epoch: 251| Step: 0
Training loss: 2.4733994007110596
Validation loss: 2.056981819932179

Epoch: 6| Step: 1
Training loss: 2.3258016109466553
Validation loss: 2.0490059186053533

Epoch: 6| Step: 2
Training loss: 2.0302205085754395
Validation loss: 2.0654023385817006

Epoch: 6| Step: 3
Training loss: 2.2356503009796143
Validation loss: 2.0937504845280803

Epoch: 6| Step: 4
Training loss: 1.5736223459243774
Validation loss: 2.083648422712921

Epoch: 6| Step: 5
Training loss: 2.3482038974761963
Validation loss: 2.063357203237472

Epoch: 6| Step: 6
Training loss: 2.1695847511291504
Validation loss: 2.061533599771479

Epoch: 6| Step: 7
Training loss: 2.435591459274292
Validation loss: 2.052148765133273

Epoch: 6| Step: 8
Training loss: 1.6914211511611938
Validation loss: 2.040910666988742

Epoch: 6| Step: 9
Training loss: 2.110626459121704
Validation loss: 2.0287463459917294

Epoch: 6| Step: 10
Training loss: 1.9735052585601807
Validation loss: 2.0425104761636383

Epoch: 6| Step: 11
Training loss: 1.7159974575042725
Validation loss: 2.0763959653915895

Epoch: 6| Step: 12
Training loss: 1.8730926513671875
Validation loss: 2.0961180656186995

Epoch: 6| Step: 13
Training loss: 2.0407209396362305
Validation loss: 2.135854108359224

Epoch: 252| Step: 0
Training loss: 1.678829312324524
Validation loss: 2.211557395996586

Epoch: 6| Step: 1
Training loss: 2.418534755706787
Validation loss: 2.2883632311256985

Epoch: 6| Step: 2
Training loss: 1.4758106470108032
Validation loss: 2.271894170391944

Epoch: 6| Step: 3
Training loss: 1.634310007095337
Validation loss: 2.2631287882404942

Epoch: 6| Step: 4
Training loss: 2.3364462852478027
Validation loss: 2.2295974095662436

Epoch: 6| Step: 5
Training loss: 2.26450777053833
Validation loss: 2.1951556410840762

Epoch: 6| Step: 6
Training loss: 2.1551458835601807
Validation loss: 2.1579626862720778

Epoch: 6| Step: 7
Training loss: 2.6166248321533203
Validation loss: 2.100515252800398

Epoch: 6| Step: 8
Training loss: 2.6711244583129883
Validation loss: 2.0815570854371592

Epoch: 6| Step: 9
Training loss: 2.1005451679229736
Validation loss: 2.0544082246800905

Epoch: 6| Step: 10
Training loss: 1.6324223279953003
Validation loss: 2.0514825467140443

Epoch: 6| Step: 11
Training loss: 1.772956371307373
Validation loss: 2.0466705842684676

Epoch: 6| Step: 12
Training loss: 2.6850242614746094
Validation loss: 2.052472545254615

Epoch: 6| Step: 13
Training loss: 1.4674593210220337
Validation loss: 2.0566851913288073

Epoch: 253| Step: 0
Training loss: 2.04303240776062
Validation loss: 2.0532493437490156

Epoch: 6| Step: 1
Training loss: 1.6177207231521606
Validation loss: 2.0535999292968423

Epoch: 6| Step: 2
Training loss: 1.8493684530258179
Validation loss: 2.070501467233063

Epoch: 6| Step: 3
Training loss: 1.6464877128601074
Validation loss: 2.075326547827772

Epoch: 6| Step: 4
Training loss: 2.698086738586426
Validation loss: 2.0871444286838656

Epoch: 6| Step: 5
Training loss: 2.5441036224365234
Validation loss: 2.0867252272944294

Epoch: 6| Step: 6
Training loss: 1.6716878414154053
Validation loss: 2.096432776861293

Epoch: 6| Step: 7
Training loss: 1.778503656387329
Validation loss: 2.1037299530480498

Epoch: 6| Step: 8
Training loss: 2.4146695137023926
Validation loss: 2.1263992363406765

Epoch: 6| Step: 9
Training loss: 2.5373833179473877
Validation loss: 2.1409058929771505

Epoch: 6| Step: 10
Training loss: 1.7751020193099976
Validation loss: 2.1635788320213236

Epoch: 6| Step: 11
Training loss: 2.2064385414123535
Validation loss: 2.19283950713373

Epoch: 6| Step: 12
Training loss: 1.4655479192733765
Validation loss: 2.173009728872648

Epoch: 6| Step: 13
Training loss: 2.784487009048462
Validation loss: 2.1899988061638287

Epoch: 254| Step: 0
Training loss: 2.1475777626037598
Validation loss: 2.1826743669407342

Epoch: 6| Step: 1
Training loss: 2.0590624809265137
Validation loss: 2.1423991854472826

Epoch: 6| Step: 2
Training loss: 1.8403184413909912
Validation loss: 2.140177693418277

Epoch: 6| Step: 3
Training loss: 1.8326616287231445
Validation loss: 2.106827228300033

Epoch: 6| Step: 4
Training loss: 1.828627586364746
Validation loss: 2.0869302211269254

Epoch: 6| Step: 5
Training loss: 2.6195180416107178
Validation loss: 2.0711821740673435

Epoch: 6| Step: 6
Training loss: 1.5148367881774902
Validation loss: 2.08119466996962

Epoch: 6| Step: 7
Training loss: 2.2194275856018066
Validation loss: 2.0923037631537325

Epoch: 6| Step: 8
Training loss: 2.3805418014526367
Validation loss: 2.0942345242346487

Epoch: 6| Step: 9
Training loss: 1.6838431358337402
Validation loss: 2.09328854340379

Epoch: 6| Step: 10
Training loss: 1.4938971996307373
Validation loss: 2.08865330296178

Epoch: 6| Step: 11
Training loss: 2.8791661262512207
Validation loss: 2.0730869103503484

Epoch: 6| Step: 12
Training loss: 1.6472629308700562
Validation loss: 2.0774792548148864

Epoch: 6| Step: 13
Training loss: 2.663280963897705
Validation loss: 2.0833314900757163

Epoch: 255| Step: 0
Training loss: 1.957852840423584
Validation loss: 2.0807538981078775

Epoch: 6| Step: 1
Training loss: 2.8375253677368164
Validation loss: 2.078950896058031

Epoch: 6| Step: 2
Training loss: 2.04978084564209
Validation loss: 2.087725120206033

Epoch: 6| Step: 3
Training loss: 1.7980223894119263
Validation loss: 2.092666167084889

Epoch: 6| Step: 4
Training loss: 2.1984119415283203
Validation loss: 2.0997033516565957

Epoch: 6| Step: 5
Training loss: 1.8669712543487549
Validation loss: 2.1154789540075485

Epoch: 6| Step: 6
Training loss: 2.663616180419922
Validation loss: 2.1285311432294947

Epoch: 6| Step: 7
Training loss: 1.5854451656341553
Validation loss: 2.1399675543590257

Epoch: 6| Step: 8
Training loss: 2.279694080352783
Validation loss: 2.1193451855772283

Epoch: 6| Step: 9
Training loss: 2.21166729927063
Validation loss: 2.121460832575316

Epoch: 6| Step: 10
Training loss: 1.3647105693817139
Validation loss: 2.0909925481324554

Epoch: 6| Step: 11
Training loss: 1.7065165042877197
Validation loss: 2.0785612983088337

Epoch: 6| Step: 12
Training loss: 2.2929506301879883
Validation loss: 2.058762432426535

Epoch: 6| Step: 13
Training loss: 1.6980133056640625
Validation loss: 2.0503467359850482

Epoch: 256| Step: 0
Training loss: 2.332900285720825
Validation loss: 2.049507192386094

Epoch: 6| Step: 1
Training loss: 2.197908639907837
Validation loss: 2.0394797427679903

Epoch: 6| Step: 2
Training loss: 2.668476104736328
Validation loss: 2.0257028507929977

Epoch: 6| Step: 3
Training loss: 2.0195560455322266
Validation loss: 2.031753872030525

Epoch: 6| Step: 4
Training loss: 2.544261932373047
Validation loss: 2.0359396934509277

Epoch: 6| Step: 5
Training loss: 2.2547495365142822
Validation loss: 2.0251149195496754

Epoch: 6| Step: 6
Training loss: 1.3436739444732666
Validation loss: 2.04715048625905

Epoch: 6| Step: 7
Training loss: 2.5466837882995605
Validation loss: 2.0579320384610083

Epoch: 6| Step: 8
Training loss: 1.648209571838379
Validation loss: 2.06435158688535

Epoch: 6| Step: 9
Training loss: 2.0179226398468018
Validation loss: 2.0656238230325843

Epoch: 6| Step: 10
Training loss: 1.5661132335662842
Validation loss: 2.0807148102791078

Epoch: 6| Step: 11
Training loss: 2.1362528800964355
Validation loss: 2.102391637781615

Epoch: 6| Step: 12
Training loss: 1.8715262413024902
Validation loss: 2.1373221361508934

Epoch: 6| Step: 13
Training loss: 1.1920239925384521
Validation loss: 2.1668763724706506

Epoch: 257| Step: 0
Training loss: 1.953012466430664
Validation loss: 2.1916658903962825

Epoch: 6| Step: 1
Training loss: 1.0092766284942627
Validation loss: 2.204314216490715

Epoch: 6| Step: 2
Training loss: 1.8265947103500366
Validation loss: 2.2520620015359696

Epoch: 6| Step: 3
Training loss: 2.557786464691162
Validation loss: 2.271657415615615

Epoch: 6| Step: 4
Training loss: 2.024130344390869
Validation loss: 2.2674972831562

Epoch: 6| Step: 5
Training loss: 2.196627140045166
Validation loss: 2.2336324709717945

Epoch: 6| Step: 6
Training loss: 2.5337014198303223
Validation loss: 2.1860033747970418

Epoch: 6| Step: 7
Training loss: 1.8791695833206177
Validation loss: 2.133430991121518

Epoch: 6| Step: 8
Training loss: 2.303689956665039
Validation loss: 2.1061587513134046

Epoch: 6| Step: 9
Training loss: 1.1272096633911133
Validation loss: 2.0892261946073143

Epoch: 6| Step: 10
Training loss: 1.8783611059188843
Validation loss: 2.0831475975692912

Epoch: 6| Step: 11
Training loss: 2.879329204559326
Validation loss: 2.0655067479738625

Epoch: 6| Step: 12
Training loss: 2.6956710815429688
Validation loss: 2.0689605307835404

Epoch: 6| Step: 13
Training loss: 1.8665106296539307
Validation loss: 2.0725557009379068

Epoch: 258| Step: 0
Training loss: 2.380831241607666
Validation loss: 2.058391101898686

Epoch: 6| Step: 1
Training loss: 1.2844370603561401
Validation loss: 2.0655734385213544

Epoch: 6| Step: 2
Training loss: 2.812786102294922
Validation loss: 2.0766913096110025

Epoch: 6| Step: 3
Training loss: 1.4660429954528809
Validation loss: 2.0879606149529897

Epoch: 6| Step: 4
Training loss: 2.5030667781829834
Validation loss: 2.0815860379126763

Epoch: 6| Step: 5
Training loss: 1.7198736667633057
Validation loss: 2.088105486285302

Epoch: 6| Step: 6
Training loss: 2.28122615814209
Validation loss: 2.088055569638488

Epoch: 6| Step: 7
Training loss: 1.4024083614349365
Validation loss: 2.118661803583945

Epoch: 6| Step: 8
Training loss: 2.020148992538452
Validation loss: 2.113892073272377

Epoch: 6| Step: 9
Training loss: 2.42293119430542
Validation loss: 2.122603472842965

Epoch: 6| Step: 10
Training loss: 2.1937625408172607
Validation loss: 2.136927625184418

Epoch: 6| Step: 11
Training loss: 1.9994745254516602
Validation loss: 2.123170124587192

Epoch: 6| Step: 12
Training loss: 1.7070550918579102
Validation loss: 2.108828406180105

Epoch: 6| Step: 13
Training loss: 2.238527536392212
Validation loss: 2.0898932308279057

Epoch: 259| Step: 0
Training loss: 1.538179874420166
Validation loss: 2.113528193966035

Epoch: 6| Step: 1
Training loss: 2.1158275604248047
Validation loss: 2.114682871808288

Epoch: 6| Step: 2
Training loss: 2.014193534851074
Validation loss: 2.107666448880267

Epoch: 6| Step: 3
Training loss: 1.953898549079895
Validation loss: 2.1059935682563373

Epoch: 6| Step: 4
Training loss: 2.7925193309783936
Validation loss: 2.122762116052771

Epoch: 6| Step: 5
Training loss: 2.0394301414489746
Validation loss: 2.072694763060539

Epoch: 6| Step: 6
Training loss: 2.566077709197998
Validation loss: 2.1138393616163604

Epoch: 6| Step: 7
Training loss: 1.4980418682098389
Validation loss: 2.0978483128291305

Epoch: 6| Step: 8
Training loss: 1.8399465084075928
Validation loss: 2.0772151511202575

Epoch: 6| Step: 9
Training loss: 2.5251355171203613
Validation loss: 2.075403557028822

Epoch: 6| Step: 10
Training loss: 1.2967849969863892
Validation loss: 2.076869997926938

Epoch: 6| Step: 11
Training loss: 1.7558622360229492
Validation loss: 2.074453297481742

Epoch: 6| Step: 12
Training loss: 2.050694704055786
Validation loss: 2.091318239447891

Epoch: 6| Step: 13
Training loss: 2.3199541568756104
Validation loss: 2.1038994814759944

Epoch: 260| Step: 0
Training loss: 2.8087124824523926
Validation loss: 2.096469748404718

Epoch: 6| Step: 1
Training loss: 2.0293872356414795
Validation loss: 2.1109855918474096

Epoch: 6| Step: 2
Training loss: 1.5721137523651123
Validation loss: 2.10567367974148

Epoch: 6| Step: 3
Training loss: 2.6315524578094482
Validation loss: 2.1170196892112814

Epoch: 6| Step: 4
Training loss: 2.1024727821350098
Validation loss: 2.1397668597518757

Epoch: 6| Step: 5
Training loss: 2.130704879760742
Validation loss: 2.1430741176810315

Epoch: 6| Step: 6
Training loss: 1.5999183654785156
Validation loss: 2.128785692235475

Epoch: 6| Step: 7
Training loss: 1.6575708389282227
Validation loss: 2.1347564522938063

Epoch: 6| Step: 8
Training loss: 1.8347883224487305
Validation loss: 2.1055052254789617

Epoch: 6| Step: 9
Training loss: 1.4387211799621582
Validation loss: 2.0793116746410245

Epoch: 6| Step: 10
Training loss: 2.903169631958008
Validation loss: 2.0764592539879585

Epoch: 6| Step: 11
Training loss: 1.4891246557235718
Validation loss: 2.045165495205951

Epoch: 6| Step: 12
Training loss: 1.6473424434661865
Validation loss: 2.035216377627465

Epoch: 6| Step: 13
Training loss: 2.410548210144043
Validation loss: 2.0273224192280925

Epoch: 261| Step: 0
Training loss: 1.9870553016662598
Validation loss: 2.0519141791969218

Epoch: 6| Step: 1
Training loss: 2.16451358795166
Validation loss: 2.0664154047607095

Epoch: 6| Step: 2
Training loss: 1.6475183963775635
Validation loss: 2.0663785139719644

Epoch: 6| Step: 3
Training loss: 2.5495944023132324
Validation loss: 2.060858840583473

Epoch: 6| Step: 4
Training loss: 1.9828739166259766
Validation loss: 2.078084735460179

Epoch: 6| Step: 5
Training loss: 2.1313982009887695
Validation loss: 2.1105591968823503

Epoch: 6| Step: 6
Training loss: 2.2167344093322754
Validation loss: 2.1098910531690045

Epoch: 6| Step: 7
Training loss: 1.6397693157196045
Validation loss: 2.127473977304274

Epoch: 6| Step: 8
Training loss: 2.7237892150878906
Validation loss: 2.126049321184876

Epoch: 6| Step: 9
Training loss: 2.098043441772461
Validation loss: 2.108224053536692

Epoch: 6| Step: 10
Training loss: 1.8663997650146484
Validation loss: 2.091061894611646

Epoch: 6| Step: 11
Training loss: 1.9317926168441772
Validation loss: 2.073242131099906

Epoch: 6| Step: 12
Training loss: 0.8846327066421509
Validation loss: 2.1075980791481594

Epoch: 6| Step: 13
Training loss: 1.9924254417419434
Validation loss: 2.093561098139773

Epoch: 262| Step: 0
Training loss: 1.3177337646484375
Validation loss: 2.1146381926792923

Epoch: 6| Step: 1
Training loss: 1.8875925540924072
Validation loss: 2.1241672141577608

Epoch: 6| Step: 2
Training loss: 2.598296880722046
Validation loss: 2.1441234016931183

Epoch: 6| Step: 3
Training loss: 2.109879493713379
Validation loss: 2.185932038932718

Epoch: 6| Step: 4
Training loss: 1.6531562805175781
Validation loss: 2.188892883639182

Epoch: 6| Step: 5
Training loss: 2.0644190311431885
Validation loss: 2.1758980084491033

Epoch: 6| Step: 6
Training loss: 1.6190799474716187
Validation loss: 2.1453631577953214

Epoch: 6| Step: 7
Training loss: 2.1169986724853516
Validation loss: 2.1339809997107393

Epoch: 6| Step: 8
Training loss: 2.255953550338745
Validation loss: 2.1371857902055145

Epoch: 6| Step: 9
Training loss: 2.91807222366333
Validation loss: 2.114816402876249

Epoch: 6| Step: 10
Training loss: 2.012103319168091
Validation loss: 2.1027545236772105

Epoch: 6| Step: 11
Training loss: 1.9473152160644531
Validation loss: 2.0802386089037825

Epoch: 6| Step: 12
Training loss: 1.6955753564834595
Validation loss: 2.0727201892483618

Epoch: 6| Step: 13
Training loss: 1.8932194709777832
Validation loss: 2.054951993368005

Epoch: 263| Step: 0
Training loss: 2.1634654998779297
Validation loss: 2.054161374286939

Epoch: 6| Step: 1
Training loss: 1.9318963289260864
Validation loss: 2.04436138368422

Epoch: 6| Step: 2
Training loss: 2.5502820014953613
Validation loss: 2.035477735662973

Epoch: 6| Step: 3
Training loss: 1.930952548980713
Validation loss: 2.0431235349306496

Epoch: 6| Step: 4
Training loss: 1.8004988431930542
Validation loss: 2.042381631430759

Epoch: 6| Step: 5
Training loss: 2.0673465728759766
Validation loss: 2.050762822551112

Epoch: 6| Step: 6
Training loss: 1.7209782600402832
Validation loss: 2.049792689661826

Epoch: 6| Step: 7
Training loss: 1.898393988609314
Validation loss: 2.065019461416429

Epoch: 6| Step: 8
Training loss: 1.7679641246795654
Validation loss: 2.061546443611063

Epoch: 6| Step: 9
Training loss: 1.7214356660842896
Validation loss: 2.06811329882632

Epoch: 6| Step: 10
Training loss: 2.1052145957946777
Validation loss: 2.0667643777785765

Epoch: 6| Step: 11
Training loss: 1.921612024307251
Validation loss: 2.0716856551426712

Epoch: 6| Step: 12
Training loss: 2.3969473838806152
Validation loss: 2.0714423733372844

Epoch: 6| Step: 13
Training loss: 2.0792007446289062
Validation loss: 2.0629583789456274

Epoch: 264| Step: 0
Training loss: 2.368367910385132
Validation loss: 2.064491738555252

Epoch: 6| Step: 1
Training loss: 2.084932565689087
Validation loss: 2.0832711048023675

Epoch: 6| Step: 2
Training loss: 2.202874183654785
Validation loss: 2.0970692160309

Epoch: 6| Step: 3
Training loss: 2.1814305782318115
Validation loss: 2.106855224537593

Epoch: 6| Step: 4
Training loss: 1.5455694198608398
Validation loss: 2.10519572483596

Epoch: 6| Step: 5
Training loss: 2.379904270172119
Validation loss: 2.114732160363146

Epoch: 6| Step: 6
Training loss: 1.4981257915496826
Validation loss: 2.125557607220065

Epoch: 6| Step: 7
Training loss: 1.9189094305038452
Validation loss: 2.098099507311339

Epoch: 6| Step: 8
Training loss: 1.4274067878723145
Validation loss: 2.117754982363793

Epoch: 6| Step: 9
Training loss: 2.227020740509033
Validation loss: 2.081473446661426

Epoch: 6| Step: 10
Training loss: 2.428551197052002
Validation loss: 2.0881270272757417

Epoch: 6| Step: 11
Training loss: 2.0673227310180664
Validation loss: 2.073329020571965

Epoch: 6| Step: 12
Training loss: 2.0360641479492188
Validation loss: 2.0827953892369426

Epoch: 6| Step: 13
Training loss: 1.34542977809906
Validation loss: 2.081480492827713

Epoch: 265| Step: 0
Training loss: 2.5884058475494385
Validation loss: 2.083332323258923

Epoch: 6| Step: 1
Training loss: 1.7972335815429688
Validation loss: 2.075885083085747

Epoch: 6| Step: 2
Training loss: 1.7766740322113037
Validation loss: 2.079199465372229

Epoch: 6| Step: 3
Training loss: 1.9423651695251465
Validation loss: 2.063222608258647

Epoch: 6| Step: 4
Training loss: 1.7166121006011963
Validation loss: 2.0690666270512406

Epoch: 6| Step: 5
Training loss: 2.2548470497131348
Validation loss: 2.060730211196407

Epoch: 6| Step: 6
Training loss: 1.8270509243011475
Validation loss: 2.0909291749359458

Epoch: 6| Step: 7
Training loss: 2.288689374923706
Validation loss: 2.115704490292457

Epoch: 6| Step: 8
Training loss: 1.3309330940246582
Validation loss: 2.1235330566283195

Epoch: 6| Step: 9
Training loss: 2.521524429321289
Validation loss: 2.100844020484596

Epoch: 6| Step: 10
Training loss: 2.0497028827667236
Validation loss: 2.09810895048162

Epoch: 6| Step: 11
Training loss: 1.2770651578903198
Validation loss: 2.108917884929206

Epoch: 6| Step: 12
Training loss: 2.5698370933532715
Validation loss: 2.077746502814754

Epoch: 6| Step: 13
Training loss: 1.5933910608291626
Validation loss: 2.0612568291284705

Epoch: 266| Step: 0
Training loss: 1.905857801437378
Validation loss: 2.03875167651843

Epoch: 6| Step: 1
Training loss: 2.5900049209594727
Validation loss: 2.024712715097653

Epoch: 6| Step: 2
Training loss: 1.9330217838287354
Validation loss: 2.024672697949153

Epoch: 6| Step: 3
Training loss: 1.7892825603485107
Validation loss: 2.0232437092770814

Epoch: 6| Step: 4
Training loss: 1.540177583694458
Validation loss: 2.03499803491818

Epoch: 6| Step: 5
Training loss: 2.378957986831665
Validation loss: 2.045370469811142

Epoch: 6| Step: 6
Training loss: 1.5117616653442383
Validation loss: 2.048656743059876

Epoch: 6| Step: 7
Training loss: 2.2206368446350098
Validation loss: 2.0446737158683037

Epoch: 6| Step: 8
Training loss: 1.6046234369277954
Validation loss: 2.0768672676496607

Epoch: 6| Step: 9
Training loss: 1.8397798538208008
Validation loss: 2.1150102179537535

Epoch: 6| Step: 10
Training loss: 2.1789939403533936
Validation loss: 2.1099268595377603

Epoch: 6| Step: 11
Training loss: 2.3841161727905273
Validation loss: 2.129035475433514

Epoch: 6| Step: 12
Training loss: 2.016759157180786
Validation loss: 2.140082028604323

Epoch: 6| Step: 13
Training loss: 1.80710768699646
Validation loss: 2.131235114989742

Epoch: 267| Step: 0
Training loss: 1.6014814376831055
Validation loss: 2.1309373263389833

Epoch: 6| Step: 1
Training loss: 1.582370400428772
Validation loss: 2.126845840484865

Epoch: 6| Step: 2
Training loss: 1.449265718460083
Validation loss: 2.1108588864726405

Epoch: 6| Step: 3
Training loss: 1.1023070812225342
Validation loss: 2.083853449872745

Epoch: 6| Step: 4
Training loss: 2.0883476734161377
Validation loss: 2.07661299808051

Epoch: 6| Step: 5
Training loss: 2.4534754753112793
Validation loss: 2.0600638492133028

Epoch: 6| Step: 6
Training loss: 2.6487016677856445
Validation loss: 2.0679595778065343

Epoch: 6| Step: 7
Training loss: 1.9887155294418335
Validation loss: 2.0425240762772097

Epoch: 6| Step: 8
Training loss: 1.7792840003967285
Validation loss: 2.042038756032144

Epoch: 6| Step: 9
Training loss: 2.555675983428955
Validation loss: 2.033635531702349

Epoch: 6| Step: 10
Training loss: 1.7407495975494385
Validation loss: 2.059549608538228

Epoch: 6| Step: 11
Training loss: 2.1117281913757324
Validation loss: 2.07999224047507

Epoch: 6| Step: 12
Training loss: 2.193307399749756
Validation loss: 2.0953254827889065

Epoch: 6| Step: 13
Training loss: 2.33876895904541
Validation loss: 2.1310830577727287

Epoch: 268| Step: 0
Training loss: 1.2301112413406372
Validation loss: 2.1461811783493205

Epoch: 6| Step: 1
Training loss: 1.5535506010055542
Validation loss: 2.108830057164674

Epoch: 6| Step: 2
Training loss: 1.3616137504577637
Validation loss: 2.101433341221143

Epoch: 6| Step: 3
Training loss: 1.8504780530929565
Validation loss: 2.08770517892735

Epoch: 6| Step: 4
Training loss: 1.8284507989883423
Validation loss: 2.100758034695861

Epoch: 6| Step: 5
Training loss: 2.347979784011841
Validation loss: 2.109868757186397

Epoch: 6| Step: 6
Training loss: 2.352780818939209
Validation loss: 2.1019394359280987

Epoch: 6| Step: 7
Training loss: 1.8079601526260376
Validation loss: 2.098718104823943

Epoch: 6| Step: 8
Training loss: 2.708939552307129
Validation loss: 2.0985085861657256

Epoch: 6| Step: 9
Training loss: 2.254499673843384
Validation loss: 2.118078326666227

Epoch: 6| Step: 10
Training loss: 1.4581284523010254
Validation loss: 2.0930567428629887

Epoch: 6| Step: 11
Training loss: 2.3548667430877686
Validation loss: 2.104132422836878

Epoch: 6| Step: 12
Training loss: 2.5825767517089844
Validation loss: 2.126435549028458

Epoch: 6| Step: 13
Training loss: 2.149739980697632
Validation loss: 2.097615982896538

Epoch: 269| Step: 0
Training loss: 2.237355947494507
Validation loss: 2.097443380663472

Epoch: 6| Step: 1
Training loss: 1.4076855182647705
Validation loss: 2.092990944462438

Epoch: 6| Step: 2
Training loss: 1.7041113376617432
Validation loss: 2.0643338080375426

Epoch: 6| Step: 3
Training loss: 1.7840737104415894
Validation loss: 2.065318444723724

Epoch: 6| Step: 4
Training loss: 2.024845838546753
Validation loss: 2.0896639823913574

Epoch: 6| Step: 5
Training loss: 2.236051559448242
Validation loss: 2.0754699245575936

Epoch: 6| Step: 6
Training loss: 1.5176537036895752
Validation loss: 2.0822744164415585

Epoch: 6| Step: 7
Training loss: 2.5956223011016846
Validation loss: 2.0792453186486357

Epoch: 6| Step: 8
Training loss: 2.073230743408203
Validation loss: 2.054911703191778

Epoch: 6| Step: 9
Training loss: 1.8105382919311523
Validation loss: 2.070846474298867

Epoch: 6| Step: 10
Training loss: 2.138211488723755
Validation loss: 2.071865415060392

Epoch: 6| Step: 11
Training loss: 1.8608957529067993
Validation loss: 2.0759351779055852

Epoch: 6| Step: 12
Training loss: 2.0248382091522217
Validation loss: 2.075616486610905

Epoch: 6| Step: 13
Training loss: 1.581390619277954
Validation loss: 2.083718902321272

Epoch: 270| Step: 0
Training loss: 2.140322685241699
Validation loss: 2.0694870384790565

Epoch: 6| Step: 1
Training loss: 1.6897947788238525
Validation loss: 2.070602919465752

Epoch: 6| Step: 2
Training loss: 1.869876503944397
Validation loss: 2.0792535504987164

Epoch: 6| Step: 3
Training loss: 1.6277031898498535
Validation loss: 2.0663306072194088

Epoch: 6| Step: 4
Training loss: 2.1457366943359375
Validation loss: 2.081657713459384

Epoch: 6| Step: 5
Training loss: 1.5800654888153076
Validation loss: 2.08106804022225

Epoch: 6| Step: 6
Training loss: 2.3042497634887695
Validation loss: 2.096113085746765

Epoch: 6| Step: 7
Training loss: 1.9724900722503662
Validation loss: 2.085460219331967

Epoch: 6| Step: 8
Training loss: 1.0903421640396118
Validation loss: 2.105931601216716

Epoch: 6| Step: 9
Training loss: 2.019707441329956
Validation loss: 2.096020088400892

Epoch: 6| Step: 10
Training loss: 2.2316014766693115
Validation loss: 2.104208884700652

Epoch: 6| Step: 11
Training loss: 1.8224104642868042
Validation loss: 2.0767531138594433

Epoch: 6| Step: 12
Training loss: 2.2855963706970215
Validation loss: 2.0636761009052234

Epoch: 6| Step: 13
Training loss: 2.256218194961548
Validation loss: 2.0538562087602514

Epoch: 271| Step: 0
Training loss: 2.2683019638061523
Validation loss: 2.0419001848466936

Epoch: 6| Step: 1
Training loss: 1.645648717880249
Validation loss: 2.043433450883435

Epoch: 6| Step: 2
Training loss: 1.9740726947784424
Validation loss: 2.0334364098887288

Epoch: 6| Step: 3
Training loss: 1.1796417236328125
Validation loss: 2.039104459106281

Epoch: 6| Step: 4
Training loss: 2.5138814449310303
Validation loss: 2.038313747734152

Epoch: 6| Step: 5
Training loss: 2.063971996307373
Validation loss: 2.0624024842375066

Epoch: 6| Step: 6
Training loss: 2.6666135787963867
Validation loss: 2.080116582173173

Epoch: 6| Step: 7
Training loss: 2.2254176139831543
Validation loss: 2.089063775154852

Epoch: 6| Step: 8
Training loss: 1.4116588830947876
Validation loss: 2.1390393305850286

Epoch: 6| Step: 9
Training loss: 2.0803327560424805
Validation loss: 2.1519507900361092

Epoch: 6| Step: 10
Training loss: 1.4841725826263428
Validation loss: 2.125383657793845

Epoch: 6| Step: 11
Training loss: 1.9872887134552002
Validation loss: 2.117623434271864

Epoch: 6| Step: 12
Training loss: 1.8391824960708618
Validation loss: 2.0691604511712187

Epoch: 6| Step: 13
Training loss: 1.7335493564605713
Validation loss: 2.0497665969274377

Epoch: 272| Step: 0
Training loss: 1.5432120561599731
Validation loss: 2.0422151755261164

Epoch: 6| Step: 1
Training loss: 2.095475673675537
Validation loss: 2.033242223083332

Epoch: 6| Step: 2
Training loss: 1.7000722885131836
Validation loss: 2.033488594075685

Epoch: 6| Step: 3
Training loss: 1.4696112871170044
Validation loss: 2.0332440266045193

Epoch: 6| Step: 4
Training loss: 2.0957930088043213
Validation loss: 2.050998838998938

Epoch: 6| Step: 5
Training loss: 1.6104316711425781
Validation loss: 2.059138200616324

Epoch: 6| Step: 6
Training loss: 2.173429489135742
Validation loss: 2.0603957496663576

Epoch: 6| Step: 7
Training loss: 2.4569506645202637
Validation loss: 2.068307726613937

Epoch: 6| Step: 8
Training loss: 2.2464332580566406
Validation loss: 2.097735171676964

Epoch: 6| Step: 9
Training loss: 1.9182016849517822
Validation loss: 2.1057779096787974

Epoch: 6| Step: 10
Training loss: 2.529344081878662
Validation loss: 2.1182067471165813

Epoch: 6| Step: 11
Training loss: 1.3712544441223145
Validation loss: 2.120393950452087

Epoch: 6| Step: 12
Training loss: 1.8218929767608643
Validation loss: 2.123210951846133

Epoch: 6| Step: 13
Training loss: 2.323213815689087
Validation loss: 2.1165130317852063

Epoch: 273| Step: 0
Training loss: 1.4856643676757812
Validation loss: 2.1313771560627925

Epoch: 6| Step: 1
Training loss: 1.7977147102355957
Validation loss: 2.1102904529981714

Epoch: 6| Step: 2
Training loss: 1.967730164527893
Validation loss: 2.1033669915250552

Epoch: 6| Step: 3
Training loss: 1.565802812576294
Validation loss: 2.095668508160499

Epoch: 6| Step: 4
Training loss: 2.5729074478149414
Validation loss: 2.0676123237097137

Epoch: 6| Step: 5
Training loss: 2.439037799835205
Validation loss: 2.077346255702357

Epoch: 6| Step: 6
Training loss: 2.3426852226257324
Validation loss: 2.0794792124020156

Epoch: 6| Step: 7
Training loss: 2.0614120960235596
Validation loss: 2.0835512697055774

Epoch: 6| Step: 8
Training loss: 1.7165967226028442
Validation loss: 2.0808148076457362

Epoch: 6| Step: 9
Training loss: 1.5319492816925049
Validation loss: 2.0870837037281325

Epoch: 6| Step: 10
Training loss: 1.9394203424453735
Validation loss: 2.1200615923891784

Epoch: 6| Step: 11
Training loss: 1.7684006690979004
Validation loss: 2.118486970983526

Epoch: 6| Step: 12
Training loss: 2.193079710006714
Validation loss: 2.118881743441346

Epoch: 6| Step: 13
Training loss: 1.3129196166992188
Validation loss: 2.137760449481267

Epoch: 274| Step: 0
Training loss: 1.8652790784835815
Validation loss: 2.1126117629389607

Epoch: 6| Step: 1
Training loss: 2.0354552268981934
Validation loss: 2.0899585293185328

Epoch: 6| Step: 2
Training loss: 1.8492050170898438
Validation loss: 2.0941333975843204

Epoch: 6| Step: 3
Training loss: 2.1147663593292236
Validation loss: 2.0560795709651005

Epoch: 6| Step: 4
Training loss: 2.3173739910125732
Validation loss: 2.0478123413619174

Epoch: 6| Step: 5
Training loss: 1.9086867570877075
Validation loss: 2.023322602753998

Epoch: 6| Step: 6
Training loss: 1.0831024646759033
Validation loss: 2.0233238140741983

Epoch: 6| Step: 7
Training loss: 1.9141870737075806
Validation loss: 2.0161417914975073

Epoch: 6| Step: 8
Training loss: 2.4370455741882324
Validation loss: 2.03794276842507

Epoch: 6| Step: 9
Training loss: 1.3531697988510132
Validation loss: 2.031233964427825

Epoch: 6| Step: 10
Training loss: 1.365451455116272
Validation loss: 2.0380312217179166

Epoch: 6| Step: 11
Training loss: 2.2378392219543457
Validation loss: 2.0783727194673274

Epoch: 6| Step: 12
Training loss: 2.13887619972229
Validation loss: 2.119332182791925

Epoch: 6| Step: 13
Training loss: 2.155843496322632
Validation loss: 2.1621258630547473

Epoch: 275| Step: 0
Training loss: 2.0367319583892822
Validation loss: 2.155312982938623

Epoch: 6| Step: 1
Training loss: 2.0145304203033447
Validation loss: 2.174106736336985

Epoch: 6| Step: 2
Training loss: 2.360940933227539
Validation loss: 2.1521494029670634

Epoch: 6| Step: 3
Training loss: 1.472714900970459
Validation loss: 2.1104493730811664

Epoch: 6| Step: 4
Training loss: 1.6202943325042725
Validation loss: 2.089586898844729

Epoch: 6| Step: 5
Training loss: 1.7496103048324585
Validation loss: 2.0794733339740383

Epoch: 6| Step: 6
Training loss: 1.8865253925323486
Validation loss: 2.0574764051745014

Epoch: 6| Step: 7
Training loss: 1.680319905281067
Validation loss: 2.052009787610782

Epoch: 6| Step: 8
Training loss: 2.1030383110046387
Validation loss: 2.055745437581052

Epoch: 6| Step: 9
Training loss: 2.0798962116241455
Validation loss: 2.081677882902084

Epoch: 6| Step: 10
Training loss: 2.3625171184539795
Validation loss: 2.099464708758939

Epoch: 6| Step: 11
Training loss: 1.6574313640594482
Validation loss: 2.099605675666563

Epoch: 6| Step: 12
Training loss: 1.968658208847046
Validation loss: 2.0970577245117514

Epoch: 6| Step: 13
Training loss: 1.5461876392364502
Validation loss: 2.0741615244137344

Epoch: 276| Step: 0
Training loss: 1.9081956148147583
Validation loss: 2.0914226014127015

Epoch: 6| Step: 1
Training loss: 2.1175262928009033
Validation loss: 2.0852110949895715

Epoch: 6| Step: 2
Training loss: 1.9910387992858887
Validation loss: 2.087231127164697

Epoch: 6| Step: 3
Training loss: 2.685393810272217
Validation loss: 2.089386228592165

Epoch: 6| Step: 4
Training loss: 2.3267722129821777
Validation loss: 2.0868726058672835

Epoch: 6| Step: 5
Training loss: 1.5688034296035767
Validation loss: 2.0956295536410425

Epoch: 6| Step: 6
Training loss: 2.1660056114196777
Validation loss: 2.073831891500822

Epoch: 6| Step: 7
Training loss: 2.1400372982025146
Validation loss: 2.0662932857390373

Epoch: 6| Step: 8
Training loss: 1.2997660636901855
Validation loss: 2.0620694391189085

Epoch: 6| Step: 9
Training loss: 1.9018213748931885
Validation loss: 2.0857640620200866

Epoch: 6| Step: 10
Training loss: 2.075850009918213
Validation loss: 2.106668528690133

Epoch: 6| Step: 11
Training loss: 1.4638386964797974
Validation loss: 2.1032034068979244

Epoch: 6| Step: 12
Training loss: 1.4636250734329224
Validation loss: 2.1058275648342666

Epoch: 6| Step: 13
Training loss: 1.3944252729415894
Validation loss: 2.0576444313090336

Epoch: 277| Step: 0
Training loss: 2.281074047088623
Validation loss: 2.0587000103407007

Epoch: 6| Step: 1
Training loss: 1.6273787021636963
Validation loss: 2.0716782616030787

Epoch: 6| Step: 2
Training loss: 2.189678430557251
Validation loss: 2.063694069462438

Epoch: 6| Step: 3
Training loss: 2.039431571960449
Validation loss: 2.0804191609864593

Epoch: 6| Step: 4
Training loss: 1.80178964138031
Validation loss: 2.10527305833755

Epoch: 6| Step: 5
Training loss: 2.4110729694366455
Validation loss: 2.104781044426785

Epoch: 6| Step: 6
Training loss: 1.5868773460388184
Validation loss: 2.076930098636176

Epoch: 6| Step: 7
Training loss: 1.7076525688171387
Validation loss: 2.057043888235605

Epoch: 6| Step: 8
Training loss: 1.899837613105774
Validation loss: 2.0222330759930354

Epoch: 6| Step: 9
Training loss: 1.9441320896148682
Validation loss: 2.0288120110829673

Epoch: 6| Step: 10
Training loss: 1.7802807092666626
Validation loss: 2.042428292253966

Epoch: 6| Step: 11
Training loss: 1.7019259929656982
Validation loss: 2.0275118094618603

Epoch: 6| Step: 12
Training loss: 1.7798163890838623
Validation loss: 2.058045446231801

Epoch: 6| Step: 13
Training loss: 1.6024065017700195
Validation loss: 2.0735712512846916

Epoch: 278| Step: 0
Training loss: 1.992661714553833
Validation loss: 2.1243600191608554

Epoch: 6| Step: 1
Training loss: 1.9510554075241089
Validation loss: 2.115927953873911

Epoch: 6| Step: 2
Training loss: 1.476265788078308
Validation loss: 2.1336261046830045

Epoch: 6| Step: 3
Training loss: 2.478746175765991
Validation loss: 2.1009366999390306

Epoch: 6| Step: 4
Training loss: 1.3430118560791016
Validation loss: 2.1017277817572317

Epoch: 6| Step: 5
Training loss: 1.9550058841705322
Validation loss: 2.090437419952885

Epoch: 6| Step: 6
Training loss: 1.9157994985580444
Validation loss: 2.06916146380927

Epoch: 6| Step: 7
Training loss: 1.8605772256851196
Validation loss: 2.058182179286916

Epoch: 6| Step: 8
Training loss: 1.7365891933441162
Validation loss: 2.058191180229187

Epoch: 6| Step: 9
Training loss: 1.9511828422546387
Validation loss: 2.0516687836698306

Epoch: 6| Step: 10
Training loss: 1.4506151676177979
Validation loss: 2.045373051397262

Epoch: 6| Step: 11
Training loss: 2.124819755554199
Validation loss: 2.0570638000324206

Epoch: 6| Step: 12
Training loss: 2.3789238929748535
Validation loss: 2.0784492800312657

Epoch: 6| Step: 13
Training loss: 1.3864132165908813
Validation loss: 2.0616968216434604

Epoch: 279| Step: 0
Training loss: 1.713179588317871
Validation loss: 2.1010503692011677

Epoch: 6| Step: 1
Training loss: 1.9724221229553223
Validation loss: 2.1613514679734425

Epoch: 6| Step: 2
Training loss: 1.8958253860473633
Validation loss: 2.183339580412834

Epoch: 6| Step: 3
Training loss: 1.8811147212982178
Validation loss: 2.1789591850772982

Epoch: 6| Step: 4
Training loss: 1.8479223251342773
Validation loss: 2.1425505338176603

Epoch: 6| Step: 5
Training loss: 1.7263047695159912
Validation loss: 2.1055833831910165

Epoch: 6| Step: 6
Training loss: 2.317547082901001
Validation loss: 2.059499361181772

Epoch: 6| Step: 7
Training loss: 1.506628394126892
Validation loss: 2.038496249465532

Epoch: 6| Step: 8
Training loss: 1.4007389545440674
Validation loss: 2.0403402825837493

Epoch: 6| Step: 9
Training loss: 2.0834503173828125
Validation loss: 2.024447370600957

Epoch: 6| Step: 10
Training loss: 1.6250901222229004
Validation loss: 2.0356983036123295

Epoch: 6| Step: 11
Training loss: 2.079834222793579
Validation loss: 2.0354239197187525

Epoch: 6| Step: 12
Training loss: 2.015064239501953
Validation loss: 2.0564454473474973

Epoch: 6| Step: 13
Training loss: 2.654759645462036
Validation loss: 2.0690489815127466

Epoch: 280| Step: 0
Training loss: 0.9631928205490112
Validation loss: 2.1063596202481176

Epoch: 6| Step: 1
Training loss: 1.8074605464935303
Validation loss: 2.1160097699011526

Epoch: 6| Step: 2
Training loss: 1.7073297500610352
Validation loss: 2.118698973809519

Epoch: 6| Step: 3
Training loss: 2.027146816253662
Validation loss: 2.130985349737188

Epoch: 6| Step: 4
Training loss: 2.034045934677124
Validation loss: 2.115799346277791

Epoch: 6| Step: 5
Training loss: 1.5017492771148682
Validation loss: 2.109944828094975

Epoch: 6| Step: 6
Training loss: 1.7688859701156616
Validation loss: 2.0852659645900933

Epoch: 6| Step: 7
Training loss: 1.4869396686553955
Validation loss: 2.085891064777169

Epoch: 6| Step: 8
Training loss: 2.078956127166748
Validation loss: 2.0480734661061275

Epoch: 6| Step: 9
Training loss: 2.577937602996826
Validation loss: 2.080665451224132

Epoch: 6| Step: 10
Training loss: 2.4986400604248047
Validation loss: 2.0691651862154723

Epoch: 6| Step: 11
Training loss: 1.6836020946502686
Validation loss: 2.066303967147745

Epoch: 6| Step: 12
Training loss: 1.8154795169830322
Validation loss: 2.054585087683893

Epoch: 6| Step: 13
Training loss: 2.4517202377319336
Validation loss: 2.0466918817130466

Epoch: 281| Step: 0
Training loss: 1.6029448509216309
Validation loss: 2.053892889330464

Epoch: 6| Step: 1
Training loss: 1.6114461421966553
Validation loss: 2.076928356642364

Epoch: 6| Step: 2
Training loss: 1.9438238143920898
Validation loss: 2.0719937521924257

Epoch: 6| Step: 3
Training loss: 1.7985460758209229
Validation loss: 2.068612321730583

Epoch: 6| Step: 4
Training loss: 2.377450704574585
Validation loss: 2.1021207365938412

Epoch: 6| Step: 5
Training loss: 1.967219352722168
Validation loss: 2.0817377695473294

Epoch: 6| Step: 6
Training loss: 2.228971481323242
Validation loss: 2.0676621083290345

Epoch: 6| Step: 7
Training loss: 1.751432180404663
Validation loss: 2.0466242298003166

Epoch: 6| Step: 8
Training loss: 1.912483811378479
Validation loss: 2.0320085915186072

Epoch: 6| Step: 9
Training loss: 1.9850572347640991
Validation loss: 2.0330368036864908

Epoch: 6| Step: 10
Training loss: 1.8291747570037842
Validation loss: 2.0241988474322903

Epoch: 6| Step: 11
Training loss: 1.3634400367736816
Validation loss: 2.034187022075858

Epoch: 6| Step: 12
Training loss: 1.2220485210418701
Validation loss: 2.0495829812942015

Epoch: 6| Step: 13
Training loss: 2.5163416862487793
Validation loss: 2.047140634188088

Epoch: 282| Step: 0
Training loss: 1.3893455266952515
Validation loss: 2.0484711175323813

Epoch: 6| Step: 1
Training loss: 1.3107664585113525
Validation loss: 2.084950175336612

Epoch: 6| Step: 2
Training loss: 1.9312849044799805
Validation loss: 2.078450925888554

Epoch: 6| Step: 3
Training loss: 1.9473658800125122
Validation loss: 2.06121329851048

Epoch: 6| Step: 4
Training loss: 1.3922630548477173
Validation loss: 2.071873322609932

Epoch: 6| Step: 5
Training loss: 1.4400482177734375
Validation loss: 2.0905251464536114

Epoch: 6| Step: 6
Training loss: 1.8825608491897583
Validation loss: 2.081396825851933

Epoch: 6| Step: 7
Training loss: 2.431042432785034
Validation loss: 2.1083839542122296

Epoch: 6| Step: 8
Training loss: 1.9578471183776855
Validation loss: 2.0744244488336707

Epoch: 6| Step: 9
Training loss: 1.5027743577957153
Validation loss: 2.074807333689864

Epoch: 6| Step: 10
Training loss: 2.075164794921875
Validation loss: 2.073886776483187

Epoch: 6| Step: 11
Training loss: 1.6940160989761353
Validation loss: 2.0748013886072303

Epoch: 6| Step: 12
Training loss: 2.4959349632263184
Validation loss: 2.074797050927275

Epoch: 6| Step: 13
Training loss: 2.280662775039673
Validation loss: 2.0604922130543697

Epoch: 283| Step: 0
Training loss: 1.2169816493988037
Validation loss: 2.052535746687202

Epoch: 6| Step: 1
Training loss: 1.6696038246154785
Validation loss: 2.076559115481633

Epoch: 6| Step: 2
Training loss: 1.2050862312316895
Validation loss: 2.0747207544183217

Epoch: 6| Step: 3
Training loss: 2.3101754188537598
Validation loss: 2.080641934948583

Epoch: 6| Step: 4
Training loss: 2.031771659851074
Validation loss: 2.092320062780893

Epoch: 6| Step: 5
Training loss: 1.9304442405700684
Validation loss: 2.1058818345428794

Epoch: 6| Step: 6
Training loss: 2.045198917388916
Validation loss: 2.1130267240667857

Epoch: 6| Step: 7
Training loss: 2.3302526473999023
Validation loss: 2.1382232250705844

Epoch: 6| Step: 8
Training loss: 2.444436550140381
Validation loss: 2.1063744278364283

Epoch: 6| Step: 9
Training loss: 1.4474108219146729
Validation loss: 2.058084636606196

Epoch: 6| Step: 10
Training loss: 1.4679019451141357
Validation loss: 2.044523395517821

Epoch: 6| Step: 11
Training loss: 2.139338970184326
Validation loss: 2.0504157799546436

Epoch: 6| Step: 12
Training loss: 1.994981050491333
Validation loss: 2.0286757587104716

Epoch: 6| Step: 13
Training loss: 1.3620754480361938
Validation loss: 2.014434658071046

Epoch: 284| Step: 0
Training loss: 1.6247584819793701
Validation loss: 2.0170348177674

Epoch: 6| Step: 1
Training loss: 1.2643585205078125
Validation loss: 2.0371188937976794

Epoch: 6| Step: 2
Training loss: 1.6656177043914795
Validation loss: 2.042556434549311

Epoch: 6| Step: 3
Training loss: 1.0634574890136719
Validation loss: 2.0588193311486194

Epoch: 6| Step: 4
Training loss: 1.924979329109192
Validation loss: 2.083533501112333

Epoch: 6| Step: 5
Training loss: 2.4397382736206055
Validation loss: 2.087905768425234

Epoch: 6| Step: 6
Training loss: 1.6813745498657227
Validation loss: 2.1022574568307526

Epoch: 6| Step: 7
Training loss: 2.189478874206543
Validation loss: 2.112863617558633

Epoch: 6| Step: 8
Training loss: 2.090961456298828
Validation loss: 2.132748298747565

Epoch: 6| Step: 9
Training loss: 1.7733380794525146
Validation loss: 2.176734511570264

Epoch: 6| Step: 10
Training loss: 2.843813419342041
Validation loss: 2.1367248514647126

Epoch: 6| Step: 11
Training loss: 2.0579192638397217
Validation loss: 2.132342105270714

Epoch: 6| Step: 12
Training loss: 1.235150933265686
Validation loss: 2.100970960432483

Epoch: 6| Step: 13
Training loss: 1.8597806692123413
Validation loss: 2.0411008865602556

Epoch: 285| Step: 0
Training loss: 1.1921941041946411
Validation loss: 2.0400296295842817

Epoch: 6| Step: 1
Training loss: 1.9906864166259766
Validation loss: 2.0247429647753314

Epoch: 6| Step: 2
Training loss: 2.2902026176452637
Validation loss: 1.9969950158108947

Epoch: 6| Step: 3
Training loss: 1.7818222045898438
Validation loss: 1.998356189779056

Epoch: 6| Step: 4
Training loss: 1.9565417766571045
Validation loss: 1.9940573848703855

Epoch: 6| Step: 5
Training loss: 1.591604232788086
Validation loss: 1.9917991110073623

Epoch: 6| Step: 6
Training loss: 2.0417113304138184
Validation loss: 2.0231498338842906

Epoch: 6| Step: 7
Training loss: 1.7218265533447266
Validation loss: 2.0727464537466727

Epoch: 6| Step: 8
Training loss: 2.2264204025268555
Validation loss: 2.1112927441955893

Epoch: 6| Step: 9
Training loss: 1.9372682571411133
Validation loss: 2.160672546714865

Epoch: 6| Step: 10
Training loss: 2.4099645614624023
Validation loss: 2.1697936314408497

Epoch: 6| Step: 11
Training loss: 1.5480806827545166
Validation loss: 2.20283591875466

Epoch: 6| Step: 12
Training loss: 1.590279459953308
Validation loss: 2.1856599392429477

Epoch: 6| Step: 13
Training loss: 2.1295595169067383
Validation loss: 2.1433152691010506

Epoch: 286| Step: 0
Training loss: 1.4691400527954102
Validation loss: 2.120067590026445

Epoch: 6| Step: 1
Training loss: 1.70119309425354
Validation loss: 2.095149455531951

Epoch: 6| Step: 2
Training loss: 1.9853774309158325
Validation loss: 2.083610673104563

Epoch: 6| Step: 3
Training loss: 1.4386252164840698
Validation loss: 2.0619709414820515

Epoch: 6| Step: 4
Training loss: 1.1381361484527588
Validation loss: 2.0763095373748452

Epoch: 6| Step: 5
Training loss: 1.8898813724517822
Validation loss: 2.08888917328209

Epoch: 6| Step: 6
Training loss: 1.9221148490905762
Validation loss: 2.13095381311191

Epoch: 6| Step: 7
Training loss: 2.2715537548065186
Validation loss: 2.1241654093547533

Epoch: 6| Step: 8
Training loss: 1.766541600227356
Validation loss: 2.103668059072187

Epoch: 6| Step: 9
Training loss: 1.6572859287261963
Validation loss: 2.1190252252804336

Epoch: 6| Step: 10
Training loss: 2.275899887084961
Validation loss: 2.0857208159662064

Epoch: 6| Step: 11
Training loss: 2.428370952606201
Validation loss: 2.0592846267966816

Epoch: 6| Step: 12
Training loss: 1.6879618167877197
Validation loss: 2.056668696864959

Epoch: 6| Step: 13
Training loss: 1.7459220886230469
Validation loss: 2.046728212346313

Epoch: 287| Step: 0
Training loss: 1.5943500995635986
Validation loss: 2.029092732296195

Epoch: 6| Step: 1
Training loss: 1.8575413227081299
Validation loss: 2.035375272074053

Epoch: 6| Step: 2
Training loss: 1.2201768159866333
Validation loss: 2.0040567613417104

Epoch: 6| Step: 3
Training loss: 1.5685338973999023
Validation loss: 2.0171408807077715

Epoch: 6| Step: 4
Training loss: 1.5022432804107666
Validation loss: 2.0226236094710646

Epoch: 6| Step: 5
Training loss: 2.334381103515625
Validation loss: 2.002356216471682

Epoch: 6| Step: 6
Training loss: 2.195956230163574
Validation loss: 2.0244076533984114

Epoch: 6| Step: 7
Training loss: 1.8273959159851074
Validation loss: 2.032715279568908

Epoch: 6| Step: 8
Training loss: 1.8278148174285889
Validation loss: 2.0448270690056587

Epoch: 6| Step: 9
Training loss: 1.9088237285614014
Validation loss: 2.089517298565116

Epoch: 6| Step: 10
Training loss: 1.5283780097961426
Validation loss: 2.1952449147419264

Epoch: 6| Step: 11
Training loss: 2.0833356380462646
Validation loss: 2.2375416294220956

Epoch: 6| Step: 12
Training loss: 2.287806987762451
Validation loss: 2.2577405206618772

Epoch: 6| Step: 13
Training loss: 2.3128693103790283
Validation loss: 2.228813802042315

Epoch: 288| Step: 0
Training loss: 2.2688262462615967
Validation loss: 2.157261392121674

Epoch: 6| Step: 1
Training loss: 2.6633987426757812
Validation loss: 2.0952590511691187

Epoch: 6| Step: 2
Training loss: 2.25327730178833
Validation loss: 2.0449842227402555

Epoch: 6| Step: 3
Training loss: 1.2068085670471191
Validation loss: 2.027230444774833

Epoch: 6| Step: 4
Training loss: 1.7198798656463623
Validation loss: 2.0023814708955827

Epoch: 6| Step: 5
Training loss: 1.8740572929382324
Validation loss: 2.001446504746714

Epoch: 6| Step: 6
Training loss: 1.9481661319732666
Validation loss: 1.994359144600489

Epoch: 6| Step: 7
Training loss: 1.6307684183120728
Validation loss: 2.0114566100540983

Epoch: 6| Step: 8
Training loss: 1.9566972255706787
Validation loss: 2.001801085728471

Epoch: 6| Step: 9
Training loss: 2.2276594638824463
Validation loss: 2.01301900417574

Epoch: 6| Step: 10
Training loss: 1.5456657409667969
Validation loss: 2.0080137688626527

Epoch: 6| Step: 11
Training loss: 1.6929864883422852
Validation loss: 2.0348625106196248

Epoch: 6| Step: 12
Training loss: 1.4779973030090332
Validation loss: 2.0643104558349936

Epoch: 6| Step: 13
Training loss: 1.4440308809280396
Validation loss: 2.097369678558842

Epoch: 289| Step: 0
Training loss: 1.8182580471038818
Validation loss: 2.1357264364919355

Epoch: 6| Step: 1
Training loss: 2.367401599884033
Validation loss: 2.175803807473952

Epoch: 6| Step: 2
Training loss: 1.6051745414733887
Validation loss: 2.213979826178602

Epoch: 6| Step: 3
Training loss: 1.787961721420288
Validation loss: 2.2388011845209266

Epoch: 6| Step: 4
Training loss: 2.0135059356689453
Validation loss: 2.2569498849171463

Epoch: 6| Step: 5
Training loss: 2.0161819458007812
Validation loss: 2.2035794411936114

Epoch: 6| Step: 6
Training loss: 1.1744585037231445
Validation loss: 2.135977465619323

Epoch: 6| Step: 7
Training loss: 2.5372824668884277
Validation loss: 2.0727340611078406

Epoch: 6| Step: 8
Training loss: 1.7278234958648682
Validation loss: 2.0362583334727953

Epoch: 6| Step: 9
Training loss: 1.72398042678833
Validation loss: 2.0305271687046176

Epoch: 6| Step: 10
Training loss: 1.4395309686660767
Validation loss: 2.0046650209734516

Epoch: 6| Step: 11
Training loss: 2.467855215072632
Validation loss: 2.0023448057072137

Epoch: 6| Step: 12
Training loss: 1.9270286560058594
Validation loss: 2.007730384026804

Epoch: 6| Step: 13
Training loss: 1.295765995979309
Validation loss: 1.9811648066325853

Epoch: 290| Step: 0
Training loss: 1.355666160583496
Validation loss: 1.9744046862407396

Epoch: 6| Step: 1
Training loss: 1.89811372756958
Validation loss: 1.9931843126973798

Epoch: 6| Step: 2
Training loss: 0.8662411570549011
Validation loss: 1.9970991432025869

Epoch: 6| Step: 3
Training loss: 2.2477025985717773
Validation loss: 2.023497639163848

Epoch: 6| Step: 4
Training loss: 2.3855910301208496
Validation loss: 2.044237421404931

Epoch: 6| Step: 5
Training loss: 1.906189203262329
Validation loss: 2.059686904312462

Epoch: 6| Step: 6
Training loss: 1.5431808233261108
Validation loss: 2.09885052968097

Epoch: 6| Step: 7
Training loss: 1.7699029445648193
Validation loss: 2.150668374953731

Epoch: 6| Step: 8
Training loss: 1.2368433475494385
Validation loss: 2.1880748220669326

Epoch: 6| Step: 9
Training loss: 1.9766342639923096
Validation loss: 2.2173078367787022

Epoch: 6| Step: 10
Training loss: 2.2923617362976074
Validation loss: 2.242958943049113

Epoch: 6| Step: 11
Training loss: 2.6030097007751465
Validation loss: 2.2156980063325618

Epoch: 6| Step: 12
Training loss: 1.9522590637207031
Validation loss: 2.1547913858967442

Epoch: 6| Step: 13
Training loss: 1.635085105895996
Validation loss: 2.101711368048063

Epoch: 291| Step: 0
Training loss: 1.3986324071884155
Validation loss: 2.0603977505878737

Epoch: 6| Step: 1
Training loss: 2.204497814178467
Validation loss: 2.0119801336719143

Epoch: 6| Step: 2
Training loss: 1.8698538541793823
Validation loss: 1.9907409849987234

Epoch: 6| Step: 3
Training loss: 1.9606961011886597
Validation loss: 1.967180590475759

Epoch: 6| Step: 4
Training loss: 1.8659918308258057
Validation loss: 1.9767888963863414

Epoch: 6| Step: 5
Training loss: 2.6257951259613037
Validation loss: 1.9577147319752684

Epoch: 6| Step: 6
Training loss: 2.20934796333313
Validation loss: 1.9671782473082184

Epoch: 6| Step: 7
Training loss: 1.5915882587432861
Validation loss: 1.9605344803102556

Epoch: 6| Step: 8
Training loss: 1.5657418966293335
Validation loss: 1.96273983165782

Epoch: 6| Step: 9
Training loss: 2.061295986175537
Validation loss: 1.985027543960079

Epoch: 6| Step: 10
Training loss: 1.4513764381408691
Validation loss: 2.012395679309804

Epoch: 6| Step: 11
Training loss: 1.6416163444519043
Validation loss: 2.032672518043108

Epoch: 6| Step: 12
Training loss: 1.975759506225586
Validation loss: 2.047397477652437

Epoch: 6| Step: 13
Training loss: 1.476497769355774
Validation loss: 2.1125702640061736

Epoch: 292| Step: 0
Training loss: 1.2675514221191406
Validation loss: 2.1258004519247238

Epoch: 6| Step: 1
Training loss: 2.6642205715179443
Validation loss: 2.1762326289248723

Epoch: 6| Step: 2
Training loss: 1.4509878158569336
Validation loss: 2.1942817870006768

Epoch: 6| Step: 3
Training loss: 1.7903568744659424
Validation loss: 2.1603467310628583

Epoch: 6| Step: 4
Training loss: 2.4166362285614014
Validation loss: 2.145958633833034

Epoch: 6| Step: 5
Training loss: 1.637439489364624
Validation loss: 2.1053904307785856

Epoch: 6| Step: 6
Training loss: 1.446678638458252
Validation loss: 2.111456991523825

Epoch: 6| Step: 7
Training loss: 1.9403148889541626
Validation loss: 2.0639469110837547

Epoch: 6| Step: 8
Training loss: 1.7933270931243896
Validation loss: 2.055465116295763

Epoch: 6| Step: 9
Training loss: 1.5149941444396973
Validation loss: 2.0583791168787147

Epoch: 6| Step: 10
Training loss: 1.6975278854370117
Validation loss: 2.0282835652751308

Epoch: 6| Step: 11
Training loss: 1.9673678874969482
Validation loss: 2.0052107559737338

Epoch: 6| Step: 12
Training loss: 1.4699206352233887
Validation loss: 2.013228375424621

Epoch: 6| Step: 13
Training loss: 2.2442166805267334
Validation loss: 1.9860869863981843

Epoch: 293| Step: 0
Training loss: 1.6050009727478027
Validation loss: 2.006396092394347

Epoch: 6| Step: 1
Training loss: 1.5213146209716797
Validation loss: 1.9926567641637658

Epoch: 6| Step: 2
Training loss: 1.8138771057128906
Validation loss: 2.0082207187529533

Epoch: 6| Step: 3
Training loss: 1.9485437870025635
Validation loss: 2.0261122975298154

Epoch: 6| Step: 4
Training loss: 1.696196436882019
Validation loss: 2.0738910064902356

Epoch: 6| Step: 5
Training loss: 2.0186400413513184
Validation loss: 2.112808955613003

Epoch: 6| Step: 6
Training loss: 1.4099396467208862
Validation loss: 2.1166556496773996

Epoch: 6| Step: 7
Training loss: 1.8841609954833984
Validation loss: 2.09077234806553

Epoch: 6| Step: 8
Training loss: 1.5841195583343506
Validation loss: 2.0844051440556846

Epoch: 6| Step: 9
Training loss: 1.9944789409637451
Validation loss: 2.0684078457534953

Epoch: 6| Step: 10
Training loss: 2.0220515727996826
Validation loss: 2.0455886394746843

Epoch: 6| Step: 11
Training loss: 1.7670471668243408
Validation loss: 2.040479770270727

Epoch: 6| Step: 12
Training loss: 1.9220688343048096
Validation loss: 2.0379962305868826

Epoch: 6| Step: 13
Training loss: 2.124424695968628
Validation loss: 2.043304599741454

Epoch: 294| Step: 0
Training loss: 1.2828190326690674
Validation loss: 2.026910335786881

Epoch: 6| Step: 1
Training loss: 1.9715324640274048
Validation loss: 2.0375471615022227

Epoch: 6| Step: 2
Training loss: 1.9712028503417969
Validation loss: 2.038524555903609

Epoch: 6| Step: 3
Training loss: 1.4287793636322021
Validation loss: 2.053551986653318

Epoch: 6| Step: 4
Training loss: 1.6368634700775146
Validation loss: 2.0397890998471166

Epoch: 6| Step: 5
Training loss: 1.5578042268753052
Validation loss: 2.07741238481255

Epoch: 6| Step: 6
Training loss: 2.0819597244262695
Validation loss: 2.0611308749004076

Epoch: 6| Step: 7
Training loss: 1.602568507194519
Validation loss: 2.0692112907286613

Epoch: 6| Step: 8
Training loss: 1.900365948677063
Validation loss: 2.0601913236802623

Epoch: 6| Step: 9
Training loss: 1.9488401412963867
Validation loss: 2.047445712550994

Epoch: 6| Step: 10
Training loss: 1.9355926513671875
Validation loss: 2.034522159125215

Epoch: 6| Step: 11
Training loss: 1.6669485569000244
Validation loss: 2.027335896286913

Epoch: 6| Step: 12
Training loss: 2.1954751014709473
Validation loss: 2.035538778510145

Epoch: 6| Step: 13
Training loss: 0.8706899285316467
Validation loss: 2.073552700781053

Epoch: 295| Step: 0
Training loss: 1.2757936716079712
Validation loss: 2.0996400848511727

Epoch: 6| Step: 1
Training loss: 1.8730244636535645
Validation loss: 2.135592374750363

Epoch: 6| Step: 2
Training loss: 2.2746851444244385
Validation loss: 2.1796860541066816

Epoch: 6| Step: 3
Training loss: 1.5769107341766357
Validation loss: 2.166660983075378

Epoch: 6| Step: 4
Training loss: 1.2948142290115356
Validation loss: 2.132405011884628

Epoch: 6| Step: 5
Training loss: 1.9345470666885376
Validation loss: 2.0876261380410965

Epoch: 6| Step: 6
Training loss: 1.5057998895645142
Validation loss: 2.087852998446393

Epoch: 6| Step: 7
Training loss: 1.069122076034546
Validation loss: 2.040116387028848

Epoch: 6| Step: 8
Training loss: 1.94418466091156
Validation loss: 2.0470305181318715

Epoch: 6| Step: 9
Training loss: 1.457693338394165
Validation loss: 2.0603813894333376

Epoch: 6| Step: 10
Training loss: 2.2666468620300293
Validation loss: 2.0383673303870746

Epoch: 6| Step: 11
Training loss: 1.8275258541107178
Validation loss: 2.0117726543898224

Epoch: 6| Step: 12
Training loss: 2.097844123840332
Validation loss: 2.0386154420914187

Epoch: 6| Step: 13
Training loss: 2.3269855976104736
Validation loss: 2.041407779980731

Epoch: 296| Step: 0
Training loss: 2.0639586448669434
Validation loss: 2.050718538222774

Epoch: 6| Step: 1
Training loss: 0.9985555410385132
Validation loss: 2.0620079694255704

Epoch: 6| Step: 2
Training loss: 2.3076369762420654
Validation loss: 2.0876074619190668

Epoch: 6| Step: 3
Training loss: 1.2602449655532837
Validation loss: 2.128008762995402

Epoch: 6| Step: 4
Training loss: 1.776911973953247
Validation loss: 2.1385200638924875

Epoch: 6| Step: 5
Training loss: 2.0773754119873047
Validation loss: 2.1502067478754188

Epoch: 6| Step: 6
Training loss: 1.6212959289550781
Validation loss: 2.13772222047211

Epoch: 6| Step: 7
Training loss: 2.0243568420410156
Validation loss: 2.1357876818667174

Epoch: 6| Step: 8
Training loss: 1.4326248168945312
Validation loss: 2.0991334594706053

Epoch: 6| Step: 9
Training loss: 1.9597631692886353
Validation loss: 2.0907556933741414

Epoch: 6| Step: 10
Training loss: 1.9531974792480469
Validation loss: 2.048715542721492

Epoch: 6| Step: 11
Training loss: 1.6333845853805542
Validation loss: 2.0379724746109336

Epoch: 6| Step: 12
Training loss: 1.7767221927642822
Validation loss: 2.014586299978277

Epoch: 6| Step: 13
Training loss: 1.8477952480316162
Validation loss: 1.988420332631757

Epoch: 297| Step: 0
Training loss: 1.4819190502166748
Validation loss: 2.0057366496773175

Epoch: 6| Step: 1
Training loss: 1.5861923694610596
Validation loss: 1.9931545565205235

Epoch: 6| Step: 2
Training loss: 1.8293620347976685
Validation loss: 2.0172099349319295

Epoch: 6| Step: 3
Training loss: 1.5894806385040283
Validation loss: 2.011563571550513

Epoch: 6| Step: 4
Training loss: 1.9869881868362427
Validation loss: 2.0400246958578787

Epoch: 6| Step: 5
Training loss: 1.8111070394515991
Validation loss: 2.046822768385692

Epoch: 6| Step: 6
Training loss: 2.2559127807617188
Validation loss: 2.080420471006824

Epoch: 6| Step: 7
Training loss: 1.7679311037063599
Validation loss: 2.092610550183122

Epoch: 6| Step: 8
Training loss: 1.8057175874710083
Validation loss: 2.1009368537574686

Epoch: 6| Step: 9
Training loss: 1.3706982135772705
Validation loss: 2.0940501228455575

Epoch: 6| Step: 10
Training loss: 1.9627925157546997
Validation loss: 2.0740319067432034

Epoch: 6| Step: 11
Training loss: 1.817570686340332
Validation loss: 2.076970097839191

Epoch: 6| Step: 12
Training loss: 1.065688133239746
Validation loss: 2.064008266695084

Epoch: 6| Step: 13
Training loss: 2.5801167488098145
Validation loss: 2.0295784306782547

Epoch: 298| Step: 0
Training loss: 1.1552056074142456
Validation loss: 2.0252936129928916

Epoch: 6| Step: 1
Training loss: 1.8494343757629395
Validation loss: 2.01401884325089

Epoch: 6| Step: 2
Training loss: 2.2337441444396973
Validation loss: 2.0146420732621224

Epoch: 6| Step: 3
Training loss: 1.9842944145202637
Validation loss: 2.0016925206748386

Epoch: 6| Step: 4
Training loss: 1.5830614566802979
Validation loss: 2.0142349261109547

Epoch: 6| Step: 5
Training loss: 2.0011911392211914
Validation loss: 1.983497783701907

Epoch: 6| Step: 6
Training loss: 1.1723310947418213
Validation loss: 2.028684603270664

Epoch: 6| Step: 7
Training loss: 1.5700058937072754
Validation loss: 2.0695473032612957

Epoch: 6| Step: 8
Training loss: 1.8936457633972168
Validation loss: 2.085282885900108

Epoch: 6| Step: 9
Training loss: 1.9814094305038452
Validation loss: 2.198199397774153

Epoch: 6| Step: 10
Training loss: 1.8914272785186768
Validation loss: 2.2194175643305623

Epoch: 6| Step: 11
Training loss: 1.7400249242782593
Validation loss: 2.2310749125737015

Epoch: 6| Step: 12
Training loss: 1.9760479927062988
Validation loss: 2.223487038766184

Epoch: 6| Step: 13
Training loss: 1.5128324031829834
Validation loss: 2.1413645026504353

Epoch: 299| Step: 0
Training loss: 1.8136346340179443
Validation loss: 2.10253918299111

Epoch: 6| Step: 1
Training loss: 1.6644864082336426
Validation loss: 2.065671137584153

Epoch: 6| Step: 2
Training loss: 1.1612237691879272
Validation loss: 2.037990439322687

Epoch: 6| Step: 3
Training loss: 1.0284132957458496
Validation loss: 2.0087372103045062

Epoch: 6| Step: 4
Training loss: 1.7665413618087769
Validation loss: 1.9845521642315773

Epoch: 6| Step: 5
Training loss: 2.455921173095703
Validation loss: 1.9888952214230773

Epoch: 6| Step: 6
Training loss: 1.6062062978744507
Validation loss: 1.9884179804914741

Epoch: 6| Step: 7
Training loss: 2.1345906257629395
Validation loss: 2.036447053314537

Epoch: 6| Step: 8
Training loss: 2.158263921737671
Validation loss: 2.0535414757267123

Epoch: 6| Step: 9
Training loss: 1.5669827461242676
Validation loss: 2.0620707478574527

Epoch: 6| Step: 10
Training loss: 1.7955129146575928
Validation loss: 2.1387303824065835

Epoch: 6| Step: 11
Training loss: 1.8134210109710693
Validation loss: 2.167686708511845

Epoch: 6| Step: 12
Training loss: 1.8178783655166626
Validation loss: 2.155548475121939

Epoch: 6| Step: 13
Training loss: 1.947283387184143
Validation loss: 2.125975283243323

Epoch: 300| Step: 0
Training loss: 1.5949734449386597
Validation loss: 2.0919994949012675

Epoch: 6| Step: 1
Training loss: 1.877570629119873
Validation loss: 2.066307193489485

Epoch: 6| Step: 2
Training loss: 1.921032190322876
Validation loss: 2.052741967221742

Epoch: 6| Step: 3
Training loss: 2.4011008739471436
Validation loss: 2.0184493346880843

Epoch: 6| Step: 4
Training loss: 1.484114646911621
Validation loss: 2.009125860788489

Epoch: 6| Step: 5
Training loss: 1.3804047107696533
Validation loss: 1.996999220181537

Epoch: 6| Step: 6
Training loss: 1.73740816116333
Validation loss: 1.992893559958345

Epoch: 6| Step: 7
Training loss: 1.5470130443572998
Validation loss: 2.0159264841387348

Epoch: 6| Step: 8
Training loss: 1.9443690776824951
Validation loss: 2.031380957172763

Epoch: 6| Step: 9
Training loss: 1.4464821815490723
Validation loss: 2.0491415480131745

Epoch: 6| Step: 10
Training loss: 1.4587154388427734
Validation loss: 2.064588833880681

Epoch: 6| Step: 11
Training loss: 2.340155601501465
Validation loss: 2.089793569298201

Epoch: 6| Step: 12
Training loss: 1.454193353652954
Validation loss: 2.0833900795188

Epoch: 6| Step: 13
Training loss: 1.3884176015853882
Validation loss: 2.110647532247728

Testing loss: 2.2897875573900013

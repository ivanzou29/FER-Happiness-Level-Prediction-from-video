Epoch: 1| Step: 0
Training loss: 4.7265305519104
Validation loss: 5.219715708045549

Epoch: 6| Step: 1
Training loss: 4.8143720626831055
Validation loss: 5.213070269553892

Epoch: 6| Step: 2
Training loss: 5.306804180145264
Validation loss: 5.206422354585381

Epoch: 6| Step: 3
Training loss: 6.4357452392578125
Validation loss: 5.199805941632999

Epoch: 6| Step: 4
Training loss: 4.10391902923584
Validation loss: 5.1928578910007275

Epoch: 6| Step: 5
Training loss: 4.597757339477539
Validation loss: 5.18510615441107

Epoch: 6| Step: 6
Training loss: 3.994899272918701
Validation loss: 5.177080359510196

Epoch: 6| Step: 7
Training loss: 4.277791976928711
Validation loss: 5.1678239607041885

Epoch: 6| Step: 8
Training loss: 5.742981910705566
Validation loss: 5.15761677424113

Epoch: 6| Step: 9
Training loss: 5.515544891357422
Validation loss: 5.145891481830228

Epoch: 6| Step: 10
Training loss: 4.166289329528809
Validation loss: 5.133224635995845

Epoch: 6| Step: 11
Training loss: 5.001173496246338
Validation loss: 5.118825004946801

Epoch: 6| Step: 12
Training loss: 5.5736284255981445
Validation loss: 5.103959473230505

Epoch: 6| Step: 13
Training loss: 5.377898693084717
Validation loss: 5.086975292492938

Epoch: 2| Step: 0
Training loss: 4.45247745513916
Validation loss: 5.0688726722553215

Epoch: 6| Step: 1
Training loss: 4.793058395385742
Validation loss: 5.049233610912036

Epoch: 6| Step: 2
Training loss: 3.764857292175293
Validation loss: 5.028437645204606

Epoch: 6| Step: 3
Training loss: 6.7401251792907715
Validation loss: 5.006006025498913

Epoch: 6| Step: 4
Training loss: 5.61824369430542
Validation loss: 4.981722078015728

Epoch: 6| Step: 5
Training loss: 6.567058086395264
Validation loss: 4.956204255421956

Epoch: 6| Step: 6
Training loss: 3.5898561477661133
Validation loss: 4.929314018577657

Epoch: 6| Step: 7
Training loss: 4.599258899688721
Validation loss: 4.900301056523477

Epoch: 6| Step: 8
Training loss: 3.7534093856811523
Validation loss: 4.869396691681237

Epoch: 6| Step: 9
Training loss: 3.372148036956787
Validation loss: 4.837987838252898

Epoch: 6| Step: 10
Training loss: 3.6525306701660156
Validation loss: 4.803773364713115

Epoch: 6| Step: 11
Training loss: 5.842851638793945
Validation loss: 4.768794023862449

Epoch: 6| Step: 12
Training loss: 4.447403907775879
Validation loss: 4.7324492239183

Epoch: 6| Step: 13
Training loss: 4.417027950286865
Validation loss: 4.6954605912649505

Epoch: 3| Step: 0
Training loss: 4.216589450836182
Validation loss: 4.658617901545699

Epoch: 6| Step: 1
Training loss: 3.821735143661499
Validation loss: 4.618243073904386

Epoch: 6| Step: 2
Training loss: 5.051364421844482
Validation loss: 4.578251059337329

Epoch: 6| Step: 3
Training loss: 4.88527250289917
Validation loss: 4.535658682546308

Epoch: 6| Step: 4
Training loss: 4.218779563903809
Validation loss: 4.4933364775873

Epoch: 6| Step: 5
Training loss: 4.29201602935791
Validation loss: 4.4501775823613645

Epoch: 6| Step: 6
Training loss: 3.7889761924743652
Validation loss: 4.406813762521231

Epoch: 6| Step: 7
Training loss: 4.20853328704834
Validation loss: 4.363341787809967

Epoch: 6| Step: 8
Training loss: 2.923112630844116
Validation loss: 4.32020926731889

Epoch: 6| Step: 9
Training loss: 5.153534412384033
Validation loss: 4.278766401352421

Epoch: 6| Step: 10
Training loss: 3.6111838817596436
Validation loss: 4.237385506271034

Epoch: 6| Step: 11
Training loss: 4.19141149520874
Validation loss: 4.197420197148477

Epoch: 6| Step: 12
Training loss: 3.575871706008911
Validation loss: 4.157102138765397

Epoch: 6| Step: 13
Training loss: 5.332918167114258
Validation loss: 4.117152865215014

Epoch: 4| Step: 0
Training loss: 4.103128433227539
Validation loss: 4.077601555855043

Epoch: 6| Step: 1
Training loss: 3.8684287071228027
Validation loss: 4.036956433326967

Epoch: 6| Step: 2
Training loss: 3.234503984451294
Validation loss: 3.9962313739202355

Epoch: 6| Step: 3
Training loss: 3.1857619285583496
Validation loss: 3.9600593095184653

Epoch: 6| Step: 4
Training loss: 3.9252407550811768
Validation loss: 3.9239631622068343

Epoch: 6| Step: 5
Training loss: 3.528195381164551
Validation loss: 3.889654472310056

Epoch: 6| Step: 6
Training loss: 3.1366310119628906
Validation loss: 3.8565522598963913

Epoch: 6| Step: 7
Training loss: 4.961969375610352
Validation loss: 3.821872075398763

Epoch: 6| Step: 8
Training loss: 3.460165023803711
Validation loss: 3.7858778635660806

Epoch: 6| Step: 9
Training loss: 3.926527261734009
Validation loss: 3.753029787412254

Epoch: 6| Step: 10
Training loss: 3.377901792526245
Validation loss: 3.7232653710149948

Epoch: 6| Step: 11
Training loss: 3.461343288421631
Validation loss: 3.699792610701694

Epoch: 6| Step: 12
Training loss: 4.277731418609619
Validation loss: 3.6801915117489394

Epoch: 6| Step: 13
Training loss: 3.667552947998047
Validation loss: 3.660347241227345

Epoch: 5| Step: 0
Training loss: 3.5862550735473633
Validation loss: 3.639918329895184

Epoch: 6| Step: 1
Training loss: 2.6486916542053223
Validation loss: 3.620271575066351

Epoch: 6| Step: 2
Training loss: 3.109898805618286
Validation loss: 3.59638867070598

Epoch: 6| Step: 3
Training loss: 3.590939998626709
Validation loss: 3.576189689738776

Epoch: 6| Step: 4
Training loss: 3.698054075241089
Validation loss: 3.553103716142716

Epoch: 6| Step: 5
Training loss: 3.10500431060791
Validation loss: 3.5354147008670274

Epoch: 6| Step: 6
Training loss: 3.9652099609375
Validation loss: 3.5150514289896977

Epoch: 6| Step: 7
Training loss: 4.01895809173584
Validation loss: 3.5044448170610654

Epoch: 6| Step: 8
Training loss: 3.1239089965820312
Validation loss: 3.4899742090573875

Epoch: 6| Step: 9
Training loss: 3.612936496734619
Validation loss: 3.4804256372554327

Epoch: 6| Step: 10
Training loss: 3.208449363708496
Validation loss: 3.4690684503124607

Epoch: 6| Step: 11
Training loss: 3.8779263496398926
Validation loss: 3.4568818307692006

Epoch: 6| Step: 12
Training loss: 3.5578200817108154
Validation loss: 3.446548802878267

Epoch: 6| Step: 13
Training loss: 3.1872220039367676
Validation loss: 3.4337501500242498

Epoch: 6| Step: 0
Training loss: 2.2354776859283447
Validation loss: 3.425992727279663

Epoch: 6| Step: 1
Training loss: 3.4389727115631104
Validation loss: 3.4222435130867908

Epoch: 6| Step: 2
Training loss: 2.60916805267334
Validation loss: 3.4195606964890675

Epoch: 6| Step: 3
Training loss: 2.6708903312683105
Validation loss: 3.41368019452659

Epoch: 6| Step: 4
Training loss: 3.1887741088867188
Validation loss: 3.411703678869432

Epoch: 6| Step: 5
Training loss: 2.795814037322998
Validation loss: 3.4001560262454453

Epoch: 6| Step: 6
Training loss: 4.950681686401367
Validation loss: 3.393987963276525

Epoch: 6| Step: 7
Training loss: 3.1682631969451904
Validation loss: 3.3856021409393637

Epoch: 6| Step: 8
Training loss: 4.416928768157959
Validation loss: 3.377090738665673

Epoch: 6| Step: 9
Training loss: 4.594852447509766
Validation loss: 3.3632731847865607

Epoch: 6| Step: 10
Training loss: 2.4243340492248535
Validation loss: 3.3540824485081497

Epoch: 6| Step: 11
Training loss: 3.446053981781006
Validation loss: 3.344355506281699

Epoch: 6| Step: 12
Training loss: 3.0773181915283203
Validation loss: 3.334125223980155

Epoch: 6| Step: 13
Training loss: 3.8096461296081543
Validation loss: 3.326532586928337

Epoch: 7| Step: 0
Training loss: 3.1388964653015137
Validation loss: 3.312161389217582

Epoch: 6| Step: 1
Training loss: 3.3170297145843506
Validation loss: 3.3061506722563054

Epoch: 6| Step: 2
Training loss: 3.814517021179199
Validation loss: 3.302657409380841

Epoch: 6| Step: 3
Training loss: 2.895425796508789
Validation loss: 3.3044728053513395

Epoch: 6| Step: 4
Training loss: 2.8412368297576904
Validation loss: 3.3000808146692093

Epoch: 6| Step: 5
Training loss: 2.824439525604248
Validation loss: 3.28283856761071

Epoch: 6| Step: 6
Training loss: 3.453071355819702
Validation loss: 3.270288734025853

Epoch: 6| Step: 7
Training loss: 3.034273147583008
Validation loss: 3.2597346023846696

Epoch: 6| Step: 8
Training loss: 3.6032485961914062
Validation loss: 3.260142103318245

Epoch: 6| Step: 9
Training loss: 2.6998050212860107
Validation loss: 3.2563035565037883

Epoch: 6| Step: 10
Training loss: 3.267594575881958
Validation loss: 3.2501035608271116

Epoch: 6| Step: 11
Training loss: 4.168483257293701
Validation loss: 3.2411755259319017

Epoch: 6| Step: 12
Training loss: 3.214482307434082
Validation loss: 3.234704012511879

Epoch: 6| Step: 13
Training loss: 3.1696431636810303
Validation loss: 3.228425295122208

Epoch: 8| Step: 0
Training loss: 2.4251961708068848
Validation loss: 3.2259897237182944

Epoch: 6| Step: 1
Training loss: 4.247191429138184
Validation loss: 3.218095746091617

Epoch: 6| Step: 2
Training loss: 3.216035842895508
Validation loss: 3.2115555911935787

Epoch: 6| Step: 3
Training loss: 3.124729633331299
Validation loss: 3.2040522611269386

Epoch: 6| Step: 4
Training loss: 3.791130542755127
Validation loss: 3.1969229713562997

Epoch: 6| Step: 5
Training loss: 3.0620343685150146
Validation loss: 3.1886147017120035

Epoch: 6| Step: 6
Training loss: 2.9652819633483887
Validation loss: 3.1824620436596613

Epoch: 6| Step: 7
Training loss: 3.5047543048858643
Validation loss: 3.179686430961855

Epoch: 6| Step: 8
Training loss: 3.935892105102539
Validation loss: 3.1726049710345525

Epoch: 6| Step: 9
Training loss: 3.0940191745758057
Validation loss: 3.16847623291836

Epoch: 6| Step: 10
Training loss: 2.1262869834899902
Validation loss: 3.1697857046640046

Epoch: 6| Step: 11
Training loss: 2.9617199897766113
Validation loss: 3.1578474967710433

Epoch: 6| Step: 12
Training loss: 2.6994950771331787
Validation loss: 3.1555560686255015

Epoch: 6| Step: 13
Training loss: 3.633995532989502
Validation loss: 3.157536486143707

Epoch: 9| Step: 0
Training loss: 2.341975450515747
Validation loss: 3.1530635792721986

Epoch: 6| Step: 1
Training loss: 3.071688175201416
Validation loss: 3.1511821567371325

Epoch: 6| Step: 2
Training loss: 3.1508185863494873
Validation loss: 3.1469057426657727

Epoch: 6| Step: 3
Training loss: 3.2351579666137695
Validation loss: 3.138911436962825

Epoch: 6| Step: 4
Training loss: 2.5251624584198
Validation loss: 3.1293050217372116

Epoch: 6| Step: 5
Training loss: 3.126796245574951
Validation loss: 3.1269786511698077

Epoch: 6| Step: 6
Training loss: 2.758312225341797
Validation loss: 3.117041177647088

Epoch: 6| Step: 7
Training loss: 3.101973056793213
Validation loss: 3.1115776287612094

Epoch: 6| Step: 8
Training loss: 4.403689861297607
Validation loss: 3.111168681934316

Epoch: 6| Step: 9
Training loss: 2.8664064407348633
Validation loss: 3.101841595865065

Epoch: 6| Step: 10
Training loss: 4.096553802490234
Validation loss: 3.1052715239986295

Epoch: 6| Step: 11
Training loss: 3.6361942291259766
Validation loss: 3.0980497816557526

Epoch: 6| Step: 12
Training loss: 2.3323607444763184
Validation loss: 3.091320353169595

Epoch: 6| Step: 13
Training loss: 3.4521777629852295
Validation loss: 3.0904938354287097

Epoch: 10| Step: 0
Training loss: 2.79679536819458
Validation loss: 3.090384537173856

Epoch: 6| Step: 1
Training loss: 2.027204990386963
Validation loss: 3.084547876029886

Epoch: 6| Step: 2
Training loss: 3.850221633911133
Validation loss: 3.0801600538274294

Epoch: 6| Step: 3
Training loss: 2.971583127975464
Validation loss: 3.0759732133598736

Epoch: 6| Step: 4
Training loss: 1.864203691482544
Validation loss: 3.075513767939742

Epoch: 6| Step: 5
Training loss: 3.3283519744873047
Validation loss: 3.086255596530053

Epoch: 6| Step: 6
Training loss: 3.446187973022461
Validation loss: 3.100214942809074

Epoch: 6| Step: 7
Training loss: 3.539004325866699
Validation loss: 3.106654736303514

Epoch: 6| Step: 8
Training loss: 3.6541829109191895
Validation loss: 3.0604053492187173

Epoch: 6| Step: 9
Training loss: 3.8234763145446777
Validation loss: 3.0566578603559926

Epoch: 6| Step: 10
Training loss: 3.9698081016540527
Validation loss: 3.0597185498924664

Epoch: 6| Step: 11
Training loss: 2.9590115547180176
Validation loss: 3.0672352775450675

Epoch: 6| Step: 12
Training loss: 2.510159969329834
Validation loss: 3.0698263183716805

Epoch: 6| Step: 13
Training loss: 2.5756442546844482
Validation loss: 3.0580942605131414

Epoch: 11| Step: 0
Training loss: 2.9572765827178955
Validation loss: 3.056605692832701

Epoch: 6| Step: 1
Training loss: 2.6286747455596924
Validation loss: 3.055932157783098

Epoch: 6| Step: 2
Training loss: 2.719571113586426
Validation loss: 3.051486228101997

Epoch: 6| Step: 3
Training loss: 2.9466536045074463
Validation loss: 3.049775062068816

Epoch: 6| Step: 4
Training loss: 3.1384758949279785
Validation loss: 3.0437699953715005

Epoch: 6| Step: 5
Training loss: 3.379671335220337
Validation loss: 3.042610655548752

Epoch: 6| Step: 6
Training loss: 2.675887107849121
Validation loss: 3.041698581428938

Epoch: 6| Step: 7
Training loss: 3.632725954055786
Validation loss: 3.0388531992512364

Epoch: 6| Step: 8
Training loss: 3.914602756500244
Validation loss: 3.0331321275362404

Epoch: 6| Step: 9
Training loss: 2.627202272415161
Validation loss: 3.029936423865698

Epoch: 6| Step: 10
Training loss: 2.5312719345092773
Validation loss: 3.0227540667339037

Epoch: 6| Step: 11
Training loss: 3.1333179473876953
Validation loss: 3.0201043621186288

Epoch: 6| Step: 12
Training loss: 3.217344284057617
Validation loss: 3.016270427293675

Epoch: 6| Step: 13
Training loss: 4.110029220581055
Validation loss: 3.0155442735200286

Epoch: 12| Step: 0
Training loss: 3.159233570098877
Validation loss: 3.0188091698513237

Epoch: 6| Step: 1
Training loss: 3.8968329429626465
Validation loss: 3.024774723155524

Epoch: 6| Step: 2
Training loss: 2.8856606483459473
Validation loss: 3.0214864592398367

Epoch: 6| Step: 3
Training loss: 2.605257272720337
Validation loss: 3.0281800839208786

Epoch: 6| Step: 4
Training loss: 2.5544605255126953
Validation loss: 3.001877966747489

Epoch: 6| Step: 5
Training loss: 2.883863925933838
Validation loss: 2.999725223869406

Epoch: 6| Step: 6
Training loss: 3.393989086151123
Validation loss: 2.996741707606982

Epoch: 6| Step: 7
Training loss: 2.632317066192627
Validation loss: 2.998946702608498

Epoch: 6| Step: 8
Training loss: 2.4752089977264404
Validation loss: 3.0027652735351236

Epoch: 6| Step: 9
Training loss: 2.9103775024414062
Validation loss: 2.9984859702407674

Epoch: 6| Step: 10
Training loss: 3.0436229705810547
Validation loss: 2.996341597649359

Epoch: 6| Step: 11
Training loss: 4.052024841308594
Validation loss: 2.996333622163342

Epoch: 6| Step: 12
Training loss: 3.260444164276123
Validation loss: 2.9936703379436205

Epoch: 6| Step: 13
Training loss: 3.1861727237701416
Validation loss: 2.988689684098767

Epoch: 13| Step: 0
Training loss: 3.4900519847869873
Validation loss: 2.98059997763685

Epoch: 6| Step: 1
Training loss: 2.8854787349700928
Validation loss: 2.9778372549241587

Epoch: 6| Step: 2
Training loss: 3.1382975578308105
Validation loss: 2.974835052285143

Epoch: 6| Step: 3
Training loss: 3.6159613132476807
Validation loss: 2.9709912833347114

Epoch: 6| Step: 4
Training loss: 3.2375540733337402
Validation loss: 2.970815899551556

Epoch: 6| Step: 5
Training loss: 2.165360450744629
Validation loss: 2.9731174925322175

Epoch: 6| Step: 6
Training loss: 3.2237348556518555
Validation loss: 2.9757462214398127

Epoch: 6| Step: 7
Training loss: 2.831960678100586
Validation loss: 2.9811625839561544

Epoch: 6| Step: 8
Training loss: 3.183767795562744
Validation loss: 2.970709393101354

Epoch: 6| Step: 9
Training loss: 3.566572666168213
Validation loss: 2.965489287530222

Epoch: 6| Step: 10
Training loss: 2.35379695892334
Validation loss: 2.966928799947103

Epoch: 6| Step: 11
Training loss: 3.7694215774536133
Validation loss: 2.9668065501797583

Epoch: 6| Step: 12
Training loss: 2.072849750518799
Validation loss: 2.9654389683918287

Epoch: 6| Step: 13
Training loss: 3.1392180919647217
Validation loss: 2.9669513164028043

Epoch: 14| Step: 0
Training loss: 2.373185634613037
Validation loss: 2.969904858578918

Epoch: 6| Step: 1
Training loss: 2.069347620010376
Validation loss: 2.9689239225079938

Epoch: 6| Step: 2
Training loss: 3.3924102783203125
Validation loss: 2.973722775777181

Epoch: 6| Step: 3
Training loss: 2.303407669067383
Validation loss: 2.973238629679526

Epoch: 6| Step: 4
Training loss: 3.707183599472046
Validation loss: 2.9688050182916785

Epoch: 6| Step: 5
Training loss: 3.309948682785034
Validation loss: 2.9638666004262944

Epoch: 6| Step: 6
Training loss: 3.7484254837036133
Validation loss: 2.957839383873888

Epoch: 6| Step: 7
Training loss: 3.7181472778320312
Validation loss: 2.9529788007018385

Epoch: 6| Step: 8
Training loss: 2.9962477684020996
Validation loss: 2.95246648275724

Epoch: 6| Step: 9
Training loss: 3.3033599853515625
Validation loss: 2.9480370577945503

Epoch: 6| Step: 10
Training loss: 3.2463998794555664
Validation loss: 2.9463009244652203

Epoch: 6| Step: 11
Training loss: 3.097036838531494
Validation loss: 2.9400845881431334

Epoch: 6| Step: 12
Training loss: 1.8788378238677979
Validation loss: 2.9398559549803376

Epoch: 6| Step: 13
Training loss: 3.5145111083984375
Validation loss: 2.9350051726064375

Epoch: 15| Step: 0
Training loss: 2.81725811958313
Validation loss: 2.9313778864440097

Epoch: 6| Step: 1
Training loss: 2.8055500984191895
Validation loss: 2.935474326533656

Epoch: 6| Step: 2
Training loss: 2.7465338706970215
Validation loss: 2.958543946666102

Epoch: 6| Step: 3
Training loss: 3.093446731567383
Validation loss: 2.948002599900769

Epoch: 6| Step: 4
Training loss: 3.336031436920166
Validation loss: 2.9319611518613753

Epoch: 6| Step: 5
Training loss: 3.6758317947387695
Validation loss: 2.9310215365502144

Epoch: 6| Step: 6
Training loss: 2.3537678718566895
Validation loss: 2.926905116727275

Epoch: 6| Step: 7
Training loss: 3.2643253803253174
Validation loss: 2.9224838210690405

Epoch: 6| Step: 8
Training loss: 2.9914121627807617
Validation loss: 2.9210625848462506

Epoch: 6| Step: 9
Training loss: 2.5038809776306152
Validation loss: 2.9149180714802077

Epoch: 6| Step: 10
Training loss: 3.4900429248809814
Validation loss: 2.913686519028038

Epoch: 6| Step: 11
Training loss: 3.1619527339935303
Validation loss: 2.9098692504308556

Epoch: 6| Step: 12
Training loss: 3.0616211891174316
Validation loss: 2.906446326163507

Epoch: 6| Step: 13
Training loss: 2.555695056915283
Validation loss: 2.900684946326799

Epoch: 16| Step: 0
Training loss: 3.742335319519043
Validation loss: 2.9019975457140195

Epoch: 6| Step: 1
Training loss: 3.81750226020813
Validation loss: 2.8938792777317826

Epoch: 6| Step: 2
Training loss: 2.384289026260376
Validation loss: 2.8928410545472176

Epoch: 6| Step: 3
Training loss: 2.766957998275757
Validation loss: 2.8892294668382212

Epoch: 6| Step: 4
Training loss: 3.2519054412841797
Validation loss: 2.889338844565935

Epoch: 6| Step: 5
Training loss: 2.72902250289917
Validation loss: 2.88931437717971

Epoch: 6| Step: 6
Training loss: 2.639922618865967
Validation loss: 2.88813187742746

Epoch: 6| Step: 7
Training loss: 3.1554276943206787
Validation loss: 2.885730738280922

Epoch: 6| Step: 8
Training loss: 3.9655466079711914
Validation loss: 2.8852765329422487

Epoch: 6| Step: 9
Training loss: 1.931154727935791
Validation loss: 2.8874936642185336

Epoch: 6| Step: 10
Training loss: 3.2458066940307617
Validation loss: 2.880430239503102

Epoch: 6| Step: 11
Training loss: 3.2080960273742676
Validation loss: 2.8780719798098326

Epoch: 6| Step: 12
Training loss: 2.6948275566101074
Validation loss: 2.876342070999966

Epoch: 6| Step: 13
Training loss: 1.657342791557312
Validation loss: 2.8771553936825005

Epoch: 17| Step: 0
Training loss: 2.309713363647461
Validation loss: 2.876796753175797

Epoch: 6| Step: 1
Training loss: 2.8973898887634277
Validation loss: 2.87878131610091

Epoch: 6| Step: 2
Training loss: 2.9209885597229004
Validation loss: 2.8721077698533253

Epoch: 6| Step: 3
Training loss: 3.690670967102051
Validation loss: 2.869493738297493

Epoch: 6| Step: 4
Training loss: 2.9786453247070312
Validation loss: 2.864612166599561

Epoch: 6| Step: 5
Training loss: 2.604004383087158
Validation loss: 2.864421380463467

Epoch: 6| Step: 6
Training loss: 2.7195639610290527
Validation loss: 2.872620254434565

Epoch: 6| Step: 7
Training loss: 2.991682291030884
Validation loss: 2.8759046831438617

Epoch: 6| Step: 8
Training loss: 3.016857147216797
Validation loss: 2.874116087472567

Epoch: 6| Step: 9
Training loss: 2.9909744262695312
Validation loss: 2.8656964609699864

Epoch: 6| Step: 10
Training loss: 3.465305805206299
Validation loss: 2.8651996351057485

Epoch: 6| Step: 11
Training loss: 2.7781171798706055
Validation loss: 2.864067967220019

Epoch: 6| Step: 12
Training loss: 3.3178062438964844
Validation loss: 2.8599245214975006

Epoch: 6| Step: 13
Training loss: 2.7120561599731445
Validation loss: 2.8524492735503824

Epoch: 18| Step: 0
Training loss: 2.790311336517334
Validation loss: 2.8510770438819804

Epoch: 6| Step: 1
Training loss: 3.260288715362549
Validation loss: 2.852805696507936

Epoch: 6| Step: 2
Training loss: 3.3758320808410645
Validation loss: 2.854365482125231

Epoch: 6| Step: 3
Training loss: 2.971571922302246
Validation loss: 2.8532305122703634

Epoch: 6| Step: 4
Training loss: 2.4198453426361084
Validation loss: 2.8525731589204524

Epoch: 6| Step: 5
Training loss: 3.22481107711792
Validation loss: 2.848737834602274

Epoch: 6| Step: 6
Training loss: 3.090977907180786
Validation loss: 2.843828870404151

Epoch: 6| Step: 7
Training loss: 2.9710259437561035
Validation loss: 2.841967136629166

Epoch: 6| Step: 8
Training loss: 3.237457275390625
Validation loss: 2.8422387261544504

Epoch: 6| Step: 9
Training loss: 3.2381203174591064
Validation loss: 2.8395904699961343

Epoch: 6| Step: 10
Training loss: 3.3962912559509277
Validation loss: 2.8370308081309

Epoch: 6| Step: 11
Training loss: 2.837155342102051
Validation loss: 2.836111309707806

Epoch: 6| Step: 12
Training loss: 1.766471028327942
Validation loss: 2.84244187929297

Epoch: 6| Step: 13
Training loss: 2.6077489852905273
Validation loss: 2.8422099287791918

Epoch: 19| Step: 0
Training loss: 4.61463737487793
Validation loss: 2.8431401329655803

Epoch: 6| Step: 1
Training loss: 2.473031520843506
Validation loss: 2.8454833517792406

Epoch: 6| Step: 2
Training loss: 1.647797703742981
Validation loss: 2.8465338701842935

Epoch: 6| Step: 3
Training loss: 2.353710174560547
Validation loss: 2.843743126879456

Epoch: 6| Step: 4
Training loss: 2.60951566696167
Validation loss: 2.838834701045867

Epoch: 6| Step: 5
Training loss: 2.978524923324585
Validation loss: 2.8409072276084655

Epoch: 6| Step: 6
Training loss: 2.801973342895508
Validation loss: 2.843021954259565

Epoch: 6| Step: 7
Training loss: 3.1779439449310303
Validation loss: 2.8404311569788123

Epoch: 6| Step: 8
Training loss: 3.304670810699463
Validation loss: 2.8487322791930167

Epoch: 6| Step: 9
Training loss: 3.146022319793701
Validation loss: 2.849055000530776

Epoch: 6| Step: 10
Training loss: 3.106661319732666
Validation loss: 2.841489809815602

Epoch: 6| Step: 11
Training loss: 3.71028470993042
Validation loss: 2.8409466487105175

Epoch: 6| Step: 12
Training loss: 2.7856578826904297
Validation loss: 2.8412852569292952

Epoch: 6| Step: 13
Training loss: 2.2883520126342773
Validation loss: 2.8336650453588015

Epoch: 20| Step: 0
Training loss: 3.3617660999298096
Validation loss: 2.8327575217011156

Epoch: 6| Step: 1
Training loss: 2.956434965133667
Validation loss: 2.830545130596366

Epoch: 6| Step: 2
Training loss: 2.3167197704315186
Validation loss: 2.8234855487782466

Epoch: 6| Step: 3
Training loss: 2.3363406658172607
Validation loss: 2.829145634046165

Epoch: 6| Step: 4
Training loss: 2.7677738666534424
Validation loss: 2.828723358851607

Epoch: 6| Step: 5
Training loss: 3.9271068572998047
Validation loss: 2.823229976879653

Epoch: 6| Step: 6
Training loss: 2.4177443981170654
Validation loss: 2.8234488823080577

Epoch: 6| Step: 7
Training loss: 3.412010669708252
Validation loss: 2.8219544682451474

Epoch: 6| Step: 8
Training loss: 2.879977226257324
Validation loss: 2.8208739039718465

Epoch: 6| Step: 9
Training loss: 3.254183769226074
Validation loss: 2.8210486519721245

Epoch: 6| Step: 10
Training loss: 2.813723564147949
Validation loss: 2.8189146672525713

Epoch: 6| Step: 11
Training loss: 3.276728868484497
Validation loss: 2.8206498648530696

Epoch: 6| Step: 12
Training loss: 2.754382610321045
Validation loss: 2.8172543510313957

Epoch: 6| Step: 13
Training loss: 2.546565294265747
Validation loss: 2.8193675061707855

Epoch: 21| Step: 0
Training loss: 2.3906068801879883
Validation loss: 2.8185677400199314

Epoch: 6| Step: 1
Training loss: 3.00754451751709
Validation loss: 2.826230823352773

Epoch: 6| Step: 2
Training loss: 2.57891845703125
Validation loss: 2.8320125610597673

Epoch: 6| Step: 3
Training loss: 2.631958484649658
Validation loss: 2.8603232701619468

Epoch: 6| Step: 4
Training loss: 3.401540517807007
Validation loss: 2.8357131199170182

Epoch: 6| Step: 5
Training loss: 3.176478862762451
Validation loss: 2.8212932309796734

Epoch: 6| Step: 6
Training loss: 3.4787487983703613
Validation loss: 2.8095291353041127

Epoch: 6| Step: 7
Training loss: 3.030097723007202
Validation loss: 2.8083666986034763

Epoch: 6| Step: 8
Training loss: 3.0577657222747803
Validation loss: 2.8097490931069977

Epoch: 6| Step: 9
Training loss: 3.601612091064453
Validation loss: 2.809654064075921

Epoch: 6| Step: 10
Training loss: 2.6316065788269043
Validation loss: 2.810594815079884

Epoch: 6| Step: 11
Training loss: 2.022469997406006
Validation loss: 2.814021720681139

Epoch: 6| Step: 12
Training loss: 2.8467986583709717
Validation loss: 2.8145864445676088

Epoch: 6| Step: 13
Training loss: 3.5241682529449463
Validation loss: 2.812769374539775

Epoch: 22| Step: 0
Training loss: 3.0511550903320312
Validation loss: 2.8061209365885746

Epoch: 6| Step: 1
Training loss: 2.559596061706543
Validation loss: 2.8061997993018037

Epoch: 6| Step: 2
Training loss: 1.856027364730835
Validation loss: 2.8044111497940554

Epoch: 6| Step: 3
Training loss: 3.1302261352539062
Validation loss: 2.7991829046639065

Epoch: 6| Step: 4
Training loss: 2.6848349571228027
Validation loss: 2.8044055943847983

Epoch: 6| Step: 5
Training loss: 2.9109349250793457
Validation loss: 2.815205822708786

Epoch: 6| Step: 6
Training loss: 2.7874841690063477
Validation loss: 2.8120204966555358

Epoch: 6| Step: 7
Training loss: 3.2137675285339355
Validation loss: 2.8081855825198594

Epoch: 6| Step: 8
Training loss: 3.1351795196533203
Validation loss: 2.8064662179639264

Epoch: 6| Step: 9
Training loss: 3.5244076251983643
Validation loss: 2.8018332476256997

Epoch: 6| Step: 10
Training loss: 2.5688154697418213
Validation loss: 2.7973859258877334

Epoch: 6| Step: 11
Training loss: 2.858105421066284
Validation loss: 2.7954733320461806

Epoch: 6| Step: 12
Training loss: 3.78165602684021
Validation loss: 2.789184134493592

Epoch: 6| Step: 13
Training loss: 2.880119800567627
Validation loss: 2.793639034353277

Epoch: 23| Step: 0
Training loss: 2.362290382385254
Validation loss: 2.8036818504333496

Epoch: 6| Step: 1
Training loss: 3.1058616638183594
Validation loss: 2.8160738099005913

Epoch: 6| Step: 2
Training loss: 2.625201463699341
Validation loss: 2.8044376296381794

Epoch: 6| Step: 3
Training loss: 3.6878955364227295
Validation loss: 2.799621210303358

Epoch: 6| Step: 4
Training loss: 2.463249921798706
Validation loss: 2.7936665063263266

Epoch: 6| Step: 5
Training loss: 3.0596911907196045
Validation loss: 2.791169276801489

Epoch: 6| Step: 6
Training loss: 3.2281627655029297
Validation loss: 2.789611621569562

Epoch: 6| Step: 7
Training loss: 3.3325717449188232
Validation loss: 2.793409926916963

Epoch: 6| Step: 8
Training loss: 3.224738597869873
Validation loss: 2.7988214467161443

Epoch: 6| Step: 9
Training loss: 3.1586155891418457
Validation loss: 2.794194039478097

Epoch: 6| Step: 10
Training loss: 2.52178955078125
Validation loss: 2.7955236870755433

Epoch: 6| Step: 11
Training loss: 3.295900344848633
Validation loss: 2.7997812327518257

Epoch: 6| Step: 12
Training loss: 2.1165919303894043
Validation loss: 2.803495950596307

Epoch: 6| Step: 13
Training loss: 2.5272703170776367
Validation loss: 2.8044221119214128

Epoch: 24| Step: 0
Training loss: 2.800952911376953
Validation loss: 2.8182950096745647

Epoch: 6| Step: 1
Training loss: 2.7358014583587646
Validation loss: 2.808289558656754

Epoch: 6| Step: 2
Training loss: 2.604461431503296
Validation loss: 2.8064467701860654

Epoch: 6| Step: 3
Training loss: 1.9033753871917725
Validation loss: 2.802531114188574

Epoch: 6| Step: 4
Training loss: 3.1781997680664062
Validation loss: 2.795316591057726

Epoch: 6| Step: 5
Training loss: 2.2486486434936523
Validation loss: 2.791568822758172

Epoch: 6| Step: 6
Training loss: 3.435682773590088
Validation loss: 2.7870993537287556

Epoch: 6| Step: 7
Training loss: 3.3695144653320312
Validation loss: 2.787060611991472

Epoch: 6| Step: 8
Training loss: 3.178661346435547
Validation loss: 2.784657334768644

Epoch: 6| Step: 9
Training loss: 3.1500163078308105
Validation loss: 2.781470952495452

Epoch: 6| Step: 10
Training loss: 3.2569150924682617
Validation loss: 2.784909645716349

Epoch: 6| Step: 11
Training loss: 2.798720359802246
Validation loss: 2.7856768382492887

Epoch: 6| Step: 12
Training loss: 3.7013704776763916
Validation loss: 2.7822578927522064

Epoch: 6| Step: 13
Training loss: 2.2402079105377197
Validation loss: 2.7794159971257693

Epoch: 25| Step: 0
Training loss: 3.3194563388824463
Validation loss: 2.780764490045527

Epoch: 6| Step: 1
Training loss: 3.375972270965576
Validation loss: 2.778393760804207

Epoch: 6| Step: 2
Training loss: 3.3273367881774902
Validation loss: 2.7790121186164116

Epoch: 6| Step: 3
Training loss: 3.0437560081481934
Validation loss: 2.778440749773415

Epoch: 6| Step: 4
Training loss: 2.665881633758545
Validation loss: 2.781748094866353

Epoch: 6| Step: 5
Training loss: 2.1874208450317383
Validation loss: 2.780809784448275

Epoch: 6| Step: 6
Training loss: 2.4736697673797607
Validation loss: 2.788738432750907

Epoch: 6| Step: 7
Training loss: 2.9699618816375732
Validation loss: 2.7984546538322204

Epoch: 6| Step: 8
Training loss: 3.267238140106201
Validation loss: 2.8101163961554088

Epoch: 6| Step: 9
Training loss: 2.4159045219421387
Validation loss: 2.8293511636795534

Epoch: 6| Step: 10
Training loss: 3.432112216949463
Validation loss: 2.8106603673709336

Epoch: 6| Step: 11
Training loss: 2.899484157562256
Validation loss: 2.7795739071343535

Epoch: 6| Step: 12
Training loss: 2.6858773231506348
Validation loss: 2.7652235569492465

Epoch: 6| Step: 13
Training loss: 2.544015884399414
Validation loss: 2.7653538206572175

Epoch: 26| Step: 0
Training loss: 2.630560874938965
Validation loss: 2.7737124017489854

Epoch: 6| Step: 1
Training loss: 3.2595977783203125
Validation loss: 2.7737254865707888

Epoch: 6| Step: 2
Training loss: 3.8208491802215576
Validation loss: 2.7802487829680085

Epoch: 6| Step: 3
Training loss: 2.1272106170654297
Validation loss: 2.7722190990242908

Epoch: 6| Step: 4
Training loss: 3.737297534942627
Validation loss: 2.7722834771679294

Epoch: 6| Step: 5
Training loss: 2.0764966011047363
Validation loss: 2.763393453372422

Epoch: 6| Step: 6
Training loss: 2.648808002471924
Validation loss: 2.7632641151387203

Epoch: 6| Step: 7
Training loss: 2.8959121704101562
Validation loss: 2.7615014378742506

Epoch: 6| Step: 8
Training loss: 3.4049463272094727
Validation loss: 2.764269580123245

Epoch: 6| Step: 9
Training loss: 3.198556661605835
Validation loss: 2.7654538692966586

Epoch: 6| Step: 10
Training loss: 2.3499321937561035
Validation loss: 2.765213986878754

Epoch: 6| Step: 11
Training loss: 3.183150053024292
Validation loss: 2.7609551183639036

Epoch: 6| Step: 12
Training loss: 2.0999743938446045
Validation loss: 2.755857744524556

Epoch: 6| Step: 13
Training loss: 3.6335232257843018
Validation loss: 2.7561637663072154

Epoch: 27| Step: 0
Training loss: 2.7679848670959473
Validation loss: 2.7618606449455343

Epoch: 6| Step: 1
Training loss: 3.2904272079467773
Validation loss: 2.7563083812754643

Epoch: 6| Step: 2
Training loss: 3.4823403358459473
Validation loss: 2.760150101877028

Epoch: 6| Step: 3
Training loss: 2.5635972023010254
Validation loss: 2.7681732741735314

Epoch: 6| Step: 4
Training loss: 2.9060122966766357
Validation loss: 2.7706514225211194

Epoch: 6| Step: 5
Training loss: 2.361058235168457
Validation loss: 2.773146298623854

Epoch: 6| Step: 6
Training loss: 2.9592204093933105
Validation loss: 2.7784382656056392

Epoch: 6| Step: 7
Training loss: 3.3085250854492188
Validation loss: 2.7761499958653606

Epoch: 6| Step: 8
Training loss: 2.008349657058716
Validation loss: 2.7671261474650395

Epoch: 6| Step: 9
Training loss: 2.8835158348083496
Validation loss: 2.7582104231721614

Epoch: 6| Step: 10
Training loss: 3.048412561416626
Validation loss: 2.761827638072352

Epoch: 6| Step: 11
Training loss: 2.688797950744629
Validation loss: 2.7603936990102134

Epoch: 6| Step: 12
Training loss: 3.4562973976135254
Validation loss: 2.7586632954177035

Epoch: 6| Step: 13
Training loss: 2.8624303340911865
Validation loss: 2.7578118283261537

Epoch: 28| Step: 0
Training loss: 2.762133836746216
Validation loss: 2.751463741384527

Epoch: 6| Step: 1
Training loss: 2.71166729927063
Validation loss: 2.7515599932721866

Epoch: 6| Step: 2
Training loss: 3.3322837352752686
Validation loss: 2.7524212098890737

Epoch: 6| Step: 3
Training loss: 2.204077959060669
Validation loss: 2.751025951036843

Epoch: 6| Step: 4
Training loss: 3.016991376876831
Validation loss: 2.7474852813187467

Epoch: 6| Step: 5
Training loss: 3.722141742706299
Validation loss: 2.7522351639245146

Epoch: 6| Step: 6
Training loss: 2.8690905570983887
Validation loss: 2.751234519866205

Epoch: 6| Step: 7
Training loss: 3.1234097480773926
Validation loss: 2.7517130041635163

Epoch: 6| Step: 8
Training loss: 2.797074317932129
Validation loss: 2.7519380636112665

Epoch: 6| Step: 9
Training loss: 2.420506715774536
Validation loss: 2.752987964178926

Epoch: 6| Step: 10
Training loss: 3.3808841705322266
Validation loss: 2.7493571568560857

Epoch: 6| Step: 11
Training loss: 2.4956541061401367
Validation loss: 2.7510716504948114

Epoch: 6| Step: 12
Training loss: 3.0913357734680176
Validation loss: 2.75210052920926

Epoch: 6| Step: 13
Training loss: 2.2079973220825195
Validation loss: 2.748408558548138

Epoch: 29| Step: 0
Training loss: 3.0814971923828125
Validation loss: 2.7536200118321243

Epoch: 6| Step: 1
Training loss: 1.9385814666748047
Validation loss: 2.7489408011077554

Epoch: 6| Step: 2
Training loss: 2.698843002319336
Validation loss: 2.7477822149953535

Epoch: 6| Step: 3
Training loss: 3.1238584518432617
Validation loss: 2.756360169379942

Epoch: 6| Step: 4
Training loss: 3.5550496578216553
Validation loss: 2.753701512531568

Epoch: 6| Step: 5
Training loss: 2.9274485111236572
Validation loss: 2.7551342569371706

Epoch: 6| Step: 6
Training loss: 3.22542142868042
Validation loss: 2.7512363823511268

Epoch: 6| Step: 7
Training loss: 2.9057185649871826
Validation loss: 2.75302912599297

Epoch: 6| Step: 8
Training loss: 2.5363240242004395
Validation loss: 2.746703622161701

Epoch: 6| Step: 9
Training loss: 2.838155746459961
Validation loss: 2.7425107289386053

Epoch: 6| Step: 10
Training loss: 2.3835272789001465
Validation loss: 2.7425477556003037

Epoch: 6| Step: 11
Training loss: 2.9903345108032227
Validation loss: 2.745823957586801

Epoch: 6| Step: 12
Training loss: 3.011888027191162
Validation loss: 2.7439941129376813

Epoch: 6| Step: 13
Training loss: 3.423103094100952
Validation loss: 2.7440781439504316

Epoch: 30| Step: 0
Training loss: 2.1158926486968994
Validation loss: 2.745015923694898

Epoch: 6| Step: 1
Training loss: 3.3679537773132324
Validation loss: 2.7435664925523984

Epoch: 6| Step: 2
Training loss: 2.8083677291870117
Validation loss: 2.7459270569585983

Epoch: 6| Step: 3
Training loss: 2.38617205619812
Validation loss: 2.7425697926552064

Epoch: 6| Step: 4
Training loss: 3.261775016784668
Validation loss: 2.7434104642560406

Epoch: 6| Step: 5
Training loss: 3.7340402603149414
Validation loss: 2.7429606119791665

Epoch: 6| Step: 6
Training loss: 1.9332185983657837
Validation loss: 2.7413294110246884

Epoch: 6| Step: 7
Training loss: 1.90519118309021
Validation loss: 2.7419984058667253

Epoch: 6| Step: 8
Training loss: 2.4866552352905273
Validation loss: 2.742549732167234

Epoch: 6| Step: 9
Training loss: 3.3236093521118164
Validation loss: 2.744677335985245

Epoch: 6| Step: 10
Training loss: 3.113189220428467
Validation loss: 2.74173241789623

Epoch: 6| Step: 11
Training loss: 3.827191114425659
Validation loss: 2.7411384915792816

Epoch: 6| Step: 12
Training loss: 3.2342071533203125
Validation loss: 2.740561398126746

Epoch: 6| Step: 13
Training loss: 2.6397178173065186
Validation loss: 2.743151851879653

Epoch: 31| Step: 0
Training loss: 2.4761712551116943
Validation loss: 2.7364720016397457

Epoch: 6| Step: 1
Training loss: 1.8932229280471802
Validation loss: 2.7408550682888237

Epoch: 6| Step: 2
Training loss: 3.2754125595092773
Validation loss: 2.7426098521037767

Epoch: 6| Step: 3
Training loss: 3.2593328952789307
Validation loss: 2.7371121760337584

Epoch: 6| Step: 4
Training loss: 2.4485936164855957
Validation loss: 2.7352864562824206

Epoch: 6| Step: 5
Training loss: 2.8182373046875
Validation loss: 2.736259993686471

Epoch: 6| Step: 6
Training loss: 3.055992841720581
Validation loss: 2.7331116737857943

Epoch: 6| Step: 7
Training loss: 2.7372450828552246
Validation loss: 2.7328264687650945

Epoch: 6| Step: 8
Training loss: 2.7646265029907227
Validation loss: 2.732553764056134

Epoch: 6| Step: 9
Training loss: 3.063786506652832
Validation loss: 2.729100122246691

Epoch: 6| Step: 10
Training loss: 3.308535575866699
Validation loss: 2.7314162523515764

Epoch: 6| Step: 11
Training loss: 2.393260955810547
Validation loss: 2.7268135240001063

Epoch: 6| Step: 12
Training loss: 3.5524587631225586
Validation loss: 2.729335618275468

Epoch: 6| Step: 13
Training loss: 3.2633371353149414
Validation loss: 2.727673412651144

Epoch: 32| Step: 0
Training loss: 3.02118182182312
Validation loss: 2.7302750772045505

Epoch: 6| Step: 1
Training loss: 3.6437246799468994
Validation loss: 2.7290630443121797

Epoch: 6| Step: 2
Training loss: 2.8906054496765137
Validation loss: 2.7291000479011127

Epoch: 6| Step: 3
Training loss: 3.059572696685791
Validation loss: 2.7313795140994492

Epoch: 6| Step: 4
Training loss: 2.343419075012207
Validation loss: 2.7325950053430375

Epoch: 6| Step: 5
Training loss: 1.5037974119186401
Validation loss: 2.733397276170792

Epoch: 6| Step: 6
Training loss: 2.889209747314453
Validation loss: 2.747486242683985

Epoch: 6| Step: 7
Training loss: 2.6123294830322266
Validation loss: 2.745733843054823

Epoch: 6| Step: 8
Training loss: 3.4944515228271484
Validation loss: 2.749670023559242

Epoch: 6| Step: 9
Training loss: 3.194397211074829
Validation loss: 2.7518051260261127

Epoch: 6| Step: 10
Training loss: 3.59999942779541
Validation loss: 2.7548476547323246

Epoch: 6| Step: 11
Training loss: 2.5767509937286377
Validation loss: 2.761261781056722

Epoch: 6| Step: 12
Training loss: 2.438957691192627
Validation loss: 2.7430356394860054

Epoch: 6| Step: 13
Training loss: 2.951150894165039
Validation loss: 2.7263666763100574

Epoch: 33| Step: 0
Training loss: 2.447404384613037
Validation loss: 2.7164820035298667

Epoch: 6| Step: 1
Training loss: 2.510647773742676
Validation loss: 2.728599632939985

Epoch: 6| Step: 2
Training loss: 3.290085554122925
Validation loss: 2.752880491236205

Epoch: 6| Step: 3
Training loss: 3.7498364448547363
Validation loss: 2.777126091782765

Epoch: 6| Step: 4
Training loss: 3.1140189170837402
Validation loss: 2.772215643236714

Epoch: 6| Step: 5
Training loss: 2.822789192199707
Validation loss: 2.726275659376575

Epoch: 6| Step: 6
Training loss: 2.848273754119873
Validation loss: 2.7143439349307807

Epoch: 6| Step: 7
Training loss: 2.6821911334991455
Validation loss: 2.719579988910306

Epoch: 6| Step: 8
Training loss: 2.4182844161987305
Validation loss: 2.719338188889206

Epoch: 6| Step: 9
Training loss: 2.7831573486328125
Validation loss: 2.727775853167298

Epoch: 6| Step: 10
Training loss: 2.745863676071167
Validation loss: 2.7535182891353482

Epoch: 6| Step: 11
Training loss: 2.954176425933838
Validation loss: 2.759092500132899

Epoch: 6| Step: 12
Training loss: 2.4418933391571045
Validation loss: 2.7665602443038777

Epoch: 6| Step: 13
Training loss: 4.0244340896606445
Validation loss: 2.7543484985187487

Epoch: 34| Step: 0
Training loss: 3.152285575866699
Validation loss: 2.726017198254985

Epoch: 6| Step: 1
Training loss: 3.074082851409912
Validation loss: 2.7149460520795596

Epoch: 6| Step: 2
Training loss: 2.687861442565918
Validation loss: 2.715749976455524

Epoch: 6| Step: 3
Training loss: 1.9746193885803223
Validation loss: 2.716699669438024

Epoch: 6| Step: 4
Training loss: 2.166034698486328
Validation loss: 2.719995073092881

Epoch: 6| Step: 5
Training loss: 2.4474735260009766
Validation loss: 2.724620503763999

Epoch: 6| Step: 6
Training loss: 2.7529797554016113
Validation loss: 2.7282754349452194

Epoch: 6| Step: 7
Training loss: 2.7391316890716553
Validation loss: 2.7292280248416367

Epoch: 6| Step: 8
Training loss: 2.947628974914551
Validation loss: 2.7199736820754183

Epoch: 6| Step: 9
Training loss: 3.870178699493408
Validation loss: 2.714197140867992

Epoch: 6| Step: 10
Training loss: 3.386472702026367
Validation loss: 2.7103932442203647

Epoch: 6| Step: 11
Training loss: 3.633331060409546
Validation loss: 2.7098843077177643

Epoch: 6| Step: 12
Training loss: 2.93053936958313
Validation loss: 2.7099420844867663

Epoch: 6| Step: 13
Training loss: 1.9895482063293457
Validation loss: 2.7167059221575336

Epoch: 35| Step: 0
Training loss: 2.2198245525360107
Validation loss: 2.7162131494091404

Epoch: 6| Step: 1
Training loss: 2.5967392921447754
Validation loss: 2.714340950853081

Epoch: 6| Step: 2
Training loss: 3.06754732131958
Validation loss: 2.725676757033153

Epoch: 6| Step: 3
Training loss: 2.5750136375427246
Validation loss: 2.7340007443581857

Epoch: 6| Step: 4
Training loss: 2.8908281326293945
Validation loss: 2.7679043149435394

Epoch: 6| Step: 5
Training loss: 3.1462178230285645
Validation loss: 2.724586973908127

Epoch: 6| Step: 6
Training loss: 3.208056688308716
Validation loss: 2.704050084596039

Epoch: 6| Step: 7
Training loss: 2.7634541988372803
Validation loss: 2.7009319079819547

Epoch: 6| Step: 8
Training loss: 2.3723363876342773
Validation loss: 2.706282038842478

Epoch: 6| Step: 9
Training loss: 2.499117851257324
Validation loss: 2.7131442075134604

Epoch: 6| Step: 10
Training loss: 3.8239994049072266
Validation loss: 2.715622837825488

Epoch: 6| Step: 11
Training loss: 3.052834987640381
Validation loss: 2.7138046808140253

Epoch: 6| Step: 12
Training loss: 2.977076530456543
Validation loss: 2.719165753292781

Epoch: 6| Step: 13
Training loss: 2.833564281463623
Validation loss: 2.716339265146563

Epoch: 36| Step: 0
Training loss: 3.5719845294952393
Validation loss: 2.7143765777669926

Epoch: 6| Step: 1
Training loss: 2.678481101989746
Validation loss: 2.7127610022021877

Epoch: 6| Step: 2
Training loss: 2.3249993324279785
Validation loss: 2.71482644414389

Epoch: 6| Step: 3
Training loss: 2.7127320766448975
Validation loss: 2.7159504249531734

Epoch: 6| Step: 4
Training loss: 3.9122812747955322
Validation loss: 2.7119882670781945

Epoch: 6| Step: 5
Training loss: 2.7195146083831787
Validation loss: 2.705463114605155

Epoch: 6| Step: 6
Training loss: 3.389082908630371
Validation loss: 2.701079691610029

Epoch: 6| Step: 7
Training loss: 3.238222122192383
Validation loss: 2.6995011555251254

Epoch: 6| Step: 8
Training loss: 1.4757373332977295
Validation loss: 2.7034857478193057

Epoch: 6| Step: 9
Training loss: 2.572510242462158
Validation loss: 2.705985966549125

Epoch: 6| Step: 10
Training loss: 3.0353965759277344
Validation loss: 2.6997924876469437

Epoch: 6| Step: 11
Training loss: 2.5202417373657227
Validation loss: 2.698501181858842

Epoch: 6| Step: 12
Training loss: 2.636359214782715
Validation loss: 2.698357937156513

Epoch: 6| Step: 13
Training loss: 3.127608299255371
Validation loss: 2.695163985734345

Epoch: 37| Step: 0
Training loss: 2.7147204875946045
Validation loss: 2.695734290666478

Epoch: 6| Step: 1
Training loss: 2.1432745456695557
Validation loss: 2.6964916439466577

Epoch: 6| Step: 2
Training loss: 3.229663372039795
Validation loss: 2.6922345622893302

Epoch: 6| Step: 3
Training loss: 2.530642032623291
Validation loss: 2.694902596935149

Epoch: 6| Step: 4
Training loss: 3.1731271743774414
Validation loss: 2.7011717468179683

Epoch: 6| Step: 5
Training loss: 2.371840000152588
Validation loss: 2.707327896548856

Epoch: 6| Step: 6
Training loss: 3.3575353622436523
Validation loss: 2.6981332379002727

Epoch: 6| Step: 7
Training loss: 3.021084785461426
Validation loss: 2.694456746501307

Epoch: 6| Step: 8
Training loss: 2.5109972953796387
Validation loss: 2.6878557974292385

Epoch: 6| Step: 9
Training loss: 2.807711124420166
Validation loss: 2.6910491451140373

Epoch: 6| Step: 10
Training loss: 3.897343158721924
Validation loss: 2.685846382571805

Epoch: 6| Step: 11
Training loss: 3.085655689239502
Validation loss: 2.69025876445155

Epoch: 6| Step: 12
Training loss: 2.453120231628418
Validation loss: 2.7128669549060125

Epoch: 6| Step: 13
Training loss: 2.249903678894043
Validation loss: 2.713254569679178

Epoch: 38| Step: 0
Training loss: 2.7122669219970703
Validation loss: 2.694267067857968

Epoch: 6| Step: 1
Training loss: 2.080439805984497
Validation loss: 2.688042204867127

Epoch: 6| Step: 2
Training loss: 2.9989073276519775
Validation loss: 2.686078856068273

Epoch: 6| Step: 3
Training loss: 3.1418097019195557
Validation loss: 2.684233650084465

Epoch: 6| Step: 4
Training loss: 3.2353930473327637
Validation loss: 2.6831481123483307

Epoch: 6| Step: 5
Training loss: 2.5593066215515137
Validation loss: 2.679898541460755

Epoch: 6| Step: 6
Training loss: 2.920858383178711
Validation loss: 2.684539759030906

Epoch: 6| Step: 7
Training loss: 3.5492234230041504
Validation loss: 2.6858102608752508

Epoch: 6| Step: 8
Training loss: 2.804971694946289
Validation loss: 2.682272090706774

Epoch: 6| Step: 9
Training loss: 2.91322660446167
Validation loss: 2.6866497173104236

Epoch: 6| Step: 10
Training loss: 2.664172410964966
Validation loss: 2.6883994994624967

Epoch: 6| Step: 11
Training loss: 2.3558130264282227
Validation loss: 2.685512137669389

Epoch: 6| Step: 12
Training loss: 2.545938014984131
Validation loss: 2.689740865461288

Epoch: 6| Step: 13
Training loss: 3.44753360748291
Validation loss: 2.687754323405604

Epoch: 39| Step: 0
Training loss: 3.109313488006592
Validation loss: 2.6898402090995543

Epoch: 6| Step: 1
Training loss: 2.7544240951538086
Validation loss: 2.686465686367404

Epoch: 6| Step: 2
Training loss: 2.75357723236084
Validation loss: 2.6817988836637108

Epoch: 6| Step: 3
Training loss: 2.3302407264709473
Validation loss: 2.684066754515453

Epoch: 6| Step: 4
Training loss: 2.2673745155334473
Validation loss: 2.684752671949325

Epoch: 6| Step: 5
Training loss: 2.7567031383514404
Validation loss: 2.6890557248105287

Epoch: 6| Step: 6
Training loss: 2.982999563217163
Validation loss: 2.688825907245759

Epoch: 6| Step: 7
Training loss: 3.8822741508483887
Validation loss: 2.692184104714342

Epoch: 6| Step: 8
Training loss: 4.050790786743164
Validation loss: 2.6905557083827194

Epoch: 6| Step: 9
Training loss: 2.5905094146728516
Validation loss: 2.699615358024515

Epoch: 6| Step: 10
Training loss: 3.310091018676758
Validation loss: 2.6900434263290895

Epoch: 6| Step: 11
Training loss: 1.7857956886291504
Validation loss: 2.687028851560367

Epoch: 6| Step: 12
Training loss: 2.219010353088379
Validation loss: 2.692184071387014

Epoch: 6| Step: 13
Training loss: 2.79486083984375
Validation loss: 2.6994437863749843

Epoch: 40| Step: 0
Training loss: 3.315518617630005
Validation loss: 2.6948200733430925

Epoch: 6| Step: 1
Training loss: 2.672194480895996
Validation loss: 2.693919451005997

Epoch: 6| Step: 2
Training loss: 3.3323683738708496
Validation loss: 2.6848184395861883

Epoch: 6| Step: 3
Training loss: 2.343899965286255
Validation loss: 2.6822061410514255

Epoch: 6| Step: 4
Training loss: 3.209472417831421
Validation loss: 2.6821028314610964

Epoch: 6| Step: 5
Training loss: 3.087678909301758
Validation loss: 2.6836271414192776

Epoch: 6| Step: 6
Training loss: 3.1638331413269043
Validation loss: 2.679988109937278

Epoch: 6| Step: 7
Training loss: 2.9773342609405518
Validation loss: 2.6768199038761917

Epoch: 6| Step: 8
Training loss: 2.6612539291381836
Validation loss: 2.6805366854513846

Epoch: 6| Step: 9
Training loss: 2.064965009689331
Validation loss: 2.678561931015343

Epoch: 6| Step: 10
Training loss: 2.4608263969421387
Validation loss: 2.6757023257593953

Epoch: 6| Step: 11
Training loss: 2.7075109481811523
Validation loss: 2.6761671496975805

Epoch: 6| Step: 12
Training loss: 2.6903724670410156
Validation loss: 2.6694993075504097

Epoch: 6| Step: 13
Training loss: 2.686673402786255
Validation loss: 2.6668012578000306

Epoch: 41| Step: 0
Training loss: 2.8749923706054688
Validation loss: 2.667422466380622

Epoch: 6| Step: 1
Training loss: 2.914522886276245
Validation loss: 2.6738736988395773

Epoch: 6| Step: 2
Training loss: 2.44842529296875
Validation loss: 2.669259399496099

Epoch: 6| Step: 3
Training loss: 2.9962401390075684
Validation loss: 2.667390723382273

Epoch: 6| Step: 4
Training loss: 2.3522329330444336
Validation loss: 2.667889961632349

Epoch: 6| Step: 5
Training loss: 2.380037784576416
Validation loss: 2.6662176142456713

Epoch: 6| Step: 6
Training loss: 2.759758949279785
Validation loss: 2.6644575006218365

Epoch: 6| Step: 7
Training loss: 3.0165066719055176
Validation loss: 2.665099802837577

Epoch: 6| Step: 8
Training loss: 2.777331590652466
Validation loss: 2.668277468732608

Epoch: 6| Step: 9
Training loss: 2.513073205947876
Validation loss: 2.6638862830336376

Epoch: 6| Step: 10
Training loss: 3.0589966773986816
Validation loss: 2.6652538494397233

Epoch: 6| Step: 11
Training loss: 2.859048366546631
Validation loss: 2.658631940041819

Epoch: 6| Step: 12
Training loss: 3.414618492126465
Validation loss: 2.6675233276941444

Epoch: 6| Step: 13
Training loss: 3.2990431785583496
Validation loss: 2.6694716689407185

Epoch: 42| Step: 0
Training loss: 2.281487464904785
Validation loss: 2.677264375071372

Epoch: 6| Step: 1
Training loss: 3.1770284175872803
Validation loss: 2.680553808007189

Epoch: 6| Step: 2
Training loss: 2.805076837539673
Validation loss: 2.6744517510937107

Epoch: 6| Step: 3
Training loss: 2.1100716590881348
Validation loss: 2.6697925060026106

Epoch: 6| Step: 4
Training loss: 2.3661904335021973
Validation loss: 2.664470729007516

Epoch: 6| Step: 5
Training loss: 3.0512046813964844
Validation loss: 2.6634926872868694

Epoch: 6| Step: 6
Training loss: 2.4855122566223145
Validation loss: 2.6659331270443496

Epoch: 6| Step: 7
Training loss: 3.304165840148926
Validation loss: 2.6695502445261967

Epoch: 6| Step: 8
Training loss: 2.6015591621398926
Validation loss: 2.6911509242109073

Epoch: 6| Step: 9
Training loss: 3.8433656692504883
Validation loss: 2.7264143472076743

Epoch: 6| Step: 10
Training loss: 3.2124557495117188
Validation loss: 2.736930436985467

Epoch: 6| Step: 11
Training loss: 3.576711654663086
Validation loss: 2.740876264469598

Epoch: 6| Step: 12
Training loss: 2.3774466514587402
Validation loss: 2.7411188105101227

Epoch: 6| Step: 13
Training loss: 2.309779167175293
Validation loss: 2.7420941065716486

Epoch: 43| Step: 0
Training loss: 2.547307014465332
Validation loss: 2.7417901075014504

Epoch: 6| Step: 1
Training loss: 2.988089084625244
Validation loss: 2.7406283424746607

Epoch: 6| Step: 2
Training loss: 2.9474716186523438
Validation loss: 2.736249836542273

Epoch: 6| Step: 3
Training loss: 3.8756585121154785
Validation loss: 2.7388966057890203

Epoch: 6| Step: 4
Training loss: 3.2737085819244385
Validation loss: 2.738656408043318

Epoch: 6| Step: 5
Training loss: 1.9416005611419678
Validation loss: 2.7403506540483042

Epoch: 6| Step: 6
Training loss: 3.6771249771118164
Validation loss: 2.7370203874444448

Epoch: 6| Step: 7
Training loss: 2.3652963638305664
Validation loss: 2.738936108927573

Epoch: 6| Step: 8
Training loss: 2.2351036071777344
Validation loss: 2.7379020490954

Epoch: 6| Step: 9
Training loss: 2.450164794921875
Validation loss: 2.739620262576688

Epoch: 6| Step: 10
Training loss: 3.0527875423431396
Validation loss: 2.734794524408156

Epoch: 6| Step: 11
Training loss: 3.4209022521972656
Validation loss: 2.7352018766505743

Epoch: 6| Step: 12
Training loss: 2.6726675033569336
Validation loss: 2.7368338108062744

Epoch: 6| Step: 13
Training loss: 2.4814178943634033
Validation loss: 2.733950632874684

Epoch: 44| Step: 0
Training loss: 2.678997039794922
Validation loss: 2.740549620761666

Epoch: 6| Step: 1
Training loss: 2.422849416732788
Validation loss: 2.748168960694344

Epoch: 6| Step: 2
Training loss: 3.148984909057617
Validation loss: 2.7422386523216002

Epoch: 6| Step: 3
Training loss: 2.8650271892547607
Validation loss: 2.7406671252301944

Epoch: 6| Step: 4
Training loss: 1.9416239261627197
Validation loss: 2.738055311223512

Epoch: 6| Step: 5
Training loss: 2.380664110183716
Validation loss: 2.7426803368394093

Epoch: 6| Step: 6
Training loss: 3.266439437866211
Validation loss: 2.741721896715062

Epoch: 6| Step: 7
Training loss: 3.11702299118042
Validation loss: 2.754219226939704

Epoch: 6| Step: 8
Training loss: 3.0469858646392822
Validation loss: 2.740120005863969

Epoch: 6| Step: 9
Training loss: 3.435842990875244
Validation loss: 2.73729565066676

Epoch: 6| Step: 10
Training loss: 2.9160866737365723
Validation loss: 2.736655148126746

Epoch: 6| Step: 11
Training loss: 3.0288186073303223
Validation loss: 2.7352556310674196

Epoch: 6| Step: 12
Training loss: 3.124565601348877
Validation loss: 2.7312045340896933

Epoch: 6| Step: 13
Training loss: 2.3394229412078857
Validation loss: 2.7286010224332093

Epoch: 45| Step: 0
Training loss: 3.0788779258728027
Validation loss: 2.7238758533231673

Epoch: 6| Step: 1
Training loss: 2.344632625579834
Validation loss: 2.7251300324675856

Epoch: 6| Step: 2
Training loss: 3.2866573333740234
Validation loss: 2.718892043636691

Epoch: 6| Step: 3
Training loss: 3.0461301803588867
Validation loss: 2.7228058333038003

Epoch: 6| Step: 4
Training loss: 2.1826956272125244
Validation loss: 2.7179184395779847

Epoch: 6| Step: 5
Training loss: 2.710256814956665
Validation loss: 2.723419584253783

Epoch: 6| Step: 6
Training loss: 2.709591865539551
Validation loss: 2.718600085986558

Epoch: 6| Step: 7
Training loss: 3.203097343444824
Validation loss: 2.7189394248429166

Epoch: 6| Step: 8
Training loss: 2.658327579498291
Validation loss: 2.723504002376269

Epoch: 6| Step: 9
Training loss: 3.6383073329925537
Validation loss: 2.7188016060859925

Epoch: 6| Step: 10
Training loss: 2.7469353675842285
Validation loss: 2.715984267573203

Epoch: 6| Step: 11
Training loss: 2.1851091384887695
Validation loss: 2.7143356107896373

Epoch: 6| Step: 12
Training loss: 2.877152681350708
Validation loss: 2.719122637984573

Epoch: 6| Step: 13
Training loss: 3.6632254123687744
Validation loss: 2.7202331481441373

Epoch: 46| Step: 0
Training loss: 2.95672607421875
Validation loss: 2.7173947595780894

Epoch: 6| Step: 1
Training loss: 2.563795804977417
Validation loss: 2.719316577398649

Epoch: 6| Step: 2
Training loss: 2.6676931381225586
Validation loss: 2.7186337209516958

Epoch: 6| Step: 3
Training loss: 2.964970588684082
Validation loss: 2.7169547542448966

Epoch: 6| Step: 4
Training loss: 2.188295841217041
Validation loss: 2.7164151950549056

Epoch: 6| Step: 5
Training loss: 2.983253002166748
Validation loss: 2.712085361121803

Epoch: 6| Step: 6
Training loss: 3.3700482845306396
Validation loss: 2.7125309795461674

Epoch: 6| Step: 7
Training loss: 3.372492790222168
Validation loss: 2.7111135246933147

Epoch: 6| Step: 8
Training loss: 2.896808624267578
Validation loss: 2.713843863497498

Epoch: 6| Step: 9
Training loss: 2.7591488361358643
Validation loss: 2.710630903961838

Epoch: 6| Step: 10
Training loss: 3.0137832164764404
Validation loss: 2.7073924054381666

Epoch: 6| Step: 11
Training loss: 2.519862174987793
Validation loss: 2.7054005643372894

Epoch: 6| Step: 12
Training loss: 2.3583502769470215
Validation loss: 2.703822635835217

Epoch: 6| Step: 13
Training loss: 3.4533438682556152
Validation loss: 2.7021982080192974

Epoch: 47| Step: 0
Training loss: 2.4880781173706055
Validation loss: 2.705542546446605

Epoch: 6| Step: 1
Training loss: 3.1685781478881836
Validation loss: 2.709785920317455

Epoch: 6| Step: 2
Training loss: 3.0447328090667725
Validation loss: 2.7084086479679232

Epoch: 6| Step: 3
Training loss: 2.826289653778076
Validation loss: 2.718524927734047

Epoch: 6| Step: 4
Training loss: 2.957409381866455
Validation loss: 2.7292063236236572

Epoch: 6| Step: 5
Training loss: 2.772733211517334
Validation loss: 2.7253571530824066

Epoch: 6| Step: 6
Training loss: 2.779881000518799
Validation loss: 2.7190749055595806

Epoch: 6| Step: 7
Training loss: 2.977417469024658
Validation loss: 2.7157186564578804

Epoch: 6| Step: 8
Training loss: 1.979230284690857
Validation loss: 2.7096358114673245

Epoch: 6| Step: 9
Training loss: 3.3807950019836426
Validation loss: 2.707386068118516

Epoch: 6| Step: 10
Training loss: 3.6788134574890137
Validation loss: 2.699944670482348

Epoch: 6| Step: 11
Training loss: 3.019310474395752
Validation loss: 2.7031043806383686

Epoch: 6| Step: 12
Training loss: 2.1920998096466064
Validation loss: 2.7001470186377086

Epoch: 6| Step: 13
Training loss: 2.2270009517669678
Validation loss: 2.703626168671475

Epoch: 48| Step: 0
Training loss: 4.580844879150391
Validation loss: 2.704088141841273

Epoch: 6| Step: 1
Training loss: 3.1741347312927246
Validation loss: 2.7066958463320168

Epoch: 6| Step: 2
Training loss: 2.9331045150756836
Validation loss: 2.7043948968251548

Epoch: 6| Step: 3
Training loss: 2.4339852333068848
Validation loss: 2.6998609317246305

Epoch: 6| Step: 4
Training loss: 2.3189053535461426
Validation loss: 2.703011066682877

Epoch: 6| Step: 5
Training loss: 3.231480598449707
Validation loss: 2.7027060729201122

Epoch: 6| Step: 6
Training loss: 3.0709915161132812
Validation loss: 2.7002635668682795

Epoch: 6| Step: 7
Training loss: 2.688991069793701
Validation loss: 2.702024080420053

Epoch: 6| Step: 8
Training loss: 2.739797592163086
Validation loss: 2.6979370501733597

Epoch: 6| Step: 9
Training loss: 2.6021838188171387
Validation loss: 2.6941475765679472

Epoch: 6| Step: 10
Training loss: 2.5965592861175537
Validation loss: 2.6969502690017864

Epoch: 6| Step: 11
Training loss: 1.5980143547058105
Validation loss: 2.6996480162425707

Epoch: 6| Step: 12
Training loss: 2.8896617889404297
Validation loss: 2.7027141535153953

Epoch: 6| Step: 13
Training loss: 3.0408527851104736
Validation loss: 2.7080032107650593

Epoch: 49| Step: 0
Training loss: 2.8513646125793457
Validation loss: 2.709001302719116

Epoch: 6| Step: 1
Training loss: 2.51607084274292
Validation loss: 2.7084158774345153

Epoch: 6| Step: 2
Training loss: 2.3007729053497314
Validation loss: 2.7048113987010014

Epoch: 6| Step: 3
Training loss: 2.4931905269622803
Validation loss: 2.7168446971524145

Epoch: 6| Step: 4
Training loss: 3.256114959716797
Validation loss: 2.706231230048723

Epoch: 6| Step: 5
Training loss: 2.543084144592285
Validation loss: 2.7036923541817615

Epoch: 6| Step: 6
Training loss: 2.9021353721618652
Validation loss: 2.701472948956233

Epoch: 6| Step: 7
Training loss: 3.2469239234924316
Validation loss: 2.6967900850439586

Epoch: 6| Step: 8
Training loss: 2.757277488708496
Validation loss: 2.6934312030833256

Epoch: 6| Step: 9
Training loss: 2.6987791061401367
Validation loss: 2.6920035680135093

Epoch: 6| Step: 10
Training loss: 2.958779811859131
Validation loss: 2.6954074700673423

Epoch: 6| Step: 11
Training loss: 2.3502306938171387
Validation loss: 2.695449775265109

Epoch: 6| Step: 12
Training loss: 3.9496660232543945
Validation loss: 2.6952800212367887

Epoch: 6| Step: 13
Training loss: 2.8506481647491455
Validation loss: 2.69438446721723

Epoch: 50| Step: 0
Training loss: 2.2545487880706787
Validation loss: 2.693479768691524

Epoch: 6| Step: 1
Training loss: 2.5045857429504395
Validation loss: 2.691808572379492

Epoch: 6| Step: 2
Training loss: 3.361989736557007
Validation loss: 2.6905385089176956

Epoch: 6| Step: 3
Training loss: 3.970705032348633
Validation loss: 2.694266329529465

Epoch: 6| Step: 4
Training loss: 2.4834699630737305
Validation loss: 2.6909799370714413

Epoch: 6| Step: 5
Training loss: 2.5842361450195312
Validation loss: 2.693213998630483

Epoch: 6| Step: 6
Training loss: 2.7005176544189453
Validation loss: 2.7011834267647035

Epoch: 6| Step: 7
Training loss: 1.245305061340332
Validation loss: 2.6981979595717562

Epoch: 6| Step: 8
Training loss: 2.6777877807617188
Validation loss: 2.6982664626131774

Epoch: 6| Step: 9
Training loss: 3.0667319297790527
Validation loss: 2.6971804429126043

Epoch: 6| Step: 10
Training loss: 4.094219207763672
Validation loss: 2.6957872170273975

Epoch: 6| Step: 11
Training loss: 2.4879260063171387
Validation loss: 2.6952206755197174

Epoch: 6| Step: 12
Training loss: 3.4076178073883057
Validation loss: 2.6931730111440024

Epoch: 6| Step: 13
Training loss: 2.6442184448242188
Validation loss: 2.699049024171727

Testing loss: 2.737084325154622

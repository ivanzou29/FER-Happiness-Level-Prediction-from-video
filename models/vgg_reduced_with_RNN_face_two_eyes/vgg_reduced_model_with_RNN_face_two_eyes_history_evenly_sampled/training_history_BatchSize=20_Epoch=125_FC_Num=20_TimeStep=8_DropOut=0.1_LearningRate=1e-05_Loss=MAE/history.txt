Epoch: 1| Step: 0
Training loss: 4.209583282470703
Validation loss: 5.175635204520277

Epoch: 5| Step: 1
Training loss: 4.664231300354004
Validation loss: 5.166689011358446

Epoch: 5| Step: 2
Training loss: 4.568031311035156
Validation loss: 5.157623644798033

Epoch: 5| Step: 3
Training loss: 5.1661906242370605
Validation loss: 5.148647451913485

Epoch: 5| Step: 4
Training loss: 5.425950527191162
Validation loss: 5.139319958225373

Epoch: 5| Step: 5
Training loss: 4.919554710388184
Validation loss: 5.129593377472252

Epoch: 5| Step: 6
Training loss: 4.838528633117676
Validation loss: 5.119448789986231

Epoch: 5| Step: 7
Training loss: 5.658040523529053
Validation loss: 5.109411285769555

Epoch: 5| Step: 8
Training loss: 5.920609951019287
Validation loss: 5.097709958271314

Epoch: 5| Step: 9
Training loss: 5.432497978210449
Validation loss: 5.086390736282513

Epoch: 5| Step: 10
Training loss: 3.078230142593384
Validation loss: 5.072951204033308

Epoch: 2| Step: 0
Training loss: 4.650264263153076
Validation loss: 5.0596188011989796

Epoch: 5| Step: 1
Training loss: 5.107481956481934
Validation loss: 5.044263183429677

Epoch: 5| Step: 2
Training loss: 5.360335350036621
Validation loss: 5.027771298603345

Epoch: 5| Step: 3
Training loss: 4.262126445770264
Validation loss: 5.00983706853723

Epoch: 5| Step: 4
Training loss: 4.954321384429932
Validation loss: 4.9904595805752665

Epoch: 5| Step: 5
Training loss: 5.277878761291504
Validation loss: 4.969667188582882

Epoch: 5| Step: 6
Training loss: 4.129473686218262
Validation loss: 4.946484094024987

Epoch: 5| Step: 7
Training loss: 4.917273998260498
Validation loss: 4.924193474554246

Epoch: 5| Step: 8
Training loss: 5.271814823150635
Validation loss: 4.898819244036111

Epoch: 5| Step: 9
Training loss: 4.043247699737549
Validation loss: 4.871148622164163

Epoch: 5| Step: 10
Training loss: 4.263235569000244
Validation loss: 4.84219999723537

Epoch: 3| Step: 0
Training loss: 4.18996524810791
Validation loss: 4.811116295476114

Epoch: 5| Step: 1
Training loss: 5.185267448425293
Validation loss: 4.780327750790503

Epoch: 5| Step: 2
Training loss: 4.117134094238281
Validation loss: 4.747210138587541

Epoch: 5| Step: 3
Training loss: 4.388121604919434
Validation loss: 4.713380531598163

Epoch: 5| Step: 4
Training loss: 6.357565402984619
Validation loss: 4.67911401871712

Epoch: 5| Step: 5
Training loss: 3.420996904373169
Validation loss: 4.644226079346032

Epoch: 5| Step: 6
Training loss: 4.190549850463867
Validation loss: 4.608885688166464

Epoch: 5| Step: 7
Training loss: 4.682575225830078
Validation loss: 4.572871551718763

Epoch: 5| Step: 8
Training loss: 3.621769666671753
Validation loss: 4.537359278689149

Epoch: 5| Step: 9
Training loss: 4.335132598876953
Validation loss: 4.501695156097412

Epoch: 5| Step: 10
Training loss: 4.193626403808594
Validation loss: 4.464189170509257

Epoch: 4| Step: 0
Training loss: 3.8937835693359375
Validation loss: 4.4286176876355245

Epoch: 5| Step: 1
Training loss: 4.094082832336426
Validation loss: 4.3911439423920005

Epoch: 5| Step: 2
Training loss: 4.6546149253845215
Validation loss: 4.355268229720413

Epoch: 5| Step: 3
Training loss: 3.928466796875
Validation loss: 4.320923441199846

Epoch: 5| Step: 4
Training loss: 3.8569037914276123
Validation loss: 4.2837150455803

Epoch: 5| Step: 5
Training loss: 4.376039981842041
Validation loss: 4.248563105060208

Epoch: 5| Step: 6
Training loss: 3.820974826812744
Validation loss: 4.212696126712266

Epoch: 5| Step: 7
Training loss: 4.226881504058838
Validation loss: 4.178133615883448

Epoch: 5| Step: 8
Training loss: 3.896083354949951
Validation loss: 4.1448337237040205

Epoch: 5| Step: 9
Training loss: 3.75891375541687
Validation loss: 4.111711604620821

Epoch: 5| Step: 10
Training loss: 4.131145477294922
Validation loss: 4.082093290103379

Epoch: 5| Step: 0
Training loss: 4.3939409255981445
Validation loss: 4.053453671034946

Epoch: 5| Step: 1
Training loss: 2.2458746433258057
Validation loss: 4.028153396421863

Epoch: 5| Step: 2
Training loss: 3.784343719482422
Validation loss: 4.006536109473116

Epoch: 5| Step: 3
Training loss: 3.6943373680114746
Validation loss: 3.983894048198577

Epoch: 5| Step: 4
Training loss: 3.6934401988983154
Validation loss: 3.961402775138937

Epoch: 5| Step: 5
Training loss: 5.556014060974121
Validation loss: 3.9401292493266444

Epoch: 5| Step: 6
Training loss: 4.257813453674316
Validation loss: 3.9182743462183143

Epoch: 5| Step: 7
Training loss: 3.039407730102539
Validation loss: 3.897845575886388

Epoch: 5| Step: 8
Training loss: 3.7958121299743652
Validation loss: 3.8778390935672227

Epoch: 5| Step: 9
Training loss: 3.500333786010742
Validation loss: 3.857590836863364

Epoch: 5| Step: 10
Training loss: 3.8354294300079346
Validation loss: 3.8393756856200514

Epoch: 6| Step: 0
Training loss: 3.462128162384033
Validation loss: 3.8203495702435895

Epoch: 5| Step: 1
Training loss: 3.8144736289978027
Validation loss: 3.802679643836073

Epoch: 5| Step: 2
Training loss: 4.564413547515869
Validation loss: 3.784392951637186

Epoch: 5| Step: 3
Training loss: 4.0022873878479
Validation loss: 3.770050912775019

Epoch: 5| Step: 4
Training loss: 3.2313830852508545
Validation loss: 3.7544054985046387

Epoch: 5| Step: 5
Training loss: 3.9946250915527344
Validation loss: 3.7379058663563063

Epoch: 5| Step: 6
Training loss: 3.6959381103515625
Validation loss: 3.7226536812320834

Epoch: 5| Step: 7
Training loss: 3.2666916847229004
Validation loss: 3.70885681849654

Epoch: 5| Step: 8
Training loss: 3.7777676582336426
Validation loss: 3.6952320068113265

Epoch: 5| Step: 9
Training loss: 3.424783229827881
Validation loss: 3.6810496186697357

Epoch: 5| Step: 10
Training loss: 2.676532506942749
Validation loss: 3.669682348928144

Epoch: 7| Step: 0
Training loss: 3.093088150024414
Validation loss: 3.6571002621804514

Epoch: 5| Step: 1
Training loss: 4.051733493804932
Validation loss: 3.640421772515902

Epoch: 5| Step: 2
Training loss: 3.108076572418213
Validation loss: 3.628978536974999

Epoch: 5| Step: 3
Training loss: 3.966027021408081
Validation loss: 3.6175319328103015

Epoch: 5| Step: 4
Training loss: 2.8680834770202637
Validation loss: 3.605587710616409

Epoch: 5| Step: 5
Training loss: 4.481871604919434
Validation loss: 3.592804762624925

Epoch: 5| Step: 6
Training loss: 3.5018391609191895
Validation loss: 3.5837504633011354

Epoch: 5| Step: 7
Training loss: 3.4956352710723877
Validation loss: 3.571364684771466

Epoch: 5| Step: 8
Training loss: 3.347947597503662
Validation loss: 3.559119165584605

Epoch: 5| Step: 9
Training loss: 3.21699857711792
Validation loss: 3.548754143458541

Epoch: 5| Step: 10
Training loss: 3.572270393371582
Validation loss: 3.53597092628479

Epoch: 8| Step: 0
Training loss: 3.0259337425231934
Validation loss: 3.5256480068288822

Epoch: 5| Step: 1
Training loss: 3.933129072189331
Validation loss: 3.512480128195978

Epoch: 5| Step: 2
Training loss: 3.090296745300293
Validation loss: 3.5002229726442726

Epoch: 5| Step: 3
Training loss: 3.899594783782959
Validation loss: 3.489056518000941

Epoch: 5| Step: 4
Training loss: 3.405137538909912
Validation loss: 3.474713404973348

Epoch: 5| Step: 5
Training loss: 3.5200552940368652
Validation loss: 3.4624262394443637

Epoch: 5| Step: 6
Training loss: 3.8634419441223145
Validation loss: 3.4503057695204213

Epoch: 5| Step: 7
Training loss: 3.513212203979492
Validation loss: 3.4384301606044976

Epoch: 5| Step: 8
Training loss: 2.9032936096191406
Validation loss: 3.4305039785241567

Epoch: 5| Step: 9
Training loss: 2.666118621826172
Validation loss: 3.4197156890746085

Epoch: 5| Step: 10
Training loss: 3.6279029846191406
Validation loss: 3.412695477085729

Epoch: 9| Step: 0
Training loss: 3.4485859870910645
Validation loss: 3.4017785697854976

Epoch: 5| Step: 1
Training loss: 3.177741289138794
Validation loss: 3.395298111823297

Epoch: 5| Step: 2
Training loss: 2.623350143432617
Validation loss: 3.388468065569478

Epoch: 5| Step: 3
Training loss: 3.6944167613983154
Validation loss: 3.3809853779372347

Epoch: 5| Step: 4
Training loss: 2.687898874282837
Validation loss: 3.3752438611881708

Epoch: 5| Step: 5
Training loss: 4.384345054626465
Validation loss: 3.3655542199329664

Epoch: 5| Step: 6
Training loss: 4.028412818908691
Validation loss: 3.3545994348423456

Epoch: 5| Step: 7
Training loss: 2.1202163696289062
Validation loss: 3.3486996876296176

Epoch: 5| Step: 8
Training loss: 3.8138389587402344
Validation loss: 3.342954238255819

Epoch: 5| Step: 9
Training loss: 3.6219611167907715
Validation loss: 3.3314832384868334

Epoch: 5| Step: 10
Training loss: 2.861170530319214
Validation loss: 3.3211331752038773

Epoch: 10| Step: 0
Training loss: 3.3822579383850098
Validation loss: 3.3158434539712887

Epoch: 5| Step: 1
Training loss: 2.9598288536071777
Validation loss: 3.311774161554152

Epoch: 5| Step: 2
Training loss: 3.118354558944702
Validation loss: 3.3027792848566526

Epoch: 5| Step: 3
Training loss: 3.1762051582336426
Validation loss: 3.296199724238406

Epoch: 5| Step: 4
Training loss: 3.5310988426208496
Validation loss: 3.2891640740056194

Epoch: 5| Step: 5
Training loss: 2.968338966369629
Validation loss: 3.2805943283983456

Epoch: 5| Step: 6
Training loss: 3.391322612762451
Validation loss: 3.2740517995690785

Epoch: 5| Step: 7
Training loss: 2.781625986099243
Validation loss: 3.2656225286504275

Epoch: 5| Step: 8
Training loss: 3.7978298664093018
Validation loss: 3.2600802221605854

Epoch: 5| Step: 9
Training loss: 3.451744556427002
Validation loss: 3.248737455696188

Epoch: 5| Step: 10
Training loss: 3.217111587524414
Validation loss: 3.2428775525862172

Epoch: 11| Step: 0
Training loss: 3.826242446899414
Validation loss: 3.2350699773398777

Epoch: 5| Step: 1
Training loss: 3.733499050140381
Validation loss: 3.228231219835179

Epoch: 5| Step: 2
Training loss: 3.583740234375
Validation loss: 3.2233688626238095

Epoch: 5| Step: 3
Training loss: 3.4039828777313232
Validation loss: 3.2200270775825746

Epoch: 5| Step: 4
Training loss: 3.379749298095703
Validation loss: 3.2123368068407943

Epoch: 5| Step: 5
Training loss: 3.1542460918426514
Validation loss: 3.2004748134202856

Epoch: 5| Step: 6
Training loss: 3.4820556640625
Validation loss: 3.2008399476287184

Epoch: 5| Step: 7
Training loss: 2.5057995319366455
Validation loss: 3.196695073958366

Epoch: 5| Step: 8
Training loss: 3.046992778778076
Validation loss: 3.190716238432033

Epoch: 5| Step: 9
Training loss: 2.251237392425537
Validation loss: 3.186036476524927

Epoch: 5| Step: 10
Training loss: 2.8061695098876953
Validation loss: 3.1862857854494484

Epoch: 12| Step: 0
Training loss: 3.5521888732910156
Validation loss: 3.184694028669788

Epoch: 5| Step: 1
Training loss: 3.348614454269409
Validation loss: 3.1728475170750774

Epoch: 5| Step: 2
Training loss: 3.3017170429229736
Validation loss: 3.1721804834181264

Epoch: 5| Step: 3
Training loss: 3.232968807220459
Validation loss: 3.1688428002019084

Epoch: 5| Step: 4
Training loss: 3.0647847652435303
Validation loss: 3.16685208453927

Epoch: 5| Step: 5
Training loss: 3.0132808685302734
Validation loss: 3.162036672715218

Epoch: 5| Step: 6
Training loss: 2.535583257675171
Validation loss: 3.1603125192785777

Epoch: 5| Step: 7
Training loss: 2.98781681060791
Validation loss: 3.1528144164751937

Epoch: 5| Step: 8
Training loss: 3.8290188312530518
Validation loss: 3.1511405360314155

Epoch: 5| Step: 9
Training loss: 2.8954129219055176
Validation loss: 3.1507862665319957

Epoch: 5| Step: 10
Training loss: 3.074678421020508
Validation loss: 3.146007409659765

Epoch: 13| Step: 0
Training loss: 2.4751296043395996
Validation loss: 3.1488926077401764

Epoch: 5| Step: 1
Training loss: 2.6623222827911377
Validation loss: 3.1428503733809277

Epoch: 5| Step: 2
Training loss: 3.6939697265625
Validation loss: 3.139941938461796

Epoch: 5| Step: 3
Training loss: 2.9835729598999023
Validation loss: 3.1348465129893315

Epoch: 5| Step: 4
Training loss: 2.950655460357666
Validation loss: 3.1336911160458802

Epoch: 5| Step: 5
Training loss: 3.674619197845459
Validation loss: 3.133282684510754

Epoch: 5| Step: 6
Training loss: 3.183135986328125
Validation loss: 3.1299482135362524

Epoch: 5| Step: 7
Training loss: 2.6616787910461426
Validation loss: 3.126152474393127

Epoch: 5| Step: 8
Training loss: 3.533482789993286
Validation loss: 3.125158453500399

Epoch: 5| Step: 9
Training loss: 3.2555911540985107
Validation loss: 3.1133626994266304

Epoch: 5| Step: 10
Training loss: 3.6075210571289062
Validation loss: 3.113975747939079

Epoch: 14| Step: 0
Training loss: 3.630155086517334
Validation loss: 3.116375897520332

Epoch: 5| Step: 1
Training loss: 2.619274139404297
Validation loss: 3.1162980218087473

Epoch: 5| Step: 2
Training loss: 3.198084831237793
Validation loss: 3.114355766645042

Epoch: 5| Step: 3
Training loss: 4.080946445465088
Validation loss: 3.1074560842206402

Epoch: 5| Step: 4
Training loss: 2.3098857402801514
Validation loss: 3.1015919434126986

Epoch: 5| Step: 5
Training loss: 2.3926684856414795
Validation loss: 3.1052506457092943

Epoch: 5| Step: 6
Training loss: 4.010476112365723
Validation loss: 3.1051309595825853

Epoch: 5| Step: 7
Training loss: 3.7453815937042236
Validation loss: 3.106888909493723

Epoch: 5| Step: 8
Training loss: 2.037180185317993
Validation loss: 3.101803961620536

Epoch: 5| Step: 9
Training loss: 3.3602371215820312
Validation loss: 3.092878762111869

Epoch: 5| Step: 10
Training loss: 3.049804925918579
Validation loss: 3.089645001196092

Epoch: 15| Step: 0
Training loss: 2.671192169189453
Validation loss: 3.087524429444344

Epoch: 5| Step: 1
Training loss: 3.3851075172424316
Validation loss: 3.085397164026896

Epoch: 5| Step: 2
Training loss: 3.1576199531555176
Validation loss: 3.08122359296327

Epoch: 5| Step: 3
Training loss: 3.1109981536865234
Validation loss: 3.0782672025824107

Epoch: 5| Step: 4
Training loss: 3.588085889816284
Validation loss: 3.078665864083075

Epoch: 5| Step: 5
Training loss: 2.676231861114502
Validation loss: 3.0776746811405307

Epoch: 5| Step: 6
Training loss: 2.786868095397949
Validation loss: 3.0792526198971655

Epoch: 5| Step: 7
Training loss: 3.10675048828125
Validation loss: 3.0878438052310737

Epoch: 5| Step: 8
Training loss: 3.7457923889160156
Validation loss: 3.0718988398069977

Epoch: 5| Step: 9
Training loss: 3.543708324432373
Validation loss: 3.0664570690483175

Epoch: 5| Step: 10
Training loss: 2.3875231742858887
Validation loss: 3.057768488443026

Epoch: 16| Step: 0
Training loss: 3.1592469215393066
Validation loss: 3.0537075714398454

Epoch: 5| Step: 1
Training loss: 3.2050271034240723
Validation loss: 3.053565284257294

Epoch: 5| Step: 2
Training loss: 3.0612101554870605
Validation loss: 3.0520124999425744

Epoch: 5| Step: 3
Training loss: 2.7609169483184814
Validation loss: 3.0550054657843804

Epoch: 5| Step: 4
Training loss: 3.3419017791748047
Validation loss: 3.0476582434869584

Epoch: 5| Step: 5
Training loss: 3.1891627311706543
Validation loss: 3.0463670581899662

Epoch: 5| Step: 6
Training loss: 3.025153398513794
Validation loss: 3.0434248703782276

Epoch: 5| Step: 7
Training loss: 2.4337849617004395
Validation loss: 3.0434758534995456

Epoch: 5| Step: 8
Training loss: 3.0910329818725586
Validation loss: 3.0367373804892264

Epoch: 5| Step: 9
Training loss: 3.310666561126709
Validation loss: 3.0349909259426977

Epoch: 5| Step: 10
Training loss: 3.473428726196289
Validation loss: 3.0348842964377454

Epoch: 17| Step: 0
Training loss: 3.1156744956970215
Validation loss: 3.034108818218272

Epoch: 5| Step: 1
Training loss: 3.2196221351623535
Validation loss: 3.0300785982480614

Epoch: 5| Step: 2
Training loss: 3.4711766242980957
Validation loss: 3.0330070628914783

Epoch: 5| Step: 3
Training loss: 3.1228671073913574
Validation loss: 3.031066343348513

Epoch: 5| Step: 4
Training loss: 2.77862286567688
Validation loss: 3.0281057511606524

Epoch: 5| Step: 5
Training loss: 2.9408583641052246
Validation loss: 3.0188602247545795

Epoch: 5| Step: 6
Training loss: 3.8381149768829346
Validation loss: 3.0140507016130673

Epoch: 5| Step: 7
Training loss: 2.867032527923584
Validation loss: 3.007069300579768

Epoch: 5| Step: 8
Training loss: 3.0508217811584473
Validation loss: 3.009513880616875

Epoch: 5| Step: 9
Training loss: 2.9398741722106934
Validation loss: 3.0165822839224212

Epoch: 5| Step: 10
Training loss: 2.374513626098633
Validation loss: 3.020198424657186

Epoch: 18| Step: 0
Training loss: 3.3634376525878906
Validation loss: 3.0104534882371143

Epoch: 5| Step: 1
Training loss: 3.2454628944396973
Validation loss: 2.9996920221595356

Epoch: 5| Step: 2
Training loss: 3.8131370544433594
Validation loss: 2.995089743726997

Epoch: 5| Step: 3
Training loss: 2.987179756164551
Validation loss: 2.9922860591642317

Epoch: 5| Step: 4
Training loss: 3.1378111839294434
Validation loss: 2.9885384395558345

Epoch: 5| Step: 5
Training loss: 3.177964448928833
Validation loss: 2.984851957649313

Epoch: 5| Step: 6
Training loss: 2.850916624069214
Validation loss: 2.980406807314965

Epoch: 5| Step: 7
Training loss: 2.4088339805603027
Validation loss: 2.9812905608966784

Epoch: 5| Step: 8
Training loss: 3.673658847808838
Validation loss: 2.9745534081612863

Epoch: 5| Step: 9
Training loss: 2.389711856842041
Validation loss: 2.9736346506303355

Epoch: 5| Step: 10
Training loss: 2.4370853900909424
Validation loss: 2.966612510783698

Epoch: 19| Step: 0
Training loss: 2.9340858459472656
Validation loss: 2.9668336632431194

Epoch: 5| Step: 1
Training loss: 3.561784267425537
Validation loss: 2.9689270450222875

Epoch: 5| Step: 2
Training loss: 2.5802626609802246
Validation loss: 2.9635176068993023

Epoch: 5| Step: 3
Training loss: 3.5349643230438232
Validation loss: 2.9582648251646306

Epoch: 5| Step: 4
Training loss: 2.796278476715088
Validation loss: 2.960274642513644

Epoch: 5| Step: 5
Training loss: 2.925729513168335
Validation loss: 2.9585390552397697

Epoch: 5| Step: 6
Training loss: 3.254366397857666
Validation loss: 2.9603960488432195

Epoch: 5| Step: 7
Training loss: 3.512434720993042
Validation loss: 2.9633669237936697

Epoch: 5| Step: 8
Training loss: 2.7992618083953857
Validation loss: 2.9616386172592

Epoch: 5| Step: 9
Training loss: 2.9043209552764893
Validation loss: 2.9490204216331564

Epoch: 5| Step: 10
Training loss: 2.444244861602783
Validation loss: 2.9461738242897937

Epoch: 20| Step: 0
Training loss: 2.7567200660705566
Validation loss: 2.945953433231641

Epoch: 5| Step: 1
Training loss: 3.009056568145752
Validation loss: 2.9432069024732037

Epoch: 5| Step: 2
Training loss: 3.0902583599090576
Validation loss: 2.9422577760552846

Epoch: 5| Step: 3
Training loss: 3.3768794536590576
Validation loss: 2.936156319033715

Epoch: 5| Step: 4
Training loss: 1.8626773357391357
Validation loss: 2.935949002542803

Epoch: 5| Step: 5
Training loss: 3.0043747425079346
Validation loss: 2.9340821594320317

Epoch: 5| Step: 6
Training loss: 3.3605518341064453
Validation loss: 2.9352934616868214

Epoch: 5| Step: 7
Training loss: 3.88740873336792
Validation loss: 2.9285001216396207

Epoch: 5| Step: 8
Training loss: 2.94002103805542
Validation loss: 2.9315857887268066

Epoch: 5| Step: 9
Training loss: 2.9194140434265137
Validation loss: 2.9265440638347338

Epoch: 5| Step: 10
Training loss: 2.936959981918335
Validation loss: 2.9232896733027633

Epoch: 21| Step: 0
Training loss: 2.285799741744995
Validation loss: 2.9218962935991186

Epoch: 5| Step: 1
Training loss: 2.867462396621704
Validation loss: 2.9186073451913814

Epoch: 5| Step: 2
Training loss: 3.3515868186950684
Validation loss: 2.91569689525071

Epoch: 5| Step: 3
Training loss: 2.667379856109619
Validation loss: 2.916679959143362

Epoch: 5| Step: 4
Training loss: 3.997908115386963
Validation loss: 2.914648986631824

Epoch: 5| Step: 5
Training loss: 2.880918025970459
Validation loss: 2.9123347882301576

Epoch: 5| Step: 6
Training loss: 3.0606796741485596
Validation loss: 2.908306716590799

Epoch: 5| Step: 7
Training loss: 3.268371105194092
Validation loss: 2.90668047371731

Epoch: 5| Step: 8
Training loss: 2.8083958625793457
Validation loss: 2.906650158666795

Epoch: 5| Step: 9
Training loss: 2.295607328414917
Validation loss: 2.904975532203592

Epoch: 5| Step: 10
Training loss: 3.5829017162323
Validation loss: 2.9082538645754576

Epoch: 22| Step: 0
Training loss: 2.827509641647339
Validation loss: 2.9125120280891337

Epoch: 5| Step: 1
Training loss: 3.961243152618408
Validation loss: 2.9160205779537076

Epoch: 5| Step: 2
Training loss: 3.566215991973877
Validation loss: 2.897962024134974

Epoch: 5| Step: 3
Training loss: 2.0276567935943604
Validation loss: 2.8973934778603176

Epoch: 5| Step: 4
Training loss: 2.577932357788086
Validation loss: 2.898531501011182

Epoch: 5| Step: 5
Training loss: 2.92616605758667
Validation loss: 2.9018498671952115

Epoch: 5| Step: 6
Training loss: 3.509477138519287
Validation loss: 2.9151352374784407

Epoch: 5| Step: 7
Training loss: 3.8769690990448
Validation loss: 2.902990997478526

Epoch: 5| Step: 8
Training loss: 2.7074530124664307
Validation loss: 2.8970586561387583

Epoch: 5| Step: 9
Training loss: 2.7041726112365723
Validation loss: 2.896577932501352

Epoch: 5| Step: 10
Training loss: 2.093897819519043
Validation loss: 2.8896763709283646

Epoch: 23| Step: 0
Training loss: 3.476938247680664
Validation loss: 2.886700814770114

Epoch: 5| Step: 1
Training loss: 3.419849395751953
Validation loss: 2.887742780870007

Epoch: 5| Step: 2
Training loss: 2.467362403869629
Validation loss: 2.887412712138186

Epoch: 5| Step: 3
Training loss: 2.531198263168335
Validation loss: 2.893289153293897

Epoch: 5| Step: 4
Training loss: 2.8177993297576904
Validation loss: 2.899974702506937

Epoch: 5| Step: 5
Training loss: 3.3736164569854736
Validation loss: 2.904985092019522

Epoch: 5| Step: 6
Training loss: 3.0745534896850586
Validation loss: 2.898033226689985

Epoch: 5| Step: 7
Training loss: 3.0852723121643066
Validation loss: 2.8903388618141093

Epoch: 5| Step: 8
Training loss: 2.8382680416107178
Validation loss: 2.8858207887218845

Epoch: 5| Step: 9
Training loss: 2.1544604301452637
Validation loss: 2.8839934179859776

Epoch: 5| Step: 10
Training loss: 3.6511623859405518
Validation loss: 2.878813382117979

Epoch: 24| Step: 0
Training loss: 3.3987326622009277
Validation loss: 2.890762672629408

Epoch: 5| Step: 1
Training loss: 3.60418963432312
Validation loss: 2.8903640393287904

Epoch: 5| Step: 2
Training loss: 3.1030209064483643
Validation loss: 2.8852024539824455

Epoch: 5| Step: 3
Training loss: 2.90458607673645
Validation loss: 2.877029506109094

Epoch: 5| Step: 4
Training loss: 2.592872142791748
Validation loss: 2.8734063743263163

Epoch: 5| Step: 5
Training loss: 3.1096646785736084
Validation loss: 2.8719764627436155

Epoch: 5| Step: 6
Training loss: 2.463141918182373
Validation loss: 2.8725266661695255

Epoch: 5| Step: 7
Training loss: 3.0785274505615234
Validation loss: 2.8777141417226484

Epoch: 5| Step: 8
Training loss: 2.8263802528381348
Validation loss: 2.8716097416416293

Epoch: 5| Step: 9
Training loss: 2.8044700622558594
Validation loss: 2.8678693566271054

Epoch: 5| Step: 10
Training loss: 2.770082712173462
Validation loss: 2.866389413033762

Epoch: 25| Step: 0
Training loss: 2.8141331672668457
Validation loss: 2.8683872889446955

Epoch: 5| Step: 1
Training loss: 2.8421788215637207
Validation loss: 2.8675181173509166

Epoch: 5| Step: 2
Training loss: 2.9649624824523926
Validation loss: 2.870428905692152

Epoch: 5| Step: 3
Training loss: 3.4752120971679688
Validation loss: 2.8736456260886243

Epoch: 5| Step: 4
Training loss: 2.786562204360962
Validation loss: 2.86983000334873

Epoch: 5| Step: 5
Training loss: 2.4997951984405518
Validation loss: 2.8649453655365975

Epoch: 5| Step: 6
Training loss: 3.036881923675537
Validation loss: 2.8657721037505777

Epoch: 5| Step: 7
Training loss: 3.084507703781128
Validation loss: 2.8644146175794702

Epoch: 5| Step: 8
Training loss: 3.08333158493042
Validation loss: 2.863382916296682

Epoch: 5| Step: 9
Training loss: 2.78277850151062
Validation loss: 2.864879867082001

Epoch: 5| Step: 10
Training loss: 3.2443759441375732
Validation loss: 2.85721690424027

Epoch: 26| Step: 0
Training loss: 2.410008430480957
Validation loss: 2.8505923337833856

Epoch: 5| Step: 1
Training loss: 3.2880756855010986
Validation loss: 2.854426719809091

Epoch: 5| Step: 2
Training loss: 3.315011501312256
Validation loss: 2.850000981361635

Epoch: 5| Step: 3
Training loss: 2.578636407852173
Validation loss: 2.8489100189619165

Epoch: 5| Step: 4
Training loss: 2.5691423416137695
Validation loss: 2.8465866837450253

Epoch: 5| Step: 5
Training loss: 3.0238120555877686
Validation loss: 2.846661149814565

Epoch: 5| Step: 6
Training loss: 2.957383394241333
Validation loss: 2.8414105369198706

Epoch: 5| Step: 7
Training loss: 3.353848695755005
Validation loss: 2.8378408057715303

Epoch: 5| Step: 8
Training loss: 2.5827832221984863
Validation loss: 2.8375322434210006

Epoch: 5| Step: 9
Training loss: 2.6321330070495605
Validation loss: 2.8366449084333194

Epoch: 5| Step: 10
Training loss: 3.83301043510437
Validation loss: 2.830801353659681

Epoch: 27| Step: 0
Training loss: 3.1101317405700684
Validation loss: 2.8324267428408385

Epoch: 5| Step: 1
Training loss: 2.9971776008605957
Validation loss: 2.8330523660106044

Epoch: 5| Step: 2
Training loss: 3.2468457221984863
Validation loss: 2.8317429634832565

Epoch: 5| Step: 3
Training loss: 2.6104822158813477
Validation loss: 2.8271695131896646

Epoch: 5| Step: 4
Training loss: 2.298865795135498
Validation loss: 2.8272915783748833

Epoch: 5| Step: 5
Training loss: 3.4812190532684326
Validation loss: 2.829332931067354

Epoch: 5| Step: 6
Training loss: 3.112971782684326
Validation loss: 2.8251020549446024

Epoch: 5| Step: 7
Training loss: 3.9173736572265625
Validation loss: 2.8247022885148243

Epoch: 5| Step: 8
Training loss: 1.929109811782837
Validation loss: 2.8215505128265708

Epoch: 5| Step: 9
Training loss: 2.871117353439331
Validation loss: 2.8174853324890137

Epoch: 5| Step: 10
Training loss: 2.6528947353363037
Validation loss: 2.8162055605201313

Epoch: 28| Step: 0
Training loss: 3.226956605911255
Validation loss: 2.817721207936605

Epoch: 5| Step: 1
Training loss: 2.6868698596954346
Validation loss: 2.8203254566397717

Epoch: 5| Step: 2
Training loss: 2.9462192058563232
Validation loss: 2.8275127103251796

Epoch: 5| Step: 3
Training loss: 2.718233108520508
Validation loss: 2.8224663606254

Epoch: 5| Step: 4
Training loss: 2.900386333465576
Validation loss: 2.8157692596476567

Epoch: 5| Step: 5
Training loss: 3.0534512996673584
Validation loss: 2.816925443628783

Epoch: 5| Step: 6
Training loss: 3.479193925857544
Validation loss: 2.812481044441141

Epoch: 5| Step: 7
Training loss: 2.1020500659942627
Validation loss: 2.8094793955485025

Epoch: 5| Step: 8
Training loss: 2.175527572631836
Validation loss: 2.809793333853445

Epoch: 5| Step: 9
Training loss: 3.8016533851623535
Validation loss: 2.805956925115278

Epoch: 5| Step: 10
Training loss: 3.113335609436035
Validation loss: 2.8098152247808312

Epoch: 29| Step: 0
Training loss: 2.9800305366516113
Validation loss: 2.8060544383141304

Epoch: 5| Step: 1
Training loss: 2.8869128227233887
Validation loss: 2.812816225072389

Epoch: 5| Step: 2
Training loss: 3.1717288494110107
Validation loss: 2.8097406561656664

Epoch: 5| Step: 3
Training loss: 3.1316568851470947
Validation loss: 2.810093274680517

Epoch: 5| Step: 4
Training loss: 2.338595151901245
Validation loss: 2.8129379262206373

Epoch: 5| Step: 5
Training loss: 2.463160276412964
Validation loss: 2.817808640900479

Epoch: 5| Step: 6
Training loss: 2.4955451488494873
Validation loss: 2.8105942280061784

Epoch: 5| Step: 7
Training loss: 3.1119160652160645
Validation loss: 2.810888433969149

Epoch: 5| Step: 8
Training loss: 2.8922078609466553
Validation loss: 2.8075900052183416

Epoch: 5| Step: 9
Training loss: 3.970371961593628
Validation loss: 2.8073963067864858

Epoch: 5| Step: 10
Training loss: 2.6663756370544434
Validation loss: 2.8061073416022846

Epoch: 30| Step: 0
Training loss: 2.419410467147827
Validation loss: 2.8026595090025213

Epoch: 5| Step: 1
Training loss: 2.5977683067321777
Validation loss: 2.808371005519744

Epoch: 5| Step: 2
Training loss: 2.31156587600708
Validation loss: 2.802086512247721

Epoch: 5| Step: 3
Training loss: 3.5856080055236816
Validation loss: 2.7969786120999243

Epoch: 5| Step: 4
Training loss: 2.802705764770508
Validation loss: 2.797160630585045

Epoch: 5| Step: 5
Training loss: 2.6401219367980957
Validation loss: 2.7964413499319427

Epoch: 5| Step: 6
Training loss: 3.189487934112549
Validation loss: 2.7953034831631567

Epoch: 5| Step: 7
Training loss: 3.310900926589966
Validation loss: 2.7928924406728437

Epoch: 5| Step: 8
Training loss: 3.025254726409912
Validation loss: 2.7948949619006087

Epoch: 5| Step: 9
Training loss: 2.819518804550171
Validation loss: 2.794356192311933

Epoch: 5| Step: 10
Training loss: 3.4338581562042236
Validation loss: 2.7944158943750526

Epoch: 31| Step: 0
Training loss: 3.576552152633667
Validation loss: 2.807081981371808

Epoch: 5| Step: 1
Training loss: 3.336047410964966
Validation loss: 2.8077454772046817

Epoch: 5| Step: 2
Training loss: 2.7401652336120605
Validation loss: 2.813667117908437

Epoch: 5| Step: 3
Training loss: 2.9054880142211914
Validation loss: 2.8052420616149902

Epoch: 5| Step: 4
Training loss: 2.790222644805908
Validation loss: 2.8023034270091722

Epoch: 5| Step: 5
Training loss: 2.6525204181671143
Validation loss: 2.7950821589398127

Epoch: 5| Step: 6
Training loss: 3.339041233062744
Validation loss: 2.798757383900304

Epoch: 5| Step: 7
Training loss: 1.7461717128753662
Validation loss: 2.7923356409995788

Epoch: 5| Step: 8
Training loss: 2.540584087371826
Validation loss: 2.7934993979751424

Epoch: 5| Step: 9
Training loss: 2.698474407196045
Validation loss: 2.7975069963803856

Epoch: 5| Step: 10
Training loss: 3.817331314086914
Validation loss: 2.8265024769690728

Epoch: 32| Step: 0
Training loss: 2.1969785690307617
Validation loss: 2.821965097099222

Epoch: 5| Step: 1
Training loss: 3.3883965015411377
Validation loss: 2.788098499339114

Epoch: 5| Step: 2
Training loss: 2.669194459915161
Validation loss: 2.783440984705443

Epoch: 5| Step: 3
Training loss: 2.818251132965088
Validation loss: 2.78532249184065

Epoch: 5| Step: 4
Training loss: 3.6622493267059326
Validation loss: 2.793877360641315

Epoch: 5| Step: 5
Training loss: 2.810882329940796
Validation loss: 2.7886865856826946

Epoch: 5| Step: 6
Training loss: 3.401879072189331
Validation loss: 2.7986248436794487

Epoch: 5| Step: 7
Training loss: 2.78863263130188
Validation loss: 2.7948190576286724

Epoch: 5| Step: 8
Training loss: 2.4142825603485107
Validation loss: 2.7934654399912846

Epoch: 5| Step: 9
Training loss: 2.6966357231140137
Validation loss: 2.7897090911865234

Epoch: 5| Step: 10
Training loss: 3.175417900085449
Validation loss: 2.7884866088949223

Epoch: 33| Step: 0
Training loss: 2.65266752243042
Validation loss: 2.7788428644980154

Epoch: 5| Step: 1
Training loss: 2.374504804611206
Validation loss: 2.7768695405734483

Epoch: 5| Step: 2
Training loss: 3.1825270652770996
Validation loss: 2.78196362269822

Epoch: 5| Step: 3
Training loss: 2.3038203716278076
Validation loss: 2.7782943556385655

Epoch: 5| Step: 4
Training loss: 3.0305473804473877
Validation loss: 2.786814558890558

Epoch: 5| Step: 5
Training loss: 3.1548187732696533
Validation loss: 2.779082205987746

Epoch: 5| Step: 6
Training loss: 3.4839859008789062
Validation loss: 2.7795695591998357

Epoch: 5| Step: 7
Training loss: 3.2932896614074707
Validation loss: 2.7772485568959224

Epoch: 5| Step: 8
Training loss: 2.797602415084839
Validation loss: 2.775714846067531

Epoch: 5| Step: 9
Training loss: 3.09672212600708
Validation loss: 2.7762355137896795

Epoch: 5| Step: 10
Training loss: 2.498906135559082
Validation loss: 2.7754051890424503

Epoch: 34| Step: 0
Training loss: 3.0426082611083984
Validation loss: 2.7799830923798265

Epoch: 5| Step: 1
Training loss: 3.1812424659729004
Validation loss: 2.783093467835457

Epoch: 5| Step: 2
Training loss: 2.7577528953552246
Validation loss: 2.786999507616925

Epoch: 5| Step: 3
Training loss: 3.281562089920044
Validation loss: 2.7851868419237036

Epoch: 5| Step: 4
Training loss: 2.758734941482544
Validation loss: 2.7783671373962076

Epoch: 5| Step: 5
Training loss: 2.2648746967315674
Validation loss: 2.779523270104521

Epoch: 5| Step: 6
Training loss: 2.7657310962677
Validation loss: 2.779951234017649

Epoch: 5| Step: 7
Training loss: 3.68574595451355
Validation loss: 2.778249345799928

Epoch: 5| Step: 8
Training loss: 2.4938807487487793
Validation loss: 2.779375155766805

Epoch: 5| Step: 9
Training loss: 3.302960157394409
Validation loss: 2.779414135922668

Epoch: 5| Step: 10
Training loss: 2.206758499145508
Validation loss: 2.780112656213904

Epoch: 35| Step: 0
Training loss: 2.4996750354766846
Validation loss: 2.781085042543309

Epoch: 5| Step: 1
Training loss: 2.7981181144714355
Validation loss: 2.781904464126915

Epoch: 5| Step: 2
Training loss: 2.9031786918640137
Validation loss: 2.7755481094442387

Epoch: 5| Step: 3
Training loss: 2.646595001220703
Validation loss: 2.7695568069334953

Epoch: 5| Step: 4
Training loss: 3.1785061359405518
Validation loss: 2.7691746014420704

Epoch: 5| Step: 5
Training loss: 3.59675931930542
Validation loss: 2.770194089540871

Epoch: 5| Step: 6
Training loss: 2.383690357208252
Validation loss: 2.765482146252868

Epoch: 5| Step: 7
Training loss: 3.391716718673706
Validation loss: 2.767318520494687

Epoch: 5| Step: 8
Training loss: 3.1685523986816406
Validation loss: 2.7663804818225164

Epoch: 5| Step: 9
Training loss: 2.850616931915283
Validation loss: 2.765065936632054

Epoch: 5| Step: 10
Training loss: 2.2933664321899414
Validation loss: 2.766833592486638

Epoch: 36| Step: 0
Training loss: 2.3135571479797363
Validation loss: 2.765377385641939

Epoch: 5| Step: 1
Training loss: 2.5231575965881348
Validation loss: 2.7651298610113

Epoch: 5| Step: 2
Training loss: 2.8702595233917236
Validation loss: 2.7662726243336997

Epoch: 5| Step: 3
Training loss: 3.2669506072998047
Validation loss: 2.7628383328837733

Epoch: 5| Step: 4
Training loss: 3.1773715019226074
Validation loss: 2.7634665760942685

Epoch: 5| Step: 5
Training loss: 3.087841749191284
Validation loss: 2.7622563582594677

Epoch: 5| Step: 6
Training loss: 2.9616446495056152
Validation loss: 2.7609604866273942

Epoch: 5| Step: 7
Training loss: 3.228229522705078
Validation loss: 2.761020198945076

Epoch: 5| Step: 8
Training loss: 3.121061325073242
Validation loss: 2.763275259284563

Epoch: 5| Step: 9
Training loss: 2.712106704711914
Validation loss: 2.7619844098244943

Epoch: 5| Step: 10
Training loss: 2.4107630252838135
Validation loss: 2.7610019945329234

Epoch: 37| Step: 0
Training loss: 2.6544241905212402
Validation loss: 2.760035550722512

Epoch: 5| Step: 1
Training loss: 2.8681728839874268
Validation loss: 2.7610973542736423

Epoch: 5| Step: 2
Training loss: 3.38358736038208
Validation loss: 2.759764317543276

Epoch: 5| Step: 3
Training loss: 2.3423333168029785
Validation loss: 2.7598748796729633

Epoch: 5| Step: 4
Training loss: 2.887267827987671
Validation loss: 2.7570381702915316

Epoch: 5| Step: 5
Training loss: 3.094787120819092
Validation loss: 2.755688226351174

Epoch: 5| Step: 6
Training loss: 2.5435028076171875
Validation loss: 2.7523292367176344

Epoch: 5| Step: 7
Training loss: 2.711181163787842
Validation loss: 2.7530720233917236

Epoch: 5| Step: 8
Training loss: 2.9667410850524902
Validation loss: 2.751119985375353

Epoch: 5| Step: 9
Training loss: 2.6605215072631836
Validation loss: 2.754770319948914

Epoch: 5| Step: 10
Training loss: 3.698225975036621
Validation loss: 2.7559731980805755

Epoch: 38| Step: 0
Training loss: 3.0479941368103027
Validation loss: 2.759209753364645

Epoch: 5| Step: 1
Training loss: 2.082183361053467
Validation loss: 2.7532768428966565

Epoch: 5| Step: 2
Training loss: 3.763758420944214
Validation loss: 2.7588703427263486

Epoch: 5| Step: 3
Training loss: 2.8127293586730957
Validation loss: 2.7711671911260134

Epoch: 5| Step: 4
Training loss: 2.4821994304656982
Validation loss: 2.7674954501531457

Epoch: 5| Step: 5
Training loss: 3.4425101280212402
Validation loss: 2.7615657852542017

Epoch: 5| Step: 6
Training loss: 3.025190830230713
Validation loss: 2.753962383475355

Epoch: 5| Step: 7
Training loss: 3.321044921875
Validation loss: 2.7534122543950237

Epoch: 5| Step: 8
Training loss: 2.5294127464294434
Validation loss: 2.7492688830180834

Epoch: 5| Step: 9
Training loss: 3.1996490955352783
Validation loss: 2.7480849142997497

Epoch: 5| Step: 10
Training loss: 1.742875099182129
Validation loss: 2.7489330717312392

Epoch: 39| Step: 0
Training loss: 2.3798556327819824
Validation loss: 2.74902686765117

Epoch: 5| Step: 1
Training loss: 3.351893186569214
Validation loss: 2.750580931222567

Epoch: 5| Step: 2
Training loss: 2.860572338104248
Validation loss: 2.7511199059024936

Epoch: 5| Step: 3
Training loss: 2.430481433868408
Validation loss: 2.7512612676107757

Epoch: 5| Step: 4
Training loss: 3.3669304847717285
Validation loss: 2.7500709923364783

Epoch: 5| Step: 5
Training loss: 2.6360745429992676
Validation loss: 2.7536047556067027

Epoch: 5| Step: 6
Training loss: 2.7095634937286377
Validation loss: 2.7569881767354985

Epoch: 5| Step: 7
Training loss: 2.666944980621338
Validation loss: 2.754925730407879

Epoch: 5| Step: 8
Training loss: 3.5854556560516357
Validation loss: 2.74501524689377

Epoch: 5| Step: 9
Training loss: 2.7622783184051514
Validation loss: 2.743358891497376

Epoch: 5| Step: 10
Training loss: 2.8414390087127686
Validation loss: 2.742319712074854

Epoch: 40| Step: 0
Training loss: 2.696897029876709
Validation loss: 2.7472649774243756

Epoch: 5| Step: 1
Training loss: 3.143493175506592
Validation loss: 2.7528100731552287

Epoch: 5| Step: 2
Training loss: 2.6425235271453857
Validation loss: 2.7600849956594486

Epoch: 5| Step: 3
Training loss: 2.481147527694702
Validation loss: 2.7670904462055494

Epoch: 5| Step: 4
Training loss: 3.3667120933532715
Validation loss: 2.7657597295699583

Epoch: 5| Step: 5
Training loss: 2.8763253688812256
Validation loss: 2.7675014311267483

Epoch: 5| Step: 6
Training loss: 2.7424590587615967
Validation loss: 2.7694766059998543

Epoch: 5| Step: 7
Training loss: 2.6662843227386475
Validation loss: 2.758364026264478

Epoch: 5| Step: 8
Training loss: 2.7597498893737793
Validation loss: 2.755651843163275

Epoch: 5| Step: 9
Training loss: 3.050809383392334
Validation loss: 2.7459049583763204

Epoch: 5| Step: 10
Training loss: 3.1930103302001953
Validation loss: 2.739267728661978

Epoch: 41| Step: 0
Training loss: 3.2513890266418457
Validation loss: 2.7386018230069067

Epoch: 5| Step: 1
Training loss: 2.1035358905792236
Validation loss: 2.7501207244011665

Epoch: 5| Step: 2
Training loss: 2.2660515308380127
Validation loss: 2.7968415367987847

Epoch: 5| Step: 3
Training loss: 2.7464466094970703
Validation loss: 2.7453550446418022

Epoch: 5| Step: 4
Training loss: 3.5634453296661377
Validation loss: 2.743198497321016

Epoch: 5| Step: 5
Training loss: 2.885143280029297
Validation loss: 2.740790585035919

Epoch: 5| Step: 6
Training loss: 3.2686424255371094
Validation loss: 2.7405484735324817

Epoch: 5| Step: 7
Training loss: 2.963379144668579
Validation loss: 2.742530802244781

Epoch: 5| Step: 8
Training loss: 3.0011634826660156
Validation loss: 2.755499609055058

Epoch: 5| Step: 9
Training loss: 2.595233917236328
Validation loss: 2.7654148147952173

Epoch: 5| Step: 10
Training loss: 2.975567579269409
Validation loss: 2.782414118448893

Epoch: 42| Step: 0
Training loss: 2.7160115242004395
Validation loss: 2.748471329289098

Epoch: 5| Step: 1
Training loss: 3.280247449874878
Validation loss: 2.743137321164531

Epoch: 5| Step: 2
Training loss: 2.827589750289917
Validation loss: 2.74157021122594

Epoch: 5| Step: 3
Training loss: 2.6481339931488037
Validation loss: 2.7487983037066717

Epoch: 5| Step: 4
Training loss: 2.324777603149414
Validation loss: 2.77256707222231

Epoch: 5| Step: 5
Training loss: 2.571911573410034
Validation loss: 2.7935567543070805

Epoch: 5| Step: 6
Training loss: 3.080679178237915
Validation loss: 2.7934855876430387

Epoch: 5| Step: 7
Training loss: 3.0147740840911865
Validation loss: 2.7595008470678843

Epoch: 5| Step: 8
Training loss: 2.618053913116455
Validation loss: 2.756527159803657

Epoch: 5| Step: 9
Training loss: 3.197800397872925
Validation loss: 2.7569480480686313

Epoch: 5| Step: 10
Training loss: 3.504150867462158
Validation loss: 2.7601032923626643

Epoch: 43| Step: 0
Training loss: 2.9588074684143066
Validation loss: 2.757635716469057

Epoch: 5| Step: 1
Training loss: 2.6392924785614014
Validation loss: 2.7512045803890435

Epoch: 5| Step: 2
Training loss: 2.896167755126953
Validation loss: 2.762215155427174

Epoch: 5| Step: 3
Training loss: 3.098128080368042
Validation loss: 2.76528948865911

Epoch: 5| Step: 4
Training loss: 2.474332094192505
Validation loss: 2.754005583383704

Epoch: 5| Step: 5
Training loss: 2.305248260498047
Validation loss: 2.749532325293428

Epoch: 5| Step: 6
Training loss: 3.1456298828125
Validation loss: 2.7396230877086682

Epoch: 5| Step: 7
Training loss: 3.279590606689453
Validation loss: 2.729486483399586

Epoch: 5| Step: 8
Training loss: 2.905940532684326
Validation loss: 2.7271573312820925

Epoch: 5| Step: 9
Training loss: 3.1458373069763184
Validation loss: 2.7267671208227835

Epoch: 5| Step: 10
Training loss: 2.593987464904785
Validation loss: 2.7242789729948966

Epoch: 44| Step: 0
Training loss: 2.7503466606140137
Validation loss: 2.7382421980621996

Epoch: 5| Step: 1
Training loss: 3.995712995529175
Validation loss: 2.788557626867807

Epoch: 5| Step: 2
Training loss: 2.5517125129699707
Validation loss: 2.7503402489487843

Epoch: 5| Step: 3
Training loss: 2.5320255756378174
Validation loss: 2.7610993487860567

Epoch: 5| Step: 4
Training loss: 3.5437190532684326
Validation loss: 2.760027124035743

Epoch: 5| Step: 5
Training loss: 1.836094856262207
Validation loss: 2.7598028336801836

Epoch: 5| Step: 6
Training loss: 2.101694107055664
Validation loss: 2.747443517049154

Epoch: 5| Step: 7
Training loss: 3.6087405681610107
Validation loss: 2.7569320355692217

Epoch: 5| Step: 8
Training loss: 3.1124589443206787
Validation loss: 2.761769207574988

Epoch: 5| Step: 9
Training loss: 3.4152214527130127
Validation loss: 2.753792557665097

Epoch: 5| Step: 10
Training loss: 2.1760098934173584
Validation loss: 2.761743232768069

Epoch: 45| Step: 0
Training loss: 2.097482919692993
Validation loss: 2.7585339315475954

Epoch: 5| Step: 1
Training loss: 3.145116090774536
Validation loss: 2.7716727461866153

Epoch: 5| Step: 2
Training loss: 2.436116933822632
Validation loss: 2.771171751842704

Epoch: 5| Step: 3
Training loss: 2.5721681118011475
Validation loss: 2.7748974600145893

Epoch: 5| Step: 4
Training loss: 2.7607169151306152
Validation loss: 2.768402253427813

Epoch: 5| Step: 5
Training loss: 3.3808326721191406
Validation loss: 2.7573646781265095

Epoch: 5| Step: 6
Training loss: 2.7819085121154785
Validation loss: 2.748559505708756

Epoch: 5| Step: 7
Training loss: 3.3077006340026855
Validation loss: 2.734148597204557

Epoch: 5| Step: 8
Training loss: 2.7502284049987793
Validation loss: 2.7265668376799552

Epoch: 5| Step: 9
Training loss: 3.044032096862793
Validation loss: 2.7189235507801013

Epoch: 5| Step: 10
Training loss: 3.2797043323516846
Validation loss: 2.718738402089765

Epoch: 46| Step: 0
Training loss: 2.672318935394287
Validation loss: 2.716119748289867

Epoch: 5| Step: 1
Training loss: 2.993363857269287
Validation loss: 2.7133828337474535

Epoch: 5| Step: 2
Training loss: 2.5112128257751465
Validation loss: 2.711444575299499

Epoch: 5| Step: 3
Training loss: 2.4638304710388184
Validation loss: 2.7086770355060534

Epoch: 5| Step: 4
Training loss: 2.942941665649414
Validation loss: 2.710662290614138

Epoch: 5| Step: 5
Training loss: 3.404259204864502
Validation loss: 2.7138549820069344

Epoch: 5| Step: 6
Training loss: 3.766066312789917
Validation loss: 2.7148838914850706

Epoch: 5| Step: 7
Training loss: 2.4766616821289062
Validation loss: 2.717449449723767

Epoch: 5| Step: 8
Training loss: 3.0542376041412354
Validation loss: 2.719195476142309

Epoch: 5| Step: 9
Training loss: 2.396373987197876
Validation loss: 2.7184189878484255

Epoch: 5| Step: 10
Training loss: 2.606920003890991
Validation loss: 2.7147695044035554

Epoch: 47| Step: 0
Training loss: 2.070051908493042
Validation loss: 2.7063812850624003

Epoch: 5| Step: 1
Training loss: 2.4077587127685547
Validation loss: 2.7053938527261057

Epoch: 5| Step: 2
Training loss: 2.2423057556152344
Validation loss: 2.699929304020379

Epoch: 5| Step: 3
Training loss: 2.5814805030822754
Validation loss: 2.7050943682270665

Epoch: 5| Step: 4
Training loss: 2.9016661643981934
Validation loss: 2.7114365485406693

Epoch: 5| Step: 5
Training loss: 3.3927001953125
Validation loss: 2.7062039606032835

Epoch: 5| Step: 6
Training loss: 2.649810791015625
Validation loss: 2.70135167337233

Epoch: 5| Step: 7
Training loss: 2.9543566703796387
Validation loss: 2.6989750554484706

Epoch: 5| Step: 8
Training loss: 3.107717990875244
Validation loss: 2.7008014135463263

Epoch: 5| Step: 9
Training loss: 3.435288906097412
Validation loss: 2.70369299509192

Epoch: 5| Step: 10
Training loss: 3.599006175994873
Validation loss: 2.699409597663469

Epoch: 48| Step: 0
Training loss: 2.6372454166412354
Validation loss: 2.698664290930635

Epoch: 5| Step: 1
Training loss: 2.6453065872192383
Validation loss: 2.698857210015738

Epoch: 5| Step: 2
Training loss: 3.150134563446045
Validation loss: 2.6992452759896555

Epoch: 5| Step: 3
Training loss: 2.8314805030822754
Validation loss: 2.6959438426520235

Epoch: 5| Step: 4
Training loss: 2.510875940322876
Validation loss: 2.695768215323007

Epoch: 5| Step: 5
Training loss: 2.7472026348114014
Validation loss: 2.6946436641036824

Epoch: 5| Step: 6
Training loss: 2.7313010692596436
Validation loss: 2.6913463710456766

Epoch: 5| Step: 7
Training loss: 3.356196165084839
Validation loss: 2.6930098072175057

Epoch: 5| Step: 8
Training loss: 3.1976747512817383
Validation loss: 2.6951901297415457

Epoch: 5| Step: 9
Training loss: 2.692211627960205
Validation loss: 2.6936983831467165

Epoch: 5| Step: 10
Training loss: 2.6051549911499023
Validation loss: 2.6917248772036646

Epoch: 49| Step: 0
Training loss: 2.85292911529541
Validation loss: 2.689797427064629

Epoch: 5| Step: 1
Training loss: 2.6235501766204834
Validation loss: 2.6939383322192776

Epoch: 5| Step: 2
Training loss: 2.638784885406494
Validation loss: 2.6987387903275026

Epoch: 5| Step: 3
Training loss: 2.9474995136260986
Validation loss: 2.694742518086587

Epoch: 5| Step: 4
Training loss: 3.1347033977508545
Validation loss: 2.6933826451660483

Epoch: 5| Step: 5
Training loss: 2.5285418033599854
Validation loss: 2.688052910630421

Epoch: 5| Step: 6
Training loss: 2.4455971717834473
Validation loss: 2.689732433647238

Epoch: 5| Step: 7
Training loss: 2.7626233100891113
Validation loss: 2.6889285554168043

Epoch: 5| Step: 8
Training loss: 2.5791659355163574
Validation loss: 2.690452596192719

Epoch: 5| Step: 9
Training loss: 2.8675894737243652
Validation loss: 2.6865867824964624

Epoch: 5| Step: 10
Training loss: 3.9138412475585938
Validation loss: 2.6890738702589467

Epoch: 50| Step: 0
Training loss: 2.7667012214660645
Validation loss: 2.6883781289541595

Epoch: 5| Step: 1
Training loss: 3.4552512168884277
Validation loss: 2.686286977542344

Epoch: 5| Step: 2
Training loss: 3.067370891571045
Validation loss: 2.6873979209571757

Epoch: 5| Step: 3
Training loss: 3.2054505348205566
Validation loss: 2.6821570780969437

Epoch: 5| Step: 4
Training loss: 3.070922374725342
Validation loss: 2.6873315072828725

Epoch: 5| Step: 5
Training loss: 3.1661105155944824
Validation loss: 2.683359299936602

Epoch: 5| Step: 6
Training loss: 2.556753635406494
Validation loss: 2.6801050709139917

Epoch: 5| Step: 7
Training loss: 2.7067904472351074
Validation loss: 2.6796041739884244

Epoch: 5| Step: 8
Training loss: 1.9271290302276611
Validation loss: 2.6827170079754246

Epoch: 5| Step: 9
Training loss: 2.5327296257019043
Validation loss: 2.687318940316477

Epoch: 5| Step: 10
Training loss: 2.5975911617279053
Validation loss: 2.6922964947198027

Epoch: 51| Step: 0
Training loss: 2.9344699382781982
Validation loss: 2.688252951509209

Epoch: 5| Step: 1
Training loss: 2.893407106399536
Validation loss: 2.681304500948998

Epoch: 5| Step: 2
Training loss: 2.6926581859588623
Validation loss: 2.6774514772558726

Epoch: 5| Step: 3
Training loss: 3.3696236610412598
Validation loss: 2.6764428795024915

Epoch: 5| Step: 4
Training loss: 2.4694457054138184
Validation loss: 2.675143358527973

Epoch: 5| Step: 5
Training loss: 3.2563819885253906
Validation loss: 2.6771067393723356

Epoch: 5| Step: 6
Training loss: 2.481480836868286
Validation loss: 2.6759534728142524

Epoch: 5| Step: 7
Training loss: 2.6375505924224854
Validation loss: 2.674947036209927

Epoch: 5| Step: 8
Training loss: 2.9105029106140137
Validation loss: 2.6778063645926853

Epoch: 5| Step: 9
Training loss: 2.712984800338745
Validation loss: 2.6783018112182617

Epoch: 5| Step: 10
Training loss: 2.6740331649780273
Validation loss: 2.677440884292767

Epoch: 52| Step: 0
Training loss: 2.787428379058838
Validation loss: 2.6784690374969156

Epoch: 5| Step: 1
Training loss: 2.9540371894836426
Validation loss: 2.675169260271134

Epoch: 5| Step: 2
Training loss: 1.4913166761398315
Validation loss: 2.675159949128346

Epoch: 5| Step: 3
Training loss: 3.3064143657684326
Validation loss: 2.67370149653445

Epoch: 5| Step: 4
Training loss: 3.01979923248291
Validation loss: 2.675737134871944

Epoch: 5| Step: 5
Training loss: 2.7946105003356934
Validation loss: 2.6771809695869364

Epoch: 5| Step: 6
Training loss: 2.6879265308380127
Validation loss: 2.6759141465669036

Epoch: 5| Step: 7
Training loss: 2.6750998497009277
Validation loss: 2.676918501495033

Epoch: 5| Step: 8
Training loss: 3.288949489593506
Validation loss: 2.675932502233854

Epoch: 5| Step: 9
Training loss: 3.2091546058654785
Validation loss: 2.6760419645617084

Epoch: 5| Step: 10
Training loss: 2.748617649078369
Validation loss: 2.6798733793279177

Epoch: 53| Step: 0
Training loss: 2.6157193183898926
Validation loss: 2.679147712645992

Epoch: 5| Step: 1
Training loss: 2.6083927154541016
Validation loss: 2.6764684441269084

Epoch: 5| Step: 2
Training loss: 3.1114420890808105
Validation loss: 2.6788695294369935

Epoch: 5| Step: 3
Training loss: 3.032214403152466
Validation loss: 2.678003321411789

Epoch: 5| Step: 4
Training loss: 3.1645851135253906
Validation loss: 2.6787350395674348

Epoch: 5| Step: 5
Training loss: 1.993682861328125
Validation loss: 2.677362685562462

Epoch: 5| Step: 6
Training loss: 3.549715042114258
Validation loss: 2.6782891878517727

Epoch: 5| Step: 7
Training loss: 2.737823247909546
Validation loss: 2.6735229761369768

Epoch: 5| Step: 8
Training loss: 3.0128555297851562
Validation loss: 2.6716389450975644

Epoch: 5| Step: 9
Training loss: 2.3829054832458496
Validation loss: 2.6756645146236626

Epoch: 5| Step: 10
Training loss: 2.6657097339630127
Validation loss: 2.676545669955592

Epoch: 54| Step: 0
Training loss: 2.261551856994629
Validation loss: 2.6703692533636607

Epoch: 5| Step: 1
Training loss: 2.716902494430542
Validation loss: 2.667333379868538

Epoch: 5| Step: 2
Training loss: 3.52325439453125
Validation loss: 2.6639207729729275

Epoch: 5| Step: 3
Training loss: 3.4570565223693848
Validation loss: 2.669122818977602

Epoch: 5| Step: 4
Training loss: 2.7843782901763916
Validation loss: 2.6708211232257146

Epoch: 5| Step: 5
Training loss: 3.3163421154022217
Validation loss: 2.6743028266455537

Epoch: 5| Step: 6
Training loss: 2.785191059112549
Validation loss: 2.678060734143821

Epoch: 5| Step: 7
Training loss: 2.377880334854126
Validation loss: 2.6746849501004784

Epoch: 5| Step: 8
Training loss: 2.5492191314697266
Validation loss: 2.6680660452893985

Epoch: 5| Step: 9
Training loss: 2.792877197265625
Validation loss: 2.664233902449249

Epoch: 5| Step: 10
Training loss: 2.3030426502227783
Validation loss: 2.6694663160590717

Epoch: 55| Step: 0
Training loss: 2.5644187927246094
Validation loss: 2.6784060744829077

Epoch: 5| Step: 1
Training loss: 2.210723638534546
Validation loss: 2.6896239403755433

Epoch: 5| Step: 2
Training loss: 2.0576679706573486
Validation loss: 2.6828370504481818

Epoch: 5| Step: 3
Training loss: 3.1126513481140137
Validation loss: 2.684640653671757

Epoch: 5| Step: 4
Training loss: 3.186635971069336
Validation loss: 2.673633339584515

Epoch: 5| Step: 5
Training loss: 3.2530016899108887
Validation loss: 2.6795292259544454

Epoch: 5| Step: 6
Training loss: 2.730339765548706
Validation loss: 2.68338321357645

Epoch: 5| Step: 7
Training loss: 3.0954642295837402
Validation loss: 2.693827498343683

Epoch: 5| Step: 8
Training loss: 2.8205103874206543
Validation loss: 2.7042336463928223

Epoch: 5| Step: 9
Training loss: 2.646162271499634
Validation loss: 2.697553539788851

Epoch: 5| Step: 10
Training loss: 3.403923511505127
Validation loss: 2.685560000840054

Epoch: 56| Step: 0
Training loss: 2.710822820663452
Validation loss: 2.673578211056289

Epoch: 5| Step: 1
Training loss: 3.484187364578247
Validation loss: 2.6618367395093365

Epoch: 5| Step: 2
Training loss: 2.7971227169036865
Validation loss: 2.6599586394525345

Epoch: 5| Step: 3
Training loss: 2.232741117477417
Validation loss: 2.659155596968948

Epoch: 5| Step: 4
Training loss: 3.882864475250244
Validation loss: 2.6620466324590866

Epoch: 5| Step: 5
Training loss: 2.9924564361572266
Validation loss: 2.666993543665896

Epoch: 5| Step: 6
Training loss: 2.2175564765930176
Validation loss: 2.666311348638227

Epoch: 5| Step: 7
Training loss: 2.9011242389678955
Validation loss: 2.662935677395072

Epoch: 5| Step: 8
Training loss: 2.1212010383605957
Validation loss: 2.6609087374902542

Epoch: 5| Step: 9
Training loss: 2.994155168533325
Validation loss: 2.6551972973731255

Epoch: 5| Step: 10
Training loss: 2.451493978500366
Validation loss: 2.654671715151879

Epoch: 57| Step: 0
Training loss: 1.5327131748199463
Validation loss: 2.6538570773216987

Epoch: 5| Step: 1
Training loss: 2.3744616508483887
Validation loss: 2.6542507038321546

Epoch: 5| Step: 2
Training loss: 2.859405040740967
Validation loss: 2.651208371244451

Epoch: 5| Step: 3
Training loss: 2.5436272621154785
Validation loss: 2.654739059427733

Epoch: 5| Step: 4
Training loss: 3.1137566566467285
Validation loss: 2.6539294770968858

Epoch: 5| Step: 5
Training loss: 3.2456626892089844
Validation loss: 2.652516565015239

Epoch: 5| Step: 6
Training loss: 2.898214101791382
Validation loss: 2.6505081756140596

Epoch: 5| Step: 7
Training loss: 2.2026469707489014
Validation loss: 2.649768506326983

Epoch: 5| Step: 8
Training loss: 3.133333206176758
Validation loss: 2.650903691527664

Epoch: 5| Step: 9
Training loss: 3.462759494781494
Validation loss: 2.6493300648145777

Epoch: 5| Step: 10
Training loss: 3.496081590652466
Validation loss: 2.6492128013282694

Epoch: 58| Step: 0
Training loss: 2.505728244781494
Validation loss: 2.64803119115932

Epoch: 5| Step: 1
Training loss: 2.8011605739593506
Validation loss: 2.6486756211967877

Epoch: 5| Step: 2
Training loss: 3.1864192485809326
Validation loss: 2.650182318943803

Epoch: 5| Step: 3
Training loss: 3.299170970916748
Validation loss: 2.649093284401842

Epoch: 5| Step: 4
Training loss: 2.8148674964904785
Validation loss: 2.6524209309649724

Epoch: 5| Step: 5
Training loss: 3.0302810668945312
Validation loss: 2.657017730897473

Epoch: 5| Step: 6
Training loss: 2.959712505340576
Validation loss: 2.6579559054425967

Epoch: 5| Step: 7
Training loss: 2.5613067150115967
Validation loss: 2.6607068148992394

Epoch: 5| Step: 8
Training loss: 2.834801197052002
Validation loss: 2.654450334528441

Epoch: 5| Step: 9
Training loss: 2.608788251876831
Validation loss: 2.650837085580313

Epoch: 5| Step: 10
Training loss: 2.0474812984466553
Validation loss: 2.6439642085823962

Epoch: 59| Step: 0
Training loss: 2.3890414237976074
Validation loss: 2.641786044643771

Epoch: 5| Step: 1
Training loss: 2.137608051300049
Validation loss: 2.6453036108324604

Epoch: 5| Step: 2
Training loss: 2.6399242877960205
Validation loss: 2.6514889014664518

Epoch: 5| Step: 3
Training loss: 3.4699387550354004
Validation loss: 2.659431257555562

Epoch: 5| Step: 4
Training loss: 3.165527820587158
Validation loss: 2.6620701205345894

Epoch: 5| Step: 5
Training loss: 2.9482181072235107
Validation loss: 2.6603993395323395

Epoch: 5| Step: 6
Training loss: 2.5049359798431396
Validation loss: 2.6526752748797016

Epoch: 5| Step: 7
Training loss: 2.456179141998291
Validation loss: 2.6485671894524687

Epoch: 5| Step: 8
Training loss: 2.513554096221924
Validation loss: 2.6462841828664145

Epoch: 5| Step: 9
Training loss: 2.8407270908355713
Validation loss: 2.6405713430014988

Epoch: 5| Step: 10
Training loss: 3.7855381965637207
Validation loss: 2.640256509985975

Epoch: 60| Step: 0
Training loss: 3.1297898292541504
Validation loss: 2.6526409528588735

Epoch: 5| Step: 1
Training loss: 2.987281322479248
Validation loss: 2.6597144552456435

Epoch: 5| Step: 2
Training loss: 2.803018569946289
Validation loss: 2.6648130801416214

Epoch: 5| Step: 3
Training loss: 2.3464438915252686
Validation loss: 2.6587774061387583

Epoch: 5| Step: 4
Training loss: 2.675255537033081
Validation loss: 2.654740341248051

Epoch: 5| Step: 5
Training loss: 2.712860584259033
Validation loss: 2.647183774619974

Epoch: 5| Step: 6
Training loss: 2.5620155334472656
Validation loss: 2.6384093376897995

Epoch: 5| Step: 7
Training loss: 2.684056043624878
Validation loss: 2.6374639541872087

Epoch: 5| Step: 8
Training loss: 3.3344433307647705
Validation loss: 2.635445976770052

Epoch: 5| Step: 9
Training loss: 2.661053419113159
Validation loss: 2.6365655391447005

Epoch: 5| Step: 10
Training loss: 2.8423709869384766
Validation loss: 2.6365025376760833

Epoch: 61| Step: 0
Training loss: 2.5201516151428223
Validation loss: 2.640328473942254

Epoch: 5| Step: 1
Training loss: 2.7236595153808594
Validation loss: 2.640293541774955

Epoch: 5| Step: 2
Training loss: 3.2503082752227783
Validation loss: 2.6554142352073424

Epoch: 5| Step: 3
Training loss: 3.50054931640625
Validation loss: 2.639265803880589

Epoch: 5| Step: 4
Training loss: 2.2685186862945557
Validation loss: 2.636431089011572

Epoch: 5| Step: 5
Training loss: 2.7533435821533203
Validation loss: 2.640564695481331

Epoch: 5| Step: 6
Training loss: 2.1910860538482666
Validation loss: 2.638665194152504

Epoch: 5| Step: 7
Training loss: 2.621802568435669
Validation loss: 2.639584151647424

Epoch: 5| Step: 8
Training loss: 3.2863268852233887
Validation loss: 2.638594412034558

Epoch: 5| Step: 9
Training loss: 2.581296682357788
Validation loss: 2.6374955690035256

Epoch: 5| Step: 10
Training loss: 3.077305316925049
Validation loss: 2.632823198072372

Epoch: 62| Step: 0
Training loss: 2.503234624862671
Validation loss: 2.6339077449614003

Epoch: 5| Step: 1
Training loss: 3.306769609451294
Validation loss: 2.638044029153803

Epoch: 5| Step: 2
Training loss: 3.3122951984405518
Validation loss: 2.6434533954948507

Epoch: 5| Step: 3
Training loss: 2.526160717010498
Validation loss: 2.6413774285265195

Epoch: 5| Step: 4
Training loss: 2.8037800788879395
Validation loss: 2.6377611698642855

Epoch: 5| Step: 5
Training loss: 2.787748098373413
Validation loss: 2.6376753007211993

Epoch: 5| Step: 6
Training loss: 2.9873692989349365
Validation loss: 2.6383253118043304

Epoch: 5| Step: 7
Training loss: 2.812228202819824
Validation loss: 2.6392407263478925

Epoch: 5| Step: 8
Training loss: 2.5350189208984375
Validation loss: 2.6421631433630504

Epoch: 5| Step: 9
Training loss: 2.719876289367676
Validation loss: 2.6394249905822096

Epoch: 5| Step: 10
Training loss: 2.237452268600464
Validation loss: 2.6342353308072655

Epoch: 63| Step: 0
Training loss: 3.0302417278289795
Validation loss: 2.6305077409231536

Epoch: 5| Step: 1
Training loss: 2.8730998039245605
Validation loss: 2.6288730918720202

Epoch: 5| Step: 2
Training loss: 2.2079765796661377
Validation loss: 2.62347831777347

Epoch: 5| Step: 3
Training loss: 2.977731704711914
Validation loss: 2.624334107163132

Epoch: 5| Step: 4
Training loss: 2.696415901184082
Validation loss: 2.6221193959636073

Epoch: 5| Step: 5
Training loss: 2.541640043258667
Validation loss: 2.6229358027058263

Epoch: 5| Step: 6
Training loss: 3.0074217319488525
Validation loss: 2.623466089207639

Epoch: 5| Step: 7
Training loss: 2.510028123855591
Validation loss: 2.623381030174994

Epoch: 5| Step: 8
Training loss: 3.0422775745391846
Validation loss: 2.6273605336425123

Epoch: 5| Step: 9
Training loss: 2.236290216445923
Validation loss: 2.6241530859342186

Epoch: 5| Step: 10
Training loss: 3.539259433746338
Validation loss: 2.620530387406708

Epoch: 64| Step: 0
Training loss: 3.036472797393799
Validation loss: 2.623003905819308

Epoch: 5| Step: 1
Training loss: 3.1788382530212402
Validation loss: 2.622219903494722

Epoch: 5| Step: 2
Training loss: 1.9376624822616577
Validation loss: 2.6213100264149327

Epoch: 5| Step: 3
Training loss: 2.9445512294769287
Validation loss: 2.6212852411372687

Epoch: 5| Step: 4
Training loss: 2.507354259490967
Validation loss: 2.6191817099048245

Epoch: 5| Step: 5
Training loss: 2.6225762367248535
Validation loss: 2.6215094648381716

Epoch: 5| Step: 6
Training loss: 2.5576350688934326
Validation loss: 2.618603990923974

Epoch: 5| Step: 7
Training loss: 3.1113243103027344
Validation loss: 2.6208535394360943

Epoch: 5| Step: 8
Training loss: 2.952179431915283
Validation loss: 2.621782615620603

Epoch: 5| Step: 9
Training loss: 2.9216153621673584
Validation loss: 2.6192338697371946

Epoch: 5| Step: 10
Training loss: 2.785266160964966
Validation loss: 2.619225361013925

Epoch: 65| Step: 0
Training loss: 2.7679553031921387
Validation loss: 2.6221551331140662

Epoch: 5| Step: 1
Training loss: 2.467164993286133
Validation loss: 2.6213141615672777

Epoch: 5| Step: 2
Training loss: 2.6396336555480957
Validation loss: 2.6175682519071843

Epoch: 5| Step: 3
Training loss: 2.9661548137664795
Validation loss: 2.617217625341108

Epoch: 5| Step: 4
Training loss: 3.4205164909362793
Validation loss: 2.6225051649155153

Epoch: 5| Step: 5
Training loss: 2.5680906772613525
Validation loss: 2.617646383982833

Epoch: 5| Step: 6
Training loss: 2.8841681480407715
Validation loss: 2.6183671797475507

Epoch: 5| Step: 7
Training loss: 2.30161452293396
Validation loss: 2.61845830435394

Epoch: 5| Step: 8
Training loss: 2.9099068641662598
Validation loss: 2.6218533951749086

Epoch: 5| Step: 9
Training loss: 2.775756359100342
Validation loss: 2.6173879818249772

Epoch: 5| Step: 10
Training loss: 2.7835638523101807
Validation loss: 2.6201253424408617

Epoch: 66| Step: 0
Training loss: 3.2567429542541504
Validation loss: 2.6185500262885966

Epoch: 5| Step: 1
Training loss: 2.9952967166900635
Validation loss: 2.617998000114195

Epoch: 5| Step: 2
Training loss: 2.198864459991455
Validation loss: 2.6152873039245605

Epoch: 5| Step: 3
Training loss: 3.312892436981201
Validation loss: 2.6116573041485203

Epoch: 5| Step: 4
Training loss: 2.6157851219177246
Validation loss: 2.6114575068155923

Epoch: 5| Step: 5
Training loss: 2.3054006099700928
Validation loss: 2.6130698855205248

Epoch: 5| Step: 6
Training loss: 2.7259793281555176
Validation loss: 2.608884375582459

Epoch: 5| Step: 7
Training loss: 2.942509174346924
Validation loss: 2.6093109218023156

Epoch: 5| Step: 8
Training loss: 3.087594509124756
Validation loss: 2.6137224140987603

Epoch: 5| Step: 9
Training loss: 2.6798911094665527
Validation loss: 2.6166129394244124

Epoch: 5| Step: 10
Training loss: 2.2373688220977783
Validation loss: 2.6170650400141233

Epoch: 67| Step: 0
Training loss: 2.499054431915283
Validation loss: 2.615223987128145

Epoch: 5| Step: 1
Training loss: 2.6388838291168213
Validation loss: 2.615866530326105

Epoch: 5| Step: 2
Training loss: 2.0593295097351074
Validation loss: 2.6174788654491468

Epoch: 5| Step: 3
Training loss: 3.0680878162384033
Validation loss: 2.6133899534902265

Epoch: 5| Step: 4
Training loss: 3.4324772357940674
Validation loss: 2.6104513240116898

Epoch: 5| Step: 5
Training loss: 2.5109200477600098
Validation loss: 2.6088718470706733

Epoch: 5| Step: 6
Training loss: 3.2644362449645996
Validation loss: 2.604852955828431

Epoch: 5| Step: 7
Training loss: 2.5777766704559326
Validation loss: 2.605486290429228

Epoch: 5| Step: 8
Training loss: 2.8003628253936768
Validation loss: 2.604044952700215

Epoch: 5| Step: 9
Training loss: 2.8169405460357666
Validation loss: 2.606617504550565

Epoch: 5| Step: 10
Training loss: 2.704718828201294
Validation loss: 2.6034663108087357

Epoch: 68| Step: 0
Training loss: 2.356468915939331
Validation loss: 2.60963229081964

Epoch: 5| Step: 1
Training loss: 2.9844253063201904
Validation loss: 2.6067123797632035

Epoch: 5| Step: 2
Training loss: 2.8526453971862793
Validation loss: 2.611576457177439

Epoch: 5| Step: 3
Training loss: 2.728039503097534
Validation loss: 2.603563344606789

Epoch: 5| Step: 4
Training loss: 2.530691623687744
Validation loss: 2.6097896611818703

Epoch: 5| Step: 5
Training loss: 3.1207408905029297
Validation loss: 2.6071550102644068

Epoch: 5| Step: 6
Training loss: 2.587669849395752
Validation loss: 2.604838304622199

Epoch: 5| Step: 7
Training loss: 2.9504106044769287
Validation loss: 2.606992031938286

Epoch: 5| Step: 8
Training loss: 2.7403087615966797
Validation loss: 2.607066087825324

Epoch: 5| Step: 9
Training loss: 2.5409741401672363
Validation loss: 2.6058213762057725

Epoch: 5| Step: 10
Training loss: 2.9777519702911377
Validation loss: 2.6022959601494575

Epoch: 69| Step: 0
Training loss: 2.32661771774292
Validation loss: 2.6062645425078688

Epoch: 5| Step: 1
Training loss: 2.6170907020568848
Validation loss: 2.6009333646425636

Epoch: 5| Step: 2
Training loss: 2.9229214191436768
Validation loss: 2.60248423904501

Epoch: 5| Step: 3
Training loss: 3.7422542572021484
Validation loss: 2.5975824607315885

Epoch: 5| Step: 4
Training loss: 1.9384536743164062
Validation loss: 2.600216806575816

Epoch: 5| Step: 5
Training loss: 2.1358466148376465
Validation loss: 2.6000222339425036

Epoch: 5| Step: 6
Training loss: 2.619178295135498
Validation loss: 2.6000663183068715

Epoch: 5| Step: 7
Training loss: 2.951052665710449
Validation loss: 2.5964681153656333

Epoch: 5| Step: 8
Training loss: 4.048379421234131
Validation loss: 2.5971992631112375

Epoch: 5| Step: 9
Training loss: 2.323676586151123
Validation loss: 2.5971540866359586

Epoch: 5| Step: 10
Training loss: 2.654520273208618
Validation loss: 2.5975824658588698

Epoch: 70| Step: 0
Training loss: 2.865088701248169
Validation loss: 2.596524974351288

Epoch: 5| Step: 1
Training loss: 2.2946436405181885
Validation loss: 2.5958403413013746

Epoch: 5| Step: 2
Training loss: 2.4163804054260254
Validation loss: 2.6013728700658327

Epoch: 5| Step: 3
Training loss: 2.5749807357788086
Validation loss: 2.602063189270676

Epoch: 5| Step: 4
Training loss: 2.1716275215148926
Validation loss: 2.6029292229683167

Epoch: 5| Step: 5
Training loss: 3.9691596031188965
Validation loss: 2.5971462060046453

Epoch: 5| Step: 6
Training loss: 2.5942153930664062
Validation loss: 2.594825711301578

Epoch: 5| Step: 7
Training loss: 2.310788154602051
Validation loss: 2.594099824146558

Epoch: 5| Step: 8
Training loss: 2.467419385910034
Validation loss: 2.5934621287930395

Epoch: 5| Step: 9
Training loss: 3.6128058433532715
Validation loss: 2.5952506962642876

Epoch: 5| Step: 10
Training loss: 3.1277897357940674
Validation loss: 2.596000909805298

Epoch: 71| Step: 0
Training loss: 3.0445363521575928
Validation loss: 2.5953830647212204

Epoch: 5| Step: 1
Training loss: 2.7136576175689697
Validation loss: 2.594393294344666

Epoch: 5| Step: 2
Training loss: 2.5482518672943115
Validation loss: 2.591465873102988

Epoch: 5| Step: 3
Training loss: 2.3478658199310303
Validation loss: 2.5935919438638995

Epoch: 5| Step: 4
Training loss: 2.4150567054748535
Validation loss: 2.5926827974216913

Epoch: 5| Step: 5
Training loss: 2.269754409790039
Validation loss: 2.6006901315463486

Epoch: 5| Step: 6
Training loss: 2.7386410236358643
Validation loss: 2.6093331818939536

Epoch: 5| Step: 7
Training loss: 3.677684783935547
Validation loss: 2.618047447614772

Epoch: 5| Step: 8
Training loss: 2.851867198944092
Validation loss: 2.6136621018891693

Epoch: 5| Step: 9
Training loss: 2.8829100131988525
Validation loss: 2.6027618044166156

Epoch: 5| Step: 10
Training loss: 2.8857760429382324
Validation loss: 2.5895790105224936

Epoch: 72| Step: 0
Training loss: 2.648642063140869
Validation loss: 2.586484347620318

Epoch: 5| Step: 1
Training loss: 2.570335865020752
Validation loss: 2.584980385277861

Epoch: 5| Step: 2
Training loss: 3.6013362407684326
Validation loss: 2.5884018213518205

Epoch: 5| Step: 3
Training loss: 2.162814140319824
Validation loss: 2.5925261359060965

Epoch: 5| Step: 4
Training loss: 2.941756248474121
Validation loss: 2.598500726043537

Epoch: 5| Step: 5
Training loss: 3.118722438812256
Validation loss: 2.5968595999543385

Epoch: 5| Step: 6
Training loss: 3.362778425216675
Validation loss: 2.5968511437857025

Epoch: 5| Step: 7
Training loss: 1.653746247291565
Validation loss: 2.598843343796269

Epoch: 5| Step: 8
Training loss: 2.901474952697754
Validation loss: 2.5962518825325915

Epoch: 5| Step: 9
Training loss: 2.7904927730560303
Validation loss: 2.5938350769781295

Epoch: 5| Step: 10
Training loss: 2.58719801902771
Validation loss: 2.5972904005358295

Epoch: 73| Step: 0
Training loss: 1.862401008605957
Validation loss: 2.5986736871862925

Epoch: 5| Step: 1
Training loss: 2.251932382583618
Validation loss: 2.6054694011647213

Epoch: 5| Step: 2
Training loss: 2.977903366088867
Validation loss: 2.6030872483407297

Epoch: 5| Step: 3
Training loss: 3.2127742767333984
Validation loss: 2.594005546262187

Epoch: 5| Step: 4
Training loss: 3.5046935081481934
Validation loss: 2.5803726975635817

Epoch: 5| Step: 5
Training loss: 2.8563010692596436
Validation loss: 2.5774477015259447

Epoch: 5| Step: 6
Training loss: 2.3570098876953125
Validation loss: 2.578047434488932

Epoch: 5| Step: 7
Training loss: 2.747772693634033
Validation loss: 2.578558744922761

Epoch: 5| Step: 8
Training loss: 2.955933094024658
Validation loss: 2.5829180927686792

Epoch: 5| Step: 9
Training loss: 2.375990390777588
Validation loss: 2.589993594795145

Epoch: 5| Step: 10
Training loss: 3.23696231842041
Validation loss: 2.5935240663507932

Epoch: 74| Step: 0
Training loss: 2.1592681407928467
Validation loss: 2.5919885020102225

Epoch: 5| Step: 1
Training loss: 2.801130771636963
Validation loss: 2.601044624082504

Epoch: 5| Step: 2
Training loss: 2.9820120334625244
Validation loss: 2.6050296368137484

Epoch: 5| Step: 3
Training loss: 2.9792251586914062
Validation loss: 2.5951214682671333

Epoch: 5| Step: 4
Training loss: 2.7020201683044434
Validation loss: 2.588795095361689

Epoch: 5| Step: 5
Training loss: 2.8987388610839844
Validation loss: 2.588294547091248

Epoch: 5| Step: 6
Training loss: 3.4331161975860596
Validation loss: 2.577859265829927

Epoch: 5| Step: 7
Training loss: 2.3026041984558105
Validation loss: 2.5789949073586413

Epoch: 5| Step: 8
Training loss: 2.5389087200164795
Validation loss: 2.5798340356478127

Epoch: 5| Step: 9
Training loss: 2.3881397247314453
Validation loss: 2.5795408884684243

Epoch: 5| Step: 10
Training loss: 3.0728611946105957
Validation loss: 2.5786412710784585

Epoch: 75| Step: 0
Training loss: 2.3158185482025146
Validation loss: 2.5790880572411323

Epoch: 5| Step: 1
Training loss: 3.2712249755859375
Validation loss: 2.5776673670737975

Epoch: 5| Step: 2
Training loss: 2.875793933868408
Validation loss: 2.5774282255480365

Epoch: 5| Step: 3
Training loss: 2.7523086071014404
Validation loss: 2.5793486282389653

Epoch: 5| Step: 4
Training loss: 2.853593349456787
Validation loss: 2.5784209646204466

Epoch: 5| Step: 5
Training loss: 2.6817786693573
Validation loss: 2.5795601593550814

Epoch: 5| Step: 6
Training loss: 3.1209049224853516
Validation loss: 2.5808092368546354

Epoch: 5| Step: 7
Training loss: 2.7953243255615234
Validation loss: 2.5781775982149187

Epoch: 5| Step: 8
Training loss: 2.479065418243408
Validation loss: 2.5795276754645893

Epoch: 5| Step: 9
Training loss: 2.3855197429656982
Validation loss: 2.575595291711951

Epoch: 5| Step: 10
Training loss: 2.5907540321350098
Validation loss: 2.5742806234667377

Epoch: 76| Step: 0
Training loss: 2.691716194152832
Validation loss: 2.5718351692281742

Epoch: 5| Step: 1
Training loss: 2.9228172302246094
Validation loss: 2.5715253430028118

Epoch: 5| Step: 2
Training loss: 3.6236870288848877
Validation loss: 2.570372866046044

Epoch: 5| Step: 3
Training loss: 2.7018208503723145
Validation loss: 2.5730635786569245

Epoch: 5| Step: 4
Training loss: 2.7838568687438965
Validation loss: 2.5759620692140315

Epoch: 5| Step: 5
Training loss: 2.3573336601257324
Validation loss: 2.5810578356507006

Epoch: 5| Step: 6
Training loss: 2.2295432090759277
Validation loss: 2.572821904254216

Epoch: 5| Step: 7
Training loss: 2.4111456871032715
Validation loss: 2.56732375391068

Epoch: 5| Step: 8
Training loss: 3.1571526527404785
Validation loss: 2.5650585569361204

Epoch: 5| Step: 9
Training loss: 2.8249332904815674
Validation loss: 2.5713151398525445

Epoch: 5| Step: 10
Training loss: 2.3745455741882324
Validation loss: 2.5780577736516155

Epoch: 77| Step: 0
Training loss: 2.4567220211029053
Validation loss: 2.587602348737819

Epoch: 5| Step: 1
Training loss: 3.56657338142395
Validation loss: 2.5910772456917712

Epoch: 5| Step: 2
Training loss: 2.519573926925659
Validation loss: 2.597577738505538

Epoch: 5| Step: 3
Training loss: 3.5449721813201904
Validation loss: 2.5957522866546467

Epoch: 5| Step: 4
Training loss: 2.2990856170654297
Validation loss: 2.588153226401216

Epoch: 5| Step: 5
Training loss: 2.895364284515381
Validation loss: 2.575837148133145

Epoch: 5| Step: 6
Training loss: 1.9482221603393555
Validation loss: 2.569590517269668

Epoch: 5| Step: 7
Training loss: 2.587970018386841
Validation loss: 2.5650974114735923

Epoch: 5| Step: 8
Training loss: 2.7605767250061035
Validation loss: 2.569137478387484

Epoch: 5| Step: 9
Training loss: 2.3258614540100098
Validation loss: 2.568988843630719

Epoch: 5| Step: 10
Training loss: 3.350071907043457
Validation loss: 2.564898390923777

Epoch: 78| Step: 0
Training loss: 2.663099765777588
Validation loss: 2.565760753488028

Epoch: 5| Step: 1
Training loss: 2.446964740753174
Validation loss: 2.5668364212077153

Epoch: 5| Step: 2
Training loss: 2.552339553833008
Validation loss: 2.564957921222974

Epoch: 5| Step: 3
Training loss: 2.891711950302124
Validation loss: 2.564818015662573

Epoch: 5| Step: 4
Training loss: 2.428532361984253
Validation loss: 2.5634982073178856

Epoch: 5| Step: 5
Training loss: 3.22023344039917
Validation loss: 2.5632349496246665

Epoch: 5| Step: 6
Training loss: 2.6342289447784424
Validation loss: 2.5600205826502975

Epoch: 5| Step: 7
Training loss: 2.965294361114502
Validation loss: 2.55920321454284

Epoch: 5| Step: 8
Training loss: 2.9040260314941406
Validation loss: 2.5588986540353424

Epoch: 5| Step: 9
Training loss: 2.6636245250701904
Validation loss: 2.558740149262131

Epoch: 5| Step: 10
Training loss: 2.6379737854003906
Validation loss: 2.5566502489069456

Epoch: 79| Step: 0
Training loss: 2.557359457015991
Validation loss: 2.5573397349285822

Epoch: 5| Step: 1
Training loss: 2.4055304527282715
Validation loss: 2.5583876486747497

Epoch: 5| Step: 2
Training loss: 2.3926095962524414
Validation loss: 2.5574785073598227

Epoch: 5| Step: 3
Training loss: 1.9282200336456299
Validation loss: 2.5569492206778577

Epoch: 5| Step: 4
Training loss: 3.227458953857422
Validation loss: 2.5553147203178814

Epoch: 5| Step: 5
Training loss: 3.011369228363037
Validation loss: 2.553439471029466

Epoch: 5| Step: 6
Training loss: 2.1884970664978027
Validation loss: 2.556964510230608

Epoch: 5| Step: 7
Training loss: 3.4358038902282715
Validation loss: 2.556220446863482

Epoch: 5| Step: 8
Training loss: 2.596872329711914
Validation loss: 2.5559699791733936

Epoch: 5| Step: 9
Training loss: 3.0337562561035156
Validation loss: 2.555835964859173

Epoch: 5| Step: 10
Training loss: 3.299790143966675
Validation loss: 2.554807657836586

Epoch: 80| Step: 0
Training loss: 2.91471529006958
Validation loss: 2.5536142472297914

Epoch: 5| Step: 1
Training loss: 2.0496606826782227
Validation loss: 2.5537743030055875

Epoch: 5| Step: 2
Training loss: 2.662752866744995
Validation loss: 2.5531194748417025

Epoch: 5| Step: 3
Training loss: 2.425020217895508
Validation loss: 2.5529073643428024

Epoch: 5| Step: 4
Training loss: 2.8106815814971924
Validation loss: 2.5508804218743437

Epoch: 5| Step: 5
Training loss: 2.934720277786255
Validation loss: 2.554246493565139

Epoch: 5| Step: 6
Training loss: 3.0661587715148926
Validation loss: 2.5507232066123717

Epoch: 5| Step: 7
Training loss: 3.064054489135742
Validation loss: 2.5505002596045054

Epoch: 5| Step: 8
Training loss: 2.715348482131958
Validation loss: 2.550399608509515

Epoch: 5| Step: 9
Training loss: 3.0697453022003174
Validation loss: 2.5521699972050165

Epoch: 5| Step: 10
Training loss: 2.142568826675415
Validation loss: 2.5540658863641883

Epoch: 81| Step: 0
Training loss: 3.3255043029785156
Validation loss: 2.5526865913021948

Epoch: 5| Step: 1
Training loss: 2.4878737926483154
Validation loss: 2.5506589540871243

Epoch: 5| Step: 2
Training loss: 2.793234348297119
Validation loss: 2.549310671385898

Epoch: 5| Step: 3
Training loss: 3.0746512413024902
Validation loss: 2.5529847862899944

Epoch: 5| Step: 4
Training loss: 2.4036946296691895
Validation loss: 2.553477466747325

Epoch: 5| Step: 5
Training loss: 3.412022352218628
Validation loss: 2.553229872898389

Epoch: 5| Step: 6
Training loss: 2.7388272285461426
Validation loss: 2.5520843869896344

Epoch: 5| Step: 7
Training loss: 3.1519970893859863
Validation loss: 2.5533403555552163

Epoch: 5| Step: 8
Training loss: 2.8842029571533203
Validation loss: 2.552660813895605

Epoch: 5| Step: 9
Training loss: 1.3376471996307373
Validation loss: 2.5500814401975243

Epoch: 5| Step: 10
Training loss: 2.262636184692383
Validation loss: 2.548767876881425

Epoch: 82| Step: 0
Training loss: 3.0560011863708496
Validation loss: 2.5484838716445433

Epoch: 5| Step: 1
Training loss: 2.326140880584717
Validation loss: 2.549044901324857

Epoch: 5| Step: 2
Training loss: 2.438082218170166
Validation loss: 2.5449497417737077

Epoch: 5| Step: 3
Training loss: 3.117366313934326
Validation loss: 2.542874582352177

Epoch: 5| Step: 4
Training loss: 2.6511430740356445
Validation loss: 2.5424937637903358

Epoch: 5| Step: 5
Training loss: 2.5935323238372803
Validation loss: 2.5428504815665622

Epoch: 5| Step: 6
Training loss: 1.6629507541656494
Validation loss: 2.5389528479627383

Epoch: 5| Step: 7
Training loss: 2.7873971462249756
Validation loss: 2.5409723661279164

Epoch: 5| Step: 8
Training loss: 2.9536423683166504
Validation loss: 2.54164558072244

Epoch: 5| Step: 9
Training loss: 3.401535749435425
Validation loss: 2.540416781620313

Epoch: 5| Step: 10
Training loss: 2.929270029067993
Validation loss: 2.5408378313946467

Epoch: 83| Step: 0
Training loss: 2.389193534851074
Validation loss: 2.5397536139334402

Epoch: 5| Step: 1
Training loss: 2.5632033348083496
Validation loss: 2.5425098173079954

Epoch: 5| Step: 2
Training loss: 2.949850082397461
Validation loss: 2.543966434335196

Epoch: 5| Step: 3
Training loss: 3.2067551612854004
Validation loss: 2.543844658841369

Epoch: 5| Step: 4
Training loss: 2.9817910194396973
Validation loss: 2.5397172871456353

Epoch: 5| Step: 5
Training loss: 2.545581817626953
Validation loss: 2.5374146046177035

Epoch: 5| Step: 6
Training loss: 3.1255364418029785
Validation loss: 2.53800102203123

Epoch: 5| Step: 7
Training loss: 2.458583354949951
Validation loss: 2.538243341189559

Epoch: 5| Step: 8
Training loss: 2.965670347213745
Validation loss: 2.538005967294016

Epoch: 5| Step: 9
Training loss: 2.635512113571167
Validation loss: 2.5369069730081866

Epoch: 5| Step: 10
Training loss: 1.9352002143859863
Validation loss: 2.5344858579738165

Epoch: 84| Step: 0
Training loss: 2.983745574951172
Validation loss: 2.536611073760576

Epoch: 5| Step: 1
Training loss: 2.309150218963623
Validation loss: 2.536974322411322

Epoch: 5| Step: 2
Training loss: 3.522066593170166
Validation loss: 2.5375443197065786

Epoch: 5| Step: 3
Training loss: 3.482475996017456
Validation loss: 2.5455383254635717

Epoch: 5| Step: 4
Training loss: 2.2229559421539307
Validation loss: 2.5504237016042075

Epoch: 5| Step: 5
Training loss: 3.0426571369171143
Validation loss: 2.567785629662134

Epoch: 5| Step: 6
Training loss: 2.3697092533111572
Validation loss: 2.5729109548753306

Epoch: 5| Step: 7
Training loss: 1.6485105752944946
Validation loss: 2.5848848319822744

Epoch: 5| Step: 8
Training loss: 2.376711130142212
Validation loss: 2.5788577782210482

Epoch: 5| Step: 9
Training loss: 3.3555679321289062
Validation loss: 2.555869689551733

Epoch: 5| Step: 10
Training loss: 2.663752794265747
Validation loss: 2.5427409090021604

Epoch: 85| Step: 0
Training loss: 2.4788012504577637
Validation loss: 2.53593260242093

Epoch: 5| Step: 1
Training loss: 2.411978244781494
Validation loss: 2.5376676897848807

Epoch: 5| Step: 2
Training loss: 2.3130669593811035
Validation loss: 2.539111065608199

Epoch: 5| Step: 3
Training loss: 2.2883687019348145
Validation loss: 2.5487404843812347

Epoch: 5| Step: 4
Training loss: 2.7998604774475098
Validation loss: 2.550834068688013

Epoch: 5| Step: 5
Training loss: 3.154364824295044
Validation loss: 2.542653781111522

Epoch: 5| Step: 6
Training loss: 3.3641247749328613
Validation loss: 2.537080962170837

Epoch: 5| Step: 7
Training loss: 3.4904274940490723
Validation loss: 2.5360979226327713

Epoch: 5| Step: 8
Training loss: 3.1802074909210205
Validation loss: 2.5346982863641556

Epoch: 5| Step: 9
Training loss: 2.5530967712402344
Validation loss: 2.5335276178134385

Epoch: 5| Step: 10
Training loss: 1.6902129650115967
Validation loss: 2.534834723318777

Epoch: 86| Step: 0
Training loss: 3.088693380355835
Validation loss: 2.537766028476018

Epoch: 5| Step: 1
Training loss: 2.4931271076202393
Validation loss: 2.537721562129195

Epoch: 5| Step: 2
Training loss: 3.648719072341919
Validation loss: 2.54061544838772

Epoch: 5| Step: 3
Training loss: 2.6704440116882324
Validation loss: 2.545221418462774

Epoch: 5| Step: 4
Training loss: 2.2583584785461426
Validation loss: 2.535010778775779

Epoch: 5| Step: 5
Training loss: 2.5257599353790283
Validation loss: 2.535051585525595

Epoch: 5| Step: 6
Training loss: 2.8420844078063965
Validation loss: 2.5309311779596473

Epoch: 5| Step: 7
Training loss: 1.996087670326233
Validation loss: 2.531018828832975

Epoch: 5| Step: 8
Training loss: 2.676706314086914
Validation loss: 2.529674037810295

Epoch: 5| Step: 9
Training loss: 2.806760311126709
Validation loss: 2.5261235929304555

Epoch: 5| Step: 10
Training loss: 2.7703471183776855
Validation loss: 2.525583767121838

Epoch: 87| Step: 0
Training loss: 2.935671329498291
Validation loss: 2.527360439300537

Epoch: 5| Step: 1
Training loss: 2.9916043281555176
Validation loss: 2.5319472794891684

Epoch: 5| Step: 2
Training loss: 2.290285587310791
Validation loss: 2.53103272632886

Epoch: 5| Step: 3
Training loss: 2.4027647972106934
Validation loss: 2.5276933229097756

Epoch: 5| Step: 4
Training loss: 2.214047431945801
Validation loss: 2.5259134102893133

Epoch: 5| Step: 5
Training loss: 3.1403017044067383
Validation loss: 2.522070533485823

Epoch: 5| Step: 6
Training loss: 3.4013640880584717
Validation loss: 2.5249328946554535

Epoch: 5| Step: 7
Training loss: 2.1409778594970703
Validation loss: 2.5235979762128604

Epoch: 5| Step: 8
Training loss: 2.6300761699676514
Validation loss: 2.5249009542567755

Epoch: 5| Step: 9
Training loss: 2.5992798805236816
Validation loss: 2.5297081291034655

Epoch: 5| Step: 10
Training loss: 3.150280237197876
Validation loss: 2.5320513171534382

Epoch: 88| Step: 0
Training loss: 2.796356439590454
Validation loss: 2.5382907236776044

Epoch: 5| Step: 1
Training loss: 2.8248496055603027
Validation loss: 2.546227168011409

Epoch: 5| Step: 2
Training loss: 2.338254690170288
Validation loss: 2.553573775035079

Epoch: 5| Step: 3
Training loss: 2.993704080581665
Validation loss: 2.5553988872035855

Epoch: 5| Step: 4
Training loss: 2.3980274200439453
Validation loss: 2.5524484598508446

Epoch: 5| Step: 5
Training loss: 2.8680081367492676
Validation loss: 2.535891435479605

Epoch: 5| Step: 6
Training loss: 2.886739730834961
Validation loss: 2.5274681173345095

Epoch: 5| Step: 7
Training loss: 2.878328561782837
Validation loss: 2.519569399536297

Epoch: 5| Step: 8
Training loss: 2.6918976306915283
Validation loss: 2.5234875089378765

Epoch: 5| Step: 9
Training loss: 2.348639726638794
Validation loss: 2.5251728129643265

Epoch: 5| Step: 10
Training loss: 2.920015573501587
Validation loss: 2.5282326667539534

Epoch: 89| Step: 0
Training loss: 3.3580684661865234
Validation loss: 2.5282086967140116

Epoch: 5| Step: 1
Training loss: 1.7231706380844116
Validation loss: 2.534038761610626

Epoch: 5| Step: 2
Training loss: 1.8653249740600586
Validation loss: 2.5310998347497757

Epoch: 5| Step: 3
Training loss: 3.2422783374786377
Validation loss: 2.529363621947586

Epoch: 5| Step: 4
Training loss: 2.4048030376434326
Validation loss: 2.523383355909778

Epoch: 5| Step: 5
Training loss: 2.621758222579956
Validation loss: 2.5187096172763455

Epoch: 5| Step: 6
Training loss: 3.011183500289917
Validation loss: 2.521074010479835

Epoch: 5| Step: 7
Training loss: 3.1578125953674316
Validation loss: 2.5294076999028525

Epoch: 5| Step: 8
Training loss: 2.325528860092163
Validation loss: 2.5391275395629225

Epoch: 5| Step: 9
Training loss: 3.1211764812469482
Validation loss: 2.5578237502805647

Epoch: 5| Step: 10
Training loss: 3.0908987522125244
Validation loss: 2.5630430765049432

Epoch: 90| Step: 0
Training loss: 2.195159435272217
Validation loss: 2.561706578859719

Epoch: 5| Step: 1
Training loss: 2.597111225128174
Validation loss: 2.549627575823056

Epoch: 5| Step: 2
Training loss: 2.786301851272583
Validation loss: 2.5393086812829457

Epoch: 5| Step: 3
Training loss: 2.17096209526062
Validation loss: 2.5255454304397746

Epoch: 5| Step: 4
Training loss: 2.530085563659668
Validation loss: 2.5195302194164646

Epoch: 5| Step: 5
Training loss: 2.700237274169922
Validation loss: 2.519525299790085

Epoch: 5| Step: 6
Training loss: 2.5722362995147705
Validation loss: 2.5132520685913744

Epoch: 5| Step: 7
Training loss: 2.8605082035064697
Validation loss: 2.513315180296539

Epoch: 5| Step: 8
Training loss: 3.7387733459472656
Validation loss: 2.513054975899317

Epoch: 5| Step: 9
Training loss: 2.8919551372528076
Validation loss: 2.5118150249604256

Epoch: 5| Step: 10
Training loss: 2.712442398071289
Validation loss: 2.5132875288686445

Epoch: 91| Step: 0
Training loss: 2.449497699737549
Validation loss: 2.512617690588838

Epoch: 5| Step: 1
Training loss: 2.723902463912964
Validation loss: 2.512072960535685

Epoch: 5| Step: 2
Training loss: 4.11782169342041
Validation loss: 2.5132997651253977

Epoch: 5| Step: 3
Training loss: 2.3437881469726562
Validation loss: 2.51468155717337

Epoch: 5| Step: 4
Training loss: 3.0098414421081543
Validation loss: 2.5151934726263887

Epoch: 5| Step: 5
Training loss: 2.8131542205810547
Validation loss: 2.515582899893484

Epoch: 5| Step: 6
Training loss: 1.9070193767547607
Validation loss: 2.5157440311165264

Epoch: 5| Step: 7
Training loss: 2.7671356201171875
Validation loss: 2.5130368483963834

Epoch: 5| Step: 8
Training loss: 1.8206462860107422
Validation loss: 2.51189390818278

Epoch: 5| Step: 9
Training loss: 2.852907419204712
Validation loss: 2.5123760392588954

Epoch: 5| Step: 10
Training loss: 2.956632614135742
Validation loss: 2.5130392095094085

Epoch: 92| Step: 0
Training loss: 2.7684249877929688
Validation loss: 2.515850538848549

Epoch: 5| Step: 1
Training loss: 2.1528103351593018
Validation loss: 2.518848813990111

Epoch: 5| Step: 2
Training loss: 3.108301877975464
Validation loss: 2.519488726892779

Epoch: 5| Step: 3
Training loss: 3.1382994651794434
Validation loss: 2.5183770348948817

Epoch: 5| Step: 4
Training loss: 2.6963603496551514
Validation loss: 2.526528481514223

Epoch: 5| Step: 5
Training loss: 1.8374496698379517
Validation loss: 2.5316750426446237

Epoch: 5| Step: 6
Training loss: 2.9134387969970703
Validation loss: 2.5346087178876324

Epoch: 5| Step: 7
Training loss: 2.9043924808502197
Validation loss: 2.5353277960131244

Epoch: 5| Step: 8
Training loss: 2.3646886348724365
Validation loss: 2.527420264418407

Epoch: 5| Step: 9
Training loss: 2.9545910358428955
Validation loss: 2.5202811379586496

Epoch: 5| Step: 10
Training loss: 2.942178249359131
Validation loss: 2.5136700778879146

Epoch: 93| Step: 0
Training loss: 2.524606227874756
Validation loss: 2.5088465700867357

Epoch: 5| Step: 1
Training loss: 3.1442654132843018
Validation loss: 2.508240338294737

Epoch: 5| Step: 2
Training loss: 2.666271924972534
Validation loss: 2.506452606570336

Epoch: 5| Step: 3
Training loss: 2.859968662261963
Validation loss: 2.505431818705733

Epoch: 5| Step: 4
Training loss: 2.470160961151123
Validation loss: 2.503869641211725

Epoch: 5| Step: 5
Training loss: 2.543882131576538
Validation loss: 2.5046786031415387

Epoch: 5| Step: 6
Training loss: 3.587146759033203
Validation loss: 2.5078161249878588

Epoch: 5| Step: 7
Training loss: 1.7304290533065796
Validation loss: 2.5065941374789

Epoch: 5| Step: 8
Training loss: 3.254093647003174
Validation loss: 2.5054940408275974

Epoch: 5| Step: 9
Training loss: 2.0084567070007324
Validation loss: 2.503697572215911

Epoch: 5| Step: 10
Training loss: 2.9216179847717285
Validation loss: 2.5037995769131567

Epoch: 94| Step: 0
Training loss: 2.4698638916015625
Validation loss: 2.503345510011078

Epoch: 5| Step: 1
Training loss: 3.0305562019348145
Validation loss: 2.5077749195919243

Epoch: 5| Step: 2
Training loss: 2.387132167816162
Validation loss: 2.5079349164039857

Epoch: 5| Step: 3
Training loss: 2.3769781589508057
Validation loss: 2.50594320348514

Epoch: 5| Step: 4
Training loss: 3.3211770057678223
Validation loss: 2.50625971312164

Epoch: 5| Step: 5
Training loss: 2.817734718322754
Validation loss: 2.5040919268003075

Epoch: 5| Step: 6
Training loss: 2.461379051208496
Validation loss: 2.5035388546605266

Epoch: 5| Step: 7
Training loss: 2.4813952445983887
Validation loss: 2.5038840334902526

Epoch: 5| Step: 8
Training loss: 1.9074608087539673
Validation loss: 2.5014238280634724

Epoch: 5| Step: 9
Training loss: 3.046093702316284
Validation loss: 2.502631224611754

Epoch: 5| Step: 10
Training loss: 3.4249393939971924
Validation loss: 2.50274311342547

Epoch: 95| Step: 0
Training loss: 3.1296751499176025
Validation loss: 2.50162999860702

Epoch: 5| Step: 1
Training loss: 2.0250916481018066
Validation loss: 2.499605263433149

Epoch: 5| Step: 2
Training loss: 2.828172206878662
Validation loss: 2.503527820751231

Epoch: 5| Step: 3
Training loss: 3.3751492500305176
Validation loss: 2.503575531385278

Epoch: 5| Step: 4
Training loss: 2.3317642211914062
Validation loss: 2.5056576190456266

Epoch: 5| Step: 5
Training loss: 1.7546457052230835
Validation loss: 2.5064094912621284

Epoch: 5| Step: 6
Training loss: 3.4300246238708496
Validation loss: 2.5080717737956713

Epoch: 5| Step: 7
Training loss: 2.7976157665252686
Validation loss: 2.5093683991380917

Epoch: 5| Step: 8
Training loss: 2.6347992420196533
Validation loss: 2.5141296130354687

Epoch: 5| Step: 9
Training loss: 2.708543300628662
Validation loss: 2.5173973293714624

Epoch: 5| Step: 10
Training loss: 2.6001596450805664
Validation loss: 2.5191085851320656

Epoch: 96| Step: 0
Training loss: 2.371386766433716
Validation loss: 2.5139247473850044

Epoch: 5| Step: 1
Training loss: 2.863323211669922
Validation loss: 2.5068748638194096

Epoch: 5| Step: 2
Training loss: 2.331841230392456
Validation loss: 2.4992127597972913

Epoch: 5| Step: 3
Training loss: 2.7554171085357666
Validation loss: 2.4993062224439395

Epoch: 5| Step: 4
Training loss: 2.6697545051574707
Validation loss: 2.4961695030171382

Epoch: 5| Step: 5
Training loss: 2.5201003551483154
Validation loss: 2.4978171471626527

Epoch: 5| Step: 6
Training loss: 2.0830235481262207
Validation loss: 2.4982860959986204

Epoch: 5| Step: 7
Training loss: 2.922025203704834
Validation loss: 2.498038971295921

Epoch: 5| Step: 8
Training loss: 3.7968196868896484
Validation loss: 2.4959786271536224

Epoch: 5| Step: 9
Training loss: 2.0996251106262207
Validation loss: 2.4979178802941435

Epoch: 5| Step: 10
Training loss: 3.252078056335449
Validation loss: 2.4958830341216056

Epoch: 97| Step: 0
Training loss: 2.4021944999694824
Validation loss: 2.494064692528017

Epoch: 5| Step: 1
Training loss: 2.962970733642578
Validation loss: 2.4949252297801356

Epoch: 5| Step: 2
Training loss: 2.5250306129455566
Validation loss: 2.4977083129267537

Epoch: 5| Step: 3
Training loss: 2.4723846912384033
Validation loss: 2.494428647461758

Epoch: 5| Step: 4
Training loss: 2.7603485584259033
Validation loss: 2.4939367027692896

Epoch: 5| Step: 5
Training loss: 2.5329370498657227
Validation loss: 2.4925814623473794

Epoch: 5| Step: 6
Training loss: 2.7946650981903076
Validation loss: 2.4922729563969437

Epoch: 5| Step: 7
Training loss: 2.6479499340057373
Validation loss: 2.49193989076922

Epoch: 5| Step: 8
Training loss: 2.8457188606262207
Validation loss: 2.498555475665677

Epoch: 5| Step: 9
Training loss: 3.4730963706970215
Validation loss: 2.4973562327764367

Epoch: 5| Step: 10
Training loss: 2.0686278343200684
Validation loss: 2.4964567076775337

Epoch: 98| Step: 0
Training loss: 2.863828659057617
Validation loss: 2.502658292811404

Epoch: 5| Step: 1
Training loss: 2.2604832649230957
Validation loss: 2.4969539847425235

Epoch: 5| Step: 2
Training loss: 2.5038468837738037
Validation loss: 2.491086636820147

Epoch: 5| Step: 3
Training loss: 2.7864649295806885
Validation loss: 2.489841345817812

Epoch: 5| Step: 4
Training loss: 3.529982089996338
Validation loss: 2.4927119349920623

Epoch: 5| Step: 5
Training loss: 2.080373764038086
Validation loss: 2.4922299897798927

Epoch: 5| Step: 6
Training loss: 3.127316474914551
Validation loss: 2.4950533387481526

Epoch: 5| Step: 7
Training loss: 2.5405728816986084
Validation loss: 2.4931650302743398

Epoch: 5| Step: 8
Training loss: 2.7088866233825684
Validation loss: 2.492183498156968

Epoch: 5| Step: 9
Training loss: 2.4703736305236816
Validation loss: 2.4906341645025436

Epoch: 5| Step: 10
Training loss: 2.782266139984131
Validation loss: 2.4931171248036046

Epoch: 99| Step: 0
Training loss: 2.821226119995117
Validation loss: 2.493653025678409

Epoch: 5| Step: 1
Training loss: 3.274583101272583
Validation loss: 2.48998357916391

Epoch: 5| Step: 2
Training loss: 3.2985358238220215
Validation loss: 2.4938658027238745

Epoch: 5| Step: 3
Training loss: 2.169722080230713
Validation loss: 2.509834292114422

Epoch: 5| Step: 4
Training loss: 1.9302705526351929
Validation loss: 2.512039287115938

Epoch: 5| Step: 5
Training loss: 2.023759126663208
Validation loss: 2.5025423290908977

Epoch: 5| Step: 6
Training loss: 2.919492483139038
Validation loss: 2.4961476428534395

Epoch: 5| Step: 7
Training loss: 2.776487112045288
Validation loss: 2.4879140648790585

Epoch: 5| Step: 8
Training loss: 2.3799800872802734
Validation loss: 2.4898729273068008

Epoch: 5| Step: 9
Training loss: 3.3747894763946533
Validation loss: 2.492477883574783

Epoch: 5| Step: 10
Training loss: 2.6292004585266113
Validation loss: 2.4940087949076006

Epoch: 100| Step: 0
Training loss: 2.5766165256500244
Validation loss: 2.4962445894877114

Epoch: 5| Step: 1
Training loss: 3.0935778617858887
Validation loss: 2.4993035049848658

Epoch: 5| Step: 2
Training loss: 3.0992677211761475
Validation loss: 2.502594893978488

Epoch: 5| Step: 3
Training loss: 2.14402437210083
Validation loss: 2.502611583279025

Epoch: 5| Step: 4
Training loss: 2.433289051055908
Validation loss: 2.5095962760269

Epoch: 5| Step: 5
Training loss: 2.907402515411377
Validation loss: 2.4958425285995647

Epoch: 5| Step: 6
Training loss: 3.198559522628784
Validation loss: 2.4882966497892975

Epoch: 5| Step: 7
Training loss: 2.6966538429260254
Validation loss: 2.487426698848765

Epoch: 5| Step: 8
Training loss: 2.1899285316467285
Validation loss: 2.4872940714641283

Epoch: 5| Step: 9
Training loss: 2.538325548171997
Validation loss: 2.490181828057894

Epoch: 5| Step: 10
Training loss: 2.7662017345428467
Validation loss: 2.4965856036832257

Epoch: 101| Step: 0
Training loss: 2.837895393371582
Validation loss: 2.4967529235347623

Epoch: 5| Step: 1
Training loss: 2.2226829528808594
Validation loss: 2.49097434166939

Epoch: 5| Step: 2
Training loss: 1.922329306602478
Validation loss: 2.490802503401233

Epoch: 5| Step: 3
Training loss: 2.8532700538635254
Validation loss: 2.4878399910465365

Epoch: 5| Step: 4
Training loss: 3.1782867908477783
Validation loss: 2.4859894526902067

Epoch: 5| Step: 5
Training loss: 3.0575149059295654
Validation loss: 2.484721622159404

Epoch: 5| Step: 6
Training loss: 3.0028586387634277
Validation loss: 2.489699512399653

Epoch: 5| Step: 7
Training loss: 2.587043285369873
Validation loss: 2.4896325603608163

Epoch: 5| Step: 8
Training loss: 2.9719154834747314
Validation loss: 2.491738737270396

Epoch: 5| Step: 9
Training loss: 2.2883622646331787
Validation loss: 2.4934015812412387

Epoch: 5| Step: 10
Training loss: 2.5057268142700195
Validation loss: 2.4924054017630954

Epoch: 102| Step: 0
Training loss: 2.730144739151001
Validation loss: 2.4911641818220898

Epoch: 5| Step: 1
Training loss: 2.477537155151367
Validation loss: 2.488941805337065

Epoch: 5| Step: 2
Training loss: 1.888620138168335
Validation loss: 2.486627135225522

Epoch: 5| Step: 3
Training loss: 3.3543877601623535
Validation loss: 2.4863194650219334

Epoch: 5| Step: 4
Training loss: 2.8185343742370605
Validation loss: 2.4880971139477146

Epoch: 5| Step: 5
Training loss: 2.0846543312072754
Validation loss: 2.4968486037305606

Epoch: 5| Step: 6
Training loss: 2.589228630065918
Validation loss: 2.5033639707872943

Epoch: 5| Step: 7
Training loss: 2.969358205795288
Validation loss: 2.520578263908304

Epoch: 5| Step: 8
Training loss: 2.4623055458068848
Validation loss: 2.5312208078240834

Epoch: 5| Step: 9
Training loss: 2.8035290241241455
Validation loss: 2.530414189061811

Epoch: 5| Step: 10
Training loss: 3.522686004638672
Validation loss: 2.5112005305546585

Epoch: 103| Step: 0
Training loss: 2.262274980545044
Validation loss: 2.4892965542372836

Epoch: 5| Step: 1
Training loss: 2.997548818588257
Validation loss: 2.481390678754417

Epoch: 5| Step: 2
Training loss: 3.1469082832336426
Validation loss: 2.477849268144177

Epoch: 5| Step: 3
Training loss: 2.013659954071045
Validation loss: 2.4783349729353383

Epoch: 5| Step: 4
Training loss: 2.5245792865753174
Validation loss: 2.4801928509948072

Epoch: 5| Step: 5
Training loss: 2.4636099338531494
Validation loss: 2.48118995851086

Epoch: 5| Step: 6
Training loss: 3.3775787353515625
Validation loss: 2.478703050203221

Epoch: 5| Step: 7
Training loss: 3.1193041801452637
Validation loss: 2.4741656908424954

Epoch: 5| Step: 8
Training loss: 3.2271926403045654
Validation loss: 2.4772321383158364

Epoch: 5| Step: 9
Training loss: 1.6739568710327148
Validation loss: 2.477969032461925

Epoch: 5| Step: 10
Training loss: 2.6554508209228516
Validation loss: 2.4752869606018066

Epoch: 104| Step: 0
Training loss: 3.2221202850341797
Validation loss: 2.473285944231095

Epoch: 5| Step: 1
Training loss: 2.633535861968994
Validation loss: 2.4738232012717956

Epoch: 5| Step: 2
Training loss: 2.317101001739502
Validation loss: 2.4765207664940947

Epoch: 5| Step: 3
Training loss: 3.4176974296569824
Validation loss: 2.4746151380641486

Epoch: 5| Step: 4
Training loss: 2.2686705589294434
Validation loss: 2.474873891440771

Epoch: 5| Step: 5
Training loss: 2.2500596046447754
Validation loss: 2.4725000704488447

Epoch: 5| Step: 6
Training loss: 3.2609457969665527
Validation loss: 2.4747597094505065

Epoch: 5| Step: 7
Training loss: 2.6474509239196777
Validation loss: 2.471099030586981

Epoch: 5| Step: 8
Training loss: 2.3053231239318848
Validation loss: 2.4827036473058883

Epoch: 5| Step: 9
Training loss: 2.0667078495025635
Validation loss: 2.509622435415945

Epoch: 5| Step: 10
Training loss: 3.2046287059783936
Validation loss: 2.5225382312651603

Epoch: 105| Step: 0
Training loss: 2.0461833477020264
Validation loss: 2.5105153924675396

Epoch: 5| Step: 1
Training loss: 2.0641674995422363
Validation loss: 2.478609117128516

Epoch: 5| Step: 2
Training loss: 2.489015579223633
Validation loss: 2.4767056870204147

Epoch: 5| Step: 3
Training loss: 3.0544703006744385
Validation loss: 2.473446217916345

Epoch: 5| Step: 4
Training loss: 3.0915369987487793
Validation loss: 2.478034157906809

Epoch: 5| Step: 5
Training loss: 2.7871131896972656
Validation loss: 2.476886792849469

Epoch: 5| Step: 6
Training loss: 2.73319673538208
Validation loss: 2.4804973858658985

Epoch: 5| Step: 7
Training loss: 2.3180534839630127
Validation loss: 2.486194020958357

Epoch: 5| Step: 8
Training loss: 2.732090473175049
Validation loss: 2.4842564777661393

Epoch: 5| Step: 9
Training loss: 3.06638765335083
Validation loss: 2.4904018063699045

Epoch: 5| Step: 10
Training loss: 3.0814547538757324
Validation loss: 2.4845913533241517

Epoch: 106| Step: 0
Training loss: 2.6301348209381104
Validation loss: 2.485077711843675

Epoch: 5| Step: 1
Training loss: 2.2120940685272217
Validation loss: 2.485138931582051

Epoch: 5| Step: 2
Training loss: 2.663339138031006
Validation loss: 2.4828672511603243

Epoch: 5| Step: 3
Training loss: 3.3659827709198
Validation loss: 2.4847418826113463

Epoch: 5| Step: 4
Training loss: 3.030113697052002
Validation loss: 2.479214177336744

Epoch: 5| Step: 5
Training loss: 1.9382232427597046
Validation loss: 2.4828920210561445

Epoch: 5| Step: 6
Training loss: 3.546694278717041
Validation loss: 2.4800047515540995

Epoch: 5| Step: 7
Training loss: 2.313847064971924
Validation loss: 2.4833882777921614

Epoch: 5| Step: 8
Training loss: 2.015702724456787
Validation loss: 2.4854221805449455

Epoch: 5| Step: 9
Training loss: 3.0166471004486084
Validation loss: 2.488412441745881

Epoch: 5| Step: 10
Training loss: 2.670316696166992
Validation loss: 2.4873468183702037

Epoch: 107| Step: 0
Training loss: 2.5409889221191406
Validation loss: 2.4958260008083877

Epoch: 5| Step: 1
Training loss: 3.0842080116271973
Validation loss: 2.4945410682309057

Epoch: 5| Step: 2
Training loss: 2.28525447845459
Validation loss: 2.4919836136602584

Epoch: 5| Step: 3
Training loss: 2.587728977203369
Validation loss: 2.481953497855894

Epoch: 5| Step: 4
Training loss: 2.161224126815796
Validation loss: 2.481106701717582

Epoch: 5| Step: 5
Training loss: 2.404223918914795
Validation loss: 2.4787838894833802

Epoch: 5| Step: 6
Training loss: 3.135899305343628
Validation loss: 2.473902699767902

Epoch: 5| Step: 7
Training loss: 2.9118714332580566
Validation loss: 2.466588379234396

Epoch: 5| Step: 8
Training loss: 3.1223742961883545
Validation loss: 2.4656565150906964

Epoch: 5| Step: 9
Training loss: 2.8679327964782715
Validation loss: 2.4720694711131435

Epoch: 5| Step: 10
Training loss: 2.260183334350586
Validation loss: 2.472806392177459

Epoch: 108| Step: 0
Training loss: 3.3360767364501953
Validation loss: 2.478262976933551

Epoch: 5| Step: 1
Training loss: 2.6238818168640137
Validation loss: 2.478296733671619

Epoch: 5| Step: 2
Training loss: 2.368586778640747
Validation loss: 2.477839723710091

Epoch: 5| Step: 3
Training loss: 2.9941115379333496
Validation loss: 2.476725826981247

Epoch: 5| Step: 4
Training loss: 2.7124037742614746
Validation loss: 2.475534498050649

Epoch: 5| Step: 5
Training loss: 2.699213743209839
Validation loss: 2.4751392128647014

Epoch: 5| Step: 6
Training loss: 2.548576831817627
Validation loss: 2.4721607892744

Epoch: 5| Step: 7
Training loss: 2.393838405609131
Validation loss: 2.4717610036173174

Epoch: 5| Step: 8
Training loss: 2.4075350761413574
Validation loss: 2.4672344628200737

Epoch: 5| Step: 9
Training loss: 2.5879220962524414
Validation loss: 2.471981026793039

Epoch: 5| Step: 10
Training loss: 2.802546501159668
Validation loss: 2.4785272382920787

Epoch: 109| Step: 0
Training loss: 2.9649646282196045
Validation loss: 2.478563262570289

Epoch: 5| Step: 1
Training loss: 2.017702579498291
Validation loss: 2.494580199641566

Epoch: 5| Step: 2
Training loss: 2.7313075065612793
Validation loss: 2.490765720285395

Epoch: 5| Step: 3
Training loss: 3.1433138847351074
Validation loss: 2.5000229650928127

Epoch: 5| Step: 4
Training loss: 3.386561870574951
Validation loss: 2.4965398798706713

Epoch: 5| Step: 5
Training loss: 2.8340225219726562
Validation loss: 2.5018666328922397

Epoch: 5| Step: 6
Training loss: 2.325685501098633
Validation loss: 2.4884105010699202

Epoch: 5| Step: 7
Training loss: 2.805640935897827
Validation loss: 2.4852853359714633

Epoch: 5| Step: 8
Training loss: 2.32568359375
Validation loss: 2.479417739375945

Epoch: 5| Step: 9
Training loss: 2.2602314949035645
Validation loss: 2.4797424654806814

Epoch: 5| Step: 10
Training loss: 2.5528273582458496
Validation loss: 2.4673935110851

Epoch: 110| Step: 0
Training loss: 2.4763779640197754
Validation loss: 2.4700215952370757

Epoch: 5| Step: 1
Training loss: 2.6505510807037354
Validation loss: 2.4604725453161422

Epoch: 5| Step: 2
Training loss: 3.186603546142578
Validation loss: 2.4614953789659726

Epoch: 5| Step: 3
Training loss: 2.5922534465789795
Validation loss: 2.462688189680858

Epoch: 5| Step: 4
Training loss: 2.701143980026245
Validation loss: 2.4613634130006194

Epoch: 5| Step: 5
Training loss: 2.1453113555908203
Validation loss: 2.460119573018884

Epoch: 5| Step: 6
Training loss: 1.971147894859314
Validation loss: 2.4607804129200597

Epoch: 5| Step: 7
Training loss: 3.0286688804626465
Validation loss: 2.461607707444058

Epoch: 5| Step: 8
Training loss: 2.8372325897216797
Validation loss: 2.46172829597227

Epoch: 5| Step: 9
Training loss: 2.996939182281494
Validation loss: 2.4608666640456005

Epoch: 5| Step: 10
Training loss: 2.855177164077759
Validation loss: 2.4633789318864063

Epoch: 111| Step: 0
Training loss: 3.3660502433776855
Validation loss: 2.460609297598562

Epoch: 5| Step: 1
Training loss: 2.456603527069092
Validation loss: 2.459385948796426

Epoch: 5| Step: 2
Training loss: 2.7294564247131348
Validation loss: 2.460917067784135

Epoch: 5| Step: 3
Training loss: 2.904470920562744
Validation loss: 2.46643231761071

Epoch: 5| Step: 4
Training loss: 2.3597633838653564
Validation loss: 2.48366577394547

Epoch: 5| Step: 5
Training loss: 2.4663593769073486
Validation loss: 2.497095948906355

Epoch: 5| Step: 6
Training loss: 2.4959678649902344
Validation loss: 2.504684240587296

Epoch: 5| Step: 7
Training loss: 2.7292535305023193
Validation loss: 2.4828307090267057

Epoch: 5| Step: 8
Training loss: 2.5190532207489014
Validation loss: 2.468533931239959

Epoch: 5| Step: 9
Training loss: 2.933356761932373
Validation loss: 2.457447131474813

Epoch: 5| Step: 10
Training loss: 2.513272523880005
Validation loss: 2.469426331981536

Epoch: 112| Step: 0
Training loss: 3.273160457611084
Validation loss: 2.484007648242417

Epoch: 5| Step: 1
Training loss: 3.0455451011657715
Validation loss: 2.5052423887355353

Epoch: 5| Step: 2
Training loss: 2.6520779132843018
Validation loss: 2.534397384171845

Epoch: 5| Step: 3
Training loss: 2.956176996231079
Validation loss: 2.5436046918233237

Epoch: 5| Step: 4
Training loss: 2.6310813426971436
Validation loss: 2.5381035035656345

Epoch: 5| Step: 5
Training loss: 2.184738874435425
Validation loss: 2.516512055550852

Epoch: 5| Step: 6
Training loss: 2.7924487590789795
Validation loss: 2.48014566462527

Epoch: 5| Step: 7
Training loss: 2.134150266647339
Validation loss: 2.4631908324456986

Epoch: 5| Step: 8
Training loss: 3.0898983478546143
Validation loss: 2.453565051478724

Epoch: 5| Step: 9
Training loss: 2.313305377960205
Validation loss: 2.462322060779859

Epoch: 5| Step: 10
Training loss: 2.529426097869873
Validation loss: 2.467857663349439

Epoch: 113| Step: 0
Training loss: 2.891559600830078
Validation loss: 2.4775771223088747

Epoch: 5| Step: 1
Training loss: 3.2241244316101074
Validation loss: 2.4798048209118586

Epoch: 5| Step: 2
Training loss: 2.754249095916748
Validation loss: 2.471407073800282

Epoch: 5| Step: 3
Training loss: 2.7178244590759277
Validation loss: 2.4714941081180366

Epoch: 5| Step: 4
Training loss: 2.651258945465088
Validation loss: 2.4710452018245572

Epoch: 5| Step: 5
Training loss: 2.7113819122314453
Validation loss: 2.4633613837662565

Epoch: 5| Step: 6
Training loss: 2.698028087615967
Validation loss: 2.4635694924221245

Epoch: 5| Step: 7
Training loss: 2.2257931232452393
Validation loss: 2.4573020422330467

Epoch: 5| Step: 8
Training loss: 2.509648323059082
Validation loss: 2.4584414625680573

Epoch: 5| Step: 9
Training loss: 2.590932846069336
Validation loss: 2.4516514039808706

Epoch: 5| Step: 10
Training loss: 2.4813590049743652
Validation loss: 2.4505930715991604

Epoch: 114| Step: 0
Training loss: 2.2152156829833984
Validation loss: 2.4519671086342103

Epoch: 5| Step: 1
Training loss: 2.514838695526123
Validation loss: 2.457063144253146

Epoch: 5| Step: 2
Training loss: 2.535655975341797
Validation loss: 2.4582076047056463

Epoch: 5| Step: 3
Training loss: 2.610883951187134
Validation loss: 2.464119198501751

Epoch: 5| Step: 4
Training loss: 2.5233211517333984
Validation loss: 2.488223962886359

Epoch: 5| Step: 5
Training loss: 2.7764410972595215
Validation loss: 2.492591852782875

Epoch: 5| Step: 6
Training loss: 2.1468050479888916
Validation loss: 2.4833016344296035

Epoch: 5| Step: 7
Training loss: 3.338040590286255
Validation loss: 2.465663029301551

Epoch: 5| Step: 8
Training loss: 2.9727654457092285
Validation loss: 2.4540675865706576

Epoch: 5| Step: 9
Training loss: 2.851127862930298
Validation loss: 2.447057852181055

Epoch: 5| Step: 10
Training loss: 2.8176703453063965
Validation loss: 2.4447270644608365

Epoch: 115| Step: 0
Training loss: 2.7583699226379395
Validation loss: 2.444789212237122

Epoch: 5| Step: 1
Training loss: 2.6834475994110107
Validation loss: 2.449531179602428

Epoch: 5| Step: 2
Training loss: 3.0469813346862793
Validation loss: 2.4481028331223356

Epoch: 5| Step: 3
Training loss: 2.7731094360351562
Validation loss: 2.451590553406746

Epoch: 5| Step: 4
Training loss: 2.4347023963928223
Validation loss: 2.4549589131468084

Epoch: 5| Step: 5
Training loss: 2.6959128379821777
Validation loss: 2.4631750993831183

Epoch: 5| Step: 6
Training loss: 2.444352149963379
Validation loss: 2.466254985460671

Epoch: 5| Step: 7
Training loss: 3.021573305130005
Validation loss: 2.453848454260057

Epoch: 5| Step: 8
Training loss: 2.313228130340576
Validation loss: 2.4514128520924556

Epoch: 5| Step: 9
Training loss: 2.1104767322540283
Validation loss: 2.446673608595325

Epoch: 5| Step: 10
Training loss: 3.1148552894592285
Validation loss: 2.4476646595103766

Epoch: 116| Step: 0
Training loss: 2.4165120124816895
Validation loss: 2.4518802755622455

Epoch: 5| Step: 1
Training loss: 1.8138008117675781
Validation loss: 2.455303515157392

Epoch: 5| Step: 2
Training loss: 2.444216012954712
Validation loss: 2.4833755621346096

Epoch: 5| Step: 3
Training loss: 2.9798903465270996
Validation loss: 2.5037495551570768

Epoch: 5| Step: 4
Training loss: 2.855226516723633
Validation loss: 2.509965701769757

Epoch: 5| Step: 5
Training loss: 2.4264345169067383
Validation loss: 2.5084558840720885

Epoch: 5| Step: 6
Training loss: 2.5908637046813965
Validation loss: 2.5221961441860405

Epoch: 5| Step: 7
Training loss: 3.1467087268829346
Validation loss: 2.51150171731108

Epoch: 5| Step: 8
Training loss: 3.0546340942382812
Validation loss: 2.5047340367429998

Epoch: 5| Step: 9
Training loss: 3.241925001144409
Validation loss: 2.5058684656696935

Epoch: 5| Step: 10
Training loss: 2.611208915710449
Validation loss: 2.504030866007651

Epoch: 117| Step: 0
Training loss: 2.5940146446228027
Validation loss: 2.4998555952502834

Epoch: 5| Step: 1
Training loss: 1.8968645334243774
Validation loss: 2.494672808595883

Epoch: 5| Step: 2
Training loss: 3.0558085441589355
Validation loss: 2.4904246663534515

Epoch: 5| Step: 3
Training loss: 3.0008339881896973
Validation loss: 2.488171018579955

Epoch: 5| Step: 4
Training loss: 3.026690721511841
Validation loss: 2.4788047318817465

Epoch: 5| Step: 5
Training loss: 2.5499470233917236
Validation loss: 2.4694953400601625

Epoch: 5| Step: 6
Training loss: 2.049830913543701
Validation loss: 2.458964852876561

Epoch: 5| Step: 7
Training loss: 2.974083423614502
Validation loss: 2.4510710931593374

Epoch: 5| Step: 8
Training loss: 2.929708480834961
Validation loss: 2.444366398678031

Epoch: 5| Step: 9
Training loss: 3.095149517059326
Validation loss: 2.4407329328598513

Epoch: 5| Step: 10
Training loss: 2.126528024673462
Validation loss: 2.4468951968736548

Epoch: 118| Step: 0
Training loss: 3.088634967803955
Validation loss: 2.450945797786918

Epoch: 5| Step: 1
Training loss: 2.350895404815674
Validation loss: 2.4617071510643087

Epoch: 5| Step: 2
Training loss: 2.31274676322937
Validation loss: 2.4854034018772904

Epoch: 5| Step: 3
Training loss: 2.2687735557556152
Validation loss: 2.503568644164711

Epoch: 5| Step: 4
Training loss: 3.4790546894073486
Validation loss: 2.490900183236727

Epoch: 5| Step: 5
Training loss: 2.4802069664001465
Validation loss: 2.4893861816775416

Epoch: 5| Step: 6
Training loss: 2.9805760383605957
Validation loss: 2.4772548162808983

Epoch: 5| Step: 7
Training loss: 2.2428138256073
Validation loss: 2.4552665090048187

Epoch: 5| Step: 8
Training loss: 2.481325149536133
Validation loss: 2.447479524920064

Epoch: 5| Step: 9
Training loss: 2.7571957111358643
Validation loss: 2.441874942471904

Epoch: 5| Step: 10
Training loss: 2.9962315559387207
Validation loss: 2.4370082552715013

Epoch: 119| Step: 0
Training loss: 3.1105434894561768
Validation loss: 2.435004598350935

Epoch: 5| Step: 1
Training loss: 2.728942394256592
Validation loss: 2.435308656384868

Epoch: 5| Step: 2
Training loss: 2.5902304649353027
Validation loss: 2.43292748543524

Epoch: 5| Step: 3
Training loss: 2.422853469848633
Validation loss: 2.4357902875510593

Epoch: 5| Step: 4
Training loss: 3.306396961212158
Validation loss: 2.4344955003389748

Epoch: 5| Step: 5
Training loss: 3.0601117610931396
Validation loss: 2.4327443722755677

Epoch: 5| Step: 6
Training loss: 2.3725736141204834
Validation loss: 2.434027966632638

Epoch: 5| Step: 7
Training loss: 2.159956932067871
Validation loss: 2.4338246391665552

Epoch: 5| Step: 8
Training loss: 1.6528403759002686
Validation loss: 2.4321269655740387

Epoch: 5| Step: 9
Training loss: 2.7088871002197266
Validation loss: 2.4337684185274187

Epoch: 5| Step: 10
Training loss: 3.1943206787109375
Validation loss: 2.436794557879048

Epoch: 120| Step: 0
Training loss: 2.977581024169922
Validation loss: 2.4388273121208273

Epoch: 5| Step: 1
Training loss: 2.8921992778778076
Validation loss: 2.4356131643377323

Epoch: 5| Step: 2
Training loss: 2.3931825160980225
Validation loss: 2.438014481657295

Epoch: 5| Step: 3
Training loss: 3.069312572479248
Validation loss: 2.4349859247925463

Epoch: 5| Step: 4
Training loss: 2.8970274925231934
Validation loss: 2.434649154704104

Epoch: 5| Step: 5
Training loss: 2.823639392852783
Validation loss: 2.429849778452227

Epoch: 5| Step: 6
Training loss: 1.8498995304107666
Validation loss: 2.43410054842631

Epoch: 5| Step: 7
Training loss: 2.701786518096924
Validation loss: 2.431385886284613

Epoch: 5| Step: 8
Training loss: 2.4844913482666016
Validation loss: 2.432346290157687

Epoch: 5| Step: 9
Training loss: 2.5569987297058105
Validation loss: 2.4329084555308023

Epoch: 5| Step: 10
Training loss: 2.496765613555908
Validation loss: 2.433420173583492

Epoch: 121| Step: 0
Training loss: 2.358872175216675
Validation loss: 2.4333785631323375

Epoch: 5| Step: 1
Training loss: 2.9762814044952393
Validation loss: 2.434143235606532

Epoch: 5| Step: 2
Training loss: 2.4562411308288574
Validation loss: 2.440414628674907

Epoch: 5| Step: 3
Training loss: 2.5484232902526855
Validation loss: 2.440251073529643

Epoch: 5| Step: 4
Training loss: 2.658499002456665
Validation loss: 2.4492868069679505

Epoch: 5| Step: 5
Training loss: 3.366330623626709
Validation loss: 2.4636424997801423

Epoch: 5| Step: 6
Training loss: 2.5472168922424316
Validation loss: 2.4752867426923526

Epoch: 5| Step: 7
Training loss: 2.451242208480835
Validation loss: 2.477832225061232

Epoch: 5| Step: 8
Training loss: 2.998403310775757
Validation loss: 2.4870921591276764

Epoch: 5| Step: 9
Training loss: 2.0876340866088867
Validation loss: 2.4721506026483353

Epoch: 5| Step: 10
Training loss: 2.8635025024414062
Validation loss: 2.4630635912700365

Epoch: 122| Step: 0
Training loss: 2.31390118598938
Validation loss: 2.446972703420988

Epoch: 5| Step: 1
Training loss: 2.2919762134552
Validation loss: 2.443651068595148

Epoch: 5| Step: 2
Training loss: 2.9876608848571777
Validation loss: 2.43916259273406

Epoch: 5| Step: 3
Training loss: 3.452580690383911
Validation loss: 2.4330711210927656

Epoch: 5| Step: 4
Training loss: 2.9058711528778076
Validation loss: 2.428341588666362

Epoch: 5| Step: 5
Training loss: 2.8371224403381348
Validation loss: 2.4297931501942296

Epoch: 5| Step: 6
Training loss: 2.1818299293518066
Validation loss: 2.433273825594174

Epoch: 5| Step: 7
Training loss: 1.7747459411621094
Validation loss: 2.436612718848772

Epoch: 5| Step: 8
Training loss: 2.0135157108306885
Validation loss: 2.4411245494760494

Epoch: 5| Step: 9
Training loss: 3.8386390209198
Validation loss: 2.439451945725308

Epoch: 5| Step: 10
Training loss: 2.571441173553467
Validation loss: 2.441957712173462

Epoch: 123| Step: 0
Training loss: 2.7634522914886475
Validation loss: 2.443474632437511

Epoch: 5| Step: 1
Training loss: 2.8382129669189453
Validation loss: 2.4503225562393025

Epoch: 5| Step: 2
Training loss: 3.285301923751831
Validation loss: 2.4611401352831113

Epoch: 5| Step: 3
Training loss: 1.8315279483795166
Validation loss: 2.459319555631248

Epoch: 5| Step: 4
Training loss: 2.4574179649353027
Validation loss: 2.4650192568379063

Epoch: 5| Step: 5
Training loss: 2.8721580505371094
Validation loss: 2.4566224749370287

Epoch: 5| Step: 6
Training loss: 2.8527579307556152
Validation loss: 2.4495573056641446

Epoch: 5| Step: 7
Training loss: 2.149585485458374
Validation loss: 2.4414133717936854

Epoch: 5| Step: 8
Training loss: 3.157102108001709
Validation loss: 2.433934962877663

Epoch: 5| Step: 9
Training loss: 2.97465443611145
Validation loss: 2.425637302860137

Epoch: 5| Step: 10
Training loss: 1.8604962825775146
Validation loss: 2.4245590445815877

Epoch: 124| Step: 0
Training loss: 2.595799684524536
Validation loss: 2.425521453221639

Epoch: 5| Step: 1
Training loss: 2.2564661502838135
Validation loss: 2.422120345536099

Epoch: 5| Step: 2
Training loss: 3.0984580516815186
Validation loss: 2.4295510425362536

Epoch: 5| Step: 3
Training loss: 2.847088098526001
Validation loss: 2.433922916330317

Epoch: 5| Step: 4
Training loss: 2.38783597946167
Validation loss: 2.4407112982965287

Epoch: 5| Step: 5
Training loss: 3.0176873207092285
Validation loss: 2.4450842590742212

Epoch: 5| Step: 6
Training loss: 2.206040620803833
Validation loss: 2.451570064790787

Epoch: 5| Step: 7
Training loss: 2.321871280670166
Validation loss: 2.451534912150393

Epoch: 5| Step: 8
Training loss: 3.1931777000427246
Validation loss: 2.438968663574547

Epoch: 5| Step: 9
Training loss: 2.9209346771240234
Validation loss: 2.435564605138635

Epoch: 5| Step: 10
Training loss: 2.156844139099121
Validation loss: 2.433400059259066

Epoch: 125| Step: 0
Training loss: 2.5718135833740234
Validation loss: 2.428469847607356

Epoch: 5| Step: 1
Training loss: 2.6649374961853027
Validation loss: 2.4290818475907847

Epoch: 5| Step: 2
Training loss: 2.796560764312744
Validation loss: 2.424184763303367

Epoch: 5| Step: 3
Training loss: 2.1321158409118652
Validation loss: 2.4230585354630665

Epoch: 5| Step: 4
Training loss: 2.898667812347412
Validation loss: 2.421985077601607

Epoch: 5| Step: 5
Training loss: 2.5917232036590576
Validation loss: 2.4189341709177983

Epoch: 5| Step: 6
Training loss: 2.5790717601776123
Validation loss: 2.420879856232674

Epoch: 5| Step: 7
Training loss: 2.5349183082580566
Validation loss: 2.4202336406195037

Epoch: 5| Step: 8
Training loss: 2.4795892238616943
Validation loss: 2.417262554168701

Epoch: 5| Step: 9
Training loss: 2.842179775238037
Validation loss: 2.416106467605919

Epoch: 5| Step: 10
Training loss: 3.0444986820220947
Validation loss: 2.420323725669615

Testing loss: 2.573050445980496

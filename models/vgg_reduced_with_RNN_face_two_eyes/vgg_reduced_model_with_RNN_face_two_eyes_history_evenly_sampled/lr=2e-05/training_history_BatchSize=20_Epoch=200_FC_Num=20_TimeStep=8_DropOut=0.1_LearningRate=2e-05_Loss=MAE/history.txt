Epoch: 1| Step: 0
Training loss: 4.475796699523926
Validation loss: 5.137911386387323

Epoch: 5| Step: 1
Training loss: 4.553462982177734
Validation loss: 5.115939596647857

Epoch: 5| Step: 2
Training loss: 5.722847938537598
Validation loss: 5.100272378613872

Epoch: 5| Step: 3
Training loss: 4.686827659606934
Validation loss: 5.088347070960588

Epoch: 5| Step: 4
Training loss: 4.4093499183654785
Validation loss: 5.075173813809631

Epoch: 5| Step: 5
Training loss: 4.768660068511963
Validation loss: 5.06072982152303

Epoch: 5| Step: 6
Training loss: 4.36672830581665
Validation loss: 5.044349547355406

Epoch: 5| Step: 7
Training loss: 3.7449722290039062
Validation loss: 5.024501123735981

Epoch: 5| Step: 8
Training loss: 6.16079044342041
Validation loss: 5.002304723185878

Epoch: 5| Step: 9
Training loss: 5.897508144378662
Validation loss: 4.977374158879762

Epoch: 5| Step: 10
Training loss: 4.483409881591797
Validation loss: 4.949423492595714

Epoch: 2| Step: 0
Training loss: 5.102441310882568
Validation loss: 4.918444736029512

Epoch: 5| Step: 1
Training loss: 4.3950910568237305
Validation loss: 4.883805033981159

Epoch: 5| Step: 2
Training loss: 3.8581299781799316
Validation loss: 4.846201219866352

Epoch: 5| Step: 3
Training loss: 4.640698432922363
Validation loss: 4.8064490851535595

Epoch: 5| Step: 4
Training loss: 5.653498649597168
Validation loss: 4.765302965717931

Epoch: 5| Step: 5
Training loss: 4.33061408996582
Validation loss: 4.722527427058066

Epoch: 5| Step: 6
Training loss: 5.049476623535156
Validation loss: 4.679057782696139

Epoch: 5| Step: 7
Training loss: 5.17848014831543
Validation loss: 4.635067852594519

Epoch: 5| Step: 8
Training loss: 4.060176849365234
Validation loss: 4.588453769683838

Epoch: 5| Step: 9
Training loss: 3.6711437702178955
Validation loss: 4.543413090449508

Epoch: 5| Step: 10
Training loss: 3.5222463607788086
Validation loss: 4.496816199312928

Epoch: 3| Step: 0
Training loss: 4.446541786193848
Validation loss: 4.452793859666394

Epoch: 5| Step: 1
Training loss: 3.9857802391052246
Validation loss: 4.408082036561863

Epoch: 5| Step: 2
Training loss: 3.4615845680236816
Validation loss: 4.361977249063472

Epoch: 5| Step: 3
Training loss: 4.168436050415039
Validation loss: 4.313922974371141

Epoch: 5| Step: 4
Training loss: 5.182939052581787
Validation loss: 4.25750724218225

Epoch: 5| Step: 5
Training loss: 3.9627068042755127
Validation loss: 4.201140352474746

Epoch: 5| Step: 6
Training loss: 3.946089506149292
Validation loss: 4.140658209400792

Epoch: 5| Step: 7
Training loss: 2.9536709785461426
Validation loss: 4.075066715158442

Epoch: 5| Step: 8
Training loss: 3.819293975830078
Validation loss: 4.050252919555993

Epoch: 5| Step: 9
Training loss: 4.019355773925781
Validation loss: 4.024849243061517

Epoch: 5| Step: 10
Training loss: 4.455113887786865
Validation loss: 3.9953181717985418

Epoch: 4| Step: 0
Training loss: 2.666121006011963
Validation loss: 3.9643442271858134

Epoch: 5| Step: 1
Training loss: 3.773386001586914
Validation loss: 3.9363249912056872

Epoch: 5| Step: 2
Training loss: 3.8217530250549316
Validation loss: 3.9036700033372447

Epoch: 5| Step: 3
Training loss: 3.6172614097595215
Validation loss: 3.8848961245629097

Epoch: 5| Step: 4
Training loss: 4.03535795211792
Validation loss: 3.872206413617698

Epoch: 5| Step: 5
Training loss: 3.3753585815429688
Validation loss: 3.8539380770857616

Epoch: 5| Step: 6
Training loss: 3.442826747894287
Validation loss: 3.8345692439745833

Epoch: 5| Step: 7
Training loss: 3.4996562004089355
Validation loss: 3.8165472040894213

Epoch: 5| Step: 8
Training loss: 3.1465423107147217
Validation loss: 3.801171943705569

Epoch: 5| Step: 9
Training loss: 5.162461280822754
Validation loss: 3.7866417182389127

Epoch: 5| Step: 10
Training loss: 4.673244476318359
Validation loss: 3.7747024028531966

Epoch: 5| Step: 0
Training loss: 3.7558321952819824
Validation loss: 3.761330768626223

Epoch: 5| Step: 1
Training loss: 3.9728851318359375
Validation loss: 3.746579944446523

Epoch: 5| Step: 2
Training loss: 3.595607042312622
Validation loss: 3.731360835413779

Epoch: 5| Step: 3
Training loss: 3.197746992111206
Validation loss: 3.7160142083321848

Epoch: 5| Step: 4
Training loss: 3.2938125133514404
Validation loss: 3.705320671040525

Epoch: 5| Step: 5
Training loss: 3.7836830615997314
Validation loss: 3.6946010281962733

Epoch: 5| Step: 6
Training loss: 3.0718510150909424
Validation loss: 3.6859379583789456

Epoch: 5| Step: 7
Training loss: 3.748727321624756
Validation loss: 3.680508716132051

Epoch: 5| Step: 8
Training loss: 4.027484893798828
Validation loss: 3.6696911422155236

Epoch: 5| Step: 9
Training loss: 3.638650417327881
Validation loss: 3.657019571591449

Epoch: 5| Step: 10
Training loss: 3.5625483989715576
Validation loss: 3.6487513357593166

Epoch: 6| Step: 0
Training loss: 3.9323418140411377
Validation loss: 3.6495607053079913

Epoch: 5| Step: 1
Training loss: 3.6112587451934814
Validation loss: 3.6335071491938766

Epoch: 5| Step: 2
Training loss: 3.3360188007354736
Validation loss: 3.621230874010312

Epoch: 5| Step: 3
Training loss: 3.2331154346466064
Validation loss: 3.6221624881990495

Epoch: 5| Step: 4
Training loss: 4.447291851043701
Validation loss: 3.6341610185561644

Epoch: 5| Step: 5
Training loss: 3.2717483043670654
Validation loss: 3.60239738802756

Epoch: 5| Step: 6
Training loss: 3.4506382942199707
Validation loss: 3.5930314192207913

Epoch: 5| Step: 7
Training loss: 3.8427929878234863
Validation loss: 3.5917858923635175

Epoch: 5| Step: 8
Training loss: 3.2745375633239746
Validation loss: 3.5958418179583806

Epoch: 5| Step: 9
Training loss: 2.5677242279052734
Validation loss: 3.588047440334033

Epoch: 5| Step: 10
Training loss: 3.8938679695129395
Validation loss: 3.580731015051565

Epoch: 7| Step: 0
Training loss: 4.339616775512695
Validation loss: 3.5617627789897304

Epoch: 5| Step: 1
Training loss: 4.166914939880371
Validation loss: 3.542932125829881

Epoch: 5| Step: 2
Training loss: 2.5224785804748535
Validation loss: 3.5287753433309574

Epoch: 5| Step: 3
Training loss: 3.01458740234375
Validation loss: 3.521154196031632

Epoch: 5| Step: 4
Training loss: 3.256244659423828
Validation loss: 3.5124743112953762

Epoch: 5| Step: 5
Training loss: 3.8356316089630127
Validation loss: 3.4981002474343903

Epoch: 5| Step: 6
Training loss: 3.164623975753784
Validation loss: 3.4843480945915304

Epoch: 5| Step: 7
Training loss: 2.7925686836242676
Validation loss: 3.468250472058532

Epoch: 5| Step: 8
Training loss: 3.5381646156311035
Validation loss: 3.465141968060565

Epoch: 5| Step: 9
Training loss: 4.217778205871582
Validation loss: 3.5054709116617837

Epoch: 5| Step: 10
Training loss: 2.9677913188934326
Validation loss: 3.4494401126779537

Epoch: 8| Step: 0
Training loss: 3.597294569015503
Validation loss: 3.4291957834715485

Epoch: 5| Step: 1
Training loss: 3.523190975189209
Validation loss: 3.419247634949223

Epoch: 5| Step: 2
Training loss: 3.293118953704834
Validation loss: 3.4134162113230717

Epoch: 5| Step: 3
Training loss: 2.5532631874084473
Validation loss: 3.4097431449479956

Epoch: 5| Step: 4
Training loss: 2.5706143379211426
Validation loss: 3.4041027843311267

Epoch: 5| Step: 5
Training loss: 3.127690553665161
Validation loss: 3.3962815166801534

Epoch: 5| Step: 6
Training loss: 4.0342841148376465
Validation loss: 3.3882176850431707

Epoch: 5| Step: 7
Training loss: 3.4060683250427246
Validation loss: 3.378816138031662

Epoch: 5| Step: 8
Training loss: 3.6052627563476562
Validation loss: 3.370725170258553

Epoch: 5| Step: 9
Training loss: 3.3094711303710938
Validation loss: 3.3634062838810745

Epoch: 5| Step: 10
Training loss: 3.844310998916626
Validation loss: 3.356063227499685

Epoch: 9| Step: 0
Training loss: 3.4656734466552734
Validation loss: 3.3490124389689457

Epoch: 5| Step: 1
Training loss: 3.2577662467956543
Validation loss: 3.3413573849585747

Epoch: 5| Step: 2
Training loss: 2.7864315509796143
Validation loss: 3.334558889430056

Epoch: 5| Step: 3
Training loss: 3.635605573654175
Validation loss: 3.328776672322263

Epoch: 5| Step: 4
Training loss: 4.415274143218994
Validation loss: 3.3235341143864456

Epoch: 5| Step: 5
Training loss: 2.269345283508301
Validation loss: 3.3163990718062206

Epoch: 5| Step: 6
Training loss: 3.1326568126678467
Validation loss: 3.310876915531774

Epoch: 5| Step: 7
Training loss: 1.9693561792373657
Validation loss: 3.3071993858583513

Epoch: 5| Step: 8
Training loss: 4.600689888000488
Validation loss: 3.304957825650451

Epoch: 5| Step: 9
Training loss: 3.2170987129211426
Validation loss: 3.2955190289405083

Epoch: 5| Step: 10
Training loss: 3.3904175758361816
Validation loss: 3.2996898569086546

Epoch: 10| Step: 0
Training loss: 2.8654885292053223
Validation loss: 3.2889611028855845

Epoch: 5| Step: 1
Training loss: 3.4316070079803467
Validation loss: 3.281703292682607

Epoch: 5| Step: 2
Training loss: 3.5275015830993652
Validation loss: 3.2750954140899

Epoch: 5| Step: 3
Training loss: 1.7340911626815796
Validation loss: 3.2691042269429853

Epoch: 5| Step: 4
Training loss: 3.907721757888794
Validation loss: 3.2662766569404194

Epoch: 5| Step: 5
Training loss: 3.1615092754364014
Validation loss: 3.2620412329191804

Epoch: 5| Step: 6
Training loss: 3.7488350868225098
Validation loss: 3.255538886593234

Epoch: 5| Step: 7
Training loss: 3.4278101921081543
Validation loss: 3.2512139274228002

Epoch: 5| Step: 8
Training loss: 2.999953508377075
Validation loss: 3.244970908728979

Epoch: 5| Step: 9
Training loss: 3.4632232189178467
Validation loss: 3.240344137273809

Epoch: 5| Step: 10
Training loss: 3.4129791259765625
Validation loss: 3.236047342259397

Epoch: 11| Step: 0
Training loss: 3.408592939376831
Validation loss: 3.23265262316632

Epoch: 5| Step: 1
Training loss: 3.664501190185547
Validation loss: 3.2346229732677503

Epoch: 5| Step: 2
Training loss: 3.7150745391845703
Validation loss: 3.294876226814844

Epoch: 5| Step: 3
Training loss: 2.672856569290161
Validation loss: 3.2299217434339624

Epoch: 5| Step: 4
Training loss: 3.5600154399871826
Validation loss: 3.2193610283636276

Epoch: 5| Step: 5
Training loss: 3.0017685890197754
Validation loss: 3.235969453729609

Epoch: 5| Step: 6
Training loss: 3.459874391555786
Validation loss: 3.2319819132486978

Epoch: 5| Step: 7
Training loss: 2.9178032875061035
Validation loss: 3.214418577891524

Epoch: 5| Step: 8
Training loss: 3.181811809539795
Validation loss: 3.205375361186202

Epoch: 5| Step: 9
Training loss: 2.851360321044922
Validation loss: 3.203840832556448

Epoch: 5| Step: 10
Training loss: 2.857118844985962
Validation loss: 3.2001865961218394

Epoch: 12| Step: 0
Training loss: 4.089818477630615
Validation loss: 3.194658807528916

Epoch: 5| Step: 1
Training loss: 3.107769012451172
Validation loss: 3.1924622725414973

Epoch: 5| Step: 2
Training loss: 2.628671884536743
Validation loss: 3.1900235863142115

Epoch: 5| Step: 3
Training loss: 3.1423518657684326
Validation loss: 3.1888442270217405

Epoch: 5| Step: 4
Training loss: 3.408360004425049
Validation loss: 3.230063235887917

Epoch: 5| Step: 5
Training loss: 3.687204360961914
Validation loss: 3.2291424607717865

Epoch: 5| Step: 6
Training loss: 3.4509730339050293
Validation loss: 3.1721157463647986

Epoch: 5| Step: 7
Training loss: 2.9474472999572754
Validation loss: 3.1676725264518493

Epoch: 5| Step: 8
Training loss: 2.646204710006714
Validation loss: 3.1844727018828034

Epoch: 5| Step: 9
Training loss: 3.1090822219848633
Validation loss: 3.2021843105234127

Epoch: 5| Step: 10
Training loss: 2.8185231685638428
Validation loss: 3.17399840201101

Epoch: 13| Step: 0
Training loss: 3.5303146839141846
Validation loss: 3.1627981585841023

Epoch: 5| Step: 1
Training loss: 3.887038469314575
Validation loss: 3.1521970431009927

Epoch: 5| Step: 2
Training loss: 3.274409770965576
Validation loss: 3.150230415405766

Epoch: 5| Step: 3
Training loss: 3.7092604637145996
Validation loss: 3.1471572845212874

Epoch: 5| Step: 4
Training loss: 2.560065984725952
Validation loss: 3.146760881588023

Epoch: 5| Step: 5
Training loss: 3.131735324859619
Validation loss: 3.138661135909378

Epoch: 5| Step: 6
Training loss: 3.3112289905548096
Validation loss: 3.133322479904339

Epoch: 5| Step: 7
Training loss: 2.070404529571533
Validation loss: 3.1282316664213776

Epoch: 5| Step: 8
Training loss: 2.9949426651000977
Validation loss: 3.124899015631727

Epoch: 5| Step: 9
Training loss: 2.8925013542175293
Validation loss: 3.1228104842606412

Epoch: 5| Step: 10
Training loss: 3.400669574737549
Validation loss: 3.119614516535113

Epoch: 14| Step: 0
Training loss: 3.6161346435546875
Validation loss: 3.1183931545544694

Epoch: 5| Step: 1
Training loss: 2.936833143234253
Validation loss: 3.1184615396684214

Epoch: 5| Step: 2
Training loss: 3.326529026031494
Validation loss: 3.1111950438509703

Epoch: 5| Step: 3
Training loss: 3.8973751068115234
Validation loss: 3.10751929847143

Epoch: 5| Step: 4
Training loss: 3.6945109367370605
Validation loss: 3.1042481494206253

Epoch: 5| Step: 5
Training loss: 1.981324553489685
Validation loss: 3.102811054516864

Epoch: 5| Step: 6
Training loss: 3.239692211151123
Validation loss: 3.1006050981501097

Epoch: 5| Step: 7
Training loss: 3.565568447113037
Validation loss: 3.098987574218422

Epoch: 5| Step: 8
Training loss: 2.5124523639678955
Validation loss: 3.0966951359984694

Epoch: 5| Step: 9
Training loss: 3.0828092098236084
Validation loss: 3.095340374977358

Epoch: 5| Step: 10
Training loss: 2.496814727783203
Validation loss: 3.091942864079629

Epoch: 15| Step: 0
Training loss: 3.629235029220581
Validation loss: 3.096094808270854

Epoch: 5| Step: 1
Training loss: 3.1628644466400146
Validation loss: 3.0949483199786116

Epoch: 5| Step: 2
Training loss: 2.6474297046661377
Validation loss: 3.0876536036050446

Epoch: 5| Step: 3
Training loss: 3.8722782135009766
Validation loss: 3.087066747808969

Epoch: 5| Step: 4
Training loss: 3.3458847999572754
Validation loss: 3.0867748414316485

Epoch: 5| Step: 5
Training loss: 2.283564329147339
Validation loss: 3.0892036371333624

Epoch: 5| Step: 6
Training loss: 2.942073345184326
Validation loss: 3.0840148054143435

Epoch: 5| Step: 7
Training loss: 3.624568223953247
Validation loss: 3.078585247839651

Epoch: 5| Step: 8
Training loss: 3.911510944366455
Validation loss: 3.072216095462922

Epoch: 5| Step: 9
Training loss: 2.210123300552368
Validation loss: 3.0680747749984905

Epoch: 5| Step: 10
Training loss: 2.615877151489258
Validation loss: 3.0655708723170783

Epoch: 16| Step: 0
Training loss: 3.5816574096679688
Validation loss: 3.0698972696899087

Epoch: 5| Step: 1
Training loss: 3.1105713844299316
Validation loss: 3.0845498141422065

Epoch: 5| Step: 2
Training loss: 3.565647840499878
Validation loss: 3.1145186834437872

Epoch: 5| Step: 3
Training loss: 2.8289060592651367
Validation loss: 3.073536826718238

Epoch: 5| Step: 4
Training loss: 3.805957078933716
Validation loss: 3.0607835067215787

Epoch: 5| Step: 5
Training loss: 1.9540847539901733
Validation loss: 3.056654812187277

Epoch: 5| Step: 6
Training loss: 3.3968257904052734
Validation loss: 3.0519062524200766

Epoch: 5| Step: 7
Training loss: 3.9084694385528564
Validation loss: 3.047980539260372

Epoch: 5| Step: 8
Training loss: 3.374892473220825
Validation loss: 3.046978642863612

Epoch: 5| Step: 9
Training loss: 1.6197872161865234
Validation loss: 3.050697618915189

Epoch: 5| Step: 10
Training loss: 2.9315402507781982
Validation loss: 3.049472896001672

Epoch: 17| Step: 0
Training loss: 3.6577651500701904
Validation loss: 3.0510917043173187

Epoch: 5| Step: 1
Training loss: 2.102972984313965
Validation loss: 3.0483753937546925

Epoch: 5| Step: 2
Training loss: 3.024014949798584
Validation loss: 3.0466908562567925

Epoch: 5| Step: 3
Training loss: 3.5849990844726562
Validation loss: 3.0437600945913665

Epoch: 5| Step: 4
Training loss: 3.685715913772583
Validation loss: 3.034745770116006

Epoch: 5| Step: 5
Training loss: 3.8042094707489014
Validation loss: 3.0357056125517814

Epoch: 5| Step: 6
Training loss: 2.5085644721984863
Validation loss: 3.0313535249361427

Epoch: 5| Step: 7
Training loss: 3.282482862472534
Validation loss: 3.0300398642016995

Epoch: 5| Step: 8
Training loss: 2.206580400466919
Validation loss: 3.0285137699496363

Epoch: 5| Step: 9
Training loss: 2.5825467109680176
Validation loss: 3.029267200859644

Epoch: 5| Step: 10
Training loss: 3.5857937335968018
Validation loss: 3.030162478006014

Epoch: 18| Step: 0
Training loss: 3.0016093254089355
Validation loss: 3.0278976476320656

Epoch: 5| Step: 1
Training loss: 3.1317591667175293
Validation loss: 3.023293223432315

Epoch: 5| Step: 2
Training loss: 3.1637685298919678
Validation loss: 3.0227672771740983

Epoch: 5| Step: 3
Training loss: 3.032548427581787
Validation loss: 3.017660656282979

Epoch: 5| Step: 4
Training loss: 3.568953275680542
Validation loss: 3.0155621626043834

Epoch: 5| Step: 5
Training loss: 2.38262677192688
Validation loss: 3.013183509149859

Epoch: 5| Step: 6
Training loss: 2.6883814334869385
Validation loss: 3.013287518614082

Epoch: 5| Step: 7
Training loss: 2.9628472328186035
Validation loss: 3.012647359601913

Epoch: 5| Step: 8
Training loss: 3.0377285480499268
Validation loss: 3.011976352301977

Epoch: 5| Step: 9
Training loss: 3.5420784950256348
Validation loss: 3.0100186742762083

Epoch: 5| Step: 10
Training loss: 3.2982444763183594
Validation loss: 3.0085868707267185

Epoch: 19| Step: 0
Training loss: 2.3162877559661865
Validation loss: 3.008179633848129

Epoch: 5| Step: 1
Training loss: 4.335831642150879
Validation loss: 3.007077652920959

Epoch: 5| Step: 2
Training loss: 2.8038392066955566
Validation loss: 3.003548042748564

Epoch: 5| Step: 3
Training loss: 2.3168864250183105
Validation loss: 3.0027975420798025

Epoch: 5| Step: 4
Training loss: 2.7678778171539307
Validation loss: 3.002106394819034

Epoch: 5| Step: 5
Training loss: 3.61285662651062
Validation loss: 3.0045307502951673

Epoch: 5| Step: 6
Training loss: 3.3410637378692627
Validation loss: 2.9968138612726682

Epoch: 5| Step: 7
Training loss: 2.9293129444122314
Validation loss: 2.9955785428324053

Epoch: 5| Step: 8
Training loss: 3.2986900806427
Validation loss: 2.9945011497825704

Epoch: 5| Step: 9
Training loss: 2.982801914215088
Validation loss: 2.99234034938197

Epoch: 5| Step: 10
Training loss: 2.915424108505249
Validation loss: 2.991877099519135

Epoch: 20| Step: 0
Training loss: 3.598567247390747
Validation loss: 2.99469284601109

Epoch: 5| Step: 1
Training loss: 2.660158634185791
Validation loss: 2.9970117281841975

Epoch: 5| Step: 2
Training loss: 3.121851682662964
Validation loss: 2.9980331415771158

Epoch: 5| Step: 3
Training loss: 2.636418581008911
Validation loss: 3.00668534668543

Epoch: 5| Step: 4
Training loss: 3.8827736377716064
Validation loss: 3.0005811311865367

Epoch: 5| Step: 5
Training loss: 2.6192898750305176
Validation loss: 2.9938619136810303

Epoch: 5| Step: 6
Training loss: 3.504074811935425
Validation loss: 2.9875512225653535

Epoch: 5| Step: 7
Training loss: 3.1880745887756348
Validation loss: 2.9828642799008276

Epoch: 5| Step: 8
Training loss: 2.406984567642212
Validation loss: 2.9824578326235534

Epoch: 5| Step: 9
Training loss: 2.9091413021087646
Validation loss: 2.9801083662176646

Epoch: 5| Step: 10
Training loss: 3.0215799808502197
Validation loss: 2.97836285509089

Epoch: 21| Step: 0
Training loss: 3.099278211593628
Validation loss: 2.9811222373798327

Epoch: 5| Step: 1
Training loss: 3.1677727699279785
Validation loss: 2.986012561346895

Epoch: 5| Step: 2
Training loss: 2.6361050605773926
Validation loss: 2.9851376266889673

Epoch: 5| Step: 3
Training loss: 2.17098331451416
Validation loss: 2.9847367860937632

Epoch: 5| Step: 4
Training loss: 3.6454086303710938
Validation loss: 2.978825133333924

Epoch: 5| Step: 5
Training loss: 2.447096586227417
Validation loss: 2.972208451199275

Epoch: 5| Step: 6
Training loss: 3.6111836433410645
Validation loss: 2.9701288002793507

Epoch: 5| Step: 7
Training loss: 2.7122650146484375
Validation loss: 2.9712672284854356

Epoch: 5| Step: 8
Training loss: 3.6724066734313965
Validation loss: 2.9747815388505177

Epoch: 5| Step: 9
Training loss: 3.2672367095947266
Validation loss: 2.969785838998774

Epoch: 5| Step: 10
Training loss: 3.0704424381256104
Validation loss: 2.96573031333185

Epoch: 22| Step: 0
Training loss: 3.2027580738067627
Validation loss: 2.964250736339118

Epoch: 5| Step: 1
Training loss: 3.6913676261901855
Validation loss: 2.963530630193731

Epoch: 5| Step: 2
Training loss: 2.7748475074768066
Validation loss: 2.962042813659996

Epoch: 5| Step: 3
Training loss: 2.4567952156066895
Validation loss: 2.9597726380953224

Epoch: 5| Step: 4
Training loss: 3.170745849609375
Validation loss: 2.9591662114666355

Epoch: 5| Step: 5
Training loss: 3.1896209716796875
Validation loss: 2.957968270906838

Epoch: 5| Step: 6
Training loss: 3.185987710952759
Validation loss: 2.955095575701806

Epoch: 5| Step: 7
Training loss: 3.8465371131896973
Validation loss: 2.9528241670259865

Epoch: 5| Step: 8
Training loss: 2.3591060638427734
Validation loss: 2.9553366655944497

Epoch: 5| Step: 9
Training loss: 2.933427095413208
Validation loss: 2.9512824704570155

Epoch: 5| Step: 10
Training loss: 2.4575867652893066
Validation loss: 2.951592130045737

Epoch: 23| Step: 0
Training loss: 3.1363449096679688
Validation loss: 2.9503778924224195

Epoch: 5| Step: 1
Training loss: 2.433072805404663
Validation loss: 2.9509241145144225

Epoch: 5| Step: 2
Training loss: 3.6894500255584717
Validation loss: 2.948342136157456

Epoch: 5| Step: 3
Training loss: 3.050896167755127
Validation loss: 2.947778450545444

Epoch: 5| Step: 4
Training loss: 2.891080141067505
Validation loss: 2.94407138516826

Epoch: 5| Step: 5
Training loss: 2.5960724353790283
Validation loss: 2.9413261311028593

Epoch: 5| Step: 6
Training loss: 3.8389530181884766
Validation loss: 2.944282682993079

Epoch: 5| Step: 7
Training loss: 2.936460256576538
Validation loss: 2.945802968035462

Epoch: 5| Step: 8
Training loss: 2.570744037628174
Validation loss: 2.937537782935686

Epoch: 5| Step: 9
Training loss: 3.1368072032928467
Validation loss: 2.9357906823517173

Epoch: 5| Step: 10
Training loss: 2.9165542125701904
Validation loss: 2.9349519591177664

Epoch: 24| Step: 0
Training loss: 3.25947904586792
Validation loss: 2.931834090140558

Epoch: 5| Step: 1
Training loss: 3.3246147632598877
Validation loss: 2.933731604647893

Epoch: 5| Step: 2
Training loss: 3.2625274658203125
Validation loss: 2.9306837358782367

Epoch: 5| Step: 3
Training loss: 3.2029166221618652
Validation loss: 2.932505702459684

Epoch: 5| Step: 4
Training loss: 2.302372694015503
Validation loss: 2.931759741998488

Epoch: 5| Step: 5
Training loss: 3.0264620780944824
Validation loss: 2.9290747463062243

Epoch: 5| Step: 6
Training loss: 2.8494181632995605
Validation loss: 2.9294532422096498

Epoch: 5| Step: 7
Training loss: 3.1468966007232666
Validation loss: 2.932192535810573

Epoch: 5| Step: 8
Training loss: 2.6233103275299072
Validation loss: 2.930294959775863

Epoch: 5| Step: 9
Training loss: 3.339963436126709
Validation loss: 2.9262359167939875

Epoch: 5| Step: 10
Training loss: 2.7123067378997803
Validation loss: 2.9300534238097486

Epoch: 25| Step: 0
Training loss: 3.274845838546753
Validation loss: 2.925381411788284

Epoch: 5| Step: 1
Training loss: 3.963326930999756
Validation loss: 2.9298751623399797

Epoch: 5| Step: 2
Training loss: 3.3598263263702393
Validation loss: 2.9283998038179133

Epoch: 5| Step: 3
Training loss: 2.528557300567627
Validation loss: 2.92284809389422

Epoch: 5| Step: 4
Training loss: 2.484832286834717
Validation loss: 2.93133907933389

Epoch: 5| Step: 5
Training loss: 2.9924569129943848
Validation loss: 2.9504006165330128

Epoch: 5| Step: 6
Training loss: 3.368368148803711
Validation loss: 2.939166053648918

Epoch: 5| Step: 7
Training loss: 3.8434906005859375
Validation loss: 2.920434680036319

Epoch: 5| Step: 8
Training loss: 2.868323802947998
Validation loss: 2.914532228182721

Epoch: 5| Step: 9
Training loss: 2.4485952854156494
Validation loss: 2.9206518947437243

Epoch: 5| Step: 10
Training loss: 1.7426007986068726
Validation loss: 2.9284712319732993

Epoch: 26| Step: 0
Training loss: 2.357307195663452
Validation loss: 2.967721826286726

Epoch: 5| Step: 1
Training loss: 3.3405280113220215
Validation loss: 3.0916154333340224

Epoch: 5| Step: 2
Training loss: 2.6338977813720703
Validation loss: 3.131213149716777

Epoch: 5| Step: 3
Training loss: 3.369988203048706
Validation loss: 3.15663331554782

Epoch: 5| Step: 4
Training loss: 3.4416003227233887
Validation loss: 3.0925012506464475

Epoch: 5| Step: 5
Training loss: 3.2127697467803955
Validation loss: 3.016991433276925

Epoch: 5| Step: 6
Training loss: 2.76833176612854
Validation loss: 2.9198857276670394

Epoch: 5| Step: 7
Training loss: 3.663761854171753
Validation loss: 3.105779032553396

Epoch: 5| Step: 8
Training loss: 2.9693286418914795
Validation loss: 3.1496248399057696

Epoch: 5| Step: 9
Training loss: 3.467076063156128
Validation loss: 3.039936432274439

Epoch: 5| Step: 10
Training loss: 2.8101985454559326
Validation loss: 2.9216771151429866

Epoch: 27| Step: 0
Training loss: 3.058405637741089
Validation loss: 2.918045900201285

Epoch: 5| Step: 1
Training loss: 3.478158473968506
Validation loss: 2.9870687146340646

Epoch: 5| Step: 2
Training loss: 3.2569363117218018
Validation loss: 3.1725847490372194

Epoch: 5| Step: 3
Training loss: 3.3411662578582764
Validation loss: 3.2286186602807816

Epoch: 5| Step: 4
Training loss: 4.186199188232422
Validation loss: 3.2401413712450253

Epoch: 5| Step: 5
Training loss: 3.040708303451538
Validation loss: 3.219121589455553

Epoch: 5| Step: 6
Training loss: 2.544498920440674
Validation loss: 3.1811849788952897

Epoch: 5| Step: 7
Training loss: 2.713000535964966
Validation loss: 3.1384670298586608

Epoch: 5| Step: 8
Training loss: 3.2937023639678955
Validation loss: 3.0934354874395553

Epoch: 5| Step: 9
Training loss: 3.3246548175811768
Validation loss: 3.0760162517588627

Epoch: 5| Step: 10
Training loss: 2.2482807636260986
Validation loss: 3.0678327596315773

Epoch: 28| Step: 0
Training loss: 2.3800482749938965
Validation loss: 3.059272607167562

Epoch: 5| Step: 1
Training loss: 2.9924912452697754
Validation loss: 3.0633150787763697

Epoch: 5| Step: 2
Training loss: 3.464519500732422
Validation loss: 3.054541523738574

Epoch: 5| Step: 3
Training loss: 3.3162741661071777
Validation loss: 3.0366391853619645

Epoch: 5| Step: 4
Training loss: 3.084744691848755
Validation loss: 3.0271866449745755

Epoch: 5| Step: 5
Training loss: 3.285654067993164
Validation loss: 3.017937265416627

Epoch: 5| Step: 6
Training loss: 3.5254898071289062
Validation loss: 3.013003518504481

Epoch: 5| Step: 7
Training loss: 2.72873854637146
Validation loss: 3.0071019818705897

Epoch: 5| Step: 8
Training loss: 3.934488296508789
Validation loss: 2.996895505535987

Epoch: 5| Step: 9
Training loss: 3.0272254943847656
Validation loss: 2.988042744257117

Epoch: 5| Step: 10
Training loss: 1.8784180879592896
Validation loss: 2.9791201724801013

Epoch: 29| Step: 0
Training loss: 2.480483293533325
Validation loss: 2.975632821359942

Epoch: 5| Step: 1
Training loss: 3.236219882965088
Validation loss: 2.972791682007492

Epoch: 5| Step: 2
Training loss: 4.004436016082764
Validation loss: 2.9636044989350023

Epoch: 5| Step: 3
Training loss: 2.6535255908966064
Validation loss: 2.958489023229127

Epoch: 5| Step: 4
Training loss: 2.8393921852111816
Validation loss: 2.9538630285570697

Epoch: 5| Step: 5
Training loss: 2.8441967964172363
Validation loss: 2.951814333597819

Epoch: 5| Step: 6
Training loss: 3.404374361038208
Validation loss: 2.946797452947145

Epoch: 5| Step: 7
Training loss: 3.0617947578430176
Validation loss: 2.943099921749484

Epoch: 5| Step: 8
Training loss: 3.1676383018493652
Validation loss: 2.939151474224624

Epoch: 5| Step: 9
Training loss: 2.7036523818969727
Validation loss: 2.9330518912243586

Epoch: 5| Step: 10
Training loss: 2.913940906524658
Validation loss: 2.9271658235980618

Epoch: 30| Step: 0
Training loss: 2.9125404357910156
Validation loss: 2.923570191988381

Epoch: 5| Step: 1
Training loss: 2.9200243949890137
Validation loss: 2.9218645941826606

Epoch: 5| Step: 2
Training loss: 2.8801355361938477
Validation loss: 2.9150985697264313

Epoch: 5| Step: 3
Training loss: 4.082643985748291
Validation loss: 2.909229040145874

Epoch: 5| Step: 4
Training loss: 2.62648344039917
Validation loss: 2.903248753598941

Epoch: 5| Step: 5
Training loss: 2.4271464347839355
Validation loss: 2.9058569451814056

Epoch: 5| Step: 6
Training loss: 3.326526165008545
Validation loss: 2.9050172708367787

Epoch: 5| Step: 7
Training loss: 3.182796001434326
Validation loss: 2.9043235137898433

Epoch: 5| Step: 8
Training loss: 3.3323841094970703
Validation loss: 2.898183591904179

Epoch: 5| Step: 9
Training loss: 2.78507924079895
Validation loss: 2.89404655784689

Epoch: 5| Step: 10
Training loss: 2.422921657562256
Validation loss: 2.89134644949308

Epoch: 31| Step: 0
Training loss: 2.867435932159424
Validation loss: 2.889038362810689

Epoch: 5| Step: 1
Training loss: 2.4877877235412598
Validation loss: 2.8929639888066117

Epoch: 5| Step: 2
Training loss: 2.7723729610443115
Validation loss: 2.8919428163959133

Epoch: 5| Step: 3
Training loss: 3.713113307952881
Validation loss: 2.890419337057298

Epoch: 5| Step: 4
Training loss: 2.8846449851989746
Validation loss: 2.885538160160024

Epoch: 5| Step: 5
Training loss: 2.417203426361084
Validation loss: 2.880389459671513

Epoch: 5| Step: 6
Training loss: 3.3715126514434814
Validation loss: 2.8721251308277087

Epoch: 5| Step: 7
Training loss: 3.0080783367156982
Validation loss: 2.8672105009837816

Epoch: 5| Step: 8
Training loss: 3.2981274127960205
Validation loss: 2.860406485936975

Epoch: 5| Step: 9
Training loss: 2.8760628700256348
Validation loss: 2.8581310472180768

Epoch: 5| Step: 10
Training loss: 3.0294106006622314
Validation loss: 2.8587498254673456

Epoch: 32| Step: 0
Training loss: 2.1345367431640625
Validation loss: 2.8596258624907462

Epoch: 5| Step: 1
Training loss: 3.078984022140503
Validation loss: 2.8622406093023156

Epoch: 5| Step: 2
Training loss: 2.0480387210845947
Validation loss: 2.858884173054849

Epoch: 5| Step: 3
Training loss: 3.150113582611084
Validation loss: 2.8477388287103302

Epoch: 5| Step: 4
Training loss: 3.2783827781677246
Validation loss: 2.8510884443918862

Epoch: 5| Step: 5
Training loss: 2.6655914783477783
Validation loss: 2.848286480031988

Epoch: 5| Step: 6
Training loss: 3.2970569133758545
Validation loss: 2.852246658776396

Epoch: 5| Step: 7
Training loss: 3.564405918121338
Validation loss: 2.857775180570541

Epoch: 5| Step: 8
Training loss: 2.222198486328125
Validation loss: 2.853308429000198

Epoch: 5| Step: 9
Training loss: 3.5773768424987793
Validation loss: 2.8503713582151677

Epoch: 5| Step: 10
Training loss: 3.577230930328369
Validation loss: 2.8466712223586215

Epoch: 33| Step: 0
Training loss: 2.4934184551239014
Validation loss: 2.836356060479277

Epoch: 5| Step: 1
Training loss: 2.8918862342834473
Validation loss: 2.829450612427086

Epoch: 5| Step: 2
Training loss: 2.4736227989196777
Validation loss: 2.8277641009258967

Epoch: 5| Step: 3
Training loss: 3.047093629837036
Validation loss: 2.8273561077733196

Epoch: 5| Step: 4
Training loss: 3.074678421020508
Validation loss: 2.8267668498459684

Epoch: 5| Step: 5
Training loss: 2.883507251739502
Validation loss: 2.824387558044926

Epoch: 5| Step: 6
Training loss: 3.4426841735839844
Validation loss: 2.821275052203927

Epoch: 5| Step: 7
Training loss: 2.9526963233947754
Validation loss: 2.8192945757219867

Epoch: 5| Step: 8
Training loss: 3.2006733417510986
Validation loss: 2.8177442704477618

Epoch: 5| Step: 9
Training loss: 3.0122203826904297
Validation loss: 2.8161111134354786

Epoch: 5| Step: 10
Training loss: 2.852574348449707
Validation loss: 2.81453675095753

Epoch: 34| Step: 0
Training loss: 2.8896050453186035
Validation loss: 2.8144477823729157

Epoch: 5| Step: 1
Training loss: 2.668165445327759
Validation loss: 2.8148526273747927

Epoch: 5| Step: 2
Training loss: 3.0273962020874023
Validation loss: 2.820214484327583

Epoch: 5| Step: 3
Training loss: 2.916177988052368
Validation loss: 2.8171630033882717

Epoch: 5| Step: 4
Training loss: 3.2135462760925293
Validation loss: 2.817103083415698

Epoch: 5| Step: 5
Training loss: 3.660872220993042
Validation loss: 2.8153090682080997

Epoch: 5| Step: 6
Training loss: 2.356025218963623
Validation loss: 2.8143817481174263

Epoch: 5| Step: 7
Training loss: 2.47237491607666
Validation loss: 2.8085578846675094

Epoch: 5| Step: 8
Training loss: 3.5520682334899902
Validation loss: 2.8086118326392224

Epoch: 5| Step: 9
Training loss: 3.091376781463623
Validation loss: 2.8076132843571324

Epoch: 5| Step: 10
Training loss: 2.2561681270599365
Validation loss: 2.8075871518863145

Epoch: 35| Step: 0
Training loss: 2.025918483734131
Validation loss: 2.8060393230889433

Epoch: 5| Step: 1
Training loss: 2.5355145931243896
Validation loss: 2.8079217121165287

Epoch: 5| Step: 2
Training loss: 2.710341691970825
Validation loss: 2.8039898769829863

Epoch: 5| Step: 3
Training loss: 3.2648727893829346
Validation loss: 2.8015916321867254

Epoch: 5| Step: 4
Training loss: 2.900949478149414
Validation loss: 2.8018804980862524

Epoch: 5| Step: 5
Training loss: 2.4819843769073486
Validation loss: 2.8014471966733216

Epoch: 5| Step: 6
Training loss: 2.6213560104370117
Validation loss: 2.7988929466534684

Epoch: 5| Step: 7
Training loss: 3.3983395099639893
Validation loss: 2.7988478419601277

Epoch: 5| Step: 8
Training loss: 3.7084453105926514
Validation loss: 2.8007310487890757

Epoch: 5| Step: 9
Training loss: 3.660142183303833
Validation loss: 2.8017091597280195

Epoch: 5| Step: 10
Training loss: 2.7681050300598145
Validation loss: 2.801024067786432

Epoch: 36| Step: 0
Training loss: 2.9438986778259277
Validation loss: 2.8078433313677387

Epoch: 5| Step: 1
Training loss: 3.3454155921936035
Validation loss: 2.803484644941104

Epoch: 5| Step: 2
Training loss: 2.1304361820220947
Validation loss: 2.8105795691090245

Epoch: 5| Step: 3
Training loss: 3.2274138927459717
Validation loss: 2.816797953779979

Epoch: 5| Step: 4
Training loss: 2.8855578899383545
Validation loss: 2.8066914799392864

Epoch: 5| Step: 5
Training loss: 3.678377628326416
Validation loss: 2.7986380592469247

Epoch: 5| Step: 6
Training loss: 2.6049387454986572
Validation loss: 2.794282579934725

Epoch: 5| Step: 7
Training loss: 2.5159244537353516
Validation loss: 2.790288543188444

Epoch: 5| Step: 8
Training loss: 2.701505184173584
Validation loss: 2.7892253680895736

Epoch: 5| Step: 9
Training loss: 2.8759117126464844
Validation loss: 2.788521256498111

Epoch: 5| Step: 10
Training loss: 3.190828323364258
Validation loss: 2.7908027069542998

Epoch: 37| Step: 0
Training loss: 2.623103380203247
Validation loss: 2.788186038694074

Epoch: 5| Step: 1
Training loss: 3.3329429626464844
Validation loss: 2.785587695337111

Epoch: 5| Step: 2
Training loss: 2.7706611156463623
Validation loss: 2.783891970111478

Epoch: 5| Step: 3
Training loss: 2.8631176948547363
Validation loss: 2.778107766182192

Epoch: 5| Step: 4
Training loss: 2.4722938537597656
Validation loss: 2.778873135966639

Epoch: 5| Step: 5
Training loss: 1.9324915409088135
Validation loss: 2.77680112982309

Epoch: 5| Step: 6
Training loss: 3.3898377418518066
Validation loss: 2.7719129875142086

Epoch: 5| Step: 7
Training loss: 2.969679355621338
Validation loss: 2.7640213658732753

Epoch: 5| Step: 8
Training loss: 4.449936866760254
Validation loss: 2.760601343647126

Epoch: 5| Step: 9
Training loss: 2.650106430053711
Validation loss: 2.7562509890525573

Epoch: 5| Step: 10
Training loss: 2.3994576930999756
Validation loss: 2.7563930660165767

Epoch: 38| Step: 0
Training loss: 3.2306666374206543
Validation loss: 2.7556558706427134

Epoch: 5| Step: 1
Training loss: 2.7422938346862793
Validation loss: 2.760592173504573

Epoch: 5| Step: 2
Training loss: 2.1625990867614746
Validation loss: 2.787809633439587

Epoch: 5| Step: 3
Training loss: 2.3637967109680176
Validation loss: 2.783249988350817

Epoch: 5| Step: 4
Training loss: 3.2468204498291016
Validation loss: 2.776899032695319

Epoch: 5| Step: 5
Training loss: 3.7707676887512207
Validation loss: 2.757776770540463

Epoch: 5| Step: 6
Training loss: 3.002204418182373
Validation loss: 2.7540277024751068

Epoch: 5| Step: 7
Training loss: 2.243940830230713
Validation loss: 2.7580652108756443

Epoch: 5| Step: 8
Training loss: 3.3820056915283203
Validation loss: 2.768572361238541

Epoch: 5| Step: 9
Training loss: 2.834181308746338
Validation loss: 2.7787703057771087

Epoch: 5| Step: 10
Training loss: 2.7858381271362305
Validation loss: 2.78076365429868

Epoch: 39| Step: 0
Training loss: 2.183474063873291
Validation loss: 2.762065513159639

Epoch: 5| Step: 1
Training loss: 3.2756717205047607
Validation loss: 2.7508103180957097

Epoch: 5| Step: 2
Training loss: 3.433013916015625
Validation loss: 2.7461758326458674

Epoch: 5| Step: 3
Training loss: 2.8646559715270996
Validation loss: 2.7465096212202504

Epoch: 5| Step: 4
Training loss: 2.7142748832702637
Validation loss: 2.7454865619700444

Epoch: 5| Step: 5
Training loss: 2.47882342338562
Validation loss: 2.741724142464258

Epoch: 5| Step: 6
Training loss: 2.8968071937561035
Validation loss: 2.7375416268584547

Epoch: 5| Step: 7
Training loss: 3.490990400314331
Validation loss: 2.7385038227163334

Epoch: 5| Step: 8
Training loss: 2.7300021648406982
Validation loss: 2.7403295809222805

Epoch: 5| Step: 9
Training loss: 3.0945065021514893
Validation loss: 2.7396340626542286

Epoch: 5| Step: 10
Training loss: 2.4016358852386475
Validation loss: 2.7409653099634315

Epoch: 40| Step: 0
Training loss: 2.228039264678955
Validation loss: 2.7409703577718427

Epoch: 5| Step: 1
Training loss: 3.105912923812866
Validation loss: 2.7374937329241025

Epoch: 5| Step: 2
Training loss: 2.6942503452301025
Validation loss: 2.7316823313313146

Epoch: 5| Step: 3
Training loss: 2.865947961807251
Validation loss: 2.7317462787833264

Epoch: 5| Step: 4
Training loss: 2.536865234375
Validation loss: 2.72996598674405

Epoch: 5| Step: 5
Training loss: 3.1887755393981934
Validation loss: 2.7267651250285487

Epoch: 5| Step: 6
Training loss: 3.068213939666748
Validation loss: 2.7287121793275237

Epoch: 5| Step: 7
Training loss: 3.262014865875244
Validation loss: 2.7275247932762228

Epoch: 5| Step: 8
Training loss: 3.2220542430877686
Validation loss: 2.7267542039194415

Epoch: 5| Step: 9
Training loss: 2.712578296661377
Validation loss: 2.724033142930718

Epoch: 5| Step: 10
Training loss: 2.629700183868408
Validation loss: 2.727429702717771

Epoch: 41| Step: 0
Training loss: 2.531297206878662
Validation loss: 2.7262324902319137

Epoch: 5| Step: 1
Training loss: 3.1264729499816895
Validation loss: 2.7295170061049925

Epoch: 5| Step: 2
Training loss: 3.2395412921905518
Validation loss: 2.727958853526782

Epoch: 5| Step: 3
Training loss: 3.2314629554748535
Validation loss: 2.730787605367681

Epoch: 5| Step: 4
Training loss: 2.50129771232605
Validation loss: 2.729051072110412

Epoch: 5| Step: 5
Training loss: 2.7724404335021973
Validation loss: 2.7253260868851856

Epoch: 5| Step: 6
Training loss: 2.5605406761169434
Validation loss: 2.722156575931016

Epoch: 5| Step: 7
Training loss: 4.0721049308776855
Validation loss: 2.720571508971594

Epoch: 5| Step: 8
Training loss: 2.3979434967041016
Validation loss: 2.7181977969343945

Epoch: 5| Step: 9
Training loss: 2.3854587078094482
Validation loss: 2.7130763812731673

Epoch: 5| Step: 10
Training loss: 2.6171839237213135
Validation loss: 2.7117894003468175

Epoch: 42| Step: 0
Training loss: 2.0006766319274902
Validation loss: 2.70828588034517

Epoch: 5| Step: 1
Training loss: 2.6025002002716064
Validation loss: 2.7091362912167787

Epoch: 5| Step: 2
Training loss: 3.230872631072998
Validation loss: 2.709698825754145

Epoch: 5| Step: 3
Training loss: 2.300279140472412
Validation loss: 2.7033038446980138

Epoch: 5| Step: 4
Training loss: 3.490920305252075
Validation loss: 2.6970497972221783

Epoch: 5| Step: 5
Training loss: 2.8419060707092285
Validation loss: 2.6947334863806285

Epoch: 5| Step: 6
Training loss: 2.7604918479919434
Validation loss: 2.6929017343828754

Epoch: 5| Step: 7
Training loss: 3.4208927154541016
Validation loss: 2.691021083503641

Epoch: 5| Step: 8
Training loss: 2.586050510406494
Validation loss: 2.6902231708649667

Epoch: 5| Step: 9
Training loss: 2.9341626167297363
Validation loss: 2.691388989007601

Epoch: 5| Step: 10
Training loss: 3.1685593128204346
Validation loss: 2.688711086908976

Epoch: 43| Step: 0
Training loss: 3.2776522636413574
Validation loss: 2.6818881650124826

Epoch: 5| Step: 1
Training loss: 3.4453177452087402
Validation loss: 2.680713169036373

Epoch: 5| Step: 2
Training loss: 2.0570757389068604
Validation loss: 2.6837107878859325

Epoch: 5| Step: 3
Training loss: 2.380317211151123
Validation loss: 2.7038795742937314

Epoch: 5| Step: 4
Training loss: 2.229856014251709
Validation loss: 2.7064742375445623

Epoch: 5| Step: 5
Training loss: 3.3124451637268066
Validation loss: 2.6800045300555486

Epoch: 5| Step: 6
Training loss: 3.2575035095214844
Validation loss: 2.6769410487144225

Epoch: 5| Step: 7
Training loss: 2.4276928901672363
Validation loss: 2.6764793857451408

Epoch: 5| Step: 8
Training loss: 2.7348804473876953
Validation loss: 2.679976288990308

Epoch: 5| Step: 9
Training loss: 2.722102642059326
Validation loss: 2.681759757380332

Epoch: 5| Step: 10
Training loss: 3.4045581817626953
Validation loss: 2.679227687979257

Epoch: 44| Step: 0
Training loss: 2.9307079315185547
Validation loss: 2.675014098485311

Epoch: 5| Step: 1
Training loss: 3.58107328414917
Validation loss: 2.6724450972772416

Epoch: 5| Step: 2
Training loss: 2.955944776535034
Validation loss: 2.6684393934024278

Epoch: 5| Step: 3
Training loss: 2.6875765323638916
Validation loss: 2.6708293345666703

Epoch: 5| Step: 4
Training loss: 2.561819076538086
Validation loss: 2.6786994754627185

Epoch: 5| Step: 5
Training loss: 2.8785886764526367
Validation loss: 2.6791068097596527

Epoch: 5| Step: 6
Training loss: 2.9684479236602783
Validation loss: 2.6782337773230767

Epoch: 5| Step: 7
Training loss: 3.138679027557373
Validation loss: 2.678271021894229

Epoch: 5| Step: 8
Training loss: 2.5878055095672607
Validation loss: 2.674745536619617

Epoch: 5| Step: 9
Training loss: 2.473893404006958
Validation loss: 2.67297601443465

Epoch: 5| Step: 10
Training loss: 2.339709520339966
Validation loss: 2.666722136159097

Epoch: 45| Step: 0
Training loss: 2.378455400466919
Validation loss: 2.6707558631896973

Epoch: 5| Step: 1
Training loss: 2.8260626792907715
Validation loss: 2.6671259915956886

Epoch: 5| Step: 2
Training loss: 2.788459300994873
Validation loss: 2.663700431905767

Epoch: 5| Step: 3
Training loss: 2.6820247173309326
Validation loss: 2.662491467691237

Epoch: 5| Step: 4
Training loss: 3.6166205406188965
Validation loss: 2.66082586267943

Epoch: 5| Step: 5
Training loss: 2.9138834476470947
Validation loss: 2.6605947786761868

Epoch: 5| Step: 6
Training loss: 2.2050013542175293
Validation loss: 2.66263686713352

Epoch: 5| Step: 7
Training loss: 3.1692004203796387
Validation loss: 2.6624973845738236

Epoch: 5| Step: 8
Training loss: 2.835017681121826
Validation loss: 2.662779797789871

Epoch: 5| Step: 9
Training loss: 3.158174514770508
Validation loss: 2.6607057689338602

Epoch: 5| Step: 10
Training loss: 2.430093765258789
Validation loss: 2.660543623790946

Epoch: 46| Step: 0
Training loss: 2.9831042289733887
Validation loss: 2.6600304649722193

Epoch: 5| Step: 1
Training loss: 2.9344801902770996
Validation loss: 2.6539636965720885

Epoch: 5| Step: 2
Training loss: 2.9460697174072266
Validation loss: 2.6567574624092347

Epoch: 5| Step: 3
Training loss: 2.644949436187744
Validation loss: 2.6515765933580298

Epoch: 5| Step: 4
Training loss: 2.7457079887390137
Validation loss: 2.652222074488158

Epoch: 5| Step: 5
Training loss: 2.9162516593933105
Validation loss: 2.6519676459732877

Epoch: 5| Step: 6
Training loss: 2.572516918182373
Validation loss: 2.6502871000638573

Epoch: 5| Step: 7
Training loss: 2.8998425006866455
Validation loss: 2.6501638299675396

Epoch: 5| Step: 8
Training loss: 2.52213191986084
Validation loss: 2.6512450479692027

Epoch: 5| Step: 9
Training loss: 3.186244249343872
Validation loss: 2.659315888599683

Epoch: 5| Step: 10
Training loss: 2.5737667083740234
Validation loss: 2.6645532936178227

Epoch: 47| Step: 0
Training loss: 2.577666759490967
Validation loss: 2.6911539467432166

Epoch: 5| Step: 1
Training loss: 3.2339749336242676
Validation loss: 2.692618316219699

Epoch: 5| Step: 2
Training loss: 2.543893337249756
Validation loss: 2.6557997862497964

Epoch: 5| Step: 3
Training loss: 2.6112513542175293
Validation loss: 2.646827438826202

Epoch: 5| Step: 4
Training loss: 3.215900421142578
Validation loss: 2.660743180141654

Epoch: 5| Step: 5
Training loss: 2.7114548683166504
Validation loss: 2.6734251386375836

Epoch: 5| Step: 6
Training loss: 3.1826906204223633
Validation loss: 2.671612649835566

Epoch: 5| Step: 7
Training loss: 2.970153331756592
Validation loss: 2.67036037547614

Epoch: 5| Step: 8
Training loss: 2.7579874992370605
Validation loss: 2.660029508734262

Epoch: 5| Step: 9
Training loss: 2.988676071166992
Validation loss: 2.6497540935393302

Epoch: 5| Step: 10
Training loss: 2.246986150741577
Validation loss: 2.6493443109655894

Epoch: 48| Step: 0
Training loss: 2.9402248859405518
Validation loss: 2.646975945400935

Epoch: 5| Step: 1
Training loss: 2.934936046600342
Validation loss: 2.64626214068423

Epoch: 5| Step: 2
Training loss: 2.718907117843628
Validation loss: 2.64488899066884

Epoch: 5| Step: 3
Training loss: 2.792614698410034
Validation loss: 2.647833567793651

Epoch: 5| Step: 4
Training loss: 3.115638017654419
Validation loss: 2.643018789188836

Epoch: 5| Step: 5
Training loss: 3.1394896507263184
Validation loss: 2.6423477075433217

Epoch: 5| Step: 6
Training loss: 2.574211597442627
Validation loss: 2.6361151818306214

Epoch: 5| Step: 7
Training loss: 2.9186978340148926
Validation loss: 2.636320137208508

Epoch: 5| Step: 8
Training loss: 1.8846622705459595
Validation loss: 2.6367934109062277

Epoch: 5| Step: 9
Training loss: 2.8534090518951416
Validation loss: 2.636089769742822

Epoch: 5| Step: 10
Training loss: 3.007383108139038
Validation loss: 2.6362803315603607

Epoch: 49| Step: 0
Training loss: 2.9394302368164062
Validation loss: 2.6355779273535616

Epoch: 5| Step: 1
Training loss: 2.907726764678955
Validation loss: 2.6360983951117403

Epoch: 5| Step: 2
Training loss: 2.822366237640381
Validation loss: 2.633775700805008

Epoch: 5| Step: 3
Training loss: 2.8533737659454346
Validation loss: 2.6304329005620812

Epoch: 5| Step: 4
Training loss: 3.202263593673706
Validation loss: 2.6308440393017185

Epoch: 5| Step: 5
Training loss: 2.984853744506836
Validation loss: 2.627337199385448

Epoch: 5| Step: 6
Training loss: 2.8515689373016357
Validation loss: 2.632079819197296

Epoch: 5| Step: 7
Training loss: 2.4263386726379395
Validation loss: 2.628383780038485

Epoch: 5| Step: 8
Training loss: 3.0449156761169434
Validation loss: 2.6253408616588962

Epoch: 5| Step: 9
Training loss: 2.4214775562286377
Validation loss: 2.626009638591479

Epoch: 5| Step: 10
Training loss: 2.217649221420288
Validation loss: 2.630390792764643

Epoch: 50| Step: 0
Training loss: 3.109682321548462
Validation loss: 2.6481187830689135

Epoch: 5| Step: 1
Training loss: 2.706925868988037
Validation loss: 2.6479823204778854

Epoch: 5| Step: 2
Training loss: 2.321507453918457
Validation loss: 2.6485339287788636

Epoch: 5| Step: 3
Training loss: 3.113060474395752
Validation loss: 2.643808126449585

Epoch: 5| Step: 4
Training loss: 2.297006130218506
Validation loss: 2.637947000483031

Epoch: 5| Step: 5
Training loss: 3.0769290924072266
Validation loss: 2.636953665364173

Epoch: 5| Step: 6
Training loss: 2.9164109230041504
Validation loss: 2.632065657646425

Epoch: 5| Step: 7
Training loss: 2.5723063945770264
Validation loss: 2.626499838726495

Epoch: 5| Step: 8
Training loss: 2.7170825004577637
Validation loss: 2.625893879962224

Epoch: 5| Step: 9
Training loss: 2.66499662399292
Validation loss: 2.6213874483621247

Epoch: 5| Step: 10
Training loss: 3.3289058208465576
Validation loss: 2.622053551417525

Epoch: 51| Step: 0
Training loss: 3.0612354278564453
Validation loss: 2.6262775544197328

Epoch: 5| Step: 1
Training loss: 3.0544490814208984
Validation loss: 2.6306665507696008

Epoch: 5| Step: 2
Training loss: 3.3891875743865967
Validation loss: 2.6298814024976505

Epoch: 5| Step: 3
Training loss: 2.6968252658843994
Validation loss: 2.628938890272571

Epoch: 5| Step: 4
Training loss: 2.53177547454834
Validation loss: 2.627117895310925

Epoch: 5| Step: 5
Training loss: 2.2746217250823975
Validation loss: 2.624568780263265

Epoch: 5| Step: 6
Training loss: 2.725393772125244
Validation loss: 2.6216300379845405

Epoch: 5| Step: 7
Training loss: 2.5827929973602295
Validation loss: 2.6164560548720823

Epoch: 5| Step: 8
Training loss: 2.448631763458252
Validation loss: 2.61820436164897

Epoch: 5| Step: 9
Training loss: 3.11034893989563
Validation loss: 2.62548912725141

Epoch: 5| Step: 10
Training loss: 2.7955162525177
Validation loss: 2.6387544139739005

Epoch: 52| Step: 0
Training loss: 2.493068218231201
Validation loss: 2.6667679125262844

Epoch: 5| Step: 1
Training loss: 2.488839864730835
Validation loss: 2.6926073464014197

Epoch: 5| Step: 2
Training loss: 3.1146135330200195
Validation loss: 2.6666222387744534

Epoch: 5| Step: 3
Training loss: 2.979684829711914
Validation loss: 2.660302318552489

Epoch: 5| Step: 4
Training loss: 2.9222917556762695
Validation loss: 2.6594164602218138

Epoch: 5| Step: 5
Training loss: 3.4291534423828125
Validation loss: 2.6289247505126463

Epoch: 5| Step: 6
Training loss: 2.573117256164551
Validation loss: 2.6177310174511326

Epoch: 5| Step: 7
Training loss: 2.989581823348999
Validation loss: 2.613649942541635

Epoch: 5| Step: 8
Training loss: 2.3762168884277344
Validation loss: 2.627399739398751

Epoch: 5| Step: 9
Training loss: 2.968687057495117
Validation loss: 2.627744710573586

Epoch: 5| Step: 10
Training loss: 2.4830946922302246
Validation loss: 2.6212451073431198

Epoch: 53| Step: 0
Training loss: 3.043771266937256
Validation loss: 2.6217205421898955

Epoch: 5| Step: 1
Training loss: 3.652588367462158
Validation loss: 2.6158393070261967

Epoch: 5| Step: 2
Training loss: 2.4803247451782227
Validation loss: 2.6203869465858705

Epoch: 5| Step: 3
Training loss: 2.9500820636749268
Validation loss: 2.627624729628204

Epoch: 5| Step: 4
Training loss: 2.5332698822021484
Validation loss: 2.6294605065417547

Epoch: 5| Step: 5
Training loss: 2.463933229446411
Validation loss: 2.630122359080981

Epoch: 5| Step: 6
Training loss: 2.638293743133545
Validation loss: 2.6220664619117655

Epoch: 5| Step: 7
Training loss: 3.02618145942688
Validation loss: 2.6125335898450626

Epoch: 5| Step: 8
Training loss: 2.2509047985076904
Validation loss: 2.6146305607211207

Epoch: 5| Step: 9
Training loss: 2.5573489665985107
Validation loss: 2.616136484248664

Epoch: 5| Step: 10
Training loss: 3.0994420051574707
Validation loss: 2.6207043816966396

Epoch: 54| Step: 0
Training loss: 2.379851818084717
Validation loss: 2.617758979079544

Epoch: 5| Step: 1
Training loss: 2.5411739349365234
Validation loss: 2.6135445512751097

Epoch: 5| Step: 2
Training loss: 2.45076584815979
Validation loss: 2.600564746446507

Epoch: 5| Step: 3
Training loss: 3.206547260284424
Validation loss: 2.603769712550666

Epoch: 5| Step: 4
Training loss: 3.091310977935791
Validation loss: 2.6157567475431707

Epoch: 5| Step: 5
Training loss: 2.32102108001709
Validation loss: 2.6271317466612785

Epoch: 5| Step: 6
Training loss: 2.8035717010498047
Validation loss: 2.6285284026976554

Epoch: 5| Step: 7
Training loss: 3.203585386276245
Validation loss: 2.6263827969951015

Epoch: 5| Step: 8
Training loss: 2.5613224506378174
Validation loss: 2.6202963859804216

Epoch: 5| Step: 9
Training loss: 2.7782270908355713
Validation loss: 2.611829742308586

Epoch: 5| Step: 10
Training loss: 3.4940783977508545
Validation loss: 2.601116995657644

Epoch: 55| Step: 0
Training loss: 2.539670467376709
Validation loss: 2.598625657378986

Epoch: 5| Step: 1
Training loss: 2.912221908569336
Validation loss: 2.5948055380134174

Epoch: 5| Step: 2
Training loss: 2.5884716510772705
Validation loss: 2.6056430621813704

Epoch: 5| Step: 3
Training loss: 2.817399740219116
Validation loss: 2.644711927701068

Epoch: 5| Step: 4
Training loss: 2.519503116607666
Validation loss: 2.648108213178573

Epoch: 5| Step: 5
Training loss: 2.7479920387268066
Validation loss: 2.62710323128649

Epoch: 5| Step: 6
Training loss: 2.421408176422119
Validation loss: 2.6200251297284196

Epoch: 5| Step: 7
Training loss: 2.705132007598877
Validation loss: 2.6046887572093675

Epoch: 5| Step: 8
Training loss: 3.48834228515625
Validation loss: 2.5930085592372443

Epoch: 5| Step: 9
Training loss: 2.873944044113159
Validation loss: 2.590119374695645

Epoch: 5| Step: 10
Training loss: 2.8200523853302
Validation loss: 2.5874755151810183

Epoch: 56| Step: 0
Training loss: 2.67034649848938
Validation loss: 2.5938773693576938

Epoch: 5| Step: 1
Training loss: 3.121169328689575
Validation loss: 2.5899386380308416

Epoch: 5| Step: 2
Training loss: 3.489001750946045
Validation loss: 2.5950718105480237

Epoch: 5| Step: 3
Training loss: 2.834383249282837
Validation loss: 2.5932696762905327

Epoch: 5| Step: 4
Training loss: 2.47499418258667
Validation loss: 2.590079597247544

Epoch: 5| Step: 5
Training loss: 2.4512627124786377
Validation loss: 2.584322396145072

Epoch: 5| Step: 6
Training loss: 3.4612343311309814
Validation loss: 2.5837195047768216

Epoch: 5| Step: 7
Training loss: 3.2481250762939453
Validation loss: 2.5817364159450737

Epoch: 5| Step: 8
Training loss: 2.2582383155822754
Validation loss: 2.5849730225019556

Epoch: 5| Step: 9
Training loss: 2.0709118843078613
Validation loss: 2.58389510134215

Epoch: 5| Step: 10
Training loss: 2.2384676933288574
Validation loss: 2.5864093790772142

Epoch: 57| Step: 0
Training loss: 2.8004345893859863
Validation loss: 2.5918669803168184

Epoch: 5| Step: 1
Training loss: 2.80806303024292
Validation loss: 2.6048506485518588

Epoch: 5| Step: 2
Training loss: 3.00488543510437
Validation loss: 2.6115080605271044

Epoch: 5| Step: 3
Training loss: 2.6036226749420166
Validation loss: 2.609687956430579

Epoch: 5| Step: 4
Training loss: 3.377899169921875
Validation loss: 2.601648804961994

Epoch: 5| Step: 5
Training loss: 2.8362467288970947
Validation loss: 2.604628183508432

Epoch: 5| Step: 6
Training loss: 3.2134780883789062
Validation loss: 2.5979281574167232

Epoch: 5| Step: 7
Training loss: 2.5973713397979736
Validation loss: 2.589202819332

Epoch: 5| Step: 8
Training loss: 2.4809300899505615
Validation loss: 2.5848346781987015

Epoch: 5| Step: 9
Training loss: 1.692744255065918
Validation loss: 2.58227861824856

Epoch: 5| Step: 10
Training loss: 2.893325090408325
Validation loss: 2.5758260706419587

Epoch: 58| Step: 0
Training loss: 2.8156754970550537
Validation loss: 2.576208645297635

Epoch: 5| Step: 1
Training loss: 2.742173671722412
Validation loss: 2.5773601865255706

Epoch: 5| Step: 2
Training loss: 2.105936050415039
Validation loss: 2.5829848474071873

Epoch: 5| Step: 3
Training loss: 3.0144505500793457
Validation loss: 2.5871115961382465

Epoch: 5| Step: 4
Training loss: 2.1707539558410645
Validation loss: 2.5768339762123684

Epoch: 5| Step: 5
Training loss: 3.1417624950408936
Validation loss: 2.5688331075893935

Epoch: 5| Step: 6
Training loss: 3.2118964195251465
Validation loss: 2.5747961305802867

Epoch: 5| Step: 7
Training loss: 2.4289393424987793
Validation loss: 2.575606107711792

Epoch: 5| Step: 8
Training loss: 2.7887587547302246
Validation loss: 2.5789015652031027

Epoch: 5| Step: 9
Training loss: 3.3546829223632812
Validation loss: 2.5826392276312715

Epoch: 5| Step: 10
Training loss: 2.463960886001587
Validation loss: 2.5899584921457435

Epoch: 59| Step: 0
Training loss: 2.1354286670684814
Validation loss: 2.590979991420623

Epoch: 5| Step: 1
Training loss: 2.750352382659912
Validation loss: 2.624046943520987

Epoch: 5| Step: 2
Training loss: 2.5710740089416504
Validation loss: 2.6495466206663396

Epoch: 5| Step: 3
Training loss: 3.1952404975891113
Validation loss: 2.67407319366291

Epoch: 5| Step: 4
Training loss: 2.799973249435425
Validation loss: 2.600064298158051

Epoch: 5| Step: 5
Training loss: 2.5641560554504395
Validation loss: 2.565731825367097

Epoch: 5| Step: 6
Training loss: 3.692307233810425
Validation loss: 2.558939528721635

Epoch: 5| Step: 7
Training loss: 2.9137399196624756
Validation loss: 2.5705619063428653

Epoch: 5| Step: 8
Training loss: 2.4623544216156006
Validation loss: 2.587644656499227

Epoch: 5| Step: 9
Training loss: 2.740293502807617
Validation loss: 2.6291195884827645

Epoch: 5| Step: 10
Training loss: 2.664768934249878
Validation loss: 2.690123155552854

Epoch: 60| Step: 0
Training loss: 3.4146885871887207
Validation loss: 2.660001280487225

Epoch: 5| Step: 1
Training loss: 2.5531582832336426
Validation loss: 2.593185870878158

Epoch: 5| Step: 2
Training loss: 2.883070230484009
Validation loss: 2.5649877427726664

Epoch: 5| Step: 3
Training loss: 2.3567214012145996
Validation loss: 2.573418099393127

Epoch: 5| Step: 4
Training loss: 2.5552382469177246
Validation loss: 2.6580809803419214

Epoch: 5| Step: 5
Training loss: 2.961662530899048
Validation loss: 2.7661042700531664

Epoch: 5| Step: 6
Training loss: 2.6804909706115723
Validation loss: 2.682358531541722

Epoch: 5| Step: 7
Training loss: 3.137026786804199
Validation loss: 2.589663856772966

Epoch: 5| Step: 8
Training loss: 3.1846675872802734
Validation loss: 2.5597628470390075

Epoch: 5| Step: 9
Training loss: 2.441152572631836
Validation loss: 2.553159720154219

Epoch: 5| Step: 10
Training loss: 2.489496946334839
Validation loss: 2.5583480019723215

Epoch: 61| Step: 0
Training loss: 2.8906631469726562
Validation loss: 2.5817748423545592

Epoch: 5| Step: 1
Training loss: 2.698892831802368
Validation loss: 2.6099301230522896

Epoch: 5| Step: 2
Training loss: 2.125495433807373
Validation loss: 2.658180093252531

Epoch: 5| Step: 3
Training loss: 3.0917491912841797
Validation loss: 2.791765743686307

Epoch: 5| Step: 4
Training loss: 2.9353480339050293
Validation loss: 2.765067905508062

Epoch: 5| Step: 5
Training loss: 2.770382881164551
Validation loss: 2.6694947647792038

Epoch: 5| Step: 6
Training loss: 2.730083703994751
Validation loss: 2.5620815497572704

Epoch: 5| Step: 7
Training loss: 2.2913413047790527
Validation loss: 2.5452209006073656

Epoch: 5| Step: 8
Training loss: 3.626849412918091
Validation loss: 2.5526651310664352

Epoch: 5| Step: 9
Training loss: 2.375290632247925
Validation loss: 2.585717696015553

Epoch: 5| Step: 10
Training loss: 3.244506597518921
Validation loss: 2.697656023886896

Epoch: 62| Step: 0
Training loss: 2.894535541534424
Validation loss: 2.7814210896850913

Epoch: 5| Step: 1
Training loss: 2.7904772758483887
Validation loss: 2.8526452074768724

Epoch: 5| Step: 2
Training loss: 3.2002978324890137
Validation loss: 2.771137652858611

Epoch: 5| Step: 3
Training loss: 2.8499112129211426
Validation loss: 2.695021819042903

Epoch: 5| Step: 4
Training loss: 2.58135986328125
Validation loss: 2.5742830717435448

Epoch: 5| Step: 5
Training loss: 3.149629592895508
Validation loss: 2.543447991853119

Epoch: 5| Step: 6
Training loss: 2.7444021701812744
Validation loss: 2.5432275533676147

Epoch: 5| Step: 7
Training loss: 2.840599298477173
Validation loss: 2.5711489492847073

Epoch: 5| Step: 8
Training loss: 2.5248310565948486
Validation loss: 2.5905686706625004

Epoch: 5| Step: 9
Training loss: 2.346557140350342
Validation loss: 2.598014695670015

Epoch: 5| Step: 10
Training loss: 2.6560213565826416
Validation loss: 2.602894513837753

Epoch: 63| Step: 0
Training loss: 3.7305023670196533
Validation loss: 2.615038069345618

Epoch: 5| Step: 1
Training loss: 2.4281134605407715
Validation loss: 2.6107090160410893

Epoch: 5| Step: 2
Training loss: 3.581408739089966
Validation loss: 2.6161553141891316

Epoch: 5| Step: 3
Training loss: 2.8442916870117188
Validation loss: 2.604393292498845

Epoch: 5| Step: 4
Training loss: 1.7639448642730713
Validation loss: 2.591124347461167

Epoch: 5| Step: 5
Training loss: 3.0386667251586914
Validation loss: 2.5877280671109437

Epoch: 5| Step: 6
Training loss: 2.561744213104248
Validation loss: 2.573845958196989

Epoch: 5| Step: 7
Training loss: 2.4106357097625732
Validation loss: 2.55484769164875

Epoch: 5| Step: 8
Training loss: 1.9181785583496094
Validation loss: 2.5452549201185986

Epoch: 5| Step: 9
Training loss: 3.0035040378570557
Validation loss: 2.5327692134405977

Epoch: 5| Step: 10
Training loss: 3.1864101886749268
Validation loss: 2.534739141823143

Epoch: 64| Step: 0
Training loss: 2.685560464859009
Validation loss: 2.5323118855876308

Epoch: 5| Step: 1
Training loss: 2.5271918773651123
Validation loss: 2.5392148751084522

Epoch: 5| Step: 2
Training loss: 2.5286850929260254
Validation loss: 2.5567791602944814

Epoch: 5| Step: 3
Training loss: 2.886965274810791
Validation loss: 2.560836607410062

Epoch: 5| Step: 4
Training loss: 2.894087314605713
Validation loss: 2.54615387352564

Epoch: 5| Step: 5
Training loss: 2.4775922298431396
Validation loss: 2.539466022163309

Epoch: 5| Step: 6
Training loss: 2.5902535915374756
Validation loss: 2.539180665887812

Epoch: 5| Step: 7
Training loss: 3.594486951828003
Validation loss: 2.532542642726693

Epoch: 5| Step: 8
Training loss: 1.8797677755355835
Validation loss: 2.530674165295016

Epoch: 5| Step: 9
Training loss: 2.798698902130127
Validation loss: 2.530828781025384

Epoch: 5| Step: 10
Training loss: 3.272413969039917
Validation loss: 2.5289429669739096

Epoch: 65| Step: 0
Training loss: 2.6058647632598877
Validation loss: 2.5275163599239883

Epoch: 5| Step: 1
Training loss: 2.885796546936035
Validation loss: 2.5292697157911075

Epoch: 5| Step: 2
Training loss: 2.0061633586883545
Validation loss: 2.5270746036242415

Epoch: 5| Step: 3
Training loss: 2.917447090148926
Validation loss: 2.5286967113453853

Epoch: 5| Step: 4
Training loss: 2.735617160797119
Validation loss: 2.5268044523013535

Epoch: 5| Step: 5
Training loss: 3.4073257446289062
Validation loss: 2.5272846606469925

Epoch: 5| Step: 6
Training loss: 2.7909493446350098
Validation loss: 2.5263501905625865

Epoch: 5| Step: 7
Training loss: 2.7335457801818848
Validation loss: 2.5247302696269047

Epoch: 5| Step: 8
Training loss: 2.821925401687622
Validation loss: 2.522111964482133

Epoch: 5| Step: 9
Training loss: 2.187471389770508
Validation loss: 2.5246261601806967

Epoch: 5| Step: 10
Training loss: 2.815329074859619
Validation loss: 2.5245880772990565

Epoch: 66| Step: 0
Training loss: 3.2296669483184814
Validation loss: 2.527377695165655

Epoch: 5| Step: 1
Training loss: 2.6933188438415527
Validation loss: 2.526385252193738

Epoch: 5| Step: 2
Training loss: 2.448580265045166
Validation loss: 2.5242082918843916

Epoch: 5| Step: 3
Training loss: 2.6986243724823
Validation loss: 2.5236904518578642

Epoch: 5| Step: 4
Training loss: 3.7124686241149902
Validation loss: 2.525685243709113

Epoch: 5| Step: 5
Training loss: 2.3429667949676514
Validation loss: 2.5258673544852965

Epoch: 5| Step: 6
Training loss: 2.9576315879821777
Validation loss: 2.5257366190674486

Epoch: 5| Step: 7
Training loss: 1.8394855260849
Validation loss: 2.5270320369351293

Epoch: 5| Step: 8
Training loss: 2.7493269443511963
Validation loss: 2.5369629783015095

Epoch: 5| Step: 9
Training loss: 2.4838247299194336
Validation loss: 2.5559220826754006

Epoch: 5| Step: 10
Training loss: 2.6488637924194336
Validation loss: 2.5933712502961517

Epoch: 67| Step: 0
Training loss: 3.2186343669891357
Validation loss: 2.660441252493089

Epoch: 5| Step: 1
Training loss: 1.8589227199554443
Validation loss: 2.6347389093009372

Epoch: 5| Step: 2
Training loss: 3.525378704071045
Validation loss: 2.6324085650905484

Epoch: 5| Step: 3
Training loss: 2.398221731185913
Validation loss: 2.6211540493913876

Epoch: 5| Step: 4
Training loss: 2.719755172729492
Validation loss: 2.6221712943046325

Epoch: 5| Step: 5
Training loss: 2.7820262908935547
Validation loss: 2.5753379662831626

Epoch: 5| Step: 6
Training loss: 2.6637120246887207
Validation loss: 2.548543046879512

Epoch: 5| Step: 7
Training loss: 2.5630438327789307
Validation loss: 2.53027509104821

Epoch: 5| Step: 8
Training loss: 2.532865524291992
Validation loss: 2.523598009540189

Epoch: 5| Step: 9
Training loss: 2.9122414588928223
Validation loss: 2.5169160032785065

Epoch: 5| Step: 10
Training loss: 2.757617235183716
Validation loss: 2.519645906263782

Epoch: 68| Step: 0
Training loss: 2.444239854812622
Validation loss: 2.5194217569084576

Epoch: 5| Step: 1
Training loss: 2.485234022140503
Validation loss: 2.521222347854286

Epoch: 5| Step: 2
Training loss: 2.4156370162963867
Validation loss: 2.5215101139519804

Epoch: 5| Step: 3
Training loss: 3.2889835834503174
Validation loss: 2.5153216136399137

Epoch: 5| Step: 4
Training loss: 2.972062587738037
Validation loss: 2.5122101127460437

Epoch: 5| Step: 5
Training loss: 2.2337231636047363
Validation loss: 2.5124126685562955

Epoch: 5| Step: 6
Training loss: 2.312920093536377
Validation loss: 2.5112611016919537

Epoch: 5| Step: 7
Training loss: 2.8834095001220703
Validation loss: 2.5112735507308797

Epoch: 5| Step: 8
Training loss: 2.875586986541748
Validation loss: 2.507089279031241

Epoch: 5| Step: 9
Training loss: 3.104215621948242
Validation loss: 2.5046425070813907

Epoch: 5| Step: 10
Training loss: 2.8365414142608643
Validation loss: 2.5089110046304683

Epoch: 69| Step: 0
Training loss: 2.85137677192688
Validation loss: 2.5078672286002868

Epoch: 5| Step: 1
Training loss: 2.8778507709503174
Validation loss: 2.512610443176762

Epoch: 5| Step: 2
Training loss: 3.1468241214752197
Validation loss: 2.514832245406284

Epoch: 5| Step: 3
Training loss: 2.9293594360351562
Validation loss: 2.521484185290593

Epoch: 5| Step: 4
Training loss: 2.803161859512329
Validation loss: 2.519043145641204

Epoch: 5| Step: 5
Training loss: 1.6694831848144531
Validation loss: 2.527533079988213

Epoch: 5| Step: 6
Training loss: 2.301997661590576
Validation loss: 2.518759794132684

Epoch: 5| Step: 7
Training loss: 2.7437949180603027
Validation loss: 2.51685594999662

Epoch: 5| Step: 8
Training loss: 2.6761345863342285
Validation loss: 2.5109387931003364

Epoch: 5| Step: 9
Training loss: 2.8401284217834473
Validation loss: 2.5115300750219696

Epoch: 5| Step: 10
Training loss: 3.0014185905456543
Validation loss: 2.5079246080049904

Epoch: 70| Step: 0
Training loss: 2.0839951038360596
Validation loss: 2.504644565684821

Epoch: 5| Step: 1
Training loss: 2.710599422454834
Validation loss: 2.5043972025635424

Epoch: 5| Step: 2
Training loss: 3.142493963241577
Validation loss: 2.503517855880081

Epoch: 5| Step: 3
Training loss: 2.9308853149414062
Validation loss: 2.502021679314234

Epoch: 5| Step: 4
Training loss: 3.27954363822937
Validation loss: 2.5040738941520773

Epoch: 5| Step: 5
Training loss: 2.4850921630859375
Validation loss: 2.501282997028802

Epoch: 5| Step: 6
Training loss: 2.2457315921783447
Validation loss: 2.504448116466563

Epoch: 5| Step: 7
Training loss: 2.6803975105285645
Validation loss: 2.503636990824053

Epoch: 5| Step: 8
Training loss: 2.7395026683807373
Validation loss: 2.5029856825387604

Epoch: 5| Step: 9
Training loss: 2.2679977416992188
Validation loss: 2.5022836474962133

Epoch: 5| Step: 10
Training loss: 3.2334096431732178
Validation loss: 2.4982742391606814

Epoch: 71| Step: 0
Training loss: 2.520468235015869
Validation loss: 2.4976977738001014

Epoch: 5| Step: 1
Training loss: 2.779815673828125
Validation loss: 2.4987738568295716

Epoch: 5| Step: 2
Training loss: 2.503261089324951
Validation loss: 2.4960354963938394

Epoch: 5| Step: 3
Training loss: 2.665435314178467
Validation loss: 2.4943642103543846

Epoch: 5| Step: 4
Training loss: 2.8854923248291016
Validation loss: 2.4951202843778875

Epoch: 5| Step: 5
Training loss: 3.3610692024230957
Validation loss: 2.4929040529394664

Epoch: 5| Step: 6
Training loss: 2.5770153999328613
Validation loss: 2.487273939194218

Epoch: 5| Step: 7
Training loss: 2.6330454349517822
Validation loss: 2.4894218316642185

Epoch: 5| Step: 8
Training loss: 2.4505302906036377
Validation loss: 2.489290234863117

Epoch: 5| Step: 9
Training loss: 2.6261403560638428
Validation loss: 2.487878725092898

Epoch: 5| Step: 10
Training loss: 2.6931817531585693
Validation loss: 2.484562857176668

Epoch: 72| Step: 0
Training loss: 2.834381580352783
Validation loss: 2.4916002288941415

Epoch: 5| Step: 1
Training loss: 2.838319778442383
Validation loss: 2.495307563453592

Epoch: 5| Step: 2
Training loss: 2.275993824005127
Validation loss: 2.5101658041759203

Epoch: 5| Step: 3
Training loss: 2.3897058963775635
Validation loss: 2.557707030286071

Epoch: 5| Step: 4
Training loss: 2.6309831142425537
Validation loss: 2.5568013780860492

Epoch: 5| Step: 5
Training loss: 2.9088330268859863
Validation loss: 2.548577154836347

Epoch: 5| Step: 6
Training loss: 2.3390164375305176
Validation loss: 2.499783908167193

Epoch: 5| Step: 7
Training loss: 2.468647003173828
Validation loss: 2.49004683186931

Epoch: 5| Step: 8
Training loss: 2.9821879863739014
Validation loss: 2.4843881078945693

Epoch: 5| Step: 9
Training loss: 3.5304808616638184
Validation loss: 2.4864856658443326

Epoch: 5| Step: 10
Training loss: 2.5450620651245117
Validation loss: 2.4891354909507175

Epoch: 73| Step: 0
Training loss: 2.487917423248291
Validation loss: 2.48736931175314

Epoch: 5| Step: 1
Training loss: 2.5497875213623047
Validation loss: 2.487917947512801

Epoch: 5| Step: 2
Training loss: 2.5478014945983887
Validation loss: 2.4899096488952637

Epoch: 5| Step: 3
Training loss: 2.4485607147216797
Validation loss: 2.490974999243213

Epoch: 5| Step: 4
Training loss: 2.985403537750244
Validation loss: 2.4890135231838433

Epoch: 5| Step: 5
Training loss: 2.0606865882873535
Validation loss: 2.4915163388816257

Epoch: 5| Step: 6
Training loss: 2.9937381744384766
Validation loss: 2.483240435200353

Epoch: 5| Step: 7
Training loss: 3.075073480606079
Validation loss: 2.4880235015705066

Epoch: 5| Step: 8
Training loss: 2.7760069370269775
Validation loss: 2.4814306869301745

Epoch: 5| Step: 9
Training loss: 2.909679412841797
Validation loss: 2.4832164754149733

Epoch: 5| Step: 10
Training loss: 2.793478012084961
Validation loss: 2.4836664251101914

Epoch: 74| Step: 0
Training loss: 2.117628812789917
Validation loss: 2.483478384633218

Epoch: 5| Step: 1
Training loss: 2.8986918926239014
Validation loss: 2.4816431794115292

Epoch: 5| Step: 2
Training loss: 2.696136951446533
Validation loss: 2.4839634523596814

Epoch: 5| Step: 3
Training loss: 2.7503433227539062
Validation loss: 2.487745269652336

Epoch: 5| Step: 4
Training loss: 2.9333462715148926
Validation loss: 2.4837325055112123

Epoch: 5| Step: 5
Training loss: 2.5905094146728516
Validation loss: 2.4888881098839546

Epoch: 5| Step: 6
Training loss: 2.8334574699401855
Validation loss: 2.49061474492473

Epoch: 5| Step: 7
Training loss: 2.814350128173828
Validation loss: 2.498711439871019

Epoch: 5| Step: 8
Training loss: 2.5909695625305176
Validation loss: 2.500931898752848

Epoch: 5| Step: 9
Training loss: 2.846780776977539
Validation loss: 2.5054785359290337

Epoch: 5| Step: 10
Training loss: 2.5287528038024902
Validation loss: 2.506422996520996

Epoch: 75| Step: 0
Training loss: 2.767531394958496
Validation loss: 2.489155851384645

Epoch: 5| Step: 1
Training loss: 1.5961765050888062
Validation loss: 2.479494945977324

Epoch: 5| Step: 2
Training loss: 2.7779808044433594
Validation loss: 2.4711558485543854

Epoch: 5| Step: 3
Training loss: 2.563382625579834
Validation loss: 2.471061142542029

Epoch: 5| Step: 4
Training loss: 2.9669322967529297
Validation loss: 2.4726529352126585

Epoch: 5| Step: 5
Training loss: 2.2091758251190186
Validation loss: 2.472851704525691

Epoch: 5| Step: 6
Training loss: 2.678269624710083
Validation loss: 2.4755041035272742

Epoch: 5| Step: 7
Training loss: 2.683176040649414
Validation loss: 2.475308092691565

Epoch: 5| Step: 8
Training loss: 3.061089277267456
Validation loss: 2.4752988687125583

Epoch: 5| Step: 9
Training loss: 2.776669979095459
Validation loss: 2.476529134217129

Epoch: 5| Step: 10
Training loss: 3.613140821456909
Validation loss: 2.4730640611340924

Epoch: 76| Step: 0
Training loss: 2.0981781482696533
Validation loss: 2.468774882696008

Epoch: 5| Step: 1
Training loss: 2.3776445388793945
Validation loss: 2.466243436259608

Epoch: 5| Step: 2
Training loss: 3.0815844535827637
Validation loss: 2.4680359055919032

Epoch: 5| Step: 3
Training loss: 3.0696051120758057
Validation loss: 2.4678313911602063

Epoch: 5| Step: 4
Training loss: 3.084613084793091
Validation loss: 2.4666655601993686

Epoch: 5| Step: 5
Training loss: 2.3053441047668457
Validation loss: 2.4649670918782554

Epoch: 5| Step: 6
Training loss: 2.6708550453186035
Validation loss: 2.4656879645521923

Epoch: 5| Step: 7
Training loss: 2.853177309036255
Validation loss: 2.4615109248827864

Epoch: 5| Step: 8
Training loss: 2.4566245079040527
Validation loss: 2.4654756463984007

Epoch: 5| Step: 9
Training loss: 1.8941667079925537
Validation loss: 2.4694402064046552

Epoch: 5| Step: 10
Training loss: 3.7666447162628174
Validation loss: 2.470703281382079

Epoch: 77| Step: 0
Training loss: 2.7473304271698
Validation loss: 2.4758070950867026

Epoch: 5| Step: 1
Training loss: 2.360802173614502
Validation loss: 2.4773343045224427

Epoch: 5| Step: 2
Training loss: 3.1370856761932373
Validation loss: 2.49143624049361

Epoch: 5| Step: 3
Training loss: 2.898951292037964
Validation loss: 2.490949107754615

Epoch: 5| Step: 4
Training loss: 3.0887444019317627
Validation loss: 2.465839268058859

Epoch: 5| Step: 5
Training loss: 2.6019136905670166
Validation loss: 2.460424946200463

Epoch: 5| Step: 6
Training loss: 2.162169933319092
Validation loss: 2.4554172049286547

Epoch: 5| Step: 7
Training loss: 2.5633978843688965
Validation loss: 2.4532970305412047

Epoch: 5| Step: 8
Training loss: 2.407972812652588
Validation loss: 2.4561234904873754

Epoch: 5| Step: 9
Training loss: 2.4809060096740723
Validation loss: 2.4525230776879097

Epoch: 5| Step: 10
Training loss: 3.018284559249878
Validation loss: 2.450576384862264

Epoch: 78| Step: 0
Training loss: 2.0226502418518066
Validation loss: 2.4466785359126266

Epoch: 5| Step: 1
Training loss: 2.232780933380127
Validation loss: 2.446667343057612

Epoch: 5| Step: 2
Training loss: 2.421029567718506
Validation loss: 2.4453877377253708

Epoch: 5| Step: 3
Training loss: 2.2278831005096436
Validation loss: 2.446338056236185

Epoch: 5| Step: 4
Training loss: 2.5077052116394043
Validation loss: 2.444141185411843

Epoch: 5| Step: 5
Training loss: 2.421391725540161
Validation loss: 2.4437214046396236

Epoch: 5| Step: 6
Training loss: 3.0353901386260986
Validation loss: 2.4457282122745307

Epoch: 5| Step: 7
Training loss: 3.197172164916992
Validation loss: 2.446077912084518

Epoch: 5| Step: 8
Training loss: 3.3241400718688965
Validation loss: 2.459942538251159

Epoch: 5| Step: 9
Training loss: 3.1156105995178223
Validation loss: 2.457696376308318

Epoch: 5| Step: 10
Training loss: 2.775907516479492
Validation loss: 2.467059063655074

Epoch: 79| Step: 0
Training loss: 2.6727259159088135
Validation loss: 2.4605074595379572

Epoch: 5| Step: 1
Training loss: 3.190852642059326
Validation loss: 2.456615688980267

Epoch: 5| Step: 2
Training loss: 2.3752200603485107
Validation loss: 2.4398276882786907

Epoch: 5| Step: 3
Training loss: 2.97051739692688
Validation loss: 2.4349132686532955

Epoch: 5| Step: 4
Training loss: 2.811168909072876
Validation loss: 2.4348558379757788

Epoch: 5| Step: 5
Training loss: 1.938113808631897
Validation loss: 2.4292702879956973

Epoch: 5| Step: 6
Training loss: 2.5929954051971436
Validation loss: 2.425606584036222

Epoch: 5| Step: 7
Training loss: 1.7877819538116455
Validation loss: 2.4260987286926596

Epoch: 5| Step: 8
Training loss: 3.0625834465026855
Validation loss: 2.426988576048164

Epoch: 5| Step: 9
Training loss: 2.494440793991089
Validation loss: 2.4279382254487727

Epoch: 5| Step: 10
Training loss: 3.3741228580474854
Validation loss: 2.421470429307671

Epoch: 80| Step: 0
Training loss: 2.717911958694458
Validation loss: 2.4239884884126726

Epoch: 5| Step: 1
Training loss: 2.494492769241333
Validation loss: 2.425989894456761

Epoch: 5| Step: 2
Training loss: 2.5726630687713623
Validation loss: 2.4307743669838033

Epoch: 5| Step: 3
Training loss: 3.168785572052002
Validation loss: 2.426708782872846

Epoch: 5| Step: 4
Training loss: 2.5165047645568848
Validation loss: 2.429081127207766

Epoch: 5| Step: 5
Training loss: 2.8658790588378906
Validation loss: 2.422223162907426

Epoch: 5| Step: 6
Training loss: 2.2710986137390137
Validation loss: 2.417572400903189

Epoch: 5| Step: 7
Training loss: 3.007746696472168
Validation loss: 2.4174962556490334

Epoch: 5| Step: 8
Training loss: 2.8782098293304443
Validation loss: 2.4230416692713255

Epoch: 5| Step: 9
Training loss: 1.7937841415405273
Validation loss: 2.432827659832534

Epoch: 5| Step: 10
Training loss: 2.9376020431518555
Validation loss: 2.451149904599754

Epoch: 81| Step: 0
Training loss: 2.2254481315612793
Validation loss: 2.481697387592767

Epoch: 5| Step: 1
Training loss: 2.591022491455078
Validation loss: 2.5026414163651003

Epoch: 5| Step: 2
Training loss: 3.0246264934539795
Validation loss: 2.4940257687722482

Epoch: 5| Step: 3
Training loss: 3.069075345993042
Validation loss: 2.472744575110815

Epoch: 5| Step: 4
Training loss: 3.0379750728607178
Validation loss: 2.460527281607351

Epoch: 5| Step: 5
Training loss: 2.3245980739593506
Validation loss: 2.4417324655799457

Epoch: 5| Step: 6
Training loss: 2.5037686824798584
Validation loss: 2.43327417168566

Epoch: 5| Step: 7
Training loss: 2.3119122982025146
Validation loss: 2.4218305797987085

Epoch: 5| Step: 8
Training loss: 2.1377015113830566
Validation loss: 2.4157031812975482

Epoch: 5| Step: 9
Training loss: 3.0390782356262207
Validation loss: 2.4202275558184554

Epoch: 5| Step: 10
Training loss: 3.014010429382324
Validation loss: 2.411796905661142

Epoch: 82| Step: 0
Training loss: 2.6610026359558105
Validation loss: 2.4182039614646667

Epoch: 5| Step: 1
Training loss: 2.774294376373291
Validation loss: 2.414800118374568

Epoch: 5| Step: 2
Training loss: 2.7785263061523438
Validation loss: 2.416994156376008

Epoch: 5| Step: 3
Training loss: 2.260007858276367
Validation loss: 2.4243481928302395

Epoch: 5| Step: 4
Training loss: 2.5537610054016113
Validation loss: 2.4193065089564167

Epoch: 5| Step: 5
Training loss: 2.8149452209472656
Validation loss: 2.4168844325568086

Epoch: 5| Step: 6
Training loss: 2.714151382446289
Validation loss: 2.4144992366913827

Epoch: 5| Step: 7
Training loss: 2.483081102371216
Validation loss: 2.4113285387715986

Epoch: 5| Step: 8
Training loss: 2.934269428253174
Validation loss: 2.4086511519647416

Epoch: 5| Step: 9
Training loss: 2.6477503776550293
Validation loss: 2.4165134763204925

Epoch: 5| Step: 10
Training loss: 2.5355870723724365
Validation loss: 2.4247200437771377

Epoch: 83| Step: 0
Training loss: 2.399919033050537
Validation loss: 2.4220406278487174

Epoch: 5| Step: 1
Training loss: 2.0853402614593506
Validation loss: 2.424002270544729

Epoch: 5| Step: 2
Training loss: 2.7485928535461426
Validation loss: 2.423610141200404

Epoch: 5| Step: 3
Training loss: 2.5882761478424072
Validation loss: 2.4259996798730667

Epoch: 5| Step: 4
Training loss: 2.3332314491271973
Validation loss: 2.424653655739241

Epoch: 5| Step: 5
Training loss: 3.3946533203125
Validation loss: 2.421679094273557

Epoch: 5| Step: 6
Training loss: 2.866140604019165
Validation loss: 2.4190386854192263

Epoch: 5| Step: 7
Training loss: 2.662477731704712
Validation loss: 2.4183226580260904

Epoch: 5| Step: 8
Training loss: 2.724726676940918
Validation loss: 2.417123345918553

Epoch: 5| Step: 9
Training loss: 2.817596673965454
Validation loss: 2.4189725755363383

Epoch: 5| Step: 10
Training loss: 2.565037250518799
Validation loss: 2.425225552692208

Epoch: 84| Step: 0
Training loss: 2.7682747840881348
Validation loss: 2.434975101101783

Epoch: 5| Step: 1
Training loss: 2.5771617889404297
Validation loss: 2.43959839113297

Epoch: 5| Step: 2
Training loss: 2.549663543701172
Validation loss: 2.43611829767945

Epoch: 5| Step: 3
Training loss: 1.7538411617279053
Validation loss: 2.432798795802619

Epoch: 5| Step: 4
Training loss: 3.369777202606201
Validation loss: 2.4419871966044107

Epoch: 5| Step: 5
Training loss: 2.3323779106140137
Validation loss: 2.4386004914519606

Epoch: 5| Step: 6
Training loss: 2.12642240524292
Validation loss: 2.4387283812287035

Epoch: 5| Step: 7
Training loss: 2.276211977005005
Validation loss: 2.4285619489608274

Epoch: 5| Step: 8
Training loss: 3.4778366088867188
Validation loss: 2.4243660434599845

Epoch: 5| Step: 9
Training loss: 3.0691030025482178
Validation loss: 2.41709610467316

Epoch: 5| Step: 10
Training loss: 2.8174357414245605
Validation loss: 2.409257834957492

Epoch: 85| Step: 0
Training loss: 2.9333600997924805
Validation loss: 2.408401676403579

Epoch: 5| Step: 1
Training loss: 3.104130506515503
Validation loss: 2.408019019711402

Epoch: 5| Step: 2
Training loss: 2.864482879638672
Validation loss: 2.402858729003578

Epoch: 5| Step: 3
Training loss: 2.7373039722442627
Validation loss: 2.401385453439528

Epoch: 5| Step: 4
Training loss: 2.188004970550537
Validation loss: 2.4024036366452455

Epoch: 5| Step: 5
Training loss: 3.0648932456970215
Validation loss: 2.396792442567887

Epoch: 5| Step: 6
Training loss: 2.680081605911255
Validation loss: 2.402896081247637

Epoch: 5| Step: 7
Training loss: 2.3766961097717285
Validation loss: 2.408368638766709

Epoch: 5| Step: 8
Training loss: 2.021475076675415
Validation loss: 2.410389736134519

Epoch: 5| Step: 9
Training loss: 2.094388484954834
Validation loss: 2.4165054854526313

Epoch: 5| Step: 10
Training loss: 3.103229284286499
Validation loss: 2.43129224418312

Epoch: 86| Step: 0
Training loss: 2.9852728843688965
Validation loss: 2.4392939690620667

Epoch: 5| Step: 1
Training loss: 2.4049134254455566
Validation loss: 2.456049208999962

Epoch: 5| Step: 2
Training loss: 2.989147901535034
Validation loss: 2.466347546987636

Epoch: 5| Step: 3
Training loss: 2.4772346019744873
Validation loss: 2.468449402880925

Epoch: 5| Step: 4
Training loss: 1.853571891784668
Validation loss: 2.4400183616145963

Epoch: 5| Step: 5
Training loss: 2.6844019889831543
Validation loss: 2.401786142779935

Epoch: 5| Step: 6
Training loss: 2.1901464462280273
Validation loss: 2.389526090314311

Epoch: 5| Step: 7
Training loss: 3.6190319061279297
Validation loss: 2.3835413379053914

Epoch: 5| Step: 8
Training loss: 2.4800713062286377
Validation loss: 2.387057827365014

Epoch: 5| Step: 9
Training loss: 2.320513963699341
Validation loss: 2.3875884009945776

Epoch: 5| Step: 10
Training loss: 3.0556845664978027
Validation loss: 2.3948433271018406

Epoch: 87| Step: 0
Training loss: 2.32728910446167
Validation loss: 2.4007213474601827

Epoch: 5| Step: 1
Training loss: 2.920097827911377
Validation loss: 2.4116616915631037

Epoch: 5| Step: 2
Training loss: 2.054051160812378
Validation loss: 2.4156710870804323

Epoch: 5| Step: 3
Training loss: 2.604879856109619
Validation loss: 2.406521504925143

Epoch: 5| Step: 4
Training loss: 2.2942893505096436
Validation loss: 2.3997494354042956

Epoch: 5| Step: 5
Training loss: 2.9833855628967285
Validation loss: 2.389451478117256

Epoch: 5| Step: 6
Training loss: 2.7857844829559326
Validation loss: 2.3877979452892015

Epoch: 5| Step: 7
Training loss: 2.7625527381896973
Validation loss: 2.3801597113250406

Epoch: 5| Step: 8
Training loss: 2.083393096923828
Validation loss: 2.389652375252016

Epoch: 5| Step: 9
Training loss: 3.151672601699829
Validation loss: 2.4419211187670307

Epoch: 5| Step: 10
Training loss: 3.422919511795044
Validation loss: 2.4105926072725685

Epoch: 88| Step: 0
Training loss: 2.127190351486206
Validation loss: 2.3959478921787714

Epoch: 5| Step: 1
Training loss: 2.922611713409424
Validation loss: 2.3817884076026177

Epoch: 5| Step: 2
Training loss: 2.362269639968872
Validation loss: 2.376927477057262

Epoch: 5| Step: 3
Training loss: 2.220367193222046
Validation loss: 2.3770470478201426

Epoch: 5| Step: 4
Training loss: 2.257986545562744
Validation loss: 2.3769613632591824

Epoch: 5| Step: 5
Training loss: 3.3118317127227783
Validation loss: 2.378251867909585

Epoch: 5| Step: 6
Training loss: 2.162257194519043
Validation loss: 2.3798744088859967

Epoch: 5| Step: 7
Training loss: 2.699690103530884
Validation loss: 2.3784382753474738

Epoch: 5| Step: 8
Training loss: 3.003934144973755
Validation loss: 2.377610621913787

Epoch: 5| Step: 9
Training loss: 2.7424867153167725
Validation loss: 2.3785154896397747

Epoch: 5| Step: 10
Training loss: 3.291435480117798
Validation loss: 2.3766282040585756

Epoch: 89| Step: 0
Training loss: 3.2650203704833984
Validation loss: 2.3775617307232273

Epoch: 5| Step: 1
Training loss: 3.143515110015869
Validation loss: 2.3777248654314267

Epoch: 5| Step: 2
Training loss: 2.422913074493408
Validation loss: 2.3803439909412014

Epoch: 5| Step: 3
Training loss: 2.76623797416687
Validation loss: 2.385022072381871

Epoch: 5| Step: 4
Training loss: 2.860691785812378
Validation loss: 2.392563860903504

Epoch: 5| Step: 5
Training loss: 2.280478000640869
Validation loss: 2.397011895333567

Epoch: 5| Step: 6
Training loss: 2.4467978477478027
Validation loss: 2.3917981270820863

Epoch: 5| Step: 7
Training loss: 2.487205982208252
Validation loss: 2.407866111365698

Epoch: 5| Step: 8
Training loss: 2.6687281131744385
Validation loss: 2.409524676620319

Epoch: 5| Step: 9
Training loss: 2.6737704277038574
Validation loss: 2.4054046907732562

Epoch: 5| Step: 10
Training loss: 1.7879829406738281
Validation loss: 2.400103385730456

Epoch: 90| Step: 0
Training loss: 2.4523367881774902
Validation loss: 2.3961644480305333

Epoch: 5| Step: 1
Training loss: 2.7432854175567627
Validation loss: 2.3781191969430573

Epoch: 5| Step: 2
Training loss: 2.3190670013427734
Validation loss: 2.374528154250114

Epoch: 5| Step: 3
Training loss: 1.9093334674835205
Validation loss: 2.3786476042962845

Epoch: 5| Step: 4
Training loss: 2.9612414836883545
Validation loss: 2.3842452777329313

Epoch: 5| Step: 5
Training loss: 2.848245143890381
Validation loss: 2.387716136952882

Epoch: 5| Step: 6
Training loss: 2.9578781127929688
Validation loss: 2.380503403243198

Epoch: 5| Step: 7
Training loss: 2.253408670425415
Validation loss: 2.3861328401873187

Epoch: 5| Step: 8
Training loss: 2.8212966918945312
Validation loss: 2.3840056439881683

Epoch: 5| Step: 9
Training loss: 2.8171589374542236
Validation loss: 2.3835097256527153

Epoch: 5| Step: 10
Training loss: 2.808687210083008
Validation loss: 2.3845382249483498

Epoch: 91| Step: 0
Training loss: 2.0212273597717285
Validation loss: 2.372544550126599

Epoch: 5| Step: 1
Training loss: 2.447571277618408
Validation loss: 2.370463099530948

Epoch: 5| Step: 2
Training loss: 3.4864068031311035
Validation loss: 2.371099765582751

Epoch: 5| Step: 3
Training loss: 2.7498536109924316
Validation loss: 2.371129810169179

Epoch: 5| Step: 4
Training loss: 2.2766451835632324
Validation loss: 2.3770151420306136

Epoch: 5| Step: 5
Training loss: 2.683623790740967
Validation loss: 2.377033390024657

Epoch: 5| Step: 6
Training loss: 2.400846481323242
Validation loss: 2.371073812566778

Epoch: 5| Step: 7
Training loss: 2.810847759246826
Validation loss: 2.3864685361103346

Epoch: 5| Step: 8
Training loss: 3.053179979324341
Validation loss: 2.3910760084788003

Epoch: 5| Step: 9
Training loss: 2.7823281288146973
Validation loss: 2.39080366780681

Epoch: 5| Step: 10
Training loss: 2.0300393104553223
Validation loss: 2.384786252052553

Epoch: 92| Step: 0
Training loss: 2.740149974822998
Validation loss: 2.3742179793696248

Epoch: 5| Step: 1
Training loss: 2.2290728092193604
Validation loss: 2.3687498287488054

Epoch: 5| Step: 2
Training loss: 2.7723941802978516
Validation loss: 2.362783762716478

Epoch: 5| Step: 3
Training loss: 2.941640853881836
Validation loss: 2.358272447380968

Epoch: 5| Step: 4
Training loss: 2.4410836696624756
Validation loss: 2.3537561457644225

Epoch: 5| Step: 5
Training loss: 3.490063190460205
Validation loss: 2.3561697416408087

Epoch: 5| Step: 6
Training loss: 2.335454225540161
Validation loss: 2.352903607071087

Epoch: 5| Step: 7
Training loss: 2.2388505935668945
Validation loss: 2.3561025742561585

Epoch: 5| Step: 8
Training loss: 2.959290027618408
Validation loss: 2.353652690046577

Epoch: 5| Step: 9
Training loss: 2.2538700103759766
Validation loss: 2.352286393924426

Epoch: 5| Step: 10
Training loss: 2.3637707233428955
Validation loss: 2.3584314610368464

Epoch: 93| Step: 0
Training loss: 2.53000807762146
Validation loss: 2.3595104858439457

Epoch: 5| Step: 1
Training loss: 3.1054694652557373
Validation loss: 2.3657124196329424

Epoch: 5| Step: 2
Training loss: 3.0325818061828613
Validation loss: 2.36544434742261

Epoch: 5| Step: 3
Training loss: 2.917417049407959
Validation loss: 2.3698170467089583

Epoch: 5| Step: 4
Training loss: 2.3609368801116943
Validation loss: 2.3679484898044216

Epoch: 5| Step: 5
Training loss: 2.010956287384033
Validation loss: 2.381141967670892

Epoch: 5| Step: 6
Training loss: 2.2219231128692627
Validation loss: 2.373392112793461

Epoch: 5| Step: 7
Training loss: 2.228325128555298
Validation loss: 2.3661161750875492

Epoch: 5| Step: 8
Training loss: 2.977985382080078
Validation loss: 2.3701556703095794

Epoch: 5| Step: 9
Training loss: 2.575678825378418
Validation loss: 2.3585652843598397

Epoch: 5| Step: 10
Training loss: 2.6888504028320312
Validation loss: 2.358918377148208

Epoch: 94| Step: 0
Training loss: 2.98065447807312
Validation loss: 2.3557081273806992

Epoch: 5| Step: 1
Training loss: 2.241692543029785
Validation loss: 2.3481446337956253

Epoch: 5| Step: 2
Training loss: 2.633254289627075
Validation loss: 2.355111880968976

Epoch: 5| Step: 3
Training loss: 2.2377867698669434
Validation loss: 2.3541387896383963

Epoch: 5| Step: 4
Training loss: 2.749521255493164
Validation loss: 2.360793759745936

Epoch: 5| Step: 5
Training loss: 2.349125385284424
Validation loss: 2.358645116129229

Epoch: 5| Step: 6
Training loss: 2.816201686859131
Validation loss: 2.366330995354601

Epoch: 5| Step: 7
Training loss: 3.234701633453369
Validation loss: 2.362530034075501

Epoch: 5| Step: 8
Training loss: 2.3841781616210938
Validation loss: 2.349324321234098

Epoch: 5| Step: 9
Training loss: 2.2137954235076904
Validation loss: 2.347519164444298

Epoch: 5| Step: 10
Training loss: 2.7995028495788574
Validation loss: 2.346411079488775

Epoch: 95| Step: 0
Training loss: 2.797773599624634
Validation loss: 2.3441981397649294

Epoch: 5| Step: 1
Training loss: 2.8255109786987305
Validation loss: 2.338931886098718

Epoch: 5| Step: 2
Training loss: 2.8089873790740967
Validation loss: 2.337341990522159

Epoch: 5| Step: 3
Training loss: 2.4438247680664062
Validation loss: 2.3314865532741753

Epoch: 5| Step: 4
Training loss: 2.0385308265686035
Validation loss: 2.335216958035705

Epoch: 5| Step: 5
Training loss: 2.9027419090270996
Validation loss: 2.335202096610941

Epoch: 5| Step: 6
Training loss: 2.884824752807617
Validation loss: 2.3326418451083604

Epoch: 5| Step: 7
Training loss: 2.4686012268066406
Validation loss: 2.3360190981177875

Epoch: 5| Step: 8
Training loss: 1.6798784732818604
Validation loss: 2.3332704985013573

Epoch: 5| Step: 9
Training loss: 2.5711047649383545
Validation loss: 2.3441443135661464

Epoch: 5| Step: 10
Training loss: 3.381570816040039
Validation loss: 2.367454541626797

Epoch: 96| Step: 0
Training loss: 1.9351472854614258
Validation loss: 2.3900411385361866

Epoch: 5| Step: 1
Training loss: 3.3416569232940674
Validation loss: 2.388173846788304

Epoch: 5| Step: 2
Training loss: 3.221315383911133
Validation loss: 2.3611578582435526

Epoch: 5| Step: 3
Training loss: 2.083712339401245
Validation loss: 2.3434083013124365

Epoch: 5| Step: 4
Training loss: 2.5561623573303223
Validation loss: 2.3378072015700804

Epoch: 5| Step: 5
Training loss: 2.70501446723938
Validation loss: 2.33030286142903

Epoch: 5| Step: 6
Training loss: 2.5494437217712402
Validation loss: 2.3307475300245386

Epoch: 5| Step: 7
Training loss: 2.504697799682617
Validation loss: 2.3313517647404827

Epoch: 5| Step: 8
Training loss: 2.7326512336730957
Validation loss: 2.3302768340674778

Epoch: 5| Step: 9
Training loss: 2.2847046852111816
Validation loss: 2.3350766397291616

Epoch: 5| Step: 10
Training loss: 2.729515552520752
Validation loss: 2.331358999334356

Epoch: 97| Step: 0
Training loss: 2.6020116806030273
Validation loss: 2.3286273864007767

Epoch: 5| Step: 1
Training loss: 2.3637657165527344
Validation loss: 2.3311000126664356

Epoch: 5| Step: 2
Training loss: 2.8903868198394775
Validation loss: 2.3240109541082896

Epoch: 5| Step: 3
Training loss: 2.5108816623687744
Validation loss: 2.330210506275136

Epoch: 5| Step: 4
Training loss: 2.517580509185791
Validation loss: 2.332516734318067

Epoch: 5| Step: 5
Training loss: 2.4236416816711426
Validation loss: 2.3386651392905944

Epoch: 5| Step: 6
Training loss: 2.4206786155700684
Validation loss: 2.3407597618718303

Epoch: 5| Step: 7
Training loss: 3.209566831588745
Validation loss: 2.3518937249337473

Epoch: 5| Step: 8
Training loss: 2.6822030544281006
Validation loss: 2.3587104607653875

Epoch: 5| Step: 9
Training loss: 1.8395442962646484
Validation loss: 2.3744611765748713

Epoch: 5| Step: 10
Training loss: 3.204078197479248
Validation loss: 2.379837989807129

Epoch: 98| Step: 0
Training loss: 2.044602155685425
Validation loss: 2.366428716208345

Epoch: 5| Step: 1
Training loss: 2.6741790771484375
Validation loss: 2.366382911641111

Epoch: 5| Step: 2
Training loss: 2.4131124019622803
Validation loss: 2.3550562986763577

Epoch: 5| Step: 3
Training loss: 2.602381467819214
Validation loss: 2.334200379669025

Epoch: 5| Step: 4
Training loss: 2.94748592376709
Validation loss: 2.3300902510202057

Epoch: 5| Step: 5
Training loss: 2.6022682189941406
Validation loss: 2.327429376622682

Epoch: 5| Step: 6
Training loss: 2.518033504486084
Validation loss: 2.350841760635376

Epoch: 5| Step: 7
Training loss: 2.6078059673309326
Validation loss: 2.3315347920181932

Epoch: 5| Step: 8
Training loss: 2.567484140396118
Validation loss: 2.334749437147571

Epoch: 5| Step: 9
Training loss: 2.945206880569458
Validation loss: 2.3250516332605833

Epoch: 5| Step: 10
Training loss: 2.6966817378997803
Validation loss: 2.3241358726255354

Epoch: 99| Step: 0
Training loss: 2.2427353858947754
Validation loss: 2.3358009246087845

Epoch: 5| Step: 1
Training loss: 2.5472664833068848
Validation loss: 2.3415194352467856

Epoch: 5| Step: 2
Training loss: 1.7381694316864014
Validation loss: 2.3704802656686432

Epoch: 5| Step: 3
Training loss: 3.617211103439331
Validation loss: 2.4190307227514123

Epoch: 5| Step: 4
Training loss: 2.5285212993621826
Validation loss: 2.394793628364481

Epoch: 5| Step: 5
Training loss: 2.366954803466797
Validation loss: 2.3477633627512122

Epoch: 5| Step: 6
Training loss: 2.9142565727233887
Validation loss: 2.3324475493482364

Epoch: 5| Step: 7
Training loss: 2.175973892211914
Validation loss: 2.316886535254858

Epoch: 5| Step: 8
Training loss: 2.648700475692749
Validation loss: 2.3129205165370816

Epoch: 5| Step: 9
Training loss: 3.02644944190979
Validation loss: 2.3134639288789485

Epoch: 5| Step: 10
Training loss: 2.7091073989868164
Validation loss: 2.3152986623907603

Epoch: 100| Step: 0
Training loss: 2.3443119525909424
Validation loss: 2.3289103033722087

Epoch: 5| Step: 1
Training loss: 3.061491012573242
Validation loss: 2.3339258727206977

Epoch: 5| Step: 2
Training loss: 2.0737969875335693
Validation loss: 2.330288683214495

Epoch: 5| Step: 3
Training loss: 3.0257694721221924
Validation loss: 2.3141541404108845

Epoch: 5| Step: 4
Training loss: 2.705414295196533
Validation loss: 2.310297176402102

Epoch: 5| Step: 5
Training loss: 2.3826775550842285
Validation loss: 2.3122391367471344

Epoch: 5| Step: 6
Training loss: 2.755074977874756
Validation loss: 2.310951325201219

Epoch: 5| Step: 7
Training loss: 2.582157850265503
Validation loss: 2.315037194118705

Epoch: 5| Step: 8
Training loss: 2.0051674842834473
Validation loss: 2.3335987649938112

Epoch: 5| Step: 9
Training loss: 2.7663187980651855
Validation loss: 2.373279594605969

Epoch: 5| Step: 10
Training loss: 2.880181074142456
Validation loss: 2.4400419137811147

Epoch: 101| Step: 0
Training loss: 2.204054594039917
Validation loss: 2.4441636223946848

Epoch: 5| Step: 1
Training loss: 2.8179657459259033
Validation loss: 2.4524015713763494

Epoch: 5| Step: 2
Training loss: 2.977461576461792
Validation loss: 2.453624943251251

Epoch: 5| Step: 3
Training loss: 3.145770311355591
Validation loss: 2.3894184532985894

Epoch: 5| Step: 4
Training loss: 2.661794900894165
Validation loss: 2.348627944146433

Epoch: 5| Step: 5
Training loss: 2.1328647136688232
Validation loss: 2.32266136651398

Epoch: 5| Step: 6
Training loss: 2.752197027206421
Validation loss: 2.31161452621542

Epoch: 5| Step: 7
Training loss: 1.9772411584854126
Validation loss: 2.310958534158686

Epoch: 5| Step: 8
Training loss: 2.6093075275421143
Validation loss: 2.3165524326344973

Epoch: 5| Step: 9
Training loss: 2.296905755996704
Validation loss: 2.3164497472906627

Epoch: 5| Step: 10
Training loss: 2.9846835136413574
Validation loss: 2.3252745828320904

Epoch: 102| Step: 0
Training loss: 3.0086708068847656
Validation loss: 2.3381677366072133

Epoch: 5| Step: 1
Training loss: 2.383352041244507
Validation loss: 2.335010964383361

Epoch: 5| Step: 2
Training loss: 3.101714611053467
Validation loss: 2.3231493247452604

Epoch: 5| Step: 3
Training loss: 2.737983226776123
Validation loss: 2.3148082558826735

Epoch: 5| Step: 4
Training loss: 2.326414108276367
Validation loss: 2.3108259939378306

Epoch: 5| Step: 5
Training loss: 2.3529934883117676
Validation loss: 2.3070870317438597

Epoch: 5| Step: 6
Training loss: 2.2509701251983643
Validation loss: 2.3136436926421298

Epoch: 5| Step: 7
Training loss: 2.86198353767395
Validation loss: 2.3164901438579766

Epoch: 5| Step: 8
Training loss: 2.423786163330078
Validation loss: 2.323999315179804

Epoch: 5| Step: 9
Training loss: 2.612440824508667
Validation loss: 2.344269293610768

Epoch: 5| Step: 10
Training loss: 2.238215208053589
Validation loss: 2.370440095983526

Epoch: 103| Step: 0
Training loss: 2.5388472080230713
Validation loss: 2.3849384553970827

Epoch: 5| Step: 1
Training loss: 2.402686595916748
Validation loss: 2.367995341618856

Epoch: 5| Step: 2
Training loss: 2.7848689556121826
Validation loss: 2.3209966818491616

Epoch: 5| Step: 3
Training loss: 3.1640148162841797
Validation loss: 2.289531730836438

Epoch: 5| Step: 4
Training loss: 1.661389708518982
Validation loss: 2.2869564064087404

Epoch: 5| Step: 5
Training loss: 2.887638568878174
Validation loss: 2.28372601796222

Epoch: 5| Step: 6
Training loss: 2.310150384902954
Validation loss: 2.288177890162314

Epoch: 5| Step: 7
Training loss: 3.216026782989502
Validation loss: 2.2910073662316925

Epoch: 5| Step: 8
Training loss: 2.656176805496216
Validation loss: 2.2927105734425206

Epoch: 5| Step: 9
Training loss: 1.8887007236480713
Validation loss: 2.2939497142709713

Epoch: 5| Step: 10
Training loss: 2.7753851413726807
Validation loss: 2.2898212786643737

Epoch: 104| Step: 0
Training loss: 3.032336473464966
Validation loss: 2.283372171463505

Epoch: 5| Step: 1
Training loss: 2.117219924926758
Validation loss: 2.27810315931997

Epoch: 5| Step: 2
Training loss: 2.667630434036255
Validation loss: 2.2761277255191597

Epoch: 5| Step: 3
Training loss: 2.523555278778076
Validation loss: 2.2859856108183503

Epoch: 5| Step: 4
Training loss: 2.4864513874053955
Validation loss: 2.3109091097308743

Epoch: 5| Step: 5
Training loss: 1.9680025577545166
Validation loss: 2.332536994770009

Epoch: 5| Step: 6
Training loss: 2.8612382411956787
Validation loss: 2.350610722777664

Epoch: 5| Step: 7
Training loss: 2.7798240184783936
Validation loss: 2.3393634019359464

Epoch: 5| Step: 8
Training loss: 2.4872138500213623
Validation loss: 2.297553413657732

Epoch: 5| Step: 9
Training loss: 3.0359315872192383
Validation loss: 2.2768381680211713

Epoch: 5| Step: 10
Training loss: 2.3946304321289062
Validation loss: 2.268536542051582

Epoch: 105| Step: 0
Training loss: 2.5007171630859375
Validation loss: 2.271997400509414

Epoch: 5| Step: 1
Training loss: 2.47196626663208
Validation loss: 2.2708623306725615

Epoch: 5| Step: 2
Training loss: 3.646836042404175
Validation loss: 2.270131585418537

Epoch: 5| Step: 3
Training loss: 1.7707058191299438
Validation loss: 2.2715194481675343

Epoch: 5| Step: 4
Training loss: 2.447237730026245
Validation loss: 2.2724338423821235

Epoch: 5| Step: 5
Training loss: 1.9006624221801758
Validation loss: 2.2832761297943773

Epoch: 5| Step: 6
Training loss: 2.7242817878723145
Validation loss: 2.295885056577703

Epoch: 5| Step: 7
Training loss: 2.618730068206787
Validation loss: 2.304246130809989

Epoch: 5| Step: 8
Training loss: 2.6902058124542236
Validation loss: 2.3129040964188112

Epoch: 5| Step: 9
Training loss: 2.3982887268066406
Validation loss: 2.3227231220532487

Epoch: 5| Step: 10
Training loss: 3.1466798782348633
Validation loss: 2.316954502495386

Epoch: 106| Step: 0
Training loss: 2.2633156776428223
Validation loss: 2.291457967091632

Epoch: 5| Step: 1
Training loss: 2.4093050956726074
Validation loss: 2.2794341989742812

Epoch: 5| Step: 2
Training loss: 2.4775447845458984
Validation loss: 2.2719501885034705

Epoch: 5| Step: 3
Training loss: 2.3495049476623535
Validation loss: 2.2734182752588743

Epoch: 5| Step: 4
Training loss: 3.1315419673919678
Validation loss: 2.2691971050795687

Epoch: 5| Step: 5
Training loss: 2.5374531745910645
Validation loss: 2.2718162023892967

Epoch: 5| Step: 6
Training loss: 2.851865291595459
Validation loss: 2.2807581450349543

Epoch: 5| Step: 7
Training loss: 2.9356436729431152
Validation loss: 2.2825196327701693

Epoch: 5| Step: 8
Training loss: 2.1153242588043213
Validation loss: 2.281006413121377

Epoch: 5| Step: 9
Training loss: 2.676746368408203
Validation loss: 2.307666540145874

Epoch: 5| Step: 10
Training loss: 2.2083094120025635
Validation loss: 2.3352230389912925

Epoch: 107| Step: 0
Training loss: 2.923173427581787
Validation loss: 2.295418764955254

Epoch: 5| Step: 1
Training loss: 2.064404010772705
Validation loss: 2.2714433375225274

Epoch: 5| Step: 2
Training loss: 2.783165454864502
Validation loss: 2.2654465270298783

Epoch: 5| Step: 3
Training loss: 2.223640203475952
Validation loss: 2.2644066413243613

Epoch: 5| Step: 4
Training loss: 2.2031428813934326
Validation loss: 2.269316109277869

Epoch: 5| Step: 5
Training loss: 2.7897441387176514
Validation loss: 2.270903774487075

Epoch: 5| Step: 6
Training loss: 2.4651198387145996
Validation loss: 2.281673833888064

Epoch: 5| Step: 7
Training loss: 2.4920058250427246
Validation loss: 2.281763667701393

Epoch: 5| Step: 8
Training loss: 2.8537755012512207
Validation loss: 2.281433359269173

Epoch: 5| Step: 9
Training loss: 2.33622145652771
Validation loss: 2.2884085947467434

Epoch: 5| Step: 10
Training loss: 2.999356269836426
Validation loss: 2.2834379006457586

Epoch: 108| Step: 0
Training loss: 2.593975067138672
Validation loss: 2.28219558090292

Epoch: 5| Step: 1
Training loss: 2.4334988594055176
Validation loss: 2.281676491101583

Epoch: 5| Step: 2
Training loss: 2.256589889526367
Validation loss: 2.29162077237201

Epoch: 5| Step: 3
Training loss: 2.0093441009521484
Validation loss: 2.288736948402979

Epoch: 5| Step: 4
Training loss: 2.8206398487091064
Validation loss: 2.29658402422423

Epoch: 5| Step: 5
Training loss: 2.8273541927337646
Validation loss: 2.294619929405951

Epoch: 5| Step: 6
Training loss: 3.421715259552002
Validation loss: 2.292287988047446

Epoch: 5| Step: 7
Training loss: 2.225870370864868
Validation loss: 2.2853994113142773

Epoch: 5| Step: 8
Training loss: 3.0008604526519775
Validation loss: 2.298408241682155

Epoch: 5| Step: 9
Training loss: 2.5210793018341064
Validation loss: 2.2870034351143786

Epoch: 5| Step: 10
Training loss: 1.715999722480774
Validation loss: 2.297715340891192

Epoch: 109| Step: 0
Training loss: 2.953403949737549
Validation loss: 2.300682824145081

Epoch: 5| Step: 1
Training loss: 2.1691250801086426
Validation loss: 2.2992323265280774

Epoch: 5| Step: 2
Training loss: 2.6531829833984375
Validation loss: 2.269661718799222

Epoch: 5| Step: 3
Training loss: 2.5921382904052734
Validation loss: 2.2558422562896565

Epoch: 5| Step: 4
Training loss: 3.0152578353881836
Validation loss: 2.2468869352853424

Epoch: 5| Step: 5
Training loss: 1.5755455493927002
Validation loss: 2.254726955967565

Epoch: 5| Step: 6
Training loss: 1.9858500957489014
Validation loss: 2.2504804518914994

Epoch: 5| Step: 7
Training loss: 2.751512050628662
Validation loss: 2.252693719761346

Epoch: 5| Step: 8
Training loss: 2.6163265705108643
Validation loss: 2.268166906090193

Epoch: 5| Step: 9
Training loss: 2.404006004333496
Validation loss: 2.306590777571483

Epoch: 5| Step: 10
Training loss: 3.2716472148895264
Validation loss: 2.349078796243155

Epoch: 110| Step: 0
Training loss: 2.786424160003662
Validation loss: 2.3647884989297516

Epoch: 5| Step: 1
Training loss: 2.579796552658081
Validation loss: 2.3729217770279094

Epoch: 5| Step: 2
Training loss: 2.6423466205596924
Validation loss: 2.3789859151327484

Epoch: 5| Step: 3
Training loss: 2.5996031761169434
Validation loss: 2.3641707756186046

Epoch: 5| Step: 4
Training loss: 2.7627053260803223
Validation loss: 2.3158927373988654

Epoch: 5| Step: 5
Training loss: 2.1295359134674072
Validation loss: 2.286418325157576

Epoch: 5| Step: 6
Training loss: 2.4806275367736816
Validation loss: 2.2736027086934736

Epoch: 5| Step: 7
Training loss: 2.1170125007629395
Validation loss: 2.2532851785741825

Epoch: 5| Step: 8
Training loss: 2.4050052165985107
Validation loss: 2.254596410259124

Epoch: 5| Step: 9
Training loss: 2.9156947135925293
Validation loss: 2.258116727234215

Epoch: 5| Step: 10
Training loss: 2.3594937324523926
Validation loss: 2.2615393900102183

Epoch: 111| Step: 0
Training loss: 2.267967700958252
Validation loss: 2.25842261058028

Epoch: 5| Step: 1
Training loss: 2.040724277496338
Validation loss: 2.249816712512765

Epoch: 5| Step: 2
Training loss: 2.3701186180114746
Validation loss: 2.2465304379822104

Epoch: 5| Step: 3
Training loss: 2.7141964435577393
Validation loss: 2.2426404209547144

Epoch: 5| Step: 4
Training loss: 3.2015891075134277
Validation loss: 2.2490625586560977

Epoch: 5| Step: 5
Training loss: 2.072814464569092
Validation loss: 2.255895632569508

Epoch: 5| Step: 6
Training loss: 3.146768808364868
Validation loss: 2.2740110351193334

Epoch: 5| Step: 7
Training loss: 2.4409265518188477
Validation loss: 2.283472312394009

Epoch: 5| Step: 8
Training loss: 2.4201323986053467
Validation loss: 2.3042060072704027

Epoch: 5| Step: 9
Training loss: 2.1484501361846924
Validation loss: 2.3167041476054857

Epoch: 5| Step: 10
Training loss: 2.8990020751953125
Validation loss: 2.3420495897211056

Epoch: 112| Step: 0
Training loss: 2.6629421710968018
Validation loss: 2.357110841299898

Epoch: 5| Step: 1
Training loss: 2.488835334777832
Validation loss: 2.3141492259117866

Epoch: 5| Step: 2
Training loss: 1.827218770980835
Validation loss: 2.284321267117736

Epoch: 5| Step: 3
Training loss: 2.376180410385132
Validation loss: 2.258668275289638

Epoch: 5| Step: 4
Training loss: 2.8606553077697754
Validation loss: 2.2480529251919

Epoch: 5| Step: 5
Training loss: 2.56795072555542
Validation loss: 2.248203423715407

Epoch: 5| Step: 6
Training loss: 2.7851648330688477
Validation loss: 2.242961114452731

Epoch: 5| Step: 7
Training loss: 2.289048433303833
Validation loss: 2.240981169926223

Epoch: 5| Step: 8
Training loss: 2.79217529296875
Validation loss: 2.238297403499644

Epoch: 5| Step: 9
Training loss: 2.418621063232422
Validation loss: 2.2438009990158903

Epoch: 5| Step: 10
Training loss: 2.7248659133911133
Validation loss: 2.246540295180454

Epoch: 113| Step: 0
Training loss: 2.1180877685546875
Validation loss: 2.256092935480097

Epoch: 5| Step: 1
Training loss: 2.610201358795166
Validation loss: 2.2448830014915875

Epoch: 5| Step: 2
Training loss: 2.5737977027893066
Validation loss: 2.245443355652594

Epoch: 5| Step: 3
Training loss: 2.8364875316619873
Validation loss: 2.2518181480387205

Epoch: 5| Step: 4
Training loss: 1.7592283487319946
Validation loss: 2.2659003632042998

Epoch: 5| Step: 5
Training loss: 2.994427442550659
Validation loss: 2.2870314557065248

Epoch: 5| Step: 6
Training loss: 2.8462700843811035
Validation loss: 2.3064086924317064

Epoch: 5| Step: 7
Training loss: 3.0522074699401855
Validation loss: 2.3182442854809504

Epoch: 5| Step: 8
Training loss: 2.493863344192505
Validation loss: 2.294520375549152

Epoch: 5| Step: 9
Training loss: 2.2501304149627686
Validation loss: 2.2669428010140695

Epoch: 5| Step: 10
Training loss: 2.1282997131347656
Validation loss: 2.258598683982767

Epoch: 114| Step: 0
Training loss: 2.783332109451294
Validation loss: 2.2499667367627545

Epoch: 5| Step: 1
Training loss: 2.2493042945861816
Validation loss: 2.2558630166515226

Epoch: 5| Step: 2
Training loss: 2.368332862854004
Validation loss: 2.2495965060367378

Epoch: 5| Step: 3
Training loss: 2.5303194522857666
Validation loss: 2.236594384716403

Epoch: 5| Step: 4
Training loss: 1.611676573753357
Validation loss: 2.2421799244419223

Epoch: 5| Step: 5
Training loss: 2.488328218460083
Validation loss: 2.2434941491773053

Epoch: 5| Step: 6
Training loss: 2.6805732250213623
Validation loss: 2.26975756050438

Epoch: 5| Step: 7
Training loss: 2.527346611022949
Validation loss: 2.2806519744216756

Epoch: 5| Step: 8
Training loss: 2.6091818809509277
Validation loss: 2.294545065972113

Epoch: 5| Step: 9
Training loss: 2.9562249183654785
Validation loss: 2.2952014271930983

Epoch: 5| Step: 10
Training loss: 2.941568374633789
Validation loss: 2.276067119772716

Epoch: 115| Step: 0
Training loss: 2.4956765174865723
Validation loss: 2.2630804841236403

Epoch: 5| Step: 1
Training loss: 2.5429234504699707
Validation loss: 2.245419873986193

Epoch: 5| Step: 2
Training loss: 2.383699417114258
Validation loss: 2.2381094168591242

Epoch: 5| Step: 3
Training loss: 2.0991768836975098
Validation loss: 2.2282502805033038

Epoch: 5| Step: 4
Training loss: 2.4558024406433105
Validation loss: 2.23540138429211

Epoch: 5| Step: 5
Training loss: 2.3091883659362793
Validation loss: 2.2282524442160003

Epoch: 5| Step: 6
Training loss: 2.308866024017334
Validation loss: 2.2288222107835995

Epoch: 5| Step: 7
Training loss: 2.6042580604553223
Validation loss: 2.2242747737515356

Epoch: 5| Step: 8
Training loss: 2.9163060188293457
Validation loss: 2.2269265433793426

Epoch: 5| Step: 9
Training loss: 1.960893988609314
Validation loss: 2.228920357201689

Epoch: 5| Step: 10
Training loss: 3.669435501098633
Validation loss: 2.244814734305105

Epoch: 116| Step: 0
Training loss: 2.639519214630127
Validation loss: 2.2568170921776884

Epoch: 5| Step: 1
Training loss: 2.69966197013855
Validation loss: 2.281107723072011

Epoch: 5| Step: 2
Training loss: 2.9010276794433594
Validation loss: 2.290467792941678

Epoch: 5| Step: 3
Training loss: 2.295029640197754
Validation loss: 2.297696675023725

Epoch: 5| Step: 4
Training loss: 2.295849561691284
Validation loss: 2.2948524669934343

Epoch: 5| Step: 5
Training loss: 2.4656786918640137
Validation loss: 2.2835328630221787

Epoch: 5| Step: 6
Training loss: 2.426246166229248
Validation loss: 2.2401947167611893

Epoch: 5| Step: 7
Training loss: 2.6883513927459717
Validation loss: 2.223436414554555

Epoch: 5| Step: 8
Training loss: 2.326934337615967
Validation loss: 2.215799846956807

Epoch: 5| Step: 9
Training loss: 2.329411506652832
Validation loss: 2.2231588671284337

Epoch: 5| Step: 10
Training loss: 2.4672722816467285
Validation loss: 2.2365520090185185

Epoch: 117| Step: 0
Training loss: 2.634223222732544
Validation loss: 2.238102202774376

Epoch: 5| Step: 1
Training loss: 2.438767194747925
Validation loss: 2.236625443222702

Epoch: 5| Step: 2
Training loss: 3.2925968170166016
Validation loss: 2.2274706543132825

Epoch: 5| Step: 3
Training loss: 1.6706387996673584
Validation loss: 2.215495599213467

Epoch: 5| Step: 4
Training loss: 2.422781229019165
Validation loss: 2.204335097343691

Epoch: 5| Step: 5
Training loss: 2.3613522052764893
Validation loss: 2.2042391223292195

Epoch: 5| Step: 6
Training loss: 2.091400623321533
Validation loss: 2.2202288066187212

Epoch: 5| Step: 7
Training loss: 2.208029270172119
Validation loss: 2.2808960894102692

Epoch: 5| Step: 8
Training loss: 2.618807792663574
Validation loss: 2.3293184798250914

Epoch: 5| Step: 9
Training loss: 3.528754472732544
Validation loss: 2.3625750823687484

Epoch: 5| Step: 10
Training loss: 2.6284878253936768
Validation loss: 2.3456654215371735

Epoch: 118| Step: 0
Training loss: 2.4161200523376465
Validation loss: 2.308466967716012

Epoch: 5| Step: 1
Training loss: 2.388587236404419
Validation loss: 2.287544386361235

Epoch: 5| Step: 2
Training loss: 2.667473793029785
Validation loss: 2.262262228996523

Epoch: 5| Step: 3
Training loss: 2.861612558364868
Validation loss: 2.2308588694500666

Epoch: 5| Step: 4
Training loss: 2.001279354095459
Validation loss: 2.229424358696066

Epoch: 5| Step: 5
Training loss: 2.391404151916504
Validation loss: 2.2255269019834456

Epoch: 5| Step: 6
Training loss: 2.7386975288391113
Validation loss: 2.220698551465106

Epoch: 5| Step: 7
Training loss: 2.3222739696502686
Validation loss: 2.217679854362242

Epoch: 5| Step: 8
Training loss: 2.73004412651062
Validation loss: 2.2241989489524596

Epoch: 5| Step: 9
Training loss: 2.4791512489318848
Validation loss: 2.219122314965853

Epoch: 5| Step: 10
Training loss: 2.6079561710357666
Validation loss: 2.217773791282408

Epoch: 119| Step: 0
Training loss: 2.8863630294799805
Validation loss: 2.212590904646022

Epoch: 5| Step: 1
Training loss: 2.065751552581787
Validation loss: 2.2195180513525523

Epoch: 5| Step: 2
Training loss: 1.993410348892212
Validation loss: 2.2314561977181384

Epoch: 5| Step: 3
Training loss: 1.9581716060638428
Validation loss: 2.251134228962724

Epoch: 5| Step: 4
Training loss: 2.5249578952789307
Validation loss: 2.2580340549510014

Epoch: 5| Step: 5
Training loss: 2.5680034160614014
Validation loss: 2.270240834964219

Epoch: 5| Step: 6
Training loss: 1.891445517539978
Validation loss: 2.291146170708441

Epoch: 5| Step: 7
Training loss: 3.2591145038604736
Validation loss: 2.291955312093099

Epoch: 5| Step: 8
Training loss: 2.69096302986145
Validation loss: 2.2849238431581886

Epoch: 5| Step: 9
Training loss: 2.3345255851745605
Validation loss: 2.266190093050721

Epoch: 5| Step: 10
Training loss: 3.371492385864258
Validation loss: 2.252555657458562

Epoch: 120| Step: 0
Training loss: 2.4785656929016113
Validation loss: 2.2168341246984338

Epoch: 5| Step: 1
Training loss: 2.752686023712158
Validation loss: 2.1998159090677896

Epoch: 5| Step: 2
Training loss: 3.018404722213745
Validation loss: 2.19597634192436

Epoch: 5| Step: 3
Training loss: 1.7816702127456665
Validation loss: 2.1887275185636295

Epoch: 5| Step: 4
Training loss: 2.2423787117004395
Validation loss: 2.1890920310892086

Epoch: 5| Step: 5
Training loss: 3.051162004470825
Validation loss: 2.1853219155342347

Epoch: 5| Step: 6
Training loss: 2.553922653198242
Validation loss: 2.192979294766662

Epoch: 5| Step: 7
Training loss: 2.859138011932373
Validation loss: 2.190416561659946

Epoch: 5| Step: 8
Training loss: 2.5063283443450928
Validation loss: 2.193929510731851

Epoch: 5| Step: 9
Training loss: 1.8740650415420532
Validation loss: 2.2005647074791694

Epoch: 5| Step: 10
Training loss: 2.1999058723449707
Validation loss: 2.2146153155193535

Epoch: 121| Step: 0
Training loss: 2.4328091144561768
Validation loss: 2.2268573058548795

Epoch: 5| Step: 1
Training loss: 2.054677963256836
Validation loss: 2.2218207518259683

Epoch: 5| Step: 2
Training loss: 2.3973140716552734
Validation loss: 2.220071200401552

Epoch: 5| Step: 3
Training loss: 1.922468900680542
Validation loss: 2.2119789033807735

Epoch: 5| Step: 4
Training loss: 2.744439125061035
Validation loss: 2.2200624814597507

Epoch: 5| Step: 5
Training loss: 3.683326005935669
Validation loss: 2.220306734884939

Epoch: 5| Step: 6
Training loss: 2.4682319164276123
Validation loss: 2.2247990049341673

Epoch: 5| Step: 7
Training loss: 2.2255642414093018
Validation loss: 2.2299035826037006

Epoch: 5| Step: 8
Training loss: 2.332132339477539
Validation loss: 2.2411347691730787

Epoch: 5| Step: 9
Training loss: 2.4277985095977783
Validation loss: 2.229365389834168

Epoch: 5| Step: 10
Training loss: 2.46612286567688
Validation loss: 2.230355571675044

Epoch: 122| Step: 0
Training loss: 2.804774761199951
Validation loss: 2.219555385651127

Epoch: 5| Step: 1
Training loss: 3.1227970123291016
Validation loss: 2.2166344811839442

Epoch: 5| Step: 2
Training loss: 2.7977089881896973
Validation loss: 2.2121928891827984

Epoch: 5| Step: 3
Training loss: 2.227098226547241
Validation loss: 2.1963927438182216

Epoch: 5| Step: 4
Training loss: 2.555216073989868
Validation loss: 2.199559211730957

Epoch: 5| Step: 5
Training loss: 2.124502658843994
Validation loss: 2.193900713356592

Epoch: 5| Step: 6
Training loss: 2.4260687828063965
Validation loss: 2.188806538940758

Epoch: 5| Step: 7
Training loss: 2.0982251167297363
Validation loss: 2.1874137706654047

Epoch: 5| Step: 8
Training loss: 2.4090325832366943
Validation loss: 2.183718418562284

Epoch: 5| Step: 9
Training loss: 2.090447425842285
Validation loss: 2.2039887366756314

Epoch: 5| Step: 10
Training loss: 2.6989049911499023
Validation loss: 2.2153122707079818

Epoch: 123| Step: 0
Training loss: 1.8253024816513062
Validation loss: 2.2560118552177184

Epoch: 5| Step: 1
Training loss: 2.840327739715576
Validation loss: 2.3174586372990764

Epoch: 5| Step: 2
Training loss: 1.8172273635864258
Validation loss: 2.3140340261561896

Epoch: 5| Step: 3
Training loss: 2.8508782386779785
Validation loss: 2.343330734519548

Epoch: 5| Step: 4
Training loss: 2.861036777496338
Validation loss: 2.345403371318694

Epoch: 5| Step: 5
Training loss: 2.3665542602539062
Validation loss: 2.3092234467947357

Epoch: 5| Step: 6
Training loss: 2.972730875015259
Validation loss: 2.2358258513994116

Epoch: 5| Step: 7
Training loss: 2.516313076019287
Validation loss: 2.2033587360894806

Epoch: 5| Step: 8
Training loss: 2.382131576538086
Validation loss: 2.187895812014098

Epoch: 5| Step: 9
Training loss: 2.4183719158172607
Validation loss: 2.1852425734202066

Epoch: 5| Step: 10
Training loss: 2.424894332885742
Validation loss: 2.1968036774666078

Epoch: 124| Step: 0
Training loss: 2.320786237716675
Validation loss: 2.2102467475398893

Epoch: 5| Step: 1
Training loss: 2.307793140411377
Validation loss: 2.2118233788398003

Epoch: 5| Step: 2
Training loss: 2.7800707817077637
Validation loss: 2.213431486519434

Epoch: 5| Step: 3
Training loss: 3.0241332054138184
Validation loss: 2.211575138953424

Epoch: 5| Step: 4
Training loss: 1.9739536046981812
Validation loss: 2.209660872336357

Epoch: 5| Step: 5
Training loss: 1.9804182052612305
Validation loss: 2.2063177413837884

Epoch: 5| Step: 6
Training loss: 2.454954147338867
Validation loss: 2.199937315397365

Epoch: 5| Step: 7
Training loss: 2.436051607131958
Validation loss: 2.216461318795399

Epoch: 5| Step: 8
Training loss: 2.316981077194214
Validation loss: 2.2447039363204793

Epoch: 5| Step: 9
Training loss: 3.001551389694214
Validation loss: 2.288304146899972

Epoch: 5| Step: 10
Training loss: 3.082753896713257
Validation loss: 2.310431916226623

Epoch: 125| Step: 0
Training loss: 2.2903358936309814
Validation loss: 2.297608547313239

Epoch: 5| Step: 1
Training loss: 2.374915361404419
Validation loss: 2.2421596383535736

Epoch: 5| Step: 2
Training loss: 3.263401746749878
Validation loss: 2.2066046794255576

Epoch: 5| Step: 3
Training loss: 2.651416301727295
Validation loss: 2.1783455828184723

Epoch: 5| Step: 4
Training loss: 2.3132777214050293
Validation loss: 2.1753287392277874

Epoch: 5| Step: 5
Training loss: 2.3938732147216797
Validation loss: 2.1755578646095852

Epoch: 5| Step: 6
Training loss: 2.5363211631774902
Validation loss: 2.1824798737802813

Epoch: 5| Step: 7
Training loss: 2.078096628189087
Validation loss: 2.1858326081306703

Epoch: 5| Step: 8
Training loss: 2.836475372314453
Validation loss: 2.1863639611069874

Epoch: 5| Step: 9
Training loss: 2.006314277648926
Validation loss: 2.190826413451984

Epoch: 5| Step: 10
Training loss: 2.698486566543579
Validation loss: 2.2136973719443045

Epoch: 126| Step: 0
Training loss: 3.3211867809295654
Validation loss: 2.235348663022441

Epoch: 5| Step: 1
Training loss: 2.5908539295196533
Validation loss: 2.259216785430908

Epoch: 5| Step: 2
Training loss: 2.3215062618255615
Validation loss: 2.279026631386049

Epoch: 5| Step: 3
Training loss: 2.3375141620635986
Validation loss: 2.276880928265151

Epoch: 5| Step: 4
Training loss: 1.8799244165420532
Validation loss: 2.280483502213673

Epoch: 5| Step: 5
Training loss: 3.307116985321045
Validation loss: 2.2642961138038227

Epoch: 5| Step: 6
Training loss: 2.352332592010498
Validation loss: 2.2221000732914096

Epoch: 5| Step: 7
Training loss: 2.203187942504883
Validation loss: 2.1986821107966925

Epoch: 5| Step: 8
Training loss: 1.7236055135726929
Validation loss: 2.18887097092085

Epoch: 5| Step: 9
Training loss: 2.486639976501465
Validation loss: 2.1719735130187003

Epoch: 5| Step: 10
Training loss: 2.7342238426208496
Validation loss: 2.1730479091726322

Epoch: 127| Step: 0
Training loss: 2.4904274940490723
Validation loss: 2.171688918144472

Epoch: 5| Step: 1
Training loss: 2.7975592613220215
Validation loss: 2.1760749432348434

Epoch: 5| Step: 2
Training loss: 2.5940661430358887
Validation loss: 2.1768302020206245

Epoch: 5| Step: 3
Training loss: 1.9269880056381226
Validation loss: 2.1890101714800765

Epoch: 5| Step: 4
Training loss: 2.2445836067199707
Validation loss: 2.1920309194954495

Epoch: 5| Step: 5
Training loss: 2.8411035537719727
Validation loss: 2.1944816215063936

Epoch: 5| Step: 6
Training loss: 2.300379991531372
Validation loss: 2.2043412693085207

Epoch: 5| Step: 7
Training loss: 2.689631938934326
Validation loss: 2.2082710112294843

Epoch: 5| Step: 8
Training loss: 1.9702469110488892
Validation loss: 2.209315073105597

Epoch: 5| Step: 9
Training loss: 2.214114189147949
Validation loss: 2.201752006366689

Epoch: 5| Step: 10
Training loss: 2.973846673965454
Validation loss: 2.2035133761744343

Epoch: 128| Step: 0
Training loss: 2.071934938430786
Validation loss: 2.199519221500684

Epoch: 5| Step: 1
Training loss: 2.525982141494751
Validation loss: 2.1996796259316067

Epoch: 5| Step: 2
Training loss: 1.9116601943969727
Validation loss: 2.1975443875917824

Epoch: 5| Step: 3
Training loss: 2.6634790897369385
Validation loss: 2.204543972528109

Epoch: 5| Step: 4
Training loss: 2.6074795722961426
Validation loss: 2.208002678809627

Epoch: 5| Step: 5
Training loss: 1.9693498611450195
Validation loss: 2.206056953758322

Epoch: 5| Step: 6
Training loss: 2.9345407485961914
Validation loss: 2.196477349086474

Epoch: 5| Step: 7
Training loss: 2.2309486865997314
Validation loss: 2.1973227121496715

Epoch: 5| Step: 8
Training loss: 2.657881259918213
Validation loss: 2.1932527852314774

Epoch: 5| Step: 9
Training loss: 2.5605578422546387
Validation loss: 2.1900545063839165

Epoch: 5| Step: 10
Training loss: 2.785435914993286
Validation loss: 2.1947580588761197

Epoch: 129| Step: 0
Training loss: 2.087646961212158
Validation loss: 2.1923665872184177

Epoch: 5| Step: 1
Training loss: 2.9007115364074707
Validation loss: 2.2044668146359023

Epoch: 5| Step: 2
Training loss: 2.036118984222412
Validation loss: 2.2059551823523735

Epoch: 5| Step: 3
Training loss: 2.373983383178711
Validation loss: 2.2375180439282487

Epoch: 5| Step: 4
Training loss: 1.8279340267181396
Validation loss: 2.247336918307889

Epoch: 5| Step: 5
Training loss: 2.1387290954589844
Validation loss: 2.2348982236718618

Epoch: 5| Step: 6
Training loss: 2.463222026824951
Validation loss: 2.2339500842555875

Epoch: 5| Step: 7
Training loss: 2.3907036781311035
Validation loss: 2.2300168211742113

Epoch: 5| Step: 8
Training loss: 2.83665132522583
Validation loss: 2.229519574872909

Epoch: 5| Step: 9
Training loss: 2.592231035232544
Validation loss: 2.2181725245650097

Epoch: 5| Step: 10
Training loss: 3.282899856567383
Validation loss: 2.2123859966954877

Epoch: 130| Step: 0
Training loss: 2.17775559425354
Validation loss: 2.2120789891930035

Epoch: 5| Step: 1
Training loss: 2.540309190750122
Validation loss: 2.214777841362902

Epoch: 5| Step: 2
Training loss: 2.413485050201416
Validation loss: 2.2197099065267913

Epoch: 5| Step: 3
Training loss: 2.1347904205322266
Validation loss: 2.2128867539026404

Epoch: 5| Step: 4
Training loss: 2.4674181938171387
Validation loss: 2.2118863879993396

Epoch: 5| Step: 5
Training loss: 2.4365363121032715
Validation loss: 2.1984347604936167

Epoch: 5| Step: 6
Training loss: 2.668865442276001
Validation loss: 2.164253052844796

Epoch: 5| Step: 7
Training loss: 2.53275465965271
Validation loss: 2.1503289104789816

Epoch: 5| Step: 8
Training loss: 2.2801709175109863
Validation loss: 2.1415647716932398

Epoch: 5| Step: 9
Training loss: 2.779735565185547
Validation loss: 2.138565530059158

Epoch: 5| Step: 10
Training loss: 2.362504720687866
Validation loss: 2.1442440120122765

Epoch: 131| Step: 0
Training loss: 2.410797119140625
Validation loss: 2.1464991338791384

Epoch: 5| Step: 1
Training loss: 2.6677021980285645
Validation loss: 2.142254980661536

Epoch: 5| Step: 2
Training loss: 2.144583225250244
Validation loss: 2.1560669483677035

Epoch: 5| Step: 3
Training loss: 2.5381693840026855
Validation loss: 2.1567573598636094

Epoch: 5| Step: 4
Training loss: 2.4227888584136963
Validation loss: 2.1509674915703396

Epoch: 5| Step: 5
Training loss: 3.009033679962158
Validation loss: 2.1557468111797045

Epoch: 5| Step: 6
Training loss: 1.7238948345184326
Validation loss: 2.1523423553794943

Epoch: 5| Step: 7
Training loss: 1.8915290832519531
Validation loss: 2.1608450438386653

Epoch: 5| Step: 8
Training loss: 2.887390613555908
Validation loss: 2.1778919273807156

Epoch: 5| Step: 9
Training loss: 2.1811797618865967
Validation loss: 2.205020002139512

Epoch: 5| Step: 10
Training loss: 2.722081184387207
Validation loss: 2.198373894537649

Epoch: 132| Step: 0
Training loss: 1.7706750631332397
Validation loss: 2.1984207694248488

Epoch: 5| Step: 1
Training loss: 2.7178940773010254
Validation loss: 2.193594609537432

Epoch: 5| Step: 2
Training loss: 2.838223695755005
Validation loss: 2.1776844237440374

Epoch: 5| Step: 3
Training loss: 2.222541093826294
Validation loss: 2.1754997353399954

Epoch: 5| Step: 4
Training loss: 1.6145296096801758
Validation loss: 2.170755063333819

Epoch: 5| Step: 5
Training loss: 2.630959987640381
Validation loss: 2.169193007612741

Epoch: 5| Step: 6
Training loss: 2.366103172302246
Validation loss: 2.174114873332362

Epoch: 5| Step: 7
Training loss: 1.888757348060608
Validation loss: 2.1882970948373117

Epoch: 5| Step: 8
Training loss: 3.182889461517334
Validation loss: 2.2014520655396166

Epoch: 5| Step: 9
Training loss: 2.437936305999756
Validation loss: 2.2068343777810373

Epoch: 5| Step: 10
Training loss: 2.9739792346954346
Validation loss: 2.1989028505099717

Epoch: 133| Step: 0
Training loss: 2.599102735519409
Validation loss: 2.1659323553885184

Epoch: 5| Step: 1
Training loss: 2.0297164916992188
Validation loss: 2.151737241334813

Epoch: 5| Step: 2
Training loss: 2.99410343170166
Validation loss: 2.1463955845884097

Epoch: 5| Step: 3
Training loss: 2.3877477645874023
Validation loss: 2.1541430360527447

Epoch: 5| Step: 4
Training loss: 2.1773266792297363
Validation loss: 2.1451037878631265

Epoch: 5| Step: 5
Training loss: 2.4703164100646973
Validation loss: 2.1410104061967585

Epoch: 5| Step: 6
Training loss: 2.2282042503356934
Validation loss: 2.1414240944770073

Epoch: 5| Step: 7
Training loss: 2.2589688301086426
Validation loss: 2.159921289772116

Epoch: 5| Step: 8
Training loss: 2.0492165088653564
Validation loss: 2.202465526519283

Epoch: 5| Step: 9
Training loss: 3.1998491287231445
Validation loss: 2.249810039356191

Epoch: 5| Step: 10
Training loss: 2.1012938022613525
Validation loss: 2.2488860468710623

Epoch: 134| Step: 0
Training loss: 2.4691977500915527
Validation loss: 2.2165653628687703

Epoch: 5| Step: 1
Training loss: 2.8486297130584717
Validation loss: 2.149412255133352

Epoch: 5| Step: 2
Training loss: 2.283599615097046
Validation loss: 2.1252177082082278

Epoch: 5| Step: 3
Training loss: 2.5270867347717285
Validation loss: 2.120090433346328

Epoch: 5| Step: 4
Training loss: 1.9438358545303345
Validation loss: 2.1371744294320383

Epoch: 5| Step: 5
Training loss: 2.5039989948272705
Validation loss: 2.1384468616977816

Epoch: 5| Step: 6
Training loss: 2.0355286598205566
Validation loss: 2.140848710972776

Epoch: 5| Step: 7
Training loss: 2.1538288593292236
Validation loss: 2.1335314217434136

Epoch: 5| Step: 8
Training loss: 2.6094233989715576
Validation loss: 2.1292472988046627

Epoch: 5| Step: 9
Training loss: 2.2731120586395264
Validation loss: 2.1497352661625033

Epoch: 5| Step: 10
Training loss: 3.137693405151367
Validation loss: 2.1510242723649546

Epoch: 135| Step: 0
Training loss: 2.0174355506896973
Validation loss: 2.1640200332928727

Epoch: 5| Step: 1
Training loss: 2.99755597114563
Validation loss: 2.170182639552701

Epoch: 5| Step: 2
Training loss: 2.2329726219177246
Validation loss: 2.1787300866137267

Epoch: 5| Step: 3
Training loss: 3.1147117614746094
Validation loss: 2.1819247212461246

Epoch: 5| Step: 4
Training loss: 2.3866324424743652
Validation loss: 2.173318962897024

Epoch: 5| Step: 5
Training loss: 1.7867460250854492
Validation loss: 2.1701130303003455

Epoch: 5| Step: 6
Training loss: 2.752364158630371
Validation loss: 2.1630233244229387

Epoch: 5| Step: 7
Training loss: 2.3306174278259277
Validation loss: 2.1525782782544374

Epoch: 5| Step: 8
Training loss: 2.947026491165161
Validation loss: 2.1429144708059167

Epoch: 5| Step: 9
Training loss: 1.5013277530670166
Validation loss: 2.1429443410647813

Epoch: 5| Step: 10
Training loss: 2.1514391899108887
Validation loss: 2.150901150959794

Epoch: 136| Step: 0
Training loss: 2.819963216781616
Validation loss: 2.151744674610835

Epoch: 5| Step: 1
Training loss: 3.1981711387634277
Validation loss: 2.1682585285555933

Epoch: 5| Step: 2
Training loss: 2.22737717628479
Validation loss: 2.170722476897701

Epoch: 5| Step: 3
Training loss: 2.0151207447052
Validation loss: 2.188112476820587

Epoch: 5| Step: 4
Training loss: 2.417114734649658
Validation loss: 2.1875487937722156

Epoch: 5| Step: 5
Training loss: 2.272138833999634
Validation loss: 2.185382584089874

Epoch: 5| Step: 6
Training loss: 2.6408579349517822
Validation loss: 2.185866194386636

Epoch: 5| Step: 7
Training loss: 1.8231874704360962
Validation loss: 2.175741239260602

Epoch: 5| Step: 8
Training loss: 2.7421889305114746
Validation loss: 2.1893577626956406

Epoch: 5| Step: 9
Training loss: 1.9148445129394531
Validation loss: 2.1754965230982792

Epoch: 5| Step: 10
Training loss: 2.0415189266204834
Validation loss: 2.1833342685494372

Epoch: 137| Step: 0
Training loss: 2.7314677238464355
Validation loss: 2.165993744327176

Epoch: 5| Step: 1
Training loss: 2.234205722808838
Validation loss: 2.151743273581228

Epoch: 5| Step: 2
Training loss: 2.2655999660491943
Validation loss: 2.1521142272539038

Epoch: 5| Step: 3
Training loss: 2.242199182510376
Validation loss: 2.1475783227592387

Epoch: 5| Step: 4
Training loss: 2.5278284549713135
Validation loss: 2.148152588516153

Epoch: 5| Step: 5
Training loss: 2.323589324951172
Validation loss: 2.1506397595969577

Epoch: 5| Step: 6
Training loss: 2.0696723461151123
Validation loss: 2.1447837557843936

Epoch: 5| Step: 7
Training loss: 2.8680427074432373
Validation loss: 2.1336960843814317

Epoch: 5| Step: 8
Training loss: 2.6602723598480225
Validation loss: 2.114476357736895

Epoch: 5| Step: 9
Training loss: 2.3209099769592285
Validation loss: 2.1158476414219027

Epoch: 5| Step: 10
Training loss: 1.8481744527816772
Validation loss: 2.121275737721433

Epoch: 138| Step: 0
Training loss: 2.139472246170044
Validation loss: 2.131143287945819

Epoch: 5| Step: 1
Training loss: 1.8485946655273438
Validation loss: 2.1520641388431674

Epoch: 5| Step: 2
Training loss: 2.5171244144439697
Validation loss: 2.17936598613698

Epoch: 5| Step: 3
Training loss: 2.3892130851745605
Validation loss: 2.195732673009237

Epoch: 5| Step: 4
Training loss: 2.3168959617614746
Validation loss: 2.209176125064973

Epoch: 5| Step: 5
Training loss: 2.3477911949157715
Validation loss: 2.234222017308717

Epoch: 5| Step: 6
Training loss: 2.817347526550293
Validation loss: 2.225512348195558

Epoch: 5| Step: 7
Training loss: 2.499037981033325
Validation loss: 2.196342450316234

Epoch: 5| Step: 8
Training loss: 2.329270839691162
Validation loss: 2.1774447169355167

Epoch: 5| Step: 9
Training loss: 2.1588644981384277
Validation loss: 2.1863569059679584

Epoch: 5| Step: 10
Training loss: 2.939129114151001
Validation loss: 2.1886931106608403

Epoch: 139| Step: 0
Training loss: 2.5359089374542236
Validation loss: 2.248329484334556

Epoch: 5| Step: 1
Training loss: 2.2513766288757324
Validation loss: 2.2822924813916607

Epoch: 5| Step: 2
Training loss: 1.7944412231445312
Validation loss: 2.2892909947262017

Epoch: 5| Step: 3
Training loss: 2.584439754486084
Validation loss: 2.251771838434281

Epoch: 5| Step: 4
Training loss: 2.0474705696105957
Validation loss: 2.1783946816639235

Epoch: 5| Step: 5
Training loss: 2.2053005695343018
Validation loss: 2.1178111209664294

Epoch: 5| Step: 6
Training loss: 2.813013792037964
Validation loss: 2.094918889384116

Epoch: 5| Step: 7
Training loss: 2.227017641067505
Validation loss: 2.0811604415216753

Epoch: 5| Step: 8
Training loss: 2.361560344696045
Validation loss: 2.082208441149804

Epoch: 5| Step: 9
Training loss: 2.5310637950897217
Validation loss: 2.0754077383266982

Epoch: 5| Step: 10
Training loss: 3.027768611907959
Validation loss: 2.0778196960367183

Epoch: 140| Step: 0
Training loss: 2.3424713611602783
Validation loss: 2.09041323584895

Epoch: 5| Step: 1
Training loss: 2.0483665466308594
Validation loss: 2.08498703792531

Epoch: 5| Step: 2
Training loss: 1.7611926794052124
Validation loss: 2.100587139847458

Epoch: 5| Step: 3
Training loss: 3.396437168121338
Validation loss: 2.122286768369777

Epoch: 5| Step: 4
Training loss: 2.939088821411133
Validation loss: 2.1388394755701863

Epoch: 5| Step: 5
Training loss: 2.8681652545928955
Validation loss: 2.1593121867026053

Epoch: 5| Step: 6
Training loss: 2.1403820514678955
Validation loss: 2.1684158002176592

Epoch: 5| Step: 7
Training loss: 2.288050651550293
Validation loss: 2.167944972233106

Epoch: 5| Step: 8
Training loss: 2.2991092205047607
Validation loss: 2.1633188545062976

Epoch: 5| Step: 9
Training loss: 2.1048052310943604
Validation loss: 2.148616930489899

Epoch: 5| Step: 10
Training loss: 1.858597755432129
Validation loss: 2.1441294980305496

Epoch: 141| Step: 0
Training loss: 3.1066858768463135
Validation loss: 2.1399380596735145

Epoch: 5| Step: 1
Training loss: 1.9702749252319336
Validation loss: 2.12720424385481

Epoch: 5| Step: 2
Training loss: 2.225419282913208
Validation loss: 2.1223707583642777

Epoch: 5| Step: 3
Training loss: 2.1253199577331543
Validation loss: 2.132730089208131

Epoch: 5| Step: 4
Training loss: 2.2195146083831787
Validation loss: 2.1319421798952165

Epoch: 5| Step: 5
Training loss: 2.3957531452178955
Validation loss: 2.1249149794219644

Epoch: 5| Step: 6
Training loss: 1.8547245264053345
Validation loss: 2.1268288961020847

Epoch: 5| Step: 7
Training loss: 2.617844820022583
Validation loss: 2.1450267094437794

Epoch: 5| Step: 8
Training loss: 2.3807411193847656
Validation loss: 2.1611575823958202

Epoch: 5| Step: 9
Training loss: 2.3293323516845703
Validation loss: 2.142606117392099

Epoch: 5| Step: 10
Training loss: 2.5510950088500977
Validation loss: 2.1518372617742068

Epoch: 142| Step: 0
Training loss: 2.2955822944641113
Validation loss: 2.1432607404647337

Epoch: 5| Step: 1
Training loss: 2.292757511138916
Validation loss: 2.143510810790523

Epoch: 5| Step: 2
Training loss: 2.323958158493042
Validation loss: 2.1566628345879177

Epoch: 5| Step: 3
Training loss: 2.3605523109436035
Validation loss: 2.1585149418923164

Epoch: 5| Step: 4
Training loss: 2.6832435131073
Validation loss: 2.1579414542003343

Epoch: 5| Step: 5
Training loss: 2.6002907752990723
Validation loss: 2.165090363512757

Epoch: 5| Step: 6
Training loss: 2.020354747772217
Validation loss: 2.1392804089412896

Epoch: 5| Step: 7
Training loss: 2.340165615081787
Validation loss: 2.131880276946611

Epoch: 5| Step: 8
Training loss: 2.7624118328094482
Validation loss: 2.1212198759919856

Epoch: 5| Step: 9
Training loss: 1.325373649597168
Validation loss: 2.1062494695827527

Epoch: 5| Step: 10
Training loss: 2.730968475341797
Validation loss: 2.1136558645515033

Epoch: 143| Step: 0
Training loss: 2.5094799995422363
Validation loss: 2.1187796413257556

Epoch: 5| Step: 1
Training loss: 2.1389384269714355
Validation loss: 2.12492012721236

Epoch: 5| Step: 2
Training loss: 1.13191819190979
Validation loss: 2.126449651615594

Epoch: 5| Step: 3
Training loss: 2.3942742347717285
Validation loss: 2.138859292512299

Epoch: 5| Step: 4
Training loss: 2.2410879135131836
Validation loss: 2.157471492726316

Epoch: 5| Step: 5
Training loss: 2.4610068798065186
Validation loss: 2.1947543441608386

Epoch: 5| Step: 6
Training loss: 2.3721253871917725
Validation loss: 2.245077362624548

Epoch: 5| Step: 7
Training loss: 3.0751097202301025
Validation loss: 2.3147409398068666

Epoch: 5| Step: 8
Training loss: 2.2411632537841797
Validation loss: 2.3837329828610985

Epoch: 5| Step: 9
Training loss: 2.497896909713745
Validation loss: 2.3759295812217136

Epoch: 5| Step: 10
Training loss: 3.337615966796875
Validation loss: 2.2869345295813774

Epoch: 144| Step: 0
Training loss: 1.7751353979110718
Validation loss: 2.1968977861506964

Epoch: 5| Step: 1
Training loss: 2.7282307147979736
Validation loss: 2.133458892504374

Epoch: 5| Step: 2
Training loss: 2.4905128479003906
Validation loss: 2.1119032329128635

Epoch: 5| Step: 3
Training loss: 2.5506386756896973
Validation loss: 2.1180581354325816

Epoch: 5| Step: 4
Training loss: 2.0018484592437744
Validation loss: 2.1245478250647105

Epoch: 5| Step: 5
Training loss: 2.2691612243652344
Validation loss: 2.1275899230792956

Epoch: 5| Step: 6
Training loss: 1.7893686294555664
Validation loss: 2.141329374364627

Epoch: 5| Step: 7
Training loss: 2.7850921154022217
Validation loss: 2.144515593846639

Epoch: 5| Step: 8
Training loss: 2.2069430351257324
Validation loss: 2.1572578517339562

Epoch: 5| Step: 9
Training loss: 2.200000286102295
Validation loss: 2.1714792328496135

Epoch: 5| Step: 10
Training loss: 3.2261040210723877
Validation loss: 2.179291143212267

Epoch: 145| Step: 0
Training loss: 2.4264094829559326
Validation loss: 2.1869295797040387

Epoch: 5| Step: 1
Training loss: 2.620783805847168
Validation loss: 2.214255779020248

Epoch: 5| Step: 2
Training loss: 2.272477388381958
Validation loss: 2.217099056448988

Epoch: 5| Step: 3
Training loss: 2.0202956199645996
Validation loss: 2.230725360173051

Epoch: 5| Step: 4
Training loss: 2.3467307090759277
Validation loss: 2.2602727464450303

Epoch: 5| Step: 5
Training loss: 2.103621006011963
Validation loss: 2.2715449897191857

Epoch: 5| Step: 6
Training loss: 2.632901430130005
Validation loss: 2.2677248882991012

Epoch: 5| Step: 7
Training loss: 2.2828264236450195
Validation loss: 2.244406892407325

Epoch: 5| Step: 8
Training loss: 2.277719497680664
Validation loss: 2.2241793114651918

Epoch: 5| Step: 9
Training loss: 2.592782735824585
Validation loss: 2.2193188193023845

Epoch: 5| Step: 10
Training loss: 1.9924049377441406
Validation loss: 2.1753703676244265

Epoch: 146| Step: 0
Training loss: 2.3812105655670166
Validation loss: 2.1711918564252954

Epoch: 5| Step: 1
Training loss: 1.790116548538208
Validation loss: 2.1482068877066336

Epoch: 5| Step: 2
Training loss: 2.5174784660339355
Validation loss: 2.142621640236147

Epoch: 5| Step: 3
Training loss: 2.4842989444732666
Validation loss: 2.126097886793075

Epoch: 5| Step: 4
Training loss: 2.3883919715881348
Validation loss: 2.129845884538466

Epoch: 5| Step: 5
Training loss: 2.2634100914001465
Validation loss: 2.131964075949884

Epoch: 5| Step: 6
Training loss: 2.3884081840515137
Validation loss: 2.1244485557720227

Epoch: 5| Step: 7
Training loss: 2.7061753273010254
Validation loss: 2.123573813387143

Epoch: 5| Step: 8
Training loss: 2.7944514751434326
Validation loss: 2.119338745711952

Epoch: 5| Step: 9
Training loss: 1.5847089290618896
Validation loss: 2.1199750131176365

Epoch: 5| Step: 10
Training loss: 2.0189499855041504
Validation loss: 2.1270206718034643

Epoch: 147| Step: 0
Training loss: 2.4085631370544434
Validation loss: 2.118179008524905

Epoch: 5| Step: 1
Training loss: 2.369178295135498
Validation loss: 2.1224422224106325

Epoch: 5| Step: 2
Training loss: 2.0192723274230957
Validation loss: 2.095349334901379

Epoch: 5| Step: 3
Training loss: 2.476804256439209
Validation loss: 2.111280184920116

Epoch: 5| Step: 4
Training loss: 2.0862467288970947
Validation loss: 2.113143297933763

Epoch: 5| Step: 5
Training loss: 2.2774851322174072
Validation loss: 2.1345289984057025

Epoch: 5| Step: 6
Training loss: 2.4044010639190674
Validation loss: 2.153488197634297

Epoch: 5| Step: 7
Training loss: 2.574103832244873
Validation loss: 2.1916319516397293

Epoch: 5| Step: 8
Training loss: 2.259188413619995
Validation loss: 2.216550673207929

Epoch: 5| Step: 9
Training loss: 2.550989866256714
Validation loss: 2.2063680284766742

Epoch: 5| Step: 10
Training loss: 1.9518486261367798
Validation loss: 2.1979908225356892

Epoch: 148| Step: 0
Training loss: 2.422111988067627
Validation loss: 2.182004451751709

Epoch: 5| Step: 1
Training loss: 2.1610147953033447
Validation loss: 2.15961350805016

Epoch: 5| Step: 2
Training loss: 1.6359580755233765
Validation loss: 2.165124499669639

Epoch: 5| Step: 3
Training loss: 2.784231185913086
Validation loss: 2.179501889854349

Epoch: 5| Step: 4
Training loss: 2.2598400115966797
Validation loss: 2.2010525990557928

Epoch: 5| Step: 5
Training loss: 2.002297878265381
Validation loss: 2.1954569380770446

Epoch: 5| Step: 6
Training loss: 2.9516959190368652
Validation loss: 2.176408770263836

Epoch: 5| Step: 7
Training loss: 1.9094938039779663
Validation loss: 2.1481274661197456

Epoch: 5| Step: 8
Training loss: 2.6105587482452393
Validation loss: 2.131653954905848

Epoch: 5| Step: 9
Training loss: 2.301912307739258
Validation loss: 2.1132425569718882

Epoch: 5| Step: 10
Training loss: 2.2125535011291504
Validation loss: 2.1221667438425045

Epoch: 149| Step: 0
Training loss: 1.9923045635223389
Validation loss: 2.120761594464702

Epoch: 5| Step: 1
Training loss: 2.078400135040283
Validation loss: 2.122977195247527

Epoch: 5| Step: 2
Training loss: 2.2790348529815674
Validation loss: 2.1392063479269705

Epoch: 5| Step: 3
Training loss: 2.422663688659668
Validation loss: 2.1426350557675926

Epoch: 5| Step: 4
Training loss: 1.5276060104370117
Validation loss: 2.1579731613077144

Epoch: 5| Step: 5
Training loss: 2.6198678016662598
Validation loss: 2.1759012373544837

Epoch: 5| Step: 6
Training loss: 1.989612340927124
Validation loss: 2.1766982911735453

Epoch: 5| Step: 7
Training loss: 2.658163547515869
Validation loss: 2.169973334958476

Epoch: 5| Step: 8
Training loss: 2.628135919570923
Validation loss: 2.170206203255602

Epoch: 5| Step: 9
Training loss: 2.6133198738098145
Validation loss: 2.173775936967583

Epoch: 5| Step: 10
Training loss: 2.201098680496216
Validation loss: 2.1618670660962342

Epoch: 150| Step: 0
Training loss: 1.986405611038208
Validation loss: 2.1520887280023224

Epoch: 5| Step: 1
Training loss: 2.4349708557128906
Validation loss: 2.1426728105032318

Epoch: 5| Step: 2
Training loss: 1.9708244800567627
Validation loss: 2.125817455271239

Epoch: 5| Step: 3
Training loss: 2.135335922241211
Validation loss: 2.124538885649814

Epoch: 5| Step: 4
Training loss: 1.8380149602890015
Validation loss: 2.1262656206725747

Epoch: 5| Step: 5
Training loss: 2.612136125564575
Validation loss: 2.1205119779033046

Epoch: 5| Step: 6
Training loss: 2.4427146911621094
Validation loss: 2.132544284225792

Epoch: 5| Step: 7
Training loss: 2.9584271907806396
Validation loss: 2.1362801944055865

Epoch: 5| Step: 8
Training loss: 2.2282662391662598
Validation loss: 2.149146972164031

Epoch: 5| Step: 9
Training loss: 1.9426075220108032
Validation loss: 2.1538828572919293

Epoch: 5| Step: 10
Training loss: 2.1316733360290527
Validation loss: 2.1725690967293194

Epoch: 151| Step: 0
Training loss: 2.3272502422332764
Validation loss: 2.1701912239033687

Epoch: 5| Step: 1
Training loss: 2.4692180156707764
Validation loss: 2.187349537367462

Epoch: 5| Step: 2
Training loss: 1.9371230602264404
Validation loss: 2.1763445638841197

Epoch: 5| Step: 3
Training loss: 1.7377309799194336
Validation loss: 2.174120969669793

Epoch: 5| Step: 4
Training loss: 2.4031569957733154
Validation loss: 2.187374604645596

Epoch: 5| Step: 5
Training loss: 1.8034851551055908
Validation loss: 2.1947897172743276

Epoch: 5| Step: 6
Training loss: 2.2263450622558594
Validation loss: 2.1869904661691315

Epoch: 5| Step: 7
Training loss: 2.3004794120788574
Validation loss: 2.1903208917187107

Epoch: 5| Step: 8
Training loss: 2.511352062225342
Validation loss: 2.166136359655729

Epoch: 5| Step: 9
Training loss: 2.6805098056793213
Validation loss: 2.1553262690062165

Epoch: 5| Step: 10
Training loss: 2.2085821628570557
Validation loss: 2.146550711765084

Epoch: 152| Step: 0
Training loss: 2.6762280464172363
Validation loss: 2.132714327945504

Epoch: 5| Step: 1
Training loss: 2.661238193511963
Validation loss: 2.1269118990949405

Epoch: 5| Step: 2
Training loss: 1.8264449834823608
Validation loss: 2.119210796971475

Epoch: 5| Step: 3
Training loss: 1.850807785987854
Validation loss: 2.1068672800576813

Epoch: 5| Step: 4
Training loss: 1.9122724533081055
Validation loss: 2.1055011249357656

Epoch: 5| Step: 5
Training loss: 2.6232688426971436
Validation loss: 2.1304758569245696

Epoch: 5| Step: 6
Training loss: 2.104219913482666
Validation loss: 2.151645827037032

Epoch: 5| Step: 7
Training loss: 2.5408103466033936
Validation loss: 2.173844668173021

Epoch: 5| Step: 8
Training loss: 2.0380160808563232
Validation loss: 2.2187888981193624

Epoch: 5| Step: 9
Training loss: 2.4888954162597656
Validation loss: 2.2401609010593866

Epoch: 5| Step: 10
Training loss: 1.8746013641357422
Validation loss: 2.2043222919587167

Epoch: 153| Step: 0
Training loss: 2.18888783454895
Validation loss: 2.167339724879111

Epoch: 5| Step: 1
Training loss: 1.9788072109222412
Validation loss: 2.135818273790421

Epoch: 5| Step: 2
Training loss: 2.37520170211792
Validation loss: 2.1453875264813824

Epoch: 5| Step: 3
Training loss: 2.4822006225585938
Validation loss: 2.1747127886741393

Epoch: 5| Step: 4
Training loss: 2.1376259326934814
Validation loss: 2.178238866149738

Epoch: 5| Step: 5
Training loss: 2.2772016525268555
Validation loss: 2.1697894309156682

Epoch: 5| Step: 6
Training loss: 2.319894790649414
Validation loss: 2.1854400198946715

Epoch: 5| Step: 7
Training loss: 1.5512821674346924
Validation loss: 2.188087870997767

Epoch: 5| Step: 8
Training loss: 2.5863213539123535
Validation loss: 2.2156654224600842

Epoch: 5| Step: 9
Training loss: 2.5305490493774414
Validation loss: 2.2173812517555813

Epoch: 5| Step: 10
Training loss: 2.1801440715789795
Validation loss: 2.183589066228559

Epoch: 154| Step: 0
Training loss: 2.3332443237304688
Validation loss: 2.1565024032387683

Epoch: 5| Step: 1
Training loss: 2.2105422019958496
Validation loss: 2.1471726330377723

Epoch: 5| Step: 2
Training loss: 2.5519700050354004
Validation loss: 2.1413381253519366

Epoch: 5| Step: 3
Training loss: 2.127957344055176
Validation loss: 2.1404500853630806

Epoch: 5| Step: 4
Training loss: 2.124206066131592
Validation loss: 2.14340812929215

Epoch: 5| Step: 5
Training loss: 1.51390540599823
Validation loss: 2.1375568810329644

Epoch: 5| Step: 6
Training loss: 2.408933639526367
Validation loss: 2.153858341196532

Epoch: 5| Step: 7
Training loss: 2.3834738731384277
Validation loss: 2.1612490992392264

Epoch: 5| Step: 8
Training loss: 1.9766654968261719
Validation loss: 2.1587908960157827

Epoch: 5| Step: 9
Training loss: 2.4948270320892334
Validation loss: 2.1580114928624963

Epoch: 5| Step: 10
Training loss: 1.9547935724258423
Validation loss: 2.1683938105901084

Epoch: 155| Step: 0
Training loss: 1.831324577331543
Validation loss: 2.1722199404111473

Epoch: 5| Step: 1
Training loss: 1.742629051208496
Validation loss: 2.1649585090657717

Epoch: 5| Step: 2
Training loss: 2.1894679069519043
Validation loss: 2.146474460119842

Epoch: 5| Step: 3
Training loss: 1.7931079864501953
Validation loss: 2.1411257405434885

Epoch: 5| Step: 4
Training loss: 2.363663673400879
Validation loss: 2.144191441997405

Epoch: 5| Step: 5
Training loss: 2.1297154426574707
Validation loss: 2.1504206311318184

Epoch: 5| Step: 6
Training loss: 2.655972957611084
Validation loss: 2.1626372055340837

Epoch: 5| Step: 7
Training loss: 2.1311659812927246
Validation loss: 2.1864074481430875

Epoch: 5| Step: 8
Training loss: 2.7263967990875244
Validation loss: 2.1698530168943506

Epoch: 5| Step: 9
Training loss: 2.056485414505005
Validation loss: 2.1385634535102436

Epoch: 5| Step: 10
Training loss: 2.400158166885376
Validation loss: 2.152826339967789

Epoch: 156| Step: 0
Training loss: 1.7761462926864624
Validation loss: 2.13817108831098

Epoch: 5| Step: 1
Training loss: 2.2368063926696777
Validation loss: 2.146270543016413

Epoch: 5| Step: 2
Training loss: 1.7536566257476807
Validation loss: 2.148913659075255

Epoch: 5| Step: 3
Training loss: 1.9104785919189453
Validation loss: 2.156635438242266

Epoch: 5| Step: 4
Training loss: 2.2683615684509277
Validation loss: 2.1593387921651206

Epoch: 5| Step: 5
Training loss: 2.478203296661377
Validation loss: 2.164329339099187

Epoch: 5| Step: 6
Training loss: 2.4287614822387695
Validation loss: 2.196024869077949

Epoch: 5| Step: 7
Training loss: 1.821955919265747
Validation loss: 2.219301798010385

Epoch: 5| Step: 8
Training loss: 2.17926287651062
Validation loss: 2.2868976387926327

Epoch: 5| Step: 9
Training loss: 2.5357227325439453
Validation loss: 2.2485089096971738

Epoch: 5| Step: 10
Training loss: 3.012467861175537
Validation loss: 2.224308051088805

Epoch: 157| Step: 0
Training loss: 2.8834762573242188
Validation loss: 2.16715177412956

Epoch: 5| Step: 1
Training loss: 2.17572021484375
Validation loss: 2.1100738317735734

Epoch: 5| Step: 2
Training loss: 2.3459982872009277
Validation loss: 2.0897863141952024

Epoch: 5| Step: 3
Training loss: 2.0556232929229736
Validation loss: 2.081459064637461

Epoch: 5| Step: 4
Training loss: 1.7923848628997803
Validation loss: 2.082545257383777

Epoch: 5| Step: 5
Training loss: 2.753213405609131
Validation loss: 2.0844436127652406

Epoch: 5| Step: 6
Training loss: 1.982398271560669
Validation loss: 2.09241501233911

Epoch: 5| Step: 7
Training loss: 1.4485361576080322
Validation loss: 2.1187420583540395

Epoch: 5| Step: 8
Training loss: 2.210954189300537
Validation loss: 2.141810458193543

Epoch: 5| Step: 9
Training loss: 2.5824999809265137
Validation loss: 2.185398429952642

Epoch: 5| Step: 10
Training loss: 1.838827133178711
Validation loss: 2.203629752641083

Epoch: 158| Step: 0
Training loss: 1.9872963428497314
Validation loss: 2.2294334775658062

Epoch: 5| Step: 1
Training loss: 2.0701630115509033
Validation loss: 2.223870474805114

Epoch: 5| Step: 2
Training loss: 1.8942521810531616
Validation loss: 2.2268375760765484

Epoch: 5| Step: 3
Training loss: 1.8985481262207031
Validation loss: 2.226538209504979

Epoch: 5| Step: 4
Training loss: 2.372168779373169
Validation loss: 2.1911726972108245

Epoch: 5| Step: 5
Training loss: 2.054715633392334
Validation loss: 2.1831584002382014

Epoch: 5| Step: 6
Training loss: 2.3229222297668457
Validation loss: 2.183495003690002

Epoch: 5| Step: 7
Training loss: 2.9277541637420654
Validation loss: 2.179307617166991

Epoch: 5| Step: 8
Training loss: 2.5491557121276855
Validation loss: 2.148211584296278

Epoch: 5| Step: 9
Training loss: 1.6264864206314087
Validation loss: 2.124627426106443

Epoch: 5| Step: 10
Training loss: 2.3211374282836914
Validation loss: 2.087999807891025

Epoch: 159| Step: 0
Training loss: 2.469116687774658
Validation loss: 2.073005069968521

Epoch: 5| Step: 1
Training loss: 2.4032015800476074
Validation loss: 2.0901290729481685

Epoch: 5| Step: 2
Training loss: 2.334653854370117
Validation loss: 2.11223405022775

Epoch: 5| Step: 3
Training loss: 2.273303270339966
Validation loss: 2.148276171376628

Epoch: 5| Step: 4
Training loss: 1.2560685873031616
Validation loss: 2.1889853387750606

Epoch: 5| Step: 5
Training loss: 2.286790132522583
Validation loss: 2.221162147419427

Epoch: 5| Step: 6
Training loss: 1.9917278289794922
Validation loss: 2.2240049813383367

Epoch: 5| Step: 7
Training loss: 2.60317063331604
Validation loss: 2.16769685540148

Epoch: 5| Step: 8
Training loss: 2.085376501083374
Validation loss: 2.119845510810934

Epoch: 5| Step: 9
Training loss: 2.2379796504974365
Validation loss: 2.1155066887537637

Epoch: 5| Step: 10
Training loss: 1.9482511281967163
Validation loss: 2.1204007017997

Epoch: 160| Step: 0
Training loss: 2.4412682056427
Validation loss: 2.131976327588481

Epoch: 5| Step: 1
Training loss: 2.432394027709961
Validation loss: 2.131999067080918

Epoch: 5| Step: 2
Training loss: 2.264845132827759
Validation loss: 2.135992221934821

Epoch: 5| Step: 3
Training loss: 2.349489688873291
Validation loss: 2.136357481761645

Epoch: 5| Step: 4
Training loss: 2.1792025566101074
Validation loss: 2.1396988168839486

Epoch: 5| Step: 5
Training loss: 1.8815720081329346
Validation loss: 2.1476493009956936

Epoch: 5| Step: 6
Training loss: 1.9412330389022827
Validation loss: 2.161944063760901

Epoch: 5| Step: 7
Training loss: 2.5643157958984375
Validation loss: 2.177786716850855

Epoch: 5| Step: 8
Training loss: 2.2215821743011475
Validation loss: 2.1668358079848753

Epoch: 5| Step: 9
Training loss: 2.0763399600982666
Validation loss: 2.173005191228723

Epoch: 5| Step: 10
Training loss: 1.2500149011611938
Validation loss: 2.159172046569086

Epoch: 161| Step: 0
Training loss: 1.9605934619903564
Validation loss: 2.124658371812554

Epoch: 5| Step: 1
Training loss: 2.3416075706481934
Validation loss: 2.107233924250449

Epoch: 5| Step: 2
Training loss: 2.1820080280303955
Validation loss: 2.093887174001304

Epoch: 5| Step: 3
Training loss: 2.3361897468566895
Validation loss: 2.090806822622976

Epoch: 5| Step: 4
Training loss: 3.0268044471740723
Validation loss: 2.082429462863553

Epoch: 5| Step: 5
Training loss: 3.0038962364196777
Validation loss: 2.109247217896164

Epoch: 5| Step: 6
Training loss: 2.1247801780700684
Validation loss: 2.1128904409306024

Epoch: 5| Step: 7
Training loss: 2.3122739791870117
Validation loss: 2.1369686690709924

Epoch: 5| Step: 8
Training loss: 1.0569746494293213
Validation loss: 2.1591836880612116

Epoch: 5| Step: 9
Training loss: 1.153164267539978
Validation loss: 2.2005163008166897

Epoch: 5| Step: 10
Training loss: 2.1253459453582764
Validation loss: 2.2218261777713733

Epoch: 162| Step: 0
Training loss: 2.4120564460754395
Validation loss: 2.1915159379282305

Epoch: 5| Step: 1
Training loss: 2.048640012741089
Validation loss: 2.1636441702483804

Epoch: 5| Step: 2
Training loss: 1.8438301086425781
Validation loss: 2.123817697648079

Epoch: 5| Step: 3
Training loss: 2.763821601867676
Validation loss: 2.1011960660257647

Epoch: 5| Step: 4
Training loss: 2.1346523761749268
Validation loss: 2.0931232924102456

Epoch: 5| Step: 5
Training loss: 2.2013580799102783
Validation loss: 2.1012529455205446

Epoch: 5| Step: 6
Training loss: 2.299107074737549
Validation loss: 2.0901729522212857

Epoch: 5| Step: 7
Training loss: 2.4153411388397217
Validation loss: 2.1082592369407736

Epoch: 5| Step: 8
Training loss: 1.7668107748031616
Validation loss: 2.1102560207407963

Epoch: 5| Step: 9
Training loss: 1.3245254755020142
Validation loss: 2.127308175127993

Epoch: 5| Step: 10
Training loss: 2.221034288406372
Validation loss: 2.1302640412443425

Epoch: 163| Step: 0
Training loss: 1.4225919246673584
Validation loss: 2.1257674232605965

Epoch: 5| Step: 1
Training loss: 2.340581178665161
Validation loss: 2.126675941610849

Epoch: 5| Step: 2
Training loss: 1.8658664226531982
Validation loss: 2.1371851813408638

Epoch: 5| Step: 3
Training loss: 2.5726311206817627
Validation loss: 2.169207908773935

Epoch: 5| Step: 4
Training loss: 2.493067979812622
Validation loss: 2.1829821499445106

Epoch: 5| Step: 5
Training loss: 2.155057430267334
Validation loss: 2.1250429448261055

Epoch: 5| Step: 6
Training loss: 2.7083182334899902
Validation loss: 2.0940929035986624

Epoch: 5| Step: 7
Training loss: 1.7347424030303955
Validation loss: 2.0823765313753517

Epoch: 5| Step: 8
Training loss: 1.4555944204330444
Validation loss: 2.0746732052936347

Epoch: 5| Step: 9
Training loss: 1.9002172946929932
Validation loss: 2.0842100497215026

Epoch: 5| Step: 10
Training loss: 2.529785394668579
Validation loss: 2.100264033963603

Epoch: 164| Step: 0
Training loss: 2.171964168548584
Validation loss: 2.108885544602589

Epoch: 5| Step: 1
Training loss: 2.1510493755340576
Validation loss: 2.1597832889967066

Epoch: 5| Step: 2
Training loss: 1.8621113300323486
Validation loss: 2.203614834816225

Epoch: 5| Step: 3
Training loss: 2.1866822242736816
Validation loss: 2.196519867066414

Epoch: 5| Step: 4
Training loss: 2.2443838119506836
Validation loss: 2.194736406367312

Epoch: 5| Step: 5
Training loss: 2.122952938079834
Validation loss: 2.197182301552065

Epoch: 5| Step: 6
Training loss: 2.016620635986328
Validation loss: 2.151129645685996

Epoch: 5| Step: 7
Training loss: 1.966109037399292
Validation loss: 2.138801138888123

Epoch: 5| Step: 8
Training loss: 2.66766095161438
Validation loss: 2.113040890744937

Epoch: 5| Step: 9
Training loss: 2.0146193504333496
Validation loss: 2.088043761509721

Epoch: 5| Step: 10
Training loss: 2.0468497276306152
Validation loss: 2.0891758190688265

Epoch: 165| Step: 0
Training loss: 2.297271251678467
Validation loss: 2.080234025114326

Epoch: 5| Step: 1
Training loss: 1.7350775003433228
Validation loss: 2.0665224598300074

Epoch: 5| Step: 2
Training loss: 2.572004795074463
Validation loss: 2.0742191601825017

Epoch: 5| Step: 3
Training loss: 1.8017733097076416
Validation loss: 2.099593952137937

Epoch: 5| Step: 4
Training loss: 1.869370698928833
Validation loss: 2.1296031346885105

Epoch: 5| Step: 5
Training loss: 1.6434965133666992
Validation loss: 2.1393510321135163

Epoch: 5| Step: 6
Training loss: 2.303041458129883
Validation loss: 2.1293522363067954

Epoch: 5| Step: 7
Training loss: 2.1347432136535645
Validation loss: 2.1236596492029007

Epoch: 5| Step: 8
Training loss: 2.2174174785614014
Validation loss: 2.137086558085616

Epoch: 5| Step: 9
Training loss: 1.9756486415863037
Validation loss: 2.1388724619342434

Epoch: 5| Step: 10
Training loss: 2.2790443897247314
Validation loss: 2.1400908859827186

Epoch: 166| Step: 0
Training loss: 2.8685240745544434
Validation loss: 2.111461321512858

Epoch: 5| Step: 1
Training loss: 2.1344826221466064
Validation loss: 2.091420991446382

Epoch: 5| Step: 2
Training loss: 2.176614761352539
Validation loss: 2.083459327297826

Epoch: 5| Step: 3
Training loss: 2.246983289718628
Validation loss: 2.092565372426023

Epoch: 5| Step: 4
Training loss: 2.4253973960876465
Validation loss: 2.0870228223903204

Epoch: 5| Step: 5
Training loss: 2.0900464057922363
Validation loss: 2.0804073579849733

Epoch: 5| Step: 6
Training loss: 1.5405819416046143
Validation loss: 2.095357451387631

Epoch: 5| Step: 7
Training loss: 2.228332996368408
Validation loss: 2.0837681319123957

Epoch: 5| Step: 8
Training loss: 1.6979084014892578
Validation loss: 2.0982468717841694

Epoch: 5| Step: 9
Training loss: 1.710430383682251
Validation loss: 2.0945731183534027

Epoch: 5| Step: 10
Training loss: 1.6405848264694214
Validation loss: 2.0991852565478255

Epoch: 167| Step: 0
Training loss: 1.5712828636169434
Validation loss: 2.090352794175507

Epoch: 5| Step: 1
Training loss: 2.1166419982910156
Validation loss: 2.1042026909448768

Epoch: 5| Step: 2
Training loss: 2.5409510135650635
Validation loss: 2.093770983398602

Epoch: 5| Step: 3
Training loss: 1.5635607242584229
Validation loss: 2.0824563246901318

Epoch: 5| Step: 4
Training loss: 1.534959077835083
Validation loss: 2.078869337676674

Epoch: 5| Step: 5
Training loss: 1.6403892040252686
Validation loss: 2.0759940685764438

Epoch: 5| Step: 6
Training loss: 2.6868209838867188
Validation loss: 2.0717167931218303

Epoch: 5| Step: 7
Training loss: 2.1045775413513184
Validation loss: 2.0676647488788893

Epoch: 5| Step: 8
Training loss: 2.7638206481933594
Validation loss: 2.0576788661300496

Epoch: 5| Step: 9
Training loss: 2.745495080947876
Validation loss: 2.072963795354289

Epoch: 5| Step: 10
Training loss: 1.1570297479629517
Validation loss: 2.071759631556849

Epoch: 168| Step: 0
Training loss: 1.9676601886749268
Validation loss: 2.0706786404373827

Epoch: 5| Step: 1
Training loss: 1.6680387258529663
Validation loss: 2.071746145525286

Epoch: 5| Step: 2
Training loss: 2.117924451828003
Validation loss: 2.074025683505561

Epoch: 5| Step: 3
Training loss: 2.382355213165283
Validation loss: 2.0755283012185046

Epoch: 5| Step: 4
Training loss: 2.654177665710449
Validation loss: 2.0899839106426445

Epoch: 5| Step: 5
Training loss: 1.7175588607788086
Validation loss: 2.0977619386488393

Epoch: 5| Step: 6
Training loss: 2.1379051208496094
Validation loss: 2.136898402244814

Epoch: 5| Step: 7
Training loss: 1.9024187326431274
Validation loss: 2.14772863798244

Epoch: 5| Step: 8
Training loss: 2.0161314010620117
Validation loss: 2.123593668783865

Epoch: 5| Step: 9
Training loss: 2.070003032684326
Validation loss: 2.1024815741405694

Epoch: 5| Step: 10
Training loss: 1.8283318281173706
Validation loss: 2.0805439231216267

Epoch: 169| Step: 0
Training loss: 2.627002239227295
Validation loss: 2.0489596295100387

Epoch: 5| Step: 1
Training loss: 1.8730757236480713
Validation loss: 2.0391192077308573

Epoch: 5| Step: 2
Training loss: 1.6560232639312744
Validation loss: 2.0262964335821008

Epoch: 5| Step: 3
Training loss: 2.6309854984283447
Validation loss: 2.0406751171235116

Epoch: 5| Step: 4
Training loss: 1.6908442974090576
Validation loss: 2.058030374588505

Epoch: 5| Step: 5
Training loss: 2.102550983428955
Validation loss: 2.0672628148909538

Epoch: 5| Step: 6
Training loss: 2.398876667022705
Validation loss: 2.070984402010518

Epoch: 5| Step: 7
Training loss: 2.0244174003601074
Validation loss: 2.075293920373404

Epoch: 5| Step: 8
Training loss: 1.8117468357086182
Validation loss: 2.1124136704270557

Epoch: 5| Step: 9
Training loss: 2.164862632751465
Validation loss: 2.2031548202678723

Epoch: 5| Step: 10
Training loss: 1.724410891532898
Validation loss: 2.2446015445134972

Epoch: 170| Step: 0
Training loss: 2.599827289581299
Validation loss: 2.1720630763679423

Epoch: 5| Step: 1
Training loss: 2.0931336879730225
Validation loss: 2.076079078899917

Epoch: 5| Step: 2
Training loss: 1.861708402633667
Validation loss: 2.0662526520349647

Epoch: 5| Step: 3
Training loss: 1.5983259677886963
Validation loss: 2.07649968260078

Epoch: 5| Step: 4
Training loss: 2.639167308807373
Validation loss: 2.095130278218177

Epoch: 5| Step: 5
Training loss: 2.1747658252716064
Validation loss: 2.111665746217133

Epoch: 5| Step: 6
Training loss: 1.8727153539657593
Validation loss: 2.1060495171495663

Epoch: 5| Step: 7
Training loss: 1.8722307682037354
Validation loss: 2.0895103626353766

Epoch: 5| Step: 8
Training loss: 2.299149990081787
Validation loss: 2.096011492513841

Epoch: 5| Step: 9
Training loss: 1.628469467163086
Validation loss: 2.0805356323078112

Epoch: 5| Step: 10
Training loss: 1.8673813343048096
Validation loss: 2.0660314726573166

Epoch: 171| Step: 0
Training loss: 1.4994537830352783
Validation loss: 2.0768970776629705

Epoch: 5| Step: 1
Training loss: 1.6280733346939087
Validation loss: 2.070133445083454

Epoch: 5| Step: 2
Training loss: 1.6596670150756836
Validation loss: 2.102903384034352

Epoch: 5| Step: 3
Training loss: 2.2732980251312256
Validation loss: 2.132981619527263

Epoch: 5| Step: 4
Training loss: 2.482210874557495
Validation loss: 2.147840497314289

Epoch: 5| Step: 5
Training loss: 1.5667734146118164
Validation loss: 2.1573911200287523

Epoch: 5| Step: 6
Training loss: 2.3944334983825684
Validation loss: 2.15332039197286

Epoch: 5| Step: 7
Training loss: 1.841822862625122
Validation loss: 2.1535199380690053

Epoch: 5| Step: 8
Training loss: 2.8558852672576904
Validation loss: 2.150010904958171

Epoch: 5| Step: 9
Training loss: 1.8526455163955688
Validation loss: 2.124097808714836

Epoch: 5| Step: 10
Training loss: 2.390169143676758
Validation loss: 2.1009739906557146

Epoch: 172| Step: 0
Training loss: 1.7790149450302124
Validation loss: 2.0826236791508173

Epoch: 5| Step: 1
Training loss: 1.688686728477478
Validation loss: 2.062291859298624

Epoch: 5| Step: 2
Training loss: 2.478966236114502
Validation loss: 2.070152623679048

Epoch: 5| Step: 3
Training loss: 1.9250104427337646
Validation loss: 2.0854177449339177

Epoch: 5| Step: 4
Training loss: 1.5704948902130127
Validation loss: 2.1070697256313857

Epoch: 5| Step: 5
Training loss: 2.493025541305542
Validation loss: 2.1016344293471305

Epoch: 5| Step: 6
Training loss: 2.8414416313171387
Validation loss: 2.089564346498059

Epoch: 5| Step: 7
Training loss: 1.835852026939392
Validation loss: 2.090478863767398

Epoch: 5| Step: 8
Training loss: 2.5201616287231445
Validation loss: 2.0785840621558567

Epoch: 5| Step: 9
Training loss: 1.592712640762329
Validation loss: 2.0485548357809744

Epoch: 5| Step: 10
Training loss: 1.5388097763061523
Validation loss: 2.0424516918838664

Epoch: 173| Step: 0
Training loss: 2.2888317108154297
Validation loss: 2.0317766486957507

Epoch: 5| Step: 1
Training loss: 1.305452585220337
Validation loss: 2.038641946290129

Epoch: 5| Step: 2
Training loss: 2.7730751037597656
Validation loss: 2.047804714531027

Epoch: 5| Step: 3
Training loss: 2.091578960418701
Validation loss: 2.0710462716317948

Epoch: 5| Step: 4
Training loss: 2.0356876850128174
Validation loss: 2.098378530112646

Epoch: 5| Step: 5
Training loss: 1.6364704370498657
Validation loss: 2.1129672604222454

Epoch: 5| Step: 6
Training loss: 1.8815853595733643
Validation loss: 2.1243422954313216

Epoch: 5| Step: 7
Training loss: 1.7724215984344482
Validation loss: 2.108447273572286

Epoch: 5| Step: 8
Training loss: 1.8922672271728516
Validation loss: 2.122221487824635

Epoch: 5| Step: 9
Training loss: 2.2802252769470215
Validation loss: 2.0973383175429476

Epoch: 5| Step: 10
Training loss: 1.9878023862838745
Validation loss: 2.0825462149035547

Epoch: 174| Step: 0
Training loss: 1.604680061340332
Validation loss: 2.0528181188849994

Epoch: 5| Step: 1
Training loss: 2.1964364051818848
Validation loss: 2.0451279776070708

Epoch: 5| Step: 2
Training loss: 2.1435463428497314
Validation loss: 2.0678728703529603

Epoch: 5| Step: 3
Training loss: 2.0470407009124756
Validation loss: 2.069310857403663

Epoch: 5| Step: 4
Training loss: 1.9061161279678345
Validation loss: 2.073256333669027

Epoch: 5| Step: 5
Training loss: 2.023404598236084
Validation loss: 2.0830302264100764

Epoch: 5| Step: 6
Training loss: 1.666290044784546
Validation loss: 2.1056786224406254

Epoch: 5| Step: 7
Training loss: 2.1630232334136963
Validation loss: 2.1431047480593444

Epoch: 5| Step: 8
Training loss: 1.9185903072357178
Validation loss: 2.1497700214385986

Epoch: 5| Step: 9
Training loss: 2.12064790725708
Validation loss: 2.1638713446996545

Epoch: 5| Step: 10
Training loss: 2.260082721710205
Validation loss: 2.148659747133973

Epoch: 175| Step: 0
Training loss: 1.7943624258041382
Validation loss: 2.1287889813864105

Epoch: 5| Step: 1
Training loss: 1.7372772693634033
Validation loss: 2.130000909169515

Epoch: 5| Step: 2
Training loss: 2.451185941696167
Validation loss: 2.141178669468049

Epoch: 5| Step: 3
Training loss: 2.292525053024292
Validation loss: 2.1063216065847747

Epoch: 5| Step: 4
Training loss: 2.1292855739593506
Validation loss: 2.0843900890760523

Epoch: 5| Step: 5
Training loss: 1.3789653778076172
Validation loss: 2.0626698027374926

Epoch: 5| Step: 6
Training loss: 1.9317413568496704
Validation loss: 2.0810707038448704

Epoch: 5| Step: 7
Training loss: 2.0976810455322266
Validation loss: 2.1415479490833897

Epoch: 5| Step: 8
Training loss: 2.357898235321045
Validation loss: 2.1773773752233034

Epoch: 5| Step: 9
Training loss: 2.1476078033447266
Validation loss: 2.142660638337494

Epoch: 5| Step: 10
Training loss: 1.7069180011749268
Validation loss: 2.078299883873232

Epoch: 176| Step: 0
Training loss: 2.422934055328369
Validation loss: 2.0379112330816125

Epoch: 5| Step: 1
Training loss: 1.3769487142562866
Validation loss: 2.045802595794842

Epoch: 5| Step: 2
Training loss: 2.0338778495788574
Validation loss: 2.0487533551390453

Epoch: 5| Step: 3
Training loss: 2.0947983264923096
Validation loss: 2.0728970791703913

Epoch: 5| Step: 4
Training loss: 1.7259571552276611
Validation loss: 2.100895579143237

Epoch: 5| Step: 5
Training loss: 2.3521811962127686
Validation loss: 2.11625599861145

Epoch: 5| Step: 6
Training loss: 2.2333242893218994
Validation loss: 2.1319385369618735

Epoch: 5| Step: 7
Training loss: 2.0226569175720215
Validation loss: 2.166721419621539

Epoch: 5| Step: 8
Training loss: 2.0149178504943848
Validation loss: 2.1753151211687314

Epoch: 5| Step: 9
Training loss: 1.602861762046814
Validation loss: 2.206664134097356

Epoch: 5| Step: 10
Training loss: 2.0263864994049072
Validation loss: 2.201704353414556

Epoch: 177| Step: 0
Training loss: 2.440415620803833
Validation loss: 2.153325851245593

Epoch: 5| Step: 1
Training loss: 1.575998067855835
Validation loss: 2.11950207525684

Epoch: 5| Step: 2
Training loss: 2.027975082397461
Validation loss: 2.102830010075723

Epoch: 5| Step: 3
Training loss: 2.230421304702759
Validation loss: 2.080821587193397

Epoch: 5| Step: 4
Training loss: 2.4524266719818115
Validation loss: 2.078074477052176

Epoch: 5| Step: 5
Training loss: 1.975563406944275
Validation loss: 2.082884680840277

Epoch: 5| Step: 6
Training loss: 1.240852952003479
Validation loss: 2.0822348338301464

Epoch: 5| Step: 7
Training loss: 2.184119701385498
Validation loss: 2.0845088715194375

Epoch: 5| Step: 8
Training loss: 1.3728303909301758
Validation loss: 2.0855166783896824

Epoch: 5| Step: 9
Training loss: 2.237358808517456
Validation loss: 2.082639814704977

Epoch: 5| Step: 10
Training loss: 1.568459391593933
Validation loss: 2.0874213300725466

Epoch: 178| Step: 0
Training loss: 1.3524816036224365
Validation loss: 2.103784635502805

Epoch: 5| Step: 1
Training loss: 1.9806455373764038
Validation loss: 2.1228323815971293

Epoch: 5| Step: 2
Training loss: 2.166649341583252
Validation loss: 2.1664518284541305

Epoch: 5| Step: 3
Training loss: 1.8264102935791016
Validation loss: 2.1644428724883706

Epoch: 5| Step: 4
Training loss: 1.561070203781128
Validation loss: 2.1621803365727907

Epoch: 5| Step: 5
Training loss: 1.3376127481460571
Validation loss: 2.1219979434885006

Epoch: 5| Step: 6
Training loss: 2.486889362335205
Validation loss: 2.0864350488108974

Epoch: 5| Step: 7
Training loss: 2.2503788471221924
Validation loss: 2.0671198060435634

Epoch: 5| Step: 8
Training loss: 1.590571641921997
Validation loss: 2.067431470399262

Epoch: 5| Step: 9
Training loss: 2.1961677074432373
Validation loss: 2.062564460180139

Epoch: 5| Step: 10
Training loss: 2.6816539764404297
Validation loss: 2.105778340370424

Epoch: 179| Step: 0
Training loss: 1.6683318614959717
Validation loss: 2.105372751912763

Epoch: 5| Step: 1
Training loss: 2.1939237117767334
Validation loss: 2.099286658789522

Epoch: 5| Step: 2
Training loss: 2.4456965923309326
Validation loss: 2.0836289492986535

Epoch: 5| Step: 3
Training loss: 2.0747392177581787
Validation loss: 2.0552069628110496

Epoch: 5| Step: 4
Training loss: 1.6510655879974365
Validation loss: 2.0616835214758433

Epoch: 5| Step: 5
Training loss: 2.299398899078369
Validation loss: 2.084123375595257

Epoch: 5| Step: 6
Training loss: 1.5243204832077026
Validation loss: 2.0888148302673013

Epoch: 5| Step: 7
Training loss: 2.5458884239196777
Validation loss: 2.117263683708765

Epoch: 5| Step: 8
Training loss: 1.47037672996521
Validation loss: 2.1176638859574513

Epoch: 5| Step: 9
Training loss: 1.866253137588501
Validation loss: 2.1315024847625406

Epoch: 5| Step: 10
Training loss: 1.3686566352844238
Validation loss: 2.1607527784121934

Epoch: 180| Step: 0
Training loss: 2.085939407348633
Validation loss: 2.1789161236055437

Epoch: 5| Step: 1
Training loss: 2.150331974029541
Validation loss: 2.177495038637551

Epoch: 5| Step: 2
Training loss: 1.9061874151229858
Validation loss: 2.134013054191425

Epoch: 5| Step: 3
Training loss: 1.5900264978408813
Validation loss: 2.137283355959

Epoch: 5| Step: 4
Training loss: 2.040009021759033
Validation loss: 2.1050085649695447

Epoch: 5| Step: 5
Training loss: 1.8062078952789307
Validation loss: 2.0961073880554526

Epoch: 5| Step: 6
Training loss: 1.269601583480835
Validation loss: 2.09132743138139

Epoch: 5| Step: 7
Training loss: 2.1716814041137695
Validation loss: 2.0887615783240205

Epoch: 5| Step: 8
Training loss: 2.2807118892669678
Validation loss: 2.1092555330645655

Epoch: 5| Step: 9
Training loss: 2.279204845428467
Validation loss: 2.126415060412499

Epoch: 5| Step: 10
Training loss: 1.5373294353485107
Validation loss: 2.1550673105383433

Epoch: 181| Step: 0
Training loss: 1.6259677410125732
Validation loss: 2.146232612671391

Epoch: 5| Step: 1
Training loss: 1.7159219980239868
Validation loss: 2.1195902337310133

Epoch: 5| Step: 2
Training loss: 1.444917917251587
Validation loss: 2.112157221763365

Epoch: 5| Step: 3
Training loss: 2.2086453437805176
Validation loss: 2.114506079304603

Epoch: 5| Step: 4
Training loss: 2.0786757469177246
Validation loss: 2.1198673479018675

Epoch: 5| Step: 5
Training loss: 1.698054552078247
Validation loss: 2.101185239771361

Epoch: 5| Step: 6
Training loss: 2.033236503601074
Validation loss: 2.1262377718443513

Epoch: 5| Step: 7
Training loss: 2.656280994415283
Validation loss: 2.1298965177228375

Epoch: 5| Step: 8
Training loss: 2.043900966644287
Validation loss: 2.128855014360079

Epoch: 5| Step: 9
Training loss: 1.8386809825897217
Validation loss: 2.1568963322588193

Epoch: 5| Step: 10
Training loss: 1.5713878870010376
Validation loss: 2.184216796710927

Epoch: 182| Step: 0
Training loss: 1.723077416419983
Validation loss: 2.146317232039667

Epoch: 5| Step: 1
Training loss: 1.9180080890655518
Validation loss: 2.1361047837042038

Epoch: 5| Step: 2
Training loss: 1.9412453174591064
Validation loss: 2.111326059987468

Epoch: 5| Step: 3
Training loss: 1.7767359018325806
Validation loss: 2.087413380222936

Epoch: 5| Step: 4
Training loss: 1.9973310232162476
Validation loss: 2.076136049403939

Epoch: 5| Step: 5
Training loss: 1.4938702583312988
Validation loss: 2.0836008389790854

Epoch: 5| Step: 6
Training loss: 2.040283203125
Validation loss: 2.0887111233126734

Epoch: 5| Step: 7
Training loss: 1.347114086151123
Validation loss: 2.0980279086738505

Epoch: 5| Step: 8
Training loss: 2.2408182621002197
Validation loss: 2.1125886901732414

Epoch: 5| Step: 9
Training loss: 2.400723695755005
Validation loss: 2.152443898621426

Epoch: 5| Step: 10
Training loss: 2.0218610763549805
Validation loss: 2.1981691801419823

Epoch: 183| Step: 0
Training loss: 1.84531569480896
Validation loss: 2.260195298861432

Epoch: 5| Step: 1
Training loss: 1.9015045166015625
Validation loss: 2.2812573115030923

Epoch: 5| Step: 2
Training loss: 2.2323999404907227
Validation loss: 2.2809733665117653

Epoch: 5| Step: 3
Training loss: 2.291071653366089
Validation loss: 2.170920107954292

Epoch: 5| Step: 4
Training loss: 1.9368728399276733
Validation loss: 2.100043067368128

Epoch: 5| Step: 5
Training loss: 1.8134711980819702
Validation loss: 2.0846340361461846

Epoch: 5| Step: 6
Training loss: 1.5878775119781494
Validation loss: 2.079367893998341

Epoch: 5| Step: 7
Training loss: 2.1281580924987793
Validation loss: 2.071072457939066

Epoch: 5| Step: 8
Training loss: 2.096695899963379
Validation loss: 2.100963589965656

Epoch: 5| Step: 9
Training loss: 1.9932416677474976
Validation loss: 2.1102003038570447

Epoch: 5| Step: 10
Training loss: 1.501376748085022
Validation loss: 2.128817125033307

Epoch: 184| Step: 0
Training loss: 1.6211519241333008
Validation loss: 2.151701860530402

Epoch: 5| Step: 1
Training loss: 1.9669933319091797
Validation loss: 2.18262622433324

Epoch: 5| Step: 2
Training loss: 2.3145573139190674
Validation loss: 2.1918205291994157

Epoch: 5| Step: 3
Training loss: 1.4694629907608032
Validation loss: 2.157890022441905

Epoch: 5| Step: 4
Training loss: 1.5986703634262085
Validation loss: 2.113038568086522

Epoch: 5| Step: 5
Training loss: 1.945788025856018
Validation loss: 2.08303560749177

Epoch: 5| Step: 6
Training loss: 1.8004024028778076
Validation loss: 2.048090583534651

Epoch: 5| Step: 7
Training loss: 1.9188802242279053
Validation loss: 2.041013499741913

Epoch: 5| Step: 8
Training loss: 2.1949002742767334
Validation loss: 2.0418782721283617

Epoch: 5| Step: 9
Training loss: 2.184509038925171
Validation loss: 2.0333924524245726

Epoch: 5| Step: 10
Training loss: 1.6939570903778076
Validation loss: 2.0498083086423975

Epoch: 185| Step: 0
Training loss: 2.0229620933532715
Validation loss: 2.0498826478117254

Epoch: 5| Step: 1
Training loss: 2.02432918548584
Validation loss: 2.0942300929818103

Epoch: 5| Step: 2
Training loss: 1.9331741333007812
Validation loss: 2.105248994724725

Epoch: 5| Step: 3
Training loss: 1.8079090118408203
Validation loss: 2.1219806696778987

Epoch: 5| Step: 4
Training loss: 1.4025553464889526
Validation loss: 2.11502573310688

Epoch: 5| Step: 5
Training loss: 2.0918288230895996
Validation loss: 2.101808117282006

Epoch: 5| Step: 6
Training loss: 1.7550767660140991
Validation loss: 2.101589734836291

Epoch: 5| Step: 7
Training loss: 1.8183796405792236
Validation loss: 2.093811099247266

Epoch: 5| Step: 8
Training loss: 1.5877025127410889
Validation loss: 2.0854219211045133

Epoch: 5| Step: 9
Training loss: 2.0380005836486816
Validation loss: 2.101775277045465

Epoch: 5| Step: 10
Training loss: 2.135336399078369
Validation loss: 2.1128935839540217

Epoch: 186| Step: 0
Training loss: 1.3484925031661987
Validation loss: 2.113867054703415

Epoch: 5| Step: 1
Training loss: 1.683953046798706
Validation loss: 2.1298708492709744

Epoch: 5| Step: 2
Training loss: 1.3302032947540283
Validation loss: 2.132890260347756

Epoch: 5| Step: 3
Training loss: 1.6685937643051147
Validation loss: 2.130344424196469

Epoch: 5| Step: 4
Training loss: 2.189312219619751
Validation loss: 2.120083453834698

Epoch: 5| Step: 5
Training loss: 1.9053850173950195
Validation loss: 2.1039982764951644

Epoch: 5| Step: 6
Training loss: 1.8589534759521484
Validation loss: 2.090230531589959

Epoch: 5| Step: 7
Training loss: 2.0141940116882324
Validation loss: 2.1170040458761235

Epoch: 5| Step: 8
Training loss: 2.5559043884277344
Validation loss: 2.1134933271715717

Epoch: 5| Step: 9
Training loss: 1.8198330402374268
Validation loss: 2.1225929772982033

Epoch: 5| Step: 10
Training loss: 1.7277084589004517
Validation loss: 2.117734736011874

Epoch: 187| Step: 0
Training loss: 2.459813117980957
Validation loss: 2.114173363613826

Epoch: 5| Step: 1
Training loss: 1.7453334331512451
Validation loss: 2.1271531402423816

Epoch: 5| Step: 2
Training loss: 1.982132911682129
Validation loss: 2.1119984247351207

Epoch: 5| Step: 3
Training loss: 1.734686255455017
Validation loss: 2.1117309947167673

Epoch: 5| Step: 4
Training loss: 1.6043437719345093
Validation loss: 2.102453775303338

Epoch: 5| Step: 5
Training loss: 1.8689380884170532
Validation loss: 2.107079782793599

Epoch: 5| Step: 6
Training loss: 1.7783912420272827
Validation loss: 2.130779109975343

Epoch: 5| Step: 7
Training loss: 1.7219536304473877
Validation loss: 2.1702844699223838

Epoch: 5| Step: 8
Training loss: 1.4429352283477783
Validation loss: 2.190854077698082

Epoch: 5| Step: 9
Training loss: 1.1604783535003662
Validation loss: 2.2141817769696637

Epoch: 5| Step: 10
Training loss: 2.5792336463928223
Validation loss: 2.1581222562379736

Epoch: 188| Step: 0
Training loss: 2.0062012672424316
Validation loss: 2.1955962988638107

Epoch: 5| Step: 1
Training loss: 2.2583649158477783
Validation loss: 2.2041569602104927

Epoch: 5| Step: 2
Training loss: 1.891437292098999
Validation loss: 2.139999046120592

Epoch: 5| Step: 3
Training loss: 1.428207516670227
Validation loss: 2.1201765229625087

Epoch: 5| Step: 4
Training loss: 2.114631175994873
Validation loss: 2.126035735171328

Epoch: 5| Step: 5
Training loss: 2.1316170692443848
Validation loss: 2.1589310707584506

Epoch: 5| Step: 6
Training loss: 1.5752395391464233
Validation loss: 2.196157650281024

Epoch: 5| Step: 7
Training loss: 1.6853796243667603
Validation loss: 2.1867648452840824

Epoch: 5| Step: 8
Training loss: 1.3930269479751587
Validation loss: 2.1929031738670925

Epoch: 5| Step: 9
Training loss: 1.6820497512817383
Validation loss: 2.150521093799222

Epoch: 5| Step: 10
Training loss: 2.0429341793060303
Validation loss: 2.113146589648339

Epoch: 189| Step: 0
Training loss: 1.9240341186523438
Validation loss: 2.085660840875359

Epoch: 5| Step: 1
Training loss: 1.9643936157226562
Validation loss: 2.0671404497597807

Epoch: 5| Step: 2
Training loss: 1.968775749206543
Validation loss: 2.0705890168425856

Epoch: 5| Step: 3
Training loss: 2.1848738193511963
Validation loss: 2.070166926230154

Epoch: 5| Step: 4
Training loss: 2.125671625137329
Validation loss: 2.088778143287987

Epoch: 5| Step: 5
Training loss: 1.5359833240509033
Validation loss: 2.102898492608019

Epoch: 5| Step: 6
Training loss: 1.4138715267181396
Validation loss: 2.115236497694446

Epoch: 5| Step: 7
Training loss: 1.2865654230117798
Validation loss: 2.1386247399032756

Epoch: 5| Step: 8
Training loss: 1.5713380575180054
Validation loss: 2.1180256669239332

Epoch: 5| Step: 9
Training loss: 2.070495128631592
Validation loss: 2.0803904674386464

Epoch: 5| Step: 10
Training loss: 1.9484764337539673
Validation loss: 2.0761652697799025

Epoch: 190| Step: 0
Training loss: 1.520179033279419
Validation loss: 2.0939774846517913

Epoch: 5| Step: 1
Training loss: 2.121843099594116
Validation loss: 2.154152104931493

Epoch: 5| Step: 2
Training loss: 2.204577922821045
Validation loss: 2.136859711780343

Epoch: 5| Step: 3
Training loss: 1.261949896812439
Validation loss: 2.1576203684653006

Epoch: 5| Step: 4
Training loss: 1.7912952899932861
Validation loss: 2.1836379446009153

Epoch: 5| Step: 5
Training loss: 1.722170114517212
Validation loss: 2.2275993747095906

Epoch: 5| Step: 6
Training loss: 1.6431105136871338
Validation loss: 2.2513570195885113

Epoch: 5| Step: 7
Training loss: 1.8906548023223877
Validation loss: 2.2490025720288678

Epoch: 5| Step: 8
Training loss: 1.8202638626098633
Validation loss: 2.176543435742778

Epoch: 5| Step: 9
Training loss: 1.799727201461792
Validation loss: 2.099854753863427

Epoch: 5| Step: 10
Training loss: 2.226529359817505
Validation loss: 2.0718087175840973

Epoch: 191| Step: 0
Training loss: 1.8472788333892822
Validation loss: 2.093017008996779

Epoch: 5| Step: 1
Training loss: 2.3351895809173584
Validation loss: 2.0934656537989134

Epoch: 5| Step: 2
Training loss: 2.13450026512146
Validation loss: 2.105643274963543

Epoch: 5| Step: 3
Training loss: 2.5947105884552
Validation loss: 2.099452082828809

Epoch: 5| Step: 4
Training loss: 1.6995868682861328
Validation loss: 2.12790661217064

Epoch: 5| Step: 5
Training loss: 1.5972596406936646
Validation loss: 2.183617015038767

Epoch: 5| Step: 6
Training loss: 1.2263211011886597
Validation loss: 2.2308512759465042

Epoch: 5| Step: 7
Training loss: 1.712040662765503
Validation loss: 2.285286177871048

Epoch: 5| Step: 8
Training loss: 1.5719501972198486
Validation loss: 2.2933753831412202

Epoch: 5| Step: 9
Training loss: 1.7459148168563843
Validation loss: 2.2716406686331636

Epoch: 5| Step: 10
Training loss: 2.0160598754882812
Validation loss: 2.2308995518633115

Epoch: 192| Step: 0
Training loss: 1.5508767366409302
Validation loss: 2.166834892765168

Epoch: 5| Step: 1
Training loss: 1.5831466913223267
Validation loss: 2.1520205518250823

Epoch: 5| Step: 2
Training loss: 2.2236952781677246
Validation loss: 2.14008996563573

Epoch: 5| Step: 3
Training loss: 1.8260447978973389
Validation loss: 2.159483745533933

Epoch: 5| Step: 4
Training loss: 1.6794803142547607
Validation loss: 2.1431417593391995

Epoch: 5| Step: 5
Training loss: 1.4875993728637695
Validation loss: 2.1235078611681537

Epoch: 5| Step: 6
Training loss: 2.207881450653076
Validation loss: 2.1320731806498703

Epoch: 5| Step: 7
Training loss: 1.9744594097137451
Validation loss: 2.1591259638468423

Epoch: 5| Step: 8
Training loss: 1.4960530996322632
Validation loss: 2.2496842158738004

Epoch: 5| Step: 9
Training loss: 2.3946850299835205
Validation loss: 2.337264815966288

Epoch: 5| Step: 10
Training loss: 1.6851165294647217
Validation loss: 2.3365136987419537

Epoch: 193| Step: 0
Training loss: 1.9673534631729126
Validation loss: 2.3313866020530782

Epoch: 5| Step: 1
Training loss: 1.5249629020690918
Validation loss: 2.266639950454876

Epoch: 5| Step: 2
Training loss: 1.2736986875534058
Validation loss: 2.226841518955846

Epoch: 5| Step: 3
Training loss: 1.8026376962661743
Validation loss: 2.2048692036700506

Epoch: 5| Step: 4
Training loss: 1.880446195602417
Validation loss: 2.1966615851207445

Epoch: 5| Step: 5
Training loss: 2.1071674823760986
Validation loss: 2.189155593995125

Epoch: 5| Step: 6
Training loss: 2.295151472091675
Validation loss: 2.185191992790468

Epoch: 5| Step: 7
Training loss: 1.7998807430267334
Validation loss: 2.174386910212937

Epoch: 5| Step: 8
Training loss: 1.6057918071746826
Validation loss: 2.1238653198365243

Epoch: 5| Step: 9
Training loss: 2.0923855304718018
Validation loss: 2.1340401595638645

Epoch: 5| Step: 10
Training loss: 1.5631647109985352
Validation loss: 2.161172605329944

Epoch: 194| Step: 0
Training loss: 1.8655064105987549
Validation loss: 2.2605988197429205

Epoch: 5| Step: 1
Training loss: 2.1134111881256104
Validation loss: 2.3185373301147134

Epoch: 5| Step: 2
Training loss: 1.8955036401748657
Validation loss: 2.3379416465759277

Epoch: 5| Step: 3
Training loss: 2.0692102909088135
Validation loss: 2.288867700484491

Epoch: 5| Step: 4
Training loss: 1.502959966659546
Validation loss: 2.2193697216690227

Epoch: 5| Step: 5
Training loss: 1.3963171243667603
Validation loss: 2.157505364828212

Epoch: 5| Step: 6
Training loss: 2.177269458770752
Validation loss: 2.124011401207216

Epoch: 5| Step: 7
Training loss: 1.9103052616119385
Validation loss: 2.1419686476389566

Epoch: 5| Step: 8
Training loss: 2.1373984813690186
Validation loss: 2.1537302694013043

Epoch: 5| Step: 9
Training loss: 1.466834306716919
Validation loss: 2.1725380574503252

Epoch: 5| Step: 10
Training loss: 1.4505451917648315
Validation loss: 2.1792805963946926

Epoch: 195| Step: 0
Training loss: 1.8069303035736084
Validation loss: 2.2101767191322903

Epoch: 5| Step: 1
Training loss: 2.054987668991089
Validation loss: 2.2086837304535734

Epoch: 5| Step: 2
Training loss: 1.0982195138931274
Validation loss: 2.207569879870261

Epoch: 5| Step: 3
Training loss: 1.5961650609970093
Validation loss: 2.20927708507866

Epoch: 5| Step: 4
Training loss: 1.0716476440429688
Validation loss: 2.194982967069072

Epoch: 5| Step: 5
Training loss: 2.448267698287964
Validation loss: 2.2007197487738823

Epoch: 5| Step: 6
Training loss: 0.8245059847831726
Validation loss: 2.217629699296849

Epoch: 5| Step: 7
Training loss: 1.721613883972168
Validation loss: 2.246832832213371

Epoch: 5| Step: 8
Training loss: 1.8647282123565674
Validation loss: 2.278824565231159

Epoch: 5| Step: 9
Training loss: 2.6290929317474365
Validation loss: 2.219230795419344

Epoch: 5| Step: 10
Training loss: 2.351391553878784
Validation loss: 2.157919465854604

Epoch: 196| Step: 0
Training loss: 1.7565504312515259
Validation loss: 2.1383742606768044

Epoch: 5| Step: 1
Training loss: 1.408348560333252
Validation loss: 2.145115262718611

Epoch: 5| Step: 2
Training loss: 1.8371708393096924
Validation loss: 2.1251159201386156

Epoch: 5| Step: 3
Training loss: 1.1986936330795288
Validation loss: 2.15503143366947

Epoch: 5| Step: 4
Training loss: 1.9497638940811157
Validation loss: 2.2125273737856137

Epoch: 5| Step: 5
Training loss: 1.7880535125732422
Validation loss: 2.257321339781566

Epoch: 5| Step: 6
Training loss: 2.2819271087646484
Validation loss: 2.278047518063617

Epoch: 5| Step: 7
Training loss: 1.5938246250152588
Validation loss: 2.2875197318292435

Epoch: 5| Step: 8
Training loss: 2.3923516273498535
Validation loss: 2.235564984301085

Epoch: 5| Step: 9
Training loss: 1.4510548114776611
Validation loss: 2.186065914810345

Epoch: 5| Step: 10
Training loss: 1.6590590476989746
Validation loss: 2.1510881095804195

Epoch: 197| Step: 0
Training loss: 1.8140462636947632
Validation loss: 2.0936888981890935

Epoch: 5| Step: 1
Training loss: 1.6620553731918335
Validation loss: 2.0988635837390857

Epoch: 5| Step: 2
Training loss: 2.097121477127075
Validation loss: 2.0797224980528637

Epoch: 5| Step: 3
Training loss: 1.1467578411102295
Validation loss: 2.0637612240288847

Epoch: 5| Step: 4
Training loss: 2.4920496940612793
Validation loss: 2.0870393578724196

Epoch: 5| Step: 5
Training loss: 1.760939359664917
Validation loss: 2.1207480353693806

Epoch: 5| Step: 6
Training loss: 1.761682152748108
Validation loss: 2.1300359464460805

Epoch: 5| Step: 7
Training loss: 1.582432508468628
Validation loss: 2.1324877328770135

Epoch: 5| Step: 8
Training loss: 1.7662912607192993
Validation loss: 2.1253906590964204

Epoch: 5| Step: 9
Training loss: 1.8999483585357666
Validation loss: 2.126599665611021

Epoch: 5| Step: 10
Training loss: 1.0498366355895996
Validation loss: 2.137212049576544

Epoch: 198| Step: 0
Training loss: 1.7209179401397705
Validation loss: 2.1404403435286654

Epoch: 5| Step: 1
Training loss: 1.9980268478393555
Validation loss: 2.132850262426561

Epoch: 5| Step: 2
Training loss: 1.0941553115844727
Validation loss: 2.130182117544195

Epoch: 5| Step: 3
Training loss: 1.6735813617706299
Validation loss: 2.095915861027215

Epoch: 5| Step: 4
Training loss: 1.5141401290893555
Validation loss: 2.109874674068984

Epoch: 5| Step: 5
Training loss: 1.620499610900879
Validation loss: 2.128636549877864

Epoch: 5| Step: 6
Training loss: 1.8468443155288696
Validation loss: 2.1579673444071124

Epoch: 5| Step: 7
Training loss: 1.5901298522949219
Validation loss: 2.178103485415059

Epoch: 5| Step: 8
Training loss: 1.9074528217315674
Validation loss: 2.1980154283585085

Epoch: 5| Step: 9
Training loss: 1.919952630996704
Validation loss: 2.244566180372751

Epoch: 5| Step: 10
Training loss: 2.015080213546753
Validation loss: 2.221369611319675

Epoch: 199| Step: 0
Training loss: 1.9004061222076416
Validation loss: 2.203346155023062

Epoch: 5| Step: 1
Training loss: 1.5316908359527588
Validation loss: 2.205844115185481

Epoch: 5| Step: 2
Training loss: 2.0508151054382324
Validation loss: 2.198800144657012

Epoch: 5| Step: 3
Training loss: 1.7411683797836304
Validation loss: 2.197391176736483

Epoch: 5| Step: 4
Training loss: 1.4111278057098389
Validation loss: 2.201432422925067

Epoch: 5| Step: 5
Training loss: 1.8822933435440063
Validation loss: 2.18099731527349

Epoch: 5| Step: 6
Training loss: 1.523197054862976
Validation loss: 2.173744998952394

Epoch: 5| Step: 7
Training loss: 0.8872379064559937
Validation loss: 2.166927218437195

Epoch: 5| Step: 8
Training loss: 1.7006508111953735
Validation loss: 2.1824766025748303

Epoch: 5| Step: 9
Training loss: 2.024730682373047
Validation loss: 2.1905367733329855

Epoch: 5| Step: 10
Training loss: 1.8153772354125977
Validation loss: 2.226478979151736

Epoch: 200| Step: 0
Training loss: 1.7142677307128906
Validation loss: 2.238225381861451

Epoch: 5| Step: 1
Training loss: 1.7322505712509155
Validation loss: 2.2060229368107294

Epoch: 5| Step: 2
Training loss: 1.5823090076446533
Validation loss: 2.2288417687980075

Epoch: 5| Step: 3
Training loss: 1.4035165309906006
Validation loss: 2.210352743825605

Epoch: 5| Step: 4
Training loss: 2.1190900802612305
Validation loss: 2.18387080777076

Epoch: 5| Step: 5
Training loss: 1.7961254119873047
Validation loss: 2.1700963820180585

Epoch: 5| Step: 6
Training loss: 1.1253011226654053
Validation loss: 2.1503289668790755

Epoch: 5| Step: 7
Training loss: 1.8871889114379883
Validation loss: 2.1709831068592687

Epoch: 5| Step: 8
Training loss: 1.5357660055160522
Validation loss: 2.1655455071439027

Epoch: 5| Step: 9
Training loss: 1.7302675247192383
Validation loss: 2.164907491335305

Epoch: 5| Step: 10
Training loss: 1.7437326908111572
Validation loss: 2.1532490637994584

Testing loss: 2.3049210839801364

Epoch: 1| Step: 0
Training loss: 5.373035430908203
Validation loss: 5.127353252903108

Epoch: 6| Step: 1
Training loss: 6.220240592956543
Validation loss: 5.113957974218553

Epoch: 6| Step: 2
Training loss: 3.597050905227661
Validation loss: 5.105952749970139

Epoch: 6| Step: 3
Training loss: 5.27488899230957
Validation loss: 5.098789322760798

Epoch: 6| Step: 4
Training loss: 4.9994001388549805
Validation loss: 5.091257669592417

Epoch: 6| Step: 5
Training loss: 5.520089149475098
Validation loss: 5.08282289197368

Epoch: 6| Step: 6
Training loss: 4.17060661315918
Validation loss: 5.073336450002527

Epoch: 6| Step: 7
Training loss: 4.349092483520508
Validation loss: 5.062317961005754

Epoch: 6| Step: 8
Training loss: 5.134490966796875
Validation loss: 5.049064938740064

Epoch: 6| Step: 9
Training loss: 3.7097184658050537
Validation loss: 5.034325866289036

Epoch: 6| Step: 10
Training loss: 5.468526840209961
Validation loss: 5.0166348693191365

Epoch: 6| Step: 11
Training loss: 4.0119428634643555
Validation loss: 4.9964893300046205

Epoch: 6| Step: 12
Training loss: 5.13475227355957
Validation loss: 4.974438295569471

Epoch: 6| Step: 13
Training loss: 5.084233283996582
Validation loss: 4.9496544304714405

Epoch: 2| Step: 0
Training loss: 4.2167840003967285
Validation loss: 4.921621599505024

Epoch: 6| Step: 1
Training loss: 6.241113662719727
Validation loss: 4.890751572065456

Epoch: 6| Step: 2
Training loss: 4.362061500549316
Validation loss: 4.855958569434382

Epoch: 6| Step: 3
Training loss: 4.424138069152832
Validation loss: 4.816795487557688

Epoch: 6| Step: 4
Training loss: 4.564235687255859
Validation loss: 4.772664629003053

Epoch: 6| Step: 5
Training loss: 4.830432891845703
Validation loss: 4.724911505176175

Epoch: 6| Step: 6
Training loss: 4.552706241607666
Validation loss: 4.673404647457984

Epoch: 6| Step: 7
Training loss: 4.6322174072265625
Validation loss: 4.619449436023671

Epoch: 6| Step: 8
Training loss: 4.988201141357422
Validation loss: 4.563409359224381

Epoch: 6| Step: 9
Training loss: 3.1547183990478516
Validation loss: 4.508904308401128

Epoch: 6| Step: 10
Training loss: 3.111027479171753
Validation loss: 4.452996371894755

Epoch: 6| Step: 11
Training loss: 4.9068779945373535
Validation loss: 4.402843147195796

Epoch: 6| Step: 12
Training loss: 4.394583702087402
Validation loss: 4.35651453592444

Epoch: 6| Step: 13
Training loss: 3.376863718032837
Validation loss: 4.317512471188781

Epoch: 3| Step: 0
Training loss: 3.769465208053589
Validation loss: 4.280781892038161

Epoch: 6| Step: 1
Training loss: 4.978071212768555
Validation loss: 4.243935364548878

Epoch: 6| Step: 2
Training loss: 4.200461387634277
Validation loss: 4.20871949964954

Epoch: 6| Step: 3
Training loss: 3.2042670249938965
Validation loss: 4.177917552250688

Epoch: 6| Step: 4
Training loss: 3.774362564086914
Validation loss: 4.141385650122038

Epoch: 6| Step: 5
Training loss: 4.141963005065918
Validation loss: 4.104543901258899

Epoch: 6| Step: 6
Training loss: 4.517147064208984
Validation loss: 4.076641954401488

Epoch: 6| Step: 7
Training loss: 4.803652763366699
Validation loss: 4.045837863799064

Epoch: 6| Step: 8
Training loss: 4.150078773498535
Validation loss: 4.0079919394626415

Epoch: 6| Step: 9
Training loss: 3.3697757720947266
Validation loss: 3.961556778159193

Epoch: 6| Step: 10
Training loss: 3.043133020401001
Validation loss: 3.9297738793075725

Epoch: 6| Step: 11
Training loss: 4.765860080718994
Validation loss: 3.900214748997842

Epoch: 6| Step: 12
Training loss: 2.830889940261841
Validation loss: 3.8709474353380102

Epoch: 6| Step: 13
Training loss: 2.507652997970581
Validation loss: 3.84299373626709

Epoch: 4| Step: 0
Training loss: 4.302011489868164
Validation loss: 3.8185954504115607

Epoch: 6| Step: 1
Training loss: 3.939866304397583
Validation loss: 3.795471852825534

Epoch: 6| Step: 2
Training loss: 3.74320650100708
Validation loss: 3.7677776095687703

Epoch: 6| Step: 3
Training loss: 4.5347442626953125
Validation loss: 3.7422636196177494

Epoch: 6| Step: 4
Training loss: 3.150581121444702
Validation loss: 3.7179074338687363

Epoch: 6| Step: 5
Training loss: 2.5312328338623047
Validation loss: 3.7012640455717682

Epoch: 6| Step: 6
Training loss: 3.6424317359924316
Validation loss: 3.6863941838664394

Epoch: 6| Step: 7
Training loss: 3.6563069820404053
Validation loss: 3.6715214893382084

Epoch: 6| Step: 8
Training loss: 2.694502830505371
Validation loss: 3.6619027558193413

Epoch: 6| Step: 9
Training loss: 4.074833393096924
Validation loss: 3.651532724339475

Epoch: 6| Step: 10
Training loss: 4.295196533203125
Validation loss: 3.644234782905989

Epoch: 6| Step: 11
Training loss: 3.13537335395813
Validation loss: 3.640288168384183

Epoch: 6| Step: 12
Training loss: 3.0481882095336914
Validation loss: 3.6353646837255007

Epoch: 6| Step: 13
Training loss: 3.787693977355957
Validation loss: 3.622292516052082

Epoch: 5| Step: 0
Training loss: 4.339574337005615
Validation loss: 3.6088709113418416

Epoch: 6| Step: 1
Training loss: 3.6740760803222656
Validation loss: 3.600457601649787

Epoch: 6| Step: 2
Training loss: 3.5867092609405518
Validation loss: 3.5945711802410822

Epoch: 6| Step: 3
Training loss: 3.92874813079834
Validation loss: 3.588644737838417

Epoch: 6| Step: 4
Training loss: 2.8937039375305176
Validation loss: 3.5842342427981797

Epoch: 6| Step: 5
Training loss: 2.8740406036376953
Validation loss: 3.5748353312092442

Epoch: 6| Step: 6
Training loss: 2.550253391265869
Validation loss: 3.5683208434812483

Epoch: 6| Step: 7
Training loss: 3.7064614295959473
Validation loss: 3.5587512293169574

Epoch: 6| Step: 8
Training loss: 4.060971736907959
Validation loss: 3.5512006462261243

Epoch: 6| Step: 9
Training loss: 3.830533504486084
Validation loss: 3.5431944170305805

Epoch: 6| Step: 10
Training loss: 3.0481619834899902
Validation loss: 3.542865614737234

Epoch: 6| Step: 11
Training loss: 3.913635015487671
Validation loss: 3.536182775292345

Epoch: 6| Step: 12
Training loss: 3.4772720336914062
Validation loss: 3.524797280629476

Epoch: 6| Step: 13
Training loss: 2.6279497146606445
Validation loss: 3.5182930628458657

Epoch: 6| Step: 0
Training loss: 3.329120635986328
Validation loss: 3.5137846803152435

Epoch: 6| Step: 1
Training loss: 3.3071095943450928
Validation loss: 3.510357874695973

Epoch: 6| Step: 2
Training loss: 3.2390332221984863
Validation loss: 3.5019345462963147

Epoch: 6| Step: 3
Training loss: 4.009145736694336
Validation loss: 3.494793748342863

Epoch: 6| Step: 4
Training loss: 4.2174530029296875
Validation loss: 3.4844047715587

Epoch: 6| Step: 5
Training loss: 4.05599308013916
Validation loss: 3.4797581652159333

Epoch: 6| Step: 6
Training loss: 3.5683538913726807
Validation loss: 3.4652865573924077

Epoch: 6| Step: 7
Training loss: 2.7656054496765137
Validation loss: 3.45414339598789

Epoch: 6| Step: 8
Training loss: 4.236687183380127
Validation loss: 3.4432202513499925

Epoch: 6| Step: 9
Training loss: 3.6029248237609863
Validation loss: 3.4361729211704706

Epoch: 6| Step: 10
Training loss: 1.2863776683807373
Validation loss: 3.435589651907644

Epoch: 6| Step: 11
Training loss: 2.9278500080108643
Validation loss: 3.4372761249542236

Epoch: 6| Step: 12
Training loss: 4.207540988922119
Validation loss: 3.4198598938603557

Epoch: 6| Step: 13
Training loss: 2.7339465618133545
Validation loss: 3.416759926785705

Epoch: 7| Step: 0
Training loss: 3.5776076316833496
Validation loss: 3.426175476402365

Epoch: 6| Step: 1
Training loss: 2.758674383163452
Validation loss: 3.435167302367508

Epoch: 6| Step: 2
Training loss: 3.589843511581421
Validation loss: 3.415779734170565

Epoch: 6| Step: 3
Training loss: 1.6519787311553955
Validation loss: 3.3956090301595707

Epoch: 6| Step: 4
Training loss: 4.299278736114502
Validation loss: 3.3906418687553814

Epoch: 6| Step: 5
Training loss: 3.683248996734619
Validation loss: 3.3869953078608357

Epoch: 6| Step: 6
Training loss: 4.0114617347717285
Validation loss: 3.3864139177465953

Epoch: 6| Step: 7
Training loss: 3.179008960723877
Validation loss: 3.3751706871935117

Epoch: 6| Step: 8
Training loss: 3.1722464561462402
Validation loss: 3.368576870169691

Epoch: 6| Step: 9
Training loss: 2.8582189083099365
Validation loss: 3.3650210647172827

Epoch: 6| Step: 10
Training loss: 3.5434842109680176
Validation loss: 3.359402779609926

Epoch: 6| Step: 11
Training loss: 2.3192548751831055
Validation loss: 3.355905132908975

Epoch: 6| Step: 12
Training loss: 4.4173455238342285
Validation loss: 3.350138892409622

Epoch: 6| Step: 13
Training loss: 3.9168004989624023
Validation loss: 3.3454826006325344

Epoch: 8| Step: 0
Training loss: 2.4197824001312256
Validation loss: 3.342373876161473

Epoch: 6| Step: 1
Training loss: 3.630446434020996
Validation loss: 3.335995735660676

Epoch: 6| Step: 2
Training loss: 4.2487359046936035
Validation loss: 3.3291698194319204

Epoch: 6| Step: 3
Training loss: 3.3106048107147217
Validation loss: 3.3246206109241774

Epoch: 6| Step: 4
Training loss: 3.5587925910949707
Validation loss: 3.3196063862052014

Epoch: 6| Step: 5
Training loss: 3.888524293899536
Validation loss: 3.313973572946364

Epoch: 6| Step: 6
Training loss: 2.6805319786071777
Validation loss: 3.3080307078617874

Epoch: 6| Step: 7
Training loss: 1.7411952018737793
Validation loss: 3.30062521401272

Epoch: 6| Step: 8
Training loss: 2.670773506164551
Validation loss: 3.294669953725671

Epoch: 6| Step: 9
Training loss: 3.23832368850708
Validation loss: 3.2892676168872463

Epoch: 6| Step: 10
Training loss: 3.383749485015869
Validation loss: 3.2833204397591214

Epoch: 6| Step: 11
Training loss: 3.478774070739746
Validation loss: 3.2757678672831547

Epoch: 6| Step: 12
Training loss: 3.7165422439575195
Validation loss: 3.268317586632185

Epoch: 6| Step: 13
Training loss: 4.35237979888916
Validation loss: 3.2618699099427912

Epoch: 9| Step: 0
Training loss: 2.3028924465179443
Validation loss: 3.2512970406522035

Epoch: 6| Step: 1
Training loss: 3.623206853866577
Validation loss: 3.2436497134547078

Epoch: 6| Step: 2
Training loss: 3.225259780883789
Validation loss: 3.234054714120844

Epoch: 6| Step: 3
Training loss: 3.4788389205932617
Validation loss: 3.2301917999021468

Epoch: 6| Step: 4
Training loss: 2.792691230773926
Validation loss: 3.218881699346727

Epoch: 6| Step: 5
Training loss: 2.644893169403076
Validation loss: 3.213349429509973

Epoch: 6| Step: 6
Training loss: 3.4596378803253174
Validation loss: 3.212435522387105

Epoch: 6| Step: 7
Training loss: 2.563753128051758
Validation loss: 3.200581794144005

Epoch: 6| Step: 8
Training loss: 4.057111740112305
Validation loss: 3.2020342785825013

Epoch: 6| Step: 9
Training loss: 3.370511770248413
Validation loss: 3.1986064603251796

Epoch: 6| Step: 10
Training loss: 3.5998709201812744
Validation loss: 3.1922844840634252

Epoch: 6| Step: 11
Training loss: 2.7528343200683594
Validation loss: 3.183083003567111

Epoch: 6| Step: 12
Training loss: 3.707200527191162
Validation loss: 3.1802753838159705

Epoch: 6| Step: 13
Training loss: 3.35939359664917
Validation loss: 3.1703703531654934

Epoch: 10| Step: 0
Training loss: 4.056177139282227
Validation loss: 3.169078570540233

Epoch: 6| Step: 1
Training loss: 3.0949320793151855
Validation loss: 3.1607625535739365

Epoch: 6| Step: 2
Training loss: 2.6959774494171143
Validation loss: 3.150879201068673

Epoch: 6| Step: 3
Training loss: 2.9205102920532227
Validation loss: 3.1409172268324

Epoch: 6| Step: 4
Training loss: 3.73648738861084
Validation loss: 3.141435097622615

Epoch: 6| Step: 5
Training loss: 2.6566662788391113
Validation loss: 3.1413309522854385

Epoch: 6| Step: 6
Training loss: 3.4135894775390625
Validation loss: 3.1350789557221117

Epoch: 6| Step: 7
Training loss: 3.511483669281006
Validation loss: 3.1270763079325357

Epoch: 6| Step: 8
Training loss: 3.53428053855896
Validation loss: 3.1235563165398053

Epoch: 6| Step: 9
Training loss: 2.4092869758605957
Validation loss: 3.1151177883148193

Epoch: 6| Step: 10
Training loss: 3.320924758911133
Validation loss: 3.1121976221761396

Epoch: 6| Step: 11
Training loss: 2.96763277053833
Validation loss: 3.1049419628676547

Epoch: 6| Step: 12
Training loss: 2.657470226287842
Validation loss: 3.1053643611169632

Epoch: 6| Step: 13
Training loss: 3.2241086959838867
Validation loss: 3.102797408257761

Epoch: 11| Step: 0
Training loss: 3.3204586505889893
Validation loss: 3.0947936555390716

Epoch: 6| Step: 1
Training loss: 2.0654523372650146
Validation loss: 3.088731768310711

Epoch: 6| Step: 2
Training loss: 4.052674293518066
Validation loss: 3.088283374745359

Epoch: 6| Step: 3
Training loss: 2.797905445098877
Validation loss: 3.0823273479297595

Epoch: 6| Step: 4
Training loss: 2.9546494483947754
Validation loss: 3.083201334040652

Epoch: 6| Step: 5
Training loss: 3.4275929927825928
Validation loss: 3.076816640874391

Epoch: 6| Step: 6
Training loss: 3.6786675453186035
Validation loss: 3.0692104088362826

Epoch: 6| Step: 7
Training loss: 2.8223907947540283
Validation loss: 3.0677874575379076

Epoch: 6| Step: 8
Training loss: 2.590334892272949
Validation loss: 3.0661462045484975

Epoch: 6| Step: 9
Training loss: 3.7896337509155273
Validation loss: 3.069588194611252

Epoch: 6| Step: 10
Training loss: 2.6487152576446533
Validation loss: 3.0616907893970446

Epoch: 6| Step: 11
Training loss: 3.078031539916992
Validation loss: 3.0618427261229484

Epoch: 6| Step: 12
Training loss: 3.3144924640655518
Validation loss: 3.049293634712055

Epoch: 6| Step: 13
Training loss: 2.9841859340667725
Validation loss: 3.0563549110966344

Epoch: 12| Step: 0
Training loss: 3.7573020458221436
Validation loss: 3.0654796067104546

Epoch: 6| Step: 1
Training loss: 2.231515645980835
Validation loss: 3.065556151892549

Epoch: 6| Step: 2
Training loss: 2.717149019241333
Validation loss: 3.0720886773960565

Epoch: 6| Step: 3
Training loss: 2.3494367599487305
Validation loss: 3.0666898373634583

Epoch: 6| Step: 4
Training loss: 3.189822196960449
Validation loss: 3.0578870388769333

Epoch: 6| Step: 5
Training loss: 3.1218695640563965
Validation loss: 3.042382860696444

Epoch: 6| Step: 6
Training loss: 2.7214012145996094
Validation loss: 3.0268655592395413

Epoch: 6| Step: 7
Training loss: 3.5428144931793213
Validation loss: 3.0207677195149083

Epoch: 6| Step: 8
Training loss: 2.6343226432800293
Validation loss: 3.021295103975522

Epoch: 6| Step: 9
Training loss: 4.023073196411133
Validation loss: 3.0227506929828274

Epoch: 6| Step: 10
Training loss: 2.9304003715515137
Validation loss: 3.0190371005765853

Epoch: 6| Step: 11
Training loss: 3.929013252258301
Validation loss: 3.0137541986280874

Epoch: 6| Step: 12
Training loss: 2.9535059928894043
Validation loss: 3.0010670077416206

Epoch: 6| Step: 13
Training loss: 3.1527888774871826
Validation loss: 2.9968951799536265

Epoch: 13| Step: 0
Training loss: 3.2346129417419434
Validation loss: 2.9941111046780824

Epoch: 6| Step: 1
Training loss: 2.5939548015594482
Validation loss: 2.994796791384297

Epoch: 6| Step: 2
Training loss: 3.168470859527588
Validation loss: 2.9892048887027207

Epoch: 6| Step: 3
Training loss: 3.0766258239746094
Validation loss: 2.9828913596368607

Epoch: 6| Step: 4
Training loss: 2.5623064041137695
Validation loss: 2.9774642323934906

Epoch: 6| Step: 5
Training loss: 2.594254970550537
Validation loss: 2.976891722730411

Epoch: 6| Step: 6
Training loss: 4.162603378295898
Validation loss: 2.9735715850707023

Epoch: 6| Step: 7
Training loss: 2.6044132709503174
Validation loss: 2.9642934440284647

Epoch: 6| Step: 8
Training loss: 2.730156898498535
Validation loss: 2.9591943807499383

Epoch: 6| Step: 9
Training loss: 3.764904022216797
Validation loss: 2.9584333691545712

Epoch: 6| Step: 10
Training loss: 3.2662181854248047
Validation loss: 2.952639264445151

Epoch: 6| Step: 11
Training loss: 2.849581241607666
Validation loss: 2.94699828727271

Epoch: 6| Step: 12
Training loss: 2.6504054069519043
Validation loss: 2.943518510428808

Epoch: 6| Step: 13
Training loss: 3.6874020099639893
Validation loss: 2.938201868405906

Epoch: 14| Step: 0
Training loss: 3.4527196884155273
Validation loss: 2.9347448143907773

Epoch: 6| Step: 1
Training loss: 3.3182578086853027
Validation loss: 2.933000790175571

Epoch: 6| Step: 2
Training loss: 3.3948240280151367
Validation loss: 2.930052311189713

Epoch: 6| Step: 3
Training loss: 2.7082772254943848
Validation loss: 2.925615405523649

Epoch: 6| Step: 4
Training loss: 3.355996608734131
Validation loss: 2.9226744328775713

Epoch: 6| Step: 5
Training loss: 3.3776144981384277
Validation loss: 2.9175106607457644

Epoch: 6| Step: 6
Training loss: 2.306051731109619
Validation loss: 2.9169186084501204

Epoch: 6| Step: 7
Training loss: 3.244460105895996
Validation loss: 2.920796086711268

Epoch: 6| Step: 8
Training loss: 3.0550832748413086
Validation loss: 2.909832715988159

Epoch: 6| Step: 9
Training loss: 2.651050090789795
Validation loss: 2.9038184945301344

Epoch: 6| Step: 10
Training loss: 3.2699925899505615
Validation loss: 2.906482734987813

Epoch: 6| Step: 11
Training loss: 2.8846733570098877
Validation loss: 2.9039656167389243

Epoch: 6| Step: 12
Training loss: 2.411588430404663
Validation loss: 2.898919577239662

Epoch: 6| Step: 13
Training loss: 2.5400516986846924
Validation loss: 2.8917814685452368

Epoch: 15| Step: 0
Training loss: 2.951674461364746
Validation loss: 2.890528940385388

Epoch: 6| Step: 1
Training loss: 2.000797986984253
Validation loss: 2.8893390368389826

Epoch: 6| Step: 2
Training loss: 3.236826181411743
Validation loss: 2.886726789577033

Epoch: 6| Step: 3
Training loss: 3.3172764778137207
Validation loss: 2.8825589328683834

Epoch: 6| Step: 4
Training loss: 2.9642276763916016
Validation loss: 2.8798993146547707

Epoch: 6| Step: 5
Training loss: 2.2511684894561768
Validation loss: 2.877608847874467

Epoch: 6| Step: 6
Training loss: 3.5140159130096436
Validation loss: 2.875102549470881

Epoch: 6| Step: 7
Training loss: 2.456042528152466
Validation loss: 2.8692275580539497

Epoch: 6| Step: 8
Training loss: 2.526815891265869
Validation loss: 2.8666136469892276

Epoch: 6| Step: 9
Training loss: 3.2440123558044434
Validation loss: 2.871659022505565

Epoch: 6| Step: 10
Training loss: 3.9640069007873535
Validation loss: 2.890406277871901

Epoch: 6| Step: 11
Training loss: 2.8731319904327393
Validation loss: 2.8835396151388846

Epoch: 6| Step: 12
Training loss: 3.6046667098999023
Validation loss: 2.898823143333517

Epoch: 6| Step: 13
Training loss: 2.832575798034668
Validation loss: 2.8587109529843895

Epoch: 16| Step: 0
Training loss: 2.78002667427063
Validation loss: 2.866074700509348

Epoch: 6| Step: 1
Training loss: 3.0724568367004395
Validation loss: 2.878614153913272

Epoch: 6| Step: 2
Training loss: 2.1451401710510254
Validation loss: 2.8874136119760494

Epoch: 6| Step: 3
Training loss: 4.169337749481201
Validation loss: 2.9154364831985964

Epoch: 6| Step: 4
Training loss: 2.8521158695220947
Validation loss: 2.868446921789518

Epoch: 6| Step: 5
Training loss: 2.772639274597168
Validation loss: 2.88219545220816

Epoch: 6| Step: 6
Training loss: 2.535015821456909
Validation loss: 2.908192462818597

Epoch: 6| Step: 7
Training loss: 3.3047046661376953
Validation loss: 2.925917661318215

Epoch: 6| Step: 8
Training loss: 3.146261692047119
Validation loss: 2.9402495481634654

Epoch: 6| Step: 9
Training loss: 2.90160870552063
Validation loss: 2.917866422284034

Epoch: 6| Step: 10
Training loss: 2.8589677810668945
Validation loss: 2.911223303887152

Epoch: 6| Step: 11
Training loss: 3.6732358932495117
Validation loss: 2.945842645501578

Epoch: 6| Step: 12
Training loss: 2.733081340789795
Validation loss: 2.8920237197670886

Epoch: 6| Step: 13
Training loss: 3.166882038116455
Validation loss: 2.946818951637514

Epoch: 17| Step: 0
Training loss: 3.0299947261810303
Validation loss: 3.0388713857179046

Epoch: 6| Step: 1
Training loss: 3.2848727703094482
Validation loss: 3.1275315900002756

Epoch: 6| Step: 2
Training loss: 3.060575008392334
Validation loss: 2.9803309773886077

Epoch: 6| Step: 3
Training loss: 3.183577299118042
Validation loss: 2.9012776292780393

Epoch: 6| Step: 4
Training loss: 1.9325042963027954
Validation loss: 2.8809136805995816

Epoch: 6| Step: 5
Training loss: 3.3222668170928955
Validation loss: 2.882037970327562

Epoch: 6| Step: 6
Training loss: 2.94954776763916
Validation loss: 2.8988829402513403

Epoch: 6| Step: 7
Training loss: 3.503948926925659
Validation loss: 2.924041868537985

Epoch: 6| Step: 8
Training loss: 1.7693482637405396
Validation loss: 2.918959145904869

Epoch: 6| Step: 9
Training loss: 3.159435510635376
Validation loss: 2.9017967357430408

Epoch: 6| Step: 10
Training loss: 3.8719635009765625
Validation loss: 2.8882900745637956

Epoch: 6| Step: 11
Training loss: 2.5778427124023438
Validation loss: 2.8698970502422703

Epoch: 6| Step: 12
Training loss: 3.2551705837249756
Validation loss: 2.8536828179513254

Epoch: 6| Step: 13
Training loss: 3.5403616428375244
Validation loss: 2.8403772231071227

Epoch: 18| Step: 0
Training loss: 3.3354692459106445
Validation loss: 2.8366380532582602

Epoch: 6| Step: 1
Training loss: 2.5711708068847656
Validation loss: 2.834591068247313

Epoch: 6| Step: 2
Training loss: 2.8440866470336914
Validation loss: 2.83586395940473

Epoch: 6| Step: 3
Training loss: 2.5796115398406982
Validation loss: 2.836098796577864

Epoch: 6| Step: 4
Training loss: 2.863863706588745
Validation loss: 2.8350344037496917

Epoch: 6| Step: 5
Training loss: 2.7737135887145996
Validation loss: 2.833487267135292

Epoch: 6| Step: 6
Training loss: 2.878786325454712
Validation loss: 2.831473717125513

Epoch: 6| Step: 7
Training loss: 2.5625405311584473
Validation loss: 2.833015716204079

Epoch: 6| Step: 8
Training loss: 3.7219185829162598
Validation loss: 2.8317728196420977

Epoch: 6| Step: 9
Training loss: 3.1920828819274902
Validation loss: 2.826117492491199

Epoch: 6| Step: 10
Training loss: 2.7832465171813965
Validation loss: 2.819641287608813

Epoch: 6| Step: 11
Training loss: 2.231079578399658
Validation loss: 2.8182037389406593

Epoch: 6| Step: 12
Training loss: 3.7239761352539062
Validation loss: 2.8132497700311805

Epoch: 6| Step: 13
Training loss: 3.554661512374878
Validation loss: 2.811606760947935

Epoch: 19| Step: 0
Training loss: 2.954448699951172
Validation loss: 2.8104236895038235

Epoch: 6| Step: 1
Training loss: 3.2390947341918945
Validation loss: 2.808358618008193

Epoch: 6| Step: 2
Training loss: 2.2919626235961914
Validation loss: 2.808188403806379

Epoch: 6| Step: 3
Training loss: 3.1506175994873047
Validation loss: 2.8063335495610393

Epoch: 6| Step: 4
Training loss: 4.004817962646484
Validation loss: 2.8056082853706936

Epoch: 6| Step: 5
Training loss: 1.9993071556091309
Validation loss: 2.804818066217566

Epoch: 6| Step: 6
Training loss: 2.476065158843994
Validation loss: 2.804301195247199

Epoch: 6| Step: 7
Training loss: 2.8816895484924316
Validation loss: 2.8042525629843436

Epoch: 6| Step: 8
Training loss: 2.2414159774780273
Validation loss: 2.8001580597251974

Epoch: 6| Step: 9
Training loss: 3.430093765258789
Validation loss: 2.800514151973109

Epoch: 6| Step: 10
Training loss: 2.1823554039001465
Validation loss: 2.798810371788599

Epoch: 6| Step: 11
Training loss: 3.0181379318237305
Validation loss: 2.7989517411878033

Epoch: 6| Step: 12
Training loss: 4.336178779602051
Validation loss: 2.795917513549969

Epoch: 6| Step: 13
Training loss: 2.891842842102051
Validation loss: 2.7954835096995034

Epoch: 20| Step: 0
Training loss: 3.2444911003112793
Validation loss: 2.793446566468926

Epoch: 6| Step: 1
Training loss: 2.900522232055664
Validation loss: 2.7919390124659382

Epoch: 6| Step: 2
Training loss: 3.2275924682617188
Validation loss: 2.791445005324579

Epoch: 6| Step: 3
Training loss: 3.181697368621826
Validation loss: 2.7895862184545046

Epoch: 6| Step: 4
Training loss: 2.816453218460083
Validation loss: 2.788748307894635

Epoch: 6| Step: 5
Training loss: 3.2339038848876953
Validation loss: 2.7882052031896447

Epoch: 6| Step: 6
Training loss: 2.6854796409606934
Validation loss: 2.7871863380555184

Epoch: 6| Step: 7
Training loss: 3.06764554977417
Validation loss: 2.7868239648880495

Epoch: 6| Step: 8
Training loss: 2.6313819885253906
Validation loss: 2.785796344921153

Epoch: 6| Step: 9
Training loss: 3.2317004203796387
Validation loss: 2.784952558496947

Epoch: 6| Step: 10
Training loss: 3.185964822769165
Validation loss: 2.784804569777622

Epoch: 6| Step: 11
Training loss: 2.5072503089904785
Validation loss: 2.7835696769017044

Epoch: 6| Step: 12
Training loss: 2.5574350357055664
Validation loss: 2.782966160005139

Epoch: 6| Step: 13
Training loss: 2.2009520530700684
Validation loss: 2.781734925444408

Epoch: 21| Step: 0
Training loss: 2.6255455017089844
Validation loss: 2.781325335143715

Epoch: 6| Step: 1
Training loss: 3.153895854949951
Validation loss: 2.7812162983802056

Epoch: 6| Step: 2
Training loss: 3.2850403785705566
Validation loss: 2.780070099779355

Epoch: 6| Step: 3
Training loss: 2.580389976501465
Validation loss: 2.7797628141218618

Epoch: 6| Step: 4
Training loss: 2.0872678756713867
Validation loss: 2.779419345240439

Epoch: 6| Step: 5
Training loss: 3.121115207672119
Validation loss: 2.778807109402072

Epoch: 6| Step: 6
Training loss: 3.5100512504577637
Validation loss: 2.777804466985887

Epoch: 6| Step: 7
Training loss: 3.0634379386901855
Validation loss: 2.7779080842130925

Epoch: 6| Step: 8
Training loss: 2.7560009956359863
Validation loss: 2.7763850868389173

Epoch: 6| Step: 9
Training loss: 2.9313430786132812
Validation loss: 2.7761595095357587

Epoch: 6| Step: 10
Training loss: 3.032397747039795
Validation loss: 2.776241825472924

Epoch: 6| Step: 11
Training loss: 3.3964219093322754
Validation loss: 2.7757326659335884

Epoch: 6| Step: 12
Training loss: 2.9777238368988037
Validation loss: 2.7737179417763986

Epoch: 6| Step: 13
Training loss: 1.9508448839187622
Validation loss: 2.7745799326127574

Epoch: 22| Step: 0
Training loss: 2.9456286430358887
Validation loss: 2.7729990892512824

Epoch: 6| Step: 1
Training loss: 2.7519454956054688
Validation loss: 2.773583363461238

Epoch: 6| Step: 2
Training loss: 3.1267576217651367
Validation loss: 2.772466474963773

Epoch: 6| Step: 3
Training loss: 2.3359055519104004
Validation loss: 2.7716732255874144

Epoch: 6| Step: 4
Training loss: 2.738266944885254
Validation loss: 2.7721058912174676

Epoch: 6| Step: 5
Training loss: 3.494598865509033
Validation loss: 2.7705161007501746

Epoch: 6| Step: 6
Training loss: 3.132016181945801
Validation loss: 2.7711018490534958

Epoch: 6| Step: 7
Training loss: 2.0212388038635254
Validation loss: 2.7697249715046217

Epoch: 6| Step: 8
Training loss: 3.245422601699829
Validation loss: 2.7694418609783216

Epoch: 6| Step: 9
Training loss: 2.9794468879699707
Validation loss: 2.7688239069395166

Epoch: 6| Step: 10
Training loss: 2.597271203994751
Validation loss: 2.767872110489876

Epoch: 6| Step: 11
Training loss: 3.167344093322754
Validation loss: 2.766590779827487

Epoch: 6| Step: 12
Training loss: 3.5228681564331055
Validation loss: 2.766125043233236

Epoch: 6| Step: 13
Training loss: 2.644056558609009
Validation loss: 2.7653055652495353

Epoch: 23| Step: 0
Training loss: 3.474299907684326
Validation loss: 2.7655730606407247

Epoch: 6| Step: 1
Training loss: 2.6748290061950684
Validation loss: 2.764355554375597

Epoch: 6| Step: 2
Training loss: 3.0510127544403076
Validation loss: 2.7639530063957296

Epoch: 6| Step: 3
Training loss: 2.2380735874176025
Validation loss: 2.7640861900903846

Epoch: 6| Step: 4
Training loss: 3.1986308097839355
Validation loss: 2.7635092658381306

Epoch: 6| Step: 5
Training loss: 2.5259177684783936
Validation loss: 2.762610125285323

Epoch: 6| Step: 6
Training loss: 3.2459685802459717
Validation loss: 2.763106956276842

Epoch: 6| Step: 7
Training loss: 2.369504928588867
Validation loss: 2.761615709591937

Epoch: 6| Step: 8
Training loss: 3.2266845703125
Validation loss: 2.7603208787979616

Epoch: 6| Step: 9
Training loss: 3.631588935852051
Validation loss: 2.761389914379325

Epoch: 6| Step: 10
Training loss: 3.4235916137695312
Validation loss: 2.76027415644738

Epoch: 6| Step: 11
Training loss: 2.238987922668457
Validation loss: 2.758339002568235

Epoch: 6| Step: 12
Training loss: 2.889832019805908
Validation loss: 2.758584125067598

Epoch: 6| Step: 13
Training loss: 2.281190872192383
Validation loss: 2.7572181788823937

Epoch: 24| Step: 0
Training loss: 3.4474310874938965
Validation loss: 2.756876435331119

Epoch: 6| Step: 1
Training loss: 3.2365777492523193
Validation loss: 2.756259641339702

Epoch: 6| Step: 2
Training loss: 3.180400848388672
Validation loss: 2.7569745304763957

Epoch: 6| Step: 3
Training loss: 3.0614709854125977
Validation loss: 2.755734428282707

Epoch: 6| Step: 4
Training loss: 3.0099048614501953
Validation loss: 2.7559334411416003

Epoch: 6| Step: 5
Training loss: 2.8979616165161133
Validation loss: 2.7543647109821277

Epoch: 6| Step: 6
Training loss: 2.5195798873901367
Validation loss: 2.754169656384376

Epoch: 6| Step: 7
Training loss: 1.9468870162963867
Validation loss: 2.7535220243597545

Epoch: 6| Step: 8
Training loss: 2.4872374534606934
Validation loss: 2.7542288329011653

Epoch: 6| Step: 9
Training loss: 3.478929042816162
Validation loss: 2.7514650629412745

Epoch: 6| Step: 10
Training loss: 3.037644147872925
Validation loss: 2.753192501683389

Epoch: 6| Step: 11
Training loss: 2.683290719985962
Validation loss: 2.7512239692031697

Epoch: 6| Step: 12
Training loss: 2.763777017593384
Validation loss: 2.7512104434351765

Epoch: 6| Step: 13
Training loss: 2.9049463272094727
Validation loss: 2.749738895764915

Epoch: 25| Step: 0
Training loss: 4.370416641235352
Validation loss: 2.747490208636048

Epoch: 6| Step: 1
Training loss: 2.2775797843933105
Validation loss: 2.7517438473240023

Epoch: 6| Step: 2
Training loss: 3.4697225093841553
Validation loss: 2.748463128202705

Epoch: 6| Step: 3
Training loss: 3.306126832962036
Validation loss: 2.748520456334596

Epoch: 6| Step: 4
Training loss: 2.527977705001831
Validation loss: 2.748413024410125

Epoch: 6| Step: 5
Training loss: 3.2553462982177734
Validation loss: 2.7500099776893534

Epoch: 6| Step: 6
Training loss: 2.200056552886963
Validation loss: 2.7493609689897105

Epoch: 6| Step: 7
Training loss: 2.9052820205688477
Validation loss: 2.75154524208397

Epoch: 6| Step: 8
Training loss: 1.923496961593628
Validation loss: 2.754767494816934

Epoch: 6| Step: 9
Training loss: 2.3991336822509766
Validation loss: 2.7547854454286638

Epoch: 6| Step: 10
Training loss: 2.200298309326172
Validation loss: 2.7564707802188013

Epoch: 6| Step: 11
Training loss: 2.9184513092041016
Validation loss: 2.752933097142045

Epoch: 6| Step: 12
Training loss: 3.1644465923309326
Validation loss: 2.74916898563344

Epoch: 6| Step: 13
Training loss: 4.291030406951904
Validation loss: 2.7499865152502574

Epoch: 26| Step: 0
Training loss: 3.547604560852051
Validation loss: 2.74775836031924

Epoch: 6| Step: 1
Training loss: 2.7180354595184326
Validation loss: 2.746826692294049

Epoch: 6| Step: 2
Training loss: 3.3444695472717285
Validation loss: 2.743013210194085

Epoch: 6| Step: 3
Training loss: 2.491128444671631
Validation loss: 2.7485951403135895

Epoch: 6| Step: 4
Training loss: 3.361874580383301
Validation loss: 2.7565451539972776

Epoch: 6| Step: 5
Training loss: 2.8172521591186523
Validation loss: 2.7426283949164936

Epoch: 6| Step: 6
Training loss: 3.0938217639923096
Validation loss: 2.739522357140818

Epoch: 6| Step: 7
Training loss: 2.5526702404022217
Validation loss: 2.7390471325125745

Epoch: 6| Step: 8
Training loss: 2.4856576919555664
Validation loss: 2.7433795313681326

Epoch: 6| Step: 9
Training loss: 2.703721761703491
Validation loss: 2.739407777786255

Epoch: 6| Step: 10
Training loss: 1.9962540864944458
Validation loss: 2.7485956274053103

Epoch: 6| Step: 11
Training loss: 3.2735772132873535
Validation loss: 2.7597260141885407

Epoch: 6| Step: 12
Training loss: 3.0743370056152344
Validation loss: 2.7897984391899517

Epoch: 6| Step: 13
Training loss: 3.3637280464172363
Validation loss: 2.8310378725810716

Epoch: 27| Step: 0
Training loss: 3.0484652519226074
Validation loss: 2.7701619312327397

Epoch: 6| Step: 1
Training loss: 2.7450437545776367
Validation loss: 2.7478587165955575

Epoch: 6| Step: 2
Training loss: 3.0361995697021484
Validation loss: 2.733642580688641

Epoch: 6| Step: 3
Training loss: 2.0055270195007324
Validation loss: 2.736832357222034

Epoch: 6| Step: 4
Training loss: 2.9984962940216064
Validation loss: 2.741338845222227

Epoch: 6| Step: 5
Training loss: 2.8826351165771484
Validation loss: 2.7454489097800305

Epoch: 6| Step: 6
Training loss: 1.6982343196868896
Validation loss: 2.751104829131916

Epoch: 6| Step: 7
Training loss: 2.9153828620910645
Validation loss: 2.788695461006575

Epoch: 6| Step: 8
Training loss: 3.311115264892578
Validation loss: 2.8151229889162126

Epoch: 6| Step: 9
Training loss: 3.1196093559265137
Validation loss: 2.747277529008927

Epoch: 6| Step: 10
Training loss: 3.3928022384643555
Validation loss: 2.7354865099794123

Epoch: 6| Step: 11
Training loss: 3.3013687133789062
Validation loss: 2.7296720525269866

Epoch: 6| Step: 12
Training loss: 2.7541849613189697
Validation loss: 2.733172603832778

Epoch: 6| Step: 13
Training loss: 3.889268159866333
Validation loss: 2.7595156174834057

Epoch: 28| Step: 0
Training loss: 2.8600873947143555
Validation loss: 2.7555098200357087

Epoch: 6| Step: 1
Training loss: 2.613701820373535
Validation loss: 2.7383053738583802

Epoch: 6| Step: 2
Training loss: 2.076772689819336
Validation loss: 2.721347344818936

Epoch: 6| Step: 3
Training loss: 2.8909759521484375
Validation loss: 2.727925108325097

Epoch: 6| Step: 4
Training loss: 2.999682903289795
Validation loss: 2.72249952695703

Epoch: 6| Step: 5
Training loss: 3.981006383895874
Validation loss: 2.727045838550855

Epoch: 6| Step: 6
Training loss: 2.6292123794555664
Validation loss: 2.7289394563244236

Epoch: 6| Step: 7
Training loss: 2.607537269592285
Validation loss: 2.730124488953621

Epoch: 6| Step: 8
Training loss: 4.05945348739624
Validation loss: 2.730077676875617

Epoch: 6| Step: 9
Training loss: 2.0329763889312744
Validation loss: 2.7321199652969197

Epoch: 6| Step: 10
Training loss: 2.2596089839935303
Validation loss: 2.7322607501860587

Epoch: 6| Step: 11
Training loss: 3.1708545684814453
Validation loss: 2.7319204371462584

Epoch: 6| Step: 12
Training loss: 3.6533823013305664
Validation loss: 2.730761440851355

Epoch: 6| Step: 13
Training loss: 2.5053603649139404
Validation loss: 2.729141599388533

Epoch: 29| Step: 0
Training loss: 2.5634381771087646
Validation loss: 2.727335981143418

Epoch: 6| Step: 1
Training loss: 3.566364288330078
Validation loss: 2.7261461109243412

Epoch: 6| Step: 2
Training loss: 2.8687503337860107
Validation loss: 2.7245880865281626

Epoch: 6| Step: 3
Training loss: 2.916311740875244
Validation loss: 2.722071780953356

Epoch: 6| Step: 4
Training loss: 2.553011655807495
Validation loss: 2.7217807103228826

Epoch: 6| Step: 5
Training loss: 2.8485360145568848
Validation loss: 2.720841764121927

Epoch: 6| Step: 6
Training loss: 3.5071797370910645
Validation loss: 2.719490581943143

Epoch: 6| Step: 7
Training loss: 2.581818103790283
Validation loss: 2.7186316059481714

Epoch: 6| Step: 8
Training loss: 2.822171688079834
Validation loss: 2.7175987048815657

Epoch: 6| Step: 9
Training loss: 3.182222843170166
Validation loss: 2.7176092952810307

Epoch: 6| Step: 10
Training loss: 3.274235248565674
Validation loss: 2.71597263889928

Epoch: 6| Step: 11
Training loss: 2.6534013748168945
Validation loss: 2.7158402755696285

Epoch: 6| Step: 12
Training loss: 2.4043922424316406
Validation loss: 2.713553556831934

Epoch: 6| Step: 13
Training loss: 2.3956146240234375
Validation loss: 2.7115727111857426

Epoch: 30| Step: 0
Training loss: 3.2718982696533203
Validation loss: 2.71108074854779

Epoch: 6| Step: 1
Training loss: 2.9415903091430664
Validation loss: 2.7120656915890273

Epoch: 6| Step: 2
Training loss: 3.250882625579834
Validation loss: 2.7073589422369517

Epoch: 6| Step: 3
Training loss: 2.584799289703369
Validation loss: 2.7090158283069568

Epoch: 6| Step: 4
Training loss: 2.9630234241485596
Validation loss: 2.7066420714060464

Epoch: 6| Step: 5
Training loss: 3.016458511352539
Validation loss: 2.701597736727807

Epoch: 6| Step: 6
Training loss: 2.127784252166748
Validation loss: 2.7052120982959704

Epoch: 6| Step: 7
Training loss: 2.9579250812530518
Validation loss: 2.701165642789615

Epoch: 6| Step: 8
Training loss: 3.049107789993286
Validation loss: 2.6986184017632597

Epoch: 6| Step: 9
Training loss: 2.1010499000549316
Validation loss: 2.6994053702200613

Epoch: 6| Step: 10
Training loss: 3.1854662895202637
Validation loss: 2.699921936117193

Epoch: 6| Step: 11
Training loss: 2.948408603668213
Validation loss: 2.7056845131740777

Epoch: 6| Step: 12
Training loss: 2.5835001468658447
Validation loss: 2.7187320673337547

Epoch: 6| Step: 13
Training loss: 3.525912284851074
Validation loss: 2.8783840774207987

Epoch: 31| Step: 0
Training loss: 3.5766968727111816
Validation loss: 2.827919431912002

Epoch: 6| Step: 1
Training loss: 2.871638774871826
Validation loss: 2.6953336731080086

Epoch: 6| Step: 2
Training loss: 3.3709936141967773
Validation loss: 2.733236740994197

Epoch: 6| Step: 3
Training loss: 3.802825927734375
Validation loss: 2.915429187077348

Epoch: 6| Step: 4
Training loss: 2.820249319076538
Validation loss: 3.021766080651232

Epoch: 6| Step: 5
Training loss: 3.850923538208008
Validation loss: 3.0672198905739734

Epoch: 6| Step: 6
Training loss: 3.257267475128174
Validation loss: 3.071448303038074

Epoch: 6| Step: 7
Training loss: 1.8878281116485596
Validation loss: 2.9209844835342897

Epoch: 6| Step: 8
Training loss: 3.005329132080078
Validation loss: 2.8517050409829743

Epoch: 6| Step: 9
Training loss: 2.2573044300079346
Validation loss: 2.8089419180347073

Epoch: 6| Step: 10
Training loss: 3.839433193206787
Validation loss: 2.7531992132945726

Epoch: 6| Step: 11
Training loss: 2.101132869720459
Validation loss: 2.707700066669013

Epoch: 6| Step: 12
Training loss: 1.8965444564819336
Validation loss: 2.7204015049883115

Epoch: 6| Step: 13
Training loss: 2.930065870285034
Validation loss: 2.7461695876172794

Epoch: 32| Step: 0
Training loss: 3.3171355724334717
Validation loss: 2.8414692160903767

Epoch: 6| Step: 1
Training loss: 2.6297500133514404
Validation loss: 2.989957550520538

Epoch: 6| Step: 2
Training loss: 2.5950522422790527
Validation loss: 2.863751467838082

Epoch: 6| Step: 3
Training loss: 2.121387481689453
Validation loss: 2.760981503353324

Epoch: 6| Step: 4
Training loss: 3.718306064605713
Validation loss: 2.7104575839093936

Epoch: 6| Step: 5
Training loss: 2.7084507942199707
Validation loss: 2.69775055813533

Epoch: 6| Step: 6
Training loss: 2.899702787399292
Validation loss: 2.700177913071007

Epoch: 6| Step: 7
Training loss: 2.7607758045196533
Validation loss: 2.705317779253888

Epoch: 6| Step: 8
Training loss: 2.7741265296936035
Validation loss: 2.727360592093519

Epoch: 6| Step: 9
Training loss: 2.9456968307495117
Validation loss: 2.754489832026984

Epoch: 6| Step: 10
Training loss: 2.1560709476470947
Validation loss: 2.7542347087655017

Epoch: 6| Step: 11
Training loss: 3.7761011123657227
Validation loss: 2.7674227529956448

Epoch: 6| Step: 12
Training loss: 3.0657172203063965
Validation loss: 2.7332308369298137

Epoch: 6| Step: 13
Training loss: 3.6171133518218994
Validation loss: 2.7193899385390745

Epoch: 33| Step: 0
Training loss: 2.888965606689453
Validation loss: 2.7028182783434467

Epoch: 6| Step: 1
Training loss: 3.0466277599334717
Validation loss: 2.6939445311023342

Epoch: 6| Step: 2
Training loss: 2.7701354026794434
Validation loss: 2.742215548792193

Epoch: 6| Step: 3
Training loss: 3.1465916633605957
Validation loss: 2.7527159337074525

Epoch: 6| Step: 4
Training loss: 3.447775363922119
Validation loss: 2.767813080100603

Epoch: 6| Step: 5
Training loss: 2.9061238765716553
Validation loss: 2.764830466239683

Epoch: 6| Step: 6
Training loss: 2.587099552154541
Validation loss: 2.780220083011094

Epoch: 6| Step: 7
Training loss: 3.219144582748413
Validation loss: 2.7826266134938886

Epoch: 6| Step: 8
Training loss: 3.0713908672332764
Validation loss: 2.747142478983889

Epoch: 6| Step: 9
Training loss: 1.8938825130462646
Validation loss: 2.7381452386097243

Epoch: 6| Step: 10
Training loss: 2.4011359214782715
Validation loss: 2.733509607212518

Epoch: 6| Step: 11
Training loss: 2.655550479888916
Validation loss: 2.7314959213297856

Epoch: 6| Step: 12
Training loss: 3.0030369758605957
Validation loss: 2.7354468325132966

Epoch: 6| Step: 13
Training loss: 3.84926176071167
Validation loss: 2.701577950549382

Epoch: 34| Step: 0
Training loss: 3.1986284255981445
Validation loss: 2.6786147958488873

Epoch: 6| Step: 1
Training loss: 3.4376187324523926
Validation loss: 2.69119288844447

Epoch: 6| Step: 2
Training loss: 2.951657772064209
Validation loss: 2.694296775325652

Epoch: 6| Step: 3
Training loss: 2.9774203300476074
Validation loss: 2.6946219987766717

Epoch: 6| Step: 4
Training loss: 2.611696720123291
Validation loss: 2.698185705369519

Epoch: 6| Step: 5
Training loss: 1.931564211845398
Validation loss: 2.6843063677510908

Epoch: 6| Step: 6
Training loss: 3.2032978534698486
Validation loss: 2.6788118962318666

Epoch: 6| Step: 7
Training loss: 2.5940942764282227
Validation loss: 2.6750888952644925

Epoch: 6| Step: 8
Training loss: 2.824925422668457
Validation loss: 2.672667077792588

Epoch: 6| Step: 9
Training loss: 2.698601245880127
Validation loss: 2.6745768721385668

Epoch: 6| Step: 10
Training loss: 2.219545364379883
Validation loss: 2.6727294383510465

Epoch: 6| Step: 11
Training loss: 2.7832016944885254
Validation loss: 2.669704201400921

Epoch: 6| Step: 12
Training loss: 3.5349676609039307
Validation loss: 2.6698741246295232

Epoch: 6| Step: 13
Training loss: 3.163079023361206
Validation loss: 2.67320817260332

Epoch: 35| Step: 0
Training loss: 2.6162824630737305
Validation loss: 2.6711090585236907

Epoch: 6| Step: 1
Training loss: 2.6747331619262695
Validation loss: 2.671804707537415

Epoch: 6| Step: 2
Training loss: 3.0837955474853516
Validation loss: 2.6723480070790937

Epoch: 6| Step: 3
Training loss: 2.9871315956115723
Validation loss: 2.673325777053833

Epoch: 6| Step: 4
Training loss: 2.737947463989258
Validation loss: 2.667676494967553

Epoch: 6| Step: 5
Training loss: 3.080439805984497
Validation loss: 2.6663193138696815

Epoch: 6| Step: 6
Training loss: 2.619788646697998
Validation loss: 2.6666622084956013

Epoch: 6| Step: 7
Training loss: 3.1790518760681152
Validation loss: 2.6634642078030493

Epoch: 6| Step: 8
Training loss: 2.8841514587402344
Validation loss: 2.6665592834513676

Epoch: 6| Step: 9
Training loss: 3.4389870166778564
Validation loss: 2.6643059125510593

Epoch: 6| Step: 10
Training loss: 2.809479236602783
Validation loss: 2.6628691457932994

Epoch: 6| Step: 11
Training loss: 2.6375207901000977
Validation loss: 2.6620170301006687

Epoch: 6| Step: 12
Training loss: 2.3986518383026123
Validation loss: 2.663139215079687

Epoch: 6| Step: 13
Training loss: 2.7380776405334473
Validation loss: 2.662126241191741

Epoch: 36| Step: 0
Training loss: 3.1043381690979004
Validation loss: 2.6586639086405435

Epoch: 6| Step: 1
Training loss: 2.3931946754455566
Validation loss: 2.660791911104674

Epoch: 6| Step: 2
Training loss: 3.1999454498291016
Validation loss: 2.6611402778215307

Epoch: 6| Step: 3
Training loss: 2.5002264976501465
Validation loss: 2.6603677554797103

Epoch: 6| Step: 4
Training loss: 2.5034894943237305
Validation loss: 2.6557750137903358

Epoch: 6| Step: 5
Training loss: 2.763939619064331
Validation loss: 2.6546248748738277

Epoch: 6| Step: 6
Training loss: 3.086095094680786
Validation loss: 2.6536668269864974

Epoch: 6| Step: 7
Training loss: 3.2652134895324707
Validation loss: 2.6534899691099763

Epoch: 6| Step: 8
Training loss: 3.230239152908325
Validation loss: 2.6548616117046726

Epoch: 6| Step: 9
Training loss: 2.801417112350464
Validation loss: 2.6571805554051555

Epoch: 6| Step: 10
Training loss: 3.0488524436950684
Validation loss: 2.6536497762126308

Epoch: 6| Step: 11
Training loss: 2.2876064777374268
Validation loss: 2.6478929340198474

Epoch: 6| Step: 12
Training loss: 2.9019412994384766
Validation loss: 2.645071760300667

Epoch: 6| Step: 13
Training loss: 2.6493818759918213
Validation loss: 2.645122689585532

Epoch: 37| Step: 0
Training loss: 3.3448104858398438
Validation loss: 2.6382763783137

Epoch: 6| Step: 1
Training loss: 2.9516782760620117
Validation loss: 2.6398326145705355

Epoch: 6| Step: 2
Training loss: 2.563342809677124
Validation loss: 2.643605955185429

Epoch: 6| Step: 3
Training loss: 3.0455942153930664
Validation loss: 2.6519087540206088

Epoch: 6| Step: 4
Training loss: 2.956132411956787
Validation loss: 2.6598032879573044

Epoch: 6| Step: 5
Training loss: 2.3217530250549316
Validation loss: 2.6464668653344594

Epoch: 6| Step: 6
Training loss: 2.6319360733032227
Validation loss: 2.641490885006484

Epoch: 6| Step: 7
Training loss: 2.4196300506591797
Validation loss: 2.631373707966138

Epoch: 6| Step: 8
Training loss: 2.712864875793457
Validation loss: 2.624202492416546

Epoch: 6| Step: 9
Training loss: 3.2792792320251465
Validation loss: 2.6282907685925885

Epoch: 6| Step: 10
Training loss: 3.0737574100494385
Validation loss: 2.6353894202939925

Epoch: 6| Step: 11
Training loss: 2.3756937980651855
Validation loss: 2.6360691465357298

Epoch: 6| Step: 12
Training loss: 2.8187179565429688
Validation loss: 2.6321059619226763

Epoch: 6| Step: 13
Training loss: 3.011810779571533
Validation loss: 2.6308051386187152

Epoch: 38| Step: 0
Training loss: 3.198698043823242
Validation loss: 2.6379812032945695

Epoch: 6| Step: 1
Training loss: 2.6147561073303223
Validation loss: 2.6538979417534283

Epoch: 6| Step: 2
Training loss: 2.586543083190918
Validation loss: 2.65599912212741

Epoch: 6| Step: 3
Training loss: 2.7357678413391113
Validation loss: 2.6702870476630425

Epoch: 6| Step: 4
Training loss: 3.399718761444092
Validation loss: 2.65892465396594

Epoch: 6| Step: 5
Training loss: 1.981714129447937
Validation loss: 2.637844277966407

Epoch: 6| Step: 6
Training loss: 2.5486900806427
Validation loss: 2.6311032951519056

Epoch: 6| Step: 7
Training loss: 3.3661298751831055
Validation loss: 2.636190963047807

Epoch: 6| Step: 8
Training loss: 3.2617945671081543
Validation loss: 2.6322899659474692

Epoch: 6| Step: 9
Training loss: 2.907745838165283
Validation loss: 2.6350963448965423

Epoch: 6| Step: 10
Training loss: 2.9290170669555664
Validation loss: 2.6332510184216242

Epoch: 6| Step: 11
Training loss: 3.012495517730713
Validation loss: 2.627412993420837

Epoch: 6| Step: 12
Training loss: 2.536034107208252
Validation loss: 2.628882295341902

Epoch: 6| Step: 13
Training loss: 1.8894786834716797
Validation loss: 2.6272581495264524

Epoch: 39| Step: 0
Training loss: 1.8225703239440918
Validation loss: 2.6198001625717326

Epoch: 6| Step: 1
Training loss: 3.794551372528076
Validation loss: 2.6089355868677937

Epoch: 6| Step: 2
Training loss: 2.5457119941711426
Validation loss: 2.6271609619099605

Epoch: 6| Step: 3
Training loss: 3.026235580444336
Validation loss: 2.661274307517595

Epoch: 6| Step: 4
Training loss: 2.895902633666992
Validation loss: 2.6821910976081766

Epoch: 6| Step: 5
Training loss: 2.424913167953491
Validation loss: 2.656826767870175

Epoch: 6| Step: 6
Training loss: 2.465947151184082
Validation loss: 2.6163465079440864

Epoch: 6| Step: 7
Training loss: 3.058199882507324
Validation loss: 2.6121526636103147

Epoch: 6| Step: 8
Training loss: 3.3378567695617676
Validation loss: 2.608841229510564

Epoch: 6| Step: 9
Training loss: 2.3256661891937256
Validation loss: 2.612462992309242

Epoch: 6| Step: 10
Training loss: 2.720194101333618
Validation loss: 2.6083325955175583

Epoch: 6| Step: 11
Training loss: 3.7409019470214844
Validation loss: 2.6168698649252615

Epoch: 6| Step: 12
Training loss: 2.773829936981201
Validation loss: 2.6152404405737437

Epoch: 6| Step: 13
Training loss: 1.9325597286224365
Validation loss: 2.623035430908203

Epoch: 40| Step: 0
Training loss: 2.8886265754699707
Validation loss: 2.6115373129485757

Epoch: 6| Step: 1
Training loss: 2.5517020225524902
Validation loss: 2.6341057618459067

Epoch: 6| Step: 2
Training loss: 3.283266067504883
Validation loss: 2.627422071272327

Epoch: 6| Step: 3
Training loss: 2.7894608974456787
Validation loss: 2.626461298235001

Epoch: 6| Step: 4
Training loss: 3.149613618850708
Validation loss: 2.622306610948296

Epoch: 6| Step: 5
Training loss: 2.7917518615722656
Validation loss: 2.61592859350225

Epoch: 6| Step: 6
Training loss: 3.033411979675293
Validation loss: 2.6107049834343696

Epoch: 6| Step: 7
Training loss: 2.9542994499206543
Validation loss: 2.6157052952756166

Epoch: 6| Step: 8
Training loss: 3.083456039428711
Validation loss: 2.609778383726715

Epoch: 6| Step: 9
Training loss: 1.5493426322937012
Validation loss: 2.6087200180176766

Epoch: 6| Step: 10
Training loss: 3.2323622703552246
Validation loss: 2.608906856147192

Epoch: 6| Step: 11
Training loss: 2.8875865936279297
Validation loss: 2.603163150049025

Epoch: 6| Step: 12
Training loss: 2.193599224090576
Validation loss: 2.603247134916244

Epoch: 6| Step: 13
Training loss: 3.0798635482788086
Validation loss: 2.6080056364818285

Epoch: 41| Step: 0
Training loss: 3.6943042278289795
Validation loss: 2.5989010231469267

Epoch: 6| Step: 1
Training loss: 2.3434762954711914
Validation loss: 2.6139632886455906

Epoch: 6| Step: 2
Training loss: 2.2438769340515137
Validation loss: 2.609687448829733

Epoch: 6| Step: 3
Training loss: 2.3071656227111816
Validation loss: 2.5969278761135635

Epoch: 6| Step: 4
Training loss: 3.1380841732025146
Validation loss: 2.5953702542089645

Epoch: 6| Step: 5
Training loss: 2.673839569091797
Validation loss: 2.5876359580665507

Epoch: 6| Step: 6
Training loss: 3.0521297454833984
Validation loss: 2.6061922965511197

Epoch: 6| Step: 7
Training loss: 2.979639768600464
Validation loss: 2.6040886755912536

Epoch: 6| Step: 8
Training loss: 2.7609381675720215
Validation loss: 2.60126426143031

Epoch: 6| Step: 9
Training loss: 1.9273974895477295
Validation loss: 2.59498272403594

Epoch: 6| Step: 10
Training loss: 2.7545242309570312
Validation loss: 2.590505528193648

Epoch: 6| Step: 11
Training loss: 2.9117026329040527
Validation loss: 2.5996582174813874

Epoch: 6| Step: 12
Training loss: 3.2458982467651367
Validation loss: 2.5981102348655782

Epoch: 6| Step: 13
Training loss: 2.825218915939331
Validation loss: 2.6082852309749973

Epoch: 42| Step: 0
Training loss: 1.9712669849395752
Validation loss: 2.6111134226604173

Epoch: 6| Step: 1
Training loss: 2.843775749206543
Validation loss: 2.617850524122997

Epoch: 6| Step: 2
Training loss: 2.756978988647461
Validation loss: 2.6237600182974212

Epoch: 6| Step: 3
Training loss: 2.2744863033294678
Validation loss: 2.61445455397329

Epoch: 6| Step: 4
Training loss: 2.525660276412964
Validation loss: 2.6170110369241364

Epoch: 6| Step: 5
Training loss: 2.7601754665374756
Validation loss: 2.614117540338988

Epoch: 6| Step: 6
Training loss: 2.9708914756774902
Validation loss: 2.598526382959017

Epoch: 6| Step: 7
Training loss: 2.8937511444091797
Validation loss: 2.595850788136964

Epoch: 6| Step: 8
Training loss: 3.0090365409851074
Validation loss: 2.592017947986562

Epoch: 6| Step: 9
Training loss: 2.722666025161743
Validation loss: 2.5882097854409167

Epoch: 6| Step: 10
Training loss: 3.067920207977295
Validation loss: 2.588875388586393

Epoch: 6| Step: 11
Training loss: 2.3941240310668945
Validation loss: 2.585813763321087

Epoch: 6| Step: 12
Training loss: 3.9483444690704346
Validation loss: 2.591549478551393

Epoch: 6| Step: 13
Training loss: 2.3304030895233154
Validation loss: 2.5850794674247823

Epoch: 43| Step: 0
Training loss: 2.7725377082824707
Validation loss: 2.5818383719331477

Epoch: 6| Step: 1
Training loss: 2.674656867980957
Validation loss: 2.5864494897985972

Epoch: 6| Step: 2
Training loss: 3.0074939727783203
Validation loss: 2.5915644681581886

Epoch: 6| Step: 3
Training loss: 2.578500509262085
Validation loss: 2.5996106927112868

Epoch: 6| Step: 4
Training loss: 2.4132115840911865
Validation loss: 2.592425231010683

Epoch: 6| Step: 5
Training loss: 2.9649722576141357
Validation loss: 2.5865849218060895

Epoch: 6| Step: 6
Training loss: 3.248837947845459
Validation loss: 2.5805146514728503

Epoch: 6| Step: 7
Training loss: 2.829233407974243
Validation loss: 2.579310576121012

Epoch: 6| Step: 8
Training loss: 2.587106943130493
Validation loss: 2.5777149277348674

Epoch: 6| Step: 9
Training loss: 2.93129563331604
Validation loss: 2.579376538594564

Epoch: 6| Step: 10
Training loss: 2.358780860900879
Validation loss: 2.576472072191136

Epoch: 6| Step: 11
Training loss: 2.4219844341278076
Validation loss: 2.5737873200447328

Epoch: 6| Step: 12
Training loss: 2.8911564350128174
Validation loss: 2.59136450675226

Epoch: 6| Step: 13
Training loss: 2.922363758087158
Validation loss: 2.6236973270293205

Epoch: 44| Step: 0
Training loss: 2.2370030879974365
Validation loss: 2.6642420022718367

Epoch: 6| Step: 1
Training loss: 2.825376033782959
Validation loss: 2.6998360618468253

Epoch: 6| Step: 2
Training loss: 2.4509878158569336
Validation loss: 2.718257027287637

Epoch: 6| Step: 3
Training loss: 2.2534680366516113
Validation loss: 2.6551848714069655

Epoch: 6| Step: 4
Training loss: 2.489535331726074
Validation loss: 2.585552297612672

Epoch: 6| Step: 5
Training loss: 2.9149136543273926
Validation loss: 2.5721586160762335

Epoch: 6| Step: 6
Training loss: 2.8799874782562256
Validation loss: 2.5678394968791673

Epoch: 6| Step: 7
Training loss: 2.9470863342285156
Validation loss: 2.5680685709881526

Epoch: 6| Step: 8
Training loss: 2.2531399726867676
Validation loss: 2.5708224850316204

Epoch: 6| Step: 9
Training loss: 3.2829160690307617
Validation loss: 2.5719863650619343

Epoch: 6| Step: 10
Training loss: 3.0034196376800537
Validation loss: 2.571500885871149

Epoch: 6| Step: 11
Training loss: 2.8625986576080322
Validation loss: 2.569202797387236

Epoch: 6| Step: 12
Training loss: 3.2033891677856445
Validation loss: 2.56578657960379

Epoch: 6| Step: 13
Training loss: 3.523538827896118
Validation loss: 2.561519589475406

Epoch: 45| Step: 0
Training loss: 2.98264217376709
Validation loss: 2.558446853391586

Epoch: 6| Step: 1
Training loss: 2.9323580265045166
Validation loss: 2.5712701902594617

Epoch: 6| Step: 2
Training loss: 3.4547863006591797
Validation loss: 2.5863178647974485

Epoch: 6| Step: 3
Training loss: 2.846923351287842
Validation loss: 2.5974432012086273

Epoch: 6| Step: 4
Training loss: 3.3882267475128174
Validation loss: 2.6218290303343084

Epoch: 6| Step: 5
Training loss: 1.6998684406280518
Validation loss: 2.6329676156402915

Epoch: 6| Step: 6
Training loss: 2.9124693870544434
Validation loss: 2.6543428641493603

Epoch: 6| Step: 7
Training loss: 3.604536771774292
Validation loss: 2.674361087942636

Epoch: 6| Step: 8
Training loss: 2.8541295528411865
Validation loss: 2.68019074778403

Epoch: 6| Step: 9
Training loss: 2.0576765537261963
Validation loss: 2.696880225212343

Epoch: 6| Step: 10
Training loss: 2.9395594596862793
Validation loss: 2.7014247960941766

Epoch: 6| Step: 11
Training loss: 2.5071797370910645
Validation loss: 2.6777470598938646

Epoch: 6| Step: 12
Training loss: 2.16896390914917
Validation loss: 2.628664944761543

Epoch: 6| Step: 13
Training loss: 2.164214611053467
Validation loss: 2.5925206612515193

Epoch: 46| Step: 0
Training loss: 2.210275173187256
Validation loss: 2.5558685589862127

Epoch: 6| Step: 1
Training loss: 3.104663610458374
Validation loss: 2.556433662291496

Epoch: 6| Step: 2
Training loss: 1.6459250450134277
Validation loss: 2.5729949705062376

Epoch: 6| Step: 3
Training loss: 3.437756061553955
Validation loss: 2.602648896555747

Epoch: 6| Step: 4
Training loss: 3.3476312160491943
Validation loss: 2.6178608273947113

Epoch: 6| Step: 5
Training loss: 2.2559380531311035
Validation loss: 2.631599990270471

Epoch: 6| Step: 6
Training loss: 2.5175070762634277
Validation loss: 2.626709453521236

Epoch: 6| Step: 7
Training loss: 3.6662158966064453
Validation loss: 2.6210248675397647

Epoch: 6| Step: 8
Training loss: 3.0603487491607666
Validation loss: 2.591394470584008

Epoch: 6| Step: 9
Training loss: 2.3296618461608887
Validation loss: 2.5670296735661005

Epoch: 6| Step: 10
Training loss: 2.769338846206665
Validation loss: 2.5849665236729447

Epoch: 6| Step: 11
Training loss: 1.500635027885437
Validation loss: 2.6137606738716044

Epoch: 6| Step: 12
Training loss: 4.276279926300049
Validation loss: 2.632814150984569

Epoch: 6| Step: 13
Training loss: 2.7862086296081543
Validation loss: 2.634574444063248

Epoch: 47| Step: 0
Training loss: 2.767514705657959
Validation loss: 2.6149604628163

Epoch: 6| Step: 1
Training loss: 2.8562629222869873
Validation loss: 2.5968122392572384

Epoch: 6| Step: 2
Training loss: 2.50083065032959
Validation loss: 2.593766907209991

Epoch: 6| Step: 3
Training loss: 1.9264812469482422
Validation loss: 2.581979851568899

Epoch: 6| Step: 4
Training loss: 3.2630720138549805
Validation loss: 2.582900195993403

Epoch: 6| Step: 5
Training loss: 2.4799113273620605
Validation loss: 2.5776937469359367

Epoch: 6| Step: 6
Training loss: 2.739997625350952
Validation loss: 2.5663543875499437

Epoch: 6| Step: 7
Training loss: 2.344533681869507
Validation loss: 2.5746891498565674

Epoch: 6| Step: 8
Training loss: 3.4226303100585938
Validation loss: 2.5811319710105978

Epoch: 6| Step: 9
Training loss: 3.750117778778076
Validation loss: 2.5806497168797318

Epoch: 6| Step: 10
Training loss: 2.347322940826416
Validation loss: 2.541604518890381

Epoch: 6| Step: 11
Training loss: 3.2775158882141113
Validation loss: 2.531632702837708

Epoch: 6| Step: 12
Training loss: 1.9585566520690918
Validation loss: 2.536159610235563

Epoch: 6| Step: 13
Training loss: 2.6029462814331055
Validation loss: 2.5394452925651305

Epoch: 48| Step: 0
Training loss: 2.687943696975708
Validation loss: 2.5397549547174925

Epoch: 6| Step: 1
Training loss: 3.2170114517211914
Validation loss: 2.539380265820411

Epoch: 6| Step: 2
Training loss: 2.163747787475586
Validation loss: 2.5381780542353147

Epoch: 6| Step: 3
Training loss: 2.1002330780029297
Validation loss: 2.5385015062106553

Epoch: 6| Step: 4
Training loss: 2.6214547157287598
Validation loss: 2.5525935862653997

Epoch: 6| Step: 5
Training loss: 2.534308910369873
Validation loss: 2.545108946420813

Epoch: 6| Step: 6
Training loss: 2.72463321685791
Validation loss: 2.5460789742008334

Epoch: 6| Step: 7
Training loss: 3.1552906036376953
Validation loss: 2.551088527966571

Epoch: 6| Step: 8
Training loss: 2.3858773708343506
Validation loss: 2.5537197512965046

Epoch: 6| Step: 9
Training loss: 2.9975602626800537
Validation loss: 2.5566268044133342

Epoch: 6| Step: 10
Training loss: 2.9546451568603516
Validation loss: 2.552950979560934

Epoch: 6| Step: 11
Training loss: 3.016326665878296
Validation loss: 2.5453066479775215

Epoch: 6| Step: 12
Training loss: 2.7130398750305176
Validation loss: 2.5428808658353743

Epoch: 6| Step: 13
Training loss: 3.1110360622406006
Validation loss: 2.5403117261907107

Epoch: 49| Step: 0
Training loss: 2.881603717803955
Validation loss: 2.5312362050497406

Epoch: 6| Step: 1
Training loss: 2.131626844406128
Validation loss: 2.5293984464419785

Epoch: 6| Step: 2
Training loss: 3.1942057609558105
Validation loss: 2.5299480986851517

Epoch: 6| Step: 3
Training loss: 2.160619020462036
Validation loss: 2.5271742061902116

Epoch: 6| Step: 4
Training loss: 2.1136538982391357
Validation loss: 2.5332331606136855

Epoch: 6| Step: 5
Training loss: 3.1744158267974854
Validation loss: 2.5304524744710615

Epoch: 6| Step: 6
Training loss: 2.9281132221221924
Validation loss: 2.531031125335283

Epoch: 6| Step: 7
Training loss: 2.925779342651367
Validation loss: 2.536248819802397

Epoch: 6| Step: 8
Training loss: 2.9197278022766113
Validation loss: 2.5364323469900314

Epoch: 6| Step: 9
Training loss: 2.728212833404541
Validation loss: 2.5340998557306107

Epoch: 6| Step: 10
Training loss: 2.8105435371398926
Validation loss: 2.5209951170029177

Epoch: 6| Step: 11
Training loss: 2.644312620162964
Validation loss: 2.5186531723186536

Epoch: 6| Step: 12
Training loss: 2.5574536323547363
Validation loss: 2.5177095628553823

Epoch: 6| Step: 13
Training loss: 2.9396822452545166
Validation loss: 2.515017273605511

Epoch: 50| Step: 0
Training loss: 2.45589017868042
Validation loss: 2.519449536518384

Epoch: 6| Step: 1
Training loss: 3.1929783821105957
Validation loss: 2.519856478578301

Epoch: 6| Step: 2
Training loss: 2.573488235473633
Validation loss: 2.5202616337806947

Epoch: 6| Step: 3
Training loss: 2.385688304901123
Validation loss: 2.5171541808753886

Epoch: 6| Step: 4
Training loss: 2.836884021759033
Validation loss: 2.5119224312484905

Epoch: 6| Step: 5
Training loss: 2.5098423957824707
Validation loss: 2.5127823686087005

Epoch: 6| Step: 6
Training loss: 3.0511550903320312
Validation loss: 2.5176048355717815

Epoch: 6| Step: 7
Training loss: 1.992374300956726
Validation loss: 2.524784593171971

Epoch: 6| Step: 8
Training loss: 3.0715441703796387
Validation loss: 2.5295927345111804

Epoch: 6| Step: 9
Training loss: 2.4629406929016113
Validation loss: 2.5344988069226666

Epoch: 6| Step: 10
Training loss: 2.9709692001342773
Validation loss: 2.5447738708988314

Epoch: 6| Step: 11
Training loss: 2.543609619140625
Validation loss: 2.544021826918407

Epoch: 6| Step: 12
Training loss: 3.253075122833252
Validation loss: 2.538583988784462

Epoch: 6| Step: 13
Training loss: 2.6947450637817383
Validation loss: 2.5372901296102874

Epoch: 51| Step: 0
Training loss: 3.0017027854919434
Validation loss: 2.5339559329453336

Epoch: 6| Step: 1
Training loss: 2.38368821144104
Validation loss: 2.542678456152639

Epoch: 6| Step: 2
Training loss: 2.7148256301879883
Validation loss: 2.544945240020752

Epoch: 6| Step: 3
Training loss: 2.489417791366577
Validation loss: 2.539529300505115

Epoch: 6| Step: 4
Training loss: 2.3560643196105957
Validation loss: 2.547118863751811

Epoch: 6| Step: 5
Training loss: 2.708970546722412
Validation loss: 2.5326456433983258

Epoch: 6| Step: 6
Training loss: 3.515604019165039
Validation loss: 2.5209243553940968

Epoch: 6| Step: 7
Training loss: 2.950864315032959
Validation loss: 2.5168102120840423

Epoch: 6| Step: 8
Training loss: 2.138162136077881
Validation loss: 2.5066883205085673

Epoch: 6| Step: 9
Training loss: 3.1144490242004395
Validation loss: 2.5103818780632428

Epoch: 6| Step: 10
Training loss: 3.016242027282715
Validation loss: 2.5035340221979285

Epoch: 6| Step: 11
Training loss: 2.178128242492676
Validation loss: 2.5030827983733146

Epoch: 6| Step: 12
Training loss: 2.910273551940918
Validation loss: 2.504189304126206

Epoch: 6| Step: 13
Training loss: 2.0225558280944824
Validation loss: 2.505253194480814

Epoch: 52| Step: 0
Training loss: 3.4048547744750977
Validation loss: 2.5015202337695706

Epoch: 6| Step: 1
Training loss: 1.732292652130127
Validation loss: 2.49853229266341

Epoch: 6| Step: 2
Training loss: 2.873945713043213
Validation loss: 2.507298110633768

Epoch: 6| Step: 3
Training loss: 2.1149775981903076
Validation loss: 2.5121126918382544

Epoch: 6| Step: 4
Training loss: 2.334329605102539
Validation loss: 2.514382505929598

Epoch: 6| Step: 5
Training loss: 2.9057772159576416
Validation loss: 2.516888200595815

Epoch: 6| Step: 6
Training loss: 3.0195205211639404
Validation loss: 2.5376033859868206

Epoch: 6| Step: 7
Training loss: 2.4256584644317627
Validation loss: 2.5067162026641188

Epoch: 6| Step: 8
Training loss: 3.3708605766296387
Validation loss: 2.5176274289367018

Epoch: 6| Step: 9
Training loss: 3.16043758392334
Validation loss: 2.505435013001965

Epoch: 6| Step: 10
Training loss: 2.805443048477173
Validation loss: 2.5057020443741993

Epoch: 6| Step: 11
Training loss: 2.7359375953674316
Validation loss: 2.4957461228934665

Epoch: 6| Step: 12
Training loss: 2.627139091491699
Validation loss: 2.4925712359848844

Epoch: 6| Step: 13
Training loss: 1.8489447832107544
Validation loss: 2.4940986120572655

Epoch: 53| Step: 0
Training loss: 2.1041860580444336
Validation loss: 2.4963708436617287

Epoch: 6| Step: 1
Training loss: 3.106013536453247
Validation loss: 2.4993008285440426

Epoch: 6| Step: 2
Training loss: 2.294971466064453
Validation loss: 2.491057485662481

Epoch: 6| Step: 3
Training loss: 2.935093402862549
Validation loss: 2.4977754623659196

Epoch: 6| Step: 4
Training loss: 2.4894533157348633
Validation loss: 2.49872753953421

Epoch: 6| Step: 5
Training loss: 2.339613914489746
Validation loss: 2.4915318232710644

Epoch: 6| Step: 6
Training loss: 2.183806896209717
Validation loss: 2.493754986793764

Epoch: 6| Step: 7
Training loss: 2.51424241065979
Validation loss: 2.491951027224141

Epoch: 6| Step: 8
Training loss: 3.251298189163208
Validation loss: 2.492546259715993

Epoch: 6| Step: 9
Training loss: 3.549921989440918
Validation loss: 2.4892354088444866

Epoch: 6| Step: 10
Training loss: 2.25795578956604
Validation loss: 2.489873919435727

Epoch: 6| Step: 11
Training loss: 2.9979910850524902
Validation loss: 2.489809754074261

Epoch: 6| Step: 12
Training loss: 2.2047762870788574
Validation loss: 2.491596988452378

Epoch: 6| Step: 13
Training loss: 4.142499923706055
Validation loss: 2.4955994467581473

Epoch: 54| Step: 0
Training loss: 3.2513718605041504
Validation loss: 2.494671211447767

Epoch: 6| Step: 1
Training loss: 3.270507574081421
Validation loss: 2.4987959554118495

Epoch: 6| Step: 2
Training loss: 2.736971855163574
Validation loss: 2.5002791343196744

Epoch: 6| Step: 3
Training loss: 2.320479393005371
Validation loss: 2.501357745098811

Epoch: 6| Step: 4
Training loss: 2.571068525314331
Validation loss: 2.502910008994482

Epoch: 6| Step: 5
Training loss: 2.951376438140869
Validation loss: 2.494340628705999

Epoch: 6| Step: 6
Training loss: 2.3930680751800537
Validation loss: 2.4918278032733547

Epoch: 6| Step: 7
Training loss: 2.8728394508361816
Validation loss: 2.5001004049854894

Epoch: 6| Step: 8
Training loss: 1.619049072265625
Validation loss: 2.4991706161088842

Epoch: 6| Step: 9
Training loss: 3.6320419311523438
Validation loss: 2.4973618189493814

Epoch: 6| Step: 10
Training loss: 2.5468151569366455
Validation loss: 2.4959633837464037

Epoch: 6| Step: 11
Training loss: 2.0097436904907227
Validation loss: 2.4932764166144916

Epoch: 6| Step: 12
Training loss: 3.2612991333007812
Validation loss: 2.4950559933980307

Epoch: 6| Step: 13
Training loss: 1.651852011680603
Validation loss: 2.5163052825517553

Epoch: 55| Step: 0
Training loss: 2.497143030166626
Validation loss: 2.527668188976985

Epoch: 6| Step: 1
Training loss: 3.3687667846679688
Validation loss: 2.5018071013112224

Epoch: 6| Step: 2
Training loss: 3.2134196758270264
Validation loss: 2.498198693798434

Epoch: 6| Step: 3
Training loss: 1.966876745223999
Validation loss: 2.481988263386552

Epoch: 6| Step: 4
Training loss: 2.565032482147217
Validation loss: 2.4757900109855075

Epoch: 6| Step: 5
Training loss: 2.786228656768799
Validation loss: 2.470633796466294

Epoch: 6| Step: 6
Training loss: 2.3868627548217773
Validation loss: 2.475964323166878

Epoch: 6| Step: 7
Training loss: 3.1052379608154297
Validation loss: 2.4751356827315463

Epoch: 6| Step: 8
Training loss: 2.832014560699463
Validation loss: 2.4772492275443128

Epoch: 6| Step: 9
Training loss: 3.528583288192749
Validation loss: 2.4774975007580173

Epoch: 6| Step: 10
Training loss: 2.4922187328338623
Validation loss: 2.4715047523539555

Epoch: 6| Step: 11
Training loss: 2.7219367027282715
Validation loss: 2.4776500296849076

Epoch: 6| Step: 12
Training loss: 1.8598706722259521
Validation loss: 2.471930411554152

Epoch: 6| Step: 13
Training loss: 2.3067097663879395
Validation loss: 2.471505452227849

Epoch: 56| Step: 0
Training loss: 2.697861671447754
Validation loss: 2.4726685195840816

Epoch: 6| Step: 1
Training loss: 2.94797420501709
Validation loss: 2.47247100645496

Epoch: 6| Step: 2
Training loss: 2.969909191131592
Validation loss: 2.469756798077655

Epoch: 6| Step: 3
Training loss: 2.1538023948669434
Validation loss: 2.4711046526508946

Epoch: 6| Step: 4
Training loss: 3.102647304534912
Validation loss: 2.4698329305136077

Epoch: 6| Step: 5
Training loss: 2.5249152183532715
Validation loss: 2.469878578698763

Epoch: 6| Step: 6
Training loss: 2.7323780059814453
Validation loss: 2.466464683573733

Epoch: 6| Step: 7
Training loss: 1.9042422771453857
Validation loss: 2.4627862617533696

Epoch: 6| Step: 8
Training loss: 2.4211015701293945
Validation loss: 2.4728175696506294

Epoch: 6| Step: 9
Training loss: 2.639021396636963
Validation loss: 2.475172145392305

Epoch: 6| Step: 10
Training loss: 3.334617853164673
Validation loss: 2.4759646718220045

Epoch: 6| Step: 11
Training loss: 2.426754951477051
Validation loss: 2.4873519430878344

Epoch: 6| Step: 12
Training loss: 3.0030109882354736
Validation loss: 2.483854862951463

Epoch: 6| Step: 13
Training loss: 2.4536936283111572
Validation loss: 2.503173238487654

Epoch: 57| Step: 0
Training loss: 2.682513475418091
Validation loss: 2.517258495412847

Epoch: 6| Step: 1
Training loss: 2.980712890625
Validation loss: 2.502980968003632

Epoch: 6| Step: 2
Training loss: 2.586409330368042
Validation loss: 2.4988754180169876

Epoch: 6| Step: 3
Training loss: 2.739246368408203
Validation loss: 2.4910199949818272

Epoch: 6| Step: 4
Training loss: 2.4237751960754395
Validation loss: 2.4773800578168643

Epoch: 6| Step: 5
Training loss: 2.005557060241699
Validation loss: 2.4682784593233498

Epoch: 6| Step: 6
Training loss: 2.562055826187134
Validation loss: 2.471517029628959

Epoch: 6| Step: 7
Training loss: 2.9684364795684814
Validation loss: 2.476461564340899

Epoch: 6| Step: 8
Training loss: 2.3619165420532227
Validation loss: 2.474870445907757

Epoch: 6| Step: 9
Training loss: 2.83553147315979
Validation loss: 2.4809114651013444

Epoch: 6| Step: 10
Training loss: 2.715524196624756
Validation loss: 2.4806964705067296

Epoch: 6| Step: 11
Training loss: 2.69502592086792
Validation loss: 2.490435984826857

Epoch: 6| Step: 12
Training loss: 3.0351462364196777
Validation loss: 2.487663725371002

Epoch: 6| Step: 13
Training loss: 3.137270450592041
Validation loss: 2.4934100566371793

Epoch: 58| Step: 0
Training loss: 2.8579063415527344
Validation loss: 2.4912746696061987

Epoch: 6| Step: 1
Training loss: 2.11384654045105
Validation loss: 2.4851965391507713

Epoch: 6| Step: 2
Training loss: 2.0060839653015137
Validation loss: 2.4738827225982503

Epoch: 6| Step: 3
Training loss: 2.6804943084716797
Validation loss: 2.4757412172132924

Epoch: 6| Step: 4
Training loss: 2.8762221336364746
Validation loss: 2.4727389709923857

Epoch: 6| Step: 5
Training loss: 3.3333919048309326
Validation loss: 2.466813207954489

Epoch: 6| Step: 6
Training loss: 1.8024510145187378
Validation loss: 2.4703728921951784

Epoch: 6| Step: 7
Training loss: 3.492020606994629
Validation loss: 2.466753880182902

Epoch: 6| Step: 8
Training loss: 2.872455358505249
Validation loss: 2.4698294849805933

Epoch: 6| Step: 9
Training loss: 2.584900140762329
Validation loss: 2.4683260917663574

Epoch: 6| Step: 10
Training loss: 2.4514198303222656
Validation loss: 2.459160194602064

Epoch: 6| Step: 11
Training loss: 2.195003032684326
Validation loss: 2.4611973634330173

Epoch: 6| Step: 12
Training loss: 3.0173065662384033
Validation loss: 2.4633583843067126

Epoch: 6| Step: 13
Training loss: 3.5326826572418213
Validation loss: 2.466866657298098

Epoch: 59| Step: 0
Training loss: 2.2455992698669434
Validation loss: 2.47117357612938

Epoch: 6| Step: 1
Training loss: 2.523193359375
Validation loss: 2.4878374017694944

Epoch: 6| Step: 2
Training loss: 2.4866209030151367
Validation loss: 2.505322733233052

Epoch: 6| Step: 3
Training loss: 3.2485501766204834
Validation loss: 2.4887069374002437

Epoch: 6| Step: 4
Training loss: 1.5339163541793823
Validation loss: 2.48148710496964

Epoch: 6| Step: 5
Training loss: 3.506052017211914
Validation loss: 2.4657042616157123

Epoch: 6| Step: 6
Training loss: 2.664069652557373
Validation loss: 2.4633453661395657

Epoch: 6| Step: 7
Training loss: 2.6650118827819824
Validation loss: 2.4609377922550326

Epoch: 6| Step: 8
Training loss: 2.6531336307525635
Validation loss: 2.4605434812525266

Epoch: 6| Step: 9
Training loss: 2.8758087158203125
Validation loss: 2.461546731251542

Epoch: 6| Step: 10
Training loss: 2.1408469676971436
Validation loss: 2.4670030173435005

Epoch: 6| Step: 11
Training loss: 3.477386474609375
Validation loss: 2.461671167804349

Epoch: 6| Step: 12
Training loss: 2.626101493835449
Validation loss: 2.4608276633806128

Epoch: 6| Step: 13
Training loss: 2.7854528427124023
Validation loss: 2.468862638678602

Epoch: 60| Step: 0
Training loss: 2.8786258697509766
Validation loss: 2.468171937491304

Epoch: 6| Step: 1
Training loss: 2.341639518737793
Validation loss: 2.4856767564691524

Epoch: 6| Step: 2
Training loss: 3.7604801654815674
Validation loss: 2.511329855970157

Epoch: 6| Step: 3
Training loss: 2.636812210083008
Validation loss: 2.521045213104576

Epoch: 6| Step: 4
Training loss: 2.63535737991333
Validation loss: 2.529381336704377

Epoch: 6| Step: 5
Training loss: 2.5416276454925537
Validation loss: 2.519953607231058

Epoch: 6| Step: 6
Training loss: 2.417381763458252
Validation loss: 2.5197165166178057

Epoch: 6| Step: 7
Training loss: 2.231466054916382
Validation loss: 2.475635592655469

Epoch: 6| Step: 8
Training loss: 2.727844715118408
Validation loss: 2.479696773713635

Epoch: 6| Step: 9
Training loss: 2.542623996734619
Validation loss: 2.4643066416504564

Epoch: 6| Step: 10
Training loss: 2.6200592517852783
Validation loss: 2.4519545955042683

Epoch: 6| Step: 11
Training loss: 2.8313326835632324
Validation loss: 2.4450483757962465

Epoch: 6| Step: 12
Training loss: 2.4051294326782227
Validation loss: 2.4446282822598695

Epoch: 6| Step: 13
Training loss: 3.1057727336883545
Validation loss: 2.442005331798266

Epoch: 61| Step: 0
Training loss: 2.160945415496826
Validation loss: 2.44900889935032

Epoch: 6| Step: 1
Training loss: 2.6602983474731445
Validation loss: 2.446404500674176

Epoch: 6| Step: 2
Training loss: 3.049015760421753
Validation loss: 2.4480717259068645

Epoch: 6| Step: 3
Training loss: 2.1320912837982178
Validation loss: 2.451071012404657

Epoch: 6| Step: 4
Training loss: 3.169589042663574
Validation loss: 2.449002447948661

Epoch: 6| Step: 5
Training loss: 2.7090530395507812
Validation loss: 2.4499347799567768

Epoch: 6| Step: 6
Training loss: 3.1528711318969727
Validation loss: 2.448538157247728

Epoch: 6| Step: 7
Training loss: 2.5039474964141846
Validation loss: 2.4596811853429323

Epoch: 6| Step: 8
Training loss: 2.2193150520324707
Validation loss: 2.481857774078205

Epoch: 6| Step: 9
Training loss: 2.9928078651428223
Validation loss: 2.4984414628756944

Epoch: 6| Step: 10
Training loss: 2.3227007389068604
Validation loss: 2.5173047306717082

Epoch: 6| Step: 11
Training loss: 2.6745686531066895
Validation loss: 2.5417987941413798

Epoch: 6| Step: 12
Training loss: 3.061673164367676
Validation loss: 2.577410750491645

Epoch: 6| Step: 13
Training loss: 2.8556346893310547
Validation loss: 2.5632688383902273

Epoch: 62| Step: 0
Training loss: 2.2037525177001953
Validation loss: 2.5458894852669007

Epoch: 6| Step: 1
Training loss: 2.9094009399414062
Validation loss: 2.5364720821380615

Epoch: 6| Step: 2
Training loss: 2.9570584297180176
Validation loss: 2.5467956963405816

Epoch: 6| Step: 3
Training loss: 2.2758896350860596
Validation loss: 2.5270736730226906

Epoch: 6| Step: 4
Training loss: 1.9368979930877686
Validation loss: 2.5180299717892884

Epoch: 6| Step: 5
Training loss: 2.8027446269989014
Validation loss: 2.5094639280790925

Epoch: 6| Step: 6
Training loss: 2.8438053131103516
Validation loss: 2.5036029328582106

Epoch: 6| Step: 7
Training loss: 2.919766426086426
Validation loss: 2.489038357170679

Epoch: 6| Step: 8
Training loss: 3.1877224445343018
Validation loss: 2.490028653093564

Epoch: 6| Step: 9
Training loss: 3.425337553024292
Validation loss: 2.4927967568879486

Epoch: 6| Step: 10
Training loss: 2.342235803604126
Validation loss: 2.4882931401652675

Epoch: 6| Step: 11
Training loss: 2.6071219444274902
Validation loss: 2.4810236756519606

Epoch: 6| Step: 12
Training loss: 2.134523630142212
Validation loss: 2.4762606595152166

Epoch: 6| Step: 13
Training loss: 3.4415793418884277
Validation loss: 2.4665412774649997

Epoch: 63| Step: 0
Training loss: 2.4160754680633545
Validation loss: 2.458731876906528

Epoch: 6| Step: 1
Training loss: 2.678722858428955
Validation loss: 2.4535090026035102

Epoch: 6| Step: 2
Training loss: 2.2914276123046875
Validation loss: 2.441720308796052

Epoch: 6| Step: 3
Training loss: 3.4870715141296387
Validation loss: 2.4321202437082925

Epoch: 6| Step: 4
Training loss: 2.827482223510742
Validation loss: 2.431611171332739

Epoch: 6| Step: 5
Training loss: 3.0485963821411133
Validation loss: 2.4348499672387236

Epoch: 6| Step: 6
Training loss: 2.177269458770752
Validation loss: 2.4532380206610567

Epoch: 6| Step: 7
Training loss: 3.3512606620788574
Validation loss: 2.4601920497032905

Epoch: 6| Step: 8
Training loss: 2.9655838012695312
Validation loss: 2.45912084784559

Epoch: 6| Step: 9
Training loss: 2.3014514446258545
Validation loss: 2.4631350142981416

Epoch: 6| Step: 10
Training loss: 2.800309419631958
Validation loss: 2.464438948580014

Epoch: 6| Step: 11
Training loss: 2.884693145751953
Validation loss: 2.4779329684472855

Epoch: 6| Step: 12
Training loss: 1.7142789363861084
Validation loss: 2.442822274341378

Epoch: 6| Step: 13
Training loss: 2.2136073112487793
Validation loss: 2.4202765226364136

Epoch: 64| Step: 0
Training loss: 2.6430459022521973
Validation loss: 2.4237677897176435

Epoch: 6| Step: 1
Training loss: 2.49560809135437
Validation loss: 2.437081442084364

Epoch: 6| Step: 2
Training loss: 2.124540090560913
Validation loss: 2.4433194693698677

Epoch: 6| Step: 3
Training loss: 2.5023465156555176
Validation loss: 2.4480925990689184

Epoch: 6| Step: 4
Training loss: 3.0106215476989746
Validation loss: 2.4465284937171528

Epoch: 6| Step: 5
Training loss: 2.0126521587371826
Validation loss: 2.428933423052552

Epoch: 6| Step: 6
Training loss: 3.026012897491455
Validation loss: 2.4267751247652116

Epoch: 6| Step: 7
Training loss: 2.6739888191223145
Validation loss: 2.4210591905860492

Epoch: 6| Step: 8
Training loss: 2.0573043823242188
Validation loss: 2.42146820663124

Epoch: 6| Step: 9
Training loss: 2.958737850189209
Validation loss: 2.428494313711761

Epoch: 6| Step: 10
Training loss: 3.1515965461730957
Validation loss: 2.443935026404678

Epoch: 6| Step: 11
Training loss: 2.5802879333496094
Validation loss: 2.4367400971792077

Epoch: 6| Step: 12
Training loss: 3.4411351680755615
Validation loss: 2.4378243569404847

Epoch: 6| Step: 13
Training loss: 2.9328324794769287
Validation loss: 2.442245239852577

Epoch: 65| Step: 0
Training loss: 2.81168794631958
Validation loss: 2.4369162539000153

Epoch: 6| Step: 1
Training loss: 2.8257946968078613
Validation loss: 2.432863248291836

Epoch: 6| Step: 2
Training loss: 2.7457163333892822
Validation loss: 2.435763000160135

Epoch: 6| Step: 3
Training loss: 2.396026134490967
Validation loss: 2.42514459548458

Epoch: 6| Step: 4
Training loss: 2.980717420578003
Validation loss: 2.4212756900377173

Epoch: 6| Step: 5
Training loss: 2.8069305419921875
Validation loss: 2.415721934328797

Epoch: 6| Step: 6
Training loss: 2.1013169288635254
Validation loss: 2.408166919985125

Epoch: 6| Step: 7
Training loss: 2.316035509109497
Validation loss: 2.405605862217565

Epoch: 6| Step: 8
Training loss: 3.853752851486206
Validation loss: 2.4151159781281666

Epoch: 6| Step: 9
Training loss: 1.9408462047576904
Validation loss: 2.4168935450174476

Epoch: 6| Step: 10
Training loss: 2.60457706451416
Validation loss: 2.424374347092003

Epoch: 6| Step: 11
Training loss: 2.183955192565918
Validation loss: 2.414391868857927

Epoch: 6| Step: 12
Training loss: 2.5253541469573975
Validation loss: 2.42393756938237

Epoch: 6| Step: 13
Training loss: 3.3407199382781982
Validation loss: 2.4239184138595418

Epoch: 66| Step: 0
Training loss: 2.716456890106201
Validation loss: 2.4277927696063952

Epoch: 6| Step: 1
Training loss: 1.9322600364685059
Validation loss: 2.426671948484195

Epoch: 6| Step: 2
Training loss: 2.85457706451416
Validation loss: 2.447567355248236

Epoch: 6| Step: 3
Training loss: 2.6075408458709717
Validation loss: 2.454810678317983

Epoch: 6| Step: 4
Training loss: 3.333510398864746
Validation loss: 2.4696564956377913

Epoch: 6| Step: 5
Training loss: 2.0743753910064697
Validation loss: 2.436427159975934

Epoch: 6| Step: 6
Training loss: 3.1482656002044678
Validation loss: 2.418961887718529

Epoch: 6| Step: 7
Training loss: 2.576265335083008
Validation loss: 2.402981055680142

Epoch: 6| Step: 8
Training loss: 3.275451183319092
Validation loss: 2.4023542763084493

Epoch: 6| Step: 9
Training loss: 2.1810569763183594
Validation loss: 2.400695539289905

Epoch: 6| Step: 10
Training loss: 2.4573755264282227
Validation loss: 2.4007068449451077

Epoch: 6| Step: 11
Training loss: 1.5207195281982422
Validation loss: 2.4137548656873804

Epoch: 6| Step: 12
Training loss: 3.2209091186523438
Validation loss: 2.4199708584816224

Epoch: 6| Step: 13
Training loss: 3.501131772994995
Validation loss: 2.422332843144735

Epoch: 67| Step: 0
Training loss: 2.527310848236084
Validation loss: 2.4272199138518302

Epoch: 6| Step: 1
Training loss: 2.8138480186462402
Validation loss: 2.4350427222508255

Epoch: 6| Step: 2
Training loss: 2.110153913497925
Validation loss: 2.436503897431076

Epoch: 6| Step: 3
Training loss: 2.779348850250244
Validation loss: 2.42811821609415

Epoch: 6| Step: 4
Training loss: 2.9611270427703857
Validation loss: 2.4211719113011516

Epoch: 6| Step: 5
Training loss: 2.188495397567749
Validation loss: 2.4150036329864175

Epoch: 6| Step: 6
Training loss: 2.181060552597046
Validation loss: 2.406368177424195

Epoch: 6| Step: 7
Training loss: 2.924330949783325
Validation loss: 2.393738785097676

Epoch: 6| Step: 8
Training loss: 2.809849739074707
Validation loss: 2.4034095348850375

Epoch: 6| Step: 9
Training loss: 2.8154563903808594
Validation loss: 2.4021890829968195

Epoch: 6| Step: 10
Training loss: 3.690340518951416
Validation loss: 2.4145540780918573

Epoch: 6| Step: 11
Training loss: 2.5638046264648438
Validation loss: 2.4215068599229217

Epoch: 6| Step: 12
Training loss: 2.5613787174224854
Validation loss: 2.4376922397203344

Epoch: 6| Step: 13
Training loss: 2.040285587310791
Validation loss: 2.4469606748191257

Epoch: 68| Step: 0
Training loss: 2.4380831718444824
Validation loss: 2.455830407399003

Epoch: 6| Step: 1
Training loss: 2.5319671630859375
Validation loss: 2.4767885464493946

Epoch: 6| Step: 2
Training loss: 2.1878154277801514
Validation loss: 2.4883688137095463

Epoch: 6| Step: 3
Training loss: 2.7430191040039062
Validation loss: 2.479923827673799

Epoch: 6| Step: 4
Training loss: 2.55320143699646
Validation loss: 2.465482496446179

Epoch: 6| Step: 5
Training loss: 2.4647421836853027
Validation loss: 2.453915557553691

Epoch: 6| Step: 6
Training loss: 2.9129631519317627
Validation loss: 2.4465990527983634

Epoch: 6| Step: 7
Training loss: 2.4567322731018066
Validation loss: 2.447358956900976

Epoch: 6| Step: 8
Training loss: 2.886226177215576
Validation loss: 2.421709091432633

Epoch: 6| Step: 9
Training loss: 2.2276248931884766
Validation loss: 2.417349184713056

Epoch: 6| Step: 10
Training loss: 3.6375865936279297
Validation loss: 2.4018129200063725

Epoch: 6| Step: 11
Training loss: 3.2807517051696777
Validation loss: 2.396641961989864

Epoch: 6| Step: 12
Training loss: 2.0684099197387695
Validation loss: 2.396782059823313

Epoch: 6| Step: 13
Training loss: 2.469161033630371
Validation loss: 2.402302811222692

Epoch: 69| Step: 0
Training loss: 2.1526970863342285
Validation loss: 2.4029100428345385

Epoch: 6| Step: 1
Training loss: 2.2208633422851562
Validation loss: 2.4061328980230514

Epoch: 6| Step: 2
Training loss: 2.5476081371307373
Validation loss: 2.4135767259905414

Epoch: 6| Step: 3
Training loss: 2.71018385887146
Validation loss: 2.419941202286751

Epoch: 6| Step: 4
Training loss: 2.9349312782287598
Validation loss: 2.416271835245112

Epoch: 6| Step: 5
Training loss: 2.4822282791137695
Validation loss: 2.412688104055261

Epoch: 6| Step: 6
Training loss: 2.871058225631714
Validation loss: 2.4111529319517073

Epoch: 6| Step: 7
Training loss: 2.885061025619507
Validation loss: 2.398854527422177

Epoch: 6| Step: 8
Training loss: 2.873840808868408
Validation loss: 2.3966966316264164

Epoch: 6| Step: 9
Training loss: 3.006605386734009
Validation loss: 2.390902506407871

Epoch: 6| Step: 10
Training loss: 2.866889238357544
Validation loss: 2.3870765880871843

Epoch: 6| Step: 11
Training loss: 2.592550277709961
Validation loss: 2.4114524010689027

Epoch: 6| Step: 12
Training loss: 2.6658267974853516
Validation loss: 2.431505093010523

Epoch: 6| Step: 13
Training loss: 1.865997552871704
Validation loss: 2.4674138561371834

Epoch: 70| Step: 0
Training loss: 2.4953737258911133
Validation loss: 2.4798394146785943

Epoch: 6| Step: 1
Training loss: 2.745908737182617
Validation loss: 2.5096805044399795

Epoch: 6| Step: 2
Training loss: 3.248539447784424
Validation loss: 2.470766680214995

Epoch: 6| Step: 3
Training loss: 3.040153980255127
Validation loss: 2.4277849043569257

Epoch: 6| Step: 4
Training loss: 3.069314956665039
Validation loss: 2.4116767965337282

Epoch: 6| Step: 5
Training loss: 2.3531150817871094
Validation loss: 2.3992348512013755

Epoch: 6| Step: 6
Training loss: 2.2769341468811035
Validation loss: 2.3949087025016866

Epoch: 6| Step: 7
Training loss: 1.8618171215057373
Validation loss: 2.3948714246032057

Epoch: 6| Step: 8
Training loss: 3.4354376792907715
Validation loss: 2.391213350398566

Epoch: 6| Step: 9
Training loss: 2.4231348037719727
Validation loss: 2.387682636578878

Epoch: 6| Step: 10
Training loss: 2.175121784210205
Validation loss: 2.392409768155826

Epoch: 6| Step: 11
Training loss: 3.337689161300659
Validation loss: 2.389148462203241

Epoch: 6| Step: 12
Training loss: 1.9415080547332764
Validation loss: 2.396457718264672

Epoch: 6| Step: 13
Training loss: 2.639240264892578
Validation loss: 2.3990771488476823

Epoch: 71| Step: 0
Training loss: 2.25753116607666
Validation loss: 2.4098248789387364

Epoch: 6| Step: 1
Training loss: 3.9646124839782715
Validation loss: 2.4180788865653415

Epoch: 6| Step: 2
Training loss: 2.678654193878174
Validation loss: 2.4262657524437032

Epoch: 6| Step: 3
Training loss: 2.0886571407318115
Validation loss: 2.45188888683114

Epoch: 6| Step: 4
Training loss: 2.413335084915161
Validation loss: 2.4655445980769333

Epoch: 6| Step: 5
Training loss: 1.9147893190383911
Validation loss: 2.4975621136285926

Epoch: 6| Step: 6
Training loss: 3.058927059173584
Validation loss: 2.614702986132714

Epoch: 6| Step: 7
Training loss: 2.6899545192718506
Validation loss: 2.6001043473520586

Epoch: 6| Step: 8
Training loss: 3.697782039642334
Validation loss: 2.5892396357751664

Epoch: 6| Step: 9
Training loss: 2.9101810455322266
Validation loss: 2.5360666474988385

Epoch: 6| Step: 10
Training loss: 2.1960978507995605
Validation loss: 2.479230624373241

Epoch: 6| Step: 11
Training loss: 2.7002768516540527
Validation loss: 2.4658577570351223

Epoch: 6| Step: 12
Training loss: 2.7529492378234863
Validation loss: 2.4470492075848322

Epoch: 6| Step: 13
Training loss: 2.0666234493255615
Validation loss: 2.4412435664925525

Epoch: 72| Step: 0
Training loss: 2.9300780296325684
Validation loss: 2.4443741511273127

Epoch: 6| Step: 1
Training loss: 2.312521457672119
Validation loss: 2.43318800516026

Epoch: 6| Step: 2
Training loss: 2.883169651031494
Validation loss: 2.428707430439611

Epoch: 6| Step: 3
Training loss: 3.0684852600097656
Validation loss: 2.4243193736640354

Epoch: 6| Step: 4
Training loss: 2.7105774879455566
Validation loss: 2.419712766524284

Epoch: 6| Step: 5
Training loss: 3.1398792266845703
Validation loss: 2.4320305778134252

Epoch: 6| Step: 6
Training loss: 2.8553521633148193
Validation loss: 2.4397677272878666

Epoch: 6| Step: 7
Training loss: 2.4811031818389893
Validation loss: 2.4355358436543453

Epoch: 6| Step: 8
Training loss: 2.156057596206665
Validation loss: 2.3938492677545034

Epoch: 6| Step: 9
Training loss: 2.3175246715545654
Validation loss: 2.3818596716850036

Epoch: 6| Step: 10
Training loss: 2.3674237728118896
Validation loss: 2.3695503896282566

Epoch: 6| Step: 11
Training loss: 2.3494138717651367
Validation loss: 2.3673244573736705

Epoch: 6| Step: 12
Training loss: 2.629328489303589
Validation loss: 2.367380316539477

Epoch: 6| Step: 13
Training loss: 3.0767478942871094
Validation loss: 2.3714761836554414

Epoch: 73| Step: 0
Training loss: 2.4458887577056885
Validation loss: 2.3768060463731007

Epoch: 6| Step: 1
Training loss: 3.539252758026123
Validation loss: 2.3948503848045104

Epoch: 6| Step: 2
Training loss: 2.151425838470459
Validation loss: 2.3865564330931632

Epoch: 6| Step: 3
Training loss: 3.1541876792907715
Validation loss: 2.4032016031203733

Epoch: 6| Step: 4
Training loss: 2.343052387237549
Validation loss: 2.377969247038646

Epoch: 6| Step: 5
Training loss: 2.3232932090759277
Validation loss: 2.3787740994525213

Epoch: 6| Step: 6
Training loss: 3.2309765815734863
Validation loss: 2.3895052761159916

Epoch: 6| Step: 7
Training loss: 3.0511860847473145
Validation loss: 2.3872804487905195

Epoch: 6| Step: 8
Training loss: 1.8791054487228394
Validation loss: 2.3731778949819584

Epoch: 6| Step: 9
Training loss: 2.548598527908325
Validation loss: 2.371529684271864

Epoch: 6| Step: 10
Training loss: 2.426476001739502
Validation loss: 2.373398971813981

Epoch: 6| Step: 11
Training loss: 2.357452154159546
Validation loss: 2.3813316924597627

Epoch: 6| Step: 12
Training loss: 2.611117124557495
Validation loss: 2.3728062042626004

Epoch: 6| Step: 13
Training loss: 2.4210760593414307
Validation loss: 2.3837188200284074

Epoch: 74| Step: 0
Training loss: 2.9567208290100098
Validation loss: 2.390113426793006

Epoch: 6| Step: 1
Training loss: 2.7452235221862793
Validation loss: 2.38912808510565

Epoch: 6| Step: 2
Training loss: 3.0810155868530273
Validation loss: 2.4220598666898665

Epoch: 6| Step: 3
Training loss: 2.237557888031006
Validation loss: 2.41981610175102

Epoch: 6| Step: 4
Training loss: 2.5205817222595215
Validation loss: 2.4091824408500426

Epoch: 6| Step: 5
Training loss: 3.2179789543151855
Validation loss: 2.4155691823651715

Epoch: 6| Step: 6
Training loss: 2.865964412689209
Validation loss: 2.391884748653699

Epoch: 6| Step: 7
Training loss: 1.8550106287002563
Validation loss: 2.3690390304852555

Epoch: 6| Step: 8
Training loss: 2.809354305267334
Validation loss: 2.3522679190481863

Epoch: 6| Step: 9
Training loss: 2.2239696979522705
Validation loss: 2.3454054068493586

Epoch: 6| Step: 10
Training loss: 2.5298683643341064
Validation loss: 2.3511663944490495

Epoch: 6| Step: 11
Training loss: 2.9345717430114746
Validation loss: 2.3508183802327802

Epoch: 6| Step: 12
Training loss: 2.001127004623413
Validation loss: 2.3525076425203713

Epoch: 6| Step: 13
Training loss: 3.0033979415893555
Validation loss: 2.348991737570814

Epoch: 75| Step: 0
Training loss: 2.2674474716186523
Validation loss: 2.347258931847029

Epoch: 6| Step: 1
Training loss: 3.109917640686035
Validation loss: 2.3459986332924134

Epoch: 6| Step: 2
Training loss: 2.6928634643554688
Validation loss: 2.343052871765629

Epoch: 6| Step: 3
Training loss: 3.1019461154937744
Validation loss: 2.340041434893044

Epoch: 6| Step: 4
Training loss: 1.8836815357208252
Validation loss: 2.3435545531652306

Epoch: 6| Step: 5
Training loss: 2.992624282836914
Validation loss: 2.3458793906755346

Epoch: 6| Step: 6
Training loss: 2.623469829559326
Validation loss: 2.3414513859697568

Epoch: 6| Step: 7
Training loss: 2.5292882919311523
Validation loss: 2.342386878946776

Epoch: 6| Step: 8
Training loss: 1.8513531684875488
Validation loss: 2.3451714720777286

Epoch: 6| Step: 9
Training loss: 2.7411136627197266
Validation loss: 2.3428738835037395

Epoch: 6| Step: 10
Training loss: 2.57576322555542
Validation loss: 2.3436596829404115

Epoch: 6| Step: 11
Training loss: 2.6140079498291016
Validation loss: 2.3461533105501564

Epoch: 6| Step: 12
Training loss: 2.7887237071990967
Validation loss: 2.3747826032741095

Epoch: 6| Step: 13
Training loss: 2.7618796825408936
Validation loss: 2.3923405703677925

Epoch: 76| Step: 0
Training loss: 1.8561683893203735
Validation loss: 2.5083738629535963

Epoch: 6| Step: 1
Training loss: 2.813154697418213
Validation loss: 2.5975303931902816

Epoch: 6| Step: 2
Training loss: 3.134418487548828
Validation loss: 2.64813655935308

Epoch: 6| Step: 3
Training loss: 3.146202564239502
Validation loss: 2.602841846404537

Epoch: 6| Step: 4
Training loss: 2.8515682220458984
Validation loss: 2.5424238584374868

Epoch: 6| Step: 5
Training loss: 2.370640277862549
Validation loss: 2.4907392583867556

Epoch: 6| Step: 6
Training loss: 2.7370848655700684
Validation loss: 2.466040616394371

Epoch: 6| Step: 7
Training loss: 2.707240581512451
Validation loss: 2.4464837351152973

Epoch: 6| Step: 8
Training loss: 2.6700658798217773
Validation loss: 2.4392805099487305

Epoch: 6| Step: 9
Training loss: 2.701481580734253
Validation loss: 2.4901632531996696

Epoch: 6| Step: 10
Training loss: 2.9168217182159424
Validation loss: 2.462068009120162

Epoch: 6| Step: 11
Training loss: 2.854048728942871
Validation loss: 2.4288020287790606

Epoch: 6| Step: 12
Training loss: 2.375955820083618
Validation loss: 2.3985555453967025

Epoch: 6| Step: 13
Training loss: 2.574266195297241
Validation loss: 2.389274707404516

Epoch: 77| Step: 0
Training loss: 2.8212995529174805
Validation loss: 2.384637758296023

Epoch: 6| Step: 1
Training loss: 2.2956724166870117
Validation loss: 2.362995022086687

Epoch: 6| Step: 2
Training loss: 2.1388089656829834
Validation loss: 2.3545968122379755

Epoch: 6| Step: 3
Training loss: 2.4006974697113037
Validation loss: 2.3715931548867175

Epoch: 6| Step: 4
Training loss: 2.180860996246338
Validation loss: 2.3901453736007854

Epoch: 6| Step: 5
Training loss: 3.0502872467041016
Validation loss: 2.3994213227302796

Epoch: 6| Step: 6
Training loss: 3.1794939041137695
Validation loss: 2.4145595130100044

Epoch: 6| Step: 7
Training loss: 2.805281639099121
Validation loss: 2.389988021184039

Epoch: 6| Step: 8
Training loss: 2.4937448501586914
Validation loss: 2.3742745499457083

Epoch: 6| Step: 9
Training loss: 2.3577768802642822
Validation loss: 2.3733544247124785

Epoch: 6| Step: 10
Training loss: 2.9349865913391113
Validation loss: 2.3720282354662494

Epoch: 6| Step: 11
Training loss: 2.6240203380584717
Validation loss: 2.3643665390629924

Epoch: 6| Step: 12
Training loss: 2.721496820449829
Validation loss: 2.37813994192308

Epoch: 6| Step: 13
Training loss: 2.6189870834350586
Validation loss: 2.382729653389223

Epoch: 78| Step: 0
Training loss: 1.9362773895263672
Validation loss: 2.404885302307785

Epoch: 6| Step: 1
Training loss: 2.909358501434326
Validation loss: 2.462201399187888

Epoch: 6| Step: 2
Training loss: 2.476628303527832
Validation loss: 2.526564118682697

Epoch: 6| Step: 3
Training loss: 2.530518054962158
Validation loss: 2.5850898681148404

Epoch: 6| Step: 4
Training loss: 2.5495331287384033
Validation loss: 2.522657694355134

Epoch: 6| Step: 5
Training loss: 3.229537010192871
Validation loss: 2.4248550143293155

Epoch: 6| Step: 6
Training loss: 2.6732401847839355
Validation loss: 2.3715458480260705

Epoch: 6| Step: 7
Training loss: 2.4809837341308594
Validation loss: 2.3402090713541996

Epoch: 6| Step: 8
Training loss: 2.6865596771240234
Validation loss: 2.338222690807876

Epoch: 6| Step: 9
Training loss: 2.486424446105957
Validation loss: 2.3374124470577446

Epoch: 6| Step: 10
Training loss: 2.7098019123077393
Validation loss: 2.3406186655003536

Epoch: 6| Step: 11
Training loss: 2.411982774734497
Validation loss: 2.3526840722689064

Epoch: 6| Step: 12
Training loss: 2.877894639968872
Validation loss: 2.3771352998671995

Epoch: 6| Step: 13
Training loss: 2.9820423126220703
Validation loss: 2.3765841248214885

Epoch: 79| Step: 0
Training loss: 2.7194435596466064
Validation loss: 2.3491735137918943

Epoch: 6| Step: 1
Training loss: 3.112868309020996
Validation loss: 2.336070055602699

Epoch: 6| Step: 2
Training loss: 2.7325055599212646
Validation loss: 2.338476345103274

Epoch: 6| Step: 3
Training loss: 2.634206771850586
Validation loss: 2.330477342810682

Epoch: 6| Step: 4
Training loss: 2.630261182785034
Validation loss: 2.332410643177648

Epoch: 6| Step: 5
Training loss: 3.137341022491455
Validation loss: 2.3312717688980924

Epoch: 6| Step: 6
Training loss: 2.3342373371124268
Validation loss: 2.329322122758435

Epoch: 6| Step: 7
Training loss: 2.272306442260742
Validation loss: 2.326908165408719

Epoch: 6| Step: 8
Training loss: 1.634021520614624
Validation loss: 2.3277610501935406

Epoch: 6| Step: 9
Training loss: 3.0301637649536133
Validation loss: 2.3247249280252764

Epoch: 6| Step: 10
Training loss: 2.277615785598755
Validation loss: 2.326038747705439

Epoch: 6| Step: 11
Training loss: 2.3715476989746094
Validation loss: 2.33187589850477

Epoch: 6| Step: 12
Training loss: 3.1711971759796143
Validation loss: 2.33354873811045

Epoch: 6| Step: 13
Training loss: 2.282914161682129
Validation loss: 2.333290305188907

Epoch: 80| Step: 0
Training loss: 2.7172369956970215
Validation loss: 2.3318572992919595

Epoch: 6| Step: 1
Training loss: 2.4486780166625977
Validation loss: 2.333121468943934

Epoch: 6| Step: 2
Training loss: 3.4421868324279785
Validation loss: 2.333768370330975

Epoch: 6| Step: 3
Training loss: 1.5777701139450073
Validation loss: 2.3300785480007047

Epoch: 6| Step: 4
Training loss: 3.0182008743286133
Validation loss: 2.3438077203689085

Epoch: 6| Step: 5
Training loss: 2.1927316188812256
Validation loss: 2.3414717361491215

Epoch: 6| Step: 6
Training loss: 2.9735031127929688
Validation loss: 2.3535096235172723

Epoch: 6| Step: 7
Training loss: 2.9459877014160156
Validation loss: 2.3441940699854205

Epoch: 6| Step: 8
Training loss: 3.344489812850952
Validation loss: 2.3403420012484313

Epoch: 6| Step: 9
Training loss: 2.4064412117004395
Validation loss: 2.3390763869849582

Epoch: 6| Step: 10
Training loss: 2.060950756072998
Validation loss: 2.336235571933049

Epoch: 6| Step: 11
Training loss: 2.636880874633789
Validation loss: 2.3336171732153943

Epoch: 6| Step: 12
Training loss: 2.0535635948181152
Validation loss: 2.3299316590832126

Epoch: 6| Step: 13
Training loss: 2.4856841564178467
Validation loss: 2.3275000792677685

Epoch: 81| Step: 0
Training loss: 2.8254952430725098
Validation loss: 2.328193069786154

Epoch: 6| Step: 1
Training loss: 3.1319897174835205
Validation loss: 2.3306207631223943

Epoch: 6| Step: 2
Training loss: 2.632465362548828
Validation loss: 2.3422151662970103

Epoch: 6| Step: 3
Training loss: 2.8144125938415527
Validation loss: 2.3584398736235914

Epoch: 6| Step: 4
Training loss: 2.159693956375122
Validation loss: 2.3565891558124172

Epoch: 6| Step: 5
Training loss: 2.475480794906616
Validation loss: 2.3476636076486237

Epoch: 6| Step: 6
Training loss: 2.5386335849761963
Validation loss: 2.3506111483420096

Epoch: 6| Step: 7
Training loss: 2.5151772499084473
Validation loss: 2.347225260990922

Epoch: 6| Step: 8
Training loss: 2.5520505905151367
Validation loss: 2.348982330291502

Epoch: 6| Step: 9
Training loss: 1.809856653213501
Validation loss: 2.3302117432317426

Epoch: 6| Step: 10
Training loss: 2.331173896789551
Validation loss: 2.3407472538691696

Epoch: 6| Step: 11
Training loss: 3.0807013511657715
Validation loss: 2.341527933715492

Epoch: 6| Step: 12
Training loss: 2.4140443801879883
Validation loss: 2.3322987300093456

Epoch: 6| Step: 13
Training loss: 2.941603422164917
Validation loss: 2.3169349995992516

Epoch: 82| Step: 0
Training loss: 2.4216246604919434
Validation loss: 2.308503717504522

Epoch: 6| Step: 1
Training loss: 2.5291097164154053
Validation loss: 2.3033493039428548

Epoch: 6| Step: 2
Training loss: 1.614920973777771
Validation loss: 2.3069975709402435

Epoch: 6| Step: 3
Training loss: 2.451180934906006
Validation loss: 2.3032843348800496

Epoch: 6| Step: 4
Training loss: 3.1933822631835938
Validation loss: 2.3030026087196926

Epoch: 6| Step: 5
Training loss: 3.453920364379883
Validation loss: 2.3077753846363356

Epoch: 6| Step: 6
Training loss: 2.853767156600952
Validation loss: 2.308796085337157

Epoch: 6| Step: 7
Training loss: 2.4286136627197266
Validation loss: 2.3086489938920542

Epoch: 6| Step: 8
Training loss: 2.5567378997802734
Validation loss: 2.3032556092867287

Epoch: 6| Step: 9
Training loss: 2.901243209838867
Validation loss: 2.3064242280939573

Epoch: 6| Step: 10
Training loss: 2.0834107398986816
Validation loss: 2.3043331843550487

Epoch: 6| Step: 11
Training loss: 3.040125846862793
Validation loss: 2.304669657061177

Epoch: 6| Step: 12
Training loss: 2.280487060546875
Validation loss: 2.302323736170287

Epoch: 6| Step: 13
Training loss: 2.393075942993164
Validation loss: 2.3059832588318856

Epoch: 83| Step: 0
Training loss: 2.810800075531006
Validation loss: 2.31110563073107

Epoch: 6| Step: 1
Training loss: 3.0290536880493164
Validation loss: 2.3234849232499317

Epoch: 6| Step: 2
Training loss: 2.3767473697662354
Validation loss: 2.3223518671527987

Epoch: 6| Step: 3
Training loss: 2.5558853149414062
Validation loss: 2.3314807568826983

Epoch: 6| Step: 4
Training loss: 2.8856728076934814
Validation loss: 2.3477003471825713

Epoch: 6| Step: 5
Training loss: 2.1021783351898193
Validation loss: 2.3359022268684964

Epoch: 6| Step: 6
Training loss: 2.8048086166381836
Validation loss: 2.3268880485206522

Epoch: 6| Step: 7
Training loss: 2.412869453430176
Validation loss: 2.336831787581085

Epoch: 6| Step: 8
Training loss: 2.7505886554718018
Validation loss: 2.3424904423375286

Epoch: 6| Step: 9
Training loss: 1.835289478302002
Validation loss: 2.3347014201584684

Epoch: 6| Step: 10
Training loss: 2.581782341003418
Validation loss: 2.36822671274985

Epoch: 6| Step: 11
Training loss: 2.720456838607788
Validation loss: 2.4523426435327016

Epoch: 6| Step: 12
Training loss: 2.6724820137023926
Validation loss: 2.5369126207085064

Epoch: 6| Step: 13
Training loss: 2.6273491382598877
Validation loss: 2.621763542134275

Epoch: 84| Step: 0
Training loss: 3.349437713623047
Validation loss: 2.7196692189862652

Epoch: 6| Step: 1
Training loss: 2.5394349098205566
Validation loss: 2.780668745758713

Epoch: 6| Step: 2
Training loss: 3.546821117401123
Validation loss: 2.730902966632638

Epoch: 6| Step: 3
Training loss: 2.652215003967285
Validation loss: 2.5955703079059558

Epoch: 6| Step: 4
Training loss: 2.4766969680786133
Validation loss: 2.504669543235533

Epoch: 6| Step: 5
Training loss: 3.133159637451172
Validation loss: 2.454891663725658

Epoch: 6| Step: 6
Training loss: 2.1689298152923584
Validation loss: 2.3974467836400515

Epoch: 6| Step: 7
Training loss: 2.454249858856201
Validation loss: 2.3542339545424267

Epoch: 6| Step: 8
Training loss: 2.1390299797058105
Validation loss: 2.31698896808009

Epoch: 6| Step: 9
Training loss: 3.240976572036743
Validation loss: 2.3367385146438435

Epoch: 6| Step: 10
Training loss: 2.385061025619507
Validation loss: 2.346109897859635

Epoch: 6| Step: 11
Training loss: 2.4856600761413574
Validation loss: 2.351360326172203

Epoch: 6| Step: 12
Training loss: 2.3215980529785156
Validation loss: 2.37380842239626

Epoch: 6| Step: 13
Training loss: 2.8812785148620605
Validation loss: 2.3639986515045166

Epoch: 85| Step: 0
Training loss: 3.172008514404297
Validation loss: 2.3479684501565914

Epoch: 6| Step: 1
Training loss: 3.261735677719116
Validation loss: 2.3044118522315897

Epoch: 6| Step: 2
Training loss: 2.399275302886963
Validation loss: 2.2953417839542514

Epoch: 6| Step: 3
Training loss: 3.0627856254577637
Validation loss: 2.3014232112515356

Epoch: 6| Step: 4
Training loss: 2.8786797523498535
Validation loss: 2.2988950437115085

Epoch: 6| Step: 5
Training loss: 2.0790843963623047
Validation loss: 2.31294773983699

Epoch: 6| Step: 6
Training loss: 2.01961088180542
Validation loss: 2.3503055418691328

Epoch: 6| Step: 7
Training loss: 2.8113529682159424
Validation loss: 2.384032408396403

Epoch: 6| Step: 8
Training loss: 2.041067600250244
Validation loss: 2.4261968392197804

Epoch: 6| Step: 9
Training loss: 2.590998649597168
Validation loss: 2.4479005823853197

Epoch: 6| Step: 10
Training loss: 2.379922866821289
Validation loss: 2.3707916018783406

Epoch: 6| Step: 11
Training loss: 2.2559447288513184
Validation loss: 2.305597507825462

Epoch: 6| Step: 12
Training loss: 3.2531871795654297
Validation loss: 2.290571431959829

Epoch: 6| Step: 13
Training loss: 2.4825916290283203
Validation loss: 2.28618618237075

Epoch: 86| Step: 0
Training loss: 1.998729944229126
Validation loss: 2.2998717215753373

Epoch: 6| Step: 1
Training loss: 2.5235719680786133
Validation loss: 2.3001853701888875

Epoch: 6| Step: 2
Training loss: 2.638812303543091
Validation loss: 2.302033949923772

Epoch: 6| Step: 3
Training loss: 2.557075023651123
Validation loss: 2.306478736221149

Epoch: 6| Step: 4
Training loss: 2.9346516132354736
Validation loss: 2.3059376465376986

Epoch: 6| Step: 5
Training loss: 2.731955051422119
Validation loss: 2.300981770279587

Epoch: 6| Step: 6
Training loss: 2.481541633605957
Validation loss: 2.3045274544787664

Epoch: 6| Step: 7
Training loss: 1.8853272199630737
Validation loss: 2.30882611069628

Epoch: 6| Step: 8
Training loss: 3.8172991275787354
Validation loss: 2.3089240545867593

Epoch: 6| Step: 9
Training loss: 2.6991665363311768
Validation loss: 2.301590044011352

Epoch: 6| Step: 10
Training loss: 2.8103554248809814
Validation loss: 2.299162123792915

Epoch: 6| Step: 11
Training loss: 2.8366854190826416
Validation loss: 2.3052607633734263

Epoch: 6| Step: 12
Training loss: 2.0160248279571533
Validation loss: 2.293734588930684

Epoch: 6| Step: 13
Training loss: 2.4797163009643555
Validation loss: 2.2916304437063073

Epoch: 87| Step: 0
Training loss: 2.7245030403137207
Validation loss: 2.2865734331069456

Epoch: 6| Step: 1
Training loss: 3.0085396766662598
Validation loss: 2.279392421886485

Epoch: 6| Step: 2
Training loss: 2.0278172492980957
Validation loss: 2.2791147283328477

Epoch: 6| Step: 3
Training loss: 1.8875333070755005
Validation loss: 2.2755992284385105

Epoch: 6| Step: 4
Training loss: 2.0229198932647705
Validation loss: 2.282831914963261

Epoch: 6| Step: 5
Training loss: 3.070148468017578
Validation loss: 2.2842239513192126

Epoch: 6| Step: 6
Training loss: 3.448606491088867
Validation loss: 2.296756944348735

Epoch: 6| Step: 7
Training loss: 2.859762191772461
Validation loss: 2.303360585243471

Epoch: 6| Step: 8
Training loss: 2.4675629138946533
Validation loss: 2.3208164502215642

Epoch: 6| Step: 9
Training loss: 2.6656360626220703
Validation loss: 2.3339053379592074

Epoch: 6| Step: 10
Training loss: 2.5790514945983887
Validation loss: 2.348937219189059

Epoch: 6| Step: 11
Training loss: 2.3946869373321533
Validation loss: 2.357369504949098

Epoch: 6| Step: 12
Training loss: 2.2189993858337402
Validation loss: 2.3514272730837584

Epoch: 6| Step: 13
Training loss: 2.6523098945617676
Validation loss: 2.3371387502198577

Epoch: 88| Step: 0
Training loss: 2.8498711585998535
Validation loss: 2.3324282451342513

Epoch: 6| Step: 1
Training loss: 1.6790852546691895
Validation loss: 2.316620519084315

Epoch: 6| Step: 2
Training loss: 2.4462404251098633
Validation loss: 2.3106016933277087

Epoch: 6| Step: 3
Training loss: 1.8236258029937744
Validation loss: 2.310926975742463

Epoch: 6| Step: 4
Training loss: 1.9001213312149048
Validation loss: 2.3148246221644904

Epoch: 6| Step: 5
Training loss: 2.838066816329956
Validation loss: 2.313398794461322

Epoch: 6| Step: 6
Training loss: 2.8030757904052734
Validation loss: 2.2980707922289447

Epoch: 6| Step: 7
Training loss: 2.4627161026000977
Validation loss: 2.301035028631969

Epoch: 6| Step: 8
Training loss: 3.1381359100341797
Validation loss: 2.307310360734181

Epoch: 6| Step: 9
Training loss: 3.089427947998047
Validation loss: 2.30696282335507

Epoch: 6| Step: 10
Training loss: 2.897824764251709
Validation loss: 2.300649235325475

Epoch: 6| Step: 11
Training loss: 2.60306978225708
Validation loss: 2.301407006479079

Epoch: 6| Step: 12
Training loss: 2.5600359439849854
Validation loss: 2.304052875887963

Epoch: 6| Step: 13
Training loss: 2.779805898666382
Validation loss: 2.297120012262816

Epoch: 89| Step: 0
Training loss: 2.584501266479492
Validation loss: 2.30432298362896

Epoch: 6| Step: 1
Training loss: 2.4702866077423096
Validation loss: 2.3022907651880735

Epoch: 6| Step: 2
Training loss: 2.3283214569091797
Validation loss: 2.2992847401608705

Epoch: 6| Step: 3
Training loss: 1.7227909564971924
Validation loss: 2.3093221443955616

Epoch: 6| Step: 4
Training loss: 2.940127372741699
Validation loss: 2.305938348975233

Epoch: 6| Step: 5
Training loss: 2.517199993133545
Validation loss: 2.299420905369584

Epoch: 6| Step: 6
Training loss: 2.771160364151001
Validation loss: 2.2905609774333175

Epoch: 6| Step: 7
Training loss: 3.104066848754883
Validation loss: 2.28502457885332

Epoch: 6| Step: 8
Training loss: 2.300262451171875
Validation loss: 2.2828523292336413

Epoch: 6| Step: 9
Training loss: 2.578887939453125
Validation loss: 2.2914592348119265

Epoch: 6| Step: 10
Training loss: 2.4017319679260254
Validation loss: 2.2868987744854343

Epoch: 6| Step: 11
Training loss: 2.754345417022705
Validation loss: 2.285945846188453

Epoch: 6| Step: 12
Training loss: 2.7384426593780518
Validation loss: 2.2778676299638647

Epoch: 6| Step: 13
Training loss: 2.4298946857452393
Validation loss: 2.2732699635208293

Epoch: 90| Step: 0
Training loss: 2.2201433181762695
Validation loss: 2.270132580111104

Epoch: 6| Step: 1
Training loss: 2.470491409301758
Validation loss: 2.2744221097679547

Epoch: 6| Step: 2
Training loss: 2.995817184448242
Validation loss: 2.2765937671866467

Epoch: 6| Step: 3
Training loss: 2.7250490188598633
Validation loss: 2.2714636787291496

Epoch: 6| Step: 4
Training loss: 2.7763330936431885
Validation loss: 2.274372685340143

Epoch: 6| Step: 5
Training loss: 3.052821636199951
Validation loss: 2.275590786369898

Epoch: 6| Step: 6
Training loss: 1.9886784553527832
Validation loss: 2.271338890957576

Epoch: 6| Step: 7
Training loss: 2.646493434906006
Validation loss: 2.270159218900947

Epoch: 6| Step: 8
Training loss: 2.2393927574157715
Validation loss: 2.2812923718524236

Epoch: 6| Step: 9
Training loss: 2.212310314178467
Validation loss: 2.2822023078959477

Epoch: 6| Step: 10
Training loss: 2.3598575592041016
Validation loss: 2.2764823923828783

Epoch: 6| Step: 11
Training loss: 2.746765613555908
Validation loss: 2.2660019141371532

Epoch: 6| Step: 12
Training loss: 2.357780933380127
Validation loss: 2.2689489920934043

Epoch: 6| Step: 13
Training loss: 2.964737892150879
Validation loss: 2.265149934317476

Epoch: 91| Step: 0
Training loss: 2.558366298675537
Validation loss: 2.26518690201544

Epoch: 6| Step: 1
Training loss: 2.4572176933288574
Validation loss: 2.2771371872194353

Epoch: 6| Step: 2
Training loss: 2.6142196655273438
Validation loss: 2.276154278427042

Epoch: 6| Step: 3
Training loss: 2.5560896396636963
Validation loss: 2.283888437414682

Epoch: 6| Step: 4
Training loss: 1.9457449913024902
Validation loss: 2.2858818602818314

Epoch: 6| Step: 5
Training loss: 3.006190061569214
Validation loss: 2.2828288667945453

Epoch: 6| Step: 6
Training loss: 2.653791904449463
Validation loss: 2.2908730840170257

Epoch: 6| Step: 7
Training loss: 2.6278412342071533
Validation loss: 2.2886689029714113

Epoch: 6| Step: 8
Training loss: 2.691201686859131
Validation loss: 2.2900771710180465

Epoch: 6| Step: 9
Training loss: 3.0372884273529053
Validation loss: 2.2894557291461575

Epoch: 6| Step: 10
Training loss: 2.6112208366394043
Validation loss: 2.2939450920269056

Epoch: 6| Step: 11
Training loss: 2.3977696895599365
Validation loss: 2.3108474823736374

Epoch: 6| Step: 12
Training loss: 2.4223856925964355
Validation loss: 2.3159339581766436

Epoch: 6| Step: 13
Training loss: 1.4210137128829956
Validation loss: 2.3064031139496834

Epoch: 92| Step: 0
Training loss: 2.58978533744812
Validation loss: 2.2992208414180304

Epoch: 6| Step: 1
Training loss: 2.50266695022583
Validation loss: 2.3120600561941824

Epoch: 6| Step: 2
Training loss: 2.2141222953796387
Validation loss: 2.316232442855835

Epoch: 6| Step: 3
Training loss: 2.196406841278076
Validation loss: 2.320134160339191

Epoch: 6| Step: 4
Training loss: 3.148131847381592
Validation loss: 2.3103631555393176

Epoch: 6| Step: 5
Training loss: 3.2269301414489746
Validation loss: 2.3108975041297173

Epoch: 6| Step: 6
Training loss: 2.3282361030578613
Validation loss: 2.3015847462479786

Epoch: 6| Step: 7
Training loss: 2.4511029720306396
Validation loss: 2.281660556793213

Epoch: 6| Step: 8
Training loss: 2.2295799255371094
Validation loss: 2.2674000840033255

Epoch: 6| Step: 9
Training loss: 2.365708827972412
Validation loss: 2.2571455893977994

Epoch: 6| Step: 10
Training loss: 2.2247540950775146
Validation loss: 2.251118783027895

Epoch: 6| Step: 11
Training loss: 3.0748918056488037
Validation loss: 2.2503036619514547

Epoch: 6| Step: 12
Training loss: 2.3648436069488525
Validation loss: 2.2423952753825853

Epoch: 6| Step: 13
Training loss: 2.3606884479522705
Validation loss: 2.2455247089427006

Epoch: 93| Step: 0
Training loss: 1.6456875801086426
Validation loss: 2.2463375009516233

Epoch: 6| Step: 1
Training loss: 3.10986065864563
Validation loss: 2.24339577844066

Epoch: 6| Step: 2
Training loss: 2.4335269927978516
Validation loss: 2.243348565152896

Epoch: 6| Step: 3
Training loss: 2.234158515930176
Validation loss: 2.2502663776438725

Epoch: 6| Step: 4
Training loss: 2.3110618591308594
Validation loss: 2.2480540813938266

Epoch: 6| Step: 5
Training loss: 2.448641777038574
Validation loss: 2.2490165938613234

Epoch: 6| Step: 6
Training loss: 2.3814761638641357
Validation loss: 2.2567140261332193

Epoch: 6| Step: 7
Training loss: 2.777177572250366
Validation loss: 2.271427390395954

Epoch: 6| Step: 8
Training loss: 2.2702596187591553
Validation loss: 2.2970245038309405

Epoch: 6| Step: 9
Training loss: 3.0439772605895996
Validation loss: 2.306383661044541

Epoch: 6| Step: 10
Training loss: 3.588320016860962
Validation loss: 2.319941759109497

Epoch: 6| Step: 11
Training loss: 2.6033029556274414
Validation loss: 2.3181724343248593

Epoch: 6| Step: 12
Training loss: 2.490128517150879
Validation loss: 2.297705522147558

Epoch: 6| Step: 13
Training loss: 1.683398962020874
Validation loss: 2.280497440727808

Epoch: 94| Step: 0
Training loss: 2.1324949264526367
Validation loss: 2.2610857089360556

Epoch: 6| Step: 1
Training loss: 3.094292402267456
Validation loss: 2.2483832118331746

Epoch: 6| Step: 2
Training loss: 3.4937262535095215
Validation loss: 2.233558557366812

Epoch: 6| Step: 3
Training loss: 1.574140191078186
Validation loss: 2.239519937064058

Epoch: 6| Step: 4
Training loss: 2.3381989002227783
Validation loss: 2.24388385844487

Epoch: 6| Step: 5
Training loss: 2.246753215789795
Validation loss: 2.242703381405082

Epoch: 6| Step: 6
Training loss: 1.8537812232971191
Validation loss: 2.2426029725741317

Epoch: 6| Step: 7
Training loss: 2.2134487628936768
Validation loss: 2.237834028018418

Epoch: 6| Step: 8
Training loss: 2.3134279251098633
Validation loss: 2.239282992578322

Epoch: 6| Step: 9
Training loss: 2.9855966567993164
Validation loss: 2.2395263743656937

Epoch: 6| Step: 10
Training loss: 3.091437339782715
Validation loss: 2.2453836907622633

Epoch: 6| Step: 11
Training loss: 2.663996934890747
Validation loss: 2.2415181129209456

Epoch: 6| Step: 12
Training loss: 2.4836158752441406
Validation loss: 2.2474638979922057

Epoch: 6| Step: 13
Training loss: 3.351304769515991
Validation loss: 2.251567748285109

Epoch: 95| Step: 0
Training loss: 2.410715103149414
Validation loss: 2.258242586607574

Epoch: 6| Step: 1
Training loss: 2.4320261478424072
Validation loss: 2.2784116332248976

Epoch: 6| Step: 2
Training loss: 2.242633819580078
Validation loss: 2.2767857736156834

Epoch: 6| Step: 3
Training loss: 2.648591995239258
Validation loss: 2.2807636978805705

Epoch: 6| Step: 4
Training loss: 2.9247002601623535
Validation loss: 2.292843608446019

Epoch: 6| Step: 5
Training loss: 2.4257476329803467
Validation loss: 2.2728799645618727

Epoch: 6| Step: 6
Training loss: 2.7084102630615234
Validation loss: 2.2526585017481158

Epoch: 6| Step: 7
Training loss: 2.9374146461486816
Validation loss: 2.243996940633302

Epoch: 6| Step: 8
Training loss: 2.9972429275512695
Validation loss: 2.242853092890914

Epoch: 6| Step: 9
Training loss: 2.249509334564209
Validation loss: 2.2424777912837204

Epoch: 6| Step: 10
Training loss: 1.705657720565796
Validation loss: 2.2462345195072952

Epoch: 6| Step: 11
Training loss: 2.0519256591796875
Validation loss: 2.2451880798544934

Epoch: 6| Step: 12
Training loss: 3.062378406524658
Validation loss: 2.2542221699991534

Epoch: 6| Step: 13
Training loss: 2.318549633026123
Validation loss: 2.2416899870800715

Epoch: 96| Step: 0
Training loss: 2.7235093116760254
Validation loss: 2.2520052258686354

Epoch: 6| Step: 1
Training loss: 2.4496545791625977
Validation loss: 2.250527689533849

Epoch: 6| Step: 2
Training loss: 2.463871955871582
Validation loss: 2.2541871583589943

Epoch: 6| Step: 3
Training loss: 2.6258273124694824
Validation loss: 2.250067590385355

Epoch: 6| Step: 4
Training loss: 2.5328030586242676
Validation loss: 2.237908196705644

Epoch: 6| Step: 5
Training loss: 2.214289665222168
Validation loss: 2.231972473923878

Epoch: 6| Step: 6
Training loss: 2.8454580307006836
Validation loss: 2.238955987397061

Epoch: 6| Step: 7
Training loss: 2.424471378326416
Validation loss: 2.24296143234417

Epoch: 6| Step: 8
Training loss: 2.460706949234009
Validation loss: 2.241531754052767

Epoch: 6| Step: 9
Training loss: 1.9058724641799927
Validation loss: 2.2703082433310886

Epoch: 6| Step: 10
Training loss: 2.381300449371338
Validation loss: 2.292232439082156

Epoch: 6| Step: 11
Training loss: 2.9786763191223145
Validation loss: 2.2884284834707938

Epoch: 6| Step: 12
Training loss: 2.7784581184387207
Validation loss: 2.286917107079619

Epoch: 6| Step: 13
Training loss: 2.065023183822632
Validation loss: 2.2883065080129974

Epoch: 97| Step: 0
Training loss: 2.0539352893829346
Validation loss: 2.313631425621689

Epoch: 6| Step: 1
Training loss: 2.954428195953369
Validation loss: 2.33713440228534

Epoch: 6| Step: 2
Training loss: 2.6728813648223877
Validation loss: 2.3452325251794632

Epoch: 6| Step: 3
Training loss: 2.5764613151550293
Validation loss: 2.3634424440322386

Epoch: 6| Step: 4
Training loss: 2.2923996448516846
Validation loss: 2.3598396496106218

Epoch: 6| Step: 5
Training loss: 2.6192870140075684
Validation loss: 2.3381591612292874

Epoch: 6| Step: 6
Training loss: 3.104287624359131
Validation loss: 2.329355247559086

Epoch: 6| Step: 7
Training loss: 2.083512783050537
Validation loss: 2.313957655301658

Epoch: 6| Step: 8
Training loss: 2.494990825653076
Validation loss: 2.311989027966735

Epoch: 6| Step: 9
Training loss: 2.740851879119873
Validation loss: 2.31416662790442

Epoch: 6| Step: 10
Training loss: 2.766927719116211
Validation loss: 2.294634865176293

Epoch: 6| Step: 11
Training loss: 2.068647861480713
Validation loss: 2.266543462712278

Epoch: 6| Step: 12
Training loss: 2.4094529151916504
Validation loss: 2.2567124418033067

Epoch: 6| Step: 13
Training loss: 2.845492124557495
Validation loss: 2.233439653150497

Epoch: 98| Step: 0
Training loss: 2.5076608657836914
Validation loss: 2.2252856877542313

Epoch: 6| Step: 1
Training loss: 2.496945381164551
Validation loss: 2.2111111558893675

Epoch: 6| Step: 2
Training loss: 2.2933225631713867
Validation loss: 2.2045493561734437

Epoch: 6| Step: 3
Training loss: 2.7186503410339355
Validation loss: 2.2070531255455426

Epoch: 6| Step: 4
Training loss: 1.582606554031372
Validation loss: 2.205945586645475

Epoch: 6| Step: 5
Training loss: 2.836324691772461
Validation loss: 2.215828687913956

Epoch: 6| Step: 6
Training loss: 2.196967124938965
Validation loss: 2.225598754421357

Epoch: 6| Step: 7
Training loss: 2.4526805877685547
Validation loss: 2.2327405919310865

Epoch: 6| Step: 8
Training loss: 2.449991226196289
Validation loss: 2.244787754551057

Epoch: 6| Step: 9
Training loss: 3.268293857574463
Validation loss: 2.2463982758983487

Epoch: 6| Step: 10
Training loss: 2.292370557785034
Validation loss: 2.2468245439631964

Epoch: 6| Step: 11
Training loss: 2.4984045028686523
Validation loss: 2.243919636613579

Epoch: 6| Step: 12
Training loss: 3.006211519241333
Validation loss: 2.233650172910383

Epoch: 6| Step: 13
Training loss: 2.6490285396575928
Validation loss: 2.2237004362126833

Epoch: 99| Step: 0
Training loss: 2.7064311504364014
Validation loss: 2.224893646855508

Epoch: 6| Step: 1
Training loss: 3.00197172164917
Validation loss: 2.2280942137523363

Epoch: 6| Step: 2
Training loss: 2.2583508491516113
Validation loss: 2.2317018239728865

Epoch: 6| Step: 3
Training loss: 2.4480042457580566
Validation loss: 2.233876717987881

Epoch: 6| Step: 4
Training loss: 2.135347604751587
Validation loss: 2.250778467424454

Epoch: 6| Step: 5
Training loss: 2.3647689819335938
Validation loss: 2.279453600606611

Epoch: 6| Step: 6
Training loss: 2.645991325378418
Validation loss: 2.308346281769455

Epoch: 6| Step: 7
Training loss: 2.782449722290039
Validation loss: 2.297311134235833

Epoch: 6| Step: 8
Training loss: 3.143317699432373
Validation loss: 2.2936268519329768

Epoch: 6| Step: 9
Training loss: 2.2263364791870117
Validation loss: 2.2753051660394155

Epoch: 6| Step: 10
Training loss: 2.351285457611084
Validation loss: 2.2450126422348844

Epoch: 6| Step: 11
Training loss: 2.121896266937256
Validation loss: 2.220335107977672

Epoch: 6| Step: 12
Training loss: 2.5915074348449707
Validation loss: 2.2033800373795214

Epoch: 6| Step: 13
Training loss: 2.0711512565612793
Validation loss: 2.1971630306654077

Epoch: 100| Step: 0
Training loss: 2.6101152896881104
Validation loss: 2.200103969984157

Epoch: 6| Step: 1
Training loss: 2.907621383666992
Validation loss: 2.1981587999610492

Epoch: 6| Step: 2
Training loss: 3.1789050102233887
Validation loss: 2.2031056880950928

Epoch: 6| Step: 3
Training loss: 3.221937656402588
Validation loss: 2.206655692028743

Epoch: 6| Step: 4
Training loss: 2.5884923934936523
Validation loss: 2.206976743154628

Epoch: 6| Step: 5
Training loss: 2.7412939071655273
Validation loss: 2.2022216089310183

Epoch: 6| Step: 6
Training loss: 2.126315116882324
Validation loss: 2.1981317932887743

Epoch: 6| Step: 7
Training loss: 1.9093801975250244
Validation loss: 2.2036758315178657

Epoch: 6| Step: 8
Training loss: 2.4976396560668945
Validation loss: 2.2076207155822427

Epoch: 6| Step: 9
Training loss: 3.2423696517944336
Validation loss: 2.2121007327110536

Epoch: 6| Step: 10
Training loss: 1.7719457149505615
Validation loss: 2.223257182746805

Epoch: 6| Step: 11
Training loss: 2.1074252128601074
Validation loss: 2.228346129899384

Epoch: 6| Step: 12
Training loss: 2.0894393920898438
Validation loss: 2.2512535331069783

Epoch: 6| Step: 13
Training loss: 1.6019171476364136
Validation loss: 2.2761919729171263

Epoch: 101| Step: 0
Training loss: 2.98903751373291
Validation loss: 2.3142474876937045

Epoch: 6| Step: 1
Training loss: 2.2379531860351562
Validation loss: 2.3773839063541864

Epoch: 6| Step: 2
Training loss: 2.7270612716674805
Validation loss: 2.3354129304168043

Epoch: 6| Step: 3
Training loss: 2.0021588802337646
Validation loss: 2.28717121770305

Epoch: 6| Step: 4
Training loss: 3.2286558151245117
Validation loss: 2.249557702772079

Epoch: 6| Step: 5
Training loss: 2.889883518218994
Validation loss: 2.218383219934279

Epoch: 6| Step: 6
Training loss: 1.8971002101898193
Validation loss: 2.2020285232092744

Epoch: 6| Step: 7
Training loss: 2.125476360321045
Validation loss: 2.197157206073884

Epoch: 6| Step: 8
Training loss: 2.1439931392669678
Validation loss: 2.1957617985304965

Epoch: 6| Step: 9
Training loss: 2.5634233951568604
Validation loss: 2.193223004700035

Epoch: 6| Step: 10
Training loss: 2.1357293128967285
Validation loss: 2.2014611664638726

Epoch: 6| Step: 11
Training loss: 2.8079583644866943
Validation loss: 2.198277158121909

Epoch: 6| Step: 12
Training loss: 2.9807982444763184
Validation loss: 2.199846965010448

Epoch: 6| Step: 13
Training loss: 2.4810454845428467
Validation loss: 2.2025913307743687

Epoch: 102| Step: 0
Training loss: 2.257741928100586
Validation loss: 2.189745846615043

Epoch: 6| Step: 1
Training loss: 2.3372349739074707
Validation loss: 2.190169590775685

Epoch: 6| Step: 2
Training loss: 3.5025017261505127
Validation loss: 2.1865981701881654

Epoch: 6| Step: 3
Training loss: 2.9317450523376465
Validation loss: 2.17996778539432

Epoch: 6| Step: 4
Training loss: 1.9879931211471558
Validation loss: 2.179212752208915

Epoch: 6| Step: 5
Training loss: 2.4858975410461426
Validation loss: 2.1853963046945553

Epoch: 6| Step: 6
Training loss: 2.2529783248901367
Validation loss: 2.18215411965565

Epoch: 6| Step: 7
Training loss: 2.023858070373535
Validation loss: 2.1800633989354616

Epoch: 6| Step: 8
Training loss: 2.852329730987549
Validation loss: 2.18259935737938

Epoch: 6| Step: 9
Training loss: 2.464162826538086
Validation loss: 2.178490497732675

Epoch: 6| Step: 10
Training loss: 2.809460401535034
Validation loss: 2.1832633992677093

Epoch: 6| Step: 11
Training loss: 2.274369716644287
Validation loss: 2.187303066253662

Epoch: 6| Step: 12
Training loss: 2.814265251159668
Validation loss: 2.193862889402656

Epoch: 6| Step: 13
Training loss: 1.210218071937561
Validation loss: 2.198495549540366

Epoch: 103| Step: 0
Training loss: 2.547708034515381
Validation loss: 2.228479869904057

Epoch: 6| Step: 1
Training loss: 2.1378350257873535
Validation loss: 2.2194637213983843

Epoch: 6| Step: 2
Training loss: 1.6950812339782715
Validation loss: 2.240105496939792

Epoch: 6| Step: 3
Training loss: 3.0201468467712402
Validation loss: 2.2418922378170874

Epoch: 6| Step: 4
Training loss: 2.405203104019165
Validation loss: 2.2471704354850193

Epoch: 6| Step: 5
Training loss: 2.6494204998016357
Validation loss: 2.235057250145943

Epoch: 6| Step: 6
Training loss: 2.651057243347168
Validation loss: 2.2263420730508785

Epoch: 6| Step: 7
Training loss: 1.9568824768066406
Validation loss: 2.211851853196339

Epoch: 6| Step: 8
Training loss: 2.2974629402160645
Validation loss: 2.2073619698965423

Epoch: 6| Step: 9
Training loss: 2.345473051071167
Validation loss: 2.199458401690247

Epoch: 6| Step: 10
Training loss: 2.6846513748168945
Validation loss: 2.2007396938980266

Epoch: 6| Step: 11
Training loss: 2.2447800636291504
Validation loss: 2.1942904354423605

Epoch: 6| Step: 12
Training loss: 3.286151647567749
Validation loss: 2.207154454723481

Epoch: 6| Step: 13
Training loss: 2.6465747356414795
Validation loss: 2.1984507614566433

Epoch: 104| Step: 0
Training loss: 2.518259286880493
Validation loss: 2.185828257632512

Epoch: 6| Step: 1
Training loss: 2.205068588256836
Validation loss: 2.1841127000829226

Epoch: 6| Step: 2
Training loss: 2.563494920730591
Validation loss: 2.18263513811173

Epoch: 6| Step: 3
Training loss: 2.4529733657836914
Validation loss: 2.194382393231956

Epoch: 6| Step: 4
Training loss: 2.667388916015625
Validation loss: 2.1911935857547227

Epoch: 6| Step: 5
Training loss: 2.3289971351623535
Validation loss: 2.1941287030455885

Epoch: 6| Step: 6
Training loss: 1.8266422748565674
Validation loss: 2.200437386830648

Epoch: 6| Step: 7
Training loss: 2.6697278022766113
Validation loss: 2.195419990888206

Epoch: 6| Step: 8
Training loss: 2.383657932281494
Validation loss: 2.1959181703546995

Epoch: 6| Step: 9
Training loss: 2.462364435195923
Validation loss: 2.180782884679815

Epoch: 6| Step: 10
Training loss: 2.576489210128784
Validation loss: 2.1927059465839016

Epoch: 6| Step: 11
Training loss: 2.8518006801605225
Validation loss: 2.1985772822492864

Epoch: 6| Step: 12
Training loss: 2.159937858581543
Validation loss: 2.2192832423794653

Epoch: 6| Step: 13
Training loss: 3.18723464012146
Validation loss: 2.225478782448717

Epoch: 105| Step: 0
Training loss: 2.920866012573242
Validation loss: 2.29550773866715

Epoch: 6| Step: 1
Training loss: 2.681720495223999
Validation loss: 2.324914388759162

Epoch: 6| Step: 2
Training loss: 2.1597275733947754
Validation loss: 2.310358585849885

Epoch: 6| Step: 3
Training loss: 1.6438385248184204
Validation loss: 2.302152590085101

Epoch: 6| Step: 4
Training loss: 2.4594764709472656
Validation loss: 2.303216277912099

Epoch: 6| Step: 5
Training loss: 3.5013585090637207
Validation loss: 2.2844288605515675

Epoch: 6| Step: 6
Training loss: 2.8535094261169434
Validation loss: 2.2426102443407943

Epoch: 6| Step: 7
Training loss: 1.786489725112915
Validation loss: 2.2046799070091656

Epoch: 6| Step: 8
Training loss: 1.9935529232025146
Validation loss: 2.1835548159896687

Epoch: 6| Step: 9
Training loss: 2.602351665496826
Validation loss: 2.1697894706520984

Epoch: 6| Step: 10
Training loss: 2.2757561206817627
Validation loss: 2.162020191069572

Epoch: 6| Step: 11
Training loss: 2.2268803119659424
Validation loss: 2.158059161196473

Epoch: 6| Step: 12
Training loss: 2.869069814682007
Validation loss: 2.156955013992966

Epoch: 6| Step: 13
Training loss: 2.9638137817382812
Validation loss: 2.157484362202306

Epoch: 106| Step: 0
Training loss: 2.2405529022216797
Validation loss: 2.160059790457449

Epoch: 6| Step: 1
Training loss: 2.3287854194641113
Validation loss: 2.150480480604274

Epoch: 6| Step: 2
Training loss: 2.566945791244507
Validation loss: 2.169206105252748

Epoch: 6| Step: 3
Training loss: 1.7553527355194092
Validation loss: 2.16816968558937

Epoch: 6| Step: 4
Training loss: 2.412142276763916
Validation loss: 2.1883567648549236

Epoch: 6| Step: 5
Training loss: 3.2091219425201416
Validation loss: 2.1976983521574285

Epoch: 6| Step: 6
Training loss: 2.298846483230591
Validation loss: 2.2136741966329594

Epoch: 6| Step: 7
Training loss: 2.5784783363342285
Validation loss: 2.2545552458814395

Epoch: 6| Step: 8
Training loss: 3.216829299926758
Validation loss: 2.2570528778978574

Epoch: 6| Step: 9
Training loss: 2.231278657913208
Validation loss: 2.2351205348968506

Epoch: 6| Step: 10
Training loss: 1.922213077545166
Validation loss: 2.210563170012607

Epoch: 6| Step: 11
Training loss: 3.246765613555908
Validation loss: 2.197759123258693

Epoch: 6| Step: 12
Training loss: 2.7665209770202637
Validation loss: 2.1677634600670106

Epoch: 6| Step: 13
Training loss: 1.2470687627792358
Validation loss: 2.1634381355777865

Epoch: 107| Step: 0
Training loss: 2.3697075843811035
Validation loss: 2.1686561415272374

Epoch: 6| Step: 1
Training loss: 2.708583354949951
Validation loss: 2.1643700702216035

Epoch: 6| Step: 2
Training loss: 2.9173202514648438
Validation loss: 2.1641297519847913

Epoch: 6| Step: 3
Training loss: 2.0642166137695312
Validation loss: 2.1715435879204863

Epoch: 6| Step: 4
Training loss: 2.47165584564209
Validation loss: 2.190040662724485

Epoch: 6| Step: 5
Training loss: 1.9955114126205444
Validation loss: 2.2042651945544827

Epoch: 6| Step: 6
Training loss: 2.5788686275482178
Validation loss: 2.217678289259634

Epoch: 6| Step: 7
Training loss: 2.5545191764831543
Validation loss: 2.232687988588887

Epoch: 6| Step: 8
Training loss: 3.1429362297058105
Validation loss: 2.226666304372972

Epoch: 6| Step: 9
Training loss: 1.9334187507629395
Validation loss: 2.2700578166592504

Epoch: 6| Step: 10
Training loss: 3.1505038738250732
Validation loss: 2.307218723399665

Epoch: 6| Step: 11
Training loss: 2.5559868812561035
Validation loss: 2.2701012998498897

Epoch: 6| Step: 12
Training loss: 2.1490049362182617
Validation loss: 2.238138614162322

Epoch: 6| Step: 13
Training loss: 1.4544497728347778
Validation loss: 2.198757788186432

Epoch: 108| Step: 0
Training loss: 2.133044958114624
Validation loss: 2.183452252418764

Epoch: 6| Step: 1
Training loss: 2.8433051109313965
Validation loss: 2.165335714176137

Epoch: 6| Step: 2
Training loss: 2.9297406673431396
Validation loss: 2.1737643082936606

Epoch: 6| Step: 3
Training loss: 2.746819496154785
Validation loss: 2.1825730364809752

Epoch: 6| Step: 4
Training loss: 2.7585952281951904
Validation loss: 2.1922888037978963

Epoch: 6| Step: 5
Training loss: 2.4201035499572754
Validation loss: 2.1864036642095095

Epoch: 6| Step: 6
Training loss: 2.8834171295166016
Validation loss: 2.184036006209671

Epoch: 6| Step: 7
Training loss: 1.5003607273101807
Validation loss: 2.178596786273423

Epoch: 6| Step: 8
Training loss: 2.8900513648986816
Validation loss: 2.170798986188827

Epoch: 6| Step: 9
Training loss: 2.1130001544952393
Validation loss: 2.1745425654995825

Epoch: 6| Step: 10
Training loss: 2.042383909225464
Validation loss: 2.1673852628277195

Epoch: 6| Step: 11
Training loss: 2.439723014831543
Validation loss: 2.1754987842293194

Epoch: 6| Step: 12
Training loss: 2.351379632949829
Validation loss: 2.1692938778990056

Epoch: 6| Step: 13
Training loss: 2.8866539001464844
Validation loss: 2.179450899042109

Epoch: 109| Step: 0
Training loss: 2.46219539642334
Validation loss: 2.1640984576235534

Epoch: 6| Step: 1
Training loss: 2.7874488830566406
Validation loss: 2.1587217879551712

Epoch: 6| Step: 2
Training loss: 2.301905632019043
Validation loss: 2.1604346972639843

Epoch: 6| Step: 3
Training loss: 2.301687479019165
Validation loss: 2.164323506816741

Epoch: 6| Step: 4
Training loss: 2.1781208515167236
Validation loss: 2.1527706474386235

Epoch: 6| Step: 5
Training loss: 3.293043613433838
Validation loss: 2.1517820794095277

Epoch: 6| Step: 6
Training loss: 2.6147217750549316
Validation loss: 2.145556631908622

Epoch: 6| Step: 7
Training loss: 2.1280202865600586
Validation loss: 2.1594460574529504

Epoch: 6| Step: 8
Training loss: 2.5155344009399414
Validation loss: 2.1718242988791516

Epoch: 6| Step: 9
Training loss: 1.5763232707977295
Validation loss: 2.185541888718964

Epoch: 6| Step: 10
Training loss: 2.8199987411499023
Validation loss: 2.188776034180836

Epoch: 6| Step: 11
Training loss: 2.158033847808838
Validation loss: 2.210328984004195

Epoch: 6| Step: 12
Training loss: 2.3849871158599854
Validation loss: 2.236395953803934

Epoch: 6| Step: 13
Training loss: 3.246572732925415
Validation loss: 2.2577567420979983

Epoch: 110| Step: 0
Training loss: 2.196457862854004
Validation loss: 2.2194370338993687

Epoch: 6| Step: 1
Training loss: 2.084254264831543
Validation loss: 2.2120773740994033

Epoch: 6| Step: 2
Training loss: 2.3563623428344727
Validation loss: 2.1815618981597242

Epoch: 6| Step: 3
Training loss: 2.7570765018463135
Validation loss: 2.1611050457082768

Epoch: 6| Step: 4
Training loss: 1.720447063446045
Validation loss: 2.1470297613451557

Epoch: 6| Step: 5
Training loss: 2.6132431030273438
Validation loss: 2.141634659100604

Epoch: 6| Step: 6
Training loss: 2.2241101264953613
Validation loss: 2.138848799531178

Epoch: 6| Step: 7
Training loss: 2.1269030570983887
Validation loss: 2.1396135155872633

Epoch: 6| Step: 8
Training loss: 2.9311797618865967
Validation loss: 2.1377218179805304

Epoch: 6| Step: 9
Training loss: 3.2331249713897705
Validation loss: 2.1339284578959146

Epoch: 6| Step: 10
Training loss: 2.836733818054199
Validation loss: 2.1299249228610786

Epoch: 6| Step: 11
Training loss: 2.6436891555786133
Validation loss: 2.1285029124188166

Epoch: 6| Step: 12
Training loss: 2.530702829360962
Validation loss: 2.1334526923394974

Epoch: 6| Step: 13
Training loss: 1.7238092422485352
Validation loss: 2.1338790834590955

Epoch: 111| Step: 0
Training loss: 1.9860715866088867
Validation loss: 2.127504938392229

Epoch: 6| Step: 1
Training loss: 2.794633388519287
Validation loss: 2.1256496406370595

Epoch: 6| Step: 2
Training loss: 2.317617654800415
Validation loss: 2.139394837041055

Epoch: 6| Step: 3
Training loss: 2.538391590118408
Validation loss: 2.131886601448059

Epoch: 6| Step: 4
Training loss: 2.0662245750427246
Validation loss: 2.158911351234682

Epoch: 6| Step: 5
Training loss: 2.6095693111419678
Validation loss: 2.1727979157560613

Epoch: 6| Step: 6
Training loss: 2.52912974357605
Validation loss: 2.221790667503111

Epoch: 6| Step: 7
Training loss: 2.3548743724823
Validation loss: 2.283190634942824

Epoch: 6| Step: 8
Training loss: 2.1057631969451904
Validation loss: 2.3005204098199004

Epoch: 6| Step: 9
Training loss: 2.9253695011138916
Validation loss: 2.298419487091803

Epoch: 6| Step: 10
Training loss: 2.572392225265503
Validation loss: 2.2565219940677768

Epoch: 6| Step: 11
Training loss: 2.9614787101745605
Validation loss: 2.2030053010550876

Epoch: 6| Step: 12
Training loss: 2.355257987976074
Validation loss: 2.173646003969254

Epoch: 6| Step: 13
Training loss: 2.370863676071167
Validation loss: 2.1632866013434624

Epoch: 112| Step: 0
Training loss: 3.1994552612304688
Validation loss: 2.160800701828413

Epoch: 6| Step: 1
Training loss: 2.5769786834716797
Validation loss: 2.1792950014914236

Epoch: 6| Step: 2
Training loss: 3.2121005058288574
Validation loss: 2.177470048268636

Epoch: 6| Step: 3
Training loss: 2.4739432334899902
Validation loss: 2.1686205697315994

Epoch: 6| Step: 4
Training loss: 3.2109551429748535
Validation loss: 2.1615517062525593

Epoch: 6| Step: 5
Training loss: 2.650644540786743
Validation loss: 2.171338927361273

Epoch: 6| Step: 6
Training loss: 2.4508838653564453
Validation loss: 2.1799959905685915

Epoch: 6| Step: 7
Training loss: 2.6388113498687744
Validation loss: 2.2111097228142524

Epoch: 6| Step: 8
Training loss: 2.5420961380004883
Validation loss: 2.1785882672955914

Epoch: 6| Step: 9
Training loss: 1.6197431087493896
Validation loss: 2.1763345579947195

Epoch: 6| Step: 10
Training loss: 2.122293472290039
Validation loss: 2.153714597866099

Epoch: 6| Step: 11
Training loss: 2.4785823822021484
Validation loss: 2.1557427555002193

Epoch: 6| Step: 12
Training loss: 1.7499973773956299
Validation loss: 2.163664061536071

Epoch: 6| Step: 13
Training loss: 1.6033687591552734
Validation loss: 2.165761245194302

Epoch: 113| Step: 0
Training loss: 2.2735438346862793
Validation loss: 2.1683953398017475

Epoch: 6| Step: 1
Training loss: 2.674203395843506
Validation loss: 2.186917822848084

Epoch: 6| Step: 2
Training loss: 3.080005407333374
Validation loss: 2.259852022253057

Epoch: 6| Step: 3
Training loss: 2.6890337467193604
Validation loss: 2.3702950477600098

Epoch: 6| Step: 4
Training loss: 2.1047916412353516
Validation loss: 2.4359793432297243

Epoch: 6| Step: 5
Training loss: 3.480865716934204
Validation loss: 2.417415782969485

Epoch: 6| Step: 6
Training loss: 2.062479257583618
Validation loss: 2.3705147927807224

Epoch: 6| Step: 7
Training loss: 2.022151470184326
Validation loss: 2.309284706269541

Epoch: 6| Step: 8
Training loss: 2.6924233436584473
Validation loss: 2.247490723927816

Epoch: 6| Step: 9
Training loss: 2.5338432788848877
Validation loss: 2.2216507286153813

Epoch: 6| Step: 10
Training loss: 2.239659547805786
Validation loss: 2.2007999779075704

Epoch: 6| Step: 11
Training loss: 2.18839430809021
Validation loss: 2.235511697748656

Epoch: 6| Step: 12
Training loss: 2.415327787399292
Validation loss: 2.2607540251106344

Epoch: 6| Step: 13
Training loss: 2.7451398372650146
Validation loss: 2.2749790530050955

Epoch: 114| Step: 0
Training loss: 2.8080108165740967
Validation loss: 2.25889685077052

Epoch: 6| Step: 1
Training loss: 2.165276050567627
Validation loss: 2.2232256730397544

Epoch: 6| Step: 2
Training loss: 2.2137885093688965
Validation loss: 2.212122953066262

Epoch: 6| Step: 3
Training loss: 2.309058666229248
Validation loss: 2.1505139104781614

Epoch: 6| Step: 4
Training loss: 2.261706829071045
Validation loss: 2.12746246399418

Epoch: 6| Step: 5
Training loss: 3.024569034576416
Validation loss: 2.1227841633622364

Epoch: 6| Step: 6
Training loss: 2.3776731491088867
Validation loss: 2.1270076792727233

Epoch: 6| Step: 7
Training loss: 2.5656914710998535
Validation loss: 2.1292800711047266

Epoch: 6| Step: 8
Training loss: 2.7153329849243164
Validation loss: 2.13588354023554

Epoch: 6| Step: 9
Training loss: 2.709010124206543
Validation loss: 2.1576908762736986

Epoch: 6| Step: 10
Training loss: 2.1524643898010254
Validation loss: 2.1648069171495337

Epoch: 6| Step: 11
Training loss: 2.450913667678833
Validation loss: 2.1729860510877383

Epoch: 6| Step: 12
Training loss: 2.2447028160095215
Validation loss: 2.1943183714343655

Epoch: 6| Step: 13
Training loss: 2.0614640712738037
Validation loss: 2.1949155023021083

Epoch: 115| Step: 0
Training loss: 2.858154058456421
Validation loss: 2.2030856429889636

Epoch: 6| Step: 1
Training loss: 2.4947586059570312
Validation loss: 2.2165141951653267

Epoch: 6| Step: 2
Training loss: 2.88669490814209
Validation loss: 2.191543963647658

Epoch: 6| Step: 3
Training loss: 1.990823745727539
Validation loss: 2.185358424340525

Epoch: 6| Step: 4
Training loss: 2.0928287506103516
Validation loss: 2.1688715514316352

Epoch: 6| Step: 5
Training loss: 2.6471707820892334
Validation loss: 2.15568789615426

Epoch: 6| Step: 6
Training loss: 2.6511709690093994
Validation loss: 2.1400237109071467

Epoch: 6| Step: 7
Training loss: 2.020009756088257
Validation loss: 2.1348758333472797

Epoch: 6| Step: 8
Training loss: 2.6943414211273193
Validation loss: 2.131586890066824

Epoch: 6| Step: 9
Training loss: 2.7650206089019775
Validation loss: 2.1200230224158174

Epoch: 6| Step: 10
Training loss: 2.5136687755584717
Validation loss: 2.1132750100986932

Epoch: 6| Step: 11
Training loss: 2.6229147911071777
Validation loss: 2.111372035036805

Epoch: 6| Step: 12
Training loss: 1.5874805450439453
Validation loss: 2.10208527247111

Epoch: 6| Step: 13
Training loss: 1.9334921836853027
Validation loss: 2.102884974530948

Epoch: 116| Step: 0
Training loss: 2.0679702758789062
Validation loss: 2.106891350079608

Epoch: 6| Step: 1
Training loss: 2.770340919494629
Validation loss: 2.1014298072425266

Epoch: 6| Step: 2
Training loss: 3.0667223930358887
Validation loss: 2.0983441260553177

Epoch: 6| Step: 3
Training loss: 2.0796735286712646
Validation loss: 2.1017991471034225

Epoch: 6| Step: 4
Training loss: 2.906318187713623
Validation loss: 2.097423320175499

Epoch: 6| Step: 5
Training loss: 2.2782812118530273
Validation loss: 2.0993234636963054

Epoch: 6| Step: 6
Training loss: 2.2473928928375244
Validation loss: 2.1062915325164795

Epoch: 6| Step: 7
Training loss: 2.6015148162841797
Validation loss: 2.1075019644152735

Epoch: 6| Step: 8
Training loss: 1.8234091997146606
Validation loss: 2.1225478085138465

Epoch: 6| Step: 9
Training loss: 2.8752689361572266
Validation loss: 2.132605278363792

Epoch: 6| Step: 10
Training loss: 2.1128993034362793
Validation loss: 2.131041287094034

Epoch: 6| Step: 11
Training loss: 2.4350075721740723
Validation loss: 2.1553985149629655

Epoch: 6| Step: 12
Training loss: 2.5185601711273193
Validation loss: 2.1853653359156784

Epoch: 6| Step: 13
Training loss: 1.5833120346069336
Validation loss: 2.225407144074799

Epoch: 117| Step: 0
Training loss: 2.4798834323883057
Validation loss: 2.226750376403973

Epoch: 6| Step: 1
Training loss: 2.0936813354492188
Validation loss: 2.198000648970245

Epoch: 6| Step: 2
Training loss: 2.013152599334717
Validation loss: 2.1824244965789137

Epoch: 6| Step: 3
Training loss: 2.651707649230957
Validation loss: 2.144466515510313

Epoch: 6| Step: 4
Training loss: 2.4663143157958984
Validation loss: 2.118262683191607

Epoch: 6| Step: 5
Training loss: 2.4584386348724365
Validation loss: 2.1158069410631732

Epoch: 6| Step: 6
Training loss: 2.5499606132507324
Validation loss: 2.1132275314741236

Epoch: 6| Step: 7
Training loss: 2.2195188999176025
Validation loss: 2.1174632118594263

Epoch: 6| Step: 8
Training loss: 2.560142993927002
Validation loss: 2.130751904620919

Epoch: 6| Step: 9
Training loss: 1.9096003770828247
Validation loss: 2.1337286349265807

Epoch: 6| Step: 10
Training loss: 3.0419445037841797
Validation loss: 2.137712763201806

Epoch: 6| Step: 11
Training loss: 2.6500449180603027
Validation loss: 2.148390254666728

Epoch: 6| Step: 12
Training loss: 2.446579694747925
Validation loss: 2.1459755589885097

Epoch: 6| Step: 13
Training loss: 2.0907435417175293
Validation loss: 2.1174286386018157

Epoch: 118| Step: 0
Training loss: 2.727278232574463
Validation loss: 2.1145088544455906

Epoch: 6| Step: 1
Training loss: 2.3736398220062256
Validation loss: 2.1074815001539005

Epoch: 6| Step: 2
Training loss: 1.864630937576294
Validation loss: 2.1049758875241844

Epoch: 6| Step: 3
Training loss: 1.9399755001068115
Validation loss: 2.104312296836607

Epoch: 6| Step: 4
Training loss: 2.8273282051086426
Validation loss: 2.096529786304761

Epoch: 6| Step: 5
Training loss: 2.501983642578125
Validation loss: 2.0904152367704656

Epoch: 6| Step: 6
Training loss: 1.6415889263153076
Validation loss: 2.099817760529057

Epoch: 6| Step: 7
Training loss: 2.6056947708129883
Validation loss: 2.0948420622015513

Epoch: 6| Step: 8
Training loss: 3.1482326984405518
Validation loss: 2.0953151974626767

Epoch: 6| Step: 9
Training loss: 1.9658865928649902
Validation loss: 2.122767117715651

Epoch: 6| Step: 10
Training loss: 1.84079909324646
Validation loss: 2.1380090098227225

Epoch: 6| Step: 11
Training loss: 2.650059223175049
Validation loss: 2.1390970855630855

Epoch: 6| Step: 12
Training loss: 2.881448268890381
Validation loss: 2.1610400625454482

Epoch: 6| Step: 13
Training loss: 3.0257351398468018
Validation loss: 2.154353678867381

Epoch: 119| Step: 0
Training loss: 2.6305577754974365
Validation loss: 2.1217193372787966

Epoch: 6| Step: 1
Training loss: 2.9174551963806152
Validation loss: 2.0999608603856896

Epoch: 6| Step: 2
Training loss: 2.079730987548828
Validation loss: 2.0876956524387484

Epoch: 6| Step: 3
Training loss: 2.2710068225860596
Validation loss: 2.0775761014671734

Epoch: 6| Step: 4
Training loss: 2.7251994609832764
Validation loss: 2.0925973051337787

Epoch: 6| Step: 5
Training loss: 2.1541664600372314
Validation loss: 2.0855531666868474

Epoch: 6| Step: 6
Training loss: 2.6053192615509033
Validation loss: 2.0893442758949856

Epoch: 6| Step: 7
Training loss: 1.7370988130569458
Validation loss: 2.0823578501260407

Epoch: 6| Step: 8
Training loss: 1.998299241065979
Validation loss: 2.0942596620129

Epoch: 6| Step: 9
Training loss: 2.512643575668335
Validation loss: 2.1056677449134087

Epoch: 6| Step: 10
Training loss: 2.2021021842956543
Validation loss: 2.113730658767044

Epoch: 6| Step: 11
Training loss: 2.4416704177856445
Validation loss: 2.1129778841490388

Epoch: 6| Step: 12
Training loss: 2.932730197906494
Validation loss: 2.107701955303069

Epoch: 6| Step: 13
Training loss: 2.162461280822754
Validation loss: 2.105764291619742

Epoch: 120| Step: 0
Training loss: 2.5379137992858887
Validation loss: 2.0997729224543416

Epoch: 6| Step: 1
Training loss: 2.3550822734832764
Validation loss: 2.0985113036247993

Epoch: 6| Step: 2
Training loss: 2.5446243286132812
Validation loss: 2.100165410708356

Epoch: 6| Step: 3
Training loss: 2.702958822250366
Validation loss: 2.105942690244285

Epoch: 6| Step: 4
Training loss: 2.846742630004883
Validation loss: 2.0996952620885705

Epoch: 6| Step: 5
Training loss: 1.7206242084503174
Validation loss: 2.1048525148822415

Epoch: 6| Step: 6
Training loss: 2.1091325283050537
Validation loss: 2.100148821389803

Epoch: 6| Step: 7
Training loss: 2.0049428939819336
Validation loss: 2.0928038973962106

Epoch: 6| Step: 8
Training loss: 2.3660407066345215
Validation loss: 2.088418309406568

Epoch: 6| Step: 9
Training loss: 2.94744610786438
Validation loss: 2.088767150396942

Epoch: 6| Step: 10
Training loss: 1.7806475162506104
Validation loss: 2.1065038481066303

Epoch: 6| Step: 11
Training loss: 2.302766799926758
Validation loss: 2.114348428223723

Epoch: 6| Step: 12
Training loss: 2.7954227924346924
Validation loss: 2.1427633454722743

Epoch: 6| Step: 13
Training loss: 2.471259593963623
Validation loss: 2.162919564913678

Epoch: 121| Step: 0
Training loss: 1.1375123262405396
Validation loss: 2.1528616874448714

Epoch: 6| Step: 1
Training loss: 2.37807035446167
Validation loss: 2.1312661017141035

Epoch: 6| Step: 2
Training loss: 2.7606000900268555
Validation loss: 2.1248129465246715

Epoch: 6| Step: 3
Training loss: 1.785976529121399
Validation loss: 2.0953491016100814

Epoch: 6| Step: 4
Training loss: 2.687619924545288
Validation loss: 2.0894839712368545

Epoch: 6| Step: 5
Training loss: 2.311307430267334
Validation loss: 2.0822723091289563

Epoch: 6| Step: 6
Training loss: 2.4194231033325195
Validation loss: 2.0791980553698797

Epoch: 6| Step: 7
Training loss: 3.02667498588562
Validation loss: 2.0814250387171263

Epoch: 6| Step: 8
Training loss: 2.8392417430877686
Validation loss: 2.082724412282308

Epoch: 6| Step: 9
Training loss: 2.2202486991882324
Validation loss: 2.0811294201881654

Epoch: 6| Step: 10
Training loss: 2.951575756072998
Validation loss: 2.079543446981779

Epoch: 6| Step: 11
Training loss: 2.479846477508545
Validation loss: 2.0800414290479434

Epoch: 6| Step: 12
Training loss: 1.8660238981246948
Validation loss: 2.0849578329311904

Epoch: 6| Step: 13
Training loss: 2.3195078372955322
Validation loss: 2.0875404188709874

Epoch: 122| Step: 0
Training loss: 2.5965747833251953
Validation loss: 2.0928489238985124

Epoch: 6| Step: 1
Training loss: 1.7481179237365723
Validation loss: 2.0930988788604736

Epoch: 6| Step: 2
Training loss: 2.8743324279785156
Validation loss: 2.101454298983338

Epoch: 6| Step: 3
Training loss: 2.6366357803344727
Validation loss: 2.0923811774100027

Epoch: 6| Step: 4
Training loss: 2.484471321105957
Validation loss: 2.0972794896812847

Epoch: 6| Step: 5
Training loss: 2.2279560565948486
Validation loss: 2.100401762993105

Epoch: 6| Step: 6
Training loss: 1.9946061372756958
Validation loss: 2.09990555496626

Epoch: 6| Step: 7
Training loss: 2.593356132507324
Validation loss: 2.099492355059552

Epoch: 6| Step: 8
Training loss: 2.2266056537628174
Validation loss: 2.111090211458104

Epoch: 6| Step: 9
Training loss: 1.843008279800415
Validation loss: 2.1020236989503265

Epoch: 6| Step: 10
Training loss: 3.0093765258789062
Validation loss: 2.099666432667804

Epoch: 6| Step: 11
Training loss: 3.1652700901031494
Validation loss: 2.0813019442301925

Epoch: 6| Step: 12
Training loss: 1.702648639678955
Validation loss: 2.084201169270341

Epoch: 6| Step: 13
Training loss: 1.8122831583023071
Validation loss: 2.085951979442309

Epoch: 123| Step: 0
Training loss: 2.336348056793213
Validation loss: 2.08256870700467

Epoch: 6| Step: 1
Training loss: 2.6213295459747314
Validation loss: 2.1054552857593825

Epoch: 6| Step: 2
Training loss: 1.884967565536499
Validation loss: 2.1353155412981586

Epoch: 6| Step: 3
Training loss: 2.355348587036133
Validation loss: 2.1339421913187993

Epoch: 6| Step: 4
Training loss: 2.116615056991577
Validation loss: 2.128652677741102

Epoch: 6| Step: 5
Training loss: 2.6247029304504395
Validation loss: 2.106873550722676

Epoch: 6| Step: 6
Training loss: 2.1883902549743652
Validation loss: 2.1190122994043494

Epoch: 6| Step: 7
Training loss: 2.57424259185791
Validation loss: 2.096404916496687

Epoch: 6| Step: 8
Training loss: 2.4410958290100098
Validation loss: 2.0828557552829867

Epoch: 6| Step: 9
Training loss: 2.1964125633239746
Validation loss: 2.086392302666941

Epoch: 6| Step: 10
Training loss: 2.321676254272461
Validation loss: 2.083340524345316

Epoch: 6| Step: 11
Training loss: 2.702086925506592
Validation loss: 2.0787253815640687

Epoch: 6| Step: 12
Training loss: 2.154580593109131
Validation loss: 2.080270077592583

Epoch: 6| Step: 13
Training loss: 2.8332033157348633
Validation loss: 2.0564415788137786

Epoch: 124| Step: 0
Training loss: 2.392580509185791
Validation loss: 2.0637274685726372

Epoch: 6| Step: 1
Training loss: 1.8539854288101196
Validation loss: 2.0665320939915155

Epoch: 6| Step: 2
Training loss: 2.702460527420044
Validation loss: 2.0570956917219263

Epoch: 6| Step: 3
Training loss: 2.0885307788848877
Validation loss: 2.0573290368562103

Epoch: 6| Step: 4
Training loss: 2.4820456504821777
Validation loss: 2.0582881307089202

Epoch: 6| Step: 5
Training loss: 2.284965753555298
Validation loss: 2.074072357146971

Epoch: 6| Step: 6
Training loss: 2.4963512420654297
Validation loss: 2.0902694092001965

Epoch: 6| Step: 7
Training loss: 2.392334461212158
Validation loss: 2.1107150764875513

Epoch: 6| Step: 8
Training loss: 2.290357828140259
Validation loss: 2.0889575532687608

Epoch: 6| Step: 9
Training loss: 2.026278018951416
Validation loss: 2.080719304341142

Epoch: 6| Step: 10
Training loss: 3.221543312072754
Validation loss: 2.067178113486177

Epoch: 6| Step: 11
Training loss: 2.596731662750244
Validation loss: 2.0705008519593107

Epoch: 6| Step: 12
Training loss: 1.8742730617523193
Validation loss: 2.076094035179384

Epoch: 6| Step: 13
Training loss: 2.150137186050415
Validation loss: 2.086272342230684

Epoch: 125| Step: 0
Training loss: 2.5385570526123047
Validation loss: 2.0866251376367386

Epoch: 6| Step: 1
Training loss: 2.727316379547119
Validation loss: 2.0941987165840725

Epoch: 6| Step: 2
Training loss: 2.411867380142212
Validation loss: 2.08016514009045

Epoch: 6| Step: 3
Training loss: 2.284132480621338
Validation loss: 2.0784911289009997

Epoch: 6| Step: 4
Training loss: 1.772445797920227
Validation loss: 2.0692477456984983

Epoch: 6| Step: 5
Training loss: 2.8435401916503906
Validation loss: 2.0883064962202504

Epoch: 6| Step: 6
Training loss: 2.5454063415527344
Validation loss: 2.121893810969527

Epoch: 6| Step: 7
Training loss: 2.381211280822754
Validation loss: 2.1053970552259877

Epoch: 6| Step: 8
Training loss: 1.7556791305541992
Validation loss: 2.077613720329859

Epoch: 6| Step: 9
Training loss: 2.2014763355255127
Validation loss: 2.0558297352124284

Epoch: 6| Step: 10
Training loss: 2.3675343990325928
Validation loss: 2.065131800149077

Epoch: 6| Step: 11
Training loss: 2.8025693893432617
Validation loss: 2.05557922650409

Epoch: 6| Step: 12
Training loss: 1.9302250146865845
Validation loss: 2.064261964572373

Epoch: 6| Step: 13
Training loss: 2.551809787750244
Validation loss: 2.0549563233570387

Epoch: 126| Step: 0
Training loss: 2.5349926948547363
Validation loss: 2.0583631825703446

Epoch: 6| Step: 1
Training loss: 2.056520462036133
Validation loss: 2.055923769550939

Epoch: 6| Step: 2
Training loss: 1.9765982627868652
Validation loss: 2.0611268243482037

Epoch: 6| Step: 3
Training loss: 2.792149066925049
Validation loss: 2.0784471675913823

Epoch: 6| Step: 4
Training loss: 2.1773006916046143
Validation loss: 2.091303802305652

Epoch: 6| Step: 5
Training loss: 2.586768627166748
Validation loss: 2.1099437462386263

Epoch: 6| Step: 6
Training loss: 3.1318728923797607
Validation loss: 2.134839037413238

Epoch: 6| Step: 7
Training loss: 2.6955037117004395
Validation loss: 2.1021757946219495

Epoch: 6| Step: 8
Training loss: 2.377758026123047
Validation loss: 2.083358797975766

Epoch: 6| Step: 9
Training loss: 1.8248958587646484
Validation loss: 2.0626902439260997

Epoch: 6| Step: 10
Training loss: 2.5635976791381836
Validation loss: 2.0625803086065475

Epoch: 6| Step: 11
Training loss: 1.9648425579071045
Validation loss: 2.0637586655155307

Epoch: 6| Step: 12
Training loss: 1.9966334104537964
Validation loss: 2.070620116367135

Epoch: 6| Step: 13
Training loss: 2.1739699840545654
Validation loss: 2.0745295068269134

Epoch: 127| Step: 0
Training loss: 1.9633984565734863
Validation loss: 2.0671492186925744

Epoch: 6| Step: 1
Training loss: 2.091447353363037
Validation loss: 2.072834309711251

Epoch: 6| Step: 2
Training loss: 2.652181625366211
Validation loss: 2.0710077952313166

Epoch: 6| Step: 3
Training loss: 2.1591622829437256
Validation loss: 2.0697332453984085

Epoch: 6| Step: 4
Training loss: 2.2442383766174316
Validation loss: 2.0577714430388583

Epoch: 6| Step: 5
Training loss: 2.5338222980499268
Validation loss: 2.06216279409265

Epoch: 6| Step: 6
Training loss: 2.224790573120117
Validation loss: 2.051427459204069

Epoch: 6| Step: 7
Training loss: 3.1608192920684814
Validation loss: 2.0586016908768685

Epoch: 6| Step: 8
Training loss: 2.360400676727295
Validation loss: 2.054281929487823

Epoch: 6| Step: 9
Training loss: 3.257981777191162
Validation loss: 2.0582418185408398

Epoch: 6| Step: 10
Training loss: 1.3258752822875977
Validation loss: 2.0562871886837866

Epoch: 6| Step: 11
Training loss: 1.7419933080673218
Validation loss: 2.0657556774795696

Epoch: 6| Step: 12
Training loss: 2.2862236499786377
Validation loss: 2.059146391448154

Epoch: 6| Step: 13
Training loss: 3.0723471641540527
Validation loss: 2.063022066188115

Epoch: 128| Step: 0
Training loss: 2.4061293601989746
Validation loss: 2.0732415465898413

Epoch: 6| Step: 1
Training loss: 2.251075267791748
Validation loss: 2.0913140132863033

Epoch: 6| Step: 2
Training loss: 2.8608505725860596
Validation loss: 2.0946110627984487

Epoch: 6| Step: 3
Training loss: 1.9167848825454712
Validation loss: 2.0981792942170174

Epoch: 6| Step: 4
Training loss: 2.6407229900360107
Validation loss: 2.1150478829619703

Epoch: 6| Step: 5
Training loss: 2.2446279525756836
Validation loss: 2.1205287633403653

Epoch: 6| Step: 6
Training loss: 2.294511079788208
Validation loss: 2.1226868398727907

Epoch: 6| Step: 7
Training loss: 2.838440418243408
Validation loss: 2.096541143232776

Epoch: 6| Step: 8
Training loss: 1.7199360132217407
Validation loss: 2.06216142254491

Epoch: 6| Step: 9
Training loss: 1.8435442447662354
Validation loss: 2.0581416635103125

Epoch: 6| Step: 10
Training loss: 2.6665873527526855
Validation loss: 2.049798869317578

Epoch: 6| Step: 11
Training loss: 1.9245952367782593
Validation loss: 2.0417894342894196

Epoch: 6| Step: 12
Training loss: 2.2482171058654785
Validation loss: 2.0385083767675583

Epoch: 6| Step: 13
Training loss: 3.005303382873535
Validation loss: 2.038909637799827

Epoch: 129| Step: 0
Training loss: 2.5786375999450684
Validation loss: 2.0402697363207416

Epoch: 6| Step: 1
Training loss: 2.632005214691162
Validation loss: 2.0366365332757272

Epoch: 6| Step: 2
Training loss: 1.580159306526184
Validation loss: 2.0385834478562876

Epoch: 6| Step: 3
Training loss: 2.687727928161621
Validation loss: 2.0329313098743396

Epoch: 6| Step: 4
Training loss: 2.570762872695923
Validation loss: 2.0387590854398665

Epoch: 6| Step: 5
Training loss: 1.840712547302246
Validation loss: 2.0396474099928334

Epoch: 6| Step: 6
Training loss: 2.4695656299591064
Validation loss: 2.0553430229104976

Epoch: 6| Step: 7
Training loss: 2.1802847385406494
Validation loss: 2.1122825863540813

Epoch: 6| Step: 8
Training loss: 1.8283172845840454
Validation loss: 2.173658214589601

Epoch: 6| Step: 9
Training loss: 2.6277642250061035
Validation loss: 2.129441604819349

Epoch: 6| Step: 10
Training loss: 2.396034002304077
Validation loss: 2.122529250319286

Epoch: 6| Step: 11
Training loss: 1.822070837020874
Validation loss: 2.0809047709229174

Epoch: 6| Step: 12
Training loss: 2.330442428588867
Validation loss: 2.0559118563129055

Epoch: 6| Step: 13
Training loss: 3.6601133346557617
Validation loss: 2.0445517211832027

Epoch: 130| Step: 0
Training loss: 2.577341079711914
Validation loss: 2.0543869938901675

Epoch: 6| Step: 1
Training loss: 1.7412257194519043
Validation loss: 2.1005540329922914

Epoch: 6| Step: 2
Training loss: 2.4958269596099854
Validation loss: 2.134464761262299

Epoch: 6| Step: 3
Training loss: 2.73819637298584
Validation loss: 2.1482239589896253

Epoch: 6| Step: 4
Training loss: 1.597011685371399
Validation loss: 2.151815496465211

Epoch: 6| Step: 5
Training loss: 1.9184659719467163
Validation loss: 2.1565177338097685

Epoch: 6| Step: 6
Training loss: 2.02217435836792
Validation loss: 2.1254986870673394

Epoch: 6| Step: 7
Training loss: 3.2502102851867676
Validation loss: 2.1113615177010976

Epoch: 6| Step: 8
Training loss: 2.019028663635254
Validation loss: 2.126657161661374

Epoch: 6| Step: 9
Training loss: 3.1201648712158203
Validation loss: 2.1985876573029386

Epoch: 6| Step: 10
Training loss: 2.350893259048462
Validation loss: 2.251495638201314

Epoch: 6| Step: 11
Training loss: 2.7400918006896973
Validation loss: 2.3045311794486096

Epoch: 6| Step: 12
Training loss: 2.398927688598633
Validation loss: 2.33963438259658

Epoch: 6| Step: 13
Training loss: 3.021848201751709
Validation loss: 2.319580852344472

Epoch: 131| Step: 0
Training loss: 2.1123616695404053
Validation loss: 2.2644864231027584

Epoch: 6| Step: 1
Training loss: 2.4903616905212402
Validation loss: 2.1815588551182903

Epoch: 6| Step: 2
Training loss: 2.6066699028015137
Validation loss: 2.103658973529775

Epoch: 6| Step: 3
Training loss: 2.5265555381774902
Validation loss: 2.0651688960290726

Epoch: 6| Step: 4
Training loss: 2.8206424713134766
Validation loss: 2.0676151398689515

Epoch: 6| Step: 5
Training loss: 1.1921696662902832
Validation loss: 2.0763505133249427

Epoch: 6| Step: 6
Training loss: 3.0169677734375
Validation loss: 2.07828515319414

Epoch: 6| Step: 7
Training loss: 2.398723602294922
Validation loss: 2.072776279141826

Epoch: 6| Step: 8
Training loss: 1.921010971069336
Validation loss: 2.0569916040666643

Epoch: 6| Step: 9
Training loss: 2.5568318367004395
Validation loss: 2.0666837307714645

Epoch: 6| Step: 10
Training loss: 2.2497267723083496
Validation loss: 2.055739979590139

Epoch: 6| Step: 11
Training loss: 2.0530996322631836
Validation loss: 2.0647991729039017

Epoch: 6| Step: 12
Training loss: 2.326437473297119
Validation loss: 2.072648502165271

Epoch: 6| Step: 13
Training loss: 3.1722512245178223
Validation loss: 2.095750308805896

Epoch: 132| Step: 0
Training loss: 2.3702425956726074
Validation loss: 2.099597751453359

Epoch: 6| Step: 1
Training loss: 2.7744929790496826
Validation loss: 2.1173735754464262

Epoch: 6| Step: 2
Training loss: 2.0009052753448486
Validation loss: 2.1185536128218456

Epoch: 6| Step: 3
Training loss: 2.600437879562378
Validation loss: 2.1004891267386814

Epoch: 6| Step: 4
Training loss: 3.3823394775390625
Validation loss: 2.0761433955161803

Epoch: 6| Step: 5
Training loss: 1.8631713390350342
Validation loss: 2.071168209916802

Epoch: 6| Step: 6
Training loss: 1.904029369354248
Validation loss: 2.0532406876164098

Epoch: 6| Step: 7
Training loss: 2.121347188949585
Validation loss: 2.0742195216558312

Epoch: 6| Step: 8
Training loss: 2.627619743347168
Validation loss: 2.090181718590439

Epoch: 6| Step: 9
Training loss: 2.213501453399658
Validation loss: 2.0993182607876357

Epoch: 6| Step: 10
Training loss: 2.4337949752807617
Validation loss: 2.0755610004548104

Epoch: 6| Step: 11
Training loss: 2.4043655395507812
Validation loss: 2.056637610158613

Epoch: 6| Step: 12
Training loss: 1.9442782402038574
Validation loss: 2.0415710582528064

Epoch: 6| Step: 13
Training loss: 1.3546710014343262
Validation loss: 2.0409535707965976

Epoch: 133| Step: 0
Training loss: 2.2670931816101074
Validation loss: 2.056818680096698

Epoch: 6| Step: 1
Training loss: 2.5036845207214355
Validation loss: 2.080035983875234

Epoch: 6| Step: 2
Training loss: 2.367494583129883
Validation loss: 2.0863590650661017

Epoch: 6| Step: 3
Training loss: 2.322514533996582
Validation loss: 2.090789971813079

Epoch: 6| Step: 4
Training loss: 3.036808490753174
Validation loss: 2.08403108965966

Epoch: 6| Step: 5
Training loss: 2.4868667125701904
Validation loss: 2.0638162474478445

Epoch: 6| Step: 6
Training loss: 1.7023767232894897
Validation loss: 2.0598202443891958

Epoch: 6| Step: 7
Training loss: 2.065976142883301
Validation loss: 2.039635560845816

Epoch: 6| Step: 8
Training loss: 2.5362510681152344
Validation loss: 2.0244322592212307

Epoch: 6| Step: 9
Training loss: 2.512734889984131
Validation loss: 2.026326597377818

Epoch: 6| Step: 10
Training loss: 2.0865285396575928
Validation loss: 2.0310298883786766

Epoch: 6| Step: 11
Training loss: 2.6805460453033447
Validation loss: 2.039856356959189

Epoch: 6| Step: 12
Training loss: 2.0864014625549316
Validation loss: 2.034862963102197

Epoch: 6| Step: 13
Training loss: 1.644120216369629
Validation loss: 2.0413929557287567

Epoch: 134| Step: 0
Training loss: 2.3244104385375977
Validation loss: 2.051339710912397

Epoch: 6| Step: 1
Training loss: 2.3788561820983887
Validation loss: 2.093222697575887

Epoch: 6| Step: 2
Training loss: 2.7420313358306885
Validation loss: 2.178718166966592

Epoch: 6| Step: 3
Training loss: 2.309037208557129
Validation loss: 2.274236666258945

Epoch: 6| Step: 4
Training loss: 2.7719831466674805
Validation loss: 2.3574230030018795

Epoch: 6| Step: 5
Training loss: 2.6057329177856445
Validation loss: 2.3514933419483963

Epoch: 6| Step: 6
Training loss: 2.5532727241516113
Validation loss: 2.2573456610402753

Epoch: 6| Step: 7
Training loss: 3.1126532554626465
Validation loss: 2.1556561416195286

Epoch: 6| Step: 8
Training loss: 2.2284820079803467
Validation loss: 2.071096468997258

Epoch: 6| Step: 9
Training loss: 1.6501777172088623
Validation loss: 2.0449723223204255

Epoch: 6| Step: 10
Training loss: 2.4718945026397705
Validation loss: 2.058559668961392

Epoch: 6| Step: 11
Training loss: 2.4199390411376953
Validation loss: 2.0925568675482147

Epoch: 6| Step: 12
Training loss: 1.804176688194275
Validation loss: 2.090697216731246

Epoch: 6| Step: 13
Training loss: 1.8681896924972534
Validation loss: 2.0899344823693715

Epoch: 135| Step: 0
Training loss: 2.3965964317321777
Validation loss: 2.080541326153663

Epoch: 6| Step: 1
Training loss: 2.454411029815674
Validation loss: 2.0759027286242415

Epoch: 6| Step: 2
Training loss: 2.3251538276672363
Validation loss: 2.055812802366031

Epoch: 6| Step: 3
Training loss: 2.284291982650757
Validation loss: 2.056168240885581

Epoch: 6| Step: 4
Training loss: 3.0312018394470215
Validation loss: 2.0346385945555983

Epoch: 6| Step: 5
Training loss: 2.379516363143921
Validation loss: 2.036154471417909

Epoch: 6| Step: 6
Training loss: 2.498692512512207
Validation loss: 2.036616515087825

Epoch: 6| Step: 7
Training loss: 2.2858986854553223
Validation loss: 2.0367071654206965

Epoch: 6| Step: 8
Training loss: 2.625678062438965
Validation loss: 2.0450900164983605

Epoch: 6| Step: 9
Training loss: 1.8084580898284912
Validation loss: 2.0546380037902505

Epoch: 6| Step: 10
Training loss: 2.978182315826416
Validation loss: 2.057770365027971

Epoch: 6| Step: 11
Training loss: 1.723809003829956
Validation loss: 2.0598495698744252

Epoch: 6| Step: 12
Training loss: 1.436323642730713
Validation loss: 2.0659591459458873

Epoch: 6| Step: 13
Training loss: 1.4803316593170166
Validation loss: 2.086763665240298

Epoch: 136| Step: 0
Training loss: 1.6964442729949951
Validation loss: 2.109498070132348

Epoch: 6| Step: 1
Training loss: 2.90484356880188
Validation loss: 2.1050998318579888

Epoch: 6| Step: 2
Training loss: 2.1613709926605225
Validation loss: 2.1052871186246156

Epoch: 6| Step: 3
Training loss: 2.3814034461975098
Validation loss: 2.1183321629801104

Epoch: 6| Step: 4
Training loss: 2.1362056732177734
Validation loss: 2.107636959322037

Epoch: 6| Step: 5
Training loss: 2.151918888092041
Validation loss: 2.072031658182862

Epoch: 6| Step: 6
Training loss: 2.4794065952301025
Validation loss: 2.0438111007854505

Epoch: 6| Step: 7
Training loss: 2.6933422088623047
Validation loss: 2.0317323502673896

Epoch: 6| Step: 8
Training loss: 1.9757585525512695
Validation loss: 2.034257444002295

Epoch: 6| Step: 9
Training loss: 2.3044228553771973
Validation loss: 2.0383136374976045

Epoch: 6| Step: 10
Training loss: 2.3351147174835205
Validation loss: 2.0467587491517425

Epoch: 6| Step: 11
Training loss: 2.173262119293213
Validation loss: 2.0488024129662463

Epoch: 6| Step: 12
Training loss: 2.5070009231567383
Validation loss: 2.0512123377092424

Epoch: 6| Step: 13
Training loss: 2.4926578998565674
Validation loss: 2.0272554312982867

Epoch: 137| Step: 0
Training loss: 2.3407530784606934
Validation loss: 2.014777227114606

Epoch: 6| Step: 1
Training loss: 2.2008304595947266
Validation loss: 2.003326936434674

Epoch: 6| Step: 2
Training loss: 1.8526461124420166
Validation loss: 2.012490052048878

Epoch: 6| Step: 3
Training loss: 2.2295167446136475
Validation loss: 2.0060351369201497

Epoch: 6| Step: 4
Training loss: 1.9521843194961548
Validation loss: 2.0055249301336144

Epoch: 6| Step: 5
Training loss: 2.806023120880127
Validation loss: 2.0087509001455

Epoch: 6| Step: 6
Training loss: 2.874572277069092
Validation loss: 2.001586829462359

Epoch: 6| Step: 7
Training loss: 2.989370584487915
Validation loss: 1.992569738818753

Epoch: 6| Step: 8
Training loss: 2.2858784198760986
Validation loss: 2.002132531135313

Epoch: 6| Step: 9
Training loss: 2.253335952758789
Validation loss: 2.006674443521807

Epoch: 6| Step: 10
Training loss: 1.9607105255126953
Validation loss: 2.0054542659431376

Epoch: 6| Step: 11
Training loss: 2.143951177597046
Validation loss: 2.0099156569409113

Epoch: 6| Step: 12
Training loss: 2.2117528915405273
Validation loss: 1.9989805016466367

Epoch: 6| Step: 13
Training loss: 1.81285560131073
Validation loss: 1.9956220324321459

Epoch: 138| Step: 0
Training loss: 2.1420562267303467
Validation loss: 1.9930990114006946

Epoch: 6| Step: 1
Training loss: 1.6233007907867432
Validation loss: 1.9934592746919202

Epoch: 6| Step: 2
Training loss: 2.0804784297943115
Validation loss: 2.0223472079923077

Epoch: 6| Step: 3
Training loss: 1.6804757118225098
Validation loss: 2.061979974469831

Epoch: 6| Step: 4
Training loss: 2.3174033164978027
Validation loss: 2.093032257531279

Epoch: 6| Step: 5
Training loss: 2.6055188179016113
Validation loss: 2.109416837333351

Epoch: 6| Step: 6
Training loss: 2.103285789489746
Validation loss: 2.0989686724960164

Epoch: 6| Step: 7
Training loss: 2.402740955352783
Validation loss: 2.070516883686025

Epoch: 6| Step: 8
Training loss: 2.005484104156494
Validation loss: 2.0349686696965206

Epoch: 6| Step: 9
Training loss: 2.259063482284546
Validation loss: 2.0171203126189527

Epoch: 6| Step: 10
Training loss: 2.539475440979004
Validation loss: 2.002714726232713

Epoch: 6| Step: 11
Training loss: 2.541431427001953
Validation loss: 2.0221365933777182

Epoch: 6| Step: 12
Training loss: 3.0589287281036377
Validation loss: 2.016879707254389

Epoch: 6| Step: 13
Training loss: 3.2799386978149414
Validation loss: 2.0233338635454894

Epoch: 139| Step: 0
Training loss: 1.8889364004135132
Validation loss: 2.0241744672098467

Epoch: 6| Step: 1
Training loss: 2.75101900100708
Validation loss: 2.0212980803622993

Epoch: 6| Step: 2
Training loss: 2.5141892433166504
Validation loss: 2.0298231404314757

Epoch: 6| Step: 3
Training loss: 1.957392692565918
Validation loss: 2.059574604034424

Epoch: 6| Step: 4
Training loss: 2.271763324737549
Validation loss: 2.0735956045889083

Epoch: 6| Step: 5
Training loss: 2.34240460395813
Validation loss: 2.0851840831900157

Epoch: 6| Step: 6
Training loss: 2.8498921394348145
Validation loss: 2.063772883466495

Epoch: 6| Step: 7
Training loss: 1.9441533088684082
Validation loss: 2.0470841443666847

Epoch: 6| Step: 8
Training loss: 1.8654975891113281
Validation loss: 2.0402684852641118

Epoch: 6| Step: 9
Training loss: 2.6006827354431152
Validation loss: 2.039390571655766

Epoch: 6| Step: 10
Training loss: 2.074090003967285
Validation loss: 2.0368972337374123

Epoch: 6| Step: 11
Training loss: 1.8803225755691528
Validation loss: 2.0408209523847027

Epoch: 6| Step: 12
Training loss: 2.4026360511779785
Validation loss: 2.032586469445177

Epoch: 6| Step: 13
Training loss: 2.137612819671631
Validation loss: 2.025965134302775

Epoch: 140| Step: 0
Training loss: 1.6987733840942383
Validation loss: 2.0325592538361907

Epoch: 6| Step: 1
Training loss: 2.6367359161376953
Validation loss: 2.034791338828302

Epoch: 6| Step: 2
Training loss: 2.5572478771209717
Validation loss: 2.057633158981159

Epoch: 6| Step: 3
Training loss: 2.1955690383911133
Validation loss: 2.05965167989013

Epoch: 6| Step: 4
Training loss: 2.298654079437256
Validation loss: 2.071225230411817

Epoch: 6| Step: 5
Training loss: 2.019435167312622
Validation loss: 2.0679984707986154

Epoch: 6| Step: 6
Training loss: 2.1798558235168457
Validation loss: 2.077150490976149

Epoch: 6| Step: 7
Training loss: 2.1164979934692383
Validation loss: 2.08585338951439

Epoch: 6| Step: 8
Training loss: 2.0053186416625977
Validation loss: 2.0916198786868843

Epoch: 6| Step: 9
Training loss: 2.252669334411621
Validation loss: 2.091093022336242

Epoch: 6| Step: 10
Training loss: 2.7772836685180664
Validation loss: 2.0870521812028784

Epoch: 6| Step: 11
Training loss: 2.0325660705566406
Validation loss: 2.092482069487213

Epoch: 6| Step: 12
Training loss: 2.3863935470581055
Validation loss: 2.0904108016721663

Epoch: 6| Step: 13
Training loss: 2.2909903526306152
Validation loss: 2.0829210589008946

Epoch: 141| Step: 0
Training loss: 2.465160369873047
Validation loss: 2.0768223795839535

Epoch: 6| Step: 1
Training loss: 2.46053409576416
Validation loss: 2.074374694978037

Epoch: 6| Step: 2
Training loss: 2.3318400382995605
Validation loss: 2.0750617109319216

Epoch: 6| Step: 3
Training loss: 1.7290563583374023
Validation loss: 2.0845372753758586

Epoch: 6| Step: 4
Training loss: 1.5476022958755493
Validation loss: 2.067243488886023

Epoch: 6| Step: 5
Training loss: 2.7778432369232178
Validation loss: 2.0950921350909817

Epoch: 6| Step: 6
Training loss: 2.51632022857666
Validation loss: 2.1136436718766407

Epoch: 6| Step: 7
Training loss: 2.098684549331665
Validation loss: 2.080796444287864

Epoch: 6| Step: 8
Training loss: 2.7962098121643066
Validation loss: 2.031882505263052

Epoch: 6| Step: 9
Training loss: 2.2223610877990723
Validation loss: 2.035346874626734

Epoch: 6| Step: 10
Training loss: 2.1955206394195557
Validation loss: 2.011522840428096

Epoch: 6| Step: 11
Training loss: 2.1997475624084473
Validation loss: 2.025858506079643

Epoch: 6| Step: 12
Training loss: 2.074312210083008
Validation loss: 2.058144138705346

Epoch: 6| Step: 13
Training loss: 1.5991815328598022
Validation loss: 2.0990679725523917

Epoch: 142| Step: 0
Training loss: 2.7418036460876465
Validation loss: 2.102934096449165

Epoch: 6| Step: 1
Training loss: 2.788907527923584
Validation loss: 2.0900193414380475

Epoch: 6| Step: 2
Training loss: 2.337702512741089
Validation loss: 2.060441219678489

Epoch: 6| Step: 3
Training loss: 1.4366137981414795
Validation loss: 2.018029110406035

Epoch: 6| Step: 4
Training loss: 2.5017805099487305
Validation loss: 2.0047520745185112

Epoch: 6| Step: 5
Training loss: 2.727403163909912
Validation loss: 2.008197517805202

Epoch: 6| Step: 6
Training loss: 1.9071738719940186
Validation loss: 2.0152167722743046

Epoch: 6| Step: 7
Training loss: 1.6093566417694092
Validation loss: 2.0152519326056204

Epoch: 6| Step: 8
Training loss: 1.8534640073776245
Validation loss: 2.018736511148432

Epoch: 6| Step: 9
Training loss: 2.6006569862365723
Validation loss: 2.0271163114937405

Epoch: 6| Step: 10
Training loss: 1.5829856395721436
Validation loss: 2.0165423885468514

Epoch: 6| Step: 11
Training loss: 2.101316452026367
Validation loss: 2.0286704955562467

Epoch: 6| Step: 12
Training loss: 2.428931713104248
Validation loss: 2.026530670863326

Epoch: 6| Step: 13
Training loss: 2.689763069152832
Validation loss: 2.02498629272625

Epoch: 143| Step: 0
Training loss: 2.0548908710479736
Validation loss: 2.053970420232383

Epoch: 6| Step: 1
Training loss: 1.800549030303955
Validation loss: 2.0643452803293862

Epoch: 6| Step: 2
Training loss: 1.784803867340088
Validation loss: 2.076037169784628

Epoch: 6| Step: 3
Training loss: 2.004094123840332
Validation loss: 2.074170930411226

Epoch: 6| Step: 4
Training loss: 2.2897565364837646
Validation loss: 2.0700610030081963

Epoch: 6| Step: 5
Training loss: 2.428861141204834
Validation loss: 2.0474636580354426

Epoch: 6| Step: 6
Training loss: 2.0309178829193115
Validation loss: 2.0157530025769304

Epoch: 6| Step: 7
Training loss: 2.1486310958862305
Validation loss: 2.0153802197466613

Epoch: 6| Step: 8
Training loss: 1.424293041229248
Validation loss: 2.0059690219099804

Epoch: 6| Step: 9
Training loss: 3.145932912826538
Validation loss: 2.0099287968809887

Epoch: 6| Step: 10
Training loss: 1.996990442276001
Validation loss: 2.0085103563083115

Epoch: 6| Step: 11
Training loss: 2.4581127166748047
Validation loss: 2.046932957505667

Epoch: 6| Step: 12
Training loss: 2.5928707122802734
Validation loss: 2.051684157822722

Epoch: 6| Step: 13
Training loss: 3.08133602142334
Validation loss: 2.0802428709563388

Epoch: 144| Step: 0
Training loss: 1.8991031646728516
Validation loss: 2.057589405326433

Epoch: 6| Step: 1
Training loss: 2.301661968231201
Validation loss: 2.064484844925583

Epoch: 6| Step: 2
Training loss: 1.5226120948791504
Validation loss: 2.072429008381341

Epoch: 6| Step: 3
Training loss: 1.8515965938568115
Validation loss: 2.0675427195846394

Epoch: 6| Step: 4
Training loss: 2.68563175201416
Validation loss: 2.0726146928725706

Epoch: 6| Step: 5
Training loss: 2.5662691593170166
Validation loss: 2.052676946886124

Epoch: 6| Step: 6
Training loss: 2.047314167022705
Validation loss: 2.0589350884960544

Epoch: 6| Step: 7
Training loss: 2.341928720474243
Validation loss: 2.0719524955236786

Epoch: 6| Step: 8
Training loss: 2.6465446949005127
Validation loss: 2.087092322687949

Epoch: 6| Step: 9
Training loss: 2.5216987133026123
Validation loss: 2.0818602038967993

Epoch: 6| Step: 10
Training loss: 2.582160472869873
Validation loss: 2.100106931501819

Epoch: 6| Step: 11
Training loss: 1.6127365827560425
Validation loss: 2.07849702014718

Epoch: 6| Step: 12
Training loss: 1.8926916122436523
Validation loss: 2.067778197667932

Epoch: 6| Step: 13
Training loss: 2.0605435371398926
Validation loss: 2.02874489753477

Epoch: 145| Step: 0
Training loss: 2.4445013999938965
Validation loss: 2.002467711766561

Epoch: 6| Step: 1
Training loss: 1.7546160221099854
Validation loss: 1.9887234344277331

Epoch: 6| Step: 2
Training loss: 1.641884207725525
Validation loss: 1.9884058801076745

Epoch: 6| Step: 3
Training loss: 2.377607583999634
Validation loss: 1.981251666622777

Epoch: 6| Step: 4
Training loss: 2.643308162689209
Validation loss: 1.9677585017296575

Epoch: 6| Step: 5
Training loss: 2.5583581924438477
Validation loss: 1.9640875401035431

Epoch: 6| Step: 6
Training loss: 2.4499287605285645
Validation loss: 1.971727837798416

Epoch: 6| Step: 7
Training loss: 2.017047882080078
Validation loss: 1.9663882511918263

Epoch: 6| Step: 8
Training loss: 1.859306812286377
Validation loss: 1.9798300420084307

Epoch: 6| Step: 9
Training loss: 2.213587999343872
Validation loss: 1.969868726627801

Epoch: 6| Step: 10
Training loss: 2.0409929752349854
Validation loss: 1.9625225836230862

Epoch: 6| Step: 11
Training loss: 1.8801945447921753
Validation loss: 1.9717072902187225

Epoch: 6| Step: 12
Training loss: 2.495708465576172
Validation loss: 2.000704801210793

Epoch: 6| Step: 13
Training loss: 2.6717169284820557
Validation loss: 2.0444657597490536

Epoch: 146| Step: 0
Training loss: 2.7608187198638916
Validation loss: 2.0901217511905137

Epoch: 6| Step: 1
Training loss: 2.506925582885742
Validation loss: 2.142558729776772

Epoch: 6| Step: 2
Training loss: 2.285524606704712
Validation loss: 2.177699581269295

Epoch: 6| Step: 3
Training loss: 2.2391510009765625
Validation loss: 2.150820826971403

Epoch: 6| Step: 4
Training loss: 1.9696974754333496
Validation loss: 2.1359502269375708

Epoch: 6| Step: 5
Training loss: 2.2726328372955322
Validation loss: 2.1084434191385903

Epoch: 6| Step: 6
Training loss: 2.148407459259033
Validation loss: 2.0959078342683855

Epoch: 6| Step: 7
Training loss: 2.7972781658172607
Validation loss: 2.0911015092685656

Epoch: 6| Step: 8
Training loss: 1.951258897781372
Validation loss: 2.099365431775329

Epoch: 6| Step: 9
Training loss: 2.6241438388824463
Validation loss: 2.113298785301947

Epoch: 6| Step: 10
Training loss: 2.2230019569396973
Validation loss: 2.1407828305357244

Epoch: 6| Step: 11
Training loss: 1.3153328895568848
Validation loss: 2.099132409659765

Epoch: 6| Step: 12
Training loss: 1.8049668073654175
Validation loss: 2.0918387469424995

Epoch: 6| Step: 13
Training loss: 2.101512908935547
Validation loss: 2.07843001427189

Epoch: 147| Step: 0
Training loss: 2.84625244140625
Validation loss: 2.0546336840557795

Epoch: 6| Step: 1
Training loss: 2.110358476638794
Validation loss: 2.0508687957640617

Epoch: 6| Step: 2
Training loss: 2.262267589569092
Validation loss: 2.059638338704263

Epoch: 6| Step: 3
Training loss: 2.5761845111846924
Validation loss: 2.042686039401639

Epoch: 6| Step: 4
Training loss: 1.8954582214355469
Validation loss: 2.0352729853763374

Epoch: 6| Step: 5
Training loss: 1.790988802909851
Validation loss: 2.030094469747236

Epoch: 6| Step: 6
Training loss: 2.8348402976989746
Validation loss: 2.0177474214184667

Epoch: 6| Step: 7
Training loss: 1.78006911277771
Validation loss: 2.0139283749365036

Epoch: 6| Step: 8
Training loss: 1.8799657821655273
Validation loss: 2.0215587051965858

Epoch: 6| Step: 9
Training loss: 1.91263747215271
Validation loss: 2.0224467669763873

Epoch: 6| Step: 10
Training loss: 2.447941780090332
Validation loss: 2.0154261230140604

Epoch: 6| Step: 11
Training loss: 2.027270793914795
Validation loss: 2.04447647448509

Epoch: 6| Step: 12
Training loss: 2.030583381652832
Validation loss: 2.0616143185605287

Epoch: 6| Step: 13
Training loss: 1.2532073259353638
Validation loss: 2.0504625343507334

Epoch: 148| Step: 0
Training loss: 1.4335728883743286
Validation loss: 2.07435635212929

Epoch: 6| Step: 1
Training loss: 1.8506945371627808
Validation loss: 2.0652489034078454

Epoch: 6| Step: 2
Training loss: 1.6951302289962769
Validation loss: 2.05706496648891

Epoch: 6| Step: 3
Training loss: 2.0222201347351074
Validation loss: 2.0607392159841393

Epoch: 6| Step: 4
Training loss: 3.278733253479004
Validation loss: 2.0826691171174407

Epoch: 6| Step: 5
Training loss: 2.885286808013916
Validation loss: 2.0826791486432477

Epoch: 6| Step: 6
Training loss: 2.2057242393493652
Validation loss: 2.1090722545500724

Epoch: 6| Step: 7
Training loss: 2.1643552780151367
Validation loss: 2.0988094575943483

Epoch: 6| Step: 8
Training loss: 1.4998960494995117
Validation loss: 2.1132996184851534

Epoch: 6| Step: 9
Training loss: 2.5108871459960938
Validation loss: 2.0924746785112607

Epoch: 6| Step: 10
Training loss: 1.8856611251831055
Validation loss: 2.0689752383898665

Epoch: 6| Step: 11
Training loss: 1.885237216949463
Validation loss: 2.044417223622722

Epoch: 6| Step: 12
Training loss: 2.517608404159546
Validation loss: 2.030645283319617

Epoch: 6| Step: 13
Training loss: 2.2875618934631348
Validation loss: 2.0282229121013353

Epoch: 149| Step: 0
Training loss: 2.2164523601531982
Validation loss: 2.029717106972971

Epoch: 6| Step: 1
Training loss: 2.5671305656433105
Validation loss: 2.02479108431006

Epoch: 6| Step: 2
Training loss: 1.5566892623901367
Validation loss: 2.026166787711523

Epoch: 6| Step: 3
Training loss: 2.1343376636505127
Validation loss: 2.0133741389038744

Epoch: 6| Step: 4
Training loss: 2.7692158222198486
Validation loss: 1.9903684495597758

Epoch: 6| Step: 5
Training loss: 2.5519540309906006
Validation loss: 1.994934594759377

Epoch: 6| Step: 6
Training loss: 1.7887513637542725
Validation loss: 1.9919458025245256

Epoch: 6| Step: 7
Training loss: 1.954214334487915
Validation loss: 2.0147169854051326

Epoch: 6| Step: 8
Training loss: 1.581140160560608
Validation loss: 1.9996982761608657

Epoch: 6| Step: 9
Training loss: 2.442267417907715
Validation loss: 2.024754032011955

Epoch: 6| Step: 10
Training loss: 2.194162607192993
Validation loss: 2.0405278052053144

Epoch: 6| Step: 11
Training loss: 2.3382771015167236
Validation loss: 2.0749132351208757

Epoch: 6| Step: 12
Training loss: 1.5336867570877075
Validation loss: 2.1133185176439184

Epoch: 6| Step: 13
Training loss: 1.9450149536132812
Validation loss: 2.1415105558210805

Epoch: 150| Step: 0
Training loss: 2.950061082839966
Validation loss: 2.1494316606111425

Epoch: 6| Step: 1
Training loss: 2.1279044151306152
Validation loss: 2.158704896127024

Epoch: 6| Step: 2
Training loss: 2.022517204284668
Validation loss: 2.1463495839026665

Epoch: 6| Step: 3
Training loss: 2.1152496337890625
Validation loss: 2.1132927274191253

Epoch: 6| Step: 4
Training loss: 1.761741042137146
Validation loss: 2.097977599790019

Epoch: 6| Step: 5
Training loss: 1.2434661388397217
Validation loss: 2.077591229510564

Epoch: 6| Step: 6
Training loss: 2.1106088161468506
Validation loss: 2.0778411383269937

Epoch: 6| Step: 7
Training loss: 1.7656002044677734
Validation loss: 2.0579061046723397

Epoch: 6| Step: 8
Training loss: 1.8726189136505127
Validation loss: 2.028919909590034

Epoch: 6| Step: 9
Training loss: 2.26284122467041
Validation loss: 2.0206732749938965

Epoch: 6| Step: 10
Training loss: 1.8740108013153076
Validation loss: 2.010913807858703

Epoch: 6| Step: 11
Training loss: 3.054002285003662
Validation loss: 1.9867161653375114

Epoch: 6| Step: 12
Training loss: 2.016925811767578
Validation loss: 1.9677731772904754

Epoch: 6| Step: 13
Training loss: 2.1832170486450195
Validation loss: 1.9678479086968206

Epoch: 151| Step: 0
Training loss: 2.1911678314208984
Validation loss: 1.9665804498939103

Epoch: 6| Step: 1
Training loss: 2.435521125793457
Validation loss: 1.9691164801197667

Epoch: 6| Step: 2
Training loss: 2.7797412872314453
Validation loss: 1.9820100492046726

Epoch: 6| Step: 3
Training loss: 1.881711483001709
Validation loss: 1.9910541888206237

Epoch: 6| Step: 4
Training loss: 2.0896646976470947
Validation loss: 2.0136177386007

Epoch: 6| Step: 5
Training loss: 2.306959629058838
Validation loss: 2.0447761269025904

Epoch: 6| Step: 6
Training loss: 1.9906376600265503
Validation loss: 2.0773557847545994

Epoch: 6| Step: 7
Training loss: 2.150205135345459
Validation loss: 2.1426545073909145

Epoch: 6| Step: 8
Training loss: 1.2834296226501465
Validation loss: 2.1609813526112545

Epoch: 6| Step: 9
Training loss: 2.3461923599243164
Validation loss: 2.1819576473646265

Epoch: 6| Step: 10
Training loss: 2.364896059036255
Validation loss: 2.1706454318056823

Epoch: 6| Step: 11
Training loss: 1.7471891641616821
Validation loss: 2.1370586092754076

Epoch: 6| Step: 12
Training loss: 1.691158413887024
Validation loss: 2.108012851848397

Epoch: 6| Step: 13
Training loss: 2.316279411315918
Validation loss: 2.0717545427301878

Epoch: 152| Step: 0
Training loss: 1.4339478015899658
Validation loss: 2.074921225988737

Epoch: 6| Step: 1
Training loss: 2.0382328033447266
Validation loss: 2.0879922233602053

Epoch: 6| Step: 2
Training loss: 2.183237075805664
Validation loss: 2.0504081069782214

Epoch: 6| Step: 3
Training loss: 2.5474512577056885
Validation loss: 2.0389877070662794

Epoch: 6| Step: 4
Training loss: 1.8629698753356934
Validation loss: 2.018807598339614

Epoch: 6| Step: 5
Training loss: 1.7055892944335938
Validation loss: 2.0235747445014214

Epoch: 6| Step: 6
Training loss: 2.5034916400909424
Validation loss: 2.0135883156971266

Epoch: 6| Step: 7
Training loss: 2.0143675804138184
Validation loss: 2.019373878355949

Epoch: 6| Step: 8
Training loss: 1.9442050457000732
Validation loss: 2.027156194051107

Epoch: 6| Step: 9
Training loss: 2.904649257659912
Validation loss: 2.049837266245196

Epoch: 6| Step: 10
Training loss: 2.403761863708496
Validation loss: 2.0855552073447936

Epoch: 6| Step: 11
Training loss: 1.5809659957885742
Validation loss: 2.0879159922240884

Epoch: 6| Step: 12
Training loss: 1.9807069301605225
Validation loss: 2.07501148152095

Epoch: 6| Step: 13
Training loss: 2.177213668823242
Validation loss: 2.0848029223821496

Epoch: 153| Step: 0
Training loss: 1.5752453804016113
Validation loss: 2.048384310096823

Epoch: 6| Step: 1
Training loss: 1.4235997200012207
Validation loss: 2.031501462382655

Epoch: 6| Step: 2
Training loss: 1.912252426147461
Validation loss: 2.0335953568899505

Epoch: 6| Step: 3
Training loss: 2.67289400100708
Validation loss: 2.0334174274116434

Epoch: 6| Step: 4
Training loss: 2.5389363765716553
Validation loss: 2.045170109759095

Epoch: 6| Step: 5
Training loss: 1.217421293258667
Validation loss: 2.032741718394782

Epoch: 6| Step: 6
Training loss: 1.761110544204712
Validation loss: 2.0063336023720364

Epoch: 6| Step: 7
Training loss: 3.109872817993164
Validation loss: 1.9745480809160458

Epoch: 6| Step: 8
Training loss: 2.0427026748657227
Validation loss: 1.981931350564444

Epoch: 6| Step: 9
Training loss: 1.9834908246994019
Validation loss: 1.9708634281671176

Epoch: 6| Step: 10
Training loss: 1.561631441116333
Validation loss: 1.9686737470729376

Epoch: 6| Step: 11
Training loss: 2.3324248790740967
Validation loss: 1.9636103773629794

Epoch: 6| Step: 12
Training loss: 2.9994850158691406
Validation loss: 1.9706360883610223

Epoch: 6| Step: 13
Training loss: 1.813698410987854
Validation loss: 2.0003476194156113

Epoch: 154| Step: 0
Training loss: 1.8026533126831055
Validation loss: 2.011211459354688

Epoch: 6| Step: 1
Training loss: 2.602670907974243
Validation loss: 2.0431439748374363

Epoch: 6| Step: 2
Training loss: 1.568908452987671
Validation loss: 2.0426153752111618

Epoch: 6| Step: 3
Training loss: 3.1372432708740234
Validation loss: 2.06916110233594

Epoch: 6| Step: 4
Training loss: 1.5860514640808105
Validation loss: 2.0611137343991186

Epoch: 6| Step: 5
Training loss: 2.149238109588623
Validation loss: 2.051293974281639

Epoch: 6| Step: 6
Training loss: 1.9800359010696411
Validation loss: 2.0481386825602543

Epoch: 6| Step: 7
Training loss: 1.4808404445648193
Validation loss: 2.0529974942566245

Epoch: 6| Step: 8
Training loss: 2.4332423210144043
Validation loss: 2.1378310059988372

Epoch: 6| Step: 9
Training loss: 1.401674747467041
Validation loss: 2.152704777256135

Epoch: 6| Step: 10
Training loss: 2.1137475967407227
Validation loss: 2.149848048404981

Epoch: 6| Step: 11
Training loss: 1.6871130466461182
Validation loss: 2.0728824574460267

Epoch: 6| Step: 12
Training loss: 2.2158050537109375
Validation loss: 2.0248569878198768

Epoch: 6| Step: 13
Training loss: 3.404611587524414
Validation loss: 1.987620747217568

Epoch: 155| Step: 0
Training loss: 2.128730058670044
Validation loss: 1.975972052543394

Epoch: 6| Step: 1
Training loss: 1.9148540496826172
Validation loss: 1.9741433922962477

Epoch: 6| Step: 2
Training loss: 2.364868402481079
Validation loss: 1.991650782605653

Epoch: 6| Step: 3
Training loss: 2.268353223800659
Validation loss: 2.0162937154052076

Epoch: 6| Step: 4
Training loss: 0.8095093965530396
Validation loss: 2.015053913157473

Epoch: 6| Step: 5
Training loss: 1.7665424346923828
Validation loss: 2.022595126141784

Epoch: 6| Step: 6
Training loss: 2.2419559955596924
Validation loss: 2.0101395832595004

Epoch: 6| Step: 7
Training loss: 1.6014399528503418
Validation loss: 1.9697532589717577

Epoch: 6| Step: 8
Training loss: 2.3894152641296387
Validation loss: 1.9866984121261104

Epoch: 6| Step: 9
Training loss: 1.733762264251709
Validation loss: 2.0003270833723006

Epoch: 6| Step: 10
Training loss: 2.5884718894958496
Validation loss: 2.023778771841398

Epoch: 6| Step: 11
Training loss: 2.777865171432495
Validation loss: 2.0808377829931115

Epoch: 6| Step: 12
Training loss: 2.0290584564208984
Validation loss: 2.1413488529061757

Epoch: 6| Step: 13
Training loss: 2.4017515182495117
Validation loss: 2.220263642649497

Epoch: 156| Step: 0
Training loss: 1.4748492240905762
Validation loss: 2.266053061331472

Epoch: 6| Step: 1
Training loss: 2.3194150924682617
Validation loss: 2.280712273813063

Epoch: 6| Step: 2
Training loss: 2.5579802989959717
Validation loss: 2.2325114652674687

Epoch: 6| Step: 3
Training loss: 2.23307466506958
Validation loss: 2.1862247297840733

Epoch: 6| Step: 4
Training loss: 2.2353200912475586
Validation loss: 2.1465580591591458

Epoch: 6| Step: 5
Training loss: 2.2426633834838867
Validation loss: 2.1166947349425285

Epoch: 6| Step: 6
Training loss: 1.36397123336792
Validation loss: 2.076216556692636

Epoch: 6| Step: 7
Training loss: 2.3011574745178223
Validation loss: 2.063382743507303

Epoch: 6| Step: 8
Training loss: 2.5659167766571045
Validation loss: 2.072075941229379

Epoch: 6| Step: 9
Training loss: 1.7597764730453491
Validation loss: 2.0553080920250184

Epoch: 6| Step: 10
Training loss: 2.08528995513916
Validation loss: 2.0330421001680437

Epoch: 6| Step: 11
Training loss: 1.915229320526123
Validation loss: 2.0245394937453733

Epoch: 6| Step: 12
Training loss: 1.5099058151245117
Validation loss: 2.0259955442079933

Epoch: 6| Step: 13
Training loss: 1.4792910814285278
Validation loss: 2.0135670361980313

Epoch: 157| Step: 0
Training loss: 1.7480261325836182
Validation loss: 2.0311822378507225

Epoch: 6| Step: 1
Training loss: 2.936115264892578
Validation loss: 2.0322493007106166

Epoch: 6| Step: 2
Training loss: 1.7390769720077515
Validation loss: 2.035428011289207

Epoch: 6| Step: 3
Training loss: 1.3839867115020752
Validation loss: 2.033280252128519

Epoch: 6| Step: 4
Training loss: 2.7216758728027344
Validation loss: 2.0427610592175554

Epoch: 6| Step: 5
Training loss: 2.2112197875976562
Validation loss: 2.048247064313581

Epoch: 6| Step: 6
Training loss: 1.248464584350586
Validation loss: 2.05807557670019

Epoch: 6| Step: 7
Training loss: 1.7381699085235596
Validation loss: 2.029410481452942

Epoch: 6| Step: 8
Training loss: 1.8279980421066284
Validation loss: 2.0240776564485286

Epoch: 6| Step: 9
Training loss: 1.3704696893692017
Validation loss: 2.0279311454424294

Epoch: 6| Step: 10
Training loss: 2.9485321044921875
Validation loss: 2.0291460329486477

Epoch: 6| Step: 11
Training loss: 1.9366750717163086
Validation loss: 2.0616753703804425

Epoch: 6| Step: 12
Training loss: 2.0581281185150146
Validation loss: 2.0574343665953605

Epoch: 6| Step: 13
Training loss: 2.118574380874634
Validation loss: 2.0894587629584858

Epoch: 158| Step: 0
Training loss: 1.890512466430664
Validation loss: 2.0835172219942977

Epoch: 6| Step: 1
Training loss: 2.2109107971191406
Validation loss: 2.0571816403378724

Epoch: 6| Step: 2
Training loss: 2.270265579223633
Validation loss: 2.018218622412733

Epoch: 6| Step: 3
Training loss: 3.003161907196045
Validation loss: 2.0060156955513904

Epoch: 6| Step: 4
Training loss: 1.597440481185913
Validation loss: 2.007568129929163

Epoch: 6| Step: 5
Training loss: 1.7021150588989258
Validation loss: 2.0125075130052466

Epoch: 6| Step: 6
Training loss: 1.4466787576675415
Validation loss: 2.022528406112425

Epoch: 6| Step: 7
Training loss: 2.0923643112182617
Validation loss: 2.023985828122785

Epoch: 6| Step: 8
Training loss: 1.7567161321640015
Validation loss: 2.0355700164712887

Epoch: 6| Step: 9
Training loss: 1.7394368648529053
Validation loss: 2.0306383640535417

Epoch: 6| Step: 10
Training loss: 1.7781480550765991
Validation loss: 2.0403841080204135

Epoch: 6| Step: 11
Training loss: 2.52144718170166
Validation loss: 2.0607032814333515

Epoch: 6| Step: 12
Training loss: 1.9199366569519043
Validation loss: 2.074696374195878

Epoch: 6| Step: 13
Training loss: 2.15228009223938
Validation loss: 2.0855686177489576

Epoch: 159| Step: 0
Training loss: 1.8333077430725098
Validation loss: 2.0953810253450946

Epoch: 6| Step: 1
Training loss: 2.132317066192627
Validation loss: 2.083930120673231

Epoch: 6| Step: 2
Training loss: 1.1569911241531372
Validation loss: 2.107725046014273

Epoch: 6| Step: 3
Training loss: 2.977748155593872
Validation loss: 2.118757899089526

Epoch: 6| Step: 4
Training loss: 2.29313063621521
Validation loss: 2.101116144528953

Epoch: 6| Step: 5
Training loss: 2.233430862426758
Validation loss: 2.0818106435960337

Epoch: 6| Step: 6
Training loss: 1.3598827123641968
Validation loss: 2.03787673160594

Epoch: 6| Step: 7
Training loss: 2.3255486488342285
Validation loss: 2.065416248895789

Epoch: 6| Step: 8
Training loss: 1.7170681953430176
Validation loss: 2.063710110161894

Epoch: 6| Step: 9
Training loss: 2.084935426712036
Validation loss: 2.095617425057196

Epoch: 6| Step: 10
Training loss: 2.2973456382751465
Validation loss: 2.1033962759920346

Epoch: 6| Step: 11
Training loss: 2.362266778945923
Validation loss: 2.10654144389655

Epoch: 6| Step: 12
Training loss: 2.066558361053467
Validation loss: 2.0875653336125035

Epoch: 6| Step: 13
Training loss: 0.4024082124233246
Validation loss: 2.0519025325775146

Epoch: 160| Step: 0
Training loss: 2.259305477142334
Validation loss: 2.030615391269807

Epoch: 6| Step: 1
Training loss: 2.017850399017334
Validation loss: 2.022122780481974

Epoch: 6| Step: 2
Training loss: 1.5995173454284668
Validation loss: 1.9958923452643937

Epoch: 6| Step: 3
Training loss: 1.8841689825057983
Validation loss: 2.0137359865250124

Epoch: 6| Step: 4
Training loss: 1.840880036354065
Validation loss: 2.02039457649313

Epoch: 6| Step: 5
Training loss: 1.968670129776001
Validation loss: 2.0019154651190645

Epoch: 6| Step: 6
Training loss: 1.7483991384506226
Validation loss: 2.0020068806986653

Epoch: 6| Step: 7
Training loss: 1.616072654724121
Validation loss: 2.011154979787847

Epoch: 6| Step: 8
Training loss: 1.3594990968704224
Validation loss: 2.0364083038863314

Epoch: 6| Step: 9
Training loss: 2.3252506256103516
Validation loss: 2.0674129096410607

Epoch: 6| Step: 10
Training loss: 2.1782703399658203
Validation loss: 2.0820038280179425

Epoch: 6| Step: 11
Training loss: 2.713200092315674
Validation loss: 2.09173910848556

Epoch: 6| Step: 12
Training loss: 1.8642241954803467
Validation loss: 2.111966968864523

Epoch: 6| Step: 13
Training loss: 2.625997304916382
Validation loss: 2.0878908890549854

Epoch: 161| Step: 0
Training loss: 2.15291690826416
Validation loss: 2.0598182319312968

Epoch: 6| Step: 1
Training loss: 1.9156993627548218
Validation loss: 2.0700941624179965

Epoch: 6| Step: 2
Training loss: 2.6724531650543213
Validation loss: 2.04635521417023

Epoch: 6| Step: 3
Training loss: 2.0194873809814453
Validation loss: 2.0336843408564085

Epoch: 6| Step: 4
Training loss: 2.444729804992676
Validation loss: 2.034340106030946

Epoch: 6| Step: 5
Training loss: 1.7690794467926025
Validation loss: 2.02345335355369

Epoch: 6| Step: 6
Training loss: 1.6562650203704834
Validation loss: 2.0141425863389046

Epoch: 6| Step: 7
Training loss: 2.2642266750335693
Validation loss: 2.0396507222165345

Epoch: 6| Step: 8
Training loss: 1.6898291110992432
Validation loss: 2.0417673690344698

Epoch: 6| Step: 9
Training loss: 1.4331369400024414
Validation loss: 2.0333337488994805

Epoch: 6| Step: 10
Training loss: 2.0959224700927734
Validation loss: 2.039471960836841

Epoch: 6| Step: 11
Training loss: 1.6011245250701904
Validation loss: 2.039565877247882

Epoch: 6| Step: 12
Training loss: 1.8928303718566895
Validation loss: 2.023326935306672

Epoch: 6| Step: 13
Training loss: 1.5378695726394653
Validation loss: 2.0122609266670803

Epoch: 162| Step: 0
Training loss: 1.7288357019424438
Validation loss: 2.00034785527055

Epoch: 6| Step: 1
Training loss: 1.8267301321029663
Validation loss: 1.9932087582926596

Epoch: 6| Step: 2
Training loss: 2.2324185371398926
Validation loss: 1.9727003177007039

Epoch: 6| Step: 3
Training loss: 1.9438380002975464
Validation loss: 1.9801689860641316

Epoch: 6| Step: 4
Training loss: 1.8920427560806274
Validation loss: 1.9737800872454079

Epoch: 6| Step: 5
Training loss: 2.0840225219726562
Validation loss: 1.9674963412746307

Epoch: 6| Step: 6
Training loss: 2.139554977416992
Validation loss: 1.9637589070104784

Epoch: 6| Step: 7
Training loss: 1.9450109004974365
Validation loss: 1.970693411365632

Epoch: 6| Step: 8
Training loss: 1.1849063634872437
Validation loss: 1.9911944148361043

Epoch: 6| Step: 9
Training loss: 2.4249391555786133
Validation loss: 2.0117607424336095

Epoch: 6| Step: 10
Training loss: 1.647839903831482
Validation loss: 2.0394203201417

Epoch: 6| Step: 11
Training loss: 2.4214441776275635
Validation loss: 2.0876089090942056

Epoch: 6| Step: 12
Training loss: 1.8782942295074463
Validation loss: 2.113177466136153

Epoch: 6| Step: 13
Training loss: 1.8292694091796875
Validation loss: 2.175744443811396

Epoch: 163| Step: 0
Training loss: 2.189530372619629
Validation loss: 2.154096157320084

Epoch: 6| Step: 1
Training loss: 2.3376145362854004
Validation loss: 2.0756321004641953

Epoch: 6| Step: 2
Training loss: 2.384594440460205
Validation loss: 2.0444673004970757

Epoch: 6| Step: 3
Training loss: 1.6467921733856201
Validation loss: 2.006284656063203

Epoch: 6| Step: 4
Training loss: 2.619034767150879
Validation loss: 1.9823288712450253

Epoch: 6| Step: 5
Training loss: 1.1473171710968018
Validation loss: 1.9797976991181732

Epoch: 6| Step: 6
Training loss: 2.4800899028778076
Validation loss: 1.9455053408940632

Epoch: 6| Step: 7
Training loss: 1.886164903640747
Validation loss: 1.949392085434288

Epoch: 6| Step: 8
Training loss: 1.4969828128814697
Validation loss: 1.945041327066319

Epoch: 6| Step: 9
Training loss: 1.7458655834197998
Validation loss: 1.970365785783337

Epoch: 6| Step: 10
Training loss: 2.2402331829071045
Validation loss: 2.0113051373471498

Epoch: 6| Step: 11
Training loss: 1.8960109949111938
Validation loss: 2.0831542784167874

Epoch: 6| Step: 12
Training loss: 1.6524465084075928
Validation loss: 2.1182958413195867

Epoch: 6| Step: 13
Training loss: 1.5328450202941895
Validation loss: 2.192987670180618

Epoch: 164| Step: 0
Training loss: 1.6846668720245361
Validation loss: 2.1843574098361436

Epoch: 6| Step: 1
Training loss: 2.2448878288269043
Validation loss: 2.1827872824925247

Epoch: 6| Step: 2
Training loss: 2.3629987239837646
Validation loss: 2.110909451720535

Epoch: 6| Step: 3
Training loss: 2.3520989418029785
Validation loss: 2.0742651839410104

Epoch: 6| Step: 4
Training loss: 1.880314588546753
Validation loss: 2.032523706395139

Epoch: 6| Step: 5
Training loss: 1.2090041637420654
Validation loss: 2.022515622518396

Epoch: 6| Step: 6
Training loss: 2.3168959617614746
Validation loss: 1.987850832682784

Epoch: 6| Step: 7
Training loss: 1.964609146118164
Validation loss: 2.0011950910732312

Epoch: 6| Step: 8
Training loss: 2.243330478668213
Validation loss: 1.985319455464681

Epoch: 6| Step: 9
Training loss: 2.0689096450805664
Validation loss: 1.9798038595466203

Epoch: 6| Step: 10
Training loss: 1.0266659259796143
Validation loss: 1.9832107828509422

Epoch: 6| Step: 11
Training loss: 1.963210105895996
Validation loss: 2.0289590179279284

Epoch: 6| Step: 12
Training loss: 1.9798774719238281
Validation loss: 2.063826649419723

Epoch: 6| Step: 13
Training loss: 2.2016327381134033
Validation loss: 2.060689623637866

Epoch: 165| Step: 0
Training loss: 2.0441339015960693
Validation loss: 2.0647388222397014

Epoch: 6| Step: 1
Training loss: 1.7922358512878418
Validation loss: 2.067912901601484

Epoch: 6| Step: 2
Training loss: 2.0262250900268555
Validation loss: 2.0869140022544452

Epoch: 6| Step: 3
Training loss: 2.0866641998291016
Validation loss: 2.0836624612090406

Epoch: 6| Step: 4
Training loss: 1.5201268196105957
Validation loss: 2.0807670700934624

Epoch: 6| Step: 5
Training loss: 2.5003316402435303
Validation loss: 2.0328986606290265

Epoch: 6| Step: 6
Training loss: 1.710555911064148
Validation loss: 2.0056579894916986

Epoch: 6| Step: 7
Training loss: 2.2419724464416504
Validation loss: 1.9869314957690496

Epoch: 6| Step: 8
Training loss: 1.903794288635254
Validation loss: 1.971601660533618

Epoch: 6| Step: 9
Training loss: 2.090329170227051
Validation loss: 1.9685488029192852

Epoch: 6| Step: 10
Training loss: 1.8033478260040283
Validation loss: 2.0028860210090556

Epoch: 6| Step: 11
Training loss: 1.5514017343521118
Validation loss: 2.0245575392118065

Epoch: 6| Step: 12
Training loss: 1.438910722732544
Validation loss: 2.024991017515941

Epoch: 6| Step: 13
Training loss: 2.6100759506225586
Validation loss: 2.0292665650767665

Epoch: 166| Step: 0
Training loss: 1.5356773138046265
Validation loss: 2.0320645186208908

Epoch: 6| Step: 1
Training loss: 1.5929311513900757
Validation loss: 2.030105657474969

Epoch: 6| Step: 2
Training loss: 2.002333641052246
Validation loss: 2.0309789257664836

Epoch: 6| Step: 3
Training loss: 2.0671815872192383
Validation loss: 2.035551858204667

Epoch: 6| Step: 4
Training loss: 1.6718738079071045
Validation loss: 2.0394113268903507

Epoch: 6| Step: 5
Training loss: 1.16552734375
Validation loss: 2.0535494691582135

Epoch: 6| Step: 6
Training loss: 1.6510591506958008
Validation loss: 2.083363245892268

Epoch: 6| Step: 7
Training loss: 2.4978156089782715
Validation loss: 2.05600752112686

Epoch: 6| Step: 8
Training loss: 2.0215115547180176
Validation loss: 2.0352506958028322

Epoch: 6| Step: 9
Training loss: 2.0172297954559326
Validation loss: 2.017388107956097

Epoch: 6| Step: 10
Training loss: 2.159766435623169
Validation loss: 2.0093561949268466

Epoch: 6| Step: 11
Training loss: 2.3592934608459473
Validation loss: 2.0105931989608274

Epoch: 6| Step: 12
Training loss: 2.1738924980163574
Validation loss: 2.0032073246535433

Epoch: 6| Step: 13
Training loss: 1.5619192123413086
Validation loss: 2.015248942118819

Epoch: 167| Step: 0
Training loss: 1.5505695343017578
Validation loss: 2.0024718520461873

Epoch: 6| Step: 1
Training loss: 1.742530107498169
Validation loss: 1.9769965243595902

Epoch: 6| Step: 2
Training loss: 2.4187228679656982
Validation loss: 1.9673501060855003

Epoch: 6| Step: 3
Training loss: 2.2735347747802734
Validation loss: 1.953393131174067

Epoch: 6| Step: 4
Training loss: 1.2244348526000977
Validation loss: 1.9566704803897488

Epoch: 6| Step: 5
Training loss: 2.926816940307617
Validation loss: 1.991931160291036

Epoch: 6| Step: 6
Training loss: 2.008406162261963
Validation loss: 2.0221463967395086

Epoch: 6| Step: 7
Training loss: 1.7265276908874512
Validation loss: 2.059801560576244

Epoch: 6| Step: 8
Training loss: 1.5860610008239746
Validation loss: 2.0828931177816083

Epoch: 6| Step: 9
Training loss: 1.9944686889648438
Validation loss: 2.0874290274035547

Epoch: 6| Step: 10
Training loss: 2.306067943572998
Validation loss: 2.1121380111222625

Epoch: 6| Step: 11
Training loss: 2.0206825733184814
Validation loss: 2.1223626726417133

Epoch: 6| Step: 12
Training loss: 1.595320701599121
Validation loss: 2.120888263948502

Epoch: 6| Step: 13
Training loss: 1.1393852233886719
Validation loss: 2.1510584149309384

Epoch: 168| Step: 0
Training loss: 2.3556318283081055
Validation loss: 2.1139046094750844

Epoch: 6| Step: 1
Training loss: 1.9400670528411865
Validation loss: 2.117627311778325

Epoch: 6| Step: 2
Training loss: 1.6637181043624878
Validation loss: 2.0822467137408514

Epoch: 6| Step: 3
Training loss: 1.8097556829452515
Validation loss: 2.040866131423622

Epoch: 6| Step: 4
Training loss: 1.4330642223358154
Validation loss: 2.022094320225459

Epoch: 6| Step: 5
Training loss: 2.1604108810424805
Validation loss: 1.9904883600050403

Epoch: 6| Step: 6
Training loss: 2.028073787689209
Validation loss: 1.9936163220354306

Epoch: 6| Step: 7
Training loss: 2.0269813537597656
Validation loss: 2.0132690040014123

Epoch: 6| Step: 8
Training loss: 1.9099323749542236
Validation loss: 2.0019645639645156

Epoch: 6| Step: 9
Training loss: 2.0292015075683594
Validation loss: 1.9924009179556241

Epoch: 6| Step: 10
Training loss: 1.9112894535064697
Validation loss: 1.9968334885053738

Epoch: 6| Step: 11
Training loss: 2.0499157905578613
Validation loss: 1.9897463206321961

Epoch: 6| Step: 12
Training loss: 1.5269601345062256
Validation loss: 1.9922302602439799

Epoch: 6| Step: 13
Training loss: 1.5141494274139404
Validation loss: 1.9924305292867845

Epoch: 169| Step: 0
Training loss: 1.603226661682129
Validation loss: 2.0233956434393443

Epoch: 6| Step: 1
Training loss: 1.6291389465332031
Validation loss: 2.0328810573906027

Epoch: 6| Step: 2
Training loss: 1.8499996662139893
Validation loss: 2.0698375932631956

Epoch: 6| Step: 3
Training loss: 2.036588191986084
Validation loss: 2.1099817342655633

Epoch: 6| Step: 4
Training loss: 1.5220842361450195
Validation loss: 2.1243052354422947

Epoch: 6| Step: 5
Training loss: 1.9090206623077393
Validation loss: 2.1085347462725896

Epoch: 6| Step: 6
Training loss: 1.5545614957809448
Validation loss: 2.06981990157917

Epoch: 6| Step: 7
Training loss: 2.1593027114868164
Validation loss: 2.02357800545231

Epoch: 6| Step: 8
Training loss: 2.55411696434021
Validation loss: 1.9981851077848864

Epoch: 6| Step: 9
Training loss: 2.3236494064331055
Validation loss: 1.9880162823584773

Epoch: 6| Step: 10
Training loss: 2.2521095275878906
Validation loss: 1.9796562348642657

Epoch: 6| Step: 11
Training loss: 2.0149972438812256
Validation loss: 1.9683977891040105

Epoch: 6| Step: 12
Training loss: 1.4171994924545288
Validation loss: 1.9907119838140344

Epoch: 6| Step: 13
Training loss: 1.0999979972839355
Validation loss: 1.9843026399612427

Epoch: 170| Step: 0
Training loss: 1.7757692337036133
Validation loss: 1.9790176448001657

Epoch: 6| Step: 1
Training loss: 2.254911422729492
Validation loss: 1.9864486122644076

Epoch: 6| Step: 2
Training loss: 1.5745363235473633
Validation loss: 2.0261841076676563

Epoch: 6| Step: 3
Training loss: 1.862339973449707
Validation loss: 2.027182857195536

Epoch: 6| Step: 4
Training loss: 1.2323451042175293
Validation loss: 1.9919918147466515

Epoch: 6| Step: 5
Training loss: 1.3232462406158447
Validation loss: 1.993170198573861

Epoch: 6| Step: 6
Training loss: 2.5039706230163574
Validation loss: 2.0093316467859412

Epoch: 6| Step: 7
Training loss: 2.0244150161743164
Validation loss: 2.0478126143896453

Epoch: 6| Step: 8
Training loss: 2.191307306289673
Validation loss: 2.035601974815451

Epoch: 6| Step: 9
Training loss: 2.486438274383545
Validation loss: 2.0080181911427486

Epoch: 6| Step: 10
Training loss: 2.294257640838623
Validation loss: 1.9658469064261324

Epoch: 6| Step: 11
Training loss: 1.7228295803070068
Validation loss: 1.9770956090701524

Epoch: 6| Step: 12
Training loss: 1.388279676437378
Validation loss: 1.9732355866380917

Epoch: 6| Step: 13
Training loss: 1.2484698295593262
Validation loss: 1.9667717718308972

Epoch: 171| Step: 0
Training loss: 1.6610238552093506
Validation loss: 1.975391564830657

Epoch: 6| Step: 1
Training loss: 2.0949838161468506
Validation loss: 1.9833428975074523

Epoch: 6| Step: 2
Training loss: 1.433168649673462
Validation loss: 1.9793686456577753

Epoch: 6| Step: 3
Training loss: 1.8940091133117676
Validation loss: 1.9786267895852365

Epoch: 6| Step: 4
Training loss: 1.7097837924957275
Validation loss: 2.017905021226534

Epoch: 6| Step: 5
Training loss: 2.273118019104004
Validation loss: 2.036316692188222

Epoch: 6| Step: 6
Training loss: 2.1238882541656494
Validation loss: 2.079978904416484

Epoch: 6| Step: 7
Training loss: 2.309157371520996
Validation loss: 2.077375219714257

Epoch: 6| Step: 8
Training loss: 1.4817609786987305
Validation loss: 2.045062647070936

Epoch: 6| Step: 9
Training loss: 1.863178014755249
Validation loss: 2.0057809622057023

Epoch: 6| Step: 10
Training loss: 1.759225606918335
Validation loss: 1.9791278736565703

Epoch: 6| Step: 11
Training loss: 0.6975476741790771
Validation loss: 1.962995406120054

Epoch: 6| Step: 12
Training loss: 2.2426743507385254
Validation loss: 1.949983412219632

Epoch: 6| Step: 13
Training loss: 2.877671718597412
Validation loss: 1.9507760847768476

Epoch: 172| Step: 0
Training loss: 2.0710268020629883
Validation loss: 1.9640858429734425

Epoch: 6| Step: 1
Training loss: 1.4901795387268066
Validation loss: 1.9731850034447127

Epoch: 6| Step: 2
Training loss: 1.6020092964172363
Validation loss: 1.987297317033173

Epoch: 6| Step: 3
Training loss: 1.7848265171051025
Validation loss: 2.031530632767626

Epoch: 6| Step: 4
Training loss: 1.5913236141204834
Validation loss: 2.0244515531806537

Epoch: 6| Step: 5
Training loss: 2.414431571960449
Validation loss: 2.0513042429442048

Epoch: 6| Step: 6
Training loss: 2.4584732055664062
Validation loss: 2.062050691214941

Epoch: 6| Step: 7
Training loss: 1.3436596393585205
Validation loss: 2.066903514246787

Epoch: 6| Step: 8
Training loss: 1.774048089981079
Validation loss: 2.024662725387081

Epoch: 6| Step: 9
Training loss: 1.2371222972869873
Validation loss: 2.0090391635894775

Epoch: 6| Step: 10
Training loss: 1.9508440494537354
Validation loss: 1.9948450929375106

Epoch: 6| Step: 11
Training loss: 1.852693796157837
Validation loss: 1.9834921436925088

Epoch: 6| Step: 12
Training loss: 1.9470293521881104
Validation loss: 2.0054189748661493

Epoch: 6| Step: 13
Training loss: 2.3268909454345703
Validation loss: 2.0203195541135726

Epoch: 173| Step: 0
Training loss: 1.8590140342712402
Validation loss: 2.0013117533858105

Epoch: 6| Step: 1
Training loss: 1.6304562091827393
Validation loss: 2.0092514484159407

Epoch: 6| Step: 2
Training loss: 1.5884183645248413
Validation loss: 2.0055065539575394

Epoch: 6| Step: 3
Training loss: 1.8402129411697388
Validation loss: 2.0124657205356065

Epoch: 6| Step: 4
Training loss: 2.0432398319244385
Validation loss: 2.0240855550253265

Epoch: 6| Step: 5
Training loss: 1.784254789352417
Validation loss: 2.0299702767402894

Epoch: 6| Step: 6
Training loss: 1.7677099704742432
Validation loss: 2.0217269530860325

Epoch: 6| Step: 7
Training loss: 1.2494621276855469
Validation loss: 2.0423520393269037

Epoch: 6| Step: 8
Training loss: 1.2494654655456543
Validation loss: 2.0394421136507423

Epoch: 6| Step: 9
Training loss: 2.307666301727295
Validation loss: 2.097418249294322

Epoch: 6| Step: 10
Training loss: 2.3205718994140625
Validation loss: 2.154147985160992

Epoch: 6| Step: 11
Training loss: 2.125298261642456
Validation loss: 2.1374851888225925

Epoch: 6| Step: 12
Training loss: 1.3827266693115234
Validation loss: 2.0966155118839715

Epoch: 6| Step: 13
Training loss: 2.7075374126434326
Validation loss: 2.019632600968884

Epoch: 174| Step: 0
Training loss: 1.5954432487487793
Validation loss: 1.9951717186999578

Epoch: 6| Step: 1
Training loss: 1.687976360321045
Validation loss: 1.943708139081155

Epoch: 6| Step: 2
Training loss: 2.2910261154174805
Validation loss: 1.9364067918510848

Epoch: 6| Step: 3
Training loss: 2.186178207397461
Validation loss: 1.9486297087002826

Epoch: 6| Step: 4
Training loss: 1.6005852222442627
Validation loss: 1.9724932652647778

Epoch: 6| Step: 5
Training loss: 2.0282342433929443
Validation loss: 1.9648527124876618

Epoch: 6| Step: 6
Training loss: 2.311490058898926
Validation loss: 1.9756322983772523

Epoch: 6| Step: 7
Training loss: 2.2739956378936768
Validation loss: 1.9660885590378956

Epoch: 6| Step: 8
Training loss: 2.0436251163482666
Validation loss: 1.969827933978009

Epoch: 6| Step: 9
Training loss: 1.4888113737106323
Validation loss: 1.9669244545762257

Epoch: 6| Step: 10
Training loss: 1.4243409633636475
Validation loss: 2.005813644778344

Epoch: 6| Step: 11
Training loss: 1.1291403770446777
Validation loss: 2.023712396621704

Epoch: 6| Step: 12
Training loss: 1.946122646331787
Validation loss: 2.0686396911580074

Epoch: 6| Step: 13
Training loss: 1.6262211799621582
Validation loss: 2.0545257214576966

Epoch: 175| Step: 0
Training loss: 1.3111776113510132
Validation loss: 2.0348811367506623

Epoch: 6| Step: 1
Training loss: 1.9112545251846313
Validation loss: 2.0132134114542315

Epoch: 6| Step: 2
Training loss: 1.4642688035964966
Validation loss: 2.0221912143050984

Epoch: 6| Step: 3
Training loss: 1.5541775226593018
Validation loss: 2.016613932066066

Epoch: 6| Step: 4
Training loss: 2.0274722576141357
Validation loss: 2.0075153509775796

Epoch: 6| Step: 5
Training loss: 1.8763318061828613
Validation loss: 2.0155044550536783

Epoch: 6| Step: 6
Training loss: 1.373936653137207
Validation loss: 2.0194777724563435

Epoch: 6| Step: 7
Training loss: 1.8343098163604736
Validation loss: 1.986394146437286

Epoch: 6| Step: 8
Training loss: 1.898330807685852
Validation loss: 1.994485608993038

Epoch: 6| Step: 9
Training loss: 2.0862810611724854
Validation loss: 2.017283090981104

Epoch: 6| Step: 10
Training loss: 2.0643930435180664
Validation loss: 2.0347006987499934

Epoch: 6| Step: 11
Training loss: 1.9694116115570068
Validation loss: 2.028874175522917

Epoch: 6| Step: 12
Training loss: 1.713518500328064
Validation loss: 2.0261899809683523

Epoch: 6| Step: 13
Training loss: 2.4261929988861084
Validation loss: 2.030158837636312

Testing loss: 2.221344221962823

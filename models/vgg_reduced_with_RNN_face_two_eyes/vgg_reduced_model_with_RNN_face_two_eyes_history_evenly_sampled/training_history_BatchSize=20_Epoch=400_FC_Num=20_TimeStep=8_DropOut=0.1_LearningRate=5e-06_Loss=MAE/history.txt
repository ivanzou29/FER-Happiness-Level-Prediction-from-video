Epoch: 1| Step: 0
Training loss: 5.9838056564331055
Validation loss: 5.147947213983023

Epoch: 5| Step: 1
Training loss: 4.659976005554199
Validation loss: 5.140529606931953

Epoch: 5| Step: 2
Training loss: 5.3824896812438965
Validation loss: 5.134304682413737

Epoch: 5| Step: 3
Training loss: 4.906884670257568
Validation loss: 5.128686817743445

Epoch: 5| Step: 4
Training loss: 4.95142936706543
Validation loss: 5.123389418407153

Epoch: 5| Step: 5
Training loss: 5.0712056159973145
Validation loss: 5.118764687609929

Epoch: 5| Step: 6
Training loss: 4.357331275939941
Validation loss: 5.1144134972685125

Epoch: 5| Step: 7
Training loss: 4.922749042510986
Validation loss: 5.109914502789898

Epoch: 5| Step: 8
Training loss: 4.585824489593506
Validation loss: 5.105851963002195

Epoch: 5| Step: 9
Training loss: 4.692831993103027
Validation loss: 5.1015895515359855

Epoch: 5| Step: 10
Training loss: 4.521245002746582
Validation loss: 5.097044114143618

Epoch: 2| Step: 0
Training loss: 4.10078763961792
Validation loss: 5.0921998690533385

Epoch: 5| Step: 1
Training loss: 4.761971950531006
Validation loss: 5.087712636557958

Epoch: 5| Step: 2
Training loss: 5.033825874328613
Validation loss: 5.082841068185786

Epoch: 5| Step: 3
Training loss: 4.415688514709473
Validation loss: 5.077630566012475

Epoch: 5| Step: 4
Training loss: 5.517744064331055
Validation loss: 5.0721424266856205

Epoch: 5| Step: 5
Training loss: 4.358178615570068
Validation loss: 5.067314312022219

Epoch: 5| Step: 6
Training loss: 5.1017537117004395
Validation loss: 5.061227208824568

Epoch: 5| Step: 7
Training loss: 5.908365249633789
Validation loss: 5.055603365744314

Epoch: 5| Step: 8
Training loss: 5.588846683502197
Validation loss: 5.049806651248727

Epoch: 5| Step: 9
Training loss: 4.02147912979126
Validation loss: 5.043628764408891

Epoch: 5| Step: 10
Training loss: 4.63599157333374
Validation loss: 5.0368825492038525

Epoch: 3| Step: 0
Training loss: 4.8213300704956055
Validation loss: 5.029719147630917

Epoch: 5| Step: 1
Training loss: 4.6121673583984375
Validation loss: 5.0220952392906275

Epoch: 5| Step: 2
Training loss: 4.456896781921387
Validation loss: 5.014365452592091

Epoch: 5| Step: 3
Training loss: 3.7568306922912598
Validation loss: 5.006012511509721

Epoch: 5| Step: 4
Training loss: 5.141816139221191
Validation loss: 4.997940606968378

Epoch: 5| Step: 5
Training loss: 5.056418418884277
Validation loss: 4.98832199137698

Epoch: 5| Step: 6
Training loss: 4.442244052886963
Validation loss: 4.979272688588789

Epoch: 5| Step: 7
Training loss: 4.850096225738525
Validation loss: 4.969265081549204

Epoch: 5| Step: 8
Training loss: 4.663036346435547
Validation loss: 4.958676789396552

Epoch: 5| Step: 9
Training loss: 5.887768745422363
Validation loss: 4.947616725839595

Epoch: 5| Step: 10
Training loss: 4.916679859161377
Validation loss: 4.935867719752814

Epoch: 4| Step: 0
Training loss: 4.205156326293945
Validation loss: 4.9233200985898256

Epoch: 5| Step: 1
Training loss: 4.571159362792969
Validation loss: 4.910156906292003

Epoch: 5| Step: 2
Training loss: 4.98159122467041
Validation loss: 4.896456597953715

Epoch: 5| Step: 3
Training loss: 4.5817766189575195
Validation loss: 4.882289553201327

Epoch: 5| Step: 4
Training loss: 4.346541404724121
Validation loss: 4.867258697427729

Epoch: 5| Step: 5
Training loss: 5.120352745056152
Validation loss: 4.8531755683242634

Epoch: 5| Step: 6
Training loss: 4.379897117614746
Validation loss: 4.836833687238796

Epoch: 5| Step: 7
Training loss: 4.261772155761719
Validation loss: 4.820290873127599

Epoch: 5| Step: 8
Training loss: 5.308213710784912
Validation loss: 4.803626978269187

Epoch: 5| Step: 9
Training loss: 3.857006788253784
Validation loss: 4.785427939507269

Epoch: 5| Step: 10
Training loss: 5.586344242095947
Validation loss: 4.767080450570711

Epoch: 5| Step: 0
Training loss: 4.3490424156188965
Validation loss: 4.747196633328674

Epoch: 5| Step: 1
Training loss: 4.423709392547607
Validation loss: 4.729142332589754

Epoch: 5| Step: 2
Training loss: 5.109376430511475
Validation loss: 4.709337660061416

Epoch: 5| Step: 3
Training loss: 5.490244388580322
Validation loss: 4.690193330087969

Epoch: 5| Step: 4
Training loss: 4.236514091491699
Validation loss: 4.667981573330459

Epoch: 5| Step: 5
Training loss: 4.293932914733887
Validation loss: 4.6466751918997815

Epoch: 5| Step: 6
Training loss: 3.0941526889801025
Validation loss: 4.6247544493726505

Epoch: 5| Step: 7
Training loss: 4.621926784515381
Validation loss: 4.603464124023273

Epoch: 5| Step: 8
Training loss: 3.5873799324035645
Validation loss: 4.580600584706953

Epoch: 5| Step: 9
Training loss: 4.497339725494385
Validation loss: 4.556959547022338

Epoch: 5| Step: 10
Training loss: 5.084553241729736
Validation loss: 4.534232031914495

Epoch: 6| Step: 0
Training loss: 3.9215826988220215
Validation loss: 4.510854472396194

Epoch: 5| Step: 1
Training loss: 4.597193241119385
Validation loss: 4.486388898664905

Epoch: 5| Step: 2
Training loss: 4.031880855560303
Validation loss: 4.464433618771133

Epoch: 5| Step: 3
Training loss: 3.2561447620391846
Validation loss: 4.4397807121276855

Epoch: 5| Step: 4
Training loss: 5.662613391876221
Validation loss: 4.416890677585397

Epoch: 5| Step: 5
Training loss: 5.404446601867676
Validation loss: 4.394238241257206

Epoch: 5| Step: 6
Training loss: 4.029531478881836
Validation loss: 4.372890580085016

Epoch: 5| Step: 7
Training loss: 3.595914363861084
Validation loss: 4.349283249147477

Epoch: 5| Step: 8
Training loss: 4.175619602203369
Validation loss: 4.327798958747618

Epoch: 5| Step: 9
Training loss: 4.056846618652344
Validation loss: 4.306940109499039

Epoch: 5| Step: 10
Training loss: 3.2584660053253174
Validation loss: 4.286484251740158

Epoch: 7| Step: 0
Training loss: 3.187328815460205
Validation loss: 4.266820640974148

Epoch: 5| Step: 1
Training loss: 4.351286888122559
Validation loss: 4.245866406348444

Epoch: 5| Step: 2
Training loss: 4.457831859588623
Validation loss: 4.2263588854061656

Epoch: 5| Step: 3
Training loss: 3.421356201171875
Validation loss: 4.207389739251906

Epoch: 5| Step: 4
Training loss: 4.530672550201416
Validation loss: 4.187053521474202

Epoch: 5| Step: 5
Training loss: 4.087522029876709
Validation loss: 4.169029589622252

Epoch: 5| Step: 6
Training loss: 4.816906929016113
Validation loss: 4.151907592691401

Epoch: 5| Step: 7
Training loss: 3.647505283355713
Validation loss: 4.134152507269254

Epoch: 5| Step: 8
Training loss: 2.9443135261535645
Validation loss: 4.119176582623553

Epoch: 5| Step: 9
Training loss: 4.1845855712890625
Validation loss: 4.104184117368472

Epoch: 5| Step: 10
Training loss: 4.369661808013916
Validation loss: 4.0893835252331145

Epoch: 8| Step: 0
Training loss: 4.415329933166504
Validation loss: 4.0755895568478495

Epoch: 5| Step: 1
Training loss: 3.7980270385742188
Validation loss: 4.06108707510015

Epoch: 5| Step: 2
Training loss: 3.6433403491973877
Validation loss: 4.046514654672274

Epoch: 5| Step: 3
Training loss: 3.474191188812256
Validation loss: 4.031583483501147

Epoch: 5| Step: 4
Training loss: 4.153120994567871
Validation loss: 4.017675322871054

Epoch: 5| Step: 5
Training loss: 4.950342655181885
Validation loss: 4.007186612775249

Epoch: 5| Step: 6
Training loss: 4.40470027923584
Validation loss: 3.9942096689695954

Epoch: 5| Step: 7
Training loss: 4.929940700531006
Validation loss: 3.9816885020143244

Epoch: 5| Step: 8
Training loss: 3.1125471591949463
Validation loss: 3.969895855073006

Epoch: 5| Step: 9
Training loss: 3.0620884895324707
Validation loss: 3.9601163684680896

Epoch: 5| Step: 10
Training loss: 2.214707612991333
Validation loss: 3.9499266121977117

Epoch: 9| Step: 0
Training loss: 3.8242714405059814
Validation loss: 3.9385811846743346

Epoch: 5| Step: 1
Training loss: 3.194678783416748
Validation loss: 3.9300637706633537

Epoch: 5| Step: 2
Training loss: 3.8023815155029297
Validation loss: 3.920566015346076

Epoch: 5| Step: 3
Training loss: 3.19807505607605
Validation loss: 3.9103333719315065

Epoch: 5| Step: 4
Training loss: 4.4624342918396
Validation loss: 3.902485519327143

Epoch: 5| Step: 5
Training loss: 3.919088840484619
Validation loss: 3.8949832557350077

Epoch: 5| Step: 6
Training loss: 3.840224504470825
Validation loss: 3.8844578650689896

Epoch: 5| Step: 7
Training loss: 3.9432406425476074
Validation loss: 3.875388258246965

Epoch: 5| Step: 8
Training loss: 3.674227476119995
Validation loss: 3.867613741146621

Epoch: 5| Step: 9
Training loss: 4.022599697113037
Validation loss: 3.857996679121448

Epoch: 5| Step: 10
Training loss: 3.4123809337615967
Validation loss: 3.8505047213646675

Epoch: 10| Step: 0
Training loss: 3.289597988128662
Validation loss: 3.841758143517279

Epoch: 5| Step: 1
Training loss: 3.2147135734558105
Validation loss: 3.835575542142314

Epoch: 5| Step: 2
Training loss: 3.854144334793091
Validation loss: 3.8282655387796383

Epoch: 5| Step: 3
Training loss: 4.0894365310668945
Validation loss: 3.8204938801386024

Epoch: 5| Step: 4
Training loss: 3.7401039600372314
Validation loss: 3.8129059524946314

Epoch: 5| Step: 5
Training loss: 4.019748687744141
Validation loss: 3.8058690230051675

Epoch: 5| Step: 6
Training loss: 3.6450068950653076
Validation loss: 3.796689607763803

Epoch: 5| Step: 7
Training loss: 4.314876556396484
Validation loss: 3.7920827250326834

Epoch: 5| Step: 8
Training loss: 3.759780168533325
Validation loss: 3.785061938788301

Epoch: 5| Step: 9
Training loss: 2.9976184368133545
Validation loss: 3.7767212057626374

Epoch: 5| Step: 10
Training loss: 3.632925033569336
Validation loss: 3.7725635497800765

Epoch: 11| Step: 0
Training loss: 3.7015380859375
Validation loss: 3.7622670896591677

Epoch: 5| Step: 1
Training loss: 3.5255889892578125
Validation loss: 3.757293270480248

Epoch: 5| Step: 2
Training loss: 2.8292479515075684
Validation loss: 3.751562108275711

Epoch: 5| Step: 3
Training loss: 4.437753200531006
Validation loss: 3.74427463675058

Epoch: 5| Step: 4
Training loss: 3.2325870990753174
Validation loss: 3.738669744101904

Epoch: 5| Step: 5
Training loss: 3.449237108230591
Validation loss: 3.7306690677519767

Epoch: 5| Step: 6
Training loss: 3.786022663116455
Validation loss: 3.7242408542222876

Epoch: 5| Step: 7
Training loss: 3.048973560333252
Validation loss: 3.717057502397927

Epoch: 5| Step: 8
Training loss: 5.0096755027771
Validation loss: 3.7117089456127537

Epoch: 5| Step: 9
Training loss: 3.5644078254699707
Validation loss: 3.7065727941451536

Epoch: 5| Step: 10
Training loss: 3.2686336040496826
Validation loss: 3.7004577857191845

Epoch: 12| Step: 0
Training loss: 3.808539628982544
Validation loss: 3.6946822622770905

Epoch: 5| Step: 1
Training loss: 4.414303779602051
Validation loss: 3.6937604668319866

Epoch: 5| Step: 2
Training loss: 2.917463779449463
Validation loss: 3.686052829988541

Epoch: 5| Step: 3
Training loss: 3.7304489612579346
Validation loss: 3.6818084768069688

Epoch: 5| Step: 4
Training loss: 4.022336006164551
Validation loss: 3.6741210722154185

Epoch: 5| Step: 5
Training loss: 3.2963690757751465
Validation loss: 3.6708048979441323

Epoch: 5| Step: 6
Training loss: 3.4558022022247314
Validation loss: 3.664482719154768

Epoch: 5| Step: 7
Training loss: 3.0499064922332764
Validation loss: 3.658823443997291

Epoch: 5| Step: 8
Training loss: 3.3504066467285156
Validation loss: 3.656075308399816

Epoch: 5| Step: 9
Training loss: 3.945829391479492
Validation loss: 3.650164011986025

Epoch: 5| Step: 10
Training loss: 3.3117499351501465
Validation loss: 3.645430798171669

Epoch: 13| Step: 0
Training loss: 3.4207568168640137
Validation loss: 3.6410294014920472

Epoch: 5| Step: 1
Training loss: 3.7813942432403564
Validation loss: 3.6359042608609764

Epoch: 5| Step: 2
Training loss: 2.8953158855438232
Validation loss: 3.6331023247011247

Epoch: 5| Step: 3
Training loss: 3.2178955078125
Validation loss: 3.6270803046482865

Epoch: 5| Step: 4
Training loss: 4.367151737213135
Validation loss: 3.6253510905850317

Epoch: 5| Step: 5
Training loss: 3.996523380279541
Validation loss: 3.625354725827453

Epoch: 5| Step: 6
Training loss: 2.9623913764953613
Validation loss: 3.620447584377822

Epoch: 5| Step: 7
Training loss: 3.702296495437622
Validation loss: 3.614867312933809

Epoch: 5| Step: 8
Training loss: 4.797703266143799
Validation loss: 3.6096622995150986

Epoch: 5| Step: 9
Training loss: 2.3680367469787598
Validation loss: 3.6047018061402025

Epoch: 5| Step: 10
Training loss: 3.367551326751709
Validation loss: 3.603609372210759

Epoch: 14| Step: 0
Training loss: 4.2917256355285645
Validation loss: 3.6013006600000526

Epoch: 5| Step: 1
Training loss: 3.928117275238037
Validation loss: 3.6002249717712402

Epoch: 5| Step: 2
Training loss: 3.777266263961792
Validation loss: 3.59511992239183

Epoch: 5| Step: 3
Training loss: 2.672353744506836
Validation loss: 3.5914622532424105

Epoch: 5| Step: 4
Training loss: 2.942270278930664
Validation loss: 3.5888410716928463

Epoch: 5| Step: 5
Training loss: 2.7152092456817627
Validation loss: 3.5853730581140004

Epoch: 5| Step: 6
Training loss: 3.796713352203369
Validation loss: 3.580105904609926

Epoch: 5| Step: 7
Training loss: 3.494567394256592
Validation loss: 3.5794777280540875

Epoch: 5| Step: 8
Training loss: 3.751457929611206
Validation loss: 3.57599393270349

Epoch: 5| Step: 9
Training loss: 3.4590563774108887
Validation loss: 3.569023273324454

Epoch: 5| Step: 10
Training loss: 3.761256694793701
Validation loss: 3.569445892046857

Epoch: 15| Step: 0
Training loss: 3.4274821281433105
Validation loss: 3.564421581965621

Epoch: 5| Step: 1
Training loss: 3.2239983081817627
Validation loss: 3.5601216516187115

Epoch: 5| Step: 2
Training loss: 2.9513752460479736
Validation loss: 3.556134846902663

Epoch: 5| Step: 3
Training loss: 3.183401346206665
Validation loss: 3.556782655818488

Epoch: 5| Step: 4
Training loss: 3.4597716331481934
Validation loss: 3.551717858160696

Epoch: 5| Step: 5
Training loss: 3.6502044200897217
Validation loss: 3.549887377728698

Epoch: 5| Step: 6
Training loss: 4.079580783843994
Validation loss: 3.545314804200203

Epoch: 5| Step: 7
Training loss: 3.4054877758026123
Validation loss: 3.543219594545262

Epoch: 5| Step: 8
Training loss: 3.3149967193603516
Validation loss: 3.539404987007059

Epoch: 5| Step: 9
Training loss: 3.6683545112609863
Validation loss: 3.537248860123337

Epoch: 5| Step: 10
Training loss: 3.944632053375244
Validation loss: 3.5368487578566357

Epoch: 16| Step: 0
Training loss: 3.6793975830078125
Validation loss: 3.5337320784086823

Epoch: 5| Step: 1
Training loss: 3.9404735565185547
Validation loss: 3.530182489784815

Epoch: 5| Step: 2
Training loss: 3.810814619064331
Validation loss: 3.5257581536487868

Epoch: 5| Step: 3
Training loss: 3.1331379413604736
Validation loss: 3.5239476439773396

Epoch: 5| Step: 4
Training loss: 3.7871365547180176
Validation loss: 3.5266063572258077

Epoch: 5| Step: 5
Training loss: 3.5420405864715576
Validation loss: 3.5189596068474556

Epoch: 5| Step: 6
Training loss: 2.922955274581909
Validation loss: 3.517833691771312

Epoch: 5| Step: 7
Training loss: 2.5360023975372314
Validation loss: 3.5110641576910533

Epoch: 5| Step: 8
Training loss: 3.40421724319458
Validation loss: 3.5102239296000493

Epoch: 5| Step: 9
Training loss: 3.0961546897888184
Validation loss: 3.506289179607104

Epoch: 5| Step: 10
Training loss: 4.2098283767700195
Validation loss: 3.503217351052069

Epoch: 17| Step: 0
Training loss: 2.2309165000915527
Validation loss: 3.502059885250625

Epoch: 5| Step: 1
Training loss: 3.879971742630005
Validation loss: 3.5001018329333236

Epoch: 5| Step: 2
Training loss: 3.0997352600097656
Validation loss: 3.4958359631158973

Epoch: 5| Step: 3
Training loss: 3.1589388847351074
Validation loss: 3.496760747765982

Epoch: 5| Step: 4
Training loss: 2.6444475650787354
Validation loss: 3.4948659891723306

Epoch: 5| Step: 5
Training loss: 3.0474109649658203
Validation loss: 3.4941183700356433

Epoch: 5| Step: 6
Training loss: 3.846144914627075
Validation loss: 3.4879799837707193

Epoch: 5| Step: 7
Training loss: 3.916790723800659
Validation loss: 3.4844120856254333

Epoch: 5| Step: 8
Training loss: 3.7355079650878906
Validation loss: 3.48055507803476

Epoch: 5| Step: 9
Training loss: 4.081040382385254
Validation loss: 3.4834186825700986

Epoch: 5| Step: 10
Training loss: 4.165544509887695
Validation loss: 3.4776770427662838

Epoch: 18| Step: 0
Training loss: 3.5537102222442627
Validation loss: 3.474413856383293

Epoch: 5| Step: 1
Training loss: 3.2393798828125
Validation loss: 3.4709444840749106

Epoch: 5| Step: 2
Training loss: 3.724903106689453
Validation loss: 3.4733816500632995

Epoch: 5| Step: 3
Training loss: 3.1808223724365234
Validation loss: 3.47263789433305

Epoch: 5| Step: 4
Training loss: 3.2406082153320312
Validation loss: 3.4694020107228267

Epoch: 5| Step: 5
Training loss: 3.182030200958252
Validation loss: 3.4672887248377644

Epoch: 5| Step: 6
Training loss: 4.160401821136475
Validation loss: 3.4591112752114572

Epoch: 5| Step: 7
Training loss: 3.6171321868896484
Validation loss: 3.457664712782829

Epoch: 5| Step: 8
Training loss: 2.983837842941284
Validation loss: 3.4556357373473463

Epoch: 5| Step: 9
Training loss: 3.9484851360321045
Validation loss: 3.45246551370108

Epoch: 5| Step: 10
Training loss: 2.4939475059509277
Validation loss: 3.4544491255155174

Epoch: 19| Step: 0
Training loss: 2.527186870574951
Validation loss: 3.4479534728552705

Epoch: 5| Step: 1
Training loss: 3.3329601287841797
Validation loss: 3.456272489281111

Epoch: 5| Step: 2
Training loss: 4.137801647186279
Validation loss: 3.4527535464174006

Epoch: 5| Step: 3
Training loss: 3.365278720855713
Validation loss: 3.4418030938794537

Epoch: 5| Step: 4
Training loss: 3.6133899688720703
Validation loss: 3.4384506646023003

Epoch: 5| Step: 5
Training loss: 3.351750135421753
Validation loss: 3.445224536362515

Epoch: 5| Step: 6
Training loss: 2.964287281036377
Validation loss: 3.452155905385171

Epoch: 5| Step: 7
Training loss: 3.251059055328369
Validation loss: 3.449367851339361

Epoch: 5| Step: 8
Training loss: 3.846820116043091
Validation loss: 3.445597538384058

Epoch: 5| Step: 9
Training loss: 2.843705654144287
Validation loss: 3.436616054145239

Epoch: 5| Step: 10
Training loss: 4.145056247711182
Validation loss: 3.4270283534962642

Epoch: 20| Step: 0
Training loss: 3.5768845081329346
Validation loss: 3.4240877833417667

Epoch: 5| Step: 1
Training loss: 3.4826042652130127
Validation loss: 3.42586854709092

Epoch: 5| Step: 2
Training loss: 3.311755657196045
Validation loss: 3.431062759891633

Epoch: 5| Step: 3
Training loss: 3.200407028198242
Validation loss: 3.425089261865103

Epoch: 5| Step: 4
Training loss: 3.1182336807250977
Validation loss: 3.419092639800041

Epoch: 5| Step: 5
Training loss: 3.6528713703155518
Validation loss: 3.4135933332545783

Epoch: 5| Step: 6
Training loss: 3.577528476715088
Validation loss: 3.4148256599262194

Epoch: 5| Step: 7
Training loss: 3.107556104660034
Validation loss: 3.415194921596076

Epoch: 5| Step: 8
Training loss: 3.6942336559295654
Validation loss: 3.414872802713866

Epoch: 5| Step: 9
Training loss: 2.7127468585968018
Validation loss: 3.413876279707878

Epoch: 5| Step: 10
Training loss: 3.6631736755371094
Validation loss: 3.4092494595435356

Epoch: 21| Step: 0
Training loss: 3.6418113708496094
Validation loss: 3.40559555638221

Epoch: 5| Step: 1
Training loss: 3.4717204570770264
Validation loss: 3.3990115196474138

Epoch: 5| Step: 2
Training loss: 3.514003038406372
Validation loss: 3.398765646001344

Epoch: 5| Step: 3
Training loss: 2.7756171226501465
Validation loss: 3.3950197594140166

Epoch: 5| Step: 4
Training loss: 3.6115527153015137
Validation loss: 3.39263088985156

Epoch: 5| Step: 5
Training loss: 4.142083168029785
Validation loss: 3.3918521352993545

Epoch: 5| Step: 6
Training loss: 3.328463077545166
Validation loss: 3.3887394218034643

Epoch: 5| Step: 7
Training loss: 3.6562111377716064
Validation loss: 3.385822155142343

Epoch: 5| Step: 8
Training loss: 2.7868847846984863
Validation loss: 3.38327536788038

Epoch: 5| Step: 9
Training loss: 2.712768793106079
Validation loss: 3.3847393528107674

Epoch: 5| Step: 10
Training loss: 3.1173832416534424
Validation loss: 3.385045205393145

Epoch: 22| Step: 0
Training loss: 3.820218563079834
Validation loss: 3.3867700484491166

Epoch: 5| Step: 1
Training loss: 2.739344596862793
Validation loss: 3.380166458827193

Epoch: 5| Step: 2
Training loss: 3.5909030437469482
Validation loss: 3.3758158914504515

Epoch: 5| Step: 3
Training loss: 3.1485025882720947
Validation loss: 3.374786420535016

Epoch: 5| Step: 4
Training loss: 2.951241970062256
Validation loss: 3.372011894820839

Epoch: 5| Step: 5
Training loss: 3.4560539722442627
Validation loss: 3.374056621264386

Epoch: 5| Step: 6
Training loss: 3.6950480937957764
Validation loss: 3.370545228322347

Epoch: 5| Step: 7
Training loss: 3.2473297119140625
Validation loss: 3.3681607451490176

Epoch: 5| Step: 8
Training loss: 2.8087198734283447
Validation loss: 3.3591244348915676

Epoch: 5| Step: 9
Training loss: 3.409421920776367
Validation loss: 3.3601278643454275

Epoch: 5| Step: 10
Training loss: 3.823796510696411
Validation loss: 3.360046007299936

Epoch: 23| Step: 0
Training loss: 3.959169387817383
Validation loss: 3.360345655871976

Epoch: 5| Step: 1
Training loss: 2.97929048538208
Validation loss: 3.3565294332401727

Epoch: 5| Step: 2
Training loss: 3.8121180534362793
Validation loss: 3.353561842313377

Epoch: 5| Step: 3
Training loss: 3.353635311126709
Validation loss: 3.3474903106689453

Epoch: 5| Step: 4
Training loss: 4.082991600036621
Validation loss: 3.346656084060669

Epoch: 5| Step: 5
Training loss: 2.7134997844696045
Validation loss: 3.3448612100334576

Epoch: 5| Step: 6
Training loss: 3.3173439502716064
Validation loss: 3.3436552862967215

Epoch: 5| Step: 7
Training loss: 2.9781315326690674
Validation loss: 3.3396126788149596

Epoch: 5| Step: 8
Training loss: 3.2364895343780518
Validation loss: 3.3400535198949997

Epoch: 5| Step: 9
Training loss: 2.4540491104125977
Validation loss: 3.3356630212517193

Epoch: 5| Step: 10
Training loss: 3.525222063064575
Validation loss: 3.335190406409643

Epoch: 24| Step: 0
Training loss: 2.833806037902832
Validation loss: 3.332334913233275

Epoch: 5| Step: 1
Training loss: 2.9306063652038574
Validation loss: 3.3311744659177718

Epoch: 5| Step: 2
Training loss: 2.8970000743865967
Validation loss: 3.328252282193912

Epoch: 5| Step: 3
Training loss: 4.072527885437012
Validation loss: 3.32451065381368

Epoch: 5| Step: 4
Training loss: 3.1990513801574707
Validation loss: 3.323159089652441

Epoch: 5| Step: 5
Training loss: 2.962008237838745
Validation loss: 3.3181570114627963

Epoch: 5| Step: 6
Training loss: 3.746025800704956
Validation loss: 3.322669316363591

Epoch: 5| Step: 7
Training loss: 2.986217975616455
Validation loss: 3.3239469348743396

Epoch: 5| Step: 8
Training loss: 4.142735481262207
Validation loss: 3.336649787041449

Epoch: 5| Step: 9
Training loss: 3.423279285430908
Validation loss: 3.3176975096425703

Epoch: 5| Step: 10
Training loss: 2.9034388065338135
Validation loss: 3.31160327183303

Epoch: 25| Step: 0
Training loss: 3.8176465034484863
Validation loss: 3.3181863805299163

Epoch: 5| Step: 1
Training loss: 3.0707626342773438
Validation loss: 3.305915391573342

Epoch: 5| Step: 2
Training loss: 3.1080288887023926
Validation loss: 3.303063146529659

Epoch: 5| Step: 3
Training loss: 3.7078850269317627
Validation loss: 3.3023549356768207

Epoch: 5| Step: 4
Training loss: 3.169956922531128
Validation loss: 3.3023474883007746

Epoch: 5| Step: 5
Training loss: 3.3045897483825684
Validation loss: 3.299978615135275

Epoch: 5| Step: 6
Training loss: 2.8505992889404297
Validation loss: 3.298335449669951

Epoch: 5| Step: 7
Training loss: 3.1627814769744873
Validation loss: 3.297509216493176

Epoch: 5| Step: 8
Training loss: 2.9091668128967285
Validation loss: 3.2933804142859673

Epoch: 5| Step: 9
Training loss: 3.680619478225708
Validation loss: 3.2934207300986014

Epoch: 5| Step: 10
Training loss: 3.1747443675994873
Validation loss: 3.2909034811040407

Epoch: 26| Step: 0
Training loss: 3.2508435249328613
Validation loss: 3.290457287142354

Epoch: 5| Step: 1
Training loss: 3.298842668533325
Validation loss: 3.2850209461745394

Epoch: 5| Step: 2
Training loss: 4.050629615783691
Validation loss: 3.2865063785224833

Epoch: 5| Step: 3
Training loss: 3.4880287647247314
Validation loss: 3.2844235153608423

Epoch: 5| Step: 4
Training loss: 3.186185121536255
Validation loss: 3.277141107025967

Epoch: 5| Step: 5
Training loss: 3.481332302093506
Validation loss: 3.2832258952561246

Epoch: 5| Step: 6
Training loss: 3.413079023361206
Validation loss: 3.286979442001671

Epoch: 5| Step: 7
Training loss: 2.2806899547576904
Validation loss: 3.2929631433179303

Epoch: 5| Step: 8
Training loss: 3.0604889392852783
Validation loss: 3.283105875856133

Epoch: 5| Step: 9
Training loss: 3.146965742111206
Validation loss: 3.2692176706047467

Epoch: 5| Step: 10
Training loss: 3.1428518295288086
Validation loss: 3.2762843306346605

Epoch: 27| Step: 0
Training loss: 2.7701776027679443
Validation loss: 3.2735273068951023

Epoch: 5| Step: 1
Training loss: 2.53505539894104
Validation loss: 3.2644117724510933

Epoch: 5| Step: 2
Training loss: 3.4932663440704346
Validation loss: 3.2654453349369827

Epoch: 5| Step: 3
Training loss: 3.746565341949463
Validation loss: 3.26321566489435

Epoch: 5| Step: 4
Training loss: 4.180008888244629
Validation loss: 3.265957757990847

Epoch: 5| Step: 5
Training loss: 3.223114013671875
Validation loss: 3.259996275747976

Epoch: 5| Step: 6
Training loss: 3.6623833179473877
Validation loss: 3.2593936151073826

Epoch: 5| Step: 7
Training loss: 3.5727124214172363
Validation loss: 3.262354204731603

Epoch: 5| Step: 8
Training loss: 3.3940138816833496
Validation loss: 3.259420810207244

Epoch: 5| Step: 9
Training loss: 2.6284282207489014
Validation loss: 3.2542447813095583

Epoch: 5| Step: 10
Training loss: 2.307077169418335
Validation loss: 3.25614688216999

Epoch: 28| Step: 0
Training loss: 3.0293517112731934
Validation loss: 3.25220847898914

Epoch: 5| Step: 1
Training loss: 2.563878297805786
Validation loss: 3.2496701543049147

Epoch: 5| Step: 2
Training loss: 3.196868658065796
Validation loss: 3.2468899142357612

Epoch: 5| Step: 3
Training loss: 3.385488986968994
Validation loss: 3.2415377580991356

Epoch: 5| Step: 4
Training loss: 3.575011730194092
Validation loss: 3.2452753820726947

Epoch: 5| Step: 5
Training loss: 3.0104494094848633
Validation loss: 3.242525162235383

Epoch: 5| Step: 6
Training loss: 2.8135385513305664
Validation loss: 3.2369876574444514

Epoch: 5| Step: 7
Training loss: 3.8328895568847656
Validation loss: 3.2385242472412767

Epoch: 5| Step: 8
Training loss: 3.091933488845825
Validation loss: 3.2337249273894937

Epoch: 5| Step: 9
Training loss: 3.6878867149353027
Validation loss: 3.237270442388391

Epoch: 5| Step: 10
Training loss: 3.3356480598449707
Validation loss: 3.2350651320590766

Epoch: 29| Step: 0
Training loss: 2.9331142902374268
Validation loss: 3.2319669928601993

Epoch: 5| Step: 1
Training loss: 3.6565558910369873
Validation loss: 3.227265686117193

Epoch: 5| Step: 2
Training loss: 3.130155086517334
Validation loss: 3.2253805668123308

Epoch: 5| Step: 3
Training loss: 4.528315544128418
Validation loss: 3.2282471195344002

Epoch: 5| Step: 4
Training loss: 2.7862353324890137
Validation loss: 3.225405470017464

Epoch: 5| Step: 5
Training loss: 3.623434543609619
Validation loss: 3.2268346714717087

Epoch: 5| Step: 6
Training loss: 3.7584640979766846
Validation loss: 3.2259295037997666

Epoch: 5| Step: 7
Training loss: 3.432095766067505
Validation loss: 3.2269851853770595

Epoch: 5| Step: 8
Training loss: 2.113772392272949
Validation loss: 3.223140783207391

Epoch: 5| Step: 9
Training loss: 2.4791150093078613
Validation loss: 3.230715238919822

Epoch: 5| Step: 10
Training loss: 2.896733045578003
Validation loss: 3.2267918586730957

Epoch: 30| Step: 0
Training loss: 3.817805528640747
Validation loss: 3.2153697552219516

Epoch: 5| Step: 1
Training loss: 3.659064531326294
Validation loss: 3.2089332380602436

Epoch: 5| Step: 2
Training loss: 2.6437788009643555
Validation loss: 3.207940475915068

Epoch: 5| Step: 3
Training loss: 3.8757152557373047
Validation loss: 3.2109912339077202

Epoch: 5| Step: 4
Training loss: 3.541757106781006
Validation loss: 3.2115594392181723

Epoch: 5| Step: 5
Training loss: 2.881751298904419
Validation loss: 3.211016998496107

Epoch: 5| Step: 6
Training loss: 3.0181965827941895
Validation loss: 3.206191142400106

Epoch: 5| Step: 7
Training loss: 3.2106881141662598
Validation loss: 3.2037641899560088

Epoch: 5| Step: 8
Training loss: 2.416893720626831
Validation loss: 3.1992026400822464

Epoch: 5| Step: 9
Training loss: 2.6268086433410645
Validation loss: 3.195696876895043

Epoch: 5| Step: 10
Training loss: 3.6074864864349365
Validation loss: 3.1929211257606425

Epoch: 31| Step: 0
Training loss: 3.30131459236145
Validation loss: 3.193621015035978

Epoch: 5| Step: 1
Training loss: 3.7423489093780518
Validation loss: 3.189915257115518

Epoch: 5| Step: 2
Training loss: 3.933194398880005
Validation loss: 3.1905061147546254

Epoch: 5| Step: 3
Training loss: 2.0278260707855225
Validation loss: 3.1926702658335366

Epoch: 5| Step: 4
Training loss: 2.9367311000823975
Validation loss: 3.188616201441775

Epoch: 5| Step: 5
Training loss: 2.8146138191223145
Validation loss: 3.18670565594909

Epoch: 5| Step: 6
Training loss: 3.840400218963623
Validation loss: 3.1857389993565057

Epoch: 5| Step: 7
Training loss: 2.9956648349761963
Validation loss: 3.1797331225487495

Epoch: 5| Step: 8
Training loss: 2.8272652626037598
Validation loss: 3.1759595614607616

Epoch: 5| Step: 9
Training loss: 3.642948865890503
Validation loss: 3.1692597045693347

Epoch: 5| Step: 10
Training loss: 2.9395267963409424
Validation loss: 3.1771894090919086

Epoch: 32| Step: 0
Training loss: 2.4119842052459717
Validation loss: 3.171536886563865

Epoch: 5| Step: 1
Training loss: 2.648000717163086
Validation loss: 3.172014251832039

Epoch: 5| Step: 2
Training loss: 2.9665236473083496
Validation loss: 3.1669275786287043

Epoch: 5| Step: 3
Training loss: 3.971155881881714
Validation loss: 3.1671624029836347

Epoch: 5| Step: 4
Training loss: 2.205540180206299
Validation loss: 3.168116100372807

Epoch: 5| Step: 5
Training loss: 4.077643871307373
Validation loss: 3.166595152629319

Epoch: 5| Step: 6
Training loss: 3.3269951343536377
Validation loss: 3.162211387388168

Epoch: 5| Step: 7
Training loss: 3.1250596046447754
Validation loss: 3.1586897655199935

Epoch: 5| Step: 8
Training loss: 3.5156338214874268
Validation loss: 3.1597694145735873

Epoch: 5| Step: 9
Training loss: 2.791360855102539
Validation loss: 3.157159300260646

Epoch: 5| Step: 10
Training loss: 3.9613261222839355
Validation loss: 3.1612727206240416

Epoch: 33| Step: 0
Training loss: 2.9092063903808594
Validation loss: 3.1625946516631753

Epoch: 5| Step: 1
Training loss: 3.0223679542541504
Validation loss: 3.1585696205016105

Epoch: 5| Step: 2
Training loss: 3.428260087966919
Validation loss: 3.150814494779033

Epoch: 5| Step: 3
Training loss: 2.6483561992645264
Validation loss: 3.14746856176725

Epoch: 5| Step: 4
Training loss: 3.6587910652160645
Validation loss: 3.144553589564498

Epoch: 5| Step: 5
Training loss: 2.971775770187378
Validation loss: 3.1418795636905137

Epoch: 5| Step: 6
Training loss: 2.9636998176574707
Validation loss: 3.141287624195058

Epoch: 5| Step: 7
Training loss: 4.166314601898193
Validation loss: 3.1470251544829337

Epoch: 5| Step: 8
Training loss: 2.7367806434631348
Validation loss: 3.1542310253266366

Epoch: 5| Step: 9
Training loss: 3.192418336868286
Validation loss: 3.137375485512518

Epoch: 5| Step: 10
Training loss: 2.9603466987609863
Validation loss: 3.1337212567688315

Epoch: 34| Step: 0
Training loss: 3.6524243354797363
Validation loss: 3.144401691293204

Epoch: 5| Step: 1
Training loss: 3.6744837760925293
Validation loss: 3.1354423825458815

Epoch: 5| Step: 2
Training loss: 3.6974053382873535
Validation loss: 3.1297649311762985

Epoch: 5| Step: 3
Training loss: 3.21856689453125
Validation loss: 3.1236733313529723

Epoch: 5| Step: 4
Training loss: 3.1768107414245605
Validation loss: 3.1233127809339956

Epoch: 5| Step: 5
Training loss: 1.6114113330841064
Validation loss: 3.1181366930725756

Epoch: 5| Step: 6
Training loss: 3.419917345046997
Validation loss: 3.1181307556808635

Epoch: 5| Step: 7
Training loss: 3.113759994506836
Validation loss: 3.118133537230953

Epoch: 5| Step: 8
Training loss: 3.1826846599578857
Validation loss: 3.1207665986912225

Epoch: 5| Step: 9
Training loss: 3.4745726585388184
Validation loss: 3.1198984474264164

Epoch: 5| Step: 10
Training loss: 2.2418971061706543
Validation loss: 3.117504668492143

Epoch: 35| Step: 0
Training loss: 2.725607395172119
Validation loss: 3.113698702986522

Epoch: 5| Step: 1
Training loss: 2.6276917457580566
Validation loss: 3.1113861401875815

Epoch: 5| Step: 2
Training loss: 3.2593493461608887
Validation loss: 3.110882559130269

Epoch: 5| Step: 3
Training loss: 2.5479865074157715
Validation loss: 3.103820305998607

Epoch: 5| Step: 4
Training loss: 3.1758675575256348
Validation loss: 3.108868629701676

Epoch: 5| Step: 5
Training loss: 3.4306628704071045
Validation loss: 3.107701886084772

Epoch: 5| Step: 6
Training loss: 3.1368823051452637
Validation loss: 3.1139920834572083

Epoch: 5| Step: 7
Training loss: 3.124441146850586
Validation loss: 3.1013338283825944

Epoch: 5| Step: 8
Training loss: 3.518965482711792
Validation loss: 3.0977739928871073

Epoch: 5| Step: 9
Training loss: 2.7454190254211426
Validation loss: 3.100860118865967

Epoch: 5| Step: 10
Training loss: 4.284679412841797
Validation loss: 3.095074369061378

Epoch: 36| Step: 0
Training loss: 4.005928993225098
Validation loss: 3.095601189521051

Epoch: 5| Step: 1
Training loss: 2.723348379135132
Validation loss: 3.094361146291097

Epoch: 5| Step: 2
Training loss: 2.7376856803894043
Validation loss: 3.0963626215534825

Epoch: 5| Step: 3
Training loss: 2.9065613746643066
Validation loss: 3.1013412142312653

Epoch: 5| Step: 4
Training loss: 2.9937195777893066
Validation loss: 3.089865440963417

Epoch: 5| Step: 5
Training loss: 3.0879828929901123
Validation loss: 3.0890118588683424

Epoch: 5| Step: 6
Training loss: 3.031653881072998
Validation loss: 3.089908469107843

Epoch: 5| Step: 7
Training loss: 4.155122756958008
Validation loss: 3.085962841587682

Epoch: 5| Step: 8
Training loss: 2.601144790649414
Validation loss: 3.0903956018468386

Epoch: 5| Step: 9
Training loss: 3.1544089317321777
Validation loss: 3.088842591931743

Epoch: 5| Step: 10
Training loss: 2.8748157024383545
Validation loss: 3.105613031694966

Epoch: 37| Step: 0
Training loss: 2.8383049964904785
Validation loss: 3.1175272387842976

Epoch: 5| Step: 1
Training loss: 3.039865493774414
Validation loss: 3.1101912401055776

Epoch: 5| Step: 2
Training loss: 4.087901592254639
Validation loss: 3.1066712743492535

Epoch: 5| Step: 3
Training loss: 3.1944167613983154
Validation loss: 3.08057233595079

Epoch: 5| Step: 4
Training loss: 3.125555992126465
Validation loss: 3.0772196323640886

Epoch: 5| Step: 5
Training loss: 2.523146390914917
Validation loss: 3.083728354464295

Epoch: 5| Step: 6
Training loss: 3.2534737586975098
Validation loss: 3.0974052208726124

Epoch: 5| Step: 7
Training loss: 2.669241428375244
Validation loss: 3.0943007674268497

Epoch: 5| Step: 8
Training loss: 3.3648695945739746
Validation loss: 3.087952959922052

Epoch: 5| Step: 9
Training loss: 2.441859722137451
Validation loss: 3.100582250984766

Epoch: 5| Step: 10
Training loss: 3.927628993988037
Validation loss: 3.1107453428288943

Epoch: 38| Step: 0
Training loss: 2.891693592071533
Validation loss: 3.076550696485786

Epoch: 5| Step: 1
Training loss: 3.1198978424072266
Validation loss: 3.0715879471071306

Epoch: 5| Step: 2
Training loss: 2.5453803539276123
Validation loss: 3.0718670532267582

Epoch: 5| Step: 3
Training loss: 3.3442771434783936
Validation loss: 3.0726684652349

Epoch: 5| Step: 4
Training loss: 3.2188315391540527
Validation loss: 3.0741651468379523

Epoch: 5| Step: 5
Training loss: 4.59900426864624
Validation loss: 3.0864096713322464

Epoch: 5| Step: 6
Training loss: 3.0277862548828125
Validation loss: 3.0848465299093597

Epoch: 5| Step: 7
Training loss: 3.0464091300964355
Validation loss: 3.080235012115971

Epoch: 5| Step: 8
Training loss: 2.689509153366089
Validation loss: 3.0761276880900064

Epoch: 5| Step: 9
Training loss: 3.1290249824523926
Validation loss: 3.0720602261122836

Epoch: 5| Step: 10
Training loss: 2.5165910720825195
Validation loss: 3.0672973817394626

Epoch: 39| Step: 0
Training loss: 2.8999645709991455
Validation loss: 3.0675666537336124

Epoch: 5| Step: 1
Training loss: 2.4119834899902344
Validation loss: 3.0673102589063745

Epoch: 5| Step: 2
Training loss: 2.9926366806030273
Validation loss: 3.0754646485851658

Epoch: 5| Step: 3
Training loss: 2.8495614528656006
Validation loss: 3.0742242951546945

Epoch: 5| Step: 4
Training loss: 3.256326675415039
Validation loss: 3.0683502561302594

Epoch: 5| Step: 5
Training loss: 3.504277467727661
Validation loss: 3.0586014665583128

Epoch: 5| Step: 6
Training loss: 2.8727850914001465
Validation loss: 3.0555083520950808

Epoch: 5| Step: 7
Training loss: 3.2071499824523926
Validation loss: 3.050536614592357

Epoch: 5| Step: 8
Training loss: 3.558478593826294
Validation loss: 3.0487207930575133

Epoch: 5| Step: 9
Training loss: 3.540619373321533
Validation loss: 3.0506728054374777

Epoch: 5| Step: 10
Training loss: 2.985842227935791
Validation loss: 3.050318520556214

Epoch: 40| Step: 0
Training loss: 3.1343255043029785
Validation loss: 3.0470311487874677

Epoch: 5| Step: 1
Training loss: 3.6535861492156982
Validation loss: 3.052268607642061

Epoch: 5| Step: 2
Training loss: 2.8648722171783447
Validation loss: 3.0623801087820404

Epoch: 5| Step: 3
Training loss: 2.3519527912139893
Validation loss: 3.130593212701941

Epoch: 5| Step: 4
Training loss: 2.874080181121826
Validation loss: 3.128796418507894

Epoch: 5| Step: 5
Training loss: 4.238675117492676
Validation loss: 3.0996088725264355

Epoch: 5| Step: 6
Training loss: 2.584035634994507
Validation loss: 3.0517213190755537

Epoch: 5| Step: 7
Training loss: 2.9033520221710205
Validation loss: 3.0470473843236126

Epoch: 5| Step: 8
Training loss: 3.2434439659118652
Validation loss: 3.0416877679927374

Epoch: 5| Step: 9
Training loss: 2.8324618339538574
Validation loss: 3.0401837056682957

Epoch: 5| Step: 10
Training loss: 3.5651051998138428
Validation loss: 3.045992882021012

Epoch: 41| Step: 0
Training loss: 2.488208532333374
Validation loss: 3.0506767636986187

Epoch: 5| Step: 1
Training loss: 3.549210786819458
Validation loss: 3.050449320065078

Epoch: 5| Step: 2
Training loss: 3.785141706466675
Validation loss: 3.0422585625802316

Epoch: 5| Step: 3
Training loss: 3.2538001537323
Validation loss: 3.041355476584486

Epoch: 5| Step: 4
Training loss: 1.9585071802139282
Validation loss: 3.040259086957542

Epoch: 5| Step: 5
Training loss: 3.7730555534362793
Validation loss: 3.039784082802393

Epoch: 5| Step: 6
Training loss: 2.5487163066864014
Validation loss: 3.0388116657093005

Epoch: 5| Step: 7
Training loss: 3.1613192558288574
Validation loss: 3.036124788304811

Epoch: 5| Step: 8
Training loss: 3.5367660522460938
Validation loss: 3.0409526158404607

Epoch: 5| Step: 9
Training loss: 2.7654004096984863
Validation loss: 3.0415803975956415

Epoch: 5| Step: 10
Training loss: 3.1749496459960938
Validation loss: 3.034090167732649

Epoch: 42| Step: 0
Training loss: 3.628221035003662
Validation loss: 3.032376961041522

Epoch: 5| Step: 1
Training loss: 3.4339587688446045
Validation loss: 3.030294418334961

Epoch: 5| Step: 2
Training loss: 2.6770989894866943
Validation loss: 3.0343251330878145

Epoch: 5| Step: 3
Training loss: 3.6219780445098877
Validation loss: 3.0299219905689196

Epoch: 5| Step: 4
Training loss: 1.782555341720581
Validation loss: 3.0332238853618665

Epoch: 5| Step: 5
Training loss: 3.873516798019409
Validation loss: 3.02504232621962

Epoch: 5| Step: 6
Training loss: 2.6273679733276367
Validation loss: 3.0308027831456994

Epoch: 5| Step: 7
Training loss: 3.2869887351989746
Validation loss: 3.025142736332391

Epoch: 5| Step: 8
Training loss: 2.09529185295105
Validation loss: 3.0227708355073006

Epoch: 5| Step: 9
Training loss: 3.0181784629821777
Validation loss: 3.022908995228429

Epoch: 5| Step: 10
Training loss: 3.9555044174194336
Validation loss: 3.0265924597299225

Epoch: 43| Step: 0
Training loss: 3.0476837158203125
Validation loss: 3.022050478125131

Epoch: 5| Step: 1
Training loss: 2.7541561126708984
Validation loss: 3.0204909232354935

Epoch: 5| Step: 2
Training loss: 3.6581242084503174
Validation loss: 3.024878660837809

Epoch: 5| Step: 3
Training loss: 2.90820574760437
Validation loss: 3.0128095329448743

Epoch: 5| Step: 4
Training loss: 2.4180972576141357
Validation loss: 3.011270994781166

Epoch: 5| Step: 5
Training loss: 2.873610019683838
Validation loss: 3.0115942493561776

Epoch: 5| Step: 6
Training loss: 2.758862018585205
Validation loss: 3.0145780860736804

Epoch: 5| Step: 7
Training loss: 3.5447311401367188
Validation loss: 3.0140622097958802

Epoch: 5| Step: 8
Training loss: 3.2915091514587402
Validation loss: 3.0164383995917534

Epoch: 5| Step: 9
Training loss: 3.2487289905548096
Validation loss: 3.0170702882992324

Epoch: 5| Step: 10
Training loss: 3.420003652572632
Validation loss: 3.0179771428467124

Epoch: 44| Step: 0
Training loss: 2.9159750938415527
Validation loss: 3.0178223527887815

Epoch: 5| Step: 1
Training loss: 2.9247801303863525
Validation loss: 3.0171464720079975

Epoch: 5| Step: 2
Training loss: 3.238668918609619
Validation loss: 3.0149257003620105

Epoch: 5| Step: 3
Training loss: 2.3607869148254395
Validation loss: 3.0152104029091458

Epoch: 5| Step: 4
Training loss: 3.8615517616271973
Validation loss: 3.014264360550911

Epoch: 5| Step: 5
Training loss: 2.91218638420105
Validation loss: 3.012064803031183

Epoch: 5| Step: 6
Training loss: 2.9637293815612793
Validation loss: 3.0087017679727204

Epoch: 5| Step: 7
Training loss: 3.5030837059020996
Validation loss: 3.0053951304445983

Epoch: 5| Step: 8
Training loss: 2.864327907562256
Validation loss: 3.0065273264402985

Epoch: 5| Step: 9
Training loss: 2.955753803253174
Validation loss: 3.0038263413213913

Epoch: 5| Step: 10
Training loss: 3.2893004417419434
Validation loss: 3.0054961994130123

Epoch: 45| Step: 0
Training loss: 3.458852767944336
Validation loss: 3.0057469644854145

Epoch: 5| Step: 1
Training loss: 3.4700427055358887
Validation loss: 3.004974055033858

Epoch: 5| Step: 2
Training loss: 2.587956666946411
Validation loss: 3.0041498163694977

Epoch: 5| Step: 3
Training loss: 2.850959062576294
Validation loss: 3.0030135441851873

Epoch: 5| Step: 4
Training loss: 3.6372344493865967
Validation loss: 3.004333326893468

Epoch: 5| Step: 5
Training loss: 3.3329200744628906
Validation loss: 3.005695619890767

Epoch: 5| Step: 6
Training loss: 3.115983724594116
Validation loss: 2.9993290516637985

Epoch: 5| Step: 7
Training loss: 2.3187780380249023
Validation loss: 2.9999620094094226

Epoch: 5| Step: 8
Training loss: 2.4023385047912598
Validation loss: 3.0004069369326354

Epoch: 5| Step: 9
Training loss: 2.7126686573028564
Validation loss: 2.999810367502192

Epoch: 5| Step: 10
Training loss: 3.798696517944336
Validation loss: 2.9971431762941423

Epoch: 46| Step: 0
Training loss: 3.369368076324463
Validation loss: 3.0031083578704507

Epoch: 5| Step: 1
Training loss: 3.016630172729492
Validation loss: 2.992703540350801

Epoch: 5| Step: 2
Training loss: 2.194603681564331
Validation loss: 2.9889311944284747

Epoch: 5| Step: 3
Training loss: 4.086281776428223
Validation loss: 2.985462704012471

Epoch: 5| Step: 4
Training loss: 2.792471408843994
Validation loss: 2.98828947928644

Epoch: 5| Step: 5
Training loss: 3.0567328929901123
Validation loss: 2.9886634734369095

Epoch: 5| Step: 6
Training loss: 2.6742770671844482
Validation loss: 2.985683371943812

Epoch: 5| Step: 7
Training loss: 3.1459097862243652
Validation loss: 2.989291726901967

Epoch: 5| Step: 8
Training loss: 2.64750337600708
Validation loss: 2.9848945217747844

Epoch: 5| Step: 9
Training loss: 3.0285651683807373
Validation loss: 2.9828228745409238

Epoch: 5| Step: 10
Training loss: 3.609326124191284
Validation loss: 2.9853272438049316

Epoch: 47| Step: 0
Training loss: 3.415966749191284
Validation loss: 2.9818756067624657

Epoch: 5| Step: 1
Training loss: 3.477933883666992
Validation loss: 2.980174256909278

Epoch: 5| Step: 2
Training loss: 2.8617358207702637
Validation loss: 2.9864955563699045

Epoch: 5| Step: 3
Training loss: 3.006883144378662
Validation loss: 2.9817760093237764

Epoch: 5| Step: 4
Training loss: 2.80195951461792
Validation loss: 2.9826751498765844

Epoch: 5| Step: 5
Training loss: 3.3665497303009033
Validation loss: 2.9793499695357455

Epoch: 5| Step: 6
Training loss: 2.9252235889434814
Validation loss: 2.9828602216577016

Epoch: 5| Step: 7
Training loss: 2.522728204727173
Validation loss: 2.979141168696906

Epoch: 5| Step: 8
Training loss: 2.7607064247131348
Validation loss: 2.9767924636922856

Epoch: 5| Step: 9
Training loss: 2.896082639694214
Validation loss: 2.9742422539700746

Epoch: 5| Step: 10
Training loss: 3.4547722339630127
Validation loss: 2.978834726477182

Epoch: 48| Step: 0
Training loss: 3.279651165008545
Validation loss: 2.9837927126115367

Epoch: 5| Step: 1
Training loss: 2.7243592739105225
Validation loss: 2.980624609096076

Epoch: 5| Step: 2
Training loss: 2.2579574584960938
Validation loss: 2.9796453009369555

Epoch: 5| Step: 3
Training loss: 3.657153367996216
Validation loss: 2.981230874215403

Epoch: 5| Step: 4
Training loss: 2.4753224849700928
Validation loss: 2.9841837293358258

Epoch: 5| Step: 5
Training loss: 3.203347682952881
Validation loss: 2.971793977163171

Epoch: 5| Step: 6
Training loss: 3.4456772804260254
Validation loss: 2.9690372379877235

Epoch: 5| Step: 7
Training loss: 2.412872552871704
Validation loss: 2.9669475657965547

Epoch: 5| Step: 8
Training loss: 4.042285919189453
Validation loss: 2.971785206948557

Epoch: 5| Step: 9
Training loss: 2.746534585952759
Validation loss: 2.9674549179692424

Epoch: 5| Step: 10
Training loss: 3.212394952774048
Validation loss: 2.9649660254037506

Epoch: 49| Step: 0
Training loss: 3.5579867362976074
Validation loss: 2.971712702064104

Epoch: 5| Step: 1
Training loss: 3.174710988998413
Validation loss: 2.9700105318459133

Epoch: 5| Step: 2
Training loss: 3.948655366897583
Validation loss: 2.9652265553833335

Epoch: 5| Step: 3
Training loss: 2.538508415222168
Validation loss: 2.964860862301242

Epoch: 5| Step: 4
Training loss: 2.152430295944214
Validation loss: 2.964631126772973

Epoch: 5| Step: 5
Training loss: 2.4448401927948
Validation loss: 2.9620925713610906

Epoch: 5| Step: 6
Training loss: 3.084372043609619
Validation loss: 2.963168477499357

Epoch: 5| Step: 7
Training loss: 3.547003984451294
Validation loss: 2.9608725142735306

Epoch: 5| Step: 8
Training loss: 2.7651028633117676
Validation loss: 2.960826007268762

Epoch: 5| Step: 9
Training loss: 3.7259879112243652
Validation loss: 2.963733808968657

Epoch: 5| Step: 10
Training loss: 2.2878549098968506
Validation loss: 2.9638149276856454

Epoch: 50| Step: 0
Training loss: 2.677302837371826
Validation loss: 2.9619460746806157

Epoch: 5| Step: 1
Training loss: 2.474152088165283
Validation loss: 2.964586845008276

Epoch: 5| Step: 2
Training loss: 2.2543323040008545
Validation loss: 2.9704836914616246

Epoch: 5| Step: 3
Training loss: 2.8836216926574707
Validation loss: 2.973731315264138

Epoch: 5| Step: 4
Training loss: 3.0435304641723633
Validation loss: 2.965632466859715

Epoch: 5| Step: 5
Training loss: 3.4812941551208496
Validation loss: 2.966457797634986

Epoch: 5| Step: 6
Training loss: 2.76977276802063
Validation loss: 2.957644688185825

Epoch: 5| Step: 7
Training loss: 3.155789613723755
Validation loss: 2.961017398424046

Epoch: 5| Step: 8
Training loss: 4.18599271774292
Validation loss: 2.956585232929517

Epoch: 5| Step: 9
Training loss: 3.32305645942688
Validation loss: 2.9568564045813774

Epoch: 5| Step: 10
Training loss: 3.057554244995117
Validation loss: 2.9551120240201234

Epoch: 51| Step: 0
Training loss: 2.1167006492614746
Validation loss: 2.95531887649208

Epoch: 5| Step: 1
Training loss: 2.896677255630493
Validation loss: 2.9509519197607554

Epoch: 5| Step: 2
Training loss: 2.976155996322632
Validation loss: 2.9560513701490176

Epoch: 5| Step: 3
Training loss: 3.1385977268218994
Validation loss: 2.9586117113790205

Epoch: 5| Step: 4
Training loss: 3.677072048187256
Validation loss: 2.966301677047565

Epoch: 5| Step: 5
Training loss: 3.1917166709899902
Validation loss: 2.9618165082828973

Epoch: 5| Step: 6
Training loss: 2.7439043521881104
Validation loss: 2.9667013588772027

Epoch: 5| Step: 7
Training loss: 2.6854889392852783
Validation loss: 2.9569686561502437

Epoch: 5| Step: 8
Training loss: 3.4098453521728516
Validation loss: 2.952170359191074

Epoch: 5| Step: 9
Training loss: 3.366708278656006
Validation loss: 2.954401418726931

Epoch: 5| Step: 10
Training loss: 3.138441562652588
Validation loss: 2.948826684746691

Epoch: 52| Step: 0
Training loss: 2.8914923667907715
Validation loss: 2.947703412784043

Epoch: 5| Step: 1
Training loss: 2.0921835899353027
Validation loss: 2.947473733655868

Epoch: 5| Step: 2
Training loss: 2.8451526165008545
Validation loss: 2.9484753377975954

Epoch: 5| Step: 3
Training loss: 3.1701645851135254
Validation loss: 2.9477618919905795

Epoch: 5| Step: 4
Training loss: 3.416386127471924
Validation loss: 2.9463250073053504

Epoch: 5| Step: 5
Training loss: 2.4247488975524902
Validation loss: 2.94536772594657

Epoch: 5| Step: 6
Training loss: 3.055110454559326
Validation loss: 2.9442759688182543

Epoch: 5| Step: 7
Training loss: 3.8244290351867676
Validation loss: 2.944418386746478

Epoch: 5| Step: 8
Training loss: 3.478591203689575
Validation loss: 2.9427840812231905

Epoch: 5| Step: 9
Training loss: 2.1723904609680176
Validation loss: 2.9416946031714

Epoch: 5| Step: 10
Training loss: 3.9957311153411865
Validation loss: 2.939582235069685

Epoch: 53| Step: 0
Training loss: 3.170079231262207
Validation loss: 2.9431327389132593

Epoch: 5| Step: 1
Training loss: 2.7035694122314453
Validation loss: 2.940310496155934

Epoch: 5| Step: 2
Training loss: 2.8275458812713623
Validation loss: 2.940179950447493

Epoch: 5| Step: 3
Training loss: 2.5633513927459717
Validation loss: 2.939864884140671

Epoch: 5| Step: 4
Training loss: 3.293623447418213
Validation loss: 2.9444501425630305

Epoch: 5| Step: 5
Training loss: 2.8089728355407715
Validation loss: 2.9367942451148905

Epoch: 5| Step: 6
Training loss: 3.5276572704315186
Validation loss: 2.940982846803563

Epoch: 5| Step: 7
Training loss: 3.9086146354675293
Validation loss: 2.9319215333589943

Epoch: 5| Step: 8
Training loss: 2.681076765060425
Validation loss: 2.935429198767549

Epoch: 5| Step: 9
Training loss: 2.9291224479675293
Validation loss: 2.9359553578079387

Epoch: 5| Step: 10
Training loss: 2.7048349380493164
Validation loss: 2.9343114822141585

Epoch: 54| Step: 0
Training loss: 2.2759578227996826
Validation loss: 2.9312009119218394

Epoch: 5| Step: 1
Training loss: 2.100599765777588
Validation loss: 2.9270001713947584

Epoch: 5| Step: 2
Training loss: 3.077220916748047
Validation loss: 2.9273545408761628

Epoch: 5| Step: 3
Training loss: 3.4069664478302
Validation loss: 2.921451125093686

Epoch: 5| Step: 4
Training loss: 3.7546393871307373
Validation loss: 2.9266166610102498

Epoch: 5| Step: 5
Training loss: 3.6186676025390625
Validation loss: 2.920505431390578

Epoch: 5| Step: 6
Training loss: 3.15551495552063
Validation loss: 2.9201309937302784

Epoch: 5| Step: 7
Training loss: 2.491797924041748
Validation loss: 2.925939288190616

Epoch: 5| Step: 8
Training loss: 3.0855698585510254
Validation loss: 2.914309496520668

Epoch: 5| Step: 9
Training loss: 2.829576015472412
Validation loss: 2.9175465850419897

Epoch: 5| Step: 10
Training loss: 3.2541489601135254
Validation loss: 2.9171154217053483

Epoch: 55| Step: 0
Training loss: 2.9745559692382812
Validation loss: 2.9163329908924718

Epoch: 5| Step: 1
Training loss: 2.6395790576934814
Validation loss: 2.9143917996396302

Epoch: 5| Step: 2
Training loss: 2.902782917022705
Validation loss: 2.9145779199497674

Epoch: 5| Step: 3
Training loss: 2.8161332607269287
Validation loss: 2.9142726800775014

Epoch: 5| Step: 4
Training loss: 3.5468432903289795
Validation loss: 2.9220979521351476

Epoch: 5| Step: 5
Training loss: 3.1092216968536377
Validation loss: 2.9190702643445743

Epoch: 5| Step: 6
Training loss: 2.899393081665039
Validation loss: 2.9179203792284896

Epoch: 5| Step: 7
Training loss: 3.0679752826690674
Validation loss: 2.914073880000781

Epoch: 5| Step: 8
Training loss: 2.9126288890838623
Validation loss: 2.9161508878072104

Epoch: 5| Step: 9
Training loss: 3.264237642288208
Validation loss: 2.9129433529351347

Epoch: 5| Step: 10
Training loss: 2.7637083530426025
Validation loss: 2.9121337347133185

Epoch: 56| Step: 0
Training loss: 3.563793897628784
Validation loss: 2.9074143696856756

Epoch: 5| Step: 1
Training loss: 2.0911481380462646
Validation loss: 2.9108266574080273

Epoch: 5| Step: 2
Training loss: 3.176703691482544
Validation loss: 2.9068533682054087

Epoch: 5| Step: 3
Training loss: 2.6388630867004395
Validation loss: 2.9043720358161518

Epoch: 5| Step: 4
Training loss: 2.6711859703063965
Validation loss: 2.905229781263618

Epoch: 5| Step: 5
Training loss: 3.020003318786621
Validation loss: 2.90964388590987

Epoch: 5| Step: 6
Training loss: 2.9701621532440186
Validation loss: 2.9255534295112855

Epoch: 5| Step: 7
Training loss: 3.206456422805786
Validation loss: 2.920176377860449

Epoch: 5| Step: 8
Training loss: 3.3921210765838623
Validation loss: 2.907314961956393

Epoch: 5| Step: 9
Training loss: 2.99845552444458
Validation loss: 2.9018403868521414

Epoch: 5| Step: 10
Training loss: 3.178786516189575
Validation loss: 2.9049137792279645

Epoch: 57| Step: 0
Training loss: 3.2823708057403564
Validation loss: 2.905880758839269

Epoch: 5| Step: 1
Training loss: 3.000983238220215
Validation loss: 2.906343303700929

Epoch: 5| Step: 2
Training loss: 2.940276622772217
Validation loss: 2.9030040489730013

Epoch: 5| Step: 3
Training loss: 3.100302219390869
Validation loss: 2.902342778380199

Epoch: 5| Step: 4
Training loss: 3.233442783355713
Validation loss: 2.90278983372514

Epoch: 5| Step: 5
Training loss: 3.286154270172119
Validation loss: 2.905541196946175

Epoch: 5| Step: 6
Training loss: 2.934403419494629
Validation loss: 2.910954067783971

Epoch: 5| Step: 7
Training loss: 2.476245164871216
Validation loss: 2.913639986386863

Epoch: 5| Step: 8
Training loss: 2.4062156677246094
Validation loss: 2.913530339476883

Epoch: 5| Step: 9
Training loss: 3.224529981613159
Validation loss: 2.9185300539898615

Epoch: 5| Step: 10
Training loss: 2.996788263320923
Validation loss: 2.911121065898608

Epoch: 58| Step: 0
Training loss: 3.40397310256958
Validation loss: 2.91159116068194

Epoch: 5| Step: 1
Training loss: 3.007871150970459
Validation loss: 2.905684563421434

Epoch: 5| Step: 2
Training loss: 1.9083049297332764
Validation loss: 2.89949220226657

Epoch: 5| Step: 3
Training loss: 2.796718120574951
Validation loss: 2.903086277746385

Epoch: 5| Step: 4
Training loss: 2.8980183601379395
Validation loss: 2.8947196416957404

Epoch: 5| Step: 5
Training loss: 3.202845335006714
Validation loss: 2.8936462786889847

Epoch: 5| Step: 6
Training loss: 3.0475621223449707
Validation loss: 2.8900198782643964

Epoch: 5| Step: 7
Training loss: 3.898907423019409
Validation loss: 2.8909373129567792

Epoch: 5| Step: 8
Training loss: 2.804871082305908
Validation loss: 2.8880477361781622

Epoch: 5| Step: 9
Training loss: 2.8043460845947266
Validation loss: 2.8859853949598087

Epoch: 5| Step: 10
Training loss: 3.099832534790039
Validation loss: 2.887684888737176

Epoch: 59| Step: 0
Training loss: 2.4926657676696777
Validation loss: 2.886671063720539

Epoch: 5| Step: 1
Training loss: 2.9801864624023438
Validation loss: 2.8842540530748266

Epoch: 5| Step: 2
Training loss: 3.0870697498321533
Validation loss: 2.8857207452097247

Epoch: 5| Step: 3
Training loss: 2.9282948970794678
Validation loss: 2.8909626673626643

Epoch: 5| Step: 4
Training loss: 2.975954532623291
Validation loss: 2.8876630260098364

Epoch: 5| Step: 5
Training loss: 4.05288028717041
Validation loss: 2.8853698827887095

Epoch: 5| Step: 6
Training loss: 2.7863752841949463
Validation loss: 2.889998666701778

Epoch: 5| Step: 7
Training loss: 2.5957751274108887
Validation loss: 2.8946104588047152

Epoch: 5| Step: 8
Training loss: 2.5165159702301025
Validation loss: 2.8908556404934136

Epoch: 5| Step: 9
Training loss: 3.0738346576690674
Validation loss: 2.8886374478699057

Epoch: 5| Step: 10
Training loss: 3.33909010887146
Validation loss: 2.8945801642633255

Epoch: 60| Step: 0
Training loss: 2.812654733657837
Validation loss: 2.8964977854041645

Epoch: 5| Step: 1
Training loss: 2.4691929817199707
Validation loss: 2.888506381742416

Epoch: 5| Step: 2
Training loss: 3.50639009475708
Validation loss: 2.882288453399494

Epoch: 5| Step: 3
Training loss: 3.550992488861084
Validation loss: 2.8818387882683867

Epoch: 5| Step: 4
Training loss: 3.1224396228790283
Validation loss: 2.8748686493083997

Epoch: 5| Step: 5
Training loss: 2.7781505584716797
Validation loss: 2.874965326760405

Epoch: 5| Step: 6
Training loss: 2.9825234413146973
Validation loss: 2.878338885563676

Epoch: 5| Step: 7
Training loss: 2.6348202228546143
Validation loss: 2.876689141796481

Epoch: 5| Step: 8
Training loss: 2.752101182937622
Validation loss: 2.876657544925649

Epoch: 5| Step: 9
Training loss: 2.5557451248168945
Validation loss: 2.8791752335845784

Epoch: 5| Step: 10
Training loss: 3.588395118713379
Validation loss: 2.8843672172997588

Epoch: 61| Step: 0
Training loss: 2.9868404865264893
Validation loss: 2.8883746131773917

Epoch: 5| Step: 1
Training loss: 2.9548215866088867
Validation loss: 2.8842119632228727

Epoch: 5| Step: 2
Training loss: 2.798107862472534
Validation loss: 2.886851869603639

Epoch: 5| Step: 3
Training loss: 2.9725589752197266
Validation loss: 2.885607619439402

Epoch: 5| Step: 4
Training loss: 2.5459980964660645
Validation loss: 2.8863727687507548

Epoch: 5| Step: 5
Training loss: 2.9876155853271484
Validation loss: 2.88322542559716

Epoch: 5| Step: 6
Training loss: 3.4246153831481934
Validation loss: 2.8812720493603776

Epoch: 5| Step: 7
Training loss: 2.7123899459838867
Validation loss: 2.881549714713968

Epoch: 5| Step: 8
Training loss: 3.6459107398986816
Validation loss: 2.8829245516048965

Epoch: 5| Step: 9
Training loss: 2.7454850673675537
Validation loss: 2.877160559418381

Epoch: 5| Step: 10
Training loss: 2.817121982574463
Validation loss: 2.876872824084374

Epoch: 62| Step: 0
Training loss: 3.0390353202819824
Validation loss: 2.871175427590647

Epoch: 5| Step: 1
Training loss: 2.9105873107910156
Validation loss: 2.8755825873344176

Epoch: 5| Step: 2
Training loss: 2.8659214973449707
Validation loss: 2.8710947036743164

Epoch: 5| Step: 3
Training loss: 3.317101240158081
Validation loss: 2.8762459985671507

Epoch: 5| Step: 4
Training loss: 2.646791696548462
Validation loss: 2.8830473910095873

Epoch: 5| Step: 5
Training loss: 3.24223256111145
Validation loss: 2.873513634486865

Epoch: 5| Step: 6
Training loss: 2.9764091968536377
Validation loss: 2.884006090061639

Epoch: 5| Step: 7
Training loss: 2.426574230194092
Validation loss: 2.8736714957862772

Epoch: 5| Step: 8
Training loss: 3.4745030403137207
Validation loss: 2.878914381868096

Epoch: 5| Step: 9
Training loss: 3.063750743865967
Validation loss: 2.8726138222602104

Epoch: 5| Step: 10
Training loss: 2.6049630641937256
Validation loss: 2.870884618451518

Epoch: 63| Step: 0
Training loss: 2.6201157569885254
Validation loss: 2.8748213911569245

Epoch: 5| Step: 1
Training loss: 3.568145275115967
Validation loss: 2.895073813776816

Epoch: 5| Step: 2
Training loss: 3.3381271362304688
Validation loss: 2.879626779146092

Epoch: 5| Step: 3
Training loss: 3.0134177207946777
Validation loss: 2.872444607878244

Epoch: 5| Step: 4
Training loss: 2.7388968467712402
Validation loss: 2.8714105980370634

Epoch: 5| Step: 5
Training loss: 3.461707353591919
Validation loss: 2.873391612883537

Epoch: 5| Step: 6
Training loss: 2.502068281173706
Validation loss: 2.873975007764755

Epoch: 5| Step: 7
Training loss: 2.917285203933716
Validation loss: 2.8710296077112996

Epoch: 5| Step: 8
Training loss: 2.3607373237609863
Validation loss: 2.885877555416476

Epoch: 5| Step: 9
Training loss: 3.137424945831299
Validation loss: 2.8764744958569928

Epoch: 5| Step: 10
Training loss: 2.9337158203125
Validation loss: 2.872109600292739

Epoch: 64| Step: 0
Training loss: 2.9970927238464355
Validation loss: 2.8703174385973202

Epoch: 5| Step: 1
Training loss: 3.1434710025787354
Validation loss: 2.868544009424025

Epoch: 5| Step: 2
Training loss: 3.412182331085205
Validation loss: 2.8694374535673406

Epoch: 5| Step: 3
Training loss: 2.9678244590759277
Validation loss: 2.865582758380521

Epoch: 5| Step: 4
Training loss: 2.986576557159424
Validation loss: 2.8628856930681454

Epoch: 5| Step: 5
Training loss: 3.3296170234680176
Validation loss: 2.866002259715911

Epoch: 5| Step: 6
Training loss: 2.3260345458984375
Validation loss: 2.8592004519636913

Epoch: 5| Step: 7
Training loss: 2.7124714851379395
Validation loss: 2.8575985713671614

Epoch: 5| Step: 8
Training loss: 2.6087355613708496
Validation loss: 2.8603126284896687

Epoch: 5| Step: 9
Training loss: 2.7184908390045166
Validation loss: 2.8609144610743367

Epoch: 5| Step: 10
Training loss: 3.415504217147827
Validation loss: 2.8619287244735228

Epoch: 65| Step: 0
Training loss: 3.576937437057495
Validation loss: 2.864917039871216

Epoch: 5| Step: 1
Training loss: 2.643829345703125
Validation loss: 2.866056106423819

Epoch: 5| Step: 2
Training loss: 2.761119842529297
Validation loss: 2.8651735064803914

Epoch: 5| Step: 3
Training loss: 3.180608034133911
Validation loss: 2.862782770587552

Epoch: 5| Step: 4
Training loss: 2.491551160812378
Validation loss: 2.8619825378541024

Epoch: 5| Step: 5
Training loss: 2.9164202213287354
Validation loss: 2.8581516845251924

Epoch: 5| Step: 6
Training loss: 2.2812371253967285
Validation loss: 2.8502726913780294

Epoch: 5| Step: 7
Training loss: 2.6823670864105225
Validation loss: 2.8563014153511292

Epoch: 5| Step: 8
Training loss: 3.178532361984253
Validation loss: 2.8573321603959605

Epoch: 5| Step: 9
Training loss: 2.9216489791870117
Validation loss: 2.851827831678493

Epoch: 5| Step: 10
Training loss: 4.1078362464904785
Validation loss: 2.8529030969066005

Epoch: 66| Step: 0
Training loss: 2.26739239692688
Validation loss: 2.8510129272296862

Epoch: 5| Step: 1
Training loss: 2.849226474761963
Validation loss: 2.848299495635494

Epoch: 5| Step: 2
Training loss: 3.770069122314453
Validation loss: 2.8507394201012066

Epoch: 5| Step: 3
Training loss: 2.883519411087036
Validation loss: 2.8496025070067375

Epoch: 5| Step: 4
Training loss: 2.7481226921081543
Validation loss: 2.8529446996668333

Epoch: 5| Step: 5
Training loss: 2.9903721809387207
Validation loss: 2.8517324514286493

Epoch: 5| Step: 6
Training loss: 3.371938705444336
Validation loss: 2.8526366756808375

Epoch: 5| Step: 7
Training loss: 3.5141549110412598
Validation loss: 2.8581413069079

Epoch: 5| Step: 8
Training loss: 2.765561103820801
Validation loss: 2.8533864764757055

Epoch: 5| Step: 9
Training loss: 2.5313122272491455
Validation loss: 2.8559958011873308

Epoch: 5| Step: 10
Training loss: 2.717467784881592
Validation loss: 2.852763278509981

Epoch: 67| Step: 0
Training loss: 3.106611728668213
Validation loss: 2.8502141660259617

Epoch: 5| Step: 1
Training loss: 2.629711627960205
Validation loss: 2.850260998613091

Epoch: 5| Step: 2
Training loss: 2.8674709796905518
Validation loss: 2.84848109111991

Epoch: 5| Step: 3
Training loss: 3.683274507522583
Validation loss: 2.8461904166847147

Epoch: 5| Step: 4
Training loss: 3.2733688354492188
Validation loss: 2.845388566294024

Epoch: 5| Step: 5
Training loss: 2.646695852279663
Validation loss: 2.840548979338779

Epoch: 5| Step: 6
Training loss: 2.3924479484558105
Validation loss: 2.843743565262005

Epoch: 5| Step: 7
Training loss: 2.499596118927002
Validation loss: 2.846977713287518

Epoch: 5| Step: 8
Training loss: 2.9597105979919434
Validation loss: 2.845533706808603

Epoch: 5| Step: 9
Training loss: 3.680469512939453
Validation loss: 2.8385035273849324

Epoch: 5| Step: 10
Training loss: 2.599334478378296
Validation loss: 2.835195285017772

Epoch: 68| Step: 0
Training loss: 3.5217807292938232
Validation loss: 2.8371713315286944

Epoch: 5| Step: 1
Training loss: 2.720546245574951
Validation loss: 2.837324624420494

Epoch: 5| Step: 2
Training loss: 2.837705135345459
Validation loss: 2.8411336996222056

Epoch: 5| Step: 3
Training loss: 2.946787118911743
Validation loss: 2.842667264323081

Epoch: 5| Step: 4
Training loss: 2.6176114082336426
Validation loss: 2.8415613046256443

Epoch: 5| Step: 5
Training loss: 3.1969046592712402
Validation loss: 2.8465854660157235

Epoch: 5| Step: 6
Training loss: 2.3268327713012695
Validation loss: 2.8453960239246325

Epoch: 5| Step: 7
Training loss: 2.676971673965454
Validation loss: 2.84117373599801

Epoch: 5| Step: 8
Training loss: 2.9488327503204346
Validation loss: 2.840261600350821

Epoch: 5| Step: 9
Training loss: 3.6165242195129395
Validation loss: 2.839210287217171

Epoch: 5| Step: 10
Training loss: 2.955920934677124
Validation loss: 2.8384526545001614

Epoch: 69| Step: 0
Training loss: 2.487091541290283
Validation loss: 2.8349132537841797

Epoch: 5| Step: 1
Training loss: 2.5423383712768555
Validation loss: 2.83483317846893

Epoch: 5| Step: 2
Training loss: 2.9707789421081543
Validation loss: 2.8344984746748403

Epoch: 5| Step: 3
Training loss: 2.246966600418091
Validation loss: 2.8365398991492485

Epoch: 5| Step: 4
Training loss: 3.5341217517852783
Validation loss: 2.8387702152293217

Epoch: 5| Step: 5
Training loss: 3.394881010055542
Validation loss: 2.8390916547467633

Epoch: 5| Step: 6
Training loss: 3.739873170852661
Validation loss: 2.839641709481516

Epoch: 5| Step: 7
Training loss: 2.19132661819458
Validation loss: 2.83508554838037

Epoch: 5| Step: 8
Training loss: 2.632295608520508
Validation loss: 2.8348978847585697

Epoch: 5| Step: 9
Training loss: 3.082205295562744
Validation loss: 2.83013932166561

Epoch: 5| Step: 10
Training loss: 3.622476577758789
Validation loss: 2.829915008237285

Epoch: 70| Step: 0
Training loss: 3.000779628753662
Validation loss: 2.8280556125025593

Epoch: 5| Step: 1
Training loss: 3.2770447731018066
Validation loss: 2.8299352404891804

Epoch: 5| Step: 2
Training loss: 2.3012337684631348
Validation loss: 2.829703110520558

Epoch: 5| Step: 3
Training loss: 2.5128257274627686
Validation loss: 2.828843849961476

Epoch: 5| Step: 4
Training loss: 3.212451457977295
Validation loss: 2.828463526182277

Epoch: 5| Step: 5
Training loss: 3.2770962715148926
Validation loss: 2.8289267991178777

Epoch: 5| Step: 6
Training loss: 3.109494686126709
Validation loss: 2.8274379058550765

Epoch: 5| Step: 7
Training loss: 2.872429132461548
Validation loss: 2.8266869924401723

Epoch: 5| Step: 8
Training loss: 2.6243362426757812
Validation loss: 2.8305389727315595

Epoch: 5| Step: 9
Training loss: 2.896444797515869
Validation loss: 2.826298741884129

Epoch: 5| Step: 10
Training loss: 3.206838607788086
Validation loss: 2.8265904662429646

Epoch: 71| Step: 0
Training loss: 3.011664628982544
Validation loss: 2.8253670072042816

Epoch: 5| Step: 1
Training loss: 2.3202643394470215
Validation loss: 2.826147251231696

Epoch: 5| Step: 2
Training loss: 2.579036235809326
Validation loss: 2.8249314062057005

Epoch: 5| Step: 3
Training loss: 3.844181537628174
Validation loss: 2.8273436792435183

Epoch: 5| Step: 4
Training loss: 2.9050517082214355
Validation loss: 2.822220858707223

Epoch: 5| Step: 5
Training loss: 2.224229335784912
Validation loss: 2.823569154226652

Epoch: 5| Step: 6
Training loss: 3.286202907562256
Validation loss: 2.82429523621836

Epoch: 5| Step: 7
Training loss: 3.2405714988708496
Validation loss: 2.8224559291716544

Epoch: 5| Step: 8
Training loss: 3.128248929977417
Validation loss: 2.825788736343384

Epoch: 5| Step: 9
Training loss: 3.065535306930542
Validation loss: 2.823164642498057

Epoch: 5| Step: 10
Training loss: 2.6062171459198
Validation loss: 2.8270572052207044

Epoch: 72| Step: 0
Training loss: 3.36403226852417
Validation loss: 2.836598147628128

Epoch: 5| Step: 1
Training loss: 3.7177634239196777
Validation loss: 2.844937996197772

Epoch: 5| Step: 2
Training loss: 3.0068178176879883
Validation loss: 2.834986468797089

Epoch: 5| Step: 3
Training loss: 2.744431257247925
Validation loss: 2.82630749928054

Epoch: 5| Step: 4
Training loss: 2.677337646484375
Validation loss: 2.817138284765264

Epoch: 5| Step: 5
Training loss: 2.3934407234191895
Validation loss: 2.817184140605311

Epoch: 5| Step: 6
Training loss: 2.168149471282959
Validation loss: 2.8177277734202724

Epoch: 5| Step: 7
Training loss: 3.391810655593872
Validation loss: 2.818350448403307

Epoch: 5| Step: 8
Training loss: 3.1605608463287354
Validation loss: 2.816275670964231

Epoch: 5| Step: 9
Training loss: 2.6765074729919434
Validation loss: 2.812997776974914

Epoch: 5| Step: 10
Training loss: 2.927541732788086
Validation loss: 2.8182920050877396

Epoch: 73| Step: 0
Training loss: 2.797389507293701
Validation loss: 2.821277836317657

Epoch: 5| Step: 1
Training loss: 2.2557666301727295
Validation loss: 2.831778367360433

Epoch: 5| Step: 2
Training loss: 2.742145538330078
Validation loss: 2.8394792028652724

Epoch: 5| Step: 3
Training loss: 3.4906506538391113
Validation loss: 2.8574335523830947

Epoch: 5| Step: 4
Training loss: 3.4726722240448
Validation loss: 2.8849865287862797

Epoch: 5| Step: 5
Training loss: 3.0221505165100098
Validation loss: 2.8518781021077144

Epoch: 5| Step: 6
Training loss: 3.0031962394714355
Validation loss: 2.8322311703876784

Epoch: 5| Step: 7
Training loss: 3.450857162475586
Validation loss: 2.8224559214807328

Epoch: 5| Step: 8
Training loss: 2.903735399246216
Validation loss: 2.8227377963322464

Epoch: 5| Step: 9
Training loss: 2.4739174842834473
Validation loss: 2.822470870069278

Epoch: 5| Step: 10
Training loss: 2.5983903408050537
Validation loss: 2.827042051540908

Epoch: 74| Step: 0
Training loss: 3.285446882247925
Validation loss: 2.8249644976790234

Epoch: 5| Step: 1
Training loss: 3.265239715576172
Validation loss: 2.817115501690936

Epoch: 5| Step: 2
Training loss: 3.529330015182495
Validation loss: 2.819736337149015

Epoch: 5| Step: 3
Training loss: 3.037177562713623
Validation loss: 2.816705785771852

Epoch: 5| Step: 4
Training loss: 2.427461862564087
Validation loss: 2.813698948070567

Epoch: 5| Step: 5
Training loss: 3.7220711708068848
Validation loss: 2.8167326193983837

Epoch: 5| Step: 6
Training loss: 2.814901113510132
Validation loss: 2.8174226232754287

Epoch: 5| Step: 7
Training loss: 2.2064156532287598
Validation loss: 2.812096477836691

Epoch: 5| Step: 8
Training loss: 2.5028750896453857
Validation loss: 2.813251156960764

Epoch: 5| Step: 9
Training loss: 2.8263208866119385
Validation loss: 2.808412769789337

Epoch: 5| Step: 10
Training loss: 2.544614315032959
Validation loss: 2.8074988088300152

Epoch: 75| Step: 0
Training loss: 2.76875376701355
Validation loss: 2.807889981936383

Epoch: 5| Step: 1
Training loss: 2.8887381553649902
Validation loss: 2.8085732742022445

Epoch: 5| Step: 2
Training loss: 3.5548019409179688
Validation loss: 2.8065761391834547

Epoch: 5| Step: 3
Training loss: 2.7663350105285645
Validation loss: 2.8061090643687914

Epoch: 5| Step: 4
Training loss: 2.560136318206787
Validation loss: 2.80704503931025

Epoch: 5| Step: 5
Training loss: 2.977207660675049
Validation loss: 2.8045170666069112

Epoch: 5| Step: 6
Training loss: 2.993040084838867
Validation loss: 2.8074192026610016

Epoch: 5| Step: 7
Training loss: 3.262134552001953
Validation loss: 2.8031231126477643

Epoch: 5| Step: 8
Training loss: 2.540616512298584
Validation loss: 2.808481336921774

Epoch: 5| Step: 9
Training loss: 2.7611067295074463
Validation loss: 2.8041027258801203

Epoch: 5| Step: 10
Training loss: 3.1003663539886475
Validation loss: 2.8050649678835304

Epoch: 76| Step: 0
Training loss: 2.6764817237854004
Validation loss: 2.8053781883690947

Epoch: 5| Step: 1
Training loss: 2.1895649433135986
Validation loss: 2.804515956550516

Epoch: 5| Step: 2
Training loss: 2.5933098793029785
Validation loss: 2.804527077623593

Epoch: 5| Step: 3
Training loss: 3.622303009033203
Validation loss: 2.810727462973646

Epoch: 5| Step: 4
Training loss: 3.3575425148010254
Validation loss: 2.8053383545209

Epoch: 5| Step: 5
Training loss: 2.5343291759490967
Validation loss: 2.8078886898615028

Epoch: 5| Step: 6
Training loss: 2.7855653762817383
Validation loss: 2.8094320963787776

Epoch: 5| Step: 7
Training loss: 3.212003231048584
Validation loss: 2.810076839180403

Epoch: 5| Step: 8
Training loss: 2.774733781814575
Validation loss: 2.8067597727621756

Epoch: 5| Step: 9
Training loss: 3.444039821624756
Validation loss: 2.80713237485578

Epoch: 5| Step: 10
Training loss: 2.893592119216919
Validation loss: 2.8004827883935746

Epoch: 77| Step: 0
Training loss: 3.1516525745391846
Validation loss: 2.8016850794515302

Epoch: 5| Step: 1
Training loss: 1.8497978448867798
Validation loss: 2.798731562911823

Epoch: 5| Step: 2
Training loss: 3.437778949737549
Validation loss: 2.7979194169403403

Epoch: 5| Step: 3
Training loss: 3.5140788555145264
Validation loss: 2.796786772307529

Epoch: 5| Step: 4
Training loss: 2.4308040142059326
Validation loss: 2.80020894030089

Epoch: 5| Step: 5
Training loss: 3.248883008956909
Validation loss: 2.797629092329292

Epoch: 5| Step: 6
Training loss: 3.0270259380340576
Validation loss: 2.8020866686298

Epoch: 5| Step: 7
Training loss: 3.102473258972168
Validation loss: 2.7975519190552416

Epoch: 5| Step: 8
Training loss: 2.602937936782837
Validation loss: 2.798273560821369

Epoch: 5| Step: 9
Training loss: 3.3079116344451904
Validation loss: 2.798897043351204

Epoch: 5| Step: 10
Training loss: 2.3396599292755127
Validation loss: 2.7928569368136826

Epoch: 78| Step: 0
Training loss: 2.9163150787353516
Validation loss: 2.7953576580170663

Epoch: 5| Step: 1
Training loss: 3.044330596923828
Validation loss: 2.79703014896762

Epoch: 5| Step: 2
Training loss: 2.2545394897460938
Validation loss: 2.799826850173294

Epoch: 5| Step: 3
Training loss: 2.9862680435180664
Validation loss: 2.799842837036297

Epoch: 5| Step: 4
Training loss: 2.950277328491211
Validation loss: 2.8001662197933403

Epoch: 5| Step: 5
Training loss: 3.3489227294921875
Validation loss: 2.8094838370559034

Epoch: 5| Step: 6
Training loss: 3.5558876991271973
Validation loss: 2.807626767825055

Epoch: 5| Step: 7
Training loss: 2.9534714221954346
Validation loss: 2.8080067788400958

Epoch: 5| Step: 8
Training loss: 2.9335105419158936
Validation loss: 2.798636959445092

Epoch: 5| Step: 9
Training loss: 2.1700541973114014
Validation loss: 2.7969517861643145

Epoch: 5| Step: 10
Training loss: 2.9468486309051514
Validation loss: 2.7930020952737458

Epoch: 79| Step: 0
Training loss: 3.01535964012146
Validation loss: 2.792099762988347

Epoch: 5| Step: 1
Training loss: 2.91618275642395
Validation loss: 2.794421306220434

Epoch: 5| Step: 2
Training loss: 2.396367073059082
Validation loss: 2.791687427028533

Epoch: 5| Step: 3
Training loss: 3.164088487625122
Validation loss: 2.7917587193109656

Epoch: 5| Step: 4
Training loss: 3.0340523719787598
Validation loss: 2.7968240117514007

Epoch: 5| Step: 5
Training loss: 3.163447618484497
Validation loss: 2.794760088766775

Epoch: 5| Step: 6
Training loss: 3.0403919219970703
Validation loss: 2.794821472578151

Epoch: 5| Step: 7
Training loss: 3.1791510581970215
Validation loss: 2.796017546807566

Epoch: 5| Step: 8
Training loss: 2.7187395095825195
Validation loss: 2.794838002932969

Epoch: 5| Step: 9
Training loss: 2.5825467109680176
Validation loss: 2.792780558268229

Epoch: 5| Step: 10
Training loss: 2.772094964981079
Validation loss: 2.7939909145396244

Epoch: 80| Step: 0
Training loss: 2.931211471557617
Validation loss: 2.7917476905289518

Epoch: 5| Step: 1
Training loss: 2.8134868144989014
Validation loss: 2.797076033007714

Epoch: 5| Step: 2
Training loss: 3.181610107421875
Validation loss: 2.7921987477169243

Epoch: 5| Step: 3
Training loss: 3.520885944366455
Validation loss: 2.7947001687942015

Epoch: 5| Step: 4
Training loss: 2.5030770301818848
Validation loss: 2.791465531113327

Epoch: 5| Step: 5
Training loss: 2.9834556579589844
Validation loss: 2.7914532461474018

Epoch: 5| Step: 6
Training loss: 2.6050734519958496
Validation loss: 2.7913038756257746

Epoch: 5| Step: 7
Training loss: 2.2388174533843994
Validation loss: 2.790122829457765

Epoch: 5| Step: 8
Training loss: 3.187868118286133
Validation loss: 2.788859810880435

Epoch: 5| Step: 9
Training loss: 2.7330033779144287
Validation loss: 2.790571374277915

Epoch: 5| Step: 10
Training loss: 3.3711326122283936
Validation loss: 2.7892752898636686

Epoch: 81| Step: 0
Training loss: 3.095209836959839
Validation loss: 2.790973812021235

Epoch: 5| Step: 1
Training loss: 2.5419516563415527
Validation loss: 2.7880600242204565

Epoch: 5| Step: 2
Training loss: 3.8451457023620605
Validation loss: 2.7911547409590853

Epoch: 5| Step: 3
Training loss: 2.7419419288635254
Validation loss: 2.7904164252742643

Epoch: 5| Step: 4
Training loss: 3.196566104888916
Validation loss: 2.7875233952717116

Epoch: 5| Step: 5
Training loss: 2.143432855606079
Validation loss: 2.7902229114245345

Epoch: 5| Step: 6
Training loss: 3.3053901195526123
Validation loss: 2.7890539810221684

Epoch: 5| Step: 7
Training loss: 2.8445284366607666
Validation loss: 2.7876784391300653

Epoch: 5| Step: 8
Training loss: 2.316481828689575
Validation loss: 2.789341688156128

Epoch: 5| Step: 9
Training loss: 2.9487178325653076
Validation loss: 2.788226253242903

Epoch: 5| Step: 10
Training loss: 2.951219081878662
Validation loss: 2.7880663256491385

Epoch: 82| Step: 0
Training loss: 2.674356698989868
Validation loss: 2.788386344909668

Epoch: 5| Step: 1
Training loss: 3.3022849559783936
Validation loss: 2.7911299761905464

Epoch: 5| Step: 2
Training loss: 2.9912960529327393
Validation loss: 2.7887556373432116

Epoch: 5| Step: 3
Training loss: 3.4157347679138184
Validation loss: 2.7869597737507155

Epoch: 5| Step: 4
Training loss: 2.927300214767456
Validation loss: 2.788231667651925

Epoch: 5| Step: 5
Training loss: 2.821155548095703
Validation loss: 2.7866517574556413

Epoch: 5| Step: 6
Training loss: 2.624058485031128
Validation loss: 2.784986675426524

Epoch: 5| Step: 7
Training loss: 3.2006173133850098
Validation loss: 2.784830331802368

Epoch: 5| Step: 8
Training loss: 2.7668776512145996
Validation loss: 2.787594941354567

Epoch: 5| Step: 9
Training loss: 2.9404125213623047
Validation loss: 2.783315894424274

Epoch: 5| Step: 10
Training loss: 2.1655025482177734
Validation loss: 2.786613771992345

Epoch: 83| Step: 0
Training loss: 2.765840530395508
Validation loss: 2.787169761555169

Epoch: 5| Step: 1
Training loss: 2.7230141162872314
Validation loss: 2.784562426228677

Epoch: 5| Step: 2
Training loss: 2.73417329788208
Validation loss: 2.7831723561850925

Epoch: 5| Step: 3
Training loss: 3.435016632080078
Validation loss: 2.7848128657187186

Epoch: 5| Step: 4
Training loss: 2.4563653469085693
Validation loss: 2.7845871858699347

Epoch: 5| Step: 5
Training loss: 3.445378065109253
Validation loss: 2.7833411078299246

Epoch: 5| Step: 6
Training loss: 2.9198460578918457
Validation loss: 2.7790127723447737

Epoch: 5| Step: 7
Training loss: 2.6309845447540283
Validation loss: 2.778732958660331

Epoch: 5| Step: 8
Training loss: 2.0704755783081055
Validation loss: 2.7820178103703324

Epoch: 5| Step: 9
Training loss: 3.410463809967041
Validation loss: 2.7816241274597826

Epoch: 5| Step: 10
Training loss: 3.384705066680908
Validation loss: 2.781089469950686

Epoch: 84| Step: 0
Training loss: 2.262852668762207
Validation loss: 2.783186581826979

Epoch: 5| Step: 1
Training loss: 2.6583855152130127
Validation loss: 2.77774558016049

Epoch: 5| Step: 2
Training loss: 2.402106285095215
Validation loss: 2.7796477220391713

Epoch: 5| Step: 3
Training loss: 2.7052643299102783
Validation loss: 2.7780644355281705

Epoch: 5| Step: 4
Training loss: 3.4500961303710938
Validation loss: 2.779893772576445

Epoch: 5| Step: 5
Training loss: 2.628441572189331
Validation loss: 2.778099085695

Epoch: 5| Step: 6
Training loss: 3.641390562057495
Validation loss: 2.7766800952214066

Epoch: 5| Step: 7
Training loss: 3.35020112991333
Validation loss: 2.7783796223261024

Epoch: 5| Step: 8
Training loss: 2.6536033153533936
Validation loss: 2.7771628595167592

Epoch: 5| Step: 9
Training loss: 2.8384697437286377
Validation loss: 2.7798060627393824

Epoch: 5| Step: 10
Training loss: 3.3823423385620117
Validation loss: 2.780267192471412

Epoch: 85| Step: 0
Training loss: 3.0599732398986816
Validation loss: 2.7782504558563232

Epoch: 5| Step: 1
Training loss: 2.8294503688812256
Validation loss: 2.778874853605865

Epoch: 5| Step: 2
Training loss: 3.3150603771209717
Validation loss: 2.780301801619991

Epoch: 5| Step: 3
Training loss: 2.5948586463928223
Validation loss: 2.7772127492453462

Epoch: 5| Step: 4
Training loss: 2.9261622428894043
Validation loss: 2.7775253249752905

Epoch: 5| Step: 5
Training loss: 3.2999911308288574
Validation loss: 2.7777338720137075

Epoch: 5| Step: 6
Training loss: 2.5939431190490723
Validation loss: 2.7739267310788556

Epoch: 5| Step: 7
Training loss: 2.4316391944885254
Validation loss: 2.780090593522595

Epoch: 5| Step: 8
Training loss: 2.335010528564453
Validation loss: 2.7779807813705935

Epoch: 5| Step: 9
Training loss: 3.3243796825408936
Validation loss: 2.779768407985728

Epoch: 5| Step: 10
Training loss: 3.1723434925079346
Validation loss: 2.782625041982179

Epoch: 86| Step: 0
Training loss: 2.792285442352295
Validation loss: 2.7789944089869016

Epoch: 5| Step: 1
Training loss: 3.3880958557128906
Validation loss: 2.7816265834275113

Epoch: 5| Step: 2
Training loss: 2.861856698989868
Validation loss: 2.7783049178379837

Epoch: 5| Step: 3
Training loss: 3.289419651031494
Validation loss: 2.7806530742235083

Epoch: 5| Step: 4
Training loss: 3.373854875564575
Validation loss: 2.783349408898302

Epoch: 5| Step: 5
Training loss: 2.2787137031555176
Validation loss: 2.7860854107846498

Epoch: 5| Step: 6
Training loss: 2.4805870056152344
Validation loss: 2.778940531515306

Epoch: 5| Step: 7
Training loss: 2.6950488090515137
Validation loss: 2.7856168157310894

Epoch: 5| Step: 8
Training loss: 3.3939871788024902
Validation loss: 2.792376882286482

Epoch: 5| Step: 9
Training loss: 3.430499315261841
Validation loss: 2.787104839919716

Epoch: 5| Step: 10
Training loss: 1.717759370803833
Validation loss: 2.7815118451272287

Epoch: 87| Step: 0
Training loss: 2.491312026977539
Validation loss: 2.779750911138391

Epoch: 5| Step: 1
Training loss: 2.5272793769836426
Validation loss: 2.7757085369479273

Epoch: 5| Step: 2
Training loss: 3.9891903400421143
Validation loss: 2.7770247126138337

Epoch: 5| Step: 3
Training loss: 3.3322880268096924
Validation loss: 2.774248884570214

Epoch: 5| Step: 4
Training loss: 2.435711622238159
Validation loss: 2.7773771337283555

Epoch: 5| Step: 5
Training loss: 2.39146089553833
Validation loss: 2.7740373765268633

Epoch: 5| Step: 6
Training loss: 3.192444324493408
Validation loss: 2.7759798854909916

Epoch: 5| Step: 7
Training loss: 3.120913028717041
Validation loss: 2.770583603971748

Epoch: 5| Step: 8
Training loss: 2.472569704055786
Validation loss: 2.7736056491892827

Epoch: 5| Step: 9
Training loss: 2.951240062713623
Validation loss: 2.7732686253004175

Epoch: 5| Step: 10
Training loss: 2.952723979949951
Validation loss: 2.779631268593573

Epoch: 88| Step: 0
Training loss: 2.858578681945801
Validation loss: 2.7786002133482244

Epoch: 5| Step: 1
Training loss: 2.964538097381592
Validation loss: 2.786813574452554

Epoch: 5| Step: 2
Training loss: 3.643552303314209
Validation loss: 2.809636333937286

Epoch: 5| Step: 3
Training loss: 3.082432270050049
Validation loss: 2.793846071407359

Epoch: 5| Step: 4
Training loss: 2.368098020553589
Validation loss: 2.780627686490295

Epoch: 5| Step: 5
Training loss: 3.359260082244873
Validation loss: 2.7730885244184926

Epoch: 5| Step: 6
Training loss: 3.398113965988159
Validation loss: 2.772575475836313

Epoch: 5| Step: 7
Training loss: 2.674992084503174
Validation loss: 2.773473867806055

Epoch: 5| Step: 8
Training loss: 2.769692897796631
Validation loss: 2.7726658005868234

Epoch: 5| Step: 9
Training loss: 2.572084665298462
Validation loss: 2.772605408904373

Epoch: 5| Step: 10
Training loss: 2.0098278522491455
Validation loss: 2.771631104971773

Epoch: 89| Step: 0
Training loss: 1.6372363567352295
Validation loss: 2.7679634171147502

Epoch: 5| Step: 1
Training loss: 2.903322219848633
Validation loss: 2.778565796472693

Epoch: 5| Step: 2
Training loss: 2.736100673675537
Validation loss: 2.7758018227033716

Epoch: 5| Step: 3
Training loss: 2.5814337730407715
Validation loss: 2.7716934552756687

Epoch: 5| Step: 4
Training loss: 2.9571375846862793
Validation loss: 2.774975376744424

Epoch: 5| Step: 5
Training loss: 3.700329542160034
Validation loss: 2.7742565985648864

Epoch: 5| Step: 6
Training loss: 3.3538684844970703
Validation loss: 2.773976590043755

Epoch: 5| Step: 7
Training loss: 2.3904051780700684
Validation loss: 2.769160411691153

Epoch: 5| Step: 8
Training loss: 2.710170030593872
Validation loss: 2.7687431612322406

Epoch: 5| Step: 9
Training loss: 2.9913578033447266
Validation loss: 2.7707120090402584

Epoch: 5| Step: 10
Training loss: 3.965310573577881
Validation loss: 2.768209459961102

Epoch: 90| Step: 0
Training loss: 2.9212207794189453
Validation loss: 2.772317135205833

Epoch: 5| Step: 1
Training loss: 4.0724077224731445
Validation loss: 2.7707685347526305

Epoch: 5| Step: 2
Training loss: 2.4878408908843994
Validation loss: 2.7688803262608026

Epoch: 5| Step: 3
Training loss: 2.9827499389648438
Validation loss: 2.771006061184791

Epoch: 5| Step: 4
Training loss: 2.2735037803649902
Validation loss: 2.7721195784948205

Epoch: 5| Step: 5
Training loss: 3.4259204864501953
Validation loss: 2.7711525476107033

Epoch: 5| Step: 6
Training loss: 2.079216241836548
Validation loss: 2.76848848917151

Epoch: 5| Step: 7
Training loss: 2.302344560623169
Validation loss: 2.764173666636149

Epoch: 5| Step: 8
Training loss: 3.291403293609619
Validation loss: 2.76574525525493

Epoch: 5| Step: 9
Training loss: 2.6648082733154297
Validation loss: 2.760873076736286

Epoch: 5| Step: 10
Training loss: 3.3589229583740234
Validation loss: 2.765777828872845

Epoch: 91| Step: 0
Training loss: 2.9552505016326904
Validation loss: 2.76481968356717

Epoch: 5| Step: 1
Training loss: 2.3957767486572266
Validation loss: 2.766793061328191

Epoch: 5| Step: 2
Training loss: 3.3400025367736816
Validation loss: 2.763625539759154

Epoch: 5| Step: 3
Training loss: 2.6851556301116943
Validation loss: 2.7654156069601736

Epoch: 5| Step: 4
Training loss: 3.00933837890625
Validation loss: 2.761347083635228

Epoch: 5| Step: 5
Training loss: 2.9922537803649902
Validation loss: 2.76738295760206

Epoch: 5| Step: 6
Training loss: 3.004107713699341
Validation loss: 2.765778103182393

Epoch: 5| Step: 7
Training loss: 2.7901554107666016
Validation loss: 2.7642248522850776

Epoch: 5| Step: 8
Training loss: 3.121955394744873
Validation loss: 2.761110310913414

Epoch: 5| Step: 9
Training loss: 2.4972126483917236
Validation loss: 2.7626186955359673

Epoch: 5| Step: 10
Training loss: 2.9886786937713623
Validation loss: 2.7615018506203928

Epoch: 92| Step: 0
Training loss: 2.3632616996765137
Validation loss: 2.7605935142886255

Epoch: 5| Step: 1
Training loss: 3.2750697135925293
Validation loss: 2.7657949719377743

Epoch: 5| Step: 2
Training loss: 2.5466134548187256
Validation loss: 2.764607357722457

Epoch: 5| Step: 3
Training loss: 3.1078007221221924
Validation loss: 2.7654061112352597

Epoch: 5| Step: 4
Training loss: 3.2892203330993652
Validation loss: 2.7647534903659614

Epoch: 5| Step: 5
Training loss: 2.961592197418213
Validation loss: 2.7631798815983597

Epoch: 5| Step: 6
Training loss: 2.812246799468994
Validation loss: 2.7598515889977895

Epoch: 5| Step: 7
Training loss: 2.3762831687927246
Validation loss: 2.7606609713646675

Epoch: 5| Step: 8
Training loss: 2.9065933227539062
Validation loss: 2.7601905971445064

Epoch: 5| Step: 9
Training loss: 2.735257387161255
Validation loss: 2.7584226490348898

Epoch: 5| Step: 10
Training loss: 3.449575424194336
Validation loss: 2.763979455476166

Epoch: 93| Step: 0
Training loss: 3.130025863647461
Validation loss: 2.758707618200651

Epoch: 5| Step: 1
Training loss: 2.748778820037842
Validation loss: 2.7581609859261462

Epoch: 5| Step: 2
Training loss: 2.8226912021636963
Validation loss: 2.762583624932074

Epoch: 5| Step: 3
Training loss: 3.0859203338623047
Validation loss: 2.76050204102711

Epoch: 5| Step: 4
Training loss: 2.161907911300659
Validation loss: 2.7633767538173224

Epoch: 5| Step: 5
Training loss: 3.2115445137023926
Validation loss: 2.762336141319685

Epoch: 5| Step: 6
Training loss: 2.9304022789001465
Validation loss: 2.766656114209083

Epoch: 5| Step: 7
Training loss: 3.307675838470459
Validation loss: 2.7746602822375555

Epoch: 5| Step: 8
Training loss: 2.733360767364502
Validation loss: 2.7726068829977386

Epoch: 5| Step: 9
Training loss: 2.8132827281951904
Validation loss: 2.766680778995637

Epoch: 5| Step: 10
Training loss: 2.7776317596435547
Validation loss: 2.764639578839784

Epoch: 94| Step: 0
Training loss: 2.8805720806121826
Validation loss: 2.758489303691413

Epoch: 5| Step: 1
Training loss: 2.9165778160095215
Validation loss: 2.764362155750234

Epoch: 5| Step: 2
Training loss: 2.587437391281128
Validation loss: 2.764307391258978

Epoch: 5| Step: 3
Training loss: 3.32977032661438
Validation loss: 2.7589154986925024

Epoch: 5| Step: 4
Training loss: 2.939405918121338
Validation loss: 2.761210641553325

Epoch: 5| Step: 5
Training loss: 2.5764307975769043
Validation loss: 2.7682575128411733

Epoch: 5| Step: 6
Training loss: 2.940859079360962
Validation loss: 2.7692276406031784

Epoch: 5| Step: 7
Training loss: 3.312312364578247
Validation loss: 2.7689143124447075

Epoch: 5| Step: 8
Training loss: 2.485429525375366
Validation loss: 2.770887569714618

Epoch: 5| Step: 9
Training loss: 3.2853214740753174
Validation loss: 2.7628540787645566

Epoch: 5| Step: 10
Training loss: 2.3729684352874756
Validation loss: 2.7626879163967666

Epoch: 95| Step: 0
Training loss: 2.6461219787597656
Validation loss: 2.7609437383631223

Epoch: 5| Step: 1
Training loss: 3.137233257293701
Validation loss: 2.7577619347521054

Epoch: 5| Step: 2
Training loss: 2.838921308517456
Validation loss: 2.7553551248324815

Epoch: 5| Step: 3
Training loss: 2.8145546913146973
Validation loss: 2.7515804177971295

Epoch: 5| Step: 4
Training loss: 2.8768811225891113
Validation loss: 2.7560128242738786

Epoch: 5| Step: 5
Training loss: 2.8648533821105957
Validation loss: 2.7610736175249984

Epoch: 5| Step: 6
Training loss: 3.5333895683288574
Validation loss: 2.762109774415211

Epoch: 5| Step: 7
Training loss: 2.7073092460632324
Validation loss: 2.76887789592948

Epoch: 5| Step: 8
Training loss: 2.667886734008789
Validation loss: 2.767552460393598

Epoch: 5| Step: 9
Training loss: 2.9112095832824707
Validation loss: 2.7548254330952964

Epoch: 5| Step: 10
Training loss: 2.7280330657958984
Validation loss: 2.752370301113334

Epoch: 96| Step: 0
Training loss: 3.0627191066741943
Validation loss: 2.755869993599512

Epoch: 5| Step: 1
Training loss: 3.1515579223632812
Validation loss: 2.7551167011260986

Epoch: 5| Step: 2
Training loss: 2.453624725341797
Validation loss: 2.7621244358760055

Epoch: 5| Step: 3
Training loss: 2.6040163040161133
Validation loss: 2.7673686319781887

Epoch: 5| Step: 4
Training loss: 2.7413489818573
Validation loss: 2.770090482568228

Epoch: 5| Step: 5
Training loss: 3.6657023429870605
Validation loss: 2.7724355113121772

Epoch: 5| Step: 6
Training loss: 2.8302464485168457
Validation loss: 2.7707407884700324

Epoch: 5| Step: 7
Training loss: 3.0079777240753174
Validation loss: 2.7684615094174623

Epoch: 5| Step: 8
Training loss: 2.758363723754883
Validation loss: 2.7650814287124144

Epoch: 5| Step: 9
Training loss: 2.678334951400757
Validation loss: 2.7645938575908704

Epoch: 5| Step: 10
Training loss: 2.821000099182129
Validation loss: 2.761681402883222

Epoch: 97| Step: 0
Training loss: 2.848860263824463
Validation loss: 2.756315146723101

Epoch: 5| Step: 1
Training loss: 3.8590035438537598
Validation loss: 2.7595934098766697

Epoch: 5| Step: 2
Training loss: 2.030694007873535
Validation loss: 2.7580830179234987

Epoch: 5| Step: 3
Training loss: 3.151301860809326
Validation loss: 2.7534348093053347

Epoch: 5| Step: 4
Training loss: 2.9043023586273193
Validation loss: 2.7534172560579036

Epoch: 5| Step: 5
Training loss: 2.6313278675079346
Validation loss: 2.755192966871364

Epoch: 5| Step: 6
Training loss: 3.35188627243042
Validation loss: 2.7565230554149998

Epoch: 5| Step: 7
Training loss: 2.6530144214630127
Validation loss: 2.752419117958315

Epoch: 5| Step: 8
Training loss: 2.3085129261016846
Validation loss: 2.753328405400758

Epoch: 5| Step: 9
Training loss: 3.1755847930908203
Validation loss: 2.7547014400523198

Epoch: 5| Step: 10
Training loss: 2.7294859886169434
Validation loss: 2.752539873123169

Epoch: 98| Step: 0
Training loss: 2.864619731903076
Validation loss: 2.750102843007734

Epoch: 5| Step: 1
Training loss: 3.309080123901367
Validation loss: 2.75014567118819

Epoch: 5| Step: 2
Training loss: 3.451963424682617
Validation loss: 2.747855224917012

Epoch: 5| Step: 3
Training loss: 1.9677789211273193
Validation loss: 2.7504816311661915

Epoch: 5| Step: 4
Training loss: 2.9956705570220947
Validation loss: 2.7483970965108564

Epoch: 5| Step: 5
Training loss: 2.7211527824401855
Validation loss: 2.747966525375202

Epoch: 5| Step: 6
Training loss: 3.217960834503174
Validation loss: 2.747996276424777

Epoch: 5| Step: 7
Training loss: 3.4346089363098145
Validation loss: 2.7483148190283004

Epoch: 5| Step: 8
Training loss: 2.6020328998565674
Validation loss: 2.7475624648473596

Epoch: 5| Step: 9
Training loss: 2.750169277191162
Validation loss: 2.746990285893922

Epoch: 5| Step: 10
Training loss: 2.1953113079071045
Validation loss: 2.7490448567175094

Epoch: 99| Step: 0
Training loss: 2.8135178089141846
Validation loss: 2.745346417991064

Epoch: 5| Step: 1
Training loss: 3.2653098106384277
Validation loss: 2.7460800473408034

Epoch: 5| Step: 2
Training loss: 2.7177700996398926
Validation loss: 2.7506363238057783

Epoch: 5| Step: 3
Training loss: 2.871488332748413
Validation loss: 2.7506507750480407

Epoch: 5| Step: 4
Training loss: 3.0564258098602295
Validation loss: 2.7538448533704205

Epoch: 5| Step: 5
Training loss: 2.3934881687164307
Validation loss: 2.7535515574998755

Epoch: 5| Step: 6
Training loss: 2.99640154838562
Validation loss: 2.7501975182564027

Epoch: 5| Step: 7
Training loss: 2.3890788555145264
Validation loss: 2.7532097139666156

Epoch: 5| Step: 8
Training loss: 2.977907657623291
Validation loss: 2.75407733455781

Epoch: 5| Step: 9
Training loss: 3.2707810401916504
Validation loss: 2.752347505220803

Epoch: 5| Step: 10
Training loss: 2.861414670944214
Validation loss: 2.751132011413574

Epoch: 100| Step: 0
Training loss: 2.7226924896240234
Validation loss: 2.746476414383099

Epoch: 5| Step: 1
Training loss: 2.325378656387329
Validation loss: 2.744043396365258

Epoch: 5| Step: 2
Training loss: 3.697871685028076
Validation loss: 2.7447400990352837

Epoch: 5| Step: 3
Training loss: 3.6787948608398438
Validation loss: 2.7435391103067706

Epoch: 5| Step: 4
Training loss: 2.7805984020233154
Validation loss: 2.745886887273481

Epoch: 5| Step: 5
Training loss: 2.785764217376709
Validation loss: 2.7445124451832106

Epoch: 5| Step: 6
Training loss: 2.1025795936584473
Validation loss: 2.744246126503073

Epoch: 5| Step: 7
Training loss: 2.626140832901001
Validation loss: 2.747599001853697

Epoch: 5| Step: 8
Training loss: 2.687495708465576
Validation loss: 2.7462540441943752

Epoch: 5| Step: 9
Training loss: 2.6058781147003174
Validation loss: 2.7469546256526822

Epoch: 5| Step: 10
Training loss: 3.6540045738220215
Validation loss: 2.751847856788225

Epoch: 101| Step: 0
Training loss: 2.5171287059783936
Validation loss: 2.7528138801615727

Epoch: 5| Step: 1
Training loss: 2.803618907928467
Validation loss: 2.7562652146944435

Epoch: 5| Step: 2
Training loss: 2.7004222869873047
Validation loss: 2.751051451570244

Epoch: 5| Step: 3
Training loss: 3.039738178253174
Validation loss: 2.747250954310099

Epoch: 5| Step: 4
Training loss: 2.811876058578491
Validation loss: 2.744662692469935

Epoch: 5| Step: 5
Training loss: 2.2791247367858887
Validation loss: 2.742228974578201

Epoch: 5| Step: 6
Training loss: 2.752624988555908
Validation loss: 2.7423792449376916

Epoch: 5| Step: 7
Training loss: 3.086962938308716
Validation loss: 2.7408409118652344

Epoch: 5| Step: 8
Training loss: 3.0232901573181152
Validation loss: 2.746215407566358

Epoch: 5| Step: 9
Training loss: 3.5190601348876953
Validation loss: 2.7455546138107136

Epoch: 5| Step: 10
Training loss: 3.060342788696289
Validation loss: 2.740705931058494

Epoch: 102| Step: 0
Training loss: 2.6110806465148926
Validation loss: 2.7473556585209344

Epoch: 5| Step: 1
Training loss: 3.4501025676727295
Validation loss: 2.74109544292573

Epoch: 5| Step: 2
Training loss: 2.4669077396392822
Validation loss: 2.7397766856737036

Epoch: 5| Step: 3
Training loss: 2.521252393722534
Validation loss: 2.740127417349046

Epoch: 5| Step: 4
Training loss: 3.12294340133667
Validation loss: 2.7443764978839504

Epoch: 5| Step: 5
Training loss: 2.9207820892333984
Validation loss: 2.743750036403697

Epoch: 5| Step: 6
Training loss: 3.7398502826690674
Validation loss: 2.7468769729778333

Epoch: 5| Step: 7
Training loss: 2.558223009109497
Validation loss: 2.7448056846536617

Epoch: 5| Step: 8
Training loss: 2.839329719543457
Validation loss: 2.7490094400221303

Epoch: 5| Step: 9
Training loss: 2.664512872695923
Validation loss: 2.7465324119854997

Epoch: 5| Step: 10
Training loss: 2.550222158432007
Validation loss: 2.7399021374282015

Epoch: 103| Step: 0
Training loss: 2.561864137649536
Validation loss: 2.734339657650199

Epoch: 5| Step: 1
Training loss: 3.164975643157959
Validation loss: 2.737780532529277

Epoch: 5| Step: 2
Training loss: 2.917201519012451
Validation loss: 2.7365048752036145

Epoch: 5| Step: 3
Training loss: 3.3052642345428467
Validation loss: 2.7438294041541313

Epoch: 5| Step: 4
Training loss: 2.386989116668701
Validation loss: 2.745524657669888

Epoch: 5| Step: 5
Training loss: 3.0871856212615967
Validation loss: 2.7500796574418263

Epoch: 5| Step: 6
Training loss: 2.905669689178467
Validation loss: 2.7510536306647846

Epoch: 5| Step: 7
Training loss: 2.47511625289917
Validation loss: 2.7520584188481814

Epoch: 5| Step: 8
Training loss: 2.5081915855407715
Validation loss: 2.7482546785826325

Epoch: 5| Step: 9
Training loss: 3.062066078186035
Validation loss: 2.7464954699239423

Epoch: 5| Step: 10
Training loss: 3.197700023651123
Validation loss: 2.741116726270286

Epoch: 104| Step: 0
Training loss: 2.99843168258667
Validation loss: 2.739633555053383

Epoch: 5| Step: 1
Training loss: 3.3597426414489746
Validation loss: 2.7337067819410756

Epoch: 5| Step: 2
Training loss: 2.6753926277160645
Validation loss: 2.7349885689314974

Epoch: 5| Step: 3
Training loss: 2.4292712211608887
Validation loss: 2.7334082664981967

Epoch: 5| Step: 4
Training loss: 3.892625093460083
Validation loss: 2.7330327162178616

Epoch: 5| Step: 5
Training loss: 3.6316847801208496
Validation loss: 2.7350209297672397

Epoch: 5| Step: 6
Training loss: 1.906616449356079
Validation loss: 2.7331967866548927

Epoch: 5| Step: 7
Training loss: 2.802896022796631
Validation loss: 2.7325073980516

Epoch: 5| Step: 8
Training loss: 2.8410725593566895
Validation loss: 2.7302352407927155

Epoch: 5| Step: 9
Training loss: 2.290534257888794
Validation loss: 2.7304052434941775

Epoch: 5| Step: 10
Training loss: 2.5801377296447754
Validation loss: 2.7320075009458806

Epoch: 105| Step: 0
Training loss: 2.948742151260376
Validation loss: 2.735006311888336

Epoch: 5| Step: 1
Training loss: 3.567206621170044
Validation loss: 2.7349656833115445

Epoch: 5| Step: 2
Training loss: 2.072822332382202
Validation loss: 2.7369164036166285

Epoch: 5| Step: 3
Training loss: 2.939028263092041
Validation loss: 2.7335703014045634

Epoch: 5| Step: 4
Training loss: 3.0744831562042236
Validation loss: 2.7315197349876486

Epoch: 5| Step: 5
Training loss: 2.650397777557373
Validation loss: 2.730393689165833

Epoch: 5| Step: 6
Training loss: 2.871303081512451
Validation loss: 2.7310554699231218

Epoch: 5| Step: 7
Training loss: 2.4132697582244873
Validation loss: 2.7321898552679245

Epoch: 5| Step: 8
Training loss: 2.7763850688934326
Validation loss: 2.736017173336398

Epoch: 5| Step: 9
Training loss: 3.2767608165740967
Validation loss: 2.7327695841430337

Epoch: 5| Step: 10
Training loss: 2.824075698852539
Validation loss: 2.735290253034202

Epoch: 106| Step: 0
Training loss: 2.5889060497283936
Validation loss: 2.7299998268004386

Epoch: 5| Step: 1
Training loss: 2.809461832046509
Validation loss: 2.728443225224813

Epoch: 5| Step: 2
Training loss: 2.71038818359375
Validation loss: 2.731358141027471

Epoch: 5| Step: 3
Training loss: 2.6648659706115723
Validation loss: 2.7300709473189486

Epoch: 5| Step: 4
Training loss: 2.908367872238159
Validation loss: 2.730867785792197

Epoch: 5| Step: 5
Training loss: 2.522188663482666
Validation loss: 2.727181739704583

Epoch: 5| Step: 6
Training loss: 2.6482152938842773
Validation loss: 2.7301192591267247

Epoch: 5| Step: 7
Training loss: 2.992866039276123
Validation loss: 2.730953372934813

Epoch: 5| Step: 8
Training loss: 2.986812114715576
Validation loss: 2.7315936062925603

Epoch: 5| Step: 9
Training loss: 3.3312103748321533
Validation loss: 2.729280471801758

Epoch: 5| Step: 10
Training loss: 3.313046932220459
Validation loss: 2.728404155341528

Epoch: 107| Step: 0
Training loss: 2.1443681716918945
Validation loss: 2.725702452403243

Epoch: 5| Step: 1
Training loss: 2.6988155841827393
Validation loss: 2.726565489204981

Epoch: 5| Step: 2
Training loss: 3.0075020790100098
Validation loss: 2.7252122894410165

Epoch: 5| Step: 3
Training loss: 2.826613187789917
Validation loss: 2.7255707428019535

Epoch: 5| Step: 4
Training loss: 2.6895737648010254
Validation loss: 2.729739814676264

Epoch: 5| Step: 5
Training loss: 2.6238160133361816
Validation loss: 2.733251264018397

Epoch: 5| Step: 6
Training loss: 3.1560869216918945
Validation loss: 2.7310644913745183

Epoch: 5| Step: 7
Training loss: 2.6784355640411377
Validation loss: 2.727016338738062

Epoch: 5| Step: 8
Training loss: 3.659064531326294
Validation loss: 2.729881827549268

Epoch: 5| Step: 9
Training loss: 3.4779858589172363
Validation loss: 2.731062040534071

Epoch: 5| Step: 10
Training loss: 2.417410135269165
Validation loss: 2.7281148484958115

Epoch: 108| Step: 0
Training loss: 2.991492509841919
Validation loss: 2.729639725018573

Epoch: 5| Step: 1
Training loss: 3.3629097938537598
Validation loss: 2.7286420022287676

Epoch: 5| Step: 2
Training loss: 2.8899340629577637
Validation loss: 2.7281699718967563

Epoch: 5| Step: 3
Training loss: 2.6270737648010254
Validation loss: 2.7316385520401822

Epoch: 5| Step: 4
Training loss: 3.09837007522583
Validation loss: 2.7291024269596225

Epoch: 5| Step: 5
Training loss: 2.8706202507019043
Validation loss: 2.728314220264394

Epoch: 5| Step: 6
Training loss: 2.63206148147583
Validation loss: 2.7320829822171118

Epoch: 5| Step: 7
Training loss: 3.1601390838623047
Validation loss: 2.7253577375924714

Epoch: 5| Step: 8
Training loss: 2.5788586139678955
Validation loss: 2.728837982300789

Epoch: 5| Step: 9
Training loss: 3.0183098316192627
Validation loss: 2.7255378256561937

Epoch: 5| Step: 10
Training loss: 2.0747923851013184
Validation loss: 2.72362176064522

Epoch: 109| Step: 0
Training loss: 2.6157121658325195
Validation loss: 2.722031352340534

Epoch: 5| Step: 1
Training loss: 3.6226189136505127
Validation loss: 2.7229380094876854

Epoch: 5| Step: 2
Training loss: 2.900649309158325
Validation loss: 2.7213695844014487

Epoch: 5| Step: 3
Training loss: 2.9711391925811768
Validation loss: 2.7217812538146973

Epoch: 5| Step: 4
Training loss: 3.0848824977874756
Validation loss: 2.722791705080258

Epoch: 5| Step: 5
Training loss: 2.9225640296936035
Validation loss: 2.7250631701561714

Epoch: 5| Step: 6
Training loss: 2.8491058349609375
Validation loss: 2.722736240715109

Epoch: 5| Step: 7
Training loss: 2.9303181171417236
Validation loss: 2.721012830734253

Epoch: 5| Step: 8
Training loss: 2.5260608196258545
Validation loss: 2.7254848659679456

Epoch: 5| Step: 9
Training loss: 2.4395506381988525
Validation loss: 2.72304170362411

Epoch: 5| Step: 10
Training loss: 2.484100818634033
Validation loss: 2.724316002220236

Epoch: 110| Step: 0
Training loss: 2.8962690830230713
Validation loss: 2.7231277035128687

Epoch: 5| Step: 1
Training loss: 2.883413314819336
Validation loss: 2.7206194605878604

Epoch: 5| Step: 2
Training loss: 2.8535208702087402
Validation loss: 2.7203160921732583

Epoch: 5| Step: 3
Training loss: 3.3779633045196533
Validation loss: 2.7224302214960896

Epoch: 5| Step: 4
Training loss: 2.611297130584717
Validation loss: 2.7234095476006948

Epoch: 5| Step: 5
Training loss: 2.942963123321533
Validation loss: 2.7221038931159565

Epoch: 5| Step: 6
Training loss: 2.9855692386627197
Validation loss: 2.7221459368223786

Epoch: 5| Step: 7
Training loss: 2.667870044708252
Validation loss: 2.7217285940724034

Epoch: 5| Step: 8
Training loss: 3.136061191558838
Validation loss: 2.7208324196518108

Epoch: 5| Step: 9
Training loss: 2.426445484161377
Validation loss: 2.7244714972793416

Epoch: 5| Step: 10
Training loss: 2.5582056045532227
Validation loss: 2.719928854255266

Epoch: 111| Step: 0
Training loss: 2.351227045059204
Validation loss: 2.7238640426307597

Epoch: 5| Step: 1
Training loss: 3.1495776176452637
Validation loss: 2.7224691119245303

Epoch: 5| Step: 2
Training loss: 2.463296890258789
Validation loss: 2.722077569653911

Epoch: 5| Step: 3
Training loss: 3.1801841259002686
Validation loss: 2.7206745686069613

Epoch: 5| Step: 4
Training loss: 2.967883825302124
Validation loss: 2.7213011787783716

Epoch: 5| Step: 5
Training loss: 3.0424811840057373
Validation loss: 2.71953151559317

Epoch: 5| Step: 6
Training loss: 3.037492513656616
Validation loss: 2.7205758658788537

Epoch: 5| Step: 7
Training loss: 2.934394598007202
Validation loss: 2.7188318083363194

Epoch: 5| Step: 8
Training loss: 2.857170581817627
Validation loss: 2.7190947712108655

Epoch: 5| Step: 9
Training loss: 2.7589993476867676
Validation loss: 2.7221780720577446

Epoch: 5| Step: 10
Training loss: 2.5912885665893555
Validation loss: 2.7190673556379092

Epoch: 112| Step: 0
Training loss: 3.3451170921325684
Validation loss: 2.71789728313364

Epoch: 5| Step: 1
Training loss: 1.90897536277771
Validation loss: 2.7168730510178434

Epoch: 5| Step: 2
Training loss: 2.8651225566864014
Validation loss: 2.71418495075677

Epoch: 5| Step: 3
Training loss: 3.1872191429138184
Validation loss: 2.7157729518029

Epoch: 5| Step: 4
Training loss: 2.9891324043273926
Validation loss: 2.7157290597115793

Epoch: 5| Step: 5
Training loss: 2.282205581665039
Validation loss: 2.713640759068151

Epoch: 5| Step: 6
Training loss: 2.7163896560668945
Validation loss: 2.717218647720993

Epoch: 5| Step: 7
Training loss: 2.867279529571533
Validation loss: 2.7183919722034084

Epoch: 5| Step: 8
Training loss: 2.5918476581573486
Validation loss: 2.716816025395547

Epoch: 5| Step: 9
Training loss: 3.5444259643554688
Validation loss: 2.7135239211461877

Epoch: 5| Step: 10
Training loss: 3.06396484375
Validation loss: 2.715646266937256

Epoch: 113| Step: 0
Training loss: 2.899876356124878
Validation loss: 2.7115202001346055

Epoch: 5| Step: 1
Training loss: 2.696443557739258
Validation loss: 2.7177215545408187

Epoch: 5| Step: 2
Training loss: 2.6536247730255127
Validation loss: 2.716325831669633

Epoch: 5| Step: 3
Training loss: 2.7044992446899414
Validation loss: 2.7096646626790366

Epoch: 5| Step: 4
Training loss: 2.9770302772521973
Validation loss: 2.7137836999790643

Epoch: 5| Step: 5
Training loss: 2.4514949321746826
Validation loss: 2.7146201056818806

Epoch: 5| Step: 6
Training loss: 2.7072715759277344
Validation loss: 2.716618222575034

Epoch: 5| Step: 7
Training loss: 2.7141506671905518
Validation loss: 2.720067467740787

Epoch: 5| Step: 8
Training loss: 3.589359998703003
Validation loss: 2.714486934805429

Epoch: 5| Step: 9
Training loss: 3.1601202487945557
Validation loss: 2.7179762137833463

Epoch: 5| Step: 10
Training loss: 2.758476972579956
Validation loss: 2.7124055072825444

Epoch: 114| Step: 0
Training loss: 3.5766658782958984
Validation loss: 2.718332093249085

Epoch: 5| Step: 1
Training loss: 2.939640998840332
Validation loss: 2.716089112784273

Epoch: 5| Step: 2
Training loss: 2.4993321895599365
Validation loss: 2.7178872272532475

Epoch: 5| Step: 3
Training loss: 2.8148581981658936
Validation loss: 2.7166366038783902

Epoch: 5| Step: 4
Training loss: 2.359161615371704
Validation loss: 2.715171242272982

Epoch: 5| Step: 5
Training loss: 3.2822976112365723
Validation loss: 2.718332326540383

Epoch: 5| Step: 6
Training loss: 2.7561187744140625
Validation loss: 2.7137992869141283

Epoch: 5| Step: 7
Training loss: 3.1721653938293457
Validation loss: 2.717594213383172

Epoch: 5| Step: 8
Training loss: 2.5524234771728516
Validation loss: 2.7162111805331324

Epoch: 5| Step: 9
Training loss: 2.9376683235168457
Validation loss: 2.713777406241304

Epoch: 5| Step: 10
Training loss: 2.323232650756836
Validation loss: 2.715080133048437

Epoch: 115| Step: 0
Training loss: 2.6185951232910156
Validation loss: 2.7154280037008305

Epoch: 5| Step: 1
Training loss: 3.210369110107422
Validation loss: 2.711859582572855

Epoch: 5| Step: 2
Training loss: 3.0294623374938965
Validation loss: 2.715830038952571

Epoch: 5| Step: 3
Training loss: 3.0760724544525146
Validation loss: 2.714553725334906

Epoch: 5| Step: 4
Training loss: 3.329566240310669
Validation loss: 2.713505096333001

Epoch: 5| Step: 5
Training loss: 3.2273108959198
Validation loss: 2.7154502227742183

Epoch: 5| Step: 6
Training loss: 1.643090009689331
Validation loss: 2.71557282376033

Epoch: 5| Step: 7
Training loss: 3.2649643421173096
Validation loss: 2.7157576289228214

Epoch: 5| Step: 8
Training loss: 2.4243381023406982
Validation loss: 2.712855580032513

Epoch: 5| Step: 9
Training loss: 2.8105015754699707
Validation loss: 2.7063708818087013

Epoch: 5| Step: 10
Training loss: 2.676450252532959
Validation loss: 2.706895923101774

Epoch: 116| Step: 0
Training loss: 3.0205166339874268
Validation loss: 2.7069425839249805

Epoch: 5| Step: 1
Training loss: 2.633629083633423
Validation loss: 2.7073260532912387

Epoch: 5| Step: 2
Training loss: 2.7041244506835938
Validation loss: 2.7118812709726314

Epoch: 5| Step: 3
Training loss: 3.0136961936950684
Validation loss: 2.713639018356159

Epoch: 5| Step: 4
Training loss: 1.767754316329956
Validation loss: 2.7121720647299163

Epoch: 5| Step: 5
Training loss: 3.404345750808716
Validation loss: 2.708311706460932

Epoch: 5| Step: 6
Training loss: 2.932894468307495
Validation loss: 2.708900741351548

Epoch: 5| Step: 7
Training loss: 2.9610419273376465
Validation loss: 2.7078363587779384

Epoch: 5| Step: 8
Training loss: 3.0980536937713623
Validation loss: 2.7030850482243363

Epoch: 5| Step: 9
Training loss: 2.920039653778076
Validation loss: 2.708088472325315

Epoch: 5| Step: 10
Training loss: 2.9456374645233154
Validation loss: 2.711577694903138

Epoch: 117| Step: 0
Training loss: 2.527548313140869
Validation loss: 2.7131293742887435

Epoch: 5| Step: 1
Training loss: 2.68393874168396
Validation loss: 2.7124792016962522

Epoch: 5| Step: 2
Training loss: 2.9205267429351807
Validation loss: 2.707863733332644

Epoch: 5| Step: 3
Training loss: 3.289982557296753
Validation loss: 2.7093608866455736

Epoch: 5| Step: 4
Training loss: 2.3117220401763916
Validation loss: 2.708755939237533

Epoch: 5| Step: 5
Training loss: 2.681040048599243
Validation loss: 2.7127926785458802

Epoch: 5| Step: 6
Training loss: 2.440621852874756
Validation loss: 2.7122561675246044

Epoch: 5| Step: 7
Training loss: 3.04883074760437
Validation loss: 2.7092893508172806

Epoch: 5| Step: 8
Training loss: 2.2055842876434326
Validation loss: 2.7107467036093436

Epoch: 5| Step: 9
Training loss: 3.7770028114318848
Validation loss: 2.7120541475152455

Epoch: 5| Step: 10
Training loss: 3.5358753204345703
Validation loss: 2.70737579817413

Epoch: 118| Step: 0
Training loss: 2.968092203140259
Validation loss: 2.707489569981893

Epoch: 5| Step: 1
Training loss: 3.11041259765625
Validation loss: 2.705576442903088

Epoch: 5| Step: 2
Training loss: 2.5514328479766846
Validation loss: 2.703458442482897

Epoch: 5| Step: 3
Training loss: 2.873709201812744
Validation loss: 2.705244133549352

Epoch: 5| Step: 4
Training loss: 2.4872424602508545
Validation loss: 2.700981699010377

Epoch: 5| Step: 5
Training loss: 2.8353896141052246
Validation loss: 2.703118593462052

Epoch: 5| Step: 6
Training loss: 3.6854538917541504
Validation loss: 2.7020205682323826

Epoch: 5| Step: 7
Training loss: 2.64554762840271
Validation loss: 2.7038954816838747

Epoch: 5| Step: 8
Training loss: 2.9496772289276123
Validation loss: 2.704436822604108

Epoch: 5| Step: 9
Training loss: 2.8256335258483887
Validation loss: 2.7020643782872025

Epoch: 5| Step: 10
Training loss: 2.239500045776367
Validation loss: 2.705477063373853

Epoch: 119| Step: 0
Training loss: 2.4625372886657715
Validation loss: 2.702042861651349

Epoch: 5| Step: 1
Training loss: 2.6044204235076904
Validation loss: 2.7032010042539207

Epoch: 5| Step: 2
Training loss: 3.1244194507598877
Validation loss: 2.703665215481994

Epoch: 5| Step: 3
Training loss: 3.0086898803710938
Validation loss: 2.705420660716231

Epoch: 5| Step: 4
Training loss: 3.1102852821350098
Validation loss: 2.706177252595143

Epoch: 5| Step: 5
Training loss: 2.6419930458068848
Validation loss: 2.704744521007743

Epoch: 5| Step: 6
Training loss: 3.4930405616760254
Validation loss: 2.7046378427936184

Epoch: 5| Step: 7
Training loss: 2.470933437347412
Validation loss: 2.7056440794339744

Epoch: 5| Step: 8
Training loss: 3.385707139968872
Validation loss: 2.7001740471009286

Epoch: 5| Step: 9
Training loss: 1.9902862310409546
Validation loss: 2.703115960603119

Epoch: 5| Step: 10
Training loss: 2.981494903564453
Validation loss: 2.702969310104206

Epoch: 120| Step: 0
Training loss: 3.0116257667541504
Validation loss: 2.7077877008786766

Epoch: 5| Step: 1
Training loss: 2.7455620765686035
Validation loss: 2.70951904789094

Epoch: 5| Step: 2
Training loss: 2.9898016452789307
Validation loss: 2.70922375494434

Epoch: 5| Step: 3
Training loss: 2.900632858276367
Validation loss: 2.7075444600915395

Epoch: 5| Step: 4
Training loss: 2.768261194229126
Validation loss: 2.7166912735149427

Epoch: 5| Step: 5
Training loss: 2.360318660736084
Validation loss: 2.717770891804849

Epoch: 5| Step: 6
Training loss: 2.595551013946533
Validation loss: 2.7131945676701044

Epoch: 5| Step: 7
Training loss: 2.955979585647583
Validation loss: 2.7064728019058064

Epoch: 5| Step: 8
Training loss: 3.303908586502075
Validation loss: 2.699948900489397

Epoch: 5| Step: 9
Training loss: 2.7134063243865967
Validation loss: 2.70519164300734

Epoch: 5| Step: 10
Training loss: 2.8501601219177246
Validation loss: 2.7002637027412333

Epoch: 121| Step: 0
Training loss: 3.5051941871643066
Validation loss: 2.7010575750822663

Epoch: 5| Step: 1
Training loss: 2.702345848083496
Validation loss: 2.698214925745482

Epoch: 5| Step: 2
Training loss: 3.0438058376312256
Validation loss: 2.6993220313902824

Epoch: 5| Step: 3
Training loss: 2.4026031494140625
Validation loss: 2.69950686218918

Epoch: 5| Step: 4
Training loss: 2.3913135528564453
Validation loss: 2.7027087416700137

Epoch: 5| Step: 5
Training loss: 2.969214916229248
Validation loss: 2.701376489413682

Epoch: 5| Step: 6
Training loss: 2.922152519226074
Validation loss: 2.701008996655864

Epoch: 5| Step: 7
Training loss: 3.0240578651428223
Validation loss: 2.7017087474946053

Epoch: 5| Step: 8
Training loss: 2.8735480308532715
Validation loss: 2.702479200978433

Epoch: 5| Step: 9
Training loss: 2.2841248512268066
Validation loss: 2.698819086115847

Epoch: 5| Step: 10
Training loss: 3.1055569648742676
Validation loss: 2.6974755384588756

Epoch: 122| Step: 0
Training loss: 2.119779109954834
Validation loss: 2.6985381111021964

Epoch: 5| Step: 1
Training loss: 3.143608808517456
Validation loss: 2.6969867778080765

Epoch: 5| Step: 2
Training loss: 2.54854154586792
Validation loss: 2.696792451284265

Epoch: 5| Step: 3
Training loss: 3.192810535430908
Validation loss: 2.6959425633953464

Epoch: 5| Step: 4
Training loss: 2.0641815662384033
Validation loss: 2.6969244480133057

Epoch: 5| Step: 5
Training loss: 2.6506786346435547
Validation loss: 2.6933182336950816

Epoch: 5| Step: 6
Training loss: 3.4611411094665527
Validation loss: 2.696676825964323

Epoch: 5| Step: 7
Training loss: 3.3922481536865234
Validation loss: 2.695298251285348

Epoch: 5| Step: 8
Training loss: 3.4182801246643066
Validation loss: 2.696674857088315

Epoch: 5| Step: 9
Training loss: 2.8477418422698975
Validation loss: 2.694241121251096

Epoch: 5| Step: 10
Training loss: 2.222066640853882
Validation loss: 2.6979044175917104

Epoch: 123| Step: 0
Training loss: 3.820047378540039
Validation loss: 2.6947974953600156

Epoch: 5| Step: 1
Training loss: 2.571631908416748
Validation loss: 2.69598783985261

Epoch: 5| Step: 2
Training loss: 2.360795259475708
Validation loss: 2.6952049578389814

Epoch: 5| Step: 3
Training loss: 2.3068325519561768
Validation loss: 2.695345006963258

Epoch: 5| Step: 4
Training loss: 3.1709110736846924
Validation loss: 2.6985564693327873

Epoch: 5| Step: 5
Training loss: 3.530202865600586
Validation loss: 2.693255662918091

Epoch: 5| Step: 6
Training loss: 3.1477532386779785
Validation loss: 2.6976738283711095

Epoch: 5| Step: 7
Training loss: 2.780205488204956
Validation loss: 2.6942674626586256

Epoch: 5| Step: 8
Training loss: 1.8603689670562744
Validation loss: 2.6943393445784047

Epoch: 5| Step: 9
Training loss: 3.066983699798584
Validation loss: 2.6937989393870034

Epoch: 5| Step: 10
Training loss: 2.507718801498413
Validation loss: 2.6935029363119476

Epoch: 124| Step: 0
Training loss: 2.983893632888794
Validation loss: 2.6955670720787457

Epoch: 5| Step: 1
Training loss: 2.4227683544158936
Validation loss: 2.6951196526968353

Epoch: 5| Step: 2
Training loss: 3.7227790355682373
Validation loss: 2.693208431684843

Epoch: 5| Step: 3
Training loss: 3.1480488777160645
Validation loss: 2.6971616078448553

Epoch: 5| Step: 4
Training loss: 2.5070128440856934
Validation loss: 2.691979903046803

Epoch: 5| Step: 5
Training loss: 1.8441240787506104
Validation loss: 2.6923236000922417

Epoch: 5| Step: 6
Training loss: 2.36845064163208
Validation loss: 2.691193513972785

Epoch: 5| Step: 7
Training loss: 3.0781638622283936
Validation loss: 2.689908150703676

Epoch: 5| Step: 8
Training loss: 2.6532299518585205
Validation loss: 2.6906941372861146

Epoch: 5| Step: 9
Training loss: 3.2885444164276123
Validation loss: 2.695484417741017

Epoch: 5| Step: 10
Training loss: 3.113579273223877
Validation loss: 2.695918483118857

Epoch: 125| Step: 0
Training loss: 3.4068329334259033
Validation loss: 2.710208687731015

Epoch: 5| Step: 1
Training loss: 2.644991397857666
Validation loss: 2.689100336003047

Epoch: 5| Step: 2
Training loss: 3.63958477973938
Validation loss: 2.6912504165403304

Epoch: 5| Step: 3
Training loss: 3.1960856914520264
Validation loss: 2.696582709589312

Epoch: 5| Step: 4
Training loss: 3.1633670330047607
Validation loss: 2.6914797111224105

Epoch: 5| Step: 5
Training loss: 2.5304081439971924
Validation loss: 2.6895091200387604

Epoch: 5| Step: 6
Training loss: 2.7887561321258545
Validation loss: 2.6881065855744066

Epoch: 5| Step: 7
Training loss: 3.0333688259124756
Validation loss: 2.6900385400300384

Epoch: 5| Step: 8
Training loss: 1.8364607095718384
Validation loss: 2.6883703226684244

Epoch: 5| Step: 9
Training loss: 2.2719693183898926
Validation loss: 2.6909535469547397

Epoch: 5| Step: 10
Training loss: 2.603304862976074
Validation loss: 2.6919233927162747

Epoch: 126| Step: 0
Training loss: 2.874436855316162
Validation loss: 2.6867905252723285

Epoch: 5| Step: 1
Training loss: 2.684340715408325
Validation loss: 2.687425487784929

Epoch: 5| Step: 2
Training loss: 2.61616849899292
Validation loss: 2.6887663231101087

Epoch: 5| Step: 3
Training loss: 3.2091479301452637
Validation loss: 2.688218375687958

Epoch: 5| Step: 4
Training loss: 2.8159968852996826
Validation loss: 2.6929504179185435

Epoch: 5| Step: 5
Training loss: 2.6300714015960693
Validation loss: 2.689457185806767

Epoch: 5| Step: 6
Training loss: 3.3746562004089355
Validation loss: 2.695571789177515

Epoch: 5| Step: 7
Training loss: 2.414395332336426
Validation loss: 2.699059550480176

Epoch: 5| Step: 8
Training loss: 2.642720937728882
Validation loss: 2.692240830390684

Epoch: 5| Step: 9
Training loss: 2.916840076446533
Validation loss: 2.689393807482976

Epoch: 5| Step: 10
Training loss: 2.9436511993408203
Validation loss: 2.6908494169994066

Epoch: 127| Step: 0
Training loss: 2.6205790042877197
Validation loss: 2.685976151497133

Epoch: 5| Step: 1
Training loss: 2.6582627296447754
Validation loss: 2.6894215922201834

Epoch: 5| Step: 2
Training loss: 2.5278258323669434
Validation loss: 2.6873157588384484

Epoch: 5| Step: 3
Training loss: 2.7453582286834717
Validation loss: 2.685318026491391

Epoch: 5| Step: 4
Training loss: 3.0453872680664062
Validation loss: 2.687521324362806

Epoch: 5| Step: 5
Training loss: 3.318596363067627
Validation loss: 2.6877051245781685

Epoch: 5| Step: 6
Training loss: 2.8636093139648438
Validation loss: 2.686858651458576

Epoch: 5| Step: 7
Training loss: 2.4550068378448486
Validation loss: 2.6863156057173208

Epoch: 5| Step: 8
Training loss: 3.3645026683807373
Validation loss: 2.6852922362665974

Epoch: 5| Step: 9
Training loss: 2.8183560371398926
Validation loss: 2.684911976578415

Epoch: 5| Step: 10
Training loss: 2.5784127712249756
Validation loss: 2.6883847175105924

Epoch: 128| Step: 0
Training loss: 2.794975519180298
Validation loss: 2.6886627110101844

Epoch: 5| Step: 1
Training loss: 2.6560730934143066
Validation loss: 2.6858892799705587

Epoch: 5| Step: 2
Training loss: 2.7954087257385254
Validation loss: 2.685606079716836

Epoch: 5| Step: 3
Training loss: 3.477332592010498
Validation loss: 2.6863840395404446

Epoch: 5| Step: 4
Training loss: 2.6488919258117676
Validation loss: 2.682973495093725

Epoch: 5| Step: 5
Training loss: 2.78410267829895
Validation loss: 2.6835514550567954

Epoch: 5| Step: 6
Training loss: 2.4607906341552734
Validation loss: 2.6876356473533054

Epoch: 5| Step: 7
Training loss: 3.388390064239502
Validation loss: 2.686024373577487

Epoch: 5| Step: 8
Training loss: 2.5833284854888916
Validation loss: 2.6881118461649907

Epoch: 5| Step: 9
Training loss: 2.436668872833252
Validation loss: 2.6890136093221684

Epoch: 5| Step: 10
Training loss: 2.993129253387451
Validation loss: 2.6892780642355643

Epoch: 129| Step: 0
Training loss: 3.1491291522979736
Validation loss: 2.6896967836605605

Epoch: 5| Step: 1
Training loss: 3.4123244285583496
Validation loss: 2.688202145279095

Epoch: 5| Step: 2
Training loss: 2.455305576324463
Validation loss: 2.6873546031213578

Epoch: 5| Step: 3
Training loss: 3.4996230602264404
Validation loss: 2.684925527982814

Epoch: 5| Step: 4
Training loss: 2.3428211212158203
Validation loss: 2.6870873794760755

Epoch: 5| Step: 5
Training loss: 2.7492570877075195
Validation loss: 2.6937437365131993

Epoch: 5| Step: 6
Training loss: 2.226898670196533
Validation loss: 2.691349519196377

Epoch: 5| Step: 7
Training loss: 2.91766357421875
Validation loss: 2.7056478044038177

Epoch: 5| Step: 8
Training loss: 2.6748931407928467
Validation loss: 2.6975766971547115

Epoch: 5| Step: 9
Training loss: 2.5654802322387695
Validation loss: 2.6918002943838797

Epoch: 5| Step: 10
Training loss: 3.0857059955596924
Validation loss: 2.683356927287194

Epoch: 130| Step: 0
Training loss: 3.259747266769409
Validation loss: 2.6802777449289956

Epoch: 5| Step: 1
Training loss: 2.548487663269043
Validation loss: 2.6866694188887075

Epoch: 5| Step: 2
Training loss: 3.014251232147217
Validation loss: 2.688932908478604

Epoch: 5| Step: 3
Training loss: 2.6689891815185547
Validation loss: 2.7010036360832954

Epoch: 5| Step: 4
Training loss: 2.7540879249572754
Validation loss: 2.706716275984241

Epoch: 5| Step: 5
Training loss: 2.945427417755127
Validation loss: 2.7163447949194137

Epoch: 5| Step: 6
Training loss: 2.7691264152526855
Validation loss: 2.726632677098756

Epoch: 5| Step: 7
Training loss: 2.413360834121704
Validation loss: 2.6983334325974986

Epoch: 5| Step: 8
Training loss: 3.339961528778076
Validation loss: 2.6822817633228917

Epoch: 5| Step: 9
Training loss: 2.7818565368652344
Validation loss: 2.6819415195013887

Epoch: 5| Step: 10
Training loss: 2.652444362640381
Validation loss: 2.6821418628897717

Epoch: 131| Step: 0
Training loss: 2.7130556106567383
Validation loss: 2.6851296886321037

Epoch: 5| Step: 1
Training loss: 2.59608793258667
Validation loss: 2.695762062585482

Epoch: 5| Step: 2
Training loss: 2.6558685302734375
Validation loss: 2.691383533580329

Epoch: 5| Step: 3
Training loss: 2.9283156394958496
Validation loss: 2.6907363014836467

Epoch: 5| Step: 4
Training loss: 3.4528591632843018
Validation loss: 2.690576302107944

Epoch: 5| Step: 5
Training loss: 2.627263307571411
Validation loss: 2.688983594217608

Epoch: 5| Step: 6
Training loss: 3.0671286582946777
Validation loss: 2.685085637595064

Epoch: 5| Step: 7
Training loss: 3.0336458683013916
Validation loss: 2.6851668434758342

Epoch: 5| Step: 8
Training loss: 2.9977786540985107
Validation loss: 2.6843386952595045

Epoch: 5| Step: 9
Training loss: 2.2990775108337402
Validation loss: 2.6890932667639946

Epoch: 5| Step: 10
Training loss: 2.7747747898101807
Validation loss: 2.68657511280429

Epoch: 132| Step: 0
Training loss: 2.8616509437561035
Validation loss: 2.6871468405569754

Epoch: 5| Step: 1
Training loss: 2.704073667526245
Validation loss: 2.684156697283509

Epoch: 5| Step: 2
Training loss: 2.942148208618164
Validation loss: 2.6879746811364287

Epoch: 5| Step: 3
Training loss: 2.934152603149414
Validation loss: 2.691394072706981

Epoch: 5| Step: 4
Training loss: 3.0525856018066406
Validation loss: 2.685516877840924

Epoch: 5| Step: 5
Training loss: 2.322096347808838
Validation loss: 2.683220250632173

Epoch: 5| Step: 6
Training loss: 2.983846426010132
Validation loss: 2.685537653584634

Epoch: 5| Step: 7
Training loss: 2.6301021575927734
Validation loss: 2.68151944170716

Epoch: 5| Step: 8
Training loss: 2.3377859592437744
Validation loss: 2.6793510657484814

Epoch: 5| Step: 9
Training loss: 3.45683217048645
Validation loss: 2.677532806191393

Epoch: 5| Step: 10
Training loss: 2.8798699378967285
Validation loss: 2.6766128591311875

Epoch: 133| Step: 0
Training loss: 2.9760491847991943
Validation loss: 2.676883707764328

Epoch: 5| Step: 1
Training loss: 3.5063374042510986
Validation loss: 2.676520050212901

Epoch: 5| Step: 2
Training loss: 2.213275194168091
Validation loss: 2.6785282781047206

Epoch: 5| Step: 3
Training loss: 2.936691999435425
Validation loss: 2.680299835820352

Epoch: 5| Step: 4
Training loss: 2.817457675933838
Validation loss: 2.683827292534613

Epoch: 5| Step: 5
Training loss: 2.7488009929656982
Validation loss: 2.6808025119125203

Epoch: 5| Step: 6
Training loss: 2.775900363922119
Validation loss: 2.679710013892061

Epoch: 5| Step: 7
Training loss: 2.5577006340026855
Validation loss: 2.6800730869334233

Epoch: 5| Step: 8
Training loss: 2.2045207023620605
Validation loss: 2.676929002167076

Epoch: 5| Step: 9
Training loss: 2.959291458129883
Validation loss: 2.681115324779223

Epoch: 5| Step: 10
Training loss: 3.440647602081299
Validation loss: 2.676892577960927

Epoch: 134| Step: 0
Training loss: 2.552943468093872
Validation loss: 2.679743698848191

Epoch: 5| Step: 1
Training loss: 2.9069175720214844
Validation loss: 2.6789682962561168

Epoch: 5| Step: 2
Training loss: 2.9442036151885986
Validation loss: 2.6774275559251026

Epoch: 5| Step: 3
Training loss: 2.6206867694854736
Validation loss: 2.6860557397206626

Epoch: 5| Step: 4
Training loss: 2.80385422706604
Validation loss: 2.6826760871436006

Epoch: 5| Step: 5
Training loss: 3.448401689529419
Validation loss: 2.6868909405123804

Epoch: 5| Step: 6
Training loss: 2.58455491065979
Validation loss: 2.684532685946393

Epoch: 5| Step: 7
Training loss: 2.4831032752990723
Validation loss: 2.683225452259023

Epoch: 5| Step: 8
Training loss: 3.6408157348632812
Validation loss: 2.6856595752059773

Epoch: 5| Step: 9
Training loss: 1.9754873514175415
Validation loss: 2.6800651293928905

Epoch: 5| Step: 10
Training loss: 3.151010036468506
Validation loss: 2.675078284355902

Epoch: 135| Step: 0
Training loss: 2.6733803749084473
Validation loss: 2.676557235820319

Epoch: 5| Step: 1
Training loss: 2.2588627338409424
Validation loss: 2.6796313075609106

Epoch: 5| Step: 2
Training loss: 3.189561605453491
Validation loss: 2.678491702643774

Epoch: 5| Step: 3
Training loss: 3.4499802589416504
Validation loss: 2.6813505464984524

Epoch: 5| Step: 4
Training loss: 3.2178046703338623
Validation loss: 2.681188847429009

Epoch: 5| Step: 5
Training loss: 2.3933517932891846
Validation loss: 2.6780205080586095

Epoch: 5| Step: 6
Training loss: 3.3578906059265137
Validation loss: 2.6780866910052556

Epoch: 5| Step: 7
Training loss: 2.6857411861419678
Validation loss: 2.6795348223819526

Epoch: 5| Step: 8
Training loss: 2.4105515480041504
Validation loss: 2.684472635228147

Epoch: 5| Step: 9
Training loss: 2.901946544647217
Validation loss: 2.681171435181813

Epoch: 5| Step: 10
Training loss: 2.453608751296997
Validation loss: 2.6789320258684057

Epoch: 136| Step: 0
Training loss: 3.860212802886963
Validation loss: 2.6791852417812554

Epoch: 5| Step: 1
Training loss: 2.498823642730713
Validation loss: 2.6743808818119827

Epoch: 5| Step: 2
Training loss: 2.3099732398986816
Validation loss: 2.6710480182401595

Epoch: 5| Step: 3
Training loss: 3.033008098602295
Validation loss: 2.6739839225687008

Epoch: 5| Step: 4
Training loss: 3.0123131275177
Validation loss: 2.673503880859703

Epoch: 5| Step: 5
Training loss: 2.743950605392456
Validation loss: 2.67126493043797

Epoch: 5| Step: 6
Training loss: 2.7536463737487793
Validation loss: 2.6692578254207486

Epoch: 5| Step: 7
Training loss: 2.1695382595062256
Validation loss: 2.6731957158734723

Epoch: 5| Step: 8
Training loss: 2.7864785194396973
Validation loss: 2.6704017551996375

Epoch: 5| Step: 9
Training loss: 2.993650436401367
Validation loss: 2.6705751752340667

Epoch: 5| Step: 10
Training loss: 2.8103013038635254
Validation loss: 2.6693215780360724

Epoch: 137| Step: 0
Training loss: 3.428358793258667
Validation loss: 2.6720203404785483

Epoch: 5| Step: 1
Training loss: 3.2658932209014893
Validation loss: 2.678942390667495

Epoch: 5| Step: 2
Training loss: 3.05845308303833
Validation loss: 2.6743463059907318

Epoch: 5| Step: 3
Training loss: 3.314229965209961
Validation loss: 2.6748770001114055

Epoch: 5| Step: 4
Training loss: 3.2810986042022705
Validation loss: 2.67731878834386

Epoch: 5| Step: 5
Training loss: 2.5647854804992676
Validation loss: 2.671706235536965

Epoch: 5| Step: 6
Training loss: 1.7846736907958984
Validation loss: 2.674186924452423

Epoch: 5| Step: 7
Training loss: 2.049638271331787
Validation loss: 2.6754655017647693

Epoch: 5| Step: 8
Training loss: 2.225318431854248
Validation loss: 2.671813508515717

Epoch: 5| Step: 9
Training loss: 2.6209402084350586
Validation loss: 2.6715103144286783

Epoch: 5| Step: 10
Training loss: 3.4758946895599365
Validation loss: 2.67114044004871

Epoch: 138| Step: 0
Training loss: 2.6710259914398193
Validation loss: 2.67284898091388

Epoch: 5| Step: 1
Training loss: 2.9380533695220947
Validation loss: 2.672952398177116

Epoch: 5| Step: 2
Training loss: 2.71496319770813
Validation loss: 2.671387795479067

Epoch: 5| Step: 3
Training loss: 2.774956226348877
Validation loss: 2.6697249258718183

Epoch: 5| Step: 4
Training loss: 2.1649703979492188
Validation loss: 2.6702873296635126

Epoch: 5| Step: 5
Training loss: 2.8051326274871826
Validation loss: 2.6687036278427287

Epoch: 5| Step: 6
Training loss: 2.840928792953491
Validation loss: 2.671234720496721

Epoch: 5| Step: 7
Training loss: 2.36518931388855
Validation loss: 2.6650276466082503

Epoch: 5| Step: 8
Training loss: 2.9831671714782715
Validation loss: 2.6692064859533824

Epoch: 5| Step: 9
Training loss: 2.737806797027588
Validation loss: 2.6669331391652427

Epoch: 5| Step: 10
Training loss: 4.12290620803833
Validation loss: 2.6707771260251283

Epoch: 139| Step: 0
Training loss: 3.264871120452881
Validation loss: 2.6763950035136235

Epoch: 5| Step: 1
Training loss: 3.0021443367004395
Validation loss: 2.6735186064115135

Epoch: 5| Step: 2
Training loss: 2.3900253772735596
Validation loss: 2.6749344743708128

Epoch: 5| Step: 3
Training loss: 3.313603639602661
Validation loss: 2.6765892082645046

Epoch: 5| Step: 4
Training loss: 2.5506274700164795
Validation loss: 2.6688846234352357

Epoch: 5| Step: 5
Training loss: 2.504991292953491
Validation loss: 2.669545696627709

Epoch: 5| Step: 6
Training loss: 3.0852813720703125
Validation loss: 2.6632255943872596

Epoch: 5| Step: 7
Training loss: 2.6941208839416504
Validation loss: 2.6626641314516784

Epoch: 5| Step: 8
Training loss: 2.6132047176361084
Validation loss: 2.6644255884232058

Epoch: 5| Step: 9
Training loss: 2.220470905303955
Validation loss: 2.669290027310771

Epoch: 5| Step: 10
Training loss: 3.3668699264526367
Validation loss: 2.6681166977010746

Epoch: 140| Step: 0
Training loss: 2.8201191425323486
Validation loss: 2.668696931613389

Epoch: 5| Step: 1
Training loss: 2.63070011138916
Validation loss: 2.6710531455214306

Epoch: 5| Step: 2
Training loss: 2.2631359100341797
Validation loss: 2.668215628593199

Epoch: 5| Step: 3
Training loss: 2.6648616790771484
Validation loss: 2.6674126502006286

Epoch: 5| Step: 4
Training loss: 2.7782909870147705
Validation loss: 2.6669138862240698

Epoch: 5| Step: 5
Training loss: 3.0735220909118652
Validation loss: 2.6663721402486167

Epoch: 5| Step: 6
Training loss: 3.2489216327667236
Validation loss: 2.6656901887668076

Epoch: 5| Step: 7
Training loss: 2.392148494720459
Validation loss: 2.6635878034817275

Epoch: 5| Step: 8
Training loss: 2.576035737991333
Validation loss: 2.6685856465370423

Epoch: 5| Step: 9
Training loss: 3.4242923259735107
Validation loss: 2.6691141154176448

Epoch: 5| Step: 10
Training loss: 3.126049041748047
Validation loss: 2.6694715125586397

Epoch: 141| Step: 0
Training loss: 3.3652870655059814
Validation loss: 2.665460189183553

Epoch: 5| Step: 1
Training loss: 2.6800148487091064
Validation loss: 2.6602055154820925

Epoch: 5| Step: 2
Training loss: 2.055070400238037
Validation loss: 2.6613267826777633

Epoch: 5| Step: 3
Training loss: 2.8800418376922607
Validation loss: 2.6574973290966404

Epoch: 5| Step: 4
Training loss: 3.620753526687622
Validation loss: 2.6609285980142574

Epoch: 5| Step: 5
Training loss: 2.6575608253479004
Validation loss: 2.664109968370007

Epoch: 5| Step: 6
Training loss: 3.1106491088867188
Validation loss: 2.6594177061511624

Epoch: 5| Step: 7
Training loss: 2.4368011951446533
Validation loss: 2.6632720655010593

Epoch: 5| Step: 8
Training loss: 2.6458277702331543
Validation loss: 2.6613097677948656

Epoch: 5| Step: 9
Training loss: 2.6297965049743652
Validation loss: 2.6642849368433796

Epoch: 5| Step: 10
Training loss: 2.8340988159179688
Validation loss: 2.6626486932077715

Epoch: 142| Step: 0
Training loss: 2.437039852142334
Validation loss: 2.663217552246586

Epoch: 5| Step: 1
Training loss: 2.844136953353882
Validation loss: 2.666037669745825

Epoch: 5| Step: 2
Training loss: 3.1763088703155518
Validation loss: 2.662839843380836

Epoch: 5| Step: 3
Training loss: 2.6302123069763184
Validation loss: 2.663935999716482

Epoch: 5| Step: 4
Training loss: 3.713780164718628
Validation loss: 2.668042844341647

Epoch: 5| Step: 5
Training loss: 2.7154102325439453
Validation loss: 2.6692068833176807

Epoch: 5| Step: 6
Training loss: 2.305198907852173
Validation loss: 2.669477214095413

Epoch: 5| Step: 7
Training loss: 2.6544113159179688
Validation loss: 2.66667442424323

Epoch: 5| Step: 8
Training loss: 2.543894052505493
Validation loss: 2.678778422776089

Epoch: 5| Step: 9
Training loss: 2.562765121459961
Validation loss: 2.681062703491539

Epoch: 5| Step: 10
Training loss: 3.35617995262146
Validation loss: 2.6833866642367457

Epoch: 143| Step: 0
Training loss: 3.1342153549194336
Validation loss: 2.6806827258038264

Epoch: 5| Step: 1
Training loss: 3.026414155960083
Validation loss: 2.6633993118040022

Epoch: 5| Step: 2
Training loss: 2.679774045944214
Validation loss: 2.6605569419040473

Epoch: 5| Step: 3
Training loss: 2.3214430809020996
Validation loss: 2.6579621837985132

Epoch: 5| Step: 4
Training loss: 2.8478007316589355
Validation loss: 2.657020507320281

Epoch: 5| Step: 5
Training loss: 2.6551003456115723
Validation loss: 2.6580492886163856

Epoch: 5| Step: 6
Training loss: 3.0960495471954346
Validation loss: 2.660747412712343

Epoch: 5| Step: 7
Training loss: 2.931295394897461
Validation loss: 2.657342310874693

Epoch: 5| Step: 8
Training loss: 2.7502098083496094
Validation loss: 2.655626761016025

Epoch: 5| Step: 9
Training loss: 2.7405025959014893
Validation loss: 2.656440960463657

Epoch: 5| Step: 10
Training loss: 2.6254420280456543
Validation loss: 2.657489038282825

Epoch: 144| Step: 0
Training loss: 3.3471553325653076
Validation loss: 2.65706209469867

Epoch: 5| Step: 1
Training loss: 2.7036385536193848
Validation loss: 2.6597830300690024

Epoch: 5| Step: 2
Training loss: 2.1860904693603516
Validation loss: 2.653914277271558

Epoch: 5| Step: 3
Training loss: 2.5183956623077393
Validation loss: 2.658619370511783

Epoch: 5| Step: 4
Training loss: 3.1968162059783936
Validation loss: 2.655409551435901

Epoch: 5| Step: 5
Training loss: 2.706242799758911
Validation loss: 2.659118375470561

Epoch: 5| Step: 6
Training loss: 2.5054874420166016
Validation loss: 2.6569301800061296

Epoch: 5| Step: 7
Training loss: 3.0920517444610596
Validation loss: 2.655256389289774

Epoch: 5| Step: 8
Training loss: 3.328692674636841
Validation loss: 2.65302001789052

Epoch: 5| Step: 9
Training loss: 2.299245595932007
Validation loss: 2.6542474326267036

Epoch: 5| Step: 10
Training loss: 2.972384214401245
Validation loss: 2.656524012165685

Epoch: 145| Step: 0
Training loss: 2.7551698684692383
Validation loss: 2.656052804762317

Epoch: 5| Step: 1
Training loss: 2.6682865619659424
Validation loss: 2.658059832870319

Epoch: 5| Step: 2
Training loss: 2.5189290046691895
Validation loss: 2.66037905088035

Epoch: 5| Step: 3
Training loss: 3.7239768505096436
Validation loss: 2.6593299424776466

Epoch: 5| Step: 4
Training loss: 1.8095916509628296
Validation loss: 2.6604980781514156

Epoch: 5| Step: 5
Training loss: 3.1536343097686768
Validation loss: 2.662253964331842

Epoch: 5| Step: 6
Training loss: 3.6454055309295654
Validation loss: 2.657594137294318

Epoch: 5| Step: 7
Training loss: 2.4075589179992676
Validation loss: 2.6611474790880756

Epoch: 5| Step: 8
Training loss: 2.605304479598999
Validation loss: 2.657683285333777

Epoch: 5| Step: 9
Training loss: 2.5405936241149902
Validation loss: 2.656156988554103

Epoch: 5| Step: 10
Training loss: 3.028167247772217
Validation loss: 2.658026505542058

Epoch: 146| Step: 0
Training loss: 3.0778160095214844
Validation loss: 2.6569576622337423

Epoch: 5| Step: 1
Training loss: 3.054560899734497
Validation loss: 2.658651982584307

Epoch: 5| Step: 2
Training loss: 2.9118735790252686
Validation loss: 2.6643793275279384

Epoch: 5| Step: 3
Training loss: 3.0538032054901123
Validation loss: 2.664393799279326

Epoch: 5| Step: 4
Training loss: 2.756443738937378
Validation loss: 2.667095007434968

Epoch: 5| Step: 5
Training loss: 2.882847309112549
Validation loss: 2.6658608118693032

Epoch: 5| Step: 6
Training loss: 2.2322890758514404
Validation loss: 2.6601435522879324

Epoch: 5| Step: 7
Training loss: 2.9359803199768066
Validation loss: 2.670214278723604

Epoch: 5| Step: 8
Training loss: 2.420958995819092
Validation loss: 2.665107445050311

Epoch: 5| Step: 9
Training loss: 2.8575189113616943
Validation loss: 2.6682075992707284

Epoch: 5| Step: 10
Training loss: 2.647031784057617
Validation loss: 2.656861505200786

Epoch: 147| Step: 0
Training loss: 2.3279826641082764
Validation loss: 2.6484104766640613

Epoch: 5| Step: 1
Training loss: 3.2312235832214355
Validation loss: 2.6514896474858767

Epoch: 5| Step: 2
Training loss: 2.675373077392578
Validation loss: 2.6552359545102684

Epoch: 5| Step: 3
Training loss: 2.6081175804138184
Validation loss: 2.655469668808804

Epoch: 5| Step: 4
Training loss: 2.800039052963257
Validation loss: 2.6564917872028966

Epoch: 5| Step: 5
Training loss: 3.6807334423065186
Validation loss: 2.664864314499722

Epoch: 5| Step: 6
Training loss: 2.782097101211548
Validation loss: 2.6678831615755634

Epoch: 5| Step: 7
Training loss: 3.3280937671661377
Validation loss: 2.6659294405291156

Epoch: 5| Step: 8
Training loss: 2.707041025161743
Validation loss: 2.666845488291915

Epoch: 5| Step: 9
Training loss: 2.351564645767212
Validation loss: 2.6611303949868805

Epoch: 5| Step: 10
Training loss: 2.277139186859131
Validation loss: 2.657726962079284

Epoch: 148| Step: 0
Training loss: 3.506493330001831
Validation loss: 2.656960554020379

Epoch: 5| Step: 1
Training loss: 3.317262649536133
Validation loss: 2.6484603446017028

Epoch: 5| Step: 2
Training loss: 2.4303698539733887
Validation loss: 2.649256472946495

Epoch: 5| Step: 3
Training loss: 2.9401144981384277
Validation loss: 2.646636155343825

Epoch: 5| Step: 4
Training loss: 2.1112825870513916
Validation loss: 2.646677465849025

Epoch: 5| Step: 5
Training loss: 2.837808609008789
Validation loss: 2.650864608826176

Epoch: 5| Step: 6
Training loss: 2.840151309967041
Validation loss: 2.6605713752008255

Epoch: 5| Step: 7
Training loss: 2.5647835731506348
Validation loss: 2.667526388681063

Epoch: 5| Step: 8
Training loss: 2.9211058616638184
Validation loss: 2.669666164664812

Epoch: 5| Step: 9
Training loss: 2.7453150749206543
Validation loss: 2.673597402470086

Epoch: 5| Step: 10
Training loss: 2.4975428581237793
Validation loss: 2.6633968635271956

Epoch: 149| Step: 0
Training loss: 2.993507146835327
Validation loss: 2.6608208815256753

Epoch: 5| Step: 1
Training loss: 2.6669998168945312
Validation loss: 2.65468902485345

Epoch: 5| Step: 2
Training loss: 3.2961151599884033
Validation loss: 2.654045933036394

Epoch: 5| Step: 3
Training loss: 2.4071803092956543
Validation loss: 2.6518625469617945

Epoch: 5| Step: 4
Training loss: 2.3000879287719727
Validation loss: 2.6505791397504908

Epoch: 5| Step: 5
Training loss: 3.1663622856140137
Validation loss: 2.6442993789590816

Epoch: 5| Step: 6
Training loss: 2.3197262287139893
Validation loss: 2.649157788163872

Epoch: 5| Step: 7
Training loss: 2.594782590866089
Validation loss: 2.646967316186556

Epoch: 5| Step: 8
Training loss: 2.224372386932373
Validation loss: 2.6446375129043416

Epoch: 5| Step: 9
Training loss: 3.3462398052215576
Validation loss: 2.6449277529152493

Epoch: 5| Step: 10
Training loss: 3.5268967151641846
Validation loss: 2.642140749962099

Epoch: 150| Step: 0
Training loss: 3.0022387504577637
Validation loss: 2.6478667182307087

Epoch: 5| Step: 1
Training loss: 3.1243674755096436
Validation loss: 2.64587676653298

Epoch: 5| Step: 2
Training loss: 2.813629627227783
Validation loss: 2.645001721638505

Epoch: 5| Step: 3
Training loss: 3.156466007232666
Validation loss: 2.6485541379579933

Epoch: 5| Step: 4
Training loss: 2.9072506427764893
Validation loss: 2.6434901247742357

Epoch: 5| Step: 5
Training loss: 2.928737163543701
Validation loss: 2.648221400476271

Epoch: 5| Step: 6
Training loss: 2.4839959144592285
Validation loss: 2.645218062144454

Epoch: 5| Step: 7
Training loss: 2.5341386795043945
Validation loss: 2.6484704274003223

Epoch: 5| Step: 8
Training loss: 3.052751064300537
Validation loss: 2.645924296430362

Epoch: 5| Step: 9
Training loss: 2.3064768314361572
Validation loss: 2.647935962164274

Epoch: 5| Step: 10
Training loss: 2.3266148567199707
Validation loss: 2.6499481560081564

Epoch: 151| Step: 0
Training loss: 3.1821072101593018
Validation loss: 2.6489370253778275

Epoch: 5| Step: 1
Training loss: 2.0017411708831787
Validation loss: 2.651211546313378

Epoch: 5| Step: 2
Training loss: 2.5779216289520264
Validation loss: 2.647787840135636

Epoch: 5| Step: 3
Training loss: 2.509105920791626
Validation loss: 2.645358531705795

Epoch: 5| Step: 4
Training loss: 3.058837413787842
Validation loss: 2.6474171261633597

Epoch: 5| Step: 5
Training loss: 3.356229305267334
Validation loss: 2.6412046981114212

Epoch: 5| Step: 6
Training loss: 3.2754409313201904
Validation loss: 2.644955322306643

Epoch: 5| Step: 7
Training loss: 3.267894744873047
Validation loss: 2.6457954042701313

Epoch: 5| Step: 8
Training loss: 2.358508586883545
Validation loss: 2.647839310348675

Epoch: 5| Step: 9
Training loss: 2.602717399597168
Validation loss: 2.646360033301897

Epoch: 5| Step: 10
Training loss: 2.370683431625366
Validation loss: 2.652337489589568

Epoch: 152| Step: 0
Training loss: 3.318920850753784
Validation loss: 2.649790995864458

Epoch: 5| Step: 1
Training loss: 2.8543953895568848
Validation loss: 2.650067662680021

Epoch: 5| Step: 2
Training loss: 2.6953816413879395
Validation loss: 2.655532944586969

Epoch: 5| Step: 3
Training loss: 2.7680130004882812
Validation loss: 2.6529228712922786

Epoch: 5| Step: 4
Training loss: 2.6094958782196045
Validation loss: 2.651031491576984

Epoch: 5| Step: 5
Training loss: 2.7875888347625732
Validation loss: 2.652656908958189

Epoch: 5| Step: 6
Training loss: 2.461975336074829
Validation loss: 2.649700698032174

Epoch: 5| Step: 7
Training loss: 2.809507131576538
Validation loss: 2.6439131511154996

Epoch: 5| Step: 8
Training loss: 2.657468795776367
Validation loss: 2.6510788407377017

Epoch: 5| Step: 9
Training loss: 3.2778944969177246
Validation loss: 2.6579667880970943

Epoch: 5| Step: 10
Training loss: 2.4033384323120117
Validation loss: 2.679821683514503

Epoch: 153| Step: 0
Training loss: 2.6093461513519287
Validation loss: 2.692697166114725

Epoch: 5| Step: 1
Training loss: 2.517458438873291
Validation loss: 2.718628519324846

Epoch: 5| Step: 2
Training loss: 3.381243944168091
Validation loss: 2.7259377920499412

Epoch: 5| Step: 3
Training loss: 3.0555005073547363
Validation loss: 2.73276440558895

Epoch: 5| Step: 4
Training loss: 2.7035956382751465
Validation loss: 2.736963067003476

Epoch: 5| Step: 5
Training loss: 3.097918748855591
Validation loss: 2.7177012171796573

Epoch: 5| Step: 6
Training loss: 2.492553472518921
Validation loss: 2.704335005052628

Epoch: 5| Step: 7
Training loss: 3.0341320037841797
Validation loss: 2.6843097927749797

Epoch: 5| Step: 8
Training loss: 2.9987475872039795
Validation loss: 2.6862464822748655

Epoch: 5| Step: 9
Training loss: 2.7970359325408936
Validation loss: 2.6574436438980924

Epoch: 5| Step: 10
Training loss: 2.303903818130493
Validation loss: 2.657729430865216

Epoch: 154| Step: 0
Training loss: 2.423781156539917
Validation loss: 2.6569383631470385

Epoch: 5| Step: 1
Training loss: 2.4155571460723877
Validation loss: 2.663300411675566

Epoch: 5| Step: 2
Training loss: 3.4197068214416504
Validation loss: 2.6646405932723836

Epoch: 5| Step: 3
Training loss: 2.212374448776245
Validation loss: 2.660199511435724

Epoch: 5| Step: 4
Training loss: 3.375782012939453
Validation loss: 2.6570273599317

Epoch: 5| Step: 5
Training loss: 3.5525097846984863
Validation loss: 2.64969547845984

Epoch: 5| Step: 6
Training loss: 1.7043752670288086
Validation loss: 2.6439030298622708

Epoch: 5| Step: 7
Training loss: 3.522434949874878
Validation loss: 2.637942824312436

Epoch: 5| Step: 8
Training loss: 3.487395763397217
Validation loss: 2.6372590911003853

Epoch: 5| Step: 9
Training loss: 1.8820631504058838
Validation loss: 2.636514163786365

Epoch: 5| Step: 10
Training loss: 2.7602498531341553
Validation loss: 2.63249679534666

Epoch: 155| Step: 0
Training loss: 3.0485966205596924
Validation loss: 2.6334219747974026

Epoch: 5| Step: 1
Training loss: 2.194678544998169
Validation loss: 2.6381491076561714

Epoch: 5| Step: 2
Training loss: 3.196887731552124
Validation loss: 2.6371513310299126

Epoch: 5| Step: 3
Training loss: 2.903393507003784
Validation loss: 2.63727907980642

Epoch: 5| Step: 4
Training loss: 3.2293410301208496
Validation loss: 2.637969029847012

Epoch: 5| Step: 5
Training loss: 1.7065801620483398
Validation loss: 2.634351399637038

Epoch: 5| Step: 6
Training loss: 2.9678072929382324
Validation loss: 2.633025689791608

Epoch: 5| Step: 7
Training loss: 3.3381400108337402
Validation loss: 2.6351067071319907

Epoch: 5| Step: 8
Training loss: 2.541586399078369
Validation loss: 2.634273677743891

Epoch: 5| Step: 9
Training loss: 2.3914544582366943
Validation loss: 2.637968409445978

Epoch: 5| Step: 10
Training loss: 3.2772576808929443
Validation loss: 2.636194388071696

Epoch: 156| Step: 0
Training loss: 2.625249147415161
Validation loss: 2.632713420416719

Epoch: 5| Step: 1
Training loss: 2.637320041656494
Validation loss: 2.6327819439672653

Epoch: 5| Step: 2
Training loss: 2.0477042198181152
Validation loss: 2.6297677511809976

Epoch: 5| Step: 3
Training loss: 3.568502426147461
Validation loss: 2.6317493018283638

Epoch: 5| Step: 4
Training loss: 2.5080645084381104
Validation loss: 2.6275313541453373

Epoch: 5| Step: 5
Training loss: 2.7052979469299316
Validation loss: 2.6332526322334044

Epoch: 5| Step: 6
Training loss: 3.164799451828003
Validation loss: 2.64409996873589

Epoch: 5| Step: 7
Training loss: 2.965768337249756
Validation loss: 2.640643024957308

Epoch: 5| Step: 8
Training loss: 2.8506596088409424
Validation loss: 2.6358866922317015

Epoch: 5| Step: 9
Training loss: 2.866302251815796
Validation loss: 2.635515559104181

Epoch: 5| Step: 10
Training loss: 2.743828773498535
Validation loss: 2.6266186237335205

Epoch: 157| Step: 0
Training loss: 2.7622830867767334
Validation loss: 2.626547077650665

Epoch: 5| Step: 1
Training loss: 2.3589682579040527
Validation loss: 2.6310502893181256

Epoch: 5| Step: 2
Training loss: 2.599403142929077
Validation loss: 2.630936412401097

Epoch: 5| Step: 3
Training loss: 2.815565586090088
Validation loss: 2.63265997107311

Epoch: 5| Step: 4
Training loss: 2.3480844497680664
Validation loss: 2.6312458233166764

Epoch: 5| Step: 5
Training loss: 2.6708412170410156
Validation loss: 2.6336856554913264

Epoch: 5| Step: 6
Training loss: 3.053926467895508
Validation loss: 2.637344885897893

Epoch: 5| Step: 7
Training loss: 3.320565700531006
Validation loss: 2.6395727767739245

Epoch: 5| Step: 8
Training loss: 3.4161486625671387
Validation loss: 2.6331041474496164

Epoch: 5| Step: 9
Training loss: 2.4657018184661865
Validation loss: 2.6334484213141987

Epoch: 5| Step: 10
Training loss: 2.8649661540985107
Validation loss: 2.630040937854398

Epoch: 158| Step: 0
Training loss: 3.148550271987915
Validation loss: 2.630483017172865

Epoch: 5| Step: 1
Training loss: 2.868075370788574
Validation loss: 2.6310801788042952

Epoch: 5| Step: 2
Training loss: 2.7900452613830566
Validation loss: 2.6297638672654347

Epoch: 5| Step: 3
Training loss: 2.761417865753174
Validation loss: 2.6346569086915705

Epoch: 5| Step: 4
Training loss: 2.7333128452301025
Validation loss: 2.6381604953478743

Epoch: 5| Step: 5
Training loss: 3.2071545124053955
Validation loss: 2.634004913350587

Epoch: 5| Step: 6
Training loss: 2.103353500366211
Validation loss: 2.6410161423426803

Epoch: 5| Step: 7
Training loss: 2.99583101272583
Validation loss: 2.6454194848255446

Epoch: 5| Step: 8
Training loss: 2.6275200843811035
Validation loss: 2.6546951699000534

Epoch: 5| Step: 9
Training loss: 2.545875072479248
Validation loss: 2.634736289260208

Epoch: 5| Step: 10
Training loss: 2.813352584838867
Validation loss: 2.6344629000591975

Epoch: 159| Step: 0
Training loss: 2.1682422161102295
Validation loss: 2.6310104939245407

Epoch: 5| Step: 1
Training loss: 3.5008304119110107
Validation loss: 2.626278854185535

Epoch: 5| Step: 2
Training loss: 2.3652091026306152
Validation loss: 2.626803400695965

Epoch: 5| Step: 3
Training loss: 2.605685234069824
Validation loss: 2.6308564293769097

Epoch: 5| Step: 4
Training loss: 2.6958460807800293
Validation loss: 2.625590714075232

Epoch: 5| Step: 5
Training loss: 3.2577743530273438
Validation loss: 2.6312949734349407

Epoch: 5| Step: 6
Training loss: 2.3216614723205566
Validation loss: 2.6250369112978698

Epoch: 5| Step: 7
Training loss: 2.662911891937256
Validation loss: 2.6251340014960176

Epoch: 5| Step: 8
Training loss: 3.063523769378662
Validation loss: 2.6418170826409453

Epoch: 5| Step: 9
Training loss: 2.9597935676574707
Validation loss: 2.6578407877235004

Epoch: 5| Step: 10
Training loss: 2.977994680404663
Validation loss: 2.6694849075809604

Epoch: 160| Step: 0
Training loss: 2.066300392150879
Validation loss: 2.6743308216012935

Epoch: 5| Step: 1
Training loss: 3.0731043815612793
Validation loss: 2.6785515610889723

Epoch: 5| Step: 2
Training loss: 2.404125452041626
Validation loss: 2.6537613458530878

Epoch: 5| Step: 3
Training loss: 2.8993988037109375
Validation loss: 2.648396656077395

Epoch: 5| Step: 4
Training loss: 2.8291797637939453
Validation loss: 2.640469968959849

Epoch: 5| Step: 5
Training loss: 2.366673707962036
Validation loss: 2.642700385021907

Epoch: 5| Step: 6
Training loss: 2.91413950920105
Validation loss: 2.6353783094754784

Epoch: 5| Step: 7
Training loss: 2.817094087600708
Validation loss: 2.62822655195831

Epoch: 5| Step: 8
Training loss: 3.5002455711364746
Validation loss: 2.630779981613159

Epoch: 5| Step: 9
Training loss: 2.3056838512420654
Validation loss: 2.628675837670603

Epoch: 5| Step: 10
Training loss: 3.527923583984375
Validation loss: 2.622819721057851

Epoch: 161| Step: 0
Training loss: 2.440761089324951
Validation loss: 2.6361602942148843

Epoch: 5| Step: 1
Training loss: 2.718489646911621
Validation loss: 2.6357554184493197

Epoch: 5| Step: 2
Training loss: 3.0263357162475586
Validation loss: 2.6343559295900407

Epoch: 5| Step: 3
Training loss: 2.887378692626953
Validation loss: 2.635560594579225

Epoch: 5| Step: 4
Training loss: 2.2289528846740723
Validation loss: 2.636701060879615

Epoch: 5| Step: 5
Training loss: 3.1891074180603027
Validation loss: 2.6328236185094362

Epoch: 5| Step: 6
Training loss: 2.747528076171875
Validation loss: 2.6274335820187806

Epoch: 5| Step: 7
Training loss: 2.4449517726898193
Validation loss: 2.627587692711943

Epoch: 5| Step: 8
Training loss: 2.284761905670166
Validation loss: 2.6281563594777095

Epoch: 5| Step: 9
Training loss: 3.36883544921875
Validation loss: 2.6326650778452554

Epoch: 5| Step: 10
Training loss: 3.237894058227539
Validation loss: 2.6431951830464024

Epoch: 162| Step: 0
Training loss: 3.1838138103485107
Validation loss: 2.647676508913758

Epoch: 5| Step: 1
Training loss: 3.1136465072631836
Validation loss: 2.6433097700918875

Epoch: 5| Step: 2
Training loss: 2.199162006378174
Validation loss: 2.6392834237826768

Epoch: 5| Step: 3
Training loss: 3.0454468727111816
Validation loss: 2.636185082056189

Epoch: 5| Step: 4
Training loss: 2.682119846343994
Validation loss: 2.6378132348419516

Epoch: 5| Step: 5
Training loss: 3.7415993213653564
Validation loss: 2.634523432741883

Epoch: 5| Step: 6
Training loss: 2.1068546772003174
Validation loss: 2.636757642992081

Epoch: 5| Step: 7
Training loss: 2.1107254028320312
Validation loss: 2.6452624362002135

Epoch: 5| Step: 8
Training loss: 3.0175328254699707
Validation loss: 2.6441480703251337

Epoch: 5| Step: 9
Training loss: 2.044795274734497
Validation loss: 2.6351706340748775

Epoch: 5| Step: 10
Training loss: 3.381558895111084
Validation loss: 2.6298545329801497

Epoch: 163| Step: 0
Training loss: 2.5936131477355957
Validation loss: 2.6329844279955794

Epoch: 5| Step: 1
Training loss: 2.731550931930542
Validation loss: 2.62867263824709

Epoch: 5| Step: 2
Training loss: 2.999812602996826
Validation loss: 2.6335587398980254

Epoch: 5| Step: 3
Training loss: 3.0778098106384277
Validation loss: 2.6303958739003828

Epoch: 5| Step: 4
Training loss: 2.7466464042663574
Validation loss: 2.6369902087796118

Epoch: 5| Step: 5
Training loss: 2.970081329345703
Validation loss: 2.638570967540946

Epoch: 5| Step: 6
Training loss: 2.241237163543701
Validation loss: 2.6432371498436056

Epoch: 5| Step: 7
Training loss: 3.158531665802002
Validation loss: 2.641912744891259

Epoch: 5| Step: 8
Training loss: 3.0832815170288086
Validation loss: 2.6337530151490243

Epoch: 5| Step: 9
Training loss: 2.24259614944458
Validation loss: 2.6259658131548154

Epoch: 5| Step: 10
Training loss: 2.5847599506378174
Validation loss: 2.6181579046351935

Epoch: 164| Step: 0
Training loss: 3.2549328804016113
Validation loss: 2.6202883028214976

Epoch: 5| Step: 1
Training loss: 3.681889295578003
Validation loss: 2.6139974337752148

Epoch: 5| Step: 2
Training loss: 2.255784511566162
Validation loss: 2.6160906848087104

Epoch: 5| Step: 3
Training loss: 2.4123475551605225
Validation loss: 2.617954715605705

Epoch: 5| Step: 4
Training loss: 2.8022284507751465
Validation loss: 2.615242329976892

Epoch: 5| Step: 5
Training loss: 2.6832451820373535
Validation loss: 2.619269169786925

Epoch: 5| Step: 6
Training loss: 2.6823790073394775
Validation loss: 2.616498439542709

Epoch: 5| Step: 7
Training loss: 2.9005815982818604
Validation loss: 2.621582349141439

Epoch: 5| Step: 8
Training loss: 2.4553141593933105
Validation loss: 2.6169601948030534

Epoch: 5| Step: 9
Training loss: 2.5916075706481934
Validation loss: 2.621565111221806

Epoch: 5| Step: 10
Training loss: 2.723712682723999
Validation loss: 2.625020516816006

Epoch: 165| Step: 0
Training loss: 3.5639050006866455
Validation loss: 2.6198653764622186

Epoch: 5| Step: 1
Training loss: 2.716637134552002
Validation loss: 2.6209618558165846

Epoch: 5| Step: 2
Training loss: 3.237008571624756
Validation loss: 2.6150419609521025

Epoch: 5| Step: 3
Training loss: 2.429069995880127
Validation loss: 2.6163004188127417

Epoch: 5| Step: 4
Training loss: 2.5341219902038574
Validation loss: 2.6154093370642713

Epoch: 5| Step: 5
Training loss: 2.814242124557495
Validation loss: 2.6151738294991116

Epoch: 5| Step: 6
Training loss: 2.170806407928467
Validation loss: 2.6095888871018604

Epoch: 5| Step: 7
Training loss: 2.6470160484313965
Validation loss: 2.614905475288309

Epoch: 5| Step: 8
Training loss: 2.7482829093933105
Validation loss: 2.6169992980136665

Epoch: 5| Step: 9
Training loss: 2.714329242706299
Validation loss: 2.614785122615035

Epoch: 5| Step: 10
Training loss: 2.896474838256836
Validation loss: 2.615953622325774

Epoch: 166| Step: 0
Training loss: 2.7028493881225586
Validation loss: 2.6142796880455426

Epoch: 5| Step: 1
Training loss: 2.9935684204101562
Validation loss: 2.6130090477646037

Epoch: 5| Step: 2
Training loss: 2.995725631713867
Validation loss: 2.6114668243674823

Epoch: 5| Step: 3
Training loss: 2.9141576290130615
Validation loss: 2.6135417056340042

Epoch: 5| Step: 4
Training loss: 2.5555081367492676
Validation loss: 2.6130225248234247

Epoch: 5| Step: 5
Training loss: 2.7956128120422363
Validation loss: 2.6101165612538657

Epoch: 5| Step: 6
Training loss: 1.933193564414978
Validation loss: 2.617148555735106

Epoch: 5| Step: 7
Training loss: 2.6881604194641113
Validation loss: 2.6113801899776665

Epoch: 5| Step: 8
Training loss: 3.4341583251953125
Validation loss: 2.6157566091065765

Epoch: 5| Step: 9
Training loss: 2.724480390548706
Validation loss: 2.6183275766270135

Epoch: 5| Step: 10
Training loss: 2.6668636798858643
Validation loss: 2.613549550374349

Epoch: 167| Step: 0
Training loss: 3.590522050857544
Validation loss: 2.615061875312559

Epoch: 5| Step: 1
Training loss: 2.366283416748047
Validation loss: 2.6203310156381256

Epoch: 5| Step: 2
Training loss: 2.51366925239563
Validation loss: 2.618027235872002

Epoch: 5| Step: 3
Training loss: 2.382763385772705
Validation loss: 2.615737553565733

Epoch: 5| Step: 4
Training loss: 2.7975802421569824
Validation loss: 2.614831280964677

Epoch: 5| Step: 5
Training loss: 3.4919075965881348
Validation loss: 2.6136883638238393

Epoch: 5| Step: 6
Training loss: 2.1952905654907227
Validation loss: 2.6149209391686226

Epoch: 5| Step: 7
Training loss: 3.2912955284118652
Validation loss: 2.6162039336337837

Epoch: 5| Step: 8
Training loss: 2.891989231109619
Validation loss: 2.6156796947602303

Epoch: 5| Step: 9
Training loss: 2.2248291969299316
Validation loss: 2.614071725517191

Epoch: 5| Step: 10
Training loss: 2.66225004196167
Validation loss: 2.614724866805538

Epoch: 168| Step: 0
Training loss: 2.720473527908325
Validation loss: 2.620309358002037

Epoch: 5| Step: 1
Training loss: 2.9258716106414795
Validation loss: 2.6168543548994165

Epoch: 5| Step: 2
Training loss: 3.019803762435913
Validation loss: 2.616158608467348

Epoch: 5| Step: 3
Training loss: 3.129117488861084
Validation loss: 2.616297419353198

Epoch: 5| Step: 4
Training loss: 3.666916608810425
Validation loss: 2.6178419820723997

Epoch: 5| Step: 5
Training loss: 2.6523234844207764
Validation loss: 2.6156356206504245

Epoch: 5| Step: 6
Training loss: 2.46628737449646
Validation loss: 2.611625699586766

Epoch: 5| Step: 7
Training loss: 2.0972135066986084
Validation loss: 2.6130472537009948

Epoch: 5| Step: 8
Training loss: 2.089474678039551
Validation loss: 2.6129649800638997

Epoch: 5| Step: 9
Training loss: 3.0413641929626465
Validation loss: 2.6142073882523404

Epoch: 5| Step: 10
Training loss: 2.5629985332489014
Validation loss: 2.6118238049168743

Epoch: 169| Step: 0
Training loss: 2.445979595184326
Validation loss: 2.6096632019166024

Epoch: 5| Step: 1
Training loss: 3.272911787033081
Validation loss: 2.613857066759499

Epoch: 5| Step: 2
Training loss: 2.7693259716033936
Validation loss: 2.6090277907668904

Epoch: 5| Step: 3
Training loss: 3.3632683753967285
Validation loss: 2.6099214784560667

Epoch: 5| Step: 4
Training loss: 2.516932249069214
Validation loss: 2.6172084962168047

Epoch: 5| Step: 5
Training loss: 2.5100440979003906
Validation loss: 2.60964427327597

Epoch: 5| Step: 6
Training loss: 2.325721502304077
Validation loss: 2.6161867482687837

Epoch: 5| Step: 7
Training loss: 2.8966145515441895
Validation loss: 2.615655199173958

Epoch: 5| Step: 8
Training loss: 2.4062514305114746
Validation loss: 2.6164092197213122

Epoch: 5| Step: 9
Training loss: 3.4279625415802
Validation loss: 2.612237056096395

Epoch: 5| Step: 10
Training loss: 2.453127145767212
Validation loss: 2.6108116180666032

Epoch: 170| Step: 0
Training loss: 3.4852607250213623
Validation loss: 2.605521653288154

Epoch: 5| Step: 1
Training loss: 2.443244695663452
Validation loss: 2.613987879086566

Epoch: 5| Step: 2
Training loss: 3.5508551597595215
Validation loss: 2.6166803119003132

Epoch: 5| Step: 3
Training loss: 2.3102335929870605
Validation loss: 2.612156565471362

Epoch: 5| Step: 4
Training loss: 1.7170250415802002
Validation loss: 2.6122464262029177

Epoch: 5| Step: 5
Training loss: 3.0095057487487793
Validation loss: 2.6090480589097544

Epoch: 5| Step: 6
Training loss: 2.7190518379211426
Validation loss: 2.6066935780227825

Epoch: 5| Step: 7
Training loss: 2.6372907161712646
Validation loss: 2.6095035229959795

Epoch: 5| Step: 8
Training loss: 2.8510940074920654
Validation loss: 2.608498170811643

Epoch: 5| Step: 9
Training loss: 2.8733768463134766
Validation loss: 2.607858928301001

Epoch: 5| Step: 10
Training loss: 2.8084843158721924
Validation loss: 2.605336735325475

Epoch: 171| Step: 0
Training loss: 2.4326043128967285
Validation loss: 2.6028513318748883

Epoch: 5| Step: 1
Training loss: 2.0957322120666504
Validation loss: 2.6102815212741977

Epoch: 5| Step: 2
Training loss: 2.7450215816497803
Validation loss: 2.6114521385521017

Epoch: 5| Step: 3
Training loss: 3.0099921226501465
Validation loss: 2.608401829196561

Epoch: 5| Step: 4
Training loss: 3.3340492248535156
Validation loss: 2.612717705388223

Epoch: 5| Step: 5
Training loss: 2.0372440814971924
Validation loss: 2.614341838385469

Epoch: 5| Step: 6
Training loss: 2.28096604347229
Validation loss: 2.614168502951181

Epoch: 5| Step: 7
Training loss: 3.2304294109344482
Validation loss: 2.609306602067845

Epoch: 5| Step: 8
Training loss: 3.2529683113098145
Validation loss: 2.601987754144976

Epoch: 5| Step: 9
Training loss: 2.546008586883545
Validation loss: 2.6032968285263225

Epoch: 5| Step: 10
Training loss: 3.570405960083008
Validation loss: 2.600741700459552

Epoch: 172| Step: 0
Training loss: 2.703134775161743
Validation loss: 2.5952846234844578

Epoch: 5| Step: 1
Training loss: 2.8459653854370117
Validation loss: 2.5993468402534403

Epoch: 5| Step: 2
Training loss: 2.9420838356018066
Validation loss: 2.5960745144915838

Epoch: 5| Step: 3
Training loss: 2.5079431533813477
Validation loss: 2.5981605411857687

Epoch: 5| Step: 4
Training loss: 2.37111234664917
Validation loss: 2.5992655907907793

Epoch: 5| Step: 5
Training loss: 2.4578843116760254
Validation loss: 2.5967524743849233

Epoch: 5| Step: 6
Training loss: 3.1281850337982178
Validation loss: 2.5982233016721663

Epoch: 5| Step: 7
Training loss: 2.9016709327697754
Validation loss: 2.595740261898246

Epoch: 5| Step: 8
Training loss: 2.5319228172302246
Validation loss: 2.596053574674873

Epoch: 5| Step: 9
Training loss: 3.293670177459717
Validation loss: 2.595605452855428

Epoch: 5| Step: 10
Training loss: 2.6268258094787598
Validation loss: 2.5974740751328005

Epoch: 173| Step: 0
Training loss: 2.8509037494659424
Validation loss: 2.5966362594276347

Epoch: 5| Step: 1
Training loss: 2.300304889678955
Validation loss: 2.59927398415022

Epoch: 5| Step: 2
Training loss: 3.0585825443267822
Validation loss: 2.595307614213677

Epoch: 5| Step: 3
Training loss: 2.4863109588623047
Validation loss: 2.600441942932785

Epoch: 5| Step: 4
Training loss: 2.9842629432678223
Validation loss: 2.594113947242819

Epoch: 5| Step: 5
Training loss: 2.020840883255005
Validation loss: 2.5953985491106586

Epoch: 5| Step: 6
Training loss: 3.8424301147460938
Validation loss: 2.6036191089178926

Epoch: 5| Step: 7
Training loss: 2.54758882522583
Validation loss: 2.6039123150610153

Epoch: 5| Step: 8
Training loss: 2.7060751914978027
Validation loss: 2.606616567539912

Epoch: 5| Step: 9
Training loss: 2.8203506469726562
Validation loss: 2.609830657641093

Epoch: 5| Step: 10
Training loss: 2.7403106689453125
Validation loss: 2.608892592050696

Epoch: 174| Step: 0
Training loss: 2.8610424995422363
Validation loss: 2.6044473801889727

Epoch: 5| Step: 1
Training loss: 2.9907124042510986
Validation loss: 2.6157712654400895

Epoch: 5| Step: 2
Training loss: 2.721440553665161
Validation loss: 2.6204848725308656

Epoch: 5| Step: 3
Training loss: 3.360121250152588
Validation loss: 2.620528241639496

Epoch: 5| Step: 4
Training loss: 2.8019936084747314
Validation loss: 2.6166478228825394

Epoch: 5| Step: 5
Training loss: 2.7357211112976074
Validation loss: 2.6103611120613675

Epoch: 5| Step: 6
Training loss: 3.254640579223633
Validation loss: 2.6027290000710437

Epoch: 5| Step: 7
Training loss: 2.66945481300354
Validation loss: 2.593810232736731

Epoch: 5| Step: 8
Training loss: 2.4723470211029053
Validation loss: 2.5891926673150834

Epoch: 5| Step: 9
Training loss: 2.344799041748047
Validation loss: 2.598069152524394

Epoch: 5| Step: 10
Training loss: 2.153686761856079
Validation loss: 2.6019934274817027

Epoch: 175| Step: 0
Training loss: 3.012582302093506
Validation loss: 2.607030786493773

Epoch: 5| Step: 1
Training loss: 2.7619776725769043
Validation loss: 2.610423618747342

Epoch: 5| Step: 2
Training loss: 2.718388319015503
Validation loss: 2.603597169281334

Epoch: 5| Step: 3
Training loss: 2.6924967765808105
Validation loss: 2.59516663705149

Epoch: 5| Step: 4
Training loss: 3.134307861328125
Validation loss: 2.599112231244323

Epoch: 5| Step: 5
Training loss: 2.4223384857177734
Validation loss: 2.5910952578308764

Epoch: 5| Step: 6
Training loss: 2.443878173828125
Validation loss: 2.592996992090697

Epoch: 5| Step: 7
Training loss: 3.046875
Validation loss: 2.592867907657418

Epoch: 5| Step: 8
Training loss: 2.3628005981445312
Validation loss: 2.5915076066088933

Epoch: 5| Step: 9
Training loss: 2.5327353477478027
Validation loss: 2.5895925465450493

Epoch: 5| Step: 10
Training loss: 3.3542120456695557
Validation loss: 2.5903540119048087

Epoch: 176| Step: 0
Training loss: 2.8704497814178467
Validation loss: 2.5906592274224884

Epoch: 5| Step: 1
Training loss: 2.5328972339630127
Validation loss: 2.589849784810056

Epoch: 5| Step: 2
Training loss: 2.687880039215088
Validation loss: 2.588910141298848

Epoch: 5| Step: 3
Training loss: 3.2290732860565186
Validation loss: 2.592184107790711

Epoch: 5| Step: 4
Training loss: 2.540458917617798
Validation loss: 2.593239276639877

Epoch: 5| Step: 5
Training loss: 3.4079270362854004
Validation loss: 2.5932932464025353

Epoch: 5| Step: 6
Training loss: 2.921262264251709
Validation loss: 2.593373780609459

Epoch: 5| Step: 7
Training loss: 2.3427767753601074
Validation loss: 2.5888516467104674

Epoch: 5| Step: 8
Training loss: 2.3579766750335693
Validation loss: 2.5932833994588544

Epoch: 5| Step: 9
Training loss: 2.478519916534424
Validation loss: 2.5917080909975114

Epoch: 5| Step: 10
Training loss: 2.9736738204956055
Validation loss: 2.58987243201143

Epoch: 177| Step: 0
Training loss: 3.3198444843292236
Validation loss: 2.5916320636708248

Epoch: 5| Step: 1
Training loss: 2.5162715911865234
Validation loss: 2.594275669385028

Epoch: 5| Step: 2
Training loss: 2.908078670501709
Validation loss: 2.5936577243189656

Epoch: 5| Step: 3
Training loss: 2.838986873626709
Validation loss: 2.5946737899575183

Epoch: 5| Step: 4
Training loss: 2.8522744178771973
Validation loss: 2.5896245946166334

Epoch: 5| Step: 5
Training loss: 3.0598340034484863
Validation loss: 2.591821055258474

Epoch: 5| Step: 6
Training loss: 2.724635362625122
Validation loss: 2.597557411398939

Epoch: 5| Step: 7
Training loss: 2.5365262031555176
Validation loss: 2.5998426868069555

Epoch: 5| Step: 8
Training loss: 2.090053081512451
Validation loss: 2.6055847034659436

Epoch: 5| Step: 9
Training loss: 2.831881046295166
Validation loss: 2.6060902175082954

Epoch: 5| Step: 10
Training loss: 2.7170212268829346
Validation loss: 2.5961028401569655

Epoch: 178| Step: 0
Training loss: 2.987379550933838
Validation loss: 2.5927437787414878

Epoch: 5| Step: 1
Training loss: 2.138005256652832
Validation loss: 2.5917721486860708

Epoch: 5| Step: 2
Training loss: 2.2126734256744385
Validation loss: 2.589960039302867

Epoch: 5| Step: 3
Training loss: 2.7297096252441406
Validation loss: 2.586999770133726

Epoch: 5| Step: 4
Training loss: 2.805817127227783
Validation loss: 2.593973908373105

Epoch: 5| Step: 5
Training loss: 3.306992292404175
Validation loss: 2.5907142751960346

Epoch: 5| Step: 6
Training loss: 2.8038318157196045
Validation loss: 2.595910697854975

Epoch: 5| Step: 7
Training loss: 2.882598876953125
Validation loss: 2.6007449729468233

Epoch: 5| Step: 8
Training loss: 2.899998188018799
Validation loss: 2.593805438728743

Epoch: 5| Step: 9
Training loss: 2.52864933013916
Validation loss: 2.5894336341529764

Epoch: 5| Step: 10
Training loss: 3.0337131023406982
Validation loss: 2.591323414156514

Epoch: 179| Step: 0
Training loss: 2.1393418312072754
Validation loss: 2.5879715668257846

Epoch: 5| Step: 1
Training loss: 2.8657283782958984
Validation loss: 2.5875846890993017

Epoch: 5| Step: 2
Training loss: 2.8394970893859863
Validation loss: 2.5812141613293718

Epoch: 5| Step: 3
Training loss: 2.899949550628662
Validation loss: 2.5860794000728156

Epoch: 5| Step: 4
Training loss: 2.2811710834503174
Validation loss: 2.5841005438117572

Epoch: 5| Step: 5
Training loss: 3.3118534088134766
Validation loss: 2.5795639791796283

Epoch: 5| Step: 6
Training loss: 2.4584174156188965
Validation loss: 2.5841171510757937

Epoch: 5| Step: 7
Training loss: 3.155613899230957
Validation loss: 2.592062450224353

Epoch: 5| Step: 8
Training loss: 2.737393617630005
Validation loss: 2.5936191082000732

Epoch: 5| Step: 9
Training loss: 2.3087868690490723
Validation loss: 2.5957369394199823

Epoch: 5| Step: 10
Training loss: 3.4245641231536865
Validation loss: 2.602536342477286

Epoch: 180| Step: 0
Training loss: 2.736335277557373
Validation loss: 2.5970140913481354

Epoch: 5| Step: 1
Training loss: 2.4883339405059814
Validation loss: 2.5947280955571

Epoch: 5| Step: 2
Training loss: 2.4352803230285645
Validation loss: 2.598384023994528

Epoch: 5| Step: 3
Training loss: 2.7530198097229004
Validation loss: 2.6008951894698606

Epoch: 5| Step: 4
Training loss: 2.7030932903289795
Validation loss: 2.600010402740971

Epoch: 5| Step: 5
Training loss: 2.87776780128479
Validation loss: 2.5943068663279214

Epoch: 5| Step: 6
Training loss: 2.4858558177948
Validation loss: 2.5938649305733303

Epoch: 5| Step: 7
Training loss: 3.547558546066284
Validation loss: 2.5943025568480134

Epoch: 5| Step: 8
Training loss: 2.7426857948303223
Validation loss: 2.59381773138559

Epoch: 5| Step: 9
Training loss: 2.2445735931396484
Validation loss: 2.5904148727334957

Epoch: 5| Step: 10
Training loss: 3.334803819656372
Validation loss: 2.588157182098717

Epoch: 181| Step: 0
Training loss: 2.6133294105529785
Validation loss: 2.588615399534984

Epoch: 5| Step: 1
Training loss: 3.2681727409362793
Validation loss: 2.597028696408836

Epoch: 5| Step: 2
Training loss: 2.538144588470459
Validation loss: 2.5980198332058486

Epoch: 5| Step: 3
Training loss: 2.7250454425811768
Validation loss: 2.5911344456416305

Epoch: 5| Step: 4
Training loss: 2.5535194873809814
Validation loss: 2.5989190865588445

Epoch: 5| Step: 5
Training loss: 2.710860013961792
Validation loss: 2.601669849887971

Epoch: 5| Step: 6
Training loss: 2.39217472076416
Validation loss: 2.604321536197457

Epoch: 5| Step: 7
Training loss: 2.9437572956085205
Validation loss: 2.604484265850436

Epoch: 5| Step: 8
Training loss: 2.857485055923462
Validation loss: 2.6094076120725243

Epoch: 5| Step: 9
Training loss: 2.6939384937286377
Validation loss: 2.6039307296917005

Epoch: 5| Step: 10
Training loss: 2.9405195713043213
Validation loss: 2.5954056068133284

Epoch: 182| Step: 0
Training loss: 3.2061867713928223
Validation loss: 2.598970441408055

Epoch: 5| Step: 1
Training loss: 3.0564589500427246
Validation loss: 2.610128605237571

Epoch: 5| Step: 2
Training loss: 2.9555907249450684
Validation loss: 2.6024314844480125

Epoch: 5| Step: 3
Training loss: 2.151606559753418
Validation loss: 2.598897562232069

Epoch: 5| Step: 4
Training loss: 2.1954116821289062
Validation loss: 2.5985876001337522

Epoch: 5| Step: 5
Training loss: 3.197315216064453
Validation loss: 2.5972336415321595

Epoch: 5| Step: 6
Training loss: 2.5633797645568848
Validation loss: 2.599514228041454

Epoch: 5| Step: 7
Training loss: 3.1139068603515625
Validation loss: 2.602207976002847

Epoch: 5| Step: 8
Training loss: 2.790675401687622
Validation loss: 2.6112663053697154

Epoch: 5| Step: 9
Training loss: 2.7629451751708984
Validation loss: 2.6094257152208717

Epoch: 5| Step: 10
Training loss: 2.1365280151367188
Validation loss: 2.6054461463805167

Epoch: 183| Step: 0
Training loss: 2.182143211364746
Validation loss: 2.625960032145182

Epoch: 5| Step: 1
Training loss: 2.6528000831604004
Validation loss: 2.6056035410973335

Epoch: 5| Step: 2
Training loss: 3.0688796043395996
Validation loss: 2.610751641693936

Epoch: 5| Step: 3
Training loss: 3.40999174118042
Validation loss: 2.6113999941015757

Epoch: 5| Step: 4
Training loss: 3.125553846359253
Validation loss: 2.6072591197106147

Epoch: 5| Step: 5
Training loss: 2.425126791000366
Validation loss: 2.6063097420559136

Epoch: 5| Step: 6
Training loss: 1.9181846380233765
Validation loss: 2.6010375279252247

Epoch: 5| Step: 7
Training loss: 2.515871047973633
Validation loss: 2.6018148237659084

Epoch: 5| Step: 8
Training loss: 3.02081561088562
Validation loss: 2.599183423544771

Epoch: 5| Step: 9
Training loss: 2.3889596462249756
Validation loss: 2.5893087079448085

Epoch: 5| Step: 10
Training loss: 3.7186520099639893
Validation loss: 2.585342243153562

Epoch: 184| Step: 0
Training loss: 2.3401057720184326
Validation loss: 2.581328579174575

Epoch: 5| Step: 1
Training loss: 2.9161765575408936
Validation loss: 2.5816732991126274

Epoch: 5| Step: 2
Training loss: 2.609353542327881
Validation loss: 2.5785391689628683

Epoch: 5| Step: 3
Training loss: 3.015561819076538
Validation loss: 2.5805406467888945

Epoch: 5| Step: 4
Training loss: 2.191105842590332
Validation loss: 2.583139158064319

Epoch: 5| Step: 5
Training loss: 2.490626811981201
Validation loss: 2.5811096314461

Epoch: 5| Step: 6
Training loss: 2.3724803924560547
Validation loss: 2.5840546700262252

Epoch: 5| Step: 7
Training loss: 2.731785297393799
Validation loss: 2.577266000932263

Epoch: 5| Step: 8
Training loss: 3.2401442527770996
Validation loss: 2.579694858161352

Epoch: 5| Step: 9
Training loss: 3.4837310314178467
Validation loss: 2.576379411964006

Epoch: 5| Step: 10
Training loss: 2.8209574222564697
Validation loss: 2.5883095187525593

Epoch: 185| Step: 0
Training loss: 2.3194899559020996
Validation loss: 2.5918146999933387

Epoch: 5| Step: 1
Training loss: 2.659639358520508
Validation loss: 2.590593432867399

Epoch: 5| Step: 2
Training loss: 2.7376644611358643
Validation loss: 2.6073480216405724

Epoch: 5| Step: 3
Training loss: 2.7098019123077393
Validation loss: 2.6035149687079975

Epoch: 5| Step: 4
Training loss: 2.3996505737304688
Validation loss: 2.6090747976815827

Epoch: 5| Step: 5
Training loss: 2.5756936073303223
Validation loss: 2.6073129202729914

Epoch: 5| Step: 6
Training loss: 3.2507376670837402
Validation loss: 2.6058016707820277

Epoch: 5| Step: 7
Training loss: 2.3011584281921387
Validation loss: 2.593322438578452

Epoch: 5| Step: 8
Training loss: 3.6197052001953125
Validation loss: 2.5845301958822433

Epoch: 5| Step: 9
Training loss: 2.71144437789917
Validation loss: 2.5826844861430507

Epoch: 5| Step: 10
Training loss: 2.8926305770874023
Validation loss: 2.5792699501078618

Epoch: 186| Step: 0
Training loss: 2.7256321907043457
Validation loss: 2.580713943768573

Epoch: 5| Step: 1
Training loss: 2.868619441986084
Validation loss: 2.58128696872342

Epoch: 5| Step: 2
Training loss: 2.701082229614258
Validation loss: 2.579361056768766

Epoch: 5| Step: 3
Training loss: 3.2484371662139893
Validation loss: 2.583877986477267

Epoch: 5| Step: 4
Training loss: 2.610626697540283
Validation loss: 2.5753787794420795

Epoch: 5| Step: 5
Training loss: 2.1204471588134766
Validation loss: 2.5810445765013337

Epoch: 5| Step: 6
Training loss: 2.250199794769287
Validation loss: 2.5780967179165093

Epoch: 5| Step: 7
Training loss: 2.8616623878479004
Validation loss: 2.58045446231801

Epoch: 5| Step: 8
Training loss: 2.7587602138519287
Validation loss: 2.5772072474161782

Epoch: 5| Step: 9
Training loss: 3.1738367080688477
Validation loss: 2.580091402094851

Epoch: 5| Step: 10
Training loss: 2.7871131896972656
Validation loss: 2.582103116537935

Epoch: 187| Step: 0
Training loss: 3.358062744140625
Validation loss: 2.5764812192609234

Epoch: 5| Step: 1
Training loss: 3.066889762878418
Validation loss: 2.5835637789900585

Epoch: 5| Step: 2
Training loss: 2.6748130321502686
Validation loss: 2.583030431501327

Epoch: 5| Step: 3
Training loss: 2.4097163677215576
Validation loss: 2.5795459106404293

Epoch: 5| Step: 4
Training loss: 2.565030574798584
Validation loss: 2.5795438264005925

Epoch: 5| Step: 5
Training loss: 3.1149508953094482
Validation loss: 2.580228752987359

Epoch: 5| Step: 6
Training loss: 2.9950971603393555
Validation loss: 2.5902438009938886

Epoch: 5| Step: 7
Training loss: 2.3931164741516113
Validation loss: 2.5842935628788446

Epoch: 5| Step: 8
Training loss: 2.3927650451660156
Validation loss: 2.5801834367936656

Epoch: 5| Step: 9
Training loss: 2.706700325012207
Validation loss: 2.579432723342731

Epoch: 5| Step: 10
Training loss: 2.314329147338867
Validation loss: 2.582298876136862

Epoch: 188| Step: 0
Training loss: 2.836333751678467
Validation loss: 2.576186933825093

Epoch: 5| Step: 1
Training loss: 2.6965415477752686
Validation loss: 2.58376431721513

Epoch: 5| Step: 2
Training loss: 2.7150216102600098
Validation loss: 2.5826226947128132

Epoch: 5| Step: 3
Training loss: 2.056490659713745
Validation loss: 2.5808181019239527

Epoch: 5| Step: 4
Training loss: 2.5416016578674316
Validation loss: 2.5811000895756546

Epoch: 5| Step: 5
Training loss: 2.8588850498199463
Validation loss: 2.5830026800914476

Epoch: 5| Step: 6
Training loss: 2.483225107192993
Validation loss: 2.580479503959738

Epoch: 5| Step: 7
Training loss: 3.2640786170959473
Validation loss: 2.58583741546959

Epoch: 5| Step: 8
Training loss: 2.260551691055298
Validation loss: 2.5804564773395495

Epoch: 5| Step: 9
Training loss: 3.4397149085998535
Validation loss: 2.583205046192292

Epoch: 5| Step: 10
Training loss: 2.961510419845581
Validation loss: 2.582234469793176

Epoch: 189| Step: 0
Training loss: 2.3476972579956055
Validation loss: 2.571232541914909

Epoch: 5| Step: 1
Training loss: 3.364586591720581
Validation loss: 2.5728648247257357

Epoch: 5| Step: 2
Training loss: 2.3506274223327637
Validation loss: 2.5621815983967116

Epoch: 5| Step: 3
Training loss: 2.6931991577148438
Validation loss: 2.5642734548097015

Epoch: 5| Step: 4
Training loss: 2.111253261566162
Validation loss: 2.5631537796348653

Epoch: 5| Step: 5
Training loss: 2.341763973236084
Validation loss: 2.566873711924399

Epoch: 5| Step: 6
Training loss: 3.410992383956909
Validation loss: 2.5636911469121135

Epoch: 5| Step: 7
Training loss: 3.001500129699707
Validation loss: 2.5656875692388064

Epoch: 5| Step: 8
Training loss: 2.169804096221924
Validation loss: 2.567077206027123

Epoch: 5| Step: 9
Training loss: 3.675917148590088
Validation loss: 2.571099668420771

Epoch: 5| Step: 10
Training loss: 2.5912370681762695
Validation loss: 2.570933803435295

Epoch: 190| Step: 0
Training loss: 3.1270713806152344
Validation loss: 2.5718160649781585

Epoch: 5| Step: 1
Training loss: 2.0995898246765137
Validation loss: 2.5748847300006497

Epoch: 5| Step: 2
Training loss: 2.618759870529175
Validation loss: 2.5687700215206353

Epoch: 5| Step: 3
Training loss: 2.559685230255127
Validation loss: 2.5713907339239634

Epoch: 5| Step: 4
Training loss: 2.848634958267212
Validation loss: 2.5734954469947406

Epoch: 5| Step: 5
Training loss: 3.4973742961883545
Validation loss: 2.5703387542437484

Epoch: 5| Step: 6
Training loss: 2.6729369163513184
Validation loss: 2.5663733969452562

Epoch: 5| Step: 7
Training loss: 2.579784393310547
Validation loss: 2.578014084087905

Epoch: 5| Step: 8
Training loss: 3.2036807537078857
Validation loss: 2.5735887840229976

Epoch: 5| Step: 9
Training loss: 2.403355836868286
Validation loss: 2.575900780257358

Epoch: 5| Step: 10
Training loss: 2.4063363075256348
Validation loss: 2.5723325232023835

Epoch: 191| Step: 0
Training loss: 3.4794540405273438
Validation loss: 2.5697182032369796

Epoch: 5| Step: 1
Training loss: 2.3347864151000977
Validation loss: 2.5736848538921726

Epoch: 5| Step: 2
Training loss: 2.7317867279052734
Validation loss: 2.5706247168202556

Epoch: 5| Step: 3
Training loss: 3.4107155799865723
Validation loss: 2.566119634976951

Epoch: 5| Step: 4
Training loss: 2.7098498344421387
Validation loss: 2.56989110413418

Epoch: 5| Step: 5
Training loss: 2.348684072494507
Validation loss: 2.569932186475364

Epoch: 5| Step: 6
Training loss: 1.977205514907837
Validation loss: 2.573907898318383

Epoch: 5| Step: 7
Training loss: 3.303349256515503
Validation loss: 2.5768099497723322

Epoch: 5| Step: 8
Training loss: 2.615187168121338
Validation loss: 2.5633667950989096

Epoch: 5| Step: 9
Training loss: 1.978867769241333
Validation loss: 2.5690740898091304

Epoch: 5| Step: 10
Training loss: 3.2327768802642822
Validation loss: 2.561258941568354

Epoch: 192| Step: 0
Training loss: 3.464334011077881
Validation loss: 2.56360799266446

Epoch: 5| Step: 1
Training loss: 2.4036412239074707
Validation loss: 2.5628179132297473

Epoch: 5| Step: 2
Training loss: 3.4178435802459717
Validation loss: 2.573230202480029

Epoch: 5| Step: 3
Training loss: 2.7728590965270996
Validation loss: 2.5732905095623386

Epoch: 5| Step: 4
Training loss: 2.7136549949645996
Validation loss: 2.5773654240433888

Epoch: 5| Step: 5
Training loss: 2.27555513381958
Validation loss: 2.5742786520270893

Epoch: 5| Step: 6
Training loss: 2.559070110321045
Validation loss: 2.5748316780213387

Epoch: 5| Step: 7
Training loss: 2.267634868621826
Validation loss: 2.5732480838734615

Epoch: 5| Step: 8
Training loss: 3.253728151321411
Validation loss: 2.5716938536654235

Epoch: 5| Step: 9
Training loss: 2.9582180976867676
Validation loss: 2.5626246057530886

Epoch: 5| Step: 10
Training loss: 1.8257479667663574
Validation loss: 2.5621098010770735

Epoch: 193| Step: 0
Training loss: 2.397007703781128
Validation loss: 2.564645133992677

Epoch: 5| Step: 1
Training loss: 2.6023240089416504
Validation loss: 2.5623304023537585

Epoch: 5| Step: 2
Training loss: 2.5365796089172363
Validation loss: 2.5618159283873854

Epoch: 5| Step: 3
Training loss: 3.110224485397339
Validation loss: 2.5609781306277037

Epoch: 5| Step: 4
Training loss: 3.3562419414520264
Validation loss: 2.56071670850118

Epoch: 5| Step: 5
Training loss: 2.39334774017334
Validation loss: 2.5643943381565872

Epoch: 5| Step: 6
Training loss: 2.335078239440918
Validation loss: 2.5635746063724643

Epoch: 5| Step: 7
Training loss: 3.021420478820801
Validation loss: 2.56271638665148

Epoch: 5| Step: 8
Training loss: 3.0399322509765625
Validation loss: 2.5655333457454557

Epoch: 5| Step: 9
Training loss: 2.459153175354004
Validation loss: 2.569893693411222

Epoch: 5| Step: 10
Training loss: 2.8256828784942627
Validation loss: 2.568605528082899

Epoch: 194| Step: 0
Training loss: 2.4135396480560303
Validation loss: 2.5710702865354476

Epoch: 5| Step: 1
Training loss: 2.447258710861206
Validation loss: 2.5736348193178893

Epoch: 5| Step: 2
Training loss: 3.095123767852783
Validation loss: 2.573717683874151

Epoch: 5| Step: 3
Training loss: 2.4867708683013916
Validation loss: 2.5679887033277944

Epoch: 5| Step: 4
Training loss: 2.7432944774627686
Validation loss: 2.565692904174969

Epoch: 5| Step: 5
Training loss: 2.8558216094970703
Validation loss: 2.5656746407990814

Epoch: 5| Step: 6
Training loss: 2.6405627727508545
Validation loss: 2.5672309219196277

Epoch: 5| Step: 7
Training loss: 2.785942554473877
Validation loss: 2.565051278760356

Epoch: 5| Step: 8
Training loss: 3.7764713764190674
Validation loss: 2.5716166086094354

Epoch: 5| Step: 9
Training loss: 1.9869880676269531
Validation loss: 2.567425827826223

Epoch: 5| Step: 10
Training loss: 2.8348326683044434
Validation loss: 2.5702922318571355

Epoch: 195| Step: 0
Training loss: 2.5473897457122803
Validation loss: 2.567131511626705

Epoch: 5| Step: 1
Training loss: 3.234084367752075
Validation loss: 2.563807938688545

Epoch: 5| Step: 2
Training loss: 2.9962852001190186
Validation loss: 2.556227389202323

Epoch: 5| Step: 3
Training loss: 2.41241455078125
Validation loss: 2.555583510347592

Epoch: 5| Step: 4
Training loss: 2.861492872238159
Validation loss: 2.5550876509758735

Epoch: 5| Step: 5
Training loss: 2.2430548667907715
Validation loss: 2.5527069799361692

Epoch: 5| Step: 6
Training loss: 2.865025758743286
Validation loss: 2.54991667245024

Epoch: 5| Step: 7
Training loss: 2.6878018379211426
Validation loss: 2.550965937235022

Epoch: 5| Step: 8
Training loss: 2.9999115467071533
Validation loss: 2.563341439411204

Epoch: 5| Step: 9
Training loss: 2.4097743034362793
Validation loss: 2.5524374105597056

Epoch: 5| Step: 10
Training loss: 2.647584915161133
Validation loss: 2.5635330497577624

Epoch: 196| Step: 0
Training loss: 2.128645420074463
Validation loss: 2.5578021336627264

Epoch: 5| Step: 1
Training loss: 2.6712467670440674
Validation loss: 2.548746539700416

Epoch: 5| Step: 2
Training loss: 2.469212055206299
Validation loss: 2.5462208230008363

Epoch: 5| Step: 3
Training loss: 2.5440897941589355
Validation loss: 2.5509397496459303

Epoch: 5| Step: 4
Training loss: 2.986712694168091
Validation loss: 2.5423153420930267

Epoch: 5| Step: 5
Training loss: 2.3832409381866455
Validation loss: 2.54444129749011

Epoch: 5| Step: 6
Training loss: 2.723630905151367
Validation loss: 2.5467449644560456

Epoch: 5| Step: 7
Training loss: 3.3799118995666504
Validation loss: 2.5419699094628774

Epoch: 5| Step: 8
Training loss: 2.9171977043151855
Validation loss: 2.5421671226460445

Epoch: 5| Step: 9
Training loss: 2.422086000442505
Validation loss: 2.5447243272617297

Epoch: 5| Step: 10
Training loss: 3.2131967544555664
Validation loss: 2.5402660228872813

Epoch: 197| Step: 0
Training loss: 3.0832064151763916
Validation loss: 2.5411766575228785

Epoch: 5| Step: 1
Training loss: 2.8615262508392334
Validation loss: 2.5477897633788404

Epoch: 5| Step: 2
Training loss: 3.1506645679473877
Validation loss: 2.5535531992553384

Epoch: 5| Step: 3
Training loss: 2.2324700355529785
Validation loss: 2.539862440478417

Epoch: 5| Step: 4
Training loss: 2.7103819847106934
Validation loss: 2.5392014416315223

Epoch: 5| Step: 5
Training loss: 2.045086622238159
Validation loss: 2.5467214609986994

Epoch: 5| Step: 6
Training loss: 2.9212164878845215
Validation loss: 2.545246026849234

Epoch: 5| Step: 7
Training loss: 2.8437294960021973
Validation loss: 2.5454372795679236

Epoch: 5| Step: 8
Training loss: 2.50626277923584
Validation loss: 2.5487841072902886

Epoch: 5| Step: 9
Training loss: 2.854527473449707
Validation loss: 2.550632776752595

Epoch: 5| Step: 10
Training loss: 2.602910041809082
Validation loss: 2.5651829986162085

Epoch: 198| Step: 0
Training loss: 3.7490570545196533
Validation loss: 2.5484955951731694

Epoch: 5| Step: 1
Training loss: 2.7445926666259766
Validation loss: 2.55116670362411

Epoch: 5| Step: 2
Training loss: 2.6430022716522217
Validation loss: 2.556048441958684

Epoch: 5| Step: 3
Training loss: 2.4820573329925537
Validation loss: 2.5543073569574664

Epoch: 5| Step: 4
Training loss: 2.2700374126434326
Validation loss: 2.558150686243529

Epoch: 5| Step: 5
Training loss: 3.0345711708068848
Validation loss: 2.553781165871569

Epoch: 5| Step: 6
Training loss: 3.060898542404175
Validation loss: 2.5611194410631732

Epoch: 5| Step: 7
Training loss: 2.52384614944458
Validation loss: 2.5571580138257755

Epoch: 5| Step: 8
Training loss: 2.90637469291687
Validation loss: 2.555302071314986

Epoch: 5| Step: 9
Training loss: 2.256801128387451
Validation loss: 2.5591162840525308

Epoch: 5| Step: 10
Training loss: 1.864384651184082
Validation loss: 2.5618996953451507

Epoch: 199| Step: 0
Training loss: 2.70447039604187
Validation loss: 2.5583405302416895

Epoch: 5| Step: 1
Training loss: 2.0098373889923096
Validation loss: 2.549342211856637

Epoch: 5| Step: 2
Training loss: 2.0536625385284424
Validation loss: 2.5519000843007076

Epoch: 5| Step: 3
Training loss: 2.8966052532196045
Validation loss: 2.554451375879267

Epoch: 5| Step: 4
Training loss: 3.055016279220581
Validation loss: 2.5693499503597135

Epoch: 5| Step: 5
Training loss: 3.0099563598632812
Validation loss: 2.5700167379071637

Epoch: 5| Step: 6
Training loss: 3.3354268074035645
Validation loss: 2.5687986291864866

Epoch: 5| Step: 7
Training loss: 3.418064594268799
Validation loss: 2.5684081277539654

Epoch: 5| Step: 8
Training loss: 2.0631601810455322
Validation loss: 2.5462575292074554

Epoch: 5| Step: 9
Training loss: 2.585343837738037
Validation loss: 2.5380154348188833

Epoch: 5| Step: 10
Training loss: 2.765406370162964
Validation loss: 2.542000211695189

Epoch: 200| Step: 0
Training loss: 2.41087007522583
Validation loss: 2.544771340585524

Epoch: 5| Step: 1
Training loss: 1.9044830799102783
Validation loss: 2.5484453888349634

Epoch: 5| Step: 2
Training loss: 2.028722047805786
Validation loss: 2.542698524331534

Epoch: 5| Step: 3
Training loss: 2.6907763481140137
Validation loss: 2.532606796551776

Epoch: 5| Step: 4
Training loss: 3.029064893722534
Validation loss: 2.5384385483239287

Epoch: 5| Step: 5
Training loss: 2.7750415802001953
Validation loss: 2.533559710748734

Epoch: 5| Step: 6
Training loss: 3.4318490028381348
Validation loss: 2.5326496144776702

Epoch: 5| Step: 7
Training loss: 2.3832263946533203
Validation loss: 2.52986216545105

Epoch: 5| Step: 8
Training loss: 3.4770209789276123
Validation loss: 2.5376191241766817

Epoch: 5| Step: 9
Training loss: 2.769141674041748
Validation loss: 2.5292218680022867

Epoch: 5| Step: 10
Training loss: 3.0307986736297607
Validation loss: 2.533204278638286

Epoch: 201| Step: 0
Training loss: 2.4862682819366455
Validation loss: 2.5400478634783017

Epoch: 5| Step: 1
Training loss: 2.423657178878784
Validation loss: 2.5355498662558933

Epoch: 5| Step: 2
Training loss: 2.558581590652466
Validation loss: 2.5332381468947216

Epoch: 5| Step: 3
Training loss: 2.7432215213775635
Validation loss: 2.535806353374194

Epoch: 5| Step: 4
Training loss: 2.441514253616333
Validation loss: 2.532935421953919

Epoch: 5| Step: 5
Training loss: 2.1183457374572754
Validation loss: 2.5318045898150374

Epoch: 5| Step: 6
Training loss: 3.4235129356384277
Validation loss: 2.533926087041055

Epoch: 5| Step: 7
Training loss: 2.935708522796631
Validation loss: 2.5320213123034407

Epoch: 5| Step: 8
Training loss: 2.1434683799743652
Validation loss: 2.531032669928766

Epoch: 5| Step: 9
Training loss: 3.326674699783325
Validation loss: 2.53125092803791

Epoch: 5| Step: 10
Training loss: 3.4005186557769775
Validation loss: 2.5349282756928475

Epoch: 202| Step: 0
Training loss: 2.594028949737549
Validation loss: 2.5291914580970682

Epoch: 5| Step: 1
Training loss: 2.5918350219726562
Validation loss: 2.5366131644095145

Epoch: 5| Step: 2
Training loss: 2.9553635120391846
Validation loss: 2.534492197857108

Epoch: 5| Step: 3
Training loss: 3.2251720428466797
Validation loss: 2.537356081829276

Epoch: 5| Step: 4
Training loss: 2.470794439315796
Validation loss: 2.529427384817472

Epoch: 5| Step: 5
Training loss: 3.0602474212646484
Validation loss: 2.531169299156435

Epoch: 5| Step: 6
Training loss: 2.544934034347534
Validation loss: 2.5290405006818872

Epoch: 5| Step: 7
Training loss: 2.952599287033081
Validation loss: 2.5237357103696434

Epoch: 5| Step: 8
Training loss: 2.4539453983306885
Validation loss: 2.530311502436156

Epoch: 5| Step: 9
Training loss: 2.1943252086639404
Validation loss: 2.532666537069505

Epoch: 5| Step: 10
Training loss: 2.786931276321411
Validation loss: 2.5345030471842778

Epoch: 203| Step: 0
Training loss: 2.5451817512512207
Validation loss: 2.5246491688554005

Epoch: 5| Step: 1
Training loss: 2.0562386512756348
Validation loss: 2.5298268256648893

Epoch: 5| Step: 2
Training loss: 3.3078837394714355
Validation loss: 2.5290613482075353

Epoch: 5| Step: 3
Training loss: 2.871628522872925
Validation loss: 2.5284105680322133

Epoch: 5| Step: 4
Training loss: 2.364771604537964
Validation loss: 2.528078668860979

Epoch: 5| Step: 5
Training loss: 2.4745774269104004
Validation loss: 2.5288267904712307

Epoch: 5| Step: 6
Training loss: 2.7439520359039307
Validation loss: 2.529113015820903

Epoch: 5| Step: 7
Training loss: 2.978806257247925
Validation loss: 2.5284969293943016

Epoch: 5| Step: 8
Training loss: 2.1937248706817627
Validation loss: 2.523568199526879

Epoch: 5| Step: 9
Training loss: 3.3394322395324707
Validation loss: 2.5300884221189763

Epoch: 5| Step: 10
Training loss: 3.0563340187072754
Validation loss: 2.5217932078146164

Epoch: 204| Step: 0
Training loss: 2.891080617904663
Validation loss: 2.526931780640797

Epoch: 5| Step: 1
Training loss: 2.9896774291992188
Validation loss: 2.5314686400915987

Epoch: 5| Step: 2
Training loss: 2.615231990814209
Validation loss: 2.5256751275831655

Epoch: 5| Step: 3
Training loss: 2.827070713043213
Validation loss: 2.5312915976329515

Epoch: 5| Step: 4
Training loss: 2.7785725593566895
Validation loss: 2.528391730400824

Epoch: 5| Step: 5
Training loss: 3.004990816116333
Validation loss: 2.5235253585282194

Epoch: 5| Step: 6
Training loss: 2.5385539531707764
Validation loss: 2.523381999743882

Epoch: 5| Step: 7
Training loss: 2.4303126335144043
Validation loss: 2.5214983929869947

Epoch: 5| Step: 8
Training loss: 2.8163576126098633
Validation loss: 2.52769818100878

Epoch: 5| Step: 9
Training loss: 2.1126580238342285
Validation loss: 2.523623656201106

Epoch: 5| Step: 10
Training loss: 2.898005723953247
Validation loss: 2.523974762167982

Epoch: 205| Step: 0
Training loss: 2.490804672241211
Validation loss: 2.523421149100027

Epoch: 5| Step: 1
Training loss: 3.2689876556396484
Validation loss: 2.528359679765599

Epoch: 5| Step: 2
Training loss: 2.5002098083496094
Validation loss: 2.532573743533063

Epoch: 5| Step: 3
Training loss: 2.7641069889068604
Validation loss: 2.5356607475588397

Epoch: 5| Step: 4
Training loss: 2.899414539337158
Validation loss: 2.5365847490167104

Epoch: 5| Step: 5
Training loss: 2.7688121795654297
Validation loss: 2.5323219888953754

Epoch: 5| Step: 6
Training loss: 2.7862441539764404
Validation loss: 2.5312252967588362

Epoch: 5| Step: 7
Training loss: 2.6824746131896973
Validation loss: 2.5266030475657475

Epoch: 5| Step: 8
Training loss: 2.1341590881347656
Validation loss: 2.525871406319321

Epoch: 5| Step: 9
Training loss: 2.78073787689209
Validation loss: 2.5222898555058304

Epoch: 5| Step: 10
Training loss: 2.785928249359131
Validation loss: 2.523893587050899

Epoch: 206| Step: 0
Training loss: 3.4687771797180176
Validation loss: 2.5238217333311677

Epoch: 5| Step: 1
Training loss: 2.987736701965332
Validation loss: 2.5260145202759774

Epoch: 5| Step: 2
Training loss: 2.73134183883667
Validation loss: 2.5312106301707606

Epoch: 5| Step: 3
Training loss: 2.4490301609039307
Validation loss: 2.544334632094188

Epoch: 5| Step: 4
Training loss: 2.952702045440674
Validation loss: 2.559221734282791

Epoch: 5| Step: 5
Training loss: 3.036512851715088
Validation loss: 2.570334529363981

Epoch: 5| Step: 6
Training loss: 2.0569510459899902
Validation loss: 2.5607936689930577

Epoch: 5| Step: 7
Training loss: 2.3670389652252197
Validation loss: 2.562355290177048

Epoch: 5| Step: 8
Training loss: 2.512981414794922
Validation loss: 2.560056491564679

Epoch: 5| Step: 9
Training loss: 2.486341953277588
Validation loss: 2.541532706188899

Epoch: 5| Step: 10
Training loss: 2.9280827045440674
Validation loss: 2.5235564298527215

Epoch: 207| Step: 0
Training loss: 2.5728859901428223
Validation loss: 2.5259395953147643

Epoch: 5| Step: 1
Training loss: 2.4327495098114014
Validation loss: 2.5219228703488588

Epoch: 5| Step: 2
Training loss: 3.0101330280303955
Validation loss: 2.5211025950729207

Epoch: 5| Step: 3
Training loss: 2.3023881912231445
Validation loss: 2.5201247251161965

Epoch: 5| Step: 4
Training loss: 2.1065828800201416
Validation loss: 2.528130959439021

Epoch: 5| Step: 5
Training loss: 3.14349365234375
Validation loss: 2.529070867005215

Epoch: 5| Step: 6
Training loss: 2.657447576522827
Validation loss: 2.5223318505030807

Epoch: 5| Step: 7
Training loss: 2.872629165649414
Validation loss: 2.521008529970723

Epoch: 5| Step: 8
Training loss: 2.880457639694214
Validation loss: 2.526172778939688

Epoch: 5| Step: 9
Training loss: 2.9177775382995605
Validation loss: 2.5257374061051237

Epoch: 5| Step: 10
Training loss: 2.887171506881714
Validation loss: 2.5234349158502396

Epoch: 208| Step: 0
Training loss: 3.0339436531066895
Validation loss: 2.5300304684587704

Epoch: 5| Step: 1
Training loss: 2.3510165214538574
Validation loss: 2.5345336967898953

Epoch: 5| Step: 2
Training loss: 3.3963406085968018
Validation loss: 2.529338857179047

Epoch: 5| Step: 3
Training loss: 2.491523027420044
Validation loss: 2.54181856237432

Epoch: 5| Step: 4
Training loss: 2.6459884643554688
Validation loss: 2.5442510266457834

Epoch: 5| Step: 5
Training loss: 3.0209429264068604
Validation loss: 2.541765982104886

Epoch: 5| Step: 6
Training loss: 2.150193929672241
Validation loss: 2.5460183517907256

Epoch: 5| Step: 7
Training loss: 2.118927478790283
Validation loss: 2.523849948760002

Epoch: 5| Step: 8
Training loss: 3.019346237182617
Validation loss: 2.5182587792796474

Epoch: 5| Step: 9
Training loss: 2.8810040950775146
Validation loss: 2.5101033538900395

Epoch: 5| Step: 10
Training loss: 2.6061058044433594
Validation loss: 2.512741332413048

Epoch: 209| Step: 0
Training loss: 2.233126401901245
Validation loss: 2.509632900197019

Epoch: 5| Step: 1
Training loss: 2.607550859451294
Validation loss: 2.512355622424874

Epoch: 5| Step: 2
Training loss: 2.889586925506592
Validation loss: 2.5178447205533265

Epoch: 5| Step: 3
Training loss: 3.177298069000244
Validation loss: 2.5137184409685034

Epoch: 5| Step: 4
Training loss: 2.334423542022705
Validation loss: 2.514319448060887

Epoch: 5| Step: 5
Training loss: 2.5232784748077393
Validation loss: 2.5157954051930416

Epoch: 5| Step: 6
Training loss: 2.9050405025482178
Validation loss: 2.508457588893111

Epoch: 5| Step: 7
Training loss: 2.6580312252044678
Validation loss: 2.515414927595405

Epoch: 5| Step: 8
Training loss: 2.6275291442871094
Validation loss: 2.5162902647449124

Epoch: 5| Step: 9
Training loss: 3.1757969856262207
Validation loss: 2.513763704607564

Epoch: 5| Step: 10
Training loss: 2.507424831390381
Validation loss: 2.51400468426366

Epoch: 210| Step: 0
Training loss: 3.046229839324951
Validation loss: 2.5154085492575042

Epoch: 5| Step: 1
Training loss: 2.6343493461608887
Validation loss: 2.515370661212552

Epoch: 5| Step: 2
Training loss: 1.6602379083633423
Validation loss: 2.514116123158445

Epoch: 5| Step: 3
Training loss: 2.926891803741455
Validation loss: 2.5213909482443206

Epoch: 5| Step: 4
Training loss: 3.039307117462158
Validation loss: 2.518165626833516

Epoch: 5| Step: 5
Training loss: 2.3549561500549316
Validation loss: 2.516582219831405

Epoch: 5| Step: 6
Training loss: 2.7450687885284424
Validation loss: 2.5135896359720538

Epoch: 5| Step: 7
Training loss: 2.811988115310669
Validation loss: 2.5085958460325837

Epoch: 5| Step: 8
Training loss: 3.4168198108673096
Validation loss: 2.5162688198909966

Epoch: 5| Step: 9
Training loss: 2.512969493865967
Validation loss: 2.5097139214956634

Epoch: 5| Step: 10
Training loss: 2.5637378692626953
Validation loss: 2.5112423871153142

Epoch: 211| Step: 0
Training loss: 2.8209736347198486
Validation loss: 2.5127250840587

Epoch: 5| Step: 1
Training loss: 2.570444345474243
Validation loss: 2.5137331895930792

Epoch: 5| Step: 2
Training loss: 3.5637612342834473
Validation loss: 2.510144013230519

Epoch: 5| Step: 3
Training loss: 2.5955402851104736
Validation loss: 2.520834320334978

Epoch: 5| Step: 4
Training loss: 2.7816975116729736
Validation loss: 2.521671772003174

Epoch: 5| Step: 5
Training loss: 2.3168632984161377
Validation loss: 2.5228739053972307

Epoch: 5| Step: 6
Training loss: 2.707231044769287
Validation loss: 2.5231571197509766

Epoch: 5| Step: 7
Training loss: 2.9128775596618652
Validation loss: 2.518258340897099

Epoch: 5| Step: 8
Training loss: 2.675877571105957
Validation loss: 2.526966905081144

Epoch: 5| Step: 9
Training loss: 2.517179012298584
Validation loss: 2.521802238238755

Epoch: 5| Step: 10
Training loss: 2.2064037322998047
Validation loss: 2.5291998309473835

Epoch: 212| Step: 0
Training loss: 2.6713569164276123
Validation loss: 2.5263646315502863

Epoch: 5| Step: 1
Training loss: 2.8923728466033936
Validation loss: 2.52613264770918

Epoch: 5| Step: 2
Training loss: 2.4489388465881348
Validation loss: 2.5233959844035487

Epoch: 5| Step: 3
Training loss: 3.1184277534484863
Validation loss: 2.5258106646999234

Epoch: 5| Step: 4
Training loss: 2.7090210914611816
Validation loss: 2.5245902281935497

Epoch: 5| Step: 5
Training loss: 3.152026414871216
Validation loss: 2.521913013150615

Epoch: 5| Step: 6
Training loss: 2.482076406478882
Validation loss: 2.523683545409992

Epoch: 5| Step: 7
Training loss: 2.0132577419281006
Validation loss: 2.5211147467295327

Epoch: 5| Step: 8
Training loss: 2.5805728435516357
Validation loss: 2.51464480738486

Epoch: 5| Step: 9
Training loss: 3.158968448638916
Validation loss: 2.5129080908272856

Epoch: 5| Step: 10
Training loss: 2.474531888961792
Validation loss: 2.511590498749928

Epoch: 213| Step: 0
Training loss: 2.794509172439575
Validation loss: 2.5165537403475855

Epoch: 5| Step: 1
Training loss: 2.8694167137145996
Validation loss: 2.5093095815309914

Epoch: 5| Step: 2
Training loss: 2.553077459335327
Validation loss: 2.512371163214407

Epoch: 5| Step: 3
Training loss: 2.1713156700134277
Validation loss: 2.507546204392628

Epoch: 5| Step: 4
Training loss: 3.3295750617980957
Validation loss: 2.5080646058564544

Epoch: 5| Step: 5
Training loss: 2.276989698410034
Validation loss: 2.506328467399843

Epoch: 5| Step: 6
Training loss: 2.6467809677124023
Validation loss: 2.5064854057886268

Epoch: 5| Step: 7
Training loss: 2.3617868423461914
Validation loss: 2.5074989590593564

Epoch: 5| Step: 8
Training loss: 2.5923831462860107
Validation loss: 2.5100612332743983

Epoch: 5| Step: 9
Training loss: 3.4050750732421875
Validation loss: 2.509698752434023

Epoch: 5| Step: 10
Training loss: 2.6688320636749268
Validation loss: 2.507901232729676

Epoch: 214| Step: 0
Training loss: 2.9586949348449707
Validation loss: 2.5035204323389197

Epoch: 5| Step: 1
Training loss: 1.9903295040130615
Validation loss: 2.505780445632114

Epoch: 5| Step: 2
Training loss: 2.818516969680786
Validation loss: 2.5117475422479774

Epoch: 5| Step: 3
Training loss: 3.5213820934295654
Validation loss: 2.509283922051871

Epoch: 5| Step: 4
Training loss: 2.5051844120025635
Validation loss: 2.509575092664329

Epoch: 5| Step: 5
Training loss: 2.429654359817505
Validation loss: 2.5085715452829995

Epoch: 5| Step: 6
Training loss: 3.1797194480895996
Validation loss: 2.5097695345519693

Epoch: 5| Step: 7
Training loss: 2.265076160430908
Validation loss: 2.5083859248827864

Epoch: 5| Step: 8
Training loss: 2.641767740249634
Validation loss: 2.5049090231618574

Epoch: 5| Step: 9
Training loss: 2.898048162460327
Validation loss: 2.5124666844644854

Epoch: 5| Step: 10
Training loss: 2.395385265350342
Validation loss: 2.5109268926805064

Epoch: 215| Step: 0
Training loss: 3.018709659576416
Validation loss: 2.5109536391432568

Epoch: 5| Step: 1
Training loss: 2.5123350620269775
Validation loss: 2.5127258518690705

Epoch: 5| Step: 2
Training loss: 1.9351263046264648
Validation loss: 2.511979179997598

Epoch: 5| Step: 3
Training loss: 3.2406680583953857
Validation loss: 2.5110653036384174

Epoch: 5| Step: 4
Training loss: 2.548326015472412
Validation loss: 2.5098670631326656

Epoch: 5| Step: 5
Training loss: 3.190342426300049
Validation loss: 2.513186729082497

Epoch: 5| Step: 6
Training loss: 2.2797019481658936
Validation loss: 2.5156430711028395

Epoch: 5| Step: 7
Training loss: 3.2408509254455566
Validation loss: 2.514978165267616

Epoch: 5| Step: 8
Training loss: 2.3353347778320312
Validation loss: 2.5094290548755276

Epoch: 5| Step: 9
Training loss: 2.842670440673828
Validation loss: 2.5165942663787515

Epoch: 5| Step: 10
Training loss: 2.4549636840820312
Validation loss: 2.510305537972399

Epoch: 216| Step: 0
Training loss: 2.304081678390503
Validation loss: 2.5122146580808904

Epoch: 5| Step: 1
Training loss: 2.6396756172180176
Validation loss: 2.5087623339827343

Epoch: 5| Step: 2
Training loss: 2.914555072784424
Validation loss: 2.5009364620331795

Epoch: 5| Step: 3
Training loss: 2.931797504425049
Validation loss: 2.5053504051700717

Epoch: 5| Step: 4
Training loss: 2.9643545150756836
Validation loss: 2.5047404971174014

Epoch: 5| Step: 5
Training loss: 2.9625658988952637
Validation loss: 2.5007791647347073

Epoch: 5| Step: 6
Training loss: 2.5531165599823
Validation loss: 2.506467124467255

Epoch: 5| Step: 7
Training loss: 2.067319869995117
Validation loss: 2.4954785223930114

Epoch: 5| Step: 8
Training loss: 2.994903802871704
Validation loss: 2.5008209905316754

Epoch: 5| Step: 9
Training loss: 2.5920987129211426
Validation loss: 2.503819263109597

Epoch: 5| Step: 10
Training loss: 2.659566879272461
Validation loss: 2.4991010055747083

Epoch: 217| Step: 0
Training loss: 3.1943211555480957
Validation loss: 2.5020488385231263

Epoch: 5| Step: 1
Training loss: 2.8781261444091797
Validation loss: 2.4970711046649563

Epoch: 5| Step: 2
Training loss: 1.8914813995361328
Validation loss: 2.5000424667071273

Epoch: 5| Step: 3
Training loss: 2.822139263153076
Validation loss: 2.5021251632321264

Epoch: 5| Step: 4
Training loss: 2.303328037261963
Validation loss: 2.4995106599664174

Epoch: 5| Step: 5
Training loss: 2.3944430351257324
Validation loss: 2.5052179392947944

Epoch: 5| Step: 6
Training loss: 2.5990281105041504
Validation loss: 2.512690051909416

Epoch: 5| Step: 7
Training loss: 3.1436667442321777
Validation loss: 2.5115928624265935

Epoch: 5| Step: 8
Training loss: 2.752690076828003
Validation loss: 2.5084288889361965

Epoch: 5| Step: 9
Training loss: 2.5653200149536133
Validation loss: 2.501962900161743

Epoch: 5| Step: 10
Training loss: 3.1117208003997803
Validation loss: 2.5076837642218477

Epoch: 218| Step: 0
Training loss: 2.778475284576416
Validation loss: 2.5107769684125016

Epoch: 5| Step: 1
Training loss: 2.050924777984619
Validation loss: 2.514129751472063

Epoch: 5| Step: 2
Training loss: 2.459362030029297
Validation loss: 2.5077825207864084

Epoch: 5| Step: 3
Training loss: 2.350508451461792
Validation loss: 2.505610078893682

Epoch: 5| Step: 4
Training loss: 2.9322011470794678
Validation loss: 2.5031006284939346

Epoch: 5| Step: 5
Training loss: 3.2815959453582764
Validation loss: 2.5008161862691245

Epoch: 5| Step: 6
Training loss: 2.485353946685791
Validation loss: 2.4997256263609855

Epoch: 5| Step: 7
Training loss: 3.049330234527588
Validation loss: 2.4972364082131335

Epoch: 5| Step: 8
Training loss: 2.795825242996216
Validation loss: 2.491942728719404

Epoch: 5| Step: 9
Training loss: 2.629761219024658
Validation loss: 2.49453567945829

Epoch: 5| Step: 10
Training loss: 2.7435078620910645
Validation loss: 2.496769764090097

Epoch: 219| Step: 0
Training loss: 2.241832733154297
Validation loss: 2.4935401947267595

Epoch: 5| Step: 1
Training loss: 2.7771191596984863
Validation loss: 2.494484411772861

Epoch: 5| Step: 2
Training loss: 2.5918822288513184
Validation loss: 2.5038804495206444

Epoch: 5| Step: 3
Training loss: 3.2720179557800293
Validation loss: 2.493233473070206

Epoch: 5| Step: 4
Training loss: 2.8666884899139404
Validation loss: 2.496901214763682

Epoch: 5| Step: 5
Training loss: 3.187868595123291
Validation loss: 2.5001774603320706

Epoch: 5| Step: 6
Training loss: 1.801274299621582
Validation loss: 2.4977284298148206

Epoch: 5| Step: 7
Training loss: 2.215527057647705
Validation loss: 2.4977313421105825

Epoch: 5| Step: 8
Training loss: 3.039888858795166
Validation loss: 2.4961310022620746

Epoch: 5| Step: 9
Training loss: 2.6974875926971436
Validation loss: 2.498578543304115

Epoch: 5| Step: 10
Training loss: 2.7400498390197754
Validation loss: 2.5011979687598442

Epoch: 220| Step: 0
Training loss: 3.343459367752075
Validation loss: 2.5175862748135804

Epoch: 5| Step: 1
Training loss: 3.3024303913116455
Validation loss: 2.5316530401988695

Epoch: 5| Step: 2
Training loss: 3.3600096702575684
Validation loss: 2.5350066282415904

Epoch: 5| Step: 3
Training loss: 2.638430595397949
Validation loss: 2.5402960854191936

Epoch: 5| Step: 4
Training loss: 1.9956153631210327
Validation loss: 2.522866387521067

Epoch: 5| Step: 5
Training loss: 2.647230625152588
Validation loss: 2.507909215906615

Epoch: 5| Step: 6
Training loss: 1.8166297674179077
Validation loss: 2.500472984006328

Epoch: 5| Step: 7
Training loss: 2.693864345550537
Validation loss: 2.497488590978807

Epoch: 5| Step: 8
Training loss: 2.8501880168914795
Validation loss: 2.501056299414686

Epoch: 5| Step: 9
Training loss: 2.235145092010498
Validation loss: 2.5075974233688845

Epoch: 5| Step: 10
Training loss: 2.6498305797576904
Validation loss: 2.5150070959521877

Epoch: 221| Step: 0
Training loss: 2.6295456886291504
Validation loss: 2.512475282915177

Epoch: 5| Step: 1
Training loss: 2.5854365825653076
Validation loss: 2.519864146427442

Epoch: 5| Step: 2
Training loss: 3.495983123779297
Validation loss: 2.507217932772893

Epoch: 5| Step: 3
Training loss: 3.3752083778381348
Validation loss: 2.515700711998888

Epoch: 5| Step: 4
Training loss: 2.127725124359131
Validation loss: 2.519544509149367

Epoch: 5| Step: 5
Training loss: 2.2292933464050293
Validation loss: 2.542354124848561

Epoch: 5| Step: 6
Training loss: 2.972116708755493
Validation loss: 2.5350343309422976

Epoch: 5| Step: 7
Training loss: 2.4990038871765137
Validation loss: 2.5249862593989216

Epoch: 5| Step: 8
Training loss: 2.729499340057373
Validation loss: 2.5231185702867407

Epoch: 5| Step: 9
Training loss: 2.228743076324463
Validation loss: 2.511180270102716

Epoch: 5| Step: 10
Training loss: 2.5409281253814697
Validation loss: 2.5117242977183354

Epoch: 222| Step: 0
Training loss: 3.139575242996216
Validation loss: 2.5281302787924327

Epoch: 5| Step: 1
Training loss: 2.9315788745880127
Validation loss: 2.5196050623411774

Epoch: 5| Step: 2
Training loss: 3.151149034500122
Validation loss: 2.5260305840481996

Epoch: 5| Step: 3
Training loss: 2.8555915355682373
Validation loss: 2.542074672637447

Epoch: 5| Step: 4
Training loss: 2.371605157852173
Validation loss: 2.5315578317129486

Epoch: 5| Step: 5
Training loss: 2.501797914505005
Validation loss: 2.531123004933839

Epoch: 5| Step: 6
Training loss: 2.0134568214416504
Validation loss: 2.5226721635428806

Epoch: 5| Step: 7
Training loss: 2.444021224975586
Validation loss: 2.5206320593433995

Epoch: 5| Step: 8
Training loss: 2.582458972930908
Validation loss: 2.5094084278229745

Epoch: 5| Step: 9
Training loss: 2.76826548576355
Validation loss: 2.501651556261124

Epoch: 5| Step: 10
Training loss: 2.6714303493499756
Validation loss: 2.5019537248919086

Epoch: 223| Step: 0
Training loss: 3.221853733062744
Validation loss: 2.493364559706821

Epoch: 5| Step: 1
Training loss: 3.0723228454589844
Validation loss: 2.490607556476388

Epoch: 5| Step: 2
Training loss: 2.824251174926758
Validation loss: 2.500499256195561

Epoch: 5| Step: 3
Training loss: 2.657762289047241
Validation loss: 2.505973464699202

Epoch: 5| Step: 4
Training loss: 2.381415843963623
Validation loss: 2.512410020315519

Epoch: 5| Step: 5
Training loss: 2.7759926319122314
Validation loss: 2.5114456991995535

Epoch: 5| Step: 6
Training loss: 2.0544211864471436
Validation loss: 2.509111299309679

Epoch: 5| Step: 7
Training loss: 3.014340400695801
Validation loss: 2.5104691546450377

Epoch: 5| Step: 8
Training loss: 2.8880393505096436
Validation loss: 2.498150715263941

Epoch: 5| Step: 9
Training loss: 2.2315263748168945
Validation loss: 2.48584811405469

Epoch: 5| Step: 10
Training loss: 2.380699634552002
Validation loss: 2.481004668820289

Epoch: 224| Step: 0
Training loss: 2.879154682159424
Validation loss: 2.475816193447318

Epoch: 5| Step: 1
Training loss: 2.1818976402282715
Validation loss: 2.4801308519096783

Epoch: 5| Step: 2
Training loss: 2.5043704509735107
Validation loss: 2.47756258133919

Epoch: 5| Step: 3
Training loss: 3.378807544708252
Validation loss: 2.481539149438181

Epoch: 5| Step: 4
Training loss: 2.3434441089630127
Validation loss: 2.478871222465269

Epoch: 5| Step: 5
Training loss: 3.2007956504821777
Validation loss: 2.4820220675519717

Epoch: 5| Step: 6
Training loss: 3.4111168384552
Validation loss: 2.4828883191590667

Epoch: 5| Step: 7
Training loss: 1.9007647037506104
Validation loss: 2.491080540482716

Epoch: 5| Step: 8
Training loss: 2.33650279045105
Validation loss: 2.4889214038848877

Epoch: 5| Step: 9
Training loss: 2.794487714767456
Validation loss: 2.4989701740203367

Epoch: 5| Step: 10
Training loss: 2.467571973800659
Validation loss: 2.5054271733889015

Epoch: 225| Step: 0
Training loss: 2.7425549030303955
Validation loss: 2.499139834475774

Epoch: 5| Step: 1
Training loss: 2.0629870891571045
Validation loss: 2.499164822281048

Epoch: 5| Step: 2
Training loss: 3.0934433937072754
Validation loss: 2.5000565308396534

Epoch: 5| Step: 3
Training loss: 2.770972490310669
Validation loss: 2.499370146823186

Epoch: 5| Step: 4
Training loss: 3.1323812007904053
Validation loss: 2.5053910158013784

Epoch: 5| Step: 5
Training loss: 2.449714183807373
Validation loss: 2.4972286224365234

Epoch: 5| Step: 6
Training loss: 1.939812421798706
Validation loss: 2.500329404748896

Epoch: 5| Step: 7
Training loss: 2.3945839405059814
Validation loss: 2.499492181244717

Epoch: 5| Step: 8
Training loss: 2.256282091140747
Validation loss: 2.515686529938893

Epoch: 5| Step: 9
Training loss: 3.225572109222412
Validation loss: 2.4982190952506116

Epoch: 5| Step: 10
Training loss: 3.434262990951538
Validation loss: 2.5022749388089744

Epoch: 226| Step: 0
Training loss: 2.564136266708374
Validation loss: 2.518393534486012

Epoch: 5| Step: 1
Training loss: 3.445763111114502
Validation loss: 2.52181996837739

Epoch: 5| Step: 2
Training loss: 2.2351202964782715
Validation loss: 2.521661353367631

Epoch: 5| Step: 3
Training loss: 2.782449722290039
Validation loss: 2.522242481990527

Epoch: 5| Step: 4
Training loss: 2.458202600479126
Validation loss: 2.511596246432233

Epoch: 5| Step: 5
Training loss: 2.1290392875671387
Validation loss: 2.504285027903895

Epoch: 5| Step: 6
Training loss: 2.6241118907928467
Validation loss: 2.4924934961462535

Epoch: 5| Step: 7
Training loss: 2.770129680633545
Validation loss: 2.4944853167380057

Epoch: 5| Step: 8
Training loss: 2.5673892498016357
Validation loss: 2.491531797634658

Epoch: 5| Step: 9
Training loss: 2.756866931915283
Validation loss: 2.4989045768655758

Epoch: 5| Step: 10
Training loss: 3.129201889038086
Validation loss: 2.5103019822028374

Epoch: 227| Step: 0
Training loss: 2.592664957046509
Validation loss: 2.5323812423213834

Epoch: 5| Step: 1
Training loss: 2.3676509857177734
Validation loss: 2.529217932813911

Epoch: 5| Step: 2
Training loss: 2.201315402984619
Validation loss: 2.5327730973561606

Epoch: 5| Step: 3
Training loss: 2.365255355834961
Validation loss: 2.531242911533643

Epoch: 5| Step: 4
Training loss: 2.51593017578125
Validation loss: 2.513344731382144

Epoch: 5| Step: 5
Training loss: 2.767214298248291
Validation loss: 2.49791052264552

Epoch: 5| Step: 6
Training loss: 3.2973685264587402
Validation loss: 2.4896431507602816

Epoch: 5| Step: 7
Training loss: 3.199638843536377
Validation loss: 2.4941937461976083

Epoch: 5| Step: 8
Training loss: 2.0275816917419434
Validation loss: 2.4875830373456402

Epoch: 5| Step: 9
Training loss: 3.5901246070861816
Validation loss: 2.481716466206376

Epoch: 5| Step: 10
Training loss: 2.5534472465515137
Validation loss: 2.4796227075720347

Epoch: 228| Step: 0
Training loss: 2.4790103435516357
Validation loss: 2.478006593642696

Epoch: 5| Step: 1
Training loss: 2.466017723083496
Validation loss: 2.483341916914909

Epoch: 5| Step: 2
Training loss: 2.7838141918182373
Validation loss: 2.486126063972391

Epoch: 5| Step: 3
Training loss: 2.637193441390991
Validation loss: 2.484409809112549

Epoch: 5| Step: 4
Training loss: 3.4413177967071533
Validation loss: 2.4811854977761545

Epoch: 5| Step: 5
Training loss: 1.94287109375
Validation loss: 2.475919323582803

Epoch: 5| Step: 6
Training loss: 2.5314087867736816
Validation loss: 2.4787827307178127

Epoch: 5| Step: 7
Training loss: 3.0886387825012207
Validation loss: 2.4839306954414613

Epoch: 5| Step: 8
Training loss: 2.3752999305725098
Validation loss: 2.4869598624526814

Epoch: 5| Step: 9
Training loss: 2.540945053100586
Validation loss: 2.48847980653086

Epoch: 5| Step: 10
Training loss: 3.139291763305664
Validation loss: 2.4934080544338433

Epoch: 229| Step: 0
Training loss: 1.8699100017547607
Validation loss: 2.4992758099750807

Epoch: 5| Step: 1
Training loss: 3.374675750732422
Validation loss: 2.493657914541101

Epoch: 5| Step: 2
Training loss: 2.2014832496643066
Validation loss: 2.504541850859119

Epoch: 5| Step: 3
Training loss: 2.590911865234375
Validation loss: 2.499316951279999

Epoch: 5| Step: 4
Training loss: 1.9533698558807373
Validation loss: 2.4863712902992003

Epoch: 5| Step: 5
Training loss: 3.023097515106201
Validation loss: 2.490999362801993

Epoch: 5| Step: 6
Training loss: 2.6524415016174316
Validation loss: 2.4918887487021824

Epoch: 5| Step: 7
Training loss: 2.7659733295440674
Validation loss: 2.4975009015811387

Epoch: 5| Step: 8
Training loss: 3.5682907104492188
Validation loss: 2.4941921490494923

Epoch: 5| Step: 9
Training loss: 2.919743061065674
Validation loss: 2.496816755622946

Epoch: 5| Step: 10
Training loss: 2.3223814964294434
Validation loss: 2.5013315011096258

Epoch: 230| Step: 0
Training loss: 3.5038704872131348
Validation loss: 2.494682945230956

Epoch: 5| Step: 1
Training loss: 3.5439987182617188
Validation loss: 2.5040364906352055

Epoch: 5| Step: 2
Training loss: 1.9657821655273438
Validation loss: 2.498256891004501

Epoch: 5| Step: 3
Training loss: 2.834021806716919
Validation loss: 2.507959637590634

Epoch: 5| Step: 4
Training loss: 2.3105225563049316
Validation loss: 2.5091011895928332

Epoch: 5| Step: 5
Training loss: 1.9728648662567139
Validation loss: 2.5151289483552337

Epoch: 5| Step: 6
Training loss: 2.960219144821167
Validation loss: 2.5164161651365218

Epoch: 5| Step: 7
Training loss: 3.0962777137756348
Validation loss: 2.495979614155267

Epoch: 5| Step: 8
Training loss: 2.091698169708252
Validation loss: 2.4954157977975826

Epoch: 5| Step: 9
Training loss: 2.8717122077941895
Validation loss: 2.5070328661190566

Epoch: 5| Step: 10
Training loss: 2.0046732425689697
Validation loss: 2.519539269067908

Epoch: 231| Step: 0
Training loss: 2.5360724925994873
Validation loss: 2.492609506012291

Epoch: 5| Step: 1
Training loss: 3.0213754177093506
Validation loss: 2.5018393788286435

Epoch: 5| Step: 2
Training loss: 2.545313596725464
Validation loss: 2.5030910481688795

Epoch: 5| Step: 3
Training loss: 3.030926465988159
Validation loss: 2.4989476998647056

Epoch: 5| Step: 4
Training loss: 2.7621212005615234
Validation loss: 2.483703479971937

Epoch: 5| Step: 5
Training loss: 1.8442802429199219
Validation loss: 2.4761536044459187

Epoch: 5| Step: 6
Training loss: 2.6499838829040527
Validation loss: 2.474327177129766

Epoch: 5| Step: 7
Training loss: 2.5123586654663086
Validation loss: 2.4738288284629903

Epoch: 5| Step: 8
Training loss: 2.8585445880889893
Validation loss: 2.47107187650537

Epoch: 5| Step: 9
Training loss: 2.594538927078247
Validation loss: 2.4687034737679268

Epoch: 5| Step: 10
Training loss: 2.9680614471435547
Validation loss: 2.4722661074771675

Epoch: 232| Step: 0
Training loss: 3.1172666549682617
Validation loss: 2.4764804378632577

Epoch: 5| Step: 1
Training loss: 2.5976173877716064
Validation loss: 2.476038771290933

Epoch: 5| Step: 2
Training loss: 2.3090670108795166
Validation loss: 2.4739144514965754

Epoch: 5| Step: 3
Training loss: 2.407064914703369
Validation loss: 2.480355208919894

Epoch: 5| Step: 4
Training loss: 2.441444158554077
Validation loss: 2.4737579950722317

Epoch: 5| Step: 5
Training loss: 2.2224740982055664
Validation loss: 2.4802924433062152

Epoch: 5| Step: 6
Training loss: 3.1768033504486084
Validation loss: 2.4767674323051208

Epoch: 5| Step: 7
Training loss: 2.8986918926239014
Validation loss: 2.4786250950187765

Epoch: 5| Step: 8
Training loss: 2.1965243816375732
Validation loss: 2.4764495203571935

Epoch: 5| Step: 9
Training loss: 3.0803122520446777
Validation loss: 2.4762896619817263

Epoch: 5| Step: 10
Training loss: 2.8650741577148438
Validation loss: 2.4755994427588677

Epoch: 233| Step: 0
Training loss: 2.8725297451019287
Validation loss: 2.4748964566056446

Epoch: 5| Step: 1
Training loss: 2.9406285285949707
Validation loss: 2.477544656363867

Epoch: 5| Step: 2
Training loss: 3.049170732498169
Validation loss: 2.480725990828647

Epoch: 5| Step: 3
Training loss: 2.614698886871338
Validation loss: 2.4773905995071575

Epoch: 5| Step: 4
Training loss: 2.1915841102600098
Validation loss: 2.480117413305467

Epoch: 5| Step: 5
Training loss: 3.1107418537139893
Validation loss: 2.4833416451690016

Epoch: 5| Step: 6
Training loss: 3.1580729484558105
Validation loss: 2.478326210411646

Epoch: 5| Step: 7
Training loss: 3.220951795578003
Validation loss: 2.4833419707513626

Epoch: 5| Step: 8
Training loss: 1.8654361963272095
Validation loss: 2.4739007142282303

Epoch: 5| Step: 9
Training loss: 2.005591869354248
Validation loss: 2.4822574379623576

Epoch: 5| Step: 10
Training loss: 2.189619779586792
Validation loss: 2.472327160578902

Epoch: 234| Step: 0
Training loss: 2.6061019897460938
Validation loss: 2.4797190799508044

Epoch: 5| Step: 1
Training loss: 2.3845338821411133
Validation loss: 2.4765833526529293

Epoch: 5| Step: 2
Training loss: 3.0101261138916016
Validation loss: 2.4760634463320494

Epoch: 5| Step: 3
Training loss: 2.7774837017059326
Validation loss: 2.480429482716386

Epoch: 5| Step: 4
Training loss: 2.6313483715057373
Validation loss: 2.4805735516291794

Epoch: 5| Step: 5
Training loss: 2.469412326812744
Validation loss: 2.4773294156597507

Epoch: 5| Step: 6
Training loss: 2.979055881500244
Validation loss: 2.480093620156729

Epoch: 5| Step: 7
Training loss: 2.8385777473449707
Validation loss: 2.4766257347599154

Epoch: 5| Step: 8
Training loss: 2.2901978492736816
Validation loss: 2.475444345064061

Epoch: 5| Step: 9
Training loss: 2.890941858291626
Validation loss: 2.479334303127822

Epoch: 5| Step: 10
Training loss: 2.2569515705108643
Validation loss: 2.4727873981639905

Epoch: 235| Step: 0
Training loss: 2.8775486946105957
Validation loss: 2.4703241214957288

Epoch: 5| Step: 1
Training loss: 1.8123258352279663
Validation loss: 2.4748467732501287

Epoch: 5| Step: 2
Training loss: 2.7167937755584717
Validation loss: 2.4769043922424316

Epoch: 5| Step: 3
Training loss: 2.2642340660095215
Validation loss: 2.475883963287518

Epoch: 5| Step: 4
Training loss: 2.1231648921966553
Validation loss: 2.480824655102145

Epoch: 5| Step: 5
Training loss: 3.0932068824768066
Validation loss: 2.470059792200724

Epoch: 5| Step: 6
Training loss: 3.5877349376678467
Validation loss: 2.477893126908169

Epoch: 5| Step: 7
Training loss: 2.703784227371216
Validation loss: 2.476661379619311

Epoch: 5| Step: 8
Training loss: 3.1024608612060547
Validation loss: 2.4727826144105647

Epoch: 5| Step: 9
Training loss: 2.2343955039978027
Validation loss: 2.4698678011535318

Epoch: 5| Step: 10
Training loss: 2.760348320007324
Validation loss: 2.4731872158665813

Epoch: 236| Step: 0
Training loss: 3.523446559906006
Validation loss: 2.4669811366706766

Epoch: 5| Step: 1
Training loss: 2.7674431800842285
Validation loss: 2.478718614065519

Epoch: 5| Step: 2
Training loss: 2.852795362472534
Validation loss: 2.468555899076564

Epoch: 5| Step: 3
Training loss: 2.3993544578552246
Validation loss: 2.471332370593984

Epoch: 5| Step: 4
Training loss: 2.4565539360046387
Validation loss: 2.4749707227112143

Epoch: 5| Step: 5
Training loss: 2.801079750061035
Validation loss: 2.471767512700891

Epoch: 5| Step: 6
Training loss: 2.749014377593994
Validation loss: 2.473461466450845

Epoch: 5| Step: 7
Training loss: 2.260678768157959
Validation loss: 2.463720252436976

Epoch: 5| Step: 8
Training loss: 2.3947083950042725
Validation loss: 2.4726625847560104

Epoch: 5| Step: 9
Training loss: 2.1186649799346924
Validation loss: 2.4729211227868193

Epoch: 5| Step: 10
Training loss: 2.9637973308563232
Validation loss: 2.482839976587603

Epoch: 237| Step: 0
Training loss: 3.189662218093872
Validation loss: 2.4997549492825746

Epoch: 5| Step: 1
Training loss: 2.1324710845947266
Validation loss: 2.5087571195376817

Epoch: 5| Step: 2
Training loss: 2.9307003021240234
Validation loss: 2.5040999817591842

Epoch: 5| Step: 3
Training loss: 2.9032199382781982
Validation loss: 2.502780109323481

Epoch: 5| Step: 4
Training loss: 2.6279473304748535
Validation loss: 2.4823919573137836

Epoch: 5| Step: 5
Training loss: 2.340357542037964
Validation loss: 2.4809462793411745

Epoch: 5| Step: 6
Training loss: 3.00903582572937
Validation loss: 2.4717143940669235

Epoch: 5| Step: 7
Training loss: 2.8380932807922363
Validation loss: 2.470318873723348

Epoch: 5| Step: 8
Training loss: 2.6024162769317627
Validation loss: 2.4742184531304146

Epoch: 5| Step: 9
Training loss: 2.6632561683654785
Validation loss: 2.4728408423803185

Epoch: 5| Step: 10
Training loss: 1.9939427375793457
Validation loss: 2.482893072148805

Epoch: 238| Step: 0
Training loss: 3.016298770904541
Validation loss: 2.473258674785655

Epoch: 5| Step: 1
Training loss: 2.2916131019592285
Validation loss: 2.481970653739027

Epoch: 5| Step: 2
Training loss: 2.684805393218994
Validation loss: 2.4854106390348045

Epoch: 5| Step: 3
Training loss: 2.5513877868652344
Validation loss: 2.4800274269555205

Epoch: 5| Step: 4
Training loss: 2.4195504188537598
Validation loss: 2.485071723179151

Epoch: 5| Step: 5
Training loss: 2.6297004222869873
Validation loss: 2.4799540632514545

Epoch: 5| Step: 6
Training loss: 2.2293612957000732
Validation loss: 2.479541435036608

Epoch: 5| Step: 7
Training loss: 2.794740676879883
Validation loss: 2.4794036598615747

Epoch: 5| Step: 8
Training loss: 2.9069995880126953
Validation loss: 2.4758448241859354

Epoch: 5| Step: 9
Training loss: 2.9793426990509033
Validation loss: 2.476893860806701

Epoch: 5| Step: 10
Training loss: 2.7173967361450195
Validation loss: 2.4936255793417654

Epoch: 239| Step: 0
Training loss: 2.9306678771972656
Validation loss: 2.489391198722265

Epoch: 5| Step: 1
Training loss: 3.0179543495178223
Validation loss: 2.497093887739284

Epoch: 5| Step: 2
Training loss: 2.423727035522461
Validation loss: 2.4944503025342057

Epoch: 5| Step: 3
Training loss: 2.696428060531616
Validation loss: 2.4808386935982654

Epoch: 5| Step: 4
Training loss: 2.486600160598755
Validation loss: 2.4821185194036013

Epoch: 5| Step: 5
Training loss: 2.4605584144592285
Validation loss: 2.4887218142068512

Epoch: 5| Step: 6
Training loss: 2.859522819519043
Validation loss: 2.472461397929858

Epoch: 5| Step: 7
Training loss: 2.5535635948181152
Validation loss: 2.469103508098151

Epoch: 5| Step: 8
Training loss: 2.4211740493774414
Validation loss: 2.472564143519248

Epoch: 5| Step: 9
Training loss: 2.478393793106079
Validation loss: 2.467625376998737

Epoch: 5| Step: 10
Training loss: 2.800265312194824
Validation loss: 2.4676362083804224

Epoch: 240| Step: 0
Training loss: 2.0251638889312744
Validation loss: 2.4683305178919146

Epoch: 5| Step: 1
Training loss: 2.5351028442382812
Validation loss: 2.464535479904503

Epoch: 5| Step: 2
Training loss: 2.9455316066741943
Validation loss: 2.470713923054357

Epoch: 5| Step: 3
Training loss: 2.799712657928467
Validation loss: 2.469715690100065

Epoch: 5| Step: 4
Training loss: 2.666243314743042
Validation loss: 2.463223226608769

Epoch: 5| Step: 5
Training loss: 2.573193073272705
Validation loss: 2.4690007317450737

Epoch: 5| Step: 6
Training loss: 2.598844289779663
Validation loss: 2.4619743695823093

Epoch: 5| Step: 7
Training loss: 1.822759985923767
Validation loss: 2.4735073428000174

Epoch: 5| Step: 8
Training loss: 3.7150332927703857
Validation loss: 2.473887061560026

Epoch: 5| Step: 9
Training loss: 2.25866436958313
Validation loss: 2.4702527651222805

Epoch: 5| Step: 10
Training loss: 3.233860731124878
Validation loss: 2.466876527314545

Epoch: 241| Step: 0
Training loss: 3.2602310180664062
Validation loss: 2.472824492762166

Epoch: 5| Step: 1
Training loss: 3.413132429122925
Validation loss: 2.4753614446168304

Epoch: 5| Step: 2
Training loss: 2.137504816055298
Validation loss: 2.4740819777211835

Epoch: 5| Step: 3
Training loss: 2.00321626663208
Validation loss: 2.4931023069607314

Epoch: 5| Step: 4
Training loss: 2.801426410675049
Validation loss: 2.4847415390835015

Epoch: 5| Step: 5
Training loss: 3.0861153602600098
Validation loss: 2.4790801719952653

Epoch: 5| Step: 6
Training loss: 3.032953977584839
Validation loss: 2.479334472328104

Epoch: 5| Step: 7
Training loss: 2.358774185180664
Validation loss: 2.4859989996879333

Epoch: 5| Step: 8
Training loss: 2.2753188610076904
Validation loss: 2.4818638960520425

Epoch: 5| Step: 9
Training loss: 2.0685997009277344
Validation loss: 2.475928073288292

Epoch: 5| Step: 10
Training loss: 2.4636006355285645
Validation loss: 2.4766846574762815

Epoch: 242| Step: 0
Training loss: 2.86190128326416
Validation loss: 2.4856728481990036

Epoch: 5| Step: 1
Training loss: 2.8833093643188477
Validation loss: 2.481409518949447

Epoch: 5| Step: 2
Training loss: 2.3860549926757812
Validation loss: 2.4851903325767926

Epoch: 5| Step: 3
Training loss: 2.820767879486084
Validation loss: 2.493687273353659

Epoch: 5| Step: 4
Training loss: 2.9232847690582275
Validation loss: 2.4911186797644502

Epoch: 5| Step: 5
Training loss: 2.7277634143829346
Validation loss: 2.477992880728937

Epoch: 5| Step: 6
Training loss: 2.918891429901123
Validation loss: 2.4837566575696393

Epoch: 5| Step: 7
Training loss: 2.0201902389526367
Validation loss: 2.474586489380047

Epoch: 5| Step: 8
Training loss: 3.1082677841186523
Validation loss: 2.473496716509583

Epoch: 5| Step: 9
Training loss: 2.2504029273986816
Validation loss: 2.463686204725696

Epoch: 5| Step: 10
Training loss: 2.1212456226348877
Validation loss: 2.4629268671876643

Epoch: 243| Step: 0
Training loss: 2.4527180194854736
Validation loss: 2.4666066041556736

Epoch: 5| Step: 1
Training loss: 3.38692045211792
Validation loss: 2.4627068247846378

Epoch: 5| Step: 2
Training loss: 2.1836719512939453
Validation loss: 2.46695444660802

Epoch: 5| Step: 3
Training loss: 2.84112811088562
Validation loss: 2.4763140293859665

Epoch: 5| Step: 4
Training loss: 3.1182920932769775
Validation loss: 2.482667056463098

Epoch: 5| Step: 5
Training loss: 2.7874069213867188
Validation loss: 2.5054240226745605

Epoch: 5| Step: 6
Training loss: 2.434136390686035
Validation loss: 2.5137707956375612

Epoch: 5| Step: 7
Training loss: 3.146432399749756
Validation loss: 2.5122801744809715

Epoch: 5| Step: 8
Training loss: 2.5126280784606934
Validation loss: 2.5058408885873775

Epoch: 5| Step: 9
Training loss: 2.1406219005584717
Validation loss: 2.4894452287304785

Epoch: 5| Step: 10
Training loss: 1.9883450269699097
Validation loss: 2.4746671004961898

Epoch: 244| Step: 0
Training loss: 2.4618043899536133
Validation loss: 2.474634006459226

Epoch: 5| Step: 1
Training loss: 2.4683837890625
Validation loss: 2.477726759449128

Epoch: 5| Step: 2
Training loss: 2.3242151737213135
Validation loss: 2.47548875757443

Epoch: 5| Step: 3
Training loss: 2.4738731384277344
Validation loss: 2.4884948269013436

Epoch: 5| Step: 4
Training loss: 2.2590184211730957
Validation loss: 2.488096470473915

Epoch: 5| Step: 5
Training loss: 2.857013463973999
Validation loss: 2.49487211883709

Epoch: 5| Step: 6
Training loss: 2.676389217376709
Validation loss: 2.4874861727478685

Epoch: 5| Step: 7
Training loss: 2.7574150562286377
Validation loss: 2.4921147669515302

Epoch: 5| Step: 8
Training loss: 3.1020278930664062
Validation loss: 2.4801303084178636

Epoch: 5| Step: 9
Training loss: 3.2542262077331543
Validation loss: 2.4776385727749077

Epoch: 5| Step: 10
Training loss: 2.336665153503418
Validation loss: 2.4703752994537354

Epoch: 245| Step: 0
Training loss: 2.105013370513916
Validation loss: 2.4802154135960404

Epoch: 5| Step: 1
Training loss: 3.6409735679626465
Validation loss: 2.4797696528896207

Epoch: 5| Step: 2
Training loss: 3.2082247734069824
Validation loss: 2.479679310193626

Epoch: 5| Step: 3
Training loss: 2.0886683464050293
Validation loss: 2.481475127640591

Epoch: 5| Step: 4
Training loss: 3.2259011268615723
Validation loss: 2.4770868824374292

Epoch: 5| Step: 5
Training loss: 2.534959554672241
Validation loss: 2.470452818819272

Epoch: 5| Step: 6
Training loss: 2.8585212230682373
Validation loss: 2.4844074813268517

Epoch: 5| Step: 7
Training loss: 2.5908899307250977
Validation loss: 2.478005022130987

Epoch: 5| Step: 8
Training loss: 1.8813425302505493
Validation loss: 2.486935325848159

Epoch: 5| Step: 9
Training loss: 2.594167709350586
Validation loss: 2.505181663779802

Epoch: 5| Step: 10
Training loss: 2.323496103286743
Validation loss: 2.4839171876189527

Epoch: 246| Step: 0
Training loss: 2.3827357292175293
Validation loss: 2.4939273916265017

Epoch: 5| Step: 1
Training loss: 2.590214967727661
Validation loss: 2.4865415891011557

Epoch: 5| Step: 2
Training loss: 2.5792369842529297
Validation loss: 2.4835502050256215

Epoch: 5| Step: 3
Training loss: 3.1400811672210693
Validation loss: 2.4779315968995452

Epoch: 5| Step: 4
Training loss: 2.45546293258667
Validation loss: 2.480605358718544

Epoch: 5| Step: 5
Training loss: 3.070481300354004
Validation loss: 2.4828974995561826

Epoch: 5| Step: 6
Training loss: 2.3379194736480713
Validation loss: 2.481912398851046

Epoch: 5| Step: 7
Training loss: 2.3884811401367188
Validation loss: 2.472884524253107

Epoch: 5| Step: 8
Training loss: 2.4082581996917725
Validation loss: 2.471173183892363

Epoch: 5| Step: 9
Training loss: 3.280489444732666
Validation loss: 2.470360576465566

Epoch: 5| Step: 10
Training loss: 2.421464443206787
Validation loss: 2.463825438612251

Epoch: 247| Step: 0
Training loss: 2.63920259475708
Validation loss: 2.4622532501015613

Epoch: 5| Step: 1
Training loss: 2.340494155883789
Validation loss: 2.4598724636980283

Epoch: 5| Step: 2
Training loss: 2.4094748497009277
Validation loss: 2.468188998519733

Epoch: 5| Step: 3
Training loss: 2.398911952972412
Validation loss: 2.4678425199242047

Epoch: 5| Step: 4
Training loss: 2.337756395339966
Validation loss: 2.460712581552485

Epoch: 5| Step: 5
Training loss: 4.224956035614014
Validation loss: 2.473725083053753

Epoch: 5| Step: 6
Training loss: 2.980501413345337
Validation loss: 2.4715669206393662

Epoch: 5| Step: 7
Training loss: 2.3848228454589844
Validation loss: 2.4809477611254622

Epoch: 5| Step: 8
Training loss: 1.4844800233840942
Validation loss: 2.4767857136264926

Epoch: 5| Step: 9
Training loss: 2.8820013999938965
Validation loss: 2.4750272279144614

Epoch: 5| Step: 10
Training loss: 2.961735486984253
Validation loss: 2.482109390279298

Epoch: 248| Step: 0
Training loss: 2.529996395111084
Validation loss: 2.4866909506500408

Epoch: 5| Step: 1
Training loss: 1.9628269672393799
Validation loss: 2.505690202918104

Epoch: 5| Step: 2
Training loss: 2.993419647216797
Validation loss: 2.538184917101296

Epoch: 5| Step: 3
Training loss: 2.7112903594970703
Validation loss: 2.5373693076513146

Epoch: 5| Step: 4
Training loss: 2.887467384338379
Validation loss: 2.517445489924441

Epoch: 5| Step: 5
Training loss: 2.315505266189575
Validation loss: 2.5090003962157876

Epoch: 5| Step: 6
Training loss: 2.7587358951568604
Validation loss: 2.514056913314327

Epoch: 5| Step: 7
Training loss: 2.4255363941192627
Validation loss: 2.5173675629400436

Epoch: 5| Step: 8
Training loss: 2.597144842147827
Validation loss: 2.499191258543281

Epoch: 5| Step: 9
Training loss: 3.239264726638794
Validation loss: 2.4876608002570366

Epoch: 5| Step: 10
Training loss: 2.7070205211639404
Validation loss: 2.4715192471781084

Epoch: 249| Step: 0
Training loss: 2.759683847427368
Validation loss: 2.467097064500214

Epoch: 5| Step: 1
Training loss: 3.054626941680908
Validation loss: 2.471569084352063

Epoch: 5| Step: 2
Training loss: 2.483449935913086
Validation loss: 2.4691837013408704

Epoch: 5| Step: 3
Training loss: 2.918464183807373
Validation loss: 2.4674712970692623

Epoch: 5| Step: 4
Training loss: 1.8386800289154053
Validation loss: 2.465667781009469

Epoch: 5| Step: 5
Training loss: 1.8821214437484741
Validation loss: 2.4689623540447605

Epoch: 5| Step: 6
Training loss: 3.0206847190856934
Validation loss: 2.471222536538237

Epoch: 5| Step: 7
Training loss: 2.844372510910034
Validation loss: 2.4763781665473856

Epoch: 5| Step: 8
Training loss: 3.089405059814453
Validation loss: 2.4871658202140563

Epoch: 5| Step: 9
Training loss: 3.4612298011779785
Validation loss: 2.501355660858975

Epoch: 5| Step: 10
Training loss: 1.5139997005462646
Validation loss: 2.5227668541733936

Epoch: 250| Step: 0
Training loss: 2.9097349643707275
Validation loss: 2.517780873083299

Epoch: 5| Step: 1
Training loss: 2.728820562362671
Validation loss: 2.505496455777076

Epoch: 5| Step: 2
Training loss: 2.137803077697754
Validation loss: 2.520001919038834

Epoch: 5| Step: 3
Training loss: 2.782834529876709
Validation loss: 2.4951061279542985

Epoch: 5| Step: 4
Training loss: 2.3868157863616943
Validation loss: 2.5056511022711314

Epoch: 5| Step: 5
Training loss: 2.603161573410034
Validation loss: 2.5163472083307084

Epoch: 5| Step: 6
Training loss: 2.7434773445129395
Validation loss: 2.5059646867936656

Epoch: 5| Step: 7
Training loss: 2.498798131942749
Validation loss: 2.503267603535806

Epoch: 5| Step: 8
Training loss: 2.9173226356506348
Validation loss: 2.4921962394509265

Epoch: 5| Step: 9
Training loss: 2.9267091751098633
Validation loss: 2.5068790758809736

Epoch: 5| Step: 10
Training loss: 2.4183437824249268
Validation loss: 2.4922431668927594

Epoch: 251| Step: 0
Training loss: 3.081737995147705
Validation loss: 2.4908739264293382

Epoch: 5| Step: 1
Training loss: 2.3796818256378174
Validation loss: 2.4945973901338476

Epoch: 5| Step: 2
Training loss: 2.746227979660034
Validation loss: 2.483900870046308

Epoch: 5| Step: 3
Training loss: 2.4693901538848877
Validation loss: 2.4817408054105696

Epoch: 5| Step: 4
Training loss: 2.4766640663146973
Validation loss: 2.4826345674453245

Epoch: 5| Step: 5
Training loss: 2.9209988117218018
Validation loss: 2.4710840204710602

Epoch: 5| Step: 6
Training loss: 3.5730483531951904
Validation loss: 2.472057132310765

Epoch: 5| Step: 7
Training loss: 3.1177878379821777
Validation loss: 2.476779850580359

Epoch: 5| Step: 8
Training loss: 1.9910719394683838
Validation loss: 2.466344702628351

Epoch: 5| Step: 9
Training loss: 2.254390239715576
Validation loss: 2.458905445632114

Epoch: 5| Step: 10
Training loss: 1.8587003946304321
Validation loss: 2.457557673095375

Epoch: 252| Step: 0
Training loss: 1.9653316736221313
Validation loss: 2.450895417121149

Epoch: 5| Step: 1
Training loss: 2.5251524448394775
Validation loss: 2.44896988971259

Epoch: 5| Step: 2
Training loss: 2.1900086402893066
Validation loss: 2.450217728973717

Epoch: 5| Step: 3
Training loss: 2.5369138717651367
Validation loss: 2.451973174207954

Epoch: 5| Step: 4
Training loss: 2.5366272926330566
Validation loss: 2.460561288300381

Epoch: 5| Step: 5
Training loss: 2.2915189266204834
Validation loss: 2.468124240957281

Epoch: 5| Step: 6
Training loss: 2.7692060470581055
Validation loss: 2.469707178813155

Epoch: 5| Step: 7
Training loss: 3.191970109939575
Validation loss: 2.4831118711861233

Epoch: 5| Step: 8
Training loss: 3.5647881031036377
Validation loss: 2.483185073380829

Epoch: 5| Step: 9
Training loss: 3.2604031562805176
Validation loss: 2.4706056810194448

Epoch: 5| Step: 10
Training loss: 2.109283924102783
Validation loss: 2.4736812755625737

Epoch: 253| Step: 0
Training loss: 2.452611207962036
Validation loss: 2.478751802957186

Epoch: 5| Step: 1
Training loss: 2.54770827293396
Validation loss: 2.4793357618393435

Epoch: 5| Step: 2
Training loss: 3.1028053760528564
Validation loss: 2.4727286395206245

Epoch: 5| Step: 3
Training loss: 2.2770495414733887
Validation loss: 2.4650901107377905

Epoch: 5| Step: 4
Training loss: 2.577166795730591
Validation loss: 2.4573623441880748

Epoch: 5| Step: 5
Training loss: 2.5240859985351562
Validation loss: 2.471285148333478

Epoch: 5| Step: 6
Training loss: 2.858212947845459
Validation loss: 2.4798855217554236

Epoch: 5| Step: 7
Training loss: 3.2201907634735107
Validation loss: 2.480388085047404

Epoch: 5| Step: 8
Training loss: 2.423334836959839
Validation loss: 2.482527099629884

Epoch: 5| Step: 9
Training loss: 2.3752143383026123
Validation loss: 2.4893252516305573

Epoch: 5| Step: 10
Training loss: 2.530125856399536
Validation loss: 2.506216920832152

Epoch: 254| Step: 0
Training loss: 2.7617597579956055
Validation loss: 2.516452238123904

Epoch: 5| Step: 1
Training loss: 2.6724655628204346
Validation loss: 2.5329666624787035

Epoch: 5| Step: 2
Training loss: 2.3365795612335205
Validation loss: 2.5281711188695764

Epoch: 5| Step: 3
Training loss: 3.03674578666687
Validation loss: 2.518999761150729

Epoch: 5| Step: 4
Training loss: 2.7005696296691895
Validation loss: 2.49684722961918

Epoch: 5| Step: 5
Training loss: 2.6009745597839355
Validation loss: 2.4994395061205794

Epoch: 5| Step: 6
Training loss: 3.0034291744232178
Validation loss: 2.478280246898692

Epoch: 5| Step: 7
Training loss: 2.098906993865967
Validation loss: 2.4762107608138875

Epoch: 5| Step: 8
Training loss: 2.4588050842285156
Validation loss: 2.473199777705695

Epoch: 5| Step: 9
Training loss: 2.83467435836792
Validation loss: 2.4761329235569125

Epoch: 5| Step: 10
Training loss: 2.49636173248291
Validation loss: 2.4695127805074057

Epoch: 255| Step: 0
Training loss: 2.4854938983917236
Validation loss: 2.465636186702277

Epoch: 5| Step: 1
Training loss: 2.4374217987060547
Validation loss: 2.467207585611651

Epoch: 5| Step: 2
Training loss: 2.3054652214050293
Validation loss: 2.467664281527201

Epoch: 5| Step: 3
Training loss: 2.404179811477661
Validation loss: 2.480222455916866

Epoch: 5| Step: 4
Training loss: 2.270524263381958
Validation loss: 2.5010030705441713

Epoch: 5| Step: 5
Training loss: 2.9874305725097656
Validation loss: 2.511780749085129

Epoch: 5| Step: 6
Training loss: 3.126173734664917
Validation loss: 2.525062402089437

Epoch: 5| Step: 7
Training loss: 2.777026414871216
Validation loss: 2.534048693154448

Epoch: 5| Step: 8
Training loss: 3.18684720993042
Validation loss: 2.523183566267772

Epoch: 5| Step: 9
Training loss: 2.5258984565734863
Validation loss: 2.50155956258056

Epoch: 5| Step: 10
Training loss: 2.6018059253692627
Validation loss: 2.478495115874916

Epoch: 256| Step: 0
Training loss: 2.2277235984802246
Validation loss: 2.458775535706551

Epoch: 5| Step: 1
Training loss: 3.2139296531677246
Validation loss: 2.4682804435812016

Epoch: 5| Step: 2
Training loss: 2.106180429458618
Validation loss: 2.464509023133145

Epoch: 5| Step: 3
Training loss: 2.5238430500030518
Validation loss: 2.462198847083635

Epoch: 5| Step: 4
Training loss: 3.1874608993530273
Validation loss: 2.462969236476447

Epoch: 5| Step: 5
Training loss: 2.9394540786743164
Validation loss: 2.4650313033852527

Epoch: 5| Step: 6
Training loss: 2.599440813064575
Validation loss: 2.4692978346219627

Epoch: 5| Step: 7
Training loss: 2.8160653114318848
Validation loss: 2.459660550599457

Epoch: 5| Step: 8
Training loss: 2.349375009536743
Validation loss: 2.4648419810879614

Epoch: 5| Step: 9
Training loss: 2.148873805999756
Validation loss: 2.4609444602843253

Epoch: 5| Step: 10
Training loss: 2.8728551864624023
Validation loss: 2.476615931398125

Epoch: 257| Step: 0
Training loss: 1.8865057229995728
Validation loss: 2.4724527430790726

Epoch: 5| Step: 1
Training loss: 3.044567584991455
Validation loss: 2.480958802725679

Epoch: 5| Step: 2
Training loss: 2.644467830657959
Validation loss: 2.496526082356771

Epoch: 5| Step: 3
Training loss: 2.3853259086608887
Validation loss: 2.503184954325358

Epoch: 5| Step: 4
Training loss: 2.9668259620666504
Validation loss: 2.5345067336995113

Epoch: 5| Step: 5
Training loss: 2.5665817260742188
Validation loss: 2.551163229891049

Epoch: 5| Step: 6
Training loss: 2.949740171432495
Validation loss: 2.56733000663019

Epoch: 5| Step: 7
Training loss: 2.2114145755767822
Validation loss: 2.5645660354245092

Epoch: 5| Step: 8
Training loss: 2.16947865486145
Validation loss: 2.5478401645537345

Epoch: 5| Step: 9
Training loss: 3.352153778076172
Validation loss: 2.5301577967982136

Epoch: 5| Step: 10
Training loss: 3.076774835586548
Validation loss: 2.482719688005345

Epoch: 258| Step: 0
Training loss: 2.8477981090545654
Validation loss: 2.4762730906086583

Epoch: 5| Step: 1
Training loss: 2.725187301635742
Validation loss: 2.4752374054283224

Epoch: 5| Step: 2
Training loss: 2.343179702758789
Validation loss: 2.4731120960686797

Epoch: 5| Step: 3
Training loss: 2.716550588607788
Validation loss: 2.4643508721423406

Epoch: 5| Step: 4
Training loss: 2.4051718711853027
Validation loss: 2.4551155669714815

Epoch: 5| Step: 5
Training loss: 2.420848846435547
Validation loss: 2.450802815857754

Epoch: 5| Step: 6
Training loss: 2.449158191680908
Validation loss: 2.453360652410856

Epoch: 5| Step: 7
Training loss: 2.8434512615203857
Validation loss: 2.455552324171989

Epoch: 5| Step: 8
Training loss: 3.2709991931915283
Validation loss: 2.464468709884151

Epoch: 5| Step: 9
Training loss: 2.425492286682129
Validation loss: 2.4548526476788264

Epoch: 5| Step: 10
Training loss: 2.746474504470825
Validation loss: 2.4556649654142317

Epoch: 259| Step: 0
Training loss: 2.133230686187744
Validation loss: 2.4505730008566253

Epoch: 5| Step: 1
Training loss: 2.9728810787200928
Validation loss: 2.44435893079286

Epoch: 5| Step: 2
Training loss: 2.200394630432129
Validation loss: 2.4363742156695296

Epoch: 5| Step: 3
Training loss: 2.5691447257995605
Validation loss: 2.44339777833672

Epoch: 5| Step: 4
Training loss: 3.0486903190612793
Validation loss: 2.438173124867101

Epoch: 5| Step: 5
Training loss: 2.2624731063842773
Validation loss: 2.441618791190527

Epoch: 5| Step: 6
Training loss: 2.1678192615509033
Validation loss: 2.4414944238560174

Epoch: 5| Step: 7
Training loss: 2.791512966156006
Validation loss: 2.440671997685586

Epoch: 5| Step: 8
Training loss: 2.753718614578247
Validation loss: 2.4525326272492767

Epoch: 5| Step: 9
Training loss: 2.994542360305786
Validation loss: 2.442910953234601

Epoch: 5| Step: 10
Training loss: 3.256640672683716
Validation loss: 2.4502588228512834

Epoch: 260| Step: 0
Training loss: 2.7361912727355957
Validation loss: 2.4472327052905993

Epoch: 5| Step: 1
Training loss: 2.7126059532165527
Validation loss: 2.4511294877657326

Epoch: 5| Step: 2
Training loss: 2.6651079654693604
Validation loss: 2.44789743679826

Epoch: 5| Step: 3
Training loss: 2.096559762954712
Validation loss: 2.444776109469834

Epoch: 5| Step: 4
Training loss: 2.571425437927246
Validation loss: 2.4517781170465613

Epoch: 5| Step: 5
Training loss: 2.7682385444641113
Validation loss: 2.447462384418775

Epoch: 5| Step: 6
Training loss: 2.429267168045044
Validation loss: 2.448159676726146

Epoch: 5| Step: 7
Training loss: 2.6859076023101807
Validation loss: 2.447785105756534

Epoch: 5| Step: 8
Training loss: 2.807158946990967
Validation loss: 2.4410765017232587

Epoch: 5| Step: 9
Training loss: 3.059781551361084
Validation loss: 2.447975450946439

Epoch: 5| Step: 10
Training loss: 2.464406967163086
Validation loss: 2.4463968584614415

Epoch: 261| Step: 0
Training loss: 2.3146812915802
Validation loss: 2.449293436542634

Epoch: 5| Step: 1
Training loss: 2.7396535873413086
Validation loss: 2.4528551434957855

Epoch: 5| Step: 2
Training loss: 2.7197587490081787
Validation loss: 2.452993236562257

Epoch: 5| Step: 3
Training loss: 2.686413288116455
Validation loss: 2.4528672977160384

Epoch: 5| Step: 4
Training loss: 2.524473190307617
Validation loss: 2.4460719887928297

Epoch: 5| Step: 5
Training loss: 2.5162644386291504
Validation loss: 2.4475657606637604

Epoch: 5| Step: 6
Training loss: 2.297247886657715
Validation loss: 2.4499169370179534

Epoch: 5| Step: 7
Training loss: 2.7919392585754395
Validation loss: 2.458414569977791

Epoch: 5| Step: 8
Training loss: 2.647667169570923
Validation loss: 2.452283531106928

Epoch: 5| Step: 9
Training loss: 2.7772650718688965
Validation loss: 2.4627375269448883

Epoch: 5| Step: 10
Training loss: 2.9512152671813965
Validation loss: 2.4525242543989614

Epoch: 262| Step: 0
Training loss: 2.134876012802124
Validation loss: 2.4525158443758563

Epoch: 5| Step: 1
Training loss: 3.101707935333252
Validation loss: 2.445413891987134

Epoch: 5| Step: 2
Training loss: 3.3166351318359375
Validation loss: 2.439444326585339

Epoch: 5| Step: 3
Training loss: 2.4543824195861816
Validation loss: 2.4366637045337307

Epoch: 5| Step: 4
Training loss: 2.5808072090148926
Validation loss: 2.4407206504575667

Epoch: 5| Step: 5
Training loss: 2.473032236099243
Validation loss: 2.438555694395496

Epoch: 5| Step: 6
Training loss: 2.3901119232177734
Validation loss: 2.4480097422035794

Epoch: 5| Step: 7
Training loss: 2.2751240730285645
Validation loss: 2.446329947440855

Epoch: 5| Step: 8
Training loss: 2.977799892425537
Validation loss: 2.4392063950979583

Epoch: 5| Step: 9
Training loss: 2.8428032398223877
Validation loss: 2.4498799847018335

Epoch: 5| Step: 10
Training loss: 2.2915942668914795
Validation loss: 2.4718203365161853

Epoch: 263| Step: 0
Training loss: 3.2563438415527344
Validation loss: 2.4720861552863993

Epoch: 5| Step: 1
Training loss: 3.2345778942108154
Validation loss: 2.4779118619939333

Epoch: 5| Step: 2
Training loss: 3.5753238201141357
Validation loss: 2.482840822589013

Epoch: 5| Step: 3
Training loss: 2.1468417644500732
Validation loss: 2.4919149055275867

Epoch: 5| Step: 4
Training loss: 2.808098554611206
Validation loss: 2.4892625065260034

Epoch: 5| Step: 5
Training loss: 1.9033581018447876
Validation loss: 2.479306197935535

Epoch: 5| Step: 6
Training loss: 1.776495337486267
Validation loss: 2.4622010210508942

Epoch: 5| Step: 7
Training loss: 2.25966739654541
Validation loss: 2.496485630671183

Epoch: 5| Step: 8
Training loss: 2.7280516624450684
Validation loss: 2.4933997649018482

Epoch: 5| Step: 9
Training loss: 2.5210537910461426
Validation loss: 2.502939654934791

Epoch: 5| Step: 10
Training loss: 2.571289300918579
Validation loss: 2.4858207292454217

Epoch: 264| Step: 0
Training loss: 1.8134574890136719
Validation loss: 2.483632802963257

Epoch: 5| Step: 1
Training loss: 2.5074079036712646
Validation loss: 2.4952903101521153

Epoch: 5| Step: 2
Training loss: 2.7815423011779785
Validation loss: 2.481020919738277

Epoch: 5| Step: 3
Training loss: 2.903902769088745
Validation loss: 2.4938046393855924

Epoch: 5| Step: 4
Training loss: 2.268381118774414
Validation loss: 2.493235226600401

Epoch: 5| Step: 5
Training loss: 3.0988903045654297
Validation loss: 2.4839951607488815

Epoch: 5| Step: 6
Training loss: 2.932504177093506
Validation loss: 2.484574987042335

Epoch: 5| Step: 7
Training loss: 2.9870598316192627
Validation loss: 2.495671236386863

Epoch: 5| Step: 8
Training loss: 2.6435749530792236
Validation loss: 2.4756527190567343

Epoch: 5| Step: 9
Training loss: 2.239344358444214
Validation loss: 2.482305283187538

Epoch: 5| Step: 10
Training loss: 2.490447759628296
Validation loss: 2.4655960811081754

Epoch: 265| Step: 0
Training loss: 3.0994725227355957
Validation loss: 2.4830495952278056

Epoch: 5| Step: 1
Training loss: 3.0595736503601074
Validation loss: 2.4826074902729323

Epoch: 5| Step: 2
Training loss: 2.8233017921447754
Validation loss: 2.480601264584449

Epoch: 5| Step: 3
Training loss: 2.0992159843444824
Validation loss: 2.4629601099157847

Epoch: 5| Step: 4
Training loss: 2.528500556945801
Validation loss: 2.45994225881433

Epoch: 5| Step: 5
Training loss: 2.5433106422424316
Validation loss: 2.4613422552744546

Epoch: 5| Step: 6
Training loss: 2.313697099685669
Validation loss: 2.4608922568700646

Epoch: 5| Step: 7
Training loss: 2.558230400085449
Validation loss: 2.458898890403009

Epoch: 5| Step: 8
Training loss: 3.2458033561706543
Validation loss: 2.4522435101129676

Epoch: 5| Step: 9
Training loss: 2.121796131134033
Validation loss: 2.4616693271103727

Epoch: 5| Step: 10
Training loss: 2.203364610671997
Validation loss: 2.4754606498185026

Epoch: 266| Step: 0
Training loss: 1.6309592723846436
Validation loss: 2.4930693077784714

Epoch: 5| Step: 1
Training loss: 2.106105327606201
Validation loss: 2.5213365862446446

Epoch: 5| Step: 2
Training loss: 3.0835518836975098
Validation loss: 2.522133301663142

Epoch: 5| Step: 3
Training loss: 2.597797155380249
Validation loss: 2.5120270816228722

Epoch: 5| Step: 4
Training loss: 3.2590794563293457
Validation loss: 2.506961481545561

Epoch: 5| Step: 5
Training loss: 2.0656561851501465
Validation loss: 2.468323899853614

Epoch: 5| Step: 6
Training loss: 3.4502933025360107
Validation loss: 2.4519744919192408

Epoch: 5| Step: 7
Training loss: 2.8778581619262695
Validation loss: 2.44684781823107

Epoch: 5| Step: 8
Training loss: 2.5476016998291016
Validation loss: 2.4341563409374607

Epoch: 5| Step: 9
Training loss: 2.907240390777588
Validation loss: 2.4363273164277435

Epoch: 5| Step: 10
Training loss: 2.3531978130340576
Validation loss: 2.4343208728298062

Epoch: 267| Step: 0
Training loss: 2.605419635772705
Validation loss: 2.4417014891101467

Epoch: 5| Step: 1
Training loss: 2.772838592529297
Validation loss: 2.438025833458029

Epoch: 5| Step: 2
Training loss: 2.4543166160583496
Validation loss: 2.440267647466352

Epoch: 5| Step: 3
Training loss: 2.682579517364502
Validation loss: 2.4416988049784014

Epoch: 5| Step: 4
Training loss: 2.701603412628174
Validation loss: 2.448968138746036

Epoch: 5| Step: 5
Training loss: 2.167945384979248
Validation loss: 2.4421928749289563

Epoch: 5| Step: 6
Training loss: 2.4306023120880127
Validation loss: 2.440377290530871

Epoch: 5| Step: 7
Training loss: 3.122014045715332
Validation loss: 2.4449621400525494

Epoch: 5| Step: 8
Training loss: 3.1294071674346924
Validation loss: 2.4435128601648475

Epoch: 5| Step: 9
Training loss: 2.2407171726226807
Validation loss: 2.436005602600754

Epoch: 5| Step: 10
Training loss: 2.496674060821533
Validation loss: 2.447709857776601

Epoch: 268| Step: 0
Training loss: 2.8065896034240723
Validation loss: 2.439643483008108

Epoch: 5| Step: 1
Training loss: 2.782172203063965
Validation loss: 2.4517875563713813

Epoch: 5| Step: 2
Training loss: 2.9021074771881104
Validation loss: 2.4481965649512505

Epoch: 5| Step: 3
Training loss: 2.1816351413726807
Validation loss: 2.445361380935997

Epoch: 5| Step: 4
Training loss: 2.6348791122436523
Validation loss: 2.4545063408472205

Epoch: 5| Step: 5
Training loss: 2.376708507537842
Validation loss: 2.451145395155876

Epoch: 5| Step: 6
Training loss: 2.7217235565185547
Validation loss: 2.4523415770581973

Epoch: 5| Step: 7
Training loss: 2.5126712322235107
Validation loss: 2.455430976806148

Epoch: 5| Step: 8
Training loss: 2.4782612323760986
Validation loss: 2.455811749222458

Epoch: 5| Step: 9
Training loss: 2.406729221343994
Validation loss: 2.4513669680523615

Epoch: 5| Step: 10
Training loss: 2.9366157054901123
Validation loss: 2.4434528222648044

Epoch: 269| Step: 0
Training loss: 3.419689893722534
Validation loss: 2.4593497489088323

Epoch: 5| Step: 1
Training loss: 2.325144052505493
Validation loss: 2.4495078722635903

Epoch: 5| Step: 2
Training loss: 2.7810885906219482
Validation loss: 2.4559715178705033

Epoch: 5| Step: 3
Training loss: 2.713435173034668
Validation loss: 2.4494722786770073

Epoch: 5| Step: 4
Training loss: 2.9278059005737305
Validation loss: 2.458541706044187

Epoch: 5| Step: 5
Training loss: 2.1014533042907715
Validation loss: 2.4474836472542054

Epoch: 5| Step: 6
Training loss: 3.1097283363342285
Validation loss: 2.442181364182503

Epoch: 5| Step: 7
Training loss: 2.1115989685058594
Validation loss: 2.437762955183624

Epoch: 5| Step: 8
Training loss: 2.375995397567749
Validation loss: 2.44346849636365

Epoch: 5| Step: 9
Training loss: 2.198782444000244
Validation loss: 2.4378367290701917

Epoch: 5| Step: 10
Training loss: 2.8755390644073486
Validation loss: 2.436869054712275

Epoch: 270| Step: 0
Training loss: 2.5037407875061035
Validation loss: 2.4411767067447787

Epoch: 5| Step: 1
Training loss: 3.263730525970459
Validation loss: 2.448620768003566

Epoch: 5| Step: 2
Training loss: 1.8788414001464844
Validation loss: 2.4387924850627942

Epoch: 5| Step: 3
Training loss: 2.5768251419067383
Validation loss: 2.4427897084143853

Epoch: 5| Step: 4
Training loss: 2.461812973022461
Validation loss: 2.435705933519589

Epoch: 5| Step: 5
Training loss: 2.3489527702331543
Validation loss: 2.4555734306253414

Epoch: 5| Step: 6
Training loss: 2.49484920501709
Validation loss: 2.4617786407470703

Epoch: 5| Step: 7
Training loss: 3.1496872901916504
Validation loss: 2.4851595765800885

Epoch: 5| Step: 8
Training loss: 2.911109685897827
Validation loss: 2.4900016810304377

Epoch: 5| Step: 9
Training loss: 2.3090381622314453
Validation loss: 2.4852533596818165

Epoch: 5| Step: 10
Training loss: 2.8393428325653076
Validation loss: 2.480711513949979

Epoch: 271| Step: 0
Training loss: 2.398540496826172
Validation loss: 2.4717237308461177

Epoch: 5| Step: 1
Training loss: 2.31764817237854
Validation loss: 2.4680743884014826

Epoch: 5| Step: 2
Training loss: 2.4310925006866455
Validation loss: 2.4564501995681436

Epoch: 5| Step: 3
Training loss: 2.1666433811187744
Validation loss: 2.4525386210410827

Epoch: 5| Step: 4
Training loss: 2.790243625640869
Validation loss: 2.458919963529033

Epoch: 5| Step: 5
Training loss: 2.9893524646759033
Validation loss: 2.4551308385787474

Epoch: 5| Step: 6
Training loss: 2.7537331581115723
Validation loss: 2.4594637475987917

Epoch: 5| Step: 7
Training loss: 2.765735149383545
Validation loss: 2.454824334831648

Epoch: 5| Step: 8
Training loss: 2.472661018371582
Validation loss: 2.454596875816263

Epoch: 5| Step: 9
Training loss: 2.4003307819366455
Validation loss: 2.475042786649478

Epoch: 5| Step: 10
Training loss: 3.348109006881714
Validation loss: 2.4798587829835954

Epoch: 272| Step: 0
Training loss: 2.0476279258728027
Validation loss: 2.4581218124717794

Epoch: 5| Step: 1
Training loss: 2.031597852706909
Validation loss: 2.4668119210068897

Epoch: 5| Step: 2
Training loss: 2.864638566970825
Validation loss: 2.472046890566426

Epoch: 5| Step: 3
Training loss: 2.8127973079681396
Validation loss: 2.4635390722623436

Epoch: 5| Step: 4
Training loss: 2.7153992652893066
Validation loss: 2.463003671297463

Epoch: 5| Step: 5
Training loss: 2.57716703414917
Validation loss: 2.460187107004145

Epoch: 5| Step: 6
Training loss: 2.863563299179077
Validation loss: 2.4461048136475267

Epoch: 5| Step: 7
Training loss: 3.2146084308624268
Validation loss: 2.4343965386831634

Epoch: 5| Step: 8
Training loss: 2.3959097862243652
Validation loss: 2.445321641942506

Epoch: 5| Step: 9
Training loss: 2.3826866149902344
Validation loss: 2.434015732939525

Epoch: 5| Step: 10
Training loss: 2.664449691772461
Validation loss: 2.443728082923479

Epoch: 273| Step: 0
Training loss: 2.8072457313537598
Validation loss: 2.4448238777857956

Epoch: 5| Step: 1
Training loss: 2.321329355239868
Validation loss: 2.444637075547249

Epoch: 5| Step: 2
Training loss: 3.3480923175811768
Validation loss: 2.4401276931967786

Epoch: 5| Step: 3
Training loss: 2.850053310394287
Validation loss: 2.4518449614124913

Epoch: 5| Step: 4
Training loss: 2.2658889293670654
Validation loss: 2.4450652996699014

Epoch: 5| Step: 5
Training loss: 2.126307725906372
Validation loss: 2.4418823642115437

Epoch: 5| Step: 6
Training loss: 2.7452075481414795
Validation loss: 2.4485291306690504

Epoch: 5| Step: 7
Training loss: 2.930051326751709
Validation loss: 2.4477104781776347

Epoch: 5| Step: 8
Training loss: 2.59686017036438
Validation loss: 2.448426733734787

Epoch: 5| Step: 9
Training loss: 2.286255359649658
Validation loss: 2.441195441830543

Epoch: 5| Step: 10
Training loss: 2.2956268787384033
Validation loss: 2.453995486741425

Epoch: 274| Step: 0
Training loss: 3.1970889568328857
Validation loss: 2.4433031902518323

Epoch: 5| Step: 1
Training loss: 1.977207899093628
Validation loss: 2.447922137475783

Epoch: 5| Step: 2
Training loss: 2.688321352005005
Validation loss: 2.4409686698708484

Epoch: 5| Step: 3
Training loss: 2.782437562942505
Validation loss: 2.4652166879305275

Epoch: 5| Step: 4
Training loss: 2.76863169670105
Validation loss: 2.4581115861092844

Epoch: 5| Step: 5
Training loss: 2.5551695823669434
Validation loss: 2.4505684273217314

Epoch: 5| Step: 6
Training loss: 2.116304397583008
Validation loss: 2.4619878568956928

Epoch: 5| Step: 7
Training loss: 2.949690103530884
Validation loss: 2.4629884201993226

Epoch: 5| Step: 8
Training loss: 2.6654438972473145
Validation loss: 2.454244318828788

Epoch: 5| Step: 9
Training loss: 2.226508617401123
Validation loss: 2.453048926527782

Epoch: 5| Step: 10
Training loss: 2.6685757637023926
Validation loss: 2.458648327858217

Epoch: 275| Step: 0
Training loss: 2.4169185161590576
Validation loss: 2.4372338479565037

Epoch: 5| Step: 1
Training loss: 2.849215030670166
Validation loss: 2.4530945567674536

Epoch: 5| Step: 2
Training loss: 2.7461962699890137
Validation loss: 2.448548763029037

Epoch: 5| Step: 3
Training loss: 2.302511692047119
Validation loss: 2.4439789633597098

Epoch: 5| Step: 4
Training loss: 1.8831520080566406
Validation loss: 2.448184028748543

Epoch: 5| Step: 5
Training loss: 3.539353847503662
Validation loss: 2.459134935050882

Epoch: 5| Step: 6
Training loss: 2.452683448791504
Validation loss: 2.465053778822704

Epoch: 5| Step: 7
Training loss: 3.125196695327759
Validation loss: 2.491582380828037

Epoch: 5| Step: 8
Training loss: 2.7817153930664062
Validation loss: 2.4750522721198296

Epoch: 5| Step: 9
Training loss: 2.356086015701294
Validation loss: 2.47421028665317

Epoch: 5| Step: 10
Training loss: 2.1127054691314697
Validation loss: 2.464944862550305

Epoch: 276| Step: 0
Training loss: 2.277930498123169
Validation loss: 2.4505607184543403

Epoch: 5| Step: 1
Training loss: 2.7312979698181152
Validation loss: 2.452820926584223

Epoch: 5| Step: 2
Training loss: 2.932373523712158
Validation loss: 2.4528806235200618

Epoch: 5| Step: 3
Training loss: 2.9114208221435547
Validation loss: 2.441180286868926

Epoch: 5| Step: 4
Training loss: 2.047607898712158
Validation loss: 2.4500102407188824

Epoch: 5| Step: 5
Training loss: 2.260591745376587
Validation loss: 2.447672438877885

Epoch: 5| Step: 6
Training loss: 2.5492234230041504
Validation loss: 2.453190747127738

Epoch: 5| Step: 7
Training loss: 2.49196457862854
Validation loss: 2.4460607369740806

Epoch: 5| Step: 8
Training loss: 2.562469959259033
Validation loss: 2.4447441485620316

Epoch: 5| Step: 9
Training loss: 2.89631986618042
Validation loss: 2.4550143852028796

Epoch: 5| Step: 10
Training loss: 2.8948144912719727
Validation loss: 2.4556875792882775

Epoch: 277| Step: 0
Training loss: 2.654531717300415
Validation loss: 2.459331589360391

Epoch: 5| Step: 1
Training loss: 1.8727508783340454
Validation loss: 2.4601871377678326

Epoch: 5| Step: 2
Training loss: 2.1536130905151367
Validation loss: 2.4667381701930875

Epoch: 5| Step: 3
Training loss: 2.429750442504883
Validation loss: 2.467147575911655

Epoch: 5| Step: 4
Training loss: 3.3121883869171143
Validation loss: 2.473270623914657

Epoch: 5| Step: 5
Training loss: 2.755315065383911
Validation loss: 2.4706484220361196

Epoch: 5| Step: 6
Training loss: 3.193357467651367
Validation loss: 2.4734082503985335

Epoch: 5| Step: 7
Training loss: 2.8197085857391357
Validation loss: 2.4767893898871636

Epoch: 5| Step: 8
Training loss: 2.6488852500915527
Validation loss: 2.454326001546716

Epoch: 5| Step: 9
Training loss: 2.257300615310669
Validation loss: 2.441731752887849

Epoch: 5| Step: 10
Training loss: 2.3685758113861084
Validation loss: 2.4342836308222946

Epoch: 278| Step: 0
Training loss: 3.2736027240753174
Validation loss: 2.4412421077810307

Epoch: 5| Step: 1
Training loss: 2.9446215629577637
Validation loss: 2.4507238275261334

Epoch: 5| Step: 2
Training loss: 2.2560513019561768
Validation loss: 2.443032544146302

Epoch: 5| Step: 3
Training loss: 2.5276482105255127
Validation loss: 2.443446361890403

Epoch: 5| Step: 4
Training loss: 3.5045409202575684
Validation loss: 2.4462396919086413

Epoch: 5| Step: 5
Training loss: 2.5169291496276855
Validation loss: 2.432408171315347

Epoch: 5| Step: 6
Training loss: 3.1942195892333984
Validation loss: 2.4286977526962117

Epoch: 5| Step: 7
Training loss: 1.9307544231414795
Validation loss: 2.4241723963009414

Epoch: 5| Step: 8
Training loss: 2.859274387359619
Validation loss: 2.424616439368135

Epoch: 5| Step: 9
Training loss: 1.9380178451538086
Validation loss: 2.4279780951879357

Epoch: 5| Step: 10
Training loss: 1.703465223312378
Validation loss: 2.416564172314059

Epoch: 279| Step: 0
Training loss: 2.131479024887085
Validation loss: 2.4261697389746226

Epoch: 5| Step: 1
Training loss: 2.2877323627471924
Validation loss: 2.4251586775625906

Epoch: 5| Step: 2
Training loss: 3.247631549835205
Validation loss: 2.427605346966815

Epoch: 5| Step: 3
Training loss: 2.418757915496826
Validation loss: 2.4332227835091214

Epoch: 5| Step: 4
Training loss: 2.669053316116333
Validation loss: 2.4205154552254626

Epoch: 5| Step: 5
Training loss: 2.9343719482421875
Validation loss: 2.418239455069265

Epoch: 5| Step: 6
Training loss: 2.598135471343994
Validation loss: 2.415363029767108

Epoch: 5| Step: 7
Training loss: 2.6337292194366455
Validation loss: 2.427150426372405

Epoch: 5| Step: 8
Training loss: 2.1077141761779785
Validation loss: 2.4237108768955355

Epoch: 5| Step: 9
Training loss: 2.8983800411224365
Validation loss: 2.4273643775652816

Epoch: 5| Step: 10
Training loss: 2.7097785472869873
Validation loss: 2.4248008779300156

Epoch: 280| Step: 0
Training loss: 2.294377088546753
Validation loss: 2.421364258694392

Epoch: 5| Step: 1
Training loss: 2.363363742828369
Validation loss: 2.4214483127799085

Epoch: 5| Step: 2
Training loss: 2.260300636291504
Validation loss: 2.450290861950126

Epoch: 5| Step: 3
Training loss: 3.0114033222198486
Validation loss: 2.438096482266662

Epoch: 5| Step: 4
Training loss: 3.2506649494171143
Validation loss: 2.43893491068194

Epoch: 5| Step: 5
Training loss: 2.6049387454986572
Validation loss: 2.43759798747237

Epoch: 5| Step: 6
Training loss: 2.2446975708007812
Validation loss: 2.4447183429553943

Epoch: 5| Step: 7
Training loss: 2.4633166790008545
Validation loss: 2.4522620349802

Epoch: 5| Step: 8
Training loss: 2.564945697784424
Validation loss: 2.4464092075183825

Epoch: 5| Step: 9
Training loss: 2.3172974586486816
Validation loss: 2.451862681296564

Epoch: 5| Step: 10
Training loss: 3.2217931747436523
Validation loss: 2.4570480828644126

Epoch: 281| Step: 0
Training loss: 2.6543262004852295
Validation loss: 2.4506800020894697

Epoch: 5| Step: 1
Training loss: 2.6672935485839844
Validation loss: 2.453748885021415

Epoch: 5| Step: 2
Training loss: 1.6288350820541382
Validation loss: 2.46039318525663

Epoch: 5| Step: 3
Training loss: 2.8734939098358154
Validation loss: 2.4607689508827786

Epoch: 5| Step: 4
Training loss: 2.553891658782959
Validation loss: 2.463108024289531

Epoch: 5| Step: 5
Training loss: 2.519927501678467
Validation loss: 2.4598316633573143

Epoch: 5| Step: 6
Training loss: 2.391528844833374
Validation loss: 2.465485800978958

Epoch: 5| Step: 7
Training loss: 2.786090850830078
Validation loss: 2.45697182737371

Epoch: 5| Step: 8
Training loss: 2.7753400802612305
Validation loss: 2.474074363708496

Epoch: 5| Step: 9
Training loss: 3.014841079711914
Validation loss: 2.4590870795711393

Epoch: 5| Step: 10
Training loss: 2.5701282024383545
Validation loss: 2.4422300246454056

Epoch: 282| Step: 0
Training loss: 2.2521588802337646
Validation loss: 2.445503724518643

Epoch: 5| Step: 1
Training loss: 2.6335253715515137
Validation loss: 2.441128125754736

Epoch: 5| Step: 2
Training loss: 2.890695095062256
Validation loss: 2.434553397599087

Epoch: 5| Step: 3
Training loss: 2.653376579284668
Validation loss: 2.431913441227328

Epoch: 5| Step: 4
Training loss: 2.16845965385437
Validation loss: 2.4183320024962067

Epoch: 5| Step: 5
Training loss: 2.75762677192688
Validation loss: 2.4173771104504986

Epoch: 5| Step: 6
Training loss: 2.909817695617676
Validation loss: 2.4125873837419736

Epoch: 5| Step: 7
Training loss: 2.849716901779175
Validation loss: 2.4253646327603247

Epoch: 5| Step: 8
Training loss: 2.69866681098938
Validation loss: 2.408147032542895

Epoch: 5| Step: 9
Training loss: 2.4472806453704834
Validation loss: 2.4159143688858196

Epoch: 5| Step: 10
Training loss: 2.193652629852295
Validation loss: 2.420939145549651

Epoch: 283| Step: 0
Training loss: 2.9170894622802734
Validation loss: 2.4174115221987487

Epoch: 5| Step: 1
Training loss: 2.6283421516418457
Validation loss: 2.4358375457025345

Epoch: 5| Step: 2
Training loss: 2.4234778881073
Validation loss: 2.4234474576929563

Epoch: 5| Step: 3
Training loss: 2.107001304626465
Validation loss: 2.441633621851603

Epoch: 5| Step: 4
Training loss: 3.0615947246551514
Validation loss: 2.4421501198122577

Epoch: 5| Step: 5
Training loss: 3.0144710540771484
Validation loss: 2.446006782593266

Epoch: 5| Step: 6
Training loss: 2.554426908493042
Validation loss: 2.433618204568022

Epoch: 5| Step: 7
Training loss: 2.396728515625
Validation loss: 2.4542513893496607

Epoch: 5| Step: 8
Training loss: 2.4848835468292236
Validation loss: 2.449842668348743

Epoch: 5| Step: 9
Training loss: 2.223599433898926
Validation loss: 2.4544211100506526

Epoch: 5| Step: 10
Training loss: 2.663137197494507
Validation loss: 2.4584625690214095

Epoch: 284| Step: 0
Training loss: 2.4943289756774902
Validation loss: 2.4601127537347938

Epoch: 5| Step: 1
Training loss: 2.2403557300567627
Validation loss: 2.4630729331765124

Epoch: 5| Step: 2
Training loss: 3.0716028213500977
Validation loss: 2.4481261955794467

Epoch: 5| Step: 3
Training loss: 2.706881046295166
Validation loss: 2.453000760847522

Epoch: 5| Step: 4
Training loss: 2.982022762298584
Validation loss: 2.4484654088174143

Epoch: 5| Step: 5
Training loss: 2.809690237045288
Validation loss: 2.449723697477771

Epoch: 5| Step: 6
Training loss: 2.2419612407684326
Validation loss: 2.444949614104404

Epoch: 5| Step: 7
Training loss: 2.631288528442383
Validation loss: 2.4291586799006306

Epoch: 5| Step: 8
Training loss: 2.03356671333313
Validation loss: 2.430274489105389

Epoch: 5| Step: 9
Training loss: 2.9645285606384277
Validation loss: 2.4387445526738323

Epoch: 5| Step: 10
Training loss: 2.20052170753479
Validation loss: 2.4561313531732045

Epoch: 285| Step: 0
Training loss: 2.3517401218414307
Validation loss: 2.4851302690403436

Epoch: 5| Step: 1
Training loss: 2.8434157371520996
Validation loss: 2.4643069774873796

Epoch: 5| Step: 2
Training loss: 2.550961494445801
Validation loss: 2.4762155086763444

Epoch: 5| Step: 3
Training loss: 2.534327507019043
Validation loss: 2.475189362802813

Epoch: 5| Step: 4
Training loss: 2.8178133964538574
Validation loss: 2.462777304392989

Epoch: 5| Step: 5
Training loss: 2.742427349090576
Validation loss: 2.4623302080297984

Epoch: 5| Step: 6
Training loss: 2.9785361289978027
Validation loss: 2.4581905206044516

Epoch: 5| Step: 7
Training loss: 2.7738070487976074
Validation loss: 2.4282211257565405

Epoch: 5| Step: 8
Training loss: 2.833556652069092
Validation loss: 2.4251488549734956

Epoch: 5| Step: 9
Training loss: 2.4106740951538086
Validation loss: 2.4256945194736605

Epoch: 5| Step: 10
Training loss: 1.629889965057373
Validation loss: 2.4178888131213445

Epoch: 286| Step: 0
Training loss: 2.353559970855713
Validation loss: 2.4246553964512323

Epoch: 5| Step: 1
Training loss: 2.5792031288146973
Validation loss: 2.425748150835755

Epoch: 5| Step: 2
Training loss: 2.3853039741516113
Validation loss: 2.4203031473262335

Epoch: 5| Step: 3
Training loss: 2.297868251800537
Validation loss: 2.4091675909616614

Epoch: 5| Step: 4
Training loss: 3.629185199737549
Validation loss: 2.4084866969816145

Epoch: 5| Step: 5
Training loss: 3.016836166381836
Validation loss: 2.4064281909696517

Epoch: 5| Step: 6
Training loss: 2.62581205368042
Validation loss: 2.4005811470811085

Epoch: 5| Step: 7
Training loss: 1.5626636743545532
Validation loss: 2.404449088599092

Epoch: 5| Step: 8
Training loss: 3.4654579162597656
Validation loss: 2.420604528919343

Epoch: 5| Step: 9
Training loss: 2.5430781841278076
Validation loss: 2.416095087605138

Epoch: 5| Step: 10
Training loss: 2.118713855743408
Validation loss: 2.4230508983776136

Epoch: 287| Step: 0
Training loss: 2.5094242095947266
Validation loss: 2.4308601348630843

Epoch: 5| Step: 1
Training loss: 2.515994071960449
Validation loss: 2.453734500433809

Epoch: 5| Step: 2
Training loss: 2.7196106910705566
Validation loss: 2.4608725501644995

Epoch: 5| Step: 3
Training loss: 2.6735599040985107
Validation loss: 2.4537571373806206

Epoch: 5| Step: 4
Training loss: 2.7332088947296143
Validation loss: 2.4519617711344073

Epoch: 5| Step: 5
Training loss: 2.6402153968811035
Validation loss: 2.4522296972172235

Epoch: 5| Step: 6
Training loss: 2.426149606704712
Validation loss: 2.454208048441077

Epoch: 5| Step: 7
Training loss: 2.4724910259246826
Validation loss: 2.452230079199678

Epoch: 5| Step: 8
Training loss: 2.777975082397461
Validation loss: 2.4594926603378786

Epoch: 5| Step: 9
Training loss: 2.7454566955566406
Validation loss: 2.465817395076957

Epoch: 5| Step: 10
Training loss: 2.2923104763031006
Validation loss: 2.449969978742702

Epoch: 288| Step: 0
Training loss: 3.333301067352295
Validation loss: 2.4520703361880396

Epoch: 5| Step: 1
Training loss: 2.351736545562744
Validation loss: 2.449089724530456

Epoch: 5| Step: 2
Training loss: 2.3162338733673096
Validation loss: 2.4408686904497046

Epoch: 5| Step: 3
Training loss: 2.4213645458221436
Validation loss: 2.421093653607112

Epoch: 5| Step: 4
Training loss: 2.749631881713867
Validation loss: 2.4234796095919866

Epoch: 5| Step: 5
Training loss: 1.5533055067062378
Validation loss: 2.429738742049022

Epoch: 5| Step: 6
Training loss: 2.9033443927764893
Validation loss: 2.4160505853673464

Epoch: 5| Step: 7
Training loss: 2.711665391921997
Validation loss: 2.4075026704419042

Epoch: 5| Step: 8
Training loss: 2.6869699954986572
Validation loss: 2.4193455814033427

Epoch: 5| Step: 9
Training loss: 2.1501383781433105
Validation loss: 2.425743656773721

Epoch: 5| Step: 10
Training loss: 3.2868566513061523
Validation loss: 2.431600396351148

Epoch: 289| Step: 0
Training loss: 3.40238881111145
Validation loss: 2.435072739919027

Epoch: 5| Step: 1
Training loss: 2.603437900543213
Validation loss: 2.43855203864395

Epoch: 5| Step: 2
Training loss: 2.403186798095703
Validation loss: 2.4556661100797754

Epoch: 5| Step: 3
Training loss: 2.255222797393799
Validation loss: 2.4671390851338706

Epoch: 5| Step: 4
Training loss: 2.328709125518799
Validation loss: 2.465034661754485

Epoch: 5| Step: 5
Training loss: 2.6041603088378906
Validation loss: 2.4498822637783584

Epoch: 5| Step: 6
Training loss: 2.8988893032073975
Validation loss: 2.4317224205181165

Epoch: 5| Step: 7
Training loss: 2.3782169818878174
Validation loss: 2.4219815782321397

Epoch: 5| Step: 8
Training loss: 3.0591447353363037
Validation loss: 2.4163924878643406

Epoch: 5| Step: 9
Training loss: 1.8336131572723389
Validation loss: 2.4214973706071095

Epoch: 5| Step: 10
Training loss: 2.6506643295288086
Validation loss: 2.4160711278197584

Epoch: 290| Step: 0
Training loss: 2.2653918266296387
Validation loss: 2.42084938223644

Epoch: 5| Step: 1
Training loss: 2.9823832511901855
Validation loss: 2.4231839628629785

Epoch: 5| Step: 2
Training loss: 2.621708869934082
Validation loss: 2.4195929317064184

Epoch: 5| Step: 3
Training loss: 3.057225465774536
Validation loss: 2.4168174907725346

Epoch: 5| Step: 4
Training loss: 1.762377142906189
Validation loss: 2.416610348609186

Epoch: 5| Step: 5
Training loss: 2.925328493118286
Validation loss: 2.428519869363436

Epoch: 5| Step: 6
Training loss: 3.126634120941162
Validation loss: 2.4248297393962903

Epoch: 5| Step: 7
Training loss: 2.9644317626953125
Validation loss: 2.4406195353436213

Epoch: 5| Step: 8
Training loss: 2.4655489921569824
Validation loss: 2.426212162099859

Epoch: 5| Step: 9
Training loss: 1.9697643518447876
Validation loss: 2.4403690625262517

Epoch: 5| Step: 10
Training loss: 2.169644832611084
Validation loss: 2.426594113790861

Epoch: 291| Step: 0
Training loss: 2.632087230682373
Validation loss: 2.4202455935939664

Epoch: 5| Step: 1
Training loss: 2.4014387130737305
Validation loss: 2.4215048538741244

Epoch: 5| Step: 2
Training loss: 2.3122382164001465
Validation loss: 2.4106747027366393

Epoch: 5| Step: 3
Training loss: 2.6275148391723633
Validation loss: 2.4162186038109565

Epoch: 5| Step: 4
Training loss: 1.887729287147522
Validation loss: 2.4165467498123006

Epoch: 5| Step: 5
Training loss: 2.838406562805176
Validation loss: 2.4352025601171676

Epoch: 5| Step: 6
Training loss: 3.000452756881714
Validation loss: 2.4279392098867767

Epoch: 5| Step: 7
Training loss: 2.138849973678589
Validation loss: 2.4330515553874354

Epoch: 5| Step: 8
Training loss: 3.086991310119629
Validation loss: 2.4317965148597636

Epoch: 5| Step: 9
Training loss: 2.4718151092529297
Validation loss: 2.4324135216333533

Epoch: 5| Step: 10
Training loss: 2.9287075996398926
Validation loss: 2.432564291902768

Epoch: 292| Step: 0
Training loss: 2.070253372192383
Validation loss: 2.4313343096804876

Epoch: 5| Step: 1
Training loss: 3.109506368637085
Validation loss: 2.4374543056693128

Epoch: 5| Step: 2
Training loss: 2.1976356506347656
Validation loss: 2.42648958903487

Epoch: 5| Step: 3
Training loss: 2.0321078300476074
Validation loss: 2.4340219907863165

Epoch: 5| Step: 4
Training loss: 2.8504154682159424
Validation loss: 2.44516558031882

Epoch: 5| Step: 5
Training loss: 2.9169187545776367
Validation loss: 2.4294886153231383

Epoch: 5| Step: 6
Training loss: 2.529768466949463
Validation loss: 2.443640419231948

Epoch: 5| Step: 7
Training loss: 2.480635166168213
Validation loss: 2.4322517379637687

Epoch: 5| Step: 8
Training loss: 2.5364670753479004
Validation loss: 2.4320904952223583

Epoch: 5| Step: 9
Training loss: 2.813563108444214
Validation loss: 2.415946457975654

Epoch: 5| Step: 10
Training loss: 2.755648612976074
Validation loss: 2.4274356749749955

Epoch: 293| Step: 0
Training loss: 2.390465259552002
Validation loss: 2.435899644769648

Epoch: 5| Step: 1
Training loss: 2.4880433082580566
Validation loss: 2.4337276745867986

Epoch: 5| Step: 2
Training loss: 2.336444854736328
Validation loss: 2.44249306186553

Epoch: 5| Step: 3
Training loss: 2.7417941093444824
Validation loss: 2.4385037127361504

Epoch: 5| Step: 4
Training loss: 2.5308737754821777
Validation loss: 2.448279027015932

Epoch: 5| Step: 5
Training loss: 1.7585617303848267
Validation loss: 2.4432750978777484

Epoch: 5| Step: 6
Training loss: 2.4020934104919434
Validation loss: 2.42119603003225

Epoch: 5| Step: 7
Training loss: 3.2833800315856934
Validation loss: 2.4096651795089885

Epoch: 5| Step: 8
Training loss: 2.5039286613464355
Validation loss: 2.4142944851229267

Epoch: 5| Step: 9
Training loss: 2.4967968463897705
Validation loss: 2.407952062545284

Epoch: 5| Step: 10
Training loss: 3.460211753845215
Validation loss: 2.4178969270439556

Epoch: 294| Step: 0
Training loss: 2.7010092735290527
Validation loss: 2.4251557524486254

Epoch: 5| Step: 1
Training loss: 1.7306807041168213
Validation loss: 2.4342180400766353

Epoch: 5| Step: 2
Training loss: 3.081876754760742
Validation loss: 2.4506330541385117

Epoch: 5| Step: 3
Training loss: 1.8853471279144287
Validation loss: 2.4558831825051257

Epoch: 5| Step: 4
Training loss: 2.6379032135009766
Validation loss: 2.450240565884498

Epoch: 5| Step: 5
Training loss: 3.4837646484375
Validation loss: 2.450755016778105

Epoch: 5| Step: 6
Training loss: 2.610372543334961
Validation loss: 2.4516660167324926

Epoch: 5| Step: 7
Training loss: 2.5049779415130615
Validation loss: 2.4462011706444526

Epoch: 5| Step: 8
Training loss: 2.783769369125366
Validation loss: 2.4459920570414555

Epoch: 5| Step: 9
Training loss: 2.590895891189575
Validation loss: 2.4407780401168333

Epoch: 5| Step: 10
Training loss: 2.481692314147949
Validation loss: 2.4269268897271927

Epoch: 295| Step: 0
Training loss: 2.960592746734619
Validation loss: 2.417318428716352

Epoch: 5| Step: 1
Training loss: 3.0584375858306885
Validation loss: 2.4055818998685448

Epoch: 5| Step: 2
Training loss: 2.5856575965881348
Validation loss: 2.403046756662348

Epoch: 5| Step: 3
Training loss: 2.9967308044433594
Validation loss: 2.399376925601754

Epoch: 5| Step: 4
Training loss: 2.9811367988586426
Validation loss: 2.3986783591649865

Epoch: 5| Step: 5
Training loss: 1.8987905979156494
Validation loss: 2.401537997748262

Epoch: 5| Step: 6
Training loss: 2.7675564289093018
Validation loss: 2.3966192430065525

Epoch: 5| Step: 7
Training loss: 2.223398447036743
Validation loss: 2.3958699754489365

Epoch: 5| Step: 8
Training loss: 1.9904873371124268
Validation loss: 2.3912314086832027

Epoch: 5| Step: 9
Training loss: 2.229348659515381
Validation loss: 2.3914769593105523

Epoch: 5| Step: 10
Training loss: 2.7219011783599854
Validation loss: 2.405475162690686

Epoch: 296| Step: 0
Training loss: 2.9266109466552734
Validation loss: 2.4100764746307046

Epoch: 5| Step: 1
Training loss: 3.4198296070098877
Validation loss: 2.4181679987138316

Epoch: 5| Step: 2
Training loss: 2.4674437046051025
Validation loss: 2.432284285945277

Epoch: 5| Step: 3
Training loss: 2.2631566524505615
Validation loss: 2.4218135597885295

Epoch: 5| Step: 4
Training loss: 1.8110182285308838
Validation loss: 2.443313898578767

Epoch: 5| Step: 5
Training loss: 1.657569169998169
Validation loss: 2.4400509044688237

Epoch: 5| Step: 6
Training loss: 2.8963849544525146
Validation loss: 2.445422762183733

Epoch: 5| Step: 7
Training loss: 3.103147029876709
Validation loss: 2.442695494621031

Epoch: 5| Step: 8
Training loss: 2.0573723316192627
Validation loss: 2.429417689641317

Epoch: 5| Step: 9
Training loss: 2.656135082244873
Validation loss: 2.4434313363926385

Epoch: 5| Step: 10
Training loss: 3.1960458755493164
Validation loss: 2.4387751240884104

Epoch: 297| Step: 0
Training loss: 2.448172092437744
Validation loss: 2.433675266081287

Epoch: 5| Step: 1
Training loss: 2.2954583168029785
Validation loss: 2.4189209322775564

Epoch: 5| Step: 2
Training loss: 2.1076252460479736
Validation loss: 2.423853323023806

Epoch: 5| Step: 3
Training loss: 2.744081497192383
Validation loss: 2.4202527307694957

Epoch: 5| Step: 4
Training loss: 2.9607605934143066
Validation loss: 2.4145972087819088

Epoch: 5| Step: 5
Training loss: 2.983956813812256
Validation loss: 2.425909262831493

Epoch: 5| Step: 6
Training loss: 2.910673141479492
Validation loss: 2.422952672486664

Epoch: 5| Step: 7
Training loss: 2.4810116291046143
Validation loss: 2.4200864914924867

Epoch: 5| Step: 8
Training loss: 2.1438677310943604
Validation loss: 2.4352141093182307

Epoch: 5| Step: 9
Training loss: 2.302964687347412
Validation loss: 2.4498262969396447

Epoch: 5| Step: 10
Training loss: 3.023458957672119
Validation loss: 2.442082771690943

Epoch: 298| Step: 0
Training loss: 2.776175022125244
Validation loss: 2.4690133781843286

Epoch: 5| Step: 1
Training loss: 2.112375497817993
Validation loss: 2.4611613237729637

Epoch: 5| Step: 2
Training loss: 2.152467966079712
Validation loss: 2.4432181491646716

Epoch: 5| Step: 3
Training loss: 2.2928130626678467
Validation loss: 2.4374368165128972

Epoch: 5| Step: 4
Training loss: 3.0987000465393066
Validation loss: 2.422829745918192

Epoch: 5| Step: 5
Training loss: 1.9504915475845337
Validation loss: 2.396039011657879

Epoch: 5| Step: 6
Training loss: 3.357851505279541
Validation loss: 2.4169961675520866

Epoch: 5| Step: 7
Training loss: 2.359954833984375
Validation loss: 2.414841567316363

Epoch: 5| Step: 8
Training loss: 2.9500155448913574
Validation loss: 2.427126956242387

Epoch: 5| Step: 9
Training loss: 2.803753137588501
Validation loss: 2.429571872116417

Epoch: 5| Step: 10
Training loss: 2.525606870651245
Validation loss: 2.438522884922643

Epoch: 299| Step: 0
Training loss: 2.657608985900879
Validation loss: 2.4217925674171856

Epoch: 5| Step: 1
Training loss: 2.8384909629821777
Validation loss: 2.4077026177478094

Epoch: 5| Step: 2
Training loss: 2.422884702682495
Validation loss: 2.4083815492609495

Epoch: 5| Step: 3
Training loss: 2.585019111633301
Validation loss: 2.403569165096488

Epoch: 5| Step: 4
Training loss: 2.6794848442077637
Validation loss: 2.3949731985727944

Epoch: 5| Step: 5
Training loss: 3.232771635055542
Validation loss: 2.3942108872116252

Epoch: 5| Step: 6
Training loss: 2.0554347038269043
Validation loss: 2.4148763200288177

Epoch: 5| Step: 7
Training loss: 1.8815120458602905
Validation loss: 2.418834956743384

Epoch: 5| Step: 8
Training loss: 3.031000852584839
Validation loss: 2.426674199360673

Epoch: 5| Step: 9
Training loss: 2.1488003730773926
Validation loss: 2.4267399567429737

Epoch: 5| Step: 10
Training loss: 2.8161211013793945
Validation loss: 2.4188106508665186

Epoch: 300| Step: 0
Training loss: 1.7032172679901123
Validation loss: 2.416006736857917

Epoch: 5| Step: 1
Training loss: 2.2770538330078125
Validation loss: 2.4147929606899137

Epoch: 5| Step: 2
Training loss: 2.5667717456817627
Validation loss: 2.4212610952315794

Epoch: 5| Step: 3
Training loss: 2.3334686756134033
Validation loss: 2.4114923118263163

Epoch: 5| Step: 4
Training loss: 3.178431987762451
Validation loss: 2.4183704109602076

Epoch: 5| Step: 5
Training loss: 2.6122100353240967
Validation loss: 2.4145047100641395

Epoch: 5| Step: 6
Training loss: 1.9459333419799805
Validation loss: 2.4182495993952595

Epoch: 5| Step: 7
Training loss: 2.955321788787842
Validation loss: 2.417094704925373

Epoch: 5| Step: 8
Training loss: 2.533334732055664
Validation loss: 2.4259793117482173

Epoch: 5| Step: 9
Training loss: 3.3670520782470703
Validation loss: 2.4286820760337253

Epoch: 5| Step: 10
Training loss: 2.762169122695923
Validation loss: 2.425664911987961

Epoch: 301| Step: 0
Training loss: 2.399521827697754
Validation loss: 2.413611570994059

Epoch: 5| Step: 1
Training loss: 2.956861972808838
Validation loss: 2.421070614168721

Epoch: 5| Step: 2
Training loss: 3.0556154251098633
Validation loss: 2.4131562889263196

Epoch: 5| Step: 3
Training loss: 2.6608612537384033
Validation loss: 2.4135673353748937

Epoch: 5| Step: 4
Training loss: 1.7469103336334229
Validation loss: 2.4090851763243317

Epoch: 5| Step: 5
Training loss: 2.38452410697937
Validation loss: 2.4151819623926634

Epoch: 5| Step: 6
Training loss: 2.433893918991089
Validation loss: 2.3972796419615388

Epoch: 5| Step: 7
Training loss: 3.590221405029297
Validation loss: 2.4050351932484615

Epoch: 5| Step: 8
Training loss: 2.1123385429382324
Validation loss: 2.4038571824309645

Epoch: 5| Step: 9
Training loss: 2.3963823318481445
Validation loss: 2.4066801327531055

Epoch: 5| Step: 10
Training loss: 2.582122325897217
Validation loss: 2.404267511060161

Epoch: 302| Step: 0
Training loss: 2.1485180854797363
Validation loss: 2.4068588031235563

Epoch: 5| Step: 1
Training loss: 2.4325356483459473
Validation loss: 2.409773672780683

Epoch: 5| Step: 2
Training loss: 2.4823243618011475
Validation loss: 2.4092038293038645

Epoch: 5| Step: 3
Training loss: 2.724799871444702
Validation loss: 2.4201354031921714

Epoch: 5| Step: 4
Training loss: 2.8059558868408203
Validation loss: 2.4160459797869445

Epoch: 5| Step: 5
Training loss: 2.6926989555358887
Validation loss: 2.4090435889459427

Epoch: 5| Step: 6
Training loss: 1.7531877756118774
Validation loss: 2.4170899570629163

Epoch: 5| Step: 7
Training loss: 2.7764689922332764
Validation loss: 2.4158236416437293

Epoch: 5| Step: 8
Training loss: 1.7708603143692017
Validation loss: 2.4228146050565984

Epoch: 5| Step: 9
Training loss: 3.177320718765259
Validation loss: 2.4218082517705937

Epoch: 5| Step: 10
Training loss: 3.530172109603882
Validation loss: 2.430687878721504

Epoch: 303| Step: 0
Training loss: 2.0987002849578857
Validation loss: 2.413769309238721

Epoch: 5| Step: 1
Training loss: 2.427515745162964
Validation loss: 2.4207979350961666

Epoch: 5| Step: 2
Training loss: 2.447946071624756
Validation loss: 2.4088146609644734

Epoch: 5| Step: 3
Training loss: 2.518411159515381
Validation loss: 2.416877367163217

Epoch: 5| Step: 4
Training loss: 2.358452320098877
Validation loss: 2.416777551815074

Epoch: 5| Step: 5
Training loss: 2.6026318073272705
Validation loss: 2.4179951554985455

Epoch: 5| Step: 6
Training loss: 2.5222346782684326
Validation loss: 2.4157397465039323

Epoch: 5| Step: 7
Training loss: 2.601097822189331
Validation loss: 2.424099950380223

Epoch: 5| Step: 8
Training loss: 2.266038417816162
Validation loss: 2.4178041899076073

Epoch: 5| Step: 9
Training loss: 3.069540500640869
Validation loss: 2.420599276019681

Epoch: 5| Step: 10
Training loss: 3.2378408908843994
Validation loss: 2.4334619916895384

Epoch: 304| Step: 0
Training loss: 2.80937123298645
Validation loss: 2.4179130138889438

Epoch: 5| Step: 1
Training loss: 3.1238815784454346
Validation loss: 2.423130992920168

Epoch: 5| Step: 2
Training loss: 1.6748735904693604
Validation loss: 2.4328178103252123

Epoch: 5| Step: 3
Training loss: 2.1884710788726807
Validation loss: 2.4378293855215913

Epoch: 5| Step: 4
Training loss: 2.696946144104004
Validation loss: 2.450153368775563

Epoch: 5| Step: 5
Training loss: 1.5037729740142822
Validation loss: 2.4400270792745773

Epoch: 5| Step: 6
Training loss: 2.409255266189575
Validation loss: 2.421635840528755

Epoch: 5| Step: 7
Training loss: 2.8948421478271484
Validation loss: 2.412562160081761

Epoch: 5| Step: 8
Training loss: 3.0726354122161865
Validation loss: 2.403260202818019

Epoch: 5| Step: 9
Training loss: 2.9733593463897705
Validation loss: 2.408642486859393

Epoch: 5| Step: 10
Training loss: 2.7308900356292725
Validation loss: 2.409352315369473

Epoch: 305| Step: 0
Training loss: 2.8266570568084717
Validation loss: 2.4013565996641755

Epoch: 5| Step: 1
Training loss: 1.2442222833633423
Validation loss: 2.3999520604328444

Epoch: 5| Step: 2
Training loss: 2.807786226272583
Validation loss: 2.396532174079649

Epoch: 5| Step: 3
Training loss: 2.713893413543701
Validation loss: 2.403839025446164

Epoch: 5| Step: 4
Training loss: 3.628126859664917
Validation loss: 2.3962509785929034

Epoch: 5| Step: 5
Training loss: 2.620192050933838
Validation loss: 2.390728671063659

Epoch: 5| Step: 6
Training loss: 2.0181217193603516
Validation loss: 2.4066629102153163

Epoch: 5| Step: 7
Training loss: 2.9831624031066895
Validation loss: 2.4092112356616604

Epoch: 5| Step: 8
Training loss: 2.5617945194244385
Validation loss: 2.402156429906045

Epoch: 5| Step: 9
Training loss: 2.012298107147217
Validation loss: 2.4109463807075255

Epoch: 5| Step: 10
Training loss: 2.7620046138763428
Validation loss: 2.4157766988200526

Epoch: 306| Step: 0
Training loss: 2.9190807342529297
Validation loss: 2.4161101823211997

Epoch: 5| Step: 1
Training loss: 2.5782036781311035
Validation loss: 2.410180840440976

Epoch: 5| Step: 2
Training loss: 2.507331132888794
Validation loss: 2.4092383128340527

Epoch: 5| Step: 3
Training loss: 2.7228360176086426
Validation loss: 2.4019821356701594

Epoch: 5| Step: 4
Training loss: 2.1146140098571777
Validation loss: 2.4068182463287027

Epoch: 5| Step: 5
Training loss: 2.756300210952759
Validation loss: 2.407203403852319

Epoch: 5| Step: 6
Training loss: 2.371448040008545
Validation loss: 2.4026649331533783

Epoch: 5| Step: 7
Training loss: 2.2215576171875
Validation loss: 2.423227233271445

Epoch: 5| Step: 8
Training loss: 2.3390650749206543
Validation loss: 2.4155734418540873

Epoch: 5| Step: 9
Training loss: 3.1071152687072754
Validation loss: 2.4240709735501196

Epoch: 5| Step: 10
Training loss: 2.3128669261932373
Validation loss: 2.4276674319339056

Epoch: 307| Step: 0
Training loss: 2.091318368911743
Validation loss: 2.4236992918035036

Epoch: 5| Step: 1
Training loss: 2.7204577922821045
Validation loss: 2.434709918114447

Epoch: 5| Step: 2
Training loss: 3.066145658493042
Validation loss: 2.4160660518113004

Epoch: 5| Step: 3
Training loss: 2.286639451980591
Validation loss: 2.415108685852379

Epoch: 5| Step: 4
Training loss: 1.6678493022918701
Validation loss: 2.4145224196936494

Epoch: 5| Step: 5
Training loss: 2.902515411376953
Validation loss: 2.4055395357070433

Epoch: 5| Step: 6
Training loss: 2.3700740337371826
Validation loss: 2.4099623772405807

Epoch: 5| Step: 7
Training loss: 2.2731003761291504
Validation loss: 2.4104891695002073

Epoch: 5| Step: 8
Training loss: 2.705629825592041
Validation loss: 2.4121401771422355

Epoch: 5| Step: 9
Training loss: 3.1518003940582275
Validation loss: 2.4144113038175847

Epoch: 5| Step: 10
Training loss: 2.87020206451416
Validation loss: 2.402003336978215

Epoch: 308| Step: 0
Training loss: 2.2208404541015625
Validation loss: 2.4151213502371185

Epoch: 5| Step: 1
Training loss: 2.180253505706787
Validation loss: 2.4254712238106677

Epoch: 5| Step: 2
Training loss: 2.3354499340057373
Validation loss: 2.4200288634146414

Epoch: 5| Step: 3
Training loss: 2.9037086963653564
Validation loss: 2.4136137141976306

Epoch: 5| Step: 4
Training loss: 3.1528964042663574
Validation loss: 2.429477337867983

Epoch: 5| Step: 5
Training loss: 2.813040018081665
Validation loss: 2.405077567664526

Epoch: 5| Step: 6
Training loss: 2.249023914337158
Validation loss: 2.3876633413376345

Epoch: 5| Step: 7
Training loss: 2.881162166595459
Validation loss: 2.3880684529581377

Epoch: 5| Step: 8
Training loss: 2.0555527210235596
Validation loss: 2.3829015788211616

Epoch: 5| Step: 9
Training loss: 2.8099796772003174
Validation loss: 2.372978925704956

Epoch: 5| Step: 10
Training loss: 2.5385708808898926
Validation loss: 2.3835596935723418

Epoch: 309| Step: 0
Training loss: 2.772016763687134
Validation loss: 2.3826457941403953

Epoch: 5| Step: 1
Training loss: 2.6946675777435303
Validation loss: 2.378348353088543

Epoch: 5| Step: 2
Training loss: 3.075249433517456
Validation loss: 2.3927629096533662

Epoch: 5| Step: 3
Training loss: 2.6779918670654297
Validation loss: 2.3902663646205777

Epoch: 5| Step: 4
Training loss: 1.9970899820327759
Validation loss: 2.4097173547232025

Epoch: 5| Step: 5
Training loss: 2.452212333679199
Validation loss: 2.412312412774691

Epoch: 5| Step: 6
Training loss: 2.1833999156951904
Validation loss: 2.4191300176805064

Epoch: 5| Step: 7
Training loss: 2.772853136062622
Validation loss: 2.4281771772651264

Epoch: 5| Step: 8
Training loss: 2.535674810409546
Validation loss: 2.421810101437312

Epoch: 5| Step: 9
Training loss: 2.4810872077941895
Validation loss: 2.4264869613032185

Epoch: 5| Step: 10
Training loss: 2.342437505722046
Validation loss: 2.4312033396895214

Epoch: 310| Step: 0
Training loss: 2.6326780319213867
Validation loss: 2.433858363859115

Epoch: 5| Step: 1
Training loss: 2.6391866207122803
Validation loss: 2.4291914714279996

Epoch: 5| Step: 2
Training loss: 2.0429587364196777
Validation loss: 2.4152315816571637

Epoch: 5| Step: 3
Training loss: 2.9382078647613525
Validation loss: 2.4104833731087307

Epoch: 5| Step: 4
Training loss: 2.332423686981201
Validation loss: 2.4052622369540635

Epoch: 5| Step: 5
Training loss: 3.1170973777770996
Validation loss: 2.3994054153401363

Epoch: 5| Step: 6
Training loss: 2.4824624061584473
Validation loss: 2.391751176567488

Epoch: 5| Step: 7
Training loss: 2.6166110038757324
Validation loss: 2.3980441580536547

Epoch: 5| Step: 8
Training loss: 2.5846567153930664
Validation loss: 2.3824867612572125

Epoch: 5| Step: 9
Training loss: 2.2215256690979004
Validation loss: 2.3973928215683147

Epoch: 5| Step: 10
Training loss: 2.418271541595459
Validation loss: 2.3854636710177184

Epoch: 311| Step: 0
Training loss: 2.5625367164611816
Validation loss: 2.4035568775669223

Epoch: 5| Step: 1
Training loss: 2.639530658721924
Validation loss: 2.400800733156102

Epoch: 5| Step: 2
Training loss: 2.3607563972473145
Validation loss: 2.3989499281811457

Epoch: 5| Step: 3
Training loss: 2.316011905670166
Validation loss: 2.4022581731119463

Epoch: 5| Step: 4
Training loss: 2.6734085083007812
Validation loss: 2.393573130330732

Epoch: 5| Step: 5
Training loss: 2.0890443325042725
Validation loss: 2.4131946384265857

Epoch: 5| Step: 6
Training loss: 2.96132755279541
Validation loss: 2.4365105987876974

Epoch: 5| Step: 7
Training loss: 2.50736665725708
Validation loss: 2.48213618288758

Epoch: 5| Step: 8
Training loss: 2.402005434036255
Validation loss: 2.4703129670953237

Epoch: 5| Step: 9
Training loss: 2.8697991371154785
Validation loss: 2.4509086737068753

Epoch: 5| Step: 10
Training loss: 2.8815011978149414
Validation loss: 2.4193545592728483

Epoch: 312| Step: 0
Training loss: 1.8944942951202393
Validation loss: 2.398406623512186

Epoch: 5| Step: 1
Training loss: 3.001770257949829
Validation loss: 2.4004695287314792

Epoch: 5| Step: 2
Training loss: 2.0975635051727295
Validation loss: 2.4080294280923824

Epoch: 5| Step: 3
Training loss: 2.564748764038086
Validation loss: 2.408854805013185

Epoch: 5| Step: 4
Training loss: 2.496441125869751
Validation loss: 2.414111929555093

Epoch: 5| Step: 5
Training loss: 2.6896703243255615
Validation loss: 2.401805857176422

Epoch: 5| Step: 6
Training loss: 3.4076075553894043
Validation loss: 2.397058856102728

Epoch: 5| Step: 7
Training loss: 2.5158634185791016
Validation loss: 2.3908635749611804

Epoch: 5| Step: 8
Training loss: 2.5904476642608643
Validation loss: 2.3898033429217596

Epoch: 5| Step: 9
Training loss: 2.351288080215454
Validation loss: 2.3915133476257324

Epoch: 5| Step: 10
Training loss: 2.358156204223633
Validation loss: 2.382042436189549

Epoch: 313| Step: 0
Training loss: 1.9653713703155518
Validation loss: 2.4082554847963396

Epoch: 5| Step: 1
Training loss: 2.051621437072754
Validation loss: 2.4008178762210313

Epoch: 5| Step: 2
Training loss: 2.7064156532287598
Validation loss: 2.4275017169214066

Epoch: 5| Step: 3
Training loss: 3.6402008533477783
Validation loss: 2.4315609188490015

Epoch: 5| Step: 4
Training loss: 1.5103156566619873
Validation loss: 2.4300299972616215

Epoch: 5| Step: 5
Training loss: 2.7714617252349854
Validation loss: 2.4117724946750108

Epoch: 5| Step: 6
Training loss: 3.193483829498291
Validation loss: 2.4049639906934512

Epoch: 5| Step: 7
Training loss: 2.3988308906555176
Validation loss: 2.386305978221278

Epoch: 5| Step: 8
Training loss: 2.3962113857269287
Validation loss: 2.378810312158318

Epoch: 5| Step: 9
Training loss: 2.299384117126465
Validation loss: 2.3676925448961157

Epoch: 5| Step: 10
Training loss: 3.1692800521850586
Validation loss: 2.364146145441199

Epoch: 314| Step: 0
Training loss: 2.600083112716675
Validation loss: 2.3625567805382515

Epoch: 5| Step: 1
Training loss: 2.792513370513916
Validation loss: 2.3661296803464174

Epoch: 5| Step: 2
Training loss: 2.655625820159912
Validation loss: 2.367605802833393

Epoch: 5| Step: 3
Training loss: 3.3749966621398926
Validation loss: 2.376625591708768

Epoch: 5| Step: 4
Training loss: 1.963173270225525
Validation loss: 2.38332139548435

Epoch: 5| Step: 5
Training loss: 2.6796717643737793
Validation loss: 2.3926145594607116

Epoch: 5| Step: 6
Training loss: 1.8968805074691772
Validation loss: 2.3821628734629643

Epoch: 5| Step: 7
Training loss: 2.3694005012512207
Validation loss: 2.3960802785811888

Epoch: 5| Step: 8
Training loss: 2.3386778831481934
Validation loss: 2.4090321961269585

Epoch: 5| Step: 9
Training loss: 2.9698500633239746
Validation loss: 2.41930656022923

Epoch: 5| Step: 10
Training loss: 2.419342517852783
Validation loss: 2.4364266344296035

Epoch: 315| Step: 0
Training loss: 2.2810490131378174
Validation loss: 2.429464283809867

Epoch: 5| Step: 1
Training loss: 2.2277798652648926
Validation loss: 2.444698618304345

Epoch: 5| Step: 2
Training loss: 1.9453048706054688
Validation loss: 2.428546331262076

Epoch: 5| Step: 3
Training loss: 2.5254006385803223
Validation loss: 2.425789233176939

Epoch: 5| Step: 4
Training loss: 3.0512046813964844
Validation loss: 2.406596165831371

Epoch: 5| Step: 5
Training loss: 2.734785318374634
Validation loss: 2.3943795952745663

Epoch: 5| Step: 6
Training loss: 3.194443941116333
Validation loss: 2.3996205842623146

Epoch: 5| Step: 7
Training loss: 2.9732704162597656
Validation loss: 2.3890641017626693

Epoch: 5| Step: 8
Training loss: 2.3444302082061768
Validation loss: 2.3836142786087526

Epoch: 5| Step: 9
Training loss: 1.8933703899383545
Validation loss: 2.3791600376047115

Epoch: 5| Step: 10
Training loss: 2.903169631958008
Validation loss: 2.3930019281243764

Epoch: 316| Step: 0
Training loss: 2.2256267070770264
Validation loss: 2.395883106416272

Epoch: 5| Step: 1
Training loss: 2.006155490875244
Validation loss: 2.3812845253175303

Epoch: 5| Step: 2
Training loss: 2.1592421531677246
Validation loss: 2.401559655384351

Epoch: 5| Step: 3
Training loss: 2.449605703353882
Validation loss: 2.3991092635739233

Epoch: 5| Step: 4
Training loss: 2.7255237102508545
Validation loss: 2.3967096267207975

Epoch: 5| Step: 5
Training loss: 2.547834634780884
Validation loss: 2.405192834074779

Epoch: 5| Step: 6
Training loss: 2.8629956245422363
Validation loss: 2.393526256725352

Epoch: 5| Step: 7
Training loss: 2.913755416870117
Validation loss: 2.3955075407540924

Epoch: 5| Step: 8
Training loss: 3.120560884475708
Validation loss: 2.3889842930660454

Epoch: 5| Step: 9
Training loss: 2.600623369216919
Validation loss: 2.38046089295418

Epoch: 5| Step: 10
Training loss: 2.3660449981689453
Validation loss: 2.389410113775602

Epoch: 317| Step: 0
Training loss: 3.312678575515747
Validation loss: 2.395974192568051

Epoch: 5| Step: 1
Training loss: 2.091047763824463
Validation loss: 2.397761380800637

Epoch: 5| Step: 2
Training loss: 2.8762989044189453
Validation loss: 2.4021892188697733

Epoch: 5| Step: 3
Training loss: 2.504016637802124
Validation loss: 2.393138416351811

Epoch: 5| Step: 4
Training loss: 2.0710036754608154
Validation loss: 2.3861567128089165

Epoch: 5| Step: 5
Training loss: 2.7461941242218018
Validation loss: 2.386442058829851

Epoch: 5| Step: 6
Training loss: 2.7197165489196777
Validation loss: 2.381577258468956

Epoch: 5| Step: 7
Training loss: 2.675112247467041
Validation loss: 2.3793090569075717

Epoch: 5| Step: 8
Training loss: 2.6091086864471436
Validation loss: 2.39515172794301

Epoch: 5| Step: 9
Training loss: 1.6148672103881836
Validation loss: 2.390675449884066

Epoch: 5| Step: 10
Training loss: 2.6505954265594482
Validation loss: 2.390817360211444

Epoch: 318| Step: 0
Training loss: 2.540945291519165
Validation loss: 2.3994573008629585

Epoch: 5| Step: 1
Training loss: 2.7799830436706543
Validation loss: 2.391675918332992

Epoch: 5| Step: 2
Training loss: 2.9341297149658203
Validation loss: 2.387667755926809

Epoch: 5| Step: 3
Training loss: 2.107395648956299
Validation loss: 2.387973936655188

Epoch: 5| Step: 4
Training loss: 2.426454782485962
Validation loss: 2.378377560646303

Epoch: 5| Step: 5
Training loss: 2.725888729095459
Validation loss: 2.3851542113929667

Epoch: 5| Step: 6
Training loss: 2.8798024654388428
Validation loss: 2.3866305710167013

Epoch: 5| Step: 7
Training loss: 2.0118844509124756
Validation loss: 2.392787392421435

Epoch: 5| Step: 8
Training loss: 1.8147224187850952
Validation loss: 2.4136417270988546

Epoch: 5| Step: 9
Training loss: 2.617166042327881
Validation loss: 2.40284820782241

Epoch: 5| Step: 10
Training loss: 3.037745475769043
Validation loss: 2.408894008205783

Epoch: 319| Step: 0
Training loss: 2.497739315032959
Validation loss: 2.410729738973802

Epoch: 5| Step: 1
Training loss: 2.651496410369873
Validation loss: 2.414649191723075

Epoch: 5| Step: 2
Training loss: 2.502595901489258
Validation loss: 2.4056893510203206

Epoch: 5| Step: 3
Training loss: 2.8744468688964844
Validation loss: 2.3947358349318146

Epoch: 5| Step: 4
Training loss: 2.4399521350860596
Validation loss: 2.4025771182070494

Epoch: 5| Step: 5
Training loss: 3.0140347480773926
Validation loss: 2.398175908673194

Epoch: 5| Step: 6
Training loss: 2.2904839515686035
Validation loss: 2.393155659398725

Epoch: 5| Step: 7
Training loss: 1.9377933740615845
Validation loss: 2.3997279059502388

Epoch: 5| Step: 8
Training loss: 2.501638889312744
Validation loss: 2.3879285627795803

Epoch: 5| Step: 9
Training loss: 2.6794850826263428
Validation loss: 2.3876836120441394

Epoch: 5| Step: 10
Training loss: 2.4156270027160645
Validation loss: 2.3793050191735707

Epoch: 320| Step: 0
Training loss: 2.26176118850708
Validation loss: 2.3890499761027675

Epoch: 5| Step: 1
Training loss: 2.8028950691223145
Validation loss: 2.3850994853563208

Epoch: 5| Step: 2
Training loss: 2.4072139263153076
Validation loss: 2.382908049450126

Epoch: 5| Step: 3
Training loss: 2.905222177505493
Validation loss: 2.3886418240044707

Epoch: 5| Step: 4
Training loss: 2.427499294281006
Validation loss: 2.4055759778586765

Epoch: 5| Step: 5
Training loss: 2.2537152767181396
Validation loss: 2.383638971595354

Epoch: 5| Step: 6
Training loss: 3.018428325653076
Validation loss: 2.395683427010813

Epoch: 5| Step: 7
Training loss: 2.3752827644348145
Validation loss: 2.3922843189649683

Epoch: 5| Step: 8
Training loss: 1.6886240243911743
Validation loss: 2.381269578010805

Epoch: 5| Step: 9
Training loss: 2.4732422828674316
Validation loss: 2.3854654373661166

Epoch: 5| Step: 10
Training loss: 3.236534357070923
Validation loss: 2.3902224289473666

Epoch: 321| Step: 0
Training loss: 3.030158042907715
Validation loss: 2.3924977497387956

Epoch: 5| Step: 1
Training loss: 1.8525218963623047
Validation loss: 2.399585252167076

Epoch: 5| Step: 2
Training loss: 2.1016764640808105
Validation loss: 2.391777462856744

Epoch: 5| Step: 3
Training loss: 2.711484432220459
Validation loss: 2.408796336061211

Epoch: 5| Step: 4
Training loss: 3.1954777240753174
Validation loss: 2.4034407215733684

Epoch: 5| Step: 5
Training loss: 2.5577471256256104
Validation loss: 2.3987508230311896

Epoch: 5| Step: 6
Training loss: 2.9462006092071533
Validation loss: 2.396024437360866

Epoch: 5| Step: 7
Training loss: 2.0202765464782715
Validation loss: 2.3801960534946893

Epoch: 5| Step: 8
Training loss: 2.380666494369507
Validation loss: 2.3883557294004705

Epoch: 5| Step: 9
Training loss: 2.391057252883911
Validation loss: 2.3746086987116004

Epoch: 5| Step: 10
Training loss: 2.4954657554626465
Validation loss: 2.3888450412340063

Epoch: 322| Step: 0
Training loss: 2.534249782562256
Validation loss: 2.3826285254570747

Epoch: 5| Step: 1
Training loss: 2.408874034881592
Validation loss: 2.389070887719431

Epoch: 5| Step: 2
Training loss: 2.9299750328063965
Validation loss: 2.3891919992303334

Epoch: 5| Step: 3
Training loss: 2.87235689163208
Validation loss: 2.398502557508407

Epoch: 5| Step: 4
Training loss: 2.4545254707336426
Validation loss: 2.3733181133065173

Epoch: 5| Step: 5
Training loss: 2.6366543769836426
Validation loss: 2.378058120768557

Epoch: 5| Step: 6
Training loss: 2.5896527767181396
Validation loss: 2.3904372184507308

Epoch: 5| Step: 7
Training loss: 1.9701255559921265
Validation loss: 2.3954961479351087

Epoch: 5| Step: 8
Training loss: 2.4807186126708984
Validation loss: 2.4181800260338733

Epoch: 5| Step: 9
Training loss: 2.412583351135254
Validation loss: 2.4475256576332995

Epoch: 5| Step: 10
Training loss: 2.4951748847961426
Validation loss: 2.4382477806460474

Epoch: 323| Step: 0
Training loss: 2.259789228439331
Validation loss: 2.4291762741663123

Epoch: 5| Step: 1
Training loss: 2.790194034576416
Validation loss: 2.4276203340099705

Epoch: 5| Step: 2
Training loss: 3.245964527130127
Validation loss: 2.4253543115431264

Epoch: 5| Step: 3
Training loss: 2.7931346893310547
Validation loss: 2.3960343458319224

Epoch: 5| Step: 4
Training loss: 2.3672966957092285
Validation loss: 2.3887850648613385

Epoch: 5| Step: 5
Training loss: 2.535928249359131
Validation loss: 2.3697997703347156

Epoch: 5| Step: 6
Training loss: 2.2111449241638184
Validation loss: 2.3577180524026193

Epoch: 5| Step: 7
Training loss: 2.064366102218628
Validation loss: 2.367675873541063

Epoch: 5| Step: 8
Training loss: 3.1103415489196777
Validation loss: 2.367224636898246

Epoch: 5| Step: 9
Training loss: 2.3056037425994873
Validation loss: 2.3706347173260105

Epoch: 5| Step: 10
Training loss: 2.0452497005462646
Validation loss: 2.358554547832858

Epoch: 324| Step: 0
Training loss: 2.1983134746551514
Validation loss: 2.3599184264418898

Epoch: 5| Step: 1
Training loss: 2.9523987770080566
Validation loss: 2.3673653397508847

Epoch: 5| Step: 2
Training loss: 2.1843018531799316
Validation loss: 2.3599788783698954

Epoch: 5| Step: 3
Training loss: 2.640519380569458
Validation loss: 2.360207309005081

Epoch: 5| Step: 4
Training loss: 1.772796630859375
Validation loss: 2.3580440808367986

Epoch: 5| Step: 5
Training loss: 2.7686712741851807
Validation loss: 2.3688139248919744

Epoch: 5| Step: 6
Training loss: 2.4765262603759766
Validation loss: 2.3903182783434467

Epoch: 5| Step: 7
Training loss: 2.6758193969726562
Validation loss: 2.3953230073375087

Epoch: 5| Step: 8
Training loss: 2.8142781257629395
Validation loss: 2.4353859347681843

Epoch: 5| Step: 9
Training loss: 3.104954957962036
Validation loss: 2.4167963766282603

Epoch: 5| Step: 10
Training loss: 2.2575480937957764
Validation loss: 2.415357356430382

Epoch: 325| Step: 0
Training loss: 2.56014084815979
Validation loss: 2.4008098571531233

Epoch: 5| Step: 1
Training loss: 2.7763190269470215
Validation loss: 2.4007031994481243

Epoch: 5| Step: 2
Training loss: 1.9319980144500732
Validation loss: 2.399961466430336

Epoch: 5| Step: 3
Training loss: 2.139312744140625
Validation loss: 2.3814615998216855

Epoch: 5| Step: 4
Training loss: 2.293397903442383
Validation loss: 2.3822250404665546

Epoch: 5| Step: 5
Training loss: 2.8788702487945557
Validation loss: 2.3728706708518406

Epoch: 5| Step: 6
Training loss: 3.151200532913208
Validation loss: 2.398066428399855

Epoch: 5| Step: 7
Training loss: 1.9039182662963867
Validation loss: 2.397686876276488

Epoch: 5| Step: 8
Training loss: 2.8159382343292236
Validation loss: 2.4064199270740634

Epoch: 5| Step: 9
Training loss: 2.537370204925537
Validation loss: 2.3887771714118218

Epoch: 5| Step: 10
Training loss: 2.7666773796081543
Validation loss: 2.394814178507815

Epoch: 326| Step: 0
Training loss: 2.092790365219116
Validation loss: 2.3984812023819133

Epoch: 5| Step: 1
Training loss: 2.9032161235809326
Validation loss: 2.3955350973272838

Epoch: 5| Step: 2
Training loss: 2.5731143951416016
Validation loss: 2.389268890503914

Epoch: 5| Step: 3
Training loss: 2.4302351474761963
Validation loss: 2.405531919130715

Epoch: 5| Step: 4
Training loss: 2.527113199234009
Validation loss: 2.3941197113324235

Epoch: 5| Step: 5
Training loss: 2.5526702404022217
Validation loss: 2.395316057307746

Epoch: 5| Step: 6
Training loss: 3.0201833248138428
Validation loss: 2.3976604220687703

Epoch: 5| Step: 7
Training loss: 2.428149461746216
Validation loss: 2.3816508682825233

Epoch: 5| Step: 8
Training loss: 2.6053128242492676
Validation loss: 2.3874235691562777

Epoch: 5| Step: 9
Training loss: 2.4907267093658447
Validation loss: 2.3705987545751754

Epoch: 5| Step: 10
Training loss: 1.9489420652389526
Validation loss: 2.374889257133648

Epoch: 327| Step: 0
Training loss: 2.075444221496582
Validation loss: 2.364868720372518

Epoch: 5| Step: 1
Training loss: 2.3942456245422363
Validation loss: 2.365764482046968

Epoch: 5| Step: 2
Training loss: 2.6472434997558594
Validation loss: 2.382851936483896

Epoch: 5| Step: 3
Training loss: 2.1427018642425537
Validation loss: 2.374917155952864

Epoch: 5| Step: 4
Training loss: 2.805682420730591
Validation loss: 2.3809406065171763

Epoch: 5| Step: 5
Training loss: 2.3996620178222656
Validation loss: 2.3894918580209055

Epoch: 5| Step: 6
Training loss: 2.4507789611816406
Validation loss: 2.3894943550068843

Epoch: 5| Step: 7
Training loss: 2.235063076019287
Validation loss: 2.392315879944832

Epoch: 5| Step: 8
Training loss: 2.529160976409912
Validation loss: 2.380037538466915

Epoch: 5| Step: 9
Training loss: 3.0560314655303955
Validation loss: 2.38919533324498

Epoch: 5| Step: 10
Training loss: 2.956244707107544
Validation loss: 2.3932257365154963

Epoch: 328| Step: 0
Training loss: 2.3278422355651855
Validation loss: 2.402193582186135

Epoch: 5| Step: 1
Training loss: 2.6042513847351074
Validation loss: 2.4133819944115094

Epoch: 5| Step: 2
Training loss: 2.884681224822998
Validation loss: 2.4203771237404115

Epoch: 5| Step: 3
Training loss: 2.3827548027038574
Validation loss: 2.4058622698630057

Epoch: 5| Step: 4
Training loss: 2.2069318294525146
Validation loss: 2.376169045766195

Epoch: 5| Step: 5
Training loss: 3.0287680625915527
Validation loss: 2.377999057051956

Epoch: 5| Step: 6
Training loss: 2.2768869400024414
Validation loss: 2.387192351843721

Epoch: 5| Step: 7
Training loss: 2.872187852859497
Validation loss: 2.3831719275443786

Epoch: 5| Step: 8
Training loss: 1.8310315608978271
Validation loss: 2.3822537904144614

Epoch: 5| Step: 9
Training loss: 2.8359010219573975
Validation loss: 2.4001843134562173

Epoch: 5| Step: 10
Training loss: 2.4885759353637695
Validation loss: 2.3875590088546916

Epoch: 329| Step: 0
Training loss: 2.4276742935180664
Validation loss: 2.3892579437584005

Epoch: 5| Step: 1
Training loss: 2.6386899948120117
Validation loss: 2.3863414026075795

Epoch: 5| Step: 2
Training loss: 2.4453911781311035
Validation loss: 2.3868354405126264

Epoch: 5| Step: 3
Training loss: 2.961416721343994
Validation loss: 2.381545553925217

Epoch: 5| Step: 4
Training loss: 2.5156004428863525
Validation loss: 2.3723132892321517

Epoch: 5| Step: 5
Training loss: 2.3559226989746094
Validation loss: 2.369734310334729

Epoch: 5| Step: 6
Training loss: 2.8297901153564453
Validation loss: 2.375936851706556

Epoch: 5| Step: 7
Training loss: 2.523763656616211
Validation loss: 2.384659159568048

Epoch: 5| Step: 8
Training loss: 1.9818058013916016
Validation loss: 2.362132413412935

Epoch: 5| Step: 9
Training loss: 2.67722749710083
Validation loss: 2.3666662913496777

Epoch: 5| Step: 10
Training loss: 2.003922462463379
Validation loss: 2.3887823653477493

Epoch: 330| Step: 0
Training loss: 2.5936241149902344
Validation loss: 2.4131427913583736

Epoch: 5| Step: 1
Training loss: 2.425497531890869
Validation loss: 2.415244858752015

Epoch: 5| Step: 2
Training loss: 2.5269553661346436
Validation loss: 2.405304995916223

Epoch: 5| Step: 3
Training loss: 2.77406644821167
Validation loss: 2.4061600956865536

Epoch: 5| Step: 4
Training loss: 2.1749658584594727
Validation loss: 2.383932323865993

Epoch: 5| Step: 5
Training loss: 1.950933814048767
Validation loss: 2.3924905561631724

Epoch: 5| Step: 6
Training loss: 3.011812686920166
Validation loss: 2.3902630729060017

Epoch: 5| Step: 7
Training loss: 2.1802468299865723
Validation loss: 2.388891271365586

Epoch: 5| Step: 8
Training loss: 3.1078898906707764
Validation loss: 2.396094060713245

Epoch: 5| Step: 9
Training loss: 2.473771333694458
Validation loss: 2.3701605591722714

Epoch: 5| Step: 10
Training loss: 2.280759811401367
Validation loss: 2.3870593373493483

Epoch: 331| Step: 0
Training loss: 2.3737564086914062
Validation loss: 2.389840756693194

Epoch: 5| Step: 1
Training loss: 1.997309684753418
Validation loss: 2.3911261276532243

Epoch: 5| Step: 2
Training loss: 2.1784727573394775
Validation loss: 2.3829564304761988

Epoch: 5| Step: 3
Training loss: 3.499068021774292
Validation loss: 2.3767518689555507

Epoch: 5| Step: 4
Training loss: 2.533658742904663
Validation loss: 2.374576819840298

Epoch: 5| Step: 5
Training loss: 2.8985135555267334
Validation loss: 2.3820767582103772

Epoch: 5| Step: 6
Training loss: 2.7893810272216797
Validation loss: 2.3813569238108974

Epoch: 5| Step: 7
Training loss: 2.269484043121338
Validation loss: 2.380339353315292

Epoch: 5| Step: 8
Training loss: 2.190727710723877
Validation loss: 2.3786552016453077

Epoch: 5| Step: 9
Training loss: 2.6210622787475586
Validation loss: 2.369369581181516

Epoch: 5| Step: 10
Training loss: 2.162937879562378
Validation loss: 2.3833856749278244

Epoch: 332| Step: 0
Training loss: 2.681737184524536
Validation loss: 2.37785848750863

Epoch: 5| Step: 1
Training loss: 2.9816694259643555
Validation loss: 2.396211947164228

Epoch: 5| Step: 2
Training loss: 2.257235527038574
Validation loss: 2.4042765684025262

Epoch: 5| Step: 3
Training loss: 1.8454090356826782
Validation loss: 2.392006238301595

Epoch: 5| Step: 4
Training loss: 2.390235424041748
Validation loss: 2.384432997754825

Epoch: 5| Step: 5
Training loss: 2.5777029991149902
Validation loss: 2.392549694225352

Epoch: 5| Step: 6
Training loss: 2.879157066345215
Validation loss: 2.3891857772745113

Epoch: 5| Step: 7
Training loss: 1.939462423324585
Validation loss: 2.371022334662817

Epoch: 5| Step: 8
Training loss: 2.486367702484131
Validation loss: 2.373464307477397

Epoch: 5| Step: 9
Training loss: 2.9682819843292236
Validation loss: 2.383028661051104

Epoch: 5| Step: 10
Training loss: 2.4683308601379395
Validation loss: 2.3705552060117006

Epoch: 333| Step: 0
Training loss: 2.2033727169036865
Validation loss: 2.38342019050352

Epoch: 5| Step: 1
Training loss: 2.4166746139526367
Validation loss: 2.3805630309607393

Epoch: 5| Step: 2
Training loss: 2.383822441101074
Validation loss: 2.375940510021743

Epoch: 5| Step: 3
Training loss: 2.4136130809783936
Validation loss: 2.372968958270165

Epoch: 5| Step: 4
Training loss: 2.929941415786743
Validation loss: 2.3533525261827695

Epoch: 5| Step: 5
Training loss: 2.158905029296875
Validation loss: 2.3550063845931843

Epoch: 5| Step: 6
Training loss: 3.4933998584747314
Validation loss: 2.3579215875235935

Epoch: 5| Step: 7
Training loss: 2.2321016788482666
Validation loss: 2.34878308029585

Epoch: 5| Step: 8
Training loss: 2.313419818878174
Validation loss: 2.3609637598837576

Epoch: 5| Step: 9
Training loss: 3.097717046737671
Validation loss: 2.357187166008898

Epoch: 5| Step: 10
Training loss: 1.798307180404663
Validation loss: 2.3450863335722234

Epoch: 334| Step: 0
Training loss: 1.9537245035171509
Validation loss: 2.3618026625725532

Epoch: 5| Step: 1
Training loss: 2.6037497520446777
Validation loss: 2.353238562101959

Epoch: 5| Step: 2
Training loss: 2.113068103790283
Validation loss: 2.3575398691238894

Epoch: 5| Step: 3
Training loss: 2.6900432109832764
Validation loss: 2.364661926864296

Epoch: 5| Step: 4
Training loss: 2.323657989501953
Validation loss: 2.372297179314398

Epoch: 5| Step: 5
Training loss: 2.8740646839141846
Validation loss: 2.3768543658717984

Epoch: 5| Step: 6
Training loss: 2.4257800579071045
Validation loss: 2.3751659342037734

Epoch: 5| Step: 7
Training loss: 2.9789271354675293
Validation loss: 2.3901929906619492

Epoch: 5| Step: 8
Training loss: 2.854290723800659
Validation loss: 2.378827119386324

Epoch: 5| Step: 9
Training loss: 2.3470427989959717
Validation loss: 2.3803331544322353

Epoch: 5| Step: 10
Training loss: 2.184004783630371
Validation loss: 2.3789247159034974

Epoch: 335| Step: 0
Training loss: 2.0114846229553223
Validation loss: 2.3794568918084584

Epoch: 5| Step: 1
Training loss: 2.502892255783081
Validation loss: 2.3716682362300094

Epoch: 5| Step: 2
Training loss: 2.207122802734375
Validation loss: 2.38407995623927

Epoch: 5| Step: 3
Training loss: 2.5203371047973633
Validation loss: 2.375024236658568

Epoch: 5| Step: 4
Training loss: 1.9729902744293213
Validation loss: 2.380616536704443

Epoch: 5| Step: 5
Training loss: 3.264758586883545
Validation loss: 2.3735324234090824

Epoch: 5| Step: 6
Training loss: 1.9986709356307983
Validation loss: 2.3525246061304563

Epoch: 5| Step: 7
Training loss: 3.138582468032837
Validation loss: 2.357362324191678

Epoch: 5| Step: 8
Training loss: 2.178661584854126
Validation loss: 2.3636975698573615

Epoch: 5| Step: 9
Training loss: 3.1215004920959473
Validation loss: 2.3626565471772225

Epoch: 5| Step: 10
Training loss: 2.5485405921936035
Validation loss: 2.376197697013937

Epoch: 336| Step: 0
Training loss: 2.0061540603637695
Validation loss: 2.394470291752969

Epoch: 5| Step: 1
Training loss: 2.1466078758239746
Validation loss: 2.395333561846005

Epoch: 5| Step: 2
Training loss: 2.6655802726745605
Validation loss: 2.4029271128357097

Epoch: 5| Step: 3
Training loss: 2.8701796531677246
Validation loss: 2.4093269122544156

Epoch: 5| Step: 4
Training loss: 2.7899861335754395
Validation loss: 2.4214962246597453

Epoch: 5| Step: 5
Training loss: 2.59777569770813
Validation loss: 2.3996550267742527

Epoch: 5| Step: 6
Training loss: 2.712745189666748
Validation loss: 2.378474368843981

Epoch: 5| Step: 7
Training loss: 2.2976431846618652
Validation loss: 2.367611826107066

Epoch: 5| Step: 8
Training loss: 2.1206421852111816
Validation loss: 2.3671539368168

Epoch: 5| Step: 9
Training loss: 2.0567002296447754
Validation loss: 2.372774662510041

Epoch: 5| Step: 10
Training loss: 3.35900616645813
Validation loss: 2.364185469124907

Epoch: 337| Step: 0
Training loss: 2.7773871421813965
Validation loss: 2.3774977678893716

Epoch: 5| Step: 1
Training loss: 2.4721827507019043
Validation loss: 2.3785865870855187

Epoch: 5| Step: 2
Training loss: 1.942596435546875
Validation loss: 2.3770862189672326

Epoch: 5| Step: 3
Training loss: 2.669856071472168
Validation loss: 2.3717470374158633

Epoch: 5| Step: 4
Training loss: 2.9868662357330322
Validation loss: 2.374840618461691

Epoch: 5| Step: 5
Training loss: 1.9071242809295654
Validation loss: 2.3467476624314503

Epoch: 5| Step: 6
Training loss: 2.054198741912842
Validation loss: 2.3597041278757076

Epoch: 5| Step: 7
Training loss: 3.0878853797912598
Validation loss: 2.3584463032343055

Epoch: 5| Step: 8
Training loss: 2.313596248626709
Validation loss: 2.3849440005517777

Epoch: 5| Step: 9
Training loss: 2.7148241996765137
Validation loss: 2.3597127404264224

Epoch: 5| Step: 10
Training loss: 2.554427146911621
Validation loss: 2.3776733131818872

Epoch: 338| Step: 0
Training loss: 2.671063184738159
Validation loss: 2.3941677026851202

Epoch: 5| Step: 1
Training loss: 2.8663711547851562
Validation loss: 2.390717021880611

Epoch: 5| Step: 2
Training loss: 3.2993972301483154
Validation loss: 2.39343927496223

Epoch: 5| Step: 3
Training loss: 2.9658689498901367
Validation loss: 2.405220636757471

Epoch: 5| Step: 4
Training loss: 1.9925708770751953
Validation loss: 2.3980425942328667

Epoch: 5| Step: 5
Training loss: 1.7643401622772217
Validation loss: 2.3902527363069597

Epoch: 5| Step: 6
Training loss: 2.708203077316284
Validation loss: 2.370698464814053

Epoch: 5| Step: 7
Training loss: 2.53539776802063
Validation loss: 2.352563658068257

Epoch: 5| Step: 8
Training loss: 2.5120482444763184
Validation loss: 2.3686748345692954

Epoch: 5| Step: 9
Training loss: 2.419076442718506
Validation loss: 2.359626467509936

Epoch: 5| Step: 10
Training loss: 1.5496255159378052
Validation loss: 2.3577597525811966

Epoch: 339| Step: 0
Training loss: 2.1189098358154297
Validation loss: 2.356943740639635

Epoch: 5| Step: 1
Training loss: 2.7625885009765625
Validation loss: 2.3783417952957975

Epoch: 5| Step: 2
Training loss: 2.625889301300049
Validation loss: 2.366577958547941

Epoch: 5| Step: 3
Training loss: 2.7507214546203613
Validation loss: 2.361661664901241

Epoch: 5| Step: 4
Training loss: 2.2581627368927
Validation loss: 2.3590119141404347

Epoch: 5| Step: 5
Training loss: 2.944298505783081
Validation loss: 2.3591512608271774

Epoch: 5| Step: 6
Training loss: 2.308627128601074
Validation loss: 2.3495794483410415

Epoch: 5| Step: 7
Training loss: 2.453486919403076
Validation loss: 2.356757131955957

Epoch: 5| Step: 8
Training loss: 2.42488694190979
Validation loss: 2.3668021053396244

Epoch: 5| Step: 9
Training loss: 2.258355140686035
Validation loss: 2.3710990208451466

Epoch: 5| Step: 10
Training loss: 2.3452341556549072
Validation loss: 2.404393334542551

Epoch: 340| Step: 0
Training loss: 2.005946397781372
Validation loss: 2.4030228430225002

Epoch: 5| Step: 1
Training loss: 2.3121020793914795
Validation loss: 2.4349170730959986

Epoch: 5| Step: 2
Training loss: 2.1869184970855713
Validation loss: 2.4157845486876783

Epoch: 5| Step: 3
Training loss: 2.7830965518951416
Validation loss: 2.4112217785209737

Epoch: 5| Step: 4
Training loss: 2.7292208671569824
Validation loss: 2.4124478550367456

Epoch: 5| Step: 5
Training loss: 2.709174633026123
Validation loss: 2.4078778246397614

Epoch: 5| Step: 6
Training loss: 2.806257724761963
Validation loss: 2.405824545891054

Epoch: 5| Step: 7
Training loss: 2.646620273590088
Validation loss: 2.3953044875975578

Epoch: 5| Step: 8
Training loss: 2.92533802986145
Validation loss: 2.3919073458640807

Epoch: 5| Step: 9
Training loss: 1.8237581253051758
Validation loss: 2.3866751014545398

Epoch: 5| Step: 10
Training loss: 2.424227476119995
Validation loss: 2.3852472587298323

Epoch: 341| Step: 0
Training loss: 2.769444465637207
Validation loss: 2.372937412672145

Epoch: 5| Step: 1
Training loss: 2.51720929145813
Validation loss: 2.3665842112674507

Epoch: 5| Step: 2
Training loss: 2.651048183441162
Validation loss: 2.351275218430386

Epoch: 5| Step: 3
Training loss: 2.129678964614868
Validation loss: 2.343722617754372

Epoch: 5| Step: 4
Training loss: 2.307075023651123
Validation loss: 2.3556420110887095

Epoch: 5| Step: 5
Training loss: 1.7464536428451538
Validation loss: 2.347516288039505

Epoch: 5| Step: 6
Training loss: 2.548759937286377
Validation loss: 2.3515428266217633

Epoch: 5| Step: 7
Training loss: 2.774205446243286
Validation loss: 2.344926195759927

Epoch: 5| Step: 8
Training loss: 3.215782880783081
Validation loss: 2.343884555242395

Epoch: 5| Step: 9
Training loss: 2.41017746925354
Validation loss: 2.3383672775760775

Epoch: 5| Step: 10
Training loss: 2.2565765380859375
Validation loss: 2.3537407664842505

Epoch: 342| Step: 0
Training loss: 2.4912545680999756
Validation loss: 2.3545560657337146

Epoch: 5| Step: 1
Training loss: 1.6310741901397705
Validation loss: 2.3814458590681835

Epoch: 5| Step: 2
Training loss: 2.9813668727874756
Validation loss: 2.376149223696801

Epoch: 5| Step: 3
Training loss: 2.644160509109497
Validation loss: 2.362085465461977

Epoch: 5| Step: 4
Training loss: 3.019195318222046
Validation loss: 2.3684132970789427

Epoch: 5| Step: 5
Training loss: 2.0536160469055176
Validation loss: 2.373747470558331

Epoch: 5| Step: 6
Training loss: 2.7665557861328125
Validation loss: 2.3806533223839215

Epoch: 5| Step: 7
Training loss: 2.518585205078125
Validation loss: 2.380394704880253

Epoch: 5| Step: 8
Training loss: 2.417574405670166
Validation loss: 2.3670964446119083

Epoch: 5| Step: 9
Training loss: 2.448829174041748
Validation loss: 2.3572161607844855

Epoch: 5| Step: 10
Training loss: 2.2573273181915283
Validation loss: 2.34437116243506

Epoch: 343| Step: 0
Training loss: 2.0601093769073486
Validation loss: 2.3443633740948093

Epoch: 5| Step: 1
Training loss: 2.2597765922546387
Validation loss: 2.358600667727891

Epoch: 5| Step: 2
Training loss: 2.7208352088928223
Validation loss: 2.3606281229244765

Epoch: 5| Step: 3
Training loss: 2.475879192352295
Validation loss: 2.374051924674742

Epoch: 5| Step: 4
Training loss: 2.631727933883667
Validation loss: 2.366392391984181

Epoch: 5| Step: 5
Training loss: 2.4652905464172363
Validation loss: 2.386276791172643

Epoch: 5| Step: 6
Training loss: 2.9089760780334473
Validation loss: 2.367297822429288

Epoch: 5| Step: 7
Training loss: 2.233959913253784
Validation loss: 2.362347524653199

Epoch: 5| Step: 8
Training loss: 2.8734681606292725
Validation loss: 2.363501592348981

Epoch: 5| Step: 9
Training loss: 2.326643228530884
Validation loss: 2.3622397402281403

Epoch: 5| Step: 10
Training loss: 2.3906829357147217
Validation loss: 2.354162795569307

Epoch: 344| Step: 0
Training loss: 2.1647417545318604
Validation loss: 2.3614821664748655

Epoch: 5| Step: 1
Training loss: 2.238755702972412
Validation loss: 2.3640244007110596

Epoch: 5| Step: 2
Training loss: 2.230489730834961
Validation loss: 2.369823435301422

Epoch: 5| Step: 3
Training loss: 2.616058349609375
Validation loss: 2.374171862038233

Epoch: 5| Step: 4
Training loss: 2.40031361579895
Validation loss: 2.380030162872807

Epoch: 5| Step: 5
Training loss: 2.7768211364746094
Validation loss: 2.3659861549254386

Epoch: 5| Step: 6
Training loss: 2.429009199142456
Validation loss: 2.347419754151375

Epoch: 5| Step: 7
Training loss: 2.4111242294311523
Validation loss: 2.3459152329352593

Epoch: 5| Step: 8
Training loss: 2.4875905513763428
Validation loss: 2.3670765802424443

Epoch: 5| Step: 9
Training loss: 2.742243528366089
Validation loss: 2.400936481773212

Epoch: 5| Step: 10
Training loss: 2.986104965209961
Validation loss: 2.4220210634252077

Epoch: 345| Step: 0
Training loss: 2.5100762844085693
Validation loss: 2.4435878825444046

Epoch: 5| Step: 1
Training loss: 2.139136791229248
Validation loss: 2.4298009872436523

Epoch: 5| Step: 2
Training loss: 2.7960569858551025
Validation loss: 2.4184637659339496

Epoch: 5| Step: 3
Training loss: 2.808196544647217
Validation loss: 2.4138551501817602

Epoch: 5| Step: 4
Training loss: 2.185800552368164
Validation loss: 2.4199202137608684

Epoch: 5| Step: 5
Training loss: 2.5743680000305176
Validation loss: 2.4057392843307985

Epoch: 5| Step: 6
Training loss: 2.946009397506714
Validation loss: 2.360571274193384

Epoch: 5| Step: 7
Training loss: 2.201885223388672
Validation loss: 2.3637239958650325

Epoch: 5| Step: 8
Training loss: 2.5235824584960938
Validation loss: 2.3584742520445134

Epoch: 5| Step: 9
Training loss: 2.1606454849243164
Validation loss: 2.3620784359593547

Epoch: 5| Step: 10
Training loss: 2.5948779582977295
Validation loss: 2.3567551823072534

Epoch: 346| Step: 0
Training loss: 2.4735922813415527
Validation loss: 2.360451699585043

Epoch: 5| Step: 1
Training loss: 2.27887225151062
Validation loss: 2.3762229514378372

Epoch: 5| Step: 2
Training loss: 1.8971290588378906
Validation loss: 2.3785298383364113

Epoch: 5| Step: 3
Training loss: 3.0155346393585205
Validation loss: 2.374753649516772

Epoch: 5| Step: 4
Training loss: 2.512965202331543
Validation loss: 2.378224560009536

Epoch: 5| Step: 5
Training loss: 2.692052125930786
Validation loss: 2.3776089555473736

Epoch: 5| Step: 6
Training loss: 2.7833666801452637
Validation loss: 2.3769423782184558

Epoch: 5| Step: 7
Training loss: 2.206662178039551
Validation loss: 2.3694224101240917

Epoch: 5| Step: 8
Training loss: 2.5206000804901123
Validation loss: 2.3640626579202633

Epoch: 5| Step: 9
Training loss: 2.924913167953491
Validation loss: 2.363802094613352

Epoch: 5| Step: 10
Training loss: 1.7791308164596558
Validation loss: 2.359912392913654

Epoch: 347| Step: 0
Training loss: 3.056567668914795
Validation loss: 2.3608445993033786

Epoch: 5| Step: 1
Training loss: 2.17976975440979
Validation loss: 2.3834363542577273

Epoch: 5| Step: 2
Training loss: 3.0636985301971436
Validation loss: 2.3798664872364332

Epoch: 5| Step: 3
Training loss: 2.8512096405029297
Validation loss: 2.3678685285711802

Epoch: 5| Step: 4
Training loss: 2.3977174758911133
Validation loss: 2.3650453821305306

Epoch: 5| Step: 5
Training loss: 2.0866971015930176
Validation loss: 2.3646131010465723

Epoch: 5| Step: 6
Training loss: 2.492109775543213
Validation loss: 2.35160340544998

Epoch: 5| Step: 7
Training loss: 2.227551221847534
Validation loss: 2.332670775792932

Epoch: 5| Step: 8
Training loss: 2.7529919147491455
Validation loss: 2.346470832824707

Epoch: 5| Step: 9
Training loss: 1.9142920970916748
Validation loss: 2.33342162511682

Epoch: 5| Step: 10
Training loss: 2.181417942047119
Validation loss: 2.3349797571859052

Epoch: 348| Step: 0
Training loss: 2.069884777069092
Validation loss: 2.338161248032765

Epoch: 5| Step: 1
Training loss: 2.609595537185669
Validation loss: 2.3451744433372252

Epoch: 5| Step: 2
Training loss: 2.441499710083008
Validation loss: 2.3312046425316924

Epoch: 5| Step: 3
Training loss: 2.5262768268585205
Validation loss: 2.3622609620453208

Epoch: 5| Step: 4
Training loss: 2.6856393814086914
Validation loss: 2.406577112854168

Epoch: 5| Step: 5
Training loss: 2.7381784915924072
Validation loss: 2.427961459723852

Epoch: 5| Step: 6
Training loss: 2.6415553092956543
Validation loss: 2.432100838230502

Epoch: 5| Step: 7
Training loss: 2.7273268699645996
Validation loss: 2.4393324211079586

Epoch: 5| Step: 8
Training loss: 1.8365108966827393
Validation loss: 2.4149648092126332

Epoch: 5| Step: 9
Training loss: 2.784282684326172
Validation loss: 2.4155504447157665

Epoch: 5| Step: 10
Training loss: 2.5632317066192627
Validation loss: 2.384837927356843

Epoch: 349| Step: 0
Training loss: 2.8889458179473877
Validation loss: 2.391873880099225

Epoch: 5| Step: 1
Training loss: 2.7944588661193848
Validation loss: 2.367008411756126

Epoch: 5| Step: 2
Training loss: 2.886119842529297
Validation loss: 2.3620739906064925

Epoch: 5| Step: 3
Training loss: 2.235032320022583
Validation loss: 2.363081908995105

Epoch: 5| Step: 4
Training loss: 2.3858513832092285
Validation loss: 2.367875729837725

Epoch: 5| Step: 5
Training loss: 2.0678627490997314
Validation loss: 2.368066405737272

Epoch: 5| Step: 6
Training loss: 2.5796377658843994
Validation loss: 2.366900218430386

Epoch: 5| Step: 7
Training loss: 2.164217710494995
Validation loss: 2.3598534394336004

Epoch: 5| Step: 8
Training loss: 2.6156890392303467
Validation loss: 2.3492931871004004

Epoch: 5| Step: 9
Training loss: 2.471592903137207
Validation loss: 2.359658679654521

Epoch: 5| Step: 10
Training loss: 2.343177080154419
Validation loss: 2.3814139212331464

Epoch: 350| Step: 0
Training loss: 3.5295352935791016
Validation loss: 2.393751262336649

Epoch: 5| Step: 1
Training loss: 2.6541361808776855
Validation loss: 2.397667705371816

Epoch: 5| Step: 2
Training loss: 2.71740460395813
Validation loss: 2.3930602124942246

Epoch: 5| Step: 3
Training loss: 2.57432222366333
Validation loss: 2.383562090576336

Epoch: 5| Step: 4
Training loss: 1.9579432010650635
Validation loss: 2.379150862334877

Epoch: 5| Step: 5
Training loss: 2.6979498863220215
Validation loss: 2.362879999222294

Epoch: 5| Step: 6
Training loss: 2.24979305267334
Validation loss: 2.348954826272944

Epoch: 5| Step: 7
Training loss: 2.3693058490753174
Validation loss: 2.3625881184813795

Epoch: 5| Step: 8
Training loss: 2.269728899002075
Validation loss: 2.3741912072704685

Epoch: 5| Step: 9
Training loss: 1.5395605564117432
Validation loss: 2.383981086874521

Epoch: 5| Step: 10
Training loss: 2.8334333896636963
Validation loss: 2.3686093514965427

Epoch: 351| Step: 0
Training loss: 2.0350899696350098
Validation loss: 2.37042014445028

Epoch: 5| Step: 1
Training loss: 2.062544345855713
Validation loss: 2.368759270637266

Epoch: 5| Step: 2
Training loss: 2.501629114151001
Validation loss: 2.3698055718534734

Epoch: 5| Step: 3
Training loss: 2.4599220752716064
Validation loss: 2.3517079737878617

Epoch: 5| Step: 4
Training loss: 2.7916531562805176
Validation loss: 2.3764103997138237

Epoch: 5| Step: 5
Training loss: 2.959862232208252
Validation loss: 2.3557591643384708

Epoch: 5| Step: 6
Training loss: 2.4085216522216797
Validation loss: 2.34474358507382

Epoch: 5| Step: 7
Training loss: 2.6237027645111084
Validation loss: 2.3595606921821513

Epoch: 5| Step: 8
Training loss: 1.692251205444336
Validation loss: 2.3466273764128327

Epoch: 5| Step: 9
Training loss: 3.134437084197998
Validation loss: 2.3442231814066568

Epoch: 5| Step: 10
Training loss: 2.373347520828247
Validation loss: 2.346609769328948

Epoch: 352| Step: 0
Training loss: 3.0111236572265625
Validation loss: 2.335898785180943

Epoch: 5| Step: 1
Training loss: 3.1636040210723877
Validation loss: 2.3412633942019556

Epoch: 5| Step: 2
Training loss: 2.447084903717041
Validation loss: 2.343099058315318

Epoch: 5| Step: 3
Training loss: 2.1956787109375
Validation loss: 2.3328581266505743

Epoch: 5| Step: 4
Training loss: 3.3952457904815674
Validation loss: 2.3593007236398678

Epoch: 5| Step: 5
Training loss: 1.9755446910858154
Validation loss: 2.3459103363816456

Epoch: 5| Step: 6
Training loss: 2.732511281967163
Validation loss: 2.3410435415083364

Epoch: 5| Step: 7
Training loss: 1.9813079833984375
Validation loss: 2.364536295654953

Epoch: 5| Step: 8
Training loss: 1.4454638957977295
Validation loss: 2.348745212760023

Epoch: 5| Step: 9
Training loss: 1.297698736190796
Validation loss: 2.3640517675748436

Epoch: 5| Step: 10
Training loss: 3.5722289085388184
Validation loss: 2.352929740823725

Epoch: 353| Step: 0
Training loss: 2.232983350753784
Validation loss: 2.3518915958301996

Epoch: 5| Step: 1
Training loss: 2.713848829269409
Validation loss: 2.350855258203322

Epoch: 5| Step: 2
Training loss: 2.131852149963379
Validation loss: 2.3509319059310423

Epoch: 5| Step: 3
Training loss: 1.7394495010375977
Validation loss: 2.3491812957230436

Epoch: 5| Step: 4
Training loss: 2.598865032196045
Validation loss: 2.3790716637847242

Epoch: 5| Step: 5
Training loss: 2.575711727142334
Validation loss: 2.38340772351911

Epoch: 5| Step: 6
Training loss: 3.480184555053711
Validation loss: 2.4113459382005917

Epoch: 5| Step: 7
Training loss: 2.410170793533325
Validation loss: 2.403544356746058

Epoch: 5| Step: 8
Training loss: 2.023308515548706
Validation loss: 2.3770624565821823

Epoch: 5| Step: 9
Training loss: 2.8706159591674805
Validation loss: 2.3459235750218874

Epoch: 5| Step: 10
Training loss: 2.4153897762298584
Validation loss: 2.343483542883268

Epoch: 354| Step: 0
Training loss: 3.1328117847442627
Validation loss: 2.33719531695048

Epoch: 5| Step: 1
Training loss: 2.7419931888580322
Validation loss: 2.341728551413423

Epoch: 5| Step: 2
Training loss: 1.856830358505249
Validation loss: 2.361943346197887

Epoch: 5| Step: 3
Training loss: 1.7134994268417358
Validation loss: 2.3729090254793883

Epoch: 5| Step: 4
Training loss: 2.9820029735565186
Validation loss: 2.3644721841299408

Epoch: 5| Step: 5
Training loss: 2.3914172649383545
Validation loss: 2.3659925076269333

Epoch: 5| Step: 6
Training loss: 2.6755475997924805
Validation loss: 2.3703910894291376

Epoch: 5| Step: 7
Training loss: 2.9880294799804688
Validation loss: 2.3607665672097156

Epoch: 5| Step: 8
Training loss: 2.867481231689453
Validation loss: 2.3511281526216896

Epoch: 5| Step: 9
Training loss: 1.9127992391586304
Validation loss: 2.350156840457711

Epoch: 5| Step: 10
Training loss: 1.5704030990600586
Validation loss: 2.368165144356348

Epoch: 355| Step: 0
Training loss: 2.5713047981262207
Validation loss: 2.3797184600624988

Epoch: 5| Step: 1
Training loss: 2.350926399230957
Validation loss: 2.3723514105684016

Epoch: 5| Step: 2
Training loss: 2.6021549701690674
Validation loss: 2.3784891713050103

Epoch: 5| Step: 3
Training loss: 2.2726104259490967
Validation loss: 2.3872305770074167

Epoch: 5| Step: 4
Training loss: 2.964953660964966
Validation loss: 2.3909120764783633

Epoch: 5| Step: 5
Training loss: 3.161473035812378
Validation loss: 2.390939825324602

Epoch: 5| Step: 6
Training loss: 1.8591066598892212
Validation loss: 2.3869941132042998

Epoch: 5| Step: 7
Training loss: 2.294182300567627
Validation loss: 2.3849950759641585

Epoch: 5| Step: 8
Training loss: 1.9863317012786865
Validation loss: 2.3883989651997886

Epoch: 5| Step: 9
Training loss: 3.1178364753723145
Validation loss: 2.3871080003758913

Epoch: 5| Step: 10
Training loss: 1.9432289600372314
Validation loss: 2.3815938759875555

Epoch: 356| Step: 0
Training loss: 2.0624613761901855
Validation loss: 2.373783685827768

Epoch: 5| Step: 1
Training loss: 3.055922746658325
Validation loss: 2.3841599495180192

Epoch: 5| Step: 2
Training loss: 1.8800519704818726
Validation loss: 2.3641766296919955

Epoch: 5| Step: 3
Training loss: 2.212144136428833
Validation loss: 2.358547108147734

Epoch: 5| Step: 4
Training loss: 2.862633228302002
Validation loss: 2.356477206753146

Epoch: 5| Step: 5
Training loss: 2.02736234664917
Validation loss: 2.3597028998918432

Epoch: 5| Step: 6
Training loss: 1.9810584783554077
Validation loss: 2.3566507370241228

Epoch: 5| Step: 7
Training loss: 2.6472761631011963
Validation loss: 2.364920062403525

Epoch: 5| Step: 8
Training loss: 2.54956316947937
Validation loss: 2.372098010073426

Epoch: 5| Step: 9
Training loss: 2.906919002532959
Validation loss: 2.365031837135233

Epoch: 5| Step: 10
Training loss: 2.773560047149658
Validation loss: 2.3712796934189333

Epoch: 357| Step: 0
Training loss: 2.5630197525024414
Validation loss: 2.34865802462383

Epoch: 5| Step: 1
Training loss: 2.7940022945404053
Validation loss: 2.349737059685492

Epoch: 5| Step: 2
Training loss: 2.423556089401245
Validation loss: 2.3421462018002748

Epoch: 5| Step: 3
Training loss: 1.9526031017303467
Validation loss: 2.3304572951409126

Epoch: 5| Step: 4
Training loss: 2.3920090198516846
Validation loss: 2.3273659931716097

Epoch: 5| Step: 5
Training loss: 2.6709132194519043
Validation loss: 2.342200074144589

Epoch: 5| Step: 6
Training loss: 2.5117549896240234
Validation loss: 2.337762250695177

Epoch: 5| Step: 7
Training loss: 1.8139002323150635
Validation loss: 2.349089966025404

Epoch: 5| Step: 8
Training loss: 2.504382610321045
Validation loss: 2.36796744151782

Epoch: 5| Step: 9
Training loss: 2.400451183319092
Validation loss: 2.380935420272171

Epoch: 5| Step: 10
Training loss: 2.8597824573516846
Validation loss: 2.4081818544736473

Epoch: 358| Step: 0
Training loss: 2.3545711040496826
Validation loss: 2.3811871159461235

Epoch: 5| Step: 1
Training loss: 2.956054210662842
Validation loss: 2.3709449716793594

Epoch: 5| Step: 2
Training loss: 2.26727294921875
Validation loss: 2.363350873352379

Epoch: 5| Step: 3
Training loss: 1.8946574926376343
Validation loss: 2.347162264649586

Epoch: 5| Step: 4
Training loss: 2.6961123943328857
Validation loss: 2.335574251349254

Epoch: 5| Step: 5
Training loss: 2.966390609741211
Validation loss: 2.326038637468892

Epoch: 5| Step: 6
Training loss: 2.117919445037842
Validation loss: 2.340398698724726

Epoch: 5| Step: 7
Training loss: 2.547452926635742
Validation loss: 2.3342236536805347

Epoch: 5| Step: 8
Training loss: 2.9515132904052734
Validation loss: 2.3338824292664886

Epoch: 5| Step: 9
Training loss: 2.058835983276367
Validation loss: 2.3361985350167878

Epoch: 5| Step: 10
Training loss: 2.1469171047210693
Validation loss: 2.3315090107661423

Epoch: 359| Step: 0
Training loss: 2.3357317447662354
Validation loss: 2.346603937046502

Epoch: 5| Step: 1
Training loss: 3.0325675010681152
Validation loss: 2.3291962877396615

Epoch: 5| Step: 2
Training loss: 2.1916415691375732
Validation loss: 2.324587993724372

Epoch: 5| Step: 3
Training loss: 2.2878811359405518
Validation loss: 2.327036057749102

Epoch: 5| Step: 4
Training loss: 1.8898566961288452
Validation loss: 2.3454974466754543

Epoch: 5| Step: 5
Training loss: 2.65964674949646
Validation loss: 2.3417608737945557

Epoch: 5| Step: 6
Training loss: 3.2073867321014404
Validation loss: 2.3583569654854397

Epoch: 5| Step: 7
Training loss: 2.28635835647583
Validation loss: 2.348158013436102

Epoch: 5| Step: 8
Training loss: 2.5035080909729004
Validation loss: 2.3499408101522796

Epoch: 5| Step: 9
Training loss: 2.0474541187286377
Validation loss: 2.3483892179304555

Epoch: 5| Step: 10
Training loss: 2.419099807739258
Validation loss: 2.355610534708987

Epoch: 360| Step: 0
Training loss: 3.1477668285369873
Validation loss: 2.344237945413077

Epoch: 5| Step: 1
Training loss: 2.044985294342041
Validation loss: 2.3566548388491393

Epoch: 5| Step: 2
Training loss: 2.9750359058380127
Validation loss: 2.362631021007415

Epoch: 5| Step: 3
Training loss: 2.233837604522705
Validation loss: 2.3672388753583355

Epoch: 5| Step: 4
Training loss: 3.0158278942108154
Validation loss: 2.365947405497233

Epoch: 5| Step: 5
Training loss: 2.6684467792510986
Validation loss: 2.33582474852121

Epoch: 5| Step: 6
Training loss: 2.0999791622161865
Validation loss: 2.340418287502822

Epoch: 5| Step: 7
Training loss: 1.7093136310577393
Validation loss: 2.3190846750813146

Epoch: 5| Step: 8
Training loss: 2.5663630962371826
Validation loss: 2.3273121336454987

Epoch: 5| Step: 9
Training loss: 2.3680379390716553
Validation loss: 2.3399668508960354

Epoch: 5| Step: 10
Training loss: 1.9088633060455322
Validation loss: 2.333442712342867

Epoch: 361| Step: 0
Training loss: 1.8711025714874268
Validation loss: 2.3384418795185704

Epoch: 5| Step: 1
Training loss: 2.457183361053467
Validation loss: 2.339845016438474

Epoch: 5| Step: 2
Training loss: 2.0967776775360107
Validation loss: 2.3331785919845744

Epoch: 5| Step: 3
Training loss: 2.0675411224365234
Validation loss: 2.333112665401992

Epoch: 5| Step: 4
Training loss: 2.7483432292938232
Validation loss: 2.333264561109645

Epoch: 5| Step: 5
Training loss: 3.1260952949523926
Validation loss: 2.3297974268595376

Epoch: 5| Step: 6
Training loss: 2.5146758556365967
Validation loss: 2.3284254535551994

Epoch: 5| Step: 7
Training loss: 3.1733615398406982
Validation loss: 2.3340448602553336

Epoch: 5| Step: 8
Training loss: 1.4665899276733398
Validation loss: 2.3308144205360004

Epoch: 5| Step: 9
Training loss: 2.7010726928710938
Validation loss: 2.3353705649734824

Epoch: 5| Step: 10
Training loss: 2.524240732192993
Validation loss: 2.351323894275132

Epoch: 362| Step: 0
Training loss: 3.305811643600464
Validation loss: 2.3863428972100698

Epoch: 5| Step: 1
Training loss: 2.773407220840454
Validation loss: 2.411693437125093

Epoch: 5| Step: 2
Training loss: 2.1878902912139893
Validation loss: 2.3993073509585474

Epoch: 5| Step: 3
Training loss: 2.6007068157196045
Validation loss: 2.382381111062983

Epoch: 5| Step: 4
Training loss: 2.8412585258483887
Validation loss: 2.387298135347264

Epoch: 5| Step: 5
Training loss: 1.9742510318756104
Validation loss: 2.3507280734277542

Epoch: 5| Step: 6
Training loss: 2.3859877586364746
Validation loss: 2.364618848728877

Epoch: 5| Step: 7
Training loss: 2.6575467586517334
Validation loss: 2.3455058579803794

Epoch: 5| Step: 8
Training loss: 1.868329405784607
Validation loss: 2.3474561193937897

Epoch: 5| Step: 9
Training loss: 2.1686007976531982
Validation loss: 2.349739090088875

Epoch: 5| Step: 10
Training loss: 2.4352939128875732
Validation loss: 2.3454293153619252

Epoch: 363| Step: 0
Training loss: 1.9997676610946655
Validation loss: 2.3482621985097087

Epoch: 5| Step: 1
Training loss: 2.65588116645813
Validation loss: 2.3509088921290573

Epoch: 5| Step: 2
Training loss: 2.1415977478027344
Validation loss: 2.3628278137535177

Epoch: 5| Step: 3
Training loss: 2.436786651611328
Validation loss: 2.3927126866514965

Epoch: 5| Step: 4
Training loss: 3.0552818775177
Validation loss: 2.4171778207184165

Epoch: 5| Step: 5
Training loss: 2.265237331390381
Validation loss: 2.4502770798180693

Epoch: 5| Step: 6
Training loss: 2.992122173309326
Validation loss: 2.5053258147290958

Epoch: 5| Step: 7
Training loss: 2.386763334274292
Validation loss: 2.4904852785089964

Epoch: 5| Step: 8
Training loss: 2.653775691986084
Validation loss: 2.4720886330450735

Epoch: 5| Step: 9
Training loss: 2.499098539352417
Validation loss: 2.4138249094768236

Epoch: 5| Step: 10
Training loss: 2.267730712890625
Validation loss: 2.3763051571384555

Epoch: 364| Step: 0
Training loss: 2.290557384490967
Validation loss: 2.3497126640812045

Epoch: 5| Step: 1
Training loss: 2.1206374168395996
Validation loss: 2.322130472429337

Epoch: 5| Step: 2
Training loss: 2.320528745651245
Validation loss: 2.329106992290866

Epoch: 5| Step: 3
Training loss: 3.4119067192077637
Validation loss: 2.3320654053841867

Epoch: 5| Step: 4
Training loss: 1.9829847812652588
Validation loss: 2.3412847647102932

Epoch: 5| Step: 5
Training loss: 2.3754985332489014
Validation loss: 2.344539675661313

Epoch: 5| Step: 6
Training loss: 3.101012706756592
Validation loss: 2.3482119191077446

Epoch: 5| Step: 7
Training loss: 2.6496078968048096
Validation loss: 2.322659356619722

Epoch: 5| Step: 8
Training loss: 1.6293052434921265
Validation loss: 2.306261903496199

Epoch: 5| Step: 9
Training loss: 2.3411142826080322
Validation loss: 2.310740565740934

Epoch: 5| Step: 10
Training loss: 2.9623968601226807
Validation loss: 2.3326942997594036

Epoch: 365| Step: 0
Training loss: 2.1239898204803467
Validation loss: 2.3429600141381703

Epoch: 5| Step: 1
Training loss: 2.1696841716766357
Validation loss: 2.3624390222693004

Epoch: 5| Step: 2
Training loss: 2.913057804107666
Validation loss: 2.3676005576246526

Epoch: 5| Step: 3
Training loss: 3.153012275695801
Validation loss: 2.3625002753350044

Epoch: 5| Step: 4
Training loss: 1.9923175573349
Validation loss: 2.359351183778496

Epoch: 5| Step: 5
Training loss: 1.792525291442871
Validation loss: 2.3493910861271683

Epoch: 5| Step: 6
Training loss: 2.632277011871338
Validation loss: 2.3516557755008822

Epoch: 5| Step: 7
Training loss: 2.711308717727661
Validation loss: 2.3294901642748105

Epoch: 5| Step: 8
Training loss: 2.8632216453552246
Validation loss: 2.3094634509855703

Epoch: 5| Step: 9
Training loss: 2.2035043239593506
Validation loss: 2.307146385151853

Epoch: 5| Step: 10
Training loss: 2.533628463745117
Validation loss: 2.3281417585188344

Epoch: 366| Step: 0
Training loss: 1.8963863849639893
Validation loss: 2.3222070765751663

Epoch: 5| Step: 1
Training loss: 2.728865385055542
Validation loss: 2.3068662228122836

Epoch: 5| Step: 2
Training loss: 2.7750492095947266
Validation loss: 2.325625358089324

Epoch: 5| Step: 3
Training loss: 2.142364501953125
Validation loss: 2.3174308602527907

Epoch: 5| Step: 4
Training loss: 1.4082698822021484
Validation loss: 2.304375030661142

Epoch: 5| Step: 5
Training loss: 2.501485586166382
Validation loss: 2.3311400849332093

Epoch: 5| Step: 6
Training loss: 2.1936683654785156
Validation loss: 2.3270554978360414

Epoch: 5| Step: 7
Training loss: 2.5662124156951904
Validation loss: 2.3444329179743284

Epoch: 5| Step: 8
Training loss: 2.692107677459717
Validation loss: 2.35441166867492

Epoch: 5| Step: 9
Training loss: 2.5338339805603027
Validation loss: 2.3370139368118776

Epoch: 5| Step: 10
Training loss: 3.812264919281006
Validation loss: 2.3413029563042427

Epoch: 367| Step: 0
Training loss: 2.0834450721740723
Validation loss: 2.367386062939962

Epoch: 5| Step: 1
Training loss: 2.9377331733703613
Validation loss: 2.356875470889512

Epoch: 5| Step: 2
Training loss: 2.2581028938293457
Validation loss: 2.3638103444089174

Epoch: 5| Step: 3
Training loss: 2.761136531829834
Validation loss: 2.3682731082362514

Epoch: 5| Step: 4
Training loss: 2.196554660797119
Validation loss: 2.3667665502076507

Epoch: 5| Step: 5
Training loss: 2.229828357696533
Validation loss: 2.3499437557753695

Epoch: 5| Step: 6
Training loss: 2.4539573192596436
Validation loss: 2.3473818917428293

Epoch: 5| Step: 7
Training loss: 2.250335693359375
Validation loss: 2.351174472480692

Epoch: 5| Step: 8
Training loss: 2.415996551513672
Validation loss: 2.338637451971731

Epoch: 5| Step: 9
Training loss: 2.8541109561920166
Validation loss: 2.338822146897675

Epoch: 5| Step: 10
Training loss: 2.42095685005188
Validation loss: 2.3445023080354095

Epoch: 368| Step: 0
Training loss: 2.5211567878723145
Validation loss: 2.332993115148237

Epoch: 5| Step: 1
Training loss: 2.767533302307129
Validation loss: 2.3227159284776255

Epoch: 5| Step: 2
Training loss: 2.457876205444336
Validation loss: 2.334454200601065

Epoch: 5| Step: 3
Training loss: 2.073195695877075
Validation loss: 2.335308531279205

Epoch: 5| Step: 4
Training loss: 2.9466781616210938
Validation loss: 2.3371003238103722

Epoch: 5| Step: 5
Training loss: 2.4999852180480957
Validation loss: 2.338887365915442

Epoch: 5| Step: 6
Training loss: 2.357696533203125
Validation loss: 2.3375281031413744

Epoch: 5| Step: 7
Training loss: 2.317716360092163
Validation loss: 2.3276699768599642

Epoch: 5| Step: 8
Training loss: 2.285602331161499
Validation loss: 2.32223968608405

Epoch: 5| Step: 9
Training loss: 1.9373668432235718
Validation loss: 2.3223851675628335

Epoch: 5| Step: 10
Training loss: 2.6417150497436523
Validation loss: 2.3130784444911505

Epoch: 369| Step: 0
Training loss: 1.8750946521759033
Validation loss: 2.32133158176176

Epoch: 5| Step: 1
Training loss: 2.482168674468994
Validation loss: 2.3160380317318823

Epoch: 5| Step: 2
Training loss: 3.0711121559143066
Validation loss: 2.336756101218603

Epoch: 5| Step: 3
Training loss: 2.890899896621704
Validation loss: 2.347634350099871

Epoch: 5| Step: 4
Training loss: 1.4731502532958984
Validation loss: 2.346322023740379

Epoch: 5| Step: 5
Training loss: 2.943486213684082
Validation loss: 2.364959716796875

Epoch: 5| Step: 6
Training loss: 1.7396280765533447
Validation loss: 2.3439585547293387

Epoch: 5| Step: 7
Training loss: 2.563915729522705
Validation loss: 2.351036584505471

Epoch: 5| Step: 8
Training loss: 2.5170397758483887
Validation loss: 2.3540663514085995

Epoch: 5| Step: 9
Training loss: 2.076775074005127
Validation loss: 2.3714845782967022

Epoch: 5| Step: 10
Training loss: 3.0384581089019775
Validation loss: 2.3557407881623957

Epoch: 370| Step: 0
Training loss: 2.7452828884124756
Validation loss: 2.357355297252696

Epoch: 5| Step: 1
Training loss: 1.625993013381958
Validation loss: 2.3566530878825853

Epoch: 5| Step: 2
Training loss: 2.5510447025299072
Validation loss: 2.3794263588484896

Epoch: 5| Step: 3
Training loss: 2.465888261795044
Validation loss: 2.3800680098995084

Epoch: 5| Step: 4
Training loss: 2.8258023262023926
Validation loss: 2.3886692652138333

Epoch: 5| Step: 5
Training loss: 2.5969860553741455
Validation loss: 2.3759838176029984

Epoch: 5| Step: 6
Training loss: 2.8120107650756836
Validation loss: 2.3562293975583968

Epoch: 5| Step: 7
Training loss: 2.6517910957336426
Validation loss: 2.363959326538988

Epoch: 5| Step: 8
Training loss: 2.322880268096924
Validation loss: 2.348510039749966

Epoch: 5| Step: 9
Training loss: 1.5667393207550049
Validation loss: 2.3553153135443248

Epoch: 5| Step: 10
Training loss: 2.59399151802063
Validation loss: 2.352412187924949

Epoch: 371| Step: 0
Training loss: 1.9629604816436768
Validation loss: 2.342109000811013

Epoch: 5| Step: 1
Training loss: 2.465292453765869
Validation loss: 2.3365462236506964

Epoch: 5| Step: 2
Training loss: 2.0760982036590576
Validation loss: 2.3338864695641304

Epoch: 5| Step: 3
Training loss: 2.6503067016601562
Validation loss: 2.3227638326665407

Epoch: 5| Step: 4
Training loss: 3.235940456390381
Validation loss: 2.3247487545013428

Epoch: 5| Step: 5
Training loss: 2.777879476547241
Validation loss: 2.3359041983081448

Epoch: 5| Step: 6
Training loss: 2.7685866355895996
Validation loss: 2.3195127364127868

Epoch: 5| Step: 7
Training loss: 1.8421573638916016
Validation loss: 2.3301900150955364

Epoch: 5| Step: 8
Training loss: 2.5402989387512207
Validation loss: 2.331858669557879

Epoch: 5| Step: 9
Training loss: 2.2615456581115723
Validation loss: 2.333719550922353

Epoch: 5| Step: 10
Training loss: 2.119708776473999
Validation loss: 2.345556092518632

Epoch: 372| Step: 0
Training loss: 3.0481743812561035
Validation loss: 2.341475981538014

Epoch: 5| Step: 1
Training loss: 2.178361415863037
Validation loss: 2.335373688769597

Epoch: 5| Step: 2
Training loss: 2.731909990310669
Validation loss: 2.36072342113782

Epoch: 5| Step: 3
Training loss: 2.690682888031006
Validation loss: 2.344105569265222

Epoch: 5| Step: 4
Training loss: 3.0083167552948
Validation loss: 2.3544774260572208

Epoch: 5| Step: 5
Training loss: 2.1355926990509033
Validation loss: 2.327584646081412

Epoch: 5| Step: 6
Training loss: 2.067838668823242
Validation loss: 2.3269800704012633

Epoch: 5| Step: 7
Training loss: 2.2759041786193848
Validation loss: 2.320992495424004

Epoch: 5| Step: 8
Training loss: 2.4633097648620605
Validation loss: 2.3260457156806864

Epoch: 5| Step: 9
Training loss: 1.9387775659561157
Validation loss: 2.3247848736342562

Epoch: 5| Step: 10
Training loss: 2.545971393585205
Validation loss: 2.310183307175995

Epoch: 373| Step: 0
Training loss: 2.3931069374084473
Validation loss: 2.3052324300171225

Epoch: 5| Step: 1
Training loss: 2.5068118572235107
Validation loss: 2.3221976013593775

Epoch: 5| Step: 2
Training loss: 1.649383783340454
Validation loss: 2.3306484350594143

Epoch: 5| Step: 3
Training loss: 2.0299267768859863
Validation loss: 2.354620543859338

Epoch: 5| Step: 4
Training loss: 2.325350046157837
Validation loss: 2.3900914499836583

Epoch: 5| Step: 5
Training loss: 2.197298049926758
Validation loss: 2.390043217648742

Epoch: 5| Step: 6
Training loss: 2.6669373512268066
Validation loss: 2.421694617117605

Epoch: 5| Step: 7
Training loss: 2.548715114593506
Validation loss: 2.4059015576557448

Epoch: 5| Step: 8
Training loss: 2.5085129737854004
Validation loss: 2.391792038435577

Epoch: 5| Step: 9
Training loss: 3.004757881164551
Validation loss: 2.360617288979151

Epoch: 5| Step: 10
Training loss: 2.928314447402954
Validation loss: 2.3594229067525556

Epoch: 374| Step: 0
Training loss: 2.1634814739227295
Validation loss: 2.3333131600451726

Epoch: 5| Step: 1
Training loss: 2.3367526531219482
Validation loss: 2.319499105535528

Epoch: 5| Step: 2
Training loss: 2.3895697593688965
Validation loss: 2.3151434544594056

Epoch: 5| Step: 3
Training loss: 2.393068790435791
Validation loss: 2.313008725002248

Epoch: 5| Step: 4
Training loss: 2.6246848106384277
Validation loss: 2.3239146099295667

Epoch: 5| Step: 5
Training loss: 2.4223103523254395
Validation loss: 2.323839300422258

Epoch: 5| Step: 6
Training loss: 1.7021057605743408
Validation loss: 2.3219937714197303

Epoch: 5| Step: 7
Training loss: 2.5447654724121094
Validation loss: 2.3210019091124177

Epoch: 5| Step: 8
Training loss: 3.149920701980591
Validation loss: 2.316074809720439

Epoch: 5| Step: 9
Training loss: 2.710582971572876
Validation loss: 2.3132890219329507

Epoch: 5| Step: 10
Training loss: 2.2951571941375732
Validation loss: 2.3134025681403374

Epoch: 375| Step: 0
Training loss: 3.0805106163024902
Validation loss: 2.33529822544385

Epoch: 5| Step: 1
Training loss: 2.210674524307251
Validation loss: 2.322424542519354

Epoch: 5| Step: 2
Training loss: 2.272043466567993
Validation loss: 2.31338752982437

Epoch: 5| Step: 3
Training loss: 2.099792242050171
Validation loss: 2.3375518885991906

Epoch: 5| Step: 4
Training loss: 1.7406527996063232
Validation loss: 2.351190459343695

Epoch: 5| Step: 5
Training loss: 1.954750418663025
Validation loss: 2.3544162883553454

Epoch: 5| Step: 6
Training loss: 3.0689563751220703
Validation loss: 2.3904688589034544

Epoch: 5| Step: 7
Training loss: 2.7660112380981445
Validation loss: 2.3550569434319772

Epoch: 5| Step: 8
Training loss: 2.2189853191375732
Validation loss: 2.38313950005398

Epoch: 5| Step: 9
Training loss: 2.4188051223754883
Validation loss: 2.397111479954053

Epoch: 5| Step: 10
Training loss: 3.007779121398926
Validation loss: 2.3736713624769643

Epoch: 376| Step: 0
Training loss: 2.65425968170166
Validation loss: 2.350803085552749

Epoch: 5| Step: 1
Training loss: 2.5985774993896484
Validation loss: 2.328416229576193

Epoch: 5| Step: 2
Training loss: 1.9060909748077393
Validation loss: 2.315777970898536

Epoch: 5| Step: 3
Training loss: 2.2192025184631348
Validation loss: 2.3243131047935894

Epoch: 5| Step: 4
Training loss: 2.3903682231903076
Validation loss: 2.3039948068639284

Epoch: 5| Step: 5
Training loss: 2.399162769317627
Validation loss: 2.3207421674523303

Epoch: 5| Step: 6
Training loss: 2.5659406185150146
Validation loss: 2.315304974074005

Epoch: 5| Step: 7
Training loss: 3.168262004852295
Validation loss: 2.3261910869229223

Epoch: 5| Step: 8
Training loss: 1.849737524986267
Validation loss: 2.314987620999736

Epoch: 5| Step: 9
Training loss: 2.4273064136505127
Validation loss: 2.3219615413296606

Epoch: 5| Step: 10
Training loss: 2.4253551959991455
Validation loss: 2.333034869163267

Epoch: 377| Step: 0
Training loss: 2.122591018676758
Validation loss: 2.333345925936135

Epoch: 5| Step: 1
Training loss: 2.5872278213500977
Validation loss: 2.3499511698240876

Epoch: 5| Step: 2
Training loss: 1.8028122186660767
Validation loss: 2.3492110749726653

Epoch: 5| Step: 3
Training loss: 3.179421901702881
Validation loss: 2.3876828211610035

Epoch: 5| Step: 4
Training loss: 2.77388072013855
Validation loss: 2.3883779612920617

Epoch: 5| Step: 5
Training loss: 2.7408688068389893
Validation loss: 2.3894188532265286

Epoch: 5| Step: 6
Training loss: 2.4603257179260254
Validation loss: 2.3779767226147395

Epoch: 5| Step: 7
Training loss: 1.7851994037628174
Validation loss: 2.3797870707768265

Epoch: 5| Step: 8
Training loss: 2.5156638622283936
Validation loss: 2.3975540976370535

Epoch: 5| Step: 9
Training loss: 2.0411620140075684
Validation loss: 2.358345952085269

Epoch: 5| Step: 10
Training loss: 2.6070799827575684
Validation loss: 2.343359498567479

Epoch: 378| Step: 0
Training loss: 3.1504509449005127
Validation loss: 2.3178554478512017

Epoch: 5| Step: 1
Training loss: 3.2663815021514893
Validation loss: 2.310582983878351

Epoch: 5| Step: 2
Training loss: 2.3040852546691895
Validation loss: 2.3062949488239903

Epoch: 5| Step: 3
Training loss: 2.228041410446167
Validation loss: 2.3039697447130756

Epoch: 5| Step: 4
Training loss: 2.496062755584717
Validation loss: 2.3149363917689167

Epoch: 5| Step: 5
Training loss: 2.437065362930298
Validation loss: 2.306791126087148

Epoch: 5| Step: 6
Training loss: 2.6504619121551514
Validation loss: 2.3084007591329594

Epoch: 5| Step: 7
Training loss: 1.871100664138794
Validation loss: 2.301924600396105

Epoch: 5| Step: 8
Training loss: 1.9649473428726196
Validation loss: 2.32205641910594

Epoch: 5| Step: 9
Training loss: 2.221036672592163
Validation loss: 2.304457902908325

Epoch: 5| Step: 10
Training loss: 2.074249744415283
Validation loss: 2.308711895378687

Epoch: 379| Step: 0
Training loss: 2.595224142074585
Validation loss: 2.309977049468666

Epoch: 5| Step: 1
Training loss: 2.0194976329803467
Validation loss: 2.3178670380705144

Epoch: 5| Step: 2
Training loss: 2.8368258476257324
Validation loss: 2.3532630192336215

Epoch: 5| Step: 3
Training loss: 2.0727601051330566
Validation loss: 2.370057657200803

Epoch: 5| Step: 4
Training loss: 2.157860040664673
Validation loss: 2.3653928028639926

Epoch: 5| Step: 5
Training loss: 2.351261615753174
Validation loss: 2.3528562104830177

Epoch: 5| Step: 6
Training loss: 2.629016876220703
Validation loss: 2.3331714035362325

Epoch: 5| Step: 7
Training loss: 2.234738826751709
Validation loss: 2.3301532012160107

Epoch: 5| Step: 8
Training loss: 2.4117519855499268
Validation loss: 2.321004166397997

Epoch: 5| Step: 9
Training loss: 2.153813600540161
Validation loss: 2.3154194867739113

Epoch: 5| Step: 10
Training loss: 3.3659987449645996
Validation loss: 2.310988615917903

Epoch: 380| Step: 0
Training loss: 2.1426079273223877
Validation loss: 2.3150325744382796

Epoch: 5| Step: 1
Training loss: 1.8277528285980225
Validation loss: 2.324829219489969

Epoch: 5| Step: 2
Training loss: 2.3190712928771973
Validation loss: 2.310665856125534

Epoch: 5| Step: 3
Training loss: 2.6969058513641357
Validation loss: 2.3279579249761437

Epoch: 5| Step: 4
Training loss: 2.789161205291748
Validation loss: 2.318053800572631

Epoch: 5| Step: 5
Training loss: 2.040220022201538
Validation loss: 2.3236521290194605

Epoch: 5| Step: 6
Training loss: 2.447641611099243
Validation loss: 2.332607525651173

Epoch: 5| Step: 7
Training loss: 2.978327512741089
Validation loss: 2.3216046158985426

Epoch: 5| Step: 8
Training loss: 2.310295343399048
Validation loss: 2.3134311399152203

Epoch: 5| Step: 9
Training loss: 2.5069096088409424
Validation loss: 2.328031619389852

Epoch: 5| Step: 10
Training loss: 2.3640408515930176
Validation loss: 2.3419840656301028

Epoch: 381| Step: 0
Training loss: 2.4179179668426514
Validation loss: 2.346969650637719

Epoch: 5| Step: 1
Training loss: 2.2202305793762207
Validation loss: 2.346432178251205

Epoch: 5| Step: 2
Training loss: 2.211841344833374
Validation loss: 2.3475869137753724

Epoch: 5| Step: 3
Training loss: 2.6778244972229004
Validation loss: 2.3361148218954764

Epoch: 5| Step: 4
Training loss: 2.2432761192321777
Validation loss: 2.3366153188931045

Epoch: 5| Step: 5
Training loss: 2.6785387992858887
Validation loss: 2.3211346531427033

Epoch: 5| Step: 6
Training loss: 2.2927510738372803
Validation loss: 2.3239976052315003

Epoch: 5| Step: 7
Training loss: 2.380643844604492
Validation loss: 2.314398560472714

Epoch: 5| Step: 8
Training loss: 2.305354356765747
Validation loss: 2.3239232699076333

Epoch: 5| Step: 9
Training loss: 3.076838254928589
Validation loss: 2.3205785392433085

Epoch: 5| Step: 10
Training loss: 1.776696801185608
Validation loss: 2.32582401972945

Epoch: 382| Step: 0
Training loss: 2.0600368976593018
Validation loss: 2.321524502128683

Epoch: 5| Step: 1
Training loss: 2.0905961990356445
Validation loss: 2.32683684748988

Epoch: 5| Step: 2
Training loss: 2.1285908222198486
Validation loss: 2.3219951506583922

Epoch: 5| Step: 3
Training loss: 1.9905601739883423
Validation loss: 2.3199724433242634

Epoch: 5| Step: 4
Training loss: 3.1122889518737793
Validation loss: 2.357465664545695

Epoch: 5| Step: 5
Training loss: 2.552462339401245
Validation loss: 2.337381673115556

Epoch: 5| Step: 6
Training loss: 3.5518596172332764
Validation loss: 2.3506024370911303

Epoch: 5| Step: 7
Training loss: 2.5526344776153564
Validation loss: 2.34613242841536

Epoch: 5| Step: 8
Training loss: 2.0749497413635254
Validation loss: 2.3438348295868083

Epoch: 5| Step: 9
Training loss: 2.3984951972961426
Validation loss: 2.3474514920224427

Epoch: 5| Step: 10
Training loss: 1.8367162942886353
Validation loss: 2.314561834899328

Epoch: 383| Step: 0
Training loss: 2.8404171466827393
Validation loss: 2.326547855971962

Epoch: 5| Step: 1
Training loss: 2.3246045112609863
Validation loss: 2.314041829878284

Epoch: 5| Step: 2
Training loss: 2.7457737922668457
Validation loss: 2.3160269029678835

Epoch: 5| Step: 3
Training loss: 2.2580418586730957
Validation loss: 2.3270982285981536

Epoch: 5| Step: 4
Training loss: 1.4590089321136475
Validation loss: 2.3137257791334584

Epoch: 5| Step: 5
Training loss: 2.2171103954315186
Validation loss: 2.3169105437494095

Epoch: 5| Step: 6
Training loss: 3.060129404067993
Validation loss: 2.3262839830049904

Epoch: 5| Step: 7
Training loss: 2.6309611797332764
Validation loss: 2.320382818098991

Epoch: 5| Step: 8
Training loss: 2.345386505126953
Validation loss: 2.3020524927364883

Epoch: 5| Step: 9
Training loss: 1.917909860610962
Validation loss: 2.318384675569432

Epoch: 5| Step: 10
Training loss: 2.5583279132843018
Validation loss: 2.3106974042871946

Epoch: 384| Step: 0
Training loss: 2.535679817199707
Validation loss: 2.3186121345848165

Epoch: 5| Step: 1
Training loss: 1.666264533996582
Validation loss: 2.321865230478266

Epoch: 5| Step: 2
Training loss: 2.2311148643493652
Validation loss: 2.322111857834683

Epoch: 5| Step: 3
Training loss: 2.1994128227233887
Validation loss: 2.315523778238604

Epoch: 5| Step: 4
Training loss: 2.7043838500976562
Validation loss: 2.335187601786788

Epoch: 5| Step: 5
Training loss: 3.0445218086242676
Validation loss: 2.3444606796387704

Epoch: 5| Step: 6
Training loss: 2.8575398921966553
Validation loss: 2.3541406187959897

Epoch: 5| Step: 7
Training loss: 2.8362269401550293
Validation loss: 2.3506565427267425

Epoch: 5| Step: 8
Training loss: 1.8295478820800781
Validation loss: 2.370419381767191

Epoch: 5| Step: 9
Training loss: 1.9638116359710693
Validation loss: 2.3704136315212456

Epoch: 5| Step: 10
Training loss: 2.782142162322998
Validation loss: 2.3584932358034196

Epoch: 385| Step: 0
Training loss: 2.50809907913208
Validation loss: 2.3615590141665552

Epoch: 5| Step: 1
Training loss: 2.1486904621124268
Validation loss: 2.326985283564496

Epoch: 5| Step: 2
Training loss: 2.394604206085205
Validation loss: 2.319198364852577

Epoch: 5| Step: 3
Training loss: 2.5556414127349854
Validation loss: 2.3021204548497356

Epoch: 5| Step: 4
Training loss: 2.165207862854004
Validation loss: 2.309444140362483

Epoch: 5| Step: 5
Training loss: 2.480654716491699
Validation loss: 2.298594759356591

Epoch: 5| Step: 6
Training loss: 2.0733120441436768
Validation loss: 2.3104633926063456

Epoch: 5| Step: 7
Training loss: 2.708831310272217
Validation loss: 2.2941641089736775

Epoch: 5| Step: 8
Training loss: 1.6259645223617554
Validation loss: 2.2990780133073048

Epoch: 5| Step: 9
Training loss: 3.2894256114959717
Validation loss: 2.304006838029431

Epoch: 5| Step: 10
Training loss: 2.5175204277038574
Validation loss: 2.2919902698968047

Epoch: 386| Step: 0
Training loss: 2.6065220832824707
Validation loss: 2.3017008304595947

Epoch: 5| Step: 1
Training loss: 2.7109451293945312
Validation loss: 2.3217340771869948

Epoch: 5| Step: 2
Training loss: 2.123565435409546
Validation loss: 2.3105242944532827

Epoch: 5| Step: 3
Training loss: 2.0457632541656494
Validation loss: 2.317618006019182

Epoch: 5| Step: 4
Training loss: 2.0303637981414795
Validation loss: 2.3272761247491323

Epoch: 5| Step: 5
Training loss: 2.5274875164031982
Validation loss: 2.323452834160097

Epoch: 5| Step: 6
Training loss: 2.6013612747192383
Validation loss: 2.3081867771763958

Epoch: 5| Step: 7
Training loss: 1.9434791803359985
Validation loss: 2.3468415557697253

Epoch: 5| Step: 8
Training loss: 2.7335550785064697
Validation loss: 2.3260189294815063

Epoch: 5| Step: 9
Training loss: 2.130218982696533
Validation loss: 2.3417495835211968

Epoch: 5| Step: 10
Training loss: 2.867840051651001
Validation loss: 2.3395286349840063

Epoch: 387| Step: 0
Training loss: 2.409776210784912
Validation loss: 2.321181538284466

Epoch: 5| Step: 1
Training loss: 2.3636059761047363
Validation loss: 2.3151028515190206

Epoch: 5| Step: 2
Training loss: 2.3429296016693115
Validation loss: 2.3192899316869755

Epoch: 5| Step: 3
Training loss: 2.685263156890869
Validation loss: 2.331811184524208

Epoch: 5| Step: 4
Training loss: 2.7050557136535645
Validation loss: 2.3593205495547225

Epoch: 5| Step: 5
Training loss: 1.4812414646148682
Validation loss: 2.3484095783643824

Epoch: 5| Step: 6
Training loss: 2.6785643100738525
Validation loss: 2.360272125531268

Epoch: 5| Step: 7
Training loss: 2.41605806350708
Validation loss: 2.346818211258099

Epoch: 5| Step: 8
Training loss: 2.6943249702453613
Validation loss: 2.3488341505809496

Epoch: 5| Step: 9
Training loss: 2.25614595413208
Validation loss: 2.319408788475939

Epoch: 5| Step: 10
Training loss: 2.450960874557495
Validation loss: 2.322016748048926

Epoch: 388| Step: 0
Training loss: 2.4595184326171875
Validation loss: 2.3076299569940053

Epoch: 5| Step: 1
Training loss: 2.575531482696533
Validation loss: 2.3142707194051435

Epoch: 5| Step: 2
Training loss: 2.330047845840454
Validation loss: 2.2863556723440848

Epoch: 5| Step: 3
Training loss: 2.2509937286376953
Validation loss: 2.305324887716642

Epoch: 5| Step: 4
Training loss: 2.976909637451172
Validation loss: 2.283843242993919

Epoch: 5| Step: 5
Training loss: 2.14864444732666
Validation loss: 2.295921451301985

Epoch: 5| Step: 6
Training loss: 2.4830832481384277
Validation loss: 2.308138124404415

Epoch: 5| Step: 7
Training loss: 1.825735330581665
Validation loss: 2.308467149734497

Epoch: 5| Step: 8
Training loss: 2.102381944656372
Validation loss: 2.3268397008219073

Epoch: 5| Step: 9
Training loss: 2.683594226837158
Validation loss: 2.331680174796812

Epoch: 5| Step: 10
Training loss: 2.574424982070923
Validation loss: 2.342335060078611

Epoch: 389| Step: 0
Training loss: 2.6124026775360107
Validation loss: 2.330045336036272

Epoch: 5| Step: 1
Training loss: 2.5032410621643066
Validation loss: 2.3099701814754035

Epoch: 5| Step: 2
Training loss: 2.024526834487915
Validation loss: 2.3203812260781564

Epoch: 5| Step: 3
Training loss: 2.464803457260132
Validation loss: 2.3221528658302883

Epoch: 5| Step: 4
Training loss: 2.390514850616455
Validation loss: 2.3193878012318767

Epoch: 5| Step: 5
Training loss: 2.3032188415527344
Validation loss: 2.3151550344241563

Epoch: 5| Step: 6
Training loss: 2.9888710975646973
Validation loss: 2.3298634482968237

Epoch: 5| Step: 7
Training loss: 2.644500494003296
Validation loss: 2.320519180708034

Epoch: 5| Step: 8
Training loss: 2.0798466205596924
Validation loss: 2.3192653809824297

Epoch: 5| Step: 9
Training loss: 1.9584814310073853
Validation loss: 2.3186795121879986

Epoch: 5| Step: 10
Training loss: 2.451458692550659
Validation loss: 2.316435283230197

Epoch: 390| Step: 0
Training loss: 2.3738203048706055
Validation loss: 2.3232780233506234

Epoch: 5| Step: 1
Training loss: 1.977449655532837
Validation loss: 2.2993635823649745

Epoch: 5| Step: 2
Training loss: 2.4427239894866943
Validation loss: 2.2957396507263184

Epoch: 5| Step: 3
Training loss: 2.7356133460998535
Validation loss: 2.302221900673323

Epoch: 5| Step: 4
Training loss: 2.411087989807129
Validation loss: 2.2781926380690707

Epoch: 5| Step: 5
Training loss: 2.4322502613067627
Validation loss: 2.299072657862017

Epoch: 5| Step: 6
Training loss: 2.678467273712158
Validation loss: 2.286821042337725

Epoch: 5| Step: 7
Training loss: 2.219059944152832
Validation loss: 2.2990778697434293

Epoch: 5| Step: 8
Training loss: 2.1526038646698
Validation loss: 2.307805058776691

Epoch: 5| Step: 9
Training loss: 2.382737874984741
Validation loss: 2.3356540741459018

Epoch: 5| Step: 10
Training loss: 2.4395954608917236
Validation loss: 2.3386059884102113

Epoch: 391| Step: 0
Training loss: 2.7545197010040283
Validation loss: 2.3624926510677544

Epoch: 5| Step: 1
Training loss: 2.4020886421203613
Validation loss: 2.3547262325081775

Epoch: 5| Step: 2
Training loss: 2.582815647125244
Validation loss: 2.3426170759303595

Epoch: 5| Step: 3
Training loss: 1.8155622482299805
Validation loss: 2.323629517709055

Epoch: 5| Step: 4
Training loss: 3.1057231426239014
Validation loss: 2.3354395999703357

Epoch: 5| Step: 5
Training loss: 2.2969534397125244
Validation loss: 2.3636561388610513

Epoch: 5| Step: 6
Training loss: 2.5399181842803955
Validation loss: 2.3625161775978665

Epoch: 5| Step: 7
Training loss: 1.7775146961212158
Validation loss: 2.356122242507114

Epoch: 5| Step: 8
Training loss: 2.210463047027588
Validation loss: 2.3624700089936614

Epoch: 5| Step: 9
Training loss: 1.7868578433990479
Validation loss: 2.3355541254884455

Epoch: 5| Step: 10
Training loss: 3.0176970958709717
Validation loss: 2.3468929900917956

Epoch: 392| Step: 0
Training loss: 2.488770008087158
Validation loss: 2.3273975746605986

Epoch: 5| Step: 1
Training loss: 2.4697928428649902
Validation loss: 2.3142776053438903

Epoch: 5| Step: 2
Training loss: 2.255758047103882
Validation loss: 2.3280176039664977

Epoch: 5| Step: 3
Training loss: 2.3983213901519775
Validation loss: 2.3404050360443773

Epoch: 5| Step: 4
Training loss: 2.539419651031494
Validation loss: 2.3469332623225387

Epoch: 5| Step: 5
Training loss: 2.6493170261383057
Validation loss: 2.3350473962804323

Epoch: 5| Step: 6
Training loss: 2.6964054107666016
Validation loss: 2.330708114049768

Epoch: 5| Step: 7
Training loss: 2.073378086090088
Validation loss: 2.290147678826445

Epoch: 5| Step: 8
Training loss: 2.302974224090576
Validation loss: 2.289481096370246

Epoch: 5| Step: 9
Training loss: 1.9915157556533813
Validation loss: 2.308902725096672

Epoch: 5| Step: 10
Training loss: 2.480102062225342
Validation loss: 2.325056234995524

Epoch: 393| Step: 0
Training loss: 2.326185941696167
Validation loss: 2.3373630149390108

Epoch: 5| Step: 1
Training loss: 2.7133893966674805
Validation loss: 2.3298118396471907

Epoch: 5| Step: 2
Training loss: 2.9411730766296387
Validation loss: 2.2986307015983005

Epoch: 5| Step: 3
Training loss: 2.229606866836548
Validation loss: 2.293693874471931

Epoch: 5| Step: 4
Training loss: 2.3025243282318115
Validation loss: 2.275339464987478

Epoch: 5| Step: 5
Training loss: 2.4163761138916016
Validation loss: 2.269564292764151

Epoch: 5| Step: 6
Training loss: 2.426182270050049
Validation loss: 2.271875532724524

Epoch: 5| Step: 7
Training loss: 1.482466697692871
Validation loss: 2.2733355952847387

Epoch: 5| Step: 8
Training loss: 2.3453574180603027
Validation loss: 2.27042188695682

Epoch: 5| Step: 9
Training loss: 2.8742189407348633
Validation loss: 2.2806928183442805

Epoch: 5| Step: 10
Training loss: 2.4001083374023438
Validation loss: 2.2762130691159155

Epoch: 394| Step: 0
Training loss: 1.7792459726333618
Validation loss: 2.2970060122910367

Epoch: 5| Step: 1
Training loss: 2.99332857131958
Validation loss: 2.3112900898020756

Epoch: 5| Step: 2
Training loss: 2.8550634384155273
Validation loss: 2.3317640519911245

Epoch: 5| Step: 3
Training loss: 2.6850521564483643
Validation loss: 2.3600773478067048

Epoch: 5| Step: 4
Training loss: 2.691972017288208
Validation loss: 2.3304135517407487

Epoch: 5| Step: 5
Training loss: 2.112940549850464
Validation loss: 2.3594209455674693

Epoch: 5| Step: 6
Training loss: 2.5333733558654785
Validation loss: 2.368522051841982

Epoch: 5| Step: 7
Training loss: 2.1595301628112793
Validation loss: 2.3630139032999673

Epoch: 5| Step: 8
Training loss: 2.9414820671081543
Validation loss: 2.3674858129152687

Epoch: 5| Step: 9
Training loss: 1.9819835424423218
Validation loss: 2.347707256194084

Epoch: 5| Step: 10
Training loss: 1.5464674234390259
Validation loss: 2.3107223382560154

Epoch: 395| Step: 0
Training loss: 2.5716488361358643
Validation loss: 2.312286243643812

Epoch: 5| Step: 1
Training loss: 2.0937371253967285
Validation loss: 2.3037386773734965

Epoch: 5| Step: 2
Training loss: 2.399538516998291
Validation loss: 2.2985048345340195

Epoch: 5| Step: 3
Training loss: 3.0250954627990723
Validation loss: 2.308254172725062

Epoch: 5| Step: 4
Training loss: 2.655515193939209
Validation loss: 2.3186796326791086

Epoch: 5| Step: 5
Training loss: 2.4858224391937256
Validation loss: 2.3071261016271447

Epoch: 5| Step: 6
Training loss: 1.2908130884170532
Validation loss: 2.3020860661742506

Epoch: 5| Step: 7
Training loss: 2.8374507427215576
Validation loss: 2.3154672550898727

Epoch: 5| Step: 8
Training loss: 2.452622890472412
Validation loss: 2.304269463785233

Epoch: 5| Step: 9
Training loss: 2.421204090118408
Validation loss: 2.292599308875299

Epoch: 5| Step: 10
Training loss: 2.0462772846221924
Validation loss: 2.325576134907302

Epoch: 396| Step: 0
Training loss: 2.2901248931884766
Validation loss: 2.310303657285629

Epoch: 5| Step: 1
Training loss: 2.4877471923828125
Validation loss: 2.2997840630110873

Epoch: 5| Step: 2
Training loss: 2.1958975791931152
Validation loss: 2.327457240832749

Epoch: 5| Step: 3
Training loss: 2.2201080322265625
Validation loss: 2.3292590674533638

Epoch: 5| Step: 4
Training loss: 2.5104613304138184
Validation loss: 2.3523918608183503

Epoch: 5| Step: 5
Training loss: 2.648167133331299
Validation loss: 2.346694679670436

Epoch: 5| Step: 6
Training loss: 2.8782992362976074
Validation loss: 2.349033619767876

Epoch: 5| Step: 7
Training loss: 2.206557273864746
Validation loss: 2.3456306098609843

Epoch: 5| Step: 8
Training loss: 1.9178882837295532
Validation loss: 2.3365521584787676

Epoch: 5| Step: 9
Training loss: 2.6907548904418945
Validation loss: 2.3412097807853454

Epoch: 5| Step: 10
Training loss: 2.0755832195281982
Validation loss: 2.318799962279617

Epoch: 397| Step: 0
Training loss: 2.569918155670166
Validation loss: 2.3141418515994983

Epoch: 5| Step: 1
Training loss: 2.205698251724243
Validation loss: 2.3016378597546647

Epoch: 5| Step: 2
Training loss: 1.8612552881240845
Validation loss: 2.313581898648252

Epoch: 5| Step: 3
Training loss: 2.552902936935425
Validation loss: 2.3145875571876444

Epoch: 5| Step: 4
Training loss: 2.5113720893859863
Validation loss: 2.3119225809651036

Epoch: 5| Step: 5
Training loss: 2.8984134197235107
Validation loss: 2.2866763709693827

Epoch: 5| Step: 6
Training loss: 2.1159026622772217
Validation loss: 2.2814225227602067

Epoch: 5| Step: 7
Training loss: 2.807339906692505
Validation loss: 2.2931035616064586

Epoch: 5| Step: 8
Training loss: 1.8204530477523804
Validation loss: 2.30137292287683

Epoch: 5| Step: 9
Training loss: 2.5021045207977295
Validation loss: 2.294993292900824

Epoch: 5| Step: 10
Training loss: 2.38822340965271
Validation loss: 2.29062052952346

Epoch: 398| Step: 0
Training loss: 2.3685104846954346
Validation loss: 2.2846142502241236

Epoch: 5| Step: 1
Training loss: 1.6596758365631104
Validation loss: 2.2924928242160427

Epoch: 5| Step: 2
Training loss: 2.518799304962158
Validation loss: 2.3114128830612346

Epoch: 5| Step: 3
Training loss: 2.1785595417022705
Validation loss: 2.307457603434081

Epoch: 5| Step: 4
Training loss: 2.4072279930114746
Validation loss: 2.3312783087453535

Epoch: 5| Step: 5
Training loss: 2.1068873405456543
Validation loss: 2.3491684506016393

Epoch: 5| Step: 6
Training loss: 2.772710084915161
Validation loss: 2.3411149491545973

Epoch: 5| Step: 7
Training loss: 3.003446578979492
Validation loss: 2.363271792729696

Epoch: 5| Step: 8
Training loss: 2.2932209968566895
Validation loss: 2.3323070438959266

Epoch: 5| Step: 9
Training loss: 2.805250644683838
Validation loss: 2.2906527237225602

Epoch: 5| Step: 10
Training loss: 1.9746441841125488
Validation loss: 2.2971474329630532

Epoch: 399| Step: 0
Training loss: 1.8210649490356445
Validation loss: 2.2899984493050525

Epoch: 5| Step: 1
Training loss: 2.4674925804138184
Validation loss: 2.306809002353299

Epoch: 5| Step: 2
Training loss: 2.4284515380859375
Validation loss: 2.2916573555238786

Epoch: 5| Step: 3
Training loss: 2.646331310272217
Validation loss: 2.2992476929900465

Epoch: 5| Step: 4
Training loss: 2.9974982738494873
Validation loss: 2.301323785576769

Epoch: 5| Step: 5
Training loss: 2.5494885444641113
Validation loss: 2.2842268431058494

Epoch: 5| Step: 6
Training loss: 2.327699899673462
Validation loss: 2.2876545947085143

Epoch: 5| Step: 7
Training loss: 2.415940761566162
Validation loss: 2.2818846318029586

Epoch: 5| Step: 8
Training loss: 2.3565638065338135
Validation loss: 2.3200199219488327

Epoch: 5| Step: 9
Training loss: 1.8487592935562134
Validation loss: 2.316092127112932

Epoch: 5| Step: 10
Training loss: 2.195570468902588
Validation loss: 2.305802104293659

Epoch: 400| Step: 0
Training loss: 2.099252223968506
Validation loss: 2.327250357597105

Epoch: 5| Step: 1
Training loss: 2.297171115875244
Validation loss: 2.3066515845637166

Epoch: 5| Step: 2
Training loss: 2.5325839519500732
Validation loss: 2.316576257828743

Epoch: 5| Step: 3
Training loss: 2.3104324340820312
Validation loss: 2.322244636474117

Epoch: 5| Step: 4
Training loss: 3.046008348464966
Validation loss: 2.32475374590966

Epoch: 5| Step: 5
Training loss: 1.4480791091918945
Validation loss: 2.334234873453776

Epoch: 5| Step: 6
Training loss: 2.888375759124756
Validation loss: 2.365888369980679

Epoch: 5| Step: 7
Training loss: 1.9397647380828857
Validation loss: 2.366602861753074

Epoch: 5| Step: 8
Training loss: 1.9525673389434814
Validation loss: 2.3406899385554816

Epoch: 5| Step: 9
Training loss: 2.837977886199951
Validation loss: 2.3648798645183606

Epoch: 5| Step: 10
Training loss: 2.6287660598754883
Validation loss: 2.3281690728279854

Testing loss: 2.4893870751063027

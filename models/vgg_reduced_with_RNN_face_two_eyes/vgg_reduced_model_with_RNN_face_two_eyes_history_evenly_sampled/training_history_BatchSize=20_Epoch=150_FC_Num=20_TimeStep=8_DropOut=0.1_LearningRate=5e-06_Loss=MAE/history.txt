Epoch: 1| Step: 0
Training loss: 4.8271284103393555
Validation loss: 5.149295278774795

Epoch: 5| Step: 1
Training loss: 4.617883205413818
Validation loss: 5.145926890834685

Epoch: 5| Step: 2
Training loss: 5.345759391784668
Validation loss: 5.142507019863333

Epoch: 5| Step: 3
Training loss: 4.3969221115112305
Validation loss: 5.139299920810166

Epoch: 5| Step: 4
Training loss: 4.199723720550537
Validation loss: 5.135710244537682

Epoch: 5| Step: 5
Training loss: 5.304561138153076
Validation loss: 5.132227277243009

Epoch: 5| Step: 6
Training loss: 5.076026916503906
Validation loss: 5.128634242601292

Epoch: 5| Step: 7
Training loss: 5.376636505126953
Validation loss: 5.125341369259742

Epoch: 5| Step: 8
Training loss: 5.580987453460693
Validation loss: 5.12137710150852

Epoch: 5| Step: 9
Training loss: 4.747739791870117
Validation loss: 5.117505073547363

Epoch: 5| Step: 10
Training loss: 4.7121710777282715
Validation loss: 5.113644399950581

Epoch: 2| Step: 0
Training loss: 6.007883548736572
Validation loss: 5.109502643667241

Epoch: 5| Step: 1
Training loss: 4.404470443725586
Validation loss: 5.104935497365972

Epoch: 5| Step: 2
Training loss: 4.548698902130127
Validation loss: 5.1006188495184785

Epoch: 5| Step: 3
Training loss: 3.894503116607666
Validation loss: 5.0958982898343

Epoch: 5| Step: 4
Training loss: 4.944666385650635
Validation loss: 5.091080824534099

Epoch: 5| Step: 5
Training loss: 4.74945592880249
Validation loss: 5.08627410088816

Epoch: 5| Step: 6
Training loss: 5.14386510848999
Validation loss: 5.0807682468045146

Epoch: 5| Step: 7
Training loss: 4.684494972229004
Validation loss: 5.075094556295744

Epoch: 5| Step: 8
Training loss: 5.037493705749512
Validation loss: 5.069368552136165

Epoch: 5| Step: 9
Training loss: 4.992947578430176
Validation loss: 5.063142033033474

Epoch: 5| Step: 10
Training loss: 5.344561576843262
Validation loss: 5.056718662220945

Epoch: 3| Step: 0
Training loss: 3.9959003925323486
Validation loss: 5.050249433004728

Epoch: 5| Step: 1
Training loss: 4.845950126647949
Validation loss: 5.042626470647832

Epoch: 5| Step: 2
Training loss: 5.392268180847168
Validation loss: 5.0350734802984425

Epoch: 5| Step: 3
Training loss: 4.8399505615234375
Validation loss: 5.027459580411193

Epoch: 5| Step: 4
Training loss: 5.568408489227295
Validation loss: 5.018595880077731

Epoch: 5| Step: 5
Training loss: 4.7705979347229
Validation loss: 5.010001510702153

Epoch: 5| Step: 6
Training loss: 5.17868709564209
Validation loss: 5.000445673542638

Epoch: 5| Step: 7
Training loss: 4.5284552574157715
Validation loss: 4.99046980437412

Epoch: 5| Step: 8
Training loss: 4.691944122314453
Validation loss: 4.979672531927785

Epoch: 5| Step: 9
Training loss: 4.5475754737854
Validation loss: 4.968744724027572

Epoch: 5| Step: 10
Training loss: 4.384821891784668
Validation loss: 4.957633777331281

Epoch: 4| Step: 0
Training loss: 4.876701831817627
Validation loss: 4.945552015817293

Epoch: 5| Step: 1
Training loss: 4.908026695251465
Validation loss: 4.932522061050579

Epoch: 5| Step: 2
Training loss: 4.696291446685791
Validation loss: 4.919448032174059

Epoch: 5| Step: 3
Training loss: 5.0629096031188965
Validation loss: 4.905034608738397

Epoch: 5| Step: 4
Training loss: 4.169946193695068
Validation loss: 4.8896624042141825

Epoch: 5| Step: 5
Training loss: 5.472474575042725
Validation loss: 4.874404507298624

Epoch: 5| Step: 6
Training loss: 5.15571928024292
Validation loss: 4.857659955178538

Epoch: 5| Step: 7
Training loss: 4.595703125
Validation loss: 4.840412293711016

Epoch: 5| Step: 8
Training loss: 4.359245777130127
Validation loss: 4.823284820843768

Epoch: 5| Step: 9
Training loss: 4.6288347244262695
Validation loss: 4.80395721620129

Epoch: 5| Step: 10
Training loss: 3.1223270893096924
Validation loss: 4.784741422181488

Epoch: 5| Step: 0
Training loss: 4.655113220214844
Validation loss: 4.764408137208672

Epoch: 5| Step: 1
Training loss: 4.501461982727051
Validation loss: 4.742521111683179

Epoch: 5| Step: 2
Training loss: 4.046741008758545
Validation loss: 4.720480885556949

Epoch: 5| Step: 3
Training loss: 3.6658987998962402
Validation loss: 4.696635138603948

Epoch: 5| Step: 4
Training loss: 4.1407084465026855
Validation loss: 4.674800626693234

Epoch: 5| Step: 5
Training loss: 4.941562175750732
Validation loss: 4.649415662211757

Epoch: 5| Step: 6
Training loss: 5.004255771636963
Validation loss: 4.623628016441099

Epoch: 5| Step: 7
Training loss: 4.896621227264404
Validation loss: 4.597221548839282

Epoch: 5| Step: 8
Training loss: 5.822600364685059
Validation loss: 4.570158463652416

Epoch: 5| Step: 9
Training loss: 3.000609874725342
Validation loss: 4.542696440091697

Epoch: 5| Step: 10
Training loss: 3.991626024246216
Validation loss: 4.514198821078065

Epoch: 6| Step: 0
Training loss: 3.7725205421447754
Validation loss: 4.4839589211248585

Epoch: 5| Step: 1
Training loss: 3.1948351860046387
Validation loss: 4.45325243344871

Epoch: 5| Step: 2
Training loss: 4.462183952331543
Validation loss: 4.422570690031974

Epoch: 5| Step: 3
Training loss: 4.861241817474365
Validation loss: 4.390712081745106

Epoch: 5| Step: 4
Training loss: 3.8117737770080566
Validation loss: 4.360797776970812

Epoch: 5| Step: 5
Training loss: 4.165517330169678
Validation loss: 4.330414930979411

Epoch: 5| Step: 6
Training loss: 4.171167850494385
Validation loss: 4.298662052359632

Epoch: 5| Step: 7
Training loss: 3.190155506134033
Validation loss: 4.2687798059114845

Epoch: 5| Step: 8
Training loss: 4.90116024017334
Validation loss: 4.238802856014621

Epoch: 5| Step: 9
Training loss: 5.343749046325684
Validation loss: 4.209243707759406

Epoch: 5| Step: 10
Training loss: 3.4682321548461914
Validation loss: 4.180391573136853

Epoch: 7| Step: 0
Training loss: 4.102674961090088
Validation loss: 4.153045618405906

Epoch: 5| Step: 1
Training loss: 4.497668743133545
Validation loss: 4.124891691310431

Epoch: 5| Step: 2
Training loss: 4.306059837341309
Validation loss: 4.097856983061759

Epoch: 5| Step: 3
Training loss: 3.0873970985412598
Validation loss: 4.070347255276095

Epoch: 5| Step: 4
Training loss: 4.35223388671875
Validation loss: 4.042964478974701

Epoch: 5| Step: 5
Training loss: 4.34083890914917
Validation loss: 4.01908121057736

Epoch: 5| Step: 6
Training loss: 3.9833405017852783
Validation loss: 3.9953426417484077

Epoch: 5| Step: 7
Training loss: 3.3968586921691895
Validation loss: 3.9707469735094296

Epoch: 5| Step: 8
Training loss: 2.981449604034424
Validation loss: 3.9483067758621706

Epoch: 5| Step: 9
Training loss: 3.5766117572784424
Validation loss: 3.927828250392791

Epoch: 5| Step: 10
Training loss: 3.9686279296875
Validation loss: 3.9092292683098906

Epoch: 8| Step: 0
Training loss: 3.7827048301696777
Validation loss: 3.8910626032019175

Epoch: 5| Step: 1
Training loss: 2.9498140811920166
Validation loss: 3.872862192892259

Epoch: 5| Step: 2
Training loss: 3.2982869148254395
Validation loss: 3.8547214897730018

Epoch: 5| Step: 3
Training loss: 3.137314796447754
Validation loss: 3.83769461160065

Epoch: 5| Step: 4
Training loss: 4.379519462585449
Validation loss: 3.822668388325681

Epoch: 5| Step: 5
Training loss: 3.474313259124756
Validation loss: 3.807008220303443

Epoch: 5| Step: 6
Training loss: 4.161349296569824
Validation loss: 3.789336848002608

Epoch: 5| Step: 7
Training loss: 3.7632431983947754
Validation loss: 3.7741688348913707

Epoch: 5| Step: 8
Training loss: 4.000149726867676
Validation loss: 3.7593242583736295

Epoch: 5| Step: 9
Training loss: 3.326693058013916
Validation loss: 3.744129862836612

Epoch: 5| Step: 10
Training loss: 4.360857009887695
Validation loss: 3.7314070014543432

Epoch: 9| Step: 0
Training loss: 3.558276653289795
Validation loss: 3.718403995677989

Epoch: 5| Step: 1
Training loss: 3.719378709793091
Validation loss: 3.705726041588732

Epoch: 5| Step: 2
Training loss: 3.5451760292053223
Validation loss: 3.6931051567036617

Epoch: 5| Step: 3
Training loss: 2.4714865684509277
Validation loss: 3.681884627188406

Epoch: 5| Step: 4
Training loss: 3.938856840133667
Validation loss: 3.66750059845627

Epoch: 5| Step: 5
Training loss: 3.396462917327881
Validation loss: 3.6581506267670663

Epoch: 5| Step: 6
Training loss: 3.3088219165802
Validation loss: 3.6445638236179145

Epoch: 5| Step: 7
Training loss: 2.7598862648010254
Validation loss: 3.6343409476741666

Epoch: 5| Step: 8
Training loss: 4.068217754364014
Validation loss: 3.623878389276484

Epoch: 5| Step: 9
Training loss: 4.051388740539551
Validation loss: 3.612488069841939

Epoch: 5| Step: 10
Training loss: 4.579132556915283
Validation loss: 3.599507126756894

Epoch: 10| Step: 0
Training loss: 3.799360752105713
Validation loss: 3.5889357648869997

Epoch: 5| Step: 1
Training loss: 3.456630229949951
Validation loss: 3.5773227496813704

Epoch: 5| Step: 2
Training loss: 3.137641191482544
Validation loss: 3.56517687407873

Epoch: 5| Step: 3
Training loss: 3.8839290142059326
Validation loss: 3.5516339194390083

Epoch: 5| Step: 4
Training loss: 3.5803706645965576
Validation loss: 3.541343614619265

Epoch: 5| Step: 5
Training loss: 3.056145668029785
Validation loss: 3.5286754613281577

Epoch: 5| Step: 6
Training loss: 3.905137538909912
Validation loss: 3.5186050322748

Epoch: 5| Step: 7
Training loss: 4.57172155380249
Validation loss: 3.5094351101947088

Epoch: 5| Step: 8
Training loss: 3.2854137420654297
Validation loss: 3.499119512496456

Epoch: 5| Step: 9
Training loss: 2.3247220516204834
Validation loss: 3.4899251230301394

Epoch: 5| Step: 10
Training loss: 3.008409023284912
Validation loss: 3.483362861858901

Epoch: 11| Step: 0
Training loss: 3.2307846546173096
Validation loss: 3.4749838664967525

Epoch: 5| Step: 1
Training loss: 2.6587307453155518
Validation loss: 3.464344281022267

Epoch: 5| Step: 2
Training loss: 3.1036949157714844
Validation loss: 3.4563748810880925

Epoch: 5| Step: 3
Training loss: 4.219500541687012
Validation loss: 3.4473473897544284

Epoch: 5| Step: 4
Training loss: 3.2046103477478027
Validation loss: 3.4410402031355005

Epoch: 5| Step: 5
Training loss: 3.5940613746643066
Validation loss: 3.4347656875528316

Epoch: 5| Step: 6
Training loss: 2.4904744625091553
Validation loss: 3.4267890504611436

Epoch: 5| Step: 7
Training loss: 3.532721757888794
Validation loss: 3.4201939234169583

Epoch: 5| Step: 8
Training loss: 3.4920897483825684
Validation loss: 3.4108540986173894

Epoch: 5| Step: 9
Training loss: 4.4397478103637695
Validation loss: 3.4038547854269705

Epoch: 5| Step: 10
Training loss: 3.1457018852233887
Validation loss: 3.399999095547584

Epoch: 12| Step: 0
Training loss: 3.3133785724639893
Validation loss: 3.394379295328612

Epoch: 5| Step: 1
Training loss: 3.254361629486084
Validation loss: 3.3879391070335143

Epoch: 5| Step: 2
Training loss: 3.2404227256774902
Validation loss: 3.381086762233447

Epoch: 5| Step: 3
Training loss: 2.8728060722351074
Validation loss: 3.375977188028315

Epoch: 5| Step: 4
Training loss: 3.405444383621216
Validation loss: 3.3715841590717273

Epoch: 5| Step: 5
Training loss: 3.566696882247925
Validation loss: 3.3668591232709986

Epoch: 5| Step: 6
Training loss: 3.051429271697998
Validation loss: 3.361838274104621

Epoch: 5| Step: 7
Training loss: 3.62115478515625
Validation loss: 3.3557338842781643

Epoch: 5| Step: 8
Training loss: 2.1895718574523926
Validation loss: 3.35320088940282

Epoch: 5| Step: 9
Training loss: 4.1230669021606445
Validation loss: 3.3452253777493715

Epoch: 5| Step: 10
Training loss: 4.026661396026611
Validation loss: 3.340540255269697

Epoch: 13| Step: 0
Training loss: 3.5907528400421143
Validation loss: 3.33693386918755

Epoch: 5| Step: 1
Training loss: 2.574838399887085
Validation loss: 3.330896992837229

Epoch: 5| Step: 2
Training loss: 3.035242795944214
Validation loss: 3.330266260331677

Epoch: 5| Step: 3
Training loss: 3.3909249305725098
Validation loss: 3.32677629686171

Epoch: 5| Step: 4
Training loss: 4.153722286224365
Validation loss: 3.3195550493014756

Epoch: 5| Step: 5
Training loss: 3.13283109664917
Validation loss: 3.316623026324857

Epoch: 5| Step: 6
Training loss: 3.6289658546447754
Validation loss: 3.315167914154709

Epoch: 5| Step: 7
Training loss: 3.4233405590057373
Validation loss: 3.3110280216381116

Epoch: 5| Step: 8
Training loss: 2.444995164871216
Validation loss: 3.3088601122620287

Epoch: 5| Step: 9
Training loss: 3.614035129547119
Validation loss: 3.3058502238283873

Epoch: 5| Step: 10
Training loss: 3.1303210258483887
Validation loss: 3.3002299160085697

Epoch: 14| Step: 0
Training loss: 2.9507479667663574
Validation loss: 3.2957397763447096

Epoch: 5| Step: 1
Training loss: 4.011443614959717
Validation loss: 3.291483966253137

Epoch: 5| Step: 2
Training loss: 4.148519039154053
Validation loss: 3.2871467580077467

Epoch: 5| Step: 3
Training loss: 4.16812801361084
Validation loss: 3.282964539784257

Epoch: 5| Step: 4
Training loss: 2.77561354637146
Validation loss: 3.2800673566838747

Epoch: 5| Step: 5
Training loss: 2.5167155265808105
Validation loss: 3.2811351540268108

Epoch: 5| Step: 6
Training loss: 2.288647413253784
Validation loss: 3.28313769320006

Epoch: 5| Step: 7
Training loss: 2.4318716526031494
Validation loss: 3.277574198220366

Epoch: 5| Step: 8
Training loss: 3.40483021736145
Validation loss: 3.2690465014467955

Epoch: 5| Step: 9
Training loss: 3.428549289703369
Validation loss: 3.2678200660213346

Epoch: 5| Step: 10
Training loss: 3.7684621810913086
Validation loss: 3.2625482569458666

Epoch: 15| Step: 0
Training loss: 3.1420860290527344
Validation loss: 3.259358552194411

Epoch: 5| Step: 1
Training loss: 2.613909959793091
Validation loss: 3.2580121460781304

Epoch: 5| Step: 2
Training loss: 3.6328930854797363
Validation loss: 3.255477072090231

Epoch: 5| Step: 3
Training loss: 3.1421597003936768
Validation loss: 3.2501926165755077

Epoch: 5| Step: 4
Training loss: 3.0284388065338135
Validation loss: 3.2501701180652907

Epoch: 5| Step: 5
Training loss: 2.964053153991699
Validation loss: 3.245760545935682

Epoch: 5| Step: 6
Training loss: 3.613253116607666
Validation loss: 3.2426536237039874

Epoch: 5| Step: 7
Training loss: 3.5792369842529297
Validation loss: 3.2421523037777153

Epoch: 5| Step: 8
Training loss: 3.4927070140838623
Validation loss: 3.241571552009993

Epoch: 5| Step: 9
Training loss: 3.5788345336914062
Validation loss: 3.239887686185939

Epoch: 5| Step: 10
Training loss: 2.678880453109741
Validation loss: 3.2341646968677478

Epoch: 16| Step: 0
Training loss: 3.193472385406494
Validation loss: 3.233528765298987

Epoch: 5| Step: 1
Training loss: 4.0436224937438965
Validation loss: 3.2292333161959084

Epoch: 5| Step: 2
Training loss: 3.1249613761901855
Validation loss: 3.224931824591852

Epoch: 5| Step: 3
Training loss: 3.174311399459839
Validation loss: 3.223729274606192

Epoch: 5| Step: 4
Training loss: 3.4761338233947754
Validation loss: 3.2187758953340593

Epoch: 5| Step: 5
Training loss: 3.1259102821350098
Validation loss: 3.218264182408651

Epoch: 5| Step: 6
Training loss: 2.97133469581604
Validation loss: 3.211815728936144

Epoch: 5| Step: 7
Training loss: 3.51878023147583
Validation loss: 3.2080004240876887

Epoch: 5| Step: 8
Training loss: 2.445230722427368
Validation loss: 3.2091896149419967

Epoch: 5| Step: 9
Training loss: 3.5015487670898438
Validation loss: 3.2038256840039323

Epoch: 5| Step: 10
Training loss: 2.631908655166626
Validation loss: 3.1980566260635213

Epoch: 17| Step: 0
Training loss: 3.475703477859497
Validation loss: 3.1966043210798696

Epoch: 5| Step: 1
Training loss: 2.7567286491394043
Validation loss: 3.195695887329758

Epoch: 5| Step: 2
Training loss: 3.271287202835083
Validation loss: 3.195932275505476

Epoch: 5| Step: 3
Training loss: 3.860063076019287
Validation loss: 3.19089759806151

Epoch: 5| Step: 4
Training loss: 3.819945812225342
Validation loss: 3.1888307909811697

Epoch: 5| Step: 5
Training loss: 2.347613573074341
Validation loss: 3.1856116556352183

Epoch: 5| Step: 6
Training loss: 2.936875343322754
Validation loss: 3.1810593630677912

Epoch: 5| Step: 7
Training loss: 2.959850549697876
Validation loss: 3.1775933593832035

Epoch: 5| Step: 8
Training loss: 3.4142699241638184
Validation loss: 3.1763242393411617

Epoch: 5| Step: 9
Training loss: 3.186037540435791
Validation loss: 3.1745062412754184

Epoch: 5| Step: 10
Training loss: 3.0160274505615234
Validation loss: 3.1736142096980924

Epoch: 18| Step: 0
Training loss: 3.3871474266052246
Validation loss: 3.1706843247977634

Epoch: 5| Step: 1
Training loss: 2.4985594749450684
Validation loss: 3.1696426176255748

Epoch: 5| Step: 2
Training loss: 3.0316977500915527
Validation loss: 3.167465376597579

Epoch: 5| Step: 3
Training loss: 2.8928966522216797
Validation loss: 3.164483501065162

Epoch: 5| Step: 4
Training loss: 2.8970093727111816
Validation loss: 3.1633767133118003

Epoch: 5| Step: 5
Training loss: 3.646023988723755
Validation loss: 3.155614745232367

Epoch: 5| Step: 6
Training loss: 3.667248249053955
Validation loss: 3.156411219668645

Epoch: 5| Step: 7
Training loss: 3.3997981548309326
Validation loss: 3.1520165833093787

Epoch: 5| Step: 8
Training loss: 3.401698350906372
Validation loss: 3.1485609598057245

Epoch: 5| Step: 9
Training loss: 2.667600631713867
Validation loss: 3.144057689174529

Epoch: 5| Step: 10
Training loss: 3.3822181224823
Validation loss: 3.1430739920626403

Epoch: 19| Step: 0
Training loss: 3.4698333740234375
Validation loss: 3.1425239783461376

Epoch: 5| Step: 1
Training loss: 3.626068115234375
Validation loss: 3.1450487311168382

Epoch: 5| Step: 2
Training loss: 2.763887405395508
Validation loss: 3.141052722930908

Epoch: 5| Step: 3
Training loss: 3.115739345550537
Validation loss: 3.1454964299355783

Epoch: 5| Step: 4
Training loss: 2.788336992263794
Validation loss: 3.1629478777608564

Epoch: 5| Step: 5
Training loss: 3.6469147205352783
Validation loss: 3.152524317464521

Epoch: 5| Step: 6
Training loss: 3.0534844398498535
Validation loss: 3.13506478904396

Epoch: 5| Step: 7
Training loss: 2.8321633338928223
Validation loss: 3.1318522166180354

Epoch: 5| Step: 8
Training loss: 3.351381778717041
Validation loss: 3.1298739038487917

Epoch: 5| Step: 9
Training loss: 2.753836154937744
Validation loss: 3.1304938921364407

Epoch: 5| Step: 10
Training loss: 3.277376174926758
Validation loss: 3.131460210328461

Epoch: 20| Step: 0
Training loss: 2.782881259918213
Validation loss: 3.1284087575891966

Epoch: 5| Step: 1
Training loss: 3.2743523120880127
Validation loss: 3.1239898589349564

Epoch: 5| Step: 2
Training loss: 2.881225109100342
Validation loss: 3.1212546415226434

Epoch: 5| Step: 3
Training loss: 3.504969835281372
Validation loss: 3.1131497172899145

Epoch: 5| Step: 4
Training loss: 2.248408794403076
Validation loss: 3.10572188643999

Epoch: 5| Step: 5
Training loss: 3.704077959060669
Validation loss: 3.1015069177073817

Epoch: 5| Step: 6
Training loss: 3.664356231689453
Validation loss: 3.099627758866997

Epoch: 5| Step: 7
Training loss: 2.7002482414245605
Validation loss: 3.095978880441317

Epoch: 5| Step: 8
Training loss: 2.91090989112854
Validation loss: 3.09466738341957

Epoch: 5| Step: 9
Training loss: 3.4365622997283936
Validation loss: 3.0872829806420112

Epoch: 5| Step: 10
Training loss: 3.3645639419555664
Validation loss: 3.0803251394661526

Epoch: 21| Step: 0
Training loss: 3.1265087127685547
Validation loss: 3.073963631865799

Epoch: 5| Step: 1
Training loss: 3.3647971153259277
Validation loss: 3.070822936232372

Epoch: 5| Step: 2
Training loss: 3.143667221069336
Validation loss: 3.0690673448706187

Epoch: 5| Step: 3
Training loss: 2.8489489555358887
Validation loss: 3.0731005591730916

Epoch: 5| Step: 4
Training loss: 3.1911048889160156
Validation loss: 3.069843030744983

Epoch: 5| Step: 5
Training loss: 3.670473575592041
Validation loss: 3.06656156047698

Epoch: 5| Step: 6
Training loss: 2.7240371704101562
Validation loss: 3.061129718698481

Epoch: 5| Step: 7
Training loss: 3.1337201595306396
Validation loss: 3.057840234489851

Epoch: 5| Step: 8
Training loss: 3.19340443611145
Validation loss: 3.0535589238648773

Epoch: 5| Step: 9
Training loss: 2.993856906890869
Validation loss: 3.0500134268114643

Epoch: 5| Step: 10
Training loss: 2.6438446044921875
Validation loss: 3.0534445675470496

Epoch: 22| Step: 0
Training loss: 3.1931252479553223
Validation loss: 3.049843875310754

Epoch: 5| Step: 1
Training loss: 3.4042859077453613
Validation loss: 3.0500618360375844

Epoch: 5| Step: 2
Training loss: 2.6497387886047363
Validation loss: 3.04444823726531

Epoch: 5| Step: 3
Training loss: 3.0665555000305176
Validation loss: 3.0459344874146166

Epoch: 5| Step: 4
Training loss: 2.8741300106048584
Validation loss: 3.0412386924989763

Epoch: 5| Step: 5
Training loss: 3.2510361671447754
Validation loss: 3.0391828372914302

Epoch: 5| Step: 6
Training loss: 3.5681838989257812
Validation loss: 3.0351077356646137

Epoch: 5| Step: 7
Training loss: 2.7055933475494385
Validation loss: 3.037033329727829

Epoch: 5| Step: 8
Training loss: 2.7727293968200684
Validation loss: 3.033876839504447

Epoch: 5| Step: 9
Training loss: 3.417135238647461
Validation loss: 3.031097671037079

Epoch: 5| Step: 10
Training loss: 3.06674861907959
Validation loss: 3.029048122385497

Epoch: 23| Step: 0
Training loss: 3.8266429901123047
Validation loss: 3.0293289153806624

Epoch: 5| Step: 1
Training loss: 2.942660093307495
Validation loss: 3.027316370318013

Epoch: 5| Step: 2
Training loss: 3.052974224090576
Validation loss: 3.0272657896882746

Epoch: 5| Step: 3
Training loss: 3.040140390396118
Validation loss: 3.027690036322481

Epoch: 5| Step: 4
Training loss: 2.264820098876953
Validation loss: 3.03155957242494

Epoch: 5| Step: 5
Training loss: 3.5377144813537598
Validation loss: 3.029450093546221

Epoch: 5| Step: 6
Training loss: 2.8500144481658936
Validation loss: 3.020305792490641

Epoch: 5| Step: 7
Training loss: 2.78053617477417
Validation loss: 3.012019493246591

Epoch: 5| Step: 8
Training loss: 2.8172783851623535
Validation loss: 3.0108370268216698

Epoch: 5| Step: 9
Training loss: 3.394033908843994
Validation loss: 3.0077761424485074

Epoch: 5| Step: 10
Training loss: 3.2325668334960938
Validation loss: 3.011937533655474

Epoch: 24| Step: 0
Training loss: 3.0582237243652344
Validation loss: 3.0101910483452583

Epoch: 5| Step: 1
Training loss: 3.4260573387145996
Validation loss: 3.0044314399842293

Epoch: 5| Step: 2
Training loss: 3.5247116088867188
Validation loss: 3.0023166441148326

Epoch: 5| Step: 3
Training loss: 3.302628755569458
Validation loss: 2.99992456231066

Epoch: 5| Step: 4
Training loss: 2.5186619758605957
Validation loss: 2.9966942007823656

Epoch: 5| Step: 5
Training loss: 3.2641048431396484
Validation loss: 2.996112956795641

Epoch: 5| Step: 6
Training loss: 2.219564914703369
Validation loss: 2.995949511886925

Epoch: 5| Step: 7
Training loss: 2.684488296508789
Validation loss: 2.997183984325778

Epoch: 5| Step: 8
Training loss: 3.4109387397766113
Validation loss: 2.997344316974763

Epoch: 5| Step: 9
Training loss: 2.763732433319092
Validation loss: 2.998001954888785

Epoch: 5| Step: 10
Training loss: 3.4415059089660645
Validation loss: 2.98788817210864

Epoch: 25| Step: 0
Training loss: 3.126560926437378
Validation loss: 2.981948214192544

Epoch: 5| Step: 1
Training loss: 2.658454418182373
Validation loss: 2.9807360505545013

Epoch: 5| Step: 2
Training loss: 3.2727348804473877
Validation loss: 2.9814161280150056

Epoch: 5| Step: 3
Training loss: 2.89481258392334
Validation loss: 2.98008039177105

Epoch: 5| Step: 4
Training loss: 3.2224395275115967
Validation loss: 2.974459925005513

Epoch: 5| Step: 5
Training loss: 3.2779412269592285
Validation loss: 2.9734419392001246

Epoch: 5| Step: 6
Training loss: 3.004128932952881
Validation loss: 2.9693193204941286

Epoch: 5| Step: 7
Training loss: 1.852148413658142
Validation loss: 2.969817223087434

Epoch: 5| Step: 8
Training loss: 4.042641639709473
Validation loss: 2.968880230380643

Epoch: 5| Step: 9
Training loss: 2.7907228469848633
Validation loss: 2.9672813466800156

Epoch: 5| Step: 10
Training loss: 3.3376693725585938
Validation loss: 2.96778505591936

Epoch: 26| Step: 0
Training loss: 2.8558456897735596
Validation loss: 2.972932187459802

Epoch: 5| Step: 1
Training loss: 3.4469597339630127
Validation loss: 2.9740141642990934

Epoch: 5| Step: 2
Training loss: 2.851787805557251
Validation loss: 2.9739813368807555

Epoch: 5| Step: 3
Training loss: 3.2195181846618652
Validation loss: 2.961100252725745

Epoch: 5| Step: 4
Training loss: 2.8429150581359863
Validation loss: 2.952195941760976

Epoch: 5| Step: 5
Training loss: 3.3568968772888184
Validation loss: 2.947361848687613

Epoch: 5| Step: 6
Training loss: 3.366896390914917
Validation loss: 2.9514012336730957

Epoch: 5| Step: 7
Training loss: 2.598907947540283
Validation loss: 2.9523834592552594

Epoch: 5| Step: 8
Training loss: 3.5982673168182373
Validation loss: 2.9516783555348716

Epoch: 5| Step: 9
Training loss: 2.7305617332458496
Validation loss: 2.9430269041369037

Epoch: 5| Step: 10
Training loss: 2.2709202766418457
Validation loss: 2.9451512905859176

Epoch: 27| Step: 0
Training loss: 4.224942207336426
Validation loss: 2.9411853359591578

Epoch: 5| Step: 1
Training loss: 3.0254464149475098
Validation loss: 2.9380538335410495

Epoch: 5| Step: 2
Training loss: 2.910964250564575
Validation loss: 2.9398804736393753

Epoch: 5| Step: 3
Training loss: 3.674177885055542
Validation loss: 2.931138887200304

Epoch: 5| Step: 4
Training loss: 3.010366678237915
Validation loss: 2.9293979137174544

Epoch: 5| Step: 5
Training loss: 2.2498881816864014
Validation loss: 2.9272936287746636

Epoch: 5| Step: 6
Training loss: 3.4411869049072266
Validation loss: 2.927759196168633

Epoch: 5| Step: 7
Training loss: 2.783271312713623
Validation loss: 2.9278564632579847

Epoch: 5| Step: 8
Training loss: 2.4282329082489014
Validation loss: 2.9262457406649025

Epoch: 5| Step: 9
Training loss: 2.5412094593048096
Validation loss: 2.927856012057233

Epoch: 5| Step: 10
Training loss: 2.6946887969970703
Validation loss: 2.925302779802712

Epoch: 28| Step: 0
Training loss: 2.860093593597412
Validation loss: 2.9273835817972818

Epoch: 5| Step: 1
Training loss: 3.414165496826172
Validation loss: 2.9267701743751444

Epoch: 5| Step: 2
Training loss: 3.6843056678771973
Validation loss: 2.920741404256513

Epoch: 5| Step: 3
Training loss: 2.6863150596618652
Validation loss: 2.917648664084814

Epoch: 5| Step: 4
Training loss: 2.592491626739502
Validation loss: 2.91621978821293

Epoch: 5| Step: 5
Training loss: 3.462965488433838
Validation loss: 2.916297351160357

Epoch: 5| Step: 6
Training loss: 2.366004705429077
Validation loss: 2.917006941251857

Epoch: 5| Step: 7
Training loss: 2.9231975078582764
Validation loss: 2.914462140811387

Epoch: 5| Step: 8
Training loss: 3.097852945327759
Validation loss: 2.910885513469737

Epoch: 5| Step: 9
Training loss: 3.4196407794952393
Validation loss: 2.9100427627563477

Epoch: 5| Step: 10
Training loss: 2.2318668365478516
Validation loss: 2.904988952862319

Epoch: 29| Step: 0
Training loss: 2.8038582801818848
Validation loss: 2.9056074696202434

Epoch: 5| Step: 1
Training loss: 2.2319064140319824
Validation loss: 2.907540721278037

Epoch: 5| Step: 2
Training loss: 3.521275281906128
Validation loss: 2.904224882843674

Epoch: 5| Step: 3
Training loss: 2.7019927501678467
Validation loss: 2.9013909063031598

Epoch: 5| Step: 4
Training loss: 4.339381217956543
Validation loss: 2.901031024994389

Epoch: 5| Step: 5
Training loss: 2.8461437225341797
Validation loss: 2.8964288644893195

Epoch: 5| Step: 6
Training loss: 3.65643310546875
Validation loss: 2.8956340154012046

Epoch: 5| Step: 7
Training loss: 3.093022584915161
Validation loss: 2.8959477075966458

Epoch: 5| Step: 8
Training loss: 2.459402322769165
Validation loss: 2.8925562673999416

Epoch: 5| Step: 9
Training loss: 3.1361188888549805
Validation loss: 2.8918459594890638

Epoch: 5| Step: 10
Training loss: 1.87941575050354
Validation loss: 2.894802595979424

Epoch: 30| Step: 0
Training loss: 3.163259744644165
Validation loss: 2.891718077403243

Epoch: 5| Step: 1
Training loss: 2.583979368209839
Validation loss: 2.89636976744539

Epoch: 5| Step: 2
Training loss: 2.7216901779174805
Validation loss: 2.8875137580338346

Epoch: 5| Step: 3
Training loss: 3.0799503326416016
Validation loss: 2.8919288906999814

Epoch: 5| Step: 4
Training loss: 2.5351293087005615
Validation loss: 2.8885926969589724

Epoch: 5| Step: 5
Training loss: 3.013096332550049
Validation loss: 2.885035396904074

Epoch: 5| Step: 6
Training loss: 2.398995876312256
Validation loss: 2.8794018299348894

Epoch: 5| Step: 7
Training loss: 3.782017230987549
Validation loss: 2.8796646492455595

Epoch: 5| Step: 8
Training loss: 2.6696317195892334
Validation loss: 2.8768558425288044

Epoch: 5| Step: 9
Training loss: 3.160172939300537
Validation loss: 2.8814879796838246

Epoch: 5| Step: 10
Training loss: 3.652554750442505
Validation loss: 2.8767208386492986

Epoch: 31| Step: 0
Training loss: 2.7508325576782227
Validation loss: 2.874510929148684

Epoch: 5| Step: 1
Training loss: 3.521807909011841
Validation loss: 2.872776139167047

Epoch: 5| Step: 2
Training loss: 3.0891337394714355
Validation loss: 2.8725181189916467

Epoch: 5| Step: 3
Training loss: 2.761254072189331
Validation loss: 2.872081761719078

Epoch: 5| Step: 4
Training loss: 2.5614190101623535
Validation loss: 2.866932856139316

Epoch: 5| Step: 5
Training loss: 2.8668384552001953
Validation loss: 2.8673463380464943

Epoch: 5| Step: 6
Training loss: 2.754289150238037
Validation loss: 2.864087966180617

Epoch: 5| Step: 7
Training loss: 2.7404417991638184
Validation loss: 2.859578732521303

Epoch: 5| Step: 8
Training loss: 2.782918691635132
Validation loss: 2.861369712378389

Epoch: 5| Step: 9
Training loss: 3.420879364013672
Validation loss: 2.8635466201331026

Epoch: 5| Step: 10
Training loss: 3.282693862915039
Validation loss: 2.8582310830393145

Epoch: 32| Step: 0
Training loss: 2.862095832824707
Validation loss: 2.858620528251894

Epoch: 5| Step: 1
Training loss: 2.7050583362579346
Validation loss: 2.8576707891238633

Epoch: 5| Step: 2
Training loss: 2.349754810333252
Validation loss: 2.856316089630127

Epoch: 5| Step: 3
Training loss: 2.828162670135498
Validation loss: 2.859149755970124

Epoch: 5| Step: 4
Training loss: 3.3227756023406982
Validation loss: 2.8599409749430995

Epoch: 5| Step: 5
Training loss: 3.0163731575012207
Validation loss: 2.8524425901392454

Epoch: 5| Step: 6
Training loss: 3.00793719291687
Validation loss: 2.8487080502253708

Epoch: 5| Step: 7
Training loss: 3.1879353523254395
Validation loss: 2.8514764770384757

Epoch: 5| Step: 8
Training loss: 3.299678087234497
Validation loss: 2.844217397833383

Epoch: 5| Step: 9
Training loss: 2.923372745513916
Validation loss: 2.8460131563166136

Epoch: 5| Step: 10
Training loss: 2.866328239440918
Validation loss: 2.846981725385112

Epoch: 33| Step: 0
Training loss: 3.5380325317382812
Validation loss: 2.838632655400102

Epoch: 5| Step: 1
Training loss: 2.7148122787475586
Validation loss: 2.8406931046516664

Epoch: 5| Step: 2
Training loss: 3.3270397186279297
Validation loss: 2.838088309893044

Epoch: 5| Step: 3
Training loss: 2.2115471363067627
Validation loss: 2.8388182014547367

Epoch: 5| Step: 4
Training loss: 2.7772879600524902
Validation loss: 2.839192005895799

Epoch: 5| Step: 5
Training loss: 3.220487117767334
Validation loss: 2.833090151509931

Epoch: 5| Step: 6
Training loss: 3.1025567054748535
Validation loss: 2.837256182906448

Epoch: 5| Step: 7
Training loss: 2.839043378829956
Validation loss: 2.8358289208463443

Epoch: 5| Step: 8
Training loss: 2.6198110580444336
Validation loss: 2.8449523782217376

Epoch: 5| Step: 9
Training loss: 2.51259446144104
Validation loss: 2.8430818588502946

Epoch: 5| Step: 10
Training loss: 3.510089159011841
Validation loss: 2.839246796023461

Epoch: 34| Step: 0
Training loss: 2.277871608734131
Validation loss: 2.8304028511047363

Epoch: 5| Step: 1
Training loss: 2.2499277591705322
Validation loss: 2.8272448252606135

Epoch: 5| Step: 2
Training loss: 4.028553009033203
Validation loss: 2.82613391517311

Epoch: 5| Step: 3
Training loss: 3.2146849632263184
Validation loss: 2.824272947926675

Epoch: 5| Step: 4
Training loss: 2.9029111862182617
Validation loss: 2.8272545594041065

Epoch: 5| Step: 5
Training loss: 3.377631664276123
Validation loss: 2.8231542059170303

Epoch: 5| Step: 6
Training loss: 2.1823837757110596
Validation loss: 2.8191748306315434

Epoch: 5| Step: 7
Training loss: 3.469425678253174
Validation loss: 2.820956401927497

Epoch: 5| Step: 8
Training loss: 2.4916369915008545
Validation loss: 2.819771277007236

Epoch: 5| Step: 9
Training loss: 3.061694622039795
Validation loss: 2.821188385768603

Epoch: 5| Step: 10
Training loss: 2.9023778438568115
Validation loss: 2.8151540551134335

Epoch: 35| Step: 0
Training loss: 2.7063567638397217
Validation loss: 2.8102426170020975

Epoch: 5| Step: 1
Training loss: 3.572417736053467
Validation loss: 2.814830526228874

Epoch: 5| Step: 2
Training loss: 2.122053384780884
Validation loss: 2.8100321831241732

Epoch: 5| Step: 3
Training loss: 3.038459062576294
Validation loss: 2.8106570372017483

Epoch: 5| Step: 4
Training loss: 2.7067768573760986
Validation loss: 2.8091996408277944

Epoch: 5| Step: 5
Training loss: 3.6431262493133545
Validation loss: 2.8095389386659027

Epoch: 5| Step: 6
Training loss: 2.539637327194214
Validation loss: 2.804945391993369

Epoch: 5| Step: 7
Training loss: 2.350761890411377
Validation loss: 2.8071823479026876

Epoch: 5| Step: 8
Training loss: 3.288761854171753
Validation loss: 2.8034823915009857

Epoch: 5| Step: 9
Training loss: 3.0725550651550293
Validation loss: 2.80393708649502

Epoch: 5| Step: 10
Training loss: 3.027561902999878
Validation loss: 2.803186014134397

Epoch: 36| Step: 0
Training loss: 2.5147204399108887
Validation loss: 2.8003930789168163

Epoch: 5| Step: 1
Training loss: 3.057758331298828
Validation loss: 2.7967653889809885

Epoch: 5| Step: 2
Training loss: 2.503995180130005
Validation loss: 2.7997060539901897

Epoch: 5| Step: 3
Training loss: 3.1611344814300537
Validation loss: 2.7988338034640075

Epoch: 5| Step: 4
Training loss: 3.1823034286499023
Validation loss: 2.795312689196679

Epoch: 5| Step: 5
Training loss: 2.7486190795898438
Validation loss: 2.793311354934528

Epoch: 5| Step: 6
Training loss: 2.9439053535461426
Validation loss: 2.791856950329196

Epoch: 5| Step: 7
Training loss: 2.9801127910614014
Validation loss: 2.7899858003021567

Epoch: 5| Step: 8
Training loss: 2.7979297637939453
Validation loss: 2.7887353256184566

Epoch: 5| Step: 9
Training loss: 3.012939691543579
Validation loss: 2.7882770107638453

Epoch: 5| Step: 10
Training loss: 3.053797483444214
Validation loss: 2.789151199402348

Epoch: 37| Step: 0
Training loss: 3.4526877403259277
Validation loss: 2.7905127335620183

Epoch: 5| Step: 1
Training loss: 3.0170814990997314
Validation loss: 2.7914733168899373

Epoch: 5| Step: 2
Training loss: 2.6873958110809326
Validation loss: 2.7932920045750116

Epoch: 5| Step: 3
Training loss: 2.7305855751037598
Validation loss: 2.7953715529493106

Epoch: 5| Step: 4
Training loss: 2.810136318206787
Validation loss: 2.7980683285702943

Epoch: 5| Step: 5
Training loss: 2.750143527984619
Validation loss: 2.800735765887845

Epoch: 5| Step: 6
Training loss: 3.31829571723938
Validation loss: 2.790265034603816

Epoch: 5| Step: 7
Training loss: 2.888911724090576
Validation loss: 2.7803820845901326

Epoch: 5| Step: 8
Training loss: 2.878539562225342
Validation loss: 2.777304792916903

Epoch: 5| Step: 9
Training loss: 2.7638678550720215
Validation loss: 2.788368163570281

Epoch: 5| Step: 10
Training loss: 2.552347183227539
Validation loss: 2.780643901517314

Epoch: 38| Step: 0
Training loss: 2.8557562828063965
Validation loss: 2.785213198713077

Epoch: 5| Step: 1
Training loss: 2.962172031402588
Validation loss: 2.775529443576772

Epoch: 5| Step: 2
Training loss: 3.1882550716400146
Validation loss: 2.7752596178362445

Epoch: 5| Step: 3
Training loss: 2.912520408630371
Validation loss: 2.770120010581068

Epoch: 5| Step: 4
Training loss: 2.9372193813323975
Validation loss: 2.7750941655969106

Epoch: 5| Step: 5
Training loss: 2.7875864505767822
Validation loss: 2.7705354434187695

Epoch: 5| Step: 6
Training loss: 1.9152476787567139
Validation loss: 2.77545670283738

Epoch: 5| Step: 7
Training loss: 2.833116054534912
Validation loss: 2.7830695772683747

Epoch: 5| Step: 8
Training loss: 2.7495670318603516
Validation loss: 2.778008812217302

Epoch: 5| Step: 9
Training loss: 3.684032917022705
Validation loss: 2.774468283499441

Epoch: 5| Step: 10
Training loss: 2.9516170024871826
Validation loss: 2.7715305512951267

Epoch: 39| Step: 0
Training loss: 2.1045098304748535
Validation loss: 2.7671209842927995

Epoch: 5| Step: 1
Training loss: 3.692490816116333
Validation loss: 2.7620599705685853

Epoch: 5| Step: 2
Training loss: 2.808061122894287
Validation loss: 2.7619483983644875

Epoch: 5| Step: 3
Training loss: 2.479111433029175
Validation loss: 2.7634326283649733

Epoch: 5| Step: 4
Training loss: 3.180631160736084
Validation loss: 2.7586373898290817

Epoch: 5| Step: 5
Training loss: 2.578580856323242
Validation loss: 2.7579643469984814

Epoch: 5| Step: 6
Training loss: 2.5663902759552
Validation loss: 2.7588422939341557

Epoch: 5| Step: 7
Training loss: 2.941848039627075
Validation loss: 2.7639962191222818

Epoch: 5| Step: 8
Training loss: 2.797973871231079
Validation loss: 2.7648812186333442

Epoch: 5| Step: 9
Training loss: 3.293415069580078
Validation loss: 2.759343154968754

Epoch: 5| Step: 10
Training loss: 3.245054006576538
Validation loss: 2.757567221118558

Epoch: 40| Step: 0
Training loss: 2.390657901763916
Validation loss: 2.754403360428349

Epoch: 5| Step: 1
Training loss: 3.0434539318084717
Validation loss: 2.754802547475343

Epoch: 5| Step: 2
Training loss: 3.580453395843506
Validation loss: 2.752398870324576

Epoch: 5| Step: 3
Training loss: 2.6233673095703125
Validation loss: 2.7486394912965837

Epoch: 5| Step: 4
Training loss: 2.7669482231140137
Validation loss: 2.7416116524768133

Epoch: 5| Step: 5
Training loss: 3.0181148052215576
Validation loss: 2.7418169436916227

Epoch: 5| Step: 6
Training loss: 2.9540092945098877
Validation loss: 2.7354335733639297

Epoch: 5| Step: 7
Training loss: 2.9973111152648926
Validation loss: 2.7319777396417435

Epoch: 5| Step: 8
Training loss: 2.864170551300049
Validation loss: 2.7290745678768364

Epoch: 5| Step: 9
Training loss: 2.2601609230041504
Validation loss: 2.729748320835893

Epoch: 5| Step: 10
Training loss: 2.932119846343994
Validation loss: 2.7309667346298054

Epoch: 41| Step: 0
Training loss: 2.7778995037078857
Validation loss: 2.7314485657599663

Epoch: 5| Step: 1
Training loss: 3.2637321949005127
Validation loss: 2.731874435178695

Epoch: 5| Step: 2
Training loss: 3.109217643737793
Validation loss: 2.7299456698920137

Epoch: 5| Step: 3
Training loss: 1.5284923315048218
Validation loss: 2.7316947316610687

Epoch: 5| Step: 4
Training loss: 2.592717409133911
Validation loss: 2.7303571470322145

Epoch: 5| Step: 5
Training loss: 3.0863113403320312
Validation loss: 2.7242261055977113

Epoch: 5| Step: 6
Training loss: 2.914456844329834
Validation loss: 2.7218747754250803

Epoch: 5| Step: 7
Training loss: 3.0383944511413574
Validation loss: 2.7185909824986614

Epoch: 5| Step: 8
Training loss: 2.922816514968872
Validation loss: 2.7251133636761735

Epoch: 5| Step: 9
Training loss: 3.203794002532959
Validation loss: 2.7217135711382796

Epoch: 5| Step: 10
Training loss: 2.9845478534698486
Validation loss: 2.7191602645381803

Epoch: 42| Step: 0
Training loss: 3.318387508392334
Validation loss: 2.721673224561958

Epoch: 5| Step: 1
Training loss: 2.9168524742126465
Validation loss: 2.7213230004874607

Epoch: 5| Step: 2
Training loss: 3.155810594558716
Validation loss: 2.715673659437446

Epoch: 5| Step: 3
Training loss: 2.547886371612549
Validation loss: 2.7204618095069804

Epoch: 5| Step: 4
Training loss: 3.126047372817993
Validation loss: 2.7161940938682965

Epoch: 5| Step: 5
Training loss: 2.573197603225708
Validation loss: 2.7161366093543267

Epoch: 5| Step: 6
Training loss: 2.010721206665039
Validation loss: 2.7109929823106333

Epoch: 5| Step: 7
Training loss: 2.6189446449279785
Validation loss: 2.7092464482912453

Epoch: 5| Step: 8
Training loss: 3.505535125732422
Validation loss: 2.708081929914413

Epoch: 5| Step: 9
Training loss: 3.151996612548828
Validation loss: 2.7092819752231723

Epoch: 5| Step: 10
Training loss: 2.272817611694336
Validation loss: 2.712248909857965

Epoch: 43| Step: 0
Training loss: 2.7791225910186768
Validation loss: 2.7088871156015704

Epoch: 5| Step: 1
Training loss: 2.985194206237793
Validation loss: 2.70357039923309

Epoch: 5| Step: 2
Training loss: 3.1447532176971436
Validation loss: 2.705492509308682

Epoch: 5| Step: 3
Training loss: 2.589543104171753
Validation loss: 2.702998270270645

Epoch: 5| Step: 4
Training loss: 2.8464322090148926
Validation loss: 2.7029594118877123

Epoch: 5| Step: 5
Training loss: 3.12092661857605
Validation loss: 2.699745075677031

Epoch: 5| Step: 6
Training loss: 3.1140918731689453
Validation loss: 2.7001791948913247

Epoch: 5| Step: 7
Training loss: 2.3615641593933105
Validation loss: 2.704659502993348

Epoch: 5| Step: 8
Training loss: 2.580580711364746
Validation loss: 2.7013010850516697

Epoch: 5| Step: 9
Training loss: 2.689565420150757
Validation loss: 2.701641482691611

Epoch: 5| Step: 10
Training loss: 3.0140912532806396
Validation loss: 2.6902395525286273

Epoch: 44| Step: 0
Training loss: 2.2429449558258057
Validation loss: 2.6943683214085077

Epoch: 5| Step: 1
Training loss: 2.9880051612854004
Validation loss: 2.690328859513806

Epoch: 5| Step: 2
Training loss: 3.2499194145202637
Validation loss: 2.6911185838842906

Epoch: 5| Step: 3
Training loss: 3.079493284225464
Validation loss: 2.698357500055785

Epoch: 5| Step: 4
Training loss: 3.1027331352233887
Validation loss: 2.699702642297232

Epoch: 5| Step: 5
Training loss: 2.425467014312744
Validation loss: 2.6974651916052705

Epoch: 5| Step: 6
Training loss: 2.686577558517456
Validation loss: 2.6910693158385572

Epoch: 5| Step: 7
Training loss: 2.7051286697387695
Validation loss: 2.689252725211523

Epoch: 5| Step: 8
Training loss: 3.006863832473755
Validation loss: 2.6852140477908555

Epoch: 5| Step: 9
Training loss: 2.777442455291748
Validation loss: 2.682634248528429

Epoch: 5| Step: 10
Training loss: 2.9770145416259766
Validation loss: 2.683014092906829

Epoch: 45| Step: 0
Training loss: 3.424053192138672
Validation loss: 2.6876824261039816

Epoch: 5| Step: 1
Training loss: 2.45064640045166
Validation loss: 2.69545494100099

Epoch: 5| Step: 2
Training loss: 2.9739811420440674
Validation loss: 2.6963377767993557

Epoch: 5| Step: 3
Training loss: 2.9142842292785645
Validation loss: 2.692441035342473

Epoch: 5| Step: 4
Training loss: 3.1987385749816895
Validation loss: 2.6862462079653175

Epoch: 5| Step: 5
Training loss: 2.5932674407958984
Validation loss: 2.678140107021537

Epoch: 5| Step: 6
Training loss: 2.760714530944824
Validation loss: 2.679208727293117

Epoch: 5| Step: 7
Training loss: 2.861920118331909
Validation loss: 2.6762268235606532

Epoch: 5| Step: 8
Training loss: 2.5400047302246094
Validation loss: 2.6765407464837514

Epoch: 5| Step: 9
Training loss: 2.9495949745178223
Validation loss: 2.6833704235733196

Epoch: 5| Step: 10
Training loss: 2.304563283920288
Validation loss: 2.677648393056726

Epoch: 46| Step: 0
Training loss: 3.390601634979248
Validation loss: 2.6717437185266966

Epoch: 5| Step: 1
Training loss: 2.406142234802246
Validation loss: 2.66782074077155

Epoch: 5| Step: 2
Training loss: 3.382054090499878
Validation loss: 2.670288734538581

Epoch: 5| Step: 3
Training loss: 2.765775680541992
Validation loss: 2.66792614998356

Epoch: 5| Step: 4
Training loss: 2.1306703090667725
Validation loss: 2.6686788297468618

Epoch: 5| Step: 5
Training loss: 3.040214776992798
Validation loss: 2.6666398509856193

Epoch: 5| Step: 6
Training loss: 2.468447208404541
Validation loss: 2.6674211384147726

Epoch: 5| Step: 7
Training loss: 2.8696277141571045
Validation loss: 2.668632984161377

Epoch: 5| Step: 8
Training loss: 2.958991050720215
Validation loss: 2.6656394748277563

Epoch: 5| Step: 9
Training loss: 2.871927261352539
Validation loss: 2.674186865488688

Epoch: 5| Step: 10
Training loss: 2.6341724395751953
Validation loss: 2.6778990504562215

Epoch: 47| Step: 0
Training loss: 3.276170253753662
Validation loss: 2.682679066094019

Epoch: 5| Step: 1
Training loss: 3.0225448608398438
Validation loss: 2.683386689873152

Epoch: 5| Step: 2
Training loss: 3.579859972000122
Validation loss: 2.67007250683282

Epoch: 5| Step: 3
Training loss: 2.3698761463165283
Validation loss: 2.6617236137390137

Epoch: 5| Step: 4
Training loss: 1.7954294681549072
Validation loss: 2.667143121842415

Epoch: 5| Step: 5
Training loss: 3.231017589569092
Validation loss: 2.6640498586880264

Epoch: 5| Step: 6
Training loss: 2.5971171855926514
Validation loss: 2.6668701428239063

Epoch: 5| Step: 7
Training loss: 3.033560276031494
Validation loss: 2.673631803963774

Epoch: 5| Step: 8
Training loss: 2.9992804527282715
Validation loss: 2.676735270407892

Epoch: 5| Step: 9
Training loss: 2.8087925910949707
Validation loss: 2.673619247251941

Epoch: 5| Step: 10
Training loss: 2.110595226287842
Validation loss: 2.660371757322742

Epoch: 48| Step: 0
Training loss: 2.384495258331299
Validation loss: 2.658967402673537

Epoch: 5| Step: 1
Training loss: 2.968456983566284
Validation loss: 2.6562813353794876

Epoch: 5| Step: 2
Training loss: 1.8193336725234985
Validation loss: 2.656585178067607

Epoch: 5| Step: 3
Training loss: 3.2698802947998047
Validation loss: 2.6591338393508748

Epoch: 5| Step: 4
Training loss: 3.356637477874756
Validation loss: 2.663368830116846

Epoch: 5| Step: 5
Training loss: 3.0436394214630127
Validation loss: 2.6646076222901702

Epoch: 5| Step: 6
Training loss: 3.1884799003601074
Validation loss: 2.668095273356284

Epoch: 5| Step: 7
Training loss: 3.3965868949890137
Validation loss: 2.6709299779707387

Epoch: 5| Step: 8
Training loss: 2.2807106971740723
Validation loss: 2.6735647724520777

Epoch: 5| Step: 9
Training loss: 2.703418016433716
Validation loss: 2.6730173351944133

Epoch: 5| Step: 10
Training loss: 2.5062761306762695
Validation loss: 2.666549100670763

Epoch: 49| Step: 0
Training loss: 2.682666540145874
Validation loss: 2.6633084743253645

Epoch: 5| Step: 1
Training loss: 2.6305432319641113
Validation loss: 2.655284704700593

Epoch: 5| Step: 2
Training loss: 2.720123767852783
Validation loss: 2.654746958004531

Epoch: 5| Step: 3
Training loss: 2.639624834060669
Validation loss: 2.654035240091303

Epoch: 5| Step: 4
Training loss: 2.229200839996338
Validation loss: 2.66014769769484

Epoch: 5| Step: 5
Training loss: 2.997742176055908
Validation loss: 2.6543476991755988

Epoch: 5| Step: 6
Training loss: 3.3312294483184814
Validation loss: 2.654137375534222

Epoch: 5| Step: 7
Training loss: 3.424941301345825
Validation loss: 2.6513884323899464

Epoch: 5| Step: 8
Training loss: 1.8759578466415405
Validation loss: 2.649112239960701

Epoch: 5| Step: 9
Training loss: 3.0817272663116455
Validation loss: 2.6486007680175123

Epoch: 5| Step: 10
Training loss: 3.2689995765686035
Validation loss: 2.648892192430394

Epoch: 50| Step: 0
Training loss: 2.3426144123077393
Validation loss: 2.64743112620487

Epoch: 5| Step: 1
Training loss: 3.0933327674865723
Validation loss: 2.648401778231385

Epoch: 5| Step: 2
Training loss: 2.9138314723968506
Validation loss: 2.6555672948078444

Epoch: 5| Step: 3
Training loss: 2.2683732509613037
Validation loss: 2.659654807018977

Epoch: 5| Step: 4
Training loss: 2.9461429119110107
Validation loss: 2.6604372634682605

Epoch: 5| Step: 5
Training loss: 2.455838680267334
Validation loss: 2.6530117706585954

Epoch: 5| Step: 6
Training loss: 2.849048137664795
Validation loss: 2.652420449000533

Epoch: 5| Step: 7
Training loss: 2.6564419269561768
Validation loss: 2.6525986117701374

Epoch: 5| Step: 8
Training loss: 3.453176498413086
Validation loss: 2.645999813592562

Epoch: 5| Step: 9
Training loss: 3.008601427078247
Validation loss: 2.647840799823884

Epoch: 5| Step: 10
Training loss: 2.8309435844421387
Validation loss: 2.6484401200407293

Epoch: 51| Step: 0
Training loss: 2.275150775909424
Validation loss: 2.6477193294032926

Epoch: 5| Step: 1
Training loss: 2.603012800216675
Validation loss: 2.657663353027836

Epoch: 5| Step: 2
Training loss: 3.225949764251709
Validation loss: 2.6469082217062674

Epoch: 5| Step: 3
Training loss: 3.3278090953826904
Validation loss: 2.6505531469980874

Epoch: 5| Step: 4
Training loss: 2.31569504737854
Validation loss: 2.6438192270135366

Epoch: 5| Step: 5
Training loss: 2.7515790462493896
Validation loss: 2.643577798720329

Epoch: 5| Step: 6
Training loss: 1.7588497400283813
Validation loss: 2.639308221878544

Epoch: 5| Step: 7
Training loss: 3.312115430831909
Validation loss: 2.647546850224977

Epoch: 5| Step: 8
Training loss: 3.15783429145813
Validation loss: 2.64777797268283

Epoch: 5| Step: 9
Training loss: 3.183187961578369
Validation loss: 2.649761538351736

Epoch: 5| Step: 10
Training loss: 2.869236469268799
Validation loss: 2.6543773066612983

Epoch: 52| Step: 0
Training loss: 2.3303287029266357
Validation loss: 2.6541634657049693

Epoch: 5| Step: 1
Training loss: 3.125335216522217
Validation loss: 2.6437679131825766

Epoch: 5| Step: 2
Training loss: 2.657133102416992
Validation loss: 2.632478655025523

Epoch: 5| Step: 3
Training loss: 3.024839162826538
Validation loss: 2.630772113800049

Epoch: 5| Step: 4
Training loss: 2.8706130981445312
Validation loss: 2.632929173848962

Epoch: 5| Step: 5
Training loss: 2.874377727508545
Validation loss: 2.632061061038766

Epoch: 5| Step: 6
Training loss: 2.7595324516296387
Validation loss: 2.63066481390307

Epoch: 5| Step: 7
Training loss: 2.7339272499084473
Validation loss: 2.6338192186047955

Epoch: 5| Step: 8
Training loss: 2.680042028427124
Validation loss: 2.6337255124122865

Epoch: 5| Step: 9
Training loss: 2.8173415660858154
Validation loss: 2.6304691350588234

Epoch: 5| Step: 10
Training loss: 2.825091600418091
Validation loss: 2.6339389047315045

Epoch: 53| Step: 0
Training loss: 3.1989097595214844
Validation loss: 2.6279237590810305

Epoch: 5| Step: 1
Training loss: 2.956141710281372
Validation loss: 2.628676452944356

Epoch: 5| Step: 2
Training loss: 2.71358060836792
Validation loss: 2.6318916223382436

Epoch: 5| Step: 3
Training loss: 2.6001267433166504
Validation loss: 2.6298955717394428

Epoch: 5| Step: 4
Training loss: 2.5518059730529785
Validation loss: 2.6277096553515364

Epoch: 5| Step: 5
Training loss: 3.086045742034912
Validation loss: 2.623976130639353

Epoch: 5| Step: 6
Training loss: 2.7624027729034424
Validation loss: 2.625359289107784

Epoch: 5| Step: 7
Training loss: 3.1911189556121826
Validation loss: 2.627312203889252

Epoch: 5| Step: 8
Training loss: 1.8252251148223877
Validation loss: 2.626945382805281

Epoch: 5| Step: 9
Training loss: 2.8476667404174805
Validation loss: 2.6282000874960296

Epoch: 5| Step: 10
Training loss: 2.8758082389831543
Validation loss: 2.6302173214574016

Epoch: 54| Step: 0
Training loss: 2.870110034942627
Validation loss: 2.630977499869562

Epoch: 5| Step: 1
Training loss: 3.6510696411132812
Validation loss: 2.6325632269664476

Epoch: 5| Step: 2
Training loss: 2.3080341815948486
Validation loss: 2.631013954839399

Epoch: 5| Step: 3
Training loss: 2.635420560836792
Validation loss: 2.631886315602128

Epoch: 5| Step: 4
Training loss: 3.2607884407043457
Validation loss: 2.633570494190339

Epoch: 5| Step: 5
Training loss: 2.0385096073150635
Validation loss: 2.63563552723136

Epoch: 5| Step: 6
Training loss: 3.043548822402954
Validation loss: 2.6365733838850454

Epoch: 5| Step: 7
Training loss: 2.680006742477417
Validation loss: 2.6221537948936544

Epoch: 5| Step: 8
Training loss: 2.7918801307678223
Validation loss: 2.6233118708415697

Epoch: 5| Step: 9
Training loss: 2.241112232208252
Validation loss: 2.622906505420644

Epoch: 5| Step: 10
Training loss: 3.1470913887023926
Validation loss: 2.6241244603228826

Epoch: 55| Step: 0
Training loss: 3.2571396827697754
Validation loss: 2.627809314317601

Epoch: 5| Step: 1
Training loss: 3.3301994800567627
Validation loss: 2.623059190729613

Epoch: 5| Step: 2
Training loss: 2.031585693359375
Validation loss: 2.618846680528374

Epoch: 5| Step: 3
Training loss: 2.6329174041748047
Validation loss: 2.616658531209474

Epoch: 5| Step: 4
Training loss: 2.4491031169891357
Validation loss: 2.617441395277618

Epoch: 5| Step: 5
Training loss: 3.2576229572296143
Validation loss: 2.613891324689311

Epoch: 5| Step: 6
Training loss: 1.7040889263153076
Validation loss: 2.61616781450087

Epoch: 5| Step: 7
Training loss: 2.8343875408172607
Validation loss: 2.6176293639726538

Epoch: 5| Step: 8
Training loss: 2.9831647872924805
Validation loss: 2.6191425426031953

Epoch: 5| Step: 9
Training loss: 3.331871509552002
Validation loss: 2.6221430045302196

Epoch: 5| Step: 10
Training loss: 2.7232935428619385
Validation loss: 2.6256983254545476

Epoch: 56| Step: 0
Training loss: 2.7092015743255615
Validation loss: 2.62886199643535

Epoch: 5| Step: 1
Training loss: 3.1685805320739746
Validation loss: 2.6313942914368003

Epoch: 5| Step: 2
Training loss: 2.422914505004883
Validation loss: 2.64717984455888

Epoch: 5| Step: 3
Training loss: 2.601651191711426
Validation loss: 2.642494455460579

Epoch: 5| Step: 4
Training loss: 3.653301239013672
Validation loss: 2.6344479822343394

Epoch: 5| Step: 5
Training loss: 2.6384663581848145
Validation loss: 2.619836150958974

Epoch: 5| Step: 6
Training loss: 3.1339941024780273
Validation loss: 2.618170766420262

Epoch: 5| Step: 7
Training loss: 2.1624443531036377
Validation loss: 2.6161803096853276

Epoch: 5| Step: 8
Training loss: 2.6127305030822754
Validation loss: 2.6149538409325386

Epoch: 5| Step: 9
Training loss: 2.2376081943511963
Validation loss: 2.617298787639987

Epoch: 5| Step: 10
Training loss: 3.239847183227539
Validation loss: 2.6177501601557576

Epoch: 57| Step: 0
Training loss: 3.14850115776062
Validation loss: 2.6187022219422045

Epoch: 5| Step: 1
Training loss: 2.577171564102173
Validation loss: 2.616900702958466

Epoch: 5| Step: 2
Training loss: 2.8492865562438965
Validation loss: 2.614928968491093

Epoch: 5| Step: 3
Training loss: 2.8088881969451904
Validation loss: 2.6144861610986854

Epoch: 5| Step: 4
Training loss: 3.7199554443359375
Validation loss: 2.607903431820613

Epoch: 5| Step: 5
Training loss: 2.6143336296081543
Validation loss: 2.6082051569415676

Epoch: 5| Step: 6
Training loss: 2.4032437801361084
Validation loss: 2.6103040479844615

Epoch: 5| Step: 7
Training loss: 2.518275737762451
Validation loss: 2.6081985478760092

Epoch: 5| Step: 8
Training loss: 1.7700011730194092
Validation loss: 2.6056800478248188

Epoch: 5| Step: 9
Training loss: 3.313098907470703
Validation loss: 2.6088636613661245

Epoch: 5| Step: 10
Training loss: 2.707000732421875
Validation loss: 2.616105453942412

Epoch: 58| Step: 0
Training loss: 2.864701747894287
Validation loss: 2.6259616395478607

Epoch: 5| Step: 1
Training loss: 3.3861279487609863
Validation loss: 2.617604870950022

Epoch: 5| Step: 2
Training loss: 1.9233410358428955
Validation loss: 2.6169873488846647

Epoch: 5| Step: 3
Training loss: 2.6496334075927734
Validation loss: 2.615734928397722

Epoch: 5| Step: 4
Training loss: 3.2892203330993652
Validation loss: 2.609964434818555

Epoch: 5| Step: 5
Training loss: 2.5871052742004395
Validation loss: 2.6137046865237656

Epoch: 5| Step: 6
Training loss: 3.057737112045288
Validation loss: 2.6132108575554303

Epoch: 5| Step: 7
Training loss: 2.2939982414245605
Validation loss: 2.6101314021695043

Epoch: 5| Step: 8
Training loss: 2.797856330871582
Validation loss: 2.607739892057193

Epoch: 5| Step: 9
Training loss: 2.9300613403320312
Validation loss: 2.60360634967845

Epoch: 5| Step: 10
Training loss: 2.6592814922332764
Validation loss: 2.5990829826683126

Epoch: 59| Step: 0
Training loss: 2.9069581031799316
Validation loss: 2.5995169967733402

Epoch: 5| Step: 1
Training loss: 2.605233669281006
Validation loss: 2.6008926642838346

Epoch: 5| Step: 2
Training loss: 3.5609829425811768
Validation loss: 2.599883520474998

Epoch: 5| Step: 3
Training loss: 2.208181858062744
Validation loss: 2.603479257193945

Epoch: 5| Step: 4
Training loss: 3.3363754749298096
Validation loss: 2.603845952659525

Epoch: 5| Step: 5
Training loss: 2.8627536296844482
Validation loss: 2.6062744522607453

Epoch: 5| Step: 6
Training loss: 2.8025808334350586
Validation loss: 2.6108529772809757

Epoch: 5| Step: 7
Training loss: 2.740445375442505
Validation loss: 2.6076424839676067

Epoch: 5| Step: 8
Training loss: 2.6097168922424316
Validation loss: 2.604220385192543

Epoch: 5| Step: 9
Training loss: 2.646557331085205
Validation loss: 2.6014668146769204

Epoch: 5| Step: 10
Training loss: 2.119941473007202
Validation loss: 2.5973430295144357

Epoch: 60| Step: 0
Training loss: 2.731790542602539
Validation loss: 2.594362581929853

Epoch: 5| Step: 1
Training loss: 2.7259511947631836
Validation loss: 2.597922791716873

Epoch: 5| Step: 2
Training loss: 2.9244627952575684
Validation loss: 2.5969086282996723

Epoch: 5| Step: 3
Training loss: 3.262010097503662
Validation loss: 2.59201632776568

Epoch: 5| Step: 4
Training loss: 2.2308754920959473
Validation loss: 2.6007702299343642

Epoch: 5| Step: 5
Training loss: 2.656505584716797
Validation loss: 2.6049287473001788

Epoch: 5| Step: 6
Training loss: 2.7167880535125732
Validation loss: 2.6016332949361494

Epoch: 5| Step: 7
Training loss: 2.7991855144500732
Validation loss: 2.608577054033997

Epoch: 5| Step: 8
Training loss: 3.2155940532684326
Validation loss: 2.605943464463757

Epoch: 5| Step: 9
Training loss: 2.7089157104492188
Validation loss: 2.601680124959638

Epoch: 5| Step: 10
Training loss: 2.3771402835845947
Validation loss: 2.599179975448116

Epoch: 61| Step: 0
Training loss: 2.478163242340088
Validation loss: 2.592977564821961

Epoch: 5| Step: 1
Training loss: 3.257972002029419
Validation loss: 2.590366760889689

Epoch: 5| Step: 2
Training loss: 3.0131499767303467
Validation loss: 2.5888126332272767

Epoch: 5| Step: 3
Training loss: 1.7606096267700195
Validation loss: 2.592689767960579

Epoch: 5| Step: 4
Training loss: 2.4772541522979736
Validation loss: 2.594290179591025

Epoch: 5| Step: 5
Training loss: 3.094538450241089
Validation loss: 2.595613056613553

Epoch: 5| Step: 6
Training loss: 2.857182264328003
Validation loss: 2.5977102248899397

Epoch: 5| Step: 7
Training loss: 2.8924572467803955
Validation loss: 2.599008257671069

Epoch: 5| Step: 8
Training loss: 2.928882598876953
Validation loss: 2.5999793147528045

Epoch: 5| Step: 9
Training loss: 2.98860239982605
Validation loss: 2.5998403231302896

Epoch: 5| Step: 10
Training loss: 2.6909632682800293
Validation loss: 2.5956799035431235

Epoch: 62| Step: 0
Training loss: 2.8335185050964355
Validation loss: 2.592637836292226

Epoch: 5| Step: 1
Training loss: 2.5360751152038574
Validation loss: 2.589218185793969

Epoch: 5| Step: 2
Training loss: 2.845165729522705
Validation loss: 2.5915826315520913

Epoch: 5| Step: 3
Training loss: 2.831329584121704
Validation loss: 2.5920183504781416

Epoch: 5| Step: 4
Training loss: 2.8429012298583984
Validation loss: 2.592993402993807

Epoch: 5| Step: 5
Training loss: 2.9461894035339355
Validation loss: 2.595228284917852

Epoch: 5| Step: 6
Training loss: 2.384138345718384
Validation loss: 2.587767416431058

Epoch: 5| Step: 7
Training loss: 2.455488681793213
Validation loss: 2.584384964358422

Epoch: 5| Step: 8
Training loss: 2.771214246749878
Validation loss: 2.5839205018935667

Epoch: 5| Step: 9
Training loss: 3.094259262084961
Validation loss: 2.5839169435603644

Epoch: 5| Step: 10
Training loss: 2.863321304321289
Validation loss: 2.585666718021516

Epoch: 63| Step: 0
Training loss: 2.22208833694458
Validation loss: 2.585196407892371

Epoch: 5| Step: 1
Training loss: 2.6981475353240967
Validation loss: 2.585512853437854

Epoch: 5| Step: 2
Training loss: 2.665309429168701
Validation loss: 2.584356225946898

Epoch: 5| Step: 3
Training loss: 3.236602783203125
Validation loss: 2.583590720289497

Epoch: 5| Step: 4
Training loss: 2.6224286556243896
Validation loss: 2.586203608461606

Epoch: 5| Step: 5
Training loss: 3.3477776050567627
Validation loss: 2.581553259203511

Epoch: 5| Step: 6
Training loss: 3.430812358856201
Validation loss: 2.586613614072082

Epoch: 5| Step: 7
Training loss: 1.65374755859375
Validation loss: 2.5898606059371785

Epoch: 5| Step: 8
Training loss: 3.1950559616088867
Validation loss: 2.582669593954599

Epoch: 5| Step: 9
Training loss: 2.665994167327881
Validation loss: 2.582938055838308

Epoch: 5| Step: 10
Training loss: 2.47794246673584
Validation loss: 2.582626445319063

Epoch: 64| Step: 0
Training loss: 3.0902411937713623
Validation loss: 2.590416613445487

Epoch: 5| Step: 1
Training loss: 2.325354814529419
Validation loss: 2.5913864207524124

Epoch: 5| Step: 2
Training loss: 2.6653904914855957
Validation loss: 2.590418851503762

Epoch: 5| Step: 3
Training loss: 2.848252534866333
Validation loss: 2.587358396540406

Epoch: 5| Step: 4
Training loss: 2.8279011249542236
Validation loss: 2.5856329651289087

Epoch: 5| Step: 5
Training loss: 2.7728404998779297
Validation loss: 2.580772082010905

Epoch: 5| Step: 6
Training loss: 2.2198727130889893
Validation loss: 2.5756459697600333

Epoch: 5| Step: 7
Training loss: 3.5714282989501953
Validation loss: 2.5760815092312392

Epoch: 5| Step: 8
Training loss: 2.504187822341919
Validation loss: 2.5698744455973306

Epoch: 5| Step: 9
Training loss: 2.625650405883789
Validation loss: 2.571218531618836

Epoch: 5| Step: 10
Training loss: 2.7906556129455566
Validation loss: 2.57080106325047

Epoch: 65| Step: 0
Training loss: 3.0363831520080566
Validation loss: 2.5686322104546333

Epoch: 5| Step: 1
Training loss: 2.4086945056915283
Validation loss: 2.5738319273917907

Epoch: 5| Step: 2
Training loss: 2.647918224334717
Validation loss: 2.571493815350276

Epoch: 5| Step: 3
Training loss: 3.2986178398132324
Validation loss: 2.5674279607752317

Epoch: 5| Step: 4
Training loss: 2.590254068374634
Validation loss: 2.5688655171343076

Epoch: 5| Step: 5
Training loss: 2.8085508346557617
Validation loss: 2.569102143728605

Epoch: 5| Step: 6
Training loss: 2.858560562133789
Validation loss: 2.5657865091036727

Epoch: 5| Step: 7
Training loss: 3.0958244800567627
Validation loss: 2.567998984808563

Epoch: 5| Step: 8
Training loss: 2.5038106441497803
Validation loss: 2.5683367508713917

Epoch: 5| Step: 9
Training loss: 2.5718836784362793
Validation loss: 2.5744806822910102

Epoch: 5| Step: 10
Training loss: 2.3110272884368896
Validation loss: 2.5870085095846527

Epoch: 66| Step: 0
Training loss: 3.0901241302490234
Validation loss: 2.583088231343095

Epoch: 5| Step: 1
Training loss: 2.479553461074829
Validation loss: 2.575790156600296

Epoch: 5| Step: 2
Training loss: 1.936397910118103
Validation loss: 2.5660712001144246

Epoch: 5| Step: 3
Training loss: 3.1900882720947266
Validation loss: 2.562336416654689

Epoch: 5| Step: 4
Training loss: 2.5381569862365723
Validation loss: 2.563576254793393

Epoch: 5| Step: 5
Training loss: 2.8170852661132812
Validation loss: 2.5635330856487317

Epoch: 5| Step: 6
Training loss: 2.7089779376983643
Validation loss: 2.562602976317047

Epoch: 5| Step: 7
Training loss: 2.593114137649536
Validation loss: 2.5593528388648905

Epoch: 5| Step: 8
Training loss: 2.680849552154541
Validation loss: 2.563693522125162

Epoch: 5| Step: 9
Training loss: 2.9909350872039795
Validation loss: 2.561967365203365

Epoch: 5| Step: 10
Training loss: 3.1634514331817627
Validation loss: 2.560500639741139

Epoch: 67| Step: 0
Training loss: 2.4303345680236816
Validation loss: 2.5665124154859975

Epoch: 5| Step: 1
Training loss: 1.7780860662460327
Validation loss: 2.5623973313198296

Epoch: 5| Step: 2
Training loss: 2.657397508621216
Validation loss: 2.5633484163591937

Epoch: 5| Step: 3
Training loss: 2.5657527446746826
Validation loss: 2.5676301781849196

Epoch: 5| Step: 4
Training loss: 2.969529628753662
Validation loss: 2.5662026046424784

Epoch: 5| Step: 5
Training loss: 2.8050734996795654
Validation loss: 2.568628816194432

Epoch: 5| Step: 6
Training loss: 4.173436164855957
Validation loss: 2.561497506274972

Epoch: 5| Step: 7
Training loss: 2.9902186393737793
Validation loss: 2.5645282729979484

Epoch: 5| Step: 8
Training loss: 2.8744843006134033
Validation loss: 2.5591637242224907

Epoch: 5| Step: 9
Training loss: 2.2025246620178223
Validation loss: 2.559710369315199

Epoch: 5| Step: 10
Training loss: 2.6277689933776855
Validation loss: 2.557424978543353

Epoch: 68| Step: 0
Training loss: 2.183203935623169
Validation loss: 2.555171858879828

Epoch: 5| Step: 1
Training loss: 2.441237211227417
Validation loss: 2.557058988078948

Epoch: 5| Step: 2
Training loss: 2.594499111175537
Validation loss: 2.5595838151952273

Epoch: 5| Step: 3
Training loss: 2.265162944793701
Validation loss: 2.5604759749545845

Epoch: 5| Step: 4
Training loss: 3.216792583465576
Validation loss: 2.555000579485329

Epoch: 5| Step: 5
Training loss: 2.1749701499938965
Validation loss: 2.554533532870713

Epoch: 5| Step: 6
Training loss: 3.6709494590759277
Validation loss: 2.55632697638645

Epoch: 5| Step: 7
Training loss: 3.2214064598083496
Validation loss: 2.556844772831086

Epoch: 5| Step: 8
Training loss: 2.4447085857391357
Validation loss: 2.56109353034727

Epoch: 5| Step: 9
Training loss: 3.0491251945495605
Validation loss: 2.5674792079515356

Epoch: 5| Step: 10
Training loss: 2.86246395111084
Validation loss: 2.577714240679177

Epoch: 69| Step: 0
Training loss: 3.3535804748535156
Validation loss: 2.5761021209019486

Epoch: 5| Step: 1
Training loss: 2.636679172515869
Validation loss: 2.565778696408836

Epoch: 5| Step: 2
Training loss: 2.513258457183838
Validation loss: 2.557183780977803

Epoch: 5| Step: 3
Training loss: 3.4079253673553467
Validation loss: 2.5555973463161017

Epoch: 5| Step: 4
Training loss: 2.260307550430298
Validation loss: 2.5552367907698437

Epoch: 5| Step: 5
Training loss: 2.5799036026000977
Validation loss: 2.5531048159445486

Epoch: 5| Step: 6
Training loss: 2.552449941635132
Validation loss: 2.5530912337764615

Epoch: 5| Step: 7
Training loss: 2.628392219543457
Validation loss: 2.5478064167884087

Epoch: 5| Step: 8
Training loss: 2.8419435024261475
Validation loss: 2.5512061054988573

Epoch: 5| Step: 9
Training loss: 2.674912691116333
Validation loss: 2.5540731735126947

Epoch: 5| Step: 10
Training loss: 2.661344289779663
Validation loss: 2.5524893165916525

Epoch: 70| Step: 0
Training loss: 2.899496078491211
Validation loss: 2.550285436773813

Epoch: 5| Step: 1
Training loss: 3.114751100540161
Validation loss: 2.548664431418142

Epoch: 5| Step: 2
Training loss: 2.673983097076416
Validation loss: 2.5478643037939586

Epoch: 5| Step: 3
Training loss: 2.6604437828063965
Validation loss: 2.5491261174601894

Epoch: 5| Step: 4
Training loss: 2.192523241043091
Validation loss: 2.5523740040358676

Epoch: 5| Step: 5
Training loss: 3.001466751098633
Validation loss: 2.554366542446998

Epoch: 5| Step: 6
Training loss: 2.988377094268799
Validation loss: 2.5484208265940347

Epoch: 5| Step: 7
Training loss: 2.816432476043701
Validation loss: 2.5514122337423344

Epoch: 5| Step: 8
Training loss: 2.8543100357055664
Validation loss: 2.548127205141129

Epoch: 5| Step: 9
Training loss: 2.6155343055725098
Validation loss: 2.545164836350308

Epoch: 5| Step: 10
Training loss: 2.172370195388794
Validation loss: 2.5443840783129454

Epoch: 71| Step: 0
Training loss: 3.112083673477173
Validation loss: 2.5446793776686474

Epoch: 5| Step: 1
Training loss: 2.955235242843628
Validation loss: 2.5479061859910206

Epoch: 5| Step: 2
Training loss: 1.687786340713501
Validation loss: 2.550335491857221

Epoch: 5| Step: 3
Training loss: 2.3958969116210938
Validation loss: 2.5486606449209233

Epoch: 5| Step: 4
Training loss: 1.8284146785736084
Validation loss: 2.5499722009064048

Epoch: 5| Step: 5
Training loss: 2.7910542488098145
Validation loss: 2.5435544470305085

Epoch: 5| Step: 6
Training loss: 3.2579543590545654
Validation loss: 2.5503283418634886

Epoch: 5| Step: 7
Training loss: 2.117995262145996
Validation loss: 2.5504031770972797

Epoch: 5| Step: 8
Training loss: 3.774646282196045
Validation loss: 2.5556584199269614

Epoch: 5| Step: 9
Training loss: 2.967463731765747
Validation loss: 2.54742528546241

Epoch: 5| Step: 10
Training loss: 3.1735892295837402
Validation loss: 2.5469764458235873

Epoch: 72| Step: 0
Training loss: 2.5367720127105713
Validation loss: 2.543253739674886

Epoch: 5| Step: 1
Training loss: 3.1855392456054688
Validation loss: 2.5426701679024646

Epoch: 5| Step: 2
Training loss: 2.998776435852051
Validation loss: 2.542636215045888

Epoch: 5| Step: 3
Training loss: 1.9500713348388672
Validation loss: 2.540970512615737

Epoch: 5| Step: 4
Training loss: 3.3172755241394043
Validation loss: 2.538682558203256

Epoch: 5| Step: 5
Training loss: 2.6091132164001465
Validation loss: 2.5381621673542965

Epoch: 5| Step: 6
Training loss: 3.139247417449951
Validation loss: 2.5388804507511917

Epoch: 5| Step: 7
Training loss: 2.954127550125122
Validation loss: 2.53990759388093

Epoch: 5| Step: 8
Training loss: 2.825071334838867
Validation loss: 2.537846211464174

Epoch: 5| Step: 9
Training loss: 2.2197322845458984
Validation loss: 2.5401547083290676

Epoch: 5| Step: 10
Training loss: 2.1366031169891357
Validation loss: 2.542372052387525

Epoch: 73| Step: 0
Training loss: 3.5344817638397217
Validation loss: 2.5423309495372157

Epoch: 5| Step: 1
Training loss: 3.1631264686584473
Validation loss: 2.5459471543629966

Epoch: 5| Step: 2
Training loss: 2.8199706077575684
Validation loss: 2.540543653631723

Epoch: 5| Step: 3
Training loss: 3.172297239303589
Validation loss: 2.542679618763667

Epoch: 5| Step: 4
Training loss: 3.418227434158325
Validation loss: 2.542670770358014

Epoch: 5| Step: 5
Training loss: 2.092085361480713
Validation loss: 2.540034209528277

Epoch: 5| Step: 6
Training loss: 2.2156009674072266
Validation loss: 2.5388319287248837

Epoch: 5| Step: 7
Training loss: 1.7226593494415283
Validation loss: 2.5472369322212796

Epoch: 5| Step: 8
Training loss: 2.389002561569214
Validation loss: 2.5470293850027104

Epoch: 5| Step: 9
Training loss: 2.92645001411438
Validation loss: 2.551357335941766

Epoch: 5| Step: 10
Training loss: 2.4203922748565674
Validation loss: 2.553384034864364

Epoch: 74| Step: 0
Training loss: 2.82283091545105
Validation loss: 2.5530902852294264

Epoch: 5| Step: 1
Training loss: 2.9789233207702637
Validation loss: 2.5493998912072953

Epoch: 5| Step: 2
Training loss: 2.2917091846466064
Validation loss: 2.542886995500134

Epoch: 5| Step: 3
Training loss: 2.485781669616699
Validation loss: 2.5450740988536547

Epoch: 5| Step: 4
Training loss: 3.138524293899536
Validation loss: 2.5461476156788487

Epoch: 5| Step: 5
Training loss: 2.3277392387390137
Validation loss: 2.5437441205465667

Epoch: 5| Step: 6
Training loss: 2.8730931282043457
Validation loss: 2.535397452692832

Epoch: 5| Step: 7
Training loss: 2.1959776878356934
Validation loss: 2.5323959781277563

Epoch: 5| Step: 8
Training loss: 3.0413033962249756
Validation loss: 2.5318645123512513

Epoch: 5| Step: 9
Training loss: 2.715137243270874
Validation loss: 2.5349039621250604

Epoch: 5| Step: 10
Training loss: 3.1822407245635986
Validation loss: 2.5402292179804977

Epoch: 75| Step: 0
Training loss: 3.2359840869903564
Validation loss: 2.533791680489817

Epoch: 5| Step: 1
Training loss: 2.734875440597534
Validation loss: 2.5340162220821587

Epoch: 5| Step: 2
Training loss: 3.1579430103302
Validation loss: 2.5373371031976517

Epoch: 5| Step: 3
Training loss: 2.5454277992248535
Validation loss: 2.534573235819417

Epoch: 5| Step: 4
Training loss: 2.6144344806671143
Validation loss: 2.532737442242202

Epoch: 5| Step: 5
Training loss: 2.5002036094665527
Validation loss: 2.5303476369509132

Epoch: 5| Step: 6
Training loss: 2.3473334312438965
Validation loss: 2.536375909723261

Epoch: 5| Step: 7
Training loss: 2.7131569385528564
Validation loss: 2.540190117333525

Epoch: 5| Step: 8
Training loss: 2.3710715770721436
Validation loss: 2.5377761394746843

Epoch: 5| Step: 9
Training loss: 2.840263843536377
Validation loss: 2.541294285046157

Epoch: 5| Step: 10
Training loss: 2.947627544403076
Validation loss: 2.538136753984677

Epoch: 76| Step: 0
Training loss: 2.261223316192627
Validation loss: 2.536388184434624

Epoch: 5| Step: 1
Training loss: 3.1453795433044434
Validation loss: 2.5377421584180606

Epoch: 5| Step: 2
Training loss: 3.6234068870544434
Validation loss: 2.537629158266129

Epoch: 5| Step: 3
Training loss: 2.400704860687256
Validation loss: 2.5395755280730543

Epoch: 5| Step: 4
Training loss: 2.166810989379883
Validation loss: 2.538572936929682

Epoch: 5| Step: 5
Training loss: 2.760289192199707
Validation loss: 2.5469814936319985

Epoch: 5| Step: 6
Training loss: 2.744821786880493
Validation loss: 2.5421439422074186

Epoch: 5| Step: 7
Training loss: 2.349292039871216
Validation loss: 2.540773827542541

Epoch: 5| Step: 8
Training loss: 2.273165225982666
Validation loss: 2.5391243414212297

Epoch: 5| Step: 9
Training loss: 3.1524717807769775
Validation loss: 2.5332236238705215

Epoch: 5| Step: 10
Training loss: 3.055150032043457
Validation loss: 2.5347981094032206

Epoch: 77| Step: 0
Training loss: 2.808345079421997
Validation loss: 2.530412156094787

Epoch: 5| Step: 1
Training loss: 3.207468032836914
Validation loss: 2.534714542409425

Epoch: 5| Step: 2
Training loss: 3.1277365684509277
Validation loss: 2.5376095874335176

Epoch: 5| Step: 3
Training loss: 2.129887104034424
Validation loss: 2.5335510443615656

Epoch: 5| Step: 4
Training loss: 2.393861770629883
Validation loss: 2.539239852659164

Epoch: 5| Step: 5
Training loss: 2.413883686065674
Validation loss: 2.5350224048860612

Epoch: 5| Step: 6
Training loss: 2.9081177711486816
Validation loss: 2.5350482335654636

Epoch: 5| Step: 7
Training loss: 3.349900722503662
Validation loss: 2.5369500242253786

Epoch: 5| Step: 8
Training loss: 2.7220406532287598
Validation loss: 2.5318326796254804

Epoch: 5| Step: 9
Training loss: 2.4723479747772217
Validation loss: 2.52891109835717

Epoch: 5| Step: 10
Training loss: 2.3686447143554688
Validation loss: 2.533229343352779

Epoch: 78| Step: 0
Training loss: 3.198469400405884
Validation loss: 2.5304053445016184

Epoch: 5| Step: 1
Training loss: 3.0761044025421143
Validation loss: 2.5387252992199314

Epoch: 5| Step: 2
Training loss: 2.7997443675994873
Validation loss: 2.5406724714463755

Epoch: 5| Step: 3
Training loss: 2.4819390773773193
Validation loss: 2.5454154552951938

Epoch: 5| Step: 4
Training loss: 2.612964630126953
Validation loss: 2.555813976513442

Epoch: 5| Step: 5
Training loss: 2.6170742511749268
Validation loss: 2.5521722634633384

Epoch: 5| Step: 6
Training loss: 2.8988234996795654
Validation loss: 2.5421269709064114

Epoch: 5| Step: 7
Training loss: 2.8767194747924805
Validation loss: 2.5369057347697597

Epoch: 5| Step: 8
Training loss: 2.2888777256011963
Validation loss: 2.5257530417493594

Epoch: 5| Step: 9
Training loss: 3.0125396251678467
Validation loss: 2.524889520419541

Epoch: 5| Step: 10
Training loss: 1.8908017873764038
Validation loss: 2.5217664985246557

Epoch: 79| Step: 0
Training loss: 2.9516327381134033
Validation loss: 2.5220067219067643

Epoch: 5| Step: 1
Training loss: 2.159280776977539
Validation loss: 2.5210975062462593

Epoch: 5| Step: 2
Training loss: 2.399658679962158
Validation loss: 2.527184678662208

Epoch: 5| Step: 3
Training loss: 3.477738857269287
Validation loss: 2.527046572777533

Epoch: 5| Step: 4
Training loss: 2.616581916809082
Validation loss: 2.526907887510074

Epoch: 5| Step: 5
Training loss: 2.5842316150665283
Validation loss: 2.5258031839965494

Epoch: 5| Step: 6
Training loss: 1.5345900058746338
Validation loss: 2.5254964879764024

Epoch: 5| Step: 7
Training loss: 3.223677396774292
Validation loss: 2.5238455290435464

Epoch: 5| Step: 8
Training loss: 2.801499605178833
Validation loss: 2.521246653731151

Epoch: 5| Step: 9
Training loss: 3.2141273021698
Validation loss: 2.521684036459974

Epoch: 5| Step: 10
Training loss: 3.0068531036376953
Validation loss: 2.518185523248488

Epoch: 80| Step: 0
Training loss: 2.2963500022888184
Validation loss: 2.5257604122161865

Epoch: 5| Step: 1
Training loss: 2.742112636566162
Validation loss: 2.5266072801364365

Epoch: 5| Step: 2
Training loss: 3.0417227745056152
Validation loss: 2.5330307406763874

Epoch: 5| Step: 3
Training loss: 2.567429780960083
Validation loss: 2.5397608716000795

Epoch: 5| Step: 4
Training loss: 1.712581992149353
Validation loss: 2.5459686966352564

Epoch: 5| Step: 5
Training loss: 2.590174674987793
Validation loss: 2.5485152326604372

Epoch: 5| Step: 6
Training loss: 2.5677490234375
Validation loss: 2.5432140673360517

Epoch: 5| Step: 7
Training loss: 3.006354570388794
Validation loss: 2.536246189507105

Epoch: 5| Step: 8
Training loss: 3.8632614612579346
Validation loss: 2.5282927200358403

Epoch: 5| Step: 9
Training loss: 3.2221527099609375
Validation loss: 2.522476674408041

Epoch: 5| Step: 10
Training loss: 2.1616768836975098
Validation loss: 2.5225889195678053

Epoch: 81| Step: 0
Training loss: 2.492412567138672
Validation loss: 2.5213988596393215

Epoch: 5| Step: 1
Training loss: 2.8188087940216064
Validation loss: 2.5234638401257095

Epoch: 5| Step: 2
Training loss: 3.209393262863159
Validation loss: 2.520571529224355

Epoch: 5| Step: 3
Training loss: 2.408799648284912
Validation loss: 2.5187758194502963

Epoch: 5| Step: 4
Training loss: 2.2779815196990967
Validation loss: 2.5216512218598397

Epoch: 5| Step: 5
Training loss: 2.904219388961792
Validation loss: 2.519307795391288

Epoch: 5| Step: 6
Training loss: 2.90212082862854
Validation loss: 2.5193195009744294

Epoch: 5| Step: 7
Training loss: 2.5919651985168457
Validation loss: 2.525255299383594

Epoch: 5| Step: 8
Training loss: 3.000239133834839
Validation loss: 2.5285144441871235

Epoch: 5| Step: 9
Training loss: 2.6228561401367188
Validation loss: 2.5382838890116703

Epoch: 5| Step: 10
Training loss: 2.552123546600342
Validation loss: 2.539249248402093

Epoch: 82| Step: 0
Training loss: 1.9928467273712158
Validation loss: 2.553845295342066

Epoch: 5| Step: 1
Training loss: 2.9320104122161865
Validation loss: 2.550722345229118

Epoch: 5| Step: 2
Training loss: 2.5731704235076904
Validation loss: 2.5384980786231255

Epoch: 5| Step: 3
Training loss: 2.8611159324645996
Validation loss: 2.5229854173557733

Epoch: 5| Step: 4
Training loss: 3.1627583503723145
Validation loss: 2.522653536130023

Epoch: 5| Step: 5
Training loss: 2.779449224472046
Validation loss: 2.519162690767678

Epoch: 5| Step: 6
Training loss: 2.874054431915283
Validation loss: 2.5199390201158423

Epoch: 5| Step: 7
Training loss: 2.8696682453155518
Validation loss: 2.5182526111602783

Epoch: 5| Step: 8
Training loss: 1.9331119060516357
Validation loss: 2.5174659682858374

Epoch: 5| Step: 9
Training loss: 2.404883623123169
Validation loss: 2.5180693928913405

Epoch: 5| Step: 10
Training loss: 3.6037395000457764
Validation loss: 2.522307798426638

Epoch: 83| Step: 0
Training loss: 2.5786948204040527
Validation loss: 2.5233235513010333

Epoch: 5| Step: 1
Training loss: 2.5387539863586426
Validation loss: 2.51828904562099

Epoch: 5| Step: 2
Training loss: 2.4340567588806152
Validation loss: 2.520109917527886

Epoch: 5| Step: 3
Training loss: 2.8488261699676514
Validation loss: 2.5222914898267357

Epoch: 5| Step: 4
Training loss: 2.844292640686035
Validation loss: 2.5178448538626395

Epoch: 5| Step: 5
Training loss: 2.7564749717712402
Validation loss: 2.5133965656321537

Epoch: 5| Step: 6
Training loss: 3.5713677406311035
Validation loss: 2.514563463067496

Epoch: 5| Step: 7
Training loss: 2.9716920852661133
Validation loss: 2.5150293739893104

Epoch: 5| Step: 8
Training loss: 3.3684070110321045
Validation loss: 2.518333955477643

Epoch: 5| Step: 9
Training loss: 1.8960593938827515
Validation loss: 2.515273050595355

Epoch: 5| Step: 10
Training loss: 1.8264721632003784
Validation loss: 2.517239316817253

Epoch: 84| Step: 0
Training loss: 2.2663445472717285
Validation loss: 2.5196586770396077

Epoch: 5| Step: 1
Training loss: 2.6417734622955322
Validation loss: 2.5203622618029193

Epoch: 5| Step: 2
Training loss: 2.924546003341675
Validation loss: 2.5249642377258628

Epoch: 5| Step: 3
Training loss: 2.526613473892212
Validation loss: 2.5201976427467923

Epoch: 5| Step: 4
Training loss: 2.832606077194214
Validation loss: 2.519132509026476

Epoch: 5| Step: 5
Training loss: 2.5882046222686768
Validation loss: 2.51812651336834

Epoch: 5| Step: 6
Training loss: 2.6504244804382324
Validation loss: 2.5144492477499027

Epoch: 5| Step: 7
Training loss: 2.880911111831665
Validation loss: 2.5131225457755466

Epoch: 5| Step: 8
Training loss: 3.1063072681427
Validation loss: 2.5121550662543184

Epoch: 5| Step: 9
Training loss: 2.3368496894836426
Validation loss: 2.5123738447825112

Epoch: 5| Step: 10
Training loss: 3.040931224822998
Validation loss: 2.514203868886476

Epoch: 85| Step: 0
Training loss: 2.599810838699341
Validation loss: 2.5185226753193843

Epoch: 5| Step: 1
Training loss: 2.473705530166626
Validation loss: 2.5173566649037022

Epoch: 5| Step: 2
Training loss: 2.2566332817077637
Validation loss: 2.5179294155490015

Epoch: 5| Step: 3
Training loss: 2.7066268920898438
Validation loss: 2.5133681707484747

Epoch: 5| Step: 4
Training loss: 3.026524782180786
Validation loss: 2.515737892479025

Epoch: 5| Step: 5
Training loss: 2.33323335647583
Validation loss: 2.516670396251063

Epoch: 5| Step: 6
Training loss: 3.280517578125
Validation loss: 2.526796640888337

Epoch: 5| Step: 7
Training loss: 3.108412981033325
Validation loss: 2.520782357902937

Epoch: 5| Step: 8
Training loss: 2.487999439239502
Validation loss: 2.5203466184677614

Epoch: 5| Step: 9
Training loss: 2.000114917755127
Validation loss: 2.5189272460117134

Epoch: 5| Step: 10
Training loss: 3.501765727996826
Validation loss: 2.524203751676826

Epoch: 86| Step: 0
Training loss: 2.4619460105895996
Validation loss: 2.524022181828817

Epoch: 5| Step: 1
Training loss: 2.8198885917663574
Validation loss: 2.520693435463854

Epoch: 5| Step: 2
Training loss: 2.8891210556030273
Validation loss: 2.5206965015780542

Epoch: 5| Step: 3
Training loss: 1.6888660192489624
Validation loss: 2.522227697474982

Epoch: 5| Step: 4
Training loss: 2.983673095703125
Validation loss: 2.5328285771031536

Epoch: 5| Step: 5
Training loss: 3.0199360847473145
Validation loss: 2.537833857279952

Epoch: 5| Step: 6
Training loss: 2.6116065979003906
Validation loss: 2.5387938407159623

Epoch: 5| Step: 7
Training loss: 3.477748394012451
Validation loss: 2.529250734595842

Epoch: 5| Step: 8
Training loss: 2.8758299350738525
Validation loss: 2.515789152473532

Epoch: 5| Step: 9
Training loss: 2.862583875656128
Validation loss: 2.509854462838942

Epoch: 5| Step: 10
Training loss: 1.9165844917297363
Validation loss: 2.508838730473672

Epoch: 87| Step: 0
Training loss: 2.4809553623199463
Validation loss: 2.508122815880724

Epoch: 5| Step: 1
Training loss: 2.6737594604492188
Validation loss: 2.514387607574463

Epoch: 5| Step: 2
Training loss: 3.0122694969177246
Validation loss: 2.5137611255850842

Epoch: 5| Step: 3
Training loss: 2.490553855895996
Validation loss: 2.516914500985094

Epoch: 5| Step: 4
Training loss: 2.2540221214294434
Validation loss: 2.5183566821518766

Epoch: 5| Step: 5
Training loss: 2.79343843460083
Validation loss: 2.516968898875739

Epoch: 5| Step: 6
Training loss: 3.143160104751587
Validation loss: 2.5139025231843353

Epoch: 5| Step: 7
Training loss: 2.3729217052459717
Validation loss: 2.504653166699153

Epoch: 5| Step: 8
Training loss: 3.0522661209106445
Validation loss: 2.5041473552744877

Epoch: 5| Step: 9
Training loss: 3.0338428020477295
Validation loss: 2.5107404108970397

Epoch: 5| Step: 10
Training loss: 2.5172648429870605
Validation loss: 2.512976127286111

Epoch: 88| Step: 0
Training loss: 2.5278306007385254
Validation loss: 2.5134891181863765

Epoch: 5| Step: 1
Training loss: 2.4381744861602783
Validation loss: 2.524140488716864

Epoch: 5| Step: 2
Training loss: 2.950511932373047
Validation loss: 2.525897425989951

Epoch: 5| Step: 3
Training loss: 3.08607816696167
Validation loss: 2.5206240838573826

Epoch: 5| Step: 4
Training loss: 2.745387077331543
Validation loss: 2.5278949045365855

Epoch: 5| Step: 5
Training loss: 3.0078816413879395
Validation loss: 2.5317484537760415

Epoch: 5| Step: 6
Training loss: 2.721252202987671
Validation loss: 2.5344137683991463

Epoch: 5| Step: 7
Training loss: 2.5328316688537598
Validation loss: 2.5341203956193823

Epoch: 5| Step: 8
Training loss: 2.3724968433380127
Validation loss: 2.51492932791351

Epoch: 5| Step: 9
Training loss: 2.666640520095825
Validation loss: 2.515533462647469

Epoch: 5| Step: 10
Training loss: 2.6608364582061768
Validation loss: 2.507992531663628

Epoch: 89| Step: 0
Training loss: 3.024498462677002
Validation loss: 2.507088174102127

Epoch: 5| Step: 1
Training loss: 2.2513725757598877
Validation loss: 2.506528164750786

Epoch: 5| Step: 2
Training loss: 3.5936622619628906
Validation loss: 2.5064999236855456

Epoch: 5| Step: 3
Training loss: 2.825399398803711
Validation loss: 2.511441228210285

Epoch: 5| Step: 4
Training loss: 2.996037721633911
Validation loss: 2.5096958555201048

Epoch: 5| Step: 5
Training loss: 3.3778767585754395
Validation loss: 2.511221147352649

Epoch: 5| Step: 6
Training loss: 1.9661060571670532
Validation loss: 2.512961426088887

Epoch: 5| Step: 7
Training loss: 2.6757779121398926
Validation loss: 2.509219515708185

Epoch: 5| Step: 8
Training loss: 2.1721386909484863
Validation loss: 2.5104095166729343

Epoch: 5| Step: 9
Training loss: 2.3657114505767822
Validation loss: 2.514578603929089

Epoch: 5| Step: 10
Training loss: 2.489306688308716
Validation loss: 2.516999908672866

Epoch: 90| Step: 0
Training loss: 3.1616463661193848
Validation loss: 2.5126463854184715

Epoch: 5| Step: 1
Training loss: 2.6168911457061768
Validation loss: 2.5087464189016693

Epoch: 5| Step: 2
Training loss: 2.184908390045166
Validation loss: 2.5064470998702513

Epoch: 5| Step: 3
Training loss: 3.43407940864563
Validation loss: 2.5000906631510746

Epoch: 5| Step: 4
Training loss: 2.731806755065918
Validation loss: 2.5010106537931707

Epoch: 5| Step: 5
Training loss: 2.807046413421631
Validation loss: 2.503920885824388

Epoch: 5| Step: 6
Training loss: 2.6857974529266357
Validation loss: 2.504666002847815

Epoch: 5| Step: 7
Training loss: 2.7618916034698486
Validation loss: 2.5110503345407467

Epoch: 5| Step: 8
Training loss: 2.626451015472412
Validation loss: 2.512218713760376

Epoch: 5| Step: 9
Training loss: 2.3516554832458496
Validation loss: 2.511750195616035

Epoch: 5| Step: 10
Training loss: 2.258910894393921
Validation loss: 2.5222016380679224

Epoch: 91| Step: 0
Training loss: 3.1608662605285645
Validation loss: 2.517376107554282

Epoch: 5| Step: 1
Training loss: 3.508289337158203
Validation loss: 2.514876206715902

Epoch: 5| Step: 2
Training loss: 1.6427927017211914
Validation loss: 2.5132553603059504

Epoch: 5| Step: 3
Training loss: 2.7983486652374268
Validation loss: 2.5033965508143106

Epoch: 5| Step: 4
Training loss: 2.621223211288452
Validation loss: 2.505105536471131

Epoch: 5| Step: 5
Training loss: 2.7721540927886963
Validation loss: 2.505735607557399

Epoch: 5| Step: 6
Training loss: 2.9229094982147217
Validation loss: 2.5074779218243015

Epoch: 5| Step: 7
Training loss: 2.3577475547790527
Validation loss: 2.507077432447864

Epoch: 5| Step: 8
Training loss: 2.7224087715148926
Validation loss: 2.5048135942028416

Epoch: 5| Step: 9
Training loss: 2.5869202613830566
Validation loss: 2.5060094556500836

Epoch: 5| Step: 10
Training loss: 2.461860418319702
Validation loss: 2.5055484002636326

Epoch: 92| Step: 0
Training loss: 3.360020160675049
Validation loss: 2.5169266859690347

Epoch: 5| Step: 1
Training loss: 2.651782512664795
Validation loss: 2.5111710179236626

Epoch: 5| Step: 2
Training loss: 2.314774990081787
Validation loss: 2.5067876641468336

Epoch: 5| Step: 3
Training loss: 2.5214781761169434
Validation loss: 2.5015330853000766

Epoch: 5| Step: 4
Training loss: 3.0967836380004883
Validation loss: 2.5045781417559554

Epoch: 5| Step: 5
Training loss: 2.4562265872955322
Validation loss: 2.5121067236828547

Epoch: 5| Step: 6
Training loss: 2.8568203449249268
Validation loss: 2.5143153539267917

Epoch: 5| Step: 7
Training loss: 2.451525926589966
Validation loss: 2.506540257443664

Epoch: 5| Step: 8
Training loss: 2.9706084728240967
Validation loss: 2.507692551100126

Epoch: 5| Step: 9
Training loss: 2.430590867996216
Validation loss: 2.500746211697978

Epoch: 5| Step: 10
Training loss: 2.4241864681243896
Validation loss: 2.501251902631534

Epoch: 93| Step: 0
Training loss: 2.8655242919921875
Validation loss: 2.49696683883667

Epoch: 5| Step: 1
Training loss: 2.7421138286590576
Validation loss: 2.497123954116657

Epoch: 5| Step: 2
Training loss: 2.8839004039764404
Validation loss: 2.4979514921865156

Epoch: 5| Step: 3
Training loss: 2.62019419670105
Validation loss: 2.496319265775783

Epoch: 5| Step: 4
Training loss: 2.8077492713928223
Validation loss: 2.495117679719002

Epoch: 5| Step: 5
Training loss: 2.0356552600860596
Validation loss: 2.498027096512497

Epoch: 5| Step: 6
Training loss: 2.606649875640869
Validation loss: 2.4958305281977498

Epoch: 5| Step: 7
Training loss: 3.459049940109253
Validation loss: 2.4983094738375757

Epoch: 5| Step: 8
Training loss: 2.8030002117156982
Validation loss: 2.4969408537751887

Epoch: 5| Step: 9
Training loss: 2.4741644859313965
Validation loss: 2.4979731216225574

Epoch: 5| Step: 10
Training loss: 2.2865402698516846
Validation loss: 2.496649375525854

Epoch: 94| Step: 0
Training loss: 2.6255626678466797
Validation loss: 2.496822134140999

Epoch: 5| Step: 1
Training loss: 2.7740941047668457
Validation loss: 2.5034799011804725

Epoch: 5| Step: 2
Training loss: 2.5087132453918457
Validation loss: 2.503431266353976

Epoch: 5| Step: 3
Training loss: 2.434863567352295
Validation loss: 2.50807858538884

Epoch: 5| Step: 4
Training loss: 3.0906245708465576
Validation loss: 2.5107849387712378

Epoch: 5| Step: 5
Training loss: 2.860095739364624
Validation loss: 2.515999740169894

Epoch: 5| Step: 6
Training loss: 2.507951259613037
Validation loss: 2.5068732051439184

Epoch: 5| Step: 7
Training loss: 2.940901517868042
Validation loss: 2.504212292291785

Epoch: 5| Step: 8
Training loss: 2.707354784011841
Validation loss: 2.4975388050079346

Epoch: 5| Step: 9
Training loss: 2.2745542526245117
Validation loss: 2.496577198787402

Epoch: 5| Step: 10
Training loss: 2.8344576358795166
Validation loss: 2.494814944523637

Epoch: 95| Step: 0
Training loss: 3.0889618396759033
Validation loss: 2.492802184115174

Epoch: 5| Step: 1
Training loss: 2.5323734283447266
Validation loss: 2.493150411113616

Epoch: 5| Step: 2
Training loss: 2.5742862224578857
Validation loss: 2.496623008481918

Epoch: 5| Step: 3
Training loss: 2.19793701171875
Validation loss: 2.492882877267817

Epoch: 5| Step: 4
Training loss: 2.5260751247406006
Validation loss: 2.4912071151118123

Epoch: 5| Step: 5
Training loss: 2.062253952026367
Validation loss: 2.4953850802554878

Epoch: 5| Step: 6
Training loss: 2.5428855419158936
Validation loss: 2.500200953534854

Epoch: 5| Step: 7
Training loss: 3.3298964500427246
Validation loss: 2.4951790430212535

Epoch: 5| Step: 8
Training loss: 3.199225902557373
Validation loss: 2.494928067730319

Epoch: 5| Step: 9
Training loss: 2.653857469558716
Validation loss: 2.4972022066834154

Epoch: 5| Step: 10
Training loss: 2.9312679767608643
Validation loss: 2.502482706500638

Epoch: 96| Step: 0
Training loss: 1.8514362573623657
Validation loss: 2.503466647158387

Epoch: 5| Step: 1
Training loss: 2.6178908348083496
Validation loss: 2.503339731565086

Epoch: 5| Step: 2
Training loss: 3.111842155456543
Validation loss: 2.5061371762265443

Epoch: 5| Step: 3
Training loss: 3.314548969268799
Validation loss: 2.5030845134488997

Epoch: 5| Step: 4
Training loss: 2.5385141372680664
Validation loss: 2.4999212398323962

Epoch: 5| Step: 5
Training loss: 2.439622163772583
Validation loss: 2.4946397709590133

Epoch: 5| Step: 6
Training loss: 2.8892245292663574
Validation loss: 2.4924534623340895

Epoch: 5| Step: 7
Training loss: 2.4693949222564697
Validation loss: 2.49702033176217

Epoch: 5| Step: 8
Training loss: 2.848567485809326
Validation loss: 2.4967659160655034

Epoch: 5| Step: 9
Training loss: 2.9586596488952637
Validation loss: 2.498559687727241

Epoch: 5| Step: 10
Training loss: 2.51379132270813
Validation loss: 2.506349014979537

Epoch: 97| Step: 0
Training loss: 2.344125270843506
Validation loss: 2.508032268093478

Epoch: 5| Step: 1
Training loss: 2.9868481159210205
Validation loss: 2.5184317788770123

Epoch: 5| Step: 2
Training loss: 3.332089900970459
Validation loss: 2.5268432683842157

Epoch: 5| Step: 3
Training loss: 2.5656654834747314
Validation loss: 2.5335082187447497

Epoch: 5| Step: 4
Training loss: 2.6902034282684326
Validation loss: 2.523320622341607

Epoch: 5| Step: 5
Training loss: 2.352725028991699
Validation loss: 2.513485262470861

Epoch: 5| Step: 6
Training loss: 3.1488404273986816
Validation loss: 2.5014755110586844

Epoch: 5| Step: 7
Training loss: 2.3453497886657715
Validation loss: 2.4963994282548145

Epoch: 5| Step: 8
Training loss: 3.2544944286346436
Validation loss: 2.488932281412104

Epoch: 5| Step: 9
Training loss: 2.199796199798584
Validation loss: 2.490732721103135

Epoch: 5| Step: 10
Training loss: 2.213959217071533
Validation loss: 2.4913404039157334

Epoch: 98| Step: 0
Training loss: 2.4186606407165527
Validation loss: 2.498570554999895

Epoch: 5| Step: 1
Training loss: 2.659534215927124
Validation loss: 2.4994999003666702

Epoch: 5| Step: 2
Training loss: 2.91166353225708
Validation loss: 2.50015260839975

Epoch: 5| Step: 3
Training loss: 2.880180835723877
Validation loss: 2.4954620561292096

Epoch: 5| Step: 4
Training loss: 2.5987746715545654
Validation loss: 2.4940145451535463

Epoch: 5| Step: 5
Training loss: 2.5276286602020264
Validation loss: 2.4941282374884493

Epoch: 5| Step: 6
Training loss: 2.706033229827881
Validation loss: 2.495730310358027

Epoch: 5| Step: 7
Training loss: 2.5541303157806396
Validation loss: 2.4910580163360923

Epoch: 5| Step: 8
Training loss: 2.9767417907714844
Validation loss: 2.4890764913251324

Epoch: 5| Step: 9
Training loss: 2.312488079071045
Validation loss: 2.4960648193154285

Epoch: 5| Step: 10
Training loss: 3.184786081314087
Validation loss: 2.4907461648346274

Epoch: 99| Step: 0
Training loss: 2.2082552909851074
Validation loss: 2.5011189842736847

Epoch: 5| Step: 1
Training loss: 1.9162425994873047
Validation loss: 2.498544063619388

Epoch: 5| Step: 2
Training loss: 3.2454326152801514
Validation loss: 2.5056681427904355

Epoch: 5| Step: 3
Training loss: 2.8216240406036377
Validation loss: 2.50177401368336

Epoch: 5| Step: 4
Training loss: 1.879507303237915
Validation loss: 2.5041804826387795

Epoch: 5| Step: 5
Training loss: 2.252981662750244
Validation loss: 2.5111957288557485

Epoch: 5| Step: 6
Training loss: 2.4919629096984863
Validation loss: 2.5176391422107653

Epoch: 5| Step: 7
Training loss: 3.3469386100769043
Validation loss: 2.51938557881181

Epoch: 5| Step: 8
Training loss: 2.707930326461792
Validation loss: 2.506710260145126

Epoch: 5| Step: 9
Training loss: 3.5454025268554688
Validation loss: 2.5018109019084642

Epoch: 5| Step: 10
Training loss: 3.271033525466919
Validation loss: 2.4989204983557425

Epoch: 100| Step: 0
Training loss: 2.8074862957000732
Validation loss: 2.4968322682124313

Epoch: 5| Step: 1
Training loss: 2.766108989715576
Validation loss: 2.494324932816208

Epoch: 5| Step: 2
Training loss: 2.5979790687561035
Validation loss: 2.496976142288536

Epoch: 5| Step: 3
Training loss: 3.1524887084960938
Validation loss: 2.489077542417793

Epoch: 5| Step: 4
Training loss: 2.8437576293945312
Validation loss: 2.48716167993443

Epoch: 5| Step: 5
Training loss: 3.1415224075317383
Validation loss: 2.4883506708247687

Epoch: 5| Step: 6
Training loss: 2.07006573677063
Validation loss: 2.4885239626771662

Epoch: 5| Step: 7
Training loss: 2.4442660808563232
Validation loss: 2.487454832241099

Epoch: 5| Step: 8
Training loss: 2.8405003547668457
Validation loss: 2.4908179185723744

Epoch: 5| Step: 9
Training loss: 2.9027187824249268
Validation loss: 2.489840858726091

Epoch: 5| Step: 10
Training loss: 1.8858586549758911
Validation loss: 2.4909425089436192

Epoch: 101| Step: 0
Training loss: 2.0643999576568604
Validation loss: 2.4977848786179737

Epoch: 5| Step: 1
Training loss: 2.5752761363983154
Validation loss: 2.4939256406599477

Epoch: 5| Step: 2
Training loss: 2.8999292850494385
Validation loss: 2.4975935951355965

Epoch: 5| Step: 3
Training loss: 3.2705585956573486
Validation loss: 2.495365350477157

Epoch: 5| Step: 4
Training loss: 2.3700499534606934
Validation loss: 2.503085328686622

Epoch: 5| Step: 5
Training loss: 2.095614194869995
Validation loss: 2.497697917363977

Epoch: 5| Step: 6
Training loss: 2.4751391410827637
Validation loss: 2.4955322101551998

Epoch: 5| Step: 7
Training loss: 2.8116135597229004
Validation loss: 2.490712727269819

Epoch: 5| Step: 8
Training loss: 3.2887396812438965
Validation loss: 2.4814201221671155

Epoch: 5| Step: 9
Training loss: 2.7798705101013184
Validation loss: 2.4820464913563063

Epoch: 5| Step: 10
Training loss: 2.908928632736206
Validation loss: 2.483014181096067

Epoch: 102| Step: 0
Training loss: 2.400813341140747
Validation loss: 2.485226313273112

Epoch: 5| Step: 1
Training loss: 2.1387064456939697
Validation loss: 2.479940301628523

Epoch: 5| Step: 2
Training loss: 2.843461513519287
Validation loss: 2.4889362678732923

Epoch: 5| Step: 3
Training loss: 2.282118082046509
Validation loss: 2.4877260910567416

Epoch: 5| Step: 4
Training loss: 2.9209563732147217
Validation loss: 2.490936448497157

Epoch: 5| Step: 5
Training loss: 2.447227716445923
Validation loss: 2.485670746013682

Epoch: 5| Step: 6
Training loss: 3.06138277053833
Validation loss: 2.482128994439238

Epoch: 5| Step: 7
Training loss: 2.807461977005005
Validation loss: 2.483802333954842

Epoch: 5| Step: 8
Training loss: 3.3502860069274902
Validation loss: 2.47913001173286

Epoch: 5| Step: 9
Training loss: 2.571099042892456
Validation loss: 2.48021605706984

Epoch: 5| Step: 10
Training loss: 2.6096014976501465
Validation loss: 2.483655975710961

Epoch: 103| Step: 0
Training loss: 2.4957404136657715
Validation loss: 2.4830367667700655

Epoch: 5| Step: 1
Training loss: 2.481301784515381
Validation loss: 2.4797423603714153

Epoch: 5| Step: 2
Training loss: 2.1946167945861816
Validation loss: 2.4787563252192673

Epoch: 5| Step: 3
Training loss: 2.7667975425720215
Validation loss: 2.4773396086949173

Epoch: 5| Step: 4
Training loss: 2.834289073944092
Validation loss: 2.484367437260125

Epoch: 5| Step: 5
Training loss: 2.71846079826355
Validation loss: 2.4882050611639537

Epoch: 5| Step: 6
Training loss: 3.681537628173828
Validation loss: 2.493749983849064

Epoch: 5| Step: 7
Training loss: 2.5648531913757324
Validation loss: 2.5043219571472495

Epoch: 5| Step: 8
Training loss: 2.847627639770508
Validation loss: 2.500848517622999

Epoch: 5| Step: 9
Training loss: 2.285160541534424
Validation loss: 2.5032386420875468

Epoch: 5| Step: 10
Training loss: 2.5543222427368164
Validation loss: 2.4955580670346498

Epoch: 104| Step: 0
Training loss: 3.0149292945861816
Validation loss: 2.5017861294490036

Epoch: 5| Step: 1
Training loss: 2.464401960372925
Validation loss: 2.492844679022348

Epoch: 5| Step: 2
Training loss: 2.521254062652588
Validation loss: 2.4927849359409784

Epoch: 5| Step: 3
Training loss: 2.160508155822754
Validation loss: 2.4820438020972797

Epoch: 5| Step: 4
Training loss: 2.4305684566497803
Validation loss: 2.4844275751421527

Epoch: 5| Step: 5
Training loss: 2.844761848449707
Validation loss: 2.485822008502099

Epoch: 5| Step: 6
Training loss: 2.522958755493164
Validation loss: 2.4846595025831655

Epoch: 5| Step: 7
Training loss: 2.5723648071289062
Validation loss: 2.489853097546485

Epoch: 5| Step: 8
Training loss: 2.9592013359069824
Validation loss: 2.4932283791162635

Epoch: 5| Step: 9
Training loss: 3.230041980743408
Validation loss: 2.489110380090693

Epoch: 5| Step: 10
Training loss: 2.7047171592712402
Validation loss: 2.4882729976407942

Epoch: 105| Step: 0
Training loss: 2.3874974250793457
Validation loss: 2.487341493688604

Epoch: 5| Step: 1
Training loss: 3.2821803092956543
Validation loss: 2.4863616087103404

Epoch: 5| Step: 2
Training loss: 3.0362167358398438
Validation loss: 2.481957156171081

Epoch: 5| Step: 3
Training loss: 2.7712464332580566
Validation loss: 2.4861074724505023

Epoch: 5| Step: 4
Training loss: 2.0332424640655518
Validation loss: 2.478982353723177

Epoch: 5| Step: 5
Training loss: 2.706648588180542
Validation loss: 2.4785975948456795

Epoch: 5| Step: 6
Training loss: 3.029365062713623
Validation loss: 2.4778122337915565

Epoch: 5| Step: 7
Training loss: 2.203742504119873
Validation loss: 2.4776905223887455

Epoch: 5| Step: 8
Training loss: 2.8061320781707764
Validation loss: 2.4746869687111146

Epoch: 5| Step: 9
Training loss: 2.8067948818206787
Validation loss: 2.4742777783383607

Epoch: 5| Step: 10
Training loss: 2.245856523513794
Validation loss: 2.4768342920528945

Epoch: 106| Step: 0
Training loss: 2.5744693279266357
Validation loss: 2.4773262252089796

Epoch: 5| Step: 1
Training loss: 2.643428325653076
Validation loss: 2.4738162807239

Epoch: 5| Step: 2
Training loss: 2.995438575744629
Validation loss: 2.474838859291487

Epoch: 5| Step: 3
Training loss: 2.503127336502075
Validation loss: 2.4802691423764793

Epoch: 5| Step: 4
Training loss: 2.8304789066314697
Validation loss: 2.482262997217076

Epoch: 5| Step: 5
Training loss: 3.161306142807007
Validation loss: 2.4832350079731276

Epoch: 5| Step: 6
Training loss: 2.0616257190704346
Validation loss: 2.4864033857981362

Epoch: 5| Step: 7
Training loss: 3.0377938747406006
Validation loss: 2.4890409438840804

Epoch: 5| Step: 8
Training loss: 2.7653465270996094
Validation loss: 2.4848308204322733

Epoch: 5| Step: 9
Training loss: 2.5587315559387207
Validation loss: 2.477852888004754

Epoch: 5| Step: 10
Training loss: 2.218569278717041
Validation loss: 2.4776154769364225

Epoch: 107| Step: 0
Training loss: 1.9911003112792969
Validation loss: 2.472845967097949

Epoch: 5| Step: 1
Training loss: 3.2694904804229736
Validation loss: 2.473098608755296

Epoch: 5| Step: 2
Training loss: 2.2169580459594727
Validation loss: 2.4742426590252946

Epoch: 5| Step: 3
Training loss: 2.7739508152008057
Validation loss: 2.4730310952791603

Epoch: 5| Step: 4
Training loss: 2.8855814933776855
Validation loss: 2.4769264087882092

Epoch: 5| Step: 5
Training loss: 2.754560708999634
Validation loss: 2.476743600701773

Epoch: 5| Step: 6
Training loss: 2.707716703414917
Validation loss: 2.4742701168983214

Epoch: 5| Step: 7
Training loss: 3.2825324535369873
Validation loss: 2.486473716715331

Epoch: 5| Step: 8
Training loss: 2.736598014831543
Validation loss: 2.50481241749179

Epoch: 5| Step: 9
Training loss: 2.231663465499878
Validation loss: 2.501666902213968

Epoch: 5| Step: 10
Training loss: 2.572171211242676
Validation loss: 2.494489992818525

Epoch: 108| Step: 0
Training loss: 2.9767818450927734
Validation loss: 2.4832455701725458

Epoch: 5| Step: 1
Training loss: 3.1503260135650635
Validation loss: 2.4741496475793983

Epoch: 5| Step: 2
Training loss: 3.0671606063842773
Validation loss: 2.4764488666288313

Epoch: 5| Step: 3
Training loss: 2.9270877838134766
Validation loss: 2.4722324263664985

Epoch: 5| Step: 4
Training loss: 2.1244382858276367
Validation loss: 2.4735099090042936

Epoch: 5| Step: 5
Training loss: 2.4430947303771973
Validation loss: 2.470311792947913

Epoch: 5| Step: 6
Training loss: 2.7564430236816406
Validation loss: 2.4703974313633417

Epoch: 5| Step: 7
Training loss: 2.310497283935547
Validation loss: 2.4723566270643667

Epoch: 5| Step: 8
Training loss: 2.619516372680664
Validation loss: 2.4759539147858978

Epoch: 5| Step: 9
Training loss: 2.2121636867523193
Validation loss: 2.475371388978856

Epoch: 5| Step: 10
Training loss: 2.80279278755188
Validation loss: 2.4818747299973682

Epoch: 109| Step: 0
Training loss: 2.4206702709198
Validation loss: 2.4888047505450506

Epoch: 5| Step: 1
Training loss: 2.795647144317627
Validation loss: 2.510861283989363

Epoch: 5| Step: 2
Training loss: 3.3259453773498535
Validation loss: 2.519189960213118

Epoch: 5| Step: 3
Training loss: 3.173332691192627
Validation loss: 2.516991053858111

Epoch: 5| Step: 4
Training loss: 3.004802703857422
Validation loss: 2.490620187533799

Epoch: 5| Step: 5
Training loss: 2.5498340129852295
Validation loss: 2.4914013416536394

Epoch: 5| Step: 6
Training loss: 1.854776382446289
Validation loss: 2.482636620921473

Epoch: 5| Step: 7
Training loss: 2.4801111221313477
Validation loss: 2.484436797839339

Epoch: 5| Step: 8
Training loss: 2.9732978343963623
Validation loss: 2.479545990626017

Epoch: 5| Step: 9
Training loss: 2.0636825561523438
Validation loss: 2.4779121542489655

Epoch: 5| Step: 10
Training loss: 2.8893206119537354
Validation loss: 2.47010890899166

Epoch: 110| Step: 0
Training loss: 2.3219780921936035
Validation loss: 2.468011509987616

Epoch: 5| Step: 1
Training loss: 2.1609721183776855
Validation loss: 2.4717592859780915

Epoch: 5| Step: 2
Training loss: 2.8449087142944336
Validation loss: 2.474747650084957

Epoch: 5| Step: 3
Training loss: 2.449082374572754
Validation loss: 2.472328334726313

Epoch: 5| Step: 4
Training loss: 3.1249074935913086
Validation loss: 2.4719076464253087

Epoch: 5| Step: 5
Training loss: 2.69929575920105
Validation loss: 2.4734438721851637

Epoch: 5| Step: 6
Training loss: 2.533748149871826
Validation loss: 2.4728567420795398

Epoch: 5| Step: 7
Training loss: 2.379565715789795
Validation loss: 2.47286045423118

Epoch: 5| Step: 8
Training loss: 2.6208882331848145
Validation loss: 2.470002951160554

Epoch: 5| Step: 9
Training loss: 2.829869508743286
Validation loss: 2.4781717331178728

Epoch: 5| Step: 10
Training loss: 3.532766819000244
Validation loss: 2.482667097481348

Epoch: 111| Step: 0
Training loss: 2.185819149017334
Validation loss: 2.485478006383424

Epoch: 5| Step: 1
Training loss: 2.822031021118164
Validation loss: 2.4792382947860228

Epoch: 5| Step: 2
Training loss: 3.010532855987549
Validation loss: 2.478836326188939

Epoch: 5| Step: 3
Training loss: 2.5065293312072754
Validation loss: 2.481129031027517

Epoch: 5| Step: 4
Training loss: 2.2355854511260986
Validation loss: 2.4805070764275006

Epoch: 5| Step: 5
Training loss: 2.431832790374756
Validation loss: 2.482426510062269

Epoch: 5| Step: 6
Training loss: 2.5679373741149902
Validation loss: 2.473492451893386

Epoch: 5| Step: 7
Training loss: 3.0759308338165283
Validation loss: 2.473457710717314

Epoch: 5| Step: 8
Training loss: 2.9087302684783936
Validation loss: 2.466125862572783

Epoch: 5| Step: 9
Training loss: 3.073782444000244
Validation loss: 2.4680085925645727

Epoch: 5| Step: 10
Training loss: 2.51399827003479
Validation loss: 2.468391820948611

Epoch: 112| Step: 0
Training loss: 2.890540838241577
Validation loss: 2.467790442128335

Epoch: 5| Step: 1
Training loss: 2.997126817703247
Validation loss: 2.4647343004903486

Epoch: 5| Step: 2
Training loss: 2.933945894241333
Validation loss: 2.4643646517107562

Epoch: 5| Step: 3
Training loss: 1.7942358255386353
Validation loss: 2.464717093334403

Epoch: 5| Step: 4
Training loss: 3.2413032054901123
Validation loss: 2.4664406468791347

Epoch: 5| Step: 5
Training loss: 2.5271003246307373
Validation loss: 2.461525386379611

Epoch: 5| Step: 6
Training loss: 2.739414930343628
Validation loss: 2.461399139896516

Epoch: 5| Step: 7
Training loss: 2.554291248321533
Validation loss: 2.46708688171961

Epoch: 5| Step: 8
Training loss: 2.4545352458953857
Validation loss: 2.4723846886747625

Epoch: 5| Step: 9
Training loss: 2.5797886848449707
Validation loss: 2.4779004666113083

Epoch: 5| Step: 10
Training loss: 2.6252198219299316
Validation loss: 2.4823884271806285

Epoch: 113| Step: 0
Training loss: 2.6547799110412598
Validation loss: 2.48787623067056

Epoch: 5| Step: 1
Training loss: 2.8953733444213867
Validation loss: 2.48776816039957

Epoch: 5| Step: 2
Training loss: 2.2610673904418945
Validation loss: 2.480104866848197

Epoch: 5| Step: 3
Training loss: 2.683476209640503
Validation loss: 2.4719746753733647

Epoch: 5| Step: 4
Training loss: 2.4653334617614746
Validation loss: 2.4687737059849564

Epoch: 5| Step: 5
Training loss: 2.864304780960083
Validation loss: 2.4664621609513477

Epoch: 5| Step: 6
Training loss: 2.793149471282959
Validation loss: 2.4647590575679654

Epoch: 5| Step: 7
Training loss: 2.5306122303009033
Validation loss: 2.4647265736774733

Epoch: 5| Step: 8
Training loss: 2.1010050773620605
Validation loss: 2.4609829725757724

Epoch: 5| Step: 9
Training loss: 2.935563802719116
Validation loss: 2.463030792051746

Epoch: 5| Step: 10
Training loss: 3.275179862976074
Validation loss: 2.4600133126781834

Epoch: 114| Step: 0
Training loss: 2.7353856563568115
Validation loss: 2.458997828986055

Epoch: 5| Step: 1
Training loss: 3.121919870376587
Validation loss: 2.4590452614650933

Epoch: 5| Step: 2
Training loss: 2.2093989849090576
Validation loss: 2.463245161118046

Epoch: 5| Step: 3
Training loss: 1.7557026147842407
Validation loss: 2.461423502173475

Epoch: 5| Step: 4
Training loss: 2.9437174797058105
Validation loss: 2.4632673314822617

Epoch: 5| Step: 5
Training loss: 2.5825307369232178
Validation loss: 2.463266490608133

Epoch: 5| Step: 6
Training loss: 2.6948864459991455
Validation loss: 2.4672417512504

Epoch: 5| Step: 7
Training loss: 3.1679835319519043
Validation loss: 2.4737329226668163

Epoch: 5| Step: 8
Training loss: 2.2652790546417236
Validation loss: 2.4938852402471725

Epoch: 5| Step: 9
Training loss: 2.93645977973938
Validation loss: 2.5018759799259964

Epoch: 5| Step: 10
Training loss: 2.926145553588867
Validation loss: 2.491107035708684

Epoch: 115| Step: 0
Training loss: 1.712022066116333
Validation loss: 2.4894419562432075

Epoch: 5| Step: 1
Training loss: 2.496349811553955
Validation loss: 2.4720442064346804

Epoch: 5| Step: 2
Training loss: 2.616381883621216
Validation loss: 2.4668951496001212

Epoch: 5| Step: 3
Training loss: 3.109215497970581
Validation loss: 2.46471006126814

Epoch: 5| Step: 4
Training loss: 3.7478625774383545
Validation loss: 2.462437686099801

Epoch: 5| Step: 5
Training loss: 2.475828170776367
Validation loss: 2.4634140691449566

Epoch: 5| Step: 6
Training loss: 2.0197558403015137
Validation loss: 2.464556914503856

Epoch: 5| Step: 7
Training loss: 2.4235639572143555
Validation loss: 2.4613803766107045

Epoch: 5| Step: 8
Training loss: 2.762659788131714
Validation loss: 2.4653750965672154

Epoch: 5| Step: 9
Training loss: 2.907278537750244
Validation loss: 2.463310008407921

Epoch: 5| Step: 10
Training loss: 3.1751506328582764
Validation loss: 2.471165441697644

Epoch: 116| Step: 0
Training loss: 3.017630100250244
Validation loss: 2.4861783955686834

Epoch: 5| Step: 1
Training loss: 2.495933771133423
Validation loss: 2.487482081177414

Epoch: 5| Step: 2
Training loss: 3.100583553314209
Validation loss: 2.495803484352686

Epoch: 5| Step: 3
Training loss: 2.533184289932251
Validation loss: 2.4942348336660736

Epoch: 5| Step: 4
Training loss: 2.1113457679748535
Validation loss: 2.4901919467474825

Epoch: 5| Step: 5
Training loss: 2.19291090965271
Validation loss: 2.4847116547246135

Epoch: 5| Step: 6
Training loss: 2.7549705505371094
Validation loss: 2.4751991277099936

Epoch: 5| Step: 7
Training loss: 2.5766921043395996
Validation loss: 2.477947476089642

Epoch: 5| Step: 8
Training loss: 3.040579080581665
Validation loss: 2.470323142185006

Epoch: 5| Step: 9
Training loss: 3.0910820960998535
Validation loss: 2.466113018733199

Epoch: 5| Step: 10
Training loss: 2.305389165878296
Validation loss: 2.4746512495061403

Epoch: 117| Step: 0
Training loss: 2.9969260692596436
Validation loss: 2.4824795312778924

Epoch: 5| Step: 1
Training loss: 2.717031478881836
Validation loss: 2.4963019663287747

Epoch: 5| Step: 2
Training loss: 2.8852860927581787
Validation loss: 2.477575804597588

Epoch: 5| Step: 3
Training loss: 2.458681583404541
Validation loss: 2.468076403423022

Epoch: 5| Step: 4
Training loss: 3.0809664726257324
Validation loss: 2.4580840705543436

Epoch: 5| Step: 5
Training loss: 3.000775098800659
Validation loss: 2.444564139971169

Epoch: 5| Step: 6
Training loss: 2.3778443336486816
Validation loss: 2.445698271515549

Epoch: 5| Step: 7
Training loss: 2.966499090194702
Validation loss: 2.44562933521886

Epoch: 5| Step: 8
Training loss: 2.4476771354675293
Validation loss: 2.44746728609967

Epoch: 5| Step: 9
Training loss: 2.244058609008789
Validation loss: 2.450120028629098

Epoch: 5| Step: 10
Training loss: 2.0815770626068115
Validation loss: 2.444908321544688

Epoch: 118| Step: 0
Training loss: 2.136401653289795
Validation loss: 2.44998824468223

Epoch: 5| Step: 1
Training loss: 3.1820321083068848
Validation loss: 2.445697174277357

Epoch: 5| Step: 2
Training loss: 2.8852720260620117
Validation loss: 2.4434876672683226

Epoch: 5| Step: 3
Training loss: 1.8468666076660156
Validation loss: 2.443693212283555

Epoch: 5| Step: 4
Training loss: 2.0109784603118896
Validation loss: 2.4468237507727837

Epoch: 5| Step: 5
Training loss: 2.8480629920959473
Validation loss: 2.4416314837753132

Epoch: 5| Step: 6
Training loss: 3.2536416053771973
Validation loss: 2.4503310059988372

Epoch: 5| Step: 7
Training loss: 3.3434600830078125
Validation loss: 2.453520426186182

Epoch: 5| Step: 8
Training loss: 2.535454273223877
Validation loss: 2.4522402632621025

Epoch: 5| Step: 9
Training loss: 2.2304606437683105
Validation loss: 2.4622731542074554

Epoch: 5| Step: 10
Training loss: 3.0805957317352295
Validation loss: 2.4694384131380307

Epoch: 119| Step: 0
Training loss: 1.8968197107315063
Validation loss: 2.4701259623291674

Epoch: 5| Step: 1
Training loss: 2.204845428466797
Validation loss: 2.4735410482652727

Epoch: 5| Step: 2
Training loss: 2.983290910720825
Validation loss: 2.4742628323134555

Epoch: 5| Step: 3
Training loss: 3.1670384407043457
Validation loss: 2.4663389062368744

Epoch: 5| Step: 4
Training loss: 2.5081024169921875
Validation loss: 2.4577440523332164

Epoch: 5| Step: 5
Training loss: 3.0835094451904297
Validation loss: 2.4455851944543983

Epoch: 5| Step: 6
Training loss: 2.6353626251220703
Validation loss: 2.440644774385678

Epoch: 5| Step: 7
Training loss: 2.516780138015747
Validation loss: 2.435218741816859

Epoch: 5| Step: 8
Training loss: 2.744224786758423
Validation loss: 2.4322203641296714

Epoch: 5| Step: 9
Training loss: 3.1166234016418457
Validation loss: 2.435970244869109

Epoch: 5| Step: 10
Training loss: 2.2799179553985596
Validation loss: 2.4363781175305768

Epoch: 120| Step: 0
Training loss: 2.7139732837677
Validation loss: 2.439789746397285

Epoch: 5| Step: 1
Training loss: 2.9198784828186035
Validation loss: 2.4432066281636557

Epoch: 5| Step: 2
Training loss: 3.3311076164245605
Validation loss: 2.4440160182214554

Epoch: 5| Step: 3
Training loss: 2.9524617195129395
Validation loss: 2.4450370650137625

Epoch: 5| Step: 4
Training loss: 2.6406610012054443
Validation loss: 2.445635759702293

Epoch: 5| Step: 5
Training loss: 2.6635594367980957
Validation loss: 2.442335667148713

Epoch: 5| Step: 6
Training loss: 2.1071419715881348
Validation loss: 2.4395651842958186

Epoch: 5| Step: 7
Training loss: 2.355954647064209
Validation loss: 2.438846493280062

Epoch: 5| Step: 8
Training loss: 2.4021494388580322
Validation loss: 2.4337574256363737

Epoch: 5| Step: 9
Training loss: 2.4770989418029785
Validation loss: 2.4305768371910177

Epoch: 5| Step: 10
Training loss: 2.8179941177368164
Validation loss: 2.4289072354634604

Epoch: 121| Step: 0
Training loss: 2.9992077350616455
Validation loss: 2.4356953661928893

Epoch: 5| Step: 1
Training loss: 2.3529019355773926
Validation loss: 2.4406957062341834

Epoch: 5| Step: 2
Training loss: 3.0141139030456543
Validation loss: 2.4511480664694183

Epoch: 5| Step: 3
Training loss: 2.9273688793182373
Validation loss: 2.459307416792839

Epoch: 5| Step: 4
Training loss: 2.9112229347229004
Validation loss: 2.465310432577646

Epoch: 5| Step: 5
Training loss: 2.620620012283325
Validation loss: 2.4598807545118433

Epoch: 5| Step: 6
Training loss: 2.855818271636963
Validation loss: 2.4538347836463683

Epoch: 5| Step: 7
Training loss: 2.716735363006592
Validation loss: 2.4454190782321397

Epoch: 5| Step: 8
Training loss: 2.155604600906372
Validation loss: 2.4373745149181736

Epoch: 5| Step: 9
Training loss: 2.743701934814453
Validation loss: 2.4354031919151224

Epoch: 5| Step: 10
Training loss: 1.8843024969100952
Validation loss: 2.431345103889383

Epoch: 122| Step: 0
Training loss: 2.2947399616241455
Validation loss: 2.4313087642833753

Epoch: 5| Step: 1
Training loss: 2.616257429122925
Validation loss: 2.432525865493282

Epoch: 5| Step: 2
Training loss: 2.987544536590576
Validation loss: 2.4371155897776284

Epoch: 5| Step: 3
Training loss: 2.434457302093506
Validation loss: 2.4386920031680854

Epoch: 5| Step: 4
Training loss: 2.951246738433838
Validation loss: 2.4462640849492883

Epoch: 5| Step: 5
Training loss: 2.5134925842285156
Validation loss: 2.4400801299720682

Epoch: 5| Step: 6
Training loss: 2.5916028022766113
Validation loss: 2.4477370016036497

Epoch: 5| Step: 7
Training loss: 2.540931463241577
Validation loss: 2.4429773976725917

Epoch: 5| Step: 8
Training loss: 2.9163618087768555
Validation loss: 2.4367397318604174

Epoch: 5| Step: 9
Training loss: 3.2363669872283936
Validation loss: 2.432926211305844

Epoch: 5| Step: 10
Training loss: 2.174264907836914
Validation loss: 2.43080408342423

Epoch: 123| Step: 0
Training loss: 2.99039888381958
Validation loss: 2.4274364453490063

Epoch: 5| Step: 1
Training loss: 2.9580371379852295
Validation loss: 2.4268360907031643

Epoch: 5| Step: 2
Training loss: 2.223299026489258
Validation loss: 2.4243691890470442

Epoch: 5| Step: 3
Training loss: 2.331895351409912
Validation loss: 2.426584220701648

Epoch: 5| Step: 4
Training loss: 2.762620449066162
Validation loss: 2.432325860505463

Epoch: 5| Step: 5
Training loss: 2.4594030380249023
Validation loss: 2.4342615450582197

Epoch: 5| Step: 6
Training loss: 2.5826029777526855
Validation loss: 2.4355160779850458

Epoch: 5| Step: 7
Training loss: 2.467275619506836
Validation loss: 2.441532855392784

Epoch: 5| Step: 8
Training loss: 2.8210911750793457
Validation loss: 2.4377331964431272

Epoch: 5| Step: 9
Training loss: 2.1841721534729004
Validation loss: 2.42875930570787

Epoch: 5| Step: 10
Training loss: 3.423549175262451
Validation loss: 2.4272658901829876

Epoch: 124| Step: 0
Training loss: 2.647597312927246
Validation loss: 2.4268093237312893

Epoch: 5| Step: 1
Training loss: 2.51981258392334
Validation loss: 2.430231219978743

Epoch: 5| Step: 2
Training loss: 2.087595224380493
Validation loss: 2.42996024316357

Epoch: 5| Step: 3
Training loss: 3.247570037841797
Validation loss: 2.4251827296390327

Epoch: 5| Step: 4
Training loss: 1.9481134414672852
Validation loss: 2.423197143821306

Epoch: 5| Step: 5
Training loss: 2.8874216079711914
Validation loss: 2.4248875366744174

Epoch: 5| Step: 6
Training loss: 2.962080478668213
Validation loss: 2.422187885930461

Epoch: 5| Step: 7
Training loss: 2.7139573097229004
Validation loss: 2.4236550228570097

Epoch: 5| Step: 8
Training loss: 2.938072919845581
Validation loss: 2.4236019093503236

Epoch: 5| Step: 9
Training loss: 2.4520249366760254
Validation loss: 2.4268890555186937

Epoch: 5| Step: 10
Training loss: 2.7888338565826416
Validation loss: 2.425166445393716

Epoch: 125| Step: 0
Training loss: 2.2444729804992676
Validation loss: 2.4251191487876316

Epoch: 5| Step: 1
Training loss: 3.2566826343536377
Validation loss: 2.426482113458777

Epoch: 5| Step: 2
Training loss: 3.1045544147491455
Validation loss: 2.4265650805606636

Epoch: 5| Step: 3
Training loss: 1.8016945123672485
Validation loss: 2.4278773159109135

Epoch: 5| Step: 4
Training loss: 2.44049072265625
Validation loss: 2.4273016837335404

Epoch: 5| Step: 5
Training loss: 3.0750668048858643
Validation loss: 2.4255220044043755

Epoch: 5| Step: 6
Training loss: 2.507150173187256
Validation loss: 2.4276113433222615

Epoch: 5| Step: 7
Training loss: 2.1089987754821777
Validation loss: 2.427176517824973

Epoch: 5| Step: 8
Training loss: 2.4317712783813477
Validation loss: 2.4366343226484073

Epoch: 5| Step: 9
Training loss: 3.318808078765869
Validation loss: 2.440812026300738

Epoch: 5| Step: 10
Training loss: 2.8508453369140625
Validation loss: 2.45343743729335

Epoch: 126| Step: 0
Training loss: 2.7678990364074707
Validation loss: 2.4551300617956344

Epoch: 5| Step: 1
Training loss: 2.2337770462036133
Validation loss: 2.455253506219515

Epoch: 5| Step: 2
Training loss: 2.470891237258911
Validation loss: 2.4517719771272395

Epoch: 5| Step: 3
Training loss: 2.931859254837036
Validation loss: 2.4398839191723893

Epoch: 5| Step: 4
Training loss: 2.807363986968994
Validation loss: 2.439451530415525

Epoch: 5| Step: 5
Training loss: 2.8435769081115723
Validation loss: 2.4277985211341613

Epoch: 5| Step: 6
Training loss: 2.820967197418213
Validation loss: 2.4304224701337915

Epoch: 5| Step: 7
Training loss: 2.1297690868377686
Validation loss: 2.430199815380958

Epoch: 5| Step: 8
Training loss: 2.538755178451538
Validation loss: 2.4288754335013767

Epoch: 5| Step: 9
Training loss: 2.966022491455078
Validation loss: 2.4310135738824004

Epoch: 5| Step: 10
Training loss: 2.5544955730438232
Validation loss: 2.4348385769833802

Epoch: 127| Step: 0
Training loss: 2.315310478210449
Validation loss: 2.450318092940956

Epoch: 5| Step: 1
Training loss: 2.459132671356201
Validation loss: 2.455225219008743

Epoch: 5| Step: 2
Training loss: 2.9686551094055176
Validation loss: 2.474283633693572

Epoch: 5| Step: 3
Training loss: 3.2800910472869873
Validation loss: 2.4678386539541264

Epoch: 5| Step: 4
Training loss: 2.8644556999206543
Validation loss: 2.4579137268886773

Epoch: 5| Step: 5
Training loss: 2.6697564125061035
Validation loss: 2.438789898349393

Epoch: 5| Step: 6
Training loss: 2.362220287322998
Validation loss: 2.4273864556384344

Epoch: 5| Step: 7
Training loss: 3.2045962810516357
Validation loss: 2.427836013096635

Epoch: 5| Step: 8
Training loss: 2.0623159408569336
Validation loss: 2.4176464875539145

Epoch: 5| Step: 9
Training loss: 2.339855194091797
Validation loss: 2.4170429450209423

Epoch: 5| Step: 10
Training loss: 2.6094069480895996
Validation loss: 2.4142024145331433

Epoch: 128| Step: 0
Training loss: 2.185253620147705
Validation loss: 2.4214163416175434

Epoch: 5| Step: 1
Training loss: 2.2893989086151123
Validation loss: 2.427185789231331

Epoch: 5| Step: 2
Training loss: 3.1979641914367676
Validation loss: 2.4229535287426365

Epoch: 5| Step: 3
Training loss: 3.216024875640869
Validation loss: 2.4263956700601885

Epoch: 5| Step: 4
Training loss: 2.339566469192505
Validation loss: 2.4186352376014955

Epoch: 5| Step: 5
Training loss: 2.7972254753112793
Validation loss: 2.4207309984391734

Epoch: 5| Step: 6
Training loss: 2.3988263607025146
Validation loss: 2.4193961030693463

Epoch: 5| Step: 7
Training loss: 3.0460078716278076
Validation loss: 2.4289133036008446

Epoch: 5| Step: 8
Training loss: 2.7536675930023193
Validation loss: 2.43336739847737

Epoch: 5| Step: 9
Training loss: 2.5515336990356445
Validation loss: 2.4477860440490065

Epoch: 5| Step: 10
Training loss: 2.2815449237823486
Validation loss: 2.4392739342105005

Epoch: 129| Step: 0
Training loss: 1.9599990844726562
Validation loss: 2.4609016397947907

Epoch: 5| Step: 1
Training loss: 2.4739015102386475
Validation loss: 2.476713547142603

Epoch: 5| Step: 2
Training loss: 3.1442182064056396
Validation loss: 2.451307322389336

Epoch: 5| Step: 3
Training loss: 2.279625415802002
Validation loss: 2.435833208022579

Epoch: 5| Step: 4
Training loss: 2.7712597846984863
Validation loss: 2.425784844224171

Epoch: 5| Step: 5
Training loss: 2.519550085067749
Validation loss: 2.427744911563012

Epoch: 5| Step: 6
Training loss: 2.6113243103027344
Validation loss: 2.425716766747095

Epoch: 5| Step: 7
Training loss: 3.1517679691314697
Validation loss: 2.4256656169891357

Epoch: 5| Step: 8
Training loss: 2.441248655319214
Validation loss: 2.4291922353929087

Epoch: 5| Step: 9
Training loss: 2.9684898853302
Validation loss: 2.4295162436782674

Epoch: 5| Step: 10
Training loss: 2.87156343460083
Validation loss: 2.4333700236453804

Epoch: 130| Step: 0
Training loss: 2.827923059463501
Validation loss: 2.4335683084303334

Epoch: 5| Step: 1
Training loss: 2.4203197956085205
Validation loss: 2.4328307977286716

Epoch: 5| Step: 2
Training loss: 2.140407085418701
Validation loss: 2.4273470319727415

Epoch: 5| Step: 3
Training loss: 3.099733829498291
Validation loss: 2.4274431761874946

Epoch: 5| Step: 4
Training loss: 3.0916097164154053
Validation loss: 2.420894835584907

Epoch: 5| Step: 5
Training loss: 3.1435298919677734
Validation loss: 2.4209693554908998

Epoch: 5| Step: 6
Training loss: 2.8119235038757324
Validation loss: 2.4177888542093258

Epoch: 5| Step: 7
Training loss: 2.5715298652648926
Validation loss: 2.416575301078058

Epoch: 5| Step: 8
Training loss: 2.4734597206115723
Validation loss: 2.4165497825991724

Epoch: 5| Step: 9
Training loss: 2.4292142391204834
Validation loss: 2.4220275750724216

Epoch: 5| Step: 10
Training loss: 2.151076316833496
Validation loss: 2.42689831795231

Epoch: 131| Step: 0
Training loss: 2.5540409088134766
Validation loss: 2.430317783868441

Epoch: 5| Step: 1
Training loss: 3.058715343475342
Validation loss: 2.4315669254590104

Epoch: 5| Step: 2
Training loss: 1.8211910724639893
Validation loss: 2.4401270394684165

Epoch: 5| Step: 3
Training loss: 2.9025638103485107
Validation loss: 2.4388002118756695

Epoch: 5| Step: 4
Training loss: 2.416330575942993
Validation loss: 2.4538944844276673

Epoch: 5| Step: 5
Training loss: 2.961254835128784
Validation loss: 2.445506541959701

Epoch: 5| Step: 6
Training loss: 2.6461739540100098
Validation loss: 2.448839241458524

Epoch: 5| Step: 7
Training loss: 3.061203718185425
Validation loss: 2.4354824532744703

Epoch: 5| Step: 8
Training loss: 2.411510944366455
Validation loss: 2.430426570676988

Epoch: 5| Step: 9
Training loss: 2.999985456466675
Validation loss: 2.41607734721194

Epoch: 5| Step: 10
Training loss: 2.2127912044525146
Validation loss: 2.4152434154223372

Epoch: 132| Step: 0
Training loss: 2.706465482711792
Validation loss: 2.416983301921557

Epoch: 5| Step: 1
Training loss: 2.110384225845337
Validation loss: 2.419124105925201

Epoch: 5| Step: 2
Training loss: 2.842231512069702
Validation loss: 2.420331173045661

Epoch: 5| Step: 3
Training loss: 2.4215922355651855
Validation loss: 2.417487226506715

Epoch: 5| Step: 4
Training loss: 3.6335043907165527
Validation loss: 2.427211553819718

Epoch: 5| Step: 5
Training loss: 3.312955141067505
Validation loss: 2.425327957317393

Epoch: 5| Step: 6
Training loss: 2.276151657104492
Validation loss: 2.4265506267547607

Epoch: 5| Step: 7
Training loss: 2.219343900680542
Validation loss: 2.4142101733915267

Epoch: 5| Step: 8
Training loss: 2.712968349456787
Validation loss: 2.41428873103152

Epoch: 5| Step: 9
Training loss: 2.9610750675201416
Validation loss: 2.4136836964596986

Epoch: 5| Step: 10
Training loss: 1.846968412399292
Validation loss: 2.4077056069527902

Epoch: 133| Step: 0
Training loss: 2.7708258628845215
Validation loss: 2.414170488234489

Epoch: 5| Step: 1
Training loss: 2.5106122493743896
Validation loss: 2.415748929464689

Epoch: 5| Step: 2
Training loss: 2.676562786102295
Validation loss: 2.413266417800739

Epoch: 5| Step: 3
Training loss: 2.3637518882751465
Validation loss: 2.4194876045309086

Epoch: 5| Step: 4
Training loss: 2.3383371829986572
Validation loss: 2.420947213326731

Epoch: 5| Step: 5
Training loss: 2.867218255996704
Validation loss: 2.423675544800297

Epoch: 5| Step: 6
Training loss: 2.9267184734344482
Validation loss: 2.420401660344934

Epoch: 5| Step: 7
Training loss: 2.6791882514953613
Validation loss: 2.4272068956846833

Epoch: 5| Step: 8
Training loss: 2.3341259956359863
Validation loss: 2.422457374552245

Epoch: 5| Step: 9
Training loss: 2.6830601692199707
Validation loss: 2.425741041860273

Epoch: 5| Step: 10
Training loss: 2.788456439971924
Validation loss: 2.423751979745844

Epoch: 134| Step: 0
Training loss: 3.025280475616455
Validation loss: 2.4285688887360277

Epoch: 5| Step: 1
Training loss: 2.723595142364502
Validation loss: 2.430943868493521

Epoch: 5| Step: 2
Training loss: 1.7982285022735596
Validation loss: 2.4268025685382146

Epoch: 5| Step: 3
Training loss: 3.1543209552764893
Validation loss: 2.4298036252298663

Epoch: 5| Step: 4
Training loss: 2.389571189880371
Validation loss: 2.4278683790596585

Epoch: 5| Step: 5
Training loss: 2.758054256439209
Validation loss: 2.43446014260733

Epoch: 5| Step: 6
Training loss: 2.5985586643218994
Validation loss: 2.4300365012179137

Epoch: 5| Step: 7
Training loss: 2.752159595489502
Validation loss: 2.4294528089543825

Epoch: 5| Step: 8
Training loss: 2.579857110977173
Validation loss: 2.4207157217046267

Epoch: 5| Step: 9
Training loss: 2.485502243041992
Validation loss: 2.4193595506811656

Epoch: 5| Step: 10
Training loss: 2.7084426879882812
Validation loss: 2.4153332556447675

Epoch: 135| Step: 0
Training loss: 1.8521963357925415
Validation loss: 2.4094929925857054

Epoch: 5| Step: 1
Training loss: 2.7663581371307373
Validation loss: 2.4047900758763796

Epoch: 5| Step: 2
Training loss: 2.3563480377197266
Validation loss: 2.4027037415453183

Epoch: 5| Step: 3
Training loss: 2.7533884048461914
Validation loss: 2.4019687611569642

Epoch: 5| Step: 4
Training loss: 3.135702610015869
Validation loss: 2.4019705505781275

Epoch: 5| Step: 5
Training loss: 2.9070839881896973
Validation loss: 2.4009967901373424

Epoch: 5| Step: 6
Training loss: 2.3859896659851074
Validation loss: 2.4041518165219213

Epoch: 5| Step: 7
Training loss: 2.972424030303955
Validation loss: 2.4082908233006797

Epoch: 5| Step: 8
Training loss: 2.254559278488159
Validation loss: 2.4256004005350094

Epoch: 5| Step: 9
Training loss: 2.8653762340545654
Validation loss: 2.4344562548463062

Epoch: 5| Step: 10
Training loss: 2.75250506401062
Validation loss: 2.449237505594889

Epoch: 136| Step: 0
Training loss: 3.0062098503112793
Validation loss: 2.4468385634883756

Epoch: 5| Step: 1
Training loss: 2.9107441902160645
Validation loss: 2.44549395192054

Epoch: 5| Step: 2
Training loss: 2.1052942276000977
Validation loss: 2.4256496813989457

Epoch: 5| Step: 3
Training loss: 3.081986665725708
Validation loss: 2.419232504342192

Epoch: 5| Step: 4
Training loss: 2.113832950592041
Validation loss: 2.4020803769429526

Epoch: 5| Step: 5
Training loss: 2.3587872982025146
Validation loss: 2.4031694909577728

Epoch: 5| Step: 6
Training loss: 1.8828020095825195
Validation loss: 2.4077366295681206

Epoch: 5| Step: 7
Training loss: 2.549711227416992
Validation loss: 2.4091336214414207

Epoch: 5| Step: 8
Training loss: 3.059077739715576
Validation loss: 2.41322410234841

Epoch: 5| Step: 9
Training loss: 2.9932971000671387
Validation loss: 2.4150690160771853

Epoch: 5| Step: 10
Training loss: 3.1232926845550537
Validation loss: 2.416690859743344

Epoch: 137| Step: 0
Training loss: 2.675539970397949
Validation loss: 2.414274625880744

Epoch: 5| Step: 1
Training loss: 2.0461313724517822
Validation loss: 2.415230053727345

Epoch: 5| Step: 2
Training loss: 2.3692474365234375
Validation loss: 2.410739298789732

Epoch: 5| Step: 3
Training loss: 2.526824712753296
Validation loss: 2.417085806528727

Epoch: 5| Step: 4
Training loss: 2.80617094039917
Validation loss: 2.412814309520106

Epoch: 5| Step: 5
Training loss: 2.789400815963745
Validation loss: 2.4141809376337195

Epoch: 5| Step: 6
Training loss: 3.091451644897461
Validation loss: 2.416195043953516

Epoch: 5| Step: 7
Training loss: 2.8605568408966064
Validation loss: 2.419989232094057

Epoch: 5| Step: 8
Training loss: 2.3946146965026855
Validation loss: 2.4215158877834195

Epoch: 5| Step: 9
Training loss: 2.913180112838745
Validation loss: 2.422589571245255

Epoch: 5| Step: 10
Training loss: 2.6448521614074707
Validation loss: 2.424616713677683

Epoch: 138| Step: 0
Training loss: 3.011805772781372
Validation loss: 2.4227751685727026

Epoch: 5| Step: 1
Training loss: 2.3370521068573
Validation loss: 2.423638218192644

Epoch: 5| Step: 2
Training loss: 2.929513454437256
Validation loss: 2.4155090162830968

Epoch: 5| Step: 3
Training loss: 2.988185167312622
Validation loss: 2.411802431588532

Epoch: 5| Step: 4
Training loss: 2.39921236038208
Validation loss: 2.411682362197548

Epoch: 5| Step: 5
Training loss: 2.8683578968048096
Validation loss: 2.402103013889764

Epoch: 5| Step: 6
Training loss: 2.020700693130493
Validation loss: 2.406587836562946

Epoch: 5| Step: 7
Training loss: 2.0521790981292725
Validation loss: 2.407342562111475

Epoch: 5| Step: 8
Training loss: 3.2425308227539062
Validation loss: 2.4158772396784958

Epoch: 5| Step: 9
Training loss: 2.8339929580688477
Validation loss: 2.407368201081471

Epoch: 5| Step: 10
Training loss: 2.254983901977539
Validation loss: 2.421394371217297

Epoch: 139| Step: 0
Training loss: 2.6289188861846924
Validation loss: 2.417758198194606

Epoch: 5| Step: 1
Training loss: 2.9591147899627686
Validation loss: 2.424106503045687

Epoch: 5| Step: 2
Training loss: 2.94488525390625
Validation loss: 2.423749062322801

Epoch: 5| Step: 3
Training loss: 2.812480926513672
Validation loss: 2.4137121400525494

Epoch: 5| Step: 4
Training loss: 2.3150627613067627
Validation loss: 2.4144101245428926

Epoch: 5| Step: 5
Training loss: 2.3366246223449707
Validation loss: 2.4174815198426605

Epoch: 5| Step: 6
Training loss: 2.633685827255249
Validation loss: 2.4248229226758404

Epoch: 5| Step: 7
Training loss: 2.664731502532959
Validation loss: 2.4109494275944208

Epoch: 5| Step: 8
Training loss: 3.1613831520080566
Validation loss: 2.4074686675943355

Epoch: 5| Step: 9
Training loss: 2.179456949234009
Validation loss: 2.4035843597945346

Epoch: 5| Step: 10
Training loss: 2.3123440742492676
Validation loss: 2.4100399068606797

Epoch: 140| Step: 0
Training loss: 2.558133840560913
Validation loss: 2.4072742308339765

Epoch: 5| Step: 1
Training loss: 2.7320055961608887
Validation loss: 2.4097514562709357

Epoch: 5| Step: 2
Training loss: 2.0343971252441406
Validation loss: 2.4082600685857956

Epoch: 5| Step: 3
Training loss: 2.7444331645965576
Validation loss: 2.4136633616621777

Epoch: 5| Step: 4
Training loss: 2.771087169647217
Validation loss: 2.4217160952988492

Epoch: 5| Step: 5
Training loss: 2.4636330604553223
Validation loss: 2.4162899832571707

Epoch: 5| Step: 6
Training loss: 2.1562657356262207
Validation loss: 2.4188190583259828

Epoch: 5| Step: 7
Training loss: 2.543184280395508
Validation loss: 2.4136785127783336

Epoch: 5| Step: 8
Training loss: 3.300292491912842
Validation loss: 2.408125391570471

Epoch: 5| Step: 9
Training loss: 2.8667571544647217
Validation loss: 2.4078605713382846

Epoch: 5| Step: 10
Training loss: 2.74459171295166
Validation loss: 2.4070618511528097

Epoch: 141| Step: 0
Training loss: 3.2387802600860596
Validation loss: 2.40778499521235

Epoch: 5| Step: 1
Training loss: 2.0018370151519775
Validation loss: 2.406866611972932

Epoch: 5| Step: 2
Training loss: 2.4689459800720215
Validation loss: 2.4087280355474

Epoch: 5| Step: 3
Training loss: 2.5332045555114746
Validation loss: 2.4111812832534953

Epoch: 5| Step: 4
Training loss: 2.7728450298309326
Validation loss: 2.4085192116357947

Epoch: 5| Step: 5
Training loss: 2.4717936515808105
Validation loss: 2.413703359583373

Epoch: 5| Step: 6
Training loss: 3.1844003200531006
Validation loss: 2.4120257541697514

Epoch: 5| Step: 7
Training loss: 2.7566256523132324
Validation loss: 2.4176657507496495

Epoch: 5| Step: 8
Training loss: 2.319164752960205
Validation loss: 2.414361535861928

Epoch: 5| Step: 9
Training loss: 2.563941240310669
Validation loss: 2.4207063208344164

Epoch: 5| Step: 10
Training loss: 2.5420310497283936
Validation loss: 2.4154694208534817

Epoch: 142| Step: 0
Training loss: 2.886756420135498
Validation loss: 2.424131703633134

Epoch: 5| Step: 1
Training loss: 2.9329962730407715
Validation loss: 2.4266156675995036

Epoch: 5| Step: 2
Training loss: 2.5357277393341064
Validation loss: 2.423110046694356

Epoch: 5| Step: 3
Training loss: 2.43013334274292
Validation loss: 2.4257079324414654

Epoch: 5| Step: 4
Training loss: 2.3267104625701904
Validation loss: 2.434391237074329

Epoch: 5| Step: 5
Training loss: 2.8151090145111084
Validation loss: 2.429941236331899

Epoch: 5| Step: 6
Training loss: 2.5880963802337646
Validation loss: 2.428279769036078

Epoch: 5| Step: 7
Training loss: 2.4260525703430176
Validation loss: 2.414889484323481

Epoch: 5| Step: 8
Training loss: 2.3657708168029785
Validation loss: 2.4053765830173286

Epoch: 5| Step: 9
Training loss: 2.6243605613708496
Validation loss: 2.406096801962904

Epoch: 5| Step: 10
Training loss: 2.921383857727051
Validation loss: 2.401542268773561

Epoch: 143| Step: 0
Training loss: 2.0199291706085205
Validation loss: 2.4063667584491033

Epoch: 5| Step: 1
Training loss: 2.6766200065612793
Validation loss: 2.402362859377297

Epoch: 5| Step: 2
Training loss: 2.616569995880127
Validation loss: 2.398529170661844

Epoch: 5| Step: 3
Training loss: 2.6934635639190674
Validation loss: 2.402346344404323

Epoch: 5| Step: 4
Training loss: 3.2727694511413574
Validation loss: 2.401849680049445

Epoch: 5| Step: 5
Training loss: 2.7625622749328613
Validation loss: 2.4054004966571765

Epoch: 5| Step: 6
Training loss: 2.993590831756592
Validation loss: 2.4080203451136106

Epoch: 5| Step: 7
Training loss: 2.8107776641845703
Validation loss: 2.40519150739075

Epoch: 5| Step: 8
Training loss: 2.900050640106201
Validation loss: 2.410507712312924

Epoch: 5| Step: 9
Training loss: 2.099074363708496
Validation loss: 2.413776789942095

Epoch: 5| Step: 10
Training loss: 1.935100793838501
Validation loss: 2.413892469098491

Epoch: 144| Step: 0
Training loss: 2.8712878227233887
Validation loss: 2.418723754985358

Epoch: 5| Step: 1
Training loss: 2.4227213859558105
Validation loss: 2.414402884821738

Epoch: 5| Step: 2
Training loss: 2.212472915649414
Validation loss: 2.4255822115047003

Epoch: 5| Step: 3
Training loss: 3.1706669330596924
Validation loss: 2.416242286723147

Epoch: 5| Step: 4
Training loss: 2.8479292392730713
Validation loss: 2.4108917123527935

Epoch: 5| Step: 5
Training loss: 2.5560927391052246
Validation loss: 2.4079759172213975

Epoch: 5| Step: 6
Training loss: 2.6417441368103027
Validation loss: 2.409103406372891

Epoch: 5| Step: 7
Training loss: 2.5035548210144043
Validation loss: 2.4093646246899842

Epoch: 5| Step: 8
Training loss: 2.7937583923339844
Validation loss: 2.4011779908211

Epoch: 5| Step: 9
Training loss: 2.3592047691345215
Validation loss: 2.404091073620704

Epoch: 5| Step: 10
Training loss: 2.363908529281616
Validation loss: 2.398301770610194

Epoch: 145| Step: 0
Training loss: 2.028670310974121
Validation loss: 2.3977201830956245

Epoch: 5| Step: 1
Training loss: 2.4929699897766113
Validation loss: 2.4013346190093667

Epoch: 5| Step: 2
Training loss: 2.6722686290740967
Validation loss: 2.4084177786304104

Epoch: 5| Step: 3
Training loss: 2.50822377204895
Validation loss: 2.4156353294208484

Epoch: 5| Step: 4
Training loss: 3.14113187789917
Validation loss: 2.4144228094367572

Epoch: 5| Step: 5
Training loss: 2.7407383918762207
Validation loss: 2.4212979168020268

Epoch: 5| Step: 6
Training loss: 2.535733222961426
Validation loss: 2.409407438770417

Epoch: 5| Step: 7
Training loss: 1.88718581199646
Validation loss: 2.4108996955297326

Epoch: 5| Step: 8
Training loss: 2.808171510696411
Validation loss: 2.4125107411415345

Epoch: 5| Step: 9
Training loss: 2.8912148475646973
Validation loss: 2.40919521803497

Epoch: 5| Step: 10
Training loss: 3.1139354705810547
Validation loss: 2.419256382091071

Epoch: 146| Step: 0
Training loss: 2.6036746501922607
Validation loss: 2.423728782643554

Epoch: 5| Step: 1
Training loss: 3.102780818939209
Validation loss: 2.4304379981051207

Epoch: 5| Step: 2
Training loss: 3.412619113922119
Validation loss: 2.4287690859968945

Epoch: 5| Step: 3
Training loss: 2.7992210388183594
Validation loss: 2.4206674252786944

Epoch: 5| Step: 4
Training loss: 3.6217570304870605
Validation loss: 2.409557116928921

Epoch: 5| Step: 5
Training loss: 2.0593345165252686
Validation loss: 2.40410388541478

Epoch: 5| Step: 6
Training loss: 2.315072536468506
Validation loss: 2.3987952688688874

Epoch: 5| Step: 7
Training loss: 1.9477298259735107
Validation loss: 2.4033140777259745

Epoch: 5| Step: 8
Training loss: 2.6201157569885254
Validation loss: 2.3944012477833736

Epoch: 5| Step: 9
Training loss: 2.1766037940979004
Validation loss: 2.3996663708840646

Epoch: 5| Step: 10
Training loss: 2.183750867843628
Validation loss: 2.4025190312375306

Epoch: 147| Step: 0
Training loss: 3.2002997398376465
Validation loss: 2.4129208851886053

Epoch: 5| Step: 1
Training loss: 3.047651529312134
Validation loss: 2.429141213816981

Epoch: 5| Step: 2
Training loss: 2.469996690750122
Validation loss: 2.4440430082300657

Epoch: 5| Step: 3
Training loss: 2.162482976913452
Validation loss: 2.4594233958951888

Epoch: 5| Step: 4
Training loss: 2.701920986175537
Validation loss: 2.4541381046336186

Epoch: 5| Step: 5
Training loss: 2.904249429702759
Validation loss: 2.463345766067505

Epoch: 5| Step: 6
Training loss: 2.920217514038086
Validation loss: 2.450640780951387

Epoch: 5| Step: 7
Training loss: 2.5113606452941895
Validation loss: 2.42396274176977

Epoch: 5| Step: 8
Training loss: 2.826566219329834
Validation loss: 2.404491347651328

Epoch: 5| Step: 9
Training loss: 1.9345897436141968
Validation loss: 2.3938123564566336

Epoch: 5| Step: 10
Training loss: 2.171142101287842
Validation loss: 2.3881119707579255

Epoch: 148| Step: 0
Training loss: 2.6826109886169434
Validation loss: 2.38949219770329

Epoch: 5| Step: 1
Training loss: 2.6535348892211914
Validation loss: 2.389468300727106

Epoch: 5| Step: 2
Training loss: 2.3472940921783447
Validation loss: 2.389408298718032

Epoch: 5| Step: 3
Training loss: 2.889490842819214
Validation loss: 2.3874083360036216

Epoch: 5| Step: 4
Training loss: 2.1010918617248535
Validation loss: 2.3922617512364543

Epoch: 5| Step: 5
Training loss: 2.947826385498047
Validation loss: 2.3905473985979633

Epoch: 5| Step: 6
Training loss: 2.4173121452331543
Validation loss: 2.3846628922288136

Epoch: 5| Step: 7
Training loss: 2.3233835697174072
Validation loss: 2.383701955118487

Epoch: 5| Step: 8
Training loss: 2.7495739459991455
Validation loss: 2.387002370690787

Epoch: 5| Step: 9
Training loss: 3.417313814163208
Validation loss: 2.383873990786973

Epoch: 5| Step: 10
Training loss: 2.2922768592834473
Validation loss: 2.3808792329603627

Epoch: 149| Step: 0
Training loss: 2.895190715789795
Validation loss: 2.3956887542560534

Epoch: 5| Step: 1
Training loss: 2.517437696456909
Validation loss: 2.4033261934916177

Epoch: 5| Step: 2
Training loss: 3.039445161819458
Validation loss: 2.4072397678129134

Epoch: 5| Step: 3
Training loss: 2.4038937091827393
Validation loss: 2.4252667760336273

Epoch: 5| Step: 4
Training loss: 2.32023549079895
Validation loss: 2.432256648617406

Epoch: 5| Step: 5
Training loss: 2.5155227184295654
Validation loss: 2.434730193948233

Epoch: 5| Step: 6
Training loss: 2.6598052978515625
Validation loss: 2.440769162229312

Epoch: 5| Step: 7
Training loss: 2.7963366508483887
Validation loss: 2.4315462496972855

Epoch: 5| Step: 8
Training loss: 2.7160141468048096
Validation loss: 2.428637607123262

Epoch: 5| Step: 9
Training loss: 2.626448154449463
Validation loss: 2.408395378820358

Epoch: 5| Step: 10
Training loss: 2.3564770221710205
Validation loss: 2.3956661711456957

Epoch: 150| Step: 0
Training loss: 2.6911096572875977
Validation loss: 2.3878584830991683

Epoch: 5| Step: 1
Training loss: 3.2606441974639893
Validation loss: 2.399702138798211

Epoch: 5| Step: 2
Training loss: 2.9861912727355957
Validation loss: 2.3962053304077475

Epoch: 5| Step: 3
Training loss: 2.262197494506836
Validation loss: 2.402308187177104

Epoch: 5| Step: 4
Training loss: 3.0316085815429688
Validation loss: 2.4017800387515815

Epoch: 5| Step: 5
Training loss: 2.682751178741455
Validation loss: 2.3991055129676737

Epoch: 5| Step: 6
Training loss: 1.9724321365356445
Validation loss: 2.3960806221090336

Epoch: 5| Step: 7
Training loss: 2.8668863773345947
Validation loss: 2.3935956724228395

Epoch: 5| Step: 8
Training loss: 2.592003583908081
Validation loss: 2.3932241675674275

Epoch: 5| Step: 9
Training loss: 2.184755802154541
Validation loss: 2.389046633115379

Epoch: 5| Step: 10
Training loss: 2.2621853351593018
Validation loss: 2.3856418260964016

Testing loss: 2.563155836529202

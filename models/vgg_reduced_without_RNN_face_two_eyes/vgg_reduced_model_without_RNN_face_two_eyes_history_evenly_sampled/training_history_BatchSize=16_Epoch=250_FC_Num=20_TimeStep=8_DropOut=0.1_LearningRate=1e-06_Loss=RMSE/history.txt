Epoch: 1| Step: 0
Training loss: 6.0687858792628235
Validation loss: 4.871813047661698

Epoch: 6| Step: 1
Training loss: 4.955645476486695
Validation loss: 4.867934092350408

Epoch: 6| Step: 2
Training loss: 5.084904954110153
Validation loss: 4.86007132868091

Epoch: 6| Step: 3
Training loss: 3.8705932416803375
Validation loss: 4.857914195191606

Epoch: 6| Step: 4
Training loss: 3.894381873232329
Validation loss: 4.8529851367073595

Epoch: 6| Step: 5
Training loss: 4.288441430656924
Validation loss: 4.848631751857985

Epoch: 6| Step: 6
Training loss: 4.808926134684089
Validation loss: 4.843953454716207

Epoch: 6| Step: 7
Training loss: 5.169081564512313
Validation loss: 4.837824362634886

Epoch: 6| Step: 8
Training loss: 4.372149601588659
Validation loss: 4.835243084721994

Epoch: 6| Step: 9
Training loss: 4.735779204660303
Validation loss: 4.828025881898802

Epoch: 6| Step: 10
Training loss: 5.914000577615059
Validation loss: 4.8257834364920384

Epoch: 6| Step: 11
Training loss: 5.159664248994649
Validation loss: 4.820453859182655

Epoch: 6| Step: 12
Training loss: 5.519139194265597
Validation loss: 4.816384848339439

Epoch: 6| Step: 13
Training loss: 4.277735471860306
Validation loss: 4.810129434923406

Epoch: 2| Step: 0
Training loss: 5.347711890232822
Validation loss: 4.806691286955483

Epoch: 6| Step: 1
Training loss: 4.2324365211589
Validation loss: 4.801416156912019

Epoch: 6| Step: 2
Training loss: 4.88339312954032
Validation loss: 4.7971990476608575

Epoch: 6| Step: 3
Training loss: 4.968159958455719
Validation loss: 4.791994212950171

Epoch: 6| Step: 4
Training loss: 4.977163808477275
Validation loss: 4.788732621075284

Epoch: 6| Step: 5
Training loss: 3.7346929311086408
Validation loss: 4.784237022883485

Epoch: 6| Step: 6
Training loss: 5.729610615493498
Validation loss: 4.777265576021248

Epoch: 6| Step: 7
Training loss: 5.2176706260049235
Validation loss: 4.77089054073188

Epoch: 6| Step: 8
Training loss: 3.6668707328421752
Validation loss: 4.767651702266996

Epoch: 6| Step: 9
Training loss: 4.3226142256621145
Validation loss: 4.761111569580575

Epoch: 6| Step: 10
Training loss: 5.0503869843217215
Validation loss: 4.758776425259499

Epoch: 6| Step: 11
Training loss: 4.2730973856313925
Validation loss: 4.752680843969206

Epoch: 6| Step: 12
Training loss: 5.541339013683895
Validation loss: 4.746341996895875

Epoch: 6| Step: 13
Training loss: 6.070987698231654
Validation loss: 4.742962883993375

Epoch: 3| Step: 0
Training loss: 5.225028458659228
Validation loss: 4.734769644639265

Epoch: 6| Step: 1
Training loss: 4.357668306708194
Validation loss: 4.7321250656921645

Epoch: 6| Step: 2
Training loss: 5.036790723066015
Validation loss: 4.726454192137371

Epoch: 6| Step: 3
Training loss: 5.181191710729777
Validation loss: 4.718227649580699

Epoch: 6| Step: 4
Training loss: 4.774099550677111
Validation loss: 4.715211633261043

Epoch: 6| Step: 5
Training loss: 4.754214374772036
Validation loss: 4.709075799862124

Epoch: 6| Step: 6
Training loss: 4.542350694200258
Validation loss: 4.70539377969519

Epoch: 6| Step: 7
Training loss: 5.392361480108073
Validation loss: 4.6969543931078

Epoch: 6| Step: 8
Training loss: 5.000257866885623
Validation loss: 4.692985745080553

Epoch: 6| Step: 9
Training loss: 4.512453014520146
Validation loss: 4.688634917440332

Epoch: 6| Step: 10
Training loss: 3.5387272623369874
Validation loss: 4.681954186820744

Epoch: 6| Step: 11
Training loss: 4.776571540462085
Validation loss: 4.674868617883072

Epoch: 6| Step: 12
Training loss: 5.613823871759317
Validation loss: 4.669921148165471

Epoch: 6| Step: 13
Training loss: 3.2950470543889363
Validation loss: 4.665371562772844

Epoch: 4| Step: 0
Training loss: 5.565925486365712
Validation loss: 4.657037756853765

Epoch: 6| Step: 1
Training loss: 4.66949218406885
Validation loss: 4.651643447502049

Epoch: 6| Step: 2
Training loss: 5.248350156674517
Validation loss: 4.645809758264157

Epoch: 6| Step: 3
Training loss: 5.013769644425567
Validation loss: 4.639575540028708

Epoch: 6| Step: 4
Training loss: 4.9798699471524115
Validation loss: 4.631160558670237

Epoch: 6| Step: 5
Training loss: 5.558463700275049
Validation loss: 4.629756272246215

Epoch: 6| Step: 6
Training loss: 5.162455390940647
Validation loss: 4.619063871775737

Epoch: 6| Step: 7
Training loss: 4.2725143969869706
Validation loss: 4.611670809548987

Epoch: 6| Step: 8
Training loss: 5.678341376759205
Validation loss: 4.606038859344933

Epoch: 6| Step: 9
Training loss: 3.8167145993972293
Validation loss: 4.599314716927292

Epoch: 6| Step: 10
Training loss: 3.214223252552401
Validation loss: 4.592126586944623

Epoch: 6| Step: 11
Training loss: 3.786250315553155
Validation loss: 4.586302942814995

Epoch: 6| Step: 12
Training loss: 4.108486064885864
Validation loss: 4.578441957906165

Epoch: 6| Step: 13
Training loss: 3.721154341283472
Validation loss: 4.572293549704903

Epoch: 5| Step: 0
Training loss: 5.323027468409623
Validation loss: 4.566913602341456

Epoch: 6| Step: 1
Training loss: 3.2602196452094705
Validation loss: 4.562919890686357

Epoch: 6| Step: 2
Training loss: 4.5103648875071105
Validation loss: 4.553369981673543

Epoch: 6| Step: 3
Training loss: 4.772024791592377
Validation loss: 4.544830862999745

Epoch: 6| Step: 4
Training loss: 3.91088994447247
Validation loss: 4.5397519970339255

Epoch: 6| Step: 5
Training loss: 6.177321877036178
Validation loss: 4.5313785431493026

Epoch: 6| Step: 6
Training loss: 5.3975140324136754
Validation loss: 4.527486582708002

Epoch: 6| Step: 7
Training loss: 4.012723475617584
Validation loss: 4.5186021467934685

Epoch: 6| Step: 8
Training loss: 3.8447173847363154
Validation loss: 4.512741856301334

Epoch: 6| Step: 9
Training loss: 5.262251953658176
Validation loss: 4.50360652234097

Epoch: 6| Step: 10
Training loss: 4.145389910168188
Validation loss: 4.496069347959369

Epoch: 6| Step: 11
Training loss: 4.617688973137615
Validation loss: 4.488590810377442

Epoch: 6| Step: 12
Training loss: 4.065369986087281
Validation loss: 4.483331337680656

Epoch: 6| Step: 13
Training loss: 4.682926642700905
Validation loss: 4.477524976461834

Epoch: 6| Step: 0
Training loss: 4.301142678245288
Validation loss: 4.468560522646072

Epoch: 6| Step: 1
Training loss: 4.940795764228013
Validation loss: 4.458578921617005

Epoch: 6| Step: 2
Training loss: 3.754065789078618
Validation loss: 4.453863619572472

Epoch: 6| Step: 3
Training loss: 4.773926954882808
Validation loss: 4.448819353901424

Epoch: 6| Step: 4
Training loss: 3.3773224928127585
Validation loss: 4.439598579792666

Epoch: 6| Step: 5
Training loss: 5.037746239789542
Validation loss: 4.430396979334301

Epoch: 6| Step: 6
Training loss: 3.8286831195904445
Validation loss: 4.424759063455434

Epoch: 6| Step: 7
Training loss: 4.434037913117773
Validation loss: 4.41943742549009

Epoch: 6| Step: 8
Training loss: 5.353802956820868
Validation loss: 4.4089768431081415

Epoch: 6| Step: 9
Training loss: 4.496188562952841
Validation loss: 4.400144152922806

Epoch: 6| Step: 10
Training loss: 4.498179173314925
Validation loss: 4.395876896443148

Epoch: 6| Step: 11
Training loss: 4.720325333326232
Validation loss: 4.383656364652229

Epoch: 6| Step: 12
Training loss: 4.9440310778088605
Validation loss: 4.377412766495709

Epoch: 6| Step: 13
Training loss: 4.430942472001362
Validation loss: 4.366743946182227

Epoch: 7| Step: 0
Training loss: 4.4877079397494475
Validation loss: 4.35923114265354

Epoch: 6| Step: 1
Training loss: 4.696589697664467
Validation loss: 4.348434112277374

Epoch: 6| Step: 2
Training loss: 5.024506023666441
Validation loss: 4.340577718935742

Epoch: 6| Step: 3
Training loss: 4.801398216686997
Validation loss: 4.329377687258634

Epoch: 6| Step: 4
Training loss: 4.287276210449648
Validation loss: 4.321055765678597

Epoch: 6| Step: 5
Training loss: 4.32867758628384
Validation loss: 4.308277897152735

Epoch: 6| Step: 6
Training loss: 3.480762747561168
Validation loss: 4.303734152333624

Epoch: 6| Step: 7
Training loss: 4.891969733568014
Validation loss: 4.293609121410239

Epoch: 6| Step: 8
Training loss: 3.6191332467544868
Validation loss: 4.2836079347891385

Epoch: 6| Step: 9
Training loss: 4.260977536367645
Validation loss: 4.278307214803078

Epoch: 6| Step: 10
Training loss: 4.313085986144602
Validation loss: 4.266728566105917

Epoch: 6| Step: 11
Training loss: 4.591737475025198
Validation loss: 4.257637987778073

Epoch: 6| Step: 12
Training loss: 3.783653819308161
Validation loss: 4.249006602845221

Epoch: 6| Step: 13
Training loss: 5.225055836648423
Validation loss: 4.236395841448275

Epoch: 8| Step: 0
Training loss: 3.2125848781193462
Validation loss: 4.225990888028594

Epoch: 6| Step: 1
Training loss: 4.177230768939233
Validation loss: 4.219639974202392

Epoch: 6| Step: 2
Training loss: 3.8786918527102987
Validation loss: 4.207385530389971

Epoch: 6| Step: 3
Training loss: 4.009244963989406
Validation loss: 4.197677296876317

Epoch: 6| Step: 4
Training loss: 4.454817814896139
Validation loss: 4.185421923467571

Epoch: 6| Step: 5
Training loss: 4.762897048378962
Validation loss: 4.175308737227362

Epoch: 6| Step: 6
Training loss: 4.8408253361697255
Validation loss: 4.168206769918203

Epoch: 6| Step: 7
Training loss: 4.266857374419456
Validation loss: 4.156850108572618

Epoch: 6| Step: 8
Training loss: 5.26835149703756
Validation loss: 4.1482662413309335

Epoch: 6| Step: 9
Training loss: 3.53465147682513
Validation loss: 4.131832692490762

Epoch: 6| Step: 10
Training loss: 4.0599202107824635
Validation loss: 4.124202244676877

Epoch: 6| Step: 11
Training loss: 4.717898260386857
Validation loss: 4.115434122139075

Epoch: 6| Step: 12
Training loss: 3.8754067976523494
Validation loss: 4.102473753349092

Epoch: 6| Step: 13
Training loss: 4.558277632368928
Validation loss: 4.088993745348897

Epoch: 9| Step: 0
Training loss: 3.96627550253377
Validation loss: 4.077783615803163

Epoch: 6| Step: 1
Training loss: 4.9884062820812245
Validation loss: 4.067637640574713

Epoch: 6| Step: 2
Training loss: 4.588537463714141
Validation loss: 4.054497270754846

Epoch: 6| Step: 3
Training loss: 4.10269608590983
Validation loss: 4.041202602674955

Epoch: 6| Step: 4
Training loss: 4.486062187712779
Validation loss: 4.0340313929670755

Epoch: 6| Step: 5
Training loss: 2.54053799525984
Validation loss: 4.017428494018013

Epoch: 6| Step: 6
Training loss: 4.5225262028666275
Validation loss: 4.002907854803845

Epoch: 6| Step: 7
Training loss: 4.0196940071215215
Validation loss: 3.9913337783137286

Epoch: 6| Step: 8
Training loss: 3.6546117258554642
Validation loss: 3.9765075412895245

Epoch: 6| Step: 9
Training loss: 3.614654642916946
Validation loss: 3.9684605814688587

Epoch: 6| Step: 10
Training loss: 3.976286932909539
Validation loss: 3.9573880973831645

Epoch: 6| Step: 11
Training loss: 4.117471703233516
Validation loss: 3.946620175777357

Epoch: 6| Step: 12
Training loss: 4.803835909668593
Validation loss: 3.9307197887149576

Epoch: 6| Step: 13
Training loss: 3.797771583474201
Validation loss: 3.920037490559973

Epoch: 10| Step: 0
Training loss: 3.6920035065771706
Validation loss: 3.9091655890828974

Epoch: 6| Step: 1
Training loss: 4.02747540855359
Validation loss: 3.8995265953135787

Epoch: 6| Step: 2
Training loss: 3.8822186819956994
Validation loss: 3.8825618623139677

Epoch: 6| Step: 3
Training loss: 4.1511148265229325
Validation loss: 3.8740663015514287

Epoch: 6| Step: 4
Training loss: 4.342809341999374
Validation loss: 3.856772392303488

Epoch: 6| Step: 5
Training loss: 4.225365770747716
Validation loss: 3.8433347875107824

Epoch: 6| Step: 6
Training loss: 3.9549723895974327
Validation loss: 3.829557321050411

Epoch: 6| Step: 7
Training loss: 3.376809094492096
Validation loss: 3.820797568311392

Epoch: 6| Step: 8
Training loss: 3.7068258214533443
Validation loss: 3.8014157052803785

Epoch: 6| Step: 9
Training loss: 4.351868432075774
Validation loss: 3.7929463711201716

Epoch: 6| Step: 10
Training loss: 4.104133308907638
Validation loss: 3.7787132096326386

Epoch: 6| Step: 11
Training loss: 4.211209261870707
Validation loss: 3.7629546576252455

Epoch: 6| Step: 12
Training loss: 3.8828021841852403
Validation loss: 3.7509504515216725

Epoch: 6| Step: 13
Training loss: 2.9701793241470003
Validation loss: 3.73886727076814

Epoch: 11| Step: 0
Training loss: 2.930502490808157
Validation loss: 3.7208967843231244

Epoch: 6| Step: 1
Training loss: 3.3998940339124535
Validation loss: 3.703948474598389

Epoch: 6| Step: 2
Training loss: 3.0286593033722564
Validation loss: 3.6905680956548506

Epoch: 6| Step: 3
Training loss: 4.966236558421198
Validation loss: 3.6770630458599927

Epoch: 6| Step: 4
Training loss: 4.013649302323996
Validation loss: 3.662526373547947

Epoch: 6| Step: 5
Training loss: 4.470860549734892
Validation loss: 3.649890696414081

Epoch: 6| Step: 6
Training loss: 3.9982282767911737
Validation loss: 3.6343060674115315

Epoch: 6| Step: 7
Training loss: 3.2187914799360016
Validation loss: 3.625332874690614

Epoch: 6| Step: 8
Training loss: 4.205500842373603
Validation loss: 3.608033423350969

Epoch: 6| Step: 9
Training loss: 3.26495821775659
Validation loss: 3.588094107620576

Epoch: 6| Step: 10
Training loss: 4.058382735581601
Validation loss: 3.565783980576301

Epoch: 6| Step: 11
Training loss: 4.217176581951988
Validation loss: 3.556960563953456

Epoch: 6| Step: 12
Training loss: 2.5683124030281714
Validation loss: 3.540771494782996

Epoch: 6| Step: 13
Training loss: 3.849048838843975
Validation loss: 3.5243485847170204

Epoch: 12| Step: 0
Training loss: 2.3692179114557805
Validation loss: 3.508588158697488

Epoch: 6| Step: 1
Training loss: 4.083489058243446
Validation loss: 3.494297047175144

Epoch: 6| Step: 2
Training loss: 3.8455454036981926
Validation loss: 3.474218191140824

Epoch: 6| Step: 3
Training loss: 3.971049205973737
Validation loss: 3.463718145504985

Epoch: 6| Step: 4
Training loss: 2.8085852610920043
Validation loss: 3.443237246682892

Epoch: 6| Step: 5
Training loss: 4.369232517822759
Validation loss: 3.430301046748556

Epoch: 6| Step: 6
Training loss: 3.0816950354775554
Validation loss: 3.4042879602611587

Epoch: 6| Step: 7
Training loss: 2.8027280800778613
Validation loss: 3.393091771194154

Epoch: 6| Step: 8
Training loss: 4.570555799480202
Validation loss: 3.372193801852115

Epoch: 6| Step: 9
Training loss: 3.265792185323078
Validation loss: 3.3624456613278313

Epoch: 6| Step: 10
Training loss: 3.170894226879711
Validation loss: 3.347641321429607

Epoch: 6| Step: 11
Training loss: 3.8269222939294725
Validation loss: 3.3294066270825304

Epoch: 6| Step: 12
Training loss: 3.567839118435222
Validation loss: 3.3117133576565156

Epoch: 6| Step: 13
Training loss: 3.792787211637857
Validation loss: 3.296060029158631

Epoch: 13| Step: 0
Training loss: 3.4467645815942545
Validation loss: 3.2738738502774436

Epoch: 6| Step: 1
Training loss: 3.4561011835933497
Validation loss: 3.2630692812559206

Epoch: 6| Step: 2
Training loss: 3.3898282147658394
Validation loss: 3.2533772419466707

Epoch: 6| Step: 3
Training loss: 3.2793823739970933
Validation loss: 3.231265023720356

Epoch: 6| Step: 4
Training loss: 3.5483246248233042
Validation loss: 3.2233621901378577

Epoch: 6| Step: 5
Training loss: 3.491168460187395
Validation loss: 3.2056300874349097

Epoch: 6| Step: 6
Training loss: 3.873269956226727
Validation loss: 3.185910296789566

Epoch: 6| Step: 7
Training loss: 3.8566373312144093
Validation loss: 3.173956056147886

Epoch: 6| Step: 8
Training loss: 3.3268372815961067
Validation loss: 3.157432734986026

Epoch: 6| Step: 9
Training loss: 2.7742382344297813
Validation loss: 3.134998940595895

Epoch: 6| Step: 10
Training loss: 2.2692994936666175
Validation loss: 3.115799086332503

Epoch: 6| Step: 11
Training loss: 3.0803874265078566
Validation loss: 3.10083041657964

Epoch: 6| Step: 12
Training loss: 3.2863506091922217
Validation loss: 3.0885929295083905

Epoch: 6| Step: 13
Training loss: 4.011462477361017
Validation loss: 3.078516503253647

Epoch: 14| Step: 0
Training loss: 3.7787453419022237
Validation loss: 3.051417379365304

Epoch: 6| Step: 1
Training loss: 2.9397413752458768
Validation loss: 3.0400165903716

Epoch: 6| Step: 2
Training loss: 3.066288998056852
Validation loss: 3.0236818885163093

Epoch: 6| Step: 3
Training loss: 2.8332782814811006
Validation loss: 3.0116178155357187

Epoch: 6| Step: 4
Training loss: 3.152350405623395
Validation loss: 2.997123441529086

Epoch: 6| Step: 5
Training loss: 2.8724969665670685
Validation loss: 2.9719295829802634

Epoch: 6| Step: 6
Training loss: 3.9374847411813714
Validation loss: 2.9644973554475036

Epoch: 6| Step: 7
Training loss: 3.232286304039563
Validation loss: 2.9504022720616274

Epoch: 6| Step: 8
Training loss: 2.920922761673627
Validation loss: 2.939327086324561

Epoch: 6| Step: 9
Training loss: 3.1798758486138508
Validation loss: 2.919278515042555

Epoch: 6| Step: 10
Training loss: 1.988014068338825
Validation loss: 2.9054159060662075

Epoch: 6| Step: 11
Training loss: 3.3153708631103305
Validation loss: 2.8905319770937687

Epoch: 6| Step: 12
Training loss: 3.779802534001601
Validation loss: 2.871502338149642

Epoch: 6| Step: 13
Training loss: 2.987350338814309
Validation loss: 2.862819876259844

Epoch: 15| Step: 0
Training loss: 2.732184879040369
Validation loss: 2.845408634198137

Epoch: 6| Step: 1
Training loss: 2.446814322663214
Validation loss: 2.8357691136150396

Epoch: 6| Step: 2
Training loss: 2.8518909003963793
Validation loss: 2.820287689871592

Epoch: 6| Step: 3
Training loss: 2.9682664979424636
Validation loss: 2.8149010321059356

Epoch: 6| Step: 4
Training loss: 3.3257880804248483
Validation loss: 2.8019379587488773

Epoch: 6| Step: 5
Training loss: 3.263017991844375
Validation loss: 2.78400074676225

Epoch: 6| Step: 6
Training loss: 2.9954293719252307
Validation loss: 2.7741278508301543

Epoch: 6| Step: 7
Training loss: 2.3941590360561262
Validation loss: 2.774641374626731

Epoch: 6| Step: 8
Training loss: 3.368928605194746
Validation loss: 2.7498038810238055

Epoch: 6| Step: 9
Training loss: 2.869451725390358
Validation loss: 2.746475924343043

Epoch: 6| Step: 10
Training loss: 3.6562919940330096
Validation loss: 2.742932184665119

Epoch: 6| Step: 11
Training loss: 2.7220644861917
Validation loss: 2.7283724866374675

Epoch: 6| Step: 12
Training loss: 3.109937760690109
Validation loss: 2.7160908027228943

Epoch: 6| Step: 13
Training loss: 3.4209923847005563
Validation loss: 2.705982618052059

Epoch: 16| Step: 0
Training loss: 3.144505622711953
Validation loss: 2.6941756062309374

Epoch: 6| Step: 1
Training loss: 2.7008761715063936
Validation loss: 2.679329154000017

Epoch: 6| Step: 2
Training loss: 2.7683410239146333
Validation loss: 2.6736072652301064

Epoch: 6| Step: 3
Training loss: 2.890745088299441
Validation loss: 2.651795210042951

Epoch: 6| Step: 4
Training loss: 3.4316545202834785
Validation loss: 2.6389085647664086

Epoch: 6| Step: 5
Training loss: 3.0455131421604906
Validation loss: 2.6372055492763815

Epoch: 6| Step: 6
Training loss: 2.7132195533528143
Validation loss: 2.639511499823531

Epoch: 6| Step: 7
Training loss: 2.475257313000805
Validation loss: 2.628066993564773

Epoch: 6| Step: 8
Training loss: 3.4857335200661277
Validation loss: 2.618464156545279

Epoch: 6| Step: 9
Training loss: 2.7995172731630067
Validation loss: 2.604634026377128

Epoch: 6| Step: 10
Training loss: 3.2039515475587335
Validation loss: 2.611992484845497

Epoch: 6| Step: 11
Training loss: 2.7928188470480424
Validation loss: 2.602496929443694

Epoch: 6| Step: 12
Training loss: 2.318696457895035
Validation loss: 2.5946646445505004

Epoch: 6| Step: 13
Training loss: 2.7607217200282594
Validation loss: 2.597272415170394

Epoch: 17| Step: 0
Training loss: 3.3063249431690434
Validation loss: 2.5780578823308704

Epoch: 6| Step: 1
Training loss: 2.0411492554938566
Validation loss: 2.5809474457186172

Epoch: 6| Step: 2
Training loss: 2.1505008291612318
Validation loss: 2.573265709296378

Epoch: 6| Step: 3
Training loss: 3.086222266083794
Validation loss: 2.5726069060939833

Epoch: 6| Step: 4
Training loss: 3.224733143492764
Validation loss: 2.5526436152827165

Epoch: 6| Step: 5
Training loss: 2.591960887419569
Validation loss: 2.558559788917911

Epoch: 6| Step: 6
Training loss: 2.352017279464152
Validation loss: 2.5476830410631908

Epoch: 6| Step: 7
Training loss: 3.1170262651166025
Validation loss: 2.544036104406562

Epoch: 6| Step: 8
Training loss: 3.493146180194304
Validation loss: 2.545099006227721

Epoch: 6| Step: 9
Training loss: 2.5040876349631693
Validation loss: 2.54759555903138

Epoch: 6| Step: 10
Training loss: 3.1758294471377284
Validation loss: 2.5358425753305207

Epoch: 6| Step: 11
Training loss: 2.278191698141898
Validation loss: 2.530329062385121

Epoch: 6| Step: 12
Training loss: 2.9883848085099265
Validation loss: 2.5363578583608066

Epoch: 6| Step: 13
Training loss: 3.44474251458963
Validation loss: 2.5296753877960403

Epoch: 18| Step: 0
Training loss: 3.3031091550737512
Validation loss: 2.5257300041381057

Epoch: 6| Step: 1
Training loss: 3.0400974110004406
Validation loss: 2.5169178504117315

Epoch: 6| Step: 2
Training loss: 2.578434226247013
Validation loss: 2.5259943008845065

Epoch: 6| Step: 3
Training loss: 2.7273507713942133
Validation loss: 2.52592814251397

Epoch: 6| Step: 4
Training loss: 3.156407116293629
Validation loss: 2.5349003701719224

Epoch: 6| Step: 5
Training loss: 2.6905992956406846
Validation loss: 2.5161563325162897

Epoch: 6| Step: 6
Training loss: 2.8017798522954585
Validation loss: 2.517869191243198

Epoch: 6| Step: 7
Training loss: 2.1224585650637793
Validation loss: 2.5129496838121312

Epoch: 6| Step: 8
Training loss: 2.8703198693959053
Validation loss: 2.5114302405303466

Epoch: 6| Step: 9
Training loss: 3.0131791231761724
Validation loss: 2.502342638053003

Epoch: 6| Step: 10
Training loss: 2.3614508988967757
Validation loss: 2.5116907058635545

Epoch: 6| Step: 11
Training loss: 3.353813642480203
Validation loss: 2.5256418682868387

Epoch: 6| Step: 12
Training loss: 2.9013456576847596
Validation loss: 2.5070179275437217

Epoch: 6| Step: 13
Training loss: 2.3077421989916544
Validation loss: 2.499378132129055

Epoch: 19| Step: 0
Training loss: 3.235199017070466
Validation loss: 2.513345553797999

Epoch: 6| Step: 1
Training loss: 2.418925599813703
Validation loss: 2.5114910502429013

Epoch: 6| Step: 2
Training loss: 2.876082009420898
Validation loss: 2.5007673111473268

Epoch: 6| Step: 3
Training loss: 2.713845575829296
Validation loss: 2.5173421196839407

Epoch: 6| Step: 4
Training loss: 2.7563252099257656
Validation loss: 2.502117533809203

Epoch: 6| Step: 5
Training loss: 3.289896154880865
Validation loss: 2.50277254531557

Epoch: 6| Step: 6
Training loss: 2.8132182581660126
Validation loss: 2.5019436715082652

Epoch: 6| Step: 7
Training loss: 3.218312650430031
Validation loss: 2.508461112720783

Epoch: 6| Step: 8
Training loss: 2.369833295407603
Validation loss: 2.4983609599938355

Epoch: 6| Step: 9
Training loss: 2.3666375153944594
Validation loss: 2.5161503272789596

Epoch: 6| Step: 10
Training loss: 2.4463341866783814
Validation loss: 2.5175343582421053

Epoch: 6| Step: 11
Training loss: 3.382938514811512
Validation loss: 2.504709119630967

Epoch: 6| Step: 12
Training loss: 2.7493336477116066
Validation loss: 2.5168006222279535

Epoch: 6| Step: 13
Training loss: 2.882252543167447
Validation loss: 2.510326106268076

Epoch: 20| Step: 0
Training loss: 1.9406349906725646
Validation loss: 2.509489292978757

Epoch: 6| Step: 1
Training loss: 2.8701947731623845
Validation loss: 2.5008886788183573

Epoch: 6| Step: 2
Training loss: 2.838576756998319
Validation loss: 2.497728984180347

Epoch: 6| Step: 3
Training loss: 2.7717101829753816
Validation loss: 2.4865881143980952

Epoch: 6| Step: 4
Training loss: 3.6825626746578535
Validation loss: 2.5127796143825134

Epoch: 6| Step: 5
Training loss: 2.5652610976080714
Validation loss: 2.5065597754760325

Epoch: 6| Step: 6
Training loss: 2.9635870761889715
Validation loss: 2.502332468882616

Epoch: 6| Step: 7
Training loss: 2.9233101161033375
Validation loss: 2.498922441891254

Epoch: 6| Step: 8
Training loss: 2.769858530611636
Validation loss: 2.500548569164934

Epoch: 6| Step: 9
Training loss: 2.5444660612943646
Validation loss: 2.498319975039241

Epoch: 6| Step: 10
Training loss: 2.6010816891582462
Validation loss: 2.506992035540567

Epoch: 6| Step: 11
Training loss: 2.84583298407162
Validation loss: 2.498293981603886

Epoch: 6| Step: 12
Training loss: 3.0821525443495856
Validation loss: 2.5031052183304383

Epoch: 6| Step: 13
Training loss: 2.950503522929325
Validation loss: 2.4933652535104516

Epoch: 21| Step: 0
Training loss: 3.047428178699786
Validation loss: 2.4987080896849947

Epoch: 6| Step: 1
Training loss: 3.0735127705993994
Validation loss: 2.4936925783531

Epoch: 6| Step: 2
Training loss: 2.438394040203571
Validation loss: 2.502420748525573

Epoch: 6| Step: 3
Training loss: 3.060337159275491
Validation loss: 2.505236861747636

Epoch: 6| Step: 4
Training loss: 2.224514846200033
Validation loss: 2.4975454473461736

Epoch: 6| Step: 5
Training loss: 2.4158925647279643
Validation loss: 2.5112456556812943

Epoch: 6| Step: 6
Training loss: 3.00779886614199
Validation loss: 2.507056968538462

Epoch: 6| Step: 7
Training loss: 2.4065450512211752
Validation loss: 2.503020766779331

Epoch: 6| Step: 8
Training loss: 3.602988136782294
Validation loss: 2.504953550823807

Epoch: 6| Step: 9
Training loss: 3.0836206938033013
Validation loss: 2.499761646454907

Epoch: 6| Step: 10
Training loss: 2.2981229752320806
Validation loss: 2.501784703839585

Epoch: 6| Step: 11
Training loss: 2.9308731324855373
Validation loss: 2.5022209700191236

Epoch: 6| Step: 12
Training loss: 2.845055416754102
Validation loss: 2.502226082498739

Epoch: 6| Step: 13
Training loss: 2.9221576411174355
Validation loss: 2.501277944585182

Epoch: 22| Step: 0
Training loss: 2.314395024982779
Validation loss: 2.500269753248886

Epoch: 6| Step: 1
Training loss: 3.669472545587088
Validation loss: 2.515365109209413

Epoch: 6| Step: 2
Training loss: 3.028175131557019
Validation loss: 2.500187248477812

Epoch: 6| Step: 3
Training loss: 2.528428754252734
Validation loss: 2.4970155216549252

Epoch: 6| Step: 4
Training loss: 2.2455103904468436
Validation loss: 2.490315054886054

Epoch: 6| Step: 5
Training loss: 2.7340942238656636
Validation loss: 2.490487273932852

Epoch: 6| Step: 6
Training loss: 3.4744846263704026
Validation loss: 2.502051633463758

Epoch: 6| Step: 7
Training loss: 2.657325347443694
Validation loss: 2.50029429938982

Epoch: 6| Step: 8
Training loss: 2.994965461442648
Validation loss: 2.488231257974172

Epoch: 6| Step: 9
Training loss: 2.4664577030473542
Validation loss: 2.500661517370495

Epoch: 6| Step: 10
Training loss: 2.3677754191404725
Validation loss: 2.489652437999935

Epoch: 6| Step: 11
Training loss: 2.980141398000302
Validation loss: 2.5014693587479444

Epoch: 6| Step: 12
Training loss: 2.646701119756777
Validation loss: 2.4894241976883476

Epoch: 6| Step: 13
Training loss: 3.233595307214532
Validation loss: 2.4880428490093403

Epoch: 23| Step: 0
Training loss: 3.2427589740142273
Validation loss: 2.4902735319040885

Epoch: 6| Step: 1
Training loss: 2.8513512938157284
Validation loss: 2.485389304805768

Epoch: 6| Step: 2
Training loss: 2.79622075617662
Validation loss: 2.5106079889190114

Epoch: 6| Step: 3
Training loss: 3.0681293675727086
Validation loss: 2.4932357872690822

Epoch: 6| Step: 4
Training loss: 3.066615395186463
Validation loss: 2.4861474623338586

Epoch: 6| Step: 5
Training loss: 2.9004585232477584
Validation loss: 2.504894369353331

Epoch: 6| Step: 6
Training loss: 2.6717027753681113
Validation loss: 2.4949712387882736

Epoch: 6| Step: 7
Training loss: 2.348237890718538
Validation loss: 2.505337544481917

Epoch: 6| Step: 8
Training loss: 3.174609278869815
Validation loss: 2.5080605425983045

Epoch: 6| Step: 9
Training loss: 2.374031270876114
Validation loss: 2.4951319336056583

Epoch: 6| Step: 10
Training loss: 3.133165296823913
Validation loss: 2.4977282826436187

Epoch: 6| Step: 11
Training loss: 2.8047937569639596
Validation loss: 2.5116216781481175

Epoch: 6| Step: 12
Training loss: 2.16618072733589
Validation loss: 2.4835861562419317

Epoch: 6| Step: 13
Training loss: 2.596123402300188
Validation loss: 2.5004110798186354

Epoch: 24| Step: 0
Training loss: 3.1177017437862795
Validation loss: 2.4924192487674715

Epoch: 6| Step: 1
Training loss: 2.714399166891126
Validation loss: 2.503971837387011

Epoch: 6| Step: 2
Training loss: 2.8178223159685065
Validation loss: 2.5016438319906125

Epoch: 6| Step: 3
Training loss: 2.5924915799893493
Validation loss: 2.496960873346189

Epoch: 6| Step: 4
Training loss: 2.295864168249667
Validation loss: 2.5040490923337404

Epoch: 6| Step: 5
Training loss: 3.0801264263809425
Validation loss: 2.5050818649917526

Epoch: 6| Step: 6
Training loss: 2.6134451000810586
Validation loss: 2.5075729153958224

Epoch: 6| Step: 7
Training loss: 3.1484373485479957
Validation loss: 2.5036818904689944

Epoch: 6| Step: 8
Training loss: 3.146123674817329
Validation loss: 2.4994218814143747

Epoch: 6| Step: 9
Training loss: 2.5898711834543446
Validation loss: 2.506420396334401

Epoch: 6| Step: 10
Training loss: 2.4287582533966576
Validation loss: 2.5066111263162685

Epoch: 6| Step: 11
Training loss: 3.053588513395848
Validation loss: 2.4970521990830274

Epoch: 6| Step: 12
Training loss: 2.8674871719459207
Validation loss: 2.5013853972077165

Epoch: 6| Step: 13
Training loss: 2.775759302681142
Validation loss: 2.509161062817092

Epoch: 25| Step: 0
Training loss: 2.167218932657495
Validation loss: 2.4905670145884584

Epoch: 6| Step: 1
Training loss: 2.906818949475405
Validation loss: 2.5023297703470857

Epoch: 6| Step: 2
Training loss: 2.907081567072012
Validation loss: 2.5043389404355483

Epoch: 6| Step: 3
Training loss: 3.3805098592622587
Validation loss: 2.491744134285639

Epoch: 6| Step: 4
Training loss: 2.4747942559623985
Validation loss: 2.495137445906083

Epoch: 6| Step: 5
Training loss: 3.0459825039997934
Validation loss: 2.4912178199380257

Epoch: 6| Step: 6
Training loss: 2.508972945254841
Validation loss: 2.4945358335983316

Epoch: 6| Step: 7
Training loss: 3.2569817597601993
Validation loss: 2.496762928191784

Epoch: 6| Step: 8
Training loss: 2.751684799812346
Validation loss: 2.5092588714694295

Epoch: 6| Step: 9
Training loss: 2.8719622278093886
Validation loss: 2.485606094833813

Epoch: 6| Step: 10
Training loss: 3.0126407068086154
Validation loss: 2.4896056090611336

Epoch: 6| Step: 11
Training loss: 3.293251251737084
Validation loss: 2.509134760740371

Epoch: 6| Step: 12
Training loss: 2.0948991041992695
Validation loss: 2.499761997707138

Epoch: 6| Step: 13
Training loss: 1.754142966151868
Validation loss: 2.4996369216153083

Epoch: 26| Step: 0
Training loss: 2.670785603763801
Validation loss: 2.4988683508105693

Epoch: 6| Step: 1
Training loss: 2.986431591569284
Validation loss: 2.4988848639747436

Epoch: 6| Step: 2
Training loss: 2.7994025546687524
Validation loss: 2.4956108063975826

Epoch: 6| Step: 3
Training loss: 2.9866322873328026
Validation loss: 2.5123071494382128

Epoch: 6| Step: 4
Training loss: 3.0583396211675575
Validation loss: 2.4901259501123114

Epoch: 6| Step: 5
Training loss: 2.580013254116819
Validation loss: 2.5002833657002554

Epoch: 6| Step: 6
Training loss: 2.877879649420784
Validation loss: 2.5034263306667515

Epoch: 6| Step: 7
Training loss: 2.620468588122722
Validation loss: 2.5022498179962347

Epoch: 6| Step: 8
Training loss: 2.8914823343264056
Validation loss: 2.5048662099204284

Epoch: 6| Step: 9
Training loss: 3.168245540996536
Validation loss: 2.5052982771516823

Epoch: 6| Step: 10
Training loss: 2.6873972673519635
Validation loss: 2.4970458327139737

Epoch: 6| Step: 11
Training loss: 2.577665345802036
Validation loss: 2.4984028410979673

Epoch: 6| Step: 12
Training loss: 2.810522443865322
Validation loss: 2.5114857708439153

Epoch: 6| Step: 13
Training loss: 2.3207715540481577
Validation loss: 2.50060765459658

Epoch: 27| Step: 0
Training loss: 3.090315173966999
Validation loss: 2.496254733545126

Epoch: 6| Step: 1
Training loss: 2.5165920886511626
Validation loss: 2.4999980126649897

Epoch: 6| Step: 2
Training loss: 2.9283095717908973
Validation loss: 2.4892162306860706

Epoch: 6| Step: 3
Training loss: 2.970464913813866
Validation loss: 2.5044051859597354

Epoch: 6| Step: 4
Training loss: 2.523606238613861
Validation loss: 2.487535379287741

Epoch: 6| Step: 5
Training loss: 2.9451106075636093
Validation loss: 2.490852432714872

Epoch: 6| Step: 6
Training loss: 2.7075490745260367
Validation loss: 2.4907071764080135

Epoch: 6| Step: 7
Training loss: 3.1094486093674027
Validation loss: 2.4998253310107397

Epoch: 6| Step: 8
Training loss: 2.689529185164943
Validation loss: 2.4860544557643482

Epoch: 6| Step: 9
Training loss: 2.8505795809709875
Validation loss: 2.496361019082438

Epoch: 6| Step: 10
Training loss: 3.1367637469530947
Validation loss: 2.4929904375283956

Epoch: 6| Step: 11
Training loss: 2.637154098174453
Validation loss: 2.4999091654857337

Epoch: 6| Step: 12
Training loss: 2.613417549171184
Validation loss: 2.4995660097528405

Epoch: 6| Step: 13
Training loss: 2.3748563421871673
Validation loss: 2.5034672023267417

Epoch: 28| Step: 0
Training loss: 2.6407058071435356
Validation loss: 2.496400580005283

Epoch: 6| Step: 1
Training loss: 2.192614652274015
Validation loss: 2.481721369565234

Epoch: 6| Step: 2
Training loss: 2.9676334891628677
Validation loss: 2.494711412955309

Epoch: 6| Step: 3
Training loss: 2.569940696655503
Validation loss: 2.49870345837788

Epoch: 6| Step: 4
Training loss: 2.54116161727512
Validation loss: 2.506806414156963

Epoch: 6| Step: 5
Training loss: 2.420220973117087
Validation loss: 2.504599577191202

Epoch: 6| Step: 6
Training loss: 2.564656350537065
Validation loss: 2.5009655964285633

Epoch: 6| Step: 7
Training loss: 2.8833118202715227
Validation loss: 2.503055040998708

Epoch: 6| Step: 8
Training loss: 2.6883838885623264
Validation loss: 2.50417426182913

Epoch: 6| Step: 9
Training loss: 3.513379681213346
Validation loss: 2.4951432756771026

Epoch: 6| Step: 10
Training loss: 3.23149504036798
Validation loss: 2.4955835967055333

Epoch: 6| Step: 11
Training loss: 3.084691204029789
Validation loss: 2.4974774359910494

Epoch: 6| Step: 12
Training loss: 2.8888380971339735
Validation loss: 2.507188960413292

Epoch: 6| Step: 13
Training loss: 2.7957072457441337
Validation loss: 2.5036005282686706

Epoch: 29| Step: 0
Training loss: 2.9453060241142337
Validation loss: 2.4990950997635255

Epoch: 6| Step: 1
Training loss: 3.2436653905281982
Validation loss: 2.493883064115092

Epoch: 6| Step: 2
Training loss: 2.516700088413789
Validation loss: 2.5005515382329873

Epoch: 6| Step: 3
Training loss: 2.560861109955243
Validation loss: 2.5019893452936435

Epoch: 6| Step: 4
Training loss: 3.042365703643635
Validation loss: 2.501895175948078

Epoch: 6| Step: 5
Training loss: 2.489810587408093
Validation loss: 2.517185342546054

Epoch: 6| Step: 6
Training loss: 2.4632243838583556
Validation loss: 2.492815936598262

Epoch: 6| Step: 7
Training loss: 2.6222923483155305
Validation loss: 2.4994075647442924

Epoch: 6| Step: 8
Training loss: 2.993189233173121
Validation loss: 2.492677744082978

Epoch: 6| Step: 9
Training loss: 2.8942381675697795
Validation loss: 2.4843955379335507

Epoch: 6| Step: 10
Training loss: 2.989132589105727
Validation loss: 2.506445430983385

Epoch: 6| Step: 11
Training loss: 2.2084843805826075
Validation loss: 2.507112749098172

Epoch: 6| Step: 12
Training loss: 2.9381690379439074
Validation loss: 2.4942749294609103

Epoch: 6| Step: 13
Training loss: 3.22464826573118
Validation loss: 2.500027000117372

Epoch: 30| Step: 0
Training loss: 2.435511144453977
Validation loss: 2.491384415857605

Epoch: 6| Step: 1
Training loss: 2.552309942240512
Validation loss: 2.4940621462486354

Epoch: 6| Step: 2
Training loss: 3.0756916167764268
Validation loss: 2.492711632919413

Epoch: 6| Step: 3
Training loss: 3.1746161882194333
Validation loss: 2.498816708509329

Epoch: 6| Step: 4
Training loss: 2.4179975418570545
Validation loss: 2.4913118917392576

Epoch: 6| Step: 5
Training loss: 2.8094912223834947
Validation loss: 2.497505760850977

Epoch: 6| Step: 6
Training loss: 2.676170064702533
Validation loss: 2.4907314483086433

Epoch: 6| Step: 7
Training loss: 2.832352150461456
Validation loss: 2.488334694982541

Epoch: 6| Step: 8
Training loss: 2.836574010116458
Validation loss: 2.493328776326056

Epoch: 6| Step: 9
Training loss: 3.527735623953037
Validation loss: 2.4967767466363817

Epoch: 6| Step: 10
Training loss: 2.4752933366898424
Validation loss: 2.4910231902641278

Epoch: 6| Step: 11
Training loss: 2.8607131245003363
Validation loss: 2.4849885763440276

Epoch: 6| Step: 12
Training loss: 2.5815522504177526
Validation loss: 2.5023115269821434

Epoch: 6| Step: 13
Training loss: 2.6028320147620576
Validation loss: 2.4913355840683975

Epoch: 31| Step: 0
Training loss: 3.086374140885128
Validation loss: 2.4853579619190063

Epoch: 6| Step: 1
Training loss: 2.168537457079953
Validation loss: 2.4984315402540576

Epoch: 6| Step: 2
Training loss: 2.9487040695875946
Validation loss: 2.495636967999184

Epoch: 6| Step: 3
Training loss: 2.9066834485447406
Validation loss: 2.496568525435154

Epoch: 6| Step: 4
Training loss: 3.060705009226374
Validation loss: 2.481907366204951

Epoch: 6| Step: 5
Training loss: 2.4743913345202446
Validation loss: 2.4957708560452865

Epoch: 6| Step: 6
Training loss: 3.303385448744005
Validation loss: 2.4899799111551673

Epoch: 6| Step: 7
Training loss: 2.833914211043482
Validation loss: 2.4884049924802154

Epoch: 6| Step: 8
Training loss: 3.2604050966165183
Validation loss: 2.495579923181764

Epoch: 6| Step: 9
Training loss: 2.8101902272193566
Validation loss: 2.4932218725948725

Epoch: 6| Step: 10
Training loss: 2.954387415147055
Validation loss: 2.499811149970641

Epoch: 6| Step: 11
Training loss: 2.5224796520429464
Validation loss: 2.4825314625020227

Epoch: 6| Step: 12
Training loss: 1.8751218120425026
Validation loss: 2.4795058643911667

Epoch: 6| Step: 13
Training loss: 2.486686255230423
Validation loss: 2.501926839368174

Epoch: 32| Step: 0
Training loss: 2.8469592420087753
Validation loss: 2.488008442174245

Epoch: 6| Step: 1
Training loss: 2.696948188151106
Validation loss: 2.4942363287310636

Epoch: 6| Step: 2
Training loss: 2.157332438905308
Validation loss: 2.484136213808965

Epoch: 6| Step: 3
Training loss: 2.9530205279242954
Validation loss: 2.491744321537039

Epoch: 6| Step: 4
Training loss: 3.0575657233771762
Validation loss: 2.4947465900283947

Epoch: 6| Step: 5
Training loss: 3.2713461957758594
Validation loss: 2.5031417281799726

Epoch: 6| Step: 6
Training loss: 2.8394602184702977
Validation loss: 2.499147015331413

Epoch: 6| Step: 7
Training loss: 2.605938870454449
Validation loss: 2.4849950741885594

Epoch: 6| Step: 8
Training loss: 2.0460946728681337
Validation loss: 2.492362044929669

Epoch: 6| Step: 9
Training loss: 2.6926309265777024
Validation loss: 2.4965945070563977

Epoch: 6| Step: 10
Training loss: 2.9051843145468856
Validation loss: 2.503114368374715

Epoch: 6| Step: 11
Training loss: 3.0227371705858594
Validation loss: 2.4916640840520374

Epoch: 6| Step: 12
Training loss: 3.2917007412333863
Validation loss: 2.498114073471204

Epoch: 6| Step: 13
Training loss: 1.8976080067369172
Validation loss: 2.5098103975050208

Epoch: 33| Step: 0
Training loss: 2.55259184669293
Validation loss: 2.4942989153468056

Epoch: 6| Step: 1
Training loss: 2.3694010541887627
Validation loss: 2.4942349627501024

Epoch: 6| Step: 2
Training loss: 3.0808226871349524
Validation loss: 2.4945053949208926

Epoch: 6| Step: 3
Training loss: 3.3894807493274004
Validation loss: 2.4909887813700924

Epoch: 6| Step: 4
Training loss: 2.5988281360088963
Validation loss: 2.490179971659252

Epoch: 6| Step: 5
Training loss: 2.8607927986856576
Validation loss: 2.493920229632615

Epoch: 6| Step: 6
Training loss: 2.2312529833690884
Validation loss: 2.497780101403518

Epoch: 6| Step: 7
Training loss: 2.9661696614197828
Validation loss: 2.498665727371478

Epoch: 6| Step: 8
Training loss: 2.371512211130591
Validation loss: 2.4975464614924574

Epoch: 6| Step: 9
Training loss: 3.0765420164217803
Validation loss: 2.496096845560046

Epoch: 6| Step: 10
Training loss: 3.0456430928056695
Validation loss: 2.496589284466236

Epoch: 6| Step: 11
Training loss: 3.6646635623988364
Validation loss: 2.4897310145621776

Epoch: 6| Step: 12
Training loss: 1.833069984710801
Validation loss: 2.4970426084565296

Epoch: 6| Step: 13
Training loss: 2.061793350772347
Validation loss: 2.494013904489255

Epoch: 34| Step: 0
Training loss: 2.9895042559613683
Validation loss: 2.499795726921403

Epoch: 6| Step: 1
Training loss: 2.451677417675367
Validation loss: 2.5079523619438526

Epoch: 6| Step: 2
Training loss: 3.366036415279374
Validation loss: 2.4862401163861247

Epoch: 6| Step: 3
Training loss: 2.675956776006149
Validation loss: 2.4934376839756105

Epoch: 6| Step: 4
Training loss: 2.5396802884235696
Validation loss: 2.485254190068101

Epoch: 6| Step: 5
Training loss: 2.9525278284891625
Validation loss: 2.4977518207098215

Epoch: 6| Step: 6
Training loss: 2.4263663699638403
Validation loss: 2.500181736031791

Epoch: 6| Step: 7
Training loss: 2.4351532227170285
Validation loss: 2.5068851576386737

Epoch: 6| Step: 8
Training loss: 3.280812552220504
Validation loss: 2.488231527914439

Epoch: 6| Step: 9
Training loss: 2.438868505276382
Validation loss: 2.5065748183701584

Epoch: 6| Step: 10
Training loss: 2.974130833706449
Validation loss: 2.493747612346047

Epoch: 6| Step: 11
Training loss: 2.6005302878774024
Validation loss: 2.4850382905865103

Epoch: 6| Step: 12
Training loss: 2.8789711767610826
Validation loss: 2.500769213809437

Epoch: 6| Step: 13
Training loss: 2.758218362569217
Validation loss: 2.4964800814644024

Epoch: 35| Step: 0
Training loss: 2.3262883052306167
Validation loss: 2.49655730279448

Epoch: 6| Step: 1
Training loss: 2.7204342590813324
Validation loss: 2.486854205186321

Epoch: 6| Step: 2
Training loss: 2.448467425350337
Validation loss: 2.4831557823877293

Epoch: 6| Step: 3
Training loss: 2.4737211950320375
Validation loss: 2.4942487479201856

Epoch: 6| Step: 4
Training loss: 2.7436750675474926
Validation loss: 2.484874798638552

Epoch: 6| Step: 5
Training loss: 2.7407812441543205
Validation loss: 2.4970259193542006

Epoch: 6| Step: 6
Training loss: 2.724878178944499
Validation loss: 2.4755197202463814

Epoch: 6| Step: 7
Training loss: 2.721856808519505
Validation loss: 2.486577206529093

Epoch: 6| Step: 8
Training loss: 3.100685745194779
Validation loss: 2.4862903990079017

Epoch: 6| Step: 9
Training loss: 3.4799117853230666
Validation loss: 2.4954107842232305

Epoch: 6| Step: 10
Training loss: 2.984717224631295
Validation loss: 2.4958845063011172

Epoch: 6| Step: 11
Training loss: 2.3279487491943915
Validation loss: 2.482437915117157

Epoch: 6| Step: 12
Training loss: 3.3859217496821605
Validation loss: 2.4902142012742305

Epoch: 6| Step: 13
Training loss: 2.4747778783211207
Validation loss: 2.4951014138378897

Epoch: 36| Step: 0
Training loss: 2.6535726180877983
Validation loss: 2.4826198328649673

Epoch: 6| Step: 1
Training loss: 2.661585576449068
Validation loss: 2.4873077754460065

Epoch: 6| Step: 2
Training loss: 2.381239875511613
Validation loss: 2.493130869790448

Epoch: 6| Step: 3
Training loss: 2.971710335294228
Validation loss: 2.4965127762727866

Epoch: 6| Step: 4
Training loss: 2.313363712983452
Validation loss: 2.4818680256588803

Epoch: 6| Step: 5
Training loss: 3.669435900334022
Validation loss: 2.4972140755798953

Epoch: 6| Step: 6
Training loss: 2.8986819593464253
Validation loss: 2.4977390586776482

Epoch: 6| Step: 7
Training loss: 2.6328405723999495
Validation loss: 2.495266974782427

Epoch: 6| Step: 8
Training loss: 2.9578135575556086
Validation loss: 2.4798741681004732

Epoch: 6| Step: 9
Training loss: 2.2570972827773503
Validation loss: 2.4891861296538798

Epoch: 6| Step: 10
Training loss: 3.274924147797644
Validation loss: 2.492894210855657

Epoch: 6| Step: 11
Training loss: 2.538796936232367
Validation loss: 2.4867847158940504

Epoch: 6| Step: 12
Training loss: 2.707800729663271
Validation loss: 2.496146044832549

Epoch: 6| Step: 13
Training loss: 2.5105405805932355
Validation loss: 2.4949241937123743

Epoch: 37| Step: 0
Training loss: 3.192084213580042
Validation loss: 2.4974733166752126

Epoch: 6| Step: 1
Training loss: 2.995167655009684
Validation loss: 2.491704556020026

Epoch: 6| Step: 2
Training loss: 2.528598479833601
Validation loss: 2.4985790454723618

Epoch: 6| Step: 3
Training loss: 2.7123898180544495
Validation loss: 2.491574873841755

Epoch: 6| Step: 4
Training loss: 2.1048039393511004
Validation loss: 2.475352502975199

Epoch: 6| Step: 5
Training loss: 3.18850437870715
Validation loss: 2.4856040722682335

Epoch: 6| Step: 6
Training loss: 2.7720796941720605
Validation loss: 2.4913858410255725

Epoch: 6| Step: 7
Training loss: 2.850792851506778
Validation loss: 2.4933175123696665

Epoch: 6| Step: 8
Training loss: 2.0127346399602697
Validation loss: 2.4887139591653953

Epoch: 6| Step: 9
Training loss: 3.212966908763849
Validation loss: 2.486749242174554

Epoch: 6| Step: 10
Training loss: 3.047697455101479
Validation loss: 2.4970223393267625

Epoch: 6| Step: 11
Training loss: 3.0705401697753607
Validation loss: 2.5072985318478866

Epoch: 6| Step: 12
Training loss: 2.2532941228325476
Validation loss: 2.488895449087084

Epoch: 6| Step: 13
Training loss: 2.3731828061955773
Validation loss: 2.497622061448382

Epoch: 38| Step: 0
Training loss: 2.8557863489372535
Validation loss: 2.4991836743006086

Epoch: 6| Step: 1
Training loss: 3.037703104646262
Validation loss: 2.493964362483474

Epoch: 6| Step: 2
Training loss: 2.5303122584825863
Validation loss: 2.4817904629632763

Epoch: 6| Step: 3
Training loss: 3.152551580335333
Validation loss: 2.498717273272807

Epoch: 6| Step: 4
Training loss: 2.464900721935925
Validation loss: 2.482753445036373

Epoch: 6| Step: 5
Training loss: 3.009803172675022
Validation loss: 2.494938473429588

Epoch: 6| Step: 6
Training loss: 2.9014883102767883
Validation loss: 2.491942658138438

Epoch: 6| Step: 7
Training loss: 2.035963019374312
Validation loss: 2.5084688226570204

Epoch: 6| Step: 8
Training loss: 2.526897880456271
Validation loss: 2.494549115589683

Epoch: 6| Step: 9
Training loss: 3.4064224619471104
Validation loss: 2.505176383238519

Epoch: 6| Step: 10
Training loss: 2.095618936886593
Validation loss: 2.4958840461391554

Epoch: 6| Step: 11
Training loss: 2.889743484939873
Validation loss: 2.501337465797209

Epoch: 6| Step: 12
Training loss: 2.274557879738128
Validation loss: 2.4924367200846396

Epoch: 6| Step: 13
Training loss: 3.572485598407652
Validation loss: 2.4815069964662744

Epoch: 39| Step: 0
Training loss: 2.4959393902780183
Validation loss: 2.498970626850537

Epoch: 6| Step: 1
Training loss: 2.987308997269284
Validation loss: 2.497242429091692

Epoch: 6| Step: 2
Training loss: 3.108951098813626
Validation loss: 2.4982622054392847

Epoch: 6| Step: 3
Training loss: 2.553394888873754
Validation loss: 2.508682280573452

Epoch: 6| Step: 4
Training loss: 2.6452622185224723
Validation loss: 2.498300548996924

Epoch: 6| Step: 5
Training loss: 2.5698704672799
Validation loss: 2.483440252987755

Epoch: 6| Step: 6
Training loss: 2.2116416645529346
Validation loss: 2.4849889508332663

Epoch: 6| Step: 7
Training loss: 1.7436447549322047
Validation loss: 2.4990624731049125

Epoch: 6| Step: 8
Training loss: 2.766887484398012
Validation loss: 2.4969990212822815

Epoch: 6| Step: 9
Training loss: 2.9869811178044157
Validation loss: 2.4904195618784324

Epoch: 6| Step: 10
Training loss: 2.646627792470071
Validation loss: 2.4747616430096593

Epoch: 6| Step: 11
Training loss: 3.1163411533139134
Validation loss: 2.4899856510729896

Epoch: 6| Step: 12
Training loss: 3.2711421229555615
Validation loss: 2.476156596715238

Epoch: 6| Step: 13
Training loss: 3.580476080071048
Validation loss: 2.4903638644797095

Epoch: 40| Step: 0
Training loss: 2.618532296465766
Validation loss: 2.483865799370449

Epoch: 6| Step: 1
Training loss: 3.0767529523108
Validation loss: 2.4949195851800194

Epoch: 6| Step: 2
Training loss: 3.1573420986341687
Validation loss: 2.488240801684223

Epoch: 6| Step: 3
Training loss: 2.796476793978279
Validation loss: 2.498475887903022

Epoch: 6| Step: 4
Training loss: 2.23051514051199
Validation loss: 2.4980395752160502

Epoch: 6| Step: 5
Training loss: 2.4773700733163735
Validation loss: 2.4922703992050863

Epoch: 6| Step: 6
Training loss: 2.6679495368142834
Validation loss: 2.48430381606364

Epoch: 6| Step: 7
Training loss: 2.900460824855422
Validation loss: 2.497423310533793

Epoch: 6| Step: 8
Training loss: 3.200059270309863
Validation loss: 2.4944217447105745

Epoch: 6| Step: 9
Training loss: 2.0468253792142614
Validation loss: 2.485465581916774

Epoch: 6| Step: 10
Training loss: 2.4897426942606473
Validation loss: 2.4922841828978717

Epoch: 6| Step: 11
Training loss: 2.7559362842796338
Validation loss: 2.497114026246716

Epoch: 6| Step: 12
Training loss: 3.2146153235600052
Validation loss: 2.479288026013116

Epoch: 6| Step: 13
Training loss: 2.9573260101190995
Validation loss: 2.4881076923178913

Epoch: 41| Step: 0
Training loss: 3.2762146597320307
Validation loss: 2.494824812960464

Epoch: 6| Step: 1
Training loss: 2.8683115239693118
Validation loss: 2.491557174232037

Epoch: 6| Step: 2
Training loss: 2.3053345887133108
Validation loss: 2.491225373309548

Epoch: 6| Step: 3
Training loss: 3.2240195979807664
Validation loss: 2.496336348514826

Epoch: 6| Step: 4
Training loss: 2.653761472169621
Validation loss: 2.505898701058654

Epoch: 6| Step: 5
Training loss: 2.640487215860282
Validation loss: 2.4971609392113767

Epoch: 6| Step: 6
Training loss: 2.2606837617315345
Validation loss: 2.497867110203647

Epoch: 6| Step: 7
Training loss: 2.6322556990740122
Validation loss: 2.499740011273423

Epoch: 6| Step: 8
Training loss: 3.362537687296059
Validation loss: 2.4904274882542854

Epoch: 6| Step: 9
Training loss: 2.389197927145615
Validation loss: 2.5009341832572742

Epoch: 6| Step: 10
Training loss: 2.818955367022878
Validation loss: 2.4995470174975307

Epoch: 6| Step: 11
Training loss: 2.4387683014058337
Validation loss: 2.4989866212735237

Epoch: 6| Step: 12
Training loss: 2.8022345652179435
Validation loss: 2.483429109348315

Epoch: 6| Step: 13
Training loss: 2.9264950053532335
Validation loss: 2.4985743508245104

Epoch: 42| Step: 0
Training loss: 2.910357945768451
Validation loss: 2.5015536167173846

Epoch: 6| Step: 1
Training loss: 3.1519275943650147
Validation loss: 2.4899864582639593

Epoch: 6| Step: 2
Training loss: 1.9873674312499974
Validation loss: 2.4972705174247003

Epoch: 6| Step: 3
Training loss: 3.6971579332985547
Validation loss: 2.481689640489419

Epoch: 6| Step: 4
Training loss: 2.7862469649437402
Validation loss: 2.494985072807468

Epoch: 6| Step: 5
Training loss: 2.5766970292243943
Validation loss: 2.4911691793814836

Epoch: 6| Step: 6
Training loss: 1.8205036505013241
Validation loss: 2.493004179191773

Epoch: 6| Step: 7
Training loss: 2.345022949399265
Validation loss: 2.4943749960599457

Epoch: 6| Step: 8
Training loss: 2.7465435327162964
Validation loss: 2.4960762118441173

Epoch: 6| Step: 9
Training loss: 2.7899518131210357
Validation loss: 2.480649088352616

Epoch: 6| Step: 10
Training loss: 2.825037329773151
Validation loss: 2.483620964457329

Epoch: 6| Step: 11
Training loss: 2.7802312463869314
Validation loss: 2.4830996830493035

Epoch: 6| Step: 12
Training loss: 2.895168218913731
Validation loss: 2.477946247750207

Epoch: 6| Step: 13
Training loss: 3.13496458188725
Validation loss: 2.483677907711503

Epoch: 43| Step: 0
Training loss: 3.353119744032752
Validation loss: 2.4779029257674785

Epoch: 6| Step: 1
Training loss: 2.2901317195407516
Validation loss: 2.4875957301779232

Epoch: 6| Step: 2
Training loss: 2.599899722879858
Validation loss: 2.497978701802118

Epoch: 6| Step: 3
Training loss: 3.125607545922541
Validation loss: 2.4874495076293353

Epoch: 6| Step: 4
Training loss: 2.8612661309410408
Validation loss: 2.491360523335454

Epoch: 6| Step: 5
Training loss: 2.970116070107306
Validation loss: 2.4928599542796683

Epoch: 6| Step: 6
Training loss: 2.8307607602786913
Validation loss: 2.492316697195653

Epoch: 6| Step: 7
Training loss: 2.176892091178555
Validation loss: 2.489055020567047

Epoch: 6| Step: 8
Training loss: 2.767833797514595
Validation loss: 2.5070482654321524

Epoch: 6| Step: 9
Training loss: 2.248961102859561
Validation loss: 2.4952933901584813

Epoch: 6| Step: 10
Training loss: 2.89541773020707
Validation loss: 2.487359201139469

Epoch: 6| Step: 11
Training loss: 2.929207317419314
Validation loss: 2.4920999379989683

Epoch: 6| Step: 12
Training loss: 2.3453849684709738
Validation loss: 2.4791191632862626

Epoch: 6| Step: 13
Training loss: 3.0080680441363516
Validation loss: 2.492444786101791

Epoch: 44| Step: 0
Training loss: 2.6049740162906265
Validation loss: 2.4889353130457503

Epoch: 6| Step: 1
Training loss: 3.2593792777248267
Validation loss: 2.48847832658096

Epoch: 6| Step: 2
Training loss: 2.469327931420176
Validation loss: 2.48832616077946

Epoch: 6| Step: 3
Training loss: 2.814813975934504
Validation loss: 2.4817319485883718

Epoch: 6| Step: 4
Training loss: 2.6084939531428404
Validation loss: 2.497719701504309

Epoch: 6| Step: 5
Training loss: 2.0145071315719627
Validation loss: 2.4913362488169124

Epoch: 6| Step: 6
Training loss: 3.090274284065548
Validation loss: 2.4897287441039855

Epoch: 6| Step: 7
Training loss: 2.764774515018627
Validation loss: 2.4866929357551446

Epoch: 6| Step: 8
Training loss: 3.1708692638017353
Validation loss: 2.496292267868957

Epoch: 6| Step: 9
Training loss: 2.765888697234931
Validation loss: 2.4970809897346276

Epoch: 6| Step: 10
Training loss: 2.9307688434076082
Validation loss: 2.4824360428116314

Epoch: 6| Step: 11
Training loss: 2.419810147049994
Validation loss: 2.4840746023550224

Epoch: 6| Step: 12
Training loss: 2.837283989166517
Validation loss: 2.4876993456138434

Epoch: 6| Step: 13
Training loss: 2.655642092087081
Validation loss: 2.485953341130711

Epoch: 45| Step: 0
Training loss: 2.428388634782692
Validation loss: 2.4880288357415052

Epoch: 6| Step: 1
Training loss: 2.3513179411089142
Validation loss: 2.481615145037241

Epoch: 6| Step: 2
Training loss: 3.1172453222433187
Validation loss: 2.497202234744349

Epoch: 6| Step: 3
Training loss: 3.0849688204730716
Validation loss: 2.486057868546735

Epoch: 6| Step: 4
Training loss: 2.7594717075772737
Validation loss: 2.4939957678462474

Epoch: 6| Step: 5
Training loss: 2.428439589536222
Validation loss: 2.485830120141672

Epoch: 6| Step: 6
Training loss: 2.455203591041263
Validation loss: 2.4954549677624924

Epoch: 6| Step: 7
Training loss: 3.0790774038657394
Validation loss: 2.494581489890412

Epoch: 6| Step: 8
Training loss: 3.195887879888829
Validation loss: 2.494997471829417

Epoch: 6| Step: 9
Training loss: 2.4396812265129046
Validation loss: 2.4938924887966993

Epoch: 6| Step: 10
Training loss: 3.058634283934797
Validation loss: 2.4799780668674143

Epoch: 6| Step: 11
Training loss: 3.0553992337400135
Validation loss: 2.4949051471119503

Epoch: 6| Step: 12
Training loss: 2.4526942172516826
Validation loss: 2.496359747716682

Epoch: 6| Step: 13
Training loss: 2.351212282177391
Validation loss: 2.4879229011262227

Epoch: 46| Step: 0
Training loss: 2.9803117983228145
Validation loss: 2.4937647248298402

Epoch: 6| Step: 1
Training loss: 3.061653331574576
Validation loss: 2.4803016979062393

Epoch: 6| Step: 2
Training loss: 2.5280002407605022
Validation loss: 2.481898248500655

Epoch: 6| Step: 3
Training loss: 2.7705771726976036
Validation loss: 2.483873154226789

Epoch: 6| Step: 4
Training loss: 3.123150697930284
Validation loss: 2.4928602874790964

Epoch: 6| Step: 5
Training loss: 1.6671116076055421
Validation loss: 2.4965908309126723

Epoch: 6| Step: 6
Training loss: 2.128882171774673
Validation loss: 2.4873907115416096

Epoch: 6| Step: 7
Training loss: 3.655656032733808
Validation loss: 2.4952781426100104

Epoch: 6| Step: 8
Training loss: 2.680382819092222
Validation loss: 2.482556622394204

Epoch: 6| Step: 9
Training loss: 2.451053984341282
Validation loss: 2.483660977586672

Epoch: 6| Step: 10
Training loss: 2.807434436987079
Validation loss: 2.4915481216798745

Epoch: 6| Step: 11
Training loss: 2.982826350376271
Validation loss: 2.4972371062240732

Epoch: 6| Step: 12
Training loss: 2.2270465525440835
Validation loss: 2.5038657868650294

Epoch: 6| Step: 13
Training loss: 3.0365213395261934
Validation loss: 2.4933001212557313

Epoch: 47| Step: 0
Training loss: 3.570686437611428
Validation loss: 2.4867558916025345

Epoch: 6| Step: 1
Training loss: 2.413694878397727
Validation loss: 2.4893532848061626

Epoch: 6| Step: 2
Training loss: 2.4010218511220236
Validation loss: 2.4784191869873666

Epoch: 6| Step: 3
Training loss: 2.908147213144113
Validation loss: 2.511944833950839

Epoch: 6| Step: 4
Training loss: 2.3613689157198476
Validation loss: 2.499234641242779

Epoch: 6| Step: 5
Training loss: 3.09645886748159
Validation loss: 2.4918847427501363

Epoch: 6| Step: 6
Training loss: 3.0999159586189573
Validation loss: 2.491139088568892

Epoch: 6| Step: 7
Training loss: 2.530179397796563
Validation loss: 2.488588720806267

Epoch: 6| Step: 8
Training loss: 2.2761585429638944
Validation loss: 2.4946337634997646

Epoch: 6| Step: 9
Training loss: 3.1049772053494604
Validation loss: 2.488763405850437

Epoch: 6| Step: 10
Training loss: 3.4538578964538313
Validation loss: 2.4900662237085918

Epoch: 6| Step: 11
Training loss: 1.8749567662658815
Validation loss: 2.49651046320503

Epoch: 6| Step: 12
Training loss: 2.6233880679646235
Validation loss: 2.490064275804583

Epoch: 6| Step: 13
Training loss: 1.6905836258935925
Validation loss: 2.484587239482749

Epoch: 48| Step: 0
Training loss: 2.138301172745549
Validation loss: 2.4971423398293213

Epoch: 6| Step: 1
Training loss: 2.769049986008235
Validation loss: 2.478565649564387

Epoch: 6| Step: 2
Training loss: 2.57004163062669
Validation loss: 2.494055832893238

Epoch: 6| Step: 3
Training loss: 2.6991906613159533
Validation loss: 2.497650498600891

Epoch: 6| Step: 4
Training loss: 3.4815577591851024
Validation loss: 2.487369016941029

Epoch: 6| Step: 5
Training loss: 2.4250383944274
Validation loss: 2.4826280929002165

Epoch: 6| Step: 6
Training loss: 2.900233614489654
Validation loss: 2.486414301400184

Epoch: 6| Step: 7
Training loss: 2.5393863412771385
Validation loss: 2.497497915449193

Epoch: 6| Step: 8
Training loss: 2.2611679961442737
Validation loss: 2.4844824448539264

Epoch: 6| Step: 9
Training loss: 2.9206298786288296
Validation loss: 2.4866998038931776

Epoch: 6| Step: 10
Training loss: 1.7870777565082043
Validation loss: 2.5013518570006843

Epoch: 6| Step: 11
Training loss: 3.704294655773154
Validation loss: 2.4955634569764817

Epoch: 6| Step: 12
Training loss: 2.853359544520742
Validation loss: 2.4893550478950903

Epoch: 6| Step: 13
Training loss: 2.798508321695528
Validation loss: 2.487614518422074

Epoch: 49| Step: 0
Training loss: 2.549819274182712
Validation loss: 2.487229734577296

Epoch: 6| Step: 1
Training loss: 2.500269303122119
Validation loss: 2.4990351476299857

Epoch: 6| Step: 2
Training loss: 3.0784104887734087
Validation loss: 2.4978747943385784

Epoch: 6| Step: 3
Training loss: 2.3842250951838406
Validation loss: 2.486876717374885

Epoch: 6| Step: 4
Training loss: 2.9807669514157498
Validation loss: 2.4855904248066794

Epoch: 6| Step: 5
Training loss: 2.743563141249345
Validation loss: 2.4924059147971565

Epoch: 6| Step: 6
Training loss: 2.3604044910284108
Validation loss: 2.499969152034642

Epoch: 6| Step: 7
Training loss: 2.6599419461275464
Validation loss: 2.478415999009497

Epoch: 6| Step: 8
Training loss: 2.01990875906772
Validation loss: 2.485485649783015

Epoch: 6| Step: 9
Training loss: 2.5671504551448843
Validation loss: 2.4859076006250618

Epoch: 6| Step: 10
Training loss: 3.0507634317461414
Validation loss: 2.495820703213783

Epoch: 6| Step: 11
Training loss: 3.2619842238521257
Validation loss: 2.4820257703998676

Epoch: 6| Step: 12
Training loss: 3.0253203293358495
Validation loss: 2.4991819130164874

Epoch: 6| Step: 13
Training loss: 3.2116033230367997
Validation loss: 2.499518728590051

Epoch: 50| Step: 0
Training loss: 2.660915092871196
Validation loss: 2.489355250773818

Epoch: 6| Step: 1
Training loss: 2.7150056859302873
Validation loss: 2.4821706485801776

Epoch: 6| Step: 2
Training loss: 2.8134957140423316
Validation loss: 2.49356094335779

Epoch: 6| Step: 3
Training loss: 2.7414805487291094
Validation loss: 2.4782688158880823

Epoch: 6| Step: 4
Training loss: 2.4926930455909235
Validation loss: 2.4767999793654245

Epoch: 6| Step: 5
Training loss: 2.3443507124055367
Validation loss: 2.4866267338494943

Epoch: 6| Step: 6
Training loss: 2.612875777236742
Validation loss: 2.4819712604676267

Epoch: 6| Step: 7
Training loss: 2.92996841101169
Validation loss: 2.493085863554524

Epoch: 6| Step: 8
Training loss: 3.1009583929838676
Validation loss: 2.494558679319606

Epoch: 6| Step: 9
Training loss: 2.355626140542588
Validation loss: 2.477588146550542

Epoch: 6| Step: 10
Training loss: 3.461277241101976
Validation loss: 2.4804647481658515

Epoch: 6| Step: 11
Training loss: 2.8529474109144317
Validation loss: 2.4856234222022353

Epoch: 6| Step: 12
Training loss: 2.6029681283272375
Validation loss: 2.487523490319531

Epoch: 6| Step: 13
Training loss: 2.6483824057460867
Validation loss: 2.490075475717053

Epoch: 51| Step: 0
Training loss: 2.8821200234320177
Validation loss: 2.4854731785556345

Epoch: 6| Step: 1
Training loss: 2.2699567753713823
Validation loss: 2.4892424198431153

Epoch: 6| Step: 2
Training loss: 1.9722382659737125
Validation loss: 2.4965031235008115

Epoch: 6| Step: 3
Training loss: 3.02965983731811
Validation loss: 2.488467233318222

Epoch: 6| Step: 4
Training loss: 3.4756264101412127
Validation loss: 2.479552524789565

Epoch: 6| Step: 5
Training loss: 2.555253645667112
Validation loss: 2.4911387160332983

Epoch: 6| Step: 6
Training loss: 3.0389649049440246
Validation loss: 2.4809303015120228

Epoch: 6| Step: 7
Training loss: 2.278722538701505
Validation loss: 2.4877751098147662

Epoch: 6| Step: 8
Training loss: 3.1996740592423136
Validation loss: 2.4788086397315503

Epoch: 6| Step: 9
Training loss: 1.797850435288722
Validation loss: 2.4991968423476303

Epoch: 6| Step: 10
Training loss: 2.8063127914234514
Validation loss: 2.4985991408869315

Epoch: 6| Step: 11
Training loss: 2.8450405001313595
Validation loss: 2.4864130218552405

Epoch: 6| Step: 12
Training loss: 2.933603827691578
Validation loss: 2.473726040999277

Epoch: 6| Step: 13
Training loss: 2.6705599219894007
Validation loss: 2.4792772256202285

Epoch: 52| Step: 0
Training loss: 2.735133213097653
Validation loss: 2.4859997304705486

Epoch: 6| Step: 1
Training loss: 3.165079890443624
Validation loss: 2.502376497424812

Epoch: 6| Step: 2
Training loss: 2.6793928957555737
Validation loss: 2.477875129506078

Epoch: 6| Step: 3
Training loss: 2.116191559382897
Validation loss: 2.48789765536229

Epoch: 6| Step: 4
Training loss: 3.1087225608090225
Validation loss: 2.4924096984284114

Epoch: 6| Step: 5
Training loss: 3.0240039541228745
Validation loss: 2.4835661659290413

Epoch: 6| Step: 6
Training loss: 3.101095707355973
Validation loss: 2.4927736746968665

Epoch: 6| Step: 7
Training loss: 1.9855771966210367
Validation loss: 2.4821553018295477

Epoch: 6| Step: 8
Training loss: 2.9473628589929
Validation loss: 2.4965324524086783

Epoch: 6| Step: 9
Training loss: 3.1583574265607006
Validation loss: 2.4810523264802598

Epoch: 6| Step: 10
Training loss: 1.9454636112948283
Validation loss: 2.499949726501653

Epoch: 6| Step: 11
Training loss: 3.3261277200900694
Validation loss: 2.4977819242317585

Epoch: 6| Step: 12
Training loss: 1.9663667331585075
Validation loss: 2.4856600325748466

Epoch: 6| Step: 13
Training loss: 2.5555730095787905
Validation loss: 2.483037983928316

Epoch: 53| Step: 0
Training loss: 2.5914568588195803
Validation loss: 2.4901561180267042

Epoch: 6| Step: 1
Training loss: 2.7268269145607054
Validation loss: 2.4970704762521643

Epoch: 6| Step: 2
Training loss: 2.5551639777489887
Validation loss: 2.478487862503302

Epoch: 6| Step: 3
Training loss: 2.581413806954789
Validation loss: 2.487192718523597

Epoch: 6| Step: 4
Training loss: 2.8747876959655647
Validation loss: 2.500694264639991

Epoch: 6| Step: 5
Training loss: 2.4190294838630693
Validation loss: 2.493401542006104

Epoch: 6| Step: 6
Training loss: 2.4558223788928917
Validation loss: 2.486254338742974

Epoch: 6| Step: 7
Training loss: 3.014819101755293
Validation loss: 2.4786964777899896

Epoch: 6| Step: 8
Training loss: 3.1044530203077403
Validation loss: 2.488014034141643

Epoch: 6| Step: 9
Training loss: 2.7929048971066357
Validation loss: 2.481733613791304

Epoch: 6| Step: 10
Training loss: 2.611024072788146
Validation loss: 2.48614233122673

Epoch: 6| Step: 11
Training loss: 2.584367073535872
Validation loss: 2.4793411876633678

Epoch: 6| Step: 12
Training loss: 3.4335145908643976
Validation loss: 2.5011764096528104

Epoch: 6| Step: 13
Training loss: 1.9063209145284803
Validation loss: 2.4847980514942853

Epoch: 54| Step: 0
Training loss: 3.2163883174445447
Validation loss: 2.493618281101411

Epoch: 6| Step: 1
Training loss: 2.5068235735110953
Validation loss: 2.4968576412215358

Epoch: 6| Step: 2
Training loss: 3.061324068967024
Validation loss: 2.481310379741395

Epoch: 6| Step: 3
Training loss: 2.3067636222896035
Validation loss: 2.486899214927646

Epoch: 6| Step: 4
Training loss: 2.3161711549386257
Validation loss: 2.491424125774186

Epoch: 6| Step: 5
Training loss: 3.2099376151894607
Validation loss: 2.4837282927812683

Epoch: 6| Step: 6
Training loss: 2.252566675010134
Validation loss: 2.498081330349086

Epoch: 6| Step: 7
Training loss: 2.463743130361269
Validation loss: 2.488666849885217

Epoch: 6| Step: 8
Training loss: 2.6418853099685897
Validation loss: 2.494088053246618

Epoch: 6| Step: 9
Training loss: 3.12984731478505
Validation loss: 2.483605809888512

Epoch: 6| Step: 10
Training loss: 2.7401779071104064
Validation loss: 2.4764398412665085

Epoch: 6| Step: 11
Training loss: 2.8596438088725398
Validation loss: 2.4819589740317105

Epoch: 6| Step: 12
Training loss: 2.7118338826146733
Validation loss: 2.496670618445002

Epoch: 6| Step: 13
Training loss: 2.758215164311726
Validation loss: 2.490283025579612

Epoch: 55| Step: 0
Training loss: 2.906979705099328
Validation loss: 2.480335654545201

Epoch: 6| Step: 1
Training loss: 2.6018251395321825
Validation loss: 2.486287357229474

Epoch: 6| Step: 2
Training loss: 2.682355881203181
Validation loss: 2.496075557601554

Epoch: 6| Step: 3
Training loss: 2.9538560396420284
Validation loss: 2.4854905759573196

Epoch: 6| Step: 4
Training loss: 2.8324802647818075
Validation loss: 2.4946225650404488

Epoch: 6| Step: 5
Training loss: 2.9317102392217778
Validation loss: 2.4825562774852403

Epoch: 6| Step: 6
Training loss: 3.1451356295161195
Validation loss: 2.480723133306184

Epoch: 6| Step: 7
Training loss: 2.3091822356634526
Validation loss: 2.498979395020053

Epoch: 6| Step: 8
Training loss: 2.6630750151614118
Validation loss: 2.4793902432754185

Epoch: 6| Step: 9
Training loss: 2.6734997900924755
Validation loss: 2.491718843899922

Epoch: 6| Step: 10
Training loss: 2.541765482081236
Validation loss: 2.4719580993585804

Epoch: 6| Step: 11
Training loss: 2.4387233305254457
Validation loss: 2.4844105220666477

Epoch: 6| Step: 12
Training loss: 2.4124146797233226
Validation loss: 2.4846082193397394

Epoch: 6| Step: 13
Training loss: 3.0320307719095863
Validation loss: 2.482014122558162

Epoch: 56| Step: 0
Training loss: 2.3438919024425364
Validation loss: 2.492995677941175

Epoch: 6| Step: 1
Training loss: 2.3689681302491663
Validation loss: 2.4802529776429227

Epoch: 6| Step: 2
Training loss: 3.0147324263992656
Validation loss: 2.4938816943427344

Epoch: 6| Step: 3
Training loss: 1.7027126828076697
Validation loss: 2.4863812753198116

Epoch: 6| Step: 4
Training loss: 2.897181153811492
Validation loss: 2.4961320025952154

Epoch: 6| Step: 5
Training loss: 2.798791665520552
Validation loss: 2.4781078391461207

Epoch: 6| Step: 6
Training loss: 3.102477584868554
Validation loss: 2.4858479131248816

Epoch: 6| Step: 7
Training loss: 2.5630575364375088
Validation loss: 2.4854683730267033

Epoch: 6| Step: 8
Training loss: 2.59591455796853
Validation loss: 2.4894769400164862

Epoch: 6| Step: 9
Training loss: 2.8458237684618806
Validation loss: 2.499841655823398

Epoch: 6| Step: 10
Training loss: 3.0677380044967006
Validation loss: 2.4984810696107376

Epoch: 6| Step: 11
Training loss: 2.810688198160342
Validation loss: 2.4888931876519664

Epoch: 6| Step: 12
Training loss: 3.1092388180762236
Validation loss: 2.5013127315715424

Epoch: 6| Step: 13
Training loss: 2.4185846434854947
Validation loss: 2.506841903690865

Epoch: 57| Step: 0
Training loss: 2.996383712643218
Validation loss: 2.48249332769892

Epoch: 6| Step: 1
Training loss: 3.179718542298948
Validation loss: 2.5018192716645453

Epoch: 6| Step: 2
Training loss: 2.718746667618737
Validation loss: 2.496260608975473

Epoch: 6| Step: 3
Training loss: 2.7701449770119524
Validation loss: 2.4683437851751306

Epoch: 6| Step: 4
Training loss: 3.2298488921731265
Validation loss: 2.4886867436006317

Epoch: 6| Step: 5
Training loss: 1.9841843761287685
Validation loss: 2.489529723447775

Epoch: 6| Step: 6
Training loss: 2.158427590137143
Validation loss: 2.4751712287397667

Epoch: 6| Step: 7
Training loss: 2.578000984677052
Validation loss: 2.4886585738385003

Epoch: 6| Step: 8
Training loss: 2.31531971348913
Validation loss: 2.4777489056327444

Epoch: 6| Step: 9
Training loss: 2.7791844959821432
Validation loss: 2.494446040075943

Epoch: 6| Step: 10
Training loss: 3.1697661807594004
Validation loss: 2.491833337407405

Epoch: 6| Step: 11
Training loss: 2.8415726930474694
Validation loss: 2.4826714537620993

Epoch: 6| Step: 12
Training loss: 2.4826452603439937
Validation loss: 2.4903070077069684

Epoch: 6| Step: 13
Training loss: 2.5332087720397736
Validation loss: 2.4768386365456827

Epoch: 58| Step: 0
Training loss: 2.502504048377544
Validation loss: 2.4950248291948967

Epoch: 6| Step: 1
Training loss: 2.6399611140768515
Validation loss: 2.4917468576603867

Epoch: 6| Step: 2
Training loss: 2.4857624905865268
Validation loss: 2.490705130194101

Epoch: 6| Step: 3
Training loss: 2.8671875
Validation loss: 2.4939953433131805

Epoch: 6| Step: 4
Training loss: 2.0957474930502635
Validation loss: 2.484981609080086

Epoch: 6| Step: 5
Training loss: 2.983494973376786
Validation loss: 2.4851254329278127

Epoch: 6| Step: 6
Training loss: 2.172798454407155
Validation loss: 2.4689091508729115

Epoch: 6| Step: 7
Training loss: 2.9976457259108003
Validation loss: 2.4805573199337534

Epoch: 6| Step: 8
Training loss: 3.113120119144249
Validation loss: 2.4926733432622186

Epoch: 6| Step: 9
Training loss: 2.5404558789409153
Validation loss: 2.4875030524155295

Epoch: 6| Step: 10
Training loss: 3.096834436915071
Validation loss: 2.4879152109845295

Epoch: 6| Step: 11
Training loss: 3.208952278445692
Validation loss: 2.493292265705113

Epoch: 6| Step: 12
Training loss: 2.4652821775107605
Validation loss: 2.4858397174160425

Epoch: 6| Step: 13
Training loss: 2.8278064864557666
Validation loss: 2.4811641752766245

Epoch: 59| Step: 0
Training loss: 2.5510528560053984
Validation loss: 2.4995096853850076

Epoch: 6| Step: 1
Training loss: 2.682507068663827
Validation loss: 2.4819768732672927

Epoch: 6| Step: 2
Training loss: 2.070759735744036
Validation loss: 2.4845763620386774

Epoch: 6| Step: 3
Training loss: 2.9421774776407563
Validation loss: 2.4961412942567294

Epoch: 6| Step: 4
Training loss: 2.6980090818938036
Validation loss: 2.4905970098946044

Epoch: 6| Step: 5
Training loss: 2.501396647381322
Validation loss: 2.4896454389949225

Epoch: 6| Step: 6
Training loss: 2.6257113446569496
Validation loss: 2.4888955809311084

Epoch: 6| Step: 7
Training loss: 2.6956241151103764
Validation loss: 2.4774053501964715

Epoch: 6| Step: 8
Training loss: 2.4411115056456567
Validation loss: 2.4958195692140928

Epoch: 6| Step: 9
Training loss: 3.061071880325347
Validation loss: 2.4866108815852517

Epoch: 6| Step: 10
Training loss: 3.6393312439342713
Validation loss: 2.4887834512455678

Epoch: 6| Step: 11
Training loss: 2.6420328574379037
Validation loss: 2.4735919660823624

Epoch: 6| Step: 12
Training loss: 2.323351170796571
Validation loss: 2.485902058576705

Epoch: 6| Step: 13
Training loss: 3.136234839939835
Validation loss: 2.48329457602188

Epoch: 60| Step: 0
Training loss: 3.4324174239526024
Validation loss: 2.4829568683134675

Epoch: 6| Step: 1
Training loss: 2.213070329337225
Validation loss: 2.4818622855601573

Epoch: 6| Step: 2
Training loss: 2.5927149534364453
Validation loss: 2.481878034904904

Epoch: 6| Step: 3
Training loss: 2.0219591550691787
Validation loss: 2.478769895370848

Epoch: 6| Step: 4
Training loss: 2.8911056531339367
Validation loss: 2.4902146552775113

Epoch: 6| Step: 5
Training loss: 2.8073971551008783
Validation loss: 2.4779875272611935

Epoch: 6| Step: 6
Training loss: 3.283583329134517
Validation loss: 2.494915989799591

Epoch: 6| Step: 7
Training loss: 2.371938438723123
Validation loss: 2.4854417913422098

Epoch: 6| Step: 8
Training loss: 2.6723767974974515
Validation loss: 2.4834979761364813

Epoch: 6| Step: 9
Training loss: 3.031818769126566
Validation loss: 2.4823649220392605

Epoch: 6| Step: 10
Training loss: 2.528858703599993
Validation loss: 2.487072792002502

Epoch: 6| Step: 11
Training loss: 2.6788686496591176
Validation loss: 2.4770943116694477

Epoch: 6| Step: 12
Training loss: 2.214576449402965
Validation loss: 2.483981342204584

Epoch: 6| Step: 13
Training loss: 3.185416644046269
Validation loss: 2.493446038749839

Epoch: 61| Step: 0
Training loss: 3.199499693145475
Validation loss: 2.485029105965556

Epoch: 6| Step: 1
Training loss: 3.1686281019010005
Validation loss: 2.482196333699425

Epoch: 6| Step: 2
Training loss: 2.4125916774901883
Validation loss: 2.4848524849716243

Epoch: 6| Step: 3
Training loss: 2.929073177779301
Validation loss: 2.4974520661121433

Epoch: 6| Step: 4
Training loss: 2.6450548203092894
Validation loss: 2.4852270294552943

Epoch: 6| Step: 5
Training loss: 3.4523242695231393
Validation loss: 2.4980465086258605

Epoch: 6| Step: 6
Training loss: 2.7738131416495233
Validation loss: 2.4981043673651087

Epoch: 6| Step: 7
Training loss: 2.3521637512281224
Validation loss: 2.4859752855289514

Epoch: 6| Step: 8
Training loss: 1.6838397136256353
Validation loss: 2.490303404637673

Epoch: 6| Step: 9
Training loss: 2.3908092951327924
Validation loss: 2.4873644894895035

Epoch: 6| Step: 10
Training loss: 3.076958604754229
Validation loss: 2.479191514639776

Epoch: 6| Step: 11
Training loss: 2.3979809851572598
Validation loss: 2.4857043610720595

Epoch: 6| Step: 12
Training loss: 2.340562203841998
Validation loss: 2.4787204747991556

Epoch: 6| Step: 13
Training loss: 2.8186026591765967
Validation loss: 2.486226481696196

Epoch: 62| Step: 0
Training loss: 2.6491712605869635
Validation loss: 2.4837375472392464

Epoch: 6| Step: 1
Training loss: 1.8013236000754336
Validation loss: 2.4828285960770957

Epoch: 6| Step: 2
Training loss: 2.8206442006508086
Validation loss: 2.4793538954999588

Epoch: 6| Step: 3
Training loss: 2.7635577390577013
Validation loss: 2.485908805147521

Epoch: 6| Step: 4
Training loss: 2.4497158391777822
Validation loss: 2.482448729641901

Epoch: 6| Step: 5
Training loss: 2.694437874680033
Validation loss: 2.4852278761013626

Epoch: 6| Step: 6
Training loss: 3.229697563255367
Validation loss: 2.482004770779769

Epoch: 6| Step: 7
Training loss: 3.116529046214464
Validation loss: 2.485120729886042

Epoch: 6| Step: 8
Training loss: 3.004144349105694
Validation loss: 2.4890126000491835

Epoch: 6| Step: 9
Training loss: 2.620284704653516
Validation loss: 2.490692293971795

Epoch: 6| Step: 10
Training loss: 1.8941530459723053
Validation loss: 2.484591086095474

Epoch: 6| Step: 11
Training loss: 2.3068787583470636
Validation loss: 2.477184574268891

Epoch: 6| Step: 12
Training loss: 2.7896911596456038
Validation loss: 2.4730962717428873

Epoch: 6| Step: 13
Training loss: 3.8553214465353887
Validation loss: 2.4991318887466276

Epoch: 63| Step: 0
Training loss: 2.959932745607661
Validation loss: 2.4893296271626775

Epoch: 6| Step: 1
Training loss: 2.6603877433174477
Validation loss: 2.479796903324363

Epoch: 6| Step: 2
Training loss: 2.637462641012998
Validation loss: 2.4995914022778694

Epoch: 6| Step: 3
Training loss: 2.5529053797865893
Validation loss: 2.4859699338959422

Epoch: 6| Step: 4
Training loss: 2.478394802265861
Validation loss: 2.4781664396106113

Epoch: 6| Step: 5
Training loss: 3.141960505362972
Validation loss: 2.4943744246204633

Epoch: 6| Step: 6
Training loss: 2.4748103444809466
Validation loss: 2.470843234441213

Epoch: 6| Step: 7
Training loss: 2.822110097223067
Validation loss: 2.4926093139895062

Epoch: 6| Step: 8
Training loss: 2.3259959887062442
Validation loss: 2.485466284336087

Epoch: 6| Step: 9
Training loss: 2.809303247131659
Validation loss: 2.4656067590906825

Epoch: 6| Step: 10
Training loss: 2.7434442580066305
Validation loss: 2.4890301158782493

Epoch: 6| Step: 11
Training loss: 3.084861548484053
Validation loss: 2.472893351852477

Epoch: 6| Step: 12
Training loss: 2.5809188962931717
Validation loss: 2.4859722397556085

Epoch: 6| Step: 13
Training loss: 2.7581707340808044
Validation loss: 2.4923380414322334

Epoch: 64| Step: 0
Training loss: 2.755527576395069
Validation loss: 2.4835632818439737

Epoch: 6| Step: 1
Training loss: 2.8923908510443535
Validation loss: 2.4760486073107013

Epoch: 6| Step: 2
Training loss: 2.6404208493742627
Validation loss: 2.4839927826653154

Epoch: 6| Step: 3
Training loss: 2.424314685217793
Validation loss: 2.4751616605298215

Epoch: 6| Step: 4
Training loss: 2.758744555088991
Validation loss: 2.481985777905571

Epoch: 6| Step: 5
Training loss: 2.6007153847336193
Validation loss: 2.4840720202134885

Epoch: 6| Step: 6
Training loss: 2.734878929568805
Validation loss: 2.482606895977435

Epoch: 6| Step: 7
Training loss: 2.892376508260906
Validation loss: 2.476105749037914

Epoch: 6| Step: 8
Training loss: 3.2498672164787985
Validation loss: 2.4841960002268366

Epoch: 6| Step: 9
Training loss: 2.3289000993843847
Validation loss: 2.4950702730975487

Epoch: 6| Step: 10
Training loss: 2.177763168076191
Validation loss: 2.5025300882324317

Epoch: 6| Step: 11
Training loss: 3.107493689262875
Validation loss: 2.481127277964987

Epoch: 6| Step: 12
Training loss: 2.904853731501126
Validation loss: 2.4879495387359563

Epoch: 6| Step: 13
Training loss: 2.1060882959069525
Validation loss: 2.483297611658793

Epoch: 65| Step: 0
Training loss: 2.6476848061587073
Validation loss: 2.4831079683315727

Epoch: 6| Step: 1
Training loss: 2.9696144853070097
Validation loss: 2.486155774581193

Epoch: 6| Step: 2
Training loss: 3.1446316341702127
Validation loss: 2.4905200232230866

Epoch: 6| Step: 3
Training loss: 2.413686877412562
Validation loss: 2.4966370585395

Epoch: 6| Step: 4
Training loss: 2.9737554816138987
Validation loss: 2.4808494249277966

Epoch: 6| Step: 5
Training loss: 2.5514930094257435
Validation loss: 2.496590477673996

Epoch: 6| Step: 6
Training loss: 2.6485932340942693
Validation loss: 2.4838543335539143

Epoch: 6| Step: 7
Training loss: 2.112910719451729
Validation loss: 2.482421329725074

Epoch: 6| Step: 8
Training loss: 3.022761306258206
Validation loss: 2.4896033405480518

Epoch: 6| Step: 9
Training loss: 3.1202065898283093
Validation loss: 2.4863506295788285

Epoch: 6| Step: 10
Training loss: 2.7527630536656194
Validation loss: 2.482277936851281

Epoch: 6| Step: 11
Training loss: 2.758943061243843
Validation loss: 2.476235935932203

Epoch: 6| Step: 12
Training loss: 2.117073985079591
Validation loss: 2.485040178469123

Epoch: 6| Step: 13
Training loss: 2.2435491062857693
Validation loss: 2.4825247428964734

Epoch: 66| Step: 0
Training loss: 3.426011602993372
Validation loss: 2.4886172498501438

Epoch: 6| Step: 1
Training loss: 3.0977803189805253
Validation loss: 2.4712054869865727

Epoch: 6| Step: 2
Training loss: 2.065243601250788
Validation loss: 2.4893187807397283

Epoch: 6| Step: 3
Training loss: 2.4722056287096708
Validation loss: 2.501319107578775

Epoch: 6| Step: 4
Training loss: 2.7670421525552036
Validation loss: 2.469800509334414

Epoch: 6| Step: 5
Training loss: 2.4891628937191927
Validation loss: 2.479642692220166

Epoch: 6| Step: 6
Training loss: 3.056078659902702
Validation loss: 2.4969047941238656

Epoch: 6| Step: 7
Training loss: 2.8351101259509357
Validation loss: 2.4887935068449467

Epoch: 6| Step: 8
Training loss: 2.247884073336395
Validation loss: 2.4849098658099025

Epoch: 6| Step: 9
Training loss: 2.6551985510886236
Validation loss: 2.4817790552192105

Epoch: 6| Step: 10
Training loss: 3.3672353648171365
Validation loss: 2.502445584489529

Epoch: 6| Step: 11
Training loss: 2.0549110900134115
Validation loss: 2.4927950042222244

Epoch: 6| Step: 12
Training loss: 2.009982943486205
Validation loss: 2.4928325926956534

Epoch: 6| Step: 13
Training loss: 2.8047053514512843
Validation loss: 2.4912416279021623

Epoch: 67| Step: 0
Training loss: 3.0637315103509137
Validation loss: 2.4873166991349143

Epoch: 6| Step: 1
Training loss: 2.446953268735928
Validation loss: 2.481172869984683

Epoch: 6| Step: 2
Training loss: 2.7985819291033467
Validation loss: 2.480579141026079

Epoch: 6| Step: 3
Training loss: 3.575400568788751
Validation loss: 2.480129670872457

Epoch: 6| Step: 4
Training loss: 2.6083928618376855
Validation loss: 2.4883716765344195

Epoch: 6| Step: 5
Training loss: 2.501495772168134
Validation loss: 2.485045527462071

Epoch: 6| Step: 6
Training loss: 2.352252440769268
Validation loss: 2.490540043671694

Epoch: 6| Step: 7
Training loss: 2.306717938347627
Validation loss: 2.4821798499749734

Epoch: 6| Step: 8
Training loss: 3.139311710413083
Validation loss: 2.4979258198022904

Epoch: 6| Step: 9
Training loss: 2.3481124967760527
Validation loss: 2.475419202649305

Epoch: 6| Step: 10
Training loss: 2.716042057930845
Validation loss: 2.48575601176503

Epoch: 6| Step: 11
Training loss: 2.337116262995449
Validation loss: 2.4905748658681115

Epoch: 6| Step: 12
Training loss: 2.5001121495841034
Validation loss: 2.4840150632886866

Epoch: 6| Step: 13
Training loss: 3.0098302637445746
Validation loss: 2.488062475694725

Epoch: 68| Step: 0
Training loss: 2.3064305842133903
Validation loss: 2.4891704054352037

Epoch: 6| Step: 1
Training loss: 2.699235002438408
Validation loss: 2.49904045948689

Epoch: 6| Step: 2
Training loss: 2.5752361411545817
Validation loss: 2.486403166965671

Epoch: 6| Step: 3
Training loss: 2.799468218168392
Validation loss: 2.4902540955958026

Epoch: 6| Step: 4
Training loss: 2.5705986443412954
Validation loss: 2.484359662222809

Epoch: 6| Step: 5
Training loss: 3.639276737836867
Validation loss: 2.4842539225260754

Epoch: 6| Step: 6
Training loss: 2.309873894060804
Validation loss: 2.4743377582841664

Epoch: 6| Step: 7
Training loss: 2.0568416312474604
Validation loss: 2.480885672256931

Epoch: 6| Step: 8
Training loss: 2.9819829651559013
Validation loss: 2.486227301450142

Epoch: 6| Step: 9
Training loss: 2.9211196331010942
Validation loss: 2.4958548000018617

Epoch: 6| Step: 10
Training loss: 2.3274911107140412
Validation loss: 2.478592717683039

Epoch: 6| Step: 11
Training loss: 2.8274824561157463
Validation loss: 2.497204321831715

Epoch: 6| Step: 12
Training loss: 3.039026098352043
Validation loss: 2.4853011866141617

Epoch: 6| Step: 13
Training loss: 2.167463363989025
Validation loss: 2.4798632234631808

Epoch: 69| Step: 0
Training loss: 2.793788805527823
Validation loss: 2.488112234129985

Epoch: 6| Step: 1
Training loss: 3.3415450196864485
Validation loss: 2.4903730592789963

Epoch: 6| Step: 2
Training loss: 2.7607797539836647
Validation loss: 2.4823163455468604

Epoch: 6| Step: 3
Training loss: 2.2623387147520844
Validation loss: 2.481731165570726

Epoch: 6| Step: 4
Training loss: 2.902206397111609
Validation loss: 2.4848663479849566

Epoch: 6| Step: 5
Training loss: 2.1743095529766516
Validation loss: 2.4814809436568273

Epoch: 6| Step: 6
Training loss: 2.0522891359355895
Validation loss: 2.4844755788410895

Epoch: 6| Step: 7
Training loss: 2.943454308045193
Validation loss: 2.4821778235847503

Epoch: 6| Step: 8
Training loss: 2.9155436579079024
Validation loss: 2.493973318890825

Epoch: 6| Step: 9
Training loss: 3.3796098097902756
Validation loss: 2.4812769943665454

Epoch: 6| Step: 10
Training loss: 1.983311525860741
Validation loss: 2.489625843323778

Epoch: 6| Step: 11
Training loss: 2.552283879918656
Validation loss: 2.4757272586353465

Epoch: 6| Step: 12
Training loss: 2.8169211500275284
Validation loss: 2.4873710589423927

Epoch: 6| Step: 13
Training loss: 2.264369327244298
Validation loss: 2.4705118824098613

Epoch: 70| Step: 0
Training loss: 3.419175010359201
Validation loss: 2.4956284284634513

Epoch: 6| Step: 1
Training loss: 2.232929962891189
Validation loss: 2.494046259011854

Epoch: 6| Step: 2
Training loss: 2.7038127046422185
Validation loss: 2.4811728896161935

Epoch: 6| Step: 3
Training loss: 2.7556034566790313
Validation loss: 2.4927004391948926

Epoch: 6| Step: 4
Training loss: 2.6402081944995803
Validation loss: 2.4882260466788506

Epoch: 6| Step: 5
Training loss: 3.4495618569616755
Validation loss: 2.4900769773055695

Epoch: 6| Step: 6
Training loss: 2.080819355168241
Validation loss: 2.4846180720624744

Epoch: 6| Step: 7
Training loss: 2.8900386731157237
Validation loss: 2.4790916779668066

Epoch: 6| Step: 8
Training loss: 2.70833733387187
Validation loss: 2.4858496632305918

Epoch: 6| Step: 9
Training loss: 2.5161974711651123
Validation loss: 2.491210037060006

Epoch: 6| Step: 10
Training loss: 2.148921287859566
Validation loss: 2.4879527464409477

Epoch: 6| Step: 11
Training loss: 2.708860101579031
Validation loss: 2.493769970811657

Epoch: 6| Step: 12
Training loss: 2.5490417624786676
Validation loss: 2.4849265367284894

Epoch: 6| Step: 13
Training loss: 2.2515833899371396
Validation loss: 2.4915276663715464

Epoch: 71| Step: 0
Training loss: 2.8455010357420676
Validation loss: 2.484095179924981

Epoch: 6| Step: 1
Training loss: 2.7387458091574937
Validation loss: 2.481523254280812

Epoch: 6| Step: 2
Training loss: 3.532465902651228
Validation loss: 2.4875834302988644

Epoch: 6| Step: 3
Training loss: 2.294509707834535
Validation loss: 2.4726117668951613

Epoch: 6| Step: 4
Training loss: 2.8904529417226272
Validation loss: 2.4953172183117567

Epoch: 6| Step: 5
Training loss: 2.1228204657333434
Validation loss: 2.477952825619094

Epoch: 6| Step: 6
Training loss: 2.479885435226915
Validation loss: 2.4961641417168092

Epoch: 6| Step: 7
Training loss: 3.0970463001903057
Validation loss: 2.493746127874178

Epoch: 6| Step: 8
Training loss: 2.2968529940219207
Validation loss: 2.481528975535065

Epoch: 6| Step: 9
Training loss: 3.15497646510419
Validation loss: 2.4797834523765343

Epoch: 6| Step: 10
Training loss: 1.9966778581758835
Validation loss: 2.478112102380272

Epoch: 6| Step: 11
Training loss: 2.8305289657719843
Validation loss: 2.485114307160969

Epoch: 6| Step: 12
Training loss: 2.0990249459145973
Validation loss: 2.5017218741037444

Epoch: 6| Step: 13
Training loss: 3.084265147130307
Validation loss: 2.4839861062314412

Epoch: 72| Step: 0
Training loss: 3.0739094482864404
Validation loss: 2.489144639336023

Epoch: 6| Step: 1
Training loss: 2.555661170500131
Validation loss: 2.4962533255348553

Epoch: 6| Step: 2
Training loss: 2.2605835696245222
Validation loss: 2.4876628090513724

Epoch: 6| Step: 3
Training loss: 2.472143424370184
Validation loss: 2.481511917108553

Epoch: 6| Step: 4
Training loss: 3.0789820063809525
Validation loss: 2.4725247962979298

Epoch: 6| Step: 5
Training loss: 2.799132219446913
Validation loss: 2.4754993478996434

Epoch: 6| Step: 6
Training loss: 2.0578847167160994
Validation loss: 2.4957442515810997

Epoch: 6| Step: 7
Training loss: 2.240487866747437
Validation loss: 2.483604493800966

Epoch: 6| Step: 8
Training loss: 3.324578773376522
Validation loss: 2.4918797479413692

Epoch: 6| Step: 9
Training loss: 2.9782562643225585
Validation loss: 2.4813600462805967

Epoch: 6| Step: 10
Training loss: 2.5716602073517443
Validation loss: 2.480043021802118

Epoch: 6| Step: 11
Training loss: 2.1776801816757505
Validation loss: 2.487368975972152

Epoch: 6| Step: 12
Training loss: 2.846939478122049
Validation loss: 2.4784138557592534

Epoch: 6| Step: 13
Training loss: 3.1543162247858514
Validation loss: 2.4855333825157038

Epoch: 73| Step: 0
Training loss: 2.7674115978205536
Validation loss: 2.4790824444277377

Epoch: 6| Step: 1
Training loss: 2.7596837252311475
Validation loss: 2.4848181061806405

Epoch: 6| Step: 2
Training loss: 2.604398630619524
Validation loss: 2.4711400948607807

Epoch: 6| Step: 3
Training loss: 3.084065393370565
Validation loss: 2.490118632255794

Epoch: 6| Step: 4
Training loss: 2.3968118631932596
Validation loss: 2.479877792525299

Epoch: 6| Step: 5
Training loss: 2.865913295075861
Validation loss: 2.483597324982439

Epoch: 6| Step: 6
Training loss: 2.845856106742863
Validation loss: 2.474444612768885

Epoch: 6| Step: 7
Training loss: 3.100039014263145
Validation loss: 2.490076991719168

Epoch: 6| Step: 8
Training loss: 2.4309265313794155
Validation loss: 2.4756023418979

Epoch: 6| Step: 9
Training loss: 2.8008462001872347
Validation loss: 2.489357364007094

Epoch: 6| Step: 10
Training loss: 2.5753852852283297
Validation loss: 2.4842135386653488

Epoch: 6| Step: 11
Training loss: 2.1904098775157483
Validation loss: 2.4951819122070473

Epoch: 6| Step: 12
Training loss: 2.559651536173098
Validation loss: 2.479955188134323

Epoch: 6| Step: 13
Training loss: 2.1719627088222313
Validation loss: 2.4876536021734394

Epoch: 74| Step: 0
Training loss: 2.602739589044145
Validation loss: 2.489591558251216

Epoch: 6| Step: 1
Training loss: 2.667422058482653
Validation loss: 2.4762368252525127

Epoch: 6| Step: 2
Training loss: 1.950268479962735
Validation loss: 2.487469630603003

Epoch: 6| Step: 3
Training loss: 2.7247641680099126
Validation loss: 2.4961426982220303

Epoch: 6| Step: 4
Training loss: 2.56152864979931
Validation loss: 2.4790686193904548

Epoch: 6| Step: 5
Training loss: 2.5079030053275306
Validation loss: 2.4932702669470355

Epoch: 6| Step: 6
Training loss: 3.6028229665426346
Validation loss: 2.4869964013159915

Epoch: 6| Step: 7
Training loss: 2.2251868565995636
Validation loss: 2.4838044031194957

Epoch: 6| Step: 8
Training loss: 2.6894873434755797
Validation loss: 2.4882072146274297

Epoch: 6| Step: 9
Training loss: 2.540177602190718
Validation loss: 2.495026838986144

Epoch: 6| Step: 10
Training loss: 2.7614976505764774
Validation loss: 2.4925580387840025

Epoch: 6| Step: 11
Training loss: 3.0023729158643047
Validation loss: 2.4915058917788224

Epoch: 6| Step: 12
Training loss: 2.6038415222638234
Validation loss: 2.4940034957745327

Epoch: 6| Step: 13
Training loss: 3.077935537326283
Validation loss: 2.4920931526384

Epoch: 75| Step: 0
Training loss: 2.450877915951532
Validation loss: 2.4814653322955555

Epoch: 6| Step: 1
Training loss: 2.574815973168726
Validation loss: 2.472462040074466

Epoch: 6| Step: 2
Training loss: 2.660018581239436
Validation loss: 2.489444199659916

Epoch: 6| Step: 3
Training loss: 2.788771670268838
Validation loss: 2.4828193754029737

Epoch: 6| Step: 4
Training loss: 2.766384559251133
Validation loss: 2.492907928368499

Epoch: 6| Step: 5
Training loss: 3.135864141392137
Validation loss: 2.4915185046437855

Epoch: 6| Step: 6
Training loss: 2.5134579820188496
Validation loss: 2.5006314659425146

Epoch: 6| Step: 7
Training loss: 2.3426364542200364
Validation loss: 2.474137860678055

Epoch: 6| Step: 8
Training loss: 2.80262038367929
Validation loss: 2.482092015491757

Epoch: 6| Step: 9
Training loss: 2.713389933633904
Validation loss: 2.4910965409391466

Epoch: 6| Step: 10
Training loss: 2.8152791173858875
Validation loss: 2.480328785321987

Epoch: 6| Step: 11
Training loss: 2.7889170889254036
Validation loss: 2.4990183861751274

Epoch: 6| Step: 12
Training loss: 2.0494722384796598
Validation loss: 2.4858237198755653

Epoch: 6| Step: 13
Training loss: 3.2503890758318623
Validation loss: 2.493408787509488

Epoch: 76| Step: 0
Training loss: 2.937352602892731
Validation loss: 2.496361200852772

Epoch: 6| Step: 1
Training loss: 2.553749588916389
Validation loss: 2.4827287776190325

Epoch: 6| Step: 2
Training loss: 2.670496266996499
Validation loss: 2.492337873769007

Epoch: 6| Step: 3
Training loss: 3.2380438741782847
Validation loss: 2.4912182151013544

Epoch: 6| Step: 4
Training loss: 2.4553991576724417
Validation loss: 2.4962611943610638

Epoch: 6| Step: 5
Training loss: 3.0345994200448794
Validation loss: 2.494176699214095

Epoch: 6| Step: 6
Training loss: 2.239650127982106
Validation loss: 2.4699824849557626

Epoch: 6| Step: 7
Training loss: 2.425098857634327
Validation loss: 2.482573079865915

Epoch: 6| Step: 8
Training loss: 2.398303099603626
Validation loss: 2.483043108028482

Epoch: 6| Step: 9
Training loss: 2.7920598185014467
Validation loss: 2.4837199486698087

Epoch: 6| Step: 10
Training loss: 2.6684208901515274
Validation loss: 2.48861774432047

Epoch: 6| Step: 11
Training loss: 2.9255931676653932
Validation loss: 2.4902109964822774

Epoch: 6| Step: 12
Training loss: 2.491867092666346
Validation loss: 2.4702812805193504

Epoch: 6| Step: 13
Training loss: 2.2679032019624823
Validation loss: 2.484937388911265

Epoch: 77| Step: 0
Training loss: 2.526078864279251
Validation loss: 2.4899407352192005

Epoch: 6| Step: 1
Training loss: 3.2690581021517713
Validation loss: 2.50264626802499

Epoch: 6| Step: 2
Training loss: 2.3186300323792026
Validation loss: 2.490235974807756

Epoch: 6| Step: 3
Training loss: 2.717247120058922
Validation loss: 2.4836804500071983

Epoch: 6| Step: 4
Training loss: 2.937170943639584
Validation loss: 2.4764167429874826

Epoch: 6| Step: 5
Training loss: 2.5211601255679223
Validation loss: 2.497066649376679

Epoch: 6| Step: 6
Training loss: 2.164148652603135
Validation loss: 2.4924812443382467

Epoch: 6| Step: 7
Training loss: 2.994424566353533
Validation loss: 2.481557953516251

Epoch: 6| Step: 8
Training loss: 2.3517735788909673
Validation loss: 2.4868865910051663

Epoch: 6| Step: 9
Training loss: 2.62809915609274
Validation loss: 2.476794763166824

Epoch: 6| Step: 10
Training loss: 2.330825263800329
Validation loss: 2.4789593700323493

Epoch: 6| Step: 11
Training loss: 3.2355533241024403
Validation loss: 2.4844889073912046

Epoch: 6| Step: 12
Training loss: 2.7806370949219543
Validation loss: 2.480141589082159

Epoch: 6| Step: 13
Training loss: 2.681865907723441
Validation loss: 2.481852614026737

Epoch: 78| Step: 0
Training loss: 2.2495411298984913
Validation loss: 2.4860700883213647

Epoch: 6| Step: 1
Training loss: 2.5673850412457044
Validation loss: 2.4733412783411413

Epoch: 6| Step: 2
Training loss: 2.4615560087394672
Validation loss: 2.491725104531497

Epoch: 6| Step: 3
Training loss: 3.0894711893161033
Validation loss: 2.4937470654354614

Epoch: 6| Step: 4
Training loss: 2.4751875758134827
Validation loss: 2.4952731032238993

Epoch: 6| Step: 5
Training loss: 3.1144611415475185
Validation loss: 2.4865262028431454

Epoch: 6| Step: 6
Training loss: 2.3063478857423827
Validation loss: 2.478211769552866

Epoch: 6| Step: 7
Training loss: 2.8872872542576418
Validation loss: 2.4739506270715523

Epoch: 6| Step: 8
Training loss: 3.126247614722732
Validation loss: 2.486535867539454

Epoch: 6| Step: 9
Training loss: 2.780945321871638
Validation loss: 2.4939840571830274

Epoch: 6| Step: 10
Training loss: 2.784743012191864
Validation loss: 2.4946916232894196

Epoch: 6| Step: 11
Training loss: 2.427655023089733
Validation loss: 2.4932033595055656

Epoch: 6| Step: 12
Training loss: 2.3272124684657753
Validation loss: 2.484950392588535

Epoch: 6| Step: 13
Training loss: 2.4523313173852084
Validation loss: 2.479827362302704

Epoch: 79| Step: 0
Training loss: 2.5576995917901035
Validation loss: 2.487116807220094

Epoch: 6| Step: 1
Training loss: 2.465538640316909
Validation loss: 2.482174564005569

Epoch: 6| Step: 2
Training loss: 2.948518904960617
Validation loss: 2.4945588488889547

Epoch: 6| Step: 3
Training loss: 2.5008141145748395
Validation loss: 2.501922411780279

Epoch: 6| Step: 4
Training loss: 3.1248850992060087
Validation loss: 2.49237933458711

Epoch: 6| Step: 5
Training loss: 2.9062496718539803
Validation loss: 2.497447016741545

Epoch: 6| Step: 6
Training loss: 3.1382186552931866
Validation loss: 2.4879404627528814

Epoch: 6| Step: 7
Training loss: 2.5988698777388715
Validation loss: 2.4917547798140567

Epoch: 6| Step: 8
Training loss: 2.637644332624709
Validation loss: 2.494988967095466

Epoch: 6| Step: 9
Training loss: 2.4971440214482987
Validation loss: 2.4745733848658715

Epoch: 6| Step: 10
Training loss: 2.1704329499119472
Validation loss: 2.4778330804272

Epoch: 6| Step: 11
Training loss: 2.5945413140730924
Validation loss: 2.4857951608334576

Epoch: 6| Step: 12
Training loss: 2.4006488558719323
Validation loss: 2.4860816625065203

Epoch: 6| Step: 13
Training loss: 2.879146074220483
Validation loss: 2.4978067539758584

Epoch: 80| Step: 0
Training loss: 2.4067762715891097
Validation loss: 2.5007000332736946

Epoch: 6| Step: 1
Training loss: 2.8687849454060306
Validation loss: 2.4824681083014415

Epoch: 6| Step: 2
Training loss: 2.0936516126603566
Validation loss: 2.491493406955211

Epoch: 6| Step: 3
Training loss: 3.5035619322010785
Validation loss: 2.4785852240741257

Epoch: 6| Step: 4
Training loss: 3.37494828926314
Validation loss: 2.47414861408235

Epoch: 6| Step: 5
Training loss: 2.868536442140008
Validation loss: 2.4836525155775964

Epoch: 6| Step: 6
Training loss: 2.0605125098956036
Validation loss: 2.481389066527308

Epoch: 6| Step: 7
Training loss: 2.0455267739307232
Validation loss: 2.4929789129005515

Epoch: 6| Step: 8
Training loss: 2.6836301753918934
Validation loss: 2.4987238097998885

Epoch: 6| Step: 9
Training loss: 2.9141747501629522
Validation loss: 2.4892454992060165

Epoch: 6| Step: 10
Training loss: 2.936153509081915
Validation loss: 2.491435205871185

Epoch: 6| Step: 11
Training loss: 2.6975014464027125
Validation loss: 2.4997764733583527

Epoch: 6| Step: 12
Training loss: 2.214803490113038
Validation loss: 2.4680316197085688

Epoch: 6| Step: 13
Training loss: 1.6081148417537843
Validation loss: 2.4850671121048773

Epoch: 81| Step: 0
Training loss: 2.314051957914563
Validation loss: 2.4965643645717908

Epoch: 6| Step: 1
Training loss: 3.4301471098112235
Validation loss: 2.469816523469705

Epoch: 6| Step: 2
Training loss: 2.5871304529164574
Validation loss: 2.495872916973016

Epoch: 6| Step: 3
Training loss: 2.7479615025384896
Validation loss: 2.4552777695767882

Epoch: 6| Step: 4
Training loss: 2.2592923815383212
Validation loss: 2.488495711214834

Epoch: 6| Step: 5
Training loss: 1.9382913265558308
Validation loss: 2.4907536315989316

Epoch: 6| Step: 6
Training loss: 2.4725712042912997
Validation loss: 2.48875057094431

Epoch: 6| Step: 7
Training loss: 2.9412331093215207
Validation loss: 2.491157135896389

Epoch: 6| Step: 8
Training loss: 2.658043379401662
Validation loss: 2.484359782440558

Epoch: 6| Step: 9
Training loss: 2.933316112236566
Validation loss: 2.475613458635013

Epoch: 6| Step: 10
Training loss: 3.1713456336950934
Validation loss: 2.488622208455934

Epoch: 6| Step: 11
Training loss: 2.3841418951037223
Validation loss: 2.483684283573619

Epoch: 6| Step: 12
Training loss: 2.6924982831377453
Validation loss: 2.4779163615758413

Epoch: 6| Step: 13
Training loss: 2.542397524288597
Validation loss: 2.491646721781651

Epoch: 82| Step: 0
Training loss: 2.4933734810328696
Validation loss: 2.4869700369752956

Epoch: 6| Step: 1
Training loss: 3.1288486432548095
Validation loss: 2.4911334748014844

Epoch: 6| Step: 2
Training loss: 3.2349463801236937
Validation loss: 2.479670182287602

Epoch: 6| Step: 3
Training loss: 2.829506889489916
Validation loss: 2.4862596531589243

Epoch: 6| Step: 4
Training loss: 2.7280921470875006
Validation loss: 2.4792356232459554

Epoch: 6| Step: 5
Training loss: 2.4934059919218448
Validation loss: 2.4945568489975996

Epoch: 6| Step: 6
Training loss: 3.292703746843548
Validation loss: 2.489419714909661

Epoch: 6| Step: 7
Training loss: 3.041972593486254
Validation loss: 2.476780558945366

Epoch: 6| Step: 8
Training loss: 3.0173500937809554
Validation loss: 2.491101056719538

Epoch: 6| Step: 9
Training loss: 2.3382123504286394
Validation loss: 2.4747662657722933

Epoch: 6| Step: 10
Training loss: 2.1878511419481286
Validation loss: 2.4867204324203716

Epoch: 6| Step: 11
Training loss: 1.7241358209465532
Validation loss: 2.481346971613031

Epoch: 6| Step: 12
Training loss: 2.240918800745177
Validation loss: 2.4866136250156847

Epoch: 6| Step: 13
Training loss: 1.8839297956438161
Validation loss: 2.4803542032132952

Epoch: 83| Step: 0
Training loss: 2.4168049564476166
Validation loss: 2.507172971280811

Epoch: 6| Step: 1
Training loss: 3.489389958883393
Validation loss: 2.4997429551500177

Epoch: 6| Step: 2
Training loss: 2.610292924250193
Validation loss: 2.4944611627024416

Epoch: 6| Step: 3
Training loss: 2.0029234976744923
Validation loss: 2.480241387627763

Epoch: 6| Step: 4
Training loss: 2.5404129897054455
Validation loss: 2.4924258496400977

Epoch: 6| Step: 5
Training loss: 2.8349483225584815
Validation loss: 2.4912717246795757

Epoch: 6| Step: 6
Training loss: 2.3881747585484954
Validation loss: 2.492649591654706

Epoch: 6| Step: 7
Training loss: 2.8974676848906102
Validation loss: 2.4982956501338154

Epoch: 6| Step: 8
Training loss: 3.1732807520776265
Validation loss: 2.485142194211579

Epoch: 6| Step: 9
Training loss: 2.7993064838940884
Validation loss: 2.4861853874453836

Epoch: 6| Step: 10
Training loss: 3.07943062758561
Validation loss: 2.4768195450582255

Epoch: 6| Step: 11
Training loss: 1.7838466523469318
Validation loss: 2.480100982173413

Epoch: 6| Step: 12
Training loss: 2.362403795009191
Validation loss: 2.475025368041851

Epoch: 6| Step: 13
Training loss: 2.249812436233292
Validation loss: 2.482194589280381

Epoch: 84| Step: 0
Training loss: 2.7054320616827288
Validation loss: 2.4807807057632765

Epoch: 6| Step: 1
Training loss: 1.792274091203482
Validation loss: 2.476757729859271

Epoch: 6| Step: 2
Training loss: 2.2869446627548577
Validation loss: 2.4901316886639844

Epoch: 6| Step: 3
Training loss: 2.574996622324089
Validation loss: 2.4894526255054057

Epoch: 6| Step: 4
Training loss: 2.7393578169412374
Validation loss: 2.4874867275306705

Epoch: 6| Step: 5
Training loss: 2.81619426971843
Validation loss: 2.4817641533322905

Epoch: 6| Step: 6
Training loss: 2.8979998564052085
Validation loss: 2.48824478380007

Epoch: 6| Step: 7
Training loss: 2.3855653963545884
Validation loss: 2.4924829373303456

Epoch: 6| Step: 8
Training loss: 3.0598361827906957
Validation loss: 2.489731200935298

Epoch: 6| Step: 9
Training loss: 3.251368307962578
Validation loss: 2.493373355594716

Epoch: 6| Step: 10
Training loss: 2.5856714025312026
Validation loss: 2.4733756155748234

Epoch: 6| Step: 11
Training loss: 2.668865092384337
Validation loss: 2.490693450891665

Epoch: 6| Step: 12
Training loss: 2.7826561105811547
Validation loss: 2.4725194326601567

Epoch: 6| Step: 13
Training loss: 2.3094736865358025
Validation loss: 2.48245520057163

Epoch: 85| Step: 0
Training loss: 1.7216762254910598
Validation loss: 2.493893621873711

Epoch: 6| Step: 1
Training loss: 2.251918504619235
Validation loss: 2.4791054501364487

Epoch: 6| Step: 2
Training loss: 2.063435429074168
Validation loss: 2.476091784172959

Epoch: 6| Step: 3
Training loss: 3.5421382515289173
Validation loss: 2.483698774483807

Epoch: 6| Step: 4
Training loss: 2.825587530564803
Validation loss: 2.476489769563248

Epoch: 6| Step: 5
Training loss: 2.1353991127843353
Validation loss: 2.475422396040061

Epoch: 6| Step: 6
Training loss: 2.3808778006362483
Validation loss: 2.476485572894209

Epoch: 6| Step: 7
Training loss: 2.623448594633888
Validation loss: 2.487837958931126

Epoch: 6| Step: 8
Training loss: 2.786639445946117
Validation loss: 2.4901863159457984

Epoch: 6| Step: 9
Training loss: 3.239097648757449
Validation loss: 2.491622206972772

Epoch: 6| Step: 10
Training loss: 2.497176292297853
Validation loss: 2.480860549153753

Epoch: 6| Step: 11
Training loss: 3.3486002032988655
Validation loss: 2.493546671173569

Epoch: 6| Step: 12
Training loss: 2.522893037903249
Validation loss: 2.480175137552973

Epoch: 6| Step: 13
Training loss: 2.976923883082008
Validation loss: 2.4854914155513796

Epoch: 86| Step: 0
Training loss: 2.8957322649619726
Validation loss: 2.4696110918531367

Epoch: 6| Step: 1
Training loss: 2.854309431157804
Validation loss: 2.477351906924178

Epoch: 6| Step: 2
Training loss: 2.2576304431628103
Validation loss: 2.500188087238065

Epoch: 6| Step: 3
Training loss: 2.759774264074412
Validation loss: 2.488567172374133

Epoch: 6| Step: 4
Training loss: 2.565858454426958
Validation loss: 2.4852822117109317

Epoch: 6| Step: 5
Training loss: 2.3693050569611955
Validation loss: 2.4849085199783447

Epoch: 6| Step: 6
Training loss: 3.141173357561766
Validation loss: 2.485996292343149

Epoch: 6| Step: 7
Training loss: 3.366243376103856
Validation loss: 2.49835198340121

Epoch: 6| Step: 8
Training loss: 2.2903408608103533
Validation loss: 2.474551648601861

Epoch: 6| Step: 9
Training loss: 2.4451529843887947
Validation loss: 2.476466453840101

Epoch: 6| Step: 10
Training loss: 2.900078897389515
Validation loss: 2.475564289866063

Epoch: 6| Step: 11
Training loss: 2.231503969586
Validation loss: 2.4950398717649445

Epoch: 6| Step: 12
Training loss: 2.772937396454702
Validation loss: 2.485167185315067

Epoch: 6| Step: 13
Training loss: 1.6518593657580398
Validation loss: 2.4958813283058388

Epoch: 87| Step: 0
Training loss: 2.955571205970929
Validation loss: 2.4740038857964493

Epoch: 6| Step: 1
Training loss: 3.0651568351807663
Validation loss: 2.4809191600319975

Epoch: 6| Step: 2
Training loss: 2.69285236812109
Validation loss: 2.480265389328345

Epoch: 6| Step: 3
Training loss: 2.357497725258111
Validation loss: 2.4907907444888533

Epoch: 6| Step: 4
Training loss: 2.5273626170567023
Validation loss: 2.4991093146156365

Epoch: 6| Step: 5
Training loss: 2.7327177420622832
Validation loss: 2.485192231871827

Epoch: 6| Step: 6
Training loss: 2.2442755463548383
Validation loss: 2.4834628487697628

Epoch: 6| Step: 7
Training loss: 3.2070788288188075
Validation loss: 2.4916545354484922

Epoch: 6| Step: 8
Training loss: 2.765189012547961
Validation loss: 2.476610094169958

Epoch: 6| Step: 9
Training loss: 2.0424533288635844
Validation loss: 2.492873763507518

Epoch: 6| Step: 10
Training loss: 2.9557823859757724
Validation loss: 2.47175593236339

Epoch: 6| Step: 11
Training loss: 2.584045916896343
Validation loss: 2.4956526715055825

Epoch: 6| Step: 12
Training loss: 2.3405374508057477
Validation loss: 2.4935336593775954

Epoch: 6| Step: 13
Training loss: 2.6689355754430415
Validation loss: 2.502888776334322

Epoch: 88| Step: 0
Training loss: 2.4723438228271335
Validation loss: 2.4862829646886353

Epoch: 6| Step: 1
Training loss: 3.035485837659251
Validation loss: 2.488957516979449

Epoch: 6| Step: 2
Training loss: 2.201943154927424
Validation loss: 2.4882676347369803

Epoch: 6| Step: 3
Training loss: 2.872801894084632
Validation loss: 2.487916617531048

Epoch: 6| Step: 4
Training loss: 2.479976094499859
Validation loss: 2.490625379035365

Epoch: 6| Step: 5
Training loss: 2.709544062812102
Validation loss: 2.484463547278999

Epoch: 6| Step: 6
Training loss: 2.785971074268388
Validation loss: 2.484061978526453

Epoch: 6| Step: 7
Training loss: 2.2345591649271808
Validation loss: 2.4889795270757022

Epoch: 6| Step: 8
Training loss: 2.955258038093511
Validation loss: 2.4962171606575314

Epoch: 6| Step: 9
Training loss: 2.7678857388755156
Validation loss: 2.4828580772233058

Epoch: 6| Step: 10
Training loss: 2.916074819915751
Validation loss: 2.4948330911588323

Epoch: 6| Step: 11
Training loss: 2.6476949815570805
Validation loss: 2.4883236175637666

Epoch: 6| Step: 12
Training loss: 2.272253718024448
Validation loss: 2.4973681163052817

Epoch: 6| Step: 13
Training loss: 2.6611185671923763
Validation loss: 2.5022299019957304

Epoch: 89| Step: 0
Training loss: 3.0889996365454038
Validation loss: 2.511113808637375

Epoch: 6| Step: 1
Training loss: 3.1365475732596964
Validation loss: 2.5000004922189536

Epoch: 6| Step: 2
Training loss: 2.0265251962115167
Validation loss: 2.475427161004308

Epoch: 6| Step: 3
Training loss: 2.533016483251747
Validation loss: 2.4866950729006994

Epoch: 6| Step: 4
Training loss: 2.185297047481184
Validation loss: 2.505325441196773

Epoch: 6| Step: 5
Training loss: 1.8917021402178218
Validation loss: 2.4987249117021575

Epoch: 6| Step: 6
Training loss: 2.0724922653072317
Validation loss: 2.4798153392024993

Epoch: 6| Step: 7
Training loss: 3.0496738196886137
Validation loss: 2.4815119832267114

Epoch: 6| Step: 8
Training loss: 2.4051932454341545
Validation loss: 2.4787620092855045

Epoch: 6| Step: 9
Training loss: 3.063242608166917
Validation loss: 2.491936584261614

Epoch: 6| Step: 10
Training loss: 2.9633392819093514
Validation loss: 2.4961026751444555

Epoch: 6| Step: 11
Training loss: 3.0804908296943614
Validation loss: 2.485950593877851

Epoch: 6| Step: 12
Training loss: 2.7038060030558118
Validation loss: 2.4838734545719894

Epoch: 6| Step: 13
Training loss: 2.3519765292962327
Validation loss: 2.4796867545481147

Epoch: 90| Step: 0
Training loss: 2.7947365881323947
Validation loss: 2.4754965652262144

Epoch: 6| Step: 1
Training loss: 2.450984142151002
Validation loss: 2.4955235879490862

Epoch: 6| Step: 2
Training loss: 2.8248499670107448
Validation loss: 2.4867750397939585

Epoch: 6| Step: 3
Training loss: 2.5060894236886133
Validation loss: 2.4833134628778826

Epoch: 6| Step: 4
Training loss: 2.8286847145547114
Validation loss: 2.4905542708100725

Epoch: 6| Step: 5
Training loss: 3.120922632981261
Validation loss: 2.4850400701480284

Epoch: 6| Step: 6
Training loss: 2.8150280081404144
Validation loss: 2.466773060267085

Epoch: 6| Step: 7
Training loss: 2.5894016446624057
Validation loss: 2.4814711673258087

Epoch: 6| Step: 8
Training loss: 2.788693443692635
Validation loss: 2.484539403962188

Epoch: 6| Step: 9
Training loss: 2.2297073374068264
Validation loss: 2.4828883272496185

Epoch: 6| Step: 10
Training loss: 2.7420065621785135
Validation loss: 2.4868291495962476

Epoch: 6| Step: 11
Training loss: 2.2366583834332254
Validation loss: 2.488708341987852

Epoch: 6| Step: 12
Training loss: 2.106441124640888
Validation loss: 2.4855710302652043

Epoch: 6| Step: 13
Training loss: 3.2375378462888036
Validation loss: 2.4869335546414266

Epoch: 91| Step: 0
Training loss: 3.3114322884923273
Validation loss: 2.4803675218693533

Epoch: 6| Step: 1
Training loss: 2.492207589860739
Validation loss: 2.4942743189417147

Epoch: 6| Step: 2
Training loss: 2.0880162314591604
Validation loss: 2.4949860561414487

Epoch: 6| Step: 3
Training loss: 2.93585418696653
Validation loss: 2.478462793189291

Epoch: 6| Step: 4
Training loss: 2.8861531076403737
Validation loss: 2.4757498440435963

Epoch: 6| Step: 5
Training loss: 2.299813914027405
Validation loss: 2.4889404960756902

Epoch: 6| Step: 6
Training loss: 2.7343872070040023
Validation loss: 2.491160796389302

Epoch: 6| Step: 7
Training loss: 1.9793094650419223
Validation loss: 2.475786837028846

Epoch: 6| Step: 8
Training loss: 2.003074904840519
Validation loss: 2.4950850050958064

Epoch: 6| Step: 9
Training loss: 2.1775031407505514
Validation loss: 2.4830271946842455

Epoch: 6| Step: 10
Training loss: 2.6546714131101936
Validation loss: 2.4837564069870885

Epoch: 6| Step: 11
Training loss: 3.0341147817023866
Validation loss: 2.487629959282443

Epoch: 6| Step: 12
Training loss: 3.0853484158863016
Validation loss: 2.4789748575642103

Epoch: 6| Step: 13
Training loss: 3.1292922487892563
Validation loss: 2.4921651900867334

Epoch: 92| Step: 0
Training loss: 2.0596829451850787
Validation loss: 2.468684799851556

Epoch: 6| Step: 1
Training loss: 2.108172038345476
Validation loss: 2.479128568347422

Epoch: 6| Step: 2
Training loss: 2.28777206553991
Validation loss: 2.4941735344605607

Epoch: 6| Step: 3
Training loss: 2.9813229597908664
Validation loss: 2.496486640792942

Epoch: 6| Step: 4
Training loss: 2.8202862936477224
Validation loss: 2.4822544772010344

Epoch: 6| Step: 5
Training loss: 2.9448270928962303
Validation loss: 2.4689930860667757

Epoch: 6| Step: 6
Training loss: 2.103782303959322
Validation loss: 2.4935780581507188

Epoch: 6| Step: 7
Training loss: 3.1663112608250072
Validation loss: 2.4827497360075688

Epoch: 6| Step: 8
Training loss: 3.0002461968172067
Validation loss: 2.490443464457259

Epoch: 6| Step: 9
Training loss: 2.9155253402506305
Validation loss: 2.502639144532318

Epoch: 6| Step: 10
Training loss: 2.7212442816979565
Validation loss: 2.4840464318920628

Epoch: 6| Step: 11
Training loss: 2.471818972023749
Validation loss: 2.4954950402657667

Epoch: 6| Step: 12
Training loss: 2.098447511923191
Validation loss: 2.4906500336056245

Epoch: 6| Step: 13
Training loss: 3.2395608948219765
Validation loss: 2.47840262745609

Epoch: 93| Step: 0
Training loss: 3.2895936514410957
Validation loss: 2.50523638693051

Epoch: 6| Step: 1
Training loss: 3.1520469554172688
Validation loss: 2.4913459720299493

Epoch: 6| Step: 2
Training loss: 2.7864463354309943
Validation loss: 2.497620822543961

Epoch: 6| Step: 3
Training loss: 2.5177973495696104
Validation loss: 2.48571174555429

Epoch: 6| Step: 4
Training loss: 2.0882196978892886
Validation loss: 2.4967980481251186

Epoch: 6| Step: 5
Training loss: 2.5646615564691877
Validation loss: 2.492528517059019

Epoch: 6| Step: 6
Training loss: 2.6063295054944486
Validation loss: 2.5054897310374504

Epoch: 6| Step: 7
Training loss: 2.3262171768914746
Validation loss: 2.4924936095311008

Epoch: 6| Step: 8
Training loss: 2.579509386536892
Validation loss: 2.4855834752165222

Epoch: 6| Step: 9
Training loss: 2.5503700474034403
Validation loss: 2.484283766513107

Epoch: 6| Step: 10
Training loss: 1.9510887823273029
Validation loss: 2.4770321705862886

Epoch: 6| Step: 11
Training loss: 3.1204882286227495
Validation loss: 2.483940789810576

Epoch: 6| Step: 12
Training loss: 2.2511191234004606
Validation loss: 2.4844877321042613

Epoch: 6| Step: 13
Training loss: 2.9059133026989525
Validation loss: 2.48877870773268

Epoch: 94| Step: 0
Training loss: 2.5034346828271037
Validation loss: 2.4647566540401087

Epoch: 6| Step: 1
Training loss: 2.1479913421683694
Validation loss: 2.4857131265325405

Epoch: 6| Step: 2
Training loss: 2.284190334052251
Validation loss: 2.480815518723389

Epoch: 6| Step: 3
Training loss: 3.316852804612502
Validation loss: 2.4904604236198606

Epoch: 6| Step: 4
Training loss: 2.4706667448840856
Validation loss: 2.497646892781719

Epoch: 6| Step: 5
Training loss: 2.777723498343914
Validation loss: 2.478768845617984

Epoch: 6| Step: 6
Training loss: 2.137511502062095
Validation loss: 2.500469273443978

Epoch: 6| Step: 7
Training loss: 2.3400145199317057
Validation loss: 2.4836266731376964

Epoch: 6| Step: 8
Training loss: 2.939189668426048
Validation loss: 2.492654418307395

Epoch: 6| Step: 9
Training loss: 3.245598379959255
Validation loss: 2.482340513079973

Epoch: 6| Step: 10
Training loss: 2.496062038261978
Validation loss: 2.4955223469755796

Epoch: 6| Step: 11
Training loss: 3.0081200380520188
Validation loss: 2.4770014521286936

Epoch: 6| Step: 12
Training loss: 2.2746754844527413
Validation loss: 2.484373978413182

Epoch: 6| Step: 13
Training loss: 2.6603617539759177
Validation loss: 2.478936052717045

Epoch: 95| Step: 0
Training loss: 2.5288327767084424
Validation loss: 2.486322977768156

Epoch: 6| Step: 1
Training loss: 2.182288146338586
Validation loss: 2.4818473655797626

Epoch: 6| Step: 2
Training loss: 2.78727275794549
Validation loss: 2.475393069714983

Epoch: 6| Step: 3
Training loss: 2.2868112162879624
Validation loss: 2.4761871284256918

Epoch: 6| Step: 4
Training loss: 2.3141584892893152
Validation loss: 2.478081323340943

Epoch: 6| Step: 5
Training loss: 3.4275269535265336
Validation loss: 2.4846806172133133

Epoch: 6| Step: 6
Training loss: 2.5239293236245453
Validation loss: 2.4908302837545238

Epoch: 6| Step: 7
Training loss: 2.7837432658435644
Validation loss: 2.4809936019836725

Epoch: 6| Step: 8
Training loss: 2.921600818519436
Validation loss: 2.5019555544575844

Epoch: 6| Step: 9
Training loss: 2.678919468021351
Validation loss: 2.4793760580592816

Epoch: 6| Step: 10
Training loss: 2.3373455792178985
Validation loss: 2.4966755656768282

Epoch: 6| Step: 11
Training loss: 2.499239233612135
Validation loss: 2.4989199997388236

Epoch: 6| Step: 12
Training loss: 3.0447107698228084
Validation loss: 2.4915477378871542

Epoch: 6| Step: 13
Training loss: 2.0212779653828625
Validation loss: 2.48696988544344

Epoch: 96| Step: 0
Training loss: 2.2995202020171215
Validation loss: 2.4854831681261813

Epoch: 6| Step: 1
Training loss: 2.431517865249672
Validation loss: 2.486541757686383

Epoch: 6| Step: 2
Training loss: 2.027886878378033
Validation loss: 2.499494038932214

Epoch: 6| Step: 3
Training loss: 3.3919864856792636
Validation loss: 2.4885419068766708

Epoch: 6| Step: 4
Training loss: 2.156920508471395
Validation loss: 2.4813010180783768

Epoch: 6| Step: 5
Training loss: 3.3327040714129277
Validation loss: 2.4724690939430727

Epoch: 6| Step: 6
Training loss: 2.6260128791882567
Validation loss: 2.493685944337676

Epoch: 6| Step: 7
Training loss: 3.3173110859831225
Validation loss: 2.4929083829095773

Epoch: 6| Step: 8
Training loss: 2.2553046531165455
Validation loss: 2.488889237989576

Epoch: 6| Step: 9
Training loss: 2.566839219357817
Validation loss: 2.4913105779218045

Epoch: 6| Step: 10
Training loss: 2.3226879560383242
Validation loss: 2.510660539465121

Epoch: 6| Step: 11
Training loss: 2.6172082700901584
Validation loss: 2.4691836847308823

Epoch: 6| Step: 12
Training loss: 2.573778221861229
Validation loss: 2.4932639341066145

Epoch: 6| Step: 13
Training loss: 2.434721316579024
Validation loss: 2.4905180139123835

Epoch: 97| Step: 0
Training loss: 2.6631921146210766
Validation loss: 2.4992762328045326

Epoch: 6| Step: 1
Training loss: 2.3533867638477757
Validation loss: 2.4882627398179333

Epoch: 6| Step: 2
Training loss: 2.8164567030198335
Validation loss: 2.485657878553091

Epoch: 6| Step: 3
Training loss: 2.505776788267876
Validation loss: 2.4870273854501757

Epoch: 6| Step: 4
Training loss: 3.564771095620638
Validation loss: 2.4858628781905594

Epoch: 6| Step: 5
Training loss: 2.827899987137055
Validation loss: 2.4746806306063562

Epoch: 6| Step: 6
Training loss: 2.7108643271824993
Validation loss: 2.483160937733324

Epoch: 6| Step: 7
Training loss: 2.4183535662522613
Validation loss: 2.478880014562507

Epoch: 6| Step: 8
Training loss: 2.1657980008953683
Validation loss: 2.480410881940369

Epoch: 6| Step: 9
Training loss: 2.8112864949328444
Validation loss: 2.491101263572546

Epoch: 6| Step: 10
Training loss: 2.4748759498719606
Validation loss: 2.4898382282861578

Epoch: 6| Step: 11
Training loss: 2.151761677262346
Validation loss: 2.4937708703273214

Epoch: 6| Step: 12
Training loss: 2.1957816169145072
Validation loss: 2.494732687377545

Epoch: 6| Step: 13
Training loss: 3.1135609117059713
Validation loss: 2.4891456496955304

Epoch: 98| Step: 0
Training loss: 1.7805827714834028
Validation loss: 2.489058922588037

Epoch: 6| Step: 1
Training loss: 1.7487446505810365
Validation loss: 2.503810311786156

Epoch: 6| Step: 2
Training loss: 1.7375083319375035
Validation loss: 2.489731109293268

Epoch: 6| Step: 3
Training loss: 2.947383567315815
Validation loss: 2.4903360884031414

Epoch: 6| Step: 4
Training loss: 3.5458550015843486
Validation loss: 2.484940243546394

Epoch: 6| Step: 5
Training loss: 2.705141935732034
Validation loss: 2.4908616823176626

Epoch: 6| Step: 6
Training loss: 2.7165405240251164
Validation loss: 2.4907246113586763

Epoch: 6| Step: 7
Training loss: 3.0956322694966887
Validation loss: 2.4979618008666633

Epoch: 6| Step: 8
Training loss: 3.062981703885115
Validation loss: 2.4845324132540862

Epoch: 6| Step: 9
Training loss: 2.514005246171356
Validation loss: 2.482225936995651

Epoch: 6| Step: 10
Training loss: 2.1424336832437003
Validation loss: 2.4927260415260983

Epoch: 6| Step: 11
Training loss: 2.6227582714520166
Validation loss: 2.4722376646042195

Epoch: 6| Step: 12
Training loss: 3.1591596128129575
Validation loss: 2.490193167757113

Epoch: 6| Step: 13
Training loss: 2.018596499249473
Validation loss: 2.485661588399311

Epoch: 99| Step: 0
Training loss: 2.957782765766501
Validation loss: 2.482259144361975

Epoch: 6| Step: 1
Training loss: 3.0330033098322877
Validation loss: 2.4888097819555535

Epoch: 6| Step: 2
Training loss: 2.68855938989965
Validation loss: 2.4832106271382246

Epoch: 6| Step: 3
Training loss: 1.758037908724719
Validation loss: 2.473629846358922

Epoch: 6| Step: 4
Training loss: 2.486840805811849
Validation loss: 2.4797944573269803

Epoch: 6| Step: 5
Training loss: 2.3394856632687744
Validation loss: 2.4772299542592893

Epoch: 6| Step: 6
Training loss: 3.068466291501355
Validation loss: 2.4912060895272314

Epoch: 6| Step: 7
Training loss: 2.6972444106696507
Validation loss: 2.470081176583017

Epoch: 6| Step: 8
Training loss: 2.208307326061726
Validation loss: 2.4819079446465997

Epoch: 6| Step: 9
Training loss: 2.464129989947941
Validation loss: 2.4769542680925283

Epoch: 6| Step: 10
Training loss: 3.0980530685499494
Validation loss: 2.4858960111916444

Epoch: 6| Step: 11
Training loss: 2.587273382315848
Validation loss: 2.4803145062483676

Epoch: 6| Step: 12
Training loss: 2.8376469779914664
Validation loss: 2.4805446451332855

Epoch: 6| Step: 13
Training loss: 2.3788238661458356
Validation loss: 2.479314214573208

Epoch: 100| Step: 0
Training loss: 3.1880489699408963
Validation loss: 2.4802501052095396

Epoch: 6| Step: 1
Training loss: 2.4552191281853495
Validation loss: 2.479717904453258

Epoch: 6| Step: 2
Training loss: 2.978508741026881
Validation loss: 2.476578959003419

Epoch: 6| Step: 3
Training loss: 2.3171353664179684
Validation loss: 2.48491914527028

Epoch: 6| Step: 4
Training loss: 2.7456981650750123
Validation loss: 2.4865812191458243

Epoch: 6| Step: 5
Training loss: 2.2607452458470134
Validation loss: 2.502053757490581

Epoch: 6| Step: 6
Training loss: 2.8681439459734617
Validation loss: 2.474037203521935

Epoch: 6| Step: 7
Training loss: 2.588444626727086
Validation loss: 2.492458489631147

Epoch: 6| Step: 8
Training loss: 2.849272343952864
Validation loss: 2.486732096877527

Epoch: 6| Step: 9
Training loss: 2.359274249420417
Validation loss: 2.4912593487912376

Epoch: 6| Step: 10
Training loss: 2.520692164564105
Validation loss: 2.491963327109386

Epoch: 6| Step: 11
Training loss: 2.406153689351997
Validation loss: 2.4687404775169757

Epoch: 6| Step: 12
Training loss: 2.2144170010228303
Validation loss: 2.484384414057542

Epoch: 6| Step: 13
Training loss: 2.8563394745349546
Validation loss: 2.5067340488443834

Epoch: 101| Step: 0
Training loss: 2.2298170430351805
Validation loss: 2.4885417719233747

Epoch: 6| Step: 1
Training loss: 2.606836420403417
Validation loss: 2.49887935788895

Epoch: 6| Step: 2
Training loss: 3.4279462352173096
Validation loss: 2.4943963118924417

Epoch: 6| Step: 3
Training loss: 2.9830677304943642
Validation loss: 2.494017383987698

Epoch: 6| Step: 4
Training loss: 2.5033438254727844
Validation loss: 2.5013756125799484

Epoch: 6| Step: 5
Training loss: 3.3413505144623854
Validation loss: 2.4681913886920603

Epoch: 6| Step: 6
Training loss: 2.412637728381666
Validation loss: 2.4957419855661405

Epoch: 6| Step: 7
Training loss: 3.2828546869035655
Validation loss: 2.484183256272894

Epoch: 6| Step: 8
Training loss: 2.025539528827571
Validation loss: 2.477713602603678

Epoch: 6| Step: 9
Training loss: 1.909965521216634
Validation loss: 2.493105066986376

Epoch: 6| Step: 10
Training loss: 2.6118906652049705
Validation loss: 2.477890195487813

Epoch: 6| Step: 11
Training loss: 1.97331978997317
Validation loss: 2.4956263390309155

Epoch: 6| Step: 12
Training loss: 1.989586542203489
Validation loss: 2.4916857687788294

Epoch: 6| Step: 13
Training loss: 2.890928180363282
Validation loss: 2.482035029140074

Epoch: 102| Step: 0
Training loss: 2.4208860285457487
Validation loss: 2.496533872584085

Epoch: 6| Step: 1
Training loss: 2.856747947376329
Validation loss: 2.4722748786480717

Epoch: 6| Step: 2
Training loss: 2.449785425560088
Validation loss: 2.4859896181962418

Epoch: 6| Step: 3
Training loss: 3.256795674179529
Validation loss: 2.4779333081889985

Epoch: 6| Step: 4
Training loss: 2.368457113794555
Validation loss: 2.4840367471895903

Epoch: 6| Step: 5
Training loss: 3.197028335965954
Validation loss: 2.495032974696186

Epoch: 6| Step: 6
Training loss: 2.3742333228770844
Validation loss: 2.485827444943825

Epoch: 6| Step: 7
Training loss: 2.1771316036245336
Validation loss: 2.4883521924026994

Epoch: 6| Step: 8
Training loss: 2.694056652342841
Validation loss: 2.484570635419789

Epoch: 6| Step: 9
Training loss: 2.2729280825245786
Validation loss: 2.4875097343537567

Epoch: 6| Step: 10
Training loss: 2.289829373062381
Validation loss: 2.4782705010035744

Epoch: 6| Step: 11
Training loss: 2.7223635807660496
Validation loss: 2.488043648587537

Epoch: 6| Step: 12
Training loss: 2.32704495973518
Validation loss: 2.475879955426498

Epoch: 6| Step: 13
Training loss: 3.260381257633744
Validation loss: 2.478880745736929

Epoch: 103| Step: 0
Training loss: 2.4799617699783596
Validation loss: 2.478154221725694

Epoch: 6| Step: 1
Training loss: 3.0517767186914364
Validation loss: 2.4965306235337366

Epoch: 6| Step: 2
Training loss: 2.734272982192872
Validation loss: 2.4796997552613793

Epoch: 6| Step: 3
Training loss: 2.6380856736032574
Validation loss: 2.484483312648908

Epoch: 6| Step: 4
Training loss: 2.6172217523411603
Validation loss: 2.4927264981572397

Epoch: 6| Step: 5
Training loss: 2.0596044618064453
Validation loss: 2.4982863305692247

Epoch: 6| Step: 6
Training loss: 2.5161704662656734
Validation loss: 2.50193637182253

Epoch: 6| Step: 7
Training loss: 2.8350845609564166
Validation loss: 2.469747740217725

Epoch: 6| Step: 8
Training loss: 2.0934151908436327
Validation loss: 2.473555860612323

Epoch: 6| Step: 9
Training loss: 2.700976449512797
Validation loss: 2.5023503186895617

Epoch: 6| Step: 10
Training loss: 2.312287398179514
Validation loss: 2.480327881965006

Epoch: 6| Step: 11
Training loss: 2.385678228394608
Validation loss: 2.487416335543858

Epoch: 6| Step: 12
Training loss: 2.5580769956154223
Validation loss: 2.498740941555073

Epoch: 6| Step: 13
Training loss: 3.7962957497642007
Validation loss: 2.4918076034735295

Epoch: 104| Step: 0
Training loss: 2.435952404434188
Validation loss: 2.4843464676445306

Epoch: 6| Step: 1
Training loss: 2.4717872382208577
Validation loss: 2.4781684465221483

Epoch: 6| Step: 2
Training loss: 3.6541381997217917
Validation loss: 2.4889027066782323

Epoch: 6| Step: 3
Training loss: 1.7122083151008634
Validation loss: 2.480089260169463

Epoch: 6| Step: 4
Training loss: 2.998270648967576
Validation loss: 2.483594870857303

Epoch: 6| Step: 5
Training loss: 2.5289086710411834
Validation loss: 2.497876504201637

Epoch: 6| Step: 6
Training loss: 2.4642306138032963
Validation loss: 2.4852899275357565

Epoch: 6| Step: 7
Training loss: 2.7119879102971947
Validation loss: 2.4894126400736716

Epoch: 6| Step: 8
Training loss: 2.367106722721933
Validation loss: 2.4760018938232173

Epoch: 6| Step: 9
Training loss: 2.527803311434422
Validation loss: 2.484868056482251

Epoch: 6| Step: 10
Training loss: 2.6562131991361566
Validation loss: 2.4871937971892444

Epoch: 6| Step: 11
Training loss: 3.272440537742295
Validation loss: 2.496379735195644

Epoch: 6| Step: 12
Training loss: 1.6506131535675168
Validation loss: 2.4827880744179445

Epoch: 6| Step: 13
Training loss: 2.4575741991268276
Validation loss: 2.4931878914670027

Epoch: 105| Step: 0
Training loss: 3.319758254522344
Validation loss: 2.4882585094297074

Epoch: 6| Step: 1
Training loss: 2.791399577634894
Validation loss: 2.488655732739692

Epoch: 6| Step: 2
Training loss: 2.582038636997754
Validation loss: 2.4829200120972024

Epoch: 6| Step: 3
Training loss: 3.112209848874689
Validation loss: 2.488618387131747

Epoch: 6| Step: 4
Training loss: 2.6887087210763445
Validation loss: 2.479010421849494

Epoch: 6| Step: 5
Training loss: 2.1996538670219437
Validation loss: 2.4697194187940377

Epoch: 6| Step: 6
Training loss: 2.6185599756603573
Validation loss: 2.4837865965294417

Epoch: 6| Step: 7
Training loss: 2.937962272469326
Validation loss: 2.4850626194032497

Epoch: 6| Step: 8
Training loss: 2.195118634226601
Validation loss: 2.483758239075146

Epoch: 6| Step: 9
Training loss: 2.5288241972057954
Validation loss: 2.484018135199057

Epoch: 6| Step: 10
Training loss: 2.2409643365830063
Validation loss: 2.485519787268957

Epoch: 6| Step: 11
Training loss: 2.3129314329454473
Validation loss: 2.478700102385465

Epoch: 6| Step: 12
Training loss: 2.2630801937510716
Validation loss: 2.4761009558719462

Epoch: 6| Step: 13
Training loss: 2.4476668751203214
Validation loss: 2.477866945707519

Epoch: 106| Step: 0
Training loss: 2.7880519029774695
Validation loss: 2.491683941490856

Epoch: 6| Step: 1
Training loss: 2.08217301163434
Validation loss: 2.4717357328013887

Epoch: 6| Step: 2
Training loss: 1.9349419719659524
Validation loss: 2.4865360768342537

Epoch: 6| Step: 3
Training loss: 3.7340972669424155
Validation loss: 2.486381234076891

Epoch: 6| Step: 4
Training loss: 2.455301084993193
Validation loss: 2.4976228692506854

Epoch: 6| Step: 5
Training loss: 2.1964938965939917
Validation loss: 2.5132504242223686

Epoch: 6| Step: 6
Training loss: 3.1994241315523597
Validation loss: 2.48393712177405

Epoch: 6| Step: 7
Training loss: 2.412635751966608
Validation loss: 2.5007251897739917

Epoch: 6| Step: 8
Training loss: 2.714596436577003
Validation loss: 2.484581428279699

Epoch: 6| Step: 9
Training loss: 2.4724776699187943
Validation loss: 2.480453626298973

Epoch: 6| Step: 10
Training loss: 2.2308464732571025
Validation loss: 2.4915312151943727

Epoch: 6| Step: 11
Training loss: 2.494926358743476
Validation loss: 2.4843941015323248

Epoch: 6| Step: 12
Training loss: 2.6903063294088145
Validation loss: 2.4887548499404337

Epoch: 6| Step: 13
Training loss: 2.4235959737232724
Validation loss: 2.489734243654707

Epoch: 107| Step: 0
Training loss: 2.4581541761213357
Validation loss: 2.488478958096008

Epoch: 6| Step: 1
Training loss: 2.8570850979552582
Validation loss: 2.4804061410198552

Epoch: 6| Step: 2
Training loss: 2.568316487581564
Validation loss: 2.486280054883834

Epoch: 6| Step: 3
Training loss: 2.5818641135273204
Validation loss: 2.4856605394942273

Epoch: 6| Step: 4
Training loss: 2.5565231214641995
Validation loss: 2.4967720357649164

Epoch: 6| Step: 5
Training loss: 2.055752438346737
Validation loss: 2.466467773791068

Epoch: 6| Step: 6
Training loss: 2.4068040086157145
Validation loss: 2.4854691507411806

Epoch: 6| Step: 7
Training loss: 2.4958164496851123
Validation loss: 2.4878774781452995

Epoch: 6| Step: 8
Training loss: 2.720224002842573
Validation loss: 2.474504045920788

Epoch: 6| Step: 9
Training loss: 2.8738747758230203
Validation loss: 2.4754799240074723

Epoch: 6| Step: 10
Training loss: 2.185286573738738
Validation loss: 2.4966748551161144

Epoch: 6| Step: 11
Training loss: 2.597535186470511
Validation loss: 2.4892843554950654

Epoch: 6| Step: 12
Training loss: 3.226015649595573
Validation loss: 2.4850893784538672

Epoch: 6| Step: 13
Training loss: 2.806476840428115
Validation loss: 2.4888989130805035

Epoch: 108| Step: 0
Training loss: 2.514888394952866
Validation loss: 2.4790012335100426

Epoch: 6| Step: 1
Training loss: 3.051672655061867
Validation loss: 2.4809284590689087

Epoch: 6| Step: 2
Training loss: 2.106746590140437
Validation loss: 2.4807736517597454

Epoch: 6| Step: 3
Training loss: 2.465714241994609
Validation loss: 2.4918501718840087

Epoch: 6| Step: 4
Training loss: 2.6743609369152916
Validation loss: 2.482723225376985

Epoch: 6| Step: 5
Training loss: 2.751073974358039
Validation loss: 2.489105566637478

Epoch: 6| Step: 6
Training loss: 3.4106839220725482
Validation loss: 2.4879786181358567

Epoch: 6| Step: 7
Training loss: 2.131486305717201
Validation loss: 2.4910077930786523

Epoch: 6| Step: 8
Training loss: 2.130121566634486
Validation loss: 2.4845906878144968

Epoch: 6| Step: 9
Training loss: 2.32488460459495
Validation loss: 2.4991511985658117

Epoch: 6| Step: 10
Training loss: 2.945096521528312
Validation loss: 2.4756190879203936

Epoch: 6| Step: 11
Training loss: 2.9644775164429236
Validation loss: 2.4857809358494296

Epoch: 6| Step: 12
Training loss: 2.383675440885967
Validation loss: 2.4820031646335905

Epoch: 6| Step: 13
Training loss: 1.9393376280478172
Validation loss: 2.496110236329322

Epoch: 109| Step: 0
Training loss: 3.3029637812569805
Validation loss: 2.492256804706249

Epoch: 6| Step: 1
Training loss: 2.0116506027975127
Validation loss: 2.468665578843978

Epoch: 6| Step: 2
Training loss: 3.0173534124461763
Validation loss: 2.4930580088037138

Epoch: 6| Step: 3
Training loss: 2.444700316803074
Validation loss: 2.5038803447613036

Epoch: 6| Step: 4
Training loss: 2.4254943383126597
Validation loss: 2.492339807552775

Epoch: 6| Step: 5
Training loss: 2.561311120364716
Validation loss: 2.487969208404445

Epoch: 6| Step: 6
Training loss: 2.181140265122841
Validation loss: 2.493026848708867

Epoch: 6| Step: 7
Training loss: 2.552965242764131
Validation loss: 2.487106875749739

Epoch: 6| Step: 8
Training loss: 2.6059163637009712
Validation loss: 2.4784567979897116

Epoch: 6| Step: 9
Training loss: 2.7430183912461024
Validation loss: 2.491809826769154

Epoch: 6| Step: 10
Training loss: 2.7135147025049355
Validation loss: 2.4907004994452415

Epoch: 6| Step: 11
Training loss: 2.248108810777996
Validation loss: 2.4899494517895686

Epoch: 6| Step: 12
Training loss: 2.886820996529473
Validation loss: 2.486662789745102

Epoch: 6| Step: 13
Training loss: 2.6053949028198953
Validation loss: 2.497200098378151

Epoch: 110| Step: 0
Training loss: 2.3658232833829795
Validation loss: 2.490801643168864

Epoch: 6| Step: 1
Training loss: 2.3790872439229247
Validation loss: 2.501792919044194

Epoch: 6| Step: 2
Training loss: 2.4417732146085127
Validation loss: 2.501101808887087

Epoch: 6| Step: 3
Training loss: 2.499135153905561
Validation loss: 2.4842449826755293

Epoch: 6| Step: 4
Training loss: 2.838449757805815
Validation loss: 2.5030839828066496

Epoch: 6| Step: 5
Training loss: 2.550124453331254
Validation loss: 2.489758622344393

Epoch: 6| Step: 6
Training loss: 2.552640415555134
Validation loss: 2.4735467369978155

Epoch: 6| Step: 7
Training loss: 2.3745263279836673
Validation loss: 2.4846114354857005

Epoch: 6| Step: 8
Training loss: 2.9926191770553423
Validation loss: 2.476350595570958

Epoch: 6| Step: 9
Training loss: 2.778576712685617
Validation loss: 2.4848782249056165

Epoch: 6| Step: 10
Training loss: 2.6429227504172066
Validation loss: 2.4932201970705776

Epoch: 6| Step: 11
Training loss: 2.257239245862415
Validation loss: 2.493453345812842

Epoch: 6| Step: 12
Training loss: 2.688454835404169
Validation loss: 2.4943614695589424

Epoch: 6| Step: 13
Training loss: 3.0663412488134463
Validation loss: 2.482960470686027

Epoch: 111| Step: 0
Training loss: 2.2365805670837435
Validation loss: 2.4896915917000784

Epoch: 6| Step: 1
Training loss: 2.8855842146904673
Validation loss: 2.5198590765443707

Epoch: 6| Step: 2
Training loss: 2.78281632797432
Validation loss: 2.476970670673227

Epoch: 6| Step: 3
Training loss: 3.3103754679999278
Validation loss: 2.493279941489046

Epoch: 6| Step: 4
Training loss: 2.7399031207088984
Validation loss: 2.4963292255016043

Epoch: 6| Step: 5
Training loss: 2.6611187463791093
Validation loss: 2.494630857269581

Epoch: 6| Step: 6
Training loss: 2.341500181567527
Validation loss: 2.4797652793695337

Epoch: 6| Step: 7
Training loss: 1.8418130560413946
Validation loss: 2.487256542462804

Epoch: 6| Step: 8
Training loss: 2.6492203987226697
Validation loss: 2.4920311309530967

Epoch: 6| Step: 9
Training loss: 2.792777272309898
Validation loss: 2.4945520630229296

Epoch: 6| Step: 10
Training loss: 2.5101433020791175
Validation loss: 2.4902200044800016

Epoch: 6| Step: 11
Training loss: 2.653334330106513
Validation loss: 2.4650846031179143

Epoch: 6| Step: 12
Training loss: 2.3020138507098222
Validation loss: 2.4768632777508524

Epoch: 6| Step: 13
Training loss: 2.089776325029443
Validation loss: 2.4835399881408082

Epoch: 112| Step: 0
Training loss: 3.1694058734561343
Validation loss: 2.4780807233155384

Epoch: 6| Step: 1
Training loss: 1.657845502375693
Validation loss: 2.4789922457609235

Epoch: 6| Step: 2
Training loss: 2.4139830937334783
Validation loss: 2.4771297761756625

Epoch: 6| Step: 3
Training loss: 3.111913517002792
Validation loss: 2.4821441978659475

Epoch: 6| Step: 4
Training loss: 3.154863562921061
Validation loss: 2.4768693130336503

Epoch: 6| Step: 5
Training loss: 2.329340367900327
Validation loss: 2.483100720646915

Epoch: 6| Step: 6
Training loss: 2.7134711219174568
Validation loss: 2.5024884390864366

Epoch: 6| Step: 7
Training loss: 2.7635852597955397
Validation loss: 2.5006414923520404

Epoch: 6| Step: 8
Training loss: 2.0300594655614943
Validation loss: 2.4758533463855676

Epoch: 6| Step: 9
Training loss: 2.59220518505095
Validation loss: 2.4748291728146175

Epoch: 6| Step: 10
Training loss: 1.933051262481191
Validation loss: 2.4784508751816845

Epoch: 6| Step: 11
Training loss: 1.9427320531738486
Validation loss: 2.486878350268629

Epoch: 6| Step: 12
Training loss: 3.407000257827395
Validation loss: 2.4822277919020848

Epoch: 6| Step: 13
Training loss: 2.1070821097533035
Validation loss: 2.498834522900103

Epoch: 113| Step: 0
Training loss: 3.145636815549722
Validation loss: 2.485145940933111

Epoch: 6| Step: 1
Training loss: 3.4620405036853215
Validation loss: 2.473209357278827

Epoch: 6| Step: 2
Training loss: 1.9453116420759278
Validation loss: 2.4935155470008343

Epoch: 6| Step: 3
Training loss: 2.3888633071160537
Validation loss: 2.4967088950597516

Epoch: 6| Step: 4
Training loss: 2.435291463496005
Validation loss: 2.4834780996966983

Epoch: 6| Step: 5
Training loss: 2.753221878616147
Validation loss: 2.4564187770574

Epoch: 6| Step: 6
Training loss: 2.5084720112769574
Validation loss: 2.485144482270719

Epoch: 6| Step: 7
Training loss: 2.2692734379841344
Validation loss: 2.492002765470553

Epoch: 6| Step: 8
Training loss: 2.1015809919827113
Validation loss: 2.482268012356699

Epoch: 6| Step: 9
Training loss: 2.395168798307897
Validation loss: 2.4757487339866233

Epoch: 6| Step: 10
Training loss: 2.0940989872472344
Validation loss: 2.4886376580010503

Epoch: 6| Step: 11
Training loss: 2.2977096183049914
Validation loss: 2.4944282431620324

Epoch: 6| Step: 12
Training loss: 2.873396218829436
Validation loss: 2.4783442425628595

Epoch: 6| Step: 13
Training loss: 3.550460549151896
Validation loss: 2.486590741352214

Epoch: 114| Step: 0
Training loss: 2.567498611850976
Validation loss: 2.4869772099879865

Epoch: 6| Step: 1
Training loss: 2.9880453976574577
Validation loss: 2.481272865719673

Epoch: 6| Step: 2
Training loss: 2.716027486142391
Validation loss: 2.4879461929499542

Epoch: 6| Step: 3
Training loss: 2.5357647416834226
Validation loss: 2.4789576098913955

Epoch: 6| Step: 4
Training loss: 1.950656521147399
Validation loss: 2.4789620371295684

Epoch: 6| Step: 5
Training loss: 2.974126665169322
Validation loss: 2.477467814601987

Epoch: 6| Step: 6
Training loss: 2.7171117297110543
Validation loss: 2.4927783915818247

Epoch: 6| Step: 7
Training loss: 2.967501729243224
Validation loss: 2.497223588039847

Epoch: 6| Step: 8
Training loss: 2.7498305008410453
Validation loss: 2.4945671418437314

Epoch: 6| Step: 9
Training loss: 2.3627454922804327
Validation loss: 2.4996475463498413

Epoch: 6| Step: 10
Training loss: 2.3073622877164572
Validation loss: 2.495611739662928

Epoch: 6| Step: 11
Training loss: 2.4328082977043026
Validation loss: 2.486677167419909

Epoch: 6| Step: 12
Training loss: 2.3623186152829394
Validation loss: 2.481024876526333

Epoch: 6| Step: 13
Training loss: 1.9815006614990054
Validation loss: 2.484398896756452

Epoch: 115| Step: 0
Training loss: 2.456054007983256
Validation loss: 2.4848392873358303

Epoch: 6| Step: 1
Training loss: 3.186149460653112
Validation loss: 2.494118996564295

Epoch: 6| Step: 2
Training loss: 2.668155423471438
Validation loss: 2.480597714743905

Epoch: 6| Step: 3
Training loss: 2.20769205918997
Validation loss: 2.49396664039295

Epoch: 6| Step: 4
Training loss: 2.62643366219116
Validation loss: 2.486799400072044

Epoch: 6| Step: 5
Training loss: 2.5760560027722503
Validation loss: 2.5042206412291566

Epoch: 6| Step: 6
Training loss: 2.401512337075734
Validation loss: 2.496941077380798

Epoch: 6| Step: 7
Training loss: 2.3497147001408716
Validation loss: 2.4823585014786484

Epoch: 6| Step: 8
Training loss: 1.6562911694286262
Validation loss: 2.4885156948945926

Epoch: 6| Step: 9
Training loss: 3.135889839300564
Validation loss: 2.484893986859706

Epoch: 6| Step: 10
Training loss: 3.176403402311921
Validation loss: 2.5025377150240287

Epoch: 6| Step: 11
Training loss: 2.450893285961248
Validation loss: 2.4946363090175505

Epoch: 6| Step: 12
Training loss: 2.6249143495664415
Validation loss: 2.479014872778499

Epoch: 6| Step: 13
Training loss: 1.7843924793783559
Validation loss: 2.4839968768615996

Epoch: 116| Step: 0
Training loss: 2.4680514735661157
Validation loss: 2.4920219967947643

Epoch: 6| Step: 1
Training loss: 2.2641171403376856
Validation loss: 2.477506787313257

Epoch: 6| Step: 2
Training loss: 2.3764413174463437
Validation loss: 2.507374420559694

Epoch: 6| Step: 3
Training loss: 2.8895070156441967
Validation loss: 2.4883471771097563

Epoch: 6| Step: 4
Training loss: 2.85200877420753
Validation loss: 2.491834119308244

Epoch: 6| Step: 5
Training loss: 2.266047786858578
Validation loss: 2.4923565131191254

Epoch: 6| Step: 6
Training loss: 2.38010209818239
Validation loss: 2.4820296963826127

Epoch: 6| Step: 7
Training loss: 2.673434064763692
Validation loss: 2.4822094090571323

Epoch: 6| Step: 8
Training loss: 2.685068671912915
Validation loss: 2.488859834979224

Epoch: 6| Step: 9
Training loss: 2.71511158891243
Validation loss: 2.4808564632153383

Epoch: 6| Step: 10
Training loss: 2.8532428963610452
Validation loss: 2.4824178329402207

Epoch: 6| Step: 11
Training loss: 2.692036193740382
Validation loss: 2.4996915709040226

Epoch: 6| Step: 12
Training loss: 2.5731889114800373
Validation loss: 2.491489533441981

Epoch: 6| Step: 13
Training loss: 2.3573227603180427
Validation loss: 2.502665450340595

Epoch: 117| Step: 0
Training loss: 2.304945281531536
Validation loss: 2.490981633802873

Epoch: 6| Step: 1
Training loss: 2.52582212981992
Validation loss: 2.4827487932607784

Epoch: 6| Step: 2
Training loss: 2.2176438919053103
Validation loss: 2.4801780534865583

Epoch: 6| Step: 3
Training loss: 2.660488561636129
Validation loss: 2.5025501370662644

Epoch: 6| Step: 4
Training loss: 2.69454653263597
Validation loss: 2.5009603317131344

Epoch: 6| Step: 5
Training loss: 2.4015774628986537
Validation loss: 2.4985130030582052

Epoch: 6| Step: 6
Training loss: 3.3219682222852533
Validation loss: 2.4904207426012137

Epoch: 6| Step: 7
Training loss: 2.4175183889011165
Validation loss: 2.5009881763557216

Epoch: 6| Step: 8
Training loss: 2.1756003285767744
Validation loss: 2.4898918967385155

Epoch: 6| Step: 9
Training loss: 2.5627404076918903
Validation loss: 2.4884743257800266

Epoch: 6| Step: 10
Training loss: 2.972630263827778
Validation loss: 2.4900415535770786

Epoch: 6| Step: 11
Training loss: 2.268174729568099
Validation loss: 2.489998421956581

Epoch: 6| Step: 12
Training loss: 2.5342805405028055
Validation loss: 2.4902142012742305

Epoch: 6| Step: 13
Training loss: 2.933621707381762
Validation loss: 2.486166384231033

Epoch: 118| Step: 0
Training loss: 1.7852022594975236
Validation loss: 2.480321893857523

Epoch: 6| Step: 1
Training loss: 2.8333336512247542
Validation loss: 2.4720425076309698

Epoch: 6| Step: 2
Training loss: 2.429322537306418
Validation loss: 2.471541150255123

Epoch: 6| Step: 3
Training loss: 2.9119333460903176
Validation loss: 2.4684017523009234

Epoch: 6| Step: 4
Training loss: 2.5679518227618443
Validation loss: 2.471498012143833

Epoch: 6| Step: 5
Training loss: 2.4651563539883985
Validation loss: 2.467385743385195

Epoch: 6| Step: 6
Training loss: 2.6965446862200326
Validation loss: 2.492046979085866

Epoch: 6| Step: 7
Training loss: 2.042805594061607
Validation loss: 2.480895607964822

Epoch: 6| Step: 8
Training loss: 2.895786934533486
Validation loss: 2.4736815864593074

Epoch: 6| Step: 9
Training loss: 2.266095448024934
Validation loss: 2.460619710185754

Epoch: 6| Step: 10
Training loss: 2.5755984786736357
Validation loss: 2.4806333921796506

Epoch: 6| Step: 11
Training loss: 2.6190632489386196
Validation loss: 2.4923355624839925

Epoch: 6| Step: 12
Training loss: 2.8817384781576103
Validation loss: 2.5027336720596423

Epoch: 6| Step: 13
Training loss: 2.8922279655968457
Validation loss: 2.496809024286437

Epoch: 119| Step: 0
Training loss: 2.6489559780104877
Validation loss: 2.4734685309122724

Epoch: 6| Step: 1
Training loss: 2.261569161483697
Validation loss: 2.482881261164744

Epoch: 6| Step: 2
Training loss: 2.734385027185856
Validation loss: 2.494807072697076

Epoch: 6| Step: 3
Training loss: 2.8684860739713174
Validation loss: 2.479912688456336

Epoch: 6| Step: 4
Training loss: 3.3532440306626663
Validation loss: 2.4862559173958516

Epoch: 6| Step: 5
Training loss: 1.9162886772059728
Validation loss: 2.4895986820063087

Epoch: 6| Step: 6
Training loss: 2.8725729939853446
Validation loss: 2.4834573983007067

Epoch: 6| Step: 7
Training loss: 2.571569442674752
Validation loss: 2.4823788991198956

Epoch: 6| Step: 8
Training loss: 2.6631214795540328
Validation loss: 2.484409751244018

Epoch: 6| Step: 9
Training loss: 2.0327377739556116
Validation loss: 2.476765593344163

Epoch: 6| Step: 10
Training loss: 2.130181894550235
Validation loss: 2.5081175222929555

Epoch: 6| Step: 11
Training loss: 2.545530654803567
Validation loss: 2.4961535026688817

Epoch: 6| Step: 12
Training loss: 2.7925608517156197
Validation loss: 2.4827533004752205

Epoch: 6| Step: 13
Training loss: 2.311668040140629
Validation loss: 2.4773730639543268

Epoch: 120| Step: 0
Training loss: 2.5041018214745208
Validation loss: 2.4897770761000246

Epoch: 6| Step: 1
Training loss: 2.6716978672495824
Validation loss: 2.4974460046078892

Epoch: 6| Step: 2
Training loss: 2.5220128812287013
Validation loss: 2.4775946167235547

Epoch: 6| Step: 3
Training loss: 2.370409092101196
Validation loss: 2.475773869095944

Epoch: 6| Step: 4
Training loss: 2.4775606183778462
Validation loss: 2.4914657345350792

Epoch: 6| Step: 5
Training loss: 3.051085707239297
Validation loss: 2.491911256735245

Epoch: 6| Step: 6
Training loss: 3.3600670390026144
Validation loss: 2.4835088593360077

Epoch: 6| Step: 7
Training loss: 2.8401614172825993
Validation loss: 2.4941508142080404

Epoch: 6| Step: 8
Training loss: 1.833771227068215
Validation loss: 2.4929304784422355

Epoch: 6| Step: 9
Training loss: 2.3301878117519004
Validation loss: 2.4797876992903936

Epoch: 6| Step: 10
Training loss: 2.426216025097405
Validation loss: 2.4746031052537774

Epoch: 6| Step: 11
Training loss: 2.6590554176457712
Validation loss: 2.4944711214175905

Epoch: 6| Step: 12
Training loss: 2.33256220563917
Validation loss: 2.4817175432967615

Epoch: 6| Step: 13
Training loss: 2.0545454665143357
Validation loss: 2.4955869445794487

Epoch: 121| Step: 0
Training loss: 3.0896738345643704
Validation loss: 2.4914561599615186

Epoch: 6| Step: 1
Training loss: 2.507300208700157
Validation loss: 2.4642541170796175

Epoch: 6| Step: 2
Training loss: 2.3613466020717855
Validation loss: 2.50340388742593

Epoch: 6| Step: 3
Training loss: 2.965398241544807
Validation loss: 2.5008635137115953

Epoch: 6| Step: 4
Training loss: 3.3550507859139795
Validation loss: 2.459355012866899

Epoch: 6| Step: 5
Training loss: 2.6116682015199255
Validation loss: 2.481254726868389

Epoch: 6| Step: 6
Training loss: 2.0540988789709815
Validation loss: 2.504938686505444

Epoch: 6| Step: 7
Training loss: 1.987874707101847
Validation loss: 2.4798416297082375

Epoch: 6| Step: 8
Training loss: 1.9544637745651994
Validation loss: 2.478136882994791

Epoch: 6| Step: 9
Training loss: 2.659289786097572
Validation loss: 2.4653844846580752

Epoch: 6| Step: 10
Training loss: 2.470082370141369
Validation loss: 2.5120417483983752

Epoch: 6| Step: 11
Training loss: 2.109626246430321
Validation loss: 2.4986547501322627

Epoch: 6| Step: 12
Training loss: 2.2511051430873628
Validation loss: 2.4919202318384186

Epoch: 6| Step: 13
Training loss: 3.4078330464817186
Validation loss: 2.4729896487436904

Epoch: 122| Step: 0
Training loss: 2.4102791085607804
Validation loss: 2.4919843950657534

Epoch: 6| Step: 1
Training loss: 2.0647100687503968
Validation loss: 2.4715270724681546

Epoch: 6| Step: 2
Training loss: 2.1463353085323904
Validation loss: 2.4841448975986475

Epoch: 6| Step: 3
Training loss: 2.948058934816992
Validation loss: 2.485279653517734

Epoch: 6| Step: 4
Training loss: 3.1000531653490593
Validation loss: 2.4749770069266175

Epoch: 6| Step: 5
Training loss: 2.697325554581779
Validation loss: 2.4929564568451053

Epoch: 6| Step: 6
Training loss: 2.389029575118528
Validation loss: 2.4890592032531624

Epoch: 6| Step: 7
Training loss: 2.9025617597188065
Validation loss: 2.4861752728327433

Epoch: 6| Step: 8
Training loss: 2.881179140092592
Validation loss: 2.478078155618976

Epoch: 6| Step: 9
Training loss: 2.9721661638655807
Validation loss: 2.4787638843661686

Epoch: 6| Step: 10
Training loss: 1.686102889241858
Validation loss: 2.483783137791979

Epoch: 6| Step: 11
Training loss: 2.610638523847617
Validation loss: 2.491743135268315

Epoch: 6| Step: 12
Training loss: 2.2973767206202282
Validation loss: 2.4904252925477603

Epoch: 6| Step: 13
Training loss: 2.137967764551646
Validation loss: 2.491583603215806

Epoch: 123| Step: 0
Training loss: 1.9067763555537596
Validation loss: 2.485208440820746

Epoch: 6| Step: 1
Training loss: 2.5455138893145643
Validation loss: 2.4844579669257905

Epoch: 6| Step: 2
Training loss: 2.6741359135076013
Validation loss: 2.4972778646259384

Epoch: 6| Step: 3
Training loss: 2.5265730990327313
Validation loss: 2.477997870826404

Epoch: 6| Step: 4
Training loss: 2.130851095237695
Validation loss: 2.470840071965525

Epoch: 6| Step: 5
Training loss: 2.9685816666914193
Validation loss: 2.496850704510405

Epoch: 6| Step: 6
Training loss: 2.373160000840104
Validation loss: 2.4837363385672924

Epoch: 6| Step: 7
Training loss: 2.6273068781910385
Validation loss: 2.4940954884560007

Epoch: 6| Step: 8
Training loss: 2.6519480256961394
Validation loss: 2.486853315539369

Epoch: 6| Step: 9
Training loss: 2.3813747383347
Validation loss: 2.483163045914021

Epoch: 6| Step: 10
Training loss: 2.203328873961183
Validation loss: 2.4868806264217556

Epoch: 6| Step: 11
Training loss: 2.943671540917584
Validation loss: 2.485385125226911

Epoch: 6| Step: 12
Training loss: 3.026653462127696
Validation loss: 2.5003839813188296

Epoch: 6| Step: 13
Training loss: 2.7036156181558213
Validation loss: 2.4935752935957987

Epoch: 124| Step: 0
Training loss: 2.9094370113201204
Validation loss: 2.4827373811430893

Epoch: 6| Step: 1
Training loss: 2.935696474325586
Validation loss: 2.4941519284092397

Epoch: 6| Step: 2
Training loss: 1.8499920458236019
Validation loss: 2.475874659113223

Epoch: 6| Step: 3
Training loss: 2.058290752428627
Validation loss: 2.488700849994479

Epoch: 6| Step: 4
Training loss: 2.95156512363816
Validation loss: 2.4890405742299415

Epoch: 6| Step: 5
Training loss: 2.179095553616962
Validation loss: 2.480526239479692

Epoch: 6| Step: 6
Training loss: 2.7861112479449366
Validation loss: 2.4882739844145054

Epoch: 6| Step: 7
Training loss: 2.894536356850674
Validation loss: 2.504983904503184

Epoch: 6| Step: 8
Training loss: 2.553911283668372
Validation loss: 2.4728310279653174

Epoch: 6| Step: 9
Training loss: 2.546372627372923
Validation loss: 2.484268649525894

Epoch: 6| Step: 10
Training loss: 2.8190774934583716
Validation loss: 2.492356741468558

Epoch: 6| Step: 11
Training loss: 2.3390411885627866
Validation loss: 2.4989289255609854

Epoch: 6| Step: 12
Training loss: 2.2918888909070048
Validation loss: 2.4883814515108846

Epoch: 6| Step: 13
Training loss: 2.3897011162205457
Validation loss: 2.481385714994915

Epoch: 125| Step: 0
Training loss: 2.172320752403858
Validation loss: 2.495436470726357

Epoch: 6| Step: 1
Training loss: 2.662690556114178
Validation loss: 2.4876172463178112

Epoch: 6| Step: 2
Training loss: 3.180083678961954
Validation loss: 2.492413602911786

Epoch: 6| Step: 3
Training loss: 2.3105695760454976
Validation loss: 2.4873862178835706

Epoch: 6| Step: 4
Training loss: 2.6669779734894474
Validation loss: 2.4689986577632723

Epoch: 6| Step: 5
Training loss: 2.545850114616198
Validation loss: 2.475071485739757

Epoch: 6| Step: 6
Training loss: 3.147345341646665
Validation loss: 2.4989180972075826

Epoch: 6| Step: 7
Training loss: 2.4700920223746774
Validation loss: 2.491892508088972

Epoch: 6| Step: 8
Training loss: 2.051198455516275
Validation loss: 2.4953502083138583

Epoch: 6| Step: 9
Training loss: 2.1482765831072252
Validation loss: 2.474819215859354

Epoch: 6| Step: 10
Training loss: 2.804826483298015
Validation loss: 2.4797561263926595

Epoch: 6| Step: 11
Training loss: 1.9337264198488082
Validation loss: 2.475692644396522

Epoch: 6| Step: 12
Training loss: 2.8650323319399944
Validation loss: 2.489500057684494

Epoch: 6| Step: 13
Training loss: 2.0494682831982174
Validation loss: 2.474663905168165

Epoch: 126| Step: 0
Training loss: 2.5619608614312144
Validation loss: 2.4803524647337447

Epoch: 6| Step: 1
Training loss: 2.7446427615538247
Validation loss: 2.493447901759916

Epoch: 6| Step: 2
Training loss: 2.326481591144255
Validation loss: 2.4864062967582785

Epoch: 6| Step: 3
Training loss: 2.1960341601190243
Validation loss: 2.480549799187897

Epoch: 6| Step: 4
Training loss: 2.927088691100153
Validation loss: 2.491668339524788

Epoch: 6| Step: 5
Training loss: 2.2473968706242777
Validation loss: 2.47282712573712

Epoch: 6| Step: 6
Training loss: 3.010357619677777
Validation loss: 2.5023942250051614

Epoch: 6| Step: 7
Training loss: 2.6874932577359103
Validation loss: 2.4919921765540844

Epoch: 6| Step: 8
Training loss: 2.0301750273017083
Validation loss: 2.491022773457673

Epoch: 6| Step: 9
Training loss: 2.6230568732939568
Validation loss: 2.48679676818384

Epoch: 6| Step: 10
Training loss: 2.551764352806869
Validation loss: 2.480877448788103

Epoch: 6| Step: 11
Training loss: 2.7430236063409748
Validation loss: 2.488056863228629

Epoch: 6| Step: 12
Training loss: 2.2428032963183737
Validation loss: 2.4742297407839895

Epoch: 6| Step: 13
Training loss: 2.9173702662894683
Validation loss: 2.4741780349421143

Epoch: 127| Step: 0
Training loss: 2.21481791486255
Validation loss: 2.500983142324793

Epoch: 6| Step: 1
Training loss: 1.6464252936463877
Validation loss: 2.5116279595976208

Epoch: 6| Step: 2
Training loss: 3.5085332932802777
Validation loss: 2.484604135437136

Epoch: 6| Step: 3
Training loss: 2.7518040201744016
Validation loss: 2.501127417468677

Epoch: 6| Step: 4
Training loss: 2.671509120942013
Validation loss: 2.488260757532675

Epoch: 6| Step: 5
Training loss: 2.004966054043971
Validation loss: 2.5016837592448726

Epoch: 6| Step: 6
Training loss: 2.2627638014963702
Validation loss: 2.4652300874575688

Epoch: 6| Step: 7
Training loss: 2.2408762430509013
Validation loss: 2.4909152134490333

Epoch: 6| Step: 8
Training loss: 2.73538869323915
Validation loss: 2.500849844961796

Epoch: 6| Step: 9
Training loss: 2.865448385305961
Validation loss: 2.496913036678047

Epoch: 6| Step: 10
Training loss: 2.6201379343662388
Validation loss: 2.473612544834806

Epoch: 6| Step: 11
Training loss: 2.0782795325165235
Validation loss: 2.494648612145623

Epoch: 6| Step: 12
Training loss: 3.054195276588683
Validation loss: 2.483958165950519

Epoch: 6| Step: 13
Training loss: 2.2897119219292206
Validation loss: 2.501395541531572

Epoch: 128| Step: 0
Training loss: 3.4166269687734423
Validation loss: 2.4777571353462977

Epoch: 6| Step: 1
Training loss: 2.2907500312269793
Validation loss: 2.4727098906622307

Epoch: 6| Step: 2
Training loss: 2.2446998052247813
Validation loss: 2.4913286278658684

Epoch: 6| Step: 3
Training loss: 2.4425936565579485
Validation loss: 2.4996933113173982

Epoch: 6| Step: 4
Training loss: 2.3644510195735506
Validation loss: 2.4930260754087086

Epoch: 6| Step: 5
Training loss: 3.0026241587458267
Validation loss: 2.48525850293567

Epoch: 6| Step: 6
Training loss: 2.353622193515682
Validation loss: 2.4827274151162984

Epoch: 6| Step: 7
Training loss: 2.4177303165837225
Validation loss: 2.4916378575859053

Epoch: 6| Step: 8
Training loss: 2.7457388028518532
Validation loss: 2.4925777039692574

Epoch: 6| Step: 9
Training loss: 2.3985409791971324
Validation loss: 2.4867139803216465

Epoch: 6| Step: 10
Training loss: 2.4482934107251118
Validation loss: 2.4982156784780476

Epoch: 6| Step: 11
Training loss: 2.7477133520731623
Validation loss: 2.4895170160653795

Epoch: 6| Step: 12
Training loss: 1.789736025435306
Validation loss: 2.480547246449133

Epoch: 6| Step: 13
Training loss: 2.6941861217805956
Validation loss: 2.4863545105824594

Epoch: 129| Step: 0
Training loss: 2.307563254831561
Validation loss: 2.4717187379918353

Epoch: 6| Step: 1
Training loss: 2.9059848459351416
Validation loss: 2.4926150437384424

Epoch: 6| Step: 2
Training loss: 2.1287812642496897
Validation loss: 2.4898686828346164

Epoch: 6| Step: 3
Training loss: 2.751726995400226
Validation loss: 2.4786236765459972

Epoch: 6| Step: 4
Training loss: 2.0033400778675428
Validation loss: 2.488294346957586

Epoch: 6| Step: 5
Training loss: 2.6024221655266677
Validation loss: 2.4893648581439716

Epoch: 6| Step: 6
Training loss: 2.0142216491559988
Validation loss: 2.4908400511409607

Epoch: 6| Step: 7
Training loss: 3.2256169815717737
Validation loss: 2.475575421272082

Epoch: 6| Step: 8
Training loss: 1.9249160401501144
Validation loss: 2.476250012830989

Epoch: 6| Step: 9
Training loss: 2.58914013909079
Validation loss: 2.506920709460605

Epoch: 6| Step: 10
Training loss: 3.144703356848732
Validation loss: 2.479581091113615

Epoch: 6| Step: 11
Training loss: 2.363316761096838
Validation loss: 2.485383785325883

Epoch: 6| Step: 12
Training loss: 2.074984364278322
Validation loss: 2.495974070897677

Epoch: 6| Step: 13
Training loss: 3.2738478228157715
Validation loss: 2.4752183007283075

Epoch: 130| Step: 0
Training loss: 2.0555462221271674
Validation loss: 2.4841963727715957

Epoch: 6| Step: 1
Training loss: 2.2427116605453747
Validation loss: 2.4879368742888146

Epoch: 6| Step: 2
Training loss: 2.1990244696601406
Validation loss: 2.49889794536823

Epoch: 6| Step: 3
Training loss: 1.9708547089635995
Validation loss: 2.4943187533512434

Epoch: 6| Step: 4
Training loss: 2.399892061508365
Validation loss: 2.5023005052902403

Epoch: 6| Step: 5
Training loss: 2.907794664954456
Validation loss: 2.492833702344026

Epoch: 6| Step: 6
Training loss: 2.1050001199681483
Validation loss: 2.4881976275150985

Epoch: 6| Step: 7
Training loss: 2.6606492357357703
Validation loss: 2.5188941988997517

Epoch: 6| Step: 8
Training loss: 3.5042086229447182
Validation loss: 2.5010989983231395

Epoch: 6| Step: 9
Training loss: 2.700687080840386
Validation loss: 2.4840228207194013

Epoch: 6| Step: 10
Training loss: 3.1716565587785923
Validation loss: 2.4757568605862295

Epoch: 6| Step: 11
Training loss: 2.543758053028753
Validation loss: 2.489555082411019

Epoch: 6| Step: 12
Training loss: 2.219810232523111
Validation loss: 2.497765191316499

Epoch: 6| Step: 13
Training loss: 2.0367990844475785
Validation loss: 2.491024587851681

Epoch: 131| Step: 0
Training loss: 2.3907523526446703
Validation loss: 2.499262352273458

Epoch: 6| Step: 1
Training loss: 2.569235625387634
Validation loss: 2.476536305115208

Epoch: 6| Step: 2
Training loss: 2.3672683752363795
Validation loss: 2.4736919262658956

Epoch: 6| Step: 3
Training loss: 2.086784847902228
Validation loss: 2.5008361136053705

Epoch: 6| Step: 4
Training loss: 3.050279173034374
Validation loss: 2.496609256758525

Epoch: 6| Step: 5
Training loss: 2.8948992496543666
Validation loss: 2.487906383212509

Epoch: 6| Step: 6
Training loss: 2.76571741326886
Validation loss: 2.48604383584031

Epoch: 6| Step: 7
Training loss: 2.240325260729956
Validation loss: 2.502429550707066

Epoch: 6| Step: 8
Training loss: 3.0261210673557715
Validation loss: 2.484424504149139

Epoch: 6| Step: 9
Training loss: 2.586061411306173
Validation loss: 2.4995900771714634

Epoch: 6| Step: 10
Training loss: 2.2845599056612484
Validation loss: 2.4899682109480694

Epoch: 6| Step: 11
Training loss: 2.2407635676526394
Validation loss: 2.5023209263221466

Epoch: 6| Step: 12
Training loss: 2.5037109489271296
Validation loss: 2.494490458066225

Epoch: 6| Step: 13
Training loss: 1.9197316817711083
Validation loss: 2.4948940033036107

Epoch: 132| Step: 0
Training loss: 1.5395522524424068
Validation loss: 2.480780364741006

Epoch: 6| Step: 1
Training loss: 2.9904957264032714
Validation loss: 2.490925155465118

Epoch: 6| Step: 2
Training loss: 1.844826594114882
Validation loss: 2.5002189263499788

Epoch: 6| Step: 3
Training loss: 2.202786899893581
Validation loss: 2.4906745942659283

Epoch: 6| Step: 4
Training loss: 2.1244740676787517
Validation loss: 2.4689233588456387

Epoch: 6| Step: 5
Training loss: 2.9263275003253066
Validation loss: 2.484311425542389

Epoch: 6| Step: 6
Training loss: 2.4371528378264538
Validation loss: 2.4848904164354746

Epoch: 6| Step: 7
Training loss: 1.88804349337605
Validation loss: 2.4800046781700225

Epoch: 6| Step: 8
Training loss: 1.970915073154076
Validation loss: 2.4905016377638347

Epoch: 6| Step: 9
Training loss: 2.855314446284017
Validation loss: 2.47975803690556

Epoch: 6| Step: 10
Training loss: 3.0509095696145
Validation loss: 2.4867845138365174

Epoch: 6| Step: 11
Training loss: 2.7778307008999934
Validation loss: 2.4871015672672243

Epoch: 6| Step: 12
Training loss: 3.301169829660797
Validation loss: 2.5065992649260815

Epoch: 6| Step: 13
Training loss: 3.1195874452485293
Validation loss: 2.48633387025501

Epoch: 133| Step: 0
Training loss: 2.730710085789379
Validation loss: 2.498234272947628

Epoch: 6| Step: 1
Training loss: 2.431449717185895
Validation loss: 2.4864971961565274

Epoch: 6| Step: 2
Training loss: 2.999635992218961
Validation loss: 2.50210960245772

Epoch: 6| Step: 3
Training loss: 2.619369644745741
Validation loss: 2.4927613890542464

Epoch: 6| Step: 4
Training loss: 2.0606711399881776
Validation loss: 2.50933721413254

Epoch: 6| Step: 5
Training loss: 2.1732820851577186
Validation loss: 2.4888998833686076

Epoch: 6| Step: 6
Training loss: 2.7539005360645508
Validation loss: 2.482345357712054

Epoch: 6| Step: 7
Training loss: 2.6271256514268613
Validation loss: 2.497081689913226

Epoch: 6| Step: 8
Training loss: 2.4194283216863597
Validation loss: 2.483892552736536

Epoch: 6| Step: 9
Training loss: 2.1304032688128682
Validation loss: 2.488400189522412

Epoch: 6| Step: 10
Training loss: 3.381650518081434
Validation loss: 2.489951929509795

Epoch: 6| Step: 11
Training loss: 2.469413861215379
Validation loss: 2.5020443335806912

Epoch: 6| Step: 12
Training loss: 1.7607405626032435
Validation loss: 2.4847946581320755

Epoch: 6| Step: 13
Training loss: 2.4132332473184466
Validation loss: 2.4874964121990906

Epoch: 134| Step: 0
Training loss: 1.9850177474874036
Validation loss: 2.5029363441305947

Epoch: 6| Step: 1
Training loss: 2.8155893948047037
Validation loss: 2.4753374391544067

Epoch: 6| Step: 2
Training loss: 2.5395258554072613
Validation loss: 2.469833516317581

Epoch: 6| Step: 3
Training loss: 2.514822126889896
Validation loss: 2.4928868260534496

Epoch: 6| Step: 4
Training loss: 2.689692645187128
Validation loss: 2.4907861808003875

Epoch: 6| Step: 5
Training loss: 2.584317901607897
Validation loss: 2.4974714433235

Epoch: 6| Step: 6
Training loss: 1.5268995805058612
Validation loss: 2.476654482688288

Epoch: 6| Step: 7
Training loss: 3.112667621557117
Validation loss: 2.5032542613609747

Epoch: 6| Step: 8
Training loss: 1.8536836087767927
Validation loss: 2.503305904286276

Epoch: 6| Step: 9
Training loss: 2.373044463733343
Validation loss: 2.5003047285686355

Epoch: 6| Step: 10
Training loss: 2.1325741589297573
Validation loss: 2.506620668570889

Epoch: 6| Step: 11
Training loss: 2.395521049602899
Validation loss: 2.4853641065422845

Epoch: 6| Step: 12
Training loss: 3.3133624411621354
Validation loss: 2.4967258260392478

Epoch: 6| Step: 13
Training loss: 3.1539332885943874
Validation loss: 2.5053509093959994

Epoch: 135| Step: 0
Training loss: 2.4754021751323365
Validation loss: 2.5013101067549695

Epoch: 6| Step: 1
Training loss: 1.859195347930485
Validation loss: 2.496208635430804

Epoch: 6| Step: 2
Training loss: 2.81273989184171
Validation loss: 2.49114850485062

Epoch: 6| Step: 3
Training loss: 2.4931422590115937
Validation loss: 2.5031197852589053

Epoch: 6| Step: 4
Training loss: 3.0965771328808156
Validation loss: 2.50117453189985

Epoch: 6| Step: 5
Training loss: 2.262611226251038
Validation loss: 2.490517298506798

Epoch: 6| Step: 6
Training loss: 2.7745964100473173
Validation loss: 2.4782206142725096

Epoch: 6| Step: 7
Training loss: 2.068684178714143
Validation loss: 2.488355221354511

Epoch: 6| Step: 8
Training loss: 2.4891253466433727
Validation loss: 2.485984532137774

Epoch: 6| Step: 9
Training loss: 2.449223811921603
Validation loss: 2.5037756035288314

Epoch: 6| Step: 10
Training loss: 2.8136440069705735
Validation loss: 2.4967174663247134

Epoch: 6| Step: 11
Training loss: 2.8465689635949367
Validation loss: 2.5051583243386912

Epoch: 6| Step: 12
Training loss: 2.408966116838341
Validation loss: 2.4811625851181653

Epoch: 6| Step: 13
Training loss: 2.09730342121139
Validation loss: 2.5026816854345175

Epoch: 136| Step: 0
Training loss: 2.619374741936764
Validation loss: 2.483280612870172

Epoch: 6| Step: 1
Training loss: 2.2552595125400954
Validation loss: 2.476323173731234

Epoch: 6| Step: 2
Training loss: 2.05038465752281
Validation loss: 2.4848609263870314

Epoch: 6| Step: 3
Training loss: 2.5610301756833125
Validation loss: 2.4899827620684714

Epoch: 6| Step: 4
Training loss: 1.8671730711311614
Validation loss: 2.4773097071900216

Epoch: 6| Step: 5
Training loss: 2.624124290081345
Validation loss: 2.4975434775608694

Epoch: 6| Step: 6
Training loss: 2.691754544204415
Validation loss: 2.455282447295639

Epoch: 6| Step: 7
Training loss: 2.5090661643872507
Validation loss: 2.496330362350666

Epoch: 6| Step: 8
Training loss: 1.986510561926738
Validation loss: 2.4776884628569134

Epoch: 6| Step: 9
Training loss: 2.760063484678047
Validation loss: 2.497196231150307

Epoch: 6| Step: 10
Training loss: 3.178839943138729
Validation loss: 2.481599698776707

Epoch: 6| Step: 11
Training loss: 2.305340690507653
Validation loss: 2.48921595982269

Epoch: 6| Step: 12
Training loss: 2.756925359147956
Validation loss: 2.4856487024098124

Epoch: 6| Step: 13
Training loss: 2.893665590970761
Validation loss: 2.49790707935242

Epoch: 137| Step: 0
Training loss: 2.5686604950618914
Validation loss: 2.4895427911826067

Epoch: 6| Step: 1
Training loss: 2.163581216224438
Validation loss: 2.492537731643262

Epoch: 6| Step: 2
Training loss: 2.6574776897767887
Validation loss: 2.509489108073018

Epoch: 6| Step: 3
Training loss: 2.4429170130322184
Validation loss: 2.4886579578209775

Epoch: 6| Step: 4
Training loss: 2.425315333048814
Validation loss: 2.5088823651016248

Epoch: 6| Step: 5
Training loss: 2.283991068293115
Validation loss: 2.4842217975294045

Epoch: 6| Step: 6
Training loss: 1.9241848222821276
Validation loss: 2.485032752789239

Epoch: 6| Step: 7
Training loss: 3.159523805157037
Validation loss: 2.4927568675399425

Epoch: 6| Step: 8
Training loss: 2.400265273692522
Validation loss: 2.48665058319271

Epoch: 6| Step: 9
Training loss: 2.1481793612676454
Validation loss: 2.4861291095027678

Epoch: 6| Step: 10
Training loss: 2.7967000725455646
Validation loss: 2.497882637530328

Epoch: 6| Step: 11
Training loss: 2.8043414994294764
Validation loss: 2.481581286485403

Epoch: 6| Step: 12
Training loss: 2.5277032375407473
Validation loss: 2.4835086632055456

Epoch: 6| Step: 13
Training loss: 3.1452024892565014
Validation loss: 2.4891172616300166

Epoch: 138| Step: 0
Training loss: 2.9400140271533397
Validation loss: 2.4889872293789157

Epoch: 6| Step: 1
Training loss: 2.2151725834418348
Validation loss: 2.4813625310278833

Epoch: 6| Step: 2
Training loss: 2.9282576263258075
Validation loss: 2.485240002217985

Epoch: 6| Step: 3
Training loss: 2.5620599694540442
Validation loss: 2.495663599287995

Epoch: 6| Step: 4
Training loss: 1.8209214399423976
Validation loss: 2.488682301728502

Epoch: 6| Step: 5
Training loss: 2.5763251293023948
Validation loss: 2.4977577393187427

Epoch: 6| Step: 6
Training loss: 2.154012431286826
Validation loss: 2.4828557488532725

Epoch: 6| Step: 7
Training loss: 2.750964602443384
Validation loss: 2.4804933706464527

Epoch: 6| Step: 8
Training loss: 2.5835680290997427
Validation loss: 2.4743489641077914

Epoch: 6| Step: 9
Training loss: 2.2592127065025673
Validation loss: 2.48308850794068

Epoch: 6| Step: 10
Training loss: 2.1853531247899722
Validation loss: 2.5003473891630583

Epoch: 6| Step: 11
Training loss: 2.932170822517316
Validation loss: 2.5003411521715493

Epoch: 6| Step: 12
Training loss: 2.4475253394337737
Validation loss: 2.4817271905644067

Epoch: 6| Step: 13
Training loss: 2.5408855759740288
Validation loss: 2.508977251069453

Epoch: 139| Step: 0
Training loss: 2.673652280900611
Validation loss: 2.493736431496021

Epoch: 6| Step: 1
Training loss: 3.0204991298270776
Validation loss: 2.5011459513417993

Epoch: 6| Step: 2
Training loss: 2.67209771549842
Validation loss: 2.4788960414013474

Epoch: 6| Step: 3
Training loss: 2.662925410424707
Validation loss: 2.4755586003923966

Epoch: 6| Step: 4
Training loss: 2.895953571558606
Validation loss: 2.4901734755048803

Epoch: 6| Step: 5
Training loss: 2.228726376222707
Validation loss: 2.4965076710906415

Epoch: 6| Step: 6
Training loss: 2.2125762150455413
Validation loss: 2.4854051442318403

Epoch: 6| Step: 7
Training loss: 2.282759702517117
Validation loss: 2.4891734380259964

Epoch: 6| Step: 8
Training loss: 2.6712685058409704
Validation loss: 2.4785464151954097

Epoch: 6| Step: 9
Training loss: 2.5006802587069425
Validation loss: 2.476482834807097

Epoch: 6| Step: 10
Training loss: 2.449496945486911
Validation loss: 2.492489631117734

Epoch: 6| Step: 11
Training loss: 1.9387779943140917
Validation loss: 2.484946125095459

Epoch: 6| Step: 12
Training loss: 2.201943154927424
Validation loss: 2.4616570679430696

Epoch: 6| Step: 13
Training loss: 2.251449330097956
Validation loss: 2.4964606241610596

Epoch: 140| Step: 0
Training loss: 1.783137074779618
Validation loss: 2.486915073085363

Epoch: 6| Step: 1
Training loss: 2.605646175994799
Validation loss: 2.5012030106599803

Epoch: 6| Step: 2
Training loss: 2.798861772929936
Validation loss: 2.489478684481617

Epoch: 6| Step: 3
Training loss: 3.074617818017009
Validation loss: 2.479372373963704

Epoch: 6| Step: 4
Training loss: 2.451823478505534
Validation loss: 2.4958780404014878

Epoch: 6| Step: 5
Training loss: 2.1860358651683853
Validation loss: 2.4777460975497254

Epoch: 6| Step: 6
Training loss: 1.760495592262928
Validation loss: 2.473936373360043

Epoch: 6| Step: 7
Training loss: 2.6544126551332763
Validation loss: 2.495835620808478

Epoch: 6| Step: 8
Training loss: 2.58559781981178
Validation loss: 2.497309821671443

Epoch: 6| Step: 9
Training loss: 2.3921280667267943
Validation loss: 2.516409961745215

Epoch: 6| Step: 10
Training loss: 2.6281133989402377
Validation loss: 2.4958191485863592

Epoch: 6| Step: 11
Training loss: 2.5946434963886755
Validation loss: 2.486107854779327

Epoch: 6| Step: 12
Training loss: 2.653791838507939
Validation loss: 2.4706888691551967

Epoch: 6| Step: 13
Training loss: 2.3932624180877147
Validation loss: 2.49446176392631

Epoch: 141| Step: 0
Training loss: 2.8517114887769006
Validation loss: 2.4723814650729334

Epoch: 6| Step: 1
Training loss: 2.691424321306348
Validation loss: 2.491643969231164

Epoch: 6| Step: 2
Training loss: 2.9268498627395556
Validation loss: 2.467439258099306

Epoch: 6| Step: 3
Training loss: 2.1556709728387697
Validation loss: 2.4904569762097553

Epoch: 6| Step: 4
Training loss: 2.9738661200038967
Validation loss: 2.4732218789444067

Epoch: 6| Step: 5
Training loss: 2.3307098785682054
Validation loss: 2.4907526507110687

Epoch: 6| Step: 6
Training loss: 2.093399473996077
Validation loss: 2.483350378807862

Epoch: 6| Step: 7
Training loss: 2.583096883063758
Validation loss: 2.490500468402449

Epoch: 6| Step: 8
Training loss: 2.4372974213758485
Validation loss: 2.4825399510510358

Epoch: 6| Step: 9
Training loss: 2.2872504134632785
Validation loss: 2.505032594070138

Epoch: 6| Step: 10
Training loss: 2.531298036472727
Validation loss: 2.474731468200008

Epoch: 6| Step: 11
Training loss: 2.0146803897278365
Validation loss: 2.4930743712452546

Epoch: 6| Step: 12
Training loss: 2.7650316539421835
Validation loss: 2.48321628359398

Epoch: 6| Step: 13
Training loss: 1.5410208079513203
Validation loss: 2.4796807002761945

Epoch: 142| Step: 0
Training loss: 2.4023526819574808
Validation loss: 2.473610879346855

Epoch: 6| Step: 1
Training loss: 2.685665835433414
Validation loss: 2.49950351091976

Epoch: 6| Step: 2
Training loss: 2.847999499738842
Validation loss: 2.5033280483463316

Epoch: 6| Step: 3
Training loss: 2.484117026700584
Validation loss: 2.503430937875865

Epoch: 6| Step: 4
Training loss: 2.229260118848905
Validation loss: 2.4892457401994736

Epoch: 6| Step: 5
Training loss: 2.8484023049629013
Validation loss: 2.4801107535687557

Epoch: 6| Step: 6
Training loss: 2.5243027092522468
Validation loss: 2.48817947732572

Epoch: 6| Step: 7
Training loss: 2.4577032240761447
Validation loss: 2.4722931305643407

Epoch: 6| Step: 8
Training loss: 2.4168672039970653
Validation loss: 2.4677811623590324

Epoch: 6| Step: 9
Training loss: 1.8791315970633966
Validation loss: 2.4819664032269015

Epoch: 6| Step: 10
Training loss: 2.140380622669693
Validation loss: 2.476445702113213

Epoch: 6| Step: 11
Training loss: 3.017615259429683
Validation loss: 2.498975824972296

Epoch: 6| Step: 12
Training loss: 2.2869456010227602
Validation loss: 2.4831212856388762

Epoch: 6| Step: 13
Training loss: 2.091890676694403
Validation loss: 2.466895734601532

Epoch: 143| Step: 0
Training loss: 1.5730821467713385
Validation loss: 2.5005476946424907

Epoch: 6| Step: 1
Training loss: 2.659660752540353
Validation loss: 2.4809236044365854

Epoch: 6| Step: 2
Training loss: 2.3478360987878264
Validation loss: 2.4914552575522046

Epoch: 6| Step: 3
Training loss: 2.5780582303735455
Validation loss: 2.5064725028170898

Epoch: 6| Step: 4
Training loss: 2.4447320121136933
Validation loss: 2.4970879329926094

Epoch: 6| Step: 5
Training loss: 2.4968700843352054
Validation loss: 2.4978633220098225

Epoch: 6| Step: 6
Training loss: 2.5643763301882174
Validation loss: 2.466213272478459

Epoch: 6| Step: 7
Training loss: 2.6109163220663585
Validation loss: 2.4999279647621524

Epoch: 6| Step: 8
Training loss: 1.8603811667181906
Validation loss: 2.490322422608002

Epoch: 6| Step: 9
Training loss: 2.993561511387554
Validation loss: 2.477550840029081

Epoch: 6| Step: 10
Training loss: 2.1589587239590835
Validation loss: 2.492923512292927

Epoch: 6| Step: 11
Training loss: 2.5336503315741
Validation loss: 2.4904180929226296

Epoch: 6| Step: 12
Training loss: 2.9693121126967137
Validation loss: 2.5073011713559996

Epoch: 6| Step: 13
Training loss: 3.188319830087349
Validation loss: 2.49265290438708

Epoch: 144| Step: 0
Training loss: 2.510325850958541
Validation loss: 2.4988119501865005

Epoch: 6| Step: 1
Training loss: 2.7660978096296134
Validation loss: 2.5177100306363878

Epoch: 6| Step: 2
Training loss: 2.7807577319028707
Validation loss: 2.482182896786925

Epoch: 6| Step: 3
Training loss: 2.7109040799678126
Validation loss: 2.5070629648803466

Epoch: 6| Step: 4
Training loss: 2.90777843031072
Validation loss: 2.4778003434806144

Epoch: 6| Step: 5
Training loss: 2.191772212785639
Validation loss: 2.489803289217802

Epoch: 6| Step: 6
Training loss: 2.182342771479541
Validation loss: 2.4750225796565006

Epoch: 6| Step: 7
Training loss: 2.7521895448536973
Validation loss: 2.491660909933805

Epoch: 6| Step: 8
Training loss: 2.468500631751493
Validation loss: 2.4689467364575055

Epoch: 6| Step: 9
Training loss: 2.706935247835478
Validation loss: 2.4720207895586226

Epoch: 6| Step: 10
Training loss: 1.6917070997821246
Validation loss: 2.4813768836233883

Epoch: 6| Step: 11
Training loss: 2.429568076188588
Validation loss: 2.4852009300239186

Epoch: 6| Step: 12
Training loss: 1.5157396725405488
Validation loss: 2.4916268133727644

Epoch: 6| Step: 13
Training loss: 3.2219779115626563
Validation loss: 2.4959248297178562

Epoch: 145| Step: 0
Training loss: 3.11218579403045
Validation loss: 2.4716272266270924

Epoch: 6| Step: 1
Training loss: 2.5061326149255847
Validation loss: 2.474248561102343

Epoch: 6| Step: 2
Training loss: 2.992018572866963
Validation loss: 2.4681841418687167

Epoch: 6| Step: 3
Training loss: 2.1536768058078053
Validation loss: 2.493632861298179

Epoch: 6| Step: 4
Training loss: 2.168909037362682
Validation loss: 2.506212006624291

Epoch: 6| Step: 5
Training loss: 2.4409423387367424
Validation loss: 2.473827272493848

Epoch: 6| Step: 6
Training loss: 2.9933050474035423
Validation loss: 2.486590944456479

Epoch: 6| Step: 7
Training loss: 2.29667195570973
Validation loss: 2.5001304941062545

Epoch: 6| Step: 8
Training loss: 2.675920424357323
Validation loss: 2.4931059091575922

Epoch: 6| Step: 9
Training loss: 2.08611488302437
Validation loss: 2.498681592418516

Epoch: 6| Step: 10
Training loss: 2.2476601620170378
Validation loss: 2.496014150473366

Epoch: 6| Step: 11
Training loss: 2.049325655302127
Validation loss: 2.487078875695981

Epoch: 6| Step: 12
Training loss: 2.5605913823243815
Validation loss: 2.4887051002330036

Epoch: 6| Step: 13
Training loss: 1.9248344152004195
Validation loss: 2.473412327943474

Epoch: 146| Step: 0
Training loss: 2.987449300806649
Validation loss: 2.4553575714924927

Epoch: 6| Step: 1
Training loss: 2.4167316581493323
Validation loss: 2.4707072993248955

Epoch: 6| Step: 2
Training loss: 2.3507981182797715
Validation loss: 2.4776711131063993

Epoch: 6| Step: 3
Training loss: 2.878061406098798
Validation loss: 2.482723184073297

Epoch: 6| Step: 4
Training loss: 1.836439088521149
Validation loss: 2.491342250067922

Epoch: 6| Step: 5
Training loss: 1.7598973774251705
Validation loss: 2.5125222229774478

Epoch: 6| Step: 6
Training loss: 2.788950856429284
Validation loss: 2.4969191764802456

Epoch: 6| Step: 7
Training loss: 2.812893479691042
Validation loss: 2.511561586113424

Epoch: 6| Step: 8
Training loss: 2.593593914437141
Validation loss: 2.4751170432651124

Epoch: 6| Step: 9
Training loss: 2.8282517884217695
Validation loss: 2.491338031082953

Epoch: 6| Step: 10
Training loss: 2.043170984590308
Validation loss: 2.47107063467753

Epoch: 6| Step: 11
Training loss: 2.4691409030908718
Validation loss: 2.477466881227888

Epoch: 6| Step: 12
Training loss: 2.1768980053834137
Validation loss: 2.487050821725626

Epoch: 6| Step: 13
Training loss: 2.4006082360105143
Validation loss: 2.4710000416766666

Epoch: 147| Step: 0
Training loss: 1.918516745042138
Validation loss: 2.4828839462483896

Epoch: 6| Step: 1
Training loss: 1.8498438975484501
Validation loss: 2.4651216006769037

Epoch: 6| Step: 2
Training loss: 2.141762980906579
Validation loss: 2.4970753395325485

Epoch: 6| Step: 3
Training loss: 2.238264846075906
Validation loss: 2.499086968015205

Epoch: 6| Step: 4
Training loss: 2.334926220465454
Validation loss: 2.473336456496309

Epoch: 6| Step: 5
Training loss: 2.300391172233124
Validation loss: 2.475685931081437

Epoch: 6| Step: 6
Training loss: 2.6774400010852943
Validation loss: 2.4752236895894444

Epoch: 6| Step: 7
Training loss: 1.8113350906550747
Validation loss: 2.486994933430738

Epoch: 6| Step: 8
Training loss: 2.0716642753032817
Validation loss: 2.4701594873187367

Epoch: 6| Step: 9
Training loss: 2.818122245768616
Validation loss: 2.486103746529272

Epoch: 6| Step: 10
Training loss: 3.426978220772197
Validation loss: 2.4897381646900056

Epoch: 6| Step: 11
Training loss: 3.4006301295966184
Validation loss: 2.4846505288154206

Epoch: 6| Step: 12
Training loss: 2.8121876861100823
Validation loss: 2.4878480157759144

Epoch: 6| Step: 13
Training loss: 2.0316646446153293
Validation loss: 2.4905012054295836

Epoch: 148| Step: 0
Training loss: 1.9172166643357218
Validation loss: 2.4777748735245506

Epoch: 6| Step: 1
Training loss: 2.7715542268989797
Validation loss: 2.4693193268560205

Epoch: 6| Step: 2
Training loss: 1.6537769495929997
Validation loss: 2.4700169301184527

Epoch: 6| Step: 3
Training loss: 2.175460161661902
Validation loss: 2.4843996139238866

Epoch: 6| Step: 4
Training loss: 2.0351853029956013
Validation loss: 2.497225982061402

Epoch: 6| Step: 5
Training loss: 2.707490075710491
Validation loss: 2.5052571529325025

Epoch: 6| Step: 6
Training loss: 2.5067724049701487
Validation loss: 2.495237411133713

Epoch: 6| Step: 7
Training loss: 2.6839888058413344
Validation loss: 2.480443213391695

Epoch: 6| Step: 8
Training loss: 2.4798608230177126
Validation loss: 2.4833256171828504

Epoch: 6| Step: 9
Training loss: 2.666500732107042
Validation loss: 2.47884856661889

Epoch: 6| Step: 10
Training loss: 1.602094013127255
Validation loss: 2.477347540968198

Epoch: 6| Step: 11
Training loss: 2.7150171018732747
Validation loss: 2.4895164733742656

Epoch: 6| Step: 12
Training loss: 3.30580095151391
Validation loss: 2.499394783472406

Epoch: 6| Step: 13
Training loss: 2.6153596330977145
Validation loss: 2.478976267115091

Epoch: 149| Step: 0
Training loss: 2.2216941749184236
Validation loss: 2.4697665625118996

Epoch: 6| Step: 1
Training loss: 2.379648878819837
Validation loss: 2.473290002743106

Epoch: 6| Step: 2
Training loss: 2.5855415709574125
Validation loss: 2.502416486750597

Epoch: 6| Step: 3
Training loss: 1.7803876027274046
Validation loss: 2.489510392548289

Epoch: 6| Step: 4
Training loss: 2.05029616658851
Validation loss: 2.488659104355141

Epoch: 6| Step: 5
Training loss: 2.7658534413620397
Validation loss: 2.488333583843011

Epoch: 6| Step: 6
Training loss: 2.7607936577709995
Validation loss: 2.486910639903668

Epoch: 6| Step: 7
Training loss: 2.8283800884721493
Validation loss: 2.498178713862335

Epoch: 6| Step: 8
Training loss: 2.838407423478905
Validation loss: 2.4890378798232686

Epoch: 6| Step: 9
Training loss: 2.540673880206581
Validation loss: 2.4775663643038333

Epoch: 6| Step: 10
Training loss: 2.6835985475189106
Validation loss: 2.481371565981971

Epoch: 6| Step: 11
Training loss: 2.6113625454970166
Validation loss: 2.4745097419631987

Epoch: 6| Step: 12
Training loss: 2.6285133464983064
Validation loss: 2.4900644724482386

Epoch: 6| Step: 13
Training loss: 1.4435703809894394
Validation loss: 2.4878717550031464

Epoch: 150| Step: 0
Training loss: 1.9935840573506134
Validation loss: 2.520455044138101

Epoch: 6| Step: 1
Training loss: 2.659904031097904
Validation loss: 2.500589942031378

Epoch: 6| Step: 2
Training loss: 3.254811027186198
Validation loss: 2.4728548092383194

Epoch: 6| Step: 3
Training loss: 1.9814444100578588
Validation loss: 2.4769088763236184

Epoch: 6| Step: 4
Training loss: 2.425735645203359
Validation loss: 2.4918112079686994

Epoch: 6| Step: 5
Training loss: 2.475202216932886
Validation loss: 2.4770461161776756

Epoch: 6| Step: 6
Training loss: 2.3294188725152827
Validation loss: 2.48506956271287

Epoch: 6| Step: 7
Training loss: 3.253650522429681
Validation loss: 2.4726461697282742

Epoch: 6| Step: 8
Training loss: 2.5201977225346717
Validation loss: 2.4745698997830594

Epoch: 6| Step: 9
Training loss: 2.8421336180962347
Validation loss: 2.4848034721976697

Epoch: 6| Step: 10
Training loss: 2.089482869870597
Validation loss: 2.509747673689645

Epoch: 6| Step: 11
Training loss: 2.141927503838928
Validation loss: 2.489422077303111

Epoch: 6| Step: 12
Training loss: 1.8077399790732342
Validation loss: 2.5064084015845522

Epoch: 6| Step: 13
Training loss: 1.7672945754095362
Validation loss: 2.480923859671855

Epoch: 151| Step: 0
Training loss: 2.382432851095289
Validation loss: 2.478891216916979

Epoch: 6| Step: 1
Training loss: 2.19965440896755
Validation loss: 2.4744055628175774

Epoch: 6| Step: 2
Training loss: 3.2001377970113785
Validation loss: 2.481815939741926

Epoch: 6| Step: 3
Training loss: 2.1341347420938797
Validation loss: 2.5146293211068067

Epoch: 6| Step: 4
Training loss: 2.7025542786799726
Validation loss: 2.4932513475513765

Epoch: 6| Step: 5
Training loss: 2.7735998831479094
Validation loss: 2.4946767656507696

Epoch: 6| Step: 6
Training loss: 2.645870438763611
Validation loss: 2.4929113878148015

Epoch: 6| Step: 7
Training loss: 2.998127670938981
Validation loss: 2.458516650267666

Epoch: 6| Step: 8
Training loss: 1.5881783575457094
Validation loss: 2.492931103687634

Epoch: 6| Step: 9
Training loss: 2.76631492155887
Validation loss: 2.4836916606434554

Epoch: 6| Step: 10
Training loss: 1.6550500229481564
Validation loss: 2.4803092969241716

Epoch: 6| Step: 11
Training loss: 2.315201701858412
Validation loss: 2.4829929826621178

Epoch: 6| Step: 12
Training loss: 2.672633013363618
Validation loss: 2.4791830787380866

Epoch: 6| Step: 13
Training loss: 1.5791477628329629
Validation loss: 2.498015979311302

Epoch: 152| Step: 0
Training loss: 2.7142270053164603
Validation loss: 2.4918698951244647

Epoch: 6| Step: 1
Training loss: 2.4778162914280313
Validation loss: 2.4975425650334353

Epoch: 6| Step: 2
Training loss: 2.723684808967189
Validation loss: 2.4894703215417366

Epoch: 6| Step: 3
Training loss: 2.420497477920375
Validation loss: 2.497781043608822

Epoch: 6| Step: 4
Training loss: 2.12770088439771
Validation loss: 2.48743477111947

Epoch: 6| Step: 5
Training loss: 2.6265729097231536
Validation loss: 2.4851839473488044

Epoch: 6| Step: 6
Training loss: 2.436734984240767
Validation loss: 2.4695687318162456

Epoch: 6| Step: 7
Training loss: 1.801286936691625
Validation loss: 2.4881219596384785

Epoch: 6| Step: 8
Training loss: 2.2871327256493728
Validation loss: 2.4988298805420177

Epoch: 6| Step: 9
Training loss: 2.8962474682384847
Validation loss: 2.4997447570612756

Epoch: 6| Step: 10
Training loss: 2.1052590232105812
Validation loss: 2.492456091287522

Epoch: 6| Step: 11
Training loss: 2.842902088563067
Validation loss: 2.498477750497315

Epoch: 6| Step: 12
Training loss: 2.3620514499380705
Validation loss: 2.5109638558460663

Epoch: 6| Step: 13
Training loss: 2.594128224935307
Validation loss: 2.503868686983691

Epoch: 153| Step: 0
Training loss: 1.8656176747445046
Validation loss: 2.4910391400053933

Epoch: 6| Step: 1
Training loss: 2.7804081532112517
Validation loss: 2.4764169251865153

Epoch: 6| Step: 2
Training loss: 2.4475085844955067
Validation loss: 2.477462903518806

Epoch: 6| Step: 3
Training loss: 2.7463423940598077
Validation loss: 2.490419529966998

Epoch: 6| Step: 4
Training loss: 2.3026003940162796
Validation loss: 2.4950157563441158

Epoch: 6| Step: 5
Training loss: 2.5476855526958264
Validation loss: 2.4949040681838586

Epoch: 6| Step: 6
Training loss: 2.4200142880482978
Validation loss: 2.505390506411067

Epoch: 6| Step: 7
Training loss: 2.1939498636434034
Validation loss: 2.4793820934294017

Epoch: 6| Step: 8
Training loss: 2.6958585766451097
Validation loss: 2.4951416738784915

Epoch: 6| Step: 9
Training loss: 2.3640910124839984
Validation loss: 2.4795399373884814

Epoch: 6| Step: 10
Training loss: 1.928222470779777
Validation loss: 2.488391441769141

Epoch: 6| Step: 11
Training loss: 2.634653144171732
Validation loss: 2.479652512979524

Epoch: 6| Step: 12
Training loss: 3.0969428338898726
Validation loss: 2.4807280400019516

Epoch: 6| Step: 13
Training loss: 1.9567962322629275
Validation loss: 2.471369073578783

Epoch: 154| Step: 0
Training loss: 2.645348381795047
Validation loss: 2.4677646529886395

Epoch: 6| Step: 1
Training loss: 2.0289353313866303
Validation loss: 2.4807376136112462

Epoch: 6| Step: 2
Training loss: 2.218948247944707
Validation loss: 2.496190425383268

Epoch: 6| Step: 3
Training loss: 2.276145973405163
Validation loss: 2.4757582000021947

Epoch: 6| Step: 4
Training loss: 2.0539602871095024
Validation loss: 2.4704022126160172

Epoch: 6| Step: 5
Training loss: 2.3989656842199603
Validation loss: 2.488168917473078

Epoch: 6| Step: 6
Training loss: 2.1327397260721384
Validation loss: 2.4622476819841514

Epoch: 6| Step: 7
Training loss: 2.8957484024729783
Validation loss: 2.4980049786913274

Epoch: 6| Step: 8
Training loss: 2.1522456041172195
Validation loss: 2.4879673773564277

Epoch: 6| Step: 9
Training loss: 2.4666116845816344
Validation loss: 2.496329580831177

Epoch: 6| Step: 10
Training loss: 3.023852573656168
Validation loss: 2.477035913015493

Epoch: 6| Step: 11
Training loss: 2.4865764724628443
Validation loss: 2.480496038161311

Epoch: 6| Step: 12
Training loss: 2.7700495268597822
Validation loss: 2.476673684135148

Epoch: 6| Step: 13
Training loss: 3.248289832079477
Validation loss: 2.476555397750967

Epoch: 155| Step: 0
Training loss: 1.9729953587705118
Validation loss: 2.4984590662613706

Epoch: 6| Step: 1
Training loss: 2.6085153408553654
Validation loss: 2.4763840090573477

Epoch: 6| Step: 2
Training loss: 2.54753085625916
Validation loss: 2.4873341588817808

Epoch: 6| Step: 3
Training loss: 2.035096502616861
Validation loss: 2.4794028185117796

Epoch: 6| Step: 4
Training loss: 3.274007888663266
Validation loss: 2.4992477649133753

Epoch: 6| Step: 5
Training loss: 2.6524081102970065
Validation loss: 2.476024575087994

Epoch: 6| Step: 6
Training loss: 2.8159971746950716
Validation loss: 2.4993939136747847

Epoch: 6| Step: 7
Training loss: 2.403555017300708
Validation loss: 2.4796241496023703

Epoch: 6| Step: 8
Training loss: 2.739708456414197
Validation loss: 2.4955426709209303

Epoch: 6| Step: 9
Training loss: 2.1724009803181157
Validation loss: 2.496906014389115

Epoch: 6| Step: 10
Training loss: 2.3003554152496752
Validation loss: 2.4849865847412183

Epoch: 6| Step: 11
Training loss: 2.579507815262486
Validation loss: 2.487080729044578

Epoch: 6| Step: 12
Training loss: 1.811258482771676
Validation loss: 2.4846837218336013

Epoch: 6| Step: 13
Training loss: 1.7916732462680862
Validation loss: 2.486674875616279

Epoch: 156| Step: 0
Training loss: 2.2293282447320335
Validation loss: 2.473706562764967

Epoch: 6| Step: 1
Training loss: 2.097792182368233
Validation loss: 2.5031579427740676

Epoch: 6| Step: 2
Training loss: 2.2045874104533913
Validation loss: 2.477174931043196

Epoch: 6| Step: 3
Training loss: 2.232546932158599
Validation loss: 2.510379867750351

Epoch: 6| Step: 4
Training loss: 2.03965803197255
Validation loss: 2.473238904368961

Epoch: 6| Step: 5
Training loss: 2.8557447725368563
Validation loss: 2.488269109083474

Epoch: 6| Step: 6
Training loss: 2.273738041574196
Validation loss: 2.4834179501743208

Epoch: 6| Step: 7
Training loss: 1.88260101776825
Validation loss: 2.4872966975765194

Epoch: 6| Step: 8
Training loss: 2.576011114765854
Validation loss: 2.4672859745042905

Epoch: 6| Step: 9
Training loss: 3.032536341865318
Validation loss: 2.487324928112343

Epoch: 6| Step: 10
Training loss: 2.378932307559464
Validation loss: 2.4664792528712303

Epoch: 6| Step: 11
Training loss: 2.6237576587523423
Validation loss: 2.51133735517013

Epoch: 6| Step: 12
Training loss: 2.8975581971479665
Validation loss: 2.4937165348869925

Epoch: 6| Step: 13
Training loss: 2.6563653359898165
Validation loss: 2.4927717772447147

Epoch: 157| Step: 0
Training loss: 2.73427106387286
Validation loss: 2.4831937970671834

Epoch: 6| Step: 1
Training loss: 3.192466008924707
Validation loss: 2.490709305992036

Epoch: 6| Step: 2
Training loss: 2.9907803326708926
Validation loss: 2.510539797370415

Epoch: 6| Step: 3
Training loss: 2.21591176919842
Validation loss: 2.4756096871309143

Epoch: 6| Step: 4
Training loss: 2.450846981082788
Validation loss: 2.490965425370351

Epoch: 6| Step: 5
Training loss: 2.504265769821254
Validation loss: 2.4951953310036186

Epoch: 6| Step: 6
Training loss: 1.9132533017255506
Validation loss: 2.4809614451036444

Epoch: 6| Step: 7
Training loss: 2.143491955232395
Validation loss: 2.503608685292047

Epoch: 6| Step: 8
Training loss: 2.19359350271174
Validation loss: 2.4644633292220552

Epoch: 6| Step: 9
Training loss: 2.310182595900984
Validation loss: 2.4820482478840082

Epoch: 6| Step: 10
Training loss: 2.2046026590678185
Validation loss: 2.487226106435716

Epoch: 6| Step: 11
Training loss: 1.7471132310486068
Validation loss: 2.4800853052727385

Epoch: 6| Step: 12
Training loss: 2.814747908096356
Validation loss: 2.492905246368776

Epoch: 6| Step: 13
Training loss: 2.2474555357204027
Validation loss: 2.473370871534182

Epoch: 158| Step: 0
Training loss: 2.562329728586953
Validation loss: 2.504106457130289

Epoch: 6| Step: 1
Training loss: 2.0804915183035844
Validation loss: 2.471231477473097

Epoch: 6| Step: 2
Training loss: 2.416035756156025
Validation loss: 2.496684481040275

Epoch: 6| Step: 3
Training loss: 2.9970475609575504
Validation loss: 2.491503947058279

Epoch: 6| Step: 4
Training loss: 1.9143696441226754
Validation loss: 2.489906973407596

Epoch: 6| Step: 5
Training loss: 2.7198934671632427
Validation loss: 2.516337711643129

Epoch: 6| Step: 6
Training loss: 2.567244069270792
Validation loss: 2.48284238675491

Epoch: 6| Step: 7
Training loss: 2.580697458164159
Validation loss: 2.4841864899967674

Epoch: 6| Step: 8
Training loss: 1.9532504842501979
Validation loss: 2.4875211405514053

Epoch: 6| Step: 9
Training loss: 2.0047113240254273
Validation loss: 2.4844243617489905

Epoch: 6| Step: 10
Training loss: 2.3426478528371146
Validation loss: 2.5040348870951847

Epoch: 6| Step: 11
Training loss: 3.0600208446316306
Validation loss: 2.477536201406685

Epoch: 6| Step: 12
Training loss: 2.184259466736904
Validation loss: 2.4777258035198413

Epoch: 6| Step: 13
Training loss: 2.4120551099348946
Validation loss: 2.469546468785384

Epoch: 159| Step: 0
Training loss: 2.8558117286110547
Validation loss: 2.4610500035871348

Epoch: 6| Step: 1
Training loss: 2.6276581339339686
Validation loss: 2.483715799311517

Epoch: 6| Step: 2
Training loss: 1.845086470027533
Validation loss: 2.50071368131392

Epoch: 6| Step: 3
Training loss: 2.956983840542173
Validation loss: 2.4965615571146555

Epoch: 6| Step: 4
Training loss: 3.7132753737724387
Validation loss: 2.495559131099677

Epoch: 6| Step: 5
Training loss: 2.5097985885177194
Validation loss: 2.482641281896275

Epoch: 6| Step: 6
Training loss: 1.5343473598997388
Validation loss: 2.4498512260146863

Epoch: 6| Step: 7
Training loss: 2.1293631247146485
Validation loss: 2.508299978419631

Epoch: 6| Step: 8
Training loss: 2.4030661388475725
Validation loss: 2.4913004776858174

Epoch: 6| Step: 9
Training loss: 1.9056065208112096
Validation loss: 2.490931015678371

Epoch: 6| Step: 10
Training loss: 2.1689497094451156
Validation loss: 2.459103003043062

Epoch: 6| Step: 11
Training loss: 2.403123682465707
Validation loss: 2.4744669187578

Epoch: 6| Step: 12
Training loss: 2.135930168624831
Validation loss: 2.492515977188346

Epoch: 6| Step: 13
Training loss: 2.034761416159378
Validation loss: 2.4975780332615893

Epoch: 160| Step: 0
Training loss: 2.7908985922257017
Validation loss: 2.4837238709424176

Epoch: 6| Step: 1
Training loss: 2.193344483023521
Validation loss: 2.481517278872255

Epoch: 6| Step: 2
Training loss: 2.0734178969053776
Validation loss: 2.4816939450894338

Epoch: 6| Step: 3
Training loss: 2.6239309404636137
Validation loss: 2.4728787146869764

Epoch: 6| Step: 4
Training loss: 2.1352797549637397
Validation loss: 2.4725938900686333

Epoch: 6| Step: 5
Training loss: 2.304411687349394
Validation loss: 2.5052342287627387

Epoch: 6| Step: 6
Training loss: 1.8790604177184616
Validation loss: 2.4996515513179323

Epoch: 6| Step: 7
Training loss: 2.639642837524119
Validation loss: 2.470715311767686

Epoch: 6| Step: 8
Training loss: 3.05399402444451
Validation loss: 2.498071940200087

Epoch: 6| Step: 9
Training loss: 2.3374874624640265
Validation loss: 2.4685434120255847

Epoch: 6| Step: 10
Training loss: 2.353268027043348
Validation loss: 2.459309227987721

Epoch: 6| Step: 11
Training loss: 2.6461139039589
Validation loss: 2.4886428581330846

Epoch: 6| Step: 12
Training loss: 2.3424086737972445
Validation loss: 2.473855402844809

Epoch: 6| Step: 13
Training loss: 2.6221797870535735
Validation loss: 2.4570633475818857

Epoch: 161| Step: 0
Training loss: 2.074242428287049
Validation loss: 2.49947152653537

Epoch: 6| Step: 1
Training loss: 2.5018094190530773
Validation loss: 2.498271530224701

Epoch: 6| Step: 2
Training loss: 2.509251072009896
Validation loss: 2.4818951372971885

Epoch: 6| Step: 3
Training loss: 2.4160907212523153
Validation loss: 2.5040077817407327

Epoch: 6| Step: 4
Training loss: 2.6928264264909476
Validation loss: 2.473510583191421

Epoch: 6| Step: 5
Training loss: 2.2723052361248928
Validation loss: 2.4851762621421276

Epoch: 6| Step: 6
Training loss: 2.6407402961434934
Validation loss: 2.4864428770449947

Epoch: 6| Step: 7
Training loss: 1.7172714809885197
Validation loss: 2.481379993410349

Epoch: 6| Step: 8
Training loss: 2.3704689371567698
Validation loss: 2.4782001358536747

Epoch: 6| Step: 9
Training loss: 3.29713735395342
Validation loss: 2.4786800982653716

Epoch: 6| Step: 10
Training loss: 1.9854010384882876
Validation loss: 2.470575200260164

Epoch: 6| Step: 11
Training loss: 2.735812871807858
Validation loss: 2.462255276332232

Epoch: 6| Step: 12
Training loss: 2.480230461148999
Validation loss: 2.481416635912233

Epoch: 6| Step: 13
Training loss: 1.7828295212918552
Validation loss: 2.4697957719205874

Epoch: 162| Step: 0
Training loss: 2.176560103284361
Validation loss: 2.4901340946513053

Epoch: 6| Step: 1
Training loss: 2.648716374571344
Validation loss: 2.506962742077462

Epoch: 6| Step: 2
Training loss: 2.2160069877500566
Validation loss: 2.4771024721472377

Epoch: 6| Step: 3
Training loss: 2.7808992829093597
Validation loss: 2.497085966953014

Epoch: 6| Step: 4
Training loss: 2.2496401181101704
Validation loss: 2.4931292790382833

Epoch: 6| Step: 5
Training loss: 2.4235369486667073
Validation loss: 2.4565146886765286

Epoch: 6| Step: 6
Training loss: 1.9704076055795803
Validation loss: 2.4850684222597352

Epoch: 6| Step: 7
Training loss: 2.7364769404349505
Validation loss: 2.4861539999429527

Epoch: 6| Step: 8
Training loss: 2.586982907581229
Validation loss: 2.4862955514627347

Epoch: 6| Step: 9
Training loss: 2.5109881203088222
Validation loss: 2.4881784449369904

Epoch: 6| Step: 10
Training loss: 2.2339374674051125
Validation loss: 2.468658931574319

Epoch: 6| Step: 11
Training loss: 2.153904288550125
Validation loss: 2.4784318302479282

Epoch: 6| Step: 12
Training loss: 1.9852490039760973
Validation loss: 2.4787424640965936

Epoch: 6| Step: 13
Training loss: 3.4256359317454086
Validation loss: 2.482141177860186

Epoch: 163| Step: 0
Training loss: 2.7166192485516927
Validation loss: 2.481654664175342

Epoch: 6| Step: 1
Training loss: 2.413883140839041
Validation loss: 2.4801243712303718

Epoch: 6| Step: 2
Training loss: 1.6405931015546709
Validation loss: 2.4610405586046227

Epoch: 6| Step: 3
Training loss: 2.8538057047970913
Validation loss: 2.4759067412408564

Epoch: 6| Step: 4
Training loss: 1.6343594756401127
Validation loss: 2.5015715140990764

Epoch: 6| Step: 5
Training loss: 2.315281097779935
Validation loss: 2.479982341358803

Epoch: 6| Step: 6
Training loss: 2.9011061893504015
Validation loss: 2.4673844695575258

Epoch: 6| Step: 7
Training loss: 1.9609373784160196
Validation loss: 2.497991092122584

Epoch: 6| Step: 8
Training loss: 2.014039354805431
Validation loss: 2.505130734812132

Epoch: 6| Step: 9
Training loss: 2.778326405282203
Validation loss: 2.473352900689888

Epoch: 6| Step: 10
Training loss: 1.9036147866976747
Validation loss: 2.48234864082017

Epoch: 6| Step: 11
Training loss: 2.791449713935777
Validation loss: 2.4729731192015767

Epoch: 6| Step: 12
Training loss: 2.259214817135772
Validation loss: 2.488423461425287

Epoch: 6| Step: 13
Training loss: 3.50632994317565
Validation loss: 2.4965970464676035

Epoch: 164| Step: 0
Training loss: 2.108209358560939
Validation loss: 2.4880984845102865

Epoch: 6| Step: 1
Training loss: 2.880627638984675
Validation loss: 2.4833970831070116

Epoch: 6| Step: 2
Training loss: 1.7601285469187944
Validation loss: 2.50023495175106

Epoch: 6| Step: 3
Training loss: 2.3634349930621457
Validation loss: 2.475509307297297

Epoch: 6| Step: 4
Training loss: 1.9406256535969737
Validation loss: 2.4779573425916115

Epoch: 6| Step: 5
Training loss: 2.158502590754909
Validation loss: 2.473330682604612

Epoch: 6| Step: 6
Training loss: 2.3245781628617923
Validation loss: 2.4555849890701245

Epoch: 6| Step: 7
Training loss: 2.854501709219066
Validation loss: 2.4721595913232934

Epoch: 6| Step: 8
Training loss: 2.564949911420436
Validation loss: 2.4777016851311253

Epoch: 6| Step: 9
Training loss: 2.4186277215689875
Validation loss: 2.495473554038042

Epoch: 6| Step: 10
Training loss: 2.381113115516413
Validation loss: 2.476589380912043

Epoch: 6| Step: 11
Training loss: 2.1898961703626485
Validation loss: 2.4634961961725725

Epoch: 6| Step: 12
Training loss: 2.87668792268835
Validation loss: 2.479446492198967

Epoch: 6| Step: 13
Training loss: 2.788683953762012
Validation loss: 2.484333987602747

Epoch: 165| Step: 0
Training loss: 2.7051981654886186
Validation loss: 2.4755764154225712

Epoch: 6| Step: 1
Training loss: 2.1567015244243004
Validation loss: 2.4755063672234847

Epoch: 6| Step: 2
Training loss: 2.441612295992726
Validation loss: 2.484316886520909

Epoch: 6| Step: 3
Training loss: 2.3283013398637697
Validation loss: 2.474471508399875

Epoch: 6| Step: 4
Training loss: 2.355105547635377
Validation loss: 2.48445923922266

Epoch: 6| Step: 5
Training loss: 2.354030593945509
Validation loss: 2.472610908412304

Epoch: 6| Step: 6
Training loss: 2.539405869961279
Validation loss: 2.476860843346906

Epoch: 6| Step: 7
Training loss: 2.6987246821648427
Validation loss: 2.4969657779081387

Epoch: 6| Step: 8
Training loss: 2.5819725226159833
Validation loss: 2.4958035904462847

Epoch: 6| Step: 9
Training loss: 1.8428887764274067
Validation loss: 2.474042784583072

Epoch: 6| Step: 10
Training loss: 2.564754610723279
Validation loss: 2.4801204463662363

Epoch: 6| Step: 11
Training loss: 2.6175223520630735
Validation loss: 2.470089713107238

Epoch: 6| Step: 12
Training loss: 2.1050166562977632
Validation loss: 2.472674558716538

Epoch: 6| Step: 13
Training loss: 2.688987143046383
Validation loss: 2.4736759838558657

Epoch: 166| Step: 0
Training loss: 2.7048668513783265
Validation loss: 2.484880643201477

Epoch: 6| Step: 1
Training loss: 2.3573508769471414
Validation loss: 2.471497920863007

Epoch: 6| Step: 2
Training loss: 2.4817066377794403
Validation loss: 2.45969514858904

Epoch: 6| Step: 3
Training loss: 1.9401864531473991
Validation loss: 2.4853685244314194

Epoch: 6| Step: 4
Training loss: 2.161839744895788
Validation loss: 2.46783361824939

Epoch: 6| Step: 5
Training loss: 3.023150447653969
Validation loss: 2.507444726658273

Epoch: 6| Step: 6
Training loss: 1.8869451856874546
Validation loss: 2.5036689354424952

Epoch: 6| Step: 7
Training loss: 2.066913391070493
Validation loss: 2.4885863411391695

Epoch: 6| Step: 8
Training loss: 2.2388440235187237
Validation loss: 2.5247057998079896

Epoch: 6| Step: 9
Training loss: 2.6617967025946223
Validation loss: 2.4862099659166614

Epoch: 6| Step: 10
Training loss: 2.019505867715553
Validation loss: 2.4784246919803805

Epoch: 6| Step: 11
Training loss: 2.0164318983182694
Validation loss: 2.4821859353326112

Epoch: 6| Step: 12
Training loss: 3.3209393077934086
Validation loss: 2.4946250011232634

Epoch: 6| Step: 13
Training loss: 2.585938329782958
Validation loss: 2.4871640180119705

Epoch: 167| Step: 0
Training loss: 2.4592073686368963
Validation loss: 2.4861647756175826

Epoch: 6| Step: 1
Training loss: 2.2151294234721215
Validation loss: 2.481153458477486

Epoch: 6| Step: 2
Training loss: 2.350625393215026
Validation loss: 2.4681513725003565

Epoch: 6| Step: 3
Training loss: 1.9043853895694578
Validation loss: 2.4886746510178215

Epoch: 6| Step: 4
Training loss: 2.1643535542231134
Validation loss: 2.481506310488915

Epoch: 6| Step: 5
Training loss: 3.1017900282569952
Validation loss: 2.481629404230533

Epoch: 6| Step: 6
Training loss: 2.2681933347993772
Validation loss: 2.4727321526857517

Epoch: 6| Step: 7
Training loss: 2.833382624777083
Validation loss: 2.4818675732276114

Epoch: 6| Step: 8
Training loss: 2.249072943543005
Validation loss: 2.465775884899989

Epoch: 6| Step: 9
Training loss: 1.6044038456878567
Validation loss: 2.4831325669394526

Epoch: 6| Step: 10
Training loss: 2.383520302630237
Validation loss: 2.469991453593657

Epoch: 6| Step: 11
Training loss: 2.508555649783174
Validation loss: 2.4586180262610524

Epoch: 6| Step: 12
Training loss: 2.2855568559109534
Validation loss: 2.4868428335568753

Epoch: 6| Step: 13
Training loss: 3.005809721558295
Validation loss: 2.457622532380487

Epoch: 168| Step: 0
Training loss: 3.045254790070971
Validation loss: 2.4838021375651897

Epoch: 6| Step: 1
Training loss: 2.3249768984067347
Validation loss: 2.4749992646131007

Epoch: 6| Step: 2
Training loss: 2.1652232888819003
Validation loss: 2.4643216336546523

Epoch: 6| Step: 3
Training loss: 2.1629573051834914
Validation loss: 2.4392686983994083

Epoch: 6| Step: 4
Training loss: 2.2387001484443547
Validation loss: 2.4582761271560747

Epoch: 6| Step: 5
Training loss: 2.1009791816141177
Validation loss: 2.4726289592994837

Epoch: 6| Step: 6
Training loss: 2.856863784784716
Validation loss: 2.4730112629359255

Epoch: 6| Step: 7
Training loss: 2.4477635978295322
Validation loss: 2.464861081872397

Epoch: 6| Step: 8
Training loss: 2.3787610239885018
Validation loss: 2.478418455676777

Epoch: 6| Step: 9
Training loss: 2.4002139194837295
Validation loss: 2.4612538780377213

Epoch: 6| Step: 10
Training loss: 2.342936158343102
Validation loss: 2.491690821593495

Epoch: 6| Step: 11
Training loss: 2.230493976296708
Validation loss: 2.4876590393264255

Epoch: 6| Step: 12
Training loss: 1.935840511278608
Validation loss: 2.484937303282477

Epoch: 6| Step: 13
Training loss: 2.907343176878941
Validation loss: 2.4958935205281505

Epoch: 169| Step: 0
Training loss: 2.3153083862811847
Validation loss: 2.4789323048831

Epoch: 6| Step: 1
Training loss: 2.088772222741995
Validation loss: 2.4696018374004165

Epoch: 6| Step: 2
Training loss: 2.9708983026777465
Validation loss: 2.4775899772500973

Epoch: 6| Step: 3
Training loss: 2.3652611879802454
Validation loss: 2.5009281916971746

Epoch: 6| Step: 4
Training loss: 1.899091192031561
Validation loss: 2.4778322258226075

Epoch: 6| Step: 5
Training loss: 1.511712678014897
Validation loss: 2.466681510406149

Epoch: 6| Step: 6
Training loss: 1.8304067864148839
Validation loss: 2.4692909818309

Epoch: 6| Step: 7
Training loss: 3.331071435891272
Validation loss: 2.4560627988904273

Epoch: 6| Step: 8
Training loss: 2.1395040208279954
Validation loss: 2.4628092580203

Epoch: 6| Step: 9
Training loss: 2.3075709005355685
Validation loss: 2.4557036059436275

Epoch: 6| Step: 10
Training loss: 2.8643355568740625
Validation loss: 2.4877419193997383

Epoch: 6| Step: 11
Training loss: 2.638969398965812
Validation loss: 2.4820554025850106

Epoch: 6| Step: 12
Training loss: 2.1241866406409167
Validation loss: 2.4965849593400424

Epoch: 6| Step: 13
Training loss: 2.466763240195279
Validation loss: 2.5013935645292715

Epoch: 170| Step: 0
Training loss: 2.410007070317324
Validation loss: 2.4913095887637

Epoch: 6| Step: 1
Training loss: 2.6384635972979344
Validation loss: 2.480886097999988

Epoch: 6| Step: 2
Training loss: 1.733886925007908
Validation loss: 2.4628279938570814

Epoch: 6| Step: 3
Training loss: 1.8483053926030626
Validation loss: 2.477097334723391

Epoch: 6| Step: 4
Training loss: 2.616744001019228
Validation loss: 2.478773026010492

Epoch: 6| Step: 5
Training loss: 2.3781189766806317
Validation loss: 2.4809692094990283

Epoch: 6| Step: 6
Training loss: 2.033156336704987
Validation loss: 2.4882650569466724

Epoch: 6| Step: 7
Training loss: 2.495544277598592
Validation loss: 2.4995546056990405

Epoch: 6| Step: 8
Training loss: 1.7323035164261662
Validation loss: 2.4804759164916104

Epoch: 6| Step: 9
Training loss: 3.083015855132842
Validation loss: 2.49489056304733

Epoch: 6| Step: 10
Training loss: 2.567948944600335
Validation loss: 2.463817595985243

Epoch: 6| Step: 11
Training loss: 2.4300103172428584
Validation loss: 2.4762186593465088

Epoch: 6| Step: 12
Training loss: 3.0827824555840833
Validation loss: 2.4596691404291073

Epoch: 6| Step: 13
Training loss: 1.57097607916013
Validation loss: 2.4928085968144122

Epoch: 171| Step: 0
Training loss: 2.535810906240738
Validation loss: 2.4739561140291184

Epoch: 6| Step: 1
Training loss: 1.2757623431643792
Validation loss: 2.48053013683062

Epoch: 6| Step: 2
Training loss: 2.8352431611157813
Validation loss: 2.4827052158711176

Epoch: 6| Step: 3
Training loss: 2.5398279532789383
Validation loss: 2.4909562492245185

Epoch: 6| Step: 4
Training loss: 2.3292082244313814
Validation loss: 2.4761696553053594

Epoch: 6| Step: 5
Training loss: 3.16421711861795
Validation loss: 2.4610591537374558

Epoch: 6| Step: 6
Training loss: 2.1675192427083516
Validation loss: 2.4815029105591724

Epoch: 6| Step: 7
Training loss: 2.598424903898739
Validation loss: 2.470680725885878

Epoch: 6| Step: 8
Training loss: 1.9823087011645402
Validation loss: 2.4411760738689026

Epoch: 6| Step: 9
Training loss: 2.97191924534542
Validation loss: 2.457219501557866

Epoch: 6| Step: 10
Training loss: 2.0003361419486807
Validation loss: 2.460385753882693

Epoch: 6| Step: 11
Training loss: 1.840604199743697
Validation loss: 2.4793844943383574

Epoch: 6| Step: 12
Training loss: 1.2272230179991666
Validation loss: 2.489610946185666

Epoch: 6| Step: 13
Training loss: 3.509724323282308
Validation loss: 2.4635714195473795

Epoch: 172| Step: 0
Training loss: 2.620458579953228
Validation loss: 2.469660916856905

Epoch: 6| Step: 1
Training loss: 1.9739454834317662
Validation loss: 2.4761685689902433

Epoch: 6| Step: 2
Training loss: 2.5341445008997474
Validation loss: 2.459399689881419

Epoch: 6| Step: 3
Training loss: 1.9900955286531556
Validation loss: 2.4590651825572776

Epoch: 6| Step: 4
Training loss: 2.361454533553015
Validation loss: 2.4848777296911564

Epoch: 6| Step: 5
Training loss: 1.868253778515842
Validation loss: 2.4693612551358464

Epoch: 6| Step: 6
Training loss: 2.2823811692122566
Validation loss: 2.4632479081840186

Epoch: 6| Step: 7
Training loss: 2.3575076361782314
Validation loss: 2.4620994573593387

Epoch: 6| Step: 8
Training loss: 2.5023527518594912
Validation loss: 2.476621638035423

Epoch: 6| Step: 9
Training loss: 2.5790668038882703
Validation loss: 2.4808196439978523

Epoch: 6| Step: 10
Training loss: 2.3941413101391986
Validation loss: 2.4298570178981596

Epoch: 6| Step: 11
Training loss: 3.0144296757664795
Validation loss: 2.480311261791587

Epoch: 6| Step: 12
Training loss: 2.376130788542894
Validation loss: 2.4590148582453146

Epoch: 6| Step: 13
Training loss: 1.5580207708440834
Validation loss: 2.4770214405674493

Epoch: 173| Step: 0
Training loss: 2.4738727969725094
Validation loss: 2.4807355653745407

Epoch: 6| Step: 1
Training loss: 2.6047383304022964
Validation loss: 2.4473588790170493

Epoch: 6| Step: 2
Training loss: 2.28335683026255
Validation loss: 2.4615559160484946

Epoch: 6| Step: 3
Training loss: 2.4605241700884295
Validation loss: 2.497538575159849

Epoch: 6| Step: 4
Training loss: 2.17502031426425
Validation loss: 2.4742431639006046

Epoch: 6| Step: 5
Training loss: 1.7620483902035566
Validation loss: 2.480722142251669

Epoch: 6| Step: 6
Training loss: 2.4891921072512178
Validation loss: 2.455316807373364

Epoch: 6| Step: 7
Training loss: 2.8159075968819085
Validation loss: 2.4809753660252345

Epoch: 6| Step: 8
Training loss: 2.573259884159974
Validation loss: 2.4871320378027253

Epoch: 6| Step: 9
Training loss: 2.1329634057619984
Validation loss: 2.4889383907285825

Epoch: 6| Step: 10
Training loss: 1.414638844422896
Validation loss: 2.465637164620915

Epoch: 6| Step: 11
Training loss: 2.605879400968621
Validation loss: 2.4915156709219763

Epoch: 6| Step: 12
Training loss: 2.2577153485261827
Validation loss: 2.4480566563807624

Epoch: 6| Step: 13
Training loss: 3.1517492251288943
Validation loss: 2.4888827940882834

Epoch: 174| Step: 0
Training loss: 1.6917011100883108
Validation loss: 2.466709881259589

Epoch: 6| Step: 1
Training loss: 2.4860409123498344
Validation loss: 2.4673618543313682

Epoch: 6| Step: 2
Training loss: 2.047241876934267
Validation loss: 2.477444689172071

Epoch: 6| Step: 3
Training loss: 2.3844450814083435
Validation loss: 2.4932372144634947

Epoch: 6| Step: 4
Training loss: 2.2852428452476055
Validation loss: 2.467171766959668

Epoch: 6| Step: 5
Training loss: 2.5206954750218062
Validation loss: 2.487349731347174

Epoch: 6| Step: 6
Training loss: 2.3845286708523132
Validation loss: 2.459781835432007

Epoch: 6| Step: 7
Training loss: 2.0233041137562995
Validation loss: 2.4732584277480827

Epoch: 6| Step: 8
Training loss: 2.460332499417595
Validation loss: 2.488657767246964

Epoch: 6| Step: 9
Training loss: 2.316964762420143
Validation loss: 2.4788519970785874

Epoch: 6| Step: 10
Training loss: 3.1934025286170598
Validation loss: 2.4781946313963172

Epoch: 6| Step: 11
Training loss: 2.328243380615498
Validation loss: 2.468982817955818

Epoch: 6| Step: 12
Training loss: 1.6935887868811845
Validation loss: 2.4929521701496244

Epoch: 6| Step: 13
Training loss: 3.5321510988189186
Validation loss: 2.4678796397265295

Epoch: 175| Step: 0
Training loss: 1.9268164028843522
Validation loss: 2.4855150684704586

Epoch: 6| Step: 1
Training loss: 2.9545838320180327
Validation loss: 2.460954607285409

Epoch: 6| Step: 2
Training loss: 2.2651801363152724
Validation loss: 2.4663280134721433

Epoch: 6| Step: 3
Training loss: 2.0370061451403028
Validation loss: 2.471749680271469

Epoch: 6| Step: 4
Training loss: 3.0753159461492956
Validation loss: 2.479794097560467

Epoch: 6| Step: 5
Training loss: 2.173852473120045
Validation loss: 2.453360640358576

Epoch: 6| Step: 6
Training loss: 2.793027991887005
Validation loss: 2.4850280671098526

Epoch: 6| Step: 7
Training loss: 2.005557920689948
Validation loss: 2.4576031414027635

Epoch: 6| Step: 8
Training loss: 2.716861814577038
Validation loss: 2.4662312828735695

Epoch: 6| Step: 9
Training loss: 2.5785480498962547
Validation loss: 2.470197181531573

Epoch: 6| Step: 10
Training loss: 2.3355408398718858
Validation loss: 2.463829819151764

Epoch: 6| Step: 11
Training loss: 1.8700429876761753
Validation loss: 2.473498924271637

Epoch: 6| Step: 12
Training loss: 2.0443690444453697
Validation loss: 2.4270711644379146

Epoch: 6| Step: 13
Training loss: 1.7358103818043
Validation loss: 2.4679553423687897

Epoch: 176| Step: 0
Training loss: 2.245031274692387
Validation loss: 2.44904354883395

Epoch: 6| Step: 1
Training loss: 1.9720707694928967
Validation loss: 2.4682525688151014

Epoch: 6| Step: 2
Training loss: 2.3215638970137094
Validation loss: 2.4387541374550343

Epoch: 6| Step: 3
Training loss: 2.60299780490239
Validation loss: 2.4670934388029004

Epoch: 6| Step: 4
Training loss: 2.491518510817468
Validation loss: 2.466098946125643

Epoch: 6| Step: 5
Training loss: 2.102918676540784
Validation loss: 2.469542964151219

Epoch: 6| Step: 6
Training loss: 2.6528765630609143
Validation loss: 2.487605371138221

Epoch: 6| Step: 7
Training loss: 3.033221989638697
Validation loss: 2.493592103975194

Epoch: 6| Step: 8
Training loss: 1.5562884651547353
Validation loss: 2.456654412979016

Epoch: 6| Step: 9
Training loss: 2.3401429967306924
Validation loss: 2.476874171472195

Epoch: 6| Step: 10
Training loss: 2.0171088146461735
Validation loss: 2.4654178096831654

Epoch: 6| Step: 11
Training loss: 2.5305756977655465
Validation loss: 2.4918564074942493

Epoch: 6| Step: 12
Training loss: 1.8466911779537416
Validation loss: 2.4710984281146837

Epoch: 6| Step: 13
Training loss: 3.174560011767071
Validation loss: 2.449347008289488

Epoch: 177| Step: 0
Training loss: 2.3788699693841595
Validation loss: 2.453830269349027

Epoch: 6| Step: 1
Training loss: 1.5148576189874887
Validation loss: 2.4713949788402565

Epoch: 6| Step: 2
Training loss: 1.965929827077115
Validation loss: 2.471125686958528

Epoch: 6| Step: 3
Training loss: 2.280691627204537
Validation loss: 2.4607430006997792

Epoch: 6| Step: 4
Training loss: 2.612884445742011
Validation loss: 2.4690010781142813

Epoch: 6| Step: 5
Training loss: 2.48429822503226
Validation loss: 2.4441936173667784

Epoch: 6| Step: 6
Training loss: 2.6746929099385572
Validation loss: 2.466107494355406

Epoch: 6| Step: 7
Training loss: 1.857272458008347
Validation loss: 2.4577125452378916

Epoch: 6| Step: 8
Training loss: 2.318914641013051
Validation loss: 2.4837997984685125

Epoch: 6| Step: 9
Training loss: 2.810627631961743
Validation loss: 2.4627268671546143

Epoch: 6| Step: 10
Training loss: 2.5417621990656643
Validation loss: 2.4331901780701695

Epoch: 6| Step: 11
Training loss: 2.2096669350876788
Validation loss: 2.466390563012114

Epoch: 6| Step: 12
Training loss: 2.5743480412571285
Validation loss: 2.473773408896419

Epoch: 6| Step: 13
Training loss: 2.6878808217478998
Validation loss: 2.463330094292609

Epoch: 178| Step: 0
Training loss: 3.0348605646471776
Validation loss: 2.450296749235936

Epoch: 6| Step: 1
Training loss: 1.9700485064736062
Validation loss: 2.499542976977238

Epoch: 6| Step: 2
Training loss: 2.8077314854452076
Validation loss: 2.4802022180818097

Epoch: 6| Step: 3
Training loss: 2.131358562847953
Validation loss: 2.4544971005094443

Epoch: 6| Step: 4
Training loss: 2.2681775676640905
Validation loss: 2.4768906020114434

Epoch: 6| Step: 5
Training loss: 1.9562885146569602
Validation loss: 2.4321056148214897

Epoch: 6| Step: 6
Training loss: 2.4883052040276556
Validation loss: 2.465113723448043

Epoch: 6| Step: 7
Training loss: 2.0893279107458995
Validation loss: 2.4726936091141054

Epoch: 6| Step: 8
Training loss: 2.219268308463664
Validation loss: 2.4954575401803183

Epoch: 6| Step: 9
Training loss: 2.502269096582913
Validation loss: 2.4697267867136277

Epoch: 6| Step: 10
Training loss: 2.135717516539892
Validation loss: 2.4645245706850893

Epoch: 6| Step: 11
Training loss: 2.306468624514355
Validation loss: 2.4776903304725044

Epoch: 6| Step: 12
Training loss: 2.610469383038283
Validation loss: 2.4565835909453964

Epoch: 6| Step: 13
Training loss: 2.2524415650234335
Validation loss: 2.480627613052764

Epoch: 179| Step: 0
Training loss: 1.904200718726171
Validation loss: 2.4664419300742404

Epoch: 6| Step: 1
Training loss: 2.480707784834605
Validation loss: 2.4710582681055624

Epoch: 6| Step: 2
Training loss: 1.527523332657991
Validation loss: 2.486330033556215

Epoch: 6| Step: 3
Training loss: 2.6905442672119158
Validation loss: 2.4720523108794175

Epoch: 6| Step: 4
Training loss: 2.4784085586545372
Validation loss: 2.4282872066527954

Epoch: 6| Step: 5
Training loss: 2.9094786399139836
Validation loss: 2.478280204101794

Epoch: 6| Step: 6
Training loss: 2.7855079717705205
Validation loss: 2.4166745414506576

Epoch: 6| Step: 7
Training loss: 2.355361050346684
Validation loss: 2.4564209728953155

Epoch: 6| Step: 8
Training loss: 1.9245567529823144
Validation loss: 2.469556040065436

Epoch: 6| Step: 9
Training loss: 2.367915680434423
Validation loss: 2.4737012307076065

Epoch: 6| Step: 10
Training loss: 2.6133083461844966
Validation loss: 2.4625844928560054

Epoch: 6| Step: 11
Training loss: 1.9102320119737004
Validation loss: 2.477679057502707

Epoch: 6| Step: 12
Training loss: 1.9093410257916201
Validation loss: 2.456056253206068

Epoch: 6| Step: 13
Training loss: 2.664165953476084
Validation loss: 2.4659553744749876

Epoch: 180| Step: 0
Training loss: 1.9782394110932426
Validation loss: 2.454429739834803

Epoch: 6| Step: 1
Training loss: 2.212278249371634
Validation loss: 2.4551808792769196

Epoch: 6| Step: 2
Training loss: 2.1515704252475225
Validation loss: 2.473404051642555

Epoch: 6| Step: 3
Training loss: 2.469302055345487
Validation loss: 2.473992010555524

Epoch: 6| Step: 4
Training loss: 1.8075737270445658
Validation loss: 2.4651289157811442

Epoch: 6| Step: 5
Training loss: 2.1628549008952636
Validation loss: 2.476626786797378

Epoch: 6| Step: 6
Training loss: 2.868685546682848
Validation loss: 2.468536202584101

Epoch: 6| Step: 7
Training loss: 2.1571827957873477
Validation loss: 2.458992192108204

Epoch: 6| Step: 8
Training loss: 2.75744937743049
Validation loss: 2.4625663423156565

Epoch: 6| Step: 9
Training loss: 2.4472672811808542
Validation loss: 2.4605173466212977

Epoch: 6| Step: 10
Training loss: 2.30539374441167
Validation loss: 2.4851521082596526

Epoch: 6| Step: 11
Training loss: 2.4871088496777265
Validation loss: 2.450239152099694

Epoch: 6| Step: 12
Training loss: 2.6086021695111112
Validation loss: 2.4719461603607984

Epoch: 6| Step: 13
Training loss: 2.3150859498779446
Validation loss: 2.4474673251281485

Epoch: 181| Step: 0
Training loss: 2.5042880476880964
Validation loss: 2.4811929157427417

Epoch: 6| Step: 1
Training loss: 2.3910066387255062
Validation loss: 2.4602785581557374

Epoch: 6| Step: 2
Training loss: 2.8165562519650478
Validation loss: 2.478230087914981

Epoch: 6| Step: 3
Training loss: 2.7879701500147105
Validation loss: 2.47961471437044

Epoch: 6| Step: 4
Training loss: 1.6665097003772333
Validation loss: 2.470079255471797

Epoch: 6| Step: 5
Training loss: 2.0972029268950463
Validation loss: 2.4768829547265527

Epoch: 6| Step: 6
Training loss: 2.442385838631422
Validation loss: 2.492500397915668

Epoch: 6| Step: 7
Training loss: 1.984768445516533
Validation loss: 2.470701170131032

Epoch: 6| Step: 8
Training loss: 1.8427335879369107
Validation loss: 2.4412060564552753

Epoch: 6| Step: 9
Training loss: 2.4555991749559687
Validation loss: 2.4405082343302644

Epoch: 6| Step: 10
Training loss: 2.513828180843459
Validation loss: 2.4851434001366095

Epoch: 6| Step: 11
Training loss: 2.780278411206914
Validation loss: 2.4872596108851135

Epoch: 6| Step: 12
Training loss: 1.9444366689556818
Validation loss: 2.469039293595115

Epoch: 6| Step: 13
Training loss: 2.2009744090178813
Validation loss: 2.444549675354754

Epoch: 182| Step: 0
Training loss: 2.6100746176273426
Validation loss: 2.4700179524537544

Epoch: 6| Step: 1
Training loss: 2.1828126004873702
Validation loss: 2.4638338914461113

Epoch: 6| Step: 2
Training loss: 1.9202537211027124
Validation loss: 2.4534967466215636

Epoch: 6| Step: 3
Training loss: 2.3084593861266525
Validation loss: 2.439339498188238

Epoch: 6| Step: 4
Training loss: 1.981492359247716
Validation loss: 2.4456968356998443

Epoch: 6| Step: 5
Training loss: 2.2962625945716137
Validation loss: 2.4698372395531627

Epoch: 6| Step: 6
Training loss: 2.2648524578620544
Validation loss: 2.4861858586828323

Epoch: 6| Step: 7
Training loss: 3.3307703814468117
Validation loss: 2.455155186150976

Epoch: 6| Step: 8
Training loss: 2.5575779419896096
Validation loss: 2.493906812712866

Epoch: 6| Step: 9
Training loss: 2.545913327506083
Validation loss: 2.47073683068221

Epoch: 6| Step: 10
Training loss: 1.9424009787360652
Validation loss: 2.471728428434011

Epoch: 6| Step: 11
Training loss: 1.9358774589975685
Validation loss: 2.478333183072196

Epoch: 6| Step: 12
Training loss: 2.684458763938393
Validation loss: 2.4652170645123106

Epoch: 6| Step: 13
Training loss: 1.4431167832277363
Validation loss: 2.4790883450447545

Epoch: 183| Step: 0
Training loss: 3.0112712839402893
Validation loss: 2.479598677702221

Epoch: 6| Step: 1
Training loss: 2.348272106364287
Validation loss: 2.4724803554134787

Epoch: 6| Step: 2
Training loss: 2.2513066842050717
Validation loss: 2.457633062840893

Epoch: 6| Step: 3
Training loss: 2.3086724437333275
Validation loss: 2.4756275442510853

Epoch: 6| Step: 4
Training loss: 2.5112184586924795
Validation loss: 2.4722823499058304

Epoch: 6| Step: 5
Training loss: 1.8524457539017658
Validation loss: 2.4304456216197265

Epoch: 6| Step: 6
Training loss: 2.1818314266525296
Validation loss: 2.442445025355458

Epoch: 6| Step: 7
Training loss: 2.1709488465174753
Validation loss: 2.470014898939344

Epoch: 6| Step: 8
Training loss: 1.3624682693986319
Validation loss: 2.457088067118917

Epoch: 6| Step: 9
Training loss: 2.829725960986722
Validation loss: 2.465468412988897

Epoch: 6| Step: 10
Training loss: 2.729550885437721
Validation loss: 2.4676174298560505

Epoch: 6| Step: 11
Training loss: 2.2676271208234917
Validation loss: 2.4588509694757485

Epoch: 6| Step: 12
Training loss: 2.126944157465026
Validation loss: 2.4620658973898615

Epoch: 6| Step: 13
Training loss: 2.377186120430155
Validation loss: 2.4714092482464327

Epoch: 184| Step: 0
Training loss: 2.754624899305977
Validation loss: 2.455256072362546

Epoch: 6| Step: 1
Training loss: 1.7587717109430032
Validation loss: 2.440756060514958

Epoch: 6| Step: 2
Training loss: 2.022861943812673
Validation loss: 2.4616996588894047

Epoch: 6| Step: 3
Training loss: 2.1813720975002604
Validation loss: 2.4533238316981048

Epoch: 6| Step: 4
Training loss: 2.5391577130585388
Validation loss: 2.4634908347309974

Epoch: 6| Step: 5
Training loss: 2.2065412939630455
Validation loss: 2.4683711798566956

Epoch: 6| Step: 6
Training loss: 1.1984433329414155
Validation loss: 2.466956570669236

Epoch: 6| Step: 7
Training loss: 2.4416256737332906
Validation loss: 2.4559835485918673

Epoch: 6| Step: 8
Training loss: 2.8587886021405144
Validation loss: 2.4550668515795886

Epoch: 6| Step: 9
Training loss: 2.4636492608240497
Validation loss: 2.4530254297040264

Epoch: 6| Step: 10
Training loss: 2.325953347601737
Validation loss: 2.4627707834769037

Epoch: 6| Step: 11
Training loss: 2.5534415750327466
Validation loss: 2.4631773397369594

Epoch: 6| Step: 12
Training loss: 2.5010082119239114
Validation loss: 2.4453499219868755

Epoch: 6| Step: 13
Training loss: 2.652744628095522
Validation loss: 2.473011100182282

Epoch: 185| Step: 0
Training loss: 2.323130428132373
Validation loss: 2.4659769016589257

Epoch: 6| Step: 1
Training loss: 2.535382417955828
Validation loss: 2.4579049563278916

Epoch: 6| Step: 2
Training loss: 2.486076779782629
Validation loss: 2.440832004668244

Epoch: 6| Step: 3
Training loss: 2.2455249364670635
Validation loss: 2.465900150624157

Epoch: 6| Step: 4
Training loss: 2.8528210514611945
Validation loss: 2.478416564818977

Epoch: 6| Step: 5
Training loss: 1.9094092033556807
Validation loss: 2.4673887824901506

Epoch: 6| Step: 6
Training loss: 2.0282809345975967
Validation loss: 2.4555236478469205

Epoch: 6| Step: 7
Training loss: 2.507477639917265
Validation loss: 2.4698452610463777

Epoch: 6| Step: 8
Training loss: 2.5490086517127297
Validation loss: 2.4591806529490783

Epoch: 6| Step: 9
Training loss: 1.738306349669561
Validation loss: 2.4681666598714807

Epoch: 6| Step: 10
Training loss: 2.735691211536256
Validation loss: 2.4299422777677266

Epoch: 6| Step: 11
Training loss: 2.4349129470522968
Validation loss: 2.468524131759323

Epoch: 6| Step: 12
Training loss: 1.70751066481023
Validation loss: 2.4466510615801593

Epoch: 6| Step: 13
Training loss: 2.0304357217037685
Validation loss: 2.454469390649871

Epoch: 186| Step: 0
Training loss: 2.292736347124976
Validation loss: 2.4150038862663146

Epoch: 6| Step: 1
Training loss: 2.901656591875209
Validation loss: 2.478699657132975

Epoch: 6| Step: 2
Training loss: 2.3056367632596073
Validation loss: 2.45178884570852

Epoch: 6| Step: 3
Training loss: 1.365635454150936
Validation loss: 2.4536640073303553

Epoch: 6| Step: 4
Training loss: 1.9353289592184075
Validation loss: 2.476059948790478

Epoch: 6| Step: 5
Training loss: 2.7255296035023857
Validation loss: 2.4572394975478336

Epoch: 6| Step: 6
Training loss: 2.103051888043881
Validation loss: 2.4621549507518345

Epoch: 6| Step: 7
Training loss: 2.955663165505299
Validation loss: 2.4528028188937294

Epoch: 6| Step: 8
Training loss: 2.2105298424695023
Validation loss: 2.469028758828234

Epoch: 6| Step: 9
Training loss: 2.740804209227261
Validation loss: 2.4489975319228385

Epoch: 6| Step: 10
Training loss: 2.5665393719338825
Validation loss: 2.4382100907588424

Epoch: 6| Step: 11
Training loss: 1.3893033723132773
Validation loss: 2.4324536048408754

Epoch: 6| Step: 12
Training loss: 1.9663830409841923
Validation loss: 2.440813896133092

Epoch: 6| Step: 13
Training loss: 2.3804943377550263
Validation loss: 2.4350297641074214

Epoch: 187| Step: 0
Training loss: 2.4263499602254583
Validation loss: 2.4600553604487683

Epoch: 6| Step: 1
Training loss: 2.142964108385934
Validation loss: 2.4392131720017898

Epoch: 6| Step: 2
Training loss: 2.6820282346873525
Validation loss: 2.480938503105626

Epoch: 6| Step: 3
Training loss: 2.326394481241011
Validation loss: 2.4599181794048053

Epoch: 6| Step: 4
Training loss: 1.8160162394213442
Validation loss: 2.4610663100813137

Epoch: 6| Step: 5
Training loss: 1.8295578517310707
Validation loss: 2.4725549010931633

Epoch: 6| Step: 6
Training loss: 1.7013658310069497
Validation loss: 2.4520678118389454

Epoch: 6| Step: 7
Training loss: 2.382888142370063
Validation loss: 2.4405633121418315

Epoch: 6| Step: 8
Training loss: 2.0652702684856696
Validation loss: 2.432318769478172

Epoch: 6| Step: 9
Training loss: 2.2305100098115744
Validation loss: 2.4474569007168903

Epoch: 6| Step: 10
Training loss: 2.7058609363435204
Validation loss: 2.4278842826712888

Epoch: 6| Step: 11
Training loss: 3.2887145377102702
Validation loss: 2.45320239420816

Epoch: 6| Step: 12
Training loss: 2.140752524037083
Validation loss: 2.43104193497529

Epoch: 6| Step: 13
Training loss: 2.1261323268113066
Validation loss: 2.4643398503292255

Epoch: 188| Step: 0
Training loss: 2.5763441929077704
Validation loss: 2.470919982376364

Epoch: 6| Step: 1
Training loss: 1.9690442546561602
Validation loss: 2.458399941897879

Epoch: 6| Step: 2
Training loss: 2.9890633550663637
Validation loss: 2.460172017380468

Epoch: 6| Step: 3
Training loss: 2.153926758777497
Validation loss: 2.470372103380241

Epoch: 6| Step: 4
Training loss: 1.9766495877707997
Validation loss: 2.486279418170246

Epoch: 6| Step: 5
Training loss: 2.394885785240483
Validation loss: 2.4753800339551115

Epoch: 6| Step: 6
Training loss: 2.0585108239049386
Validation loss: 2.4783077046273774

Epoch: 6| Step: 7
Training loss: 2.131356884913439
Validation loss: 2.4562504900897615

Epoch: 6| Step: 8
Training loss: 1.8997209394209877
Validation loss: 2.4599029632276257

Epoch: 6| Step: 9
Training loss: 2.869990420973473
Validation loss: 2.4581367646292476

Epoch: 6| Step: 10
Training loss: 2.1944856545915283
Validation loss: 2.4468455767355155

Epoch: 6| Step: 11
Training loss: 1.7563413440123226
Validation loss: 2.458229208385802

Epoch: 6| Step: 12
Training loss: 2.712091205925611
Validation loss: 2.4721568142236667

Epoch: 6| Step: 13
Training loss: 2.1565710740913273
Validation loss: 2.456143860176815

Epoch: 189| Step: 0
Training loss: 2.142858745937656
Validation loss: 2.4513808807112407

Epoch: 6| Step: 1
Training loss: 1.859184191233777
Validation loss: 2.4603023180065526

Epoch: 6| Step: 2
Training loss: 2.305943550901155
Validation loss: 2.4344703455661008

Epoch: 6| Step: 3
Training loss: 2.202645107516308
Validation loss: 2.4442739468796213

Epoch: 6| Step: 4
Training loss: 2.1594126630443764
Validation loss: 2.4423322578116915

Epoch: 6| Step: 5
Training loss: 2.283963196840414
Validation loss: 2.438544484753742

Epoch: 6| Step: 6
Training loss: 2.6880764010448073
Validation loss: 2.4416209892749845

Epoch: 6| Step: 7
Training loss: 2.5532270917709847
Validation loss: 2.440975212915662

Epoch: 6| Step: 8
Training loss: 1.8756668176663933
Validation loss: 2.4741031837911627

Epoch: 6| Step: 9
Training loss: 3.0644472120919692
Validation loss: 2.488928179151834

Epoch: 6| Step: 10
Training loss: 2.609168130132443
Validation loss: 2.4592591338376852

Epoch: 6| Step: 11
Training loss: 1.8440217286648126
Validation loss: 2.4448131722571356

Epoch: 6| Step: 12
Training loss: 2.131769505163382
Validation loss: 2.465408764096364

Epoch: 6| Step: 13
Training loss: 2.301085369431323
Validation loss: 2.4354193341778054

Epoch: 190| Step: 0
Training loss: 2.281708344700496
Validation loss: 2.4612914971968247

Epoch: 6| Step: 1
Training loss: 2.397917153639845
Validation loss: 2.471604339569634

Epoch: 6| Step: 2
Training loss: 2.437295758420318
Validation loss: 2.441841369575546

Epoch: 6| Step: 3
Training loss: 2.117984298035482
Validation loss: 2.4632187075130627

Epoch: 6| Step: 4
Training loss: 2.0676267080907063
Validation loss: 2.4690652409516494

Epoch: 6| Step: 5
Training loss: 2.3208303162733794
Validation loss: 2.4848865723495006

Epoch: 6| Step: 6
Training loss: 2.3418169315042396
Validation loss: 2.449734517133363

Epoch: 6| Step: 7
Training loss: 2.3008801061768707
Validation loss: 2.473943013703474

Epoch: 6| Step: 8
Training loss: 2.776865986481642
Validation loss: 2.4633802888207526

Epoch: 6| Step: 9
Training loss: 1.6684302377265885
Validation loss: 2.4725019678367044

Epoch: 6| Step: 10
Training loss: 2.5596694199383787
Validation loss: 2.4671107782512016

Epoch: 6| Step: 11
Training loss: 2.3154661384072
Validation loss: 2.4923235400643518

Epoch: 6| Step: 12
Training loss: 2.188609032832541
Validation loss: 2.461903694053778

Epoch: 6| Step: 13
Training loss: 2.23066627710084
Validation loss: 2.4581725000322283

Epoch: 191| Step: 0
Training loss: 2.668989977443682
Validation loss: 2.491019411216389

Epoch: 6| Step: 1
Training loss: 1.4979324396582503
Validation loss: 2.454121420932938

Epoch: 6| Step: 2
Training loss: 2.0588269906856387
Validation loss: 2.459094696305581

Epoch: 6| Step: 3
Training loss: 2.3100278889017036
Validation loss: 2.4401350353093463

Epoch: 6| Step: 4
Training loss: 2.2811826931155945
Validation loss: 2.4825300487737048

Epoch: 6| Step: 5
Training loss: 2.337688083527548
Validation loss: 2.454747732579663

Epoch: 6| Step: 6
Training loss: 2.545072889202299
Validation loss: 2.4644667630616834

Epoch: 6| Step: 7
Training loss: 1.9876182664915856
Validation loss: 2.4569622841155687

Epoch: 6| Step: 8
Training loss: 2.4558486882422166
Validation loss: 2.4732202422170526

Epoch: 6| Step: 9
Training loss: 2.4499238138614814
Validation loss: 2.4632228789099977

Epoch: 6| Step: 10
Training loss: 2.4194418220880514
Validation loss: 2.442480700649801

Epoch: 6| Step: 11
Training loss: 2.5676710475564177
Validation loss: 2.467277872995565

Epoch: 6| Step: 12
Training loss: 1.9031949185461106
Validation loss: 2.432180390304054

Epoch: 6| Step: 13
Training loss: 2.3940437156618506
Validation loss: 2.4474591758232958

Epoch: 192| Step: 0
Training loss: 2.1562121844085507
Validation loss: 2.4440222480758482

Epoch: 6| Step: 1
Training loss: 2.3615178362519895
Validation loss: 2.472284836517909

Epoch: 6| Step: 2
Training loss: 2.875499184436049
Validation loss: 2.4397969757067735

Epoch: 6| Step: 3
Training loss: 2.1385572707592804
Validation loss: 2.4428806399399465

Epoch: 6| Step: 4
Training loss: 2.7864191260505855
Validation loss: 2.462883161683919

Epoch: 6| Step: 5
Training loss: 2.2402060655769054
Validation loss: 2.4474365755011798

Epoch: 6| Step: 6
Training loss: 1.741089136781172
Validation loss: 2.466100200865833

Epoch: 6| Step: 7
Training loss: 1.3303623875983663
Validation loss: 2.4497291056896837

Epoch: 6| Step: 8
Training loss: 1.6756499979330768
Validation loss: 2.4577046958957762

Epoch: 6| Step: 9
Training loss: 2.2545213835094073
Validation loss: 2.4483285191358073

Epoch: 6| Step: 10
Training loss: 2.1952671032756177
Validation loss: 2.4309523223944804

Epoch: 6| Step: 11
Training loss: 2.5942178327263217
Validation loss: 2.4782721737045597

Epoch: 6| Step: 12
Training loss: 2.207193716365308
Validation loss: 2.4496251766520194

Epoch: 6| Step: 13
Training loss: 3.1163982262153067
Validation loss: 2.468178546521302

Epoch: 193| Step: 0
Training loss: 2.8293161149551187
Validation loss: 2.453130413096736

Epoch: 6| Step: 1
Training loss: 2.8690950871769125
Validation loss: 2.4409990986943457

Epoch: 6| Step: 2
Training loss: 2.2638509184208577
Validation loss: 2.435833863478793

Epoch: 6| Step: 3
Training loss: 1.7187111590071507
Validation loss: 2.454186445991107

Epoch: 6| Step: 4
Training loss: 2.042699850850355
Validation loss: 2.43677035492046

Epoch: 6| Step: 5
Training loss: 2.492084369498695
Validation loss: 2.458022932461701

Epoch: 6| Step: 6
Training loss: 2.1544427346785704
Validation loss: 2.4619071772817693

Epoch: 6| Step: 7
Training loss: 2.3763271438461664
Validation loss: 2.4804851334747546

Epoch: 6| Step: 8
Training loss: 2.005782708148704
Validation loss: 2.472891485797525

Epoch: 6| Step: 9
Training loss: 1.9051465452202485
Validation loss: 2.4243301464408904

Epoch: 6| Step: 10
Training loss: 2.0379687185535342
Validation loss: 2.42767028140559

Epoch: 6| Step: 11
Training loss: 2.368927772312991
Validation loss: 2.4620894020676656

Epoch: 6| Step: 12
Training loss: 2.4673898547478266
Validation loss: 2.453023941490603

Epoch: 6| Step: 13
Training loss: 2.1290109591150492
Validation loss: 2.4654987068864056

Epoch: 194| Step: 0
Training loss: 1.8312932136306166
Validation loss: 2.4410377305033366

Epoch: 6| Step: 1
Training loss: 2.237221352046274
Validation loss: 2.4391112985010093

Epoch: 6| Step: 2
Training loss: 2.241896234927016
Validation loss: 2.4745914338696076

Epoch: 6| Step: 3
Training loss: 2.199393934519057
Validation loss: 2.4221774989071627

Epoch: 6| Step: 4
Training loss: 2.6535453041109434
Validation loss: 2.4371886095211672

Epoch: 6| Step: 5
Training loss: 2.0585155725564177
Validation loss: 2.448269783546209

Epoch: 6| Step: 6
Training loss: 2.321207612389653
Validation loss: 2.463248913033965

Epoch: 6| Step: 7
Training loss: 2.658538012462582
Validation loss: 2.4623866848307023

Epoch: 6| Step: 8
Training loss: 2.5247472428823237
Validation loss: 2.4385900939054013

Epoch: 6| Step: 9
Training loss: 2.481095170347911
Validation loss: 2.439563329123594

Epoch: 6| Step: 10
Training loss: 2.0040606760825406
Validation loss: 2.4507463401633904

Epoch: 6| Step: 11
Training loss: 2.5344647374005724
Validation loss: 2.446156152030057

Epoch: 6| Step: 12
Training loss: 2.341154976142001
Validation loss: 2.482454652205912

Epoch: 6| Step: 13
Training loss: 1.3442077744109142
Validation loss: 2.4557026538577538

Epoch: 195| Step: 0
Training loss: 2.359891418125117
Validation loss: 2.457601363880124

Epoch: 6| Step: 1
Training loss: 2.341776512891041
Validation loss: 2.4653823727131057

Epoch: 6| Step: 2
Training loss: 2.09071448894386
Validation loss: 2.4603170081109007

Epoch: 6| Step: 3
Training loss: 2.028550452233284
Validation loss: 2.440672892628068

Epoch: 6| Step: 4
Training loss: 2.6153753127282178
Validation loss: 2.4726752160403827

Epoch: 6| Step: 5
Training loss: 2.6681912554201372
Validation loss: 2.456593487191251

Epoch: 6| Step: 6
Training loss: 1.8832795762798633
Validation loss: 2.4355487475629896

Epoch: 6| Step: 7
Training loss: 2.815629108523336
Validation loss: 2.4525425783034422

Epoch: 6| Step: 8
Training loss: 1.732379899919548
Validation loss: 2.481753445321974

Epoch: 6| Step: 9
Training loss: 2.9510188590067292
Validation loss: 2.4622752137271284

Epoch: 6| Step: 10
Training loss: 1.6983245952457529
Validation loss: 2.4909215594651233

Epoch: 6| Step: 11
Training loss: 1.7362326905806906
Validation loss: 2.4380416121141555

Epoch: 6| Step: 12
Training loss: 2.074186910323836
Validation loss: 2.463053838007872

Epoch: 6| Step: 13
Training loss: 2.5296968946465066
Validation loss: 2.4648781161635194

Epoch: 196| Step: 0
Training loss: 1.2951245731838683
Validation loss: 2.4582293601249887

Epoch: 6| Step: 1
Training loss: 2.188983741013261
Validation loss: 2.4798047220225534

Epoch: 6| Step: 2
Training loss: 2.3705066538526047
Validation loss: 2.458497657379863

Epoch: 6| Step: 3
Training loss: 2.812414379935897
Validation loss: 2.4177469947732617

Epoch: 6| Step: 4
Training loss: 1.9639526530694509
Validation loss: 2.4377942976443254

Epoch: 6| Step: 5
Training loss: 2.74073513944968
Validation loss: 2.425656897972723

Epoch: 6| Step: 6
Training loss: 2.31151797088892
Validation loss: 2.470118763015449

Epoch: 6| Step: 7
Training loss: 2.3542894539558668
Validation loss: 2.442119708420117

Epoch: 6| Step: 8
Training loss: 2.249119056328725
Validation loss: 2.4666330730037505

Epoch: 6| Step: 9
Training loss: 1.5485209945817333
Validation loss: 2.4321586050934823

Epoch: 6| Step: 10
Training loss: 2.4179270406401105
Validation loss: 2.4637369349363696

Epoch: 6| Step: 11
Training loss: 2.2423042506216584
Validation loss: 2.455229039328317

Epoch: 6| Step: 12
Training loss: 2.279345618922345
Validation loss: 2.4286706128112585

Epoch: 6| Step: 13
Training loss: 2.4376325326737347
Validation loss: 2.466271209639733

Epoch: 197| Step: 0
Training loss: 2.203419550691522
Validation loss: 2.45127691084648

Epoch: 6| Step: 1
Training loss: 1.9919194059805418
Validation loss: 2.410078364857068

Epoch: 6| Step: 2
Training loss: 1.7665958267380284
Validation loss: 2.4307147333044052

Epoch: 6| Step: 3
Training loss: 1.9372192302430717
Validation loss: 2.461408346552302

Epoch: 6| Step: 4
Training loss: 2.5474419460102005
Validation loss: 2.439365880216207

Epoch: 6| Step: 5
Training loss: 2.3159860676954165
Validation loss: 2.4323691043769378

Epoch: 6| Step: 6
Training loss: 2.3387285482135383
Validation loss: 2.4472505883769395

Epoch: 6| Step: 7
Training loss: 2.548933542928946
Validation loss: 2.4552898308687654

Epoch: 6| Step: 8
Training loss: 2.255867618694353
Validation loss: 2.469943746417996

Epoch: 6| Step: 9
Training loss: 3.0656662746687875
Validation loss: 2.4333865151329985

Epoch: 6| Step: 10
Training loss: 2.0413496848327095
Validation loss: 2.4321137997604647

Epoch: 6| Step: 11
Training loss: 2.255394931559049
Validation loss: 2.4460315393507615

Epoch: 6| Step: 12
Training loss: 2.017828867602063
Validation loss: 2.4406071787499655

Epoch: 6| Step: 13
Training loss: 2.2055624606485904
Validation loss: 2.4531305575747084

Epoch: 198| Step: 0
Training loss: 1.6697161274475805
Validation loss: 2.473559071436732

Epoch: 6| Step: 1
Training loss: 2.899570788486856
Validation loss: 2.435644250576674

Epoch: 6| Step: 2
Training loss: 2.224306590010911
Validation loss: 2.410382100662023

Epoch: 6| Step: 3
Training loss: 2.147525774231539
Validation loss: 2.437760981998958

Epoch: 6| Step: 4
Training loss: 2.2462610331727677
Validation loss: 2.4418760626166467

Epoch: 6| Step: 5
Training loss: 2.183570711563672
Validation loss: 2.435764623689458

Epoch: 6| Step: 6
Training loss: 2.591072631006393
Validation loss: 2.4427245243812026

Epoch: 6| Step: 7
Training loss: 1.9247226862130664
Validation loss: 2.45452120459499

Epoch: 6| Step: 8
Training loss: 2.3876228181095978
Validation loss: 2.444916764787833

Epoch: 6| Step: 9
Training loss: 2.136509858961902
Validation loss: 2.4598613729400225

Epoch: 6| Step: 10
Training loss: 2.3230396004436713
Validation loss: 2.442250441035106

Epoch: 6| Step: 11
Training loss: 2.047356003016666
Validation loss: 2.424337406445661

Epoch: 6| Step: 12
Training loss: 2.4215464707554295
Validation loss: 2.4124766101752138

Epoch: 6| Step: 13
Training loss: 1.7426313852612265
Validation loss: 2.482292163319652

Epoch: 199| Step: 0
Training loss: 1.8950003446311938
Validation loss: 2.408854832182141

Epoch: 6| Step: 1
Training loss: 2.5529301283016412
Validation loss: 2.4680024751176894

Epoch: 6| Step: 2
Training loss: 2.1681848366761614
Validation loss: 2.444951955239068

Epoch: 6| Step: 3
Training loss: 1.7378359106277717
Validation loss: 2.447655268012682

Epoch: 6| Step: 4
Training loss: 2.447190413723564
Validation loss: 2.4401386305086548

Epoch: 6| Step: 5
Training loss: 1.8323313908466334
Validation loss: 2.448645465747804

Epoch: 6| Step: 6
Training loss: 2.2844248587494
Validation loss: 2.454341213334144

Epoch: 6| Step: 7
Training loss: 2.6920963282985664
Validation loss: 2.460735038089676

Epoch: 6| Step: 8
Training loss: 2.8029489043005458
Validation loss: 2.4578610521530044

Epoch: 6| Step: 9
Training loss: 2.2468238241126834
Validation loss: 2.453363833725484

Epoch: 6| Step: 10
Training loss: 2.3732206806849963
Validation loss: 2.453576965895893

Epoch: 6| Step: 11
Training loss: 2.2381844225006917
Validation loss: 2.4269545167654

Epoch: 6| Step: 12
Training loss: 2.0761679705836995
Validation loss: 2.4905485280853457

Epoch: 6| Step: 13
Training loss: 2.0458721004420553
Validation loss: 2.442849573364258

Epoch: 200| Step: 0
Training loss: 2.1751361760447043
Validation loss: 2.4417657056449826

Epoch: 6| Step: 1
Training loss: 2.8678594732079965
Validation loss: 2.454820191304068

Epoch: 6| Step: 2
Training loss: 2.029103480092221
Validation loss: 2.4812895538416724

Epoch: 6| Step: 3
Training loss: 2.30281047355572
Validation loss: 2.43711327344922

Epoch: 6| Step: 4
Training loss: 2.0919637319655844
Validation loss: 2.44936547867999

Epoch: 6| Step: 5
Training loss: 2.2997573102537743
Validation loss: 2.425407502543428

Epoch: 6| Step: 6
Training loss: 2.1640261430130745
Validation loss: 2.4226817491275567

Epoch: 6| Step: 7
Training loss: 1.8250513565989404
Validation loss: 2.4313690915585853

Epoch: 6| Step: 8
Training loss: 2.2930529707302245
Validation loss: 2.46556413789767

Epoch: 6| Step: 9
Training loss: 2.408896242066375
Validation loss: 2.470307663152263

Epoch: 6| Step: 10
Training loss: 2.1190863958355113
Validation loss: 2.485702132315501

Epoch: 6| Step: 11
Training loss: 2.057665157618923
Validation loss: 2.4338981029762228

Epoch: 6| Step: 12
Training loss: 2.1658127520313415
Validation loss: 2.4581863633306225

Epoch: 6| Step: 13
Training loss: 2.859354988403334
Validation loss: 2.468963767480514

Epoch: 201| Step: 0
Training loss: 2.8651098888686573
Validation loss: 2.4632956565768502

Epoch: 6| Step: 1
Training loss: 2.4385802002906347
Validation loss: 2.4118614177938293

Epoch: 6| Step: 2
Training loss: 2.038274854048333
Validation loss: 2.46059696506299

Epoch: 6| Step: 3
Training loss: 2.0463902722134204
Validation loss: 2.436545954919237

Epoch: 6| Step: 4
Training loss: 2.163597855797169
Validation loss: 2.443331614372581

Epoch: 6| Step: 5
Training loss: 2.1806900836025864
Validation loss: 2.4258502473959895

Epoch: 6| Step: 6
Training loss: 3.034656616091496
Validation loss: 2.451101547727281

Epoch: 6| Step: 7
Training loss: 1.7802588398931831
Validation loss: 2.417496981641969

Epoch: 6| Step: 8
Training loss: 2.606202258214791
Validation loss: 2.4459912130309203

Epoch: 6| Step: 9
Training loss: 1.5055632739242648
Validation loss: 2.428421162675921

Epoch: 6| Step: 10
Training loss: 1.7989329619202474
Validation loss: 2.439339019477331

Epoch: 6| Step: 11
Training loss: 2.207756746942066
Validation loss: 2.440329050833135

Epoch: 6| Step: 12
Training loss: 2.1012420977296937
Validation loss: 2.4584996000543264

Epoch: 6| Step: 13
Training loss: 2.101303935541621
Validation loss: 2.444274761824462

Epoch: 202| Step: 0
Training loss: 1.691147147897629
Validation loss: 2.4199979210450313

Epoch: 6| Step: 1
Training loss: 2.6088014988918027
Validation loss: 2.4671295754616946

Epoch: 6| Step: 2
Training loss: 2.4101062936412516
Validation loss: 2.444986187876381

Epoch: 6| Step: 3
Training loss: 2.5702180700145267
Validation loss: 2.4372884313048258

Epoch: 6| Step: 4
Training loss: 2.488644080904234
Validation loss: 2.430330027260729

Epoch: 6| Step: 5
Training loss: 2.0448031823690185
Validation loss: 2.433636362255823

Epoch: 6| Step: 6
Training loss: 2.326662359406748
Validation loss: 2.4325755058823684

Epoch: 6| Step: 7
Training loss: 1.8429673683643215
Validation loss: 2.426211797470923

Epoch: 6| Step: 8
Training loss: 2.1055579794693915
Validation loss: 2.431618945654441

Epoch: 6| Step: 9
Training loss: 2.9778921606702036
Validation loss: 2.4548532827433447

Epoch: 6| Step: 10
Training loss: 1.7536258600825516
Validation loss: 2.4655306349662838

Epoch: 6| Step: 11
Training loss: 2.021357229168708
Validation loss: 2.429478667773262

Epoch: 6| Step: 12
Training loss: 2.3502367914902145
Validation loss: 2.455382408435066

Epoch: 6| Step: 13
Training loss: 2.0844907851423824
Validation loss: 2.4363785870504455

Epoch: 203| Step: 0
Training loss: 2.1642552919960956
Validation loss: 2.4742214464945196

Epoch: 6| Step: 1
Training loss: 2.226927610210023
Validation loss: 2.4572989733363393

Epoch: 6| Step: 2
Training loss: 2.2524444229433245
Validation loss: 2.433029974110827

Epoch: 6| Step: 3
Training loss: 1.894866536153561
Validation loss: 2.437983380020119

Epoch: 6| Step: 4
Training loss: 2.2824839559870687
Validation loss: 2.427160356286972

Epoch: 6| Step: 5
Training loss: 2.209853051249715
Validation loss: 2.4248021606555317

Epoch: 6| Step: 6
Training loss: 1.8051957905005804
Validation loss: 2.4549400794680536

Epoch: 6| Step: 7
Training loss: 2.500618667346627
Validation loss: 2.448462935643425

Epoch: 6| Step: 8
Training loss: 2.644887970569183
Validation loss: 2.4429034618623806

Epoch: 6| Step: 9
Training loss: 1.8933243830339452
Validation loss: 2.419531912903572

Epoch: 6| Step: 10
Training loss: 2.038859740951772
Validation loss: 2.435678944527062

Epoch: 6| Step: 11
Training loss: 3.0409895957660917
Validation loss: 2.4235609814526167

Epoch: 6| Step: 12
Training loss: 1.8651534617184984
Validation loss: 2.4261352826039384

Epoch: 6| Step: 13
Training loss: 2.3460765226978433
Validation loss: 2.4481570005202093

Epoch: 204| Step: 0
Training loss: 2.5817512672046643
Validation loss: 2.430809038327058

Epoch: 6| Step: 1
Training loss: 1.8269335132563338
Validation loss: 2.42772571383088

Epoch: 6| Step: 2
Training loss: 2.662102568429486
Validation loss: 2.4553355398241217

Epoch: 6| Step: 3
Training loss: 2.343923231716521
Validation loss: 2.434704237664625

Epoch: 6| Step: 4
Training loss: 1.530850572456113
Validation loss: 2.4147147438361887

Epoch: 6| Step: 5
Training loss: 2.0093464613529717
Validation loss: 2.4003330639233775

Epoch: 6| Step: 6
Training loss: 2.675866965153117
Validation loss: 2.426047343318147

Epoch: 6| Step: 7
Training loss: 1.453528399515132
Validation loss: 2.4318433301271805

Epoch: 6| Step: 8
Training loss: 1.6337986208766646
Validation loss: 2.4342653411198927

Epoch: 6| Step: 9
Training loss: 1.9936986838884836
Validation loss: 2.448222511750046

Epoch: 6| Step: 10
Training loss: 2.961087265857004
Validation loss: 2.439774946427656

Epoch: 6| Step: 11
Training loss: 2.1901068277301263
Validation loss: 2.42147075548927

Epoch: 6| Step: 12
Training loss: 2.3044304138589466
Validation loss: 2.41201022663716

Epoch: 6| Step: 13
Training loss: 2.4762773319528755
Validation loss: 2.44744294626568

Epoch: 205| Step: 0
Training loss: 1.713803195030394
Validation loss: 2.4237273067209575

Epoch: 6| Step: 1
Training loss: 2.2135296510856484
Validation loss: 2.442352704229967

Epoch: 6| Step: 2
Training loss: 2.7178574774218984
Validation loss: 2.426888688650011

Epoch: 6| Step: 3
Training loss: 2.4074705917130905
Validation loss: 2.4496385545879873

Epoch: 6| Step: 4
Training loss: 1.7316764939695999
Validation loss: 2.4292885873232772

Epoch: 6| Step: 5
Training loss: 1.8027900530151157
Validation loss: 2.437510098882621

Epoch: 6| Step: 6
Training loss: 2.535511714895451
Validation loss: 2.428909576589685

Epoch: 6| Step: 7
Training loss: 1.9345643352972717
Validation loss: 2.4302518751186284

Epoch: 6| Step: 8
Training loss: 2.30137049022322
Validation loss: 2.4225483313288163

Epoch: 6| Step: 9
Training loss: 2.1043333484865783
Validation loss: 2.4523726674057813

Epoch: 6| Step: 10
Training loss: 1.6839453380982652
Validation loss: 2.435280707997287

Epoch: 6| Step: 11
Training loss: 3.0918028898295096
Validation loss: 2.4406708412297506

Epoch: 6| Step: 12
Training loss: 2.4357940377106453
Validation loss: 2.4569143082942135

Epoch: 6| Step: 13
Training loss: 1.6829833585944296
Validation loss: 2.42768875620532

Epoch: 206| Step: 0
Training loss: 2.098553968025094
Validation loss: 2.4320692708299396

Epoch: 6| Step: 1
Training loss: 1.4991055046753545
Validation loss: 2.4322435018050648

Epoch: 6| Step: 2
Training loss: 3.023565876130336
Validation loss: 2.4463139883694733

Epoch: 6| Step: 3
Training loss: 1.6462830842443952
Validation loss: 2.409005350669403

Epoch: 6| Step: 4
Training loss: 1.9436753825986277
Validation loss: 2.4407736390578805

Epoch: 6| Step: 5
Training loss: 2.015314122555896
Validation loss: 2.423688721608314

Epoch: 6| Step: 6
Training loss: 2.2314500136711968
Validation loss: 2.4186699286761315

Epoch: 6| Step: 7
Training loss: 2.5024245902636215
Validation loss: 2.4374903133632126

Epoch: 6| Step: 8
Training loss: 2.3317946628707427
Validation loss: 2.422230938158212

Epoch: 6| Step: 9
Training loss: 2.2227434566934803
Validation loss: 2.422521795254384

Epoch: 6| Step: 10
Training loss: 2.751808352212397
Validation loss: 2.422074880300733

Epoch: 6| Step: 11
Training loss: 1.7011680909562537
Validation loss: 2.4617131805302246

Epoch: 6| Step: 12
Training loss: 2.3176597561549652
Validation loss: 2.4618914136833108

Epoch: 6| Step: 13
Training loss: 1.9500803295120133
Validation loss: 2.4224859419110567

Epoch: 207| Step: 0
Training loss: 2.0481633215989783
Validation loss: 2.4512099657872857

Epoch: 6| Step: 1
Training loss: 2.211427991398073
Validation loss: 2.405511565648805

Epoch: 6| Step: 2
Training loss: 2.1280943836132846
Validation loss: 2.4221726482400245

Epoch: 6| Step: 3
Training loss: 2.6883394571491963
Validation loss: 2.4396186594517912

Epoch: 6| Step: 4
Training loss: 2.176549697038687
Validation loss: 2.4179174229675904

Epoch: 6| Step: 5
Training loss: 2.2785960397275256
Validation loss: 2.4154583789066995

Epoch: 6| Step: 6
Training loss: 2.6914175002883063
Validation loss: 2.4142534160775826

Epoch: 6| Step: 7
Training loss: 1.8488439246944666
Validation loss: 2.432214882803716

Epoch: 6| Step: 8
Training loss: 2.2238225843346138
Validation loss: 2.4502702808781325

Epoch: 6| Step: 9
Training loss: 1.8512754579966255
Validation loss: 2.4070311653913703

Epoch: 6| Step: 10
Training loss: 1.6256555555405978
Validation loss: 2.400805108370239

Epoch: 6| Step: 11
Training loss: 1.943001228186847
Validation loss: 2.4281993822529953

Epoch: 6| Step: 12
Training loss: 2.1556035053954483
Validation loss: 2.4321601229391336

Epoch: 6| Step: 13
Training loss: 3.351826121670775
Validation loss: 2.4221807492614875

Epoch: 208| Step: 0
Training loss: 2.4829867340851957
Validation loss: 2.407897106882099

Epoch: 6| Step: 1
Training loss: 1.8627888730705302
Validation loss: 2.402108925781775

Epoch: 6| Step: 2
Training loss: 2.0426181470488607
Validation loss: 2.4210222461104287

Epoch: 6| Step: 3
Training loss: 2.1058009633981865
Validation loss: 2.4562168580322457

Epoch: 6| Step: 4
Training loss: 2.0244200688451057
Validation loss: 2.450046561717218

Epoch: 6| Step: 5
Training loss: 2.5630806055807813
Validation loss: 2.4313342201228165

Epoch: 6| Step: 6
Training loss: 2.2190193764939536
Validation loss: 2.3976788625209298

Epoch: 6| Step: 7
Training loss: 1.9410976110851557
Validation loss: 2.4387691276512777

Epoch: 6| Step: 8
Training loss: 2.8885041575719685
Validation loss: 2.4226375717009874

Epoch: 6| Step: 9
Training loss: 2.029645785166621
Validation loss: 2.450482528178083

Epoch: 6| Step: 10
Training loss: 2.315364197987809
Validation loss: 2.4477040882967938

Epoch: 6| Step: 11
Training loss: 2.2842356335335445
Validation loss: 2.428388548215603

Epoch: 6| Step: 12
Training loss: 1.8043846768052672
Validation loss: 2.445748847024489

Epoch: 6| Step: 13
Training loss: 2.434612128445289
Validation loss: 2.4339833287243398

Epoch: 209| Step: 0
Training loss: 2.063738826580519
Validation loss: 2.4120355205597472

Epoch: 6| Step: 1
Training loss: 1.7119512472341303
Validation loss: 2.4149918217521478

Epoch: 6| Step: 2
Training loss: 2.5315722978527337
Validation loss: 2.392648506326427

Epoch: 6| Step: 3
Training loss: 1.9651478101742168
Validation loss: 2.4127618620670233

Epoch: 6| Step: 4
Training loss: 2.848252975672585
Validation loss: 2.4305054060474043

Epoch: 6| Step: 5
Training loss: 2.055895548130249
Validation loss: 2.380708270142832

Epoch: 6| Step: 6
Training loss: 2.147364122846059
Validation loss: 2.4208556137371398

Epoch: 6| Step: 7
Training loss: 1.9648524866232207
Validation loss: 2.4275807809170056

Epoch: 6| Step: 8
Training loss: 1.8501910188363961
Validation loss: 2.4334550851386605

Epoch: 6| Step: 9
Training loss: 2.366548659736141
Validation loss: 2.436286078133064

Epoch: 6| Step: 10
Training loss: 2.641891446663657
Validation loss: 2.4423958417377447

Epoch: 6| Step: 11
Training loss: 1.9219483152222117
Validation loss: 2.4224066159781

Epoch: 6| Step: 12
Training loss: 2.4091908702428446
Validation loss: 2.4362738654097478

Epoch: 6| Step: 13
Training loss: 1.2176421583396295
Validation loss: 2.4312602982900167

Epoch: 210| Step: 0
Training loss: 2.483611943363392
Validation loss: 2.409438397377709

Epoch: 6| Step: 1
Training loss: 2.745121703793866
Validation loss: 2.403121224568476

Epoch: 6| Step: 2
Training loss: 2.532764311513849
Validation loss: 2.398485723081977

Epoch: 6| Step: 3
Training loss: 2.132163148059127
Validation loss: 2.4314609957248186

Epoch: 6| Step: 4
Training loss: 2.4105056187980223
Validation loss: 2.4455210703487467

Epoch: 6| Step: 5
Training loss: 2.596490630017054
Validation loss: 2.433391366624237

Epoch: 6| Step: 6
Training loss: 2.8597038371791146
Validation loss: 2.4194528355457314

Epoch: 6| Step: 7
Training loss: 1.6158595138141771
Validation loss: 2.4300019226596645

Epoch: 6| Step: 8
Training loss: 1.9135751531997272
Validation loss: 2.4156450234454447

Epoch: 6| Step: 9
Training loss: 1.4004588516193686
Validation loss: 2.419272822695572

Epoch: 6| Step: 10
Training loss: 1.3315649922546111
Validation loss: 2.427967499529219

Epoch: 6| Step: 11
Training loss: 2.0959019775474483
Validation loss: 2.401024397654314

Epoch: 6| Step: 12
Training loss: 2.3383980234686543
Validation loss: 2.462419336227434

Epoch: 6| Step: 13
Training loss: 1.4466834911709987
Validation loss: 2.41498740356655

Epoch: 211| Step: 0
Training loss: 2.1900354133425184
Validation loss: 2.460387330378181

Epoch: 6| Step: 1
Training loss: 1.8837660911191747
Validation loss: 2.4261045796113327

Epoch: 6| Step: 2
Training loss: 1.9945510307433856
Validation loss: 2.3963227045546978

Epoch: 6| Step: 3
Training loss: 2.709724352707757
Validation loss: 2.4417930074201846

Epoch: 6| Step: 4
Training loss: 2.1888520558265565
Validation loss: 2.3994496104519305

Epoch: 6| Step: 5
Training loss: 1.9017453358877472
Validation loss: 2.426650739200811

Epoch: 6| Step: 6
Training loss: 1.9767922731517915
Validation loss: 2.4260983472452526

Epoch: 6| Step: 7
Training loss: 3.189691874532016
Validation loss: 2.426744351742027

Epoch: 6| Step: 8
Training loss: 2.439310233067326
Validation loss: 2.4448679370798208

Epoch: 6| Step: 9
Training loss: 1.3549290882713287
Validation loss: 2.41465303747456

Epoch: 6| Step: 10
Training loss: 1.9573381072972744
Validation loss: 2.4264714093130424

Epoch: 6| Step: 11
Training loss: 2.5254599199575534
Validation loss: 2.4256771129298156

Epoch: 6| Step: 12
Training loss: 2.11286851727145
Validation loss: 2.4429166919103458

Epoch: 6| Step: 13
Training loss: 1.8996477704067478
Validation loss: 2.4120354504114085

Epoch: 212| Step: 0
Training loss: 1.789525401103278
Validation loss: 2.4317049418199588

Epoch: 6| Step: 1
Training loss: 1.88623634631737
Validation loss: 2.418217013897742

Epoch: 6| Step: 2
Training loss: 2.343058573141707
Validation loss: 2.4289528768510977

Epoch: 6| Step: 3
Training loss: 1.8631111883133304
Validation loss: 2.432930009216778

Epoch: 6| Step: 4
Training loss: 2.9978183124907
Validation loss: 2.4345318539587377

Epoch: 6| Step: 5
Training loss: 2.643319645477812
Validation loss: 2.4047992244442424

Epoch: 6| Step: 6
Training loss: 2.0321662596921666
Validation loss: 2.440885624722356

Epoch: 6| Step: 7
Training loss: 2.51989762355599
Validation loss: 2.4323927258062814

Epoch: 6| Step: 8
Training loss: 1.9245819628718137
Validation loss: 2.4326430817833757

Epoch: 6| Step: 9
Training loss: 2.067603761221073
Validation loss: 2.4059314048088094

Epoch: 6| Step: 10
Training loss: 1.7950008404020505
Validation loss: 2.40078906744374

Epoch: 6| Step: 11
Training loss: 1.8203216683480214
Validation loss: 2.407524049669649

Epoch: 6| Step: 12
Training loss: 2.0639324415816245
Validation loss: 2.406362749603

Epoch: 6| Step: 13
Training loss: 2.596817408908913
Validation loss: 2.4284920211328145

Epoch: 213| Step: 0
Training loss: 2.6868794634139297
Validation loss: 2.4002873964321867

Epoch: 6| Step: 1
Training loss: 2.1744645961494125
Validation loss: 2.4326498706748714

Epoch: 6| Step: 2
Training loss: 1.86353164365947
Validation loss: 2.423239831935832

Epoch: 6| Step: 3
Training loss: 2.288199095913435
Validation loss: 2.4291865952208544

Epoch: 6| Step: 4
Training loss: 2.346642299041522
Validation loss: 2.4105134303997917

Epoch: 6| Step: 5
Training loss: 1.9116761571132526
Validation loss: 2.4496993242218053

Epoch: 6| Step: 6
Training loss: 2.3241139107788755
Validation loss: 2.425840698124594

Epoch: 6| Step: 7
Training loss: 1.6590577564038758
Validation loss: 2.4290103602517568

Epoch: 6| Step: 8
Training loss: 1.9763769733674124
Validation loss: 2.418415249217476

Epoch: 6| Step: 9
Training loss: 1.9726384832507466
Validation loss: 2.386950326060431

Epoch: 6| Step: 10
Training loss: 1.8821784253687817
Validation loss: 2.420020627186685

Epoch: 6| Step: 11
Training loss: 2.151205048964233
Validation loss: 2.4241136613076875

Epoch: 6| Step: 12
Training loss: 2.6725085661762433
Validation loss: 2.416004309374606

Epoch: 6| Step: 13
Training loss: 2.6450546400343207
Validation loss: 2.4335139191433335

Epoch: 214| Step: 0
Training loss: 1.7303946933442753
Validation loss: 2.4363601897199403

Epoch: 6| Step: 1
Training loss: 1.7725259059632361
Validation loss: 2.420588751115709

Epoch: 6| Step: 2
Training loss: 2.852613114463745
Validation loss: 2.41261382108893

Epoch: 6| Step: 3
Training loss: 1.6596002343259229
Validation loss: 2.4226349706383528

Epoch: 6| Step: 4
Training loss: 1.6865394825660835
Validation loss: 2.3669915941456705

Epoch: 6| Step: 5
Training loss: 2.504104106542659
Validation loss: 2.401504266679646

Epoch: 6| Step: 6
Training loss: 2.082910978102568
Validation loss: 2.40818631364642

Epoch: 6| Step: 7
Training loss: 2.3065247534865185
Validation loss: 2.416489496692184

Epoch: 6| Step: 8
Training loss: 2.8269656845347213
Validation loss: 2.4287534317055606

Epoch: 6| Step: 9
Training loss: 2.3409954923759906
Validation loss: 2.4564655216811615

Epoch: 6| Step: 10
Training loss: 1.8277459689035733
Validation loss: 2.400406123907722

Epoch: 6| Step: 11
Training loss: 2.737597849590766
Validation loss: 2.390989311888304

Epoch: 6| Step: 12
Training loss: 1.6867119397180066
Validation loss: 2.461614673093271

Epoch: 6| Step: 13
Training loss: 1.9313573369193786
Validation loss: 2.428324936406102

Epoch: 215| Step: 0
Training loss: 2.0569906923994976
Validation loss: 2.404404517274121

Epoch: 6| Step: 1
Training loss: 2.109458865158642
Validation loss: 2.4261144934573813

Epoch: 6| Step: 2
Training loss: 2.6324656453295514
Validation loss: 2.427400257403887

Epoch: 6| Step: 3
Training loss: 2.0629974690069073
Validation loss: 2.4261468405062034

Epoch: 6| Step: 4
Training loss: 2.52934947308968
Validation loss: 2.4371757354742076

Epoch: 6| Step: 5
Training loss: 2.4237368410490037
Validation loss: 2.413621343321935

Epoch: 6| Step: 6
Training loss: 1.6008987912337118
Validation loss: 2.425131485679848

Epoch: 6| Step: 7
Training loss: 2.0728198720547
Validation loss: 2.3960292727122163

Epoch: 6| Step: 8
Training loss: 1.5963857210991255
Validation loss: 2.436001651845864

Epoch: 6| Step: 9
Training loss: 1.997687075261109
Validation loss: 2.409597741423654

Epoch: 6| Step: 10
Training loss: 2.4846042551271514
Validation loss: 2.4360985146373952

Epoch: 6| Step: 11
Training loss: 2.2144769704641467
Validation loss: 2.418730354467778

Epoch: 6| Step: 12
Training loss: 2.4016194562046365
Validation loss: 2.45848401690472

Epoch: 6| Step: 13
Training loss: 2.075450811477308
Validation loss: 2.426464016759072

Epoch: 216| Step: 0
Training loss: 2.6518385213536213
Validation loss: 2.4044590392317504

Epoch: 6| Step: 1
Training loss: 2.079111653774242
Validation loss: 2.412123296852849

Epoch: 6| Step: 2
Training loss: 2.3960027109617967
Validation loss: 2.4173556743419518

Epoch: 6| Step: 3
Training loss: 2.1348893669820295
Validation loss: 2.411315270190333

Epoch: 6| Step: 4
Training loss: 1.936436022595245
Validation loss: 2.422800088132882

Epoch: 6| Step: 5
Training loss: 1.88784478430066
Validation loss: 2.4234693261871922

Epoch: 6| Step: 6
Training loss: 1.7374914539593531
Validation loss: 2.409244176703637

Epoch: 6| Step: 7
Training loss: 1.7802154481350183
Validation loss: 2.418546457564144

Epoch: 6| Step: 8
Training loss: 2.0485269417866063
Validation loss: 2.4195866629579386

Epoch: 6| Step: 9
Training loss: 1.5360212146068857
Validation loss: 2.401406052269447

Epoch: 6| Step: 10
Training loss: 2.2334980344061295
Validation loss: 2.426731507857047

Epoch: 6| Step: 11
Training loss: 3.085230029168934
Validation loss: 2.4186319020345626

Epoch: 6| Step: 12
Training loss: 2.098368774106686
Validation loss: 2.4045417591426195

Epoch: 6| Step: 13
Training loss: 2.633159852248837
Validation loss: 2.385000119879918

Epoch: 217| Step: 0
Training loss: 2.5390548001686133
Validation loss: 2.400416774008741

Epoch: 6| Step: 1
Training loss: 2.070008107966517
Validation loss: 2.410653630787245

Epoch: 6| Step: 2
Training loss: 1.8613416385060013
Validation loss: 2.399296308484727

Epoch: 6| Step: 3
Training loss: 1.4137955724519773
Validation loss: 2.42790427740478

Epoch: 6| Step: 4
Training loss: 2.29103498133607
Validation loss: 2.41015462150179

Epoch: 6| Step: 5
Training loss: 2.4044404595123257
Validation loss: 2.414434685513338

Epoch: 6| Step: 6
Training loss: 1.9091779391775883
Validation loss: 2.400098521507442

Epoch: 6| Step: 7
Training loss: 1.5820146724632993
Validation loss: 2.388239917458318

Epoch: 6| Step: 8
Training loss: 2.20770987820899
Validation loss: 2.425445411328965

Epoch: 6| Step: 9
Training loss: 2.624386125265271
Validation loss: 2.428777241351122

Epoch: 6| Step: 10
Training loss: 2.3692623902782097
Validation loss: 2.4075534945914394

Epoch: 6| Step: 11
Training loss: 1.9432858247113438
Validation loss: 2.3727609575335022

Epoch: 6| Step: 12
Training loss: 2.651930044997796
Validation loss: 2.42603751056508

Epoch: 6| Step: 13
Training loss: 1.9660684397709254
Validation loss: 2.4254119038615194

Epoch: 218| Step: 0
Training loss: 1.5057247116496224
Validation loss: 2.4098005517795924

Epoch: 6| Step: 1
Training loss: 1.8380506974825568
Validation loss: 2.4320437478083856

Epoch: 6| Step: 2
Training loss: 2.3558140844749085
Validation loss: 2.4231322244251885

Epoch: 6| Step: 3
Training loss: 2.2280754436958126
Validation loss: 2.420038898727952

Epoch: 6| Step: 4
Training loss: 2.9397008240216245
Validation loss: 2.396704215784007

Epoch: 6| Step: 5
Training loss: 1.890339412084573
Validation loss: 2.4483716372171105

Epoch: 6| Step: 6
Training loss: 2.021218043672161
Validation loss: 2.3963929127862804

Epoch: 6| Step: 7
Training loss: 1.888148680118285
Validation loss: 2.430352457548921

Epoch: 6| Step: 8
Training loss: 2.5633859731017345
Validation loss: 2.4363814091406737

Epoch: 6| Step: 9
Training loss: 2.124490789101255
Validation loss: 2.440795666675657

Epoch: 6| Step: 10
Training loss: 2.5055175928743996
Validation loss: 2.402153557586031

Epoch: 6| Step: 11
Training loss: 1.990829064465887
Validation loss: 2.405481422178666

Epoch: 6| Step: 12
Training loss: 2.016568458414726
Validation loss: 2.4181522630608527

Epoch: 6| Step: 13
Training loss: 2.015117374343031
Validation loss: 2.4108040018744803

Epoch: 219| Step: 0
Training loss: 1.6694476608750342
Validation loss: 2.388495112871059

Epoch: 6| Step: 1
Training loss: 2.747885758286629
Validation loss: 2.4101758695408737

Epoch: 6| Step: 2
Training loss: 2.4607125315698504
Validation loss: 2.4021932335178695

Epoch: 6| Step: 3
Training loss: 2.482640266566276
Validation loss: 2.403077872779507

Epoch: 6| Step: 4
Training loss: 2.2697470163543585
Validation loss: 2.420734519912245

Epoch: 6| Step: 5
Training loss: 2.106586676157385
Validation loss: 2.371872182395816

Epoch: 6| Step: 6
Training loss: 2.3921161065389542
Validation loss: 2.435514499116138

Epoch: 6| Step: 7
Training loss: 1.6675016298529168
Validation loss: 2.4143404725094757

Epoch: 6| Step: 8
Training loss: 1.9640988099983696
Validation loss: 2.449253479847305

Epoch: 6| Step: 9
Training loss: 2.042928720815119
Validation loss: 2.421599796893157

Epoch: 6| Step: 10
Training loss: 1.9396191359786124
Validation loss: 2.3685039470286022

Epoch: 6| Step: 11
Training loss: 2.019617429329637
Validation loss: 2.3929165120146454

Epoch: 6| Step: 12
Training loss: 2.053152577114845
Validation loss: 2.3950988704876757

Epoch: 6| Step: 13
Training loss: 2.5494235340837617
Validation loss: 2.4010391920459258

Epoch: 220| Step: 0
Training loss: 1.8697427798143376
Validation loss: 2.4029084453095986

Epoch: 6| Step: 1
Training loss: 2.54387614619265
Validation loss: 2.420134125039142

Epoch: 6| Step: 2
Training loss: 1.5916455904911686
Validation loss: 2.3992034286636135

Epoch: 6| Step: 3
Training loss: 2.4222122449795775
Validation loss: 2.409817683255556

Epoch: 6| Step: 4
Training loss: 2.2572312184261274
Validation loss: 2.3871296092848433

Epoch: 6| Step: 5
Training loss: 2.1901444935270495
Validation loss: 2.416732225670999

Epoch: 6| Step: 6
Training loss: 1.662767147886014
Validation loss: 2.4193180968458248

Epoch: 6| Step: 7
Training loss: 2.394528163391414
Validation loss: 2.396234216808352

Epoch: 6| Step: 8
Training loss: 1.7248132051696674
Validation loss: 2.4141635955245384

Epoch: 6| Step: 9
Training loss: 1.6060293290962524
Validation loss: 2.421825257848054

Epoch: 6| Step: 10
Training loss: 2.702946298678995
Validation loss: 2.4174905743796793

Epoch: 6| Step: 11
Training loss: 2.626932794995682
Validation loss: 2.4000040117096955

Epoch: 6| Step: 12
Training loss: 1.9667006225671848
Validation loss: 2.417000681362017

Epoch: 6| Step: 13
Training loss: 1.8642072316057612
Validation loss: 2.394261185064798

Epoch: 221| Step: 0
Training loss: 1.9445724611677955
Validation loss: 2.41853665049545

Epoch: 6| Step: 1
Training loss: 1.6499605781007312
Validation loss: 2.4207851300978924

Epoch: 6| Step: 2
Training loss: 2.1098442403423667
Validation loss: 2.4276231945825746

Epoch: 6| Step: 3
Training loss: 2.449781435345434
Validation loss: 2.4523066398039597

Epoch: 6| Step: 4
Training loss: 1.56777886351372
Validation loss: 2.405246566415789

Epoch: 6| Step: 5
Training loss: 1.8189717868716206
Validation loss: 2.399311435147466

Epoch: 6| Step: 6
Training loss: 2.472512962685327
Validation loss: 2.420285288345164

Epoch: 6| Step: 7
Training loss: 2.0668448720004484
Validation loss: 2.4151619146905032

Epoch: 6| Step: 8
Training loss: 2.330200294431878
Validation loss: 2.4005342722877265

Epoch: 6| Step: 9
Training loss: 1.8583425011287518
Validation loss: 2.409828880590942

Epoch: 6| Step: 10
Training loss: 2.2455778109876743
Validation loss: 2.403074804616274

Epoch: 6| Step: 11
Training loss: 2.6617878350959794
Validation loss: 2.438313378194276

Epoch: 6| Step: 12
Training loss: 2.7197248092453954
Validation loss: 2.3844198152050686

Epoch: 6| Step: 13
Training loss: 1.5006245266660605
Validation loss: 2.4210163892813434

Epoch: 222| Step: 0
Training loss: 2.0367556563861067
Validation loss: 2.374243747013636

Epoch: 6| Step: 1
Training loss: 1.669150155198922
Validation loss: 2.39585488743514

Epoch: 6| Step: 2
Training loss: 3.004360685463204
Validation loss: 2.3931979379613018

Epoch: 6| Step: 3
Training loss: 2.0353154506395468
Validation loss: 2.3607438323180236

Epoch: 6| Step: 4
Training loss: 1.6427655016768996
Validation loss: 2.3873194962967874

Epoch: 6| Step: 5
Training loss: 2.4966726094554192
Validation loss: 2.4232136583763624

Epoch: 6| Step: 6
Training loss: 1.7624353951592056
Validation loss: 2.3835810209556816

Epoch: 6| Step: 7
Training loss: 2.4887077888193065
Validation loss: 2.4019264358286145

Epoch: 6| Step: 8
Training loss: 1.9616793866691784
Validation loss: 2.4065561300917846

Epoch: 6| Step: 9
Training loss: 2.280596391242816
Validation loss: 2.411020210471136

Epoch: 6| Step: 10
Training loss: 2.0120936960962124
Validation loss: 2.3767542014189127

Epoch: 6| Step: 11
Training loss: 2.4135013656533317
Validation loss: 2.4133012373549727

Epoch: 6| Step: 12
Training loss: 1.8282786247717724
Validation loss: 2.3830396425947846

Epoch: 6| Step: 13
Training loss: 1.793601728418761
Validation loss: 2.4029518647009387

Epoch: 223| Step: 0
Training loss: 1.872038728052551
Validation loss: 2.4128820443785175

Epoch: 6| Step: 1
Training loss: 2.188777659452209
Validation loss: 2.393865148374616

Epoch: 6| Step: 2
Training loss: 1.6483361090986115
Validation loss: 2.4085460991103833

Epoch: 6| Step: 3
Training loss: 1.938190029693111
Validation loss: 2.392641998235349

Epoch: 6| Step: 4
Training loss: 2.1794585514489904
Validation loss: 2.4238625480908076

Epoch: 6| Step: 5
Training loss: 2.960613464500498
Validation loss: 2.4223002240100664

Epoch: 6| Step: 6
Training loss: 2.4119522105301647
Validation loss: 2.437773551136831

Epoch: 6| Step: 7
Training loss: 2.2979524122578048
Validation loss: 2.420994629094083

Epoch: 6| Step: 8
Training loss: 1.7350381494115137
Validation loss: 2.387854310787234

Epoch: 6| Step: 9
Training loss: 1.3266466776781258
Validation loss: 2.4060660991010825

Epoch: 6| Step: 10
Training loss: 2.3475052313700635
Validation loss: 2.4190304397850255

Epoch: 6| Step: 11
Training loss: 2.7453326585493083
Validation loss: 2.3842526913433932

Epoch: 6| Step: 12
Training loss: 2.2314429619106875
Validation loss: 2.3894215326086314

Epoch: 6| Step: 13
Training loss: 1.7458138124137825
Validation loss: 2.403776394566237

Epoch: 224| Step: 0
Training loss: 2.4278127422865396
Validation loss: 2.3898854745530116

Epoch: 6| Step: 1
Training loss: 2.3443225415594
Validation loss: 2.4031893364081993

Epoch: 6| Step: 2
Training loss: 2.146068473697619
Validation loss: 2.414317013188649

Epoch: 6| Step: 3
Training loss: 2.141386022127762
Validation loss: 2.398635778568216

Epoch: 6| Step: 4
Training loss: 1.873605336616402
Validation loss: 2.4187725740925563

Epoch: 6| Step: 5
Training loss: 2.7028529741421568
Validation loss: 2.423028072959982

Epoch: 6| Step: 6
Training loss: 1.3803695392633624
Validation loss: 2.429941691175865

Epoch: 6| Step: 7
Training loss: 1.7940954852597708
Validation loss: 2.438274622261867

Epoch: 6| Step: 8
Training loss: 2.670539209731872
Validation loss: 2.389430596021219

Epoch: 6| Step: 9
Training loss: 2.270153072086357
Validation loss: 2.464579528903297

Epoch: 6| Step: 10
Training loss: 2.4085456499363156
Validation loss: 2.3858738608108756

Epoch: 6| Step: 11
Training loss: 2.1455467057381683
Validation loss: 2.4128750521828124

Epoch: 6| Step: 12
Training loss: 1.5987064854931354
Validation loss: 2.396048051397847

Epoch: 6| Step: 13
Training loss: 1.5070471525959979
Validation loss: 2.4304289013570077

Epoch: 225| Step: 0
Training loss: 2.5617246617850737
Validation loss: 2.4292094191552804

Epoch: 6| Step: 1
Training loss: 2.187200362255845
Validation loss: 2.3613880243207768

Epoch: 6| Step: 2
Training loss: 1.961428151545898
Validation loss: 2.3874484544930272

Epoch: 6| Step: 3
Training loss: 2.150711465265327
Validation loss: 2.416372865491665

Epoch: 6| Step: 4
Training loss: 1.8975982694849547
Validation loss: 2.414365671938781

Epoch: 6| Step: 5
Training loss: 1.5473932496259757
Validation loss: 2.4153565882329437

Epoch: 6| Step: 6
Training loss: 2.59089098624394
Validation loss: 2.4106787315707514

Epoch: 6| Step: 7
Training loss: 1.671463104562021
Validation loss: 2.4062679904307296

Epoch: 6| Step: 8
Training loss: 2.264177583516741
Validation loss: 2.4118394936933436

Epoch: 6| Step: 9
Training loss: 1.8543769274303987
Validation loss: 2.4081396325691333

Epoch: 6| Step: 10
Training loss: 2.177791084953878
Validation loss: 2.396382547554559

Epoch: 6| Step: 11
Training loss: 2.5121290663770592
Validation loss: 2.3707510132213443

Epoch: 6| Step: 12
Training loss: 1.9683962307322398
Validation loss: 2.382892074614563

Epoch: 6| Step: 13
Training loss: 2.2938162230201082
Validation loss: 2.3884240971607613

Epoch: 226| Step: 0
Training loss: 1.8218271396429195
Validation loss: 2.433743778151727

Epoch: 6| Step: 1
Training loss: 1.7365656584044575
Validation loss: 2.417098161353185

Epoch: 6| Step: 2
Training loss: 1.9759764273623557
Validation loss: 2.4032701361725177

Epoch: 6| Step: 3
Training loss: 2.5772178209800862
Validation loss: 2.3988555023409264

Epoch: 6| Step: 4
Training loss: 1.769710169624579
Validation loss: 2.375645489886669

Epoch: 6| Step: 5
Training loss: 2.5588401151665088
Validation loss: 2.373719329401698

Epoch: 6| Step: 6
Training loss: 2.0805364399354245
Validation loss: 2.4142428248912493

Epoch: 6| Step: 7
Training loss: 2.545848428917649
Validation loss: 2.3775924594092546

Epoch: 6| Step: 8
Training loss: 1.4270348459438844
Validation loss: 2.382845298236081

Epoch: 6| Step: 9
Training loss: 1.7863334263619262
Validation loss: 2.3963608338053186

Epoch: 6| Step: 10
Training loss: 2.609593730599025
Validation loss: 2.3920058095688397

Epoch: 6| Step: 11
Training loss: 1.901441669621667
Validation loss: 2.38718494247191

Epoch: 6| Step: 12
Training loss: 2.2863512875836896
Validation loss: 2.404489087833699

Epoch: 6| Step: 13
Training loss: 2.0042129490108853
Validation loss: 2.3945938664509354

Epoch: 227| Step: 0
Training loss: 2.5361853628255555
Validation loss: 2.3851370390755506

Epoch: 6| Step: 1
Training loss: 1.8004549934929643
Validation loss: 2.3982205632357636

Epoch: 6| Step: 2
Training loss: 1.959930222008792
Validation loss: 2.4122021096687134

Epoch: 6| Step: 3
Training loss: 1.8346602521902513
Validation loss: 2.3926873953911016

Epoch: 6| Step: 4
Training loss: 1.5867030358487242
Validation loss: 2.408552274713127

Epoch: 6| Step: 5
Training loss: 1.7711058762197909
Validation loss: 2.4085683810193084

Epoch: 6| Step: 6
Training loss: 1.9014761510973885
Validation loss: 2.3733151206804877

Epoch: 6| Step: 7
Training loss: 2.6318438188367614
Validation loss: 2.3682099342956744

Epoch: 6| Step: 8
Training loss: 2.7669487495322067
Validation loss: 2.3794636589518228

Epoch: 6| Step: 9
Training loss: 2.3030934138830084
Validation loss: 2.3888679174215115

Epoch: 6| Step: 10
Training loss: 2.51201289754946
Validation loss: 2.4148997982577867

Epoch: 6| Step: 11
Training loss: 2.185456765833996
Validation loss: 2.392975297098354

Epoch: 6| Step: 12
Training loss: 2.044519364785621
Validation loss: 2.3740322524760127

Epoch: 6| Step: 13
Training loss: 1.9063359225560657
Validation loss: 2.372118184317077

Epoch: 228| Step: 0
Training loss: 2.181512103141384
Validation loss: 2.4319173462428063

Epoch: 6| Step: 1
Training loss: 1.9493288303398033
Validation loss: 2.425506807189772

Epoch: 6| Step: 2
Training loss: 2.532849501116558
Validation loss: 2.429785165260464

Epoch: 6| Step: 3
Training loss: 1.99721667212769
Validation loss: 2.3972242907167094

Epoch: 6| Step: 4
Training loss: 2.01630703527919
Validation loss: 2.414653422872156

Epoch: 6| Step: 5
Training loss: 2.2408107026209216
Validation loss: 2.4459881441954927

Epoch: 6| Step: 6
Training loss: 2.176326334149735
Validation loss: 2.473721223013484

Epoch: 6| Step: 7
Training loss: 2.7641150884116357
Validation loss: 2.4471248811494277

Epoch: 6| Step: 8
Training loss: 2.2159659958581956
Validation loss: 2.407791735828079

Epoch: 6| Step: 9
Training loss: 1.574379205217307
Validation loss: 2.4317276345843024

Epoch: 6| Step: 10
Training loss: 2.049799686042314
Validation loss: 2.449855675505319

Epoch: 6| Step: 11
Training loss: 2.063784921536656
Validation loss: 2.438134456086164

Epoch: 6| Step: 12
Training loss: 1.8721239284266016
Validation loss: 2.4362604919550126

Epoch: 6| Step: 13
Training loss: 2.4636642608355563
Validation loss: 2.4065026953763824

Epoch: 229| Step: 0
Training loss: 1.8702991048513247
Validation loss: 2.41557232566729

Epoch: 6| Step: 1
Training loss: 2.2822420236425365
Validation loss: 2.4084112845879266

Epoch: 6| Step: 2
Training loss: 1.38194484753286
Validation loss: 2.4050807575854303

Epoch: 6| Step: 3
Training loss: 1.5936250824685523
Validation loss: 2.3915335579975894

Epoch: 6| Step: 4
Training loss: 2.0763673161038434
Validation loss: 2.405000822387634

Epoch: 6| Step: 5
Training loss: 2.2937991768453245
Validation loss: 2.377637659247226

Epoch: 6| Step: 6
Training loss: 2.400073558951664
Validation loss: 2.388404472825891

Epoch: 6| Step: 7
Training loss: 3.0337438647911927
Validation loss: 2.386737570790721

Epoch: 6| Step: 8
Training loss: 1.90523958787124
Validation loss: 2.392181135052212

Epoch: 6| Step: 9
Training loss: 2.6725854654090617
Validation loss: 2.394502759477353

Epoch: 6| Step: 10
Training loss: 1.7612656647179827
Validation loss: 2.3735075829149537

Epoch: 6| Step: 11
Training loss: 1.9978150114810123
Validation loss: 2.389897515093084

Epoch: 6| Step: 12
Training loss: 1.648842567465547
Validation loss: 2.36895910813004

Epoch: 6| Step: 13
Training loss: 2.6412592921745746
Validation loss: 2.379646535650146

Epoch: 230| Step: 0
Training loss: 1.7370131764282761
Validation loss: 2.396011158325836

Epoch: 6| Step: 1
Training loss: 2.382227791509634
Validation loss: 2.3665907188868873

Epoch: 6| Step: 2
Training loss: 2.239312751452087
Validation loss: 2.373722856171764

Epoch: 6| Step: 3
Training loss: 2.192687504926932
Validation loss: 2.3736660054685563

Epoch: 6| Step: 4
Training loss: 2.279890203843872
Validation loss: 2.38802705915306

Epoch: 6| Step: 5
Training loss: 2.033329530097448
Validation loss: 2.401911834218925

Epoch: 6| Step: 6
Training loss: 1.5340656162835657
Validation loss: 2.379612312181883

Epoch: 6| Step: 7
Training loss: 1.8635455889702324
Validation loss: 2.3998395968743007

Epoch: 6| Step: 8
Training loss: 2.412588317519494
Validation loss: 2.389832076341081

Epoch: 6| Step: 9
Training loss: 2.081803586726196
Validation loss: 2.376184514613065

Epoch: 6| Step: 10
Training loss: 2.1785085901314716
Validation loss: 2.385646962814795

Epoch: 6| Step: 11
Training loss: 2.061200744151435
Validation loss: 2.4099223423491343

Epoch: 6| Step: 12
Training loss: 2.6508306532982866
Validation loss: 2.4001084252787734

Epoch: 6| Step: 13
Training loss: 1.3482439611700405
Validation loss: 2.3719637146659913

Epoch: 231| Step: 0
Training loss: 2.242498183573962
Validation loss: 2.3878905703107884

Epoch: 6| Step: 1
Training loss: 2.5347151409265085
Validation loss: 2.3990959697764285

Epoch: 6| Step: 2
Training loss: 1.828308096299723
Validation loss: 2.364390534276582

Epoch: 6| Step: 3
Training loss: 1.6544633261431
Validation loss: 2.367882429637253

Epoch: 6| Step: 4
Training loss: 1.3658835596218037
Validation loss: 2.3916677503342894

Epoch: 6| Step: 5
Training loss: 2.0357484742929164
Validation loss: 2.3803609177692953

Epoch: 6| Step: 6
Training loss: 2.5839643528076794
Validation loss: 2.419914891492132

Epoch: 6| Step: 7
Training loss: 1.918344744403395
Validation loss: 2.399374506223557

Epoch: 6| Step: 8
Training loss: 2.0911427676383063
Validation loss: 2.41615671039739

Epoch: 6| Step: 9
Training loss: 2.388117853077605
Validation loss: 2.369459618788946

Epoch: 6| Step: 10
Training loss: 2.764216091100173
Validation loss: 2.3846306722806885

Epoch: 6| Step: 11
Training loss: 2.163947477623495
Validation loss: 2.4046616443868034

Epoch: 6| Step: 12
Training loss: 1.7457629408866817
Validation loss: 2.3839985021301007

Epoch: 6| Step: 13
Training loss: 1.8028082372674852
Validation loss: 2.379682708277126

Epoch: 232| Step: 0
Training loss: 2.3503946338205384
Validation loss: 2.389837013031879

Epoch: 6| Step: 1
Training loss: 1.2755111941119652
Validation loss: 2.376671707371073

Epoch: 6| Step: 2
Training loss: 1.8627880411345377
Validation loss: 2.3837393676913714

Epoch: 6| Step: 3
Training loss: 2.786221978451826
Validation loss: 2.380382110814214

Epoch: 6| Step: 4
Training loss: 2.4261203105055125
Validation loss: 2.3854021314740654

Epoch: 6| Step: 5
Training loss: 1.6669660140465024
Validation loss: 2.3889216725401305

Epoch: 6| Step: 6
Training loss: 1.7055402655991314
Validation loss: 2.3682821022675604

Epoch: 6| Step: 7
Training loss: 2.4471647907012515
Validation loss: 2.375525809794252

Epoch: 6| Step: 8
Training loss: 1.9042459177877675
Validation loss: 2.4091358403041174

Epoch: 6| Step: 9
Training loss: 2.010788074505245
Validation loss: 2.3665914728379542

Epoch: 6| Step: 10
Training loss: 1.9862320988342357
Validation loss: 2.4215874349379947

Epoch: 6| Step: 11
Training loss: 1.8963461451337664
Validation loss: 2.4209547222904475

Epoch: 6| Step: 12
Training loss: 2.7819232768675857
Validation loss: 2.398635065684429

Epoch: 6| Step: 13
Training loss: 1.247197490471474
Validation loss: 2.4013308725791926

Epoch: 233| Step: 0
Training loss: 2.741527597519793
Validation loss: 2.3905802376781806

Epoch: 6| Step: 1
Training loss: 2.51354392080974
Validation loss: 2.4171132667619877

Epoch: 6| Step: 2
Training loss: 1.9367809807311145
Validation loss: 2.380353777271071

Epoch: 6| Step: 3
Training loss: 1.9888451157032294
Validation loss: 2.365082124445676

Epoch: 6| Step: 4
Training loss: 1.810863018725909
Validation loss: 2.3795984678427646

Epoch: 6| Step: 5
Training loss: 2.2829183461261477
Validation loss: 2.37064018722137

Epoch: 6| Step: 6
Training loss: 2.3406508236773265
Validation loss: 2.3783983971284535

Epoch: 6| Step: 7
Training loss: 1.6858194247979594
Validation loss: 2.3970094912079083

Epoch: 6| Step: 8
Training loss: 1.708322974693093
Validation loss: 2.4176870878434973

Epoch: 6| Step: 9
Training loss: 2.182594138755538
Validation loss: 2.4102586293779673

Epoch: 6| Step: 10
Training loss: 1.1173163386393825
Validation loss: 2.3921260037058296

Epoch: 6| Step: 11
Training loss: 1.76011513680208
Validation loss: 2.3807431239508756

Epoch: 6| Step: 12
Training loss: 2.3526855017791863
Validation loss: 2.372562464775613

Epoch: 6| Step: 13
Training loss: 2.6956323406244245
Validation loss: 2.384044310664847

Epoch: 234| Step: 0
Training loss: 2.211699122076011
Validation loss: 2.369305593644139

Epoch: 6| Step: 1
Training loss: 2.1893822337778928
Validation loss: 2.4048875867917197

Epoch: 6| Step: 2
Training loss: 2.0085091773752457
Validation loss: 2.370785401372661

Epoch: 6| Step: 3
Training loss: 2.335691713396547
Validation loss: 2.4085722841107327

Epoch: 6| Step: 4
Training loss: 2.3332119864744665
Validation loss: 2.366024884539327

Epoch: 6| Step: 5
Training loss: 1.4395028963632917
Validation loss: 2.3874315420931187

Epoch: 6| Step: 6
Training loss: 2.4251688554317115
Validation loss: 2.3729942284540786

Epoch: 6| Step: 7
Training loss: 2.2388295405840957
Validation loss: 2.3951338793402575

Epoch: 6| Step: 8
Training loss: 1.3295668012658561
Validation loss: 2.391990500592635

Epoch: 6| Step: 9
Training loss: 2.130103993988913
Validation loss: 2.3804782773506137

Epoch: 6| Step: 10
Training loss: 2.1009214196305397
Validation loss: 2.3612105361502684

Epoch: 6| Step: 11
Training loss: 1.911915972926247
Validation loss: 2.3682272670419096

Epoch: 6| Step: 12
Training loss: 2.055090454969486
Validation loss: 2.392799062868214

Epoch: 6| Step: 13
Training loss: 2.286190796466717
Validation loss: 2.387105597951157

Epoch: 235| Step: 0
Training loss: 2.138422257300648
Validation loss: 2.356947730022524

Epoch: 6| Step: 1
Training loss: 1.7725688134355342
Validation loss: 2.3965000949337387

Epoch: 6| Step: 2
Training loss: 2.2172976831394378
Validation loss: 2.3622592724165234

Epoch: 6| Step: 3
Training loss: 2.0245078065608153
Validation loss: 2.346030167315673

Epoch: 6| Step: 4
Training loss: 2.1538017859185934
Validation loss: 2.365561927113557

Epoch: 6| Step: 5
Training loss: 2.3287866663439374
Validation loss: 2.4147632034743043

Epoch: 6| Step: 6
Training loss: 2.439615261043783
Validation loss: 2.3719628273213234

Epoch: 6| Step: 7
Training loss: 1.5903333055481155
Validation loss: 2.3778967923540555

Epoch: 6| Step: 8
Training loss: 1.5553668660815196
Validation loss: 2.3754880400475007

Epoch: 6| Step: 9
Training loss: 2.7721186551216976
Validation loss: 2.3844224353765484

Epoch: 6| Step: 10
Training loss: 2.297691563370885
Validation loss: 2.3526881725482403

Epoch: 6| Step: 11
Training loss: 2.2533719968394337
Validation loss: 2.3810427291416816

Epoch: 6| Step: 12
Training loss: 1.4183176835597915
Validation loss: 2.3760174146400526

Epoch: 6| Step: 13
Training loss: 1.522458901688424
Validation loss: 2.3893976183384655

Epoch: 236| Step: 0
Training loss: 2.132328411680816
Validation loss: 2.3773222824255975

Epoch: 6| Step: 1
Training loss: 1.8575784033636222
Validation loss: 2.3716357765915554

Epoch: 6| Step: 2
Training loss: 2.014696839010499
Validation loss: 2.3430521264435415

Epoch: 6| Step: 3
Training loss: 2.5419936881718583
Validation loss: 2.3758241317022444

Epoch: 6| Step: 4
Training loss: 2.0657152488026376
Validation loss: 2.3595164304906375

Epoch: 6| Step: 5
Training loss: 2.5209410988548036
Validation loss: 2.376931181988618

Epoch: 6| Step: 6
Training loss: 2.551382557678766
Validation loss: 2.363603098632232

Epoch: 6| Step: 7
Training loss: 1.3229846886796206
Validation loss: 2.392276384162728

Epoch: 6| Step: 8
Training loss: 2.289133116378231
Validation loss: 2.3659382358444305

Epoch: 6| Step: 9
Training loss: 2.250442779206823
Validation loss: 2.3568710799860146

Epoch: 6| Step: 10
Training loss: 2.4966369897413845
Validation loss: 2.3727155286563497

Epoch: 6| Step: 11
Training loss: 1.5972305887344092
Validation loss: 2.35883521957522

Epoch: 6| Step: 12
Training loss: 1.7201440879439647
Validation loss: 2.3826225744359366

Epoch: 6| Step: 13
Training loss: 1.2966147471888338
Validation loss: 2.397165883608311

Epoch: 237| Step: 0
Training loss: 1.7963991281381997
Validation loss: 2.3853500048095264

Epoch: 6| Step: 1
Training loss: 1.5856138002378255
Validation loss: 2.382006825609513

Epoch: 6| Step: 2
Training loss: 1.455873740380981
Validation loss: 2.366443009173407

Epoch: 6| Step: 3
Training loss: 1.9853038266096215
Validation loss: 2.379487693530477

Epoch: 6| Step: 4
Training loss: 2.191647548616327
Validation loss: 2.3585442162376897

Epoch: 6| Step: 5
Training loss: 1.8547263497298467
Validation loss: 2.3870787355357512

Epoch: 6| Step: 6
Training loss: 1.8443128082431295
Validation loss: 2.3945570165491805

Epoch: 6| Step: 7
Training loss: 2.1644251550541207
Validation loss: 2.408284934959467

Epoch: 6| Step: 8
Training loss: 1.8456663097766828
Validation loss: 2.394696254110833

Epoch: 6| Step: 9
Training loss: 1.9187371915209577
Validation loss: 2.3881340134674693

Epoch: 6| Step: 10
Training loss: 2.570058328879478
Validation loss: 2.3677679981577606

Epoch: 6| Step: 11
Training loss: 2.798053770098085
Validation loss: 2.3902960420996093

Epoch: 6| Step: 12
Training loss: 1.9559342589612816
Validation loss: 2.392934277001851

Epoch: 6| Step: 13
Training loss: 2.996975645855432
Validation loss: 2.3645596235816844

Epoch: 238| Step: 0
Training loss: 2.3902036008741163
Validation loss: 2.406213456190536

Epoch: 6| Step: 1
Training loss: 1.4020526126861035
Validation loss: 2.382248286747768

Epoch: 6| Step: 2
Training loss: 2.1580326617140573
Validation loss: 2.398243090733791

Epoch: 6| Step: 3
Training loss: 2.788417880579128
Validation loss: 2.3544350896756456

Epoch: 6| Step: 4
Training loss: 1.9780349976130924
Validation loss: 2.389505260173513

Epoch: 6| Step: 5
Training loss: 2.537511263667321
Validation loss: 2.3611473788476536

Epoch: 6| Step: 6
Training loss: 1.962995991322354
Validation loss: 2.402767492646877

Epoch: 6| Step: 7
Training loss: 1.8440656876645563
Validation loss: 2.3972731156787166

Epoch: 6| Step: 8
Training loss: 2.4497661556828842
Validation loss: 2.3611911628209348

Epoch: 6| Step: 9
Training loss: 2.069846968223205
Validation loss: 2.35987123278523

Epoch: 6| Step: 10
Training loss: 1.9745442335361951
Validation loss: 2.369228160183702

Epoch: 6| Step: 11
Training loss: 1.473000726454919
Validation loss: 2.37360538669098

Epoch: 6| Step: 12
Training loss: 1.9362182068846556
Validation loss: 2.378388644402394

Epoch: 6| Step: 13
Training loss: 1.4161926579798354
Validation loss: 2.370480807571404

Epoch: 239| Step: 0
Training loss: 2.6814318615252803
Validation loss: 2.3977635120597625

Epoch: 6| Step: 1
Training loss: 1.918454732138639
Validation loss: 2.385159071043602

Epoch: 6| Step: 2
Training loss: 2.0795978116959644
Validation loss: 2.3746948828779995

Epoch: 6| Step: 3
Training loss: 1.74174151678033
Validation loss: 2.393268924491666

Epoch: 6| Step: 4
Training loss: 1.9153282495748307
Validation loss: 2.395600703975965

Epoch: 6| Step: 5
Training loss: 1.7270197888484713
Validation loss: 2.400464313338931

Epoch: 6| Step: 6
Training loss: 2.029077865046552
Validation loss: 2.395161512772344

Epoch: 6| Step: 7
Training loss: 2.073555648185755
Validation loss: 2.4027495997739017

Epoch: 6| Step: 8
Training loss: 2.167636715318309
Validation loss: 2.3938214533250983

Epoch: 6| Step: 9
Training loss: 2.096159161013782
Validation loss: 2.380795296564624

Epoch: 6| Step: 10
Training loss: 2.168599025077569
Validation loss: 2.3838386444284243

Epoch: 6| Step: 11
Training loss: 1.8080783711132784
Validation loss: 2.3770519798258523

Epoch: 6| Step: 12
Training loss: 2.187091244245822
Validation loss: 2.383995636311857

Epoch: 6| Step: 13
Training loss: 2.80372343788031
Validation loss: 2.3596728986458873

Epoch: 240| Step: 0
Training loss: 2.2861059057712843
Validation loss: 2.400015037457039

Epoch: 6| Step: 1
Training loss: 2.341443567293647
Validation loss: 2.379150544040406

Epoch: 6| Step: 2
Training loss: 1.7833404321024184
Validation loss: 2.4063326474248687

Epoch: 6| Step: 3
Training loss: 2.2581422558380413
Validation loss: 2.3818654631918927

Epoch: 6| Step: 4
Training loss: 1.8847588573591256
Validation loss: 2.3687689473687175

Epoch: 6| Step: 5
Training loss: 1.8335923821046374
Validation loss: 2.3619042128786094

Epoch: 6| Step: 6
Training loss: 2.15689055285488
Validation loss: 2.3690662566969376

Epoch: 6| Step: 7
Training loss: 2.296695105326311
Validation loss: 2.3377098767305915

Epoch: 6| Step: 8
Training loss: 2.773475496273615
Validation loss: 2.386752253954936

Epoch: 6| Step: 9
Training loss: 2.0143712602439203
Validation loss: 2.3814363316655314

Epoch: 6| Step: 10
Training loss: 2.0762538660462435
Validation loss: 2.3687038827432296

Epoch: 6| Step: 11
Training loss: 1.782691104911618
Validation loss: 2.3735449499804373

Epoch: 6| Step: 12
Training loss: 1.7479878847195545
Validation loss: 2.410185217087043

Epoch: 6| Step: 13
Training loss: 1.5409962854755248
Validation loss: 2.3473961648721677

Epoch: 241| Step: 0
Training loss: 1.963889950355244
Validation loss: 2.372387986272453

Epoch: 6| Step: 1
Training loss: 2.3882969509777894
Validation loss: 2.367050837865927

Epoch: 6| Step: 2
Training loss: 2.177906033032808
Validation loss: 2.3642817202753967

Epoch: 6| Step: 3
Training loss: 2.5645537171460147
Validation loss: 2.3548536478937843

Epoch: 6| Step: 4
Training loss: 1.48037695285572
Validation loss: 2.3791787248953873

Epoch: 6| Step: 5
Training loss: 1.760400790833285
Validation loss: 2.377541265673335

Epoch: 6| Step: 6
Training loss: 1.7947101322841357
Validation loss: 2.3877444310893194

Epoch: 6| Step: 7
Training loss: 1.946641337913618
Validation loss: 2.3677842400536315

Epoch: 6| Step: 8
Training loss: 2.048215587243538
Validation loss: 2.398019987125026

Epoch: 6| Step: 9
Training loss: 1.9381572470001296
Validation loss: 2.4129011073321713

Epoch: 6| Step: 10
Training loss: 2.640208736317203
Validation loss: 2.364994628214373

Epoch: 6| Step: 11
Training loss: 1.6499445385715057
Validation loss: 2.388825766492673

Epoch: 6| Step: 12
Training loss: 1.9875159448907747
Validation loss: 2.3463983221015834

Epoch: 6| Step: 13
Training loss: 2.3081252498920963
Validation loss: 2.3873924670114373

Epoch: 242| Step: 0
Training loss: 2.035741798673869
Validation loss: 2.3921799085198994

Epoch: 6| Step: 1
Training loss: 1.6985993758252218
Validation loss: 2.3822320304709605

Epoch: 6| Step: 2
Training loss: 2.16323208627323
Validation loss: 2.3825393743471968

Epoch: 6| Step: 3
Training loss: 2.162765720337197
Validation loss: 2.3593163676216924

Epoch: 6| Step: 4
Training loss: 2.8156465518986056
Validation loss: 2.371415309114566

Epoch: 6| Step: 5
Training loss: 2.024259775762614
Validation loss: 2.3811197520171117

Epoch: 6| Step: 6
Training loss: 1.6246462216776834
Validation loss: 2.352145486489306

Epoch: 6| Step: 7
Training loss: 2.527427707321193
Validation loss: 2.3631516846439795

Epoch: 6| Step: 8
Training loss: 2.2616957694617272
Validation loss: 2.3813366350403626

Epoch: 6| Step: 9
Training loss: 1.7222087391287944
Validation loss: 2.334108817826963

Epoch: 6| Step: 10
Training loss: 2.3071236875767696
Validation loss: 2.414064320200758

Epoch: 6| Step: 11
Training loss: 1.9683102918994106
Validation loss: 2.337963591250562

Epoch: 6| Step: 12
Training loss: 1.2743808552007414
Validation loss: 2.339444473457689

Epoch: 6| Step: 13
Training loss: 1.8410941570391282
Validation loss: 2.367005796527666

Epoch: 243| Step: 0
Training loss: 1.2975703753604508
Validation loss: 2.3769116342565306

Epoch: 6| Step: 1
Training loss: 2.292489776754932
Validation loss: 2.3599835095786252

Epoch: 6| Step: 2
Training loss: 2.2316970249169015
Validation loss: 2.378397771415963

Epoch: 6| Step: 3
Training loss: 1.981705258889051
Validation loss: 2.378910952882543

Epoch: 6| Step: 4
Training loss: 1.5769432855638816
Validation loss: 2.3494652547480865

Epoch: 6| Step: 5
Training loss: 1.9582280746200695
Validation loss: 2.3734816197163258

Epoch: 6| Step: 6
Training loss: 2.13316683218818
Validation loss: 2.3874435171584394

Epoch: 6| Step: 7
Training loss: 1.436624509158939
Validation loss: 2.3659951893418594

Epoch: 6| Step: 8
Training loss: 2.308396797363566
Validation loss: 2.385232771001256

Epoch: 6| Step: 9
Training loss: 3.025556743240388
Validation loss: 2.386343446065596

Epoch: 6| Step: 10
Training loss: 1.8874447770282385
Validation loss: 2.3443859416166473

Epoch: 6| Step: 11
Training loss: 1.821562047550278
Validation loss: 2.3864604389311386

Epoch: 6| Step: 12
Training loss: 2.2928167289496275
Validation loss: 2.3715309645073144

Epoch: 6| Step: 13
Training loss: 2.3493942453986945
Validation loss: 2.3483350908450467

Epoch: 244| Step: 0
Training loss: 2.0961658717034624
Validation loss: 2.3387105424799466

Epoch: 6| Step: 1
Training loss: 1.8784467170881396
Validation loss: 2.358910003891631

Epoch: 6| Step: 2
Training loss: 2.2289583934804487
Validation loss: 2.3422396589157084

Epoch: 6| Step: 3
Training loss: 2.1588565717498915
Validation loss: 2.381603418496376

Epoch: 6| Step: 4
Training loss: 2.2013078053567523
Validation loss: 2.3856189572331647

Epoch: 6| Step: 5
Training loss: 2.2153821287487614
Validation loss: 2.3551248148201034

Epoch: 6| Step: 6
Training loss: 2.0798986223164264
Validation loss: 2.368885286057637

Epoch: 6| Step: 7
Training loss: 2.027288946980262
Validation loss: 2.3524193805633535

Epoch: 6| Step: 8
Training loss: 1.4971028801547674
Validation loss: 2.3525241407318087

Epoch: 6| Step: 9
Training loss: 2.1747852833314307
Validation loss: 2.3788951673683463

Epoch: 6| Step: 10
Training loss: 1.9766293239228985
Validation loss: 2.3666591346351646

Epoch: 6| Step: 11
Training loss: 1.9011138762951814
Validation loss: 2.3585132192269462

Epoch: 6| Step: 12
Training loss: 2.511067782181071
Validation loss: 2.3458217638559886

Epoch: 6| Step: 13
Training loss: 1.8197642798310911
Validation loss: 2.3796823390313047

Epoch: 245| Step: 0
Training loss: 1.609471512650525
Validation loss: 2.3775715380196756

Epoch: 6| Step: 1
Training loss: 1.5977557834502067
Validation loss: 2.3840737509397782

Epoch: 6| Step: 2
Training loss: 1.5555298023514104
Validation loss: 2.3524514081526413

Epoch: 6| Step: 3
Training loss: 2.3404030871652424
Validation loss: 2.3863400040168274

Epoch: 6| Step: 4
Training loss: 2.1778905975005713
Validation loss: 2.3834657599003357

Epoch: 6| Step: 5
Training loss: 2.681669430949618
Validation loss: 2.382010978864021

Epoch: 6| Step: 6
Training loss: 2.561707629999548
Validation loss: 2.38314016944686

Epoch: 6| Step: 7
Training loss: 2.38146454252621
Validation loss: 2.323526381245529

Epoch: 6| Step: 8
Training loss: 1.722855121784882
Validation loss: 2.3883715459145907

Epoch: 6| Step: 9
Training loss: 1.8575014564641938
Validation loss: 2.388603468860683

Epoch: 6| Step: 10
Training loss: 1.4769522339284207
Validation loss: 2.3168073242383853

Epoch: 6| Step: 11
Training loss: 1.8058981529658582
Validation loss: 2.3532434250862253

Epoch: 6| Step: 12
Training loss: 1.83853809331127
Validation loss: 2.3658215073379942

Epoch: 6| Step: 13
Training loss: 2.875995629446558
Validation loss: 2.362203370554613

Epoch: 246| Step: 0
Training loss: 2.0740835716897994
Validation loss: 2.3205297202754167

Epoch: 6| Step: 1
Training loss: 1.8762788861596993
Validation loss: 2.4005247280366113

Epoch: 6| Step: 2
Training loss: 2.197652526700432
Validation loss: 2.3551763445121288

Epoch: 6| Step: 3
Training loss: 2.725522780361509
Validation loss: 2.3737563399761825

Epoch: 6| Step: 4
Training loss: 1.5840809295238447
Validation loss: 2.3175830113559495

Epoch: 6| Step: 5
Training loss: 2.3148325934963605
Validation loss: 2.3731570209284545

Epoch: 6| Step: 6
Training loss: 1.34707332664397
Validation loss: 2.3534611319812098

Epoch: 6| Step: 7
Training loss: 2.1956411095388457
Validation loss: 2.380162124973307

Epoch: 6| Step: 8
Training loss: 2.1075283302946097
Validation loss: 2.345207039286873

Epoch: 6| Step: 9
Training loss: 2.3559261150011648
Validation loss: 2.331553933954786

Epoch: 6| Step: 10
Training loss: 1.8334002554845772
Validation loss: 2.3537641630019515

Epoch: 6| Step: 11
Training loss: 1.520802101267447
Validation loss: 2.3784786001423877

Epoch: 6| Step: 12
Training loss: 2.119912058967623
Validation loss: 2.3487808550300957

Epoch: 6| Step: 13
Training loss: 2.4245729247312995
Validation loss: 2.3761079143783115

Epoch: 247| Step: 0
Training loss: 1.386129377601929
Validation loss: 2.375629816487765

Epoch: 6| Step: 1
Training loss: 1.6409679236104402
Validation loss: 2.357837164142107

Epoch: 6| Step: 2
Training loss: 2.178581914496815
Validation loss: 2.378609738946508

Epoch: 6| Step: 3
Training loss: 1.958049882165721
Validation loss: 2.3513736242226027

Epoch: 6| Step: 4
Training loss: 2.22274742542839
Validation loss: 2.375028718538337

Epoch: 6| Step: 5
Training loss: 1.4057298334022836
Validation loss: 2.3610019025412776

Epoch: 6| Step: 6
Training loss: 2.2202302534962537
Validation loss: 2.3526654605627306

Epoch: 6| Step: 7
Training loss: 1.9470609597536241
Validation loss: 2.395324147506507

Epoch: 6| Step: 8
Training loss: 2.710202164094714
Validation loss: 2.3669330302166682

Epoch: 6| Step: 9
Training loss: 2.2317260832660404
Validation loss: 2.350811175285325

Epoch: 6| Step: 10
Training loss: 1.6468766764391596
Validation loss: 2.3842761868048985

Epoch: 6| Step: 11
Training loss: 2.5480642526031967
Validation loss: 2.3763108341169983

Epoch: 6| Step: 12
Training loss: 1.9245144467008561
Validation loss: 2.330780709217439

Epoch: 6| Step: 13
Training loss: 2.4406540833521944
Validation loss: 2.388090128747632

Epoch: 248| Step: 0
Training loss: 1.3658582056157589
Validation loss: 2.329292278376025

Epoch: 6| Step: 1
Training loss: 1.7504453773507886
Validation loss: 2.380309449698947

Epoch: 6| Step: 2
Training loss: 1.807016562448754
Validation loss: 2.4139085489521097

Epoch: 6| Step: 3
Training loss: 2.0093508515753444
Validation loss: 2.383369702073805

Epoch: 6| Step: 4
Training loss: 1.8732650359296268
Validation loss: 2.3324025805728135

Epoch: 6| Step: 5
Training loss: 1.3537501452614098
Validation loss: 2.367594668178416

Epoch: 6| Step: 6
Training loss: 2.0990231285474787
Validation loss: 2.38109769882375

Epoch: 6| Step: 7
Training loss: 1.717558708020435
Validation loss: 2.354039582889603

Epoch: 6| Step: 8
Training loss: 2.5743245173999005
Validation loss: 2.39124237255995

Epoch: 6| Step: 9
Training loss: 2.77754822312321
Validation loss: 2.3280844615031238

Epoch: 6| Step: 10
Training loss: 1.8006237565221337
Validation loss: 2.366548032515518

Epoch: 6| Step: 11
Training loss: 2.8505280591075186
Validation loss: 2.3505733514925433

Epoch: 6| Step: 12
Training loss: 1.9947834529325803
Validation loss: 2.3731871293688496

Epoch: 6| Step: 13
Training loss: 1.5468327680275111
Validation loss: 2.372005251968447

Epoch: 249| Step: 0
Training loss: 1.8937687863466193
Validation loss: 2.3818959873267294

Epoch: 6| Step: 1
Training loss: 1.8477676712861308
Validation loss: 2.3702429034294688

Epoch: 6| Step: 2
Training loss: 2.426718710386686
Validation loss: 2.3590249780750687

Epoch: 6| Step: 3
Training loss: 1.833705893144064
Validation loss: 2.3452385794556627

Epoch: 6| Step: 4
Training loss: 2.109934866745118
Validation loss: 2.380828435934146

Epoch: 6| Step: 5
Training loss: 2.2785337816571496
Validation loss: 2.334380783445306

Epoch: 6| Step: 6
Training loss: 2.5761676178059023
Validation loss: 2.363709551283684

Epoch: 6| Step: 7
Training loss: 2.156207319196587
Validation loss: 2.3472142121007478

Epoch: 6| Step: 8
Training loss: 1.5282087681439283
Validation loss: 2.3599692334334743

Epoch: 6| Step: 9
Training loss: 2.724104420894151
Validation loss: 2.3486702718751986

Epoch: 6| Step: 10
Training loss: 1.842759076230804
Validation loss: 2.3602503268124013

Epoch: 6| Step: 11
Training loss: 1.4564804385986891
Validation loss: 2.363552056362321

Epoch: 6| Step: 12
Training loss: 1.462819320509603
Validation loss: 2.3351541291373064

Epoch: 6| Step: 13
Training loss: 1.8081336867819018
Validation loss: 2.35310913219167

Epoch: 250| Step: 0
Training loss: 1.8823684232001479
Validation loss: 2.370699055358888

Epoch: 6| Step: 1
Training loss: 1.8631683891232234
Validation loss: 2.3543950749686293

Epoch: 6| Step: 2
Training loss: 1.9927701210324138
Validation loss: 2.366590845628678

Epoch: 6| Step: 3
Training loss: 2.181526857310884
Validation loss: 2.3634922563578504

Epoch: 6| Step: 4
Training loss: 1.6093921845638965
Validation loss: 2.377429812673214

Epoch: 6| Step: 5
Training loss: 2.20126339875302
Validation loss: 2.3442092073749996

Epoch: 6| Step: 6
Training loss: 2.1445330288016304
Validation loss: 2.39495318342425

Epoch: 6| Step: 7
Training loss: 1.7393628131029675
Validation loss: 2.3915200147515034

Epoch: 6| Step: 8
Training loss: 2.3902198597803466
Validation loss: 2.3240592788086043

Epoch: 6| Step: 9
Training loss: 1.849954349366984
Validation loss: 2.3627693529699285

Epoch: 6| Step: 10
Training loss: 1.7625672861877453
Validation loss: 2.343133098398242

Epoch: 6| Step: 11
Training loss: 1.9916519342726204
Validation loss: 2.3499829331168596

Epoch: 6| Step: 12
Training loss: 2.7574644220292894
Validation loss: 2.374193950671043

Epoch: 6| Step: 13
Training loss: 1.1945179311684675
Validation loss: 2.335558819549765

Testing loss: 2.6475138336066775

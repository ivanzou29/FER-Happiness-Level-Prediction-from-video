Epoch: 1| Step: 0
Training loss: 5.513746249609073
Validation loss: 4.919364642935857

Epoch: 5| Step: 1
Training loss: 5.301698901835858
Validation loss: 4.911635636151879

Epoch: 5| Step: 2
Training loss: 4.999998092651003
Validation loss: 4.902591528222001

Epoch: 5| Step: 3
Training loss: 5.34250773663601
Validation loss: 4.896111306297969

Epoch: 5| Step: 4
Training loss: 4.793748265232534
Validation loss: 4.8898635119933305

Epoch: 5| Step: 5
Training loss: 5.2105582812726565
Validation loss: 4.885844213801969

Epoch: 5| Step: 6
Training loss: 4.8272951270891395
Validation loss: 4.879046310896786

Epoch: 5| Step: 7
Training loss: 4.970383285694763
Validation loss: 4.870852845405676

Epoch: 5| Step: 8
Training loss: 4.663172549097475
Validation loss: 4.866658225974398

Epoch: 5| Step: 9
Training loss: 5.01280099626197
Validation loss: 4.857691397438318

Epoch: 5| Step: 10
Training loss: 3.5760450689646115
Validation loss: 4.854161702927726

Epoch: 2| Step: 0
Training loss: 6.152757922566996
Validation loss: 4.848218058399021

Epoch: 5| Step: 1
Training loss: 5.426684793884094
Validation loss: 4.841496200798322

Epoch: 5| Step: 2
Training loss: 5.602987493901046
Validation loss: 4.833393250724483

Epoch: 5| Step: 3
Training loss: 5.012977923585293
Validation loss: 4.831854878783854

Epoch: 5| Step: 4
Training loss: 3.849847314952855
Validation loss: 4.821555821534948

Epoch: 5| Step: 5
Training loss: 4.159763084781246
Validation loss: 4.8156437688925715

Epoch: 5| Step: 6
Training loss: 5.114101824913729
Validation loss: 4.811317310241955

Epoch: 5| Step: 7
Training loss: 4.615102745887954
Validation loss: 4.807153125197108

Epoch: 5| Step: 8
Training loss: 4.333419701131238
Validation loss: 4.798358935284445

Epoch: 5| Step: 9
Training loss: 3.422457571386663
Validation loss: 4.7941619585655015

Epoch: 5| Step: 10
Training loss: 5.680105915425276
Validation loss: 4.789260392039218

Epoch: 3| Step: 0
Training loss: 5.121493372684873
Validation loss: 4.783444585053442

Epoch: 5| Step: 1
Training loss: 4.57559623793634
Validation loss: 4.774890687388478

Epoch: 5| Step: 2
Training loss: 4.753986492749236
Validation loss: 4.773679838532008

Epoch: 5| Step: 3
Training loss: 4.39731292170531
Validation loss: 4.767010091269246

Epoch: 5| Step: 4
Training loss: 4.818445075811117
Validation loss: 4.76189866213547

Epoch: 5| Step: 5
Training loss: 4.078176213085402
Validation loss: 4.758170944783875

Epoch: 5| Step: 6
Training loss: 5.535476018462781
Validation loss: 4.749923018914912

Epoch: 5| Step: 7
Training loss: 5.669515155602759
Validation loss: 4.745999725921541

Epoch: 5| Step: 8
Training loss: 5.034271659713928
Validation loss: 4.739972936967414

Epoch: 5| Step: 9
Training loss: 4.779533726443607
Validation loss: 4.7358662894222

Epoch: 5| Step: 10
Training loss: 4.159580129928957
Validation loss: 4.73009997430809

Epoch: 4| Step: 0
Training loss: 3.577647481313568
Validation loss: 4.724685835248344

Epoch: 5| Step: 1
Training loss: 4.717131280200758
Validation loss: 4.718297567112715

Epoch: 5| Step: 2
Training loss: 4.760110635192983
Validation loss: 4.712204939547785

Epoch: 5| Step: 3
Training loss: 4.969888232872165
Validation loss: 4.7080363554661195

Epoch: 5| Step: 4
Training loss: 4.849410361401836
Validation loss: 4.701526786063218

Epoch: 5| Step: 5
Training loss: 5.644378174286804
Validation loss: 4.696757113892648

Epoch: 5| Step: 6
Training loss: 5.0600071173455765
Validation loss: 4.691087262662628

Epoch: 5| Step: 7
Training loss: 5.158051332192442
Validation loss: 4.6859349537439

Epoch: 5| Step: 8
Training loss: 4.193745524692564
Validation loss: 4.681821667191544

Epoch: 5| Step: 9
Training loss: 4.022283947952827
Validation loss: 4.676022792115025

Epoch: 5| Step: 10
Training loss: 5.435351944997786
Validation loss: 4.670769012673668

Epoch: 5| Step: 0
Training loss: 4.077238139918905
Validation loss: 4.667400508232201

Epoch: 5| Step: 1
Training loss: 4.249698179410965
Validation loss: 4.658483614381371

Epoch: 5| Step: 2
Training loss: 5.254559308274094
Validation loss: 4.651976516160288

Epoch: 5| Step: 3
Training loss: 4.021679303343979
Validation loss: 4.646901264758203

Epoch: 5| Step: 4
Training loss: 5.493580279370519
Validation loss: 4.6402150557209945

Epoch: 5| Step: 5
Training loss: 4.602581021550465
Validation loss: 4.632939663960826

Epoch: 5| Step: 6
Training loss: 5.666195045741606
Validation loss: 4.626570295616466

Epoch: 5| Step: 7
Training loss: 4.593591596506285
Validation loss: 4.625598743442673

Epoch: 5| Step: 8
Training loss: 4.049440726765236
Validation loss: 4.617656082038278

Epoch: 5| Step: 9
Training loss: 4.763745747236257
Validation loss: 4.612844717921259

Epoch: 5| Step: 10
Training loss: 4.9232297940086305
Validation loss: 4.606318515752168

Epoch: 6| Step: 0
Training loss: 4.8515078530789095
Validation loss: 4.59995910096679

Epoch: 5| Step: 1
Training loss: 4.254702266761351
Validation loss: 4.591919176498536

Epoch: 5| Step: 2
Training loss: 4.814787085907038
Validation loss: 4.587409866617525

Epoch: 5| Step: 3
Training loss: 4.326937185411193
Validation loss: 4.579655203781301

Epoch: 5| Step: 4
Training loss: 4.839774783230371
Validation loss: 4.574271797814208

Epoch: 5| Step: 5
Training loss: 4.5177947114909704
Validation loss: 4.569165447077746

Epoch: 5| Step: 6
Training loss: 5.488942040680713
Validation loss: 4.561298638672281

Epoch: 5| Step: 7
Training loss: 5.077303494248209
Validation loss: 4.555255577047812

Epoch: 5| Step: 8
Training loss: 4.416091857549107
Validation loss: 4.549269312177062

Epoch: 5| Step: 9
Training loss: 4.245060462476138
Validation loss: 4.539504080578232

Epoch: 5| Step: 10
Training loss: 4.24038596213955
Validation loss: 4.533217249535565

Epoch: 7| Step: 0
Training loss: 5.147367096973545
Validation loss: 4.527674409433067

Epoch: 5| Step: 1
Training loss: 4.54991504464118
Validation loss: 4.521265384191413

Epoch: 5| Step: 2
Training loss: 4.962108083474208
Validation loss: 4.512295500050294

Epoch: 5| Step: 3
Training loss: 4.683112978021883
Validation loss: 4.5047697108979445

Epoch: 5| Step: 4
Training loss: 4.227022100199062
Validation loss: 4.497684853980454

Epoch: 5| Step: 5
Training loss: 4.94023848293891
Validation loss: 4.4923846206824

Epoch: 5| Step: 6
Training loss: 2.776099246765696
Validation loss: 4.481785674431457

Epoch: 5| Step: 7
Training loss: 4.94200295927792
Validation loss: 4.47744640255762

Epoch: 5| Step: 8
Training loss: 4.97587180621985
Validation loss: 4.469915156510262

Epoch: 5| Step: 9
Training loss: 3.956787945421354
Validation loss: 4.463437809737834

Epoch: 5| Step: 10
Training loss: 4.9142076625518785
Validation loss: 4.451205544754602

Epoch: 8| Step: 0
Training loss: 4.580627174246802
Validation loss: 4.447515247607068

Epoch: 5| Step: 1
Training loss: 3.989947442686175
Validation loss: 4.440497588016644

Epoch: 5| Step: 2
Training loss: 4.772054368840523
Validation loss: 4.432330340297245

Epoch: 5| Step: 3
Training loss: 4.942988182428809
Validation loss: 4.423463912857597

Epoch: 5| Step: 4
Training loss: 4.646571398670147
Validation loss: 4.414191477498658

Epoch: 5| Step: 5
Training loss: 3.787207961705118
Validation loss: 4.407498260468482

Epoch: 5| Step: 6
Training loss: 3.844438940814813
Validation loss: 4.401740003814349

Epoch: 5| Step: 7
Training loss: 5.216187538999782
Validation loss: 4.388012946916471

Epoch: 5| Step: 8
Training loss: 4.868719114075855
Validation loss: 4.382033808800919

Epoch: 5| Step: 9
Training loss: 4.435507528516039
Validation loss: 4.374948644849378

Epoch: 5| Step: 10
Training loss: 4.32157672506517
Validation loss: 4.366941107647455

Epoch: 9| Step: 0
Training loss: 3.781929522530941
Validation loss: 4.358388095631588

Epoch: 5| Step: 1
Training loss: 5.261777111429305
Validation loss: 4.350296850640254

Epoch: 5| Step: 2
Training loss: 4.034852303563572
Validation loss: 4.3425545577895415

Epoch: 5| Step: 3
Training loss: 4.423459042274681
Validation loss: 4.329604867903592

Epoch: 5| Step: 4
Training loss: 4.898366437819375
Validation loss: 4.323264869319066

Epoch: 5| Step: 5
Training loss: 4.302249834411678
Validation loss: 4.314430766449104

Epoch: 5| Step: 6
Training loss: 3.9823105194552895
Validation loss: 4.303796102421095

Epoch: 5| Step: 7
Training loss: 4.67967975596709
Validation loss: 4.296549738918869

Epoch: 5| Step: 8
Training loss: 4.3670796225775
Validation loss: 4.286719466598563

Epoch: 5| Step: 9
Training loss: 4.37979342944609
Validation loss: 4.277761220045416

Epoch: 5| Step: 10
Training loss: 4.361434614057649
Validation loss: 4.267034946174205

Epoch: 10| Step: 0
Training loss: 4.555175545719323
Validation loss: 4.2557869129993735

Epoch: 5| Step: 1
Training loss: 4.777591477441053
Validation loss: 4.245301012917855

Epoch: 5| Step: 2
Training loss: 4.5874233145891745
Validation loss: 4.238204662148367

Epoch: 5| Step: 3
Training loss: 5.282896248019072
Validation loss: 4.226584049614446

Epoch: 5| Step: 4
Training loss: 4.182108582526872
Validation loss: 4.216090077736359

Epoch: 5| Step: 5
Training loss: 4.118691446585642
Validation loss: 4.2061818139263245

Epoch: 5| Step: 6
Training loss: 4.316572174059216
Validation loss: 4.199398697138013

Epoch: 5| Step: 7
Training loss: 3.263084043645078
Validation loss: 4.1841789842691055

Epoch: 5| Step: 8
Training loss: 4.576804742103258
Validation loss: 4.172543882448915

Epoch: 5| Step: 9
Training loss: 3.781300820253463
Validation loss: 4.165085345216682

Epoch: 5| Step: 10
Training loss: 3.7164928093405325
Validation loss: 4.150799060044323

Epoch: 11| Step: 0
Training loss: 4.766644678309661
Validation loss: 4.148744900234281

Epoch: 5| Step: 1
Training loss: 5.443152109942159
Validation loss: 4.131438339746059

Epoch: 5| Step: 2
Training loss: 3.8720098464304673
Validation loss: 4.124503875559745

Epoch: 5| Step: 3
Training loss: 3.645737230759989
Validation loss: 4.113311216196153

Epoch: 5| Step: 4
Training loss: 5.193362496767113
Validation loss: 4.094228732014118

Epoch: 5| Step: 5
Training loss: 4.125870815027999
Validation loss: 4.0866105692636845

Epoch: 5| Step: 6
Training loss: 2.943313689405729
Validation loss: 4.077870614794458

Epoch: 5| Step: 7
Training loss: 4.292781697329569
Validation loss: 4.063290853328703

Epoch: 5| Step: 8
Training loss: 3.3051431400390907
Validation loss: 4.056349477479254

Epoch: 5| Step: 9
Training loss: 4.154589098869386
Validation loss: 4.043298074693863

Epoch: 5| Step: 10
Training loss: 3.913257752186327
Validation loss: 4.035173120417627

Epoch: 12| Step: 0
Training loss: 4.397018393406278
Validation loss: 4.018260046724277

Epoch: 5| Step: 1
Training loss: 3.996356496814791
Validation loss: 4.009592756581485

Epoch: 5| Step: 2
Training loss: 4.383465668937302
Validation loss: 3.9974568932487964

Epoch: 5| Step: 3
Training loss: 3.5399244324113215
Validation loss: 3.9866562626407602

Epoch: 5| Step: 4
Training loss: 4.087761845499023
Validation loss: 3.9716438373673824

Epoch: 5| Step: 5
Training loss: 3.4666227399048193
Validation loss: 3.9647679957211763

Epoch: 5| Step: 6
Training loss: 4.484184872521121
Validation loss: 3.9464255260012955

Epoch: 5| Step: 7
Training loss: 4.040186948699817
Validation loss: 3.936234285887154

Epoch: 5| Step: 8
Training loss: 4.343009172015745
Validation loss: 3.9221085166463636

Epoch: 5| Step: 9
Training loss: 4.043672804412531
Validation loss: 3.9109797710596217

Epoch: 5| Step: 10
Training loss: 4.149367291121139
Validation loss: 3.896896913775714

Epoch: 13| Step: 0
Training loss: 4.595263173533503
Validation loss: 3.885268352400306

Epoch: 5| Step: 1
Training loss: 3.900576690682733
Validation loss: 3.8697120610728266

Epoch: 5| Step: 2
Training loss: 4.055817727682736
Validation loss: 3.859684489053757

Epoch: 5| Step: 3
Training loss: 3.5674789169319427
Validation loss: 3.8448323519147616

Epoch: 5| Step: 4
Training loss: 3.948614389301385
Validation loss: 3.8302646292085347

Epoch: 5| Step: 5
Training loss: 4.186476625110043
Validation loss: 3.8162898030790715

Epoch: 5| Step: 6
Training loss: 3.7212231530453455
Validation loss: 3.80453492185239

Epoch: 5| Step: 7
Training loss: 3.9924783320785475
Validation loss: 3.7901782729885234

Epoch: 5| Step: 8
Training loss: 4.169185385780519
Validation loss: 3.780428366031114

Epoch: 5| Step: 9
Training loss: 4.2264361406712245
Validation loss: 3.7597302607521996

Epoch: 5| Step: 10
Training loss: 2.8542985723231236
Validation loss: 3.7508632103930535

Epoch: 14| Step: 0
Training loss: 3.5228934195706656
Validation loss: 3.7364598586310174

Epoch: 5| Step: 1
Training loss: 3.262660821334452
Validation loss: 3.719330569046154

Epoch: 5| Step: 2
Training loss: 3.7127187850654053
Validation loss: 3.710333250425864

Epoch: 5| Step: 3
Training loss: 3.640235847336125
Validation loss: 3.695492354319635

Epoch: 5| Step: 4
Training loss: 4.527346699066123
Validation loss: 3.682310014020439

Epoch: 5| Step: 5
Training loss: 4.143112184162606
Validation loss: 3.6664221678364166

Epoch: 5| Step: 6
Training loss: 3.750140632535525
Validation loss: 3.6525343378402537

Epoch: 5| Step: 7
Training loss: 3.59704611178195
Validation loss: 3.6379775793460793

Epoch: 5| Step: 8
Training loss: 3.7831975792099257
Validation loss: 3.6254846420758726

Epoch: 5| Step: 9
Training loss: 3.6064049857700007
Validation loss: 3.6014860162561013

Epoch: 5| Step: 10
Training loss: 4.341411592357699
Validation loss: 3.590879347952378

Epoch: 15| Step: 0
Training loss: 3.4462187736350103
Validation loss: 3.5761863356177086

Epoch: 5| Step: 1
Training loss: 4.494170970059982
Validation loss: 3.558621052651146

Epoch: 5| Step: 2
Training loss: 3.998800455474446
Validation loss: 3.54371036194057

Epoch: 5| Step: 3
Training loss: 4.046957479100378
Validation loss: 3.525530246268077

Epoch: 5| Step: 4
Training loss: 3.962442507161283
Validation loss: 3.5041928220489273

Epoch: 5| Step: 5
Training loss: 3.2837848862739607
Validation loss: 3.49639103837063

Epoch: 5| Step: 6
Training loss: 3.2217390385087024
Validation loss: 3.4761467016155487

Epoch: 5| Step: 7
Training loss: 3.1148890384608934
Validation loss: 3.4579556935146525

Epoch: 5| Step: 8
Training loss: 3.9389758296088435
Validation loss: 3.441338120107227

Epoch: 5| Step: 9
Training loss: 3.1250950608100103
Validation loss: 3.433847376959336

Epoch: 5| Step: 10
Training loss: 3.457142697709225
Validation loss: 3.410492176775985

Epoch: 16| Step: 0
Training loss: 4.216942293911628
Validation loss: 3.4000784614577197

Epoch: 5| Step: 1
Training loss: 3.3635693292709905
Validation loss: 3.3774139965307244

Epoch: 5| Step: 2
Training loss: 3.2135688042795767
Validation loss: 3.3647071213413557

Epoch: 5| Step: 3
Training loss: 3.616328555591273
Validation loss: 3.3484730064647135

Epoch: 5| Step: 4
Training loss: 2.963745718011293
Validation loss: 3.328278057332751

Epoch: 5| Step: 5
Training loss: 4.136583652037359
Validation loss: 3.3196793927284367

Epoch: 5| Step: 6
Training loss: 3.707023017641017
Validation loss: 3.2942350053020433

Epoch: 5| Step: 7
Training loss: 3.260268641677517
Validation loss: 3.2768007927449965

Epoch: 5| Step: 8
Training loss: 3.1974237084850095
Validation loss: 3.2751797007519126

Epoch: 5| Step: 9
Training loss: 3.3595166553527864
Validation loss: 3.254213235208953

Epoch: 5| Step: 10
Training loss: 3.3184168531661227
Validation loss: 3.2401553072661664

Epoch: 17| Step: 0
Training loss: 3.0969385227190056
Validation loss: 3.225914148665846

Epoch: 5| Step: 1
Training loss: 3.002650996976431
Validation loss: 3.2043807494763255

Epoch: 5| Step: 2
Training loss: 3.34945989781273
Validation loss: 3.1962634812462305

Epoch: 5| Step: 3
Training loss: 3.182809707423143
Validation loss: 3.1717854771128744

Epoch: 5| Step: 4
Training loss: 3.99574625810745
Validation loss: 3.1586278849862275

Epoch: 5| Step: 5
Training loss: 3.3732914308601694
Validation loss: 3.1515202423859248

Epoch: 5| Step: 6
Training loss: 3.7973283410718155
Validation loss: 3.1328827093644502

Epoch: 5| Step: 7
Training loss: 3.1798385096876967
Validation loss: 3.1230993565764193

Epoch: 5| Step: 8
Training loss: 3.5101426165698264
Validation loss: 3.1022739315125896

Epoch: 5| Step: 9
Training loss: 3.124580049907707
Validation loss: 3.0901762771728802

Epoch: 5| Step: 10
Training loss: 3.1493527428623502
Validation loss: 3.0743769164734496

Epoch: 18| Step: 0
Training loss: 3.1280376166791637
Validation loss: 3.0536002402095566

Epoch: 5| Step: 1
Training loss: 3.3884517895028634
Validation loss: 3.0379891583086414

Epoch: 5| Step: 2
Training loss: 3.4582819341187396
Validation loss: 3.022122122553624

Epoch: 5| Step: 3
Training loss: 3.1112948677502996
Validation loss: 3.0019898293042724

Epoch: 5| Step: 4
Training loss: 3.1533720284548266
Validation loss: 2.9986902970583893

Epoch: 5| Step: 5
Training loss: 3.0740520039455133
Validation loss: 2.9848511362266805

Epoch: 5| Step: 6
Training loss: 3.0893839844903526
Validation loss: 2.964431132382291

Epoch: 5| Step: 7
Training loss: 3.037838098366937
Validation loss: 2.9441731569109435

Epoch: 5| Step: 8
Training loss: 3.848052803168969
Validation loss: 2.9389866566824634

Epoch: 5| Step: 9
Training loss: 3.1878231109212187
Validation loss: 2.92382884749104

Epoch: 5| Step: 10
Training loss: 2.7633095224027224
Validation loss: 2.9137580745779394

Epoch: 19| Step: 0
Training loss: 2.2620048276139744
Validation loss: 2.894885893432293

Epoch: 5| Step: 1
Training loss: 2.3488230416074627
Validation loss: 2.882909965718339

Epoch: 5| Step: 2
Training loss: 2.7021082912009113
Validation loss: 2.873930448138218

Epoch: 5| Step: 3
Training loss: 3.42746532286305
Validation loss: 2.857595565048102

Epoch: 5| Step: 4
Training loss: 3.481871522385753
Validation loss: 2.8491967088328116

Epoch: 5| Step: 5
Training loss: 3.8696468663325323
Validation loss: 2.834613068283326

Epoch: 5| Step: 6
Training loss: 3.481644454332504
Validation loss: 2.8333066080076774

Epoch: 5| Step: 7
Training loss: 3.626372636511633
Validation loss: 2.824061121145925

Epoch: 5| Step: 8
Training loss: 2.916121913355099
Validation loss: 2.813945092741976

Epoch: 5| Step: 9
Training loss: 2.852701539685483
Validation loss: 2.788156155032712

Epoch: 5| Step: 10
Training loss: 2.6322146679317306
Validation loss: 2.7789663682802535

Epoch: 20| Step: 0
Training loss: 3.8258191696322483
Validation loss: 2.7703802228804855

Epoch: 5| Step: 1
Training loss: 3.1545119835224926
Validation loss: 2.764406176084757

Epoch: 5| Step: 2
Training loss: 2.910162312546118
Validation loss: 2.7540121567364193

Epoch: 5| Step: 3
Training loss: 2.969642103575385
Validation loss: 2.7394690045065575

Epoch: 5| Step: 4
Training loss: 3.3162620342336804
Validation loss: 2.728190712456335

Epoch: 5| Step: 5
Training loss: 2.6361590660332777
Validation loss: 2.720702225130702

Epoch: 5| Step: 6
Training loss: 2.8261587990205883
Validation loss: 2.7121583263606728

Epoch: 5| Step: 7
Training loss: 2.3641075518261774
Validation loss: 2.6965471846931726

Epoch: 5| Step: 8
Training loss: 2.5878518385949545
Validation loss: 2.6975485817893854

Epoch: 5| Step: 9
Training loss: 3.8998715012728855
Validation loss: 2.687516811706509

Epoch: 5| Step: 10
Training loss: 2.0853209995374957
Validation loss: 2.6695408402969485

Epoch: 21| Step: 0
Training loss: 3.3998546232670868
Validation loss: 2.662997028785648

Epoch: 5| Step: 1
Training loss: 2.8986452753084277
Validation loss: 2.6602310180516637

Epoch: 5| Step: 2
Training loss: 2.5653755057385097
Validation loss: 2.660798335441621

Epoch: 5| Step: 3
Training loss: 3.1616783673021835
Validation loss: 2.6560907083290193

Epoch: 5| Step: 4
Training loss: 3.679254397033565
Validation loss: 2.6512860758196126

Epoch: 5| Step: 5
Training loss: 2.5542979278772298
Validation loss: 2.6424725981238755

Epoch: 5| Step: 6
Training loss: 2.7337585626301495
Validation loss: 2.6214569937013894

Epoch: 5| Step: 7
Training loss: 3.088387359486949
Validation loss: 2.6297736049979594

Epoch: 5| Step: 8
Training loss: 2.680672689677602
Validation loss: 2.6202654920864115

Epoch: 5| Step: 9
Training loss: 2.836434481031696
Validation loss: 2.628741508719325

Epoch: 5| Step: 10
Training loss: 2.714538205743414
Validation loss: 2.6162994195502236

Epoch: 22| Step: 0
Training loss: 3.3553928637555397
Validation loss: 2.610618732593967

Epoch: 5| Step: 1
Training loss: 2.5497935604231072
Validation loss: 2.596718979817108

Epoch: 5| Step: 2
Training loss: 2.4881169191529655
Validation loss: 2.5986511580287526

Epoch: 5| Step: 3
Training loss: 3.252715223669903
Validation loss: 2.6016227362659223

Epoch: 5| Step: 4
Training loss: 2.837042475041216
Validation loss: 2.6017861204110013

Epoch: 5| Step: 5
Training loss: 2.7027416508677717
Validation loss: 2.5985396484414305

Epoch: 5| Step: 6
Training loss: 2.3569668233460055
Validation loss: 2.5854641898489437

Epoch: 5| Step: 7
Training loss: 2.640799973689336
Validation loss: 2.5814646608909566

Epoch: 5| Step: 8
Training loss: 4.154444022502698
Validation loss: 2.586959046726289

Epoch: 5| Step: 9
Training loss: 2.9166881197185823
Validation loss: 2.5747771521023055

Epoch: 5| Step: 10
Training loss: 2.5332488656601924
Validation loss: 2.583942055476772

Epoch: 23| Step: 0
Training loss: 2.557934951414466
Validation loss: 2.5802533316272878

Epoch: 5| Step: 1
Training loss: 3.457128491084708
Validation loss: 2.5757581290295515

Epoch: 5| Step: 2
Training loss: 3.451338500062925
Validation loss: 2.575579643459401

Epoch: 5| Step: 3
Training loss: 2.8696288647082904
Validation loss: 2.5697859263265688

Epoch: 5| Step: 4
Training loss: 2.995795005019063
Validation loss: 2.576055115071114

Epoch: 5| Step: 5
Training loss: 2.7476472326786388
Validation loss: 2.5782454168265128

Epoch: 5| Step: 6
Training loss: 2.73765445778623
Validation loss: 2.5603354090847685

Epoch: 5| Step: 7
Training loss: 2.9543682085237184
Validation loss: 2.5674519486489036

Epoch: 5| Step: 8
Training loss: 2.209213825050097
Validation loss: 2.5761797385504024

Epoch: 5| Step: 9
Training loss: 2.8395055598362227
Validation loss: 2.576084060737751

Epoch: 5| Step: 10
Training loss: 2.9756141413374553
Validation loss: 2.57412214977264

Epoch: 24| Step: 0
Training loss: 2.3894713366957094
Validation loss: 2.561296610138578

Epoch: 5| Step: 1
Training loss: 3.038422583355374
Validation loss: 2.5634209413368625

Epoch: 5| Step: 2
Training loss: 2.722062559267991
Validation loss: 2.5698577890575462

Epoch: 5| Step: 3
Training loss: 2.584567995257028
Validation loss: 2.5713656478694276

Epoch: 5| Step: 4
Training loss: 3.534670767985233
Validation loss: 2.562607660140416

Epoch: 5| Step: 5
Training loss: 3.1052218357028507
Validation loss: 2.5589021044945652

Epoch: 5| Step: 6
Training loss: 3.049817508178304
Validation loss: 2.5578229597176

Epoch: 5| Step: 7
Training loss: 3.08827912542616
Validation loss: 2.5536194354539994

Epoch: 5| Step: 8
Training loss: 2.802211848340692
Validation loss: 2.554578281243133

Epoch: 5| Step: 9
Training loss: 2.8146424504492304
Validation loss: 2.5586536742689576

Epoch: 5| Step: 10
Training loss: 2.6949367095366457
Validation loss: 2.553350681816395

Epoch: 25| Step: 0
Training loss: 3.149914567787957
Validation loss: 2.5606206549748225

Epoch: 5| Step: 1
Training loss: 3.624728751721923
Validation loss: 2.5591449498582635

Epoch: 5| Step: 2
Training loss: 2.4353072255793484
Validation loss: 2.554221751133602

Epoch: 5| Step: 3
Training loss: 2.5862555639576787
Validation loss: 2.550220450656852

Epoch: 5| Step: 4
Training loss: 2.745371304328945
Validation loss: 2.5547390143888076

Epoch: 5| Step: 5
Training loss: 2.1953717966997366
Validation loss: 2.5523766642680377

Epoch: 5| Step: 6
Training loss: 3.0761890810530708
Validation loss: 2.541795589809078

Epoch: 5| Step: 7
Training loss: 2.642216761340733
Validation loss: 2.542229674942724

Epoch: 5| Step: 8
Training loss: 3.13127025970788
Validation loss: 2.553246827863296

Epoch: 5| Step: 9
Training loss: 2.934014870479165
Validation loss: 2.5405134327937473

Epoch: 5| Step: 10
Training loss: 3.1134926067293307
Validation loss: 2.5494184619503732

Epoch: 26| Step: 0
Training loss: 2.6394522506172704
Validation loss: 2.558535648961586

Epoch: 5| Step: 1
Training loss: 2.5615661594261883
Validation loss: 2.549451528176882

Epoch: 5| Step: 2
Training loss: 3.3321710626532504
Validation loss: 2.5459053765134674

Epoch: 5| Step: 3
Training loss: 2.810129650288083
Validation loss: 2.546441210412843

Epoch: 5| Step: 4
Training loss: 2.4191314908986588
Validation loss: 2.544163049027478

Epoch: 5| Step: 5
Training loss: 3.039897107842555
Validation loss: 2.551666718649194

Epoch: 5| Step: 6
Training loss: 3.1373557920104664
Validation loss: 2.546831339095675

Epoch: 5| Step: 7
Training loss: 2.8323907032120568
Validation loss: 2.5546823199040483

Epoch: 5| Step: 8
Training loss: 2.89014509186625
Validation loss: 2.555287806196215

Epoch: 5| Step: 9
Training loss: 2.912354651495739
Validation loss: 2.5409244335824828

Epoch: 5| Step: 10
Training loss: 3.2077195872709203
Validation loss: 2.5428106704537146

Epoch: 27| Step: 0
Training loss: 3.0161206722554943
Validation loss: 2.5438348776994

Epoch: 5| Step: 1
Training loss: 2.6695033919589193
Validation loss: 2.5467422745696693

Epoch: 5| Step: 2
Training loss: 3.2367845488537297
Validation loss: 2.538167641635309

Epoch: 5| Step: 3
Training loss: 2.971225389146766
Validation loss: 2.550585450379414

Epoch: 5| Step: 4
Training loss: 2.387241936762219
Validation loss: 2.551403372118758

Epoch: 5| Step: 5
Training loss: 2.589076784470911
Validation loss: 2.540155269701327

Epoch: 5| Step: 6
Training loss: 3.1779821073053616
Validation loss: 2.5485780931431306

Epoch: 5| Step: 7
Training loss: 3.396518349546151
Validation loss: 2.5461157794463007

Epoch: 5| Step: 8
Training loss: 2.5387490416486953
Validation loss: 2.5462320380453

Epoch: 5| Step: 9
Training loss: 3.2858807539382133
Validation loss: 2.5371781427590676

Epoch: 5| Step: 10
Training loss: 2.2457368517400216
Validation loss: 2.5403890052844647

Epoch: 28| Step: 0
Training loss: 3.1812641255902627
Validation loss: 2.54618505305961

Epoch: 5| Step: 1
Training loss: 3.1308446016397395
Validation loss: 2.5390328667286344

Epoch: 5| Step: 2
Training loss: 2.63108311574701
Validation loss: 2.551116911726328

Epoch: 5| Step: 3
Training loss: 3.4051518901271387
Validation loss: 2.5503333019285472

Epoch: 5| Step: 4
Training loss: 2.251711406456229
Validation loss: 2.5491758143438297

Epoch: 5| Step: 5
Training loss: 3.071040331740573
Validation loss: 2.5526177341320153

Epoch: 5| Step: 6
Training loss: 3.1393733780659443
Validation loss: 2.5465370717528244

Epoch: 5| Step: 7
Training loss: 2.4390992029276735
Validation loss: 2.540121911895343

Epoch: 5| Step: 8
Training loss: 2.585135897489722
Validation loss: 2.542644259942324

Epoch: 5| Step: 9
Training loss: 2.556944710269131
Validation loss: 2.5375447164091454

Epoch: 5| Step: 10
Training loss: 3.21587665785363
Validation loss: 2.5465807709234087

Epoch: 29| Step: 0
Training loss: 1.8546517269859226
Validation loss: 2.547761144031889

Epoch: 5| Step: 1
Training loss: 3.1048975005255053
Validation loss: 2.541572517279887

Epoch: 5| Step: 2
Training loss: 3.45319611489095
Validation loss: 2.5539357895542913

Epoch: 5| Step: 3
Training loss: 2.851517015904374
Validation loss: 2.550025602256811

Epoch: 5| Step: 4
Training loss: 2.738752077033101
Validation loss: 2.5527353516547064

Epoch: 5| Step: 5
Training loss: 3.1934177591662047
Validation loss: 2.5455540036372564

Epoch: 5| Step: 6
Training loss: 2.9235402629983667
Validation loss: 2.5351415142298035

Epoch: 5| Step: 7
Training loss: 2.550308908189587
Validation loss: 2.5375073123917247

Epoch: 5| Step: 8
Training loss: 2.9794027239416065
Validation loss: 2.5590356789083377

Epoch: 5| Step: 9
Training loss: 2.606561754640702
Validation loss: 2.5532952647842704

Epoch: 5| Step: 10
Training loss: 3.1617869542039694
Validation loss: 2.546345317273823

Epoch: 30| Step: 0
Training loss: 3.1247334175844887
Validation loss: 2.5353724166955214

Epoch: 5| Step: 1
Training loss: 2.752784186581481
Validation loss: 2.544482009528481

Epoch: 5| Step: 2
Training loss: 2.8802816555262667
Validation loss: 2.5407687286660074

Epoch: 5| Step: 3
Training loss: 2.5986282251110393
Validation loss: 2.5463188948819133

Epoch: 5| Step: 4
Training loss: 3.2374360715232213
Validation loss: 2.5547500306163204

Epoch: 5| Step: 5
Training loss: 2.8190253958476648
Validation loss: 2.547762635268349

Epoch: 5| Step: 6
Training loss: 3.0788099427298445
Validation loss: 2.5434197650817043

Epoch: 5| Step: 7
Training loss: 3.4289928245435983
Validation loss: 2.5478846566079745

Epoch: 5| Step: 8
Training loss: 2.502903777787276
Validation loss: 2.5282515836736397

Epoch: 5| Step: 9
Training loss: 2.904932687475255
Validation loss: 2.5456856379453683

Epoch: 5| Step: 10
Training loss: 1.9736051486727104
Validation loss: 2.537933333784675

Epoch: 31| Step: 0
Training loss: 3.0080927730619704
Validation loss: 2.5485674435498584

Epoch: 5| Step: 1
Training loss: 2.401996767370636
Validation loss: 2.528906500635815

Epoch: 5| Step: 2
Training loss: 3.0184020858789795
Validation loss: 2.5490634177047613

Epoch: 5| Step: 3
Training loss: 2.918070064966353
Validation loss: 2.5359418756401757

Epoch: 5| Step: 4
Training loss: 2.997360657928286
Validation loss: 2.532912140696889

Epoch: 5| Step: 5
Training loss: 2.8737554759022768
Validation loss: 2.5398463551407855

Epoch: 5| Step: 6
Training loss: 2.730077538140229
Validation loss: 2.5386340263307994

Epoch: 5| Step: 7
Training loss: 3.035534377336823
Validation loss: 2.5390840909054093

Epoch: 5| Step: 8
Training loss: 2.6646283127778942
Validation loss: 2.536439388221493

Epoch: 5| Step: 9
Training loss: 2.6149647853661118
Validation loss: 2.5415143640303484

Epoch: 5| Step: 10
Training loss: 3.2915102442663535
Validation loss: 2.539211749540706

Epoch: 32| Step: 0
Training loss: 2.6314124847767713
Validation loss: 2.5231531002544707

Epoch: 5| Step: 1
Training loss: 2.9397668411287645
Validation loss: 2.538876444497221

Epoch: 5| Step: 2
Training loss: 2.7968968225405635
Validation loss: 2.535714340143935

Epoch: 5| Step: 3
Training loss: 2.461400645437394
Validation loss: 2.5414732548959997

Epoch: 5| Step: 4
Training loss: 3.2061947882882564
Validation loss: 2.541460707368882

Epoch: 5| Step: 5
Training loss: 2.870340967426537
Validation loss: 2.5433835108680554

Epoch: 5| Step: 6
Training loss: 2.382756967366468
Validation loss: 2.5416152588480028

Epoch: 5| Step: 7
Training loss: 3.0299560301961272
Validation loss: 2.529139693502672

Epoch: 5| Step: 8
Training loss: 2.8766867623745203
Validation loss: 2.533877878392522

Epoch: 5| Step: 9
Training loss: 2.8289008156577533
Validation loss: 2.536895678536826

Epoch: 5| Step: 10
Training loss: 3.4415159868287195
Validation loss: 2.5422529431682115

Epoch: 33| Step: 0
Training loss: 3.0366380135628677
Validation loss: 2.5323986322153855

Epoch: 5| Step: 1
Training loss: 2.8681442784791793
Validation loss: 2.5338825056083167

Epoch: 5| Step: 2
Training loss: 2.5161706557747494
Validation loss: 2.530788193656299

Epoch: 5| Step: 3
Training loss: 2.9241601504447767
Validation loss: 2.5366988940472637

Epoch: 5| Step: 4
Training loss: 3.2537453418160505
Validation loss: 2.54131423494461

Epoch: 5| Step: 5
Training loss: 3.053355206934554
Validation loss: 2.5381089245346464

Epoch: 5| Step: 6
Training loss: 2.898697751435083
Validation loss: 2.5369172595884257

Epoch: 5| Step: 7
Training loss: 2.854752269358793
Validation loss: 2.535948743838165

Epoch: 5| Step: 8
Training loss: 2.1328272557010175
Validation loss: 2.5331153179839343

Epoch: 5| Step: 9
Training loss: 2.8477532013138465
Validation loss: 2.541734926674533

Epoch: 5| Step: 10
Training loss: 3.017305686528468
Validation loss: 2.531610268543306

Epoch: 34| Step: 0
Training loss: 3.0404557905328224
Validation loss: 2.5346624963990636

Epoch: 5| Step: 1
Training loss: 2.9966523247850505
Validation loss: 2.5464802988586213

Epoch: 5| Step: 2
Training loss: 2.9162219843886104
Validation loss: 2.533371375073011

Epoch: 5| Step: 3
Training loss: 3.381207444351541
Validation loss: 2.529713780162633

Epoch: 5| Step: 4
Training loss: 3.0101455161573805
Validation loss: 2.535306076329022

Epoch: 5| Step: 5
Training loss: 2.5928218053986627
Validation loss: 2.53096717501176

Epoch: 5| Step: 6
Training loss: 2.8529644589836085
Validation loss: 2.533599316272884

Epoch: 5| Step: 7
Training loss: 2.4371813052250695
Validation loss: 2.5278477847443908

Epoch: 5| Step: 8
Training loss: 2.5662306634069467
Validation loss: 2.538540812396241

Epoch: 5| Step: 9
Training loss: 2.7646664612457785
Validation loss: 2.539061324732203

Epoch: 5| Step: 10
Training loss: 2.713947307314098
Validation loss: 2.5292295177652786

Epoch: 35| Step: 0
Training loss: 2.5881215809382887
Validation loss: 2.537708004356507

Epoch: 5| Step: 1
Training loss: 2.543404771777266
Validation loss: 2.535441086974952

Epoch: 5| Step: 2
Training loss: 2.7445124581266707
Validation loss: 2.5294652911516304

Epoch: 5| Step: 3
Training loss: 2.7675154092664678
Validation loss: 2.527736708522167

Epoch: 5| Step: 4
Training loss: 3.065879669654977
Validation loss: 2.5249866690741167

Epoch: 5| Step: 5
Training loss: 2.9372461087503616
Validation loss: 2.53637921050569

Epoch: 5| Step: 6
Training loss: 2.5815346106027217
Validation loss: 2.5290214221026153

Epoch: 5| Step: 7
Training loss: 3.0818913842630034
Validation loss: 2.5328129528859766

Epoch: 5| Step: 8
Training loss: 3.249536334488377
Validation loss: 2.5349667535177614

Epoch: 5| Step: 9
Training loss: 2.7588353840137736
Validation loss: 2.539952399189171

Epoch: 5| Step: 10
Training loss: 3.0204405605995204
Validation loss: 2.5391348829277596

Epoch: 36| Step: 0
Training loss: 2.559118971169542
Validation loss: 2.5289791367354133

Epoch: 5| Step: 1
Training loss: 2.312022804306185
Validation loss: 2.5359746808732018

Epoch: 5| Step: 2
Training loss: 2.8938890329004
Validation loss: 2.5311934255792234

Epoch: 5| Step: 3
Training loss: 2.871874063551857
Validation loss: 2.536487835306573

Epoch: 5| Step: 4
Training loss: 2.8771911232599106
Validation loss: 2.5422846020937424

Epoch: 5| Step: 5
Training loss: 2.97978856635962
Validation loss: 2.531013781586369

Epoch: 5| Step: 6
Training loss: 2.9124260365013255
Validation loss: 2.5413074720384516

Epoch: 5| Step: 7
Training loss: 2.8447642770736308
Validation loss: 2.5305714054005346

Epoch: 5| Step: 8
Training loss: 3.450497831831747
Validation loss: 2.5322046878615057

Epoch: 5| Step: 9
Training loss: 2.7912530070663224
Validation loss: 2.537526506986349

Epoch: 5| Step: 10
Training loss: 2.742815516360353
Validation loss: 2.5344622349194887

Epoch: 37| Step: 0
Training loss: 2.3251028222758823
Validation loss: 2.5352152953679252

Epoch: 5| Step: 1
Training loss: 2.993690212982732
Validation loss: 2.5385176151743707

Epoch: 5| Step: 2
Training loss: 2.6033290787620147
Validation loss: 2.525325930935857

Epoch: 5| Step: 3
Training loss: 3.085245020949622
Validation loss: 2.5360482254872374

Epoch: 5| Step: 4
Training loss: 2.8805104396732197
Validation loss: 2.528894492918087

Epoch: 5| Step: 5
Training loss: 2.917746825249731
Validation loss: 2.536948451543436

Epoch: 5| Step: 6
Training loss: 3.313207874596474
Validation loss: 2.5318511893096063

Epoch: 5| Step: 7
Training loss: 2.576889204290981
Validation loss: 2.5215455197288783

Epoch: 5| Step: 8
Training loss: 2.6425331940817185
Validation loss: 2.5353159878567166

Epoch: 5| Step: 9
Training loss: 3.0458854437743845
Validation loss: 2.5375663030539495

Epoch: 5| Step: 10
Training loss: 2.7964723606227198
Validation loss: 2.530895810481408

Epoch: 38| Step: 0
Training loss: 2.876006240562067
Validation loss: 2.5350591411933907

Epoch: 5| Step: 1
Training loss: 3.222534029405851
Validation loss: 2.5313767199267656

Epoch: 5| Step: 2
Training loss: 3.064413601661156
Validation loss: 2.536630512501821

Epoch: 5| Step: 3
Training loss: 2.8352127199916657
Validation loss: 2.530088095471407

Epoch: 5| Step: 4
Training loss: 1.9614597552174988
Validation loss: 2.5393951253673053

Epoch: 5| Step: 5
Training loss: 2.4061764792567923
Validation loss: 2.5285936214198093

Epoch: 5| Step: 6
Training loss: 3.07424015488498
Validation loss: 2.524531500746833

Epoch: 5| Step: 7
Training loss: 2.430438940490159
Validation loss: 2.5327179608504102

Epoch: 5| Step: 8
Training loss: 3.067526448702402
Validation loss: 2.5293588961143927

Epoch: 5| Step: 9
Training loss: 2.7859188710358476
Validation loss: 2.533672272148201

Epoch: 5| Step: 10
Training loss: 3.3450377649476217
Validation loss: 2.534963478889513

Epoch: 39| Step: 0
Training loss: 2.739544238450975
Validation loss: 2.528890098352856

Epoch: 5| Step: 1
Training loss: 2.7953611571169588
Validation loss: 2.52985450564118

Epoch: 5| Step: 2
Training loss: 2.717516564509101
Validation loss: 2.546743834853883

Epoch: 5| Step: 3
Training loss: 2.686442056059044
Validation loss: 2.540549502937209

Epoch: 5| Step: 4
Training loss: 3.077213675274419
Validation loss: 2.531296973056569

Epoch: 5| Step: 5
Training loss: 3.2484725884198014
Validation loss: 2.5215539623431527

Epoch: 5| Step: 6
Training loss: 2.94236336545598
Validation loss: 2.5236136726953955

Epoch: 5| Step: 7
Training loss: 3.1799331307728034
Validation loss: 2.5301515825951473

Epoch: 5| Step: 8
Training loss: 2.442745433490386
Validation loss: 2.5213047692894532

Epoch: 5| Step: 9
Training loss: 2.5783111505061727
Validation loss: 2.5359389368868404

Epoch: 5| Step: 10
Training loss: 2.602096640250207
Validation loss: 2.5283667919449346

Epoch: 40| Step: 0
Training loss: 2.9359558592136525
Validation loss: 2.5298351150195257

Epoch: 5| Step: 1
Training loss: 3.079883673316593
Validation loss: 2.524249976550024

Epoch: 5| Step: 2
Training loss: 2.9780215394500944
Validation loss: 2.5447559521228147

Epoch: 5| Step: 3
Training loss: 2.4464999597215367
Validation loss: 2.51667573844754

Epoch: 5| Step: 4
Training loss: 2.575001251812973
Validation loss: 2.534395156581326

Epoch: 5| Step: 5
Training loss: 3.3924704596653674
Validation loss: 2.5180959314540203

Epoch: 5| Step: 6
Training loss: 2.959453925718755
Validation loss: 2.5266093215156853

Epoch: 5| Step: 7
Training loss: 2.4371914790600733
Validation loss: 2.5417618823628625

Epoch: 5| Step: 8
Training loss: 2.870310067895846
Validation loss: 2.5157068425615803

Epoch: 5| Step: 9
Training loss: 2.5561313107271
Validation loss: 2.527254631683954

Epoch: 5| Step: 10
Training loss: 2.8441998471786256
Validation loss: 2.542132151594309

Epoch: 41| Step: 0
Training loss: 3.2582015210065287
Validation loss: 2.5287455945087616

Epoch: 5| Step: 1
Training loss: 2.711612672305345
Validation loss: 2.528333550856052

Epoch: 5| Step: 2
Training loss: 2.290305363314369
Validation loss: 2.527125880066484

Epoch: 5| Step: 3
Training loss: 3.1194880894541948
Validation loss: 2.523856342086953

Epoch: 5| Step: 4
Training loss: 2.6586119080314927
Validation loss: 2.522393157861283

Epoch: 5| Step: 5
Training loss: 3.157195903182021
Validation loss: 2.5277610939250885

Epoch: 5| Step: 6
Training loss: 2.7251822139469217
Validation loss: 2.5394071177567117

Epoch: 5| Step: 7
Training loss: 2.343871863693817
Validation loss: 2.529893083770728

Epoch: 5| Step: 8
Training loss: 3.3104283314413956
Validation loss: 2.532225503523404

Epoch: 5| Step: 9
Training loss: 2.5795066136990594
Validation loss: 2.533722492723154

Epoch: 5| Step: 10
Training loss: 2.7447907787420487
Validation loss: 2.5227449661108357

Epoch: 42| Step: 0
Training loss: 2.707949351942903
Validation loss: 2.5234016881466665

Epoch: 5| Step: 1
Training loss: 2.988539421547842
Validation loss: 2.515496243220144

Epoch: 5| Step: 2
Training loss: 2.911089405758661
Validation loss: 2.530144811636606

Epoch: 5| Step: 3
Training loss: 3.14727943647483
Validation loss: 2.5271066318355

Epoch: 5| Step: 4
Training loss: 2.5691553543176675
Validation loss: 2.523368848484431

Epoch: 5| Step: 5
Training loss: 2.928134516778908
Validation loss: 2.5285987414090365

Epoch: 5| Step: 6
Training loss: 2.9771066403572246
Validation loss: 2.5237957947579304

Epoch: 5| Step: 7
Training loss: 2.75999558793973
Validation loss: 2.5305358363369232

Epoch: 5| Step: 8
Training loss: 3.1411212889612097
Validation loss: 2.5186041600815

Epoch: 5| Step: 9
Training loss: 2.0948139732636326
Validation loss: 2.5148460666546546

Epoch: 5| Step: 10
Training loss: 2.7271893936488105
Validation loss: 2.533934542613351

Epoch: 43| Step: 0
Training loss: 3.1054692106426547
Validation loss: 2.524851201953799

Epoch: 5| Step: 1
Training loss: 2.688032408030739
Validation loss: 2.5289376717764096

Epoch: 5| Step: 2
Training loss: 2.691129584411348
Validation loss: 2.522548781023423

Epoch: 5| Step: 3
Training loss: 2.9073318600766407
Validation loss: 2.52174873447156

Epoch: 5| Step: 4
Training loss: 2.639270864135567
Validation loss: 2.526408428786224

Epoch: 5| Step: 5
Training loss: 2.74347658641593
Validation loss: 2.5271727512391484

Epoch: 5| Step: 6
Training loss: 2.747070659660483
Validation loss: 2.5131786887052368

Epoch: 5| Step: 7
Training loss: 2.812472873133068
Validation loss: 2.5223232796087403

Epoch: 5| Step: 8
Training loss: 3.092226366104357
Validation loss: 2.530594236302059

Epoch: 5| Step: 9
Training loss: 3.236329451050196
Validation loss: 2.524435640577949

Epoch: 5| Step: 10
Training loss: 2.170620013669803
Validation loss: 2.5232354108632182

Epoch: 44| Step: 0
Training loss: 2.8898720250289496
Validation loss: 2.5409583660304564

Epoch: 5| Step: 1
Training loss: 2.9528110882265914
Validation loss: 2.5248810168985756

Epoch: 5| Step: 2
Training loss: 2.2710646998512933
Validation loss: 2.535622836744487

Epoch: 5| Step: 3
Training loss: 2.5731843713855924
Validation loss: 2.5326676049623233

Epoch: 5| Step: 4
Training loss: 3.2882877986930663
Validation loss: 2.5264889179815166

Epoch: 5| Step: 5
Training loss: 3.4284484988835797
Validation loss: 2.5220823330288678

Epoch: 5| Step: 6
Training loss: 3.4781476810946232
Validation loss: 2.5236017048089203

Epoch: 5| Step: 7
Training loss: 2.030549148547143
Validation loss: 2.530888400828817

Epoch: 5| Step: 8
Training loss: 2.910046138856468
Validation loss: 2.525941364989392

Epoch: 5| Step: 9
Training loss: 2.1420851134158223
Validation loss: 2.5229195725184175

Epoch: 5| Step: 10
Training loss: 2.5965757489070125
Validation loss: 2.516565660499167

Epoch: 45| Step: 0
Training loss: 2.6133518637110487
Validation loss: 2.5149988154187515

Epoch: 5| Step: 1
Training loss: 3.0738962627173603
Validation loss: 2.5248959120547982

Epoch: 5| Step: 2
Training loss: 2.802374776174471
Validation loss: 2.528819454795409

Epoch: 5| Step: 3
Training loss: 2.5019351145612
Validation loss: 2.530497692494162

Epoch: 5| Step: 4
Training loss: 2.7187258839907886
Validation loss: 2.5181599937611723

Epoch: 5| Step: 5
Training loss: 3.411826372556518
Validation loss: 2.5338420208512336

Epoch: 5| Step: 6
Training loss: 2.8004750359704165
Validation loss: 2.51496646388543

Epoch: 5| Step: 7
Training loss: 2.7583289162714726
Validation loss: 2.5230324276336358

Epoch: 5| Step: 8
Training loss: 2.5360513996487053
Validation loss: 2.525958376087353

Epoch: 5| Step: 9
Training loss: 2.641169384702345
Validation loss: 2.517915903827828

Epoch: 5| Step: 10
Training loss: 3.130565874665102
Validation loss: 2.523706600930174

Epoch: 46| Step: 0
Training loss: 2.780949865712996
Validation loss: 2.521238463459984

Epoch: 5| Step: 1
Training loss: 1.7731770059450442
Validation loss: 2.522823457010604

Epoch: 5| Step: 2
Training loss: 3.1847351649201086
Validation loss: 2.5217573390628987

Epoch: 5| Step: 3
Training loss: 3.0105277352264785
Validation loss: 2.5291247199479616

Epoch: 5| Step: 4
Training loss: 2.1496271342383224
Validation loss: 2.5219025655773075

Epoch: 5| Step: 5
Training loss: 2.895102667165739
Validation loss: 2.533505351449852

Epoch: 5| Step: 6
Training loss: 3.0719385864258375
Validation loss: 2.526727918889542

Epoch: 5| Step: 7
Training loss: 2.842248205676432
Validation loss: 2.5204018835751585

Epoch: 5| Step: 8
Training loss: 2.982676716866725
Validation loss: 2.525284363353587

Epoch: 5| Step: 9
Training loss: 2.8108191659817066
Validation loss: 2.527230098362723

Epoch: 5| Step: 10
Training loss: 3.1404376377375423
Validation loss: 2.518543693121724

Epoch: 47| Step: 0
Training loss: 2.295537857823665
Validation loss: 2.5222117141166955

Epoch: 5| Step: 1
Training loss: 2.76475450857319
Validation loss: 2.5218317364453258

Epoch: 5| Step: 2
Training loss: 2.8289163230583916
Validation loss: 2.5137164545265747

Epoch: 5| Step: 3
Training loss: 3.0006309481567595
Validation loss: 2.5329428232831126

Epoch: 5| Step: 4
Training loss: 2.174443105676536
Validation loss: 2.5213164725296133

Epoch: 5| Step: 5
Training loss: 2.7780389143797732
Validation loss: 2.5182332493154176

Epoch: 5| Step: 6
Training loss: 3.1446648421253602
Validation loss: 2.5274689079735104

Epoch: 5| Step: 7
Training loss: 3.1588353814019614
Validation loss: 2.519053487209906

Epoch: 5| Step: 8
Training loss: 3.0648680892860822
Validation loss: 2.5189833524584637

Epoch: 5| Step: 9
Training loss: 2.74333857968811
Validation loss: 2.5292734308233595

Epoch: 5| Step: 10
Training loss: 2.913708376396915
Validation loss: 2.5294497038127672

Epoch: 48| Step: 0
Training loss: 2.9146469389214973
Validation loss: 2.520156781428416

Epoch: 5| Step: 1
Training loss: 2.884465103147274
Validation loss: 2.5175659339264858

Epoch: 5| Step: 2
Training loss: 2.686937406338271
Validation loss: 2.5300211250985805

Epoch: 5| Step: 3
Training loss: 2.7674097886239015
Validation loss: 2.522002192647923

Epoch: 5| Step: 4
Training loss: 2.5069568160559887
Validation loss: 2.5299526897256177

Epoch: 5| Step: 5
Training loss: 3.5538651070484764
Validation loss: 2.5198778063412615

Epoch: 5| Step: 6
Training loss: 2.3782523122387467
Validation loss: 2.5298005337797846

Epoch: 5| Step: 7
Training loss: 2.5830256268584653
Validation loss: 2.5114961050630846

Epoch: 5| Step: 8
Training loss: 2.665817443412351
Validation loss: 2.5279939797101156

Epoch: 5| Step: 9
Training loss: 3.1461573217091887
Validation loss: 2.5191486566417094

Epoch: 5| Step: 10
Training loss: 2.554158007143991
Validation loss: 2.5243832528933785

Epoch: 49| Step: 0
Training loss: 3.2257130684718165
Validation loss: 2.5143909417210413

Epoch: 5| Step: 1
Training loss: 2.9734222588120947
Validation loss: 2.52180216399516

Epoch: 5| Step: 2
Training loss: 3.0510300694787134
Validation loss: 2.5228510237683976

Epoch: 5| Step: 3
Training loss: 2.922438623258642
Validation loss: 2.525607477306537

Epoch: 5| Step: 4
Training loss: 2.872307553264795
Validation loss: 2.5246917605508297

Epoch: 5| Step: 5
Training loss: 2.537073101345293
Validation loss: 2.515594128609275

Epoch: 5| Step: 6
Training loss: 2.4307419429944876
Validation loss: 2.528932851015187

Epoch: 5| Step: 7
Training loss: 2.3681515785753424
Validation loss: 2.530751471811635

Epoch: 5| Step: 8
Training loss: 2.474955232774088
Validation loss: 2.529666879050974

Epoch: 5| Step: 9
Training loss: 2.8156655193296563
Validation loss: 2.536698574691074

Epoch: 5| Step: 10
Training loss: 3.098677285323385
Validation loss: 2.514198696316897

Epoch: 50| Step: 0
Training loss: 2.2359688149531225
Validation loss: 2.5287074277166703

Epoch: 5| Step: 1
Training loss: 2.7021046735930168
Validation loss: 2.535129983015279

Epoch: 5| Step: 2
Training loss: 2.6723180927712624
Validation loss: 2.5250764948725597

Epoch: 5| Step: 3
Training loss: 2.394794493202465
Validation loss: 2.5277210075815573

Epoch: 5| Step: 4
Training loss: 2.7717461385093607
Validation loss: 2.5125949358239246

Epoch: 5| Step: 5
Training loss: 2.468700070419493
Validation loss: 2.5070274201886296

Epoch: 5| Step: 6
Training loss: 2.707738038198729
Validation loss: 2.52947640579446

Epoch: 5| Step: 7
Training loss: 3.088761131547532
Validation loss: 2.5120202016319237

Epoch: 5| Step: 8
Training loss: 2.584595945939301
Validation loss: 2.5182799592190386

Epoch: 5| Step: 9
Training loss: 3.869158125197046
Validation loss: 2.524178460702575

Epoch: 5| Step: 10
Training loss: 2.9927625775801463
Validation loss: 2.5340022330805336

Epoch: 51| Step: 0
Training loss: 2.8733441103786976
Validation loss: 2.524589025661899

Epoch: 5| Step: 1
Training loss: 2.9680439962843197
Validation loss: 2.516267722281535

Epoch: 5| Step: 2
Training loss: 2.011582927377905
Validation loss: 2.522093314014354

Epoch: 5| Step: 3
Training loss: 2.267064235919745
Validation loss: 2.517015305698498

Epoch: 5| Step: 4
Training loss: 2.9323056335297273
Validation loss: 2.5128755896755695

Epoch: 5| Step: 5
Training loss: 2.9062050436501043
Validation loss: 2.524397515255219

Epoch: 5| Step: 6
Training loss: 3.1502781639199418
Validation loss: 2.5275567062613864

Epoch: 5| Step: 7
Training loss: 2.348556777452946
Validation loss: 2.5179356173597998

Epoch: 5| Step: 8
Training loss: 3.036967283402398
Validation loss: 2.511133027340294

Epoch: 5| Step: 9
Training loss: 2.874442668465815
Validation loss: 2.528798524418786

Epoch: 5| Step: 10
Training loss: 3.206520922779494
Validation loss: 2.5185785469985276

Epoch: 52| Step: 0
Training loss: 3.051792031058158
Validation loss: 2.52768644508728

Epoch: 5| Step: 1
Training loss: 2.5398379036851644
Validation loss: 2.5297484314993093

Epoch: 5| Step: 2
Training loss: 2.53106943122225
Validation loss: 2.5273355417720094

Epoch: 5| Step: 3
Training loss: 2.668061418166684
Validation loss: 2.5130609974028437

Epoch: 5| Step: 4
Training loss: 2.920611266339186
Validation loss: 2.523388510272661

Epoch: 5| Step: 5
Training loss: 2.882005035703073
Validation loss: 2.5243583575466215

Epoch: 5| Step: 6
Training loss: 2.873459444693426
Validation loss: 2.5185654853761386

Epoch: 5| Step: 7
Training loss: 2.7974823872929666
Validation loss: 2.5322336776773464

Epoch: 5| Step: 8
Training loss: 2.918168271761739
Validation loss: 2.51916067163372

Epoch: 5| Step: 9
Training loss: 2.4364048380454086
Validation loss: 2.5269160132763715

Epoch: 5| Step: 10
Training loss: 3.0782707683302877
Validation loss: 2.521415313936292

Epoch: 53| Step: 0
Training loss: 3.3594651321030136
Validation loss: 2.5231325811138974

Epoch: 5| Step: 1
Training loss: 2.370498909406687
Validation loss: 2.521836053851718

Epoch: 5| Step: 2
Training loss: 3.163084279242773
Validation loss: 2.5188115856153384

Epoch: 5| Step: 3
Training loss: 2.211328478639457
Validation loss: 2.512426958658546

Epoch: 5| Step: 4
Training loss: 2.5403174482802555
Validation loss: 2.5214844573542234

Epoch: 5| Step: 5
Training loss: 3.0282740191156865
Validation loss: 2.5283401933292082

Epoch: 5| Step: 6
Training loss: 3.0250123331118033
Validation loss: 2.525154320399251

Epoch: 5| Step: 7
Training loss: 2.41258515518986
Validation loss: 2.531758494203544

Epoch: 5| Step: 8
Training loss: 2.6978294230804916
Validation loss: 2.5263741263767705

Epoch: 5| Step: 9
Training loss: 3.0137484547545035
Validation loss: 2.5172992664095846

Epoch: 5| Step: 10
Training loss: 2.6817376215785895
Validation loss: 2.5215416959391965

Epoch: 54| Step: 0
Training loss: 2.998180632449847
Validation loss: 2.5260268019473644

Epoch: 5| Step: 1
Training loss: 2.7245097918069328
Validation loss: 2.523556376470353

Epoch: 5| Step: 2
Training loss: 2.2808160238430326
Validation loss: 2.5190458066165924

Epoch: 5| Step: 3
Training loss: 2.7625393653239767
Validation loss: 2.5236917911949712

Epoch: 5| Step: 4
Training loss: 2.3798579675303473
Validation loss: 2.52414600388869

Epoch: 5| Step: 5
Training loss: 2.798009290792925
Validation loss: 2.5212774164395855

Epoch: 5| Step: 6
Training loss: 3.031173862405596
Validation loss: 2.516857599622885

Epoch: 5| Step: 7
Training loss: 3.051058044771669
Validation loss: 2.521497382867117

Epoch: 5| Step: 8
Training loss: 3.091519563230818
Validation loss: 2.5226963394481516

Epoch: 5| Step: 9
Training loss: 2.7793313598638005
Validation loss: 2.5248741805305843

Epoch: 5| Step: 10
Training loss: 2.6391657120807617
Validation loss: 2.5145557362663054

Epoch: 55| Step: 0
Training loss: 2.458557140270508
Validation loss: 2.5219412124878655

Epoch: 5| Step: 1
Training loss: 2.6536496168314896
Validation loss: 2.5193493447279254

Epoch: 5| Step: 2
Training loss: 3.2963701919324446
Validation loss: 2.5211493540572243

Epoch: 5| Step: 3
Training loss: 2.4044370881510844
Validation loss: 2.514926417742677

Epoch: 5| Step: 4
Training loss: 2.607995588070701
Validation loss: 2.520220701834039

Epoch: 5| Step: 5
Training loss: 2.5525219806318784
Validation loss: 2.5126245717456137

Epoch: 5| Step: 6
Training loss: 2.7144701364434027
Validation loss: 2.514695082479875

Epoch: 5| Step: 7
Training loss: 3.3747774686565126
Validation loss: 2.514090685972356

Epoch: 5| Step: 8
Training loss: 3.3145408821064906
Validation loss: 2.521551281328171

Epoch: 5| Step: 9
Training loss: 2.42779388723065
Validation loss: 2.521877765695934

Epoch: 5| Step: 10
Training loss: 2.6253973342271717
Validation loss: 2.507631565421575

Epoch: 56| Step: 0
Training loss: 3.0006471571673345
Validation loss: 2.515950233827628

Epoch: 5| Step: 1
Training loss: 2.6992406554375243
Validation loss: 2.5087072926508593

Epoch: 5| Step: 2
Training loss: 2.6723946406237187
Validation loss: 2.5187001190339555

Epoch: 5| Step: 3
Training loss: 3.023738717936883
Validation loss: 2.5285952709699457

Epoch: 5| Step: 4
Training loss: 2.3058473933427375
Validation loss: 2.510837853702164

Epoch: 5| Step: 5
Training loss: 2.799799748480558
Validation loss: 2.5255974221119284

Epoch: 5| Step: 6
Training loss: 2.7307521689347953
Validation loss: 2.5301380041832533

Epoch: 5| Step: 7
Training loss: 2.8012592753674252
Validation loss: 2.5133544523600286

Epoch: 5| Step: 8
Training loss: 3.0526837657753094
Validation loss: 2.5267375627287367

Epoch: 5| Step: 9
Training loss: 2.762083210293183
Validation loss: 2.5391671309829853

Epoch: 5| Step: 10
Training loss: 2.70937432065821
Validation loss: 2.519060656895928

Epoch: 57| Step: 0
Training loss: 2.8366027556174074
Validation loss: 2.5256494816218273

Epoch: 5| Step: 1
Training loss: 2.583069839165474
Validation loss: 2.514876215306029

Epoch: 5| Step: 2
Training loss: 2.49869207501122
Validation loss: 2.5183999953095837

Epoch: 5| Step: 3
Training loss: 2.92916499246809
Validation loss: 2.5131179259489564

Epoch: 5| Step: 4
Training loss: 2.954582540906216
Validation loss: 2.5213717800264814

Epoch: 5| Step: 5
Training loss: 2.8821969551064517
Validation loss: 2.538629199241207

Epoch: 5| Step: 6
Training loss: 2.720756842417702
Validation loss: 2.5275112926503973

Epoch: 5| Step: 7
Training loss: 2.9707928505413563
Validation loss: 2.5243676194486153

Epoch: 5| Step: 8
Training loss: 2.614891571072651
Validation loss: 2.527041968514071

Epoch: 5| Step: 9
Training loss: 3.050050303560262
Validation loss: 2.524702918040032

Epoch: 5| Step: 10
Training loss: 2.4416140536562527
Validation loss: 2.5261425526327272

Epoch: 58| Step: 0
Training loss: 2.639677430733531
Validation loss: 2.5176533820858125

Epoch: 5| Step: 1
Training loss: 2.8821742894286517
Validation loss: 2.5131929993518454

Epoch: 5| Step: 2
Training loss: 2.6619570290402486
Validation loss: 2.5145959560207527

Epoch: 5| Step: 3
Training loss: 2.8617443836510623
Validation loss: 2.5137013024104133

Epoch: 5| Step: 4
Training loss: 3.0727644198359187
Validation loss: 2.5147004733987357

Epoch: 5| Step: 5
Training loss: 2.624349331822231
Validation loss: 2.5171896068145876

Epoch: 5| Step: 6
Training loss: 2.8898565147181925
Validation loss: 2.511242084695995

Epoch: 5| Step: 7
Training loss: 2.4679185457065205
Validation loss: 2.5126996721320163

Epoch: 5| Step: 8
Training loss: 2.222446854152254
Validation loss: 2.5183200685946017

Epoch: 5| Step: 9
Training loss: 3.612040142634685
Validation loss: 2.5136055636460424

Epoch: 5| Step: 10
Training loss: 2.2695064576152295
Validation loss: 2.5316453506220538

Epoch: 59| Step: 0
Training loss: 2.817997539765561
Validation loss: 2.505577692758782

Epoch: 5| Step: 1
Training loss: 2.4685497927064475
Validation loss: 2.514904147466874

Epoch: 5| Step: 2
Training loss: 2.360535595134807
Validation loss: 2.5173695264955445

Epoch: 5| Step: 3
Training loss: 2.832310398419136
Validation loss: 2.5086969653377986

Epoch: 5| Step: 4
Training loss: 2.903985895106843
Validation loss: 2.5129765958025594

Epoch: 5| Step: 5
Training loss: 2.651344706684159
Validation loss: 2.5153362638648438

Epoch: 5| Step: 6
Training loss: 2.630055781305933
Validation loss: 2.5098886994604492

Epoch: 5| Step: 7
Training loss: 2.9878929933131384
Validation loss: 2.5170890528893675

Epoch: 5| Step: 8
Training loss: 3.171861470010398
Validation loss: 2.52534088031716

Epoch: 5| Step: 9
Training loss: 2.9625402455875727
Validation loss: 2.5110392294889494

Epoch: 5| Step: 10
Training loss: 2.62111949197377
Validation loss: 2.510735350413678

Epoch: 60| Step: 0
Training loss: 2.941227110819468
Validation loss: 2.50631380500923

Epoch: 5| Step: 1
Training loss: 2.634701376683763
Validation loss: 2.525513079621892

Epoch: 5| Step: 2
Training loss: 2.977516642154848
Validation loss: 2.5180611218021203

Epoch: 5| Step: 3
Training loss: 3.0520584226943543
Validation loss: 2.514352875147514

Epoch: 5| Step: 4
Training loss: 2.8503826888294
Validation loss: 2.518411589893318

Epoch: 5| Step: 5
Training loss: 2.5885261417488783
Validation loss: 2.5146830395250235

Epoch: 5| Step: 6
Training loss: 2.3209617040483317
Validation loss: 2.5132251059483566

Epoch: 5| Step: 7
Training loss: 2.5434902611544725
Validation loss: 2.5163642593497344

Epoch: 5| Step: 8
Training loss: 2.764227217558377
Validation loss: 2.524690419171168

Epoch: 5| Step: 9
Training loss: 3.3181106267586515
Validation loss: 2.517625164760797

Epoch: 5| Step: 10
Training loss: 2.1389035173496636
Validation loss: 2.518879726249578

Epoch: 61| Step: 0
Training loss: 3.046236098378691
Validation loss: 2.5049956665957853

Epoch: 5| Step: 1
Training loss: 2.9621136827926775
Validation loss: 2.5124299494026965

Epoch: 5| Step: 2
Training loss: 2.8563873858976883
Validation loss: 2.517926336889205

Epoch: 5| Step: 3
Training loss: 3.0502443121944687
Validation loss: 2.5167204399723238

Epoch: 5| Step: 4
Training loss: 2.266520356149776
Validation loss: 2.506170223286854

Epoch: 5| Step: 5
Training loss: 2.766761589336932
Validation loss: 2.5131924658535447

Epoch: 5| Step: 6
Training loss: 2.5948592422325167
Validation loss: 2.5029253548935637

Epoch: 5| Step: 7
Training loss: 2.8626366861683774
Validation loss: 2.5178306701959423

Epoch: 5| Step: 8
Training loss: 2.312614025707051
Validation loss: 2.520929037942042

Epoch: 5| Step: 9
Training loss: 2.6649999675070815
Validation loss: 2.5177691246941056

Epoch: 5| Step: 10
Training loss: 2.938261217547539
Validation loss: 2.51824215147566

Epoch: 62| Step: 0
Training loss: 2.262959036442431
Validation loss: 2.5115440692065683

Epoch: 5| Step: 1
Training loss: 2.9814948919067947
Validation loss: 2.511184449025911

Epoch: 5| Step: 2
Training loss: 2.6159287799274717
Validation loss: 2.518537125583908

Epoch: 5| Step: 3
Training loss: 2.957627995898332
Validation loss: 2.5193255281449596

Epoch: 5| Step: 4
Training loss: 2.47004656002625
Validation loss: 2.510486299712105

Epoch: 5| Step: 5
Training loss: 2.698817089354744
Validation loss: 2.512175276465993

Epoch: 5| Step: 6
Training loss: 3.0023827626941815
Validation loss: 2.515333676104704

Epoch: 5| Step: 7
Training loss: 2.499117313960669
Validation loss: 2.5123568970858585

Epoch: 5| Step: 8
Training loss: 2.5159067505566135
Validation loss: 2.511831955267656

Epoch: 5| Step: 9
Training loss: 3.4955380472287843
Validation loss: 2.51204420177682

Epoch: 5| Step: 10
Training loss: 2.7249313765818175
Validation loss: 2.5161068551076413

Epoch: 63| Step: 0
Training loss: 2.8914493519441766
Validation loss: 2.5283207951496154

Epoch: 5| Step: 1
Training loss: 2.9573173031844027
Validation loss: 2.5139773366552043

Epoch: 5| Step: 2
Training loss: 2.772357397238838
Validation loss: 2.511483500658109

Epoch: 5| Step: 3
Training loss: 2.584408495315785
Validation loss: 2.5108238002066727

Epoch: 5| Step: 4
Training loss: 2.5316498052133065
Validation loss: 2.5164085823315165

Epoch: 5| Step: 5
Training loss: 2.3815552443256283
Validation loss: 2.512109298094128

Epoch: 5| Step: 6
Training loss: 2.8832984245994813
Validation loss: 2.5242516787032563

Epoch: 5| Step: 7
Training loss: 3.158668422453191
Validation loss: 2.5110205787404847

Epoch: 5| Step: 8
Training loss: 2.8615999164324273
Validation loss: 2.521333824948879

Epoch: 5| Step: 9
Training loss: 2.8378114840242405
Validation loss: 2.518706276975017

Epoch: 5| Step: 10
Training loss: 2.440623214271664
Validation loss: 2.5130825546279203

Epoch: 64| Step: 0
Training loss: 2.9896403095695354
Validation loss: 2.51955140062767

Epoch: 5| Step: 1
Training loss: 2.939212056663841
Validation loss: 2.5109254973829853

Epoch: 5| Step: 2
Training loss: 2.693233495347447
Validation loss: 2.5121983755790858

Epoch: 5| Step: 3
Training loss: 2.334605676164069
Validation loss: 2.509362791794833

Epoch: 5| Step: 4
Training loss: 2.6028373275380634
Validation loss: 2.510501583560736

Epoch: 5| Step: 5
Training loss: 3.287953531869691
Validation loss: 2.5143764533462294

Epoch: 5| Step: 6
Training loss: 2.9387199628131335
Validation loss: 2.5120285241982536

Epoch: 5| Step: 7
Training loss: 2.7994540806319095
Validation loss: 2.521573804002123

Epoch: 5| Step: 8
Training loss: 2.3964399039845383
Validation loss: 2.5142669354572047

Epoch: 5| Step: 9
Training loss: 2.0989531587169337
Validation loss: 2.519390734472667

Epoch: 5| Step: 10
Training loss: 3.013749562298661
Validation loss: 2.5126184478741775

Epoch: 65| Step: 0
Training loss: 2.6532798766816716
Validation loss: 2.5184327531793684

Epoch: 5| Step: 1
Training loss: 2.1683962838576476
Validation loss: 2.5160783616371973

Epoch: 5| Step: 2
Training loss: 2.370184785423538
Validation loss: 2.5226272414421445

Epoch: 5| Step: 3
Training loss: 2.889772361581066
Validation loss: 2.517380627839488

Epoch: 5| Step: 4
Training loss: 2.9895388680761106
Validation loss: 2.51431201424279

Epoch: 5| Step: 5
Training loss: 2.8883186005665835
Validation loss: 2.523175975419527

Epoch: 5| Step: 6
Training loss: 2.502520149294653
Validation loss: 2.5159093835810427

Epoch: 5| Step: 7
Training loss: 3.271746578811439
Validation loss: 2.518406451229256

Epoch: 5| Step: 8
Training loss: 3.3262399698971374
Validation loss: 2.512909262236297

Epoch: 5| Step: 9
Training loss: 2.1482884580760526
Validation loss: 2.5028649946578763

Epoch: 5| Step: 10
Training loss: 2.8402351204161964
Validation loss: 2.5149441822415706

Epoch: 66| Step: 0
Training loss: 2.1452556560901015
Validation loss: 2.5135005769506136

Epoch: 5| Step: 1
Training loss: 2.6561992864816477
Validation loss: 2.502410965897175

Epoch: 5| Step: 2
Training loss: 2.754540076938694
Validation loss: 2.5120031767696327

Epoch: 5| Step: 3
Training loss: 2.8437593900085356
Validation loss: 2.509550450570626

Epoch: 5| Step: 4
Training loss: 3.4516166050729806
Validation loss: 2.532120618100418

Epoch: 5| Step: 5
Training loss: 2.421582702717852
Validation loss: 2.5119540671210467

Epoch: 5| Step: 6
Training loss: 2.8248319052589217
Validation loss: 2.5178552971391053

Epoch: 5| Step: 7
Training loss: 2.4441419243256597
Validation loss: 2.5258269275915572

Epoch: 5| Step: 8
Training loss: 2.6571847505094266
Validation loss: 2.51949993501055

Epoch: 5| Step: 9
Training loss: 2.8600136299075283
Validation loss: 2.506504686293373

Epoch: 5| Step: 10
Training loss: 3.1217099991000836
Validation loss: 2.510312478830028

Epoch: 67| Step: 0
Training loss: 2.563784858693541
Validation loss: 2.5147736146289383

Epoch: 5| Step: 1
Training loss: 2.7742985637917794
Validation loss: 2.514522297816954

Epoch: 5| Step: 2
Training loss: 2.090364480429492
Validation loss: 2.4983052446723617

Epoch: 5| Step: 3
Training loss: 3.5634916832630004
Validation loss: 2.51967043704039

Epoch: 5| Step: 4
Training loss: 2.7758956975772926
Validation loss: 2.521684510191927

Epoch: 5| Step: 5
Training loss: 2.7669995873619464
Validation loss: 2.5144124997788286

Epoch: 5| Step: 6
Training loss: 2.7604499287070805
Validation loss: 2.5053204455559452

Epoch: 5| Step: 7
Training loss: 2.289406812375483
Validation loss: 2.520481248426296

Epoch: 5| Step: 8
Training loss: 2.957059308300667
Validation loss: 2.5142914198912036

Epoch: 5| Step: 9
Training loss: 2.903074765463692
Validation loss: 2.5154439649164484

Epoch: 5| Step: 10
Training loss: 2.4538686803487932
Validation loss: 2.5159434771264615

Epoch: 68| Step: 0
Training loss: 2.976886881777165
Validation loss: 2.508902734093687

Epoch: 5| Step: 1
Training loss: 3.0017247010727472
Validation loss: 2.5009028906515685

Epoch: 5| Step: 2
Training loss: 2.3160071712705705
Validation loss: 2.5128267909009536

Epoch: 5| Step: 3
Training loss: 2.7817437237335954
Validation loss: 2.5132651001462625

Epoch: 5| Step: 4
Training loss: 2.753474294996557
Validation loss: 2.5077823993239976

Epoch: 5| Step: 5
Training loss: 2.7110071530587203
Validation loss: 2.5085507719688622

Epoch: 5| Step: 6
Training loss: 2.7799585322848155
Validation loss: 2.5178119312797844

Epoch: 5| Step: 7
Training loss: 2.8912093937880092
Validation loss: 2.507255639835023

Epoch: 5| Step: 8
Training loss: 2.976272689047136
Validation loss: 2.5200769612959646

Epoch: 5| Step: 9
Training loss: 2.2440394817829703
Validation loss: 2.5254699756996284

Epoch: 5| Step: 10
Training loss: 2.668021116399057
Validation loss: 2.5104178485366337

Epoch: 69| Step: 0
Training loss: 1.9735493366268657
Validation loss: 2.507569287039836

Epoch: 5| Step: 1
Training loss: 3.176192929094067
Validation loss: 2.523411284757269

Epoch: 5| Step: 2
Training loss: 2.296061572887469
Validation loss: 2.513305329334112

Epoch: 5| Step: 3
Training loss: 2.6675933975986874
Validation loss: 2.5069717231178155

Epoch: 5| Step: 4
Training loss: 2.7636488411418654
Validation loss: 2.508315598560035

Epoch: 5| Step: 5
Training loss: 2.927287754147353
Validation loss: 2.506716684335718

Epoch: 5| Step: 6
Training loss: 2.5889898535160167
Validation loss: 2.5255156858819774

Epoch: 5| Step: 7
Training loss: 2.8717928703444358
Validation loss: 2.512221056088036

Epoch: 5| Step: 8
Training loss: 3.1054162362894853
Validation loss: 2.5114294606473466

Epoch: 5| Step: 9
Training loss: 2.1596212155626393
Validation loss: 2.509796953171356

Epoch: 5| Step: 10
Training loss: 3.3478738898769067
Validation loss: 2.506569457022679

Epoch: 70| Step: 0
Training loss: 2.928572378922684
Validation loss: 2.5134235385650836

Epoch: 5| Step: 1
Training loss: 2.85384831196108
Validation loss: 2.520212889506102

Epoch: 5| Step: 2
Training loss: 2.4573745366431803
Validation loss: 2.5193030189520997

Epoch: 5| Step: 3
Training loss: 2.4715835148651903
Validation loss: 2.5109491047023518

Epoch: 5| Step: 4
Training loss: 3.198326275135049
Validation loss: 2.51918232880084

Epoch: 5| Step: 5
Training loss: 2.675283210236697
Validation loss: 2.513771416258827

Epoch: 5| Step: 6
Training loss: 2.179492900734436
Validation loss: 2.5196085854487094

Epoch: 5| Step: 7
Training loss: 2.0869373682601324
Validation loss: 2.515969251536063

Epoch: 5| Step: 8
Training loss: 2.614732735508011
Validation loss: 2.5082795018146

Epoch: 5| Step: 9
Training loss: 3.598798307549737
Validation loss: 2.5135675208756436

Epoch: 5| Step: 10
Training loss: 2.809179762186695
Validation loss: 2.512538421929701

Epoch: 71| Step: 0
Training loss: 3.092449955454372
Validation loss: 2.5151530063587755

Epoch: 5| Step: 1
Training loss: 2.601725438602916
Validation loss: 2.5243643376722074

Epoch: 5| Step: 2
Training loss: 1.7451874089642139
Validation loss: 2.5126491150359223

Epoch: 5| Step: 3
Training loss: 2.5797309266472244
Validation loss: 2.513321707862847

Epoch: 5| Step: 4
Training loss: 2.5928525176264117
Validation loss: 2.508380420643352

Epoch: 5| Step: 5
Training loss: 3.0116449685960855
Validation loss: 2.5189559367444287

Epoch: 5| Step: 6
Training loss: 2.8158634842651415
Validation loss: 2.4990750898909764

Epoch: 5| Step: 7
Training loss: 2.9882163046966306
Validation loss: 2.5024916896215146

Epoch: 5| Step: 8
Training loss: 3.2155621491898945
Validation loss: 2.511509551529364

Epoch: 5| Step: 9
Training loss: 2.9002719225499263
Validation loss: 2.516769165288466

Epoch: 5| Step: 10
Training loss: 2.141808732462789
Validation loss: 2.5217318118873937

Epoch: 72| Step: 0
Training loss: 2.4873733181540616
Validation loss: 2.5104301039371393

Epoch: 5| Step: 1
Training loss: 3.0313472142312468
Validation loss: 2.510150767862111

Epoch: 5| Step: 2
Training loss: 2.61855342008808
Validation loss: 2.511077908829332

Epoch: 5| Step: 3
Training loss: 3.1745084907777485
Validation loss: 2.507238473212963

Epoch: 5| Step: 4
Training loss: 3.1480150720315483
Validation loss: 2.5147679425030263

Epoch: 5| Step: 5
Training loss: 2.855889035043371
Validation loss: 2.509747436707791

Epoch: 5| Step: 6
Training loss: 2.6924742861965894
Validation loss: 2.504568563759259

Epoch: 5| Step: 7
Training loss: 2.382239801342395
Validation loss: 2.515918131438472

Epoch: 5| Step: 8
Training loss: 2.4202145698884916
Validation loss: 2.5077001979502067

Epoch: 5| Step: 9
Training loss: 2.997706331178196
Validation loss: 2.5127584014433513

Epoch: 5| Step: 10
Training loss: 1.7893535926916024
Validation loss: 2.5080283831757475

Epoch: 73| Step: 0
Training loss: 2.5654684411626114
Validation loss: 2.510518204015045

Epoch: 5| Step: 1
Training loss: 2.953107399862814
Validation loss: 2.5007911476265154

Epoch: 5| Step: 2
Training loss: 2.5988278607864737
Validation loss: 2.5102277243901754

Epoch: 5| Step: 3
Training loss: 2.962265642771135
Validation loss: 2.5181127227062916

Epoch: 5| Step: 4
Training loss: 2.51816066160914
Validation loss: 2.50497321694698

Epoch: 5| Step: 5
Training loss: 2.625594752917722
Validation loss: 2.516098057471038

Epoch: 5| Step: 6
Training loss: 2.9133028160027394
Validation loss: 2.5170480011268324

Epoch: 5| Step: 7
Training loss: 2.6635643277915575
Validation loss: 2.514576526238456

Epoch: 5| Step: 8
Training loss: 3.0188061631943084
Validation loss: 2.517171621879488

Epoch: 5| Step: 9
Training loss: 2.4731907560617197
Validation loss: 2.515284369541614

Epoch: 5| Step: 10
Training loss: 2.6684377769061753
Validation loss: 2.516199811474197

Epoch: 74| Step: 0
Training loss: 3.0883154097351073
Validation loss: 2.5211493815122803

Epoch: 5| Step: 1
Training loss: 2.4622924949626745
Validation loss: 2.51471319420874

Epoch: 5| Step: 2
Training loss: 3.1263196066360384
Validation loss: 2.5164890414329117

Epoch: 5| Step: 3
Training loss: 2.413909709749416
Validation loss: 2.505961583105713

Epoch: 5| Step: 4
Training loss: 2.6593851772880135
Validation loss: 2.5224753951928056

Epoch: 5| Step: 5
Training loss: 1.9796875385383776
Validation loss: 2.5152572641267206

Epoch: 5| Step: 6
Training loss: 2.87235669235902
Validation loss: 2.5206883781274314

Epoch: 5| Step: 7
Training loss: 2.9507365584176806
Validation loss: 2.515388356877298

Epoch: 5| Step: 8
Training loss: 2.6840874052696564
Validation loss: 2.509294449938272

Epoch: 5| Step: 9
Training loss: 2.411655448179201
Validation loss: 2.5076099592911727

Epoch: 5| Step: 10
Training loss: 3.158380676827266
Validation loss: 2.513286364910664

Epoch: 75| Step: 0
Training loss: 3.2823382798136276
Validation loss: 2.5122459533374784

Epoch: 5| Step: 1
Training loss: 2.5258428960770387
Validation loss: 2.512083569804725

Epoch: 5| Step: 2
Training loss: 2.9480162335263294
Validation loss: 2.504934416733132

Epoch: 5| Step: 3
Training loss: 2.881586408525235
Validation loss: 2.5134434607268994

Epoch: 5| Step: 4
Training loss: 2.7864175858890317
Validation loss: 2.5126081361265777

Epoch: 5| Step: 5
Training loss: 2.522662820391045
Validation loss: 2.5163660422273546

Epoch: 5| Step: 6
Training loss: 2.8173455776352325
Validation loss: 2.509518424145905

Epoch: 5| Step: 7
Training loss: 2.3506419258755855
Validation loss: 2.5137023069790625

Epoch: 5| Step: 8
Training loss: 2.432107192782892
Validation loss: 2.5120648880297467

Epoch: 5| Step: 9
Training loss: 2.6440369723067967
Validation loss: 2.5154891622315505

Epoch: 5| Step: 10
Training loss: 2.698060246676498
Validation loss: 2.5091566050825156

Epoch: 76| Step: 0
Training loss: 2.556023018233359
Validation loss: 2.5136064428046

Epoch: 5| Step: 1
Training loss: 2.6499657286821083
Validation loss: 2.52308304491443

Epoch: 5| Step: 2
Training loss: 2.379987499140118
Validation loss: 2.5105384126890553

Epoch: 5| Step: 3
Training loss: 2.9542261724751553
Validation loss: 2.502269528932997

Epoch: 5| Step: 4
Training loss: 2.87051024443661
Validation loss: 2.5223072613984523

Epoch: 5| Step: 5
Training loss: 2.5796660470330433
Validation loss: 2.501300112748345

Epoch: 5| Step: 6
Training loss: 2.5725441378559553
Validation loss: 2.5116245136825164

Epoch: 5| Step: 7
Training loss: 2.9855753297209695
Validation loss: 2.5139578103130957

Epoch: 5| Step: 8
Training loss: 3.067685777530529
Validation loss: 2.519310067851619

Epoch: 5| Step: 9
Training loss: 2.450711660762146
Validation loss: 2.5086338703156437

Epoch: 5| Step: 10
Training loss: 2.820358825472203
Validation loss: 2.5163996854006268

Epoch: 77| Step: 0
Training loss: 2.2638587117540414
Validation loss: 2.5111880405109215

Epoch: 5| Step: 1
Training loss: 2.5431495051446698
Validation loss: 2.5153925396001733

Epoch: 5| Step: 2
Training loss: 2.2582048649394353
Validation loss: 2.5071529788422646

Epoch: 5| Step: 3
Training loss: 2.851689249644513
Validation loss: 2.5079438183426053

Epoch: 5| Step: 4
Training loss: 3.564200681343183
Validation loss: 2.5153138800082737

Epoch: 5| Step: 5
Training loss: 2.880866823348422
Validation loss: 2.508441958373127

Epoch: 5| Step: 6
Training loss: 2.4820766734192428
Validation loss: 2.5122195998800967

Epoch: 5| Step: 7
Training loss: 2.4735622584193946
Validation loss: 2.508126021350746

Epoch: 5| Step: 8
Training loss: 3.112719706552072
Validation loss: 2.5147339461790144

Epoch: 5| Step: 9
Training loss: 2.4606664114701933
Validation loss: 2.509967942731961

Epoch: 5| Step: 10
Training loss: 2.7075381554392135
Validation loss: 2.508961102693866

Epoch: 78| Step: 0
Training loss: 2.6073646339105467
Validation loss: 2.4981891154429907

Epoch: 5| Step: 1
Training loss: 2.517454062897925
Validation loss: 2.50092557980301

Epoch: 5| Step: 2
Training loss: 2.5778019558203837
Validation loss: 2.506747608806704

Epoch: 5| Step: 3
Training loss: 2.945789733592847
Validation loss: 2.5039745187954554

Epoch: 5| Step: 4
Training loss: 2.256708422978377
Validation loss: 2.5147362827528754

Epoch: 5| Step: 5
Training loss: 2.4167901588545155
Validation loss: 2.513157336322302

Epoch: 5| Step: 6
Training loss: 3.1495600135755106
Validation loss: 2.5136346068228477

Epoch: 5| Step: 7
Training loss: 2.9796810284100705
Validation loss: 2.4989950077638117

Epoch: 5| Step: 8
Training loss: 2.797553464976907
Validation loss: 2.5098356506351513

Epoch: 5| Step: 9
Training loss: 2.4610144224709174
Validation loss: 2.5134236793222593

Epoch: 5| Step: 10
Training loss: 3.188092943279765
Validation loss: 2.508884229932948

Epoch: 79| Step: 0
Training loss: 2.344742831706364
Validation loss: 2.512480376221086

Epoch: 5| Step: 1
Training loss: 2.6633624726071257
Validation loss: 2.5090756932181386

Epoch: 5| Step: 2
Training loss: 2.8606792873026237
Validation loss: 2.5019506914414293

Epoch: 5| Step: 3
Training loss: 2.713202593822384
Validation loss: 2.4934889688520245

Epoch: 5| Step: 4
Training loss: 2.43954372314126
Validation loss: 2.509304339567785

Epoch: 5| Step: 5
Training loss: 2.4522579142850836
Validation loss: 2.508910293487916

Epoch: 5| Step: 6
Training loss: 2.7879048998524567
Validation loss: 2.51298520798669

Epoch: 5| Step: 7
Training loss: 3.3969211045323795
Validation loss: 2.5087069196584006

Epoch: 5| Step: 8
Training loss: 3.109294181041777
Validation loss: 2.5118294812693667

Epoch: 5| Step: 9
Training loss: 2.3431645997436243
Validation loss: 2.506633278507489

Epoch: 5| Step: 10
Training loss: 2.545938612222575
Validation loss: 2.50393870706386

Epoch: 80| Step: 0
Training loss: 2.853423548576482
Validation loss: 2.522788095760178

Epoch: 5| Step: 1
Training loss: 2.6301228625822466
Validation loss: 2.516982920499305

Epoch: 5| Step: 2
Training loss: 2.8711492208714513
Validation loss: 2.5122234296949633

Epoch: 5| Step: 3
Training loss: 2.7684548765352606
Validation loss: 2.516283184966222

Epoch: 5| Step: 4
Training loss: 2.988058642903478
Validation loss: 2.514897713137992

Epoch: 5| Step: 5
Training loss: 2.6516036741333977
Validation loss: 2.502954485638063

Epoch: 5| Step: 6
Training loss: 2.6028651736349278
Validation loss: 2.510359408621261

Epoch: 5| Step: 7
Training loss: 2.612026580552623
Validation loss: 2.5144671516450536

Epoch: 5| Step: 8
Training loss: 2.193100001012475
Validation loss: 2.510752425736474

Epoch: 5| Step: 9
Training loss: 2.664929648753677
Validation loss: 2.507809640732433

Epoch: 5| Step: 10
Training loss: 2.99129670032945
Validation loss: 2.501654523522818

Epoch: 81| Step: 0
Training loss: 3.243459429065387
Validation loss: 2.5104950582969967

Epoch: 5| Step: 1
Training loss: 2.2490746396622163
Validation loss: 2.506013144153213

Epoch: 5| Step: 2
Training loss: 2.413363457363529
Validation loss: 2.5054361265701606

Epoch: 5| Step: 3
Training loss: 3.018676636872603
Validation loss: 2.5066196130949945

Epoch: 5| Step: 4
Training loss: 2.040112221566969
Validation loss: 2.5168705957520356

Epoch: 5| Step: 5
Training loss: 3.054250076077871
Validation loss: 2.5053490490975525

Epoch: 5| Step: 6
Training loss: 2.8035390728792065
Validation loss: 2.513075811639194

Epoch: 5| Step: 7
Training loss: 2.4854629341758177
Validation loss: 2.508607601545083

Epoch: 5| Step: 8
Training loss: 2.7761270725837326
Validation loss: 2.516131879475184

Epoch: 5| Step: 9
Training loss: 2.8064953601172875
Validation loss: 2.514760982815597

Epoch: 5| Step: 10
Training loss: 2.741259121581773
Validation loss: 2.510158191749338

Epoch: 82| Step: 0
Training loss: 2.6275620446107086
Validation loss: 2.5123188231623983

Epoch: 5| Step: 1
Training loss: 2.407745293057435
Validation loss: 2.506813668981368

Epoch: 5| Step: 2
Training loss: 3.1276927790395512
Validation loss: 2.5056741372807414

Epoch: 5| Step: 3
Training loss: 3.078422880524287
Validation loss: 2.4988429704307897

Epoch: 5| Step: 4
Training loss: 3.016823485989473
Validation loss: 2.4945884853232765

Epoch: 5| Step: 5
Training loss: 2.135664489756098
Validation loss: 2.5064324708071632

Epoch: 5| Step: 6
Training loss: 2.7796525117796067
Validation loss: 2.5047653680199393

Epoch: 5| Step: 7
Training loss: 3.049948213481572
Validation loss: 2.492869304414603

Epoch: 5| Step: 8
Training loss: 2.3736194814563314
Validation loss: 2.5048202354912497

Epoch: 5| Step: 9
Training loss: 2.3539132063364376
Validation loss: 2.514091249871312

Epoch: 5| Step: 10
Training loss: 2.66240069781739
Validation loss: 2.5117078859452815

Epoch: 83| Step: 0
Training loss: 2.968886362256836
Validation loss: 2.5147803693761404

Epoch: 5| Step: 1
Training loss: 2.409744598499762
Validation loss: 2.503337195527955

Epoch: 5| Step: 2
Training loss: 2.319436675443855
Validation loss: 2.4982372293734447

Epoch: 5| Step: 3
Training loss: 2.3101077722617425
Validation loss: 2.5004843334986213

Epoch: 5| Step: 4
Training loss: 2.634567717067954
Validation loss: 2.4963820920295814

Epoch: 5| Step: 5
Training loss: 3.189363720137773
Validation loss: 2.4883717290770813

Epoch: 5| Step: 6
Training loss: 2.862552565712512
Validation loss: 2.507076408536745

Epoch: 5| Step: 7
Training loss: 3.078633842429852
Validation loss: 2.4963322124250644

Epoch: 5| Step: 8
Training loss: 2.2488337779276377
Validation loss: 2.5111082425876856

Epoch: 5| Step: 9
Training loss: 2.927687305481786
Validation loss: 2.506067830859903

Epoch: 5| Step: 10
Training loss: 2.6712372671278155
Validation loss: 2.510110919116005

Epoch: 84| Step: 0
Training loss: 2.565595478440937
Validation loss: 2.512774960039035

Epoch: 5| Step: 1
Training loss: 2.7069182489322774
Validation loss: 2.5134789524237515

Epoch: 5| Step: 2
Training loss: 2.942863761825631
Validation loss: 2.515916492937031

Epoch: 5| Step: 3
Training loss: 3.126124065414436
Validation loss: 2.5021980692607997

Epoch: 5| Step: 4
Training loss: 2.280338993804706
Validation loss: 2.502942619722269

Epoch: 5| Step: 5
Training loss: 3.178054127610935
Validation loss: 2.4986911608512696

Epoch: 5| Step: 6
Training loss: 2.635537930037066
Validation loss: 2.5046211949607855

Epoch: 5| Step: 7
Training loss: 2.9609912877494136
Validation loss: 2.5009914872685237

Epoch: 5| Step: 8
Training loss: 2.4789767366204885
Validation loss: 2.485576132640897

Epoch: 5| Step: 9
Training loss: 2.351923208358457
Validation loss: 2.5006991177974376

Epoch: 5| Step: 10
Training loss: 2.236532916557886
Validation loss: 2.4953876946019897

Epoch: 85| Step: 0
Training loss: 2.6953754804688885
Validation loss: 2.503465038537658

Epoch: 5| Step: 1
Training loss: 2.228314590413319
Validation loss: 2.5065293764447527

Epoch: 5| Step: 2
Training loss: 2.573540976041569
Validation loss: 2.5068264707192593

Epoch: 5| Step: 3
Training loss: 2.5728255859398055
Validation loss: 2.4943892100610587

Epoch: 5| Step: 4
Training loss: 2.7756928206234295
Validation loss: 2.4965248175400623

Epoch: 5| Step: 5
Training loss: 2.418276568464507
Validation loss: 2.50513362169782

Epoch: 5| Step: 6
Training loss: 2.5955307144761006
Validation loss: 2.504748297940339

Epoch: 5| Step: 7
Training loss: 3.2843661089485536
Validation loss: 2.5071249189118285

Epoch: 5| Step: 8
Training loss: 2.7647701170624326
Validation loss: 2.5147667681144665

Epoch: 5| Step: 9
Training loss: 3.0014082464389933
Validation loss: 2.506806067471144

Epoch: 5| Step: 10
Training loss: 2.799155472368405
Validation loss: 2.5020827569875754

Epoch: 86| Step: 0
Training loss: 2.6929293061619424
Validation loss: 2.5049627596157342

Epoch: 5| Step: 1
Training loss: 2.6207505298465863
Validation loss: 2.515648874036942

Epoch: 5| Step: 2
Training loss: 3.1348512631896552
Validation loss: 2.4996269870707555

Epoch: 5| Step: 3
Training loss: 2.5059048536281283
Validation loss: 2.4880867491814564

Epoch: 5| Step: 4
Training loss: 2.915870739556163
Validation loss: 2.500460883696207

Epoch: 5| Step: 5
Training loss: 2.258456867494432
Validation loss: 2.5085293239851794

Epoch: 5| Step: 6
Training loss: 2.9162304869978812
Validation loss: 2.504890062661121

Epoch: 5| Step: 7
Training loss: 2.600639752394518
Validation loss: 2.507316964891619

Epoch: 5| Step: 8
Training loss: 2.4367493671867666
Validation loss: 2.5018148151943356

Epoch: 5| Step: 9
Training loss: 3.0685037424761816
Validation loss: 2.5052737672474406

Epoch: 5| Step: 10
Training loss: 2.3424639415917317
Validation loss: 2.509987077263436

Epoch: 87| Step: 0
Training loss: 2.473170126116941
Validation loss: 2.4968334632704288

Epoch: 5| Step: 1
Training loss: 2.554441667680345
Validation loss: 2.497372552994015

Epoch: 5| Step: 2
Training loss: 2.912802088710389
Validation loss: 2.516212323990977

Epoch: 5| Step: 3
Training loss: 3.2688311302372
Validation loss: 2.5139153072603944

Epoch: 5| Step: 4
Training loss: 2.9853517222256256
Validation loss: 2.506756926572217

Epoch: 5| Step: 5
Training loss: 1.7698585594906777
Validation loss: 2.500349981154953

Epoch: 5| Step: 6
Training loss: 2.6949631616626055
Validation loss: 2.5147046807133497

Epoch: 5| Step: 7
Training loss: 2.530417411356435
Validation loss: 2.4976129200231703

Epoch: 5| Step: 8
Training loss: 2.1969890233886877
Validation loss: 2.502770908451252

Epoch: 5| Step: 9
Training loss: 3.166451764762011
Validation loss: 2.490726068811685

Epoch: 5| Step: 10
Training loss: 2.658753236554341
Validation loss: 2.482262284543547

Epoch: 88| Step: 0
Training loss: 2.411993133585758
Validation loss: 2.504000178879272

Epoch: 5| Step: 1
Training loss: 2.3200488213630983
Validation loss: 2.5091975938732376

Epoch: 5| Step: 2
Training loss: 2.157105980849334
Validation loss: 2.5091500517998404

Epoch: 5| Step: 3
Training loss: 3.447746956481542
Validation loss: 2.5034914605989194

Epoch: 5| Step: 4
Training loss: 2.285078728819976
Validation loss: 2.4921720143409036

Epoch: 5| Step: 5
Training loss: 2.820116115506194
Validation loss: 2.491524041401753

Epoch: 5| Step: 6
Training loss: 2.8366230958251317
Validation loss: 2.509043491632129

Epoch: 5| Step: 7
Training loss: 2.960638267675801
Validation loss: 2.4986479425575916

Epoch: 5| Step: 8
Training loss: 2.66536835019669
Validation loss: 2.497741195609247

Epoch: 5| Step: 9
Training loss: 3.0852130280923777
Validation loss: 2.510349280095828

Epoch: 5| Step: 10
Training loss: 2.179560503797859
Validation loss: 2.4987269082575447

Epoch: 89| Step: 0
Training loss: 2.592678078372344
Validation loss: 2.498219728841462

Epoch: 5| Step: 1
Training loss: 2.0763093287415977
Validation loss: 2.494417639875799

Epoch: 5| Step: 2
Training loss: 2.6374651721263898
Validation loss: 2.5036661799820394

Epoch: 5| Step: 3
Training loss: 3.0197629062475633
Validation loss: 2.502725528573806

Epoch: 5| Step: 4
Training loss: 2.627980311489819
Validation loss: 2.500136429130215

Epoch: 5| Step: 5
Training loss: 2.496610728232661
Validation loss: 2.488248172457152

Epoch: 5| Step: 6
Training loss: 2.4081080805453667
Validation loss: 2.5054562831222276

Epoch: 5| Step: 7
Training loss: 2.6912109126943626
Validation loss: 2.5145950925007163

Epoch: 5| Step: 8
Training loss: 2.863908852442776
Validation loss: 2.49824103855004

Epoch: 5| Step: 9
Training loss: 3.425259244811265
Validation loss: 2.500121236753264

Epoch: 5| Step: 10
Training loss: 2.5557258199096267
Validation loss: 2.4998404575015787

Epoch: 90| Step: 0
Training loss: 2.6601232675084523
Validation loss: 2.5128161499651998

Epoch: 5| Step: 1
Training loss: 2.3182757650980115
Validation loss: 2.5010297039054232

Epoch: 5| Step: 2
Training loss: 2.0549219962210192
Validation loss: 2.515911015973287

Epoch: 5| Step: 3
Training loss: 2.975502766672809
Validation loss: 2.496828381846056

Epoch: 5| Step: 4
Training loss: 2.9644522628350636
Validation loss: 2.506769005560121

Epoch: 5| Step: 5
Training loss: 2.5252633590080524
Validation loss: 2.5034517700560817

Epoch: 5| Step: 6
Training loss: 3.0442960339436698
Validation loss: 2.497202310713128

Epoch: 5| Step: 7
Training loss: 2.890984260297197
Validation loss: 2.5087677177188885

Epoch: 5| Step: 8
Training loss: 2.7422932226770764
Validation loss: 2.512208183874524

Epoch: 5| Step: 9
Training loss: 2.583654957920921
Validation loss: 2.508083549283267

Epoch: 5| Step: 10
Training loss: 2.7043889789471267
Validation loss: 2.5009848838894797

Epoch: 91| Step: 0
Training loss: 2.8906056996294014
Validation loss: 2.5068668589659193

Epoch: 5| Step: 1
Training loss: 2.0009685793596854
Validation loss: 2.503642226496247

Epoch: 5| Step: 2
Training loss: 2.6931736517615197
Validation loss: 2.4959004958494475

Epoch: 5| Step: 3
Training loss: 2.8130358715299315
Validation loss: 2.5064734489144986

Epoch: 5| Step: 4
Training loss: 3.4340379794313933
Validation loss: 2.501083674441543

Epoch: 5| Step: 5
Training loss: 2.61736677324436
Validation loss: 2.496278363556679

Epoch: 5| Step: 6
Training loss: 2.617554778333803
Validation loss: 2.504576960730699

Epoch: 5| Step: 7
Training loss: 2.093193094006472
Validation loss: 2.509047163835394

Epoch: 5| Step: 8
Training loss: 2.2808229229538144
Validation loss: 2.497617602621311

Epoch: 5| Step: 9
Training loss: 3.0263147192632323
Validation loss: 2.5045742231583525

Epoch: 5| Step: 10
Training loss: 2.8359238805522655
Validation loss: 2.5080280100823216

Epoch: 92| Step: 0
Training loss: 3.278898141054275
Validation loss: 2.503560534303608

Epoch: 5| Step: 1
Training loss: 2.686721777796391
Validation loss: 2.510275320605434

Epoch: 5| Step: 2
Training loss: 2.7168132855531955
Validation loss: 2.506703205000525

Epoch: 5| Step: 3
Training loss: 2.578029746405586
Validation loss: 2.494948795013543

Epoch: 5| Step: 4
Training loss: 2.45988446669921
Validation loss: 2.5091407674425588

Epoch: 5| Step: 5
Training loss: 2.3577481149183175
Validation loss: 2.499912388609771

Epoch: 5| Step: 6
Training loss: 2.5861511142239557
Validation loss: 2.517236519402292

Epoch: 5| Step: 7
Training loss: 3.186555311047685
Validation loss: 2.503109124565655

Epoch: 5| Step: 8
Training loss: 2.514296281470349
Validation loss: 2.5031575740755208

Epoch: 5| Step: 9
Training loss: 2.4130966081310006
Validation loss: 2.496714947058694

Epoch: 5| Step: 10
Training loss: 2.5464943443368857
Validation loss: 2.4960147574857223

Epoch: 93| Step: 0
Training loss: 3.270068344428582
Validation loss: 2.4907188700559724

Epoch: 5| Step: 1
Training loss: 2.5620491747756335
Validation loss: 2.49415965379796

Epoch: 5| Step: 2
Training loss: 2.6375808775736505
Validation loss: 2.497393431604563

Epoch: 5| Step: 3
Training loss: 2.6603915968878535
Validation loss: 2.5026194621162667

Epoch: 5| Step: 4
Training loss: 2.2820356923353606
Validation loss: 2.5015151826207895

Epoch: 5| Step: 5
Training loss: 2.539550171015717
Validation loss: 2.484010817926039

Epoch: 5| Step: 6
Training loss: 2.1996321890882005
Validation loss: 2.5024163894263243

Epoch: 5| Step: 7
Training loss: 3.1012164098172863
Validation loss: 2.50169051500607

Epoch: 5| Step: 8
Training loss: 2.5778509745765454
Validation loss: 2.4908672462660326

Epoch: 5| Step: 9
Training loss: 2.572894066674028
Validation loss: 2.4916331215649685

Epoch: 5| Step: 10
Training loss: 2.9763813113837405
Validation loss: 2.500970824230815

Epoch: 94| Step: 0
Training loss: 3.0233515448759527
Validation loss: 2.498139681893654

Epoch: 5| Step: 1
Training loss: 2.471611392774376
Validation loss: 2.4982556102387985

Epoch: 5| Step: 2
Training loss: 1.9245305516906894
Validation loss: 2.5029191601751632

Epoch: 5| Step: 3
Training loss: 2.2733085045153154
Validation loss: 2.505527282540895

Epoch: 5| Step: 4
Training loss: 3.4563415184114157
Validation loss: 2.4898430006696897

Epoch: 5| Step: 5
Training loss: 2.7630152053607904
Validation loss: 2.48779580207496

Epoch: 5| Step: 6
Training loss: 2.4646462241946634
Validation loss: 2.489646269979775

Epoch: 5| Step: 7
Training loss: 2.524410284692531
Validation loss: 2.5053612591995145

Epoch: 5| Step: 8
Training loss: 2.716186454861754
Validation loss: 2.5049930814577457

Epoch: 5| Step: 9
Training loss: 2.804151138406264
Validation loss: 2.503157665226

Epoch: 5| Step: 10
Training loss: 2.6986055022546953
Validation loss: 2.4956340978637654

Epoch: 95| Step: 0
Training loss: 2.781105980733963
Validation loss: 2.5243868641911353

Epoch: 5| Step: 1
Training loss: 2.3596962211163808
Validation loss: 2.49633195927869

Epoch: 5| Step: 2
Training loss: 2.6892959449065423
Validation loss: 2.5060667935646985

Epoch: 5| Step: 3
Training loss: 2.8715411402063284
Validation loss: 2.501109881809583

Epoch: 5| Step: 4
Training loss: 2.6479569170694153
Validation loss: 2.5158542881931973

Epoch: 5| Step: 5
Training loss: 2.3629159187858777
Validation loss: 2.494233153774917

Epoch: 5| Step: 6
Training loss: 2.44397862528368
Validation loss: 2.5019031950767983

Epoch: 5| Step: 7
Training loss: 3.3587753825334166
Validation loss: 2.482939354038439

Epoch: 5| Step: 8
Training loss: 2.9492421938741216
Validation loss: 2.5009447322634477

Epoch: 5| Step: 9
Training loss: 2.2811025284431596
Validation loss: 2.498430578798161

Epoch: 5| Step: 10
Training loss: 2.5114115620617854
Validation loss: 2.5090881707618338

Epoch: 96| Step: 0
Training loss: 2.466761500451526
Validation loss: 2.50457541665633

Epoch: 5| Step: 1
Training loss: 3.191992342860854
Validation loss: 2.5110810002105426

Epoch: 5| Step: 2
Training loss: 2.625374449407793
Validation loss: 2.510958153673503

Epoch: 5| Step: 3
Training loss: 2.425848181349422
Validation loss: 2.4917480480427474

Epoch: 5| Step: 4
Training loss: 2.4321477767129562
Validation loss: 2.5037544627718114

Epoch: 5| Step: 5
Training loss: 2.6583905739037688
Validation loss: 2.4906614038066794

Epoch: 5| Step: 6
Training loss: 2.836969361283331
Validation loss: 2.515018454979202

Epoch: 5| Step: 7
Training loss: 3.3881471079175616
Validation loss: 2.5105231362127434

Epoch: 5| Step: 8
Training loss: 2.7561860299850527
Validation loss: 2.4920836488848397

Epoch: 5| Step: 9
Training loss: 1.9394601935097462
Validation loss: 2.5024660007519763

Epoch: 5| Step: 10
Training loss: 2.3641473869619576
Validation loss: 2.507188885769629

Epoch: 97| Step: 0
Training loss: 2.506115681005087
Validation loss: 2.506831319163554

Epoch: 5| Step: 1
Training loss: 2.7269830679960605
Validation loss: 2.501764172370804

Epoch: 5| Step: 2
Training loss: 2.094997660560485
Validation loss: 2.5051622949132

Epoch: 5| Step: 3
Training loss: 3.2166619796587015
Validation loss: 2.499321689670704

Epoch: 5| Step: 4
Training loss: 2.4359094369775427
Validation loss: 2.505110263514528

Epoch: 5| Step: 5
Training loss: 2.4336276472985348
Validation loss: 2.502463892441754

Epoch: 5| Step: 6
Training loss: 2.7376509742387705
Validation loss: 2.4930725840506676

Epoch: 5| Step: 7
Training loss: 3.720689283930948
Validation loss: 2.4916996688816484

Epoch: 5| Step: 8
Training loss: 1.9514950474122303
Validation loss: 2.5033835525454466

Epoch: 5| Step: 9
Training loss: 2.7322040768272857
Validation loss: 2.510089635620806

Epoch: 5| Step: 10
Training loss: 2.1586124912239497
Validation loss: 2.4907181423567466

Epoch: 98| Step: 0
Training loss: 2.7859486526719595
Validation loss: 2.49248235877151

Epoch: 5| Step: 1
Training loss: 3.084742833923183
Validation loss: 2.4964627704022515

Epoch: 5| Step: 2
Training loss: 2.621151418946903
Validation loss: 2.492997287802818

Epoch: 5| Step: 3
Training loss: 2.9230138833665356
Validation loss: 2.501092864681735

Epoch: 5| Step: 4
Training loss: 3.080633545157681
Validation loss: 2.4969613918318414

Epoch: 5| Step: 5
Training loss: 2.543782890538333
Validation loss: 2.4993894241603156

Epoch: 5| Step: 6
Training loss: 2.429946738393297
Validation loss: 2.4855008289904315

Epoch: 5| Step: 7
Training loss: 2.4256117019740944
Validation loss: 2.495428056866719

Epoch: 5| Step: 8
Training loss: 2.708811605234404
Validation loss: 2.489775011617363

Epoch: 5| Step: 9
Training loss: 2.403818760055564
Validation loss: 2.4970625827696344

Epoch: 5| Step: 10
Training loss: 2.09013555753323
Validation loss: 2.4992314275024032

Epoch: 99| Step: 0
Training loss: 2.924410123114791
Validation loss: 2.4944491114734393

Epoch: 5| Step: 1
Training loss: 2.052421916261633
Validation loss: 2.509833451478922

Epoch: 5| Step: 2
Training loss: 2.6655102149425605
Validation loss: 2.5029653293049257

Epoch: 5| Step: 3
Training loss: 2.653474232544401
Validation loss: 2.496198256408091

Epoch: 5| Step: 4
Training loss: 2.3105022977525733
Validation loss: 2.498497894182466

Epoch: 5| Step: 5
Training loss: 2.8281492790739606
Validation loss: 2.498205642332653

Epoch: 5| Step: 6
Training loss: 3.0136479829841174
Validation loss: 2.502757420156841

Epoch: 5| Step: 7
Training loss: 2.6193580850079163
Validation loss: 2.500655869620045

Epoch: 5| Step: 8
Training loss: 2.742859065319137
Validation loss: 2.501155560571031

Epoch: 5| Step: 9
Training loss: 2.3268784635745
Validation loss: 2.5135175768813816

Epoch: 5| Step: 10
Training loss: 3.152299126707477
Validation loss: 2.501788140763942

Epoch: 100| Step: 0
Training loss: 2.6558567429193
Validation loss: 2.497969106522487

Epoch: 5| Step: 1
Training loss: 2.105636335225222
Validation loss: 2.4983108053832366

Epoch: 5| Step: 2
Training loss: 2.501948741518896
Validation loss: 2.5043596667902435

Epoch: 5| Step: 3
Training loss: 3.2857127041546352
Validation loss: 2.501215743743386

Epoch: 5| Step: 4
Training loss: 2.1691642695367412
Validation loss: 2.486596479814549

Epoch: 5| Step: 5
Training loss: 2.9892554197454486
Validation loss: 2.4920714868872342

Epoch: 5| Step: 6
Training loss: 2.5224595197259374
Validation loss: 2.4932621943463715

Epoch: 5| Step: 7
Training loss: 2.8823125969214494
Validation loss: 2.4974443745175288

Epoch: 5| Step: 8
Training loss: 3.093516331334995
Validation loss: 2.4983401356451056

Epoch: 5| Step: 9
Training loss: 2.6358547123072036
Validation loss: 2.4899700199329957

Epoch: 5| Step: 10
Training loss: 1.9378892446035134
Validation loss: 2.4925392754607754

Epoch: 101| Step: 0
Training loss: 2.2230015169757125
Validation loss: 2.4988634499828324

Epoch: 5| Step: 1
Training loss: 3.0223544445597055
Validation loss: 2.5110817608040814

Epoch: 5| Step: 2
Training loss: 2.4933796007620046
Validation loss: 2.490744757277258

Epoch: 5| Step: 3
Training loss: 2.7791497519310555
Validation loss: 2.5110331099188077

Epoch: 5| Step: 4
Training loss: 2.245533005751515
Validation loss: 2.4992756029919785

Epoch: 5| Step: 5
Training loss: 2.6026631910771214
Validation loss: 2.4939379854559713

Epoch: 5| Step: 6
Training loss: 3.182455521697516
Validation loss: 2.5007274892043028

Epoch: 5| Step: 7
Training loss: 2.63411311502961
Validation loss: 2.491897679824133

Epoch: 5| Step: 8
Training loss: 2.7805197110587843
Validation loss: 2.492476132441396

Epoch: 5| Step: 9
Training loss: 2.8917837758677725
Validation loss: 2.4864720823240853

Epoch: 5| Step: 10
Training loss: 2.052812540324642
Validation loss: 2.507177565469945

Epoch: 102| Step: 0
Training loss: 2.0501890653558843
Validation loss: 2.5092290527604013

Epoch: 5| Step: 1
Training loss: 2.619926090217181
Validation loss: 2.4982917902456956

Epoch: 5| Step: 2
Training loss: 3.148506561354848
Validation loss: 2.4946583358078254

Epoch: 5| Step: 3
Training loss: 2.199571489737116
Validation loss: 2.4975420086892117

Epoch: 5| Step: 4
Training loss: 2.658166530561849
Validation loss: 2.513158454338063

Epoch: 5| Step: 5
Training loss: 3.144099349221242
Validation loss: 2.5202618138107606

Epoch: 5| Step: 6
Training loss: 2.6797898164572875
Validation loss: 2.492628828610885

Epoch: 5| Step: 7
Training loss: 2.698437811007148
Validation loss: 2.4988650647845763

Epoch: 5| Step: 8
Training loss: 2.2681333139855306
Validation loss: 2.4952395358239543

Epoch: 5| Step: 9
Training loss: 2.598187247478793
Validation loss: 2.4944316110860103

Epoch: 5| Step: 10
Training loss: 2.849101135654711
Validation loss: 2.490535481067871

Epoch: 103| Step: 0
Training loss: 2.80611338803274
Validation loss: 2.498135921821139

Epoch: 5| Step: 1
Training loss: 2.5406537982204718
Validation loss: 2.5058997414925646

Epoch: 5| Step: 2
Training loss: 2.738991203185346
Validation loss: 2.4917935681667926

Epoch: 5| Step: 3
Training loss: 2.6323719957898715
Validation loss: 2.5006361756944906

Epoch: 5| Step: 4
Training loss: 2.907027930053029
Validation loss: 2.498223021873409

Epoch: 5| Step: 5
Training loss: 2.897644263453824
Validation loss: 2.5030490207164804

Epoch: 5| Step: 6
Training loss: 2.6851959998057957
Validation loss: 2.497447022900572

Epoch: 5| Step: 7
Training loss: 2.660402171773315
Validation loss: 2.4893604936899214

Epoch: 5| Step: 8
Training loss: 2.1898905090065046
Validation loss: 2.500767430063751

Epoch: 5| Step: 9
Training loss: 2.667322664238414
Validation loss: 2.4942164355958476

Epoch: 5| Step: 10
Training loss: 2.3132590001977404
Validation loss: 2.502923266943316

Epoch: 104| Step: 0
Training loss: 2.9666316455388424
Validation loss: 2.48672212984997

Epoch: 5| Step: 1
Training loss: 2.3946578970892265
Validation loss: 2.5012904436037355

Epoch: 5| Step: 2
Training loss: 2.5779894244063675
Validation loss: 2.4949971029524676

Epoch: 5| Step: 3
Training loss: 2.872121115880155
Validation loss: 2.4972729504111566

Epoch: 5| Step: 4
Training loss: 2.772103862066232
Validation loss: 2.4836375279035523

Epoch: 5| Step: 5
Training loss: 2.5333359212192903
Validation loss: 2.489531698544012

Epoch: 5| Step: 6
Training loss: 2.74041482082508
Validation loss: 2.499688972075902

Epoch: 5| Step: 7
Training loss: 2.3809747331569144
Validation loss: 2.5095499883175183

Epoch: 5| Step: 8
Training loss: 2.9423875121830205
Validation loss: 2.4889212903824265

Epoch: 5| Step: 9
Training loss: 2.466590419656219
Validation loss: 2.484668993166843

Epoch: 5| Step: 10
Training loss: 2.3596312528991055
Validation loss: 2.4969134997305455

Epoch: 105| Step: 0
Training loss: 2.4364689333520544
Validation loss: 2.4945155024775327

Epoch: 5| Step: 1
Training loss: 3.08117029423563
Validation loss: 2.48869043399509

Epoch: 5| Step: 2
Training loss: 2.9594320128607645
Validation loss: 2.4859871370424984

Epoch: 5| Step: 3
Training loss: 2.220073251632645
Validation loss: 2.4916988601879315

Epoch: 5| Step: 4
Training loss: 2.881538916146814
Validation loss: 2.498277465560249

Epoch: 5| Step: 5
Training loss: 2.6423390259880124
Validation loss: 2.5004681148956127

Epoch: 5| Step: 6
Training loss: 2.3634233920644583
Validation loss: 2.4996157863667094

Epoch: 5| Step: 7
Training loss: 2.2836385267724557
Validation loss: 2.502994913317793

Epoch: 5| Step: 8
Training loss: 2.6169233957520843
Validation loss: 2.507486932477322

Epoch: 5| Step: 9
Training loss: 2.8015871896841955
Validation loss: 2.511816005896792

Epoch: 5| Step: 10
Training loss: 2.7572490343449645
Validation loss: 2.4956275665984236

Epoch: 106| Step: 0
Training loss: 2.530933784230617
Validation loss: 2.4894881544138525

Epoch: 5| Step: 1
Training loss: 2.1056809468818005
Validation loss: 2.4938815504266456

Epoch: 5| Step: 2
Training loss: 2.8457026223082087
Validation loss: 2.498319762627044

Epoch: 5| Step: 3
Training loss: 3.1819010909536933
Validation loss: 2.4913548133396284

Epoch: 5| Step: 4
Training loss: 2.496155453010341
Validation loss: 2.513569394470068

Epoch: 5| Step: 5
Training loss: 2.908894801844969
Validation loss: 2.4968727949318223

Epoch: 5| Step: 6
Training loss: 2.5155915441716994
Validation loss: 2.502898206795271

Epoch: 5| Step: 7
Training loss: 2.487468496918673
Validation loss: 2.493023037735286

Epoch: 5| Step: 8
Training loss: 2.7675245410413463
Validation loss: 2.5037632182840115

Epoch: 5| Step: 9
Training loss: 2.001571038233155
Validation loss: 2.491530866383075

Epoch: 5| Step: 10
Training loss: 3.1050469261960267
Validation loss: 2.5006400170986005

Epoch: 107| Step: 0
Training loss: 2.354976571000947
Validation loss: 2.505527628380464

Epoch: 5| Step: 1
Training loss: 2.448824472039766
Validation loss: 2.49322603852229

Epoch: 5| Step: 2
Training loss: 2.570763916446865
Validation loss: 2.489071339285896

Epoch: 5| Step: 3
Training loss: 3.0223822119694743
Validation loss: 2.4837464507260543

Epoch: 5| Step: 4
Training loss: 2.6093153347115203
Validation loss: 2.4838756643271394

Epoch: 5| Step: 5
Training loss: 3.095402748144056
Validation loss: 2.4959276312148506

Epoch: 5| Step: 6
Training loss: 1.990676725904255
Validation loss: 2.488812014109576

Epoch: 5| Step: 7
Training loss: 2.324729132512169
Validation loss: 2.491417097789647

Epoch: 5| Step: 8
Training loss: 2.540771472645365
Validation loss: 2.4822806293005883

Epoch: 5| Step: 9
Training loss: 2.513020179761627
Validation loss: 2.4913884377103837

Epoch: 5| Step: 10
Training loss: 3.4723790061309896
Validation loss: 2.496526341435004

Epoch: 108| Step: 0
Training loss: 2.751999041767841
Validation loss: 2.4896953820305865

Epoch: 5| Step: 1
Training loss: 3.0779245379085514
Validation loss: 2.4928267143277307

Epoch: 5| Step: 2
Training loss: 1.8684310921653076
Validation loss: 2.48855300804792

Epoch: 5| Step: 3
Training loss: 2.611583574535701
Validation loss: 2.48687015898656

Epoch: 5| Step: 4
Training loss: 2.7332024376238504
Validation loss: 2.492281786186616

Epoch: 5| Step: 5
Training loss: 2.8823389010642693
Validation loss: 2.4994474938741145

Epoch: 5| Step: 6
Training loss: 2.5670156002246802
Validation loss: 2.4902046033578373

Epoch: 5| Step: 7
Training loss: 2.7747745353589393
Validation loss: 2.4927999416625197

Epoch: 5| Step: 8
Training loss: 1.9104683899888866
Validation loss: 2.4898672011982566

Epoch: 5| Step: 9
Training loss: 2.509734655998133
Validation loss: 2.491836112639865

Epoch: 5| Step: 10
Training loss: 3.235622883988135
Validation loss: 2.4930403742245777

Epoch: 109| Step: 0
Training loss: 2.785843046237796
Validation loss: 2.4828674856745683

Epoch: 5| Step: 1
Training loss: 2.6973927307012855
Validation loss: 2.481889679253531

Epoch: 5| Step: 2
Training loss: 2.549063836083688
Validation loss: 2.475849814437903

Epoch: 5| Step: 3
Training loss: 3.2636651541859285
Validation loss: 2.5010228351427446

Epoch: 5| Step: 4
Training loss: 2.868308531590955
Validation loss: 2.4900177809839037

Epoch: 5| Step: 5
Training loss: 2.6180878414341042
Validation loss: 2.4971087544375714

Epoch: 5| Step: 6
Training loss: 2.3592273590175377
Validation loss: 2.494644252825026

Epoch: 5| Step: 7
Training loss: 2.639274929208214
Validation loss: 2.493798929913031

Epoch: 5| Step: 8
Training loss: 2.300075351475912
Validation loss: 2.4854674230594846

Epoch: 5| Step: 9
Training loss: 2.3263323750123845
Validation loss: 2.5009383768282674

Epoch: 5| Step: 10
Training loss: 2.4086108825450414
Validation loss: 2.4868975222577823

Epoch: 110| Step: 0
Training loss: 2.719503364102218
Validation loss: 2.491138690820252

Epoch: 5| Step: 1
Training loss: 3.382062234117571
Validation loss: 2.5019831800217016

Epoch: 5| Step: 2
Training loss: 2.0538834423657586
Validation loss: 2.4901746589181

Epoch: 5| Step: 3
Training loss: 2.755403498544055
Validation loss: 2.4884122236855575

Epoch: 5| Step: 4
Training loss: 2.9212551173376298
Validation loss: 2.4986465143496703

Epoch: 5| Step: 5
Training loss: 2.8471114863336124
Validation loss: 2.4987400571662266

Epoch: 5| Step: 6
Training loss: 2.895841932398335
Validation loss: 2.495720077224742

Epoch: 5| Step: 7
Training loss: 2.3962386299862497
Validation loss: 2.4949835459161083

Epoch: 5| Step: 8
Training loss: 1.8991478113117486
Validation loss: 2.4873663395332946

Epoch: 5| Step: 9
Training loss: 2.0755780896976885
Validation loss: 2.485679677043355

Epoch: 5| Step: 10
Training loss: 2.6121027959367384
Validation loss: 2.5038581262353428

Epoch: 111| Step: 0
Training loss: 2.730368419981411
Validation loss: 2.5040628623799615

Epoch: 5| Step: 1
Training loss: 2.3979654748513584
Validation loss: 2.494196521745123

Epoch: 5| Step: 2
Training loss: 2.5325416754318324
Validation loss: 2.51432957100334

Epoch: 5| Step: 3
Training loss: 2.5080648039821702
Validation loss: 2.494484903227152

Epoch: 5| Step: 4
Training loss: 2.4952644319628132
Validation loss: 2.4995693512671573

Epoch: 5| Step: 5
Training loss: 2.3940211090266286
Validation loss: 2.4893603175874386

Epoch: 5| Step: 6
Training loss: 2.1385641828622286
Validation loss: 2.4908331635421987

Epoch: 5| Step: 7
Training loss: 3.2322451448099447
Validation loss: 2.497637943389068

Epoch: 5| Step: 8
Training loss: 2.791801582226775
Validation loss: 2.4735417777089714

Epoch: 5| Step: 9
Training loss: 2.848437292420196
Validation loss: 2.4857022602034835

Epoch: 5| Step: 10
Training loss: 2.6706784787169964
Validation loss: 2.4897439741537455

Epoch: 112| Step: 0
Training loss: 2.86798733149541
Validation loss: 2.485707462350998

Epoch: 5| Step: 1
Training loss: 2.519238169575796
Validation loss: 2.481509340565188

Epoch: 5| Step: 2
Training loss: 2.397530946661117
Validation loss: 2.501358765343384

Epoch: 5| Step: 3
Training loss: 2.750585580381687
Validation loss: 2.4924362459147495

Epoch: 5| Step: 4
Training loss: 2.3204318956681496
Validation loss: 2.4864437152864456

Epoch: 5| Step: 5
Training loss: 2.732319086198783
Validation loss: 2.4849904059761783

Epoch: 5| Step: 6
Training loss: 2.506907837642442
Validation loss: 2.4922788900717214

Epoch: 5| Step: 7
Training loss: 2.0035758238710937
Validation loss: 2.4903997427401996

Epoch: 5| Step: 8
Training loss: 2.7109081255725744
Validation loss: 2.5005345870215083

Epoch: 5| Step: 9
Training loss: 3.2599960072030734
Validation loss: 2.4806953319582994

Epoch: 5| Step: 10
Training loss: 2.6546500380164653
Validation loss: 2.504988938681484

Epoch: 113| Step: 0
Training loss: 1.6600007825872576
Validation loss: 2.473770050155109

Epoch: 5| Step: 1
Training loss: 2.8390386770421445
Validation loss: 2.4912626314716393

Epoch: 5| Step: 2
Training loss: 2.6819430720398594
Validation loss: 2.4908482499523146

Epoch: 5| Step: 3
Training loss: 3.1224076772194223
Validation loss: 2.492398134086261

Epoch: 5| Step: 4
Training loss: 2.2725233619420746
Validation loss: 2.4969882086870103

Epoch: 5| Step: 5
Training loss: 2.5033537304178903
Validation loss: 2.508418733198252

Epoch: 5| Step: 6
Training loss: 2.7249710991403013
Validation loss: 2.4899379686849943

Epoch: 5| Step: 7
Training loss: 2.5791915999169257
Validation loss: 2.4907172664412713

Epoch: 5| Step: 8
Training loss: 2.8674708753867764
Validation loss: 2.511876521520856

Epoch: 5| Step: 9
Training loss: 2.2579973930499264
Validation loss: 2.4875260647559205

Epoch: 5| Step: 10
Training loss: 3.156320099004981
Validation loss: 2.4982613947651986

Epoch: 114| Step: 0
Training loss: 2.0106264574818167
Validation loss: 2.4806645239970937

Epoch: 5| Step: 1
Training loss: 3.0282118211076474
Validation loss: 2.5087159133597425

Epoch: 5| Step: 2
Training loss: 2.2667475583097088
Validation loss: 2.4934744484741254

Epoch: 5| Step: 3
Training loss: 2.666788893123766
Validation loss: 2.4850514025722044

Epoch: 5| Step: 4
Training loss: 2.8106927787441256
Validation loss: 2.4963138867178496

Epoch: 5| Step: 5
Training loss: 2.9766121607364866
Validation loss: 2.479642439954649

Epoch: 5| Step: 6
Training loss: 2.0956988019089846
Validation loss: 2.4919664473467886

Epoch: 5| Step: 7
Training loss: 2.55103023885273
Validation loss: 2.5000412891423887

Epoch: 5| Step: 8
Training loss: 2.446027364838209
Validation loss: 2.489155469009082

Epoch: 5| Step: 9
Training loss: 2.94789281684924
Validation loss: 2.489827984362331

Epoch: 5| Step: 10
Training loss: 2.7927558444310887
Validation loss: 2.4908546558338442

Epoch: 115| Step: 0
Training loss: 2.0946503027348196
Validation loss: 2.507891248692607

Epoch: 5| Step: 1
Training loss: 2.533027495779025
Validation loss: 2.5021006423875947

Epoch: 5| Step: 2
Training loss: 2.870079473797387
Validation loss: 2.486801965979953

Epoch: 5| Step: 3
Training loss: 2.606304074831054
Validation loss: 2.4873538751587416

Epoch: 5| Step: 4
Training loss: 2.351767496190926
Validation loss: 2.4936985112225467

Epoch: 5| Step: 5
Training loss: 3.1676372914416566
Validation loss: 2.5005788389355623

Epoch: 5| Step: 6
Training loss: 2.8233620173841105
Validation loss: 2.493753451534299

Epoch: 5| Step: 7
Training loss: 2.7412455535866527
Validation loss: 2.4928899826708566

Epoch: 5| Step: 8
Training loss: 2.6240661413184068
Validation loss: 2.483229379906911

Epoch: 5| Step: 9
Training loss: 2.093722670647197
Validation loss: 2.5000252219947985

Epoch: 5| Step: 10
Training loss: 2.7161947936597794
Validation loss: 2.489306012548544

Epoch: 116| Step: 0
Training loss: 2.3255932245951696
Validation loss: 2.5021660414528735

Epoch: 5| Step: 1
Training loss: 2.627668659085406
Validation loss: 2.4964792198933927

Epoch: 5| Step: 2
Training loss: 2.6075182493308193
Validation loss: 2.5001599793879055

Epoch: 5| Step: 3
Training loss: 2.4325808258585604
Validation loss: 2.4948244789958274

Epoch: 5| Step: 4
Training loss: 2.7376378238071593
Validation loss: 2.4926475172093725

Epoch: 5| Step: 5
Training loss: 2.6096982470183643
Validation loss: 2.513099458945693

Epoch: 5| Step: 6
Training loss: 2.5788077635219677
Validation loss: 2.4950491716035605

Epoch: 5| Step: 7
Training loss: 3.0267503513692966
Validation loss: 2.4804005515383794

Epoch: 5| Step: 8
Training loss: 2.355584744306573
Validation loss: 2.4948128960469793

Epoch: 5| Step: 9
Training loss: 3.0967911694494634
Validation loss: 2.4976198402473098

Epoch: 5| Step: 10
Training loss: 2.2945605184255404
Validation loss: 2.4984386398299594

Epoch: 117| Step: 0
Training loss: 2.6575813995842648
Validation loss: 2.4966851582266725

Epoch: 5| Step: 1
Training loss: 2.6726442534694037
Validation loss: 2.4871395221527166

Epoch: 5| Step: 2
Training loss: 3.042942110662438
Validation loss: 2.502586826087978

Epoch: 5| Step: 3
Training loss: 2.854906603459654
Validation loss: 2.489371918698078

Epoch: 5| Step: 4
Training loss: 2.0355253562163163
Validation loss: 2.480845268701639

Epoch: 5| Step: 5
Training loss: 2.5604561819488683
Validation loss: 2.502116673154787

Epoch: 5| Step: 6
Training loss: 2.6361224369100102
Validation loss: 2.4792032987956496

Epoch: 5| Step: 7
Training loss: 2.7907121839072198
Validation loss: 2.481305687032707

Epoch: 5| Step: 8
Training loss: 2.105520725493382
Validation loss: 2.4910489703796492

Epoch: 5| Step: 9
Training loss: 2.4819500682643523
Validation loss: 2.492720122797314

Epoch: 5| Step: 10
Training loss: 2.7739217845778503
Validation loss: 2.4950925888866333

Epoch: 118| Step: 0
Training loss: 2.1579942144717634
Validation loss: 2.4885566909144137

Epoch: 5| Step: 1
Training loss: 3.5912624872231533
Validation loss: 2.504960894935328

Epoch: 5| Step: 2
Training loss: 2.3751394331307676
Validation loss: 2.495279194664053

Epoch: 5| Step: 3
Training loss: 2.7437777785022486
Validation loss: 2.510731102752106

Epoch: 5| Step: 4
Training loss: 2.941658647506489
Validation loss: 2.478525242322619

Epoch: 5| Step: 5
Training loss: 2.5019960064753217
Validation loss: 2.496729788461256

Epoch: 5| Step: 6
Training loss: 2.694997840979566
Validation loss: 2.504366519235267

Epoch: 5| Step: 7
Training loss: 1.7078502095937524
Validation loss: 2.505782784603965

Epoch: 5| Step: 8
Training loss: 2.162054018106271
Validation loss: 2.4925323997571684

Epoch: 5| Step: 9
Training loss: 2.7339503367224047
Validation loss: 2.4931623936206004

Epoch: 5| Step: 10
Training loss: 2.7867719717373607
Validation loss: 2.480163005537263

Epoch: 119| Step: 0
Training loss: 2.296186590508044
Validation loss: 2.4774134051343784

Epoch: 5| Step: 1
Training loss: 2.7645194219940867
Validation loss: 2.487086904458106

Epoch: 5| Step: 2
Training loss: 2.8629085199288933
Validation loss: 2.5016295403340743

Epoch: 5| Step: 3
Training loss: 2.3302661855033415
Validation loss: 2.4968179437937783

Epoch: 5| Step: 4
Training loss: 2.342569079118457
Validation loss: 2.4742953315224088

Epoch: 5| Step: 5
Training loss: 2.217771072420029
Validation loss: 2.492285232101215

Epoch: 5| Step: 6
Training loss: 2.606433421022424
Validation loss: 2.4903506414917533

Epoch: 5| Step: 7
Training loss: 3.3618482844462623
Validation loss: 2.4806603731451236

Epoch: 5| Step: 8
Training loss: 2.310273516261697
Validation loss: 2.491878073057836

Epoch: 5| Step: 9
Training loss: 2.54426553361875
Validation loss: 2.4924063966871706

Epoch: 5| Step: 10
Training loss: 2.938414573332522
Validation loss: 2.490445411035295

Epoch: 120| Step: 0
Training loss: 2.100404101047061
Validation loss: 2.476738872712958

Epoch: 5| Step: 1
Training loss: 3.006638969271257
Validation loss: 2.483279240862785

Epoch: 5| Step: 2
Training loss: 2.1170171127168267
Validation loss: 2.4835725761419116

Epoch: 5| Step: 3
Training loss: 3.319113984056612
Validation loss: 2.4961183161520575

Epoch: 5| Step: 4
Training loss: 2.8004095663525446
Validation loss: 2.504574063479335

Epoch: 5| Step: 5
Training loss: 2.673620534993087
Validation loss: 2.4814851246459813

Epoch: 5| Step: 6
Training loss: 2.3560378366161987
Validation loss: 2.4936116448321606

Epoch: 5| Step: 7
Training loss: 2.5734915046144042
Validation loss: 2.4790484602817666

Epoch: 5| Step: 8
Training loss: 2.513942938048547
Validation loss: 2.4979546250159106

Epoch: 5| Step: 9
Training loss: 2.3908419043182447
Validation loss: 2.4922620600339394

Epoch: 5| Step: 10
Training loss: 2.5564876828298893
Validation loss: 2.497805354024961

Epoch: 121| Step: 0
Training loss: 2.3822282919205415
Validation loss: 2.4855692613979294

Epoch: 5| Step: 1
Training loss: 2.5001265493788383
Validation loss: 2.4817235812366256

Epoch: 5| Step: 2
Training loss: 2.850171897121233
Validation loss: 2.484758385286999

Epoch: 5| Step: 3
Training loss: 2.8344236594736993
Validation loss: 2.489032115060358

Epoch: 5| Step: 4
Training loss: 3.0278863236066313
Validation loss: 2.501630708592441

Epoch: 5| Step: 5
Training loss: 2.6554278728305114
Validation loss: 2.4962152689039154

Epoch: 5| Step: 6
Training loss: 2.8599822853446755
Validation loss: 2.4861077557854294

Epoch: 5| Step: 7
Training loss: 2.262322274493498
Validation loss: 2.4895150100642405

Epoch: 5| Step: 8
Training loss: 2.6954302333604963
Validation loss: 2.505447908027514

Epoch: 5| Step: 9
Training loss: 1.6689245982751237
Validation loss: 2.4968856250492157

Epoch: 5| Step: 10
Training loss: 2.6526238317464133
Validation loss: 2.4849508212460227

Epoch: 122| Step: 0
Training loss: 2.8116578430718713
Validation loss: 2.4973143929429775

Epoch: 5| Step: 1
Training loss: 2.715517687680062
Validation loss: 2.4962297435555247

Epoch: 5| Step: 2
Training loss: 1.8716155978963902
Validation loss: 2.4948362345241306

Epoch: 5| Step: 3
Training loss: 2.423244753464427
Validation loss: 2.4919070304833544

Epoch: 5| Step: 4
Training loss: 2.8048915098156537
Validation loss: 2.486102585411184

Epoch: 5| Step: 5
Training loss: 2.825037160983462
Validation loss: 2.491517608430737

Epoch: 5| Step: 6
Training loss: 2.724614012792872
Validation loss: 2.4823202390544843

Epoch: 5| Step: 7
Training loss: 2.709876476724355
Validation loss: 2.4927405256400337

Epoch: 5| Step: 8
Training loss: 2.662342758191959
Validation loss: 2.493457212676832

Epoch: 5| Step: 9
Training loss: 1.9842398289576846
Validation loss: 2.4882558512668407

Epoch: 5| Step: 10
Training loss: 2.8812233284400515
Validation loss: 2.4956582843526105

Epoch: 123| Step: 0
Training loss: 2.9328095344117933
Validation loss: 2.484443203900602

Epoch: 5| Step: 1
Training loss: 2.703507236511242
Validation loss: 2.4880591558200362

Epoch: 5| Step: 2
Training loss: 2.851199777604853
Validation loss: 2.4922131096579148

Epoch: 5| Step: 3
Training loss: 2.5703775229534185
Validation loss: 2.478384457254049

Epoch: 5| Step: 4
Training loss: 2.5026119892316987
Validation loss: 2.486018987608521

Epoch: 5| Step: 5
Training loss: 2.2894680458133267
Validation loss: 2.4940906815522395

Epoch: 5| Step: 6
Training loss: 3.389286181260214
Validation loss: 2.4952269109455125

Epoch: 5| Step: 7
Training loss: 2.0450645768618547
Validation loss: 2.4921219151982785

Epoch: 5| Step: 8
Training loss: 2.2738976291505395
Validation loss: 2.4847768586384595

Epoch: 5| Step: 9
Training loss: 2.266350570778486
Validation loss: 2.4867129803144334

Epoch: 5| Step: 10
Training loss: 2.4195963322696645
Validation loss: 2.49198347330138

Epoch: 124| Step: 0
Training loss: 2.7613330879334113
Validation loss: 2.476446772519821

Epoch: 5| Step: 1
Training loss: 2.2578571671552146
Validation loss: 2.4998261237436847

Epoch: 5| Step: 2
Training loss: 3.0246481824846367
Validation loss: 2.493513196710594

Epoch: 5| Step: 3
Training loss: 2.3210790119454363
Validation loss: 2.496610746715931

Epoch: 5| Step: 4
Training loss: 2.210599300546828
Validation loss: 2.4833720349620703

Epoch: 5| Step: 5
Training loss: 2.923443378420149
Validation loss: 2.4873872763675138

Epoch: 5| Step: 6
Training loss: 2.7541910660009683
Validation loss: 2.479223322218828

Epoch: 5| Step: 7
Training loss: 2.0371382831047495
Validation loss: 2.4799336995892785

Epoch: 5| Step: 8
Training loss: 2.476706226883489
Validation loss: 2.4872802837277543

Epoch: 5| Step: 9
Training loss: 2.5877091254159983
Validation loss: 2.486087036069712

Epoch: 5| Step: 10
Training loss: 3.0993238388488287
Validation loss: 2.494205067211186

Epoch: 125| Step: 0
Training loss: 2.455507130190922
Validation loss: 2.4968944221018523

Epoch: 5| Step: 1
Training loss: 2.443148499437051
Validation loss: 2.4900748039399296

Epoch: 5| Step: 2
Training loss: 2.589167856249512
Validation loss: 2.48285807619077

Epoch: 5| Step: 3
Training loss: 3.069734708276795
Validation loss: 2.4924571586754327

Epoch: 5| Step: 4
Training loss: 2.7230574587259353
Validation loss: 2.491690859661903

Epoch: 5| Step: 5
Training loss: 2.7142948171993573
Validation loss: 2.4892212967527696

Epoch: 5| Step: 6
Training loss: 2.2352566346843465
Validation loss: 2.483949515050709

Epoch: 5| Step: 7
Training loss: 2.565133579040342
Validation loss: 2.494937679143658

Epoch: 5| Step: 8
Training loss: 2.326561012037018
Validation loss: 2.4984749480119173

Epoch: 5| Step: 9
Training loss: 2.6634044561649177
Validation loss: 2.485647031062996

Epoch: 5| Step: 10
Training loss: 2.8998214008977334
Validation loss: 2.497884300177077

Epoch: 126| Step: 0
Training loss: 2.520484448294704
Validation loss: 2.4939537384650596

Epoch: 5| Step: 1
Training loss: 2.7322862767350475
Validation loss: 2.473463433099504

Epoch: 5| Step: 2
Training loss: 2.857731748246165
Validation loss: 2.4828669152004603

Epoch: 5| Step: 3
Training loss: 2.3960172389115604
Validation loss: 2.4839803875403916

Epoch: 5| Step: 4
Training loss: 2.2708970390506464
Validation loss: 2.484638894826123

Epoch: 5| Step: 5
Training loss: 2.5068596191238743
Validation loss: 2.487023816800188

Epoch: 5| Step: 6
Training loss: 3.0211521550450096
Validation loss: 2.481020426620005

Epoch: 5| Step: 7
Training loss: 2.3072229953532593
Validation loss: 2.487747325444496

Epoch: 5| Step: 8
Training loss: 2.9479743403711667
Validation loss: 2.4921231208319514

Epoch: 5| Step: 9
Training loss: 1.9953053927612385
Validation loss: 2.494645942293884

Epoch: 5| Step: 10
Training loss: 2.627801217943974
Validation loss: 2.484535379793137

Epoch: 127| Step: 0
Training loss: 1.9192867683842985
Validation loss: 2.4922075620868496

Epoch: 5| Step: 1
Training loss: 1.8712402159002508
Validation loss: 2.47904048304534

Epoch: 5| Step: 2
Training loss: 2.8384620212032057
Validation loss: 2.4905293492033858

Epoch: 5| Step: 3
Training loss: 2.2551242968353753
Validation loss: 2.4877742338935804

Epoch: 5| Step: 4
Training loss: 2.7981026794756687
Validation loss: 2.497001839541124

Epoch: 5| Step: 5
Training loss: 2.9439843216146864
Validation loss: 2.493901003180668

Epoch: 5| Step: 6
Training loss: 2.310072165678928
Validation loss: 2.4838997485697862

Epoch: 5| Step: 7
Training loss: 2.811656231939026
Validation loss: 2.490006370264477

Epoch: 5| Step: 8
Training loss: 2.0380738883583094
Validation loss: 2.484066789872213

Epoch: 5| Step: 9
Training loss: 3.194829256035403
Validation loss: 2.478077039363275

Epoch: 5| Step: 10
Training loss: 3.160489700134035
Validation loss: 2.489399046445907

Epoch: 128| Step: 0
Training loss: 2.6730845429607943
Validation loss: 2.477815034343649

Epoch: 5| Step: 1
Training loss: 2.827799657159519
Validation loss: 2.479165313408733

Epoch: 5| Step: 2
Training loss: 2.173757930695082
Validation loss: 2.484526180930943

Epoch: 5| Step: 3
Training loss: 3.440254494989493
Validation loss: 2.487283700497037

Epoch: 5| Step: 4
Training loss: 1.787240512393171
Validation loss: 2.489212509659177

Epoch: 5| Step: 5
Training loss: 2.4212062773845777
Validation loss: 2.4900496633872833

Epoch: 5| Step: 6
Training loss: 2.7026181490018093
Validation loss: 2.4792451364396086

Epoch: 5| Step: 7
Training loss: 2.3047249483848433
Validation loss: 2.4849306840650818

Epoch: 5| Step: 8
Training loss: 2.4563138608905675
Validation loss: 2.5006976743582685

Epoch: 5| Step: 9
Training loss: 2.1161404093144656
Validation loss: 2.4947936276636598

Epoch: 5| Step: 10
Training loss: 3.167316035563964
Validation loss: 2.4800294150646702

Epoch: 129| Step: 0
Training loss: 2.1633975112439447
Validation loss: 2.4831289596544424

Epoch: 5| Step: 1
Training loss: 2.4100283398683313
Validation loss: 2.487512119688505

Epoch: 5| Step: 2
Training loss: 2.8080870870361028
Validation loss: 2.4912036400676514

Epoch: 5| Step: 3
Training loss: 2.6360046776135264
Validation loss: 2.490474571436256

Epoch: 5| Step: 4
Training loss: 2.668880547003935
Validation loss: 2.497400379126514

Epoch: 5| Step: 5
Training loss: 2.74487955047996
Validation loss: 2.4879594936334506

Epoch: 5| Step: 6
Training loss: 2.6279971859363833
Validation loss: 2.4969820356869454

Epoch: 5| Step: 7
Training loss: 2.2632747693507276
Validation loss: 2.4883197185107986

Epoch: 5| Step: 8
Training loss: 2.2990827265486566
Validation loss: 2.479213059769327

Epoch: 5| Step: 9
Training loss: 2.6407424629766196
Validation loss: 2.493684589877841

Epoch: 5| Step: 10
Training loss: 3.109523443771293
Validation loss: 2.49371138748482

Epoch: 130| Step: 0
Training loss: 2.6767222733697387
Validation loss: 2.4910854984425854

Epoch: 5| Step: 1
Training loss: 2.257115239910845
Validation loss: 2.484273413001789

Epoch: 5| Step: 2
Training loss: 1.7306258773939251
Validation loss: 2.4888354047383823

Epoch: 5| Step: 3
Training loss: 3.242584718937173
Validation loss: 2.4953695504787476

Epoch: 5| Step: 4
Training loss: 2.595001286559622
Validation loss: 2.4763449296441404

Epoch: 5| Step: 5
Training loss: 2.8011616511032655
Validation loss: 2.4933040726705937

Epoch: 5| Step: 6
Training loss: 2.213561748338748
Validation loss: 2.478257211421369

Epoch: 5| Step: 7
Training loss: 2.5352990061766314
Validation loss: 2.4848631909759953

Epoch: 5| Step: 8
Training loss: 2.3627386305651217
Validation loss: 2.4818090529234653

Epoch: 5| Step: 9
Training loss: 2.884172816438538
Validation loss: 2.4974495008812325

Epoch: 5| Step: 10
Training loss: 2.785820880359595
Validation loss: 2.4769870317459692

Epoch: 131| Step: 0
Training loss: 2.580194186214328
Validation loss: 2.489108463869724

Epoch: 5| Step: 1
Training loss: 2.1743204085516443
Validation loss: 2.491210094173638

Epoch: 5| Step: 2
Training loss: 2.646569147195102
Validation loss: 2.479823890806498

Epoch: 5| Step: 3
Training loss: 2.490305031166743
Validation loss: 2.4843041978796467

Epoch: 5| Step: 4
Training loss: 2.528533042542455
Validation loss: 2.48738929748312

Epoch: 5| Step: 5
Training loss: 2.698061925642562
Validation loss: 2.4804014641709227

Epoch: 5| Step: 6
Training loss: 2.7324181230149813
Validation loss: 2.502776905843137

Epoch: 5| Step: 7
Training loss: 2.653890571626534
Validation loss: 2.4849771853447997

Epoch: 5| Step: 8
Training loss: 2.4228441821796602
Validation loss: 2.490968176354995

Epoch: 5| Step: 9
Training loss: 2.428826869625739
Validation loss: 2.472213703700195

Epoch: 5| Step: 10
Training loss: 3.042773337204729
Validation loss: 2.483281022201048

Epoch: 132| Step: 0
Training loss: 2.906614055186568
Validation loss: 2.473443419017108

Epoch: 5| Step: 1
Training loss: 2.4105376648159003
Validation loss: 2.477961940252846

Epoch: 5| Step: 2
Training loss: 3.063212720497959
Validation loss: 2.4780520233100516

Epoch: 5| Step: 3
Training loss: 2.133863253143855
Validation loss: 2.4858153106062626

Epoch: 5| Step: 4
Training loss: 2.205033361826986
Validation loss: 2.479377981272873

Epoch: 5| Step: 5
Training loss: 2.492106182242598
Validation loss: 2.469109834673001

Epoch: 5| Step: 6
Training loss: 2.3632184989732035
Validation loss: 2.496368476790015

Epoch: 5| Step: 7
Training loss: 2.610343616272277
Validation loss: 2.488013143879018

Epoch: 5| Step: 8
Training loss: 2.018515000970532
Validation loss: 2.4766533528518853

Epoch: 5| Step: 9
Training loss: 2.801305830814902
Validation loss: 2.4810241010352785

Epoch: 5| Step: 10
Training loss: 3.184500236407005
Validation loss: 2.4938461056970187

Epoch: 133| Step: 0
Training loss: 2.8942160904568404
Validation loss: 2.493443569128375

Epoch: 5| Step: 1
Training loss: 2.191443458282045
Validation loss: 2.4747516210377984

Epoch: 5| Step: 2
Training loss: 2.745839005398331
Validation loss: 2.4803530735118

Epoch: 5| Step: 3
Training loss: 2.4807864008282583
Validation loss: 2.486464059309497

Epoch: 5| Step: 4
Training loss: 3.1500335509920347
Validation loss: 2.4798351209447627

Epoch: 5| Step: 5
Training loss: 2.3146778882819667
Validation loss: 2.4931891510828526

Epoch: 5| Step: 6
Training loss: 1.8826591460903555
Validation loss: 2.4701133858502744

Epoch: 5| Step: 7
Training loss: 2.298430143405492
Validation loss: 2.494938306968651

Epoch: 5| Step: 8
Training loss: 2.65866535548819
Validation loss: 2.4792259911001904

Epoch: 5| Step: 9
Training loss: 2.5883105128113937
Validation loss: 2.4928366785439886

Epoch: 5| Step: 10
Training loss: 2.8932211275215063
Validation loss: 2.4764505241153447

Epoch: 134| Step: 0
Training loss: 2.2490611766807582
Validation loss: 2.481834302776125

Epoch: 5| Step: 1
Training loss: 3.2275937258986684
Validation loss: 2.496302943296577

Epoch: 5| Step: 2
Training loss: 2.4165359220544076
Validation loss: 2.480476109502906

Epoch: 5| Step: 3
Training loss: 2.7739207531776513
Validation loss: 2.477661231723531

Epoch: 5| Step: 4
Training loss: 2.424821500992604
Validation loss: 2.482684181726335

Epoch: 5| Step: 5
Training loss: 2.5233806213960293
Validation loss: 2.4856738075436695

Epoch: 5| Step: 6
Training loss: 2.221489088664882
Validation loss: 2.4808125301685657

Epoch: 5| Step: 7
Training loss: 2.4432172970282537
Validation loss: 2.4736400962163905

Epoch: 5| Step: 8
Training loss: 2.6189668442709197
Validation loss: 2.4942214293226876

Epoch: 5| Step: 9
Training loss: 2.4213537824344202
Validation loss: 2.481334561205025

Epoch: 5| Step: 10
Training loss: 2.6909269617574507
Validation loss: 2.48072017874312

Epoch: 135| Step: 0
Training loss: 2.312227490838838
Validation loss: 2.4886553588022253

Epoch: 5| Step: 1
Training loss: 2.5689264055923933
Validation loss: 2.4861967146490467

Epoch: 5| Step: 2
Training loss: 2.621895998483077
Validation loss: 2.474509253997911

Epoch: 5| Step: 3
Training loss: 3.0748357093155767
Validation loss: 2.4897452664023447

Epoch: 5| Step: 4
Training loss: 2.9160679520589694
Validation loss: 2.4831534475861576

Epoch: 5| Step: 5
Training loss: 1.769012240322758
Validation loss: 2.4772381236041547

Epoch: 5| Step: 6
Training loss: 2.7125751045482485
Validation loss: 2.486161778025511

Epoch: 5| Step: 7
Training loss: 2.1525322750210565
Validation loss: 2.4772728745826353

Epoch: 5| Step: 8
Training loss: 2.226492897418139
Validation loss: 2.47298975862927

Epoch: 5| Step: 9
Training loss: 2.607864673748562
Validation loss: 2.481837928470253

Epoch: 5| Step: 10
Training loss: 3.113532425956163
Validation loss: 2.476635999472998

Epoch: 136| Step: 0
Training loss: 2.640804668386101
Validation loss: 2.4927552184424426

Epoch: 5| Step: 1
Training loss: 2.9207926494759793
Validation loss: 2.487148538182495

Epoch: 5| Step: 2
Training loss: 2.3061735890040085
Validation loss: 2.4724204064940003

Epoch: 5| Step: 3
Training loss: 2.0858220557795586
Validation loss: 2.4896314203313303

Epoch: 5| Step: 4
Training loss: 2.7205842068131405
Validation loss: 2.479101742888518

Epoch: 5| Step: 5
Training loss: 2.4004933247583655
Validation loss: 2.485713504006671

Epoch: 5| Step: 6
Training loss: 2.4647944183974815
Validation loss: 2.4758046442038424

Epoch: 5| Step: 7
Training loss: 2.803057608756519
Validation loss: 2.490656818265856

Epoch: 5| Step: 8
Training loss: 2.971138886463204
Validation loss: 2.4907237241234474

Epoch: 5| Step: 9
Training loss: 2.47091956114128
Validation loss: 2.490379654766263

Epoch: 5| Step: 10
Training loss: 2.204097127508553
Validation loss: 2.480743038020063

Epoch: 137| Step: 0
Training loss: 2.339921800333773
Validation loss: 2.4784337014408746

Epoch: 5| Step: 1
Training loss: 2.5078196302651263
Validation loss: 2.492801286833061

Epoch: 5| Step: 2
Training loss: 2.57527419176692
Validation loss: 2.4732222829432815

Epoch: 5| Step: 3
Training loss: 2.698646142400313
Validation loss: 2.47841162458413

Epoch: 5| Step: 4
Training loss: 2.3163027040648014
Validation loss: 2.4703701399472817

Epoch: 5| Step: 5
Training loss: 2.600490589807666
Validation loss: 2.4843909821035264

Epoch: 5| Step: 6
Training loss: 2.5105640373397162
Validation loss: 2.4741333864688975

Epoch: 5| Step: 7
Training loss: 2.057734445816445
Validation loss: 2.4851040788390213

Epoch: 5| Step: 8
Training loss: 2.8137945798497004
Validation loss: 2.4651085698787254

Epoch: 5| Step: 9
Training loss: 2.991754804356
Validation loss: 2.484122872014359

Epoch: 5| Step: 10
Training loss: 2.528092098139869
Validation loss: 2.477134790376544

Epoch: 138| Step: 0
Training loss: 2.5726661920572584
Validation loss: 2.492840355081883

Epoch: 5| Step: 1
Training loss: 2.60988516017782
Validation loss: 2.491136385111813

Epoch: 5| Step: 2
Training loss: 2.6456561492304584
Validation loss: 2.4671232357899693

Epoch: 5| Step: 3
Training loss: 2.4226246319687816
Validation loss: 2.478981791554028

Epoch: 5| Step: 4
Training loss: 3.2102535661033382
Validation loss: 2.4764781847104422

Epoch: 5| Step: 5
Training loss: 2.119666530905272
Validation loss: 2.4817640221423996

Epoch: 5| Step: 6
Training loss: 2.3625260084224284
Validation loss: 2.499301274392362

Epoch: 5| Step: 7
Training loss: 1.8827267702457908
Validation loss: 2.4904727679647123

Epoch: 5| Step: 8
Training loss: 2.5588941557901745
Validation loss: 2.4739670381678924

Epoch: 5| Step: 9
Training loss: 3.0312326961691998
Validation loss: 2.478265967016191

Epoch: 5| Step: 10
Training loss: 2.434490302003295
Validation loss: 2.4816493612043047

Epoch: 139| Step: 0
Training loss: 2.4681548354856413
Validation loss: 2.4877438237797884

Epoch: 5| Step: 1
Training loss: 2.374436863843644
Validation loss: 2.4682359410754233

Epoch: 5| Step: 2
Training loss: 2.424965738270772
Validation loss: 2.478094369719464

Epoch: 5| Step: 3
Training loss: 3.068438164120113
Validation loss: 2.4884518249538443

Epoch: 5| Step: 4
Training loss: 2.2288769922966924
Validation loss: 2.466391856062842

Epoch: 5| Step: 5
Training loss: 3.08932116462156
Validation loss: 2.5005098479297687

Epoch: 5| Step: 6
Training loss: 2.880464419430736
Validation loss: 2.4953749019866027

Epoch: 5| Step: 7
Training loss: 2.3772411814465997
Validation loss: 2.4854822140388335

Epoch: 5| Step: 8
Training loss: 2.137890705294741
Validation loss: 2.475103493344843

Epoch: 5| Step: 9
Training loss: 2.5204106651718607
Validation loss: 2.488979943194324

Epoch: 5| Step: 10
Training loss: 2.2581747747785257
Validation loss: 2.4926139432491823

Epoch: 140| Step: 0
Training loss: 2.6810092168789703
Validation loss: 2.4790132057493337

Epoch: 5| Step: 1
Training loss: 2.9449355797449384
Validation loss: 2.480681307154462

Epoch: 5| Step: 2
Training loss: 2.503847404181272
Validation loss: 2.4916512682033507

Epoch: 5| Step: 3
Training loss: 2.355124073526251
Validation loss: 2.478560692043146

Epoch: 5| Step: 4
Training loss: 2.5192872868199676
Validation loss: 2.4744362891542035

Epoch: 5| Step: 5
Training loss: 2.9308518194187396
Validation loss: 2.4898054432570036

Epoch: 5| Step: 6
Training loss: 2.6078946602908606
Validation loss: 2.4866915645995284

Epoch: 5| Step: 7
Training loss: 2.660808824887669
Validation loss: 2.4874988753576526

Epoch: 5| Step: 8
Training loss: 2.1524068888907224
Validation loss: 2.470682304113336

Epoch: 5| Step: 9
Training loss: 1.776644860659266
Validation loss: 2.4815840158478752

Epoch: 5| Step: 10
Training loss: 2.735682583560828
Validation loss: 2.4861671864750443

Epoch: 141| Step: 0
Training loss: 2.9804364323895
Validation loss: 2.482702155243751

Epoch: 5| Step: 1
Training loss: 2.5491159327748916
Validation loss: 2.468562820917181

Epoch: 5| Step: 2
Training loss: 2.5792726678794113
Validation loss: 2.4781412641121414

Epoch: 5| Step: 3
Training loss: 2.8959321661533983
Validation loss: 2.4744777919479617

Epoch: 5| Step: 4
Training loss: 2.2875543514944643
Validation loss: 2.4779709622588344

Epoch: 5| Step: 5
Training loss: 2.972811841464403
Validation loss: 2.469747593857295

Epoch: 5| Step: 6
Training loss: 2.304760741023731
Validation loss: 2.482960959055421

Epoch: 5| Step: 7
Training loss: 2.9362730345100605
Validation loss: 2.4707422884609107

Epoch: 5| Step: 8
Training loss: 2.1217671654875367
Validation loss: 2.4692603866541973

Epoch: 5| Step: 9
Training loss: 1.8091842313616964
Validation loss: 2.481031580561479

Epoch: 5| Step: 10
Training loss: 2.261913442714545
Validation loss: 2.487525263980756

Epoch: 142| Step: 0
Training loss: 2.274969155238725
Validation loss: 2.4729712900083927

Epoch: 5| Step: 1
Training loss: 2.6012936931498603
Validation loss: 2.47929617203227

Epoch: 5| Step: 2
Training loss: 2.657165100441107
Validation loss: 2.4977062880586027

Epoch: 5| Step: 3
Training loss: 1.9852006050808393
Validation loss: 2.4725692405309503

Epoch: 5| Step: 4
Training loss: 2.9781073618763823
Validation loss: 2.4774543437652206

Epoch: 5| Step: 5
Training loss: 2.3657859959109757
Validation loss: 2.4816435714012077

Epoch: 5| Step: 6
Training loss: 2.617063787013112
Validation loss: 2.487874218305321

Epoch: 5| Step: 7
Training loss: 2.3983354080788564
Validation loss: 2.47642519970005

Epoch: 5| Step: 8
Training loss: 2.3583864073515635
Validation loss: 2.49052804089192

Epoch: 5| Step: 9
Training loss: 2.802013089080669
Validation loss: 2.4781479004298097

Epoch: 5| Step: 10
Training loss: 2.8235436525640205
Validation loss: 2.4672856799327776

Epoch: 143| Step: 0
Training loss: 2.3279643163338606
Validation loss: 2.4875853193416373

Epoch: 5| Step: 1
Training loss: 3.0792362898688395
Validation loss: 2.476938728596408

Epoch: 5| Step: 2
Training loss: 1.9803279548271908
Validation loss: 2.475729038676606

Epoch: 5| Step: 3
Training loss: 2.525464640253338
Validation loss: 2.474877786462259

Epoch: 5| Step: 4
Training loss: 2.6603276089723513
Validation loss: 2.4837719110541094

Epoch: 5| Step: 5
Training loss: 2.6916480765473327
Validation loss: 2.4929080682273015

Epoch: 5| Step: 6
Training loss: 2.4657009949473547
Validation loss: 2.482515871171784

Epoch: 5| Step: 7
Training loss: 2.549524158136644
Validation loss: 2.483288107795566

Epoch: 5| Step: 8
Training loss: 2.6726267688399776
Validation loss: 2.48574244764471

Epoch: 5| Step: 9
Training loss: 2.6642937434004996
Validation loss: 2.4712990368319736

Epoch: 5| Step: 10
Training loss: 2.0922232228107505
Validation loss: 2.47914473628286

Epoch: 144| Step: 0
Training loss: 2.033053727024993
Validation loss: 2.484040304650225

Epoch: 5| Step: 1
Training loss: 2.345974083083301
Validation loss: 2.474222456731139

Epoch: 5| Step: 2
Training loss: 1.9511453590462413
Validation loss: 2.484295379978793

Epoch: 5| Step: 3
Training loss: 2.3051982766225243
Validation loss: 2.4645191750730144

Epoch: 5| Step: 4
Training loss: 2.6085278626398565
Validation loss: 2.4702850103445977

Epoch: 5| Step: 5
Training loss: 2.146769038754662
Validation loss: 2.476930795280693

Epoch: 5| Step: 6
Training loss: 3.2674405102615327
Validation loss: 2.4685021604842783

Epoch: 5| Step: 7
Training loss: 2.82890182701256
Validation loss: 2.485875588267417

Epoch: 5| Step: 8
Training loss: 2.843634634507449
Validation loss: 2.465610626996888

Epoch: 5| Step: 9
Training loss: 2.590075728439756
Validation loss: 2.473045437853822

Epoch: 5| Step: 10
Training loss: 2.8666366339004865
Validation loss: 2.4765020996959906

Epoch: 145| Step: 0
Training loss: 2.2111172552465357
Validation loss: 2.472502301705894

Epoch: 5| Step: 1
Training loss: 2.8534984131175807
Validation loss: 2.4896343715270963

Epoch: 5| Step: 2
Training loss: 2.541213500684425
Validation loss: 2.4870265041116166

Epoch: 5| Step: 3
Training loss: 2.4628874304536676
Validation loss: 2.4806967932368145

Epoch: 5| Step: 4
Training loss: 2.6185890202900945
Validation loss: 2.4841244850460886

Epoch: 5| Step: 5
Training loss: 2.2288996694035133
Validation loss: 2.488034231889354

Epoch: 5| Step: 6
Training loss: 2.462291817167373
Validation loss: 2.4724132964939907

Epoch: 5| Step: 7
Training loss: 2.608931452349617
Validation loss: 2.4845161864807612

Epoch: 5| Step: 8
Training loss: 2.867104843569728
Validation loss: 2.4834552831438685

Epoch: 5| Step: 9
Training loss: 2.2667482945759847
Validation loss: 2.4728562191675465

Epoch: 5| Step: 10
Training loss: 2.6836722860824334
Validation loss: 2.4736385209124436

Epoch: 146| Step: 0
Training loss: 2.775191834838415
Validation loss: 2.485377847049124

Epoch: 5| Step: 1
Training loss: 2.411263926986413
Validation loss: 2.471552603697565

Epoch: 5| Step: 2
Training loss: 2.7910233985360167
Validation loss: 2.474091261361889

Epoch: 5| Step: 3
Training loss: 2.150818770885374
Validation loss: 2.4906997130713857

Epoch: 5| Step: 4
Training loss: 2.915165342218713
Validation loss: 2.4801277523764544

Epoch: 5| Step: 5
Training loss: 2.390271029726658
Validation loss: 2.4750982652847604

Epoch: 5| Step: 6
Training loss: 2.248913184580451
Validation loss: 2.4953192042377705

Epoch: 5| Step: 7
Training loss: 1.9172261154153454
Validation loss: 2.4897362556573097

Epoch: 5| Step: 8
Training loss: 2.7287400084455475
Validation loss: 2.4745689010842264

Epoch: 5| Step: 9
Training loss: 2.762818458808727
Validation loss: 2.4792353916199157

Epoch: 5| Step: 10
Training loss: 2.5361139166785
Validation loss: 2.479866558445147

Epoch: 147| Step: 0
Training loss: 1.9774095733068222
Validation loss: 2.4850915891910286

Epoch: 5| Step: 1
Training loss: 2.799227444472612
Validation loss: 2.4933463533430156

Epoch: 5| Step: 2
Training loss: 2.866675889939249
Validation loss: 2.495012137469628

Epoch: 5| Step: 3
Training loss: 2.6314090417924483
Validation loss: 2.4924027976813776

Epoch: 5| Step: 4
Training loss: 2.802231672442309
Validation loss: 2.4831723881682515

Epoch: 5| Step: 5
Training loss: 2.0031046611925714
Validation loss: 2.4821084047775073

Epoch: 5| Step: 6
Training loss: 1.7820543848766117
Validation loss: 2.4754200130366795

Epoch: 5| Step: 7
Training loss: 2.4737610962528755
Validation loss: 2.4811430071772835

Epoch: 5| Step: 8
Training loss: 2.607964597075583
Validation loss: 2.4866114640876447

Epoch: 5| Step: 9
Training loss: 2.5187910545164827
Validation loss: 2.4549177359880185

Epoch: 5| Step: 10
Training loss: 3.112966945199434
Validation loss: 2.472055251950076

Epoch: 148| Step: 0
Training loss: 2.22600604683191
Validation loss: 2.469948941276343

Epoch: 5| Step: 1
Training loss: 2.7584830271351013
Validation loss: 2.4705834637277393

Epoch: 5| Step: 2
Training loss: 2.561989524051591
Validation loss: 2.4795489128132133

Epoch: 5| Step: 3
Training loss: 2.6588581519824035
Validation loss: 2.480742977048492

Epoch: 5| Step: 4
Training loss: 2.4046861595264093
Validation loss: 2.4938967746468648

Epoch: 5| Step: 5
Training loss: 2.5527657563105546
Validation loss: 2.485374921743771

Epoch: 5| Step: 6
Training loss: 2.746423910379042
Validation loss: 2.469579198880156

Epoch: 5| Step: 7
Training loss: 2.2844247543823935
Validation loss: 2.478639933621141

Epoch: 5| Step: 8
Training loss: 2.744062342494399
Validation loss: 2.4764321133956817

Epoch: 5| Step: 9
Training loss: 2.127911088362384
Validation loss: 2.493251077126365

Epoch: 5| Step: 10
Training loss: 2.6492068993256397
Validation loss: 2.475579920835433

Epoch: 149| Step: 0
Training loss: 2.428353584360455
Validation loss: 2.467419855954637

Epoch: 5| Step: 1
Training loss: 2.7141624419105574
Validation loss: 2.467214959412155

Epoch: 5| Step: 2
Training loss: 2.2279487443846144
Validation loss: 2.4921813855729633

Epoch: 5| Step: 3
Training loss: 2.209992762937822
Validation loss: 2.465671030531402

Epoch: 5| Step: 4
Training loss: 2.6505549695442894
Validation loss: 2.4851535045068336

Epoch: 5| Step: 5
Training loss: 2.4136744313829217
Validation loss: 2.490790298824561

Epoch: 5| Step: 6
Training loss: 2.696275002934414
Validation loss: 2.4829942949438717

Epoch: 5| Step: 7
Training loss: 2.589993801919598
Validation loss: 2.4909448417749185

Epoch: 5| Step: 8
Training loss: 2.837194075036353
Validation loss: 2.486924016223222

Epoch: 5| Step: 9
Training loss: 2.0918353991696477
Validation loss: 2.4778085492265967

Epoch: 5| Step: 10
Training loss: 2.7347422325943906
Validation loss: 2.4758355623410826

Epoch: 150| Step: 0
Training loss: 2.4979449408688543
Validation loss: 2.47148343310099

Epoch: 5| Step: 1
Training loss: 2.881062625245404
Validation loss: 2.4862119024044502

Epoch: 5| Step: 2
Training loss: 1.509223949385108
Validation loss: 2.4722555186009814

Epoch: 5| Step: 3
Training loss: 2.248336388897169
Validation loss: 2.4807722814660798

Epoch: 5| Step: 4
Training loss: 2.744414900244465
Validation loss: 2.4682249370418115

Epoch: 5| Step: 5
Training loss: 2.770005200377723
Validation loss: 2.4836511076484293

Epoch: 5| Step: 6
Training loss: 2.9376576158159122
Validation loss: 2.4763066762757067

Epoch: 5| Step: 7
Training loss: 2.2766891245295167
Validation loss: 2.46816398422483

Epoch: 5| Step: 8
Training loss: 2.4802670854907243
Validation loss: 2.460387894081874

Epoch: 5| Step: 9
Training loss: 2.5755577483326477
Validation loss: 2.481050615355383

Epoch: 5| Step: 10
Training loss: 2.5095964307937804
Validation loss: 2.482769294050538

Testing loss: 2.4973803888664774

Epoch: 1| Step: 0
Training loss: 3.77915620803833
Validation loss: 4.510468247116253

Epoch: 6| Step: 1
Training loss: 4.4899492263793945
Validation loss: 4.502802787288543

Epoch: 6| Step: 2
Training loss: 3.450773000717163
Validation loss: 4.498558054688156

Epoch: 6| Step: 3
Training loss: 5.016828536987305
Validation loss: 4.493790344525409

Epoch: 6| Step: 4
Training loss: 4.2979536056518555
Validation loss: 4.488724329138315

Epoch: 6| Step: 5
Training loss: 6.119991302490234
Validation loss: 4.482384712465348

Epoch: 6| Step: 6
Training loss: 3.7173259258270264
Validation loss: 4.475190106258597

Epoch: 6| Step: 7
Training loss: 5.237833023071289
Validation loss: 4.469327562598772

Epoch: 6| Step: 8
Training loss: 3.374408483505249
Validation loss: 4.465168214613391

Epoch: 6| Step: 9
Training loss: 3.7191731929779053
Validation loss: 4.457927570548109

Epoch: 6| Step: 10
Training loss: 4.184407711029053
Validation loss: 4.453576108460785

Epoch: 6| Step: 11
Training loss: 4.352216720581055
Validation loss: 4.4508804557144

Epoch: 6| Step: 12
Training loss: 4.39605712890625
Validation loss: 4.441648549931024

Epoch: 6| Step: 13
Training loss: 3.459681749343872
Validation loss: 4.435463377224502

Epoch: 2| Step: 0
Training loss: 3.8030996322631836
Validation loss: 4.431865599847609

Epoch: 6| Step: 1
Training loss: 4.277312278747559
Validation loss: 4.428492079498947

Epoch: 6| Step: 2
Training loss: 4.607968807220459
Validation loss: 4.420238843528173

Epoch: 6| Step: 3
Training loss: 5.282454490661621
Validation loss: 4.416446255099389

Epoch: 6| Step: 4
Training loss: 3.9627737998962402
Validation loss: 4.408298687268329

Epoch: 6| Step: 5
Training loss: 5.985223770141602
Validation loss: 4.404039659807759

Epoch: 6| Step: 6
Training loss: 4.704756736755371
Validation loss: 4.399446287462788

Epoch: 6| Step: 7
Training loss: 3.201746940612793
Validation loss: 4.394572914287608

Epoch: 6| Step: 8
Training loss: 3.1680164337158203
Validation loss: 4.389713764190674

Epoch: 6| Step: 9
Training loss: 4.238035202026367
Validation loss: 4.384072119189847

Epoch: 6| Step: 10
Training loss: 3.750135898590088
Validation loss: 4.378401766541184

Epoch: 6| Step: 11
Training loss: 4.453224182128906
Validation loss: 4.373253324980377

Epoch: 6| Step: 12
Training loss: 3.9010777473449707
Validation loss: 4.367207593815301

Epoch: 6| Step: 13
Training loss: 3.1062991619110107
Validation loss: 4.362964532708609

Epoch: 3| Step: 0
Training loss: 3.8642852306365967
Validation loss: 4.354294382115846

Epoch: 6| Step: 1
Training loss: 3.9491312503814697
Validation loss: 4.355351084022112

Epoch: 6| Step: 2
Training loss: 3.5569300651550293
Validation loss: 4.345530053620697

Epoch: 6| Step: 3
Training loss: 3.966552734375
Validation loss: 4.34110326151694

Epoch: 6| Step: 4
Training loss: 3.248593330383301
Validation loss: 4.336222333292807

Epoch: 6| Step: 5
Training loss: 4.152491092681885
Validation loss: 4.329001426696777

Epoch: 6| Step: 6
Training loss: 4.350336074829102
Validation loss: 4.3243328678992485

Epoch: 6| Step: 7
Training loss: 4.100078105926514
Validation loss: 4.320478203476116

Epoch: 6| Step: 8
Training loss: 4.8270769119262695
Validation loss: 4.313760060136036

Epoch: 6| Step: 9
Training loss: 3.5899133682250977
Validation loss: 4.311188379923503

Epoch: 6| Step: 10
Training loss: 5.4731340408325195
Validation loss: 4.304043923654864

Epoch: 6| Step: 11
Training loss: 4.424612998962402
Validation loss: 4.295512932603077

Epoch: 6| Step: 12
Training loss: 4.634425163269043
Validation loss: 4.290488145684683

Epoch: 6| Step: 13
Training loss: 3.456239938735962
Validation loss: 4.283005037615376

Epoch: 4| Step: 0
Training loss: 3.1517348289489746
Validation loss: 4.277975810471402

Epoch: 6| Step: 1
Training loss: 4.870356559753418
Validation loss: 4.272854117936985

Epoch: 6| Step: 2
Training loss: 5.261303901672363
Validation loss: 4.268737567368374

Epoch: 6| Step: 3
Training loss: 4.049201011657715
Validation loss: 4.25968421915526

Epoch: 6| Step: 4
Training loss: 4.075183868408203
Validation loss: 4.256798200709845

Epoch: 6| Step: 5
Training loss: 3.7367565631866455
Validation loss: 4.246655177044612

Epoch: 6| Step: 6
Training loss: 4.333226680755615
Validation loss: 4.244715987995106

Epoch: 6| Step: 7
Training loss: 3.5933210849761963
Validation loss: 4.238701102554157

Epoch: 6| Step: 8
Training loss: 3.535465717315674
Validation loss: 4.232372850500127

Epoch: 6| Step: 9
Training loss: 6.101312637329102
Validation loss: 4.224812271774456

Epoch: 6| Step: 10
Training loss: 4.054330825805664
Validation loss: 4.219605374079879

Epoch: 6| Step: 11
Training loss: 2.524358034133911
Validation loss: 4.214782125206404

Epoch: 6| Step: 12
Training loss: 3.902545928955078
Validation loss: 4.204490528311781

Epoch: 6| Step: 13
Training loss: 3.3199756145477295
Validation loss: 4.202886202002085

Epoch: 5| Step: 0
Training loss: 4.943887710571289
Validation loss: 4.195713381613454

Epoch: 6| Step: 1
Training loss: 3.308148145675659
Validation loss: 4.19011946647398

Epoch: 6| Step: 2
Training loss: 3.8227405548095703
Validation loss: 4.183207714429465

Epoch: 6| Step: 3
Training loss: 3.853454828262329
Validation loss: 4.177079141780895

Epoch: 6| Step: 4
Training loss: 5.06760311126709
Validation loss: 4.169845601563813

Epoch: 6| Step: 5
Training loss: 4.828279972076416
Validation loss: 4.164905855732579

Epoch: 6| Step: 6
Training loss: 2.9866318702697754
Validation loss: 4.157399633879303

Epoch: 6| Step: 7
Training loss: 3.665727138519287
Validation loss: 4.151936167029924

Epoch: 6| Step: 8
Training loss: 3.80483341217041
Validation loss: 4.1467140669463785

Epoch: 6| Step: 9
Training loss: 4.401448726654053
Validation loss: 4.1365821566633

Epoch: 6| Step: 10
Training loss: 3.9984447956085205
Validation loss: 4.131373769493513

Epoch: 6| Step: 11
Training loss: 3.6363725662231445
Validation loss: 4.12291121226485

Epoch: 6| Step: 12
Training loss: 4.117915630340576
Validation loss: 4.118237126258112

Epoch: 6| Step: 13
Training loss: 2.705094337463379
Validation loss: 4.111122356948032

Epoch: 6| Step: 0
Training loss: 3.3290462493896484
Validation loss: 4.103887393910398

Epoch: 6| Step: 1
Training loss: 4.061171531677246
Validation loss: 4.096016830013644

Epoch: 6| Step: 2
Training loss: 4.379062175750732
Validation loss: 4.090099878208612

Epoch: 6| Step: 3
Training loss: 4.324852466583252
Validation loss: 4.080946327537618

Epoch: 6| Step: 4
Training loss: 4.823170185089111
Validation loss: 4.077117853267218

Epoch: 6| Step: 5
Training loss: 5.628360748291016
Validation loss: 4.069510218917682

Epoch: 6| Step: 6
Training loss: 4.714421272277832
Validation loss: 4.065142759712794

Epoch: 6| Step: 7
Training loss: 3.5103394985198975
Validation loss: 4.05552315455611

Epoch: 6| Step: 8
Training loss: 3.3977222442626953
Validation loss: 4.043153178307318

Epoch: 6| Step: 9
Training loss: 2.99320125579834
Validation loss: 4.040163360616212

Epoch: 6| Step: 10
Training loss: 3.742452621459961
Validation loss: 4.031507315174226

Epoch: 6| Step: 11
Training loss: 2.2949841022491455
Validation loss: 4.021491619848436

Epoch: 6| Step: 12
Training loss: 3.170905828475952
Validation loss: 4.01373617623442

Epoch: 6| Step: 13
Training loss: 4.422009468078613
Validation loss: 4.0098056690667265

Epoch: 7| Step: 0
Training loss: 2.4217991828918457
Validation loss: 4.004951938506095

Epoch: 6| Step: 1
Training loss: 3.1112523078918457
Validation loss: 3.9947279755787184

Epoch: 6| Step: 2
Training loss: 3.8918399810791016
Validation loss: 3.986727791447793

Epoch: 6| Step: 3
Training loss: 4.244884490966797
Validation loss: 3.97989071056407

Epoch: 6| Step: 4
Training loss: 4.727537631988525
Validation loss: 3.973624534504388

Epoch: 6| Step: 5
Training loss: 2.143462657928467
Validation loss: 3.9671980027229554

Epoch: 6| Step: 6
Training loss: 3.377781867980957
Validation loss: 3.9581873211809384

Epoch: 6| Step: 7
Training loss: 2.601621389389038
Validation loss: 3.946745857115715

Epoch: 6| Step: 8
Training loss: 5.020011901855469
Validation loss: 3.945184284640897

Epoch: 6| Step: 9
Training loss: 3.9459447860717773
Validation loss: 3.93389973332805

Epoch: 6| Step: 10
Training loss: 3.782381534576416
Validation loss: 3.9289276676793254

Epoch: 6| Step: 11
Training loss: 5.178569316864014
Validation loss: 3.911923554635817

Epoch: 6| Step: 12
Training loss: 4.5799055099487305
Validation loss: 3.9051415176801783

Epoch: 6| Step: 13
Training loss: 4.540168762207031
Validation loss: 3.901839651087279

Epoch: 8| Step: 0
Training loss: 4.259328842163086
Validation loss: 3.8887819705470914

Epoch: 6| Step: 1
Training loss: 3.8073813915252686
Validation loss: 3.8847551499643633

Epoch: 6| Step: 2
Training loss: 2.346034526824951
Validation loss: 3.8755519877197924

Epoch: 6| Step: 3
Training loss: 3.7648985385894775
Validation loss: 3.87128597177485

Epoch: 6| Step: 4
Training loss: 4.9796953201293945
Validation loss: 3.863471508026123

Epoch: 6| Step: 5
Training loss: 3.654414653778076
Validation loss: 3.854254643122355

Epoch: 6| Step: 6
Training loss: 3.9149320125579834
Validation loss: 3.842804278096845

Epoch: 6| Step: 7
Training loss: 3.198613166809082
Validation loss: 3.8381910580460743

Epoch: 6| Step: 8
Training loss: 3.0517516136169434
Validation loss: 3.826497365069646

Epoch: 6| Step: 9
Training loss: 3.7222728729248047
Validation loss: 3.820376806361701

Epoch: 6| Step: 10
Training loss: 4.334027290344238
Validation loss: 3.8116511580764607

Epoch: 6| Step: 11
Training loss: 3.1074869632720947
Validation loss: 3.800629941366052

Epoch: 6| Step: 12
Training loss: 3.175337791442871
Validation loss: 3.7923675173072406

Epoch: 6| Step: 13
Training loss: 5.252927780151367
Validation loss: 3.783640266746603

Epoch: 9| Step: 0
Training loss: 4.238285064697266
Validation loss: 3.7710602975660756

Epoch: 6| Step: 1
Training loss: 4.247187614440918
Validation loss: 3.762611522469469

Epoch: 6| Step: 2
Training loss: 3.6387715339660645
Validation loss: 3.756455657302692

Epoch: 6| Step: 3
Training loss: 3.9911599159240723
Validation loss: 3.7425840234243744

Epoch: 6| Step: 4
Training loss: 3.0115137100219727
Validation loss: 3.7331783745878484

Epoch: 6| Step: 5
Training loss: 3.0007753372192383
Validation loss: 3.7284743862767376

Epoch: 6| Step: 6
Training loss: 3.511490821838379
Validation loss: 3.714981953303019

Epoch: 6| Step: 7
Training loss: 4.9342780113220215
Validation loss: 3.706837636168285

Epoch: 6| Step: 8
Training loss: 3.709282159805298
Validation loss: 3.6976170257855485

Epoch: 6| Step: 9
Training loss: 3.2400598526000977
Validation loss: 3.6869853952879548

Epoch: 6| Step: 10
Training loss: 3.1133995056152344
Validation loss: 3.6781976710083666

Epoch: 6| Step: 11
Training loss: 2.843879222869873
Validation loss: 3.665511700414842

Epoch: 6| Step: 12
Training loss: 3.3426976203918457
Validation loss: 3.6590738681054886

Epoch: 6| Step: 13
Training loss: 3.4292144775390625
Validation loss: 3.6466738716248543

Epoch: 10| Step: 0
Training loss: 5.052419662475586
Validation loss: 3.633166818208592

Epoch: 6| Step: 1
Training loss: 2.405602216720581
Validation loss: 3.622141686818933

Epoch: 6| Step: 2
Training loss: 3.3669137954711914
Validation loss: 3.6102299177518455

Epoch: 6| Step: 3
Training loss: 3.00968599319458
Validation loss: 3.6034638599682878

Epoch: 6| Step: 4
Training loss: 4.363865852355957
Validation loss: 3.5876363528672086

Epoch: 6| Step: 5
Training loss: 3.4669337272644043
Validation loss: 3.5721757617048038

Epoch: 6| Step: 6
Training loss: 3.46150803565979
Validation loss: 3.56121886673794

Epoch: 6| Step: 7
Training loss: 2.5660250186920166
Validation loss: 3.554143364711474

Epoch: 6| Step: 8
Training loss: 3.351499080657959
Validation loss: 3.53389403640583

Epoch: 6| Step: 9
Training loss: 3.1189937591552734
Validation loss: 3.5258072627488004

Epoch: 6| Step: 10
Training loss: 2.516087532043457
Validation loss: 3.5079879350559686

Epoch: 6| Step: 11
Training loss: 4.945199012756348
Validation loss: 3.505326665857787

Epoch: 6| Step: 12
Training loss: 3.3214263916015625
Validation loss: 3.4926387904792704

Epoch: 6| Step: 13
Training loss: 3.668980121612549
Validation loss: 3.4773506579860562

Epoch: 11| Step: 0
Training loss: 2.336033344268799
Validation loss: 3.463058540897985

Epoch: 6| Step: 1
Training loss: 3.0054171085357666
Validation loss: 3.4548523144055436

Epoch: 6| Step: 2
Training loss: 3.192711353302002
Validation loss: 3.4442661910928707

Epoch: 6| Step: 3
Training loss: 3.884033203125
Validation loss: 3.432162379705778

Epoch: 6| Step: 4
Training loss: 4.1262054443359375
Validation loss: 3.4232722764374106

Epoch: 6| Step: 5
Training loss: 4.298646926879883
Validation loss: 3.4032255218875025

Epoch: 6| Step: 6
Training loss: 2.6873281002044678
Validation loss: 3.3946372719221216

Epoch: 6| Step: 7
Training loss: 3.430452823638916
Validation loss: 3.3712982772499003

Epoch: 6| Step: 8
Training loss: 3.1541452407836914
Validation loss: 3.3670121469805316

Epoch: 6| Step: 9
Training loss: 2.774149179458618
Validation loss: 3.3565370728892665

Epoch: 6| Step: 10
Training loss: 2.9565916061401367
Validation loss: 3.3381318071837067

Epoch: 6| Step: 11
Training loss: 3.7340996265411377
Validation loss: 3.318057947261359

Epoch: 6| Step: 12
Training loss: 3.95778751373291
Validation loss: 3.3067866192069104

Epoch: 6| Step: 13
Training loss: 2.2835822105407715
Validation loss: 3.2877372336643997

Epoch: 12| Step: 0
Training loss: 3.094492197036743
Validation loss: 3.2752965598978023

Epoch: 6| Step: 1
Training loss: 3.2908787727355957
Validation loss: 3.2657737321751092

Epoch: 6| Step: 2
Training loss: 3.0975394248962402
Validation loss: 3.2477936385780253

Epoch: 6| Step: 3
Training loss: 2.695967197418213
Validation loss: 3.2383476277833343

Epoch: 6| Step: 4
Training loss: 3.7553799152374268
Validation loss: 3.2148139784413

Epoch: 6| Step: 5
Training loss: 2.639988422393799
Validation loss: 3.1994139917435183

Epoch: 6| Step: 6
Training loss: 3.191427707672119
Validation loss: 3.1821995089131017

Epoch: 6| Step: 7
Training loss: 3.238142967224121
Validation loss: 3.177701847527617

Epoch: 6| Step: 8
Training loss: 2.3595685958862305
Validation loss: 3.152796824773153

Epoch: 6| Step: 9
Training loss: 3.9434094429016113
Validation loss: 3.13985768441231

Epoch: 6| Step: 10
Training loss: 3.37705397605896
Validation loss: 3.1266961738627446

Epoch: 6| Step: 11
Training loss: 2.7131850719451904
Validation loss: 3.112626665381975

Epoch: 6| Step: 12
Training loss: 2.864442825317383
Validation loss: 3.0960077983076855

Epoch: 6| Step: 13
Training loss: 4.009706974029541
Validation loss: 3.0762953937694593

Epoch: 13| Step: 0
Training loss: 1.6156085729599
Validation loss: 3.056989990254884

Epoch: 6| Step: 1
Training loss: 2.648970603942871
Validation loss: 3.0427772024626374

Epoch: 6| Step: 2
Training loss: 3.813478946685791
Validation loss: 3.031271670454292

Epoch: 6| Step: 3
Training loss: 3.3012595176696777
Validation loss: 3.0177484353383384

Epoch: 6| Step: 4
Training loss: 2.548919200897217
Validation loss: 3.0061567547500774

Epoch: 6| Step: 5
Training loss: 2.927103042602539
Validation loss: 2.986307159546883

Epoch: 6| Step: 6
Training loss: 3.3382151126861572
Validation loss: 2.968205157146659

Epoch: 6| Step: 7
Training loss: 2.5705089569091797
Validation loss: 2.9512953732603338

Epoch: 6| Step: 8
Training loss: 3.4672646522521973
Validation loss: 2.9297012103501188

Epoch: 6| Step: 9
Training loss: 2.989356517791748
Validation loss: 2.9116588741220455

Epoch: 6| Step: 10
Training loss: 3.0365958213806152
Validation loss: 2.90729921607561

Epoch: 6| Step: 11
Training loss: 3.375072956085205
Validation loss: 2.8771692347782913

Epoch: 6| Step: 12
Training loss: 2.8840548992156982
Validation loss: 2.862283355446272

Epoch: 6| Step: 13
Training loss: 2.823744773864746
Validation loss: 2.852986463936426

Epoch: 14| Step: 0
Training loss: 2.781179428100586
Validation loss: 2.824746311351817

Epoch: 6| Step: 1
Training loss: 3.4060277938842773
Validation loss: 2.809114069067022

Epoch: 6| Step: 2
Training loss: 2.8102333545684814
Validation loss: 2.796076543869511

Epoch: 6| Step: 3
Training loss: 3.022655487060547
Validation loss: 2.784142335255941

Epoch: 6| Step: 4
Training loss: 3.0767111778259277
Validation loss: 2.7602198713569233

Epoch: 6| Step: 5
Training loss: 1.8376039266586304
Validation loss: 2.742578045014412

Epoch: 6| Step: 6
Training loss: 2.375213623046875
Validation loss: 2.724569202751242

Epoch: 6| Step: 7
Training loss: 3.0194900035858154
Validation loss: 2.7088716619758197

Epoch: 6| Step: 8
Training loss: 3.6854264736175537
Validation loss: 2.691820111325992

Epoch: 6| Step: 9
Training loss: 2.691086769104004
Validation loss: 2.6598998397909184

Epoch: 6| Step: 10
Training loss: 2.882701873779297
Validation loss: 2.6430820188214703

Epoch: 6| Step: 11
Training loss: 2.4007363319396973
Validation loss: 2.632139341805571

Epoch: 6| Step: 12
Training loss: 2.342655658721924
Validation loss: 2.605483349933419

Epoch: 6| Step: 13
Training loss: 2.711388349533081
Validation loss: 2.588331722444104

Epoch: 15| Step: 0
Training loss: 2.7470898628234863
Validation loss: 2.5803716567254837

Epoch: 6| Step: 1
Training loss: 2.950253963470459
Validation loss: 2.561276219224417

Epoch: 6| Step: 2
Training loss: 1.9757493734359741
Validation loss: 2.5449565456759546

Epoch: 6| Step: 3
Training loss: 2.126284599304199
Validation loss: 2.5358791812773673

Epoch: 6| Step: 4
Training loss: 2.1774768829345703
Validation loss: 2.5198603086574103

Epoch: 6| Step: 5
Training loss: 3.031933307647705
Validation loss: 2.512937673958399

Epoch: 6| Step: 6
Training loss: 2.780683994293213
Validation loss: 2.4842967781969296

Epoch: 6| Step: 7
Training loss: 2.9198198318481445
Validation loss: 2.4913932584947154

Epoch: 6| Step: 8
Training loss: 2.8269052505493164
Validation loss: 2.4732372324953795

Epoch: 6| Step: 9
Training loss: 2.4665238857269287
Validation loss: 2.4583216072410665

Epoch: 6| Step: 10
Training loss: 2.8263025283813477
Validation loss: 2.4446017716520574

Epoch: 6| Step: 11
Training loss: 2.763676404953003
Validation loss: 2.4326856918232416

Epoch: 6| Step: 12
Training loss: 1.878337025642395
Validation loss: 2.4158060781417356

Epoch: 6| Step: 13
Training loss: 3.189696788787842
Validation loss: 2.4001845518747964

Epoch: 16| Step: 0
Training loss: 2.578946113586426
Validation loss: 2.369057034933439

Epoch: 6| Step: 1
Training loss: 2.371084451675415
Validation loss: 2.3598656526175876

Epoch: 6| Step: 2
Training loss: 2.691016912460327
Validation loss: 2.3412726643264934

Epoch: 6| Step: 3
Training loss: 2.476635456085205
Validation loss: 2.330825933846094

Epoch: 6| Step: 4
Training loss: 2.1176066398620605
Validation loss: 2.307248979486445

Epoch: 6| Step: 5
Training loss: 2.09013295173645
Validation loss: 2.2851743287937616

Epoch: 6| Step: 6
Training loss: 2.986316204071045
Validation loss: 2.250982920328776

Epoch: 6| Step: 7
Training loss: 2.8510799407958984
Validation loss: 2.2582308092424945

Epoch: 6| Step: 8
Training loss: 2.208117723464966
Validation loss: 2.23106244046201

Epoch: 6| Step: 9
Training loss: 2.525639533996582
Validation loss: 2.2320190821924517

Epoch: 6| Step: 10
Training loss: 2.618619441986084
Validation loss: 2.206880251566569

Epoch: 6| Step: 11
Training loss: 2.6191635131835938
Validation loss: 2.199360279626744

Epoch: 6| Step: 12
Training loss: 1.722957968711853
Validation loss: 2.17994874523532

Epoch: 6| Step: 13
Training loss: 2.8797240257263184
Validation loss: 2.18255712139991

Epoch: 17| Step: 0
Training loss: 2.580934762954712
Validation loss: 2.1592041446316625

Epoch: 6| Step: 1
Training loss: 2.0239102840423584
Validation loss: 2.1786138280745475

Epoch: 6| Step: 2
Training loss: 2.692570686340332
Validation loss: 2.1671076282378166

Epoch: 6| Step: 3
Training loss: 2.184474468231201
Validation loss: 2.1342022406157626

Epoch: 6| Step: 4
Training loss: 2.3660268783569336
Validation loss: 2.1601592302322388

Epoch: 6| Step: 5
Training loss: 1.6240553855895996
Validation loss: 2.134513742180281

Epoch: 6| Step: 6
Training loss: 2.4473953247070312
Validation loss: 2.138853088501961

Epoch: 6| Step: 7
Training loss: 3.0738630294799805
Validation loss: 2.124607265636485

Epoch: 6| Step: 8
Training loss: 2.6826517581939697
Validation loss: 2.1119260890509493

Epoch: 6| Step: 9
Training loss: 2.5177245140075684
Validation loss: 2.11941207608869

Epoch: 6| Step: 10
Training loss: 2.1942787170410156
Validation loss: 2.1024263417848976

Epoch: 6| Step: 11
Training loss: 2.589627981185913
Validation loss: 2.09056176934191

Epoch: 6| Step: 12
Training loss: 1.719873070716858
Validation loss: 2.0941336360028995

Epoch: 6| Step: 13
Training loss: 2.3557934761047363
Validation loss: 2.102898143952893

Epoch: 18| Step: 0
Training loss: 1.8849809169769287
Validation loss: 2.1001884245103404

Epoch: 6| Step: 1
Training loss: 2.4142627716064453
Validation loss: 2.1022823574722453

Epoch: 6| Step: 2
Training loss: 2.5032243728637695
Validation loss: 2.077826110265588

Epoch: 6| Step: 3
Training loss: 2.225095272064209
Validation loss: 2.092269347560021

Epoch: 6| Step: 4
Training loss: 2.8745617866516113
Validation loss: 2.0795297827771915

Epoch: 6| Step: 5
Training loss: 2.676508665084839
Validation loss: 2.0793778434876473

Epoch: 6| Step: 6
Training loss: 2.5948679447174072
Validation loss: 2.078230281029978

Epoch: 6| Step: 7
Training loss: 2.0181021690368652
Validation loss: 2.078966412492978

Epoch: 6| Step: 8
Training loss: 2.457350015640259
Validation loss: 2.0703961208302486

Epoch: 6| Step: 9
Training loss: 1.7749378681182861
Validation loss: 2.0516107082366943

Epoch: 6| Step: 10
Training loss: 3.200639486312866
Validation loss: 2.0796960707633727

Epoch: 6| Step: 11
Training loss: 1.6788851022720337
Validation loss: 2.060752850706859

Epoch: 6| Step: 12
Training loss: 2.3257896900177
Validation loss: 2.0680426679631716

Epoch: 6| Step: 13
Training loss: 2.2936289310455322
Validation loss: 2.0700207833320863

Epoch: 19| Step: 0
Training loss: 1.6688685417175293
Validation loss: 2.0737517828582437

Epoch: 6| Step: 1
Training loss: 3.0274085998535156
Validation loss: 2.0585985798989572

Epoch: 6| Step: 2
Training loss: 2.3471145629882812
Validation loss: 2.061336883934595

Epoch: 6| Step: 3
Training loss: 2.547872543334961
Validation loss: 2.0608456673160678

Epoch: 6| Step: 4
Training loss: 2.0393056869506836
Validation loss: 2.076009588856851

Epoch: 6| Step: 5
Training loss: 2.5326642990112305
Validation loss: 2.0716943971572386

Epoch: 6| Step: 6
Training loss: 2.5379738807678223
Validation loss: 2.0421590420507614

Epoch: 6| Step: 7
Training loss: 1.4789860248565674
Validation loss: 2.069656495125063

Epoch: 6| Step: 8
Training loss: 2.812005043029785
Validation loss: 2.0604643731988888

Epoch: 6| Step: 9
Training loss: 2.088890314102173
Validation loss: 2.046526970401887

Epoch: 6| Step: 10
Training loss: 2.935072898864746
Validation loss: 2.0469567570635068

Epoch: 6| Step: 11
Training loss: 2.1969876289367676
Validation loss: 2.0584377806673766

Epoch: 6| Step: 12
Training loss: 2.4539012908935547
Validation loss: 2.041234402246373

Epoch: 6| Step: 13
Training loss: 1.9906725883483887
Validation loss: 2.058563583640642

Epoch: 20| Step: 0
Training loss: 3.372238874435425
Validation loss: 2.048451531317926

Epoch: 6| Step: 1
Training loss: 2.1503548622131348
Validation loss: 2.0681911335196546

Epoch: 6| Step: 2
Training loss: 2.463766098022461
Validation loss: 2.0438019075701312

Epoch: 6| Step: 3
Training loss: 2.1386666297912598
Validation loss: 2.042846897596954

Epoch: 6| Step: 4
Training loss: 2.7560994625091553
Validation loss: 2.0675061992419663

Epoch: 6| Step: 5
Training loss: 1.739801049232483
Validation loss: 2.066560945203227

Epoch: 6| Step: 6
Training loss: 2.297908306121826
Validation loss: 2.045166870599152

Epoch: 6| Step: 7
Training loss: 1.9609588384628296
Validation loss: 2.056080420811971

Epoch: 6| Step: 8
Training loss: 2.3441109657287598
Validation loss: 2.073650521616782

Epoch: 6| Step: 9
Training loss: 2.8344268798828125
Validation loss: 2.051313761741884

Epoch: 6| Step: 10
Training loss: 1.9704127311706543
Validation loss: 2.0721605234248663

Epoch: 6| Step: 11
Training loss: 2.3316738605499268
Validation loss: 2.066641402500932

Epoch: 6| Step: 12
Training loss: 2.474010467529297
Validation loss: 2.074906590164349

Epoch: 6| Step: 13
Training loss: 1.4628527164459229
Validation loss: 2.0651225825791717

Epoch: 21| Step: 0
Training loss: 3.026750087738037
Validation loss: 2.0694593691056773

Epoch: 6| Step: 1
Training loss: 2.121767282485962
Validation loss: 2.0701021314949117

Epoch: 6| Step: 2
Training loss: 2.4524319171905518
Validation loss: 2.059443907071185

Epoch: 6| Step: 3
Training loss: 1.720048189163208
Validation loss: 2.060087247561383

Epoch: 6| Step: 4
Training loss: 2.479679584503174
Validation loss: 2.0745397947167836

Epoch: 6| Step: 5
Training loss: 2.5377705097198486
Validation loss: 2.0620320817475677

Epoch: 6| Step: 6
Training loss: 2.4626708030700684
Validation loss: 2.055609920973419

Epoch: 6| Step: 7
Training loss: 2.021304130554199
Validation loss: 2.060518051988335

Epoch: 6| Step: 8
Training loss: 2.668980121612549
Validation loss: 2.0774896990868355

Epoch: 6| Step: 9
Training loss: 2.708284378051758
Validation loss: 2.0585440333171556

Epoch: 6| Step: 10
Training loss: 2.7758378982543945
Validation loss: 2.070383882009855

Epoch: 6| Step: 11
Training loss: 1.942007303237915
Validation loss: 2.080299545359868

Epoch: 6| Step: 12
Training loss: 1.581985354423523
Validation loss: 2.081728204604118

Epoch: 6| Step: 13
Training loss: 1.8475621938705444
Validation loss: 2.063788124310073

Epoch: 22| Step: 0
Training loss: 1.8866784572601318
Validation loss: 2.053397173522621

Epoch: 6| Step: 1
Training loss: 2.242785930633545
Validation loss: 2.0650960886350243

Epoch: 6| Step: 2
Training loss: 2.6643126010894775
Validation loss: 2.0622300178773942

Epoch: 6| Step: 3
Training loss: 2.3867297172546387
Validation loss: 2.0554108914508613

Epoch: 6| Step: 4
Training loss: 2.6933205127716064
Validation loss: 2.0512475993043635

Epoch: 6| Step: 5
Training loss: 2.1063008308410645
Validation loss: 2.0486204854903685

Epoch: 6| Step: 6
Training loss: 2.451955795288086
Validation loss: 2.052474550021592

Epoch: 6| Step: 7
Training loss: 1.9758061170578003
Validation loss: 2.059559840028004

Epoch: 6| Step: 8
Training loss: 1.9492884874343872
Validation loss: 2.0526559147783505

Epoch: 6| Step: 9
Training loss: 2.4688467979431152
Validation loss: 2.0561391256188832

Epoch: 6| Step: 10
Training loss: 2.5330400466918945
Validation loss: 2.061152288990636

Epoch: 6| Step: 11
Training loss: 2.5976996421813965
Validation loss: 2.0384704733407624

Epoch: 6| Step: 12
Training loss: 2.221823215484619
Validation loss: 2.059470340769778

Epoch: 6| Step: 13
Training loss: 2.3690829277038574
Validation loss: 2.0636294016274075

Epoch: 23| Step: 0
Training loss: 2.111754894256592
Validation loss: 2.040168845525352

Epoch: 6| Step: 1
Training loss: 2.0944762229919434
Validation loss: 2.0589246621695896

Epoch: 6| Step: 2
Training loss: 2.728564739227295
Validation loss: 2.046812412559345

Epoch: 6| Step: 3
Training loss: 2.5237367153167725
Validation loss: 2.0629569920160438

Epoch: 6| Step: 4
Training loss: 3.1946780681610107
Validation loss: 2.072358562100318

Epoch: 6| Step: 5
Training loss: 2.560586929321289
Validation loss: 2.0683894054864043

Epoch: 6| Step: 6
Training loss: 2.0066447257995605
Validation loss: 2.043269234318887

Epoch: 6| Step: 7
Training loss: 2.0744175910949707
Validation loss: 2.060826946330327

Epoch: 6| Step: 8
Training loss: 1.9032185077667236
Validation loss: 2.0698557182024886

Epoch: 6| Step: 9
Training loss: 2.3270978927612305
Validation loss: 2.0591077112382457

Epoch: 6| Step: 10
Training loss: 2.1238369941711426
Validation loss: 2.0596833536701817

Epoch: 6| Step: 11
Training loss: 2.1161770820617676
Validation loss: 2.0630991343528993

Epoch: 6| Step: 12
Training loss: 2.6218717098236084
Validation loss: 2.051352322742503

Epoch: 6| Step: 13
Training loss: 2.1249170303344727
Validation loss: 2.0526435195758777

Epoch: 24| Step: 0
Training loss: 2.2794604301452637
Validation loss: 2.062788760790261

Epoch: 6| Step: 1
Training loss: 2.2169950008392334
Validation loss: 2.060145874177256

Epoch: 6| Step: 2
Training loss: 2.798884868621826
Validation loss: 2.046189843967397

Epoch: 6| Step: 3
Training loss: 2.14168643951416
Validation loss: 2.0490849864098335

Epoch: 6| Step: 4
Training loss: 2.703887462615967
Validation loss: 2.040562473317628

Epoch: 6| Step: 5
Training loss: 1.6613481044769287
Validation loss: 2.0449407472405383

Epoch: 6| Step: 6
Training loss: 2.6149492263793945
Validation loss: 2.059255865312392

Epoch: 6| Step: 7
Training loss: 2.4825477600097656
Validation loss: 2.074004650115967

Epoch: 6| Step: 8
Training loss: 2.0711452960968018
Validation loss: 2.0584958535368725

Epoch: 6| Step: 9
Training loss: 2.215686559677124
Validation loss: 2.058526213451098

Epoch: 6| Step: 10
Training loss: 2.8080525398254395
Validation loss: 2.054878724518643

Epoch: 6| Step: 11
Training loss: 2.145899534225464
Validation loss: 2.0770450099822013

Epoch: 6| Step: 12
Training loss: 1.6321417093276978
Validation loss: 2.0673872155527913

Epoch: 6| Step: 13
Training loss: 2.8743715286254883
Validation loss: 2.0546610586104856

Epoch: 25| Step: 0
Training loss: 2.0274858474731445
Validation loss: 2.0513118185022825

Epoch: 6| Step: 1
Training loss: 2.1380276679992676
Validation loss: 2.0442998960453975

Epoch: 6| Step: 2
Training loss: 2.484978199005127
Validation loss: 2.05835279598031

Epoch: 6| Step: 3
Training loss: 2.225839138031006
Validation loss: 2.0466683539011146

Epoch: 6| Step: 4
Training loss: 1.89876127243042
Validation loss: 2.0525642248892013

Epoch: 6| Step: 5
Training loss: 2.6578469276428223
Validation loss: 2.0525410098414265

Epoch: 6| Step: 6
Training loss: 2.252847909927368
Validation loss: 2.0759036540985107

Epoch: 6| Step: 7
Training loss: 2.870037317276001
Validation loss: 2.0634372336890108

Epoch: 6| Step: 8
Training loss: 2.445840358734131
Validation loss: 2.071586183322373

Epoch: 6| Step: 9
Training loss: 2.291511058807373
Validation loss: 2.0720873135392384

Epoch: 6| Step: 10
Training loss: 2.133798122406006
Validation loss: 2.0597093592407885

Epoch: 6| Step: 11
Training loss: 2.541839599609375
Validation loss: 2.05272178752448

Epoch: 6| Step: 12
Training loss: 2.3499178886413574
Validation loss: 2.0672911495290776

Epoch: 6| Step: 13
Training loss: 2.2330880165100098
Validation loss: 2.073012633990216

Epoch: 26| Step: 0
Training loss: 2.7562108039855957
Validation loss: 2.0622043942892425

Epoch: 6| Step: 1
Training loss: 1.8529934883117676
Validation loss: 2.0710681228227514

Epoch: 6| Step: 2
Training loss: 2.3982386589050293
Validation loss: 2.049993498350984

Epoch: 6| Step: 3
Training loss: 1.7862440347671509
Validation loss: 2.065408432355491

Epoch: 6| Step: 4
Training loss: 2.149859666824341
Validation loss: 2.0655379167167087

Epoch: 6| Step: 5
Training loss: 2.174466848373413
Validation loss: 2.062102643392419

Epoch: 6| Step: 6
Training loss: 2.1016507148742676
Validation loss: 2.035301152096

Epoch: 6| Step: 7
Training loss: 2.8144659996032715
Validation loss: 2.052312861206711

Epoch: 6| Step: 8
Training loss: 2.151311159133911
Validation loss: 2.050101275085121

Epoch: 6| Step: 9
Training loss: 2.898427724838257
Validation loss: 2.060780877708107

Epoch: 6| Step: 10
Training loss: 2.5593271255493164
Validation loss: 2.067681543288692

Epoch: 6| Step: 11
Training loss: 2.542065382003784
Validation loss: 2.0584640733657347

Epoch: 6| Step: 12
Training loss: 1.7846907377243042
Validation loss: 2.057816336231847

Epoch: 6| Step: 13
Training loss: 2.634300708770752
Validation loss: 2.0578753563665573

Epoch: 27| Step: 0
Training loss: 2.79209041595459
Validation loss: 2.0798342202299382

Epoch: 6| Step: 1
Training loss: 2.0377376079559326
Validation loss: 2.051525221076063

Epoch: 6| Step: 2
Training loss: 2.2147295475006104
Validation loss: 2.059753235950265

Epoch: 6| Step: 3
Training loss: 1.8206722736358643
Validation loss: 2.0655011451372536

Epoch: 6| Step: 4
Training loss: 2.8025546073913574
Validation loss: 2.0528225668015017

Epoch: 6| Step: 5
Training loss: 2.7529029846191406
Validation loss: 2.054737623019885

Epoch: 6| Step: 6
Training loss: 2.832314968109131
Validation loss: 2.0549031931866883

Epoch: 6| Step: 7
Training loss: 2.7911715507507324
Validation loss: 2.057771969867009

Epoch: 6| Step: 8
Training loss: 1.722254991531372
Validation loss: 2.059301527597571

Epoch: 6| Step: 9
Training loss: 1.952842354774475
Validation loss: 2.0692122392756964

Epoch: 6| Step: 10
Training loss: 2.377495050430298
Validation loss: 2.072229311030398

Epoch: 6| Step: 11
Training loss: 1.426928997039795
Validation loss: 2.064576547632935

Epoch: 6| Step: 12
Training loss: 2.52409291267395
Validation loss: 2.0760371761937297

Epoch: 6| Step: 13
Training loss: 2.2383546829223633
Validation loss: 2.081782317930652

Epoch: 28| Step: 0
Training loss: 1.838921070098877
Validation loss: 2.06693075805582

Epoch: 6| Step: 1
Training loss: 1.6804347038269043
Validation loss: 2.0577564572775238

Epoch: 6| Step: 2
Training loss: 2.411728858947754
Validation loss: 2.064772895587388

Epoch: 6| Step: 3
Training loss: 2.118075370788574
Validation loss: 2.066774211904054

Epoch: 6| Step: 4
Training loss: 2.3419957160949707
Validation loss: 2.0766825188872633

Epoch: 6| Step: 5
Training loss: 1.9248470067977905
Validation loss: 2.065547220168575

Epoch: 6| Step: 6
Training loss: 2.798259973526001
Validation loss: 2.0496882994969687

Epoch: 6| Step: 7
Training loss: 1.9084078073501587
Validation loss: 2.04454436866186

Epoch: 6| Step: 8
Training loss: 3.033885955810547
Validation loss: 2.0610248683601298

Epoch: 6| Step: 9
Training loss: 2.3965322971343994
Validation loss: 2.079665558312529

Epoch: 6| Step: 10
Training loss: 2.5659079551696777
Validation loss: 2.064499605086542

Epoch: 6| Step: 11
Training loss: 2.304152488708496
Validation loss: 2.0599193803725706

Epoch: 6| Step: 12
Training loss: 2.729353904724121
Validation loss: 2.063346005255176

Epoch: 6| Step: 13
Training loss: 2.2224600315093994
Validation loss: 2.0481699666669293

Epoch: 29| Step: 0
Training loss: 2.0854742527008057
Validation loss: 2.0692629506511073

Epoch: 6| Step: 1
Training loss: 2.3587169647216797
Validation loss: 2.0590193476728214

Epoch: 6| Step: 2
Training loss: 2.2281386852264404
Validation loss: 2.0616286339298373

Epoch: 6| Step: 3
Training loss: 2.2950448989868164
Validation loss: 2.05677616980768

Epoch: 6| Step: 4
Training loss: 1.9575681686401367
Validation loss: 2.041296661541026

Epoch: 6| Step: 5
Training loss: 2.0835072994232178
Validation loss: 2.055343997093939

Epoch: 6| Step: 6
Training loss: 2.3854947090148926
Validation loss: 2.05263433148784

Epoch: 6| Step: 7
Training loss: 2.425313949584961
Validation loss: 2.065054496129354

Epoch: 6| Step: 8
Training loss: 2.0927248001098633
Validation loss: 2.0663803046749485

Epoch: 6| Step: 9
Training loss: 2.693413734436035
Validation loss: 2.072221940563571

Epoch: 6| Step: 10
Training loss: 2.0464465618133545
Validation loss: 2.064882593770181

Epoch: 6| Step: 11
Training loss: 2.2191076278686523
Validation loss: 2.070335944493612

Epoch: 6| Step: 12
Training loss: 2.4090635776519775
Validation loss: 2.08445022567626

Epoch: 6| Step: 13
Training loss: 3.4711971282958984
Validation loss: 2.0730510591178812

Epoch: 30| Step: 0
Training loss: 2.6567139625549316
Validation loss: 2.065943702574699

Epoch: 6| Step: 1
Training loss: 2.0260143280029297
Validation loss: 2.069214733698035

Epoch: 6| Step: 2
Training loss: 2.172405481338501
Validation loss: 2.0652093989874727

Epoch: 6| Step: 3
Training loss: 1.7197420597076416
Validation loss: 2.0630362649117746

Epoch: 6| Step: 4
Training loss: 2.630859375
Validation loss: 2.0704478333073277

Epoch: 6| Step: 5
Training loss: 2.9332027435302734
Validation loss: 2.0666362534287157

Epoch: 6| Step: 6
Training loss: 1.6968624591827393
Validation loss: 2.0648400488720147

Epoch: 6| Step: 7
Training loss: 2.1866888999938965
Validation loss: 2.0632518440164547

Epoch: 6| Step: 8
Training loss: 2.165503740310669
Validation loss: 2.0662311418082124

Epoch: 6| Step: 9
Training loss: 2.538003921508789
Validation loss: 2.062335636026116

Epoch: 6| Step: 10
Training loss: 2.37496280670166
Validation loss: 2.0536887414993776

Epoch: 6| Step: 11
Training loss: 2.5906615257263184
Validation loss: 2.0748669972983738

Epoch: 6| Step: 12
Training loss: 2.01419997215271
Validation loss: 2.067679887176842

Epoch: 6| Step: 13
Training loss: 2.620811939239502
Validation loss: 2.058659509945941

Epoch: 31| Step: 0
Training loss: 2.1166176795959473
Validation loss: 2.0703103080872567

Epoch: 6| Step: 1
Training loss: 2.5346951484680176
Validation loss: 2.0682987372080484

Epoch: 6| Step: 2
Training loss: 2.319380044937134
Validation loss: 2.0664603338446668

Epoch: 6| Step: 3
Training loss: 2.084946393966675
Validation loss: 2.0573620937203847

Epoch: 6| Step: 4
Training loss: 1.9585609436035156
Validation loss: 2.0653784095600085

Epoch: 6| Step: 5
Training loss: 2.6925055980682373
Validation loss: 2.06902822115088

Epoch: 6| Step: 6
Training loss: 2.236447334289551
Validation loss: 2.068055950185304

Epoch: 6| Step: 7
Training loss: 2.7397894859313965
Validation loss: 2.07002991502003

Epoch: 6| Step: 8
Training loss: 2.3492989540100098
Validation loss: 2.069665023075637

Epoch: 6| Step: 9
Training loss: 1.880742073059082
Validation loss: 2.0709506721906763

Epoch: 6| Step: 10
Training loss: 1.862912654876709
Validation loss: 2.067007733929542

Epoch: 6| Step: 11
Training loss: 2.2799923419952393
Validation loss: 2.063491254724482

Epoch: 6| Step: 12
Training loss: 2.2231640815734863
Validation loss: 2.0633799927209013

Epoch: 6| Step: 13
Training loss: 3.0869300365448
Validation loss: 2.0677709220558085

Epoch: 32| Step: 0
Training loss: 1.885869026184082
Validation loss: 2.0630315683221303

Epoch: 6| Step: 1
Training loss: 1.9301624298095703
Validation loss: 2.058643630755845

Epoch: 6| Step: 2
Training loss: 2.051800489425659
Validation loss: 2.0658767838631906

Epoch: 6| Step: 3
Training loss: 2.7060365676879883
Validation loss: 2.0503179591189147

Epoch: 6| Step: 4
Training loss: 1.8149796724319458
Validation loss: 2.062283656930411

Epoch: 6| Step: 5
Training loss: 2.0195653438568115
Validation loss: 2.0714181007877475

Epoch: 6| Step: 6
Training loss: 2.633474349975586
Validation loss: 2.0540072353937293

Epoch: 6| Step: 7
Training loss: 2.1424708366394043
Validation loss: 2.0708407817348355

Epoch: 6| Step: 8
Training loss: 1.9331953525543213
Validation loss: 2.0606372638415267

Epoch: 6| Step: 9
Training loss: 2.557708263397217
Validation loss: 2.0534731136855258

Epoch: 6| Step: 10
Training loss: 2.453582525253296
Validation loss: 2.0752560041284047

Epoch: 6| Step: 11
Training loss: 2.8344149589538574
Validation loss: 2.049114123467476

Epoch: 6| Step: 12
Training loss: 2.382162094116211
Validation loss: 2.064870593368366

Epoch: 6| Step: 13
Training loss: 3.2693662643432617
Validation loss: 2.063068333492484

Epoch: 33| Step: 0
Training loss: 2.350836753845215
Validation loss: 2.065376253538234

Epoch: 6| Step: 1
Training loss: 2.438321113586426
Validation loss: 2.0648906359108548

Epoch: 6| Step: 2
Training loss: 2.706881046295166
Validation loss: 2.0390399348351265

Epoch: 6| Step: 3
Training loss: 2.648963212966919
Validation loss: 2.056998796360467

Epoch: 6| Step: 4
Training loss: 2.093433141708374
Validation loss: 2.0612923124785065

Epoch: 6| Step: 5
Training loss: 1.9192949533462524
Validation loss: 2.0674170627388904

Epoch: 6| Step: 6
Training loss: 1.7553712129592896
Validation loss: 2.0715251174024356

Epoch: 6| Step: 7
Training loss: 1.8372288942337036
Validation loss: 2.0708146082457675

Epoch: 6| Step: 8
Training loss: 2.5663652420043945
Validation loss: 2.0692136185143584

Epoch: 6| Step: 9
Training loss: 3.105046033859253
Validation loss: 2.0697805855863836

Epoch: 6| Step: 10
Training loss: 1.9819068908691406
Validation loss: 2.0645557449709986

Epoch: 6| Step: 11
Training loss: 2.514596462249756
Validation loss: 2.0792817159365584

Epoch: 6| Step: 12
Training loss: 2.156496047973633
Validation loss: 2.0692924684093845

Epoch: 6| Step: 13
Training loss: 1.9155341386795044
Validation loss: 2.0673457896837624

Epoch: 34| Step: 0
Training loss: 2.9497604370117188
Validation loss: 2.063067110635901

Epoch: 6| Step: 1
Training loss: 2.148744583129883
Validation loss: 2.0505772098418205

Epoch: 6| Step: 2
Training loss: 2.759972095489502
Validation loss: 2.0730754636949107

Epoch: 6| Step: 3
Training loss: 2.2955949306488037
Validation loss: 2.0714101765745427

Epoch: 6| Step: 4
Training loss: 1.7856369018554688
Validation loss: 2.0644729624512377

Epoch: 6| Step: 5
Training loss: 2.2427072525024414
Validation loss: 2.0554299700644707

Epoch: 6| Step: 6
Training loss: 2.685718536376953
Validation loss: 2.0530520395566056

Epoch: 6| Step: 7
Training loss: 2.395197868347168
Validation loss: 2.0611024979622132

Epoch: 6| Step: 8
Training loss: 1.9625113010406494
Validation loss: 2.0638374513195408

Epoch: 6| Step: 9
Training loss: 2.517578125
Validation loss: 2.0530260519314836

Epoch: 6| Step: 10
Training loss: 2.0206098556518555
Validation loss: 2.0665385082203853

Epoch: 6| Step: 11
Training loss: 1.6949195861816406
Validation loss: 2.0545024948735393

Epoch: 6| Step: 12
Training loss: 2.095247983932495
Validation loss: 2.0720479039735693

Epoch: 6| Step: 13
Training loss: 2.957599639892578
Validation loss: 2.0465550012485956

Epoch: 35| Step: 0
Training loss: 1.8469090461730957
Validation loss: 2.075730034100112

Epoch: 6| Step: 1
Training loss: 1.5157414674758911
Validation loss: 2.06569347330319

Epoch: 6| Step: 2
Training loss: 2.3859171867370605
Validation loss: 2.06518877449856

Epoch: 6| Step: 3
Training loss: 2.3952009677886963
Validation loss: 2.0690555546873357

Epoch: 6| Step: 4
Training loss: 2.6205074787139893
Validation loss: 2.0614824730862855

Epoch: 6| Step: 5
Training loss: 2.5721116065979004
Validation loss: 2.071528560371809

Epoch: 6| Step: 6
Training loss: 2.4246392250061035
Validation loss: 2.066088896925731

Epoch: 6| Step: 7
Training loss: 2.58009672164917
Validation loss: 2.0618782056275236

Epoch: 6| Step: 8
Training loss: 2.671417474746704
Validation loss: 2.060704182553035

Epoch: 6| Step: 9
Training loss: 2.79274320602417
Validation loss: 2.0506934709446405

Epoch: 6| Step: 10
Training loss: 2.4415597915649414
Validation loss: 2.0698953520867134

Epoch: 6| Step: 11
Training loss: 1.8079009056091309
Validation loss: 2.0640111277180333

Epoch: 6| Step: 12
Training loss: 1.7893866300582886
Validation loss: 2.061586149277226

Epoch: 6| Step: 13
Training loss: 2.155658006668091
Validation loss: 2.07448398938743

Epoch: 36| Step: 0
Training loss: 1.8872168064117432
Validation loss: 2.0702464170353387

Epoch: 6| Step: 1
Training loss: 2.450974941253662
Validation loss: 2.0473849004314792

Epoch: 6| Step: 2
Training loss: 2.6973416805267334
Validation loss: 2.057364989352483

Epoch: 6| Step: 3
Training loss: 2.0374128818511963
Validation loss: 2.058094145149313

Epoch: 6| Step: 4
Training loss: 2.3363354206085205
Validation loss: 2.050399559800343

Epoch: 6| Step: 5
Training loss: 2.2100305557250977
Validation loss: 2.071576560697248

Epoch: 6| Step: 6
Training loss: 2.2314276695251465
Validation loss: 2.06663014042762

Epoch: 6| Step: 7
Training loss: 2.356527805328369
Validation loss: 2.0581828240425355

Epoch: 6| Step: 8
Training loss: 2.105473279953003
Validation loss: 2.0787241279437976

Epoch: 6| Step: 9
Training loss: 2.673802137374878
Validation loss: 2.047877780852779

Epoch: 6| Step: 10
Training loss: 2.5723366737365723
Validation loss: 2.0510351145139305

Epoch: 6| Step: 11
Training loss: 2.0771737098693848
Validation loss: 2.051334491340063

Epoch: 6| Step: 12
Training loss: 2.0457515716552734
Validation loss: 2.067927401552918

Epoch: 6| Step: 13
Training loss: 2.127411127090454
Validation loss: 2.0492279529571533

Epoch: 37| Step: 0
Training loss: 2.804368019104004
Validation loss: 2.0547585256638063

Epoch: 6| Step: 1
Training loss: 2.7876486778259277
Validation loss: 2.0719813351990073

Epoch: 6| Step: 2
Training loss: 2.0043246746063232
Validation loss: 2.072248653698993

Epoch: 6| Step: 3
Training loss: 1.959993600845337
Validation loss: 2.0601335789567683

Epoch: 6| Step: 4
Training loss: 1.6425034999847412
Validation loss: 2.055962811234177

Epoch: 6| Step: 5
Training loss: 2.5888659954071045
Validation loss: 2.0456198312902965

Epoch: 6| Step: 6
Training loss: 2.517547130584717
Validation loss: 2.0719973348802134

Epoch: 6| Step: 7
Training loss: 1.955690860748291
Validation loss: 2.0371021070787982

Epoch: 6| Step: 8
Training loss: 2.784937858581543
Validation loss: 2.068481856776822

Epoch: 6| Step: 9
Training loss: 2.5589206218719482
Validation loss: 2.056147236977854

Epoch: 6| Step: 10
Training loss: 1.8115826845169067
Validation loss: 2.0550768003668836

Epoch: 6| Step: 11
Training loss: 2.468724012374878
Validation loss: 2.048809705242034

Epoch: 6| Step: 12
Training loss: 1.6416311264038086
Validation loss: 2.0569174917795325

Epoch: 6| Step: 13
Training loss: 2.4321815967559814
Validation loss: 2.0609381403974307

Epoch: 38| Step: 0
Training loss: 2.0166358947753906
Validation loss: 2.054070827781513

Epoch: 6| Step: 1
Training loss: 2.5895581245422363
Validation loss: 2.0608328183492026

Epoch: 6| Step: 2
Training loss: 2.09881591796875
Validation loss: 2.0744622497148413

Epoch: 6| Step: 3
Training loss: 2.3251793384552
Validation loss: 2.04972703226151

Epoch: 6| Step: 4
Training loss: 1.965759515762329
Validation loss: 2.07278452381011

Epoch: 6| Step: 5
Training loss: 2.0780930519104004
Validation loss: 2.0588061860812608

Epoch: 6| Step: 6
Training loss: 2.5875368118286133
Validation loss: 2.0600921184785905

Epoch: 6| Step: 7
Training loss: 1.552147626876831
Validation loss: 2.0713143015420563

Epoch: 6| Step: 8
Training loss: 2.361219644546509
Validation loss: 2.0702749759920183

Epoch: 6| Step: 9
Training loss: 2.2518746852874756
Validation loss: 2.057598001213484

Epoch: 6| Step: 10
Training loss: 2.810420513153076
Validation loss: 2.055306621777114

Epoch: 6| Step: 11
Training loss: 2.592785120010376
Validation loss: 2.083040907818784

Epoch: 6| Step: 12
Training loss: 2.4179112911224365
Validation loss: 2.0439021343825967

Epoch: 6| Step: 13
Training loss: 2.367011785507202
Validation loss: 2.063654820124308

Epoch: 39| Step: 0
Training loss: 2.448774576187134
Validation loss: 2.0480030505887923

Epoch: 6| Step: 1
Training loss: 2.651461601257324
Validation loss: 2.0611518352262435

Epoch: 6| Step: 2
Training loss: 3.1686909198760986
Validation loss: 2.0616803887069866

Epoch: 6| Step: 3
Training loss: 2.449655055999756
Validation loss: 2.050847581637803

Epoch: 6| Step: 4
Training loss: 2.3091373443603516
Validation loss: 2.052801811566917

Epoch: 6| Step: 5
Training loss: 1.4221161603927612
Validation loss: 2.0883051656907603

Epoch: 6| Step: 6
Training loss: 2.2287354469299316
Validation loss: 2.056857790998233

Epoch: 6| Step: 7
Training loss: 1.8990408182144165
Validation loss: 2.056336247792808

Epoch: 6| Step: 8
Training loss: 1.6715967655181885
Validation loss: 2.0581206506298435

Epoch: 6| Step: 9
Training loss: 2.4408397674560547
Validation loss: 2.078088786012383

Epoch: 6| Step: 10
Training loss: 1.8730372190475464
Validation loss: 2.065872077018984

Epoch: 6| Step: 11
Training loss: 2.577529191970825
Validation loss: 2.0695283464206162

Epoch: 6| Step: 12
Training loss: 2.371755599975586
Validation loss: 2.0743909728142524

Epoch: 6| Step: 13
Training loss: 2.3394415378570557
Validation loss: 2.0446584583610616

Epoch: 40| Step: 0
Training loss: 2.332895517349243
Validation loss: 2.050360348916823

Epoch: 6| Step: 1
Training loss: 1.7911518812179565
Validation loss: 2.052775511177637

Epoch: 6| Step: 2
Training loss: 2.3802144527435303
Validation loss: 2.0627432228416525

Epoch: 6| Step: 3
Training loss: 2.4770102500915527
Validation loss: 2.0635432402292886

Epoch: 6| Step: 4
Training loss: 1.5533788204193115
Validation loss: 2.054140244760821

Epoch: 6| Step: 5
Training loss: 2.4363629817962646
Validation loss: 2.077376365661621

Epoch: 6| Step: 6
Training loss: 2.4247946739196777
Validation loss: 2.0528953049772527

Epoch: 6| Step: 7
Training loss: 2.078718662261963
Validation loss: 2.044174236636008

Epoch: 6| Step: 8
Training loss: 1.7177479267120361
Validation loss: 2.0582241166022515

Epoch: 6| Step: 9
Training loss: 2.9380483627319336
Validation loss: 2.0630124076720207

Epoch: 6| Step: 10
Training loss: 2.0649397373199463
Validation loss: 2.071881607014646

Epoch: 6| Step: 11
Training loss: 2.490201473236084
Validation loss: 2.055311106866406

Epoch: 6| Step: 12
Training loss: 2.646066665649414
Validation loss: 2.06257971384192

Epoch: 6| Step: 13
Training loss: 2.567645311355591
Validation loss: 2.0532787640889487

Epoch: 41| Step: 0
Training loss: 2.172985076904297
Validation loss: 2.0583695775719097

Epoch: 6| Step: 1
Training loss: 1.7904071807861328
Validation loss: 2.0658874883446643

Epoch: 6| Step: 2
Training loss: 1.9100042581558228
Validation loss: 2.0545997645265315

Epoch: 6| Step: 3
Training loss: 2.1849896907806396
Validation loss: 2.0657105368952595

Epoch: 6| Step: 4
Training loss: 3.1332643032073975
Validation loss: 2.063593524758534

Epoch: 6| Step: 5
Training loss: 2.647580623626709
Validation loss: 2.0556952773883777

Epoch: 6| Step: 6
Training loss: 2.415958881378174
Validation loss: 2.046548710074476

Epoch: 6| Step: 7
Training loss: 2.3820035457611084
Validation loss: 2.045647987755396

Epoch: 6| Step: 8
Training loss: 2.383951187133789
Validation loss: 2.0547789245523433

Epoch: 6| Step: 9
Training loss: 1.8749363422393799
Validation loss: 2.063837533356041

Epoch: 6| Step: 10
Training loss: 1.7148287296295166
Validation loss: 2.059984741672393

Epoch: 6| Step: 11
Training loss: 2.837773561477661
Validation loss: 2.0487786492993756

Epoch: 6| Step: 12
Training loss: 2.042250156402588
Validation loss: 2.0725179692750335

Epoch: 6| Step: 13
Training loss: 2.3253679275512695
Validation loss: 2.060329234728249

Epoch: 42| Step: 0
Training loss: 2.827453136444092
Validation loss: 2.0564837109658027

Epoch: 6| Step: 1
Training loss: 2.9017152786254883
Validation loss: 2.0382955740856867

Epoch: 6| Step: 2
Training loss: 1.8539495468139648
Validation loss: 2.0460821172242523

Epoch: 6| Step: 3
Training loss: 1.6349890232086182
Validation loss: 2.062312972161078

Epoch: 6| Step: 4
Training loss: 1.9994628429412842
Validation loss: 2.069689699398574

Epoch: 6| Step: 5
Training loss: 2.1904397010803223
Validation loss: 2.046749799482284

Epoch: 6| Step: 6
Training loss: 2.244670867919922
Validation loss: 2.0560451246077016

Epoch: 6| Step: 7
Training loss: 2.1975812911987305
Validation loss: 2.0476393417645524

Epoch: 6| Step: 8
Training loss: 2.6972954273223877
Validation loss: 2.04261799012461

Epoch: 6| Step: 9
Training loss: 2.29668927192688
Validation loss: 2.05372917139402

Epoch: 6| Step: 10
Training loss: 1.9280966520309448
Validation loss: 2.0683114849111086

Epoch: 6| Step: 11
Training loss: 2.101630449295044
Validation loss: 2.0494946818197928

Epoch: 6| Step: 12
Training loss: 2.186626672744751
Validation loss: 2.059892508291429

Epoch: 6| Step: 13
Training loss: 2.7733914852142334
Validation loss: 2.049361078969894

Epoch: 43| Step: 0
Training loss: 2.7538161277770996
Validation loss: 2.0741082468340473

Epoch: 6| Step: 1
Training loss: 1.876343846321106
Validation loss: 2.059083911680406

Epoch: 6| Step: 2
Training loss: 2.118366003036499
Validation loss: 2.0311989297149

Epoch: 6| Step: 3
Training loss: 2.304928779602051
Validation loss: 2.062253244461552

Epoch: 6| Step: 4
Training loss: 1.7017345428466797
Validation loss: 2.0547455305694253

Epoch: 6| Step: 5
Training loss: 2.5043373107910156
Validation loss: 2.052893861647575

Epoch: 6| Step: 6
Training loss: 2.0509347915649414
Validation loss: 2.0714210720472437

Epoch: 6| Step: 7
Training loss: 2.2434487342834473
Validation loss: 2.0548452613174275

Epoch: 6| Step: 8
Training loss: 2.271374225616455
Validation loss: 2.06898138600011

Epoch: 6| Step: 9
Training loss: 2.5786967277526855
Validation loss: 2.0671131623688566

Epoch: 6| Step: 10
Training loss: 2.6157078742980957
Validation loss: 2.057773733651766

Epoch: 6| Step: 11
Training loss: 2.446967840194702
Validation loss: 2.071162248170504

Epoch: 6| Step: 12
Training loss: 2.339496612548828
Validation loss: 2.0658730588933474

Epoch: 6| Step: 13
Training loss: 1.7563645839691162
Validation loss: 2.048226338560863

Epoch: 44| Step: 0
Training loss: 2.470945119857788
Validation loss: 2.066386292057653

Epoch: 6| Step: 1
Training loss: 1.9677212238311768
Validation loss: 2.0601061697929137

Epoch: 6| Step: 2
Training loss: 2.9848275184631348
Validation loss: 2.0623151089555476

Epoch: 6| Step: 3
Training loss: 1.998463749885559
Validation loss: 2.0611010495052544

Epoch: 6| Step: 4
Training loss: 2.7229294776916504
Validation loss: 2.0651659555332635

Epoch: 6| Step: 5
Training loss: 2.204796314239502
Validation loss: 2.0821618546721754

Epoch: 6| Step: 6
Training loss: 2.132991313934326
Validation loss: 2.045350513150615

Epoch: 6| Step: 7
Training loss: 2.0404751300811768
Validation loss: 2.0660217026228547

Epoch: 6| Step: 8
Training loss: 1.802724003791809
Validation loss: 2.0864313289683354

Epoch: 6| Step: 9
Training loss: 2.3085455894470215
Validation loss: 2.077262686144921

Epoch: 6| Step: 10
Training loss: 2.5388593673706055
Validation loss: 2.050012896137853

Epoch: 6| Step: 11
Training loss: 2.098843574523926
Validation loss: 2.0622800652698805

Epoch: 6| Step: 12
Training loss: 2.349998950958252
Validation loss: 2.0508563890252063

Epoch: 6| Step: 13
Training loss: 1.9242594242095947
Validation loss: 2.0615216814061648

Epoch: 45| Step: 0
Training loss: 2.09818696975708
Validation loss: 2.0644709333296745

Epoch: 6| Step: 1
Training loss: 2.1098408699035645
Validation loss: 2.0567218257534887

Epoch: 6| Step: 2
Training loss: 2.839951992034912
Validation loss: 2.0651716288699897

Epoch: 6| Step: 3
Training loss: 2.1633853912353516
Validation loss: 2.065574589596

Epoch: 6| Step: 4
Training loss: 2.2320871353149414
Validation loss: 2.046508055861278

Epoch: 6| Step: 5
Training loss: 2.21359920501709
Validation loss: 2.0632827166588075

Epoch: 6| Step: 6
Training loss: 2.3666982650756836
Validation loss: 2.0670195497492307

Epoch: 6| Step: 7
Training loss: 1.880427598953247
Validation loss: 2.0616782262761104

Epoch: 6| Step: 8
Training loss: 1.8969330787658691
Validation loss: 2.061280245422035

Epoch: 6| Step: 9
Training loss: 2.506563663482666
Validation loss: 2.041388887231068

Epoch: 6| Step: 10
Training loss: 2.26379656791687
Validation loss: 2.0579759818251415

Epoch: 6| Step: 11
Training loss: 2.8353750705718994
Validation loss: 2.054822257770005

Epoch: 6| Step: 12
Training loss: 2.082205057144165
Validation loss: 2.053398019524031

Epoch: 6| Step: 13
Training loss: 2.000640630722046
Validation loss: 2.050482793520856

Epoch: 46| Step: 0
Training loss: 2.371896982192993
Validation loss: 2.0550418643541235

Epoch: 6| Step: 1
Training loss: 2.6224288940429688
Validation loss: 2.0700168481437107

Epoch: 6| Step: 2
Training loss: 1.7507575750350952
Validation loss: 2.044036316615279

Epoch: 6| Step: 3
Training loss: 2.4712109565734863
Validation loss: 2.0528854477790093

Epoch: 6| Step: 4
Training loss: 2.000175952911377
Validation loss: 2.0488580426862164

Epoch: 6| Step: 5
Training loss: 1.8820741176605225
Validation loss: 2.0623674161972536

Epoch: 6| Step: 6
Training loss: 2.356088638305664
Validation loss: 2.0654336944703133

Epoch: 6| Step: 7
Training loss: 1.8713269233703613
Validation loss: 2.0579841829115346

Epoch: 6| Step: 8
Training loss: 2.8956546783447266
Validation loss: 2.0604939614572833

Epoch: 6| Step: 9
Training loss: 1.8756630420684814
Validation loss: 2.0580905329796577

Epoch: 6| Step: 10
Training loss: 2.616692066192627
Validation loss: 2.05514092855556

Epoch: 6| Step: 11
Training loss: 1.792833685874939
Validation loss: 2.055366018766998

Epoch: 6| Step: 12
Training loss: 2.660179615020752
Validation loss: 2.0782316743686633

Epoch: 6| Step: 13
Training loss: 2.897092819213867
Validation loss: 2.055880860615802

Epoch: 47| Step: 0
Training loss: 2.277448892593384
Validation loss: 2.069542997626848

Epoch: 6| Step: 1
Training loss: 1.931514024734497
Validation loss: 2.0842508346803728

Epoch: 6| Step: 2
Training loss: 2.356245279312134
Validation loss: 2.0600262559870237

Epoch: 6| Step: 3
Training loss: 2.164375066757202
Validation loss: 2.051221519388178

Epoch: 6| Step: 4
Training loss: 2.6326942443847656
Validation loss: 2.043537314220141

Epoch: 6| Step: 5
Training loss: 2.06197190284729
Validation loss: 2.0479127899292977

Epoch: 6| Step: 6
Training loss: 1.7498672008514404
Validation loss: 2.053722334164445

Epoch: 6| Step: 7
Training loss: 2.530569076538086
Validation loss: 2.0508733846807994

Epoch: 6| Step: 8
Training loss: 2.2255055904388428
Validation loss: 2.048041048870292

Epoch: 6| Step: 9
Training loss: 2.3132734298706055
Validation loss: 2.055205924536592

Epoch: 6| Step: 10
Training loss: 2.0479073524475098
Validation loss: 2.0516566973860546

Epoch: 6| Step: 11
Training loss: 2.4232120513916016
Validation loss: 2.0531320828263477

Epoch: 6| Step: 12
Training loss: 2.3049092292785645
Validation loss: 2.059962295716809

Epoch: 6| Step: 13
Training loss: 2.8637242317199707
Validation loss: 2.0668960976344284

Epoch: 48| Step: 0
Training loss: 2.2482268810272217
Validation loss: 2.044665326354324

Epoch: 6| Step: 1
Training loss: 1.8911383152008057
Validation loss: 2.041086530172697

Epoch: 6| Step: 2
Training loss: 2.8943777084350586
Validation loss: 2.0574382761473298

Epoch: 6| Step: 3
Training loss: 3.189457416534424
Validation loss: 2.050984610793411

Epoch: 6| Step: 4
Training loss: 1.681549310684204
Validation loss: 2.0555717278552312

Epoch: 6| Step: 5
Training loss: 2.648073673248291
Validation loss: 2.0720853472268708

Epoch: 6| Step: 6
Training loss: 2.4741268157958984
Validation loss: 2.0686959246153473

Epoch: 6| Step: 7
Training loss: 2.082303047180176
Validation loss: 2.0681485129940893

Epoch: 6| Step: 8
Training loss: 1.6243054866790771
Validation loss: 2.0379253023414203

Epoch: 6| Step: 9
Training loss: 2.0547282695770264
Validation loss: 2.039442898124777

Epoch: 6| Step: 10
Training loss: 2.0152149200439453
Validation loss: 2.064103226507864

Epoch: 6| Step: 11
Training loss: 2.751676082611084
Validation loss: 2.050838138467522

Epoch: 6| Step: 12
Training loss: 2.2070939540863037
Validation loss: 2.059558932499219

Epoch: 6| Step: 13
Training loss: 1.4896634817123413
Validation loss: 2.0400173228274108

Epoch: 49| Step: 0
Training loss: 1.9337761402130127
Validation loss: 2.078028722475934

Epoch: 6| Step: 1
Training loss: 2.2205066680908203
Validation loss: 2.079072288287583

Epoch: 6| Step: 2
Training loss: 2.495882511138916
Validation loss: 2.0568758928647606

Epoch: 6| Step: 3
Training loss: 1.9224321842193604
Validation loss: 2.062162055764147

Epoch: 6| Step: 4
Training loss: 2.2883424758911133
Validation loss: 2.053173306167767

Epoch: 6| Step: 5
Training loss: 2.2895212173461914
Validation loss: 2.0508551828322874

Epoch: 6| Step: 6
Training loss: 1.5994296073913574
Validation loss: 2.0725408536131664

Epoch: 6| Step: 7
Training loss: 2.0675103664398193
Validation loss: 2.0611042053468767

Epoch: 6| Step: 8
Training loss: 2.7145605087280273
Validation loss: 2.0521392847902034

Epoch: 6| Step: 9
Training loss: 2.504857301712036
Validation loss: 2.073117394601145

Epoch: 6| Step: 10
Training loss: 2.3552422523498535
Validation loss: 2.053588528786936

Epoch: 6| Step: 11
Training loss: 1.595934510231018
Validation loss: 2.051685090987913

Epoch: 6| Step: 12
Training loss: 3.088070869445801
Validation loss: 2.0678544762314006

Epoch: 6| Step: 13
Training loss: 2.5335893630981445
Validation loss: 2.0618831111538793

Epoch: 50| Step: 0
Training loss: 2.5256600379943848
Validation loss: 2.044515348249866

Epoch: 6| Step: 1
Training loss: 1.7654310464859009
Validation loss: 2.073986940486457

Epoch: 6| Step: 2
Training loss: 2.3013157844543457
Validation loss: 2.05229010633243

Epoch: 6| Step: 3
Training loss: 1.8350553512573242
Validation loss: 2.082306645249808

Epoch: 6| Step: 4
Training loss: 1.781498670578003
Validation loss: 2.0628622552399993

Epoch: 6| Step: 5
Training loss: 2.3017094135284424
Validation loss: 2.0674510194409277

Epoch: 6| Step: 6
Training loss: 2.3210558891296387
Validation loss: 2.0669095990478352

Epoch: 6| Step: 7
Training loss: 2.919429302215576
Validation loss: 2.077101287021432

Epoch: 6| Step: 8
Training loss: 2.780789375305176
Validation loss: 2.082448326131349

Epoch: 6| Step: 9
Training loss: 1.9881571531295776
Validation loss: 2.0729618751874535

Epoch: 6| Step: 10
Training loss: 2.606330156326294
Validation loss: 2.0676666305911158

Epoch: 6| Step: 11
Training loss: 1.8282387256622314
Validation loss: 2.066876511419973

Epoch: 6| Step: 12
Training loss: 2.2591874599456787
Validation loss: 2.0690416443732476

Epoch: 6| Step: 13
Training loss: 2.6728620529174805
Validation loss: 2.072347761482321

Epoch: 51| Step: 0
Training loss: 2.1577939987182617
Validation loss: 2.0654749921573106

Epoch: 6| Step: 1
Training loss: 2.4173800945281982
Validation loss: 2.06062824239013

Epoch: 6| Step: 2
Training loss: 2.2975716590881348
Validation loss: 2.0773224561445174

Epoch: 6| Step: 3
Training loss: 1.7359870672225952
Validation loss: 2.0596788390990226

Epoch: 6| Step: 4
Training loss: 1.8418265581130981
Validation loss: 2.0630059472976194

Epoch: 6| Step: 5
Training loss: 1.544970154762268
Validation loss: 2.0564369155514624

Epoch: 6| Step: 6
Training loss: 2.75295090675354
Validation loss: 2.051239675091159

Epoch: 6| Step: 7
Training loss: 3.624377727508545
Validation loss: 2.058368452133671

Epoch: 6| Step: 8
Training loss: 1.972982406616211
Validation loss: 2.0674406264417913

Epoch: 6| Step: 9
Training loss: 2.33549165725708
Validation loss: 2.075431472511702

Epoch: 6| Step: 10
Training loss: 2.504957437515259
Validation loss: 2.0451952565100884

Epoch: 6| Step: 11
Training loss: 2.402972936630249
Validation loss: 2.057126416954943

Epoch: 6| Step: 12
Training loss: 1.7876026630401611
Validation loss: 2.04826497262524

Epoch: 6| Step: 13
Training loss: 2.2012367248535156
Validation loss: 2.0479972208699873

Epoch: 52| Step: 0
Training loss: 2.7758970260620117
Validation loss: 2.0597806951051116

Epoch: 6| Step: 1
Training loss: 2.005070209503174
Validation loss: 2.054143010929067

Epoch: 6| Step: 2
Training loss: 2.347085475921631
Validation loss: 2.068032117300136

Epoch: 6| Step: 3
Training loss: 2.6248340606689453
Validation loss: 2.061993093900783

Epoch: 6| Step: 4
Training loss: 1.8958384990692139
Validation loss: 2.0546454703936012

Epoch: 6| Step: 5
Training loss: 2.1462032794952393
Validation loss: 2.07711366427842

Epoch: 6| Step: 6
Training loss: 1.953489899635315
Validation loss: 2.0429595670392438

Epoch: 6| Step: 7
Training loss: 2.1610562801361084
Validation loss: 2.061293782726411

Epoch: 6| Step: 8
Training loss: 1.6897356510162354
Validation loss: 2.0620525652362454

Epoch: 6| Step: 9
Training loss: 2.4943597316741943
Validation loss: 2.050788483312053

Epoch: 6| Step: 10
Training loss: 2.6049342155456543
Validation loss: 2.0498004087837796

Epoch: 6| Step: 11
Training loss: 2.613107204437256
Validation loss: 2.0454990812527236

Epoch: 6| Step: 12
Training loss: 2.06929349899292
Validation loss: 2.0348213488055813

Epoch: 6| Step: 13
Training loss: 2.0665478706359863
Validation loss: 2.046088500689435

Epoch: 53| Step: 0
Training loss: 2.4814705848693848
Validation loss: 2.0743384412539903

Epoch: 6| Step: 1
Training loss: 2.0992891788482666
Validation loss: 2.0725960449505876

Epoch: 6| Step: 2
Training loss: 1.8479095697402954
Validation loss: 2.0504393577575684

Epoch: 6| Step: 3
Training loss: 2.4315710067749023
Validation loss: 2.0533114479434107

Epoch: 6| Step: 4
Training loss: 2.001741647720337
Validation loss: 2.0545056866061304

Epoch: 6| Step: 5
Training loss: 2.6676242351531982
Validation loss: 2.0415782236283824

Epoch: 6| Step: 6
Training loss: 2.3882365226745605
Validation loss: 2.058989063385994

Epoch: 6| Step: 7
Training loss: 2.590442657470703
Validation loss: 2.050343328906644

Epoch: 6| Step: 8
Training loss: 2.2264816761016846
Validation loss: 2.0472706979320896

Epoch: 6| Step: 9
Training loss: 2.962109088897705
Validation loss: 2.0458453598842827

Epoch: 6| Step: 10
Training loss: 2.02894926071167
Validation loss: 2.0598823537108717

Epoch: 6| Step: 11
Training loss: 1.7217605113983154
Validation loss: 2.056199507046771

Epoch: 6| Step: 12
Training loss: 1.583578109741211
Validation loss: 2.0495038660623695

Epoch: 6| Step: 13
Training loss: 2.6250340938568115
Validation loss: 2.048146637537146

Epoch: 54| Step: 0
Training loss: 2.936526298522949
Validation loss: 2.051870596024298

Epoch: 6| Step: 1
Training loss: 2.9880051612854004
Validation loss: 2.0569770233605498

Epoch: 6| Step: 2
Training loss: 1.9051841497421265
Validation loss: 2.0577583415533907

Epoch: 6| Step: 3
Training loss: 2.0958735942840576
Validation loss: 2.079913700780561

Epoch: 6| Step: 4
Training loss: 2.3692355155944824
Validation loss: 2.0547845594344603

Epoch: 6| Step: 5
Training loss: 1.8302814960479736
Validation loss: 2.0503257795046737

Epoch: 6| Step: 6
Training loss: 2.22990083694458
Validation loss: 2.0573898182120374

Epoch: 6| Step: 7
Training loss: 2.016705274581909
Validation loss: 2.0717488091479064

Epoch: 6| Step: 8
Training loss: 2.375882863998413
Validation loss: 2.0712846107380365

Epoch: 6| Step: 9
Training loss: 1.8364481925964355
Validation loss: 2.043041721467049

Epoch: 6| Step: 10
Training loss: 2.2922704219818115
Validation loss: 2.046088857035483

Epoch: 6| Step: 11
Training loss: 1.9407521486282349
Validation loss: 2.043924895665979

Epoch: 6| Step: 12
Training loss: 2.134242534637451
Validation loss: 2.058656463059046

Epoch: 6| Step: 13
Training loss: 2.643734931945801
Validation loss: 2.0594679501748856

Epoch: 55| Step: 0
Training loss: 2.0099596977233887
Validation loss: 2.056551082159883

Epoch: 6| Step: 1
Training loss: 2.5986170768737793
Validation loss: 2.0468502698406095

Epoch: 6| Step: 2
Training loss: 2.0802297592163086
Validation loss: 2.065426309903463

Epoch: 6| Step: 3
Training loss: 2.05002498626709
Validation loss: 2.061256170272827

Epoch: 6| Step: 4
Training loss: 3.119044542312622
Validation loss: 2.046300274069591

Epoch: 6| Step: 5
Training loss: 2.5198965072631836
Validation loss: 2.0653482150006037

Epoch: 6| Step: 6
Training loss: 2.090040683746338
Validation loss: 2.054364612025599

Epoch: 6| Step: 7
Training loss: 2.443425178527832
Validation loss: 2.039967203652987

Epoch: 6| Step: 8
Training loss: 2.296734094619751
Validation loss: 2.0668609539667764

Epoch: 6| Step: 9
Training loss: 2.3604001998901367
Validation loss: 2.0590010509696057

Epoch: 6| Step: 10
Training loss: 1.9646958112716675
Validation loss: 2.0790239495615803

Epoch: 6| Step: 11
Training loss: 2.1679205894470215
Validation loss: 2.055680380072645

Epoch: 6| Step: 12
Training loss: 1.8796052932739258
Validation loss: 2.066766619682312

Epoch: 6| Step: 13
Training loss: 1.2113428115844727
Validation loss: 2.0615303336933093

Epoch: 56| Step: 0
Training loss: 2.6726455688476562
Validation loss: 2.0609868777695524

Epoch: 6| Step: 1
Training loss: 1.9457520246505737
Validation loss: 2.062032109947615

Epoch: 6| Step: 2
Training loss: 2.3321285247802734
Validation loss: 2.0400854759318854

Epoch: 6| Step: 3
Training loss: 3.2863731384277344
Validation loss: 2.0460121887986378

Epoch: 6| Step: 4
Training loss: 1.9161269664764404
Validation loss: 2.066020452848045

Epoch: 6| Step: 5
Training loss: 2.4146041870117188
Validation loss: 2.0616294094311294

Epoch: 6| Step: 6
Training loss: 2.1580519676208496
Validation loss: 2.0620653757485012

Epoch: 6| Step: 7
Training loss: 2.2378711700439453
Validation loss: 2.0578072250530286

Epoch: 6| Step: 8
Training loss: 1.993107557296753
Validation loss: 2.05160706017607

Epoch: 6| Step: 9
Training loss: 1.9320744276046753
Validation loss: 2.053995604156166

Epoch: 6| Step: 10
Training loss: 2.342825412750244
Validation loss: 2.0675268673127696

Epoch: 6| Step: 11
Training loss: 2.0955731868743896
Validation loss: 2.0699954955808577

Epoch: 6| Step: 12
Training loss: 1.7312073707580566
Validation loss: 2.0566906544470016

Epoch: 6| Step: 13
Training loss: 2.5591492652893066
Validation loss: 2.0544291901332077

Epoch: 57| Step: 0
Training loss: 1.9572964906692505
Validation loss: 2.0551482233949887

Epoch: 6| Step: 1
Training loss: 2.0495128631591797
Validation loss: 2.0533961788300545

Epoch: 6| Step: 2
Training loss: 3.200934886932373
Validation loss: 2.0562755779553483

Epoch: 6| Step: 3
Training loss: 1.8053096532821655
Validation loss: 2.055208247195008

Epoch: 6| Step: 4
Training loss: 1.7535877227783203
Validation loss: 2.044113407852829

Epoch: 6| Step: 5
Training loss: 2.0449137687683105
Validation loss: 2.033982164116316

Epoch: 6| Step: 6
Training loss: 2.8666110038757324
Validation loss: 2.053330047156221

Epoch: 6| Step: 7
Training loss: 3.0306577682495117
Validation loss: 2.0604181776764574

Epoch: 6| Step: 8
Training loss: 1.8896232843399048
Validation loss: 2.0579619189744354

Epoch: 6| Step: 9
Training loss: 2.2181501388549805
Validation loss: 2.049994630198325

Epoch: 6| Step: 10
Training loss: 2.2963201999664307
Validation loss: 2.0606778937001384

Epoch: 6| Step: 11
Training loss: 2.1767196655273438
Validation loss: 2.071217683053786

Epoch: 6| Step: 12
Training loss: 1.9381619691848755
Validation loss: 2.0650301364160355

Epoch: 6| Step: 13
Training loss: 2.3014864921569824
Validation loss: 2.05144436641406

Epoch: 58| Step: 0
Training loss: 1.8456025123596191
Validation loss: 2.055153485267393

Epoch: 6| Step: 1
Training loss: 2.008913993835449
Validation loss: 2.055111976080043

Epoch: 6| Step: 2
Training loss: 1.9553011655807495
Validation loss: 2.0462793701438495

Epoch: 6| Step: 3
Training loss: 2.0261857509613037
Validation loss: 2.043932584024245

Epoch: 6| Step: 4
Training loss: 2.306572437286377
Validation loss: 2.0479787857301774

Epoch: 6| Step: 5
Training loss: 1.9525370597839355
Validation loss: 2.0671234541041876

Epoch: 6| Step: 6
Training loss: 2.650135040283203
Validation loss: 2.0404638910806305

Epoch: 6| Step: 7
Training loss: 2.2393336296081543
Validation loss: 2.063273272206706

Epoch: 6| Step: 8
Training loss: 2.431356191635132
Validation loss: 2.057201249625093

Epoch: 6| Step: 9
Training loss: 1.9438302516937256
Validation loss: 2.0460864664405904

Epoch: 6| Step: 10
Training loss: 1.9473323822021484
Validation loss: 2.0507405240048646

Epoch: 6| Step: 11
Training loss: 2.130831718444824
Validation loss: 2.046967303881081

Epoch: 6| Step: 12
Training loss: 3.2697629928588867
Validation loss: 2.065281755180769

Epoch: 6| Step: 13
Training loss: 3.1503021717071533
Validation loss: 2.0595833281035065

Epoch: 59| Step: 0
Training loss: 1.6344716548919678
Validation loss: 2.0284040922759683

Epoch: 6| Step: 1
Training loss: 1.9298820495605469
Validation loss: 2.0671821960838894

Epoch: 6| Step: 2
Training loss: 1.8818978071212769
Validation loss: 2.0473828110643613

Epoch: 6| Step: 3
Training loss: 2.7017502784729004
Validation loss: 2.0615777725814493

Epoch: 6| Step: 4
Training loss: 1.7233364582061768
Validation loss: 2.0537082072227233

Epoch: 6| Step: 5
Training loss: 2.3587048053741455
Validation loss: 2.0579703084884153

Epoch: 6| Step: 6
Training loss: 2.1636545658111572
Validation loss: 2.0582729026835453

Epoch: 6| Step: 7
Training loss: 2.7508697509765625
Validation loss: 2.044732727030272

Epoch: 6| Step: 8
Training loss: 2.38075590133667
Validation loss: 2.0367070603114303

Epoch: 6| Step: 9
Training loss: 3.362980365753174
Validation loss: 2.0740908371504916

Epoch: 6| Step: 10
Training loss: 1.9892747402191162
Validation loss: 2.0763767637232298

Epoch: 6| Step: 11
Training loss: 1.80398690700531
Validation loss: 2.0504931762654293

Epoch: 6| Step: 12
Training loss: 2.4767074584960938
Validation loss: 2.0517809621749388

Epoch: 6| Step: 13
Training loss: 2.3479745388031006
Validation loss: 2.0618812678962626

Epoch: 60| Step: 0
Training loss: 2.078169584274292
Validation loss: 2.051484980890828

Epoch: 6| Step: 1
Training loss: 2.3504984378814697
Validation loss: 2.0561590374156995

Epoch: 6| Step: 2
Training loss: 1.957047939300537
Validation loss: 2.049907466416718

Epoch: 6| Step: 3
Training loss: 3.175546646118164
Validation loss: 2.036951570100682

Epoch: 6| Step: 4
Training loss: 2.07418155670166
Validation loss: 2.033614630340248

Epoch: 6| Step: 5
Training loss: 1.9538856744766235
Validation loss: 2.05899106302569

Epoch: 6| Step: 6
Training loss: 2.490128517150879
Validation loss: 2.061884198137509

Epoch: 6| Step: 7
Training loss: 2.1515798568725586
Validation loss: 2.0458171572736514

Epoch: 6| Step: 8
Training loss: 1.4584037065505981
Validation loss: 2.0422674250859085

Epoch: 6| Step: 9
Training loss: 2.2040932178497314
Validation loss: 2.063189221966651

Epoch: 6| Step: 10
Training loss: 2.962211847305298
Validation loss: 2.0600837225555093

Epoch: 6| Step: 11
Training loss: 2.181601047515869
Validation loss: 2.061181401693693

Epoch: 6| Step: 12
Training loss: 1.9531264305114746
Validation loss: 2.0575569778360348

Epoch: 6| Step: 13
Training loss: 2.4573919773101807
Validation loss: 2.0403676045838224

Epoch: 61| Step: 0
Training loss: 2.3641295433044434
Validation loss: 2.0574715470754974

Epoch: 6| Step: 1
Training loss: 2.03635835647583
Validation loss: 2.069046663981612

Epoch: 6| Step: 2
Training loss: 1.7003612518310547
Validation loss: 2.058833699072561

Epoch: 6| Step: 3
Training loss: 2.5245795249938965
Validation loss: 2.0570015702196347

Epoch: 6| Step: 4
Training loss: 2.418666362762451
Validation loss: 2.0518803583678378

Epoch: 6| Step: 5
Training loss: 2.6544365882873535
Validation loss: 2.056596076616677

Epoch: 6| Step: 6
Training loss: 2.636279582977295
Validation loss: 2.0326007591780795

Epoch: 6| Step: 7
Training loss: 2.1811084747314453
Validation loss: 2.044047401797387

Epoch: 6| Step: 8
Training loss: 1.98689603805542
Validation loss: 2.055276829709289

Epoch: 6| Step: 9
Training loss: 1.7898280620574951
Validation loss: 2.0436148540948027

Epoch: 6| Step: 10
Training loss: 2.356693744659424
Validation loss: 2.0568550273936284

Epoch: 6| Step: 11
Training loss: 1.7439284324645996
Validation loss: 2.038687470138714

Epoch: 6| Step: 12
Training loss: 2.6469969749450684
Validation loss: 2.05055247840061

Epoch: 6| Step: 13
Training loss: 2.16035532951355
Validation loss: 2.055379371489248

Epoch: 62| Step: 0
Training loss: 1.634124517440796
Validation loss: 2.06290788291603

Epoch: 6| Step: 1
Training loss: 2.135718822479248
Validation loss: 2.0658883253733316

Epoch: 6| Step: 2
Training loss: 1.6121783256530762
Validation loss: 2.0640479954340125

Epoch: 6| Step: 3
Training loss: 1.4035545587539673
Validation loss: 2.053640604019165

Epoch: 6| Step: 4
Training loss: 2.1843128204345703
Validation loss: 2.0405220882866972

Epoch: 6| Step: 5
Training loss: 2.391204357147217
Validation loss: 2.0534936381924536

Epoch: 6| Step: 6
Training loss: 1.9072117805480957
Validation loss: 2.064078769376201

Epoch: 6| Step: 7
Training loss: 2.075779438018799
Validation loss: 2.0414385436683573

Epoch: 6| Step: 8
Training loss: 2.436006546020508
Validation loss: 2.05678919951121

Epoch: 6| Step: 9
Training loss: 2.4358932971954346
Validation loss: 2.0382424605790006

Epoch: 6| Step: 10
Training loss: 3.2895867824554443
Validation loss: 2.03995273446524

Epoch: 6| Step: 11
Training loss: 2.616580009460449
Validation loss: 2.046491062769326

Epoch: 6| Step: 12
Training loss: 2.8542795181274414
Validation loss: 2.0654466229100383

Epoch: 6| Step: 13
Training loss: 2.6053950786590576
Validation loss: 2.0447359828538794

Epoch: 63| Step: 0
Training loss: 2.1813180446624756
Validation loss: 2.0479428434884674

Epoch: 6| Step: 1
Training loss: 1.9090244770050049
Validation loss: 2.066425028667655

Epoch: 6| Step: 2
Training loss: 2.321282386779785
Validation loss: 2.0398625263603787

Epoch: 6| Step: 3
Training loss: 2.0206193923950195
Validation loss: 2.04959285900157

Epoch: 6| Step: 4
Training loss: 2.663454532623291
Validation loss: 2.0455418171421176

Epoch: 6| Step: 5
Training loss: 2.6518821716308594
Validation loss: 2.0616448643387004

Epoch: 6| Step: 6
Training loss: 2.070384979248047
Validation loss: 2.0371348550242763

Epoch: 6| Step: 7
Training loss: 2.5878469944000244
Validation loss: 2.0528384600916216

Epoch: 6| Step: 8
Training loss: 1.6728402376174927
Validation loss: 2.0520205600287325

Epoch: 6| Step: 9
Training loss: 1.918560266494751
Validation loss: 2.0561308168595835

Epoch: 6| Step: 10
Training loss: 1.568784236907959
Validation loss: 2.0448782969546575

Epoch: 6| Step: 11
Training loss: 2.3988852500915527
Validation loss: 2.0565186623604066

Epoch: 6| Step: 12
Training loss: 2.9063072204589844
Validation loss: 2.0403588356510287

Epoch: 6| Step: 13
Training loss: 2.3774843215942383
Validation loss: 2.0442312430309992

Epoch: 64| Step: 0
Training loss: 2.361989974975586
Validation loss: 2.049215321899742

Epoch: 6| Step: 1
Training loss: 2.240504741668701
Validation loss: 2.0418916235687914

Epoch: 6| Step: 2
Training loss: 2.4669785499572754
Validation loss: 2.062901287950495

Epoch: 6| Step: 3
Training loss: 1.9204273223876953
Validation loss: 2.0544397677144697

Epoch: 6| Step: 4
Training loss: 1.7453417778015137
Validation loss: 2.037153019699999

Epoch: 6| Step: 5
Training loss: 1.7596909999847412
Validation loss: 2.0578363146833194

Epoch: 6| Step: 6
Training loss: 1.7877618074417114
Validation loss: 2.057792536674007

Epoch: 6| Step: 7
Training loss: 1.7975062131881714
Validation loss: 2.026650804345326

Epoch: 6| Step: 8
Training loss: 2.1179778575897217
Validation loss: 2.036576806858022

Epoch: 6| Step: 9
Training loss: 2.2296793460845947
Validation loss: 2.053273231752457

Epoch: 6| Step: 10
Training loss: 2.4241702556610107
Validation loss: 2.075268937695411

Epoch: 6| Step: 11
Training loss: 2.9018001556396484
Validation loss: 2.0534961646603

Epoch: 6| Step: 12
Training loss: 2.765798568725586
Validation loss: 2.0477385918299356

Epoch: 6| Step: 13
Training loss: 3.1205108165740967
Validation loss: 2.0275378432325137

Epoch: 65| Step: 0
Training loss: 1.950131893157959
Validation loss: 2.0487900164819535

Epoch: 6| Step: 1
Training loss: 2.000683546066284
Validation loss: 2.048252537686338

Epoch: 6| Step: 2
Training loss: 2.6628546714782715
Validation loss: 2.054596277975267

Epoch: 6| Step: 3
Training loss: 2.798841953277588
Validation loss: 2.061098911428964

Epoch: 6| Step: 4
Training loss: 1.772191047668457
Validation loss: 2.067748978573789

Epoch: 6| Step: 5
Training loss: 1.33448326587677
Validation loss: 2.0536435432331537

Epoch: 6| Step: 6
Training loss: 1.9626882076263428
Validation loss: 2.040870387067077

Epoch: 6| Step: 7
Training loss: 2.8423173427581787
Validation loss: 2.0510271723552416

Epoch: 6| Step: 8
Training loss: 2.8019208908081055
Validation loss: 2.05051040521232

Epoch: 6| Step: 9
Training loss: 1.8211220502853394
Validation loss: 2.058701629279762

Epoch: 6| Step: 10
Training loss: 2.01511287689209
Validation loss: 2.0545225451069493

Epoch: 6| Step: 11
Training loss: 2.7157797813415527
Validation loss: 2.054101801687671

Epoch: 6| Step: 12
Training loss: 2.376817226409912
Validation loss: 2.059345793980424

Epoch: 6| Step: 13
Training loss: 1.6076664924621582
Validation loss: 2.067647813468851

Epoch: 66| Step: 0
Training loss: 2.39426326751709
Validation loss: 2.0557795955288793

Epoch: 6| Step: 1
Training loss: 2.726912021636963
Validation loss: 2.0739151905941706

Epoch: 6| Step: 2
Training loss: 1.7250185012817383
Validation loss: 2.064448277155558

Epoch: 6| Step: 3
Training loss: 1.9296550750732422
Validation loss: 2.0374282534404466

Epoch: 6| Step: 4
Training loss: 2.055816411972046
Validation loss: 2.0664526929137526

Epoch: 6| Step: 5
Training loss: 2.0139646530151367
Validation loss: 2.030736210525677

Epoch: 6| Step: 6
Training loss: 2.5795512199401855
Validation loss: 2.0556650520652853

Epoch: 6| Step: 7
Training loss: 2.0959808826446533
Validation loss: 2.061526467723231

Epoch: 6| Step: 8
Training loss: 2.678530216217041
Validation loss: 2.0300809260337584

Epoch: 6| Step: 9
Training loss: 1.8347535133361816
Validation loss: 2.0616560841119416

Epoch: 6| Step: 10
Training loss: 2.251749277114868
Validation loss: 2.0517328964766635

Epoch: 6| Step: 11
Training loss: 2.1303412914276123
Validation loss: 2.0355309388970815

Epoch: 6| Step: 12
Training loss: 2.385852098464966
Validation loss: 2.0634142788507606

Epoch: 6| Step: 13
Training loss: 2.209169387817383
Validation loss: 2.0507379834369948

Epoch: 67| Step: 0
Training loss: 1.6958856582641602
Validation loss: 2.0554514187638477

Epoch: 6| Step: 1
Training loss: 1.7616808414459229
Validation loss: 2.031909934936031

Epoch: 6| Step: 2
Training loss: 2.5898735523223877
Validation loss: 2.0674337904940367

Epoch: 6| Step: 3
Training loss: 2.276303768157959
Validation loss: 2.055021530838423

Epoch: 6| Step: 4
Training loss: 2.687352180480957
Validation loss: 2.0671040986173894

Epoch: 6| Step: 5
Training loss: 2.0730807781219482
Validation loss: 2.051844007225447

Epoch: 6| Step: 6
Training loss: 2.3916473388671875
Validation loss: 2.055629584097093

Epoch: 6| Step: 7
Training loss: 1.928551197052002
Validation loss: 2.0496358512550272

Epoch: 6| Step: 8
Training loss: 2.6409120559692383
Validation loss: 2.0507100551359114

Epoch: 6| Step: 9
Training loss: 2.144212245941162
Validation loss: 2.0488213134068314

Epoch: 6| Step: 10
Training loss: 1.9805908203125
Validation loss: 2.0428790559050856

Epoch: 6| Step: 11
Training loss: 3.226017713546753
Validation loss: 2.033732197617972

Epoch: 6| Step: 12
Training loss: 2.0024032592773438
Validation loss: 2.0493628645455964

Epoch: 6| Step: 13
Training loss: 1.1564021110534668
Validation loss: 2.0513887149031445

Epoch: 68| Step: 0
Training loss: 1.9726927280426025
Validation loss: 2.034404513656452

Epoch: 6| Step: 1
Training loss: 3.335397720336914
Validation loss: 2.067803426455426

Epoch: 6| Step: 2
Training loss: 1.9646978378295898
Validation loss: 2.0616810860172397

Epoch: 6| Step: 3
Training loss: 2.048640251159668
Validation loss: 2.067309448795934

Epoch: 6| Step: 4
Training loss: 2.877784252166748
Validation loss: 2.073716213626246

Epoch: 6| Step: 5
Training loss: 2.554548501968384
Validation loss: 2.0383276542027793

Epoch: 6| Step: 6
Training loss: 1.7958273887634277
Validation loss: 2.0471362349807576

Epoch: 6| Step: 7
Training loss: 1.7340737581253052
Validation loss: 2.043265328612379

Epoch: 6| Step: 8
Training loss: 2.5526528358459473
Validation loss: 2.047644281900057

Epoch: 6| Step: 9
Training loss: 2.26771879196167
Validation loss: 2.0636400035632554

Epoch: 6| Step: 10
Training loss: 1.9378767013549805
Validation loss: 2.053527089857286

Epoch: 6| Step: 11
Training loss: 2.3752598762512207
Validation loss: 2.0633563713360856

Epoch: 6| Step: 12
Training loss: 1.6811792850494385
Validation loss: 2.0512453535551667

Epoch: 6| Step: 13
Training loss: 1.6816469430923462
Validation loss: 2.0478737277369343

Epoch: 69| Step: 0
Training loss: 2.4658432006835938
Validation loss: 2.046309058384229

Epoch: 6| Step: 1
Training loss: 2.4948277473449707
Validation loss: 2.0582632428856305

Epoch: 6| Step: 2
Training loss: 2.6362853050231934
Validation loss: 2.0686496162927277

Epoch: 6| Step: 3
Training loss: 1.9782747030258179
Validation loss: 2.05665228443761

Epoch: 6| Step: 4
Training loss: 1.896806240081787
Validation loss: 2.0422711603103147

Epoch: 6| Step: 5
Training loss: 2.195111036300659
Validation loss: 2.047157931071456

Epoch: 6| Step: 6
Training loss: 2.653841495513916
Validation loss: 2.077501840488885

Epoch: 6| Step: 7
Training loss: 1.1805729866027832
Validation loss: 2.042353368574573

Epoch: 6| Step: 8
Training loss: 2.5109519958496094
Validation loss: 2.0520115847228677

Epoch: 6| Step: 9
Training loss: 2.01230525970459
Validation loss: 2.0634688715780936

Epoch: 6| Step: 10
Training loss: 2.685549736022949
Validation loss: 2.0516448328571935

Epoch: 6| Step: 11
Training loss: 2.4924349784851074
Validation loss: 2.035866896311442

Epoch: 6| Step: 12
Training loss: 1.597640037536621
Validation loss: 2.044219914303031

Epoch: 6| Step: 13
Training loss: 2.5281291007995605
Validation loss: 2.0610070126031035

Epoch: 70| Step: 0
Training loss: 2.458312511444092
Validation loss: 2.0538597247933827

Epoch: 6| Step: 1
Training loss: 2.7659554481506348
Validation loss: 2.0654848878101637

Epoch: 6| Step: 2
Training loss: 1.8391605615615845
Validation loss: 2.058932240291308

Epoch: 6| Step: 3
Training loss: 2.145617961883545
Validation loss: 2.0586522702247865

Epoch: 6| Step: 4
Training loss: 1.4576396942138672
Validation loss: 2.0401727461045787

Epoch: 6| Step: 5
Training loss: 2.733558177947998
Validation loss: 2.0387259785847

Epoch: 6| Step: 6
Training loss: 1.952735185623169
Validation loss: 2.0460069666626635

Epoch: 6| Step: 7
Training loss: 2.5821919441223145
Validation loss: 2.0481696949210217

Epoch: 6| Step: 8
Training loss: 2.3901584148406982
Validation loss: 2.037051644376529

Epoch: 6| Step: 9
Training loss: 1.7880513668060303
Validation loss: 2.0302303427009174

Epoch: 6| Step: 10
Training loss: 1.5459673404693604
Validation loss: 2.0379744011868715

Epoch: 6| Step: 11
Training loss: 2.0650243759155273
Validation loss: 2.036519729962913

Epoch: 6| Step: 12
Training loss: 2.938218116760254
Validation loss: 2.0419074591769966

Epoch: 6| Step: 13
Training loss: 2.604771614074707
Validation loss: 2.051153100946898

Epoch: 71| Step: 0
Training loss: 2.7240052223205566
Validation loss: 2.0356507711513068

Epoch: 6| Step: 1
Training loss: 1.6033775806427002
Validation loss: 2.0395231452039493

Epoch: 6| Step: 2
Training loss: 2.3741140365600586
Validation loss: 2.060801309923972

Epoch: 6| Step: 3
Training loss: 2.215139389038086
Validation loss: 2.0524985585161435

Epoch: 6| Step: 4
Training loss: 2.016777992248535
Validation loss: 2.045575830244249

Epoch: 6| Step: 5
Training loss: 2.603402614593506
Validation loss: 2.0301657479296447

Epoch: 6| Step: 6
Training loss: 2.0447278022766113
Validation loss: 2.055701555744294

Epoch: 6| Step: 7
Training loss: 2.9615402221679688
Validation loss: 2.043519402063021

Epoch: 6| Step: 8
Training loss: 1.5113284587860107
Validation loss: 2.0411828410240913

Epoch: 6| Step: 9
Training loss: 2.871626615524292
Validation loss: 2.0303812296159807

Epoch: 6| Step: 10
Training loss: 2.085855484008789
Validation loss: 2.036536260317731

Epoch: 6| Step: 11
Training loss: 2.292917490005493
Validation loss: 2.0308131633266324

Epoch: 6| Step: 12
Training loss: 1.5800445079803467
Validation loss: 2.047013659631052

Epoch: 6| Step: 13
Training loss: 2.2133517265319824
Validation loss: 2.0598159195274435

Epoch: 72| Step: 0
Training loss: 2.7112696170806885
Validation loss: 2.067349144207534

Epoch: 6| Step: 1
Training loss: 2.2991738319396973
Validation loss: 2.0449300837773148

Epoch: 6| Step: 2
Training loss: 3.16512131690979
Validation loss: 2.0611913204193115

Epoch: 6| Step: 3
Training loss: 1.7511260509490967
Validation loss: 2.0473368475514073

Epoch: 6| Step: 4
Training loss: 1.7400882244110107
Validation loss: 2.060036797677317

Epoch: 6| Step: 5
Training loss: 1.5822117328643799
Validation loss: 2.0628981295452324

Epoch: 6| Step: 6
Training loss: 1.863628625869751
Validation loss: 2.043805629976334

Epoch: 6| Step: 7
Training loss: 2.4474644660949707
Validation loss: 2.0368151664733887

Epoch: 6| Step: 8
Training loss: 2.0035319328308105
Validation loss: 2.039158037913743

Epoch: 6| Step: 9
Training loss: 1.9479396343231201
Validation loss: 2.066119868268249

Epoch: 6| Step: 10
Training loss: 2.8528013229370117
Validation loss: 2.0662651215830157

Epoch: 6| Step: 11
Training loss: 2.3069369792938232
Validation loss: 2.0518107260427167

Epoch: 6| Step: 12
Training loss: 2.227773904800415
Validation loss: 2.0618150695677726

Epoch: 6| Step: 13
Training loss: 1.7810554504394531
Validation loss: 2.0609012188449984

Epoch: 73| Step: 0
Training loss: 2.5636816024780273
Validation loss: 2.0508683035450597

Epoch: 6| Step: 1
Training loss: 2.098870038986206
Validation loss: 2.039327380477741

Epoch: 6| Step: 2
Training loss: 2.3443899154663086
Validation loss: 2.043730725524246

Epoch: 6| Step: 3
Training loss: 2.0146965980529785
Validation loss: 2.057067160965294

Epoch: 6| Step: 4
Training loss: 2.2187819480895996
Validation loss: 2.0324771763176046

Epoch: 6| Step: 5
Training loss: 1.9868934154510498
Validation loss: 2.0514564155250468

Epoch: 6| Step: 6
Training loss: 2.8010830879211426
Validation loss: 2.040175804527857

Epoch: 6| Step: 7
Training loss: 1.7896168231964111
Validation loss: 2.0434145440337477

Epoch: 6| Step: 8
Training loss: 1.8644365072250366
Validation loss: 2.050014042085217

Epoch: 6| Step: 9
Training loss: 2.225318670272827
Validation loss: 2.0614127805156093

Epoch: 6| Step: 10
Training loss: 2.239107370376587
Validation loss: 2.046198566754659

Epoch: 6| Step: 11
Training loss: 2.35628604888916
Validation loss: 2.0381771056882796

Epoch: 6| Step: 12
Training loss: 2.2571215629577637
Validation loss: 2.0363158602868356

Epoch: 6| Step: 13
Training loss: 2.060638427734375
Validation loss: 2.054351237512404

Epoch: 74| Step: 0
Training loss: 1.9410524368286133
Validation loss: 2.0658090294048352

Epoch: 6| Step: 1
Training loss: 1.9712879657745361
Validation loss: 2.062298059463501

Epoch: 6| Step: 2
Training loss: 1.9773263931274414
Validation loss: 2.054570912032999

Epoch: 6| Step: 3
Training loss: 2.437039852142334
Validation loss: 2.034211307443598

Epoch: 6| Step: 4
Training loss: 2.271707534790039
Validation loss: 2.0610171748745825

Epoch: 6| Step: 5
Training loss: 2.349858522415161
Validation loss: 2.053123576666719

Epoch: 6| Step: 6
Training loss: 1.7395106554031372
Validation loss: 2.03517323411921

Epoch: 6| Step: 7
Training loss: 1.6220567226409912
Validation loss: 2.0485680974939817

Epoch: 6| Step: 8
Training loss: 2.3836958408355713
Validation loss: 2.069643200084727

Epoch: 6| Step: 9
Training loss: 2.6790614128112793
Validation loss: 2.0423959198818413

Epoch: 6| Step: 10
Training loss: 3.127028226852417
Validation loss: 2.0361928452727613

Epoch: 6| Step: 11
Training loss: 2.2420969009399414
Validation loss: 2.0590031967368176

Epoch: 6| Step: 12
Training loss: 1.8133060932159424
Validation loss: 2.032779675658031

Epoch: 6| Step: 13
Training loss: 2.6860640048980713
Validation loss: 2.0553946059237242

Epoch: 75| Step: 0
Training loss: 2.9157004356384277
Validation loss: 2.0434808269623788

Epoch: 6| Step: 1
Training loss: 2.2059290409088135
Validation loss: 2.036054480460382

Epoch: 6| Step: 2
Training loss: 2.6556127071380615
Validation loss: 2.035000838259215

Epoch: 6| Step: 3
Training loss: 1.8821141719818115
Validation loss: 2.034919000441028

Epoch: 6| Step: 4
Training loss: 1.6298332214355469
Validation loss: 2.059383426943133

Epoch: 6| Step: 5
Training loss: 2.1887502670288086
Validation loss: 2.040645726265446

Epoch: 6| Step: 6
Training loss: 2.233541965484619
Validation loss: 2.052324900063135

Epoch: 6| Step: 7
Training loss: 1.7558658123016357
Validation loss: 2.0338177014422674

Epoch: 6| Step: 8
Training loss: 2.3131167888641357
Validation loss: 2.04624145133521

Epoch: 6| Step: 9
Training loss: 1.1276172399520874
Validation loss: 2.039853931755148

Epoch: 6| Step: 10
Training loss: 2.0693650245666504
Validation loss: 2.0347612083599134

Epoch: 6| Step: 11
Training loss: 2.5853867530822754
Validation loss: 2.0370076356395597

Epoch: 6| Step: 12
Training loss: 3.0175812244415283
Validation loss: 2.0481168070147113

Epoch: 6| Step: 13
Training loss: 2.611365556716919
Validation loss: 2.0369452763629217

Epoch: 76| Step: 0
Training loss: 2.1472113132476807
Validation loss: 2.058301910277336

Epoch: 6| Step: 1
Training loss: 2.462769031524658
Validation loss: 2.0380040996818134

Epoch: 6| Step: 2
Training loss: 2.2779343128204346
Validation loss: 2.040922418717415

Epoch: 6| Step: 3
Training loss: 2.484201192855835
Validation loss: 2.0330294870561167

Epoch: 6| Step: 4
Training loss: 2.6597094535827637
Validation loss: 2.0524044280411093

Epoch: 6| Step: 5
Training loss: 1.7354414463043213
Validation loss: 2.045117433353137

Epoch: 6| Step: 6
Training loss: 2.3198113441467285
Validation loss: 2.048912822559316

Epoch: 6| Step: 7
Training loss: 1.9979511499404907
Validation loss: 2.0371532183821484

Epoch: 6| Step: 8
Training loss: 1.5594561100006104
Validation loss: 2.0606346771281254

Epoch: 6| Step: 9
Training loss: 2.0076398849487305
Validation loss: 2.0404568551689066

Epoch: 6| Step: 10
Training loss: 2.6527645587921143
Validation loss: 2.0388938996099655

Epoch: 6| Step: 11
Training loss: 2.2831802368164062
Validation loss: 2.0242620693740023

Epoch: 6| Step: 12
Training loss: 2.5838727951049805
Validation loss: 2.047670707907728

Epoch: 6| Step: 13
Training loss: 1.7710118293762207
Validation loss: 2.039465190261923

Epoch: 77| Step: 0
Training loss: 1.6281882524490356
Validation loss: 2.056034034298312

Epoch: 6| Step: 1
Training loss: 2.256181240081787
Validation loss: 2.04154529494624

Epoch: 6| Step: 2
Training loss: 2.3603367805480957
Validation loss: 2.061294391591062

Epoch: 6| Step: 3
Training loss: 2.6902389526367188
Validation loss: 2.044334911531018

Epoch: 6| Step: 4
Training loss: 2.3635358810424805
Validation loss: 2.0497208077420472

Epoch: 6| Step: 5
Training loss: 1.8217697143554688
Validation loss: 2.0373860969338367

Epoch: 6| Step: 6
Training loss: 2.6243762969970703
Validation loss: 2.046207933015721

Epoch: 6| Step: 7
Training loss: 2.0368411540985107
Validation loss: 2.04311369311425

Epoch: 6| Step: 8
Training loss: 1.791690468788147
Validation loss: 2.0634138635409776

Epoch: 6| Step: 9
Training loss: 2.3666210174560547
Validation loss: 2.0627674569365797

Epoch: 6| Step: 10
Training loss: 2.365225315093994
Validation loss: 2.0591937495816137

Epoch: 6| Step: 11
Training loss: 2.5700631141662598
Validation loss: 2.053465276636103

Epoch: 6| Step: 12
Training loss: 1.4533390998840332
Validation loss: 2.046352932530065

Epoch: 6| Step: 13
Training loss: 2.7440683841705322
Validation loss: 2.055823115892308

Epoch: 78| Step: 0
Training loss: 2.3467845916748047
Validation loss: 2.062880030242346

Epoch: 6| Step: 1
Training loss: 1.6211638450622559
Validation loss: 2.0377455142236527

Epoch: 6| Step: 2
Training loss: 1.738878846168518
Validation loss: 2.0455870436083887

Epoch: 6| Step: 3
Training loss: 2.6292638778686523
Validation loss: 2.0541757191381147

Epoch: 6| Step: 4
Training loss: 2.0080080032348633
Validation loss: 2.046159598135179

Epoch: 6| Step: 5
Training loss: 2.291287422180176
Validation loss: 2.0468693228178125

Epoch: 6| Step: 6
Training loss: 1.7257513999938965
Validation loss: 2.0470359171590498

Epoch: 6| Step: 7
Training loss: 2.5544300079345703
Validation loss: 2.042730869785432

Epoch: 6| Step: 8
Training loss: 2.581789493560791
Validation loss: 2.053430172704881

Epoch: 6| Step: 9
Training loss: 2.0712413787841797
Validation loss: 2.0416301629876576

Epoch: 6| Step: 10
Training loss: 2.7286670207977295
Validation loss: 2.0242260809867614

Epoch: 6| Step: 11
Training loss: 2.1055877208709717
Validation loss: 2.029601909781015

Epoch: 6| Step: 12
Training loss: 2.869208335876465
Validation loss: 2.0483630985342045

Epoch: 6| Step: 13
Training loss: 1.5620888471603394
Validation loss: 2.0503837805922314

Epoch: 79| Step: 0
Training loss: 2.257845878601074
Validation loss: 2.0468376144286125

Epoch: 6| Step: 1
Training loss: 2.0009407997131348
Validation loss: 2.0488590476333455

Epoch: 6| Step: 2
Training loss: 1.6264578104019165
Validation loss: 2.0401175381034933

Epoch: 6| Step: 3
Training loss: 2.2698521614074707
Validation loss: 2.0184100904772357

Epoch: 6| Step: 4
Training loss: 1.6868951320648193
Validation loss: 2.045107385163666

Epoch: 6| Step: 5
Training loss: 1.8048478364944458
Validation loss: 2.034121481321191

Epoch: 6| Step: 6
Training loss: 2.647402286529541
Validation loss: 2.0276076383488153

Epoch: 6| Step: 7
Training loss: 2.1250219345092773
Validation loss: 2.0644945598417714

Epoch: 6| Step: 8
Training loss: 2.7529215812683105
Validation loss: 2.053244989405396

Epoch: 6| Step: 9
Training loss: 2.3910818099975586
Validation loss: 2.0367167534366732

Epoch: 6| Step: 10
Training loss: 1.8116607666015625
Validation loss: 2.0568055657930273

Epoch: 6| Step: 11
Training loss: 2.5943360328674316
Validation loss: 2.0634438478818504

Epoch: 6| Step: 12
Training loss: 2.622249126434326
Validation loss: 2.0321411727577128

Epoch: 6| Step: 13
Training loss: 2.0523805618286133
Validation loss: 2.0470080567944433

Epoch: 80| Step: 0
Training loss: 1.9036751985549927
Validation loss: 2.03877229075278

Epoch: 6| Step: 1
Training loss: 1.5939489603042603
Validation loss: 2.0480602377204487

Epoch: 6| Step: 2
Training loss: 1.6985397338867188
Validation loss: 2.0487779673709663

Epoch: 6| Step: 3
Training loss: 2.109766960144043
Validation loss: 2.0503831550639164

Epoch: 6| Step: 4
Training loss: 2.518934965133667
Validation loss: 2.077081254733506

Epoch: 6| Step: 5
Training loss: 2.0010929107666016
Validation loss: 2.067354361216227

Epoch: 6| Step: 6
Training loss: 1.9634332656860352
Validation loss: 2.042126911942677

Epoch: 6| Step: 7
Training loss: 2.1330738067626953
Validation loss: 2.04381428610894

Epoch: 6| Step: 8
Training loss: 2.9167206287384033
Validation loss: 2.0539943800177625

Epoch: 6| Step: 9
Training loss: 2.678405284881592
Validation loss: 2.034810737896991

Epoch: 6| Step: 10
Training loss: 2.2843475341796875
Validation loss: 2.0578875028958885

Epoch: 6| Step: 11
Training loss: 2.3528599739074707
Validation loss: 2.031062028741324

Epoch: 6| Step: 12
Training loss: 1.8990304470062256
Validation loss: 2.0411841612990185

Epoch: 6| Step: 13
Training loss: 3.2534420490264893
Validation loss: 2.038050256749635

Epoch: 81| Step: 0
Training loss: 1.3358879089355469
Validation loss: 2.039172295601137

Epoch: 6| Step: 1
Training loss: 1.9873427152633667
Validation loss: 2.046241832035844

Epoch: 6| Step: 2
Training loss: 2.403177499771118
Validation loss: 2.0395782224593626

Epoch: 6| Step: 3
Training loss: 2.629215955734253
Validation loss: 2.0510690622432257

Epoch: 6| Step: 4
Training loss: 2.4499077796936035
Validation loss: 2.045994585560214

Epoch: 6| Step: 5
Training loss: 2.3204147815704346
Validation loss: 2.0399840236991964

Epoch: 6| Step: 6
Training loss: 2.0683557987213135
Validation loss: 2.0169618411730696

Epoch: 6| Step: 7
Training loss: 1.73173189163208
Validation loss: 2.04177160673244

Epoch: 6| Step: 8
Training loss: 2.672487735748291
Validation loss: 2.06274890002384

Epoch: 6| Step: 9
Training loss: 2.21353816986084
Validation loss: 2.0429243503078336

Epoch: 6| Step: 10
Training loss: 1.6491445302963257
Validation loss: 2.058624985397503

Epoch: 6| Step: 11
Training loss: 2.979337453842163
Validation loss: 2.040184597815237

Epoch: 6| Step: 12
Training loss: 2.094451427459717
Validation loss: 2.0444556141412384

Epoch: 6| Step: 13
Training loss: 2.2895898818969727
Validation loss: 2.043837107637877

Epoch: 82| Step: 0
Training loss: 2.587637424468994
Validation loss: 2.039061818071591

Epoch: 6| Step: 1
Training loss: 2.1811118125915527
Validation loss: 2.0230517387390137

Epoch: 6| Step: 2
Training loss: 2.6548774242401123
Validation loss: 2.0365123184778358

Epoch: 6| Step: 3
Training loss: 2.5454349517822266
Validation loss: 2.0363922349868284

Epoch: 6| Step: 4
Training loss: 2.0063705444335938
Validation loss: 2.0448796441478114

Epoch: 6| Step: 5
Training loss: 2.0055992603302
Validation loss: 2.0409209471876903

Epoch: 6| Step: 6
Training loss: 1.7200521230697632
Validation loss: 2.0324807538781116

Epoch: 6| Step: 7
Training loss: 1.8097786903381348
Validation loss: 2.02768212492748

Epoch: 6| Step: 8
Training loss: 2.6368355751037598
Validation loss: 2.0463279729248374

Epoch: 6| Step: 9
Training loss: 2.7589774131774902
Validation loss: 2.036402151148806

Epoch: 6| Step: 10
Training loss: 1.9354510307312012
Validation loss: 2.0430777431816183

Epoch: 6| Step: 11
Training loss: 1.9055711030960083
Validation loss: 2.050615954142745

Epoch: 6| Step: 12
Training loss: 1.7787245512008667
Validation loss: 2.0458668085836593

Epoch: 6| Step: 13
Training loss: 2.5590476989746094
Validation loss: 2.041228404609106

Epoch: 83| Step: 0
Training loss: 1.7048592567443848
Validation loss: 2.034027909719816

Epoch: 6| Step: 1
Training loss: 2.249457836151123
Validation loss: 2.038524031639099

Epoch: 6| Step: 2
Training loss: 1.7448515892028809
Validation loss: 2.0543900407770628

Epoch: 6| Step: 3
Training loss: 2.211728096008301
Validation loss: 2.0225313094354447

Epoch: 6| Step: 4
Training loss: 2.2562975883483887
Validation loss: 2.0404415771525395

Epoch: 6| Step: 5
Training loss: 1.9412245750427246
Validation loss: 2.0524686254480833

Epoch: 6| Step: 6
Training loss: 2.449061870574951
Validation loss: 2.0473183534478627

Epoch: 6| Step: 7
Training loss: 1.9343433380126953
Validation loss: 2.034718827534747

Epoch: 6| Step: 8
Training loss: 2.102834463119507
Validation loss: 2.0097546782544864

Epoch: 6| Step: 9
Training loss: 2.396059989929199
Validation loss: 2.0195976534197406

Epoch: 6| Step: 10
Training loss: 2.2984330654144287
Validation loss: 2.038584683531074

Epoch: 6| Step: 11
Training loss: 2.1492857933044434
Validation loss: 2.026221993148968

Epoch: 6| Step: 12
Training loss: 2.992415428161621
Validation loss: 2.052764768241554

Epoch: 6| Step: 13
Training loss: 2.5329763889312744
Validation loss: 2.030111176993257

Epoch: 84| Step: 0
Training loss: 2.720426082611084
Validation loss: 2.0391919869248585

Epoch: 6| Step: 1
Training loss: 1.8199498653411865
Validation loss: 2.037033352800595

Epoch: 6| Step: 2
Training loss: 1.8657565116882324
Validation loss: 2.0225214253189745

Epoch: 6| Step: 3
Training loss: 2.0112414360046387
Validation loss: 2.016780650743874

Epoch: 6| Step: 4
Training loss: 2.1291043758392334
Validation loss: 2.0525699969260924

Epoch: 6| Step: 5
Training loss: 1.5898747444152832
Validation loss: 2.0506041049957275

Epoch: 6| Step: 6
Training loss: 1.4532588720321655
Validation loss: 2.027614270487139

Epoch: 6| Step: 7
Training loss: 2.473271369934082
Validation loss: 2.054195519416563

Epoch: 6| Step: 8
Training loss: 2.694464683532715
Validation loss: 2.0532711000852686

Epoch: 6| Step: 9
Training loss: 2.1301357746124268
Validation loss: 2.0361963343876663

Epoch: 6| Step: 10
Training loss: 2.4354517459869385
Validation loss: 2.0164254493610834

Epoch: 6| Step: 11
Training loss: 2.6769423484802246
Validation loss: 2.0375158453500397

Epoch: 6| Step: 12
Training loss: 2.724346876144409
Validation loss: 2.045202569295001

Epoch: 6| Step: 13
Training loss: 2.004643678665161
Validation loss: 2.0554527646751812

Epoch: 85| Step: 0
Training loss: 2.1504323482513428
Validation loss: 2.045954167201955

Epoch: 6| Step: 1
Training loss: 2.354030132293701
Validation loss: 2.0178074862367366

Epoch: 6| Step: 2
Training loss: 1.6833267211914062
Validation loss: 2.0430157351237472

Epoch: 6| Step: 3
Training loss: 2.0105843544006348
Validation loss: 2.0339200547946397

Epoch: 6| Step: 4
Training loss: 2.1354193687438965
Validation loss: 2.039882215120459

Epoch: 6| Step: 5
Training loss: 2.3203959465026855
Validation loss: 2.031877210063319

Epoch: 6| Step: 6
Training loss: 2.0497398376464844
Validation loss: 2.0105846466556674

Epoch: 6| Step: 7
Training loss: 2.0093231201171875
Validation loss: 2.04175151548078

Epoch: 6| Step: 8
Training loss: 2.1715505123138428
Validation loss: 2.04439289082763

Epoch: 6| Step: 9
Training loss: 3.0252397060394287
Validation loss: 2.0249383539281864

Epoch: 6| Step: 10
Training loss: 2.4312033653259277
Validation loss: 2.0333149343408565

Epoch: 6| Step: 11
Training loss: 1.5825600624084473
Validation loss: 2.0297911077417354

Epoch: 6| Step: 12
Training loss: 2.590989351272583
Validation loss: 2.062636133163206

Epoch: 6| Step: 13
Training loss: 2.062318801879883
Validation loss: 2.040149760502641

Epoch: 86| Step: 0
Training loss: 2.0400984287261963
Validation loss: 2.0343205877529678

Epoch: 6| Step: 1
Training loss: 2.2976436614990234
Validation loss: 2.0290590614400883

Epoch: 6| Step: 2
Training loss: 1.2928996086120605
Validation loss: 2.047893593388219

Epoch: 6| Step: 3
Training loss: 2.28070330619812
Validation loss: 2.025495357410882

Epoch: 6| Step: 4
Training loss: 2.4043500423431396
Validation loss: 2.053075974987399

Epoch: 6| Step: 5
Training loss: 2.2360925674438477
Validation loss: 2.0215645092789845

Epoch: 6| Step: 6
Training loss: 2.762732982635498
Validation loss: 2.034602003712808

Epoch: 6| Step: 7
Training loss: 2.1772336959838867
Validation loss: 2.0527698814228015

Epoch: 6| Step: 8
Training loss: 2.162318468093872
Validation loss: 2.048770417449295

Epoch: 6| Step: 9
Training loss: 2.2919962406158447
Validation loss: 2.047176750757361

Epoch: 6| Step: 10
Training loss: 2.6575427055358887
Validation loss: 2.033379139438752

Epoch: 6| Step: 11
Training loss: 1.6118474006652832
Validation loss: 2.03882307903741

Epoch: 6| Step: 12
Training loss: 2.3142385482788086
Validation loss: 2.025019681581887

Epoch: 6| Step: 13
Training loss: 1.834168791770935
Validation loss: 2.0381243677549463

Epoch: 87| Step: 0
Training loss: 2.499321460723877
Validation loss: 2.0403847822579007

Epoch: 6| Step: 1
Training loss: 1.48506498336792
Validation loss: 2.02300593673542

Epoch: 6| Step: 2
Training loss: 2.4607293605804443
Validation loss: 2.0311262402483212

Epoch: 6| Step: 3
Training loss: 2.2703981399536133
Validation loss: 2.0273121454382457

Epoch: 6| Step: 4
Training loss: 1.7041836977005005
Validation loss: 2.042875139944015

Epoch: 6| Step: 5
Training loss: 2.7978925704956055
Validation loss: 2.0441961006451677

Epoch: 6| Step: 6
Training loss: 1.907576084136963
Validation loss: 2.0574199640622703

Epoch: 6| Step: 7
Training loss: 1.780922770500183
Validation loss: 2.0080770215680523

Epoch: 6| Step: 8
Training loss: 1.3536067008972168
Validation loss: 2.036821730675236

Epoch: 6| Step: 9
Training loss: 1.7409958839416504
Validation loss: 2.0175710096154162

Epoch: 6| Step: 10
Training loss: 1.8236231803894043
Validation loss: 2.029973219799739

Epoch: 6| Step: 11
Training loss: 2.6147732734680176
Validation loss: 2.0372863277312248

Epoch: 6| Step: 12
Training loss: 3.0049822330474854
Validation loss: 2.0340339752935592

Epoch: 6| Step: 13
Training loss: 3.8008406162261963
Validation loss: 2.0350489513848418

Epoch: 88| Step: 0
Training loss: 2.50801420211792
Validation loss: 2.0497151497871644

Epoch: 6| Step: 1
Training loss: 2.465867519378662
Validation loss: 2.026584763680735

Epoch: 6| Step: 2
Training loss: 2.0167160034179688
Validation loss: 2.042896911662112

Epoch: 6| Step: 3
Training loss: 2.468517303466797
Validation loss: 2.0111350423546246

Epoch: 6| Step: 4
Training loss: 2.0255117416381836
Validation loss: 2.036923800745318

Epoch: 6| Step: 5
Training loss: 2.0873327255249023
Validation loss: 2.0043501495033182

Epoch: 6| Step: 6
Training loss: 1.8818742036819458
Validation loss: 2.034170522484728

Epoch: 6| Step: 7
Training loss: 2.1308767795562744
Validation loss: 2.0108284097845837

Epoch: 6| Step: 8
Training loss: 2.227604866027832
Validation loss: 2.0403677699386433

Epoch: 6| Step: 9
Training loss: 2.2222418785095215
Validation loss: 2.0411185795261013

Epoch: 6| Step: 10
Training loss: 1.647339940071106
Validation loss: 2.036754438954015

Epoch: 6| Step: 11
Training loss: 2.7394585609436035
Validation loss: 2.02030546562646

Epoch: 6| Step: 12
Training loss: 2.0584511756896973
Validation loss: 2.028951052696474

Epoch: 6| Step: 13
Training loss: 2.2725882530212402
Validation loss: 2.0414647299756288

Epoch: 89| Step: 0
Training loss: 2.6492843627929688
Validation loss: 2.0503995700549056

Epoch: 6| Step: 1
Training loss: 2.0727200508117676
Validation loss: 2.040313677121234

Epoch: 6| Step: 2
Training loss: 2.4268863201141357
Validation loss: 2.0284997647808445

Epoch: 6| Step: 3
Training loss: 1.6135427951812744
Validation loss: 2.0335572778537707

Epoch: 6| Step: 4
Training loss: 2.275819778442383
Validation loss: 2.0247120549601894

Epoch: 6| Step: 5
Training loss: 1.339423656463623
Validation loss: 2.0446336115560224

Epoch: 6| Step: 6
Training loss: 2.532650947570801
Validation loss: 2.0494957534215783

Epoch: 6| Step: 7
Training loss: 2.1709725856781006
Validation loss: 2.0250251498273624

Epoch: 6| Step: 8
Training loss: 2.1016592979431152
Validation loss: 2.0388430856889292

Epoch: 6| Step: 9
Training loss: 2.191678524017334
Validation loss: 2.0304942310497327

Epoch: 6| Step: 10
Training loss: 2.043511390686035
Validation loss: 2.0544394088047806

Epoch: 6| Step: 11
Training loss: 2.4073095321655273
Validation loss: 2.0386728702052945

Epoch: 6| Step: 12
Training loss: 2.7421770095825195
Validation loss: 2.0337843202775523

Epoch: 6| Step: 13
Training loss: 1.6771531105041504
Validation loss: 2.017377063792239

Epoch: 90| Step: 0
Training loss: 2.5928292274475098
Validation loss: 2.0201558323316675

Epoch: 6| Step: 1
Training loss: 1.2720410823822021
Validation loss: 2.043583877625004

Epoch: 6| Step: 2
Training loss: 2.3427276611328125
Validation loss: 2.0442808546045774

Epoch: 6| Step: 3
Training loss: 2.9361464977264404
Validation loss: 2.0206811325524443

Epoch: 6| Step: 4
Training loss: 2.3904480934143066
Validation loss: 2.015560893602269

Epoch: 6| Step: 5
Training loss: 1.9563134908676147
Validation loss: 2.032061279460948

Epoch: 6| Step: 6
Training loss: 1.9018175601959229
Validation loss: 2.022996402555896

Epoch: 6| Step: 7
Training loss: 1.8628370761871338
Validation loss: 2.0412652261795534

Epoch: 6| Step: 8
Training loss: 1.938233494758606
Validation loss: 2.0379299835492204

Epoch: 6| Step: 9
Training loss: 2.2530159950256348
Validation loss: 2.03334221916814

Epoch: 6| Step: 10
Training loss: 2.161383867263794
Validation loss: 2.059886988773141

Epoch: 6| Step: 11
Training loss: 2.5743746757507324
Validation loss: 2.0325232128943167

Epoch: 6| Step: 12
Training loss: 2.0539615154266357
Validation loss: 2.031413819200249

Epoch: 6| Step: 13
Training loss: 2.221226692199707
Validation loss: 2.0660922809313704

Epoch: 91| Step: 0
Training loss: 1.8508309125900269
Validation loss: 2.022340897590883

Epoch: 6| Step: 1
Training loss: 1.9996041059494019
Validation loss: 2.024426221847534

Epoch: 6| Step: 2
Training loss: 1.637547492980957
Validation loss: 2.0117381516323296

Epoch: 6| Step: 3
Training loss: 2.1067988872528076
Validation loss: 2.022526457745542

Epoch: 6| Step: 4
Training loss: 2.918153762817383
Validation loss: 2.035820109869844

Epoch: 6| Step: 5
Training loss: 1.7682722806930542
Validation loss: 2.0413555227300173

Epoch: 6| Step: 6
Training loss: 2.5817060470581055
Validation loss: 2.039668324173138

Epoch: 6| Step: 7
Training loss: 2.206361770629883
Validation loss: 2.02245113542003

Epoch: 6| Step: 8
Training loss: 2.0313472747802734
Validation loss: 2.0431895102224042

Epoch: 6| Step: 9
Training loss: 2.4703102111816406
Validation loss: 2.0345310831582673

Epoch: 6| Step: 10
Training loss: 2.2078654766082764
Validation loss: 2.024560033634145

Epoch: 6| Step: 11
Training loss: 2.522413492202759
Validation loss: 2.0208090787292807

Epoch: 6| Step: 12
Training loss: 2.2320549488067627
Validation loss: 2.0253387407589982

Epoch: 6| Step: 13
Training loss: 1.5179885625839233
Validation loss: 2.0353424126102078

Epoch: 92| Step: 0
Training loss: 2.3111743927001953
Validation loss: 2.029926061630249

Epoch: 6| Step: 1
Training loss: 1.554649829864502
Validation loss: 2.0235374422483545

Epoch: 6| Step: 2
Training loss: 2.516716480255127
Validation loss: 2.024837565678422

Epoch: 6| Step: 3
Training loss: 2.7501983642578125
Validation loss: 2.040244415242185

Epoch: 6| Step: 4
Training loss: 2.4971561431884766
Validation loss: 2.0206692603326615

Epoch: 6| Step: 5
Training loss: 1.3141236305236816
Validation loss: 2.032085264882734

Epoch: 6| Step: 6
Training loss: 2.6325788497924805
Validation loss: 2.040069253213944

Epoch: 6| Step: 7
Training loss: 1.6302012205123901
Validation loss: 2.032189764002318

Epoch: 6| Step: 8
Training loss: 1.7235050201416016
Validation loss: 2.027337884390226

Epoch: 6| Step: 9
Training loss: 2.03092098236084
Validation loss: 2.0323028769544376

Epoch: 6| Step: 10
Training loss: 2.9597697257995605
Validation loss: 2.0356860468464513

Epoch: 6| Step: 11
Training loss: 1.9964756965637207
Validation loss: 2.0171175874689573

Epoch: 6| Step: 12
Training loss: 2.3587565422058105
Validation loss: 2.0219091651260213

Epoch: 6| Step: 13
Training loss: 1.8437979221343994
Validation loss: 2.024639307811696

Epoch: 93| Step: 0
Training loss: 1.7214232683181763
Validation loss: 2.0289760533199517

Epoch: 6| Step: 1
Training loss: 2.168609619140625
Validation loss: 2.0585771965724167

Epoch: 6| Step: 2
Training loss: 3.1171183586120605
Validation loss: 2.034554445615379

Epoch: 6| Step: 3
Training loss: 2.4854576587677
Validation loss: 2.0366809368133545

Epoch: 6| Step: 4
Training loss: 2.082308769226074
Validation loss: 2.04329784711202

Epoch: 6| Step: 5
Training loss: 2.1493916511535645
Validation loss: 2.034246272938226

Epoch: 6| Step: 6
Training loss: 2.1027798652648926
Validation loss: 2.0144691326284923

Epoch: 6| Step: 7
Training loss: 2.1622185707092285
Validation loss: 2.0307768890934605

Epoch: 6| Step: 8
Training loss: 1.7165379524230957
Validation loss: 2.0401719667578257

Epoch: 6| Step: 9
Training loss: 2.271535873413086
Validation loss: 2.0507730591681694

Epoch: 6| Step: 10
Training loss: 2.5518078804016113
Validation loss: 2.0397578516314105

Epoch: 6| Step: 11
Training loss: 1.8885434865951538
Validation loss: 2.053696914385724

Epoch: 6| Step: 12
Training loss: 2.2225871086120605
Validation loss: 2.0286209429464033

Epoch: 6| Step: 13
Training loss: 1.9319220781326294
Validation loss: 2.0281359700746435

Epoch: 94| Step: 0
Training loss: 2.5701818466186523
Validation loss: 2.0401610341123355

Epoch: 6| Step: 1
Training loss: 2.6396665573120117
Validation loss: 2.0371632909262054

Epoch: 6| Step: 2
Training loss: 2.0996577739715576
Validation loss: 2.0175637173396286

Epoch: 6| Step: 3
Training loss: 1.858638048171997
Validation loss: 2.0211788262090375

Epoch: 6| Step: 4
Training loss: 2.5882644653320312
Validation loss: 2.0174764971579275

Epoch: 6| Step: 5
Training loss: 2.351616859436035
Validation loss: 2.002079084355344

Epoch: 6| Step: 6
Training loss: 2.0210487842559814
Validation loss: 2.0189468168443248

Epoch: 6| Step: 7
Training loss: 2.1580090522766113
Validation loss: 2.048515973552581

Epoch: 6| Step: 8
Training loss: 1.8813364505767822
Validation loss: 2.0435730693160847

Epoch: 6| Step: 9
Training loss: 2.1691818237304688
Validation loss: 2.0367514600035963

Epoch: 6| Step: 10
Training loss: 2.127809762954712
Validation loss: 2.0452564762484644

Epoch: 6| Step: 11
Training loss: 2.200458526611328
Validation loss: 2.0399914979934692

Epoch: 6| Step: 12
Training loss: 1.6627964973449707
Validation loss: 2.022972409443189

Epoch: 6| Step: 13
Training loss: 2.0557286739349365
Validation loss: 2.039463658486643

Epoch: 95| Step: 0
Training loss: 2.534985065460205
Validation loss: 2.032207614632063

Epoch: 6| Step: 1
Training loss: 2.0569562911987305
Validation loss: 2.030977805455526

Epoch: 6| Step: 2
Training loss: 1.864736795425415
Validation loss: 2.042834988204382

Epoch: 6| Step: 3
Training loss: 1.9776009321212769
Validation loss: 2.024672355703128

Epoch: 6| Step: 4
Training loss: 2.37807035446167
Validation loss: 2.0083121971417497

Epoch: 6| Step: 5
Training loss: 2.0851759910583496
Validation loss: 2.039228467531102

Epoch: 6| Step: 6
Training loss: 1.9319722652435303
Validation loss: 2.0450514747250463

Epoch: 6| Step: 7
Training loss: 2.1516714096069336
Validation loss: 2.010806927116968

Epoch: 6| Step: 8
Training loss: 2.3530452251434326
Validation loss: 2.0475172791429745

Epoch: 6| Step: 9
Training loss: 1.6271424293518066
Validation loss: 2.0301070315863496

Epoch: 6| Step: 10
Training loss: 1.6289907693862915
Validation loss: 2.0351273577700377

Epoch: 6| Step: 11
Training loss: 2.481825590133667
Validation loss: 2.036583746633222

Epoch: 6| Step: 12
Training loss: 3.1397154331207275
Validation loss: 2.0472891253809773

Epoch: 6| Step: 13
Training loss: 1.989861011505127
Validation loss: 2.044284451392389

Epoch: 96| Step: 0
Training loss: 2.650479555130005
Validation loss: 2.043844969041886

Epoch: 6| Step: 1
Training loss: 1.6514229774475098
Validation loss: 2.019127797054988

Epoch: 6| Step: 2
Training loss: 2.8714776039123535
Validation loss: 2.0180318406833115

Epoch: 6| Step: 3
Training loss: 1.5810116529464722
Validation loss: 2.026683831727633

Epoch: 6| Step: 4
Training loss: 2.2898459434509277
Validation loss: 2.019476702136378

Epoch: 6| Step: 5
Training loss: 2.7237493991851807
Validation loss: 2.0403987079538326

Epoch: 6| Step: 6
Training loss: 2.248563289642334
Validation loss: 2.023106418630128

Epoch: 6| Step: 7
Training loss: 2.2106704711914062
Validation loss: 2.0251453333003546

Epoch: 6| Step: 8
Training loss: 2.253976821899414
Validation loss: 2.0446324374086116

Epoch: 6| Step: 9
Training loss: 2.0684292316436768
Validation loss: 2.031384744951802

Epoch: 6| Step: 10
Training loss: 1.5201395750045776
Validation loss: 2.0399893329989527

Epoch: 6| Step: 11
Training loss: 1.739063024520874
Validation loss: 2.0194578888595744

Epoch: 6| Step: 12
Training loss: 2.126194477081299
Validation loss: 2.025819518232858

Epoch: 6| Step: 13
Training loss: 2.556002616882324
Validation loss: 1.9985455787310036

Epoch: 97| Step: 0
Training loss: 1.4023857116699219
Validation loss: 2.048847467668595

Epoch: 6| Step: 1
Training loss: 1.3799598217010498
Validation loss: 2.0390573137549945

Epoch: 6| Step: 2
Training loss: 2.683363437652588
Validation loss: 2.0094036004876576

Epoch: 6| Step: 3
Training loss: 1.4059717655181885
Validation loss: 2.025812695103307

Epoch: 6| Step: 4
Training loss: 2.4922122955322266
Validation loss: 2.036203574108821

Epoch: 6| Step: 5
Training loss: 1.8162819147109985
Validation loss: 2.011149699969958

Epoch: 6| Step: 6
Training loss: 2.276710271835327
Validation loss: 2.0312184377383162

Epoch: 6| Step: 7
Training loss: 3.0649003982543945
Validation loss: 2.037070455089692

Epoch: 6| Step: 8
Training loss: 2.5448665618896484
Validation loss: 2.030307541611374

Epoch: 6| Step: 9
Training loss: 2.80061674118042
Validation loss: 2.0367636783148653

Epoch: 6| Step: 10
Training loss: 1.8429465293884277
Validation loss: 2.0050452652797905

Epoch: 6| Step: 11
Training loss: 2.16646671295166
Validation loss: 2.028402760464658

Epoch: 6| Step: 12
Training loss: 2.3474178314208984
Validation loss: 2.0504950528503745

Epoch: 6| Step: 13
Training loss: 2.43520188331604
Validation loss: 2.002338106914233

Epoch: 98| Step: 0
Training loss: 2.3000147342681885
Validation loss: 2.0148407592568347

Epoch: 6| Step: 1
Training loss: 1.4664230346679688
Validation loss: 2.019247915155144

Epoch: 6| Step: 2
Training loss: 1.9908149242401123
Validation loss: 2.0277677992338776

Epoch: 6| Step: 3
Training loss: 1.4441754817962646
Validation loss: 2.032130182430308

Epoch: 6| Step: 4
Training loss: 2.5724408626556396
Validation loss: 2.035122871398926

Epoch: 6| Step: 5
Training loss: 2.8946242332458496
Validation loss: 2.043404645817254

Epoch: 6| Step: 6
Training loss: 3.2162952423095703
Validation loss: 2.028710070476737

Epoch: 6| Step: 7
Training loss: 2.3704726696014404
Validation loss: 2.0442772398712816

Epoch: 6| Step: 8
Training loss: 2.2977874279022217
Validation loss: 2.023764285989987

Epoch: 6| Step: 9
Training loss: 1.2908051013946533
Validation loss: 2.0073926307821788

Epoch: 6| Step: 10
Training loss: 1.7810635566711426
Validation loss: 2.0432826062684417

Epoch: 6| Step: 11
Training loss: 1.9308433532714844
Validation loss: 2.0321370299144457

Epoch: 6| Step: 12
Training loss: 2.519988775253296
Validation loss: 2.033699567599963

Epoch: 6| Step: 13
Training loss: 2.3065710067749023
Validation loss: 2.0176928069001887

Epoch: 99| Step: 0
Training loss: 1.892106056213379
Validation loss: 2.012778628257013

Epoch: 6| Step: 1
Training loss: 2.070828914642334
Validation loss: 2.026416455545733

Epoch: 6| Step: 2
Training loss: 2.9067869186401367
Validation loss: 2.020571075459962

Epoch: 6| Step: 3
Training loss: 1.8829771280288696
Validation loss: 2.049128053008869

Epoch: 6| Step: 4
Training loss: 2.1091222763061523
Validation loss: 2.0264255949245986

Epoch: 6| Step: 5
Training loss: 1.6421871185302734
Validation loss: 2.0221184684384252

Epoch: 6| Step: 6
Training loss: 3.0857291221618652
Validation loss: 2.0013485159925235

Epoch: 6| Step: 7
Training loss: 2.8153836727142334
Validation loss: 2.0286012464954006

Epoch: 6| Step: 8
Training loss: 2.4670042991638184
Validation loss: 2.030118203932239

Epoch: 6| Step: 9
Training loss: 2.0306947231292725
Validation loss: 2.0354195064113987

Epoch: 6| Step: 10
Training loss: 1.2289173603057861
Validation loss: 2.0340313937074397

Epoch: 6| Step: 11
Training loss: 2.620849132537842
Validation loss: 2.0180387189311366

Epoch: 6| Step: 12
Training loss: 1.5852100849151611
Validation loss: 2.0422403735499226

Epoch: 6| Step: 13
Training loss: 1.8552995920181274
Validation loss: 2.0370673389844995

Epoch: 100| Step: 0
Training loss: 2.2314867973327637
Validation loss: 2.0263729915823987

Epoch: 6| Step: 1
Training loss: 2.211578845977783
Validation loss: 2.0269539099867626

Epoch: 6| Step: 2
Training loss: 1.5350472927093506
Validation loss: 2.0350873547215618

Epoch: 6| Step: 3
Training loss: 2.3884778022766113
Validation loss: 2.024692563600438

Epoch: 6| Step: 4
Training loss: 2.193767547607422
Validation loss: 2.0378296529093096

Epoch: 6| Step: 5
Training loss: 2.0767569541931152
Validation loss: 2.0142630838578746

Epoch: 6| Step: 6
Training loss: 1.717020034790039
Validation loss: 2.0309251636587162

Epoch: 6| Step: 7
Training loss: 2.383915662765503
Validation loss: 2.0135888886708084

Epoch: 6| Step: 8
Training loss: 3.2533366680145264
Validation loss: 2.0259856254823747

Epoch: 6| Step: 9
Training loss: 1.9886846542358398
Validation loss: 2.0475134029183337

Epoch: 6| Step: 10
Training loss: 2.1985721588134766
Validation loss: 2.03541535587721

Epoch: 6| Step: 11
Training loss: 2.1714441776275635
Validation loss: 2.0153044603204213

Epoch: 6| Step: 12
Training loss: 1.9530055522918701
Validation loss: 2.0403613595552343

Epoch: 6| Step: 13
Training loss: 1.8783068656921387
Validation loss: 2.011449611315163

Epoch: 101| Step: 0
Training loss: 2.492661476135254
Validation loss: 2.020086678125525

Epoch: 6| Step: 1
Training loss: 2.2048861980438232
Validation loss: 2.026188314601939

Epoch: 6| Step: 2
Training loss: 2.0835933685302734
Validation loss: 2.02905693105472

Epoch: 6| Step: 3
Training loss: 2.194542169570923
Validation loss: 2.003737288136636

Epoch: 6| Step: 4
Training loss: 1.9094222784042358
Validation loss: 2.017561161389915

Epoch: 6| Step: 5
Training loss: 1.794710636138916
Validation loss: 2.0394905997860815

Epoch: 6| Step: 6
Training loss: 1.7787489891052246
Validation loss: 2.022884620133267

Epoch: 6| Step: 7
Training loss: 1.6090065240859985
Validation loss: 2.031278285928952

Epoch: 6| Step: 8
Training loss: 2.387038230895996
Validation loss: 2.0442864664139284

Epoch: 6| Step: 9
Training loss: 2.6182188987731934
Validation loss: 1.9960618916378225

Epoch: 6| Step: 10
Training loss: 1.7571394443511963
Validation loss: 2.02828957316696

Epoch: 6| Step: 11
Training loss: 1.853980541229248
Validation loss: 2.021597498206682

Epoch: 6| Step: 12
Training loss: 3.169562339782715
Validation loss: 2.0153555331691617

Epoch: 6| Step: 13
Training loss: 2.649919271469116
Validation loss: 2.0252207709896948

Epoch: 102| Step: 0
Training loss: 2.8064370155334473
Validation loss: 2.0252057557464926

Epoch: 6| Step: 1
Training loss: 2.130464792251587
Validation loss: 2.0310587600995134

Epoch: 6| Step: 2
Training loss: 2.3256208896636963
Validation loss: 2.0204660969395793

Epoch: 6| Step: 3
Training loss: 2.182241678237915
Validation loss: 2.0305491339775825

Epoch: 6| Step: 4
Training loss: 2.3001885414123535
Validation loss: 2.0045957796035276

Epoch: 6| Step: 5
Training loss: 2.645432472229004
Validation loss: 2.010863273374496

Epoch: 6| Step: 6
Training loss: 2.171535015106201
Validation loss: 2.026653532058962

Epoch: 6| Step: 7
Training loss: 1.8479652404785156
Validation loss: 2.0231861939994236

Epoch: 6| Step: 8
Training loss: 2.3356714248657227
Validation loss: 2.008706597871678

Epoch: 6| Step: 9
Training loss: 1.5546553134918213
Validation loss: 2.0404662739846016

Epoch: 6| Step: 10
Training loss: 1.9222877025604248
Validation loss: 2.0520858764648438

Epoch: 6| Step: 11
Training loss: 2.298231363296509
Validation loss: 2.017306081710323

Epoch: 6| Step: 12
Training loss: 1.473933219909668
Validation loss: 2.0405109300408313

Epoch: 6| Step: 13
Training loss: 1.8609150648117065
Validation loss: 2.0011765533877957

Epoch: 103| Step: 0
Training loss: 2.353010654449463
Validation loss: 2.0081892987733245

Epoch: 6| Step: 1
Training loss: 1.915455937385559
Validation loss: 2.007843391869658

Epoch: 6| Step: 2
Training loss: 2.2317702770233154
Validation loss: 2.027212251899063

Epoch: 6| Step: 3
Training loss: 1.9605622291564941
Validation loss: 2.0175319358866703

Epoch: 6| Step: 4
Training loss: 2.4823060035705566
Validation loss: 2.053676120696529

Epoch: 6| Step: 5
Training loss: 2.6040148735046387
Validation loss: 2.0261687988876016

Epoch: 6| Step: 6
Training loss: 2.1822762489318848
Validation loss: 2.034610890573071

Epoch: 6| Step: 7
Training loss: 2.0313522815704346
Validation loss: 2.0294229010100007

Epoch: 6| Step: 8
Training loss: 1.8286895751953125
Validation loss: 2.019266895068589

Epoch: 6| Step: 9
Training loss: 2.1974518299102783
Validation loss: 2.01183831819924

Epoch: 6| Step: 10
Training loss: 2.598921298980713
Validation loss: 2.036073443710163

Epoch: 6| Step: 11
Training loss: 1.4786128997802734
Validation loss: 2.0189111104575534

Epoch: 6| Step: 12
Training loss: 2.448610305786133
Validation loss: 2.0263662261347615

Epoch: 6| Step: 13
Training loss: 1.7628921270370483
Validation loss: 2.0114265616222093

Epoch: 104| Step: 0
Training loss: 2.7458672523498535
Validation loss: 2.018414379448019

Epoch: 6| Step: 1
Training loss: 1.707821249961853
Validation loss: 2.0181371050496257

Epoch: 6| Step: 2
Training loss: 2.6695120334625244
Validation loss: 2.026105350063693

Epoch: 6| Step: 3
Training loss: 2.775207757949829
Validation loss: 2.019908737110835

Epoch: 6| Step: 4
Training loss: 2.7031216621398926
Validation loss: 2.0265439812855055

Epoch: 6| Step: 5
Training loss: 1.4534995555877686
Validation loss: 2.0312899440847416

Epoch: 6| Step: 6
Training loss: 1.9607034921646118
Validation loss: 2.0256283719052552

Epoch: 6| Step: 7
Training loss: 1.6989285945892334
Validation loss: 2.0281790789737495

Epoch: 6| Step: 8
Training loss: 1.8934956789016724
Validation loss: 2.0235944665888304

Epoch: 6| Step: 9
Training loss: 2.4822046756744385
Validation loss: 2.039217657940362

Epoch: 6| Step: 10
Training loss: 1.6497348546981812
Validation loss: 2.0220763529500654

Epoch: 6| Step: 11
Training loss: 2.5937116146087646
Validation loss: 2.028654399738517

Epoch: 6| Step: 12
Training loss: 1.8339548110961914
Validation loss: 2.0473665896282403

Epoch: 6| Step: 13
Training loss: 1.928346872329712
Validation loss: 2.0178341352811424

Epoch: 105| Step: 0
Training loss: 1.8427419662475586
Validation loss: 2.0077614104875954

Epoch: 6| Step: 1
Training loss: 1.5662461519241333
Validation loss: 2.0347202875280894

Epoch: 6| Step: 2
Training loss: 2.6792337894439697
Validation loss: 2.0143782759225495

Epoch: 6| Step: 3
Training loss: 1.9756665229797363
Validation loss: 2.0201774463858655

Epoch: 6| Step: 4
Training loss: 1.3674890995025635
Validation loss: 2.023055317581341

Epoch: 6| Step: 5
Training loss: 2.8306264877319336
Validation loss: 2.037174088980562

Epoch: 6| Step: 6
Training loss: 1.8468332290649414
Validation loss: 2.0147549260047173

Epoch: 6| Step: 7
Training loss: 2.309631109237671
Validation loss: 2.0288669729745514

Epoch: 6| Step: 8
Training loss: 2.4473886489868164
Validation loss: 2.0503366557500695

Epoch: 6| Step: 9
Training loss: 2.6475396156311035
Validation loss: 2.02939913862495

Epoch: 6| Step: 10
Training loss: 2.30137038230896
Validation loss: 2.019883212222848

Epoch: 6| Step: 11
Training loss: 1.803763508796692
Validation loss: 2.0282908613963793

Epoch: 6| Step: 12
Training loss: 2.448234796524048
Validation loss: 2.016370182396263

Epoch: 6| Step: 13
Training loss: 2.0123207569122314
Validation loss: 2.029729127883911

Epoch: 106| Step: 0
Training loss: 1.6872984170913696
Validation loss: 2.055435698519471

Epoch: 6| Step: 1
Training loss: 2.1961708068847656
Validation loss: 2.00524317320957

Epoch: 6| Step: 2
Training loss: 1.7401670217514038
Validation loss: 2.014691017007315

Epoch: 6| Step: 3
Training loss: 2.217327356338501
Validation loss: 2.0292056068297355

Epoch: 6| Step: 4
Training loss: 2.51668643951416
Validation loss: 2.0266076992916804

Epoch: 6| Step: 5
Training loss: 2.1192617416381836
Validation loss: 2.0120999633624987

Epoch: 6| Step: 6
Training loss: 2.599039077758789
Validation loss: 2.0100433403445828

Epoch: 6| Step: 7
Training loss: 1.6958280801773071
Validation loss: 2.048488143951662

Epoch: 6| Step: 8
Training loss: 2.065467119216919
Validation loss: 2.0106785720394504

Epoch: 6| Step: 9
Training loss: 2.2838501930236816
Validation loss: 2.0306332662541378

Epoch: 6| Step: 10
Training loss: 1.9117188453674316
Validation loss: 2.039854536774338

Epoch: 6| Step: 11
Training loss: 2.4163570404052734
Validation loss: 2.0356208996106218

Epoch: 6| Step: 12
Training loss: 2.2583091259002686
Validation loss: 2.0131828169668875

Epoch: 6| Step: 13
Training loss: 2.4292163848876953
Validation loss: 2.048284048675209

Epoch: 107| Step: 0
Training loss: 2.366410255432129
Validation loss: 2.030365140207352

Epoch: 6| Step: 1
Training loss: 2.2449684143066406
Validation loss: 2.046056483381538

Epoch: 6| Step: 2
Training loss: 2.6881179809570312
Validation loss: 2.026324696438287

Epoch: 6| Step: 3
Training loss: 1.7065293788909912
Validation loss: 2.0259912283189836

Epoch: 6| Step: 4
Training loss: 1.7490944862365723
Validation loss: 2.02669906744393

Epoch: 6| Step: 5
Training loss: 2.6532578468322754
Validation loss: 2.033509762056412

Epoch: 6| Step: 6
Training loss: 1.6722815036773682
Validation loss: 2.0204766552935363

Epoch: 6| Step: 7
Training loss: 2.2730565071105957
Validation loss: 2.0247042102198445

Epoch: 6| Step: 8
Training loss: 1.6878983974456787
Validation loss: 2.041077998376662

Epoch: 6| Step: 9
Training loss: 1.6997787952423096
Validation loss: 2.0424639845407135

Epoch: 6| Step: 10
Training loss: 2.7750535011291504
Validation loss: 2.00412626804844

Epoch: 6| Step: 11
Training loss: 2.281846523284912
Validation loss: 2.0156176218422512

Epoch: 6| Step: 12
Training loss: 1.7560501098632812
Validation loss: 2.0466288840898903

Epoch: 6| Step: 13
Training loss: 2.2180540561676025
Validation loss: 2.0335508905431277

Epoch: 108| Step: 0
Training loss: 2.3152923583984375
Validation loss: 2.0235796474641368

Epoch: 6| Step: 1
Training loss: 2.0657272338867188
Validation loss: 2.006407017348915

Epoch: 6| Step: 2
Training loss: 2.290252685546875
Validation loss: 2.029055127533533

Epoch: 6| Step: 3
Training loss: 2.457489013671875
Validation loss: 2.042897424390239

Epoch: 6| Step: 4
Training loss: 2.223628282546997
Validation loss: 2.001660285457488

Epoch: 6| Step: 5
Training loss: 2.0297651290893555
Validation loss: 2.0245125716732395

Epoch: 6| Step: 6
Training loss: 1.9333689212799072
Validation loss: 2.0111912796574254

Epoch: 6| Step: 7
Training loss: 2.177755832672119
Validation loss: 2.0501494946018344

Epoch: 6| Step: 8
Training loss: 2.354269027709961
Validation loss: 2.0230459423475367

Epoch: 6| Step: 9
Training loss: 2.2354531288146973
Validation loss: 2.0320805208657378

Epoch: 6| Step: 10
Training loss: 1.8868682384490967
Validation loss: 2.0244590966932234

Epoch: 6| Step: 11
Training loss: 1.9470146894454956
Validation loss: 2.0209968987331597

Epoch: 6| Step: 12
Training loss: 1.7675267457962036
Validation loss: 1.9992224631770965

Epoch: 6| Step: 13
Training loss: 1.8526102304458618
Validation loss: 2.032065745322935

Epoch: 109| Step: 0
Training loss: 1.9141407012939453
Validation loss: 2.041744742342221

Epoch: 6| Step: 1
Training loss: 2.3161916732788086
Validation loss: 1.9985080970230924

Epoch: 6| Step: 2
Training loss: 2.13210129737854
Validation loss: 2.038333249348466

Epoch: 6| Step: 3
Training loss: 1.9815242290496826
Validation loss: 2.03470044238593

Epoch: 6| Step: 4
Training loss: 1.2767413854599
Validation loss: 2.0319190614966938

Epoch: 6| Step: 5
Training loss: 1.8365777730941772
Validation loss: 2.004447662702171

Epoch: 6| Step: 6
Training loss: 2.3292150497436523
Validation loss: 2.0313852986981793

Epoch: 6| Step: 7
Training loss: 2.083024740219116
Validation loss: 2.0339394205360004

Epoch: 6| Step: 8
Training loss: 2.586172580718994
Validation loss: 2.0293577819742183

Epoch: 6| Step: 9
Training loss: 1.5052216053009033
Validation loss: 2.036437201243575

Epoch: 6| Step: 10
Training loss: 2.4192135334014893
Validation loss: 2.0177899227347424

Epoch: 6| Step: 11
Training loss: 2.273662805557251
Validation loss: 2.0197011065739456

Epoch: 6| Step: 12
Training loss: 2.3193466663360596
Validation loss: 2.022796403977179

Epoch: 6| Step: 13
Training loss: 3.343280076980591
Validation loss: 2.0296734276638237

Epoch: 110| Step: 0
Training loss: 2.7512450218200684
Validation loss: 2.019359693732313

Epoch: 6| Step: 1
Training loss: 1.9121041297912598
Validation loss: 2.0233264405240297

Epoch: 6| Step: 2
Training loss: 2.147615909576416
Validation loss: 2.038650212749358

Epoch: 6| Step: 3
Training loss: 1.4872617721557617
Validation loss: 2.0274879868312548

Epoch: 6| Step: 4
Training loss: 1.9519755840301514
Validation loss: 2.021603648380567

Epoch: 6| Step: 5
Training loss: 2.101637601852417
Validation loss: 2.036396441921111

Epoch: 6| Step: 6
Training loss: 1.9962518215179443
Validation loss: 2.041226035805159

Epoch: 6| Step: 7
Training loss: 2.6814050674438477
Validation loss: 2.0427681553748345

Epoch: 6| Step: 8
Training loss: 1.9732826948165894
Validation loss: 2.0184260645220355

Epoch: 6| Step: 9
Training loss: 2.019864082336426
Validation loss: 2.023599147796631

Epoch: 6| Step: 10
Training loss: 2.014397382736206
Validation loss: 2.012539848204582

Epoch: 6| Step: 11
Training loss: 2.9888076782226562
Validation loss: 2.0270739293867543

Epoch: 6| Step: 12
Training loss: 1.7902088165283203
Validation loss: 2.024099524303149

Epoch: 6| Step: 13
Training loss: 1.9830061197280884
Validation loss: 2.0259897990893294

Epoch: 111| Step: 0
Training loss: 2.6403257846832275
Validation loss: 2.02612405438577

Epoch: 6| Step: 1
Training loss: 1.8114376068115234
Validation loss: 2.0199743317019556

Epoch: 6| Step: 2
Training loss: 1.45166015625
Validation loss: 2.0175388141344954

Epoch: 6| Step: 3
Training loss: 2.3971543312072754
Validation loss: 1.996797074553787

Epoch: 6| Step: 4
Training loss: 2.3419599533081055
Validation loss: 2.0568815072377524

Epoch: 6| Step: 5
Training loss: 1.8387532234191895
Validation loss: 2.020584052608859

Epoch: 6| Step: 6
Training loss: 2.4452643394470215
Validation loss: 2.020618930939705

Epoch: 6| Step: 7
Training loss: 2.4687395095825195
Validation loss: 1.9950070996438303

Epoch: 6| Step: 8
Training loss: 2.4427053928375244
Validation loss: 2.0024202446783743

Epoch: 6| Step: 9
Training loss: 2.0422277450561523
Validation loss: 2.0263652468240387

Epoch: 6| Step: 10
Training loss: 1.4550132751464844
Validation loss: 2.0215805525420816

Epoch: 6| Step: 11
Training loss: 2.111663818359375
Validation loss: 2.0083987277041198

Epoch: 6| Step: 12
Training loss: 2.696890354156494
Validation loss: 2.018692986939543

Epoch: 6| Step: 13
Training loss: 1.1757607460021973
Validation loss: 2.0104596999383744

Epoch: 112| Step: 0
Training loss: 1.9123806953430176
Validation loss: 2.0449290788301857

Epoch: 6| Step: 1
Training loss: 2.1905975341796875
Validation loss: 2.0274559554233345

Epoch: 6| Step: 2
Training loss: 1.715867042541504
Validation loss: 2.0289276594756753

Epoch: 6| Step: 3
Training loss: 1.950615406036377
Validation loss: 2.0181182802364392

Epoch: 6| Step: 4
Training loss: 1.8040766716003418
Validation loss: 2.0440687402602165

Epoch: 6| Step: 5
Training loss: 2.2099599838256836
Validation loss: 1.9886880997688539

Epoch: 6| Step: 6
Training loss: 2.4530014991760254
Validation loss: 2.0246898640868483

Epoch: 6| Step: 7
Training loss: 2.365352153778076
Validation loss: 2.014452231827603

Epoch: 6| Step: 8
Training loss: 1.7579336166381836
Validation loss: 2.0216599792562504

Epoch: 6| Step: 9
Training loss: 1.7472641468048096
Validation loss: 2.0152922650819183

Epoch: 6| Step: 10
Training loss: 1.7141857147216797
Validation loss: 2.0088587781434417

Epoch: 6| Step: 11
Training loss: 2.5680603981018066
Validation loss: 2.0323869912855086

Epoch: 6| Step: 12
Training loss: 2.7894127368927
Validation loss: 2.0152515980505172

Epoch: 6| Step: 13
Training loss: 3.2731988430023193
Validation loss: 2.016665353569933

Epoch: 113| Step: 0
Training loss: 1.0321639776229858
Validation loss: 2.0101471306175314

Epoch: 6| Step: 1
Training loss: 1.6088004112243652
Validation loss: 2.0092230766050276

Epoch: 6| Step: 2
Training loss: 1.5635035037994385
Validation loss: 2.013409312053393

Epoch: 6| Step: 3
Training loss: 2.9798007011413574
Validation loss: 2.0182689787239156

Epoch: 6| Step: 4
Training loss: 2.845461845397949
Validation loss: 2.0040093365535943

Epoch: 6| Step: 5
Training loss: 2.135319948196411
Validation loss: 2.020953598842826

Epoch: 6| Step: 6
Training loss: 2.155852794647217
Validation loss: 2.0168649355570474

Epoch: 6| Step: 7
Training loss: 1.297540545463562
Validation loss: 2.002281673492924

Epoch: 6| Step: 8
Training loss: 2.086455821990967
Validation loss: 1.9973694842348817

Epoch: 6| Step: 9
Training loss: 2.269988536834717
Validation loss: 2.0240451110306608

Epoch: 6| Step: 10
Training loss: 2.448552131652832
Validation loss: 2.018190360838367

Epoch: 6| Step: 11
Training loss: 2.2713372707366943
Validation loss: 2.0116580609352357

Epoch: 6| Step: 12
Training loss: 3.312897205352783
Validation loss: 2.0175462743287444

Epoch: 6| Step: 13
Training loss: 1.70831298828125
Validation loss: 2.0139597961979527

Epoch: 114| Step: 0
Training loss: 1.094508171081543
Validation loss: 2.026564144319104

Epoch: 6| Step: 1
Training loss: 2.045586347579956
Validation loss: 1.995294911887056

Epoch: 6| Step: 2
Training loss: 1.8307099342346191
Validation loss: 2.0232123546702887

Epoch: 6| Step: 3
Training loss: 2.0133769512176514
Validation loss: 2.0202122067892425

Epoch: 6| Step: 4
Training loss: 1.8018200397491455
Validation loss: 2.0270230565019833

Epoch: 6| Step: 5
Training loss: 3.2717742919921875
Validation loss: 2.0133614206826813

Epoch: 6| Step: 6
Training loss: 2.3500006198883057
Validation loss: 2.024770236784412

Epoch: 6| Step: 7
Training loss: 2.2823100090026855
Validation loss: 2.043345264209214

Epoch: 6| Step: 8
Training loss: 1.983160376548767
Validation loss: 2.012761357010052

Epoch: 6| Step: 9
Training loss: 1.8994313478469849
Validation loss: 2.0267818012545185

Epoch: 6| Step: 10
Training loss: 1.3960037231445312
Validation loss: 2.016482996684249

Epoch: 6| Step: 11
Training loss: 2.4649648666381836
Validation loss: 2.0198596997927596

Epoch: 6| Step: 12
Training loss: 2.672699213027954
Validation loss: 2.03519808348789

Epoch: 6| Step: 13
Training loss: 2.7283544540405273
Validation loss: 2.0295999511595695

Epoch: 115| Step: 0
Training loss: 1.794450283050537
Validation loss: 2.0030123187649633

Epoch: 6| Step: 1
Training loss: 2.2796027660369873
Validation loss: 2.0289493350572485

Epoch: 6| Step: 2
Training loss: 2.475097179412842
Validation loss: 2.003476086483207

Epoch: 6| Step: 3
Training loss: 1.4530434608459473
Validation loss: 2.023947133812853

Epoch: 6| Step: 4
Training loss: 1.6896402835845947
Validation loss: 2.01802719536648

Epoch: 6| Step: 5
Training loss: 2.236666440963745
Validation loss: 2.006665746370951

Epoch: 6| Step: 6
Training loss: 2.4003539085388184
Validation loss: 2.0340924057909238

Epoch: 6| Step: 7
Training loss: 1.9402209520339966
Validation loss: 2.0221020214019285

Epoch: 6| Step: 8
Training loss: 2.279242992401123
Validation loss: 2.0274526598632976

Epoch: 6| Step: 9
Training loss: 2.1811611652374268
Validation loss: 2.0308563273440123

Epoch: 6| Step: 10
Training loss: 2.489198684692383
Validation loss: 2.025074989564957

Epoch: 6| Step: 11
Training loss: 2.4134116172790527
Validation loss: 2.0410124486492527

Epoch: 6| Step: 12
Training loss: 1.8516759872436523
Validation loss: 2.0081967333311677

Epoch: 6| Step: 13
Training loss: 2.4220170974731445
Validation loss: 2.0176972932713007

Epoch: 116| Step: 0
Training loss: 2.3354015350341797
Validation loss: 2.0150689694189254

Epoch: 6| Step: 1
Training loss: 2.095423698425293
Validation loss: 2.0341112280404694

Epoch: 6| Step: 2
Training loss: 1.8661260604858398
Validation loss: 2.018634798706219

Epoch: 6| Step: 3
Training loss: 2.5943098068237305
Validation loss: 2.0365424361280215

Epoch: 6| Step: 4
Training loss: 2.0581963062286377
Validation loss: 2.0361822792278823

Epoch: 6| Step: 5
Training loss: 2.052396297454834
Validation loss: 2.040129462877909

Epoch: 6| Step: 6
Training loss: 1.935516357421875
Validation loss: 2.0125500438033894

Epoch: 6| Step: 7
Training loss: 2.0915088653564453
Validation loss: 2.0127256275505148

Epoch: 6| Step: 8
Training loss: 1.9508408308029175
Validation loss: 2.016360418770903

Epoch: 6| Step: 9
Training loss: 1.9612584114074707
Validation loss: 2.0220406888633646

Epoch: 6| Step: 10
Training loss: 2.1900203227996826
Validation loss: 2.027231554831228

Epoch: 6| Step: 11
Training loss: 1.670222520828247
Validation loss: 2.008171114870297

Epoch: 6| Step: 12
Training loss: 2.4191646575927734
Validation loss: 2.0068094717558993

Epoch: 6| Step: 13
Training loss: 2.959751605987549
Validation loss: 2.020421511383467

Epoch: 117| Step: 0
Training loss: 1.701051115989685
Validation loss: 2.021542174841768

Epoch: 6| Step: 1
Training loss: 2.2179927825927734
Validation loss: 2.0081735041833695

Epoch: 6| Step: 2
Training loss: 1.7312641143798828
Validation loss: 2.0407958876702095

Epoch: 6| Step: 3
Training loss: 1.570462703704834
Validation loss: 2.0315622001565914

Epoch: 6| Step: 4
Training loss: 2.2070958614349365
Validation loss: 2.045546554750012

Epoch: 6| Step: 5
Training loss: 2.1800737380981445
Validation loss: 2.0350745006274154

Epoch: 6| Step: 6
Training loss: 2.718106985092163
Validation loss: 1.9981382918614212

Epoch: 6| Step: 7
Training loss: 2.5123450756073
Validation loss: 2.002961212588895

Epoch: 6| Step: 8
Training loss: 1.9492473602294922
Validation loss: 2.027527082350946

Epoch: 6| Step: 9
Training loss: 1.526259183883667
Validation loss: 2.026287281385032

Epoch: 6| Step: 10
Training loss: 1.8516929149627686
Validation loss: 2.0246537193175285

Epoch: 6| Step: 11
Training loss: 2.886564254760742
Validation loss: 2.037441479262485

Epoch: 6| Step: 12
Training loss: 2.3972725868225098
Validation loss: 2.0170013186752156

Epoch: 6| Step: 13
Training loss: 2.5870304107666016
Validation loss: 2.037733577912854

Epoch: 118| Step: 0
Training loss: 2.1458024978637695
Validation loss: 2.024758285091769

Epoch: 6| Step: 1
Training loss: 2.0453553199768066
Validation loss: 2.034969465706938

Epoch: 6| Step: 2
Training loss: 2.1054279804229736
Validation loss: 2.0323517450722317

Epoch: 6| Step: 3
Training loss: 2.7349424362182617
Validation loss: 1.9787588529689337

Epoch: 6| Step: 4
Training loss: 2.3198914527893066
Validation loss: 2.0357769548252063

Epoch: 6| Step: 5
Training loss: 1.856104850769043
Validation loss: 1.9936655080446632

Epoch: 6| Step: 6
Training loss: 2.258458137512207
Validation loss: 2.0138047587487007

Epoch: 6| Step: 7
Training loss: 2.5444273948669434
Validation loss: 2.0116482088642735

Epoch: 6| Step: 8
Training loss: 1.9228676557540894
Validation loss: 2.0045757191155547

Epoch: 6| Step: 9
Training loss: 1.75765860080719
Validation loss: 2.0124893470477034

Epoch: 6| Step: 10
Training loss: 2.098646402359009
Validation loss: 1.995943982114074

Epoch: 6| Step: 11
Training loss: 1.7488304376602173
Validation loss: 2.013447474407893

Epoch: 6| Step: 12
Training loss: 1.8415818214416504
Validation loss: 2.002100468963705

Epoch: 6| Step: 13
Training loss: 2.28131365776062
Validation loss: 2.024314990607641

Epoch: 119| Step: 0
Training loss: 1.6103227138519287
Validation loss: 2.008920351664225

Epoch: 6| Step: 1
Training loss: 2.1250557899475098
Validation loss: 2.0089187109342186

Epoch: 6| Step: 2
Training loss: 2.241899013519287
Validation loss: 2.0273649154170865

Epoch: 6| Step: 3
Training loss: 1.6038250923156738
Validation loss: 2.0307894522143948

Epoch: 6| Step: 4
Training loss: 2.7299938201904297
Validation loss: 2.0316828207303117

Epoch: 6| Step: 5
Training loss: 2.329836845397949
Validation loss: 2.062538177736344

Epoch: 6| Step: 6
Training loss: 2.624314308166504
Validation loss: 2.0288616764929985

Epoch: 6| Step: 7
Training loss: 2.4923477172851562
Validation loss: 2.019994467817327

Epoch: 6| Step: 8
Training loss: 2.1937708854675293
Validation loss: 2.0185301188499696

Epoch: 6| Step: 9
Training loss: 2.214308261871338
Validation loss: 2.0254210349052184

Epoch: 6| Step: 10
Training loss: 2.3473758697509766
Validation loss: 2.024077869230701

Epoch: 6| Step: 11
Training loss: 1.0834028720855713
Validation loss: 2.0259567358160533

Epoch: 6| Step: 12
Training loss: 1.6114141941070557
Validation loss: 2.020409386645081

Epoch: 6| Step: 13
Training loss: 2.4712185859680176
Validation loss: 1.999500310549172

Epoch: 120| Step: 0
Training loss: 2.063231945037842
Validation loss: 2.0167941739482265

Epoch: 6| Step: 1
Training loss: 2.573625326156616
Validation loss: 2.001391057045229

Epoch: 6| Step: 2
Training loss: 2.3768959045410156
Validation loss: 2.0080627754170406

Epoch: 6| Step: 3
Training loss: 1.6344244480133057
Validation loss: 2.023309960160204

Epoch: 6| Step: 4
Training loss: 2.276944637298584
Validation loss: 2.020976830554265

Epoch: 6| Step: 5
Training loss: 2.218282699584961
Validation loss: 2.008805992782757

Epoch: 6| Step: 6
Training loss: 1.6734892129898071
Validation loss: 2.0280336077495287

Epoch: 6| Step: 7
Training loss: 1.6788487434387207
Validation loss: 2.0306008515819425

Epoch: 6| Step: 8
Training loss: 1.9124622344970703
Validation loss: 2.0289143849444646

Epoch: 6| Step: 9
Training loss: 2.667916774749756
Validation loss: 2.0451651337326213

Epoch: 6| Step: 10
Training loss: 2.3902781009674072
Validation loss: 2.0272572143103487

Epoch: 6| Step: 11
Training loss: 2.575674057006836
Validation loss: 2.007588440372098

Epoch: 6| Step: 12
Training loss: 1.6058717966079712
Validation loss: 2.015974916437621

Epoch: 6| Step: 13
Training loss: 1.8535943031311035
Validation loss: 2.033834627879563

Epoch: 121| Step: 0
Training loss: 1.9089646339416504
Validation loss: 1.9986893246250768

Epoch: 6| Step: 1
Training loss: 2.3782827854156494
Validation loss: 2.028784110981931

Epoch: 6| Step: 2
Training loss: 2.2651240825653076
Validation loss: 2.0195573632435133

Epoch: 6| Step: 3
Training loss: 1.7702685594558716
Validation loss: 2.0232363593193794

Epoch: 6| Step: 4
Training loss: 1.754037857055664
Validation loss: 2.0302754614942815

Epoch: 6| Step: 5
Training loss: 2.114213466644287
Validation loss: 2.0208383349962133

Epoch: 6| Step: 6
Training loss: 2.155956506729126
Validation loss: 2.0387957660100793

Epoch: 6| Step: 7
Training loss: 2.6832144260406494
Validation loss: 2.0125754033365557

Epoch: 6| Step: 8
Training loss: 1.7440507411956787
Validation loss: 2.0274779412054245

Epoch: 6| Step: 9
Training loss: 2.644956111907959
Validation loss: 2.026630578502532

Epoch: 6| Step: 10
Training loss: 2.2190818786621094
Validation loss: 2.021603452262058

Epoch: 6| Step: 11
Training loss: 2.2197790145874023
Validation loss: 2.0196741729654293

Epoch: 6| Step: 12
Training loss: 1.518043041229248
Validation loss: 2.028750047888807

Epoch: 6| Step: 13
Training loss: 2.146277666091919
Validation loss: 2.02233338099654

Epoch: 122| Step: 0
Training loss: 2.1482319831848145
Validation loss: 2.022216107255669

Epoch: 6| Step: 1
Training loss: 2.4307446479797363
Validation loss: 2.018214459060341

Epoch: 6| Step: 2
Training loss: 2.2200281620025635
Validation loss: 2.0262464579715522

Epoch: 6| Step: 3
Training loss: 1.9583089351654053
Validation loss: 2.0296473387748963

Epoch: 6| Step: 4
Training loss: 2.2507448196411133
Validation loss: 2.002825721617668

Epoch: 6| Step: 5
Training loss: 1.7566421031951904
Validation loss: 2.016651174073578

Epoch: 6| Step: 6
Training loss: 2.396474599838257
Validation loss: 2.042492542215573

Epoch: 6| Step: 7
Training loss: 1.4766627550125122
Validation loss: 2.0222594109914636

Epoch: 6| Step: 8
Training loss: 3.003905773162842
Validation loss: 2.02360971255969

Epoch: 6| Step: 9
Training loss: 2.41764235496521
Validation loss: 2.0331734329141598

Epoch: 6| Step: 10
Training loss: 1.3394052982330322
Validation loss: 2.0441724933603758

Epoch: 6| Step: 11
Training loss: 2.025167942047119
Validation loss: 2.041962371077589

Epoch: 6| Step: 12
Training loss: 2.1318535804748535
Validation loss: 2.021334086695025

Epoch: 6| Step: 13
Training loss: 1.8028733730316162
Validation loss: 2.0282362327780774

Epoch: 123| Step: 0
Training loss: 2.185879945755005
Validation loss: 2.0016187006427395

Epoch: 6| Step: 1
Training loss: 1.5304025411605835
Validation loss: 2.02655973229357

Epoch: 6| Step: 2
Training loss: 2.316952705383301
Validation loss: 1.9976149220620432

Epoch: 6| Step: 3
Training loss: 1.9323278665542603
Validation loss: 2.0158879013471704

Epoch: 6| Step: 4
Training loss: 2.042191505432129
Validation loss: 2.035398151284905

Epoch: 6| Step: 5
Training loss: 2.3350889682769775
Validation loss: 2.043155315101788

Epoch: 6| Step: 6
Training loss: 1.844610333442688
Validation loss: 2.035643941612654

Epoch: 6| Step: 7
Training loss: 2.088677406311035
Validation loss: 2.049122687309019

Epoch: 6| Step: 8
Training loss: 2.836139678955078
Validation loss: 2.0328716411385486

Epoch: 6| Step: 9
Training loss: 2.1559994220733643
Validation loss: 2.0194222991184523

Epoch: 6| Step: 10
Training loss: 2.1248154640197754
Validation loss: 2.0414847097089215

Epoch: 6| Step: 11
Training loss: 2.058037519454956
Validation loss: 2.013941796877051

Epoch: 6| Step: 12
Training loss: 1.9010744094848633
Validation loss: 2.0436930579523884

Epoch: 6| Step: 13
Training loss: 2.3632683753967285
Validation loss: 2.030358955424319

Epoch: 124| Step: 0
Training loss: 2.42447829246521
Validation loss: 2.0416696097261164

Epoch: 6| Step: 1
Training loss: 2.425431489944458
Validation loss: 2.03329570575427

Epoch: 6| Step: 2
Training loss: 1.5363762378692627
Validation loss: 2.0303951668482956

Epoch: 6| Step: 3
Training loss: 2.1868090629577637
Validation loss: 2.020833512788178

Epoch: 6| Step: 4
Training loss: 2.0357072353363037
Validation loss: 2.026956744091485

Epoch: 6| Step: 5
Training loss: 1.8349344730377197
Validation loss: 2.0279633383597098

Epoch: 6| Step: 6
Training loss: 2.2488651275634766
Validation loss: 2.022760519417383

Epoch: 6| Step: 7
Training loss: 1.9118224382400513
Validation loss: 2.021542633733442

Epoch: 6| Step: 8
Training loss: 2.0719447135925293
Validation loss: 2.004501176136796

Epoch: 6| Step: 9
Training loss: 2.0757195949554443
Validation loss: 2.0048773237453994

Epoch: 6| Step: 10
Training loss: 2.354961395263672
Validation loss: 2.0399970739118514

Epoch: 6| Step: 11
Training loss: 1.8682085275650024
Validation loss: 1.9983102044751566

Epoch: 6| Step: 12
Training loss: 2.3916752338409424
Validation loss: 2.0429854521187405

Epoch: 6| Step: 13
Training loss: 1.882102131843567
Validation loss: 2.0317271576132825

Epoch: 125| Step: 0
Training loss: 2.4853320121765137
Validation loss: 2.0238563296615437

Epoch: 6| Step: 1
Training loss: 2.067294120788574
Validation loss: 2.0164602277099446

Epoch: 6| Step: 2
Training loss: 2.2547693252563477
Validation loss: 2.0114847331918697

Epoch: 6| Step: 3
Training loss: 1.9618451595306396
Validation loss: 2.0245732210015737

Epoch: 6| Step: 4
Training loss: 2.5560684204101562
Validation loss: 2.017403453908941

Epoch: 6| Step: 5
Training loss: 2.13291597366333
Validation loss: 2.053522821395628

Epoch: 6| Step: 6
Training loss: 1.2715153694152832
Validation loss: 2.0381144913293983

Epoch: 6| Step: 7
Training loss: 1.9198590517044067
Validation loss: 2.0095293444971882

Epoch: 6| Step: 8
Training loss: 1.8426854610443115
Validation loss: 2.0331526174340198

Epoch: 6| Step: 9
Training loss: 1.8469974994659424
Validation loss: 2.032752586949256

Epoch: 6| Step: 10
Training loss: 1.865840196609497
Validation loss: 2.0259564756065287

Epoch: 6| Step: 11
Training loss: 2.1059980392456055
Validation loss: 2.017341870133595

Epoch: 6| Step: 12
Training loss: 2.1145293712615967
Validation loss: 2.024587400497929

Epoch: 6| Step: 13
Training loss: 3.776416778564453
Validation loss: 2.0090277194976807

Testing loss: 2.1119109365675186

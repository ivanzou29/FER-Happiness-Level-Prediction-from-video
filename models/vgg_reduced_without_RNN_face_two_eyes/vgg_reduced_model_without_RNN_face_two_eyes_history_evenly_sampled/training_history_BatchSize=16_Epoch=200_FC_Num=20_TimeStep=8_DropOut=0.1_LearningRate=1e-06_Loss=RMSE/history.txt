Epoch: 1| Step: 0
Training loss: 4.912062002753679
Validation loss: 5.101035071256123

Epoch: 6| Step: 1
Training loss: 5.4997679921546965
Validation loss: 5.093957269305419

Epoch: 6| Step: 2
Training loss: 4.8445242970463935
Validation loss: 5.086873521156751

Epoch: 6| Step: 3
Training loss: 4.648250800478648
Validation loss: 5.0818766089071365

Epoch: 6| Step: 4
Training loss: 5.061878872667502
Validation loss: 5.074292191524473

Epoch: 6| Step: 5
Training loss: 5.719712473438555
Validation loss: 5.066696843919332

Epoch: 6| Step: 6
Training loss: 4.9449773256034675
Validation loss: 5.061997725346898

Epoch: 6| Step: 7
Training loss: 5.182102566692283
Validation loss: 5.053893512988538

Epoch: 6| Step: 8
Training loss: 5.175943025184027
Validation loss: 5.04906765447422

Epoch: 6| Step: 9
Training loss: 5.052182456810551
Validation loss: 5.043338275119686

Epoch: 6| Step: 10
Training loss: 5.596425359695296
Validation loss: 5.034225451278111

Epoch: 6| Step: 11
Training loss: 5.655569130398876
Validation loss: 5.0286423775180165

Epoch: 6| Step: 12
Training loss: 4.415268778757628
Validation loss: 5.0233538746382

Epoch: 6| Step: 13
Training loss: 4.885561335630634
Validation loss: 5.018339650012374

Epoch: 2| Step: 0
Training loss: 6.370619521743573
Validation loss: 5.011751614622243

Epoch: 6| Step: 1
Training loss: 5.075047520006317
Validation loss: 5.00466282114694

Epoch: 6| Step: 2
Training loss: 4.731993828660644
Validation loss: 4.9981161485900465

Epoch: 6| Step: 3
Training loss: 4.063386790982956
Validation loss: 4.992393049710157

Epoch: 6| Step: 4
Training loss: 4.469053458200725
Validation loss: 4.988182699355965

Epoch: 6| Step: 5
Training loss: 5.299323402258869
Validation loss: 4.980649691823065

Epoch: 6| Step: 6
Training loss: 6.039384011695453
Validation loss: 4.974474723806264

Epoch: 6| Step: 7
Training loss: 5.352465387737571
Validation loss: 4.969223603554649

Epoch: 6| Step: 8
Training loss: 4.875958861905835
Validation loss: 4.963431491715183

Epoch: 6| Step: 9
Training loss: 4.994880534930494
Validation loss: 4.955593527221317

Epoch: 6| Step: 10
Training loss: 5.563525501830784
Validation loss: 4.950635590925928

Epoch: 6| Step: 11
Training loss: 4.34315541540967
Validation loss: 4.943174381726056

Epoch: 6| Step: 12
Training loss: 3.490466621924572
Validation loss: 4.938450564079912

Epoch: 6| Step: 13
Training loss: 5.554788795116301
Validation loss: 4.933753161427566

Epoch: 3| Step: 0
Training loss: 5.151397826478789
Validation loss: 4.927130268394297

Epoch: 6| Step: 1
Training loss: 4.187345815068967
Validation loss: 4.922864343057367

Epoch: 6| Step: 2
Training loss: 5.747471792608985
Validation loss: 4.917130732895779

Epoch: 6| Step: 3
Training loss: 5.481367628864504
Validation loss: 4.908292626430217

Epoch: 6| Step: 4
Training loss: 3.618718591530701
Validation loss: 4.905837149878872

Epoch: 6| Step: 5
Training loss: 5.397720399697048
Validation loss: 4.897186782719819

Epoch: 6| Step: 6
Training loss: 4.8816736953248885
Validation loss: 4.891532997389763

Epoch: 6| Step: 7
Training loss: 5.089087850983641
Validation loss: 4.888711535537615

Epoch: 6| Step: 8
Training loss: 5.387578750353424
Validation loss: 4.881734831512604

Epoch: 6| Step: 9
Training loss: 5.556056201417123
Validation loss: 4.872898497613789

Epoch: 6| Step: 10
Training loss: 4.407015037405363
Validation loss: 4.868432803826778

Epoch: 6| Step: 11
Training loss: 4.3296477465695755
Validation loss: 4.861705013429351

Epoch: 6| Step: 12
Training loss: 4.976712354862104
Validation loss: 4.856115400077814

Epoch: 6| Step: 13
Training loss: 4.801438338626136
Validation loss: 4.849086520764104

Epoch: 4| Step: 0
Training loss: 5.661446728967906
Validation loss: 4.843096928494704

Epoch: 6| Step: 1
Training loss: 4.6865823483286855
Validation loss: 4.836756491551047

Epoch: 6| Step: 2
Training loss: 5.509341196874994
Validation loss: 4.83185911061392

Epoch: 6| Step: 3
Training loss: 3.695543600521706
Validation loss: 4.826155505652649

Epoch: 6| Step: 4
Training loss: 4.995335883073009
Validation loss: 4.817135216553411

Epoch: 6| Step: 5
Training loss: 4.640403870731091
Validation loss: 4.810271984449898

Epoch: 6| Step: 6
Training loss: 4.174367324495348
Validation loss: 4.80587310582746

Epoch: 6| Step: 7
Training loss: 4.9377389198153026
Validation loss: 4.802598998120595

Epoch: 6| Step: 8
Training loss: 5.7915699170857105
Validation loss: 4.792717702908831

Epoch: 6| Step: 9
Training loss: 5.004621658582859
Validation loss: 4.787779678807122

Epoch: 6| Step: 10
Training loss: 3.913503641216767
Validation loss: 4.781339941184847

Epoch: 6| Step: 11
Training loss: 4.830661144617392
Validation loss: 4.774340809602117

Epoch: 6| Step: 12
Training loss: 5.167098037595856
Validation loss: 4.770287320161214

Epoch: 6| Step: 13
Training loss: 4.991512724594134
Validation loss: 4.7637830800811924

Epoch: 5| Step: 0
Training loss: 5.111005890265354
Validation loss: 4.758506730854025

Epoch: 6| Step: 1
Training loss: 5.3720416422101245
Validation loss: 4.749548041576776

Epoch: 6| Step: 2
Training loss: 4.421907782011876
Validation loss: 4.743090458855131

Epoch: 6| Step: 3
Training loss: 5.509665233044982
Validation loss: 4.735302503166717

Epoch: 6| Step: 4
Training loss: 4.897123948556877
Validation loss: 4.731163850356697

Epoch: 6| Step: 5
Training loss: 5.192706333518401
Validation loss: 4.7260243728417

Epoch: 6| Step: 6
Training loss: 4.651025080749169
Validation loss: 4.717492307375009

Epoch: 6| Step: 7
Training loss: 5.403992286207904
Validation loss: 4.707576848113088

Epoch: 6| Step: 8
Training loss: 4.681459527616982
Validation loss: 4.705312860818058

Epoch: 6| Step: 9
Training loss: 4.09163418903915
Validation loss: 4.696964879181283

Epoch: 6| Step: 10
Training loss: 4.826369870862686
Validation loss: 4.688370619561295

Epoch: 6| Step: 11
Training loss: 4.284530830470923
Validation loss: 4.681903627248734

Epoch: 6| Step: 12
Training loss: 4.3560305656432625
Validation loss: 4.67555195448394

Epoch: 6| Step: 13
Training loss: 3.505293521535556
Validation loss: 4.667336824892873

Epoch: 6| Step: 0
Training loss: 3.9539236837536764
Validation loss: 4.661141967853822

Epoch: 6| Step: 1
Training loss: 3.9883360079033556
Validation loss: 4.656811835746231

Epoch: 6| Step: 2
Training loss: 4.2830592877980305
Validation loss: 4.647530857240068

Epoch: 6| Step: 3
Training loss: 4.117066122795777
Validation loss: 4.642165276057291

Epoch: 6| Step: 4
Training loss: 5.066808303727185
Validation loss: 4.633442360271077

Epoch: 6| Step: 5
Training loss: 4.738146598393322
Validation loss: 4.6286351762968865

Epoch: 6| Step: 6
Training loss: 5.744572607332765
Validation loss: 4.618843681758242

Epoch: 6| Step: 7
Training loss: 4.878232617752088
Validation loss: 4.613183542034651

Epoch: 6| Step: 8
Training loss: 3.8498402549993775
Validation loss: 4.606967054156289

Epoch: 6| Step: 9
Training loss: 5.6565661262984035
Validation loss: 4.598469525154206

Epoch: 6| Step: 10
Training loss: 4.636001698583336
Validation loss: 4.588772210072581

Epoch: 6| Step: 11
Training loss: 5.042406117251361
Validation loss: 4.586371812810679

Epoch: 6| Step: 12
Training loss: 4.287991527264324
Validation loss: 4.575197456340012

Epoch: 6| Step: 13
Training loss: 5.388404632169013
Validation loss: 4.56737550108972

Epoch: 7| Step: 0
Training loss: 4.567548493126323
Validation loss: 4.555909563398205

Epoch: 6| Step: 1
Training loss: 3.9356275677986847
Validation loss: 4.550085209523374

Epoch: 6| Step: 2
Training loss: 5.098989588557595
Validation loss: 4.541935571343551

Epoch: 6| Step: 3
Training loss: 4.598080209584162
Validation loss: 4.535831776840665

Epoch: 6| Step: 4
Training loss: 4.483057552964646
Validation loss: 4.526265746699641

Epoch: 6| Step: 5
Training loss: 3.912564110763861
Validation loss: 4.515010959612533

Epoch: 6| Step: 6
Training loss: 4.541525718998324
Validation loss: 4.506325579355005

Epoch: 6| Step: 7
Training loss: 4.756196447564708
Validation loss: 4.501173010089425

Epoch: 6| Step: 8
Training loss: 3.8192384298231357
Validation loss: 4.4906706501060825

Epoch: 6| Step: 9
Training loss: 3.894400239537278
Validation loss: 4.48271182372203

Epoch: 6| Step: 10
Training loss: 5.5168638918254755
Validation loss: 4.474474291967501

Epoch: 6| Step: 11
Training loss: 5.666783425119318
Validation loss: 4.465318464464453

Epoch: 6| Step: 12
Training loss: 5.1228674196552495
Validation loss: 4.45709815792104

Epoch: 6| Step: 13
Training loss: 3.4470534305266103
Validation loss: 4.449986604156739

Epoch: 8| Step: 0
Training loss: 5.346768781363861
Validation loss: 4.43635131143479

Epoch: 6| Step: 1
Training loss: 5.404734139904281
Validation loss: 4.430683059487294

Epoch: 6| Step: 2
Training loss: 4.642611608195937
Validation loss: 4.419020191519764

Epoch: 6| Step: 3
Training loss: 5.228040637277918
Validation loss: 4.410112128820036

Epoch: 6| Step: 4
Training loss: 3.990461181528293
Validation loss: 4.398782490899647

Epoch: 6| Step: 5
Training loss: 3.6569780456284526
Validation loss: 4.391445837130996

Epoch: 6| Step: 6
Training loss: 4.36585718956012
Validation loss: 4.381992582401035

Epoch: 6| Step: 7
Training loss: 3.677077483405772
Validation loss: 4.370084020502857

Epoch: 6| Step: 8
Training loss: 4.484182745769951
Validation loss: 4.362772985989976

Epoch: 6| Step: 9
Training loss: 3.45519432791094
Validation loss: 4.35359338298335

Epoch: 6| Step: 10
Training loss: 5.418549410969343
Validation loss: 4.344559906864512

Epoch: 6| Step: 11
Training loss: 3.309697531165459
Validation loss: 4.332836438947934

Epoch: 6| Step: 12
Training loss: 5.089365376778768
Validation loss: 4.325085824597028

Epoch: 6| Step: 13
Training loss: 3.200558857993242
Validation loss: 4.316094996007596

Epoch: 9| Step: 0
Training loss: 3.4497871664989166
Validation loss: 4.304155510895283

Epoch: 6| Step: 1
Training loss: 4.003782391376934
Validation loss: 4.295135388132897

Epoch: 6| Step: 2
Training loss: 4.293325949084011
Validation loss: 4.28569712491367

Epoch: 6| Step: 3
Training loss: 3.957869144460546
Validation loss: 4.274298154093021

Epoch: 6| Step: 4
Training loss: 3.91355225669329
Validation loss: 4.2630948665978705

Epoch: 6| Step: 5
Training loss: 5.425781953068709
Validation loss: 4.254784120221646

Epoch: 6| Step: 6
Training loss: 5.086266385415554
Validation loss: 4.239530077402217

Epoch: 6| Step: 7
Training loss: 4.722640498965669
Validation loss: 4.233408135168368

Epoch: 6| Step: 8
Training loss: 3.6387506334152904
Validation loss: 4.221724074042182

Epoch: 6| Step: 9
Training loss: 3.751990489386439
Validation loss: 4.2101864425876645

Epoch: 6| Step: 10
Training loss: 4.585648847655467
Validation loss: 4.201289577536976

Epoch: 6| Step: 11
Training loss: 4.731149714224673
Validation loss: 4.189931723924121

Epoch: 6| Step: 12
Training loss: 5.060121330646072
Validation loss: 4.179438751795462

Epoch: 6| Step: 13
Training loss: 2.86818584139033
Validation loss: 4.165446989323953

Epoch: 10| Step: 0
Training loss: 4.789114275056587
Validation loss: 4.155788462929332

Epoch: 6| Step: 1
Training loss: 4.299995315904062
Validation loss: 4.143913058479149

Epoch: 6| Step: 2
Training loss: 4.315891204070832
Validation loss: 4.127576764624823

Epoch: 6| Step: 3
Training loss: 3.0951688961356325
Validation loss: 4.116753216025006

Epoch: 6| Step: 4
Training loss: 5.032775268606465
Validation loss: 4.1065246036700485

Epoch: 6| Step: 5
Training loss: 3.4803364018923393
Validation loss: 4.08981959186391

Epoch: 6| Step: 6
Training loss: 4.131251366596313
Validation loss: 4.081521377871555

Epoch: 6| Step: 7
Training loss: 3.5553223430788665
Validation loss: 4.0699970783768284

Epoch: 6| Step: 8
Training loss: 4.129778319683341
Validation loss: 4.056956967529531

Epoch: 6| Step: 9
Training loss: 4.7469219474153475
Validation loss: 4.047391147079729

Epoch: 6| Step: 10
Training loss: 3.9860625638822573
Validation loss: 4.0327782728918535

Epoch: 6| Step: 11
Training loss: 4.900167010828181
Validation loss: 4.019407954503935

Epoch: 6| Step: 12
Training loss: 3.795885624335205
Validation loss: 4.008725877629386

Epoch: 6| Step: 13
Training loss: 3.6411326304153477
Validation loss: 3.9893601496831708

Epoch: 11| Step: 0
Training loss: 3.471816211274814
Validation loss: 3.9810791900058606

Epoch: 6| Step: 1
Training loss: 5.056671932236997
Validation loss: 3.9620360607269403

Epoch: 6| Step: 2
Training loss: 4.045127461880692
Validation loss: 3.9555490378462506

Epoch: 6| Step: 3
Training loss: 4.728760061835672
Validation loss: 3.93872041763207

Epoch: 6| Step: 4
Training loss: 3.833378141942949
Validation loss: 3.9215166233527623

Epoch: 6| Step: 5
Training loss: 4.259178124605631
Validation loss: 3.905746970981827

Epoch: 6| Step: 6
Training loss: 3.0827991607114473
Validation loss: 3.894937663783116

Epoch: 6| Step: 7
Training loss: 4.985703246502204
Validation loss: 3.8803125920658696

Epoch: 6| Step: 8
Training loss: 3.692228227762833
Validation loss: 3.8638778748901355

Epoch: 6| Step: 9
Training loss: 3.7075639991098086
Validation loss: 3.8533202426161615

Epoch: 6| Step: 10
Training loss: 3.957738905326729
Validation loss: 3.839119618722164

Epoch: 6| Step: 11
Training loss: 3.619222443464325
Validation loss: 3.8215497268221617

Epoch: 6| Step: 12
Training loss: 3.7672595516799903
Validation loss: 3.8046749455310382

Epoch: 6| Step: 13
Training loss: 3.0324346056347484
Validation loss: 3.7880210176689957

Epoch: 12| Step: 0
Training loss: 3.6491890568179532
Validation loss: 3.7752130145575964

Epoch: 6| Step: 1
Training loss: 3.5491347319566575
Validation loss: 3.761698389218135

Epoch: 6| Step: 2
Training loss: 4.645832466615787
Validation loss: 3.7410592173918524

Epoch: 6| Step: 3
Training loss: 4.31214129296775
Validation loss: 3.7307733697555925

Epoch: 6| Step: 4
Training loss: 4.169948861885741
Validation loss: 3.7118725181722283

Epoch: 6| Step: 5
Training loss: 3.8416893870798243
Validation loss: 3.6913638192628113

Epoch: 6| Step: 6
Training loss: 4.084781052734565
Validation loss: 3.6781792157879787

Epoch: 6| Step: 7
Training loss: 3.1242531456166462
Validation loss: 3.6567657782829865

Epoch: 6| Step: 8
Training loss: 2.422204468989665
Validation loss: 3.643992133422161

Epoch: 6| Step: 9
Training loss: 4.023407160079781
Validation loss: 3.6252923182106374

Epoch: 6| Step: 10
Training loss: 3.378371497379203
Validation loss: 3.6117482410272514

Epoch: 6| Step: 11
Training loss: 3.243193909123803
Validation loss: 3.596937878439358

Epoch: 6| Step: 12
Training loss: 4.609095264287063
Validation loss: 3.5811662371478836

Epoch: 6| Step: 13
Training loss: 3.754905543859998
Validation loss: 3.565735725382663

Epoch: 13| Step: 0
Training loss: 3.16479453578512
Validation loss: 3.552379743140031

Epoch: 6| Step: 1
Training loss: 3.1442205240014967
Validation loss: 3.530053886020412

Epoch: 6| Step: 2
Training loss: 3.747902728753346
Validation loss: 3.5196333955240737

Epoch: 6| Step: 3
Training loss: 4.118521023812308
Validation loss: 3.503192828204913

Epoch: 6| Step: 4
Training loss: 3.436788866871822
Validation loss: 3.482746565024852

Epoch: 6| Step: 5
Training loss: 4.230572446501497
Validation loss: 3.4640965531284067

Epoch: 6| Step: 6
Training loss: 3.772634227227863
Validation loss: 3.4522085923109813

Epoch: 6| Step: 7
Training loss: 2.974636465989981
Validation loss: 3.4280106377452304

Epoch: 6| Step: 8
Training loss: 3.783966727878678
Validation loss: 3.4179727297523876

Epoch: 6| Step: 9
Training loss: 3.211551356981587
Validation loss: 3.391516899963933

Epoch: 6| Step: 10
Training loss: 3.5868164846855506
Validation loss: 3.3773525368625186

Epoch: 6| Step: 11
Training loss: 3.986968628686508
Validation loss: 3.3580750339011534

Epoch: 6| Step: 12
Training loss: 3.695524374954627
Validation loss: 3.340493176453762

Epoch: 6| Step: 13
Training loss: 3.00367051329356
Validation loss: 3.32019422017642

Epoch: 14| Step: 0
Training loss: 3.7979345530836675
Validation loss: 3.3031530495651564

Epoch: 6| Step: 1
Training loss: 3.909854294191363
Validation loss: 3.285455408765958

Epoch: 6| Step: 2
Training loss: 3.12472624533356
Validation loss: 3.271917776419398

Epoch: 6| Step: 3
Training loss: 3.188683234113588
Validation loss: 3.242742348202317

Epoch: 6| Step: 4
Training loss: 3.019077834499164
Validation loss: 3.230303354626248

Epoch: 6| Step: 5
Training loss: 2.532168563137358
Validation loss: 3.2142721254641304

Epoch: 6| Step: 6
Training loss: 3.0981337190143288
Validation loss: 3.1941473684674504

Epoch: 6| Step: 7
Training loss: 2.7399012933479994
Validation loss: 3.1795447508663535

Epoch: 6| Step: 8
Training loss: 3.7268173802380673
Validation loss: 3.1677240836332166

Epoch: 6| Step: 9
Training loss: 4.397658175713515
Validation loss: 3.1438461924084193

Epoch: 6| Step: 10
Training loss: 3.6000924575376985
Validation loss: 3.1267836371987427

Epoch: 6| Step: 11
Training loss: 1.9536865037593194
Validation loss: 3.119051949936651

Epoch: 6| Step: 12
Training loss: 3.988215969945059
Validation loss: 3.0806196044521514

Epoch: 6| Step: 13
Training loss: 3.220659300793412
Validation loss: 3.077733796789949

Epoch: 15| Step: 0
Training loss: 3.532504373751536
Validation loss: 3.0528496887435597

Epoch: 6| Step: 1
Training loss: 3.1703752251102966
Validation loss: 3.0476642622364225

Epoch: 6| Step: 2
Training loss: 3.2244807217104925
Validation loss: 3.0141402311887098

Epoch: 6| Step: 3
Training loss: 2.759382973267386
Validation loss: 2.994386151195419

Epoch: 6| Step: 4
Training loss: 3.8476182035317543
Validation loss: 2.986584350322105

Epoch: 6| Step: 5
Training loss: 3.51789314854642
Validation loss: 2.959341506256499

Epoch: 6| Step: 6
Training loss: 2.7021366142562986
Validation loss: 2.9443996408879207

Epoch: 6| Step: 7
Training loss: 2.741524292825186
Validation loss: 2.927110576331343

Epoch: 6| Step: 8
Training loss: 2.7283924159614714
Validation loss: 2.9083477454182898

Epoch: 6| Step: 9
Training loss: 3.3385129898593284
Validation loss: 2.8947827574476923

Epoch: 6| Step: 10
Training loss: 3.547466615246929
Validation loss: 2.8800488707310943

Epoch: 6| Step: 11
Training loss: 3.200378729342899
Validation loss: 2.8522793936145394

Epoch: 6| Step: 12
Training loss: 2.8734690695060916
Validation loss: 2.8452059410511463

Epoch: 6| Step: 13
Training loss: 2.5585499534061684
Validation loss: 2.832658300630664

Epoch: 16| Step: 0
Training loss: 2.405922210957924
Validation loss: 2.81046798750294

Epoch: 6| Step: 1
Training loss: 3.017403508044439
Validation loss: 2.8031053095611274

Epoch: 6| Step: 2
Training loss: 3.3047154923644535
Validation loss: 2.7837223550636034

Epoch: 6| Step: 3
Training loss: 2.925358455245238
Validation loss: 2.7728807772467725

Epoch: 6| Step: 4
Training loss: 3.2726247138031304
Validation loss: 2.762214564270012

Epoch: 6| Step: 5
Training loss: 3.0119388168422954
Validation loss: 2.747607146566142

Epoch: 6| Step: 6
Training loss: 3.2134812574515177
Validation loss: 2.7267446713597723

Epoch: 6| Step: 7
Training loss: 3.094432408758195
Validation loss: 2.72243783724183

Epoch: 6| Step: 8
Training loss: 2.325554574434734
Validation loss: 2.714554016087437

Epoch: 6| Step: 9
Training loss: 2.9168283871820426
Validation loss: 2.6919037267030874

Epoch: 6| Step: 10
Training loss: 3.3943223614615725
Validation loss: 2.683730895284029

Epoch: 6| Step: 11
Training loss: 2.862718638769927
Validation loss: 2.6789156487587

Epoch: 6| Step: 12
Training loss: 2.673880554978109
Validation loss: 2.6601959722346566

Epoch: 6| Step: 13
Training loss: 3.0852541396259565
Validation loss: 2.6515008743156385

Epoch: 17| Step: 0
Training loss: 3.164141467957469
Validation loss: 2.6483439748023083

Epoch: 6| Step: 1
Training loss: 2.310352771954466
Validation loss: 2.626409748806726

Epoch: 6| Step: 2
Training loss: 2.5726768495144485
Validation loss: 2.6212823582101805

Epoch: 6| Step: 3
Training loss: 2.573542550959024
Validation loss: 2.6111071185195875

Epoch: 6| Step: 4
Training loss: 3.1082589911876437
Validation loss: 2.611019796816804

Epoch: 6| Step: 5
Training loss: 2.050739629413965
Validation loss: 2.5895788350232576

Epoch: 6| Step: 6
Training loss: 2.994405457292578
Validation loss: 2.590453333934857

Epoch: 6| Step: 7
Training loss: 3.4248947684278837
Validation loss: 2.5800254660962705

Epoch: 6| Step: 8
Training loss: 2.5863309716464093
Validation loss: 2.58436903467973

Epoch: 6| Step: 9
Training loss: 3.1533039809696315
Validation loss: 2.562710959682028

Epoch: 6| Step: 10
Training loss: 2.7583159508594415
Validation loss: 2.555952008250418

Epoch: 6| Step: 11
Training loss: 3.45445067216354
Validation loss: 2.560854819126944

Epoch: 6| Step: 12
Training loss: 2.4444841925924727
Validation loss: 2.5600574516539543

Epoch: 6| Step: 13
Training loss: 3.7513476175574434
Validation loss: 2.5446791764161767

Epoch: 18| Step: 0
Training loss: 2.491429611292706
Validation loss: 2.553138196134427

Epoch: 6| Step: 1
Training loss: 2.7268054056041
Validation loss: 2.5461981753746064

Epoch: 6| Step: 2
Training loss: 2.889757510773064
Validation loss: 2.530696225456165

Epoch: 6| Step: 3
Training loss: 3.101485474828752
Validation loss: 2.5350318649511547

Epoch: 6| Step: 4
Training loss: 2.9144198528374874
Validation loss: 2.5355678684111966

Epoch: 6| Step: 5
Training loss: 3.3394328260546646
Validation loss: 2.5334859968660113

Epoch: 6| Step: 6
Training loss: 2.6007373864556387
Validation loss: 2.5329887336163104

Epoch: 6| Step: 7
Training loss: 2.553490968059688
Validation loss: 2.5302050383346453

Epoch: 6| Step: 8
Training loss: 2.687534509481607
Validation loss: 2.5204966842448235

Epoch: 6| Step: 9
Training loss: 2.696803733617823
Validation loss: 2.5247974480613404

Epoch: 6| Step: 10
Training loss: 3.1349373553525535
Validation loss: 2.524811408546637

Epoch: 6| Step: 11
Training loss: 3.3929897798698523
Validation loss: 2.524748638048224

Epoch: 6| Step: 12
Training loss: 2.3850801277695783
Validation loss: 2.5040999029165967

Epoch: 6| Step: 13
Training loss: 2.606634837035103
Validation loss: 2.5133651879170595

Epoch: 19| Step: 0
Training loss: 2.3998792458991476
Validation loss: 2.5052283477717094

Epoch: 6| Step: 1
Training loss: 2.7450008035182645
Validation loss: 2.5004604797408803

Epoch: 6| Step: 2
Training loss: 2.788802789302182
Validation loss: 2.501866315171861

Epoch: 6| Step: 3
Training loss: 3.0936789456309355
Validation loss: 2.5060765056590024

Epoch: 6| Step: 4
Training loss: 2.241981204441584
Validation loss: 2.5070284253863466

Epoch: 6| Step: 5
Training loss: 2.913988864162639
Validation loss: 2.5069114843404656

Epoch: 6| Step: 6
Training loss: 3.156403490621164
Validation loss: 2.483599559756741

Epoch: 6| Step: 7
Training loss: 3.3899372299819612
Validation loss: 2.5121694780557315

Epoch: 6| Step: 8
Training loss: 2.5345428147830256
Validation loss: 2.4977778002862383

Epoch: 6| Step: 9
Training loss: 2.5777998285702397
Validation loss: 2.485895709543897

Epoch: 6| Step: 10
Training loss: 2.3358160389225837
Validation loss: 2.49981052131839

Epoch: 6| Step: 11
Training loss: 3.224166608665652
Validation loss: 2.4845970768114785

Epoch: 6| Step: 12
Training loss: 2.9795915705217086
Validation loss: 2.507311571389686

Epoch: 6| Step: 13
Training loss: 3.1150332394903724
Validation loss: 2.5003612272722107

Epoch: 20| Step: 0
Training loss: 2.91620416153104
Validation loss: 2.515647401469828

Epoch: 6| Step: 1
Training loss: 2.506212145740661
Validation loss: 2.498175538785579

Epoch: 6| Step: 2
Training loss: 3.1329752054557485
Validation loss: 2.4881611549379294

Epoch: 6| Step: 3
Training loss: 2.9773002771483457
Validation loss: 2.4872741057022405

Epoch: 6| Step: 4
Training loss: 2.683435427064185
Validation loss: 2.489223848835321

Epoch: 6| Step: 5
Training loss: 2.647386910794177
Validation loss: 2.4945104759387244

Epoch: 6| Step: 6
Training loss: 2.8886411715533593
Validation loss: 2.487272270019697

Epoch: 6| Step: 7
Training loss: 2.998080593255015
Validation loss: 2.506609894922994

Epoch: 6| Step: 8
Training loss: 2.9487449821245124
Validation loss: 2.503483283737034

Epoch: 6| Step: 9
Training loss: 3.339763511006167
Validation loss: 2.495598869096631

Epoch: 6| Step: 10
Training loss: 2.6400142166448486
Validation loss: 2.489094396864108

Epoch: 6| Step: 11
Training loss: 2.444160360628827
Validation loss: 2.4932554404251994

Epoch: 6| Step: 12
Training loss: 2.815018098817036
Validation loss: 2.493114115941813

Epoch: 6| Step: 13
Training loss: 2.642516141786239
Validation loss: 2.5074251259284166

Epoch: 21| Step: 0
Training loss: 3.0092684306034845
Validation loss: 2.4788248335406675

Epoch: 6| Step: 1
Training loss: 2.591895486066782
Validation loss: 2.501154285492498

Epoch: 6| Step: 2
Training loss: 2.7597751279803657
Validation loss: 2.4973351704112683

Epoch: 6| Step: 3
Training loss: 3.151692792386429
Validation loss: 2.477190686386281

Epoch: 6| Step: 4
Training loss: 2.9175838844772612
Validation loss: 2.5017831698262056

Epoch: 6| Step: 5
Training loss: 2.721111981629007
Validation loss: 2.4876592114271694

Epoch: 6| Step: 6
Training loss: 2.994794461898605
Validation loss: 2.4808060136184906

Epoch: 6| Step: 7
Training loss: 2.6425201116426327
Validation loss: 2.504337104977388

Epoch: 6| Step: 8
Training loss: 2.996236188504212
Validation loss: 2.484104221452421

Epoch: 6| Step: 9
Training loss: 2.91562248794104
Validation loss: 2.4894636989302064

Epoch: 6| Step: 10
Training loss: 2.250405381029668
Validation loss: 2.4990291710080816

Epoch: 6| Step: 11
Training loss: 3.1257257763155013
Validation loss: 2.478672879780432

Epoch: 6| Step: 12
Training loss: 2.6756785129398777
Validation loss: 2.49004843641598

Epoch: 6| Step: 13
Training loss: 2.6827740479314763
Validation loss: 2.5044081299826018

Epoch: 22| Step: 0
Training loss: 2.1489118572564974
Validation loss: 2.4929518405617217

Epoch: 6| Step: 1
Training loss: 2.778476232031966
Validation loss: 2.4954062176832754

Epoch: 6| Step: 2
Training loss: 2.984802854535679
Validation loss: 2.4935534289324104

Epoch: 6| Step: 3
Training loss: 3.019953492166499
Validation loss: 2.4821460053231785

Epoch: 6| Step: 4
Training loss: 2.7584480223385324
Validation loss: 2.4958077381974637

Epoch: 6| Step: 5
Training loss: 3.0091614707860366
Validation loss: 2.490072954878896

Epoch: 6| Step: 6
Training loss: 3.2048702788620784
Validation loss: 2.4994310541740057

Epoch: 6| Step: 7
Training loss: 2.485529793154999
Validation loss: 2.5128970046439427

Epoch: 6| Step: 8
Training loss: 2.438250401725568
Validation loss: 2.486550716085447

Epoch: 6| Step: 9
Training loss: 2.9556859129487045
Validation loss: 2.4930202694799117

Epoch: 6| Step: 10
Training loss: 3.3304830444802316
Validation loss: 2.4952567171863573

Epoch: 6| Step: 11
Training loss: 2.8070347561643145
Validation loss: 2.4939962674177543

Epoch: 6| Step: 12
Training loss: 2.727412837215322
Validation loss: 2.489634136749613

Epoch: 6| Step: 13
Training loss: 2.561945227139486
Validation loss: 2.511796136147902

Epoch: 23| Step: 0
Training loss: 2.852555945848273
Validation loss: 2.4883910028868215

Epoch: 6| Step: 1
Training loss: 3.2186840754303923
Validation loss: 2.4954121156582785

Epoch: 6| Step: 2
Training loss: 3.511377371042611
Validation loss: 2.492398680264261

Epoch: 6| Step: 3
Training loss: 2.9730129754751746
Validation loss: 2.5010792679113205

Epoch: 6| Step: 4
Training loss: 3.290007901631522
Validation loss: 2.4954935866253076

Epoch: 6| Step: 5
Training loss: 2.305796004272249
Validation loss: 2.5069080667116834

Epoch: 6| Step: 6
Training loss: 3.307632612941961
Validation loss: 2.4960835646149144

Epoch: 6| Step: 7
Training loss: 2.7259649370643504
Validation loss: 2.504928343655182

Epoch: 6| Step: 8
Training loss: 2.7211510590074743
Validation loss: 2.5028271502088697

Epoch: 6| Step: 9
Training loss: 2.4732355822176957
Validation loss: 2.510894059750506

Epoch: 6| Step: 10
Training loss: 2.3174083272668295
Validation loss: 2.4949403065542373

Epoch: 6| Step: 11
Training loss: 2.2535440296030105
Validation loss: 2.487049038448883

Epoch: 6| Step: 12
Training loss: 2.3897116917328107
Validation loss: 2.5113098508813323

Epoch: 6| Step: 13
Training loss: 2.802280083500191
Validation loss: 2.4917862593230677

Epoch: 24| Step: 0
Training loss: 3.0416051431643503
Validation loss: 2.495378917921512

Epoch: 6| Step: 1
Training loss: 2.6344504312777683
Validation loss: 2.5113045088301673

Epoch: 6| Step: 2
Training loss: 3.0014574960892046
Validation loss: 2.4896794700175655

Epoch: 6| Step: 3
Training loss: 2.838735330023879
Validation loss: 2.495763086338423

Epoch: 6| Step: 4
Training loss: 3.001616995861356
Validation loss: 2.4845820226079076

Epoch: 6| Step: 5
Training loss: 2.8878186599142315
Validation loss: 2.503375739901273

Epoch: 6| Step: 6
Training loss: 3.043986511441195
Validation loss: 2.4886916345953733

Epoch: 6| Step: 7
Training loss: 3.48455290062245
Validation loss: 2.4918213825245497

Epoch: 6| Step: 8
Training loss: 2.8373419696645352
Validation loss: 2.495534220436866

Epoch: 6| Step: 9
Training loss: 3.071576118093635
Validation loss: 2.4788010800678073

Epoch: 6| Step: 10
Training loss: 2.0442772608046513
Validation loss: 2.494805033442427

Epoch: 6| Step: 11
Training loss: 2.3177708535209325
Validation loss: 2.483884007915503

Epoch: 6| Step: 12
Training loss: 2.281388265521742
Validation loss: 2.4894920408240724

Epoch: 6| Step: 13
Training loss: 2.4412441352426115
Validation loss: 2.493450443861652

Epoch: 25| Step: 0
Training loss: 3.3221314238859967
Validation loss: 2.4881424347027723

Epoch: 6| Step: 1
Training loss: 2.063708673683775
Validation loss: 2.4858858582755596

Epoch: 6| Step: 2
Training loss: 2.7566818210442037
Validation loss: 2.4943807351139626

Epoch: 6| Step: 3
Training loss: 2.8560531581772044
Validation loss: 2.485113761445208

Epoch: 6| Step: 4
Training loss: 3.32912891988796
Validation loss: 2.4799900571330724

Epoch: 6| Step: 5
Training loss: 3.09155488407651
Validation loss: 2.4924975509105733

Epoch: 6| Step: 6
Training loss: 2.1738985363849395
Validation loss: 2.492116807707862

Epoch: 6| Step: 7
Training loss: 3.190346736898004
Validation loss: 2.4759821580754275

Epoch: 6| Step: 8
Training loss: 2.8633590208891593
Validation loss: 2.4896024220210595

Epoch: 6| Step: 9
Training loss: 3.0861414588453178
Validation loss: 2.4900248479231384

Epoch: 6| Step: 10
Training loss: 2.8435434119672216
Validation loss: 2.494228378485255

Epoch: 6| Step: 11
Training loss: 2.2623323915898434
Validation loss: 2.4843687229099123

Epoch: 6| Step: 12
Training loss: 2.2241667055044485
Validation loss: 2.4770807073635748

Epoch: 6| Step: 13
Training loss: 3.0201317509591954
Validation loss: 2.4878492054476253

Epoch: 26| Step: 0
Training loss: 2.8871118590166556
Validation loss: 2.494106173800352

Epoch: 6| Step: 1
Training loss: 3.4024070970351383
Validation loss: 2.476543595819722

Epoch: 6| Step: 2
Training loss: 2.14104518803852
Validation loss: 2.49596226937262

Epoch: 6| Step: 3
Training loss: 3.093117234996299
Validation loss: 2.480335544468271

Epoch: 6| Step: 4
Training loss: 2.690123320096207
Validation loss: 2.4811382159931115

Epoch: 6| Step: 5
Training loss: 2.1513999905295362
Validation loss: 2.4730407439749165

Epoch: 6| Step: 6
Training loss: 3.024134671366188
Validation loss: 2.4759821259779184

Epoch: 6| Step: 7
Training loss: 2.658500256773524
Validation loss: 2.472810615843248

Epoch: 6| Step: 8
Training loss: 2.8569841238614857
Validation loss: 2.4956790499278365

Epoch: 6| Step: 9
Training loss: 3.1258630705147517
Validation loss: 2.4963310771172904

Epoch: 6| Step: 10
Training loss: 3.3073040498917203
Validation loss: 2.5083619851941927

Epoch: 6| Step: 11
Training loss: 3.1756039202778243
Validation loss: 2.482372013863247

Epoch: 6| Step: 12
Training loss: 2.062703209316387
Validation loss: 2.4875587551255935

Epoch: 6| Step: 13
Training loss: 2.260730586835787
Validation loss: 2.489102858911865

Epoch: 27| Step: 0
Training loss: 2.4641147025026746
Validation loss: 2.4933854634319785

Epoch: 6| Step: 1
Training loss: 3.391287180847555
Validation loss: 2.4845007737339424

Epoch: 6| Step: 2
Training loss: 1.9740271912762988
Validation loss: 2.5007217964957955

Epoch: 6| Step: 3
Training loss: 3.0207723852564317
Validation loss: 2.4894585169895698

Epoch: 6| Step: 4
Training loss: 2.808672271182667
Validation loss: 2.4838802799326767

Epoch: 6| Step: 5
Training loss: 2.5354593386751345
Validation loss: 2.482962741138274

Epoch: 6| Step: 6
Training loss: 2.483035224157716
Validation loss: 2.4844687024906102

Epoch: 6| Step: 7
Training loss: 2.613259901293123
Validation loss: 2.505376837801349

Epoch: 6| Step: 8
Training loss: 3.089473504454926
Validation loss: 2.5021129672144746

Epoch: 6| Step: 9
Training loss: 3.5647242779998596
Validation loss: 2.4771111117715345

Epoch: 6| Step: 10
Training loss: 1.8444060112244056
Validation loss: 2.4809467491053745

Epoch: 6| Step: 11
Training loss: 2.652802957101285
Validation loss: 2.4889490049971057

Epoch: 6| Step: 12
Training loss: 3.5310131432421805
Validation loss: 2.4807882413100817

Epoch: 6| Step: 13
Training loss: 2.4686158542161345
Validation loss: 2.4822873722879755

Epoch: 28| Step: 0
Training loss: 2.616786732520882
Validation loss: 2.485609931102965

Epoch: 6| Step: 1
Training loss: 2.4177199622426118
Validation loss: 2.4886441179891117

Epoch: 6| Step: 2
Training loss: 2.2128844837665778
Validation loss: 2.480450286938631

Epoch: 6| Step: 3
Training loss: 3.1798305619881857
Validation loss: 2.4803046922445673

Epoch: 6| Step: 4
Training loss: 3.3308658686132024
Validation loss: 2.497777293260112

Epoch: 6| Step: 5
Training loss: 2.434138988566927
Validation loss: 2.4736293323103506

Epoch: 6| Step: 6
Training loss: 2.9131604145797083
Validation loss: 2.492276617305784

Epoch: 6| Step: 7
Training loss: 2.7577719536845415
Validation loss: 2.5005286836997622

Epoch: 6| Step: 8
Training loss: 2.377558083451495
Validation loss: 2.485910816121217

Epoch: 6| Step: 9
Training loss: 2.82164489889414
Validation loss: 2.4848015851669425

Epoch: 6| Step: 10
Training loss: 2.612495794703553
Validation loss: 2.4773197296790377

Epoch: 6| Step: 11
Training loss: 3.3020046156556964
Validation loss: 2.4800125675399016

Epoch: 6| Step: 12
Training loss: 3.0122329530235956
Validation loss: 2.4789119626502347

Epoch: 6| Step: 13
Training loss: 3.211358035873657
Validation loss: 2.493543685539914

Epoch: 29| Step: 0
Training loss: 2.9228398163186964
Validation loss: 2.498365259470649

Epoch: 6| Step: 1
Training loss: 2.3867461207641902
Validation loss: 2.49209872823925

Epoch: 6| Step: 2
Training loss: 3.4864445172710377
Validation loss: 2.4904958805065696

Epoch: 6| Step: 3
Training loss: 2.1829084982818996
Validation loss: 2.4918970954710655

Epoch: 6| Step: 4
Training loss: 3.6565783752613195
Validation loss: 2.4877859856387174

Epoch: 6| Step: 5
Training loss: 3.1012287104412875
Validation loss: 2.491553839464147

Epoch: 6| Step: 6
Training loss: 2.1623314499630975
Validation loss: 2.4761711482437265

Epoch: 6| Step: 7
Training loss: 3.2272381017490575
Validation loss: 2.5008314944400807

Epoch: 6| Step: 8
Training loss: 2.915240584009953
Validation loss: 2.485549696507358

Epoch: 6| Step: 9
Training loss: 1.9369864244590922
Validation loss: 2.5008935131469117

Epoch: 6| Step: 10
Training loss: 2.2930813555386336
Validation loss: 2.4807212659069933

Epoch: 6| Step: 11
Training loss: 2.786342116930031
Validation loss: 2.4983941991694443

Epoch: 6| Step: 12
Training loss: 2.756021150183944
Validation loss: 2.490467319455136

Epoch: 6| Step: 13
Training loss: 3.074741731047745
Validation loss: 2.474641834046833

Epoch: 30| Step: 0
Training loss: 2.0158792971096102
Validation loss: 2.494444824773731

Epoch: 6| Step: 1
Training loss: 3.37120308814756
Validation loss: 2.4832051317227113

Epoch: 6| Step: 2
Training loss: 2.209029705685414
Validation loss: 2.4917741817822106

Epoch: 6| Step: 3
Training loss: 3.2890970042541396
Validation loss: 2.4618175802181064

Epoch: 6| Step: 4
Training loss: 2.3008492270306076
Validation loss: 2.4820622205507075

Epoch: 6| Step: 5
Training loss: 2.56470933899541
Validation loss: 2.4905409479566485

Epoch: 6| Step: 6
Training loss: 2.6478684975969644
Validation loss: 2.4874691111695193

Epoch: 6| Step: 7
Training loss: 2.355043388663528
Validation loss: 2.4904735955853776

Epoch: 6| Step: 8
Training loss: 2.872669270361181
Validation loss: 2.4778721477509973

Epoch: 6| Step: 9
Training loss: 3.1248739598605653
Validation loss: 2.4714901827261797

Epoch: 6| Step: 10
Training loss: 2.6870269359037806
Validation loss: 2.487524697151333

Epoch: 6| Step: 11
Training loss: 2.6583716502362926
Validation loss: 2.4748304987482093

Epoch: 6| Step: 12
Training loss: 3.2686782508109022
Validation loss: 2.48550437146259

Epoch: 6| Step: 13
Training loss: 3.929191542890865
Validation loss: 2.4796398015039283

Epoch: 31| Step: 0
Training loss: 2.9817301429515286
Validation loss: 2.482178816123695

Epoch: 6| Step: 1
Training loss: 2.8410843313094922
Validation loss: 2.488622986730029

Epoch: 6| Step: 2
Training loss: 2.2882647377322254
Validation loss: 2.4920246139058335

Epoch: 6| Step: 3
Training loss: 2.5700869011937124
Validation loss: 2.47602507052004

Epoch: 6| Step: 4
Training loss: 2.5358393943364015
Validation loss: 2.4782270879665984

Epoch: 6| Step: 5
Training loss: 3.5605092425928846
Validation loss: 2.4895227977264573

Epoch: 6| Step: 6
Training loss: 2.1636878835840347
Validation loss: 2.489958086991665

Epoch: 6| Step: 7
Training loss: 3.082008661494378
Validation loss: 2.4894639023149776

Epoch: 6| Step: 8
Training loss: 2.999679548314807
Validation loss: 2.4852230172309855

Epoch: 6| Step: 9
Training loss: 2.86803222186983
Validation loss: 2.4909912441628013

Epoch: 6| Step: 10
Training loss: 3.4979530888460437
Validation loss: 2.484133514082795

Epoch: 6| Step: 11
Training loss: 2.3414267660317067
Validation loss: 2.4932240735503584

Epoch: 6| Step: 12
Training loss: 2.4236866727585724
Validation loss: 2.4893493065232293

Epoch: 6| Step: 13
Training loss: 2.735524660657875
Validation loss: 2.4828445705816353

Epoch: 32| Step: 0
Training loss: 2.8519082891798218
Validation loss: 2.496799115967067

Epoch: 6| Step: 1
Training loss: 3.063835475667786
Validation loss: 2.492449458866682

Epoch: 6| Step: 2
Training loss: 2.333626104334266
Validation loss: 2.491770793804916

Epoch: 6| Step: 3
Training loss: 2.9961223973940077
Validation loss: 2.4942271265890352

Epoch: 6| Step: 4
Training loss: 3.040087529478329
Validation loss: 2.482828097872454

Epoch: 6| Step: 5
Training loss: 2.9309743267641206
Validation loss: 2.4980211404213764

Epoch: 6| Step: 6
Training loss: 3.161308993101706
Validation loss: 2.49732616036139

Epoch: 6| Step: 7
Training loss: 2.596781510232146
Validation loss: 2.4844414569316573

Epoch: 6| Step: 8
Training loss: 2.4004629324442948
Validation loss: 2.487143520462399

Epoch: 6| Step: 9
Training loss: 2.2195666247579733
Validation loss: 2.4854095537962206

Epoch: 6| Step: 10
Training loss: 3.0328182607573444
Validation loss: 2.475689405273454

Epoch: 6| Step: 11
Training loss: 3.1123229194731907
Validation loss: 2.471142599221165

Epoch: 6| Step: 12
Training loss: 2.733031722229794
Validation loss: 2.467787229202858

Epoch: 6| Step: 13
Training loss: 2.09328660534177
Validation loss: 2.4907732626553845

Epoch: 33| Step: 0
Training loss: 2.8129425548352347
Validation loss: 2.4826427056293525

Epoch: 6| Step: 1
Training loss: 2.4736133428591267
Validation loss: 2.4883206467837065

Epoch: 6| Step: 2
Training loss: 2.369157531627639
Validation loss: 2.4729312718574508

Epoch: 6| Step: 3
Training loss: 2.967738250192438
Validation loss: 2.4842097678341193

Epoch: 6| Step: 4
Training loss: 2.77579417506182
Validation loss: 2.481780147084157

Epoch: 6| Step: 5
Training loss: 2.5845227938440423
Validation loss: 2.4843817692919377

Epoch: 6| Step: 6
Training loss: 3.1488707743611024
Validation loss: 2.4900384762281953

Epoch: 6| Step: 7
Training loss: 2.5369982970415834
Validation loss: 2.489686183698828

Epoch: 6| Step: 8
Training loss: 2.7348308946121778
Validation loss: 2.4793576013368654

Epoch: 6| Step: 9
Training loss: 3.4912968466265726
Validation loss: 2.4874134270648875

Epoch: 6| Step: 10
Training loss: 1.8895111586116835
Validation loss: 2.473941771231082

Epoch: 6| Step: 11
Training loss: 2.989861841996803
Validation loss: 2.4998275656350466

Epoch: 6| Step: 12
Training loss: 3.033091978455
Validation loss: 2.4964908895855973

Epoch: 6| Step: 13
Training loss: 3.1488244360847855
Validation loss: 2.4768307815730797

Epoch: 34| Step: 0
Training loss: 2.7688882831976267
Validation loss: 2.4685753884799406

Epoch: 6| Step: 1
Training loss: 3.066138772171743
Validation loss: 2.4758366236923

Epoch: 6| Step: 2
Training loss: 3.2629243187458825
Validation loss: 2.4861133705893192

Epoch: 6| Step: 3
Training loss: 2.2642450800030716
Validation loss: 2.4914705367280816

Epoch: 6| Step: 4
Training loss: 2.0786783729274165
Validation loss: 2.4927851087438104

Epoch: 6| Step: 5
Training loss: 2.90412858220163
Validation loss: 2.4917119021426273

Epoch: 6| Step: 6
Training loss: 2.7512653214176654
Validation loss: 2.4946978379242415

Epoch: 6| Step: 7
Training loss: 2.2838291585865944
Validation loss: 2.4852016387074074

Epoch: 6| Step: 8
Training loss: 3.515333104201662
Validation loss: 2.4889798123847644

Epoch: 6| Step: 9
Training loss: 1.9732406505951403
Validation loss: 2.4976626370247823

Epoch: 6| Step: 10
Training loss: 2.4802595876380185
Validation loss: 2.4832809535490896

Epoch: 6| Step: 11
Training loss: 3.2308187053022275
Validation loss: 2.4852723997878643

Epoch: 6| Step: 12
Training loss: 3.4044984286200037
Validation loss: 2.4961397141529207

Epoch: 6| Step: 13
Training loss: 2.266793732260498
Validation loss: 2.4835164072502303

Epoch: 35| Step: 0
Training loss: 3.6421715067115876
Validation loss: 2.473738668865457

Epoch: 6| Step: 1
Training loss: 3.4407018834808647
Validation loss: 2.473493712512542

Epoch: 6| Step: 2
Training loss: 2.5858295337461814
Validation loss: 2.481480463261032

Epoch: 6| Step: 3
Training loss: 2.3689198214165654
Validation loss: 2.489163942178345

Epoch: 6| Step: 4
Training loss: 2.841743012537235
Validation loss: 2.481763590352

Epoch: 6| Step: 5
Training loss: 2.5566188033231256
Validation loss: 2.474884467786028

Epoch: 6| Step: 6
Training loss: 3.0432290197282414
Validation loss: 2.470621230852219

Epoch: 6| Step: 7
Training loss: 3.2176523651824263
Validation loss: 2.491134886734643

Epoch: 6| Step: 8
Training loss: 2.7761533522668977
Validation loss: 2.4834522420220946

Epoch: 6| Step: 9
Training loss: 2.3320155055346863
Validation loss: 2.4732782691894997

Epoch: 6| Step: 10
Training loss: 2.3745105640644684
Validation loss: 2.491767746370447

Epoch: 6| Step: 11
Training loss: 3.046155952313576
Validation loss: 2.4829203863821796

Epoch: 6| Step: 12
Training loss: 2.3071907543793397
Validation loss: 2.4796338163872425

Epoch: 6| Step: 13
Training loss: 1.3123221958160307
Validation loss: 2.4733589269178853

Epoch: 36| Step: 0
Training loss: 2.481368639053048
Validation loss: 2.4781705579154703

Epoch: 6| Step: 1
Training loss: 3.2326375376508114
Validation loss: 2.476070554072796

Epoch: 6| Step: 2
Training loss: 2.6313959946530794
Validation loss: 2.4714929709477107

Epoch: 6| Step: 3
Training loss: 2.769765566879477
Validation loss: 2.473102482076039

Epoch: 6| Step: 4
Training loss: 2.943905279190667
Validation loss: 2.4885598360265755

Epoch: 6| Step: 5
Training loss: 3.1371233953467366
Validation loss: 2.4679165585070164

Epoch: 6| Step: 6
Training loss: 2.8478829669051495
Validation loss: 2.4888516394030757

Epoch: 6| Step: 7
Training loss: 1.6657072007613503
Validation loss: 2.480724367215005

Epoch: 6| Step: 8
Training loss: 2.5099690989381744
Validation loss: 2.478321648213426

Epoch: 6| Step: 9
Training loss: 2.669413274670587
Validation loss: 2.475722601941949

Epoch: 6| Step: 10
Training loss: 3.238714725209533
Validation loss: 2.4902041843553886

Epoch: 6| Step: 11
Training loss: 2.965619494610933
Validation loss: 2.487173740013029

Epoch: 6| Step: 12
Training loss: 2.9872831385603136
Validation loss: 2.4849050653817004

Epoch: 6| Step: 13
Training loss: 2.5424063393174703
Validation loss: 2.489392458658839

Epoch: 37| Step: 0
Training loss: 3.0071635549432956
Validation loss: 2.4787368243221954

Epoch: 6| Step: 1
Training loss: 2.035412323815962
Validation loss: 2.4994133445532682

Epoch: 6| Step: 2
Training loss: 3.309820278855079
Validation loss: 2.488011949648908

Epoch: 6| Step: 3
Training loss: 2.318759179932553
Validation loss: 2.472106836267182

Epoch: 6| Step: 4
Training loss: 2.788071058136741
Validation loss: 2.4804823770650177

Epoch: 6| Step: 5
Training loss: 2.931269104332067
Validation loss: 2.488830357457301

Epoch: 6| Step: 6
Training loss: 2.560549761481735
Validation loss: 2.4816837904774784

Epoch: 6| Step: 7
Training loss: 3.0202061144489596
Validation loss: 2.4920997137408336

Epoch: 6| Step: 8
Training loss: 1.9278560086017145
Validation loss: 2.482595535872377

Epoch: 6| Step: 9
Training loss: 2.8390687412325626
Validation loss: 2.488303087840262

Epoch: 6| Step: 10
Training loss: 2.7995924721195995
Validation loss: 2.4761165891479173

Epoch: 6| Step: 11
Training loss: 2.9079515951711437
Validation loss: 2.4810947446407297

Epoch: 6| Step: 12
Training loss: 3.0259992604175983
Validation loss: 2.4652643796017584

Epoch: 6| Step: 13
Training loss: 3.4727422062091122
Validation loss: 2.4811199160022253

Epoch: 38| Step: 0
Training loss: 2.890462509949461
Validation loss: 2.497642880493464

Epoch: 6| Step: 1
Training loss: 3.1914173065330202
Validation loss: 2.4888680974811677

Epoch: 6| Step: 2
Training loss: 2.850576402701166
Validation loss: 2.4942128160755366

Epoch: 6| Step: 3
Training loss: 2.92359229228008
Validation loss: 2.4874219690373933

Epoch: 6| Step: 4
Training loss: 2.7983406531499657
Validation loss: 2.4938162281125136

Epoch: 6| Step: 5
Training loss: 2.236665631941638
Validation loss: 2.4912715409943256

Epoch: 6| Step: 6
Training loss: 2.607822710294946
Validation loss: 2.4920638897877727

Epoch: 6| Step: 7
Training loss: 2.929768228054429
Validation loss: 2.4870190884856886

Epoch: 6| Step: 8
Training loss: 2.8396246194578523
Validation loss: 2.501350276088779

Epoch: 6| Step: 9
Training loss: 2.4996061014761146
Validation loss: 2.4874831925193956

Epoch: 6| Step: 10
Training loss: 3.376850044967394
Validation loss: 2.4789078952178114

Epoch: 6| Step: 11
Training loss: 2.5769283407400616
Validation loss: 2.4772435463619025

Epoch: 6| Step: 12
Training loss: 2.208519465921403
Validation loss: 2.4878588454256545

Epoch: 6| Step: 13
Training loss: 2.878932916525497
Validation loss: 2.4901301587966396

Epoch: 39| Step: 0
Training loss: 2.769305608728461
Validation loss: 2.47963262742643

Epoch: 6| Step: 1
Training loss: 2.372113431222191
Validation loss: 2.4887431501545825

Epoch: 6| Step: 2
Training loss: 2.6735374231353624
Validation loss: 2.490171092204693

Epoch: 6| Step: 3
Training loss: 2.920815505237742
Validation loss: 2.4917569403970496

Epoch: 6| Step: 4
Training loss: 3.1044871188323673
Validation loss: 2.4999788385951773

Epoch: 6| Step: 5
Training loss: 2.6732679160557744
Validation loss: 2.4790432524336716

Epoch: 6| Step: 6
Training loss: 2.521680852325267
Validation loss: 2.4806731026436273

Epoch: 6| Step: 7
Training loss: 3.131650637886263
Validation loss: 2.4916603913740727

Epoch: 6| Step: 8
Training loss: 3.2133661074833437
Validation loss: 2.477097547920483

Epoch: 6| Step: 9
Training loss: 2.768127429967121
Validation loss: 2.4667453053945025

Epoch: 6| Step: 10
Training loss: 2.3974518878711586
Validation loss: 2.4847277742131078

Epoch: 6| Step: 11
Training loss: 2.722378906835717
Validation loss: 2.4783601890791718

Epoch: 6| Step: 12
Training loss: 2.845058265987752
Validation loss: 2.4734395260467936

Epoch: 6| Step: 13
Training loss: 2.7072230679807543
Validation loss: 2.4795560860990107

Epoch: 40| Step: 0
Training loss: 2.1873790162553894
Validation loss: 2.4811087908726304

Epoch: 6| Step: 1
Training loss: 3.2328351911925677
Validation loss: 2.4754588005070812

Epoch: 6| Step: 2
Training loss: 3.2365579655862198
Validation loss: 2.480861479184319

Epoch: 6| Step: 3
Training loss: 3.0435666634177685
Validation loss: 2.478784461550908

Epoch: 6| Step: 4
Training loss: 2.574734857482267
Validation loss: 2.483694234922917

Epoch: 6| Step: 5
Training loss: 3.547359215037673
Validation loss: 2.493787071833904

Epoch: 6| Step: 6
Training loss: 2.593825189810487
Validation loss: 2.4684591977474395

Epoch: 6| Step: 7
Training loss: 2.742933818406748
Validation loss: 2.4876444055669262

Epoch: 6| Step: 8
Training loss: 2.1821120255731326
Validation loss: 2.479292976909897

Epoch: 6| Step: 9
Training loss: 2.7076900500716623
Validation loss: 2.477970020280386

Epoch: 6| Step: 10
Training loss: 2.205926615327049
Validation loss: 2.4868666777350543

Epoch: 6| Step: 11
Training loss: 2.663703603425474
Validation loss: 2.4899002562032004

Epoch: 6| Step: 12
Training loss: 3.009593565105432
Validation loss: 2.48707784285039

Epoch: 6| Step: 13
Training loss: 2.4015675352958357
Validation loss: 2.480949471415941

Epoch: 41| Step: 0
Training loss: 2.9810169760685508
Validation loss: 2.4915943842180686

Epoch: 6| Step: 1
Training loss: 3.0209210788654985
Validation loss: 2.4804087042393963

Epoch: 6| Step: 2
Training loss: 3.1592064032390477
Validation loss: 2.477187968741637

Epoch: 6| Step: 3
Training loss: 2.9421272356746297
Validation loss: 2.481815649477427

Epoch: 6| Step: 4
Training loss: 2.1131756486438804
Validation loss: 2.479712695943426

Epoch: 6| Step: 5
Training loss: 2.3333460489562383
Validation loss: 2.471286861241803

Epoch: 6| Step: 6
Training loss: 2.601844841010062
Validation loss: 2.478408764498107

Epoch: 6| Step: 7
Training loss: 3.137270678160307
Validation loss: 2.482629432223258

Epoch: 6| Step: 8
Training loss: 3.0368372755421977
Validation loss: 2.4757597814647583

Epoch: 6| Step: 9
Training loss: 3.2861553038327638
Validation loss: 2.4732679297266458

Epoch: 6| Step: 10
Training loss: 2.8289372242030124
Validation loss: 2.4642272160465923

Epoch: 6| Step: 11
Training loss: 2.8048313284630737
Validation loss: 2.4678312310345527

Epoch: 6| Step: 12
Training loss: 1.3780173659732184
Validation loss: 2.48695597642909

Epoch: 6| Step: 13
Training loss: 2.678522596368261
Validation loss: 2.4766409515307655

Epoch: 42| Step: 0
Training loss: 3.329553718331758
Validation loss: 2.484326118147775

Epoch: 6| Step: 1
Training loss: 2.016248503067776
Validation loss: 2.477699575407621

Epoch: 6| Step: 2
Training loss: 3.3410725078827137
Validation loss: 2.4878039253888264

Epoch: 6| Step: 3
Training loss: 2.4312137826609845
Validation loss: 2.4793219882631945

Epoch: 6| Step: 4
Training loss: 2.4162100217998024
Validation loss: 2.4671331271670742

Epoch: 6| Step: 5
Training loss: 2.055486951433511
Validation loss: 2.4784135206176385

Epoch: 6| Step: 6
Training loss: 3.1351026882650586
Validation loss: 2.481784510407209

Epoch: 6| Step: 7
Training loss: 2.6342952186748736
Validation loss: 2.477948395543226

Epoch: 6| Step: 8
Training loss: 2.591690256761677
Validation loss: 2.4802561002088686

Epoch: 6| Step: 9
Training loss: 3.4418090168945827
Validation loss: 2.4776193704375546

Epoch: 6| Step: 10
Training loss: 2.8185936082985528
Validation loss: 2.471021849644071

Epoch: 6| Step: 11
Training loss: 2.8642330071057662
Validation loss: 2.47451533542679

Epoch: 6| Step: 12
Training loss: 2.396851055319099
Validation loss: 2.4728574030927915

Epoch: 6| Step: 13
Training loss: 3.071885189066663
Validation loss: 2.465740453076092

Epoch: 43| Step: 0
Training loss: 2.745733419249312
Validation loss: 2.478557434947328

Epoch: 6| Step: 1
Training loss: 3.5370241741533124
Validation loss: 2.476718216427216

Epoch: 6| Step: 2
Training loss: 2.7577799073797764
Validation loss: 2.477838215290961

Epoch: 6| Step: 3
Training loss: 2.4571292536222726
Validation loss: 2.4797233083481838

Epoch: 6| Step: 4
Training loss: 2.2551181648911593
Validation loss: 2.491420868002266

Epoch: 6| Step: 5
Training loss: 2.9847918314208286
Validation loss: 2.4751052997289595

Epoch: 6| Step: 6
Training loss: 2.4346989897176665
Validation loss: 2.48548862549877

Epoch: 6| Step: 7
Training loss: 2.5810017574914728
Validation loss: 2.484728245726998

Epoch: 6| Step: 8
Training loss: 2.8933646750913384
Validation loss: 2.488814024797595

Epoch: 6| Step: 9
Training loss: 3.061515182654342
Validation loss: 2.4841391178689323

Epoch: 6| Step: 10
Training loss: 2.8882588367866835
Validation loss: 2.4799848553972144

Epoch: 6| Step: 11
Training loss: 2.9563011568252295
Validation loss: 2.4820067900810465

Epoch: 6| Step: 12
Training loss: 2.8619118365257843
Validation loss: 2.4886421648514503

Epoch: 6| Step: 13
Training loss: 1.638998624213376
Validation loss: 2.4764334327773443

Epoch: 44| Step: 0
Training loss: 3.0556036704542615
Validation loss: 2.479464354772821

Epoch: 6| Step: 1
Training loss: 2.76914581500673
Validation loss: 2.487199744529897

Epoch: 6| Step: 2
Training loss: 2.6126283020930208
Validation loss: 2.4807938247510766

Epoch: 6| Step: 3
Training loss: 3.268957892201851
Validation loss: 2.4829267440556233

Epoch: 6| Step: 4
Training loss: 2.4460179100597377
Validation loss: 2.47350530149717

Epoch: 6| Step: 5
Training loss: 2.5294965157093694
Validation loss: 2.4803277693037575

Epoch: 6| Step: 6
Training loss: 2.790203982864194
Validation loss: 2.4746679671446867

Epoch: 6| Step: 7
Training loss: 2.760732342405951
Validation loss: 2.481245687356027

Epoch: 6| Step: 8
Training loss: 2.2444076976768534
Validation loss: 2.4639340935192906

Epoch: 6| Step: 9
Training loss: 2.436193458366834
Validation loss: 2.475098074184584

Epoch: 6| Step: 10
Training loss: 1.9599133739012604
Validation loss: 2.4830245990638913

Epoch: 6| Step: 11
Training loss: 3.5316330693512943
Validation loss: 2.4861197525796643

Epoch: 6| Step: 12
Training loss: 3.0873575154527626
Validation loss: 2.4822955539062166

Epoch: 6| Step: 13
Training loss: 3.0824401352167126
Validation loss: 2.483471482783762

Epoch: 45| Step: 0
Training loss: 2.7428927913829866
Validation loss: 2.4839446983173685

Epoch: 6| Step: 1
Training loss: 3.495888065702322
Validation loss: 2.4804205652966247

Epoch: 6| Step: 2
Training loss: 2.3795754379366802
Validation loss: 2.473353494606715

Epoch: 6| Step: 3
Training loss: 2.884351862094442
Validation loss: 2.4826144745151155

Epoch: 6| Step: 4
Training loss: 2.8252973383161355
Validation loss: 2.4677654518651964

Epoch: 6| Step: 5
Training loss: 2.9981368955336833
Validation loss: 2.4702754719850675

Epoch: 6| Step: 6
Training loss: 2.833998003768487
Validation loss: 2.489941943968201

Epoch: 6| Step: 7
Training loss: 2.630284213239975
Validation loss: 2.4754837650952743

Epoch: 6| Step: 8
Training loss: 2.3741336045306856
Validation loss: 2.4871911770564923

Epoch: 6| Step: 9
Training loss: 3.0192872574347267
Validation loss: 2.4853397724247412

Epoch: 6| Step: 10
Training loss: 2.724363123112392
Validation loss: 2.4815059117128326

Epoch: 6| Step: 11
Training loss: 2.393655489258495
Validation loss: 2.4798634116116935

Epoch: 6| Step: 12
Training loss: 2.3600997316979617
Validation loss: 2.4695262142980954

Epoch: 6| Step: 13
Training loss: 3.058350691022824
Validation loss: 2.4702777457938434

Epoch: 46| Step: 0
Training loss: 2.3500892865689575
Validation loss: 2.477370716976473

Epoch: 6| Step: 1
Training loss: 3.668760626548441
Validation loss: 2.47908782488928

Epoch: 6| Step: 2
Training loss: 2.6234109701118173
Validation loss: 2.482333434591798

Epoch: 6| Step: 3
Training loss: 3.0485631716431634
Validation loss: 2.478623773770202

Epoch: 6| Step: 4
Training loss: 2.6436150230058195
Validation loss: 2.4857979845754072

Epoch: 6| Step: 5
Training loss: 2.5899965635298976
Validation loss: 2.475058012218549

Epoch: 6| Step: 6
Training loss: 3.1013414126373218
Validation loss: 2.4657745936056252

Epoch: 6| Step: 7
Training loss: 2.1985065940079607
Validation loss: 2.4788178629141258

Epoch: 6| Step: 8
Training loss: 2.7364633487287104
Validation loss: 2.479009953384615

Epoch: 6| Step: 9
Training loss: 2.4062871558553285
Validation loss: 2.4806905161404105

Epoch: 6| Step: 10
Training loss: 3.1950534258511185
Validation loss: 2.4863543806658575

Epoch: 6| Step: 11
Training loss: 3.024789433992905
Validation loss: 2.4698074576633213

Epoch: 6| Step: 12
Training loss: 2.2284727234518042
Validation loss: 2.4848228520956965

Epoch: 6| Step: 13
Training loss: 2.5250294863046183
Validation loss: 2.4776584990744643

Epoch: 47| Step: 0
Training loss: 2.858462328729491
Validation loss: 2.4826770061199284

Epoch: 6| Step: 1
Training loss: 2.7716280340123816
Validation loss: 2.483170073514218

Epoch: 6| Step: 2
Training loss: 2.984260596833737
Validation loss: 2.490768764807322

Epoch: 6| Step: 3
Training loss: 2.5450852547326233
Validation loss: 2.467811016488755

Epoch: 6| Step: 4
Training loss: 2.8228264169002584
Validation loss: 2.469255613942719

Epoch: 6| Step: 5
Training loss: 2.5368293214131388
Validation loss: 2.490947546469065

Epoch: 6| Step: 6
Training loss: 2.9418708262141426
Validation loss: 2.4870432865995764

Epoch: 6| Step: 7
Training loss: 3.069779755088462
Validation loss: 2.494483581574443

Epoch: 6| Step: 8
Training loss: 2.925761692608199
Validation loss: 2.461059056861042

Epoch: 6| Step: 9
Training loss: 2.599787292582761
Validation loss: 2.476831658259226

Epoch: 6| Step: 10
Training loss: 2.5930707110948665
Validation loss: 2.4590559905838845

Epoch: 6| Step: 11
Training loss: 2.7665108161906495
Validation loss: 2.48696798665573

Epoch: 6| Step: 12
Training loss: 2.682251707106527
Validation loss: 2.4731631317781906

Epoch: 6| Step: 13
Training loss: 2.4514624921095014
Validation loss: 2.4823014210662393

Epoch: 48| Step: 0
Training loss: 2.2291394763845043
Validation loss: 2.4799734129871545

Epoch: 6| Step: 1
Training loss: 2.856400406965401
Validation loss: 2.4781274131340347

Epoch: 6| Step: 2
Training loss: 3.0170378068547175
Validation loss: 2.470746523937898

Epoch: 6| Step: 3
Training loss: 2.6696749547203464
Validation loss: 2.4803303873844738

Epoch: 6| Step: 4
Training loss: 2.618612965933768
Validation loss: 2.479137424273439

Epoch: 6| Step: 5
Training loss: 2.6379046450626054
Validation loss: 2.4696329650269297

Epoch: 6| Step: 6
Training loss: 3.599481672483814
Validation loss: 2.4812986427871087

Epoch: 6| Step: 7
Training loss: 2.4220401830639524
Validation loss: 2.4784102984976393

Epoch: 6| Step: 8
Training loss: 2.817673312173454
Validation loss: 2.4738358458338148

Epoch: 6| Step: 9
Training loss: 2.0317945484015016
Validation loss: 2.47608484416073

Epoch: 6| Step: 10
Training loss: 3.1487385721820833
Validation loss: 2.479842918846626

Epoch: 6| Step: 11
Training loss: 3.3140255276255655
Validation loss: 2.4699153130676827

Epoch: 6| Step: 12
Training loss: 2.6675897331839886
Validation loss: 2.476396143022905

Epoch: 6| Step: 13
Training loss: 1.511254685412796
Validation loss: 2.4753248184086725

Epoch: 49| Step: 0
Training loss: 2.3834305758967744
Validation loss: 2.47641946717219

Epoch: 6| Step: 1
Training loss: 2.603334665269629
Validation loss: 2.47813476950377

Epoch: 6| Step: 2
Training loss: 2.7390577926638815
Validation loss: 2.485801922132785

Epoch: 6| Step: 3
Training loss: 2.7966660576032747
Validation loss: 2.47831603696261

Epoch: 6| Step: 4
Training loss: 2.9615223490670197
Validation loss: 2.4843893413835327

Epoch: 6| Step: 5
Training loss: 2.3478382312994994
Validation loss: 2.4798632451726252

Epoch: 6| Step: 6
Training loss: 1.902389494484977
Validation loss: 2.477223420005861

Epoch: 6| Step: 7
Training loss: 3.0811730798865833
Validation loss: 2.4768458301013228

Epoch: 6| Step: 8
Training loss: 3.472225338404635
Validation loss: 2.4712447208090444

Epoch: 6| Step: 9
Training loss: 2.0116348397245347
Validation loss: 2.4697532385887158

Epoch: 6| Step: 10
Training loss: 2.1496853620382956
Validation loss: 2.4686426347320247

Epoch: 6| Step: 11
Training loss: 3.9068451695027275
Validation loss: 2.474617219412262

Epoch: 6| Step: 12
Training loss: 3.0464625935338954
Validation loss: 2.472886205891079

Epoch: 6| Step: 13
Training loss: 2.1441736809143475
Validation loss: 2.490781528587932

Epoch: 50| Step: 0
Training loss: 2.88405857189662
Validation loss: 2.480267414180192

Epoch: 6| Step: 1
Training loss: 2.810780826292685
Validation loss: 2.474224990091924

Epoch: 6| Step: 2
Training loss: 2.802217804094113
Validation loss: 2.4828384052672936

Epoch: 6| Step: 3
Training loss: 2.891852700646478
Validation loss: 2.476804279007723

Epoch: 6| Step: 4
Training loss: 3.2531842991300763
Validation loss: 2.4708134325124522

Epoch: 6| Step: 5
Training loss: 2.982065475543293
Validation loss: 2.4839801986716483

Epoch: 6| Step: 6
Training loss: 3.0700481582492687
Validation loss: 2.4828915358144337

Epoch: 6| Step: 7
Training loss: 2.3896540246920788
Validation loss: 2.483449943113244

Epoch: 6| Step: 8
Training loss: 2.5868542478161736
Validation loss: 2.4869192021584268

Epoch: 6| Step: 9
Training loss: 3.018741558687085
Validation loss: 2.4784567664414356

Epoch: 6| Step: 10
Training loss: 2.274154392748409
Validation loss: 2.475117687511118

Epoch: 6| Step: 11
Training loss: 2.151803338250582
Validation loss: 2.4751301021610357

Epoch: 6| Step: 12
Training loss: 2.5633971341861392
Validation loss: 2.473510888940645

Epoch: 6| Step: 13
Training loss: 2.7286022175269373
Validation loss: 2.4617682494154165

Epoch: 51| Step: 0
Training loss: 2.3343263511235537
Validation loss: 2.474709419435905

Epoch: 6| Step: 1
Training loss: 3.709063183116472
Validation loss: 2.4844974914060263

Epoch: 6| Step: 2
Training loss: 2.264991619103023
Validation loss: 2.4858746554719864

Epoch: 6| Step: 3
Training loss: 3.4009228968462324
Validation loss: 2.4613860419992877

Epoch: 6| Step: 4
Training loss: 2.055039292305605
Validation loss: 2.4762072611339843

Epoch: 6| Step: 5
Training loss: 2.160444519785521
Validation loss: 2.4779157956521445

Epoch: 6| Step: 6
Training loss: 2.6006575376514514
Validation loss: 2.477950719730444

Epoch: 6| Step: 7
Training loss: 2.2674817073803357
Validation loss: 2.4790264633572354

Epoch: 6| Step: 8
Training loss: 2.9923873316716576
Validation loss: 2.4697818263913383

Epoch: 6| Step: 9
Training loss: 2.8065592436117988
Validation loss: 2.4810928196580715

Epoch: 6| Step: 10
Training loss: 2.7669631393177725
Validation loss: 2.486097952276503

Epoch: 6| Step: 11
Training loss: 3.169407678855789
Validation loss: 2.4591453160941166

Epoch: 6| Step: 12
Training loss: 2.8620426262347607
Validation loss: 2.4815991378252273

Epoch: 6| Step: 13
Training loss: 2.589876522817716
Validation loss: 2.48376499043699

Epoch: 52| Step: 0
Training loss: 2.045123683295063
Validation loss: 2.4723472509061843

Epoch: 6| Step: 1
Training loss: 3.518285804064882
Validation loss: 2.477018102789873

Epoch: 6| Step: 2
Training loss: 2.653653300496016
Validation loss: 2.4778793393408516

Epoch: 6| Step: 3
Training loss: 2.793836082822008
Validation loss: 2.4811023613784533

Epoch: 6| Step: 4
Training loss: 3.1149989502539173
Validation loss: 2.4786246162074304

Epoch: 6| Step: 5
Training loss: 2.4041491167914364
Validation loss: 2.472988265843618

Epoch: 6| Step: 6
Training loss: 2.839512612872517
Validation loss: 2.469264631940601

Epoch: 6| Step: 7
Training loss: 3.0978034081927386
Validation loss: 2.4777740390756495

Epoch: 6| Step: 8
Training loss: 2.616730242967209
Validation loss: 2.4866779189825827

Epoch: 6| Step: 9
Training loss: 2.5372599145220285
Validation loss: 2.4682342169108606

Epoch: 6| Step: 10
Training loss: 1.89515810948552
Validation loss: 2.4625393853672026

Epoch: 6| Step: 11
Training loss: 2.9113721112544906
Validation loss: 2.465755860404076

Epoch: 6| Step: 12
Training loss: 2.8902940715342944
Validation loss: 2.4824722989876715

Epoch: 6| Step: 13
Training loss: 3.085691649868607
Validation loss: 2.4836330991937143

Epoch: 53| Step: 0
Training loss: 2.4838033534301265
Validation loss: 2.469285182387452

Epoch: 6| Step: 1
Training loss: 2.6672407863413827
Validation loss: 2.473695900711474

Epoch: 6| Step: 2
Training loss: 2.8132328350262488
Validation loss: 2.46745203866186

Epoch: 6| Step: 3
Training loss: 2.257125380346644
Validation loss: 2.458958414057304

Epoch: 6| Step: 4
Training loss: 2.904310994732826
Validation loss: 2.4646964969738976

Epoch: 6| Step: 5
Training loss: 2.532683355202969
Validation loss: 2.470222836514865

Epoch: 6| Step: 6
Training loss: 3.3111239760175253
Validation loss: 2.465332174752941

Epoch: 6| Step: 7
Training loss: 2.613478124298913
Validation loss: 2.466629248274018

Epoch: 6| Step: 8
Training loss: 3.250253814176269
Validation loss: 2.470224804216174

Epoch: 6| Step: 9
Training loss: 2.9099209479117647
Validation loss: 2.4794448130525066

Epoch: 6| Step: 10
Training loss: 2.792136499145126
Validation loss: 2.4741704709734016

Epoch: 6| Step: 11
Training loss: 2.75606457698557
Validation loss: 2.4687425315518623

Epoch: 6| Step: 12
Training loss: 2.4631756005828414
Validation loss: 2.4695136105686015

Epoch: 6| Step: 13
Training loss: 2.556466419413497
Validation loss: 2.4782020310145136

Epoch: 54| Step: 0
Training loss: 2.9350203741818883
Validation loss: 2.477189033653012

Epoch: 6| Step: 1
Training loss: 2.4473116079578654
Validation loss: 2.4882750878534874

Epoch: 6| Step: 2
Training loss: 2.765200307533559
Validation loss: 2.472715011800725

Epoch: 6| Step: 3
Training loss: 2.4280717880722196
Validation loss: 2.464274495004439

Epoch: 6| Step: 4
Training loss: 2.589983307773604
Validation loss: 2.4750997034555344

Epoch: 6| Step: 5
Training loss: 3.4238817435993365
Validation loss: 2.4777876938938554

Epoch: 6| Step: 6
Training loss: 3.1896947149010324
Validation loss: 2.477951109250145

Epoch: 6| Step: 7
Training loss: 2.5403599637196645
Validation loss: 2.4767713835525664

Epoch: 6| Step: 8
Training loss: 3.0152680971503796
Validation loss: 2.4795265357089122

Epoch: 6| Step: 9
Training loss: 2.748224552269568
Validation loss: 2.481324910344508

Epoch: 6| Step: 10
Training loss: 2.6748668138378853
Validation loss: 2.4778373855192184

Epoch: 6| Step: 11
Training loss: 2.717442340537423
Validation loss: 2.484279397260229

Epoch: 6| Step: 12
Training loss: 2.439330856154481
Validation loss: 2.4753962931732616

Epoch: 6| Step: 13
Training loss: 2.228649245727047
Validation loss: 2.494049932738775

Epoch: 55| Step: 0
Training loss: 2.4033182292186726
Validation loss: 2.4782232407861002

Epoch: 6| Step: 1
Training loss: 2.887249599680376
Validation loss: 2.4767249574794707

Epoch: 6| Step: 2
Training loss: 2.3478269594301455
Validation loss: 2.471410804224968

Epoch: 6| Step: 3
Training loss: 2.500729835789721
Validation loss: 2.488634621151881

Epoch: 6| Step: 4
Training loss: 2.5674990761522154
Validation loss: 2.489974989744238

Epoch: 6| Step: 5
Training loss: 2.2125700729394393
Validation loss: 2.4865359551752095

Epoch: 6| Step: 6
Training loss: 1.8492062541247771
Validation loss: 2.4724781520638075

Epoch: 6| Step: 7
Training loss: 2.860167846822894
Validation loss: 2.4850198222734186

Epoch: 6| Step: 8
Training loss: 2.7938372775447102
Validation loss: 2.4716241299813566

Epoch: 6| Step: 9
Training loss: 2.825531333985864
Validation loss: 2.4716551699064766

Epoch: 6| Step: 10
Training loss: 3.080719759430419
Validation loss: 2.4782383253407763

Epoch: 6| Step: 11
Training loss: 3.463174411357159
Validation loss: 2.4845649397425054

Epoch: 6| Step: 12
Training loss: 3.2117823767756435
Validation loss: 2.4827160189055673

Epoch: 6| Step: 13
Training loss: 3.394790551715179
Validation loss: 2.4752751333065004

Epoch: 56| Step: 0
Training loss: 2.850946396413761
Validation loss: 2.4810949781597267

Epoch: 6| Step: 1
Training loss: 2.3753336621859815
Validation loss: 2.473053972438998

Epoch: 6| Step: 2
Training loss: 2.8917768503207126
Validation loss: 2.474026467249166

Epoch: 6| Step: 3
Training loss: 1.549498981594897
Validation loss: 2.462683848427698

Epoch: 6| Step: 4
Training loss: 2.4718399990608275
Validation loss: 2.4677037069828893

Epoch: 6| Step: 5
Training loss: 2.8285727805042202
Validation loss: 2.4585615656633073

Epoch: 6| Step: 6
Training loss: 2.7415673405086998
Validation loss: 2.479394142412015

Epoch: 6| Step: 7
Training loss: 2.920241608226114
Validation loss: 2.463678788335085

Epoch: 6| Step: 8
Training loss: 2.898549368208416
Validation loss: 2.4785515640962936

Epoch: 6| Step: 9
Training loss: 2.932964801034763
Validation loss: 2.4818598684550492

Epoch: 6| Step: 10
Training loss: 2.5653231816133903
Validation loss: 2.469579320336454

Epoch: 6| Step: 11
Training loss: 2.799668009694731
Validation loss: 2.4778983999016404

Epoch: 6| Step: 12
Training loss: 3.108038073332308
Validation loss: 2.4858062237369247

Epoch: 6| Step: 13
Training loss: 3.230182381314209
Validation loss: 2.4761396767674917

Epoch: 57| Step: 0
Training loss: 3.0006889505847854
Validation loss: 2.4913964181131196

Epoch: 6| Step: 1
Training loss: 2.698946948796099
Validation loss: 2.471028901399011

Epoch: 6| Step: 2
Training loss: 3.0447857858760043
Validation loss: 2.4651711430804006

Epoch: 6| Step: 3
Training loss: 3.4683507741122663
Validation loss: 2.480624204684422

Epoch: 6| Step: 4
Training loss: 2.435780530059902
Validation loss: 2.47650967310617

Epoch: 6| Step: 5
Training loss: 2.791114714607348
Validation loss: 2.4817515879950114

Epoch: 6| Step: 6
Training loss: 2.38119922493695
Validation loss: 2.474960175764868

Epoch: 6| Step: 7
Training loss: 2.5474698360854795
Validation loss: 2.484313398595726

Epoch: 6| Step: 8
Training loss: 2.309116981969445
Validation loss: 2.478107582586297

Epoch: 6| Step: 9
Training loss: 3.4932190739046147
Validation loss: 2.475646586940461

Epoch: 6| Step: 10
Training loss: 2.582639036913408
Validation loss: 2.4803196034168002

Epoch: 6| Step: 11
Training loss: 2.130697466243735
Validation loss: 2.4867670182722534

Epoch: 6| Step: 12
Training loss: 2.6992760747406397
Validation loss: 2.4641726078566055

Epoch: 6| Step: 13
Training loss: 2.4418547439613785
Validation loss: 2.484102985094373

Epoch: 58| Step: 0
Training loss: 2.434660798559119
Validation loss: 2.464232516586618

Epoch: 6| Step: 1
Training loss: 3.0745897469120016
Validation loss: 2.4700090015565035

Epoch: 6| Step: 2
Training loss: 2.666136172297544
Validation loss: 2.4878615395635575

Epoch: 6| Step: 3
Training loss: 3.085997452938193
Validation loss: 2.4792101576945633

Epoch: 6| Step: 4
Training loss: 2.3315467011586617
Validation loss: 2.472709641318138

Epoch: 6| Step: 5
Training loss: 2.7716874739767574
Validation loss: 2.4835630495893035

Epoch: 6| Step: 6
Training loss: 3.6248347803808376
Validation loss: 2.478189228305912

Epoch: 6| Step: 7
Training loss: 2.9986880930735387
Validation loss: 2.4800047422609164

Epoch: 6| Step: 8
Training loss: 3.0087331180392085
Validation loss: 2.4846048112729218

Epoch: 6| Step: 9
Training loss: 2.993136820583616
Validation loss: 2.469038895920422

Epoch: 6| Step: 10
Training loss: 2.0177166162838454
Validation loss: 2.477549994639867

Epoch: 6| Step: 11
Training loss: 1.9405206084008533
Validation loss: 2.4793842854738464

Epoch: 6| Step: 12
Training loss: 2.4494643384833568
Validation loss: 2.4704511354500323

Epoch: 6| Step: 13
Training loss: 2.458311296353465
Validation loss: 2.4869314568729184

Epoch: 59| Step: 0
Training loss: 2.5091140554388285
Validation loss: 2.487110628788502

Epoch: 6| Step: 1
Training loss: 2.6206780230459588
Validation loss: 2.4682776862633027

Epoch: 6| Step: 2
Training loss: 2.8970837169134716
Validation loss: 2.476091162958275

Epoch: 6| Step: 3
Training loss: 3.1723085304480576
Validation loss: 2.4703589300850317

Epoch: 6| Step: 4
Training loss: 2.7636999984766057
Validation loss: 2.4840185026102226

Epoch: 6| Step: 5
Training loss: 2.909751013889262
Validation loss: 2.473088914898409

Epoch: 6| Step: 6
Training loss: 2.3106714184457764
Validation loss: 2.466365602010937

Epoch: 6| Step: 7
Training loss: 2.8173777350474176
Validation loss: 2.464256447420579

Epoch: 6| Step: 8
Training loss: 3.025803697238337
Validation loss: 2.477896400013611

Epoch: 6| Step: 9
Training loss: 2.444854595954804
Validation loss: 2.47524161992282

Epoch: 6| Step: 10
Training loss: 2.785253579256822
Validation loss: 2.479726144173543

Epoch: 6| Step: 11
Training loss: 2.850323133387104
Validation loss: 2.4696697537921843

Epoch: 6| Step: 12
Training loss: 2.6148404201444198
Validation loss: 2.470433629023721

Epoch: 6| Step: 13
Training loss: 2.484589285576042
Validation loss: 2.4817142108021892

Epoch: 60| Step: 0
Training loss: 3.0360206737894995
Validation loss: 2.489995042888466

Epoch: 6| Step: 1
Training loss: 2.891332921126693
Validation loss: 2.4743034660038865

Epoch: 6| Step: 2
Training loss: 3.319312665408269
Validation loss: 2.4864514378173936

Epoch: 6| Step: 3
Training loss: 2.054810378893096
Validation loss: 2.4712315428289164

Epoch: 6| Step: 4
Training loss: 2.638808398664454
Validation loss: 2.459417064288415

Epoch: 6| Step: 5
Training loss: 2.1134914216119483
Validation loss: 2.458984605405919

Epoch: 6| Step: 6
Training loss: 3.244915726659187
Validation loss: 2.4913375597910177

Epoch: 6| Step: 7
Training loss: 3.16983793644643
Validation loss: 2.460075107773981

Epoch: 6| Step: 8
Training loss: 2.0210336671088998
Validation loss: 2.482036144648611

Epoch: 6| Step: 9
Training loss: 2.0635870178672038
Validation loss: 2.459066333506272

Epoch: 6| Step: 10
Training loss: 2.9301331041327408
Validation loss: 2.464659784798491

Epoch: 6| Step: 11
Training loss: 2.7210871856023227
Validation loss: 2.4780257127830123

Epoch: 6| Step: 12
Training loss: 3.081356617783315
Validation loss: 2.4718316677125447

Epoch: 6| Step: 13
Training loss: 2.2555313994873774
Validation loss: 2.4918603077005432

Epoch: 61| Step: 0
Training loss: 1.574322036876019
Validation loss: 2.4758607043348233

Epoch: 6| Step: 1
Training loss: 2.1320637377216856
Validation loss: 2.4795861778998605

Epoch: 6| Step: 2
Training loss: 3.286028044392305
Validation loss: 2.479385495233394

Epoch: 6| Step: 3
Training loss: 2.821240977626532
Validation loss: 2.472918943099024

Epoch: 6| Step: 4
Training loss: 2.73255240596904
Validation loss: 2.4869964518260725

Epoch: 6| Step: 5
Training loss: 2.7781672162344484
Validation loss: 2.471248298759086

Epoch: 6| Step: 6
Training loss: 2.362014102922759
Validation loss: 2.4740448414693064

Epoch: 6| Step: 7
Training loss: 3.093710542677676
Validation loss: 2.4763885640893544

Epoch: 6| Step: 8
Training loss: 3.0630947236232626
Validation loss: 2.4846603849816353

Epoch: 6| Step: 9
Training loss: 2.9340467242971586
Validation loss: 2.47604821697402

Epoch: 6| Step: 10
Training loss: 2.8887392233528297
Validation loss: 2.461625054733446

Epoch: 6| Step: 11
Training loss: 2.7456413753793165
Validation loss: 2.480375042134323

Epoch: 6| Step: 12
Training loss: 2.471456468646781
Validation loss: 2.4760831109665156

Epoch: 6| Step: 13
Training loss: 3.0799264041975802
Validation loss: 2.4838550034009845

Epoch: 62| Step: 0
Training loss: 2.897398235387022
Validation loss: 2.4832150695063695

Epoch: 6| Step: 1
Training loss: 2.7601436455981667
Validation loss: 2.4719301600221324

Epoch: 6| Step: 2
Training loss: 2.354715964000364
Validation loss: 2.464700293498591

Epoch: 6| Step: 3
Training loss: 2.3263961209869577
Validation loss: 2.4694135518444105

Epoch: 6| Step: 4
Training loss: 2.2225423290911808
Validation loss: 2.474960498944139

Epoch: 6| Step: 5
Training loss: 2.637189989718245
Validation loss: 2.472033328652323

Epoch: 6| Step: 6
Training loss: 3.5549467893499984
Validation loss: 2.4829907907060753

Epoch: 6| Step: 7
Training loss: 2.3832277030033855
Validation loss: 2.4749890576739415

Epoch: 6| Step: 8
Training loss: 2.0159552251899604
Validation loss: 2.467957323304444

Epoch: 6| Step: 9
Training loss: 2.8656107961454484
Validation loss: 2.470506307381078

Epoch: 6| Step: 10
Training loss: 3.157036711592034
Validation loss: 2.452514045652282

Epoch: 6| Step: 11
Training loss: 3.465548300689815
Validation loss: 2.471494056984041

Epoch: 6| Step: 12
Training loss: 2.737630421218531
Validation loss: 2.4721459453433385

Epoch: 6| Step: 13
Training loss: 2.1995215199070963
Validation loss: 2.468348967821656

Epoch: 63| Step: 0
Training loss: 2.04446572186736
Validation loss: 2.4740808983554423

Epoch: 6| Step: 1
Training loss: 2.4316333694043357
Validation loss: 2.4681282407973635

Epoch: 6| Step: 2
Training loss: 2.989311250185949
Validation loss: 2.4573845215088603

Epoch: 6| Step: 3
Training loss: 2.6506028227450624
Validation loss: 2.475776757595362

Epoch: 6| Step: 4
Training loss: 2.184666678801835
Validation loss: 2.4719642389180327

Epoch: 6| Step: 5
Training loss: 2.5326239542173137
Validation loss: 2.4628546874160095

Epoch: 6| Step: 6
Training loss: 3.143826044273963
Validation loss: 2.486224750416219

Epoch: 6| Step: 7
Training loss: 2.6992587626206253
Validation loss: 2.4725095628311906

Epoch: 6| Step: 8
Training loss: 2.4357297288767055
Validation loss: 2.475872045122172

Epoch: 6| Step: 9
Training loss: 2.5135645039471197
Validation loss: 2.4735942440973298

Epoch: 6| Step: 10
Training loss: 3.0579953442594867
Validation loss: 2.4742929961307336

Epoch: 6| Step: 11
Training loss: 3.4445869218078453
Validation loss: 2.4752459875219

Epoch: 6| Step: 12
Training loss: 2.8835534278202215
Validation loss: 2.4737892864266677

Epoch: 6| Step: 13
Training loss: 3.030204671361622
Validation loss: 2.4823148552716363

Epoch: 64| Step: 0
Training loss: 2.8578643977655105
Validation loss: 2.4678708244290015

Epoch: 6| Step: 1
Training loss: 3.03934051889467
Validation loss: 2.4716243415760975

Epoch: 6| Step: 2
Training loss: 2.4856404854165253
Validation loss: 2.486820838568792

Epoch: 6| Step: 3
Training loss: 2.8670765701995484
Validation loss: 2.472012113476575

Epoch: 6| Step: 4
Training loss: 2.676904595688181
Validation loss: 2.4731884984065675

Epoch: 6| Step: 5
Training loss: 2.3355390023805267
Validation loss: 2.4654071305008194

Epoch: 6| Step: 6
Training loss: 2.9798104895427273
Validation loss: 2.4620037703332076

Epoch: 6| Step: 7
Training loss: 2.6108891097400853
Validation loss: 2.4845420021273585

Epoch: 6| Step: 8
Training loss: 2.8292524082260893
Validation loss: 2.464579538265042

Epoch: 6| Step: 9
Training loss: 2.4779524407552738
Validation loss: 2.469720733457548

Epoch: 6| Step: 10
Training loss: 2.512970845148605
Validation loss: 2.481541178353985

Epoch: 6| Step: 11
Training loss: 2.829752585443508
Validation loss: 2.4871810531344547

Epoch: 6| Step: 12
Training loss: 3.077720654260094
Validation loss: 2.4733518476014105

Epoch: 6| Step: 13
Training loss: 2.4104067086486944
Validation loss: 2.468097440084155

Epoch: 65| Step: 0
Training loss: 3.1099788520314746
Validation loss: 2.4781301938878113

Epoch: 6| Step: 1
Training loss: 3.012023831165563
Validation loss: 2.4636640922615327

Epoch: 6| Step: 2
Training loss: 1.769334929567339
Validation loss: 2.47225787873053

Epoch: 6| Step: 3
Training loss: 3.2667754122740487
Validation loss: 2.4727861229404855

Epoch: 6| Step: 4
Training loss: 2.2948893573161646
Validation loss: 2.4909798481943257

Epoch: 6| Step: 5
Training loss: 2.74959960103347
Validation loss: 2.4635178764870154

Epoch: 6| Step: 6
Training loss: 2.6200340163956484
Validation loss: 2.472217534308025

Epoch: 6| Step: 7
Training loss: 2.681793986447932
Validation loss: 2.4874974438402746

Epoch: 6| Step: 8
Training loss: 2.5249981379738164
Validation loss: 2.4867857035015524

Epoch: 6| Step: 9
Training loss: 2.6097799203975334
Validation loss: 2.470367342156394

Epoch: 6| Step: 10
Training loss: 2.8291229679139303
Validation loss: 2.4716498966939944

Epoch: 6| Step: 11
Training loss: 3.067696969098132
Validation loss: 2.470943692815702

Epoch: 6| Step: 12
Training loss: 2.7465576821837367
Validation loss: 2.474949587477069

Epoch: 6| Step: 13
Training loss: 2.565070653912716
Validation loss: 2.470249316287968

Epoch: 66| Step: 0
Training loss: 2.66556737016854
Validation loss: 2.474816915664398

Epoch: 6| Step: 1
Training loss: 3.157458989714437
Validation loss: 2.480980359020834

Epoch: 6| Step: 2
Training loss: 2.7844394012778277
Validation loss: 2.463581788258972

Epoch: 6| Step: 3
Training loss: 3.376929120341953
Validation loss: 2.474416093341904

Epoch: 6| Step: 4
Training loss: 2.8005929455309313
Validation loss: 2.4660552392904522

Epoch: 6| Step: 5
Training loss: 2.7310561611081807
Validation loss: 2.4694473156157035

Epoch: 6| Step: 6
Training loss: 2.1891680216219336
Validation loss: 2.4583123527564554

Epoch: 6| Step: 7
Training loss: 2.317625397187702
Validation loss: 2.475403888090411

Epoch: 6| Step: 8
Training loss: 3.078759142621645
Validation loss: 2.467445999046492

Epoch: 6| Step: 9
Training loss: 2.422783170670405
Validation loss: 2.4756352332114875

Epoch: 6| Step: 10
Training loss: 2.1639933110273004
Validation loss: 2.484311245986202

Epoch: 6| Step: 11
Training loss: 2.810842322221986
Validation loss: 2.4659007109883495

Epoch: 6| Step: 12
Training loss: 2.6033529816039493
Validation loss: 2.468701676913056

Epoch: 6| Step: 13
Training loss: 3.163444552889932
Validation loss: 2.4686433263608576

Epoch: 67| Step: 0
Training loss: 2.7839554904142094
Validation loss: 2.455945730694612

Epoch: 6| Step: 1
Training loss: 2.55538212376582
Validation loss: 2.4698762392522386

Epoch: 6| Step: 2
Training loss: 3.2220331132869626
Validation loss: 2.469274423376717

Epoch: 6| Step: 3
Training loss: 2.440942143387115
Validation loss: 2.4656392139628323

Epoch: 6| Step: 4
Training loss: 2.103951270028675
Validation loss: 2.4807148400588592

Epoch: 6| Step: 5
Training loss: 2.6998043060026498
Validation loss: 2.4712432601657293

Epoch: 6| Step: 6
Training loss: 3.1486380160636394
Validation loss: 2.4820883034088546

Epoch: 6| Step: 7
Training loss: 3.297773627438237
Validation loss: 2.478338741528824

Epoch: 6| Step: 8
Training loss: 2.745792985618735
Validation loss: 2.470772040406039

Epoch: 6| Step: 9
Training loss: 2.7373691405629863
Validation loss: 2.48904510610003

Epoch: 6| Step: 10
Training loss: 2.4505477293013462
Validation loss: 2.4800055558016387

Epoch: 6| Step: 11
Training loss: 2.6602106522922373
Validation loss: 2.4756375062396527

Epoch: 6| Step: 12
Training loss: 2.145251766270345
Validation loss: 2.4778827101004266

Epoch: 6| Step: 13
Training loss: 2.998935987612989
Validation loss: 2.4649225869861318

Epoch: 68| Step: 0
Training loss: 2.3755814693791226
Validation loss: 2.4600984928747316

Epoch: 6| Step: 1
Training loss: 2.8498356018836724
Validation loss: 2.4721140332215574

Epoch: 6| Step: 2
Training loss: 2.7226626424837526
Validation loss: 2.4768603475645166

Epoch: 6| Step: 3
Training loss: 2.452906896529502
Validation loss: 2.46361596598726

Epoch: 6| Step: 4
Training loss: 2.1201713902991814
Validation loss: 2.4830030761662427

Epoch: 6| Step: 5
Training loss: 3.2513747608809047
Validation loss: 2.4613394731072775

Epoch: 6| Step: 6
Training loss: 3.1577311147161784
Validation loss: 2.4609526941546904

Epoch: 6| Step: 7
Training loss: 3.2151633714425647
Validation loss: 2.4726126544085267

Epoch: 6| Step: 8
Training loss: 2.947590480771368
Validation loss: 2.4761661527980148

Epoch: 6| Step: 9
Training loss: 2.4759491843954504
Validation loss: 2.461658442628739

Epoch: 6| Step: 10
Training loss: 2.393430472379957
Validation loss: 2.4796859284965156

Epoch: 6| Step: 11
Training loss: 3.1657131834259395
Validation loss: 2.467982046951226

Epoch: 6| Step: 12
Training loss: 2.451480581593734
Validation loss: 2.469834939389329

Epoch: 6| Step: 13
Training loss: 1.6742855056932748
Validation loss: 2.4846956574064802

Epoch: 69| Step: 0
Training loss: 2.8235247380554562
Validation loss: 2.4800244956158077

Epoch: 6| Step: 1
Training loss: 2.9975591902818475
Validation loss: 2.468034893292278

Epoch: 6| Step: 2
Training loss: 2.9135384996160796
Validation loss: 2.4583520035185087

Epoch: 6| Step: 3
Training loss: 1.784587878140471
Validation loss: 2.4613778882624846

Epoch: 6| Step: 4
Training loss: 2.837628493571577
Validation loss: 2.4596733954948276

Epoch: 6| Step: 5
Training loss: 2.845006644135613
Validation loss: 2.4757471020362893

Epoch: 6| Step: 6
Training loss: 2.292793020243912
Validation loss: 2.475775872251751

Epoch: 6| Step: 7
Training loss: 2.2026421849819946
Validation loss: 2.4701302432913836

Epoch: 6| Step: 8
Training loss: 3.0878050742220764
Validation loss: 2.4677942361811476

Epoch: 6| Step: 9
Training loss: 2.241353694050972
Validation loss: 2.480460031119583

Epoch: 6| Step: 10
Training loss: 3.21493392947638
Validation loss: 2.471522174474841

Epoch: 6| Step: 11
Training loss: 3.183857800351468
Validation loss: 2.4707089895976235

Epoch: 6| Step: 12
Training loss: 2.915456484455967
Validation loss: 2.471066040793759

Epoch: 6| Step: 13
Training loss: 2.0334770318622555
Validation loss: 2.480573213984137

Epoch: 70| Step: 0
Training loss: 3.04804117431264
Validation loss: 2.4707175903597247

Epoch: 6| Step: 1
Training loss: 2.476959870525825
Validation loss: 2.4654502410469568

Epoch: 6| Step: 2
Training loss: 2.435461512387398
Validation loss: 2.472014845107891

Epoch: 6| Step: 3
Training loss: 2.1955293704392074
Validation loss: 2.477569892769697

Epoch: 6| Step: 4
Training loss: 2.3636859641839636
Validation loss: 2.4642447894517234

Epoch: 6| Step: 5
Training loss: 3.2592009368970047
Validation loss: 2.4763821653022373

Epoch: 6| Step: 6
Training loss: 2.386607665428409
Validation loss: 2.4607627544875457

Epoch: 6| Step: 7
Training loss: 3.166924884373894
Validation loss: 2.454701923384305

Epoch: 6| Step: 8
Training loss: 2.6384803143363844
Validation loss: 2.4748119501157806

Epoch: 6| Step: 9
Training loss: 2.8325312732071097
Validation loss: 2.4665377606341745

Epoch: 6| Step: 10
Training loss: 2.6201050850854952
Validation loss: 2.4793771851039867

Epoch: 6| Step: 11
Training loss: 2.908457256252908
Validation loss: 2.4697316560888276

Epoch: 6| Step: 12
Training loss: 2.427896409830468
Validation loss: 2.481163512969034

Epoch: 6| Step: 13
Training loss: 3.123630223475604
Validation loss: 2.476073358879201

Epoch: 71| Step: 0
Training loss: 1.7398296189473004
Validation loss: 2.472683071772552

Epoch: 6| Step: 1
Training loss: 2.693885314799877
Validation loss: 2.4743541673309104

Epoch: 6| Step: 2
Training loss: 3.117039574194124
Validation loss: 2.4743019729747084

Epoch: 6| Step: 3
Training loss: 3.0061810713576556
Validation loss: 2.4819500816922324

Epoch: 6| Step: 4
Training loss: 3.1530792626200737
Validation loss: 2.469929168580329

Epoch: 6| Step: 5
Training loss: 1.977166848651374
Validation loss: 2.4741718842975313

Epoch: 6| Step: 6
Training loss: 2.069051797755889
Validation loss: 2.4639457061374133

Epoch: 6| Step: 7
Training loss: 2.9517110032970475
Validation loss: 2.4827048513634007

Epoch: 6| Step: 8
Training loss: 2.9783132615682324
Validation loss: 2.4602618535990874

Epoch: 6| Step: 9
Training loss: 3.1737935695234247
Validation loss: 2.4660982693765265

Epoch: 6| Step: 10
Training loss: 2.8293823482080103
Validation loss: 2.4627594484605435

Epoch: 6| Step: 11
Training loss: 2.8303320266001197
Validation loss: 2.4789296967017185

Epoch: 6| Step: 12
Training loss: 2.227341258068691
Validation loss: 2.4553527853300765

Epoch: 6| Step: 13
Training loss: 2.890459375533814
Validation loss: 2.4783758799952333

Epoch: 72| Step: 0
Training loss: 2.5875235588733507
Validation loss: 2.463571835795173

Epoch: 6| Step: 1
Training loss: 2.473537776068656
Validation loss: 2.4872280709925807

Epoch: 6| Step: 2
Training loss: 3.3319139001536504
Validation loss: 2.4646581964735885

Epoch: 6| Step: 3
Training loss: 3.041766143286952
Validation loss: 2.480473580717848

Epoch: 6| Step: 4
Training loss: 3.3803704942424178
Validation loss: 2.4651829713782925

Epoch: 6| Step: 5
Training loss: 1.9280303140449375
Validation loss: 2.4743316437818703

Epoch: 6| Step: 6
Training loss: 2.2137611070954177
Validation loss: 2.4629165257581827

Epoch: 6| Step: 7
Training loss: 2.7721523692322085
Validation loss: 2.4606586426570796

Epoch: 6| Step: 8
Training loss: 2.948734632761694
Validation loss: 2.4662203275877235

Epoch: 6| Step: 9
Training loss: 2.564041976335156
Validation loss: 2.4606337086827414

Epoch: 6| Step: 10
Training loss: 2.369111943837881
Validation loss: 2.481589744193491

Epoch: 6| Step: 11
Training loss: 2.626259274654725
Validation loss: 2.4720007933526658

Epoch: 6| Step: 12
Training loss: 2.8537064527012017
Validation loss: 2.4822929100133444

Epoch: 6| Step: 13
Training loss: 2.5758160973766904
Validation loss: 2.463961632430061

Epoch: 73| Step: 0
Training loss: 3.0971311338189738
Validation loss: 2.4699768749996376

Epoch: 6| Step: 1
Training loss: 2.9922228778709803
Validation loss: 2.49030353228936

Epoch: 6| Step: 2
Training loss: 2.9193801428378587
Validation loss: 2.479702689329427

Epoch: 6| Step: 3
Training loss: 2.7710280577038606
Validation loss: 2.4732182924473336

Epoch: 6| Step: 4
Training loss: 2.764961378576256
Validation loss: 2.472630502068794

Epoch: 6| Step: 5
Training loss: 3.0340891647463737
Validation loss: 2.4668179053536896

Epoch: 6| Step: 6
Training loss: 2.539041278456988
Validation loss: 2.4772065792697653

Epoch: 6| Step: 7
Training loss: 2.5511175287404493
Validation loss: 2.455057933887276

Epoch: 6| Step: 8
Training loss: 2.74628119012122
Validation loss: 2.4711377440379945

Epoch: 6| Step: 9
Training loss: 1.8543566774312294
Validation loss: 2.4589938091142813

Epoch: 6| Step: 10
Training loss: 2.76700475725841
Validation loss: 2.4675868149681963

Epoch: 6| Step: 11
Training loss: 2.6018859845455773
Validation loss: 2.4560838679218455

Epoch: 6| Step: 12
Training loss: 2.7378854941756656
Validation loss: 2.4653312549821336

Epoch: 6| Step: 13
Training loss: 2.112453108047019
Validation loss: 2.455476211880477

Epoch: 74| Step: 0
Training loss: 2.918351930841881
Validation loss: 2.470596310519128

Epoch: 6| Step: 1
Training loss: 1.9848525306250984
Validation loss: 2.4670148655136996

Epoch: 6| Step: 2
Training loss: 2.8827142466107016
Validation loss: 2.4744925450138178

Epoch: 6| Step: 3
Training loss: 2.7029374779721893
Validation loss: 2.482586834787751

Epoch: 6| Step: 4
Training loss: 2.5993400910022157
Validation loss: 2.468464601364254

Epoch: 6| Step: 5
Training loss: 2.840936631502693
Validation loss: 2.460390885564562

Epoch: 6| Step: 6
Training loss: 2.370515806346979
Validation loss: 2.456610046590561

Epoch: 6| Step: 7
Training loss: 3.319264396845999
Validation loss: 2.481991158270646

Epoch: 6| Step: 8
Training loss: 3.507699671504622
Validation loss: 2.4680762035408015

Epoch: 6| Step: 9
Training loss: 2.9904777084001943
Validation loss: 2.4750716141770295

Epoch: 6| Step: 10
Training loss: 2.739947325067898
Validation loss: 2.47754033834995

Epoch: 6| Step: 11
Training loss: 2.2720129000069194
Validation loss: 2.4775586606416913

Epoch: 6| Step: 12
Training loss: 1.8334994096383133
Validation loss: 2.482950813755208

Epoch: 6| Step: 13
Training loss: 2.463421830104846
Validation loss: 2.4860962312189328

Epoch: 75| Step: 0
Training loss: 2.2030919931522943
Validation loss: 2.468327973362256

Epoch: 6| Step: 1
Training loss: 2.7434969218330503
Validation loss: 2.473542885647091

Epoch: 6| Step: 2
Training loss: 2.9312443779930093
Validation loss: 2.4799344759381854

Epoch: 6| Step: 3
Training loss: 3.0190144993628767
Validation loss: 2.48660986813386

Epoch: 6| Step: 4
Training loss: 2.7101046907255046
Validation loss: 2.4737622559094015

Epoch: 6| Step: 5
Training loss: 3.2718273199445664
Validation loss: 2.467565749610299

Epoch: 6| Step: 6
Training loss: 2.1199311781454706
Validation loss: 2.478886716124811

Epoch: 6| Step: 7
Training loss: 2.2634672215580296
Validation loss: 2.47721987034877

Epoch: 6| Step: 8
Training loss: 3.191492609468796
Validation loss: 2.467451280204179

Epoch: 6| Step: 9
Training loss: 2.2686409706000434
Validation loss: 2.480853851890019

Epoch: 6| Step: 10
Training loss: 2.630854843804097
Validation loss: 2.4545247285887037

Epoch: 6| Step: 11
Training loss: 3.386929656152832
Validation loss: 2.469931730217273

Epoch: 6| Step: 12
Training loss: 2.0732296526867304
Validation loss: 2.469186999874711

Epoch: 6| Step: 13
Training loss: 2.9291070388505682
Validation loss: 2.4722484921211128

Epoch: 76| Step: 0
Training loss: 2.5953584762258157
Validation loss: 2.4905171296916375

Epoch: 6| Step: 1
Training loss: 2.54232306420824
Validation loss: 2.475812137945016

Epoch: 6| Step: 2
Training loss: 2.9478714650780793
Validation loss: 2.4650887843571385

Epoch: 6| Step: 3
Training loss: 3.023554363505758
Validation loss: 2.463677040169774

Epoch: 6| Step: 4
Training loss: 3.1278447744972273
Validation loss: 2.485639960959586

Epoch: 6| Step: 5
Training loss: 2.6562154431058658
Validation loss: 2.47786286414428

Epoch: 6| Step: 6
Training loss: 2.5109622937481557
Validation loss: 2.4728126571667106

Epoch: 6| Step: 7
Training loss: 2.809347717356358
Validation loss: 2.4702789361438375

Epoch: 6| Step: 8
Training loss: 2.7651317609189836
Validation loss: 2.477521261582567

Epoch: 6| Step: 9
Training loss: 2.552930875422893
Validation loss: 2.4867121720608876

Epoch: 6| Step: 10
Training loss: 2.198931443197428
Validation loss: 2.464705621105026

Epoch: 6| Step: 11
Training loss: 2.3224966128420075
Validation loss: 2.4776265658652723

Epoch: 6| Step: 12
Training loss: 2.202728452233126
Validation loss: 2.474684226379512

Epoch: 6| Step: 13
Training loss: 3.8641566257759057
Validation loss: 2.474193165918624

Epoch: 77| Step: 0
Training loss: 2.858972406446973
Validation loss: 2.4652895909284935

Epoch: 6| Step: 1
Training loss: 2.4218073681648
Validation loss: 2.466878746458426

Epoch: 6| Step: 2
Training loss: 2.6391707710384362
Validation loss: 2.464788135125861

Epoch: 6| Step: 3
Training loss: 2.435126298139508
Validation loss: 2.457231399953115

Epoch: 6| Step: 4
Training loss: 2.2405870421105996
Validation loss: 2.450697242598073

Epoch: 6| Step: 5
Training loss: 2.36872355665278
Validation loss: 2.4604901077854477

Epoch: 6| Step: 6
Training loss: 2.789160718711101
Validation loss: 2.4593087213700646

Epoch: 6| Step: 7
Training loss: 3.603768887187515
Validation loss: 2.459008638397019

Epoch: 6| Step: 8
Training loss: 2.7584519117823096
Validation loss: 2.452469724052844

Epoch: 6| Step: 9
Training loss: 2.9672134438455706
Validation loss: 2.455623553232334

Epoch: 6| Step: 10
Training loss: 2.1871516359051064
Validation loss: 2.4580319457794615

Epoch: 6| Step: 11
Training loss: 2.8536295884369665
Validation loss: 2.461692813686634

Epoch: 6| Step: 12
Training loss: 2.5161780466175827
Validation loss: 2.46515979726217

Epoch: 6| Step: 13
Training loss: 3.1576734297988787
Validation loss: 2.454694967806253

Epoch: 78| Step: 0
Training loss: 3.372819266622191
Validation loss: 2.4681095150731736

Epoch: 6| Step: 1
Training loss: 2.1627817047620903
Validation loss: 2.4787158382086907

Epoch: 6| Step: 2
Training loss: 2.6666133001073797
Validation loss: 2.4545265135616567

Epoch: 6| Step: 3
Training loss: 2.78119899402953
Validation loss: 2.4617805449795505

Epoch: 6| Step: 4
Training loss: 3.314765731105566
Validation loss: 2.4791988585455287

Epoch: 6| Step: 5
Training loss: 2.324589444900176
Validation loss: 2.467490772681761

Epoch: 6| Step: 6
Training loss: 2.2301965871469216
Validation loss: 2.464450423934068

Epoch: 6| Step: 7
Training loss: 2.6432350396653246
Validation loss: 2.47542275954885

Epoch: 6| Step: 8
Training loss: 2.7965162675842388
Validation loss: 2.47788204898496

Epoch: 6| Step: 9
Training loss: 2.5220026714241226
Validation loss: 2.464456597800649

Epoch: 6| Step: 10
Training loss: 2.6595096110288425
Validation loss: 2.477209214102743

Epoch: 6| Step: 11
Training loss: 2.5859166631406545
Validation loss: 2.4839519156732663

Epoch: 6| Step: 12
Training loss: 2.5163942665399817
Validation loss: 2.472204811565202

Epoch: 6| Step: 13
Training loss: 3.1226710987477677
Validation loss: 2.466735935222471

Epoch: 79| Step: 0
Training loss: 2.8222483914915015
Validation loss: 2.4700552668837377

Epoch: 6| Step: 1
Training loss: 2.4613936712936084
Validation loss: 2.4803275109063794

Epoch: 6| Step: 2
Training loss: 2.695823643080813
Validation loss: 2.498946762747226

Epoch: 6| Step: 3
Training loss: 2.9191165081320953
Validation loss: 2.4785586192522726

Epoch: 6| Step: 4
Training loss: 2.0624285598866767
Validation loss: 2.4861960732739408

Epoch: 6| Step: 5
Training loss: 2.8479854356843397
Validation loss: 2.4731222179986947

Epoch: 6| Step: 6
Training loss: 2.7311036513726488
Validation loss: 2.4797976114846088

Epoch: 6| Step: 7
Training loss: 3.5673837481606356
Validation loss: 2.484391644582613

Epoch: 6| Step: 8
Training loss: 2.2502394654741833
Validation loss: 2.464717370506393

Epoch: 6| Step: 9
Training loss: 2.2248043145698593
Validation loss: 2.4785929452318856

Epoch: 6| Step: 10
Training loss: 2.8704575852391945
Validation loss: 2.472144985071679

Epoch: 6| Step: 11
Training loss: 2.692384052241735
Validation loss: 2.467280997435902

Epoch: 6| Step: 12
Training loss: 2.320549744059485
Validation loss: 2.484171660858854

Epoch: 6| Step: 13
Training loss: 3.219447680663606
Validation loss: 2.4784954593247654

Epoch: 80| Step: 0
Training loss: 3.2571978458026862
Validation loss: 2.4729345332534356

Epoch: 6| Step: 1
Training loss: 2.3061752431274947
Validation loss: 2.47874662281437

Epoch: 6| Step: 2
Training loss: 2.525774083597841
Validation loss: 2.4838877606633285

Epoch: 6| Step: 3
Training loss: 2.61771189432222
Validation loss: 2.4711330776585125

Epoch: 6| Step: 4
Training loss: 2.893536395434504
Validation loss: 2.466157526322313

Epoch: 6| Step: 5
Training loss: 2.3306477850375398
Validation loss: 2.479463990822612

Epoch: 6| Step: 6
Training loss: 2.917870699908877
Validation loss: 2.4639281638941437

Epoch: 6| Step: 7
Training loss: 2.788584435787731
Validation loss: 2.4710990785956395

Epoch: 6| Step: 8
Training loss: 2.345888814953258
Validation loss: 2.4614849908091636

Epoch: 6| Step: 9
Training loss: 2.7447138177495325
Validation loss: 2.4630948737547036

Epoch: 6| Step: 10
Training loss: 3.272445346268066
Validation loss: 2.458984391680972

Epoch: 6| Step: 11
Training loss: 2.5932986314833126
Validation loss: 2.4756397254173015

Epoch: 6| Step: 12
Training loss: 2.437803787472113
Validation loss: 2.457569093901396

Epoch: 6| Step: 13
Training loss: 2.4975988300919467
Validation loss: 2.4728028621162745

Epoch: 81| Step: 0
Training loss: 2.4285735002076105
Validation loss: 2.455462399056374

Epoch: 6| Step: 1
Training loss: 2.9391163376415377
Validation loss: 2.4721913151615906

Epoch: 6| Step: 2
Training loss: 2.345118822287991
Validation loss: 2.459868099219081

Epoch: 6| Step: 3
Training loss: 3.0167994608726616
Validation loss: 2.4658340912146306

Epoch: 6| Step: 4
Training loss: 3.0365978141318024
Validation loss: 2.4594766854652605

Epoch: 6| Step: 5
Training loss: 2.5061849381142567
Validation loss: 2.4566938332692025

Epoch: 6| Step: 6
Training loss: 3.0008683537377716
Validation loss: 2.4795593201709174

Epoch: 6| Step: 7
Training loss: 2.234459295216626
Validation loss: 2.448493403294631

Epoch: 6| Step: 8
Training loss: 3.36170403231077
Validation loss: 2.4651685712968487

Epoch: 6| Step: 9
Training loss: 2.4217511545326
Validation loss: 2.4694997745249525

Epoch: 6| Step: 10
Training loss: 2.6262034427777423
Validation loss: 2.4700350160496862

Epoch: 6| Step: 11
Training loss: 2.6671083104468294
Validation loss: 2.466752049277346

Epoch: 6| Step: 12
Training loss: 2.4408337219314378
Validation loss: 2.484287678608065

Epoch: 6| Step: 13
Training loss: 2.3972581579531234
Validation loss: 2.463856412113806

Epoch: 82| Step: 0
Training loss: 2.409463693294971
Validation loss: 2.4674630087391116

Epoch: 6| Step: 1
Training loss: 2.2425407104879502
Validation loss: 2.46368647192315

Epoch: 6| Step: 2
Training loss: 2.0435114590085774
Validation loss: 2.4684273771316567

Epoch: 6| Step: 3
Training loss: 3.2363979629044373
Validation loss: 2.459993825351647

Epoch: 6| Step: 4
Training loss: 3.2042182359319726
Validation loss: 2.4623380994338984

Epoch: 6| Step: 5
Training loss: 2.3941381234435863
Validation loss: 2.4564265496259887

Epoch: 6| Step: 6
Training loss: 2.829214992494364
Validation loss: 2.470864871559661

Epoch: 6| Step: 7
Training loss: 2.505603518547374
Validation loss: 2.475276212504899

Epoch: 6| Step: 8
Training loss: 2.676131934136703
Validation loss: 2.4730914214317883

Epoch: 6| Step: 9
Training loss: 2.633034535449196
Validation loss: 2.470446447019156

Epoch: 6| Step: 10
Training loss: 3.3667147629830607
Validation loss: 2.461456430585716

Epoch: 6| Step: 11
Training loss: 1.891019102273418
Validation loss: 2.4583868629574543

Epoch: 6| Step: 12
Training loss: 3.2709641319087996
Validation loss: 2.464866867803386

Epoch: 6| Step: 13
Training loss: 2.542218122432681
Validation loss: 2.4780784214925955

Epoch: 83| Step: 0
Training loss: 2.7398348113893967
Validation loss: 2.46847704843582

Epoch: 6| Step: 1
Training loss: 2.5079170751901416
Validation loss: 2.46775166627377

Epoch: 6| Step: 2
Training loss: 2.485463030101036
Validation loss: 2.457396630360073

Epoch: 6| Step: 3
Training loss: 1.9738989211294944
Validation loss: 2.45369071490934

Epoch: 6| Step: 4
Training loss: 2.147931403458882
Validation loss: 2.456688928660068

Epoch: 6| Step: 5
Training loss: 2.9778860758862202
Validation loss: 2.450942812635746

Epoch: 6| Step: 6
Training loss: 2.3546958148609516
Validation loss: 2.462974164714916

Epoch: 6| Step: 7
Training loss: 2.8618316936361223
Validation loss: 2.455269638886677

Epoch: 6| Step: 8
Training loss: 2.1645652655201433
Validation loss: 2.470201777031179

Epoch: 6| Step: 9
Training loss: 2.4734918951052514
Validation loss: 2.483614901711899

Epoch: 6| Step: 10
Training loss: 2.998620669846531
Validation loss: 2.469264583144247

Epoch: 6| Step: 11
Training loss: 3.744241743955993
Validation loss: 2.4536593547363004

Epoch: 6| Step: 12
Training loss: 2.8535386854176
Validation loss: 2.456843745750259

Epoch: 6| Step: 13
Training loss: 3.203332810522613
Validation loss: 2.4723773464572467

Epoch: 84| Step: 0
Training loss: 2.02684152957219
Validation loss: 2.4689012758639906

Epoch: 6| Step: 1
Training loss: 2.9331538737916336
Validation loss: 2.471310987254919

Epoch: 6| Step: 2
Training loss: 2.9017850972269263
Validation loss: 2.460140767962502

Epoch: 6| Step: 3
Training loss: 3.5085086938762005
Validation loss: 2.4825466210474696

Epoch: 6| Step: 4
Training loss: 2.9946599162878567
Validation loss: 2.462958993960032

Epoch: 6| Step: 5
Training loss: 2.8493868117977716
Validation loss: 2.461091933163399

Epoch: 6| Step: 6
Training loss: 2.7414348906278994
Validation loss: 2.4590335708653424

Epoch: 6| Step: 7
Training loss: 2.51846134623707
Validation loss: 2.4524198822030523

Epoch: 6| Step: 8
Training loss: 3.195807756688816
Validation loss: 2.4750585280419752

Epoch: 6| Step: 9
Training loss: 1.639924108563608
Validation loss: 2.4744210270491203

Epoch: 6| Step: 10
Training loss: 2.296695935801494
Validation loss: 2.4589681719884595

Epoch: 6| Step: 11
Training loss: 2.3344862905015207
Validation loss: 2.46120431302979

Epoch: 6| Step: 12
Training loss: 2.3147481352068002
Validation loss: 2.4595197170308545

Epoch: 6| Step: 13
Training loss: 3.0349447797751083
Validation loss: 2.468106632661808

Epoch: 85| Step: 0
Training loss: 2.793447514827677
Validation loss: 2.4753411655037594

Epoch: 6| Step: 1
Training loss: 2.956615826995029
Validation loss: 2.454469098195859

Epoch: 6| Step: 2
Training loss: 2.8026647898047368
Validation loss: 2.454767451063883

Epoch: 6| Step: 3
Training loss: 3.185882157784719
Validation loss: 2.4667180636199877

Epoch: 6| Step: 4
Training loss: 1.4297033423708148
Validation loss: 2.462738809207327

Epoch: 6| Step: 5
Training loss: 2.6057008013699052
Validation loss: 2.4773852085978563

Epoch: 6| Step: 6
Training loss: 3.2033691313215695
Validation loss: 2.458706814541787

Epoch: 6| Step: 7
Training loss: 2.4638352545121505
Validation loss: 2.4701960098231015

Epoch: 6| Step: 8
Training loss: 2.5306510687663253
Validation loss: 2.4708619420414553

Epoch: 6| Step: 9
Training loss: 2.9268892886822906
Validation loss: 2.4651177736020613

Epoch: 6| Step: 10
Training loss: 2.025648051045117
Validation loss: 2.4643873533860274

Epoch: 6| Step: 11
Training loss: 2.1874803814689456
Validation loss: 2.455350415215653

Epoch: 6| Step: 12
Training loss: 3.0195056672388123
Validation loss: 2.4712256296763644

Epoch: 6| Step: 13
Training loss: 3.20655363849917
Validation loss: 2.475287080068689

Epoch: 86| Step: 0
Training loss: 2.7953837590866097
Validation loss: 2.4579219240962336

Epoch: 6| Step: 1
Training loss: 2.883627179411178
Validation loss: 2.4732001038516156

Epoch: 6| Step: 2
Training loss: 3.156040600166205
Validation loss: 2.462282230144112

Epoch: 6| Step: 3
Training loss: 2.556931842628086
Validation loss: 2.4630359844447396

Epoch: 6| Step: 4
Training loss: 2.8523409682265264
Validation loss: 2.4718777485427417

Epoch: 6| Step: 5
Training loss: 2.300184188599503
Validation loss: 2.4607886099256304

Epoch: 6| Step: 6
Training loss: 3.3257833490237387
Validation loss: 2.4702054125344084

Epoch: 6| Step: 7
Training loss: 2.5468828634128995
Validation loss: 2.468996103464911

Epoch: 6| Step: 8
Training loss: 1.9929668622398633
Validation loss: 2.455197472219631

Epoch: 6| Step: 9
Training loss: 2.416785226303348
Validation loss: 2.465900664204702

Epoch: 6| Step: 10
Training loss: 2.7909257579347595
Validation loss: 2.4658811506679257

Epoch: 6| Step: 11
Training loss: 2.3033637949157133
Validation loss: 2.458795328529709

Epoch: 6| Step: 12
Training loss: 2.768807083815865
Validation loss: 2.4649700606123597

Epoch: 6| Step: 13
Training loss: 2.610690213722955
Validation loss: 2.466739745236821

Epoch: 87| Step: 0
Training loss: 2.7148190723642354
Validation loss: 2.4537520413564757

Epoch: 6| Step: 1
Training loss: 3.2667897168812825
Validation loss: 2.4684934844977438

Epoch: 6| Step: 2
Training loss: 2.9495910813708135
Validation loss: 2.4615112208880134

Epoch: 6| Step: 3
Training loss: 2.7958515361831946
Validation loss: 2.468054270866088

Epoch: 6| Step: 4
Training loss: 2.4493949376545854
Validation loss: 2.4757756568698657

Epoch: 6| Step: 5
Training loss: 2.620001984806619
Validation loss: 2.458684560033355

Epoch: 6| Step: 6
Training loss: 2.149672274784473
Validation loss: 2.468837506472515

Epoch: 6| Step: 7
Training loss: 2.7946304606550565
Validation loss: 2.4682336955066413

Epoch: 6| Step: 8
Training loss: 2.1989230944762808
Validation loss: 2.4600437221715157

Epoch: 6| Step: 9
Training loss: 2.667575701110427
Validation loss: 2.4542997343131145

Epoch: 6| Step: 10
Training loss: 2.72105178734938
Validation loss: 2.4849227112686005

Epoch: 6| Step: 11
Training loss: 2.652813831858818
Validation loss: 2.463774363223421

Epoch: 6| Step: 12
Training loss: 2.3760919570339127
Validation loss: 2.466800639192906

Epoch: 6| Step: 13
Training loss: 3.163525947947929
Validation loss: 2.464350624641874

Epoch: 88| Step: 0
Training loss: 3.0889134990007543
Validation loss: 2.4651636164721666

Epoch: 6| Step: 1
Training loss: 3.1026415737412245
Validation loss: 2.4668309666137604

Epoch: 6| Step: 2
Training loss: 2.310023244435333
Validation loss: 2.4698898604057025

Epoch: 6| Step: 3
Training loss: 3.4311484177636036
Validation loss: 2.4665589594945936

Epoch: 6| Step: 4
Training loss: 2.294614444999942
Validation loss: 2.4522082050830583

Epoch: 6| Step: 5
Training loss: 2.137210991489505
Validation loss: 2.4768645498087656

Epoch: 6| Step: 6
Training loss: 2.603135192045155
Validation loss: 2.4590634258994855

Epoch: 6| Step: 7
Training loss: 2.619708586349363
Validation loss: 2.4655308907552627

Epoch: 6| Step: 8
Training loss: 2.912970862575838
Validation loss: 2.45821213740677

Epoch: 6| Step: 9
Training loss: 2.8450720093099893
Validation loss: 2.4703895365624806

Epoch: 6| Step: 10
Training loss: 2.3719890984193035
Validation loss: 2.472743163099033

Epoch: 6| Step: 11
Training loss: 2.8217742597777966
Validation loss: 2.467118941097343

Epoch: 6| Step: 12
Training loss: 2.2547810937564914
Validation loss: 2.4781533356797096

Epoch: 6| Step: 13
Training loss: 2.1850681411278847
Validation loss: 2.4648445675046413

Epoch: 89| Step: 0
Training loss: 2.868051009114264
Validation loss: 2.4585653956445657

Epoch: 6| Step: 1
Training loss: 2.9149394416080394
Validation loss: 2.469572342299119

Epoch: 6| Step: 2
Training loss: 2.7559464060385035
Validation loss: 2.4625232395815977

Epoch: 6| Step: 3
Training loss: 2.3124611310012018
Validation loss: 2.4762017460347083

Epoch: 6| Step: 4
Training loss: 2.2312015859443
Validation loss: 2.471364087098577

Epoch: 6| Step: 5
Training loss: 2.8540041986339673
Validation loss: 2.4606960581212474

Epoch: 6| Step: 6
Training loss: 1.3941430157838408
Validation loss: 2.4635178202923367

Epoch: 6| Step: 7
Training loss: 2.76831561743365
Validation loss: 2.4684951586311086

Epoch: 6| Step: 8
Training loss: 3.182579431254258
Validation loss: 2.467952852433256

Epoch: 6| Step: 9
Training loss: 2.7542162565151975
Validation loss: 2.465147354240159

Epoch: 6| Step: 10
Training loss: 2.597720221398627
Validation loss: 2.4662783321492934

Epoch: 6| Step: 11
Training loss: 3.1652320489894885
Validation loss: 2.4704668392418925

Epoch: 6| Step: 12
Training loss: 2.542287615192147
Validation loss: 2.445961968801324

Epoch: 6| Step: 13
Training loss: 3.033603658962055
Validation loss: 2.466711052545411

Epoch: 90| Step: 0
Training loss: 2.5052621773520047
Validation loss: 2.4628742348203647

Epoch: 6| Step: 1
Training loss: 2.7468463848476645
Validation loss: 2.470822483214546

Epoch: 6| Step: 2
Training loss: 2.6648621016390233
Validation loss: 2.4674708135321026

Epoch: 6| Step: 3
Training loss: 2.3166108579797275
Validation loss: 2.4678352627045688

Epoch: 6| Step: 4
Training loss: 2.619873399514962
Validation loss: 2.4532806606270556

Epoch: 6| Step: 5
Training loss: 2.959472777091752
Validation loss: 2.4655163668902396

Epoch: 6| Step: 6
Training loss: 2.2221560057735283
Validation loss: 2.4627315000158823

Epoch: 6| Step: 7
Training loss: 3.010733635042994
Validation loss: 2.452066985892602

Epoch: 6| Step: 8
Training loss: 2.5442041539981965
Validation loss: 2.4804655863604705

Epoch: 6| Step: 9
Training loss: 2.8029718703819246
Validation loss: 2.4589979730824827

Epoch: 6| Step: 10
Training loss: 3.1080752009029275
Validation loss: 2.4567352569986576

Epoch: 6| Step: 11
Training loss: 2.9976716542985264
Validation loss: 2.4751778253765

Epoch: 6| Step: 12
Training loss: 2.384970166481611
Validation loss: 2.455966031418849

Epoch: 6| Step: 13
Training loss: 2.604467105065293
Validation loss: 2.46872385202509

Epoch: 91| Step: 0
Training loss: 2.5641357039486348
Validation loss: 2.4740335425628137

Epoch: 6| Step: 1
Training loss: 2.884238616595537
Validation loss: 2.451842951788994

Epoch: 6| Step: 2
Training loss: 2.8530152683110233
Validation loss: 2.4506975250413383

Epoch: 6| Step: 3
Training loss: 2.986442129629826
Validation loss: 2.467655401846019

Epoch: 6| Step: 4
Training loss: 2.4045983132522784
Validation loss: 2.4466422347605086

Epoch: 6| Step: 5
Training loss: 2.981418283379363
Validation loss: 2.4681315521653584

Epoch: 6| Step: 6
Training loss: 3.2246735518484986
Validation loss: 2.466686747471167

Epoch: 6| Step: 7
Training loss: 2.6703392210505683
Validation loss: 2.461803761319831

Epoch: 6| Step: 8
Training loss: 2.5556855191710093
Validation loss: 2.449476498000239

Epoch: 6| Step: 9
Training loss: 2.852416362689735
Validation loss: 2.4794189774210933

Epoch: 6| Step: 10
Training loss: 2.523809730310423
Validation loss: 2.462017744560629

Epoch: 6| Step: 11
Training loss: 2.3190440811435358
Validation loss: 2.4549694804251962

Epoch: 6| Step: 12
Training loss: 2.135906281204736
Validation loss: 2.4504000607722367

Epoch: 6| Step: 13
Training loss: 2.137642223244378
Validation loss: 2.4778435870733757

Epoch: 92| Step: 0
Training loss: 2.3992439430178645
Validation loss: 2.4539322442910074

Epoch: 6| Step: 1
Training loss: 2.8092211790481185
Validation loss: 2.4743045622037783

Epoch: 6| Step: 2
Training loss: 2.0284088218306056
Validation loss: 2.462038224296984

Epoch: 6| Step: 3
Training loss: 3.214397561685597
Validation loss: 2.4676448414319507

Epoch: 6| Step: 4
Training loss: 2.8162460810688654
Validation loss: 2.4672587168791207

Epoch: 6| Step: 5
Training loss: 3.1057386748847375
Validation loss: 2.4647578460163846

Epoch: 6| Step: 6
Training loss: 2.2284049993768096
Validation loss: 2.4486647161765553

Epoch: 6| Step: 7
Training loss: 2.738989201123668
Validation loss: 2.4471994166548323

Epoch: 6| Step: 8
Training loss: 3.0324663690901974
Validation loss: 2.4636948563095187

Epoch: 6| Step: 9
Training loss: 2.887746006117853
Validation loss: 2.470371476576231

Epoch: 6| Step: 10
Training loss: 2.84187154245764
Validation loss: 2.460298702255969

Epoch: 6| Step: 11
Training loss: 2.560092725713604
Validation loss: 2.471000283412089

Epoch: 6| Step: 12
Training loss: 2.250390442773871
Validation loss: 2.462469866851273

Epoch: 6| Step: 13
Training loss: 2.4158725310671394
Validation loss: 2.4718662893469725

Epoch: 93| Step: 0
Training loss: 2.94975451724989
Validation loss: 2.4511405725477795

Epoch: 6| Step: 1
Training loss: 2.2540674532094562
Validation loss: 2.4647500295122375

Epoch: 6| Step: 2
Training loss: 2.8481680955570927
Validation loss: 2.4542343492654948

Epoch: 6| Step: 3
Training loss: 3.0182962396435813
Validation loss: 2.4607512872877226

Epoch: 6| Step: 4
Training loss: 2.2292481404564444
Validation loss: 2.471899194073831

Epoch: 6| Step: 5
Training loss: 2.965794427210466
Validation loss: 2.471055603889673

Epoch: 6| Step: 6
Training loss: 2.577462868483157
Validation loss: 2.4706205932160836

Epoch: 6| Step: 7
Training loss: 2.834787182767347
Validation loss: 2.460423027883989

Epoch: 6| Step: 8
Training loss: 2.4509829748551466
Validation loss: 2.454010208504658

Epoch: 6| Step: 9
Training loss: 2.628277412921102
Validation loss: 2.4672658531629246

Epoch: 6| Step: 10
Training loss: 3.218910435502154
Validation loss: 2.469186123589083

Epoch: 6| Step: 11
Training loss: 2.7333384265697136
Validation loss: 2.4734543454202265

Epoch: 6| Step: 12
Training loss: 2.0601520467403556
Validation loss: 2.468434274281572

Epoch: 6| Step: 13
Training loss: 2.519884850580829
Validation loss: 2.4782139010814634

Epoch: 94| Step: 0
Training loss: 2.405306544445823
Validation loss: 2.4736451599826768

Epoch: 6| Step: 1
Training loss: 2.9914137036441644
Validation loss: 2.469135964024541

Epoch: 6| Step: 2
Training loss: 2.7645132125398004
Validation loss: 2.476951230373998

Epoch: 6| Step: 3
Training loss: 3.037537493690198
Validation loss: 2.4770602102062953

Epoch: 6| Step: 4
Training loss: 2.195957075634072
Validation loss: 2.4608822364676666

Epoch: 6| Step: 5
Training loss: 2.8399110809697796
Validation loss: 2.4658063902330407

Epoch: 6| Step: 6
Training loss: 2.938732619078011
Validation loss: 2.4760833604885772

Epoch: 6| Step: 7
Training loss: 2.030806625100841
Validation loss: 2.473934483221576

Epoch: 6| Step: 8
Training loss: 2.820934279949108
Validation loss: 2.4539083266482637

Epoch: 6| Step: 9
Training loss: 2.3791966754292373
Validation loss: 2.4744406395258873

Epoch: 6| Step: 10
Training loss: 2.3649029172685863
Validation loss: 2.4714637992316937

Epoch: 6| Step: 11
Training loss: 3.1965610144866528
Validation loss: 2.4627253639841826

Epoch: 6| Step: 12
Training loss: 2.713062607742677
Validation loss: 2.457703283272309

Epoch: 6| Step: 13
Training loss: 2.4537976551529432
Validation loss: 2.456392163846759

Epoch: 95| Step: 0
Training loss: 2.9351898199562503
Validation loss: 2.4729274247316098

Epoch: 6| Step: 1
Training loss: 2.3730306993725967
Validation loss: 2.473316105554088

Epoch: 6| Step: 2
Training loss: 2.5138998810005018
Validation loss: 2.449836454317561

Epoch: 6| Step: 3
Training loss: 2.546399967382241
Validation loss: 2.4659848930856176

Epoch: 6| Step: 4
Training loss: 2.0447339223856225
Validation loss: 2.4785082170089083

Epoch: 6| Step: 5
Training loss: 2.8278577478072506
Validation loss: 2.4667017809766687

Epoch: 6| Step: 6
Training loss: 3.022815729125905
Validation loss: 2.4701560395979807

Epoch: 6| Step: 7
Training loss: 2.6417144694851777
Validation loss: 2.4639440663695495

Epoch: 6| Step: 8
Training loss: 3.1930003859761356
Validation loss: 2.4632821769264184

Epoch: 6| Step: 9
Training loss: 1.7938971349469495
Validation loss: 2.4642301924650742

Epoch: 6| Step: 10
Training loss: 2.969367194015915
Validation loss: 2.4683573389672087

Epoch: 6| Step: 11
Training loss: 2.800242352215275
Validation loss: 2.457079076422951

Epoch: 6| Step: 12
Training loss: 2.6137064543662794
Validation loss: 2.453671136136334

Epoch: 6| Step: 13
Training loss: 3.076575494354224
Validation loss: 2.4708513439732323

Epoch: 96| Step: 0
Training loss: 3.379272864413746
Validation loss: 2.4635146624615185

Epoch: 6| Step: 1
Training loss: 2.892771320987427
Validation loss: 2.4605607809120054

Epoch: 6| Step: 2
Training loss: 2.5040612611680686
Validation loss: 2.4788371830847575

Epoch: 6| Step: 3
Training loss: 2.5397534178643144
Validation loss: 2.462982024326206

Epoch: 6| Step: 4
Training loss: 2.9131664708739056
Validation loss: 2.4652552939354537

Epoch: 6| Step: 5
Training loss: 2.75348096228537
Validation loss: 2.4625974323866715

Epoch: 6| Step: 6
Training loss: 2.795148433776944
Validation loss: 2.4823258252461353

Epoch: 6| Step: 7
Training loss: 1.7615853743381709
Validation loss: 2.4682578773290533

Epoch: 6| Step: 8
Training loss: 2.3311109404214836
Validation loss: 2.4647123029598395

Epoch: 6| Step: 9
Training loss: 3.053952492095221
Validation loss: 2.4597436736281844

Epoch: 6| Step: 10
Training loss: 3.038127843702061
Validation loss: 2.4608767172343136

Epoch: 6| Step: 11
Training loss: 2.586357336163104
Validation loss: 2.4800533712663255

Epoch: 6| Step: 12
Training loss: 2.2454888366815857
Validation loss: 2.4660160767633874

Epoch: 6| Step: 13
Training loss: 1.989869088894111
Validation loss: 2.4802995170085995

Epoch: 97| Step: 0
Training loss: 2.7070599619125795
Validation loss: 2.4683494466187117

Epoch: 6| Step: 1
Training loss: 2.92116631874037
Validation loss: 2.4741391704033364

Epoch: 6| Step: 2
Training loss: 2.750539726698557
Validation loss: 2.4657590512341767

Epoch: 6| Step: 3
Training loss: 1.9685969217521477
Validation loss: 2.456214836315825

Epoch: 6| Step: 4
Training loss: 2.977086459123284
Validation loss: 2.46340112771566

Epoch: 6| Step: 5
Training loss: 2.856604729792939
Validation loss: 2.4530197339384974

Epoch: 6| Step: 6
Training loss: 2.6244162864190175
Validation loss: 2.4662256851745274

Epoch: 6| Step: 7
Training loss: 3.294157381579432
Validation loss: 2.476654208380826

Epoch: 6| Step: 8
Training loss: 2.6001509769327313
Validation loss: 2.460885146092498

Epoch: 6| Step: 9
Training loss: 2.6956359669183447
Validation loss: 2.4568785931985913

Epoch: 6| Step: 10
Training loss: 1.839710400845272
Validation loss: 2.4631751436773963

Epoch: 6| Step: 11
Training loss: 2.093565975471598
Validation loss: 2.4626179650789477

Epoch: 6| Step: 12
Training loss: 2.77510618058248
Validation loss: 2.4549680941619734

Epoch: 6| Step: 13
Training loss: 2.9410878448597284
Validation loss: 2.4650854189827096

Epoch: 98| Step: 0
Training loss: 1.9018466934569696
Validation loss: 2.4606286983635197

Epoch: 6| Step: 1
Training loss: 2.5880544243562915
Validation loss: 2.43986971557263

Epoch: 6| Step: 2
Training loss: 2.7288782292501956
Validation loss: 2.461655164210436

Epoch: 6| Step: 3
Training loss: 1.9879587807555654
Validation loss: 2.4578982747415306

Epoch: 6| Step: 4
Training loss: 3.42067275853328
Validation loss: 2.4591661095623176

Epoch: 6| Step: 5
Training loss: 1.9586169734106014
Validation loss: 2.4641871677314233

Epoch: 6| Step: 6
Training loss: 3.307080712186212
Validation loss: 2.4641722333249287

Epoch: 6| Step: 7
Training loss: 2.0703385261483214
Validation loss: 2.475735523027836

Epoch: 6| Step: 8
Training loss: 2.9144146172147773
Validation loss: 2.4618784824171938

Epoch: 6| Step: 9
Training loss: 2.7801665006491585
Validation loss: 2.4646403077246672

Epoch: 6| Step: 10
Training loss: 1.976265805861056
Validation loss: 2.443786931504325

Epoch: 6| Step: 11
Training loss: 2.456046339138396
Validation loss: 2.4623898623276483

Epoch: 6| Step: 12
Training loss: 3.5362912542607314
Validation loss: 2.4647189192680288

Epoch: 6| Step: 13
Training loss: 3.0918146110021136
Validation loss: 2.461153807462966

Epoch: 99| Step: 0
Training loss: 2.1063332561032695
Validation loss: 2.460746367845744

Epoch: 6| Step: 1
Training loss: 2.6787673660548483
Validation loss: 2.4511542936324613

Epoch: 6| Step: 2
Training loss: 2.3628458930416762
Validation loss: 2.4651759112203244

Epoch: 6| Step: 3
Training loss: 2.996371618165128
Validation loss: 2.4565501137258092

Epoch: 6| Step: 4
Training loss: 2.569708013906132
Validation loss: 2.459887889213602

Epoch: 6| Step: 5
Training loss: 2.8741130289934973
Validation loss: 2.4524143826030023

Epoch: 6| Step: 6
Training loss: 2.8617093922604986
Validation loss: 2.46107791018925

Epoch: 6| Step: 7
Training loss: 2.111254949298521
Validation loss: 2.4541132811885924

Epoch: 6| Step: 8
Training loss: 2.8032568042604167
Validation loss: 2.4676559638894164

Epoch: 6| Step: 9
Training loss: 1.9822082705118418
Validation loss: 2.4747548986813124

Epoch: 6| Step: 10
Training loss: 2.8774503134539935
Validation loss: 2.4673658119589583

Epoch: 6| Step: 11
Training loss: 2.8806143963635793
Validation loss: 2.468916879447038

Epoch: 6| Step: 12
Training loss: 2.7395471103918165
Validation loss: 2.4711556946815434

Epoch: 6| Step: 13
Training loss: 3.496836322369392
Validation loss: 2.4785128560564655

Epoch: 100| Step: 0
Training loss: 1.9845090205073332
Validation loss: 2.4778875220638703

Epoch: 6| Step: 1
Training loss: 2.6409640122792384
Validation loss: 2.4761138423699958

Epoch: 6| Step: 2
Training loss: 2.574199485417293
Validation loss: 2.4817467773257773

Epoch: 6| Step: 3
Training loss: 3.0428623479572194
Validation loss: 2.469433650495237

Epoch: 6| Step: 4
Training loss: 2.0988611493830764
Validation loss: 2.473143353681759

Epoch: 6| Step: 5
Training loss: 3.060154230556839
Validation loss: 2.462031797584962

Epoch: 6| Step: 6
Training loss: 2.6501147155403824
Validation loss: 2.465224515563035

Epoch: 6| Step: 7
Training loss: 3.3083885156239585
Validation loss: 2.471059746495149

Epoch: 6| Step: 8
Training loss: 2.6459491934652513
Validation loss: 2.461809231613405

Epoch: 6| Step: 9
Training loss: 2.1809824170047065
Validation loss: 2.4659490806464883

Epoch: 6| Step: 10
Training loss: 2.794118705843793
Validation loss: 2.46256782111863

Epoch: 6| Step: 11
Training loss: 2.614237839768914
Validation loss: 2.456680244347281

Epoch: 6| Step: 12
Training loss: 3.109493694244434
Validation loss: 2.45278857502718

Epoch: 6| Step: 13
Training loss: 2.1990150370862174
Validation loss: 2.471769306684101

Epoch: 101| Step: 0
Training loss: 3.0133506618552897
Validation loss: 2.4505777850080377

Epoch: 6| Step: 1
Training loss: 2.737732488087645
Validation loss: 2.44842877988666

Epoch: 6| Step: 2
Training loss: 2.3451503385003822
Validation loss: 2.4640738293967233

Epoch: 6| Step: 3
Training loss: 2.269973895557222
Validation loss: 2.4690980407612364

Epoch: 6| Step: 4
Training loss: 2.5699342953784354
Validation loss: 2.4601714124660305

Epoch: 6| Step: 5
Training loss: 2.3794378679455637
Validation loss: 2.4547332827705195

Epoch: 6| Step: 6
Training loss: 3.0354176608790695
Validation loss: 2.4751702054257256

Epoch: 6| Step: 7
Training loss: 2.3640057926978386
Validation loss: 2.4659575826131914

Epoch: 6| Step: 8
Training loss: 2.7875104587512682
Validation loss: 2.468089862673782

Epoch: 6| Step: 9
Training loss: 2.705553584540883
Validation loss: 2.4541449886415956

Epoch: 6| Step: 10
Training loss: 2.3358131809388265
Validation loss: 2.4852107638918888

Epoch: 6| Step: 11
Training loss: 2.8831604951715692
Validation loss: 2.452732426546766

Epoch: 6| Step: 12
Training loss: 3.3992311449817825
Validation loss: 2.4585623790004196

Epoch: 6| Step: 13
Training loss: 2.046374077690764
Validation loss: 2.460713777596513

Epoch: 102| Step: 0
Training loss: 2.366267362229641
Validation loss: 2.455792356577695

Epoch: 6| Step: 1
Training loss: 2.7630548981422183
Validation loss: 2.457918568726206

Epoch: 6| Step: 2
Training loss: 2.3553733996170405
Validation loss: 2.438209845246727

Epoch: 6| Step: 3
Training loss: 3.8467377006764023
Validation loss: 2.446752039639892

Epoch: 6| Step: 4
Training loss: 2.9819917599764394
Validation loss: 2.4713311399770235

Epoch: 6| Step: 5
Training loss: 2.685066185673596
Validation loss: 2.4493081616113423

Epoch: 6| Step: 6
Training loss: 2.2977079580870887
Validation loss: 2.462355283402686

Epoch: 6| Step: 7
Training loss: 3.107506271932937
Validation loss: 2.471396682127877

Epoch: 6| Step: 8
Training loss: 2.626940328006702
Validation loss: 2.46313151662292

Epoch: 6| Step: 9
Training loss: 2.088279295412712
Validation loss: 2.4580818261047437

Epoch: 6| Step: 10
Training loss: 2.6732276927816745
Validation loss: 2.46161009020252

Epoch: 6| Step: 11
Training loss: 2.2484443372843836
Validation loss: 2.467703513751759

Epoch: 6| Step: 12
Training loss: 2.591528067199639
Validation loss: 2.460358149939451

Epoch: 6| Step: 13
Training loss: 1.8824720272706368
Validation loss: 2.4610581703895993

Epoch: 103| Step: 0
Training loss: 2.3588716083627763
Validation loss: 2.474692420703312

Epoch: 6| Step: 1
Training loss: 2.469258509547977
Validation loss: 2.468013900316151

Epoch: 6| Step: 2
Training loss: 3.058704125762261
Validation loss: 2.4642199872151895

Epoch: 6| Step: 3
Training loss: 2.5782820220293257
Validation loss: 2.4719687844637965

Epoch: 6| Step: 4
Training loss: 3.391022408395905
Validation loss: 2.4831497133444578

Epoch: 6| Step: 5
Training loss: 2.1334181823347604
Validation loss: 2.4704955868948395

Epoch: 6| Step: 6
Training loss: 2.7991627974245437
Validation loss: 2.4449117096773856

Epoch: 6| Step: 7
Training loss: 3.004492574747667
Validation loss: 2.4644799288432315

Epoch: 6| Step: 8
Training loss: 2.9090431491225823
Validation loss: 2.4563083021687393

Epoch: 6| Step: 9
Training loss: 2.7113668228243477
Validation loss: 2.446730286728699

Epoch: 6| Step: 10
Training loss: 2.3764869652857548
Validation loss: 2.464716442705135

Epoch: 6| Step: 11
Training loss: 2.2943214182882614
Validation loss: 2.4639102968973488

Epoch: 6| Step: 12
Training loss: 2.226216071768296
Validation loss: 2.4783418044415337

Epoch: 6| Step: 13
Training loss: 2.4917959067041835
Validation loss: 2.442484417299264

Epoch: 104| Step: 0
Training loss: 3.2345477808998013
Validation loss: 2.4705048846960547

Epoch: 6| Step: 1
Training loss: 2.9831898518020155
Validation loss: 2.480600296368058

Epoch: 6| Step: 2
Training loss: 3.269038264618068
Validation loss: 2.4618062022823337

Epoch: 6| Step: 3
Training loss: 2.4498823565829015
Validation loss: 2.4597949658405165

Epoch: 6| Step: 4
Training loss: 2.0883974578868254
Validation loss: 2.455883353560886

Epoch: 6| Step: 5
Training loss: 2.632781891687342
Validation loss: 2.4688352770269097

Epoch: 6| Step: 6
Training loss: 2.2768836889904844
Validation loss: 2.469547991680256

Epoch: 6| Step: 7
Training loss: 2.600393955021715
Validation loss: 2.4511390162525104

Epoch: 6| Step: 8
Training loss: 2.9717072865749277
Validation loss: 2.4764284430367973

Epoch: 6| Step: 9
Training loss: 2.457097039001477
Validation loss: 2.4701290201772403

Epoch: 6| Step: 10
Training loss: 2.809346359497321
Validation loss: 2.4647862348516836

Epoch: 6| Step: 11
Training loss: 2.819859180316273
Validation loss: 2.457263678018122

Epoch: 6| Step: 12
Training loss: 1.8936711512098823
Validation loss: 2.4591234809910243

Epoch: 6| Step: 13
Training loss: 2.1648918242883086
Validation loss: 2.468828901237494

Epoch: 105| Step: 0
Training loss: 2.476933304136316
Validation loss: 2.475436794468449

Epoch: 6| Step: 1
Training loss: 1.9974719401502545
Validation loss: 2.4590584478268194

Epoch: 6| Step: 2
Training loss: 3.0006099716601566
Validation loss: 2.459642161629859

Epoch: 6| Step: 3
Training loss: 3.076088633546359
Validation loss: 2.474859642174802

Epoch: 6| Step: 4
Training loss: 2.5333171927623885
Validation loss: 2.4718375866753046

Epoch: 6| Step: 5
Training loss: 2.244165060177566
Validation loss: 2.448187331659512

Epoch: 6| Step: 6
Training loss: 2.307418188295895
Validation loss: 2.461550767007404

Epoch: 6| Step: 7
Training loss: 3.017528190323088
Validation loss: 2.4781712799888145

Epoch: 6| Step: 8
Training loss: 2.590073887422822
Validation loss: 2.4463235824298053

Epoch: 6| Step: 9
Training loss: 2.444950747315642
Validation loss: 2.4673234528575763

Epoch: 6| Step: 10
Training loss: 2.716114915700462
Validation loss: 2.457799146069499

Epoch: 6| Step: 11
Training loss: 2.82806522875266
Validation loss: 2.467082793891397

Epoch: 6| Step: 12
Training loss: 2.4634177651946216
Validation loss: 2.442689554773881

Epoch: 6| Step: 13
Training loss: 3.4453102855718507
Validation loss: 2.4536220438100833

Epoch: 106| Step: 0
Training loss: 2.430355949077231
Validation loss: 2.45167216841636

Epoch: 6| Step: 1
Training loss: 3.171626339641173
Validation loss: 2.458699477723855

Epoch: 6| Step: 2
Training loss: 2.6520630093794897
Validation loss: 2.4640089258477906

Epoch: 6| Step: 3
Training loss: 2.8146500740260927
Validation loss: 2.4582934198186157

Epoch: 6| Step: 4
Training loss: 2.729442834834456
Validation loss: 2.464298067563196

Epoch: 6| Step: 5
Training loss: 2.8835818703612652
Validation loss: 2.4619989447665693

Epoch: 6| Step: 6
Training loss: 2.214471371949486
Validation loss: 2.4665178504187275

Epoch: 6| Step: 7
Training loss: 2.5487270992909687
Validation loss: 2.454802504479441

Epoch: 6| Step: 8
Training loss: 2.662195888577269
Validation loss: 2.4641360002371506

Epoch: 6| Step: 9
Training loss: 2.3550417688638268
Validation loss: 2.462324854005136

Epoch: 6| Step: 10
Training loss: 2.416147461287804
Validation loss: 2.456624806777782

Epoch: 6| Step: 11
Training loss: 2.7672073234309007
Validation loss: 2.468256348444969

Epoch: 6| Step: 12
Training loss: 2.7473572256741328
Validation loss: 2.462563233755437

Epoch: 6| Step: 13
Training loss: 2.627967065880588
Validation loss: 2.460817098724193

Epoch: 107| Step: 0
Training loss: 2.538936670559992
Validation loss: 2.4551296692255615

Epoch: 6| Step: 1
Training loss: 2.397845863160888
Validation loss: 2.4566446395961568

Epoch: 6| Step: 2
Training loss: 2.260002367811735
Validation loss: 2.4533599956239804

Epoch: 6| Step: 3
Training loss: 2.240505105698737
Validation loss: 2.4609933518991056

Epoch: 6| Step: 4
Training loss: 2.6796136517817155
Validation loss: 2.464381957993662

Epoch: 6| Step: 5
Training loss: 2.4311595517473235
Validation loss: 2.462516180126569

Epoch: 6| Step: 6
Training loss: 2.4204619192221934
Validation loss: 2.4768971490456138

Epoch: 6| Step: 7
Training loss: 2.714998924156805
Validation loss: 2.44518575372947

Epoch: 6| Step: 8
Training loss: 4.08106625480683
Validation loss: 2.4423778004287833

Epoch: 6| Step: 9
Training loss: 2.784178317366376
Validation loss: 2.4726501183795953

Epoch: 6| Step: 10
Training loss: 2.3361193736027634
Validation loss: 2.453670421480477

Epoch: 6| Step: 11
Training loss: 2.7940067523172933
Validation loss: 2.4588918501614634

Epoch: 6| Step: 12
Training loss: 2.439732434032554
Validation loss: 2.464610863505678

Epoch: 6| Step: 13
Training loss: 2.4493491884873326
Validation loss: 2.4659513158203255

Epoch: 108| Step: 0
Training loss: 2.748702003021793
Validation loss: 2.474377655260865

Epoch: 6| Step: 1
Training loss: 1.902655480005535
Validation loss: 2.4573286167970823

Epoch: 6| Step: 2
Training loss: 2.6927188501699897
Validation loss: 2.4679210356766674

Epoch: 6| Step: 3
Training loss: 2.6104534912759405
Validation loss: 2.464207385995836

Epoch: 6| Step: 4
Training loss: 2.8648688896497188
Validation loss: 2.4494149682152417

Epoch: 6| Step: 5
Training loss: 2.9855408313714507
Validation loss: 2.4528954694222533

Epoch: 6| Step: 6
Training loss: 3.119343788634381
Validation loss: 2.472089392380598

Epoch: 6| Step: 7
Training loss: 2.007927204818709
Validation loss: 2.454325312094122

Epoch: 6| Step: 8
Training loss: 2.301297244903277
Validation loss: 2.470200749582822

Epoch: 6| Step: 9
Training loss: 2.964424917921542
Validation loss: 2.459517177383138

Epoch: 6| Step: 10
Training loss: 2.5066318287694345
Validation loss: 2.4589663719947765

Epoch: 6| Step: 11
Training loss: 2.5653708588789272
Validation loss: 2.4609697653951494

Epoch: 6| Step: 12
Training loss: 3.025454299483256
Validation loss: 2.4550834526024894

Epoch: 6| Step: 13
Training loss: 2.2265504000150935
Validation loss: 2.4667233006074003

Epoch: 109| Step: 0
Training loss: 3.1145692312544373
Validation loss: 2.457294983846312

Epoch: 6| Step: 1
Training loss: 2.5575545435484064
Validation loss: 2.451304238463409

Epoch: 6| Step: 2
Training loss: 2.6292127682794693
Validation loss: 2.4624131978561974

Epoch: 6| Step: 3
Training loss: 3.090196823221783
Validation loss: 2.4601133613501838

Epoch: 6| Step: 4
Training loss: 2.7002761911485806
Validation loss: 2.4637978562847866

Epoch: 6| Step: 5
Training loss: 3.0026716416059696
Validation loss: 2.4638758152665607

Epoch: 6| Step: 6
Training loss: 2.623711224215373
Validation loss: 2.443622589000007

Epoch: 6| Step: 7
Training loss: 1.9295659200692208
Validation loss: 2.4711057763608792

Epoch: 6| Step: 8
Training loss: 2.225852879998136
Validation loss: 2.4717717440282763

Epoch: 6| Step: 9
Training loss: 2.4350090014156205
Validation loss: 2.467492913471626

Epoch: 6| Step: 10
Training loss: 2.2518059582360475
Validation loss: 2.4585814819177925

Epoch: 6| Step: 11
Training loss: 2.4305455695053104
Validation loss: 2.456690849807068

Epoch: 6| Step: 12
Training loss: 2.8610279746049616
Validation loss: 2.4688492237492525

Epoch: 6| Step: 13
Training loss: 3.1633453686736117
Validation loss: 2.4613794011089665

Epoch: 110| Step: 0
Training loss: 2.9289361015583317
Validation loss: 2.460207111541735

Epoch: 6| Step: 1
Training loss: 3.3363248434922688
Validation loss: 2.4732417839086316

Epoch: 6| Step: 2
Training loss: 3.342270568414567
Validation loss: 2.46302011878126

Epoch: 6| Step: 3
Training loss: 2.771109794266969
Validation loss: 2.449201983618371

Epoch: 6| Step: 4
Training loss: 2.4235120593369675
Validation loss: 2.4691503762692792

Epoch: 6| Step: 5
Training loss: 2.336355420750253
Validation loss: 2.46316538628007

Epoch: 6| Step: 6
Training loss: 2.522687960106716
Validation loss: 2.4495300167400664

Epoch: 6| Step: 7
Training loss: 2.6116905674168374
Validation loss: 2.46061319537729

Epoch: 6| Step: 8
Training loss: 2.7353947944861012
Validation loss: 2.459610828350216

Epoch: 6| Step: 9
Training loss: 2.632409129951804
Validation loss: 2.4737989884480553

Epoch: 6| Step: 10
Training loss: 2.137367944932539
Validation loss: 2.471574209711917

Epoch: 6| Step: 11
Training loss: 2.7010908466358154
Validation loss: 2.463052367302992

Epoch: 6| Step: 12
Training loss: 2.105354150261295
Validation loss: 2.4632027493567383

Epoch: 6| Step: 13
Training loss: 1.7203954969406177
Validation loss: 2.4476985592484644

Epoch: 111| Step: 0
Training loss: 2.847565156293344
Validation loss: 2.4616196262090972

Epoch: 6| Step: 1
Training loss: 2.9587389350042788
Validation loss: 2.4481924177039796

Epoch: 6| Step: 2
Training loss: 3.3464113856469146
Validation loss: 2.4667892031787844

Epoch: 6| Step: 3
Training loss: 2.6006288428210578
Validation loss: 2.465368961664349

Epoch: 6| Step: 4
Training loss: 3.09099386731332
Validation loss: 2.456280841313866

Epoch: 6| Step: 5
Training loss: 2.9093735839315835
Validation loss: 2.46944281577868

Epoch: 6| Step: 6
Training loss: 2.3748860582826787
Validation loss: 2.4644216338509026

Epoch: 6| Step: 7
Training loss: 2.3938317825251096
Validation loss: 2.45474850436167

Epoch: 6| Step: 8
Training loss: 2.1529086110819704
Validation loss: 2.459694588895625

Epoch: 6| Step: 9
Training loss: 2.5105230116314714
Validation loss: 2.4577538558340564

Epoch: 6| Step: 10
Training loss: 2.2987030436048546
Validation loss: 2.461006009681065

Epoch: 6| Step: 11
Training loss: 2.7582535431568656
Validation loss: 2.4439236884298947

Epoch: 6| Step: 12
Training loss: 2.3256140359539614
Validation loss: 2.464271079102567

Epoch: 6| Step: 13
Training loss: 1.6328252764480322
Validation loss: 2.4700515294463465

Epoch: 112| Step: 0
Training loss: 2.780699600450278
Validation loss: 2.458809303006314

Epoch: 6| Step: 1
Training loss: 2.8371697052953158
Validation loss: 2.4597841371799154

Epoch: 6| Step: 2
Training loss: 2.2524269578206932
Validation loss: 2.4688922492702576

Epoch: 6| Step: 3
Training loss: 2.2662855566552507
Validation loss: 2.451479582900404

Epoch: 6| Step: 4
Training loss: 3.3659213843228564
Validation loss: 2.4510984800605433

Epoch: 6| Step: 5
Training loss: 2.8273985546204696
Validation loss: 2.4610450066156817

Epoch: 6| Step: 6
Training loss: 2.2927698312287954
Validation loss: 2.4677408829164738

Epoch: 6| Step: 7
Training loss: 2.9013694883868357
Validation loss: 2.4391408571475455

Epoch: 6| Step: 8
Training loss: 2.39450098115045
Validation loss: 2.457289156091132

Epoch: 6| Step: 9
Training loss: 2.8186297270496543
Validation loss: 2.452243206220302

Epoch: 6| Step: 10
Training loss: 1.8827467784253789
Validation loss: 2.4673803654590127

Epoch: 6| Step: 11
Training loss: 3.053376914245003
Validation loss: 2.447856032214679

Epoch: 6| Step: 12
Training loss: 2.1796473564361714
Validation loss: 2.4518769334692707

Epoch: 6| Step: 13
Training loss: 2.9069441099774718
Validation loss: 2.4713570248892998

Epoch: 113| Step: 0
Training loss: 2.9726939455868
Validation loss: 2.4516539914631186

Epoch: 6| Step: 1
Training loss: 2.3910331626963113
Validation loss: 2.4558751987786707

Epoch: 6| Step: 2
Training loss: 2.8637741518105306
Validation loss: 2.448852605375439

Epoch: 6| Step: 3
Training loss: 2.3438152049849
Validation loss: 2.4670021762118446

Epoch: 6| Step: 4
Training loss: 2.302470340102826
Validation loss: 2.468250381425675

Epoch: 6| Step: 5
Training loss: 2.7207376514881214
Validation loss: 2.4599321605296645

Epoch: 6| Step: 6
Training loss: 2.753623569129871
Validation loss: 2.441052213073208

Epoch: 6| Step: 7
Training loss: 3.2169465873820995
Validation loss: 2.4577547560142996

Epoch: 6| Step: 8
Training loss: 2.0861908833771627
Validation loss: 2.4606052042165336

Epoch: 6| Step: 9
Training loss: 3.2567106071274834
Validation loss: 2.4512647288852376

Epoch: 6| Step: 10
Training loss: 2.835103902914287
Validation loss: 2.461699638061188

Epoch: 6| Step: 11
Training loss: 2.4793057335923256
Validation loss: 2.4651479511738654

Epoch: 6| Step: 12
Training loss: 2.2372666434388178
Validation loss: 2.4656738705616723

Epoch: 6| Step: 13
Training loss: 1.7265799681181184
Validation loss: 2.4615624335692843

Epoch: 114| Step: 0
Training loss: 2.8733300044253625
Validation loss: 2.459972242683463

Epoch: 6| Step: 1
Training loss: 2.569903402034515
Validation loss: 2.4544975412743777

Epoch: 6| Step: 2
Training loss: 2.2452226359330174
Validation loss: 2.448016964531787

Epoch: 6| Step: 3
Training loss: 2.679248473568227
Validation loss: 2.45393038053269

Epoch: 6| Step: 4
Training loss: 2.811495283594709
Validation loss: 2.477279372485472

Epoch: 6| Step: 5
Training loss: 3.124430490097543
Validation loss: 2.444811467226594

Epoch: 6| Step: 6
Training loss: 3.3563190133442227
Validation loss: 2.457492074019472

Epoch: 6| Step: 7
Training loss: 2.1182266444189737
Validation loss: 2.4740830754089935

Epoch: 6| Step: 8
Training loss: 2.524595107295431
Validation loss: 2.4764408076368705

Epoch: 6| Step: 9
Training loss: 3.027900181978251
Validation loss: 2.4683648688195374

Epoch: 6| Step: 10
Training loss: 1.8957256524103225
Validation loss: 2.4615808768723335

Epoch: 6| Step: 11
Training loss: 1.354251193807469
Validation loss: 2.4629634593240044

Epoch: 6| Step: 12
Training loss: 3.1008143063044935
Validation loss: 2.4668804242856823

Epoch: 6| Step: 13
Training loss: 2.529986501816035
Validation loss: 2.4642462771306337

Epoch: 115| Step: 0
Training loss: 2.0719619801763116
Validation loss: 2.4674679932326917

Epoch: 6| Step: 1
Training loss: 2.323566659442768
Validation loss: 2.4625162613296765

Epoch: 6| Step: 2
Training loss: 2.5146677790518304
Validation loss: 2.4682330640049694

Epoch: 6| Step: 3
Training loss: 3.3976188287452636
Validation loss: 2.4621365304911107

Epoch: 6| Step: 4
Training loss: 3.1034596980092988
Validation loss: 2.4639706385900975

Epoch: 6| Step: 5
Training loss: 2.460764464092754
Validation loss: 2.4615302530124517

Epoch: 6| Step: 6
Training loss: 3.170155928474157
Validation loss: 2.469995232638479

Epoch: 6| Step: 7
Training loss: 2.1944415445214704
Validation loss: 2.4723580525385813

Epoch: 6| Step: 8
Training loss: 2.6259912708192585
Validation loss: 2.4682743418549795

Epoch: 6| Step: 9
Training loss: 3.03334083416469
Validation loss: 2.47057332103991

Epoch: 6| Step: 10
Training loss: 1.8918928828936452
Validation loss: 2.4598779218045914

Epoch: 6| Step: 11
Training loss: 2.4297587393469966
Validation loss: 2.4655751896788116

Epoch: 6| Step: 12
Training loss: 2.9321213848389167
Validation loss: 2.46336706980992

Epoch: 6| Step: 13
Training loss: 2.180683305025938
Validation loss: 2.46181981810172

Epoch: 116| Step: 0
Training loss: 3.096414054802635
Validation loss: 2.4600906516664387

Epoch: 6| Step: 1
Training loss: 2.2693830169225153
Validation loss: 2.448282906061775

Epoch: 6| Step: 2
Training loss: 2.654251244203984
Validation loss: 2.4704293027277044

Epoch: 6| Step: 3
Training loss: 2.4399458522729405
Validation loss: 2.453229503392567

Epoch: 6| Step: 4
Training loss: 2.912119199555862
Validation loss: 2.462069767214363

Epoch: 6| Step: 5
Training loss: 2.5953987121042656
Validation loss: 2.4417434736439914

Epoch: 6| Step: 6
Training loss: 2.637775576689753
Validation loss: 2.464426384716313

Epoch: 6| Step: 7
Training loss: 2.8875402538581807
Validation loss: 2.4564604391987492

Epoch: 6| Step: 8
Training loss: 2.9172994926217086
Validation loss: 2.4481021406765042

Epoch: 6| Step: 9
Training loss: 2.8020347014299407
Validation loss: 2.453149932699093

Epoch: 6| Step: 10
Training loss: 1.7472569220445775
Validation loss: 2.4697298167096844

Epoch: 6| Step: 11
Training loss: 2.7412198090010524
Validation loss: 2.4631444808611285

Epoch: 6| Step: 12
Training loss: 2.2778992801400695
Validation loss: 2.457816393064334

Epoch: 6| Step: 13
Training loss: 2.5070052229070257
Validation loss: 2.448977061946996

Epoch: 117| Step: 0
Training loss: 2.435296554366375
Validation loss: 2.452877373127456

Epoch: 6| Step: 1
Training loss: 2.7287388725945085
Validation loss: 2.4336680131001485

Epoch: 6| Step: 2
Training loss: 3.301093272974122
Validation loss: 2.4542523049833394

Epoch: 6| Step: 3
Training loss: 2.2877947841459108
Validation loss: 2.445243208840889

Epoch: 6| Step: 4
Training loss: 2.6463817618771386
Validation loss: 2.459385972060362

Epoch: 6| Step: 5
Training loss: 3.185186107234509
Validation loss: 2.4535675621517856

Epoch: 6| Step: 6
Training loss: 2.9639151303783056
Validation loss: 2.451320827841153

Epoch: 6| Step: 7
Training loss: 1.7883677903713473
Validation loss: 2.460548729829075

Epoch: 6| Step: 8
Training loss: 2.143650339551504
Validation loss: 2.440929420421239

Epoch: 6| Step: 9
Training loss: 2.6016447953504946
Validation loss: 2.4522483497144982

Epoch: 6| Step: 10
Training loss: 2.5170219760036954
Validation loss: 2.4642468087415508

Epoch: 6| Step: 11
Training loss: 2.811859736149947
Validation loss: 2.4606933139248737

Epoch: 6| Step: 12
Training loss: 2.637263308205197
Validation loss: 2.4585762004764975

Epoch: 6| Step: 13
Training loss: 1.9475514958714302
Validation loss: 2.4647337765128783

Epoch: 118| Step: 0
Training loss: 2.9841838995180203
Validation loss: 2.4712735953094045

Epoch: 6| Step: 1
Training loss: 2.843643354155856
Validation loss: 2.4523161780123104

Epoch: 6| Step: 2
Training loss: 2.799083924300431
Validation loss: 2.4641758225844907

Epoch: 6| Step: 3
Training loss: 1.7240623911390327
Validation loss: 2.4572390624912424

Epoch: 6| Step: 4
Training loss: 2.8329574859262237
Validation loss: 2.46102953958616

Epoch: 6| Step: 5
Training loss: 2.3297405398066546
Validation loss: 2.4253906508331076

Epoch: 6| Step: 6
Training loss: 3.028198279093508
Validation loss: 2.4532783700196306

Epoch: 6| Step: 7
Training loss: 2.62808101222617
Validation loss: 2.452471571150589

Epoch: 6| Step: 8
Training loss: 2.7386188815899657
Validation loss: 2.4654574454747307

Epoch: 6| Step: 9
Training loss: 1.9960966166197616
Validation loss: 2.449880989939481

Epoch: 6| Step: 10
Training loss: 2.3448586448572057
Validation loss: 2.446970732041008

Epoch: 6| Step: 11
Training loss: 2.89571217528179
Validation loss: 2.4741981922877296

Epoch: 6| Step: 12
Training loss: 2.5994696919941087
Validation loss: 2.4510188427996895

Epoch: 6| Step: 13
Training loss: 2.4730083582482734
Validation loss: 2.4546538699637566

Epoch: 119| Step: 0
Training loss: 2.692227308938754
Validation loss: 2.461450177329418

Epoch: 6| Step: 1
Training loss: 2.513469459665347
Validation loss: 2.4421458877302835

Epoch: 6| Step: 2
Training loss: 3.2734313819117946
Validation loss: 2.448277347954129

Epoch: 6| Step: 3
Training loss: 2.9062196463363645
Validation loss: 2.450402458168375

Epoch: 6| Step: 4
Training loss: 2.972243331723302
Validation loss: 2.454112379672639

Epoch: 6| Step: 5
Training loss: 2.841555241015761
Validation loss: 2.4499947117373173

Epoch: 6| Step: 6
Training loss: 2.064393532577699
Validation loss: 2.4417420977205846

Epoch: 6| Step: 7
Training loss: 2.6979598602804167
Validation loss: 2.4617850083155797

Epoch: 6| Step: 8
Training loss: 2.6641164645549273
Validation loss: 2.4411695449768187

Epoch: 6| Step: 9
Training loss: 2.370548292387854
Validation loss: 2.4513697921271085

Epoch: 6| Step: 10
Training loss: 2.302480487885777
Validation loss: 2.463450704199216

Epoch: 6| Step: 11
Training loss: 2.0682028332079887
Validation loss: 2.4587386978783585

Epoch: 6| Step: 12
Training loss: 2.730203028943846
Validation loss: 2.463985713638053

Epoch: 6| Step: 13
Training loss: 2.1026127683173645
Validation loss: 2.4554996987233313

Epoch: 120| Step: 0
Training loss: 2.6205489250438703
Validation loss: 2.458903538722248

Epoch: 6| Step: 1
Training loss: 2.5578526479172
Validation loss: 2.445379406272099

Epoch: 6| Step: 2
Training loss: 2.3727606204342537
Validation loss: 2.462041188784059

Epoch: 6| Step: 3
Training loss: 3.0721267115865283
Validation loss: 2.463250855603932

Epoch: 6| Step: 4
Training loss: 2.7197489163971382
Validation loss: 2.4567930242786313

Epoch: 6| Step: 5
Training loss: 2.4845529168678033
Validation loss: 2.4628208301409966

Epoch: 6| Step: 6
Training loss: 2.7468179152180423
Validation loss: 2.462140398642759

Epoch: 6| Step: 7
Training loss: 2.6073675600031043
Validation loss: 2.4557014762773473

Epoch: 6| Step: 8
Training loss: 2.546055199611307
Validation loss: 2.4554312703247443

Epoch: 6| Step: 9
Training loss: 2.4658568608058737
Validation loss: 2.452153409736719

Epoch: 6| Step: 10
Training loss: 2.4170285863736516
Validation loss: 2.447336956039668

Epoch: 6| Step: 11
Training loss: 2.510868480329604
Validation loss: 2.4515325946531914

Epoch: 6| Step: 12
Training loss: 1.9137769836476894
Validation loss: 2.460929009858653

Epoch: 6| Step: 13
Training loss: 3.934085515941503
Validation loss: 2.455982856007504

Epoch: 121| Step: 0
Training loss: 2.567180917258945
Validation loss: 2.4614010693430473

Epoch: 6| Step: 1
Training loss: 2.750458419044465
Validation loss: 2.4485401861433833

Epoch: 6| Step: 2
Training loss: 2.118968707024164
Validation loss: 2.45438946793879

Epoch: 6| Step: 3
Training loss: 2.467366180810773
Validation loss: 2.454438809171827

Epoch: 6| Step: 4
Training loss: 2.0854185223134487
Validation loss: 2.4664223054445293

Epoch: 6| Step: 5
Training loss: 2.5515066520339578
Validation loss: 2.453061888560711

Epoch: 6| Step: 6
Training loss: 2.3922962007050796
Validation loss: 2.456817878014049

Epoch: 6| Step: 7
Training loss: 2.372713996553765
Validation loss: 2.447947147749504

Epoch: 6| Step: 8
Training loss: 3.197479184987407
Validation loss: 2.450648622120047

Epoch: 6| Step: 9
Training loss: 2.7300830399456313
Validation loss: 2.4706950751633974

Epoch: 6| Step: 10
Training loss: 2.8799857984298662
Validation loss: 2.4526176484478004

Epoch: 6| Step: 11
Training loss: 2.5418567482166634
Validation loss: 2.4445501703490957

Epoch: 6| Step: 12
Training loss: 2.956355190220246
Validation loss: 2.4497739718580385

Epoch: 6| Step: 13
Training loss: 3.0177472014728255
Validation loss: 2.454400202370933

Epoch: 122| Step: 0
Training loss: 3.2028951422861893
Validation loss: 2.455928711694061

Epoch: 6| Step: 1
Training loss: 3.4000021654009935
Validation loss: 2.461814534235513

Epoch: 6| Step: 2
Training loss: 2.8647533759308543
Validation loss: 2.4736553045843803

Epoch: 6| Step: 3
Training loss: 2.4585844870684657
Validation loss: 2.4704763182318903

Epoch: 6| Step: 4
Training loss: 2.513176715871806
Validation loss: 2.457481874691966

Epoch: 6| Step: 5
Training loss: 2.0400514506415393
Validation loss: 2.462536199734735

Epoch: 6| Step: 6
Training loss: 2.514435196510817
Validation loss: 2.452447341340739

Epoch: 6| Step: 7
Training loss: 1.7717827290186787
Validation loss: 2.4665668378244052

Epoch: 6| Step: 8
Training loss: 2.3244462615144403
Validation loss: 2.45685133175921

Epoch: 6| Step: 9
Training loss: 3.1215153911845994
Validation loss: 2.459320509064135

Epoch: 6| Step: 10
Training loss: 2.50951634219196
Validation loss: 2.4697113948160285

Epoch: 6| Step: 11
Training loss: 2.367913364629833
Validation loss: 2.464395639682872

Epoch: 6| Step: 12
Training loss: 2.7717939638078284
Validation loss: 2.4568353896351596

Epoch: 6| Step: 13
Training loss: 2.256783960619551
Validation loss: 2.46034788173953

Epoch: 123| Step: 0
Training loss: 2.1124474648709253
Validation loss: 2.4529884540215954

Epoch: 6| Step: 1
Training loss: 2.2725014348964545
Validation loss: 2.4589654941520274

Epoch: 6| Step: 2
Training loss: 2.5175828600736114
Validation loss: 2.4622795574736105

Epoch: 6| Step: 3
Training loss: 2.5028288095709446
Validation loss: 2.4483596366054043

Epoch: 6| Step: 4
Training loss: 2.9028198344597635
Validation loss: 2.4499778449802387

Epoch: 6| Step: 5
Training loss: 2.5661503912486054
Validation loss: 2.450358333253806

Epoch: 6| Step: 6
Training loss: 2.7691781877554886
Validation loss: 2.4688569660100694

Epoch: 6| Step: 7
Training loss: 2.686753013949196
Validation loss: 2.47177182077865

Epoch: 6| Step: 8
Training loss: 3.206719293882884
Validation loss: 2.4610242915242884

Epoch: 6| Step: 9
Training loss: 2.157918754243206
Validation loss: 2.4600048969472503

Epoch: 6| Step: 10
Training loss: 2.514153091382213
Validation loss: 2.453666790732956

Epoch: 6| Step: 11
Training loss: 2.9595167631617745
Validation loss: 2.4576795288061715

Epoch: 6| Step: 12
Training loss: 2.8383805442134658
Validation loss: 2.4596284868526244

Epoch: 6| Step: 13
Training loss: 2.2395799740315163
Validation loss: 2.4670760498735205

Epoch: 124| Step: 0
Training loss: 2.0832032989610605
Validation loss: 2.468245638959321

Epoch: 6| Step: 1
Training loss: 2.2929858023482197
Validation loss: 2.4580457139183607

Epoch: 6| Step: 2
Training loss: 2.2349635062769253
Validation loss: 2.440992887553172

Epoch: 6| Step: 3
Training loss: 2.799024725409218
Validation loss: 2.4597894056054406

Epoch: 6| Step: 4
Training loss: 3.166997758388178
Validation loss: 2.454087040984487

Epoch: 6| Step: 5
Training loss: 2.3722127319650363
Validation loss: 2.4505615666890224

Epoch: 6| Step: 6
Training loss: 2.512267247137298
Validation loss: 2.463803033926437

Epoch: 6| Step: 7
Training loss: 2.1778996836781825
Validation loss: 2.445103591025287

Epoch: 6| Step: 8
Training loss: 3.368793148974661
Validation loss: 2.4563996197494338

Epoch: 6| Step: 9
Training loss: 1.9962914176757582
Validation loss: 2.4604113267446044

Epoch: 6| Step: 10
Training loss: 2.9432995947707457
Validation loss: 2.447205950411415

Epoch: 6| Step: 11
Training loss: 3.040247512077328
Validation loss: 2.4580434465246825

Epoch: 6| Step: 12
Training loss: 2.7508128438758734
Validation loss: 2.4414473902363696

Epoch: 6| Step: 13
Training loss: 2.4215209702177063
Validation loss: 2.4626542090505334

Epoch: 125| Step: 0
Training loss: 2.6984279153076014
Validation loss: 2.4481128555536094

Epoch: 6| Step: 1
Training loss: 2.341044377443675
Validation loss: 2.4526178334600774

Epoch: 6| Step: 2
Training loss: 2.03946012515109
Validation loss: 2.4609119128127426

Epoch: 6| Step: 3
Training loss: 3.2277306758079676
Validation loss: 2.4511687953936003

Epoch: 6| Step: 4
Training loss: 2.130771764532869
Validation loss: 2.458555680431362

Epoch: 6| Step: 5
Training loss: 1.9754128712658756
Validation loss: 2.440591150475833

Epoch: 6| Step: 6
Training loss: 2.750539900059857
Validation loss: 2.4477680772984542

Epoch: 6| Step: 7
Training loss: 2.95364972655075
Validation loss: 2.457168428936753

Epoch: 6| Step: 8
Training loss: 2.9223443128692046
Validation loss: 2.4575510847010817

Epoch: 6| Step: 9
Training loss: 2.462189661599508
Validation loss: 2.446756573349304

Epoch: 6| Step: 10
Training loss: 2.8476863906583914
Validation loss: 2.459742816386508

Epoch: 6| Step: 11
Training loss: 2.2669095311296963
Validation loss: 2.455341126850909

Epoch: 6| Step: 12
Training loss: 2.9147377356822446
Validation loss: 2.433085229637232

Epoch: 6| Step: 13
Training loss: 2.705112322098825
Validation loss: 2.4604217634759276

Epoch: 126| Step: 0
Training loss: 2.2537838590239794
Validation loss: 2.447017861441408

Epoch: 6| Step: 1
Training loss: 2.5435813716697604
Validation loss: 2.465352584841611

Epoch: 6| Step: 2
Training loss: 2.3718982318449084
Validation loss: 2.4467380843119932

Epoch: 6| Step: 3
Training loss: 2.5079867578731823
Validation loss: 2.454323919722084

Epoch: 6| Step: 4
Training loss: 1.8252143185331664
Validation loss: 2.453929195833562

Epoch: 6| Step: 5
Training loss: 2.27162208041318
Validation loss: 2.461427709635903

Epoch: 6| Step: 6
Training loss: 3.4245416710471317
Validation loss: 2.459833433776779

Epoch: 6| Step: 7
Training loss: 2.300713581845262
Validation loss: 2.447024110196388

Epoch: 6| Step: 8
Training loss: 2.989512231138544
Validation loss: 2.44165220745268

Epoch: 6| Step: 9
Training loss: 3.0672231570098485
Validation loss: 2.4467611929673794

Epoch: 6| Step: 10
Training loss: 2.790325744235187
Validation loss: 2.4633314867776064

Epoch: 6| Step: 11
Training loss: 3.124195453073304
Validation loss: 2.444412530794511

Epoch: 6| Step: 12
Training loss: 2.427104791592765
Validation loss: 2.458001281382033

Epoch: 6| Step: 13
Training loss: 1.7956514256063465
Validation loss: 2.463467060884515

Epoch: 127| Step: 0
Training loss: 2.428276609108657
Validation loss: 2.45345698918027

Epoch: 6| Step: 1
Training loss: 2.8189040283613878
Validation loss: 2.439011222328348

Epoch: 6| Step: 2
Training loss: 2.3710998836733372
Validation loss: 2.454478625911855

Epoch: 6| Step: 3
Training loss: 2.6464841047336116
Validation loss: 2.457660928964893

Epoch: 6| Step: 4
Training loss: 2.508410040948791
Validation loss: 2.4530994786777565

Epoch: 6| Step: 5
Training loss: 2.1204021302790177
Validation loss: 2.4590827761716736

Epoch: 6| Step: 6
Training loss: 2.6528904931409083
Validation loss: 2.471197798248332

Epoch: 6| Step: 7
Training loss: 2.8276893743883122
Validation loss: 2.460850201718784

Epoch: 6| Step: 8
Training loss: 2.5467834456200387
Validation loss: 2.4411068196755696

Epoch: 6| Step: 9
Training loss: 2.899966430469767
Validation loss: 2.4522711837712814

Epoch: 6| Step: 10
Training loss: 2.882186532225659
Validation loss: 2.43984328120194

Epoch: 6| Step: 11
Training loss: 2.8212427523025085
Validation loss: 2.4676947175637944

Epoch: 6| Step: 12
Training loss: 2.542042740895461
Validation loss: 2.4566540712165112

Epoch: 6| Step: 13
Training loss: 2.160695345006298
Validation loss: 2.45783133479903

Epoch: 128| Step: 0
Training loss: 2.8493478196126474
Validation loss: 2.444317387151301

Epoch: 6| Step: 1
Training loss: 2.0531889233443295
Validation loss: 2.4510267417935876

Epoch: 6| Step: 2
Training loss: 1.9302787338349376
Validation loss: 2.442300725549205

Epoch: 6| Step: 3
Training loss: 2.9631719285957687
Validation loss: 2.4625980315000255

Epoch: 6| Step: 4
Training loss: 2.799844029033414
Validation loss: 2.4529331880280894

Epoch: 6| Step: 5
Training loss: 2.610277031413543
Validation loss: 2.4584508460733434

Epoch: 6| Step: 6
Training loss: 2.595149478465985
Validation loss: 2.4536200345833676

Epoch: 6| Step: 7
Training loss: 2.570825960302265
Validation loss: 2.433366139817709

Epoch: 6| Step: 8
Training loss: 2.577734437894235
Validation loss: 2.449706937587436

Epoch: 6| Step: 9
Training loss: 2.3961418174397457
Validation loss: 2.4437653693536294

Epoch: 6| Step: 10
Training loss: 3.3658267499974706
Validation loss: 2.432488671012841

Epoch: 6| Step: 11
Training loss: 2.5304257027950303
Validation loss: 2.4459137793085537

Epoch: 6| Step: 12
Training loss: 2.1423686924049226
Validation loss: 2.46775329208415

Epoch: 6| Step: 13
Training loss: 3.0596960816892724
Validation loss: 2.446524235858432

Epoch: 129| Step: 0
Training loss: 2.805011273665731
Validation loss: 2.460383279209947

Epoch: 6| Step: 1
Training loss: 2.2455961787526637
Validation loss: 2.434515167552001

Epoch: 6| Step: 2
Training loss: 2.1509333868845064
Validation loss: 2.4615698029919466

Epoch: 6| Step: 3
Training loss: 2.652888785586649
Validation loss: 2.457616421667668

Epoch: 6| Step: 4
Training loss: 2.831084668045808
Validation loss: 2.4410997371434218

Epoch: 6| Step: 5
Training loss: 2.1550642429697784
Validation loss: 2.4751934442861443

Epoch: 6| Step: 6
Training loss: 2.971031998407133
Validation loss: 2.4296540985169752

Epoch: 6| Step: 7
Training loss: 2.754770735739734
Validation loss: 2.4461819008994494

Epoch: 6| Step: 8
Training loss: 2.98031579821335
Validation loss: 2.450868545783984

Epoch: 6| Step: 9
Training loss: 1.8660989565067185
Validation loss: 2.4510065779713743

Epoch: 6| Step: 10
Training loss: 2.6833106812020535
Validation loss: 2.4455937542228643

Epoch: 6| Step: 11
Training loss: 2.9387996314695872
Validation loss: 2.455265328680716

Epoch: 6| Step: 12
Training loss: 2.9294347221678914
Validation loss: 2.45791091091398

Epoch: 6| Step: 13
Training loss: 1.8676006546369048
Validation loss: 2.451367612680549

Epoch: 130| Step: 0
Training loss: 2.228391304526947
Validation loss: 2.4472793722623045

Epoch: 6| Step: 1
Training loss: 3.399739333987607
Validation loss: 2.44576128077375

Epoch: 6| Step: 2
Training loss: 2.599360361647336
Validation loss: 2.4514342914688396

Epoch: 6| Step: 3
Training loss: 1.7479246640240294
Validation loss: 2.447338324103729

Epoch: 6| Step: 4
Training loss: 2.6665115510013333
Validation loss: 2.458131798237949

Epoch: 6| Step: 5
Training loss: 2.6366283259957894
Validation loss: 2.4495050304495245

Epoch: 6| Step: 6
Training loss: 1.8184971741808296
Validation loss: 2.4518310016270943

Epoch: 6| Step: 7
Training loss: 2.292418015739433
Validation loss: 2.4487373193034796

Epoch: 6| Step: 8
Training loss: 2.7594968498863106
Validation loss: 2.4502889509454273

Epoch: 6| Step: 9
Training loss: 2.999331081837509
Validation loss: 2.450221865370781

Epoch: 6| Step: 10
Training loss: 2.7918777575798415
Validation loss: 2.446728160777532

Epoch: 6| Step: 11
Training loss: 2.4591885603892427
Validation loss: 2.4475150074448884

Epoch: 6| Step: 12
Training loss: 2.824766915757287
Validation loss: 2.4430914494290574

Epoch: 6| Step: 13
Training loss: 3.0810563899064234
Validation loss: 2.4505553588618016

Epoch: 131| Step: 0
Training loss: 2.5565965151788346
Validation loss: 2.464540618062257

Epoch: 6| Step: 1
Training loss: 2.830420474031103
Validation loss: 2.4520342426462127

Epoch: 6| Step: 2
Training loss: 2.231946999812752
Validation loss: 2.4527456798780634

Epoch: 6| Step: 3
Training loss: 3.6214627074876296
Validation loss: 2.4725478049588454

Epoch: 6| Step: 4
Training loss: 2.6652902587506513
Validation loss: 2.4481203670398655

Epoch: 6| Step: 5
Training loss: 2.201750197846209
Validation loss: 2.456711200724481

Epoch: 6| Step: 6
Training loss: 2.386388278189915
Validation loss: 2.4488153421278778

Epoch: 6| Step: 7
Training loss: 2.619854925667253
Validation loss: 2.4561655495485395

Epoch: 6| Step: 8
Training loss: 2.2960284483074274
Validation loss: 2.4587607981050916

Epoch: 6| Step: 9
Training loss: 2.969511556823512
Validation loss: 2.448256937402357

Epoch: 6| Step: 10
Training loss: 2.6112673173899013
Validation loss: 2.4629528184211797

Epoch: 6| Step: 11
Training loss: 2.5161619383424787
Validation loss: 2.4408013279471796

Epoch: 6| Step: 12
Training loss: 2.324469031954949
Validation loss: 2.4724789214218923

Epoch: 6| Step: 13
Training loss: 1.6912581663082578
Validation loss: 2.4499441644845183

Epoch: 132| Step: 0
Training loss: 2.9403331972606583
Validation loss: 2.4476531847604117

Epoch: 6| Step: 1
Training loss: 2.728717728204232
Validation loss: 2.4470482774164437

Epoch: 6| Step: 2
Training loss: 3.0167999350544497
Validation loss: 2.4532248713971385

Epoch: 6| Step: 3
Training loss: 2.304469004473292
Validation loss: 2.4515455961770427

Epoch: 6| Step: 4
Training loss: 3.3733024566604355
Validation loss: 2.44557692105794

Epoch: 6| Step: 5
Training loss: 2.4232264531985956
Validation loss: 2.4677593267920317

Epoch: 6| Step: 6
Training loss: 2.5930053377342968
Validation loss: 2.458831183647049

Epoch: 6| Step: 7
Training loss: 2.71650059042211
Validation loss: 2.459204936561978

Epoch: 6| Step: 8
Training loss: 1.5189421230035949
Validation loss: 2.444408933490971

Epoch: 6| Step: 9
Training loss: 1.7763651742446673
Validation loss: 2.4542950306965667

Epoch: 6| Step: 10
Training loss: 1.8565181294487603
Validation loss: 2.4506101397264417

Epoch: 6| Step: 11
Training loss: 3.231362824527037
Validation loss: 2.450863971564375

Epoch: 6| Step: 12
Training loss: 2.738795255341953
Validation loss: 2.4613612604300164

Epoch: 6| Step: 13
Training loss: 2.3118342910569174
Validation loss: 2.4642867328098434

Epoch: 133| Step: 0
Training loss: 3.2482899788758446
Validation loss: 2.4632563518249366

Epoch: 6| Step: 1
Training loss: 2.000641124008495
Validation loss: 2.456561484698279

Epoch: 6| Step: 2
Training loss: 3.075236557956174
Validation loss: 2.473549153934982

Epoch: 6| Step: 3
Training loss: 2.4655330316871797
Validation loss: 2.4599610109713543

Epoch: 6| Step: 4
Training loss: 2.399767876367028
Validation loss: 2.455412939529991

Epoch: 6| Step: 5
Training loss: 2.425379918019983
Validation loss: 2.466602526966874

Epoch: 6| Step: 6
Training loss: 3.3000867485435283
Validation loss: 2.447868035295293

Epoch: 6| Step: 7
Training loss: 2.1873973277383723
Validation loss: 2.4460480025610414

Epoch: 6| Step: 8
Training loss: 2.6904821484222867
Validation loss: 2.4452758847127782

Epoch: 6| Step: 9
Training loss: 2.231335686926982
Validation loss: 2.4488876864023608

Epoch: 6| Step: 10
Training loss: 2.1129200850619396
Validation loss: 2.4397856579464823

Epoch: 6| Step: 11
Training loss: 2.85935365429191
Validation loss: 2.4737550658229104

Epoch: 6| Step: 12
Training loss: 2.711214254802286
Validation loss: 2.4479562641432944

Epoch: 6| Step: 13
Training loss: 2.00014161562224
Validation loss: 2.4468036733467207

Epoch: 134| Step: 0
Training loss: 2.534897048936318
Validation loss: 2.4549331558932086

Epoch: 6| Step: 1
Training loss: 2.469176822898526
Validation loss: 2.4389367435164453

Epoch: 6| Step: 2
Training loss: 2.1251357540075944
Validation loss: 2.452747329222957

Epoch: 6| Step: 3
Training loss: 2.3205702924251574
Validation loss: 2.4499212271185953

Epoch: 6| Step: 4
Training loss: 2.954566401960902
Validation loss: 2.453082181798393

Epoch: 6| Step: 5
Training loss: 2.726400725803816
Validation loss: 2.4623489439324926

Epoch: 6| Step: 6
Training loss: 2.084801067860843
Validation loss: 2.4348498878972022

Epoch: 6| Step: 7
Training loss: 2.910883337756818
Validation loss: 2.454614804913265

Epoch: 6| Step: 8
Training loss: 2.71366213325497
Validation loss: 2.44343163635001

Epoch: 6| Step: 9
Training loss: 2.3376224016587486
Validation loss: 2.4453671246910136

Epoch: 6| Step: 10
Training loss: 2.6291901887142735
Validation loss: 2.4596532181319644

Epoch: 6| Step: 11
Training loss: 2.626900030684403
Validation loss: 2.4548375897684385

Epoch: 6| Step: 12
Training loss: 3.0685794200219787
Validation loss: 2.4444370971947134

Epoch: 6| Step: 13
Training loss: 2.3168825428595268
Validation loss: 2.4637476598583947

Epoch: 135| Step: 0
Training loss: 2.5028513860045316
Validation loss: 2.4527664429380454

Epoch: 6| Step: 1
Training loss: 2.932059261311207
Validation loss: 2.461777278179012

Epoch: 6| Step: 2
Training loss: 2.784407634094321
Validation loss: 2.4527059123527195

Epoch: 6| Step: 3
Training loss: 2.3397642704833888
Validation loss: 2.4507338333750925

Epoch: 6| Step: 4
Training loss: 2.482729096689288
Validation loss: 2.4437052316212813

Epoch: 6| Step: 5
Training loss: 2.183053210782082
Validation loss: 2.44596942715071

Epoch: 6| Step: 6
Training loss: 2.698533055259763
Validation loss: 2.4508078705193386

Epoch: 6| Step: 7
Training loss: 2.5654146319623887
Validation loss: 2.4502569053449954

Epoch: 6| Step: 8
Training loss: 2.7113800127400673
Validation loss: 2.444482129710138

Epoch: 6| Step: 9
Training loss: 1.6382292193953696
Validation loss: 2.4643646258724416

Epoch: 6| Step: 10
Training loss: 2.4220699108816497
Validation loss: 2.451709154539586

Epoch: 6| Step: 11
Training loss: 2.691751355550401
Validation loss: 2.4614093985014804

Epoch: 6| Step: 12
Training loss: 3.314679706226622
Validation loss: 2.449275721135508

Epoch: 6| Step: 13
Training loss: 2.623426783382482
Validation loss: 2.4576145398404483

Epoch: 136| Step: 0
Training loss: 2.305780287466658
Validation loss: 2.447011011312514

Epoch: 6| Step: 1
Training loss: 2.9473735367400726
Validation loss: 2.4392538699732937

Epoch: 6| Step: 2
Training loss: 2.973515109534239
Validation loss: 2.448169313657234

Epoch: 6| Step: 3
Training loss: 2.8532355430221394
Validation loss: 2.4664565035773576

Epoch: 6| Step: 4
Training loss: 2.4176164163924563
Validation loss: 2.460533238841845

Epoch: 6| Step: 5
Training loss: 2.9923615168474824
Validation loss: 2.453700551775924

Epoch: 6| Step: 6
Training loss: 2.9253343309570576
Validation loss: 2.4709721226325247

Epoch: 6| Step: 7
Training loss: 2.6675526418738147
Validation loss: 2.445415367938364

Epoch: 6| Step: 8
Training loss: 2.7209452393337283
Validation loss: 2.4528908535184524

Epoch: 6| Step: 9
Training loss: 1.9961141148552946
Validation loss: 2.4410403770707445

Epoch: 6| Step: 10
Training loss: 1.9974002034032994
Validation loss: 2.4486709811444936

Epoch: 6| Step: 11
Training loss: 2.458267555865452
Validation loss: 2.4554510334347466

Epoch: 6| Step: 12
Training loss: 2.046127648817426
Validation loss: 2.4416800732699353

Epoch: 6| Step: 13
Training loss: 3.0065729931310337
Validation loss: 2.444370990530824

Epoch: 137| Step: 0
Training loss: 2.499213858024577
Validation loss: 2.457072471887025

Epoch: 6| Step: 1
Training loss: 2.8079478405838407
Validation loss: 2.433107374274579

Epoch: 6| Step: 2
Training loss: 2.3430853346272857
Validation loss: 2.444736615108999

Epoch: 6| Step: 3
Training loss: 2.883744913467928
Validation loss: 2.459497396392685

Epoch: 6| Step: 4
Training loss: 3.0027056255331455
Validation loss: 2.4458039588070593

Epoch: 6| Step: 5
Training loss: 2.5706439050989753
Validation loss: 2.4407655850423375

Epoch: 6| Step: 6
Training loss: 2.240635883299547
Validation loss: 2.4419717260321656

Epoch: 6| Step: 7
Training loss: 2.806847210994774
Validation loss: 2.432001744340874

Epoch: 6| Step: 8
Training loss: 2.261566104250876
Validation loss: 2.442056090009886

Epoch: 6| Step: 9
Training loss: 2.409809205033271
Validation loss: 2.4527081543690255

Epoch: 6| Step: 10
Training loss: 2.472793261933935
Validation loss: 2.449296261359888

Epoch: 6| Step: 11
Training loss: 2.6728936646072103
Validation loss: 2.422038129644288

Epoch: 6| Step: 12
Training loss: 2.338352549721609
Validation loss: 2.443911707412063

Epoch: 6| Step: 13
Training loss: 2.8171929095444286
Validation loss: 2.4532198198212574

Epoch: 138| Step: 0
Training loss: 2.6413547026809474
Validation loss: 2.452385647750061

Epoch: 6| Step: 1
Training loss: 2.1031544834921436
Validation loss: 2.432259567170843

Epoch: 6| Step: 2
Training loss: 2.323992344704132
Validation loss: 2.445495383709954

Epoch: 6| Step: 3
Training loss: 2.454425734191578
Validation loss: 2.459797058611775

Epoch: 6| Step: 4
Training loss: 2.4934240639679923
Validation loss: 2.459085713988597

Epoch: 6| Step: 5
Training loss: 2.8645408118588875
Validation loss: 2.4306063988246547

Epoch: 6| Step: 6
Training loss: 2.5839892651892975
Validation loss: 2.452341697032453

Epoch: 6| Step: 7
Training loss: 3.042325893363606
Validation loss: 2.4304683461831385

Epoch: 6| Step: 8
Training loss: 2.4926488562912876
Validation loss: 2.445224840462451

Epoch: 6| Step: 9
Training loss: 2.9319032961834512
Validation loss: 2.4520707120584593

Epoch: 6| Step: 10
Training loss: 1.8201097367289016
Validation loss: 2.447215569259412

Epoch: 6| Step: 11
Training loss: 2.275205363970951
Validation loss: 2.4450672546547723

Epoch: 6| Step: 12
Training loss: 3.377076252093099
Validation loss: 2.4394327814394523

Epoch: 6| Step: 13
Training loss: 2.0018755939636397
Validation loss: 2.4654259536984378

Epoch: 139| Step: 0
Training loss: 1.937101015417912
Validation loss: 2.4329276478165607

Epoch: 6| Step: 1
Training loss: 2.4717230941837496
Validation loss: 2.4455235673989457

Epoch: 6| Step: 2
Training loss: 2.6408762614776706
Validation loss: 2.4650793642091124

Epoch: 6| Step: 3
Training loss: 2.7216276527251346
Validation loss: 2.4461171137160282

Epoch: 6| Step: 4
Training loss: 2.5853136122516833
Validation loss: 2.4630470309163397

Epoch: 6| Step: 5
Training loss: 2.925064223146851
Validation loss: 2.4589258428567997

Epoch: 6| Step: 6
Training loss: 2.6051984243548953
Validation loss: 2.4398346966420217

Epoch: 6| Step: 7
Training loss: 2.5450594931433166
Validation loss: 2.445267846574758

Epoch: 6| Step: 8
Training loss: 2.6445502727055397
Validation loss: 2.4423480573794647

Epoch: 6| Step: 9
Training loss: 1.8791359108764354
Validation loss: 2.4664296120136897

Epoch: 6| Step: 10
Training loss: 2.7125990115610477
Validation loss: 2.439898037794843

Epoch: 6| Step: 11
Training loss: 2.936649280486526
Validation loss: 2.458093692688042

Epoch: 6| Step: 12
Training loss: 2.5086096331563708
Validation loss: 2.4631378343037245

Epoch: 6| Step: 13
Training loss: 2.804518840680942
Validation loss: 2.443135418599057

Epoch: 140| Step: 0
Training loss: 2.5217415510792245
Validation loss: 2.4482542557037315

Epoch: 6| Step: 1
Training loss: 2.975683207560545
Validation loss: 2.4531294832645205

Epoch: 6| Step: 2
Training loss: 2.447794084658653
Validation loss: 2.4746265550799476

Epoch: 6| Step: 3
Training loss: 2.502781846126289
Validation loss: 2.4644396032463503

Epoch: 6| Step: 4
Training loss: 2.2860892192688
Validation loss: 2.4468062570989595

Epoch: 6| Step: 5
Training loss: 2.461110136243839
Validation loss: 2.4486130651766493

Epoch: 6| Step: 6
Training loss: 2.9868607479568823
Validation loss: 2.465512832612032

Epoch: 6| Step: 7
Training loss: 2.407551203071319
Validation loss: 2.4417520588425803

Epoch: 6| Step: 8
Training loss: 2.8428853156123988
Validation loss: 2.45606923704613

Epoch: 6| Step: 9
Training loss: 2.703949201761299
Validation loss: 2.4434100710639717

Epoch: 6| Step: 10
Training loss: 2.2073476378932564
Validation loss: 2.4421089777454252

Epoch: 6| Step: 11
Training loss: 2.0708269739597034
Validation loss: 2.4437263484477807

Epoch: 6| Step: 12
Training loss: 2.665890690038534
Validation loss: 2.449354677176529

Epoch: 6| Step: 13
Training loss: 3.041928232172801
Validation loss: 2.4344247513749893

Epoch: 141| Step: 0
Training loss: 2.0597360760403003
Validation loss: 2.4244133472986396

Epoch: 6| Step: 1
Training loss: 2.6231239517715172
Validation loss: 2.438876170321913

Epoch: 6| Step: 2
Training loss: 2.933649664497093
Validation loss: 2.4513108041577456

Epoch: 6| Step: 3
Training loss: 1.9953390885913835
Validation loss: 2.439300754355065

Epoch: 6| Step: 4
Training loss: 2.1725333334381554
Validation loss: 2.45541380402575

Epoch: 6| Step: 5
Training loss: 2.260786480397636
Validation loss: 2.4507982736156633

Epoch: 6| Step: 6
Training loss: 2.345677511439051
Validation loss: 2.4579047299928405

Epoch: 6| Step: 7
Training loss: 2.993225077120778
Validation loss: 2.4512486019169124

Epoch: 6| Step: 8
Training loss: 3.2764999158100436
Validation loss: 2.440043094324385

Epoch: 6| Step: 9
Training loss: 2.800543653979173
Validation loss: 2.4360258821120437

Epoch: 6| Step: 10
Training loss: 2.1103511105968975
Validation loss: 2.4573839341649433

Epoch: 6| Step: 11
Training loss: 3.1004519840650917
Validation loss: 2.435085686996578

Epoch: 6| Step: 12
Training loss: 2.655271013152856
Validation loss: 2.456884422971083

Epoch: 6| Step: 13
Training loss: 1.7611050435525615
Validation loss: 2.439186009514212

Epoch: 142| Step: 0
Training loss: 2.613936041436964
Validation loss: 2.4524930128535067

Epoch: 6| Step: 1
Training loss: 2.9695069000682963
Validation loss: 2.4295311539751374

Epoch: 6| Step: 2
Training loss: 2.424970850818487
Validation loss: 2.4637679909858536

Epoch: 6| Step: 3
Training loss: 2.6389436504192147
Validation loss: 2.464245824585046

Epoch: 6| Step: 4
Training loss: 2.4991787515237625
Validation loss: 2.466116785830691

Epoch: 6| Step: 5
Training loss: 2.7816550999197704
Validation loss: 2.4689836911985874

Epoch: 6| Step: 6
Training loss: 3.1920749519363256
Validation loss: 2.4561220505793497

Epoch: 6| Step: 7
Training loss: 2.5340843814869976
Validation loss: 2.4634588510752606

Epoch: 6| Step: 8
Training loss: 2.4511483362291093
Validation loss: 2.4645588383192303

Epoch: 6| Step: 9
Training loss: 1.9478162715831897
Validation loss: 2.4594764634442576

Epoch: 6| Step: 10
Training loss: 2.57171425670498
Validation loss: 2.448745246595361

Epoch: 6| Step: 11
Training loss: 2.4817917547047945
Validation loss: 2.461145301401483

Epoch: 6| Step: 12
Training loss: 2.4265554178678643
Validation loss: 2.4648121437729893

Epoch: 6| Step: 13
Training loss: 1.5760189393449788
Validation loss: 2.444642835299558

Epoch: 143| Step: 0
Training loss: 3.021926068384328
Validation loss: 2.4532733425886284

Epoch: 6| Step: 1
Training loss: 2.11247929218684
Validation loss: 2.4463260409344576

Epoch: 6| Step: 2
Training loss: 2.102764820925146
Validation loss: 2.4598920464574743

Epoch: 6| Step: 3
Training loss: 2.580047815160724
Validation loss: 2.4500144621970725

Epoch: 6| Step: 4
Training loss: 2.2330824307757116
Validation loss: 2.45948935939153

Epoch: 6| Step: 5
Training loss: 2.833701184646968
Validation loss: 2.4574637146308334

Epoch: 6| Step: 6
Training loss: 2.3342537313519114
Validation loss: 2.446794763257245

Epoch: 6| Step: 7
Training loss: 2.8693799359863474
Validation loss: 2.4433859350087297

Epoch: 6| Step: 8
Training loss: 2.5797794466597725
Validation loss: 2.460606010626893

Epoch: 6| Step: 9
Training loss: 2.5014527868054395
Validation loss: 2.4443654382112334

Epoch: 6| Step: 10
Training loss: 3.0558060649331025
Validation loss: 2.4304701456590756

Epoch: 6| Step: 11
Training loss: 2.946786688637316
Validation loss: 2.45468503988548

Epoch: 6| Step: 12
Training loss: 2.233612357212522
Validation loss: 2.4466235583457707

Epoch: 6| Step: 13
Training loss: 2.086608550183579
Validation loss: 2.4306578808490253

Epoch: 144| Step: 0
Training loss: 2.234694558080325
Validation loss: 2.45676631285887

Epoch: 6| Step: 1
Training loss: 2.9452041889381073
Validation loss: 2.440242611634583

Epoch: 6| Step: 2
Training loss: 2.8621034372486007
Validation loss: 2.4420481714478415

Epoch: 6| Step: 3
Training loss: 2.2337417238633974
Validation loss: 2.4468305186718045

Epoch: 6| Step: 4
Training loss: 2.373239567472424
Validation loss: 2.4321490985094827

Epoch: 6| Step: 5
Training loss: 2.529517534565064
Validation loss: 2.44372269243519

Epoch: 6| Step: 6
Training loss: 2.9504778264887963
Validation loss: 2.4378610716376055

Epoch: 6| Step: 7
Training loss: 2.2284480092174364
Validation loss: 2.440432620459056

Epoch: 6| Step: 8
Training loss: 1.727245765229198
Validation loss: 2.4440348857114285

Epoch: 6| Step: 9
Training loss: 3.136297784397941
Validation loss: 2.4316250816362732

Epoch: 6| Step: 10
Training loss: 2.784504304531481
Validation loss: 2.431987894116341

Epoch: 6| Step: 11
Training loss: 2.541064978082882
Validation loss: 2.4457786844574954

Epoch: 6| Step: 12
Training loss: 2.8993602836352177
Validation loss: 2.4493760749923323

Epoch: 6| Step: 13
Training loss: 2.0894553705996066
Validation loss: 2.4470685733805757

Epoch: 145| Step: 0
Training loss: 2.583343034131284
Validation loss: 2.445012950401989

Epoch: 6| Step: 1
Training loss: 2.6400908883661995
Validation loss: 2.4356032049385146

Epoch: 6| Step: 2
Training loss: 3.201137763651005
Validation loss: 2.445574415151859

Epoch: 6| Step: 3
Training loss: 1.9931169922058642
Validation loss: 2.450549159388051

Epoch: 6| Step: 4
Training loss: 3.030892734513619
Validation loss: 2.4532284578633985

Epoch: 6| Step: 5
Training loss: 2.4944981114939875
Validation loss: 2.448614656580598

Epoch: 6| Step: 6
Training loss: 2.577123366476432
Validation loss: 2.445992318774982

Epoch: 6| Step: 7
Training loss: 2.4750121646158303
Validation loss: 2.435038974132776

Epoch: 6| Step: 8
Training loss: 2.1320405897674686
Validation loss: 2.4218247518576455

Epoch: 6| Step: 9
Training loss: 2.5086260750445275
Validation loss: 2.4459975865118677

Epoch: 6| Step: 10
Training loss: 3.035619516255808
Validation loss: 2.4488595922039464

Epoch: 6| Step: 11
Training loss: 2.5094751093409666
Validation loss: 2.4477603458367874

Epoch: 6| Step: 12
Training loss: 1.772267766677541
Validation loss: 2.446229889135002

Epoch: 6| Step: 13
Training loss: 2.5844700271128938
Validation loss: 2.455666242559816

Epoch: 146| Step: 0
Training loss: 2.5767904815765683
Validation loss: 2.432895344558738

Epoch: 6| Step: 1
Training loss: 2.8171546565980177
Validation loss: 2.4388063641194306

Epoch: 6| Step: 2
Training loss: 2.5331750778492634
Validation loss: 2.4405085105995425

Epoch: 6| Step: 3
Training loss: 2.6718231553072678
Validation loss: 2.4424587428086793

Epoch: 6| Step: 4
Training loss: 2.124418964610047
Validation loss: 2.434417633079712

Epoch: 6| Step: 5
Training loss: 2.14540224158507
Validation loss: 2.442161229241442

Epoch: 6| Step: 6
Training loss: 2.5762553516019553
Validation loss: 2.442893899495486

Epoch: 6| Step: 7
Training loss: 2.7277289139082344
Validation loss: 2.449369141969765

Epoch: 6| Step: 8
Training loss: 2.2784646156548622
Validation loss: 2.452836908516129

Epoch: 6| Step: 9
Training loss: 2.520347664026671
Validation loss: 2.4684152019169567

Epoch: 6| Step: 10
Training loss: 2.7246788535608197
Validation loss: 2.451398396660695

Epoch: 6| Step: 11
Training loss: 2.736217292321789
Validation loss: 2.452844151559199

Epoch: 6| Step: 12
Training loss: 3.0330710692438423
Validation loss: 2.4521378244172714

Epoch: 6| Step: 13
Training loss: 1.9857702681558724
Validation loss: 2.449543237162309

Epoch: 147| Step: 0
Training loss: 2.5852033143357676
Validation loss: 2.453309987945998

Epoch: 6| Step: 1
Training loss: 2.374369186149241
Validation loss: 2.4661548474563304

Epoch: 6| Step: 2
Training loss: 2.3890806707635504
Validation loss: 2.4456227911076334

Epoch: 6| Step: 3
Training loss: 2.7969283626303403
Validation loss: 2.443862136329634

Epoch: 6| Step: 4
Training loss: 2.602164717155692
Validation loss: 2.457765946176402

Epoch: 6| Step: 5
Training loss: 2.514352038053671
Validation loss: 2.4530234001318445

Epoch: 6| Step: 6
Training loss: 2.6031443509301946
Validation loss: 2.443329587240556

Epoch: 6| Step: 7
Training loss: 2.8952378866068926
Validation loss: 2.4399427259258575

Epoch: 6| Step: 8
Training loss: 2.215992247970738
Validation loss: 2.4375561575445737

Epoch: 6| Step: 9
Training loss: 2.1712498485540976
Validation loss: 2.4435751821894707

Epoch: 6| Step: 10
Training loss: 2.7456459776461
Validation loss: 2.434815821249627

Epoch: 6| Step: 11
Training loss: 2.7232176805247246
Validation loss: 2.4310607047311152

Epoch: 6| Step: 12
Training loss: 2.435730707714909
Validation loss: 2.456817057838531

Epoch: 6| Step: 13
Training loss: 2.6519974719879853
Validation loss: 2.432802635219922

Epoch: 148| Step: 0
Training loss: 2.4493120044921026
Validation loss: 2.434922563924781

Epoch: 6| Step: 1
Training loss: 2.863777648445977
Validation loss: 2.4441638890719735

Epoch: 6| Step: 2
Training loss: 2.5695320962154655
Validation loss: 2.451186315996229

Epoch: 6| Step: 3
Training loss: 2.1898220272123163
Validation loss: 2.4387927901087423

Epoch: 6| Step: 4
Training loss: 2.1759547056574955
Validation loss: 2.4478496803294276

Epoch: 6| Step: 5
Training loss: 2.7164596030147017
Validation loss: 2.4330482069611454

Epoch: 6| Step: 6
Training loss: 2.5828832931647567
Validation loss: 2.4507970309161395

Epoch: 6| Step: 7
Training loss: 2.8100638116942216
Validation loss: 2.443770289409633

Epoch: 6| Step: 8
Training loss: 2.459859363598676
Validation loss: 2.446662497419518

Epoch: 6| Step: 9
Training loss: 2.622912166996602
Validation loss: 2.4432081786912474

Epoch: 6| Step: 10
Training loss: 2.4135126271570515
Validation loss: 2.4395104030134123

Epoch: 6| Step: 11
Training loss: 2.612475352193837
Validation loss: 2.4499177456848416

Epoch: 6| Step: 12
Training loss: 2.300382362593149
Validation loss: 2.4440179138385036

Epoch: 6| Step: 13
Training loss: 3.082180855991505
Validation loss: 2.4365544574240996

Epoch: 149| Step: 0
Training loss: 3.568820499719243
Validation loss: 2.450302068401808

Epoch: 6| Step: 1
Training loss: 2.292678216727446
Validation loss: 2.4359098242742396

Epoch: 6| Step: 2
Training loss: 2.481949876142366
Validation loss: 2.4272967487717754

Epoch: 6| Step: 3
Training loss: 2.067968805339771
Validation loss: 2.4385935578700564

Epoch: 6| Step: 4
Training loss: 3.0883510760233226
Validation loss: 2.432123905724063

Epoch: 6| Step: 5
Training loss: 2.7165153352054263
Validation loss: 2.4289959842519657

Epoch: 6| Step: 6
Training loss: 1.820940097789309
Validation loss: 2.441038074977352

Epoch: 6| Step: 7
Training loss: 2.997561099182961
Validation loss: 2.437330016021841

Epoch: 6| Step: 8
Training loss: 2.0965768889273013
Validation loss: 2.438404030247434

Epoch: 6| Step: 9
Training loss: 2.510672866178865
Validation loss: 2.437677567847715

Epoch: 6| Step: 10
Training loss: 1.9428982134793589
Validation loss: 2.439069539720092

Epoch: 6| Step: 11
Training loss: 2.2692307816645934
Validation loss: 2.4245511377798366

Epoch: 6| Step: 12
Training loss: 2.933404705595374
Validation loss: 2.4275638196787175

Epoch: 6| Step: 13
Training loss: 2.394427199226621
Validation loss: 2.433812893203904

Epoch: 150| Step: 0
Training loss: 2.9270699569740417
Validation loss: 2.447369271896809

Epoch: 6| Step: 1
Training loss: 2.916600326510235
Validation loss: 2.4483512127921356

Epoch: 6| Step: 2
Training loss: 2.7521036944814092
Validation loss: 2.4410070484908863

Epoch: 6| Step: 3
Training loss: 2.17797806407016
Validation loss: 2.4332563767237274

Epoch: 6| Step: 4
Training loss: 2.5075956351849054
Validation loss: 2.4333925571081227

Epoch: 6| Step: 5
Training loss: 2.6067089234786387
Validation loss: 2.451866756789623

Epoch: 6| Step: 6
Training loss: 2.173729194247599
Validation loss: 2.4465622868849017

Epoch: 6| Step: 7
Training loss: 2.7401565899620057
Validation loss: 2.4433129714097004

Epoch: 6| Step: 8
Training loss: 2.579903468906252
Validation loss: 2.444824039963836

Epoch: 6| Step: 9
Training loss: 2.4073178782756193
Validation loss: 2.4435734002327543

Epoch: 6| Step: 10
Training loss: 2.729516819824005
Validation loss: 2.424584894518589

Epoch: 6| Step: 11
Training loss: 1.833446744821068
Validation loss: 2.444913459726139

Epoch: 6| Step: 12
Training loss: 2.7006118646261212
Validation loss: 2.437994569973028

Epoch: 6| Step: 13
Training loss: 2.525964092099089
Validation loss: 2.44315822974705

Epoch: 151| Step: 0
Training loss: 1.8648007512978473
Validation loss: 2.4321832325492108

Epoch: 6| Step: 1
Training loss: 2.5746145682151576
Validation loss: 2.432551196984159

Epoch: 6| Step: 2
Training loss: 2.6725925129078822
Validation loss: 2.4363474670281198

Epoch: 6| Step: 3
Training loss: 2.9587590802226202
Validation loss: 2.4463988476018463

Epoch: 6| Step: 4
Training loss: 2.9581972189380497
Validation loss: 2.4274508582516217

Epoch: 6| Step: 5
Training loss: 1.8256787618617525
Validation loss: 2.439075750503746

Epoch: 6| Step: 6
Training loss: 2.024016190776977
Validation loss: 2.4499372142341818

Epoch: 6| Step: 7
Training loss: 2.887882725836282
Validation loss: 2.4405419526071137

Epoch: 6| Step: 8
Training loss: 2.73407312089185
Validation loss: 2.4415758231390234

Epoch: 6| Step: 9
Training loss: 2.658630740311651
Validation loss: 2.4408242711921724

Epoch: 6| Step: 10
Training loss: 2.7420252564517056
Validation loss: 2.4530683314571733

Epoch: 6| Step: 11
Training loss: 2.705876972663877
Validation loss: 2.452534799174943

Epoch: 6| Step: 12
Training loss: 2.15547785547925
Validation loss: 2.444363341667178

Epoch: 6| Step: 13
Training loss: 2.2086152190712367
Validation loss: 2.4425205664321394

Epoch: 152| Step: 0
Training loss: 2.390827843531354
Validation loss: 2.4460025167511543

Epoch: 6| Step: 1
Training loss: 3.0867132443181253
Validation loss: 2.443357169522123

Epoch: 6| Step: 2
Training loss: 3.07316910735917
Validation loss: 2.4498905014898043

Epoch: 6| Step: 3
Training loss: 2.345283921872645
Validation loss: 2.4452519138524114

Epoch: 6| Step: 4
Training loss: 2.4952865035506937
Validation loss: 2.4264612666006893

Epoch: 6| Step: 5
Training loss: 2.86277127375638
Validation loss: 2.442158433775345

Epoch: 6| Step: 6
Training loss: 1.9954481779254838
Validation loss: 2.433433936596457

Epoch: 6| Step: 7
Training loss: 3.0741346800965097
Validation loss: 2.457935270434125

Epoch: 6| Step: 8
Training loss: 2.4470385227639695
Validation loss: 2.466259866803294

Epoch: 6| Step: 9
Training loss: 2.6612598521855193
Validation loss: 2.447599546426379

Epoch: 6| Step: 10
Training loss: 2.3924269523351223
Validation loss: 2.4403086294810175

Epoch: 6| Step: 11
Training loss: 1.9235933886773937
Validation loss: 2.4579681958193107

Epoch: 6| Step: 12
Training loss: 2.3541982080377752
Validation loss: 2.4372345448792623

Epoch: 6| Step: 13
Training loss: 1.8614797780559766
Validation loss: 2.4357327253833843

Epoch: 153| Step: 0
Training loss: 2.348323073553122
Validation loss: 2.431720363434505

Epoch: 6| Step: 1
Training loss: 2.4329864576960802
Validation loss: 2.4541247762714398

Epoch: 6| Step: 2
Training loss: 2.1560591461193597
Validation loss: 2.4320586676297444

Epoch: 6| Step: 3
Training loss: 2.287477849634208
Validation loss: 2.446557849218223

Epoch: 6| Step: 4
Training loss: 2.852393794711281
Validation loss: 2.4333534825943697

Epoch: 6| Step: 5
Training loss: 2.767015183186897
Validation loss: 2.4384657842533337

Epoch: 6| Step: 6
Training loss: 1.838126059035405
Validation loss: 2.434506165100668

Epoch: 6| Step: 7
Training loss: 2.7693606217824005
Validation loss: 2.438235144430571

Epoch: 6| Step: 8
Training loss: 2.648311555731675
Validation loss: 2.4393100034304815

Epoch: 6| Step: 9
Training loss: 2.29254042411948
Validation loss: 2.424625972839674

Epoch: 6| Step: 10
Training loss: 1.9386807812177365
Validation loss: 2.447016566010535

Epoch: 6| Step: 11
Training loss: 2.839690276436652
Validation loss: 2.4438037486748767

Epoch: 6| Step: 12
Training loss: 3.1512169318940053
Validation loss: 2.439596808257177

Epoch: 6| Step: 13
Training loss: 3.1788684436784034
Validation loss: 2.448975203315592

Epoch: 154| Step: 0
Training loss: 3.2357606729907475
Validation loss: 2.426021223295992

Epoch: 6| Step: 1
Training loss: 2.958262339711717
Validation loss: 2.4463610906003734

Epoch: 6| Step: 2
Training loss: 2.3910611820601706
Validation loss: 2.441902967363222

Epoch: 6| Step: 3
Training loss: 2.6433915312310496
Validation loss: 2.430864018459226

Epoch: 6| Step: 4
Training loss: 1.4780340380433552
Validation loss: 2.43844634712312

Epoch: 6| Step: 5
Training loss: 2.568014027399748
Validation loss: 2.4330410893800907

Epoch: 6| Step: 6
Training loss: 2.576877546510403
Validation loss: 2.441211548225865

Epoch: 6| Step: 7
Training loss: 2.74302908217992
Validation loss: 2.4526280504016884

Epoch: 6| Step: 8
Training loss: 1.6000715060944866
Validation loss: 2.4338625894359613

Epoch: 6| Step: 9
Training loss: 2.342507401722356
Validation loss: 2.4476616057390785

Epoch: 6| Step: 10
Training loss: 1.9719501704716065
Validation loss: 2.4486106843506534

Epoch: 6| Step: 11
Training loss: 2.5144627889601847
Validation loss: 2.4367916170599546

Epoch: 6| Step: 12
Training loss: 2.95095923394252
Validation loss: 2.433903820317063

Epoch: 6| Step: 13
Training loss: 3.131558364638554
Validation loss: 2.427089452614786

Epoch: 155| Step: 0
Training loss: 2.217977254917759
Validation loss: 2.4470912242446556

Epoch: 6| Step: 1
Training loss: 2.377911940866786
Validation loss: 2.4559118303657677

Epoch: 6| Step: 2
Training loss: 2.9230745678479053
Validation loss: 2.444137385241803

Epoch: 6| Step: 3
Training loss: 2.114327275850817
Validation loss: 2.442760184049265

Epoch: 6| Step: 4
Training loss: 2.456336282471655
Validation loss: 2.4283310347874654

Epoch: 6| Step: 5
Training loss: 3.290253991987109
Validation loss: 2.430201825498801

Epoch: 6| Step: 6
Training loss: 1.9929867207170417
Validation loss: 2.425645919025448

Epoch: 6| Step: 7
Training loss: 2.1697689246275274
Validation loss: 2.4308360813841126

Epoch: 6| Step: 8
Training loss: 2.4419802059711664
Validation loss: 2.4359699450251115

Epoch: 6| Step: 9
Training loss: 2.418643493673118
Validation loss: 2.4431382056020174

Epoch: 6| Step: 10
Training loss: 2.8853609559410995
Validation loss: 2.4378302670932164

Epoch: 6| Step: 11
Training loss: 2.167833478676804
Validation loss: 2.4220367790436

Epoch: 6| Step: 12
Training loss: 2.6926473958678883
Validation loss: 2.4373007514908505

Epoch: 6| Step: 13
Training loss: 3.17856887523371
Validation loss: 2.4317185121712033

Epoch: 156| Step: 0
Training loss: 1.8417865837837153
Validation loss: 2.432082557705576

Epoch: 6| Step: 1
Training loss: 3.2489993682419245
Validation loss: 2.4266458668381388

Epoch: 6| Step: 2
Training loss: 2.276478624336936
Validation loss: 2.4161426038199485

Epoch: 6| Step: 3
Training loss: 2.5690929917091028
Validation loss: 2.440403937817023

Epoch: 6| Step: 4
Training loss: 1.8299978721064303
Validation loss: 2.417715942434743

Epoch: 6| Step: 5
Training loss: 2.6037035314082466
Validation loss: 2.456784111808526

Epoch: 6| Step: 6
Training loss: 2.58284175461611
Validation loss: 2.437688518898144

Epoch: 6| Step: 7
Training loss: 3.389004227667969
Validation loss: 2.429021149847568

Epoch: 6| Step: 8
Training loss: 2.587636153467472
Validation loss: 2.437738374844815

Epoch: 6| Step: 9
Training loss: 2.191375569011494
Validation loss: 2.4462512514046613

Epoch: 6| Step: 10
Training loss: 1.9675132257581713
Validation loss: 2.4474171090571795

Epoch: 6| Step: 11
Training loss: 2.753758030142485
Validation loss: 2.440009644428951

Epoch: 6| Step: 12
Training loss: 2.196781413661072
Validation loss: 2.436203273280325

Epoch: 6| Step: 13
Training loss: 3.0551663777049827
Validation loss: 2.451092630251039

Epoch: 157| Step: 0
Training loss: 2.4756536410552203
Validation loss: 2.44157642111069

Epoch: 6| Step: 1
Training loss: 2.4304013690786417
Validation loss: 2.437336434765415

Epoch: 6| Step: 2
Training loss: 2.6233675285673197
Validation loss: 2.436789515051657

Epoch: 6| Step: 3
Training loss: 2.415900558408329
Validation loss: 2.4324557938566054

Epoch: 6| Step: 4
Training loss: 2.5066304971574462
Validation loss: 2.421338479020639

Epoch: 6| Step: 5
Training loss: 2.4366707002403736
Validation loss: 2.447761538758299

Epoch: 6| Step: 6
Training loss: 2.7883506742744983
Validation loss: 2.426990409197837

Epoch: 6| Step: 7
Training loss: 2.2420355449584597
Validation loss: 2.4328512029639495

Epoch: 6| Step: 8
Training loss: 2.687709534039658
Validation loss: 2.433537943997868

Epoch: 6| Step: 9
Training loss: 2.661140427884746
Validation loss: 2.4345606426380995

Epoch: 6| Step: 10
Training loss: 2.6075137690091896
Validation loss: 2.4218145240539592

Epoch: 6| Step: 11
Training loss: 2.61153226752069
Validation loss: 2.4415909939341693

Epoch: 6| Step: 12
Training loss: 2.370501926726287
Validation loss: 2.4308197007647783

Epoch: 6| Step: 13
Training loss: 2.5584997261767364
Validation loss: 2.4252205459056353

Epoch: 158| Step: 0
Training loss: 2.6013654571236455
Validation loss: 2.431525976255063

Epoch: 6| Step: 1
Training loss: 3.0842560255299927
Validation loss: 2.431878175287402

Epoch: 6| Step: 2
Training loss: 2.2408279390886077
Validation loss: 2.4275445001337363

Epoch: 6| Step: 3
Training loss: 2.2013942332024055
Validation loss: 2.440309217783073

Epoch: 6| Step: 4
Training loss: 3.049576251501297
Validation loss: 2.4576202077452822

Epoch: 6| Step: 5
Training loss: 3.0365208684240916
Validation loss: 2.442472826498311

Epoch: 6| Step: 6
Training loss: 1.8519151569071863
Validation loss: 2.4392442659732616

Epoch: 6| Step: 7
Training loss: 2.3516666224547484
Validation loss: 2.443760313959541

Epoch: 6| Step: 8
Training loss: 2.8878886700250384
Validation loss: 2.441819187608264

Epoch: 6| Step: 9
Training loss: 2.3759017035705194
Validation loss: 2.439361242389233

Epoch: 6| Step: 10
Training loss: 2.100150843152787
Validation loss: 2.433669439411537

Epoch: 6| Step: 11
Training loss: 2.563821219390135
Validation loss: 2.417464593085792

Epoch: 6| Step: 12
Training loss: 2.340163576815446
Validation loss: 2.42528406472611

Epoch: 6| Step: 13
Training loss: 2.0363327989317885
Validation loss: 2.445574178765271

Epoch: 159| Step: 0
Training loss: 2.476274058395046
Validation loss: 2.4456783030111215

Epoch: 6| Step: 1
Training loss: 2.443348153482675
Validation loss: 2.434440594364599

Epoch: 6| Step: 2
Training loss: 2.130100412285406
Validation loss: 2.431255329456388

Epoch: 6| Step: 3
Training loss: 3.16930868125682
Validation loss: 2.436378551274486

Epoch: 6| Step: 4
Training loss: 2.4700487800754356
Validation loss: 2.4212244224494404

Epoch: 6| Step: 5
Training loss: 2.048115128931932
Validation loss: 2.437528348704765

Epoch: 6| Step: 6
Training loss: 2.5874292962923193
Validation loss: 2.4486786060830097

Epoch: 6| Step: 7
Training loss: 2.5527099047818167
Validation loss: 2.442418593666257

Epoch: 6| Step: 8
Training loss: 3.125521654458307
Validation loss: 2.444134142584361

Epoch: 6| Step: 9
Training loss: 2.3687216442494443
Validation loss: 2.4471056070835555

Epoch: 6| Step: 10
Training loss: 2.9488530014308703
Validation loss: 2.442352674839479

Epoch: 6| Step: 11
Training loss: 1.9986094170443092
Validation loss: 2.4386395161730374

Epoch: 6| Step: 12
Training loss: 2.1179497391858204
Validation loss: 2.4559841587131226

Epoch: 6| Step: 13
Training loss: 2.726668653635111
Validation loss: 2.454764615647297

Epoch: 160| Step: 0
Training loss: 2.500435123724045
Validation loss: 2.4294616269248004

Epoch: 6| Step: 1
Training loss: 2.7556266443079376
Validation loss: 2.4466776749192616

Epoch: 6| Step: 2
Training loss: 2.6565751718188158
Validation loss: 2.40760313017088

Epoch: 6| Step: 3
Training loss: 2.9758948831452003
Validation loss: 2.417322538340699

Epoch: 6| Step: 4
Training loss: 2.373923208175747
Validation loss: 2.4330948221241924

Epoch: 6| Step: 5
Training loss: 2.0522449900923454
Validation loss: 2.417340166389333

Epoch: 6| Step: 6
Training loss: 2.3195606387863275
Validation loss: 2.4373122143724544

Epoch: 6| Step: 7
Training loss: 2.4830888982030817
Validation loss: 2.439539027854484

Epoch: 6| Step: 8
Training loss: 1.9404068951739222
Validation loss: 2.442494766369635

Epoch: 6| Step: 9
Training loss: 2.703503003454048
Validation loss: 2.4288775249276617

Epoch: 6| Step: 10
Training loss: 2.357678036808725
Validation loss: 2.433843524235878

Epoch: 6| Step: 11
Training loss: 2.569766186568652
Validation loss: 2.4125935827466343

Epoch: 6| Step: 12
Training loss: 2.4582320585780346
Validation loss: 2.4120461203754058

Epoch: 6| Step: 13
Training loss: 3.0509642718288386
Validation loss: 2.446225666753009

Epoch: 161| Step: 0
Training loss: 2.812323840240532
Validation loss: 2.4256457752884195

Epoch: 6| Step: 1
Training loss: 2.7376069069807003
Validation loss: 2.432287729262798

Epoch: 6| Step: 2
Training loss: 2.484875898428495
Validation loss: 2.4280366179885116

Epoch: 6| Step: 3
Training loss: 2.8629897985406534
Validation loss: 2.4237955968135614

Epoch: 6| Step: 4
Training loss: 2.2083946675354196
Validation loss: 2.4207108763288425

Epoch: 6| Step: 5
Training loss: 2.1097296522605546
Validation loss: 2.4391114361892376

Epoch: 6| Step: 6
Training loss: 2.5002265827496295
Validation loss: 2.432081202141898

Epoch: 6| Step: 7
Training loss: 2.505466587980436
Validation loss: 2.4274940356248083

Epoch: 6| Step: 8
Training loss: 2.1689448728049983
Validation loss: 2.4267548535060035

Epoch: 6| Step: 9
Training loss: 2.7457386291874197
Validation loss: 2.4310515964243926

Epoch: 6| Step: 10
Training loss: 1.9088595929313719
Validation loss: 2.4319923109329435

Epoch: 6| Step: 11
Training loss: 2.641245210483467
Validation loss: 2.4484088051288473

Epoch: 6| Step: 12
Training loss: 3.018182016332981
Validation loss: 2.4387449572283506

Epoch: 6| Step: 13
Training loss: 2.0683674438434445
Validation loss: 2.4227179852751335

Epoch: 162| Step: 0
Training loss: 2.8067873264058694
Validation loss: 2.4420306682167436

Epoch: 6| Step: 1
Training loss: 2.6217589578876543
Validation loss: 2.420414286503752

Epoch: 6| Step: 2
Training loss: 2.4703647788956404
Validation loss: 2.430554946297737

Epoch: 6| Step: 3
Training loss: 2.384713837032379
Validation loss: 2.4352830576414655

Epoch: 6| Step: 4
Training loss: 2.9945865107492433
Validation loss: 2.4482368832307686

Epoch: 6| Step: 5
Training loss: 2.795670575001322
Validation loss: 2.427860247803812

Epoch: 6| Step: 6
Training loss: 1.375485507906771
Validation loss: 2.4244989476921877

Epoch: 6| Step: 7
Training loss: 2.097445855697884
Validation loss: 2.434201153035353

Epoch: 6| Step: 8
Training loss: 2.4992722405698244
Validation loss: 2.4372059371510026

Epoch: 6| Step: 9
Training loss: 2.498699422136456
Validation loss: 2.439816347437944

Epoch: 6| Step: 10
Training loss: 1.9745319173922977
Validation loss: 2.434526472957826

Epoch: 6| Step: 11
Training loss: 3.0153340411773306
Validation loss: 2.4435246794484726

Epoch: 6| Step: 12
Training loss: 2.01928615005059
Validation loss: 2.4492452893836885

Epoch: 6| Step: 13
Training loss: 3.2401638839623654
Validation loss: 2.4262144042087423

Epoch: 163| Step: 0
Training loss: 2.7819115355609374
Validation loss: 2.439864457721573

Epoch: 6| Step: 1
Training loss: 2.517058443896656
Validation loss: 2.430970984221015

Epoch: 6| Step: 2
Training loss: 2.2528735990119375
Validation loss: 2.42015634792729

Epoch: 6| Step: 3
Training loss: 1.9096807656460455
Validation loss: 2.4411413225713607

Epoch: 6| Step: 4
Training loss: 2.3376576905940603
Validation loss: 2.432321650559227

Epoch: 6| Step: 5
Training loss: 2.525594916935066
Validation loss: 2.418504164561359

Epoch: 6| Step: 6
Training loss: 2.7316520866329235
Validation loss: 2.417116996962505

Epoch: 6| Step: 7
Training loss: 2.6716442343122497
Validation loss: 2.4246302011165617

Epoch: 6| Step: 8
Training loss: 2.8020722248372447
Validation loss: 2.4397931204659415

Epoch: 6| Step: 9
Training loss: 2.2766983400136476
Validation loss: 2.4314625793763707

Epoch: 6| Step: 10
Training loss: 3.024281938307109
Validation loss: 2.4469288679887367

Epoch: 6| Step: 11
Training loss: 2.1603682622633973
Validation loss: 2.439701422991252

Epoch: 6| Step: 12
Training loss: 2.3524634577540966
Validation loss: 2.4095889469714513

Epoch: 6| Step: 13
Training loss: 2.572046871630313
Validation loss: 2.4271294381127957

Epoch: 164| Step: 0
Training loss: 1.8774473113325534
Validation loss: 2.417798884742554

Epoch: 6| Step: 1
Training loss: 1.9912300950831727
Validation loss: 2.4256721308098528

Epoch: 6| Step: 2
Training loss: 2.7521093255162494
Validation loss: 2.4384785784221386

Epoch: 6| Step: 3
Training loss: 2.227334621468272
Validation loss: 2.452319300607093

Epoch: 6| Step: 4
Training loss: 2.5894474974953936
Validation loss: 2.429161632913048

Epoch: 6| Step: 5
Training loss: 2.0921957595582388
Validation loss: 2.4319265269430415

Epoch: 6| Step: 6
Training loss: 2.5771005155524573
Validation loss: 2.4356016345049345

Epoch: 6| Step: 7
Training loss: 2.8348839013818474
Validation loss: 2.4309201742782207

Epoch: 6| Step: 8
Training loss: 1.9590815541893518
Validation loss: 2.4338255564647597

Epoch: 6| Step: 9
Training loss: 3.5279260705999977
Validation loss: 2.439519719075689

Epoch: 6| Step: 10
Training loss: 2.331312928360366
Validation loss: 2.4386749706610784

Epoch: 6| Step: 11
Training loss: 2.8482050948927893
Validation loss: 2.430655601618347

Epoch: 6| Step: 12
Training loss: 2.4957938096149497
Validation loss: 2.4297647555248867

Epoch: 6| Step: 13
Training loss: 2.4100680095526235
Validation loss: 2.431207959873765

Epoch: 165| Step: 0
Training loss: 2.2353400433844746
Validation loss: 2.448868511007087

Epoch: 6| Step: 1
Training loss: 2.0865612454744102
Validation loss: 2.415932436301632

Epoch: 6| Step: 2
Training loss: 2.675466076186166
Validation loss: 2.436586487012045

Epoch: 6| Step: 3
Training loss: 2.7141381094529913
Validation loss: 2.418719308572629

Epoch: 6| Step: 4
Training loss: 2.8350268706809305
Validation loss: 2.4244332363654024

Epoch: 6| Step: 5
Training loss: 1.8105558294344373
Validation loss: 2.4270348285037633

Epoch: 6| Step: 6
Training loss: 2.4836382463230495
Validation loss: 2.4452995209000705

Epoch: 6| Step: 7
Training loss: 1.922075834868159
Validation loss: 2.4327061949805118

Epoch: 6| Step: 8
Training loss: 2.581101612324104
Validation loss: 2.4388759201466437

Epoch: 6| Step: 9
Training loss: 3.0358248137540293
Validation loss: 2.4476852429781344

Epoch: 6| Step: 10
Training loss: 2.2524242057284125
Validation loss: 2.4470903379514315

Epoch: 6| Step: 11
Training loss: 2.4465170139241144
Validation loss: 2.4409946771700377

Epoch: 6| Step: 12
Training loss: 2.9054252520733934
Validation loss: 2.448334425810205

Epoch: 6| Step: 13
Training loss: 2.8781309908494923
Validation loss: 2.436199741725999

Epoch: 166| Step: 0
Training loss: 2.905258009412817
Validation loss: 2.4314227642498207

Epoch: 6| Step: 1
Training loss: 2.5033544923351214
Validation loss: 2.435689119374743

Epoch: 6| Step: 2
Training loss: 1.9375025841480373
Validation loss: 2.447721517442797

Epoch: 6| Step: 3
Training loss: 2.063312254979321
Validation loss: 2.4263798560632766

Epoch: 6| Step: 4
Training loss: 2.2903447124117284
Validation loss: 2.432702717363625

Epoch: 6| Step: 5
Training loss: 2.8281776886979495
Validation loss: 2.434249693402964

Epoch: 6| Step: 6
Training loss: 2.704845167795078
Validation loss: 2.4556036662312715

Epoch: 6| Step: 7
Training loss: 2.6414167131489195
Validation loss: 2.443643128483335

Epoch: 6| Step: 8
Training loss: 2.6226661843301198
Validation loss: 2.4294218687012044

Epoch: 6| Step: 9
Training loss: 1.6791581140242007
Validation loss: 2.434225023137477

Epoch: 6| Step: 10
Training loss: 2.130455083631852
Validation loss: 2.4509360430346683

Epoch: 6| Step: 11
Training loss: 2.9650717988093858
Validation loss: 2.420556347266839

Epoch: 6| Step: 12
Training loss: 2.8462525140468937
Validation loss: 2.4445943125347105

Epoch: 6| Step: 13
Training loss: 1.8914668833339117
Validation loss: 2.437966698834888

Epoch: 167| Step: 0
Training loss: 2.678587128729448
Validation loss: 2.4215056064560945

Epoch: 6| Step: 1
Training loss: 2.737326549452254
Validation loss: 2.431255318911874

Epoch: 6| Step: 2
Training loss: 1.953922688667854
Validation loss: 2.4260979298515637

Epoch: 6| Step: 3
Training loss: 3.0299887638702043
Validation loss: 2.4251642158250766

Epoch: 6| Step: 4
Training loss: 1.8331900309418765
Validation loss: 2.437174324889391

Epoch: 6| Step: 5
Training loss: 2.441590618038526
Validation loss: 2.4396130343168587

Epoch: 6| Step: 6
Training loss: 1.7974030755323713
Validation loss: 2.4424715249832234

Epoch: 6| Step: 7
Training loss: 2.2339249804612638
Validation loss: 2.428017274753251

Epoch: 6| Step: 8
Training loss: 2.82427716755368
Validation loss: 2.4477991212486216

Epoch: 6| Step: 9
Training loss: 2.417004419160969
Validation loss: 2.4497358283941684

Epoch: 6| Step: 10
Training loss: 2.4601665924232825
Validation loss: 2.4344130037282006

Epoch: 6| Step: 11
Training loss: 2.0084778392082945
Validation loss: 2.438749223047361

Epoch: 6| Step: 12
Training loss: 3.0955818994896185
Validation loss: 2.4286022191221237

Epoch: 6| Step: 13
Training loss: 2.893692945408114
Validation loss: 2.420522993477235

Epoch: 168| Step: 0
Training loss: 2.6752519292828922
Validation loss: 2.4247175553078066

Epoch: 6| Step: 1
Training loss: 2.2738814821652604
Validation loss: 2.4235050597288916

Epoch: 6| Step: 2
Training loss: 2.9230952851108705
Validation loss: 2.4250500246864855

Epoch: 6| Step: 3
Training loss: 2.5405363060365893
Validation loss: 2.422941514894378

Epoch: 6| Step: 4
Training loss: 2.7126832117885664
Validation loss: 2.423121045716423

Epoch: 6| Step: 5
Training loss: 2.2220653425578374
Validation loss: 2.4257431646850827

Epoch: 6| Step: 6
Training loss: 2.883478682130793
Validation loss: 2.4225646731645907

Epoch: 6| Step: 7
Training loss: 2.1347224029604916
Validation loss: 2.407496726649408

Epoch: 6| Step: 8
Training loss: 2.240135822606108
Validation loss: 2.415222159170314

Epoch: 6| Step: 9
Training loss: 2.79835360351758
Validation loss: 2.405470631440722

Epoch: 6| Step: 10
Training loss: 2.046494659730014
Validation loss: 2.4364543761895714

Epoch: 6| Step: 11
Training loss: 2.521884877541259
Validation loss: 2.4448521527478846

Epoch: 6| Step: 12
Training loss: 2.2862568087969173
Validation loss: 2.43457633363603

Epoch: 6| Step: 13
Training loss: 2.035612263907581
Validation loss: 2.4284732104103544

Epoch: 169| Step: 0
Training loss: 2.9779184211742513
Validation loss: 2.4366922020758484

Epoch: 6| Step: 1
Training loss: 2.2486701850234025
Validation loss: 2.421220916164755

Epoch: 6| Step: 2
Training loss: 2.338150864014259
Validation loss: 2.4217480163416365

Epoch: 6| Step: 3
Training loss: 2.682810573321691
Validation loss: 2.4284116989957276

Epoch: 6| Step: 4
Training loss: 2.3012916503982868
Validation loss: 2.422566700740348

Epoch: 6| Step: 5
Training loss: 2.3134599703591903
Validation loss: 2.43362167437977

Epoch: 6| Step: 6
Training loss: 1.695473869835595
Validation loss: 2.40991328738927

Epoch: 6| Step: 7
Training loss: 3.5250519227877493
Validation loss: 2.410127134711178

Epoch: 6| Step: 8
Training loss: 2.798528768391901
Validation loss: 2.4218590105040345

Epoch: 6| Step: 9
Training loss: 2.155066344972449
Validation loss: 2.430682243487115

Epoch: 6| Step: 10
Training loss: 2.0249995384686734
Validation loss: 2.4393172861133876

Epoch: 6| Step: 11
Training loss: 1.8446810682915837
Validation loss: 2.4202982670649185

Epoch: 6| Step: 12
Training loss: 2.731038526645312
Validation loss: 2.423604102778386

Epoch: 6| Step: 13
Training loss: 2.331531873746507
Validation loss: 2.438794357436223

Epoch: 170| Step: 0
Training loss: 1.913073599572581
Validation loss: 2.4463650109422925

Epoch: 6| Step: 1
Training loss: 2.079429503845322
Validation loss: 2.436306875188761

Epoch: 6| Step: 2
Training loss: 2.687168012118707
Validation loss: 2.433061924709626

Epoch: 6| Step: 3
Training loss: 2.204486399237661
Validation loss: 2.4291528586320577

Epoch: 6| Step: 4
Training loss: 2.1874468115743517
Validation loss: 2.414520015359002

Epoch: 6| Step: 5
Training loss: 2.7034082870660043
Validation loss: 2.422795864069003

Epoch: 6| Step: 6
Training loss: 2.774334227968538
Validation loss: 2.4423920892656206

Epoch: 6| Step: 7
Training loss: 2.56546072765053
Validation loss: 2.41762474794151

Epoch: 6| Step: 8
Training loss: 3.2193611823977584
Validation loss: 2.4267945793633094

Epoch: 6| Step: 9
Training loss: 2.471953812028799
Validation loss: 2.4183954887481605

Epoch: 6| Step: 10
Training loss: 2.602309294500868
Validation loss: 2.428513255464855

Epoch: 6| Step: 11
Training loss: 1.975374611202579
Validation loss: 2.436284516557863

Epoch: 6| Step: 12
Training loss: 2.4246955444501412
Validation loss: 2.414188780871511

Epoch: 6| Step: 13
Training loss: 2.7669188495588273
Validation loss: 2.421988720031757

Epoch: 171| Step: 0
Training loss: 1.9591867602881132
Validation loss: 2.4164130166517683

Epoch: 6| Step: 1
Training loss: 2.4892152862687555
Validation loss: 2.4255083566787117

Epoch: 6| Step: 2
Training loss: 2.7139855215238824
Validation loss: 2.4253413126899805

Epoch: 6| Step: 3
Training loss: 2.1406706645021
Validation loss: 2.4033132882185644

Epoch: 6| Step: 4
Training loss: 2.91844261233524
Validation loss: 2.426375659362836

Epoch: 6| Step: 5
Training loss: 2.5626376975675162
Validation loss: 2.427767287583243

Epoch: 6| Step: 6
Training loss: 2.569288797790672
Validation loss: 2.4402795976489244

Epoch: 6| Step: 7
Training loss: 2.1420847795094855
Validation loss: 2.4270279974986515

Epoch: 6| Step: 8
Training loss: 2.2744686763851414
Validation loss: 2.429173656602653

Epoch: 6| Step: 9
Training loss: 2.329870913453824
Validation loss: 2.4306205659562305

Epoch: 6| Step: 10
Training loss: 2.9400944716657214
Validation loss: 2.443994000919883

Epoch: 6| Step: 11
Training loss: 2.5066071938331356
Validation loss: 2.415177605403637

Epoch: 6| Step: 12
Training loss: 2.365765638718791
Validation loss: 2.4322245256276407

Epoch: 6| Step: 13
Training loss: 2.724842304951431
Validation loss: 2.408311762562172

Epoch: 172| Step: 0
Training loss: 2.378796304000516
Validation loss: 2.418787378635651

Epoch: 6| Step: 1
Training loss: 2.1825542671299805
Validation loss: 2.3980268954168724

Epoch: 6| Step: 2
Training loss: 2.515106337254016
Validation loss: 2.449382965082727

Epoch: 6| Step: 3
Training loss: 2.4914333434140463
Validation loss: 2.454141499619615

Epoch: 6| Step: 4
Training loss: 2.5217158346515616
Validation loss: 2.427960443605512

Epoch: 6| Step: 5
Training loss: 2.1797102513373554
Validation loss: 2.416568820600572

Epoch: 6| Step: 6
Training loss: 2.562850369947062
Validation loss: 2.4385923499509223

Epoch: 6| Step: 7
Training loss: 2.6208110264575293
Validation loss: 2.4317482471268232

Epoch: 6| Step: 8
Training loss: 2.3712018410995994
Validation loss: 2.4103101136528555

Epoch: 6| Step: 9
Training loss: 2.198043265169419
Validation loss: 2.4382027989858903

Epoch: 6| Step: 10
Training loss: 2.4803160132293987
Validation loss: 2.433428351420619

Epoch: 6| Step: 11
Training loss: 2.926753413566514
Validation loss: 2.4132572945431296

Epoch: 6| Step: 12
Training loss: 2.483378852409992
Validation loss: 2.4379994375313077

Epoch: 6| Step: 13
Training loss: 2.6515227495224987
Validation loss: 2.4580761264115245

Epoch: 173| Step: 0
Training loss: 1.686735757209302
Validation loss: 2.421727770174985

Epoch: 6| Step: 1
Training loss: 2.1120283604798473
Validation loss: 2.4179871367086228

Epoch: 6| Step: 2
Training loss: 2.0618078052849245
Validation loss: 2.4180707233281016

Epoch: 6| Step: 3
Training loss: 2.6165781706729474
Validation loss: 2.406270863815861

Epoch: 6| Step: 4
Training loss: 3.3897265109580443
Validation loss: 2.4299828097619374

Epoch: 6| Step: 5
Training loss: 2.5461014585094457
Validation loss: 2.422427539070847

Epoch: 6| Step: 6
Training loss: 2.475200483120635
Validation loss: 2.4453449474484343

Epoch: 6| Step: 7
Training loss: 2.5318736497229914
Validation loss: 2.428991807903062

Epoch: 6| Step: 8
Training loss: 2.364261442841079
Validation loss: 2.43848649753691

Epoch: 6| Step: 9
Training loss: 2.6959192450000287
Validation loss: 2.427144579321709

Epoch: 6| Step: 10
Training loss: 1.8828112337100253
Validation loss: 2.4287260414052128

Epoch: 6| Step: 11
Training loss: 2.9279812739873035
Validation loss: 2.4034704490732883

Epoch: 6| Step: 12
Training loss: 2.492039212685273
Validation loss: 2.4316905195488636

Epoch: 6| Step: 13
Training loss: 2.0890436369929524
Validation loss: 2.4068319327893404

Epoch: 174| Step: 0
Training loss: 2.0810475843819414
Validation loss: 2.438575803816489

Epoch: 6| Step: 1
Training loss: 2.8166992204428936
Validation loss: 2.4245576876315673

Epoch: 6| Step: 2
Training loss: 2.5031413369473117
Validation loss: 2.434367727026875

Epoch: 6| Step: 3
Training loss: 2.4123786065586463
Validation loss: 2.433990434070658

Epoch: 6| Step: 4
Training loss: 2.9070328509216905
Validation loss: 2.4341302322277527

Epoch: 6| Step: 5
Training loss: 2.183165042399671
Validation loss: 2.441765972322774

Epoch: 6| Step: 6
Training loss: 1.9870085774482775
Validation loss: 2.4265614155754767

Epoch: 6| Step: 7
Training loss: 2.2988971968834053
Validation loss: 2.4220207083194727

Epoch: 6| Step: 8
Training loss: 2.3598340326884677
Validation loss: 2.414861937309512

Epoch: 6| Step: 9
Training loss: 2.555072627071128
Validation loss: 2.425975246808805

Epoch: 6| Step: 10
Training loss: 2.2980296029092333
Validation loss: 2.4015531775611008

Epoch: 6| Step: 11
Training loss: 2.255788562538474
Validation loss: 2.415418649160175

Epoch: 6| Step: 12
Training loss: 3.191415513584322
Validation loss: 2.4263168794361487

Epoch: 6| Step: 13
Training loss: 1.9961132787656188
Validation loss: 2.4230077745358103

Epoch: 175| Step: 0
Training loss: 2.741355748117715
Validation loss: 2.411781029159674

Epoch: 6| Step: 1
Training loss: 2.7770485164427723
Validation loss: 2.410009630757834

Epoch: 6| Step: 2
Training loss: 1.611066355394201
Validation loss: 2.4317324830596667

Epoch: 6| Step: 3
Training loss: 2.4919885059661064
Validation loss: 2.407739455040147

Epoch: 6| Step: 4
Training loss: 2.3569825022925563
Validation loss: 2.407072758950343

Epoch: 6| Step: 5
Training loss: 2.519745289781969
Validation loss: 2.4165400997734325

Epoch: 6| Step: 6
Training loss: 3.09655711431571
Validation loss: 2.4210662655886357

Epoch: 6| Step: 7
Training loss: 2.6778376564235846
Validation loss: 2.4406999692436724

Epoch: 6| Step: 8
Training loss: 2.069695146751879
Validation loss: 2.393186630140059

Epoch: 6| Step: 9
Training loss: 2.2726611093080353
Validation loss: 2.420857589795293

Epoch: 6| Step: 10
Training loss: 1.9694259936220624
Validation loss: 2.429698863215748

Epoch: 6| Step: 11
Training loss: 2.4880493630912373
Validation loss: 2.431491099688826

Epoch: 6| Step: 12
Training loss: 2.031782813983385
Validation loss: 2.427299549735871

Epoch: 6| Step: 13
Training loss: 3.295330969666931
Validation loss: 2.4280989735326672

Epoch: 176| Step: 0
Training loss: 2.144729021468914
Validation loss: 2.428599683561556

Epoch: 6| Step: 1
Training loss: 2.178292870859608
Validation loss: 2.4000059194816545

Epoch: 6| Step: 2
Training loss: 2.0586042892504763
Validation loss: 2.415761765131852

Epoch: 6| Step: 3
Training loss: 2.7738783794879462
Validation loss: 2.452087679946675

Epoch: 6| Step: 4
Training loss: 2.204154565365701
Validation loss: 2.3950177934470185

Epoch: 6| Step: 5
Training loss: 2.1622007879628202
Validation loss: 2.410142480592799

Epoch: 6| Step: 6
Training loss: 2.781163503609123
Validation loss: 2.4270871225012622

Epoch: 6| Step: 7
Training loss: 2.214649333191679
Validation loss: 2.420851953895271

Epoch: 6| Step: 8
Training loss: 2.7300216461553126
Validation loss: 2.431138082175829

Epoch: 6| Step: 9
Training loss: 3.2717213650168344
Validation loss: 2.420150340702137

Epoch: 6| Step: 10
Training loss: 2.4175013273722685
Validation loss: 2.4189623458272993

Epoch: 6| Step: 11
Training loss: 2.1808020364014373
Validation loss: 2.4214991145176046

Epoch: 6| Step: 12
Training loss: 2.4399972496642572
Validation loss: 2.4195851213316892

Epoch: 6| Step: 13
Training loss: 2.356635113315596
Validation loss: 2.42024793746175

Epoch: 177| Step: 0
Training loss: 2.976373140814884
Validation loss: 2.4281577324908215

Epoch: 6| Step: 1
Training loss: 2.66984159492624
Validation loss: 2.4208547951445167

Epoch: 6| Step: 2
Training loss: 2.5360380499479285
Validation loss: 2.405818006020117

Epoch: 6| Step: 3
Training loss: 2.2905792315023277
Validation loss: 2.430080966897789

Epoch: 6| Step: 4
Training loss: 2.802820545958642
Validation loss: 2.4108391054021103

Epoch: 6| Step: 5
Training loss: 2.4284502908804
Validation loss: 2.41640933735008

Epoch: 6| Step: 6
Training loss: 2.588053503129059
Validation loss: 2.423802628371109

Epoch: 6| Step: 7
Training loss: 2.2436744996015694
Validation loss: 2.441582891165253

Epoch: 6| Step: 8
Training loss: 2.2527214122681714
Validation loss: 2.4156348671136083

Epoch: 6| Step: 9
Training loss: 2.564198559040057
Validation loss: 2.4383726479987162

Epoch: 6| Step: 10
Training loss: 2.1688835344786987
Validation loss: 2.4219599586851914

Epoch: 6| Step: 11
Training loss: 2.135266467759043
Validation loss: 2.4226267526165772

Epoch: 6| Step: 12
Training loss: 2.2066718154839475
Validation loss: 2.4170396331011723

Epoch: 6| Step: 13
Training loss: 1.8185867183840079
Validation loss: 2.424787747019142

Epoch: 178| Step: 0
Training loss: 2.5792398527625564
Validation loss: 2.4313583355791417

Epoch: 6| Step: 1
Training loss: 2.214362736430328
Validation loss: 2.4225054261437315

Epoch: 6| Step: 2
Training loss: 3.0326464081319093
Validation loss: 2.4264118657027365

Epoch: 6| Step: 3
Training loss: 2.601724064022963
Validation loss: 2.4107504518201477

Epoch: 6| Step: 4
Training loss: 2.212751311776209
Validation loss: 2.425117109942887

Epoch: 6| Step: 5
Training loss: 2.7958837702636585
Validation loss: 2.412299425425645

Epoch: 6| Step: 6
Training loss: 2.2052957648698683
Validation loss: 2.400688792672676

Epoch: 6| Step: 7
Training loss: 1.873741872208713
Validation loss: 2.4207682250294305

Epoch: 6| Step: 8
Training loss: 2.5113692210993306
Validation loss: 2.4267107597367215

Epoch: 6| Step: 9
Training loss: 2.0616058954745973
Validation loss: 2.4150896545731495

Epoch: 6| Step: 10
Training loss: 2.6788803085980137
Validation loss: 2.4234875833636247

Epoch: 6| Step: 11
Training loss: 2.6394387012610925
Validation loss: 2.412526331490117

Epoch: 6| Step: 12
Training loss: 1.9902568362899307
Validation loss: 2.4209571393149276

Epoch: 6| Step: 13
Training loss: 2.6891895572274054
Validation loss: 2.426200017981922

Epoch: 179| Step: 0
Training loss: 2.0731960728135066
Validation loss: 2.410389630815579

Epoch: 6| Step: 1
Training loss: 2.4664654361848024
Validation loss: 2.4264140506600893

Epoch: 6| Step: 2
Training loss: 2.4841575287458144
Validation loss: 2.424931853097693

Epoch: 6| Step: 3
Training loss: 2.545213122034846
Validation loss: 2.4308490512296186

Epoch: 6| Step: 4
Training loss: 2.7222365024304547
Validation loss: 2.4189765864531942

Epoch: 6| Step: 5
Training loss: 2.2954211142480676
Validation loss: 2.438684319349504

Epoch: 6| Step: 6
Training loss: 2.6693649548319467
Validation loss: 2.4241278515785125

Epoch: 6| Step: 7
Training loss: 2.745198740200691
Validation loss: 2.4140901256997913

Epoch: 6| Step: 8
Training loss: 2.4557826716290343
Validation loss: 2.417969700509959

Epoch: 6| Step: 9
Training loss: 2.125515146083525
Validation loss: 2.438746027363047

Epoch: 6| Step: 10
Training loss: 2.7965706601068057
Validation loss: 2.4186215642727342

Epoch: 6| Step: 11
Training loss: 2.253846166064402
Validation loss: 2.398559823724234

Epoch: 6| Step: 12
Training loss: 2.159144352520686
Validation loss: 2.419088540478611

Epoch: 6| Step: 13
Training loss: 2.2393296800435776
Validation loss: 2.417000431043833

Epoch: 180| Step: 0
Training loss: 2.582429194407548
Validation loss: 2.433672692325373

Epoch: 6| Step: 1
Training loss: 2.4441699201386844
Validation loss: 2.407880885890616

Epoch: 6| Step: 2
Training loss: 2.0608844787857232
Validation loss: 2.4024861109908344

Epoch: 6| Step: 3
Training loss: 2.391467675183793
Validation loss: 2.4358333293505074

Epoch: 6| Step: 4
Training loss: 2.596222032700326
Validation loss: 2.4058650091947125

Epoch: 6| Step: 5
Training loss: 2.4407376037489166
Validation loss: 2.4399446765441595

Epoch: 6| Step: 6
Training loss: 2.5789577179689815
Validation loss: 2.41175160210201

Epoch: 6| Step: 7
Training loss: 1.8228053758345641
Validation loss: 2.4105474725223677

Epoch: 6| Step: 8
Training loss: 2.8513912619818154
Validation loss: 2.425438340669305

Epoch: 6| Step: 9
Training loss: 2.1536278745063147
Validation loss: 2.421669792084803

Epoch: 6| Step: 10
Training loss: 2.717750453269232
Validation loss: 2.4024921933290666

Epoch: 6| Step: 11
Training loss: 2.3754447721317367
Validation loss: 2.4336437082674407

Epoch: 6| Step: 12
Training loss: 2.450264496052713
Validation loss: 2.413450413118631

Epoch: 6| Step: 13
Training loss: 2.2937501164158265
Validation loss: 2.443866970696519

Epoch: 181| Step: 0
Training loss: 2.635625677653699
Validation loss: 2.41989349166109

Epoch: 6| Step: 1
Training loss: 2.1597449685466565
Validation loss: 2.417664582465375

Epoch: 6| Step: 2
Training loss: 2.2222518150690385
Validation loss: 2.433470476699188

Epoch: 6| Step: 3
Training loss: 2.3300529536177064
Validation loss: 2.4346308096033633

Epoch: 6| Step: 4
Training loss: 2.458862592546285
Validation loss: 2.4334188108170944

Epoch: 6| Step: 5
Training loss: 2.602184324440107
Validation loss: 2.4105270849550746

Epoch: 6| Step: 6
Training loss: 3.0875644692551183
Validation loss: 2.4262226132484583

Epoch: 6| Step: 7
Training loss: 2.5287031382632486
Validation loss: 2.408347615618602

Epoch: 6| Step: 8
Training loss: 2.489615425530822
Validation loss: 2.4299516178525242

Epoch: 6| Step: 9
Training loss: 2.16527416035098
Validation loss: 2.4303237345277093

Epoch: 6| Step: 10
Training loss: 2.0520139059595732
Validation loss: 2.413178860921516

Epoch: 6| Step: 11
Training loss: 1.9820030634420454
Validation loss: 2.4260959982160393

Epoch: 6| Step: 12
Training loss: 2.7022064941698645
Validation loss: 2.4221305180462718

Epoch: 6| Step: 13
Training loss: 2.3006242526881446
Validation loss: 2.4144818113413455

Epoch: 182| Step: 0
Training loss: 2.7643143301405346
Validation loss: 2.425875411807624

Epoch: 6| Step: 1
Training loss: 2.0987453936400766
Validation loss: 2.4288398681438057

Epoch: 6| Step: 2
Training loss: 2.3215664644473213
Validation loss: 2.4256587432934076

Epoch: 6| Step: 3
Training loss: 2.6700027777268036
Validation loss: 2.4247979850128454

Epoch: 6| Step: 4
Training loss: 2.6573237324605183
Validation loss: 2.4252068430003475

Epoch: 6| Step: 5
Training loss: 2.4024305870674993
Validation loss: 2.4283429681031072

Epoch: 6| Step: 6
Training loss: 2.1454381362790076
Validation loss: 2.405818533492258

Epoch: 6| Step: 7
Training loss: 2.1411412862514303
Validation loss: 2.4305652575633716

Epoch: 6| Step: 8
Training loss: 1.9522760606184806
Validation loss: 2.420731260732314

Epoch: 6| Step: 9
Training loss: 2.190539183411815
Validation loss: 2.4172081575828015

Epoch: 6| Step: 10
Training loss: 2.2076294214968897
Validation loss: 2.424720960851133

Epoch: 6| Step: 11
Training loss: 2.87308903085575
Validation loss: 2.42426913848917

Epoch: 6| Step: 12
Training loss: 2.8094011825342866
Validation loss: 2.4303250093203848

Epoch: 6| Step: 13
Training loss: 2.5141029255084306
Validation loss: 2.426676841882637

Epoch: 183| Step: 0
Training loss: 1.8072634061716375
Validation loss: 2.4045296048196607

Epoch: 6| Step: 1
Training loss: 2.129290120924667
Validation loss: 2.401809210592073

Epoch: 6| Step: 2
Training loss: 1.7611915493126695
Validation loss: 2.4091545902523195

Epoch: 6| Step: 3
Training loss: 2.7907870221066293
Validation loss: 2.412210084758135

Epoch: 6| Step: 4
Training loss: 2.2233435133091395
Validation loss: 2.418829019106911

Epoch: 6| Step: 5
Training loss: 2.611887196487025
Validation loss: 2.411730807756862

Epoch: 6| Step: 6
Training loss: 2.09447785437144
Validation loss: 2.4143568417624914

Epoch: 6| Step: 7
Training loss: 3.0868868755723082
Validation loss: 2.444407563263026

Epoch: 6| Step: 8
Training loss: 2.7172086007437954
Validation loss: 2.4275762895427966

Epoch: 6| Step: 9
Training loss: 2.2682636548882846
Validation loss: 2.43105553855456

Epoch: 6| Step: 10
Training loss: 2.226475657057797
Validation loss: 2.4325686272033527

Epoch: 6| Step: 11
Training loss: 2.730736278694685
Validation loss: 2.417616317775247

Epoch: 6| Step: 12
Training loss: 2.830233467635354
Validation loss: 2.4098926114863657

Epoch: 6| Step: 13
Training loss: 2.1085397832861004
Validation loss: 2.4136125199819594

Epoch: 184| Step: 0
Training loss: 2.43292687648855
Validation loss: 2.4258495689297406

Epoch: 6| Step: 1
Training loss: 2.2588218887787126
Validation loss: 2.421817516606662

Epoch: 6| Step: 2
Training loss: 2.2911061612945343
Validation loss: 2.4254977882143773

Epoch: 6| Step: 3
Training loss: 2.868268300423252
Validation loss: 2.415238730490739

Epoch: 6| Step: 4
Training loss: 2.774977479448093
Validation loss: 2.4058564035586016

Epoch: 6| Step: 5
Training loss: 1.9001659170287075
Validation loss: 2.4143748110735213

Epoch: 6| Step: 6
Training loss: 2.6796388315835986
Validation loss: 2.418567554498986

Epoch: 6| Step: 7
Training loss: 2.605422081021783
Validation loss: 2.400734974710311

Epoch: 6| Step: 8
Training loss: 2.7104723968366975
Validation loss: 2.4291352603725267

Epoch: 6| Step: 9
Training loss: 2.101702263591386
Validation loss: 2.3998484420093553

Epoch: 6| Step: 10
Training loss: 2.5641463968614104
Validation loss: 2.429451525187628

Epoch: 6| Step: 11
Training loss: 2.5390013702076373
Validation loss: 2.412892621350836

Epoch: 6| Step: 12
Training loss: 1.9390611204377584
Validation loss: 2.3944209946877084

Epoch: 6| Step: 13
Training loss: 1.2770682688203652
Validation loss: 2.4136083595009623

Epoch: 185| Step: 0
Training loss: 3.2121627214954778
Validation loss: 2.4073511338824174

Epoch: 6| Step: 1
Training loss: 2.355120631568477
Validation loss: 2.391941670691553

Epoch: 6| Step: 2
Training loss: 2.42311064669904
Validation loss: 2.427447293897901

Epoch: 6| Step: 3
Training loss: 2.7370221212968415
Validation loss: 2.415034798323207

Epoch: 6| Step: 4
Training loss: 2.0940531041535544
Validation loss: 2.4102025303741934

Epoch: 6| Step: 5
Training loss: 3.0739550544068908
Validation loss: 2.405958687800024

Epoch: 6| Step: 6
Training loss: 2.2140059997628923
Validation loss: 2.4298058122177193

Epoch: 6| Step: 7
Training loss: 1.9706351927285868
Validation loss: 2.4257251485385916

Epoch: 6| Step: 8
Training loss: 2.0750362990547946
Validation loss: 2.3999065862100366

Epoch: 6| Step: 9
Training loss: 2.122649407402076
Validation loss: 2.417526370842969

Epoch: 6| Step: 10
Training loss: 2.2375559538765764
Validation loss: 2.3968535774038253

Epoch: 6| Step: 11
Training loss: 2.29836147242253
Validation loss: 2.4081382017824833

Epoch: 6| Step: 12
Training loss: 2.1225894270211336
Validation loss: 2.4300288449358827

Epoch: 6| Step: 13
Training loss: 2.543317310921066
Validation loss: 2.4182608363719598

Epoch: 186| Step: 0
Training loss: 2.3173017395215814
Validation loss: 2.404525537372424

Epoch: 6| Step: 1
Training loss: 2.210286937745407
Validation loss: 2.4208408366680714

Epoch: 6| Step: 2
Training loss: 2.4474494542327467
Validation loss: 2.4155268327257304

Epoch: 6| Step: 3
Training loss: 2.964702215824656
Validation loss: 2.410025053976962

Epoch: 6| Step: 4
Training loss: 2.2911176081574642
Validation loss: 2.419801677881812

Epoch: 6| Step: 5
Training loss: 2.5085830217795553
Validation loss: 2.417608150559721

Epoch: 6| Step: 6
Training loss: 2.4381694363493076
Validation loss: 2.410687224281864

Epoch: 6| Step: 7
Training loss: 2.1570218681178397
Validation loss: 2.4109693256561884

Epoch: 6| Step: 8
Training loss: 2.2256784926180244
Validation loss: 2.3886241170711684

Epoch: 6| Step: 9
Training loss: 2.241520480053662
Validation loss: 2.407950691113485

Epoch: 6| Step: 10
Training loss: 1.9879227410583928
Validation loss: 2.408911743717585

Epoch: 6| Step: 11
Training loss: 2.6293556770138795
Validation loss: 2.409879176735092

Epoch: 6| Step: 12
Training loss: 2.8781684333763007
Validation loss: 2.4221261626283366

Epoch: 6| Step: 13
Training loss: 2.1802010973649133
Validation loss: 2.4000361349263675

Epoch: 187| Step: 0
Training loss: 3.10117258844777
Validation loss: 2.412778916734148

Epoch: 6| Step: 1
Training loss: 2.5084226822596616
Validation loss: 2.4100888477624727

Epoch: 6| Step: 2
Training loss: 2.3107837803843743
Validation loss: 2.4074628788550685

Epoch: 6| Step: 3
Training loss: 1.949645765848101
Validation loss: 2.401862241663245

Epoch: 6| Step: 4
Training loss: 1.7967772084038145
Validation loss: 2.429348814949352

Epoch: 6| Step: 5
Training loss: 2.415528873638211
Validation loss: 2.4267698692275976

Epoch: 6| Step: 6
Training loss: 2.298295081606148
Validation loss: 2.409412992665806

Epoch: 6| Step: 7
Training loss: 2.8364553267768526
Validation loss: 2.3949434804506073

Epoch: 6| Step: 8
Training loss: 2.438383773608798
Validation loss: 2.4002756333719106

Epoch: 6| Step: 9
Training loss: 2.4761479268418167
Validation loss: 2.399808343802676

Epoch: 6| Step: 10
Training loss: 2.2906162860283072
Validation loss: 2.4096651818679007

Epoch: 6| Step: 11
Training loss: 2.1570688434379677
Validation loss: 2.399328310821549

Epoch: 6| Step: 12
Training loss: 2.638693018107835
Validation loss: 2.3982237851277795

Epoch: 6| Step: 13
Training loss: 1.8437396550292409
Validation loss: 2.4097093290208433

Epoch: 188| Step: 0
Training loss: 2.094471365938044
Validation loss: 2.421456375488089

Epoch: 6| Step: 1
Training loss: 3.1985971594516474
Validation loss: 2.404518263927998

Epoch: 6| Step: 2
Training loss: 1.9936663953896492
Validation loss: 2.4153222843242363

Epoch: 6| Step: 3
Training loss: 2.1773708707272186
Validation loss: 2.4041379863075947

Epoch: 6| Step: 4
Training loss: 2.5241178652391865
Validation loss: 2.401981334226352

Epoch: 6| Step: 5
Training loss: 1.7347870758180362
Validation loss: 2.417718586962223

Epoch: 6| Step: 6
Training loss: 2.691816545059462
Validation loss: 2.4193067892491977

Epoch: 6| Step: 7
Training loss: 2.881128330930358
Validation loss: 2.4008460260224225

Epoch: 6| Step: 8
Training loss: 2.5837973413804076
Validation loss: 2.40464406252276

Epoch: 6| Step: 9
Training loss: 2.6411777798067053
Validation loss: 2.4216124160372416

Epoch: 6| Step: 10
Training loss: 1.4967367916770036
Validation loss: 2.409219860642117

Epoch: 6| Step: 11
Training loss: 2.5538476152477734
Validation loss: 2.4218534894048176

Epoch: 6| Step: 12
Training loss: 2.222811138743125
Validation loss: 2.4220037949394153

Epoch: 6| Step: 13
Training loss: 1.9335058327156047
Validation loss: 2.4299764760395126

Epoch: 189| Step: 0
Training loss: 2.3159684640783054
Validation loss: 2.3920292765687026

Epoch: 6| Step: 1
Training loss: 2.269335634924457
Validation loss: 2.3885391854923546

Epoch: 6| Step: 2
Training loss: 1.5122242952124403
Validation loss: 2.4290726749684644

Epoch: 6| Step: 3
Training loss: 2.697268453564773
Validation loss: 2.399282842714927

Epoch: 6| Step: 4
Training loss: 2.9297477207352376
Validation loss: 2.4070618454102153

Epoch: 6| Step: 5
Training loss: 2.612543249913007
Validation loss: 2.4113256573493125

Epoch: 6| Step: 6
Training loss: 2.356830766172229
Validation loss: 2.410464619438302

Epoch: 6| Step: 7
Training loss: 1.7202293271947404
Validation loss: 2.399885246180817

Epoch: 6| Step: 8
Training loss: 2.5987744670851924
Validation loss: 2.413340382650959

Epoch: 6| Step: 9
Training loss: 2.6292156700569187
Validation loss: 2.4192168998704697

Epoch: 6| Step: 10
Training loss: 2.404903084433969
Validation loss: 2.413603466120338

Epoch: 6| Step: 11
Training loss: 1.9044127443715249
Validation loss: 2.432889876692544

Epoch: 6| Step: 12
Training loss: 2.357578022738756
Validation loss: 2.3900763713016593

Epoch: 6| Step: 13
Training loss: 2.5070289980318647
Validation loss: 2.410612630754449

Epoch: 190| Step: 0
Training loss: 1.77777322463936
Validation loss: 2.4110607006400895

Epoch: 6| Step: 1
Training loss: 2.6369227351650766
Validation loss: 2.414205821210865

Epoch: 6| Step: 2
Training loss: 2.70381570271497
Validation loss: 2.39842539597819

Epoch: 6| Step: 3
Training loss: 2.235986515256998
Validation loss: 2.408991136234809

Epoch: 6| Step: 4
Training loss: 3.071432516025786
Validation loss: 2.404176542902649

Epoch: 6| Step: 5
Training loss: 2.4859679291575354
Validation loss: 2.4083197696952614

Epoch: 6| Step: 6
Training loss: 2.2041123795148834
Validation loss: 2.4254873211720587

Epoch: 6| Step: 7
Training loss: 2.276725043762694
Validation loss: 2.4267479953025477

Epoch: 6| Step: 8
Training loss: 2.5460696204915796
Validation loss: 2.4425730043426666

Epoch: 6| Step: 9
Training loss: 2.3082466187239055
Validation loss: 2.4222433243469834

Epoch: 6| Step: 10
Training loss: 2.14501325127481
Validation loss: 2.4418675009511497

Epoch: 6| Step: 11
Training loss: 2.373606222658708
Validation loss: 2.42417491428935

Epoch: 6| Step: 12
Training loss: 2.187507629381227
Validation loss: 2.408074502171667

Epoch: 6| Step: 13
Training loss: 2.315394368700737
Validation loss: 2.413242036988493

Epoch: 191| Step: 0
Training loss: 2.3449169051489274
Validation loss: 2.4089178694284064

Epoch: 6| Step: 1
Training loss: 2.155490022604797
Validation loss: 2.4042506483538326

Epoch: 6| Step: 2
Training loss: 2.6261828346142915
Validation loss: 2.438372716338

Epoch: 6| Step: 3
Training loss: 2.218137522232425
Validation loss: 2.4209509604107438

Epoch: 6| Step: 4
Training loss: 2.334075593961433
Validation loss: 2.404562420284496

Epoch: 6| Step: 5
Training loss: 2.2740863516456913
Validation loss: 2.3891043628647113

Epoch: 6| Step: 6
Training loss: 2.024231508215535
Validation loss: 2.424714399810195

Epoch: 6| Step: 7
Training loss: 2.443429630284919
Validation loss: 2.4347386080680877

Epoch: 6| Step: 8
Training loss: 2.601617486046166
Validation loss: 2.3929880489780815

Epoch: 6| Step: 9
Training loss: 2.4079725367035048
Validation loss: 2.4115340927627558

Epoch: 6| Step: 10
Training loss: 2.398400719574966
Validation loss: 2.4069775313995714

Epoch: 6| Step: 11
Training loss: 1.98287745159714
Validation loss: 2.4133492717846607

Epoch: 6| Step: 12
Training loss: 3.0175354593464325
Validation loss: 2.4031389290533047

Epoch: 6| Step: 13
Training loss: 1.4984124842873665
Validation loss: 2.4071737667571567

Epoch: 192| Step: 0
Training loss: 2.2531603764099284
Validation loss: 2.392061970937259

Epoch: 6| Step: 1
Training loss: 2.216489473110942
Validation loss: 2.396885711825247

Epoch: 6| Step: 2
Training loss: 1.9986621435155365
Validation loss: 2.3940131215333986

Epoch: 6| Step: 3
Training loss: 2.132596406713995
Validation loss: 2.40683811279039

Epoch: 6| Step: 4
Training loss: 2.4434078708778886
Validation loss: 2.397636990521669

Epoch: 6| Step: 5
Training loss: 3.3536944955390884
Validation loss: 2.399128809340074

Epoch: 6| Step: 6
Training loss: 1.8198220570528145
Validation loss: 2.412955016575425

Epoch: 6| Step: 7
Training loss: 2.470162868612745
Validation loss: 2.435262800307611

Epoch: 6| Step: 8
Training loss: 2.022042754818313
Validation loss: 2.4212129776379374

Epoch: 6| Step: 9
Training loss: 2.231285253080662
Validation loss: 2.409569400319376

Epoch: 6| Step: 10
Training loss: 2.637153284506588
Validation loss: 2.4129577906270776

Epoch: 6| Step: 11
Training loss: 2.03933737352729
Validation loss: 2.416504881736562

Epoch: 6| Step: 12
Training loss: 2.7868452047195515
Validation loss: 2.4015704484644007

Epoch: 6| Step: 13
Training loss: 2.1019317416025194
Validation loss: 2.415908017242736

Epoch: 193| Step: 0
Training loss: 2.2230419501406087
Validation loss: 2.4224957631522366

Epoch: 6| Step: 1
Training loss: 3.0169951335600946
Validation loss: 2.404348113212594

Epoch: 6| Step: 2
Training loss: 1.8790861111084864
Validation loss: 2.3848139030925495

Epoch: 6| Step: 3
Training loss: 2.0502303482526045
Validation loss: 2.4180092701253604

Epoch: 6| Step: 4
Training loss: 2.4527498189163803
Validation loss: 2.3885234636101913

Epoch: 6| Step: 5
Training loss: 1.8248491851989614
Validation loss: 2.389206320239214

Epoch: 6| Step: 6
Training loss: 2.869120847713065
Validation loss: 2.413937648368513

Epoch: 6| Step: 7
Training loss: 2.6390562194062883
Validation loss: 2.400731914231407

Epoch: 6| Step: 8
Training loss: 2.2671608816170865
Validation loss: 2.4010174424649775

Epoch: 6| Step: 9
Training loss: 2.1247980358413443
Validation loss: 2.422939416212519

Epoch: 6| Step: 10
Training loss: 2.3036791648468675
Validation loss: 2.4038964022298135

Epoch: 6| Step: 11
Training loss: 1.9979528440422716
Validation loss: 2.4062349936661005

Epoch: 6| Step: 12
Training loss: 2.8410578130461044
Validation loss: 2.3949670353424986

Epoch: 6| Step: 13
Training loss: 1.636253199679843
Validation loss: 2.394243853940788

Epoch: 194| Step: 0
Training loss: 2.287667223545329
Validation loss: 2.4269020033602824

Epoch: 6| Step: 1
Training loss: 2.4172443050325723
Validation loss: 2.4105146906762625

Epoch: 6| Step: 2
Training loss: 2.8345077735900603
Validation loss: 2.413814049895134

Epoch: 6| Step: 3
Training loss: 2.094158303602981
Validation loss: 2.392321347288082

Epoch: 6| Step: 4
Training loss: 2.350300092001219
Validation loss: 2.4282594584722594

Epoch: 6| Step: 5
Training loss: 2.64461437190499
Validation loss: 2.3831903823375873

Epoch: 6| Step: 6
Training loss: 2.341856636767424
Validation loss: 2.4066762239067967

Epoch: 6| Step: 7
Training loss: 2.397021145698303
Validation loss: 2.420509834850808

Epoch: 6| Step: 8
Training loss: 1.5837818397266015
Validation loss: 2.397837916206201

Epoch: 6| Step: 9
Training loss: 2.5286197890182205
Validation loss: 2.415223956206651

Epoch: 6| Step: 10
Training loss: 2.5822260749942814
Validation loss: 2.41219299099738

Epoch: 6| Step: 11
Training loss: 1.4298855170193228
Validation loss: 2.41737706058185

Epoch: 6| Step: 12
Training loss: 2.8790513394155086
Validation loss: 2.426856047784816

Epoch: 6| Step: 13
Training loss: 1.9940465891875405
Validation loss: 2.4069909908671856

Epoch: 195| Step: 0
Training loss: 2.4280131664076245
Validation loss: 2.418557310801008

Epoch: 6| Step: 1
Training loss: 2.0889535879820116
Validation loss: 2.4198186861112463

Epoch: 6| Step: 2
Training loss: 2.2113541389125233
Validation loss: 2.4139541127478896

Epoch: 6| Step: 3
Training loss: 2.3945190031102497
Validation loss: 2.41075479269734

Epoch: 6| Step: 4
Training loss: 1.6573806358383707
Validation loss: 2.418034728181144

Epoch: 6| Step: 5
Training loss: 1.948556240841232
Validation loss: 2.4068542625217337

Epoch: 6| Step: 6
Training loss: 2.5120021725420454
Validation loss: 2.403982024020173

Epoch: 6| Step: 7
Training loss: 2.0186432706887256
Validation loss: 2.4012462238855328

Epoch: 6| Step: 8
Training loss: 2.106278584000509
Validation loss: 2.4134467728519673

Epoch: 6| Step: 9
Training loss: 3.3527962074591473
Validation loss: 2.42288056297329

Epoch: 6| Step: 10
Training loss: 2.5080925615742964
Validation loss: 2.4048313305131543

Epoch: 6| Step: 11
Training loss: 2.6040959666509664
Validation loss: 2.418741277363282

Epoch: 6| Step: 12
Training loss: 2.203084958842138
Validation loss: 2.395328335464553

Epoch: 6| Step: 13
Training loss: 2.086122997479778
Validation loss: 2.416062222874477

Epoch: 196| Step: 0
Training loss: 3.060061359662963
Validation loss: 2.4110822862582193

Epoch: 6| Step: 1
Training loss: 1.9711748962996047
Validation loss: 2.398589864999582

Epoch: 6| Step: 2
Training loss: 2.151611646701759
Validation loss: 2.3915087440195046

Epoch: 6| Step: 3
Training loss: 2.6523774584508875
Validation loss: 2.4078657843160256

Epoch: 6| Step: 4
Training loss: 2.3352504983045272
Validation loss: 2.404359381335792

Epoch: 6| Step: 5
Training loss: 1.656361846026382
Validation loss: 2.392075870141883

Epoch: 6| Step: 6
Training loss: 2.341412001186839
Validation loss: 2.3996752306003266

Epoch: 6| Step: 7
Training loss: 2.331412944263823
Validation loss: 2.407853306068153

Epoch: 6| Step: 8
Training loss: 2.4180094726288606
Validation loss: 2.3959705727225415

Epoch: 6| Step: 9
Training loss: 2.807322929552679
Validation loss: 2.40199358842852

Epoch: 6| Step: 10
Training loss: 2.1002107832346
Validation loss: 2.3972707405431364

Epoch: 6| Step: 11
Training loss: 2.489384812190366
Validation loss: 2.39377008189414

Epoch: 6| Step: 12
Training loss: 1.794633346131153
Validation loss: 2.3941342514313813

Epoch: 6| Step: 13
Training loss: 1.9266568373503081
Validation loss: 2.3890076883011933

Epoch: 197| Step: 0
Training loss: 2.9545465509372626
Validation loss: 2.420641909306931

Epoch: 6| Step: 1
Training loss: 2.0626308515250824
Validation loss: 2.406280259558861

Epoch: 6| Step: 2
Training loss: 2.496901881296455
Validation loss: 2.4217498132991793

Epoch: 6| Step: 3
Training loss: 2.3105011626725447
Validation loss: 2.401697876115817

Epoch: 6| Step: 4
Training loss: 2.4439871124202495
Validation loss: 2.424944184260033

Epoch: 6| Step: 5
Training loss: 2.011223533935149
Validation loss: 2.4410199574685847

Epoch: 6| Step: 6
Training loss: 2.412913718845642
Validation loss: 2.429111876388978

Epoch: 6| Step: 7
Training loss: 2.659238682266542
Validation loss: 2.4201916632455065

Epoch: 6| Step: 8
Training loss: 2.408814487989648
Validation loss: 2.4223352085640237

Epoch: 6| Step: 9
Training loss: 1.6412719177734991
Validation loss: 2.4005540334765647

Epoch: 6| Step: 10
Training loss: 1.9726153378918077
Validation loss: 2.418364161697186

Epoch: 6| Step: 11
Training loss: 2.373566144479234
Validation loss: 2.4080973878094722

Epoch: 6| Step: 12
Training loss: 2.2900842462893234
Validation loss: 2.3885674970697077

Epoch: 6| Step: 13
Training loss: 2.302044610665886
Validation loss: 2.391974918217791

Epoch: 198| Step: 0
Training loss: 2.6302107002071393
Validation loss: 2.4230647980217865

Epoch: 6| Step: 1
Training loss: 2.330456072331319
Validation loss: 2.4076465846874804

Epoch: 6| Step: 2
Training loss: 1.6898865954028588
Validation loss: 2.3830358289304363

Epoch: 6| Step: 3
Training loss: 2.1098363301225014
Validation loss: 2.400682926816486

Epoch: 6| Step: 4
Training loss: 2.4811821339740265
Validation loss: 2.428573912953366

Epoch: 6| Step: 5
Training loss: 2.1865140600373945
Validation loss: 2.411792576128997

Epoch: 6| Step: 6
Training loss: 1.817019976227394
Validation loss: 2.4248037920035377

Epoch: 6| Step: 7
Training loss: 2.3793942807465753
Validation loss: 2.3834656146951887

Epoch: 6| Step: 8
Training loss: 3.3368093168828703
Validation loss: 2.411064004259113

Epoch: 6| Step: 9
Training loss: 2.1416515478449045
Validation loss: 2.38193339930825

Epoch: 6| Step: 10
Training loss: 2.4391909871541353
Validation loss: 2.421862225560038

Epoch: 6| Step: 11
Training loss: 1.9540361644166528
Validation loss: 2.376306126088631

Epoch: 6| Step: 12
Training loss: 2.0625391869723666
Validation loss: 2.3920872453734745

Epoch: 6| Step: 13
Training loss: 2.744005084508517
Validation loss: 2.41827721513189

Epoch: 199| Step: 0
Training loss: 2.349455843481109
Validation loss: 2.3952377056893375

Epoch: 6| Step: 1
Training loss: 1.701956812040561
Validation loss: 2.4223950788584623

Epoch: 6| Step: 2
Training loss: 2.1023783748614817
Validation loss: 2.422031883639028

Epoch: 6| Step: 3
Training loss: 1.8896481220728096
Validation loss: 2.4322597084089903

Epoch: 6| Step: 4
Training loss: 2.7474088599211073
Validation loss: 2.424912354002109

Epoch: 6| Step: 5
Training loss: 2.265950462378938
Validation loss: 2.416667247287

Epoch: 6| Step: 6
Training loss: 1.5458291111123277
Validation loss: 2.411238861001268

Epoch: 6| Step: 7
Training loss: 1.9749448285666003
Validation loss: 2.451774208023693

Epoch: 6| Step: 8
Training loss: 2.078634099338579
Validation loss: 2.4219091216510935

Epoch: 6| Step: 9
Training loss: 2.321236269185628
Validation loss: 2.405050041635434

Epoch: 6| Step: 10
Training loss: 2.5529110766358394
Validation loss: 2.417551632502523

Epoch: 6| Step: 11
Training loss: 3.21630381248933
Validation loss: 2.4041436891141457

Epoch: 6| Step: 12
Training loss: 2.711331385599673
Validation loss: 2.4345830865993148

Epoch: 6| Step: 13
Training loss: 2.970215927392228
Validation loss: 2.413430775583512

Epoch: 200| Step: 0
Training loss: 2.0572155387088342
Validation loss: 2.3876784646598477

Epoch: 6| Step: 1
Training loss: 2.5740300805469043
Validation loss: 2.4219153743240573

Epoch: 6| Step: 2
Training loss: 2.5459855287534667
Validation loss: 2.4163689771221906

Epoch: 6| Step: 3
Training loss: 2.210696149936981
Validation loss: 2.4026139040825356

Epoch: 6| Step: 4
Training loss: 2.106547176779007
Validation loss: 2.3752574554430694

Epoch: 6| Step: 5
Training loss: 1.9273224209771558
Validation loss: 2.4187900288809576

Epoch: 6| Step: 6
Training loss: 1.8230740143215074
Validation loss: 2.4222609049424713

Epoch: 6| Step: 7
Training loss: 2.269295816471495
Validation loss: 2.3813450003988232

Epoch: 6| Step: 8
Training loss: 2.4275882397690993
Validation loss: 2.3853169562021996

Epoch: 6| Step: 9
Training loss: 2.2341656353369608
Validation loss: 2.418481598945285

Epoch: 6| Step: 10
Training loss: 2.409573130495029
Validation loss: 2.394474474180245

Epoch: 6| Step: 11
Training loss: 2.248691178167982
Validation loss: 2.4136951407420724

Epoch: 6| Step: 12
Training loss: 2.0519192108773505
Validation loss: 2.39672777476434

Epoch: 6| Step: 13
Training loss: 3.4177896158415453
Validation loss: 2.3956127858849996

Testing loss: 2.481610574047801

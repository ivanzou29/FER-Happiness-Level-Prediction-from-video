Epoch: 1| Step: 0
Training loss: 7.120574951171875
Validation loss: 7.699383653620238

Epoch: 5| Step: 1
Training loss: 8.302595138549805
Validation loss: 7.694476184024606

Epoch: 5| Step: 2
Training loss: 6.965773582458496
Validation loss: 7.685676610598001

Epoch: 5| Step: 3
Training loss: 7.959360599517822
Validation loss: 7.679439329331921

Epoch: 5| Step: 4
Training loss: 7.68994665145874
Validation loss: 7.672542546385078

Epoch: 5| Step: 5
Training loss: 6.720480442047119
Validation loss: 7.667783173181677

Epoch: 5| Step: 6
Training loss: 8.061490058898926
Validation loss: 7.65819429069437

Epoch: 5| Step: 7
Training loss: 6.948075294494629
Validation loss: 7.654186443615985

Epoch: 5| Step: 8
Training loss: 7.772037506103516
Validation loss: 7.647252267406833

Epoch: 5| Step: 9
Training loss: 6.737240791320801
Validation loss: 7.6400433407034924

Epoch: 5| Step: 10
Training loss: 7.827136039733887
Validation loss: 7.63242599528323

Epoch: 2| Step: 0
Training loss: 8.09516716003418
Validation loss: 7.626923643132692

Epoch: 5| Step: 1
Training loss: 7.129055023193359
Validation loss: 7.619152581819924

Epoch: 5| Step: 2
Training loss: 8.093854904174805
Validation loss: 7.613513726060108

Epoch: 5| Step: 3
Training loss: 7.427398681640625
Validation loss: 7.608023233311151

Epoch: 5| Step: 4
Training loss: 7.0588483810424805
Validation loss: 7.59920927786058

Epoch: 5| Step: 5
Training loss: 6.836747169494629
Validation loss: 7.595720280883133

Epoch: 5| Step: 6
Training loss: 7.066021919250488
Validation loss: 7.586819361614925

Epoch: 5| Step: 7
Training loss: 7.061805725097656
Validation loss: 7.580550265568559

Epoch: 5| Step: 8
Training loss: 6.854389190673828
Validation loss: 7.577084454157019

Epoch: 5| Step: 9
Training loss: 8.555597305297852
Validation loss: 7.56918579019526

Epoch: 5| Step: 10
Training loss: 7.01247501373291
Validation loss: 7.5629545591210805

Epoch: 3| Step: 0
Training loss: 8.660628318786621
Validation loss: 7.556591505645423

Epoch: 5| Step: 1
Training loss: 6.263983726501465
Validation loss: 7.549831523690172

Epoch: 5| Step: 2
Training loss: 7.347085475921631
Validation loss: 7.542497029868505

Epoch: 5| Step: 3
Training loss: 8.060032844543457
Validation loss: 7.538765045904344

Epoch: 5| Step: 4
Training loss: 6.799433708190918
Validation loss: 7.533305880843952

Epoch: 5| Step: 5
Training loss: 8.43669319152832
Validation loss: 7.5229501929334415

Epoch: 5| Step: 6
Training loss: 6.707435607910156
Validation loss: 7.51929885084911

Epoch: 5| Step: 7
Training loss: 7.539568901062012
Validation loss: 7.512253094744938

Epoch: 5| Step: 8
Training loss: 6.662480354309082
Validation loss: 7.503857581846176

Epoch: 5| Step: 9
Training loss: 7.231594085693359
Validation loss: 7.498199662854595

Epoch: 5| Step: 10
Training loss: 6.647863864898682
Validation loss: 7.4908397274632605

Epoch: 4| Step: 0
Training loss: 7.8527727127075195
Validation loss: 7.487364287017494

Epoch: 5| Step: 1
Training loss: 7.232403755187988
Validation loss: 7.479668940267255

Epoch: 5| Step: 2
Training loss: 7.510261535644531
Validation loss: 7.471659291175104

Epoch: 5| Step: 3
Training loss: 7.255755424499512
Validation loss: 7.466477383849441

Epoch: 5| Step: 4
Training loss: 7.086177825927734
Validation loss: 7.459286879467708

Epoch: 5| Step: 5
Training loss: 7.320208549499512
Validation loss: 7.454120984641454

Epoch: 5| Step: 6
Training loss: 6.513627529144287
Validation loss: 7.446293682180425

Epoch: 5| Step: 7
Training loss: 8.053391456604004
Validation loss: 7.440711247023716

Epoch: 5| Step: 8
Training loss: 6.332974910736084
Validation loss: 7.431799165664181

Epoch: 5| Step: 9
Training loss: 7.337263584136963
Validation loss: 7.425869669965518

Epoch: 5| Step: 10
Training loss: 7.1604204177856445
Validation loss: 7.417491430877357

Epoch: 5| Step: 0
Training loss: 5.436932563781738
Validation loss: 7.411784561731482

Epoch: 5| Step: 1
Training loss: 8.29084587097168
Validation loss: 7.401577303486485

Epoch: 5| Step: 2
Training loss: 5.81854772567749
Validation loss: 7.396409003965316

Epoch: 5| Step: 3
Training loss: 7.494541168212891
Validation loss: 7.387068968947216

Epoch: 5| Step: 4
Training loss: 6.527123928070068
Validation loss: 7.383289193594328

Epoch: 5| Step: 5
Training loss: 7.683746337890625
Validation loss: 7.3739981394942085

Epoch: 5| Step: 6
Training loss: 6.512774467468262
Validation loss: 7.365659170253302

Epoch: 5| Step: 7
Training loss: 7.541609287261963
Validation loss: 7.35917180584323

Epoch: 5| Step: 8
Training loss: 8.369987487792969
Validation loss: 7.348597506041168

Epoch: 5| Step: 9
Training loss: 7.482682228088379
Validation loss: 7.3417835850869455

Epoch: 5| Step: 10
Training loss: 7.692495822906494
Validation loss: 7.336253699435983

Epoch: 6| Step: 0
Training loss: 7.3618268966674805
Validation loss: 7.329658231427593

Epoch: 5| Step: 1
Training loss: 6.749748229980469
Validation loss: 7.319174305085213

Epoch: 5| Step: 2
Training loss: 7.128597259521484
Validation loss: 7.309269710253644

Epoch: 5| Step: 3
Training loss: 6.184180736541748
Validation loss: 7.30129369612663

Epoch: 5| Step: 4
Training loss: 6.452490329742432
Validation loss: 7.293541416045158

Epoch: 5| Step: 5
Training loss: 7.001172065734863
Validation loss: 7.286191694198116

Epoch: 5| Step: 6
Training loss: 7.085056304931641
Validation loss: 7.2773634951601744

Epoch: 5| Step: 7
Training loss: 7.120390892028809
Validation loss: 7.268713120491274

Epoch: 5| Step: 8
Training loss: 7.067746162414551
Validation loss: 7.261443307322841

Epoch: 5| Step: 9
Training loss: 7.580116271972656
Validation loss: 7.252208002151981

Epoch: 5| Step: 10
Training loss: 8.200932502746582
Validation loss: 7.240099471102479

Epoch: 7| Step: 0
Training loss: 6.351381778717041
Validation loss: 7.234033215430475

Epoch: 5| Step: 1
Training loss: 7.069378852844238
Validation loss: 7.225143422362625

Epoch: 5| Step: 2
Training loss: 6.929949760437012
Validation loss: 7.215012709299724

Epoch: 5| Step: 3
Training loss: 6.562779426574707
Validation loss: 7.20670348341747

Epoch: 5| Step: 4
Training loss: 6.859822750091553
Validation loss: 7.19731266267838

Epoch: 5| Step: 5
Training loss: 6.899697780609131
Validation loss: 7.187321473193425

Epoch: 5| Step: 6
Training loss: 8.382261276245117
Validation loss: 7.179084142049153

Epoch: 5| Step: 7
Training loss: 6.013315200805664
Validation loss: 7.166937761409308

Epoch: 5| Step: 8
Training loss: 6.745717525482178
Validation loss: 7.158233611814437

Epoch: 5| Step: 9
Training loss: 7.666186332702637
Validation loss: 7.150899528175272

Epoch: 5| Step: 10
Training loss: 7.239647388458252
Validation loss: 7.136594382665491

Epoch: 8| Step: 0
Training loss: 7.416827201843262
Validation loss: 7.128771238429572

Epoch: 5| Step: 1
Training loss: 7.226168155670166
Validation loss: 7.116277848520586

Epoch: 5| Step: 2
Training loss: 6.755579471588135
Validation loss: 7.106335332316737

Epoch: 5| Step: 3
Training loss: 7.215997219085693
Validation loss: 7.099156507881739

Epoch: 5| Step: 4
Training loss: 6.989778995513916
Validation loss: 7.084820619193456

Epoch: 5| Step: 5
Training loss: 6.692843437194824
Validation loss: 7.076118305165281

Epoch: 5| Step: 6
Training loss: 6.103699684143066
Validation loss: 7.062474045702206

Epoch: 5| Step: 7
Training loss: 6.613831996917725
Validation loss: 7.054689294548445

Epoch: 5| Step: 8
Training loss: 6.751338958740234
Validation loss: 7.043307355655137

Epoch: 5| Step: 9
Training loss: 7.1285271644592285
Validation loss: 7.031910706591862

Epoch: 5| Step: 10
Training loss: 6.452158451080322
Validation loss: 7.016372101281279

Epoch: 9| Step: 0
Training loss: 6.541323661804199
Validation loss: 7.009322063897246

Epoch: 5| Step: 1
Training loss: 6.573977470397949
Validation loss: 6.998368981064007

Epoch: 5| Step: 2
Training loss: 7.610403537750244
Validation loss: 6.98612500775245

Epoch: 5| Step: 3
Training loss: 7.295163154602051
Validation loss: 6.977616951029788

Epoch: 5| Step: 4
Training loss: 5.78185510635376
Validation loss: 6.963074027851063

Epoch: 5| Step: 5
Training loss: 6.355380058288574
Validation loss: 6.951198885517735

Epoch: 5| Step: 6
Training loss: 6.255390167236328
Validation loss: 6.939957490531347

Epoch: 5| Step: 7
Training loss: 5.88291072845459
Validation loss: 6.930786855759159

Epoch: 5| Step: 8
Training loss: 7.042821407318115
Validation loss: 6.914527605938655

Epoch: 5| Step: 9
Training loss: 7.922144889831543
Validation loss: 6.9048893220963015

Epoch: 5| Step: 10
Training loss: 6.703901767730713
Validation loss: 6.890224010713639

Epoch: 10| Step: 0
Training loss: 7.6984429359436035
Validation loss: 6.878378001592493

Epoch: 5| Step: 1
Training loss: 7.151162624359131
Validation loss: 6.864952446312032

Epoch: 5| Step: 2
Training loss: 5.858748435974121
Validation loss: 6.853159345606322

Epoch: 5| Step: 3
Training loss: 6.406965732574463
Validation loss: 6.8420986719028924

Epoch: 5| Step: 4
Training loss: 6.193719387054443
Validation loss: 6.826264719809255

Epoch: 5| Step: 5
Training loss: 6.28442907333374
Validation loss: 6.813066933744697

Epoch: 5| Step: 6
Training loss: 5.937877178192139
Validation loss: 6.799763151394424

Epoch: 5| Step: 7
Training loss: 6.922773838043213
Validation loss: 6.787961375328802

Epoch: 5| Step: 8
Training loss: 6.560049533843994
Validation loss: 6.77501287767964

Epoch: 5| Step: 9
Training loss: 7.178788661956787
Validation loss: 6.758106570090017

Epoch: 5| Step: 10
Training loss: 6.202738285064697
Validation loss: 6.744125468756563

Epoch: 11| Step: 0
Training loss: 6.886760711669922
Validation loss: 6.728278257513559

Epoch: 5| Step: 1
Training loss: 7.842787742614746
Validation loss: 6.7153312980487785

Epoch: 5| Step: 2
Training loss: 5.357088088989258
Validation loss: 6.703365448982485

Epoch: 5| Step: 3
Training loss: 6.214746475219727
Validation loss: 6.68621019137803

Epoch: 5| Step: 4
Training loss: 5.881467819213867
Validation loss: 6.677393605632167

Epoch: 5| Step: 5
Training loss: 6.32403039932251
Validation loss: 6.662270551086754

Epoch: 5| Step: 6
Training loss: 5.365569114685059
Validation loss: 6.643181139423001

Epoch: 5| Step: 7
Training loss: 6.209906578063965
Validation loss: 6.628457130924348

Epoch: 5| Step: 8
Training loss: 7.211084842681885
Validation loss: 6.610583587359357

Epoch: 5| Step: 9
Training loss: 6.5658135414123535
Validation loss: 6.592111690070039

Epoch: 5| Step: 10
Training loss: 6.885952949523926
Validation loss: 6.5810553540465655

Epoch: 12| Step: 0
Training loss: 6.306666374206543
Validation loss: 6.565189689718267

Epoch: 5| Step: 1
Training loss: 5.917776107788086
Validation loss: 6.547849311623522

Epoch: 5| Step: 2
Training loss: 6.600635528564453
Validation loss: 6.530742229953889

Epoch: 5| Step: 3
Training loss: 6.9073896408081055
Validation loss: 6.5186325452661

Epoch: 5| Step: 4
Training loss: 7.167439937591553
Validation loss: 6.497617588248304

Epoch: 5| Step: 5
Training loss: 6.44128942489624
Validation loss: 6.481256346548757

Epoch: 5| Step: 6
Training loss: 6.775276184082031
Validation loss: 6.4657744028235

Epoch: 5| Step: 7
Training loss: 5.601772785186768
Validation loss: 6.448795559585736

Epoch: 5| Step: 8
Training loss: 5.30751371383667
Validation loss: 6.429579370765276

Epoch: 5| Step: 9
Training loss: 5.601998329162598
Validation loss: 6.407922796023789

Epoch: 5| Step: 10
Training loss: 6.025496482849121
Validation loss: 6.395059939353697

Epoch: 13| Step: 0
Training loss: 7.176403999328613
Validation loss: 6.371987952980944

Epoch: 5| Step: 1
Training loss: 5.49770450592041
Validation loss: 6.3603197631015576

Epoch: 5| Step: 2
Training loss: 5.636849403381348
Validation loss: 6.337462620068622

Epoch: 5| Step: 3
Training loss: 6.853245735168457
Validation loss: 6.314402026514853

Epoch: 5| Step: 4
Training loss: 5.843278884887695
Validation loss: 6.290573007317

Epoch: 5| Step: 5
Training loss: 6.4250006675720215
Validation loss: 6.280470848083496

Epoch: 5| Step: 6
Training loss: 5.296605587005615
Validation loss: 6.251919100361485

Epoch: 5| Step: 7
Training loss: 6.3946533203125
Validation loss: 6.235972937717233

Epoch: 5| Step: 8
Training loss: 5.592042446136475
Validation loss: 6.211502270032001

Epoch: 5| Step: 9
Training loss: 4.986162185668945
Validation loss: 6.193740926763063

Epoch: 5| Step: 10
Training loss: 6.764684200286865
Validation loss: 6.170996640318183

Epoch: 14| Step: 0
Training loss: 5.869123458862305
Validation loss: 6.14204926131874

Epoch: 5| Step: 1
Training loss: 5.34235143661499
Validation loss: 6.1225173652813

Epoch: 5| Step: 2
Training loss: 5.864717960357666
Validation loss: 6.101426206609254

Epoch: 5| Step: 3
Training loss: 5.2024335861206055
Validation loss: 6.072362053778864

Epoch: 5| Step: 4
Training loss: 5.557027339935303
Validation loss: 6.048994705241213

Epoch: 5| Step: 5
Training loss: 7.212708950042725
Validation loss: 6.025100108115904

Epoch: 5| Step: 6
Training loss: 5.6692705154418945
Validation loss: 6.001721905123803

Epoch: 5| Step: 7
Training loss: 6.087940216064453
Validation loss: 5.973857233601231

Epoch: 5| Step: 8
Training loss: 5.316061973571777
Validation loss: 5.948849780585176

Epoch: 5| Step: 9
Training loss: 5.434613227844238
Validation loss: 5.922097365061442

Epoch: 5| Step: 10
Training loss: 6.060657501220703
Validation loss: 5.897331909466815

Epoch: 15| Step: 0
Training loss: 4.997939109802246
Validation loss: 5.866998375103038

Epoch: 5| Step: 1
Training loss: 4.755641460418701
Validation loss: 5.846523961713237

Epoch: 5| Step: 2
Training loss: 5.935304641723633
Validation loss: 5.8114335357501945

Epoch: 5| Step: 3
Training loss: 5.6281867027282715
Validation loss: 5.791954814746815

Epoch: 5| Step: 4
Training loss: 5.371802806854248
Validation loss: 5.755728880564372

Epoch: 5| Step: 5
Training loss: 5.147303581237793
Validation loss: 5.731281136953703

Epoch: 5| Step: 6
Training loss: 5.894594192504883
Validation loss: 5.696615777989869

Epoch: 5| Step: 7
Training loss: 5.74445104598999
Validation loss: 5.663585826914797

Epoch: 5| Step: 8
Training loss: 5.119259834289551
Validation loss: 5.641295089516588

Epoch: 5| Step: 9
Training loss: 5.956667900085449
Validation loss: 5.605699559693695

Epoch: 5| Step: 10
Training loss: 5.666821479797363
Validation loss: 5.569778662855907

Epoch: 16| Step: 0
Training loss: 5.336656093597412
Validation loss: 5.540302938030612

Epoch: 5| Step: 1
Training loss: 5.557887554168701
Validation loss: 5.506432297409222

Epoch: 5| Step: 2
Training loss: 6.4650678634643555
Validation loss: 5.480880809086625

Epoch: 5| Step: 3
Training loss: 5.963768482208252
Validation loss: 5.436928933666598

Epoch: 5| Step: 4
Training loss: 4.66583251953125
Validation loss: 5.407730353775845

Epoch: 5| Step: 5
Training loss: 4.056873798370361
Validation loss: 5.368044422518823

Epoch: 5| Step: 6
Training loss: 6.099827766418457
Validation loss: 5.333896078089232

Epoch: 5| Step: 7
Training loss: 5.087624549865723
Validation loss: 5.29756857246481

Epoch: 5| Step: 8
Training loss: 4.097867488861084
Validation loss: 5.269281007910288

Epoch: 5| Step: 9
Training loss: 4.052281379699707
Validation loss: 5.229627142670334

Epoch: 5| Step: 10
Training loss: 4.81008768081665
Validation loss: 5.186715484947286

Epoch: 17| Step: 0
Training loss: 4.770997047424316
Validation loss: 5.15207367045905

Epoch: 5| Step: 1
Training loss: 5.036624431610107
Validation loss: 5.115522702534993

Epoch: 5| Step: 2
Training loss: 4.2431111335754395
Validation loss: 5.081820805867513

Epoch: 5| Step: 3
Training loss: 4.3457818031311035
Validation loss: 5.040349606544741

Epoch: 5| Step: 4
Training loss: 5.637280464172363
Validation loss: 4.996380631641675

Epoch: 5| Step: 5
Training loss: 3.5955607891082764
Validation loss: 4.966516397332632

Epoch: 5| Step: 6
Training loss: 5.030050277709961
Validation loss: 4.925136437980077

Epoch: 5| Step: 7
Training loss: 5.389391899108887
Validation loss: 4.8915040057192565

Epoch: 5| Step: 8
Training loss: 5.542778968811035
Validation loss: 4.842401453243789

Epoch: 5| Step: 9
Training loss: 4.314981937408447
Validation loss: 4.823361125043643

Epoch: 5| Step: 10
Training loss: 3.7202696800231934
Validation loss: 4.76721082451523

Epoch: 18| Step: 0
Training loss: 4.080409526824951
Validation loss: 4.73065693660449

Epoch: 5| Step: 1
Training loss: 4.565610408782959
Validation loss: 4.692168445997341

Epoch: 5| Step: 2
Training loss: 4.521430015563965
Validation loss: 4.657371464595999

Epoch: 5| Step: 3
Training loss: 4.978713512420654
Validation loss: 4.62056283027895

Epoch: 5| Step: 4
Training loss: 4.397963523864746
Validation loss: 4.584874014700612

Epoch: 5| Step: 5
Training loss: 5.2268476486206055
Validation loss: 4.53779278519333

Epoch: 5| Step: 6
Training loss: 3.58748197555542
Validation loss: 4.50437569361861

Epoch: 5| Step: 7
Training loss: 3.0130391120910645
Validation loss: 4.461947220627979

Epoch: 5| Step: 8
Training loss: 4.4522600173950195
Validation loss: 4.418118082067018

Epoch: 5| Step: 9
Training loss: 3.914829969406128
Validation loss: 4.3844479027614796

Epoch: 5| Step: 10
Training loss: 4.502511501312256
Validation loss: 4.348412293259815

Epoch: 19| Step: 0
Training loss: 4.124607086181641
Validation loss: 4.307983608656032

Epoch: 5| Step: 1
Training loss: 3.5027427673339844
Validation loss: 4.266787047027259

Epoch: 5| Step: 2
Training loss: 3.140169620513916
Validation loss: 4.224700102242091

Epoch: 5| Step: 3
Training loss: 4.700295925140381
Validation loss: 4.187755407825593

Epoch: 5| Step: 4
Training loss: 4.282408237457275
Validation loss: 4.156530846831619

Epoch: 5| Step: 5
Training loss: 2.941807270050049
Validation loss: 4.121813871527231

Epoch: 5| Step: 6
Training loss: 4.512739658355713
Validation loss: 4.063900116951235

Epoch: 5| Step: 7
Training loss: 3.470507860183716
Validation loss: 4.025333686541486

Epoch: 5| Step: 8
Training loss: 4.061602592468262
Validation loss: 3.990715462674377

Epoch: 5| Step: 9
Training loss: 4.1056976318359375
Validation loss: 3.9610379485673803

Epoch: 5| Step: 10
Training loss: 3.7773914337158203
Validation loss: 3.9249022032624934

Epoch: 20| Step: 0
Training loss: 2.865060329437256
Validation loss: 3.875808628656531

Epoch: 5| Step: 1
Training loss: 3.4238548278808594
Validation loss: 3.83607421382781

Epoch: 5| Step: 2
Training loss: 3.3611462116241455
Validation loss: 3.804842410549041

Epoch: 5| Step: 3
Training loss: 3.725074052810669
Validation loss: 3.7647346040253997

Epoch: 5| Step: 4
Training loss: 2.900358200073242
Validation loss: 3.721239438620947

Epoch: 5| Step: 5
Training loss: 3.5589661598205566
Validation loss: 3.689600234390587

Epoch: 5| Step: 6
Training loss: 4.8188066482543945
Validation loss: 3.6459664785733787

Epoch: 5| Step: 7
Training loss: 4.158542156219482
Validation loss: 3.617179052804106

Epoch: 5| Step: 8
Training loss: 2.6678683757781982
Validation loss: 3.5688547703527633

Epoch: 5| Step: 9
Training loss: 3.8219528198242188
Validation loss: 3.5393190025001444

Epoch: 5| Step: 10
Training loss: 3.2494373321533203
Validation loss: 3.505659149539086

Epoch: 21| Step: 0
Training loss: 2.5833473205566406
Validation loss: 3.462472282430177

Epoch: 5| Step: 1
Training loss: 4.128777027130127
Validation loss: 3.4459944130271993

Epoch: 5| Step: 2
Training loss: 3.9564177989959717
Validation loss: 3.39977660230411

Epoch: 5| Step: 3
Training loss: 3.1716928482055664
Validation loss: 3.3771635870779715

Epoch: 5| Step: 4
Training loss: 3.5633106231689453
Validation loss: 3.334766193102765

Epoch: 5| Step: 5
Training loss: 2.733428955078125
Validation loss: 3.310884198834819

Epoch: 5| Step: 6
Training loss: 3.088491439819336
Validation loss: 3.274347592425603

Epoch: 5| Step: 7
Training loss: 3.163961410522461
Validation loss: 3.2534659959936656

Epoch: 5| Step: 8
Training loss: 2.9664883613586426
Validation loss: 3.221995125534714

Epoch: 5| Step: 9
Training loss: 3.221221446990967
Validation loss: 3.2105741347036054

Epoch: 5| Step: 10
Training loss: 2.7303977012634277
Validation loss: 3.1564842603539907

Epoch: 22| Step: 0
Training loss: 3.245119571685791
Validation loss: 3.1188560301257717

Epoch: 5| Step: 1
Training loss: 2.514735460281372
Validation loss: 3.1071107823361634

Epoch: 5| Step: 2
Training loss: 3.7926907539367676
Validation loss: 3.0810486655081473

Epoch: 5| Step: 3
Training loss: 3.0931107997894287
Validation loss: 3.0499101761848695

Epoch: 5| Step: 4
Training loss: 3.252521514892578
Validation loss: 3.012404026523713

Epoch: 5| Step: 5
Training loss: 3.248924970626831
Validation loss: 2.9963210269969

Epoch: 5| Step: 6
Training loss: 2.64581298828125
Validation loss: 2.9717766854070846

Epoch: 5| Step: 7
Training loss: 2.3198962211608887
Validation loss: 2.9699235680282756

Epoch: 5| Step: 8
Training loss: 3.2137649059295654
Validation loss: 2.9264196272819274

Epoch: 5| Step: 9
Training loss: 3.3565616607666016
Validation loss: 2.8966836416593162

Epoch: 5| Step: 10
Training loss: 2.6579177379608154
Validation loss: 2.8836451217692387

Epoch: 23| Step: 0
Training loss: 3.2943482398986816
Validation loss: 2.8572168914220666

Epoch: 5| Step: 1
Training loss: 2.0110714435577393
Validation loss: 2.8476510676004554

Epoch: 5| Step: 2
Training loss: 2.648664951324463
Validation loss: 2.8263690984377297

Epoch: 5| Step: 3
Training loss: 3.135469913482666
Validation loss: 2.806487037289527

Epoch: 5| Step: 4
Training loss: 2.842636823654175
Validation loss: 2.7865713565580306

Epoch: 5| Step: 5
Training loss: 2.857126474380493
Validation loss: 2.7730216159615466

Epoch: 5| Step: 6
Training loss: 3.1536240577697754
Validation loss: 2.7541558357977096

Epoch: 5| Step: 7
Training loss: 2.573390245437622
Validation loss: 2.724398966758482

Epoch: 5| Step: 8
Training loss: 2.9381613731384277
Validation loss: 2.7264944225229244

Epoch: 5| Step: 9
Training loss: 2.8598086833953857
Validation loss: 2.7068855301026375

Epoch: 5| Step: 10
Training loss: 2.6905999183654785
Validation loss: 2.6786454031544347

Epoch: 24| Step: 0
Training loss: 2.2011077404022217
Validation loss: 2.6621391465587

Epoch: 5| Step: 1
Training loss: 2.35276198387146
Validation loss: 2.6298871988891275

Epoch: 5| Step: 2
Training loss: 3.2086386680603027
Validation loss: 2.6233395914877615

Epoch: 5| Step: 3
Training loss: 3.1492888927459717
Validation loss: 2.6259210930075696

Epoch: 5| Step: 4
Training loss: 3.266007900238037
Validation loss: 2.595960468374273

Epoch: 5| Step: 5
Training loss: 3.251295566558838
Validation loss: 2.5877593025084464

Epoch: 5| Step: 6
Training loss: 1.9276889562606812
Validation loss: 2.562858086760326

Epoch: 5| Step: 7
Training loss: 3.2742931842803955
Validation loss: 2.542255632338985

Epoch: 5| Step: 8
Training loss: 2.2132763862609863
Validation loss: 2.5550345887419996

Epoch: 5| Step: 9
Training loss: 2.510148763656616
Validation loss: 2.5329077089986494

Epoch: 5| Step: 10
Training loss: 2.4930026531219482
Validation loss: 2.5131541836646294

Epoch: 25| Step: 0
Training loss: 2.685359239578247
Validation loss: 2.5266947592458417

Epoch: 5| Step: 1
Training loss: 2.440186023712158
Validation loss: 2.5039453660288165

Epoch: 5| Step: 2
Training loss: 2.2022900581359863
Validation loss: 2.493066500591975

Epoch: 5| Step: 3
Training loss: 3.5667271614074707
Validation loss: 2.4559267977232575

Epoch: 5| Step: 4
Training loss: 2.781463623046875
Validation loss: 2.4634250748542046

Epoch: 5| Step: 5
Training loss: 2.8876230716705322
Validation loss: 2.469631815469393

Epoch: 5| Step: 6
Training loss: 2.660208225250244
Validation loss: 2.4600242491691344

Epoch: 5| Step: 7
Training loss: 2.791163682937622
Validation loss: 2.451161712728521

Epoch: 5| Step: 8
Training loss: 2.0314338207244873
Validation loss: 2.4575726755203737

Epoch: 5| Step: 9
Training loss: 2.3085150718688965
Validation loss: 2.4465908927302205

Epoch: 5| Step: 10
Training loss: 2.7589516639709473
Validation loss: 2.4390521741682485

Epoch: 26| Step: 0
Training loss: 2.954314947128296
Validation loss: 2.4266362728611117

Epoch: 5| Step: 1
Training loss: 1.900204062461853
Validation loss: 2.424268571279382

Epoch: 5| Step: 2
Training loss: 2.6811375617980957
Validation loss: 2.432484557551722

Epoch: 5| Step: 3
Training loss: 2.8330283164978027
Validation loss: 2.419785204754081

Epoch: 5| Step: 4
Training loss: 2.3118293285369873
Validation loss: 2.439662459076092

Epoch: 5| Step: 5
Training loss: 2.2804954051971436
Validation loss: 2.426305260709537

Epoch: 5| Step: 6
Training loss: 2.867915153503418
Validation loss: 2.410542142006659

Epoch: 5| Step: 7
Training loss: 2.788588047027588
Validation loss: 2.4213044835675146

Epoch: 5| Step: 8
Training loss: 2.425907850265503
Validation loss: 2.4055143581923617

Epoch: 5| Step: 9
Training loss: 3.1902542114257812
Validation loss: 2.4072387526112218

Epoch: 5| Step: 10
Training loss: 2.4917943477630615
Validation loss: 2.4253719852816675

Epoch: 27| Step: 0
Training loss: 2.168391704559326
Validation loss: 2.4111145286149878

Epoch: 5| Step: 1
Training loss: 2.709285259246826
Validation loss: 2.4090828716114

Epoch: 5| Step: 2
Training loss: 2.4842443466186523
Validation loss: 2.4233430688099196

Epoch: 5| Step: 3
Training loss: 2.7075562477111816
Validation loss: 2.4136264606188704

Epoch: 5| Step: 4
Training loss: 2.487098217010498
Validation loss: 2.3989951379837526

Epoch: 5| Step: 5
Training loss: 2.7620849609375
Validation loss: 2.4252762281766502

Epoch: 5| Step: 6
Training loss: 3.08158278465271
Validation loss: 2.3906140942727365

Epoch: 5| Step: 7
Training loss: 3.084341287612915
Validation loss: 2.4053669488558205

Epoch: 5| Step: 8
Training loss: 2.030996799468994
Validation loss: 2.4100828939868557

Epoch: 5| Step: 9
Training loss: 2.255399465560913
Validation loss: 2.4025738367470364

Epoch: 5| Step: 10
Training loss: 2.8636419773101807
Validation loss: 2.39049119077703

Epoch: 28| Step: 0
Training loss: 1.9664783477783203
Validation loss: 2.4068952683479554

Epoch: 5| Step: 1
Training loss: 2.936814069747925
Validation loss: 2.424012273870489

Epoch: 5| Step: 2
Training loss: 1.813259482383728
Validation loss: 2.4209708065114994

Epoch: 5| Step: 3
Training loss: 1.5028584003448486
Validation loss: 2.4017376284445486

Epoch: 5| Step: 4
Training loss: 2.642066240310669
Validation loss: 2.386020920609915

Epoch: 5| Step: 5
Training loss: 3.0586795806884766
Validation loss: 2.403606519904188

Epoch: 5| Step: 6
Training loss: 2.9776158332824707
Validation loss: 2.401287627476518

Epoch: 5| Step: 7
Training loss: 2.7968103885650635
Validation loss: 2.404362996419271

Epoch: 5| Step: 8
Training loss: 2.6698241233825684
Validation loss: 2.4121691334632134

Epoch: 5| Step: 9
Training loss: 3.3828799724578857
Validation loss: 2.4116945112905195

Epoch: 5| Step: 10
Training loss: 2.725099563598633
Validation loss: 2.3986106995613343

Epoch: 29| Step: 0
Training loss: 1.8499772548675537
Validation loss: 2.4107221070156304

Epoch: 5| Step: 1
Training loss: 2.1803650856018066
Validation loss: 2.397693077723185

Epoch: 5| Step: 2
Training loss: 3.2267584800720215
Validation loss: 2.3930955740713302

Epoch: 5| Step: 3
Training loss: 2.7354464530944824
Validation loss: 2.4027474464908725

Epoch: 5| Step: 4
Training loss: 2.855036973953247
Validation loss: 2.4028254298753637

Epoch: 5| Step: 5
Training loss: 2.4368720054626465
Validation loss: 2.3870941131345687

Epoch: 5| Step: 6
Training loss: 3.148391008377075
Validation loss: 2.402685391005649

Epoch: 5| Step: 7
Training loss: 2.567138910293579
Validation loss: 2.379286358433385

Epoch: 5| Step: 8
Training loss: 2.2725634574890137
Validation loss: 2.4114507654661774

Epoch: 5| Step: 9
Training loss: 2.431567430496216
Validation loss: 2.422969953988188

Epoch: 5| Step: 10
Training loss: 2.6143548488616943
Validation loss: 2.406330972589472

Epoch: 30| Step: 0
Training loss: 2.392393112182617
Validation loss: 2.3896492578650035

Epoch: 5| Step: 1
Training loss: 2.2861270904541016
Validation loss: 2.384266463659143

Epoch: 5| Step: 2
Training loss: 2.166731595993042
Validation loss: 2.405016013371047

Epoch: 5| Step: 3
Training loss: 1.9973514080047607
Validation loss: 2.382798405103786

Epoch: 5| Step: 4
Training loss: 2.189987897872925
Validation loss: 2.3923512915129304

Epoch: 5| Step: 5
Training loss: 3.276219606399536
Validation loss: 2.3908052367548787

Epoch: 5| Step: 6
Training loss: 2.8183770179748535
Validation loss: 2.37203493425923

Epoch: 5| Step: 7
Training loss: 2.396259069442749
Validation loss: 2.388604115414363

Epoch: 5| Step: 8
Training loss: 2.713172197341919
Validation loss: 2.38236298612369

Epoch: 5| Step: 9
Training loss: 3.2392916679382324
Validation loss: 2.395891340829993

Epoch: 5| Step: 10
Training loss: 2.727588415145874
Validation loss: 2.400226095671295

Epoch: 31| Step: 0
Training loss: 2.5133273601531982
Validation loss: 2.3881092968807427

Epoch: 5| Step: 1
Training loss: 3.0776095390319824
Validation loss: 2.3901944519371114

Epoch: 5| Step: 2
Training loss: 2.24052095413208
Validation loss: 2.38637299178749

Epoch: 5| Step: 3
Training loss: 2.4699134826660156
Validation loss: 2.381153239998766

Epoch: 5| Step: 4
Training loss: 2.1890807151794434
Validation loss: 2.389913346177788

Epoch: 5| Step: 5
Training loss: 2.2888808250427246
Validation loss: 2.383318083260649

Epoch: 5| Step: 6
Training loss: 2.32226824760437
Validation loss: 2.386843837717528

Epoch: 5| Step: 7
Training loss: 2.9898386001586914
Validation loss: 2.3614429197003766

Epoch: 5| Step: 8
Training loss: 2.666917324066162
Validation loss: 2.3950749648514615

Epoch: 5| Step: 9
Training loss: 2.9856271743774414
Validation loss: 2.3887095643628027

Epoch: 5| Step: 10
Training loss: 2.420011043548584
Validation loss: 2.3908372207354476

Epoch: 32| Step: 0
Training loss: 2.857306957244873
Validation loss: 2.395750966123355

Epoch: 5| Step: 1
Training loss: 2.5284109115600586
Validation loss: 2.3801075540563112

Epoch: 5| Step: 2
Training loss: 3.0320026874542236
Validation loss: 2.4034782763450377

Epoch: 5| Step: 3
Training loss: 1.9569275379180908
Validation loss: 2.384289182642455

Epoch: 5| Step: 4
Training loss: 2.652991771697998
Validation loss: 2.3959737631582443

Epoch: 5| Step: 5
Training loss: 2.458883762359619
Validation loss: 2.3665639405609458

Epoch: 5| Step: 6
Training loss: 2.801820993423462
Validation loss: 2.3971132924479823

Epoch: 5| Step: 7
Training loss: 2.619621753692627
Validation loss: 2.3904477934683523

Epoch: 5| Step: 8
Training loss: 1.9571346044540405
Validation loss: 2.3894065862060874

Epoch: 5| Step: 9
Training loss: 3.279245376586914
Validation loss: 2.3890345250406573

Epoch: 5| Step: 10
Training loss: 1.861129879951477
Validation loss: 2.3772943840231946

Epoch: 33| Step: 0
Training loss: 2.1706485748291016
Validation loss: 2.386127460387445

Epoch: 5| Step: 1
Training loss: 2.597365140914917
Validation loss: 2.379340289741434

Epoch: 5| Step: 2
Training loss: 2.5180904865264893
Validation loss: 2.3859331338636336

Epoch: 5| Step: 3
Training loss: 2.3344950675964355
Validation loss: 2.3616827277727026

Epoch: 5| Step: 4
Training loss: 3.0096163749694824
Validation loss: 2.3711574205788235

Epoch: 5| Step: 5
Training loss: 2.1486237049102783
Validation loss: 2.3860687107168217

Epoch: 5| Step: 6
Training loss: 3.019594669342041
Validation loss: 2.380105580053022

Epoch: 5| Step: 7
Training loss: 2.6091179847717285
Validation loss: 2.357435809668674

Epoch: 5| Step: 8
Training loss: 2.6408653259277344
Validation loss: 2.366159756978353

Epoch: 5| Step: 9
Training loss: 2.6179394721984863
Validation loss: 2.392180601755778

Epoch: 5| Step: 10
Training loss: 2.3417065143585205
Validation loss: 2.349915122473112

Epoch: 34| Step: 0
Training loss: 2.7667794227600098
Validation loss: 2.3721202355559154

Epoch: 5| Step: 1
Training loss: 2.63457989692688
Validation loss: 2.3590176054226455

Epoch: 5| Step: 2
Training loss: 2.7256882190704346
Validation loss: 2.350168658841041

Epoch: 5| Step: 3
Training loss: 3.1021742820739746
Validation loss: 2.3750263157711236

Epoch: 5| Step: 4
Training loss: 2.3497514724731445
Validation loss: 2.3436123145523893

Epoch: 5| Step: 5
Training loss: 2.890029191970825
Validation loss: 2.338057638496481

Epoch: 5| Step: 6
Training loss: 2.219719648361206
Validation loss: 2.3463266075298352

Epoch: 5| Step: 7
Training loss: 2.984707832336426
Validation loss: 2.3466928030854914

Epoch: 5| Step: 8
Training loss: 1.719700574874878
Validation loss: 2.3400217179329164

Epoch: 5| Step: 9
Training loss: 2.531768798828125
Validation loss: 2.3651267533661215

Epoch: 5| Step: 10
Training loss: 1.8557424545288086
Validation loss: 2.343686916494882

Epoch: 35| Step: 0
Training loss: 3.1003527641296387
Validation loss: 2.345935470314436

Epoch: 5| Step: 1
Training loss: 3.214656114578247
Validation loss: 2.3432506515133764

Epoch: 5| Step: 2
Training loss: 2.5691001415252686
Validation loss: 2.3395082796773603

Epoch: 5| Step: 3
Training loss: 2.2395899295806885
Validation loss: 2.350462189284704

Epoch: 5| Step: 4
Training loss: 2.3833377361297607
Validation loss: 2.345667121230915

Epoch: 5| Step: 5
Training loss: 2.668581962585449
Validation loss: 2.34739375370805

Epoch: 5| Step: 6
Training loss: 2.334437847137451
Validation loss: 2.3542662102689027

Epoch: 5| Step: 7
Training loss: 2.392047166824341
Validation loss: 2.3620970249176025

Epoch: 5| Step: 8
Training loss: 2.4911744594573975
Validation loss: 2.3576724477993545

Epoch: 5| Step: 9
Training loss: 1.963658094406128
Validation loss: 2.34422460679085

Epoch: 5| Step: 10
Training loss: 2.470672130584717
Validation loss: 2.3517245656700543

Epoch: 36| Step: 0
Training loss: 3.4145851135253906
Validation loss: 2.336660882478119

Epoch: 5| Step: 1
Training loss: 1.621792197227478
Validation loss: 2.3419712820360736

Epoch: 5| Step: 2
Training loss: 2.036407947540283
Validation loss: 2.359242998143678

Epoch: 5| Step: 3
Training loss: 1.90799081325531
Validation loss: 2.3396424888282694

Epoch: 5| Step: 4
Training loss: 2.167233943939209
Validation loss: 2.3380799293518066

Epoch: 5| Step: 5
Training loss: 2.7528462409973145
Validation loss: 2.342309941527664

Epoch: 5| Step: 6
Training loss: 2.8796231746673584
Validation loss: 2.339842806580246

Epoch: 5| Step: 7
Training loss: 2.9014830589294434
Validation loss: 2.3323839890059603

Epoch: 5| Step: 8
Training loss: 2.559217929840088
Validation loss: 2.333679993947347

Epoch: 5| Step: 9
Training loss: 3.263986110687256
Validation loss: 2.3248501567430395

Epoch: 5| Step: 10
Training loss: 2.0957746505737305
Validation loss: 2.3594233143714165

Epoch: 37| Step: 0
Training loss: 2.4003658294677734
Validation loss: 2.3372286263332573

Epoch: 5| Step: 1
Training loss: 3.1345412731170654
Validation loss: 2.3409020913544523

Epoch: 5| Step: 2
Training loss: 2.6230287551879883
Validation loss: 2.3281073800979124

Epoch: 5| Step: 3
Training loss: 3.2562687397003174
Validation loss: 2.3315433738052205

Epoch: 5| Step: 4
Training loss: 2.611736536026001
Validation loss: 2.3278574815360447

Epoch: 5| Step: 5
Training loss: 2.301743745803833
Validation loss: 2.3329360510713313

Epoch: 5| Step: 6
Training loss: 2.2845396995544434
Validation loss: 2.318990176723849

Epoch: 5| Step: 7
Training loss: 2.273904323577881
Validation loss: 2.336362679799398

Epoch: 5| Step: 8
Training loss: 1.9813801050186157
Validation loss: 2.3292603672191663

Epoch: 5| Step: 9
Training loss: 2.075550079345703
Validation loss: 2.358653101869809

Epoch: 5| Step: 10
Training loss: 2.6180412769317627
Validation loss: 2.3515556730249876

Epoch: 38| Step: 0
Training loss: 2.2427074909210205
Validation loss: 2.3396228590319232

Epoch: 5| Step: 1
Training loss: 2.363050937652588
Validation loss: 2.3079778866101335

Epoch: 5| Step: 2
Training loss: 1.6618404388427734
Validation loss: 2.3163386211600354

Epoch: 5| Step: 3
Training loss: 2.1074233055114746
Validation loss: 2.3496907680265364

Epoch: 5| Step: 4
Training loss: 3.091787815093994
Validation loss: 2.3423334167849634

Epoch: 5| Step: 5
Training loss: 3.019829750061035
Validation loss: 2.3099139633999077

Epoch: 5| Step: 6
Training loss: 2.3819477558135986
Validation loss: 2.3222551986735356

Epoch: 5| Step: 7
Training loss: 2.884807586669922
Validation loss: 2.3212348261187152

Epoch: 5| Step: 8
Training loss: 2.611297607421875
Validation loss: 2.320986235013572

Epoch: 5| Step: 9
Training loss: 2.5885376930236816
Validation loss: 2.3428049587434336

Epoch: 5| Step: 10
Training loss: 2.6829187870025635
Validation loss: 2.3223130549153974

Epoch: 39| Step: 0
Training loss: 2.170944929122925
Validation loss: 2.3456514099592805

Epoch: 5| Step: 1
Training loss: 2.450147867202759
Validation loss: 2.3282123304182485

Epoch: 5| Step: 2
Training loss: 2.1353466510772705
Validation loss: 2.3195853053882556

Epoch: 5| Step: 3
Training loss: 2.8242146968841553
Validation loss: 2.3324381151506977

Epoch: 5| Step: 4
Training loss: 3.114697217941284
Validation loss: 2.3406363507752777

Epoch: 5| Step: 5
Training loss: 2.2576701641082764
Validation loss: 2.33215933204979

Epoch: 5| Step: 6
Training loss: 2.249509811401367
Validation loss: 2.330230164271529

Epoch: 5| Step: 7
Training loss: 3.024974822998047
Validation loss: 2.343993058768652

Epoch: 5| Step: 8
Training loss: 3.143765449523926
Validation loss: 2.322108091846589

Epoch: 5| Step: 9
Training loss: 2.1806750297546387
Validation loss: 2.3296335397228116

Epoch: 5| Step: 10
Training loss: 1.9840013980865479
Validation loss: 2.356016630767494

Epoch: 40| Step: 0
Training loss: 2.568296194076538
Validation loss: 2.327967859083606

Epoch: 5| Step: 1
Training loss: 2.228034257888794
Validation loss: 2.3157681983004332

Epoch: 5| Step: 2
Training loss: 2.4102184772491455
Validation loss: 2.332478347645011

Epoch: 5| Step: 3
Training loss: 2.695453643798828
Validation loss: 2.341143136383385

Epoch: 5| Step: 4
Training loss: 2.2016968727111816
Validation loss: 2.3234438434723885

Epoch: 5| Step: 5
Training loss: 2.6407766342163086
Validation loss: 2.3232995079409693

Epoch: 5| Step: 6
Training loss: 2.72001314163208
Validation loss: 2.347054591742895

Epoch: 5| Step: 7
Training loss: 2.6378352642059326
Validation loss: 2.312147750649401

Epoch: 5| Step: 8
Training loss: 2.561769723892212
Validation loss: 2.336215701154483

Epoch: 5| Step: 9
Training loss: 2.3347771167755127
Validation loss: 2.3170564546379993

Epoch: 5| Step: 10
Training loss: 2.405402898788452
Validation loss: 2.335031622199602

Epoch: 41| Step: 0
Training loss: 2.984297275543213
Validation loss: 2.3239321580497165

Epoch: 5| Step: 1
Training loss: 2.665597438812256
Validation loss: 2.3232577257258917

Epoch: 5| Step: 2
Training loss: 2.835810422897339
Validation loss: 2.30381941282621

Epoch: 5| Step: 3
Training loss: 1.844104528427124
Validation loss: 2.3212212183142222

Epoch: 5| Step: 4
Training loss: 2.704101800918579
Validation loss: 2.353270107699979

Epoch: 5| Step: 5
Training loss: 2.386605739593506
Validation loss: 2.3023512312161025

Epoch: 5| Step: 6
Training loss: 2.01760196685791
Validation loss: 2.3165412474704046

Epoch: 5| Step: 7
Training loss: 2.554523468017578
Validation loss: 2.320863800664102

Epoch: 5| Step: 8
Training loss: 2.616982936859131
Validation loss: 2.3325699221703315

Epoch: 5| Step: 9
Training loss: 2.7612836360931396
Validation loss: 2.3106193004115934

Epoch: 5| Step: 10
Training loss: 1.9840126037597656
Validation loss: 2.3032492668397966

Epoch: 42| Step: 0
Training loss: 2.9774296283721924
Validation loss: 2.2854667120082404

Epoch: 5| Step: 1
Training loss: 2.7504019737243652
Validation loss: 2.303046744356873

Epoch: 5| Step: 2
Training loss: 1.5790565013885498
Validation loss: 2.299809335380472

Epoch: 5| Step: 3
Training loss: 2.2634189128875732
Validation loss: 2.3190479611837738

Epoch: 5| Step: 4
Training loss: 3.0845913887023926
Validation loss: 2.3066643796941286

Epoch: 5| Step: 5
Training loss: 2.9418044090270996
Validation loss: 2.2777481848193752

Epoch: 5| Step: 6
Training loss: 1.6046741008758545
Validation loss: 2.2976061977365965

Epoch: 5| Step: 7
Training loss: 2.540457248687744
Validation loss: 2.319445780528489

Epoch: 5| Step: 8
Training loss: 2.5212981700897217
Validation loss: 2.2993502181063414

Epoch: 5| Step: 9
Training loss: 2.34157133102417
Validation loss: 2.3001970680811072

Epoch: 5| Step: 10
Training loss: 2.6169955730438232
Validation loss: 2.298330327515961

Epoch: 43| Step: 0
Training loss: 2.080709934234619
Validation loss: 2.284021205799554

Epoch: 5| Step: 1
Training loss: 2.8353466987609863
Validation loss: 2.313757324731478

Epoch: 5| Step: 2
Training loss: 1.996339201927185
Validation loss: 2.2853908320908904

Epoch: 5| Step: 3
Training loss: 2.6904819011688232
Validation loss: 2.313306817444422

Epoch: 5| Step: 4
Training loss: 1.7085011005401611
Validation loss: 2.3126569255705802

Epoch: 5| Step: 5
Training loss: 2.3139145374298096
Validation loss: 2.323451908685828

Epoch: 5| Step: 6
Training loss: 2.668893337249756
Validation loss: 2.29868806305752

Epoch: 5| Step: 7
Training loss: 2.3503270149230957
Validation loss: 2.329434117963237

Epoch: 5| Step: 8
Training loss: 2.3176281452178955
Validation loss: 2.311118807843936

Epoch: 5| Step: 9
Training loss: 3.6636009216308594
Validation loss: 2.31453735597672

Epoch: 5| Step: 10
Training loss: 2.735741138458252
Validation loss: 2.317226404784828

Epoch: 44| Step: 0
Training loss: 2.5768558979034424
Validation loss: 2.2710000648293445

Epoch: 5| Step: 1
Training loss: 1.969199538230896
Validation loss: 2.3041526963633876

Epoch: 5| Step: 2
Training loss: 2.318213939666748
Validation loss: 2.2958051286717898

Epoch: 5| Step: 3
Training loss: 2.822173595428467
Validation loss: 2.2939041840132846

Epoch: 5| Step: 4
Training loss: 1.925861120223999
Validation loss: 2.300562914981637

Epoch: 5| Step: 5
Training loss: 2.581063747406006
Validation loss: 2.3094024453111874

Epoch: 5| Step: 6
Training loss: 2.4696431159973145
Validation loss: 2.2843520846418155

Epoch: 5| Step: 7
Training loss: 3.0670132637023926
Validation loss: 2.2934309820975027

Epoch: 5| Step: 8
Training loss: 2.681476354598999
Validation loss: 2.2817698524844263

Epoch: 5| Step: 9
Training loss: 2.565004825592041
Validation loss: 2.301835985593898

Epoch: 5| Step: 10
Training loss: 2.205066442489624
Validation loss: 2.2562933967959498

Epoch: 45| Step: 0
Training loss: 2.129624843597412
Validation loss: 2.2933065788720244

Epoch: 5| Step: 1
Training loss: 2.4004440307617188
Validation loss: 2.280844896070419

Epoch: 5| Step: 2
Training loss: 2.808314085006714
Validation loss: 2.298240553948187

Epoch: 5| Step: 3
Training loss: 1.9696067571640015
Validation loss: 2.3102236768250823

Epoch: 5| Step: 4
Training loss: 2.6980276107788086
Validation loss: 2.2884719012885966

Epoch: 5| Step: 5
Training loss: 3.5784752368927
Validation loss: 2.299610701940393

Epoch: 5| Step: 6
Training loss: 2.5353071689605713
Validation loss: 2.293376076606012

Epoch: 5| Step: 7
Training loss: 2.3354318141937256
Validation loss: 2.295613024824409

Epoch: 5| Step: 8
Training loss: 1.7303603887557983
Validation loss: 2.306572832087035

Epoch: 5| Step: 9
Training loss: 2.9785046577453613
Validation loss: 2.3010359169334493

Epoch: 5| Step: 10
Training loss: 1.900213599205017
Validation loss: 2.304472206741251

Epoch: 46| Step: 0
Training loss: 2.8943872451782227
Validation loss: 2.2790782759266515

Epoch: 5| Step: 1
Training loss: 2.8954110145568848
Validation loss: 2.30038353448273

Epoch: 5| Step: 2
Training loss: 2.1381278038024902
Validation loss: 2.2990455806896253

Epoch: 5| Step: 3
Training loss: 2.2498416900634766
Validation loss: 2.2833340193635676

Epoch: 5| Step: 4
Training loss: 2.169703960418701
Validation loss: 2.2799472911383516

Epoch: 5| Step: 5
Training loss: 2.2185308933258057
Validation loss: 2.2809220257625786

Epoch: 5| Step: 6
Training loss: 2.210397720336914
Validation loss: 2.297012165028562

Epoch: 5| Step: 7
Training loss: 2.9615345001220703
Validation loss: 2.316488609519056

Epoch: 5| Step: 8
Training loss: 2.614511013031006
Validation loss: 2.3167181373924337

Epoch: 5| Step: 9
Training loss: 2.3302478790283203
Validation loss: 2.301037676872746

Epoch: 5| Step: 10
Training loss: 2.3849170207977295
Validation loss: 2.2895209597003077

Epoch: 47| Step: 0
Training loss: 2.2234270572662354
Validation loss: 2.283456274258193

Epoch: 5| Step: 1
Training loss: 2.9960761070251465
Validation loss: 2.28439361305647

Epoch: 5| Step: 2
Training loss: 2.874187707901001
Validation loss: 2.283570797212662

Epoch: 5| Step: 3
Training loss: 1.849552869796753
Validation loss: 2.302157509711481

Epoch: 5| Step: 4
Training loss: 2.6777796745300293
Validation loss: 2.2768228387319915

Epoch: 5| Step: 5
Training loss: 2.2552876472473145
Validation loss: 2.2842553892443256

Epoch: 5| Step: 6
Training loss: 2.1959903240203857
Validation loss: 2.2837643879716114

Epoch: 5| Step: 7
Training loss: 2.7958624362945557
Validation loss: 2.3117471766728226

Epoch: 5| Step: 8
Training loss: 2.922010898590088
Validation loss: 2.2758750966800156

Epoch: 5| Step: 9
Training loss: 1.7106964588165283
Validation loss: 2.2672556164444133

Epoch: 5| Step: 10
Training loss: 2.5834853649139404
Validation loss: 2.2866837645089753

Epoch: 48| Step: 0
Training loss: 2.65483021736145
Validation loss: 2.2974381164837907

Epoch: 5| Step: 1
Training loss: 3.487879514694214
Validation loss: 2.2957715270339802

Epoch: 5| Step: 2
Training loss: 2.712465763092041
Validation loss: 2.2803359416223343

Epoch: 5| Step: 3
Training loss: 1.9289734363555908
Validation loss: 2.2753819470764487

Epoch: 5| Step: 4
Training loss: 2.265921115875244
Validation loss: 2.3073237198655323

Epoch: 5| Step: 5
Training loss: 2.0366108417510986
Validation loss: 2.2775027252012685

Epoch: 5| Step: 6
Training loss: 2.231227159500122
Validation loss: 2.2963599030689528

Epoch: 5| Step: 7
Training loss: 2.455880641937256
Validation loss: 2.2633168402538506

Epoch: 5| Step: 8
Training loss: 2.4484987258911133
Validation loss: 2.2756626913624425

Epoch: 5| Step: 9
Training loss: 2.0986528396606445
Validation loss: 2.2782362968690935

Epoch: 5| Step: 10
Training loss: 2.7040834426879883
Validation loss: 2.2808799628288514

Epoch: 49| Step: 0
Training loss: 2.3119537830352783
Validation loss: 2.274455384541583

Epoch: 5| Step: 1
Training loss: 3.130422592163086
Validation loss: 2.285207866340555

Epoch: 5| Step: 2
Training loss: 1.788744330406189
Validation loss: 2.28993635536522

Epoch: 5| Step: 3
Training loss: 2.622175693511963
Validation loss: 2.2737812829274002

Epoch: 5| Step: 4
Training loss: 1.775259256362915
Validation loss: 2.288949861321398

Epoch: 5| Step: 5
Training loss: 3.192430019378662
Validation loss: 2.271363014815956

Epoch: 5| Step: 6
Training loss: 2.176851272583008
Validation loss: 2.2868125182326122

Epoch: 5| Step: 7
Training loss: 2.882256031036377
Validation loss: 2.2648133359929568

Epoch: 5| Step: 8
Training loss: 2.3134586811065674
Validation loss: 2.2723603953597364

Epoch: 5| Step: 9
Training loss: 2.7929015159606934
Validation loss: 2.2982602939810803

Epoch: 5| Step: 10
Training loss: 1.9521865844726562
Validation loss: 2.2652573059963923

Epoch: 50| Step: 0
Training loss: 2.470081329345703
Validation loss: 2.263693473672354

Epoch: 5| Step: 1
Training loss: 2.4167323112487793
Validation loss: 2.2670391836474018

Epoch: 5| Step: 2
Training loss: 2.0516788959503174
Validation loss: 2.263077643609816

Epoch: 5| Step: 3
Training loss: 1.9956287145614624
Validation loss: 2.289274015734273

Epoch: 5| Step: 4
Training loss: 2.4334182739257812
Validation loss: 2.2655793774512505

Epoch: 5| Step: 5
Training loss: 2.459929943084717
Validation loss: 2.2837405076590915

Epoch: 5| Step: 6
Training loss: 3.072765350341797
Validation loss: 2.2811821327414563

Epoch: 5| Step: 7
Training loss: 2.32869291305542
Validation loss: 2.2619584978267713

Epoch: 5| Step: 8
Training loss: 2.798253297805786
Validation loss: 2.269920156848046

Epoch: 5| Step: 9
Training loss: 2.1887762546539307
Validation loss: 2.2782497739279144

Epoch: 5| Step: 10
Training loss: 2.6798744201660156
Validation loss: 2.266781650563722

Epoch: 51| Step: 0
Training loss: 2.862708568572998
Validation loss: 2.2833904886758454

Epoch: 5| Step: 1
Training loss: 2.3370230197906494
Validation loss: 2.269531311527375

Epoch: 5| Step: 2
Training loss: 2.6571412086486816
Validation loss: 2.2708825372880503

Epoch: 5| Step: 3
Training loss: 2.43174409866333
Validation loss: 2.2518185389939176

Epoch: 5| Step: 4
Training loss: 3.1879866123199463
Validation loss: 2.268984067824579

Epoch: 5| Step: 5
Training loss: 1.831709623336792
Validation loss: 2.24566787545399

Epoch: 5| Step: 6
Training loss: 1.9002788066864014
Validation loss: 2.2818020851381364

Epoch: 5| Step: 7
Training loss: 2.1890690326690674
Validation loss: 2.270073616376487

Epoch: 5| Step: 8
Training loss: 2.3668735027313232
Validation loss: 2.265011597705144

Epoch: 5| Step: 9
Training loss: 2.307570219039917
Validation loss: 2.248888343893072

Epoch: 5| Step: 10
Training loss: 2.5648436546325684
Validation loss: 2.292337415038898

Epoch: 52| Step: 0
Training loss: 2.9330368041992188
Validation loss: 2.2364575311701786

Epoch: 5| Step: 1
Training loss: 2.2059085369110107
Validation loss: 2.250253479967835

Epoch: 5| Step: 2
Training loss: 2.2973198890686035
Validation loss: 2.24670087393894

Epoch: 5| Step: 3
Training loss: 2.2878305912017822
Validation loss: 2.2643971571358303

Epoch: 5| Step: 4
Training loss: 2.135289192199707
Validation loss: 2.2527395807286745

Epoch: 5| Step: 5
Training loss: 2.221710681915283
Validation loss: 2.2654446273721676

Epoch: 5| Step: 6
Training loss: 2.0671539306640625
Validation loss: 2.27737703118273

Epoch: 5| Step: 7
Training loss: 1.9621334075927734
Validation loss: 2.238268195941884

Epoch: 5| Step: 8
Training loss: 2.70133376121521
Validation loss: 2.275699564205703

Epoch: 5| Step: 9
Training loss: 3.185084581375122
Validation loss: 2.2723312570202734

Epoch: 5| Step: 10
Training loss: 2.948606014251709
Validation loss: 2.250790339644237

Epoch: 53| Step: 0
Training loss: 2.3693528175354004
Validation loss: 2.2539294868387203

Epoch: 5| Step: 1
Training loss: 2.7518677711486816
Validation loss: 2.2541920036397953

Epoch: 5| Step: 2
Training loss: 2.273151397705078
Validation loss: 2.2591884431018623

Epoch: 5| Step: 3
Training loss: 2.314657211303711
Validation loss: 2.249453242107104

Epoch: 5| Step: 4
Training loss: 2.4177801609039307
Validation loss: 2.2384914377684235

Epoch: 5| Step: 5
Training loss: 2.643932819366455
Validation loss: 2.2536835926835255

Epoch: 5| Step: 6
Training loss: 2.5580389499664307
Validation loss: 2.2409170417375464

Epoch: 5| Step: 7
Training loss: 2.4429519176483154
Validation loss: 2.2378032566398702

Epoch: 5| Step: 8
Training loss: 2.456404209136963
Validation loss: 2.2859633532903527

Epoch: 5| Step: 9
Training loss: 1.9738245010375977
Validation loss: 2.3015545542522142

Epoch: 5| Step: 10
Training loss: 2.3638343811035156
Validation loss: 2.2674939606779363

Epoch: 54| Step: 0
Training loss: 2.9961419105529785
Validation loss: 2.249251086224792

Epoch: 5| Step: 1
Training loss: 2.4436047077178955
Validation loss: 2.2602132725459274

Epoch: 5| Step: 2
Training loss: 2.3706486225128174
Validation loss: 2.240918110775691

Epoch: 5| Step: 3
Training loss: 2.9788753986358643
Validation loss: 2.2689321041107178

Epoch: 5| Step: 4
Training loss: 1.7042274475097656
Validation loss: 2.2676314538524998

Epoch: 5| Step: 5
Training loss: 2.4230430126190186
Validation loss: 2.259990179410545

Epoch: 5| Step: 6
Training loss: 2.029116153717041
Validation loss: 2.24250671299555

Epoch: 5| Step: 7
Training loss: 2.452134847640991
Validation loss: 2.2563583825224187

Epoch: 5| Step: 8
Training loss: 2.680614948272705
Validation loss: 2.2609668418925297

Epoch: 5| Step: 9
Training loss: 2.176837682723999
Validation loss: 2.243645134792533

Epoch: 5| Step: 10
Training loss: 2.3885371685028076
Validation loss: 2.256779270787393

Epoch: 55| Step: 0
Training loss: 3.062084674835205
Validation loss: 2.260634106974448

Epoch: 5| Step: 1
Training loss: 2.011314868927002
Validation loss: 2.2500869509994343

Epoch: 5| Step: 2
Training loss: 2.9223949909210205
Validation loss: 2.2687544463783182

Epoch: 5| Step: 3
Training loss: 2.4125492572784424
Validation loss: 2.2604521371984996

Epoch: 5| Step: 4
Training loss: 2.0230276584625244
Validation loss: 2.2479703785270773

Epoch: 5| Step: 5
Training loss: 1.6134370565414429
Validation loss: 2.268211410891625

Epoch: 5| Step: 6
Training loss: 2.1807408332824707
Validation loss: 2.2421284798652894

Epoch: 5| Step: 7
Training loss: 2.036055326461792
Validation loss: 2.2411028108289166

Epoch: 5| Step: 8
Training loss: 2.480250835418701
Validation loss: 2.25462116349128

Epoch: 5| Step: 9
Training loss: 2.8717827796936035
Validation loss: 2.2560314363048923

Epoch: 5| Step: 10
Training loss: 2.750652551651001
Validation loss: 2.2536902607128186

Epoch: 56| Step: 0
Training loss: 2.257425308227539
Validation loss: 2.2631637024623092

Epoch: 5| Step: 1
Training loss: 2.528491258621216
Validation loss: 2.2541389119240547

Epoch: 5| Step: 2
Training loss: 2.0306522846221924
Validation loss: 2.258877282501549

Epoch: 5| Step: 3
Training loss: 2.4008593559265137
Validation loss: 2.221355809960314

Epoch: 5| Step: 4
Training loss: 2.5164952278137207
Validation loss: 2.2601102705924743

Epoch: 5| Step: 5
Training loss: 2.558467388153076
Validation loss: 2.244320506690651

Epoch: 5| Step: 6
Training loss: 2.451862335205078
Validation loss: 2.2588601394366195

Epoch: 5| Step: 7
Training loss: 2.8973212242126465
Validation loss: 2.2659438528040403

Epoch: 5| Step: 8
Training loss: 2.5256237983703613
Validation loss: 2.237734294706775

Epoch: 5| Step: 9
Training loss: 1.832961082458496
Validation loss: 2.2614879031335153

Epoch: 5| Step: 10
Training loss: 2.5373806953430176
Validation loss: 2.233138204902731

Epoch: 57| Step: 0
Training loss: 2.1766717433929443
Validation loss: 2.2498068642872635

Epoch: 5| Step: 1
Training loss: 1.9968122243881226
Validation loss: 2.2448712907811648

Epoch: 5| Step: 2
Training loss: 1.8496513366699219
Validation loss: 2.2265870545500066

Epoch: 5| Step: 3
Training loss: 2.0035736560821533
Validation loss: 2.254333714003204

Epoch: 5| Step: 4
Training loss: 3.208500385284424
Validation loss: 2.2466700512875795

Epoch: 5| Step: 5
Training loss: 2.0355892181396484
Validation loss: 2.2201204838291293

Epoch: 5| Step: 6
Training loss: 2.965759754180908
Validation loss: 2.258489590819164

Epoch: 5| Step: 7
Training loss: 2.4282679557800293
Validation loss: 2.2402435861608034

Epoch: 5| Step: 8
Training loss: 2.2944724559783936
Validation loss: 2.25639921106318

Epoch: 5| Step: 9
Training loss: 2.921910524368286
Validation loss: 2.234730592337988

Epoch: 5| Step: 10
Training loss: 2.366903781890869
Validation loss: 2.245758471950408

Epoch: 58| Step: 0
Training loss: 2.336857318878174
Validation loss: 2.239938694943664

Epoch: 5| Step: 1
Training loss: 2.095414638519287
Validation loss: 2.2711369247846704

Epoch: 5| Step: 2
Training loss: 2.0953450202941895
Validation loss: 2.2364918493455455

Epoch: 5| Step: 3
Training loss: 2.89359188079834
Validation loss: 2.253287846042264

Epoch: 5| Step: 4
Training loss: 2.3751749992370605
Validation loss: 2.256877919679047

Epoch: 5| Step: 5
Training loss: 2.232570171356201
Validation loss: 2.2334373830467142

Epoch: 5| Step: 6
Training loss: 2.2496211528778076
Validation loss: 2.2585666230929795

Epoch: 5| Step: 7
Training loss: 2.035409450531006
Validation loss: 2.2482698348260697

Epoch: 5| Step: 8
Training loss: 3.25838041305542
Validation loss: 2.245775124078156

Epoch: 5| Step: 9
Training loss: 2.4624242782592773
Validation loss: 2.225685993830363

Epoch: 5| Step: 10
Training loss: 2.2891438007354736
Validation loss: 2.252366545379803

Epoch: 59| Step: 0
Training loss: 2.913872241973877
Validation loss: 2.259636850767238

Epoch: 5| Step: 1
Training loss: 2.581925630569458
Validation loss: 2.2615343345108854

Epoch: 5| Step: 2
Training loss: 2.783674716949463
Validation loss: 2.2344021002451577

Epoch: 5| Step: 3
Training loss: 2.138455867767334
Validation loss: 2.281378992142216

Epoch: 5| Step: 4
Training loss: 2.046955108642578
Validation loss: 2.2244638473756853

Epoch: 5| Step: 5
Training loss: 2.2296993732452393
Validation loss: 2.2393589506867113

Epoch: 5| Step: 6
Training loss: 2.481489658355713
Validation loss: 2.234757190109581

Epoch: 5| Step: 7
Training loss: 2.0156407356262207
Validation loss: 2.2481603417345273

Epoch: 5| Step: 8
Training loss: 2.34132719039917
Validation loss: 2.23578502029501

Epoch: 5| Step: 9
Training loss: 2.3025872707366943
Validation loss: 2.2035951819471133

Epoch: 5| Step: 10
Training loss: 2.6644396781921387
Validation loss: 2.2362222543326755

Epoch: 60| Step: 0
Training loss: 2.8762152194976807
Validation loss: 2.253482054638606

Epoch: 5| Step: 1
Training loss: 2.2583823204040527
Validation loss: 2.2338713881789998

Epoch: 5| Step: 2
Training loss: 2.0844368934631348
Validation loss: 2.2260359948681248

Epoch: 5| Step: 3
Training loss: 2.857367753982544
Validation loss: 2.253526600458289

Epoch: 5| Step: 4
Training loss: 2.404381513595581
Validation loss: 2.2235957422564105

Epoch: 5| Step: 5
Training loss: 1.84779953956604
Validation loss: 2.2725749579809045

Epoch: 5| Step: 6
Training loss: 2.2369213104248047
Validation loss: 2.219442106062366

Epoch: 5| Step: 7
Training loss: 1.962243676185608
Validation loss: 2.228830293942523

Epoch: 5| Step: 8
Training loss: 2.483414649963379
Validation loss: 2.2301608529142154

Epoch: 5| Step: 9
Training loss: 2.9633781909942627
Validation loss: 2.2355357921251686

Epoch: 5| Step: 10
Training loss: 2.0643224716186523
Validation loss: 2.237166376524074

Epoch: 61| Step: 0
Training loss: 2.1525113582611084
Validation loss: 2.194856870558954

Epoch: 5| Step: 1
Training loss: 2.1085972785949707
Validation loss: 2.23013396416941

Epoch: 5| Step: 2
Training loss: 1.9169628620147705
Validation loss: 2.252854762538787

Epoch: 5| Step: 3
Training loss: 2.778193712234497
Validation loss: 2.230873138673844

Epoch: 5| Step: 4
Training loss: 2.378124237060547
Validation loss: 2.2184626645939325

Epoch: 5| Step: 5
Training loss: 3.1878161430358887
Validation loss: 2.217434372953189

Epoch: 5| Step: 6
Training loss: 2.5769495964050293
Validation loss: 2.218969750147994

Epoch: 5| Step: 7
Training loss: 2.245936870574951
Validation loss: 2.2704616413321546

Epoch: 5| Step: 8
Training loss: 2.427473545074463
Validation loss: 2.224208547222999

Epoch: 5| Step: 9
Training loss: 2.29075288772583
Validation loss: 2.2144606754344

Epoch: 5| Step: 10
Training loss: 2.024740695953369
Validation loss: 2.249821324502268

Epoch: 62| Step: 0
Training loss: 2.723115921020508
Validation loss: 2.210552930831909

Epoch: 5| Step: 1
Training loss: 2.79852557182312
Validation loss: 2.2328288785872923

Epoch: 5| Step: 2
Training loss: 2.339773654937744
Validation loss: 2.2400240949405137

Epoch: 5| Step: 3
Training loss: 2.5086512565612793
Validation loss: 2.2357678182663454

Epoch: 5| Step: 4
Training loss: 2.0064330101013184
Validation loss: 2.2155889823872554

Epoch: 5| Step: 5
Training loss: 2.8245139122009277
Validation loss: 2.230606408529384

Epoch: 5| Step: 6
Training loss: 2.5476012229919434
Validation loss: 2.2303165851100797

Epoch: 5| Step: 7
Training loss: 2.3160672187805176
Validation loss: 2.2618400563475904

Epoch: 5| Step: 8
Training loss: 1.913112998008728
Validation loss: 2.2560462823478122

Epoch: 5| Step: 9
Training loss: 2.178539752960205
Validation loss: 2.2432014762714343

Epoch: 5| Step: 10
Training loss: 1.7544647455215454
Validation loss: 2.2136350549677366

Epoch: 63| Step: 0
Training loss: 1.8766613006591797
Validation loss: 2.2130001514188704

Epoch: 5| Step: 1
Training loss: 2.4064342975616455
Validation loss: 2.225334646881268

Epoch: 5| Step: 2
Training loss: 1.616633415222168
Validation loss: 2.248112278599893

Epoch: 5| Step: 3
Training loss: 2.0168309211730957
Validation loss: 2.2334046799649476

Epoch: 5| Step: 4
Training loss: 2.7385706901550293
Validation loss: 2.2422758212653537

Epoch: 5| Step: 5
Training loss: 2.6440529823303223
Validation loss: 2.228908996428213

Epoch: 5| Step: 6
Training loss: 2.2236075401306152
Validation loss: 2.239540955071808

Epoch: 5| Step: 7
Training loss: 2.2259418964385986
Validation loss: 2.2305455515461583

Epoch: 5| Step: 8
Training loss: 2.208214282989502
Validation loss: 2.2377634343280586

Epoch: 5| Step: 9
Training loss: 3.4444007873535156
Validation loss: 2.2608933038609003

Epoch: 5| Step: 10
Training loss: 2.609769105911255
Validation loss: 2.2381432107699815

Epoch: 64| Step: 0
Training loss: 2.238095760345459
Validation loss: 2.2066001443452734

Epoch: 5| Step: 1
Training loss: 1.9908020496368408
Validation loss: 2.205437949908677

Epoch: 5| Step: 2
Training loss: 2.2849135398864746
Validation loss: 2.221094767252604

Epoch: 5| Step: 3
Training loss: 2.233208417892456
Validation loss: 2.233503031474288

Epoch: 5| Step: 4
Training loss: 2.8147876262664795
Validation loss: 2.22407861422467

Epoch: 5| Step: 5
Training loss: 1.7716522216796875
Validation loss: 2.235471956191524

Epoch: 5| Step: 6
Training loss: 2.7384891510009766
Validation loss: 2.201178963466357

Epoch: 5| Step: 7
Training loss: 2.679339647293091
Validation loss: 2.20377435991841

Epoch: 5| Step: 8
Training loss: 2.043668031692505
Validation loss: 2.2330174651197208

Epoch: 5| Step: 9
Training loss: 3.1948916912078857
Validation loss: 2.2546842072599675

Epoch: 5| Step: 10
Training loss: 1.97482168674469
Validation loss: 2.1997255766263573

Epoch: 65| Step: 0
Training loss: 2.7611536979675293
Validation loss: 2.207861413237869

Epoch: 5| Step: 1
Training loss: 1.9904978275299072
Validation loss: 2.219554906250328

Epoch: 5| Step: 2
Training loss: 2.3200795650482178
Validation loss: 2.2292808589114936

Epoch: 5| Step: 3
Training loss: 2.779585599899292
Validation loss: 2.207776798996874

Epoch: 5| Step: 4
Training loss: 2.272991895675659
Validation loss: 2.2210309300371396

Epoch: 5| Step: 5
Training loss: 2.526885509490967
Validation loss: 2.2351667637466104

Epoch: 5| Step: 6
Training loss: 2.510256290435791
Validation loss: 2.222747205406107

Epoch: 5| Step: 7
Training loss: 2.1997058391571045
Validation loss: 2.2228484692112094

Epoch: 5| Step: 8
Training loss: 2.6669273376464844
Validation loss: 2.1927451702856247

Epoch: 5| Step: 9
Training loss: 2.2504754066467285
Validation loss: 2.2121150057802916

Epoch: 5| Step: 10
Training loss: 1.4483668804168701
Validation loss: 2.2081028979311705

Epoch: 66| Step: 0
Training loss: 2.091508388519287
Validation loss: 2.2248882221919235

Epoch: 5| Step: 1
Training loss: 2.1332714557647705
Validation loss: 2.225530849990024

Epoch: 5| Step: 2
Training loss: 2.2804765701293945
Validation loss: 2.2263168263179

Epoch: 5| Step: 3
Training loss: 3.1053338050842285
Validation loss: 2.192373546220923

Epoch: 5| Step: 4
Training loss: 2.357454538345337
Validation loss: 2.2067670386324645

Epoch: 5| Step: 5
Training loss: 2.202106237411499
Validation loss: 2.208169332114599

Epoch: 5| Step: 6
Training loss: 2.3226513862609863
Validation loss: 2.2228018699153775

Epoch: 5| Step: 7
Training loss: 2.422661066055298
Validation loss: 2.233600929219236

Epoch: 5| Step: 8
Training loss: 2.34411883354187
Validation loss: 2.198333345433717

Epoch: 5| Step: 9
Training loss: 2.5267679691314697
Validation loss: 2.2233300157772597

Epoch: 5| Step: 10
Training loss: 1.7069003582000732
Validation loss: 2.208617136042605

Epoch: 67| Step: 0
Training loss: 2.9202792644500732
Validation loss: 2.213375445335142

Epoch: 5| Step: 1
Training loss: 2.9090416431427
Validation loss: 2.2350088627107683

Epoch: 5| Step: 2
Training loss: 1.618300199508667
Validation loss: 2.245637592448983

Epoch: 5| Step: 3
Training loss: 2.4003734588623047
Validation loss: 2.208652368155859

Epoch: 5| Step: 4
Training loss: 2.495391368865967
Validation loss: 2.2321601324183966

Epoch: 5| Step: 5
Training loss: 2.8853650093078613
Validation loss: 2.2205827543812413

Epoch: 5| Step: 6
Training loss: 1.8882672786712646
Validation loss: 2.229552076708886

Epoch: 5| Step: 7
Training loss: 2.4446094036102295
Validation loss: 2.2241828492892686

Epoch: 5| Step: 8
Training loss: 2.1790881156921387
Validation loss: 2.2210074419616372

Epoch: 5| Step: 9
Training loss: 1.7981417179107666
Validation loss: 2.215344246997628

Epoch: 5| Step: 10
Training loss: 2.289034843444824
Validation loss: 2.206568310337682

Epoch: 68| Step: 0
Training loss: 2.079988956451416
Validation loss: 2.2119442916685537

Epoch: 5| Step: 1
Training loss: 2.5151853561401367
Validation loss: 2.2358661390119985

Epoch: 5| Step: 2
Training loss: 2.037259578704834
Validation loss: 2.228953984475905

Epoch: 5| Step: 3
Training loss: 2.7554547786712646
Validation loss: 2.23271139847335

Epoch: 5| Step: 4
Training loss: 2.6229376792907715
Validation loss: 2.2350164433961273

Epoch: 5| Step: 5
Training loss: 2.3972713947296143
Validation loss: 2.1960688714058167

Epoch: 5| Step: 6
Training loss: 2.82002592086792
Validation loss: 2.190655913404239

Epoch: 5| Step: 7
Training loss: 2.1672441959381104
Validation loss: 2.182761425613075

Epoch: 5| Step: 8
Training loss: 1.5476385354995728
Validation loss: 2.2015876731564923

Epoch: 5| Step: 9
Training loss: 3.032477855682373
Validation loss: 2.1910755608671453

Epoch: 5| Step: 10
Training loss: 1.6494864225387573
Validation loss: 2.2048855366245395

Epoch: 69| Step: 0
Training loss: 2.4496374130249023
Validation loss: 2.224454087595786

Epoch: 5| Step: 1
Training loss: 2.6363062858581543
Validation loss: 2.205915889432353

Epoch: 5| Step: 2
Training loss: 2.6636760234832764
Validation loss: 2.1880303352109847

Epoch: 5| Step: 3
Training loss: 2.194973945617676
Validation loss: 2.2251461910945114

Epoch: 5| Step: 4
Training loss: 1.5814521312713623
Validation loss: 2.188532496011385

Epoch: 5| Step: 5
Training loss: 2.6497890949249268
Validation loss: 2.2225690554547053

Epoch: 5| Step: 6
Training loss: 2.036372661590576
Validation loss: 2.1903359261892175

Epoch: 5| Step: 7
Training loss: 1.9653241634368896
Validation loss: 2.201751160365279

Epoch: 5| Step: 8
Training loss: 2.009936809539795
Validation loss: 2.184041110418176

Epoch: 5| Step: 9
Training loss: 2.4105024337768555
Validation loss: 2.19477411495742

Epoch: 5| Step: 10
Training loss: 3.1342785358428955
Validation loss: 2.239822613295688

Epoch: 70| Step: 0
Training loss: 1.7427383661270142
Validation loss: 2.205384382637598

Epoch: 5| Step: 1
Training loss: 2.459033966064453
Validation loss: 2.185229629598638

Epoch: 5| Step: 2
Training loss: 2.5348305702209473
Validation loss: 2.220244792199904

Epoch: 5| Step: 3
Training loss: 2.627763271331787
Validation loss: 2.2091087705345562

Epoch: 5| Step: 4
Training loss: 2.1526167392730713
Validation loss: 2.1898166223238875

Epoch: 5| Step: 5
Training loss: 2.389826536178589
Validation loss: 2.2000710579656784

Epoch: 5| Step: 6
Training loss: 2.3725523948669434
Validation loss: 2.1848985405378443

Epoch: 5| Step: 7
Training loss: 2.453773260116577
Validation loss: 2.228690092281629

Epoch: 5| Step: 8
Training loss: 1.9645068645477295
Validation loss: 2.1883821410517537

Epoch: 5| Step: 9
Training loss: 2.754485607147217
Validation loss: 2.21283112290085

Epoch: 5| Step: 10
Training loss: 2.2088451385498047
Validation loss: 2.1825498855242165

Epoch: 71| Step: 0
Training loss: 2.359179973602295
Validation loss: 2.2295883509420578

Epoch: 5| Step: 1
Training loss: 2.2041079998016357
Validation loss: 2.200384291269446

Epoch: 5| Step: 2
Training loss: 2.2621517181396484
Validation loss: 2.2173505188316427

Epoch: 5| Step: 3
Training loss: 2.0519397258758545
Validation loss: 2.224940243587699

Epoch: 5| Step: 4
Training loss: 2.2395408153533936
Validation loss: 2.173654002528037

Epoch: 5| Step: 5
Training loss: 2.4007909297943115
Validation loss: 2.2015081233875726

Epoch: 5| Step: 6
Training loss: 2.3640124797821045
Validation loss: 2.2025660199503743

Epoch: 5| Step: 7
Training loss: 3.023942470550537
Validation loss: 2.207275304743039

Epoch: 5| Step: 8
Training loss: 2.2302236557006836
Validation loss: 2.17114633001307

Epoch: 5| Step: 9
Training loss: 2.007688045501709
Validation loss: 2.1864256025642477

Epoch: 5| Step: 10
Training loss: 2.4152071475982666
Validation loss: 2.1748130706048783

Epoch: 72| Step: 0
Training loss: 2.694457769393921
Validation loss: 2.19879513402139

Epoch: 5| Step: 1
Training loss: 2.2207729816436768
Validation loss: 2.195672289017708

Epoch: 5| Step: 2
Training loss: 2.3776354789733887
Validation loss: 2.1814119815826416

Epoch: 5| Step: 3
Training loss: 1.9503675699234009
Validation loss: 2.2139124665209042

Epoch: 5| Step: 4
Training loss: 2.0807392597198486
Validation loss: 2.2085969871090305

Epoch: 5| Step: 5
Training loss: 2.5906848907470703
Validation loss: 2.205037513086873

Epoch: 5| Step: 6
Training loss: 2.679377555847168
Validation loss: 2.2093954599031838

Epoch: 5| Step: 7
Training loss: 2.0325374603271484
Validation loss: 2.2153498100978073

Epoch: 5| Step: 8
Training loss: 2.167726755142212
Validation loss: 2.1997605651937504

Epoch: 5| Step: 9
Training loss: 2.4414730072021484
Validation loss: 2.1953333372710855

Epoch: 5| Step: 10
Training loss: 2.6376097202301025
Validation loss: 2.227086123599801

Epoch: 73| Step: 0
Training loss: 2.246914863586426
Validation loss: 2.221500240346437

Epoch: 5| Step: 1
Training loss: 1.8087602853775024
Validation loss: 2.21189861143789

Epoch: 5| Step: 2
Training loss: 2.482475996017456
Validation loss: 2.194310930467421

Epoch: 5| Step: 3
Training loss: 2.274718999862671
Validation loss: 2.209652257222001

Epoch: 5| Step: 4
Training loss: 1.7359453439712524
Validation loss: 2.1968068025445424

Epoch: 5| Step: 5
Training loss: 2.267014265060425
Validation loss: 2.1981909685237433

Epoch: 5| Step: 6
Training loss: 2.47521710395813
Validation loss: 2.237885877650271

Epoch: 5| Step: 7
Training loss: 2.2551894187927246
Validation loss: 2.1937781354432464

Epoch: 5| Step: 8
Training loss: 2.5844364166259766
Validation loss: 2.1944745279127553

Epoch: 5| Step: 9
Training loss: 2.8322904109954834
Validation loss: 2.2144923543417327

Epoch: 5| Step: 10
Training loss: 2.440816879272461
Validation loss: 2.194589804577571

Epoch: 74| Step: 0
Training loss: 2.1438651084899902
Validation loss: 2.21298389280996

Epoch: 5| Step: 1
Training loss: 2.8879945278167725
Validation loss: 2.210327127928375

Epoch: 5| Step: 2
Training loss: 1.9362332820892334
Validation loss: 2.166273150392758

Epoch: 5| Step: 3
Training loss: 1.7099106311798096
Validation loss: 2.2059473068483415

Epoch: 5| Step: 4
Training loss: 1.9602209329605103
Validation loss: 2.1998934861152404

Epoch: 5| Step: 5
Training loss: 2.5710840225219727
Validation loss: 2.195394085299584

Epoch: 5| Step: 6
Training loss: 2.257904291152954
Validation loss: 2.1651843927239858

Epoch: 5| Step: 7
Training loss: 2.4834144115448
Validation loss: 2.1956371466318765

Epoch: 5| Step: 8
Training loss: 2.347374677658081
Validation loss: 2.177559111707954

Epoch: 5| Step: 9
Training loss: 2.5615429878234863
Validation loss: 2.2006442444298857

Epoch: 5| Step: 10
Training loss: 2.6055569648742676
Validation loss: 2.198850876541548

Epoch: 75| Step: 0
Training loss: 2.2887394428253174
Validation loss: 2.2123187921380483

Epoch: 5| Step: 1
Training loss: 1.7442127466201782
Validation loss: 2.1717866031072472

Epoch: 5| Step: 2
Training loss: 1.8731663227081299
Validation loss: 2.1816719296158

Epoch: 5| Step: 3
Training loss: 2.2651126384735107
Validation loss: 2.230065953346991

Epoch: 5| Step: 4
Training loss: 2.4671247005462646
Validation loss: 2.173144978861655

Epoch: 5| Step: 5
Training loss: 2.0593361854553223
Validation loss: 2.2011548126897504

Epoch: 5| Step: 6
Training loss: 2.824920415878296
Validation loss: 2.1767269513940297

Epoch: 5| Step: 7
Training loss: 2.2231392860412598
Validation loss: 2.210944838421319

Epoch: 5| Step: 8
Training loss: 2.5007412433624268
Validation loss: 2.1776292862430697

Epoch: 5| Step: 9
Training loss: 3.1139636039733887
Validation loss: 2.1849652182671333

Epoch: 5| Step: 10
Training loss: 1.893187165260315
Validation loss: 2.2069110261496676

Epoch: 76| Step: 0
Training loss: 2.31538724899292
Validation loss: 2.2259217128958753

Epoch: 5| Step: 1
Training loss: 2.4032795429229736
Validation loss: 2.197712136853126

Epoch: 5| Step: 2
Training loss: 2.4225096702575684
Validation loss: 2.2039270170273317

Epoch: 5| Step: 3
Training loss: 2.2806496620178223
Validation loss: 2.2376266038545998

Epoch: 5| Step: 4
Training loss: 2.1793198585510254
Validation loss: 2.2087175961463683

Epoch: 5| Step: 5
Training loss: 2.1695070266723633
Validation loss: 2.2215649363815144

Epoch: 5| Step: 6
Training loss: 1.9368669986724854
Validation loss: 2.1842387286565637

Epoch: 5| Step: 7
Training loss: 2.530855894088745
Validation loss: 2.1537399343265

Epoch: 5| Step: 8
Training loss: 2.3845326900482178
Validation loss: 2.1618145332541516

Epoch: 5| Step: 9
Training loss: 1.999979019165039
Validation loss: 2.2325549843490764

Epoch: 5| Step: 10
Training loss: 2.971815586090088
Validation loss: 2.174976953896143

Epoch: 77| Step: 0
Training loss: 1.735020399093628
Validation loss: 2.1749274538409327

Epoch: 5| Step: 1
Training loss: 2.4520397186279297
Validation loss: 2.2090027563033567

Epoch: 5| Step: 2
Training loss: 2.282064437866211
Validation loss: 2.2243186261064265

Epoch: 5| Step: 3
Training loss: 2.595458984375
Validation loss: 2.1662109769800657

Epoch: 5| Step: 4
Training loss: 2.468010902404785
Validation loss: 2.1639246581703104

Epoch: 5| Step: 5
Training loss: 1.598510503768921
Validation loss: 2.200339182730644

Epoch: 5| Step: 6
Training loss: 2.7289443016052246
Validation loss: 2.199838243505006

Epoch: 5| Step: 7
Training loss: 1.9021838903427124
Validation loss: 2.1960497415193947

Epoch: 5| Step: 8
Training loss: 2.402862071990967
Validation loss: 2.182363825459634

Epoch: 5| Step: 9
Training loss: 2.32304048538208
Validation loss: 2.170285996570382

Epoch: 5| Step: 10
Training loss: 2.943125009536743
Validation loss: 2.213665949401035

Epoch: 78| Step: 0
Training loss: 1.7661497592926025
Validation loss: 2.191471536954244

Epoch: 5| Step: 1
Training loss: 2.475475549697876
Validation loss: 2.2036920439812446

Epoch: 5| Step: 2
Training loss: 2.368957996368408
Validation loss: 2.188697035594653

Epoch: 5| Step: 3
Training loss: 2.217327833175659
Validation loss: 2.187540741376979

Epoch: 5| Step: 4
Training loss: 2.02952241897583
Validation loss: 2.1710682774102814

Epoch: 5| Step: 5
Training loss: 2.5807676315307617
Validation loss: 2.160179038201609

Epoch: 5| Step: 6
Training loss: 2.2032361030578613
Validation loss: 2.1779863116561726

Epoch: 5| Step: 7
Training loss: 2.592068910598755
Validation loss: 2.1592148593676987

Epoch: 5| Step: 8
Training loss: 1.8928802013397217
Validation loss: 2.173741094527706

Epoch: 5| Step: 9
Training loss: 2.428165912628174
Validation loss: 2.1814950063664424

Epoch: 5| Step: 10
Training loss: 2.6256496906280518
Validation loss: 2.192826537675755

Epoch: 79| Step: 0
Training loss: 1.925571084022522
Validation loss: 2.1508688311423025

Epoch: 5| Step: 1
Training loss: 1.844551682472229
Validation loss: 2.1728590355124524

Epoch: 5| Step: 2
Training loss: 2.4795730113983154
Validation loss: 2.180638685021349

Epoch: 5| Step: 3
Training loss: 2.4398751258850098
Validation loss: 2.2139224954830703

Epoch: 5| Step: 4
Training loss: 2.2810776233673096
Validation loss: 2.185192823410034

Epoch: 5| Step: 5
Training loss: 2.5689902305603027
Validation loss: 2.1784299394135833

Epoch: 5| Step: 6
Training loss: 2.236555337905884
Validation loss: 2.2231881054498817

Epoch: 5| Step: 7
Training loss: 2.3064002990722656
Validation loss: 2.1658746529650945

Epoch: 5| Step: 8
Training loss: 2.5131468772888184
Validation loss: 2.1820773770732265

Epoch: 5| Step: 9
Training loss: 2.2889909744262695
Validation loss: 2.179261689545006

Epoch: 5| Step: 10
Training loss: 2.2395050525665283
Validation loss: 2.174042952957974

Epoch: 80| Step: 0
Training loss: 1.6193748712539673
Validation loss: 2.2020513370472896

Epoch: 5| Step: 1
Training loss: 3.363215208053589
Validation loss: 2.1950374918599285

Epoch: 5| Step: 2
Training loss: 2.0378458499908447
Validation loss: 2.193217028853714

Epoch: 5| Step: 3
Training loss: 2.585045099258423
Validation loss: 2.1919433339949577

Epoch: 5| Step: 4
Training loss: 1.7789745330810547
Validation loss: 2.1771422739951842

Epoch: 5| Step: 5
Training loss: 2.4092230796813965
Validation loss: 2.1781878445738103

Epoch: 5| Step: 6
Training loss: 2.3411941528320312
Validation loss: 2.197056088396298

Epoch: 5| Step: 7
Training loss: 2.4536471366882324
Validation loss: 2.1767195283725695

Epoch: 5| Step: 8
Training loss: 2.358863115310669
Validation loss: 2.195293523932016

Epoch: 5| Step: 9
Training loss: 1.7199008464813232
Validation loss: 2.219011575944962

Epoch: 5| Step: 10
Training loss: 2.652952194213867
Validation loss: 2.213286143477245

Epoch: 81| Step: 0
Training loss: 2.3039166927337646
Validation loss: 2.1667794053272535

Epoch: 5| Step: 1
Training loss: 1.5697968006134033
Validation loss: 2.178906261280019

Epoch: 5| Step: 2
Training loss: 1.6829273700714111
Validation loss: 2.2021284949394966

Epoch: 5| Step: 3
Training loss: 2.7891414165496826
Validation loss: 2.186118515588904

Epoch: 5| Step: 4
Training loss: 2.4247615337371826
Validation loss: 2.176666195674609

Epoch: 5| Step: 5
Training loss: 2.273411989212036
Validation loss: 2.1765536954326015

Epoch: 5| Step: 6
Training loss: 2.235257387161255
Validation loss: 2.172072859220607

Epoch: 5| Step: 7
Training loss: 2.6012401580810547
Validation loss: 2.181915831822221

Epoch: 5| Step: 8
Training loss: 2.8109915256500244
Validation loss: 2.164551196559783

Epoch: 5| Step: 9
Training loss: 2.367983341217041
Validation loss: 2.162553205285021

Epoch: 5| Step: 10
Training loss: 2.3578805923461914
Validation loss: 2.167897569235935

Epoch: 82| Step: 0
Training loss: 2.299889087677002
Validation loss: 2.1896273371993855

Epoch: 5| Step: 1
Training loss: 1.8342796564102173
Validation loss: 2.152151051387992

Epoch: 5| Step: 2
Training loss: 1.9736248254776
Validation loss: 2.1778946589398127

Epoch: 5| Step: 3
Training loss: 2.257007360458374
Validation loss: 2.2081271935534734

Epoch: 5| Step: 4
Training loss: 2.2721590995788574
Validation loss: 2.1908518524580103

Epoch: 5| Step: 5
Training loss: 2.4009132385253906
Validation loss: 2.1503546827582904

Epoch: 5| Step: 6
Training loss: 2.7413251399993896
Validation loss: 2.190829159111105

Epoch: 5| Step: 7
Training loss: 2.3947062492370605
Validation loss: 2.181124638485652

Epoch: 5| Step: 8
Training loss: 2.404366970062256
Validation loss: 2.147415453387845

Epoch: 5| Step: 9
Training loss: 1.8410890102386475
Validation loss: 2.198057684847104

Epoch: 5| Step: 10
Training loss: 2.974273443222046
Validation loss: 2.219578904490317

Epoch: 83| Step: 0
Training loss: 2.5328006744384766
Validation loss: 2.1936556985301356

Epoch: 5| Step: 1
Training loss: 2.016387939453125
Validation loss: 2.17495963650365

Epoch: 5| Step: 2
Training loss: 1.861931562423706
Validation loss: 2.2212754449536725

Epoch: 5| Step: 3
Training loss: 2.0913143157958984
Validation loss: 2.162457864771607

Epoch: 5| Step: 4
Training loss: 1.9520972967147827
Validation loss: 2.2222300370534263

Epoch: 5| Step: 5
Training loss: 2.5599217414855957
Validation loss: 2.1736252871892785

Epoch: 5| Step: 6
Training loss: 2.502253293991089
Validation loss: 2.1732967694600425

Epoch: 5| Step: 7
Training loss: 2.219980239868164
Validation loss: 2.1799630964955976

Epoch: 5| Step: 8
Training loss: 3.4512362480163574
Validation loss: 2.1905089988503406

Epoch: 5| Step: 9
Training loss: 2.3538689613342285
Validation loss: 2.184154901453244

Epoch: 5| Step: 10
Training loss: 1.4954715967178345
Validation loss: 2.2089723489617787

Epoch: 84| Step: 0
Training loss: 2.4683334827423096
Validation loss: 2.1805527620418097

Epoch: 5| Step: 1
Training loss: 2.268578290939331
Validation loss: 2.1949057245767243

Epoch: 5| Step: 2
Training loss: 2.1131389141082764
Validation loss: 2.201668526536675

Epoch: 5| Step: 3
Training loss: 2.199875593185425
Validation loss: 2.163251038520567

Epoch: 5| Step: 4
Training loss: 1.8108234405517578
Validation loss: 2.168382506216726

Epoch: 5| Step: 5
Training loss: 1.7468725442886353
Validation loss: 2.16810885296073

Epoch: 5| Step: 6
Training loss: 2.410707712173462
Validation loss: 2.2052445565500567

Epoch: 5| Step: 7
Training loss: 2.682523012161255
Validation loss: 2.2280275949867825

Epoch: 5| Step: 8
Training loss: 2.6410861015319824
Validation loss: 2.1852466316633326

Epoch: 5| Step: 9
Training loss: 2.2016117572784424
Validation loss: 2.2174202165296

Epoch: 5| Step: 10
Training loss: 2.4833767414093018
Validation loss: 2.1975276649639173

Epoch: 85| Step: 0
Training loss: 2.0728297233581543
Validation loss: 2.182073408557523

Epoch: 5| Step: 1
Training loss: 1.8216485977172852
Validation loss: 2.200094997241933

Epoch: 5| Step: 2
Training loss: 2.715118646621704
Validation loss: 2.1551792544703328

Epoch: 5| Step: 3
Training loss: 2.177156686782837
Validation loss: 2.195017332671791

Epoch: 5| Step: 4
Training loss: 2.483920097351074
Validation loss: 2.1757820883104877

Epoch: 5| Step: 5
Training loss: 2.332540988922119
Validation loss: 2.193743710876793

Epoch: 5| Step: 6
Training loss: 2.2594902515411377
Validation loss: 2.16806346883056

Epoch: 5| Step: 7
Training loss: 2.759718179702759
Validation loss: 2.174036900202433

Epoch: 5| Step: 8
Training loss: 2.0127463340759277
Validation loss: 2.1714685193953978

Epoch: 5| Step: 9
Training loss: 1.7985137701034546
Validation loss: 2.1774775571720575

Epoch: 5| Step: 10
Training loss: 2.422290563583374
Validation loss: 2.189568599065145

Epoch: 86| Step: 0
Training loss: 2.1634249687194824
Validation loss: 2.2096610594821233

Epoch: 5| Step: 1
Training loss: 1.8311290740966797
Validation loss: 2.183077589158089

Epoch: 5| Step: 2
Training loss: 2.5126419067382812
Validation loss: 2.1550872389988234

Epoch: 5| Step: 3
Training loss: 2.3024637699127197
Validation loss: 2.1840506804886686

Epoch: 5| Step: 4
Training loss: 2.6213316917419434
Validation loss: 2.20096601722061

Epoch: 5| Step: 5
Training loss: 2.043107748031616
Validation loss: 2.1979984839757285

Epoch: 5| Step: 6
Training loss: 2.362704038619995
Validation loss: 2.178511065821494

Epoch: 5| Step: 7
Training loss: 1.646583914756775
Validation loss: 2.2113682813541864

Epoch: 5| Step: 8
Training loss: 2.543064832687378
Validation loss: 2.2202894354379303

Epoch: 5| Step: 9
Training loss: 2.516892194747925
Validation loss: 2.2154369302975234

Epoch: 5| Step: 10
Training loss: 2.5792312622070312
Validation loss: 2.199832729113999

Epoch: 87| Step: 0
Training loss: 2.6608684062957764
Validation loss: 2.195731129697574

Epoch: 5| Step: 1
Training loss: 3.265866756439209
Validation loss: 2.1737279456148864

Epoch: 5| Step: 2
Training loss: 2.2230570316314697
Validation loss: 2.1978578913596367

Epoch: 5| Step: 3
Training loss: 2.3452422618865967
Validation loss: 2.178116447182112

Epoch: 5| Step: 4
Training loss: 1.854996919631958
Validation loss: 2.182045590492987

Epoch: 5| Step: 5
Training loss: 2.38741135597229
Validation loss: 2.2124761330184115

Epoch: 5| Step: 6
Training loss: 1.7121250629425049
Validation loss: 2.1980928246692946

Epoch: 5| Step: 7
Training loss: 1.5600229501724243
Validation loss: 2.207062849434473

Epoch: 5| Step: 8
Training loss: 2.21484375
Validation loss: 2.203248849479101

Epoch: 5| Step: 9
Training loss: 2.5544800758361816
Validation loss: 2.188675744559175

Epoch: 5| Step: 10
Training loss: 1.9188977479934692
Validation loss: 2.199486017227173

Epoch: 88| Step: 0
Training loss: 1.9496656656265259
Validation loss: 2.1802904734047512

Epoch: 5| Step: 1
Training loss: 2.4853947162628174
Validation loss: 2.210181138848746

Epoch: 5| Step: 2
Training loss: 2.151522159576416
Validation loss: 2.1884200521694717

Epoch: 5| Step: 3
Training loss: 2.668581485748291
Validation loss: 2.1892580652749665

Epoch: 5| Step: 4
Training loss: 1.6075270175933838
Validation loss: 2.18834327882336

Epoch: 5| Step: 5
Training loss: 2.185516357421875
Validation loss: 2.1922911700382026

Epoch: 5| Step: 6
Training loss: 2.1400234699249268
Validation loss: 2.1629906200593516

Epoch: 5| Step: 7
Training loss: 2.5316925048828125
Validation loss: 2.1770627011534986

Epoch: 5| Step: 8
Training loss: 2.8765029907226562
Validation loss: 2.190171798070272

Epoch: 5| Step: 9
Training loss: 2.2594027519226074
Validation loss: 2.178797111716322

Epoch: 5| Step: 10
Training loss: 2.0439395904541016
Validation loss: 2.185274129272789

Epoch: 89| Step: 0
Training loss: 2.37127947807312
Validation loss: 2.1689611891264557

Epoch: 5| Step: 1
Training loss: 2.7586939334869385
Validation loss: 2.181898537502494

Epoch: 5| Step: 2
Training loss: 1.9837738275527954
Validation loss: 2.160292597227199

Epoch: 5| Step: 3
Training loss: 2.425837278366089
Validation loss: 2.1773661285318355

Epoch: 5| Step: 4
Training loss: 1.6360619068145752
Validation loss: 2.1942786914046093

Epoch: 5| Step: 5
Training loss: 1.646754503250122
Validation loss: 2.1771166452797512

Epoch: 5| Step: 6
Training loss: 2.3242812156677246
Validation loss: 2.1554482598458566

Epoch: 5| Step: 7
Training loss: 2.706756114959717
Validation loss: 2.2040005448043987

Epoch: 5| Step: 8
Training loss: 3.0247905254364014
Validation loss: 2.2061375623108237

Epoch: 5| Step: 9
Training loss: 2.0198822021484375
Validation loss: 2.162658199187248

Epoch: 5| Step: 10
Training loss: 1.7254304885864258
Validation loss: 2.1669025344233357

Epoch: 90| Step: 0
Training loss: 2.569714069366455
Validation loss: 2.1958510439882994

Epoch: 5| Step: 1
Training loss: 2.1411821842193604
Validation loss: 2.16638474054234

Epoch: 5| Step: 2
Training loss: 1.9359763860702515
Validation loss: 2.1695756989140667

Epoch: 5| Step: 3
Training loss: 2.0036652088165283
Validation loss: 2.180835134239607

Epoch: 5| Step: 4
Training loss: 1.5111440420150757
Validation loss: 2.1638360664408696

Epoch: 5| Step: 5
Training loss: 2.438826084136963
Validation loss: 2.1885629212984474

Epoch: 5| Step: 6
Training loss: 2.2726261615753174
Validation loss: 2.1789051307144987

Epoch: 5| Step: 7
Training loss: 2.3360519409179688
Validation loss: 2.196683209429505

Epoch: 5| Step: 8
Training loss: 2.7616872787475586
Validation loss: 2.1858284165782313

Epoch: 5| Step: 9
Training loss: 2.2280051708221436
Validation loss: 2.199758985991119

Epoch: 5| Step: 10
Training loss: 2.7156591415405273
Validation loss: 2.191930845219602

Epoch: 91| Step: 0
Training loss: 2.4535281658172607
Validation loss: 2.1940199047006588

Epoch: 5| Step: 1
Training loss: 2.3745312690734863
Validation loss: 2.171668650001608

Epoch: 5| Step: 2
Training loss: 2.039354085922241
Validation loss: 2.1800383752392185

Epoch: 5| Step: 3
Training loss: 2.8039770126342773
Validation loss: 2.19999252083481

Epoch: 5| Step: 4
Training loss: 2.289151430130005
Validation loss: 2.187652726327219

Epoch: 5| Step: 5
Training loss: 2.269871950149536
Validation loss: 2.2127480481260564

Epoch: 5| Step: 6
Training loss: 2.8546149730682373
Validation loss: 2.174763430831253

Epoch: 5| Step: 7
Training loss: 2.0770509243011475
Validation loss: 2.194966536696239

Epoch: 5| Step: 8
Training loss: 1.83364737033844
Validation loss: 2.20250561160426

Epoch: 5| Step: 9
Training loss: 1.8639018535614014
Validation loss: 2.199661462537704

Epoch: 5| Step: 10
Training loss: 1.803203821182251
Validation loss: 2.2204044672750656

Epoch: 92| Step: 0
Training loss: 2.59550404548645
Validation loss: 2.19483446562162

Epoch: 5| Step: 1
Training loss: 2.489856481552124
Validation loss: 2.178061672436294

Epoch: 5| Step: 2
Training loss: 1.963653564453125
Validation loss: 2.171214513881232

Epoch: 5| Step: 3
Training loss: 2.498762369155884
Validation loss: 2.1974915458310034

Epoch: 5| Step: 4
Training loss: 2.573216199874878
Validation loss: 2.2000028779429774

Epoch: 5| Step: 5
Training loss: 2.162578582763672
Validation loss: 2.1998418351655364

Epoch: 5| Step: 6
Training loss: 2.258345603942871
Validation loss: 2.1967298651254303

Epoch: 5| Step: 7
Training loss: 1.9076725244522095
Validation loss: 2.2068545523510186

Epoch: 5| Step: 8
Training loss: 2.2603237628936768
Validation loss: 2.1842657955743934

Epoch: 5| Step: 9
Training loss: 1.9902502298355103
Validation loss: 2.2192757501397082

Epoch: 5| Step: 10
Training loss: 1.9697144031524658
Validation loss: 2.1811838406388477

Epoch: 93| Step: 0
Training loss: 1.602683424949646
Validation loss: 2.183260021671172

Epoch: 5| Step: 1
Training loss: 1.8787723779678345
Validation loss: 2.17961795355684

Epoch: 5| Step: 2
Training loss: 1.495067834854126
Validation loss: 2.1352460563823743

Epoch: 5| Step: 3
Training loss: 2.549107313156128
Validation loss: 2.2165509910993677

Epoch: 5| Step: 4
Training loss: 2.509009599685669
Validation loss: 2.171341485874627

Epoch: 5| Step: 5
Training loss: 1.5141854286193848
Validation loss: 2.196003472933205

Epoch: 5| Step: 6
Training loss: 3.3472161293029785
Validation loss: 2.1632917696429836

Epoch: 5| Step: 7
Training loss: 1.9200979471206665
Validation loss: 2.182746402678951

Epoch: 5| Step: 8
Training loss: 3.043433427810669
Validation loss: 2.198449960318945

Epoch: 5| Step: 9
Training loss: 2.489089012145996
Validation loss: 2.1816671227896087

Epoch: 5| Step: 10
Training loss: 2.2205469608306885
Validation loss: 2.1883346534544423

Epoch: 94| Step: 0
Training loss: 2.5348174571990967
Validation loss: 2.157495742203087

Epoch: 5| Step: 1
Training loss: 2.1664047241210938
Validation loss: 2.2095059310236285

Epoch: 5| Step: 2
Training loss: 2.4100756645202637
Validation loss: 2.227766749679401

Epoch: 5| Step: 3
Training loss: 1.644386649131775
Validation loss: 2.161218632933914

Epoch: 5| Step: 4
Training loss: 1.795723557472229
Validation loss: 2.1946521933360765

Epoch: 5| Step: 5
Training loss: 2.3939833641052246
Validation loss: 2.2061670287962882

Epoch: 5| Step: 6
Training loss: 2.414259195327759
Validation loss: 2.147352956956433

Epoch: 5| Step: 7
Training loss: 2.234135150909424
Validation loss: 2.177137945287971

Epoch: 5| Step: 8
Training loss: 2.3145267963409424
Validation loss: 2.204609338955213

Epoch: 5| Step: 9
Training loss: 3.274475574493408
Validation loss: 2.143744373834261

Epoch: 5| Step: 10
Training loss: 1.5054463148117065
Validation loss: 2.1469533340905302

Epoch: 95| Step: 0
Training loss: 2.8314766883850098
Validation loss: 2.194887025381929

Epoch: 5| Step: 1
Training loss: 2.162868022918701
Validation loss: 2.162572537699053

Epoch: 5| Step: 2
Training loss: 2.458143949508667
Validation loss: 2.205405035326558

Epoch: 5| Step: 3
Training loss: 2.1203839778900146
Validation loss: 2.1807502033889934

Epoch: 5| Step: 4
Training loss: 2.4295976161956787
Validation loss: 2.1571522041033675

Epoch: 5| Step: 5
Training loss: 1.9225059747695923
Validation loss: 2.1814670165379844

Epoch: 5| Step: 6
Training loss: 1.7383861541748047
Validation loss: 2.1834286348794096

Epoch: 5| Step: 7
Training loss: 2.6021454334259033
Validation loss: 2.176832245242211

Epoch: 5| Step: 8
Training loss: 2.2186310291290283
Validation loss: 2.1838502166091756

Epoch: 5| Step: 9
Training loss: 2.052354097366333
Validation loss: 2.1909051505468224

Epoch: 5| Step: 10
Training loss: 2.1300060749053955
Validation loss: 2.195191591016708

Epoch: 96| Step: 0
Training loss: 2.3961498737335205
Validation loss: 2.183549625899202

Epoch: 5| Step: 1
Training loss: 2.043034076690674
Validation loss: 2.1922563968166227

Epoch: 5| Step: 2
Training loss: 1.6166538000106812
Validation loss: 2.136673669661245

Epoch: 5| Step: 3
Training loss: 2.011186122894287
Validation loss: 2.222304346740887

Epoch: 5| Step: 4
Training loss: 2.238373279571533
Validation loss: 2.145854006531418

Epoch: 5| Step: 5
Training loss: 1.913240671157837
Validation loss: 2.210924788187909

Epoch: 5| Step: 6
Training loss: 2.47782564163208
Validation loss: 2.206783539505415

Epoch: 5| Step: 7
Training loss: 2.7326018810272217
Validation loss: 2.177270766227476

Epoch: 5| Step: 8
Training loss: 2.351425886154175
Validation loss: 2.155644424500004

Epoch: 5| Step: 9
Training loss: 2.041086435317993
Validation loss: 2.172474786799441

Epoch: 5| Step: 10
Training loss: 2.714841365814209
Validation loss: 2.174809732744771

Epoch: 97| Step: 0
Training loss: 1.8574869632720947
Validation loss: 2.172789704415106

Epoch: 5| Step: 1
Training loss: 2.4039223194122314
Validation loss: 2.2034716324139665

Epoch: 5| Step: 2
Training loss: 2.6993117332458496
Validation loss: 2.167920186955442

Epoch: 5| Step: 3
Training loss: 2.164302349090576
Validation loss: 2.2056957188472954

Epoch: 5| Step: 4
Training loss: 1.875973105430603
Validation loss: 2.177463500730453

Epoch: 5| Step: 5
Training loss: 2.0463387966156006
Validation loss: 2.1915471938348587

Epoch: 5| Step: 6
Training loss: 2.077152729034424
Validation loss: 2.1910728331535094

Epoch: 5| Step: 7
Training loss: 1.9944826364517212
Validation loss: 2.142531587231544

Epoch: 5| Step: 8
Training loss: 2.086911916732788
Validation loss: 2.2001478031117427

Epoch: 5| Step: 9
Training loss: 2.122447967529297
Validation loss: 2.175653860133181

Epoch: 5| Step: 10
Training loss: 3.2619383335113525
Validation loss: 2.1894191900889077

Epoch: 98| Step: 0
Training loss: 1.8838167190551758
Validation loss: 2.1861080866987987

Epoch: 5| Step: 1
Training loss: 2.32906436920166
Validation loss: 2.156651084141065

Epoch: 5| Step: 2
Training loss: 2.920703887939453
Validation loss: 2.15471141312712

Epoch: 5| Step: 3
Training loss: 2.3913321495056152
Validation loss: 2.202064093723092

Epoch: 5| Step: 4
Training loss: 1.7659518718719482
Validation loss: 2.154880782609345

Epoch: 5| Step: 5
Training loss: 2.1962027549743652
Validation loss: 2.184595287487071

Epoch: 5| Step: 6
Training loss: 2.402883768081665
Validation loss: 2.1580340477728073

Epoch: 5| Step: 7
Training loss: 2.568293333053589
Validation loss: 2.189333046636274

Epoch: 5| Step: 8
Training loss: 2.1018338203430176
Validation loss: 2.161856979452154

Epoch: 5| Step: 9
Training loss: 1.920983910560608
Validation loss: 2.2286783687530027

Epoch: 5| Step: 10
Training loss: 1.9500514268875122
Validation loss: 2.1821597289013606

Epoch: 99| Step: 0
Training loss: 1.7110008001327515
Validation loss: 2.189924975877167

Epoch: 5| Step: 1
Training loss: 2.299029588699341
Validation loss: 2.1593473162702335

Epoch: 5| Step: 2
Training loss: 2.7048728466033936
Validation loss: 2.1926680252116215

Epoch: 5| Step: 3
Training loss: 2.4007060527801514
Validation loss: 2.1603315389284523

Epoch: 5| Step: 4
Training loss: 2.8592376708984375
Validation loss: 2.151700794055898

Epoch: 5| Step: 5
Training loss: 2.3648719787597656
Validation loss: 2.1537014643351235

Epoch: 5| Step: 6
Training loss: 2.2760252952575684
Validation loss: 2.1686495580980854

Epoch: 5| Step: 7
Training loss: 2.2901597023010254
Validation loss: 2.1799582076329056

Epoch: 5| Step: 8
Training loss: 1.5123870372772217
Validation loss: 2.1756625611294984

Epoch: 5| Step: 9
Training loss: 2.1536808013916016
Validation loss: 2.1507984745887017

Epoch: 5| Step: 10
Training loss: 1.8294415473937988
Validation loss: 2.180088056031094

Epoch: 100| Step: 0
Training loss: 2.0516393184661865
Validation loss: 2.15331034891067

Epoch: 5| Step: 1
Training loss: 2.310032367706299
Validation loss: 2.2090923119616765

Epoch: 5| Step: 2
Training loss: 2.5399081707000732
Validation loss: 2.1475174888487785

Epoch: 5| Step: 3
Training loss: 1.6989787817001343
Validation loss: 2.16344863881347

Epoch: 5| Step: 4
Training loss: 2.7025766372680664
Validation loss: 2.159460525358877

Epoch: 5| Step: 5
Training loss: 2.0535781383514404
Validation loss: 2.1894682030523978

Epoch: 5| Step: 6
Training loss: 2.366208553314209
Validation loss: 2.1615365141181537

Epoch: 5| Step: 7
Training loss: 1.757997751235962
Validation loss: 2.223642287715789

Epoch: 5| Step: 8
Training loss: 1.8523576259613037
Validation loss: 2.214311276712725

Epoch: 5| Step: 9
Training loss: 2.403972625732422
Validation loss: 2.1877831592354724

Epoch: 5| Step: 10
Training loss: 2.650146007537842
Validation loss: 2.1710020495999243

Epoch: 101| Step: 0
Training loss: 2.806547164916992
Validation loss: 2.241600695476737

Epoch: 5| Step: 1
Training loss: 2.2031478881835938
Validation loss: 2.1674889338913785

Epoch: 5| Step: 2
Training loss: 2.1310622692108154
Validation loss: 2.1500941591878093

Epoch: 5| Step: 3
Training loss: 2.15144681930542
Validation loss: 2.193019510597311

Epoch: 5| Step: 4
Training loss: 2.2640621662139893
Validation loss: 2.190289851157896

Epoch: 5| Step: 5
Training loss: 2.3538084030151367
Validation loss: 2.205799646275018

Epoch: 5| Step: 6
Training loss: 1.5656441450119019
Validation loss: 2.164626302257661

Epoch: 5| Step: 7
Training loss: 2.7760274410247803
Validation loss: 2.208669408675163

Epoch: 5| Step: 8
Training loss: 2.006552219390869
Validation loss: 2.179464601701306

Epoch: 5| Step: 9
Training loss: 1.3930753469467163
Validation loss: 2.156294076673446

Epoch: 5| Step: 10
Training loss: 2.949906349182129
Validation loss: 2.1907168844694733

Epoch: 102| Step: 0
Training loss: 2.3056564331054688
Validation loss: 2.166005229437223

Epoch: 5| Step: 1
Training loss: 2.422110080718994
Validation loss: 2.141703005759947

Epoch: 5| Step: 2
Training loss: 1.794398546218872
Validation loss: 2.169566085261683

Epoch: 5| Step: 3
Training loss: 2.1369996070861816
Validation loss: 2.184882458820138

Epoch: 5| Step: 4
Training loss: 2.3961260318756104
Validation loss: 2.201968785255186

Epoch: 5| Step: 5
Training loss: 1.886565923690796
Validation loss: 2.1601642639406267

Epoch: 5| Step: 6
Training loss: 1.919843077659607
Validation loss: 2.1369849071707776

Epoch: 5| Step: 7
Training loss: 2.393977403640747
Validation loss: 2.1631466522011706

Epoch: 5| Step: 8
Training loss: 2.4009976387023926
Validation loss: 2.1833128160045994

Epoch: 5| Step: 9
Training loss: 2.423351287841797
Validation loss: 2.1796425542523785

Epoch: 5| Step: 10
Training loss: 2.065505266189575
Validation loss: 2.1903652709017516

Epoch: 103| Step: 0
Training loss: 2.3300728797912598
Validation loss: 2.1959635980667604

Epoch: 5| Step: 1
Training loss: 1.840898871421814
Validation loss: 2.169319409196095

Epoch: 5| Step: 2
Training loss: 1.9196487665176392
Validation loss: 2.1541081090127268

Epoch: 5| Step: 3
Training loss: 2.491978883743286
Validation loss: 2.156482276096139

Epoch: 5| Step: 4
Training loss: 2.4536755084991455
Validation loss: 2.204381057011184

Epoch: 5| Step: 5
Training loss: 1.442457914352417
Validation loss: 2.1722310922479116

Epoch: 5| Step: 6
Training loss: 1.528148889541626
Validation loss: 2.1888654103843113

Epoch: 5| Step: 7
Training loss: 2.7488503456115723
Validation loss: 2.1535146185146865

Epoch: 5| Step: 8
Training loss: 2.6116726398468018
Validation loss: 2.2088483520733413

Epoch: 5| Step: 9
Training loss: 2.574354648590088
Validation loss: 2.1772809502899007

Epoch: 5| Step: 10
Training loss: 2.3924460411071777
Validation loss: 2.157490781558457

Epoch: 104| Step: 0
Training loss: 2.2245290279388428
Validation loss: 2.1567622743627077

Epoch: 5| Step: 1
Training loss: 2.6533780097961426
Validation loss: 2.1666661898295083

Epoch: 5| Step: 2
Training loss: 1.7694694995880127
Validation loss: 2.1848942720761864

Epoch: 5| Step: 3
Training loss: 2.417381763458252
Validation loss: 2.184302804290607

Epoch: 5| Step: 4
Training loss: 2.315051317214966
Validation loss: 2.1637357076009116

Epoch: 5| Step: 5
Training loss: 1.7434971332550049
Validation loss: 2.186291879223239

Epoch: 5| Step: 6
Training loss: 1.7387861013412476
Validation loss: 2.1697375825656358

Epoch: 5| Step: 7
Training loss: 2.5268893241882324
Validation loss: 2.1592915775955364

Epoch: 5| Step: 8
Training loss: 2.0668983459472656
Validation loss: 2.1668405148290817

Epoch: 5| Step: 9
Training loss: 2.1243908405303955
Validation loss: 2.1454056719298005

Epoch: 5| Step: 10
Training loss: 2.607588768005371
Validation loss: 2.124324119219216

Epoch: 105| Step: 0
Training loss: 2.282672882080078
Validation loss: 2.1892604340789137

Epoch: 5| Step: 1
Training loss: 2.01686429977417
Validation loss: 2.174085287637608

Epoch: 5| Step: 2
Training loss: 2.5986568927764893
Validation loss: 2.239984812275056

Epoch: 5| Step: 3
Training loss: 2.0121142864227295
Validation loss: 2.188276690821494

Epoch: 5| Step: 4
Training loss: 1.9644025564193726
Validation loss: 2.187672343305362

Epoch: 5| Step: 5
Training loss: 1.607473611831665
Validation loss: 2.160704706304817

Epoch: 5| Step: 6
Training loss: 2.03423810005188
Validation loss: 2.185416644619357

Epoch: 5| Step: 7
Training loss: 2.4826271533966064
Validation loss: 2.1904511092811503

Epoch: 5| Step: 8
Training loss: 1.9216865301132202
Validation loss: 2.1616263799769904

Epoch: 5| Step: 9
Training loss: 3.5749778747558594
Validation loss: 2.133991200436828

Epoch: 5| Step: 10
Training loss: 1.9515742063522339
Validation loss: 2.1555263560305358

Epoch: 106| Step: 0
Training loss: 1.7107927799224854
Validation loss: 2.1401302019755044

Epoch: 5| Step: 1
Training loss: 2.4471404552459717
Validation loss: 2.1444460499671196

Epoch: 5| Step: 2
Training loss: 2.055048942565918
Validation loss: 2.173653708991184

Epoch: 5| Step: 3
Training loss: 2.5619614124298096
Validation loss: 2.191144815055273

Epoch: 5| Step: 4
Training loss: 1.6139400005340576
Validation loss: 2.1670399378704768

Epoch: 5| Step: 5
Training loss: 2.2366740703582764
Validation loss: 2.196777307859031

Epoch: 5| Step: 6
Training loss: 2.6389167308807373
Validation loss: 2.1741732807569605

Epoch: 5| Step: 7
Training loss: 1.9615951776504517
Validation loss: 2.1608372836984615

Epoch: 5| Step: 8
Training loss: 2.043644428253174
Validation loss: 2.1751959554610716

Epoch: 5| Step: 9
Training loss: 2.5357720851898193
Validation loss: 2.1866341047389533

Epoch: 5| Step: 10
Training loss: 2.0840823650360107
Validation loss: 2.1467643399392404

Epoch: 107| Step: 0
Training loss: 1.5633409023284912
Validation loss: 2.1715556267769105

Epoch: 5| Step: 1
Training loss: 2.4483065605163574
Validation loss: 2.1457887208589943

Epoch: 5| Step: 2
Training loss: 1.9896141290664673
Validation loss: 2.176584361701883

Epoch: 5| Step: 3
Training loss: 1.9067814350128174
Validation loss: 2.167404285041235

Epoch: 5| Step: 4
Training loss: 2.083591938018799
Validation loss: 2.18265248498609

Epoch: 5| Step: 5
Training loss: 2.286792278289795
Validation loss: 2.1757914020169165

Epoch: 5| Step: 6
Training loss: 2.2084567546844482
Validation loss: 2.202391911578435

Epoch: 5| Step: 7
Training loss: 2.0898261070251465
Validation loss: 2.1414650909362303

Epoch: 5| Step: 8
Training loss: 2.3627915382385254
Validation loss: 2.1668731217743247

Epoch: 5| Step: 9
Training loss: 2.6284542083740234
Validation loss: 2.187108503874912

Epoch: 5| Step: 10
Training loss: 2.715373992919922
Validation loss: 2.1466605086480417

Epoch: 108| Step: 0
Training loss: 1.5230649709701538
Validation loss: 2.183763216900569

Epoch: 5| Step: 1
Training loss: 1.764992356300354
Validation loss: 2.144778154229605

Epoch: 5| Step: 2
Training loss: 2.637500286102295
Validation loss: 2.20326699877298

Epoch: 5| Step: 3
Training loss: 2.0853052139282227
Validation loss: 2.1516971998317267

Epoch: 5| Step: 4
Training loss: 2.6588993072509766
Validation loss: 2.2231241477433072

Epoch: 5| Step: 5
Training loss: 2.8680176734924316
Validation loss: 2.190744535897368

Epoch: 5| Step: 6
Training loss: 1.931433081626892
Validation loss: 2.1527802610910065

Epoch: 5| Step: 7
Training loss: 2.7895829677581787
Validation loss: 2.170431419085431

Epoch: 5| Step: 8
Training loss: 1.8462566137313843
Validation loss: 2.197450073816443

Epoch: 5| Step: 9
Training loss: 2.190904378890991
Validation loss: 2.1577682680981134

Epoch: 5| Step: 10
Training loss: 1.763356328010559
Validation loss: 2.1478039628715924

Epoch: 109| Step: 0
Training loss: 1.7263247966766357
Validation loss: 2.1529600825361026

Epoch: 5| Step: 1
Training loss: 2.3772666454315186
Validation loss: 2.197493378834058

Epoch: 5| Step: 2
Training loss: 2.232819080352783
Validation loss: 2.172152816608388

Epoch: 5| Step: 3
Training loss: 1.9303710460662842
Validation loss: 2.210871086325697

Epoch: 5| Step: 4
Training loss: 2.2086405754089355
Validation loss: 2.125034677085056

Epoch: 5| Step: 5
Training loss: 1.8562030792236328
Validation loss: 2.1517714787555

Epoch: 5| Step: 6
Training loss: 1.86257803440094
Validation loss: 2.1443306374293503

Epoch: 5| Step: 7
Training loss: 2.096233606338501
Validation loss: 2.180488150606873

Epoch: 5| Step: 8
Training loss: 2.850332736968994
Validation loss: 2.1801917450402373

Epoch: 5| Step: 9
Training loss: 2.5397098064422607
Validation loss: 2.1694822965129728

Epoch: 5| Step: 10
Training loss: 2.1976609230041504
Validation loss: 2.1760648296725367

Epoch: 110| Step: 0
Training loss: 1.8171775341033936
Validation loss: 2.1537007208793395

Epoch: 5| Step: 1
Training loss: 1.8865139484405518
Validation loss: 2.164411316635788

Epoch: 5| Step: 2
Training loss: 2.161604881286621
Validation loss: 2.1605578737874187

Epoch: 5| Step: 3
Training loss: 2.890364408493042
Validation loss: 2.170744865171371

Epoch: 5| Step: 4
Training loss: 1.6220073699951172
Validation loss: 2.1674827375719623

Epoch: 5| Step: 5
Training loss: 2.119096517562866
Validation loss: 2.1866403138765724

Epoch: 5| Step: 6
Training loss: 2.106107711791992
Validation loss: 2.1682305541089786

Epoch: 5| Step: 7
Training loss: 2.9988064765930176
Validation loss: 2.1697619704790014

Epoch: 5| Step: 8
Training loss: 2.356390953063965
Validation loss: 2.1852788258624334

Epoch: 5| Step: 9
Training loss: 1.900173544883728
Validation loss: 2.1504605252255677

Epoch: 5| Step: 10
Training loss: 2.285728693008423
Validation loss: 2.189829654591058

Epoch: 111| Step: 0
Training loss: 2.082249641418457
Validation loss: 2.201572877104564

Epoch: 5| Step: 1
Training loss: 2.42244291305542
Validation loss: 2.2070836277418238

Epoch: 5| Step: 2
Training loss: 2.189812660217285
Validation loss: 2.1947038096766316

Epoch: 5| Step: 3
Training loss: 1.7713381052017212
Validation loss: 2.1184102181465394

Epoch: 5| Step: 4
Training loss: 2.1187851428985596
Validation loss: 2.1484793539970153

Epoch: 5| Step: 5
Training loss: 2.3384203910827637
Validation loss: 2.171846535898024

Epoch: 5| Step: 6
Training loss: 2.384842872619629
Validation loss: 2.1706166523759083

Epoch: 5| Step: 7
Training loss: 2.2679054737091064
Validation loss: 2.216268988065822

Epoch: 5| Step: 8
Training loss: 1.9162870645523071
Validation loss: 2.15922652393259

Epoch: 5| Step: 9
Training loss: 2.655944347381592
Validation loss: 2.2022228394785235

Epoch: 5| Step: 10
Training loss: 2.0228071212768555
Validation loss: 2.1730518033427577

Epoch: 112| Step: 0
Training loss: 1.462289810180664
Validation loss: 2.1397997333157446

Epoch: 5| Step: 1
Training loss: 2.691664695739746
Validation loss: 2.163228655374178

Epoch: 5| Step: 2
Training loss: 2.1052358150482178
Validation loss: 2.174851761069349

Epoch: 5| Step: 3
Training loss: 2.049255132675171
Validation loss: 2.153736881030503

Epoch: 5| Step: 4
Training loss: 1.8665530681610107
Validation loss: 2.1782969056919055

Epoch: 5| Step: 5
Training loss: 2.501450300216675
Validation loss: 2.1622881581706386

Epoch: 5| Step: 6
Training loss: 2.338491678237915
Validation loss: 2.1542816521019064

Epoch: 5| Step: 7
Training loss: 2.583005428314209
Validation loss: 2.174924824827461

Epoch: 5| Step: 8
Training loss: 2.1826014518737793
Validation loss: 2.2025388748415056

Epoch: 5| Step: 9
Training loss: 2.2540488243103027
Validation loss: 2.16753884284727

Epoch: 5| Step: 10
Training loss: 2.2842774391174316
Validation loss: 2.188560685803813

Epoch: 113| Step: 0
Training loss: 2.4260094165802
Validation loss: 2.193908514515046

Epoch: 5| Step: 1
Training loss: 2.3067851066589355
Validation loss: 2.1791732208703154

Epoch: 5| Step: 2
Training loss: 2.6825575828552246
Validation loss: 2.1421454311698995

Epoch: 5| Step: 3
Training loss: 2.0218424797058105
Validation loss: 2.142051166103732

Epoch: 5| Step: 4
Training loss: 2.1766998767852783
Validation loss: 2.155987472944362

Epoch: 5| Step: 5
Training loss: 1.4053232669830322
Validation loss: 2.194414246466852

Epoch: 5| Step: 6
Training loss: 2.535959243774414
Validation loss: 2.1819547542961697

Epoch: 5| Step: 7
Training loss: 1.8860633373260498
Validation loss: 2.149217495354273

Epoch: 5| Step: 8
Training loss: 1.7439746856689453
Validation loss: 2.175748969918938

Epoch: 5| Step: 9
Training loss: 1.9354655742645264
Validation loss: 2.1971442494341122

Epoch: 5| Step: 10
Training loss: 3.1846141815185547
Validation loss: 2.198629076762866

Epoch: 114| Step: 0
Training loss: 2.611237049102783
Validation loss: 2.1925508604254773

Epoch: 5| Step: 1
Training loss: 2.133256435394287
Validation loss: 2.1536679126883067

Epoch: 5| Step: 2
Training loss: 1.4271042346954346
Validation loss: 2.2044190719563472

Epoch: 5| Step: 3
Training loss: 2.4350099563598633
Validation loss: 2.1711053066356207

Epoch: 5| Step: 4
Training loss: 1.8634555339813232
Validation loss: 2.174886662472961

Epoch: 5| Step: 5
Training loss: 2.964738607406616
Validation loss: 2.166689807368863

Epoch: 5| Step: 6
Training loss: 2.1723172664642334
Validation loss: 2.2108754496420584

Epoch: 5| Step: 7
Training loss: 2.6333677768707275
Validation loss: 2.1915304609524306

Epoch: 5| Step: 8
Training loss: 2.352295398712158
Validation loss: 2.2012084248245403

Epoch: 5| Step: 9
Training loss: 1.2820111513137817
Validation loss: 2.167036417991884

Epoch: 5| Step: 10
Training loss: 2.1215715408325195
Validation loss: 2.1456456171569003

Epoch: 115| Step: 0
Training loss: 1.8459762334823608
Validation loss: 2.1723301038947156

Epoch: 5| Step: 1
Training loss: 1.8382209539413452
Validation loss: 2.1644585491508566

Epoch: 5| Step: 2
Training loss: 2.221687078475952
Validation loss: 2.1906677753694597

Epoch: 5| Step: 3
Training loss: 2.460530996322632
Validation loss: 2.185965666206934

Epoch: 5| Step: 4
Training loss: 2.093519926071167
Validation loss: 2.1708963583874445

Epoch: 5| Step: 5
Training loss: 1.8252006769180298
Validation loss: 2.156523302037229

Epoch: 5| Step: 6
Training loss: 2.0731396675109863
Validation loss: 2.163910101818782

Epoch: 5| Step: 7
Training loss: 2.640604019165039
Validation loss: 2.1837585997837845

Epoch: 5| Step: 8
Training loss: 3.0464274883270264
Validation loss: 2.151001881527644

Epoch: 5| Step: 9
Training loss: 1.4009037017822266
Validation loss: 2.1684158412359094

Epoch: 5| Step: 10
Training loss: 2.4709489345550537
Validation loss: 2.17435029245192

Epoch: 116| Step: 0
Training loss: 2.85532808303833
Validation loss: 2.18938712663548

Epoch: 5| Step: 1
Training loss: 2.251086711883545
Validation loss: 2.1828206764754428

Epoch: 5| Step: 2
Training loss: 1.466723084449768
Validation loss: 2.208789563948108

Epoch: 5| Step: 3
Training loss: 2.73852276802063
Validation loss: 2.1827960091252483

Epoch: 5| Step: 4
Training loss: 2.124633550643921
Validation loss: 2.154159451043734

Epoch: 5| Step: 5
Training loss: 1.606745958328247
Validation loss: 2.1842022557412424

Epoch: 5| Step: 6
Training loss: 1.671121597290039
Validation loss: 2.1544335349913566

Epoch: 5| Step: 7
Training loss: 2.3310141563415527
Validation loss: 2.2002233305285053

Epoch: 5| Step: 8
Training loss: 2.2580761909484863
Validation loss: 2.1599163188729236

Epoch: 5| Step: 9
Training loss: 2.6652822494506836
Validation loss: 2.227871787163519

Epoch: 5| Step: 10
Training loss: 2.106518507003784
Validation loss: 2.177519395787229

Epoch: 117| Step: 0
Training loss: 1.9373279809951782
Validation loss: 2.161612738845169

Epoch: 5| Step: 1
Training loss: 2.6125073432922363
Validation loss: 2.1681073250309115

Epoch: 5| Step: 2
Training loss: 1.9086863994598389
Validation loss: 2.1948487553545224

Epoch: 5| Step: 3
Training loss: 2.4013400077819824
Validation loss: 2.1599533275891374

Epoch: 5| Step: 4
Training loss: 2.233562469482422
Validation loss: 2.18351399770347

Epoch: 5| Step: 5
Training loss: 1.9013872146606445
Validation loss: 2.1536335740038144

Epoch: 5| Step: 6
Training loss: 1.8951785564422607
Validation loss: 2.216879592146925

Epoch: 5| Step: 7
Training loss: 2.601261615753174
Validation loss: 2.176045379331035

Epoch: 5| Step: 8
Training loss: 2.2358574867248535
Validation loss: 2.1912039338901477

Epoch: 5| Step: 9
Training loss: 2.4223122596740723
Validation loss: 2.213736585391465

Epoch: 5| Step: 10
Training loss: 1.977617621421814
Validation loss: 2.167090421081871

Epoch: 118| Step: 0
Training loss: 2.0216498374938965
Validation loss: 2.1958541011297577

Epoch: 5| Step: 1
Training loss: 2.45127272605896
Validation loss: 2.1832316460147982

Epoch: 5| Step: 2
Training loss: 2.2399449348449707
Validation loss: 2.1850592782420497

Epoch: 5| Step: 3
Training loss: 2.243584632873535
Validation loss: 2.1807137714919222

Epoch: 5| Step: 4
Training loss: 2.8228111267089844
Validation loss: 2.142886472004716

Epoch: 5| Step: 5
Training loss: 2.0402462482452393
Validation loss: 2.1640998483985983

Epoch: 5| Step: 6
Training loss: 1.9763514995574951
Validation loss: 2.1689464315291374

Epoch: 5| Step: 7
Training loss: 2.603431224822998
Validation loss: 2.1410654975521948

Epoch: 5| Step: 8
Training loss: 1.787023901939392
Validation loss: 2.144276019065611

Epoch: 5| Step: 9
Training loss: 2.47639799118042
Validation loss: 2.154259584283316

Epoch: 5| Step: 10
Training loss: 1.1020891666412354
Validation loss: 2.1655208321027857

Epoch: 119| Step: 0
Training loss: 2.4349286556243896
Validation loss: 2.173062306578441

Epoch: 5| Step: 1
Training loss: 2.6293883323669434
Validation loss: 2.177838420355192

Epoch: 5| Step: 2
Training loss: 2.2500760555267334
Validation loss: 2.170299571047547

Epoch: 5| Step: 3
Training loss: 2.3530173301696777
Validation loss: 2.137202262878418

Epoch: 5| Step: 4
Training loss: 2.1340034008026123
Validation loss: 2.1496286366575506

Epoch: 5| Step: 5
Training loss: 2.0057313442230225
Validation loss: 2.155003627141317

Epoch: 5| Step: 6
Training loss: 2.1254265308380127
Validation loss: 2.1625444119976414

Epoch: 5| Step: 7
Training loss: 2.166673183441162
Validation loss: 2.1424352007527507

Epoch: 5| Step: 8
Training loss: 1.9416210651397705
Validation loss: 2.139866530254323

Epoch: 5| Step: 9
Training loss: 1.899937391281128
Validation loss: 2.1792927121603363

Epoch: 5| Step: 10
Training loss: 2.2026634216308594
Validation loss: 2.162623954075639

Epoch: 120| Step: 0
Training loss: 2.6808347702026367
Validation loss: 2.181635074718024

Epoch: 5| Step: 1
Training loss: 2.7234551906585693
Validation loss: 2.152911660491779

Epoch: 5| Step: 2
Training loss: 2.174635410308838
Validation loss: 2.1733823130207677

Epoch: 5| Step: 3
Training loss: 2.4500253200531006
Validation loss: 2.15469108089324

Epoch: 5| Step: 4
Training loss: 2.2773566246032715
Validation loss: 2.149381124845115

Epoch: 5| Step: 5
Training loss: 2.231062412261963
Validation loss: 2.1423024105769333

Epoch: 5| Step: 6
Training loss: 1.949960470199585
Validation loss: 2.181493656609648

Epoch: 5| Step: 7
Training loss: 2.271040678024292
Validation loss: 2.169315068952499

Epoch: 5| Step: 8
Training loss: 1.445712685585022
Validation loss: 2.181935424445778

Epoch: 5| Step: 9
Training loss: 1.8390929698944092
Validation loss: 2.19128083157283

Epoch: 5| Step: 10
Training loss: 1.5230202674865723
Validation loss: 2.145015449934108

Epoch: 121| Step: 0
Training loss: 2.335239887237549
Validation loss: 2.1832406700298352

Epoch: 5| Step: 1
Training loss: 2.3878002166748047
Validation loss: 2.1714788303580335

Epoch: 5| Step: 2
Training loss: 2.2590255737304688
Validation loss: 2.1495002392799623

Epoch: 5| Step: 3
Training loss: 2.6420063972473145
Validation loss: 2.1945558453118927

Epoch: 5| Step: 4
Training loss: 1.5794428586959839
Validation loss: 2.1248001719033844

Epoch: 5| Step: 5
Training loss: 2.6547045707702637
Validation loss: 2.2038231101087344

Epoch: 5| Step: 6
Training loss: 2.1796226501464844
Validation loss: 2.137664867985633

Epoch: 5| Step: 7
Training loss: 1.6969633102416992
Validation loss: 2.15324071914919

Epoch: 5| Step: 8
Training loss: 2.351588010787964
Validation loss: 2.124266953878505

Epoch: 5| Step: 9
Training loss: 1.7229543924331665
Validation loss: 2.1628481905947448

Epoch: 5| Step: 10
Training loss: 1.7881159782409668
Validation loss: 2.147707990420762

Epoch: 122| Step: 0
Training loss: 3.4122302532196045
Validation loss: 2.1457641252907376

Epoch: 5| Step: 1
Training loss: 3.110194683074951
Validation loss: 2.1481022270776893

Epoch: 5| Step: 2
Training loss: 1.6320230960845947
Validation loss: 2.136816123480438

Epoch: 5| Step: 3
Training loss: 1.8808997869491577
Validation loss: 2.1635379560532106

Epoch: 5| Step: 4
Training loss: 1.7986177206039429
Validation loss: 2.1740302116640153

Epoch: 5| Step: 5
Training loss: 1.517329454421997
Validation loss: 2.167542921599521

Epoch: 5| Step: 6
Training loss: 2.2429728507995605
Validation loss: 2.1507247724840717

Epoch: 5| Step: 7
Training loss: 1.9724286794662476
Validation loss: 2.172163745408417

Epoch: 5| Step: 8
Training loss: 1.9735603332519531
Validation loss: 2.1517669206024497

Epoch: 5| Step: 9
Training loss: 1.9939842224121094
Validation loss: 2.188314184065788

Epoch: 5| Step: 10
Training loss: 2.226261615753174
Validation loss: 2.193162020816598

Epoch: 123| Step: 0
Training loss: 1.920060157775879
Validation loss: 2.157601571852161

Epoch: 5| Step: 1
Training loss: 2.129873752593994
Validation loss: 2.16911316943425

Epoch: 5| Step: 2
Training loss: 2.4713406562805176
Validation loss: 2.1540410339191394

Epoch: 5| Step: 3
Training loss: 2.3105266094207764
Validation loss: 2.176851628929056

Epoch: 5| Step: 4
Training loss: 1.7208659648895264
Validation loss: 2.1521402994791665

Epoch: 5| Step: 5
Training loss: 2.510552167892456
Validation loss: 2.165359212506202

Epoch: 5| Step: 6
Training loss: 2.2036895751953125
Validation loss: 2.210528658282372

Epoch: 5| Step: 7
Training loss: 2.272261381149292
Validation loss: 2.142769628955472

Epoch: 5| Step: 8
Training loss: 1.879604697227478
Validation loss: 2.2010399244164907

Epoch: 5| Step: 9
Training loss: 2.2130978107452393
Validation loss: 2.213780865874342

Epoch: 5| Step: 10
Training loss: 2.401845693588257
Validation loss: 2.133957862854004

Epoch: 124| Step: 0
Training loss: 1.94827139377594
Validation loss: 2.182551758263701

Epoch: 5| Step: 1
Training loss: 3.0705161094665527
Validation loss: 2.157619892909963

Epoch: 5| Step: 2
Training loss: 1.9410988092422485
Validation loss: 2.1625639571938464

Epoch: 5| Step: 3
Training loss: 2.2339797019958496
Validation loss: 2.1468554850547545

Epoch: 5| Step: 4
Training loss: 2.018115758895874
Validation loss: 2.181104975361978

Epoch: 5| Step: 5
Training loss: 2.074195384979248
Validation loss: 2.161970538477744

Epoch: 5| Step: 6
Training loss: 1.8236192464828491
Validation loss: 2.192262336771975

Epoch: 5| Step: 7
Training loss: 2.339684009552002
Validation loss: 2.183658835708454

Epoch: 5| Step: 8
Training loss: 2.4065375328063965
Validation loss: 2.196171586231519

Epoch: 5| Step: 9
Training loss: 2.1514499187469482
Validation loss: 2.189487544439172

Epoch: 5| Step: 10
Training loss: 1.8748584985733032
Validation loss: 2.1527431549564486

Epoch: 125| Step: 0
Training loss: 2.5416340827941895
Validation loss: 2.1441079724219536

Epoch: 5| Step: 1
Training loss: 2.565812349319458
Validation loss: 2.1865070686545423

Epoch: 5| Step: 2
Training loss: 2.195793628692627
Validation loss: 2.1924256765714256

Epoch: 5| Step: 3
Training loss: 2.079214572906494
Validation loss: 2.147205345092281

Epoch: 5| Step: 4
Training loss: 1.7367607355117798
Validation loss: 2.1543369575213362

Epoch: 5| Step: 5
Training loss: 2.8131301403045654
Validation loss: 2.186412657460859

Epoch: 5| Step: 6
Training loss: 1.5931885242462158
Validation loss: 2.1562801971230456

Epoch: 5| Step: 7
Training loss: 1.9493707418441772
Validation loss: 2.177430711766725

Epoch: 5| Step: 8
Training loss: 1.6740407943725586
Validation loss: 2.177109924695825

Epoch: 5| Step: 9
Training loss: 2.6660799980163574
Validation loss: 2.1826511493293186

Epoch: 5| Step: 10
Training loss: 2.115631580352783
Validation loss: 2.203264260804781

Testing loss: 2.0855992105272083

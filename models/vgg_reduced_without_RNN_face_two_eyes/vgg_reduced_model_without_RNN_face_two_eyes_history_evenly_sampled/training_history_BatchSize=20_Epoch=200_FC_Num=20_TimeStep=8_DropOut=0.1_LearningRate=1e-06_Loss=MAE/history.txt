Epoch: 1| Step: 0
Training loss: 6.158483982086182
Validation loss: 6.436035597196189

Epoch: 5| Step: 1
Training loss: 6.443994998931885
Validation loss: 6.429241903366581

Epoch: 5| Step: 2
Training loss: 5.588059425354004
Validation loss: 6.420212268829346

Epoch: 5| Step: 3
Training loss: 6.208584785461426
Validation loss: 6.415723046948833

Epoch: 5| Step: 4
Training loss: 6.6448469161987305
Validation loss: 6.408962060046452

Epoch: 5| Step: 5
Training loss: 6.043726444244385
Validation loss: 6.399559579869752

Epoch: 5| Step: 6
Training loss: 6.886618614196777
Validation loss: 6.394149334199967

Epoch: 5| Step: 7
Training loss: 6.896790981292725
Validation loss: 6.383802542122462

Epoch: 5| Step: 8
Training loss: 5.592437744140625
Validation loss: 6.376922069057342

Epoch: 5| Step: 9
Training loss: 5.389231204986572
Validation loss: 6.369292172052527

Epoch: 5| Step: 10
Training loss: 6.344783306121826
Validation loss: 6.364499538175521

Epoch: 2| Step: 0
Training loss: 5.980656623840332
Validation loss: 6.358430077952724

Epoch: 5| Step: 1
Training loss: 6.524084568023682
Validation loss: 6.346692485194052

Epoch: 5| Step: 2
Training loss: 6.5567169189453125
Validation loss: 6.341911290281562

Epoch: 5| Step: 3
Training loss: 6.2602739334106445
Validation loss: 6.335138500377696

Epoch: 5| Step: 4
Training loss: 5.672735691070557
Validation loss: 6.328060873093143

Epoch: 5| Step: 5
Training loss: 6.081813812255859
Validation loss: 6.321289975156066

Epoch: 5| Step: 6
Training loss: 5.917603015899658
Validation loss: 6.313666328307121

Epoch: 5| Step: 7
Training loss: 6.07450008392334
Validation loss: 6.30606948175738

Epoch: 5| Step: 8
Training loss: 5.509551048278809
Validation loss: 6.298713776373094

Epoch: 5| Step: 9
Training loss: 6.019268989562988
Validation loss: 6.292166668881652

Epoch: 5| Step: 10
Training loss: 6.785553932189941
Validation loss: 6.285270526844968

Epoch: 3| Step: 0
Training loss: 5.83971643447876
Validation loss: 6.275648840012089

Epoch: 5| Step: 1
Training loss: 6.326486110687256
Validation loss: 6.268981123483309

Epoch: 5| Step: 2
Training loss: 6.470980167388916
Validation loss: 6.264328684858096

Epoch: 5| Step: 3
Training loss: 6.492464542388916
Validation loss: 6.254386799309843

Epoch: 5| Step: 4
Training loss: 6.901059150695801
Validation loss: 6.249502699862244

Epoch: 5| Step: 5
Training loss: 4.844555854797363
Validation loss: 6.239933362571142

Epoch: 5| Step: 6
Training loss: 6.040079116821289
Validation loss: 6.232748205943774

Epoch: 5| Step: 7
Training loss: 6.74862003326416
Validation loss: 6.226229421554073

Epoch: 5| Step: 8
Training loss: 5.731678485870361
Validation loss: 6.219742180198751

Epoch: 5| Step: 9
Training loss: 5.671947479248047
Validation loss: 6.212026503778273

Epoch: 5| Step: 10
Training loss: 5.175686836242676
Validation loss: 6.202535598508773

Epoch: 4| Step: 0
Training loss: 5.425778865814209
Validation loss: 6.196654217217558

Epoch: 5| Step: 1
Training loss: 5.990805149078369
Validation loss: 6.188670404495731

Epoch: 5| Step: 2
Training loss: 5.949806213378906
Validation loss: 6.181106131563904

Epoch: 5| Step: 3
Training loss: 5.101346015930176
Validation loss: 6.173059607064852

Epoch: 5| Step: 4
Training loss: 4.874518394470215
Validation loss: 6.166297410124091

Epoch: 5| Step: 5
Training loss: 5.6031999588012695
Validation loss: 6.156861033490909

Epoch: 5| Step: 6
Training loss: 6.273684501647949
Validation loss: 6.151660401334045

Epoch: 5| Step: 7
Training loss: 7.566054344177246
Validation loss: 6.142862863438104

Epoch: 5| Step: 8
Training loss: 6.4220685958862305
Validation loss: 6.133735954120595

Epoch: 5| Step: 9
Training loss: 5.987106800079346
Validation loss: 6.125682887210641

Epoch: 5| Step: 10
Training loss: 6.323779106140137
Validation loss: 6.11802287255564

Epoch: 5| Step: 0
Training loss: 6.495022773742676
Validation loss: 6.112303200588431

Epoch: 5| Step: 1
Training loss: 6.371984004974365
Validation loss: 6.103314338191863

Epoch: 5| Step: 2
Training loss: 5.343257427215576
Validation loss: 6.096180438995361

Epoch: 5| Step: 3
Training loss: 5.2560553550720215
Validation loss: 6.08671018128754

Epoch: 5| Step: 4
Training loss: 6.167871952056885
Validation loss: 6.079003062299503

Epoch: 5| Step: 5
Training loss: 6.238797187805176
Validation loss: 6.071102065424765

Epoch: 5| Step: 6
Training loss: 6.856775760650635
Validation loss: 6.061432433384721

Epoch: 5| Step: 7
Training loss: 6.158688545227051
Validation loss: 6.055678147141651

Epoch: 5| Step: 8
Training loss: 4.9125471115112305
Validation loss: 6.044052144532563

Epoch: 5| Step: 9
Training loss: 5.335474967956543
Validation loss: 6.035484083237186

Epoch: 5| Step: 10
Training loss: 5.202584743499756
Validation loss: 6.025649383503904

Epoch: 6| Step: 0
Training loss: 5.940281391143799
Validation loss: 6.015925715046544

Epoch: 5| Step: 1
Training loss: 5.509552001953125
Validation loss: 6.010948278570688

Epoch: 5| Step: 2
Training loss: 6.983518123626709
Validation loss: 6.001867530166462

Epoch: 5| Step: 3
Training loss: 4.625970363616943
Validation loss: 5.99160066727669

Epoch: 5| Step: 4
Training loss: 5.683236122131348
Validation loss: 5.9795074462890625

Epoch: 5| Step: 5
Training loss: 5.6516923904418945
Validation loss: 5.9746700922648115

Epoch: 5| Step: 6
Training loss: 7.044807434082031
Validation loss: 5.962737170598841

Epoch: 5| Step: 7
Training loss: 4.813053131103516
Validation loss: 5.954684488234982

Epoch: 5| Step: 8
Training loss: 7.083447456359863
Validation loss: 5.943792763576712

Epoch: 5| Step: 9
Training loss: 4.941788673400879
Validation loss: 5.933351998688073

Epoch: 5| Step: 10
Training loss: 4.976900100708008
Validation loss: 5.920234951921689

Epoch: 7| Step: 0
Training loss: 4.687716484069824
Validation loss: 5.910983116396012

Epoch: 5| Step: 1
Training loss: 6.695777893066406
Validation loss: 5.902689077520884

Epoch: 5| Step: 2
Training loss: 4.917150497436523
Validation loss: 5.891306779717886

Epoch: 5| Step: 3
Training loss: 4.749066352844238
Validation loss: 5.877361456553142

Epoch: 5| Step: 4
Training loss: 5.372995376586914
Validation loss: 5.866964868319932

Epoch: 5| Step: 5
Training loss: 6.04995059967041
Validation loss: 5.8548894748892835

Epoch: 5| Step: 6
Training loss: 7.226229667663574
Validation loss: 5.84715296119772

Epoch: 5| Step: 7
Training loss: 5.5277299880981445
Validation loss: 5.83377891971219

Epoch: 5| Step: 8
Training loss: 5.521976947784424
Validation loss: 5.822583875348491

Epoch: 5| Step: 9
Training loss: 5.47824239730835
Validation loss: 5.8097921494514715

Epoch: 5| Step: 10
Training loss: 5.8354949951171875
Validation loss: 5.794644366028488

Epoch: 8| Step: 0
Training loss: 4.415159702301025
Validation loss: 5.782710752179546

Epoch: 5| Step: 1
Training loss: 5.508917808532715
Validation loss: 5.767524832038469

Epoch: 5| Step: 2
Training loss: 5.77765417098999
Validation loss: 5.752800618448565

Epoch: 5| Step: 3
Training loss: 6.087050437927246
Validation loss: 5.744137384558237

Epoch: 5| Step: 4
Training loss: 6.491188049316406
Validation loss: 5.728325828429191

Epoch: 5| Step: 5
Training loss: 3.4501140117645264
Validation loss: 5.714104780586817

Epoch: 5| Step: 6
Training loss: 5.412198066711426
Validation loss: 5.704279063850321

Epoch: 5| Step: 7
Training loss: 5.998650550842285
Validation loss: 5.685680589368267

Epoch: 5| Step: 8
Training loss: 5.351294040679932
Validation loss: 5.669584520401493

Epoch: 5| Step: 9
Training loss: 6.0850019454956055
Validation loss: 5.653761350980369

Epoch: 5| Step: 10
Training loss: 5.918757438659668
Validation loss: 5.638640716511716

Epoch: 9| Step: 0
Training loss: 4.983013153076172
Validation loss: 5.622970596436532

Epoch: 5| Step: 1
Training loss: 4.117140769958496
Validation loss: 5.608854052841022

Epoch: 5| Step: 2
Training loss: 5.7839508056640625
Validation loss: 5.59311762163716

Epoch: 5| Step: 3
Training loss: 5.700082778930664
Validation loss: 5.579153009640273

Epoch: 5| Step: 4
Training loss: 5.727522850036621
Validation loss: 5.566228866577148

Epoch: 5| Step: 5
Training loss: 4.969791889190674
Validation loss: 5.548064380563716

Epoch: 5| Step: 6
Training loss: 5.54119873046875
Validation loss: 5.535517559256605

Epoch: 5| Step: 7
Training loss: 4.9491472244262695
Validation loss: 5.513443787892659

Epoch: 5| Step: 8
Training loss: 5.6952338218688965
Validation loss: 5.495993870560841

Epoch: 5| Step: 9
Training loss: 4.938050746917725
Validation loss: 5.484257236603768

Epoch: 5| Step: 10
Training loss: 6.316751003265381
Validation loss: 5.468658483156594

Epoch: 10| Step: 0
Training loss: 4.973834991455078
Validation loss: 5.450967886114634

Epoch: 5| Step: 1
Training loss: 5.379504203796387
Validation loss: 5.433212459728282

Epoch: 5| Step: 2
Training loss: 5.118300437927246
Validation loss: 5.419299448690107

Epoch: 5| Step: 3
Training loss: 5.695068359375
Validation loss: 5.395843141822405

Epoch: 5| Step: 4
Training loss: 5.29683780670166
Validation loss: 5.375244596953033

Epoch: 5| Step: 5
Training loss: 5.561567783355713
Validation loss: 5.3561066760811755

Epoch: 5| Step: 6
Training loss: 5.3562092781066895
Validation loss: 5.344310837407266

Epoch: 5| Step: 7
Training loss: 4.839691162109375
Validation loss: 5.322903817699801

Epoch: 5| Step: 8
Training loss: 4.582810401916504
Validation loss: 5.307147395226263

Epoch: 5| Step: 9
Training loss: 5.0819549560546875
Validation loss: 5.285628980205905

Epoch: 5| Step: 10
Training loss: 4.463948726654053
Validation loss: 5.266891115455217

Epoch: 11| Step: 0
Training loss: 4.228605270385742
Validation loss: 5.249978188545473

Epoch: 5| Step: 1
Training loss: 5.200681209564209
Validation loss: 5.227105222722535

Epoch: 5| Step: 2
Training loss: 6.205832004547119
Validation loss: 5.2079090302990325

Epoch: 5| Step: 3
Training loss: 3.5768775939941406
Validation loss: 5.185649533425608

Epoch: 5| Step: 4
Training loss: 5.195908546447754
Validation loss: 5.171412529483918

Epoch: 5| Step: 5
Training loss: 4.9945783615112305
Validation loss: 5.145362500221498

Epoch: 5| Step: 6
Training loss: 4.916407108306885
Validation loss: 5.1320468584696455

Epoch: 5| Step: 7
Training loss: 5.339018821716309
Validation loss: 5.113216800074423

Epoch: 5| Step: 8
Training loss: 4.7206854820251465
Validation loss: 5.082267874030657

Epoch: 5| Step: 9
Training loss: 4.71066427230835
Validation loss: 5.062896154260122

Epoch: 5| Step: 10
Training loss: 5.022264003753662
Validation loss: 5.035721886542536

Epoch: 12| Step: 0
Training loss: 4.37979793548584
Validation loss: 5.019775436770532

Epoch: 5| Step: 1
Training loss: 4.4443488121032715
Validation loss: 4.993803552401963

Epoch: 5| Step: 2
Training loss: 4.194403648376465
Validation loss: 4.974296759533626

Epoch: 5| Step: 3
Training loss: 4.746649742126465
Validation loss: 4.950957272642402

Epoch: 5| Step: 4
Training loss: 4.329684734344482
Validation loss: 4.935561615933654

Epoch: 5| Step: 5
Training loss: 4.375791549682617
Validation loss: 4.909796791691934

Epoch: 5| Step: 6
Training loss: 4.814057350158691
Validation loss: 4.892958533379339

Epoch: 5| Step: 7
Training loss: 4.708479881286621
Validation loss: 4.868821395340786

Epoch: 5| Step: 8
Training loss: 5.264327049255371
Validation loss: 4.8415058197513705

Epoch: 5| Step: 9
Training loss: 5.102686882019043
Validation loss: 4.8192338943481445

Epoch: 5| Step: 10
Training loss: 5.122511863708496
Validation loss: 4.792683950034521

Epoch: 13| Step: 0
Training loss: 5.53342342376709
Validation loss: 4.76753951657203

Epoch: 5| Step: 1
Training loss: 3.675320863723755
Validation loss: 4.743458060808079

Epoch: 5| Step: 2
Training loss: 3.818451404571533
Validation loss: 4.721098546058901

Epoch: 5| Step: 3
Training loss: 4.4451212882995605
Validation loss: 4.690697987874349

Epoch: 5| Step: 4
Training loss: 5.660536766052246
Validation loss: 4.677730990994361

Epoch: 5| Step: 5
Training loss: 3.3865203857421875
Validation loss: 4.648883204306325

Epoch: 5| Step: 6
Training loss: 3.8508238792419434
Validation loss: 4.623348728302987

Epoch: 5| Step: 7
Training loss: 4.223352909088135
Validation loss: 4.602248258488153

Epoch: 5| Step: 8
Training loss: 4.932486057281494
Validation loss: 4.572289600167223

Epoch: 5| Step: 9
Training loss: 4.282841205596924
Validation loss: 4.54843306797807

Epoch: 5| Step: 10
Training loss: 4.7271037101745605
Validation loss: 4.52591493052821

Epoch: 14| Step: 0
Training loss: 5.676884651184082
Validation loss: 4.497021208527268

Epoch: 5| Step: 1
Training loss: 4.009223461151123
Validation loss: 4.4753631776379

Epoch: 5| Step: 2
Training loss: 4.431045055389404
Validation loss: 4.439728711241035

Epoch: 5| Step: 3
Training loss: 5.016911506652832
Validation loss: 4.408959650224255

Epoch: 5| Step: 4
Training loss: 4.6155619621276855
Validation loss: 4.384657408601495

Epoch: 5| Step: 5
Training loss: 3.9394423961639404
Validation loss: 4.3604055425172215

Epoch: 5| Step: 6
Training loss: 2.6220569610595703
Validation loss: 4.333321555968253

Epoch: 5| Step: 7
Training loss: 4.275347709655762
Validation loss: 4.301418837680612

Epoch: 5| Step: 8
Training loss: 3.2867233753204346
Validation loss: 4.264724418681155

Epoch: 5| Step: 9
Training loss: 3.886530637741089
Validation loss: 4.247829396237609

Epoch: 5| Step: 10
Training loss: 3.637244701385498
Validation loss: 4.218250884804674

Epoch: 15| Step: 0
Training loss: 4.438925266265869
Validation loss: 4.18739184512887

Epoch: 5| Step: 1
Training loss: 3.4522979259490967
Validation loss: 4.166435236571937

Epoch: 5| Step: 2
Training loss: 4.059612274169922
Validation loss: 4.142254608933643

Epoch: 5| Step: 3
Training loss: 3.080409526824951
Validation loss: 4.1189962715230966

Epoch: 5| Step: 4
Training loss: 3.4806809425354004
Validation loss: 4.079458513567524

Epoch: 5| Step: 5
Training loss: 4.296633720397949
Validation loss: 4.051302899596512

Epoch: 5| Step: 6
Training loss: 4.254274368286133
Validation loss: 4.033864231519802

Epoch: 5| Step: 7
Training loss: 4.648717403411865
Validation loss: 3.9992958960994596

Epoch: 5| Step: 8
Training loss: 3.6161351203918457
Validation loss: 3.9806505839029946

Epoch: 5| Step: 9
Training loss: 3.763476610183716
Validation loss: 3.951231684736026

Epoch: 5| Step: 10
Training loss: 3.1168789863586426
Validation loss: 3.9184103678631526

Epoch: 16| Step: 0
Training loss: 4.608664512634277
Validation loss: 3.8909420018555014

Epoch: 5| Step: 1
Training loss: 3.0394692420959473
Validation loss: 3.860987268468385

Epoch: 5| Step: 2
Training loss: 3.5281879901885986
Validation loss: 3.8234713103181575

Epoch: 5| Step: 3
Training loss: 2.887190341949463
Validation loss: 3.801386874209168

Epoch: 5| Step: 4
Training loss: 3.110827922821045
Validation loss: 3.768915912156464

Epoch: 5| Step: 5
Training loss: 3.38942289352417
Validation loss: 3.748212363130303

Epoch: 5| Step: 6
Training loss: 4.607671737670898
Validation loss: 3.7145635825331493

Epoch: 5| Step: 7
Training loss: 4.309499263763428
Validation loss: 3.702703211897163

Epoch: 5| Step: 8
Training loss: 3.393890380859375
Validation loss: 3.670167174390567

Epoch: 5| Step: 9
Training loss: 3.029508590698242
Validation loss: 3.642562896974625

Epoch: 5| Step: 10
Training loss: 3.457339286804199
Validation loss: 3.6138073039311234

Epoch: 17| Step: 0
Training loss: 2.891894578933716
Validation loss: 3.5948574286635204

Epoch: 5| Step: 1
Training loss: 4.2979841232299805
Validation loss: 3.560268912264096

Epoch: 5| Step: 2
Training loss: 2.6820030212402344
Validation loss: 3.539849591511552

Epoch: 5| Step: 3
Training loss: 3.779726505279541
Validation loss: 3.5063302542573664

Epoch: 5| Step: 4
Training loss: 4.0240631103515625
Validation loss: 3.4772463485758793

Epoch: 5| Step: 5
Training loss: 3.1937079429626465
Validation loss: 3.4599185528293734

Epoch: 5| Step: 6
Training loss: 2.572195053100586
Validation loss: 3.4243480569572857

Epoch: 5| Step: 7
Training loss: 2.702126979827881
Validation loss: 3.3971429870974634

Epoch: 5| Step: 8
Training loss: 3.4136109352111816
Validation loss: 3.373878417476531

Epoch: 5| Step: 9
Training loss: 3.4453914165496826
Validation loss: 3.3603128387082006

Epoch: 5| Step: 10
Training loss: 3.6614553928375244
Validation loss: 3.319879326769101

Epoch: 18| Step: 0
Training loss: 3.718895673751831
Validation loss: 3.3022091721975677

Epoch: 5| Step: 1
Training loss: 2.7914092540740967
Validation loss: 3.26584166865195

Epoch: 5| Step: 2
Training loss: 2.970698356628418
Validation loss: 3.2477864091114332

Epoch: 5| Step: 3
Training loss: 3.3725719451904297
Validation loss: 3.2228648611294326

Epoch: 5| Step: 4
Training loss: 2.6398708820343018
Validation loss: 3.192078757029708

Epoch: 5| Step: 5
Training loss: 3.378932237625122
Validation loss: 3.175096796404931

Epoch: 5| Step: 6
Training loss: 2.6888556480407715
Validation loss: 3.143915848065448

Epoch: 5| Step: 7
Training loss: 2.993072986602783
Validation loss: 3.121315566442346

Epoch: 5| Step: 8
Training loss: 4.310502529144287
Validation loss: 3.093230229552074

Epoch: 5| Step: 9
Training loss: 2.6932711601257324
Validation loss: 3.074444350375924

Epoch: 5| Step: 10
Training loss: 2.653165340423584
Validation loss: 3.0496118940332884

Epoch: 19| Step: 0
Training loss: 3.028000593185425
Validation loss: 3.025490476239112

Epoch: 5| Step: 1
Training loss: 3.2130470275878906
Validation loss: 3.010320812143305

Epoch: 5| Step: 2
Training loss: 3.642063617706299
Validation loss: 2.9819106337844685

Epoch: 5| Step: 3
Training loss: 3.0969319343566895
Validation loss: 2.971428373808502

Epoch: 5| Step: 4
Training loss: 2.3718509674072266
Validation loss: 2.94617824656989

Epoch: 5| Step: 5
Training loss: 2.7058475017547607
Validation loss: 2.925602877011863

Epoch: 5| Step: 6
Training loss: 3.0059494972229004
Validation loss: 2.89990988854439

Epoch: 5| Step: 7
Training loss: 3.1575207710266113
Validation loss: 2.8818914403197584

Epoch: 5| Step: 8
Training loss: 2.6857481002807617
Validation loss: 2.857786024770429

Epoch: 5| Step: 9
Training loss: 2.770596504211426
Validation loss: 2.8331298776852187

Epoch: 5| Step: 10
Training loss: 2.904237747192383
Validation loss: 2.8089060629567792

Epoch: 20| Step: 0
Training loss: 3.178908109664917
Validation loss: 2.7961494051000124

Epoch: 5| Step: 1
Training loss: 2.432267427444458
Validation loss: 2.7761757399446223

Epoch: 5| Step: 2
Training loss: 3.489227294921875
Validation loss: 2.7675121189445577

Epoch: 5| Step: 3
Training loss: 2.6374497413635254
Validation loss: 2.7455014003220426

Epoch: 5| Step: 4
Training loss: 3.0259881019592285
Validation loss: 2.7096116106997252

Epoch: 5| Step: 5
Training loss: 2.7055439949035645
Validation loss: 2.6935863212872575

Epoch: 5| Step: 6
Training loss: 2.9209682941436768
Validation loss: 2.683185041591685

Epoch: 5| Step: 7
Training loss: 2.4766907691955566
Validation loss: 2.672299288934277

Epoch: 5| Step: 8
Training loss: 2.6228606700897217
Validation loss: 2.655944170490388

Epoch: 5| Step: 9
Training loss: 2.1443402767181396
Validation loss: 2.611027456098987

Epoch: 5| Step: 10
Training loss: 3.3062305450439453
Validation loss: 2.6325894068646174

Epoch: 21| Step: 0
Training loss: 2.68696928024292
Validation loss: 2.597727883246637

Epoch: 5| Step: 1
Training loss: 2.0546867847442627
Validation loss: 2.58404274909727

Epoch: 5| Step: 2
Training loss: 2.7324557304382324
Validation loss: 2.5783522718696186

Epoch: 5| Step: 3
Training loss: 3.2647042274475098
Validation loss: 2.5793995241965018

Epoch: 5| Step: 4
Training loss: 3.4285950660705566
Validation loss: 2.545684519634452

Epoch: 5| Step: 5
Training loss: 2.4114208221435547
Validation loss: 2.5421580447945544

Epoch: 5| Step: 6
Training loss: 1.9816516637802124
Validation loss: 2.5265112692309963

Epoch: 5| Step: 7
Training loss: 2.8060922622680664
Validation loss: 2.5062889668249313

Epoch: 5| Step: 8
Training loss: 2.888866424560547
Validation loss: 2.509859772138698

Epoch: 5| Step: 9
Training loss: 2.920004367828369
Validation loss: 2.4879674347498084

Epoch: 5| Step: 10
Training loss: 2.175429344177246
Validation loss: 2.4928971593097975

Epoch: 22| Step: 0
Training loss: 2.4353268146514893
Validation loss: 2.4689041670932563

Epoch: 5| Step: 1
Training loss: 3.1663291454315186
Validation loss: 2.4584631432769117

Epoch: 5| Step: 2
Training loss: 1.862238883972168
Validation loss: 2.458843731111096

Epoch: 5| Step: 3
Training loss: 2.758646011352539
Validation loss: 2.439626734743836

Epoch: 5| Step: 4
Training loss: 3.01448392868042
Validation loss: 2.445140997568766

Epoch: 5| Step: 5
Training loss: 2.240280866622925
Validation loss: 2.441206221939415

Epoch: 5| Step: 6
Training loss: 2.966329336166382
Validation loss: 2.420396891973352

Epoch: 5| Step: 7
Training loss: 1.8769975900650024
Validation loss: 2.4099114889739663

Epoch: 5| Step: 8
Training loss: 2.743435859680176
Validation loss: 2.3962518950944305

Epoch: 5| Step: 9
Training loss: 2.87194561958313
Validation loss: 2.3865174375554568

Epoch: 5| Step: 10
Training loss: 2.8642032146453857
Validation loss: 2.3797744961195093

Epoch: 23| Step: 0
Training loss: 2.145925283432007
Validation loss: 2.3835567479492514

Epoch: 5| Step: 1
Training loss: 2.74639630317688
Validation loss: 2.378802732754779

Epoch: 5| Step: 2
Training loss: 2.493318557739258
Validation loss: 2.3755774677440686

Epoch: 5| Step: 3
Training loss: 2.962730884552002
Validation loss: 2.3703073532350603

Epoch: 5| Step: 4
Training loss: 2.485027313232422
Validation loss: 2.3662904052324194

Epoch: 5| Step: 5
Training loss: 2.2155442237854004
Validation loss: 2.3564477864132134

Epoch: 5| Step: 6
Training loss: 2.6034841537475586
Validation loss: 2.3520438722384873

Epoch: 5| Step: 7
Training loss: 2.3978612422943115
Validation loss: 2.3517582698534896

Epoch: 5| Step: 8
Training loss: 3.2409636974334717
Validation loss: 2.34493274329811

Epoch: 5| Step: 9
Training loss: 2.3397068977355957
Validation loss: 2.334640610602594

Epoch: 5| Step: 10
Training loss: 2.7409725189208984
Validation loss: 2.3383015637756674

Epoch: 24| Step: 0
Training loss: 2.4514403343200684
Validation loss: 2.3323048135285736

Epoch: 5| Step: 1
Training loss: 2.8325631618499756
Validation loss: 2.322175378440529

Epoch: 5| Step: 2
Training loss: 2.345257520675659
Validation loss: 2.330158350288227

Epoch: 5| Step: 3
Training loss: 3.2366862297058105
Validation loss: 2.3366350166259275

Epoch: 5| Step: 4
Training loss: 2.466142177581787
Validation loss: 2.3379394469722623

Epoch: 5| Step: 5
Training loss: 2.1980538368225098
Validation loss: 2.331464047073036

Epoch: 5| Step: 6
Training loss: 2.6143240928649902
Validation loss: 2.3229328099117486

Epoch: 5| Step: 7
Training loss: 2.170132875442505
Validation loss: 2.3279849201120357

Epoch: 5| Step: 8
Training loss: 3.0600476264953613
Validation loss: 2.319173953866446

Epoch: 5| Step: 9
Training loss: 2.467919111251831
Validation loss: 2.3505035574718187

Epoch: 5| Step: 10
Training loss: 2.2162208557128906
Validation loss: 2.3298769407374884

Epoch: 25| Step: 0
Training loss: 2.0768632888793945
Validation loss: 2.3238836885780416

Epoch: 5| Step: 1
Training loss: 3.0006632804870605
Validation loss: 2.330737030634316

Epoch: 5| Step: 2
Training loss: 2.804090738296509
Validation loss: 2.349482500424949

Epoch: 5| Step: 3
Training loss: 2.5369629859924316
Validation loss: 2.3189891256311888

Epoch: 5| Step: 4
Training loss: 2.6597447395324707
Validation loss: 2.3229457357878327

Epoch: 5| Step: 5
Training loss: 2.328402519226074
Validation loss: 2.3205685487357517

Epoch: 5| Step: 6
Training loss: 3.3590846061706543
Validation loss: 2.326950055296703

Epoch: 5| Step: 7
Training loss: 1.8938897848129272
Validation loss: 2.321103557463615

Epoch: 5| Step: 8
Training loss: 2.768669843673706
Validation loss: 2.308371365711253

Epoch: 5| Step: 9
Training loss: 2.4388937950134277
Validation loss: 2.3158454433564217

Epoch: 5| Step: 10
Training loss: 2.284087896347046
Validation loss: 2.3129395720779256

Epoch: 26| Step: 0
Training loss: 2.619204044342041
Validation loss: 2.3211157168111494

Epoch: 5| Step: 1
Training loss: 2.8970017433166504
Validation loss: 2.3412299438189437

Epoch: 5| Step: 2
Training loss: 2.2801132202148438
Validation loss: 2.3155134313849994

Epoch: 5| Step: 3
Training loss: 2.6868739128112793
Validation loss: 2.3171880552845616

Epoch: 5| Step: 4
Training loss: 2.5599544048309326
Validation loss: 2.3080310026804605

Epoch: 5| Step: 5
Training loss: 2.9614837169647217
Validation loss: 2.2942989385256203

Epoch: 5| Step: 6
Training loss: 2.4938721656799316
Validation loss: 2.312021450329852

Epoch: 5| Step: 7
Training loss: 1.7559707164764404
Validation loss: 2.3228187073943434

Epoch: 5| Step: 8
Training loss: 2.7300891876220703
Validation loss: 2.319145438491657

Epoch: 5| Step: 9
Training loss: 2.0854034423828125
Validation loss: 2.3093015173430085

Epoch: 5| Step: 10
Training loss: 3.0160694122314453
Validation loss: 2.3168872915288454

Epoch: 27| Step: 0
Training loss: 2.4781551361083984
Validation loss: 2.3331621000843663

Epoch: 5| Step: 1
Training loss: 2.979820966720581
Validation loss: 2.3227579260385163

Epoch: 5| Step: 2
Training loss: 3.1571764945983887
Validation loss: 2.312919521844515

Epoch: 5| Step: 3
Training loss: 2.122446298599243
Validation loss: 2.3162010715853785

Epoch: 5| Step: 4
Training loss: 2.642993211746216
Validation loss: 2.319898546382945

Epoch: 5| Step: 5
Training loss: 2.22861909866333
Validation loss: 2.3155951474302556

Epoch: 5| Step: 6
Training loss: 2.559318780899048
Validation loss: 2.291308582469981

Epoch: 5| Step: 7
Training loss: 2.498279094696045
Validation loss: 2.290955674263739

Epoch: 5| Step: 8
Training loss: 2.4457881450653076
Validation loss: 2.3183612131303355

Epoch: 5| Step: 9
Training loss: 2.631547689437866
Validation loss: 2.3021973358687533

Epoch: 5| Step: 10
Training loss: 2.059732437133789
Validation loss: 2.3057374159495034

Epoch: 28| Step: 0
Training loss: 3.1559805870056152
Validation loss: 2.317153681990921

Epoch: 5| Step: 1
Training loss: 1.9903888702392578
Validation loss: 2.331747483181697

Epoch: 5| Step: 2
Training loss: 3.080134868621826
Validation loss: 2.31989013507802

Epoch: 5| Step: 3
Training loss: 2.382012128829956
Validation loss: 2.322660230821179

Epoch: 5| Step: 4
Training loss: 2.8083033561706543
Validation loss: 2.316339579961633

Epoch: 5| Step: 5
Training loss: 2.6837196350097656
Validation loss: 2.299233741657708

Epoch: 5| Step: 6
Training loss: 2.267369031906128
Validation loss: 2.31789937198803

Epoch: 5| Step: 7
Training loss: 1.8319238424301147
Validation loss: 2.2922882854297595

Epoch: 5| Step: 8
Training loss: 2.3410849571228027
Validation loss: 2.298963867208009

Epoch: 5| Step: 9
Training loss: 2.8238229751586914
Validation loss: 2.2974264134642897

Epoch: 5| Step: 10
Training loss: 2.509380340576172
Validation loss: 2.301113182498563

Epoch: 29| Step: 0
Training loss: 2.565518856048584
Validation loss: 2.296768803750315

Epoch: 5| Step: 1
Training loss: 2.092562437057495
Validation loss: 2.2917173062601397

Epoch: 5| Step: 2
Training loss: 1.786770224571228
Validation loss: 2.31248442844678

Epoch: 5| Step: 3
Training loss: 2.557591676712036
Validation loss: 2.310065233579246

Epoch: 5| Step: 4
Training loss: 2.4233384132385254
Validation loss: 2.301882582326089

Epoch: 5| Step: 5
Training loss: 3.284569501876831
Validation loss: 2.307360517081394

Epoch: 5| Step: 6
Training loss: 2.4236555099487305
Validation loss: 2.291114886601766

Epoch: 5| Step: 7
Training loss: 2.827385425567627
Validation loss: 2.285641138271619

Epoch: 5| Step: 8
Training loss: 2.241736888885498
Validation loss: 2.283657045774562

Epoch: 5| Step: 9
Training loss: 3.2167868614196777
Validation loss: 2.2803633187406804

Epoch: 5| Step: 10
Training loss: 2.2393076419830322
Validation loss: 2.3028248151143393

Epoch: 30| Step: 0
Training loss: 2.0485212802886963
Validation loss: 2.289881629328574

Epoch: 5| Step: 1
Training loss: 1.8874855041503906
Validation loss: 2.2841654567308325

Epoch: 5| Step: 2
Training loss: 2.3404860496520996
Validation loss: 2.2739447598816245

Epoch: 5| Step: 3
Training loss: 2.9098198413848877
Validation loss: 2.3050022663608676

Epoch: 5| Step: 4
Training loss: 2.565758228302002
Validation loss: 2.297903496731994

Epoch: 5| Step: 5
Training loss: 2.059004306793213
Validation loss: 2.301821918897731

Epoch: 5| Step: 6
Training loss: 2.767385959625244
Validation loss: 2.305795843883227

Epoch: 5| Step: 7
Training loss: 2.5270168781280518
Validation loss: 2.301764375420027

Epoch: 5| Step: 8
Training loss: 3.0034191608428955
Validation loss: 2.2921805458684124

Epoch: 5| Step: 9
Training loss: 2.6469082832336426
Validation loss: 2.300003138921594

Epoch: 5| Step: 10
Training loss: 2.9947361946105957
Validation loss: 2.281068604479554

Epoch: 31| Step: 0
Training loss: 2.6122043132781982
Validation loss: 2.2939132439192904

Epoch: 5| Step: 1
Training loss: 1.538268804550171
Validation loss: 2.297176730248236

Epoch: 5| Step: 2
Training loss: 2.8618221282958984
Validation loss: 2.296970239249609

Epoch: 5| Step: 3
Training loss: 2.3434345722198486
Validation loss: 2.2885137988675024

Epoch: 5| Step: 4
Training loss: 2.7581980228424072
Validation loss: 2.271054329410676

Epoch: 5| Step: 5
Training loss: 2.5877232551574707
Validation loss: 2.2815728597743536

Epoch: 5| Step: 6
Training loss: 2.778857707977295
Validation loss: 2.2842242666470107

Epoch: 5| Step: 7
Training loss: 2.73590087890625
Validation loss: 2.2739112761712845

Epoch: 5| Step: 8
Training loss: 2.464003086090088
Validation loss: 2.3119520115595993

Epoch: 5| Step: 9
Training loss: 2.7009854316711426
Validation loss: 2.2860760842600176

Epoch: 5| Step: 10
Training loss: 2.0083725452423096
Validation loss: 2.2824535318600234

Epoch: 32| Step: 0
Training loss: 2.0542759895324707
Validation loss: 2.2767062623013734

Epoch: 5| Step: 1
Training loss: 2.7250494956970215
Validation loss: 2.282827890047463

Epoch: 5| Step: 2
Training loss: 2.9487433433532715
Validation loss: 2.2963484461589525

Epoch: 5| Step: 3
Training loss: 2.343079090118408
Validation loss: 2.2666743698940484

Epoch: 5| Step: 4
Training loss: 2.1063029766082764
Validation loss: 2.25889483574898

Epoch: 5| Step: 5
Training loss: 3.3864669799804688
Validation loss: 2.289249107401858

Epoch: 5| Step: 6
Training loss: 2.303508758544922
Validation loss: 2.288566691901094

Epoch: 5| Step: 7
Training loss: 2.873378038406372
Validation loss: 2.2611992359161377

Epoch: 5| Step: 8
Training loss: 2.4461474418640137
Validation loss: 2.2755165689735004

Epoch: 5| Step: 9
Training loss: 1.5109405517578125
Validation loss: 2.264707398670976

Epoch: 5| Step: 10
Training loss: 2.859856367111206
Validation loss: 2.270601636619978

Epoch: 33| Step: 0
Training loss: 2.5606675148010254
Validation loss: 2.273628980882706

Epoch: 5| Step: 1
Training loss: 2.5645639896392822
Validation loss: 2.283066608572519

Epoch: 5| Step: 2
Training loss: 2.6132283210754395
Validation loss: 2.264398520992648

Epoch: 5| Step: 3
Training loss: 2.6562459468841553
Validation loss: 2.25536702012503

Epoch: 5| Step: 4
Training loss: 2.0450711250305176
Validation loss: 2.2685987308461177

Epoch: 5| Step: 5
Training loss: 2.0882010459899902
Validation loss: 2.2784896563458186

Epoch: 5| Step: 6
Training loss: 2.875080108642578
Validation loss: 2.2652244811416953

Epoch: 5| Step: 7
Training loss: 2.3723549842834473
Validation loss: 2.2810377100462556

Epoch: 5| Step: 8
Training loss: 2.2981250286102295
Validation loss: 2.2800710713991554

Epoch: 5| Step: 9
Training loss: 2.7178444862365723
Validation loss: 2.2752012283571306

Epoch: 5| Step: 10
Training loss: 2.483707904815674
Validation loss: 2.2687929163696947

Epoch: 34| Step: 0
Training loss: 2.5401854515075684
Validation loss: 2.2542467489037463

Epoch: 5| Step: 1
Training loss: 2.09863543510437
Validation loss: 2.2908770294599634

Epoch: 5| Step: 2
Training loss: 2.664247512817383
Validation loss: 2.2657861248139413

Epoch: 5| Step: 3
Training loss: 2.4748363494873047
Validation loss: 2.26079664691802

Epoch: 5| Step: 4
Training loss: 2.9345860481262207
Validation loss: 2.254876616180584

Epoch: 5| Step: 5
Training loss: 2.0493736267089844
Validation loss: 2.261148670668243

Epoch: 5| Step: 6
Training loss: 2.8373606204986572
Validation loss: 2.279765377762497

Epoch: 5| Step: 7
Training loss: 2.3854570388793945
Validation loss: 2.2643565849591325

Epoch: 5| Step: 8
Training loss: 2.5842032432556152
Validation loss: 2.2801030041069112

Epoch: 5| Step: 9
Training loss: 2.187173366546631
Validation loss: 2.2686102826108216

Epoch: 5| Step: 10
Training loss: 2.629579544067383
Validation loss: 2.2666627771110943

Epoch: 35| Step: 0
Training loss: 2.2188611030578613
Validation loss: 2.2614778792986305

Epoch: 5| Step: 1
Training loss: 2.0597002506256104
Validation loss: 2.260099531501852

Epoch: 5| Step: 2
Training loss: 2.5898232460021973
Validation loss: 2.2755036328428533

Epoch: 5| Step: 3
Training loss: 2.26935076713562
Validation loss: 2.2450694127749373

Epoch: 5| Step: 4
Training loss: 2.480085849761963
Validation loss: 2.265004495138763

Epoch: 5| Step: 5
Training loss: 2.5753111839294434
Validation loss: 2.2566935785355104

Epoch: 5| Step: 6
Training loss: 2.2565054893493652
Validation loss: 2.262547023834721

Epoch: 5| Step: 7
Training loss: 2.8146538734436035
Validation loss: 2.2741618310251543

Epoch: 5| Step: 8
Training loss: 2.1929681301116943
Validation loss: 2.2734324624461513

Epoch: 5| Step: 9
Training loss: 2.869401216506958
Validation loss: 2.2561081891418784

Epoch: 5| Step: 10
Training loss: 2.7783021926879883
Validation loss: 2.2458292309955885

Epoch: 36| Step: 0
Training loss: 2.4544966220855713
Validation loss: 2.243006193509666

Epoch: 5| Step: 1
Training loss: 1.7780860662460327
Validation loss: 2.258927342712238

Epoch: 5| Step: 2
Training loss: 2.3605878353118896
Validation loss: 2.25995107107265

Epoch: 5| Step: 3
Training loss: 2.3500895500183105
Validation loss: 2.254670725073866

Epoch: 5| Step: 4
Training loss: 3.0913453102111816
Validation loss: 2.2699278528972338

Epoch: 5| Step: 5
Training loss: 2.3221802711486816
Validation loss: 2.2502172480347338

Epoch: 5| Step: 6
Training loss: 2.3261876106262207
Validation loss: 2.2419606357492428

Epoch: 5| Step: 7
Training loss: 2.89323353767395
Validation loss: 2.266894155933011

Epoch: 5| Step: 8
Training loss: 2.881347179412842
Validation loss: 2.2651435867432625

Epoch: 5| Step: 9
Training loss: 2.14068341255188
Validation loss: 2.262814229534518

Epoch: 5| Step: 10
Training loss: 2.4098377227783203
Validation loss: 2.2501677172158354

Epoch: 37| Step: 0
Training loss: 2.6574342250823975
Validation loss: 2.251901331768241

Epoch: 5| Step: 1
Training loss: 3.041560649871826
Validation loss: 2.248201193348054

Epoch: 5| Step: 2
Training loss: 2.8425779342651367
Validation loss: 2.2406150551252466

Epoch: 5| Step: 3
Training loss: 2.5001940727233887
Validation loss: 2.2520129680633545

Epoch: 5| Step: 4
Training loss: 2.2166714668273926
Validation loss: 2.2423666933531403

Epoch: 5| Step: 5
Training loss: 1.8752679824829102
Validation loss: 2.231628461550641

Epoch: 5| Step: 6
Training loss: 2.5244038105010986
Validation loss: 2.2376450825763006

Epoch: 5| Step: 7
Training loss: 1.7822898626327515
Validation loss: 2.2462961686554777

Epoch: 5| Step: 8
Training loss: 2.2522315979003906
Validation loss: 2.2504912525094967

Epoch: 5| Step: 9
Training loss: 2.665618419647217
Validation loss: 2.254287419780608

Epoch: 5| Step: 10
Training loss: 2.5325801372528076
Validation loss: 2.2557716600356565

Epoch: 38| Step: 0
Training loss: 2.549260377883911
Validation loss: 2.2472132252108667

Epoch: 5| Step: 1
Training loss: 2.232221841812134
Validation loss: 2.2388898044504146

Epoch: 5| Step: 2
Training loss: 3.4255638122558594
Validation loss: 2.232312992054929

Epoch: 5| Step: 3
Training loss: 1.9973466396331787
Validation loss: 2.2411191155833583

Epoch: 5| Step: 4
Training loss: 1.951808214187622
Validation loss: 2.253481998238512

Epoch: 5| Step: 5
Training loss: 3.2881062030792236
Validation loss: 2.2454723927282516

Epoch: 5| Step: 6
Training loss: 2.5517139434814453
Validation loss: 2.2482784243040186

Epoch: 5| Step: 7
Training loss: 1.951208472251892
Validation loss: 2.2301134447897635

Epoch: 5| Step: 8
Training loss: 2.3375895023345947
Validation loss: 2.23552990728809

Epoch: 5| Step: 9
Training loss: 2.538985013961792
Validation loss: 2.260810293177123

Epoch: 5| Step: 10
Training loss: 2.0393009185791016
Validation loss: 2.2521315313154653

Epoch: 39| Step: 0
Training loss: 2.295753002166748
Validation loss: 2.2538582278836157

Epoch: 5| Step: 1
Training loss: 2.45676851272583
Validation loss: 2.248053962184537

Epoch: 5| Step: 2
Training loss: 2.5213112831115723
Validation loss: 2.2266061664909444

Epoch: 5| Step: 3
Training loss: 2.8295774459838867
Validation loss: 2.2496456843550487

Epoch: 5| Step: 4
Training loss: 2.385406970977783
Validation loss: 2.234227593227099

Epoch: 5| Step: 5
Training loss: 1.8025925159454346
Validation loss: 2.247706659378544

Epoch: 5| Step: 6
Training loss: 2.461602210998535
Validation loss: 2.2347196302106305

Epoch: 5| Step: 7
Training loss: 2.3762121200561523
Validation loss: 2.254601988741147

Epoch: 5| Step: 8
Training loss: 2.7552456855773926
Validation loss: 2.2371855064104964

Epoch: 5| Step: 9
Training loss: 2.567061185836792
Validation loss: 2.2183882677426903

Epoch: 5| Step: 10
Training loss: 2.45578670501709
Validation loss: 2.2322571995437785

Epoch: 40| Step: 0
Training loss: 2.267200469970703
Validation loss: 2.230803064120713

Epoch: 5| Step: 1
Training loss: 2.4323060512542725
Validation loss: 2.2154710946544522

Epoch: 5| Step: 2
Training loss: 2.68734073638916
Validation loss: 2.239870232920493

Epoch: 5| Step: 3
Training loss: 2.0613224506378174
Validation loss: 2.218440271192981

Epoch: 5| Step: 4
Training loss: 2.4714713096618652
Validation loss: 2.225975428858111

Epoch: 5| Step: 5
Training loss: 2.8978524208068848
Validation loss: 2.2305502558267243

Epoch: 5| Step: 6
Training loss: 2.2342276573181152
Validation loss: 2.2181573478124474

Epoch: 5| Step: 7
Training loss: 2.2357089519500732
Validation loss: 2.2382443643385366

Epoch: 5| Step: 8
Training loss: 2.7442469596862793
Validation loss: 2.2359759294858543

Epoch: 5| Step: 9
Training loss: 2.349390745162964
Validation loss: 2.242464280897571

Epoch: 5| Step: 10
Training loss: 2.516538143157959
Validation loss: 2.2334077550518896

Epoch: 41| Step: 0
Training loss: 3.5588276386260986
Validation loss: 2.2212393104389148

Epoch: 5| Step: 1
Training loss: 2.1200766563415527
Validation loss: 2.2378577955307497

Epoch: 5| Step: 2
Training loss: 2.419525384902954
Validation loss: 2.2275381267711682

Epoch: 5| Step: 3
Training loss: 2.3719887733459473
Validation loss: 2.2386444409688315

Epoch: 5| Step: 4
Training loss: 2.2127583026885986
Validation loss: 2.238922575468658

Epoch: 5| Step: 5
Training loss: 2.707122325897217
Validation loss: 2.2312909813337427

Epoch: 5| Step: 6
Training loss: 2.383650064468384
Validation loss: 2.24399596388622

Epoch: 5| Step: 7
Training loss: 2.5528221130371094
Validation loss: 2.231270444008612

Epoch: 5| Step: 8
Training loss: 2.2909538745880127
Validation loss: 2.2320594428687968

Epoch: 5| Step: 9
Training loss: 1.8392326831817627
Validation loss: 2.233360613546064

Epoch: 5| Step: 10
Training loss: 2.334352731704712
Validation loss: 2.225920766912481

Epoch: 42| Step: 0
Training loss: 2.287492275238037
Validation loss: 2.228070000166534

Epoch: 5| Step: 1
Training loss: 2.139342784881592
Validation loss: 2.2222956816355386

Epoch: 5| Step: 2
Training loss: 1.9717648029327393
Validation loss: 2.2385656615739227

Epoch: 5| Step: 3
Training loss: 2.349971055984497
Validation loss: 2.220200364307691

Epoch: 5| Step: 4
Training loss: 2.8360495567321777
Validation loss: 2.2212616525670534

Epoch: 5| Step: 5
Training loss: 2.8808016777038574
Validation loss: 2.23652732500466

Epoch: 5| Step: 6
Training loss: 2.327023983001709
Validation loss: 2.2242908939238517

Epoch: 5| Step: 7
Training loss: 2.5563879013061523
Validation loss: 2.211682378604848

Epoch: 5| Step: 8
Training loss: 1.9182708263397217
Validation loss: 2.23603198605199

Epoch: 5| Step: 9
Training loss: 2.670914888381958
Validation loss: 2.2088699238274687

Epoch: 5| Step: 10
Training loss: 2.6106960773468018
Validation loss: 2.220375833972808

Epoch: 43| Step: 0
Training loss: 2.1990268230438232
Validation loss: 2.237639950167748

Epoch: 5| Step: 1
Training loss: 2.101231575012207
Validation loss: 2.246444771366735

Epoch: 5| Step: 2
Training loss: 2.7801966667175293
Validation loss: 2.2133723920391453

Epoch: 5| Step: 3
Training loss: 2.8886501789093018
Validation loss: 2.229750120511619

Epoch: 5| Step: 4
Training loss: 2.3120663166046143
Validation loss: 2.2223319379232263

Epoch: 5| Step: 5
Training loss: 2.7147231101989746
Validation loss: 2.232169807598155

Epoch: 5| Step: 6
Training loss: 2.709078788757324
Validation loss: 2.2148695094611055

Epoch: 5| Step: 7
Training loss: 2.50927734375
Validation loss: 2.2208185952196837

Epoch: 5| Step: 8
Training loss: 1.9535558223724365
Validation loss: 2.2132542492240987

Epoch: 5| Step: 9
Training loss: 2.4340407848358154
Validation loss: 2.2198618458163355

Epoch: 5| Step: 10
Training loss: 1.9756591320037842
Validation loss: 2.222769529588761

Epoch: 44| Step: 0
Training loss: 2.4108567237854004
Validation loss: 2.225682063769269

Epoch: 5| Step: 1
Training loss: 3.494584321975708
Validation loss: 2.229499988658454

Epoch: 5| Step: 2
Training loss: 1.7657943964004517
Validation loss: 2.217956894187517

Epoch: 5| Step: 3
Training loss: 2.7073769569396973
Validation loss: 2.224219132495183

Epoch: 5| Step: 4
Training loss: 1.9796558618545532
Validation loss: 2.206434611351259

Epoch: 5| Step: 5
Training loss: 2.8848423957824707
Validation loss: 2.212977822108935

Epoch: 5| Step: 6
Training loss: 2.159771680831909
Validation loss: 2.214870238816866

Epoch: 5| Step: 7
Training loss: 2.000214099884033
Validation loss: 2.208296283598869

Epoch: 5| Step: 8
Training loss: 1.8666908740997314
Validation loss: 2.245212871541259

Epoch: 5| Step: 9
Training loss: 1.9123140573501587
Validation loss: 2.224188603380675

Epoch: 5| Step: 10
Training loss: 3.498680353164673
Validation loss: 2.235341646338022

Epoch: 45| Step: 0
Training loss: 2.2093348503112793
Validation loss: 2.2226339206900647

Epoch: 5| Step: 1
Training loss: 2.4113242626190186
Validation loss: 2.2212271818550686

Epoch: 5| Step: 2
Training loss: 2.6923623085021973
Validation loss: 2.225725363659602

Epoch: 5| Step: 3
Training loss: 2.5005953311920166
Validation loss: 2.2165895687636508

Epoch: 5| Step: 4
Training loss: 2.500788688659668
Validation loss: 2.2161377783744567

Epoch: 5| Step: 5
Training loss: 2.5585172176361084
Validation loss: 2.2100900027059738

Epoch: 5| Step: 6
Training loss: 1.9251892566680908
Validation loss: 2.2200824368384575

Epoch: 5| Step: 7
Training loss: 2.091362476348877
Validation loss: 2.2539143613589707

Epoch: 5| Step: 8
Training loss: 2.5497586727142334
Validation loss: 2.2212481908900763

Epoch: 5| Step: 9
Training loss: 2.2798352241516113
Validation loss: 2.216899807735156

Epoch: 5| Step: 10
Training loss: 2.914950370788574
Validation loss: 2.194772143517771

Epoch: 46| Step: 0
Training loss: 3.276381731033325
Validation loss: 2.196262387819188

Epoch: 5| Step: 1
Training loss: 1.9480011463165283
Validation loss: 2.2297133040684525

Epoch: 5| Step: 2
Training loss: 2.6294987201690674
Validation loss: 2.221098417876869

Epoch: 5| Step: 3
Training loss: 2.2683568000793457
Validation loss: 2.2387348195557952

Epoch: 5| Step: 4
Training loss: 2.2943882942199707
Validation loss: 2.2230411524413736

Epoch: 5| Step: 5
Training loss: 1.8846275806427002
Validation loss: 2.2520476438665904

Epoch: 5| Step: 6
Training loss: 1.9467321634292603
Validation loss: 2.1935745695585847

Epoch: 5| Step: 7
Training loss: 2.4101920127868652
Validation loss: 2.2354298099394767

Epoch: 5| Step: 8
Training loss: 2.5276153087615967
Validation loss: 2.2117639664680726

Epoch: 5| Step: 9
Training loss: 2.581725597381592
Validation loss: 2.1984150563516924

Epoch: 5| Step: 10
Training loss: 2.516892194747925
Validation loss: 2.2219157988025295

Epoch: 47| Step: 0
Training loss: 2.699791669845581
Validation loss: 2.223378404494255

Epoch: 5| Step: 1
Training loss: 2.610949993133545
Validation loss: 2.194088899961082

Epoch: 5| Step: 2
Training loss: 2.3859810829162598
Validation loss: 2.225849525902861

Epoch: 5| Step: 3
Training loss: 2.2311177253723145
Validation loss: 2.2006258451810448

Epoch: 5| Step: 4
Training loss: 2.063019037246704
Validation loss: 2.2163796245410876

Epoch: 5| Step: 5
Training loss: 2.697666645050049
Validation loss: 2.2203123800216185

Epoch: 5| Step: 6
Training loss: 2.336097240447998
Validation loss: 2.2418234809752433

Epoch: 5| Step: 7
Training loss: 2.798170566558838
Validation loss: 2.215728344455842

Epoch: 5| Step: 8
Training loss: 2.5223634243011475
Validation loss: 2.2289312347289054

Epoch: 5| Step: 9
Training loss: 1.9910885095596313
Validation loss: 2.2284542283704205

Epoch: 5| Step: 10
Training loss: 2.1986799240112305
Validation loss: 2.2253229220708213

Epoch: 48| Step: 0
Training loss: 2.1836395263671875
Validation loss: 2.2015238731138167

Epoch: 5| Step: 1
Training loss: 1.7639497518539429
Validation loss: 2.214698286466701

Epoch: 5| Step: 2
Training loss: 2.547402858734131
Validation loss: 2.2198011977698213

Epoch: 5| Step: 3
Training loss: 2.673196792602539
Validation loss: 2.2091067529493764

Epoch: 5| Step: 4
Training loss: 2.8461945056915283
Validation loss: 2.2223941305632233

Epoch: 5| Step: 5
Training loss: 2.123774766921997
Validation loss: 2.2169908708141697

Epoch: 5| Step: 6
Training loss: 2.3921024799346924
Validation loss: 2.2066741322958343

Epoch: 5| Step: 7
Training loss: 2.807788848876953
Validation loss: 2.214370604484312

Epoch: 5| Step: 8
Training loss: 2.1496622562408447
Validation loss: 2.203670024871826

Epoch: 5| Step: 9
Training loss: 2.1888041496276855
Validation loss: 2.1911754813245548

Epoch: 5| Step: 10
Training loss: 2.576525926589966
Validation loss: 2.196352694624214

Epoch: 49| Step: 0
Training loss: 2.131044864654541
Validation loss: 2.195555442123003

Epoch: 5| Step: 1
Training loss: 2.113712787628174
Validation loss: 2.2038776746360202

Epoch: 5| Step: 2
Training loss: 2.143206834793091
Validation loss: 2.2014039549776303

Epoch: 5| Step: 3
Training loss: 2.0756030082702637
Validation loss: 2.195188671030024

Epoch: 5| Step: 4
Training loss: 2.7878365516662598
Validation loss: 2.184744196553384

Epoch: 5| Step: 5
Training loss: 2.5391342639923096
Validation loss: 2.2021787294777493

Epoch: 5| Step: 6
Training loss: 2.779580593109131
Validation loss: 2.198228597640991

Epoch: 5| Step: 7
Training loss: 2.1909143924713135
Validation loss: 2.1966627067135227

Epoch: 5| Step: 8
Training loss: 1.8189541101455688
Validation loss: 2.2076816097382577

Epoch: 5| Step: 9
Training loss: 2.94427752494812
Validation loss: 2.1836311996624036

Epoch: 5| Step: 10
Training loss: 2.92929744720459
Validation loss: 2.171020784685689

Epoch: 50| Step: 0
Training loss: 2.6957170963287354
Validation loss: 2.17545247334306

Epoch: 5| Step: 1
Training loss: 2.777125597000122
Validation loss: 2.177513776286956

Epoch: 5| Step: 2
Training loss: 3.2993130683898926
Validation loss: 2.193407756026073

Epoch: 5| Step: 3
Training loss: 1.4264808893203735
Validation loss: 2.1742225436754126

Epoch: 5| Step: 4
Training loss: 2.7852330207824707
Validation loss: 2.189754378411078

Epoch: 5| Step: 5
Training loss: 2.4219970703125
Validation loss: 2.179668447022797

Epoch: 5| Step: 6
Training loss: 2.443587064743042
Validation loss: 2.2028609142508557

Epoch: 5| Step: 7
Training loss: 1.6955839395523071
Validation loss: 2.18149213124347

Epoch: 5| Step: 8
Training loss: 1.527635931968689
Validation loss: 2.169691995907855

Epoch: 5| Step: 9
Training loss: 2.4258084297180176
Validation loss: 2.173619624107115

Epoch: 5| Step: 10
Training loss: 2.8673288822174072
Validation loss: 2.1673862088111138

Epoch: 51| Step: 0
Training loss: 3.199171781539917
Validation loss: 2.171375331058297

Epoch: 5| Step: 1
Training loss: 2.1079115867614746
Validation loss: 2.187214646288144

Epoch: 5| Step: 2
Training loss: 2.0191473960876465
Validation loss: 2.1738485995159356

Epoch: 5| Step: 3
Training loss: 2.2851033210754395
Validation loss: 2.202187997038646

Epoch: 5| Step: 4
Training loss: 2.406620740890503
Validation loss: 2.1770889451426845

Epoch: 5| Step: 5
Training loss: 2.253361940383911
Validation loss: 2.174646508309149

Epoch: 5| Step: 6
Training loss: 2.6868436336517334
Validation loss: 2.1658803442473054

Epoch: 5| Step: 7
Training loss: 2.488677978515625
Validation loss: 2.186327093391008

Epoch: 5| Step: 8
Training loss: 1.862100601196289
Validation loss: 2.162723309250288

Epoch: 5| Step: 9
Training loss: 1.886286973953247
Validation loss: 2.1890899981221845

Epoch: 5| Step: 10
Training loss: 2.925217390060425
Validation loss: 2.191218791469451

Epoch: 52| Step: 0
Training loss: 2.572638988494873
Validation loss: 2.177796963722475

Epoch: 5| Step: 1
Training loss: 2.6705453395843506
Validation loss: 2.1797642272005797

Epoch: 5| Step: 2
Training loss: 2.1316022872924805
Validation loss: 2.1633663331308672

Epoch: 5| Step: 3
Training loss: 2.7166709899902344
Validation loss: 2.182264963785807

Epoch: 5| Step: 4
Training loss: 2.186626434326172
Validation loss: 2.173387517211258

Epoch: 5| Step: 5
Training loss: 2.3461461067199707
Validation loss: 2.18285338083903

Epoch: 5| Step: 6
Training loss: 1.6061303615570068
Validation loss: 2.167959277347852

Epoch: 5| Step: 7
Training loss: 2.1658661365509033
Validation loss: 2.1604830500900105

Epoch: 5| Step: 8
Training loss: 2.7847423553466797
Validation loss: 2.175047997505434

Epoch: 5| Step: 9
Training loss: 2.5403037071228027
Validation loss: 2.1801448381075295

Epoch: 5| Step: 10
Training loss: 2.3345155715942383
Validation loss: 2.1613081706467496

Epoch: 53| Step: 0
Training loss: 2.3716495037078857
Validation loss: 2.175825644564885

Epoch: 5| Step: 1
Training loss: 2.319899320602417
Validation loss: 2.1822906463376937

Epoch: 5| Step: 2
Training loss: 2.661870002746582
Validation loss: 2.192672762819516

Epoch: 5| Step: 3
Training loss: 2.921771764755249
Validation loss: 2.1789255052484493

Epoch: 5| Step: 4
Training loss: 2.5646815299987793
Validation loss: 2.159497263611004

Epoch: 5| Step: 5
Training loss: 2.4433703422546387
Validation loss: 2.1729965440688597

Epoch: 5| Step: 6
Training loss: 2.2858664989471436
Validation loss: 2.1603562729333037

Epoch: 5| Step: 7
Training loss: 2.2109169960021973
Validation loss: 2.186704945820634

Epoch: 5| Step: 8
Training loss: 1.989666223526001
Validation loss: 2.1567911742835917

Epoch: 5| Step: 9
Training loss: 2.0629355907440186
Validation loss: 2.1608615741934827

Epoch: 5| Step: 10
Training loss: 2.199726104736328
Validation loss: 2.172493657758159

Epoch: 54| Step: 0
Training loss: 2.7789788246154785
Validation loss: 2.173174727347589

Epoch: 5| Step: 1
Training loss: 2.8930163383483887
Validation loss: 2.1669712130741408

Epoch: 5| Step: 2
Training loss: 2.504279613494873
Validation loss: 2.1948746865795505

Epoch: 5| Step: 3
Training loss: 2.6865222454071045
Validation loss: 2.185251446180446

Epoch: 5| Step: 4
Training loss: 2.6930301189422607
Validation loss: 2.164879568161503

Epoch: 5| Step: 5
Training loss: 2.082939863204956
Validation loss: 2.162028302428543

Epoch: 5| Step: 6
Training loss: 2.2181379795074463
Validation loss: 2.1811396178378852

Epoch: 5| Step: 7
Training loss: 1.7997970581054688
Validation loss: 2.177676021411855

Epoch: 5| Step: 8
Training loss: 2.2536709308624268
Validation loss: 2.187845350593649

Epoch: 5| Step: 9
Training loss: 2.140951156616211
Validation loss: 2.1952548719221547

Epoch: 5| Step: 10
Training loss: 1.746930480003357
Validation loss: 2.1823452711105347

Epoch: 55| Step: 0
Training loss: 2.319106101989746
Validation loss: 2.1717861724156204

Epoch: 5| Step: 1
Training loss: 2.3101985454559326
Validation loss: 2.184390127017934

Epoch: 5| Step: 2
Training loss: 2.323119640350342
Validation loss: 2.1909720769492527

Epoch: 5| Step: 3
Training loss: 2.1448395252227783
Validation loss: 2.1731008534790366

Epoch: 5| Step: 4
Training loss: 2.797714948654175
Validation loss: 2.186477909805954

Epoch: 5| Step: 5
Training loss: 2.299177408218384
Validation loss: 2.1611756893896286

Epoch: 5| Step: 6
Training loss: 2.505890369415283
Validation loss: 2.1773466397357244

Epoch: 5| Step: 7
Training loss: 2.131810426712036
Validation loss: 2.1850644542324926

Epoch: 5| Step: 8
Training loss: 2.0551085472106934
Validation loss: 2.1819227228882494

Epoch: 5| Step: 9
Training loss: 2.946967601776123
Validation loss: 2.166073295377916

Epoch: 5| Step: 10
Training loss: 1.9420478343963623
Validation loss: 2.17346635172444

Epoch: 56| Step: 0
Training loss: 1.9086475372314453
Validation loss: 2.1832094013050036

Epoch: 5| Step: 1
Training loss: 2.3280692100524902
Validation loss: 2.181636888493774

Epoch: 5| Step: 2
Training loss: 2.1883671283721924
Validation loss: 2.169115997129871

Epoch: 5| Step: 3
Training loss: 3.0731685161590576
Validation loss: 2.1749141370096514

Epoch: 5| Step: 4
Training loss: 1.9155542850494385
Validation loss: 2.1936910357526553

Epoch: 5| Step: 5
Training loss: 2.2128498554229736
Validation loss: 2.2026704908699117

Epoch: 5| Step: 6
Training loss: 2.4004709720611572
Validation loss: 2.173755899552376

Epoch: 5| Step: 7
Training loss: 2.5682244300842285
Validation loss: 2.1814492338447162

Epoch: 5| Step: 8
Training loss: 2.064281940460205
Validation loss: 2.190120986712876

Epoch: 5| Step: 9
Training loss: 3.2336585521698
Validation loss: 2.167646687517884

Epoch: 5| Step: 10
Training loss: 1.8925583362579346
Validation loss: 2.1790812579534387

Epoch: 57| Step: 0
Training loss: 2.8773066997528076
Validation loss: 2.1749812685033327

Epoch: 5| Step: 1
Training loss: 2.3323304653167725
Validation loss: 2.1542116160033853

Epoch: 5| Step: 2
Training loss: 2.4808502197265625
Validation loss: 2.164711993227723

Epoch: 5| Step: 3
Training loss: 1.7374910116195679
Validation loss: 2.162635418676561

Epoch: 5| Step: 4
Training loss: 2.528019428253174
Validation loss: 2.171043831815002

Epoch: 5| Step: 5
Training loss: 2.279895782470703
Validation loss: 2.1760886869122906

Epoch: 5| Step: 6
Training loss: 3.174177646636963
Validation loss: 2.1682942093059583

Epoch: 5| Step: 7
Training loss: 2.3819420337677
Validation loss: 2.175051123865189

Epoch: 5| Step: 8
Training loss: 1.8722797632217407
Validation loss: 2.150893900984077

Epoch: 5| Step: 9
Training loss: 2.2271971702575684
Validation loss: 2.1590461423320155

Epoch: 5| Step: 10
Training loss: 1.8030424118041992
Validation loss: 2.1577158025515977

Epoch: 58| Step: 0
Training loss: 2.2742879390716553
Validation loss: 2.1652375600671254

Epoch: 5| Step: 1
Training loss: 3.1071338653564453
Validation loss: 2.1588782815523047

Epoch: 5| Step: 2
Training loss: 2.384244203567505
Validation loss: 2.164189273311246

Epoch: 5| Step: 3
Training loss: 1.8894360065460205
Validation loss: 2.169576488515382

Epoch: 5| Step: 4
Training loss: 2.4327237606048584
Validation loss: 2.1689739560568206

Epoch: 5| Step: 5
Training loss: 2.1546833515167236
Validation loss: 2.163020190372262

Epoch: 5| Step: 6
Training loss: 2.0611727237701416
Validation loss: 2.166110554049092

Epoch: 5| Step: 7
Training loss: 2.0404419898986816
Validation loss: 2.164731743515179

Epoch: 5| Step: 8
Training loss: 2.2247543334960938
Validation loss: 2.158268336326845

Epoch: 5| Step: 9
Training loss: 2.5081021785736084
Validation loss: 2.1704977622596164

Epoch: 5| Step: 10
Training loss: 2.647275447845459
Validation loss: 2.1625041090032107

Epoch: 59| Step: 0
Training loss: 2.5822556018829346
Validation loss: 2.159690991524727

Epoch: 5| Step: 1
Training loss: 1.935254454612732
Validation loss: 2.1523492336273193

Epoch: 5| Step: 2
Training loss: 2.7729129791259766
Validation loss: 2.1582492372041107

Epoch: 5| Step: 3
Training loss: 1.5780292749404907
Validation loss: 2.1835736792574645

Epoch: 5| Step: 4
Training loss: 2.250009775161743
Validation loss: 2.168600474634478

Epoch: 5| Step: 5
Training loss: 3.2115814685821533
Validation loss: 2.1683396267634567

Epoch: 5| Step: 6
Training loss: 1.718127965927124
Validation loss: 2.1597489464667534

Epoch: 5| Step: 7
Training loss: 2.0343070030212402
Validation loss: 2.1663585760260142

Epoch: 5| Step: 8
Training loss: 2.96685528755188
Validation loss: 2.1740267481855167

Epoch: 5| Step: 9
Training loss: 2.1395630836486816
Validation loss: 2.19748854637146

Epoch: 5| Step: 10
Training loss: 2.591418981552124
Validation loss: 2.1638569447302047

Epoch: 60| Step: 0
Training loss: 2.458270788192749
Validation loss: 2.176317813575909

Epoch: 5| Step: 1
Training loss: 1.4683669805526733
Validation loss: 2.160886638907976

Epoch: 5| Step: 2
Training loss: 2.5477397441864014
Validation loss: 2.1647228835731425

Epoch: 5| Step: 3
Training loss: 2.261573076248169
Validation loss: 2.167488662145471

Epoch: 5| Step: 4
Training loss: 2.5145039558410645
Validation loss: 2.1806456504329557

Epoch: 5| Step: 5
Training loss: 2.500824213027954
Validation loss: 2.169685063823577

Epoch: 5| Step: 6
Training loss: 2.5146801471710205
Validation loss: 2.1565771795088247

Epoch: 5| Step: 7
Training loss: 2.5145435333251953
Validation loss: 2.1557224771027923

Epoch: 5| Step: 8
Training loss: 2.5924181938171387
Validation loss: 2.1431119236894833

Epoch: 5| Step: 9
Training loss: 1.7222139835357666
Validation loss: 2.1720352429215626

Epoch: 5| Step: 10
Training loss: 2.4558041095733643
Validation loss: 2.155066297900292

Epoch: 61| Step: 0
Training loss: 2.3853583335876465
Validation loss: 2.1709385943669144

Epoch: 5| Step: 1
Training loss: 1.673492193222046
Validation loss: 2.160832743490896

Epoch: 5| Step: 2
Training loss: 2.176137924194336
Validation loss: 2.153915952610713

Epoch: 5| Step: 3
Training loss: 2.2966277599334717
Validation loss: 2.178350971591088

Epoch: 5| Step: 4
Training loss: 1.9056060314178467
Validation loss: 2.1691704950025006

Epoch: 5| Step: 5
Training loss: 2.538102865219116
Validation loss: 2.1638275115720687

Epoch: 5| Step: 6
Training loss: 2.7650837898254395
Validation loss: 2.1849907470005814

Epoch: 5| Step: 7
Training loss: 2.0884652137756348
Validation loss: 2.1762062311172485

Epoch: 5| Step: 8
Training loss: 2.7189536094665527
Validation loss: 2.146796223937824

Epoch: 5| Step: 9
Training loss: 2.7203164100646973
Validation loss: 2.1569632868612967

Epoch: 5| Step: 10
Training loss: 2.4747836589813232
Validation loss: 2.147674932274767

Epoch: 62| Step: 0
Training loss: 2.12770938873291
Validation loss: 2.1573435004039476

Epoch: 5| Step: 1
Training loss: 2.6701884269714355
Validation loss: 2.1722186637181107

Epoch: 5| Step: 2
Training loss: 1.5455814599990845
Validation loss: 2.156000539820681

Epoch: 5| Step: 3
Training loss: 2.3167357444763184
Validation loss: 2.173252272349532

Epoch: 5| Step: 4
Training loss: 2.590843677520752
Validation loss: 2.1462291158655638

Epoch: 5| Step: 5
Training loss: 1.989315390586853
Validation loss: 2.167704992396857

Epoch: 5| Step: 6
Training loss: 2.010576009750366
Validation loss: 2.1424236118152575

Epoch: 5| Step: 7
Training loss: 2.8256561756134033
Validation loss: 2.1683647555689656

Epoch: 5| Step: 8
Training loss: 2.5621793270111084
Validation loss: 2.1594154219473563

Epoch: 5| Step: 9
Training loss: 2.980153799057007
Validation loss: 2.1748998729131555

Epoch: 5| Step: 10
Training loss: 1.7733235359191895
Validation loss: 2.1792270650145826

Epoch: 63| Step: 0
Training loss: 2.4503979682922363
Validation loss: 2.1599115594740836

Epoch: 5| Step: 1
Training loss: 2.0579259395599365
Validation loss: 2.165336355086296

Epoch: 5| Step: 2
Training loss: 2.1058506965637207
Validation loss: 2.1651816752649125

Epoch: 5| Step: 3
Training loss: 2.377659559249878
Validation loss: 2.1494934597323017

Epoch: 5| Step: 4
Training loss: 2.1046979427337646
Validation loss: 2.1594010809416413

Epoch: 5| Step: 5
Training loss: 2.878047227859497
Validation loss: 2.172051954013045

Epoch: 5| Step: 6
Training loss: 2.5613207817077637
Validation loss: 2.1783563783091884

Epoch: 5| Step: 7
Training loss: 2.752755641937256
Validation loss: 2.1684851184968026

Epoch: 5| Step: 8
Training loss: 2.357728958129883
Validation loss: 2.183318773905436

Epoch: 5| Step: 9
Training loss: 2.0232656002044678
Validation loss: 2.1737988610421457

Epoch: 5| Step: 10
Training loss: 1.8042573928833008
Validation loss: 2.1683182536914782

Epoch: 64| Step: 0
Training loss: 2.4687838554382324
Validation loss: 2.16300707222313

Epoch: 5| Step: 1
Training loss: 2.154284954071045
Validation loss: 2.152291101794089

Epoch: 5| Step: 2
Training loss: 2.1057281494140625
Validation loss: 2.174266499857749

Epoch: 5| Step: 3
Training loss: 2.0116569995880127
Validation loss: 2.184723779719363

Epoch: 5| Step: 4
Training loss: 2.9379115104675293
Validation loss: 2.1609033615358415

Epoch: 5| Step: 5
Training loss: 1.8919779062271118
Validation loss: 2.192202583436043

Epoch: 5| Step: 6
Training loss: 2.3563272953033447
Validation loss: 2.1537280608248968

Epoch: 5| Step: 7
Training loss: 2.8845272064208984
Validation loss: 2.1722673805811072

Epoch: 5| Step: 8
Training loss: 1.9884824752807617
Validation loss: 2.1716231722985544

Epoch: 5| Step: 9
Training loss: 2.127060651779175
Validation loss: 2.160029604870786

Epoch: 5| Step: 10
Training loss: 2.494478940963745
Validation loss: 2.1695473155667706

Epoch: 65| Step: 0
Training loss: 2.395476818084717
Validation loss: 2.187880000760478

Epoch: 5| Step: 1
Training loss: 2.322000741958618
Validation loss: 2.159200781135149

Epoch: 5| Step: 2
Training loss: 2.3528146743774414
Validation loss: 2.1721827881310576

Epoch: 5| Step: 3
Training loss: 1.901060700416565
Validation loss: 2.153616051520071

Epoch: 5| Step: 4
Training loss: 3.0435919761657715
Validation loss: 2.187386599920129

Epoch: 5| Step: 5
Training loss: 1.8632316589355469
Validation loss: 2.174335046481061

Epoch: 5| Step: 6
Training loss: 2.6924374103546143
Validation loss: 2.18881288138769

Epoch: 5| Step: 7
Training loss: 2.1763246059417725
Validation loss: 2.1721713824938704

Epoch: 5| Step: 8
Training loss: 2.320495128631592
Validation loss: 2.1741504361552577

Epoch: 5| Step: 9
Training loss: 2.4702773094177246
Validation loss: 2.178743100935413

Epoch: 5| Step: 10
Training loss: 1.7873414754867554
Validation loss: 2.190444384851763

Epoch: 66| Step: 0
Training loss: 1.9336318969726562
Validation loss: 2.184477149799306

Epoch: 5| Step: 1
Training loss: 2.3039357662200928
Validation loss: 2.1775432222632953

Epoch: 5| Step: 2
Training loss: 2.697465181350708
Validation loss: 2.17890227225519

Epoch: 5| Step: 3
Training loss: 2.0849952697753906
Validation loss: 2.1901712955967074

Epoch: 5| Step: 4
Training loss: 2.515819549560547
Validation loss: 2.1711633833505775

Epoch: 5| Step: 5
Training loss: 2.2440037727355957
Validation loss: 2.1599399966578328

Epoch: 5| Step: 6
Training loss: 2.5177574157714844
Validation loss: 2.17450564394715

Epoch: 5| Step: 7
Training loss: 2.495046377182007
Validation loss: 2.17446707910107

Epoch: 5| Step: 8
Training loss: 2.234692096710205
Validation loss: 2.1475647803275817

Epoch: 5| Step: 9
Training loss: 2.4712586402893066
Validation loss: 2.1769250900514665

Epoch: 5| Step: 10
Training loss: 1.5461894273757935
Validation loss: 2.1665245063843264

Epoch: 67| Step: 0
Training loss: 2.0513782501220703
Validation loss: 2.179380214342507

Epoch: 5| Step: 1
Training loss: 2.0854039192199707
Validation loss: 2.1551577583436043

Epoch: 5| Step: 2
Training loss: 3.0617077350616455
Validation loss: 2.178652148092947

Epoch: 5| Step: 3
Training loss: 2.5656538009643555
Validation loss: 2.1549615629257692

Epoch: 5| Step: 4
Training loss: 2.4476475715637207
Validation loss: 2.1676365124282015

Epoch: 5| Step: 5
Training loss: 1.875281572341919
Validation loss: 2.1704016295812463

Epoch: 5| Step: 6
Training loss: 2.420466899871826
Validation loss: 2.1811601320902505

Epoch: 5| Step: 7
Training loss: 1.7015092372894287
Validation loss: 2.1630254663446897

Epoch: 5| Step: 8
Training loss: 2.774862766265869
Validation loss: 2.1437350062913794

Epoch: 5| Step: 9
Training loss: 2.0682156085968018
Validation loss: 2.1620203141243226

Epoch: 5| Step: 10
Training loss: 2.1747355461120605
Validation loss: 2.1856747558040004

Epoch: 68| Step: 0
Training loss: 1.9109182357788086
Validation loss: 2.164514590335149

Epoch: 5| Step: 1
Training loss: 2.3105883598327637
Validation loss: 2.1630554699128672

Epoch: 5| Step: 2
Training loss: 2.3087031841278076
Validation loss: 2.143510615953835

Epoch: 5| Step: 3
Training loss: 2.1157913208007812
Validation loss: 2.1429965752427296

Epoch: 5| Step: 4
Training loss: 1.897897720336914
Validation loss: 2.156776361568

Epoch: 5| Step: 5
Training loss: 2.383552074432373
Validation loss: 2.178414526806083

Epoch: 5| Step: 6
Training loss: 2.112332820892334
Validation loss: 2.170824585422393

Epoch: 5| Step: 7
Training loss: 2.4843862056732178
Validation loss: 2.174729633074935

Epoch: 5| Step: 8
Training loss: 1.9118671417236328
Validation loss: 2.15443669596026

Epoch: 5| Step: 9
Training loss: 2.9437994956970215
Validation loss: 2.1481149196624756

Epoch: 5| Step: 10
Training loss: 2.8813085556030273
Validation loss: 2.16397863306025

Epoch: 69| Step: 0
Training loss: 2.7637031078338623
Validation loss: 2.170852038168138

Epoch: 5| Step: 1
Training loss: 2.3357856273651123
Validation loss: 2.1405969486441663

Epoch: 5| Step: 2
Training loss: 2.6040337085723877
Validation loss: 2.1745300433968984

Epoch: 5| Step: 3
Training loss: 2.6484360694885254
Validation loss: 2.160506620202013

Epoch: 5| Step: 4
Training loss: 2.354985475540161
Validation loss: 2.174715917597535

Epoch: 5| Step: 5
Training loss: 2.7702081203460693
Validation loss: 2.1728021560176725

Epoch: 5| Step: 6
Training loss: 2.542396306991577
Validation loss: 2.1511481244076966

Epoch: 5| Step: 7
Training loss: 2.2305941581726074
Validation loss: 2.172521283549647

Epoch: 5| Step: 8
Training loss: 1.694409966468811
Validation loss: 2.1659752450963503

Epoch: 5| Step: 9
Training loss: 1.7943944931030273
Validation loss: 2.1504803165312736

Epoch: 5| Step: 10
Training loss: 1.3119724988937378
Validation loss: 2.1528808583495436

Epoch: 70| Step: 0
Training loss: 2.2341692447662354
Validation loss: 2.159688554784303

Epoch: 5| Step: 1
Training loss: 2.4750163555145264
Validation loss: 2.1793270136720393

Epoch: 5| Step: 2
Training loss: 2.108029365539551
Validation loss: 2.1829501762185046

Epoch: 5| Step: 3
Training loss: 2.5871758460998535
Validation loss: 2.158392979252723

Epoch: 5| Step: 4
Training loss: 2.066016435623169
Validation loss: 2.167895265804824

Epoch: 5| Step: 5
Training loss: 1.4503401517868042
Validation loss: 2.182166753276702

Epoch: 5| Step: 6
Training loss: 2.006908893585205
Validation loss: 2.1675883493115826

Epoch: 5| Step: 7
Training loss: 2.2821290493011475
Validation loss: 2.159457973254624

Epoch: 5| Step: 8
Training loss: 2.5696983337402344
Validation loss: 2.149012645085653

Epoch: 5| Step: 9
Training loss: 2.7896432876586914
Validation loss: 2.155470202046056

Epoch: 5| Step: 10
Training loss: 2.6285293102264404
Validation loss: 2.173999076248497

Epoch: 71| Step: 0
Training loss: 3.2574799060821533
Validation loss: 2.1594820496856526

Epoch: 5| Step: 1
Training loss: 2.286898136138916
Validation loss: 2.1755934710143716

Epoch: 5| Step: 2
Training loss: 2.4977478981018066
Validation loss: 2.1565462273936116

Epoch: 5| Step: 3
Training loss: 2.2068374156951904
Validation loss: 2.1807802492572415

Epoch: 5| Step: 4
Training loss: 3.3410072326660156
Validation loss: 2.1533483638558337

Epoch: 5| Step: 5
Training loss: 2.0074615478515625
Validation loss: 2.169224423746909

Epoch: 5| Step: 6
Training loss: 2.1286888122558594
Validation loss: 2.139720506565545

Epoch: 5| Step: 7
Training loss: 1.8927596807479858
Validation loss: 2.158170066854005

Epoch: 5| Step: 8
Training loss: 1.9702707529067993
Validation loss: 2.1691997576785345

Epoch: 5| Step: 9
Training loss: 1.6218023300170898
Validation loss: 2.1732088212044007

Epoch: 5| Step: 10
Training loss: 1.997960090637207
Validation loss: 2.15841398444227

Epoch: 72| Step: 0
Training loss: 2.413766384124756
Validation loss: 2.1296014760130193

Epoch: 5| Step: 1
Training loss: 2.0562853813171387
Validation loss: 2.166395466814759

Epoch: 5| Step: 2
Training loss: 2.126112461090088
Validation loss: 2.1578033893339095

Epoch: 5| Step: 3
Training loss: 2.787729263305664
Validation loss: 2.1752396809157504

Epoch: 5| Step: 4
Training loss: 2.147569179534912
Validation loss: 2.1525887238082064

Epoch: 5| Step: 5
Training loss: 2.3420653343200684
Validation loss: 2.1932169878354637

Epoch: 5| Step: 6
Training loss: 2.80391263961792
Validation loss: 2.1737042165571645

Epoch: 5| Step: 7
Training loss: 2.244542360305786
Validation loss: 2.1631714938789286

Epoch: 5| Step: 8
Training loss: 2.1483333110809326
Validation loss: 2.1714104734441286

Epoch: 5| Step: 9
Training loss: 2.0598111152648926
Validation loss: 2.1626351456488333

Epoch: 5| Step: 10
Training loss: 1.8647667169570923
Validation loss: 2.1598422629858858

Epoch: 73| Step: 0
Training loss: 1.934740662574768
Validation loss: 2.1620983897998767

Epoch: 5| Step: 1
Training loss: 2.782590866088867
Validation loss: 2.163704723440191

Epoch: 5| Step: 2
Training loss: 2.114453077316284
Validation loss: 2.1463841879239647

Epoch: 5| Step: 3
Training loss: 1.8435500860214233
Validation loss: 2.1458878978606193

Epoch: 5| Step: 4
Training loss: 2.415698766708374
Validation loss: 2.143694649460495

Epoch: 5| Step: 5
Training loss: 2.5192854404449463
Validation loss: 2.1514197934058403

Epoch: 5| Step: 6
Training loss: 2.258347749710083
Validation loss: 2.1642486049282934

Epoch: 5| Step: 7
Training loss: 2.224248170852661
Validation loss: 2.1388201149561072

Epoch: 5| Step: 8
Training loss: 2.8304686546325684
Validation loss: 2.161965316341769

Epoch: 5| Step: 9
Training loss: 1.8460922241210938
Validation loss: 2.1715000573024956

Epoch: 5| Step: 10
Training loss: 2.0467700958251953
Validation loss: 2.151421316208378

Epoch: 74| Step: 0
Training loss: 2.1384146213531494
Validation loss: 2.1260314885006157

Epoch: 5| Step: 1
Training loss: 2.2056102752685547
Validation loss: 2.1538971982976443

Epoch: 5| Step: 2
Training loss: 2.2357587814331055
Validation loss: 2.1356951318761355

Epoch: 5| Step: 3
Training loss: 1.7630027532577515
Validation loss: 2.1661952221265404

Epoch: 5| Step: 4
Training loss: 1.9101073741912842
Validation loss: 2.149529690383583

Epoch: 5| Step: 5
Training loss: 1.9517520666122437
Validation loss: 2.1523059965461813

Epoch: 5| Step: 6
Training loss: 1.4465091228485107
Validation loss: 2.159672866585434

Epoch: 5| Step: 7
Training loss: 3.1449151039123535
Validation loss: 2.1585820157040834

Epoch: 5| Step: 8
Training loss: 2.5322458744049072
Validation loss: 2.182396101695235

Epoch: 5| Step: 9
Training loss: 3.2177364826202393
Validation loss: 2.1633311330631213

Epoch: 5| Step: 10
Training loss: 2.2594165802001953
Validation loss: 2.1523025830586753

Epoch: 75| Step: 0
Training loss: 2.3469436168670654
Validation loss: 2.1676222098770963

Epoch: 5| Step: 1
Training loss: 2.9337849617004395
Validation loss: 2.119803931123467

Epoch: 5| Step: 2
Training loss: 2.2544002532958984
Validation loss: 2.151793090246057

Epoch: 5| Step: 3
Training loss: 2.3949689865112305
Validation loss: 2.1779713092311734

Epoch: 5| Step: 4
Training loss: 1.8484725952148438
Validation loss: 2.1969969105976883

Epoch: 5| Step: 5
Training loss: 1.9618263244628906
Validation loss: 2.1624277035395303

Epoch: 5| Step: 6
Training loss: 2.035743236541748
Validation loss: 2.1481548970745457

Epoch: 5| Step: 7
Training loss: 2.557222843170166
Validation loss: 2.1477830102366786

Epoch: 5| Step: 8
Training loss: 1.7124412059783936
Validation loss: 2.160163207720685

Epoch: 5| Step: 9
Training loss: 2.0869381427764893
Validation loss: 2.161292145329137

Epoch: 5| Step: 10
Training loss: 2.7432620525360107
Validation loss: 2.1309585622561875

Epoch: 76| Step: 0
Training loss: 2.636899471282959
Validation loss: 2.1469688941073675

Epoch: 5| Step: 1
Training loss: 2.330317974090576
Validation loss: 2.161786384479974

Epoch: 5| Step: 2
Training loss: 2.3071024417877197
Validation loss: 2.1428046226501465

Epoch: 5| Step: 3
Training loss: 2.5034971237182617
Validation loss: 2.1606376068566435

Epoch: 5| Step: 4
Training loss: 1.8667619228363037
Validation loss: 2.148263867183398

Epoch: 5| Step: 5
Training loss: 2.42651629447937
Validation loss: 2.173442543193858

Epoch: 5| Step: 6
Training loss: 2.0078351497650146
Validation loss: 2.1696390631378337

Epoch: 5| Step: 7
Training loss: 2.3909409046173096
Validation loss: 2.1658644266025995

Epoch: 5| Step: 8
Training loss: 2.0060269832611084
Validation loss: 2.16637150959302

Epoch: 5| Step: 9
Training loss: 2.4512863159179688
Validation loss: 2.1495475628042735

Epoch: 5| Step: 10
Training loss: 2.000162363052368
Validation loss: 2.1552308169744347

Epoch: 77| Step: 0
Training loss: 2.1213674545288086
Validation loss: 2.1508523366784535

Epoch: 5| Step: 1
Training loss: 2.636112689971924
Validation loss: 2.1671167932530886

Epoch: 5| Step: 2
Training loss: 1.9199081659317017
Validation loss: 2.160146382547194

Epoch: 5| Step: 3
Training loss: 2.2001776695251465
Validation loss: 2.164857602888538

Epoch: 5| Step: 4
Training loss: 1.8389021158218384
Validation loss: 2.1822020776810183

Epoch: 5| Step: 5
Training loss: 2.1635165214538574
Validation loss: 2.178413601331813

Epoch: 5| Step: 6
Training loss: 1.6956504583358765
Validation loss: 2.1765083728298062

Epoch: 5| Step: 7
Training loss: 2.187936782836914
Validation loss: 2.1651944665498633

Epoch: 5| Step: 8
Training loss: 2.347729444503784
Validation loss: 2.1735020555475706

Epoch: 5| Step: 9
Training loss: 3.1234991550445557
Validation loss: 2.170860778900885

Epoch: 5| Step: 10
Training loss: 2.6099445819854736
Validation loss: 2.1565433497069986

Epoch: 78| Step: 0
Training loss: 1.873440146446228
Validation loss: 2.162576501087476

Epoch: 5| Step: 1
Training loss: 2.947589874267578
Validation loss: 2.1460226966488745

Epoch: 5| Step: 2
Training loss: 1.651552438735962
Validation loss: 2.1477538103698404

Epoch: 5| Step: 3
Training loss: 2.620424747467041
Validation loss: 2.168384826311501

Epoch: 5| Step: 4
Training loss: 1.8746287822723389
Validation loss: 2.169415081700971

Epoch: 5| Step: 5
Training loss: 2.5498483180999756
Validation loss: 2.151273847908102

Epoch: 5| Step: 6
Training loss: 2.147024393081665
Validation loss: 2.1451052376019057

Epoch: 5| Step: 7
Training loss: 2.7340197563171387
Validation loss: 2.154656784508818

Epoch: 5| Step: 8
Training loss: 1.916338324546814
Validation loss: 2.1576098742023593

Epoch: 5| Step: 9
Training loss: 2.364440679550171
Validation loss: 2.1697387118493356

Epoch: 5| Step: 10
Training loss: 2.1093099117279053
Validation loss: 2.1465438078808528

Epoch: 79| Step: 0
Training loss: 1.9553998708724976
Validation loss: 2.157342690293507

Epoch: 5| Step: 1
Training loss: 2.477322816848755
Validation loss: 2.1522735831558064

Epoch: 5| Step: 2
Training loss: 2.5940968990325928
Validation loss: 2.1705504591746996

Epoch: 5| Step: 3
Training loss: 1.6877177953720093
Validation loss: 2.1605056216639857

Epoch: 5| Step: 4
Training loss: 2.8508763313293457
Validation loss: 2.1279153336760817

Epoch: 5| Step: 5
Training loss: 2.971057415008545
Validation loss: 2.1544510113295687

Epoch: 5| Step: 6
Training loss: 1.7905552387237549
Validation loss: 2.150506418238404

Epoch: 5| Step: 7
Training loss: 2.1871449947357178
Validation loss: 2.1716196229380946

Epoch: 5| Step: 8
Training loss: 2.566044330596924
Validation loss: 2.1682593617387997

Epoch: 5| Step: 9
Training loss: 1.672943115234375
Validation loss: 2.156061823650073

Epoch: 5| Step: 10
Training loss: 1.9841043949127197
Validation loss: 2.1651706554556407

Epoch: 80| Step: 0
Training loss: 2.1821563243865967
Validation loss: 2.146714015673566

Epoch: 5| Step: 1
Training loss: 1.5705268383026123
Validation loss: 2.1509840360251804

Epoch: 5| Step: 2
Training loss: 2.420316696166992
Validation loss: 2.1559894905295423

Epoch: 5| Step: 3
Training loss: 2.242703676223755
Validation loss: 2.1613950114096365

Epoch: 5| Step: 4
Training loss: 2.3715128898620605
Validation loss: 2.1732126435925885

Epoch: 5| Step: 5
Training loss: 2.3807647228240967
Validation loss: 2.1853328699706704

Epoch: 5| Step: 6
Training loss: 1.8464481830596924
Validation loss: 2.1484186264776413

Epoch: 5| Step: 7
Training loss: 2.7244794368743896
Validation loss: 2.1729993512553554

Epoch: 5| Step: 8
Training loss: 1.912265419960022
Validation loss: 2.156883562764814

Epoch: 5| Step: 9
Training loss: 2.7962985038757324
Validation loss: 2.163915980246759

Epoch: 5| Step: 10
Training loss: 2.156820058822632
Validation loss: 2.1659726455647457

Epoch: 81| Step: 0
Training loss: 2.4999337196350098
Validation loss: 2.1648097935543267

Epoch: 5| Step: 1
Training loss: 2.020120143890381
Validation loss: 2.1805772653190036

Epoch: 5| Step: 2
Training loss: 2.7340073585510254
Validation loss: 2.1380820017988964

Epoch: 5| Step: 3
Training loss: 2.4552829265594482
Validation loss: 2.1694723380509244

Epoch: 5| Step: 4
Training loss: 1.7875947952270508
Validation loss: 2.1779831763236754

Epoch: 5| Step: 5
Training loss: 2.175323486328125
Validation loss: 2.128086151615266

Epoch: 5| Step: 6
Training loss: 2.010504722595215
Validation loss: 2.1939547241375013

Epoch: 5| Step: 7
Training loss: 2.255645513534546
Validation loss: 2.1641090146956907

Epoch: 5| Step: 8
Training loss: 2.1876115798950195
Validation loss: 2.194044672032838

Epoch: 5| Step: 9
Training loss: 2.023803234100342
Validation loss: 2.160510047789543

Epoch: 5| Step: 10
Training loss: 2.655038356781006
Validation loss: 2.1718244834612777

Epoch: 82| Step: 0
Training loss: 2.3834640979766846
Validation loss: 2.172003702450824

Epoch: 5| Step: 1
Training loss: 2.727509021759033
Validation loss: 2.168801082077847

Epoch: 5| Step: 2
Training loss: 2.750436305999756
Validation loss: 2.145982919200774

Epoch: 5| Step: 3
Training loss: 1.8742315769195557
Validation loss: 2.1892366076028473

Epoch: 5| Step: 4
Training loss: 2.0322775840759277
Validation loss: 2.191074789211314

Epoch: 5| Step: 5
Training loss: 1.7814178466796875
Validation loss: 2.172651126820554

Epoch: 5| Step: 6
Training loss: 1.8883388042449951
Validation loss: 2.1694103748567644

Epoch: 5| Step: 7
Training loss: 2.0830087661743164
Validation loss: 2.1580014485184864

Epoch: 5| Step: 8
Training loss: 2.2104849815368652
Validation loss: 2.1680148416949856

Epoch: 5| Step: 9
Training loss: 2.566321611404419
Validation loss: 2.154498025935183

Epoch: 5| Step: 10
Training loss: 2.1692581176757812
Validation loss: 2.177517996039442

Epoch: 83| Step: 0
Training loss: 2.346783399581909
Validation loss: 2.1584806775534027

Epoch: 5| Step: 1
Training loss: 1.9991061687469482
Validation loss: 2.1588485933119252

Epoch: 5| Step: 2
Training loss: 1.9097111225128174
Validation loss: 2.1884132149398967

Epoch: 5| Step: 3
Training loss: 2.0735456943511963
Validation loss: 2.1493661442110614

Epoch: 5| Step: 4
Training loss: 2.656850814819336
Validation loss: 2.1602080765590874

Epoch: 5| Step: 5
Training loss: 1.9909608364105225
Validation loss: 2.143935573998318

Epoch: 5| Step: 6
Training loss: 2.319683790206909
Validation loss: 2.160068047943936

Epoch: 5| Step: 7
Training loss: 1.905651330947876
Validation loss: 2.1495685295392106

Epoch: 5| Step: 8
Training loss: 2.6254055500030518
Validation loss: 2.1572179602038477

Epoch: 5| Step: 9
Training loss: 2.4457874298095703
Validation loss: 2.1525028700469644

Epoch: 5| Step: 10
Training loss: 2.5291342735290527
Validation loss: 2.133925784018732

Epoch: 84| Step: 0
Training loss: 2.431570529937744
Validation loss: 2.1529500535739365

Epoch: 5| Step: 1
Training loss: 1.986193299293518
Validation loss: 2.1533945247691166

Epoch: 5| Step: 2
Training loss: 2.097048282623291
Validation loss: 2.139779931755476

Epoch: 5| Step: 3
Training loss: 1.8797214031219482
Validation loss: 2.1631369616395686

Epoch: 5| Step: 4
Training loss: 2.4383466243743896
Validation loss: 2.131695291047455

Epoch: 5| Step: 5
Training loss: 2.022578477859497
Validation loss: 2.140680382328649

Epoch: 5| Step: 6
Training loss: 2.876948833465576
Validation loss: 2.1292580853226366

Epoch: 5| Step: 7
Training loss: 1.96418035030365
Validation loss: 2.13973170070238

Epoch: 5| Step: 8
Training loss: 1.894646406173706
Validation loss: 2.138457282896965

Epoch: 5| Step: 9
Training loss: 2.363351345062256
Validation loss: 2.1539300077704975

Epoch: 5| Step: 10
Training loss: 2.6656363010406494
Validation loss: 2.16738881603364

Epoch: 85| Step: 0
Training loss: 3.3429081439971924
Validation loss: 2.1263693224999214

Epoch: 5| Step: 1
Training loss: 1.5364930629730225
Validation loss: 2.155485140380039

Epoch: 5| Step: 2
Training loss: 2.475039005279541
Validation loss: 2.1444603704637095

Epoch: 5| Step: 3
Training loss: 1.6464592218399048
Validation loss: 2.1830670987406084

Epoch: 5| Step: 4
Training loss: 2.1303677558898926
Validation loss: 2.1542845310703402

Epoch: 5| Step: 5
Training loss: 2.173097610473633
Validation loss: 2.144227680339608

Epoch: 5| Step: 6
Training loss: 2.512328624725342
Validation loss: 2.1463255292625836

Epoch: 5| Step: 7
Training loss: 2.609011173248291
Validation loss: 2.1247449613386586

Epoch: 5| Step: 8
Training loss: 2.1844024658203125
Validation loss: 2.1524722447959324

Epoch: 5| Step: 9
Training loss: 1.740290641784668
Validation loss: 2.1593428157990977

Epoch: 5| Step: 10
Training loss: 2.3398640155792236
Validation loss: 2.146568108630437

Epoch: 86| Step: 0
Training loss: 2.2457478046417236
Validation loss: 2.1764541595212874

Epoch: 5| Step: 1
Training loss: 2.472184658050537
Validation loss: 2.166901937095068

Epoch: 5| Step: 2
Training loss: 1.5836371183395386
Validation loss: 2.144889668751788

Epoch: 5| Step: 3
Training loss: 2.004671573638916
Validation loss: 2.157118697320261

Epoch: 5| Step: 4
Training loss: 2.0253052711486816
Validation loss: 2.149652196514991

Epoch: 5| Step: 5
Training loss: 2.692786931991577
Validation loss: 2.1136157628028625

Epoch: 5| Step: 6
Training loss: 2.2825989723205566
Validation loss: 2.1430465200895905

Epoch: 5| Step: 7
Training loss: 2.6535115242004395
Validation loss: 2.1377810457701325

Epoch: 5| Step: 8
Training loss: 1.946136474609375
Validation loss: 2.1449025497641614

Epoch: 5| Step: 9
Training loss: 2.8401761054992676
Validation loss: 2.1561515818360033

Epoch: 5| Step: 10
Training loss: 1.876459002494812
Validation loss: 2.1481512900321715

Epoch: 87| Step: 0
Training loss: 2.167874574661255
Validation loss: 2.1740991415516024

Epoch: 5| Step: 1
Training loss: 1.9436317682266235
Validation loss: 2.1537456614996797

Epoch: 5| Step: 2
Training loss: 2.5227572917938232
Validation loss: 2.129034747359573

Epoch: 5| Step: 3
Training loss: 2.1220977306365967
Validation loss: 2.1574397010187947

Epoch: 5| Step: 4
Training loss: 1.2985813617706299
Validation loss: 2.1359318302523707

Epoch: 5| Step: 5
Training loss: 2.152386426925659
Validation loss: 2.158167346831291

Epoch: 5| Step: 6
Training loss: 2.2044174671173096
Validation loss: 2.1207963087225474

Epoch: 5| Step: 7
Training loss: 2.5085926055908203
Validation loss: 2.1643439492871686

Epoch: 5| Step: 8
Training loss: 2.790947675704956
Validation loss: 2.1395984439439673

Epoch: 5| Step: 9
Training loss: 2.430328369140625
Validation loss: 2.153553811452722

Epoch: 5| Step: 10
Training loss: 2.4363136291503906
Validation loss: 2.1489248045029177

Epoch: 88| Step: 0
Training loss: 2.6059157848358154
Validation loss: 2.141452850834016

Epoch: 5| Step: 1
Training loss: 1.6842105388641357
Validation loss: 2.1489193490756455

Epoch: 5| Step: 2
Training loss: 1.8076419830322266
Validation loss: 2.149550954500834

Epoch: 5| Step: 3
Training loss: 1.6581932306289673
Validation loss: 2.1644996084192747

Epoch: 5| Step: 4
Training loss: 1.9528871774673462
Validation loss: 2.1773292761977

Epoch: 5| Step: 5
Training loss: 2.049506425857544
Validation loss: 2.1614959034868466

Epoch: 5| Step: 6
Training loss: 3.089109182357788
Validation loss: 2.128549724496821

Epoch: 5| Step: 7
Training loss: 2.870389461517334
Validation loss: 2.14778947061108

Epoch: 5| Step: 8
Training loss: 2.2117984294891357
Validation loss: 2.130546672369844

Epoch: 5| Step: 9
Training loss: 1.6606762409210205
Validation loss: 2.16865550574436

Epoch: 5| Step: 10
Training loss: 2.939229726791382
Validation loss: 2.147647368010654

Epoch: 89| Step: 0
Training loss: 2.9188084602355957
Validation loss: 2.1545597801926317

Epoch: 5| Step: 1
Training loss: 2.270639181137085
Validation loss: 2.147817641176203

Epoch: 5| Step: 2
Training loss: 1.546097993850708
Validation loss: 2.1554937080670427

Epoch: 5| Step: 3
Training loss: 2.683619737625122
Validation loss: 2.1623289264658445

Epoch: 5| Step: 4
Training loss: 2.160299062728882
Validation loss: 2.1350938415014618

Epoch: 5| Step: 5
Training loss: 1.8899555206298828
Validation loss: 2.1446927337236303

Epoch: 5| Step: 6
Training loss: 2.50616717338562
Validation loss: 2.164739102445623

Epoch: 5| Step: 7
Training loss: 2.843111991882324
Validation loss: 2.164722886136783

Epoch: 5| Step: 8
Training loss: 2.0071911811828613
Validation loss: 2.1767403797436784

Epoch: 5| Step: 9
Training loss: 1.7428264617919922
Validation loss: 2.1564313570658364

Epoch: 5| Step: 10
Training loss: 1.9935351610183716
Validation loss: 2.1552287827255907

Epoch: 90| Step: 0
Training loss: 2.467768430709839
Validation loss: 2.162080063614794

Epoch: 5| Step: 1
Training loss: 2.127713441848755
Validation loss: 2.152896117138606

Epoch: 5| Step: 2
Training loss: 1.8694677352905273
Validation loss: 2.163191577439667

Epoch: 5| Step: 3
Training loss: 2.155344247817993
Validation loss: 2.164326911331505

Epoch: 5| Step: 4
Training loss: 2.545766830444336
Validation loss: 2.1351087426626556

Epoch: 5| Step: 5
Training loss: 2.416194200515747
Validation loss: 2.1565957684670725

Epoch: 5| Step: 6
Training loss: 2.124124526977539
Validation loss: 2.1560127030136766

Epoch: 5| Step: 7
Training loss: 2.540886402130127
Validation loss: 2.159538084460843

Epoch: 5| Step: 8
Training loss: 1.9356439113616943
Validation loss: 2.1697942146690945

Epoch: 5| Step: 9
Training loss: 2.4068961143493652
Validation loss: 2.1707318931497555

Epoch: 5| Step: 10
Training loss: 1.8178107738494873
Validation loss: 2.1654903850247784

Epoch: 91| Step: 0
Training loss: 2.025526523590088
Validation loss: 2.1456959914135676

Epoch: 5| Step: 1
Training loss: 2.092459201812744
Validation loss: 2.147201209939936

Epoch: 5| Step: 2
Training loss: 2.517050266265869
Validation loss: 2.14239159194372

Epoch: 5| Step: 3
Training loss: 2.5819592475891113
Validation loss: 2.1536440016120992

Epoch: 5| Step: 4
Training loss: 2.2535934448242188
Validation loss: 2.1440585941396733

Epoch: 5| Step: 5
Training loss: 2.618609666824341
Validation loss: 2.1221969819838002

Epoch: 5| Step: 6
Training loss: 2.793452501296997
Validation loss: 2.158646014428908

Epoch: 5| Step: 7
Training loss: 1.412064790725708
Validation loss: 2.123291342489181

Epoch: 5| Step: 8
Training loss: 2.1434473991394043
Validation loss: 2.1409790669718096

Epoch: 5| Step: 9
Training loss: 2.7387702465057373
Validation loss: 2.135960122590424

Epoch: 5| Step: 10
Training loss: 1.3469678163528442
Validation loss: 2.1406700021477154

Epoch: 92| Step: 0
Training loss: 2.4434757232666016
Validation loss: 2.137832744147188

Epoch: 5| Step: 1
Training loss: 1.9021480083465576
Validation loss: 2.1587266768178632

Epoch: 5| Step: 2
Training loss: 2.4901905059814453
Validation loss: 2.1472732956691454

Epoch: 5| Step: 3
Training loss: 2.15446138381958
Validation loss: 2.1393494118926344

Epoch: 5| Step: 4
Training loss: 1.948326826095581
Validation loss: 2.1602660225283716

Epoch: 5| Step: 5
Training loss: 2.175250291824341
Validation loss: 2.1654707898375807

Epoch: 5| Step: 6
Training loss: 2.4000649452209473
Validation loss: 2.161325644421321

Epoch: 5| Step: 7
Training loss: 2.690185070037842
Validation loss: 2.1375000810110443

Epoch: 5| Step: 8
Training loss: 2.077889919281006
Validation loss: 2.1673337772328365

Epoch: 5| Step: 9
Training loss: 2.2299790382385254
Validation loss: 2.155448180372997

Epoch: 5| Step: 10
Training loss: 1.872283935546875
Validation loss: 2.156288303354735

Epoch: 93| Step: 0
Training loss: 2.8812170028686523
Validation loss: 2.14658425956644

Epoch: 5| Step: 1
Training loss: 3.014902114868164
Validation loss: 2.1237721596994708

Epoch: 5| Step: 2
Training loss: 1.757101058959961
Validation loss: 2.150547827443769

Epoch: 5| Step: 3
Training loss: 1.594789743423462
Validation loss: 2.1448522818985807

Epoch: 5| Step: 4
Training loss: 2.1610360145568848
Validation loss: 2.145685307441219

Epoch: 5| Step: 5
Training loss: 2.0608277320861816
Validation loss: 2.1627794516983854

Epoch: 5| Step: 6
Training loss: 2.4057955741882324
Validation loss: 2.174831444217313

Epoch: 5| Step: 7
Training loss: 1.975864052772522
Validation loss: 2.1790993508472236

Epoch: 5| Step: 8
Training loss: 2.0896379947662354
Validation loss: 2.1257725915601178

Epoch: 5| Step: 9
Training loss: 2.5266692638397217
Validation loss: 2.1439933366672967

Epoch: 5| Step: 10
Training loss: 1.8699004650115967
Validation loss: 2.153353429609729

Epoch: 94| Step: 0
Training loss: 1.906274437904358
Validation loss: 2.1530719777589202

Epoch: 5| Step: 1
Training loss: 2.1680586338043213
Validation loss: 2.133113168901013

Epoch: 5| Step: 2
Training loss: 2.29191255569458
Validation loss: 2.1629121611195226

Epoch: 5| Step: 3
Training loss: 3.071460247039795
Validation loss: 2.1510376725145566

Epoch: 5| Step: 4
Training loss: 2.5562736988067627
Validation loss: 2.14950152110028

Epoch: 5| Step: 5
Training loss: 2.3093161582946777
Validation loss: 2.136457843165244

Epoch: 5| Step: 6
Training loss: 1.8093712329864502
Validation loss: 2.1547411218766244

Epoch: 5| Step: 7
Training loss: 1.7020823955535889
Validation loss: 2.1391496619870587

Epoch: 5| Step: 8
Training loss: 1.6158552169799805
Validation loss: 2.1614542353537773

Epoch: 5| Step: 9
Training loss: 2.2576184272766113
Validation loss: 2.1410830687451106

Epoch: 5| Step: 10
Training loss: 2.8043463230133057
Validation loss: 2.1411511359676236

Epoch: 95| Step: 0
Training loss: 2.0055408477783203
Validation loss: 2.1437038965122674

Epoch: 5| Step: 1
Training loss: 1.9927024841308594
Validation loss: 2.1312316399748608

Epoch: 5| Step: 2
Training loss: 1.8517948389053345
Validation loss: 2.161005525178807

Epoch: 5| Step: 3
Training loss: 2.882009983062744
Validation loss: 2.1329359187874743

Epoch: 5| Step: 4
Training loss: 2.0423810482025146
Validation loss: 2.1363553975218084

Epoch: 5| Step: 5
Training loss: 2.555375576019287
Validation loss: 2.132611936138522

Epoch: 5| Step: 6
Training loss: 2.7324318885803223
Validation loss: 2.1530115117308912

Epoch: 5| Step: 7
Training loss: 1.9461135864257812
Validation loss: 2.141392095114595

Epoch: 5| Step: 8
Training loss: 1.8780046701431274
Validation loss: 2.1570931557686097

Epoch: 5| Step: 9
Training loss: 1.6815509796142578
Validation loss: 2.162988865247337

Epoch: 5| Step: 10
Training loss: 2.7868049144744873
Validation loss: 2.13851676705063

Epoch: 96| Step: 0
Training loss: 1.4403746128082275
Validation loss: 2.1460145801626225

Epoch: 5| Step: 1
Training loss: 2.5288195610046387
Validation loss: 2.1562106070979947

Epoch: 5| Step: 2
Training loss: 2.6567907333374023
Validation loss: 2.143603040326026

Epoch: 5| Step: 3
Training loss: 2.2997970581054688
Validation loss: 2.1513732043645715

Epoch: 5| Step: 4
Training loss: 1.6786658763885498
Validation loss: 2.1445563608600247

Epoch: 5| Step: 5
Training loss: 3.2065517902374268
Validation loss: 2.1607575237110095

Epoch: 5| Step: 6
Training loss: 2.567075252532959
Validation loss: 2.166840273846862

Epoch: 5| Step: 7
Training loss: 1.7731081247329712
Validation loss: 2.1477012993187032

Epoch: 5| Step: 8
Training loss: 2.100191116333008
Validation loss: 2.144378192963139

Epoch: 5| Step: 9
Training loss: 1.8013489246368408
Validation loss: 2.1338410659502913

Epoch: 5| Step: 10
Training loss: 2.112093210220337
Validation loss: 2.1423941017479025

Epoch: 97| Step: 0
Training loss: 2.0188167095184326
Validation loss: 2.146224405175896

Epoch: 5| Step: 1
Training loss: 2.8195254802703857
Validation loss: 2.1551917676002748

Epoch: 5| Step: 2
Training loss: 2.702467441558838
Validation loss: 2.1432894968217417

Epoch: 5| Step: 3
Training loss: 2.0895419120788574
Validation loss: 2.1465696698875836

Epoch: 5| Step: 4
Training loss: 2.552964687347412
Validation loss: 2.1632489312079644

Epoch: 5| Step: 5
Training loss: 1.5129674673080444
Validation loss: 2.151340041109311

Epoch: 5| Step: 6
Training loss: 2.7127718925476074
Validation loss: 2.138108968734741

Epoch: 5| Step: 7
Training loss: 2.108206272125244
Validation loss: 2.1441678821399646

Epoch: 5| Step: 8
Training loss: 2.499271869659424
Validation loss: 2.129322516020908

Epoch: 5| Step: 9
Training loss: 1.0982798337936401
Validation loss: 2.134626942296182

Epoch: 5| Step: 10
Training loss: 2.2241368293762207
Validation loss: 2.147478036983039

Epoch: 98| Step: 0
Training loss: 2.0328707695007324
Validation loss: 2.118217001679123

Epoch: 5| Step: 1
Training loss: 2.039669990539551
Validation loss: 2.151261821869881

Epoch: 5| Step: 2
Training loss: 2.5485730171203613
Validation loss: 2.132182723732405

Epoch: 5| Step: 3
Training loss: 1.9631659984588623
Validation loss: 2.1382010470154467

Epoch: 5| Step: 4
Training loss: 1.6397489309310913
Validation loss: 2.157637596130371

Epoch: 5| Step: 5
Training loss: 2.0071964263916016
Validation loss: 2.1377835350651897

Epoch: 5| Step: 6
Training loss: 2.429503917694092
Validation loss: 2.1215898247175318

Epoch: 5| Step: 7
Training loss: 2.397751569747925
Validation loss: 2.1626607423187583

Epoch: 5| Step: 8
Training loss: 2.3635032176971436
Validation loss: 2.1596127966398835

Epoch: 5| Step: 9
Training loss: 2.322427749633789
Validation loss: 2.1580427436418432

Epoch: 5| Step: 10
Training loss: 2.511286497116089
Validation loss: 2.1484883754484114

Epoch: 99| Step: 0
Training loss: 2.157076597213745
Validation loss: 2.128524059890419

Epoch: 5| Step: 1
Training loss: 1.755780577659607
Validation loss: 2.1303840683352564

Epoch: 5| Step: 2
Training loss: 2.328636646270752
Validation loss: 2.1406545023764334

Epoch: 5| Step: 3
Training loss: 2.9145655632019043
Validation loss: 2.154701199582828

Epoch: 5| Step: 4
Training loss: 1.4101113080978394
Validation loss: 2.1284583640354935

Epoch: 5| Step: 5
Training loss: 1.9023011922836304
Validation loss: 2.1398715306353826

Epoch: 5| Step: 6
Training loss: 2.5452094078063965
Validation loss: 2.1327216676486436

Epoch: 5| Step: 7
Training loss: 2.769941806793213
Validation loss: 2.131091728005358

Epoch: 5| Step: 8
Training loss: 2.521665096282959
Validation loss: 2.133321573657374

Epoch: 5| Step: 9
Training loss: 1.7495288848876953
Validation loss: 2.1599278603830645

Epoch: 5| Step: 10
Training loss: 2.161046266555786
Validation loss: 2.120780752551171

Epoch: 100| Step: 0
Training loss: 2.1712303161621094
Validation loss: 2.142487656685614

Epoch: 5| Step: 1
Training loss: 2.232999563217163
Validation loss: 2.1424301362806752

Epoch: 5| Step: 2
Training loss: 2.6173081398010254
Validation loss: 2.1441040090335313

Epoch: 5| Step: 3
Training loss: 2.0118420124053955
Validation loss: 2.1659409615301315

Epoch: 5| Step: 4
Training loss: 2.1147375106811523
Validation loss: 2.125747738345977

Epoch: 5| Step: 5
Training loss: 2.1345856189727783
Validation loss: 2.140886519544868

Epoch: 5| Step: 6
Training loss: 2.5286600589752197
Validation loss: 2.142398557355327

Epoch: 5| Step: 7
Training loss: 1.8086535930633545
Validation loss: 2.1503245010170886

Epoch: 5| Step: 8
Training loss: 2.1526036262512207
Validation loss: 2.1374898879758772

Epoch: 5| Step: 9
Training loss: 1.9835395812988281
Validation loss: 2.1471188068389893

Epoch: 5| Step: 10
Training loss: 2.507211208343506
Validation loss: 2.150502348458895

Epoch: 101| Step: 0
Training loss: 2.1537435054779053
Validation loss: 2.139377773448985

Epoch: 5| Step: 1
Training loss: 1.9496837854385376
Validation loss: 2.1515751731011177

Epoch: 5| Step: 2
Training loss: 2.143867254257202
Validation loss: 2.1429454716303016

Epoch: 5| Step: 3
Training loss: 2.464540958404541
Validation loss: 2.1370628713279642

Epoch: 5| Step: 4
Training loss: 1.4987365007400513
Validation loss: 2.154109747179093

Epoch: 5| Step: 5
Training loss: 2.3897597789764404
Validation loss: 2.1333854480456282

Epoch: 5| Step: 6
Training loss: 2.847292423248291
Validation loss: 2.168478969604738

Epoch: 5| Step: 7
Training loss: 2.311063766479492
Validation loss: 2.1379774642247025

Epoch: 5| Step: 8
Training loss: 2.0265555381774902
Validation loss: 2.1496098297898487

Epoch: 5| Step: 9
Training loss: 2.2015414237976074
Validation loss: 2.131386072404923

Epoch: 5| Step: 10
Training loss: 1.9902316331863403
Validation loss: 2.125734644551431

Epoch: 102| Step: 0
Training loss: 2.2139053344726562
Validation loss: 2.1704420556304274

Epoch: 5| Step: 1
Training loss: 2.5705463886260986
Validation loss: 2.1119121979641657

Epoch: 5| Step: 2
Training loss: 1.8116734027862549
Validation loss: 2.140080427610746

Epoch: 5| Step: 3
Training loss: 2.059443712234497
Validation loss: 2.1414831492208664

Epoch: 5| Step: 4
Training loss: 3.0200612545013428
Validation loss: 2.1460244706881944

Epoch: 5| Step: 5
Training loss: 2.0224225521087646
Validation loss: 2.1545933472212924

Epoch: 5| Step: 6
Training loss: 1.5099155902862549
Validation loss: 2.1505935679199877

Epoch: 5| Step: 7
Training loss: 2.6980905532836914
Validation loss: 2.142695620495786

Epoch: 5| Step: 8
Training loss: 2.2072088718414307
Validation loss: 2.1440671105538645

Epoch: 5| Step: 9
Training loss: 1.8501476049423218
Validation loss: 2.143358522845853

Epoch: 5| Step: 10
Training loss: 2.030440330505371
Validation loss: 2.128612140173553

Epoch: 103| Step: 0
Training loss: 1.7496986389160156
Validation loss: 2.128524767455234

Epoch: 5| Step: 1
Training loss: 2.5998198986053467
Validation loss: 2.144364510813067

Epoch: 5| Step: 2
Training loss: 1.9580978155136108
Validation loss: 2.1602370944074405

Epoch: 5| Step: 3
Training loss: 1.9420112371444702
Validation loss: 2.1384273882835143

Epoch: 5| Step: 4
Training loss: 2.700779914855957
Validation loss: 2.1399053706917712

Epoch: 5| Step: 5
Training loss: 2.1640114784240723
Validation loss: 2.1468103316522416

Epoch: 5| Step: 6
Training loss: 2.829172134399414
Validation loss: 2.1542106905291156

Epoch: 5| Step: 7
Training loss: 2.5110385417938232
Validation loss: 2.148283850762152

Epoch: 5| Step: 8
Training loss: 1.9537357091903687
Validation loss: 2.1339955509349866

Epoch: 5| Step: 9
Training loss: 1.7258942127227783
Validation loss: 2.1298123559644146

Epoch: 5| Step: 10
Training loss: 1.7079442739486694
Validation loss: 2.1456266295525337

Epoch: 104| Step: 0
Training loss: 2.4707140922546387
Validation loss: 2.1382985217596895

Epoch: 5| Step: 1
Training loss: 2.330923557281494
Validation loss: 2.127861281876923

Epoch: 5| Step: 2
Training loss: 1.7704098224639893
Validation loss: 2.1259781737481394

Epoch: 5| Step: 3
Training loss: 2.3056633472442627
Validation loss: 2.1634725627078804

Epoch: 5| Step: 4
Training loss: 1.6892849206924438
Validation loss: 2.1422627036289503

Epoch: 5| Step: 5
Training loss: 2.424895763397217
Validation loss: 2.1549805402755737

Epoch: 5| Step: 6
Training loss: 1.6955921649932861
Validation loss: 2.1535341226926414

Epoch: 5| Step: 7
Training loss: 2.1309726238250732
Validation loss: 2.1405911650708926

Epoch: 5| Step: 8
Training loss: 2.217419385910034
Validation loss: 2.1364623474818405

Epoch: 5| Step: 9
Training loss: 2.79729962348938
Validation loss: 2.123345721152521

Epoch: 5| Step: 10
Training loss: 2.203831434249878
Validation loss: 2.1484867654820925

Epoch: 105| Step: 0
Training loss: 2.3328537940979004
Validation loss: 2.1347382504452943

Epoch: 5| Step: 1
Training loss: 2.121100664138794
Validation loss: 2.1485176368426253

Epoch: 5| Step: 2
Training loss: 2.2312400341033936
Validation loss: 2.142527622561301

Epoch: 5| Step: 3
Training loss: 1.81007981300354
Validation loss: 2.1215630769729614

Epoch: 5| Step: 4
Training loss: 2.060595989227295
Validation loss: 2.1397051247217322

Epoch: 5| Step: 5
Training loss: 2.268479824066162
Validation loss: 2.1430271107663392

Epoch: 5| Step: 6
Training loss: 1.9479671716690063
Validation loss: 2.12957723294535

Epoch: 5| Step: 7
Training loss: 2.545222520828247
Validation loss: 2.1566420293623403

Epoch: 5| Step: 8
Training loss: 2.4564738273620605
Validation loss: 2.1565554321453138

Epoch: 5| Step: 9
Training loss: 2.128028631210327
Validation loss: 2.1371424787787983

Epoch: 5| Step: 10
Training loss: 2.079752206802368
Validation loss: 2.1385279650329263

Epoch: 106| Step: 0
Training loss: 2.4463562965393066
Validation loss: 2.150997979666597

Epoch: 5| Step: 1
Training loss: 2.2837133407592773
Validation loss: 2.1626170501914075

Epoch: 5| Step: 2
Training loss: 1.9117381572723389
Validation loss: 2.151996492057718

Epoch: 5| Step: 3
Training loss: 2.0841758251190186
Validation loss: 2.14534584681193

Epoch: 5| Step: 4
Training loss: 2.276394844055176
Validation loss: 2.1399950340229976

Epoch: 5| Step: 5
Training loss: 1.8326942920684814
Validation loss: 2.1357213233106878

Epoch: 5| Step: 6
Training loss: 2.3911757469177246
Validation loss: 2.1675959710151917

Epoch: 5| Step: 7
Training loss: 2.0391268730163574
Validation loss: 2.13770116272793

Epoch: 5| Step: 8
Training loss: 1.7286081314086914
Validation loss: 2.154464688352359

Epoch: 5| Step: 9
Training loss: 2.470710039138794
Validation loss: 2.1438450403110956

Epoch: 5| Step: 10
Training loss: 2.6496403217315674
Validation loss: 2.1645486636828353

Epoch: 107| Step: 0
Training loss: 1.8045142889022827
Validation loss: 2.128314038758637

Epoch: 5| Step: 1
Training loss: 1.541313648223877
Validation loss: 2.1335573888594106

Epoch: 5| Step: 2
Training loss: 2.198875904083252
Validation loss: 2.13324720885164

Epoch: 5| Step: 3
Training loss: 2.03664493560791
Validation loss: 2.127783429238104

Epoch: 5| Step: 4
Training loss: 2.5193614959716797
Validation loss: 2.14804421958103

Epoch: 5| Step: 5
Training loss: 2.198745012283325
Validation loss: 2.168645292200068

Epoch: 5| Step: 6
Training loss: 2.6187636852264404
Validation loss: 2.120851670542071

Epoch: 5| Step: 7
Training loss: 2.319200038909912
Validation loss: 2.152946606759102

Epoch: 5| Step: 8
Training loss: 1.9891679286956787
Validation loss: 2.155085802078247

Epoch: 5| Step: 9
Training loss: 2.2213027477264404
Validation loss: 2.134653796431839

Epoch: 5| Step: 10
Training loss: 2.365478038787842
Validation loss: 2.133952389481247

Epoch: 108| Step: 0
Training loss: 2.1416831016540527
Validation loss: 2.137287373183876

Epoch: 5| Step: 1
Training loss: 1.8626134395599365
Validation loss: 2.131398913680866

Epoch: 5| Step: 2
Training loss: 2.0482559204101562
Validation loss: 2.1484228257210023

Epoch: 5| Step: 3
Training loss: 1.7905094623565674
Validation loss: 2.150876345173005

Epoch: 5| Step: 4
Training loss: 1.9856195449829102
Validation loss: 2.117185124786951

Epoch: 5| Step: 5
Training loss: 2.214254140853882
Validation loss: 2.134072267881004

Epoch: 5| Step: 6
Training loss: 2.527101755142212
Validation loss: 2.152683272156664

Epoch: 5| Step: 7
Training loss: 2.859321355819702
Validation loss: 2.1353725092385405

Epoch: 5| Step: 8
Training loss: 2.385664463043213
Validation loss: 2.144555263621833

Epoch: 5| Step: 9
Training loss: 2.371912717819214
Validation loss: 2.1330372877018426

Epoch: 5| Step: 10
Training loss: 1.5253150463104248
Validation loss: 2.132334509203511

Epoch: 109| Step: 0
Training loss: 2.2402737140655518
Validation loss: 2.1226390587386263

Epoch: 5| Step: 1
Training loss: 1.7447922229766846
Validation loss: 2.1295415573222662

Epoch: 5| Step: 2
Training loss: 2.1544735431671143
Validation loss: 2.142632835654802

Epoch: 5| Step: 3
Training loss: 1.9725487232208252
Validation loss: 2.126519804359764

Epoch: 5| Step: 4
Training loss: 2.1943442821502686
Validation loss: 2.160641575372347

Epoch: 5| Step: 5
Training loss: 2.5608086585998535
Validation loss: 2.118378164947674

Epoch: 5| Step: 6
Training loss: 2.387615203857422
Validation loss: 2.1317899073323896

Epoch: 5| Step: 7
Training loss: 2.422715425491333
Validation loss: 2.1668914646230717

Epoch: 5| Step: 8
Training loss: 1.5353389978408813
Validation loss: 2.1548441558755855

Epoch: 5| Step: 9
Training loss: 2.338827133178711
Validation loss: 2.139629681905111

Epoch: 5| Step: 10
Training loss: 2.1670758724212646
Validation loss: 2.141623550845731

Epoch: 110| Step: 0
Training loss: 2.4793038368225098
Validation loss: 2.145318950376203

Epoch: 5| Step: 1
Training loss: 1.6384670734405518
Validation loss: 2.13521009619518

Epoch: 5| Step: 2
Training loss: 2.4316446781158447
Validation loss: 2.1535853916598904

Epoch: 5| Step: 3
Training loss: 2.8045802116394043
Validation loss: 2.142534239317781

Epoch: 5| Step: 4
Training loss: 1.6200487613677979
Validation loss: 2.158774865570889

Epoch: 5| Step: 5
Training loss: 2.4987881183624268
Validation loss: 2.143842207488193

Epoch: 5| Step: 6
Training loss: 2.5740694999694824
Validation loss: 2.1203350033811343

Epoch: 5| Step: 7
Training loss: 1.6570669412612915
Validation loss: 2.117165478326941

Epoch: 5| Step: 8
Training loss: 1.4990918636322021
Validation loss: 2.140519795879241

Epoch: 5| Step: 9
Training loss: 2.447169780731201
Validation loss: 2.1727938562311153

Epoch: 5| Step: 10
Training loss: 2.28381609916687
Validation loss: 2.1216962516948743

Epoch: 111| Step: 0
Training loss: 1.2925151586532593
Validation loss: 2.1327557025417203

Epoch: 5| Step: 1
Training loss: 2.3484718799591064
Validation loss: 2.117167659985122

Epoch: 5| Step: 2
Training loss: 1.8500149250030518
Validation loss: 2.1303305215733026

Epoch: 5| Step: 3
Training loss: 1.889299750328064
Validation loss: 2.1526235611208024

Epoch: 5| Step: 4
Training loss: 2.2275683879852295
Validation loss: 2.136516854327212

Epoch: 5| Step: 5
Training loss: 2.083810567855835
Validation loss: 2.1349706495961835

Epoch: 5| Step: 6
Training loss: 2.6283071041107178
Validation loss: 2.1468204439327283

Epoch: 5| Step: 7
Training loss: 2.4179835319519043
Validation loss: 2.1447061236186693

Epoch: 5| Step: 8
Training loss: 2.3568997383117676
Validation loss: 2.102924628924298

Epoch: 5| Step: 9
Training loss: 2.3587257862091064
Validation loss: 2.121688945319063

Epoch: 5| Step: 10
Training loss: 2.3515632152557373
Validation loss: 2.1474972053240706

Epoch: 112| Step: 0
Training loss: 2.6504828929901123
Validation loss: 2.1507187607467815

Epoch: 5| Step: 1
Training loss: 2.23313570022583
Validation loss: 2.158699103581008

Epoch: 5| Step: 2
Training loss: 2.1212422847747803
Validation loss: 2.137246480552099

Epoch: 5| Step: 3
Training loss: 1.9063323736190796
Validation loss: 2.139005357219327

Epoch: 5| Step: 4
Training loss: 1.3357932567596436
Validation loss: 2.135702286997149

Epoch: 5| Step: 5
Training loss: 2.1351146697998047
Validation loss: 2.144054551278391

Epoch: 5| Step: 6
Training loss: 1.924669623374939
Validation loss: 2.117615948441208

Epoch: 5| Step: 7
Training loss: 2.1688458919525146
Validation loss: 2.1320039610708914

Epoch: 5| Step: 8
Training loss: 3.0549941062927246
Validation loss: 2.131610340969537

Epoch: 5| Step: 9
Training loss: 1.9076478481292725
Validation loss: 2.133074134908697

Epoch: 5| Step: 10
Training loss: 2.312776803970337
Validation loss: 2.113053678184427

Epoch: 113| Step: 0
Training loss: 2.1803886890411377
Validation loss: 2.1260804450640114

Epoch: 5| Step: 1
Training loss: 1.3250372409820557
Validation loss: 2.1326064755839687

Epoch: 5| Step: 2
Training loss: 2.5788888931274414
Validation loss: 2.1079955331740843

Epoch: 5| Step: 3
Training loss: 1.9032741785049438
Validation loss: 2.1391812204032816

Epoch: 5| Step: 4
Training loss: 2.571044921875
Validation loss: 2.1445072133054017

Epoch: 5| Step: 5
Training loss: 2.3661160469055176
Validation loss: 2.1275340703225907

Epoch: 5| Step: 6
Training loss: 2.749419689178467
Validation loss: 2.123817200301796

Epoch: 5| Step: 7
Training loss: 1.3249425888061523
Validation loss: 2.12548614317371

Epoch: 5| Step: 8
Training loss: 2.1641950607299805
Validation loss: 2.134531230054876

Epoch: 5| Step: 9
Training loss: 2.448366641998291
Validation loss: 2.1262582540512085

Epoch: 5| Step: 10
Training loss: 2.191514492034912
Validation loss: 2.1341607532193585

Epoch: 114| Step: 0
Training loss: 2.159534454345703
Validation loss: 2.1386891385560394

Epoch: 5| Step: 1
Training loss: 2.917989492416382
Validation loss: 2.1431782476363646

Epoch: 5| Step: 2
Training loss: 2.163403034210205
Validation loss: 2.155710120354929

Epoch: 5| Step: 3
Training loss: 1.6269662380218506
Validation loss: 2.153554365199099

Epoch: 5| Step: 4
Training loss: 2.3749144077301025
Validation loss: 2.139524024019959

Epoch: 5| Step: 5
Training loss: 2.501217842102051
Validation loss: 2.1247932205918016

Epoch: 5| Step: 6
Training loss: 1.8316776752471924
Validation loss: 2.1268359948230047

Epoch: 5| Step: 7
Training loss: 1.9494788646697998
Validation loss: 2.1593871821639357

Epoch: 5| Step: 8
Training loss: 1.6584970951080322
Validation loss: 2.123441574394062

Epoch: 5| Step: 9
Training loss: 2.308100938796997
Validation loss: 2.1312321437302457

Epoch: 5| Step: 10
Training loss: 2.3638522624969482
Validation loss: 2.133295287368118

Epoch: 115| Step: 0
Training loss: 2.0705153942108154
Validation loss: 2.162171229239433

Epoch: 5| Step: 1
Training loss: 1.3622573614120483
Validation loss: 2.1173888342354887

Epoch: 5| Step: 2
Training loss: 2.252305507659912
Validation loss: 2.132384879614717

Epoch: 5| Step: 3
Training loss: 2.8004019260406494
Validation loss: 2.1586676156649025

Epoch: 5| Step: 4
Training loss: 1.8109283447265625
Validation loss: 2.126956219314247

Epoch: 5| Step: 5
Training loss: 2.308988094329834
Validation loss: 2.1293099618727163

Epoch: 5| Step: 6
Training loss: 2.303266763687134
Validation loss: 2.1116425029693113

Epoch: 5| Step: 7
Training loss: 2.199427843093872
Validation loss: 2.116797517704707

Epoch: 5| Step: 8
Training loss: 2.4527900218963623
Validation loss: 2.130835084504979

Epoch: 5| Step: 9
Training loss: 1.952722191810608
Validation loss: 2.140904805993521

Epoch: 5| Step: 10
Training loss: 2.1439452171325684
Validation loss: 2.1594218413035073

Epoch: 116| Step: 0
Training loss: 1.9396450519561768
Validation loss: 2.1356052890900643

Epoch: 5| Step: 1
Training loss: 1.6057384014129639
Validation loss: 2.118301922275174

Epoch: 5| Step: 2
Training loss: 1.4368572235107422
Validation loss: 2.1195870202074767

Epoch: 5| Step: 3
Training loss: 1.9478800296783447
Validation loss: 2.135991011896441

Epoch: 5| Step: 4
Training loss: 2.3906238079071045
Validation loss: 2.1236359355270222

Epoch: 5| Step: 5
Training loss: 2.1765873432159424
Validation loss: 2.1468290052106305

Epoch: 5| Step: 6
Training loss: 2.5467467308044434
Validation loss: 2.135572871854228

Epoch: 5| Step: 7
Training loss: 2.5481162071228027
Validation loss: 2.1430573283985095

Epoch: 5| Step: 8
Training loss: 2.5916285514831543
Validation loss: 2.128506613034074

Epoch: 5| Step: 9
Training loss: 2.681732654571533
Validation loss: 2.108995927277432

Epoch: 5| Step: 10
Training loss: 1.6633069515228271
Validation loss: 2.124388728090512

Epoch: 117| Step: 0
Training loss: 1.8649380207061768
Validation loss: 2.1191341338619107

Epoch: 5| Step: 1
Training loss: 1.8685213327407837
Validation loss: 2.1482947744348997

Epoch: 5| Step: 2
Training loss: 1.5024205446243286
Validation loss: 2.1500206403834845

Epoch: 5| Step: 3
Training loss: 2.5096049308776855
Validation loss: 2.138629453156584

Epoch: 5| Step: 4
Training loss: 2.548175811767578
Validation loss: 2.1347478615340365

Epoch: 5| Step: 5
Training loss: 2.257565975189209
Validation loss: 2.15193001685604

Epoch: 5| Step: 6
Training loss: 1.9242713451385498
Validation loss: 2.1252366650489067

Epoch: 5| Step: 7
Training loss: 2.5081276893615723
Validation loss: 2.1445071927962767

Epoch: 5| Step: 8
Training loss: 2.5128540992736816
Validation loss: 2.131987953698763

Epoch: 5| Step: 9
Training loss: 2.1138992309570312
Validation loss: 2.1252108363695044

Epoch: 5| Step: 10
Training loss: 2.0376040935516357
Validation loss: 2.123353850456976

Epoch: 118| Step: 0
Training loss: 2.219578981399536
Validation loss: 2.1379725715165496

Epoch: 5| Step: 1
Training loss: 2.092742443084717
Validation loss: 2.1178843628975654

Epoch: 5| Step: 2
Training loss: 2.3492140769958496
Validation loss: 2.1124862227388608

Epoch: 5| Step: 3
Training loss: 1.598317265510559
Validation loss: 2.1368954822581303

Epoch: 5| Step: 4
Training loss: 2.214787721633911
Validation loss: 2.1313409907843477

Epoch: 5| Step: 5
Training loss: 2.151303291320801
Validation loss: 2.137665505050331

Epoch: 5| Step: 6
Training loss: 2.11590576171875
Validation loss: 2.1395324814704155

Epoch: 5| Step: 7
Training loss: 2.033438205718994
Validation loss: 2.143655168112888

Epoch: 5| Step: 8
Training loss: 2.2707371711730957
Validation loss: 2.1583006023078837

Epoch: 5| Step: 9
Training loss: 2.5782127380371094
Validation loss: 2.1425544459332704

Epoch: 5| Step: 10
Training loss: 2.1073262691497803
Validation loss: 2.138786429999977

Epoch: 119| Step: 0
Training loss: 1.7001968622207642
Validation loss: 2.1355478404670634

Epoch: 5| Step: 1
Training loss: 2.3953213691711426
Validation loss: 2.1131083888392292

Epoch: 5| Step: 2
Training loss: 2.170107841491699
Validation loss: 2.128385874532884

Epoch: 5| Step: 3
Training loss: 2.1151092052459717
Validation loss: 2.1624023939973567

Epoch: 5| Step: 4
Training loss: 1.8528845310211182
Validation loss: 2.129665267082953

Epoch: 5| Step: 5
Training loss: 1.989833116531372
Validation loss: 2.130738689053443

Epoch: 5| Step: 6
Training loss: 2.694061040878296
Validation loss: 2.147627233177103

Epoch: 5| Step: 7
Training loss: 2.000797986984253
Validation loss: 2.1349808400677097

Epoch: 5| Step: 8
Training loss: 2.4078192710876465
Validation loss: 2.139600497420116

Epoch: 5| Step: 9
Training loss: 2.1268467903137207
Validation loss: 2.14744661700341

Epoch: 5| Step: 10
Training loss: 1.9890968799591064
Validation loss: 2.1337467752477175

Epoch: 120| Step: 0
Training loss: 2.1576907634735107
Validation loss: 2.1299560557129564

Epoch: 5| Step: 1
Training loss: 2.402383327484131
Validation loss: 2.1321248854360273

Epoch: 5| Step: 2
Training loss: 2.3081417083740234
Validation loss: 2.0857088950372513

Epoch: 5| Step: 3
Training loss: 2.4005188941955566
Validation loss: 2.10355370788164

Epoch: 5| Step: 4
Training loss: 2.2372753620147705
Validation loss: 2.138391702405868

Epoch: 5| Step: 5
Training loss: 1.797589659690857
Validation loss: 2.1278920327463458

Epoch: 5| Step: 6
Training loss: 1.766175627708435
Validation loss: 2.1236398284153273

Epoch: 5| Step: 7
Training loss: 2.045384645462036
Validation loss: 2.122956896340975

Epoch: 5| Step: 8
Training loss: 2.794917583465576
Validation loss: 2.118380864461263

Epoch: 5| Step: 9
Training loss: 1.8439610004425049
Validation loss: 2.107207913552561

Epoch: 5| Step: 10
Training loss: 1.886284589767456
Validation loss: 2.1214774065120245

Epoch: 121| Step: 0
Training loss: 1.8619247674942017
Validation loss: 2.1235351703500234

Epoch: 5| Step: 1
Training loss: 1.7731189727783203
Validation loss: 2.109699720977455

Epoch: 5| Step: 2
Training loss: 2.0850706100463867
Validation loss: 2.130408940776702

Epoch: 5| Step: 3
Training loss: 1.854870080947876
Validation loss: 2.144191772707047

Epoch: 5| Step: 4
Training loss: 2.687398672103882
Validation loss: 2.143158737049308

Epoch: 5| Step: 5
Training loss: 2.0668110847473145
Validation loss: 2.144160807773631

Epoch: 5| Step: 6
Training loss: 1.8944813013076782
Validation loss: 2.1412838505160425

Epoch: 5| Step: 7
Training loss: 2.4772143363952637
Validation loss: 2.132777020495425

Epoch: 5| Step: 8
Training loss: 2.406557559967041
Validation loss: 2.1294082082727903

Epoch: 5| Step: 9
Training loss: 2.0182652473449707
Validation loss: 2.138505442168123

Epoch: 5| Step: 10
Training loss: 2.3033089637756348
Validation loss: 2.1304260594870454

Epoch: 122| Step: 0
Training loss: 2.579374313354492
Validation loss: 2.1378460789239533

Epoch: 5| Step: 1
Training loss: 2.2804362773895264
Validation loss: 2.13898181018009

Epoch: 5| Step: 2
Training loss: 2.205357074737549
Validation loss: 2.102629955096911

Epoch: 5| Step: 3
Training loss: 2.3919591903686523
Validation loss: 2.1087475002452893

Epoch: 5| Step: 4
Training loss: 2.5147147178649902
Validation loss: 2.1169653592571134

Epoch: 5| Step: 5
Training loss: 2.401984214782715
Validation loss: 2.1437769461703557

Epoch: 5| Step: 6
Training loss: 2.0401225090026855
Validation loss: 2.14704962699644

Epoch: 5| Step: 7
Training loss: 1.5318704843521118
Validation loss: 2.141266789487613

Epoch: 5| Step: 8
Training loss: 1.7463829517364502
Validation loss: 2.101259304631141

Epoch: 5| Step: 9
Training loss: 2.0963082313537598
Validation loss: 2.150266179474451

Epoch: 5| Step: 10
Training loss: 1.5373284816741943
Validation loss: 2.149770199611623

Epoch: 123| Step: 0
Training loss: 1.888973593711853
Validation loss: 2.123309809674499

Epoch: 5| Step: 1
Training loss: 1.8766065835952759
Validation loss: 2.16316113164348

Epoch: 5| Step: 2
Training loss: 2.020724058151245
Validation loss: 2.1142011483510337

Epoch: 5| Step: 3
Training loss: 2.584871292114258
Validation loss: 2.125671843046783

Epoch: 5| Step: 4
Training loss: 2.170297622680664
Validation loss: 2.158778795631983

Epoch: 5| Step: 5
Training loss: 2.095764636993408
Validation loss: 2.140815301608014

Epoch: 5| Step: 6
Training loss: 2.008082866668701
Validation loss: 2.1334099743955877

Epoch: 5| Step: 7
Training loss: 1.6520605087280273
Validation loss: 2.1252473631212787

Epoch: 5| Step: 8
Training loss: 2.841522216796875
Validation loss: 2.1299232077854935

Epoch: 5| Step: 9
Training loss: 2.0461323261260986
Validation loss: 2.11288918090123

Epoch: 5| Step: 10
Training loss: 2.3762693405151367
Validation loss: 2.143332765948388

Epoch: 124| Step: 0
Training loss: 2.146622896194458
Validation loss: 2.1245033125723563

Epoch: 5| Step: 1
Training loss: 2.522055149078369
Validation loss: 2.1124009663058865

Epoch: 5| Step: 2
Training loss: 2.0457334518432617
Validation loss: 2.1569627651604275

Epoch: 5| Step: 3
Training loss: 2.0392794609069824
Validation loss: 2.1098285900649203

Epoch: 5| Step: 4
Training loss: 2.3186049461364746
Validation loss: 2.122710648403373

Epoch: 5| Step: 5
Training loss: 2.510061502456665
Validation loss: 2.150128279962847

Epoch: 5| Step: 6
Training loss: 1.4169971942901611
Validation loss: 2.1333363466365363

Epoch: 5| Step: 7
Training loss: 2.2440948486328125
Validation loss: 2.131855200695735

Epoch: 5| Step: 8
Training loss: 1.8367668390274048
Validation loss: 2.127349207478185

Epoch: 5| Step: 9
Training loss: 1.9138050079345703
Validation loss: 2.1168615395022976

Epoch: 5| Step: 10
Training loss: 2.519383668899536
Validation loss: 2.125622808292348

Epoch: 125| Step: 0
Training loss: 1.898249626159668
Validation loss: 2.1433018099877144

Epoch: 5| Step: 1
Training loss: 1.8454840183258057
Validation loss: 2.1105470118984098

Epoch: 5| Step: 2
Training loss: 2.0971224308013916
Validation loss: 2.138644354317778

Epoch: 5| Step: 3
Training loss: 2.7794346809387207
Validation loss: 2.1203596720131497

Epoch: 5| Step: 4
Training loss: 1.9439111948013306
Validation loss: 2.1187019322508123

Epoch: 5| Step: 5
Training loss: 1.8925228118896484
Validation loss: 2.1254950159339496

Epoch: 5| Step: 6
Training loss: 2.0834031105041504
Validation loss: 2.134092629596751

Epoch: 5| Step: 7
Training loss: 1.4952424764633179
Validation loss: 2.1374531676692348

Epoch: 5| Step: 8
Training loss: 2.6955106258392334
Validation loss: 2.1475076188323317

Epoch: 5| Step: 9
Training loss: 2.4423069953918457
Validation loss: 2.1234340488269763

Epoch: 5| Step: 10
Training loss: 2.1172122955322266
Validation loss: 2.1363095186089955

Epoch: 126| Step: 0
Training loss: 2.744502067565918
Validation loss: 2.1107310351505073

Epoch: 5| Step: 1
Training loss: 2.853283166885376
Validation loss: 2.1314794350695867

Epoch: 5| Step: 2
Training loss: 2.484403371810913
Validation loss: 2.114891252210063

Epoch: 5| Step: 3
Training loss: 2.280848979949951
Validation loss: 2.126623075495484

Epoch: 5| Step: 4
Training loss: 1.4824364185333252
Validation loss: 2.144061715372147

Epoch: 5| Step: 5
Training loss: 2.149428129196167
Validation loss: 2.1222917392689693

Epoch: 5| Step: 6
Training loss: 2.162198543548584
Validation loss: 2.142716471866895

Epoch: 5| Step: 7
Training loss: 1.7987308502197266
Validation loss: 2.1658170453963743

Epoch: 5| Step: 8
Training loss: 1.6485782861709595
Validation loss: 2.1328745862489105

Epoch: 5| Step: 9
Training loss: 1.4592390060424805
Validation loss: 2.1472161739103255

Epoch: 5| Step: 10
Training loss: 2.348961353302002
Validation loss: 2.1300392586697816

Epoch: 127| Step: 0
Training loss: 1.8852717876434326
Validation loss: 2.122093641629783

Epoch: 5| Step: 1
Training loss: 2.067903518676758
Validation loss: 2.157091330456477

Epoch: 5| Step: 2
Training loss: 1.343239188194275
Validation loss: 2.1355387395428074

Epoch: 5| Step: 3
Training loss: 1.9039462804794312
Validation loss: 2.1046240880925167

Epoch: 5| Step: 4
Training loss: 2.134373188018799
Validation loss: 2.138122304793327

Epoch: 5| Step: 5
Training loss: 2.330392599105835
Validation loss: 2.1245533317647953

Epoch: 5| Step: 6
Training loss: 2.8834023475646973
Validation loss: 2.120329422335471

Epoch: 5| Step: 7
Training loss: 2.275045871734619
Validation loss: 2.1188658193875383

Epoch: 5| Step: 8
Training loss: 2.3816256523132324
Validation loss: 2.122741614618609

Epoch: 5| Step: 9
Training loss: 2.016603946685791
Validation loss: 2.126911847822128

Epoch: 5| Step: 10
Training loss: 2.0818068981170654
Validation loss: 2.123116280442925

Epoch: 128| Step: 0
Training loss: 2.40086030960083
Validation loss: 2.0941068613401024

Epoch: 5| Step: 1
Training loss: 2.0964694023132324
Validation loss: 2.120728502991379

Epoch: 5| Step: 2
Training loss: 1.596567153930664
Validation loss: 2.1099305870712444

Epoch: 5| Step: 3
Training loss: 2.0735690593719482
Validation loss: 2.1179308365750056

Epoch: 5| Step: 4
Training loss: 2.2344002723693848
Validation loss: 2.127858764381819

Epoch: 5| Step: 5
Training loss: 2.8053622245788574
Validation loss: 2.129113970264312

Epoch: 5| Step: 6
Training loss: 1.8924200534820557
Validation loss: 2.1005241640152468

Epoch: 5| Step: 7
Training loss: 1.6383033990859985
Validation loss: 2.129801824528684

Epoch: 5| Step: 8
Training loss: 2.151071548461914
Validation loss: 2.120330254236857

Epoch: 5| Step: 9
Training loss: 1.9116184711456299
Validation loss: 2.1186439555178405

Epoch: 5| Step: 10
Training loss: 2.4054782390594482
Validation loss: 2.1377686172403316

Epoch: 129| Step: 0
Training loss: 2.5775070190429688
Validation loss: 2.1378709193198913

Epoch: 5| Step: 1
Training loss: 2.6566710472106934
Validation loss: 2.144975223848897

Epoch: 5| Step: 2
Training loss: 1.6812461614608765
Validation loss: 2.1423179757210518

Epoch: 5| Step: 3
Training loss: 1.893072485923767
Validation loss: 2.1335332893556163

Epoch: 5| Step: 4
Training loss: 1.5340969562530518
Validation loss: 2.1307530198045956

Epoch: 5| Step: 5
Training loss: 2.047794818878174
Validation loss: 2.1152006259528537

Epoch: 5| Step: 6
Training loss: 2.1605751514434814
Validation loss: 2.132853995087326

Epoch: 5| Step: 7
Training loss: 1.7399975061416626
Validation loss: 2.1154986402039886

Epoch: 5| Step: 8
Training loss: 2.0811429023742676
Validation loss: 2.1169169718219387

Epoch: 5| Step: 9
Training loss: 2.630223274230957
Validation loss: 2.116230677532893

Epoch: 5| Step: 10
Training loss: 2.17448353767395
Validation loss: 2.121861230942511

Epoch: 130| Step: 0
Training loss: 2.3946938514709473
Validation loss: 2.1330516235802763

Epoch: 5| Step: 1
Training loss: 2.584521770477295
Validation loss: 2.1102934524577153

Epoch: 5| Step: 2
Training loss: 2.3439066410064697
Validation loss: 2.1180671389384935

Epoch: 5| Step: 3
Training loss: 1.8520702123641968
Validation loss: 2.1048738469359694

Epoch: 5| Step: 4
Training loss: 2.1647586822509766
Validation loss: 2.101486990528722

Epoch: 5| Step: 5
Training loss: 2.0425424575805664
Validation loss: 2.13969269106465

Epoch: 5| Step: 6
Training loss: 1.6022050380706787
Validation loss: 2.113315828384892

Epoch: 5| Step: 7
Training loss: 2.6848795413970947
Validation loss: 2.1348916638282036

Epoch: 5| Step: 8
Training loss: 1.7618087530136108
Validation loss: 2.124787397282098

Epoch: 5| Step: 9
Training loss: 1.981437087059021
Validation loss: 2.1377719897095875

Epoch: 5| Step: 10
Training loss: 1.6164634227752686
Validation loss: 2.1303273708589616

Epoch: 131| Step: 0
Training loss: 1.7800700664520264
Validation loss: 2.1372875193113923

Epoch: 5| Step: 1
Training loss: 2.2082679271698
Validation loss: 2.127839549895256

Epoch: 5| Step: 2
Training loss: 1.9485021829605103
Validation loss: 2.121889281016524

Epoch: 5| Step: 3
Training loss: 2.212595224380493
Validation loss: 2.1114832406402915

Epoch: 5| Step: 4
Training loss: 2.1948866844177246
Validation loss: 2.131916771652878

Epoch: 5| Step: 5
Training loss: 3.0454273223876953
Validation loss: 2.103141212976107

Epoch: 5| Step: 6
Training loss: 2.0432968139648438
Validation loss: 2.1058178947817896

Epoch: 5| Step: 7
Training loss: 1.8421392440795898
Validation loss: 2.108337592053157

Epoch: 5| Step: 8
Training loss: 2.130553722381592
Validation loss: 2.129451074907857

Epoch: 5| Step: 9
Training loss: 1.5650453567504883
Validation loss: 2.128271258005532

Epoch: 5| Step: 10
Training loss: 2.2542619705200195
Validation loss: 2.1286980746894755

Epoch: 132| Step: 0
Training loss: 2.6112825870513916
Validation loss: 2.118849087786931

Epoch: 5| Step: 1
Training loss: 1.5260576009750366
Validation loss: 2.112208956031389

Epoch: 5| Step: 2
Training loss: 2.228635311126709
Validation loss: 2.1136694210831837

Epoch: 5| Step: 3
Training loss: 2.3171980381011963
Validation loss: 2.1148549228586178

Epoch: 5| Step: 4
Training loss: 2.1315665245056152
Validation loss: 2.1126964476800736

Epoch: 5| Step: 5
Training loss: 1.8471606969833374
Validation loss: 2.120999382388207

Epoch: 5| Step: 6
Training loss: 2.033953905105591
Validation loss: 2.093524755970124

Epoch: 5| Step: 7
Training loss: 2.0178208351135254
Validation loss: 2.107464617298495

Epoch: 5| Step: 8
Training loss: 1.7177276611328125
Validation loss: 2.1314939157937163

Epoch: 5| Step: 9
Training loss: 2.1636135578155518
Validation loss: 2.119714529283585

Epoch: 5| Step: 10
Training loss: 2.6789417266845703
Validation loss: 2.1014606875758015

Epoch: 133| Step: 0
Training loss: 1.9601361751556396
Validation loss: 2.134663830521286

Epoch: 5| Step: 1
Training loss: 1.864403486251831
Validation loss: 2.101275279957761

Epoch: 5| Step: 2
Training loss: 2.247474431991577
Validation loss: 2.1343566589458014

Epoch: 5| Step: 3
Training loss: 2.4886841773986816
Validation loss: 2.1517155785714426

Epoch: 5| Step: 4
Training loss: 2.726759433746338
Validation loss: 2.122889418755808

Epoch: 5| Step: 5
Training loss: 2.1807312965393066
Validation loss: 2.1487521202333513

Epoch: 5| Step: 6
Training loss: 2.198124647140503
Validation loss: 2.1255331462429417

Epoch: 5| Step: 7
Training loss: 1.9722540378570557
Validation loss: 2.1066885507234963

Epoch: 5| Step: 8
Training loss: 1.7823861837387085
Validation loss: 2.137299919641146

Epoch: 5| Step: 9
Training loss: 1.7057666778564453
Validation loss: 2.129303188734157

Epoch: 5| Step: 10
Training loss: 1.7123000621795654
Validation loss: 2.098829036117882

Epoch: 134| Step: 0
Training loss: 2.2529430389404297
Validation loss: 2.0957200809191634

Epoch: 5| Step: 1
Training loss: 2.1252689361572266
Validation loss: 2.1216986563897904

Epoch: 5| Step: 2
Training loss: 1.6529181003570557
Validation loss: 2.1074121459837882

Epoch: 5| Step: 3
Training loss: 1.884808897972107
Validation loss: 2.1096822164391957

Epoch: 5| Step: 4
Training loss: 2.689854860305786
Validation loss: 2.1060808063835226

Epoch: 5| Step: 5
Training loss: 2.538236379623413
Validation loss: 2.1209812266852266

Epoch: 5| Step: 6
Training loss: 1.9636932611465454
Validation loss: 2.1039017272251908

Epoch: 5| Step: 7
Training loss: 2.3403677940368652
Validation loss: 2.1354051892475416

Epoch: 5| Step: 8
Training loss: 1.7185983657836914
Validation loss: 2.113579104023595

Epoch: 5| Step: 9
Training loss: 1.9504696130752563
Validation loss: 2.1245993273232573

Epoch: 5| Step: 10
Training loss: 1.9935274124145508
Validation loss: 2.1053633792425996

Epoch: 135| Step: 0
Training loss: 1.9498703479766846
Validation loss: 2.092024923652731

Epoch: 5| Step: 1
Training loss: 2.584730863571167
Validation loss: 2.086299374539365

Epoch: 5| Step: 2
Training loss: 1.5888142585754395
Validation loss: 2.1052501073447605

Epoch: 5| Step: 3
Training loss: 2.4714057445526123
Validation loss: 2.1116830636096258

Epoch: 5| Step: 4
Training loss: 1.8922951221466064
Validation loss: 2.11121025136722

Epoch: 5| Step: 5
Training loss: 1.8860238790512085
Validation loss: 2.119044127002839

Epoch: 5| Step: 6
Training loss: 2.335258960723877
Validation loss: 2.1015110323506017

Epoch: 5| Step: 7
Training loss: 2.1042609214782715
Validation loss: 2.1133742768277406

Epoch: 5| Step: 8
Training loss: 2.3179943561553955
Validation loss: 2.1289305084495136

Epoch: 5| Step: 9
Training loss: 2.035439968109131
Validation loss: 2.1085702219317035

Epoch: 5| Step: 10
Training loss: 1.885125756263733
Validation loss: 2.123334538552069

Epoch: 136| Step: 0
Training loss: 2.1101837158203125
Validation loss: 2.1155003501522924

Epoch: 5| Step: 1
Training loss: 2.5412774085998535
Validation loss: 2.1274784329116985

Epoch: 5| Step: 2
Training loss: 2.132776975631714
Validation loss: 2.1072661543405182

Epoch: 5| Step: 3
Training loss: 1.9529300928115845
Validation loss: 2.1027578923010055

Epoch: 5| Step: 4
Training loss: 2.7366232872009277
Validation loss: 2.10479002101447

Epoch: 5| Step: 5
Training loss: 1.4647605419158936
Validation loss: 2.105013812741926

Epoch: 5| Step: 6
Training loss: 2.334395170211792
Validation loss: 2.1223327216281684

Epoch: 5| Step: 7
Training loss: 1.7208147048950195
Validation loss: 2.124057703120734

Epoch: 5| Step: 8
Training loss: 1.5649763345718384
Validation loss: 2.093704136469031

Epoch: 5| Step: 9
Training loss: 2.5739734172821045
Validation loss: 2.0923699204639723

Epoch: 5| Step: 10
Training loss: 1.9582183361053467
Validation loss: 2.1006685123648694

Epoch: 137| Step: 0
Training loss: 2.226668119430542
Validation loss: 2.1191370717940794

Epoch: 5| Step: 1
Training loss: 2.4621617794036865
Validation loss: 2.099286416525482

Epoch: 5| Step: 2
Training loss: 1.7612377405166626
Validation loss: 2.105741423945273

Epoch: 5| Step: 3
Training loss: 1.9690786600112915
Validation loss: 2.10780809387084

Epoch: 5| Step: 4
Training loss: 1.9938656091690063
Validation loss: 2.107422145464087

Epoch: 5| Step: 5
Training loss: 2.0157787799835205
Validation loss: 2.108479074252549

Epoch: 5| Step: 6
Training loss: 2.3049206733703613
Validation loss: 2.0937540684976885

Epoch: 5| Step: 7
Training loss: 1.6575863361358643
Validation loss: 2.131482670384069

Epoch: 5| Step: 8
Training loss: 2.2045319080352783
Validation loss: 2.104600860226539

Epoch: 5| Step: 9
Training loss: 2.312824249267578
Validation loss: 2.097673771201923

Epoch: 5| Step: 10
Training loss: 2.1435134410858154
Validation loss: 2.150781907061095

Epoch: 138| Step: 0
Training loss: 1.9268203973770142
Validation loss: 2.0976270091149116

Epoch: 5| Step: 1
Training loss: 2.2961230278015137
Validation loss: 2.1125233275915987

Epoch: 5| Step: 2
Training loss: 1.746198296546936
Validation loss: 2.1023676985053608

Epoch: 5| Step: 3
Training loss: 1.7790439128875732
Validation loss: 2.131840528980378

Epoch: 5| Step: 4
Training loss: 1.9967197179794312
Validation loss: 2.1137567976469636

Epoch: 5| Step: 5
Training loss: 2.3145060539245605
Validation loss: 2.1039034000007053

Epoch: 5| Step: 6
Training loss: 2.7685470581054688
Validation loss: 2.123974520673034

Epoch: 5| Step: 7
Training loss: 1.6662876605987549
Validation loss: 2.11843446505967

Epoch: 5| Step: 8
Training loss: 2.4859156608581543
Validation loss: 2.085960747093283

Epoch: 5| Step: 9
Training loss: 2.3790647983551025
Validation loss: 2.1193519561521468

Epoch: 5| Step: 10
Training loss: 1.4400502443313599
Validation loss: 2.0995031556775494

Epoch: 139| Step: 0
Training loss: 1.6367086172103882
Validation loss: 2.1268531507061375

Epoch: 5| Step: 1
Training loss: 2.5150959491729736
Validation loss: 2.1131003415712746

Epoch: 5| Step: 2
Training loss: 2.0033926963806152
Validation loss: 2.107499349501825

Epoch: 5| Step: 3
Training loss: 2.215928792953491
Validation loss: 2.1062250996148713

Epoch: 5| Step: 4
Training loss: 1.2908316850662231
Validation loss: 2.113171664617395

Epoch: 5| Step: 5
Training loss: 2.747739315032959
Validation loss: 2.1054549717134043

Epoch: 5| Step: 6
Training loss: 2.5682668685913086
Validation loss: 2.122932727618884

Epoch: 5| Step: 7
Training loss: 2.2945713996887207
Validation loss: 2.122433472705144

Epoch: 5| Step: 8
Training loss: 2.252060890197754
Validation loss: 2.1100976569678194

Epoch: 5| Step: 9
Training loss: 1.3525036573410034
Validation loss: 2.1026739228156304

Epoch: 5| Step: 10
Training loss: 2.201364278793335
Validation loss: 2.090238086638912

Epoch: 140| Step: 0
Training loss: 2.2284209728240967
Validation loss: 2.109720245484383

Epoch: 5| Step: 1
Training loss: 1.6447227001190186
Validation loss: 2.11609971907831

Epoch: 5| Step: 2
Training loss: 2.038292169570923
Validation loss: 2.1087355639344905

Epoch: 5| Step: 3
Training loss: 2.5047121047973633
Validation loss: 2.097157860314974

Epoch: 5| Step: 4
Training loss: 1.4464948177337646
Validation loss: 2.1028972992333035

Epoch: 5| Step: 5
Training loss: 1.7869863510131836
Validation loss: 2.120085982866185

Epoch: 5| Step: 6
Training loss: 2.001560688018799
Validation loss: 2.108672721411592

Epoch: 5| Step: 7
Training loss: 2.045494556427002
Validation loss: 2.077965421061362

Epoch: 5| Step: 8
Training loss: 3.034076690673828
Validation loss: 2.1087030672257945

Epoch: 5| Step: 9
Training loss: 2.1401286125183105
Validation loss: 2.110578053741045

Epoch: 5| Step: 10
Training loss: 2.0272927284240723
Validation loss: 2.0906968193669475

Epoch: 141| Step: 0
Training loss: 1.6241471767425537
Validation loss: 2.0833892232628277

Epoch: 5| Step: 1
Training loss: 2.856398582458496
Validation loss: 2.0879080295562744

Epoch: 5| Step: 2
Training loss: 1.888794183731079
Validation loss: 2.098244159452377

Epoch: 5| Step: 3
Training loss: 1.7845770120620728
Validation loss: 2.0963966564465593

Epoch: 5| Step: 4
Training loss: 1.5752063989639282
Validation loss: 2.1082141989020893

Epoch: 5| Step: 5
Training loss: 2.4337260723114014
Validation loss: 2.1347953145222

Epoch: 5| Step: 6
Training loss: 1.9007154703140259
Validation loss: 2.094665704234954

Epoch: 5| Step: 7
Training loss: 2.088723659515381
Validation loss: 2.093992422985774

Epoch: 5| Step: 8
Training loss: 2.411885976791382
Validation loss: 2.0953638399800947

Epoch: 5| Step: 9
Training loss: 2.068420648574829
Validation loss: 2.094015949515886

Epoch: 5| Step: 10
Training loss: 2.323094606399536
Validation loss: 2.0888463527925554

Epoch: 142| Step: 0
Training loss: 2.25980806350708
Validation loss: 2.082936256162582

Epoch: 5| Step: 1
Training loss: 1.433171272277832
Validation loss: 2.09876226609753

Epoch: 5| Step: 2
Training loss: 2.1707966327667236
Validation loss: 2.1143689681124944

Epoch: 5| Step: 3
Training loss: 3.0839715003967285
Validation loss: 2.094923642373854

Epoch: 5| Step: 4
Training loss: 1.987200140953064
Validation loss: 2.112534115391393

Epoch: 5| Step: 5
Training loss: 2.113251209259033
Validation loss: 2.1175378830202165

Epoch: 5| Step: 6
Training loss: 2.1666200160980225
Validation loss: 2.063852881872526

Epoch: 5| Step: 7
Training loss: 2.213259220123291
Validation loss: 2.115946638968683

Epoch: 5| Step: 8
Training loss: 1.791164755821228
Validation loss: 2.1061658461888633

Epoch: 5| Step: 9
Training loss: 2.0204060077667236
Validation loss: 2.0916385599361953

Epoch: 5| Step: 10
Training loss: 1.5329309701919556
Validation loss: 2.1164878158159155

Epoch: 143| Step: 0
Training loss: 2.2301535606384277
Validation loss: 2.089241616187557

Epoch: 5| Step: 1
Training loss: 1.986452341079712
Validation loss: 2.0662424577179777

Epoch: 5| Step: 2
Training loss: 2.7775044441223145
Validation loss: 2.101287946906141

Epoch: 5| Step: 3
Training loss: 1.6319431066513062
Validation loss: 2.101359095624698

Epoch: 5| Step: 4
Training loss: 2.2839484214782715
Validation loss: 2.0973571756834626

Epoch: 5| Step: 5
Training loss: 2.1224846839904785
Validation loss: 2.0851057370503745

Epoch: 5| Step: 6
Training loss: 1.2480634450912476
Validation loss: 2.113394021987915

Epoch: 5| Step: 7
Training loss: 2.2154109477996826
Validation loss: 2.0817374683195546

Epoch: 5| Step: 8
Training loss: 1.842332124710083
Validation loss: 2.096925272736498

Epoch: 5| Step: 9
Training loss: 3.0935616493225098
Validation loss: 2.117423826648343

Epoch: 5| Step: 10
Training loss: 1.3718105554580688
Validation loss: 2.07833170634444

Epoch: 144| Step: 0
Training loss: 2.3461270332336426
Validation loss: 2.074044653164443

Epoch: 5| Step: 1
Training loss: 1.6035258769989014
Validation loss: 2.082652682899147

Epoch: 5| Step: 2
Training loss: 2.1514573097229004
Validation loss: 2.0708028731807584

Epoch: 5| Step: 3
Training loss: 2.0160531997680664
Validation loss: 2.0895358490687546

Epoch: 5| Step: 4
Training loss: 2.0055203437805176
Validation loss: 2.103586353281493

Epoch: 5| Step: 5
Training loss: 1.4531934261322021
Validation loss: 2.082312801832794

Epoch: 5| Step: 6
Training loss: 2.2294533252716064
Validation loss: 2.0845415733193837

Epoch: 5| Step: 7
Training loss: 2.6807968616485596
Validation loss: 2.08855442846975

Epoch: 5| Step: 8
Training loss: 1.825940489768982
Validation loss: 2.1084670584688903

Epoch: 5| Step: 9
Training loss: 1.59611177444458
Validation loss: 2.0953030829788535

Epoch: 5| Step: 10
Training loss: 2.982361316680908
Validation loss: 2.125655112727996

Epoch: 145| Step: 0
Training loss: 1.8596975803375244
Validation loss: 2.0790058028313423

Epoch: 5| Step: 1
Training loss: 2.5512797832489014
Validation loss: 2.0737948622754825

Epoch: 5| Step: 2
Training loss: 1.8159204721450806
Validation loss: 2.109860681718396

Epoch: 5| Step: 3
Training loss: 1.9544585943222046
Validation loss: 2.069113568593097

Epoch: 5| Step: 4
Training loss: 1.4568687677383423
Validation loss: 2.0939216434314685

Epoch: 5| Step: 5
Training loss: 1.5583727359771729
Validation loss: 2.0752403992478565

Epoch: 5| Step: 6
Training loss: 2.782122850418091
Validation loss: 2.0875790285807785

Epoch: 5| Step: 7
Training loss: 1.3951120376586914
Validation loss: 2.092616670875139

Epoch: 5| Step: 8
Training loss: 2.8822007179260254
Validation loss: 2.0943389810541624

Epoch: 5| Step: 9
Training loss: 2.6525073051452637
Validation loss: 2.1090450697047736

Epoch: 5| Step: 10
Training loss: 1.575703501701355
Validation loss: 2.1204307233133624

Epoch: 146| Step: 0
Training loss: 2.5411975383758545
Validation loss: 2.0970590024866085

Epoch: 5| Step: 1
Training loss: 2.6558609008789062
Validation loss: 2.0628235442664034

Epoch: 5| Step: 2
Training loss: 1.4147696495056152
Validation loss: 2.0785476981952624

Epoch: 5| Step: 3
Training loss: 1.7883274555206299
Validation loss: 2.0851549461323726

Epoch: 5| Step: 4
Training loss: 1.6717908382415771
Validation loss: 2.0906062574796778

Epoch: 5| Step: 5
Training loss: 2.053436756134033
Validation loss: 2.1077656425455564

Epoch: 5| Step: 6
Training loss: 2.100726366043091
Validation loss: 2.087130960597787

Epoch: 5| Step: 7
Training loss: 2.123331069946289
Validation loss: 2.0918256787843603

Epoch: 5| Step: 8
Training loss: 2.100632905960083
Validation loss: 2.091975781225389

Epoch: 5| Step: 9
Training loss: 1.6800739765167236
Validation loss: 2.071438168966642

Epoch: 5| Step: 10
Training loss: 2.888301134109497
Validation loss: 2.0825104328893844

Epoch: 147| Step: 0
Training loss: 1.9725316762924194
Validation loss: 2.074656694166122

Epoch: 5| Step: 1
Training loss: 2.3659934997558594
Validation loss: 2.0767506502007924

Epoch: 5| Step: 2
Training loss: 2.1956980228424072
Validation loss: 2.0787355438355477

Epoch: 5| Step: 3
Training loss: 2.064122200012207
Validation loss: 2.100808811444108

Epoch: 5| Step: 4
Training loss: 2.031651496887207
Validation loss: 2.0952135516751196

Epoch: 5| Step: 5
Training loss: 2.0087175369262695
Validation loss: 2.1004855196963073

Epoch: 5| Step: 6
Training loss: 1.9453017711639404
Validation loss: 2.0872283110054592

Epoch: 5| Step: 7
Training loss: 2.15427565574646
Validation loss: 2.094894560434485

Epoch: 5| Step: 8
Training loss: 1.9914052486419678
Validation loss: 2.080604935205111

Epoch: 5| Step: 9
Training loss: 2.2564399242401123
Validation loss: 2.0970130235918107

Epoch: 5| Step: 10
Training loss: 1.5932159423828125
Validation loss: 2.0571242045330744

Epoch: 148| Step: 0
Training loss: 1.7662982940673828
Validation loss: 2.082983419459353

Epoch: 5| Step: 1
Training loss: 2.296926975250244
Validation loss: 2.0925643905516593

Epoch: 5| Step: 2
Training loss: 2.255674362182617
Validation loss: 2.1092275009360364

Epoch: 5| Step: 3
Training loss: 1.3872121572494507
Validation loss: 2.0960488729579474

Epoch: 5| Step: 4
Training loss: 2.6078174114227295
Validation loss: 2.093002357790547

Epoch: 5| Step: 5
Training loss: 1.5606536865234375
Validation loss: 2.0823711233754314

Epoch: 5| Step: 6
Training loss: 2.558591365814209
Validation loss: 2.087731910008256

Epoch: 5| Step: 7
Training loss: 2.5914053916931152
Validation loss: 2.1050807045352076

Epoch: 5| Step: 8
Training loss: 1.6539615392684937
Validation loss: 2.078466675614798

Epoch: 5| Step: 9
Training loss: 2.0003738403320312
Validation loss: 2.1158993577444427

Epoch: 5| Step: 10
Training loss: 2.1066808700561523
Validation loss: 2.0692598345459148

Epoch: 149| Step: 0
Training loss: 2.0403456687927246
Validation loss: 2.116114165193291

Epoch: 5| Step: 1
Training loss: 1.135125756263733
Validation loss: 2.131610608869983

Epoch: 5| Step: 2
Training loss: 1.9721126556396484
Validation loss: 2.0943418369498303

Epoch: 5| Step: 3
Training loss: 3.1336803436279297
Validation loss: 2.080545248523835

Epoch: 5| Step: 4
Training loss: 1.6273224353790283
Validation loss: 2.105382909056961

Epoch: 5| Step: 5
Training loss: 1.8250024318695068
Validation loss: 2.0844617184772285

Epoch: 5| Step: 6
Training loss: 2.139136552810669
Validation loss: 2.0840377346161874

Epoch: 5| Step: 7
Training loss: 2.4543139934539795
Validation loss: 2.072756626272714

Epoch: 5| Step: 8
Training loss: 1.6108694076538086
Validation loss: 2.0926475986357658

Epoch: 5| Step: 9
Training loss: 2.08015513420105
Validation loss: 2.0671708083921865

Epoch: 5| Step: 10
Training loss: 2.745058298110962
Validation loss: 2.048707390344271

Epoch: 150| Step: 0
Training loss: 1.8931442499160767
Validation loss: 2.0713109008727537

Epoch: 5| Step: 1
Training loss: 2.485029697418213
Validation loss: 2.0917837260871806

Epoch: 5| Step: 2
Training loss: 2.2912392616271973
Validation loss: 2.0759105887464298

Epoch: 5| Step: 3
Training loss: 1.9495375156402588
Validation loss: 2.05572872777139

Epoch: 5| Step: 4
Training loss: 2.1940417289733887
Validation loss: 2.0979381594606625

Epoch: 5| Step: 5
Training loss: 1.5967236757278442
Validation loss: 2.067758106416272

Epoch: 5| Step: 6
Training loss: 2.781615734100342
Validation loss: 2.0783488032638386

Epoch: 5| Step: 7
Training loss: 2.4604926109313965
Validation loss: 2.0761708008345736

Epoch: 5| Step: 8
Training loss: 1.6418730020523071
Validation loss: 2.0744335215578795

Epoch: 5| Step: 9
Training loss: 1.292997121810913
Validation loss: 2.0503956656302176

Epoch: 5| Step: 10
Training loss: 1.9131108522415161
Validation loss: 2.05579024745572

Epoch: 151| Step: 0
Training loss: 2.054002285003662
Validation loss: 2.052160955244495

Epoch: 5| Step: 1
Training loss: 2.2359046936035156
Validation loss: 2.077136995971844

Epoch: 5| Step: 2
Training loss: 2.3608810901641846
Validation loss: 2.0837342008467643

Epoch: 5| Step: 3
Training loss: 1.9285824298858643
Validation loss: 2.048479574982838

Epoch: 5| Step: 4
Training loss: 2.041761636734009
Validation loss: 2.0829881596308883

Epoch: 5| Step: 5
Training loss: 2.1303977966308594
Validation loss: 2.0762664797485515

Epoch: 5| Step: 6
Training loss: 2.026973247528076
Validation loss: 2.065841820932204

Epoch: 5| Step: 7
Training loss: 1.847590684890747
Validation loss: 2.0579845956576768

Epoch: 5| Step: 8
Training loss: 2.1547598838806152
Validation loss: 2.085247843496261

Epoch: 5| Step: 9
Training loss: 2.0493252277374268
Validation loss: 2.0721498779071275

Epoch: 5| Step: 10
Training loss: 1.5967968702316284
Validation loss: 2.060039418999867

Epoch: 152| Step: 0
Training loss: 2.8154234886169434
Validation loss: 2.074103065716323

Epoch: 5| Step: 1
Training loss: 1.731309175491333
Validation loss: 2.05559278047213

Epoch: 5| Step: 2
Training loss: 2.32833194732666
Validation loss: 2.1321117903596614

Epoch: 5| Step: 3
Training loss: 1.8484607934951782
Validation loss: 2.0752359090312833

Epoch: 5| Step: 4
Training loss: 1.9650490283966064
Validation loss: 2.0656695647906234

Epoch: 5| Step: 5
Training loss: 1.8285471200942993
Validation loss: 2.07405884035172

Epoch: 5| Step: 6
Training loss: 1.560714840888977
Validation loss: 2.1121752364661104

Epoch: 5| Step: 7
Training loss: 2.4090211391448975
Validation loss: 2.0959908398248817

Epoch: 5| Step: 8
Training loss: 1.9389318227767944
Validation loss: 2.0656248241342525

Epoch: 5| Step: 9
Training loss: 2.7115061283111572
Validation loss: 2.0997799596478863

Epoch: 5| Step: 10
Training loss: 1.539366364479065
Validation loss: 2.0963649621573825

Epoch: 153| Step: 0
Training loss: 2.3408925533294678
Validation loss: 2.0783246665872555

Epoch: 5| Step: 1
Training loss: 1.609575629234314
Validation loss: 2.1028341606099117

Epoch: 5| Step: 2
Training loss: 2.4073116779327393
Validation loss: 2.0614139226175126

Epoch: 5| Step: 3
Training loss: 1.75759756565094
Validation loss: 2.0827673276265464

Epoch: 5| Step: 4
Training loss: 2.221447229385376
Validation loss: 2.0772234162976666

Epoch: 5| Step: 5
Training loss: 2.1390819549560547
Validation loss: 2.064364206406378

Epoch: 5| Step: 6
Training loss: 2.205254316329956
Validation loss: 2.08113924149544

Epoch: 5| Step: 7
Training loss: 2.141223430633545
Validation loss: 2.1023772993395404

Epoch: 5| Step: 8
Training loss: 2.050015449523926
Validation loss: 2.065469809757766

Epoch: 5| Step: 9
Training loss: 2.16774845123291
Validation loss: 2.1053901487781155

Epoch: 5| Step: 10
Training loss: 1.2785675525665283
Validation loss: 2.082084307106592

Epoch: 154| Step: 0
Training loss: 2.7648329734802246
Validation loss: 2.0852267831884403

Epoch: 5| Step: 1
Training loss: 1.3905069828033447
Validation loss: 2.059738536034861

Epoch: 5| Step: 2
Training loss: 2.2357966899871826
Validation loss: 2.066518125995513

Epoch: 5| Step: 3
Training loss: 2.911201238632202
Validation loss: 2.0657124544984553

Epoch: 5| Step: 4
Training loss: 2.0492124557495117
Validation loss: 2.0783864887811805

Epoch: 5| Step: 5
Training loss: 2.0437023639678955
Validation loss: 2.079365417521487

Epoch: 5| Step: 6
Training loss: 1.9146705865859985
Validation loss: 2.0852352701207644

Epoch: 5| Step: 7
Training loss: 2.105534791946411
Validation loss: 2.101516339086717

Epoch: 5| Step: 8
Training loss: 1.9352991580963135
Validation loss: 2.096884519823136

Epoch: 5| Step: 9
Training loss: 1.527390480041504
Validation loss: 2.0805623941524054

Epoch: 5| Step: 10
Training loss: 1.8229453563690186
Validation loss: 2.0633918623770438

Epoch: 155| Step: 0
Training loss: 2.431297779083252
Validation loss: 2.077279854846257

Epoch: 5| Step: 1
Training loss: 1.9845097064971924
Validation loss: 2.1047932717107956

Epoch: 5| Step: 2
Training loss: 2.262976884841919
Validation loss: 2.071583717100082

Epoch: 5| Step: 3
Training loss: 2.354785919189453
Validation loss: 2.0677427809725524

Epoch: 5| Step: 4
Training loss: 1.6247568130493164
Validation loss: 2.0905952594613515

Epoch: 5| Step: 5
Training loss: 1.6567319631576538
Validation loss: 2.103458309686312

Epoch: 5| Step: 6
Training loss: 2.0550804138183594
Validation loss: 2.0718722958718576

Epoch: 5| Step: 7
Training loss: 2.053276538848877
Validation loss: 2.08142150345669

Epoch: 5| Step: 8
Training loss: 2.459629535675049
Validation loss: 2.057083140137375

Epoch: 5| Step: 9
Training loss: 1.6951444149017334
Validation loss: 2.0607586112073673

Epoch: 5| Step: 10
Training loss: 1.759004831314087
Validation loss: 2.125114660109243

Epoch: 156| Step: 0
Training loss: 1.6713950634002686
Validation loss: 2.074325374377671

Epoch: 5| Step: 1
Training loss: 2.050349712371826
Validation loss: 2.0592894169592086

Epoch: 5| Step: 2
Training loss: 2.148468494415283
Validation loss: 2.091120263581635

Epoch: 5| Step: 3
Training loss: 2.495516061782837
Validation loss: 2.0754520047095513

Epoch: 5| Step: 4
Training loss: 2.1973464488983154
Validation loss: 2.0583577066339473

Epoch: 5| Step: 5
Training loss: 2.191091537475586
Validation loss: 2.073321828278162

Epoch: 5| Step: 6
Training loss: 1.70964777469635
Validation loss: 2.0416179703127955

Epoch: 5| Step: 7
Training loss: 1.4475672245025635
Validation loss: 2.116876648318383

Epoch: 5| Step: 8
Training loss: 1.7164853811264038
Validation loss: 2.0837938734280166

Epoch: 5| Step: 9
Training loss: 2.5150070190429688
Validation loss: 2.0811652214296403

Epoch: 5| Step: 10
Training loss: 2.3842949867248535
Validation loss: 2.045040640779721

Epoch: 157| Step: 0
Training loss: 1.7691694498062134
Validation loss: 2.05998521338227

Epoch: 5| Step: 1
Training loss: 2.6062381267547607
Validation loss: 2.0391617372471798

Epoch: 5| Step: 2
Training loss: 1.9430853128433228
Validation loss: 2.079719594729844

Epoch: 5| Step: 3
Training loss: 1.4369456768035889
Validation loss: 2.080395261446635

Epoch: 5| Step: 4
Training loss: 2.362061023712158
Validation loss: 2.077987112024779

Epoch: 5| Step: 5
Training loss: 1.6950950622558594
Validation loss: 2.0800082760472454

Epoch: 5| Step: 6
Training loss: 2.012941598892212
Validation loss: 2.05181263595499

Epoch: 5| Step: 7
Training loss: 2.2827136516571045
Validation loss: 2.028174736166513

Epoch: 5| Step: 8
Training loss: 1.9787533283233643
Validation loss: 2.057553493848411

Epoch: 5| Step: 9
Training loss: 2.8037467002868652
Validation loss: 2.0694952998110043

Epoch: 5| Step: 10
Training loss: 1.2183858156204224
Validation loss: 2.089329445233909

Epoch: 158| Step: 0
Training loss: 1.7948077917099
Validation loss: 2.038595249575953

Epoch: 5| Step: 1
Training loss: 2.7523789405822754
Validation loss: 2.068205391207049

Epoch: 5| Step: 2
Training loss: 2.0065529346466064
Validation loss: 2.0593755796391475

Epoch: 5| Step: 3
Training loss: 2.06400728225708
Validation loss: 2.0977395080750987

Epoch: 5| Step: 4
Training loss: 1.6890243291854858
Validation loss: 2.098176820303804

Epoch: 5| Step: 5
Training loss: 1.7508201599121094
Validation loss: 2.0814075021333593

Epoch: 5| Step: 6
Training loss: 2.1672768592834473
Validation loss: 2.070866189977174

Epoch: 5| Step: 7
Training loss: 1.8751814365386963
Validation loss: 2.0722027286406486

Epoch: 5| Step: 8
Training loss: 1.9636433124542236
Validation loss: 2.0596946157434934

Epoch: 5| Step: 9
Training loss: 1.8675334453582764
Validation loss: 2.0909030399014874

Epoch: 5| Step: 10
Training loss: 2.530912160873413
Validation loss: 2.1234561551001763

Epoch: 159| Step: 0
Training loss: 1.3274668455123901
Validation loss: 2.07204367012106

Epoch: 5| Step: 1
Training loss: 1.866061806678772
Validation loss: 2.0814718097768803

Epoch: 5| Step: 2
Training loss: 1.881026268005371
Validation loss: 2.1041555840481996

Epoch: 5| Step: 3
Training loss: 1.7034456729888916
Validation loss: 2.0901223113459926

Epoch: 5| Step: 4
Training loss: 1.9069783687591553
Validation loss: 2.0837981931624876

Epoch: 5| Step: 5
Training loss: 1.8851162195205688
Validation loss: 2.091233153497019

Epoch: 5| Step: 6
Training loss: 2.9531795978546143
Validation loss: 2.0784050764576083

Epoch: 5| Step: 7
Training loss: 1.6796143054962158
Validation loss: 2.0724994610714655

Epoch: 5| Step: 8
Training loss: 2.207132577896118
Validation loss: 2.089449636397823

Epoch: 5| Step: 9
Training loss: 2.291093349456787
Validation loss: 2.0822501362011

Epoch: 5| Step: 10
Training loss: 2.7970261573791504
Validation loss: 2.043478629922354

Epoch: 160| Step: 0
Training loss: 1.8353195190429688
Validation loss: 2.077023644601145

Epoch: 5| Step: 1
Training loss: 2.1826090812683105
Validation loss: 2.069340769962598

Epoch: 5| Step: 2
Training loss: 1.8426520824432373
Validation loss: 2.06824662352121

Epoch: 5| Step: 3
Training loss: 2.5238986015319824
Validation loss: 2.056907102625857

Epoch: 5| Step: 4
Training loss: 1.5952086448669434
Validation loss: 2.050946602257349

Epoch: 5| Step: 5
Training loss: 1.8939549922943115
Validation loss: 2.0589669301945674

Epoch: 5| Step: 6
Training loss: 2.416314125061035
Validation loss: 2.080966975099297

Epoch: 5| Step: 7
Training loss: 2.0219554901123047
Validation loss: 2.045842706516225

Epoch: 5| Step: 8
Training loss: 2.0540506839752197
Validation loss: 2.0735163842478106

Epoch: 5| Step: 9
Training loss: 1.4535317420959473
Validation loss: 2.0715965058213923

Epoch: 5| Step: 10
Training loss: 2.643979787826538
Validation loss: 2.0973253198849258

Epoch: 161| Step: 0
Training loss: 2.5743610858917236
Validation loss: 2.0730001670058056

Epoch: 5| Step: 1
Training loss: 2.1786932945251465
Validation loss: 2.0835547575386624

Epoch: 5| Step: 2
Training loss: 2.038872241973877
Validation loss: 2.064835161291143

Epoch: 5| Step: 3
Training loss: 1.951620101928711
Validation loss: 2.048725051264609

Epoch: 5| Step: 4
Training loss: 1.7828028202056885
Validation loss: 2.048456258671258

Epoch: 5| Step: 5
Training loss: 1.144224762916565
Validation loss: 2.0980043052345194

Epoch: 5| Step: 6
Training loss: 2.3899130821228027
Validation loss: 2.0796617692516697

Epoch: 5| Step: 7
Training loss: 2.000654458999634
Validation loss: 2.084944189235728

Epoch: 5| Step: 8
Training loss: 2.352644443511963
Validation loss: 2.0873797708942043

Epoch: 5| Step: 9
Training loss: 1.9143222570419312
Validation loss: 2.110653784967238

Epoch: 5| Step: 10
Training loss: 1.9291218519210815
Validation loss: 2.0651010800433416

Epoch: 162| Step: 0
Training loss: 1.8839470148086548
Validation loss: 2.0876741204210507

Epoch: 5| Step: 1
Training loss: 1.9968410730361938
Validation loss: 2.082325900754621

Epoch: 5| Step: 2
Training loss: 2.125098705291748
Validation loss: 2.095287589616673

Epoch: 5| Step: 3
Training loss: 2.3582427501678467
Validation loss: 2.02313732588163

Epoch: 5| Step: 4
Training loss: 1.829334020614624
Validation loss: 2.097535250007465

Epoch: 5| Step: 5
Training loss: 2.872591733932495
Validation loss: 2.0653379860744683

Epoch: 5| Step: 6
Training loss: 2.0532374382019043
Validation loss: 2.0801671422937864

Epoch: 5| Step: 7
Training loss: 2.0698046684265137
Validation loss: 2.091536174538315

Epoch: 5| Step: 8
Training loss: 1.5629671812057495
Validation loss: 2.080136565751927

Epoch: 5| Step: 9
Training loss: 1.8872343301773071
Validation loss: 2.0305551021329817

Epoch: 5| Step: 10
Training loss: 1.6704761981964111
Validation loss: 2.07192632203461

Epoch: 163| Step: 0
Training loss: 2.145831346511841
Validation loss: 2.0482179016195317

Epoch: 5| Step: 1
Training loss: 1.280633568763733
Validation loss: 2.081621345653329

Epoch: 5| Step: 2
Training loss: 2.4355874061584473
Validation loss: 2.0802416801452637

Epoch: 5| Step: 3
Training loss: 1.2780656814575195
Validation loss: 2.0515011715632614

Epoch: 5| Step: 4
Training loss: 1.8114093542099
Validation loss: 2.054377455865183

Epoch: 5| Step: 5
Training loss: 2.5713882446289062
Validation loss: 2.0401767543567124

Epoch: 5| Step: 6
Training loss: 2.0423078536987305
Validation loss: 2.082343834702687

Epoch: 5| Step: 7
Training loss: 1.639809012413025
Validation loss: 2.0732021998333674

Epoch: 5| Step: 8
Training loss: 1.6683708429336548
Validation loss: 2.083332969296363

Epoch: 5| Step: 9
Training loss: 2.806804656982422
Validation loss: 2.07627970428877

Epoch: 5| Step: 10
Training loss: 2.7930312156677246
Validation loss: 2.0889134612134708

Epoch: 164| Step: 0
Training loss: 1.582234501838684
Validation loss: 2.0890690524091005

Epoch: 5| Step: 1
Training loss: 2.653390407562256
Validation loss: 2.0306122431191067

Epoch: 5| Step: 2
Training loss: 1.7464317083358765
Validation loss: 2.0757594877673733

Epoch: 5| Step: 3
Training loss: 2.159863233566284
Validation loss: 2.055438672342608

Epoch: 5| Step: 4
Training loss: 1.889063835144043
Validation loss: 2.083691978967318

Epoch: 5| Step: 5
Training loss: 1.976452112197876
Validation loss: 2.0714837197334535

Epoch: 5| Step: 6
Training loss: 1.5586235523223877
Validation loss: 2.0579649068975963

Epoch: 5| Step: 7
Training loss: 1.9093717336654663
Validation loss: 2.074674165377053

Epoch: 5| Step: 8
Training loss: 1.6836779117584229
Validation loss: 2.0746280506093013

Epoch: 5| Step: 9
Training loss: 2.316606044769287
Validation loss: 2.0861930744622343

Epoch: 5| Step: 10
Training loss: 2.731210947036743
Validation loss: 2.0668044859363186

Epoch: 165| Step: 0
Training loss: 2.2241551876068115
Validation loss: 2.0687402397073726

Epoch: 5| Step: 1
Training loss: 1.9261261224746704
Validation loss: 2.0563565402902584

Epoch: 5| Step: 2
Training loss: 1.5792944431304932
Validation loss: 2.1018998007620535

Epoch: 5| Step: 3
Training loss: 1.8509438037872314
Validation loss: 2.0434354915413806

Epoch: 5| Step: 4
Training loss: 2.4881529808044434
Validation loss: 2.0776729993922736

Epoch: 5| Step: 5
Training loss: 2.0695722103118896
Validation loss: 2.055379488134897

Epoch: 5| Step: 6
Training loss: 1.808157205581665
Validation loss: 2.058838104688993

Epoch: 5| Step: 7
Training loss: 1.8985553979873657
Validation loss: 2.0851234633435487

Epoch: 5| Step: 8
Training loss: 1.7795751094818115
Validation loss: 2.0530294551644275

Epoch: 5| Step: 9
Training loss: 1.8892406225204468
Validation loss: 2.0972681814624416

Epoch: 5| Step: 10
Training loss: 2.7469406127929688
Validation loss: 2.09157943853768

Epoch: 166| Step: 0
Training loss: 1.5355972051620483
Validation loss: 2.085589985693655

Epoch: 5| Step: 1
Training loss: 1.9782154560089111
Validation loss: 2.079396229918285

Epoch: 5| Step: 2
Training loss: 1.3903257846832275
Validation loss: 2.0717517868165047

Epoch: 5| Step: 3
Training loss: 1.885106086730957
Validation loss: 2.0301905267982074

Epoch: 5| Step: 4
Training loss: 2.6388492584228516
Validation loss: 2.065451168244885

Epoch: 5| Step: 5
Training loss: 2.532738208770752
Validation loss: 2.06655377470037

Epoch: 5| Step: 6
Training loss: 2.347250461578369
Validation loss: 2.071692866663779

Epoch: 5| Step: 7
Training loss: 1.3691279888153076
Validation loss: 2.0953555209662325

Epoch: 5| Step: 8
Training loss: 2.7513275146484375
Validation loss: 2.0820915699005127

Epoch: 5| Step: 9
Training loss: 1.8826383352279663
Validation loss: 2.050001609709955

Epoch: 5| Step: 10
Training loss: 1.7675987482070923
Validation loss: 2.0591869713157736

Epoch: 167| Step: 0
Training loss: 2.238132953643799
Validation loss: 2.0679144423495055

Epoch: 5| Step: 1
Training loss: 1.9498361349105835
Validation loss: 2.082715319048974

Epoch: 5| Step: 2
Training loss: 2.241206407546997
Validation loss: 2.0728873386177966

Epoch: 5| Step: 3
Training loss: 1.970160722732544
Validation loss: 2.0769382779316237

Epoch: 5| Step: 4
Training loss: 2.0108542442321777
Validation loss: 2.075398874539201

Epoch: 5| Step: 5
Training loss: 1.5363428592681885
Validation loss: 2.0817144250357025

Epoch: 5| Step: 6
Training loss: 1.7181293964385986
Validation loss: 2.044510995188067

Epoch: 5| Step: 7
Training loss: 2.036625862121582
Validation loss: 2.0207962579624628

Epoch: 5| Step: 8
Training loss: 2.2545077800750732
Validation loss: 2.0690260151381135

Epoch: 5| Step: 9
Training loss: 2.1122071743011475
Validation loss: 2.052904095700992

Epoch: 5| Step: 10
Training loss: 2.094181776046753
Validation loss: 2.074384230439381

Epoch: 168| Step: 0
Training loss: 1.8885101079940796
Validation loss: 2.0838130276690245

Epoch: 5| Step: 1
Training loss: 2.016629695892334
Validation loss: 2.095911305437806

Epoch: 5| Step: 2
Training loss: 1.7457268238067627
Validation loss: 2.0513582537251134

Epoch: 5| Step: 3
Training loss: 2.0910041332244873
Validation loss: 2.0624603276611655

Epoch: 5| Step: 4
Training loss: 1.5501105785369873
Validation loss: 2.081250813699538

Epoch: 5| Step: 5
Training loss: 2.337989091873169
Validation loss: 2.0472935553519958

Epoch: 5| Step: 6
Training loss: 2.885378360748291
Validation loss: 2.075611445211595

Epoch: 5| Step: 7
Training loss: 1.6501506567001343
Validation loss: 2.061853661332079

Epoch: 5| Step: 8
Training loss: 2.198385715484619
Validation loss: 2.0658427848610827

Epoch: 5| Step: 9
Training loss: 2.0834298133850098
Validation loss: 2.0731091230146346

Epoch: 5| Step: 10
Training loss: 1.6822726726531982
Validation loss: 2.067922092253162

Epoch: 169| Step: 0
Training loss: 1.5935444831848145
Validation loss: 2.0796745310547533

Epoch: 5| Step: 1
Training loss: 1.6815992593765259
Validation loss: 2.04015028092169

Epoch: 5| Step: 2
Training loss: 2.577737331390381
Validation loss: 2.0513882521660096

Epoch: 5| Step: 3
Training loss: 1.9763820171356201
Validation loss: 2.0892050855903217

Epoch: 5| Step: 4
Training loss: 2.5586514472961426
Validation loss: 2.0850470809526342

Epoch: 5| Step: 5
Training loss: 2.023672103881836
Validation loss: 2.041008508333596

Epoch: 5| Step: 6
Training loss: 1.709568738937378
Validation loss: 2.0535073357243694

Epoch: 5| Step: 7
Training loss: 2.27983021736145
Validation loss: 2.0351127373274935

Epoch: 5| Step: 8
Training loss: 2.5177502632141113
Validation loss: 2.0241536876206756

Epoch: 5| Step: 9
Training loss: 1.5243251323699951
Validation loss: 2.028874074259112

Epoch: 5| Step: 10
Training loss: 1.5739027261734009
Validation loss: 2.0535231738962154

Epoch: 170| Step: 0
Training loss: 2.5645439624786377
Validation loss: 2.038417186788333

Epoch: 5| Step: 1
Training loss: 1.8779869079589844
Validation loss: 2.050697047223327

Epoch: 5| Step: 2
Training loss: 1.8640258312225342
Validation loss: 2.0619519474685832

Epoch: 5| Step: 3
Training loss: 1.8875138759613037
Validation loss: 2.046772438992736

Epoch: 5| Step: 4
Training loss: 1.8872601985931396
Validation loss: 2.0774661905022076

Epoch: 5| Step: 5
Training loss: 2.158597469329834
Validation loss: 2.0718961620843537

Epoch: 5| Step: 6
Training loss: 1.9277664422988892
Validation loss: 2.058298464744322

Epoch: 5| Step: 7
Training loss: 2.286273717880249
Validation loss: 2.0648528696388326

Epoch: 5| Step: 8
Training loss: 1.8757892847061157
Validation loss: 2.055021516738399

Epoch: 5| Step: 9
Training loss: 1.5827701091766357
Validation loss: 2.0705921252568564

Epoch: 5| Step: 10
Training loss: 2.1106979846954346
Validation loss: 2.067436856608237

Epoch: 171| Step: 0
Training loss: 1.7615759372711182
Validation loss: 2.0464319234253256

Epoch: 5| Step: 1
Training loss: 1.9657589197158813
Validation loss: 2.025733442716701

Epoch: 5| Step: 2
Training loss: 1.5819200277328491
Validation loss: 2.0350643486104985

Epoch: 5| Step: 3
Training loss: 1.769476294517517
Validation loss: 2.0495051158371793

Epoch: 5| Step: 4
Training loss: 1.8912683725357056
Validation loss: 2.076887329419454

Epoch: 5| Step: 5
Training loss: 3.0461599826812744
Validation loss: 2.045652207507882

Epoch: 5| Step: 6
Training loss: 2.052130937576294
Validation loss: 2.091149107102425

Epoch: 5| Step: 7
Training loss: 1.7666972875595093
Validation loss: 2.0212280340092157

Epoch: 5| Step: 8
Training loss: 3.1675174236297607
Validation loss: 2.0863538044755177

Epoch: 5| Step: 9
Training loss: 1.6558809280395508
Validation loss: 2.0408084264365574

Epoch: 5| Step: 10
Training loss: 1.2250633239746094
Validation loss: 2.08109212178056

Epoch: 172| Step: 0
Training loss: 1.5553158521652222
Validation loss: 2.052249945620055

Epoch: 5| Step: 1
Training loss: 2.0477330684661865
Validation loss: 2.06778464522413

Epoch: 5| Step: 2
Training loss: 1.7693564891815186
Validation loss: 2.081453461800852

Epoch: 5| Step: 3
Training loss: 2.5415282249450684
Validation loss: 2.026202306952528

Epoch: 5| Step: 4
Training loss: 1.8392703533172607
Validation loss: 2.045165172187231

Epoch: 5| Step: 5
Training loss: 2.1287930011749268
Validation loss: 2.0744688485258367

Epoch: 5| Step: 6
Training loss: 2.3756184577941895
Validation loss: 2.0917574885070964

Epoch: 5| Step: 7
Training loss: 1.453966736793518
Validation loss: 2.085857939976518

Epoch: 5| Step: 8
Training loss: 1.9580875635147095
Validation loss: 2.0777235748947307

Epoch: 5| Step: 9
Training loss: 2.335252285003662
Validation loss: 2.066146945440641

Epoch: 5| Step: 10
Training loss: 2.0298330783843994
Validation loss: 2.0401507475042857

Epoch: 173| Step: 0
Training loss: 1.601250410079956
Validation loss: 2.036329787264588

Epoch: 5| Step: 1
Training loss: 1.7950303554534912
Validation loss: 2.0480632448709137

Epoch: 5| Step: 2
Training loss: 2.134981155395508
Validation loss: 2.02429892939906

Epoch: 5| Step: 3
Training loss: 2.168086528778076
Validation loss: 2.081049783255464

Epoch: 5| Step: 4
Training loss: 1.9192081689834595
Validation loss: 2.0718365612850396

Epoch: 5| Step: 5
Training loss: 1.6977866888046265
Validation loss: 2.062295922669031

Epoch: 5| Step: 6
Training loss: 2.7160286903381348
Validation loss: 1.9837078663610643

Epoch: 5| Step: 7
Training loss: 2.1001906394958496
Validation loss: 2.071288847154187

Epoch: 5| Step: 8
Training loss: 2.6624703407287598
Validation loss: 2.0721530247760076

Epoch: 5| Step: 9
Training loss: 1.7067070007324219
Validation loss: 2.1032996408401

Epoch: 5| Step: 10
Training loss: 1.2524328231811523
Validation loss: 2.0554282588343464

Epoch: 174| Step: 0
Training loss: 1.7008447647094727
Validation loss: 2.045298477654816

Epoch: 5| Step: 1
Training loss: 2.0894782543182373
Validation loss: 2.069681813639979

Epoch: 5| Step: 2
Training loss: 2.3528454303741455
Validation loss: 2.0273728934667443

Epoch: 5| Step: 3
Training loss: 2.0607008934020996
Validation loss: 2.0447226083406838

Epoch: 5| Step: 4
Training loss: 2.0206868648529053
Validation loss: 2.042056337479622

Epoch: 5| Step: 5
Training loss: 2.0747246742248535
Validation loss: 2.0433433850606284

Epoch: 5| Step: 6
Training loss: 1.8191101551055908
Validation loss: 2.1023031639796432

Epoch: 5| Step: 7
Training loss: 1.9108402729034424
Validation loss: 2.0482372801790953

Epoch: 5| Step: 8
Training loss: 1.640652060508728
Validation loss: 2.058710839158745

Epoch: 5| Step: 9
Training loss: 2.059230089187622
Validation loss: 2.0565726910868

Epoch: 5| Step: 10
Training loss: 2.4164624214172363
Validation loss: 2.0540209072892384

Epoch: 175| Step: 0
Training loss: 1.965627908706665
Validation loss: 2.092934910969068

Epoch: 5| Step: 1
Training loss: 1.9262077808380127
Validation loss: 2.096162093582974

Epoch: 5| Step: 2
Training loss: 1.2092983722686768
Validation loss: 2.066955392078687

Epoch: 5| Step: 3
Training loss: 2.2100586891174316
Validation loss: 2.068726667793848

Epoch: 5| Step: 4
Training loss: 2.0539886951446533
Validation loss: 2.0570838246294247

Epoch: 5| Step: 5
Training loss: 2.252354383468628
Validation loss: 2.0431999750034784

Epoch: 5| Step: 6
Training loss: 2.3016839027404785
Validation loss: 2.0659184750690254

Epoch: 5| Step: 7
Training loss: 1.5970512628555298
Validation loss: 2.0844885610765025

Epoch: 5| Step: 8
Training loss: 1.8844718933105469
Validation loss: 2.0846697912421277

Epoch: 5| Step: 9
Training loss: 2.48846697807312
Validation loss: 2.033292890876852

Epoch: 5| Step: 10
Training loss: 2.1895275115966797
Validation loss: 2.052462130464533

Epoch: 176| Step: 0
Training loss: 2.1047115325927734
Validation loss: 2.0391876300175986

Epoch: 5| Step: 1
Training loss: 2.002974033355713
Validation loss: 2.057491265317445

Epoch: 5| Step: 2
Training loss: 1.2991819381713867
Validation loss: 2.0738783779964653

Epoch: 5| Step: 3
Training loss: 2.074784278869629
Validation loss: 2.0369450328170613

Epoch: 5| Step: 4
Training loss: 2.3801655769348145
Validation loss: 2.143452134183658

Epoch: 5| Step: 5
Training loss: 2.293330669403076
Validation loss: 2.086675855421251

Epoch: 5| Step: 6
Training loss: 1.9849439859390259
Validation loss: 2.0468909458447526

Epoch: 5| Step: 7
Training loss: 2.10380482673645
Validation loss: 2.0670381412711194

Epoch: 5| Step: 8
Training loss: 1.5948556661605835
Validation loss: 2.066153990325107

Epoch: 5| Step: 9
Training loss: 1.2342474460601807
Validation loss: 2.051910090190108

Epoch: 5| Step: 10
Training loss: 2.520609140396118
Validation loss: 2.027706507713564

Epoch: 177| Step: 0
Training loss: 1.987614393234253
Validation loss: 2.033489377267899

Epoch: 5| Step: 1
Training loss: 2.5182294845581055
Validation loss: 2.0487805361388833

Epoch: 5| Step: 2
Training loss: 2.041106700897217
Validation loss: 2.072025824618596

Epoch: 5| Step: 3
Training loss: 1.4552007913589478
Validation loss: 2.0310340389128654

Epoch: 5| Step: 4
Training loss: 2.7246298789978027
Validation loss: 2.042601454642511

Epoch: 5| Step: 5
Training loss: 2.113805055618286
Validation loss: 2.067885470646684

Epoch: 5| Step: 6
Training loss: 2.0880777835845947
Validation loss: 2.02089968035298

Epoch: 5| Step: 7
Training loss: 1.7980115413665771
Validation loss: 2.065130942611284

Epoch: 5| Step: 8
Training loss: 2.000373363494873
Validation loss: 2.0425503805119503

Epoch: 5| Step: 9
Training loss: 1.5655925273895264
Validation loss: 2.0754187081449773

Epoch: 5| Step: 10
Training loss: 1.6380023956298828
Validation loss: 2.0416475367802445

Epoch: 178| Step: 0
Training loss: 2.094656229019165
Validation loss: 2.080783182574857

Epoch: 5| Step: 1
Training loss: 1.6888408660888672
Validation loss: 2.036722380627868

Epoch: 5| Step: 2
Training loss: 2.8540306091308594
Validation loss: 2.050781785800893

Epoch: 5| Step: 3
Training loss: 2.3732502460479736
Validation loss: 2.0611631870269775

Epoch: 5| Step: 4
Training loss: 1.2686063051223755
Validation loss: 2.054940777440225

Epoch: 5| Step: 5
Training loss: 2.3986520767211914
Validation loss: 2.0640760365352837

Epoch: 5| Step: 6
Training loss: 1.6321595907211304
Validation loss: 2.0843390495546403

Epoch: 5| Step: 7
Training loss: 1.8780502080917358
Validation loss: 2.0302280533698296

Epoch: 5| Step: 8
Training loss: 1.4993137121200562
Validation loss: 2.068199837079612

Epoch: 5| Step: 9
Training loss: 2.053954601287842
Validation loss: 2.043894278105869

Epoch: 5| Step: 10
Training loss: 2.0863327980041504
Validation loss: 2.0489394075127056

Epoch: 179| Step: 0
Training loss: 2.4482407569885254
Validation loss: 2.0376672052568003

Epoch: 5| Step: 1
Training loss: 1.4136250019073486
Validation loss: 2.031121733368084

Epoch: 5| Step: 2
Training loss: 1.8127117156982422
Validation loss: 2.0480047169552056

Epoch: 5| Step: 3
Training loss: 2.457815647125244
Validation loss: 2.0298575611524683

Epoch: 5| Step: 4
Training loss: 2.47589111328125
Validation loss: 2.052214609679355

Epoch: 5| Step: 5
Training loss: 2.447506904602051
Validation loss: 2.0853428584273144

Epoch: 5| Step: 6
Training loss: 1.9058904647827148
Validation loss: 2.09771502530703

Epoch: 5| Step: 7
Training loss: 1.705134391784668
Validation loss: 2.07274595896403

Epoch: 5| Step: 8
Training loss: 1.869847059249878
Validation loss: 2.041861100863385

Epoch: 5| Step: 9
Training loss: 1.800964117050171
Validation loss: 2.04123443172824

Epoch: 5| Step: 10
Training loss: 1.6029633283615112
Validation loss: 2.0615563020911267

Epoch: 180| Step: 0
Training loss: 1.8628524541854858
Validation loss: 2.073729181802401

Epoch: 5| Step: 1
Training loss: 2.154719829559326
Validation loss: 2.0544703827109387

Epoch: 5| Step: 2
Training loss: 1.5324108600616455
Validation loss: 2.0450492482031546

Epoch: 5| Step: 3
Training loss: 1.2392289638519287
Validation loss: 2.069156212191428

Epoch: 5| Step: 4
Training loss: 1.952108383178711
Validation loss: 2.0813245401587537

Epoch: 5| Step: 5
Training loss: 2.5217232704162598
Validation loss: 2.0537921613262546

Epoch: 5| Step: 6
Training loss: 1.6640266180038452
Validation loss: 2.0574800327260006

Epoch: 5| Step: 7
Training loss: 2.236847400665283
Validation loss: 2.008772219381025

Epoch: 5| Step: 8
Training loss: 2.751826524734497
Validation loss: 2.031838381162254

Epoch: 5| Step: 9
Training loss: 2.1866698265075684
Validation loss: 2.0477427180095384

Epoch: 5| Step: 10
Training loss: 1.739134430885315
Validation loss: 2.0296391210248395

Epoch: 181| Step: 0
Training loss: 2.382216215133667
Validation loss: 2.062980059654482

Epoch: 5| Step: 1
Training loss: 2.1360530853271484
Validation loss: 2.058767895544729

Epoch: 5| Step: 2
Training loss: 2.5399069786071777
Validation loss: 2.046074075083579

Epoch: 5| Step: 3
Training loss: 2.203294038772583
Validation loss: 2.040768887407036

Epoch: 5| Step: 4
Training loss: 1.5230008363723755
Validation loss: 2.0829304700256674

Epoch: 5| Step: 5
Training loss: 1.4136797189712524
Validation loss: 2.0522412792328866

Epoch: 5| Step: 6
Training loss: 1.2505496740341187
Validation loss: 2.052769048239595

Epoch: 5| Step: 7
Training loss: 1.5038974285125732
Validation loss: 2.0593693282014582

Epoch: 5| Step: 8
Training loss: 2.5938808917999268
Validation loss: 2.05230015067644

Epoch: 5| Step: 9
Training loss: 2.0888452529907227
Validation loss: 2.019019555020076

Epoch: 5| Step: 10
Training loss: 2.098214626312256
Validation loss: 2.0508957908999537

Epoch: 182| Step: 0
Training loss: 1.9277957677841187
Validation loss: 2.069503608570304

Epoch: 5| Step: 1
Training loss: 2.338202476501465
Validation loss: 2.0477920014371156

Epoch: 5| Step: 2
Training loss: 2.2087676525115967
Validation loss: 2.039773841058054

Epoch: 5| Step: 3
Training loss: 2.2009708881378174
Validation loss: 2.0495222614657496

Epoch: 5| Step: 4
Training loss: 2.195330858230591
Validation loss: 2.051612057993489

Epoch: 5| Step: 5
Training loss: 1.6278257369995117
Validation loss: 2.0307574682338263

Epoch: 5| Step: 6
Training loss: 1.7366348505020142
Validation loss: 2.0491578425130537

Epoch: 5| Step: 7
Training loss: 1.999868631362915
Validation loss: 2.0665390453030987

Epoch: 5| Step: 8
Training loss: 2.4587810039520264
Validation loss: 2.0502264243300243

Epoch: 5| Step: 9
Training loss: 1.552618384361267
Validation loss: 2.036094557854437

Epoch: 5| Step: 10
Training loss: 1.484282374382019
Validation loss: 2.0526478828922397

Epoch: 183| Step: 0
Training loss: 2.0158474445343018
Validation loss: 2.0854455014710784

Epoch: 5| Step: 1
Training loss: 2.2747414112091064
Validation loss: 2.047514421965486

Epoch: 5| Step: 2
Training loss: 1.9686428308486938
Validation loss: 2.0723909357542634

Epoch: 5| Step: 3
Training loss: 2.4659438133239746
Validation loss: 2.0820744652901926

Epoch: 5| Step: 4
Training loss: 1.3312206268310547
Validation loss: 2.093988759543306

Epoch: 5| Step: 5
Training loss: 2.4779183864593506
Validation loss: 2.0259221125674505

Epoch: 5| Step: 6
Training loss: 1.8727357387542725
Validation loss: 2.061798546903877

Epoch: 5| Step: 7
Training loss: 2.3618178367614746
Validation loss: 2.066108436994655

Epoch: 5| Step: 8
Training loss: 2.1618475914001465
Validation loss: 2.0490649733492123

Epoch: 5| Step: 9
Training loss: 1.320351004600525
Validation loss: 2.028023194241267

Epoch: 5| Step: 10
Training loss: 1.72879159450531
Validation loss: 2.0313612043216662

Epoch: 184| Step: 0
Training loss: 2.345634937286377
Validation loss: 2.0602397559791483

Epoch: 5| Step: 1
Training loss: 2.0802395343780518
Validation loss: 2.062136177093752

Epoch: 5| Step: 2
Training loss: 1.9061212539672852
Validation loss: 2.0867853331309494

Epoch: 5| Step: 3
Training loss: 1.779741883277893
Validation loss: 2.023800173113423

Epoch: 5| Step: 4
Training loss: 2.1036505699157715
Validation loss: 2.063838751085343

Epoch: 5| Step: 5
Training loss: 1.4306552410125732
Validation loss: 2.0243149970167424

Epoch: 5| Step: 6
Training loss: 2.0203464031219482
Validation loss: 2.0235708926313665

Epoch: 5| Step: 7
Training loss: 2.1301839351654053
Validation loss: 2.059187880126379

Epoch: 5| Step: 8
Training loss: 1.5750668048858643
Validation loss: 2.0858230808729767

Epoch: 5| Step: 9
Training loss: 2.2527737617492676
Validation loss: 2.078871969253786

Epoch: 5| Step: 10
Training loss: 2.115950107574463
Validation loss: 2.091226987941291

Epoch: 185| Step: 0
Training loss: 1.7415339946746826
Validation loss: 2.0644293549240276

Epoch: 5| Step: 1
Training loss: 2.240262508392334
Validation loss: 2.0892056085730113

Epoch: 5| Step: 2
Training loss: 2.6427574157714844
Validation loss: 2.079567378567111

Epoch: 5| Step: 3
Training loss: 1.5065419673919678
Validation loss: 2.0283740246167747

Epoch: 5| Step: 4
Training loss: 2.785731315612793
Validation loss: 2.046366173733947

Epoch: 5| Step: 5
Training loss: 1.9956562519073486
Validation loss: 2.0484537924489667

Epoch: 5| Step: 6
Training loss: 2.069042205810547
Validation loss: 2.0681224356415453

Epoch: 5| Step: 7
Training loss: 1.9635003805160522
Validation loss: 2.0763232067067134

Epoch: 5| Step: 8
Training loss: 1.3428678512573242
Validation loss: 2.0631838895941295

Epoch: 5| Step: 9
Training loss: 1.4930102825164795
Validation loss: 2.0645568922001827

Epoch: 5| Step: 10
Training loss: 1.691711187362671
Validation loss: 2.065287478508488

Epoch: 186| Step: 0
Training loss: 2.1987204551696777
Validation loss: 2.0626900337075673

Epoch: 5| Step: 1
Training loss: 1.8223552703857422
Validation loss: 2.090968208928262

Epoch: 5| Step: 2
Training loss: 1.8763843774795532
Validation loss: 2.0672960781281993

Epoch: 5| Step: 3
Training loss: 2.0189998149871826
Validation loss: 2.0635987840672976

Epoch: 5| Step: 4
Training loss: 1.865966796875
Validation loss: 2.0615370991409465

Epoch: 5| Step: 5
Training loss: 1.3857297897338867
Validation loss: 2.0420572539811492

Epoch: 5| Step: 6
Training loss: 2.2227416038513184
Validation loss: 2.050882488168696

Epoch: 5| Step: 7
Training loss: 1.5188570022583008
Validation loss: 2.0296661674335437

Epoch: 5| Step: 8
Training loss: 1.470428228378296
Validation loss: 2.0665548706567414

Epoch: 5| Step: 9
Training loss: 2.8528146743774414
Validation loss: 2.0317356765911145

Epoch: 5| Step: 10
Training loss: 2.1427066326141357
Validation loss: 2.0172746284033662

Epoch: 187| Step: 0
Training loss: 1.9708493947982788
Validation loss: 2.0530023267192226

Epoch: 5| Step: 1
Training loss: 1.770784616470337
Validation loss: 2.031178806417732

Epoch: 5| Step: 2
Training loss: 2.3327083587646484
Validation loss: 2.0341206391652427

Epoch: 5| Step: 3
Training loss: 1.3805210590362549
Validation loss: 2.056995819973689

Epoch: 5| Step: 4
Training loss: 1.8154243230819702
Validation loss: 2.0424471580854027

Epoch: 5| Step: 5
Training loss: 1.7665611505508423
Validation loss: 2.0373103618621826

Epoch: 5| Step: 6
Training loss: 2.659585475921631
Validation loss: 2.041251932421038

Epoch: 5| Step: 7
Training loss: 2.4749159812927246
Validation loss: 2.047037434834306

Epoch: 5| Step: 8
Training loss: 1.8958200216293335
Validation loss: 2.0150787804716375

Epoch: 5| Step: 9
Training loss: 2.41749906539917
Validation loss: 2.0727971574311614

Epoch: 5| Step: 10
Training loss: 1.3012628555297852
Validation loss: 2.055358591900077

Epoch: 188| Step: 0
Training loss: 1.3217066526412964
Validation loss: 2.0015307190597698

Epoch: 5| Step: 1
Training loss: 1.7708930969238281
Validation loss: 2.008135659720308

Epoch: 5| Step: 2
Training loss: 1.5665920972824097
Validation loss: 2.04605161118251

Epoch: 5| Step: 3
Training loss: 2.2907750606536865
Validation loss: 2.062026649393061

Epoch: 5| Step: 4
Training loss: 2.6889684200286865
Validation loss: 2.025447878786313

Epoch: 5| Step: 5
Training loss: 1.986181616783142
Validation loss: 2.0698810085173576

Epoch: 5| Step: 6
Training loss: 2.506718158721924
Validation loss: 2.073314082237982

Epoch: 5| Step: 7
Training loss: 1.5941402912139893
Validation loss: 2.0689020567042853

Epoch: 5| Step: 8
Training loss: 1.7900686264038086
Validation loss: 2.091834247753184

Epoch: 5| Step: 9
Training loss: 1.9973911046981812
Validation loss: 2.0469983623873804

Epoch: 5| Step: 10
Training loss: 2.144192695617676
Validation loss: 2.0825323443258963

Epoch: 189| Step: 0
Training loss: 2.1969597339630127
Validation loss: 2.042924342616912

Epoch: 5| Step: 1
Training loss: 1.865485429763794
Validation loss: 2.0825381022627636

Epoch: 5| Step: 2
Training loss: 1.6704108715057373
Validation loss: 2.046773611858327

Epoch: 5| Step: 3
Training loss: 1.3963907957077026
Validation loss: 2.039493642827516

Epoch: 5| Step: 4
Training loss: 2.314955472946167
Validation loss: 2.027214461757291

Epoch: 5| Step: 5
Training loss: 2.289668560028076
Validation loss: 2.0806289872815533

Epoch: 5| Step: 6
Training loss: 1.5650017261505127
Validation loss: 2.029559980156601

Epoch: 5| Step: 7
Training loss: 2.406586170196533
Validation loss: 2.0308220309595906

Epoch: 5| Step: 8
Training loss: 2.2907724380493164
Validation loss: 2.0144662498145975

Epoch: 5| Step: 9
Training loss: 1.8919658660888672
Validation loss: 2.038652409789383

Epoch: 5| Step: 10
Training loss: 1.664893388748169
Validation loss: 2.067032192343025

Epoch: 190| Step: 0
Training loss: 2.0559067726135254
Validation loss: 2.0285186767578125

Epoch: 5| Step: 1
Training loss: 2.1168785095214844
Validation loss: 2.0303779571287093

Epoch: 5| Step: 2
Training loss: 1.8882217407226562
Validation loss: 2.034230698821365

Epoch: 5| Step: 3
Training loss: 2.424315929412842
Validation loss: 2.0127254250229045

Epoch: 5| Step: 4
Training loss: 1.3513787984848022
Validation loss: 2.030319797095432

Epoch: 5| Step: 5
Training loss: 2.390237331390381
Validation loss: 1.998357831790883

Epoch: 5| Step: 6
Training loss: 2.328713893890381
Validation loss: 2.0360417660846504

Epoch: 5| Step: 7
Training loss: 1.4910610914230347
Validation loss: 2.027015404034686

Epoch: 5| Step: 8
Training loss: 1.53496515750885
Validation loss: 2.038871713863906

Epoch: 5| Step: 9
Training loss: 2.1888749599456787
Validation loss: 1.995669763575318

Epoch: 5| Step: 10
Training loss: 1.482340931892395
Validation loss: 2.061324727150702

Epoch: 191| Step: 0
Training loss: 2.1041934490203857
Validation loss: 2.0123754509033693

Epoch: 5| Step: 1
Training loss: 2.1692006587982178
Validation loss: 2.001856288602275

Epoch: 5| Step: 2
Training loss: 1.9060719013214111
Validation loss: 2.030751250123465

Epoch: 5| Step: 3
Training loss: 1.8101660013198853
Validation loss: 2.0192106462294057

Epoch: 5| Step: 4
Training loss: 1.6731939315795898
Validation loss: 2.02278055426895

Epoch: 5| Step: 5
Training loss: 2.1772265434265137
Validation loss: 2.059686630002914

Epoch: 5| Step: 6
Training loss: 1.8604278564453125
Validation loss: 2.0467158799530356

Epoch: 5| Step: 7
Training loss: 1.9277080297470093
Validation loss: 2.0237133259414346

Epoch: 5| Step: 8
Training loss: 1.9355649948120117
Validation loss: 2.0487718197607223

Epoch: 5| Step: 9
Training loss: 2.5008082389831543
Validation loss: 2.0718901298379384

Epoch: 5| Step: 10
Training loss: 1.3281140327453613
Validation loss: 2.0638397662870345

Epoch: 192| Step: 0
Training loss: 1.8658313751220703
Validation loss: 2.0524779878636843

Epoch: 5| Step: 1
Training loss: 2.0249056816101074
Validation loss: 2.0745838701084094

Epoch: 5| Step: 2
Training loss: 2.2851815223693848
Validation loss: 2.091811283942192

Epoch: 5| Step: 3
Training loss: 2.2511186599731445
Validation loss: 2.0285172449645175

Epoch: 5| Step: 4
Training loss: 1.5288918018341064
Validation loss: 2.073289112378192

Epoch: 5| Step: 5
Training loss: 1.8031485080718994
Validation loss: 2.053183424857355

Epoch: 5| Step: 6
Training loss: 1.7525240182876587
Validation loss: 2.0570012190008677

Epoch: 5| Step: 7
Training loss: 2.026013135910034
Validation loss: 2.051502889202487

Epoch: 5| Step: 8
Training loss: 1.898991346359253
Validation loss: 2.0431832998029646

Epoch: 5| Step: 9
Training loss: 2.0858407020568848
Validation loss: 2.0147525110552387

Epoch: 5| Step: 10
Training loss: 2.0047287940979004
Validation loss: 2.0411372133480605

Epoch: 193| Step: 0
Training loss: 2.252803087234497
Validation loss: 2.041595797384939

Epoch: 5| Step: 1
Training loss: 1.4591087102890015
Validation loss: 2.0328685878425516

Epoch: 5| Step: 2
Training loss: 1.0278326272964478
Validation loss: 2.0460405670186526

Epoch: 5| Step: 3
Training loss: 1.934591293334961
Validation loss: 2.066134388728808

Epoch: 5| Step: 4
Training loss: 1.5821658372879028
Validation loss: 2.05473824983002

Epoch: 5| Step: 5
Training loss: 1.822648048400879
Validation loss: 2.019559657701882

Epoch: 5| Step: 6
Training loss: 2.9919257164001465
Validation loss: 2.033267326252435

Epoch: 5| Step: 7
Training loss: 2.456575393676758
Validation loss: 2.0423739033360637

Epoch: 5| Step: 8
Training loss: 2.0382778644561768
Validation loss: 2.028560766609766

Epoch: 5| Step: 9
Training loss: 1.5644830465316772
Validation loss: 2.0429532886833273

Epoch: 5| Step: 10
Training loss: 1.9886329174041748
Validation loss: 2.040677544891193

Epoch: 194| Step: 0
Training loss: 1.840497612953186
Validation loss: 2.064565658569336

Epoch: 5| Step: 1
Training loss: 1.9758151769638062
Validation loss: 1.9902890190001457

Epoch: 5| Step: 2
Training loss: 1.9379276037216187
Validation loss: 2.0205558358982043

Epoch: 5| Step: 3
Training loss: 1.6239593029022217
Validation loss: 1.9946280499940277

Epoch: 5| Step: 4
Training loss: 1.7511875629425049
Validation loss: 2.0188226661374493

Epoch: 5| Step: 5
Training loss: 2.2495217323303223
Validation loss: 2.0165432409573625

Epoch: 5| Step: 6
Training loss: 1.582090139389038
Validation loss: 2.0040281241939915

Epoch: 5| Step: 7
Training loss: 1.8012325763702393
Validation loss: 1.9633403362766388

Epoch: 5| Step: 8
Training loss: 2.3831863403320312
Validation loss: 2.016616339324623

Epoch: 5| Step: 9
Training loss: 2.027627468109131
Validation loss: 2.014700044867813

Epoch: 5| Step: 10
Training loss: 2.3517143726348877
Validation loss: 2.0222514265327045

Epoch: 195| Step: 0
Training loss: 1.6239426136016846
Validation loss: 2.019334075271442

Epoch: 5| Step: 1
Training loss: 1.9834940433502197
Validation loss: 2.0133316850149505

Epoch: 5| Step: 2
Training loss: 1.7739547491073608
Validation loss: 2.047976989899912

Epoch: 5| Step: 3
Training loss: 2.031813859939575
Validation loss: 2.0104210504921536

Epoch: 5| Step: 4
Training loss: 1.5449786186218262
Validation loss: 2.0219816264285835

Epoch: 5| Step: 5
Training loss: 2.1566519737243652
Validation loss: 2.051329028221869

Epoch: 5| Step: 6
Training loss: 2.0275464057922363
Validation loss: 1.9668815648683937

Epoch: 5| Step: 7
Training loss: 1.913601279258728
Validation loss: 2.022801847868068

Epoch: 5| Step: 8
Training loss: 1.8038651943206787
Validation loss: 1.9834521573076966

Epoch: 5| Step: 9
Training loss: 2.0543105602264404
Validation loss: 2.0504113448563444

Epoch: 5| Step: 10
Training loss: 2.8231353759765625
Validation loss: 2.010269949513097

Epoch: 196| Step: 0
Training loss: 1.8914772272109985
Validation loss: 2.0257881918261127

Epoch: 5| Step: 1
Training loss: 1.418980360031128
Validation loss: 2.0076066114569224

Epoch: 5| Step: 2
Training loss: 1.9048271179199219
Validation loss: 2.058632368682533

Epoch: 5| Step: 3
Training loss: 2.1550745964050293
Validation loss: 1.981115315550117

Epoch: 5| Step: 4
Training loss: 2.1763792037963867
Validation loss: 2.0268263252832557

Epoch: 5| Step: 5
Training loss: 2.06270432472229
Validation loss: 2.0270488749268236

Epoch: 5| Step: 6
Training loss: 2.352841377258301
Validation loss: 2.011668077079199

Epoch: 5| Step: 7
Training loss: 2.0569417476654053
Validation loss: 2.005662420744537

Epoch: 5| Step: 8
Training loss: 1.9528146982192993
Validation loss: 2.025364932193551

Epoch: 5| Step: 9
Training loss: 1.4493181705474854
Validation loss: 2.035513824032199

Epoch: 5| Step: 10
Training loss: 1.867894172668457
Validation loss: 2.03877188569756

Epoch: 197| Step: 0
Training loss: 2.0553195476531982
Validation loss: 2.0045742911677205

Epoch: 5| Step: 1
Training loss: 1.382047414779663
Validation loss: 2.0167920281810146

Epoch: 5| Step: 2
Training loss: 2.2651429176330566
Validation loss: 2.042170760452106

Epoch: 5| Step: 3
Training loss: 2.33892822265625
Validation loss: 2.0268876552581787

Epoch: 5| Step: 4
Training loss: 1.837651014328003
Validation loss: 2.0133139036035024

Epoch: 5| Step: 5
Training loss: 2.506415605545044
Validation loss: 2.0562404765877673

Epoch: 5| Step: 6
Training loss: 1.8325560092926025
Validation loss: 2.0081761549877863

Epoch: 5| Step: 7
Training loss: 1.4226016998291016
Validation loss: 2.003461842895836

Epoch: 5| Step: 8
Training loss: 2.017165422439575
Validation loss: 2.0089803306005334

Epoch: 5| Step: 9
Training loss: 1.1557739973068237
Validation loss: 2.0296299175549577

Epoch: 5| Step: 10
Training loss: 2.625962495803833
Validation loss: 2.029905826814713

Epoch: 198| Step: 0
Training loss: 2.0025787353515625
Validation loss: 2.006332423097344

Epoch: 5| Step: 1
Training loss: 1.8870407342910767
Validation loss: 2.011131102038968

Epoch: 5| Step: 2
Training loss: 3.0597341060638428
Validation loss: 2.0730393176437705

Epoch: 5| Step: 3
Training loss: 2.4819366931915283
Validation loss: 2.0475921220676874

Epoch: 5| Step: 4
Training loss: 2.1387135982513428
Validation loss: 2.0584725000525035

Epoch: 5| Step: 5
Training loss: 1.8168083429336548
Validation loss: 2.052493877308343

Epoch: 5| Step: 6
Training loss: 1.5884971618652344
Validation loss: 1.9858649059008526

Epoch: 5| Step: 7
Training loss: 2.5528087615966797
Validation loss: 2.009632251595938

Epoch: 5| Step: 8
Training loss: 1.0182701349258423
Validation loss: 2.0402576372187626

Epoch: 5| Step: 9
Training loss: 1.522973656654358
Validation loss: 2.004529618447827

Epoch: 5| Step: 10
Training loss: 1.4319519996643066
Validation loss: 2.0265620575156262

Epoch: 199| Step: 0
Training loss: 2.4155375957489014
Validation loss: 1.966507742481847

Epoch: 5| Step: 1
Training loss: 1.932504653930664
Validation loss: 2.0007427866740892

Epoch: 5| Step: 2
Training loss: 1.89776611328125
Validation loss: 2.008733457134616

Epoch: 5| Step: 3
Training loss: 1.8995729684829712
Validation loss: 1.9964897145507157

Epoch: 5| Step: 4
Training loss: 1.479112982749939
Validation loss: 2.0059420434377526

Epoch: 5| Step: 5
Training loss: 2.244020938873291
Validation loss: 2.008116065814931

Epoch: 5| Step: 6
Training loss: 1.6372735500335693
Validation loss: 2.007820160158219

Epoch: 5| Step: 7
Training loss: 2.2172622680664062
Validation loss: 1.978388117205712

Epoch: 5| Step: 8
Training loss: 2.417118787765503
Validation loss: 2.017899908045287

Epoch: 5| Step: 9
Training loss: 1.847198247909546
Validation loss: 1.9968443762871526

Epoch: 5| Step: 10
Training loss: 1.3091522455215454
Validation loss: 2.0407510983046664

Epoch: 200| Step: 0
Training loss: 1.442964792251587
Validation loss: 2.0227403692019883

Epoch: 5| Step: 1
Training loss: 2.4990487098693848
Validation loss: 2.0139707954980994

Epoch: 5| Step: 2
Training loss: 1.976424217224121
Validation loss: 1.9991478125254314

Epoch: 5| Step: 3
Training loss: 2.2189295291900635
Validation loss: 2.0293659446060017

Epoch: 5| Step: 4
Training loss: 1.5337625741958618
Validation loss: 2.036046030700848

Epoch: 5| Step: 5
Training loss: 2.4244329929351807
Validation loss: 1.9978496387440672

Epoch: 5| Step: 6
Training loss: 2.1152472496032715
Validation loss: 2.0293920809222805

Epoch: 5| Step: 7
Training loss: 2.113260507583618
Validation loss: 2.024108332972373

Epoch: 5| Step: 8
Training loss: 1.3372714519500732
Validation loss: 1.979750338421073

Epoch: 5| Step: 9
Training loss: 2.0215377807617188
Validation loss: 2.061111891141502

Epoch: 5| Step: 10
Training loss: 1.6918883323669434
Validation loss: 2.0277450623050814

Testing loss: 2.1717651420169406

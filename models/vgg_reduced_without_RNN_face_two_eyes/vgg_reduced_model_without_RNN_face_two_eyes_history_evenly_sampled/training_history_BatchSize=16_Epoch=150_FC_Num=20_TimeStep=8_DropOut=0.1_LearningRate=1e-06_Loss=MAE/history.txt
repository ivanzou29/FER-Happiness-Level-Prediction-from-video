Epoch: 1| Step: 0
Training loss: 4.76380729675293
Validation loss: 5.5524352904289

Epoch: 6| Step: 1
Training loss: 6.388466835021973
Validation loss: 5.549063087791525

Epoch: 6| Step: 2
Training loss: 6.420416831970215
Validation loss: 5.542064143765357

Epoch: 6| Step: 3
Training loss: 5.615277290344238
Validation loss: 5.537056938294442

Epoch: 6| Step: 4
Training loss: 5.3951568603515625
Validation loss: 5.531083106994629

Epoch: 6| Step: 5
Training loss: 6.252274036407471
Validation loss: 5.526284181943503

Epoch: 6| Step: 6
Training loss: 6.557608604431152
Validation loss: 5.517905030199277

Epoch: 6| Step: 7
Training loss: 5.322481155395508
Validation loss: 5.510342264688143

Epoch: 6| Step: 8
Training loss: 4.54141902923584
Validation loss: 5.505088975352626

Epoch: 6| Step: 9
Training loss: 4.871360778808594
Validation loss: 5.500066582874585

Epoch: 6| Step: 10
Training loss: 3.755432605743408
Validation loss: 5.493342579052013

Epoch: 6| Step: 11
Training loss: 3.7690486907958984
Validation loss: 5.486731170326151

Epoch: 6| Step: 12
Training loss: 5.01185417175293
Validation loss: 5.481581580254339

Epoch: 6| Step: 13
Training loss: 5.955308437347412
Validation loss: 5.473789076651296

Epoch: 2| Step: 0
Training loss: 5.783299446105957
Validation loss: 5.4711684462844685

Epoch: 6| Step: 1
Training loss: 4.4910125732421875
Validation loss: 5.4634043580742295

Epoch: 6| Step: 2
Training loss: 5.359168529510498
Validation loss: 5.458302954191803

Epoch: 6| Step: 3
Training loss: 6.248911380767822
Validation loss: 5.4521838003589265

Epoch: 6| Step: 4
Training loss: 7.286627292633057
Validation loss: 5.446342042697373

Epoch: 6| Step: 5
Training loss: 5.059542655944824
Validation loss: 5.439863328010805

Epoch: 6| Step: 6
Training loss: 4.787105083465576
Validation loss: 5.433229466920258

Epoch: 6| Step: 7
Training loss: 4.803995132446289
Validation loss: 5.426972394348473

Epoch: 6| Step: 8
Training loss: 5.381460666656494
Validation loss: 5.417336581855692

Epoch: 6| Step: 9
Training loss: 5.990378379821777
Validation loss: 5.414153714333811

Epoch: 6| Step: 10
Training loss: 4.642892360687256
Validation loss: 5.408630201893468

Epoch: 6| Step: 11
Training loss: 4.29127311706543
Validation loss: 5.400714351284888

Epoch: 6| Step: 12
Training loss: 4.928326606750488
Validation loss: 5.394076301205542

Epoch: 6| Step: 13
Training loss: 3.1468472480773926
Validation loss: 5.3903698664839546

Epoch: 3| Step: 0
Training loss: 4.946792125701904
Validation loss: 5.3811706419914

Epoch: 6| Step: 1
Training loss: 4.535409450531006
Validation loss: 5.375000071781938

Epoch: 6| Step: 2
Training loss: 5.18707275390625
Validation loss: 5.371195736751761

Epoch: 6| Step: 3
Training loss: 5.582087993621826
Validation loss: 5.363135486520747

Epoch: 6| Step: 4
Training loss: 5.902617454528809
Validation loss: 5.355285136930404

Epoch: 6| Step: 5
Training loss: 4.827873706817627
Validation loss: 5.347346454538325

Epoch: 6| Step: 6
Training loss: 5.719943046569824
Validation loss: 5.343181210179483

Epoch: 6| Step: 7
Training loss: 5.981868743896484
Validation loss: 5.336325804392497

Epoch: 6| Step: 8
Training loss: 5.878833770751953
Validation loss: 5.330028154516733

Epoch: 6| Step: 9
Training loss: 4.043745040893555
Validation loss: 5.323671920325166

Epoch: 6| Step: 10
Training loss: 5.535788059234619
Validation loss: 5.3142047953862015

Epoch: 6| Step: 11
Training loss: 4.578876495361328
Validation loss: 5.308427241540724

Epoch: 6| Step: 12
Training loss: 4.093745231628418
Validation loss: 5.303181253453737

Epoch: 6| Step: 13
Training loss: 4.92305850982666
Validation loss: 5.296280481482065

Epoch: 4| Step: 0
Training loss: 5.3785481452941895
Validation loss: 5.286925105638401

Epoch: 6| Step: 1
Training loss: 4.811502933502197
Validation loss: 5.281088936713434

Epoch: 6| Step: 2
Training loss: 4.8885345458984375
Validation loss: 5.274243805998115

Epoch: 6| Step: 3
Training loss: 4.407990455627441
Validation loss: 5.269262359988305

Epoch: 6| Step: 4
Training loss: 5.262673377990723
Validation loss: 5.259523263541601

Epoch: 6| Step: 5
Training loss: 6.07397985458374
Validation loss: 5.253163235161894

Epoch: 6| Step: 6
Training loss: 5.165875434875488
Validation loss: 5.244696176180276

Epoch: 6| Step: 7
Training loss: 5.162847995758057
Validation loss: 5.237150458879368

Epoch: 6| Step: 8
Training loss: 5.473604202270508
Validation loss: 5.2298446932146625

Epoch: 6| Step: 9
Training loss: 4.588707447052002
Validation loss: 5.222151710141089

Epoch: 6| Step: 10
Training loss: 5.948334693908691
Validation loss: 5.214663003080634

Epoch: 6| Step: 11
Training loss: 3.677837371826172
Validation loss: 5.208848881465133

Epoch: 6| Step: 12
Training loss: 4.859379768371582
Validation loss: 5.1993223005725495

Epoch: 6| Step: 13
Training loss: 4.514022350311279
Validation loss: 5.194622875541769

Epoch: 5| Step: 0
Training loss: 4.357483386993408
Validation loss: 5.185722843293221

Epoch: 6| Step: 1
Training loss: 6.023964881896973
Validation loss: 5.177162319101313

Epoch: 6| Step: 2
Training loss: 4.792055130004883
Validation loss: 5.1672941125849245

Epoch: 6| Step: 3
Training loss: 5.180859088897705
Validation loss: 5.160028749896634

Epoch: 6| Step: 4
Training loss: 6.89832878112793
Validation loss: 5.154060276605749

Epoch: 6| Step: 5
Training loss: 5.001988410949707
Validation loss: 5.144343206959386

Epoch: 6| Step: 6
Training loss: 5.589615345001221
Validation loss: 5.136176786115093

Epoch: 6| Step: 7
Training loss: 3.521106243133545
Validation loss: 5.126372250177527

Epoch: 6| Step: 8
Training loss: 5.2299981117248535
Validation loss: 5.11791681474255

Epoch: 6| Step: 9
Training loss: 4.721957683563232
Validation loss: 5.108826770577379

Epoch: 6| Step: 10
Training loss: 4.503852844238281
Validation loss: 5.102168590791764

Epoch: 6| Step: 11
Training loss: 4.578487873077393
Validation loss: 5.091514348983765

Epoch: 6| Step: 12
Training loss: 3.999631404876709
Validation loss: 5.079801159520303

Epoch: 6| Step: 13
Training loss: 4.079184055328369
Validation loss: 5.074551787427676

Epoch: 6| Step: 0
Training loss: 5.149062633514404
Validation loss: 5.064814311201855

Epoch: 6| Step: 1
Training loss: 4.329396724700928
Validation loss: 5.054120622655397

Epoch: 6| Step: 2
Training loss: 5.086590766906738
Validation loss: 5.046975299876223

Epoch: 6| Step: 3
Training loss: 4.977756977081299
Validation loss: 5.037613499549128

Epoch: 6| Step: 4
Training loss: 5.444722652435303
Validation loss: 5.029340815800492

Epoch: 6| Step: 5
Training loss: 5.4288153648376465
Validation loss: 5.017027383209557

Epoch: 6| Step: 6
Training loss: 4.236218452453613
Validation loss: 5.004974170397687

Epoch: 6| Step: 7
Training loss: 4.414180755615234
Validation loss: 4.998706212607763

Epoch: 6| Step: 8
Training loss: 4.670112133026123
Validation loss: 4.989328722799978

Epoch: 6| Step: 9
Training loss: 4.620078086853027
Validation loss: 4.977512467292048

Epoch: 6| Step: 10
Training loss: 4.343377113342285
Validation loss: 4.96826825603362

Epoch: 6| Step: 11
Training loss: 4.685184001922607
Validation loss: 4.958054173377253

Epoch: 6| Step: 12
Training loss: 4.7881245613098145
Validation loss: 4.949103652790028

Epoch: 6| Step: 13
Training loss: 4.918118476867676
Validation loss: 4.936572618381952

Epoch: 7| Step: 0
Training loss: 4.063326835632324
Validation loss: 4.926098223655455

Epoch: 6| Step: 1
Training loss: 4.509916305541992
Validation loss: 4.917381840367471

Epoch: 6| Step: 2
Training loss: 4.397738933563232
Validation loss: 4.906210366115775

Epoch: 6| Step: 3
Training loss: 5.017414093017578
Validation loss: 4.893242051524501

Epoch: 6| Step: 4
Training loss: 5.0696821212768555
Validation loss: 4.881563709628198

Epoch: 6| Step: 5
Training loss: 3.6625585556030273
Validation loss: 4.871070892580094

Epoch: 6| Step: 6
Training loss: 4.89622688293457
Validation loss: 4.8574990457104095

Epoch: 6| Step: 7
Training loss: 5.082592010498047
Validation loss: 4.8483430031807195

Epoch: 6| Step: 8
Training loss: 4.259355545043945
Validation loss: 4.834847173383159

Epoch: 6| Step: 9
Training loss: 4.385302543640137
Validation loss: 4.824551505427206

Epoch: 6| Step: 10
Training loss: 5.4504194259643555
Validation loss: 4.808995831397272

Epoch: 6| Step: 11
Training loss: 4.880472183227539
Validation loss: 4.801224800848192

Epoch: 6| Step: 12
Training loss: 4.145454406738281
Validation loss: 4.787712481714064

Epoch: 6| Step: 13
Training loss: 5.497358322143555
Validation loss: 4.77869902887652

Epoch: 8| Step: 0
Training loss: 4.889773845672607
Validation loss: 4.762657339854907

Epoch: 6| Step: 1
Training loss: 4.867107391357422
Validation loss: 4.75167453417214

Epoch: 6| Step: 2
Training loss: 3.497297763824463
Validation loss: 4.739386948206091

Epoch: 6| Step: 3
Training loss: 3.655989646911621
Validation loss: 4.726799980286629

Epoch: 6| Step: 4
Training loss: 4.690583229064941
Validation loss: 4.714584899205033

Epoch: 6| Step: 5
Training loss: 4.651021957397461
Validation loss: 4.702127000337006

Epoch: 6| Step: 6
Training loss: 4.8210835456848145
Validation loss: 4.693231290386569

Epoch: 6| Step: 7
Training loss: 4.418852806091309
Validation loss: 4.678257506380799

Epoch: 6| Step: 8
Training loss: 5.745131492614746
Validation loss: 4.663832367107433

Epoch: 6| Step: 9
Training loss: 4.314028739929199
Validation loss: 4.6532867441895185

Epoch: 6| Step: 10
Training loss: 3.7749037742614746
Validation loss: 4.637606390060917

Epoch: 6| Step: 11
Training loss: 3.7856736183166504
Validation loss: 4.6216884274636545

Epoch: 6| Step: 12
Training loss: 4.1558732986450195
Validation loss: 4.607884237843175

Epoch: 6| Step: 13
Training loss: 5.742401599884033
Validation loss: 4.596573475868471

Epoch: 9| Step: 0
Training loss: 2.847564458847046
Validation loss: 4.580166288601455

Epoch: 6| Step: 1
Training loss: 4.458787441253662
Validation loss: 4.561000508646811

Epoch: 6| Step: 2
Training loss: 5.429920196533203
Validation loss: 4.552567287157941

Epoch: 6| Step: 3
Training loss: 4.2603864669799805
Validation loss: 4.531810311860935

Epoch: 6| Step: 4
Training loss: 3.7115538120269775
Validation loss: 4.516158093688309

Epoch: 6| Step: 5
Training loss: 3.623030185699463
Validation loss: 4.5037717203940115

Epoch: 6| Step: 6
Training loss: 3.8887407779693604
Validation loss: 4.486960113689464

Epoch: 6| Step: 7
Training loss: 3.898956775665283
Validation loss: 4.478041233554963

Epoch: 6| Step: 8
Training loss: 3.8707830905914307
Validation loss: 4.459032068970383

Epoch: 6| Step: 9
Training loss: 4.83216667175293
Validation loss: 4.4424619931046685

Epoch: 6| Step: 10
Training loss: 4.124528408050537
Validation loss: 4.422972950884091

Epoch: 6| Step: 11
Training loss: 4.741778373718262
Validation loss: 4.415774037761073

Epoch: 6| Step: 12
Training loss: 4.744023323059082
Validation loss: 4.392119958836545

Epoch: 6| Step: 13
Training loss: 5.830641269683838
Validation loss: 4.373883190975394

Epoch: 10| Step: 0
Training loss: 3.467031478881836
Validation loss: 4.357244301867741

Epoch: 6| Step: 1
Training loss: 3.500875473022461
Validation loss: 4.342733301142211

Epoch: 6| Step: 2
Training loss: 4.198096752166748
Validation loss: 4.3260063048331965

Epoch: 6| Step: 3
Training loss: 4.4927473068237305
Validation loss: 4.304182498685775

Epoch: 6| Step: 4
Training loss: 3.3404643535614014
Validation loss: 4.289017364542971

Epoch: 6| Step: 5
Training loss: 4.433392524719238
Validation loss: 4.27214692741312

Epoch: 6| Step: 6
Training loss: 4.019055366516113
Validation loss: 4.255919292408933

Epoch: 6| Step: 7
Training loss: 4.253497123718262
Validation loss: 4.228457097084291

Epoch: 6| Step: 8
Training loss: 4.115259170532227
Validation loss: 4.216722101293584

Epoch: 6| Step: 9
Training loss: 4.5642499923706055
Validation loss: 4.196826496431904

Epoch: 6| Step: 10
Training loss: 4.100255966186523
Validation loss: 4.174845890332294

Epoch: 6| Step: 11
Training loss: 4.813375473022461
Validation loss: 4.162660849991665

Epoch: 6| Step: 12
Training loss: 3.399101495742798
Validation loss: 4.1414509332308205

Epoch: 6| Step: 13
Training loss: 3.823519468307495
Validation loss: 4.123039202023578

Epoch: 11| Step: 0
Training loss: 4.445383071899414
Validation loss: 4.103834777749995

Epoch: 6| Step: 1
Training loss: 3.742279052734375
Validation loss: 4.0874784197858585

Epoch: 6| Step: 2
Training loss: 3.456264019012451
Validation loss: 4.068903789725355

Epoch: 6| Step: 3
Training loss: 3.0476582050323486
Validation loss: 4.0551578972929265

Epoch: 6| Step: 4
Training loss: 3.979705333709717
Validation loss: 4.0352906257875505

Epoch: 6| Step: 5
Training loss: 3.8521246910095215
Validation loss: 4.015454912698397

Epoch: 6| Step: 6
Training loss: 4.3855414390563965
Validation loss: 4.007435039807391

Epoch: 6| Step: 7
Training loss: 4.0247578620910645
Validation loss: 3.9838923485048356

Epoch: 6| Step: 8
Training loss: 3.278494358062744
Validation loss: 3.9687683966852005

Epoch: 6| Step: 9
Training loss: 2.5833709239959717
Validation loss: 3.9459709710972284

Epoch: 6| Step: 10
Training loss: 4.129745960235596
Validation loss: 3.9227026098517963

Epoch: 6| Step: 11
Training loss: 3.933129072189331
Validation loss: 3.9101849268841486

Epoch: 6| Step: 12
Training loss: 4.765101909637451
Validation loss: 3.8938263103526127

Epoch: 6| Step: 13
Training loss: 3.8862762451171875
Validation loss: 3.8712825262418358

Epoch: 12| Step: 0
Training loss: 3.2608883380889893
Validation loss: 3.8490625914706977

Epoch: 6| Step: 1
Training loss: 3.4150516986846924
Validation loss: 3.8326503589589107

Epoch: 6| Step: 2
Training loss: 4.522273063659668
Validation loss: 3.815762637763895

Epoch: 6| Step: 3
Training loss: 2.9518728256225586
Validation loss: 3.7937420670704176

Epoch: 6| Step: 4
Training loss: 4.278704643249512
Validation loss: 3.7675728336457284

Epoch: 6| Step: 5
Training loss: 4.337170600891113
Validation loss: 3.7567053046277774

Epoch: 6| Step: 6
Training loss: 2.281932830810547
Validation loss: 3.732575111491706

Epoch: 6| Step: 7
Training loss: 3.162504196166992
Validation loss: 3.7164434463747087

Epoch: 6| Step: 8
Training loss: 4.288213729858398
Validation loss: 3.6934618514071227

Epoch: 6| Step: 9
Training loss: 4.836481094360352
Validation loss: 3.668981623905961

Epoch: 6| Step: 10
Training loss: 3.028946876525879
Validation loss: 3.650810254517422

Epoch: 6| Step: 11
Training loss: 2.9601426124572754
Validation loss: 3.6247126774121354

Epoch: 6| Step: 12
Training loss: 3.079761505126953
Validation loss: 3.6182475859119045

Epoch: 6| Step: 13
Training loss: 3.908313751220703
Validation loss: 3.58651533690832

Epoch: 13| Step: 0
Training loss: 4.157382011413574
Validation loss: 3.563856094114242

Epoch: 6| Step: 1
Training loss: 3.531710624694824
Validation loss: 3.549094764135217

Epoch: 6| Step: 2
Training loss: 4.1809892654418945
Validation loss: 3.534257863157539

Epoch: 6| Step: 3
Training loss: 2.712538242340088
Validation loss: 3.502617302761283

Epoch: 6| Step: 4
Training loss: 2.8236501216888428
Validation loss: 3.4938699737671883

Epoch: 6| Step: 5
Training loss: 2.4426279067993164
Validation loss: 3.4674436969141804

Epoch: 6| Step: 6
Training loss: 3.8650293350219727
Validation loss: 3.43925226888349

Epoch: 6| Step: 7
Training loss: 3.627959966659546
Validation loss: 3.434450049554148

Epoch: 6| Step: 8
Training loss: 3.762875556945801
Validation loss: 3.406875861588345

Epoch: 6| Step: 9
Training loss: 3.467679977416992
Validation loss: 3.3837327572607223

Epoch: 6| Step: 10
Training loss: 2.918956995010376
Validation loss: 3.3674883252830914

Epoch: 6| Step: 11
Training loss: 3.0119199752807617
Validation loss: 3.3503003402422835

Epoch: 6| Step: 12
Training loss: 3.337385654449463
Validation loss: 3.3248577374283985

Epoch: 6| Step: 13
Training loss: 2.527066946029663
Validation loss: 3.299549005364859

Epoch: 14| Step: 0
Training loss: 3.389373779296875
Validation loss: 3.28422410257401

Epoch: 6| Step: 1
Training loss: 2.5281412601470947
Validation loss: 3.2654400974191646

Epoch: 6| Step: 2
Training loss: 3.9047205448150635
Validation loss: 3.2406930410733787

Epoch: 6| Step: 3
Training loss: 3.019951820373535
Validation loss: 3.2242612069652927

Epoch: 6| Step: 4
Training loss: 2.712698221206665
Validation loss: 3.1998629006006385

Epoch: 6| Step: 5
Training loss: 3.8162007331848145
Validation loss: 3.1764325890489804

Epoch: 6| Step: 6
Training loss: 2.5725650787353516
Validation loss: 3.161129133675688

Epoch: 6| Step: 7
Training loss: 2.664951801300049
Validation loss: 3.1313969191684516

Epoch: 6| Step: 8
Training loss: 2.7229228019714355
Validation loss: 3.1225274967890915

Epoch: 6| Step: 9
Training loss: 3.206747531890869
Validation loss: 3.103292226791382

Epoch: 6| Step: 10
Training loss: 3.1600193977355957
Validation loss: 3.0862771003477034

Epoch: 6| Step: 11
Training loss: 3.5649619102478027
Validation loss: 3.0654166718964935

Epoch: 6| Step: 12
Training loss: 3.6661744117736816
Validation loss: 3.0451532230582288

Epoch: 6| Step: 13
Training loss: 2.0605876445770264
Validation loss: 3.0346367230979343

Epoch: 15| Step: 0
Training loss: 3.1318373680114746
Validation loss: 2.998328962633687

Epoch: 6| Step: 1
Training loss: 2.724308729171753
Validation loss: 2.9808440567344747

Epoch: 6| Step: 2
Training loss: 2.4661965370178223
Validation loss: 2.963256137345427

Epoch: 6| Step: 3
Training loss: 3.10919189453125
Validation loss: 2.9448841746135423

Epoch: 6| Step: 4
Training loss: 3.003051280975342
Validation loss: 2.928210900675866

Epoch: 6| Step: 5
Training loss: 3.0492851734161377
Validation loss: 2.906695386414887

Epoch: 6| Step: 6
Training loss: 2.8314907550811768
Validation loss: 2.88641038761344

Epoch: 6| Step: 7
Training loss: 4.1091108322143555
Validation loss: 2.863430448757705

Epoch: 6| Step: 8
Training loss: 2.930621862411499
Validation loss: 2.8487081937892462

Epoch: 6| Step: 9
Training loss: 2.6758992671966553
Validation loss: 2.8221056845880326

Epoch: 6| Step: 10
Training loss: 2.974268913269043
Validation loss: 2.8082359221673783

Epoch: 6| Step: 11
Training loss: 2.6831140518188477
Validation loss: 2.7917832661700506

Epoch: 6| Step: 12
Training loss: 2.2772769927978516
Validation loss: 2.758506226283248

Epoch: 6| Step: 13
Training loss: 2.785118818283081
Validation loss: 2.7323276586430048

Epoch: 16| Step: 0
Training loss: 3.1068804264068604
Validation loss: 2.7217209569869505

Epoch: 6| Step: 1
Training loss: 2.9618704319000244
Validation loss: 2.707359529310657

Epoch: 6| Step: 2
Training loss: 2.2986655235290527
Validation loss: 2.684389057979789

Epoch: 6| Step: 3
Training loss: 2.92411470413208
Validation loss: 2.6639367047176568

Epoch: 6| Step: 4
Training loss: 3.4083473682403564
Validation loss: 2.6399709742556334

Epoch: 6| Step: 5
Training loss: 2.9188778400421143
Validation loss: 2.62296905825215

Epoch: 6| Step: 6
Training loss: 2.4804840087890625
Validation loss: 2.6016110117717455

Epoch: 6| Step: 7
Training loss: 2.3930511474609375
Validation loss: 2.595787568758893

Epoch: 6| Step: 8
Training loss: 3.282107353210449
Validation loss: 2.5772495372321016

Epoch: 6| Step: 9
Training loss: 2.096944570541382
Validation loss: 2.558480865211897

Epoch: 6| Step: 10
Training loss: 2.508039712905884
Validation loss: 2.5379074773480816

Epoch: 6| Step: 11
Training loss: 2.5977845191955566
Validation loss: 2.523806156650666

Epoch: 6| Step: 12
Training loss: 3.253852605819702
Validation loss: 2.5076202807887906

Epoch: 6| Step: 13
Training loss: 1.942851185798645
Validation loss: 2.494517141772855

Epoch: 17| Step: 0
Training loss: 2.260559558868408
Validation loss: 2.4831622057063605

Epoch: 6| Step: 1
Training loss: 3.290508508682251
Validation loss: 2.48383233880484

Epoch: 6| Step: 2
Training loss: 2.7562177181243896
Validation loss: 2.4600015301858225

Epoch: 6| Step: 3
Training loss: 2.7687320709228516
Validation loss: 2.4306501598768335

Epoch: 6| Step: 4
Training loss: 2.5991013050079346
Validation loss: 2.413991102608301

Epoch: 6| Step: 5
Training loss: 3.188143491744995
Validation loss: 2.4114417235056558

Epoch: 6| Step: 6
Training loss: 2.757953643798828
Validation loss: 2.4185391754232426

Epoch: 6| Step: 7
Training loss: 2.3998324871063232
Validation loss: 2.3845612284957722

Epoch: 6| Step: 8
Training loss: 2.280630588531494
Validation loss: 2.383564874690066

Epoch: 6| Step: 9
Training loss: 2.3710923194885254
Validation loss: 2.3764949434547016

Epoch: 6| Step: 10
Training loss: 2.684811592102051
Validation loss: 2.358744687931512

Epoch: 6| Step: 11
Training loss: 2.33988618850708
Validation loss: 2.36201205817602

Epoch: 6| Step: 12
Training loss: 2.4990787506103516
Validation loss: 2.3476476874402774

Epoch: 6| Step: 13
Training loss: 2.0496480464935303
Validation loss: 2.3583436089177288

Epoch: 18| Step: 0
Training loss: 2.2578039169311523
Validation loss: 2.3363751314019643

Epoch: 6| Step: 1
Training loss: 2.4844717979431152
Validation loss: 2.343848446364044

Epoch: 6| Step: 2
Training loss: 3.0042667388916016
Validation loss: 2.32183434373589

Epoch: 6| Step: 3
Training loss: 2.5601613521575928
Validation loss: 2.3170028040485997

Epoch: 6| Step: 4
Training loss: 2.7281877994537354
Validation loss: 2.309168872012887

Epoch: 6| Step: 5
Training loss: 3.23824405670166
Validation loss: 2.2955234153296358

Epoch: 6| Step: 6
Training loss: 2.627235174179077
Validation loss: 2.30169447903992

Epoch: 6| Step: 7
Training loss: 3.059126377105713
Validation loss: 2.299198132689281

Epoch: 6| Step: 8
Training loss: 2.5758566856384277
Validation loss: 2.2931614614302114

Epoch: 6| Step: 9
Training loss: 1.863006591796875
Validation loss: 2.2853730622158257

Epoch: 6| Step: 10
Training loss: 2.1961982250213623
Validation loss: 2.254637416972909

Epoch: 6| Step: 11
Training loss: 2.2229228019714355
Validation loss: 2.2535264594580537

Epoch: 6| Step: 12
Training loss: 2.261596918106079
Validation loss: 2.266744062464724

Epoch: 6| Step: 13
Training loss: 2.332326650619507
Validation loss: 2.2726349240990094

Epoch: 19| Step: 0
Training loss: 1.4288175106048584
Validation loss: 2.2569593357783493

Epoch: 6| Step: 1
Training loss: 2.395580530166626
Validation loss: 2.2509117818647817

Epoch: 6| Step: 2
Training loss: 2.3160343170166016
Validation loss: 2.250871260960897

Epoch: 6| Step: 3
Training loss: 2.8921704292297363
Validation loss: 2.2351186147300144

Epoch: 6| Step: 4
Training loss: 3.144430160522461
Validation loss: 2.2406198581059775

Epoch: 6| Step: 5
Training loss: 3.202664852142334
Validation loss: 2.218130394976626

Epoch: 6| Step: 6
Training loss: 3.141252279281616
Validation loss: 2.2312148899160404

Epoch: 6| Step: 7
Training loss: 2.5887627601623535
Validation loss: 2.222532974776401

Epoch: 6| Step: 8
Training loss: 3.0838756561279297
Validation loss: 2.2133744634607786

Epoch: 6| Step: 9
Training loss: 2.576234817504883
Validation loss: 2.2186927180136404

Epoch: 6| Step: 10
Training loss: 1.9907312393188477
Validation loss: 2.2224612133477324

Epoch: 6| Step: 11
Training loss: 2.108614444732666
Validation loss: 2.218227788966189

Epoch: 6| Step: 12
Training loss: 1.8432459831237793
Validation loss: 2.2153299790556713

Epoch: 6| Step: 13
Training loss: 2.0485424995422363
Validation loss: 2.215937242713026

Epoch: 20| Step: 0
Training loss: 2.284304141998291
Validation loss: 2.2010107360860354

Epoch: 6| Step: 1
Training loss: 2.445878744125366
Validation loss: 2.2009798352436354

Epoch: 6| Step: 2
Training loss: 2.7998452186584473
Validation loss: 2.22198454026253

Epoch: 6| Step: 3
Training loss: 2.378047227859497
Validation loss: 2.196956067956904

Epoch: 6| Step: 4
Training loss: 2.680687665939331
Validation loss: 2.2127215734092136

Epoch: 6| Step: 5
Training loss: 2.27908992767334
Validation loss: 2.188394610599805

Epoch: 6| Step: 6
Training loss: 2.412877082824707
Validation loss: 2.1924113509475545

Epoch: 6| Step: 7
Training loss: 2.3034234046936035
Validation loss: 2.2142013708750405

Epoch: 6| Step: 8
Training loss: 2.6598458290100098
Validation loss: 2.2018222091018513

Epoch: 6| Step: 9
Training loss: 2.5865609645843506
Validation loss: 2.2119720699966594

Epoch: 6| Step: 10
Training loss: 2.1500279903411865
Validation loss: 2.1945644155625375

Epoch: 6| Step: 11
Training loss: 2.305117130279541
Validation loss: 2.2010551037326938

Epoch: 6| Step: 12
Training loss: 2.775599956512451
Validation loss: 2.193737914485316

Epoch: 6| Step: 13
Training loss: 2.7613306045532227
Validation loss: 2.203252806458422

Epoch: 21| Step: 0
Training loss: 2.593017101287842
Validation loss: 2.178091238903743

Epoch: 6| Step: 1
Training loss: 2.900719165802002
Validation loss: 2.1937735772901967

Epoch: 6| Step: 2
Training loss: 2.66945481300354
Validation loss: 2.1831383371865876

Epoch: 6| Step: 3
Training loss: 2.309799909591675
Validation loss: 2.1808407947581303

Epoch: 6| Step: 4
Training loss: 2.8519697189331055
Validation loss: 2.164234253668016

Epoch: 6| Step: 5
Training loss: 2.2483654022216797
Validation loss: 2.1726295973664973

Epoch: 6| Step: 6
Training loss: 3.080684185028076
Validation loss: 2.1851181573765253

Epoch: 6| Step: 7
Training loss: 3.0241031646728516
Validation loss: 2.172853949249432

Epoch: 6| Step: 8
Training loss: 1.5589067935943604
Validation loss: 2.167585893343854

Epoch: 6| Step: 9
Training loss: 2.5391218662261963
Validation loss: 2.172713828343217

Epoch: 6| Step: 10
Training loss: 2.179253101348877
Validation loss: 2.181952896938529

Epoch: 6| Step: 11
Training loss: 1.7885150909423828
Validation loss: 2.1803189605794926

Epoch: 6| Step: 12
Training loss: 2.2364883422851562
Validation loss: 2.159733105731267

Epoch: 6| Step: 13
Training loss: 2.77569580078125
Validation loss: 2.166893779590566

Epoch: 22| Step: 0
Training loss: 3.190798282623291
Validation loss: 2.182975817752141

Epoch: 6| Step: 1
Training loss: 2.7233262062072754
Validation loss: 2.1692887929178055

Epoch: 6| Step: 2
Training loss: 3.1040823459625244
Validation loss: 2.1749163058496292

Epoch: 6| Step: 3
Training loss: 2.3296120166778564
Validation loss: 2.176155815842331

Epoch: 6| Step: 4
Training loss: 2.0885889530181885
Validation loss: 2.1824217022106214

Epoch: 6| Step: 5
Training loss: 2.054415225982666
Validation loss: 2.18060903908104

Epoch: 6| Step: 6
Training loss: 2.0057268142700195
Validation loss: 2.177328609651135

Epoch: 6| Step: 7
Training loss: 1.9678305387496948
Validation loss: 2.1843121590152865

Epoch: 6| Step: 8
Training loss: 2.4499902725219727
Validation loss: 2.1788379787116923

Epoch: 6| Step: 9
Training loss: 2.239264488220215
Validation loss: 2.1862342190998856

Epoch: 6| Step: 10
Training loss: 2.5604143142700195
Validation loss: 2.1811049292164464

Epoch: 6| Step: 11
Training loss: 2.2708935737609863
Validation loss: 2.1851345544220298

Epoch: 6| Step: 12
Training loss: 2.932833194732666
Validation loss: 2.176757784299953

Epoch: 6| Step: 13
Training loss: 2.5742433071136475
Validation loss: 2.16795055712423

Epoch: 23| Step: 0
Training loss: 2.1189756393432617
Validation loss: 2.1859913974679928

Epoch: 6| Step: 1
Training loss: 2.6006460189819336
Validation loss: 2.171494799275552

Epoch: 6| Step: 2
Training loss: 2.0092759132385254
Validation loss: 2.169725937228049

Epoch: 6| Step: 3
Training loss: 2.7938127517700195
Validation loss: 2.174020169883646

Epoch: 6| Step: 4
Training loss: 2.780069351196289
Validation loss: 2.185417115047414

Epoch: 6| Step: 5
Training loss: 2.7723608016967773
Validation loss: 2.1878980180268646

Epoch: 6| Step: 6
Training loss: 2.9285764694213867
Validation loss: 2.1735671899651967

Epoch: 6| Step: 7
Training loss: 1.8495144844055176
Validation loss: 2.177895507504863

Epoch: 6| Step: 8
Training loss: 2.04067325592041
Validation loss: 2.190095886107414

Epoch: 6| Step: 9
Training loss: 2.8305466175079346
Validation loss: 2.1688613173782185

Epoch: 6| Step: 10
Training loss: 1.978369951248169
Validation loss: 2.1704548276880735

Epoch: 6| Step: 11
Training loss: 2.2116403579711914
Validation loss: 2.188845762642481

Epoch: 6| Step: 12
Training loss: 2.894043445587158
Validation loss: 2.1810698342579666

Epoch: 6| Step: 13
Training loss: 2.544727087020874
Validation loss: 2.1671318597691034

Epoch: 24| Step: 0
Training loss: 2.8840441703796387
Validation loss: 2.1734322732494724

Epoch: 6| Step: 1
Training loss: 2.644069194793701
Validation loss: 2.162506854662331

Epoch: 6| Step: 2
Training loss: 1.9928659200668335
Validation loss: 2.170982560803813

Epoch: 6| Step: 3
Training loss: 1.8059468269348145
Validation loss: 2.1845230953667754

Epoch: 6| Step: 4
Training loss: 2.199667453765869
Validation loss: 2.169655058973579

Epoch: 6| Step: 5
Training loss: 2.0962820053100586
Validation loss: 2.175641240612153

Epoch: 6| Step: 6
Training loss: 2.103762149810791
Validation loss: 2.1826473923139673

Epoch: 6| Step: 7
Training loss: 2.7729005813598633
Validation loss: 2.189729085532568

Epoch: 6| Step: 8
Training loss: 2.4799559116363525
Validation loss: 2.178288552068895

Epoch: 6| Step: 9
Training loss: 2.4632155895233154
Validation loss: 2.1758458460530927

Epoch: 6| Step: 10
Training loss: 3.5808587074279785
Validation loss: 2.179794357668969

Epoch: 6| Step: 11
Training loss: 2.032829761505127
Validation loss: 2.162114385635622

Epoch: 6| Step: 12
Training loss: 2.1500344276428223
Validation loss: 2.1867906021815475

Epoch: 6| Step: 13
Training loss: 3.769286632537842
Validation loss: 2.1776652566848265

Epoch: 25| Step: 0
Training loss: 2.2590270042419434
Validation loss: 2.1621945545237553

Epoch: 6| Step: 1
Training loss: 2.883608818054199
Validation loss: 2.166219175502818

Epoch: 6| Step: 2
Training loss: 1.8847129344940186
Validation loss: 2.1883597899508733

Epoch: 6| Step: 3
Training loss: 2.8208134174346924
Validation loss: 2.1609857607913274

Epoch: 6| Step: 4
Training loss: 2.497457265853882
Validation loss: 2.1558797256920927

Epoch: 6| Step: 5
Training loss: 2.9955363273620605
Validation loss: 2.17083070611441

Epoch: 6| Step: 6
Training loss: 1.554121732711792
Validation loss: 2.1635170393092658

Epoch: 6| Step: 7
Training loss: 2.526613235473633
Validation loss: 2.181689618736185

Epoch: 6| Step: 8
Training loss: 2.2833571434020996
Validation loss: 2.1653250263583277

Epoch: 6| Step: 9
Training loss: 2.281464099884033
Validation loss: 2.161870305256177

Epoch: 6| Step: 10
Training loss: 2.604468584060669
Validation loss: 2.1642395565586705

Epoch: 6| Step: 11
Training loss: 2.556954860687256
Validation loss: 2.1766046631720757

Epoch: 6| Step: 12
Training loss: 2.1763925552368164
Validation loss: 2.17457756944882

Epoch: 6| Step: 13
Training loss: 3.387367010116577
Validation loss: 2.159004565208189

Epoch: 26| Step: 0
Training loss: 2.9985923767089844
Validation loss: 2.1587572969416136

Epoch: 6| Step: 1
Training loss: 2.1041088104248047
Validation loss: 2.143980049317883

Epoch: 6| Step: 2
Training loss: 2.04453706741333
Validation loss: 2.1615156819743495

Epoch: 6| Step: 3
Training loss: 3.185363292694092
Validation loss: 2.1503315484651955

Epoch: 6| Step: 4
Training loss: 2.123699188232422
Validation loss: 2.155638494799214

Epoch: 6| Step: 5
Training loss: 2.869826316833496
Validation loss: 2.1576763942677486

Epoch: 6| Step: 6
Training loss: 2.2835304737091064
Validation loss: 2.1406160631487445

Epoch: 6| Step: 7
Training loss: 2.37388014793396
Validation loss: 2.151627309860722

Epoch: 6| Step: 8
Training loss: 2.354971408843994
Validation loss: 2.1502452358122794

Epoch: 6| Step: 9
Training loss: 2.2186412811279297
Validation loss: 2.168142677635275

Epoch: 6| Step: 10
Training loss: 1.904056429862976
Validation loss: 2.15414507414705

Epoch: 6| Step: 11
Training loss: 2.821953535079956
Validation loss: 2.1385699728483796

Epoch: 6| Step: 12
Training loss: 2.298509359359741
Validation loss: 2.156057434697305

Epoch: 6| Step: 13
Training loss: 2.5282230377197266
Validation loss: 2.1557548328112532

Epoch: 27| Step: 0
Training loss: 2.5218167304992676
Validation loss: 2.1743310395107476

Epoch: 6| Step: 1
Training loss: 2.093984603881836
Validation loss: 2.1723922503891813

Epoch: 6| Step: 2
Training loss: 2.7680952548980713
Validation loss: 2.1705133017673286

Epoch: 6| Step: 3
Training loss: 2.447709560394287
Validation loss: 2.1606721339687223

Epoch: 6| Step: 4
Training loss: 2.5472934246063232
Validation loss: 2.170124807665425

Epoch: 6| Step: 5
Training loss: 2.7847678661346436
Validation loss: 2.1509267924934306

Epoch: 6| Step: 6
Training loss: 2.1941978931427
Validation loss: 2.1550250873770764

Epoch: 6| Step: 7
Training loss: 1.7631807327270508
Validation loss: 2.1490051490004345

Epoch: 6| Step: 8
Training loss: 1.9614973068237305
Validation loss: 2.1529907821327128

Epoch: 6| Step: 9
Training loss: 3.1299686431884766
Validation loss: 2.146451473236084

Epoch: 6| Step: 10
Training loss: 2.1512670516967773
Validation loss: 2.1571175436819754

Epoch: 6| Step: 11
Training loss: 2.58833646774292
Validation loss: 2.1466284080218245

Epoch: 6| Step: 12
Training loss: 2.1642861366271973
Validation loss: 2.146547696923697

Epoch: 6| Step: 13
Training loss: 3.0854947566986084
Validation loss: 2.171917651289253

Epoch: 28| Step: 0
Training loss: 1.9686846733093262
Validation loss: 2.1742956369153914

Epoch: 6| Step: 1
Training loss: 2.1386024951934814
Validation loss: 2.180733614070441

Epoch: 6| Step: 2
Training loss: 2.656367301940918
Validation loss: 2.1572867644730436

Epoch: 6| Step: 3
Training loss: 2.747528076171875
Validation loss: 2.171371313833421

Epoch: 6| Step: 4
Training loss: 2.136394500732422
Validation loss: 2.1671843272383495

Epoch: 6| Step: 5
Training loss: 2.3414244651794434
Validation loss: 2.168194724667457

Epoch: 6| Step: 6
Training loss: 2.2711751461029053
Validation loss: 2.17691405101489

Epoch: 6| Step: 7
Training loss: 2.537618637084961
Validation loss: 2.1828075326899046

Epoch: 6| Step: 8
Training loss: 2.5313820838928223
Validation loss: 2.161349486279231

Epoch: 6| Step: 9
Training loss: 2.351120948791504
Validation loss: 2.1764310944464897

Epoch: 6| Step: 10
Training loss: 2.6401102542877197
Validation loss: 2.1696528593699136

Epoch: 6| Step: 11
Training loss: 2.265347480773926
Validation loss: 2.1855800856826124

Epoch: 6| Step: 12
Training loss: 3.1778993606567383
Validation loss: 2.1778253329697477

Epoch: 6| Step: 13
Training loss: 1.981688380241394
Validation loss: 2.1665389717266126

Epoch: 29| Step: 0
Training loss: 2.035050392150879
Validation loss: 2.170393410549369

Epoch: 6| Step: 1
Training loss: 2.3674991130828857
Validation loss: 2.16961310884004

Epoch: 6| Step: 2
Training loss: 1.9699745178222656
Validation loss: 2.166374651334619

Epoch: 6| Step: 3
Training loss: 2.150151014328003
Validation loss: 2.161616543287872

Epoch: 6| Step: 4
Training loss: 1.7691923379898071
Validation loss: 2.1701746192029727

Epoch: 6| Step: 5
Training loss: 2.3056116104125977
Validation loss: 2.1730521340523996

Epoch: 6| Step: 6
Training loss: 2.2012929916381836
Validation loss: 2.154949798378893

Epoch: 6| Step: 7
Training loss: 2.804764747619629
Validation loss: 2.1798611199983986

Epoch: 6| Step: 8
Training loss: 2.816411018371582
Validation loss: 2.168074869340466

Epoch: 6| Step: 9
Training loss: 2.9171395301818848
Validation loss: 2.1464713388873684

Epoch: 6| Step: 10
Training loss: 3.1813783645629883
Validation loss: 2.1665162655615036

Epoch: 6| Step: 11
Training loss: 2.322195529937744
Validation loss: 2.16395830082637

Epoch: 6| Step: 12
Training loss: 2.7062125205993652
Validation loss: 2.167672793070475

Epoch: 6| Step: 13
Training loss: 2.170116901397705
Validation loss: 2.146527841526975

Epoch: 30| Step: 0
Training loss: 2.5476479530334473
Validation loss: 2.1723546417810584

Epoch: 6| Step: 1
Training loss: 2.442718029022217
Validation loss: 2.177847466161174

Epoch: 6| Step: 2
Training loss: 2.284351110458374
Validation loss: 2.170865943354945

Epoch: 6| Step: 3
Training loss: 2.9464941024780273
Validation loss: 2.1708272900632632

Epoch: 6| Step: 4
Training loss: 2.638162136077881
Validation loss: 2.1598530482220393

Epoch: 6| Step: 5
Training loss: 2.3904025554656982
Validation loss: 2.161111654773835

Epoch: 6| Step: 6
Training loss: 1.7917654514312744
Validation loss: 2.161827302748157

Epoch: 6| Step: 7
Training loss: 2.653188705444336
Validation loss: 2.1425724875542427

Epoch: 6| Step: 8
Training loss: 2.018756866455078
Validation loss: 2.1414854757247435

Epoch: 6| Step: 9
Training loss: 2.3823750019073486
Validation loss: 2.1536677575880483

Epoch: 6| Step: 10
Training loss: 2.572788715362549
Validation loss: 2.1736397538133847

Epoch: 6| Step: 11
Training loss: 2.469777822494507
Validation loss: 2.1581568615410918

Epoch: 6| Step: 12
Training loss: 2.2559287548065186
Validation loss: 2.1637098250850553

Epoch: 6| Step: 13
Training loss: 2.0589969158172607
Validation loss: 2.159001883640084

Epoch: 31| Step: 0
Training loss: 2.7958216667175293
Validation loss: 2.1309190296357676

Epoch: 6| Step: 1
Training loss: 1.754164695739746
Validation loss: 2.167710442696848

Epoch: 6| Step: 2
Training loss: 2.058274984359741
Validation loss: 2.1619334900251

Epoch: 6| Step: 3
Training loss: 2.110179901123047
Validation loss: 2.150851793186639

Epoch: 6| Step: 4
Training loss: 2.04353928565979
Validation loss: 2.164980150038196

Epoch: 6| Step: 5
Training loss: 3.2460696697235107
Validation loss: 2.1569424188265236

Epoch: 6| Step: 6
Training loss: 3.0338029861450195
Validation loss: 2.1564314096204695

Epoch: 6| Step: 7
Training loss: 2.2240138053894043
Validation loss: 2.1459951964757775

Epoch: 6| Step: 8
Training loss: 2.8546321392059326
Validation loss: 2.1543200605659076

Epoch: 6| Step: 9
Training loss: 2.5635671615600586
Validation loss: 2.1369425930002683

Epoch: 6| Step: 10
Training loss: 2.444549322128296
Validation loss: 2.128057628549555

Epoch: 6| Step: 11
Training loss: 2.264822006225586
Validation loss: 2.147615937776463

Epoch: 6| Step: 12
Training loss: 1.761844515800476
Validation loss: 2.1374050596708893

Epoch: 6| Step: 13
Training loss: 2.4210715293884277
Validation loss: 2.144044389006912

Epoch: 32| Step: 0
Training loss: 3.1622228622436523
Validation loss: 2.1337877640160183

Epoch: 6| Step: 1
Training loss: 2.3297119140625
Validation loss: 2.1452621618906655

Epoch: 6| Step: 2
Training loss: 1.768860101699829
Validation loss: 2.1439266243288593

Epoch: 6| Step: 3
Training loss: 2.0784034729003906
Validation loss: 2.1384941121583343

Epoch: 6| Step: 4
Training loss: 2.1366536617279053
Validation loss: 2.138901577200941

Epoch: 6| Step: 5
Training loss: 2.388434886932373
Validation loss: 2.145696627196445

Epoch: 6| Step: 6
Training loss: 2.7967238426208496
Validation loss: 2.1489293857287337

Epoch: 6| Step: 7
Training loss: 1.8169790506362915
Validation loss: 2.1488863986025573

Epoch: 6| Step: 8
Training loss: 1.7282004356384277
Validation loss: 2.1349987214611423

Epoch: 6| Step: 9
Training loss: 3.231417179107666
Validation loss: 2.153011691185736

Epoch: 6| Step: 10
Training loss: 2.407205581665039
Validation loss: 2.1523355796772945

Epoch: 6| Step: 11
Training loss: 2.251361131668091
Validation loss: 2.1307254055494904

Epoch: 6| Step: 12
Training loss: 2.954735040664673
Validation loss: 2.1588979600578226

Epoch: 6| Step: 13
Training loss: 2.655304431915283
Validation loss: 2.150359513939068

Epoch: 33| Step: 0
Training loss: 2.7269320487976074
Validation loss: 2.149928205756731

Epoch: 6| Step: 1
Training loss: 1.6102306842803955
Validation loss: 2.1403891194251274

Epoch: 6| Step: 2
Training loss: 2.1685521602630615
Validation loss: 2.1379221254779446

Epoch: 6| Step: 3
Training loss: 2.830561637878418
Validation loss: 2.1462245846307404

Epoch: 6| Step: 4
Training loss: 2.230689525604248
Validation loss: 2.15002025840103

Epoch: 6| Step: 5
Training loss: 3.3430428504943848
Validation loss: 2.14885889586582

Epoch: 6| Step: 6
Training loss: 2.2738966941833496
Validation loss: 2.1577482864420903

Epoch: 6| Step: 7
Training loss: 2.2016725540161133
Validation loss: 2.144411228036368

Epoch: 6| Step: 8
Training loss: 2.6812562942504883
Validation loss: 2.141671819071616

Epoch: 6| Step: 9
Training loss: 2.2346272468566895
Validation loss: 2.155696597150577

Epoch: 6| Step: 10
Training loss: 2.40177583694458
Validation loss: 2.109491514903243

Epoch: 6| Step: 11
Training loss: 2.2661962509155273
Validation loss: 2.142244133898007

Epoch: 6| Step: 12
Training loss: 2.2646865844726562
Validation loss: 2.1381100839184177

Epoch: 6| Step: 13
Training loss: 1.7437463998794556
Validation loss: 2.1490266194907566

Epoch: 34| Step: 0
Training loss: 2.080810546875
Validation loss: 2.1424878694677867

Epoch: 6| Step: 1
Training loss: 2.7842304706573486
Validation loss: 2.1329731095221733

Epoch: 6| Step: 2
Training loss: 2.1209187507629395
Validation loss: 2.1545400004233084

Epoch: 6| Step: 3
Training loss: 2.3557815551757812
Validation loss: 2.1462399498108895

Epoch: 6| Step: 4
Training loss: 2.6569857597351074
Validation loss: 2.1301504694005495

Epoch: 6| Step: 5
Training loss: 2.631779193878174
Validation loss: 2.1555783876808743

Epoch: 6| Step: 6
Training loss: 2.2247190475463867
Validation loss: 2.1391890894982124

Epoch: 6| Step: 7
Training loss: 2.231607675552368
Validation loss: 2.1283768761542534

Epoch: 6| Step: 8
Training loss: 1.9536638259887695
Validation loss: 2.1545046144916165

Epoch: 6| Step: 9
Training loss: 2.2382304668426514
Validation loss: 2.118934049401232

Epoch: 6| Step: 10
Training loss: 2.8241264820098877
Validation loss: 2.1446977200046664

Epoch: 6| Step: 11
Training loss: 2.6651387214660645
Validation loss: 2.1327744530093287

Epoch: 6| Step: 12
Training loss: 2.185718059539795
Validation loss: 2.139306422202818

Epoch: 6| Step: 13
Training loss: 2.4237146377563477
Validation loss: 2.127968498455581

Epoch: 35| Step: 0
Training loss: 1.7764865159988403
Validation loss: 2.1444456526028213

Epoch: 6| Step: 1
Training loss: 2.5626959800720215
Validation loss: 2.135731436872995

Epoch: 6| Step: 2
Training loss: 2.170578956604004
Validation loss: 2.1365434559442664

Epoch: 6| Step: 3
Training loss: 3.2836174964904785
Validation loss: 2.1127100990664576

Epoch: 6| Step: 4
Training loss: 2.1532037258148193
Validation loss: 2.128916553271714

Epoch: 6| Step: 5
Training loss: 2.383009910583496
Validation loss: 2.1452801330115205

Epoch: 6| Step: 6
Training loss: 1.9160311222076416
Validation loss: 2.1268777667835193

Epoch: 6| Step: 7
Training loss: 2.5397093296051025
Validation loss: 2.1350058906821796

Epoch: 6| Step: 8
Training loss: 2.956667423248291
Validation loss: 2.1323647909266974

Epoch: 6| Step: 9
Training loss: 2.112851142883301
Validation loss: 2.118498108720267

Epoch: 6| Step: 10
Training loss: 1.9096335172653198
Validation loss: 2.138966196326799

Epoch: 6| Step: 11
Training loss: 2.6461219787597656
Validation loss: 2.133560865156112

Epoch: 6| Step: 12
Training loss: 1.889306902885437
Validation loss: 2.1261370323037587

Epoch: 6| Step: 13
Training loss: 3.2251362800598145
Validation loss: 2.0981700292197605

Epoch: 36| Step: 0
Training loss: 2.871842384338379
Validation loss: 2.1133678318351827

Epoch: 6| Step: 1
Training loss: 2.274970531463623
Validation loss: 2.131810152402488

Epoch: 6| Step: 2
Training loss: 2.747117519378662
Validation loss: 2.1298920210971626

Epoch: 6| Step: 3
Training loss: 2.020491600036621
Validation loss: 2.1204554201454244

Epoch: 6| Step: 4
Training loss: 2.5523314476013184
Validation loss: 2.1138641629167783

Epoch: 6| Step: 5
Training loss: 2.006225109100342
Validation loss: 2.131439039784093

Epoch: 6| Step: 6
Training loss: 2.4206361770629883
Validation loss: 2.128990706577096

Epoch: 6| Step: 7
Training loss: 2.023556709289551
Validation loss: 2.125876793297388

Epoch: 6| Step: 8
Training loss: 1.8913559913635254
Validation loss: 2.1250495039006716

Epoch: 6| Step: 9
Training loss: 2.339465618133545
Validation loss: 2.1313005403805803

Epoch: 6| Step: 10
Training loss: 2.511422634124756
Validation loss: 2.1073519017106745

Epoch: 6| Step: 11
Training loss: 2.616694927215576
Validation loss: 2.138597055148053

Epoch: 6| Step: 12
Training loss: 2.3064613342285156
Validation loss: 2.1055239708192888

Epoch: 6| Step: 13
Training loss: 2.4017231464385986
Validation loss: 2.1352284210984425

Epoch: 37| Step: 0
Training loss: 2.672741413116455
Validation loss: 2.125934916157876

Epoch: 6| Step: 1
Training loss: 2.2791554927825928
Validation loss: 2.1305254274798977

Epoch: 6| Step: 2
Training loss: 1.6152424812316895
Validation loss: 2.1545605915848927

Epoch: 6| Step: 3
Training loss: 2.16764235496521
Validation loss: 2.1126000778649443

Epoch: 6| Step: 4
Training loss: 2.0519299507141113
Validation loss: 2.1244625865772204

Epoch: 6| Step: 5
Training loss: 2.878981113433838
Validation loss: 2.134923650372413

Epoch: 6| Step: 6
Training loss: 3.024224281311035
Validation loss: 2.114906995527206

Epoch: 6| Step: 7
Training loss: 1.984701156616211
Validation loss: 2.100499012136972

Epoch: 6| Step: 8
Training loss: 2.0459885597229004
Validation loss: 2.1211459393142373

Epoch: 6| Step: 9
Training loss: 2.579782485961914
Validation loss: 2.145989392393379

Epoch: 6| Step: 10
Training loss: 2.500016212463379
Validation loss: 2.147037452267062

Epoch: 6| Step: 11
Training loss: 2.7199668884277344
Validation loss: 2.1442731657335834

Epoch: 6| Step: 12
Training loss: 2.2734017372131348
Validation loss: 2.1486024215657222

Epoch: 6| Step: 13
Training loss: 2.4395785331726074
Validation loss: 2.1478538359365156

Epoch: 38| Step: 0
Training loss: 2.071739912033081
Validation loss: 2.1385410472910893

Epoch: 6| Step: 1
Training loss: 2.404423713684082
Validation loss: 2.124679055265201

Epoch: 6| Step: 2
Training loss: 2.3055295944213867
Validation loss: 2.1191902442645003

Epoch: 6| Step: 3
Training loss: 2.3848347663879395
Validation loss: 2.1382484589853594

Epoch: 6| Step: 4
Training loss: 1.9919607639312744
Validation loss: 2.135537383376911

Epoch: 6| Step: 5
Training loss: 2.092935085296631
Validation loss: 2.11334059058979

Epoch: 6| Step: 6
Training loss: 2.7834725379943848
Validation loss: 2.132228776972781

Epoch: 6| Step: 7
Training loss: 3.3676681518554688
Validation loss: 2.133781757406009

Epoch: 6| Step: 8
Training loss: 1.8053160905838013
Validation loss: 2.1474137203667754

Epoch: 6| Step: 9
Training loss: 1.9446806907653809
Validation loss: 2.140124677329935

Epoch: 6| Step: 10
Training loss: 2.330190896987915
Validation loss: 2.1188234026714037

Epoch: 6| Step: 11
Training loss: 2.381042718887329
Validation loss: 2.115988205837947

Epoch: 6| Step: 12
Training loss: 2.175473690032959
Validation loss: 2.105623314457555

Epoch: 6| Step: 13
Training loss: 3.417374849319458
Validation loss: 2.1187338367585213

Epoch: 39| Step: 0
Training loss: 2.575505256652832
Validation loss: 2.124593437358897

Epoch: 6| Step: 1
Training loss: 1.431315541267395
Validation loss: 2.1152518885110014

Epoch: 6| Step: 2
Training loss: 2.555612087249756
Validation loss: 2.1232474093796103

Epoch: 6| Step: 3
Training loss: 2.014249801635742
Validation loss: 2.109552924351026

Epoch: 6| Step: 4
Training loss: 1.986711859703064
Validation loss: 2.120908557727773

Epoch: 6| Step: 5
Training loss: 2.8803646564483643
Validation loss: 2.1117850195976997

Epoch: 6| Step: 6
Training loss: 2.3855130672454834
Validation loss: 2.1283378908711095

Epoch: 6| Step: 7
Training loss: 2.7755136489868164
Validation loss: 2.123289247994782

Epoch: 6| Step: 8
Training loss: 2.935905933380127
Validation loss: 2.1206388576056368

Epoch: 6| Step: 9
Training loss: 2.2708215713500977
Validation loss: 2.1128799197494343

Epoch: 6| Step: 10
Training loss: 1.5355288982391357
Validation loss: 2.1110531463417956

Epoch: 6| Step: 11
Training loss: 2.9206576347351074
Validation loss: 2.119022580885118

Epoch: 6| Step: 12
Training loss: 2.35736346244812
Validation loss: 2.1157047517838015

Epoch: 6| Step: 13
Training loss: 2.112880229949951
Validation loss: 2.1153951127042054

Epoch: 40| Step: 0
Training loss: 2.8141427040100098
Validation loss: 2.1289361112861225

Epoch: 6| Step: 1
Training loss: 2.3881425857543945
Validation loss: 2.1242211198294036

Epoch: 6| Step: 2
Training loss: 2.513378620147705
Validation loss: 2.1462595155162196

Epoch: 6| Step: 3
Training loss: 2.3217272758483887
Validation loss: 2.1242860517194195

Epoch: 6| Step: 4
Training loss: 2.058197259902954
Validation loss: 2.1258964371937576

Epoch: 6| Step: 5
Training loss: 2.472099781036377
Validation loss: 2.124007789037561

Epoch: 6| Step: 6
Training loss: 2.26214599609375
Validation loss: 2.1036424816295667

Epoch: 6| Step: 7
Training loss: 2.9128692150115967
Validation loss: 2.1354209107737385

Epoch: 6| Step: 8
Training loss: 1.8911091089248657
Validation loss: 2.133183180644948

Epoch: 6| Step: 9
Training loss: 2.336606025695801
Validation loss: 2.145432501710871

Epoch: 6| Step: 10
Training loss: 2.751697540283203
Validation loss: 2.1459166901085966

Epoch: 6| Step: 11
Training loss: 1.9347724914550781
Validation loss: 2.143096423918201

Epoch: 6| Step: 12
Training loss: 2.227177143096924
Validation loss: 2.129307544359597

Epoch: 6| Step: 13
Training loss: 1.8317750692367554
Validation loss: 2.144541773744809

Epoch: 41| Step: 0
Training loss: 1.8236722946166992
Validation loss: 2.138682101362495

Epoch: 6| Step: 1
Training loss: 1.5998399257659912
Validation loss: 2.1245690776455786

Epoch: 6| Step: 2
Training loss: 2.7781243324279785
Validation loss: 2.128150914305

Epoch: 6| Step: 3
Training loss: 2.3076460361480713
Validation loss: 2.137817067484702

Epoch: 6| Step: 4
Training loss: 2.4721410274505615
Validation loss: 2.1290512354143205

Epoch: 6| Step: 5
Training loss: 2.7230734825134277
Validation loss: 2.124479014386413

Epoch: 6| Step: 6
Training loss: 2.963776111602783
Validation loss: 2.1139711795314664

Epoch: 6| Step: 7
Training loss: 2.354783535003662
Validation loss: 2.1324555771325224

Epoch: 6| Step: 8
Training loss: 2.467683792114258
Validation loss: 2.121861332206316

Epoch: 6| Step: 9
Training loss: 2.4375996589660645
Validation loss: 2.106710062232069

Epoch: 6| Step: 10
Training loss: 1.8292770385742188
Validation loss: 2.0942381915225776

Epoch: 6| Step: 11
Training loss: 2.30430269241333
Validation loss: 2.1367187371817966

Epoch: 6| Step: 12
Training loss: 2.4496328830718994
Validation loss: 2.1041590577812603

Epoch: 6| Step: 13
Training loss: 2.1709063053131104
Validation loss: 2.1342921667201544

Epoch: 42| Step: 0
Training loss: 2.679335117340088
Validation loss: 2.1010644743519444

Epoch: 6| Step: 1
Training loss: 1.9625165462493896
Validation loss: 2.108382386545981

Epoch: 6| Step: 2
Training loss: 2.0748376846313477
Validation loss: 2.104286345102454

Epoch: 6| Step: 3
Training loss: 2.469806671142578
Validation loss: 2.121552216109409

Epoch: 6| Step: 4
Training loss: 3.093937873840332
Validation loss: 2.1184942158319617

Epoch: 6| Step: 5
Training loss: 2.9863619804382324
Validation loss: 2.1259351097127444

Epoch: 6| Step: 6
Training loss: 2.1868715286254883
Validation loss: 2.110307473008351

Epoch: 6| Step: 7
Training loss: 1.7320419549942017
Validation loss: 2.107771208209376

Epoch: 6| Step: 8
Training loss: 1.9148283004760742
Validation loss: 2.1102653088108188

Epoch: 6| Step: 9
Training loss: 3.1703062057495117
Validation loss: 2.1388518194998465

Epoch: 6| Step: 10
Training loss: 1.5936031341552734
Validation loss: 2.1137090280491817

Epoch: 6| Step: 11
Training loss: 2.4365944862365723
Validation loss: 2.136677513840378

Epoch: 6| Step: 12
Training loss: 1.9008207321166992
Validation loss: 2.1239754217927174

Epoch: 6| Step: 13
Training loss: 2.7313973903656006
Validation loss: 2.1277368991605696

Epoch: 43| Step: 0
Training loss: 1.7599557638168335
Validation loss: 2.1249568180371354

Epoch: 6| Step: 1
Training loss: 2.940523624420166
Validation loss: 2.1356157090074275

Epoch: 6| Step: 2
Training loss: 2.0259056091308594
Validation loss: 2.1178172890857985

Epoch: 6| Step: 3
Training loss: 1.8543543815612793
Validation loss: 2.1366576430618123

Epoch: 6| Step: 4
Training loss: 1.9569391012191772
Validation loss: 2.1187866375010502

Epoch: 6| Step: 5
Training loss: 2.0238547325134277
Validation loss: 2.1196547426203245

Epoch: 6| Step: 6
Training loss: 2.8963327407836914
Validation loss: 2.123006664296632

Epoch: 6| Step: 7
Training loss: 2.189143180847168
Validation loss: 2.1274909844962497

Epoch: 6| Step: 8
Training loss: 3.07974910736084
Validation loss: 2.1271705601804998

Epoch: 6| Step: 9
Training loss: 2.8663973808288574
Validation loss: 2.1199593736279394

Epoch: 6| Step: 10
Training loss: 1.838456392288208
Validation loss: 2.1242622508797595

Epoch: 6| Step: 11
Training loss: 2.2520389556884766
Validation loss: 2.117341701702405

Epoch: 6| Step: 12
Training loss: 2.6338062286376953
Validation loss: 2.1236593531024073

Epoch: 6| Step: 13
Training loss: 2.3426687717437744
Validation loss: 2.121892559912897

Epoch: 44| Step: 0
Training loss: 2.2272567749023438
Validation loss: 2.112797846076309

Epoch: 6| Step: 1
Training loss: 2.5689749717712402
Validation loss: 2.1198773307185017

Epoch: 6| Step: 2
Training loss: 1.7146508693695068
Validation loss: 2.126035600580195

Epoch: 6| Step: 3
Training loss: 3.1220803260803223
Validation loss: 2.1211461431236676

Epoch: 6| Step: 4
Training loss: 1.7962584495544434
Validation loss: 2.136693407130498

Epoch: 6| Step: 5
Training loss: 2.2602055072784424
Validation loss: 2.1386025746663413

Epoch: 6| Step: 6
Training loss: 2.010197401046753
Validation loss: 2.1310933610444427

Epoch: 6| Step: 7
Training loss: 2.2209198474884033
Validation loss: 2.118316529899515

Epoch: 6| Step: 8
Training loss: 2.956033706665039
Validation loss: 2.121065769144284

Epoch: 6| Step: 9
Training loss: 2.817291498184204
Validation loss: 2.1214474657530427

Epoch: 6| Step: 10
Training loss: 1.8057949542999268
Validation loss: 2.1227486800122004

Epoch: 6| Step: 11
Training loss: 2.324867010116577
Validation loss: 2.10120117023427

Epoch: 6| Step: 12
Training loss: 2.517162799835205
Validation loss: 2.1158352436557895

Epoch: 6| Step: 13
Training loss: 2.1078386306762695
Validation loss: 2.113297986727889

Epoch: 45| Step: 0
Training loss: 2.392899990081787
Validation loss: 2.1197489205227105

Epoch: 6| Step: 1
Training loss: 1.9771108627319336
Validation loss: 2.1036135945268857

Epoch: 6| Step: 2
Training loss: 1.8194475173950195
Validation loss: 2.1276046793947936

Epoch: 6| Step: 3
Training loss: 1.879317283630371
Validation loss: 2.1239072789428053

Epoch: 6| Step: 4
Training loss: 1.9787663221359253
Validation loss: 2.110205809275309

Epoch: 6| Step: 5
Training loss: 2.4148802757263184
Validation loss: 2.1020286031948623

Epoch: 6| Step: 6
Training loss: 3.037137985229492
Validation loss: 2.134848292155932

Epoch: 6| Step: 7
Training loss: 2.1405680179595947
Validation loss: 2.1248358782901557

Epoch: 6| Step: 8
Training loss: 2.39351487159729
Validation loss: 2.1057940785602858

Epoch: 6| Step: 9
Training loss: 2.972775936126709
Validation loss: 2.1367589555760866

Epoch: 6| Step: 10
Training loss: 2.903745412826538
Validation loss: 2.109656008340979

Epoch: 6| Step: 11
Training loss: 1.7901346683502197
Validation loss: 2.1109805504480996

Epoch: 6| Step: 12
Training loss: 2.076963424682617
Validation loss: 2.1075240514611684

Epoch: 6| Step: 13
Training loss: 2.716027021408081
Validation loss: 2.109249148317563

Epoch: 46| Step: 0
Training loss: 2.6766881942749023
Validation loss: 2.1134047226239274

Epoch: 6| Step: 1
Training loss: 2.447204828262329
Validation loss: 2.11608205046705

Epoch: 6| Step: 2
Training loss: 2.9653494358062744
Validation loss: 2.1010254480505504

Epoch: 6| Step: 3
Training loss: 2.7640209197998047
Validation loss: 2.1204397204101726

Epoch: 6| Step: 4
Training loss: 2.1477603912353516
Validation loss: 2.128428925750076

Epoch: 6| Step: 5
Training loss: 2.64522647857666
Validation loss: 2.1463776198766564

Epoch: 6| Step: 6
Training loss: 3.279265880584717
Validation loss: 2.116594570939259

Epoch: 6| Step: 7
Training loss: 2.2638957500457764
Validation loss: 2.1219721122454573

Epoch: 6| Step: 8
Training loss: 2.2453722953796387
Validation loss: 2.122455314923358

Epoch: 6| Step: 9
Training loss: 1.5951714515686035
Validation loss: 2.137746521221694

Epoch: 6| Step: 10
Training loss: 1.8155759572982788
Validation loss: 2.1336926670484644

Epoch: 6| Step: 11
Training loss: 1.425497055053711
Validation loss: 2.126626612037741

Epoch: 6| Step: 12
Training loss: 1.726362705230713
Validation loss: 2.1199801147625013

Epoch: 6| Step: 13
Training loss: 2.488518714904785
Validation loss: 2.127581170810166

Epoch: 47| Step: 0
Training loss: 2.0076138973236084
Validation loss: 2.1447078643306607

Epoch: 6| Step: 1
Training loss: 2.2954955101013184
Validation loss: 2.1209438923866517

Epoch: 6| Step: 2
Training loss: 2.1236040592193604
Validation loss: 2.1427032152811685

Epoch: 6| Step: 3
Training loss: 2.735710382461548
Validation loss: 2.13829844356865

Epoch: 6| Step: 4
Training loss: 2.276660442352295
Validation loss: 2.130121211851797

Epoch: 6| Step: 5
Training loss: 2.4653563499450684
Validation loss: 2.123619860218417

Epoch: 6| Step: 6
Training loss: 2.518639087677002
Validation loss: 2.137459247343002

Epoch: 6| Step: 7
Training loss: 2.331648826599121
Validation loss: 2.1254884068683912

Epoch: 6| Step: 8
Training loss: 2.774606227874756
Validation loss: 2.126504957035024

Epoch: 6| Step: 9
Training loss: 1.3947827816009521
Validation loss: 2.1463788376059583

Epoch: 6| Step: 10
Training loss: 2.6246204376220703
Validation loss: 2.138173249460036

Epoch: 6| Step: 11
Training loss: 2.212325096130371
Validation loss: 2.120282814066897

Epoch: 6| Step: 12
Training loss: 2.355644702911377
Validation loss: 2.104159342345371

Epoch: 6| Step: 13
Training loss: 2.009904146194458
Validation loss: 2.1404761575883433

Epoch: 48| Step: 0
Training loss: 2.0616302490234375
Validation loss: 2.144531676846166

Epoch: 6| Step: 1
Training loss: 3.0261378288269043
Validation loss: 2.14046327529415

Epoch: 6| Step: 2
Training loss: 2.0840628147125244
Validation loss: 2.122296617877099

Epoch: 6| Step: 3
Training loss: 2.083071708679199
Validation loss: 2.1486423015594482

Epoch: 6| Step: 4
Training loss: 3.1509556770324707
Validation loss: 2.1083017139024633

Epoch: 6| Step: 5
Training loss: 1.988997459411621
Validation loss: 2.128272707744311

Epoch: 6| Step: 6
Training loss: 1.9622615575790405
Validation loss: 2.139985074279129

Epoch: 6| Step: 7
Training loss: 2.5763745307922363
Validation loss: 2.119663341071016

Epoch: 6| Step: 8
Training loss: 1.4927482604980469
Validation loss: 2.118193567440074

Epoch: 6| Step: 9
Training loss: 1.7516860961914062
Validation loss: 2.133249799410502

Epoch: 6| Step: 10
Training loss: 2.0995678901672363
Validation loss: 2.129666604021544

Epoch: 6| Step: 11
Training loss: 2.885190486907959
Validation loss: 2.126254609836045

Epoch: 6| Step: 12
Training loss: 2.8664348125457764
Validation loss: 2.108496730045606

Epoch: 6| Step: 13
Training loss: 2.4514074325561523
Validation loss: 2.1159976028626963

Epoch: 49| Step: 0
Training loss: 2.471855878829956
Validation loss: 2.099990988290438

Epoch: 6| Step: 1
Training loss: 1.9088225364685059
Validation loss: 2.128031640924433

Epoch: 6| Step: 2
Training loss: 2.392976760864258
Validation loss: 2.103521126572804

Epoch: 6| Step: 3
Training loss: 2.1589651107788086
Validation loss: 2.141323571564049

Epoch: 6| Step: 4
Training loss: 2.1471621990203857
Validation loss: 2.107043545733216

Epoch: 6| Step: 5
Training loss: 2.447340488433838
Validation loss: 2.1366281394035584

Epoch: 6| Step: 6
Training loss: 2.202746868133545
Validation loss: 2.104487319146433

Epoch: 6| Step: 7
Training loss: 2.0897889137268066
Validation loss: 2.1272394195679696

Epoch: 6| Step: 8
Training loss: 2.1851930618286133
Validation loss: 2.121422390783987

Epoch: 6| Step: 9
Training loss: 2.3776893615722656
Validation loss: 2.121109467680736

Epoch: 6| Step: 10
Training loss: 2.1759021282196045
Validation loss: 2.1056333293196974

Epoch: 6| Step: 11
Training loss: 2.6129097938537598
Validation loss: 2.1094964294023413

Epoch: 6| Step: 12
Training loss: 2.8085317611694336
Validation loss: 2.0971060517013713

Epoch: 6| Step: 13
Training loss: 2.2298405170440674
Validation loss: 2.1079755829226587

Epoch: 50| Step: 0
Training loss: 1.442439317703247
Validation loss: 2.091534746590481

Epoch: 6| Step: 1
Training loss: 2.330899238586426
Validation loss: 2.1204339560642036

Epoch: 6| Step: 2
Training loss: 2.744274616241455
Validation loss: 2.1093900998433432

Epoch: 6| Step: 3
Training loss: 2.202106475830078
Validation loss: 2.1259758498079036

Epoch: 6| Step: 4
Training loss: 2.4117929935455322
Validation loss: 2.1133955370995308

Epoch: 6| Step: 5
Training loss: 2.3169350624084473
Validation loss: 2.108588175107074

Epoch: 6| Step: 6
Training loss: 2.822770833969116
Validation loss: 2.120157749422135

Epoch: 6| Step: 7
Training loss: 2.253671646118164
Validation loss: 2.0973036622488372

Epoch: 6| Step: 8
Training loss: 2.304595947265625
Validation loss: 2.1099828891856696

Epoch: 6| Step: 9
Training loss: 2.178712844848633
Validation loss: 2.109310847456737

Epoch: 6| Step: 10
Training loss: 1.9685702323913574
Validation loss: 2.129339087393976

Epoch: 6| Step: 11
Training loss: 2.068855047225952
Validation loss: 2.112756395852694

Epoch: 6| Step: 12
Training loss: 2.2680366039276123
Validation loss: 2.12574069217969

Epoch: 6| Step: 13
Training loss: 3.2606966495513916
Validation loss: 2.1322049658785582

Epoch: 51| Step: 0
Training loss: 2.2630271911621094
Validation loss: 2.129263392058752

Epoch: 6| Step: 1
Training loss: 2.4834887981414795
Validation loss: 2.128345540774766

Epoch: 6| Step: 2
Training loss: 1.8055377006530762
Validation loss: 2.113137778415475

Epoch: 6| Step: 3
Training loss: 3.234750747680664
Validation loss: 2.147099562870559

Epoch: 6| Step: 4
Training loss: 2.1569347381591797
Validation loss: 2.1235296110953055

Epoch: 6| Step: 5
Training loss: 2.707369565963745
Validation loss: 2.1268792075495564

Epoch: 6| Step: 6
Training loss: 2.0809173583984375
Validation loss: 2.118093300891179

Epoch: 6| Step: 7
Training loss: 1.9702082872390747
Validation loss: 2.12294013269486

Epoch: 6| Step: 8
Training loss: 2.8617262840270996
Validation loss: 2.1383320246973345

Epoch: 6| Step: 9
Training loss: 2.3753116130828857
Validation loss: 2.1380870444800264

Epoch: 6| Step: 10
Training loss: 2.23738431930542
Validation loss: 2.1599211820992092

Epoch: 6| Step: 11
Training loss: 1.9114012718200684
Validation loss: 2.135430095016315

Epoch: 6| Step: 12
Training loss: 1.7656350135803223
Validation loss: 2.134592492093322

Epoch: 6| Step: 13
Training loss: 2.0242719650268555
Validation loss: 2.1232325902549167

Epoch: 52| Step: 0
Training loss: 2.0021207332611084
Validation loss: 2.1430026638892388

Epoch: 6| Step: 1
Training loss: 1.6761045455932617
Validation loss: 2.1094012465528262

Epoch: 6| Step: 2
Training loss: 2.743715763092041
Validation loss: 2.1251611478867067

Epoch: 6| Step: 3
Training loss: 2.4658126831054688
Validation loss: 2.122404590729744

Epoch: 6| Step: 4
Training loss: 1.627366542816162
Validation loss: 2.129195700409592

Epoch: 6| Step: 5
Training loss: 3.0261144638061523
Validation loss: 2.0916783566116006

Epoch: 6| Step: 6
Training loss: 2.600944995880127
Validation loss: 2.130281040745397

Epoch: 6| Step: 7
Training loss: 2.362743377685547
Validation loss: 2.149734527834

Epoch: 6| Step: 8
Training loss: 1.5865199565887451
Validation loss: 2.122732568812627

Epoch: 6| Step: 9
Training loss: 2.244694709777832
Validation loss: 2.1333016964697067

Epoch: 6| Step: 10
Training loss: 2.3756561279296875
Validation loss: 2.1303832223338466

Epoch: 6| Step: 11
Training loss: 2.5295827388763428
Validation loss: 2.138486821164367

Epoch: 6| Step: 12
Training loss: 3.0366265773773193
Validation loss: 2.1291436764501754

Epoch: 6| Step: 13
Training loss: 1.366274356842041
Validation loss: 2.1198200089957124

Epoch: 53| Step: 0
Training loss: 2.664393424987793
Validation loss: 2.1302660203749135

Epoch: 6| Step: 1
Training loss: 2.0591142177581787
Validation loss: 2.115439284232355

Epoch: 6| Step: 2
Training loss: 2.3109755516052246
Validation loss: 2.1398768232714747

Epoch: 6| Step: 3
Training loss: 2.5597853660583496
Validation loss: 2.1161602927792456

Epoch: 6| Step: 4
Training loss: 2.1660919189453125
Validation loss: 2.128927235962242

Epoch: 6| Step: 5
Training loss: 2.368649482727051
Validation loss: 2.108684259076272

Epoch: 6| Step: 6
Training loss: 2.2665469646453857
Validation loss: 2.1237345408367854

Epoch: 6| Step: 7
Training loss: 1.460803508758545
Validation loss: 2.1112137097184376

Epoch: 6| Step: 8
Training loss: 2.03825306892395
Validation loss: 2.129210240097456

Epoch: 6| Step: 9
Training loss: 2.899092674255371
Validation loss: 2.120039152842696

Epoch: 6| Step: 10
Training loss: 2.7412121295928955
Validation loss: 2.107546760189918

Epoch: 6| Step: 11
Training loss: 2.8637635707855225
Validation loss: 2.124830389535555

Epoch: 6| Step: 12
Training loss: 1.4497876167297363
Validation loss: 2.0994257465485604

Epoch: 6| Step: 13
Training loss: 2.0851986408233643
Validation loss: 2.105625678134221

Epoch: 54| Step: 0
Training loss: 1.737938404083252
Validation loss: 2.113891145234467

Epoch: 6| Step: 1
Training loss: 2.291464328765869
Validation loss: 2.115674075259957

Epoch: 6| Step: 2
Training loss: 2.330627918243408
Validation loss: 2.128214136246712

Epoch: 6| Step: 3
Training loss: 2.689426898956299
Validation loss: 2.1136876049862114

Epoch: 6| Step: 4
Training loss: 2.428281784057617
Validation loss: 2.141934517891176

Epoch: 6| Step: 5
Training loss: 2.390289068222046
Validation loss: 2.105010615882053

Epoch: 6| Step: 6
Training loss: 2.2040586471557617
Validation loss: 2.136114271738196

Epoch: 6| Step: 7
Training loss: 2.624931573867798
Validation loss: 2.135197559992472

Epoch: 6| Step: 8
Training loss: 2.4134788513183594
Validation loss: 2.123119620866673

Epoch: 6| Step: 9
Training loss: 2.0803563594818115
Validation loss: 2.12511505106444

Epoch: 6| Step: 10
Training loss: 1.8922762870788574
Validation loss: 2.1342358640445176

Epoch: 6| Step: 11
Training loss: 2.514694929122925
Validation loss: 2.1293681616424234

Epoch: 6| Step: 12
Training loss: 1.9783076047897339
Validation loss: 2.1192967391783193

Epoch: 6| Step: 13
Training loss: 2.6911325454711914
Validation loss: 2.1214179967039373

Epoch: 55| Step: 0
Training loss: 1.8341021537780762
Validation loss: 2.1366687333712013

Epoch: 6| Step: 1
Training loss: 2.6227164268493652
Validation loss: 2.1220268229002595

Epoch: 6| Step: 2
Training loss: 2.0424039363861084
Validation loss: 2.1151394433872674

Epoch: 6| Step: 3
Training loss: 1.8146705627441406
Validation loss: 2.1021943284619238

Epoch: 6| Step: 4
Training loss: 2.1565122604370117
Validation loss: 2.1153906109512493

Epoch: 6| Step: 5
Training loss: 2.940241813659668
Validation loss: 2.1121963236921575

Epoch: 6| Step: 6
Training loss: 1.9540574550628662
Validation loss: 2.112881429733769

Epoch: 6| Step: 7
Training loss: 1.8438506126403809
Validation loss: 2.1290528056442097

Epoch: 6| Step: 8
Training loss: 2.3545055389404297
Validation loss: 2.1459952759486374

Epoch: 6| Step: 9
Training loss: 1.9293498992919922
Validation loss: 2.1341656574638943

Epoch: 6| Step: 10
Training loss: 2.5915064811706543
Validation loss: 2.109216328590147

Epoch: 6| Step: 11
Training loss: 2.7274203300476074
Validation loss: 2.151164170234434

Epoch: 6| Step: 12
Training loss: 2.4012176990509033
Validation loss: 2.1516595168780257

Epoch: 6| Step: 13
Training loss: 2.7255494594573975
Validation loss: 2.1460084966433945

Epoch: 56| Step: 0
Training loss: 1.994091272354126
Validation loss: 2.11736471678621

Epoch: 6| Step: 1
Training loss: 1.9684525728225708
Validation loss: 2.115728598768993

Epoch: 6| Step: 2
Training loss: 2.126898765563965
Validation loss: 2.1332264920716644

Epoch: 6| Step: 3
Training loss: 2.306351900100708
Validation loss: 2.1343636705029394

Epoch: 6| Step: 4
Training loss: 1.937004804611206
Validation loss: 2.1219668657548967

Epoch: 6| Step: 5
Training loss: 1.6597011089324951
Validation loss: 2.1153697557346796

Epoch: 6| Step: 6
Training loss: 2.566943883895874
Validation loss: 2.10731537624072

Epoch: 6| Step: 7
Training loss: 2.343754529953003
Validation loss: 2.1217897297233663

Epoch: 6| Step: 8
Training loss: 3.3367574214935303
Validation loss: 2.1429678932312997

Epoch: 6| Step: 9
Training loss: 1.9505208730697632
Validation loss: 2.121189499414095

Epoch: 6| Step: 10
Training loss: 1.723534345626831
Validation loss: 2.1344290215481996

Epoch: 6| Step: 11
Training loss: 2.6189005374908447
Validation loss: 2.124109293824883

Epoch: 6| Step: 12
Training loss: 2.7417473793029785
Validation loss: 2.125025321078557

Epoch: 6| Step: 13
Training loss: 2.6876659393310547
Validation loss: 2.1017283944673437

Epoch: 57| Step: 0
Training loss: 2.006946325302124
Validation loss: 2.12622146965355

Epoch: 6| Step: 1
Training loss: 1.8431488275527954
Validation loss: 2.1246650295872844

Epoch: 6| Step: 2
Training loss: 2.6482276916503906
Validation loss: 2.1226759059454805

Epoch: 6| Step: 3
Training loss: 1.6771056652069092
Validation loss: 2.1335879987285984

Epoch: 6| Step: 4
Training loss: 2.5819919109344482
Validation loss: 2.1177477990427325

Epoch: 6| Step: 5
Training loss: 1.7535701990127563
Validation loss: 2.1174531034244004

Epoch: 6| Step: 6
Training loss: 2.5647075176239014
Validation loss: 2.114764682708248

Epoch: 6| Step: 7
Training loss: 2.3551249504089355
Validation loss: 2.1251118542045675

Epoch: 6| Step: 8
Training loss: 2.4960741996765137
Validation loss: 2.13204126973306

Epoch: 6| Step: 9
Training loss: 2.8299856185913086
Validation loss: 2.10679223973264

Epoch: 6| Step: 10
Training loss: 2.0912797451019287
Validation loss: 2.1222352738021524

Epoch: 6| Step: 11
Training loss: 1.9598073959350586
Validation loss: 2.125081375081052

Epoch: 6| Step: 12
Training loss: 2.4168143272399902
Validation loss: 2.112958736317132

Epoch: 6| Step: 13
Training loss: 2.636039972305298
Validation loss: 2.123462297583139

Epoch: 58| Step: 0
Training loss: 2.0898187160491943
Validation loss: 2.1029762747467204

Epoch: 6| Step: 1
Training loss: 2.1001815795898438
Validation loss: 2.1214450700308687

Epoch: 6| Step: 2
Training loss: 1.995462417602539
Validation loss: 2.106630230462679

Epoch: 6| Step: 3
Training loss: 2.909970760345459
Validation loss: 2.1175703169197164

Epoch: 6| Step: 4
Training loss: 1.8980094194412231
Validation loss: 2.105406638114683

Epoch: 6| Step: 5
Training loss: 2.1385278701782227
Validation loss: 2.1286701950975644

Epoch: 6| Step: 6
Training loss: 2.1502461433410645
Validation loss: 2.1202558573856147

Epoch: 6| Step: 7
Training loss: 1.8252053260803223
Validation loss: 2.1203987085691063

Epoch: 6| Step: 8
Training loss: 2.2489333152770996
Validation loss: 2.1207028845305085

Epoch: 6| Step: 9
Training loss: 1.7968566417694092
Validation loss: 2.0995028224042667

Epoch: 6| Step: 10
Training loss: 2.2146806716918945
Validation loss: 2.0868327976554952

Epoch: 6| Step: 11
Training loss: 2.480135679244995
Validation loss: 2.093831839100007

Epoch: 6| Step: 12
Training loss: 3.0409340858459473
Validation loss: 2.1078732475157707

Epoch: 6| Step: 13
Training loss: 3.3799047470092773
Validation loss: 2.1215532031110538

Epoch: 59| Step: 0
Training loss: 2.2398266792297363
Validation loss: 2.102774623901613

Epoch: 6| Step: 1
Training loss: 2.51958966255188
Validation loss: 2.1284279464393534

Epoch: 6| Step: 2
Training loss: 1.360688328742981
Validation loss: 2.1344161328449043

Epoch: 6| Step: 3
Training loss: 1.7913286685943604
Validation loss: 2.1035915882356706

Epoch: 6| Step: 4
Training loss: 2.526968002319336
Validation loss: 2.1064064502716064

Epoch: 6| Step: 5
Training loss: 2.9998555183410645
Validation loss: 2.117046566419704

Epoch: 6| Step: 6
Training loss: 2.0702569484710693
Validation loss: 2.112211960618214

Epoch: 6| Step: 7
Training loss: 2.5447278022766113
Validation loss: 2.103862900887766

Epoch: 6| Step: 8
Training loss: 1.3982266187667847
Validation loss: 2.1142946135613228

Epoch: 6| Step: 9
Training loss: 2.866206645965576
Validation loss: 2.1481725221039145

Epoch: 6| Step: 10
Training loss: 1.7808411121368408
Validation loss: 2.1365097235607844

Epoch: 6| Step: 11
Training loss: 2.2784605026245117
Validation loss: 2.1115881140514086

Epoch: 6| Step: 12
Training loss: 2.672170639038086
Validation loss: 2.0973273554155902

Epoch: 6| Step: 13
Training loss: 2.3342556953430176
Validation loss: 2.123005426058205

Epoch: 60| Step: 0
Training loss: 2.0511488914489746
Validation loss: 2.117686325503934

Epoch: 6| Step: 1
Training loss: 2.0662636756896973
Validation loss: 2.1252973028408584

Epoch: 6| Step: 2
Training loss: 2.3764631748199463
Validation loss: 2.119895910704008

Epoch: 6| Step: 3
Training loss: 2.0713744163513184
Validation loss: 2.1068928472457396

Epoch: 6| Step: 4
Training loss: 2.4546852111816406
Validation loss: 2.1070346678456953

Epoch: 6| Step: 5
Training loss: 1.7603636980056763
Validation loss: 2.1043915594777753

Epoch: 6| Step: 6
Training loss: 2.5013060569763184
Validation loss: 2.112770208748438

Epoch: 6| Step: 7
Training loss: 2.4191598892211914
Validation loss: 2.125622369909799

Epoch: 6| Step: 8
Training loss: 2.1646392345428467
Validation loss: 2.0884907271272395

Epoch: 6| Step: 9
Training loss: 2.653150796890259
Validation loss: 2.1312557599877797

Epoch: 6| Step: 10
Training loss: 2.664114475250244
Validation loss: 2.1213203040502404

Epoch: 6| Step: 11
Training loss: 2.1646604537963867
Validation loss: 2.1221538833392564

Epoch: 6| Step: 12
Training loss: 2.4269959926605225
Validation loss: 2.1228408300748436

Epoch: 6| Step: 13
Training loss: 1.5527211427688599
Validation loss: 2.1156523919874624

Epoch: 61| Step: 0
Training loss: 2.0635499954223633
Validation loss: 2.1252245467196227

Epoch: 6| Step: 1
Training loss: 2.378638744354248
Validation loss: 2.113862463223037

Epoch: 6| Step: 2
Training loss: 2.0383524894714355
Validation loss: 2.115292590151551

Epoch: 6| Step: 3
Training loss: 2.332733631134033
Validation loss: 2.1059920890356905

Epoch: 6| Step: 4
Training loss: 1.6641979217529297
Validation loss: 2.1131719055996148

Epoch: 6| Step: 5
Training loss: 1.9226033687591553
Validation loss: 2.1129035411342496

Epoch: 6| Step: 6
Training loss: 2.769516944885254
Validation loss: 2.1113124380829515

Epoch: 6| Step: 7
Training loss: 1.7954504489898682
Validation loss: 2.1043871859068513

Epoch: 6| Step: 8
Training loss: 2.8148550987243652
Validation loss: 2.118314867378563

Epoch: 6| Step: 9
Training loss: 2.5277414321899414
Validation loss: 2.1052074432373047

Epoch: 6| Step: 10
Training loss: 1.9630541801452637
Validation loss: 2.0970884151356195

Epoch: 6| Step: 11
Training loss: 2.204352855682373
Validation loss: 2.097250420560119

Epoch: 6| Step: 12
Training loss: 2.7042198181152344
Validation loss: 2.1022323754525956

Epoch: 6| Step: 13
Training loss: 2.6225597858428955
Validation loss: 2.115266138507474

Epoch: 62| Step: 0
Training loss: 1.6877540349960327
Validation loss: 2.1302347080681914

Epoch: 6| Step: 1
Training loss: 1.852461814880371
Validation loss: 2.092534303665161

Epoch: 6| Step: 2
Training loss: 1.6656615734100342
Validation loss: 2.1029310534077306

Epoch: 6| Step: 3
Training loss: 2.43355393409729
Validation loss: 2.1305115851022864

Epoch: 6| Step: 4
Training loss: 2.258826732635498
Validation loss: 2.1355111573332097

Epoch: 6| Step: 5
Training loss: 1.7625410556793213
Validation loss: 2.1120349232868483

Epoch: 6| Step: 6
Training loss: 2.3428235054016113
Validation loss: 2.083924675500521

Epoch: 6| Step: 7
Training loss: 2.6259894371032715
Validation loss: 2.0899111609305105

Epoch: 6| Step: 8
Training loss: 2.821748733520508
Validation loss: 2.0887210881838234

Epoch: 6| Step: 9
Training loss: 2.480250835418701
Validation loss: 2.0884119361959477

Epoch: 6| Step: 10
Training loss: 2.8214995861053467
Validation loss: 2.1044495439016693

Epoch: 6| Step: 11
Training loss: 2.6661391258239746
Validation loss: 2.109887333326442

Epoch: 6| Step: 12
Training loss: 1.8481063842773438
Validation loss: 2.115857203801473

Epoch: 6| Step: 13
Training loss: 2.478287935256958
Validation loss: 2.1118212412762385

Epoch: 63| Step: 0
Training loss: 2.0489206314086914
Validation loss: 2.1043655064798172

Epoch: 6| Step: 1
Training loss: 1.7926700115203857
Validation loss: 2.136294860993662

Epoch: 6| Step: 2
Training loss: 2.640791654586792
Validation loss: 2.10703271178789

Epoch: 6| Step: 3
Training loss: 2.348233699798584
Validation loss: 2.1174110212633686

Epoch: 6| Step: 4
Training loss: 2.600105047225952
Validation loss: 2.113408932121851

Epoch: 6| Step: 5
Training loss: 1.9344199895858765
Validation loss: 2.105889474191973

Epoch: 6| Step: 6
Training loss: 2.563704013824463
Validation loss: 2.1145465271447295

Epoch: 6| Step: 7
Training loss: 2.5192480087280273
Validation loss: 2.1132025693052556

Epoch: 6| Step: 8
Training loss: 2.2354421615600586
Validation loss: 2.1021309565472346

Epoch: 6| Step: 9
Training loss: 1.9957629442214966
Validation loss: 2.1353366067332606

Epoch: 6| Step: 10
Training loss: 2.6311593055725098
Validation loss: 2.1205341367311377

Epoch: 6| Step: 11
Training loss: 2.811678647994995
Validation loss: 2.1165231556020756

Epoch: 6| Step: 12
Training loss: 1.928704023361206
Validation loss: 2.1233928024127917

Epoch: 6| Step: 13
Training loss: 1.0289006233215332
Validation loss: 2.1174570975765103

Epoch: 64| Step: 0
Training loss: 2.712493658065796
Validation loss: 2.1250282974653345

Epoch: 6| Step: 1
Training loss: 1.9772239923477173
Validation loss: 2.0925452222106276

Epoch: 6| Step: 2
Training loss: 2.9151110649108887
Validation loss: 2.107038818379884

Epoch: 6| Step: 3
Training loss: 1.8649821281433105
Validation loss: 2.1188169474242837

Epoch: 6| Step: 4
Training loss: 2.5498552322387695
Validation loss: 2.10032388471788

Epoch: 6| Step: 5
Training loss: 2.3739657402038574
Validation loss: 2.110488364773412

Epoch: 6| Step: 6
Training loss: 2.0859031677246094
Validation loss: 2.1124175697244625

Epoch: 6| Step: 7
Training loss: 2.1987009048461914
Validation loss: 2.092780356766075

Epoch: 6| Step: 8
Training loss: 2.2698168754577637
Validation loss: 2.1071386901281213

Epoch: 6| Step: 9
Training loss: 2.2980003356933594
Validation loss: 2.1134274005889893

Epoch: 6| Step: 10
Training loss: 2.150343894958496
Validation loss: 2.107499079037738

Epoch: 6| Step: 11
Training loss: 2.0360898971557617
Validation loss: 2.1194972299760386

Epoch: 6| Step: 12
Training loss: 1.8137454986572266
Validation loss: 2.131935360611126

Epoch: 6| Step: 13
Training loss: 2.3734123706817627
Validation loss: 2.1347710368453816

Epoch: 65| Step: 0
Training loss: 2.4125900268554688
Validation loss: 2.112974334788579

Epoch: 6| Step: 1
Training loss: 1.9647681713104248
Validation loss: 2.131805746786056

Epoch: 6| Step: 2
Training loss: 1.7296464443206787
Validation loss: 2.11222134482476

Epoch: 6| Step: 3
Training loss: 2.082728862762451
Validation loss: 2.110301657389569

Epoch: 6| Step: 4
Training loss: 3.0236258506774902
Validation loss: 2.116893281218826

Epoch: 6| Step: 5
Training loss: 1.8097054958343506
Validation loss: 2.1334737193199897

Epoch: 6| Step: 6
Training loss: 2.570277690887451
Validation loss: 2.1199076047507663

Epoch: 6| Step: 7
Training loss: 2.2323384284973145
Validation loss: 2.129114104855445

Epoch: 6| Step: 8
Training loss: 2.7222390174865723
Validation loss: 2.1157657830945906

Epoch: 6| Step: 9
Training loss: 1.8766591548919678
Validation loss: 2.1206953858816497

Epoch: 6| Step: 10
Training loss: 1.6034717559814453
Validation loss: 2.117987460987542

Epoch: 6| Step: 11
Training loss: 2.418647050857544
Validation loss: 2.1296219364289315

Epoch: 6| Step: 12
Training loss: 3.0042524337768555
Validation loss: 2.1281397137590634

Epoch: 6| Step: 13
Training loss: 1.645159125328064
Validation loss: 2.1194873086867796

Epoch: 66| Step: 0
Training loss: 1.5706727504730225
Validation loss: 2.1107237339019775

Epoch: 6| Step: 1
Training loss: 2.1578216552734375
Validation loss: 2.126508979387181

Epoch: 6| Step: 2
Training loss: 1.8820617198944092
Validation loss: 2.1011552246668006

Epoch: 6| Step: 3
Training loss: 2.119781970977783
Validation loss: 2.1011555912674114

Epoch: 6| Step: 4
Training loss: 1.9176692962646484
Validation loss: 2.120454162679693

Epoch: 6| Step: 5
Training loss: 2.2799878120422363
Validation loss: 2.1379588880846576

Epoch: 6| Step: 6
Training loss: 2.301469326019287
Validation loss: 2.1255114309249388

Epoch: 6| Step: 7
Training loss: 3.278632402420044
Validation loss: 2.125363798551662

Epoch: 6| Step: 8
Training loss: 2.379103899002075
Validation loss: 2.1271181439840667

Epoch: 6| Step: 9
Training loss: 1.7209478616714478
Validation loss: 2.122803536794519

Epoch: 6| Step: 10
Training loss: 3.1998348236083984
Validation loss: 2.1014110478021766

Epoch: 6| Step: 11
Training loss: 1.850003719329834
Validation loss: 2.1208512501050065

Epoch: 6| Step: 12
Training loss: 1.7390129566192627
Validation loss: 2.1278009542854885

Epoch: 6| Step: 13
Training loss: 3.3554766178131104
Validation loss: 2.117830293152922

Epoch: 67| Step: 0
Training loss: 2.028262138366699
Validation loss: 2.0950319984907746

Epoch: 6| Step: 1
Training loss: 2.3549182415008545
Validation loss: 2.144077830417182

Epoch: 6| Step: 2
Training loss: 2.400224208831787
Validation loss: 2.1075411406896447

Epoch: 6| Step: 3
Training loss: 1.9847667217254639
Validation loss: 2.104312453218686

Epoch: 6| Step: 4
Training loss: 2.521787166595459
Validation loss: 2.1432461482222362

Epoch: 6| Step: 5
Training loss: 2.1889991760253906
Validation loss: 2.1144599991460002

Epoch: 6| Step: 6
Training loss: 1.638083815574646
Validation loss: 2.1139745122642926

Epoch: 6| Step: 7
Training loss: 2.217210054397583
Validation loss: 2.1044743189247708

Epoch: 6| Step: 8
Training loss: 2.191406488418579
Validation loss: 2.1048573447811987

Epoch: 6| Step: 9
Training loss: 3.0045676231384277
Validation loss: 2.1078887665143577

Epoch: 6| Step: 10
Training loss: 1.7063156366348267
Validation loss: 2.141839240186958

Epoch: 6| Step: 11
Training loss: 2.774561643600464
Validation loss: 2.1192094305510163

Epoch: 6| Step: 12
Training loss: 2.0251827239990234
Validation loss: 2.1254463336801015

Epoch: 6| Step: 13
Training loss: 2.8513026237487793
Validation loss: 2.098973666467974

Epoch: 68| Step: 0
Training loss: 1.8801043033599854
Validation loss: 2.1027087652555077

Epoch: 6| Step: 1
Training loss: 2.5372414588928223
Validation loss: 2.1304384995532293

Epoch: 6| Step: 2
Training loss: 2.0822598934173584
Validation loss: 2.1057707032849713

Epoch: 6| Step: 3
Training loss: 2.306955575942993
Validation loss: 2.1044859078622635

Epoch: 6| Step: 4
Training loss: 1.5475554466247559
Validation loss: 2.1113595116523003

Epoch: 6| Step: 5
Training loss: 2.7719287872314453
Validation loss: 2.1200236351259294

Epoch: 6| Step: 6
Training loss: 2.2980217933654785
Validation loss: 2.1094502300344486

Epoch: 6| Step: 7
Training loss: 1.8058059215545654
Validation loss: 2.103492647088984

Epoch: 6| Step: 8
Training loss: 2.235970973968506
Validation loss: 2.111037096669597

Epoch: 6| Step: 9
Training loss: 2.900872230529785
Validation loss: 2.0977085533962456

Epoch: 6| Step: 10
Training loss: 2.528412342071533
Validation loss: 2.1080470213326077

Epoch: 6| Step: 11
Training loss: 1.6717143058776855
Validation loss: 2.1137260467775407

Epoch: 6| Step: 12
Training loss: 2.5272879600524902
Validation loss: 2.115084837841731

Epoch: 6| Step: 13
Training loss: 2.176980495452881
Validation loss: 2.1103894402903896

Epoch: 69| Step: 0
Training loss: 2.818878173828125
Validation loss: 2.1032585251715874

Epoch: 6| Step: 1
Training loss: 2.179184913635254
Validation loss: 2.087142677717311

Epoch: 6| Step: 2
Training loss: 2.8101954460144043
Validation loss: 2.110832901411159

Epoch: 6| Step: 3
Training loss: 2.414547920227051
Validation loss: 2.1243794887296614

Epoch: 6| Step: 4
Training loss: 1.9431864023208618
Validation loss: 2.1279524346833587

Epoch: 6| Step: 5
Training loss: 3.1315670013427734
Validation loss: 2.1223945271584297

Epoch: 6| Step: 6
Training loss: 1.6423755884170532
Validation loss: 2.115612329975251

Epoch: 6| Step: 7
Training loss: 2.065824508666992
Validation loss: 2.1253795059778358

Epoch: 6| Step: 8
Training loss: 1.9376499652862549
Validation loss: 2.1209786399718253

Epoch: 6| Step: 9
Training loss: 2.1610827445983887
Validation loss: 2.1265343927568003

Epoch: 6| Step: 10
Training loss: 2.1002514362335205
Validation loss: 2.1160184587201765

Epoch: 6| Step: 11
Training loss: 1.9168872833251953
Validation loss: 2.113659874085457

Epoch: 6| Step: 12
Training loss: 2.012080192565918
Validation loss: 2.132052649733841

Epoch: 6| Step: 13
Training loss: 2.132145643234253
Validation loss: 2.1301597805433374

Epoch: 70| Step: 0
Training loss: 2.3548429012298584
Validation loss: 2.130459103533017

Epoch: 6| Step: 1
Training loss: 1.9153366088867188
Validation loss: 2.133767997064898

Epoch: 6| Step: 2
Training loss: 2.719820499420166
Validation loss: 2.127590754980682

Epoch: 6| Step: 3
Training loss: 2.0289623737335205
Validation loss: 2.110710556789111

Epoch: 6| Step: 4
Training loss: 2.275845527648926
Validation loss: 2.137109802615258

Epoch: 6| Step: 5
Training loss: 1.9776370525360107
Validation loss: 2.115626332580402

Epoch: 6| Step: 6
Training loss: 2.6052358150482178
Validation loss: 2.1134636325220906

Epoch: 6| Step: 7
Training loss: 2.9200572967529297
Validation loss: 2.15206249554952

Epoch: 6| Step: 8
Training loss: 2.3064374923706055
Validation loss: 2.1102077986604426

Epoch: 6| Step: 9
Training loss: 2.6220664978027344
Validation loss: 2.128666413727627

Epoch: 6| Step: 10
Training loss: 1.6989848613739014
Validation loss: 2.109141274165082

Epoch: 6| Step: 11
Training loss: 1.4838778972625732
Validation loss: 2.1433288769055436

Epoch: 6| Step: 12
Training loss: 2.299485683441162
Validation loss: 2.1186294530027654

Epoch: 6| Step: 13
Training loss: 1.9431242942810059
Validation loss: 2.1239024618620514

Epoch: 71| Step: 0
Training loss: 2.9506969451904297
Validation loss: 2.13890080810875

Epoch: 6| Step: 1
Training loss: 2.609063148498535
Validation loss: 2.124201041395946

Epoch: 6| Step: 2
Training loss: 1.945885181427002
Validation loss: 2.136530578777354

Epoch: 6| Step: 3
Training loss: 2.0561182498931885
Validation loss: 2.099631678673529

Epoch: 6| Step: 4
Training loss: 2.046957492828369
Validation loss: 2.12961248941319

Epoch: 6| Step: 5
Training loss: 2.569749355316162
Validation loss: 2.094150374012609

Epoch: 6| Step: 6
Training loss: 2.402434825897217
Validation loss: 2.126412396789879

Epoch: 6| Step: 7
Training loss: 1.8157286643981934
Validation loss: 2.119399819322812

Epoch: 6| Step: 8
Training loss: 2.1908998489379883
Validation loss: 2.100823780541779

Epoch: 6| Step: 9
Training loss: 2.4601709842681885
Validation loss: 2.1122927358073573

Epoch: 6| Step: 10
Training loss: 1.329697847366333
Validation loss: 2.1353254292600896

Epoch: 6| Step: 11
Training loss: 1.0895217657089233
Validation loss: 2.130897798845845

Epoch: 6| Step: 12
Training loss: 3.009922504425049
Validation loss: 2.1258407715828187

Epoch: 6| Step: 13
Training loss: 3.335861921310425
Validation loss: 2.1083251583960747

Epoch: 72| Step: 0
Training loss: 2.1565022468566895
Validation loss: 2.142781775484803

Epoch: 6| Step: 1
Training loss: 1.8168718814849854
Validation loss: 2.1049816108519033

Epoch: 6| Step: 2
Training loss: 3.0985608100891113
Validation loss: 2.120100659708823

Epoch: 6| Step: 3
Training loss: 2.3185830116271973
Validation loss: 2.1019345586017897

Epoch: 6| Step: 4
Training loss: 1.7475143671035767
Validation loss: 2.096514448042839

Epoch: 6| Step: 5
Training loss: 1.8114190101623535
Validation loss: 2.104841160517867

Epoch: 6| Step: 6
Training loss: 1.9135315418243408
Validation loss: 2.1100355732825493

Epoch: 6| Step: 7
Training loss: 2.509854555130005
Validation loss: 2.100543519502045

Epoch: 6| Step: 8
Training loss: 2.022012233734131
Validation loss: 2.1037145660769556

Epoch: 6| Step: 9
Training loss: 2.461735725402832
Validation loss: 2.1100325199865524

Epoch: 6| Step: 10
Training loss: 2.115161657333374
Validation loss: 2.1021305002192014

Epoch: 6| Step: 11
Training loss: 2.248053550720215
Validation loss: 2.1391000657953243

Epoch: 6| Step: 12
Training loss: 3.1206319332122803
Validation loss: 2.115339914957682

Epoch: 6| Step: 13
Training loss: 1.4505131244659424
Validation loss: 2.0993910540816603

Epoch: 73| Step: 0
Training loss: 1.9511821269989014
Validation loss: 2.1159856857792025

Epoch: 6| Step: 1
Training loss: 1.567074179649353
Validation loss: 2.109773576900523

Epoch: 6| Step: 2
Training loss: 2.3688769340515137
Validation loss: 2.1015976577676754

Epoch: 6| Step: 3
Training loss: 2.9721100330352783
Validation loss: 2.094571272532145

Epoch: 6| Step: 4
Training loss: 2.2181782722473145
Validation loss: 2.1070911499761764

Epoch: 6| Step: 5
Training loss: 1.681654691696167
Validation loss: 2.106604758129325

Epoch: 6| Step: 6
Training loss: 2.5974926948547363
Validation loss: 2.0979014288994575

Epoch: 6| Step: 7
Training loss: 1.8863861560821533
Validation loss: 2.1177059014638266

Epoch: 6| Step: 8
Training loss: 2.636321544647217
Validation loss: 2.1063036687912478

Epoch: 6| Step: 9
Training loss: 1.5520020723342896
Validation loss: 2.1109979921771633

Epoch: 6| Step: 10
Training loss: 2.4381213188171387
Validation loss: 2.095154498213081

Epoch: 6| Step: 11
Training loss: 3.648071765899658
Validation loss: 2.0983213045263804

Epoch: 6| Step: 12
Training loss: 1.582349419593811
Validation loss: 2.1250966851429274

Epoch: 6| Step: 13
Training loss: 1.9011292457580566
Validation loss: 2.121900094452725

Epoch: 74| Step: 0
Training loss: 2.061408519744873
Validation loss: 2.116205535909181

Epoch: 6| Step: 1
Training loss: 2.034791946411133
Validation loss: 2.1059233732120965

Epoch: 6| Step: 2
Training loss: 1.9959495067596436
Validation loss: 2.1043016654188915

Epoch: 6| Step: 3
Training loss: 2.7065675258636475
Validation loss: 2.1024115880330405

Epoch: 6| Step: 4
Training loss: 2.277207136154175
Validation loss: 2.1062917465804727

Epoch: 6| Step: 5
Training loss: 1.918059229850769
Validation loss: 2.116316990185809

Epoch: 6| Step: 6
Training loss: 2.4228262901306152
Validation loss: 2.103164803597235

Epoch: 6| Step: 7
Training loss: 2.3543155193328857
Validation loss: 2.0971199902155067

Epoch: 6| Step: 8
Training loss: 2.16767954826355
Validation loss: 2.1268205617063787

Epoch: 6| Step: 9
Training loss: 1.8454113006591797
Validation loss: 2.1303318392845894

Epoch: 6| Step: 10
Training loss: 2.331110954284668
Validation loss: 2.1021011080793155

Epoch: 6| Step: 11
Training loss: 1.9141805171966553
Validation loss: 2.1299148195533344

Epoch: 6| Step: 12
Training loss: 2.5582292079925537
Validation loss: 2.1243355966383413

Epoch: 6| Step: 13
Training loss: 2.6568353176116943
Validation loss: 2.1031110684076944

Epoch: 75| Step: 0
Training loss: 2.0752859115600586
Validation loss: 2.087388002744285

Epoch: 6| Step: 1
Training loss: 2.453669548034668
Validation loss: 2.1019280392636537

Epoch: 6| Step: 2
Training loss: 1.6367563009262085
Validation loss: 2.1058849134752826

Epoch: 6| Step: 3
Training loss: 2.683840751647949
Validation loss: 2.094247064282817

Epoch: 6| Step: 4
Training loss: 1.3613333702087402
Validation loss: 2.121506235932791

Epoch: 6| Step: 5
Training loss: 1.9318218231201172
Validation loss: 2.106646789017544

Epoch: 6| Step: 6
Training loss: 2.3401050567626953
Validation loss: 2.1042006810506186

Epoch: 6| Step: 7
Training loss: 2.793248414993286
Validation loss: 2.139338311328683

Epoch: 6| Step: 8
Training loss: 2.5727436542510986
Validation loss: 2.116364902065646

Epoch: 6| Step: 9
Training loss: 2.0288565158843994
Validation loss: 2.1070468912842455

Epoch: 6| Step: 10
Training loss: 2.4892654418945312
Validation loss: 2.108877012806554

Epoch: 6| Step: 11
Training loss: 2.1965644359588623
Validation loss: 2.1271664763009674

Epoch: 6| Step: 12
Training loss: 2.458343029022217
Validation loss: 2.1050688323154243

Epoch: 6| Step: 13
Training loss: 2.490602731704712
Validation loss: 2.0991919130407353

Epoch: 76| Step: 0
Training loss: 2.8200809955596924
Validation loss: 2.118394531229491

Epoch: 6| Step: 1
Training loss: 2.4974489212036133
Validation loss: 2.0947297080870597

Epoch: 6| Step: 2
Training loss: 1.9820172786712646
Validation loss: 2.107956322290564

Epoch: 6| Step: 3
Training loss: 2.270777463912964
Validation loss: 2.122047178206905

Epoch: 6| Step: 4
Training loss: 2.41949200630188
Validation loss: 2.1116703069338234

Epoch: 6| Step: 5
Training loss: 2.854503631591797
Validation loss: 2.0957383930042224

Epoch: 6| Step: 6
Training loss: 2.379685878753662
Validation loss: 2.123527664010243

Epoch: 6| Step: 7
Training loss: 1.3215599060058594
Validation loss: 2.1015761283136185

Epoch: 6| Step: 8
Training loss: 1.9040024280548096
Validation loss: 2.0972049031206357

Epoch: 6| Step: 9
Training loss: 1.743620753288269
Validation loss: 2.101509186529344

Epoch: 6| Step: 10
Training loss: 2.189497947692871
Validation loss: 2.1014249376071397

Epoch: 6| Step: 11
Training loss: 1.9195269346237183
Validation loss: 2.0893835354876775

Epoch: 6| Step: 12
Training loss: 2.5519371032714844
Validation loss: 2.077588488978724

Epoch: 6| Step: 13
Training loss: 2.2447381019592285
Validation loss: 2.096157307265907

Epoch: 77| Step: 0
Training loss: 3.042523145675659
Validation loss: 2.0912523782381447

Epoch: 6| Step: 1
Training loss: 2.5796964168548584
Validation loss: 2.096480961768858

Epoch: 6| Step: 2
Training loss: 2.2212226390838623
Validation loss: 2.111030747813563

Epoch: 6| Step: 3
Training loss: 1.3732819557189941
Validation loss: 2.10451949796369

Epoch: 6| Step: 4
Training loss: 1.8612538576126099
Validation loss: 2.1156047146807433

Epoch: 6| Step: 5
Training loss: 1.9633440971374512
Validation loss: 2.092535113775602

Epoch: 6| Step: 6
Training loss: 1.3184683322906494
Validation loss: 2.103544286502305

Epoch: 6| Step: 7
Training loss: 2.8691492080688477
Validation loss: 2.1019937915186726

Epoch: 6| Step: 8
Training loss: 2.6929807662963867
Validation loss: 2.1034262846874934

Epoch: 6| Step: 9
Training loss: 1.5630059242248535
Validation loss: 2.0900859973763906

Epoch: 6| Step: 10
Training loss: 1.9822028875350952
Validation loss: 2.110634555098831

Epoch: 6| Step: 11
Training loss: 2.5892629623413086
Validation loss: 2.1066398748787503

Epoch: 6| Step: 12
Training loss: 2.8871078491210938
Validation loss: 2.0930614714981406

Epoch: 6| Step: 13
Training loss: 2.3111355304718018
Validation loss: 2.088353374952911

Epoch: 78| Step: 0
Training loss: 2.0672569274902344
Validation loss: 2.0932620917597125

Epoch: 6| Step: 1
Training loss: 2.5743627548217773
Validation loss: 2.1055260191681566

Epoch: 6| Step: 2
Training loss: 2.609281539916992
Validation loss: 2.1043732307290517

Epoch: 6| Step: 3
Training loss: 2.04069185256958
Validation loss: 2.1088056000330115

Epoch: 6| Step: 4
Training loss: 1.3885810375213623
Validation loss: 2.0910476971698064

Epoch: 6| Step: 5
Training loss: 1.7805386781692505
Validation loss: 2.1374954074941654

Epoch: 6| Step: 6
Training loss: 2.814600944519043
Validation loss: 2.1403265153208086

Epoch: 6| Step: 7
Training loss: 2.1042797565460205
Validation loss: 2.1173531445123817

Epoch: 6| Step: 8
Training loss: 1.8111118078231812
Validation loss: 2.118187609539237

Epoch: 6| Step: 9
Training loss: 2.491400718688965
Validation loss: 2.1311546576920377

Epoch: 6| Step: 10
Training loss: 2.7937207221984863
Validation loss: 2.140546288541568

Epoch: 6| Step: 11
Training loss: 2.2442855834960938
Validation loss: 2.131010624670213

Epoch: 6| Step: 12
Training loss: 1.757979154586792
Validation loss: 2.1154119994050715

Epoch: 6| Step: 13
Training loss: 2.500941753387451
Validation loss: 2.1088853292567755

Epoch: 79| Step: 0
Training loss: 2.2785730361938477
Validation loss: 2.0862811611544703

Epoch: 6| Step: 1
Training loss: 2.3956799507141113
Validation loss: 2.088813363864858

Epoch: 6| Step: 2
Training loss: 2.0685219764709473
Validation loss: 2.118021370262228

Epoch: 6| Step: 3
Training loss: 2.3864150047302246
Validation loss: 2.1074757922080254

Epoch: 6| Step: 4
Training loss: 2.529782295227051
Validation loss: 2.1059074299309843

Epoch: 6| Step: 5
Training loss: 2.4271883964538574
Validation loss: 2.1263894983517226

Epoch: 6| Step: 6
Training loss: 2.7937183380126953
Validation loss: 2.1030608005421136

Epoch: 6| Step: 7
Training loss: 2.3998141288757324
Validation loss: 2.129950887413435

Epoch: 6| Step: 8
Training loss: 2.5584263801574707
Validation loss: 2.1408248050238496

Epoch: 6| Step: 9
Training loss: 1.9876713752746582
Validation loss: 2.120019474337178

Epoch: 6| Step: 10
Training loss: 0.9178078770637512
Validation loss: 2.1183404537939254

Epoch: 6| Step: 11
Training loss: 1.6091712713241577
Validation loss: 2.1128105796793455

Epoch: 6| Step: 12
Training loss: 2.1602940559387207
Validation loss: 2.118240705100439

Epoch: 6| Step: 13
Training loss: 2.7005414962768555
Validation loss: 2.11744531508415

Epoch: 80| Step: 0
Training loss: 2.379418134689331
Validation loss: 2.107732231898974

Epoch: 6| Step: 1
Training loss: 1.9844350814819336
Validation loss: 2.0983735194770237

Epoch: 6| Step: 2
Training loss: 1.7930757999420166
Validation loss: 2.1195129527840564

Epoch: 6| Step: 3
Training loss: 1.5707042217254639
Validation loss: 2.126106533952939

Epoch: 6| Step: 4
Training loss: 2.585566997528076
Validation loss: 2.1131797016307874

Epoch: 6| Step: 5
Training loss: 2.5049452781677246
Validation loss: 2.114042235958961

Epoch: 6| Step: 6
Training loss: 2.952082872390747
Validation loss: 2.1066522905903478

Epoch: 6| Step: 7
Training loss: 1.844714641571045
Validation loss: 2.1065274041186095

Epoch: 6| Step: 8
Training loss: 2.2279305458068848
Validation loss: 2.1051741441090903

Epoch: 6| Step: 9
Training loss: 2.5681464672088623
Validation loss: 2.106585894861529

Epoch: 6| Step: 10
Training loss: 2.375953197479248
Validation loss: 2.0912126905174664

Epoch: 6| Step: 11
Training loss: 2.302330732345581
Validation loss: 2.0963291711704706

Epoch: 6| Step: 12
Training loss: 2.00589656829834
Validation loss: 2.1132482777359667

Epoch: 6| Step: 13
Training loss: 1.4847626686096191
Validation loss: 2.0925607886365665

Epoch: 81| Step: 0
Training loss: 1.9858143329620361
Validation loss: 2.0827140321013746

Epoch: 6| Step: 1
Training loss: 2.5105512142181396
Validation loss: 2.1077859452975694

Epoch: 6| Step: 2
Training loss: 1.7294840812683105
Validation loss: 2.109275020578856

Epoch: 6| Step: 3
Training loss: 3.0765814781188965
Validation loss: 2.0804543700269473

Epoch: 6| Step: 4
Training loss: 1.8799469470977783
Validation loss: 2.084015944952606

Epoch: 6| Step: 5
Training loss: 2.2953062057495117
Validation loss: 2.1141140845514115

Epoch: 6| Step: 6
Training loss: 1.880591869354248
Validation loss: 2.092709702830161

Epoch: 6| Step: 7
Training loss: 1.7423714399337769
Validation loss: 2.0834873004626204

Epoch: 6| Step: 8
Training loss: 2.1298398971557617
Validation loss: 2.076074024682404

Epoch: 6| Step: 9
Training loss: 2.0932459831237793
Validation loss: 2.1217813530275897

Epoch: 6| Step: 10
Training loss: 2.865670919418335
Validation loss: 2.121369009376854

Epoch: 6| Step: 11
Training loss: 2.1982359886169434
Validation loss: 2.1096293695511354

Epoch: 6| Step: 12
Training loss: 2.254605293273926
Validation loss: 2.104679956231066

Epoch: 6| Step: 13
Training loss: 2.243868827819824
Validation loss: 2.096425951168101

Epoch: 82| Step: 0
Training loss: 2.4716012477874756
Validation loss: 2.103012460534291

Epoch: 6| Step: 1
Training loss: 1.979781150817871
Validation loss: 2.125687099272205

Epoch: 6| Step: 2
Training loss: 1.6723382472991943
Validation loss: 2.0906241734822593

Epoch: 6| Step: 3
Training loss: 2.154771327972412
Validation loss: 2.0923428945643927

Epoch: 6| Step: 4
Training loss: 2.682244062423706
Validation loss: 2.098829605246103

Epoch: 6| Step: 5
Training loss: 2.1440606117248535
Validation loss: 2.126107867046069

Epoch: 6| Step: 6
Training loss: 2.2477076053619385
Validation loss: 2.100728693828788

Epoch: 6| Step: 7
Training loss: 1.813977599143982
Validation loss: 2.1077772237921275

Epoch: 6| Step: 8
Training loss: 2.7707014083862305
Validation loss: 2.117003051183557

Epoch: 6| Step: 9
Training loss: 2.0065388679504395
Validation loss: 2.112227601389731

Epoch: 6| Step: 10
Training loss: 2.2592663764953613
Validation loss: 2.109074273417073

Epoch: 6| Step: 11
Training loss: 2.0661935806274414
Validation loss: 2.1085477080396426

Epoch: 6| Step: 12
Training loss: 2.076502799987793
Validation loss: 2.0877070144940446

Epoch: 6| Step: 13
Training loss: 2.6190922260284424
Validation loss: 2.103487237807243

Epoch: 83| Step: 0
Training loss: 2.1284356117248535
Validation loss: 2.136134050225699

Epoch: 6| Step: 1
Training loss: 2.3917951583862305
Validation loss: 2.1100435282594416

Epoch: 6| Step: 2
Training loss: 2.4733381271362305
Validation loss: 2.09409858078085

Epoch: 6| Step: 3
Training loss: 2.062819004058838
Validation loss: 2.109594697593361

Epoch: 6| Step: 4
Training loss: 1.6322338581085205
Validation loss: 2.1023959728979293

Epoch: 6| Step: 5
Training loss: 1.877874493598938
Validation loss: 2.0859700723360945

Epoch: 6| Step: 6
Training loss: 2.911593437194824
Validation loss: 2.0716492693911315

Epoch: 6| Step: 7
Training loss: 1.7954189777374268
Validation loss: 2.1022105755344516

Epoch: 6| Step: 8
Training loss: 2.1808791160583496
Validation loss: 2.1045130657893356

Epoch: 6| Step: 9
Training loss: 2.3659780025482178
Validation loss: 2.10876973213688

Epoch: 6| Step: 10
Training loss: 1.7901058197021484
Validation loss: 2.0927168656420965

Epoch: 6| Step: 11
Training loss: 2.5514392852783203
Validation loss: 2.117729212648125

Epoch: 6| Step: 12
Training loss: 2.3963751792907715
Validation loss: 2.108487521448443

Epoch: 6| Step: 13
Training loss: 2.454376459121704
Validation loss: 2.1052148252405147

Epoch: 84| Step: 0
Training loss: 1.967913031578064
Validation loss: 2.105836861877031

Epoch: 6| Step: 1
Training loss: 2.338991641998291
Validation loss: 2.122431689693082

Epoch: 6| Step: 2
Training loss: 2.5146191120147705
Validation loss: 2.098702758871099

Epoch: 6| Step: 3
Training loss: 2.6255273818969727
Validation loss: 2.111062167793192

Epoch: 6| Step: 4
Training loss: 2.2696101665496826
Validation loss: 2.0887832718510784

Epoch: 6| Step: 5
Training loss: 1.764068603515625
Validation loss: 2.1027360513646114

Epoch: 6| Step: 6
Training loss: 1.5053761005401611
Validation loss: 2.1029894480141262

Epoch: 6| Step: 7
Training loss: 2.5023233890533447
Validation loss: 2.096527358537079

Epoch: 6| Step: 8
Training loss: 2.3319239616394043
Validation loss: 2.105494080051299

Epoch: 6| Step: 9
Training loss: 2.050947666168213
Validation loss: 2.093699751361724

Epoch: 6| Step: 10
Training loss: 2.360928535461426
Validation loss: 2.1068003075097197

Epoch: 6| Step: 11
Training loss: 2.0599234104156494
Validation loss: 2.0832437969023183

Epoch: 6| Step: 12
Training loss: 1.6699388027191162
Validation loss: 2.0963937967054305

Epoch: 6| Step: 13
Training loss: 3.261366844177246
Validation loss: 2.107874519081526

Epoch: 85| Step: 0
Training loss: 1.8344557285308838
Validation loss: 2.117469569688202

Epoch: 6| Step: 1
Training loss: 2.324016571044922
Validation loss: 2.105295976003011

Epoch: 6| Step: 2
Training loss: 2.102710247039795
Validation loss: 2.134771218863867

Epoch: 6| Step: 3
Training loss: 2.1826813220977783
Validation loss: 2.1050913462074856

Epoch: 6| Step: 4
Training loss: 2.102011203765869
Validation loss: 2.108076331435993

Epoch: 6| Step: 5
Training loss: 2.976175308227539
Validation loss: 2.1247478172343266

Epoch: 6| Step: 6
Training loss: 2.5800156593322754
Validation loss: 2.105895009092105

Epoch: 6| Step: 7
Training loss: 1.936621904373169
Validation loss: 2.0967007016622894

Epoch: 6| Step: 8
Training loss: 1.69181489944458
Validation loss: 2.1001541435077624

Epoch: 6| Step: 9
Training loss: 1.9297256469726562
Validation loss: 2.1007203901967695

Epoch: 6| Step: 10
Training loss: 2.0734195709228516
Validation loss: 2.1066995846327914

Epoch: 6| Step: 11
Training loss: 2.685944080352783
Validation loss: 2.0924820489780878

Epoch: 6| Step: 12
Training loss: 2.077117443084717
Validation loss: 2.118044650682839

Epoch: 6| Step: 13
Training loss: 2.4447085857391357
Validation loss: 2.1203210225669284

Epoch: 86| Step: 0
Training loss: 2.3634250164031982
Validation loss: 2.118489116750738

Epoch: 6| Step: 1
Training loss: 2.00833797454834
Validation loss: 2.1139562604247883

Epoch: 6| Step: 2
Training loss: 2.887782573699951
Validation loss: 2.1036093260652278

Epoch: 6| Step: 3
Training loss: 2.08681058883667
Validation loss: 2.1232940509755123

Epoch: 6| Step: 4
Training loss: 2.0010826587677
Validation loss: 2.102465424486386

Epoch: 6| Step: 5
Training loss: 2.0844967365264893
Validation loss: 2.127600646788074

Epoch: 6| Step: 6
Training loss: 1.4444258213043213
Validation loss: 2.1137993489542315

Epoch: 6| Step: 7
Training loss: 2.195343017578125
Validation loss: 2.073773791713099

Epoch: 6| Step: 8
Training loss: 2.509117364883423
Validation loss: 2.1043528010768275

Epoch: 6| Step: 9
Training loss: 2.0395612716674805
Validation loss: 2.1025497426268873

Epoch: 6| Step: 10
Training loss: 2.2530312538146973
Validation loss: 2.0954073423980386

Epoch: 6| Step: 11
Training loss: 2.3367691040039062
Validation loss: 2.107385047020451

Epoch: 6| Step: 12
Training loss: 2.124006509780884
Validation loss: 2.1084443843492897

Epoch: 6| Step: 13
Training loss: 2.6557912826538086
Validation loss: 2.109862983867686

Epoch: 87| Step: 0
Training loss: 2.9033524990081787
Validation loss: 2.0961103054784958

Epoch: 6| Step: 1
Training loss: 2.6124205589294434
Validation loss: 2.1223966921529462

Epoch: 6| Step: 2
Training loss: 2.5024070739746094
Validation loss: 2.088207342291391

Epoch: 6| Step: 3
Training loss: 1.9042994976043701
Validation loss: 2.110753991270578

Epoch: 6| Step: 4
Training loss: 1.9492762088775635
Validation loss: 2.0860263814208326

Epoch: 6| Step: 5
Training loss: 2.301893949508667
Validation loss: 2.09628612508056

Epoch: 6| Step: 6
Training loss: 1.819440484046936
Validation loss: 2.0985067057353195

Epoch: 6| Step: 7
Training loss: 1.6965454816818237
Validation loss: 2.1009705246135755

Epoch: 6| Step: 8
Training loss: 2.4886693954467773
Validation loss: 2.099956004850326

Epoch: 6| Step: 9
Training loss: 2.2506680488586426
Validation loss: 2.09094956485174

Epoch: 6| Step: 10
Training loss: 2.304685592651367
Validation loss: 2.096165664734379

Epoch: 6| Step: 11
Training loss: 1.8609501123428345
Validation loss: 2.0874719094204646

Epoch: 6| Step: 12
Training loss: 2.065180778503418
Validation loss: 2.100736828260524

Epoch: 6| Step: 13
Training loss: 1.9110732078552246
Validation loss: 2.098606854356745

Epoch: 88| Step: 0
Training loss: 2.697965621948242
Validation loss: 2.1465216951985515

Epoch: 6| Step: 1
Training loss: 1.923543930053711
Validation loss: 2.1042288951976325

Epoch: 6| Step: 2
Training loss: 2.6562681198120117
Validation loss: 2.0787222949407433

Epoch: 6| Step: 3
Training loss: 2.087280035018921
Validation loss: 2.1113195060401835

Epoch: 6| Step: 4
Training loss: 2.4984631538391113
Validation loss: 2.088698176927464

Epoch: 6| Step: 5
Training loss: 2.6446967124938965
Validation loss: 2.1169088091901553

Epoch: 6| Step: 6
Training loss: 2.188382625579834
Validation loss: 2.0812807659949026

Epoch: 6| Step: 7
Training loss: 1.987534523010254
Validation loss: 2.1103030712373796

Epoch: 6| Step: 8
Training loss: 1.7450042963027954
Validation loss: 2.084528455170252

Epoch: 6| Step: 9
Training loss: 2.1083972454071045
Validation loss: 2.096718152364095

Epoch: 6| Step: 10
Training loss: 2.0216164588928223
Validation loss: 2.0928470139862387

Epoch: 6| Step: 11
Training loss: 1.8913400173187256
Validation loss: 2.1106019378990255

Epoch: 6| Step: 12
Training loss: 2.1450862884521484
Validation loss: 2.093791878351601

Epoch: 6| Step: 13
Training loss: 1.937691569328308
Validation loss: 2.112941201015185

Epoch: 89| Step: 0
Training loss: 1.8757688999176025
Validation loss: 2.1005437245932956

Epoch: 6| Step: 1
Training loss: 1.6554079055786133
Validation loss: 2.1055523913393737

Epoch: 6| Step: 2
Training loss: 1.9144872426986694
Validation loss: 2.0675500259604505

Epoch: 6| Step: 3
Training loss: 1.9629077911376953
Validation loss: 2.0924244209002425

Epoch: 6| Step: 4
Training loss: 2.066800117492676
Validation loss: 2.0987679317433345

Epoch: 6| Step: 5
Training loss: 2.1857476234436035
Validation loss: 2.1103628425187964

Epoch: 6| Step: 6
Training loss: 2.070927381515503
Validation loss: 2.1134158718970513

Epoch: 6| Step: 7
Training loss: 3.172224521636963
Validation loss: 2.092781556549893

Epoch: 6| Step: 8
Training loss: 1.8994470834732056
Validation loss: 2.105427174157994

Epoch: 6| Step: 9
Training loss: 3.208559036254883
Validation loss: 2.101473149432931

Epoch: 6| Step: 10
Training loss: 1.6511621475219727
Validation loss: 2.10310963917804

Epoch: 6| Step: 11
Training loss: 2.2843146324157715
Validation loss: 2.1073972768681024

Epoch: 6| Step: 12
Training loss: 2.2418031692504883
Validation loss: 2.111034401001469

Epoch: 6| Step: 13
Training loss: 2.898749828338623
Validation loss: 2.084194947314519

Epoch: 90| Step: 0
Training loss: 2.4058244228363037
Validation loss: 2.116899956939041

Epoch: 6| Step: 1
Training loss: 1.8787485361099243
Validation loss: 2.1250269028448288

Epoch: 6| Step: 2
Training loss: 2.7844655513763428
Validation loss: 2.1075966973458566

Epoch: 6| Step: 3
Training loss: 1.62620210647583
Validation loss: 2.1119952868389826

Epoch: 6| Step: 4
Training loss: 2.0670464038848877
Validation loss: 2.1212452701343003

Epoch: 6| Step: 5
Training loss: 2.7976157665252686
Validation loss: 2.1063024613165084

Epoch: 6| Step: 6
Training loss: 1.2383208274841309
Validation loss: 2.1218216162855907

Epoch: 6| Step: 7
Training loss: 1.4685311317443848
Validation loss: 2.0905390683040825

Epoch: 6| Step: 8
Training loss: 2.709517478942871
Validation loss: 2.0863017112978044

Epoch: 6| Step: 9
Training loss: 2.4939017295837402
Validation loss: 2.111207357016943

Epoch: 6| Step: 10
Training loss: 2.7169132232666016
Validation loss: 2.1378468467343237

Epoch: 6| Step: 11
Training loss: 2.2186336517333984
Validation loss: 2.125120691073838

Epoch: 6| Step: 12
Training loss: 1.9994865655899048
Validation loss: 2.1189454883657475

Epoch: 6| Step: 13
Training loss: 2.0969743728637695
Validation loss: 2.1240140084297425

Epoch: 91| Step: 0
Training loss: 1.8693251609802246
Validation loss: 2.089184723874574

Epoch: 6| Step: 1
Training loss: 1.6246378421783447
Validation loss: 2.0732159973472677

Epoch: 6| Step: 2
Training loss: 2.1494946479797363
Validation loss: 2.1123814890461583

Epoch: 6| Step: 3
Training loss: 2.260496139526367
Validation loss: 2.087426999563812

Epoch: 6| Step: 4
Training loss: 2.138197183609009
Validation loss: 2.1155058312159714

Epoch: 6| Step: 5
Training loss: 1.5236629247665405
Validation loss: 2.128763706453385

Epoch: 6| Step: 6
Training loss: 2.814297914505005
Validation loss: 2.119525417204826

Epoch: 6| Step: 7
Training loss: 2.152134656906128
Validation loss: 2.0937669969374135

Epoch: 6| Step: 8
Training loss: 2.1088905334472656
Validation loss: 2.120376458732031

Epoch: 6| Step: 9
Training loss: 2.7002639770507812
Validation loss: 2.1162736595317884

Epoch: 6| Step: 10
Training loss: 2.447330951690674
Validation loss: 2.1062350888406076

Epoch: 6| Step: 11
Training loss: 2.1218626499176025
Validation loss: 2.1147077109224055

Epoch: 6| Step: 12
Training loss: 2.5064358711242676
Validation loss: 2.108145498460339

Epoch: 6| Step: 13
Training loss: 2.1334123611450195
Validation loss: 2.1130548164408696

Epoch: 92| Step: 0
Training loss: 2.072266101837158
Validation loss: 2.09664926990386

Epoch: 6| Step: 1
Training loss: 2.349304437637329
Validation loss: 2.0929588899817517

Epoch: 6| Step: 2
Training loss: 1.9174305200576782
Validation loss: 2.10053059106232

Epoch: 6| Step: 3
Training loss: 1.870516061782837
Validation loss: 2.112808753085393

Epoch: 6| Step: 4
Training loss: 1.7893147468566895
Validation loss: 2.1012506536258164

Epoch: 6| Step: 5
Training loss: 1.7746703624725342
Validation loss: 2.094545651507634

Epoch: 6| Step: 6
Training loss: 1.7132817506790161
Validation loss: 2.1066357345991236

Epoch: 6| Step: 7
Training loss: 2.406726360321045
Validation loss: 2.104187055300641

Epoch: 6| Step: 8
Training loss: 2.616476535797119
Validation loss: 2.1007927694628314

Epoch: 6| Step: 9
Training loss: 2.0115761756896973
Validation loss: 2.1055218609430457

Epoch: 6| Step: 10
Training loss: 2.5632543563842773
Validation loss: 2.104328881027878

Epoch: 6| Step: 11
Training loss: 1.9821170568466187
Validation loss: 2.117739804329411

Epoch: 6| Step: 12
Training loss: 2.350404977798462
Validation loss: 2.09271062830443

Epoch: 6| Step: 13
Training loss: 3.461627960205078
Validation loss: 2.09711697793776

Epoch: 93| Step: 0
Training loss: 1.7175980806350708
Validation loss: 2.0939455109257854

Epoch: 6| Step: 1
Training loss: 2.7975516319274902
Validation loss: 2.1017971346455235

Epoch: 6| Step: 2
Training loss: 1.889815330505371
Validation loss: 2.1066099289924867

Epoch: 6| Step: 3
Training loss: 2.0026562213897705
Validation loss: 2.1086172621737242

Epoch: 6| Step: 4
Training loss: 2.4982123374938965
Validation loss: 2.1077149709065757

Epoch: 6| Step: 5
Training loss: 3.0226545333862305
Validation loss: 2.0843028253124607

Epoch: 6| Step: 6
Training loss: 1.956918478012085
Validation loss: 2.0917792397160686

Epoch: 6| Step: 7
Training loss: 2.164370536804199
Validation loss: 2.0940234379101823

Epoch: 6| Step: 8
Training loss: 1.682031273841858
Validation loss: 2.1165888642752044

Epoch: 6| Step: 9
Training loss: 2.2886099815368652
Validation loss: 2.112292975507757

Epoch: 6| Step: 10
Training loss: 2.218411922454834
Validation loss: 2.128696281422851

Epoch: 6| Step: 11
Training loss: 1.583073377609253
Validation loss: 2.0917690915446125

Epoch: 6| Step: 12
Training loss: 2.1016082763671875
Validation loss: 2.108607740812404

Epoch: 6| Step: 13
Training loss: 2.620710611343384
Validation loss: 2.094453388644803

Epoch: 94| Step: 0
Training loss: 2.860750198364258
Validation loss: 2.13392577632781

Epoch: 6| Step: 1
Training loss: 2.2758584022521973
Validation loss: 2.082550371846845

Epoch: 6| Step: 2
Training loss: 3.144690990447998
Validation loss: 2.089056791797761

Epoch: 6| Step: 3
Training loss: 2.2397446632385254
Validation loss: 2.125828234098291

Epoch: 6| Step: 4
Training loss: 1.8017463684082031
Validation loss: 2.1129944952585364

Epoch: 6| Step: 5
Training loss: 1.352802038192749
Validation loss: 2.097188707320921

Epoch: 6| Step: 6
Training loss: 2.2288875579833984
Validation loss: 2.0991841670005553

Epoch: 6| Step: 7
Training loss: 2.4232656955718994
Validation loss: 2.0916165203176518

Epoch: 6| Step: 8
Training loss: 2.1790242195129395
Validation loss: 2.1165954656498407

Epoch: 6| Step: 9
Training loss: 1.7961769104003906
Validation loss: 2.1159295074401365

Epoch: 6| Step: 10
Training loss: 1.5997874736785889
Validation loss: 2.0985023770281064

Epoch: 6| Step: 11
Training loss: 1.5636944770812988
Validation loss: 2.113446112601988

Epoch: 6| Step: 12
Training loss: 2.6893224716186523
Validation loss: 2.120947020028227

Epoch: 6| Step: 13
Training loss: 2.330948829650879
Validation loss: 2.0944680424146753

Epoch: 95| Step: 0
Training loss: 2.427661895751953
Validation loss: 2.107499417438302

Epoch: 6| Step: 1
Training loss: 1.6741728782653809
Validation loss: 2.091538643324247

Epoch: 6| Step: 2
Training loss: 1.9950041770935059
Validation loss: 2.1332337817838116

Epoch: 6| Step: 3
Training loss: 2.1120448112487793
Validation loss: 2.0941326515648955

Epoch: 6| Step: 4
Training loss: 2.4963741302490234
Validation loss: 2.101321804908014

Epoch: 6| Step: 5
Training loss: 2.1795785427093506
Validation loss: 2.094986645124292

Epoch: 6| Step: 6
Training loss: 2.3957321643829346
Validation loss: 2.1093215916746404

Epoch: 6| Step: 7
Training loss: 2.177281618118286
Validation loss: 2.1210601073439403

Epoch: 6| Step: 8
Training loss: 1.689058542251587
Validation loss: 2.117396849457936

Epoch: 6| Step: 9
Training loss: 2.731562852859497
Validation loss: 2.084803371019261

Epoch: 6| Step: 10
Training loss: 1.6272804737091064
Validation loss: 2.104289911126578

Epoch: 6| Step: 11
Training loss: 2.3792219161987305
Validation loss: 2.1132212044090353

Epoch: 6| Step: 12
Training loss: 2.553785562515259
Validation loss: 2.1147790262776036

Epoch: 6| Step: 13
Training loss: 2.1348392963409424
Validation loss: 2.1201101246700493

Epoch: 96| Step: 0
Training loss: 2.1075921058654785
Validation loss: 2.0830661558335826

Epoch: 6| Step: 1
Training loss: 1.9513942003250122
Validation loss: 2.102934939886934

Epoch: 6| Step: 2
Training loss: 2.0282952785491943
Validation loss: 2.0956131232682096

Epoch: 6| Step: 3
Training loss: 2.4635515213012695
Validation loss: 2.103376829495994

Epoch: 6| Step: 4
Training loss: 2.273204803466797
Validation loss: 2.1072169324403167

Epoch: 6| Step: 5
Training loss: 1.8636982440948486
Validation loss: 2.0797772753623223

Epoch: 6| Step: 6
Training loss: 2.236863136291504
Validation loss: 2.1007259661151516

Epoch: 6| Step: 7
Training loss: 2.4809956550598145
Validation loss: 2.0875146722280853

Epoch: 6| Step: 8
Training loss: 2.1419854164123535
Validation loss: 2.1133058353136946

Epoch: 6| Step: 9
Training loss: 3.3295583724975586
Validation loss: 2.0901387891461773

Epoch: 6| Step: 10
Training loss: 1.6512260437011719
Validation loss: 2.123423073881416

Epoch: 6| Step: 11
Training loss: 2.099843978881836
Validation loss: 2.1049251761487735

Epoch: 6| Step: 12
Training loss: 1.7649033069610596
Validation loss: 2.12154862957616

Epoch: 6| Step: 13
Training loss: 2.047212839126587
Validation loss: 2.0952734806204356

Epoch: 97| Step: 0
Training loss: 1.9197626113891602
Validation loss: 2.095470138775405

Epoch: 6| Step: 1
Training loss: 2.3897621631622314
Validation loss: 2.119997734664589

Epoch: 6| Step: 2
Training loss: 2.545581817626953
Validation loss: 2.0935231229310394

Epoch: 6| Step: 3
Training loss: 2.113678455352783
Validation loss: 2.1186743987503873

Epoch: 6| Step: 4
Training loss: 2.6201555728912354
Validation loss: 2.1189752445426038

Epoch: 6| Step: 5
Training loss: 1.7896900177001953
Validation loss: 2.0975552117952736

Epoch: 6| Step: 6
Training loss: 1.9234473705291748
Validation loss: 2.1228347747556624

Epoch: 6| Step: 7
Training loss: 1.779737949371338
Validation loss: 2.106695680208104

Epoch: 6| Step: 8
Training loss: 2.420161008834839
Validation loss: 2.1162560152751144

Epoch: 6| Step: 9
Training loss: 2.2019896507263184
Validation loss: 2.11303569809083

Epoch: 6| Step: 10
Training loss: 2.0777597427368164
Validation loss: 2.117704660661759

Epoch: 6| Step: 11
Training loss: 2.542489528656006
Validation loss: 2.0953896814777004

Epoch: 6| Step: 12
Training loss: 2.10693097114563
Validation loss: 2.123262889923588

Epoch: 6| Step: 13
Training loss: 1.6764488220214844
Validation loss: 2.1187573120158207

Epoch: 98| Step: 0
Training loss: 1.8162388801574707
Validation loss: 2.1084354846708235

Epoch: 6| Step: 1
Training loss: 2.3030660152435303
Validation loss: 2.1297486930765133

Epoch: 6| Step: 2
Training loss: 2.021515130996704
Validation loss: 2.1185664079522573

Epoch: 6| Step: 3
Training loss: 2.1850688457489014
Validation loss: 2.116363286972046

Epoch: 6| Step: 4
Training loss: 2.0729012489318848
Validation loss: 2.1208355593424972

Epoch: 6| Step: 5
Training loss: 1.4375396966934204
Validation loss: 2.1173753148765972

Epoch: 6| Step: 6
Training loss: 2.6301913261413574
Validation loss: 2.0700401439461658

Epoch: 6| Step: 7
Training loss: 2.203970193862915
Validation loss: 2.102816306134706

Epoch: 6| Step: 8
Training loss: 1.5409550666809082
Validation loss: 2.093925283801171

Epoch: 6| Step: 9
Training loss: 2.147224187850952
Validation loss: 2.106825053050954

Epoch: 6| Step: 10
Training loss: 2.293464183807373
Validation loss: 2.084136260453091

Epoch: 6| Step: 11
Training loss: 2.581909418106079
Validation loss: 2.0937285115641933

Epoch: 6| Step: 12
Training loss: 2.3974170684814453
Validation loss: 2.10669635188195

Epoch: 6| Step: 13
Training loss: 2.6434011459350586
Validation loss: 2.0903926459691857

Epoch: 99| Step: 0
Training loss: 2.3482296466827393
Validation loss: 2.1015198256379817

Epoch: 6| Step: 1
Training loss: 1.9446173906326294
Validation loss: 2.1123908245435326

Epoch: 6| Step: 2
Training loss: 1.8303420543670654
Validation loss: 2.0928296735209804

Epoch: 6| Step: 3
Training loss: 2.210557460784912
Validation loss: 2.1165849444686726

Epoch: 6| Step: 4
Training loss: 1.602484107017517
Validation loss: 2.113726900469872

Epoch: 6| Step: 5
Training loss: 2.102874755859375
Validation loss: 2.102036614571848

Epoch: 6| Step: 6
Training loss: 2.3813390731811523
Validation loss: 2.0840591435791342

Epoch: 6| Step: 7
Training loss: 2.000335693359375
Validation loss: 2.1029695336536696

Epoch: 6| Step: 8
Training loss: 3.078371047973633
Validation loss: 2.1048769694502636

Epoch: 6| Step: 9
Training loss: 2.141782522201538
Validation loss: 2.0958206063957623

Epoch: 6| Step: 10
Training loss: 2.025541305541992
Validation loss: 2.0774096929898827

Epoch: 6| Step: 11
Training loss: 2.485917329788208
Validation loss: 2.1003500569251274

Epoch: 6| Step: 12
Training loss: 2.3420908451080322
Validation loss: 2.068509824814335

Epoch: 6| Step: 13
Training loss: 2.131963014602661
Validation loss: 2.082978561360349

Epoch: 100| Step: 0
Training loss: 1.9247210025787354
Validation loss: 2.0766936989240747

Epoch: 6| Step: 1
Training loss: 1.9730231761932373
Validation loss: 2.1226905776608374

Epoch: 6| Step: 2
Training loss: 2.5673389434814453
Validation loss: 2.1002987430941675

Epoch: 6| Step: 3
Training loss: 2.054769992828369
Validation loss: 2.08545623799806

Epoch: 6| Step: 4
Training loss: 1.7830479145050049
Validation loss: 2.0875740897270942

Epoch: 6| Step: 5
Training loss: 2.849478244781494
Validation loss: 2.0910829062102945

Epoch: 6| Step: 6
Training loss: 1.4881675243377686
Validation loss: 2.098586497768279

Epoch: 6| Step: 7
Training loss: 2.3002352714538574
Validation loss: 2.1191661511698077

Epoch: 6| Step: 8
Training loss: 1.8023037910461426
Validation loss: 2.117616593196828

Epoch: 6| Step: 9
Training loss: 2.1903128623962402
Validation loss: 2.0982196869388705

Epoch: 6| Step: 10
Training loss: 1.9952569007873535
Validation loss: 2.0930205493844967

Epoch: 6| Step: 11
Training loss: 2.7302422523498535
Validation loss: 2.113225938171469

Epoch: 6| Step: 12
Training loss: 2.8579204082489014
Validation loss: 2.0799033936633857

Epoch: 6| Step: 13
Training loss: 1.6746419668197632
Validation loss: 2.101528993216894

Epoch: 101| Step: 0
Training loss: 2.395522117614746
Validation loss: 2.1101065476735434

Epoch: 6| Step: 1
Training loss: 1.5774235725402832
Validation loss: 2.076098597177895

Epoch: 6| Step: 2
Training loss: 2.579984188079834
Validation loss: 2.104296745792512

Epoch: 6| Step: 3
Training loss: 2.306070327758789
Validation loss: 2.0716136937500327

Epoch: 6| Step: 4
Training loss: 2.1798293590545654
Validation loss: 2.103783425464425

Epoch: 6| Step: 5
Training loss: 2.577582836151123
Validation loss: 2.101043467880577

Epoch: 6| Step: 6
Training loss: 1.9898914098739624
Validation loss: 2.0953490503372683

Epoch: 6| Step: 7
Training loss: 2.4269485473632812
Validation loss: 2.0949217965525966

Epoch: 6| Step: 8
Training loss: 2.224766254425049
Validation loss: 2.080388076843754

Epoch: 6| Step: 9
Training loss: 1.8355345726013184
Validation loss: 2.1319224424259637

Epoch: 6| Step: 10
Training loss: 1.4303436279296875
Validation loss: 2.124947855549474

Epoch: 6| Step: 11
Training loss: 1.8392202854156494
Validation loss: 2.092355091084716

Epoch: 6| Step: 12
Training loss: 2.89422607421875
Validation loss: 2.1024419364108833

Epoch: 6| Step: 13
Training loss: 1.9018563032150269
Validation loss: 2.097176497982394

Epoch: 102| Step: 0
Training loss: 2.963242292404175
Validation loss: 2.1152470688666067

Epoch: 6| Step: 1
Training loss: 1.7979021072387695
Validation loss: 2.1014534632364907

Epoch: 6| Step: 2
Training loss: 1.7381795644760132
Validation loss: 2.0901742519870883

Epoch: 6| Step: 3
Training loss: 1.859786033630371
Validation loss: 2.1041807564355994

Epoch: 6| Step: 4
Training loss: 1.9178214073181152
Validation loss: 2.1100762531321537

Epoch: 6| Step: 5
Training loss: 3.025775909423828
Validation loss: 2.112821873798165

Epoch: 6| Step: 6
Training loss: 2.161813259124756
Validation loss: 2.120926818540019

Epoch: 6| Step: 7
Training loss: 2.2903547286987305
Validation loss: 2.114276228412505

Epoch: 6| Step: 8
Training loss: 1.891446828842163
Validation loss: 2.1069003920401297

Epoch: 6| Step: 9
Training loss: 2.59445858001709
Validation loss: 2.089009032454542

Epoch: 6| Step: 10
Training loss: 1.7529925107955933
Validation loss: 2.0876070376365417

Epoch: 6| Step: 11
Training loss: 2.368406057357788
Validation loss: 2.1007516127760693

Epoch: 6| Step: 12
Training loss: 1.5468751192092896
Validation loss: 2.101505047531538

Epoch: 6| Step: 13
Training loss: 2.238980531692505
Validation loss: 2.105404241110689

Epoch: 103| Step: 0
Training loss: 2.084118604660034
Validation loss: 2.0980553229649863

Epoch: 6| Step: 1
Training loss: 1.8088345527648926
Validation loss: 2.101806089442263

Epoch: 6| Step: 2
Training loss: 2.2369227409362793
Validation loss: 2.104937372669097

Epoch: 6| Step: 3
Training loss: 2.201097011566162
Validation loss: 2.0992632066049883

Epoch: 6| Step: 4
Training loss: 2.055985689163208
Validation loss: 2.088612287275253

Epoch: 6| Step: 5
Training loss: 2.133660316467285
Validation loss: 2.09348306604611

Epoch: 6| Step: 6
Training loss: 2.171684741973877
Validation loss: 2.0816701150709584

Epoch: 6| Step: 7
Training loss: 2.0780348777770996
Validation loss: 2.0925243362303703

Epoch: 6| Step: 8
Training loss: 2.1811487674713135
Validation loss: 2.0922000510718233

Epoch: 6| Step: 9
Training loss: 2.6880242824554443
Validation loss: 2.093215068181356

Epoch: 6| Step: 10
Training loss: 2.368197441101074
Validation loss: 2.0986613265929686

Epoch: 6| Step: 11
Training loss: 2.3750083446502686
Validation loss: 2.097483781076247

Epoch: 6| Step: 12
Training loss: 2.0870773792266846
Validation loss: 2.049634256670552

Epoch: 6| Step: 13
Training loss: 1.546062707901001
Validation loss: 2.101600855909368

Epoch: 104| Step: 0
Training loss: 1.9919140338897705
Validation loss: 2.0722479051159275

Epoch: 6| Step: 1
Training loss: 2.1069862842559814
Validation loss: 2.0755129629565823

Epoch: 6| Step: 2
Training loss: 3.137939214706421
Validation loss: 2.1042673792890323

Epoch: 6| Step: 3
Training loss: 2.331940174102783
Validation loss: 2.0896667665050876

Epoch: 6| Step: 4
Training loss: 1.786581039428711
Validation loss: 2.0963196293000252

Epoch: 6| Step: 5
Training loss: 2.0870213508605957
Validation loss: 2.082898901354882

Epoch: 6| Step: 6
Training loss: 1.6860029697418213
Validation loss: 2.1017959040980183

Epoch: 6| Step: 7
Training loss: 1.6426447629928589
Validation loss: 2.081465607048363

Epoch: 6| Step: 8
Training loss: 2.450753927230835
Validation loss: 2.1032674927865305

Epoch: 6| Step: 9
Training loss: 1.751071810722351
Validation loss: 2.0958167455529653

Epoch: 6| Step: 10
Training loss: 1.6826236248016357
Validation loss: 2.0818933799702632

Epoch: 6| Step: 11
Training loss: 2.793621063232422
Validation loss: 2.0771055465103476

Epoch: 6| Step: 12
Training loss: 2.2889833450317383
Validation loss: 2.098662281549105

Epoch: 6| Step: 13
Training loss: 2.1264050006866455
Validation loss: 2.082659211210025

Epoch: 105| Step: 0
Training loss: 2.99711537361145
Validation loss: 2.0663317147121636

Epoch: 6| Step: 1
Training loss: 1.8802404403686523
Validation loss: 2.1130625329991823

Epoch: 6| Step: 2
Training loss: 1.9718658924102783
Validation loss: 2.0796999316061697

Epoch: 6| Step: 3
Training loss: 2.085085391998291
Validation loss: 2.109585794069434

Epoch: 6| Step: 4
Training loss: 2.210477590560913
Validation loss: 2.0874868631362915

Epoch: 6| Step: 5
Training loss: 1.5155394077301025
Validation loss: 2.111351020874516

Epoch: 6| Step: 6
Training loss: 1.8083873987197876
Validation loss: 2.1124268975309146

Epoch: 6| Step: 7
Training loss: 2.300774097442627
Validation loss: 2.0746826882003457

Epoch: 6| Step: 8
Training loss: 2.4437694549560547
Validation loss: 2.079558018715151

Epoch: 6| Step: 9
Training loss: 1.4889843463897705
Validation loss: 2.097665564988249

Epoch: 6| Step: 10
Training loss: 1.670182704925537
Validation loss: 2.0883375316537838

Epoch: 6| Step: 11
Training loss: 2.2972540855407715
Validation loss: 2.071975158106896

Epoch: 6| Step: 12
Training loss: 2.1424150466918945
Validation loss: 2.0975353435803483

Epoch: 6| Step: 13
Training loss: 3.605278968811035
Validation loss: 2.0850126922771497

Epoch: 106| Step: 0
Training loss: 1.8399105072021484
Validation loss: 2.067466879403719

Epoch: 6| Step: 1
Training loss: 2.355440616607666
Validation loss: 2.085650218430386

Epoch: 6| Step: 2
Training loss: 2.003664493560791
Validation loss: 2.072354366702418

Epoch: 6| Step: 3
Training loss: 2.950680732727051
Validation loss: 2.1026592023911013

Epoch: 6| Step: 4
Training loss: 2.461124897003174
Validation loss: 2.1085599007145053

Epoch: 6| Step: 5
Training loss: 1.6601966619491577
Validation loss: 2.086656208961241

Epoch: 6| Step: 6
Training loss: 1.8701632022857666
Validation loss: 2.0919129758752804

Epoch: 6| Step: 7
Training loss: 1.7994742393493652
Validation loss: 2.111843357804001

Epoch: 6| Step: 8
Training loss: 2.6137936115264893
Validation loss: 2.095879275311706

Epoch: 6| Step: 9
Training loss: 2.66502046585083
Validation loss: 2.109455990534957

Epoch: 6| Step: 10
Training loss: 1.6556249856948853
Validation loss: 2.1143489358245686

Epoch: 6| Step: 11
Training loss: 2.0857362747192383
Validation loss: 2.115436810319142

Epoch: 6| Step: 12
Training loss: 1.776642084121704
Validation loss: 2.099224467431345

Epoch: 6| Step: 13
Training loss: 2.247448444366455
Validation loss: 2.0888233415542112

Epoch: 107| Step: 0
Training loss: 2.6612584590911865
Validation loss: 2.0935262736453804

Epoch: 6| Step: 1
Training loss: 1.5017178058624268
Validation loss: 2.112916799001796

Epoch: 6| Step: 2
Training loss: 2.301344871520996
Validation loss: 2.06832782812016

Epoch: 6| Step: 3
Training loss: 1.7748377323150635
Validation loss: 2.122941437587943

Epoch: 6| Step: 4
Training loss: 2.1272940635681152
Validation loss: 2.111037495315716

Epoch: 6| Step: 5
Training loss: 1.8437714576721191
Validation loss: 2.091998843736546

Epoch: 6| Step: 6
Training loss: 1.982203483581543
Validation loss: 2.1177263490615355

Epoch: 6| Step: 7
Training loss: 2.728085994720459
Validation loss: 2.1036034399463284

Epoch: 6| Step: 8
Training loss: 1.320176124572754
Validation loss: 2.1085223331246326

Epoch: 6| Step: 9
Training loss: 2.9619686603546143
Validation loss: 2.073122032227055

Epoch: 6| Step: 10
Training loss: 2.4380853176116943
Validation loss: 2.0859955651785738

Epoch: 6| Step: 11
Training loss: 1.9194347858428955
Validation loss: 2.0608496973591466

Epoch: 6| Step: 12
Training loss: 1.9651702642440796
Validation loss: 2.1094518515371505

Epoch: 6| Step: 13
Training loss: 2.51639723777771
Validation loss: 2.0971552838561354

Epoch: 108| Step: 0
Training loss: 1.9137799739837646
Validation loss: 2.086703710658576

Epoch: 6| Step: 1
Training loss: 1.9756088256835938
Validation loss: 2.0933768351872764

Epoch: 6| Step: 2
Training loss: 2.2467734813690186
Validation loss: 2.070962134227958

Epoch: 6| Step: 3
Training loss: 1.6183664798736572
Validation loss: 2.0762471281072146

Epoch: 6| Step: 4
Training loss: 2.657017707824707
Validation loss: 2.083056470399262

Epoch: 6| Step: 5
Training loss: 2.0933570861816406
Validation loss: 2.06278565109417

Epoch: 6| Step: 6
Training loss: 2.3089256286621094
Validation loss: 2.0972658613676667

Epoch: 6| Step: 7
Training loss: 2.2213730812072754
Validation loss: 2.0733957572649886

Epoch: 6| Step: 8
Training loss: 2.1813461780548096
Validation loss: 2.0918735047822357

Epoch: 6| Step: 9
Training loss: 2.1885740756988525
Validation loss: 2.0595470500248734

Epoch: 6| Step: 10
Training loss: 2.3005149364471436
Validation loss: 2.109581806326425

Epoch: 6| Step: 11
Training loss: 1.9566757678985596
Validation loss: 2.1100479531031784

Epoch: 6| Step: 12
Training loss: 2.6553573608398438
Validation loss: 2.085386153190367

Epoch: 6| Step: 13
Training loss: 1.4596329927444458
Validation loss: 2.098493609377133

Epoch: 109| Step: 0
Training loss: 2.325732707977295
Validation loss: 2.0859267993639876

Epoch: 6| Step: 1
Training loss: 1.5874052047729492
Validation loss: 2.073932073449576

Epoch: 6| Step: 2
Training loss: 1.9813685417175293
Validation loss: 2.0927879015604653

Epoch: 6| Step: 3
Training loss: 1.751584529876709
Validation loss: 2.0789163612550303

Epoch: 6| Step: 4
Training loss: 1.9399940967559814
Validation loss: 2.104480228116435

Epoch: 6| Step: 5
Training loss: 2.481980323791504
Validation loss: 2.0818073339359735

Epoch: 6| Step: 6
Training loss: 2.5374529361724854
Validation loss: 2.094293845597134

Epoch: 6| Step: 7
Training loss: 2.722508430480957
Validation loss: 2.1055460309469574

Epoch: 6| Step: 8
Training loss: 1.7489511966705322
Validation loss: 2.101647864105881

Epoch: 6| Step: 9
Training loss: 2.383417844772339
Validation loss: 2.1020186716510403

Epoch: 6| Step: 10
Training loss: 1.8320320844650269
Validation loss: 2.07211741324394

Epoch: 6| Step: 11
Training loss: 2.430619239807129
Validation loss: 2.101541288437382

Epoch: 6| Step: 12
Training loss: 1.793379545211792
Validation loss: 2.0771181198858444

Epoch: 6| Step: 13
Training loss: 2.3471322059631348
Validation loss: 2.097284904090307

Epoch: 110| Step: 0
Training loss: 2.0967025756835938
Validation loss: 2.046110491598806

Epoch: 6| Step: 1
Training loss: 2.6711297035217285
Validation loss: 2.080514746327554

Epoch: 6| Step: 2
Training loss: 2.3643226623535156
Validation loss: 2.0813659403913762

Epoch: 6| Step: 3
Training loss: 1.5715852975845337
Validation loss: 2.0962202164434616

Epoch: 6| Step: 4
Training loss: 1.5404460430145264
Validation loss: 2.092169811648707

Epoch: 6| Step: 5
Training loss: 2.936817169189453
Validation loss: 2.1022899535394486

Epoch: 6| Step: 6
Training loss: 2.5794410705566406
Validation loss: 2.051052242197016

Epoch: 6| Step: 7
Training loss: 2.6332085132598877
Validation loss: 2.0619407353862638

Epoch: 6| Step: 8
Training loss: 2.723206043243408
Validation loss: 2.06091377299319

Epoch: 6| Step: 9
Training loss: 1.3954532146453857
Validation loss: 2.0845487399767806

Epoch: 6| Step: 10
Training loss: 1.0051993131637573
Validation loss: 2.09837027647162

Epoch: 6| Step: 11
Training loss: 2.5169780254364014
Validation loss: 2.08163550848602

Epoch: 6| Step: 12
Training loss: 1.748285174369812
Validation loss: 2.0859863142813406

Epoch: 6| Step: 13
Training loss: 2.108255624771118
Validation loss: 2.0865396274033414

Epoch: 111| Step: 0
Training loss: 2.4270920753479004
Validation loss: 2.0686250476426977

Epoch: 6| Step: 1
Training loss: 2.1293230056762695
Validation loss: 2.0988525370115876

Epoch: 6| Step: 2
Training loss: 2.5055627822875977
Validation loss: 2.0931389716363724

Epoch: 6| Step: 3
Training loss: 1.791059136390686
Validation loss: 2.0929229644037064

Epoch: 6| Step: 4
Training loss: 3.006340503692627
Validation loss: 2.0754377047220864

Epoch: 6| Step: 5
Training loss: 1.780942678451538
Validation loss: 2.1211205426082818

Epoch: 6| Step: 6
Training loss: 2.107731342315674
Validation loss: 2.095183434024934

Epoch: 6| Step: 7
Training loss: 1.5657377243041992
Validation loss: 2.0945425597570275

Epoch: 6| Step: 8
Training loss: 1.6586230993270874
Validation loss: 2.0926122306495585

Epoch: 6| Step: 9
Training loss: 2.232736349105835
Validation loss: 2.0824310984662784

Epoch: 6| Step: 10
Training loss: 1.921032428741455
Validation loss: 2.0856604114655526

Epoch: 6| Step: 11
Training loss: 2.4031825065612793
Validation loss: 2.0709583169670513

Epoch: 6| Step: 12
Training loss: 2.4029407501220703
Validation loss: 2.07191865674911

Epoch: 6| Step: 13
Training loss: 1.6150522232055664
Validation loss: 2.0963170605321086

Epoch: 112| Step: 0
Training loss: 2.096064805984497
Validation loss: 2.1030768220142653

Epoch: 6| Step: 1
Training loss: 2.3998217582702637
Validation loss: 2.0943833525462816

Epoch: 6| Step: 2
Training loss: 2.136303186416626
Validation loss: 2.096690086908238

Epoch: 6| Step: 3
Training loss: 2.681549072265625
Validation loss: 2.0906068612170476

Epoch: 6| Step: 4
Training loss: 2.8309526443481445
Validation loss: 2.0863022893987675

Epoch: 6| Step: 5
Training loss: 1.9857630729675293
Validation loss: 2.0799727491153184

Epoch: 6| Step: 6
Training loss: 2.0703654289245605
Validation loss: 2.0765827740392377

Epoch: 6| Step: 7
Training loss: 1.6964786052703857
Validation loss: 2.099273230439873

Epoch: 6| Step: 8
Training loss: 1.7214109897613525
Validation loss: 2.1026508833772395

Epoch: 6| Step: 9
Training loss: 2.0623950958251953
Validation loss: 2.073684007890763

Epoch: 6| Step: 10
Training loss: 1.3399571180343628
Validation loss: 2.0580619791502595

Epoch: 6| Step: 11
Training loss: 2.3472347259521484
Validation loss: 2.058821560234152

Epoch: 6| Step: 12
Training loss: 1.8996546268463135
Validation loss: 2.063955073715538

Epoch: 6| Step: 13
Training loss: 2.4251513481140137
Validation loss: 2.0670055907259703

Epoch: 113| Step: 0
Training loss: 2.388221025466919
Validation loss: 2.077150688376478

Epoch: 6| Step: 1
Training loss: 1.919395923614502
Validation loss: 2.1193403813146774

Epoch: 6| Step: 2
Training loss: 2.145315170288086
Validation loss: 2.0691147619678127

Epoch: 6| Step: 3
Training loss: 2.169246196746826
Validation loss: 2.076718294492332

Epoch: 6| Step: 4
Training loss: 2.488680839538574
Validation loss: 2.100769449305791

Epoch: 6| Step: 5
Training loss: 2.153005599975586
Validation loss: 2.075754024649179

Epoch: 6| Step: 6
Training loss: 2.809154510498047
Validation loss: 2.086398857896046

Epoch: 6| Step: 7
Training loss: 1.7534704208374023
Validation loss: 2.0761784968837613

Epoch: 6| Step: 8
Training loss: 1.8455228805541992
Validation loss: 2.0820545637479393

Epoch: 6| Step: 9
Training loss: 2.082880735397339
Validation loss: 2.0804275928005094

Epoch: 6| Step: 10
Training loss: 2.4166979789733887
Validation loss: 2.0812230981806272

Epoch: 6| Step: 11
Training loss: 1.7197226285934448
Validation loss: 2.084830343082387

Epoch: 6| Step: 12
Training loss: 1.9714410305023193
Validation loss: 2.08696392274672

Epoch: 6| Step: 13
Training loss: 1.5123227834701538
Validation loss: 2.076449396789715

Epoch: 114| Step: 0
Training loss: 2.1588451862335205
Validation loss: 2.085108857001028

Epoch: 6| Step: 1
Training loss: 2.0835986137390137
Validation loss: 2.0622807728346957

Epoch: 6| Step: 2
Training loss: 1.594688892364502
Validation loss: 2.080244625768354

Epoch: 6| Step: 3
Training loss: 2.408945322036743
Validation loss: 2.0673892036561043

Epoch: 6| Step: 4
Training loss: 1.961758017539978
Validation loss: 2.070286414956534

Epoch: 6| Step: 5
Training loss: 2.428832530975342
Validation loss: 2.071991979434926

Epoch: 6| Step: 6
Training loss: 1.7930878400802612
Validation loss: 2.079992607075681

Epoch: 6| Step: 7
Training loss: 2.81160306930542
Validation loss: 2.094370144669728

Epoch: 6| Step: 8
Training loss: 1.5511192083358765
Validation loss: 2.0621102574051067

Epoch: 6| Step: 9
Training loss: 1.9761455059051514
Validation loss: 2.082227558218023

Epoch: 6| Step: 10
Training loss: 2.4289450645446777
Validation loss: 2.0869430572755876

Epoch: 6| Step: 11
Training loss: 2.395022392272949
Validation loss: 2.092962649560744

Epoch: 6| Step: 12
Training loss: 1.6460745334625244
Validation loss: 2.0752324891346756

Epoch: 6| Step: 13
Training loss: 2.5537452697753906
Validation loss: 2.070311942408162

Epoch: 115| Step: 0
Training loss: 2.110036849975586
Validation loss: 2.0772602199226298

Epoch: 6| Step: 1
Training loss: 1.8999786376953125
Validation loss: 2.072710998596684

Epoch: 6| Step: 2
Training loss: 2.2744264602661133
Validation loss: 2.075060688039308

Epoch: 6| Step: 3
Training loss: 1.6947336196899414
Validation loss: 2.076253911500336

Epoch: 6| Step: 4
Training loss: 2.1290178298950195
Validation loss: 2.074292093194941

Epoch: 6| Step: 5
Training loss: 2.2414188385009766
Validation loss: 2.077149360410629

Epoch: 6| Step: 6
Training loss: 2.0476341247558594
Validation loss: 2.0980214457358084

Epoch: 6| Step: 7
Training loss: 2.5961227416992188
Validation loss: 2.079423132763114

Epoch: 6| Step: 8
Training loss: 1.865024447441101
Validation loss: 2.089009997665241

Epoch: 6| Step: 9
Training loss: 2.3973562717437744
Validation loss: 2.0839674267717587

Epoch: 6| Step: 10
Training loss: 2.0501792430877686
Validation loss: 2.05078952030469

Epoch: 6| Step: 11
Training loss: 1.8559420108795166
Validation loss: 2.0583844723240023

Epoch: 6| Step: 12
Training loss: 2.4439077377319336
Validation loss: 2.0789790691867953

Epoch: 6| Step: 13
Training loss: 2.0580790042877197
Validation loss: 2.0739126974536526

Epoch: 116| Step: 0
Training loss: 1.6472290754318237
Validation loss: 2.0692304667606147

Epoch: 6| Step: 1
Training loss: 1.838923692703247
Validation loss: 2.0659675469962497

Epoch: 6| Step: 2
Training loss: 2.9248406887054443
Validation loss: 2.0567559298648628

Epoch: 6| Step: 3
Training loss: 2.2414608001708984
Validation loss: 2.1074517747407318

Epoch: 6| Step: 4
Training loss: 1.8603034019470215
Validation loss: 2.0925932802179807

Epoch: 6| Step: 5
Training loss: 2.394782304763794
Validation loss: 2.0873767906619656

Epoch: 6| Step: 6
Training loss: 1.6918843984603882
Validation loss: 2.0974264003897227

Epoch: 6| Step: 7
Training loss: 1.9042214155197144
Validation loss: 2.0496232304521786

Epoch: 6| Step: 8
Training loss: 2.115234613418579
Validation loss: 2.0886369674436507

Epoch: 6| Step: 9
Training loss: 2.20877742767334
Validation loss: 2.079438263370145

Epoch: 6| Step: 10
Training loss: 1.80068838596344
Validation loss: 2.069170956970543

Epoch: 6| Step: 11
Training loss: 2.6888022422790527
Validation loss: 2.078167082161032

Epoch: 6| Step: 12
Training loss: 2.2465648651123047
Validation loss: 2.083846381915513

Epoch: 6| Step: 13
Training loss: 2.1366047859191895
Validation loss: 2.095756830707673

Epoch: 117| Step: 0
Training loss: 2.1214327812194824
Validation loss: 2.084213541400048

Epoch: 6| Step: 1
Training loss: 2.1917457580566406
Validation loss: 2.0667929213534117

Epoch: 6| Step: 2
Training loss: 1.9733505249023438
Validation loss: 2.0855562917647825

Epoch: 6| Step: 3
Training loss: 2.267195463180542
Validation loss: 2.0717052810935566

Epoch: 6| Step: 4
Training loss: 3.3639132976531982
Validation loss: 2.0844812957189416

Epoch: 6| Step: 5
Training loss: 1.6236567497253418
Validation loss: 2.0601181343037593

Epoch: 6| Step: 6
Training loss: 2.0778400897979736
Validation loss: 2.055142548776442

Epoch: 6| Step: 7
Training loss: 2.0492520332336426
Validation loss: 2.0740377646620556

Epoch: 6| Step: 8
Training loss: 1.8455400466918945
Validation loss: 2.055829510893873

Epoch: 6| Step: 9
Training loss: 2.5376617908477783
Validation loss: 2.083849955630559

Epoch: 6| Step: 10
Training loss: 2.494016170501709
Validation loss: 2.072619873990295

Epoch: 6| Step: 11
Training loss: 1.6527087688446045
Validation loss: 2.1009860525849047

Epoch: 6| Step: 12
Training loss: 1.6165986061096191
Validation loss: 2.0972282758323093

Epoch: 6| Step: 13
Training loss: 1.6109999418258667
Validation loss: 2.0946296620112594

Epoch: 118| Step: 0
Training loss: 2.2960424423217773
Validation loss: 2.0552172814646075

Epoch: 6| Step: 1
Training loss: 2.099231481552124
Validation loss: 2.0834300030944166

Epoch: 6| Step: 2
Training loss: 1.9940978288650513
Validation loss: 2.0937717806908394

Epoch: 6| Step: 3
Training loss: 2.7728967666625977
Validation loss: 2.1000391078251663

Epoch: 6| Step: 4
Training loss: 2.1912407875061035
Validation loss: 2.072708998956988

Epoch: 6| Step: 5
Training loss: 2.031745195388794
Validation loss: 2.102000190365699

Epoch: 6| Step: 6
Training loss: 3.0351107120513916
Validation loss: 2.077042853960427

Epoch: 6| Step: 7
Training loss: 1.659623622894287
Validation loss: 2.0839843262908277

Epoch: 6| Step: 8
Training loss: 1.866417646408081
Validation loss: 2.09804908562732

Epoch: 6| Step: 9
Training loss: 2.102745532989502
Validation loss: 2.1038987008474206

Epoch: 6| Step: 10
Training loss: 1.8698198795318604
Validation loss: 2.0847013637583744

Epoch: 6| Step: 11
Training loss: 1.6988167762756348
Validation loss: 2.082703290447112

Epoch: 6| Step: 12
Training loss: 2.1136813163757324
Validation loss: 2.061373908673563

Epoch: 6| Step: 13
Training loss: 1.5952870845794678
Validation loss: 2.098615356670913

Epoch: 119| Step: 0
Training loss: 1.8974730968475342
Validation loss: 2.074575170393913

Epoch: 6| Step: 1
Training loss: 2.548464775085449
Validation loss: 2.0702055474763275

Epoch: 6| Step: 2
Training loss: 2.444427728652954
Validation loss: 2.0603301063660653

Epoch: 6| Step: 3
Training loss: 1.2732360363006592
Validation loss: 2.0792116247197634

Epoch: 6| Step: 4
Training loss: 2.9729676246643066
Validation loss: 2.0932031985252135

Epoch: 6| Step: 5
Training loss: 2.283384084701538
Validation loss: 2.061107191988217

Epoch: 6| Step: 6
Training loss: 1.4695041179656982
Validation loss: 2.077348252778412

Epoch: 6| Step: 7
Training loss: 2.0793378353118896
Validation loss: 2.0548874280786

Epoch: 6| Step: 8
Training loss: 2.818598508834839
Validation loss: 2.077580557074598

Epoch: 6| Step: 9
Training loss: 1.7237153053283691
Validation loss: 2.0800664911987963

Epoch: 6| Step: 10
Training loss: 1.837449312210083
Validation loss: 2.0951801243648736

Epoch: 6| Step: 11
Training loss: 1.771719217300415
Validation loss: 2.0669932262871855

Epoch: 6| Step: 12
Training loss: 2.064044713973999
Validation loss: 2.1289039324688654

Epoch: 6| Step: 13
Training loss: 1.8880887031555176
Validation loss: 2.0752271798349198

Epoch: 120| Step: 0
Training loss: 1.954650640487671
Validation loss: 2.0643695426243607

Epoch: 6| Step: 1
Training loss: 1.8340872526168823
Validation loss: 2.0648286522075696

Epoch: 6| Step: 2
Training loss: 1.8948476314544678
Validation loss: 2.0763640237110916

Epoch: 6| Step: 3
Training loss: 2.6669976711273193
Validation loss: 2.0516505702849357

Epoch: 6| Step: 4
Training loss: 2.241743564605713
Validation loss: 2.072349658576391

Epoch: 6| Step: 5
Training loss: 2.8839190006256104
Validation loss: 2.06567717623967

Epoch: 6| Step: 6
Training loss: 2.7678582668304443
Validation loss: 2.0672701892032417

Epoch: 6| Step: 7
Training loss: 1.6698741912841797
Validation loss: 2.0824503078255603

Epoch: 6| Step: 8
Training loss: 1.4288558959960938
Validation loss: 2.0634382155633744

Epoch: 6| Step: 9
Training loss: 1.8014518022537231
Validation loss: 2.0503612615728892

Epoch: 6| Step: 10
Training loss: 1.8841606378555298
Validation loss: 2.063429272303017

Epoch: 6| Step: 11
Training loss: 2.1626648902893066
Validation loss: 2.0579807296875985

Epoch: 6| Step: 12
Training loss: 1.90084969997406
Validation loss: 2.0623166048398582

Epoch: 6| Step: 13
Training loss: 2.572766065597534
Validation loss: 2.0578010210426907

Epoch: 121| Step: 0
Training loss: 2.1132349967956543
Validation loss: 2.105957700360206

Epoch: 6| Step: 1
Training loss: 1.6122870445251465
Validation loss: 2.06398449021001

Epoch: 6| Step: 2
Training loss: 2.172492265701294
Validation loss: 2.0482617257743754

Epoch: 6| Step: 3
Training loss: 2.147979736328125
Validation loss: 2.0785138965934835

Epoch: 6| Step: 4
Training loss: 2.260067939758301
Validation loss: 2.096662405998476

Epoch: 6| Step: 5
Training loss: 1.8859004974365234
Validation loss: 2.08359145989982

Epoch: 6| Step: 6
Training loss: 1.8472322225570679
Validation loss: 2.0850928137379308

Epoch: 6| Step: 7
Training loss: 1.4924201965332031
Validation loss: 2.0730372885222077

Epoch: 6| Step: 8
Training loss: 2.5430424213409424
Validation loss: 2.0639052467961467

Epoch: 6| Step: 9
Training loss: 3.137922763824463
Validation loss: 2.078062529204994

Epoch: 6| Step: 10
Training loss: 1.8749935626983643
Validation loss: 2.0995478142974195

Epoch: 6| Step: 11
Training loss: 2.0486230850219727
Validation loss: 2.062936839237008

Epoch: 6| Step: 12
Training loss: 2.3701319694519043
Validation loss: 2.073438572627242

Epoch: 6| Step: 13
Training loss: 1.527867317199707
Validation loss: 2.0604600009097847

Epoch: 122| Step: 0
Training loss: 1.6327166557312012
Validation loss: 2.0773120105907483

Epoch: 6| Step: 1
Training loss: 2.532721519470215
Validation loss: 2.0608609850688646

Epoch: 6| Step: 2
Training loss: 2.5304641723632812
Validation loss: 2.0802683125260057

Epoch: 6| Step: 3
Training loss: 2.2636613845825195
Validation loss: 2.0978805198464343

Epoch: 6| Step: 4
Training loss: 1.6472971439361572
Validation loss: 2.087145297758041

Epoch: 6| Step: 5
Training loss: 2.2252182960510254
Validation loss: 2.050455753521253

Epoch: 6| Step: 6
Training loss: 1.5759556293487549
Validation loss: 2.073788619810535

Epoch: 6| Step: 7
Training loss: 2.8563220500946045
Validation loss: 2.0887078008344098

Epoch: 6| Step: 8
Training loss: 2.3138630390167236
Validation loss: 2.050193181601904

Epoch: 6| Step: 9
Training loss: 1.8570287227630615
Validation loss: 2.0409915677962767

Epoch: 6| Step: 10
Training loss: 1.954308032989502
Validation loss: 2.0363285336443173

Epoch: 6| Step: 11
Training loss: 1.4291568994522095
Validation loss: 2.072288482419906

Epoch: 6| Step: 12
Training loss: 2.4736099243164062
Validation loss: 2.0633030835018364

Epoch: 6| Step: 13
Training loss: 1.942872405052185
Validation loss: 2.0843557926916305

Epoch: 123| Step: 0
Training loss: 1.8826427459716797
Validation loss: 2.0704561305302445

Epoch: 6| Step: 1
Training loss: 2.178276538848877
Validation loss: 2.0697912913496777

Epoch: 6| Step: 2
Training loss: 1.982326626777649
Validation loss: 2.0711222361492854

Epoch: 6| Step: 3
Training loss: 2.1420392990112305
Validation loss: 2.0658251598317134

Epoch: 6| Step: 4
Training loss: 2.4258594512939453
Validation loss: 2.077838832332242

Epoch: 6| Step: 5
Training loss: 2.3165183067321777
Validation loss: 2.0599202750831522

Epoch: 6| Step: 6
Training loss: 1.9626365900039673
Validation loss: 2.087458161897557

Epoch: 6| Step: 7
Training loss: 2.0844502449035645
Validation loss: 2.0694733512017036

Epoch: 6| Step: 8
Training loss: 1.8914018869400024
Validation loss: 2.0739653174595167

Epoch: 6| Step: 9
Training loss: 2.2232284545898438
Validation loss: 2.068915436344762

Epoch: 6| Step: 10
Training loss: 1.482912302017212
Validation loss: 2.0801494736825266

Epoch: 6| Step: 11
Training loss: 2.123182773590088
Validation loss: 2.061600415937362

Epoch: 6| Step: 12
Training loss: 2.058586597442627
Validation loss: 2.0573950108661445

Epoch: 6| Step: 13
Training loss: 2.611382484436035
Validation loss: 2.061743208157119

Epoch: 124| Step: 0
Training loss: 2.4747390747070312
Validation loss: 2.0838632557981756

Epoch: 6| Step: 1
Training loss: 2.169346809387207
Validation loss: 2.0801004107280443

Epoch: 6| Step: 2
Training loss: 2.6716854572296143
Validation loss: 2.0737947622934976

Epoch: 6| Step: 3
Training loss: 1.6996713876724243
Validation loss: 2.068321707428143

Epoch: 6| Step: 4
Training loss: 1.957958459854126
Validation loss: 2.0636446860528763

Epoch: 6| Step: 5
Training loss: 1.713179111480713
Validation loss: 2.0416095269623624

Epoch: 6| Step: 6
Training loss: 1.7380214929580688
Validation loss: 2.0893163501575427

Epoch: 6| Step: 7
Training loss: 1.5340982675552368
Validation loss: 2.0684539887212936

Epoch: 6| Step: 8
Training loss: 2.182154655456543
Validation loss: 2.070529063542684

Epoch: 6| Step: 9
Training loss: 2.2083351612091064
Validation loss: 2.059560279692373

Epoch: 6| Step: 10
Training loss: 2.562891721725464
Validation loss: 2.0743563149565007

Epoch: 6| Step: 11
Training loss: 1.6064209938049316
Validation loss: 2.0564245844400055

Epoch: 6| Step: 12
Training loss: 2.692032814025879
Validation loss: 2.077683557746231

Epoch: 6| Step: 13
Training loss: 2.2450201511383057
Validation loss: 2.0514813161665395

Epoch: 125| Step: 0
Training loss: 1.647446632385254
Validation loss: 2.0533428333138906

Epoch: 6| Step: 1
Training loss: 1.6262401342391968
Validation loss: 2.043377463535596

Epoch: 6| Step: 2
Training loss: 2.4903998374938965
Validation loss: 2.0677432655006327

Epoch: 6| Step: 3
Training loss: 2.504760980606079
Validation loss: 2.078189206379716

Epoch: 6| Step: 4
Training loss: 2.121821880340576
Validation loss: 2.0621530945583055

Epoch: 6| Step: 5
Training loss: 2.134861469268799
Validation loss: 2.0673413481763614

Epoch: 6| Step: 6
Training loss: 2.2242038249969482
Validation loss: 2.067437485982013

Epoch: 6| Step: 7
Training loss: 1.4374206066131592
Validation loss: 2.0618198046120266

Epoch: 6| Step: 8
Training loss: 1.816427230834961
Validation loss: 2.071244139825144

Epoch: 6| Step: 9
Training loss: 2.4966378211975098
Validation loss: 2.055698089702155

Epoch: 6| Step: 10
Training loss: 2.4780235290527344
Validation loss: 2.0802520808353218

Epoch: 6| Step: 11
Training loss: 1.9044355154037476
Validation loss: 2.0846333580632366

Epoch: 6| Step: 12
Training loss: 2.2594175338745117
Validation loss: 2.073870466601464

Epoch: 6| Step: 13
Training loss: 1.5788159370422363
Validation loss: 2.0694750380772415

Epoch: 126| Step: 0
Training loss: 2.4582161903381348
Validation loss: 2.0535211614383164

Epoch: 6| Step: 1
Training loss: 1.8812329769134521
Validation loss: 2.0572530761841805

Epoch: 6| Step: 2
Training loss: 1.641926646232605
Validation loss: 2.073549739776119

Epoch: 6| Step: 3
Training loss: 2.2468600273132324
Validation loss: 2.0716370331343783

Epoch: 6| Step: 4
Training loss: 2.0494179725646973
Validation loss: 2.0758247272942656

Epoch: 6| Step: 5
Training loss: 1.9044080972671509
Validation loss: 2.0990515652523247

Epoch: 6| Step: 6
Training loss: 1.3220841884613037
Validation loss: 2.0585454561377086

Epoch: 6| Step: 7
Training loss: 2.7820398807525635
Validation loss: 2.05583934117389

Epoch: 6| Step: 8
Training loss: 1.3729076385498047
Validation loss: 2.0469553239883913

Epoch: 6| Step: 9
Training loss: 1.97755765914917
Validation loss: 2.0727490763510428

Epoch: 6| Step: 10
Training loss: 2.4126200675964355
Validation loss: 2.0942089275647233

Epoch: 6| Step: 11
Training loss: 2.7355194091796875
Validation loss: 2.075557436994327

Epoch: 6| Step: 12
Training loss: 2.5421459674835205
Validation loss: 2.0585772042633383

Epoch: 6| Step: 13
Training loss: 1.8136086463928223
Validation loss: 2.0456109123845256

Epoch: 127| Step: 0
Training loss: 1.720871925354004
Validation loss: 2.070713568759221

Epoch: 6| Step: 1
Training loss: 2.251300811767578
Validation loss: 2.0541705546840543

Epoch: 6| Step: 2
Training loss: 2.4326558113098145
Validation loss: 2.065767357426305

Epoch: 6| Step: 3
Training loss: 2.407989978790283
Validation loss: 2.0776480000506163

Epoch: 6| Step: 4
Training loss: 1.0891530513763428
Validation loss: 2.091776247947447

Epoch: 6| Step: 5
Training loss: 2.239612579345703
Validation loss: 2.07292769288504

Epoch: 6| Step: 6
Training loss: 2.9717023372650146
Validation loss: 2.0733699029491794

Epoch: 6| Step: 7
Training loss: 1.8194299936294556
Validation loss: 2.076693901451685

Epoch: 6| Step: 8
Training loss: 2.3267862796783447
Validation loss: 2.0617571492348947

Epoch: 6| Step: 9
Training loss: 1.77584707736969
Validation loss: 2.0719738570592736

Epoch: 6| Step: 10
Training loss: 1.815906286239624
Validation loss: 2.0851407012631817

Epoch: 6| Step: 11
Training loss: 2.327058792114258
Validation loss: 2.063186578853156

Epoch: 6| Step: 12
Training loss: 1.947990894317627
Validation loss: 2.0921775576888875

Epoch: 6| Step: 13
Training loss: 1.6850762367248535
Validation loss: 2.061618302458076

Epoch: 128| Step: 0
Training loss: 1.669460415840149
Validation loss: 2.07558762386281

Epoch: 6| Step: 1
Training loss: 1.913133144378662
Validation loss: 2.0926076686510475

Epoch: 6| Step: 2
Training loss: 1.430513620376587
Validation loss: 2.0576816425528577

Epoch: 6| Step: 3
Training loss: 2.1964380741119385
Validation loss: 2.0735672648235033

Epoch: 6| Step: 4
Training loss: 1.8367903232574463
Validation loss: 2.0710312397249284

Epoch: 6| Step: 5
Training loss: 2.090585947036743
Validation loss: 2.081114048598915

Epoch: 6| Step: 6
Training loss: 2.7607169151306152
Validation loss: 2.0746296054573468

Epoch: 6| Step: 7
Training loss: 3.3559441566467285
Validation loss: 2.0716409811409573

Epoch: 6| Step: 8
Training loss: 2.3196334838867188
Validation loss: 2.056789710957517

Epoch: 6| Step: 9
Training loss: 2.13417649269104
Validation loss: 2.0356541628478677

Epoch: 6| Step: 10
Training loss: 1.4918818473815918
Validation loss: 2.076761486709759

Epoch: 6| Step: 11
Training loss: 1.731308102607727
Validation loss: 2.0900183211090746

Epoch: 6| Step: 12
Training loss: 1.9092967510223389
Validation loss: 2.044020647643715

Epoch: 6| Step: 13
Training loss: 2.0709893703460693
Validation loss: 2.07763635599485

Epoch: 129| Step: 0
Training loss: 1.530923843383789
Validation loss: 2.0286442079851703

Epoch: 6| Step: 1
Training loss: 2.5776960849761963
Validation loss: 2.0635904342897478

Epoch: 6| Step: 2
Training loss: 2.5986428260803223
Validation loss: 2.0789656510917087

Epoch: 6| Step: 3
Training loss: 2.244252920150757
Validation loss: 2.065358543908724

Epoch: 6| Step: 4
Training loss: 1.8516418933868408
Validation loss: 2.043779242423273

Epoch: 6| Step: 5
Training loss: 1.9478521347045898
Validation loss: 2.0655522115768923

Epoch: 6| Step: 6
Training loss: 1.7211240530014038
Validation loss: 2.083680918139796

Epoch: 6| Step: 7
Training loss: 2.1904306411743164
Validation loss: 2.072144995453537

Epoch: 6| Step: 8
Training loss: 2.183302879333496
Validation loss: 2.092151957173501

Epoch: 6| Step: 9
Training loss: 1.8463716506958008
Validation loss: 2.0829680914519937

Epoch: 6| Step: 10
Training loss: 1.9938567876815796
Validation loss: 2.1044555235934514

Epoch: 6| Step: 11
Training loss: 1.697519063949585
Validation loss: 2.06164526683028

Epoch: 6| Step: 12
Training loss: 2.6835949420928955
Validation loss: 2.0582131237112065

Epoch: 6| Step: 13
Training loss: 1.7879949808120728
Validation loss: 2.058335354251246

Epoch: 130| Step: 0
Training loss: 2.4113669395446777
Validation loss: 2.0561712685451714

Epoch: 6| Step: 1
Training loss: 1.6695289611816406
Validation loss: 2.06480638698865

Epoch: 6| Step: 2
Training loss: 1.5707035064697266
Validation loss: 2.0769567566533245

Epoch: 6| Step: 3
Training loss: 1.604220986366272
Validation loss: 2.08150525118715

Epoch: 6| Step: 4
Training loss: 1.9887819290161133
Validation loss: 2.0533128682003228

Epoch: 6| Step: 5
Training loss: 2.1445398330688477
Validation loss: 2.1080826661920034

Epoch: 6| Step: 6
Training loss: 2.110980272293091
Validation loss: 2.0890682576805033

Epoch: 6| Step: 7
Training loss: 2.3799567222595215
Validation loss: 2.0639688507203133

Epoch: 6| Step: 8
Training loss: 1.6855093240737915
Validation loss: 2.078194155487963

Epoch: 6| Step: 9
Training loss: 2.602365016937256
Validation loss: 2.0471806615911503

Epoch: 6| Step: 10
Training loss: 2.338778018951416
Validation loss: 2.0893681023710515

Epoch: 6| Step: 11
Training loss: 1.7642221450805664
Validation loss: 2.083906530052103

Epoch: 6| Step: 12
Training loss: 2.143415927886963
Validation loss: 2.0651173104522047

Epoch: 6| Step: 13
Training loss: 2.6827328205108643
Validation loss: 2.069763934740456

Epoch: 131| Step: 0
Training loss: 1.682963252067566
Validation loss: 2.0964776187814693

Epoch: 6| Step: 1
Training loss: 1.9643020629882812
Validation loss: 2.0311305010190575

Epoch: 6| Step: 2
Training loss: 2.4303297996520996
Validation loss: 2.0418899559205577

Epoch: 6| Step: 3
Training loss: 2.041774272918701
Validation loss: 2.0360852979844615

Epoch: 6| Step: 4
Training loss: 2.6117124557495117
Validation loss: 2.075295240648331

Epoch: 6| Step: 5
Training loss: 1.6768417358398438
Validation loss: 2.0667339704369985

Epoch: 6| Step: 6
Training loss: 2.9759039878845215
Validation loss: 2.0540478793523644

Epoch: 6| Step: 7
Training loss: 2.130197525024414
Validation loss: 2.0652261485335646

Epoch: 6| Step: 8
Training loss: 1.9671568870544434
Validation loss: 2.0417236076888217

Epoch: 6| Step: 9
Training loss: 1.53200101852417
Validation loss: 2.0613867595631588

Epoch: 6| Step: 10
Training loss: 1.4327797889709473
Validation loss: 2.038581073925059

Epoch: 6| Step: 11
Training loss: 1.87663996219635
Validation loss: 2.0620568939434585

Epoch: 6| Step: 12
Training loss: 2.070927858352661
Validation loss: 2.0409378069703297

Epoch: 6| Step: 13
Training loss: 2.4535725116729736
Validation loss: 2.0616067904298023

Epoch: 132| Step: 0
Training loss: 2.157484531402588
Validation loss: 2.0654310154658493

Epoch: 6| Step: 1
Training loss: 1.3757917881011963
Validation loss: 2.040768769479567

Epoch: 6| Step: 2
Training loss: 1.7086670398712158
Validation loss: 2.0323276801775862

Epoch: 6| Step: 3
Training loss: 2.2116332054138184
Validation loss: 2.0724788096643265

Epoch: 6| Step: 4
Training loss: 1.896167278289795
Validation loss: 2.076140670366185

Epoch: 6| Step: 5
Training loss: 1.9425846338272095
Validation loss: 2.062342033591322

Epoch: 6| Step: 6
Training loss: 2.226372718811035
Validation loss: 2.0549973749345347

Epoch: 6| Step: 7
Training loss: 1.2656126022338867
Validation loss: 2.064381532771613

Epoch: 6| Step: 8
Training loss: 1.7467647790908813
Validation loss: 2.0426053821399646

Epoch: 6| Step: 9
Training loss: 2.129316568374634
Validation loss: 2.0612778573907833

Epoch: 6| Step: 10
Training loss: 2.5247135162353516
Validation loss: 2.083833586785101

Epoch: 6| Step: 11
Training loss: 2.4590606689453125
Validation loss: 2.0580738154790734

Epoch: 6| Step: 12
Training loss: 2.864306926727295
Validation loss: 2.0607166751738517

Epoch: 6| Step: 13
Training loss: 2.1693315505981445
Validation loss: 2.081223217389917

Epoch: 133| Step: 0
Training loss: 2.2500081062316895
Validation loss: 2.058569180068149

Epoch: 6| Step: 1
Training loss: 1.9431593418121338
Validation loss: 2.0361540163716962

Epoch: 6| Step: 2
Training loss: 1.9369893074035645
Validation loss: 2.058926213172174

Epoch: 6| Step: 3
Training loss: 1.8176268339157104
Validation loss: 2.051056238912767

Epoch: 6| Step: 4
Training loss: 2.256438732147217
Validation loss: 2.0744196073983305

Epoch: 6| Step: 5
Training loss: 2.33461594581604
Validation loss: 2.0660741611193587

Epoch: 6| Step: 6
Training loss: 1.8407996892929077
Validation loss: 2.071364172043339

Epoch: 6| Step: 7
Training loss: 2.5780062675476074
Validation loss: 2.077951619702001

Epoch: 6| Step: 8
Training loss: 2.1114587783813477
Validation loss: 2.0498753260540705

Epoch: 6| Step: 9
Training loss: 1.8423011302947998
Validation loss: 2.0344757521024315

Epoch: 6| Step: 10
Training loss: 1.8244574069976807
Validation loss: 2.0389425882729153

Epoch: 6| Step: 11
Training loss: 1.8518141508102417
Validation loss: 2.0679806201688704

Epoch: 6| Step: 12
Training loss: 1.55000901222229
Validation loss: 2.0446811286352014

Epoch: 6| Step: 13
Training loss: 3.2232372760772705
Validation loss: 2.069441996594911

Epoch: 134| Step: 0
Training loss: 2.1649203300476074
Validation loss: 2.054594293717415

Epoch: 6| Step: 1
Training loss: 1.8809185028076172
Validation loss: 2.052823974240211

Epoch: 6| Step: 2
Training loss: 1.8052160739898682
Validation loss: 2.0712880472983084

Epoch: 6| Step: 3
Training loss: 2.0799732208251953
Validation loss: 2.059789457628804

Epoch: 6| Step: 4
Training loss: 1.6405689716339111
Validation loss: 2.0483661364483576

Epoch: 6| Step: 5
Training loss: 2.7261877059936523
Validation loss: 2.0361501439925163

Epoch: 6| Step: 6
Training loss: 2.6114158630371094
Validation loss: 2.074023626183951

Epoch: 6| Step: 7
Training loss: 3.157792091369629
Validation loss: 2.041951892196491

Epoch: 6| Step: 8
Training loss: 1.4401452541351318
Validation loss: 2.0487525386195027

Epoch: 6| Step: 9
Training loss: 1.2901830673217773
Validation loss: 2.0416289811493247

Epoch: 6| Step: 10
Training loss: 2.0481483936309814
Validation loss: 2.048512189619003

Epoch: 6| Step: 11
Training loss: 1.4061822891235352
Validation loss: 2.062595175158593

Epoch: 6| Step: 12
Training loss: 1.8455090522766113
Validation loss: 2.0723150289186867

Epoch: 6| Step: 13
Training loss: 2.775454521179199
Validation loss: 2.0490319023850145

Epoch: 135| Step: 0
Training loss: 2.476855754852295
Validation loss: 2.087051183946671

Epoch: 6| Step: 1
Training loss: 1.7183574438095093
Validation loss: 2.041791187819614

Epoch: 6| Step: 2
Training loss: 1.8786946535110474
Validation loss: 2.07987021118082

Epoch: 6| Step: 3
Training loss: 2.7683215141296387
Validation loss: 2.08892535778784

Epoch: 6| Step: 4
Training loss: 1.6340652704238892
Validation loss: 2.023489434231994

Epoch: 6| Step: 5
Training loss: 2.288386821746826
Validation loss: 2.0750806664907806

Epoch: 6| Step: 6
Training loss: 2.1619842052459717
Validation loss: 2.0837883513460875

Epoch: 6| Step: 7
Training loss: 1.539222002029419
Validation loss: 2.0955472069401897

Epoch: 6| Step: 8
Training loss: 1.9997833967208862
Validation loss: 2.0673645247695265

Epoch: 6| Step: 9
Training loss: 2.0385899543762207
Validation loss: 2.060370401669574

Epoch: 6| Step: 10
Training loss: 2.1897335052490234
Validation loss: 2.056457475949359

Epoch: 6| Step: 11
Training loss: 1.9515600204467773
Validation loss: 2.068176151603781

Epoch: 6| Step: 12
Training loss: 2.2182135581970215
Validation loss: 2.067187701502154

Epoch: 6| Step: 13
Training loss: 1.5743627548217773
Validation loss: 2.0614298056530695

Epoch: 136| Step: 0
Training loss: 2.398174285888672
Validation loss: 2.070020038594482

Epoch: 6| Step: 1
Training loss: 2.4813804626464844
Validation loss: 2.0555162493900587

Epoch: 6| Step: 2
Training loss: 2.1403541564941406
Validation loss: 2.0461206384884414

Epoch: 6| Step: 3
Training loss: 1.6508610248565674
Validation loss: 2.038801880292995

Epoch: 6| Step: 4
Training loss: 2.206202983856201
Validation loss: 2.028858812906409

Epoch: 6| Step: 5
Training loss: 2.331077814102173
Validation loss: 2.0496627399998326

Epoch: 6| Step: 6
Training loss: 1.9597800970077515
Validation loss: 2.0667038758595786

Epoch: 6| Step: 7
Training loss: 0.9523422122001648
Validation loss: 2.0784991402779855

Epoch: 6| Step: 8
Training loss: 1.4266247749328613
Validation loss: 2.0742723070165163

Epoch: 6| Step: 9
Training loss: 2.0025601387023926
Validation loss: 2.0801342213025658

Epoch: 6| Step: 10
Training loss: 2.2352192401885986
Validation loss: 2.063776359763197

Epoch: 6| Step: 11
Training loss: 1.9292922019958496
Validation loss: 2.044920976443957

Epoch: 6| Step: 12
Training loss: 2.410797595977783
Validation loss: 2.0411713123321533

Epoch: 6| Step: 13
Training loss: 2.2881836891174316
Validation loss: 2.0238593778302594

Epoch: 137| Step: 0
Training loss: 2.0717074871063232
Validation loss: 2.045418889291825

Epoch: 6| Step: 1
Training loss: 1.5567506551742554
Validation loss: 2.0503731120017266

Epoch: 6| Step: 2
Training loss: 2.2817091941833496
Validation loss: 2.024169187391958

Epoch: 6| Step: 3
Training loss: 2.644643783569336
Validation loss: 2.0066652067245974

Epoch: 6| Step: 4
Training loss: 1.6159005165100098
Validation loss: 2.0420071130157798

Epoch: 6| Step: 5
Training loss: 2.121732711791992
Validation loss: 2.0725738258771997

Epoch: 6| Step: 6
Training loss: 2.647915840148926
Validation loss: 2.036421227198775

Epoch: 6| Step: 7
Training loss: 2.5006728172302246
Validation loss: 2.0569849168100665

Epoch: 6| Step: 8
Training loss: 1.2010213136672974
Validation loss: 2.0551331991790445

Epoch: 6| Step: 9
Training loss: 2.239096164703369
Validation loss: 2.0619118495654036

Epoch: 6| Step: 10
Training loss: 1.6872241497039795
Validation loss: 2.026397676878078

Epoch: 6| Step: 11
Training loss: 1.529990792274475
Validation loss: 2.0376331383182156

Epoch: 6| Step: 12
Training loss: 2.679600715637207
Validation loss: 2.038256332438479

Epoch: 6| Step: 13
Training loss: 1.723140001296997
Validation loss: 2.0849971745603826

Epoch: 138| Step: 0
Training loss: 2.341789722442627
Validation loss: 2.0865667622576476

Epoch: 6| Step: 1
Training loss: 1.8821966648101807
Validation loss: 2.037847908594275

Epoch: 6| Step: 2
Training loss: 2.023881673812866
Validation loss: 2.0443982706275037

Epoch: 6| Step: 3
Training loss: 1.7537226676940918
Validation loss: 2.06862138676387

Epoch: 6| Step: 4
Training loss: 2.2017903327941895
Validation loss: 2.0878100318293416

Epoch: 6| Step: 5
Training loss: 2.0014634132385254
Validation loss: 2.015162209028839

Epoch: 6| Step: 6
Training loss: 1.7478352785110474
Validation loss: 2.056172163255753

Epoch: 6| Step: 7
Training loss: 1.5017294883728027
Validation loss: 2.0482749990237656

Epoch: 6| Step: 8
Training loss: 2.065422773361206
Validation loss: 2.0785152809594267

Epoch: 6| Step: 9
Training loss: 1.329591989517212
Validation loss: 2.046406794619817

Epoch: 6| Step: 10
Training loss: 2.2797818183898926
Validation loss: 2.060850417742165

Epoch: 6| Step: 11
Training loss: 2.3853893280029297
Validation loss: 2.0664362202408495

Epoch: 6| Step: 12
Training loss: 2.5017948150634766
Validation loss: 2.0512105316244145

Epoch: 6| Step: 13
Training loss: 3.1604461669921875
Validation loss: 2.0564196853227514

Epoch: 139| Step: 0
Training loss: 1.5015385150909424
Validation loss: 2.0759192166789884

Epoch: 6| Step: 1
Training loss: 2.653118133544922
Validation loss: 2.0632035847633117

Epoch: 6| Step: 2
Training loss: 2.483229637145996
Validation loss: 2.0413983906469038

Epoch: 6| Step: 3
Training loss: 2.3377456665039062
Validation loss: 2.044044158791983

Epoch: 6| Step: 4
Training loss: 2.21828556060791
Validation loss: 2.05256917912473

Epoch: 6| Step: 5
Training loss: 2.0408833026885986
Validation loss: 2.04154291460591

Epoch: 6| Step: 6
Training loss: 1.9805898666381836
Validation loss: 2.0896086026263494

Epoch: 6| Step: 7
Training loss: 2.3904941082000732
Validation loss: 2.033452490324615

Epoch: 6| Step: 8
Training loss: 2.1711549758911133
Validation loss: 2.0623810752745597

Epoch: 6| Step: 9
Training loss: 1.8939905166625977
Validation loss: 2.0575528170472834

Epoch: 6| Step: 10
Training loss: 1.3338632583618164
Validation loss: 2.0526886909238753

Epoch: 6| Step: 11
Training loss: 1.6987204551696777
Validation loss: 2.044397146471085

Epoch: 6| Step: 12
Training loss: 2.209623336791992
Validation loss: 2.072974334480942

Epoch: 6| Step: 13
Training loss: 2.1145687103271484
Validation loss: 2.048782366578297

Epoch: 140| Step: 0
Training loss: 1.466896414756775
Validation loss: 2.0361981340633926

Epoch: 6| Step: 1
Training loss: 2.027902841567993
Validation loss: 2.015011026013282

Epoch: 6| Step: 2
Training loss: 1.6133747100830078
Validation loss: 2.022304811785298

Epoch: 6| Step: 3
Training loss: 2.758127212524414
Validation loss: 2.05227647032789

Epoch: 6| Step: 4
Training loss: 2.354384422302246
Validation loss: 2.060292855385811

Epoch: 6| Step: 5
Training loss: 1.5339462757110596
Validation loss: 2.0691212966877925

Epoch: 6| Step: 6
Training loss: 2.027125835418701
Validation loss: 2.059333820496836

Epoch: 6| Step: 7
Training loss: 2.1976571083068848
Validation loss: 2.0573303276492703

Epoch: 6| Step: 8
Training loss: 1.6353789567947388
Validation loss: 2.0850418177984094

Epoch: 6| Step: 9
Training loss: 1.5661873817443848
Validation loss: 2.05810280769102

Epoch: 6| Step: 10
Training loss: 2.6776347160339355
Validation loss: 2.032372841271021

Epoch: 6| Step: 11
Training loss: 1.7559912204742432
Validation loss: 2.070454666691442

Epoch: 6| Step: 12
Training loss: 2.1909687519073486
Validation loss: 2.0326097883203977

Epoch: 6| Step: 13
Training loss: 3.0841946601867676
Validation loss: 2.071123468619521

Epoch: 141| Step: 0
Training loss: 2.7459330558776855
Validation loss: 2.0496870215221117

Epoch: 6| Step: 1
Training loss: 1.9253489971160889
Validation loss: 2.039926008511615

Epoch: 6| Step: 2
Training loss: 2.100576877593994
Validation loss: 2.077415382990273

Epoch: 6| Step: 3
Training loss: 1.5388108491897583
Validation loss: 2.0414788569173505

Epoch: 6| Step: 4
Training loss: 1.7154104709625244
Validation loss: 2.073840715551889

Epoch: 6| Step: 5
Training loss: 2.2350776195526123
Validation loss: 2.0550210937376945

Epoch: 6| Step: 6
Training loss: 1.8267509937286377
Validation loss: 2.0519957337328183

Epoch: 6| Step: 7
Training loss: 1.6719019412994385
Validation loss: 2.0533326338696223

Epoch: 6| Step: 8
Training loss: 2.1280159950256348
Validation loss: 2.0823019332783197

Epoch: 6| Step: 9
Training loss: 2.1692214012145996
Validation loss: 2.0321340266094414

Epoch: 6| Step: 10
Training loss: 1.5378082990646362
Validation loss: 2.0376223633366246

Epoch: 6| Step: 11
Training loss: 1.9405992031097412
Validation loss: 2.034872426781603

Epoch: 6| Step: 12
Training loss: 2.268022298812866
Validation loss: 2.0704018890216784

Epoch: 6| Step: 13
Training loss: 2.699779987335205
Validation loss: 2.057729018631802

Epoch: 142| Step: 0
Training loss: 2.1305019855499268
Validation loss: 2.029242509154863

Epoch: 6| Step: 1
Training loss: 1.6934740543365479
Validation loss: 2.0460478977490495

Epoch: 6| Step: 2
Training loss: 2.7730467319488525
Validation loss: 2.059017542869814

Epoch: 6| Step: 3
Training loss: 1.904962420463562
Validation loss: 2.051783974452685

Epoch: 6| Step: 4
Training loss: 2.243807554244995
Validation loss: 2.0270467817142444

Epoch: 6| Step: 5
Training loss: 1.46305251121521
Validation loss: 2.041248583024548

Epoch: 6| Step: 6
Training loss: 2.660006523132324
Validation loss: 2.0529407301256732

Epoch: 6| Step: 7
Training loss: 2.656982660293579
Validation loss: 2.0467764754449167

Epoch: 6| Step: 8
Training loss: 1.830204725265503
Validation loss: 2.0404773527576077

Epoch: 6| Step: 9
Training loss: 1.9085700511932373
Validation loss: 2.032482172853203

Epoch: 6| Step: 10
Training loss: 1.6829204559326172
Validation loss: 2.0343563043943016

Epoch: 6| Step: 11
Training loss: 1.0386836528778076
Validation loss: 2.0597720735816547

Epoch: 6| Step: 12
Training loss: 2.3878798484802246
Validation loss: 2.0688191626661565

Epoch: 6| Step: 13
Training loss: 1.7491945028305054
Validation loss: 2.029302704718805

Epoch: 143| Step: 0
Training loss: 1.3021093606948853
Validation loss: 2.0385788025394564

Epoch: 6| Step: 1
Training loss: 2.4940948486328125
Validation loss: 2.0578725722528275

Epoch: 6| Step: 2
Training loss: 2.6792197227478027
Validation loss: 2.030207203280541

Epoch: 6| Step: 3
Training loss: 1.922744870185852
Validation loss: 2.052473888602308

Epoch: 6| Step: 4
Training loss: 2.1443495750427246
Validation loss: 2.0347231318873744

Epoch: 6| Step: 5
Training loss: 1.680532455444336
Validation loss: 2.014655866930562

Epoch: 6| Step: 6
Training loss: 2.1154489517211914
Validation loss: 2.0482207728970434

Epoch: 6| Step: 7
Training loss: 1.288339376449585
Validation loss: 2.061204807732695

Epoch: 6| Step: 8
Training loss: 2.407742977142334
Validation loss: 2.058118365144217

Epoch: 6| Step: 9
Training loss: 2.2943496704101562
Validation loss: 2.0890607718498475

Epoch: 6| Step: 10
Training loss: 2.117548704147339
Validation loss: 2.081582607761506

Epoch: 6| Step: 11
Training loss: 2.262143135070801
Validation loss: 2.052454512606385

Epoch: 6| Step: 12
Training loss: 1.6056491136550903
Validation loss: 2.046899067458286

Epoch: 6| Step: 13
Training loss: 1.7307950258255005
Validation loss: 2.0503775535091275

Epoch: 144| Step: 0
Training loss: 1.7299033403396606
Validation loss: 2.057558777511761

Epoch: 6| Step: 1
Training loss: 1.875103235244751
Validation loss: 2.023600357835011

Epoch: 6| Step: 2
Training loss: 1.2635924816131592
Validation loss: 2.033238839077693

Epoch: 6| Step: 3
Training loss: 2.333590507507324
Validation loss: 2.0519311453706477

Epoch: 6| Step: 4
Training loss: 1.6088571548461914
Validation loss: 2.026521102074654

Epoch: 6| Step: 5
Training loss: 2.2742600440979004
Validation loss: 2.041561645846213

Epoch: 6| Step: 6
Training loss: 1.5082600116729736
Validation loss: 2.0153741375092538

Epoch: 6| Step: 7
Training loss: 2.54355525970459
Validation loss: 2.0375271792052896

Epoch: 6| Step: 8
Training loss: 1.7729203701019287
Validation loss: 2.0440179365937428

Epoch: 6| Step: 9
Training loss: 2.292788028717041
Validation loss: 2.073889176050822

Epoch: 6| Step: 10
Training loss: 2.133761405944824
Validation loss: 2.0722740106685187

Epoch: 6| Step: 11
Training loss: 2.614455461502075
Validation loss: 2.0480507022591046

Epoch: 6| Step: 12
Training loss: 2.1400623321533203
Validation loss: 2.0521201241400933

Epoch: 6| Step: 13
Training loss: 2.129887104034424
Validation loss: 2.0719684580320954

Epoch: 145| Step: 0
Training loss: 2.4182865619659424
Validation loss: 2.065444541233842

Epoch: 6| Step: 1
Training loss: 2.1367616653442383
Validation loss: 2.0040440354295956

Epoch: 6| Step: 2
Training loss: 1.856884479522705
Validation loss: 2.0435531767465736

Epoch: 6| Step: 3
Training loss: 1.0615234375
Validation loss: 2.031105331195298

Epoch: 6| Step: 4
Training loss: 1.4602729082107544
Validation loss: 2.059308801927874

Epoch: 6| Step: 5
Training loss: 1.8609782457351685
Validation loss: 2.0519144073609383

Epoch: 6| Step: 6
Training loss: 2.111057758331299
Validation loss: 2.0321054548345585

Epoch: 6| Step: 7
Training loss: 2.003849506378174
Validation loss: 2.0568941395769835

Epoch: 6| Step: 8
Training loss: 1.677807092666626
Validation loss: 2.0490929016502957

Epoch: 6| Step: 9
Training loss: 2.235903263092041
Validation loss: 2.0792343847213255

Epoch: 6| Step: 10
Training loss: 3.2323689460754395
Validation loss: 2.034939019910751

Epoch: 6| Step: 11
Training loss: 2.792694568634033
Validation loss: 2.0225009777212657

Epoch: 6| Step: 12
Training loss: 1.9179236888885498
Validation loss: 2.0336448992452314

Epoch: 6| Step: 13
Training loss: 1.4784350395202637
Validation loss: 2.006747805944053

Epoch: 146| Step: 0
Training loss: 1.918745517730713
Validation loss: 2.0301278829574585

Epoch: 6| Step: 1
Training loss: 2.4682557582855225
Validation loss: 1.9980266145480576

Epoch: 6| Step: 2
Training loss: 2.3560054302215576
Validation loss: 2.0486583145715858

Epoch: 6| Step: 3
Training loss: 2.155761241912842
Validation loss: 2.061377117710729

Epoch: 6| Step: 4
Training loss: 1.3177237510681152
Validation loss: 2.026707951740552

Epoch: 6| Step: 5
Training loss: 1.9144940376281738
Validation loss: 2.052460209015877

Epoch: 6| Step: 6
Training loss: 2.375243663787842
Validation loss: 2.011357568925427

Epoch: 6| Step: 7
Training loss: 1.9772722721099854
Validation loss: 2.0500203345411565

Epoch: 6| Step: 8
Training loss: 1.7863892316818237
Validation loss: 2.053381363550822

Epoch: 6| Step: 9
Training loss: 1.7737507820129395
Validation loss: 2.0326855105738484

Epoch: 6| Step: 10
Training loss: 2.1488218307495117
Validation loss: 2.0252721591662337

Epoch: 6| Step: 11
Training loss: 1.392444133758545
Validation loss: 2.0595724608308528

Epoch: 6| Step: 12
Training loss: 2.0805158615112305
Validation loss: 2.0224153944241103

Epoch: 6| Step: 13
Training loss: 2.528245210647583
Validation loss: 2.014819302866536

Epoch: 147| Step: 0
Training loss: 2.156686782836914
Validation loss: 2.015018101661436

Epoch: 6| Step: 1
Training loss: 2.7383387088775635
Validation loss: 2.0296711998601116

Epoch: 6| Step: 2
Training loss: 2.1173152923583984
Validation loss: 2.0472384819420437

Epoch: 6| Step: 3
Training loss: 1.6233069896697998
Validation loss: 2.0569231920344855

Epoch: 6| Step: 4
Training loss: 0.8473542928695679
Validation loss: 2.025012440578912

Epoch: 6| Step: 5
Training loss: 2.1008031368255615
Validation loss: 2.030151013405092

Epoch: 6| Step: 6
Training loss: 2.885598659515381
Validation loss: 2.041061432130875

Epoch: 6| Step: 7
Training loss: 2.356951951980591
Validation loss: 2.0470586720333306

Epoch: 6| Step: 8
Training loss: 1.867113709449768
Validation loss: 2.0556069663775864

Epoch: 6| Step: 9
Training loss: 1.7109179496765137
Validation loss: 2.036406332446683

Epoch: 6| Step: 10
Training loss: 1.8517186641693115
Validation loss: 2.0638268968110443

Epoch: 6| Step: 11
Training loss: 1.9416637420654297
Validation loss: 2.058701761307255

Epoch: 6| Step: 12
Training loss: 1.8947566747665405
Validation loss: 2.0778774356329315

Epoch: 6| Step: 13
Training loss: 2.1905553340911865
Validation loss: 2.0443230636658205

Epoch: 148| Step: 0
Training loss: 1.7997416257858276
Validation loss: 2.061202238964778

Epoch: 6| Step: 1
Training loss: 2.2225866317749023
Validation loss: 2.033848095965642

Epoch: 6| Step: 2
Training loss: 1.9247492551803589
Validation loss: 2.0662431306736444

Epoch: 6| Step: 3
Training loss: 2.1188771724700928
Validation loss: 2.0354029260655886

Epoch: 6| Step: 4
Training loss: 2.2544009685516357
Validation loss: 2.071343445008801

Epoch: 6| Step: 5
Training loss: 1.7544667720794678
Validation loss: 2.0376962307960755

Epoch: 6| Step: 6
Training loss: 2.451679229736328
Validation loss: 2.017919368641351

Epoch: 6| Step: 7
Training loss: 1.9878325462341309
Validation loss: 2.05174167694584

Epoch: 6| Step: 8
Training loss: 1.7396351099014282
Validation loss: 2.0414248410091607

Epoch: 6| Step: 9
Training loss: 2.4243550300598145
Validation loss: 2.030667997175647

Epoch: 6| Step: 10
Training loss: 1.746357798576355
Validation loss: 2.0371724572232974

Epoch: 6| Step: 11
Training loss: 1.4811432361602783
Validation loss: 2.0447140534718833

Epoch: 6| Step: 12
Training loss: 2.207223415374756
Validation loss: 2.0560627227188437

Epoch: 6| Step: 13
Training loss: 1.9956223964691162
Validation loss: 2.021238365480977

Epoch: 149| Step: 0
Training loss: 2.870961904525757
Validation loss: 2.051351877950853

Epoch: 6| Step: 1
Training loss: 1.9993057250976562
Validation loss: 2.057592479131555

Epoch: 6| Step: 2
Training loss: 2.507408618927002
Validation loss: 2.018254332644965

Epoch: 6| Step: 3
Training loss: 1.8557639122009277
Validation loss: 2.0216645374093005

Epoch: 6| Step: 4
Training loss: 1.3552329540252686
Validation loss: 2.032276802165534

Epoch: 6| Step: 5
Training loss: 1.6436803340911865
Validation loss: 2.02835407692899

Epoch: 6| Step: 6
Training loss: 1.4643547534942627
Validation loss: 2.037220283221173

Epoch: 6| Step: 7
Training loss: 1.7578691244125366
Validation loss: 2.0459432653201524

Epoch: 6| Step: 8
Training loss: 1.9924107789993286
Validation loss: 2.0415404406926965

Epoch: 6| Step: 9
Training loss: 1.823103666305542
Validation loss: 2.0308281663925416

Epoch: 6| Step: 10
Training loss: 2.0535523891448975
Validation loss: 2.0981091901820195

Epoch: 6| Step: 11
Training loss: 2.341658592224121
Validation loss: 2.037183412941553

Epoch: 6| Step: 12
Training loss: 1.9907768964767456
Validation loss: 2.029759404479816

Epoch: 6| Step: 13
Training loss: 2.841947078704834
Validation loss: 2.0417641029563

Epoch: 150| Step: 0
Training loss: 1.839216947555542
Validation loss: 2.0299931213419926

Epoch: 6| Step: 1
Training loss: 2.086627960205078
Validation loss: 2.042476643798172

Epoch: 6| Step: 2
Training loss: 2.0810699462890625
Validation loss: 2.024289038873488

Epoch: 6| Step: 3
Training loss: 2.7277636528015137
Validation loss: 2.027297906978156

Epoch: 6| Step: 4
Training loss: 2.370940685272217
Validation loss: 2.029337106212493

Epoch: 6| Step: 5
Training loss: 1.8818228244781494
Validation loss: 2.0427818990522817

Epoch: 6| Step: 6
Training loss: 1.7520262002944946
Validation loss: 2.0230416277403473

Epoch: 6| Step: 7
Training loss: 1.729008436203003
Validation loss: 1.9747420126391995

Epoch: 6| Step: 8
Training loss: 2.710488796234131
Validation loss: 2.071363123514319

Epoch: 6| Step: 9
Training loss: 1.4700472354888916
Validation loss: 2.0336453671096475

Epoch: 6| Step: 10
Training loss: 2.125271797180176
Validation loss: 2.034206780054236

Epoch: 6| Step: 11
Training loss: 1.5658481121063232
Validation loss: 2.0274034046357676

Epoch: 6| Step: 12
Training loss: 1.7779814004898071
Validation loss: 2.033845457979428

Epoch: 6| Step: 13
Training loss: 2.1375443935394287
Validation loss: 2.0036490040440715

Testing loss: 2.0388366487291125

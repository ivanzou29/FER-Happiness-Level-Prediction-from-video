Epoch: 1| Step: 0
Training loss: 6.275432109832764
Validation loss: 5.944928656342209

Epoch: 5| Step: 1
Training loss: 5.22385311126709
Validation loss: 5.936162887081023

Epoch: 5| Step: 2
Training loss: 5.833310127258301
Validation loss: 5.931638758669617

Epoch: 5| Step: 3
Training loss: 5.522066593170166
Validation loss: 5.921844769549626

Epoch: 5| Step: 4
Training loss: 6.179642677307129
Validation loss: 5.912873273254723

Epoch: 5| Step: 5
Training loss: 5.780737400054932
Validation loss: 5.906760836160311

Epoch: 5| Step: 6
Training loss: 6.26479434967041
Validation loss: 5.8984511334409

Epoch: 5| Step: 7
Training loss: 4.808960437774658
Validation loss: 5.890490203775386

Epoch: 5| Step: 8
Training loss: 5.837889671325684
Validation loss: 5.8838467187778924

Epoch: 5| Step: 9
Training loss: 5.774107456207275
Validation loss: 5.876686080809562

Epoch: 5| Step: 10
Training loss: 5.12302827835083
Validation loss: 5.870787379562214

Epoch: 2| Step: 0
Training loss: 5.946142196655273
Validation loss: 5.861909163895474

Epoch: 5| Step: 1
Training loss: 5.356010913848877
Validation loss: 5.855940141985493

Epoch: 5| Step: 2
Training loss: 4.837924480438232
Validation loss: 5.846320449665028

Epoch: 5| Step: 3
Training loss: 5.911074638366699
Validation loss: 5.843911304268786

Epoch: 5| Step: 4
Training loss: 6.883504390716553
Validation loss: 5.835090929462064

Epoch: 5| Step: 5
Training loss: 6.476931571960449
Validation loss: 5.826608273290819

Epoch: 5| Step: 6
Training loss: 5.461340427398682
Validation loss: 5.822601795196533

Epoch: 5| Step: 7
Training loss: 5.288842678070068
Validation loss: 5.81509869585755

Epoch: 5| Step: 8
Training loss: 5.642223358154297
Validation loss: 5.810394722928283

Epoch: 5| Step: 9
Training loss: 4.920530319213867
Validation loss: 5.804619178977064

Epoch: 5| Step: 10
Training loss: 5.029890537261963
Validation loss: 5.797261268861832

Epoch: 3| Step: 0
Training loss: 4.551048755645752
Validation loss: 5.788458511393557

Epoch: 5| Step: 1
Training loss: 6.692704200744629
Validation loss: 5.784077757148332

Epoch: 5| Step: 2
Training loss: 6.427999973297119
Validation loss: 5.7780879543673604

Epoch: 5| Step: 3
Training loss: 4.9568772315979
Validation loss: 5.770641249995077

Epoch: 5| Step: 4
Training loss: 4.913636684417725
Validation loss: 5.761403509365615

Epoch: 5| Step: 5
Training loss: 5.925518989562988
Validation loss: 5.757713420416719

Epoch: 5| Step: 6
Training loss: 5.773678779602051
Validation loss: 5.750173778944118

Epoch: 5| Step: 7
Training loss: 4.9175615310668945
Validation loss: 5.74602424457509

Epoch: 5| Step: 8
Training loss: 5.4445695877075195
Validation loss: 5.737578622756466

Epoch: 5| Step: 9
Training loss: 5.485690116882324
Validation loss: 5.732900952780119

Epoch: 5| Step: 10
Training loss: 6.034739017486572
Validation loss: 5.727292814562397

Epoch: 4| Step: 0
Training loss: 4.543889045715332
Validation loss: 5.720504412087061

Epoch: 5| Step: 1
Training loss: 6.0044379234313965
Validation loss: 5.712717892021261

Epoch: 5| Step: 2
Training loss: 4.601820945739746
Validation loss: 5.7069898061854865

Epoch: 5| Step: 3
Training loss: 6.013083457946777
Validation loss: 5.70100268497262

Epoch: 5| Step: 4
Training loss: 5.01625919342041
Validation loss: 5.6939703931090655

Epoch: 5| Step: 5
Training loss: 6.1154093742370605
Validation loss: 5.688646701074416

Epoch: 5| Step: 6
Training loss: 5.508416175842285
Validation loss: 5.680224018712198

Epoch: 5| Step: 7
Training loss: 5.1833062171936035
Validation loss: 5.674673054807929

Epoch: 5| Step: 8
Training loss: 5.196120738983154
Validation loss: 5.66848922032182

Epoch: 5| Step: 9
Training loss: 5.845458507537842
Validation loss: 5.660464835423295

Epoch: 5| Step: 10
Training loss: 6.332739353179932
Validation loss: 5.6533127753965315

Epoch: 5| Step: 0
Training loss: 5.473474979400635
Validation loss: 5.647642638093682

Epoch: 5| Step: 1
Training loss: 5.9265456199646
Validation loss: 5.640933944332984

Epoch: 5| Step: 2
Training loss: 4.372993469238281
Validation loss: 5.632376870801372

Epoch: 5| Step: 3
Training loss: 6.6698174476623535
Validation loss: 5.625539277189521

Epoch: 5| Step: 4
Training loss: 4.839959621429443
Validation loss: 5.616267865703952

Epoch: 5| Step: 5
Training loss: 4.694985389709473
Validation loss: 5.6096191303704375

Epoch: 5| Step: 6
Training loss: 4.497884273529053
Validation loss: 5.604264900248538

Epoch: 5| Step: 7
Training loss: 6.272907257080078
Validation loss: 5.593518564777989

Epoch: 5| Step: 8
Training loss: 5.369454383850098
Validation loss: 5.587515354156494

Epoch: 5| Step: 9
Training loss: 6.044285774230957
Validation loss: 5.576305753441267

Epoch: 5| Step: 10
Training loss: 5.152393341064453
Validation loss: 5.569883223502867

Epoch: 6| Step: 0
Training loss: 4.947810173034668
Validation loss: 5.560498027391331

Epoch: 5| Step: 1
Training loss: 5.886772155761719
Validation loss: 5.552237095371369

Epoch: 5| Step: 2
Training loss: 5.319478511810303
Validation loss: 5.54497379385015

Epoch: 5| Step: 3
Training loss: 5.538130760192871
Validation loss: 5.537492439311038

Epoch: 5| Step: 4
Training loss: 4.237195014953613
Validation loss: 5.5284552779248965

Epoch: 5| Step: 5
Training loss: 5.83621072769165
Validation loss: 5.51858699962657

Epoch: 5| Step: 6
Training loss: 6.492388725280762
Validation loss: 5.511415194439632

Epoch: 5| Step: 7
Training loss: 5.02941370010376
Validation loss: 5.49970914471534

Epoch: 5| Step: 8
Training loss: 5.260013103485107
Validation loss: 5.491742575040427

Epoch: 5| Step: 9
Training loss: 5.788664817810059
Validation loss: 5.483611552946029

Epoch: 5| Step: 10
Training loss: 3.7378463745117188
Validation loss: 5.473387225981681

Epoch: 7| Step: 0
Training loss: 6.032402038574219
Validation loss: 5.462802328089232

Epoch: 5| Step: 1
Training loss: 4.5731024742126465
Validation loss: 5.4544978859604045

Epoch: 5| Step: 2
Training loss: 4.190196990966797
Validation loss: 5.446747892646379

Epoch: 5| Step: 3
Training loss: 6.413646697998047
Validation loss: 5.4347842534383135

Epoch: 5| Step: 4
Training loss: 4.131277084350586
Validation loss: 5.42646207604357

Epoch: 5| Step: 5
Training loss: 5.873256206512451
Validation loss: 5.4142145751624975

Epoch: 5| Step: 6
Training loss: 6.233597278594971
Validation loss: 5.40406560385099

Epoch: 5| Step: 7
Training loss: 5.689481735229492
Validation loss: 5.394484222576183

Epoch: 5| Step: 8
Training loss: 4.976491451263428
Validation loss: 5.382271679498816

Epoch: 5| Step: 9
Training loss: 4.147469520568848
Validation loss: 5.374738703491867

Epoch: 5| Step: 10
Training loss: 4.8556904792785645
Validation loss: 5.362516121197772

Epoch: 8| Step: 0
Training loss: 5.6498942375183105
Validation loss: 5.354630201093612

Epoch: 5| Step: 1
Training loss: 5.361830711364746
Validation loss: 5.34329467691401

Epoch: 5| Step: 2
Training loss: 3.405060291290283
Validation loss: 5.329441260266048

Epoch: 5| Step: 3
Training loss: 4.922240257263184
Validation loss: 5.319651962608419

Epoch: 5| Step: 4
Training loss: 5.1198225021362305
Validation loss: 5.308636408980175

Epoch: 5| Step: 5
Training loss: 3.6874210834503174
Validation loss: 5.299028088969569

Epoch: 5| Step: 6
Training loss: 6.556484222412109
Validation loss: 5.2864038508425475

Epoch: 5| Step: 7
Training loss: 5.549136638641357
Validation loss: 5.277017034510131

Epoch: 5| Step: 8
Training loss: 4.877140998840332
Validation loss: 5.263423755604734

Epoch: 5| Step: 9
Training loss: 4.914345741271973
Validation loss: 5.251089854906964

Epoch: 5| Step: 10
Training loss: 5.913829803466797
Validation loss: 5.2425818340752715

Epoch: 9| Step: 0
Training loss: 4.813754558563232
Validation loss: 5.2269458924570396

Epoch: 5| Step: 1
Training loss: 6.095796585083008
Validation loss: 5.218393120714413

Epoch: 5| Step: 2
Training loss: 4.344913482666016
Validation loss: 5.20172982574791

Epoch: 5| Step: 3
Training loss: 4.955996036529541
Validation loss: 5.186761891970071

Epoch: 5| Step: 4
Training loss: 4.277893543243408
Validation loss: 5.177791836441204

Epoch: 5| Step: 5
Training loss: 6.167981147766113
Validation loss: 5.161992924187773

Epoch: 5| Step: 6
Training loss: 4.363561153411865
Validation loss: 5.1513528054760345

Epoch: 5| Step: 7
Training loss: 4.328060626983643
Validation loss: 5.138459087699972

Epoch: 5| Step: 8
Training loss: 4.870129585266113
Validation loss: 5.1274227993462675

Epoch: 5| Step: 9
Training loss: 5.316501617431641
Validation loss: 5.1098901123128915

Epoch: 5| Step: 10
Training loss: 4.783782958984375
Validation loss: 5.095287184561452

Epoch: 10| Step: 0
Training loss: 4.8576178550720215
Validation loss: 5.082039038340251

Epoch: 5| Step: 1
Training loss: 3.2597649097442627
Validation loss: 5.0702385030766965

Epoch: 5| Step: 2
Training loss: 5.306694984436035
Validation loss: 5.057012301619335

Epoch: 5| Step: 3
Training loss: 4.315225124359131
Validation loss: 5.041573283492878

Epoch: 5| Step: 4
Training loss: 4.492547035217285
Validation loss: 5.025446122692477

Epoch: 5| Step: 5
Training loss: 5.841711044311523
Validation loss: 5.01334197034118

Epoch: 5| Step: 6
Training loss: 5.735511302947998
Validation loss: 4.99315236204414

Epoch: 5| Step: 7
Training loss: 4.389125823974609
Validation loss: 4.980797649711691

Epoch: 5| Step: 8
Training loss: 4.357449054718018
Validation loss: 4.968079167027628

Epoch: 5| Step: 9
Training loss: 5.071646213531494
Validation loss: 4.954526767935804

Epoch: 5| Step: 10
Training loss: 5.047288417816162
Validation loss: 4.937337916384461

Epoch: 11| Step: 0
Training loss: 4.317021369934082
Validation loss: 4.922238273005331

Epoch: 5| Step: 1
Training loss: 4.578926086425781
Validation loss: 4.909540699374292

Epoch: 5| Step: 2
Training loss: 4.8549299240112305
Validation loss: 4.892869410976287

Epoch: 5| Step: 3
Training loss: 5.031207084655762
Validation loss: 4.876463546547838

Epoch: 5| Step: 4
Training loss: 5.383927822113037
Validation loss: 4.857842296682378

Epoch: 5| Step: 5
Training loss: 2.9569332599639893
Validation loss: 4.843885683244275

Epoch: 5| Step: 6
Training loss: 5.223985195159912
Validation loss: 4.831390944860315

Epoch: 5| Step: 7
Training loss: 4.219622611999512
Validation loss: 4.816473963440106

Epoch: 5| Step: 8
Training loss: 5.445530414581299
Validation loss: 4.79979157704179

Epoch: 5| Step: 9
Training loss: 4.63044548034668
Validation loss: 4.7857301671017884

Epoch: 5| Step: 10
Training loss: 4.118220329284668
Validation loss: 4.763977681436846

Epoch: 12| Step: 0
Training loss: 5.619519233703613
Validation loss: 4.750920295715332

Epoch: 5| Step: 1
Training loss: 5.797833442687988
Validation loss: 4.730107732998428

Epoch: 5| Step: 2
Training loss: 3.7070152759552
Validation loss: 4.710650138957526

Epoch: 5| Step: 3
Training loss: 5.595510959625244
Validation loss: 4.696622546001147

Epoch: 5| Step: 4
Training loss: 4.327840328216553
Validation loss: 4.680904480718797

Epoch: 5| Step: 5
Training loss: 4.012870788574219
Validation loss: 4.664802756360782

Epoch: 5| Step: 6
Training loss: 3.6256961822509766
Validation loss: 4.6454183722055085

Epoch: 5| Step: 7
Training loss: 2.941708564758301
Validation loss: 4.626940988725232

Epoch: 5| Step: 8
Training loss: 4.2467756271362305
Validation loss: 4.611706513230518

Epoch: 5| Step: 9
Training loss: 4.665108680725098
Validation loss: 4.586878099749165

Epoch: 5| Step: 10
Training loss: 4.162047386169434
Validation loss: 4.573093173324421

Epoch: 13| Step: 0
Training loss: 4.169812202453613
Validation loss: 4.555004847946988

Epoch: 5| Step: 1
Training loss: 3.2871956825256348
Validation loss: 4.536580636937131

Epoch: 5| Step: 2
Training loss: 4.6016364097595215
Validation loss: 4.5186655470120005

Epoch: 5| Step: 3
Training loss: 4.811195373535156
Validation loss: 4.495043098285634

Epoch: 5| Step: 4
Training loss: 4.562516689300537
Validation loss: 4.483047116187311

Epoch: 5| Step: 5
Training loss: 4.598351955413818
Validation loss: 4.465362748792095

Epoch: 5| Step: 6
Training loss: 4.140101432800293
Validation loss: 4.435844890532955

Epoch: 5| Step: 7
Training loss: 4.850785255432129
Validation loss: 4.420101893845425

Epoch: 5| Step: 8
Training loss: 3.5421996116638184
Validation loss: 4.398097602269983

Epoch: 5| Step: 9
Training loss: 3.680107593536377
Validation loss: 4.379426128120833

Epoch: 5| Step: 10
Training loss: 4.290022850036621
Validation loss: 4.361635372202883

Epoch: 14| Step: 0
Training loss: 4.148575305938721
Validation loss: 4.339917018849363

Epoch: 5| Step: 1
Training loss: 4.292290687561035
Validation loss: 4.32401486878754

Epoch: 5| Step: 2
Training loss: 4.664736747741699
Validation loss: 4.289058798102922

Epoch: 5| Step: 3
Training loss: 3.179671287536621
Validation loss: 4.270194033140777

Epoch: 5| Step: 4
Training loss: 4.498154640197754
Validation loss: 4.2524021056390575

Epoch: 5| Step: 5
Training loss: 3.9379963874816895
Validation loss: 4.229857288381105

Epoch: 5| Step: 6
Training loss: 3.524608612060547
Validation loss: 4.206619913860034

Epoch: 5| Step: 7
Training loss: 3.5293006896972656
Validation loss: 4.185611860726469

Epoch: 5| Step: 8
Training loss: 3.2894558906555176
Validation loss: 4.170915252418928

Epoch: 5| Step: 9
Training loss: 4.446927070617676
Validation loss: 4.1454586880181425

Epoch: 5| Step: 10
Training loss: 4.838840484619141
Validation loss: 4.12944403002339

Epoch: 15| Step: 0
Training loss: 3.693277359008789
Validation loss: 4.110412407946843

Epoch: 5| Step: 1
Training loss: 3.7896804809570312
Validation loss: 4.075970095972861

Epoch: 5| Step: 2
Training loss: 3.572319507598877
Validation loss: 4.070570453520744

Epoch: 5| Step: 3
Training loss: 4.173212051391602
Validation loss: 4.044798015266337

Epoch: 5| Step: 4
Training loss: 4.35908317565918
Validation loss: 4.025901943124751

Epoch: 5| Step: 5
Training loss: 3.9731392860412598
Validation loss: 4.003364116914811

Epoch: 5| Step: 6
Training loss: 3.7376503944396973
Validation loss: 3.985473463612218

Epoch: 5| Step: 7
Training loss: 3.7366485595703125
Validation loss: 3.9468803918489845

Epoch: 5| Step: 8
Training loss: 3.519306182861328
Validation loss: 3.938460639728013

Epoch: 5| Step: 9
Training loss: 3.5962166786193848
Validation loss: 3.9088707457306566

Epoch: 5| Step: 10
Training loss: 3.730536699295044
Validation loss: 3.8997873747220604

Epoch: 16| Step: 0
Training loss: 3.918914318084717
Validation loss: 3.869398868212136

Epoch: 5| Step: 1
Training loss: 3.576188564300537
Validation loss: 3.846181059396395

Epoch: 5| Step: 2
Training loss: 3.6977152824401855
Validation loss: 3.83226607179129

Epoch: 5| Step: 3
Training loss: 4.05049991607666
Validation loss: 3.8019659878105245

Epoch: 5| Step: 4
Training loss: 3.776569366455078
Validation loss: 3.779023637053787

Epoch: 5| Step: 5
Training loss: 2.9754996299743652
Validation loss: 3.759058508821713

Epoch: 5| Step: 6
Training loss: 4.4754414558410645
Validation loss: 3.7324922033535537

Epoch: 5| Step: 7
Training loss: 3.1104736328125
Validation loss: 3.7157198100961666

Epoch: 5| Step: 8
Training loss: 3.8535423278808594
Validation loss: 3.6793054252542476

Epoch: 5| Step: 9
Training loss: 3.4853217601776123
Validation loss: 3.6595228256717807

Epoch: 5| Step: 10
Training loss: 2.568498134613037
Validation loss: 3.6391016078251663

Epoch: 17| Step: 0
Training loss: 4.068561553955078
Validation loss: 3.611867291952974

Epoch: 5| Step: 1
Training loss: 3.4018821716308594
Validation loss: 3.588372384348223

Epoch: 5| Step: 2
Training loss: 4.09030294418335
Validation loss: 3.575676502720002

Epoch: 5| Step: 3
Training loss: 3.657749891281128
Validation loss: 3.5432254627186763

Epoch: 5| Step: 4
Training loss: 3.1727607250213623
Validation loss: 3.527818590082148

Epoch: 5| Step: 5
Training loss: 3.4707541465759277
Validation loss: 3.5009023758672897

Epoch: 5| Step: 6
Training loss: 3.248335599899292
Validation loss: 3.4800956633783158

Epoch: 5| Step: 7
Training loss: 2.9353575706481934
Validation loss: 3.456825453747985

Epoch: 5| Step: 8
Training loss: 2.935354709625244
Validation loss: 3.4357047850085842

Epoch: 5| Step: 9
Training loss: 2.699812412261963
Validation loss: 3.4053366056052585

Epoch: 5| Step: 10
Training loss: 3.4687042236328125
Validation loss: 3.388545282425419

Epoch: 18| Step: 0
Training loss: 3.220446825027466
Validation loss: 3.3656597675815707

Epoch: 5| Step: 1
Training loss: 4.138872146606445
Validation loss: 3.3453088678339475

Epoch: 5| Step: 2
Training loss: 3.667832612991333
Validation loss: 3.3349180452285276

Epoch: 5| Step: 3
Training loss: 2.6846463680267334
Validation loss: 3.302064388029037

Epoch: 5| Step: 4
Training loss: 2.658613920211792
Validation loss: 3.275675283965244

Epoch: 5| Step: 5
Training loss: 3.540358066558838
Validation loss: 3.255364933321553

Epoch: 5| Step: 6
Training loss: 2.7451255321502686
Validation loss: 3.2298675531982095

Epoch: 5| Step: 7
Training loss: 3.8760242462158203
Validation loss: 3.2098993203973256

Epoch: 5| Step: 8
Training loss: 2.7391791343688965
Validation loss: 3.189128832150531

Epoch: 5| Step: 9
Training loss: 2.6172754764556885
Validation loss: 3.152227622206493

Epoch: 5| Step: 10
Training loss: 3.2040624618530273
Validation loss: 3.159381030708231

Epoch: 19| Step: 0
Training loss: 3.6650424003601074
Validation loss: 3.126101388726183

Epoch: 5| Step: 1
Training loss: 2.4602787494659424
Validation loss: 3.106597513280889

Epoch: 5| Step: 2
Training loss: 3.6034951210021973
Validation loss: 3.0861495720442904

Epoch: 5| Step: 3
Training loss: 2.6625876426696777
Validation loss: 3.068925324306693

Epoch: 5| Step: 4
Training loss: 2.660468339920044
Validation loss: 3.0475374293583695

Epoch: 5| Step: 5
Training loss: 2.906766414642334
Validation loss: 3.026936802812802

Epoch: 5| Step: 6
Training loss: 3.4329237937927246
Validation loss: 3.01268405811761

Epoch: 5| Step: 7
Training loss: 3.461430072784424
Validation loss: 3.004118219498665

Epoch: 5| Step: 8
Training loss: 2.7333884239196777
Validation loss: 2.9753820306511334

Epoch: 5| Step: 9
Training loss: 2.8815999031066895
Validation loss: 2.9602936595998783

Epoch: 5| Step: 10
Training loss: 2.8075482845306396
Validation loss: 2.9502919745701615

Epoch: 20| Step: 0
Training loss: 2.306018352508545
Validation loss: 2.926244394753569

Epoch: 5| Step: 1
Training loss: 2.7522971630096436
Validation loss: 2.912598963706724

Epoch: 5| Step: 2
Training loss: 3.6047141551971436
Validation loss: 2.888497101363315

Epoch: 5| Step: 3
Training loss: 2.4366226196289062
Validation loss: 2.870163648359237

Epoch: 5| Step: 4
Training loss: 2.9803993701934814
Validation loss: 2.843900885633243

Epoch: 5| Step: 5
Training loss: 2.6535868644714355
Validation loss: 2.8464238541100615

Epoch: 5| Step: 6
Training loss: 3.205786943435669
Validation loss: 2.810190395642352

Epoch: 5| Step: 7
Training loss: 3.021296977996826
Validation loss: 2.7919723961942937

Epoch: 5| Step: 8
Training loss: 3.1522345542907715
Validation loss: 2.781925291143438

Epoch: 5| Step: 9
Training loss: 2.7445995807647705
Validation loss: 2.7542056627171014

Epoch: 5| Step: 10
Training loss: 2.9371960163116455
Validation loss: 2.7414522940112698

Epoch: 21| Step: 0
Training loss: 2.6816694736480713
Validation loss: 2.7249777470865557

Epoch: 5| Step: 1
Training loss: 2.560607433319092
Validation loss: 2.6914436432623092

Epoch: 5| Step: 2
Training loss: 2.4849941730499268
Validation loss: 2.6860603747829312

Epoch: 5| Step: 3
Training loss: 3.105877637863159
Validation loss: 2.6597972300744828

Epoch: 5| Step: 4
Training loss: 2.6313533782958984
Validation loss: 2.665666590454758

Epoch: 5| Step: 5
Training loss: 2.906891345977783
Validation loss: 2.647972863207581

Epoch: 5| Step: 6
Training loss: 2.215064287185669
Validation loss: 2.6273649174679994

Epoch: 5| Step: 7
Training loss: 2.614032030105591
Validation loss: 2.6090373813465075

Epoch: 5| Step: 8
Training loss: 3.693470001220703
Validation loss: 2.578649808001775

Epoch: 5| Step: 9
Training loss: 3.108717441558838
Validation loss: 2.5747582245898504

Epoch: 5| Step: 10
Training loss: 1.9554089307785034
Validation loss: 2.551913866432764

Epoch: 22| Step: 0
Training loss: 2.8006701469421387
Validation loss: 2.523485678498463

Epoch: 5| Step: 1
Training loss: 2.978299617767334
Validation loss: 2.5267808001528502

Epoch: 5| Step: 2
Training loss: 2.3751027584075928
Validation loss: 2.5099123318990073

Epoch: 5| Step: 3
Training loss: 2.408808708190918
Validation loss: 2.4978696095046176

Epoch: 5| Step: 4
Training loss: 2.3698244094848633
Validation loss: 2.4807474843917356

Epoch: 5| Step: 5
Training loss: 2.2345893383026123
Validation loss: 2.4592962290651057

Epoch: 5| Step: 6
Training loss: 2.6307554244995117
Validation loss: 2.454862230567522

Epoch: 5| Step: 7
Training loss: 2.7685418128967285
Validation loss: 2.448021109386157

Epoch: 5| Step: 8
Training loss: 2.877727508544922
Validation loss: 2.4336364730711906

Epoch: 5| Step: 9
Training loss: 2.3046951293945312
Validation loss: 2.432760238647461

Epoch: 5| Step: 10
Training loss: 3.069305658340454
Validation loss: 2.4261761942217426

Epoch: 23| Step: 0
Training loss: 2.3994228839874268
Validation loss: 2.3989381021068943

Epoch: 5| Step: 1
Training loss: 2.4440815448760986
Validation loss: 2.3957213663285777

Epoch: 5| Step: 2
Training loss: 2.3498425483703613
Validation loss: 2.3764862142583376

Epoch: 5| Step: 3
Training loss: 2.5971105098724365
Validation loss: 2.387078364690145

Epoch: 5| Step: 4
Training loss: 2.336312770843506
Validation loss: 2.371005504362045

Epoch: 5| Step: 5
Training loss: 2.8899874687194824
Validation loss: 2.3747291641850627

Epoch: 5| Step: 6
Training loss: 2.932079553604126
Validation loss: 2.3512724702076246

Epoch: 5| Step: 7
Training loss: 2.4608988761901855
Validation loss: 2.3401442471370903

Epoch: 5| Step: 8
Training loss: 1.9626023769378662
Validation loss: 2.3319625649400937

Epoch: 5| Step: 9
Training loss: 3.0444881916046143
Validation loss: 2.337166828493918

Epoch: 5| Step: 10
Training loss: 2.553760528564453
Validation loss: 2.3279892603556314

Epoch: 24| Step: 0
Training loss: 2.4911282062530518
Validation loss: 2.3206781648820445

Epoch: 5| Step: 1
Training loss: 2.4293932914733887
Validation loss: 2.3250210182641142

Epoch: 5| Step: 2
Training loss: 2.824287176132202
Validation loss: 2.297736055107527

Epoch: 5| Step: 3
Training loss: 2.746001720428467
Validation loss: 2.2933791760475404

Epoch: 5| Step: 4
Training loss: 2.044456958770752
Validation loss: 2.289919135391071

Epoch: 5| Step: 5
Training loss: 2.616854667663574
Validation loss: 2.295781304759364

Epoch: 5| Step: 6
Training loss: 2.291548252105713
Validation loss: 2.292403439039825

Epoch: 5| Step: 7
Training loss: 2.98213267326355
Validation loss: 2.2723819645502235

Epoch: 5| Step: 8
Training loss: 2.246649980545044
Validation loss: 2.26438021403487

Epoch: 5| Step: 9
Training loss: 2.2527496814727783
Validation loss: 2.27945460555374

Epoch: 5| Step: 10
Training loss: 2.9398539066314697
Validation loss: 2.2690161325598277

Epoch: 25| Step: 0
Training loss: 2.483394145965576
Validation loss: 2.270939906438192

Epoch: 5| Step: 1
Training loss: 2.6739420890808105
Validation loss: 2.258787760170557

Epoch: 5| Step: 2
Training loss: 1.789542555809021
Validation loss: 2.261636580190351

Epoch: 5| Step: 3
Training loss: 2.2109038829803467
Validation loss: 2.243780211735797

Epoch: 5| Step: 4
Training loss: 2.5768487453460693
Validation loss: 2.2527528039870726

Epoch: 5| Step: 5
Training loss: 3.159435749053955
Validation loss: 2.2599058253790743

Epoch: 5| Step: 6
Training loss: 2.3568506240844727
Validation loss: 2.24148622123144

Epoch: 5| Step: 7
Training loss: 2.335233688354492
Validation loss: 2.238147945814235

Epoch: 5| Step: 8
Training loss: 2.0967767238616943
Validation loss: 2.246540218271235

Epoch: 5| Step: 9
Training loss: 2.772327423095703
Validation loss: 2.2488451055301133

Epoch: 5| Step: 10
Training loss: 3.2525038719177246
Validation loss: 2.2347852773563837

Epoch: 26| Step: 0
Training loss: 2.349987745285034
Validation loss: 2.2474544227764173

Epoch: 5| Step: 1
Training loss: 2.4637207984924316
Validation loss: 2.238991545092675

Epoch: 5| Step: 2
Training loss: 2.7003116607666016
Validation loss: 2.231731546822415

Epoch: 5| Step: 3
Training loss: 2.0930118560791016
Validation loss: 2.2363646620063373

Epoch: 5| Step: 4
Training loss: 2.5530552864074707
Validation loss: 2.232126651271697

Epoch: 5| Step: 5
Training loss: 2.2102913856506348
Validation loss: 2.2451738798490135

Epoch: 5| Step: 6
Training loss: 3.058130979537964
Validation loss: 2.225780625497141

Epoch: 5| Step: 7
Training loss: 2.1270687580108643
Validation loss: 2.2428612837227444

Epoch: 5| Step: 8
Training loss: 2.8774356842041016
Validation loss: 2.2284681989300634

Epoch: 5| Step: 9
Training loss: 2.087869167327881
Validation loss: 2.246440932314883

Epoch: 5| Step: 10
Training loss: 2.9526891708374023
Validation loss: 2.2383492890224663

Epoch: 27| Step: 0
Training loss: 2.7630820274353027
Validation loss: 2.2358366238173617

Epoch: 5| Step: 1
Training loss: 2.1554577350616455
Validation loss: 2.231739487699283

Epoch: 5| Step: 2
Training loss: 3.081566572189331
Validation loss: 2.22873749784244

Epoch: 5| Step: 3
Training loss: 2.394343852996826
Validation loss: 2.239972911855226

Epoch: 5| Step: 4
Training loss: 2.246556520462036
Validation loss: 2.2388458482680784

Epoch: 5| Step: 5
Training loss: 2.3547964096069336
Validation loss: 2.2326382052513862

Epoch: 5| Step: 6
Training loss: 2.6564743518829346
Validation loss: 2.2125303386360087

Epoch: 5| Step: 7
Training loss: 2.839488983154297
Validation loss: 2.22663809663506

Epoch: 5| Step: 8
Training loss: 1.9028198719024658
Validation loss: 2.224294909866907

Epoch: 5| Step: 9
Training loss: 2.0379040241241455
Validation loss: 2.239226702720888

Epoch: 5| Step: 10
Training loss: 3.0329766273498535
Validation loss: 2.2292815152034966

Epoch: 28| Step: 0
Training loss: 2.120461940765381
Validation loss: 2.2303058870377077

Epoch: 5| Step: 1
Training loss: 1.9027551412582397
Validation loss: 2.2143495441764913

Epoch: 5| Step: 2
Training loss: 3.3315269947052
Validation loss: 2.215301018889232

Epoch: 5| Step: 3
Training loss: 2.501676082611084
Validation loss: 2.222549625622329

Epoch: 5| Step: 4
Training loss: 2.282278537750244
Validation loss: 2.2202892944376957

Epoch: 5| Step: 5
Training loss: 1.8641865253448486
Validation loss: 2.2148629055228284

Epoch: 5| Step: 6
Training loss: 2.7132511138916016
Validation loss: 2.210612699549685

Epoch: 5| Step: 7
Training loss: 2.898942470550537
Validation loss: 2.2197423211989866

Epoch: 5| Step: 8
Training loss: 2.202056884765625
Validation loss: 2.2137890579879924

Epoch: 5| Step: 9
Training loss: 2.325472831726074
Validation loss: 2.2178077774663127

Epoch: 5| Step: 10
Training loss: 3.246832847595215
Validation loss: 2.2350866602313135

Epoch: 29| Step: 0
Training loss: 2.977255344390869
Validation loss: 2.2051385602643414

Epoch: 5| Step: 1
Training loss: 2.3130533695220947
Validation loss: 2.2124881052201792

Epoch: 5| Step: 2
Training loss: 1.9471327066421509
Validation loss: 2.211154717271046

Epoch: 5| Step: 3
Training loss: 2.3759560585021973
Validation loss: 2.22025954082448

Epoch: 5| Step: 4
Training loss: 2.1972155570983887
Validation loss: 2.222894186614662

Epoch: 5| Step: 5
Training loss: 2.895615339279175
Validation loss: 2.2198718593966578

Epoch: 5| Step: 6
Training loss: 2.120251417160034
Validation loss: 2.2147515948100756

Epoch: 5| Step: 7
Training loss: 1.9321200847625732
Validation loss: 2.2070965920725176

Epoch: 5| Step: 8
Training loss: 2.8567967414855957
Validation loss: 2.2147944152996106

Epoch: 5| Step: 9
Training loss: 2.895209789276123
Validation loss: 2.1962215131328953

Epoch: 5| Step: 10
Training loss: 2.7388193607330322
Validation loss: 2.2067517336978706

Epoch: 30| Step: 0
Training loss: 2.2083332538604736
Validation loss: 2.2148155473893687

Epoch: 5| Step: 1
Training loss: 3.024003505706787
Validation loss: 2.206755881668419

Epoch: 5| Step: 2
Training loss: 3.2437758445739746
Validation loss: 2.211385933301782

Epoch: 5| Step: 3
Training loss: 2.4508306980133057
Validation loss: 2.2137993753597303

Epoch: 5| Step: 4
Training loss: 2.5274834632873535
Validation loss: 2.2088179844681934

Epoch: 5| Step: 5
Training loss: 1.5529283285140991
Validation loss: 2.2145915672343266

Epoch: 5| Step: 6
Training loss: 2.4543280601501465
Validation loss: 2.1903698854548956

Epoch: 5| Step: 7
Training loss: 2.852442979812622
Validation loss: 2.199064331669961

Epoch: 5| Step: 8
Training loss: 2.391605854034424
Validation loss: 2.204050635778776

Epoch: 5| Step: 9
Training loss: 2.314377784729004
Validation loss: 2.2066050755080355

Epoch: 5| Step: 10
Training loss: 1.9438626766204834
Validation loss: 2.1987347756662676

Epoch: 31| Step: 0
Training loss: 2.2420923709869385
Validation loss: 2.1813284248434086

Epoch: 5| Step: 1
Training loss: 2.2494685649871826
Validation loss: 2.2009625101602204

Epoch: 5| Step: 2
Training loss: 2.684300184249878
Validation loss: 2.204462435937697

Epoch: 5| Step: 3
Training loss: 2.543426036834717
Validation loss: 2.210399112393779

Epoch: 5| Step: 4
Training loss: 2.4612865447998047
Validation loss: 2.195373709483813

Epoch: 5| Step: 5
Training loss: 2.53765869140625
Validation loss: 2.2188596904918714

Epoch: 5| Step: 6
Training loss: 2.6358108520507812
Validation loss: 2.1931597443037134

Epoch: 5| Step: 7
Training loss: 2.674487829208374
Validation loss: 2.1981138388315835

Epoch: 5| Step: 8
Training loss: 2.640237808227539
Validation loss: 2.2118065818663566

Epoch: 5| Step: 9
Training loss: 2.1165266036987305
Validation loss: 2.208702104066008

Epoch: 5| Step: 10
Training loss: 2.0208981037139893
Validation loss: 2.1951971951351372

Epoch: 32| Step: 0
Training loss: 2.3654162883758545
Validation loss: 2.184794884856029

Epoch: 5| Step: 1
Training loss: 2.3516788482666016
Validation loss: 2.1859458710557673

Epoch: 5| Step: 2
Training loss: 2.18546724319458
Validation loss: 2.192885406555668

Epoch: 5| Step: 3
Training loss: 3.2345480918884277
Validation loss: 2.1820409067215456

Epoch: 5| Step: 4
Training loss: 2.2547454833984375
Validation loss: 2.2109849658063663

Epoch: 5| Step: 5
Training loss: 2.6576340198516846
Validation loss: 2.1827773278759373

Epoch: 5| Step: 6
Training loss: 2.184339761734009
Validation loss: 2.188037626204952

Epoch: 5| Step: 7
Training loss: 2.14302921295166
Validation loss: 2.172538608633062

Epoch: 5| Step: 8
Training loss: 2.2568857669830322
Validation loss: 2.1939691599979194

Epoch: 5| Step: 9
Training loss: 2.6852264404296875
Validation loss: 2.175514304509727

Epoch: 5| Step: 10
Training loss: 2.5708084106445312
Validation loss: 2.186618730586062

Epoch: 33| Step: 0
Training loss: 2.886012554168701
Validation loss: 2.1741734832845707

Epoch: 5| Step: 1
Training loss: 2.1164021492004395
Validation loss: 2.1905195238769695

Epoch: 5| Step: 2
Training loss: 2.017874240875244
Validation loss: 2.16631487108046

Epoch: 5| Step: 3
Training loss: 1.9581416845321655
Validation loss: 2.1927981722739434

Epoch: 5| Step: 4
Training loss: 2.5581231117248535
Validation loss: 2.180779508365098

Epoch: 5| Step: 5
Training loss: 2.8635573387145996
Validation loss: 2.2003299100424654

Epoch: 5| Step: 6
Training loss: 2.5000882148742676
Validation loss: 2.183824844257806

Epoch: 5| Step: 7
Training loss: 2.524138927459717
Validation loss: 2.1705341082747265

Epoch: 5| Step: 8
Training loss: 2.259265422821045
Validation loss: 2.1842617629676737

Epoch: 5| Step: 9
Training loss: 2.7149569988250732
Validation loss: 2.181589521387572

Epoch: 5| Step: 10
Training loss: 2.2783401012420654
Validation loss: 2.181147966333615

Epoch: 34| Step: 0
Training loss: 2.4716668128967285
Validation loss: 2.18796968460083

Epoch: 5| Step: 1
Training loss: 2.932634115219116
Validation loss: 2.192524253681142

Epoch: 5| Step: 2
Training loss: 2.556016445159912
Validation loss: 2.1987605569183186

Epoch: 5| Step: 3
Training loss: 2.0206210613250732
Validation loss: 2.183677634885234

Epoch: 5| Step: 4
Training loss: 2.0700764656066895
Validation loss: 2.1741651309433805

Epoch: 5| Step: 5
Training loss: 2.121502637863159
Validation loss: 2.19357120349843

Epoch: 5| Step: 6
Training loss: 2.879767656326294
Validation loss: 2.192286701612575

Epoch: 5| Step: 7
Training loss: 2.327427387237549
Validation loss: 2.1747569012385544

Epoch: 5| Step: 8
Training loss: 2.444849729537964
Validation loss: 2.1992964411294587

Epoch: 5| Step: 9
Training loss: 2.3670506477355957
Validation loss: 2.20286270623566

Epoch: 5| Step: 10
Training loss: 2.4344403743743896
Validation loss: 2.201554418891989

Epoch: 35| Step: 0
Training loss: 2.172332763671875
Validation loss: 2.2098167660415813

Epoch: 5| Step: 1
Training loss: 1.8748047351837158
Validation loss: 2.194918901689591

Epoch: 5| Step: 2
Training loss: 1.945277452468872
Validation loss: 2.1861485999117614

Epoch: 5| Step: 3
Training loss: 2.977797746658325
Validation loss: 2.2083719725249917

Epoch: 5| Step: 4
Training loss: 2.4857547283172607
Validation loss: 2.193469898675078

Epoch: 5| Step: 5
Training loss: 2.403743267059326
Validation loss: 2.2041330363160823

Epoch: 5| Step: 6
Training loss: 2.2598824501037598
Validation loss: 2.1976851878627652

Epoch: 5| Step: 7
Training loss: 2.444782257080078
Validation loss: 2.2137482217563096

Epoch: 5| Step: 8
Training loss: 3.0634660720825195
Validation loss: 2.209995969649284

Epoch: 5| Step: 9
Training loss: 2.5304198265075684
Validation loss: 2.1977921788410475

Epoch: 5| Step: 10
Training loss: 2.4518587589263916
Validation loss: 2.194338925423161

Epoch: 36| Step: 0
Training loss: 2.7893459796905518
Validation loss: 2.2030381374461676

Epoch: 5| Step: 1
Training loss: 1.9617341756820679
Validation loss: 2.2061743069720525

Epoch: 5| Step: 2
Training loss: 2.0862362384796143
Validation loss: 2.199941332622241

Epoch: 5| Step: 3
Training loss: 2.1891942024230957
Validation loss: 2.1961909724820043

Epoch: 5| Step: 4
Training loss: 2.028888463973999
Validation loss: 2.2080972527944915

Epoch: 5| Step: 5
Training loss: 2.4584574699401855
Validation loss: 2.2134393261324976

Epoch: 5| Step: 6
Training loss: 2.9222021102905273
Validation loss: 2.2016014463158062

Epoch: 5| Step: 7
Training loss: 2.134014129638672
Validation loss: 2.1979527217085644

Epoch: 5| Step: 8
Training loss: 2.677255392074585
Validation loss: 2.1880668260717906

Epoch: 5| Step: 9
Training loss: 2.1138806343078613
Validation loss: 2.1819064719702608

Epoch: 5| Step: 10
Training loss: 3.365201950073242
Validation loss: 2.1850908443491948

Epoch: 37| Step: 0
Training loss: 2.547318935394287
Validation loss: 2.1898447249525335

Epoch: 5| Step: 1
Training loss: 1.8771724700927734
Validation loss: 2.182561587261897

Epoch: 5| Step: 2
Training loss: 2.408742666244507
Validation loss: 2.1748469593704387

Epoch: 5| Step: 3
Training loss: 2.863643169403076
Validation loss: 2.1867606832135107

Epoch: 5| Step: 4
Training loss: 3.1203107833862305
Validation loss: 2.1902591233612387

Epoch: 5| Step: 5
Training loss: 1.866790771484375
Validation loss: 2.1927606226295553

Epoch: 5| Step: 6
Training loss: 2.126345157623291
Validation loss: 2.1947235548368065

Epoch: 5| Step: 7
Training loss: 2.0461251735687256
Validation loss: 2.20025481459915

Epoch: 5| Step: 8
Training loss: 2.174558162689209
Validation loss: 2.198739215891848

Epoch: 5| Step: 9
Training loss: 2.540452480316162
Validation loss: 2.1766378213000555

Epoch: 5| Step: 10
Training loss: 2.983269691467285
Validation loss: 2.176412249124178

Epoch: 38| Step: 0
Training loss: 2.467061758041382
Validation loss: 2.1741606984087216

Epoch: 5| Step: 1
Training loss: 3.3674304485321045
Validation loss: 2.177473352801415

Epoch: 5| Step: 2
Training loss: 1.6185085773468018
Validation loss: 2.169385189651161

Epoch: 5| Step: 3
Training loss: 1.95687997341156
Validation loss: 2.1786380737058577

Epoch: 5| Step: 4
Training loss: 3.015712022781372
Validation loss: 2.176140151998048

Epoch: 5| Step: 5
Training loss: 2.2993807792663574
Validation loss: 2.1571782968377553

Epoch: 5| Step: 6
Training loss: 2.6966235637664795
Validation loss: 2.1554286377404326

Epoch: 5| Step: 7
Training loss: 1.9380226135253906
Validation loss: 2.1655913809294343

Epoch: 5| Step: 8
Training loss: 2.0989456176757812
Validation loss: 2.1768470182213733

Epoch: 5| Step: 9
Training loss: 2.7778263092041016
Validation loss: 2.1664722247790267

Epoch: 5| Step: 10
Training loss: 2.3558671474456787
Validation loss: 2.1653415310767388

Epoch: 39| Step: 0
Training loss: 2.005913257598877
Validation loss: 2.1661086851550686

Epoch: 5| Step: 1
Training loss: 1.8221876621246338
Validation loss: 2.159824464910774

Epoch: 5| Step: 2
Training loss: 2.3587546348571777
Validation loss: 2.16451955610706

Epoch: 5| Step: 3
Training loss: 2.7035624980926514
Validation loss: 2.1773450143875612

Epoch: 5| Step: 4
Training loss: 2.0317869186401367
Validation loss: 2.1688001412217335

Epoch: 5| Step: 5
Training loss: 2.5215842723846436
Validation loss: 2.1735761191255305

Epoch: 5| Step: 6
Training loss: 2.686579465866089
Validation loss: 2.1630867706832064

Epoch: 5| Step: 7
Training loss: 2.3610846996307373
Validation loss: 2.166439756270378

Epoch: 5| Step: 8
Training loss: 2.5674359798431396
Validation loss: 2.160571264964278

Epoch: 5| Step: 9
Training loss: 2.460449457168579
Validation loss: 2.162839279379896

Epoch: 5| Step: 10
Training loss: 2.8888909816741943
Validation loss: 2.178295748208159

Epoch: 40| Step: 0
Training loss: 2.055577516555786
Validation loss: 2.1794325869570494

Epoch: 5| Step: 1
Training loss: 3.1562838554382324
Validation loss: 2.154585369171635

Epoch: 5| Step: 2
Training loss: 1.8196837902069092
Validation loss: 2.1878855125878447

Epoch: 5| Step: 3
Training loss: 2.66416597366333
Validation loss: 2.1737072467803955

Epoch: 5| Step: 4
Training loss: 2.2839646339416504
Validation loss: 2.1818622530147596

Epoch: 5| Step: 5
Training loss: 2.8014614582061768
Validation loss: 2.1813139787284275

Epoch: 5| Step: 6
Training loss: 2.414999485015869
Validation loss: 2.167994604315809

Epoch: 5| Step: 7
Training loss: 2.6974995136260986
Validation loss: 2.2017503400002756

Epoch: 5| Step: 8
Training loss: 1.8908145427703857
Validation loss: 2.1853699453415407

Epoch: 5| Step: 9
Training loss: 2.2902350425720215
Validation loss: 2.1839066243940786

Epoch: 5| Step: 10
Training loss: 2.1054940223693848
Validation loss: 2.183590796685988

Epoch: 41| Step: 0
Training loss: 2.6248927116394043
Validation loss: 2.1745870882464993

Epoch: 5| Step: 1
Training loss: 2.2821907997131348
Validation loss: 2.1701132328279558

Epoch: 5| Step: 2
Training loss: 2.2614550590515137
Validation loss: 2.1768122565361763

Epoch: 5| Step: 3
Training loss: 2.3257529735565186
Validation loss: 2.1776814396663378

Epoch: 5| Step: 4
Training loss: 2.239419460296631
Validation loss: 2.1872505808389313

Epoch: 5| Step: 5
Training loss: 2.3318703174591064
Validation loss: 2.18913209566506

Epoch: 5| Step: 6
Training loss: 2.3482789993286133
Validation loss: 2.1862062036350207

Epoch: 5| Step: 7
Training loss: 2.393138885498047
Validation loss: 2.1744547556805354

Epoch: 5| Step: 8
Training loss: 2.392275333404541
Validation loss: 2.1848333189564366

Epoch: 5| Step: 9
Training loss: 2.1341803073883057
Validation loss: 2.1738567454840547

Epoch: 5| Step: 10
Training loss: 2.981480836868286
Validation loss: 2.171784129194034

Epoch: 42| Step: 0
Training loss: 2.1568732261657715
Validation loss: 2.164152972159847

Epoch: 5| Step: 1
Training loss: 3.0068459510803223
Validation loss: 2.1705244766768588

Epoch: 5| Step: 2
Training loss: 2.3282954692840576
Validation loss: 2.1988914653819096

Epoch: 5| Step: 3
Training loss: 2.5155999660491943
Validation loss: 2.1642059010844075

Epoch: 5| Step: 4
Training loss: 2.4301280975341797
Validation loss: 2.1656456788380942

Epoch: 5| Step: 5
Training loss: 2.3534092903137207
Validation loss: 2.1697992099228727

Epoch: 5| Step: 6
Training loss: 2.4231300354003906
Validation loss: 2.16394067195154

Epoch: 5| Step: 7
Training loss: 2.429145336151123
Validation loss: 2.172156003213698

Epoch: 5| Step: 8
Training loss: 2.727937936782837
Validation loss: 2.1688355040806595

Epoch: 5| Step: 9
Training loss: 1.5665128231048584
Validation loss: 2.164173900440175

Epoch: 5| Step: 10
Training loss: 2.3897528648376465
Validation loss: 2.174254325128371

Epoch: 43| Step: 0
Training loss: 2.19868540763855
Validation loss: 2.1638678914757183

Epoch: 5| Step: 1
Training loss: 2.1491944789886475
Validation loss: 2.1638783152385423

Epoch: 5| Step: 2
Training loss: 2.4253697395324707
Validation loss: 2.1624074443694083

Epoch: 5| Step: 3
Training loss: 2.0129475593566895
Validation loss: 2.1718924096835557

Epoch: 5| Step: 4
Training loss: 2.1893696784973145
Validation loss: 2.171256926751906

Epoch: 5| Step: 5
Training loss: 3.3930790424346924
Validation loss: 2.1682841393255416

Epoch: 5| Step: 6
Training loss: 1.8574155569076538
Validation loss: 2.1636580959443124

Epoch: 5| Step: 7
Training loss: 2.5367465019226074
Validation loss: 2.1584918870720813

Epoch: 5| Step: 8
Training loss: 2.515082836151123
Validation loss: 2.1614449562564975

Epoch: 5| Step: 9
Training loss: 2.321148633956909
Validation loss: 2.1712148086999052

Epoch: 5| Step: 10
Training loss: 2.4478185176849365
Validation loss: 2.1749967221290833

Epoch: 44| Step: 0
Training loss: 2.4922571182250977
Validation loss: 2.1604730980370634

Epoch: 5| Step: 1
Training loss: 2.588097095489502
Validation loss: 2.169637287816694

Epoch: 5| Step: 2
Training loss: 2.2993693351745605
Validation loss: 2.153518117884154

Epoch: 5| Step: 3
Training loss: 1.9677352905273438
Validation loss: 2.1639480078092186

Epoch: 5| Step: 4
Training loss: 2.539604663848877
Validation loss: 2.16519453576816

Epoch: 5| Step: 5
Training loss: 1.9734233617782593
Validation loss: 2.1583595993698284

Epoch: 5| Step: 6
Training loss: 2.704068183898926
Validation loss: 2.1624783597966677

Epoch: 5| Step: 7
Training loss: 2.2443039417266846
Validation loss: 2.146314131316318

Epoch: 5| Step: 8
Training loss: 2.4891343116760254
Validation loss: 2.1603040361917145

Epoch: 5| Step: 9
Training loss: 2.201385736465454
Validation loss: 2.165420675790438

Epoch: 5| Step: 10
Training loss: 2.552952289581299
Validation loss: 2.152479660126471

Epoch: 45| Step: 0
Training loss: 2.386143445968628
Validation loss: 2.1561524624465616

Epoch: 5| Step: 1
Training loss: 2.6936745643615723
Validation loss: 2.1787452851572344

Epoch: 5| Step: 2
Training loss: 2.687497615814209
Validation loss: 2.150738262361096

Epoch: 5| Step: 3
Training loss: 2.2389676570892334
Validation loss: 2.168149022645848

Epoch: 5| Step: 4
Training loss: 1.8966392278671265
Validation loss: 2.177138026042651

Epoch: 5| Step: 5
Training loss: 2.458897113800049
Validation loss: 2.1904989339972056

Epoch: 5| Step: 6
Training loss: 1.9341294765472412
Validation loss: 2.1885058392760572

Epoch: 5| Step: 7
Training loss: 2.259761095046997
Validation loss: 2.1702724169659358

Epoch: 5| Step: 8
Training loss: 2.7266056537628174
Validation loss: 2.1843454478889384

Epoch: 5| Step: 9
Training loss: 2.2910995483398438
Validation loss: 2.176935080559023

Epoch: 5| Step: 10
Training loss: 2.429302453994751
Validation loss: 2.1706668292322466

Epoch: 46| Step: 0
Training loss: 2.4457993507385254
Validation loss: 2.1810472857567573

Epoch: 5| Step: 1
Training loss: 2.414771556854248
Validation loss: 2.186698405973373

Epoch: 5| Step: 2
Training loss: 2.7690188884735107
Validation loss: 2.174595853333832

Epoch: 5| Step: 3
Training loss: 2.007798671722412
Validation loss: 2.178127634909845

Epoch: 5| Step: 4
Training loss: 2.0836005210876465
Validation loss: 2.18820624069501

Epoch: 5| Step: 5
Training loss: 2.621060848236084
Validation loss: 2.1840740224366546

Epoch: 5| Step: 6
Training loss: 2.533623456954956
Validation loss: 2.185003783113213

Epoch: 5| Step: 7
Training loss: 1.7054780721664429
Validation loss: 2.1659151315689087

Epoch: 5| Step: 8
Training loss: 2.713549852371216
Validation loss: 2.192920641232562

Epoch: 5| Step: 9
Training loss: 1.9588035345077515
Validation loss: 2.184517401520924

Epoch: 5| Step: 10
Training loss: 2.8455910682678223
Validation loss: 2.1734964834746493

Epoch: 47| Step: 0
Training loss: 2.9309895038604736
Validation loss: 2.162370361307616

Epoch: 5| Step: 1
Training loss: 2.2510249614715576
Validation loss: 2.178187652300763

Epoch: 5| Step: 2
Training loss: 2.462376356124878
Validation loss: 2.158970889224801

Epoch: 5| Step: 3
Training loss: 2.769094705581665
Validation loss: 2.1707614673081266

Epoch: 5| Step: 4
Training loss: 1.8747230768203735
Validation loss: 2.1749398182797175

Epoch: 5| Step: 5
Training loss: 1.9597784280776978
Validation loss: 2.1729127489110476

Epoch: 5| Step: 6
Training loss: 2.2461843490600586
Validation loss: 2.1632325854352725

Epoch: 5| Step: 7
Training loss: 2.9070942401885986
Validation loss: 2.173121422849676

Epoch: 5| Step: 8
Training loss: 2.274730920791626
Validation loss: 2.167021438639651

Epoch: 5| Step: 9
Training loss: 2.2362775802612305
Validation loss: 2.1507406850014963

Epoch: 5| Step: 10
Training loss: 1.8546401262283325
Validation loss: 2.1413373383142615

Epoch: 48| Step: 0
Training loss: 2.795372486114502
Validation loss: 2.165819788491854

Epoch: 5| Step: 1
Training loss: 2.65458345413208
Validation loss: 2.1569752821358303

Epoch: 5| Step: 2
Training loss: 1.9629167318344116
Validation loss: 2.168223962988905

Epoch: 5| Step: 3
Training loss: 1.9472110271453857
Validation loss: 2.1663471524433424

Epoch: 5| Step: 4
Training loss: 1.9826465845108032
Validation loss: 2.15589447944395

Epoch: 5| Step: 5
Training loss: 2.556962251663208
Validation loss: 2.1513202241671983

Epoch: 5| Step: 6
Training loss: 2.703979730606079
Validation loss: 2.1607758716870378

Epoch: 5| Step: 7
Training loss: 2.1824135780334473
Validation loss: 2.1487948868864324

Epoch: 5| Step: 8
Training loss: 3.0037682056427
Validation loss: 2.1551823052026893

Epoch: 5| Step: 9
Training loss: 1.904987096786499
Validation loss: 2.151010767106087

Epoch: 5| Step: 10
Training loss: 2.1924245357513428
Validation loss: 2.1369342829591487

Epoch: 49| Step: 0
Training loss: 2.3221449851989746
Validation loss: 2.174058484774764

Epoch: 5| Step: 1
Training loss: 2.187661647796631
Validation loss: 2.1527976720563826

Epoch: 5| Step: 2
Training loss: 2.740750312805176
Validation loss: 2.1545261157456266

Epoch: 5| Step: 3
Training loss: 2.601123332977295
Validation loss: 2.1703872244845153

Epoch: 5| Step: 4
Training loss: 2.229290246963501
Validation loss: 2.180190204292215

Epoch: 5| Step: 5
Training loss: 2.013540267944336
Validation loss: 2.179301487502231

Epoch: 5| Step: 6
Training loss: 2.648409605026245
Validation loss: 2.176410221284436

Epoch: 5| Step: 7
Training loss: 1.8823535442352295
Validation loss: 2.175721614591537

Epoch: 5| Step: 8
Training loss: 2.4469332695007324
Validation loss: 2.164114193249774

Epoch: 5| Step: 9
Training loss: 2.410048007965088
Validation loss: 2.175215869821528

Epoch: 5| Step: 10
Training loss: 2.394754648208618
Validation loss: 2.1787959529507543

Epoch: 50| Step: 0
Training loss: 2.533172845840454
Validation loss: 2.159758015345502

Epoch: 5| Step: 1
Training loss: 2.5771453380584717
Validation loss: 2.1901389450155277

Epoch: 5| Step: 2
Training loss: 1.6497421264648438
Validation loss: 2.1824383428019862

Epoch: 5| Step: 3
Training loss: 2.8299479484558105
Validation loss: 2.1993394039010488

Epoch: 5| Step: 4
Training loss: 2.103569507598877
Validation loss: 2.1802666302650207

Epoch: 5| Step: 5
Training loss: 2.044710874557495
Validation loss: 2.177905874867593

Epoch: 5| Step: 6
Training loss: 2.644254207611084
Validation loss: 2.1710776359804216

Epoch: 5| Step: 7
Training loss: 2.1705331802368164
Validation loss: 2.182196201816682

Epoch: 5| Step: 8
Training loss: 2.404977798461914
Validation loss: 2.1596621531312183

Epoch: 5| Step: 9
Training loss: 2.460085391998291
Validation loss: 2.1635807162971905

Epoch: 5| Step: 10
Training loss: 2.2415170669555664
Validation loss: 2.1647078567935574

Epoch: 51| Step: 0
Training loss: 2.1812961101531982
Validation loss: 2.1705965047241538

Epoch: 5| Step: 1
Training loss: 2.1852362155914307
Validation loss: 2.167902756762761

Epoch: 5| Step: 2
Training loss: 2.6741836071014404
Validation loss: 2.185247581492188

Epoch: 5| Step: 3
Training loss: 2.543900966644287
Validation loss: 2.159783201832925

Epoch: 5| Step: 4
Training loss: 1.7949682474136353
Validation loss: 2.1590775623116443

Epoch: 5| Step: 5
Training loss: 2.641787052154541
Validation loss: 2.152000909210533

Epoch: 5| Step: 6
Training loss: 1.869255781173706
Validation loss: 2.162130440435102

Epoch: 5| Step: 7
Training loss: 2.992175340652466
Validation loss: 2.153562807267712

Epoch: 5| Step: 8
Training loss: 2.349571704864502
Validation loss: 2.156742813766644

Epoch: 5| Step: 9
Training loss: 2.063636064529419
Validation loss: 2.159760864832068

Epoch: 5| Step: 10
Training loss: 2.4253923892974854
Validation loss: 2.1546260592758015

Epoch: 52| Step: 0
Training loss: 2.868924140930176
Validation loss: 2.1542955354977678

Epoch: 5| Step: 1
Training loss: 2.1893935203552246
Validation loss: 2.159698030000092

Epoch: 5| Step: 2
Training loss: 1.902642011642456
Validation loss: 2.1639608631851854

Epoch: 5| Step: 3
Training loss: 2.065262794494629
Validation loss: 2.1672443959020797

Epoch: 5| Step: 4
Training loss: 2.2187697887420654
Validation loss: 2.161653623786024

Epoch: 5| Step: 5
Training loss: 1.4953705072402954
Validation loss: 2.166506895454981

Epoch: 5| Step: 6
Training loss: 2.1633963584899902
Validation loss: 2.1650692955140145

Epoch: 5| Step: 7
Training loss: 2.5743191242218018
Validation loss: 2.17457551340903

Epoch: 5| Step: 8
Training loss: 3.129702091217041
Validation loss: 2.150295122977226

Epoch: 5| Step: 9
Training loss: 2.6526031494140625
Validation loss: 2.167394773934477

Epoch: 5| Step: 10
Training loss: 2.3036611080169678
Validation loss: 2.164150139336945

Epoch: 53| Step: 0
Training loss: 3.4408087730407715
Validation loss: 2.1812247383979058

Epoch: 5| Step: 1
Training loss: 1.8916761875152588
Validation loss: 2.1892294063363025

Epoch: 5| Step: 2
Training loss: 2.030792713165283
Validation loss: 2.1585409859175324

Epoch: 5| Step: 3
Training loss: 2.7737507820129395
Validation loss: 2.1601873354245256

Epoch: 5| Step: 4
Training loss: 2.372610092163086
Validation loss: 2.171567552833147

Epoch: 5| Step: 5
Training loss: 2.473036527633667
Validation loss: 2.1802559847472818

Epoch: 5| Step: 6
Training loss: 1.8262832164764404
Validation loss: 2.168215949048278

Epoch: 5| Step: 7
Training loss: 2.682941436767578
Validation loss: 2.1653771092814784

Epoch: 5| Step: 8
Training loss: 2.029989004135132
Validation loss: 2.1646972548577095

Epoch: 5| Step: 9
Training loss: 1.6985862255096436
Validation loss: 2.1722664461340955

Epoch: 5| Step: 10
Training loss: 2.4721925258636475
Validation loss: 2.1791684883897022

Epoch: 54| Step: 0
Training loss: 3.0986053943634033
Validation loss: 2.1561969352024857

Epoch: 5| Step: 1
Training loss: 2.3531880378723145
Validation loss: 2.170941869417826

Epoch: 5| Step: 2
Training loss: 2.3145925998687744
Validation loss: 2.145633166836154

Epoch: 5| Step: 3
Training loss: 1.962643027305603
Validation loss: 2.1588347470888527

Epoch: 5| Step: 4
Training loss: 2.5310614109039307
Validation loss: 2.170200394045922

Epoch: 5| Step: 5
Training loss: 2.3499741554260254
Validation loss: 2.1611949577126452

Epoch: 5| Step: 6
Training loss: 1.8521463871002197
Validation loss: 2.160243295854138

Epoch: 5| Step: 7
Training loss: 2.2730751037597656
Validation loss: 2.16997984404205

Epoch: 5| Step: 8
Training loss: 2.0807273387908936
Validation loss: 2.1422474435580674

Epoch: 5| Step: 9
Training loss: 2.7410895824432373
Validation loss: 2.1556248408491894

Epoch: 5| Step: 10
Training loss: 2.001847505569458
Validation loss: 2.148340825111635

Epoch: 55| Step: 0
Training loss: 2.362597703933716
Validation loss: 2.162280371112208

Epoch: 5| Step: 1
Training loss: 2.352825164794922
Validation loss: 2.1387895819961384

Epoch: 5| Step: 2
Training loss: 2.286045551300049
Validation loss: 2.132352743097531

Epoch: 5| Step: 3
Training loss: 2.755460262298584
Validation loss: 2.132310959600633

Epoch: 5| Step: 4
Training loss: 1.7282066345214844
Validation loss: 2.158042635968936

Epoch: 5| Step: 5
Training loss: 2.2319555282592773
Validation loss: 2.146113277763449

Epoch: 5| Step: 6
Training loss: 2.2006688117980957
Validation loss: 2.1338988324647308

Epoch: 5| Step: 7
Training loss: 2.887031316757202
Validation loss: 2.1468476351871284

Epoch: 5| Step: 8
Training loss: 2.6130733489990234
Validation loss: 2.1551769856483705

Epoch: 5| Step: 9
Training loss: 2.0565648078918457
Validation loss: 2.151235249734694

Epoch: 5| Step: 10
Training loss: 1.904158592224121
Validation loss: 2.153530378495493

Epoch: 56| Step: 0
Training loss: 1.955582618713379
Validation loss: 2.1266239689242457

Epoch: 5| Step: 1
Training loss: 2.8970909118652344
Validation loss: 2.1507031763753583

Epoch: 5| Step: 2
Training loss: 1.7779394388198853
Validation loss: 2.1319126826460644

Epoch: 5| Step: 3
Training loss: 2.6979660987854004
Validation loss: 2.1385043385208293

Epoch: 5| Step: 4
Training loss: 1.387281894683838
Validation loss: 2.165613059074648

Epoch: 5| Step: 5
Training loss: 2.166578769683838
Validation loss: 2.169363421778525

Epoch: 5| Step: 6
Training loss: 2.31957745552063
Validation loss: 2.1621102440741753

Epoch: 5| Step: 7
Training loss: 2.6200242042541504
Validation loss: 2.143943194420107

Epoch: 5| Step: 8
Training loss: 2.983586549758911
Validation loss: 2.156153996785482

Epoch: 5| Step: 9
Training loss: 2.0176596641540527
Validation loss: 2.1378208539819203

Epoch: 5| Step: 10
Training loss: 2.7651195526123047
Validation loss: 2.166269120349679

Epoch: 57| Step: 0
Training loss: 2.1171562671661377
Validation loss: 2.15876954601657

Epoch: 5| Step: 1
Training loss: 2.6488847732543945
Validation loss: 2.1650560235464447

Epoch: 5| Step: 2
Training loss: 2.466280460357666
Validation loss: 2.139423237052015

Epoch: 5| Step: 3
Training loss: 2.2955613136291504
Validation loss: 2.1675058487922914

Epoch: 5| Step: 4
Training loss: 1.5775182247161865
Validation loss: 2.143072073177625

Epoch: 5| Step: 5
Training loss: 2.4413857460021973
Validation loss: 2.1438661980372604

Epoch: 5| Step: 6
Training loss: 2.196829080581665
Validation loss: 2.1444688163777834

Epoch: 5| Step: 7
Training loss: 3.2038581371307373
Validation loss: 2.152921662535719

Epoch: 5| Step: 8
Training loss: 2.5258326530456543
Validation loss: 2.1295852097131873

Epoch: 5| Step: 9
Training loss: 2.2291903495788574
Validation loss: 2.1400992319148076

Epoch: 5| Step: 10
Training loss: 1.537958025932312
Validation loss: 2.1694326029028943

Epoch: 58| Step: 0
Training loss: 1.9430058002471924
Validation loss: 2.1607110346517255

Epoch: 5| Step: 1
Training loss: 2.482668161392212
Validation loss: 2.1340869870237125

Epoch: 5| Step: 2
Training loss: 2.572639226913452
Validation loss: 2.1717676936939196

Epoch: 5| Step: 3
Training loss: 1.9401451349258423
Validation loss: 2.1441773099284016

Epoch: 5| Step: 4
Training loss: 1.9558255672454834
Validation loss: 2.138198778193484

Epoch: 5| Step: 5
Training loss: 2.4701175689697266
Validation loss: 2.1614047737531763

Epoch: 5| Step: 6
Training loss: 3.0102882385253906
Validation loss: 2.140899404402702

Epoch: 5| Step: 7
Training loss: 2.6152548789978027
Validation loss: 2.158455958930395

Epoch: 5| Step: 8
Training loss: 2.282918930053711
Validation loss: 2.1419849523933987

Epoch: 5| Step: 9
Training loss: 1.7164971828460693
Validation loss: 2.1446825804248935

Epoch: 5| Step: 10
Training loss: 2.3041317462921143
Validation loss: 2.1423377144721245

Epoch: 59| Step: 0
Training loss: 1.7547342777252197
Validation loss: 2.162826940577517

Epoch: 5| Step: 1
Training loss: 1.9363254308700562
Validation loss: 2.1441814835353563

Epoch: 5| Step: 2
Training loss: 2.387873411178589
Validation loss: 2.170499501689788

Epoch: 5| Step: 3
Training loss: 2.4323081970214844
Validation loss: 2.158523590334

Epoch: 5| Step: 4
Training loss: 1.9631493091583252
Validation loss: 2.149578191900766

Epoch: 5| Step: 5
Training loss: 2.4847190380096436
Validation loss: 2.1713990895978865

Epoch: 5| Step: 6
Training loss: 2.614961862564087
Validation loss: 2.164858536053729

Epoch: 5| Step: 7
Training loss: 2.9113659858703613
Validation loss: 2.1538281850917365

Epoch: 5| Step: 8
Training loss: 2.4634671211242676
Validation loss: 2.173888406445903

Epoch: 5| Step: 9
Training loss: 2.045501708984375
Validation loss: 2.1570534872752365

Epoch: 5| Step: 10
Training loss: 2.2843098640441895
Validation loss: 2.156067225240892

Epoch: 60| Step: 0
Training loss: 2.362056255340576
Validation loss: 2.170149867252637

Epoch: 5| Step: 1
Training loss: 2.2631595134735107
Validation loss: 2.1611717516376125

Epoch: 5| Step: 2
Training loss: 1.8871835470199585
Validation loss: 2.1596465982416624

Epoch: 5| Step: 3
Training loss: 2.1411869525909424
Validation loss: 2.1443897831824517

Epoch: 5| Step: 4
Training loss: 2.471177101135254
Validation loss: 2.167013414444462

Epoch: 5| Step: 5
Training loss: 2.3416075706481934
Validation loss: 2.175042667696553

Epoch: 5| Step: 6
Training loss: 2.6490375995635986
Validation loss: 2.151276534603488

Epoch: 5| Step: 7
Training loss: 2.373555898666382
Validation loss: 2.157821450182187

Epoch: 5| Step: 8
Training loss: 1.949835181236267
Validation loss: 2.159825640340005

Epoch: 5| Step: 9
Training loss: 1.7307155132293701
Validation loss: 2.1718015939958635

Epoch: 5| Step: 10
Training loss: 3.2424697875976562
Validation loss: 2.159500245125063

Epoch: 61| Step: 0
Training loss: 2.266051769256592
Validation loss: 2.1591942400060673

Epoch: 5| Step: 1
Training loss: 2.2781362533569336
Validation loss: 2.1559061593906854

Epoch: 5| Step: 2
Training loss: 2.62250018119812
Validation loss: 2.1514914625434467

Epoch: 5| Step: 3
Training loss: 2.581251859664917
Validation loss: 2.1489735085477113

Epoch: 5| Step: 4
Training loss: 1.8816925287246704
Validation loss: 2.1643784456355597

Epoch: 5| Step: 5
Training loss: 1.6007471084594727
Validation loss: 2.1800155229465936

Epoch: 5| Step: 6
Training loss: 2.6059327125549316
Validation loss: 2.181382425369755

Epoch: 5| Step: 7
Training loss: 2.0279178619384766
Validation loss: 2.1658283023424048

Epoch: 5| Step: 8
Training loss: 2.3381505012512207
Validation loss: 2.1748060487931773

Epoch: 5| Step: 9
Training loss: 2.8387997150421143
Validation loss: 2.1452148242663314

Epoch: 5| Step: 10
Training loss: 2.093435287475586
Validation loss: 2.178850486714353

Epoch: 62| Step: 0
Training loss: 2.0557804107666016
Validation loss: 2.158842421347095

Epoch: 5| Step: 1
Training loss: 1.8651574850082397
Validation loss: 2.1768305199120634

Epoch: 5| Step: 2
Training loss: 2.3548035621643066
Validation loss: 2.1511299981865832

Epoch: 5| Step: 3
Training loss: 2.007982015609741
Validation loss: 2.163185106810703

Epoch: 5| Step: 4
Training loss: 2.4848837852478027
Validation loss: 2.159025312751852

Epoch: 5| Step: 5
Training loss: 2.6270930767059326
Validation loss: 2.1673421834104802

Epoch: 5| Step: 6
Training loss: 1.922480821609497
Validation loss: 2.1742917235179613

Epoch: 5| Step: 7
Training loss: 2.715604543685913
Validation loss: 2.1745176994672386

Epoch: 5| Step: 8
Training loss: 2.0798590183258057
Validation loss: 2.135225660057478

Epoch: 5| Step: 9
Training loss: 2.364607095718384
Validation loss: 2.188130268486597

Epoch: 5| Step: 10
Training loss: 2.986382246017456
Validation loss: 2.179701384677682

Epoch: 63| Step: 0
Training loss: 2.9822819232940674
Validation loss: 2.1372107408379994

Epoch: 5| Step: 1
Training loss: 2.336153745651245
Validation loss: 2.171399326734645

Epoch: 5| Step: 2
Training loss: 1.9492133855819702
Validation loss: 2.1505702900630173

Epoch: 5| Step: 3
Training loss: 1.8606477975845337
Validation loss: 2.1487499231933267

Epoch: 5| Step: 4
Training loss: 2.240417003631592
Validation loss: 2.1387089606254333

Epoch: 5| Step: 5
Training loss: 2.660338878631592
Validation loss: 2.163974297943936

Epoch: 5| Step: 6
Training loss: 2.082686185836792
Validation loss: 2.1562314725691274

Epoch: 5| Step: 7
Training loss: 2.359422206878662
Validation loss: 2.146984459251486

Epoch: 5| Step: 8
Training loss: 2.083890438079834
Validation loss: 2.153653901110413

Epoch: 5| Step: 9
Training loss: 2.251893997192383
Validation loss: 2.1549316452395533

Epoch: 5| Step: 10
Training loss: 2.380859375
Validation loss: 2.1653370165055796

Epoch: 64| Step: 0
Training loss: 1.6468474864959717
Validation loss: 2.1694812338839293

Epoch: 5| Step: 1
Training loss: 1.5747644901275635
Validation loss: 2.1381091635714293

Epoch: 5| Step: 2
Training loss: 1.6215120553970337
Validation loss: 2.160969175318236

Epoch: 5| Step: 3
Training loss: 2.7913098335266113
Validation loss: 2.1442037602906585

Epoch: 5| Step: 4
Training loss: 2.309147357940674
Validation loss: 2.1495545128340363

Epoch: 5| Step: 5
Training loss: 2.424917697906494
Validation loss: 2.1397366677561114

Epoch: 5| Step: 6
Training loss: 3.346031904220581
Validation loss: 2.1347056691364577

Epoch: 5| Step: 7
Training loss: 1.9841715097427368
Validation loss: 2.150566695838846

Epoch: 5| Step: 8
Training loss: 2.305933713912964
Validation loss: 2.1715596670745523

Epoch: 5| Step: 9
Training loss: 2.447941303253174
Validation loss: 2.1497511658617245

Epoch: 5| Step: 10
Training loss: 2.7696409225463867
Validation loss: 2.1518277557947303

Epoch: 65| Step: 0
Training loss: 2.1448168754577637
Validation loss: 2.143004557137848

Epoch: 5| Step: 1
Training loss: 3.05125093460083
Validation loss: 2.138397732088643

Epoch: 5| Step: 2
Training loss: 1.9454209804534912
Validation loss: 2.173090855280558

Epoch: 5| Step: 3
Training loss: 2.4671471118927
Validation loss: 2.1531359303382134

Epoch: 5| Step: 4
Training loss: 3.241886615753174
Validation loss: 2.1452794459558304

Epoch: 5| Step: 5
Training loss: 2.0712952613830566
Validation loss: 2.1494875723315823

Epoch: 5| Step: 6
Training loss: 2.040184497833252
Validation loss: 2.1538559723925847

Epoch: 5| Step: 7
Training loss: 1.9592857360839844
Validation loss: 2.1820254659139984

Epoch: 5| Step: 8
Training loss: 2.285268545150757
Validation loss: 2.1606315438465407

Epoch: 5| Step: 9
Training loss: 1.8150417804718018
Validation loss: 2.1749920639940488

Epoch: 5| Step: 10
Training loss: 1.9128758907318115
Validation loss: 2.163952342925533

Epoch: 66| Step: 0
Training loss: 1.895546555519104
Validation loss: 2.1723570413486932

Epoch: 5| Step: 1
Training loss: 2.5209946632385254
Validation loss: 2.1420948172128327

Epoch: 5| Step: 2
Training loss: 2.223208427429199
Validation loss: 2.1687873537822435

Epoch: 5| Step: 3
Training loss: 2.667468309402466
Validation loss: 2.158672657064212

Epoch: 5| Step: 4
Training loss: 2.126835584640503
Validation loss: 2.1475169171569166

Epoch: 5| Step: 5
Training loss: 2.236436367034912
Validation loss: 2.1584046220266693

Epoch: 5| Step: 6
Training loss: 2.472398281097412
Validation loss: 2.1589554535445346

Epoch: 5| Step: 7
Training loss: 1.5437166690826416
Validation loss: 2.1385980857315885

Epoch: 5| Step: 8
Training loss: 2.210458278656006
Validation loss: 2.15214943116711

Epoch: 5| Step: 9
Training loss: 2.75687837600708
Validation loss: 2.1628926748870523

Epoch: 5| Step: 10
Training loss: 2.371919631958008
Validation loss: 2.1645166027930474

Epoch: 67| Step: 0
Training loss: 2.836876630783081
Validation loss: 2.166066279975317

Epoch: 5| Step: 1
Training loss: 2.147477865219116
Validation loss: 2.1673178211335213

Epoch: 5| Step: 2
Training loss: 2.558579206466675
Validation loss: 2.1589643596321024

Epoch: 5| Step: 3
Training loss: 2.2599613666534424
Validation loss: 2.161992073059082

Epoch: 5| Step: 4
Training loss: 2.1238462924957275
Validation loss: 2.1635265222159763

Epoch: 5| Step: 5
Training loss: 2.0401084423065186
Validation loss: 2.1647416135316253

Epoch: 5| Step: 6
Training loss: 1.8644282817840576
Validation loss: 2.1367008609156453

Epoch: 5| Step: 7
Training loss: 2.8470559120178223
Validation loss: 2.1321644398473922

Epoch: 5| Step: 8
Training loss: 1.8937203884124756
Validation loss: 2.1581342784307336

Epoch: 5| Step: 9
Training loss: 2.4920730590820312
Validation loss: 2.1600259401464976

Epoch: 5| Step: 10
Training loss: 2.0202248096466064
Validation loss: 2.1677737338568575

Epoch: 68| Step: 0
Training loss: 1.799070954322815
Validation loss: 2.1525879162614063

Epoch: 5| Step: 1
Training loss: 2.5018703937530518
Validation loss: 2.1707310625301894

Epoch: 5| Step: 2
Training loss: 2.557535409927368
Validation loss: 2.162571043096563

Epoch: 5| Step: 3
Training loss: 2.5812466144561768
Validation loss: 2.1657889889132593

Epoch: 5| Step: 4
Training loss: 2.6000213623046875
Validation loss: 2.1628069980170137

Epoch: 5| Step: 5
Training loss: 1.8034112453460693
Validation loss: 2.150884884659962

Epoch: 5| Step: 6
Training loss: 2.8003056049346924
Validation loss: 2.1743689608830277

Epoch: 5| Step: 7
Training loss: 1.9079463481903076
Validation loss: 2.1623618602752686

Epoch: 5| Step: 8
Training loss: 2.9881653785705566
Validation loss: 2.1453871470625683

Epoch: 5| Step: 9
Training loss: 1.6676690578460693
Validation loss: 2.158397105432326

Epoch: 5| Step: 10
Training loss: 1.6614166498184204
Validation loss: 2.1384879478844265

Epoch: 69| Step: 0
Training loss: 2.249816656112671
Validation loss: 2.15394881720184

Epoch: 5| Step: 1
Training loss: 2.3613152503967285
Validation loss: 2.1532177694382204

Epoch: 5| Step: 2
Training loss: 2.298119068145752
Validation loss: 2.1352549983609106

Epoch: 5| Step: 3
Training loss: 2.4957573413848877
Validation loss: 2.1481571607692267

Epoch: 5| Step: 4
Training loss: 2.9409475326538086
Validation loss: 2.1422692139943442

Epoch: 5| Step: 5
Training loss: 1.8968608379364014
Validation loss: 2.1456215996896066

Epoch: 5| Step: 6
Training loss: 2.242497682571411
Validation loss: 2.1378027290426274

Epoch: 5| Step: 7
Training loss: 1.5461528301239014
Validation loss: 2.148344011716945

Epoch: 5| Step: 8
Training loss: 2.0708727836608887
Validation loss: 2.1750167826170563

Epoch: 5| Step: 9
Training loss: 2.708968162536621
Validation loss: 2.160410229877759

Epoch: 5| Step: 10
Training loss: 2.2452423572540283
Validation loss: 2.1463711133567234

Epoch: 70| Step: 0
Training loss: 2.3997650146484375
Validation loss: 2.159575818687357

Epoch: 5| Step: 1
Training loss: 2.5685677528381348
Validation loss: 2.1651542699465187

Epoch: 5| Step: 2
Training loss: 2.379727840423584
Validation loss: 2.1625619293541036

Epoch: 5| Step: 3
Training loss: 2.1398959159851074
Validation loss: 2.128793289584498

Epoch: 5| Step: 4
Training loss: 2.685739755630493
Validation loss: 2.150542131034277

Epoch: 5| Step: 5
Training loss: 2.7768588066101074
Validation loss: 2.1447056313996673

Epoch: 5| Step: 6
Training loss: 1.6299762725830078
Validation loss: 2.1372843327060824

Epoch: 5| Step: 7
Training loss: 1.993821144104004
Validation loss: 2.146288848692371

Epoch: 5| Step: 8
Training loss: 2.293942928314209
Validation loss: 2.1473437175955823

Epoch: 5| Step: 9
Training loss: 1.9704891443252563
Validation loss: 2.1395741483216644

Epoch: 5| Step: 10
Training loss: 2.2592945098876953
Validation loss: 2.1539005874305643

Epoch: 71| Step: 0
Training loss: 1.5829176902770996
Validation loss: 2.1411734755321215

Epoch: 5| Step: 1
Training loss: 1.851445198059082
Validation loss: 2.1403664760692145

Epoch: 5| Step: 2
Training loss: 2.6113314628601074
Validation loss: 2.155637962843782

Epoch: 5| Step: 3
Training loss: 1.7700386047363281
Validation loss: 2.15959434611823

Epoch: 5| Step: 4
Training loss: 2.507929801940918
Validation loss: 2.1480868375429543

Epoch: 5| Step: 5
Training loss: 2.63798451423645
Validation loss: 2.1520688431237334

Epoch: 5| Step: 6
Training loss: 2.1017277240753174
Validation loss: 2.1544092009144444

Epoch: 5| Step: 7
Training loss: 2.358187198638916
Validation loss: 2.151878692770517

Epoch: 5| Step: 8
Training loss: 2.0655910968780518
Validation loss: 2.1525917642860004

Epoch: 5| Step: 9
Training loss: 2.715402603149414
Validation loss: 2.154073771610055

Epoch: 5| Step: 10
Training loss: 2.803974151611328
Validation loss: 2.14772762918985

Epoch: 72| Step: 0
Training loss: 2.5153310298919678
Validation loss: 2.151807637624843

Epoch: 5| Step: 1
Training loss: 1.7697875499725342
Validation loss: 2.1653183173107844

Epoch: 5| Step: 2
Training loss: 2.5066781044006348
Validation loss: 2.15270806512525

Epoch: 5| Step: 3
Training loss: 1.8086553812026978
Validation loss: 2.1367904011921217

Epoch: 5| Step: 4
Training loss: 2.3628265857696533
Validation loss: 2.1638509406838367

Epoch: 5| Step: 5
Training loss: 2.5416741371154785
Validation loss: 2.1570341048702115

Epoch: 5| Step: 6
Training loss: 1.509444236755371
Validation loss: 2.15811655598302

Epoch: 5| Step: 7
Training loss: 2.506071090698242
Validation loss: 2.1618527596996677

Epoch: 5| Step: 8
Training loss: 2.286524534225464
Validation loss: 2.1475539412549747

Epoch: 5| Step: 9
Training loss: 2.782118082046509
Validation loss: 2.1727760645651046

Epoch: 5| Step: 10
Training loss: 2.352895736694336
Validation loss: 2.1506966019189484

Epoch: 73| Step: 0
Training loss: 1.395995020866394
Validation loss: 2.155637829534469

Epoch: 5| Step: 1
Training loss: 2.9821975231170654
Validation loss: 2.1653607840179117

Epoch: 5| Step: 2
Training loss: 2.0791964530944824
Validation loss: 2.1615350220793035

Epoch: 5| Step: 3
Training loss: 2.365650177001953
Validation loss: 2.1758495312865063

Epoch: 5| Step: 4
Training loss: 1.8659257888793945
Validation loss: 2.15916633477775

Epoch: 5| Step: 5
Training loss: 2.9345040321350098
Validation loss: 2.1604995535266016

Epoch: 5| Step: 6
Training loss: 2.419076919555664
Validation loss: 2.180185889685026

Epoch: 5| Step: 7
Training loss: 2.1803696155548096
Validation loss: 2.1791685858080463

Epoch: 5| Step: 8
Training loss: 2.1087470054626465
Validation loss: 2.144209490027479

Epoch: 5| Step: 9
Training loss: 1.9542930126190186
Validation loss: 2.1706056441030195

Epoch: 5| Step: 10
Training loss: 2.7635812759399414
Validation loss: 2.174977487133395

Epoch: 74| Step: 0
Training loss: 2.2939059734344482
Validation loss: 2.156049320774694

Epoch: 5| Step: 1
Training loss: 2.0366077423095703
Validation loss: 2.154358447238963

Epoch: 5| Step: 2
Training loss: 1.6675617694854736
Validation loss: 2.1737021246264057

Epoch: 5| Step: 3
Training loss: 2.3873190879821777
Validation loss: 2.169775139900946

Epoch: 5| Step: 4
Training loss: 2.419975757598877
Validation loss: 2.1582946520979687

Epoch: 5| Step: 5
Training loss: 2.7796437740325928
Validation loss: 2.1501051097787838

Epoch: 5| Step: 6
Training loss: 2.3957371711730957
Validation loss: 2.1566244709876274

Epoch: 5| Step: 7
Training loss: 2.6200833320617676
Validation loss: 2.159966795675216

Epoch: 5| Step: 8
Training loss: 1.8866803646087646
Validation loss: 2.170604162318732

Epoch: 5| Step: 9
Training loss: 2.220672130584717
Validation loss: 2.166621514545974

Epoch: 5| Step: 10
Training loss: 2.1457719802856445
Validation loss: 2.1648556673398582

Epoch: 75| Step: 0
Training loss: 1.2738393545150757
Validation loss: 2.166823423036965

Epoch: 5| Step: 1
Training loss: 2.087918758392334
Validation loss: 2.1686912505857405

Epoch: 5| Step: 2
Training loss: 2.836824893951416
Validation loss: 2.1610533832221903

Epoch: 5| Step: 3
Training loss: 1.7210781574249268
Validation loss: 2.162831216730097

Epoch: 5| Step: 4
Training loss: 2.420822858810425
Validation loss: 2.157457846467213

Epoch: 5| Step: 5
Training loss: 2.217156410217285
Validation loss: 2.1649433156495452

Epoch: 5| Step: 6
Training loss: 2.8373985290527344
Validation loss: 2.1516459782918296

Epoch: 5| Step: 7
Training loss: 1.9384349584579468
Validation loss: 2.163754799032724

Epoch: 5| Step: 8
Training loss: 2.3182170391082764
Validation loss: 2.158258976474885

Epoch: 5| Step: 9
Training loss: 2.4231414794921875
Validation loss: 2.182026408051932

Epoch: 5| Step: 10
Training loss: 2.7347989082336426
Validation loss: 2.149139440187844

Epoch: 76| Step: 0
Training loss: 2.3282008171081543
Validation loss: 2.1576094499198337

Epoch: 5| Step: 1
Training loss: 2.366541862487793
Validation loss: 2.165955515318019

Epoch: 5| Step: 2
Training loss: 2.2779228687286377
Validation loss: 2.1657436176012923

Epoch: 5| Step: 3
Training loss: 2.6397299766540527
Validation loss: 2.1675511842132895

Epoch: 5| Step: 4
Training loss: 2.297093152999878
Validation loss: 2.162806559634465

Epoch: 5| Step: 5
Training loss: 1.8494380712509155
Validation loss: 2.1496555715478878

Epoch: 5| Step: 6
Training loss: 2.6217145919799805
Validation loss: 2.16352310744665

Epoch: 5| Step: 7
Training loss: 1.709678292274475
Validation loss: 2.1682170232137046

Epoch: 5| Step: 8
Training loss: 2.406132698059082
Validation loss: 2.164002731282224

Epoch: 5| Step: 9
Training loss: 2.464257001876831
Validation loss: 2.17154691039875

Epoch: 5| Step: 10
Training loss: 1.7331994771957397
Validation loss: 2.1607089068299983

Epoch: 77| Step: 0
Training loss: 2.6382205486297607
Validation loss: 2.1684227963929534

Epoch: 5| Step: 1
Training loss: 2.1349010467529297
Validation loss: 2.169445646706448

Epoch: 5| Step: 2
Training loss: 2.6988303661346436
Validation loss: 2.1596814599088443

Epoch: 5| Step: 3
Training loss: 2.4616189002990723
Validation loss: 2.1536663078492686

Epoch: 5| Step: 4
Training loss: 2.7223124504089355
Validation loss: 2.1679543372123473

Epoch: 5| Step: 5
Training loss: 1.5985033512115479
Validation loss: 2.13580652975267

Epoch: 5| Step: 6
Training loss: 2.20047664642334
Validation loss: 2.168144708038658

Epoch: 5| Step: 7
Training loss: 1.7367312908172607
Validation loss: 2.146866880437379

Epoch: 5| Step: 8
Training loss: 2.5923705101013184
Validation loss: 2.166925499516149

Epoch: 5| Step: 9
Training loss: 2.2501142024993896
Validation loss: 2.152455417058801

Epoch: 5| Step: 10
Training loss: 1.6863059997558594
Validation loss: 2.1780049647054365

Epoch: 78| Step: 0
Training loss: 1.9386329650878906
Validation loss: 2.179563419793242

Epoch: 5| Step: 1
Training loss: 2.2795281410217285
Validation loss: 2.1654122350036458

Epoch: 5| Step: 2
Training loss: 2.012829542160034
Validation loss: 2.153325708963538

Epoch: 5| Step: 3
Training loss: 1.8596441745758057
Validation loss: 2.173354697483842

Epoch: 5| Step: 4
Training loss: 2.793574094772339
Validation loss: 2.180418937436996

Epoch: 5| Step: 5
Training loss: 2.6255218982696533
Validation loss: 2.177949628522319

Epoch: 5| Step: 6
Training loss: 2.0270564556121826
Validation loss: 2.1857472440247894

Epoch: 5| Step: 7
Training loss: 2.5431017875671387
Validation loss: 2.1576012590880036

Epoch: 5| Step: 8
Training loss: 2.1234662532806396
Validation loss: 2.15671177705129

Epoch: 5| Step: 9
Training loss: 2.8124122619628906
Validation loss: 2.180518960440031

Epoch: 5| Step: 10
Training loss: 1.5859301090240479
Validation loss: 2.171538637530419

Epoch: 79| Step: 0
Training loss: 1.7808330059051514
Validation loss: 2.177784030155469

Epoch: 5| Step: 1
Training loss: 2.283684015274048
Validation loss: 2.1713909077387985

Epoch: 5| Step: 2
Training loss: 1.7561094760894775
Validation loss: 2.1622875659696517

Epoch: 5| Step: 3
Training loss: 2.6934146881103516
Validation loss: 2.1665518399207824

Epoch: 5| Step: 4
Training loss: 2.0087618827819824
Validation loss: 2.161865790685018

Epoch: 5| Step: 5
Training loss: 1.9355919361114502
Validation loss: 2.164400267344649

Epoch: 5| Step: 6
Training loss: 2.52508544921875
Validation loss: 2.157895524014709

Epoch: 5| Step: 7
Training loss: 2.048809051513672
Validation loss: 2.145210030258343

Epoch: 5| Step: 8
Training loss: 2.3159775733947754
Validation loss: 2.1671409209569297

Epoch: 5| Step: 9
Training loss: 2.657346487045288
Validation loss: 2.161654354423605

Epoch: 5| Step: 10
Training loss: 2.838061571121216
Validation loss: 2.165993131617064

Epoch: 80| Step: 0
Training loss: 2.2039999961853027
Validation loss: 2.1770797391091623

Epoch: 5| Step: 1
Training loss: 1.9070888757705688
Validation loss: 2.1813764008142615

Epoch: 5| Step: 2
Training loss: 2.6143527030944824
Validation loss: 2.175302349111085

Epoch: 5| Step: 3
Training loss: 2.5328879356384277
Validation loss: 2.1718772201127905

Epoch: 5| Step: 4
Training loss: 1.959821343421936
Validation loss: 2.1640032850286013

Epoch: 5| Step: 5
Training loss: 1.793858289718628
Validation loss: 2.1595491850247948

Epoch: 5| Step: 6
Training loss: 2.297428607940674
Validation loss: 2.186419698499864

Epoch: 5| Step: 7
Training loss: 2.121811628341675
Validation loss: 2.1596187750498452

Epoch: 5| Step: 8
Training loss: 2.5972063541412354
Validation loss: 2.1459811169614076

Epoch: 5| Step: 9
Training loss: 2.0130388736724854
Validation loss: 2.1878435855270713

Epoch: 5| Step: 10
Training loss: 2.797384262084961
Validation loss: 2.159002160513273

Epoch: 81| Step: 0
Training loss: 2.1066489219665527
Validation loss: 2.1733030324341147

Epoch: 5| Step: 1
Training loss: 2.331688642501831
Validation loss: 2.1793859235702024

Epoch: 5| Step: 2
Training loss: 2.6354787349700928
Validation loss: 2.172559412576819

Epoch: 5| Step: 3
Training loss: 1.8665952682495117
Validation loss: 2.1783471235664944

Epoch: 5| Step: 4
Training loss: 1.6130300760269165
Validation loss: 2.164536896572318

Epoch: 5| Step: 5
Training loss: 2.428514242172241
Validation loss: 2.162566743871217

Epoch: 5| Step: 6
Training loss: 3.248450517654419
Validation loss: 2.177010084993096

Epoch: 5| Step: 7
Training loss: 2.279256820678711
Validation loss: 2.134965608196874

Epoch: 5| Step: 8
Training loss: 1.791486382484436
Validation loss: 2.1602694885705107

Epoch: 5| Step: 9
Training loss: 2.241957187652588
Validation loss: 2.173216437780729

Epoch: 5| Step: 10
Training loss: 2.1528067588806152
Validation loss: 2.1656564794560915

Epoch: 82| Step: 0
Training loss: 2.3162639141082764
Validation loss: 2.1772995866755003

Epoch: 5| Step: 1
Training loss: 2.1912405490875244
Validation loss: 2.1731456710446264

Epoch: 5| Step: 2
Training loss: 2.8462207317352295
Validation loss: 2.1873642449737876

Epoch: 5| Step: 3
Training loss: 2.1025052070617676
Validation loss: 2.1683147081764798

Epoch: 5| Step: 4
Training loss: 2.7452564239501953
Validation loss: 2.1859814377241236

Epoch: 5| Step: 5
Training loss: 2.1320884227752686
Validation loss: 2.1525563757906676

Epoch: 5| Step: 6
Training loss: 2.622624158859253
Validation loss: 2.1911218115078506

Epoch: 5| Step: 7
Training loss: 2.1527600288391113
Validation loss: 2.169171843477475

Epoch: 5| Step: 8
Training loss: 1.4828453063964844
Validation loss: 2.179237317013484

Epoch: 5| Step: 9
Training loss: 1.9711501598358154
Validation loss: 2.182498329429216

Epoch: 5| Step: 10
Training loss: 2.157885789871216
Validation loss: 2.1715518787343013

Epoch: 83| Step: 0
Training loss: 2.2878408432006836
Validation loss: 2.1717746539782454

Epoch: 5| Step: 1
Training loss: 1.9734315872192383
Validation loss: 2.170112512444937

Epoch: 5| Step: 2
Training loss: 2.120450019836426
Validation loss: 2.1756467767941055

Epoch: 5| Step: 3
Training loss: 2.4352548122406006
Validation loss: 2.1447711965089202

Epoch: 5| Step: 4
Training loss: 2.377567768096924
Validation loss: 2.153484406009797

Epoch: 5| Step: 5
Training loss: 2.4978489875793457
Validation loss: 2.161834927015407

Epoch: 5| Step: 6
Training loss: 2.035817861557007
Validation loss: 2.1382270346405687

Epoch: 5| Step: 7
Training loss: 2.2959556579589844
Validation loss: 2.1703649925929245

Epoch: 5| Step: 8
Training loss: 2.108463764190674
Validation loss: 2.193161133796938

Epoch: 5| Step: 9
Training loss: 1.8941930532455444
Validation loss: 2.139739974852531

Epoch: 5| Step: 10
Training loss: 2.6462903022766113
Validation loss: 2.161015719495794

Epoch: 84| Step: 0
Training loss: 1.8015639781951904
Validation loss: 2.163260075353807

Epoch: 5| Step: 1
Training loss: 2.430180311203003
Validation loss: 2.1759536189417683

Epoch: 5| Step: 2
Training loss: 2.6866443157196045
Validation loss: 2.1632613674286874

Epoch: 5| Step: 3
Training loss: 1.8595237731933594
Validation loss: 2.1782320058473976

Epoch: 5| Step: 4
Training loss: 2.2188568115234375
Validation loss: 2.1538791874403596

Epoch: 5| Step: 5
Training loss: 1.9746595621109009
Validation loss: 2.1472746095349713

Epoch: 5| Step: 6
Training loss: 2.5769879817962646
Validation loss: 2.136677856086403

Epoch: 5| Step: 7
Training loss: 2.9873950481414795
Validation loss: 2.166701926979967

Epoch: 5| Step: 8
Training loss: 2.3262438774108887
Validation loss: 2.163257065639701

Epoch: 5| Step: 9
Training loss: 1.8466068506240845
Validation loss: 2.17171617989899

Epoch: 5| Step: 10
Training loss: 1.862012505531311
Validation loss: 2.164397560140138

Epoch: 85| Step: 0
Training loss: 2.2034730911254883
Validation loss: 2.1815760110014226

Epoch: 5| Step: 1
Training loss: 2.8081653118133545
Validation loss: 2.157571264492568

Epoch: 5| Step: 2
Training loss: 2.152266025543213
Validation loss: 2.1804906732292584

Epoch: 5| Step: 3
Training loss: 1.804551362991333
Validation loss: 2.1633912568451255

Epoch: 5| Step: 4
Training loss: 1.995473861694336
Validation loss: 2.1621859022366103

Epoch: 5| Step: 5
Training loss: 2.451465129852295
Validation loss: 2.1868910738216933

Epoch: 5| Step: 6
Training loss: 2.2598884105682373
Validation loss: 2.144511274112168

Epoch: 5| Step: 7
Training loss: 2.688307523727417
Validation loss: 2.172901230473672

Epoch: 5| Step: 8
Training loss: 2.116331100463867
Validation loss: 2.1649414929010535

Epoch: 5| Step: 9
Training loss: 1.885725975036621
Validation loss: 2.1507059886891353

Epoch: 5| Step: 10
Training loss: 2.2140462398529053
Validation loss: 2.190737972977341

Epoch: 86| Step: 0
Training loss: 2.0586349964141846
Validation loss: 2.1624749142636537

Epoch: 5| Step: 1
Training loss: 2.40639591217041
Validation loss: 2.1773093541463218

Epoch: 5| Step: 2
Training loss: 2.279240608215332
Validation loss: 2.1553972510881323

Epoch: 5| Step: 3
Training loss: 2.6388285160064697
Validation loss: 2.15416113022835

Epoch: 5| Step: 4
Training loss: 2.076429843902588
Validation loss: 2.158789914141419

Epoch: 5| Step: 5
Training loss: 2.708258628845215
Validation loss: 2.1687777990935952

Epoch: 5| Step: 6
Training loss: 2.084260940551758
Validation loss: 2.163312622295913

Epoch: 5| Step: 7
Training loss: 2.2038276195526123
Validation loss: 2.1530938263862365

Epoch: 5| Step: 8
Training loss: 2.146057605743408
Validation loss: 2.170550469429262

Epoch: 5| Step: 9
Training loss: 2.213306427001953
Validation loss: 2.1624030605439217

Epoch: 5| Step: 10
Training loss: 1.7540826797485352
Validation loss: 2.1719072249627884

Epoch: 87| Step: 0
Training loss: 2.2229676246643066
Validation loss: 2.1714846831496044

Epoch: 5| Step: 1
Training loss: 2.3807249069213867
Validation loss: 2.1516535615408294

Epoch: 5| Step: 2
Training loss: 2.32930326461792
Validation loss: 2.1606879208677556

Epoch: 5| Step: 3
Training loss: 1.8608262538909912
Validation loss: 2.180653659246301

Epoch: 5| Step: 4
Training loss: 2.2216944694519043
Validation loss: 2.1543904799287037

Epoch: 5| Step: 5
Training loss: 2.255967378616333
Validation loss: 2.1523996027567054

Epoch: 5| Step: 6
Training loss: 2.6184802055358887
Validation loss: 2.169297663114404

Epoch: 5| Step: 7
Training loss: 2.88127064704895
Validation loss: 2.1693092828155844

Epoch: 5| Step: 8
Training loss: 1.614286184310913
Validation loss: 2.1745663689028834

Epoch: 5| Step: 9
Training loss: 2.1239473819732666
Validation loss: 2.160446481038165

Epoch: 5| Step: 10
Training loss: 1.9334216117858887
Validation loss: 2.155121950693028

Epoch: 88| Step: 0
Training loss: 2.0720958709716797
Validation loss: 2.17618997891744

Epoch: 5| Step: 1
Training loss: 2.033649444580078
Validation loss: 2.1550800236322547

Epoch: 5| Step: 2
Training loss: 2.5878074169158936
Validation loss: 2.1746391070786344

Epoch: 5| Step: 3
Training loss: 2.000948905944824
Validation loss: 2.1432157742079867

Epoch: 5| Step: 4
Training loss: 1.593387246131897
Validation loss: 2.162526443440427

Epoch: 5| Step: 5
Training loss: 2.8924877643585205
Validation loss: 2.194195839666551

Epoch: 5| Step: 6
Training loss: 1.9740091562271118
Validation loss: 2.1711146164965887

Epoch: 5| Step: 7
Training loss: 2.68645977973938
Validation loss: 2.165085300322502

Epoch: 5| Step: 8
Training loss: 1.8746941089630127
Validation loss: 2.1601535568955126

Epoch: 5| Step: 9
Training loss: 2.8610053062438965
Validation loss: 2.1799869537353516

Epoch: 5| Step: 10
Training loss: 1.7122652530670166
Validation loss: 2.1714280741189116

Epoch: 89| Step: 0
Training loss: 2.6417620182037354
Validation loss: 2.1625994123438352

Epoch: 5| Step: 1
Training loss: 2.76572585105896
Validation loss: 2.1600617900971444

Epoch: 5| Step: 2
Training loss: 1.8515545129776
Validation loss: 2.176492196257396

Epoch: 5| Step: 3
Training loss: 2.0621469020843506
Validation loss: 2.148289503589753

Epoch: 5| Step: 4
Training loss: 2.5822575092315674
Validation loss: 2.1660792289241666

Epoch: 5| Step: 5
Training loss: 1.7674747705459595
Validation loss: 2.172600389808737

Epoch: 5| Step: 6
Training loss: 1.4997457265853882
Validation loss: 2.1822752644938808

Epoch: 5| Step: 7
Training loss: 3.0562195777893066
Validation loss: 2.1646666501158025

Epoch: 5| Step: 8
Training loss: 1.9933589696884155
Validation loss: 2.1824061537301667

Epoch: 5| Step: 9
Training loss: 2.169684648513794
Validation loss: 2.16683397241818

Epoch: 5| Step: 10
Training loss: 2.24357271194458
Validation loss: 2.1679643661745134

Epoch: 90| Step: 0
Training loss: 1.6294962167739868
Validation loss: 2.178593877823122

Epoch: 5| Step: 1
Training loss: 2.448350429534912
Validation loss: 2.1665114587353123

Epoch: 5| Step: 2
Training loss: 2.650855302810669
Validation loss: 2.176162963272423

Epoch: 5| Step: 3
Training loss: 1.8647420406341553
Validation loss: 2.1759080169021443

Epoch: 5| Step: 4
Training loss: 2.15446400642395
Validation loss: 2.1751191128966627

Epoch: 5| Step: 5
Training loss: 2.4588992595672607
Validation loss: 2.185274839401245

Epoch: 5| Step: 6
Training loss: 2.504333972930908
Validation loss: 2.1815993862767376

Epoch: 5| Step: 7
Training loss: 1.9946556091308594
Validation loss: 2.1859337411901003

Epoch: 5| Step: 8
Training loss: 2.194857597351074
Validation loss: 2.170994104877595

Epoch: 5| Step: 9
Training loss: 2.307206392288208
Validation loss: 2.172873003508455

Epoch: 5| Step: 10
Training loss: 2.3181612491607666
Validation loss: 2.167655024477231

Epoch: 91| Step: 0
Training loss: 2.687521457672119
Validation loss: 2.1791213609839

Epoch: 5| Step: 1
Training loss: 2.3301525115966797
Validation loss: 2.1569429546274166

Epoch: 5| Step: 2
Training loss: 2.846128463745117
Validation loss: 2.1648555929942797

Epoch: 5| Step: 3
Training loss: 2.2882869243621826
Validation loss: 2.1764838259707213

Epoch: 5| Step: 4
Training loss: 1.6490757465362549
Validation loss: 2.1723728026113203

Epoch: 5| Step: 5
Training loss: 1.968117117881775
Validation loss: 2.1589548805708527

Epoch: 5| Step: 6
Training loss: 2.3333706855773926
Validation loss: 2.1439355804074194

Epoch: 5| Step: 7
Training loss: 2.0917539596557617
Validation loss: 2.1625052216232463

Epoch: 5| Step: 8
Training loss: 2.3494346141815186
Validation loss: 2.1703030422169673

Epoch: 5| Step: 9
Training loss: 2.031247615814209
Validation loss: 2.174918714390006

Epoch: 5| Step: 10
Training loss: 1.8484405279159546
Validation loss: 2.1544471735595376

Epoch: 92| Step: 0
Training loss: 2.023540735244751
Validation loss: 2.1584829130480365

Epoch: 5| Step: 1
Training loss: 2.0606026649475098
Validation loss: 2.1578090267796672

Epoch: 5| Step: 2
Training loss: 2.7976090908050537
Validation loss: 2.15606673302189

Epoch: 5| Step: 3
Training loss: 1.775844931602478
Validation loss: 2.16386438697897

Epoch: 5| Step: 4
Training loss: 2.016552209854126
Validation loss: 2.179432007574266

Epoch: 5| Step: 5
Training loss: 2.1656980514526367
Validation loss: 2.1738293196565364

Epoch: 5| Step: 6
Training loss: 2.341737985610962
Validation loss: 2.1758131621986307

Epoch: 5| Step: 7
Training loss: 2.446101665496826
Validation loss: 2.1640151739120483

Epoch: 5| Step: 8
Training loss: 1.811627745628357
Validation loss: 2.169622964756463

Epoch: 5| Step: 9
Training loss: 2.141129970550537
Validation loss: 2.187019558363063

Epoch: 5| Step: 10
Training loss: 2.982592821121216
Validation loss: 2.1674626488839426

Epoch: 93| Step: 0
Training loss: 1.9422924518585205
Validation loss: 2.168783423721149

Epoch: 5| Step: 1
Training loss: 2.438671827316284
Validation loss: 2.178508091998357

Epoch: 5| Step: 2
Training loss: 2.3422982692718506
Validation loss: 2.1778614982481925

Epoch: 5| Step: 3
Training loss: 2.5686135292053223
Validation loss: 2.168602599892565

Epoch: 5| Step: 4
Training loss: 2.0525951385498047
Validation loss: 2.178057384747331

Epoch: 5| Step: 5
Training loss: 1.4690840244293213
Validation loss: 2.161416870291515

Epoch: 5| Step: 6
Training loss: 1.7535181045532227
Validation loss: 2.158155929657721

Epoch: 5| Step: 7
Training loss: 2.0512468814849854
Validation loss: 2.1513871377514255

Epoch: 5| Step: 8
Training loss: 2.478806972503662
Validation loss: 2.1603629922354095

Epoch: 5| Step: 9
Training loss: 2.259021520614624
Validation loss: 2.1579639322014263

Epoch: 5| Step: 10
Training loss: 3.0280187129974365
Validation loss: 2.168419966133692

Epoch: 94| Step: 0
Training loss: 1.789337158203125
Validation loss: 2.1783961378118044

Epoch: 5| Step: 1
Training loss: 2.4322762489318848
Validation loss: 2.174124574148527

Epoch: 5| Step: 2
Training loss: 2.9510536193847656
Validation loss: 2.1624829461497646

Epoch: 5| Step: 3
Training loss: 1.9250491857528687
Validation loss: 2.168846079098281

Epoch: 5| Step: 4
Training loss: 1.4886332750320435
Validation loss: 2.1558522332099175

Epoch: 5| Step: 5
Training loss: 2.276545763015747
Validation loss: 2.1642799172350156

Epoch: 5| Step: 6
Training loss: 2.087803602218628
Validation loss: 2.162253600294872

Epoch: 5| Step: 7
Training loss: 2.7454962730407715
Validation loss: 2.1795969983582855

Epoch: 5| Step: 8
Training loss: 2.5469346046447754
Validation loss: 2.146667677869079

Epoch: 5| Step: 9
Training loss: 1.6859099864959717
Validation loss: 2.1609276417763

Epoch: 5| Step: 10
Training loss: 2.4569034576416016
Validation loss: 2.14541656227522

Epoch: 95| Step: 0
Training loss: 2.1115241050720215
Validation loss: 2.160942498073783

Epoch: 5| Step: 1
Training loss: 2.573002338409424
Validation loss: 2.1889937910982358

Epoch: 5| Step: 2
Training loss: 2.3283286094665527
Validation loss: 2.157817563702983

Epoch: 5| Step: 3
Training loss: 1.9377448558807373
Validation loss: 2.17148264761894

Epoch: 5| Step: 4
Training loss: 2.018547534942627
Validation loss: 2.1532493201635217

Epoch: 5| Step: 5
Training loss: 2.665705442428589
Validation loss: 2.165273779182024

Epoch: 5| Step: 6
Training loss: 2.112117290496826
Validation loss: 2.134471975347047

Epoch: 5| Step: 7
Training loss: 2.035590171813965
Validation loss: 2.1743033983374156

Epoch: 5| Step: 8
Training loss: 2.8596749305725098
Validation loss: 2.1710417885934152

Epoch: 5| Step: 9
Training loss: 1.46993088722229
Validation loss: 2.1586895245377735

Epoch: 5| Step: 10
Training loss: 2.2684240341186523
Validation loss: 2.1657216984738588

Epoch: 96| Step: 0
Training loss: 2.0141396522521973
Validation loss: 2.161527684939805

Epoch: 5| Step: 1
Training loss: 2.013845920562744
Validation loss: 2.166908933270362

Epoch: 5| Step: 2
Training loss: 2.4280905723571777
Validation loss: 2.1586196896850423

Epoch: 5| Step: 3
Training loss: 1.9832687377929688
Validation loss: 2.1708541326625372

Epoch: 5| Step: 4
Training loss: 1.6318693161010742
Validation loss: 2.152584186164282

Epoch: 5| Step: 5
Training loss: 2.788329601287842
Validation loss: 2.1652892071713685

Epoch: 5| Step: 6
Training loss: 2.9792332649230957
Validation loss: 2.191711402708484

Epoch: 5| Step: 7
Training loss: 1.9512450695037842
Validation loss: 2.1713588840218

Epoch: 5| Step: 8
Training loss: 2.11311411857605
Validation loss: 2.145385553759913

Epoch: 5| Step: 9
Training loss: 2.426365375518799
Validation loss: 2.1620170044642624

Epoch: 5| Step: 10
Training loss: 1.990238904953003
Validation loss: 2.17172900835673

Epoch: 97| Step: 0
Training loss: 2.086430311203003
Validation loss: 2.196041944206402

Epoch: 5| Step: 1
Training loss: 2.468266487121582
Validation loss: 2.1813768007422007

Epoch: 5| Step: 2
Training loss: 2.2557873725891113
Validation loss: 2.159959109880591

Epoch: 5| Step: 3
Training loss: 2.6467981338500977
Validation loss: 2.155223828490062

Epoch: 5| Step: 4
Training loss: 2.466015338897705
Validation loss: 2.179376966209822

Epoch: 5| Step: 5
Training loss: 2.0778515338897705
Validation loss: 2.1764032328000633

Epoch: 5| Step: 6
Training loss: 2.246825695037842
Validation loss: 2.160464591877435

Epoch: 5| Step: 7
Training loss: 2.4397549629211426
Validation loss: 2.1579696657837077

Epoch: 5| Step: 8
Training loss: 2.3507275581359863
Validation loss: 2.1419171479440506

Epoch: 5| Step: 9
Training loss: 1.6217687129974365
Validation loss: 2.1833814190280054

Epoch: 5| Step: 10
Training loss: 1.5621747970581055
Validation loss: 2.1676370328472507

Epoch: 98| Step: 0
Training loss: 2.1710314750671387
Validation loss: 2.1825724032617386

Epoch: 5| Step: 1
Training loss: 2.745512008666992
Validation loss: 2.1571312771048596

Epoch: 5| Step: 2
Training loss: 1.83795964717865
Validation loss: 2.1364667492528118

Epoch: 5| Step: 3
Training loss: 2.0425503253936768
Validation loss: 2.1594509027337514

Epoch: 5| Step: 4
Training loss: 2.0336334705352783
Validation loss: 2.161192391508369

Epoch: 5| Step: 5
Training loss: 1.6707346439361572
Validation loss: 2.1830325383012013

Epoch: 5| Step: 6
Training loss: 2.346039295196533
Validation loss: 2.177215458244406

Epoch: 5| Step: 7
Training loss: 2.4707157611846924
Validation loss: 2.1696032888145855

Epoch: 5| Step: 8
Training loss: 2.6240334510803223
Validation loss: 2.171000830588802

Epoch: 5| Step: 9
Training loss: 2.2049827575683594
Validation loss: 2.167794983874085

Epoch: 5| Step: 10
Training loss: 2.149507522583008
Validation loss: 2.1468754224879767

Epoch: 99| Step: 0
Training loss: 2.279262065887451
Validation loss: 2.174850348503359

Epoch: 5| Step: 1
Training loss: 2.0973050594329834
Validation loss: 2.161512380005211

Epoch: 5| Step: 2
Training loss: 1.6960859298706055
Validation loss: 2.1632869871713782

Epoch: 5| Step: 3
Training loss: 2.091770648956299
Validation loss: 2.1613909890574794

Epoch: 5| Step: 4
Training loss: 3.115874767303467
Validation loss: 2.1548834231591996

Epoch: 5| Step: 5
Training loss: 2.1890323162078857
Validation loss: 2.163326988938034

Epoch: 5| Step: 6
Training loss: 2.1563808917999268
Validation loss: 2.1503599202761086

Epoch: 5| Step: 7
Training loss: 2.105496644973755
Validation loss: 2.135878129671979

Epoch: 5| Step: 8
Training loss: 2.114656925201416
Validation loss: 2.1762672931917253

Epoch: 5| Step: 9
Training loss: 1.9974582195281982
Validation loss: 2.1482371194388277

Epoch: 5| Step: 10
Training loss: 2.350497007369995
Validation loss: 2.159857532029511

Epoch: 100| Step: 0
Training loss: 1.9218151569366455
Validation loss: 2.179582744516352

Epoch: 5| Step: 1
Training loss: 2.9845311641693115
Validation loss: 2.1576123442701114

Epoch: 5| Step: 2
Training loss: 2.6910762786865234
Validation loss: 2.1638215229075444

Epoch: 5| Step: 3
Training loss: 2.438619613647461
Validation loss: 2.1676563550067205

Epoch: 5| Step: 4
Training loss: 2.444303274154663
Validation loss: 2.167310345557428

Epoch: 5| Step: 5
Training loss: 2.0059735774993896
Validation loss: 2.135558574430404

Epoch: 5| Step: 6
Training loss: 1.6661558151245117
Validation loss: 2.1639735339790263

Epoch: 5| Step: 7
Training loss: 1.712235450744629
Validation loss: 2.1713961388475154

Epoch: 5| Step: 8
Training loss: 2.2907395362854004
Validation loss: 2.1791791685165895

Epoch: 5| Step: 9
Training loss: 2.2809360027313232
Validation loss: 2.1508453635759253

Epoch: 5| Step: 10
Training loss: 1.8275700807571411
Validation loss: 2.179575691940964

Epoch: 101| Step: 0
Training loss: 1.9359076023101807
Validation loss: 2.165364465405864

Epoch: 5| Step: 1
Training loss: 2.1152374744415283
Validation loss: 2.176320811753632

Epoch: 5| Step: 2
Training loss: 2.558884620666504
Validation loss: 2.1704938180984987

Epoch: 5| Step: 3
Training loss: 2.285329818725586
Validation loss: 2.1432928680091776

Epoch: 5| Step: 4
Training loss: 2.052863359451294
Validation loss: 2.1567242042992705

Epoch: 5| Step: 5
Training loss: 2.4479241371154785
Validation loss: 2.1801526674660305

Epoch: 5| Step: 6
Training loss: 2.175846576690674
Validation loss: 2.1791383066485004

Epoch: 5| Step: 7
Training loss: 1.7901731729507446
Validation loss: 2.1815063620126374

Epoch: 5| Step: 8
Training loss: 2.0616707801818848
Validation loss: 2.1831886204340125

Epoch: 5| Step: 9
Training loss: 2.37693452835083
Validation loss: 2.1865027694291967

Epoch: 5| Step: 10
Training loss: 2.4433605670928955
Validation loss: 2.1792046280317408

Epoch: 102| Step: 0
Training loss: 2.8459115028381348
Validation loss: 2.168418135694278

Epoch: 5| Step: 1
Training loss: 1.495577335357666
Validation loss: 2.168611431634554

Epoch: 5| Step: 2
Training loss: 2.3658924102783203
Validation loss: 2.1702990711376233

Epoch: 5| Step: 3
Training loss: 2.435603618621826
Validation loss: 2.152460059811992

Epoch: 5| Step: 4
Training loss: 2.407475709915161
Validation loss: 2.1454137973887946

Epoch: 5| Step: 5
Training loss: 2.1268458366394043
Validation loss: 2.154683587371662

Epoch: 5| Step: 6
Training loss: 2.1606550216674805
Validation loss: 2.1676774486418693

Epoch: 5| Step: 7
Training loss: 2.1122539043426514
Validation loss: 2.180430075173737

Epoch: 5| Step: 8
Training loss: 1.9920661449432373
Validation loss: 2.1641260911059637

Epoch: 5| Step: 9
Training loss: 2.1180405616760254
Validation loss: 2.1611459229582097

Epoch: 5| Step: 10
Training loss: 2.138294219970703
Validation loss: 2.1711161674991732

Epoch: 103| Step: 0
Training loss: 1.8677297830581665
Validation loss: 2.1698056446608676

Epoch: 5| Step: 1
Training loss: 2.707519292831421
Validation loss: 2.1780118147532144

Epoch: 5| Step: 2
Training loss: 2.4442012310028076
Validation loss: 2.153548471389278

Epoch: 5| Step: 3
Training loss: 2.1641159057617188
Validation loss: 2.161098628915766

Epoch: 5| Step: 4
Training loss: 1.8389285802841187
Validation loss: 2.169976983019101

Epoch: 5| Step: 5
Training loss: 2.4378254413604736
Validation loss: 2.1621931957942184

Epoch: 5| Step: 6
Training loss: 2.1062188148498535
Validation loss: 2.1834432771128993

Epoch: 5| Step: 7
Training loss: 2.3670761585235596
Validation loss: 2.178380350912771

Epoch: 5| Step: 8
Training loss: 1.637948989868164
Validation loss: 2.1651757955551147

Epoch: 5| Step: 9
Training loss: 2.721235752105713
Validation loss: 2.1799387803641697

Epoch: 5| Step: 10
Training loss: 2.004906177520752
Validation loss: 2.1692292382640224

Epoch: 104| Step: 0
Training loss: 2.5261070728302
Validation loss: 2.1690101867081015

Epoch: 5| Step: 1
Training loss: 1.880321741104126
Validation loss: 2.1437670056537916

Epoch: 5| Step: 2
Training loss: 2.1962952613830566
Validation loss: 2.159353612571634

Epoch: 5| Step: 3
Training loss: 1.775578260421753
Validation loss: 2.165102128059633

Epoch: 5| Step: 4
Training loss: 2.5485904216766357
Validation loss: 2.1715255706541

Epoch: 5| Step: 5
Training loss: 2.164058208465576
Validation loss: 2.143048463329192

Epoch: 5| Step: 6
Training loss: 2.2579777240753174
Validation loss: 2.169474550472793

Epoch: 5| Step: 7
Training loss: 2.247441530227661
Validation loss: 2.1599043620529996

Epoch: 5| Step: 8
Training loss: 2.3723855018615723
Validation loss: 2.1678606861381122

Epoch: 5| Step: 9
Training loss: 2.3117311000823975
Validation loss: 2.159140666325887

Epoch: 5| Step: 10
Training loss: 1.7265907526016235
Validation loss: 2.168162804777904

Epoch: 105| Step: 0
Training loss: 2.4705214500427246
Validation loss: 2.1533591157646588

Epoch: 5| Step: 1
Training loss: 2.65537166595459
Validation loss: 2.155060024671657

Epoch: 5| Step: 2
Training loss: 2.2419095039367676
Validation loss: 2.165989504065565

Epoch: 5| Step: 3
Training loss: 1.8775752782821655
Validation loss: 2.158261855443319

Epoch: 5| Step: 4
Training loss: 2.3714394569396973
Validation loss: 2.184571646874951

Epoch: 5| Step: 5
Training loss: 1.7362077236175537
Validation loss: 2.153461317862234

Epoch: 5| Step: 6
Training loss: 2.4043128490448
Validation loss: 2.183400700169225

Epoch: 5| Step: 7
Training loss: 2.5611660480499268
Validation loss: 2.1677607490170385

Epoch: 5| Step: 8
Training loss: 2.2622246742248535
Validation loss: 2.1650324354889574

Epoch: 5| Step: 9
Training loss: 1.7633860111236572
Validation loss: 2.1780532406222437

Epoch: 5| Step: 10
Training loss: 1.6922391653060913
Validation loss: 2.175169539707963

Epoch: 106| Step: 0
Training loss: 1.9772393703460693
Validation loss: 2.1572303643790622

Epoch: 5| Step: 1
Training loss: 2.2217600345611572
Validation loss: 2.16651374934822

Epoch: 5| Step: 2
Training loss: 1.7433630228042603
Validation loss: 2.1696879633011354

Epoch: 5| Step: 3
Training loss: 2.1897711753845215
Validation loss: 2.188871278557726

Epoch: 5| Step: 4
Training loss: 2.3827624320983887
Validation loss: 2.166625033142746

Epoch: 5| Step: 5
Training loss: 2.2083580493927
Validation loss: 2.180047885064156

Epoch: 5| Step: 6
Training loss: 3.0330605506896973
Validation loss: 2.157836103952059

Epoch: 5| Step: 7
Training loss: 1.756976842880249
Validation loss: 2.157243541491929

Epoch: 5| Step: 8
Training loss: 2.3034894466400146
Validation loss: 2.152626975890129

Epoch: 5| Step: 9
Training loss: 2.449631929397583
Validation loss: 2.1720868015802033

Epoch: 5| Step: 10
Training loss: 1.782832145690918
Validation loss: 2.1833870833919895

Epoch: 107| Step: 0
Training loss: 2.2730841636657715
Validation loss: 2.1632177111923054

Epoch: 5| Step: 1
Training loss: 2.458470344543457
Validation loss: 2.1663080594872914

Epoch: 5| Step: 2
Training loss: 1.931588888168335
Validation loss: 2.149265020124374

Epoch: 5| Step: 3
Training loss: 1.8345134258270264
Validation loss: 2.1706021575517553

Epoch: 5| Step: 4
Training loss: 2.0764973163604736
Validation loss: 2.171327305096452

Epoch: 5| Step: 5
Training loss: 2.206282377243042
Validation loss: 2.172825144183251

Epoch: 5| Step: 6
Training loss: 2.391496419906616
Validation loss: 2.1687450767845236

Epoch: 5| Step: 7
Training loss: 2.695714235305786
Validation loss: 2.175562377898924

Epoch: 5| Step: 8
Training loss: 2.055041790008545
Validation loss: 2.1609415725995134

Epoch: 5| Step: 9
Training loss: 2.1796534061431885
Validation loss: 2.1641094428236767

Epoch: 5| Step: 10
Training loss: 1.7640074491500854
Validation loss: 2.172058907888269

Epoch: 108| Step: 0
Training loss: 2.7290148735046387
Validation loss: 2.160349808713441

Epoch: 5| Step: 1
Training loss: 2.4260575771331787
Validation loss: 2.1495349996833393

Epoch: 5| Step: 2
Training loss: 2.1034305095672607
Validation loss: 2.1681439620192333

Epoch: 5| Step: 3
Training loss: 2.3418233394622803
Validation loss: 2.17024625244961

Epoch: 5| Step: 4
Training loss: 1.9164339303970337
Validation loss: 2.1548408615973687

Epoch: 5| Step: 5
Training loss: 2.321850061416626
Validation loss: 2.1629581207870157

Epoch: 5| Step: 6
Training loss: 2.2908689975738525
Validation loss: 2.169062165803807

Epoch: 5| Step: 7
Training loss: 1.9188652038574219
Validation loss: 2.1730741147072083

Epoch: 5| Step: 8
Training loss: 2.0066800117492676
Validation loss: 2.1738637121774818

Epoch: 5| Step: 9
Training loss: 2.1240928173065186
Validation loss: 2.161259183319666

Epoch: 5| Step: 10
Training loss: 1.9077092409133911
Validation loss: 2.150944273958924

Epoch: 109| Step: 0
Training loss: 1.633863091468811
Validation loss: 2.1625296377366587

Epoch: 5| Step: 1
Training loss: 2.9505856037139893
Validation loss: 2.1679604053497314

Epoch: 5| Step: 2
Training loss: 1.9806667566299438
Validation loss: 2.158585589419129

Epoch: 5| Step: 3
Training loss: 2.0853538513183594
Validation loss: 2.1549390746701147

Epoch: 5| Step: 4
Training loss: 1.6499313116073608
Validation loss: 2.1527254658360637

Epoch: 5| Step: 5
Training loss: 2.0817456245422363
Validation loss: 2.1756200995496524

Epoch: 5| Step: 6
Training loss: 2.0235538482666016
Validation loss: 2.142430966900241

Epoch: 5| Step: 7
Training loss: 2.549229860305786
Validation loss: 2.160515626271566

Epoch: 5| Step: 8
Training loss: 2.4648890495300293
Validation loss: 2.15515911194586

Epoch: 5| Step: 9
Training loss: 2.3603014945983887
Validation loss: 2.1359691799327893

Epoch: 5| Step: 10
Training loss: 2.2108445167541504
Validation loss: 2.154879823807747

Epoch: 110| Step: 0
Training loss: 1.6985384225845337
Validation loss: 2.161566730468504

Epoch: 5| Step: 1
Training loss: 1.8155651092529297
Validation loss: 2.1526377688172045

Epoch: 5| Step: 2
Training loss: 2.2841954231262207
Validation loss: 2.1513887836087133

Epoch: 5| Step: 3
Training loss: 1.6872808933258057
Validation loss: 2.1763858590074765

Epoch: 5| Step: 4
Training loss: 1.9065790176391602
Validation loss: 2.1538187688396824

Epoch: 5| Step: 5
Training loss: 2.680676221847534
Validation loss: 2.16888169575763

Epoch: 5| Step: 6
Training loss: 3.1005661487579346
Validation loss: 2.152613842359153

Epoch: 5| Step: 7
Training loss: 2.056385040283203
Validation loss: 2.1677302955299296

Epoch: 5| Step: 8
Training loss: 1.9966495037078857
Validation loss: 2.16830192330063

Epoch: 5| Step: 9
Training loss: 2.140733480453491
Validation loss: 2.141973349355882

Epoch: 5| Step: 10
Training loss: 2.6868393421173096
Validation loss: 2.1600006716225737

Epoch: 111| Step: 0
Training loss: 1.5335402488708496
Validation loss: 2.1399623860595045

Epoch: 5| Step: 1
Training loss: 2.1450579166412354
Validation loss: 2.1573657604955856

Epoch: 5| Step: 2
Training loss: 1.8368186950683594
Validation loss: 2.1580101700239283

Epoch: 5| Step: 3
Training loss: 1.9899466037750244
Validation loss: 2.171454979527381

Epoch: 5| Step: 4
Training loss: 1.8835971355438232
Validation loss: 2.129248451161128

Epoch: 5| Step: 5
Training loss: 2.076660633087158
Validation loss: 2.1636613235678723

Epoch: 5| Step: 6
Training loss: 2.7003891468048096
Validation loss: 2.176943886664606

Epoch: 5| Step: 7
Training loss: 2.705129384994507
Validation loss: 2.156885826459495

Epoch: 5| Step: 8
Training loss: 2.83571195602417
Validation loss: 2.159405562185472

Epoch: 5| Step: 9
Training loss: 2.443727493286133
Validation loss: 2.167907712280109

Epoch: 5| Step: 10
Training loss: 2.0562336444854736
Validation loss: 2.1572396985946165

Epoch: 112| Step: 0
Training loss: 1.5530481338500977
Validation loss: 2.159775972366333

Epoch: 5| Step: 1
Training loss: 2.3614795207977295
Validation loss: 2.157300561986944

Epoch: 5| Step: 2
Training loss: 1.8099952936172485
Validation loss: 2.15094066691655

Epoch: 5| Step: 3
Training loss: 2.173182964324951
Validation loss: 2.1660465271242204

Epoch: 5| Step: 4
Training loss: 2.3766589164733887
Validation loss: 2.1668854964676725

Epoch: 5| Step: 5
Training loss: 2.3512110710144043
Validation loss: 2.143276350472563

Epoch: 5| Step: 6
Training loss: 1.7656879425048828
Validation loss: 2.156566354536241

Epoch: 5| Step: 7
Training loss: 2.8828234672546387
Validation loss: 2.1621367675001903

Epoch: 5| Step: 8
Training loss: 2.0843923091888428
Validation loss: 2.1881720071197837

Epoch: 5| Step: 9
Training loss: 2.62996244430542
Validation loss: 2.174103683040988

Epoch: 5| Step: 10
Training loss: 2.1111674308776855
Validation loss: 2.145132731365901

Epoch: 113| Step: 0
Training loss: 1.9142494201660156
Validation loss: 2.154355602879678

Epoch: 5| Step: 1
Training loss: 2.3201658725738525
Validation loss: 2.168652052520424

Epoch: 5| Step: 2
Training loss: 1.8573558330535889
Validation loss: 2.168475158752934

Epoch: 5| Step: 3
Training loss: 2.493929147720337
Validation loss: 2.155996484141196

Epoch: 5| Step: 4
Training loss: 2.713819980621338
Validation loss: 2.170676654384982

Epoch: 5| Step: 5
Training loss: 2.293259382247925
Validation loss: 2.162917624237717

Epoch: 5| Step: 6
Training loss: 1.657493233680725
Validation loss: 2.142047797479937

Epoch: 5| Step: 7
Training loss: 2.19201922416687
Validation loss: 2.1552393359522664

Epoch: 5| Step: 8
Training loss: 2.7586960792541504
Validation loss: 2.166967309931273

Epoch: 5| Step: 9
Training loss: 1.6128019094467163
Validation loss: 2.1816682123368785

Epoch: 5| Step: 10
Training loss: 2.2119948863983154
Validation loss: 2.148447367452806

Epoch: 114| Step: 0
Training loss: 2.4827513694763184
Validation loss: 2.1608819500092538

Epoch: 5| Step: 1
Training loss: 2.73962140083313
Validation loss: 2.169183818242883

Epoch: 5| Step: 2
Training loss: 1.5977587699890137
Validation loss: 2.1448097152094685

Epoch: 5| Step: 3
Training loss: 2.072387933731079
Validation loss: 2.1626404229030816

Epoch: 5| Step: 4
Training loss: 1.3937863111495972
Validation loss: 2.169705543466794

Epoch: 5| Step: 5
Training loss: 3.155709981918335
Validation loss: 2.170513553004111

Epoch: 5| Step: 6
Training loss: 1.9338783025741577
Validation loss: 2.185172068175449

Epoch: 5| Step: 7
Training loss: 1.6225181818008423
Validation loss: 2.191550713713451

Epoch: 5| Step: 8
Training loss: 2.5119616985321045
Validation loss: 2.177590057414065

Epoch: 5| Step: 9
Training loss: 2.2471141815185547
Validation loss: 2.17652448274756

Epoch: 5| Step: 10
Training loss: 2.3252689838409424
Validation loss: 2.1510805622223885

Epoch: 115| Step: 0
Training loss: 2.6714258193969727
Validation loss: 2.151491492025314

Epoch: 5| Step: 1
Training loss: 2.0489325523376465
Validation loss: 2.1653598649527437

Epoch: 5| Step: 2
Training loss: 2.7373175621032715
Validation loss: 2.17040071692518

Epoch: 5| Step: 3
Training loss: 2.1011550426483154
Validation loss: 2.157937629248506

Epoch: 5| Step: 4
Training loss: 1.4778010845184326
Validation loss: 2.170883527366064

Epoch: 5| Step: 5
Training loss: 2.6906676292419434
Validation loss: 2.1727942625681558

Epoch: 5| Step: 6
Training loss: 1.8009229898452759
Validation loss: 2.1513647520413963

Epoch: 5| Step: 7
Training loss: 1.678710699081421
Validation loss: 2.143595451949745

Epoch: 5| Step: 8
Training loss: 2.6811845302581787
Validation loss: 2.172569597921064

Epoch: 5| Step: 9
Training loss: 2.387080192565918
Validation loss: 2.145802115881315

Epoch: 5| Step: 10
Training loss: 1.7426146268844604
Validation loss: 2.167420333431613

Epoch: 116| Step: 0
Training loss: 1.788958191871643
Validation loss: 2.12943450866207

Epoch: 5| Step: 1
Training loss: 2.2609915733337402
Validation loss: 2.151288858024023

Epoch: 5| Step: 2
Training loss: 1.7925240993499756
Validation loss: 2.1676867110754854

Epoch: 5| Step: 3
Training loss: 2.885397434234619
Validation loss: 2.1499242141682613

Epoch: 5| Step: 4
Training loss: 2.1424036026000977
Validation loss: 2.1617366319061606

Epoch: 5| Step: 5
Training loss: 2.4474143981933594
Validation loss: 2.1561995347340903

Epoch: 5| Step: 6
Training loss: 2.3772289752960205
Validation loss: 2.130725311976607

Epoch: 5| Step: 7
Training loss: 1.7060463428497314
Validation loss: 2.1696667030293453

Epoch: 5| Step: 8
Training loss: 1.826573133468628
Validation loss: 2.155671173526395

Epoch: 5| Step: 9
Training loss: 2.828653335571289
Validation loss: 2.149948404681298

Epoch: 5| Step: 10
Training loss: 1.9107329845428467
Validation loss: 2.1804070857263382

Epoch: 117| Step: 0
Training loss: 2.040626287460327
Validation loss: 2.1490844308689074

Epoch: 5| Step: 1
Training loss: 2.366105079650879
Validation loss: 2.1498758305785475

Epoch: 5| Step: 2
Training loss: 1.8218753337860107
Validation loss: 2.158513871572351

Epoch: 5| Step: 3
Training loss: 2.26557993888855
Validation loss: 2.156497323384849

Epoch: 5| Step: 4
Training loss: 2.3715462684631348
Validation loss: 2.151300805871205

Epoch: 5| Step: 5
Training loss: 1.6468425989151
Validation loss: 2.1551799671624297

Epoch: 5| Step: 6
Training loss: 2.716170072555542
Validation loss: 2.16493582981889

Epoch: 5| Step: 7
Training loss: 1.8826849460601807
Validation loss: 2.139502058746994

Epoch: 5| Step: 8
Training loss: 2.3782858848571777
Validation loss: 2.1645866491461314

Epoch: 5| Step: 9
Training loss: 2.396177291870117
Validation loss: 2.1784666866384526

Epoch: 5| Step: 10
Training loss: 1.7853280305862427
Validation loss: 2.150693021794801

Epoch: 118| Step: 0
Training loss: 2.188443660736084
Validation loss: 2.1706968174185803

Epoch: 5| Step: 1
Training loss: 1.5787426233291626
Validation loss: 2.1560777246311145

Epoch: 5| Step: 2
Training loss: 2.4470012187957764
Validation loss: 2.157705549270876

Epoch: 5| Step: 3
Training loss: 2.530287981033325
Validation loss: 2.1541149641877864

Epoch: 5| Step: 4
Training loss: 1.9771902561187744
Validation loss: 2.1558767826326433

Epoch: 5| Step: 5
Training loss: 2.6793031692504883
Validation loss: 2.1443312321939776

Epoch: 5| Step: 6
Training loss: 2.015885353088379
Validation loss: 2.1574606203263804

Epoch: 5| Step: 7
Training loss: 2.2051644325256348
Validation loss: 2.1637564359172696

Epoch: 5| Step: 8
Training loss: 1.8531134128570557
Validation loss: 2.1592867118056103

Epoch: 5| Step: 9
Training loss: 1.8756778240203857
Validation loss: 2.1865876618252007

Epoch: 5| Step: 10
Training loss: 2.5633907318115234
Validation loss: 2.1633682456067813

Epoch: 119| Step: 0
Training loss: 1.6307194232940674
Validation loss: 2.1466570515786447

Epoch: 5| Step: 1
Training loss: 2.251859188079834
Validation loss: 2.1546250338195474

Epoch: 5| Step: 2
Training loss: 1.9239057302474976
Validation loss: 2.178162149203721

Epoch: 5| Step: 3
Training loss: 2.0385825634002686
Validation loss: 2.1551149980996245

Epoch: 5| Step: 4
Training loss: 2.1515591144561768
Validation loss: 2.152595730238063

Epoch: 5| Step: 5
Training loss: 1.8927828073501587
Validation loss: 2.136205686035977

Epoch: 5| Step: 6
Training loss: 1.9167861938476562
Validation loss: 2.1716441005788822

Epoch: 5| Step: 7
Training loss: 3.3437466621398926
Validation loss: 2.1610182267363354

Epoch: 5| Step: 8
Training loss: 2.2588953971862793
Validation loss: 2.166067638704854

Epoch: 5| Step: 9
Training loss: 2.192589282989502
Validation loss: 2.146441508364934

Epoch: 5| Step: 10
Training loss: 2.3539628982543945
Validation loss: 2.147389163253128

Epoch: 120| Step: 0
Training loss: 1.685418725013733
Validation loss: 2.1433466660079135

Epoch: 5| Step: 1
Training loss: 2.1006217002868652
Validation loss: 2.166895212665681

Epoch: 5| Step: 2
Training loss: 2.608139991760254
Validation loss: 2.149201126508815

Epoch: 5| Step: 3
Training loss: 2.5608863830566406
Validation loss: 2.1575295796958347

Epoch: 5| Step: 4
Training loss: 2.6224637031555176
Validation loss: 2.1499216120730162

Epoch: 5| Step: 5
Training loss: 2.515122890472412
Validation loss: 2.164438122062273

Epoch: 5| Step: 6
Training loss: 2.7421300411224365
Validation loss: 2.1564614439523346

Epoch: 5| Step: 7
Training loss: 1.7712428569793701
Validation loss: 2.1470169508329002

Epoch: 5| Step: 8
Training loss: 1.5322519540786743
Validation loss: 2.13127225957891

Epoch: 5| Step: 9
Training loss: 1.6498441696166992
Validation loss: 2.184370579258088

Epoch: 5| Step: 10
Training loss: 1.9245786666870117
Validation loss: 2.1595525844122774

Epoch: 121| Step: 0
Training loss: 2.287022352218628
Validation loss: 2.175150650803761

Epoch: 5| Step: 1
Training loss: 2.2316555976867676
Validation loss: 2.1514296557313655

Epoch: 5| Step: 2
Training loss: 2.7930872440338135
Validation loss: 2.167089100806944

Epoch: 5| Step: 3
Training loss: 2.360852003097534
Validation loss: 2.1554176140857

Epoch: 5| Step: 4
Training loss: 2.3559253215789795
Validation loss: 2.170083539460295

Epoch: 5| Step: 5
Training loss: 2.1635944843292236
Validation loss: 2.1535506504838184

Epoch: 5| Step: 6
Training loss: 2.1514852046966553
Validation loss: 2.16237937378627

Epoch: 5| Step: 7
Training loss: 1.7648141384124756
Validation loss: 2.153582977992232

Epoch: 5| Step: 8
Training loss: 2.0350348949432373
Validation loss: 2.1573185613078456

Epoch: 5| Step: 9
Training loss: 1.289351224899292
Validation loss: 2.148604723715013

Epoch: 5| Step: 10
Training loss: 2.3846845626831055
Validation loss: 2.157773810048257

Epoch: 122| Step: 0
Training loss: 2.3364713191986084
Validation loss: 2.1693011304383636

Epoch: 5| Step: 1
Training loss: 2.371802806854248
Validation loss: 2.174834928204936

Epoch: 5| Step: 2
Training loss: 2.2441184520721436
Validation loss: 2.1332223133374284

Epoch: 5| Step: 3
Training loss: 2.1955819129943848
Validation loss: 2.1606200587364937

Epoch: 5| Step: 4
Training loss: 1.7662582397460938
Validation loss: 2.1485367231471564

Epoch: 5| Step: 5
Training loss: 2.1245908737182617
Validation loss: 2.1534728055359214

Epoch: 5| Step: 6
Training loss: 1.7102206945419312
Validation loss: 2.1604704036507556

Epoch: 5| Step: 7
Training loss: 2.317931652069092
Validation loss: 2.1334787902011665

Epoch: 5| Step: 8
Training loss: 2.360546112060547
Validation loss: 2.1689016665181806

Epoch: 5| Step: 9
Training loss: 1.6888755559921265
Validation loss: 2.1522985299428306

Epoch: 5| Step: 10
Training loss: 2.7090394496917725
Validation loss: 2.1398639781500703

Epoch: 123| Step: 0
Training loss: 1.749366044998169
Validation loss: 2.1607781251271567

Epoch: 5| Step: 1
Training loss: 2.0743820667266846
Validation loss: 2.1528512508638444

Epoch: 5| Step: 2
Training loss: 2.7248754501342773
Validation loss: 2.1571058201533493

Epoch: 5| Step: 3
Training loss: 1.6792888641357422
Validation loss: 2.167269052997712

Epoch: 5| Step: 4
Training loss: 3.1943414211273193
Validation loss: 2.1578201504163843

Epoch: 5| Step: 5
Training loss: 1.652357816696167
Validation loss: 2.150583754303635

Epoch: 5| Step: 6
Training loss: 2.5040526390075684
Validation loss: 2.1657566537139235

Epoch: 5| Step: 7
Training loss: 2.279771327972412
Validation loss: 2.15968676792678

Epoch: 5| Step: 8
Training loss: 1.824958086013794
Validation loss: 2.1597293089794856

Epoch: 5| Step: 9
Training loss: 2.1025595664978027
Validation loss: 2.1655130129988476

Epoch: 5| Step: 10
Training loss: 1.9611921310424805
Validation loss: 2.177039486105724

Epoch: 124| Step: 0
Training loss: 1.8745571374893188
Validation loss: 2.1803455480965237

Epoch: 5| Step: 1
Training loss: 2.4050230979919434
Validation loss: 2.154289235350906

Epoch: 5| Step: 2
Training loss: 2.468595266342163
Validation loss: 2.1750227379542526

Epoch: 5| Step: 3
Training loss: 2.200916290283203
Validation loss: 2.1502778222483974

Epoch: 5| Step: 4
Training loss: 1.8655920028686523
Validation loss: 2.1506030072448072

Epoch: 5| Step: 5
Training loss: 2.035898208618164
Validation loss: 2.164056431862616

Epoch: 5| Step: 6
Training loss: 1.8824338912963867
Validation loss: 2.157552147424349

Epoch: 5| Step: 7
Training loss: 1.6542136669158936
Validation loss: 2.1674339181633404

Epoch: 5| Step: 8
Training loss: 2.7913947105407715
Validation loss: 2.163927467920447

Epoch: 5| Step: 9
Training loss: 2.1869285106658936
Validation loss: 2.1358110545783915

Epoch: 5| Step: 10
Training loss: 2.2686405181884766
Validation loss: 2.1618194656987346

Epoch: 125| Step: 0
Training loss: 2.4335274696350098
Validation loss: 2.1417297496590564

Epoch: 5| Step: 1
Training loss: 2.451075315475464
Validation loss: 2.174243718065241

Epoch: 5| Step: 2
Training loss: 1.6600821018218994
Validation loss: 2.1388800785105717

Epoch: 5| Step: 3
Training loss: 1.9919105768203735
Validation loss: 2.141871134440104

Epoch: 5| Step: 4
Training loss: 2.3345789909362793
Validation loss: 2.1792497993797384

Epoch: 5| Step: 5
Training loss: 1.4127393960952759
Validation loss: 2.1458468001375914

Epoch: 5| Step: 6
Training loss: 2.1534836292266846
Validation loss: 2.145425986218196

Epoch: 5| Step: 7
Training loss: 2.4750759601593018
Validation loss: 2.1530984294029976

Epoch: 5| Step: 8
Training loss: 2.4912140369415283
Validation loss: 2.1565858164141254

Epoch: 5| Step: 9
Training loss: 2.2063100337982178
Validation loss: 2.1449083999920915

Epoch: 5| Step: 10
Training loss: 1.948291540145874
Validation loss: 2.1502705491999143

Epoch: 126| Step: 0
Training loss: 1.8593158721923828
Validation loss: 2.1553164707717074

Epoch: 5| Step: 1
Training loss: 2.5308234691619873
Validation loss: 2.150011574068377

Epoch: 5| Step: 2
Training loss: 1.8715217113494873
Validation loss: 2.161886566428728

Epoch: 5| Step: 3
Training loss: 2.1966493129730225
Validation loss: 2.1688842286345777

Epoch: 5| Step: 4
Training loss: 1.8662903308868408
Validation loss: 2.141790774560744

Epoch: 5| Step: 5
Training loss: 2.56426739692688
Validation loss: 2.1379025802817395

Epoch: 5| Step: 6
Training loss: 2.497966766357422
Validation loss: 2.15664247287217

Epoch: 5| Step: 7
Training loss: 1.8604075908660889
Validation loss: 2.1723590358611076

Epoch: 5| Step: 8
Training loss: 1.9334808588027954
Validation loss: 2.141687716207197

Epoch: 5| Step: 9
Training loss: 2.229196071624756
Validation loss: 2.1677424459047216

Epoch: 5| Step: 10
Training loss: 2.3534750938415527
Validation loss: 2.141671073052191

Epoch: 127| Step: 0
Training loss: 1.9755525588989258
Validation loss: 2.1726748571600965

Epoch: 5| Step: 1
Training loss: 2.4333529472351074
Validation loss: 2.160244336692236

Epoch: 5| Step: 2
Training loss: 1.9968172311782837
Validation loss: 2.159005561182576

Epoch: 5| Step: 3
Training loss: 1.8386560678482056
Validation loss: 2.134811798731486

Epoch: 5| Step: 4
Training loss: 1.9262882471084595
Validation loss: 2.156545836438415

Epoch: 5| Step: 5
Training loss: 2.008758544921875
Validation loss: 2.1703953666071736

Epoch: 5| Step: 6
Training loss: 2.558884382247925
Validation loss: 2.1785916410466677

Epoch: 5| Step: 7
Training loss: 2.5041017532348633
Validation loss: 2.1726816982351322

Epoch: 5| Step: 8
Training loss: 2.548140048980713
Validation loss: 2.1780276119068103

Epoch: 5| Step: 9
Training loss: 2.2864811420440674
Validation loss: 2.159024630823443

Epoch: 5| Step: 10
Training loss: 1.6692410707473755
Validation loss: 2.14008907605243

Epoch: 128| Step: 0
Training loss: 1.5443027019500732
Validation loss: 2.162978838848811

Epoch: 5| Step: 1
Training loss: 1.9766517877578735
Validation loss: 2.1545789087972333

Epoch: 5| Step: 2
Training loss: 2.538989782333374
Validation loss: 2.1578871050188617

Epoch: 5| Step: 3
Training loss: 1.6241657733917236
Validation loss: 2.151229750725531

Epoch: 5| Step: 4
Training loss: 1.9925715923309326
Validation loss: 2.132342743617232

Epoch: 5| Step: 5
Training loss: 2.3643765449523926
Validation loss: 2.158191624508109

Epoch: 5| Step: 6
Training loss: 2.22310471534729
Validation loss: 2.1581701001813336

Epoch: 5| Step: 7
Training loss: 2.10465407371521
Validation loss: 2.136614652090175

Epoch: 5| Step: 8
Training loss: 2.5492897033691406
Validation loss: 2.164082163123674

Epoch: 5| Step: 9
Training loss: 2.15813946723938
Validation loss: 2.143513592340613

Epoch: 5| Step: 10
Training loss: 2.6714015007019043
Validation loss: 2.1418345615427983

Epoch: 129| Step: 0
Training loss: 2.346874237060547
Validation loss: 2.1462529910508024

Epoch: 5| Step: 1
Training loss: 2.750948905944824
Validation loss: 2.1304751544870357

Epoch: 5| Step: 2
Training loss: 2.064326286315918
Validation loss: 2.142054227090651

Epoch: 5| Step: 3
Training loss: 1.646772027015686
Validation loss: 2.134182324973486

Epoch: 5| Step: 4
Training loss: 1.817016363143921
Validation loss: 2.142322862020103

Epoch: 5| Step: 5
Training loss: 1.8408759832382202
Validation loss: 2.1479046844667002

Epoch: 5| Step: 6
Training loss: 2.060591459274292
Validation loss: 2.1315058456954135

Epoch: 5| Step: 7
Training loss: 2.217430591583252
Validation loss: 2.1491153188931045

Epoch: 5| Step: 8
Training loss: 3.0331530570983887
Validation loss: 2.1430517076164164

Epoch: 5| Step: 9
Training loss: 1.5611767768859863
Validation loss: 2.151549918677217

Epoch: 5| Step: 10
Training loss: 2.148082971572876
Validation loss: 2.1613597869873047

Epoch: 130| Step: 0
Training loss: 2.498016834259033
Validation loss: 2.136710161803871

Epoch: 5| Step: 1
Training loss: 1.6108077764511108
Validation loss: 2.165642343541627

Epoch: 5| Step: 2
Training loss: 2.1249518394470215
Validation loss: 2.142347789579822

Epoch: 5| Step: 3
Training loss: 2.317535400390625
Validation loss: 2.1358240394182104

Epoch: 5| Step: 4
Training loss: 2.923194169998169
Validation loss: 2.1504090934671383

Epoch: 5| Step: 5
Training loss: 1.9414293766021729
Validation loss: 2.1567601567955426

Epoch: 5| Step: 6
Training loss: 2.2542614936828613
Validation loss: 2.1511365175247192

Epoch: 5| Step: 7
Training loss: 2.2313590049743652
Validation loss: 2.1584890657855618

Epoch: 5| Step: 8
Training loss: 1.771156907081604
Validation loss: 2.1565961837768555

Epoch: 5| Step: 9
Training loss: 1.6266876459121704
Validation loss: 2.1653093343139975

Epoch: 5| Step: 10
Training loss: 2.25205397605896
Validation loss: 2.1565695770325197

Epoch: 131| Step: 0
Training loss: 1.9894459247589111
Validation loss: 2.1356426490250455

Epoch: 5| Step: 1
Training loss: 2.1449832916259766
Validation loss: 2.1367664619158675

Epoch: 5| Step: 2
Training loss: 2.4303555488586426
Validation loss: 2.165631955669772

Epoch: 5| Step: 3
Training loss: 2.033493995666504
Validation loss: 2.1532652942083215

Epoch: 5| Step: 4
Training loss: 2.499089002609253
Validation loss: 2.149829859374672

Epoch: 5| Step: 5
Training loss: 1.4728970527648926
Validation loss: 2.171022153669788

Epoch: 5| Step: 6
Training loss: 2.2698111534118652
Validation loss: 2.145204710704024

Epoch: 5| Step: 7
Training loss: 1.7610975503921509
Validation loss: 2.1587019453766527

Epoch: 5| Step: 8
Training loss: 2.5352699756622314
Validation loss: 2.145060867391607

Epoch: 5| Step: 9
Training loss: 2.110478162765503
Validation loss: 2.1357434206111456

Epoch: 5| Step: 10
Training loss: 2.359602689743042
Validation loss: 2.1575279594749532

Epoch: 132| Step: 0
Training loss: 1.9468107223510742
Validation loss: 2.152081076816846

Epoch: 5| Step: 1
Training loss: 2.056713104248047
Validation loss: 2.148583899262131

Epoch: 5| Step: 2
Training loss: 1.7647349834442139
Validation loss: 2.1542584332086707

Epoch: 5| Step: 3
Training loss: 2.3332817554473877
Validation loss: 2.1561699272483907

Epoch: 5| Step: 4
Training loss: 1.7521336078643799
Validation loss: 2.142434538051646

Epoch: 5| Step: 5
Training loss: 2.5182793140411377
Validation loss: 2.139842458950576

Epoch: 5| Step: 6
Training loss: 2.460669755935669
Validation loss: 2.158967364218927

Epoch: 5| Step: 7
Training loss: 2.8991498947143555
Validation loss: 2.1597648692387406

Epoch: 5| Step: 8
Training loss: 2.2273762226104736
Validation loss: 2.1650967828689085

Epoch: 5| Step: 9
Training loss: 1.6133664846420288
Validation loss: 2.136221698535386

Epoch: 5| Step: 10
Training loss: 2.058894634246826
Validation loss: 2.1507585279403196

Epoch: 133| Step: 0
Training loss: 2.2939748764038086
Validation loss: 2.133602032097437

Epoch: 5| Step: 1
Training loss: 2.6033549308776855
Validation loss: 2.1264504283987065

Epoch: 5| Step: 2
Training loss: 2.091855764389038
Validation loss: 2.1395673162193707

Epoch: 5| Step: 3
Training loss: 1.2844274044036865
Validation loss: 2.159599737454486

Epoch: 5| Step: 4
Training loss: 1.5801215171813965
Validation loss: 2.1391754637482348

Epoch: 5| Step: 5
Training loss: 2.7648983001708984
Validation loss: 2.1822167468327347

Epoch: 5| Step: 6
Training loss: 1.9690204858779907
Validation loss: 2.152299381071521

Epoch: 5| Step: 7
Training loss: 1.9720207452774048
Validation loss: 2.1418283254869523

Epoch: 5| Step: 8
Training loss: 1.9214046001434326
Validation loss: 2.1469009742941907

Epoch: 5| Step: 9
Training loss: 2.4066452980041504
Validation loss: 2.141343355178833

Epoch: 5| Step: 10
Training loss: 2.7619619369506836
Validation loss: 2.154928198424719

Epoch: 134| Step: 0
Training loss: 1.9283462762832642
Validation loss: 2.1568734325388426

Epoch: 5| Step: 1
Training loss: 1.819687843322754
Validation loss: 2.1460533500999532

Epoch: 5| Step: 2
Training loss: 2.073659658432007
Validation loss: 2.165608795740271

Epoch: 5| Step: 3
Training loss: 2.506587028503418
Validation loss: 2.136814163577172

Epoch: 5| Step: 4
Training loss: 1.8533977270126343
Validation loss: 2.134595565898444

Epoch: 5| Step: 5
Training loss: 1.7924187183380127
Validation loss: 2.142414067381172

Epoch: 5| Step: 6
Training loss: 2.055168390274048
Validation loss: 2.149081814673639

Epoch: 5| Step: 7
Training loss: 2.604374885559082
Validation loss: 2.1448869089926443

Epoch: 5| Step: 8
Training loss: 2.35260009765625
Validation loss: 2.164079100854935

Epoch: 5| Step: 9
Training loss: 2.2522153854370117
Validation loss: 2.146681211327994

Epoch: 5| Step: 10
Training loss: 2.2553746700286865
Validation loss: 2.167241488733599

Epoch: 135| Step: 0
Training loss: 1.6221625804901123
Validation loss: 2.1222320115694435

Epoch: 5| Step: 1
Training loss: 2.038729429244995
Validation loss: 2.1412639284646637

Epoch: 5| Step: 2
Training loss: 1.9699277877807617
Validation loss: 2.1314202918801257

Epoch: 5| Step: 3
Training loss: 2.4746267795562744
Validation loss: 2.154477088682113

Epoch: 5| Step: 4
Training loss: 2.485048770904541
Validation loss: 2.164414974950975

Epoch: 5| Step: 5
Training loss: 2.340282917022705
Validation loss: 2.1324780807700208

Epoch: 5| Step: 6
Training loss: 2.5286879539489746
Validation loss: 2.156745292807138

Epoch: 5| Step: 7
Training loss: 1.727003812789917
Validation loss: 2.151811166476178

Epoch: 5| Step: 8
Training loss: 2.5014145374298096
Validation loss: 2.1493400642948766

Epoch: 5| Step: 9
Training loss: 1.6571534872055054
Validation loss: 2.1474949313748266

Epoch: 5| Step: 10
Training loss: 2.293334484100342
Validation loss: 2.1223738962604153

Epoch: 136| Step: 0
Training loss: 2.2049474716186523
Validation loss: 2.151099520344888

Epoch: 5| Step: 1
Training loss: 1.928680419921875
Validation loss: 2.128111849548996

Epoch: 5| Step: 2
Training loss: 1.8674907684326172
Validation loss: 2.1507201194763184

Epoch: 5| Step: 3
Training loss: 2.2882628440856934
Validation loss: 2.1483369617051977

Epoch: 5| Step: 4
Training loss: 1.6533100605010986
Validation loss: 2.134487634064049

Epoch: 5| Step: 5
Training loss: 2.3016018867492676
Validation loss: 2.128274122873942

Epoch: 5| Step: 6
Training loss: 2.364177703857422
Validation loss: 2.14249304674005

Epoch: 5| Step: 7
Training loss: 2.3663558959960938
Validation loss: 2.1531539450409594

Epoch: 5| Step: 8
Training loss: 2.0586509704589844
Validation loss: 2.142401005632134

Epoch: 5| Step: 9
Training loss: 2.182155132293701
Validation loss: 2.143745204453827

Epoch: 5| Step: 10
Training loss: 2.3626275062561035
Validation loss: 2.134752424814368

Epoch: 137| Step: 0
Training loss: 2.985003709793091
Validation loss: 2.1549294635813725

Epoch: 5| Step: 1
Training loss: 1.5298166275024414
Validation loss: 2.1286283821187992

Epoch: 5| Step: 2
Training loss: 2.624027729034424
Validation loss: 2.13737754924323

Epoch: 5| Step: 3
Training loss: 1.7456897497177124
Validation loss: 2.132910046526181

Epoch: 5| Step: 4
Training loss: 1.6632083654403687
Validation loss: 2.140695507808398

Epoch: 5| Step: 5
Training loss: 2.2562685012817383
Validation loss: 2.152929352175805

Epoch: 5| Step: 6
Training loss: 2.425004482269287
Validation loss: 2.1348281496314594

Epoch: 5| Step: 7
Training loss: 2.0508313179016113
Validation loss: 2.129175780921854

Epoch: 5| Step: 8
Training loss: 2.3800556659698486
Validation loss: 2.146901140930832

Epoch: 5| Step: 9
Training loss: 1.9361202716827393
Validation loss: 2.135032912736298

Epoch: 5| Step: 10
Training loss: 1.7357897758483887
Validation loss: 2.143114495021041

Epoch: 138| Step: 0
Training loss: 2.3073925971984863
Validation loss: 2.1493698755900064

Epoch: 5| Step: 1
Training loss: 2.576422691345215
Validation loss: 2.1531035259205806

Epoch: 5| Step: 2
Training loss: 1.7256866693496704
Validation loss: 2.151283582051595

Epoch: 5| Step: 3
Training loss: 2.538048267364502
Validation loss: 2.1493279164837253

Epoch: 5| Step: 4
Training loss: 1.8238718509674072
Validation loss: 2.1311295211956067

Epoch: 5| Step: 5
Training loss: 1.7986621856689453
Validation loss: 2.155625480477528

Epoch: 5| Step: 6
Training loss: 2.616633653640747
Validation loss: 2.150244064228509

Epoch: 5| Step: 7
Training loss: 1.967063546180725
Validation loss: 2.1699248616413405

Epoch: 5| Step: 8
Training loss: 1.9777072668075562
Validation loss: 2.1500703083571566

Epoch: 5| Step: 9
Training loss: 2.686800479888916
Validation loss: 2.1384258526627735

Epoch: 5| Step: 10
Training loss: 1.2519174814224243
Validation loss: 2.1523393866836384

Epoch: 139| Step: 0
Training loss: 2.5806007385253906
Validation loss: 2.1649368040023313

Epoch: 5| Step: 1
Training loss: 2.217576503753662
Validation loss: 2.1745016933769308

Epoch: 5| Step: 2
Training loss: 2.3121438026428223
Validation loss: 2.15213006542575

Epoch: 5| Step: 3
Training loss: 1.8797019720077515
Validation loss: 2.1388140019550117

Epoch: 5| Step: 4
Training loss: 1.8480350971221924
Validation loss: 2.1619121515622703

Epoch: 5| Step: 5
Training loss: 1.8169158697128296
Validation loss: 2.1484438142468854

Epoch: 5| Step: 6
Training loss: 2.578932046890259
Validation loss: 2.1339641463372017

Epoch: 5| Step: 7
Training loss: 2.062002420425415
Validation loss: 2.1545093649177143

Epoch: 5| Step: 8
Training loss: 2.1297802925109863
Validation loss: 2.1323812994905698

Epoch: 5| Step: 9
Training loss: 2.4532032012939453
Validation loss: 2.1475351241327103

Epoch: 5| Step: 10
Training loss: 1.348021149635315
Validation loss: 2.117355121079312

Epoch: 140| Step: 0
Training loss: 2.2393524646759033
Validation loss: 2.131588110359766

Epoch: 5| Step: 1
Training loss: 2.2072861194610596
Validation loss: 2.138305796089993

Epoch: 5| Step: 2
Training loss: 1.7182643413543701
Validation loss: 2.1531292623089207

Epoch: 5| Step: 3
Training loss: 2.237708568572998
Validation loss: 2.1235666095569568

Epoch: 5| Step: 4
Training loss: 2.4147896766662598
Validation loss: 2.139694201048984

Epoch: 5| Step: 5
Training loss: 2.3524885177612305
Validation loss: 2.1468433718527518

Epoch: 5| Step: 6
Training loss: 1.9143588542938232
Validation loss: 2.1274154519522064

Epoch: 5| Step: 7
Training loss: 2.3520846366882324
Validation loss: 2.141311084070513

Epoch: 5| Step: 8
Training loss: 1.955251693725586
Validation loss: 2.132852469721148

Epoch: 5| Step: 9
Training loss: 2.2705678939819336
Validation loss: 2.1525456956637803

Epoch: 5| Step: 10
Training loss: 1.7770541906356812
Validation loss: 2.12294328084556

Epoch: 141| Step: 0
Training loss: 2.8010096549987793
Validation loss: 2.1473978668130855

Epoch: 5| Step: 1
Training loss: 2.3850555419921875
Validation loss: 2.1406839432254916

Epoch: 5| Step: 2
Training loss: 1.728101134300232
Validation loss: 2.155304329369658

Epoch: 5| Step: 3
Training loss: 2.3087894916534424
Validation loss: 2.0990488298477663

Epoch: 5| Step: 4
Training loss: 2.142686367034912
Validation loss: 2.128193388703049

Epoch: 5| Step: 5
Training loss: 2.5987565517425537
Validation loss: 2.1384976025550597

Epoch: 5| Step: 6
Training loss: 1.6520477533340454
Validation loss: 2.1356204837881108

Epoch: 5| Step: 7
Training loss: 2.1397452354431152
Validation loss: 2.1362386570181897

Epoch: 5| Step: 8
Training loss: 2.180182456970215
Validation loss: 2.1432674930941675

Epoch: 5| Step: 9
Training loss: 1.3891925811767578
Validation loss: 2.1393627171875327

Epoch: 5| Step: 10
Training loss: 2.103715658187866
Validation loss: 2.1370028988007577

Epoch: 142| Step: 0
Training loss: 2.291649103164673
Validation loss: 2.1361328453146

Epoch: 5| Step: 1
Training loss: 1.9934043884277344
Validation loss: 2.122987298555272

Epoch: 5| Step: 2
Training loss: 1.7101144790649414
Validation loss: 2.1469928603018484

Epoch: 5| Step: 3
Training loss: 2.5521154403686523
Validation loss: 2.1467073117533038

Epoch: 5| Step: 4
Training loss: 1.6506694555282593
Validation loss: 2.174112637837728

Epoch: 5| Step: 5
Training loss: 2.3869528770446777
Validation loss: 2.150849078291206

Epoch: 5| Step: 6
Training loss: 1.7916768789291382
Validation loss: 2.1501367220314602

Epoch: 5| Step: 7
Training loss: 2.4018747806549072
Validation loss: 2.1205756741185344

Epoch: 5| Step: 8
Training loss: 1.843152642250061
Validation loss: 2.167814549579415

Epoch: 5| Step: 9
Training loss: 2.4768033027648926
Validation loss: 2.125869240812076

Epoch: 5| Step: 10
Training loss: 2.254323959350586
Validation loss: 2.1335550738919165

Epoch: 143| Step: 0
Training loss: 2.1817104816436768
Validation loss: 2.125883835618214

Epoch: 5| Step: 1
Training loss: 2.5247645378112793
Validation loss: 2.138485453462088

Epoch: 5| Step: 2
Training loss: 2.697359800338745
Validation loss: 2.1324107570032917

Epoch: 5| Step: 3
Training loss: 2.053147554397583
Validation loss: 2.153365768412108

Epoch: 5| Step: 4
Training loss: 1.8287149667739868
Validation loss: 2.136959240000735

Epoch: 5| Step: 5
Training loss: 1.2904236316680908
Validation loss: 2.147070025884977

Epoch: 5| Step: 6
Training loss: 1.6712194681167603
Validation loss: 2.1381220151019353

Epoch: 5| Step: 7
Training loss: 2.996814012527466
Validation loss: 2.132479603572558

Epoch: 5| Step: 8
Training loss: 2.2964465618133545
Validation loss: 2.1330440249494327

Epoch: 5| Step: 9
Training loss: 1.80255126953125
Validation loss: 2.1407824049713793

Epoch: 5| Step: 10
Training loss: 2.019026756286621
Validation loss: 2.1454766873390443

Epoch: 144| Step: 0
Training loss: 1.3623348474502563
Validation loss: 2.145205795124013

Epoch: 5| Step: 1
Training loss: 1.9443836212158203
Validation loss: 2.1413041391680316

Epoch: 5| Step: 2
Training loss: 2.4605038166046143
Validation loss: 2.137089103780767

Epoch: 5| Step: 3
Training loss: 2.689828395843506
Validation loss: 2.127236362426512

Epoch: 5| Step: 4
Training loss: 2.1446073055267334
Validation loss: 2.1264625698007564

Epoch: 5| Step: 5
Training loss: 2.9735822677612305
Validation loss: 2.151099137080613

Epoch: 5| Step: 6
Training loss: 1.845990538597107
Validation loss: 2.1497828037508073

Epoch: 5| Step: 7
Training loss: 1.9803546667099
Validation loss: 2.1435387211461223

Epoch: 5| Step: 8
Training loss: 1.5307021141052246
Validation loss: 2.140087596831783

Epoch: 5| Step: 9
Training loss: 2.1238059997558594
Validation loss: 2.1418867265024493

Epoch: 5| Step: 10
Training loss: 2.219996929168701
Validation loss: 2.1522629876290598

Epoch: 145| Step: 0
Training loss: 2.044780969619751
Validation loss: 2.139666498348277

Epoch: 5| Step: 1
Training loss: 2.055697202682495
Validation loss: 2.1267243034096173

Epoch: 5| Step: 2
Training loss: 2.4744389057159424
Validation loss: 2.1325090931307886

Epoch: 5| Step: 3
Training loss: 2.710998296737671
Validation loss: 2.1306349961988387

Epoch: 5| Step: 4
Training loss: 1.9364328384399414
Validation loss: 2.1148238592250372

Epoch: 5| Step: 5
Training loss: 2.671452045440674
Validation loss: 2.1654558617581605

Epoch: 5| Step: 6
Training loss: 1.7432425022125244
Validation loss: 2.1286178622194516

Epoch: 5| Step: 7
Training loss: 1.3765826225280762
Validation loss: 2.1300799103193384

Epoch: 5| Step: 8
Training loss: 1.6351032257080078
Validation loss: 2.1236765923038607

Epoch: 5| Step: 9
Training loss: 2.6313490867614746
Validation loss: 2.157018252598342

Epoch: 5| Step: 10
Training loss: 2.098970651626587
Validation loss: 2.158453333762384

Epoch: 146| Step: 0
Training loss: 2.207029104232788
Validation loss: 2.160112411745133

Epoch: 5| Step: 1
Training loss: 2.775535821914673
Validation loss: 2.1483140555761193

Epoch: 5| Step: 2
Training loss: 1.8142375946044922
Validation loss: 2.1420702293354976

Epoch: 5| Step: 3
Training loss: 1.8170061111450195
Validation loss: 2.1293478294085433

Epoch: 5| Step: 4
Training loss: 2.3521580696105957
Validation loss: 2.129772663116455

Epoch: 5| Step: 5
Training loss: 1.6880314350128174
Validation loss: 2.1153320420172905

Epoch: 5| Step: 6
Training loss: 2.5786185264587402
Validation loss: 2.1483519602847356

Epoch: 5| Step: 7
Training loss: 1.481369137763977
Validation loss: 2.1432882765287995

Epoch: 5| Step: 8
Training loss: 2.5418834686279297
Validation loss: 2.1302899724693707

Epoch: 5| Step: 9
Training loss: 2.3321385383605957
Validation loss: 2.1591144146457797

Epoch: 5| Step: 10
Training loss: 1.521046757698059
Validation loss: 2.145966955410537

Epoch: 147| Step: 0
Training loss: 1.8186928033828735
Validation loss: 2.1235566216130413

Epoch: 5| Step: 1
Training loss: 1.6371371746063232
Validation loss: 2.1414345182398313

Epoch: 5| Step: 2
Training loss: 2.0787353515625
Validation loss: 2.1089525761142855

Epoch: 5| Step: 3
Training loss: 2.3687584400177
Validation loss: 2.1199266090187976

Epoch: 5| Step: 4
Training loss: 1.8526920080184937
Validation loss: 2.1254409666984313

Epoch: 5| Step: 5
Training loss: 2.2005324363708496
Validation loss: 2.131694056654489

Epoch: 5| Step: 6
Training loss: 2.3197007179260254
Validation loss: 2.1256462399677565

Epoch: 5| Step: 7
Training loss: 2.4646077156066895
Validation loss: 2.1467268108039774

Epoch: 5| Step: 8
Training loss: 2.5499818325042725
Validation loss: 2.13354968255566

Epoch: 5| Step: 9
Training loss: 1.583847999572754
Validation loss: 2.138694497846788

Epoch: 5| Step: 10
Training loss: 2.4773178100585938
Validation loss: 2.1134942898186306

Epoch: 148| Step: 0
Training loss: 2.078122854232788
Validation loss: 2.0991181353087067

Epoch: 5| Step: 1
Training loss: 2.043788433074951
Validation loss: 2.13603118670884

Epoch: 5| Step: 2
Training loss: 1.9303321838378906
Validation loss: 2.122925804507348

Epoch: 5| Step: 3
Training loss: 2.5278942584991455
Validation loss: 2.1325805853771906

Epoch: 5| Step: 4
Training loss: 2.2876198291778564
Validation loss: 2.105884228983233

Epoch: 5| Step: 5
Training loss: 1.9343509674072266
Validation loss: 2.1103871740320677

Epoch: 5| Step: 6
Training loss: 2.168896198272705
Validation loss: 2.128653642951801

Epoch: 5| Step: 7
Training loss: 2.1846563816070557
Validation loss: 2.111455096993395

Epoch: 5| Step: 8
Training loss: 1.8239225149154663
Validation loss: 2.139839505636564

Epoch: 5| Step: 9
Training loss: 2.2299492359161377
Validation loss: 2.114501554478881

Epoch: 5| Step: 10
Training loss: 2.032492160797119
Validation loss: 2.1236050359664427

Epoch: 149| Step: 0
Training loss: 2.5862984657287598
Validation loss: 2.1162529119881253

Epoch: 5| Step: 1
Training loss: 1.913347840309143
Validation loss: 2.1074482804985455

Epoch: 5| Step: 2
Training loss: 1.9234743118286133
Validation loss: 2.142726298301451

Epoch: 5| Step: 3
Training loss: 2.5351922512054443
Validation loss: 2.1287118465669694

Epoch: 5| Step: 4
Training loss: 2.4057533740997314
Validation loss: 2.1376256917112615

Epoch: 5| Step: 5
Training loss: 1.4165929555892944
Validation loss: 2.105442805956769

Epoch: 5| Step: 6
Training loss: 2.5510056018829346
Validation loss: 2.1198282267457698

Epoch: 5| Step: 7
Training loss: 1.8023402690887451
Validation loss: 2.1286127785200715

Epoch: 5| Step: 8
Training loss: 2.006786346435547
Validation loss: 2.1159852832876225

Epoch: 5| Step: 9
Training loss: 1.9382470846176147
Validation loss: 2.1425692855670886

Epoch: 5| Step: 10
Training loss: 2.198338031768799
Validation loss: 2.1418392478778796

Epoch: 150| Step: 0
Training loss: 1.748300313949585
Validation loss: 2.1228663293264245

Epoch: 5| Step: 1
Training loss: 2.196991443634033
Validation loss: 2.12398346008793

Epoch: 5| Step: 2
Training loss: 2.3257057666778564
Validation loss: 2.1239201971279678

Epoch: 5| Step: 3
Training loss: 1.856154441833496
Validation loss: 2.1347585185881583

Epoch: 5| Step: 4
Training loss: 1.625543236732483
Validation loss: 2.1288460326451126

Epoch: 5| Step: 5
Training loss: 1.7266756296157837
Validation loss: 2.1316242974291564

Epoch: 5| Step: 6
Training loss: 2.1673989295959473
Validation loss: 2.121896195155318

Epoch: 5| Step: 7
Training loss: 2.8521111011505127
Validation loss: 2.1163058280944824

Epoch: 5| Step: 8
Training loss: 2.601168155670166
Validation loss: 2.120412222800716

Epoch: 5| Step: 9
Training loss: 2.0606179237365723
Validation loss: 2.119471760206325

Epoch: 5| Step: 10
Training loss: 1.874672532081604
Validation loss: 2.112816946480864

Epoch: 151| Step: 0
Training loss: 2.5083048343658447
Validation loss: 2.1319786810105845

Epoch: 5| Step: 1
Training loss: 1.5828430652618408
Validation loss: 2.133395976917718

Epoch: 5| Step: 2
Training loss: 2.1400985717773438
Validation loss: 2.134811241139648

Epoch: 5| Step: 3
Training loss: 2.34578275680542
Validation loss: 2.120604717603294

Epoch: 5| Step: 4
Training loss: 2.4769506454467773
Validation loss: 2.1482458370988087

Epoch: 5| Step: 5
Training loss: 2.5076472759246826
Validation loss: 2.1314893050860335

Epoch: 5| Step: 6
Training loss: 2.599947690963745
Validation loss: 2.113246497287545

Epoch: 5| Step: 7
Training loss: 1.9469131231307983
Validation loss: 2.1336016231967556

Epoch: 5| Step: 8
Training loss: 1.413144826889038
Validation loss: 2.1267234202354186

Epoch: 5| Step: 9
Training loss: 1.9124553203582764
Validation loss: 2.1355425375764088

Epoch: 5| Step: 10
Training loss: 1.7307002544403076
Validation loss: 2.104308675694209

Epoch: 152| Step: 0
Training loss: 1.70818293094635
Validation loss: 2.129417088723952

Epoch: 5| Step: 1
Training loss: 2.8863415718078613
Validation loss: 2.118161088676863

Epoch: 5| Step: 2
Training loss: 1.9658530950546265
Validation loss: 2.1306101096573697

Epoch: 5| Step: 3
Training loss: 2.2598941326141357
Validation loss: 2.1451563835144043

Epoch: 5| Step: 4
Training loss: 1.7770557403564453
Validation loss: 2.1076300951742355

Epoch: 5| Step: 5
Training loss: 2.124002456665039
Validation loss: 2.134528956105632

Epoch: 5| Step: 6
Training loss: 2.480231285095215
Validation loss: 2.11957146019064

Epoch: 5| Step: 7
Training loss: 1.6994413137435913
Validation loss: 2.103311125950147

Epoch: 5| Step: 8
Training loss: 2.137769937515259
Validation loss: 2.1308873648284585

Epoch: 5| Step: 9
Training loss: 1.871319055557251
Validation loss: 2.129341111388258

Epoch: 5| Step: 10
Training loss: 2.1369285583496094
Validation loss: 2.137600114268641

Epoch: 153| Step: 0
Training loss: 2.4610390663146973
Validation loss: 2.1267227306160876

Epoch: 5| Step: 1
Training loss: 1.9962375164031982
Validation loss: 2.1151817280759095

Epoch: 5| Step: 2
Training loss: 2.1696937084198
Validation loss: 2.1163209561378724

Epoch: 5| Step: 3
Training loss: 2.1272730827331543
Validation loss: 2.1411541559362925

Epoch: 5| Step: 4
Training loss: 2.3467915058135986
Validation loss: 2.121323344528034

Epoch: 5| Step: 5
Training loss: 1.8241440057754517
Validation loss: 2.1055900127657

Epoch: 5| Step: 6
Training loss: 1.8957383632659912
Validation loss: 2.1209054095770723

Epoch: 5| Step: 7
Training loss: 2.156691074371338
Validation loss: 2.1341921026988695

Epoch: 5| Step: 8
Training loss: 1.7677291631698608
Validation loss: 2.1193365807174356

Epoch: 5| Step: 9
Training loss: 2.660841464996338
Validation loss: 2.1201808580788235

Epoch: 5| Step: 10
Training loss: 1.6016703844070435
Validation loss: 2.096677504559999

Epoch: 154| Step: 0
Training loss: 1.8804528713226318
Validation loss: 2.1508726560941307

Epoch: 5| Step: 1
Training loss: 2.3068454265594482
Validation loss: 2.1150323242269535

Epoch: 5| Step: 2
Training loss: 2.2192869186401367
Validation loss: 2.1241090272062566

Epoch: 5| Step: 3
Training loss: 2.4209346771240234
Validation loss: 2.142953958562625

Epoch: 5| Step: 4
Training loss: 2.1820802688598633
Validation loss: 2.1097797347653295

Epoch: 5| Step: 5
Training loss: 2.871786594390869
Validation loss: 2.1221826537962882

Epoch: 5| Step: 6
Training loss: 1.3947529792785645
Validation loss: 2.1056923763726347

Epoch: 5| Step: 7
Training loss: 2.1355693340301514
Validation loss: 2.1493768743289414

Epoch: 5| Step: 8
Training loss: 1.3173611164093018
Validation loss: 2.1399956300694454

Epoch: 5| Step: 9
Training loss: 1.6973098516464233
Validation loss: 2.1022863670061995

Epoch: 5| Step: 10
Training loss: 2.552544355392456
Validation loss: 2.130319313336444

Epoch: 155| Step: 0
Training loss: 1.9536689519882202
Validation loss: 2.106156546582458

Epoch: 5| Step: 1
Training loss: 2.5757546424865723
Validation loss: 2.1176076832637993

Epoch: 5| Step: 2
Training loss: 2.16640043258667
Validation loss: 2.1450492874268563

Epoch: 5| Step: 3
Training loss: 1.5565025806427002
Validation loss: 2.115409517800936

Epoch: 5| Step: 4
Training loss: 1.440991759300232
Validation loss: 2.1244916274983394

Epoch: 5| Step: 5
Training loss: 2.3652710914611816
Validation loss: 2.1222811616877073

Epoch: 5| Step: 6
Training loss: 2.632277250289917
Validation loss: 2.117004894441174

Epoch: 5| Step: 7
Training loss: 2.605081796646118
Validation loss: 2.1380227176092004

Epoch: 5| Step: 8
Training loss: 1.7982250452041626
Validation loss: 2.1218569124898603

Epoch: 5| Step: 9
Training loss: 2.0621562004089355
Validation loss: 2.1260586887277584

Epoch: 5| Step: 10
Training loss: 1.653874397277832
Validation loss: 2.110597784801196

Epoch: 156| Step: 0
Training loss: 2.356536388397217
Validation loss: 2.1090780637597524

Epoch: 5| Step: 1
Training loss: 2.5800726413726807
Validation loss: 2.1093277021120955

Epoch: 5| Step: 2
Training loss: 1.8091719150543213
Validation loss: 2.0966132366529076

Epoch: 5| Step: 3
Training loss: 1.9921804666519165
Validation loss: 2.1076541280233734

Epoch: 5| Step: 4
Training loss: 2.2923014163970947
Validation loss: 2.1440450568352976

Epoch: 5| Step: 5
Training loss: 1.7967945337295532
Validation loss: 2.1044713169015865

Epoch: 5| Step: 6
Training loss: 1.9515491724014282
Validation loss: 2.1320822110740085

Epoch: 5| Step: 7
Training loss: 2.1568808555603027
Validation loss: 2.1368881348640687

Epoch: 5| Step: 8
Training loss: 2.2407913208007812
Validation loss: 2.1318029536995837

Epoch: 5| Step: 9
Training loss: 2.1786017417907715
Validation loss: 2.1222261023777786

Epoch: 5| Step: 10
Training loss: 1.5275675058364868
Validation loss: 2.1196404169964533

Epoch: 157| Step: 0
Training loss: 2.7216591835021973
Validation loss: 2.106260317628102

Epoch: 5| Step: 1
Training loss: 1.629101037979126
Validation loss: 2.1205438426745835

Epoch: 5| Step: 2
Training loss: 1.6809418201446533
Validation loss: 2.1368289711654826

Epoch: 5| Step: 3
Training loss: 2.3800861835479736
Validation loss: 2.103146473566691

Epoch: 5| Step: 4
Training loss: 1.8446824550628662
Validation loss: 2.1093916559732087

Epoch: 5| Step: 5
Training loss: 2.1122829914093018
Validation loss: 2.161401735839023

Epoch: 5| Step: 6
Training loss: 2.445939540863037
Validation loss: 2.1222652850612516

Epoch: 5| Step: 7
Training loss: 2.6265006065368652
Validation loss: 2.1484107150826404

Epoch: 5| Step: 8
Training loss: 1.8230994939804077
Validation loss: 2.1180279306186143

Epoch: 5| Step: 9
Training loss: 1.7424942255020142
Validation loss: 2.115365989746586

Epoch: 5| Step: 10
Training loss: 1.9977641105651855
Validation loss: 2.121630900649614

Epoch: 158| Step: 0
Training loss: 2.29038667678833
Validation loss: 2.1205267098642167

Epoch: 5| Step: 1
Training loss: 1.654070496559143
Validation loss: 2.129186486685148

Epoch: 5| Step: 2
Training loss: 2.4122982025146484
Validation loss: 2.119334670805162

Epoch: 5| Step: 3
Training loss: 1.739453911781311
Validation loss: 2.111907564183717

Epoch: 5| Step: 4
Training loss: 2.00285267829895
Validation loss: 2.112165222885788

Epoch: 5| Step: 5
Training loss: 2.4942023754119873
Validation loss: 2.102750362888459

Epoch: 5| Step: 6
Training loss: 2.2600417137145996
Validation loss: 2.112107066697972

Epoch: 5| Step: 7
Training loss: 1.4946541786193848
Validation loss: 2.136900617230323

Epoch: 5| Step: 8
Training loss: 2.4328362941741943
Validation loss: 2.1022236219016452

Epoch: 5| Step: 9
Training loss: 1.9132106304168701
Validation loss: 2.1171178305020897

Epoch: 5| Step: 10
Training loss: 1.9743156433105469
Validation loss: 2.1443589092582784

Epoch: 159| Step: 0
Training loss: 1.615431547164917
Validation loss: 2.1098600023536274

Epoch: 5| Step: 1
Training loss: 2.2757511138916016
Validation loss: 2.1268302420134186

Epoch: 5| Step: 2
Training loss: 2.7747421264648438
Validation loss: 2.111201406807028

Epoch: 5| Step: 3
Training loss: 1.8162342309951782
Validation loss: 2.1030471683830343

Epoch: 5| Step: 4
Training loss: 2.233022689819336
Validation loss: 2.1160894696430494

Epoch: 5| Step: 5
Training loss: 1.6390517950057983
Validation loss: 2.1176965800664758

Epoch: 5| Step: 6
Training loss: 2.1668365001678467
Validation loss: 2.1066391903867006

Epoch: 5| Step: 7
Training loss: 2.8054964542388916
Validation loss: 2.1149763932792087

Epoch: 5| Step: 8
Training loss: 1.5950125455856323
Validation loss: 2.1122042825145106

Epoch: 5| Step: 9
Training loss: 1.7103359699249268
Validation loss: 2.1111041858632076

Epoch: 5| Step: 10
Training loss: 2.3098020553588867
Validation loss: 2.10797974371141

Epoch: 160| Step: 0
Training loss: 1.8233616352081299
Validation loss: 2.114774550161054

Epoch: 5| Step: 1
Training loss: 1.8299938440322876
Validation loss: 2.1131422314592587

Epoch: 5| Step: 2
Training loss: 2.2210826873779297
Validation loss: 2.104081820416194

Epoch: 5| Step: 3
Training loss: 1.9110019207000732
Validation loss: 2.112937819573187

Epoch: 5| Step: 4
Training loss: 2.0961246490478516
Validation loss: 2.100100701855075

Epoch: 5| Step: 5
Training loss: 2.6084628105163574
Validation loss: 2.118246901419855

Epoch: 5| Step: 6
Training loss: 2.327739715576172
Validation loss: 2.087625206157725

Epoch: 5| Step: 7
Training loss: 1.6266472339630127
Validation loss: 2.102608819161692

Epoch: 5| Step: 8
Training loss: 2.099353313446045
Validation loss: 2.10575694166204

Epoch: 5| Step: 9
Training loss: 2.034162998199463
Validation loss: 2.141452161214685

Epoch: 5| Step: 10
Training loss: 2.0207278728485107
Validation loss: 2.0999960848080215

Epoch: 161| Step: 0
Training loss: 2.068939685821533
Validation loss: 2.0922838167477678

Epoch: 5| Step: 1
Training loss: 1.5922244787216187
Validation loss: 2.1057962730366695

Epoch: 5| Step: 2
Training loss: 1.7813894748687744
Validation loss: 2.120204253863263

Epoch: 5| Step: 3
Training loss: 2.1732332706451416
Validation loss: 2.128691837351809

Epoch: 5| Step: 4
Training loss: 1.9930543899536133
Validation loss: 2.0907029208316597

Epoch: 5| Step: 5
Training loss: 2.1795268058776855
Validation loss: 2.114290328436

Epoch: 5| Step: 6
Training loss: 2.2366833686828613
Validation loss: 2.125475166946329

Epoch: 5| Step: 7
Training loss: 2.3482136726379395
Validation loss: 2.1168513605671544

Epoch: 5| Step: 8
Training loss: 1.9812809228897095
Validation loss: 2.120520745554278

Epoch: 5| Step: 9
Training loss: 2.2814524173736572
Validation loss: 2.105277958736625

Epoch: 5| Step: 10
Training loss: 2.178560256958008
Validation loss: 2.103763425222007

Epoch: 162| Step: 0
Training loss: 1.7720924615859985
Validation loss: 2.123233954111735

Epoch: 5| Step: 1
Training loss: 2.0838634967803955
Validation loss: 2.085594992483816

Epoch: 5| Step: 2
Training loss: 2.759838342666626
Validation loss: 2.0973630566750803

Epoch: 5| Step: 3
Training loss: 2.2106919288635254
Validation loss: 2.102606560594292

Epoch: 5| Step: 4
Training loss: 1.6276180744171143
Validation loss: 2.100466083454829

Epoch: 5| Step: 5
Training loss: 2.3104453086853027
Validation loss: 2.098295634792697

Epoch: 5| Step: 6
Training loss: 1.6878480911254883
Validation loss: 2.1115569478722027

Epoch: 5| Step: 7
Training loss: 2.3360953330993652
Validation loss: 2.1091062612431024

Epoch: 5| Step: 8
Training loss: 2.0585103034973145
Validation loss: 2.1123762746011057

Epoch: 5| Step: 9
Training loss: 2.1665821075439453
Validation loss: 2.0851720815063803

Epoch: 5| Step: 10
Training loss: 1.6729540824890137
Validation loss: 2.0985004209703013

Epoch: 163| Step: 0
Training loss: 2.79245924949646
Validation loss: 2.132830327556979

Epoch: 5| Step: 1
Training loss: 2.2252721786499023
Validation loss: 2.0975630283355713

Epoch: 5| Step: 2
Training loss: 2.359215259552002
Validation loss: 2.0988820150334346

Epoch: 5| Step: 3
Training loss: 2.143153190612793
Validation loss: 2.133878638667445

Epoch: 5| Step: 4
Training loss: 1.608184814453125
Validation loss: 2.1201139470582366

Epoch: 5| Step: 5
Training loss: 1.5360398292541504
Validation loss: 2.119306719431313

Epoch: 5| Step: 6
Training loss: 1.8362150192260742
Validation loss: 2.12253132302274

Epoch: 5| Step: 7
Training loss: 2.628081798553467
Validation loss: 2.102661073848765

Epoch: 5| Step: 8
Training loss: 1.6478383541107178
Validation loss: 2.102077030366467

Epoch: 5| Step: 9
Training loss: 2.357295036315918
Validation loss: 2.1123753465631956

Epoch: 5| Step: 10
Training loss: 1.5535764694213867
Validation loss: 2.1183511108480473

Epoch: 164| Step: 0
Training loss: 1.3603103160858154
Validation loss: 2.105401505706131

Epoch: 5| Step: 1
Training loss: 1.9201133251190186
Validation loss: 2.108702003314931

Epoch: 5| Step: 2
Training loss: 2.5068018436431885
Validation loss: 2.095814756167832

Epoch: 5| Step: 3
Training loss: 2.2117161750793457
Validation loss: 2.108800075387442

Epoch: 5| Step: 4
Training loss: 2.411860227584839
Validation loss: 2.105959710254464

Epoch: 5| Step: 5
Training loss: 2.4419987201690674
Validation loss: 2.089115608123041

Epoch: 5| Step: 6
Training loss: 1.8646557331085205
Validation loss: 2.0802159617024083

Epoch: 5| Step: 7
Training loss: 1.7357265949249268
Validation loss: 2.1236299622443413

Epoch: 5| Step: 8
Training loss: 1.8052866458892822
Validation loss: 2.1316969727957122

Epoch: 5| Step: 9
Training loss: 2.0392181873321533
Validation loss: 2.095243723161759

Epoch: 5| Step: 10
Training loss: 2.3853843212127686
Validation loss: 2.130944836524225

Epoch: 165| Step: 0
Training loss: 1.9468272924423218
Validation loss: 2.0975004652495026

Epoch: 5| Step: 1
Training loss: 2.116793155670166
Validation loss: 2.1119974326061945

Epoch: 5| Step: 2
Training loss: 1.916537880897522
Validation loss: 2.100286544010203

Epoch: 5| Step: 3
Training loss: 2.3546910285949707
Validation loss: 2.0734671162020777

Epoch: 5| Step: 4
Training loss: 2.1561362743377686
Validation loss: 2.124794558812213

Epoch: 5| Step: 5
Training loss: 1.6834156513214111
Validation loss: 2.0864227330812843

Epoch: 5| Step: 6
Training loss: 2.2699615955352783
Validation loss: 2.1148711148128716

Epoch: 5| Step: 7
Training loss: 1.8028011322021484
Validation loss: 2.1034483191787556

Epoch: 5| Step: 8
Training loss: 2.707265853881836
Validation loss: 2.093782046789764

Epoch: 5| Step: 9
Training loss: 2.069812774658203
Validation loss: 2.0982417983393513

Epoch: 5| Step: 10
Training loss: 1.349156379699707
Validation loss: 2.100587380829678

Epoch: 166| Step: 0
Training loss: 2.8696532249450684
Validation loss: 2.1161897246555617

Epoch: 5| Step: 1
Training loss: 2.14632248878479
Validation loss: 2.1165063650377336

Epoch: 5| Step: 2
Training loss: 1.871861219406128
Validation loss: 2.1164060536251275

Epoch: 5| Step: 3
Training loss: 1.9855530261993408
Validation loss: 2.0947016862130936

Epoch: 5| Step: 4
Training loss: 1.9065831899642944
Validation loss: 2.112465675159167

Epoch: 5| Step: 5
Training loss: 2.2059757709503174
Validation loss: 2.1278310411719867

Epoch: 5| Step: 6
Training loss: 1.8488162755966187
Validation loss: 2.127169524469683

Epoch: 5| Step: 7
Training loss: 1.6904939413070679
Validation loss: 2.1205705788827713

Epoch: 5| Step: 8
Training loss: 1.9656330347061157
Validation loss: 2.1100269568863737

Epoch: 5| Step: 9
Training loss: 2.5064406394958496
Validation loss: 2.123880483770883

Epoch: 5| Step: 10
Training loss: 1.6019854545593262
Validation loss: 2.140796099939654

Epoch: 167| Step: 0
Training loss: 1.8185443878173828
Validation loss: 2.1236408500261206

Epoch: 5| Step: 1
Training loss: 2.0532846450805664
Validation loss: 2.102983426022273

Epoch: 5| Step: 2
Training loss: 2.1507036685943604
Validation loss: 2.097546510798957

Epoch: 5| Step: 3
Training loss: 2.255244016647339
Validation loss: 2.1270878314971924

Epoch: 5| Step: 4
Training loss: 1.986396074295044
Validation loss: 2.1188853966292513

Epoch: 5| Step: 5
Training loss: 1.5109758377075195
Validation loss: 2.091005171498945

Epoch: 5| Step: 6
Training loss: 2.37152099609375
Validation loss: 2.0765310769440024

Epoch: 5| Step: 7
Training loss: 2.961517333984375
Validation loss: 2.119417969898511

Epoch: 5| Step: 8
Training loss: 1.7721798419952393
Validation loss: 2.119994196840512

Epoch: 5| Step: 9
Training loss: 2.3339037895202637
Validation loss: 2.107260437421901

Epoch: 5| Step: 10
Training loss: 1.6086997985839844
Validation loss: 2.114485389442854

Epoch: 168| Step: 0
Training loss: 1.7209491729736328
Validation loss: 2.1039799144191127

Epoch: 5| Step: 1
Training loss: 1.9042373895645142
Validation loss: 2.1078903393078874

Epoch: 5| Step: 2
Training loss: 2.3230912685394287
Validation loss: 2.1102843899880686

Epoch: 5| Step: 3
Training loss: 2.004610776901245
Validation loss: 2.092494462126045

Epoch: 5| Step: 4
Training loss: 2.543307304382324
Validation loss: 2.0987212158018544

Epoch: 5| Step: 5
Training loss: 2.216325044631958
Validation loss: 2.1012386173330326

Epoch: 5| Step: 6
Training loss: 1.5581893920898438
Validation loss: 2.093581438064575

Epoch: 5| Step: 7
Training loss: 2.0143914222717285
Validation loss: 2.088264565314016

Epoch: 5| Step: 8
Training loss: 2.102858066558838
Validation loss: 2.0945872158132572

Epoch: 5| Step: 9
Training loss: 2.118861436843872
Validation loss: 2.110119573531612

Epoch: 5| Step: 10
Training loss: 1.8364721536636353
Validation loss: 2.0958239801468386

Epoch: 169| Step: 0
Training loss: 2.0578560829162598
Validation loss: 2.091519601883427

Epoch: 5| Step: 1
Training loss: 1.5263737440109253
Validation loss: 2.1057611998691352

Epoch: 5| Step: 2
Training loss: 2.4432296752929688
Validation loss: 2.1058320717145036

Epoch: 5| Step: 3
Training loss: 2.154364585876465
Validation loss: 2.0836095502299647

Epoch: 5| Step: 4
Training loss: 1.893355131149292
Validation loss: 2.08966052404014

Epoch: 5| Step: 5
Training loss: 1.9910653829574585
Validation loss: 2.081911080627031

Epoch: 5| Step: 6
Training loss: 2.292799472808838
Validation loss: 2.0975836015516713

Epoch: 5| Step: 7
Training loss: 1.925201416015625
Validation loss: 2.099881218325707

Epoch: 5| Step: 8
Training loss: 1.77968430519104
Validation loss: 2.093358773057179

Epoch: 5| Step: 9
Training loss: 2.2641215324401855
Validation loss: 2.102027808466265

Epoch: 5| Step: 10
Training loss: 2.1705875396728516
Validation loss: 2.1185195766469485

Epoch: 170| Step: 0
Training loss: 2.064859390258789
Validation loss: 2.103233532239032

Epoch: 5| Step: 1
Training loss: 0.8389075398445129
Validation loss: 2.07307876822769

Epoch: 5| Step: 2
Training loss: 2.0302276611328125
Validation loss: 2.1008056953389156

Epoch: 5| Step: 3
Training loss: 2.514037609100342
Validation loss: 2.0907035860964047

Epoch: 5| Step: 4
Training loss: 1.7799389362335205
Validation loss: 2.063532203756353

Epoch: 5| Step: 5
Training loss: 2.122269868850708
Validation loss: 2.1247561029208604

Epoch: 5| Step: 6
Training loss: 2.5739591121673584
Validation loss: 2.10179123058114

Epoch: 5| Step: 7
Training loss: 2.7519989013671875
Validation loss: 2.10960808620658

Epoch: 5| Step: 8
Training loss: 2.0982115268707275
Validation loss: 2.093819083706025

Epoch: 5| Step: 9
Training loss: 1.9565668106079102
Validation loss: 2.1007999220202045

Epoch: 5| Step: 10
Training loss: 1.6244471073150635
Validation loss: 2.1185145108930525

Epoch: 171| Step: 0
Training loss: 2.17915940284729
Validation loss: 2.07123843444291

Epoch: 5| Step: 1
Training loss: 2.0196900367736816
Validation loss: 2.115695979005547

Epoch: 5| Step: 2
Training loss: 1.2167927026748657
Validation loss: 2.1297482957122145

Epoch: 5| Step: 3
Training loss: 2.175933361053467
Validation loss: 2.0973536814412763

Epoch: 5| Step: 4
Training loss: 2.144221782684326
Validation loss: 2.1084469569626676

Epoch: 5| Step: 5
Training loss: 1.7148414850234985
Validation loss: 2.1150223914013115

Epoch: 5| Step: 6
Training loss: 2.142256736755371
Validation loss: 2.091882453169874

Epoch: 5| Step: 7
Training loss: 2.8011631965637207
Validation loss: 2.099031968783307

Epoch: 5| Step: 8
Training loss: 2.418720006942749
Validation loss: 2.0945154313118226

Epoch: 5| Step: 9
Training loss: 1.5598353147506714
Validation loss: 2.093944513669578

Epoch: 5| Step: 10
Training loss: 2.1449739933013916
Validation loss: 2.1101636835323867

Epoch: 172| Step: 0
Training loss: 2.0777761936187744
Validation loss: 2.095072110493978

Epoch: 5| Step: 1
Training loss: 1.6665832996368408
Validation loss: 2.115194125842023

Epoch: 5| Step: 2
Training loss: 1.5978243350982666
Validation loss: 2.090696947548979

Epoch: 5| Step: 3
Training loss: 1.7768785953521729
Validation loss: 2.1179975360952397

Epoch: 5| Step: 4
Training loss: 2.037139892578125
Validation loss: 2.1189197545410483

Epoch: 5| Step: 5
Training loss: 1.8320600986480713
Validation loss: 2.1213276309351765

Epoch: 5| Step: 6
Training loss: 2.854175567626953
Validation loss: 2.105602531022923

Epoch: 5| Step: 7
Training loss: 2.3052194118499756
Validation loss: 2.111519562300815

Epoch: 5| Step: 8
Training loss: 2.9386799335479736
Validation loss: 2.1045202388558337

Epoch: 5| Step: 9
Training loss: 1.4277441501617432
Validation loss: 2.068441785791869

Epoch: 5| Step: 10
Training loss: 1.742074728012085
Validation loss: 2.0866636255735993

Epoch: 173| Step: 0
Training loss: 2.186117172241211
Validation loss: 2.1038958205971667

Epoch: 5| Step: 1
Training loss: 2.135560989379883
Validation loss: 2.0977489153544107

Epoch: 5| Step: 2
Training loss: 1.5261688232421875
Validation loss: 2.077614494549331

Epoch: 5| Step: 3
Training loss: 1.8939831256866455
Validation loss: 2.0946692241135465

Epoch: 5| Step: 4
Training loss: 2.279712200164795
Validation loss: 2.0965201162522837

Epoch: 5| Step: 5
Training loss: 2.407090663909912
Validation loss: 2.068882626871909

Epoch: 5| Step: 6
Training loss: 1.890076994895935
Validation loss: 2.1009843708366476

Epoch: 5| Step: 7
Training loss: 2.5250957012176514
Validation loss: 2.1070246081198416

Epoch: 5| Step: 8
Training loss: 1.7405071258544922
Validation loss: 2.0964580146215295

Epoch: 5| Step: 9
Training loss: 1.8352493047714233
Validation loss: 2.109482975416286

Epoch: 5| Step: 10
Training loss: 1.978492259979248
Validation loss: 2.114730679860679

Epoch: 174| Step: 0
Training loss: 2.0219621658325195
Validation loss: 2.1008463034065823

Epoch: 5| Step: 1
Training loss: 2.0702919960021973
Validation loss: 2.0803118367348947

Epoch: 5| Step: 2
Training loss: 1.815254807472229
Validation loss: 2.064658716160764

Epoch: 5| Step: 3
Training loss: 1.9580949544906616
Validation loss: 2.1019709866533995

Epoch: 5| Step: 4
Training loss: 2.088909864425659
Validation loss: 2.0982305721570085

Epoch: 5| Step: 5
Training loss: 1.8293975591659546
Validation loss: 2.072985624754301

Epoch: 5| Step: 6
Training loss: 2.3028388023376465
Validation loss: 2.117796713306058

Epoch: 5| Step: 7
Training loss: 1.508092999458313
Validation loss: 2.086698810259501

Epoch: 5| Step: 8
Training loss: 1.7926305532455444
Validation loss: 2.093884521915067

Epoch: 5| Step: 9
Training loss: 2.2599122524261475
Validation loss: 2.0883794728145806

Epoch: 5| Step: 10
Training loss: 2.586012363433838
Validation loss: 2.0952397033732426

Epoch: 175| Step: 0
Training loss: 2.25893497467041
Validation loss: 2.08736527094277

Epoch: 5| Step: 1
Training loss: 2.6936399936676025
Validation loss: 2.07017272005799

Epoch: 5| Step: 2
Training loss: 1.9290943145751953
Validation loss: 2.0989239959306616

Epoch: 5| Step: 3
Training loss: 1.8313262462615967
Validation loss: 2.106559048416794

Epoch: 5| Step: 4
Training loss: 1.8094936609268188
Validation loss: 2.086462272110806

Epoch: 5| Step: 5
Training loss: 1.8015693426132202
Validation loss: 2.0899449420231644

Epoch: 5| Step: 6
Training loss: 1.9055664539337158
Validation loss: 2.081853319239873

Epoch: 5| Step: 7
Training loss: 2.248854160308838
Validation loss: 2.0729116034764115

Epoch: 5| Step: 8
Training loss: 1.6658521890640259
Validation loss: 2.0715120992352887

Epoch: 5| Step: 9
Training loss: 1.889593482017517
Validation loss: 2.068149141086045

Epoch: 5| Step: 10
Training loss: 2.327280044555664
Validation loss: 2.0549522317865843

Testing loss: 2.0841735336515637

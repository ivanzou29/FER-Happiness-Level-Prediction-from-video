Epoch: 1| Step: 0
Training loss: 4.175557429396871
Validation loss: 3.841980669065976

Epoch: 6| Step: 1
Training loss: 3.8877664505391767
Validation loss: 3.8245402672768574

Epoch: 6| Step: 2
Training loss: 3.0245479151341583
Validation loss: 3.8088811573581673

Epoch: 6| Step: 3
Training loss: 3.415519071927528
Validation loss: 3.793672946103429

Epoch: 6| Step: 4
Training loss: 3.568390911361739
Validation loss: 3.777197923445751

Epoch: 6| Step: 5
Training loss: 4.153134662740358
Validation loss: 3.7632314585661866

Epoch: 6| Step: 6
Training loss: 3.803586953410978
Validation loss: 3.7467840604163207

Epoch: 6| Step: 7
Training loss: 4.280246366379998
Validation loss: 3.729486860171743

Epoch: 6| Step: 8
Training loss: 4.830657196194277
Validation loss: 3.7127164625603966

Epoch: 6| Step: 9
Training loss: 2.8078024735056792
Validation loss: 3.6945934711760473

Epoch: 6| Step: 10
Training loss: 3.7405669779089834
Validation loss: 3.6770655097565474

Epoch: 6| Step: 11
Training loss: 4.234353603857959
Validation loss: 3.660824058295771

Epoch: 6| Step: 12
Training loss: 4.103127259183379
Validation loss: 3.640090532050353

Epoch: 6| Step: 13
Training loss: 3.657777516076764
Validation loss: 3.623033077628434

Epoch: 2| Step: 0
Training loss: 4.108282720061873
Validation loss: 3.6016829023005568

Epoch: 6| Step: 1
Training loss: 3.55117913126768
Validation loss: 3.5855263645399904

Epoch: 6| Step: 2
Training loss: 3.669556750313914
Validation loss: 3.564869499796283

Epoch: 6| Step: 3
Training loss: 3.469096948266884
Validation loss: 3.544664565206692

Epoch: 6| Step: 4
Training loss: 3.700594910916642
Validation loss: 3.5248315820523826

Epoch: 6| Step: 5
Training loss: 3.493264256320977
Validation loss: 3.5025783533140156

Epoch: 6| Step: 6
Training loss: 3.050198507884364
Validation loss: 3.481423580606553

Epoch: 6| Step: 7
Training loss: 3.3561060414616883
Validation loss: 3.45748097283715

Epoch: 6| Step: 8
Training loss: 3.688245536309696
Validation loss: 3.434946857542905

Epoch: 6| Step: 9
Training loss: 4.257930760972326
Validation loss: 3.4127498754139394

Epoch: 6| Step: 10
Training loss: 3.770451661635122
Validation loss: 3.3848609272548686

Epoch: 6| Step: 11
Training loss: 3.641192739856903
Validation loss: 3.362281169175723

Epoch: 6| Step: 12
Training loss: 3.08219617201726
Validation loss: 3.3381886565888323

Epoch: 6| Step: 13
Training loss: 3.3056473296289943
Validation loss: 3.3098222477754455

Epoch: 3| Step: 0
Training loss: 2.8513220280668508
Validation loss: 3.28366271430696

Epoch: 6| Step: 1
Training loss: 3.4820332548338127
Validation loss: 3.258511426372473

Epoch: 6| Step: 2
Training loss: 3.5001265639173593
Validation loss: 3.223993345367341

Epoch: 6| Step: 3
Training loss: 3.5628042258481156
Validation loss: 3.192328915394356

Epoch: 6| Step: 4
Training loss: 3.131661144073744
Validation loss: 3.166508888195612

Epoch: 6| Step: 5
Training loss: 2.714066252763489
Validation loss: 3.1293910647483334

Epoch: 6| Step: 6
Training loss: 2.3406365632406922
Validation loss: 3.100411714880458

Epoch: 6| Step: 7
Training loss: 3.329147539971082
Validation loss: 3.0684922430553607

Epoch: 6| Step: 8
Training loss: 3.0257441275378176
Validation loss: 3.031455291927587

Epoch: 6| Step: 9
Training loss: 3.8044834591399055
Validation loss: 2.9904431603030437

Epoch: 6| Step: 10
Training loss: 3.239404131644089
Validation loss: 2.954657451356479

Epoch: 6| Step: 11
Training loss: 2.7810385977306877
Validation loss: 2.9164966442869926

Epoch: 6| Step: 12
Training loss: 3.480898093169948
Validation loss: 2.8773624002151053

Epoch: 6| Step: 13
Training loss: 3.0809297902082062
Validation loss: 2.8320242484872145

Epoch: 4| Step: 0
Training loss: 3.1062116777907542
Validation loss: 2.7934233039927396

Epoch: 6| Step: 1
Training loss: 2.873986438716571
Validation loss: 2.7590822173244445

Epoch: 6| Step: 2
Training loss: 2.7730617214890376
Validation loss: 2.7108257464883083

Epoch: 6| Step: 3
Training loss: 2.36085726426807
Validation loss: 2.6732692835769982

Epoch: 6| Step: 4
Training loss: 2.646695264458395
Validation loss: 2.642431856117052

Epoch: 6| Step: 5
Training loss: 3.2711543676837995
Validation loss: 2.6003108254508165

Epoch: 6| Step: 6
Training loss: 2.3683809098528203
Validation loss: 2.5641491398188054

Epoch: 6| Step: 7
Training loss: 2.62901054277479
Validation loss: 2.538963198553979

Epoch: 6| Step: 8
Training loss: 2.545132561699354
Validation loss: 2.5105197827278265

Epoch: 6| Step: 9
Training loss: 2.3499946310103748
Validation loss: 2.4854528460196934

Epoch: 6| Step: 10
Training loss: 2.740012412077921
Validation loss: 2.464574558331649

Epoch: 6| Step: 11
Training loss: 2.873893856713954
Validation loss: 2.4648844155867953

Epoch: 6| Step: 12
Training loss: 2.0222244455979714
Validation loss: 2.453024195970982

Epoch: 6| Step: 13
Training loss: 2.218189329846678
Validation loss: 2.4543446548071617

Epoch: 5| Step: 0
Training loss: 2.5062733142557962
Validation loss: 2.4500529173241956

Epoch: 6| Step: 1
Training loss: 2.528632800732914
Validation loss: 2.4683189498650573

Epoch: 6| Step: 2
Training loss: 2.3280764581112905
Validation loss: 2.462387949270919

Epoch: 6| Step: 3
Training loss: 1.8862763512087173
Validation loss: 2.479310669977558

Epoch: 6| Step: 4
Training loss: 2.2567265945020374
Validation loss: 2.4873632856719694

Epoch: 6| Step: 5
Training loss: 2.886663743502904
Validation loss: 2.497410195753203

Epoch: 6| Step: 6
Training loss: 2.7559170787888214
Validation loss: 2.5098342114290864

Epoch: 6| Step: 7
Training loss: 2.0638051382978837
Validation loss: 2.5112099218643635

Epoch: 6| Step: 8
Training loss: 2.883337453791443
Validation loss: 2.513435390189642

Epoch: 6| Step: 9
Training loss: 2.321670083698169
Validation loss: 2.513135037108437

Epoch: 6| Step: 10
Training loss: 2.5921450325750612
Validation loss: 2.5118095575334216

Epoch: 6| Step: 11
Training loss: 2.5007220179304044
Validation loss: 2.5045934280685387

Epoch: 6| Step: 12
Training loss: 3.13545553155851
Validation loss: 2.4966531444110367

Epoch: 6| Step: 13
Training loss: 2.5080327682970585
Validation loss: 2.485177876037501

Epoch: 6| Step: 0
Training loss: 2.233973540406385
Validation loss: 2.4765065344294195

Epoch: 6| Step: 1
Training loss: 2.6280123365568455
Validation loss: 2.4758705672763375

Epoch: 6| Step: 2
Training loss: 2.496497752846628
Validation loss: 2.465890354886503

Epoch: 6| Step: 3
Training loss: 2.805367475023883
Validation loss: 2.4624927430279944

Epoch: 6| Step: 4
Training loss: 2.6322845020103594
Validation loss: 2.450125137688491

Epoch: 6| Step: 5
Training loss: 2.0224866363498837
Validation loss: 2.449983954863655

Epoch: 6| Step: 6
Training loss: 3.1173891083573886
Validation loss: 2.4540400489134115

Epoch: 6| Step: 7
Training loss: 2.7663917125286535
Validation loss: 2.4375784119603536

Epoch: 6| Step: 8
Training loss: 1.3699355926131438
Validation loss: 2.4393270443173836

Epoch: 6| Step: 9
Training loss: 1.8932858493160665
Validation loss: 2.4477767633811283

Epoch: 6| Step: 10
Training loss: 2.756227897157515
Validation loss: 2.448097292681857

Epoch: 6| Step: 11
Training loss: 2.6228015412831516
Validation loss: 2.447543766486346

Epoch: 6| Step: 12
Training loss: 2.4471233841095272
Validation loss: 2.448349087986679

Epoch: 6| Step: 13
Training loss: 2.5902352475881223
Validation loss: 2.444907661701818

Epoch: 7| Step: 0
Training loss: 3.0315114331326325
Validation loss: 2.447931968695247

Epoch: 6| Step: 1
Training loss: 2.6657666435934906
Validation loss: 2.4414508296711164

Epoch: 6| Step: 2
Training loss: 2.1135019127228176
Validation loss: 2.4444303048572618

Epoch: 6| Step: 3
Training loss: 2.519329778615088
Validation loss: 2.4406509248249653

Epoch: 6| Step: 4
Training loss: 2.377399637729338
Validation loss: 2.4448402931980633

Epoch: 6| Step: 5
Training loss: 2.0924921385185566
Validation loss: 2.444240802412691

Epoch: 6| Step: 6
Training loss: 2.5160849962175202
Validation loss: 2.4479823786796318

Epoch: 6| Step: 7
Training loss: 2.391403570114474
Validation loss: 2.4493279359239235

Epoch: 6| Step: 8
Training loss: 2.18494702272766
Validation loss: 2.4440744696820187

Epoch: 6| Step: 9
Training loss: 2.4083641969843685
Validation loss: 2.4342460105194417

Epoch: 6| Step: 10
Training loss: 2.5826047874971736
Validation loss: 2.4424216801071092

Epoch: 6| Step: 11
Training loss: 2.525619649864898
Validation loss: 2.4435587516285175

Epoch: 6| Step: 12
Training loss: 2.8494289830025252
Validation loss: 2.4396615836485833

Epoch: 6| Step: 13
Training loss: 2.565605607697104
Validation loss: 2.439743330150438

Epoch: 8| Step: 0
Training loss: 2.5089953713767765
Validation loss: 2.4450135622122957

Epoch: 6| Step: 1
Training loss: 3.032156110795883
Validation loss: 2.4366576377590046

Epoch: 6| Step: 2
Training loss: 2.807651494355616
Validation loss: 2.4405893815714452

Epoch: 6| Step: 3
Training loss: 2.3984144377453926
Validation loss: 2.440956485264061

Epoch: 6| Step: 4
Training loss: 2.5219771467318446
Validation loss: 2.4366927970375762

Epoch: 6| Step: 5
Training loss: 2.32690930468539
Validation loss: 2.432938987205762

Epoch: 6| Step: 6
Training loss: 2.1756032874369904
Validation loss: 2.4370124524925014

Epoch: 6| Step: 7
Training loss: 2.631087918397707
Validation loss: 2.444336156963197

Epoch: 6| Step: 8
Training loss: 2.7638539823780457
Validation loss: 2.4410277213064844

Epoch: 6| Step: 9
Training loss: 2.265422943576155
Validation loss: 2.441714433673674

Epoch: 6| Step: 10
Training loss: 2.7713372673365857
Validation loss: 2.4405754771399932

Epoch: 6| Step: 11
Training loss: 1.9563686444559099
Validation loss: 2.4591297756087873

Epoch: 6| Step: 12
Training loss: 2.271234867631283
Validation loss: 2.445587613978252

Epoch: 6| Step: 13
Training loss: 2.039322292103869
Validation loss: 2.447411973478832

Epoch: 9| Step: 0
Training loss: 2.220045436875284
Validation loss: 2.4406444286659084

Epoch: 6| Step: 1
Training loss: 2.217929312209918
Validation loss: 2.456284725471134

Epoch: 6| Step: 2
Training loss: 2.247968710460995
Validation loss: 2.4496674681782586

Epoch: 6| Step: 3
Training loss: 2.71399913795137
Validation loss: 2.4590155145290935

Epoch: 6| Step: 4
Training loss: 3.0557264819647334
Validation loss: 2.447101024258675

Epoch: 6| Step: 5
Training loss: 2.076451940298613
Validation loss: 2.4442251304538796

Epoch: 6| Step: 6
Training loss: 2.909543048336297
Validation loss: 2.4454105995333917

Epoch: 6| Step: 7
Training loss: 2.6839653546332944
Validation loss: 2.4541902112452085

Epoch: 6| Step: 8
Training loss: 2.5027157338202684
Validation loss: 2.4449516574524455

Epoch: 6| Step: 9
Training loss: 2.670186183881528
Validation loss: 2.4466191823502306

Epoch: 6| Step: 10
Training loss: 2.345096252353044
Validation loss: 2.4347973210335025

Epoch: 6| Step: 11
Training loss: 2.014861206121067
Validation loss: 2.4292057700524095

Epoch: 6| Step: 12
Training loss: 2.719795902813335
Validation loss: 2.4345334414043864

Epoch: 6| Step: 13
Training loss: 2.1247615680444754
Validation loss: 2.4419189647176047

Epoch: 10| Step: 0
Training loss: 2.1237928664364536
Validation loss: 2.4214807579168176

Epoch: 6| Step: 1
Training loss: 2.2027455537461287
Validation loss: 2.4261099264572317

Epoch: 6| Step: 2
Training loss: 2.210049939691725
Validation loss: 2.4451981295325904

Epoch: 6| Step: 3
Training loss: 2.8043555273098786
Validation loss: 2.444534129125522

Epoch: 6| Step: 4
Training loss: 2.6606554187615603
Validation loss: 2.435486671174772

Epoch: 6| Step: 5
Training loss: 2.1608868927453204
Validation loss: 2.4326611033400676

Epoch: 6| Step: 6
Training loss: 2.6806050945529285
Validation loss: 2.431129330318455

Epoch: 6| Step: 7
Training loss: 1.8841514416832696
Validation loss: 2.446108005140596

Epoch: 6| Step: 8
Training loss: 1.9709788225062974
Validation loss: 2.44229554116122

Epoch: 6| Step: 9
Training loss: 2.4757223057854794
Validation loss: 2.435861444336071

Epoch: 6| Step: 10
Training loss: 2.9111185620562816
Validation loss: 2.4499506082551568

Epoch: 6| Step: 11
Training loss: 2.353256477239244
Validation loss: 2.4286956398051007

Epoch: 6| Step: 12
Training loss: 2.5949742748477456
Validation loss: 2.4420195518729444

Epoch: 6| Step: 13
Training loss: 3.2591779669423757
Validation loss: 2.43748941989787

Epoch: 11| Step: 0
Training loss: 2.617029441501713
Validation loss: 2.444983422639864

Epoch: 6| Step: 1
Training loss: 2.6998149914404492
Validation loss: 2.4442053940474415

Epoch: 6| Step: 2
Training loss: 2.2684977241422297
Validation loss: 2.4269887444615397

Epoch: 6| Step: 3
Training loss: 2.7441615636712275
Validation loss: 2.4381155883221473

Epoch: 6| Step: 4
Training loss: 2.100359604336671
Validation loss: 2.4381623631593508

Epoch: 6| Step: 5
Training loss: 3.0494666399265467
Validation loss: 2.433267831791704

Epoch: 6| Step: 6
Training loss: 2.228964811318572
Validation loss: 2.4335166792404106

Epoch: 6| Step: 7
Training loss: 1.976884416141089
Validation loss: 2.4241629673298077

Epoch: 6| Step: 8
Training loss: 2.2540323364321195
Validation loss: 2.420182257953447

Epoch: 6| Step: 9
Training loss: 2.7709039855974624
Validation loss: 2.428021349304538

Epoch: 6| Step: 10
Training loss: 2.446623429462925
Validation loss: 2.4299914138286423

Epoch: 6| Step: 11
Training loss: 2.3681046625926743
Validation loss: 2.4329045249930856

Epoch: 6| Step: 12
Training loss: 2.5764470043865355
Validation loss: 2.43779677027173

Epoch: 6| Step: 13
Training loss: 2.1208833200176067
Validation loss: 2.435040537296344

Epoch: 12| Step: 0
Training loss: 2.6073712176141837
Validation loss: 2.427372458158958

Epoch: 6| Step: 1
Training loss: 2.652634617358374
Validation loss: 2.428007994802553

Epoch: 6| Step: 2
Training loss: 1.8838832232493425
Validation loss: 2.436145210341609

Epoch: 6| Step: 3
Training loss: 2.7217397802950645
Validation loss: 2.4293822069112716

Epoch: 6| Step: 4
Training loss: 2.270469484723543
Validation loss: 2.4277009273730243

Epoch: 6| Step: 5
Training loss: 2.4841379496405263
Validation loss: 2.4250830619718093

Epoch: 6| Step: 6
Training loss: 2.781383725498299
Validation loss: 2.4229412643963997

Epoch: 6| Step: 7
Training loss: 2.096163596895808
Validation loss: 2.4227611766286175

Epoch: 6| Step: 8
Training loss: 2.393506576073233
Validation loss: 2.4324075697104575

Epoch: 6| Step: 9
Training loss: 2.484096583474373
Validation loss: 2.420606941584565

Epoch: 6| Step: 10
Training loss: 3.0291183684712557
Validation loss: 2.434657126300764

Epoch: 6| Step: 11
Training loss: 2.536730543461046
Validation loss: 2.4346228842651265

Epoch: 6| Step: 12
Training loss: 1.9112943601481256
Validation loss: 2.431001853537314

Epoch: 6| Step: 13
Training loss: 2.5097810143618644
Validation loss: 2.4207751822477133

Epoch: 13| Step: 0
Training loss: 2.883595760802604
Validation loss: 2.418698432366082

Epoch: 6| Step: 1
Training loss: 2.6076299805763443
Validation loss: 2.4182039391963195

Epoch: 6| Step: 2
Training loss: 2.352035525579158
Validation loss: 2.4212602552972364

Epoch: 6| Step: 3
Training loss: 2.2147018683198763
Validation loss: 2.42357746296442

Epoch: 6| Step: 4
Training loss: 2.5428256743184625
Validation loss: 2.426805182851087

Epoch: 6| Step: 5
Training loss: 1.8683657579673858
Validation loss: 2.425857272469556

Epoch: 6| Step: 6
Training loss: 2.441370995839213
Validation loss: 2.4381631617463415

Epoch: 6| Step: 7
Training loss: 2.540873377671997
Validation loss: 2.426961958528508

Epoch: 6| Step: 8
Training loss: 3.277576097293145
Validation loss: 2.428476651329816

Epoch: 6| Step: 9
Training loss: 2.628723591641472
Validation loss: 2.432377764030951

Epoch: 6| Step: 10
Training loss: 2.097038988048391
Validation loss: 2.431174351774612

Epoch: 6| Step: 11
Training loss: 2.2182119012503914
Validation loss: 2.4366885896923725

Epoch: 6| Step: 12
Training loss: 1.7241473675338066
Validation loss: 2.4469959529572507

Epoch: 6| Step: 13
Training loss: 2.6008518840792725
Validation loss: 2.4381315032760122

Epoch: 14| Step: 0
Training loss: 1.7006762337669123
Validation loss: 2.4345009522670247

Epoch: 6| Step: 1
Training loss: 2.71816703421799
Validation loss: 2.4415635366001043

Epoch: 6| Step: 2
Training loss: 2.6733528199599457
Validation loss: 2.4508454894545117

Epoch: 6| Step: 3
Training loss: 2.52106508392974
Validation loss: 2.430778225886665

Epoch: 6| Step: 4
Training loss: 2.2873026361424498
Validation loss: 2.459243336586215

Epoch: 6| Step: 5
Training loss: 2.7523970127501407
Validation loss: 2.4475800032458634

Epoch: 6| Step: 6
Training loss: 2.4951817812089074
Validation loss: 2.4423424230482516

Epoch: 6| Step: 7
Training loss: 2.7676639260914766
Validation loss: 2.4350558032900844

Epoch: 6| Step: 8
Training loss: 2.733081882432417
Validation loss: 2.430700027773963

Epoch: 6| Step: 9
Training loss: 2.6274324681743764
Validation loss: 2.4227113653722707

Epoch: 6| Step: 10
Training loss: 2.294971533704751
Validation loss: 2.422842993125947

Epoch: 6| Step: 11
Training loss: 1.8146808920678068
Validation loss: 2.41523997901391

Epoch: 6| Step: 12
Training loss: 2.4477987599215516
Validation loss: 2.4176889153889176

Epoch: 6| Step: 13
Training loss: 2.388278083437292
Validation loss: 2.428454414321534

Epoch: 15| Step: 0
Training loss: 2.4443402292211625
Validation loss: 2.4216870142674116

Epoch: 6| Step: 1
Training loss: 2.4655217176883815
Validation loss: 2.4256333916772967

Epoch: 6| Step: 2
Training loss: 2.3342934631243053
Validation loss: 2.4222118676641875

Epoch: 6| Step: 3
Training loss: 2.49243411574912
Validation loss: 2.4291437323541616

Epoch: 6| Step: 4
Training loss: 2.1612473227824656
Validation loss: 2.4239448152456697

Epoch: 6| Step: 5
Training loss: 1.917245017434825
Validation loss: 2.4241645573352995

Epoch: 6| Step: 6
Training loss: 2.4995725266246893
Validation loss: 2.418194120932173

Epoch: 6| Step: 7
Training loss: 2.554583532801778
Validation loss: 2.4198399022637616

Epoch: 6| Step: 8
Training loss: 2.669336909274902
Validation loss: 2.416230496694413

Epoch: 6| Step: 9
Training loss: 2.293265068494578
Validation loss: 2.4144376856202197

Epoch: 6| Step: 10
Training loss: 2.854573204621313
Validation loss: 2.424655868259047

Epoch: 6| Step: 11
Training loss: 2.3649398154087073
Validation loss: 2.431829230208621

Epoch: 6| Step: 12
Training loss: 2.832436831141126
Validation loss: 2.4222505257811617

Epoch: 6| Step: 13
Training loss: 2.1979369634140107
Validation loss: 2.4191174631330705

Epoch: 16| Step: 0
Training loss: 1.985159951498941
Validation loss: 2.436246044037636

Epoch: 6| Step: 1
Training loss: 2.7748395787513873
Validation loss: 2.4232971937833363

Epoch: 6| Step: 2
Training loss: 2.542825580557185
Validation loss: 2.4293339053951364

Epoch: 6| Step: 3
Training loss: 2.520809068379121
Validation loss: 2.433248529088661

Epoch: 6| Step: 4
Training loss: 2.7572369285562117
Validation loss: 2.426393621020721

Epoch: 6| Step: 5
Training loss: 2.4362298639521285
Validation loss: 2.4157858317577046

Epoch: 6| Step: 6
Training loss: 2.971732799445271
Validation loss: 2.4260028567935468

Epoch: 6| Step: 7
Training loss: 1.9870674310347713
Validation loss: 2.4247032714808046

Epoch: 6| Step: 8
Training loss: 2.356331586563615
Validation loss: 2.418352046366597

Epoch: 6| Step: 9
Training loss: 3.038388998917063
Validation loss: 2.431122007817278

Epoch: 6| Step: 10
Training loss: 1.8285695375066584
Validation loss: 2.422879115497302

Epoch: 6| Step: 11
Training loss: 1.93047881793396
Validation loss: 2.423125979653155

Epoch: 6| Step: 12
Training loss: 2.1848412841486144
Validation loss: 2.4239078480952485

Epoch: 6| Step: 13
Training loss: 2.538355802343472
Validation loss: 2.4286758426451382

Epoch: 17| Step: 0
Training loss: 2.7317567333418786
Validation loss: 2.4278901905112984

Epoch: 6| Step: 1
Training loss: 2.130673855853375
Validation loss: 2.4150236617483065

Epoch: 6| Step: 2
Training loss: 2.4512826598159196
Validation loss: 2.4250118327425634

Epoch: 6| Step: 3
Training loss: 2.9517240884954834
Validation loss: 2.4386279601860745

Epoch: 6| Step: 4
Training loss: 1.9032090743212142
Validation loss: 2.448449686876587

Epoch: 6| Step: 5
Training loss: 2.2789242528937304
Validation loss: 2.446676196990131

Epoch: 6| Step: 6
Training loss: 3.158445293552396
Validation loss: 2.4499700956400963

Epoch: 6| Step: 7
Training loss: 2.0456464735210007
Validation loss: 2.4359001549938535

Epoch: 6| Step: 8
Training loss: 2.6473571914613503
Validation loss: 2.436457321299429

Epoch: 6| Step: 9
Training loss: 2.176238362973042
Validation loss: 2.4408944694709045

Epoch: 6| Step: 10
Training loss: 2.3890234874785903
Validation loss: 2.4338612417102863

Epoch: 6| Step: 11
Training loss: 2.287286583789149
Validation loss: 2.4215065952788573

Epoch: 6| Step: 12
Training loss: 2.1632611825879455
Validation loss: 2.422810076607343

Epoch: 6| Step: 13
Training loss: 2.5863411118769433
Validation loss: 2.4247402183103888

Epoch: 18| Step: 0
Training loss: 2.470973691336346
Validation loss: 2.4255128508252084

Epoch: 6| Step: 1
Training loss: 1.878289896855714
Validation loss: 2.4214652832643933

Epoch: 6| Step: 2
Training loss: 2.566567890528972
Validation loss: 2.4263210382008094

Epoch: 6| Step: 3
Training loss: 2.1776599272588597
Validation loss: 2.4202176401566247

Epoch: 6| Step: 4
Training loss: 2.007223673765801
Validation loss: 2.426561313095834

Epoch: 6| Step: 5
Training loss: 2.1471279522773967
Validation loss: 2.420812935888224

Epoch: 6| Step: 6
Training loss: 2.798821310180119
Validation loss: 2.4245323041480225

Epoch: 6| Step: 7
Training loss: 2.121160404289095
Validation loss: 2.426109828185269

Epoch: 6| Step: 8
Training loss: 2.891175913506786
Validation loss: 2.4233283245615596

Epoch: 6| Step: 9
Training loss: 2.5029397845948305
Validation loss: 2.42611841877763

Epoch: 6| Step: 10
Training loss: 2.2433709932239307
Validation loss: 2.4084906776358146

Epoch: 6| Step: 11
Training loss: 2.40604330388877
Validation loss: 2.4151501391583747

Epoch: 6| Step: 12
Training loss: 3.0032018423421305
Validation loss: 2.409896993269242

Epoch: 6| Step: 13
Training loss: 2.5897754412772307
Validation loss: 2.416501110807806

Epoch: 19| Step: 0
Training loss: 2.1576155609265615
Validation loss: 2.4207449624964092

Epoch: 6| Step: 1
Training loss: 2.864062027168963
Validation loss: 2.423199937245647

Epoch: 6| Step: 2
Training loss: 2.5654378657796935
Validation loss: 2.4218755414408415

Epoch: 6| Step: 3
Training loss: 2.5452606139681606
Validation loss: 2.420220217865365

Epoch: 6| Step: 4
Training loss: 2.4666844672342623
Validation loss: 2.403141656296198

Epoch: 6| Step: 5
Training loss: 2.560282142856817
Validation loss: 2.4082129922475435

Epoch: 6| Step: 6
Training loss: 2.7336142217322537
Validation loss: 2.4191867305945487

Epoch: 6| Step: 7
Training loss: 2.2649154078032128
Validation loss: 2.413089576717418

Epoch: 6| Step: 8
Training loss: 2.408018378900493
Validation loss: 2.4034999887201773

Epoch: 6| Step: 9
Training loss: 1.6856045143995768
Validation loss: 2.4224493022609446

Epoch: 6| Step: 10
Training loss: 3.079621082133561
Validation loss: 2.4091698077179275

Epoch: 6| Step: 11
Training loss: 1.8379022352640386
Validation loss: 2.412403841358842

Epoch: 6| Step: 12
Training loss: 2.162499920619015
Validation loss: 2.4122377513335076

Epoch: 6| Step: 13
Training loss: 2.3859702277296555
Validation loss: 2.4189466431005373

Epoch: 20| Step: 0
Training loss: 1.855406107597994
Validation loss: 2.404465414049034

Epoch: 6| Step: 1
Training loss: 2.167095716495214
Validation loss: 2.413509515431014

Epoch: 6| Step: 2
Training loss: 2.2598742934518703
Validation loss: 2.4197804078918392

Epoch: 6| Step: 3
Training loss: 3.103187116325837
Validation loss: 2.4236424713389075

Epoch: 6| Step: 4
Training loss: 2.4089352375856508
Validation loss: 2.4254033464403384

Epoch: 6| Step: 5
Training loss: 2.3954158474499487
Validation loss: 2.4268337388828716

Epoch: 6| Step: 6
Training loss: 2.3238569218423453
Validation loss: 2.423015580217873

Epoch: 6| Step: 7
Training loss: 2.726328317810711
Validation loss: 2.4054451128847387

Epoch: 6| Step: 8
Training loss: 2.6241726252598334
Validation loss: 2.4249895475758763

Epoch: 6| Step: 9
Training loss: 1.8892287465942306
Validation loss: 2.412946638622588

Epoch: 6| Step: 10
Training loss: 2.5563775401487745
Validation loss: 2.408835322675801

Epoch: 6| Step: 11
Training loss: 2.870007035515964
Validation loss: 2.4079979331904315

Epoch: 6| Step: 12
Training loss: 2.145811938824215
Validation loss: 2.4085233691476033

Epoch: 6| Step: 13
Training loss: 2.476067622948349
Validation loss: 2.4152706049127177

Epoch: 21| Step: 0
Training loss: 2.883362260206647
Validation loss: 2.4120472188295237

Epoch: 6| Step: 1
Training loss: 2.8314472727648075
Validation loss: 2.4087135287633235

Epoch: 6| Step: 2
Training loss: 2.1842256290298265
Validation loss: 2.4166678450570576

Epoch: 6| Step: 3
Training loss: 1.9633854028935354
Validation loss: 2.414400589363198

Epoch: 6| Step: 4
Training loss: 2.4806497342622675
Validation loss: 2.4244803658443863

Epoch: 6| Step: 5
Training loss: 2.5508041497644074
Validation loss: 2.4189342569882184

Epoch: 6| Step: 6
Training loss: 2.4578397115182002
Validation loss: 2.431796631411013

Epoch: 6| Step: 7
Training loss: 2.2289567890180306
Validation loss: 2.421098596962542

Epoch: 6| Step: 8
Training loss: 2.053401878418346
Validation loss: 2.420858543142172

Epoch: 6| Step: 9
Training loss: 2.6102287129422983
Validation loss: 2.423638651222879

Epoch: 6| Step: 10
Training loss: 2.475634861449507
Validation loss: 2.4299799179953228

Epoch: 6| Step: 11
Training loss: 2.4660624107637137
Validation loss: 2.4252984492510934

Epoch: 6| Step: 12
Training loss: 2.720009347675193
Validation loss: 2.4276292511656745

Epoch: 6| Step: 13
Training loss: 1.7279455936838537
Validation loss: 2.414059718191114

Epoch: 22| Step: 0
Training loss: 1.8158825847545843
Validation loss: 2.4228680287336806

Epoch: 6| Step: 1
Training loss: 3.150626583450849
Validation loss: 2.4222237940830804

Epoch: 6| Step: 2
Training loss: 2.2877742540392982
Validation loss: 2.42206059226724

Epoch: 6| Step: 3
Training loss: 2.514768656106918
Validation loss: 2.4254685188556624

Epoch: 6| Step: 4
Training loss: 2.7756755556425627
Validation loss: 2.413286316828144

Epoch: 6| Step: 5
Training loss: 1.926750078683636
Validation loss: 2.4160274422062833

Epoch: 6| Step: 6
Training loss: 2.609772703277942
Validation loss: 2.4258376405215336

Epoch: 6| Step: 7
Training loss: 2.2386608500917236
Validation loss: 2.4210522802706915

Epoch: 6| Step: 8
Training loss: 2.930326753174651
Validation loss: 2.4124560726033972

Epoch: 6| Step: 9
Training loss: 2.2114280992101176
Validation loss: 2.4182018851694607

Epoch: 6| Step: 10
Training loss: 2.5846735236419933
Validation loss: 2.411765856870288

Epoch: 6| Step: 11
Training loss: 2.3884078571576857
Validation loss: 2.405831915794759

Epoch: 6| Step: 12
Training loss: 2.0993638573748608
Validation loss: 2.410140636688226

Epoch: 6| Step: 13
Training loss: 2.0406695934990413
Validation loss: 2.4137685652364103

Epoch: 23| Step: 0
Training loss: 2.9370797039227288
Validation loss: 2.4133660341606262

Epoch: 6| Step: 1
Training loss: 2.3126088709591732
Validation loss: 2.4072344694078645

Epoch: 6| Step: 2
Training loss: 2.18629575006459
Validation loss: 2.3962629154250603

Epoch: 6| Step: 3
Training loss: 2.4190854650682945
Validation loss: 2.4117319900648497

Epoch: 6| Step: 4
Training loss: 2.763749084472505
Validation loss: 2.4125915374915032

Epoch: 6| Step: 5
Training loss: 2.6988383796641733
Validation loss: 2.4102097005084873

Epoch: 6| Step: 6
Training loss: 2.430482397038088
Validation loss: 2.405857632175943

Epoch: 6| Step: 7
Training loss: 2.2177583869480504
Validation loss: 2.4049636490573514

Epoch: 6| Step: 8
Training loss: 2.4936356597157014
Validation loss: 2.407514207018338

Epoch: 6| Step: 9
Training loss: 2.4079104552744726
Validation loss: 2.40304608093498

Epoch: 6| Step: 10
Training loss: 2.260813055719689
Validation loss: 2.4193087856462587

Epoch: 6| Step: 11
Training loss: 2.8538068744140057
Validation loss: 2.4102239779575907

Epoch: 6| Step: 12
Training loss: 2.1284134320645904
Validation loss: 2.41435535387036

Epoch: 6| Step: 13
Training loss: 1.6376474153979006
Validation loss: 2.415508113137056

Epoch: 24| Step: 0
Training loss: 2.9005842869898846
Validation loss: 2.412015884790996

Epoch: 6| Step: 1
Training loss: 2.3994134663079234
Validation loss: 2.4195226094140176

Epoch: 6| Step: 2
Training loss: 2.373970812610892
Validation loss: 2.4122329412566077

Epoch: 6| Step: 3
Training loss: 1.6456723999661214
Validation loss: 2.4085987072453445

Epoch: 6| Step: 4
Training loss: 2.219059774775592
Validation loss: 2.409230487743021

Epoch: 6| Step: 5
Training loss: 2.469338262463807
Validation loss: 2.4285457991067765

Epoch: 6| Step: 6
Training loss: 2.5285290823062248
Validation loss: 2.4127182000397687

Epoch: 6| Step: 7
Training loss: 2.1759938216682686
Validation loss: 2.425705307054835

Epoch: 6| Step: 8
Training loss: 2.4685212041366418
Validation loss: 2.4080497154508937

Epoch: 6| Step: 9
Training loss: 2.2432276210657065
Validation loss: 2.4330009526293868

Epoch: 6| Step: 10
Training loss: 2.4589855384975405
Validation loss: 2.41349688737994

Epoch: 6| Step: 11
Training loss: 2.8346993762282566
Validation loss: 2.409561685667952

Epoch: 6| Step: 12
Training loss: 2.1896151534604598
Validation loss: 2.40395156277116

Epoch: 6| Step: 13
Training loss: 2.632777907147091
Validation loss: 2.411759727763416

Epoch: 25| Step: 0
Training loss: 1.9638233606730136
Validation loss: 2.408342962234766

Epoch: 6| Step: 1
Training loss: 1.9456633592274346
Validation loss: 2.4193737773256556

Epoch: 6| Step: 2
Training loss: 1.7310137095137097
Validation loss: 2.4042057089277233

Epoch: 6| Step: 3
Training loss: 2.8720973745431095
Validation loss: 2.411814040663888

Epoch: 6| Step: 4
Training loss: 2.3192047660276005
Validation loss: 2.4134600894879594

Epoch: 6| Step: 5
Training loss: 2.5089418714410305
Validation loss: 2.404436261836333

Epoch: 6| Step: 6
Training loss: 2.7339614119412223
Validation loss: 2.412182681968461

Epoch: 6| Step: 7
Training loss: 2.768808978208211
Validation loss: 2.41806496802094

Epoch: 6| Step: 8
Training loss: 3.0952486974234183
Validation loss: 2.4194839159226005

Epoch: 6| Step: 9
Training loss: 2.0650573827792638
Validation loss: 2.3934083663270362

Epoch: 6| Step: 10
Training loss: 2.5456105469298764
Validation loss: 2.4019929045578476

Epoch: 6| Step: 11
Training loss: 2.0727408508680427
Validation loss: 2.391104074649589

Epoch: 6| Step: 12
Training loss: 2.3648261955222156
Validation loss: 2.405776452040756

Epoch: 6| Step: 13
Training loss: 2.419004449595049
Validation loss: 2.404777621339718

Epoch: 26| Step: 0
Training loss: 3.0248417710465723
Validation loss: 2.4156566076608357

Epoch: 6| Step: 1
Training loss: 1.5893145906081407
Validation loss: 2.406397852132795

Epoch: 6| Step: 2
Training loss: 2.54583138456962
Validation loss: 2.41004492669024

Epoch: 6| Step: 3
Training loss: 2.762575008703101
Validation loss: 2.4139847892096595

Epoch: 6| Step: 4
Training loss: 2.9697549825485723
Validation loss: 2.405981701181725

Epoch: 6| Step: 5
Training loss: 2.253052230696849
Validation loss: 2.4039625053613096

Epoch: 6| Step: 6
Training loss: 2.367980622727783
Validation loss: 2.4036427694502978

Epoch: 6| Step: 7
Training loss: 1.8273322840180435
Validation loss: 2.4063095812005986

Epoch: 6| Step: 8
Training loss: 2.172093991003984
Validation loss: 2.403713185423043

Epoch: 6| Step: 9
Training loss: 2.3403687564762703
Validation loss: 2.4001144693150005

Epoch: 6| Step: 10
Training loss: 2.486279986830244
Validation loss: 2.3926313038839218

Epoch: 6| Step: 11
Training loss: 2.3296614319886295
Validation loss: 2.4036581935268577

Epoch: 6| Step: 12
Training loss: 1.7907397807616672
Validation loss: 2.401476116690955

Epoch: 6| Step: 13
Training loss: 2.8118692326534265
Validation loss: 2.413526926227504

Epoch: 27| Step: 0
Training loss: 2.5809486416447935
Validation loss: 2.411278181713918

Epoch: 6| Step: 1
Training loss: 2.050400820369013
Validation loss: 2.4152431872245814

Epoch: 6| Step: 2
Training loss: 2.1086401895763722
Validation loss: 2.410669841894904

Epoch: 6| Step: 3
Training loss: 2.337761310516172
Validation loss: 2.4086586427809227

Epoch: 6| Step: 4
Training loss: 2.969578998666526
Validation loss: 2.4024062068439407

Epoch: 6| Step: 5
Training loss: 2.75392962506702
Validation loss: 2.4197873213344865

Epoch: 6| Step: 6
Training loss: 2.7345894865654294
Validation loss: 2.4126998858090816

Epoch: 6| Step: 7
Training loss: 2.1774176260327183
Validation loss: 2.4051053102902427

Epoch: 6| Step: 8
Training loss: 2.577332715915396
Validation loss: 2.3941549614304716

Epoch: 6| Step: 9
Training loss: 2.017273576172014
Validation loss: 2.403420688145615

Epoch: 6| Step: 10
Training loss: 1.8235201753965118
Validation loss: 2.4160366360679233

Epoch: 6| Step: 11
Training loss: 2.7859330772583024
Validation loss: 2.4126072584696

Epoch: 6| Step: 12
Training loss: 2.5055888647650515
Validation loss: 2.418281843039069

Epoch: 6| Step: 13
Training loss: 1.96362885973842
Validation loss: 2.400468660001317

Epoch: 28| Step: 0
Training loss: 2.2006016342105705
Validation loss: 2.4070018072740917

Epoch: 6| Step: 1
Training loss: 2.2109541066762586
Validation loss: 2.4248324067580747

Epoch: 6| Step: 2
Training loss: 2.5186202893804492
Validation loss: 2.4264334653144104

Epoch: 6| Step: 3
Training loss: 2.942218805066545
Validation loss: 2.4531150623290325

Epoch: 6| Step: 4
Training loss: 1.8732096071326423
Validation loss: 2.4485984069365787

Epoch: 6| Step: 5
Training loss: 2.4354380175652888
Validation loss: 2.429917875532186

Epoch: 6| Step: 6
Training loss: 2.621871355347493
Validation loss: 2.4536354845854835

Epoch: 6| Step: 7
Training loss: 2.3048530131245624
Validation loss: 2.437244548616604

Epoch: 6| Step: 8
Training loss: 1.8753127155198945
Validation loss: 2.432702551913243

Epoch: 6| Step: 9
Training loss: 2.6060339273218958
Validation loss: 2.444114123288078

Epoch: 6| Step: 10
Training loss: 2.5876848937976633
Validation loss: 2.4392319826669624

Epoch: 6| Step: 11
Training loss: 2.2679844639227515
Validation loss: 2.4277064188060864

Epoch: 6| Step: 12
Training loss: 2.8804266756614902
Validation loss: 2.4333615669371826

Epoch: 6| Step: 13
Training loss: 2.096343754017877
Validation loss: 2.430032458252106

Epoch: 29| Step: 0
Training loss: 2.832625543786802
Validation loss: 2.405297507814709

Epoch: 6| Step: 1
Training loss: 1.958471746490709
Validation loss: 2.415575049485539

Epoch: 6| Step: 2
Training loss: 2.289993461366339
Validation loss: 2.411284773466929

Epoch: 6| Step: 3
Training loss: 2.1677800276811037
Validation loss: 2.4102170618135346

Epoch: 6| Step: 4
Training loss: 2.866282140831926
Validation loss: 2.412928013231171

Epoch: 6| Step: 5
Training loss: 2.76239566519646
Validation loss: 2.396690511089692

Epoch: 6| Step: 6
Training loss: 2.3336191569970377
Validation loss: 2.4037415694447475

Epoch: 6| Step: 7
Training loss: 2.3085691705676505
Validation loss: 2.4125748611193742

Epoch: 6| Step: 8
Training loss: 2.378697679017416
Validation loss: 2.4023699172334196

Epoch: 6| Step: 9
Training loss: 2.127785932106393
Validation loss: 2.40236672491237

Epoch: 6| Step: 10
Training loss: 1.7344747978572232
Validation loss: 2.3887267544405675

Epoch: 6| Step: 11
Training loss: 3.0619615353909766
Validation loss: 2.397981382856092

Epoch: 6| Step: 12
Training loss: 2.5153074834192775
Validation loss: 2.400309119251759

Epoch: 6| Step: 13
Training loss: 1.738733811347681
Validation loss: 2.4036785686959847

Epoch: 30| Step: 0
Training loss: 2.0440069007981
Validation loss: 2.3992968416646145

Epoch: 6| Step: 1
Training loss: 2.6345915174822183
Validation loss: 2.4061282403698745

Epoch: 6| Step: 2
Training loss: 2.490698103268635
Validation loss: 2.4061514929222323

Epoch: 6| Step: 3
Training loss: 2.073113040791824
Validation loss: 2.3881750746862127

Epoch: 6| Step: 4
Training loss: 2.828993690232879
Validation loss: 2.4052539349166837

Epoch: 6| Step: 5
Training loss: 1.7813995030889935
Validation loss: 2.409104573749243

Epoch: 6| Step: 6
Training loss: 2.400924734935418
Validation loss: 2.3985836386296118

Epoch: 6| Step: 7
Training loss: 2.007659550047389
Validation loss: 2.4042477224383547

Epoch: 6| Step: 8
Training loss: 2.8737511617650706
Validation loss: 2.4133546484836637

Epoch: 6| Step: 9
Training loss: 2.6068064216880025
Validation loss: 2.3995668235573397

Epoch: 6| Step: 10
Training loss: 2.2916902945485305
Validation loss: 2.4018427794260164

Epoch: 6| Step: 11
Training loss: 2.313011731412818
Validation loss: 2.4050461205695095

Epoch: 6| Step: 12
Training loss: 3.1026846059366098
Validation loss: 2.4217239989101103

Epoch: 6| Step: 13
Training loss: 1.5547726861820128
Validation loss: 2.408045557069568

Epoch: 31| Step: 0
Training loss: 2.1774111657575435
Validation loss: 2.400706987094112

Epoch: 6| Step: 1
Training loss: 2.656709160107927
Validation loss: 2.3992246315744383

Epoch: 6| Step: 2
Training loss: 2.76769002770307
Validation loss: 2.401583105067966

Epoch: 6| Step: 3
Training loss: 2.602117347545873
Validation loss: 2.3994430935627142

Epoch: 6| Step: 4
Training loss: 2.6416942530815173
Validation loss: 2.3981832396423104

Epoch: 6| Step: 5
Training loss: 2.2293603284142898
Validation loss: 2.3918980053480703

Epoch: 6| Step: 6
Training loss: 1.7651798733702926
Validation loss: 2.402260532333433

Epoch: 6| Step: 7
Training loss: 2.344903890759059
Validation loss: 2.404075201224051

Epoch: 6| Step: 8
Training loss: 2.0633693365716193
Validation loss: 2.4035611507973655

Epoch: 6| Step: 9
Training loss: 2.6840615565806534
Validation loss: 2.394774664682569

Epoch: 6| Step: 10
Training loss: 2.1802566496235114
Validation loss: 2.38687209881725

Epoch: 6| Step: 11
Training loss: 2.0198430128753513
Validation loss: 2.404467025342999

Epoch: 6| Step: 12
Training loss: 2.3576033047547105
Validation loss: 2.402656563755956

Epoch: 6| Step: 13
Training loss: 2.8362455174497687
Validation loss: 2.3860982992684137

Epoch: 32| Step: 0
Training loss: 2.542993313953462
Validation loss: 2.401542807113396

Epoch: 6| Step: 1
Training loss: 2.3598484802097657
Validation loss: 2.3968309785529684

Epoch: 6| Step: 2
Training loss: 2.2331744616732325
Validation loss: 2.411317105754486

Epoch: 6| Step: 3
Training loss: 2.392161953600939
Validation loss: 2.392940289341308

Epoch: 6| Step: 4
Training loss: 1.959447347713769
Validation loss: 2.395846098713798

Epoch: 6| Step: 5
Training loss: 2.737395792332003
Validation loss: 2.4167702395069557

Epoch: 6| Step: 6
Training loss: 2.0716342377624435
Validation loss: 2.40334253405513

Epoch: 6| Step: 7
Training loss: 2.2373072450607383
Validation loss: 2.4082205824104506

Epoch: 6| Step: 8
Training loss: 2.3479598828061543
Validation loss: 2.416347636068903

Epoch: 6| Step: 9
Training loss: 2.413646279412454
Validation loss: 2.411830195076429

Epoch: 6| Step: 10
Training loss: 2.1232984125368812
Validation loss: 2.4067640539878843

Epoch: 6| Step: 11
Training loss: 2.6486607460296976
Validation loss: 2.4137106168837414

Epoch: 6| Step: 12
Training loss: 2.287103745789934
Validation loss: 2.4275172968469847

Epoch: 6| Step: 13
Training loss: 2.8153110443426503
Validation loss: 2.422135681561374

Epoch: 33| Step: 0
Training loss: 2.649731705237572
Validation loss: 2.4078213981383136

Epoch: 6| Step: 1
Training loss: 1.9405213455799364
Validation loss: 2.4155962864599156

Epoch: 6| Step: 2
Training loss: 2.276374519038494
Validation loss: 2.419452735678881

Epoch: 6| Step: 3
Training loss: 2.019103368296243
Validation loss: 2.4184368380956305

Epoch: 6| Step: 4
Training loss: 2.0763652492553977
Validation loss: 2.415035985648271

Epoch: 6| Step: 5
Training loss: 1.6641012554401682
Validation loss: 2.4138035805127185

Epoch: 6| Step: 6
Training loss: 2.7332001696302535
Validation loss: 2.4146068827479357

Epoch: 6| Step: 7
Training loss: 2.8799588105117304
Validation loss: 2.40999226396092

Epoch: 6| Step: 8
Training loss: 1.5613266926506675
Validation loss: 2.4069522972245423

Epoch: 6| Step: 9
Training loss: 2.7798300558055984
Validation loss: 2.4131373800457276

Epoch: 6| Step: 10
Training loss: 2.9298915944534576
Validation loss: 2.4094194288565043

Epoch: 6| Step: 11
Training loss: 2.3482108833582576
Validation loss: 2.4024133191463877

Epoch: 6| Step: 12
Training loss: 2.095265651238877
Validation loss: 2.417869812354038

Epoch: 6| Step: 13
Training loss: 2.8787651286222506
Validation loss: 2.4061168617307627

Epoch: 34| Step: 0
Training loss: 2.504330698777686
Validation loss: 2.40138601838938

Epoch: 6| Step: 1
Training loss: 2.930011700811881
Validation loss: 2.403862350702739

Epoch: 6| Step: 2
Training loss: 2.7387771483920864
Validation loss: 2.405890565919713

Epoch: 6| Step: 3
Training loss: 1.9363451561593399
Validation loss: 2.397770684375333

Epoch: 6| Step: 4
Training loss: 3.0879877546887893
Validation loss: 2.40991364695012

Epoch: 6| Step: 5
Training loss: 1.918902323389334
Validation loss: 2.397685493265216

Epoch: 6| Step: 6
Training loss: 1.816063370695482
Validation loss: 2.397604459052192

Epoch: 6| Step: 7
Training loss: 1.8776829915072084
Validation loss: 2.403000441532251

Epoch: 6| Step: 8
Training loss: 2.8368932201256745
Validation loss: 2.397811658887195

Epoch: 6| Step: 9
Training loss: 2.8054306193400005
Validation loss: 2.385997607080454

Epoch: 6| Step: 10
Training loss: 1.9391188472703553
Validation loss: 2.3981268202107255

Epoch: 6| Step: 11
Training loss: 2.2042579713694366
Validation loss: 2.392444325553004

Epoch: 6| Step: 12
Training loss: 1.9465826705987448
Validation loss: 2.397873587487894

Epoch: 6| Step: 13
Training loss: 2.184675409400604
Validation loss: 2.3996141428183106

Epoch: 35| Step: 0
Training loss: 2.078292381027948
Validation loss: 2.396710390077647

Epoch: 6| Step: 1
Training loss: 2.0112339657843172
Validation loss: 2.4069423422633336

Epoch: 6| Step: 2
Training loss: 2.319776890648688
Validation loss: 2.392221113284824

Epoch: 6| Step: 3
Training loss: 2.3803798577563025
Validation loss: 2.4002718460248498

Epoch: 6| Step: 4
Training loss: 2.4667663330700322
Validation loss: 2.4005171119383553

Epoch: 6| Step: 5
Training loss: 2.25117313743078
Validation loss: 2.3936223871728557

Epoch: 6| Step: 6
Training loss: 2.933169480284636
Validation loss: 2.4049020434790807

Epoch: 6| Step: 7
Training loss: 2.774686805990523
Validation loss: 2.4142604446324776

Epoch: 6| Step: 8
Training loss: 2.421182841175534
Validation loss: 2.4136894703273257

Epoch: 6| Step: 9
Training loss: 1.914362171614579
Validation loss: 2.4141225303255793

Epoch: 6| Step: 10
Training loss: 2.636051076489708
Validation loss: 2.418822614843143

Epoch: 6| Step: 11
Training loss: 2.030070740167201
Validation loss: 2.406451617294749

Epoch: 6| Step: 12
Training loss: 2.3807354989499974
Validation loss: 2.3997282516755623

Epoch: 6| Step: 13
Training loss: 2.3974656114707495
Validation loss: 2.3998971612408013

Epoch: 36| Step: 0
Training loss: 2.2601570179499864
Validation loss: 2.420724886918034

Epoch: 6| Step: 1
Training loss: 2.323428749011555
Validation loss: 2.4021795449311893

Epoch: 6| Step: 2
Training loss: 2.352358863804818
Validation loss: 2.412148540966503

Epoch: 6| Step: 3
Training loss: 1.8193004234070664
Validation loss: 2.420704761927471

Epoch: 6| Step: 4
Training loss: 2.7224498435200193
Validation loss: 2.406251015601006

Epoch: 6| Step: 5
Training loss: 2.783286902521996
Validation loss: 2.397851050103586

Epoch: 6| Step: 6
Training loss: 2.5672334821301352
Validation loss: 2.4072084706047656

Epoch: 6| Step: 7
Training loss: 1.9175016063499437
Validation loss: 2.398060324768338

Epoch: 6| Step: 8
Training loss: 2.011580082827046
Validation loss: 2.389977660453809

Epoch: 6| Step: 9
Training loss: 2.7953819679937952
Validation loss: 2.3925081619102215

Epoch: 6| Step: 10
Training loss: 2.5440602106370593
Validation loss: 2.4024205472089295

Epoch: 6| Step: 11
Training loss: 2.157334317667249
Validation loss: 2.3925607361453562

Epoch: 6| Step: 12
Training loss: 2.189372759661888
Validation loss: 2.407100510666078

Epoch: 6| Step: 13
Training loss: 2.5128576565012906
Validation loss: 2.404413422384215

Epoch: 37| Step: 0
Training loss: 2.2526077417481263
Validation loss: 2.400493589613442

Epoch: 6| Step: 1
Training loss: 2.4261788796363826
Validation loss: 2.4034825466208214

Epoch: 6| Step: 2
Training loss: 2.868941848755908
Validation loss: 2.3992648442505993

Epoch: 6| Step: 3
Training loss: 1.8355878781132673
Validation loss: 2.4019223223473176

Epoch: 6| Step: 4
Training loss: 2.274704832274862
Validation loss: 2.399638452053079

Epoch: 6| Step: 5
Training loss: 2.428371256905489
Validation loss: 2.3969893831482896

Epoch: 6| Step: 6
Training loss: 2.7199941835621746
Validation loss: 2.390006299043811

Epoch: 6| Step: 7
Training loss: 2.6429399805084617
Validation loss: 2.392878972037328

Epoch: 6| Step: 8
Training loss: 2.516311457073433
Validation loss: 2.3916785800340277

Epoch: 6| Step: 9
Training loss: 2.1778519534542635
Validation loss: 2.388897240301315

Epoch: 6| Step: 10
Training loss: 1.7016003985427368
Validation loss: 2.4044455661240267

Epoch: 6| Step: 11
Training loss: 2.4428037992148983
Validation loss: 2.3937677573665073

Epoch: 6| Step: 12
Training loss: 2.2200154738703137
Validation loss: 2.3893190111895803

Epoch: 6| Step: 13
Training loss: 2.3851193127488117
Validation loss: 2.401909649920205

Epoch: 38| Step: 0
Training loss: 1.5566703381927345
Validation loss: 2.4062900622447065

Epoch: 6| Step: 1
Training loss: 2.7356296821468757
Validation loss: 2.4018032882325704

Epoch: 6| Step: 2
Training loss: 2.6463079752673115
Validation loss: 2.4011848445633293

Epoch: 6| Step: 3
Training loss: 1.7417181092594152
Validation loss: 2.4046217871617372

Epoch: 6| Step: 4
Training loss: 2.4861496638807967
Validation loss: 2.392893427619122

Epoch: 6| Step: 5
Training loss: 2.311803558227536
Validation loss: 2.3975198172637624

Epoch: 6| Step: 6
Training loss: 2.605895869577496
Validation loss: 2.4049058768345404

Epoch: 6| Step: 7
Training loss: 3.0350431325446183
Validation loss: 2.3894723178536936

Epoch: 6| Step: 8
Training loss: 1.8348623171093255
Validation loss: 2.382507921774771

Epoch: 6| Step: 9
Training loss: 2.3040837935800282
Validation loss: 2.377909517822785

Epoch: 6| Step: 10
Training loss: 2.5077950069970116
Validation loss: 2.3910727486769057

Epoch: 6| Step: 11
Training loss: 2.3442024302904576
Validation loss: 2.3899438257412604

Epoch: 6| Step: 12
Training loss: 2.3449592014171663
Validation loss: 2.395013707705076

Epoch: 6| Step: 13
Training loss: 2.284343033552576
Validation loss: 2.3927885500180226

Epoch: 39| Step: 0
Training loss: 2.22731021573537
Validation loss: 2.4013593936528412

Epoch: 6| Step: 1
Training loss: 2.047554194626272
Validation loss: 2.4100259738461616

Epoch: 6| Step: 2
Training loss: 3.1318708036627183
Validation loss: 2.4220221033849123

Epoch: 6| Step: 3
Training loss: 2.169298139005691
Validation loss: 2.4050193959126154

Epoch: 6| Step: 4
Training loss: 3.073731515897856
Validation loss: 2.417370436127836

Epoch: 6| Step: 5
Training loss: 2.4400156195719083
Validation loss: 2.427039613998521

Epoch: 6| Step: 6
Training loss: 2.105308285969869
Validation loss: 2.421058738728618

Epoch: 6| Step: 7
Training loss: 2.279631576218187
Validation loss: 2.407792575365095

Epoch: 6| Step: 8
Training loss: 2.0845213236771523
Validation loss: 2.410376787528928

Epoch: 6| Step: 9
Training loss: 1.5901114123675282
Validation loss: 2.401742858997343

Epoch: 6| Step: 10
Training loss: 2.0887549870886355
Validation loss: 2.3961839142974393

Epoch: 6| Step: 11
Training loss: 2.5656605280136584
Validation loss: 2.398150316041315

Epoch: 6| Step: 12
Training loss: 2.534558241836673
Validation loss: 2.3948200045503714

Epoch: 6| Step: 13
Training loss: 2.176407071689306
Validation loss: 2.394642224211335

Epoch: 40| Step: 0
Training loss: 2.659623102433326
Validation loss: 2.391449148363897

Epoch: 6| Step: 1
Training loss: 2.41955120212703
Validation loss: 2.412000019925414

Epoch: 6| Step: 2
Training loss: 2.701856458831075
Validation loss: 2.405002592684167

Epoch: 6| Step: 3
Training loss: 2.410953433168957
Validation loss: 2.3978259935864825

Epoch: 6| Step: 4
Training loss: 1.8772996151959451
Validation loss: 2.3989159172130172

Epoch: 6| Step: 5
Training loss: 2.140542801441318
Validation loss: 2.3944988155190803

Epoch: 6| Step: 6
Training loss: 1.397048132149854
Validation loss: 2.378309545654192

Epoch: 6| Step: 7
Training loss: 2.3210363832290413
Validation loss: 2.3931426543551546

Epoch: 6| Step: 8
Training loss: 2.7087169864545686
Validation loss: 2.3904561168050007

Epoch: 6| Step: 9
Training loss: 2.113487698947187
Validation loss: 2.404336903680713

Epoch: 6| Step: 10
Training loss: 2.2232755257537304
Validation loss: 2.398826327372676

Epoch: 6| Step: 11
Training loss: 2.3305252298537478
Validation loss: 2.3982132300480172

Epoch: 6| Step: 12
Training loss: 2.9640346624327014
Validation loss: 2.3784510819608635

Epoch: 6| Step: 13
Training loss: 2.219785314437773
Validation loss: 2.4103387055665784

Epoch: 41| Step: 0
Training loss: 2.196770777610908
Validation loss: 2.3901053800986434

Epoch: 6| Step: 1
Training loss: 1.8689055260078884
Validation loss: 2.40726489177776

Epoch: 6| Step: 2
Training loss: 2.8363574849353235
Validation loss: 2.4227847123780006

Epoch: 6| Step: 3
Training loss: 2.801870647616796
Validation loss: 2.423803009140791

Epoch: 6| Step: 4
Training loss: 1.8794207275023802
Validation loss: 2.4293108093087814

Epoch: 6| Step: 5
Training loss: 2.774792837022453
Validation loss: 2.4450509253401065

Epoch: 6| Step: 6
Training loss: 2.0109390080942044
Validation loss: 2.4344845402320017

Epoch: 6| Step: 7
Training loss: 1.7103204877469265
Validation loss: 2.432537782512207

Epoch: 6| Step: 8
Training loss: 2.3071753570886053
Validation loss: 2.422149109453926

Epoch: 6| Step: 9
Training loss: 2.6099506588360706
Validation loss: 2.421418579737093

Epoch: 6| Step: 10
Training loss: 2.489522148455186
Validation loss: 2.43096486293413

Epoch: 6| Step: 11
Training loss: 2.3903208021712232
Validation loss: 2.4368705833481785

Epoch: 6| Step: 12
Training loss: 1.557638157431335
Validation loss: 2.4121158491657804

Epoch: 6| Step: 13
Training loss: 2.973549266324305
Validation loss: 2.408329993600961

Epoch: 42| Step: 0
Training loss: 1.839361107242652
Validation loss: 2.395669887330195

Epoch: 6| Step: 1
Training loss: 2.7023881558433698
Validation loss: 2.3930520430401527

Epoch: 6| Step: 2
Training loss: 2.13943548635776
Validation loss: 2.3982583475043526

Epoch: 6| Step: 3
Training loss: 2.6629117119049197
Validation loss: 2.3760922162472093

Epoch: 6| Step: 4
Training loss: 2.403884815198605
Validation loss: 2.3888448765369636

Epoch: 6| Step: 5
Training loss: 2.5366853355225354
Validation loss: 2.388441696991827

Epoch: 6| Step: 6
Training loss: 2.115489433599387
Validation loss: 2.3909090409425557

Epoch: 6| Step: 7
Training loss: 2.299528496552613
Validation loss: 2.3794495077636215

Epoch: 6| Step: 8
Training loss: 2.2016895568874433
Validation loss: 2.3745369710801234

Epoch: 6| Step: 9
Training loss: 1.9106737929613027
Validation loss: 2.3875628205135038

Epoch: 6| Step: 10
Training loss: 2.4366903672150695
Validation loss: 2.3761601542411044

Epoch: 6| Step: 11
Training loss: 2.5602915481609654
Validation loss: 2.386104806539634

Epoch: 6| Step: 12
Training loss: 2.3904827798663186
Validation loss: 2.3736278518712393

Epoch: 6| Step: 13
Training loss: 2.5676712332644516
Validation loss: 2.3874915620478303

Epoch: 43| Step: 0
Training loss: 1.686164327358096
Validation loss: 2.394193823616445

Epoch: 6| Step: 1
Training loss: 2.578152188244424
Validation loss: 2.4132277476500215

Epoch: 6| Step: 2
Training loss: 2.3056826754331308
Validation loss: 2.402059762601406

Epoch: 6| Step: 3
Training loss: 2.8676704187146567
Validation loss: 2.426351401404532

Epoch: 6| Step: 4
Training loss: 2.5767396846293478
Validation loss: 2.4137172019938813

Epoch: 6| Step: 5
Training loss: 1.9834962836678618
Validation loss: 2.4225265774943696

Epoch: 6| Step: 6
Training loss: 1.8212617028357194
Validation loss: 2.4369371367486194

Epoch: 6| Step: 7
Training loss: 1.8123089919895805
Validation loss: 2.448221493402643

Epoch: 6| Step: 8
Training loss: 2.944302737426833
Validation loss: 2.4824239719320986

Epoch: 6| Step: 9
Training loss: 2.1348422386612476
Validation loss: 2.474593220943804

Epoch: 6| Step: 10
Training loss: 2.37210428489207
Validation loss: 2.4385107504215493

Epoch: 6| Step: 11
Training loss: 2.7156306820931815
Validation loss: 2.425880434196428

Epoch: 6| Step: 12
Training loss: 2.6724892964162135
Validation loss: 2.4192099890365197

Epoch: 6| Step: 13
Training loss: 1.7570896272981538
Validation loss: 2.399806771844465

Epoch: 44| Step: 0
Training loss: 2.6789950262908504
Validation loss: 2.4004606397622923

Epoch: 6| Step: 1
Training loss: 2.236135149629751
Validation loss: 2.38805448174907

Epoch: 6| Step: 2
Training loss: 2.450923831137475
Validation loss: 2.3863384237237355

Epoch: 6| Step: 3
Training loss: 1.9188810769703608
Validation loss: 2.370274845931487

Epoch: 6| Step: 4
Training loss: 1.963898873322517
Validation loss: 2.3747647403176977

Epoch: 6| Step: 5
Training loss: 2.220476580146919
Validation loss: 2.3869531061685243

Epoch: 6| Step: 6
Training loss: 2.638874082914165
Validation loss: 2.3840831599655212

Epoch: 6| Step: 7
Training loss: 2.039526291057029
Validation loss: 2.3753777253789172

Epoch: 6| Step: 8
Training loss: 2.6829399634037663
Validation loss: 2.3903623615346965

Epoch: 6| Step: 9
Training loss: 2.821143368728606
Validation loss: 2.3842381532661476

Epoch: 6| Step: 10
Training loss: 2.202696846555864
Validation loss: 2.3716695841487363

Epoch: 6| Step: 11
Training loss: 2.185300538717511
Validation loss: 2.409478898700564

Epoch: 6| Step: 12
Training loss: 1.8748180301102855
Validation loss: 2.387280254004389

Epoch: 6| Step: 13
Training loss: 2.5629925138235436
Validation loss: 2.4021389840377325

Epoch: 45| Step: 0
Training loss: 2.1613832269205018
Validation loss: 2.392785145632828

Epoch: 6| Step: 1
Training loss: 2.780739383758525
Validation loss: 2.416938517067727

Epoch: 6| Step: 2
Training loss: 2.1073000274306293
Validation loss: 2.41960467501316

Epoch: 6| Step: 3
Training loss: 2.0809436698669725
Validation loss: 2.4394282188803555

Epoch: 6| Step: 4
Training loss: 1.6376296538064163
Validation loss: 2.4305827463233696

Epoch: 6| Step: 5
Training loss: 2.563329236837949
Validation loss: 2.442254841128877

Epoch: 6| Step: 6
Training loss: 2.0655358830559916
Validation loss: 2.4372546406687805

Epoch: 6| Step: 7
Training loss: 2.6958343442967054
Validation loss: 2.431990118074104

Epoch: 6| Step: 8
Training loss: 1.8684333252273981
Validation loss: 2.4231426408066685

Epoch: 6| Step: 9
Training loss: 2.444149630522206
Validation loss: 2.430939379464671

Epoch: 6| Step: 10
Training loss: 2.6151315385401146
Validation loss: 2.408656250667285

Epoch: 6| Step: 11
Training loss: 2.6019758749509774
Validation loss: 2.4012020385760646

Epoch: 6| Step: 12
Training loss: 1.504804070731563
Validation loss: 2.3926133839610735

Epoch: 6| Step: 13
Training loss: 2.990500031573779
Validation loss: 2.3946415272678503

Epoch: 46| Step: 0
Training loss: 3.108325263482494
Validation loss: 2.375055379389207

Epoch: 6| Step: 1
Training loss: 2.4796160336094983
Validation loss: 2.3765871951667523

Epoch: 6| Step: 2
Training loss: 2.015471814862497
Validation loss: 2.3826394357003973

Epoch: 6| Step: 3
Training loss: 1.8478396689606078
Validation loss: 2.377226179368308

Epoch: 6| Step: 4
Training loss: 1.703698787801134
Validation loss: 2.3697443080543583

Epoch: 6| Step: 5
Training loss: 1.957177984710031
Validation loss: 2.3739056659350353

Epoch: 6| Step: 6
Training loss: 2.5072214731413056
Validation loss: 2.3880888673720646

Epoch: 6| Step: 7
Training loss: 2.3181415510165286
Validation loss: 2.3877078440206683

Epoch: 6| Step: 8
Training loss: 2.0241383402970263
Validation loss: 2.3870687691134425

Epoch: 6| Step: 9
Training loss: 2.5384776219389766
Validation loss: 2.4075969707677625

Epoch: 6| Step: 10
Training loss: 2.64352510546341
Validation loss: 2.42666699081548

Epoch: 6| Step: 11
Training loss: 2.4928425373488046
Validation loss: 2.448586024762128

Epoch: 6| Step: 12
Training loss: 2.504829891010156
Validation loss: 2.4410894651248594

Epoch: 6| Step: 13
Training loss: 2.4140494633294836
Validation loss: 2.4421550528107123

Epoch: 47| Step: 0
Training loss: 1.932064553732307
Validation loss: 2.422419136201567

Epoch: 6| Step: 1
Training loss: 2.3369954752978237
Validation loss: 2.404921251502689

Epoch: 6| Step: 2
Training loss: 1.9168089799982684
Validation loss: 2.4072448275829412

Epoch: 6| Step: 3
Training loss: 2.307117280476661
Validation loss: 2.391910797268587

Epoch: 6| Step: 4
Training loss: 2.310364845825264
Validation loss: 2.39565326732715

Epoch: 6| Step: 5
Training loss: 2.15421220988294
Validation loss: 2.408197721097297

Epoch: 6| Step: 6
Training loss: 2.4328088857125327
Validation loss: 2.402643101347302

Epoch: 6| Step: 7
Training loss: 2.215235330725746
Validation loss: 2.3892845434837233

Epoch: 6| Step: 8
Training loss: 2.69202282048227
Validation loss: 2.3870284425356436

Epoch: 6| Step: 9
Training loss: 2.7243348561020553
Validation loss: 2.3896366145594072

Epoch: 6| Step: 10
Training loss: 2.502924448425311
Validation loss: 2.382053942398675

Epoch: 6| Step: 11
Training loss: 1.728139993833646
Validation loss: 2.377481444173096

Epoch: 6| Step: 12
Training loss: 2.544990731842612
Validation loss: 2.3634145651802454

Epoch: 6| Step: 13
Training loss: 2.2295629796716363
Validation loss: 2.3784254033909584

Epoch: 48| Step: 0
Training loss: 2.802693202550374
Validation loss: 2.3728008545659653

Epoch: 6| Step: 1
Training loss: 1.771570239357185
Validation loss: 2.396174146764678

Epoch: 6| Step: 2
Training loss: 2.519251040468401
Validation loss: 2.379726984502583

Epoch: 6| Step: 3
Training loss: 2.4762191775164206
Validation loss: 2.364578655983308

Epoch: 6| Step: 4
Training loss: 2.088152905540033
Validation loss: 2.3834747144695463

Epoch: 6| Step: 5
Training loss: 2.1739959240746214
Validation loss: 2.387656502645565

Epoch: 6| Step: 6
Training loss: 2.4497414355174536
Validation loss: 2.3696391185805337

Epoch: 6| Step: 7
Training loss: 1.9034858424163976
Validation loss: 2.377876380359459

Epoch: 6| Step: 8
Training loss: 2.7726941469149504
Validation loss: 2.3683707004859156

Epoch: 6| Step: 9
Training loss: 2.2728629418540867
Validation loss: 2.3765164519862214

Epoch: 6| Step: 10
Training loss: 2.321525077071467
Validation loss: 2.377860488250273

Epoch: 6| Step: 11
Training loss: 2.4681608245483035
Validation loss: 2.3891948502786162

Epoch: 6| Step: 12
Training loss: 1.788470974211811
Validation loss: 2.4103963723038584

Epoch: 6| Step: 13
Training loss: 2.115674255789418
Validation loss: 2.410981352879676

Epoch: 49| Step: 0
Training loss: 2.334396211050304
Validation loss: 2.4245871667678123

Epoch: 6| Step: 1
Training loss: 2.2161032780567047
Validation loss: 2.44797082124152

Epoch: 6| Step: 2
Training loss: 2.3766980375205335
Validation loss: 2.4570764033096015

Epoch: 6| Step: 3
Training loss: 2.252433732187366
Validation loss: 2.4540842452616602

Epoch: 6| Step: 4
Training loss: 2.1488094493371315
Validation loss: 2.4513402387085854

Epoch: 6| Step: 5
Training loss: 3.052403056786823
Validation loss: 2.4365014288653004

Epoch: 6| Step: 6
Training loss: 2.180819091206177
Validation loss: 2.431350322997261

Epoch: 6| Step: 7
Training loss: 1.991603990257604
Validation loss: 2.43630252669946

Epoch: 6| Step: 8
Training loss: 2.419705015599693
Validation loss: 2.4158696197577756

Epoch: 6| Step: 9
Training loss: 2.4085527771054838
Validation loss: 2.4413395091788974

Epoch: 6| Step: 10
Training loss: 2.4990860221989353
Validation loss: 2.436078504812728

Epoch: 6| Step: 11
Training loss: 1.9822876532456033
Validation loss: 2.4343729501502596

Epoch: 6| Step: 12
Training loss: 2.3224395353168266
Validation loss: 2.415593185642997

Epoch: 6| Step: 13
Training loss: 1.9164031372791488
Validation loss: 2.396146734443938

Epoch: 50| Step: 0
Training loss: 2.3384131132162094
Validation loss: 2.3922391441147535

Epoch: 6| Step: 1
Training loss: 2.547828449043541
Validation loss: 2.392013736512644

Epoch: 6| Step: 2
Training loss: 2.608880732888393
Validation loss: 2.384817053604554

Epoch: 6| Step: 3
Training loss: 2.2553468328386352
Validation loss: 2.3758427479946675

Epoch: 6| Step: 4
Training loss: 2.363300619755332
Validation loss: 2.3852823766586027

Epoch: 6| Step: 5
Training loss: 2.463189829138001
Validation loss: 2.373006301763274

Epoch: 6| Step: 6
Training loss: 2.095273616466964
Validation loss: 2.383652352498624

Epoch: 6| Step: 7
Training loss: 2.0867047559696665
Validation loss: 2.381609353411871

Epoch: 6| Step: 8
Training loss: 2.0245803492366354
Validation loss: 2.3864118729432313

Epoch: 6| Step: 9
Training loss: 2.2569460146645848
Validation loss: 2.3739556392192966

Epoch: 6| Step: 10
Training loss: 2.2722538229504834
Validation loss: 2.3848508027632316

Epoch: 6| Step: 11
Training loss: 2.3817846864360983
Validation loss: 2.385504280935623

Epoch: 6| Step: 12
Training loss: 2.454428065508065
Validation loss: 2.3841615620513674

Epoch: 6| Step: 13
Training loss: 1.8364011138581937
Validation loss: 2.4129682692570293

Epoch: 51| Step: 0
Training loss: 2.2457604202390566
Validation loss: 2.409413029906172

Epoch: 6| Step: 1
Training loss: 2.43419187980879
Validation loss: 2.411343571106546

Epoch: 6| Step: 2
Training loss: 1.5368048670443941
Validation loss: 2.4139545009120935

Epoch: 6| Step: 3
Training loss: 1.8509302790209659
Validation loss: 2.4114247862842215

Epoch: 6| Step: 4
Training loss: 2.3745655867161166
Validation loss: 2.418634030422982

Epoch: 6| Step: 5
Training loss: 1.8207333450739391
Validation loss: 2.4038972375283634

Epoch: 6| Step: 6
Training loss: 3.16743195639058
Validation loss: 2.399328499141606

Epoch: 6| Step: 7
Training loss: 1.6895483794949622
Validation loss: 2.4142910911863487

Epoch: 6| Step: 8
Training loss: 2.406622325280917
Validation loss: 2.4117646705927545

Epoch: 6| Step: 9
Training loss: 2.2849687548281206
Validation loss: 2.400855387469758

Epoch: 6| Step: 10
Training loss: 2.160105920243246
Validation loss: 2.3850057045250574

Epoch: 6| Step: 11
Training loss: 2.237546364074782
Validation loss: 2.4084218450030628

Epoch: 6| Step: 12
Training loss: 2.92153439091972
Validation loss: 2.4102852414327147

Epoch: 6| Step: 13
Training loss: 2.1825490236839333
Validation loss: 2.4028625176272276

Epoch: 52| Step: 0
Training loss: 2.045222306794595
Validation loss: 2.394267015258409

Epoch: 6| Step: 1
Training loss: 2.03867122944699
Validation loss: 2.403305836964249

Epoch: 6| Step: 2
Training loss: 2.0480169939811286
Validation loss: 2.4111249760577267

Epoch: 6| Step: 3
Training loss: 2.1291880009945343
Validation loss: 2.4175739941849588

Epoch: 6| Step: 4
Training loss: 2.0519174679821206
Validation loss: 2.4276244388484933

Epoch: 6| Step: 5
Training loss: 2.2353309773721786
Validation loss: 2.426660678283233

Epoch: 6| Step: 6
Training loss: 3.0259431613946766
Validation loss: 2.4594381619154344

Epoch: 6| Step: 7
Training loss: 2.0335253369507122
Validation loss: 2.464223276786056

Epoch: 6| Step: 8
Training loss: 1.9600289356275622
Validation loss: 2.4587186387477633

Epoch: 6| Step: 9
Training loss: 1.5723876147599278
Validation loss: 2.4543605211914143

Epoch: 6| Step: 10
Training loss: 2.5100215321259376
Validation loss: 2.4715893991559397

Epoch: 6| Step: 11
Training loss: 3.029457899643177
Validation loss: 2.46497872975982

Epoch: 6| Step: 12
Training loss: 2.177518907502136
Validation loss: 2.4386755724977727

Epoch: 6| Step: 13
Training loss: 2.4964771244980577
Validation loss: 2.4006872405127795

Epoch: 53| Step: 0
Training loss: 2.1890514866336086
Validation loss: 2.3894963477836435

Epoch: 6| Step: 1
Training loss: 2.511327828795771
Validation loss: 2.3787438348158036

Epoch: 6| Step: 2
Training loss: 2.739416912714726
Validation loss: 2.381192199459422

Epoch: 6| Step: 3
Training loss: 2.059817448091969
Validation loss: 2.371270690047471

Epoch: 6| Step: 4
Training loss: 1.679621921412423
Validation loss: 2.3826956965267105

Epoch: 6| Step: 5
Training loss: 2.136514322659161
Validation loss: 2.396827306359963

Epoch: 6| Step: 6
Training loss: 1.5750753566334839
Validation loss: 2.392277613860472

Epoch: 6| Step: 7
Training loss: 2.5621711706056858
Validation loss: 2.370352665379197

Epoch: 6| Step: 8
Training loss: 2.7701268167764757
Validation loss: 2.377052800291409

Epoch: 6| Step: 9
Training loss: 2.442681893298459
Validation loss: 2.3749983938111345

Epoch: 6| Step: 10
Training loss: 2.0788623390878795
Validation loss: 2.370033450246037

Epoch: 6| Step: 11
Training loss: 1.7421844892946239
Validation loss: 2.3861228253016944

Epoch: 6| Step: 12
Training loss: 2.3255941472701944
Validation loss: 2.3713514511614546

Epoch: 6| Step: 13
Training loss: 2.759250082469627
Validation loss: 2.3933159138285545

Epoch: 54| Step: 0
Training loss: 1.9545369652126225
Validation loss: 2.391647269893593

Epoch: 6| Step: 1
Training loss: 2.1820637319122813
Validation loss: 2.413884902231865

Epoch: 6| Step: 2
Training loss: 2.1271817843686986
Validation loss: 2.4208461996405193

Epoch: 6| Step: 3
Training loss: 2.490655601629365
Validation loss: 2.4188792086580695

Epoch: 6| Step: 4
Training loss: 2.496134153227142
Validation loss: 2.4239409546228443

Epoch: 6| Step: 5
Training loss: 1.926058426229272
Validation loss: 2.4351646370162516

Epoch: 6| Step: 6
Training loss: 2.2445768060253526
Validation loss: 2.4335979137109494

Epoch: 6| Step: 7
Training loss: 1.9998872248325255
Validation loss: 2.430512119692257

Epoch: 6| Step: 8
Training loss: 2.9159197986389693
Validation loss: 2.4175903813095667

Epoch: 6| Step: 9
Training loss: 1.6271625214606984
Validation loss: 2.4042188981419095

Epoch: 6| Step: 10
Training loss: 2.6048153082161107
Validation loss: 2.4023612830703347

Epoch: 6| Step: 11
Training loss: 2.5600190597063084
Validation loss: 2.4031872103426544

Epoch: 6| Step: 12
Training loss: 1.8586362124351206
Validation loss: 2.3963127292388093

Epoch: 6| Step: 13
Training loss: 2.2809184630394355
Validation loss: 2.3924783906455693

Epoch: 55| Step: 0
Training loss: 2.6766865556230566
Validation loss: 2.397836036110889

Epoch: 6| Step: 1
Training loss: 1.7757859330256112
Validation loss: 2.4162075878248883

Epoch: 6| Step: 2
Training loss: 1.9887865544861292
Validation loss: 2.4144568589280437

Epoch: 6| Step: 3
Training loss: 1.9104894179785152
Validation loss: 2.4205036834014244

Epoch: 6| Step: 4
Training loss: 1.9487380541618218
Validation loss: 2.414158890167061

Epoch: 6| Step: 5
Training loss: 2.5237681641664236
Validation loss: 2.430243539916246

Epoch: 6| Step: 6
Training loss: 1.7695321931478585
Validation loss: 2.415047034324881

Epoch: 6| Step: 7
Training loss: 2.1924603488864305
Validation loss: 2.391318318344414

Epoch: 6| Step: 8
Training loss: 2.1211155561868273
Validation loss: 2.3989555304761163

Epoch: 6| Step: 9
Training loss: 2.43085768017778
Validation loss: 2.400372216713046

Epoch: 6| Step: 10
Training loss: 1.687861792316586
Validation loss: 2.4059731790744676

Epoch: 6| Step: 11
Training loss: 2.018197008102327
Validation loss: 2.4088971823210943

Epoch: 6| Step: 12
Training loss: 3.09264438794743
Validation loss: 2.4107871278391215

Epoch: 6| Step: 13
Training loss: 2.9440876566649563
Validation loss: 2.3847574686363426

Epoch: 56| Step: 0
Training loss: 2.775862372490888
Validation loss: 2.397187908757549

Epoch: 6| Step: 1
Training loss: 1.9162679616903728
Validation loss: 2.400752678372283

Epoch: 6| Step: 2
Training loss: 1.8453547073175638
Validation loss: 2.411297891001811

Epoch: 6| Step: 3
Training loss: 2.6146867789870116
Validation loss: 2.427025239000879

Epoch: 6| Step: 4
Training loss: 2.0063985751744737
Validation loss: 2.434089263221662

Epoch: 6| Step: 5
Training loss: 2.023411341906813
Validation loss: 2.4578468574180645

Epoch: 6| Step: 6
Training loss: 2.6377240560378525
Validation loss: 2.458667123623155

Epoch: 6| Step: 7
Training loss: 2.5163192264984975
Validation loss: 2.47404742164156

Epoch: 6| Step: 8
Training loss: 2.332665438657257
Validation loss: 2.4449351611703047

Epoch: 6| Step: 9
Training loss: 1.79531620247694
Validation loss: 2.4710893291442284

Epoch: 6| Step: 10
Training loss: 2.145790605829507
Validation loss: 2.436197984628491

Epoch: 6| Step: 11
Training loss: 1.784097102717178
Validation loss: 2.4145125676323906

Epoch: 6| Step: 12
Training loss: 2.402818983260016
Validation loss: 2.4282616523246565

Epoch: 6| Step: 13
Training loss: 2.372625971385198
Validation loss: 2.392966617521423

Epoch: 57| Step: 0
Training loss: 2.234279817274435
Validation loss: 2.3766816192342066

Epoch: 6| Step: 1
Training loss: 1.9741306951017132
Validation loss: 2.377430090880899

Epoch: 6| Step: 2
Training loss: 2.3985987307752947
Validation loss: 2.3628180267924552

Epoch: 6| Step: 3
Training loss: 2.1736140252832845
Validation loss: 2.3822519945967713

Epoch: 6| Step: 4
Training loss: 2.384099793956865
Validation loss: 2.3776920938824384

Epoch: 6| Step: 5
Training loss: 2.9547968577416652
Validation loss: 2.3729978454297136

Epoch: 6| Step: 6
Training loss: 2.400150842694681
Validation loss: 2.389513584328503

Epoch: 6| Step: 7
Training loss: 2.3528628381843335
Validation loss: 2.3802580769110726

Epoch: 6| Step: 8
Training loss: 2.459555101404415
Validation loss: 2.3724504137233664

Epoch: 6| Step: 9
Training loss: 2.1218777326473566
Validation loss: 2.3898298649105514

Epoch: 6| Step: 10
Training loss: 2.2758502545094936
Validation loss: 2.382615578440825

Epoch: 6| Step: 11
Training loss: 1.855406621595539
Validation loss: 2.387511625810848

Epoch: 6| Step: 12
Training loss: 2.045430612905731
Validation loss: 2.369260595711419

Epoch: 6| Step: 13
Training loss: 1.5709681114976617
Validation loss: 2.3710952582818

Epoch: 58| Step: 0
Training loss: 2.002912308325991
Validation loss: 2.383422840100499

Epoch: 6| Step: 1
Training loss: 2.216215701109437
Validation loss: 2.385251140820642

Epoch: 6| Step: 2
Training loss: 2.2636437533642026
Validation loss: 2.40738665994306

Epoch: 6| Step: 3
Training loss: 2.090309618780951
Validation loss: 2.406037374903257

Epoch: 6| Step: 4
Training loss: 2.078971288925901
Validation loss: 2.440052961519929

Epoch: 6| Step: 5
Training loss: 2.4324388044291023
Validation loss: 2.442923698339265

Epoch: 6| Step: 6
Training loss: 2.6740843801259695
Validation loss: 2.4780402041948713

Epoch: 6| Step: 7
Training loss: 2.2316804657619396
Validation loss: 2.467016754458937

Epoch: 6| Step: 8
Training loss: 1.4896215776277886
Validation loss: 2.499684091317603

Epoch: 6| Step: 9
Training loss: 2.178599752737844
Validation loss: 2.4941702422492282

Epoch: 6| Step: 10
Training loss: 2.7943719500151976
Validation loss: 2.5053138684245275

Epoch: 6| Step: 11
Training loss: 2.34470571741437
Validation loss: 2.494943472133332

Epoch: 6| Step: 12
Training loss: 2.4119354061758065
Validation loss: 2.443150077086732

Epoch: 6| Step: 13
Training loss: 1.833814716718318
Validation loss: 2.4275279204114706

Epoch: 59| Step: 0
Training loss: 2.0526389000899785
Validation loss: 2.401131904765425

Epoch: 6| Step: 1
Training loss: 1.86857559768175
Validation loss: 2.3843147920178875

Epoch: 6| Step: 2
Training loss: 1.293598678511285
Validation loss: 2.3864213307605033

Epoch: 6| Step: 3
Training loss: 1.7488843222478638
Validation loss: 2.3786751186460533

Epoch: 6| Step: 4
Training loss: 2.0366742994391247
Validation loss: 2.3883700571300355

Epoch: 6| Step: 5
Training loss: 2.470205047220024
Validation loss: 2.3741246333562747

Epoch: 6| Step: 6
Training loss: 2.465822536354195
Validation loss: 2.383676970379541

Epoch: 6| Step: 7
Training loss: 2.240088779869945
Validation loss: 2.3851588219517255

Epoch: 6| Step: 8
Training loss: 3.1274277983822807
Validation loss: 2.38279215486106

Epoch: 6| Step: 9
Training loss: 2.5947364861496345
Validation loss: 2.3899051105943427

Epoch: 6| Step: 10
Training loss: 2.3462803723539434
Validation loss: 2.3711220552618326

Epoch: 6| Step: 11
Training loss: 2.3899217955112397
Validation loss: 2.3863004493742475

Epoch: 6| Step: 12
Training loss: 1.6500756679870798
Validation loss: 2.3798647715380814

Epoch: 6| Step: 13
Training loss: 2.446796588445196
Validation loss: 2.3937233437829337

Epoch: 60| Step: 0
Training loss: 1.5461564369851504
Validation loss: 2.3915272591220575

Epoch: 6| Step: 1
Training loss: 2.0581827929758227
Validation loss: 2.425390469028908

Epoch: 6| Step: 2
Training loss: 1.744802795678321
Validation loss: 2.4236460783098885

Epoch: 6| Step: 3
Training loss: 2.6096107239381823
Validation loss: 2.4530241149762033

Epoch: 6| Step: 4
Training loss: 1.7207909690439065
Validation loss: 2.455620907244237

Epoch: 6| Step: 5
Training loss: 2.2078689470789667
Validation loss: 2.4792884160982305

Epoch: 6| Step: 6
Training loss: 2.2412928481083068
Validation loss: 2.5103046318369997

Epoch: 6| Step: 7
Training loss: 2.0406480960210738
Validation loss: 2.4839868782176016

Epoch: 6| Step: 8
Training loss: 2.7102463250629247
Validation loss: 2.500973337159714

Epoch: 6| Step: 9
Training loss: 2.7940033390295547
Validation loss: 2.5002041971737032

Epoch: 6| Step: 10
Training loss: 2.694411151938114
Validation loss: 2.4710508159711106

Epoch: 6| Step: 11
Training loss: 2.2723610548257445
Validation loss: 2.4306853392934067

Epoch: 6| Step: 12
Training loss: 1.9011686796495004
Validation loss: 2.4330332984762335

Epoch: 6| Step: 13
Training loss: 2.0575671305313628
Validation loss: 2.398332226955997

Epoch: 61| Step: 0
Training loss: 2.1033703143722278
Validation loss: 2.3844773443479856

Epoch: 6| Step: 1
Training loss: 2.2614186142136883
Validation loss: 2.3692336183449703

Epoch: 6| Step: 2
Training loss: 2.28126964821911
Validation loss: 2.3868206478994938

Epoch: 6| Step: 3
Training loss: 2.565760515283831
Validation loss: 2.3861889956233058

Epoch: 6| Step: 4
Training loss: 1.9717495789216082
Validation loss: 2.379459920092071

Epoch: 6| Step: 5
Training loss: 2.5406626192993396
Validation loss: 2.3819581965304972

Epoch: 6| Step: 6
Training loss: 2.3850581359175593
Validation loss: 2.3807111969907164

Epoch: 6| Step: 7
Training loss: 2.1729707218028733
Validation loss: 2.378827541074416

Epoch: 6| Step: 8
Training loss: 1.7784404347472107
Validation loss: 2.384591677625171

Epoch: 6| Step: 9
Training loss: 2.0429284874064932
Validation loss: 2.382036685238855

Epoch: 6| Step: 10
Training loss: 1.69708581110212
Validation loss: 2.392441701308296

Epoch: 6| Step: 11
Training loss: 2.3872227612650345
Validation loss: 2.3927591392914627

Epoch: 6| Step: 12
Training loss: 2.4933545480257404
Validation loss: 2.39436184572732

Epoch: 6| Step: 13
Training loss: 1.6526267528543395
Validation loss: 2.42230461986142

Epoch: 62| Step: 0
Training loss: 2.1810641846305274
Validation loss: 2.4264367897367976

Epoch: 6| Step: 1
Training loss: 2.748366911362796
Validation loss: 2.4446873947508276

Epoch: 6| Step: 2
Training loss: 2.2211797600300582
Validation loss: 2.4328436106132223

Epoch: 6| Step: 3
Training loss: 2.2864861164418104
Validation loss: 2.4668226484863656

Epoch: 6| Step: 4
Training loss: 2.5296298835135342
Validation loss: 2.5189708155412487

Epoch: 6| Step: 5
Training loss: 1.8812446239305904
Validation loss: 2.487980240002305

Epoch: 6| Step: 6
Training loss: 1.3706160797371587
Validation loss: 2.4821339542063283

Epoch: 6| Step: 7
Training loss: 1.8733164540590412
Validation loss: 2.490531889652011

Epoch: 6| Step: 8
Training loss: 1.7300818292746525
Validation loss: 2.465665588560544

Epoch: 6| Step: 9
Training loss: 2.413520036012421
Validation loss: 2.44590719545819

Epoch: 6| Step: 10
Training loss: 2.280572032778595
Validation loss: 2.4067057388529522

Epoch: 6| Step: 11
Training loss: 2.5663116762801845
Validation loss: 2.4159774674465893

Epoch: 6| Step: 12
Training loss: 1.5462204289981651
Validation loss: 2.4012897608766672

Epoch: 6| Step: 13
Training loss: 2.443372645597455
Validation loss: 2.3807414992925535

Epoch: 63| Step: 0
Training loss: 1.8693815612167668
Validation loss: 2.376501763023527

Epoch: 6| Step: 1
Training loss: 2.1039829992325436
Validation loss: 2.38496131104519

Epoch: 6| Step: 2
Training loss: 1.951061165449634
Validation loss: 2.382367869099278

Epoch: 6| Step: 3
Training loss: 2.284935678123483
Validation loss: 2.4142229012508416

Epoch: 6| Step: 4
Training loss: 2.3489769189836602
Validation loss: 2.384825792941283

Epoch: 6| Step: 5
Training loss: 2.3075479633475475
Validation loss: 2.390529406263153

Epoch: 6| Step: 6
Training loss: 1.9119555028581778
Validation loss: 2.3906040315394117

Epoch: 6| Step: 7
Training loss: 2.1187358979512605
Validation loss: 2.3924113395842705

Epoch: 6| Step: 8
Training loss: 2.440150458277768
Validation loss: 2.3816735552596286

Epoch: 6| Step: 9
Training loss: 2.6750515799139643
Validation loss: 2.3874897645394717

Epoch: 6| Step: 10
Training loss: 1.9259799444673558
Validation loss: 2.4159284538428962

Epoch: 6| Step: 11
Training loss: 2.4961708307677446
Validation loss: 2.4039297848850008

Epoch: 6| Step: 12
Training loss: 1.997862508104059
Validation loss: 2.420521938586769

Epoch: 6| Step: 13
Training loss: 1.853963719380269
Validation loss: 2.430702855929756

Epoch: 64| Step: 0
Training loss: 2.1151501752415744
Validation loss: 2.454296812180644

Epoch: 6| Step: 1
Training loss: 2.5374869285209236
Validation loss: 2.527179710728702

Epoch: 6| Step: 2
Training loss: 2.646466357180363
Validation loss: 2.515823210337556

Epoch: 6| Step: 3
Training loss: 2.2728897955611735
Validation loss: 2.5087380211630794

Epoch: 6| Step: 4
Training loss: 2.4794168960420233
Validation loss: 2.5007158526094786

Epoch: 6| Step: 5
Training loss: 2.124766168631811
Validation loss: 2.4789459840876384

Epoch: 6| Step: 6
Training loss: 1.5073303084131522
Validation loss: 2.465728762094817

Epoch: 6| Step: 7
Training loss: 2.1486152714094002
Validation loss: 2.4264995926241504

Epoch: 6| Step: 8
Training loss: 2.477471410468981
Validation loss: 2.4169083562113856

Epoch: 6| Step: 9
Training loss: 1.8712089359769737
Validation loss: 2.429524578752877

Epoch: 6| Step: 10
Training loss: 2.379328347328173
Validation loss: 2.413505638115973

Epoch: 6| Step: 11
Training loss: 1.6072284494041968
Validation loss: 2.408717768476084

Epoch: 6| Step: 12
Training loss: 1.9572720864862734
Validation loss: 2.41601942427513

Epoch: 6| Step: 13
Training loss: 1.8196092155659238
Validation loss: 2.432917893449653

Epoch: 65| Step: 0
Training loss: 1.9337815318776332
Validation loss: 2.4074704266584006

Epoch: 6| Step: 1
Training loss: 2.1187292587480355
Validation loss: 2.4152126843723036

Epoch: 6| Step: 2
Training loss: 1.9860582196440757
Validation loss: 2.4153668892875966

Epoch: 6| Step: 3
Training loss: 2.370078358059042
Validation loss: 2.386336475481967

Epoch: 6| Step: 4
Training loss: 2.356229896358462
Validation loss: 2.3937376448641947

Epoch: 6| Step: 5
Training loss: 1.9535396288407911
Validation loss: 2.4053677349266653

Epoch: 6| Step: 6
Training loss: 1.9860483158146591
Validation loss: 2.377859402034937

Epoch: 6| Step: 7
Training loss: 1.8239207381711682
Validation loss: 2.4099504495278072

Epoch: 6| Step: 8
Training loss: 2.519607803009736
Validation loss: 2.388162961590066

Epoch: 6| Step: 9
Training loss: 2.030412471940955
Validation loss: 2.438939093580179

Epoch: 6| Step: 10
Training loss: 2.756374081264497
Validation loss: 2.449081425160678

Epoch: 6| Step: 11
Training loss: 1.64302613887786
Validation loss: 2.452698332336492

Epoch: 6| Step: 12
Training loss: 2.06530986465376
Validation loss: 2.4456845245279535

Epoch: 6| Step: 13
Training loss: 2.4751111901478224
Validation loss: 2.441471288196203

Epoch: 66| Step: 0
Training loss: 2.060206207028441
Validation loss: 2.429902904321034

Epoch: 6| Step: 1
Training loss: 1.9259059780499286
Validation loss: 2.425600881622908

Epoch: 6| Step: 2
Training loss: 1.806350800232606
Validation loss: 2.3960256140023892

Epoch: 6| Step: 3
Training loss: 1.7401418950332337
Validation loss: 2.3938543577495555

Epoch: 6| Step: 4
Training loss: 1.8345221725250982
Validation loss: 2.3867560434211312

Epoch: 6| Step: 5
Training loss: 2.925123234948612
Validation loss: 2.3958952080297045

Epoch: 6| Step: 6
Training loss: 1.6263068886027472
Validation loss: 2.3879967663688215

Epoch: 6| Step: 7
Training loss: 2.078571242986492
Validation loss: 2.415542140853939

Epoch: 6| Step: 8
Training loss: 2.2245074509204144
Validation loss: 2.4044581921335837

Epoch: 6| Step: 9
Training loss: 2.68267264515392
Validation loss: 2.4328184979935905

Epoch: 6| Step: 10
Training loss: 2.2160498078667015
Validation loss: 2.4346195546992684

Epoch: 6| Step: 11
Training loss: 2.068362487271915
Validation loss: 2.412960224692753

Epoch: 6| Step: 12
Training loss: 2.3599809285379285
Validation loss: 2.39537676455227

Epoch: 6| Step: 13
Training loss: 1.9116818940881797
Validation loss: 2.3862880686565378

Epoch: 67| Step: 0
Training loss: 2.7593886758502144
Validation loss: 2.392365929153185

Epoch: 6| Step: 1
Training loss: 1.674177064434528
Validation loss: 2.3781954533905294

Epoch: 6| Step: 2
Training loss: 2.0266454298805727
Validation loss: 2.3794080583957777

Epoch: 6| Step: 3
Training loss: 2.2014220670117908
Validation loss: 2.374865051250299

Epoch: 6| Step: 4
Training loss: 2.137736132333543
Validation loss: 2.3922501568901335

Epoch: 6| Step: 5
Training loss: 1.7296994395821759
Validation loss: 2.3869030970227985

Epoch: 6| Step: 6
Training loss: 2.047062988966959
Validation loss: 2.3789462382145556

Epoch: 6| Step: 7
Training loss: 1.677232873856198
Validation loss: 2.394937286883194

Epoch: 6| Step: 8
Training loss: 1.8733821246670346
Validation loss: 2.4122132313721507

Epoch: 6| Step: 9
Training loss: 2.1476021547189816
Validation loss: 2.4069324367884284

Epoch: 6| Step: 10
Training loss: 2.2507278536679762
Validation loss: 2.4071491345068634

Epoch: 6| Step: 11
Training loss: 1.915436294496036
Validation loss: 2.4081091696176546

Epoch: 6| Step: 12
Training loss: 2.868987389011263
Validation loss: 2.4121815617888505

Epoch: 6| Step: 13
Training loss: 1.8406666982490254
Validation loss: 2.4264847149553876

Epoch: 68| Step: 0
Training loss: 1.7836494346878893
Validation loss: 2.428900162836551

Epoch: 6| Step: 1
Training loss: 1.6480134536092166
Validation loss: 2.418463151599131

Epoch: 6| Step: 2
Training loss: 2.3852304666357265
Validation loss: 2.4320340288985554

Epoch: 6| Step: 3
Training loss: 2.3616489795066467
Validation loss: 2.4250096533933854

Epoch: 6| Step: 4
Training loss: 2.137144391556197
Validation loss: 2.4430915895163285

Epoch: 6| Step: 5
Training loss: 2.1568883420917886
Validation loss: 2.4417345482392103

Epoch: 6| Step: 6
Training loss: 1.7629541772349824
Validation loss: 2.4338608008948097

Epoch: 6| Step: 7
Training loss: 2.236092394269349
Validation loss: 2.4223483945807813

Epoch: 6| Step: 8
Training loss: 1.8732465810588008
Validation loss: 2.4461036352970917

Epoch: 6| Step: 9
Training loss: 2.4371447181982044
Validation loss: 2.4286069030935664

Epoch: 6| Step: 10
Training loss: 2.4140133158293606
Validation loss: 2.4047606511811943

Epoch: 6| Step: 11
Training loss: 2.0768987723501593
Validation loss: 2.402006519497767

Epoch: 6| Step: 12
Training loss: 1.565944080236187
Validation loss: 2.382460763120504

Epoch: 6| Step: 13
Training loss: 2.25342934255323
Validation loss: 2.3889238542269946

Epoch: 69| Step: 0
Training loss: 2.6641322151973785
Validation loss: 2.402040424167698

Epoch: 6| Step: 1
Training loss: 1.8206316623114054
Validation loss: 2.443983527340294

Epoch: 6| Step: 2
Training loss: 1.8647473083795796
Validation loss: 2.420552317076978

Epoch: 6| Step: 3
Training loss: 2.0611079749226024
Validation loss: 2.4702862494672653

Epoch: 6| Step: 4
Training loss: 1.7634880164409565
Validation loss: 2.4976524616300626

Epoch: 6| Step: 5
Training loss: 1.850490340126467
Validation loss: 2.496340298204575

Epoch: 6| Step: 6
Training loss: 2.3224214673187107
Validation loss: 2.5033770799010258

Epoch: 6| Step: 7
Training loss: 2.2343639026713054
Validation loss: 2.4500852813280467

Epoch: 6| Step: 8
Training loss: 2.253877054484871
Validation loss: 2.4166667187350916

Epoch: 6| Step: 9
Training loss: 1.3487886398196778
Validation loss: 2.4014308280315024

Epoch: 6| Step: 10
Training loss: 1.7712867904212741
Validation loss: 2.3848248931826412

Epoch: 6| Step: 11
Training loss: 2.300699281122899
Validation loss: 2.4110550402446713

Epoch: 6| Step: 12
Training loss: 1.8423223625690675
Validation loss: 2.394764858215818

Epoch: 6| Step: 13
Training loss: 2.7778555912139185
Validation loss: 2.3878243440739437

Epoch: 70| Step: 0
Training loss: 2.086743374146878
Validation loss: 2.4055062420244093

Epoch: 6| Step: 1
Training loss: 1.5430349601350102
Validation loss: 2.3773669360924456

Epoch: 6| Step: 2
Training loss: 1.935399670589569
Validation loss: 2.367357371732544

Epoch: 6| Step: 3
Training loss: 2.069401724230058
Validation loss: 2.3996017728338717

Epoch: 6| Step: 4
Training loss: 1.7795062313587042
Validation loss: 2.4029758686542557

Epoch: 6| Step: 5
Training loss: 2.4417204875894702
Validation loss: 2.402207450825205

Epoch: 6| Step: 6
Training loss: 2.0326840547039176
Validation loss: 2.4058263826902975

Epoch: 6| Step: 7
Training loss: 2.3806308452026683
Validation loss: 2.4135584546787983

Epoch: 6| Step: 8
Training loss: 1.7601900425091295
Validation loss: 2.429411190631275

Epoch: 6| Step: 9
Training loss: 1.862926137421328
Validation loss: 2.461534118992386

Epoch: 6| Step: 10
Training loss: 2.212434295713186
Validation loss: 2.4965597880761727

Epoch: 6| Step: 11
Training loss: 1.5206370118117916
Validation loss: 2.5115911393009847

Epoch: 6| Step: 12
Training loss: 1.9262500370330113
Validation loss: 2.478549549220323

Epoch: 6| Step: 13
Training loss: 2.8928590132019347
Validation loss: 2.4899791472045685

Epoch: 71| Step: 0
Training loss: 2.3309529174016603
Validation loss: 2.4991149209815466

Epoch: 6| Step: 1
Training loss: 2.4304735684569323
Validation loss: 2.455793860599037

Epoch: 6| Step: 2
Training loss: 2.2153354213420444
Validation loss: 2.4292796570512807

Epoch: 6| Step: 3
Training loss: 2.2996303095382253
Validation loss: 2.4170011228659543

Epoch: 6| Step: 4
Training loss: 2.2129634565739753
Validation loss: 2.4009500901196943

Epoch: 6| Step: 5
Training loss: 1.809502721307484
Validation loss: 2.402588556320139

Epoch: 6| Step: 6
Training loss: 1.502910730392342
Validation loss: 2.376915393572712

Epoch: 6| Step: 7
Training loss: 1.796502646966901
Validation loss: 2.4032255875098105

Epoch: 6| Step: 8
Training loss: 1.799918474894313
Validation loss: 2.3906254986531854

Epoch: 6| Step: 9
Training loss: 2.2756246945727487
Validation loss: 2.3720639970657915

Epoch: 6| Step: 10
Training loss: 1.5694173662368467
Validation loss: 2.395997793662043

Epoch: 6| Step: 11
Training loss: 2.03178938526588
Validation loss: 2.41763638629444

Epoch: 6| Step: 12
Training loss: 2.170658017606789
Validation loss: 2.4682384556530264

Epoch: 6| Step: 13
Training loss: 2.4187927321175793
Validation loss: 2.4644735776744167

Epoch: 72| Step: 0
Training loss: 1.392592591883349
Validation loss: 2.5219618475486354

Epoch: 6| Step: 1
Training loss: 2.4377425635518497
Validation loss: 2.5402369203984443

Epoch: 6| Step: 2
Training loss: 2.5393614607648844
Validation loss: 2.559953882303681

Epoch: 6| Step: 3
Training loss: 2.1637991734673863
Validation loss: 2.542096208536552

Epoch: 6| Step: 4
Training loss: 1.5711264706945505
Validation loss: 2.532589295153927

Epoch: 6| Step: 5
Training loss: 2.2425818545096687
Validation loss: 2.4768059226822006

Epoch: 6| Step: 6
Training loss: 1.5161433955594166
Validation loss: 2.460153267047875

Epoch: 6| Step: 7
Training loss: 1.953737025690717
Validation loss: 2.4533423633210574

Epoch: 6| Step: 8
Training loss: 1.0136229516132265
Validation loss: 2.4301730178859198

Epoch: 6| Step: 9
Training loss: 1.8545257420690677
Validation loss: 2.418665968743798

Epoch: 6| Step: 10
Training loss: 2.321221067775191
Validation loss: 2.3935898739715955

Epoch: 6| Step: 11
Training loss: 2.5430972862280474
Validation loss: 2.383301514855535

Epoch: 6| Step: 12
Training loss: 1.9004149009040112
Validation loss: 2.3976868025182894

Epoch: 6| Step: 13
Training loss: 2.441913716790633
Validation loss: 2.4237512601416653

Epoch: 73| Step: 0
Training loss: 1.824738846997348
Validation loss: 2.3913522083848116

Epoch: 6| Step: 1
Training loss: 1.718230429058492
Validation loss: 2.4006273376935474

Epoch: 6| Step: 2
Training loss: 2.283262854089353
Validation loss: 2.3808592581703296

Epoch: 6| Step: 3
Training loss: 1.570885245412328
Validation loss: 2.4017185462450423

Epoch: 6| Step: 4
Training loss: 2.2176587282160356
Validation loss: 2.3852203460487704

Epoch: 6| Step: 5
Training loss: 2.3744285297510026
Validation loss: 2.4084585218880292

Epoch: 6| Step: 6
Training loss: 2.1242161595397846
Validation loss: 2.4485599619465566

Epoch: 6| Step: 7
Training loss: 2.200243316546617
Validation loss: 2.469687834999551

Epoch: 6| Step: 8
Training loss: 1.625201066122379
Validation loss: 2.475252978564382

Epoch: 6| Step: 9
Training loss: 1.748511703321142
Validation loss: 2.4274778304706626

Epoch: 6| Step: 10
Training loss: 1.8348948013543864
Validation loss: 2.40601219719617

Epoch: 6| Step: 11
Training loss: 1.3515884749722438
Validation loss: 2.4499026798200365

Epoch: 6| Step: 12
Training loss: 2.6629326625537137
Validation loss: 2.4455306308294054

Epoch: 6| Step: 13
Training loss: 2.197014525582969
Validation loss: 2.440114648580005

Epoch: 74| Step: 0
Training loss: 1.9628813330586972
Validation loss: 2.420782388314129

Epoch: 6| Step: 1
Training loss: 2.2485506900080967
Validation loss: 2.430398507872503

Epoch: 6| Step: 2
Training loss: 2.2305861139902823
Validation loss: 2.4419162878694425

Epoch: 6| Step: 3
Training loss: 1.8361517598546382
Validation loss: 2.438402237152173

Epoch: 6| Step: 4
Training loss: 1.7576742160233967
Validation loss: 2.423331530262916

Epoch: 6| Step: 5
Training loss: 2.3017186585156284
Validation loss: 2.4234778401903148

Epoch: 6| Step: 6
Training loss: 1.85664693268311
Validation loss: 2.400634902182856

Epoch: 6| Step: 7
Training loss: 2.092726386071344
Validation loss: 2.4224218345946618

Epoch: 6| Step: 8
Training loss: 2.333386511423758
Validation loss: 2.4162855889228254

Epoch: 6| Step: 9
Training loss: 1.7836071279577954
Validation loss: 2.414675654263817

Epoch: 6| Step: 10
Training loss: 2.282693797947769
Validation loss: 2.4118161083688285

Epoch: 6| Step: 11
Training loss: 1.8295964896796828
Validation loss: 2.4151084140271046

Epoch: 6| Step: 12
Training loss: 1.4458268358742365
Validation loss: 2.4088776183490315

Epoch: 6| Step: 13
Training loss: 1.6899482486633726
Validation loss: 2.479289113287974

Epoch: 75| Step: 0
Training loss: 1.4833824453188733
Validation loss: 2.493461705675319

Epoch: 6| Step: 1
Training loss: 2.581421657516506
Validation loss: 2.435334882404508

Epoch: 6| Step: 2
Training loss: 1.7661198960513393
Validation loss: 2.520623069590296

Epoch: 6| Step: 3
Training loss: 2.039213796096173
Validation loss: 2.4723401261787896

Epoch: 6| Step: 4
Training loss: 1.7819587234528873
Validation loss: 2.464685587133574

Epoch: 6| Step: 5
Training loss: 1.9880257013221514
Validation loss: 2.4343617933092183

Epoch: 6| Step: 6
Training loss: 2.2307564838763656
Validation loss: 2.433168377140539

Epoch: 6| Step: 7
Training loss: 2.0973759469231865
Validation loss: 2.3895593981776067

Epoch: 6| Step: 8
Training loss: 1.9319038790293988
Validation loss: 2.4149255619200076

Epoch: 6| Step: 9
Training loss: 2.215370075318721
Validation loss: 2.4193627237717856

Epoch: 6| Step: 10
Training loss: 1.5659495613091927
Validation loss: 2.4088818907645857

Epoch: 6| Step: 11
Training loss: 2.317982540983157
Validation loss: 2.4090808383726796

Epoch: 6| Step: 12
Training loss: 1.6252978858811487
Validation loss: 2.3882855705743755

Epoch: 6| Step: 13
Training loss: 1.6480822428949664
Validation loss: 2.4048709302860813

Testing loss: 2.120786409410299

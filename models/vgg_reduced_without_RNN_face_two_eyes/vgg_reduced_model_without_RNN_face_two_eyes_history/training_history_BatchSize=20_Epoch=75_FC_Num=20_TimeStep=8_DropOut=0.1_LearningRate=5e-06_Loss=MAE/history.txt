Epoch: 1| Step: 0
Training loss: 8.595375061035156
Validation loss: 8.300923804442087

Epoch: 5| Step: 1
Training loss: 9.22889232635498
Validation loss: 8.263324558734894

Epoch: 5| Step: 2
Training loss: 8.217415809631348
Validation loss: 8.231134295463562

Epoch: 5| Step: 3
Training loss: 8.088768005371094
Validation loss: 8.196198125680288

Epoch: 5| Step: 4
Training loss: 8.841872215270996
Validation loss: 8.163220047950745

Epoch: 5| Step: 5
Training loss: 7.415889739990234
Validation loss: 8.131727774937948

Epoch: 5| Step: 6
Training loss: 8.24874496459961
Validation loss: 8.101821064949036

Epoch: 5| Step: 7
Training loss: 7.232701778411865
Validation loss: 8.07317978143692

Epoch: 5| Step: 8
Training loss: 9.207043647766113
Validation loss: 8.044723471005758

Epoch: 5| Step: 9
Training loss: 8.311114311218262
Validation loss: 8.016635239124298

Epoch: 5| Step: 10
Training loss: 7.040820121765137
Validation loss: 7.987022717793782

Epoch: 5| Step: 11
Training loss: 7.004268646240234
Validation loss: 7.955718060334523

Epoch: 2| Step: 0
Training loss: 7.7178754806518555
Validation loss: 7.924576600392659

Epoch: 5| Step: 1
Training loss: 8.203001022338867
Validation loss: 7.892638484636943

Epoch: 5| Step: 2
Training loss: 7.5237579345703125
Validation loss: 7.8595442573229475

Epoch: 5| Step: 3
Training loss: 7.550213813781738
Validation loss: 7.823268711566925

Epoch: 5| Step: 4
Training loss: 7.199179172515869
Validation loss: 7.7881196935971575

Epoch: 5| Step: 5
Training loss: 8.804194450378418
Validation loss: 7.755200962225596

Epoch: 5| Step: 6
Training loss: 7.513017177581787
Validation loss: 7.715577224890391

Epoch: 5| Step: 7
Training loss: 8.192719459533691
Validation loss: 7.676583290100098

Epoch: 5| Step: 8
Training loss: 8.08729362487793
Validation loss: 7.635970830917358

Epoch: 5| Step: 9
Training loss: 7.486161708831787
Validation loss: 7.592606246471405

Epoch: 5| Step: 10
Training loss: 7.444197654724121
Validation loss: 7.549565335114797

Epoch: 5| Step: 11
Training loss: 8.367258071899414
Validation loss: 7.500739514827728

Epoch: 3| Step: 0
Training loss: 7.348755836486816
Validation loss: 7.450576305389404

Epoch: 5| Step: 1
Training loss: 7.544286251068115
Validation loss: 7.401283244291942

Epoch: 5| Step: 2
Training loss: 7.096189975738525
Validation loss: 7.347890893618266

Epoch: 5| Step: 3
Training loss: 7.005239963531494
Validation loss: 7.289938708146413

Epoch: 5| Step: 4
Training loss: 7.47214412689209
Validation loss: 7.235401511192322

Epoch: 5| Step: 5
Training loss: 7.214997291564941
Validation loss: 7.174166162808736

Epoch: 5| Step: 6
Training loss: 7.127490997314453
Validation loss: 7.113608817259471

Epoch: 5| Step: 7
Training loss: 7.0420427322387695
Validation loss: 7.0417472918828325

Epoch: 5| Step: 8
Training loss: 7.282087802886963
Validation loss: 6.971910973389943

Epoch: 5| Step: 9
Training loss: 7.297873497009277
Validation loss: 6.896833757559459

Epoch: 5| Step: 10
Training loss: 7.394967079162598
Validation loss: 6.8193949063618975

Epoch: 5| Step: 11
Training loss: 4.876102447509766
Validation loss: 6.739726344744365

Epoch: 4| Step: 0
Training loss: 6.917569160461426
Validation loss: 6.654454330603282

Epoch: 5| Step: 1
Training loss: 6.635349273681641
Validation loss: 6.573043922583262

Epoch: 5| Step: 2
Training loss: 6.0023369789123535
Validation loss: 6.470671991507213

Epoch: 5| Step: 3
Training loss: 6.255764961242676
Validation loss: 6.372250199317932

Epoch: 5| Step: 4
Training loss: 7.176666259765625
Validation loss: 6.27193162838618

Epoch: 5| Step: 5
Training loss: 5.960411548614502
Validation loss: 6.153930803140004

Epoch: 5| Step: 6
Training loss: 6.32218074798584
Validation loss: 6.038593570391337

Epoch: 5| Step: 7
Training loss: 5.8621110916137695
Validation loss: 5.916441917419434

Epoch: 5| Step: 8
Training loss: 6.1374993324279785
Validation loss: 5.793512304623921

Epoch: 5| Step: 9
Training loss: 5.6999945640563965
Validation loss: 5.669772028923035

Epoch: 5| Step: 10
Training loss: 5.14411735534668
Validation loss: 5.53167720635732

Epoch: 5| Step: 11
Training loss: 5.4018964767456055
Validation loss: 5.390865206718445

Epoch: 5| Step: 0
Training loss: 5.741765022277832
Validation loss: 5.233263850212097

Epoch: 5| Step: 1
Training loss: 4.181031227111816
Validation loss: 5.076237122217814

Epoch: 5| Step: 2
Training loss: 4.233715057373047
Validation loss: 4.919182817141215

Epoch: 5| Step: 3
Training loss: 4.70983362197876
Validation loss: 4.738990167776744

Epoch: 5| Step: 4
Training loss: 4.475463390350342
Validation loss: 4.566290388504664

Epoch: 5| Step: 5
Training loss: 4.2795515060424805
Validation loss: 4.387660066286723

Epoch: 5| Step: 6
Training loss: 5.004918098449707
Validation loss: 4.230865071217219

Epoch: 5| Step: 7
Training loss: 4.668190956115723
Validation loss: 4.034831682840983

Epoch: 5| Step: 8
Training loss: 4.268457889556885
Validation loss: 3.8636119067668915

Epoch: 5| Step: 9
Training loss: 3.8642916679382324
Validation loss: 3.6956006487210593

Epoch: 5| Step: 10
Training loss: 2.8371095657348633
Validation loss: 3.5295803944269815

Epoch: 5| Step: 11
Training loss: 3.0630712509155273
Validation loss: 3.338073174158732

Epoch: 6| Step: 0
Training loss: 3.201455593109131
Validation loss: 3.1685464282830558

Epoch: 5| Step: 1
Training loss: 3.760805130004883
Validation loss: 3.005080630381902

Epoch: 5| Step: 2
Training loss: 2.8745675086975098
Validation loss: 2.8666042586167655

Epoch: 5| Step: 3
Training loss: 3.1617300510406494
Validation loss: 2.707585026820501

Epoch: 5| Step: 4
Training loss: 2.652660608291626
Validation loss: 2.579588601986567

Epoch: 5| Step: 5
Training loss: 2.230992555618286
Validation loss: 2.495549291372299

Epoch: 5| Step: 6
Training loss: 1.9920876026153564
Validation loss: 2.4348649779955545

Epoch: 5| Step: 7
Training loss: 2.0898139476776123
Validation loss: 2.4261079331239066

Epoch: 5| Step: 8
Training loss: 1.906034231185913
Validation loss: 2.383065094550451

Epoch: 5| Step: 9
Training loss: 2.9531338214874268
Validation loss: 2.3484191298484802

Epoch: 5| Step: 10
Training loss: 2.449070930480957
Validation loss: 2.365249196688334

Epoch: 5| Step: 11
Training loss: 1.2418094873428345
Validation loss: 2.3641806493202844

Epoch: 7| Step: 0
Training loss: 1.675464391708374
Validation loss: 2.367973988254865

Epoch: 5| Step: 1
Training loss: 3.4046967029571533
Validation loss: 2.3713067372639975

Epoch: 5| Step: 2
Training loss: 2.820023536682129
Validation loss: 2.417334864536921

Epoch: 5| Step: 3
Training loss: 2.8189430236816406
Validation loss: 2.415989468495051

Epoch: 5| Step: 4
Training loss: 2.0689988136291504
Validation loss: 2.394917219877243

Epoch: 5| Step: 5
Training loss: 2.8791091442108154
Validation loss: 2.416339526573817

Epoch: 5| Step: 6
Training loss: 1.982985258102417
Validation loss: 2.3938951094945273

Epoch: 5| Step: 7
Training loss: 2.4888648986816406
Validation loss: 2.384835143884023

Epoch: 5| Step: 8
Training loss: 2.1921606063842773
Validation loss: 2.3726301789283752

Epoch: 5| Step: 9
Training loss: 2.4013102054595947
Validation loss: 2.3607880224784217

Epoch: 5| Step: 10
Training loss: 2.554276943206787
Validation loss: 2.345654164751371

Epoch: 5| Step: 11
Training loss: 2.607816696166992
Validation loss: 2.3314599096775055

Epoch: 8| Step: 0
Training loss: 1.6055755615234375
Validation loss: 2.3367203970750174

Epoch: 5| Step: 1
Training loss: 2.1063308715820312
Validation loss: 2.3223583698272705

Epoch: 5| Step: 2
Training loss: 2.5486395359039307
Validation loss: 2.313631236553192

Epoch: 5| Step: 3
Training loss: 2.3696439266204834
Validation loss: 2.3105755945046744

Epoch: 5| Step: 4
Training loss: 2.534806966781616
Validation loss: 2.3408015370368958

Epoch: 5| Step: 5
Training loss: 1.821698784828186
Validation loss: 2.3331592877705893

Epoch: 5| Step: 6
Training loss: 2.790766954421997
Validation loss: 2.317911386489868

Epoch: 5| Step: 7
Training loss: 2.9920101165771484
Validation loss: 2.361648519833883

Epoch: 5| Step: 8
Training loss: 2.5190651416778564
Validation loss: 2.345457990964254

Epoch: 5| Step: 9
Training loss: 2.562189817428589
Validation loss: 2.329862987001737

Epoch: 5| Step: 10
Training loss: 2.2103819847106934
Validation loss: 2.3363579163948693

Epoch: 5| Step: 11
Training loss: 3.1663451194763184
Validation loss: 2.3367366989453635

Epoch: 9| Step: 0
Training loss: 2.1776204109191895
Validation loss: 2.333371713757515

Epoch: 5| Step: 1
Training loss: 2.17372727394104
Validation loss: 2.326438009738922

Epoch: 5| Step: 2
Training loss: 2.368551731109619
Validation loss: 2.3108468602101007

Epoch: 5| Step: 3
Training loss: 2.0060908794403076
Validation loss: 2.28868260482947

Epoch: 5| Step: 4
Training loss: 3.1278529167175293
Validation loss: 2.2854017118612924

Epoch: 5| Step: 5
Training loss: 1.93341863155365
Validation loss: 2.3046661416689553

Epoch: 5| Step: 6
Training loss: 2.3752455711364746
Validation loss: 2.2858488659063974

Epoch: 5| Step: 7
Training loss: 2.429090976715088
Validation loss: 2.309116909901301

Epoch: 5| Step: 8
Training loss: 2.389385223388672
Validation loss: 2.298962637782097

Epoch: 5| Step: 9
Training loss: 2.164522171020508
Validation loss: 2.2710568259159722

Epoch: 5| Step: 10
Training loss: 2.3359620571136475
Validation loss: 2.2657550871372223

Epoch: 5| Step: 11
Training loss: 3.6616690158843994
Validation loss: 2.2612602363030114

Epoch: 10| Step: 0
Training loss: 2.5286808013916016
Validation loss: 2.295172795653343

Epoch: 5| Step: 1
Training loss: 2.2413861751556396
Validation loss: 2.263329267501831

Epoch: 5| Step: 2
Training loss: 2.552860736846924
Validation loss: 2.24737748503685

Epoch: 5| Step: 3
Training loss: 2.091778039932251
Validation loss: 2.267542983094851

Epoch: 5| Step: 4
Training loss: 1.9111793041229248
Validation loss: 2.2540389696756997

Epoch: 5| Step: 5
Training loss: 1.8898738622665405
Validation loss: 2.264198730389277

Epoch: 5| Step: 6
Training loss: 2.257288694381714
Validation loss: 2.224678779641787

Epoch: 5| Step: 7
Training loss: 2.382763147354126
Validation loss: 2.299501051505407

Epoch: 5| Step: 8
Training loss: 2.487165927886963
Validation loss: 2.243728538354238

Epoch: 5| Step: 9
Training loss: 2.2094779014587402
Validation loss: 2.228230411807696

Epoch: 5| Step: 10
Training loss: 2.180850028991699
Validation loss: 2.2571112314860025

Epoch: 5| Step: 11
Training loss: 3.3565361499786377
Validation loss: 2.2245745261510215

Epoch: 11| Step: 0
Training loss: 2.359380006790161
Validation loss: 2.233550180991491

Epoch: 5| Step: 1
Training loss: 2.022646188735962
Validation loss: 2.2490194688240686

Epoch: 5| Step: 2
Training loss: 1.9406490325927734
Validation loss: 2.2303500870863595

Epoch: 5| Step: 3
Training loss: 2.1080262660980225
Validation loss: 2.237977534532547

Epoch: 5| Step: 4
Training loss: 2.600285291671753
Validation loss: 2.237873673439026

Epoch: 5| Step: 5
Training loss: 2.1557929515838623
Validation loss: 2.2371020913124084

Epoch: 5| Step: 6
Training loss: 2.6947238445281982
Validation loss: 2.253287211060524

Epoch: 5| Step: 7
Training loss: 3.196488618850708
Validation loss: 2.2708995838960013

Epoch: 5| Step: 8
Training loss: 1.9740829467773438
Validation loss: 2.221507042646408

Epoch: 5| Step: 9
Training loss: 1.772931694984436
Validation loss: 2.2128978619972863

Epoch: 5| Step: 10
Training loss: 1.9547500610351562
Validation loss: 2.2135207752386727

Epoch: 5| Step: 11
Training loss: 1.9747729301452637
Validation loss: 2.2224870969851813

Epoch: 12| Step: 0
Training loss: 2.584414005279541
Validation loss: 2.2339321672916412

Epoch: 5| Step: 1
Training loss: 1.8848356008529663
Validation loss: 2.205553968747457

Epoch: 5| Step: 2
Training loss: 2.157444477081299
Validation loss: 2.248488798737526

Epoch: 5| Step: 3
Training loss: 2.0339560508728027
Validation loss: 2.208043326934179

Epoch: 5| Step: 4
Training loss: 2.125819444656372
Validation loss: 2.232862581809362

Epoch: 5| Step: 5
Training loss: 2.2057156562805176
Validation loss: 2.18828983604908

Epoch: 5| Step: 6
Training loss: 1.8853594064712524
Validation loss: 2.2005407313505807

Epoch: 5| Step: 7
Training loss: 1.6505882740020752
Validation loss: 2.1842561066150665

Epoch: 5| Step: 8
Training loss: 3.14496111869812
Validation loss: 2.237196614344915

Epoch: 5| Step: 9
Training loss: 1.9710776805877686
Validation loss: 2.214148243268331

Epoch: 5| Step: 10
Training loss: 2.2477593421936035
Validation loss: 2.234573930501938

Epoch: 5| Step: 11
Training loss: 3.3192787170410156
Validation loss: 2.187198430299759

Epoch: 13| Step: 0
Training loss: 1.8577781915664673
Validation loss: 2.1930405497550964

Epoch: 5| Step: 1
Training loss: 2.7957499027252197
Validation loss: 2.1756490568319955

Epoch: 5| Step: 2
Training loss: 2.4167802333831787
Validation loss: 2.1787520945072174

Epoch: 5| Step: 3
Training loss: 2.7227730751037598
Validation loss: 2.204727739095688

Epoch: 5| Step: 4
Training loss: 2.644437313079834
Validation loss: 2.183138678471247

Epoch: 5| Step: 5
Training loss: 1.4383167028427124
Validation loss: 2.200924833615621

Epoch: 5| Step: 6
Training loss: 2.0479202270507812
Validation loss: 2.1749594509601593

Epoch: 5| Step: 7
Training loss: 1.926897644996643
Validation loss: 2.1631264140208564

Epoch: 5| Step: 8
Training loss: 2.4703497886657715
Validation loss: 2.2083155115445456

Epoch: 5| Step: 9
Training loss: 1.7776730060577393
Validation loss: 2.21091757218043

Epoch: 5| Step: 10
Training loss: 2.153593063354492
Validation loss: 2.1830032815535865

Epoch: 5| Step: 11
Training loss: 2.0934345722198486
Validation loss: 2.2036519845326743

Epoch: 14| Step: 0
Training loss: 1.722577452659607
Validation loss: 2.1627300331989923

Epoch: 5| Step: 1
Training loss: 2.5860533714294434
Validation loss: 2.172833353281021

Epoch: 5| Step: 2
Training loss: 2.1835086345672607
Validation loss: 2.1530130803585052

Epoch: 5| Step: 3
Training loss: 2.3096885681152344
Validation loss: 2.1979578038056693

Epoch: 5| Step: 4
Training loss: 1.6364017724990845
Validation loss: 2.1869087467590966

Epoch: 5| Step: 5
Training loss: 1.9101192951202393
Validation loss: 2.2076395054658255

Epoch: 5| Step: 6
Training loss: 2.4543404579162598
Validation loss: 2.198900520801544

Epoch: 5| Step: 7
Training loss: 2.456007480621338
Validation loss: 2.23075337211291

Epoch: 5| Step: 8
Training loss: 2.1082680225372314
Validation loss: 2.227645685275396

Epoch: 5| Step: 9
Training loss: 2.699169635772705
Validation loss: 2.1847985287507377

Epoch: 5| Step: 10
Training loss: 1.9778108596801758
Validation loss: 2.2172749042510986

Epoch: 5| Step: 11
Training loss: 3.7937393188476562
Validation loss: 2.18058288594087

Epoch: 15| Step: 0
Training loss: 2.09312105178833
Validation loss: 2.1356977621714273

Epoch: 5| Step: 1
Training loss: 2.1068150997161865
Validation loss: 2.1710122525691986

Epoch: 5| Step: 2
Training loss: 2.7203574180603027
Validation loss: 2.1393634180227914

Epoch: 5| Step: 3
Training loss: 2.492058277130127
Validation loss: 2.1199514120817184

Epoch: 5| Step: 4
Training loss: 2.1168532371520996
Validation loss: 2.1775498588879905

Epoch: 5| Step: 5
Training loss: 1.7281219959259033
Validation loss: 2.163120304544767

Epoch: 5| Step: 6
Training loss: 1.9263269901275635
Validation loss: 2.167234023412069

Epoch: 5| Step: 7
Training loss: 2.6580028533935547
Validation loss: 2.1981781075398126

Epoch: 5| Step: 8
Training loss: 1.955786108970642
Validation loss: 2.133063847819964

Epoch: 5| Step: 9
Training loss: 2.1755518913269043
Validation loss: 2.1572011609872184

Epoch: 5| Step: 10
Training loss: 1.8566802740097046
Validation loss: 2.120119273662567

Epoch: 5| Step: 11
Training loss: 1.2099512815475464
Validation loss: 2.133546104033788

Epoch: 16| Step: 0
Training loss: 2.4315905570983887
Validation loss: 2.1145465672016144

Epoch: 5| Step: 1
Training loss: 1.9692268371582031
Validation loss: 2.137615183989207

Epoch: 5| Step: 2
Training loss: 2.153677463531494
Validation loss: 2.1308016777038574

Epoch: 5| Step: 3
Training loss: 2.0977649688720703
Validation loss: 2.1117301682631173

Epoch: 5| Step: 4
Training loss: 1.9736989736557007
Validation loss: 2.150842090447744

Epoch: 5| Step: 5
Training loss: 1.4435374736785889
Validation loss: 2.1318206191062927

Epoch: 5| Step: 6
Training loss: 1.782788634300232
Validation loss: 2.1314570754766464

Epoch: 5| Step: 7
Training loss: 2.0663418769836426
Validation loss: 2.1339755256970725

Epoch: 5| Step: 8
Training loss: 2.5801122188568115
Validation loss: 2.13299223780632

Epoch: 5| Step: 9
Training loss: 1.9544661045074463
Validation loss: 2.111098741491636

Epoch: 5| Step: 10
Training loss: 2.887284517288208
Validation loss: 2.1385816683371863

Epoch: 5| Step: 11
Training loss: 2.1415863037109375
Validation loss: 2.122932478785515

Epoch: 17| Step: 0
Training loss: 1.9937671422958374
Validation loss: 2.1087293922901154

Epoch: 5| Step: 1
Training loss: 2.4653022289276123
Validation loss: 2.150857518116633

Epoch: 5| Step: 2
Training loss: 2.067615509033203
Validation loss: 2.157016932964325

Epoch: 5| Step: 3
Training loss: 2.0198111534118652
Validation loss: 2.155333643158277

Epoch: 5| Step: 4
Training loss: 2.12758207321167
Validation loss: 2.1567390461762748

Epoch: 5| Step: 5
Training loss: 2.4656169414520264
Validation loss: 2.118521571159363

Epoch: 5| Step: 6
Training loss: 2.520388126373291
Validation loss: 2.1353071133295694

Epoch: 5| Step: 7
Training loss: 1.4092570543289185
Validation loss: 2.1276334325472512

Epoch: 5| Step: 8
Training loss: 2.1328811645507812
Validation loss: 2.0988601793845496

Epoch: 5| Step: 9
Training loss: 2.0447888374328613
Validation loss: 2.1053712914387384

Epoch: 5| Step: 10
Training loss: 2.3496479988098145
Validation loss: 2.117259681224823

Epoch: 5| Step: 11
Training loss: 1.1333867311477661
Validation loss: 2.1124770740667977

Epoch: 18| Step: 0
Training loss: 1.7590806484222412
Validation loss: 2.1675649732351303

Epoch: 5| Step: 1
Training loss: 1.281457543373108
Validation loss: 2.103336989879608

Epoch: 5| Step: 2
Training loss: 2.2115485668182373
Validation loss: 2.1039162625869117

Epoch: 5| Step: 3
Training loss: 2.3170409202575684
Validation loss: 2.144832288225492

Epoch: 5| Step: 4
Training loss: 2.2828783988952637
Validation loss: 2.129568417867025

Epoch: 5| Step: 5
Training loss: 1.7185642719268799
Validation loss: 2.1012671689192453

Epoch: 5| Step: 6
Training loss: 2.7648448944091797
Validation loss: 2.1361531913280487

Epoch: 5| Step: 7
Training loss: 2.3556969165802
Validation loss: 2.093407596151034

Epoch: 5| Step: 8
Training loss: 2.2393898963928223
Validation loss: 2.0749306678771973

Epoch: 5| Step: 9
Training loss: 1.560204029083252
Validation loss: 2.131401280562083

Epoch: 5| Step: 10
Training loss: 2.0256900787353516
Validation loss: 2.132960925499598

Epoch: 5| Step: 11
Training loss: 3.600933074951172
Validation loss: 2.1084394603967667

Epoch: 19| Step: 0
Training loss: 1.6964709758758545
Validation loss: 2.078752184907595

Epoch: 5| Step: 1
Training loss: 2.3104348182678223
Validation loss: 2.129557286699613

Epoch: 5| Step: 2
Training loss: 2.065962553024292
Validation loss: 2.108099897702535

Epoch: 5| Step: 3
Training loss: 2.30726957321167
Validation loss: 2.114311416943868

Epoch: 5| Step: 4
Training loss: 1.7279078960418701
Validation loss: 2.1240341862042746

Epoch: 5| Step: 5
Training loss: 2.0846309661865234
Validation loss: 2.085773468017578

Epoch: 5| Step: 6
Training loss: 2.5047948360443115
Validation loss: 2.1181979974110923

Epoch: 5| Step: 7
Training loss: 2.9145359992980957
Validation loss: 2.117565924922625

Epoch: 5| Step: 8
Training loss: 1.79632568359375
Validation loss: 2.1003496746222177

Epoch: 5| Step: 9
Training loss: 1.6723705530166626
Validation loss: 2.1080866754055023

Epoch: 5| Step: 10
Training loss: 1.7214219570159912
Validation loss: 2.1459110031525293

Epoch: 5| Step: 11
Training loss: 1.1948167085647583
Validation loss: 2.1066888868808746

Epoch: 20| Step: 0
Training loss: 2.288367748260498
Validation loss: 2.1190816462039948

Epoch: 5| Step: 1
Training loss: 1.7034597396850586
Validation loss: 2.174906646211942

Epoch: 5| Step: 2
Training loss: 2.1041810512542725
Validation loss: 2.1513080398241677

Epoch: 5| Step: 3
Training loss: 1.1023279428482056
Validation loss: 2.1155169904232025

Epoch: 5| Step: 4
Training loss: 2.211320400238037
Validation loss: 2.1342959254980087

Epoch: 5| Step: 5
Training loss: 2.3500068187713623
Validation loss: 2.101362700263659

Epoch: 5| Step: 6
Training loss: 2.5625929832458496
Validation loss: 2.075357993443807

Epoch: 5| Step: 7
Training loss: 1.7717348337173462
Validation loss: 2.100852534174919

Epoch: 5| Step: 8
Training loss: 2.5108323097229004
Validation loss: 2.1092421263456345

Epoch: 5| Step: 9
Training loss: 2.5124945640563965
Validation loss: 2.095569928487142

Epoch: 5| Step: 10
Training loss: 1.4992204904556274
Validation loss: 2.078404193123182

Epoch: 5| Step: 11
Training loss: 3.769503116607666
Validation loss: 2.0972063690423965

Epoch: 21| Step: 0
Training loss: 1.4585270881652832
Validation loss: 2.102006216843923

Epoch: 5| Step: 1
Training loss: 2.124014377593994
Validation loss: 2.0939521541198096

Epoch: 5| Step: 2
Training loss: 2.151036024093628
Validation loss: 2.1470861782630286

Epoch: 5| Step: 3
Training loss: 2.380021333694458
Validation loss: 2.1051978021860123

Epoch: 5| Step: 4
Training loss: 2.2123115062713623
Validation loss: 2.1491376807292304

Epoch: 5| Step: 5
Training loss: 1.7884174585342407
Validation loss: 2.1249168813228607

Epoch: 5| Step: 6
Training loss: 2.3109564781188965
Validation loss: 2.100394998987516

Epoch: 5| Step: 7
Training loss: 1.9911552667617798
Validation loss: 2.086405431230863

Epoch: 5| Step: 8
Training loss: 2.5649056434631348
Validation loss: 2.113046114643415

Epoch: 5| Step: 9
Training loss: 2.011958360671997
Validation loss: 2.084273338317871

Epoch: 5| Step: 10
Training loss: 2.011730432510376
Validation loss: 2.1048782616853714

Epoch: 5| Step: 11
Training loss: 1.7003650665283203
Validation loss: 2.111800566315651

Epoch: 22| Step: 0
Training loss: 1.8215806484222412
Validation loss: 2.151793052752813

Epoch: 5| Step: 1
Training loss: 1.8431888818740845
Validation loss: 2.1622389058272042

Epoch: 5| Step: 2
Training loss: 1.7758651971817017
Validation loss: 2.1560309678316116

Epoch: 5| Step: 3
Training loss: 1.7822742462158203
Validation loss: 2.2018774300813675

Epoch: 5| Step: 4
Training loss: 2.313176393508911
Validation loss: 2.186331475774447

Epoch: 5| Step: 5
Training loss: 1.8748903274536133
Validation loss: 2.192973474661509

Epoch: 5| Step: 6
Training loss: 1.74945867061615
Validation loss: 2.1830519636472068

Epoch: 5| Step: 7
Training loss: 3.0066025257110596
Validation loss: 2.154721607764562

Epoch: 5| Step: 8
Training loss: 1.9974409341812134
Validation loss: 2.107063814997673

Epoch: 5| Step: 9
Training loss: 1.8487526178359985
Validation loss: 2.0937121460835137

Epoch: 5| Step: 10
Training loss: 1.9143755435943604
Validation loss: 2.062968904773394

Epoch: 5| Step: 11
Training loss: 3.3488216400146484
Validation loss: 2.0610210647185645

Epoch: 23| Step: 0
Training loss: 1.8695281744003296
Validation loss: 2.0816196650266647

Epoch: 5| Step: 1
Training loss: 1.769915223121643
Validation loss: 2.1260134925444922

Epoch: 5| Step: 2
Training loss: 2.5763933658599854
Validation loss: 2.1787216116984687

Epoch: 5| Step: 3
Training loss: 1.9384562969207764
Validation loss: 2.1213779151439667

Epoch: 5| Step: 4
Training loss: 2.620574951171875
Validation loss: 2.1191555162270865

Epoch: 5| Step: 5
Training loss: 2.321504592895508
Validation loss: 2.1134258011976876

Epoch: 5| Step: 6
Training loss: 2.224841356277466
Validation loss: 2.117633248368899

Epoch: 5| Step: 7
Training loss: 1.7474762201309204
Validation loss: 2.089173674583435

Epoch: 5| Step: 8
Training loss: 1.830071210861206
Validation loss: 2.095327705144882

Epoch: 5| Step: 9
Training loss: 1.6866194009780884
Validation loss: 2.07809949417909

Epoch: 5| Step: 10
Training loss: 2.035203456878662
Validation loss: 2.1077419916788735

Epoch: 5| Step: 11
Training loss: 1.7745778560638428
Validation loss: 2.082181289792061

Epoch: 24| Step: 0
Training loss: 2.0288138389587402
Validation loss: 2.0782629946867623

Epoch: 5| Step: 1
Training loss: 3.1158459186553955
Validation loss: 2.1986780563990274

Epoch: 5| Step: 2
Training loss: 2.5695557594299316
Validation loss: 2.226559872428576

Epoch: 5| Step: 3
Training loss: 1.612007737159729
Validation loss: 2.22321780025959

Epoch: 5| Step: 4
Training loss: 2.404693365097046
Validation loss: 2.2344752748807273

Epoch: 5| Step: 5
Training loss: 2.15787672996521
Validation loss: 2.160156930486361

Epoch: 5| Step: 6
Training loss: 1.5779002904891968
Validation loss: 2.155485818783442

Epoch: 5| Step: 7
Training loss: 1.8028663396835327
Validation loss: 2.1273863216241202

Epoch: 5| Step: 8
Training loss: 1.4648796319961548
Validation loss: 2.0981334447860718

Epoch: 5| Step: 9
Training loss: 2.4777238368988037
Validation loss: 2.0866840134064355

Epoch: 5| Step: 10
Training loss: 2.117814779281616
Validation loss: 2.0564304341872535

Epoch: 5| Step: 11
Training loss: 2.4661731719970703
Validation loss: 2.0613858004411063

Epoch: 25| Step: 0
Training loss: 1.7392288446426392
Validation loss: 2.063497692346573

Epoch: 5| Step: 1
Training loss: 1.799720048904419
Validation loss: 2.074145923058192

Epoch: 5| Step: 2
Training loss: 2.891695022583008
Validation loss: 2.106550171971321

Epoch: 5| Step: 3
Training loss: 2.4785845279693604
Validation loss: 2.057986244559288

Epoch: 5| Step: 4
Training loss: 2.1155524253845215
Validation loss: 2.0605608423550925

Epoch: 5| Step: 5
Training loss: 1.9393993616104126
Validation loss: 2.0725989242394767

Epoch: 5| Step: 6
Training loss: 2.0822978019714355
Validation loss: 2.098435625433922

Epoch: 5| Step: 7
Training loss: 1.6530001163482666
Validation loss: 2.097455312808355

Epoch: 5| Step: 8
Training loss: 2.1660118103027344
Validation loss: 2.0683210442463555

Epoch: 5| Step: 9
Training loss: 1.7690083980560303
Validation loss: 2.1088589876890182

Epoch: 5| Step: 10
Training loss: 1.9163284301757812
Validation loss: 2.082339574893316

Epoch: 5| Step: 11
Training loss: 2.4846291542053223
Validation loss: 2.079462225238482

Epoch: 26| Step: 0
Training loss: 2.1344807147979736
Validation loss: 2.0703684091567993

Epoch: 5| Step: 1
Training loss: 1.7684208154678345
Validation loss: 2.045970598856608

Epoch: 5| Step: 2
Training loss: 1.8738577365875244
Validation loss: 2.048093408346176

Epoch: 5| Step: 3
Training loss: 1.7074191570281982
Validation loss: 2.064239035050074

Epoch: 5| Step: 4
Training loss: 2.308762550354004
Validation loss: 2.1030482252438865

Epoch: 5| Step: 5
Training loss: 2.3608248233795166
Validation loss: 2.0962193508942923

Epoch: 5| Step: 6
Training loss: 2.1608850955963135
Validation loss: 2.0685416758060455

Epoch: 5| Step: 7
Training loss: 2.6077840328216553
Validation loss: 2.107236554225286

Epoch: 5| Step: 8
Training loss: 2.6518445014953613
Validation loss: 2.0719116628170013

Epoch: 5| Step: 9
Training loss: 1.6980438232421875
Validation loss: 2.0845230917135873

Epoch: 5| Step: 10
Training loss: 1.3862850666046143
Validation loss: 2.0559309820334115

Epoch: 5| Step: 11
Training loss: 1.8684700727462769
Validation loss: 2.088085860013962

Epoch: 27| Step: 0
Training loss: 1.8138011693954468
Validation loss: 2.060180827975273

Epoch: 5| Step: 1
Training loss: 1.9573776721954346
Validation loss: 2.099407042066256

Epoch: 5| Step: 2
Training loss: 1.89926016330719
Validation loss: 2.167961517969767

Epoch: 5| Step: 3
Training loss: 2.320235013961792
Validation loss: 2.2003106524546943

Epoch: 5| Step: 4
Training loss: 1.9666404724121094
Validation loss: 2.193010300397873

Epoch: 5| Step: 5
Training loss: 2.8538079261779785
Validation loss: 2.2554945151011148

Epoch: 5| Step: 6
Training loss: 1.9859707355499268
Validation loss: 2.179760863383611

Epoch: 5| Step: 7
Training loss: 2.17464542388916
Validation loss: 2.2001738796631494

Epoch: 5| Step: 8
Training loss: 2.6777069568634033
Validation loss: 2.155948052803675

Epoch: 5| Step: 9
Training loss: 2.0494542121887207
Validation loss: 2.0925555179516473

Epoch: 5| Step: 10
Training loss: 1.7328383922576904
Validation loss: 2.060123791297277

Epoch: 5| Step: 11
Training loss: 1.3347288370132446
Validation loss: 2.0924316197633743

Epoch: 28| Step: 0
Training loss: 2.178694486618042
Validation loss: 2.0450376222531

Epoch: 5| Step: 1
Training loss: 1.6827783584594727
Validation loss: 2.0582102338473

Epoch: 5| Step: 2
Training loss: 1.8055353164672852
Validation loss: 2.0703076670567193

Epoch: 5| Step: 3
Training loss: 2.7581355571746826
Validation loss: 2.0538983047008514

Epoch: 5| Step: 4
Training loss: 1.2688031196594238
Validation loss: 2.0850192457437515

Epoch: 5| Step: 5
Training loss: 2.041626453399658
Validation loss: 2.0762285937865577

Epoch: 5| Step: 6
Training loss: 2.233924388885498
Validation loss: 2.0756930659214654

Epoch: 5| Step: 7
Training loss: 1.6930830478668213
Validation loss: 2.055962493022283

Epoch: 5| Step: 8
Training loss: 2.059873580932617
Validation loss: 2.05699356396993

Epoch: 5| Step: 9
Training loss: 1.8970937728881836
Validation loss: 2.0443677504857383

Epoch: 5| Step: 10
Training loss: 2.602205753326416
Validation loss: 2.047182892759641

Epoch: 5| Step: 11
Training loss: 1.8889617919921875
Validation loss: 2.067867487668991

Epoch: 29| Step: 0
Training loss: 1.8499667644500732
Validation loss: 2.060837914546331

Epoch: 5| Step: 1
Training loss: 2.3349363803863525
Validation loss: 2.0746157119671502

Epoch: 5| Step: 2
Training loss: 1.7114293575286865
Validation loss: 2.0876857240994773

Epoch: 5| Step: 3
Training loss: 2.3321709632873535
Validation loss: 2.0978883653879166

Epoch: 5| Step: 4
Training loss: 2.1889538764953613
Validation loss: 2.125861475865046

Epoch: 5| Step: 5
Training loss: 1.5656235218048096
Validation loss: 2.11165019373099

Epoch: 5| Step: 6
Training loss: 2.233181953430176
Validation loss: 2.0662811597188315

Epoch: 5| Step: 7
Training loss: 1.4189774990081787
Validation loss: 2.073478957017263

Epoch: 5| Step: 8
Training loss: 2.2163314819335938
Validation loss: 2.0762105683485665

Epoch: 5| Step: 9
Training loss: 1.6558853387832642
Validation loss: 2.0485642651716867

Epoch: 5| Step: 10
Training loss: 2.728872776031494
Validation loss: 2.062755544980367

Epoch: 5| Step: 11
Training loss: 1.0090978145599365
Validation loss: 2.077145983775457

Epoch: 30| Step: 0
Training loss: 1.5489416122436523
Validation loss: 2.0473724702994027

Epoch: 5| Step: 1
Training loss: 1.7864086627960205
Validation loss: 2.0517955223719277

Epoch: 5| Step: 2
Training loss: 1.9920686483383179
Validation loss: 2.0536505232254663

Epoch: 5| Step: 3
Training loss: 2.0406813621520996
Validation loss: 2.057909905910492

Epoch: 5| Step: 4
Training loss: 2.333794116973877
Validation loss: 2.0617943902810416

Epoch: 5| Step: 5
Training loss: 2.1802165508270264
Validation loss: 2.057243521014849

Epoch: 5| Step: 6
Training loss: 2.2619659900665283
Validation loss: 2.069946284095446

Epoch: 5| Step: 7
Training loss: 1.8897634744644165
Validation loss: 2.0688037872314453

Epoch: 5| Step: 8
Training loss: 2.0976662635803223
Validation loss: 2.0581081807613373

Epoch: 5| Step: 9
Training loss: 1.8724406957626343
Validation loss: 2.0962469627459845

Epoch: 5| Step: 10
Training loss: 1.9816381931304932
Validation loss: 2.1000607758760452

Epoch: 5| Step: 11
Training loss: 2.670544385910034
Validation loss: 2.091601406534513

Epoch: 31| Step: 0
Training loss: 2.2231173515319824
Validation loss: 2.1098832488059998

Epoch: 5| Step: 1
Training loss: 1.9483566284179688
Validation loss: 2.028852716088295

Epoch: 5| Step: 2
Training loss: 1.6092360019683838
Validation loss: 2.067345922191938

Epoch: 5| Step: 3
Training loss: 2.12440824508667
Validation loss: 2.06878270705541

Epoch: 5| Step: 4
Training loss: 2.0112504959106445
Validation loss: 2.0689966678619385

Epoch: 5| Step: 5
Training loss: 2.372248888015747
Validation loss: 2.0522769391536713

Epoch: 5| Step: 6
Training loss: 1.7567822933197021
Validation loss: 2.075815111398697

Epoch: 5| Step: 7
Training loss: 1.4821432828903198
Validation loss: 2.0247265646855035

Epoch: 5| Step: 8
Training loss: 2.079533338546753
Validation loss: 2.0889728466669717

Epoch: 5| Step: 9
Training loss: 2.4103901386260986
Validation loss: 2.0727892269690833

Epoch: 5| Step: 10
Training loss: 1.9486453533172607
Validation loss: 2.079614579677582

Epoch: 5| Step: 11
Training loss: 2.0526280403137207
Validation loss: 2.0446938971678414

Epoch: 32| Step: 0
Training loss: 2.5529911518096924
Validation loss: 2.059525484840075

Epoch: 5| Step: 1
Training loss: 1.9878199100494385
Validation loss: 2.0618020494778952

Epoch: 5| Step: 2
Training loss: 1.5808919668197632
Validation loss: 2.05171533425649

Epoch: 5| Step: 3
Training loss: 2.221600294113159
Validation loss: 2.0664936751127243

Epoch: 5| Step: 4
Training loss: 2.2734363079071045
Validation loss: 2.0487813701232276

Epoch: 5| Step: 5
Training loss: 2.054277181625366
Validation loss: 2.059727221727371

Epoch: 5| Step: 6
Training loss: 1.6836563348770142
Validation loss: 2.0816976577043533

Epoch: 5| Step: 7
Training loss: 1.7003895044326782
Validation loss: 2.084309071302414

Epoch: 5| Step: 8
Training loss: 1.9578758478164673
Validation loss: 2.0862023880084357

Epoch: 5| Step: 9
Training loss: 2.187608242034912
Validation loss: 2.0694711903731027

Epoch: 5| Step: 10
Training loss: 1.5177494287490845
Validation loss: 2.0883272886276245

Epoch: 5| Step: 11
Training loss: 2.3016116619110107
Validation loss: 2.061359609166781

Epoch: 33| Step: 0
Training loss: 1.8245331048965454
Validation loss: 2.0516287237405777

Epoch: 5| Step: 1
Training loss: 1.9103895425796509
Validation loss: 2.0649219304323196

Epoch: 5| Step: 2
Training loss: 1.9624210596084595
Validation loss: 2.0437056173880896

Epoch: 5| Step: 3
Training loss: 2.3630573749542236
Validation loss: 2.0644069810708365

Epoch: 5| Step: 4
Training loss: 2.1183533668518066
Validation loss: 2.035540853937467

Epoch: 5| Step: 5
Training loss: 1.6062955856323242
Validation loss: 2.0581354002157846

Epoch: 5| Step: 6
Training loss: 1.5584901571273804
Validation loss: 2.0272010614474616

Epoch: 5| Step: 7
Training loss: 1.9909378290176392
Validation loss: 2.0489639242490134

Epoch: 5| Step: 8
Training loss: 1.9557071924209595
Validation loss: 2.0699505607287088

Epoch: 5| Step: 9
Training loss: 2.4272570610046387
Validation loss: 2.0069085210561752

Epoch: 5| Step: 10
Training loss: 2.2688920497894287
Validation loss: 2.0552358577648797

Epoch: 5| Step: 11
Training loss: 0.886815071105957
Validation loss: 2.057148923476537

Epoch: 34| Step: 0
Training loss: 2.3969967365264893
Validation loss: 2.0609910239775977

Epoch: 5| Step: 1
Training loss: 1.9822072982788086
Validation loss: 2.087525968750318

Epoch: 5| Step: 2
Training loss: 2.426417827606201
Validation loss: 2.088451345761617

Epoch: 5| Step: 3
Training loss: 1.689074158668518
Validation loss: 2.0442301581303277

Epoch: 5| Step: 4
Training loss: 2.0262997150421143
Validation loss: 2.0862642427285514

Epoch: 5| Step: 5
Training loss: 1.943650484085083
Validation loss: 2.0276362150907516

Epoch: 5| Step: 6
Training loss: 1.9852817058563232
Validation loss: 2.051621933778127

Epoch: 5| Step: 7
Training loss: 1.8226146697998047
Validation loss: 2.0732235809167228

Epoch: 5| Step: 8
Training loss: 2.1856720447540283
Validation loss: 2.0800850987434387

Epoch: 5| Step: 9
Training loss: 1.6771900653839111
Validation loss: 2.0668583710988364

Epoch: 5| Step: 10
Training loss: 1.968136191368103
Validation loss: 2.084433322151502

Epoch: 5| Step: 11
Training loss: 1.7954570055007935
Validation loss: 2.100742350021998

Epoch: 35| Step: 0
Training loss: 2.139481782913208
Validation loss: 2.0160500556230545

Epoch: 5| Step: 1
Training loss: 1.824631690979004
Validation loss: 2.0471512426932654

Epoch: 5| Step: 2
Training loss: 2.339512586593628
Validation loss: 2.074762021501859

Epoch: 5| Step: 3
Training loss: 2.0763421058654785
Validation loss: 2.056189035375913

Epoch: 5| Step: 4
Training loss: 2.109457492828369
Validation loss: 2.096251646677653

Epoch: 5| Step: 5
Training loss: 1.683189034461975
Validation loss: 2.0807280242443085

Epoch: 5| Step: 6
Training loss: 2.1954739093780518
Validation loss: 2.1087472836176553

Epoch: 5| Step: 7
Training loss: 2.009495735168457
Validation loss: 2.0661175151666007

Epoch: 5| Step: 8
Training loss: 1.6107038259506226
Validation loss: 2.0706232488155365

Epoch: 5| Step: 9
Training loss: 1.9833940267562866
Validation loss: 2.0159753362337747

Epoch: 5| Step: 10
Training loss: 1.8644040822982788
Validation loss: 2.0390905886888504

Epoch: 5| Step: 11
Training loss: 2.1223459243774414
Validation loss: 2.0641364554564157

Epoch: 36| Step: 0
Training loss: 1.8146320581436157
Validation loss: 2.0933233400185904

Epoch: 5| Step: 1
Training loss: 2.0650999546051025
Validation loss: 2.11211089293162

Epoch: 5| Step: 2
Training loss: 2.0523457527160645
Validation loss: 2.1886473149061203

Epoch: 5| Step: 3
Training loss: 2.0068507194519043
Validation loss: 2.161348134279251

Epoch: 5| Step: 4
Training loss: 1.9171499013900757
Validation loss: 2.120033328731855

Epoch: 5| Step: 5
Training loss: 1.7356321811676025
Validation loss: 2.1221891045570374

Epoch: 5| Step: 6
Training loss: 1.952026128768921
Validation loss: 2.11106347044309

Epoch: 5| Step: 7
Training loss: 2.2844982147216797
Validation loss: 2.11344705025355

Epoch: 5| Step: 8
Training loss: 1.6687595844268799
Validation loss: 2.064924786488215

Epoch: 5| Step: 9
Training loss: 1.8462371826171875
Validation loss: 2.1018069138129554

Epoch: 5| Step: 10
Training loss: 2.2768092155456543
Validation loss: 2.030094494422277

Epoch: 5| Step: 11
Training loss: 1.9937993288040161
Validation loss: 2.0903312861919403

Epoch: 37| Step: 0
Training loss: 2.4432790279388428
Validation loss: 2.0295999298493066

Epoch: 5| Step: 1
Training loss: 2.2563438415527344
Validation loss: 2.0487463970979056

Epoch: 5| Step: 2
Training loss: 1.670290231704712
Validation loss: 2.0666176627079644

Epoch: 5| Step: 3
Training loss: 1.4558554887771606
Validation loss: 2.098298435409864

Epoch: 5| Step: 4
Training loss: 2.3371260166168213
Validation loss: 2.1126168618599572

Epoch: 5| Step: 5
Training loss: 1.9544641971588135
Validation loss: 2.0467609067757926

Epoch: 5| Step: 6
Training loss: 2.0963528156280518
Validation loss: 2.0648979345957437

Epoch: 5| Step: 7
Training loss: 1.566720962524414
Validation loss: 2.0667130996783576

Epoch: 5| Step: 8
Training loss: 1.8899027109146118
Validation loss: 2.098646511634191

Epoch: 5| Step: 9
Training loss: 2.0858044624328613
Validation loss: 2.049822380145391

Epoch: 5| Step: 10
Training loss: 2.0091254711151123
Validation loss: 2.066159094373385

Epoch: 5| Step: 11
Training loss: 1.8116310834884644
Validation loss: 2.0769492387771606

Epoch: 38| Step: 0
Training loss: 2.2587761878967285
Validation loss: 2.125209088126818

Epoch: 5| Step: 1
Training loss: 1.8010199069976807
Validation loss: 2.1469036787748337

Epoch: 5| Step: 2
Training loss: 2.16469144821167
Validation loss: 2.164099464813868

Epoch: 5| Step: 3
Training loss: 2.131192684173584
Validation loss: 2.137152006228765

Epoch: 5| Step: 4
Training loss: 2.428870439529419
Validation loss: 2.0810017635424933

Epoch: 5| Step: 5
Training loss: 1.4165147542953491
Validation loss: 2.061215082804362

Epoch: 5| Step: 6
Training loss: 1.9339420795440674
Validation loss: 2.0410438726345697

Epoch: 5| Step: 7
Training loss: 1.8930515050888062
Validation loss: 2.036731789509455

Epoch: 5| Step: 8
Training loss: 2.332000732421875
Validation loss: 2.0514623671770096

Epoch: 5| Step: 9
Training loss: 1.6358747482299805
Validation loss: 2.0560257782538733

Epoch: 5| Step: 10
Training loss: 2.282588481903076
Validation loss: 2.069358845551809

Epoch: 5| Step: 11
Training loss: 1.240514874458313
Validation loss: 1.9852367738882701

Epoch: 39| Step: 0
Training loss: 1.723411202430725
Validation loss: 2.0291672945022583

Epoch: 5| Step: 1
Training loss: 2.553779125213623
Validation loss: 2.046482970317205

Epoch: 5| Step: 2
Training loss: 1.4348725080490112
Validation loss: 2.050833726922671

Epoch: 5| Step: 3
Training loss: 2.3552420139312744
Validation loss: 2.0675700157880783

Epoch: 5| Step: 4
Training loss: 2.021350860595703
Validation loss: 2.0850661446650824

Epoch: 5| Step: 5
Training loss: 1.7013845443725586
Validation loss: 2.053802768389384

Epoch: 5| Step: 6
Training loss: 2.031330108642578
Validation loss: 2.0564540227254233

Epoch: 5| Step: 7
Training loss: 1.262470006942749
Validation loss: 2.0981923838456473

Epoch: 5| Step: 8
Training loss: 1.5507982969284058
Validation loss: 2.0335223376750946

Epoch: 5| Step: 9
Training loss: 2.2535560131073
Validation loss: 2.052500476439794

Epoch: 5| Step: 10
Training loss: 2.2733399868011475
Validation loss: 2.0562554001808167

Epoch: 5| Step: 11
Training loss: 2.199974775314331
Validation loss: 2.0620952198902764

Epoch: 40| Step: 0
Training loss: 2.0092554092407227
Validation loss: 2.0350711246331534

Epoch: 5| Step: 1
Training loss: 1.8159143924713135
Validation loss: 2.0500989804665246

Epoch: 5| Step: 2
Training loss: 1.9546585083007812
Validation loss: 2.01943772037824

Epoch: 5| Step: 3
Training loss: 1.9405359029769897
Validation loss: 2.064756135145823

Epoch: 5| Step: 4
Training loss: 2.266852617263794
Validation loss: 2.0280161102612815

Epoch: 5| Step: 5
Training loss: 2.003727674484253
Validation loss: 2.0455981294314065

Epoch: 5| Step: 6
Training loss: 1.9395153522491455
Validation loss: 2.046243796745936

Epoch: 5| Step: 7
Training loss: 2.066641330718994
Validation loss: 2.061144928137461

Epoch: 5| Step: 8
Training loss: 1.8111984729766846
Validation loss: 2.0559953848520913

Epoch: 5| Step: 9
Training loss: 1.660190224647522
Validation loss: 2.1170402069886527

Epoch: 5| Step: 10
Training loss: 1.9892117977142334
Validation loss: 2.0774250278870263

Epoch: 5| Step: 11
Training loss: 2.5393195152282715
Validation loss: 2.082843621571859

Epoch: 41| Step: 0
Training loss: 1.6915756464004517
Validation loss: 2.099617049098015

Epoch: 5| Step: 1
Training loss: 1.7200157642364502
Validation loss: 2.0386276096105576

Epoch: 5| Step: 2
Training loss: 1.399904489517212
Validation loss: 2.0454373508691788

Epoch: 5| Step: 3
Training loss: 2.1984610557556152
Validation loss: 2.07782052954038

Epoch: 5| Step: 4
Training loss: 2.0026042461395264
Validation loss: 2.0539249181747437

Epoch: 5| Step: 5
Training loss: 1.7265392541885376
Validation loss: 2.0219329049189887

Epoch: 5| Step: 6
Training loss: 2.0150084495544434
Validation loss: 2.019851878285408

Epoch: 5| Step: 7
Training loss: 1.941094160079956
Validation loss: 1.9978391379117966

Epoch: 5| Step: 8
Training loss: 2.2770161628723145
Validation loss: 2.0532306432724

Epoch: 5| Step: 9
Training loss: 1.7924667596817017
Validation loss: 2.0716041127840676

Epoch: 5| Step: 10
Training loss: 2.5306708812713623
Validation loss: 2.0826267153024673

Epoch: 5| Step: 11
Training loss: 2.235049247741699
Validation loss: 2.0678828110297522

Epoch: 42| Step: 0
Training loss: 1.9879274368286133
Validation loss: 2.056290398041407

Epoch: 5| Step: 1
Training loss: 2.4188766479492188
Validation loss: 2.0284734616676965

Epoch: 5| Step: 2
Training loss: 1.932152509689331
Validation loss: 2.067511339982351

Epoch: 5| Step: 3
Training loss: 1.5582994222640991
Validation loss: 2.066439166665077

Epoch: 5| Step: 4
Training loss: 1.8524236679077148
Validation loss: 2.0493311832348504

Epoch: 5| Step: 5
Training loss: 1.5666658878326416
Validation loss: 2.028020203113556

Epoch: 5| Step: 6
Training loss: 1.6783807277679443
Validation loss: 2.0527521669864655

Epoch: 5| Step: 7
Training loss: 2.0709567070007324
Validation loss: 2.0435037165880203

Epoch: 5| Step: 8
Training loss: 1.9171340465545654
Validation loss: 2.087017531196276

Epoch: 5| Step: 9
Training loss: 1.8835499286651611
Validation loss: 2.0611099551121392

Epoch: 5| Step: 10
Training loss: 2.6361212730407715
Validation loss: 2.040117065111796

Epoch: 5| Step: 11
Training loss: 1.2538154125213623
Validation loss: 2.0286471943060556

Epoch: 43| Step: 0
Training loss: 1.5677406787872314
Validation loss: 2.037647614876429

Epoch: 5| Step: 1
Training loss: 2.4410815238952637
Validation loss: 2.002692619959513

Epoch: 5| Step: 2
Training loss: 1.7132288217544556
Validation loss: 2.0027499894301095

Epoch: 5| Step: 3
Training loss: 1.9882707595825195
Validation loss: 2.0664974798758826

Epoch: 5| Step: 4
Training loss: 2.3822145462036133
Validation loss: 2.075055807828903

Epoch: 5| Step: 5
Training loss: 1.8749721050262451
Validation loss: 2.075086866815885

Epoch: 5| Step: 6
Training loss: 2.170452117919922
Validation loss: 2.015068103869756

Epoch: 5| Step: 7
Training loss: 1.6684688329696655
Validation loss: 2.060199255744616

Epoch: 5| Step: 8
Training loss: 1.6401201486587524
Validation loss: 2.0329579363266626

Epoch: 5| Step: 9
Training loss: 2.075387954711914
Validation loss: 2.0246495405832925

Epoch: 5| Step: 10
Training loss: 1.7708715200424194
Validation loss: 2.0346769839525223

Epoch: 5| Step: 11
Training loss: 1.7879831790924072
Validation loss: 2.0522241393725076

Epoch: 44| Step: 0
Training loss: 2.239363193511963
Validation loss: 2.0083079785108566

Epoch: 5| Step: 1
Training loss: 1.608772873878479
Validation loss: 2.0301621605952582

Epoch: 5| Step: 2
Training loss: 2.331099510192871
Validation loss: 2.0220025231440864

Epoch: 5| Step: 3
Training loss: 1.8920913934707642
Validation loss: 2.031942864259084

Epoch: 5| Step: 4
Training loss: 1.6858514547348022
Validation loss: 2.056460460027059

Epoch: 5| Step: 5
Training loss: 2.2081198692321777
Validation loss: 2.013303538163503

Epoch: 5| Step: 6
Training loss: 2.072286367416382
Validation loss: 2.0626154442628226

Epoch: 5| Step: 7
Training loss: 1.6977989673614502
Validation loss: 2.045741851131121

Epoch: 5| Step: 8
Training loss: 1.736912488937378
Validation loss: 2.019308398167292

Epoch: 5| Step: 9
Training loss: 2.447972059249878
Validation loss: 2.0361875543991723

Epoch: 5| Step: 10
Training loss: 1.3758258819580078
Validation loss: 2.0660422643025718

Epoch: 5| Step: 11
Training loss: 1.0121304988861084
Validation loss: 2.0317902863025665

Epoch: 45| Step: 0
Training loss: 2.3357560634613037
Validation loss: 2.065927043557167

Epoch: 5| Step: 1
Training loss: 1.699241042137146
Validation loss: 2.0688225626945496

Epoch: 5| Step: 2
Training loss: 2.258373498916626
Validation loss: 1.987103487054507

Epoch: 5| Step: 3
Training loss: 1.2320542335510254
Validation loss: 2.0660024185975394

Epoch: 5| Step: 4
Training loss: 1.6719707250595093
Validation loss: 2.062316248814265

Epoch: 5| Step: 5
Training loss: 2.3442087173461914
Validation loss: 2.0805803487698236

Epoch: 5| Step: 6
Training loss: 1.56325101852417
Validation loss: 1.98674742380778

Epoch: 5| Step: 7
Training loss: 2.0163369178771973
Validation loss: 1.995768095056216

Epoch: 5| Step: 8
Training loss: 2.1165966987609863
Validation loss: 2.048700918753942

Epoch: 5| Step: 9
Training loss: 1.8528270721435547
Validation loss: 2.045762822031975

Epoch: 5| Step: 10
Training loss: 1.9975101947784424
Validation loss: 2.0170179655154548

Epoch: 5| Step: 11
Training loss: 2.222156524658203
Validation loss: 2.049362992246946

Epoch: 46| Step: 0
Training loss: 1.9663759469985962
Validation loss: 2.0718374252319336

Epoch: 5| Step: 1
Training loss: 1.4706976413726807
Validation loss: 2.029878859718641

Epoch: 5| Step: 2
Training loss: 2.527587652206421
Validation loss: 2.076436400413513

Epoch: 5| Step: 3
Training loss: 2.478084087371826
Validation loss: 1.999419371287028

Epoch: 5| Step: 4
Training loss: 1.7598953247070312
Validation loss: 2.043547034263611

Epoch: 5| Step: 5
Training loss: 1.5738693475723267
Validation loss: 2.041383529702822

Epoch: 5| Step: 6
Training loss: 1.5198967456817627
Validation loss: 2.0441007067759833

Epoch: 5| Step: 7
Training loss: 2.09132981300354
Validation loss: 2.0251791179180145

Epoch: 5| Step: 8
Training loss: 2.16334867477417
Validation loss: 2.065990671515465

Epoch: 5| Step: 9
Training loss: 1.7212388515472412
Validation loss: 2.0086108992497125

Epoch: 5| Step: 10
Training loss: 2.0432169437408447
Validation loss: 2.0077080180247626

Epoch: 5| Step: 11
Training loss: 1.7173371315002441
Validation loss: 2.0868847966194153

Epoch: 47| Step: 0
Training loss: 2.149317741394043
Validation loss: 2.030366614460945

Epoch: 5| Step: 1
Training loss: 1.5153005123138428
Validation loss: 2.048624500632286

Epoch: 5| Step: 2
Training loss: 2.110369920730591
Validation loss: 2.034096583724022

Epoch: 5| Step: 3
Training loss: 2.406136989593506
Validation loss: 2.0353234757979712

Epoch: 5| Step: 4
Training loss: 2.242478609085083
Validation loss: 2.016826162735621

Epoch: 5| Step: 5
Training loss: 1.6023441553115845
Validation loss: 2.073594242334366

Epoch: 5| Step: 6
Training loss: 1.3739688396453857
Validation loss: 2.0307061225175858

Epoch: 5| Step: 7
Training loss: 1.2726348638534546
Validation loss: 2.0662688314914703

Epoch: 5| Step: 8
Training loss: 2.2461676597595215
Validation loss: 2.0754656394322715

Epoch: 5| Step: 9
Training loss: 2.3025739192962646
Validation loss: 2.046892190972964

Epoch: 5| Step: 10
Training loss: 1.3892592191696167
Validation loss: 2.052811940511068

Epoch: 5| Step: 11
Training loss: 3.6740481853485107
Validation loss: 2.039992014567057

Epoch: 48| Step: 0
Training loss: 2.139894962310791
Validation loss: 2.02646833161513

Epoch: 5| Step: 1
Training loss: 2.2930703163146973
Validation loss: 2.066261132558187

Epoch: 5| Step: 2
Training loss: 1.6224877834320068
Validation loss: 2.046376094222069

Epoch: 5| Step: 3
Training loss: 1.6138222217559814
Validation loss: 2.0730259070793786

Epoch: 5| Step: 4
Training loss: 1.9814481735229492
Validation loss: 2.0505900035301843

Epoch: 5| Step: 5
Training loss: 2.107544422149658
Validation loss: 2.051088104645411

Epoch: 5| Step: 6
Training loss: 1.788896918296814
Validation loss: 2.0360952665408454

Epoch: 5| Step: 7
Training loss: 2.3924002647399902
Validation loss: 2.0398659457763038

Epoch: 5| Step: 8
Training loss: 1.7343838214874268
Validation loss: 2.057599941889445

Epoch: 5| Step: 9
Training loss: 2.023867130279541
Validation loss: 2.0153625855843225

Epoch: 5| Step: 10
Training loss: 1.6036043167114258
Validation loss: 2.0277737379074097

Epoch: 5| Step: 11
Training loss: 2.063452959060669
Validation loss: 2.0219617734352746

Epoch: 49| Step: 0
Training loss: 2.158599853515625
Validation loss: 2.061196267604828

Epoch: 5| Step: 1
Training loss: 1.9717934131622314
Validation loss: 2.054748088121414

Epoch: 5| Step: 2
Training loss: 2.0748343467712402
Validation loss: 2.0215092599391937

Epoch: 5| Step: 3
Training loss: 1.5093820095062256
Validation loss: 2.030273492137591

Epoch: 5| Step: 4
Training loss: 2.0471346378326416
Validation loss: 2.0538982301950455

Epoch: 5| Step: 5
Training loss: 1.5614755153656006
Validation loss: 2.0770962486664453

Epoch: 5| Step: 6
Training loss: 2.1664915084838867
Validation loss: 2.018268267313639

Epoch: 5| Step: 7
Training loss: 1.4812037944793701
Validation loss: 2.0494365990161896

Epoch: 5| Step: 8
Training loss: 2.1367530822753906
Validation loss: 2.0288404673337936

Epoch: 5| Step: 9
Training loss: 2.2770113945007324
Validation loss: 2.0117496053377786

Epoch: 5| Step: 10
Training loss: 1.9452117681503296
Validation loss: 2.036261091629664

Epoch: 5| Step: 11
Training loss: 1.8605072498321533
Validation loss: 2.0311065514882407

Epoch: 50| Step: 0
Training loss: 2.5307934284210205
Validation loss: 2.0999379555384317

Epoch: 5| Step: 1
Training loss: 2.4883525371551514
Validation loss: 2.0470612744490304

Epoch: 5| Step: 2
Training loss: 1.713111162185669
Validation loss: 2.0633443842331567

Epoch: 5| Step: 3
Training loss: 1.7689049243927002
Validation loss: 2.054683049519857

Epoch: 5| Step: 4
Training loss: 1.8726335763931274
Validation loss: 2.075630709528923

Epoch: 5| Step: 5
Training loss: 1.481284737586975
Validation loss: 2.037708893418312

Epoch: 5| Step: 6
Training loss: 1.3589887619018555
Validation loss: 2.0260569701592126

Epoch: 5| Step: 7
Training loss: 2.590846538543701
Validation loss: 2.0907188653945923

Epoch: 5| Step: 8
Training loss: 1.8715699911117554
Validation loss: 2.05631094177564

Epoch: 5| Step: 9
Training loss: 1.6631383895874023
Validation loss: 2.0122104038794837

Epoch: 5| Step: 10
Training loss: 1.8683576583862305
Validation loss: 2.035964255531629

Epoch: 5| Step: 11
Training loss: 1.6793829202651978
Validation loss: 2.0131314595540366

Epoch: 51| Step: 0
Training loss: 1.9087231159210205
Validation loss: 2.01523265739282

Epoch: 5| Step: 1
Training loss: 2.0251665115356445
Validation loss: 2.0528953025738397

Epoch: 5| Step: 2
Training loss: 2.3005645275115967
Validation loss: 2.1122183998425803

Epoch: 5| Step: 3
Training loss: 1.3556615114212036
Validation loss: 2.0878183841705322

Epoch: 5| Step: 4
Training loss: 1.7326605319976807
Validation loss: 2.0966296593348184

Epoch: 5| Step: 5
Training loss: 1.6613082885742188
Validation loss: 2.0635653734207153

Epoch: 5| Step: 6
Training loss: 1.3658256530761719
Validation loss: 2.03204815586408

Epoch: 5| Step: 7
Training loss: 2.4395911693573
Validation loss: 2.0107144812742868

Epoch: 5| Step: 8
Training loss: 1.7557471990585327
Validation loss: 2.012250602245331

Epoch: 5| Step: 9
Training loss: 2.2650184631347656
Validation loss: 2.0104895879824958

Epoch: 5| Step: 10
Training loss: 2.0862574577331543
Validation loss: 1.9816113064686458

Epoch: 5| Step: 11
Training loss: 2.0264363288879395
Validation loss: 2.0229323705037436

Epoch: 52| Step: 0
Training loss: 2.2943012714385986
Validation loss: 2.01455590625604

Epoch: 5| Step: 1
Training loss: 2.306868314743042
Validation loss: 2.0414181103308997

Epoch: 5| Step: 2
Training loss: 2.039987087249756
Validation loss: 2.0549820313851037

Epoch: 5| Step: 3
Training loss: 1.7846893072128296
Validation loss: 1.9929419606924057

Epoch: 5| Step: 4
Training loss: 1.7544095516204834
Validation loss: 1.9973229467868805

Epoch: 5| Step: 5
Training loss: 1.9187949895858765
Validation loss: 2.046669269601504

Epoch: 5| Step: 6
Training loss: 1.9498004913330078
Validation loss: 2.0003758122523627

Epoch: 5| Step: 7
Training loss: 1.6783440113067627
Validation loss: 2.0334637065728507

Epoch: 5| Step: 8
Training loss: 1.3064266443252563
Validation loss: 2.0129364977280297

Epoch: 5| Step: 9
Training loss: 2.3315634727478027
Validation loss: 2.061374068260193

Epoch: 5| Step: 10
Training loss: 1.7869236469268799
Validation loss: 2.0326050867637

Epoch: 5| Step: 11
Training loss: 1.6224452257156372
Validation loss: 2.0545338839292526

Epoch: 53| Step: 0
Training loss: 2.032416820526123
Validation loss: 2.046202098329862

Epoch: 5| Step: 1
Training loss: 2.1884231567382812
Validation loss: 2.108860860268275

Epoch: 5| Step: 2
Training loss: 1.8393741846084595
Validation loss: 2.0933000594377518

Epoch: 5| Step: 3
Training loss: 2.491762638092041
Validation loss: 2.124706213672956

Epoch: 5| Step: 4
Training loss: 1.4982097148895264
Validation loss: 2.051386977235476

Epoch: 5| Step: 5
Training loss: 2.3690428733825684
Validation loss: 2.0417601515849433

Epoch: 5| Step: 6
Training loss: 2.1109280586242676
Validation loss: 2.0318920612335205

Epoch: 5| Step: 7
Training loss: 1.824427843093872
Validation loss: 2.0555528849363327

Epoch: 5| Step: 8
Training loss: 1.545417070388794
Validation loss: 1.996130645275116

Epoch: 5| Step: 9
Training loss: 1.4786027669906616
Validation loss: 2.050558879971504

Epoch: 5| Step: 10
Training loss: 2.081317186355591
Validation loss: 2.0443561573823295

Epoch: 5| Step: 11
Training loss: 0.43838995695114136
Validation loss: 2.0116034199794135

Epoch: 54| Step: 0
Training loss: 2.3168554306030273
Validation loss: 2.0143203884363174

Epoch: 5| Step: 1
Training loss: 1.5003459453582764
Validation loss: 2.047549078861872

Epoch: 5| Step: 2
Training loss: 2.836024761199951
Validation loss: 2.0362554291884103

Epoch: 5| Step: 3
Training loss: 1.7769521474838257
Validation loss: 2.0221653282642365

Epoch: 5| Step: 4
Training loss: 2.045043468475342
Validation loss: 2.0506587276856103

Epoch: 5| Step: 5
Training loss: 1.3668205738067627
Validation loss: 2.0476426035165787

Epoch: 5| Step: 6
Training loss: 1.9486280679702759
Validation loss: 2.077094371120135

Epoch: 5| Step: 7
Training loss: 1.7370884418487549
Validation loss: 2.0624892512957254

Epoch: 5| Step: 8
Training loss: 1.3584932088851929
Validation loss: 2.027782971660296

Epoch: 5| Step: 9
Training loss: 2.0356521606445312
Validation loss: 2.0459198405345282

Epoch: 5| Step: 10
Training loss: 1.6785938739776611
Validation loss: 2.0584414501984916

Epoch: 5| Step: 11
Training loss: 2.47371768951416
Validation loss: 2.051792472600937

Epoch: 55| Step: 0
Training loss: 2.023394823074341
Validation loss: 2.0662211179733276

Epoch: 5| Step: 1
Training loss: 1.788109540939331
Validation loss: 1.9945230533679326

Epoch: 5| Step: 2
Training loss: 1.9695003032684326
Validation loss: 2.003987337152163

Epoch: 5| Step: 3
Training loss: 1.8867123126983643
Validation loss: 2.0164500077565513

Epoch: 5| Step: 4
Training loss: 2.0491397380828857
Validation loss: 2.053981120387713

Epoch: 5| Step: 5
Training loss: 1.930932641029358
Validation loss: 1.9967158089081447

Epoch: 5| Step: 6
Training loss: 1.1971896886825562
Validation loss: 2.0114708989858627

Epoch: 5| Step: 7
Training loss: 1.8769588470458984
Validation loss: 2.033915360768636

Epoch: 5| Step: 8
Training loss: 2.1107115745544434
Validation loss: 2.0149480750163398

Epoch: 5| Step: 9
Training loss: 1.7264655828475952
Validation loss: 2.0042376021544137

Epoch: 5| Step: 10
Training loss: 2.0133211612701416
Validation loss: 2.023855904738108

Epoch: 5| Step: 11
Training loss: 1.2936930656433105
Validation loss: 2.011776094635328

Epoch: 56| Step: 0
Training loss: 1.7534881830215454
Validation loss: 2.0635351787010827

Epoch: 5| Step: 1
Training loss: 1.2982099056243896
Validation loss: 2.023441657423973

Epoch: 5| Step: 2
Training loss: 1.5208497047424316
Validation loss: 2.062272588411967

Epoch: 5| Step: 3
Training loss: 1.8982782363891602
Validation loss: 2.081598545114199

Epoch: 5| Step: 4
Training loss: 2.5059828758239746
Validation loss: 2.078716740012169

Epoch: 5| Step: 5
Training loss: 2.327599287033081
Validation loss: 2.0979177256425223

Epoch: 5| Step: 6
Training loss: 1.9940869808197021
Validation loss: 2.0807246367136636

Epoch: 5| Step: 7
Training loss: 2.1661148071289062
Validation loss: 2.0663538674513497

Epoch: 5| Step: 8
Training loss: 1.6133100986480713
Validation loss: 2.0174268086751304

Epoch: 5| Step: 9
Training loss: 1.9642890691757202
Validation loss: 2.0291441082954407

Epoch: 5| Step: 10
Training loss: 1.7093662023544312
Validation loss: 2.0109031200408936

Epoch: 5| Step: 11
Training loss: 1.9800283908843994
Validation loss: 2.0677327811717987

Epoch: 57| Step: 0
Training loss: 1.8003699779510498
Validation loss: 2.005219986041387

Epoch: 5| Step: 1
Training loss: 2.1815409660339355
Validation loss: 2.025012662013372

Epoch: 5| Step: 2
Training loss: 2.4717836380004883
Validation loss: 2.028792773683866

Epoch: 5| Step: 3
Training loss: 1.0834295749664307
Validation loss: 1.9847147166728973

Epoch: 5| Step: 4
Training loss: 2.1732215881347656
Validation loss: 1.9824343025684357

Epoch: 5| Step: 5
Training loss: 2.248683452606201
Validation loss: 2.023490776618322

Epoch: 5| Step: 6
Training loss: 1.7732553482055664
Validation loss: 2.0395323634147644

Epoch: 5| Step: 7
Training loss: 2.1147918701171875
Validation loss: 2.0347346464792886

Epoch: 5| Step: 8
Training loss: 1.718605637550354
Validation loss: 2.034425809979439

Epoch: 5| Step: 9
Training loss: 1.4688310623168945
Validation loss: 2.059624736507734

Epoch: 5| Step: 10
Training loss: 1.7634155750274658
Validation loss: 2.055546576778094

Epoch: 5| Step: 11
Training loss: 0.5382349491119385
Validation loss: 2.05673515299956

Epoch: 58| Step: 0
Training loss: 1.9139763116836548
Validation loss: 1.9892671555280685

Epoch: 5| Step: 1
Training loss: 1.6120529174804688
Validation loss: 2.0072614202896752

Epoch: 5| Step: 2
Training loss: 1.5810807943344116
Validation loss: 2.045869360367457

Epoch: 5| Step: 3
Training loss: 2.372008800506592
Validation loss: 2.0947822084029517

Epoch: 5| Step: 4
Training loss: 1.8882911205291748
Validation loss: 2.088840434948603

Epoch: 5| Step: 5
Training loss: 1.9448373317718506
Validation loss: 2.0520492047071457

Epoch: 5| Step: 6
Training loss: 1.7320492267608643
Validation loss: 2.0833595047394433

Epoch: 5| Step: 7
Training loss: 1.6926895380020142
Validation loss: 2.0722625950972238

Epoch: 5| Step: 8
Training loss: 2.3799126148223877
Validation loss: 2.092684358358383

Epoch: 5| Step: 9
Training loss: 1.9459354877471924
Validation loss: 1.9873486161231995

Epoch: 5| Step: 10
Training loss: 1.0642213821411133
Validation loss: 1.9835137575864792

Epoch: 5| Step: 11
Training loss: 3.6079134941101074
Validation loss: 2.0309852361679077

Epoch: 59| Step: 0
Training loss: 1.8759584426879883
Validation loss: 1.9951418340206146

Epoch: 5| Step: 1
Training loss: 1.8284822702407837
Validation loss: 2.0059631764888763

Epoch: 5| Step: 2
Training loss: 2.0730814933776855
Validation loss: 1.991765707731247

Epoch: 5| Step: 3
Training loss: 2.1938745975494385
Validation loss: 1.9620582511027653

Epoch: 5| Step: 4
Training loss: 1.8114227056503296
Validation loss: 2.02142770588398

Epoch: 5| Step: 5
Training loss: 2.0354390144348145
Validation loss: 1.9967217842737834

Epoch: 5| Step: 6
Training loss: 1.919959306716919
Validation loss: 2.019515117009481

Epoch: 5| Step: 7
Training loss: 1.608138084411621
Validation loss: 2.020161584019661

Epoch: 5| Step: 8
Training loss: 1.7746613025665283
Validation loss: 1.9737198948860168

Epoch: 5| Step: 9
Training loss: 1.8217252492904663
Validation loss: 2.014661192893982

Epoch: 5| Step: 10
Training loss: 2.1760501861572266
Validation loss: 2.025021880865097

Epoch: 5| Step: 11
Training loss: 2.0713753700256348
Validation loss: 2.028392886122068

Epoch: 60| Step: 0
Training loss: 2.219041347503662
Validation loss: 2.060352608561516

Epoch: 5| Step: 1
Training loss: 2.4942755699157715
Validation loss: 2.022955670952797

Epoch: 5| Step: 2
Training loss: 1.4364173412322998
Validation loss: 1.982959896326065

Epoch: 5| Step: 3
Training loss: 2.068767786026001
Validation loss: 2.025920351346334

Epoch: 5| Step: 4
Training loss: 1.2096760272979736
Validation loss: 2.0235505054394403

Epoch: 5| Step: 5
Training loss: 1.9980430603027344
Validation loss: 1.9859027514855068

Epoch: 5| Step: 6
Training loss: 2.057816982269287
Validation loss: 2.0175930013259253

Epoch: 5| Step: 7
Training loss: 2.010737180709839
Validation loss: 2.0182333439588547

Epoch: 5| Step: 8
Training loss: 1.9086875915527344
Validation loss: 1.977882405122121

Epoch: 5| Step: 9
Training loss: 1.02456533908844
Validation loss: 1.9651341140270233

Epoch: 5| Step: 10
Training loss: 1.999468207359314
Validation loss: 2.0213592499494553

Epoch: 5| Step: 11
Training loss: 2.0925092697143555
Validation loss: 2.002492904663086

Epoch: 61| Step: 0
Training loss: 1.4760167598724365
Validation loss: 2.021288906534513

Epoch: 5| Step: 1
Training loss: 1.421325445175171
Validation loss: 2.042335405945778

Epoch: 5| Step: 2
Training loss: 2.08980131149292
Validation loss: 2.0512101501226425

Epoch: 5| Step: 3
Training loss: 1.8646224737167358
Validation loss: 2.003273437420527

Epoch: 5| Step: 4
Training loss: 1.6885830163955688
Validation loss: 2.0831503570079803

Epoch: 5| Step: 5
Training loss: 1.8507556915283203
Validation loss: 2.0574574172496796

Epoch: 5| Step: 6
Training loss: 2.0094029903411865
Validation loss: 2.022854427496592

Epoch: 5| Step: 7
Training loss: 2.224097728729248
Validation loss: 2.0579566856225333

Epoch: 5| Step: 8
Training loss: 1.7000739574432373
Validation loss: 1.9927830298741658

Epoch: 5| Step: 9
Training loss: 2.0757288932800293
Validation loss: 2.006460795799891

Epoch: 5| Step: 10
Training loss: 2.165649890899658
Validation loss: 2.0022760132948556

Epoch: 5| Step: 11
Training loss: 1.587907075881958
Validation loss: 2.0198040902614594

Epoch: 62| Step: 0
Training loss: 1.5505331754684448
Validation loss: 2.022813300291697

Epoch: 5| Step: 1
Training loss: 1.9701225757598877
Validation loss: 2.0347454647223153

Epoch: 5| Step: 2
Training loss: 2.093651294708252
Validation loss: 2.028257966041565

Epoch: 5| Step: 3
Training loss: 1.9074409008026123
Validation loss: 1.9939132829507191

Epoch: 5| Step: 4
Training loss: 1.7922548055648804
Validation loss: 2.013252447048823

Epoch: 5| Step: 5
Training loss: 1.1345102787017822
Validation loss: 1.991580272714297

Epoch: 5| Step: 6
Training loss: 1.523465871810913
Validation loss: 1.9904619405666988

Epoch: 5| Step: 7
Training loss: 1.3786685466766357
Validation loss: 2.013265316685041

Epoch: 5| Step: 8
Training loss: 2.656245708465576
Validation loss: 2.039435401558876

Epoch: 5| Step: 9
Training loss: 2.2743301391601562
Validation loss: 2.088756258289019

Epoch: 5| Step: 10
Training loss: 2.1811397075653076
Validation loss: 2.1236929496129355

Epoch: 5| Step: 11
Training loss: 2.7002756595611572
Validation loss: 2.120421975851059

Epoch: 63| Step: 0
Training loss: 2.0313658714294434
Validation loss: 2.147404298186302

Epoch: 5| Step: 1
Training loss: 2.425006151199341
Validation loss: 2.1678501268227897

Epoch: 5| Step: 2
Training loss: 1.8210535049438477
Validation loss: 2.134347915649414

Epoch: 5| Step: 3
Training loss: 2.326913356781006
Validation loss: 2.113503778974215

Epoch: 5| Step: 4
Training loss: 1.7726157903671265
Validation loss: 2.0534839630126953

Epoch: 5| Step: 5
Training loss: 1.4082458019256592
Validation loss: 1.9825714379549026

Epoch: 5| Step: 6
Training loss: 2.046677827835083
Validation loss: 2.0257238845030465

Epoch: 5| Step: 7
Training loss: 1.6003252267837524
Validation loss: 1.9763999233643215

Epoch: 5| Step: 8
Training loss: 1.5499684810638428
Validation loss: 2.0007840444644294

Epoch: 5| Step: 9
Training loss: 2.020805835723877
Validation loss: 1.9784589111804962

Epoch: 5| Step: 10
Training loss: 1.8111600875854492
Validation loss: 2.0063279469807944

Epoch: 5| Step: 11
Training loss: 1.0872479677200317
Validation loss: 1.9884101649125416

Epoch: 64| Step: 0
Training loss: 1.8466888666152954
Validation loss: 1.9983708361784618

Epoch: 5| Step: 1
Training loss: 1.507478952407837
Validation loss: 1.990515450636546

Epoch: 5| Step: 2
Training loss: 2.2121024131774902
Validation loss: 2.038783719142278

Epoch: 5| Step: 3
Training loss: 1.473488211631775
Validation loss: 1.9469280739625294

Epoch: 5| Step: 4
Training loss: 2.4347124099731445
Validation loss: 2.0155834952990213

Epoch: 5| Step: 5
Training loss: 1.8507277965545654
Validation loss: 1.9960014174381893

Epoch: 5| Step: 6
Training loss: 2.314073085784912
Validation loss: 2.0261436303456626

Epoch: 5| Step: 7
Training loss: 2.049071788787842
Validation loss: 2.0215712239344916

Epoch: 5| Step: 8
Training loss: 1.384608268737793
Validation loss: 2.032528062661489

Epoch: 5| Step: 9
Training loss: 1.4019813537597656
Validation loss: 2.0697535326083503

Epoch: 5| Step: 10
Training loss: 1.7645705938339233
Validation loss: 2.0259568293889365

Epoch: 5| Step: 11
Training loss: 1.1977707147598267
Validation loss: 2.043480470776558

Epoch: 65| Step: 0
Training loss: 2.163510799407959
Validation loss: 2.0252802769343057

Epoch: 5| Step: 1
Training loss: 1.5819171667099
Validation loss: 2.001655454436938

Epoch: 5| Step: 2
Training loss: 1.6479275226593018
Validation loss: 2.0066480338573456

Epoch: 5| Step: 3
Training loss: 1.936989188194275
Validation loss: 2.013530741135279

Epoch: 5| Step: 4
Training loss: 1.885814905166626
Validation loss: 2.028830409049988

Epoch: 5| Step: 5
Training loss: 1.4250071048736572
Validation loss: 1.999297981460889

Epoch: 5| Step: 6
Training loss: 1.7401714324951172
Validation loss: 2.0355719725290933

Epoch: 5| Step: 7
Training loss: 2.118879795074463
Validation loss: 2.0361489951610565

Epoch: 5| Step: 8
Training loss: 1.508406400680542
Validation loss: 2.0730156948169074

Epoch: 5| Step: 9
Training loss: 2.30916166305542
Validation loss: 2.0294557909170785

Epoch: 5| Step: 10
Training loss: 2.0880558490753174
Validation loss: 1.9993082135915756

Epoch: 5| Step: 11
Training loss: 1.0642327070236206
Validation loss: 2.0035569171110788

Epoch: 66| Step: 0
Training loss: 2.1687657833099365
Validation loss: 2.0425134350856147

Epoch: 5| Step: 1
Training loss: 1.4660390615463257
Validation loss: 2.0028703610102334

Epoch: 5| Step: 2
Training loss: 2.100694417953491
Validation loss: 2.005288486679395

Epoch: 5| Step: 3
Training loss: 1.0538322925567627
Validation loss: 1.983575905362765

Epoch: 5| Step: 4
Training loss: 2.1750588417053223
Validation loss: 1.9779686331748962

Epoch: 5| Step: 5
Training loss: 1.372739553451538
Validation loss: 2.025377462307612

Epoch: 5| Step: 6
Training loss: 1.9590699672698975
Validation loss: 1.9904379695653915

Epoch: 5| Step: 7
Training loss: 2.132843017578125
Validation loss: 1.9818638165791829

Epoch: 5| Step: 8
Training loss: 1.9407848119735718
Validation loss: 1.9923562854528427

Epoch: 5| Step: 9
Training loss: 1.5740599632263184
Validation loss: 2.002748812238375

Epoch: 5| Step: 10
Training loss: 2.0050175189971924
Validation loss: 1.996874024470647

Epoch: 5| Step: 11
Training loss: 1.605405330657959
Validation loss: 1.9844649682442348

Epoch: 67| Step: 0
Training loss: 1.6539676189422607
Validation loss: 1.9853117962678273

Epoch: 5| Step: 1
Training loss: 1.698315978050232
Validation loss: 1.9864276200532913

Epoch: 5| Step: 2
Training loss: 1.2164347171783447
Validation loss: 2.0178894946972528

Epoch: 5| Step: 3
Training loss: 1.2190167903900146
Validation loss: 2.0666528741518655

Epoch: 5| Step: 4
Training loss: 1.631431221961975
Validation loss: 2.025415455301603

Epoch: 5| Step: 5
Training loss: 1.991420030593872
Validation loss: 2.043305198351542

Epoch: 5| Step: 6
Training loss: 2.178475856781006
Validation loss: 1.9998855590820312

Epoch: 5| Step: 7
Training loss: 1.6943891048431396
Validation loss: 2.036470502614975

Epoch: 5| Step: 8
Training loss: 1.6171846389770508
Validation loss: 1.9965698719024658

Epoch: 5| Step: 9
Training loss: 2.9061217308044434
Validation loss: 1.9869257658720016

Epoch: 5| Step: 10
Training loss: 1.8036645650863647
Validation loss: 2.0035283168156943

Epoch: 5| Step: 11
Training loss: 2.578932762145996
Validation loss: 1.9598220437765121

Epoch: 68| Step: 0
Training loss: 2.1809611320495605
Validation loss: 2.0070106287797294

Epoch: 5| Step: 1
Training loss: 1.6909294128417969
Validation loss: 1.992377981543541

Epoch: 5| Step: 2
Training loss: 1.6635822057724
Validation loss: 2.0271989156802497

Epoch: 5| Step: 3
Training loss: 1.8598741292953491
Validation loss: 2.028414403398832

Epoch: 5| Step: 4
Training loss: 1.9679601192474365
Validation loss: 1.9585984647274017

Epoch: 5| Step: 5
Training loss: 1.8565162420272827
Validation loss: 2.0318310658137

Epoch: 5| Step: 6
Training loss: 1.5342060327529907
Validation loss: 1.9752315282821655

Epoch: 5| Step: 7
Training loss: 2.0695881843566895
Validation loss: 2.0237381607294083

Epoch: 5| Step: 8
Training loss: 2.0789387226104736
Validation loss: 2.009626790881157

Epoch: 5| Step: 9
Training loss: 1.4276363849639893
Validation loss: 2.0245020389556885

Epoch: 5| Step: 10
Training loss: 1.7229881286621094
Validation loss: 2.052945743004481

Epoch: 5| Step: 11
Training loss: 1.538210391998291
Validation loss: 2.1011760334173837

Epoch: 69| Step: 0
Training loss: 1.4527349472045898
Validation loss: 2.058722863594691

Epoch: 5| Step: 1
Training loss: 2.5276522636413574
Validation loss: 2.039205660422643

Epoch: 5| Step: 2
Training loss: 1.9328428506851196
Validation loss: 2.041982094446818

Epoch: 5| Step: 3
Training loss: 1.5330499410629272
Validation loss: 2.0160649369160333

Epoch: 5| Step: 4
Training loss: 1.472159743309021
Validation loss: 2.040142367283503

Epoch: 5| Step: 5
Training loss: 1.7861368656158447
Validation loss: 2.018622408310572

Epoch: 5| Step: 6
Training loss: 1.942898154258728
Validation loss: 2.0109206487735114

Epoch: 5| Step: 7
Training loss: 1.7202790975570679
Validation loss: 2.0690828611453376

Epoch: 5| Step: 8
Training loss: 1.5887935161590576
Validation loss: 1.992624322573344

Epoch: 5| Step: 9
Training loss: 1.927649736404419
Validation loss: 1.9755818446477253

Epoch: 5| Step: 10
Training loss: 1.8688207864761353
Validation loss: 1.9532484610875447

Epoch: 5| Step: 11
Training loss: 2.18202543258667
Validation loss: 1.9608665456374486

Epoch: 70| Step: 0
Training loss: 2.259903907775879
Validation loss: 1.957988018790881

Epoch: 5| Step: 1
Training loss: 1.8820384740829468
Validation loss: 2.011740560332934

Epoch: 5| Step: 2
Training loss: 1.6045881509780884
Validation loss: 2.0097514738639197

Epoch: 5| Step: 3
Training loss: 1.202425241470337
Validation loss: 2.0715549339850745

Epoch: 5| Step: 4
Training loss: 1.637197494506836
Validation loss: 2.0199978897968927

Epoch: 5| Step: 5
Training loss: 1.2945775985717773
Validation loss: 2.0575449566046395

Epoch: 5| Step: 6
Training loss: 1.9185407161712646
Validation loss: 2.07230610648791

Epoch: 5| Step: 7
Training loss: 2.4540317058563232
Validation loss: 1.9944542050361633

Epoch: 5| Step: 8
Training loss: 2.0138256549835205
Validation loss: 2.0268291334311166

Epoch: 5| Step: 9
Training loss: 1.9385340213775635
Validation loss: 2.0255547016859055

Epoch: 5| Step: 10
Training loss: 1.4874212741851807
Validation loss: 1.9854831198851268

Epoch: 5| Step: 11
Training loss: 3.628161907196045
Validation loss: 1.9914681613445282

Epoch: 71| Step: 0
Training loss: 1.6398833990097046
Validation loss: 2.014604469140371

Epoch: 5| Step: 1
Training loss: 1.862562894821167
Validation loss: 2.0387063274780908

Epoch: 5| Step: 2
Training loss: 2.172900676727295
Validation loss: 2.0035605231920877

Epoch: 5| Step: 3
Training loss: 1.6868358850479126
Validation loss: 2.020840053757032

Epoch: 5| Step: 4
Training loss: 1.5028588771820068
Validation loss: 1.9911896735429764

Epoch: 5| Step: 5
Training loss: 2.1528613567352295
Validation loss: 2.011353815595309

Epoch: 5| Step: 6
Training loss: 1.5762965679168701
Validation loss: 2.054122587045034

Epoch: 5| Step: 7
Training loss: 1.5983980894088745
Validation loss: 2.081229800979296

Epoch: 5| Step: 8
Training loss: 1.4829719066619873
Validation loss: 2.030103882153829

Epoch: 5| Step: 9
Training loss: 1.9394954442977905
Validation loss: 2.0006076445182166

Epoch: 5| Step: 10
Training loss: 2.117793083190918
Validation loss: 2.014104594786962

Epoch: 5| Step: 11
Training loss: 2.337017059326172
Validation loss: 1.9928192148605983

Epoch: 72| Step: 0
Training loss: 1.852029800415039
Validation loss: 2.010811393459638

Epoch: 5| Step: 1
Training loss: 1.9128315448760986
Validation loss: 2.0065313975016275

Epoch: 5| Step: 2
Training loss: 2.2367777824401855
Validation loss: 2.0037002762158713

Epoch: 5| Step: 3
Training loss: 1.7467701435089111
Validation loss: 2.007428859670957

Epoch: 5| Step: 4
Training loss: 1.5208936929702759
Validation loss: 1.9803296873966854

Epoch: 5| Step: 5
Training loss: 2.0943102836608887
Validation loss: 2.0158497790495553

Epoch: 5| Step: 6
Training loss: 1.1660881042480469
Validation loss: 2.0040195087591806

Epoch: 5| Step: 7
Training loss: 2.5073208808898926
Validation loss: 1.938686231772105

Epoch: 5| Step: 8
Training loss: 2.003444194793701
Validation loss: 1.9537117530902226

Epoch: 5| Step: 9
Training loss: 1.494187593460083
Validation loss: 2.04941492776076

Epoch: 5| Step: 10
Training loss: 1.774964690208435
Validation loss: 2.0794735302527747

Epoch: 5| Step: 11
Training loss: 1.990866780281067
Validation loss: 2.0878407061100006

Epoch: 73| Step: 0
Training loss: 2.0795438289642334
Validation loss: 2.0916438500086465

Epoch: 5| Step: 1
Training loss: 1.691846489906311
Validation loss: 2.103673348824183

Epoch: 5| Step: 2
Training loss: 1.9494554996490479
Validation loss: 2.023709386587143

Epoch: 5| Step: 3
Training loss: 1.8114502429962158
Validation loss: 2.063672572374344

Epoch: 5| Step: 4
Training loss: 1.605196237564087
Validation loss: 1.9821891983350117

Epoch: 5| Step: 5
Training loss: 1.5252125263214111
Validation loss: 2.010435218612353

Epoch: 5| Step: 6
Training loss: 1.7508642673492432
Validation loss: 1.9581454594930012

Epoch: 5| Step: 7
Training loss: 1.355944037437439
Validation loss: 1.9974030355612438

Epoch: 5| Step: 8
Training loss: 1.536063551902771
Validation loss: 2.0010351488987603

Epoch: 5| Step: 9
Training loss: 1.9324029684066772
Validation loss: 2.0246451447407403

Epoch: 5| Step: 10
Training loss: 2.5898656845092773
Validation loss: 2.001403272151947

Epoch: 5| Step: 11
Training loss: 1.4536203145980835
Validation loss: 1.9747296075026195

Epoch: 74| Step: 0
Training loss: 1.4424606561660767
Validation loss: 1.927957832813263

Epoch: 5| Step: 1
Training loss: 1.6443634033203125
Validation loss: 1.9831291337807972

Epoch: 5| Step: 2
Training loss: 1.366998553276062
Validation loss: 1.9586529980103176

Epoch: 5| Step: 3
Training loss: 1.1454951763153076
Validation loss: 1.9743200490872066

Epoch: 5| Step: 4
Training loss: 1.9721237421035767
Validation loss: 2.010290573040644

Epoch: 5| Step: 5
Training loss: 1.650870680809021
Validation loss: 1.9821408937374752

Epoch: 5| Step: 6
Training loss: 1.9400125741958618
Validation loss: 2.026914651195208

Epoch: 5| Step: 7
Training loss: 2.2843496799468994
Validation loss: 2.086734061439832

Epoch: 5| Step: 8
Training loss: 2.8020567893981934
Validation loss: 2.0252723495165506

Epoch: 5| Step: 9
Training loss: 1.5869897603988647
Validation loss: 2.033225173751513

Epoch: 5| Step: 10
Training loss: 1.6465425491333008
Validation loss: 2.0791046023368835

Epoch: 5| Step: 11
Training loss: 2.490957736968994
Validation loss: 2.0245341012875238

Epoch: 75| Step: 0
Training loss: 1.7147433757781982
Validation loss: 2.032371903459231

Epoch: 5| Step: 1
Training loss: 2.0347704887390137
Validation loss: 2.053582708040873

Epoch: 5| Step: 2
Training loss: 1.6423126459121704
Validation loss: 2.022817095120748

Epoch: 5| Step: 3
Training loss: 1.2822054624557495
Validation loss: 2.0270677655935287

Epoch: 5| Step: 4
Training loss: 1.3821429014205933
Validation loss: 2.0056058764457703

Epoch: 5| Step: 5
Training loss: 1.6816120147705078
Validation loss: 2.02101502319177

Epoch: 5| Step: 6
Training loss: 1.6023584604263306
Validation loss: 2.0015396922826767

Epoch: 5| Step: 7
Training loss: 1.5584776401519775
Validation loss: 2.002619614203771

Epoch: 5| Step: 8
Training loss: 1.9581849575042725
Validation loss: 1.9860461056232452

Epoch: 5| Step: 9
Training loss: 1.948617935180664
Validation loss: 1.9790082921584446

Epoch: 5| Step: 10
Training loss: 2.2970051765441895
Validation loss: 2.0033678313096366

Epoch: 5| Step: 11
Training loss: 3.0472769737243652
Validation loss: 2.0325618584950766

Testing loss: 1.8830233229150017

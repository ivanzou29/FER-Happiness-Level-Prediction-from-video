Epoch: 1| Step: 0
Training loss: 7.037493972369199
Validation loss: 6.904577983908999
Epoch: 5| Step: 1
Training loss: 7.463902388642787
Validation loss: 6.8961284286603
Epoch: 5| Step: 2
Training loss: 7.221903058686298
Validation loss: 6.8871484359260124
Epoch: 5| Step: 3
Training loss: 7.095109582421703
Validation loss: 6.869650095655886
Epoch: 5| Step: 4
Training loss: 7.5605597410927725
Validation loss: 6.876232576790265
Epoch: 5| Step: 5
Training loss: 6.401171946512479
Validation loss: 6.861099703694235
Epoch: 5| Step: 6
Training loss: 7.1384335416781095
Validation loss: 6.867961241907753
Epoch: 5| Step: 7
Training loss: 7.90713676937793
Validation loss: 6.861702603251032
Epoch: 5| Step: 8
Training loss: 7.0662572538189155
Validation loss: 6.855748045703137
Epoch: 5| Step: 9
Training loss: 6.945305909023869
Validation loss: 6.850009943365206
Epoch: 2| Step: 0
Training loss: 7.802830441758428
Validation loss: 6.827993297133815
Epoch: 5| Step: 1
Training loss: 6.784288701194278
Validation loss: 6.830939889538712
Epoch: 5| Step: 2
Training loss: 6.868195651787336
Validation loss: 6.828286961345906
Epoch: 5| Step: 3
Training loss: 7.108763012378603
Validation loss: 6.822661492047667
Epoch: 5| Step: 4
Training loss: 6.6895919807668855
Validation loss: 6.818725625773048
Epoch: 5| Step: 5
Training loss: 6.921710492454696
Validation loss: 6.802386978189261
Epoch: 5| Step: 6
Training loss: 6.926566366155446
Validation loss: 6.802783119788784
Epoch: 5| Step: 7
Training loss: 6.921402133608275
Validation loss: 6.7958089086803675
Epoch: 5| Step: 8
Training loss: 7.698853335522425
Validation loss: 6.799595407619487
Epoch: 5| Step: 9
Training loss: 7.556348079679068
Validation loss: 6.769867221648884
Epoch: 3| Step: 0
Training loss: 7.143810404791646
Validation loss: 6.767069896970846
Epoch: 5| Step: 1
Training loss: 7.036771378140877
Validation loss: 6.776490817571124
Epoch: 5| Step: 2
Training loss: 6.969792172632373
Validation loss: 6.774703175774164
Epoch: 5| Step: 3
Training loss: 7.649911927358242
Validation loss: 6.765368663770785
Epoch: 5| Step: 4
Training loss: 6.6999851511321
Validation loss: 6.763661897244363
Epoch: 5| Step: 5
Training loss: 7.102322007996641
Validation loss: 6.763155224701237
Epoch: 5| Step: 6
Training loss: 6.721504250148595
Validation loss: 6.756640211278572
Epoch: 5| Step: 7
Training loss: 6.874460095099912
Validation loss: 6.740196565744751
Epoch: 5| Step: 8
Training loss: 7.243691692496133
Validation loss: 6.732339414356469
Epoch: 5| Step: 9
Training loss: 7.353174014724547
Validation loss: 6.7303146607313895
Epoch: 4| Step: 0
Training loss: 7.1751212292574555
Validation loss: 6.718402243950213
Epoch: 5| Step: 1
Training loss: 7.601711291339916
Validation loss: 6.72802775356573
Epoch: 5| Step: 2
Training loss: 7.374302717964043
Validation loss: 6.725195088709654
Epoch: 5| Step: 3
Training loss: 6.712851982904492
Validation loss: 6.714842540033782
Epoch: 5| Step: 4
Training loss: 6.010714818301824
Validation loss: 6.701937436321577
Epoch: 5| Step: 5
Training loss: 6.201843898133554
Validation loss: 6.702463439979119
Epoch: 5| Step: 6
Training loss: 6.836167406848162
Validation loss: 6.695267911072414
Epoch: 5| Step: 7
Training loss: 7.149343297717843
Validation loss: 6.687243009668921
Epoch: 5| Step: 8
Training loss: 7.545534887648632
Validation loss: 6.69064571701155
Epoch: 5| Step: 9
Training loss: 7.528214852132162
Validation loss: 6.683434958109813
Epoch: 5| Step: 0
Training loss: 7.072563091386363
Validation loss: 6.680260228808419
Epoch: 5| Step: 1
Training loss: 7.368960736523555
Validation loss: 6.670167449613361
Epoch: 5| Step: 2
Training loss: 7.143973802747035
Validation loss: 6.667005586363042
Epoch: 5| Step: 3
Training loss: 7.375153879402995
Validation loss: 6.655234838613396
Epoch: 5| Step: 4
Training loss: 7.345715328775425
Validation loss: 6.655425149863747
Epoch: 5| Step: 5
Training loss: 6.794475544050561
Validation loss: 6.647219131930231
Epoch: 5| Step: 6
Training loss: 6.687891012864307
Validation loss: 6.644247861144228
Epoch: 5| Step: 7
Training loss: 6.804942006936829
Validation loss: 6.636596176352989
Epoch: 5| Step: 8
Training loss: 6.309295747513334
Validation loss: 6.623237554876851
Epoch: 5| Step: 9
Training loss: 6.842490720509156
Validation loss: 6.620435646344816
Epoch: 6| Step: 0
Training loss: 7.1563272930119
Validation loss: 6.614747646089803
Epoch: 5| Step: 1
Training loss: 7.3031517331928075
Validation loss: 6.615186908540947
Epoch: 5| Step: 2
Training loss: 6.863918181529728
Validation loss: 6.597823935328518
Epoch: 5| Step: 3
Training loss: 6.606281828284845
Validation loss: 6.604134609795076
Epoch: 5| Step: 4
Training loss: 6.645484492634506
Validation loss: 6.598491430639711
Epoch: 5| Step: 5
Training loss: 7.355479381713638
Validation loss: 6.582403416974256
Epoch: 5| Step: 6
Training loss: 6.911598309553801
Validation loss: 6.582043651253046
Epoch: 5| Step: 7
Training loss: 6.669325044216255
Validation loss: 6.568344586826196
Epoch: 5| Step: 8
Training loss: 7.188651614602895
Validation loss: 6.574952580983696
Epoch: 5| Step: 9
Training loss: 6.50802615617538
Validation loss: 6.567532314514977
Epoch: 7| Step: 0
Training loss: 6.915015824107437
Validation loss: 6.561789796513977
Epoch: 5| Step: 1
Training loss: 7.424149014292869
Validation loss: 6.550915909056349
Epoch: 5| Step: 2
Training loss: 7.91946428625692
Validation loss: 6.541930549137173
Epoch: 5| Step: 3
Training loss: 6.509574880475046
Validation loss: 6.534632989774339
Epoch: 5| Step: 4
Training loss: 6.420678243016216
Validation loss: 6.533016916382528
Epoch: 5| Step: 5
Training loss: 6.354467499495803
Validation loss: 6.504925503891733
Epoch: 5| Step: 6
Training loss: 6.731776751891338
Validation loss: 6.515454651813565
Epoch: 5| Step: 7
Training loss: 7.404614489280538
Validation loss: 6.506492396229276
Epoch: 5| Step: 8
Training loss: 6.395715304433985
Validation loss: 6.5052836895439805
Epoch: 5| Step: 9
Training loss: 6.376276524213204
Validation loss: 6.4982761691810005
Epoch: 8| Step: 0
Training loss: 7.177190402431435
Validation loss: 6.4841548503318345
Epoch: 5| Step: 1
Training loss: 6.286379110178969
Validation loss: 6.46953330198735
Epoch: 5| Step: 2
Training loss: 6.547492897172581
Validation loss: 6.474666500726871
Epoch: 5| Step: 3
Training loss: 6.8266722925480865
Validation loss: 6.452526350360388
Epoch: 5| Step: 4
Training loss: 7.048936089935842
Validation loss: 6.454410252468503
Epoch: 5| Step: 5
Training loss: 6.508404213702123
Validation loss: 6.454372041562122
Epoch: 5| Step: 6
Training loss: 7.333350557249245
Validation loss: 6.443119413204694
Epoch: 5| Step: 7
Training loss: 7.141511223554321
Validation loss: 6.434321596517174
Epoch: 5| Step: 8
Training loss: 6.771587185097087
Validation loss: 6.415128286395216
Epoch: 5| Step: 9
Training loss: 6.192268152744056
Validation loss: 6.412206682038885
Epoch: 9| Step: 0
Training loss: 7.125792877519019
Validation loss: 6.413093029925841
Epoch: 5| Step: 1
Training loss: 6.696995918581412
Validation loss: 6.393788835404614
Epoch: 5| Step: 2
Training loss: 7.009943031155733
Validation loss: 6.384794696296762
Epoch: 5| Step: 3
Training loss: 5.781672941666152
Validation loss: 6.382622263722387
Epoch: 5| Step: 4
Training loss: 7.020676457343777
Validation loss: 6.371447200468133
Epoch: 5| Step: 5
Training loss: 7.083851634491803
Validation loss: 6.3704994477144545
Epoch: 5| Step: 6
Training loss: 7.174993896481779
Validation loss: 6.354335123992267
Epoch: 5| Step: 7
Training loss: 6.330227876873354
Validation loss: 6.3499253190259255
Epoch: 5| Step: 8
Training loss: 6.235729557968112
Validation loss: 6.331035190238868
Epoch: 5| Step: 9
Training loss: 6.526253719390879
Validation loss: 6.309272595852633
Epoch: 10| Step: 0
Training loss: 6.634126530799677
Validation loss: 6.310975004294365
Epoch: 5| Step: 1
Training loss: 6.310724594178109
Validation loss: 6.304437801425307
Epoch: 5| Step: 2
Training loss: 6.790842891731172
Validation loss: 6.294039623773616
Epoch: 5| Step: 3
Training loss: 7.141279395103218
Validation loss: 6.285437687906499
Epoch: 5| Step: 4
Training loss: 6.866290314863498
Validation loss: 6.2783095518376735
Epoch: 5| Step: 5
Training loss: 5.612242919199636
Validation loss: 6.257032637016203
Epoch: 5| Step: 6
Training loss: 6.674772040085667
Validation loss: 6.256407772781761
Epoch: 5| Step: 7
Training loss: 6.430214917045505
Validation loss: 6.237600029603918
Epoch: 5| Step: 8
Training loss: 6.536014844786503
Validation loss: 6.237156986664312
Epoch: 5| Step: 9
Training loss: 7.102732613395536
Validation loss: 6.2147834028099105
Epoch: 11| Step: 0
Training loss: 6.913001165450892
Validation loss: 6.214597176123237
Epoch: 5| Step: 1
Training loss: 6.5194604502971165
Validation loss: 6.208784380941933
Epoch: 5| Step: 2
Training loss: 6.544930909144019
Validation loss: 6.193054109070057
Epoch: 5| Step: 3
Training loss: 6.044875807578906
Validation loss: 6.168319961845495
Epoch: 5| Step: 4
Training loss: 6.957789812922675
Validation loss: 6.170120755894938
Epoch: 5| Step: 5
Training loss: 6.608842255605187
Validation loss: 6.149198181705872
Epoch: 5| Step: 6
Training loss: 6.776456624216141
Validation loss: 6.152741609114402
Epoch: 5| Step: 7
Training loss: 6.346752339487422
Validation loss: 6.1389579970901424
Epoch: 5| Step: 8
Training loss: 6.4041804250429974
Validation loss: 6.129274369883755
Epoch: 5| Step: 9
Training loss: 6.04758246174385
Validation loss: 6.113007253948616
Epoch: 12| Step: 0
Training loss: 5.783028653755422
Validation loss: 6.108161130562937
Epoch: 5| Step: 1
Training loss: 6.047504559846746
Validation loss: 6.088862136551765
Epoch: 5| Step: 2
Training loss: 7.071589199735407
Validation loss: 6.0753786948676085
Epoch: 5| Step: 3
Training loss: 5.992374979242012
Validation loss: 6.076632881190225
Epoch: 5| Step: 4
Training loss: 6.60108749215624
Validation loss: 6.061266617919341
Epoch: 5| Step: 5
Training loss: 6.529603887580849
Validation loss: 6.048599262424936
Epoch: 5| Step: 6
Training loss: 7.232430050376827
Validation loss: 6.024220474697203
Epoch: 5| Step: 7
Training loss: 5.869115926528731
Validation loss: 6.017949423403183
Epoch: 5| Step: 8
Training loss: 6.651618313168761
Validation loss: 6.00039668887711
Epoch: 5| Step: 9
Training loss: 6.156769551462395
Validation loss: 6.00158629953158
Epoch: 13| Step: 0
Training loss: 6.072706302862037
Validation loss: 5.9765719167660905
Epoch: 5| Step: 1
Training loss: 6.746674460204276
Validation loss: 5.966190930947377
Epoch: 5| Step: 2
Training loss: 6.068707935254255
Validation loss: 5.955881485077029
Epoch: 5| Step: 3
Training loss: 6.052015854617452
Validation loss: 5.925023299616139
Epoch: 5| Step: 4
Training loss: 6.424467357124161
Validation loss: 5.925308264650207
Epoch: 5| Step: 5
Training loss: 6.090142850155711
Validation loss: 5.916878825313468
Epoch: 5| Step: 6
Training loss: 7.03521860657401
Validation loss: 5.898599888052134
Epoch: 5| Step: 7
Training loss: 6.18213582880235
Validation loss: 5.875135026033123
Epoch: 5| Step: 8
Training loss: 6.100939143694075
Validation loss: 5.879629386415789
Epoch: 5| Step: 9
Training loss: 6.007474376355538
Validation loss: 5.845699992476405
Epoch: 14| Step: 0
Training loss: 5.70405332769903
Validation loss: 5.839182703980652
Epoch: 5| Step: 1
Training loss: 5.897816929793049
Validation loss: 5.820241458152899
Epoch: 5| Step: 2
Training loss: 6.4396358761412245
Validation loss: 5.809367395172562
Epoch: 5| Step: 3
Training loss: 6.255304145767389
Validation loss: 5.774904146910336
Epoch: 5| Step: 4
Training loss: 6.488925008513604
Validation loss: 5.784909833423443
Epoch: 5| Step: 5
Training loss: 6.133573487320849
Validation loss: 5.773315338715057
Epoch: 5| Step: 6
Training loss: 6.232455232890203
Validation loss: 5.747627547856617
Epoch: 5| Step: 7
Training loss: 6.302907170024426
Validation loss: 5.741905925169762
Epoch: 5| Step: 8
Training loss: 5.884537118551414
Validation loss: 5.720760409328921
Epoch: 5| Step: 9
Training loss: 6.109868256542418
Validation loss: 5.699948826794475
Epoch: 15| Step: 0
Training loss: 6.498751006804115
Validation loss: 5.699114129507665
Epoch: 5| Step: 1
Training loss: 6.2129366746171435
Validation loss: 5.664874317499518
Epoch: 5| Step: 2
Training loss: 5.578015922099655
Validation loss: 5.6567612965433325
Epoch: 5| Step: 3
Training loss: 6.079657746268912
Validation loss: 5.640695054439571
Epoch: 5| Step: 4
Training loss: 5.886634507111215
Validation loss: 5.622031725898851
Epoch: 5| Step: 5
Training loss: 6.186072599126215
Validation loss: 5.601390920914635
Epoch: 5| Step: 6
Training loss: 5.953226243807717
Validation loss: 5.587580768825417
Epoch: 5| Step: 7
Training loss: 6.309229239470651
Validation loss: 5.558832110166799
Epoch: 5| Step: 8
Training loss: 5.360509457645589
Validation loss: 5.551541694693873
Epoch: 5| Step: 9
Training loss: 5.852873020074101
Validation loss: 5.541008073452227
Epoch: 16| Step: 0
Training loss: 6.126577991484495
Validation loss: 5.520233804983286
Epoch: 5| Step: 1
Training loss: 6.294947932772626
Validation loss: 5.5023223595997175
Epoch: 5| Step: 2
Training loss: 5.403416591209148
Validation loss: 5.471710205262278
Epoch: 5| Step: 3
Training loss: 6.272879569776852
Validation loss: 5.4620504573424515
Epoch: 5| Step: 4
Training loss: 6.556633352412558
Validation loss: 5.42670943449045
Epoch: 5| Step: 5
Training loss: 5.404672205016393
Validation loss: 5.408381800150125
Epoch: 5| Step: 6
Training loss: 4.9386228419419895
Validation loss: 5.389352841664509
Epoch: 5| Step: 7
Training loss: 5.697711873485857
Validation loss: 5.353616723333572
Epoch: 5| Step: 8
Training loss: 5.3638096915101565
Validation loss: 5.3647940206235045
Epoch: 5| Step: 9
Training loss: 6.0184763306446
Validation loss: 5.3474702141249395
Epoch: 17| Step: 0
Training loss: 5.948138534051108
Validation loss: 5.313968436413809
Epoch: 5| Step: 1
Training loss: 5.570144030148732
Validation loss: 5.290148328755173
Epoch: 5| Step: 2
Training loss: 5.598453015139069
Validation loss: 5.286979625127313
Epoch: 5| Step: 3
Training loss: 5.492901295798582
Validation loss: 5.262652401084532
Epoch: 5| Step: 4
Training loss: 6.172154364721699
Validation loss: 5.232496546049006
Epoch: 5| Step: 5
Training loss: 5.740429418364672
Validation loss: 5.212279618770785
Epoch: 5| Step: 6
Training loss: 5.132677050331818
Validation loss: 5.187738999861597
Epoch: 5| Step: 7
Training loss: 5.866849724485643
Validation loss: 5.175707459721799
Epoch: 5| Step: 8
Training loss: 5.742391777297558
Validation loss: 5.153222265602588
Epoch: 5| Step: 9
Training loss: 4.998254471313937
Validation loss: 5.123056187952535
Epoch: 18| Step: 0
Training loss: 5.890049266252934
Validation loss: 5.1047444479734825
Epoch: 5| Step: 1
Training loss: 5.110603208230662
Validation loss: 5.094410272750721
Epoch: 5| Step: 2
Training loss: 5.860935989985809
Validation loss: 5.04407298789392
Epoch: 5| Step: 3
Training loss: 4.990024151638502
Validation loss: 5.04622629676876
Epoch: 5| Step: 4
Training loss: 4.938597351974712
Validation loss: 5.011712272153789
Epoch: 5| Step: 5
Training loss: 5.560115345917934
Validation loss: 4.985520531934312
Epoch: 5| Step: 6
Training loss: 4.723608484179908
Validation loss: 4.9705776144412
Epoch: 5| Step: 7
Training loss: 6.090700609218353
Validation loss: 4.948451381887702
Epoch: 5| Step: 8
Training loss: 5.154648040685207
Validation loss: 4.920743955490623
Epoch: 5| Step: 9
Training loss: 5.731658874370354
Validation loss: 4.892537186448546
Epoch: 19| Step: 0
Training loss: 5.4393356557109325
Validation loss: 4.872169883143549
Epoch: 5| Step: 1
Training loss: 5.381593607744005
Validation loss: 4.846435175856947
Epoch: 5| Step: 2
Training loss: 5.271800780438116
Validation loss: 4.824767354364431
Epoch: 5| Step: 3
Training loss: 5.180190854478082
Validation loss: 4.796350766137574
Epoch: 5| Step: 4
Training loss: 5.250284096433423
Validation loss: 4.76131593234315
Epoch: 5| Step: 5
Training loss: 5.1113990251101
Validation loss: 4.736172780867016
Epoch: 5| Step: 6
Training loss: 4.961611436137749
Validation loss: 4.719894056795987
Epoch: 5| Step: 7
Training loss: 5.292814515741778
Validation loss: 4.689901697624457
Epoch: 5| Step: 8
Training loss: 5.235830349510242
Validation loss: 4.6570163446972055
Epoch: 5| Step: 9
Training loss: 4.834123656453039
Validation loss: 4.648245320698514
Epoch: 20| Step: 0
Training loss: 4.737627479629671
Validation loss: 4.598141238495342
Epoch: 5| Step: 1
Training loss: 5.387263480037404
Validation loss: 4.574954487793035
Epoch: 5| Step: 2
Training loss: 4.778059948614981
Validation loss: 4.5563256438912685
Epoch: 5| Step: 3
Training loss: 5.192159191953282
Validation loss: 4.519917402962449
Epoch: 5| Step: 4
Training loss: 5.115769424217418
Validation loss: 4.492416976123983
Epoch: 5| Step: 5
Training loss: 4.709131808921771
Validation loss: 4.458573128208412
Epoch: 5| Step: 6
Training loss: 4.759130436305106
Validation loss: 4.4377355836888475
Epoch: 5| Step: 7
Training loss: 4.66179618534595
Validation loss: 4.386098930255341
Epoch: 5| Step: 8
Training loss: 5.679620335744558
Validation loss: 4.382998281727559
Epoch: 5| Step: 9
Training loss: 4.172534219042671
Validation loss: 4.340989616242095
Epoch: 21| Step: 0
Training loss: 4.307708068179549
Validation loss: 4.319107011833351
Epoch: 5| Step: 1
Training loss: 5.1647018059974235
Validation loss: 4.288474612836496
Epoch: 5| Step: 2
Training loss: 4.718658042163266
Validation loss: 4.249270279567631
Epoch: 5| Step: 3
Training loss: 4.639942671323005
Validation loss: 4.219109816435427
Epoch: 5| Step: 4
Training loss: 4.3162953358204526
Validation loss: 4.194172075316428
Epoch: 5| Step: 5
Training loss: 4.66896032551576
Validation loss: 4.144981221489847
Epoch: 5| Step: 6
Training loss: 4.858504140652883
Validation loss: 4.133744259138226
Epoch: 5| Step: 7
Training loss: 4.229496358914764
Validation loss: 4.099294288694352
Epoch: 5| Step: 8
Training loss: 4.864090278465291
Validation loss: 4.063062078101888
Epoch: 5| Step: 9
Training loss: 4.6194077783473775
Validation loss: 4.0214094592669865
Epoch: 22| Step: 0
Training loss: 4.249304321796484
Validation loss: 3.993337739817953
Epoch: 5| Step: 1
Training loss: 4.539394031701679
Validation loss: 3.969466977681714
Epoch: 5| Step: 2
Training loss: 4.982438527449141
Validation loss: 3.945701409233232
Epoch: 5| Step: 3
Training loss: 4.193926761728113
Validation loss: 3.8940133430941315
Epoch: 5| Step: 4
Training loss: 4.15380007019998
Validation loss: 3.873280456028364
Epoch: 5| Step: 5
Training loss: 4.258005792272524
Validation loss: 3.8386593256612422
Epoch: 5| Step: 6
Training loss: 3.8687760606421073
Validation loss: 3.7910686219799627
Epoch: 5| Step: 7
Training loss: 4.4096532354023195
Validation loss: 3.783618582673897
Epoch: 5| Step: 8
Training loss: 4.4314880479326275
Validation loss: 3.737476127692501
Epoch: 5| Step: 9
Training loss: 4.341386989340036
Validation loss: 3.712493006820489
Epoch: 23| Step: 0
Training loss: 4.243153386601932
Validation loss: 3.672921107508737
Epoch: 5| Step: 1
Training loss: 4.129104883762737
Validation loss: 3.617397094314937
Epoch: 5| Step: 2
Training loss: 3.887318627152756
Validation loss: 3.614529089325731
Epoch: 5| Step: 3
Training loss: 3.7519515999150985
Validation loss: 3.576404720860671
Epoch: 5| Step: 4
Training loss: 4.12370603669209
Validation loss: 3.538949494786985
Epoch: 5| Step: 5
Training loss: 4.290441739474412
Validation loss: 3.5015017273024927
Epoch: 5| Step: 6
Training loss: 3.665863180268095
Validation loss: 3.4653658470113022
Epoch: 5| Step: 7
Training loss: 4.052964037167399
Validation loss: 3.4382485685130892
Epoch: 5| Step: 8
Training loss: 4.091581046664698
Validation loss: 3.381917919214305
Epoch: 5| Step: 9
Training loss: 4.0435121917371
Validation loss: 3.3450954008552087
Epoch: 24| Step: 0
Training loss: 3.3615206231649655
Validation loss: 3.326169271594723
Epoch: 5| Step: 1
Training loss: 3.4639797918333732
Validation loss: 3.3044493489852043
Epoch: 5| Step: 2
Training loss: 3.9186850413120635
Validation loss: 3.2635523026517013
Epoch: 5| Step: 3
Training loss: 3.8470301084755727
Validation loss: 3.2304112938150524
Epoch: 5| Step: 4
Training loss: 3.9794664253383907
Validation loss: 3.205800315105374
Epoch: 5| Step: 5
Training loss: 4.108290844762714
Validation loss: 3.1546535996660388
Epoch: 5| Step: 6
Training loss: 3.7602957215589066
Validation loss: 3.1480395085317725
Epoch: 5| Step: 7
Training loss: 3.6310152591856406
Validation loss: 3.10850081457837
Epoch: 5| Step: 8
Training loss: 3.4160212085431785
Validation loss: 3.058933217309727
Epoch: 5| Step: 9
Training loss: 3.7332930653534833
Validation loss: 3.0428220837460387
Epoch: 25| Step: 0
Training loss: 3.8195873734674857
Validation loss: 3.009952708161845
Epoch: 5| Step: 1
Training loss: 2.8193768669093484
Validation loss: 2.931438131549778
Epoch: 5| Step: 2
Training loss: 3.2241337758747433
Validation loss: 2.9682845893225096
Epoch: 5| Step: 3
Training loss: 3.3250899109120966
Validation loss: 2.916281222073347
Epoch: 5| Step: 4
Training loss: 3.659757317590502
Validation loss: 2.886259696734072
Epoch: 5| Step: 5
Training loss: 3.536452250786758
Validation loss: 2.865647232116766
Epoch: 5| Step: 6
Training loss: 3.5735428301287575
Validation loss: 2.8343333712719923
Epoch: 5| Step: 7
Training loss: 3.2758905146449004
Validation loss: 2.814899203644519
Epoch: 5| Step: 8
Training loss: 3.5280160865157444
Validation loss: 2.7782814350300726
Epoch: 5| Step: 9
Training loss: 3.4807608296705967
Validation loss: 2.7445455393592746
Epoch: 26| Step: 0
Training loss: 2.995673715922358
Validation loss: 2.72534126956352
Epoch: 5| Step: 1
Training loss: 3.076736989262916
Validation loss: 2.7145449965288275
Epoch: 5| Step: 2
Training loss: 3.2997442839987943
Validation loss: 2.6636052720847934
Epoch: 5| Step: 3
Training loss: 2.9353878462933145
Validation loss: 2.630692955939669
Epoch: 5| Step: 4
Training loss: 3.332878336052741
Validation loss: 2.626879807876648
Epoch: 5| Step: 5
Training loss: 3.3190751945526524
Validation loss: 2.609021266003308
Epoch: 5| Step: 6
Training loss: 3.802462136545967
Validation loss: 2.5863647794356917
Epoch: 5| Step: 7
Training loss: 2.5444773990694043
Validation loss: 2.5579905725208407
Epoch: 5| Step: 8
Training loss: 3.123778753068168
Validation loss: 2.5279203761878892
Epoch: 5| Step: 9
Training loss: 3.24536286169435
Validation loss: 2.522839987299028
Epoch: 27| Step: 0
Training loss: 3.1174772123163184
Validation loss: 2.509051861464824
Epoch: 5| Step: 1
Training loss: 3.2386876347086706
Validation loss: 2.495673802676249
Epoch: 5| Step: 2
Training loss: 2.6209251929490303
Validation loss: 2.4828783159741734
Epoch: 5| Step: 3
Training loss: 2.826416932971071
Validation loss: 2.4481898611423962
Epoch: 5| Step: 4
Training loss: 3.19840037180377
Validation loss: 2.4407805609417936
Epoch: 5| Step: 5
Training loss: 3.10501467665983
Validation loss: 2.4417595481657597
Epoch: 5| Step: 6
Training loss: 3.011392578195305
Validation loss: 2.4143215484457627
Epoch: 5| Step: 7
Training loss: 2.7243165655273147
Validation loss: 2.4091438832901644
Epoch: 5| Step: 8
Training loss: 3.1325863116795474
Validation loss: 2.382331222470879
Epoch: 5| Step: 9
Training loss: 2.7115586858100262
Validation loss: 2.3433050631367953
Epoch: 28| Step: 0
Training loss: 2.737321497701906
Validation loss: 2.385159747655132
Epoch: 5| Step: 1
Training loss: 2.7711447251575576
Validation loss: 2.3404661130699944
Epoch: 5| Step: 2
Training loss: 2.783806643721231
Validation loss: 2.36231589486055
Epoch: 5| Step: 3
Training loss: 2.6711742530690112
Validation loss: 2.356451990462377
Epoch: 5| Step: 4
Training loss: 3.1566932291472884
Validation loss: 2.3481225048127112
Epoch: 5| Step: 5
Training loss: 2.4543912498765765
Validation loss: 2.340471393335287
Epoch: 5| Step: 6
Training loss: 2.654528609848259
Validation loss: 2.3331181089360955
Epoch: 5| Step: 7
Training loss: 3.0091457830264488
Validation loss: 2.3041029219195273
Epoch: 5| Step: 8
Training loss: 3.147488358698035
Validation loss: 2.329873568543691
Epoch: 5| Step: 9
Training loss: 3.1187584152566825
Validation loss: 2.321509530922769
Epoch: 29| Step: 0
Training loss: 2.8574989709994116
Validation loss: 2.3297297845855325
Epoch: 5| Step: 1
Training loss: 2.3154135212360036
Validation loss: 2.3144126831647336
Epoch: 5| Step: 2
Training loss: 2.760970687450863
Validation loss: 2.3432000358705327
Epoch: 5| Step: 3
Training loss: 2.849520351805118
Validation loss: 2.3363084471365796
Epoch: 5| Step: 4
Training loss: 2.881182450104647
Validation loss: 2.3004060098098136
Epoch: 5| Step: 5
Training loss: 2.341235630461776
Validation loss: 2.3258893637210742
Epoch: 5| Step: 6
Training loss: 3.213875051029198
Validation loss: 2.33336542316255
Epoch: 5| Step: 7
Training loss: 2.7159437407339704
Validation loss: 2.3433992343508114
Epoch: 5| Step: 8
Training loss: 2.6234171500220445
Validation loss: 2.3199967738643155
Epoch: 5| Step: 9
Training loss: 3.2309032733763576
Validation loss: 2.29892388995753
Epoch: 30| Step: 0
Training loss: 2.838452613670214
Validation loss: 2.3397085666111463
Epoch: 5| Step: 1
Training loss: 2.825594027694793
Validation loss: 2.298790547297416
Epoch: 5| Step: 2
Training loss: 3.072273695753862
Validation loss: 2.3128739145760253
Epoch: 5| Step: 3
Training loss: 2.807137526961563
Validation loss: 2.3168725678415734
Epoch: 5| Step: 4
Training loss: 2.993621721485956
Validation loss: 2.344861366528053
Epoch: 5| Step: 5
Training loss: 2.678341986502354
Validation loss: 2.3180378429721076
Epoch: 5| Step: 6
Training loss: 2.7986628609398636
Validation loss: 2.3242087511654526
Epoch: 5| Step: 7
Training loss: 2.227479230653328
Validation loss: 2.360722312826392
Epoch: 5| Step: 8
Training loss: 2.484382965267056
Validation loss: 2.3376620604174634
Epoch: 5| Step: 9
Training loss: 2.9669690060232576
Validation loss: 2.3017520437491013
Epoch: 31| Step: 0
Training loss: 2.592038428657493
Validation loss: 2.3171544524001226
Epoch: 5| Step: 1
Training loss: 2.5004628706633336
Validation loss: 2.3079173886278554
Epoch: 5| Step: 2
Training loss: 2.59113777708913
Validation loss: 2.318818635293226
Epoch: 5| Step: 3
Training loss: 3.249636996610731
Validation loss: 2.3472349673954613
Epoch: 5| Step: 4
Training loss: 2.22275643550296
Validation loss: 2.3160318990689315
Epoch: 5| Step: 5
Training loss: 2.7719636681447772
Validation loss: 2.312543595434632
Epoch: 5| Step: 6
Training loss: 2.9882673674616256
Validation loss: 2.3385117182421102
Epoch: 5| Step: 7
Training loss: 2.945302624268506
Validation loss: 2.3079064121416835
Epoch: 5| Step: 8
Training loss: 2.4550950225030075
Validation loss: 2.344368177877729
Epoch: 5| Step: 9
Training loss: 3.1395958879739148
Validation loss: 2.374383878226459
Epoch: 32| Step: 0
Training loss: 2.1916301429325715
Validation loss: 2.33043817585834
Epoch: 5| Step: 1
Training loss: 2.163300638328948
Validation loss: 2.339406716045384
Epoch: 5| Step: 2
Training loss: 3.1807145842913402
Validation loss: 2.333175929595123
Epoch: 5| Step: 3
Training loss: 3.162812162530627
Validation loss: 2.3582843024995737
Epoch: 5| Step: 4
Training loss: 2.3206272104480345
Validation loss: 2.3309040477367575
Epoch: 5| Step: 5
Training loss: 2.747090534524974
Validation loss: 2.3505403819982886
Epoch: 5| Step: 6
Training loss: 2.963721262624773
Validation loss: 2.3518228245168697
Epoch: 5| Step: 7
Training loss: 2.4950180004135634
Validation loss: 2.3190480729773566
Epoch: 5| Step: 8
Training loss: 3.007925690341795
Validation loss: 2.3421451670031463
Epoch: 5| Step: 9
Training loss: 3.030312934050126
Validation loss: 2.3002934369430594
Epoch: 33| Step: 0
Training loss: 2.6232463792045
Validation loss: 2.3111017883469804
Epoch: 5| Step: 1
Training loss: 2.8780494771154665
Validation loss: 2.3529305441454484
Epoch: 5| Step: 2
Training loss: 2.5877378713761927
Validation loss: 2.3480108145346064
Epoch: 5| Step: 3
Training loss: 2.548121422322325
Validation loss: 2.357777262358585
Epoch: 5| Step: 4
Training loss: 2.699672279955279
Validation loss: 2.350856537629541
Epoch: 5| Step: 5
Training loss: 2.8782522631082723
Validation loss: 2.316856881599948
Epoch: 5| Step: 6
Training loss: 3.0271690662807664
Validation loss: 2.3450205839725706
Epoch: 5| Step: 7
Training loss: 2.694194086204519
Validation loss: 2.3330768645543083
Epoch: 5| Step: 8
Training loss: 2.5532856399258415
Validation loss: 2.354062730312979
Epoch: 5| Step: 9
Training loss: 3.0627838314642464
Validation loss: 2.3542188072359007
Epoch: 34| Step: 0
Training loss: 2.4372206307751814
Validation loss: 2.3480696105500996
Epoch: 5| Step: 1
Training loss: 2.3184415421872933
Validation loss: 2.364448268342489
Epoch: 5| Step: 2
Training loss: 2.5554659062937977
Validation loss: 2.354721683779287
Epoch: 5| Step: 3
Training loss: 3.0169567270788633
Validation loss: 2.3438418273747907
Epoch: 5| Step: 4
Training loss: 2.978838513492353
Validation loss: 2.339550009834569
Epoch: 5| Step: 5
Training loss: 3.4543979421633324
Validation loss: 2.322833933954004
Epoch: 5| Step: 6
Training loss: 2.5666205608470913
Validation loss: 2.310355493042005
Epoch: 5| Step: 7
Training loss: 2.682489914959582
Validation loss: 2.307390017850503
Epoch: 5| Step: 8
Training loss: 2.808326506865795
Validation loss: 2.3123880501464598
Epoch: 5| Step: 9
Training loss: 2.591097751088249
Validation loss: 2.3264162354786055
Epoch: 35| Step: 0
Training loss: 2.870186134177402
Validation loss: 2.3467213348274503
Epoch: 5| Step: 1
Training loss: 3.4321210912403024
Validation loss: 2.311104126258341
Epoch: 5| Step: 2
Training loss: 2.461863994278413
Validation loss: 2.3303434997155184
Epoch: 5| Step: 3
Training loss: 2.782156892856862
Validation loss: 2.309672055477103
Epoch: 5| Step: 4
Training loss: 2.8895759947302455
Validation loss: 2.339476706487441
Epoch: 5| Step: 5
Training loss: 3.0161110283730657
Validation loss: 2.3033561295804996
Epoch: 5| Step: 6
Training loss: 2.732326677694107
Validation loss: 2.3295543389773505
Epoch: 5| Step: 7
Training loss: 2.5030585652844595
Validation loss: 2.3273375220961228
Epoch: 5| Step: 8
Training loss: 2.0413006305197077
Validation loss: 2.359732027131405
Epoch: 5| Step: 9
Training loss: 2.612369486637906
Validation loss: 2.355605223762076
Epoch: 36| Step: 0
Training loss: 3.2964290660815863
Validation loss: 2.3092360124442823
Epoch: 5| Step: 1
Training loss: 3.1728429961507167
Validation loss: 2.2889058895926655
Epoch: 5| Step: 2
Training loss: 2.54909937792739
Validation loss: 2.3097451726034337
Epoch: 5| Step: 3
Training loss: 2.4460883813955676
Validation loss: 2.3448463564799726
Epoch: 5| Step: 4
Training loss: 3.125012817356527
Validation loss: 2.326150333355428
Epoch: 5| Step: 5
Training loss: 2.1982126865246125
Validation loss: 2.3252462488528436
Epoch: 5| Step: 6
Training loss: 2.844885630797719
Validation loss: 2.2972964108996647
Epoch: 5| Step: 7
Training loss: 2.4958347908571943
Validation loss: 2.3528012134118588
Epoch: 5| Step: 8
Training loss: 2.857922127583098
Validation loss: 2.3250705850954563
Epoch: 5| Step: 9
Training loss: 2.2782128378596727
Validation loss: 2.3244437263918982
Epoch: 37| Step: 0
Training loss: 2.696804882919431
Validation loss: 2.341117715737221
Epoch: 5| Step: 1
Training loss: 3.1405390637535326
Validation loss: 2.3082447132636053
Epoch: 5| Step: 2
Training loss: 3.391671717934436
Validation loss: 2.287690912539
Epoch: 5| Step: 3
Training loss: 2.7075261796159356
Validation loss: 2.334826618383934
Epoch: 5| Step: 4
Training loss: 2.5222765256644095
Validation loss: 2.3251098805258494
Epoch: 5| Step: 5
Training loss: 2.793887199429495
Validation loss: 2.3202965382816205
Epoch: 5| Step: 6
Training loss: 2.7330073832954085
Validation loss: 2.3442021371522
Epoch: 5| Step: 7
Training loss: 2.5233409378897984
Validation loss: 2.314852855793709
Epoch: 5| Step: 8
Training loss: 2.0743810442922355
Validation loss: 2.3576480066224477
Epoch: 5| Step: 9
Training loss: 2.728066103584615
Validation loss: 2.321294139709517
Epoch: 38| Step: 0
Training loss: 2.9561726017633028
Validation loss: 2.3110775454194497
Epoch: 5| Step: 1
Training loss: 2.8002587845968216
Validation loss: 2.300627345893615
Epoch: 5| Step: 2
Training loss: 2.93455081368314
Validation loss: 2.313080248160444
Epoch: 5| Step: 3
Training loss: 2.569294736699307
Validation loss: 2.334191929370921
Epoch: 5| Step: 4
Training loss: 2.833221227634587
Validation loss: 2.337616504206244
Epoch: 5| Step: 5
Training loss: 2.5252157742301744
Validation loss: 2.3488966557245936
Epoch: 5| Step: 6
Training loss: 2.6738523784509955
Validation loss: 2.319076093166527
Epoch: 5| Step: 7
Training loss: 2.2106410390559454
Validation loss: 2.3264944988889638
Epoch: 5| Step: 8
Training loss: 3.078846338633434
Validation loss: 2.3138052287341173
Epoch: 5| Step: 9
Training loss: 2.7774791228490896
Validation loss: 2.332768173122651
Epoch: 39| Step: 0
Training loss: 2.8358588089547356
Validation loss: 2.330566565642799
Epoch: 5| Step: 1
Training loss: 2.869991583994578
Validation loss: 2.3400616553520557
Epoch: 5| Step: 2
Training loss: 2.358014857846366
Validation loss: 2.294133691013975
Epoch: 5| Step: 3
Training loss: 2.4891577214515683
Validation loss: 2.3205713586741967
Epoch: 5| Step: 4
Training loss: 2.4684616113574878
Validation loss: 2.3260107108560604
Epoch: 5| Step: 5
Training loss: 3.2242964575988315
Validation loss: 2.3276794748525687
Epoch: 5| Step: 6
Training loss: 3.041068937193256
Validation loss: 2.308914990548155
Epoch: 5| Step: 7
Training loss: 2.666785942827444
Validation loss: 2.324016212353068
Epoch: 5| Step: 8
Training loss: 2.399047177638378
Validation loss: 2.3040224337871185
Epoch: 5| Step: 9
Training loss: 2.9517667361470186
Validation loss: 2.2739639414788257
Epoch: 40| Step: 0
Training loss: 2.5910350884520734
Validation loss: 2.315711564136658
Epoch: 5| Step: 1
Training loss: 2.285657413660689
Validation loss: 2.3316130330402545
Epoch: 5| Step: 2
Training loss: 2.707758994214949
Validation loss: 2.311793046197669
Epoch: 5| Step: 3
Training loss: 2.712677674693137
Validation loss: 2.3117621334715004
Epoch: 5| Step: 4
Training loss: 2.735532679038205
Validation loss: 2.2953983660315074
Epoch: 5| Step: 5
Training loss: 3.236680540558623
Validation loss: 2.3236623252849533
Epoch: 5| Step: 6
Training loss: 2.565229776214804
Validation loss: 2.2963406452897135
Epoch: 5| Step: 7
Training loss: 3.0335651483791226
Validation loss: 2.2870914439114656
Epoch: 5| Step: 8
Training loss: 2.7640550543219566
Validation loss: 2.3071673305219687
Epoch: 5| Step: 9
Training loss: 2.588654810519808
Validation loss: 2.318389787159687
Epoch: 41| Step: 0
Training loss: 2.2917606218902695
Validation loss: 2.293512209616005
Epoch: 5| Step: 1
Training loss: 2.718948357021857
Validation loss: 2.3079144862287047
Epoch: 5| Step: 2
Training loss: 2.5623415456022993
Validation loss: 2.3140693693692818
Epoch: 5| Step: 3
Training loss: 2.4731610643111
Validation loss: 2.3312439889999226
Epoch: 5| Step: 4
Training loss: 3.0321031136806984
Validation loss: 2.298705075943495
Epoch: 5| Step: 5
Training loss: 2.534521367259742
Validation loss: 2.322231278854345
Epoch: 5| Step: 6
Training loss: 2.7172947639233787
Validation loss: 2.309401524020963
Epoch: 5| Step: 7
Training loss: 3.167797305057597
Validation loss: 2.3188532130824413
Epoch: 5| Step: 8
Training loss: 2.6772809580331813
Validation loss: 2.332088833907096
Epoch: 5| Step: 9
Training loss: 2.9555452309175565
Validation loss: 2.315670788200427
Epoch: 42| Step: 0
Training loss: 2.240827619895967
Validation loss: 2.2933969306449073
Epoch: 5| Step: 1
Training loss: 2.9494903641482075
Validation loss: 2.2702514109975636
Epoch: 5| Step: 2
Training loss: 2.511577691230051
Validation loss: 2.3078767500504753
Epoch: 5| Step: 3
Training loss: 3.080065120652636
Validation loss: 2.2938408818757567
Epoch: 5| Step: 4
Training loss: 2.604056597608778
Validation loss: 2.295141700248019
Epoch: 5| Step: 5
Training loss: 2.8150966948846166
Validation loss: 2.298272646566416
Epoch: 5| Step: 6
Training loss: 3.026520647932348
Validation loss: 2.3292349753380126
Epoch: 5| Step: 7
Training loss: 3.0356708812014865
Validation loss: 2.321494885775764
Epoch: 5| Step: 8
Training loss: 2.5060652591035497
Validation loss: 2.300035507014653
Epoch: 5| Step: 9
Training loss: 2.3930476261282325
Validation loss: 2.3168947091265557
Epoch: 43| Step: 0
Training loss: 2.5359658475315063
Validation loss: 2.3231336092412382
Epoch: 5| Step: 1
Training loss: 3.0700691262579807
Validation loss: 2.313839593896894
Epoch: 5| Step: 2
Training loss: 2.822503081895279
Validation loss: 2.3149141721707247
Epoch: 5| Step: 3
Training loss: 2.239965633912087
Validation loss: 2.3079750998130812
Epoch: 5| Step: 4
Training loss: 2.999780011058493
Validation loss: 2.2913673691730962
Epoch: 5| Step: 5
Training loss: 2.595002205320388
Validation loss: 2.305604987095754
Epoch: 5| Step: 6
Training loss: 2.743149113300313
Validation loss: 2.311853539274772
Epoch: 5| Step: 7
Training loss: 2.6466493224369207
Validation loss: 2.2922856079480924
Epoch: 5| Step: 8
Training loss: 2.9666872588168065
Validation loss: 2.3242760845280332
Epoch: 5| Step: 9
Training loss: 2.6261520582453186
Validation loss: 2.296670575168915
Epoch: 44| Step: 0
Training loss: 2.854387780602135
Validation loss: 2.298206382413233
Epoch: 5| Step: 1
Training loss: 3.049149510357147
Validation loss: 2.292743547671865
Epoch: 5| Step: 2
Training loss: 3.006972316958308
Validation loss: 2.31686550221471
Epoch: 5| Step: 3
Training loss: 2.1600570242030273
Validation loss: 2.3182550079321724
Epoch: 5| Step: 4
Training loss: 3.2277619947119454
Validation loss: 2.3055216972953225
Epoch: 5| Step: 5
Training loss: 2.954222782895116
Validation loss: 2.295434191364699
Epoch: 5| Step: 6
Training loss: 2.394258916157331
Validation loss: 2.306466888551489
Epoch: 5| Step: 7
Training loss: 2.709425710799713
Validation loss: 2.2989835136172045
Epoch: 5| Step: 8
Training loss: 2.2934684144018394
Validation loss: 2.2932435851640385
Epoch: 5| Step: 9
Training loss: 2.2918425983752764
Validation loss: 2.2996465799115344
Epoch: 45| Step: 0
Training loss: 2.405187198705213
Validation loss: 2.3192074999883747
Epoch: 5| Step: 1
Training loss: 3.0664174462375215
Validation loss: 2.3212958682827907
Epoch: 5| Step: 2
Training loss: 2.360299945940844
Validation loss: 2.311284299954542
Epoch: 5| Step: 3
Training loss: 2.60475462316148
Validation loss: 2.338704049635746
Epoch: 5| Step: 4
Training loss: 2.58778568846798
Validation loss: 2.3059865211116066
Epoch: 5| Step: 5
Training loss: 3.15668779112996
Validation loss: 2.32328481786274
Epoch: 5| Step: 6
Training loss: 2.975794735710451
Validation loss: 2.3187997087219525
Epoch: 5| Step: 7
Training loss: 2.6931681630820368
Validation loss: 2.2918723326676855
Epoch: 5| Step: 8
Training loss: 2.3174700553794794
Validation loss: 2.3211228635961243
Epoch: 5| Step: 9
Training loss: 2.8353267463890406
Validation loss: 2.300754782944272
Epoch: 46| Step: 0
Training loss: 2.640928894252837
Validation loss: 2.308943037954022
Epoch: 5| Step: 1
Training loss: 2.6195902712231383
Validation loss: 2.2926272159217875
Epoch: 5| Step: 2
Training loss: 3.0535404168691813
Validation loss: 2.317993576631916
Epoch: 5| Step: 3
Training loss: 2.8670541176187263
Validation loss: 2.291112883785636
Epoch: 5| Step: 4
Training loss: 2.6383566057436294
Validation loss: 2.2964267905651767
Epoch: 5| Step: 5
Training loss: 2.44979408722319
Validation loss: 2.2945802936587696
Epoch: 5| Step: 6
Training loss: 1.7814836850721432
Validation loss: 2.2720304126527324
Epoch: 5| Step: 7
Training loss: 2.549245936032204
Validation loss: 2.307939968204007
Epoch: 5| Step: 8
Training loss: 3.137571758042393
Validation loss: 2.2984073108017244
Epoch: 5| Step: 9
Training loss: 3.0492497506350715
Validation loss: 2.302115246188123
Epoch: 47| Step: 0
Training loss: 2.9811967633050536
Validation loss: 2.28025817713045
Epoch: 5| Step: 1
Training loss: 2.615353251825829
Validation loss: 2.3085870238773434
Epoch: 5| Step: 2
Training loss: 2.842610896085317
Validation loss: 2.259587637701376
Epoch: 5| Step: 3
Training loss: 2.0701840918683665
Validation loss: 2.297071268130609
Epoch: 5| Step: 4
Training loss: 3.1923686225881482
Validation loss: 2.2995205131260406
Epoch: 5| Step: 5
Training loss: 2.611305482091435
Validation loss: 2.2983704832987213
Epoch: 5| Step: 6
Training loss: 2.355273287683209
Validation loss: 2.303998932501395
Epoch: 5| Step: 7
Training loss: 2.7374233148015206
Validation loss: 2.2836441041341446
Epoch: 5| Step: 8
Training loss: 2.713887832634455
Validation loss: 2.2845112472096023
Epoch: 5| Step: 9
Training loss: 2.8117952099510832
Validation loss: 2.3300362344878156
Epoch: 48| Step: 0
Training loss: 3.45694352378099
Validation loss: 2.3128177327268995
Epoch: 5| Step: 1
Training loss: 2.2426256555843653
Validation loss: 2.323499376788539
Epoch: 5| Step: 2
Training loss: 2.6799093883056586
Validation loss: 2.311357258944769
Epoch: 5| Step: 3
Training loss: 2.523868771951792
Validation loss: 2.328308920598495
Epoch: 5| Step: 4
Training loss: 2.2582564923866584
Validation loss: 2.2875636474013605
Epoch: 5| Step: 5
Training loss: 3.1464261807933203
Validation loss: 2.2875962337834035
Epoch: 5| Step: 6
Training loss: 2.0828107051169122
Validation loss: 2.283075912268514
Epoch: 5| Step: 7
Training loss: 2.6003878120818587
Validation loss: 2.302805695765811
Epoch: 5| Step: 8
Training loss: 2.8161527860873248
Validation loss: 2.2771320000763566
Epoch: 5| Step: 9
Training loss: 2.7262077210028255
Validation loss: 2.2715266879264258
Epoch: 49| Step: 0
Training loss: 2.949902587834039
Validation loss: 2.2951941459053558
Epoch: 5| Step: 1
Training loss: 2.419552483125219
Validation loss: 2.2805852784885765
Epoch: 5| Step: 2
Training loss: 2.6906242840242043
Validation loss: 2.3103371827270966
Epoch: 5| Step: 3
Training loss: 2.6172135536832863
Validation loss: 2.2661888963821046
Epoch: 5| Step: 4
Training loss: 2.86144277713983
Validation loss: 2.2785720974536754
Epoch: 5| Step: 5
Training loss: 2.7842461381714014
Validation loss: 2.2939012555967344
Epoch: 5| Step: 6
Training loss: 2.3665862374227853
Validation loss: 2.2694750587200185
Epoch: 5| Step: 7
Training loss: 3.096358615539452
Validation loss: 2.2549654112149966
Epoch: 5| Step: 8
Training loss: 2.3929592529666364
Validation loss: 2.306498115129514
Epoch: 5| Step: 9
Training loss: 2.661064721032528
Validation loss: 2.2984086241703476
Epoch: 50| Step: 0
Training loss: 2.8039032841217946
Validation loss: 2.3092180369786215
Epoch: 5| Step: 1
Training loss: 2.9670787725068477
Validation loss: 2.2917303225678496
Epoch: 5| Step: 2
Training loss: 2.6781131134303564
Validation loss: 2.2817047375568866
Epoch: 5| Step: 3
Training loss: 2.740161984521356
Validation loss: 2.3023554316998402
Epoch: 5| Step: 4
Training loss: 2.630152232764691
Validation loss: 2.289406920032787
Epoch: 5| Step: 5
Training loss: 2.361101239003007
Validation loss: 2.2738810037567623
Epoch: 5| Step: 6
Training loss: 1.981772390624816
Validation loss: 2.3069420852208244
Epoch: 5| Step: 7
Training loss: 3.109428673669126
Validation loss: 2.280021087972807
Epoch: 5| Step: 8
Training loss: 2.6919470964762757
Validation loss: 2.2816030864697256
Epoch: 5| Step: 9
Training loss: 2.761837796204318
Validation loss: 2.306537033872512
Epoch: 51| Step: 0
Training loss: 2.6264711526375475
Validation loss: 2.2980994077663244
Epoch: 5| Step: 1
Training loss: 3.0135995654704058
Validation loss: 2.3065729593163784
Epoch: 5| Step: 2
Training loss: 2.492872855441042
Validation loss: 2.2867840050749337
Epoch: 5| Step: 3
Training loss: 2.693262177304817
Validation loss: 2.303603122225986
Epoch: 5| Step: 4
Training loss: 2.582530563483833
Validation loss: 2.2770402923842066
Epoch: 5| Step: 5
Training loss: 2.459295978755019
Validation loss: 2.303976522933925
Epoch: 5| Step: 6
Training loss: 2.6817241969634202
Validation loss: 2.274804509427439
Epoch: 5| Step: 7
Training loss: 2.728587188546796
Validation loss: 2.2762562090400147
Epoch: 5| Step: 8
Training loss: 2.819657014455606
Validation loss: 2.303970817350681
Epoch: 5| Step: 9
Training loss: 2.821591243227811
Validation loss: 2.278221523840062
Epoch: 52| Step: 0
Training loss: 2.981908767819225
Validation loss: 2.3059739660274468
Epoch: 5| Step: 1
Training loss: 2.4927589454937986
Validation loss: 2.266463458873158
Epoch: 5| Step: 2
Training loss: 2.5572340286088786
Validation loss: 2.3024299170975726
Epoch: 5| Step: 3
Training loss: 2.3329020396766116
Validation loss: 2.2889583184872464
Epoch: 5| Step: 4
Training loss: 2.318367602133675
Validation loss: 2.3025153781082555
Epoch: 5| Step: 5
Training loss: 2.34349669041287
Validation loss: 2.2865956681925192
Epoch: 5| Step: 6
Training loss: 2.8352729291917935
Validation loss: 2.2979985322269383
Epoch: 5| Step: 7
Training loss: 2.982909157147586
Validation loss: 2.283915323447057
Epoch: 5| Step: 8
Training loss: 3.5056368531400612
Validation loss: 2.313437783096317
Epoch: 5| Step: 9
Training loss: 2.3998007612179975
Validation loss: 2.2641665341009305
Epoch: 53| Step: 0
Training loss: 2.7969835995523016
Validation loss: 2.2963328553404025
Epoch: 5| Step: 1
Training loss: 2.6747168881343577
Validation loss: 2.299221870523602
Epoch: 5| Step: 2
Training loss: 2.5013042862323287
Validation loss: 2.268223375427697
Epoch: 5| Step: 3
Training loss: 3.140436726710579
Validation loss: 2.289097013730015
Epoch: 5| Step: 4
Training loss: 2.484358877453582
Validation loss: 2.242522614491504
Epoch: 5| Step: 5
Training loss: 2.9736255965121807
Validation loss: 2.289619838082514
Epoch: 5| Step: 6
Training loss: 2.8210778716873057
Validation loss: 2.2805448990259984
Epoch: 5| Step: 7
Training loss: 2.5962947633325286
Validation loss: 2.2696671737210274
Epoch: 5| Step: 8
Training loss: 2.249482943146075
Validation loss: 2.2826814876359545
Epoch: 5| Step: 9
Training loss: 2.5302071012386893
Validation loss: 2.2567152904229304
Epoch: 54| Step: 0
Training loss: 2.298053257589832
Validation loss: 2.2649823783234315
Epoch: 5| Step: 1
Training loss: 2.5157926515430202
Validation loss: 2.274272128739676
Epoch: 5| Step: 2
Training loss: 3.1101905864936903
Validation loss: 2.2463111845017627
Epoch: 5| Step: 3
Training loss: 2.246438068037994
Validation loss: 2.2722361538003892
Epoch: 5| Step: 4
Training loss: 2.7115221080297287
Validation loss: 2.271136570350262
Epoch: 5| Step: 5
Training loss: 3.6914269179315586
Validation loss: 2.3008659669046714
Epoch: 5| Step: 6
Training loss: 2.4774934481044326
Validation loss: 2.2555784243246193
Epoch: 5| Step: 7
Training loss: 2.2636731388855402
Validation loss: 2.2639235665820436
Epoch: 5| Step: 8
Training loss: 2.4873982394312897
Validation loss: 2.272275689406827
Epoch: 5| Step: 9
Training loss: 2.688559212541777
Validation loss: 2.2864508360173876
Epoch: 55| Step: 0
Training loss: 3.031331169378771
Validation loss: 2.2699237764794535
Epoch: 5| Step: 1
Training loss: 2.803257654765952
Validation loss: 2.2861590207649156
Epoch: 5| Step: 2
Training loss: 1.9121641749321423
Validation loss: 2.274814070678909
Epoch: 5| Step: 3
Training loss: 2.8265912865838247
Validation loss: 2.2809048425122143
Epoch: 5| Step: 4
Training loss: 2.3497506191960262
Validation loss: 2.270506868973807
Epoch: 5| Step: 5
Training loss: 2.9868505306642477
Validation loss: 2.2980762317772974
Epoch: 5| Step: 6
Training loss: 2.8651984276886124
Validation loss: 2.268086679438337
Epoch: 5| Step: 7
Training loss: 2.6639445438168825
Validation loss: 2.2643398971280115
Epoch: 5| Step: 8
Training loss: 2.305566859185674
Validation loss: 2.1982079667250547
Epoch: 5| Step: 9
Training loss: 2.7417454376808847
Validation loss: 2.306078333052568
Epoch: 56| Step: 0
Training loss: 2.3987115461120823
Validation loss: 2.2613134202650693
Epoch: 5| Step: 1
Training loss: 2.5038613063926665
Validation loss: 2.2542799336090074
Epoch: 5| Step: 2
Training loss: 2.545903962732545
Validation loss: 2.2428306832981804
Epoch: 5| Step: 3
Training loss: 2.8613641209339926
Validation loss: 2.2602598309231032
Epoch: 5| Step: 4
Training loss: 2.852215417813286
Validation loss: 2.2764783170659717
Epoch: 5| Step: 5
Training loss: 2.6819521395902526
Validation loss: 2.2961283096158907
Epoch: 5| Step: 6
Training loss: 2.6452151700279223
Validation loss: 2.267232332036505
Epoch: 5| Step: 7
Training loss: 2.6257739061583742
Validation loss: 2.2718616151767095
Epoch: 5| Step: 8
Training loss: 2.833676953141989
Validation loss: 2.27361929067718
Epoch: 5| Step: 9
Training loss: 2.713723018587665
Validation loss: 2.288111284184326
Epoch: 57| Step: 0
Training loss: 2.5884195730069135
Validation loss: 2.286674395293755
Epoch: 5| Step: 1
Training loss: 3.0428662656241783
Validation loss: 2.2645611672905215
Epoch: 5| Step: 2
Training loss: 3.1053904397555545
Validation loss: 2.276738363244101
Epoch: 5| Step: 3
Training loss: 2.783321509269528
Validation loss: 2.2843356472707668
Epoch: 5| Step: 4
Training loss: 2.2080168827213082
Validation loss: 2.240678992943217
Epoch: 5| Step: 5
Training loss: 2.5301905169016123
Validation loss: 2.2370595311694217
Epoch: 5| Step: 6
Training loss: 2.1766241827533706
Validation loss: 2.2549255650020954
Epoch: 5| Step: 7
Training loss: 2.578008568186464
Validation loss: 2.284046580535355
Epoch: 5| Step: 8
Training loss: 3.0022110738232763
Validation loss: 2.240356755339995
Epoch: 5| Step: 9
Training loss: 2.4380222641364053
Validation loss: 2.2776363385408316
Epoch: 58| Step: 0
Training loss: 2.638292444875799
Validation loss: 2.281815539896994
Epoch: 5| Step: 1
Training loss: 2.32150186695465
Validation loss: 2.261424289070634
Epoch: 5| Step: 2
Training loss: 2.769693948246912
Validation loss: 2.2732553248578142
Epoch: 5| Step: 3
Training loss: 3.04286062418216
Validation loss: 2.2626175821329735
Epoch: 5| Step: 4
Training loss: 2.9491608670851255
Validation loss: 2.2857262524359077
Epoch: 5| Step: 5
Training loss: 2.601320639241874
Validation loss: 2.267966862764333
Epoch: 5| Step: 6
Training loss: 2.4219110301629376
Validation loss: 2.2696516514792506
Epoch: 5| Step: 7
Training loss: 2.6460899368767734
Validation loss: 2.262015870716548
Epoch: 5| Step: 8
Training loss: 2.615722975268904
Validation loss: 2.283232442590091
Epoch: 5| Step: 9
Training loss: 2.6151102049382486
Validation loss: 2.2641773046708433
Epoch: 59| Step: 0
Training loss: 2.7302804862895456
Validation loss: 2.251200213846555
Epoch: 5| Step: 1
Training loss: 2.5598618487989784
Validation loss: 2.272974504440384
Epoch: 5| Step: 2
Training loss: 2.2225024232110973
Validation loss: 2.287789090673135
Epoch: 5| Step: 3
Training loss: 3.042836177812549
Validation loss: 2.280081255158244
Epoch: 5| Step: 4
Training loss: 1.788265933816564
Validation loss: 2.262019137254787
Epoch: 5| Step: 5
Training loss: 2.988735987152984
Validation loss: 2.261467847405436
Epoch: 5| Step: 6
Training loss: 2.8878893304897004
Validation loss: 2.2502056523747624
Epoch: 5| Step: 7
Training loss: 2.6634647000714335
Validation loss: 2.2258737801817197
Epoch: 5| Step: 8
Training loss: 2.53663411141691
Validation loss: 2.279485529507986
Epoch: 5| Step: 9
Training loss: 2.989186826633873
Validation loss: 2.231663723725923
Epoch: 60| Step: 0
Training loss: 2.6389428373031247
Validation loss: 2.2698285217180687
Epoch: 5| Step: 1
Training loss: 2.800117360788989
Validation loss: 2.2518733705048306
Epoch: 5| Step: 2
Training loss: 2.546676347158251
Validation loss: 2.2706394676643398
Epoch: 5| Step: 3
Training loss: 2.319796829206469
Validation loss: 2.2622379717893097
Epoch: 5| Step: 4
Training loss: 3.1610456236045184
Validation loss: 2.2710462120194315
Epoch: 5| Step: 5
Training loss: 2.6387081976705216
Validation loss: 2.272286434146856
Epoch: 5| Step: 6
Training loss: 2.9677074007028597
Validation loss: 2.2585598006095986
Epoch: 5| Step: 7
Training loss: 2.4680843180097525
Validation loss: 2.270470681558268
Epoch: 5| Step: 8
Training loss: 2.7018756956284435
Validation loss: 2.2357329372314605
Epoch: 5| Step: 9
Training loss: 2.2592298025748256
Validation loss: 2.2352436829594557
Epoch: 61| Step: 0
Training loss: 2.5141766092544042
Validation loss: 2.2314862664922166
Epoch: 5| Step: 1
Training loss: 2.733980946076084
Validation loss: 2.2579092937201364
Epoch: 5| Step: 2
Training loss: 2.4892261094550894
Validation loss: 2.2612165981591295
Epoch: 5| Step: 3
Training loss: 2.8977216057543496
Validation loss: 2.2388073225092273
Epoch: 5| Step: 4
Training loss: 2.8378719741407945
Validation loss: 2.2663998258663254
Epoch: 5| Step: 5
Training loss: 2.2946226533654484
Validation loss: 2.240032164132305
Epoch: 5| Step: 6
Training loss: 2.4575480052648313
Validation loss: 2.286836275521425
Epoch: 5| Step: 7
Training loss: 2.8161690409706903
Validation loss: 2.253182124159421
Epoch: 5| Step: 8
Training loss: 2.273228796329447
Validation loss: 2.2502203003714665
Epoch: 5| Step: 9
Training loss: 3.1204494150196984
Validation loss: 2.2788753935571204
Epoch: 62| Step: 0
Training loss: 2.409085077439017
Validation loss: 2.263188174386438
Epoch: 5| Step: 1
Training loss: 2.4484480477247677
Validation loss: 2.264853638183403
Epoch: 5| Step: 2
Training loss: 2.318785502118293
Validation loss: 2.2314443417444756
Epoch: 5| Step: 3
Training loss: 2.693058387143231
Validation loss: 2.286025613129399
Epoch: 5| Step: 4
Training loss: 2.770190592244223
Validation loss: 2.2783194149073944
Epoch: 5| Step: 5
Training loss: 2.2744003302035525
Validation loss: 2.2530881149147164
Epoch: 5| Step: 6
Training loss: 2.735514899119664
Validation loss: 2.2672965377463146
Epoch: 5| Step: 7
Training loss: 3.0940773530647303
Validation loss: 2.220927854338734
Epoch: 5| Step: 8
Training loss: 3.1270098517262412
Validation loss: 2.261246931023552
Epoch: 5| Step: 9
Training loss: 2.5785099551319557
Validation loss: 2.2781371998160203
Epoch: 63| Step: 0
Training loss: 2.153081694722513
Validation loss: 2.273488720593562
Epoch: 5| Step: 1
Training loss: 3.2405581137248154
Validation loss: 2.26814973478982
Epoch: 5| Step: 2
Training loss: 2.285100848186586
Validation loss: 2.261732946586813
Epoch: 5| Step: 3
Training loss: 3.0067374549985266
Validation loss: 2.252417091845373
Epoch: 5| Step: 4
Training loss: 2.1733741251894974
Validation loss: 2.259990914129972
Epoch: 5| Step: 5
Training loss: 2.470333122924966
Validation loss: 2.2479549876983964
Epoch: 5| Step: 6
Training loss: 2.930916408922941
Validation loss: 2.269487333641956
Epoch: 5| Step: 7
Training loss: 2.3827444598379977
Validation loss: 2.25323888580553
Epoch: 5| Step: 8
Training loss: 2.8484531956678096
Validation loss: 2.234611190636319
Epoch: 5| Step: 9
Training loss: 2.768601362315992
Validation loss: 2.2593001354986124
Epoch: 64| Step: 0
Training loss: 2.457416061566701
Validation loss: 2.2353574860850447
Epoch: 5| Step: 1
Training loss: 2.8233197101623144
Validation loss: 2.244200270565039
Epoch: 5| Step: 2
Training loss: 2.2867578355207248
Validation loss: 2.2727528274879236
Epoch: 5| Step: 3
Training loss: 2.153818611739199
Validation loss: 2.229873385575143
Epoch: 5| Step: 4
Training loss: 3.113449570761552
Validation loss: 2.2741530465935504
Epoch: 5| Step: 5
Training loss: 2.6877998362219473
Validation loss: 2.2495223646469853
Epoch: 5| Step: 6
Training loss: 2.970345479991816
Validation loss: 2.249478499362323
Epoch: 5| Step: 7
Training loss: 2.4902217371002844
Validation loss: 2.252423310819224
Epoch: 5| Step: 8
Training loss: 2.8903092495411973
Validation loss: 2.266741782100508
Epoch: 5| Step: 9
Training loss: 2.6439210083396674
Validation loss: 2.2576927387471004
Epoch: 65| Step: 0
Training loss: 2.8302931088548497
Validation loss: 2.258253894895504
Epoch: 5| Step: 1
Training loss: 2.5751036539788745
Validation loss: 2.2556862656862604
Epoch: 5| Step: 2
Training loss: 3.100341710361666
Validation loss: 2.2556722474383486
Epoch: 5| Step: 3
Training loss: 3.036726890103574
Validation loss: 2.2569385532422777
Epoch: 5| Step: 4
Training loss: 2.408700661106916
Validation loss: 2.2484701379191474
Epoch: 5| Step: 5
Training loss: 2.3134578061581377
Validation loss: 2.2416529754411636
Epoch: 5| Step: 6
Training loss: 2.6826216312645212
Validation loss: 2.2530579692129815
Epoch: 5| Step: 7
Training loss: 2.9979902529262206
Validation loss: 2.2515389658196128
Epoch: 5| Step: 8
Training loss: 1.891579442246922
Validation loss: 2.2379207978301796
Epoch: 5| Step: 9
Training loss: 2.4267621353080213
Validation loss: 2.266546414115083
Epoch: 66| Step: 0
Training loss: 2.8361476683702063
Validation loss: 2.246442234007617
Epoch: 5| Step: 1
Training loss: 2.15858598308015
Validation loss: 2.261963449677013
Epoch: 5| Step: 2
Training loss: 2.784696351076755
Validation loss: 2.286763352057218
Epoch: 5| Step: 3
Training loss: 2.3804598841613185
Validation loss: 2.267674573313715
Epoch: 5| Step: 4
Training loss: 2.896146542376935
Validation loss: 2.1964018638557157
Epoch: 5| Step: 5
Training loss: 2.534925171088164
Validation loss: 2.2214538101676995
Epoch: 5| Step: 6
Training loss: 2.782478404136953
Validation loss: 2.260407517938953
Epoch: 5| Step: 7
Training loss: 2.8725225305960156
Validation loss: 2.2505990130376134
Epoch: 5| Step: 8
Training loss: 2.5534777095337913
Validation loss: 2.2638765567974692
Epoch: 5| Step: 9
Training loss: 2.573653163338754
Validation loss: 2.2324717092268873
Epoch: 67| Step: 0
Training loss: 2.679326514127276
Validation loss: 2.226713782304127
Epoch: 5| Step: 1
Training loss: 2.757651694475209
Validation loss: 2.2398872506908116
Epoch: 5| Step: 2
Training loss: 2.6669937072324945
Validation loss: 2.229161863678897
Epoch: 5| Step: 3
Training loss: 2.5871024374600284
Validation loss: 2.222695411610756
Epoch: 5| Step: 4
Training loss: 2.770937112188909
Validation loss: 2.24532564448505
Epoch: 5| Step: 5
Training loss: 2.6546944046155243
Validation loss: 2.2563741091831337
Epoch: 5| Step: 6
Training loss: 2.5925188934608867
Validation loss: 2.2470462110642084
Epoch: 5| Step: 7
Training loss: 2.1216552440798306
Validation loss: 2.241996359528758
Epoch: 5| Step: 8
Training loss: 2.37899684890083
Validation loss: 2.2424289066529433
Epoch: 5| Step: 9
Training loss: 3.014374785117771
Validation loss: 2.2355165299117763
Epoch: 68| Step: 0
Training loss: 2.209145402646503
Validation loss: 2.206274031541409
Epoch: 5| Step: 1
Training loss: 2.8542317478157337
Validation loss: 2.2450308179464304
Epoch: 5| Step: 2
Training loss: 2.6479149587061968
Validation loss: 2.231613457779952
Epoch: 5| Step: 3
Training loss: 2.8424676372259294
Validation loss: 2.2433321483687334
Epoch: 5| Step: 4
Training loss: 2.8532260170776533
Validation loss: 2.24957781524809
Epoch: 5| Step: 5
Training loss: 2.4548028929727064
Validation loss: 2.24060881850319
Epoch: 5| Step: 6
Training loss: 2.339829280853113
Validation loss: 2.2489646623582784
Epoch: 5| Step: 7
Training loss: 2.47694225588635
Validation loss: 2.24453798824952
Epoch: 5| Step: 8
Training loss: 2.73650272964071
Validation loss: 2.2545553873734434
Epoch: 5| Step: 9
Training loss: 2.8124438810047874
Validation loss: 2.217105428789953
Epoch: 69| Step: 0
Training loss: 2.6147010037107785
Validation loss: 2.252891991554967
Epoch: 5| Step: 1
Training loss: 2.465241655494023
Validation loss: 2.2589390805166945
Epoch: 5| Step: 2
Training loss: 2.964886691419512
Validation loss: 2.237662248314628
Epoch: 5| Step: 3
Training loss: 2.1437474331409447
Validation loss: 2.262910275997299
Epoch: 5| Step: 4
Training loss: 2.646501041373951
Validation loss: 2.227489156174959
Epoch: 5| Step: 5
Training loss: 2.591625032570924
Validation loss: 2.2176554689038284
Epoch: 5| Step: 6
Training loss: 2.8073807645022026
Validation loss: 2.2546155016949654
Epoch: 5| Step: 7
Training loss: 2.141159547764803
Validation loss: 2.2339528228309624
Epoch: 5| Step: 8
Training loss: 2.670154486055186
Validation loss: 2.259324370299757
Epoch: 5| Step: 9
Training loss: 3.0803861881253076
Validation loss: 2.2466401646381264
Epoch: 70| Step: 0
Training loss: 2.63186311442369
Validation loss: 2.203228565522688
Epoch: 5| Step: 1
Training loss: 2.622907622071524
Validation loss: 2.2277503098872184
Epoch: 5| Step: 2
Training loss: 3.0419362266622354
Validation loss: 2.2037301312777835
Epoch: 5| Step: 3
Training loss: 2.5801377271515094
Validation loss: 2.2545882084762954
Epoch: 5| Step: 4
Training loss: 3.043822025645342
Validation loss: 2.210733012871962
Epoch: 5| Step: 5
Training loss: 2.2610632913496724
Validation loss: 2.2078537733365167
Epoch: 5| Step: 6
Training loss: 2.2500464116713887
Validation loss: 2.2248516548633135
Epoch: 5| Step: 7
Training loss: 2.9401442618236393
Validation loss: 2.2297406859454645
Epoch: 5| Step: 8
Training loss: 2.3982258557309475
Validation loss: 2.236096766489525
Epoch: 5| Step: 9
Training loss: 2.4275464992607243
Validation loss: 2.2076107360763046
Epoch: 71| Step: 0
Training loss: 2.539611099838247
Validation loss: 2.231869852249567
Epoch: 5| Step: 1
Training loss: 2.365940583921999
Validation loss: 2.279152806436789
Epoch: 5| Step: 2
Training loss: 2.5197059275083284
Validation loss: 2.2445670707566356
Epoch: 5| Step: 3
Training loss: 3.166513506213763
Validation loss: 2.2471813136677117
Epoch: 5| Step: 4
Training loss: 3.03362142082128
Validation loss: 2.240839875108788
Epoch: 5| Step: 5
Training loss: 2.3882169875214614
Validation loss: 2.234277205059416
Epoch: 5| Step: 6
Training loss: 2.7061553022277547
Validation loss: 2.2322445740632926
Epoch: 5| Step: 7
Training loss: 2.110429175422354
Validation loss: 2.2347780199317526
Epoch: 5| Step: 8
Training loss: 2.7198071233299714
Validation loss: 2.2266066375165816
Epoch: 5| Step: 9
Training loss: 2.5694625051969124
Validation loss: 2.2252114660932243
Epoch: 72| Step: 0
Training loss: 2.8652463573369085
Validation loss: 2.222859617914823
Epoch: 5| Step: 1
Training loss: 2.8334188074421154
Validation loss: 2.2183392376094178
Epoch: 5| Step: 2
Training loss: 2.6392835110076844
Validation loss: 2.22340102291279
Epoch: 5| Step: 3
Training loss: 2.055438814485309
Validation loss: 2.187733026041298
Epoch: 5| Step: 4
Training loss: 2.5313933532058557
Validation loss: 2.21629930641324
Epoch: 5| Step: 5
Training loss: 3.313044233418916
Validation loss: 2.2533594516642377
Epoch: 5| Step: 6
Training loss: 2.355006740422737
Validation loss: 2.218872471365237
Epoch: 5| Step: 7
Training loss: 2.531312541424517
Validation loss: 2.2092852451031666
Epoch: 5| Step: 8
Training loss: 2.953231567397624
Validation loss: 2.2108908406904906
Epoch: 5| Step: 9
Training loss: 1.8784942492472412
Validation loss: 2.2269533996278983
Epoch: 73| Step: 0
Training loss: 2.4968014282540265
Validation loss: 2.211021620760182
Epoch: 5| Step: 1
Training loss: 2.762653543339248
Validation loss: 2.2354535464349206
Epoch: 5| Step: 2
Training loss: 2.4468722015793585
Validation loss: 2.2443880522617574
Epoch: 5| Step: 3
Training loss: 2.6849106135487233
Validation loss: 2.23236180021453
Epoch: 5| Step: 4
Training loss: 2.6379109717812588
Validation loss: 2.2587711687373173
Epoch: 5| Step: 5
Training loss: 3.0589094327929662
Validation loss: 2.2563948428316736
Epoch: 5| Step: 6
Training loss: 2.4498813833989206
Validation loss: 2.2267246633816544
Epoch: 5| Step: 7
Training loss: 2.3240933937439427
Validation loss: 2.2523876829090534
Epoch: 5| Step: 8
Training loss: 2.2729328027904776
Validation loss: 2.244598553125299
Epoch: 5| Step: 9
Training loss: 3.037635762512649
Validation loss: 2.216387781930852
Epoch: 74| Step: 0
Training loss: 2.7089518989976114
Validation loss: 2.2086692073818988
Epoch: 5| Step: 1
Training loss: 2.276857405957652
Validation loss: 2.252550456048941
Epoch: 5| Step: 2
Training loss: 2.663292379179776
Validation loss: 2.2662294922726254
Epoch: 5| Step: 3
Training loss: 2.712380412757311
Validation loss: 2.2395513632076653
Epoch: 5| Step: 4
Training loss: 2.9614627744623716
Validation loss: 2.247577929908931
Epoch: 5| Step: 5
Training loss: 2.4504556893835736
Validation loss: 2.2259508362762017
Epoch: 5| Step: 6
Training loss: 2.400911229714916
Validation loss: 2.2270117744208733
Epoch: 5| Step: 7
Training loss: 2.459879329792149
Validation loss: 2.2444607663727276
Epoch: 5| Step: 8
Training loss: 2.3492813959576186
Validation loss: 2.2462792978575834
Epoch: 5| Step: 9
Training loss: 3.147715142542493
Validation loss: 2.2284219345892096
Epoch: 75| Step: 0
Training loss: 2.3238517920333623
Validation loss: 2.2296766498301075
Epoch: 5| Step: 1
Training loss: 2.666996210319421
Validation loss: 2.2048344615161612
Epoch: 5| Step: 2
Training loss: 2.4352953795510808
Validation loss: 2.1981416029743146
Epoch: 5| Step: 3
Training loss: 3.0387354971485854
Validation loss: 2.2213467765946273
Epoch: 5| Step: 4
Training loss: 2.2765518303426724
Validation loss: 2.2223902429067173
Epoch: 5| Step: 5
Training loss: 2.6608231614568845
Validation loss: 2.247473001345471
Epoch: 5| Step: 6
Training loss: 2.7097774079181978
Validation loss: 2.2345083549986793
Epoch: 5| Step: 7
Training loss: 2.67883749957348
Validation loss: 2.263923413140977
Epoch: 5| Step: 8
Training loss: 2.3642344168348837
Validation loss: 2.1920563966170885
Epoch: 5| Step: 9
Training loss: 2.9208532168536467
Validation loss: 2.2427938097307094
Epoch: 76| Step: 0
Training loss: 3.0472211592224303
Validation loss: 2.1968454612327926
Epoch: 5| Step: 1
Training loss: 2.618797239910843
Validation loss: 2.2297628344478135
Epoch: 5| Step: 2
Training loss: 2.6446282553524965
Validation loss: 2.217570398023187
Epoch: 5| Step: 3
Training loss: 2.5239159098067017
Validation loss: 2.2556893263959203
Epoch: 5| Step: 4
Training loss: 2.153948453947016
Validation loss: 2.1897920219933704
Epoch: 5| Step: 5
Training loss: 2.775145786339343
Validation loss: 2.219457480332529
Epoch: 5| Step: 6
Training loss: 2.5206920699795354
Validation loss: 2.2268169167700402
Epoch: 5| Step: 7
Training loss: 2.4726864299167777
Validation loss: 2.2155045365586217
Epoch: 5| Step: 8
Training loss: 2.629291296502309
Validation loss: 2.239353563251714
Epoch: 5| Step: 9
Training loss: 2.717660093531229
Validation loss: 2.2098394468666682
Epoch: 77| Step: 0
Training loss: 2.661070903092934
Validation loss: 2.2189094537303107
Epoch: 5| Step: 1
Training loss: 2.118916048728127
Validation loss: 2.2090660101573376
Epoch: 5| Step: 2
Training loss: 2.2851062736589043
Validation loss: 2.2093930903445704
Epoch: 5| Step: 3
Training loss: 3.6690492836113346
Validation loss: 2.227093397538503
Epoch: 5| Step: 4
Training loss: 2.980804064525837
Validation loss: 2.1924241103172366
Epoch: 5| Step: 5
Training loss: 2.5881058283162326
Validation loss: 2.244693453915235
Epoch: 5| Step: 6
Training loss: 2.1240057021529672
Validation loss: 2.2440638128084913
Epoch: 5| Step: 7
Training loss: 2.5882078962354904
Validation loss: 2.2375692734627948
Epoch: 5| Step: 8
Training loss: 2.553925566835071
Validation loss: 2.2264476689673294
Epoch: 5| Step: 9
Training loss: 2.2588851123250144
Validation loss: 2.222878949352915
Epoch: 78| Step: 0
Training loss: 2.6337384596374016
Validation loss: 2.2516119185283405
Epoch: 5| Step: 1
Training loss: 2.1830514633676015
Validation loss: 2.2399680903541253
Epoch: 5| Step: 2
Training loss: 2.3608122231288866
Validation loss: 2.219483027153763
Epoch: 5| Step: 3
Training loss: 2.848997535412607
Validation loss: 2.2277303287695145
Epoch: 5| Step: 4
Training loss: 2.3137444936397245
Validation loss: 2.211044540423693
Epoch: 5| Step: 5
Training loss: 2.6682269975329627
Validation loss: 2.2333685514696766
Epoch: 5| Step: 6
Training loss: 2.8112463806319017
Validation loss: 2.182721828757807
Epoch: 5| Step: 7
Training loss: 2.6610692007878782
Validation loss: 2.223959161801873
Epoch: 5| Step: 8
Training loss: 2.6401494066269704
Validation loss: 2.2416824548002534
Epoch: 5| Step: 9
Training loss: 2.961902310240862
Validation loss: 2.252996930848211
Epoch: 79| Step: 0
Training loss: 2.288062179774226
Validation loss: 2.2218485463433413
Epoch: 5| Step: 1
Training loss: 2.7158513017660675
Validation loss: 2.211318598595047
Epoch: 5| Step: 2
Training loss: 2.622351263754094
Validation loss: 2.2140886036237264
Epoch: 5| Step: 3
Training loss: 2.4591030489135224
Validation loss: 2.2165167555863965
Epoch: 5| Step: 4
Training loss: 2.5727291167522504
Validation loss: 2.2208988835156283
Epoch: 5| Step: 5
Training loss: 2.7470239661816507
Validation loss: 2.2127059093488066
Epoch: 5| Step: 6
Training loss: 3.285927336165109
Validation loss: 2.250728625113794
Epoch: 5| Step: 7
Training loss: 2.5416503864037914
Validation loss: 2.214266132140096
Epoch: 5| Step: 8
Training loss: 2.4534334608391433
Validation loss: 2.228486117858766
Epoch: 5| Step: 9
Training loss: 2.313642477363675
Validation loss: 2.2086929721704127
Epoch: 80| Step: 0
Training loss: 2.1963570169984123
Validation loss: 2.20350998207983
Epoch: 5| Step: 1
Training loss: 2.3329182891467606
Validation loss: 2.1832426905164075
Epoch: 5| Step: 2
Training loss: 2.4444046041343848
Validation loss: 2.220874845530985
Epoch: 5| Step: 3
Training loss: 2.83959791963842
Validation loss: 2.221214348166113
Epoch: 5| Step: 4
Training loss: 3.128488958109129
Validation loss: 2.2183413881596437
Epoch: 5| Step: 5
Training loss: 2.265761798806455
Validation loss: 2.2009154543755667
Epoch: 5| Step: 6
Training loss: 2.780893538702298
Validation loss: 2.2198162237896772
Epoch: 5| Step: 7
Training loss: 2.3803306787434466
Validation loss: 2.219513234554567
Epoch: 5| Step: 8
Training loss: 3.0190146573075127
Validation loss: 2.2047564701340736
Epoch: 5| Step: 9
Training loss: 2.3605228688611564
Validation loss: 2.1823403587281827
Epoch: 81| Step: 0
Training loss: 2.4317769085177803
Validation loss: 2.2241896000266057
Epoch: 5| Step: 1
Training loss: 2.6551487154357893
Validation loss: 2.2061024826524207
Epoch: 5| Step: 2
Training loss: 2.722098382308455
Validation loss: 2.2472118254838924
Epoch: 5| Step: 3
Training loss: 2.0619110653750594
Validation loss: 2.219299206446025
Epoch: 5| Step: 4
Training loss: 2.7369498200783573
Validation loss: 2.23513471131852
Epoch: 5| Step: 5
Training loss: 2.794383468325007
Validation loss: 2.21509082753992
Epoch: 5| Step: 6
Training loss: 2.15957738698025
Validation loss: 2.1994281018989947
Epoch: 5| Step: 7
Training loss: 2.758124228519943
Validation loss: 2.228089351774041
Epoch: 5| Step: 8
Training loss: 2.7956386795920274
Validation loss: 2.1906540588688013
Epoch: 5| Step: 9
Training loss: 2.7553554752819127
Validation loss: 2.233474959610191
Epoch: 82| Step: 0
Training loss: 3.0816944165490754
Validation loss: 2.204611820558215
Epoch: 5| Step: 1
Training loss: 2.552451832435448
Validation loss: 2.2147725990904195
Epoch: 5| Step: 2
Training loss: 2.462417206122731
Validation loss: 2.2466003088378996
Epoch: 5| Step: 3
Training loss: 2.5084361789272656
Validation loss: 2.233442791796458
Epoch: 5| Step: 4
Training loss: 3.017255589306072
Validation loss: 2.2285976277631545
Epoch: 5| Step: 5
Training loss: 2.772483984141618
Validation loss: 2.2119844029228966
Epoch: 5| Step: 6
Training loss: 2.204109350756067
Validation loss: 2.1881745371952337
Epoch: 5| Step: 7
Training loss: 2.19955544748591
Validation loss: 2.195034427926427
Epoch: 5| Step: 8
Training loss: 2.6925202432507245
Validation loss: 2.2228488468749803
Epoch: 5| Step: 9
Training loss: 2.3050353741362164
Validation loss: 2.2060696494355194
Epoch: 83| Step: 0
Training loss: 2.473659028804029
Validation loss: 2.2058184972629338
Epoch: 5| Step: 1
Training loss: 2.490176546509923
Validation loss: 2.2360717271659567
Epoch: 5| Step: 2
Training loss: 2.7216465745765555
Validation loss: 2.2093182349265743
Epoch: 5| Step: 3
Training loss: 2.8198987493532464
Validation loss: 2.1937303405728796
Epoch: 5| Step: 4
Training loss: 2.6095623017662017
Validation loss: 2.206793747020172
Epoch: 5| Step: 5
Training loss: 2.3912879174312605
Validation loss: 2.1833335741591937
Epoch: 5| Step: 6
Training loss: 2.4397056577155087
Validation loss: 2.2707109262377716
Epoch: 5| Step: 7
Training loss: 2.6882578646178437
Validation loss: 2.245718730302504
Epoch: 5| Step: 8
Training loss: 3.0586529917235294
Validation loss: 2.2371687639321687
Epoch: 5| Step: 9
Training loss: 2.225321105760397
Validation loss: 2.2217665267593523
Epoch: 84| Step: 0
Training loss: 2.2864231347319763
Validation loss: 2.210066482712587
Epoch: 5| Step: 1
Training loss: 2.4460125490866367
Validation loss: 2.194548062471227
Epoch: 5| Step: 2
Training loss: 2.331288486193713
Validation loss: 2.2051094498257315
Epoch: 5| Step: 3
Training loss: 2.9146320512547823
Validation loss: 2.2143842358506967
Epoch: 5| Step: 4
Training loss: 2.9891778934616395
Validation loss: 2.2485791472504575
Epoch: 5| Step: 5
Training loss: 2.450938617200693
Validation loss: 2.2012958469918193
Epoch: 5| Step: 6
Training loss: 2.3097525075136462
Validation loss: 2.2162617737418233
Epoch: 5| Step: 7
Training loss: 2.5018147557610164
Validation loss: 2.2014815974472675
Epoch: 5| Step: 8
Training loss: 3.057127619451993
Validation loss: 2.21503300012211
Epoch: 5| Step: 9
Training loss: 2.6366646768182256
Validation loss: 2.1998947646263325
Epoch: 85| Step: 0
Training loss: 2.4757445515939875
Validation loss: 2.2276900588435042
Epoch: 5| Step: 1
Training loss: 2.360620738200612
Validation loss: 2.2169156329241804
Epoch: 5| Step: 2
Training loss: 2.6008575675769925
Validation loss: 2.2384364629979525
Epoch: 5| Step: 3
Training loss: 2.469994533084889
Validation loss: 2.221443849918184
Epoch: 5| Step: 4
Training loss: 2.5011090679584966
Validation loss: 2.231498122217107
Epoch: 5| Step: 5
Training loss: 2.6719208652062125
Validation loss: 2.2265744777958565
Epoch: 5| Step: 6
Training loss: 2.3884489839730194
Validation loss: 2.20035754993631
Epoch: 5| Step: 7
Training loss: 3.2963341725856554
Validation loss: 2.2039615279364573
Epoch: 5| Step: 8
Training loss: 2.5652368398265053
Validation loss: 2.2250790875490236
Epoch: 5| Step: 9
Training loss: 2.533146277431726
Validation loss: 2.23397302626411
Epoch: 86| Step: 0
Training loss: 2.476330189684628
Validation loss: 2.2110877731912435
Epoch: 5| Step: 1
Training loss: 2.2411078535321165
Validation loss: 2.214069368859923
Epoch: 5| Step: 2
Training loss: 2.4385997540968365
Validation loss: 2.201375202444028
Epoch: 5| Step: 3
Training loss: 2.206381048789736
Validation loss: 2.208238746072936
Epoch: 5| Step: 4
Training loss: 2.886374983495582
Validation loss: 2.2191744494311463
Epoch: 5| Step: 5
Training loss: 2.351498422351643
Validation loss: 2.1930659168282816
Epoch: 5| Step: 6
Training loss: 2.778304951764435
Validation loss: 2.198492814035282
Epoch: 5| Step: 7
Training loss: 2.914767019079346
Validation loss: 2.2059369899868697
Epoch: 5| Step: 8
Training loss: 2.6566605194607797
Validation loss: 2.1932896452074506
Epoch: 5| Step: 9
Training loss: 2.8808817199874945
Validation loss: 2.2192447982591554
Epoch: 87| Step: 0
Training loss: 2.811409124108862
Validation loss: 2.175067235251356
Epoch: 5| Step: 1
Training loss: 2.6434915547042097
Validation loss: 2.1991205818256434
Epoch: 5| Step: 2
Training loss: 2.073040126274169
Validation loss: 2.2036844466602927
Epoch: 5| Step: 3
Training loss: 3.229142974438455
Validation loss: 2.2172860831264654
Epoch: 5| Step: 4
Training loss: 2.729344039472346
Validation loss: 2.177789234455925
Epoch: 5| Step: 5
Training loss: 2.7162093645506697
Validation loss: 2.188199083597826
Epoch: 5| Step: 6
Training loss: 2.0587586657139645
Validation loss: 2.1872762198986906
Epoch: 5| Step: 7
Training loss: 2.955244645836965
Validation loss: 2.215876688885748
Epoch: 5| Step: 8
Training loss: 2.128079371026186
Validation loss: 2.181015471678106
Epoch: 5| Step: 9
Training loss: 2.3085340566388926
Validation loss: 2.2130689466145252
Epoch: 88| Step: 0
Training loss: 2.840878388672432
Validation loss: 2.2166944578041337
Epoch: 5| Step: 1
Training loss: 2.518729622050883
Validation loss: 2.208085027466266
Epoch: 5| Step: 2
Training loss: 2.437047769703669
Validation loss: 2.198162380661578
Epoch: 5| Step: 3
Training loss: 2.5241178652391865
Validation loss: 2.2065215403822203
Epoch: 5| Step: 4
Training loss: 2.6898415589025895
Validation loss: 2.2163974882255104
Epoch: 5| Step: 5
Training loss: 2.4134314246115407
Validation loss: 2.2097358499748054
Epoch: 5| Step: 6
Training loss: 2.502911874601314
Validation loss: 2.2005321502033373
Epoch: 5| Step: 7
Training loss: 2.7585105120714584
Validation loss: 2.229525561071911
Epoch: 5| Step: 8
Training loss: 2.5604614895362503
Validation loss: 2.1759117030413364
Epoch: 5| Step: 9
Training loss: 2.5977804283347568
Validation loss: 2.2006142225658722
Epoch: 89| Step: 0
Training loss: 2.517290026366303
Validation loss: 2.224815553785376
Epoch: 5| Step: 1
Training loss: 2.5691976709399533
Validation loss: 2.1979545623237073
Epoch: 5| Step: 2
Training loss: 2.2095757594210466
Validation loss: 2.20429795586069
Epoch: 5| Step: 3
Training loss: 2.396911931157484
Validation loss: 2.2068100017815913
Epoch: 5| Step: 4
Training loss: 2.7962695270891613
Validation loss: 2.191781104529719
Epoch: 5| Step: 5
Training loss: 2.576127636857187
Validation loss: 2.194669530031534
Epoch: 5| Step: 6
Training loss: 2.13360960880114
Validation loss: 2.1920845447113684
Epoch: 5| Step: 7
Training loss: 2.6949249431229365
Validation loss: 2.192544312574698
Epoch: 5| Step: 8
Training loss: 3.1972466842845355
Validation loss: 2.1618789126300255
Epoch: 5| Step: 9
Training loss: 2.6940729359224527
Validation loss: 2.1991629439209404
Epoch: 90| Step: 0
Training loss: 2.537850522039625
Validation loss: 2.231551318136499
Epoch: 5| Step: 1
Training loss: 3.037787084003225
Validation loss: 2.213501854020878
Epoch: 5| Step: 2
Training loss: 2.739234051116862
Validation loss: 2.236241476377402
Epoch: 5| Step: 3
Training loss: 2.4003546691138418
Validation loss: 2.187199163718915
Epoch: 5| Step: 4
Training loss: 2.328901839739985
Validation loss: 2.2389172401773103
Epoch: 5| Step: 5
Training loss: 2.11222657911527
Validation loss: 2.200215339470335
Epoch: 5| Step: 6
Training loss: 2.3813033530999075
Validation loss: 2.2121951194532725
Epoch: 5| Step: 7
Training loss: 2.6823101944268073
Validation loss: 2.2022058330618663
Epoch: 5| Step: 8
Training loss: 2.6912957820727885
Validation loss: 2.2163660921015693
Epoch: 5| Step: 9
Training loss: 2.8620666175835825
Validation loss: 2.2168016989350483
Epoch: 91| Step: 0
Training loss: 2.6470900627237635
Validation loss: 2.20985826990249
Epoch: 5| Step: 1
Training loss: 2.4728494722342282
Validation loss: 2.2376577579074373
Epoch: 5| Step: 2
Training loss: 2.086719380688902
Validation loss: 2.1970167148899353
Epoch: 5| Step: 3
Training loss: 2.5791390014380795
Validation loss: 2.203860134093107
Epoch: 5| Step: 4
Training loss: 2.227120098765707
Validation loss: 2.2137435314660623
Epoch: 5| Step: 5
Training loss: 2.8090271595463165
Validation loss: 2.2217328819991065
Epoch: 5| Step: 6
Training loss: 2.9190913521513147
Validation loss: 2.194697501561792
Epoch: 5| Step: 7
Training loss: 2.6578569824515124
Validation loss: 2.1860559065684826
Epoch: 5| Step: 8
Training loss: 2.7119602176088997
Validation loss: 2.1940912515561055
Epoch: 5| Step: 9
Training loss: 2.6493196621762727
Validation loss: 2.2059799193535845
Epoch: 92| Step: 0
Training loss: 2.667457989663086
Validation loss: 2.2217005617067165
Epoch: 5| Step: 1
Training loss: 2.453181175026101
Validation loss: 2.2239685201562245
Epoch: 5| Step: 2
Training loss: 2.552754455347332
Validation loss: 2.1781195174351824
Epoch: 5| Step: 3
Training loss: 3.2972731508047777
Validation loss: 2.205250883189826
Epoch: 5| Step: 4
Training loss: 1.952381450263125
Validation loss: 2.2323365973954905
Epoch: 5| Step: 5
Training loss: 3.0862687717761434
Validation loss: 2.176149785912782
Epoch: 5| Step: 6
Training loss: 2.4252729635870875
Validation loss: 2.1760824363760776
Epoch: 5| Step: 7
Training loss: 2.4927135139620855
Validation loss: 2.1931853584385537
Epoch: 5| Step: 8
Training loss: 2.151407415470182
Validation loss: 2.18712051037039
Epoch: 5| Step: 9
Training loss: 2.5266455698600385
Validation loss: 2.1996998514798087
Epoch: 93| Step: 0
Training loss: 2.212326853383372
Validation loss: 2.1688935199342447
Epoch: 5| Step: 1
Training loss: 2.8527254424411703
Validation loss: 2.1920218475696567
Epoch: 5| Step: 2
Training loss: 2.541202148347244
Validation loss: 2.195769070938032
Epoch: 5| Step: 3
Training loss: 2.6278216319583847
Validation loss: 2.1879717321567553
Epoch: 5| Step: 4
Training loss: 2.4364659977259997
Validation loss: 2.1871206696028507
Epoch: 5| Step: 5
Training loss: 2.1035208387673316
Validation loss: 2.243854626024152
Epoch: 5| Step: 6
Training loss: 2.248740585502523
Validation loss: 2.193273356375419
Epoch: 5| Step: 7
Training loss: 3.2436990546487054
Validation loss: 2.1945435146057033
Epoch: 5| Step: 8
Training loss: 2.570587700022047
Validation loss: 2.193058990783594
Epoch: 5| Step: 9
Training loss: 2.655321564822209
Validation loss: 2.1871184891311604
Epoch: 94| Step: 0
Training loss: 1.747077818204601
Validation loss: 2.1973372234060444
Epoch: 5| Step: 1
Training loss: 2.7857696157417156
Validation loss: 2.176300860384436
Epoch: 5| Step: 2
Training loss: 2.1089343705725705
Validation loss: 2.2054122314276117
Epoch: 5| Step: 3
Training loss: 2.5744043494596185
Validation loss: 2.210459341048086
Epoch: 5| Step: 4
Training loss: 2.50091860107084
Validation loss: 2.2003075934425547
Epoch: 5| Step: 5
Training loss: 3.146905038608805
Validation loss: 2.216860794894806
Epoch: 5| Step: 6
Training loss: 2.3741632292501573
Validation loss: 2.175576019812844
Epoch: 5| Step: 7
Training loss: 2.5871522015484616
Validation loss: 2.196234006805632
Epoch: 5| Step: 8
Training loss: 2.8978642721990777
Validation loss: 2.195023035131848
Epoch: 5| Step: 9
Training loss: 2.751163149983281
Validation loss: 2.1920618721661977
Epoch: 95| Step: 0
Training loss: 2.555190010694542
Validation loss: 2.167178632921239
Epoch: 5| Step: 1
Training loss: 3.0174899486510243
Validation loss: 2.1880413475698983
Epoch: 5| Step: 2
Training loss: 2.534359093932486
Validation loss: 2.1883973985019933
Epoch: 5| Step: 3
Training loss: 2.625168749516865
Validation loss: 2.2161279150974442
Epoch: 5| Step: 4
Training loss: 2.150582979695825
Validation loss: 2.169898185333065
Epoch: 5| Step: 5
Training loss: 2.5742128224514453
Validation loss: 2.219350297506536
Epoch: 5| Step: 6
Training loss: 2.7920444479631508
Validation loss: 2.151420297940926
Epoch: 5| Step: 7
Training loss: 2.5213927496847535
Validation loss: 2.185085190890192
Epoch: 5| Step: 8
Training loss: 2.3287764284330708
Validation loss: 2.1764156078384023
Epoch: 5| Step: 9
Training loss: 2.5838897064928728
Validation loss: 2.20435635531099
Epoch: 96| Step: 0
Training loss: 3.1260821185513135
Validation loss: 2.1966666927365868
Epoch: 5| Step: 1
Training loss: 2.5971658114621374
Validation loss: 2.1452481974254747
Epoch: 5| Step: 2
Training loss: 2.5015967038524787
Validation loss: 2.1529895704149635
Epoch: 5| Step: 3
Training loss: 2.1711244452613774
Validation loss: 2.160198128253332
Epoch: 5| Step: 4
Training loss: 2.385509128250059
Validation loss: 2.20290427949184
Epoch: 5| Step: 5
Training loss: 2.687619051403362
Validation loss: 2.1676053471220347
Epoch: 5| Step: 6
Training loss: 2.3048366692324165
Validation loss: 2.1699805636034695
Epoch: 5| Step: 7
Training loss: 2.3942851053647485
Validation loss: 2.2188643106822385
Epoch: 5| Step: 8
Training loss: 2.3771262438882634
Validation loss: 2.1666284704448384
Epoch: 5| Step: 9
Training loss: 3.0305530494083617
Validation loss: 2.1782737014397346
Epoch: 97| Step: 0
Training loss: 2.628460011579157
Validation loss: 2.1996679284994616
Epoch: 5| Step: 1
Training loss: 2.366859740678238
Validation loss: 2.2033013767550647
Epoch: 5| Step: 2
Training loss: 2.6533228284861092
Validation loss: 2.198421692588679
Epoch: 5| Step: 3
Training loss: 2.715650787055491
Validation loss: 2.2003990169660415
Epoch: 5| Step: 4
Training loss: 2.776988160967703
Validation loss: 2.199945699188581
Epoch: 5| Step: 5
Training loss: 2.456697329101651
Validation loss: 2.216587639703009
Epoch: 5| Step: 6
Training loss: 2.8965461091596305
Validation loss: 2.2036814730604988
Epoch: 5| Step: 7
Training loss: 2.1016564330482854
Validation loss: 2.2089739952918537
Epoch: 5| Step: 8
Training loss: 2.0620228764609028
Validation loss: 2.200494839730346
Epoch: 5| Step: 9
Training loss: 2.937133116841135
Validation loss: 2.228464442111893
Epoch: 98| Step: 0
Training loss: 2.4360436221258737
Validation loss: 2.2095512023440844
Epoch: 5| Step: 1
Training loss: 2.7017847167229254
Validation loss: 2.2042849359939436
Epoch: 5| Step: 2
Training loss: 2.6469243320880564
Validation loss: 2.1947817364384874
Epoch: 5| Step: 3
Training loss: 2.6921682400271876
Validation loss: 2.197094148881652
Epoch: 5| Step: 4
Training loss: 2.0101985780163814
Validation loss: 2.196015177474586
Epoch: 5| Step: 5
Training loss: 2.4801840310880086
Validation loss: 2.199433380982158
Epoch: 5| Step: 6
Training loss: 2.482262823140316
Validation loss: 2.1628973232872655
Epoch: 5| Step: 7
Training loss: 2.2073491500515168
Validation loss: 2.1911008583520077
Epoch: 5| Step: 8
Training loss: 3.093816583090324
Validation loss: 2.190851161338961
Epoch: 5| Step: 9
Training loss: 2.8193128510214707
Validation loss: 2.15553601117437
Epoch: 99| Step: 0
Training loss: 3.0823564442467792
Validation loss: 2.1725173320634705
Epoch: 5| Step: 1
Training loss: 2.384405285404198
Validation loss: 2.1997380392638344
Epoch: 5| Step: 2
Training loss: 2.6344059952511003
Validation loss: 2.196037504107004
Epoch: 5| Step: 3
Training loss: 2.5122068888939464
Validation loss: 2.207914252417763
Epoch: 5| Step: 4
Training loss: 2.3015911444948767
Validation loss: 2.1894951813824854
Epoch: 5| Step: 5
Training loss: 2.594978225557853
Validation loss: 2.198665163699546
Epoch: 5| Step: 6
Training loss: 2.1250031415130732
Validation loss: 2.1757077727903993
Epoch: 5| Step: 7
Training loss: 2.8423804034703424
Validation loss: 2.171583103690586
Epoch: 5| Step: 8
Training loss: 2.2237776624085854
Validation loss: 2.1764597965403185
Epoch: 5| Step: 9
Training loss: 2.7125671061991343
Validation loss: 2.1676286207094195
Epoch: 100| Step: 0
Training loss: 2.739327006562571
Validation loss: 2.2045596042609903
Epoch: 5| Step: 1
Training loss: 2.502142750852175
Validation loss: 2.1869254871434567
Epoch: 5| Step: 2
Training loss: 2.449816373833543
Validation loss: 2.1956904558043444
Epoch: 5| Step: 3
Training loss: 2.547743479555186
Validation loss: 2.1720262281117324
Epoch: 5| Step: 4
Training loss: 2.9414036562073558
Validation loss: 2.1742338398084757
Epoch: 5| Step: 5
Training loss: 2.8658939946676107
Validation loss: 2.197071276045729
Epoch: 5| Step: 6
Training loss: 2.068498039460446
Validation loss: 2.1968892212864177
Epoch: 5| Step: 7
Training loss: 2.5380887555895284
Validation loss: 2.1962039210275077
Epoch: 5| Step: 8
Training loss: 2.7091412048488834
Validation loss: 2.169589262367233
Epoch: 5| Step: 9
Training loss: 2.0290714024839476
Validation loss: 2.189939773427726

Epoch: 1| Step: 0
Training loss: 5.492857456207275
Validation loss: 5.141041914621989

Epoch: 5| Step: 1
Training loss: 5.213986396789551
Validation loss: 5.118705400856593

Epoch: 5| Step: 2
Training loss: 3.1650397777557373
Validation loss: 5.0976442316527

Epoch: 5| Step: 3
Training loss: 4.904778957366943
Validation loss: 5.07610878380396

Epoch: 5| Step: 4
Training loss: 5.46966552734375
Validation loss: 5.05234291220224

Epoch: 5| Step: 5
Training loss: 5.178200721740723
Validation loss: 5.026169330843033

Epoch: 5| Step: 6
Training loss: 5.413359642028809
Validation loss: 4.995920955493886

Epoch: 5| Step: 7
Training loss: 3.6198654174804688
Validation loss: 4.962102300377302

Epoch: 5| Step: 8
Training loss: 4.552174091339111
Validation loss: 4.923202212138842

Epoch: 5| Step: 9
Training loss: 4.570729732513428
Validation loss: 4.879739704952445

Epoch: 5| Step: 10
Training loss: 5.3422932624816895
Validation loss: 4.829660025976038

Epoch: 2| Step: 0
Training loss: 4.468483924865723
Validation loss: 4.774760764132264

Epoch: 5| Step: 1
Training loss: 4.223184108734131
Validation loss: 4.712964170722551

Epoch: 5| Step: 2
Training loss: 5.500067234039307
Validation loss: 4.647578470168575

Epoch: 5| Step: 3
Training loss: 4.473262786865234
Validation loss: 4.574690216331072

Epoch: 5| Step: 4
Training loss: 4.9456586837768555
Validation loss: 4.496632504206832

Epoch: 5| Step: 5
Training loss: 3.846006393432617
Validation loss: 4.413914152370986

Epoch: 5| Step: 6
Training loss: 4.332148551940918
Validation loss: 4.328778161797472

Epoch: 5| Step: 7
Training loss: 3.732506513595581
Validation loss: 4.24182782634612

Epoch: 5| Step: 8
Training loss: 3.037325620651245
Validation loss: 4.15885526903214

Epoch: 5| Step: 9
Training loss: 3.2015273571014404
Validation loss: 4.077583115587952

Epoch: 5| Step: 10
Training loss: 4.573967456817627
Validation loss: 3.999230812954646

Epoch: 3| Step: 0
Training loss: 3.1649584770202637
Validation loss: 3.9216243067095355

Epoch: 5| Step: 1
Training loss: 3.1126880645751953
Validation loss: 3.8512125194713636

Epoch: 5| Step: 2
Training loss: 4.084381103515625
Validation loss: 3.7952407252404

Epoch: 5| Step: 3
Training loss: 4.126168251037598
Validation loss: 3.7462853923920663

Epoch: 5| Step: 4
Training loss: 3.526123523712158
Validation loss: 3.706286343195105

Epoch: 5| Step: 5
Training loss: 2.942974805831909
Validation loss: 3.6681018260217484

Epoch: 5| Step: 6
Training loss: 4.6030778884887695
Validation loss: 3.627027814106275

Epoch: 5| Step: 7
Training loss: 3.9805712699890137
Validation loss: 3.5893407637073147

Epoch: 5| Step: 8
Training loss: 3.487001419067383
Validation loss: 3.554191912374189

Epoch: 5| Step: 9
Training loss: 3.067720890045166
Validation loss: 3.518970843284361

Epoch: 5| Step: 10
Training loss: 3.417558431625366
Validation loss: 3.4879848418697232

Epoch: 4| Step: 0
Training loss: 2.7294015884399414
Validation loss: 3.4664968700819117

Epoch: 5| Step: 1
Training loss: 3.377842664718628
Validation loss: 3.460065967293196

Epoch: 5| Step: 2
Training loss: 4.292768955230713
Validation loss: 3.432999369918659

Epoch: 5| Step: 3
Training loss: 4.060030460357666
Validation loss: 3.4025950611278577

Epoch: 5| Step: 4
Training loss: 3.416388988494873
Validation loss: 3.380103844468312

Epoch: 5| Step: 5
Training loss: 2.371752977371216
Validation loss: 3.349885840569773

Epoch: 5| Step: 6
Training loss: 3.005579710006714
Validation loss: 3.334767121140675

Epoch: 5| Step: 7
Training loss: 3.4765353202819824
Validation loss: 3.3157489889411518

Epoch: 5| Step: 8
Training loss: 3.8424925804138184
Validation loss: 3.2795764451385825

Epoch: 5| Step: 9
Training loss: 2.8397674560546875
Validation loss: 3.2573555233658

Epoch: 5| Step: 10
Training loss: 3.1627464294433594
Validation loss: 3.2402716400802776

Epoch: 5| Step: 0
Training loss: 2.924360752105713
Validation loss: 3.2186916002663235

Epoch: 5| Step: 1
Training loss: 3.197288751602173
Validation loss: 3.192511340623261

Epoch: 5| Step: 2
Training loss: 3.2574832439422607
Validation loss: 3.145102926479873

Epoch: 5| Step: 3
Training loss: 2.584810733795166
Validation loss: 3.112121935813658

Epoch: 5| Step: 4
Training loss: 3.0715842247009277
Validation loss: 3.103860419283631

Epoch: 5| Step: 5
Training loss: 3.413980484008789
Validation loss: 3.101323296946864

Epoch: 5| Step: 6
Training loss: 3.976116180419922
Validation loss: 3.0953903916061565

Epoch: 5| Step: 7
Training loss: 3.150449514389038
Validation loss: 3.0686590415175243

Epoch: 5| Step: 8
Training loss: 2.9575071334838867
Validation loss: 3.0711548046399186

Epoch: 5| Step: 9
Training loss: 2.863553524017334
Validation loss: 3.0949768045897126

Epoch: 5| Step: 10
Training loss: 3.0643904209136963
Validation loss: 3.0623884688141527

Epoch: 6| Step: 0
Training loss: 3.4249966144561768
Validation loss: 3.0280368763913392

Epoch: 5| Step: 1
Training loss: 2.7467501163482666
Validation loss: 3.0255180430668656

Epoch: 5| Step: 2
Training loss: 2.8931808471679688
Validation loss: 3.0194557815469723

Epoch: 5| Step: 3
Training loss: 3.1040432453155518
Validation loss: 3.0121809385156118

Epoch: 5| Step: 4
Training loss: 2.867549419403076
Validation loss: 3.000281059613792

Epoch: 5| Step: 5
Training loss: 2.821702003479004
Validation loss: 2.9781670647282756

Epoch: 5| Step: 6
Training loss: 2.8479621410369873
Validation loss: 2.9667767914392615

Epoch: 5| Step: 7
Training loss: 3.3595948219299316
Validation loss: 2.9646210362834315

Epoch: 5| Step: 8
Training loss: 3.362278699874878
Validation loss: 2.9511086812583347

Epoch: 5| Step: 9
Training loss: 2.6068215370178223
Validation loss: 2.9437684423180035

Epoch: 5| Step: 10
Training loss: 3.7047812938690186
Validation loss: 2.9401787839910036

Epoch: 7| Step: 0
Training loss: 2.9702396392822266
Validation loss: 2.924239609831123

Epoch: 5| Step: 1
Training loss: 2.7566559314727783
Validation loss: 2.9117998000114196

Epoch: 5| Step: 2
Training loss: 3.1253788471221924
Validation loss: 2.9039681060339815

Epoch: 5| Step: 3
Training loss: 2.9172158241271973
Validation loss: 2.8952964300750406

Epoch: 5| Step: 4
Training loss: 2.980971097946167
Validation loss: 2.888272459788989

Epoch: 5| Step: 5
Training loss: 2.865750551223755
Validation loss: 2.877432810362949

Epoch: 5| Step: 6
Training loss: 2.6240105628967285
Validation loss: 2.873663174208774

Epoch: 5| Step: 7
Training loss: 2.724982500076294
Validation loss: 2.8623161495372815

Epoch: 5| Step: 8
Training loss: 2.752624034881592
Validation loss: 2.8564346323731127

Epoch: 5| Step: 9
Training loss: 3.9804325103759766
Validation loss: 2.848507909364598

Epoch: 5| Step: 10
Training loss: 3.069197177886963
Validation loss: 2.8389119973746677

Epoch: 8| Step: 0
Training loss: 2.5830178260803223
Validation loss: 2.8317149659638763

Epoch: 5| Step: 1
Training loss: 3.294487714767456
Validation loss: 2.83271619068679

Epoch: 5| Step: 2
Training loss: 3.100100517272949
Validation loss: 2.8137855837422032

Epoch: 5| Step: 3
Training loss: 2.8987843990325928
Validation loss: 2.810512242778655

Epoch: 5| Step: 4
Training loss: 3.1000378131866455
Validation loss: 2.8024792030293453

Epoch: 5| Step: 5
Training loss: 2.8711600303649902
Validation loss: 2.7985133381300074

Epoch: 5| Step: 6
Training loss: 3.0149242877960205
Validation loss: 2.7944092673640095

Epoch: 5| Step: 7
Training loss: 2.8821330070495605
Validation loss: 2.790962109001734

Epoch: 5| Step: 8
Training loss: 3.1706953048706055
Validation loss: 2.7835181297794467

Epoch: 5| Step: 9
Training loss: 2.908522129058838
Validation loss: 2.7685290869846138

Epoch: 5| Step: 10
Training loss: 2.312887668609619
Validation loss: 2.76434632783295

Epoch: 9| Step: 0
Training loss: 2.6610894203186035
Validation loss: 2.7596955145559003

Epoch: 5| Step: 1
Training loss: 2.6537132263183594
Validation loss: 2.7588919285804994

Epoch: 5| Step: 2
Training loss: 3.2430973052978516
Validation loss: 2.7579413049964496

Epoch: 5| Step: 3
Training loss: 2.9789419174194336
Validation loss: 2.752081032722227

Epoch: 5| Step: 4
Training loss: 2.449568510055542
Validation loss: 2.744922120084045

Epoch: 5| Step: 5
Training loss: 3.1780269145965576
Validation loss: 2.743691090614565

Epoch: 5| Step: 6
Training loss: 2.9233028888702393
Validation loss: 2.731331361237393

Epoch: 5| Step: 7
Training loss: 2.6265177726745605
Validation loss: 2.7297581139431206

Epoch: 5| Step: 8
Training loss: 3.8945727348327637
Validation loss: 2.7343518375068583

Epoch: 5| Step: 9
Training loss: 2.5630905628204346
Validation loss: 2.721368210290068

Epoch: 5| Step: 10
Training loss: 2.6286733150482178
Validation loss: 2.722128898866715

Epoch: 10| Step: 0
Training loss: 2.9833292961120605
Validation loss: 2.714341443072083

Epoch: 5| Step: 1
Training loss: 2.7820301055908203
Validation loss: 2.705442085061022

Epoch: 5| Step: 2
Training loss: 2.4880313873291016
Validation loss: 2.6974784225545902

Epoch: 5| Step: 3
Training loss: 2.7293519973754883
Validation loss: 2.6925796026824624

Epoch: 5| Step: 4
Training loss: 2.5346646308898926
Validation loss: 2.689742362627419

Epoch: 5| Step: 5
Training loss: 2.5164778232574463
Validation loss: 2.689747484781409

Epoch: 5| Step: 6
Training loss: 3.422801971435547
Validation loss: 2.6900092119811685

Epoch: 5| Step: 7
Training loss: 2.643831253051758
Validation loss: 2.698494916321129

Epoch: 5| Step: 8
Training loss: 2.970895290374756
Validation loss: 2.683616489492437

Epoch: 5| Step: 9
Training loss: 3.4804317951202393
Validation loss: 2.671968011445897

Epoch: 5| Step: 10
Training loss: 2.92606258392334
Validation loss: 2.663917738904235

Epoch: 11| Step: 0
Training loss: 3.1522507667541504
Validation loss: 2.6615149923550185

Epoch: 5| Step: 1
Training loss: 3.186522960662842
Validation loss: 2.6646009991245885

Epoch: 5| Step: 2
Training loss: 2.960817337036133
Validation loss: 2.6681681807323168

Epoch: 5| Step: 3
Training loss: 3.2806899547576904
Validation loss: 2.676029389904391

Epoch: 5| Step: 4
Training loss: 2.183048963546753
Validation loss: 2.6533852623355005

Epoch: 5| Step: 5
Training loss: 2.920727252960205
Validation loss: 2.637102206548055

Epoch: 5| Step: 6
Training loss: 2.4567747116088867
Validation loss: 2.6347386247368267

Epoch: 5| Step: 7
Training loss: 2.611525297164917
Validation loss: 2.637329529690486

Epoch: 5| Step: 8
Training loss: 2.3211586475372314
Validation loss: 2.6346774895985923

Epoch: 5| Step: 9
Training loss: 3.0812828540802
Validation loss: 2.6351819320391585

Epoch: 5| Step: 10
Training loss: 3.0631942749023438
Validation loss: 2.649782462786603

Epoch: 12| Step: 0
Training loss: 2.957869291305542
Validation loss: 2.660570994500191

Epoch: 5| Step: 1
Training loss: 2.763488292694092
Validation loss: 2.70752074128838

Epoch: 5| Step: 2
Training loss: 3.550938844680786
Validation loss: 2.681608687164963

Epoch: 5| Step: 3
Training loss: 3.291330337524414
Validation loss: 2.662865856642364

Epoch: 5| Step: 4
Training loss: 3.0957961082458496
Validation loss: 2.657967036770236

Epoch: 5| Step: 5
Training loss: 2.6771113872528076
Validation loss: 2.6608493225548857

Epoch: 5| Step: 6
Training loss: 2.165391206741333
Validation loss: 2.6642447799764652

Epoch: 5| Step: 7
Training loss: 2.333387851715088
Validation loss: 2.709799387121713

Epoch: 5| Step: 8
Training loss: 3.030362606048584
Validation loss: 2.6606430930476033

Epoch: 5| Step: 9
Training loss: 3.3481743335723877
Validation loss: 2.64520279822811

Epoch: 5| Step: 10
Training loss: 1.9730803966522217
Validation loss: 2.640448518978652

Epoch: 13| Step: 0
Training loss: 2.3309502601623535
Validation loss: 2.6468750789601314

Epoch: 5| Step: 1
Training loss: 3.4174180030822754
Validation loss: 2.6538784375754734

Epoch: 5| Step: 2
Training loss: 2.656620502471924
Validation loss: 2.6514144456514748

Epoch: 5| Step: 3
Training loss: 2.159966230392456
Validation loss: 2.660515785217285

Epoch: 5| Step: 4
Training loss: 2.528008222579956
Validation loss: 2.6834761609313307

Epoch: 5| Step: 5
Training loss: 2.8552212715148926
Validation loss: 2.637873411178589

Epoch: 5| Step: 6
Training loss: 3.321168899536133
Validation loss: 2.621050129654587

Epoch: 5| Step: 7
Training loss: 2.6134767532348633
Validation loss: 2.629285068922145

Epoch: 5| Step: 8
Training loss: 2.9781317710876465
Validation loss: 2.6383041130599154

Epoch: 5| Step: 9
Training loss: 3.3162894248962402
Validation loss: 2.626052133498653

Epoch: 5| Step: 10
Training loss: 3.046405076980591
Validation loss: 2.6100625530365975

Epoch: 14| Step: 0
Training loss: 3.2343742847442627
Validation loss: 2.607297920411633

Epoch: 5| Step: 1
Training loss: 3.4037258625030518
Validation loss: 2.612005369637602

Epoch: 5| Step: 2
Training loss: 2.9243156909942627
Validation loss: 2.6168351968129477

Epoch: 5| Step: 3
Training loss: 2.5307984352111816
Validation loss: 2.621650908582954

Epoch: 5| Step: 4
Training loss: 2.5860671997070312
Validation loss: 2.6266306010625695

Epoch: 5| Step: 5
Training loss: 2.2002792358398438
Validation loss: 2.6174070271112586

Epoch: 5| Step: 6
Training loss: 1.9803593158721924
Validation loss: 2.6115106715950915

Epoch: 5| Step: 7
Training loss: 3.6360859870910645
Validation loss: 2.604131983172509

Epoch: 5| Step: 8
Training loss: 3.143306255340576
Validation loss: 2.597793274028327

Epoch: 5| Step: 9
Training loss: 2.5403554439544678
Validation loss: 2.589477062225342

Epoch: 5| Step: 10
Training loss: 2.62576961517334
Validation loss: 2.5811993998865925

Epoch: 15| Step: 0
Training loss: 2.719454288482666
Validation loss: 2.5835473255444596

Epoch: 5| Step: 1
Training loss: 2.859264850616455
Validation loss: 2.58667468255566

Epoch: 5| Step: 2
Training loss: 2.4639976024627686
Validation loss: 2.592814637768653

Epoch: 5| Step: 3
Training loss: 3.212315797805786
Validation loss: 2.592669515199559

Epoch: 5| Step: 4
Training loss: 3.0726475715637207
Validation loss: 2.5703094518312843

Epoch: 5| Step: 5
Training loss: 2.2319211959838867
Validation loss: 2.585012097512522

Epoch: 5| Step: 6
Training loss: 3.3857429027557373
Validation loss: 2.601304982298164

Epoch: 5| Step: 7
Training loss: 2.357940673828125
Validation loss: 2.5727672807631956

Epoch: 5| Step: 8
Training loss: 2.6646618843078613
Validation loss: 2.57228151957194

Epoch: 5| Step: 9
Training loss: 3.719306230545044
Validation loss: 2.573097029039937

Epoch: 5| Step: 10
Training loss: 1.8228209018707275
Validation loss: 2.571845257154075

Epoch: 16| Step: 0
Training loss: 2.4961018562316895
Validation loss: 2.587083783200992

Epoch: 5| Step: 1
Training loss: 3.3315224647521973
Validation loss: 2.5703639522675545

Epoch: 5| Step: 2
Training loss: 2.5074238777160645
Validation loss: 2.5633608833436043

Epoch: 5| Step: 3
Training loss: 2.3439278602600098
Validation loss: 2.564853634885562

Epoch: 5| Step: 4
Training loss: 2.9341259002685547
Validation loss: 2.565223447738155

Epoch: 5| Step: 5
Training loss: 2.506298542022705
Validation loss: 2.561982565028693

Epoch: 5| Step: 6
Training loss: 3.224285125732422
Validation loss: 2.5550783321421635

Epoch: 5| Step: 7
Training loss: 3.0117218494415283
Validation loss: 2.557289077389625

Epoch: 5| Step: 8
Training loss: 2.7994375228881836
Validation loss: 2.5617095783192623

Epoch: 5| Step: 9
Training loss: 2.79719877243042
Validation loss: 2.564783106568039

Epoch: 5| Step: 10
Training loss: 2.670428991317749
Validation loss: 2.566872050685267

Epoch: 17| Step: 0
Training loss: 2.7651238441467285
Validation loss: 2.5734095265788417

Epoch: 5| Step: 1
Training loss: 2.805065631866455
Validation loss: 2.5640161703991633

Epoch: 5| Step: 2
Training loss: 2.8187146186828613
Validation loss: 2.56031556283274

Epoch: 5| Step: 3
Training loss: 3.254277467727661
Validation loss: 2.559365616049818

Epoch: 5| Step: 4
Training loss: 2.195890426635742
Validation loss: 2.5593650571761595

Epoch: 5| Step: 5
Training loss: 2.690488815307617
Validation loss: 2.5482829181096887

Epoch: 5| Step: 6
Training loss: 2.6267778873443604
Validation loss: 2.554836206538703

Epoch: 5| Step: 7
Training loss: 2.5650634765625
Validation loss: 2.5637540048168552

Epoch: 5| Step: 8
Training loss: 3.7042603492736816
Validation loss: 2.5587162458768455

Epoch: 5| Step: 9
Training loss: 2.6364073753356934
Validation loss: 2.5419619698678293

Epoch: 5| Step: 10
Training loss: 2.347684383392334
Validation loss: 2.542359954567366

Epoch: 18| Step: 0
Training loss: 3.150447368621826
Validation loss: 2.5678214078308432

Epoch: 5| Step: 1
Training loss: 2.9404470920562744
Validation loss: 2.5932066517491497

Epoch: 5| Step: 2
Training loss: 2.5008201599121094
Validation loss: 2.5751430783220517

Epoch: 5| Step: 3
Training loss: 2.2823920249938965
Validation loss: 2.5353544847939604

Epoch: 5| Step: 4
Training loss: 3.1835579872131348
Validation loss: 2.533600648244222

Epoch: 5| Step: 5
Training loss: 2.7727341651916504
Validation loss: 2.5383178726319344

Epoch: 5| Step: 6
Training loss: 2.829380512237549
Validation loss: 2.538666820013395

Epoch: 5| Step: 7
Training loss: 3.00923228263855
Validation loss: 2.5343517257321264

Epoch: 5| Step: 8
Training loss: 2.354214668273926
Validation loss: 2.5363060684614283

Epoch: 5| Step: 9
Training loss: 2.681875705718994
Validation loss: 2.5461467671137985

Epoch: 5| Step: 10
Training loss: 2.8042423725128174
Validation loss: 2.565316797584616

Epoch: 19| Step: 0
Training loss: 1.9940736293792725
Validation loss: 2.536062973801808

Epoch: 5| Step: 1
Training loss: 2.6687405109405518
Validation loss: 2.5456586755732054

Epoch: 5| Step: 2
Training loss: 3.056030750274658
Validation loss: 2.538484993801322

Epoch: 5| Step: 3
Training loss: 2.771869421005249
Validation loss: 2.5419999066219536

Epoch: 5| Step: 4
Training loss: 2.813286542892456
Validation loss: 2.5417407071718605

Epoch: 5| Step: 5
Training loss: 2.8080241680145264
Validation loss: 2.540232704531762

Epoch: 5| Step: 6
Training loss: 3.014267683029175
Validation loss: 2.5381392791707027

Epoch: 5| Step: 7
Training loss: 2.9802212715148926
Validation loss: 2.5314280166420886

Epoch: 5| Step: 8
Training loss: 2.614840269088745
Validation loss: 2.5333624680836997

Epoch: 5| Step: 9
Training loss: 2.61836314201355
Validation loss: 2.5323396395611506

Epoch: 5| Step: 10
Training loss: 2.921109914779663
Validation loss: 2.527514103920229

Epoch: 20| Step: 0
Training loss: 2.791921854019165
Validation loss: 2.5289174100404144

Epoch: 5| Step: 1
Training loss: 3.4191482067108154
Validation loss: 2.5562831688952703

Epoch: 5| Step: 2
Training loss: 2.3871960639953613
Validation loss: 2.5539757103048344

Epoch: 5| Step: 3
Training loss: 3.0329108238220215
Validation loss: 2.55337752321715

Epoch: 5| Step: 4
Training loss: 3.2898871898651123
Validation loss: 2.531397424718385

Epoch: 5| Step: 5
Training loss: 2.4390652179718018
Validation loss: 2.51889536201313

Epoch: 5| Step: 6
Training loss: 2.666002035140991
Validation loss: 2.5175005851253385

Epoch: 5| Step: 7
Training loss: 2.608100652694702
Validation loss: 2.5095153521466

Epoch: 5| Step: 8
Training loss: 2.917210578918457
Validation loss: 2.5186918986740934

Epoch: 5| Step: 9
Training loss: 2.0424015522003174
Validation loss: 2.5443046708260812

Epoch: 5| Step: 10
Training loss: 2.5383591651916504
Validation loss: 2.570703362905851

Epoch: 21| Step: 0
Training loss: 2.490311622619629
Validation loss: 2.637546270124374

Epoch: 5| Step: 1
Training loss: 3.0228443145751953
Validation loss: 2.580827510485085

Epoch: 5| Step: 2
Training loss: 2.5014853477478027
Validation loss: 2.5114637241568616

Epoch: 5| Step: 3
Training loss: 3.2835304737091064
Validation loss: 2.4943433833378617

Epoch: 5| Step: 4
Training loss: 2.022984743118286
Validation loss: 2.523968770939817

Epoch: 5| Step: 5
Training loss: 3.64094877243042
Validation loss: 2.556675910949707

Epoch: 5| Step: 6
Training loss: 2.1307854652404785
Validation loss: 2.540269587629585

Epoch: 5| Step: 7
Training loss: 2.600912570953369
Validation loss: 2.5054516048841577

Epoch: 5| Step: 8
Training loss: 2.8626930713653564
Validation loss: 2.49302291229207

Epoch: 5| Step: 9
Training loss: 3.06026554107666
Validation loss: 2.4920982160875873

Epoch: 5| Step: 10
Training loss: 2.6727499961853027
Validation loss: 2.494514073095014

Epoch: 22| Step: 0
Training loss: 2.658176898956299
Validation loss: 2.490094023366128

Epoch: 5| Step: 1
Training loss: 2.070373058319092
Validation loss: 2.48833389436045

Epoch: 5| Step: 2
Training loss: 2.346463203430176
Validation loss: 2.4872224997448664

Epoch: 5| Step: 3
Training loss: 3.2955517768859863
Validation loss: 2.4854797829863844

Epoch: 5| Step: 4
Training loss: 3.0526421070098877
Validation loss: 2.5041026838364138

Epoch: 5| Step: 5
Training loss: 2.410587787628174
Validation loss: 2.509888195222424

Epoch: 5| Step: 6
Training loss: 2.3415565490722656
Validation loss: 2.5131488820557952

Epoch: 5| Step: 7
Training loss: 3.108816623687744
Validation loss: 2.504689088431738

Epoch: 5| Step: 8
Training loss: 3.1203744411468506
Validation loss: 2.4823455579819216

Epoch: 5| Step: 9
Training loss: 2.5856833457946777
Validation loss: 2.47961619336118

Epoch: 5| Step: 10
Training loss: 2.9566400051116943
Validation loss: 2.476218826027327

Epoch: 23| Step: 0
Training loss: 2.604315996170044
Validation loss: 2.475685922048425

Epoch: 5| Step: 1
Training loss: 2.660451650619507
Validation loss: 2.4913300775712535

Epoch: 5| Step: 2
Training loss: 2.9993486404418945
Validation loss: 2.4921352222401607

Epoch: 5| Step: 3
Training loss: 2.566736936569214
Validation loss: 2.499822629395352

Epoch: 5| Step: 4
Training loss: 2.8850388526916504
Validation loss: 2.5014738421286307

Epoch: 5| Step: 5
Training loss: 2.3867878913879395
Validation loss: 2.485167864830263

Epoch: 5| Step: 6
Training loss: 2.802626371383667
Validation loss: 2.4810098871108024

Epoch: 5| Step: 7
Training loss: 2.6527485847473145
Validation loss: 2.478543204645957

Epoch: 5| Step: 8
Training loss: 3.071967601776123
Validation loss: 2.473667178102719

Epoch: 5| Step: 9
Training loss: 2.5256028175354004
Validation loss: 2.4866740780491985

Epoch: 5| Step: 10
Training loss: 2.7008230686187744
Validation loss: 2.4871755159029396

Epoch: 24| Step: 0
Training loss: 3.32873272895813
Validation loss: 2.486711181620116

Epoch: 5| Step: 1
Training loss: 2.7094507217407227
Validation loss: 2.4720184008280435

Epoch: 5| Step: 2
Training loss: 2.1968770027160645
Validation loss: 2.465710018270759

Epoch: 5| Step: 3
Training loss: 2.4459266662597656
Validation loss: 2.4653321799411567

Epoch: 5| Step: 4
Training loss: 2.5197765827178955
Validation loss: 2.466387753845543

Epoch: 5| Step: 5
Training loss: 2.3869824409484863
Validation loss: 2.4751255666055987

Epoch: 5| Step: 6
Training loss: 2.8938822746276855
Validation loss: 2.5013279786673923

Epoch: 5| Step: 7
Training loss: 2.287541389465332
Validation loss: 2.534108090144332

Epoch: 5| Step: 8
Training loss: 2.6635823249816895
Validation loss: 2.519226230600829

Epoch: 5| Step: 9
Training loss: 3.181025743484497
Validation loss: 2.5003960645327004

Epoch: 5| Step: 10
Training loss: 3.4647579193115234
Validation loss: 2.471685207018288

Epoch: 25| Step: 0
Training loss: 2.4276061058044434
Validation loss: 2.4586403164812314

Epoch: 5| Step: 1
Training loss: 2.6207563877105713
Validation loss: 2.464175380686278

Epoch: 5| Step: 2
Training loss: 2.545445680618286
Validation loss: 2.4698464537179596

Epoch: 5| Step: 3
Training loss: 2.342599868774414
Validation loss: 2.4725510484428814

Epoch: 5| Step: 4
Training loss: 2.570425510406494
Validation loss: 2.484033123139412

Epoch: 5| Step: 5
Training loss: 3.1234018802642822
Validation loss: 2.492484031185027

Epoch: 5| Step: 6
Training loss: 2.2241365909576416
Validation loss: 2.484338962903587

Epoch: 5| Step: 7
Training loss: 3.1197662353515625
Validation loss: 2.472881324829594

Epoch: 5| Step: 8
Training loss: 3.07137131690979
Validation loss: 2.4648612801746657

Epoch: 5| Step: 9
Training loss: 3.1729297637939453
Validation loss: 2.4772043484513477

Epoch: 5| Step: 10
Training loss: 2.6485557556152344
Validation loss: 2.475155791928691

Epoch: 26| Step: 0
Training loss: 2.7270960807800293
Validation loss: 2.4792725809158815

Epoch: 5| Step: 1
Training loss: 2.3795998096466064
Validation loss: 2.4923035662661315

Epoch: 5| Step: 2
Training loss: 2.4209158420562744
Validation loss: 2.469171216410975

Epoch: 5| Step: 3
Training loss: 3.1129860877990723
Validation loss: 2.4560675390305056

Epoch: 5| Step: 4
Training loss: 3.2799713611602783
Validation loss: 2.456536982649116

Epoch: 5| Step: 5
Training loss: 2.269400119781494
Validation loss: 2.4513931043686403

Epoch: 5| Step: 6
Training loss: 2.3486132621765137
Validation loss: 2.452498630810809

Epoch: 5| Step: 7
Training loss: 3.1333744525909424
Validation loss: 2.460179569900677

Epoch: 5| Step: 8
Training loss: 2.3014984130859375
Validation loss: 2.45501882542846

Epoch: 5| Step: 9
Training loss: 2.618649482727051
Validation loss: 2.4538373562597458

Epoch: 5| Step: 10
Training loss: 3.0246224403381348
Validation loss: 2.452700989220732

Epoch: 27| Step: 0
Training loss: 2.8729820251464844
Validation loss: 2.451139275745679

Epoch: 5| Step: 1
Training loss: 2.2335286140441895
Validation loss: 2.4539498077925814

Epoch: 5| Step: 2
Training loss: 2.403783082962036
Validation loss: 2.4557654960181123

Epoch: 5| Step: 3
Training loss: 2.134995698928833
Validation loss: 2.4660248833317913

Epoch: 5| Step: 4
Training loss: 2.1671042442321777
Validation loss: 2.4787406434294996

Epoch: 5| Step: 5
Training loss: 3.2191550731658936
Validation loss: 2.4842395013378513

Epoch: 5| Step: 6
Training loss: 3.228705883026123
Validation loss: 2.5048520257396083

Epoch: 5| Step: 7
Training loss: 2.4905006885528564
Validation loss: 2.5273914619158675

Epoch: 5| Step: 8
Training loss: 3.2822983264923096
Validation loss: 2.5126443524514475

Epoch: 5| Step: 9
Training loss: 2.4409427642822266
Validation loss: 2.487090536343154

Epoch: 5| Step: 10
Training loss: 3.239358425140381
Validation loss: 2.4868645950030257

Epoch: 28| Step: 0
Training loss: 2.4499619007110596
Validation loss: 2.4933171887551584

Epoch: 5| Step: 1
Training loss: 3.376011610031128
Validation loss: 2.498460077470349

Epoch: 5| Step: 2
Training loss: 2.194591522216797
Validation loss: 2.497932057226858

Epoch: 5| Step: 3
Training loss: 2.8064286708831787
Validation loss: 2.4992406291346394

Epoch: 5| Step: 4
Training loss: 3.2242825031280518
Validation loss: 2.4960868025338776

Epoch: 5| Step: 5
Training loss: 2.817622423171997
Validation loss: 2.484412145870988

Epoch: 5| Step: 6
Training loss: 2.6818130016326904
Validation loss: 2.4861914368085962

Epoch: 5| Step: 7
Training loss: 2.6372666358947754
Validation loss: 2.4789589399932535

Epoch: 5| Step: 8
Training loss: 2.7716479301452637
Validation loss: 2.481117046007546

Epoch: 5| Step: 9
Training loss: 2.752084970474243
Validation loss: 2.4800549912196335

Epoch: 5| Step: 10
Training loss: 1.8104279041290283
Validation loss: 2.4888910311524586

Epoch: 29| Step: 0
Training loss: 3.631134033203125
Validation loss: 2.505539528785213

Epoch: 5| Step: 1
Training loss: 2.8942713737487793
Validation loss: 2.5196046060131443

Epoch: 5| Step: 2
Training loss: 2.3859808444976807
Validation loss: 2.505398452922862

Epoch: 5| Step: 3
Training loss: 2.7733922004699707
Validation loss: 2.490952886560912

Epoch: 5| Step: 4
Training loss: 2.336867332458496
Validation loss: 2.4785863609724146

Epoch: 5| Step: 5
Training loss: 2.966284990310669
Validation loss: 2.4709093032344693

Epoch: 5| Step: 6
Training loss: 2.7281594276428223
Validation loss: 2.472583673333609

Epoch: 5| Step: 7
Training loss: 2.3137049674987793
Validation loss: 2.4801709472492175

Epoch: 5| Step: 8
Training loss: 2.296834945678711
Validation loss: 2.512153305033202

Epoch: 5| Step: 9
Training loss: 2.9679393768310547
Validation loss: 2.559988970397621

Epoch: 5| Step: 10
Training loss: 2.3879058361053467
Validation loss: 2.6048860908836446

Epoch: 30| Step: 0
Training loss: 2.5268821716308594
Validation loss: 2.575924914370301

Epoch: 5| Step: 1
Training loss: 2.2107656002044678
Validation loss: 2.5151174504269838

Epoch: 5| Step: 2
Training loss: 2.5698800086975098
Validation loss: 2.4770338073853524

Epoch: 5| Step: 3
Training loss: 2.5884883403778076
Validation loss: 2.4869066746004167

Epoch: 5| Step: 4
Training loss: 2.788508653640747
Validation loss: 2.489446573359992

Epoch: 5| Step: 5
Training loss: 3.0863959789276123
Validation loss: 2.4889713359135452

Epoch: 5| Step: 6
Training loss: 2.8102736473083496
Validation loss: 2.4892209114566928

Epoch: 5| Step: 7
Training loss: 2.2376270294189453
Validation loss: 2.478466587681924

Epoch: 5| Step: 8
Training loss: 2.992816686630249
Validation loss: 2.4804162927853164

Epoch: 5| Step: 9
Training loss: 3.170130729675293
Validation loss: 2.449842814476259

Epoch: 5| Step: 10
Training loss: 2.60292911529541
Validation loss: 2.4289336332710842

Epoch: 31| Step: 0
Training loss: 2.666245222091675
Validation loss: 2.421409668460969

Epoch: 5| Step: 1
Training loss: 2.539793014526367
Validation loss: 2.424292077300369

Epoch: 5| Step: 2
Training loss: 2.8968710899353027
Validation loss: 2.428373485483149

Epoch: 5| Step: 3
Training loss: 2.7413949966430664
Validation loss: 2.4292263753952517

Epoch: 5| Step: 4
Training loss: 2.9773902893066406
Validation loss: 2.4244698888512066

Epoch: 5| Step: 5
Training loss: 3.0673296451568604
Validation loss: 2.4228139462009555

Epoch: 5| Step: 6
Training loss: 2.3625168800354004
Validation loss: 2.4172410067691597

Epoch: 5| Step: 7
Training loss: 2.305392026901245
Validation loss: 2.418762166013

Epoch: 5| Step: 8
Training loss: 3.086334705352783
Validation loss: 2.419983661302956

Epoch: 5| Step: 9
Training loss: 2.3991408348083496
Validation loss: 2.4345992790755404

Epoch: 5| Step: 10
Training loss: 2.4040579795837402
Validation loss: 2.44816876739584

Epoch: 32| Step: 0
Training loss: 2.3346314430236816
Validation loss: 2.4898521310539654

Epoch: 5| Step: 1
Training loss: 3.3758347034454346
Validation loss: 2.5401706157192105

Epoch: 5| Step: 2
Training loss: 2.58393931388855
Validation loss: 2.559030645637102

Epoch: 5| Step: 3
Training loss: 2.455673933029175
Validation loss: 2.544322539401311

Epoch: 5| Step: 4
Training loss: 3.2729332447052
Validation loss: 2.5036055580262215

Epoch: 5| Step: 5
Training loss: 2.59240460395813
Validation loss: 2.486002673384964

Epoch: 5| Step: 6
Training loss: 2.1517257690429688
Validation loss: 2.448141638950635

Epoch: 5| Step: 7
Training loss: 2.9041144847869873
Validation loss: 2.415170661864742

Epoch: 5| Step: 8
Training loss: 2.5656161308288574
Validation loss: 2.401013376892254

Epoch: 5| Step: 9
Training loss: 2.2407634258270264
Validation loss: 2.400675173728697

Epoch: 5| Step: 10
Training loss: 3.1162643432617188
Validation loss: 2.40238772156418

Epoch: 33| Step: 0
Training loss: 2.7069804668426514
Validation loss: 2.4017551483646518

Epoch: 5| Step: 1
Training loss: 2.8362812995910645
Validation loss: 2.3998608204626266

Epoch: 5| Step: 2
Training loss: 2.872318983078003
Validation loss: 2.3986840299380723

Epoch: 5| Step: 3
Training loss: 2.5286381244659424
Validation loss: 2.3999162079185568

Epoch: 5| Step: 4
Training loss: 2.2873287200927734
Validation loss: 2.3916427922505203

Epoch: 5| Step: 5
Training loss: 3.026200532913208
Validation loss: 2.3961195304829586

Epoch: 5| Step: 6
Training loss: 2.340912342071533
Validation loss: 2.397904106365737

Epoch: 5| Step: 7
Training loss: 2.4362545013427734
Validation loss: 2.4002478430348058

Epoch: 5| Step: 8
Training loss: 2.08229923248291
Validation loss: 2.408858609455888

Epoch: 5| Step: 9
Training loss: 3.1743831634521484
Validation loss: 2.432657734040291

Epoch: 5| Step: 10
Training loss: 2.990215539932251
Validation loss: 2.4294379834205873

Epoch: 34| Step: 0
Training loss: 2.708664655685425
Validation loss: 2.449144873567807

Epoch: 5| Step: 1
Training loss: 2.4769392013549805
Validation loss: 2.459230020482053

Epoch: 5| Step: 2
Training loss: 2.935950756072998
Validation loss: 2.4284027827683317

Epoch: 5| Step: 3
Training loss: 2.821213960647583
Validation loss: 2.394394918154645

Epoch: 5| Step: 4
Training loss: 3.2005882263183594
Validation loss: 2.391347921022805

Epoch: 5| Step: 5
Training loss: 2.3317313194274902
Validation loss: 2.384646607983497

Epoch: 5| Step: 6
Training loss: 2.875310182571411
Validation loss: 2.3863481731824976

Epoch: 5| Step: 7
Training loss: 2.645159959793091
Validation loss: 2.381539462715067

Epoch: 5| Step: 8
Training loss: 2.427931547164917
Validation loss: 2.384272834306122

Epoch: 5| Step: 9
Training loss: 2.4832687377929688
Validation loss: 2.3787971876000844

Epoch: 5| Step: 10
Training loss: 2.257647752761841
Validation loss: 2.384872128886561

Epoch: 35| Step: 0
Training loss: 2.904757261276245
Validation loss: 2.380205500510431

Epoch: 5| Step: 1
Training loss: 2.3530545234680176
Validation loss: 2.3939089928903887

Epoch: 5| Step: 2
Training loss: 2.692523717880249
Validation loss: 2.3973575356186076

Epoch: 5| Step: 3
Training loss: 2.598071813583374
Validation loss: 2.4198754577226538

Epoch: 5| Step: 4
Training loss: 2.6194872856140137
Validation loss: 2.4471634972480034

Epoch: 5| Step: 5
Training loss: 2.605760097503662
Validation loss: 2.4605805566233974

Epoch: 5| Step: 6
Training loss: 2.1233537197113037
Validation loss: 2.4606345802225094

Epoch: 5| Step: 7
Training loss: 3.2187283039093018
Validation loss: 2.4648214437628306

Epoch: 5| Step: 8
Training loss: 2.8306643962860107
Validation loss: 2.4399144623869207

Epoch: 5| Step: 9
Training loss: 2.6678519248962402
Validation loss: 2.4179462296988374

Epoch: 5| Step: 10
Training loss: 2.491309881210327
Validation loss: 2.4068682014301257

Epoch: 36| Step: 0
Training loss: 2.436556339263916
Validation loss: 2.401449167600242

Epoch: 5| Step: 1
Training loss: 3.098814010620117
Validation loss: 2.392694914212791

Epoch: 5| Step: 2
Training loss: 2.646848678588867
Validation loss: 2.4014632573691745

Epoch: 5| Step: 3
Training loss: 2.913695812225342
Validation loss: 2.408611969281268

Epoch: 5| Step: 4
Training loss: 2.3069469928741455
Validation loss: 2.401366736299248

Epoch: 5| Step: 5
Training loss: 2.162980556488037
Validation loss: 2.4000286748332362

Epoch: 5| Step: 6
Training loss: 2.6786115169525146
Validation loss: 2.397642584257228

Epoch: 5| Step: 7
Training loss: 2.29651141166687
Validation loss: 2.3924057073490594

Epoch: 5| Step: 8
Training loss: 2.9575271606445312
Validation loss: 2.3885398910891626

Epoch: 5| Step: 9
Training loss: 3.1056549549102783
Validation loss: 2.3843755106772146

Epoch: 5| Step: 10
Training loss: 2.4296674728393555
Validation loss: 2.3907440144528627

Epoch: 37| Step: 0
Training loss: 2.64691424369812
Validation loss: 2.399852752685547

Epoch: 5| Step: 1
Training loss: 2.6456990242004395
Validation loss: 2.4070961039553405

Epoch: 5| Step: 2
Training loss: 2.2183420658111572
Validation loss: 2.43474288909666

Epoch: 5| Step: 3
Training loss: 2.9137179851531982
Validation loss: 2.442679894867764

Epoch: 5| Step: 4
Training loss: 2.6138386726379395
Validation loss: 2.4322008625153573

Epoch: 5| Step: 5
Training loss: 3.1982545852661133
Validation loss: 2.427886788563062

Epoch: 5| Step: 6
Training loss: 2.582340955734253
Validation loss: 2.4070825615236835

Epoch: 5| Step: 7
Training loss: 2.2022557258605957
Validation loss: 2.402862725719329

Epoch: 5| Step: 8
Training loss: 2.6854963302612305
Validation loss: 2.386345545450846

Epoch: 5| Step: 9
Training loss: 2.5830929279327393
Validation loss: 2.379390734498219

Epoch: 5| Step: 10
Training loss: 2.7022876739501953
Validation loss: 2.3717421395804292

Epoch: 38| Step: 0
Training loss: 3.072174310684204
Validation loss: 2.373338108421654

Epoch: 5| Step: 1
Training loss: 2.735787868499756
Validation loss: 2.3806674762438704

Epoch: 5| Step: 2
Training loss: 2.186436176300049
Validation loss: 2.378127905630296

Epoch: 5| Step: 3
Training loss: 2.332111120223999
Validation loss: 2.36949808238655

Epoch: 5| Step: 4
Training loss: 2.3318300247192383
Validation loss: 2.376693835822485

Epoch: 5| Step: 5
Training loss: 2.7418384552001953
Validation loss: 2.376437915268765

Epoch: 5| Step: 6
Training loss: 2.484243869781494
Validation loss: 2.3808430548637145

Epoch: 5| Step: 7
Training loss: 2.37908935546875
Validation loss: 2.3895005103080504

Epoch: 5| Step: 8
Training loss: 2.6565098762512207
Validation loss: 2.4229848628403037

Epoch: 5| Step: 9
Training loss: 2.8993923664093018
Validation loss: 2.4449080010896087

Epoch: 5| Step: 10
Training loss: 3.2479398250579834
Validation loss: 2.4591612610765683

Epoch: 39| Step: 0
Training loss: 2.257197618484497
Validation loss: 2.416595676893829

Epoch: 5| Step: 1
Training loss: 2.913630962371826
Validation loss: 2.4038842467851538

Epoch: 5| Step: 2
Training loss: 2.741764783859253
Validation loss: 2.3883872775621313

Epoch: 5| Step: 3
Training loss: 3.00620698928833
Validation loss: 2.367267438160476

Epoch: 5| Step: 4
Training loss: 2.2661452293395996
Validation loss: 2.3586716421188845

Epoch: 5| Step: 5
Training loss: 2.526667594909668
Validation loss: 2.3504506131654144

Epoch: 5| Step: 6
Training loss: 2.7911922931671143
Validation loss: 2.3470900084382746

Epoch: 5| Step: 7
Training loss: 2.394879102706909
Validation loss: 2.34734352942436

Epoch: 5| Step: 8
Training loss: 2.7634849548339844
Validation loss: 2.3479646187956615

Epoch: 5| Step: 9
Training loss: 2.6319594383239746
Validation loss: 2.348536942594795

Epoch: 5| Step: 10
Training loss: 2.5792298316955566
Validation loss: 2.34817652163967

Epoch: 40| Step: 0
Training loss: 2.3803038597106934
Validation loss: 2.3462759166635494

Epoch: 5| Step: 1
Training loss: 2.8344998359680176
Validation loss: 2.347419705442203

Epoch: 5| Step: 2
Training loss: 2.336061954498291
Validation loss: 2.349575073488297

Epoch: 5| Step: 3
Training loss: 2.8491339683532715
Validation loss: 2.356255608220254

Epoch: 5| Step: 4
Training loss: 2.7537577152252197
Validation loss: 2.369278610393565

Epoch: 5| Step: 5
Training loss: 2.766199827194214
Validation loss: 2.3682460528548046

Epoch: 5| Step: 6
Training loss: 2.2090420722961426
Validation loss: 2.3737171516623548

Epoch: 5| Step: 7
Training loss: 2.72310733795166
Validation loss: 2.3903495829592467

Epoch: 5| Step: 8
Training loss: 2.826106548309326
Validation loss: 2.399234938365157

Epoch: 5| Step: 9
Training loss: 2.6921684741973877
Validation loss: 2.3914400774945497

Epoch: 5| Step: 10
Training loss: 2.3905043601989746
Validation loss: 2.3682701408222155

Epoch: 41| Step: 0
Training loss: 3.0210914611816406
Validation loss: 2.3604271898987474

Epoch: 5| Step: 1
Training loss: 3.0821595191955566
Validation loss: 2.359137422295027

Epoch: 5| Step: 2
Training loss: 2.8163280487060547
Validation loss: 2.351486967455956

Epoch: 5| Step: 3
Training loss: 2.4162497520446777
Validation loss: 2.3445962206009896

Epoch: 5| Step: 4
Training loss: 2.5775551795959473
Validation loss: 2.3382709923610894

Epoch: 5| Step: 5
Training loss: 2.6605072021484375
Validation loss: 2.3333734773820445

Epoch: 5| Step: 6
Training loss: 2.8018858432769775
Validation loss: 2.3351694665929323

Epoch: 5| Step: 7
Training loss: 1.9036670923233032
Validation loss: 2.337724688232586

Epoch: 5| Step: 8
Training loss: 2.8183517456054688
Validation loss: 2.3357163808679067

Epoch: 5| Step: 9
Training loss: 2.44384765625
Validation loss: 2.344775443435997

Epoch: 5| Step: 10
Training loss: 2.1129250526428223
Validation loss: 2.3599634093623005

Epoch: 42| Step: 0
Training loss: 2.5770483016967773
Validation loss: 2.376654763375559

Epoch: 5| Step: 1
Training loss: 2.2614905834198
Validation loss: 2.3793184321413756

Epoch: 5| Step: 2
Training loss: 2.2288875579833984
Validation loss: 2.398435179905225

Epoch: 5| Step: 3
Training loss: 2.4270453453063965
Validation loss: 2.408682859072121

Epoch: 5| Step: 4
Training loss: 2.896209716796875
Validation loss: 2.410166448162448

Epoch: 5| Step: 5
Training loss: 3.0034372806549072
Validation loss: 2.3913266530600925

Epoch: 5| Step: 6
Training loss: 2.666874408721924
Validation loss: 2.3677034531870196

Epoch: 5| Step: 7
Training loss: 2.8665428161621094
Validation loss: 2.3658838784822853

Epoch: 5| Step: 8
Training loss: 2.2495925426483154
Validation loss: 2.3494276564608336

Epoch: 5| Step: 9
Training loss: 2.7833142280578613
Validation loss: 2.3433719911882953

Epoch: 5| Step: 10
Training loss: 2.758795738220215
Validation loss: 2.344006574282082

Epoch: 43| Step: 0
Training loss: 3.406296491622925
Validation loss: 2.343032044749106

Epoch: 5| Step: 1
Training loss: 2.2652535438537598
Validation loss: 2.344443962138186

Epoch: 5| Step: 2
Training loss: 2.38794207572937
Validation loss: 2.3480374249078895

Epoch: 5| Step: 3
Training loss: 1.7860314846038818
Validation loss: 2.3463226851596626

Epoch: 5| Step: 4
Training loss: 2.8332860469818115
Validation loss: 2.3380352540682723

Epoch: 5| Step: 5
Training loss: 2.799546003341675
Validation loss: 2.331346881005072

Epoch: 5| Step: 6
Training loss: 2.9136903285980225
Validation loss: 2.328290359948271

Epoch: 5| Step: 7
Training loss: 2.3859853744506836
Validation loss: 2.3234834055746756

Epoch: 5| Step: 8
Training loss: 2.745523452758789
Validation loss: 2.3191355274569605

Epoch: 5| Step: 9
Training loss: 2.774200439453125
Validation loss: 2.321594892009612

Epoch: 5| Step: 10
Training loss: 2.2575485706329346
Validation loss: 2.3211559646873066

Epoch: 44| Step: 0
Training loss: 2.90203857421875
Validation loss: 2.3216088228328253

Epoch: 5| Step: 1
Training loss: 2.941195487976074
Validation loss: 2.318858592740951

Epoch: 5| Step: 2
Training loss: 2.5867743492126465
Validation loss: 2.3221528863394134

Epoch: 5| Step: 3
Training loss: 2.7768149375915527
Validation loss: 2.3496702589014524

Epoch: 5| Step: 4
Training loss: 2.3697762489318848
Validation loss: 2.3395929157093005

Epoch: 5| Step: 5
Training loss: 2.3709194660186768
Validation loss: 2.326904172538429

Epoch: 5| Step: 6
Training loss: 2.4735543727874756
Validation loss: 2.3219778460841023

Epoch: 5| Step: 7
Training loss: 2.3627078533172607
Validation loss: 2.3153671628685406

Epoch: 5| Step: 8
Training loss: 2.4957590103149414
Validation loss: 2.3044392293499363

Epoch: 5| Step: 9
Training loss: 2.720940589904785
Validation loss: 2.301376368409844

Epoch: 5| Step: 10
Training loss: 2.4042699337005615
Validation loss: 2.304501300217003

Epoch: 45| Step: 0
Training loss: 2.505154848098755
Validation loss: 2.2999488384492937

Epoch: 5| Step: 1
Training loss: 2.3357748985290527
Validation loss: 2.3038566497064408

Epoch: 5| Step: 2
Training loss: 2.459071397781372
Validation loss: 2.3000654456435994

Epoch: 5| Step: 3
Training loss: 2.318572998046875
Validation loss: 2.2996473684105823

Epoch: 5| Step: 4
Training loss: 2.2815439701080322
Validation loss: 2.2997902183122534

Epoch: 5| Step: 5
Training loss: 3.674459934234619
Validation loss: 2.3066430963495725

Epoch: 5| Step: 6
Training loss: 2.9562864303588867
Validation loss: 2.3131030733867357

Epoch: 5| Step: 7
Training loss: 2.7905609607696533
Validation loss: 2.335251126238095

Epoch: 5| Step: 8
Training loss: 1.85703444480896
Validation loss: 2.3631232464185326

Epoch: 5| Step: 9
Training loss: 2.900275468826294
Validation loss: 2.3756215251902097

Epoch: 5| Step: 10
Training loss: 2.2763757705688477
Validation loss: 2.379685068643221

Epoch: 46| Step: 0
Training loss: 2.346024751663208
Validation loss: 2.400832114681121

Epoch: 5| Step: 1
Training loss: 2.229771852493286
Validation loss: 2.384872390377906

Epoch: 5| Step: 2
Training loss: 2.3822591304779053
Validation loss: 2.364351059800835

Epoch: 5| Step: 3
Training loss: 3.015448570251465
Validation loss: 2.317992041187902

Epoch: 5| Step: 4
Training loss: 3.289144515991211
Validation loss: 2.2965464438161542

Epoch: 5| Step: 5
Training loss: 1.9158248901367188
Validation loss: 2.28863053296202

Epoch: 5| Step: 6
Training loss: 3.176190137863159
Validation loss: 2.3062355108158563

Epoch: 5| Step: 7
Training loss: 2.247241973876953
Validation loss: 2.3371396551850023

Epoch: 5| Step: 8
Training loss: 2.9294216632843018
Validation loss: 2.346875277898645

Epoch: 5| Step: 9
Training loss: 2.837764263153076
Validation loss: 2.338544681508054

Epoch: 5| Step: 10
Training loss: 2.289381980895996
Validation loss: 2.333292325337728

Epoch: 47| Step: 0
Training loss: 2.724679470062256
Validation loss: 2.313527214911676

Epoch: 5| Step: 1
Training loss: 2.986891269683838
Validation loss: 2.297380970370385

Epoch: 5| Step: 2
Training loss: 2.180760622024536
Validation loss: 2.2990520654186124

Epoch: 5| Step: 3
Training loss: 3.000307083129883
Validation loss: 2.304303123104957

Epoch: 5| Step: 4
Training loss: 2.2192444801330566
Validation loss: 2.298114256192279

Epoch: 5| Step: 5
Training loss: 2.529543161392212
Validation loss: 2.3076621563203874

Epoch: 5| Step: 6
Training loss: 2.553908586502075
Validation loss: 2.3207507159120295

Epoch: 5| Step: 7
Training loss: 2.787118911743164
Validation loss: 2.323813351251746

Epoch: 5| Step: 8
Training loss: 2.246943235397339
Validation loss: 2.34940323522014

Epoch: 5| Step: 9
Training loss: 2.4862396717071533
Validation loss: 2.345316707447011

Epoch: 5| Step: 10
Training loss: 2.675999164581299
Validation loss: 2.3560697981106338

Epoch: 48| Step: 0
Training loss: 3.216177463531494
Validation loss: 2.367860814576508

Epoch: 5| Step: 1
Training loss: 1.9270763397216797
Validation loss: 2.3435270183829853

Epoch: 5| Step: 2
Training loss: 2.609607219696045
Validation loss: 2.310295484399283

Epoch: 5| Step: 3
Training loss: 2.478433132171631
Validation loss: 2.2909219598257415

Epoch: 5| Step: 4
Training loss: 2.216984272003174
Validation loss: 2.275884833387149

Epoch: 5| Step: 5
Training loss: 2.648108720779419
Validation loss: 2.2670227866018973

Epoch: 5| Step: 6
Training loss: 2.8302321434020996
Validation loss: 2.262828332121654

Epoch: 5| Step: 7
Training loss: 3.1152336597442627
Validation loss: 2.260649860546153

Epoch: 5| Step: 8
Training loss: 2.5553359985351562
Validation loss: 2.2651236672555246

Epoch: 5| Step: 9
Training loss: 2.6377580165863037
Validation loss: 2.2627338183823453

Epoch: 5| Step: 10
Training loss: 1.9674009084701538
Validation loss: 2.2622495441026587

Epoch: 49| Step: 0
Training loss: 2.7388596534729004
Validation loss: 2.2610184736149286

Epoch: 5| Step: 1
Training loss: 2.0266246795654297
Validation loss: 2.2659086924727245

Epoch: 5| Step: 2
Training loss: 3.316619873046875
Validation loss: 2.2691614217655633

Epoch: 5| Step: 3
Training loss: 2.2171072959899902
Validation loss: 2.2740377597911383

Epoch: 5| Step: 4
Training loss: 1.962172269821167
Validation loss: 2.299396417474234

Epoch: 5| Step: 5
Training loss: 2.201143264770508
Validation loss: 2.3388077764100927

Epoch: 5| Step: 6
Training loss: 2.826572895050049
Validation loss: 2.37394646675356

Epoch: 5| Step: 7
Training loss: 2.548325777053833
Validation loss: 2.385194932260821

Epoch: 5| Step: 8
Training loss: 2.8800060749053955
Validation loss: 2.39790105306974

Epoch: 5| Step: 9
Training loss: 2.5457005500793457
Validation loss: 2.3821764299946446

Epoch: 5| Step: 10
Training loss: 3.233274221420288
Validation loss: 2.3580905493869575

Epoch: 50| Step: 0
Training loss: 3.150655746459961
Validation loss: 2.342528220145933

Epoch: 5| Step: 1
Training loss: 2.658512592315674
Validation loss: 2.3233728357540664

Epoch: 5| Step: 2
Training loss: 2.229773759841919
Validation loss: 2.289044331478816

Epoch: 5| Step: 3
Training loss: 2.41671085357666
Validation loss: 2.268464506313365

Epoch: 5| Step: 4
Training loss: 2.9467554092407227
Validation loss: 2.255489262201453

Epoch: 5| Step: 5
Training loss: 2.8493638038635254
Validation loss: 2.2499477978675597

Epoch: 5| Step: 6
Training loss: 2.8794808387756348
Validation loss: 2.250129525379468

Epoch: 5| Step: 7
Training loss: 2.55292010307312
Validation loss: 2.254654315210158

Epoch: 5| Step: 8
Training loss: 2.0759329795837402
Validation loss: 2.2539679029936432

Epoch: 5| Step: 9
Training loss: 2.837620735168457
Validation loss: 2.2565077017712336

Epoch: 5| Step: 10
Training loss: 1.4911829233169556
Validation loss: 2.2540066190945205

Epoch: 51| Step: 0
Training loss: 2.282820224761963
Validation loss: 2.2468166184681717

Epoch: 5| Step: 1
Training loss: 2.9143710136413574
Validation loss: 2.2469836358101136

Epoch: 5| Step: 2
Training loss: 2.606823444366455
Validation loss: 2.2502485193232054

Epoch: 5| Step: 3
Training loss: 2.558211326599121
Validation loss: 2.2542430918703795

Epoch: 5| Step: 4
Training loss: 2.3800244331359863
Validation loss: 2.262844670203424

Epoch: 5| Step: 5
Training loss: 2.6458041667938232
Validation loss: 2.2844809204019527

Epoch: 5| Step: 6
Training loss: 2.1107664108276367
Validation loss: 2.296567301596365

Epoch: 5| Step: 7
Training loss: 2.6637818813323975
Validation loss: 2.3406497868158485

Epoch: 5| Step: 8
Training loss: 2.821991205215454
Validation loss: 2.3618014422796105

Epoch: 5| Step: 9
Training loss: 3.1298747062683105
Validation loss: 2.3767497667702298

Epoch: 5| Step: 10
Training loss: 1.9897263050079346
Validation loss: 2.3568709947729625

Epoch: 52| Step: 0
Training loss: 2.4762260913848877
Validation loss: 2.3308636655089674

Epoch: 5| Step: 1
Training loss: 2.8156046867370605
Validation loss: 2.302674839573522

Epoch: 5| Step: 2
Training loss: 2.3656320571899414
Validation loss: 2.2949136034134896

Epoch: 5| Step: 3
Training loss: 2.7910068035125732
Validation loss: 2.2841570659350325

Epoch: 5| Step: 4
Training loss: 2.5935213565826416
Validation loss: 2.2828831852123304

Epoch: 5| Step: 5
Training loss: 2.9034221172332764
Validation loss: 2.258505408481885

Epoch: 5| Step: 6
Training loss: 2.3295655250549316
Validation loss: 2.2509800567421863

Epoch: 5| Step: 7
Training loss: 2.4505980014801025
Validation loss: 2.281996191188853

Epoch: 5| Step: 8
Training loss: 2.5818028450012207
Validation loss: 2.2965971218642367

Epoch: 5| Step: 9
Training loss: 2.797574281692505
Validation loss: 2.2937867756812804

Epoch: 5| Step: 10
Training loss: 1.9107553958892822
Validation loss: 2.2797268052254953

Epoch: 53| Step: 0
Training loss: 2.6839046478271484
Validation loss: 2.242207965543193

Epoch: 5| Step: 1
Training loss: 2.602083683013916
Validation loss: 2.236822978142769

Epoch: 5| Step: 2
Training loss: 2.4830117225646973
Validation loss: 2.2407048440748647

Epoch: 5| Step: 3
Training loss: 2.7747771739959717
Validation loss: 2.241846739604909

Epoch: 5| Step: 4
Training loss: 2.8388662338256836
Validation loss: 2.247509438504455

Epoch: 5| Step: 5
Training loss: 2.0358357429504395
Validation loss: 2.2525760332743325

Epoch: 5| Step: 6
Training loss: 2.6526894569396973
Validation loss: 2.2426579767657864

Epoch: 5| Step: 7
Training loss: 2.524897813796997
Validation loss: 2.2411902848110405

Epoch: 5| Step: 8
Training loss: 2.858109951019287
Validation loss: 2.2348315151788856

Epoch: 5| Step: 9
Training loss: 2.5291764736175537
Validation loss: 2.234293214736446

Epoch: 5| Step: 10
Training loss: 2.2345829010009766
Validation loss: 2.2580254680366925

Epoch: 54| Step: 0
Training loss: 2.857006788253784
Validation loss: 2.2878105948048253

Epoch: 5| Step: 1
Training loss: 3.1684517860412598
Validation loss: 2.3087164971136276

Epoch: 5| Step: 2
Training loss: 2.3732028007507324
Validation loss: 2.349470064204226

Epoch: 5| Step: 3
Training loss: 2.6851887702941895
Validation loss: 2.342285781778315

Epoch: 5| Step: 4
Training loss: 1.906211495399475
Validation loss: 2.3352320219880793

Epoch: 5| Step: 5
Training loss: 1.9661051034927368
Validation loss: 2.346626712429908

Epoch: 5| Step: 6
Training loss: 2.7964088916778564
Validation loss: 2.338210068723207

Epoch: 5| Step: 7
Training loss: 2.5847413539886475
Validation loss: 2.3172863401392454

Epoch: 5| Step: 8
Training loss: 2.54339599609375
Validation loss: 2.287232711750974

Epoch: 5| Step: 9
Training loss: 2.5354936122894287
Validation loss: 2.304659579389839

Epoch: 5| Step: 10
Training loss: 2.9766218662261963
Validation loss: 2.2856015492511053

Epoch: 55| Step: 0
Training loss: 2.6728768348693848
Validation loss: 2.2683141436628116

Epoch: 5| Step: 1
Training loss: 2.536069393157959
Validation loss: 2.2632470797466975

Epoch: 5| Step: 2
Training loss: 3.268691301345825
Validation loss: 2.2602010978165494

Epoch: 5| Step: 3
Training loss: 2.2986788749694824
Validation loss: 2.2464580869161956

Epoch: 5| Step: 4
Training loss: 2.2940611839294434
Validation loss: 2.2529857620116203

Epoch: 5| Step: 5
Training loss: 2.7280831336975098
Validation loss: 2.2580489086848434

Epoch: 5| Step: 6
Training loss: 2.699824810028076
Validation loss: 2.2557145831405476

Epoch: 5| Step: 7
Training loss: 2.2460849285125732
Validation loss: 2.2481615671547512

Epoch: 5| Step: 8
Training loss: 2.248185873031616
Validation loss: 2.2492209224290747

Epoch: 5| Step: 9
Training loss: 2.5038318634033203
Validation loss: 2.2395925932033087

Epoch: 5| Step: 10
Training loss: 2.489900588989258
Validation loss: 2.2357628242943877

Epoch: 56| Step: 0
Training loss: 2.2550225257873535
Validation loss: 2.2366231051824426

Epoch: 5| Step: 1
Training loss: 3.302936553955078
Validation loss: 2.252703651305168

Epoch: 5| Step: 2
Training loss: 2.479841709136963
Validation loss: 2.279020929849276

Epoch: 5| Step: 3
Training loss: 2.296510696411133
Validation loss: 2.3041441209854616

Epoch: 5| Step: 4
Training loss: 2.852238416671753
Validation loss: 2.3015896863834833

Epoch: 5| Step: 5
Training loss: 2.0249664783477783
Validation loss: 2.3001503752123926

Epoch: 5| Step: 6
Training loss: 2.3233492374420166
Validation loss: 2.2896703238128335

Epoch: 5| Step: 7
Training loss: 2.904036045074463
Validation loss: 2.264453213701966

Epoch: 5| Step: 8
Training loss: 1.9204978942871094
Validation loss: 2.248978186679143

Epoch: 5| Step: 9
Training loss: 2.911458730697632
Validation loss: 2.230126403993176

Epoch: 5| Step: 10
Training loss: 2.686795234680176
Validation loss: 2.225398950679328

Epoch: 57| Step: 0
Training loss: 1.9737613201141357
Validation loss: 2.2274319715397333

Epoch: 5| Step: 1
Training loss: 2.5372073650360107
Validation loss: 2.2353800291656167

Epoch: 5| Step: 2
Training loss: 2.582454204559326
Validation loss: 2.2372368651051677

Epoch: 5| Step: 3
Training loss: 2.3432135581970215
Validation loss: 2.2440707888654483

Epoch: 5| Step: 4
Training loss: 2.925997018814087
Validation loss: 2.249487369291244

Epoch: 5| Step: 5
Training loss: 1.9622617959976196
Validation loss: 2.2591535173436648

Epoch: 5| Step: 6
Training loss: 3.2448456287384033
Validation loss: 2.262912024733841

Epoch: 5| Step: 7
Training loss: 2.5783228874206543
Validation loss: 2.275700097442955

Epoch: 5| Step: 8
Training loss: 2.7261483669281006
Validation loss: 2.2912854481768865

Epoch: 5| Step: 9
Training loss: 1.818189024925232
Validation loss: 2.3076135881485476

Epoch: 5| Step: 10
Training loss: 3.280785083770752
Validation loss: 2.2907776986399004

Epoch: 58| Step: 0
Training loss: 2.236417770385742
Validation loss: 2.268403422447943

Epoch: 5| Step: 1
Training loss: 2.0173983573913574
Validation loss: 2.2521101479889243

Epoch: 5| Step: 2
Training loss: 2.897615671157837
Validation loss: 2.233332549372027

Epoch: 5| Step: 3
Training loss: 2.3996806144714355
Validation loss: 2.2252479676277406

Epoch: 5| Step: 4
Training loss: 2.908285617828369
Validation loss: 2.2173228520219044

Epoch: 5| Step: 5
Training loss: 2.6232056617736816
Validation loss: 2.2170821415480746

Epoch: 5| Step: 6
Training loss: 2.464639902114868
Validation loss: 2.2144880064072145

Epoch: 5| Step: 7
Training loss: 2.7885406017303467
Validation loss: 2.218661046797229

Epoch: 5| Step: 8
Training loss: 2.1747541427612305
Validation loss: 2.2175131587571997

Epoch: 5| Step: 9
Training loss: 2.245882511138916
Validation loss: 2.214421269714191

Epoch: 5| Step: 10
Training loss: 3.1042213439941406
Validation loss: 2.2202722539183912

Epoch: 59| Step: 0
Training loss: 2.989218235015869
Validation loss: 2.2136589352802565

Epoch: 5| Step: 1
Training loss: 3.011327028274536
Validation loss: 2.221767012790967

Epoch: 5| Step: 2
Training loss: 2.0508222579956055
Validation loss: 2.228831737272201

Epoch: 5| Step: 3
Training loss: 2.690739393234253
Validation loss: 2.2362498262877106

Epoch: 5| Step: 4
Training loss: 2.7932660579681396
Validation loss: 2.2547384718412995

Epoch: 5| Step: 5
Training loss: 1.7045643329620361
Validation loss: 2.2539009842821347

Epoch: 5| Step: 6
Training loss: 3.0854878425598145
Validation loss: 2.256552116845244

Epoch: 5| Step: 7
Training loss: 2.541781187057495
Validation loss: 2.2526020619177047

Epoch: 5| Step: 8
Training loss: 2.4977757930755615
Validation loss: 2.2468991587238927

Epoch: 5| Step: 9
Training loss: 2.570509910583496
Validation loss: 2.2350508833444245

Epoch: 5| Step: 10
Training loss: 1.5712158679962158
Validation loss: 2.2237034215722034

Epoch: 60| Step: 0
Training loss: 2.787275791168213
Validation loss: 2.2096453148831605

Epoch: 5| Step: 1
Training loss: 1.8224995136260986
Validation loss: 2.2059031558293167

Epoch: 5| Step: 2
Training loss: 2.726452350616455
Validation loss: 2.2004113299872285

Epoch: 5| Step: 3
Training loss: 2.8564364910125732
Validation loss: 2.2024075190226235

Epoch: 5| Step: 4
Training loss: 2.373932361602783
Validation loss: 2.20306719118549

Epoch: 5| Step: 5
Training loss: 2.857623338699341
Validation loss: 2.219210775949622

Epoch: 5| Step: 6
Training loss: 2.5502569675445557
Validation loss: 2.2313033560270905

Epoch: 5| Step: 7
Training loss: 2.2195658683776855
Validation loss: 2.269138538709251

Epoch: 5| Step: 8
Training loss: 2.407451629638672
Validation loss: 2.323950590625886

Epoch: 5| Step: 9
Training loss: 2.115802049636841
Validation loss: 2.388660974400018

Epoch: 5| Step: 10
Training loss: 3.269728183746338
Validation loss: 2.3751521213080293

Epoch: 61| Step: 0
Training loss: 2.507899761199951
Validation loss: 2.3001662082569574

Epoch: 5| Step: 1
Training loss: 2.558532238006592
Validation loss: 2.2301957479087253

Epoch: 5| Step: 2
Training loss: 3.039606809616089
Validation loss: 2.1980915607944613

Epoch: 5| Step: 3
Training loss: 2.270017623901367
Validation loss: 2.1977118663890387

Epoch: 5| Step: 4
Training loss: 2.539119243621826
Validation loss: 2.187372787024385

Epoch: 5| Step: 5
Training loss: 2.362511396408081
Validation loss: 2.1937202189558294

Epoch: 5| Step: 6
Training loss: 2.007431983947754
Validation loss: 2.2117467567484868

Epoch: 5| Step: 7
Training loss: 2.6542725563049316
Validation loss: 2.244549161644392

Epoch: 5| Step: 8
Training loss: 3.012622356414795
Validation loss: 2.2588560529934463

Epoch: 5| Step: 9
Training loss: 2.196350574493408
Validation loss: 2.2631007445755826

Epoch: 5| Step: 10
Training loss: 3.04424786567688
Validation loss: 2.2608662779613207

Epoch: 62| Step: 0
Training loss: 3.0097742080688477
Validation loss: 2.261141621938316

Epoch: 5| Step: 1
Training loss: 2.7347137928009033
Validation loss: 2.26255545308513

Epoch: 5| Step: 2
Training loss: 2.031599760055542
Validation loss: 2.2663999552367837

Epoch: 5| Step: 3
Training loss: 3.10872220993042
Validation loss: 2.2751344685913413

Epoch: 5| Step: 4
Training loss: 1.688759207725525
Validation loss: 2.2672734465650333

Epoch: 5| Step: 5
Training loss: 2.818455934524536
Validation loss: 2.2717755558670207

Epoch: 5| Step: 6
Training loss: 2.2032999992370605
Validation loss: 2.259095284246629

Epoch: 5| Step: 7
Training loss: 2.4516823291778564
Validation loss: 2.2466163353253434

Epoch: 5| Step: 8
Training loss: 2.7638802528381348
Validation loss: 2.246662134765297

Epoch: 5| Step: 9
Training loss: 2.503896713256836
Validation loss: 2.2343661913307766

Epoch: 5| Step: 10
Training loss: 2.5386781692504883
Validation loss: 2.2339084148406982

Epoch: 63| Step: 0
Training loss: 2.594081163406372
Validation loss: 2.2447566832265546

Epoch: 5| Step: 1
Training loss: 2.9119982719421387
Validation loss: 2.2504406718797583

Epoch: 5| Step: 2
Training loss: 2.264906406402588
Validation loss: 2.240227460861206

Epoch: 5| Step: 3
Training loss: 2.4781603813171387
Validation loss: 2.2451952324118665

Epoch: 5| Step: 4
Training loss: 2.6595633029937744
Validation loss: 2.2551382510892806

Epoch: 5| Step: 5
Training loss: 2.5773746967315674
Validation loss: 2.2506789981677966

Epoch: 5| Step: 6
Training loss: 2.1726021766662598
Validation loss: 2.2446982501655497

Epoch: 5| Step: 7
Training loss: 2.057291269302368
Validation loss: 2.228163949904903

Epoch: 5| Step: 8
Training loss: 2.8013527393341064
Validation loss: 2.2178402587931645

Epoch: 5| Step: 9
Training loss: 2.4517593383789062
Validation loss: 2.2085746667718373

Epoch: 5| Step: 10
Training loss: 2.566470146179199
Validation loss: 2.2082164646476827

Epoch: 64| Step: 0
Training loss: 2.370577573776245
Validation loss: 2.1940732579077444

Epoch: 5| Step: 1
Training loss: 2.367490291595459
Validation loss: 2.1853726307551065

Epoch: 5| Step: 2
Training loss: 2.1955409049987793
Validation loss: 2.1782689120179866

Epoch: 5| Step: 3
Training loss: 2.534862756729126
Validation loss: 2.171534771560341

Epoch: 5| Step: 4
Training loss: 2.8488011360168457
Validation loss: 2.1750185310199694

Epoch: 5| Step: 5
Training loss: 2.584502935409546
Validation loss: 2.1706059953217864

Epoch: 5| Step: 6
Training loss: 2.7706170082092285
Validation loss: 2.1678037643432617

Epoch: 5| Step: 7
Training loss: 2.715907096862793
Validation loss: 2.1782276758583645

Epoch: 5| Step: 8
Training loss: 1.7829997539520264
Validation loss: 2.1783626720469487

Epoch: 5| Step: 9
Training loss: 2.76171612739563
Validation loss: 2.193919148496402

Epoch: 5| Step: 10
Training loss: 2.749927520751953
Validation loss: 2.2198208660207768

Epoch: 65| Step: 0
Training loss: 2.1627652645111084
Validation loss: 2.2404895469706547

Epoch: 5| Step: 1
Training loss: 2.519573211669922
Validation loss: 2.2854315644951275

Epoch: 5| Step: 2
Training loss: 2.713395118713379
Validation loss: 2.2999169211233816

Epoch: 5| Step: 3
Training loss: 2.807236909866333
Validation loss: 2.2859099936741654

Epoch: 5| Step: 4
Training loss: 2.0938761234283447
Validation loss: 2.2361405626420052

Epoch: 5| Step: 5
Training loss: 1.7492214441299438
Validation loss: 2.225449428763441

Epoch: 5| Step: 6
Training loss: 2.414914846420288
Validation loss: 2.199347524232762

Epoch: 5| Step: 7
Training loss: 2.367443561553955
Validation loss: 2.2017830828184723

Epoch: 5| Step: 8
Training loss: 2.7000842094421387
Validation loss: 2.190011647439772

Epoch: 5| Step: 9
Training loss: 2.7260990142822266
Validation loss: 2.2049039833007322

Epoch: 5| Step: 10
Training loss: 3.3561666011810303
Validation loss: 2.20801858363613

Epoch: 66| Step: 0
Training loss: 2.9292502403259277
Validation loss: 2.225318352381388

Epoch: 5| Step: 1
Training loss: 3.0624775886535645
Validation loss: 2.218853904354957

Epoch: 5| Step: 2
Training loss: 2.59920072555542
Validation loss: 2.205080270767212

Epoch: 5| Step: 3
Training loss: 2.897791862487793
Validation loss: 2.182432033682382

Epoch: 5| Step: 4
Training loss: 2.0439138412475586
Validation loss: 2.18500009916162

Epoch: 5| Step: 5
Training loss: 2.2934837341308594
Validation loss: 2.177003488745741

Epoch: 5| Step: 6
Training loss: 2.546117067337036
Validation loss: 2.1859301931114605

Epoch: 5| Step: 7
Training loss: 2.07443904876709
Validation loss: 2.194864011579944

Epoch: 5| Step: 8
Training loss: 2.2642009258270264
Validation loss: 2.231996152990608

Epoch: 5| Step: 9
Training loss: 2.2378287315368652
Validation loss: 2.2862442565220658

Epoch: 5| Step: 10
Training loss: 2.451634645462036
Validation loss: 2.2854041925040622

Epoch: 67| Step: 0
Training loss: 2.086038827896118
Validation loss: 2.2905974926487094

Epoch: 5| Step: 1
Training loss: 2.063417911529541
Validation loss: 2.2481844579019854

Epoch: 5| Step: 2
Training loss: 2.4643921852111816
Validation loss: 2.224853851461923

Epoch: 5| Step: 3
Training loss: 3.314840793609619
Validation loss: 2.2152096609915457

Epoch: 5| Step: 4
Training loss: 2.3067314624786377
Validation loss: 2.225711909673547

Epoch: 5| Step: 5
Training loss: 2.242974042892456
Validation loss: 2.227819981113557

Epoch: 5| Step: 6
Training loss: 2.736832857131958
Validation loss: 2.2178308169047036

Epoch: 5| Step: 7
Training loss: 2.602245330810547
Validation loss: 2.2147717680982364

Epoch: 5| Step: 8
Training loss: 2.215251922607422
Validation loss: 2.1957411791688655

Epoch: 5| Step: 9
Training loss: 3.105402946472168
Validation loss: 2.1936516441324705

Epoch: 5| Step: 10
Training loss: 2.187427520751953
Validation loss: 2.182278266517065

Epoch: 68| Step: 0
Training loss: 3.0218803882598877
Validation loss: 2.1709974145376556

Epoch: 5| Step: 1
Training loss: 2.1380016803741455
Validation loss: 2.1671483952512025

Epoch: 5| Step: 2
Training loss: 2.906400203704834
Validation loss: 2.1718005159849763

Epoch: 5| Step: 3
Training loss: 1.8703482151031494
Validation loss: 2.1647080067665345

Epoch: 5| Step: 4
Training loss: 2.2528626918792725
Validation loss: 2.1564782127257316

Epoch: 5| Step: 5
Training loss: 2.006521701812744
Validation loss: 2.1569970910267164

Epoch: 5| Step: 6
Training loss: 2.4848506450653076
Validation loss: 2.141848724375489

Epoch: 5| Step: 7
Training loss: 3.174398183822632
Validation loss: 2.1432879099281887

Epoch: 5| Step: 8
Training loss: 2.4623847007751465
Validation loss: 2.142844233461606

Epoch: 5| Step: 9
Training loss: 2.5138957500457764
Validation loss: 2.142800167042722

Epoch: 5| Step: 10
Training loss: 2.611506462097168
Validation loss: 2.1416455109914145

Epoch: 69| Step: 0
Training loss: 1.8368898630142212
Validation loss: 2.148933026098436

Epoch: 5| Step: 1
Training loss: 2.5814385414123535
Validation loss: 2.153464486522059

Epoch: 5| Step: 2
Training loss: 2.9721438884735107
Validation loss: 2.164904304729995

Epoch: 5| Step: 3
Training loss: 2.5812463760375977
Validation loss: 2.1686936783534225

Epoch: 5| Step: 4
Training loss: 3.139390707015991
Validation loss: 2.17763759756601

Epoch: 5| Step: 5
Training loss: 2.7258760929107666
Validation loss: 2.168735763078095

Epoch: 5| Step: 6
Training loss: 2.1758525371551514
Validation loss: 2.165264598784908

Epoch: 5| Step: 7
Training loss: 2.3791658878326416
Validation loss: 2.160539222019975

Epoch: 5| Step: 8
Training loss: 1.8297717571258545
Validation loss: 2.14848602971723

Epoch: 5| Step: 9
Training loss: 2.183615207672119
Validation loss: 2.151008388047577

Epoch: 5| Step: 10
Training loss: 2.9446916580200195
Validation loss: 2.155227058677263

Epoch: 70| Step: 0
Training loss: 1.9844141006469727
Validation loss: 2.165873201944495

Epoch: 5| Step: 1
Training loss: 2.6225948333740234
Validation loss: 2.16344698526526

Epoch: 5| Step: 2
Training loss: 2.139355421066284
Validation loss: 2.1495651762972594

Epoch: 5| Step: 3
Training loss: 2.841165065765381
Validation loss: 2.152137352574256

Epoch: 5| Step: 4
Training loss: 2.3889172077178955
Validation loss: 2.140010649158109

Epoch: 5| Step: 5
Training loss: 1.9796020984649658
Validation loss: 2.1398453430462907

Epoch: 5| Step: 6
Training loss: 3.0700016021728516
Validation loss: 2.147114912668864

Epoch: 5| Step: 7
Training loss: 2.458254337310791
Validation loss: 2.1484076592230026

Epoch: 5| Step: 8
Training loss: 2.6420016288757324
Validation loss: 2.156464392139066

Epoch: 5| Step: 9
Training loss: 2.5845677852630615
Validation loss: 2.164485131540606

Epoch: 5| Step: 10
Training loss: 2.439943313598633
Validation loss: 2.184494856865175

Epoch: 71| Step: 0
Training loss: 2.0774130821228027
Validation loss: 2.188701275856264

Epoch: 5| Step: 1
Training loss: 1.9130630493164062
Validation loss: 2.1710103006773096

Epoch: 5| Step: 2
Training loss: 2.015134334564209
Validation loss: 2.157006122732675

Epoch: 5| Step: 3
Training loss: 2.099560022354126
Validation loss: 2.145716018574212

Epoch: 5| Step: 4
Training loss: 2.3962650299072266
Validation loss: 2.134497942463044

Epoch: 5| Step: 5
Training loss: 2.4773178100585938
Validation loss: 2.137506341421476

Epoch: 5| Step: 6
Training loss: 2.756539821624756
Validation loss: 2.135344959074451

Epoch: 5| Step: 7
Training loss: 2.944040060043335
Validation loss: 2.143778654836839

Epoch: 5| Step: 8
Training loss: 2.8543078899383545
Validation loss: 2.14522789114265

Epoch: 5| Step: 9
Training loss: 2.832197666168213
Validation loss: 2.151582899913993

Epoch: 5| Step: 10
Training loss: 2.7383484840393066
Validation loss: 2.1449466341285297

Epoch: 72| Step: 0
Training loss: 2.803447723388672
Validation loss: 2.1451247738253687

Epoch: 5| Step: 1
Training loss: 2.2508296966552734
Validation loss: 2.1362823696546656

Epoch: 5| Step: 2
Training loss: 2.515413999557495
Validation loss: 2.142672223429526

Epoch: 5| Step: 3
Training loss: 2.7164008617401123
Validation loss: 2.1470201066745225

Epoch: 5| Step: 4
Training loss: 2.7383129596710205
Validation loss: 2.142505386824249

Epoch: 5| Step: 5
Training loss: 3.3114216327667236
Validation loss: 2.130173711366551

Epoch: 5| Step: 6
Training loss: 2.001767873764038
Validation loss: 2.123996526964249

Epoch: 5| Step: 7
Training loss: 2.601066827774048
Validation loss: 2.1204675987202632

Epoch: 5| Step: 8
Training loss: 2.3772289752960205
Validation loss: 2.110309221411264

Epoch: 5| Step: 9
Training loss: 1.598118782043457
Validation loss: 2.1121603596595024

Epoch: 5| Step: 10
Training loss: 2.0222971439361572
Validation loss: 2.1180338500648417

Epoch: 73| Step: 0
Training loss: 2.167454242706299
Validation loss: 2.1302987939567974

Epoch: 5| Step: 1
Training loss: 2.6966450214385986
Validation loss: 2.1683877488618255

Epoch: 5| Step: 2
Training loss: 2.414193630218506
Validation loss: 2.167272569030844

Epoch: 5| Step: 3
Training loss: 2.814157485961914
Validation loss: 2.159438907459218

Epoch: 5| Step: 4
Training loss: 2.32417368888855
Validation loss: 2.138115124035907

Epoch: 5| Step: 5
Training loss: 2.141237735748291
Validation loss: 2.1387041537992415

Epoch: 5| Step: 6
Training loss: 2.4591774940490723
Validation loss: 2.1389131007655973

Epoch: 5| Step: 7
Training loss: 2.206348419189453
Validation loss: 2.135655382628082

Epoch: 5| Step: 8
Training loss: 2.148402690887451
Validation loss: 2.1280284838009904

Epoch: 5| Step: 9
Training loss: 2.8061842918395996
Validation loss: 2.137245806314612

Epoch: 5| Step: 10
Training loss: 2.8855104446411133
Validation loss: 2.137593535966771

Epoch: 74| Step: 0
Training loss: 2.5979301929473877
Validation loss: 2.150651239579724

Epoch: 5| Step: 1
Training loss: 2.5452585220336914
Validation loss: 2.1635788948305192

Epoch: 5| Step: 2
Training loss: 2.5102198123931885
Validation loss: 2.193387117437137

Epoch: 5| Step: 3
Training loss: 2.72715425491333
Validation loss: 2.215512652550974

Epoch: 5| Step: 4
Training loss: 2.3944039344787598
Validation loss: 2.1986619554540163

Epoch: 5| Step: 5
Training loss: 2.509192943572998
Validation loss: 2.167166825263731

Epoch: 5| Step: 6
Training loss: 2.7457499504089355
Validation loss: 2.153639708795855

Epoch: 5| Step: 7
Training loss: 1.860630750656128
Validation loss: 2.1365918767067695

Epoch: 5| Step: 8
Training loss: 1.9924564361572266
Validation loss: 2.129148498658211

Epoch: 5| Step: 9
Training loss: 2.5941646099090576
Validation loss: 2.1297484636306763

Epoch: 5| Step: 10
Training loss: 2.4865243434906006
Validation loss: 2.1168102295167985

Epoch: 75| Step: 0
Training loss: 2.658724308013916
Validation loss: 2.111528706806962

Epoch: 5| Step: 1
Training loss: 2.6488003730773926
Validation loss: 2.104469322389172

Epoch: 5| Step: 2
Training loss: 2.688589334487915
Validation loss: 2.110276599084177

Epoch: 5| Step: 3
Training loss: 2.4357175827026367
Validation loss: 2.1247104867812125

Epoch: 5| Step: 4
Training loss: 2.693483352661133
Validation loss: 2.1261767571972263

Epoch: 5| Step: 5
Training loss: 2.1048672199249268
Validation loss: 2.1146057292979252

Epoch: 5| Step: 6
Training loss: 2.321964979171753
Validation loss: 2.101181963438629

Epoch: 5| Step: 7
Training loss: 2.443711042404175
Validation loss: 2.1013591571520736

Epoch: 5| Step: 8
Training loss: 2.3760085105895996
Validation loss: 2.110315756131244

Epoch: 5| Step: 9
Training loss: 2.683337926864624
Validation loss: 2.1302368410172

Epoch: 5| Step: 10
Training loss: 2.0435538291931152
Validation loss: 2.172984960258648

Epoch: 76| Step: 0
Training loss: 2.424246311187744
Validation loss: 2.218436679532451

Epoch: 5| Step: 1
Training loss: 2.904609441757202
Validation loss: 2.2019831108790573

Epoch: 5| Step: 2
Training loss: 2.8033363819122314
Validation loss: 2.1872139387233283

Epoch: 5| Step: 3
Training loss: 3.285588026046753
Validation loss: 2.1609283929230063

Epoch: 5| Step: 4
Training loss: 2.0638022422790527
Validation loss: 2.1229832710758334

Epoch: 5| Step: 5
Training loss: 1.499407410621643
Validation loss: 2.1056455078945366

Epoch: 5| Step: 6
Training loss: 2.3319003582000732
Validation loss: 2.0950273518921225

Epoch: 5| Step: 7
Training loss: 2.3119168281555176
Validation loss: 2.0955570051746983

Epoch: 5| Step: 8
Training loss: 2.649616003036499
Validation loss: 2.097622814998832

Epoch: 5| Step: 9
Training loss: 2.7485384941101074
Validation loss: 2.103388872197879

Epoch: 5| Step: 10
Training loss: 2.071486711502075
Validation loss: 2.1111506082678355

Epoch: 77| Step: 0
Training loss: 2.1263651847839355
Validation loss: 2.1159643819255214

Epoch: 5| Step: 1
Training loss: 2.513098955154419
Validation loss: 2.1121432678673857

Epoch: 5| Step: 2
Training loss: 2.7264888286590576
Validation loss: 2.1182387413517123

Epoch: 5| Step: 3
Training loss: 2.474391222000122
Validation loss: 2.117129930885889

Epoch: 5| Step: 4
Training loss: 2.741201877593994
Validation loss: 2.107004045158304

Epoch: 5| Step: 5
Training loss: 2.5944085121154785
Validation loss: 2.1122308713133617

Epoch: 5| Step: 6
Training loss: 2.0918993949890137
Validation loss: 2.134234764242685

Epoch: 5| Step: 7
Training loss: 2.8139991760253906
Validation loss: 2.137565428210843

Epoch: 5| Step: 8
Training loss: 1.9576795101165771
Validation loss: 2.1677487588697866

Epoch: 5| Step: 9
Training loss: 2.786414623260498
Validation loss: 2.1844891450738393

Epoch: 5| Step: 10
Training loss: 2.0436594486236572
Validation loss: 2.1977719363345893

Epoch: 78| Step: 0
Training loss: 2.156667947769165
Validation loss: 2.182121592183267

Epoch: 5| Step: 1
Training loss: 2.0028557777404785
Validation loss: 2.184312806334547

Epoch: 5| Step: 2
Training loss: 2.4106717109680176
Validation loss: 2.167869985744517

Epoch: 5| Step: 3
Training loss: 2.4683094024658203
Validation loss: 2.168018041118499

Epoch: 5| Step: 4
Training loss: 2.6744656562805176
Validation loss: 2.1587018402673865

Epoch: 5| Step: 5
Training loss: 2.879833221435547
Validation loss: 2.1350860800794376

Epoch: 5| Step: 6
Training loss: 2.583463191986084
Validation loss: 2.128189694496893

Epoch: 5| Step: 7
Training loss: 2.0115466117858887
Validation loss: 2.12480596060394

Epoch: 5| Step: 8
Training loss: 2.6352858543395996
Validation loss: 2.103807269885976

Epoch: 5| Step: 9
Training loss: 2.146061420440674
Validation loss: 2.1035232543945312

Epoch: 5| Step: 10
Training loss: 2.905263900756836
Validation loss: 2.106569599079829

Epoch: 79| Step: 0
Training loss: 2.0691256523132324
Validation loss: 2.1173589691039054

Epoch: 5| Step: 1
Training loss: 3.111907482147217
Validation loss: 2.1266221948849258

Epoch: 5| Step: 2
Training loss: 2.6460306644439697
Validation loss: 2.119567512184061

Epoch: 5| Step: 3
Training loss: 2.5644145011901855
Validation loss: 2.1219979768158286

Epoch: 5| Step: 4
Training loss: 1.55484938621521
Validation loss: 2.1083212308986212

Epoch: 5| Step: 5
Training loss: 2.5551886558532715
Validation loss: 2.1295004019173245

Epoch: 5| Step: 6
Training loss: 2.228919267654419
Validation loss: 2.138973686002916

Epoch: 5| Step: 7
Training loss: 2.3655190467834473
Validation loss: 2.1412161755305466

Epoch: 5| Step: 8
Training loss: 2.9042751789093018
Validation loss: 2.1673711794678883

Epoch: 5| Step: 9
Training loss: 1.9848816394805908
Validation loss: 2.1849237872708227

Epoch: 5| Step: 10
Training loss: 2.650343656539917
Validation loss: 2.2188221100837953

Epoch: 80| Step: 0
Training loss: 2.868821620941162
Validation loss: 2.220950024102324

Epoch: 5| Step: 1
Training loss: 2.1936373710632324
Validation loss: 2.1838528469044673

Epoch: 5| Step: 2
Training loss: 2.213839292526245
Validation loss: 2.1675761668912825

Epoch: 5| Step: 3
Training loss: 2.344771385192871
Validation loss: 2.1325297996562016

Epoch: 5| Step: 4
Training loss: 3.2305617332458496
Validation loss: 2.120948298003084

Epoch: 5| Step: 5
Training loss: 2.5814368724823
Validation loss: 2.1071407807770597

Epoch: 5| Step: 6
Training loss: 2.35105037689209
Validation loss: 2.100222485039824

Epoch: 5| Step: 7
Training loss: 2.5241403579711914
Validation loss: 2.085310141245524

Epoch: 5| Step: 8
Training loss: 2.579181671142578
Validation loss: 2.0807234779480965

Epoch: 5| Step: 9
Training loss: 1.6975551843643188
Validation loss: 2.0814801851908364

Epoch: 5| Step: 10
Training loss: 2.051253080368042
Validation loss: 2.0904457364031064

Epoch: 81| Step: 0
Training loss: 2.863654375076294
Validation loss: 2.0908946503875074

Epoch: 5| Step: 1
Training loss: 2.43412709236145
Validation loss: 2.1014145881898942

Epoch: 5| Step: 2
Training loss: 1.6222387552261353
Validation loss: 2.1167810347772416

Epoch: 5| Step: 3
Training loss: 2.605729579925537
Validation loss: 2.121516817359514

Epoch: 5| Step: 4
Training loss: 2.2026381492614746
Validation loss: 2.137916680305235

Epoch: 5| Step: 5
Training loss: 2.7905266284942627
Validation loss: 2.1287535672546714

Epoch: 5| Step: 6
Training loss: 2.3139102458953857
Validation loss: 2.122527805707788

Epoch: 5| Step: 7
Training loss: 2.0962352752685547
Validation loss: 2.1235815222545336

Epoch: 5| Step: 8
Training loss: 2.0935218334198
Validation loss: 2.150861253020584

Epoch: 5| Step: 9
Training loss: 2.7211155891418457
Validation loss: 2.161851757316179

Epoch: 5| Step: 10
Training loss: 2.755038022994995
Validation loss: 2.1308736980602307

Epoch: 82| Step: 0
Training loss: 2.139091730117798
Validation loss: 2.1227988607139996

Epoch: 5| Step: 1
Training loss: 2.1867880821228027
Validation loss: 2.094229152125697

Epoch: 5| Step: 2
Training loss: 2.726074695587158
Validation loss: 2.0766357760275564

Epoch: 5| Step: 3
Training loss: 2.6487247943878174
Validation loss: 2.0809478439310545

Epoch: 5| Step: 4
Training loss: 2.6945815086364746
Validation loss: 2.0818124496808617

Epoch: 5| Step: 5
Training loss: 2.0787360668182373
Validation loss: 2.09509515505965

Epoch: 5| Step: 6
Training loss: 1.8060195446014404
Validation loss: 2.1034235967102872

Epoch: 5| Step: 7
Training loss: 2.5536317825317383
Validation loss: 2.1068379340633268

Epoch: 5| Step: 8
Training loss: 2.3916268348693848
Validation loss: 2.1244234961848103

Epoch: 5| Step: 9
Training loss: 3.13397479057312
Validation loss: 2.1451871241292646

Epoch: 5| Step: 10
Training loss: 2.317490816116333
Validation loss: 2.162868066500592

Epoch: 83| Step: 0
Training loss: 2.666560649871826
Validation loss: 2.179163876400199

Epoch: 5| Step: 1
Training loss: 2.245415210723877
Validation loss: 2.183875733806241

Epoch: 5| Step: 2
Training loss: 2.6069557666778564
Validation loss: 2.2074646513949157

Epoch: 5| Step: 3
Training loss: 2.516033172607422
Validation loss: 2.2343897140154274

Epoch: 5| Step: 4
Training loss: 2.5555167198181152
Validation loss: 2.233314193705077

Epoch: 5| Step: 5
Training loss: 2.0506792068481445
Validation loss: 2.2323614461447603

Epoch: 5| Step: 6
Training loss: 3.005268096923828
Validation loss: 2.205806529650124

Epoch: 5| Step: 7
Training loss: 2.3388047218322754
Validation loss: 2.1863016595122633

Epoch: 5| Step: 8
Training loss: 2.4694371223449707
Validation loss: 2.1690804932707097

Epoch: 5| Step: 9
Training loss: 2.2946951389312744
Validation loss: 2.1369040525087746

Epoch: 5| Step: 10
Training loss: 1.9618525505065918
Validation loss: 2.085625671571301

Epoch: 84| Step: 0
Training loss: 2.7382445335388184
Validation loss: 2.083609628421004

Epoch: 5| Step: 1
Training loss: 2.245121955871582
Validation loss: 2.0775800174282444

Epoch: 5| Step: 2
Training loss: 2.74454927444458
Validation loss: 2.079017100795623

Epoch: 5| Step: 3
Training loss: 2.460742235183716
Validation loss: 2.0741009942946897

Epoch: 5| Step: 4
Training loss: 2.548227310180664
Validation loss: 2.0730359195381083

Epoch: 5| Step: 5
Training loss: 2.6563358306884766
Validation loss: 2.0782650465606363

Epoch: 5| Step: 6
Training loss: 2.0096817016601562
Validation loss: 2.0753960878618303

Epoch: 5| Step: 7
Training loss: 2.198155403137207
Validation loss: 2.0968942001301754

Epoch: 5| Step: 8
Training loss: 2.6697914600372314
Validation loss: 2.1632514974122405

Epoch: 5| Step: 9
Training loss: 2.243234157562256
Validation loss: 2.1650777991100023

Epoch: 5| Step: 10
Training loss: 1.864476203918457
Validation loss: 2.1419091352852444

Epoch: 85| Step: 0
Training loss: 2.408046007156372
Validation loss: 2.112182805615087

Epoch: 5| Step: 1
Training loss: 2.5611612796783447
Validation loss: 2.1001634495232695

Epoch: 5| Step: 2
Training loss: 1.7969329357147217
Validation loss: 2.089020045854712

Epoch: 5| Step: 3
Training loss: 2.8714663982391357
Validation loss: 2.079914580109299

Epoch: 5| Step: 4
Training loss: 2.640906810760498
Validation loss: 2.0634483803984938

Epoch: 5| Step: 5
Training loss: 2.9364724159240723
Validation loss: 2.059010067293721

Epoch: 5| Step: 6
Training loss: 2.4725513458251953
Validation loss: 2.0544336765043196

Epoch: 5| Step: 7
Training loss: 2.243440628051758
Validation loss: 2.0521183078007033

Epoch: 5| Step: 8
Training loss: 2.199347734451294
Validation loss: 2.060250142569183

Epoch: 5| Step: 9
Training loss: 2.38128662109375
Validation loss: 2.0601896624411307

Epoch: 5| Step: 10
Training loss: 2.084224224090576
Validation loss: 2.072061878378673

Epoch: 86| Step: 0
Training loss: 2.273712158203125
Validation loss: 2.101491689682007

Epoch: 5| Step: 1
Training loss: 2.268073558807373
Validation loss: 2.189599190988848

Epoch: 5| Step: 2
Training loss: 2.59136962890625
Validation loss: 2.302226584444764

Epoch: 5| Step: 3
Training loss: 2.5944113731384277
Validation loss: 2.3593700034644014

Epoch: 5| Step: 4
Training loss: 2.7065775394439697
Validation loss: 2.3654890624425744

Epoch: 5| Step: 5
Training loss: 2.380918025970459
Validation loss: 2.346557959433525

Epoch: 5| Step: 6
Training loss: 2.1506385803222656
Validation loss: 2.2690259231034147

Epoch: 5| Step: 7
Training loss: 2.6607911586761475
Validation loss: 2.207105882706181

Epoch: 5| Step: 8
Training loss: 2.3098320960998535
Validation loss: 2.1437665262529926

Epoch: 5| Step: 9
Training loss: 2.8543057441711426
Validation loss: 2.0961646777327343

Epoch: 5| Step: 10
Training loss: 2.3404979705810547
Validation loss: 2.0893429287018312

Epoch: 87| Step: 0
Training loss: 3.1194489002227783
Validation loss: 2.103247668153496

Epoch: 5| Step: 1
Training loss: 2.532860517501831
Validation loss: 2.1079001990697717

Epoch: 5| Step: 2
Training loss: 2.720561981201172
Validation loss: 2.1301708426526798

Epoch: 5| Step: 3
Training loss: 2.0085289478302
Validation loss: 2.132809086512494

Epoch: 5| Step: 4
Training loss: 2.4996049404144287
Validation loss: 2.1179376058681036

Epoch: 5| Step: 5
Training loss: 2.636444091796875
Validation loss: 2.08878356282429

Epoch: 5| Step: 6
Training loss: 2.01541805267334
Validation loss: 2.086205323537191

Epoch: 5| Step: 7
Training loss: 2.8459556102752686
Validation loss: 2.0893515361252653

Epoch: 5| Step: 8
Training loss: 2.0196967124938965
Validation loss: 2.1224062955507668

Epoch: 5| Step: 9
Training loss: 2.5457141399383545
Validation loss: 2.159946621105235

Epoch: 5| Step: 10
Training loss: 1.7065789699554443
Validation loss: 2.221561966403838

Epoch: 88| Step: 0
Training loss: 2.15661358833313
Validation loss: 2.2853176952690206

Epoch: 5| Step: 1
Training loss: 2.7768635749816895
Validation loss: 2.2754477890588904

Epoch: 5| Step: 2
Training loss: 2.6904072761535645
Validation loss: 2.276427338200231

Epoch: 5| Step: 3
Training loss: 2.1705093383789062
Validation loss: 2.2235869361508276

Epoch: 5| Step: 4
Training loss: 2.4148592948913574
Validation loss: 2.1697735888983614

Epoch: 5| Step: 5
Training loss: 2.4693081378936768
Validation loss: 2.1259101847166657

Epoch: 5| Step: 6
Training loss: 1.824550986289978
Validation loss: 2.097020746559225

Epoch: 5| Step: 7
Training loss: 2.5265183448791504
Validation loss: 2.0845776322067424

Epoch: 5| Step: 8
Training loss: 2.4050071239471436
Validation loss: 2.0774137819966962

Epoch: 5| Step: 9
Training loss: 2.5958735942840576
Validation loss: 2.0819753651977866

Epoch: 5| Step: 10
Training loss: 2.508486747741699
Validation loss: 2.0835697932909896

Epoch: 89| Step: 0
Training loss: 2.6265640258789062
Validation loss: 2.076746835503527

Epoch: 5| Step: 1
Training loss: 2.0693001747131348
Validation loss: 2.064416993048883

Epoch: 5| Step: 2
Training loss: 2.6413159370422363
Validation loss: 2.063906777289606

Epoch: 5| Step: 3
Training loss: 2.569404363632202
Validation loss: 2.070341899830808

Epoch: 5| Step: 4
Training loss: 2.84521222114563
Validation loss: 2.0618897022739535

Epoch: 5| Step: 5
Training loss: 1.6621145009994507
Validation loss: 2.0592984973743396

Epoch: 5| Step: 6
Training loss: 1.892648696899414
Validation loss: 2.0654855210294008

Epoch: 5| Step: 7
Training loss: 2.5262129306793213
Validation loss: 2.0610032517422914

Epoch: 5| Step: 8
Training loss: 2.9472718238830566
Validation loss: 2.0739792700736754

Epoch: 5| Step: 9
Training loss: 2.0459792613983154
Validation loss: 2.0967226925716607

Epoch: 5| Step: 10
Training loss: 2.530205249786377
Validation loss: 2.1297990352876726

Epoch: 90| Step: 0
Training loss: 1.8089386224746704
Validation loss: 2.1791213866203063

Epoch: 5| Step: 1
Training loss: 2.517329692840576
Validation loss: 2.2442851425499044

Epoch: 5| Step: 2
Training loss: 3.0110111236572266
Validation loss: 2.305809949034004

Epoch: 5| Step: 3
Training loss: 2.781269073486328
Validation loss: 2.308093305557005

Epoch: 5| Step: 4
Training loss: 2.456757068634033
Validation loss: 2.2796352858184488

Epoch: 5| Step: 5
Training loss: 2.743856906890869
Validation loss: 2.2634481447999195

Epoch: 5| Step: 6
Training loss: 1.9678771495819092
Validation loss: 2.225020762412779

Epoch: 5| Step: 7
Training loss: 2.681581974029541
Validation loss: 2.149224324892926

Epoch: 5| Step: 8
Training loss: 2.601341724395752
Validation loss: 2.0990229063136603

Epoch: 5| Step: 9
Training loss: 1.9579570293426514
Validation loss: 2.0881448458599787

Epoch: 5| Step: 10
Training loss: 2.1646242141723633
Validation loss: 2.085932767519387

Epoch: 91| Step: 0
Training loss: 2.3480095863342285
Validation loss: 2.077824648990426

Epoch: 5| Step: 1
Training loss: 3.011746883392334
Validation loss: 2.076554513746692

Epoch: 5| Step: 2
Training loss: 2.149522304534912
Validation loss: 2.0731323842079408

Epoch: 5| Step: 3
Training loss: 2.557326555252075
Validation loss: 2.0735961416716218

Epoch: 5| Step: 4
Training loss: 2.3163626194000244
Validation loss: 2.09101668993632

Epoch: 5| Step: 5
Training loss: 2.4965083599090576
Validation loss: 2.0932541931829145

Epoch: 5| Step: 6
Training loss: 1.9775726795196533
Validation loss: 2.1112483688580093

Epoch: 5| Step: 7
Training loss: 1.8886661529541016
Validation loss: 2.107391353576414

Epoch: 5| Step: 8
Training loss: 2.9826462268829346
Validation loss: 2.1068156688444075

Epoch: 5| Step: 9
Training loss: 2.6011998653411865
Validation loss: 2.104240358516734

Epoch: 5| Step: 10
Training loss: 1.996092677116394
Validation loss: 2.088896661676386

Epoch: 92| Step: 0
Training loss: 2.765667200088501
Validation loss: 2.0915498964248167

Epoch: 5| Step: 1
Training loss: 2.2542150020599365
Validation loss: 2.0908338536498365

Epoch: 5| Step: 2
Training loss: 2.4668896198272705
Validation loss: 2.105174410727716

Epoch: 5| Step: 3
Training loss: 1.8381493091583252
Validation loss: 2.0999892860330562

Epoch: 5| Step: 4
Training loss: 2.5266213417053223
Validation loss: 2.11600092405914

Epoch: 5| Step: 5
Training loss: 2.528195858001709
Validation loss: 2.115511304588728

Epoch: 5| Step: 6
Training loss: 2.26296067237854
Validation loss: 2.1184794056800103

Epoch: 5| Step: 7
Training loss: 2.407917022705078
Validation loss: 2.1216051270884853

Epoch: 5| Step: 8
Training loss: 2.3727800846099854
Validation loss: 2.130691061737717

Epoch: 5| Step: 9
Training loss: 2.1203951835632324
Validation loss: 2.135228624907873

Epoch: 5| Step: 10
Training loss: 2.565380334854126
Validation loss: 2.123153118677037

Epoch: 93| Step: 0
Training loss: 1.8363010883331299
Validation loss: 2.089967989152478

Epoch: 5| Step: 1
Training loss: 2.3541035652160645
Validation loss: 2.084797497718565

Epoch: 5| Step: 2
Training loss: 2.015934705734253
Validation loss: 2.062677709005212

Epoch: 5| Step: 3
Training loss: 2.3408043384552
Validation loss: 2.0665469913072485

Epoch: 5| Step: 4
Training loss: 2.379526138305664
Validation loss: 2.076338470623057

Epoch: 5| Step: 5
Training loss: 2.494802474975586
Validation loss: 2.088548042440927

Epoch: 5| Step: 6
Training loss: 3.510239362716675
Validation loss: 2.0708820576308877

Epoch: 5| Step: 7
Training loss: 2.1043543815612793
Validation loss: 2.0798378964906097

Epoch: 5| Step: 8
Training loss: 2.5854461193084717
Validation loss: 2.107542489164619

Epoch: 5| Step: 9
Training loss: 2.37980580329895
Validation loss: 2.1134169550352198

Epoch: 5| Step: 10
Training loss: 1.7955734729766846
Validation loss: 2.1892260556579917

Epoch: 94| Step: 0
Training loss: 1.7217556238174438
Validation loss: 2.237289467165547

Epoch: 5| Step: 1
Training loss: 1.4602560997009277
Validation loss: 2.254263654831917

Epoch: 5| Step: 2
Training loss: 2.394282579421997
Validation loss: 2.2513366950455533

Epoch: 5| Step: 3
Training loss: 1.4890477657318115
Validation loss: 2.1777666627719836

Epoch: 5| Step: 4
Training loss: 2.7309987545013428
Validation loss: 2.1016599978170087

Epoch: 5| Step: 5
Training loss: 2.8614718914031982
Validation loss: 2.0753097559816096

Epoch: 5| Step: 6
Training loss: 2.6571438312530518
Validation loss: 2.0630810209499892

Epoch: 5| Step: 7
Training loss: 3.420192003250122
Validation loss: 2.0506251704308296

Epoch: 5| Step: 8
Training loss: 2.461808443069458
Validation loss: 2.0509668909093386

Epoch: 5| Step: 9
Training loss: 2.7115893363952637
Validation loss: 2.0451205930402203

Epoch: 5| Step: 10
Training loss: 2.3570964336395264
Validation loss: 2.055856872630376

Epoch: 95| Step: 0
Training loss: 1.9171335697174072
Validation loss: 2.053732818172824

Epoch: 5| Step: 1
Training loss: 2.7824676036834717
Validation loss: 2.067121136573053

Epoch: 5| Step: 2
Training loss: 2.7611746788024902
Validation loss: 2.0918157792860463

Epoch: 5| Step: 3
Training loss: 2.3090453147888184
Validation loss: 2.1073388002252065

Epoch: 5| Step: 4
Training loss: 2.322056770324707
Validation loss: 2.123891774044242

Epoch: 5| Step: 5
Training loss: 1.8450441360473633
Validation loss: 2.126397740456366

Epoch: 5| Step: 6
Training loss: 2.5122556686401367
Validation loss: 2.149090682306597

Epoch: 5| Step: 7
Training loss: 2.626945972442627
Validation loss: 2.147150674173909

Epoch: 5| Step: 8
Training loss: 2.000018835067749
Validation loss: 2.1406781942613664

Epoch: 5| Step: 9
Training loss: 2.604544162750244
Validation loss: 2.1331731657828055

Epoch: 5| Step: 10
Training loss: 2.085813522338867
Validation loss: 2.1183247130404235

Epoch: 96| Step: 0
Training loss: 1.5571171045303345
Validation loss: 2.1473906937465874

Epoch: 5| Step: 1
Training loss: 1.9428800344467163
Validation loss: 2.175097409115043

Epoch: 5| Step: 2
Training loss: 2.2552123069763184
Validation loss: 2.2115679633232856

Epoch: 5| Step: 3
Training loss: 2.5934295654296875
Validation loss: 2.2120366801497755

Epoch: 5| Step: 4
Training loss: 2.64841365814209
Validation loss: 2.1670268389486496

Epoch: 5| Step: 5
Training loss: 2.4890170097351074
Validation loss: 2.1335645619259087

Epoch: 5| Step: 6
Training loss: 2.0977282524108887
Validation loss: 2.0937831389006747

Epoch: 5| Step: 7
Training loss: 2.730339527130127
Validation loss: 2.0910115472732054

Epoch: 5| Step: 8
Training loss: 2.5039377212524414
Validation loss: 2.0831887388742096

Epoch: 5| Step: 9
Training loss: 2.9496872425079346
Validation loss: 2.0743079813577796

Epoch: 5| Step: 10
Training loss: 2.2874975204467773
Validation loss: 2.066383531016688

Epoch: 97| Step: 0
Training loss: 2.136845350265503
Validation loss: 2.061542231549499

Epoch: 5| Step: 1
Training loss: 2.6625404357910156
Validation loss: 2.0765235372768935

Epoch: 5| Step: 2
Training loss: 3.016733407974243
Validation loss: 2.1172122275957497

Epoch: 5| Step: 3
Training loss: 2.160949945449829
Validation loss: 2.1368576275405062

Epoch: 5| Step: 4
Training loss: 2.2461676597595215
Validation loss: 2.1494811504117903

Epoch: 5| Step: 5
Training loss: 1.888119101524353
Validation loss: 2.17069980149628

Epoch: 5| Step: 6
Training loss: 2.100132703781128
Validation loss: 2.213859745251235

Epoch: 5| Step: 7
Training loss: 2.8657524585723877
Validation loss: 2.243231127339025

Epoch: 5| Step: 8
Training loss: 2.3847429752349854
Validation loss: 2.2776775975381174

Epoch: 5| Step: 9
Training loss: 2.561306953430176
Validation loss: 2.1753622601109166

Epoch: 5| Step: 10
Training loss: 2.0344796180725098
Validation loss: 2.076334148324946

Epoch: 98| Step: 0
Training loss: 1.9344463348388672
Validation loss: 2.0477123696316957

Epoch: 5| Step: 1
Training loss: 2.144688367843628
Validation loss: 2.0337927033824306

Epoch: 5| Step: 2
Training loss: 2.3349177837371826
Validation loss: 2.022231140444356

Epoch: 5| Step: 3
Training loss: 2.3376095294952393
Validation loss: 2.0199819764783307

Epoch: 5| Step: 4
Training loss: 2.68611478805542
Validation loss: 2.013396427195559

Epoch: 5| Step: 5
Training loss: 2.3922629356384277
Validation loss: 2.0130392146366898

Epoch: 5| Step: 6
Training loss: 2.8442330360412598
Validation loss: 2.0254539135963685

Epoch: 5| Step: 7
Training loss: 2.7562928199768066
Validation loss: 2.0396895870085685

Epoch: 5| Step: 8
Training loss: 2.3320305347442627
Validation loss: 2.062133650625906

Epoch: 5| Step: 9
Training loss: 1.808645248413086
Validation loss: 2.0809018483725925

Epoch: 5| Step: 10
Training loss: 2.2383556365966797
Validation loss: 2.099154226241573

Epoch: 99| Step: 0
Training loss: 2.756516218185425
Validation loss: 2.137660088077668

Epoch: 5| Step: 1
Training loss: 2.2119998931884766
Validation loss: 2.1260803720002532

Epoch: 5| Step: 2
Training loss: 1.9617000818252563
Validation loss: 2.138949332698699

Epoch: 5| Step: 3
Training loss: 2.4249067306518555
Validation loss: 2.1014834501410045

Epoch: 5| Step: 4
Training loss: 2.1708316802978516
Validation loss: 2.0839765110323505

Epoch: 5| Step: 5
Training loss: 2.3427021503448486
Validation loss: 2.096970109529393

Epoch: 5| Step: 6
Training loss: 2.716270923614502
Validation loss: 2.115582853235224

Epoch: 5| Step: 7
Training loss: 2.3648829460144043
Validation loss: 2.1114589296361452

Epoch: 5| Step: 8
Training loss: 1.885006308555603
Validation loss: 2.0812493267879693

Epoch: 5| Step: 9
Training loss: 2.3873956203460693
Validation loss: 2.0799364877003494

Epoch: 5| Step: 10
Training loss: 2.161935329437256
Validation loss: 2.07481328774524

Epoch: 100| Step: 0
Training loss: 1.8783276081085205
Validation loss: 2.065481771704971

Epoch: 5| Step: 1
Training loss: 2.012547731399536
Validation loss: 2.071861764436127

Epoch: 5| Step: 2
Training loss: 2.352522611618042
Validation loss: 2.0707861838802213

Epoch: 5| Step: 3
Training loss: 2.8567068576812744
Validation loss: 2.0923981128200406

Epoch: 5| Step: 4
Training loss: 2.1311962604522705
Validation loss: 2.0882249365570726

Epoch: 5| Step: 5
Training loss: 2.2730295658111572
Validation loss: 2.0824322085226736

Epoch: 5| Step: 6
Training loss: 2.754470109939575
Validation loss: 2.082777532198096

Epoch: 5| Step: 7
Training loss: 2.4545865058898926
Validation loss: 2.08587993344953

Epoch: 5| Step: 8
Training loss: 1.8338617086410522
Validation loss: 2.076907141234285

Epoch: 5| Step: 9
Training loss: 2.1388468742370605
Validation loss: 2.1383720341549126

Epoch: 5| Step: 10
Training loss: 2.58178448677063
Validation loss: 2.178007910328527

Testing loss: 2.264123704698351

Epoch: 1| Step: 0
Training loss: 6.183021545410156
Validation loss: 5.226169950218611

Epoch: 6| Step: 1
Training loss: 2.748706340789795
Validation loss: 5.201893493693362

Epoch: 6| Step: 2
Training loss: 4.511907577514648
Validation loss: 5.183596395677136

Epoch: 6| Step: 3
Training loss: 4.940832138061523
Validation loss: 5.164941305755287

Epoch: 6| Step: 4
Training loss: 4.311392784118652
Validation loss: 5.144574529381209

Epoch: 6| Step: 5
Training loss: 5.25791597366333
Validation loss: 5.121359737970495

Epoch: 6| Step: 6
Training loss: 4.056323051452637
Validation loss: 5.095170195384692

Epoch: 6| Step: 7
Training loss: 6.784544944763184
Validation loss: 5.065471874770298

Epoch: 6| Step: 8
Training loss: 5.100017547607422
Validation loss: 5.032974950728878

Epoch: 6| Step: 9
Training loss: 3.98685884475708
Validation loss: 4.9972467371212534

Epoch: 6| Step: 10
Training loss: 5.174720764160156
Validation loss: 4.958271636757799

Epoch: 6| Step: 11
Training loss: 4.436591625213623
Validation loss: 4.917625637464626

Epoch: 6| Step: 12
Training loss: 5.758942604064941
Validation loss: 4.872298148370558

Epoch: 6| Step: 13
Training loss: 4.780912399291992
Validation loss: 4.826567034567556

Epoch: 2| Step: 0
Training loss: 5.543392181396484
Validation loss: 4.779078260544808

Epoch: 6| Step: 1
Training loss: 5.903672218322754
Validation loss: 4.727800958900041

Epoch: 6| Step: 2
Training loss: 5.389833450317383
Validation loss: 4.675321953271025

Epoch: 6| Step: 3
Training loss: 4.02036190032959
Validation loss: 4.617882374794252

Epoch: 6| Step: 4
Training loss: 3.4351632595062256
Validation loss: 4.556596133016771

Epoch: 6| Step: 5
Training loss: 4.29996395111084
Validation loss: 4.491900772176763

Epoch: 6| Step: 6
Training loss: 3.567336082458496
Validation loss: 4.4284696527706675

Epoch: 6| Step: 7
Training loss: 3.3336172103881836
Validation loss: 4.365657811523766

Epoch: 6| Step: 8
Training loss: 4.298162937164307
Validation loss: 4.304287146496517

Epoch: 6| Step: 9
Training loss: 4.504978179931641
Validation loss: 4.244847654014506

Epoch: 6| Step: 10
Training loss: 3.5298352241516113
Validation loss: 4.195154482318509

Epoch: 6| Step: 11
Training loss: 3.7333850860595703
Validation loss: 4.154864090745167

Epoch: 6| Step: 12
Training loss: 3.745345115661621
Validation loss: 4.117593737058742

Epoch: 6| Step: 13
Training loss: 3.603207588195801
Validation loss: 4.0853336139391825

Epoch: 3| Step: 0
Training loss: 3.893130302429199
Validation loss: 4.051955710175217

Epoch: 6| Step: 1
Training loss: 4.625295162200928
Validation loss: 4.018518037693475

Epoch: 6| Step: 2
Training loss: 3.1070117950439453
Validation loss: 3.9837678170973256

Epoch: 6| Step: 3
Training loss: 3.492922067642212
Validation loss: 3.9558514574522614

Epoch: 6| Step: 4
Training loss: 4.81705379486084
Validation loss: 3.9248974323272705

Epoch: 6| Step: 5
Training loss: 3.693615436553955
Validation loss: 3.892692135226342

Epoch: 6| Step: 6
Training loss: 4.191281795501709
Validation loss: 3.860661265670612

Epoch: 6| Step: 7
Training loss: 3.7517752647399902
Validation loss: 3.8404742440869732

Epoch: 6| Step: 8
Training loss: 2.7032787799835205
Validation loss: 3.8199555540597565

Epoch: 6| Step: 9
Training loss: 4.522586345672607
Validation loss: 3.794834275399485

Epoch: 6| Step: 10
Training loss: 3.0572304725646973
Validation loss: 3.7613814287288214

Epoch: 6| Step: 11
Training loss: 2.809272527694702
Validation loss: 3.740635548868487

Epoch: 6| Step: 12
Training loss: 3.5310068130493164
Validation loss: 3.72126200122218

Epoch: 6| Step: 13
Training loss: 4.590150833129883
Validation loss: 3.7048305285874235

Epoch: 4| Step: 0
Training loss: 3.2206244468688965
Validation loss: 3.687681367320399

Epoch: 6| Step: 1
Training loss: 2.745685577392578
Validation loss: 3.6677523428393948

Epoch: 6| Step: 2
Training loss: 4.441438674926758
Validation loss: 3.651168679678312

Epoch: 6| Step: 3
Training loss: 2.846514940261841
Validation loss: 3.632687532773582

Epoch: 6| Step: 4
Training loss: 3.3826920986175537
Validation loss: 3.614925317866828

Epoch: 6| Step: 5
Training loss: 3.8176140785217285
Validation loss: 3.597804407919607

Epoch: 6| Step: 6
Training loss: 2.8566112518310547
Validation loss: 3.5821605959246234

Epoch: 6| Step: 7
Training loss: 4.481019020080566
Validation loss: 3.5839644042394494

Epoch: 6| Step: 8
Training loss: 3.9930148124694824
Validation loss: 3.554822906371086

Epoch: 6| Step: 9
Training loss: 2.7274200916290283
Validation loss: 3.5341716222865607

Epoch: 6| Step: 10
Training loss: 3.013597011566162
Validation loss: 3.520373139330136

Epoch: 6| Step: 11
Training loss: 3.2199454307556152
Validation loss: 3.510124526998048

Epoch: 6| Step: 12
Training loss: 4.393634796142578
Validation loss: 3.4932377466591458

Epoch: 6| Step: 13
Training loss: 4.189296722412109
Validation loss: 3.4753937182887906

Epoch: 5| Step: 0
Training loss: 3.7596638202667236
Validation loss: 3.455580360145979

Epoch: 6| Step: 1
Training loss: 3.400486946105957
Validation loss: 3.4410160382588706

Epoch: 6| Step: 2
Training loss: 4.43499231338501
Validation loss: 3.437977070449501

Epoch: 6| Step: 3
Training loss: 3.403029441833496
Validation loss: 3.418135686587262

Epoch: 6| Step: 4
Training loss: 2.008875846862793
Validation loss: 3.400160938180903

Epoch: 6| Step: 5
Training loss: 3.4118359088897705
Validation loss: 3.388177776849398

Epoch: 6| Step: 6
Training loss: 3.874457359313965
Validation loss: 3.3720613705214633

Epoch: 6| Step: 7
Training loss: 3.057878017425537
Validation loss: 3.3614334137209

Epoch: 6| Step: 8
Training loss: 4.452689170837402
Validation loss: 3.3508662818580546

Epoch: 6| Step: 9
Training loss: 2.2237000465393066
Validation loss: 3.3336822191874185

Epoch: 6| Step: 10
Training loss: 2.599365711212158
Validation loss: 3.320299897142636

Epoch: 6| Step: 11
Training loss: 3.2005178928375244
Validation loss: 3.30627098391133

Epoch: 6| Step: 12
Training loss: 3.44730281829834
Validation loss: 3.2952992044469362

Epoch: 6| Step: 13
Training loss: 3.2676119804382324
Validation loss: 3.278601159331619

Epoch: 6| Step: 0
Training loss: 3.585358142852783
Validation loss: 3.268364526892221

Epoch: 6| Step: 1
Training loss: 3.7600622177124023
Validation loss: 3.2515583961240706

Epoch: 6| Step: 2
Training loss: 2.7956385612487793
Validation loss: 3.2416693190092682

Epoch: 6| Step: 3
Training loss: 3.5514392852783203
Validation loss: 3.2290477701412734

Epoch: 6| Step: 4
Training loss: 4.5634446144104
Validation loss: 3.21574410828211

Epoch: 6| Step: 5
Training loss: 2.9048337936401367
Validation loss: 3.20981437929215

Epoch: 6| Step: 6
Training loss: 1.396876573562622
Validation loss: 3.193023153530654

Epoch: 6| Step: 7
Training loss: 3.6817774772644043
Validation loss: 3.181134670011459

Epoch: 6| Step: 8
Training loss: 2.8543202877044678
Validation loss: 3.170225666415307

Epoch: 6| Step: 9
Training loss: 2.7125754356384277
Validation loss: 3.157257690224596

Epoch: 6| Step: 10
Training loss: 3.0692567825317383
Validation loss: 3.151511335885653

Epoch: 6| Step: 11
Training loss: 4.1057868003845215
Validation loss: 3.144723864011867

Epoch: 6| Step: 12
Training loss: 2.1484901905059814
Validation loss: 3.1420454312396306

Epoch: 6| Step: 13
Training loss: 3.858407735824585
Validation loss: 3.136217450582853

Epoch: 7| Step: 0
Training loss: 2.465440034866333
Validation loss: 3.1320483043629634

Epoch: 6| Step: 1
Training loss: 3.143941879272461
Validation loss: 3.118048314125307

Epoch: 6| Step: 2
Training loss: 3.2801506519317627
Validation loss: 3.1057287672514557

Epoch: 6| Step: 3
Training loss: 3.065800428390503
Validation loss: 3.088142584728938

Epoch: 6| Step: 4
Training loss: 2.4474406242370605
Validation loss: 3.086764786833076

Epoch: 6| Step: 5
Training loss: 2.2752585411071777
Validation loss: 3.1215487962128012

Epoch: 6| Step: 6
Training loss: 3.966975450515747
Validation loss: 3.1726778066286476

Epoch: 6| Step: 7
Training loss: 3.575407028198242
Validation loss: 3.1433963160361014

Epoch: 6| Step: 8
Training loss: 3.7971625328063965
Validation loss: 3.1050367906529415

Epoch: 6| Step: 9
Training loss: 3.524421453475952
Validation loss: 3.0851781393892024

Epoch: 6| Step: 10
Training loss: 2.586129665374756
Validation loss: 3.0904885261289534

Epoch: 6| Step: 11
Training loss: 3.788783073425293
Validation loss: 3.1031617323557534

Epoch: 6| Step: 12
Training loss: 3.4062602519989014
Validation loss: 3.1187272405111663

Epoch: 6| Step: 13
Training loss: 2.15295147895813
Validation loss: 3.10190612013622

Epoch: 8| Step: 0
Training loss: 2.4955220222473145
Validation loss: 3.0863989963326404

Epoch: 6| Step: 1
Training loss: 3.974853992462158
Validation loss: 3.06323738508327

Epoch: 6| Step: 2
Training loss: 3.2731921672821045
Validation loss: 3.015514040506014

Epoch: 6| Step: 3
Training loss: 3.3355631828308105
Validation loss: 2.979927388570642

Epoch: 6| Step: 4
Training loss: 4.026793479919434
Validation loss: 2.9859727813351538

Epoch: 6| Step: 5
Training loss: 2.899822473526001
Validation loss: 3.0139063891544136

Epoch: 6| Step: 6
Training loss: 2.6351099014282227
Validation loss: 2.963393113946402

Epoch: 6| Step: 7
Training loss: 3.249799966812134
Validation loss: 2.946188001222508

Epoch: 6| Step: 8
Training loss: 3.1801342964172363
Validation loss: 2.949059437679988

Epoch: 6| Step: 9
Training loss: 2.5736045837402344
Validation loss: 2.985449937082106

Epoch: 6| Step: 10
Training loss: 2.938194513320923
Validation loss: 2.9659688472747803

Epoch: 6| Step: 11
Training loss: 2.9186911582946777
Validation loss: 2.95999046807648

Epoch: 6| Step: 12
Training loss: 2.5168967247009277
Validation loss: 2.9520180994464504

Epoch: 6| Step: 13
Training loss: 2.707064628601074
Validation loss: 2.949311743500412

Epoch: 9| Step: 0
Training loss: 3.2623324394226074
Validation loss: 2.946451602443572

Epoch: 6| Step: 1
Training loss: 3.449136734008789
Validation loss: 2.936049856165404

Epoch: 6| Step: 2
Training loss: 3.244114398956299
Validation loss: 2.929422201648835

Epoch: 6| Step: 3
Training loss: 3.189645290374756
Validation loss: 2.924423876629081

Epoch: 6| Step: 4
Training loss: 3.3048853874206543
Validation loss: 2.9139899874246247

Epoch: 6| Step: 5
Training loss: 2.6714818477630615
Validation loss: 2.9058005963602374

Epoch: 6| Step: 6
Training loss: 2.19144606590271
Validation loss: 2.899577448444982

Epoch: 6| Step: 7
Training loss: 2.943557024002075
Validation loss: 2.8891282363604476

Epoch: 6| Step: 8
Training loss: 2.89408540725708
Validation loss: 2.8838805280705935

Epoch: 6| Step: 9
Training loss: 2.672959804534912
Validation loss: 2.8830132330617597

Epoch: 6| Step: 10
Training loss: 2.015937566757202
Validation loss: 2.8839215617026053

Epoch: 6| Step: 11
Training loss: 2.541614055633545
Validation loss: 2.9488167762756348

Epoch: 6| Step: 12
Training loss: 4.030223369598389
Validation loss: 2.8706846467910276

Epoch: 6| Step: 13
Training loss: 4.057162284851074
Validation loss: 2.8717887709217687

Epoch: 10| Step: 0
Training loss: 2.984308958053589
Validation loss: 2.877556462441721

Epoch: 6| Step: 1
Training loss: 2.5152268409729004
Validation loss: 2.9009421128098682

Epoch: 6| Step: 2
Training loss: 3.862497329711914
Validation loss: 2.9410689723107124

Epoch: 6| Step: 3
Training loss: 3.7176740169525146
Validation loss: 2.935591305455854

Epoch: 6| Step: 4
Training loss: 2.943270206451416
Validation loss: 2.864464970045192

Epoch: 6| Step: 5
Training loss: 3.4980459213256836
Validation loss: 2.851722709594234

Epoch: 6| Step: 6
Training loss: 2.465519666671753
Validation loss: 2.846466636144987

Epoch: 6| Step: 7
Training loss: 2.4529309272766113
Validation loss: 2.850710666307839

Epoch: 6| Step: 8
Training loss: 2.597696304321289
Validation loss: 2.8631729772014003

Epoch: 6| Step: 9
Training loss: 3.404186725616455
Validation loss: 2.8523342840133177

Epoch: 6| Step: 10
Training loss: 2.7582380771636963
Validation loss: 2.8463784827980945

Epoch: 6| Step: 11
Training loss: 3.6282663345336914
Validation loss: 2.83621129169259

Epoch: 6| Step: 12
Training loss: 1.901352882385254
Validation loss: 2.830365042532644

Epoch: 6| Step: 13
Training loss: 2.992858409881592
Validation loss: 2.820412738348848

Epoch: 11| Step: 0
Training loss: 2.0406360626220703
Validation loss: 2.8168707252830587

Epoch: 6| Step: 1
Training loss: 3.0352537631988525
Validation loss: 2.8166511161353

Epoch: 6| Step: 2
Training loss: 1.9820737838745117
Validation loss: 2.8169583171926518

Epoch: 6| Step: 3
Training loss: 2.4711625576019287
Validation loss: 2.830974755748626

Epoch: 6| Step: 4
Training loss: 2.878526210784912
Validation loss: 2.8031370203982116

Epoch: 6| Step: 5
Training loss: 2.613941192626953
Validation loss: 2.807504892349243

Epoch: 6| Step: 6
Training loss: 3.0555315017700195
Validation loss: 2.811896934304186

Epoch: 6| Step: 7
Training loss: 3.7662107944488525
Validation loss: 2.805951561979068

Epoch: 6| Step: 8
Training loss: 2.1031999588012695
Validation loss: 2.8059760447471374

Epoch: 6| Step: 9
Training loss: 4.2967448234558105
Validation loss: 2.805321080710298

Epoch: 6| Step: 10
Training loss: 2.8954803943634033
Validation loss: 2.7941812443476852

Epoch: 6| Step: 11
Training loss: 3.4813895225524902
Validation loss: 2.7876470268413587

Epoch: 6| Step: 12
Training loss: 2.9362430572509766
Validation loss: 2.7792857872542513

Epoch: 6| Step: 13
Training loss: 3.953158140182495
Validation loss: 2.77608799165295

Epoch: 12| Step: 0
Training loss: 2.519691228866577
Validation loss: 2.772372948226108

Epoch: 6| Step: 1
Training loss: 2.93647837638855
Validation loss: 2.766396686594973

Epoch: 6| Step: 2
Training loss: 2.5717289447784424
Validation loss: 2.766236941019694

Epoch: 6| Step: 3
Training loss: 2.7755486965179443
Validation loss: 2.7630629180580057

Epoch: 6| Step: 4
Training loss: 3.074885606765747
Validation loss: 2.7612101518979637

Epoch: 6| Step: 5
Training loss: 2.661684989929199
Validation loss: 2.758208082568261

Epoch: 6| Step: 6
Training loss: 2.7932422161102295
Validation loss: 2.7571399365701983

Epoch: 6| Step: 7
Training loss: 2.691765785217285
Validation loss: 2.7535700182760916

Epoch: 6| Step: 8
Training loss: 3.3098132610321045
Validation loss: 2.750566808126306

Epoch: 6| Step: 9
Training loss: 2.9563848972320557
Validation loss: 2.7478707016155286

Epoch: 6| Step: 10
Training loss: 2.730900287628174
Validation loss: 2.7466963619314213

Epoch: 6| Step: 11
Training loss: 3.4304111003875732
Validation loss: 2.7423576103743685

Epoch: 6| Step: 12
Training loss: 3.377196788787842
Validation loss: 2.738191202122678

Epoch: 6| Step: 13
Training loss: 2.6839606761932373
Validation loss: 2.735646350409395

Epoch: 13| Step: 0
Training loss: 2.9875669479370117
Validation loss: 2.734256870003157

Epoch: 6| Step: 1
Training loss: 3.733417510986328
Validation loss: 2.73250566246689

Epoch: 6| Step: 2
Training loss: 3.1030397415161133
Validation loss: 2.7329213542322957

Epoch: 6| Step: 3
Training loss: 2.7816171646118164
Validation loss: 2.7367963149983394

Epoch: 6| Step: 4
Training loss: 2.8546338081359863
Validation loss: 2.7416853725269275

Epoch: 6| Step: 5
Training loss: 3.458756923675537
Validation loss: 2.7387955675842943

Epoch: 6| Step: 6
Training loss: 2.394371747970581
Validation loss: 2.7299332285440094

Epoch: 6| Step: 7
Training loss: 3.297884464263916
Validation loss: 2.72679732179129

Epoch: 6| Step: 8
Training loss: 2.674837112426758
Validation loss: 2.7218213645360803

Epoch: 6| Step: 9
Training loss: 2.9746551513671875
Validation loss: 2.715886546719459

Epoch: 6| Step: 10
Training loss: 2.138883590698242
Validation loss: 2.715992740405503

Epoch: 6| Step: 11
Training loss: 2.3948397636413574
Validation loss: 2.7122306362275155

Epoch: 6| Step: 12
Training loss: 2.4289422035217285
Validation loss: 2.711179123129896

Epoch: 6| Step: 13
Training loss: 3.419159412384033
Validation loss: 2.7064561792599258

Epoch: 14| Step: 0
Training loss: 2.5897064208984375
Validation loss: 2.714432766360621

Epoch: 6| Step: 1
Training loss: 2.2134742736816406
Validation loss: 2.701740518693001

Epoch: 6| Step: 2
Training loss: 3.3218085765838623
Validation loss: 2.707919241279684

Epoch: 6| Step: 3
Training loss: 3.4096035957336426
Validation loss: 2.704414749658236

Epoch: 6| Step: 4
Training loss: 3.2808008193969727
Validation loss: 2.706953530670494

Epoch: 6| Step: 5
Training loss: 2.3891987800598145
Validation loss: 2.7068376566774104

Epoch: 6| Step: 6
Training loss: 3.1861953735351562
Validation loss: 2.7020684416576097

Epoch: 6| Step: 7
Training loss: 2.8269662857055664
Validation loss: 2.7016947807804232

Epoch: 6| Step: 8
Training loss: 3.3999149799346924
Validation loss: 2.697820658324867

Epoch: 6| Step: 9
Training loss: 2.07534122467041
Validation loss: 2.6985164765388734

Epoch: 6| Step: 10
Training loss: 3.259693145751953
Validation loss: 2.701425216531241

Epoch: 6| Step: 11
Training loss: 3.2219481468200684
Validation loss: 2.6958510721883466

Epoch: 6| Step: 12
Training loss: 2.5016121864318848
Validation loss: 2.6944329148979596

Epoch: 6| Step: 13
Training loss: 2.1998050212860107
Validation loss: 2.6962084924021075

Epoch: 15| Step: 0
Training loss: 2.5099258422851562
Validation loss: 2.695111718229068

Epoch: 6| Step: 1
Training loss: 2.3455779552459717
Validation loss: 2.6969999446663806

Epoch: 6| Step: 2
Training loss: 3.299091339111328
Validation loss: 2.6938659452622935

Epoch: 6| Step: 3
Training loss: 2.518791675567627
Validation loss: 2.6933717727661133

Epoch: 6| Step: 4
Training loss: 3.797152519226074
Validation loss: 2.6896793842315674

Epoch: 6| Step: 5
Training loss: 2.574277877807617
Validation loss: 2.687252767624394

Epoch: 6| Step: 6
Training loss: 2.7353694438934326
Validation loss: 2.6859111247524137

Epoch: 6| Step: 7
Training loss: 2.922118663787842
Validation loss: 2.684533808820991

Epoch: 6| Step: 8
Training loss: 3.4776113033294678
Validation loss: 2.683997256781465

Epoch: 6| Step: 9
Training loss: 3.055136203765869
Validation loss: 2.6824549116114134

Epoch: 6| Step: 10
Training loss: 2.4837214946746826
Validation loss: 2.6808993944557766

Epoch: 6| Step: 11
Training loss: 2.862889289855957
Validation loss: 2.6796964573603805

Epoch: 6| Step: 12
Training loss: 2.367384433746338
Validation loss: 2.678638853052611

Epoch: 6| Step: 13
Training loss: 3.2929346561431885
Validation loss: 2.676386502481276

Epoch: 16| Step: 0
Training loss: 2.46040415763855
Validation loss: 2.679135091843144

Epoch: 6| Step: 1
Training loss: 3.1223015785217285
Validation loss: 2.6838389904268327

Epoch: 6| Step: 2
Training loss: 3.08502197265625
Validation loss: 2.673482556496897

Epoch: 6| Step: 3
Training loss: 3.530581474304199
Validation loss: 2.668283841943228

Epoch: 6| Step: 4
Training loss: 3.3031675815582275
Validation loss: 2.663460967361286

Epoch: 6| Step: 5
Training loss: 2.694577693939209
Validation loss: 2.6620786446397022

Epoch: 6| Step: 6
Training loss: 2.6199586391448975
Validation loss: 2.6584910320979294

Epoch: 6| Step: 7
Training loss: 2.596726894378662
Validation loss: 2.656711562987297

Epoch: 6| Step: 8
Training loss: 2.871798276901245
Validation loss: 2.6517769726373817

Epoch: 6| Step: 9
Training loss: 2.93820858001709
Validation loss: 2.6492517379022416

Epoch: 6| Step: 10
Training loss: 2.2482857704162598
Validation loss: 2.6460819628930863

Epoch: 6| Step: 11
Training loss: 2.7961626052856445
Validation loss: 2.6423109680093746

Epoch: 6| Step: 12
Training loss: 2.453449249267578
Validation loss: 2.6363095865454724

Epoch: 6| Step: 13
Training loss: 3.313722848892212
Validation loss: 2.6406119151781966

Epoch: 17| Step: 0
Training loss: 3.5897977352142334
Validation loss: 2.6411499079837593

Epoch: 6| Step: 1
Training loss: 2.1325535774230957
Validation loss: 2.644660552342733

Epoch: 6| Step: 2
Training loss: 2.1881449222564697
Validation loss: 2.669725471927274

Epoch: 6| Step: 3
Training loss: 3.4165701866149902
Validation loss: 2.7029996559184086

Epoch: 6| Step: 4
Training loss: 2.8574633598327637
Validation loss: 2.6824362816349154

Epoch: 6| Step: 5
Training loss: 2.530106544494629
Validation loss: 2.673167702972248

Epoch: 6| Step: 6
Training loss: 2.419358730316162
Validation loss: 2.666551395129132

Epoch: 6| Step: 7
Training loss: 2.5317065715789795
Validation loss: 2.6850433195790937

Epoch: 6| Step: 8
Training loss: 2.716564655303955
Validation loss: 2.6894424653822377

Epoch: 6| Step: 9
Training loss: 4.123915195465088
Validation loss: 2.682726457554807

Epoch: 6| Step: 10
Training loss: 2.8440306186676025
Validation loss: 2.6760335250567366

Epoch: 6| Step: 11
Training loss: 3.234926700592041
Validation loss: 2.668458669416366

Epoch: 6| Step: 12
Training loss: 2.6059062480926514
Validation loss: 2.6619099006857923

Epoch: 6| Step: 13
Training loss: 2.642679452896118
Validation loss: 2.652301142292638

Epoch: 18| Step: 0
Training loss: 3.155611753463745
Validation loss: 2.65648651635775

Epoch: 6| Step: 1
Training loss: 2.6020002365112305
Validation loss: 2.6595632440300396

Epoch: 6| Step: 2
Training loss: 2.8683276176452637
Validation loss: 2.6504428155960573

Epoch: 6| Step: 3
Training loss: 2.653597831726074
Validation loss: 2.648324046083676

Epoch: 6| Step: 4
Training loss: 3.231661319732666
Validation loss: 2.627351704464164

Epoch: 6| Step: 5
Training loss: 2.0503408908843994
Validation loss: 2.6207811524791103

Epoch: 6| Step: 6
Training loss: 2.177889585494995
Validation loss: 2.624405473791143

Epoch: 6| Step: 7
Training loss: 2.750547170639038
Validation loss: 2.636813412430466

Epoch: 6| Step: 8
Training loss: 3.185023069381714
Validation loss: 2.6418466849993636

Epoch: 6| Step: 9
Training loss: 3.671081066131592
Validation loss: 2.6413187749924196

Epoch: 6| Step: 10
Training loss: 3.522916316986084
Validation loss: 2.643781347941327

Epoch: 6| Step: 11
Training loss: 2.6492228507995605
Validation loss: 2.6463148465720554

Epoch: 6| Step: 12
Training loss: 2.462125301361084
Validation loss: 2.646233102326752

Epoch: 6| Step: 13
Training loss: 2.633012294769287
Validation loss: 2.6568268063247844

Epoch: 19| Step: 0
Training loss: 3.3975179195404053
Validation loss: 2.65443576792235

Epoch: 6| Step: 1
Training loss: 2.7753939628601074
Validation loss: 2.6327766192856656

Epoch: 6| Step: 2
Training loss: 3.2013421058654785
Validation loss: 2.6215003792957594

Epoch: 6| Step: 3
Training loss: 3.1241369247436523
Validation loss: 2.6096081579885175

Epoch: 6| Step: 4
Training loss: 3.1946403980255127
Validation loss: 2.607814045362575

Epoch: 6| Step: 5
Training loss: 2.3807950019836426
Validation loss: 2.60616748820069

Epoch: 6| Step: 6
Training loss: 2.216154098510742
Validation loss: 2.6073732273553007

Epoch: 6| Step: 7
Training loss: 2.1969029903411865
Validation loss: 2.5992120824834353

Epoch: 6| Step: 8
Training loss: 2.8767590522766113
Validation loss: 2.592782784533757

Epoch: 6| Step: 9
Training loss: 2.546776294708252
Validation loss: 2.5864004652987242

Epoch: 6| Step: 10
Training loss: 3.079162120819092
Validation loss: 2.5888322425144974

Epoch: 6| Step: 11
Training loss: 2.7303638458251953
Validation loss: 2.5947755305997786

Epoch: 6| Step: 12
Training loss: 2.531348705291748
Validation loss: 2.5996841512700564

Epoch: 6| Step: 13
Training loss: 3.379944324493408
Validation loss: 2.6339886675598803

Epoch: 20| Step: 0
Training loss: 2.7101497650146484
Validation loss: 2.6512382568851596

Epoch: 6| Step: 1
Training loss: 2.33904767036438
Validation loss: 2.647944281178136

Epoch: 6| Step: 2
Training loss: 3.0766258239746094
Validation loss: 2.6508783191762944

Epoch: 6| Step: 3
Training loss: 3.873335838317871
Validation loss: 2.726172072913057

Epoch: 6| Step: 4
Training loss: 2.742445945739746
Validation loss: 2.761466872307562

Epoch: 6| Step: 5
Training loss: 2.412661552429199
Validation loss: 2.746561973325668

Epoch: 6| Step: 6
Training loss: 2.6372241973876953
Validation loss: 2.735854233464887

Epoch: 6| Step: 7
Training loss: 3.1609442234039307
Validation loss: 2.738616415249404

Epoch: 6| Step: 8
Training loss: 2.8960275650024414
Validation loss: 2.7281058065352903

Epoch: 6| Step: 9
Training loss: 2.9599294662475586
Validation loss: 2.735123106228408

Epoch: 6| Step: 10
Training loss: 3.6770122051239014
Validation loss: 2.7422651578021306

Epoch: 6| Step: 11
Training loss: 3.2121758460998535
Validation loss: 2.7444817455866004

Epoch: 6| Step: 12
Training loss: 2.3009250164031982
Validation loss: 2.754841886540895

Epoch: 6| Step: 13
Training loss: 2.2003960609436035
Validation loss: 2.7463965082681305

Epoch: 21| Step: 0
Training loss: 2.954434394836426
Validation loss: 2.735589001768379

Epoch: 6| Step: 1
Training loss: 2.3028602600097656
Validation loss: 2.733949835582446

Epoch: 6| Step: 2
Training loss: 3.660829782485962
Validation loss: 2.79473142726447

Epoch: 6| Step: 3
Training loss: 2.9120192527770996
Validation loss: 2.766480594552973

Epoch: 6| Step: 4
Training loss: 2.504704475402832
Validation loss: 2.7486429522114415

Epoch: 6| Step: 5
Training loss: 2.559598922729492
Validation loss: 2.7299802200768584

Epoch: 6| Step: 6
Training loss: 3.9132940769195557
Validation loss: 2.7335582394753732

Epoch: 6| Step: 7
Training loss: 2.531344413757324
Validation loss: 2.72228072022879

Epoch: 6| Step: 8
Training loss: 3.037428379058838
Validation loss: 2.7187239098292526

Epoch: 6| Step: 9
Training loss: 3.186232089996338
Validation loss: 2.7080243736185055

Epoch: 6| Step: 10
Training loss: 2.5155487060546875
Validation loss: 2.679334450793523

Epoch: 6| Step: 11
Training loss: 2.584901809692383
Validation loss: 2.6733882504124797

Epoch: 6| Step: 12
Training loss: 3.2264881134033203
Validation loss: 2.6008069181954987

Epoch: 6| Step: 13
Training loss: 2.4217216968536377
Validation loss: 2.5924117257518153

Epoch: 22| Step: 0
Training loss: 3.7235569953918457
Validation loss: 2.615859244459419

Epoch: 6| Step: 1
Training loss: 2.4841766357421875
Validation loss: 2.660867803840227

Epoch: 6| Step: 2
Training loss: 2.7602341175079346
Validation loss: 2.6302725679130963

Epoch: 6| Step: 3
Training loss: 2.1514439582824707
Validation loss: 2.6210558363186416

Epoch: 6| Step: 4
Training loss: 3.374065399169922
Validation loss: 2.59821121410657

Epoch: 6| Step: 5
Training loss: 3.033601760864258
Validation loss: 2.586768647675873

Epoch: 6| Step: 6
Training loss: 2.7424464225769043
Validation loss: 2.5843807061513266

Epoch: 6| Step: 7
Training loss: 2.742882013320923
Validation loss: 2.5792736622595016

Epoch: 6| Step: 8
Training loss: 2.3599448204040527
Validation loss: 2.5814260282824115

Epoch: 6| Step: 9
Training loss: 2.8000481128692627
Validation loss: 2.585013758751654

Epoch: 6| Step: 10
Training loss: 2.7496485710144043
Validation loss: 2.59194916038103

Epoch: 6| Step: 11
Training loss: 2.873518466949463
Validation loss: 2.592582492418187

Epoch: 6| Step: 12
Training loss: 2.7595367431640625
Validation loss: 2.5662085727978776

Epoch: 6| Step: 13
Training loss: 2.5087268352508545
Validation loss: 2.55759750130356

Epoch: 23| Step: 0
Training loss: 2.3575079441070557
Validation loss: 2.5583197557797996

Epoch: 6| Step: 1
Training loss: 2.982379913330078
Validation loss: 2.5710105460177184

Epoch: 6| Step: 2
Training loss: 2.984431743621826
Validation loss: 2.586217354702693

Epoch: 6| Step: 3
Training loss: 2.4127445220947266
Validation loss: 2.6031343244737193

Epoch: 6| Step: 4
Training loss: 3.0750231742858887
Validation loss: 2.5929571249151744

Epoch: 6| Step: 5
Training loss: 2.424694776535034
Validation loss: 2.5707867478811615

Epoch: 6| Step: 6
Training loss: 2.4751083850860596
Validation loss: 2.549393484669347

Epoch: 6| Step: 7
Training loss: 3.3243911266326904
Validation loss: 2.541781827967654

Epoch: 6| Step: 8
Training loss: 2.9249489307403564
Validation loss: 2.537927532708773

Epoch: 6| Step: 9
Training loss: 2.954038143157959
Validation loss: 2.540318101964971

Epoch: 6| Step: 10
Training loss: 3.1131997108459473
Validation loss: 2.544036293542513

Epoch: 6| Step: 11
Training loss: 3.0713608264923096
Validation loss: 2.54279000015669

Epoch: 6| Step: 12
Training loss: 2.6005496978759766
Validation loss: 2.5505160003580074

Epoch: 6| Step: 13
Training loss: 1.8832505941390991
Validation loss: 2.551526342668841

Epoch: 24| Step: 0
Training loss: 3.128282070159912
Validation loss: 2.5579554727000575

Epoch: 6| Step: 1
Training loss: 3.1276071071624756
Validation loss: 2.5670511350836804

Epoch: 6| Step: 2
Training loss: 2.867185592651367
Validation loss: 2.5606088638305664

Epoch: 6| Step: 3
Training loss: 1.936118721961975
Validation loss: 2.5505759664761123

Epoch: 6| Step: 4
Training loss: 2.539104461669922
Validation loss: 2.5594539898698048

Epoch: 6| Step: 5
Training loss: 2.8493757247924805
Validation loss: 2.5389783510597805

Epoch: 6| Step: 6
Training loss: 2.0158822536468506
Validation loss: 2.533270687185308

Epoch: 6| Step: 7
Training loss: 2.519836664199829
Validation loss: 2.5375726992084133

Epoch: 6| Step: 8
Training loss: 2.972905158996582
Validation loss: 2.5489958742613434

Epoch: 6| Step: 9
Training loss: 3.1732096672058105
Validation loss: 2.552210764218402

Epoch: 6| Step: 10
Training loss: 2.727351665496826
Validation loss: 2.6045562246794343

Epoch: 6| Step: 11
Training loss: 3.1110968589782715
Validation loss: 2.6641879338090138

Epoch: 6| Step: 12
Training loss: 3.120987892150879
Validation loss: 2.6729746480141916

Epoch: 6| Step: 13
Training loss: 3.081368923187256
Validation loss: 2.6704421633033344

Epoch: 25| Step: 0
Training loss: 3.22082781791687
Validation loss: 2.6327189066076793

Epoch: 6| Step: 1
Training loss: 2.5028369426727295
Validation loss: 2.6201919483882126

Epoch: 6| Step: 2
Training loss: 2.764066457748413
Validation loss: 2.6051228994964273

Epoch: 6| Step: 3
Training loss: 2.2588119506835938
Validation loss: 2.59648125402389

Epoch: 6| Step: 4
Training loss: 2.9905645847320557
Validation loss: 2.546070396259267

Epoch: 6| Step: 5
Training loss: 2.8119020462036133
Validation loss: 2.5296909757839736

Epoch: 6| Step: 6
Training loss: 2.694992780685425
Validation loss: 2.523135580042357

Epoch: 6| Step: 7
Training loss: 2.72586727142334
Validation loss: 2.526054008032686

Epoch: 6| Step: 8
Training loss: 2.9201748371124268
Validation loss: 2.546623219725906

Epoch: 6| Step: 9
Training loss: 2.7447304725646973
Validation loss: 2.5523302580720637

Epoch: 6| Step: 10
Training loss: 2.9756016731262207
Validation loss: 2.572510133507431

Epoch: 6| Step: 11
Training loss: 2.665797233581543
Validation loss: 2.5536635678301574

Epoch: 6| Step: 12
Training loss: 2.5752005577087402
Validation loss: 2.5504907074794976

Epoch: 6| Step: 13
Training loss: 3.1139864921569824
Validation loss: 2.5551587509852585

Epoch: 26| Step: 0
Training loss: 2.503737688064575
Validation loss: 2.5684049180758897

Epoch: 6| Step: 1
Training loss: 2.7731528282165527
Validation loss: 2.544466710859729

Epoch: 6| Step: 2
Training loss: 2.807272434234619
Validation loss: 2.5125071310227916

Epoch: 6| Step: 3
Training loss: 3.3390579223632812
Validation loss: 2.51769854176429

Epoch: 6| Step: 4
Training loss: 2.3611788749694824
Validation loss: 2.5287823574517363

Epoch: 6| Step: 5
Training loss: 3.1538805961608887
Validation loss: 2.534780245955272

Epoch: 6| Step: 6
Training loss: 2.5947558879852295
Validation loss: 2.535993583740727

Epoch: 6| Step: 7
Training loss: 2.960298538208008
Validation loss: 2.5420989426233436

Epoch: 6| Step: 8
Training loss: 2.5802180767059326
Validation loss: 2.5238410157542073

Epoch: 6| Step: 9
Training loss: 2.394866943359375
Validation loss: 2.523015429896693

Epoch: 6| Step: 10
Training loss: 2.5722336769104004
Validation loss: 2.53224762024418

Epoch: 6| Step: 11
Training loss: 2.8732876777648926
Validation loss: 2.550487405510359

Epoch: 6| Step: 12
Training loss: 2.48486065864563
Validation loss: 2.5199347003813712

Epoch: 6| Step: 13
Training loss: 3.4347317218780518
Validation loss: 2.5099536065132386

Epoch: 27| Step: 0
Training loss: 2.7024176120758057
Validation loss: 2.514466724088115

Epoch: 6| Step: 1
Training loss: 2.3297882080078125
Validation loss: 2.5161840044042116

Epoch: 6| Step: 2
Training loss: 2.478095531463623
Validation loss: 2.5079924419362056

Epoch: 6| Step: 3
Training loss: 2.669614791870117
Validation loss: 2.5208196870742308

Epoch: 6| Step: 4
Training loss: 2.3965237140655518
Validation loss: 2.519457814513996

Epoch: 6| Step: 5
Training loss: 2.4715452194213867
Validation loss: 2.5825072616659184

Epoch: 6| Step: 6
Training loss: 3.1632113456726074
Validation loss: 2.592311420748311

Epoch: 6| Step: 7
Training loss: 2.992086887359619
Validation loss: 2.593221151700584

Epoch: 6| Step: 8
Training loss: 2.4836597442626953
Validation loss: 2.585286053278113

Epoch: 6| Step: 9
Training loss: 2.9246463775634766
Validation loss: 2.5755585367961595

Epoch: 6| Step: 10
Training loss: 3.2928614616394043
Validation loss: 2.5753505511950423

Epoch: 6| Step: 11
Training loss: 2.370103359222412
Validation loss: 2.57667935791836

Epoch: 6| Step: 12
Training loss: 3.11891770362854
Validation loss: 2.5737687695410942

Epoch: 6| Step: 13
Training loss: 3.8094449043273926
Validation loss: 2.5633206111128612

Epoch: 28| Step: 0
Training loss: 2.176441192626953
Validation loss: 2.560653325050108

Epoch: 6| Step: 1
Training loss: 2.6849193572998047
Validation loss: 2.561271554680281

Epoch: 6| Step: 2
Training loss: 2.8315536975860596
Validation loss: 2.5612266268781436

Epoch: 6| Step: 3
Training loss: 3.336350202560425
Validation loss: 2.550148553745721

Epoch: 6| Step: 4
Training loss: 2.0418167114257812
Validation loss: 2.5467706982807448

Epoch: 6| Step: 5
Training loss: 2.328278064727783
Validation loss: 2.552043999395063

Epoch: 6| Step: 6
Training loss: 2.6773669719696045
Validation loss: 2.5536856420578493

Epoch: 6| Step: 7
Training loss: 3.023106098175049
Validation loss: 2.5580660348297446

Epoch: 6| Step: 8
Training loss: 2.5486178398132324
Validation loss: 2.5586167714929067

Epoch: 6| Step: 9
Training loss: 2.9385762214660645
Validation loss: 2.5614063380866923

Epoch: 6| Step: 10
Training loss: 2.8082435131073
Validation loss: 2.573674281438192

Epoch: 6| Step: 11
Training loss: 3.0891530513763428
Validation loss: 2.567049854545183

Epoch: 6| Step: 12
Training loss: 3.1330699920654297
Validation loss: 2.5686499405932683

Epoch: 6| Step: 13
Training loss: 3.29372239112854
Validation loss: 2.5787788385986

Epoch: 29| Step: 0
Training loss: 1.0780000686645508
Validation loss: 2.5960186219984487

Epoch: 6| Step: 1
Training loss: 3.0808749198913574
Validation loss: 2.659445434488276

Epoch: 6| Step: 2
Training loss: 2.682717800140381
Validation loss: 2.677272496684905

Epoch: 6| Step: 3
Training loss: 2.090116262435913
Validation loss: 2.678002849701912

Epoch: 6| Step: 4
Training loss: 4.12239408493042
Validation loss: 2.677650054295858

Epoch: 6| Step: 5
Training loss: 3.543407440185547
Validation loss: 2.6658702229940765

Epoch: 6| Step: 6
Training loss: 3.5655078887939453
Validation loss: 2.650983169514646

Epoch: 6| Step: 7
Training loss: 3.0434165000915527
Validation loss: 2.6328137202929427

Epoch: 6| Step: 8
Training loss: 3.152869701385498
Validation loss: 2.6242413700267835

Epoch: 6| Step: 9
Training loss: 2.105319023132324
Validation loss: 2.615518416127851

Epoch: 6| Step: 10
Training loss: 2.955000400543213
Validation loss: 2.6155167318159536

Epoch: 6| Step: 11
Training loss: 1.7232580184936523
Validation loss: 2.621019414676133

Epoch: 6| Step: 12
Training loss: 2.955979585647583
Validation loss: 2.610941974065637

Epoch: 6| Step: 13
Training loss: 3.268137216567993
Validation loss: 2.5807710565546507

Epoch: 30| Step: 0
Training loss: 3.4682021141052246
Validation loss: 2.550154860301684

Epoch: 6| Step: 1
Training loss: 1.846393346786499
Validation loss: 2.548443178976736

Epoch: 6| Step: 2
Training loss: 3.124551296234131
Validation loss: 2.5134829295578824

Epoch: 6| Step: 3
Training loss: 3.0664918422698975
Validation loss: 2.5011246819649973

Epoch: 6| Step: 4
Training loss: 2.9469401836395264
Validation loss: 2.5137303003700833

Epoch: 6| Step: 5
Training loss: 3.261509656906128
Validation loss: 2.51316378706245

Epoch: 6| Step: 6
Training loss: 2.407395839691162
Validation loss: 2.5216363450532318

Epoch: 6| Step: 7
Training loss: 2.452211618423462
Validation loss: 2.5137081069331013

Epoch: 6| Step: 8
Training loss: 2.3397934436798096
Validation loss: 2.4894236672309136

Epoch: 6| Step: 9
Training loss: 2.947502851486206
Validation loss: 2.485971185468858

Epoch: 6| Step: 10
Training loss: 2.678539752960205
Validation loss: 2.5005181784270913

Epoch: 6| Step: 11
Training loss: 2.555734395980835
Validation loss: 2.49884372116417

Epoch: 6| Step: 12
Training loss: 2.6307692527770996
Validation loss: 2.501450346362206

Epoch: 6| Step: 13
Training loss: 2.124741792678833
Validation loss: 2.512408107839605

Epoch: 31| Step: 0
Training loss: 2.2294511795043945
Validation loss: 2.5681393120878484

Epoch: 6| Step: 1
Training loss: 2.8144383430480957
Validation loss: 2.6545761580108316

Epoch: 6| Step: 2
Training loss: 3.4149322509765625
Validation loss: 2.637027140586607

Epoch: 6| Step: 3
Training loss: 2.8320724964141846
Validation loss: 2.5648178285168064

Epoch: 6| Step: 4
Training loss: 2.6954665184020996
Validation loss: 2.4918025078312045

Epoch: 6| Step: 5
Training loss: 2.9216160774230957
Validation loss: 2.479551540907993

Epoch: 6| Step: 6
Training loss: 2.9175705909729004
Validation loss: 2.488683316015428

Epoch: 6| Step: 7
Training loss: 2.8202579021453857
Validation loss: 2.4907605289131083

Epoch: 6| Step: 8
Training loss: 2.5895094871520996
Validation loss: 2.5026985893967333

Epoch: 6| Step: 9
Training loss: 2.779264211654663
Validation loss: 2.51429219143365

Epoch: 6| Step: 10
Training loss: 2.63883638381958
Validation loss: 2.5108823724972305

Epoch: 6| Step: 11
Training loss: 3.3287134170532227
Validation loss: 2.4988740182692006

Epoch: 6| Step: 12
Training loss: 2.2999606132507324
Validation loss: 2.482389944855885

Epoch: 6| Step: 13
Training loss: 1.84380042552948
Validation loss: 2.4795874164950464

Epoch: 32| Step: 0
Training loss: 3.324201822280884
Validation loss: 2.481499320717268

Epoch: 6| Step: 1
Training loss: 3.165287494659424
Validation loss: 2.4788729067771667

Epoch: 6| Step: 2
Training loss: 2.630434989929199
Validation loss: 2.4828823151126986

Epoch: 6| Step: 3
Training loss: 2.720703125
Validation loss: 2.4901303988631054

Epoch: 6| Step: 4
Training loss: 1.8161554336547852
Validation loss: 2.4927976490348898

Epoch: 6| Step: 5
Training loss: 3.25614333152771
Validation loss: 2.50091528636153

Epoch: 6| Step: 6
Training loss: 2.7003633975982666
Validation loss: 2.4805501584083802

Epoch: 6| Step: 7
Training loss: 2.8158397674560547
Validation loss: 2.473285457139374

Epoch: 6| Step: 8
Training loss: 2.6293771266937256
Validation loss: 2.4657639341969646

Epoch: 6| Step: 9
Training loss: 2.6897192001342773
Validation loss: 2.4623494173890803

Epoch: 6| Step: 10
Training loss: 3.2002108097076416
Validation loss: 2.46124779921706

Epoch: 6| Step: 11
Training loss: 2.047032356262207
Validation loss: 2.453017996203515

Epoch: 6| Step: 12
Training loss: 2.501175880432129
Validation loss: 2.454617666941817

Epoch: 6| Step: 13
Training loss: 2.1424031257629395
Validation loss: 2.44890917757506

Epoch: 33| Step: 0
Training loss: 3.6516599655151367
Validation loss: 2.4527846690147155

Epoch: 6| Step: 1
Training loss: 2.3265421390533447
Validation loss: 2.4583942915803645

Epoch: 6| Step: 2
Training loss: 2.041868209838867
Validation loss: 2.4624557161843903

Epoch: 6| Step: 3
Training loss: 2.9054133892059326
Validation loss: 2.46963946793669

Epoch: 6| Step: 4
Training loss: 2.5163369178771973
Validation loss: 2.4520374267332015

Epoch: 6| Step: 5
Training loss: 2.6360526084899902
Validation loss: 2.4396030082497546

Epoch: 6| Step: 6
Training loss: 2.963942766189575
Validation loss: 2.439296558339109

Epoch: 6| Step: 7
Training loss: 2.895536422729492
Validation loss: 2.444711439071163

Epoch: 6| Step: 8
Training loss: 2.456200122833252
Validation loss: 2.458096586247926

Epoch: 6| Step: 9
Training loss: 2.4692037105560303
Validation loss: 2.475251425978958

Epoch: 6| Step: 10
Training loss: 3.6490941047668457
Validation loss: 2.4911485359232914

Epoch: 6| Step: 11
Training loss: 2.4864253997802734
Validation loss: 2.48084060863782

Epoch: 6| Step: 12
Training loss: 2.087376117706299
Validation loss: 2.4923223064791773

Epoch: 6| Step: 13
Training loss: 2.7178328037261963
Validation loss: 2.515582864002515

Epoch: 34| Step: 0
Training loss: 3.5884757041931152
Validation loss: 2.5093380892148582

Epoch: 6| Step: 1
Training loss: 2.5031185150146484
Validation loss: 2.4776325097648044

Epoch: 6| Step: 2
Training loss: 2.805715322494507
Validation loss: 2.4568067827532367

Epoch: 6| Step: 3
Training loss: 2.7860617637634277
Validation loss: 2.4424907571525982

Epoch: 6| Step: 4
Training loss: 2.799099922180176
Validation loss: 2.441461611819524

Epoch: 6| Step: 5
Training loss: 2.2575020790100098
Validation loss: 2.444370541521298

Epoch: 6| Step: 6
Training loss: 2.730191230773926
Validation loss: 2.4453355958384853

Epoch: 6| Step: 7
Training loss: 3.206407308578491
Validation loss: 2.449849779887866

Epoch: 6| Step: 8
Training loss: 2.941181182861328
Validation loss: 2.459572204979517

Epoch: 6| Step: 9
Training loss: 2.6368861198425293
Validation loss: 2.5097708112450055

Epoch: 6| Step: 10
Training loss: 1.7450852394104004
Validation loss: 2.5895757059897146

Epoch: 6| Step: 11
Training loss: 3.0432872772216797
Validation loss: 2.6427079067435315

Epoch: 6| Step: 12
Training loss: 1.9767844676971436
Validation loss: 2.6475869840191257

Epoch: 6| Step: 13
Training loss: 3.0667812824249268
Validation loss: 2.6259186549853255

Epoch: 35| Step: 0
Training loss: 1.883286952972412
Validation loss: 2.576103735995549

Epoch: 6| Step: 1
Training loss: 3.4111063480377197
Validation loss: 2.5607922128451768

Epoch: 6| Step: 2
Training loss: 3.237814426422119
Validation loss: 2.5457362910752654

Epoch: 6| Step: 3
Training loss: 2.665520191192627
Validation loss: 2.542422071579964

Epoch: 6| Step: 4
Training loss: 3.1015396118164062
Validation loss: 2.5239467287576325

Epoch: 6| Step: 5
Training loss: 2.353029727935791
Validation loss: 2.509413206449119

Epoch: 6| Step: 6
Training loss: 2.3785793781280518
Validation loss: 2.5067352710231656

Epoch: 6| Step: 7
Training loss: 2.8208694458007812
Validation loss: 2.5077862047380015

Epoch: 6| Step: 8
Training loss: 1.8121302127838135
Validation loss: 2.4970650980549474

Epoch: 6| Step: 9
Training loss: 1.9449106454849243
Validation loss: 2.4826079440373245

Epoch: 6| Step: 10
Training loss: 3.3090474605560303
Validation loss: 2.459415179426952

Epoch: 6| Step: 11
Training loss: 3.1979141235351562
Validation loss: 2.4499407455485356

Epoch: 6| Step: 12
Training loss: 2.8426737785339355
Validation loss: 2.4667873203113513

Epoch: 6| Step: 13
Training loss: 3.1552717685699463
Validation loss: 2.489133068310317

Epoch: 36| Step: 0
Training loss: 2.954123020172119
Validation loss: 2.4963303125032814

Epoch: 6| Step: 1
Training loss: 2.336663007736206
Validation loss: 2.512990851556101

Epoch: 6| Step: 2
Training loss: 2.5626535415649414
Validation loss: 2.5091115505464616

Epoch: 6| Step: 3
Training loss: 2.457880973815918
Validation loss: 2.518432499260031

Epoch: 6| Step: 4
Training loss: 1.9501380920410156
Validation loss: 2.4989588183741414

Epoch: 6| Step: 5
Training loss: 2.742724895477295
Validation loss: 2.4499736037305606

Epoch: 6| Step: 6
Training loss: 2.4367010593414307
Validation loss: 2.430356942197328

Epoch: 6| Step: 7
Training loss: 2.3578405380249023
Validation loss: 2.4170980427854802

Epoch: 6| Step: 8
Training loss: 3.234776735305786
Validation loss: 2.4116909016845045

Epoch: 6| Step: 9
Training loss: 3.230222702026367
Validation loss: 2.4123450658654653

Epoch: 6| Step: 10
Training loss: 2.736274480819702
Validation loss: 2.423425382183444

Epoch: 6| Step: 11
Training loss: 2.573916435241699
Validation loss: 2.4279617801789315

Epoch: 6| Step: 12
Training loss: 3.2852118015289307
Validation loss: 2.432726760064402

Epoch: 6| Step: 13
Training loss: 2.552323579788208
Validation loss: 2.423077162875924

Epoch: 37| Step: 0
Training loss: 2.3422327041625977
Validation loss: 2.411670361795733

Epoch: 6| Step: 1
Training loss: 2.596781015396118
Validation loss: 2.411816015038439

Epoch: 6| Step: 2
Training loss: 2.22186279296875
Validation loss: 2.419240346518896

Epoch: 6| Step: 3
Training loss: 2.7469582557678223
Validation loss: 2.418918894183251

Epoch: 6| Step: 4
Training loss: 3.03528094291687
Validation loss: 2.422167008922946

Epoch: 6| Step: 5
Training loss: 2.541071891784668
Validation loss: 2.4185692418006157

Epoch: 6| Step: 6
Training loss: 2.306506872177124
Validation loss: 2.4374342785086682

Epoch: 6| Step: 7
Training loss: 3.302933692932129
Validation loss: 2.466787479257071

Epoch: 6| Step: 8
Training loss: 2.640875816345215
Validation loss: 2.4895936160959224

Epoch: 6| Step: 9
Training loss: 2.914113998413086
Validation loss: 2.4783741043459986

Epoch: 6| Step: 10
Training loss: 2.4610743522644043
Validation loss: 2.4662464869919645

Epoch: 6| Step: 11
Training loss: 2.350883960723877
Validation loss: 2.407639677806567

Epoch: 6| Step: 12
Training loss: 3.0310282707214355
Validation loss: 2.403138816997569

Epoch: 6| Step: 13
Training loss: 3.0255918502807617
Validation loss: 2.3976491728136615

Epoch: 38| Step: 0
Training loss: 2.249450206756592
Validation loss: 2.4084050270818893

Epoch: 6| Step: 1
Training loss: 3.0619101524353027
Validation loss: 2.408908400484311

Epoch: 6| Step: 2
Training loss: 2.984886646270752
Validation loss: 2.4071753319873603

Epoch: 6| Step: 3
Training loss: 2.751371145248413
Validation loss: 2.406383814350251

Epoch: 6| Step: 4
Training loss: 2.518803596496582
Validation loss: 2.4033901678618563

Epoch: 6| Step: 5
Training loss: 2.6875529289245605
Validation loss: 2.4103409474895847

Epoch: 6| Step: 6
Training loss: 2.591593027114868
Validation loss: 2.4065689143314155

Epoch: 6| Step: 7
Training loss: 2.6131277084350586
Validation loss: 2.403454110186587

Epoch: 6| Step: 8
Training loss: 2.69608473777771
Validation loss: 2.399854311379053

Epoch: 6| Step: 9
Training loss: 2.5437588691711426
Validation loss: 2.4034920046406407

Epoch: 6| Step: 10
Training loss: 2.7241625785827637
Validation loss: 2.3993438008011028

Epoch: 6| Step: 11
Training loss: 2.748032331466675
Validation loss: 2.3963681561972505

Epoch: 6| Step: 12
Training loss: 2.903038263320923
Validation loss: 2.397326082311651

Epoch: 6| Step: 13
Training loss: 1.862856149673462
Validation loss: 2.414859494855327

Epoch: 39| Step: 0
Training loss: 2.5271029472351074
Validation loss: 2.417774176084867

Epoch: 6| Step: 1
Training loss: 3.249962091445923
Validation loss: 2.4288748823186403

Epoch: 6| Step: 2
Training loss: 3.1499907970428467
Validation loss: 2.431892410401375

Epoch: 6| Step: 3
Training loss: 2.345165491104126
Validation loss: 2.4630144026971634

Epoch: 6| Step: 4
Training loss: 3.0894970893859863
Validation loss: 2.5082115204103532

Epoch: 6| Step: 5
Training loss: 2.5802693367004395
Validation loss: 2.5249063353384695

Epoch: 6| Step: 6
Training loss: 3.1925735473632812
Validation loss: 2.5305639851477837

Epoch: 6| Step: 7
Training loss: 2.36657452583313
Validation loss: 2.5258546311368226

Epoch: 6| Step: 8
Training loss: 2.562297821044922
Validation loss: 2.495442836515365

Epoch: 6| Step: 9
Training loss: 2.6385111808776855
Validation loss: 2.448751211166382

Epoch: 6| Step: 10
Training loss: 1.145666480064392
Validation loss: 2.4055590937214513

Epoch: 6| Step: 11
Training loss: 2.889479637145996
Validation loss: 2.395763720235517

Epoch: 6| Step: 12
Training loss: 2.525322914123535
Validation loss: 2.393687294375512

Epoch: 6| Step: 13
Training loss: 3.4664595127105713
Validation loss: 2.4063891621046167

Epoch: 40| Step: 0
Training loss: 2.1274797916412354
Validation loss: 2.410971114712377

Epoch: 6| Step: 1
Training loss: 3.1342062950134277
Validation loss: 2.416733277741299

Epoch: 6| Step: 2
Training loss: 3.761906147003174
Validation loss: 2.4194846665987404

Epoch: 6| Step: 3
Training loss: 2.6796984672546387
Validation loss: 2.4128966690391622

Epoch: 6| Step: 4
Training loss: 2.4280381202697754
Validation loss: 2.412924107684884

Epoch: 6| Step: 5
Training loss: 2.3791165351867676
Validation loss: 2.4143759960769327

Epoch: 6| Step: 6
Training loss: 3.030520439147949
Validation loss: 2.4036224452398156

Epoch: 6| Step: 7
Training loss: 2.074824571609497
Validation loss: 2.398235659445486

Epoch: 6| Step: 8
Training loss: 2.4778223037719727
Validation loss: 2.387924443009079

Epoch: 6| Step: 9
Training loss: 2.6636626720428467
Validation loss: 2.3845257861639864

Epoch: 6| Step: 10
Training loss: 2.6809959411621094
Validation loss: 2.382056792577108

Epoch: 6| Step: 11
Training loss: 2.512239933013916
Validation loss: 2.386598438344976

Epoch: 6| Step: 12
Training loss: 2.43831205368042
Validation loss: 2.387769704223961

Epoch: 6| Step: 13
Training loss: 2.989712715148926
Validation loss: 2.398318952129733

Epoch: 41| Step: 0
Training loss: 2.126067876815796
Validation loss: 2.4235041884965796

Epoch: 6| Step: 1
Training loss: 2.528188705444336
Validation loss: 2.430289537675919

Epoch: 6| Step: 2
Training loss: 2.3128035068511963
Validation loss: 2.441061806935136

Epoch: 6| Step: 3
Training loss: 2.5487160682678223
Validation loss: 2.437032602166617

Epoch: 6| Step: 4
Training loss: 3.2621843814849854
Validation loss: 2.430790424346924

Epoch: 6| Step: 5
Training loss: 2.607574224472046
Validation loss: 2.4230421127811557

Epoch: 6| Step: 6
Training loss: 2.315117597579956
Validation loss: 2.42683675981337

Epoch: 6| Step: 7
Training loss: 2.935328483581543
Validation loss: 2.439083964593949

Epoch: 6| Step: 8
Training loss: 3.3487534523010254
Validation loss: 2.4427712168744815

Epoch: 6| Step: 9
Training loss: 2.6027731895446777
Validation loss: 2.4262751943321637

Epoch: 6| Step: 10
Training loss: 2.802236557006836
Validation loss: 2.4124997918323805

Epoch: 6| Step: 11
Training loss: 2.1512398719787598
Validation loss: 2.4013594760689685

Epoch: 6| Step: 12
Training loss: 2.9715640544891357
Validation loss: 2.418834968279767

Epoch: 6| Step: 13
Training loss: 2.6667797565460205
Validation loss: 2.431733639009537

Epoch: 42| Step: 0
Training loss: 2.9754042625427246
Validation loss: 2.433069029162007

Epoch: 6| Step: 1
Training loss: 2.481130838394165
Validation loss: 2.4528995662607174

Epoch: 6| Step: 2
Training loss: 2.1057491302490234
Validation loss: 2.4518794039244294

Epoch: 6| Step: 3
Training loss: 2.997528076171875
Validation loss: 2.457737325340189

Epoch: 6| Step: 4
Training loss: 2.5765790939331055
Validation loss: 2.4550332292433708

Epoch: 6| Step: 5
Training loss: 2.9559271335601807
Validation loss: 2.4566407588220414

Epoch: 6| Step: 6
Training loss: 3.310960531234741
Validation loss: 2.4636504829570813

Epoch: 6| Step: 7
Training loss: 2.208557367324829
Validation loss: 2.46762260313957

Epoch: 6| Step: 8
Training loss: 2.5427722930908203
Validation loss: 2.4549232067600375

Epoch: 6| Step: 9
Training loss: 2.928921937942505
Validation loss: 2.438773157776043

Epoch: 6| Step: 10
Training loss: 2.650050163269043
Validation loss: 2.4377417461846465

Epoch: 6| Step: 11
Training loss: 2.676450490951538
Validation loss: 2.434195572330106

Epoch: 6| Step: 12
Training loss: 2.44150972366333
Validation loss: 2.435294956289312

Epoch: 6| Step: 13
Training loss: 2.376893997192383
Validation loss: 2.4313015707077517

Epoch: 43| Step: 0
Training loss: 2.4623665809631348
Validation loss: 2.431487683326967

Epoch: 6| Step: 1
Training loss: 2.5034713745117188
Validation loss: 2.412938174381051

Epoch: 6| Step: 2
Training loss: 2.2834725379943848
Validation loss: 2.4030197358900502

Epoch: 6| Step: 3
Training loss: 2.8442721366882324
Validation loss: 2.430179959984236

Epoch: 6| Step: 4
Training loss: 2.835268020629883
Validation loss: 2.45106698877068

Epoch: 6| Step: 5
Training loss: 2.514662265777588
Validation loss: 2.467036372871809

Epoch: 6| Step: 6
Training loss: 1.8984346389770508
Validation loss: 2.4622160542395806

Epoch: 6| Step: 7
Training loss: 1.9822179079055786
Validation loss: 2.424875900309573

Epoch: 6| Step: 8
Training loss: 3.1185522079467773
Validation loss: 2.413836492005215

Epoch: 6| Step: 9
Training loss: 2.795036792755127
Validation loss: 2.409502383201353

Epoch: 6| Step: 10
Training loss: 3.349719285964966
Validation loss: 2.3967112853962886

Epoch: 6| Step: 11
Training loss: 2.8335719108581543
Validation loss: 2.3844882954833326

Epoch: 6| Step: 12
Training loss: 3.326204776763916
Validation loss: 2.369723058515979

Epoch: 6| Step: 13
Training loss: 1.6719200611114502
Validation loss: 2.378374086913242

Epoch: 44| Step: 0
Training loss: 2.1806163787841797
Validation loss: 2.3796095232809744

Epoch: 6| Step: 1
Training loss: 3.37660551071167
Validation loss: 2.405146296306323

Epoch: 6| Step: 2
Training loss: 2.501469373703003
Validation loss: 2.4110682831015637

Epoch: 6| Step: 3
Training loss: 2.419585704803467
Validation loss: 2.398725255843132

Epoch: 6| Step: 4
Training loss: 2.8685386180877686
Validation loss: 2.386708205746066

Epoch: 6| Step: 5
Training loss: 2.0646092891693115
Validation loss: 2.3781846415612007

Epoch: 6| Step: 6
Training loss: 2.8364291191101074
Validation loss: 2.367621716632638

Epoch: 6| Step: 7
Training loss: 3.204103469848633
Validation loss: 2.3555547691160634

Epoch: 6| Step: 8
Training loss: 3.242428779602051
Validation loss: 2.354491397898684

Epoch: 6| Step: 9
Training loss: 2.969801425933838
Validation loss: 2.348943523181382

Epoch: 6| Step: 10
Training loss: 2.867128849029541
Validation loss: 2.353507181649567

Epoch: 6| Step: 11
Training loss: 2.238570213317871
Validation loss: 2.3671591307527278

Epoch: 6| Step: 12
Training loss: 2.109651803970337
Validation loss: 2.371141020969678

Epoch: 6| Step: 13
Training loss: 1.7626135349273682
Validation loss: 2.3722048779969573

Epoch: 45| Step: 0
Training loss: 2.6891586780548096
Validation loss: 2.371912448636947

Epoch: 6| Step: 1
Training loss: 2.8404855728149414
Validation loss: 2.359378409642045

Epoch: 6| Step: 2
Training loss: 2.2549757957458496
Validation loss: 2.352496124082996

Epoch: 6| Step: 3
Training loss: 2.7610912322998047
Validation loss: 2.3557424981106996

Epoch: 6| Step: 4
Training loss: 2.458691358566284
Validation loss: 2.3520726567955426

Epoch: 6| Step: 5
Training loss: 2.5059142112731934
Validation loss: 2.351577733152656

Epoch: 6| Step: 6
Training loss: 2.3377487659454346
Validation loss: 2.35462946532875

Epoch: 6| Step: 7
Training loss: 2.5505614280700684
Validation loss: 2.359145531090357

Epoch: 6| Step: 8
Training loss: 2.8381314277648926
Validation loss: 2.3647466372418147

Epoch: 6| Step: 9
Training loss: 2.1779420375823975
Validation loss: 2.3576340957354476

Epoch: 6| Step: 10
Training loss: 2.642507553100586
Validation loss: 2.355497078229022

Epoch: 6| Step: 11
Training loss: 3.5099682807922363
Validation loss: 2.3481254244363434

Epoch: 6| Step: 12
Training loss: 3.0315332412719727
Validation loss: 2.3472109481852543

Epoch: 6| Step: 13
Training loss: 1.6004688739776611
Validation loss: 2.340852984818079

Epoch: 46| Step: 0
Training loss: 3.320190906524658
Validation loss: 2.3411406291428434

Epoch: 6| Step: 1
Training loss: 2.3930001258850098
Validation loss: 2.342851928485337

Epoch: 6| Step: 2
Training loss: 2.967844009399414
Validation loss: 2.3426637418808474

Epoch: 6| Step: 3
Training loss: 2.7933573722839355
Validation loss: 2.3467480418502644

Epoch: 6| Step: 4
Training loss: 3.441458225250244
Validation loss: 2.3441511328502367

Epoch: 6| Step: 5
Training loss: 1.6961734294891357
Validation loss: 2.3430642709937146

Epoch: 6| Step: 6
Training loss: 2.5377321243286133
Validation loss: 2.3452400340828845

Epoch: 6| Step: 7
Training loss: 2.6200551986694336
Validation loss: 2.338014651370305

Epoch: 6| Step: 8
Training loss: 2.812469005584717
Validation loss: 2.343474752159529

Epoch: 6| Step: 9
Training loss: 1.8097317218780518
Validation loss: 2.34211794535319

Epoch: 6| Step: 10
Training loss: 2.565040111541748
Validation loss: 2.351568316900602

Epoch: 6| Step: 11
Training loss: 2.0595245361328125
Validation loss: 2.3823501986842

Epoch: 6| Step: 12
Training loss: 3.1617684364318848
Validation loss: 2.450599273045858

Epoch: 6| Step: 13
Training loss: 2.671410083770752
Validation loss: 2.46216417384404

Epoch: 47| Step: 0
Training loss: 2.727661609649658
Validation loss: 2.4857635421137654

Epoch: 6| Step: 1
Training loss: 2.561810255050659
Validation loss: 2.5002295509461434

Epoch: 6| Step: 2
Training loss: 2.6799588203430176
Validation loss: 2.4421889858861126

Epoch: 6| Step: 3
Training loss: 2.6665525436401367
Validation loss: 2.3835502516838813

Epoch: 6| Step: 4
Training loss: 3.1358842849731445
Validation loss: 2.3482390475529495

Epoch: 6| Step: 5
Training loss: 2.931159019470215
Validation loss: 2.340734794575681

Epoch: 6| Step: 6
Training loss: 3.167782783508301
Validation loss: 2.34639407742408

Epoch: 6| Step: 7
Training loss: 1.841833472251892
Validation loss: 2.355973292422551

Epoch: 6| Step: 8
Training loss: 2.377218008041382
Validation loss: 2.3616956087850753

Epoch: 6| Step: 9
Training loss: 1.9821395874023438
Validation loss: 2.3726361541337866

Epoch: 6| Step: 10
Training loss: 2.3727922439575195
Validation loss: 2.371172202530728

Epoch: 6| Step: 11
Training loss: 3.4152283668518066
Validation loss: 2.3783857950600247

Epoch: 6| Step: 12
Training loss: 2.2663629055023193
Validation loss: 2.371628499800159

Epoch: 6| Step: 13
Training loss: 3.3041629791259766
Validation loss: 2.3570757630050823

Epoch: 48| Step: 0
Training loss: 3.087217330932617
Validation loss: 2.359426180521647

Epoch: 6| Step: 1
Training loss: 2.252275228500366
Validation loss: 2.3622141576582387

Epoch: 6| Step: 2
Training loss: 3.4501779079437256
Validation loss: 2.3724470958914807

Epoch: 6| Step: 3
Training loss: 2.571000814437866
Validation loss: 2.3990415680793022

Epoch: 6| Step: 4
Training loss: 2.8855926990509033
Validation loss: 2.38601605866545

Epoch: 6| Step: 5
Training loss: 2.4703421592712402
Validation loss: 2.368584366254909

Epoch: 6| Step: 6
Training loss: 2.3308725357055664
Validation loss: 2.3734381967975247

Epoch: 6| Step: 7
Training loss: 2.946068048477173
Validation loss: 2.3829130818766933

Epoch: 6| Step: 8
Training loss: 2.1419498920440674
Validation loss: 2.385488666513915

Epoch: 6| Step: 9
Training loss: 2.6457808017730713
Validation loss: 2.3933623785613687

Epoch: 6| Step: 10
Training loss: 2.178555488586426
Validation loss: 2.3905659055197113

Epoch: 6| Step: 11
Training loss: 3.0922601222991943
Validation loss: 2.375807830082473

Epoch: 6| Step: 12
Training loss: 1.8947678804397583
Validation loss: 2.3632229117936987

Epoch: 6| Step: 13
Training loss: 3.1369028091430664
Validation loss: 2.3586551784187235

Epoch: 49| Step: 0
Training loss: 2.7505922317504883
Validation loss: 2.3468593551266577

Epoch: 6| Step: 1
Training loss: 2.634608745574951
Validation loss: 2.33396335827407

Epoch: 6| Step: 2
Training loss: 2.4152398109436035
Validation loss: 2.331524218282392

Epoch: 6| Step: 3
Training loss: 2.1117143630981445
Validation loss: 2.331360658009847

Epoch: 6| Step: 4
Training loss: 1.9147772789001465
Validation loss: 2.3288585498768795

Epoch: 6| Step: 5
Training loss: 2.7244489192962646
Validation loss: 2.3290681018624255

Epoch: 6| Step: 6
Training loss: 3.1392030715942383
Validation loss: 2.32105198085949

Epoch: 6| Step: 7
Training loss: 2.3123364448547363
Validation loss: 2.323080348712142

Epoch: 6| Step: 8
Training loss: 2.556586265563965
Validation loss: 2.3221408744012155

Epoch: 6| Step: 9
Training loss: 3.034245491027832
Validation loss: 2.3178860705385924

Epoch: 6| Step: 10
Training loss: 3.312439203262329
Validation loss: 2.321821206359453

Epoch: 6| Step: 11
Training loss: 1.8042471408843994
Validation loss: 2.317476910929526

Epoch: 6| Step: 12
Training loss: 3.0613012313842773
Validation loss: 2.3165207473180627

Epoch: 6| Step: 13
Training loss: 3.0088632106781006
Validation loss: 2.3152510427659556

Epoch: 50| Step: 0
Training loss: 2.4774692058563232
Validation loss: 2.317401645004108

Epoch: 6| Step: 1
Training loss: 2.840141773223877
Validation loss: 2.3192619841585875

Epoch: 6| Step: 2
Training loss: 3.2282447814941406
Validation loss: 2.3273984565529773

Epoch: 6| Step: 3
Training loss: 1.9437586069107056
Validation loss: 2.339780394748975

Epoch: 6| Step: 4
Training loss: 2.468796730041504
Validation loss: 2.360375447939801

Epoch: 6| Step: 5
Training loss: 2.839500904083252
Validation loss: 2.4161818591497277

Epoch: 6| Step: 6
Training loss: 2.971865177154541
Validation loss: 2.444692968040384

Epoch: 6| Step: 7
Training loss: 3.353571653366089
Validation loss: 2.4942442370999243

Epoch: 6| Step: 8
Training loss: 2.368378162384033
Validation loss: 2.4458480060741468

Epoch: 6| Step: 9
Training loss: 2.390578269958496
Validation loss: 2.404609921158001

Epoch: 6| Step: 10
Training loss: 2.211060047149658
Validation loss: 2.365617621329523

Epoch: 6| Step: 11
Training loss: 2.7801809310913086
Validation loss: 2.339697778865855

Epoch: 6| Step: 12
Training loss: 2.505298137664795
Validation loss: 2.3207767253280966

Epoch: 6| Step: 13
Training loss: 2.156726837158203
Validation loss: 2.3105537276114188

Epoch: 51| Step: 0
Training loss: 2.516414165496826
Validation loss: 2.3073765385535454

Epoch: 6| Step: 1
Training loss: 2.9196982383728027
Validation loss: 2.3150039436996623

Epoch: 6| Step: 2
Training loss: 2.6402087211608887
Validation loss: 2.3120991414593113

Epoch: 6| Step: 3
Training loss: 2.5034117698669434
Validation loss: 2.3201945263852357

Epoch: 6| Step: 4
Training loss: 2.342872142791748
Validation loss: 2.3170747218593473

Epoch: 6| Step: 5
Training loss: 2.5905802249908447
Validation loss: 2.3246092001597085

Epoch: 6| Step: 6
Training loss: 2.164947271347046
Validation loss: 2.3286946127491612

Epoch: 6| Step: 7
Training loss: 2.572567939758301
Validation loss: 2.3238405642970914

Epoch: 6| Step: 8
Training loss: 2.6450769901275635
Validation loss: 2.316334543689605

Epoch: 6| Step: 9
Training loss: 2.1114330291748047
Validation loss: 2.313809241017988

Epoch: 6| Step: 10
Training loss: 2.9221062660217285
Validation loss: 2.317932226324594

Epoch: 6| Step: 11
Training loss: 2.9303107261657715
Validation loss: 2.3148962810475338

Epoch: 6| Step: 12
Training loss: 2.5101771354675293
Validation loss: 2.311529759437807

Epoch: 6| Step: 13
Training loss: 3.136744260787964
Validation loss: 2.3078747154563986

Epoch: 52| Step: 0
Training loss: 2.9039392471313477
Validation loss: 2.3031922694175475

Epoch: 6| Step: 1
Training loss: 2.391819477081299
Validation loss: 2.301792621612549

Epoch: 6| Step: 2
Training loss: 2.910813331604004
Validation loss: 2.307020038686773

Epoch: 6| Step: 3
Training loss: 1.7162768840789795
Validation loss: 2.310270080002405

Epoch: 6| Step: 4
Training loss: 2.408087730407715
Validation loss: 2.3221337820893977

Epoch: 6| Step: 5
Training loss: 2.340308666229248
Validation loss: 2.3230067465894964

Epoch: 6| Step: 6
Training loss: 2.9527435302734375
Validation loss: 2.3416357296769337

Epoch: 6| Step: 7
Training loss: 3.600247383117676
Validation loss: 2.355680106788553

Epoch: 6| Step: 8
Training loss: 2.0213451385498047
Validation loss: 2.373621035647649

Epoch: 6| Step: 9
Training loss: 3.0669589042663574
Validation loss: 2.374756997631442

Epoch: 6| Step: 10
Training loss: 2.6225805282592773
Validation loss: 2.362372208667058

Epoch: 6| Step: 11
Training loss: 2.1757235527038574
Validation loss: 2.356799856308968

Epoch: 6| Step: 12
Training loss: 2.5440969467163086
Validation loss: 2.3547636026977212

Epoch: 6| Step: 13
Training loss: 2.6923229694366455
Validation loss: 2.341588512543709

Epoch: 53| Step: 0
Training loss: 2.989006996154785
Validation loss: 2.3225854699329664

Epoch: 6| Step: 1
Training loss: 2.1481704711914062
Validation loss: 2.3082010746002197

Epoch: 6| Step: 2
Training loss: 2.8749213218688965
Validation loss: 2.2969412624195056

Epoch: 6| Step: 3
Training loss: 2.06353759765625
Validation loss: 2.2928558113754436

Epoch: 6| Step: 4
Training loss: 3.318089246749878
Validation loss: 2.2913182115042083

Epoch: 6| Step: 5
Training loss: 2.927191972732544
Validation loss: 2.294202635365148

Epoch: 6| Step: 6
Training loss: 2.670358419418335
Validation loss: 2.289595380906136

Epoch: 6| Step: 7
Training loss: 2.37247633934021
Validation loss: 2.288939832359232

Epoch: 6| Step: 8
Training loss: 3.150844097137451
Validation loss: 2.292270862928001

Epoch: 6| Step: 9
Training loss: 1.6406102180480957
Validation loss: 2.2958009525011946

Epoch: 6| Step: 10
Training loss: 2.601440668106079
Validation loss: 2.2958115454643004

Epoch: 6| Step: 11
Training loss: 2.284695863723755
Validation loss: 2.301226541560183

Epoch: 6| Step: 12
Training loss: 2.4726061820983887
Validation loss: 2.2993938230699107

Epoch: 6| Step: 13
Training loss: 2.8046159744262695
Validation loss: 2.301307888441188

Epoch: 54| Step: 0
Training loss: 2.784552574157715
Validation loss: 2.295810704590172

Epoch: 6| Step: 1
Training loss: 2.4523704051971436
Validation loss: 2.295830172877158

Epoch: 6| Step: 2
Training loss: 2.4662461280822754
Validation loss: 2.2968942529411724

Epoch: 6| Step: 3
Training loss: 2.5721275806427
Validation loss: 2.2926796918274253

Epoch: 6| Step: 4
Training loss: 3.0667524337768555
Validation loss: 2.2979117490911998

Epoch: 6| Step: 5
Training loss: 2.739795684814453
Validation loss: 2.29854098186698

Epoch: 6| Step: 6
Training loss: 1.885904312133789
Validation loss: 2.3016861613078783

Epoch: 6| Step: 7
Training loss: 2.278503656387329
Validation loss: 2.3156088090712026

Epoch: 6| Step: 8
Training loss: 2.267965316772461
Validation loss: 2.3309165893062467

Epoch: 6| Step: 9
Training loss: 2.439697265625
Validation loss: 2.3614576965249996

Epoch: 6| Step: 10
Training loss: 3.2925546169281006
Validation loss: 2.385738403566422

Epoch: 6| Step: 11
Training loss: 1.9672929048538208
Validation loss: 2.4141578533316173

Epoch: 6| Step: 12
Training loss: 3.379673480987549
Validation loss: 2.411632545532719

Epoch: 6| Step: 13
Training loss: 2.9172937870025635
Validation loss: 2.4063413604613273

Epoch: 55| Step: 0
Training loss: 2.6808104515075684
Validation loss: 2.3659951148494596

Epoch: 6| Step: 1
Training loss: 2.4487154483795166
Validation loss: 2.330553231700774

Epoch: 6| Step: 2
Training loss: 2.5398168563842773
Validation loss: 2.3189004518652476

Epoch: 6| Step: 3
Training loss: 2.636749029159546
Validation loss: 2.2941677493433796

Epoch: 6| Step: 4
Training loss: 2.37142276763916
Validation loss: 2.28908843891595

Epoch: 6| Step: 5
Training loss: 2.4442977905273438
Validation loss: 2.2879953153671755

Epoch: 6| Step: 6
Training loss: 3.0623321533203125
Validation loss: 2.292428613990866

Epoch: 6| Step: 7
Training loss: 3.3552184104919434
Validation loss: 2.2946502982929187

Epoch: 6| Step: 8
Training loss: 2.4395368099212646
Validation loss: 2.295147593303393

Epoch: 6| Step: 9
Training loss: 2.2156667709350586
Validation loss: 2.2959282270041843

Epoch: 6| Step: 10
Training loss: 2.2769775390625
Validation loss: 2.290383723474318

Epoch: 6| Step: 11
Training loss: 2.3113009929656982
Validation loss: 2.2874161504930064

Epoch: 6| Step: 12
Training loss: 2.780540704727173
Validation loss: 2.286394373063118

Epoch: 6| Step: 13
Training loss: 2.806551456451416
Validation loss: 2.293972474272533

Epoch: 56| Step: 0
Training loss: 2.8560311794281006
Validation loss: 2.303238635422081

Epoch: 6| Step: 1
Training loss: 2.032742977142334
Validation loss: 2.30609897387925

Epoch: 6| Step: 2
Training loss: 2.857172966003418
Validation loss: 2.315612639150312

Epoch: 6| Step: 3
Training loss: 1.9929206371307373
Validation loss: 2.3229570593885196

Epoch: 6| Step: 4
Training loss: 1.936200499534607
Validation loss: 2.3420706026015745

Epoch: 6| Step: 5
Training loss: 2.081183671951294
Validation loss: 2.3634539804150982

Epoch: 6| Step: 6
Training loss: 2.438997507095337
Validation loss: 2.4165246999391945

Epoch: 6| Step: 7
Training loss: 2.7298333644866943
Validation loss: 2.454964994102396

Epoch: 6| Step: 8
Training loss: 3.315566062927246
Validation loss: 2.471153623314314

Epoch: 6| Step: 9
Training loss: 2.6557822227478027
Validation loss: 2.436915066934401

Epoch: 6| Step: 10
Training loss: 2.560689926147461
Validation loss: 2.3812779380429174

Epoch: 6| Step: 11
Training loss: 3.135342597961426
Validation loss: 2.3463291096430954

Epoch: 6| Step: 12
Training loss: 2.628584384918213
Validation loss: 2.3106372151323544

Epoch: 6| Step: 13
Training loss: 3.4869847297668457
Validation loss: 2.2856807144739295

Epoch: 57| Step: 0
Training loss: 2.6658058166503906
Validation loss: 2.2789125493777695

Epoch: 6| Step: 1
Training loss: 2.3308944702148438
Validation loss: 2.2786386782123196

Epoch: 6| Step: 2
Training loss: 2.405094623565674
Validation loss: 2.2839884988723265

Epoch: 6| Step: 3
Training loss: 2.9171500205993652
Validation loss: 2.2970528064235562

Epoch: 6| Step: 4
Training loss: 3.190962553024292
Validation loss: 2.3060329652601674

Epoch: 6| Step: 5
Training loss: 3.2911183834075928
Validation loss: 2.297481208719233

Epoch: 6| Step: 6
Training loss: 2.461137294769287
Validation loss: 2.2964367225605953

Epoch: 6| Step: 7
Training loss: 2.6365807056427
Validation loss: 2.287772624723373

Epoch: 6| Step: 8
Training loss: 2.3797850608825684
Validation loss: 2.2768173781774377

Epoch: 6| Step: 9
Training loss: 2.3054966926574707
Validation loss: 2.2681875741609963

Epoch: 6| Step: 10
Training loss: 2.027073621749878
Validation loss: 2.2524709663083478

Epoch: 6| Step: 11
Training loss: 2.2349352836608887
Validation loss: 2.263614249485795

Epoch: 6| Step: 12
Training loss: 2.951174259185791
Validation loss: 2.2757857102219776

Epoch: 6| Step: 13
Training loss: 2.589310884475708
Validation loss: 2.291529909256966

Epoch: 58| Step: 0
Training loss: 2.1339900493621826
Validation loss: 2.3146117887189313

Epoch: 6| Step: 1
Training loss: 2.9142837524414062
Validation loss: 2.337617766472601

Epoch: 6| Step: 2
Training loss: 1.9761313199996948
Validation loss: 2.346059137775052

Epoch: 6| Step: 3
Training loss: 2.105896472930908
Validation loss: 2.374193614529025

Epoch: 6| Step: 4
Training loss: 3.2286667823791504
Validation loss: 2.3715775294970443

Epoch: 6| Step: 5
Training loss: 2.9695425033569336
Validation loss: 2.3570464913563063

Epoch: 6| Step: 6
Training loss: 2.3229501247406006
Validation loss: 2.342781582186299

Epoch: 6| Step: 7
Training loss: 3.0301570892333984
Validation loss: 2.3180571448418403

Epoch: 6| Step: 8
Training loss: 2.2690186500549316
Validation loss: 2.296711201308876

Epoch: 6| Step: 9
Training loss: 2.8929691314697266
Validation loss: 2.281524213411475

Epoch: 6| Step: 10
Training loss: 2.8539657592773438
Validation loss: 2.270067717439385

Epoch: 6| Step: 11
Training loss: 2.7115800380706787
Validation loss: 2.2562268780123804

Epoch: 6| Step: 12
Training loss: 2.370351791381836
Validation loss: 2.246529157443713

Epoch: 6| Step: 13
Training loss: 1.9597723484039307
Validation loss: 2.253358797360492

Epoch: 59| Step: 0
Training loss: 2.276960611343384
Validation loss: 2.256234356152114

Epoch: 6| Step: 1
Training loss: 2.3284833431243896
Validation loss: 2.257129346170733

Epoch: 6| Step: 2
Training loss: 2.697847366333008
Validation loss: 2.2603151336792977

Epoch: 6| Step: 3
Training loss: 3.3455066680908203
Validation loss: 2.2521784843937045

Epoch: 6| Step: 4
Training loss: 3.3167965412139893
Validation loss: 2.253158733408938

Epoch: 6| Step: 5
Training loss: 2.3380062580108643
Validation loss: 2.251128578698763

Epoch: 6| Step: 6
Training loss: 3.0440757274627686
Validation loss: 2.2523726032626246

Epoch: 6| Step: 7
Training loss: 1.9547767639160156
Validation loss: 2.24696207815601

Epoch: 6| Step: 8
Training loss: 2.669311046600342
Validation loss: 2.247307159567392

Epoch: 6| Step: 9
Training loss: 3.201676368713379
Validation loss: 2.245699131360618

Epoch: 6| Step: 10
Training loss: 1.9574753046035767
Validation loss: 2.2441964136656893

Epoch: 6| Step: 11
Training loss: 2.148815631866455
Validation loss: 2.2381989571356002

Epoch: 6| Step: 12
Training loss: 2.3605332374572754
Validation loss: 2.238644940878755

Epoch: 6| Step: 13
Training loss: 2.325418710708618
Validation loss: 2.241325673236642

Epoch: 60| Step: 0
Training loss: 1.6538450717926025
Validation loss: 2.2458951909054994

Epoch: 6| Step: 1
Training loss: 2.8742551803588867
Validation loss: 2.2502356139562463

Epoch: 6| Step: 2
Training loss: 2.6499738693237305
Validation loss: 2.2496633939845587

Epoch: 6| Step: 3
Training loss: 2.473172664642334
Validation loss: 2.257045198512334

Epoch: 6| Step: 4
Training loss: 2.457578420639038
Validation loss: 2.2508842637462

Epoch: 6| Step: 5
Training loss: 3.023883104324341
Validation loss: 2.267245424691067

Epoch: 6| Step: 6
Training loss: 2.019113540649414
Validation loss: 2.270665209780457

Epoch: 6| Step: 7
Training loss: 2.63877010345459
Validation loss: 2.27289826895601

Epoch: 6| Step: 8
Training loss: 3.0667171478271484
Validation loss: 2.2789840057332027

Epoch: 6| Step: 9
Training loss: 1.7007319927215576
Validation loss: 2.2661235101761354

Epoch: 6| Step: 10
Training loss: 3.425654411315918
Validation loss: 2.2659530178193124

Epoch: 6| Step: 11
Training loss: 2.962494134902954
Validation loss: 2.2574709846127416

Epoch: 6| Step: 12
Training loss: 2.520777940750122
Validation loss: 2.259929031454107

Epoch: 6| Step: 13
Training loss: 1.9600034952163696
Validation loss: 2.248040162106996

Epoch: 61| Step: 0
Training loss: 2.211517333984375
Validation loss: 2.247448871212621

Epoch: 6| Step: 1
Training loss: 2.923351764678955
Validation loss: 2.2468444249963246

Epoch: 6| Step: 2
Training loss: 1.870638132095337
Validation loss: 2.2448982166987594

Epoch: 6| Step: 3
Training loss: 2.174036979675293
Validation loss: 2.255258032070693

Epoch: 6| Step: 4
Training loss: 2.418809652328491
Validation loss: 2.265638856477635

Epoch: 6| Step: 5
Training loss: 2.6135737895965576
Validation loss: 2.2631776255946003

Epoch: 6| Step: 6
Training loss: 2.5968985557556152
Validation loss: 2.2638500044422765

Epoch: 6| Step: 7
Training loss: 3.7515087127685547
Validation loss: 2.2628469697890745

Epoch: 6| Step: 8
Training loss: 2.670866012573242
Validation loss: 2.2621552328909598

Epoch: 6| Step: 9
Training loss: 1.927324652671814
Validation loss: 2.2694120381468084

Epoch: 6| Step: 10
Training loss: 3.080726146697998
Validation loss: 2.2826455664891068

Epoch: 6| Step: 11
Training loss: 2.6651880741119385
Validation loss: 2.2764180065483175

Epoch: 6| Step: 12
Training loss: 2.3431243896484375
Validation loss: 2.26937932865594

Epoch: 6| Step: 13
Training loss: 2.270936965942383
Validation loss: 2.245806268466416

Epoch: 62| Step: 0
Training loss: 2.1193041801452637
Validation loss: 2.226065792063231

Epoch: 6| Step: 1
Training loss: 2.0738415718078613
Validation loss: 2.2297193132421023

Epoch: 6| Step: 2
Training loss: 2.917794704437256
Validation loss: 2.2275806473147486

Epoch: 6| Step: 3
Training loss: 2.3802268505096436
Validation loss: 2.228108820094857

Epoch: 6| Step: 4
Training loss: 2.752910852432251
Validation loss: 2.2268099349032164

Epoch: 6| Step: 5
Training loss: 2.5239415168762207
Validation loss: 2.2234763176210466

Epoch: 6| Step: 6
Training loss: 2.8253350257873535
Validation loss: 2.2220888599272697

Epoch: 6| Step: 7
Training loss: 2.9254350662231445
Validation loss: 2.223770797893565

Epoch: 6| Step: 8
Training loss: 2.8780739307403564
Validation loss: 2.220127353104212

Epoch: 6| Step: 9
Training loss: 2.037036657333374
Validation loss: 2.220208934558335

Epoch: 6| Step: 10
Training loss: 3.1023452281951904
Validation loss: 2.2190447648366294

Epoch: 6| Step: 11
Training loss: 2.028235673904419
Validation loss: 2.2230509147849133

Epoch: 6| Step: 12
Training loss: 2.731881618499756
Validation loss: 2.2144706813238

Epoch: 6| Step: 13
Training loss: 2.272265672683716
Validation loss: 2.2295172778508996

Epoch: 63| Step: 0
Training loss: 2.2838590145111084
Validation loss: 2.2430693000875492

Epoch: 6| Step: 1
Training loss: 2.9562368392944336
Validation loss: 2.2536972261244252

Epoch: 6| Step: 2
Training loss: 1.841554045677185
Validation loss: 2.2661882395385415

Epoch: 6| Step: 3
Training loss: 2.9025681018829346
Validation loss: 2.260128354513517

Epoch: 6| Step: 4
Training loss: 2.5725317001342773
Validation loss: 2.2557956300755984

Epoch: 6| Step: 5
Training loss: 1.8321728706359863
Validation loss: 2.249245725652223

Epoch: 6| Step: 6
Training loss: 2.867806911468506
Validation loss: 2.2283809774665424

Epoch: 6| Step: 7
Training loss: 3.1975436210632324
Validation loss: 2.2211080610111194

Epoch: 6| Step: 8
Training loss: 2.6644809246063232
Validation loss: 2.2155569727702806

Epoch: 6| Step: 9
Training loss: 3.3959801197052
Validation loss: 2.2105540972883984

Epoch: 6| Step: 10
Training loss: 2.5506112575531006
Validation loss: 2.207419732565521

Epoch: 6| Step: 11
Training loss: 2.032301425933838
Validation loss: 2.2046593107203

Epoch: 6| Step: 12
Training loss: 1.885101556777954
Validation loss: 2.20377540844743

Epoch: 6| Step: 13
Training loss: 2.679487466812134
Validation loss: 2.2026084828120407

Epoch: 64| Step: 0
Training loss: 2.6811537742614746
Validation loss: 2.2002689633318173

Epoch: 6| Step: 1
Training loss: 2.6338915824890137
Validation loss: 2.1998323061132945

Epoch: 6| Step: 2
Training loss: 2.2294564247131348
Validation loss: 2.199907083665171

Epoch: 6| Step: 3
Training loss: 2.075531005859375
Validation loss: 2.1989784074086014

Epoch: 6| Step: 4
Training loss: 2.5391035079956055
Validation loss: 2.2044289227454894

Epoch: 6| Step: 5
Training loss: 2.390101432800293
Validation loss: 2.1976774020861556

Epoch: 6| Step: 6
Training loss: 2.3538591861724854
Validation loss: 2.206485822636594

Epoch: 6| Step: 7
Training loss: 2.49515438079834
Validation loss: 2.200201820301753

Epoch: 6| Step: 8
Training loss: 2.5169405937194824
Validation loss: 2.21182438635057

Epoch: 6| Step: 9
Training loss: 2.79685115814209
Validation loss: 2.2202656038345827

Epoch: 6| Step: 10
Training loss: 2.834913730621338
Validation loss: 2.2183116943605485

Epoch: 6| Step: 11
Training loss: 2.1639742851257324
Validation loss: 2.2376670401583434

Epoch: 6| Step: 12
Training loss: 2.9387869834899902
Validation loss: 2.2399477087041384

Epoch: 6| Step: 13
Training loss: 2.901970624923706
Validation loss: 2.2318796086054977

Epoch: 65| Step: 0
Training loss: 2.6612939834594727
Validation loss: 2.231719655375327

Epoch: 6| Step: 1
Training loss: 2.01664137840271
Validation loss: 2.2241122991808

Epoch: 6| Step: 2
Training loss: 2.380703926086426
Validation loss: 2.2278130797929663

Epoch: 6| Step: 3
Training loss: 2.782121181488037
Validation loss: 2.2479346003583682

Epoch: 6| Step: 4
Training loss: 2.193118095397949
Validation loss: 2.2516943177869244

Epoch: 6| Step: 5
Training loss: 2.462907552719116
Validation loss: 2.253852350737459

Epoch: 6| Step: 6
Training loss: 2.9280943870544434
Validation loss: 2.2794081959673154

Epoch: 6| Step: 7
Training loss: 2.826779842376709
Validation loss: 2.2599592003771054

Epoch: 6| Step: 8
Training loss: 1.7461299896240234
Validation loss: 2.2328366207820114

Epoch: 6| Step: 9
Training loss: 2.7646679878234863
Validation loss: 2.2297038442345074

Epoch: 6| Step: 10
Training loss: 2.5011940002441406
Validation loss: 2.207134049425843

Epoch: 6| Step: 11
Training loss: 2.260162353515625
Validation loss: 2.205456610648863

Epoch: 6| Step: 12
Training loss: 2.7555699348449707
Validation loss: 2.2011964410863896

Epoch: 6| Step: 13
Training loss: 3.2426252365112305
Validation loss: 2.2023559936913113

Epoch: 66| Step: 0
Training loss: 2.506354331970215
Validation loss: 2.2150356308106454

Epoch: 6| Step: 1
Training loss: 2.461923599243164
Validation loss: 2.2200615149672314

Epoch: 6| Step: 2
Training loss: 2.444978713989258
Validation loss: 2.2173178298498994

Epoch: 6| Step: 3
Training loss: 2.499305248260498
Validation loss: 2.2194484203092513

Epoch: 6| Step: 4
Training loss: 3.0184648036956787
Validation loss: 2.2176950785421554

Epoch: 6| Step: 5
Training loss: 2.398874521255493
Validation loss: 2.2186267260582215

Epoch: 6| Step: 6
Training loss: 2.484044313430786
Validation loss: 2.2134162302940124

Epoch: 6| Step: 7
Training loss: 2.6817209720611572
Validation loss: 2.206774814154512

Epoch: 6| Step: 8
Training loss: 2.2733120918273926
Validation loss: 2.2014491160710654

Epoch: 6| Step: 9
Training loss: 3.2500927448272705
Validation loss: 2.1878277396643036

Epoch: 6| Step: 10
Training loss: 2.407362222671509
Validation loss: 2.181409398714701

Epoch: 6| Step: 11
Training loss: 1.9459587335586548
Validation loss: 2.190706752961682

Epoch: 6| Step: 12
Training loss: 2.5776400566101074
Validation loss: 2.2087839700842418

Epoch: 6| Step: 13
Training loss: 2.9884729385375977
Validation loss: 2.240874926249186

Epoch: 67| Step: 0
Training loss: 2.6417489051818848
Validation loss: 2.288510250788863

Epoch: 6| Step: 1
Training loss: 2.6981215476989746
Validation loss: 2.3334236273201565

Epoch: 6| Step: 2
Training loss: 2.5439631938934326
Validation loss: 2.382022652574765

Epoch: 6| Step: 3
Training loss: 2.0133345127105713
Validation loss: 2.36342821069943

Epoch: 6| Step: 4
Training loss: 2.410961627960205
Validation loss: 2.3413481955887168

Epoch: 6| Step: 5
Training loss: 2.9584028720855713
Validation loss: 2.271011152575093

Epoch: 6| Step: 6
Training loss: 2.9234447479248047
Validation loss: 2.2241638783485658

Epoch: 6| Step: 7
Training loss: 3.040032386779785
Validation loss: 2.2044591749868085

Epoch: 6| Step: 8
Training loss: 2.6416120529174805
Validation loss: 2.215310000604199

Epoch: 6| Step: 9
Training loss: 1.9277317523956299
Validation loss: 2.2074023446729107

Epoch: 6| Step: 10
Training loss: 2.7216200828552246
Validation loss: 2.2059139051745014

Epoch: 6| Step: 11
Training loss: 2.3224291801452637
Validation loss: 2.201241518861504

Epoch: 6| Step: 12
Training loss: 2.2322843074798584
Validation loss: 2.1986869586411344

Epoch: 6| Step: 13
Training loss: 2.422976493835449
Validation loss: 2.1991734094517206

Epoch: 68| Step: 0
Training loss: 2.4310836791992188
Validation loss: 2.1994730144418697

Epoch: 6| Step: 1
Training loss: 2.785642147064209
Validation loss: 2.1981808267613894

Epoch: 6| Step: 2
Training loss: 2.7304983139038086
Validation loss: 2.2019807446387505

Epoch: 6| Step: 3
Training loss: 2.4040069580078125
Validation loss: 2.1939689472157466

Epoch: 6| Step: 4
Training loss: 3.4018850326538086
Validation loss: 2.209869611647821

Epoch: 6| Step: 5
Training loss: 2.424832820892334
Validation loss: 2.205650342408047

Epoch: 6| Step: 6
Training loss: 2.1270203590393066
Validation loss: 2.226888466906804

Epoch: 6| Step: 7
Training loss: 2.4341442584991455
Validation loss: 2.2328811550653107

Epoch: 6| Step: 8
Training loss: 2.302563190460205
Validation loss: 2.2519860959822133

Epoch: 6| Step: 9
Training loss: 2.761528730392456
Validation loss: 2.237577574227446

Epoch: 6| Step: 10
Training loss: 2.181708574295044
Validation loss: 2.226022397318194

Epoch: 6| Step: 11
Training loss: 2.318657398223877
Validation loss: 2.209733973267258

Epoch: 6| Step: 12
Training loss: 2.664820909500122
Validation loss: 2.1958280942773305

Epoch: 6| Step: 13
Training loss: 2.146965980529785
Validation loss: 2.1914496344904744

Epoch: 69| Step: 0
Training loss: 2.316019058227539
Validation loss: 2.184133683481524

Epoch: 6| Step: 1
Training loss: 2.5711798667907715
Validation loss: 2.186513705920148

Epoch: 6| Step: 2
Training loss: 2.802666187286377
Validation loss: 2.1821967453084965

Epoch: 6| Step: 3
Training loss: 1.7980624437332153
Validation loss: 2.180662824261573

Epoch: 6| Step: 4
Training loss: 2.605807304382324
Validation loss: 2.1864563829155377

Epoch: 6| Step: 5
Training loss: 2.662435531616211
Validation loss: 2.172103766472109

Epoch: 6| Step: 6
Training loss: 2.41275691986084
Validation loss: 2.1858684965359267

Epoch: 6| Step: 7
Training loss: 2.6316542625427246
Validation loss: 2.1835539879337436

Epoch: 6| Step: 8
Training loss: 2.096825122833252
Validation loss: 2.183465419277068

Epoch: 6| Step: 9
Training loss: 2.334731340408325
Validation loss: 2.1844135074205298

Epoch: 6| Step: 10
Training loss: 2.422283887863159
Validation loss: 2.1852225206231557

Epoch: 6| Step: 11
Training loss: 2.944516897201538
Validation loss: 2.19237288992892

Epoch: 6| Step: 12
Training loss: 2.5799083709716797
Validation loss: 2.191421775407689

Epoch: 6| Step: 13
Training loss: 3.417881965637207
Validation loss: 2.1976754639738347

Epoch: 70| Step: 0
Training loss: 2.6358416080474854
Validation loss: 2.199721528637794

Epoch: 6| Step: 1
Training loss: 2.2933008670806885
Validation loss: 2.2113877265684065

Epoch: 6| Step: 2
Training loss: 3.316955089569092
Validation loss: 2.2316593739294235

Epoch: 6| Step: 3
Training loss: 2.1610257625579834
Validation loss: 2.2342163926811627

Epoch: 6| Step: 4
Training loss: 2.185511827468872
Validation loss: 2.2133174609112483

Epoch: 6| Step: 5
Training loss: 2.216996908187866
Validation loss: 2.1965544044330554

Epoch: 6| Step: 6
Training loss: 1.94223952293396
Validation loss: 2.191188737910281

Epoch: 6| Step: 7
Training loss: 2.299759864807129
Validation loss: 2.184354302703693

Epoch: 6| Step: 8
Training loss: 2.3090531826019287
Validation loss: 2.1721540574104554

Epoch: 6| Step: 9
Training loss: 2.7208681106567383
Validation loss: 2.1770475910555933

Epoch: 6| Step: 10
Training loss: 2.7199032306671143
Validation loss: 2.174268609733992

Epoch: 6| Step: 11
Training loss: 2.6143176555633545
Validation loss: 2.1755710776134203

Epoch: 6| Step: 12
Training loss: 3.1248199939727783
Validation loss: 2.1750958504215365

Epoch: 6| Step: 13
Training loss: 2.6411492824554443
Validation loss: 2.1816585448480423

Epoch: 71| Step: 0
Training loss: 3.0801374912261963
Validation loss: 2.1813941104437715

Epoch: 6| Step: 1
Training loss: 2.879958152770996
Validation loss: 2.1893783679572483

Epoch: 6| Step: 2
Training loss: 2.9067153930664062
Validation loss: 2.2105850635036344

Epoch: 6| Step: 3
Training loss: 2.480668544769287
Validation loss: 2.2243169251308648

Epoch: 6| Step: 4
Training loss: 1.7987009286880493
Validation loss: 2.250502529964652

Epoch: 6| Step: 5
Training loss: 1.6851387023925781
Validation loss: 2.283381272387761

Epoch: 6| Step: 6
Training loss: 2.7764158248901367
Validation loss: 2.286814271762807

Epoch: 6| Step: 7
Training loss: 2.817824125289917
Validation loss: 2.2818995368096138

Epoch: 6| Step: 8
Training loss: 3.0358829498291016
Validation loss: 2.2597858341791297

Epoch: 6| Step: 9
Training loss: 3.0639748573303223
Validation loss: 2.2434860660183813

Epoch: 6| Step: 10
Training loss: 2.258474349975586
Validation loss: 2.2211946031098724

Epoch: 6| Step: 11
Training loss: 2.245392322540283
Validation loss: 2.2029903140119327

Epoch: 6| Step: 12
Training loss: 1.8382819890975952
Validation loss: 2.1926013115913636

Epoch: 6| Step: 13
Training loss: 1.770592212677002
Validation loss: 2.173333739721647

Epoch: 72| Step: 0
Training loss: 2.306224822998047
Validation loss: 2.1658854638376543

Epoch: 6| Step: 1
Training loss: 2.556382179260254
Validation loss: 2.1575925222007175

Epoch: 6| Step: 2
Training loss: 3.0163865089416504
Validation loss: 2.1577460663292998

Epoch: 6| Step: 3
Training loss: 2.7027599811553955
Validation loss: 2.1519741755659862

Epoch: 6| Step: 4
Training loss: 2.739536762237549
Validation loss: 2.154822123947964

Epoch: 6| Step: 5
Training loss: 2.653825044631958
Validation loss: 2.1642464630065428

Epoch: 6| Step: 6
Training loss: 3.042670726776123
Validation loss: 2.169320326979442

Epoch: 6| Step: 7
Training loss: 2.3381459712982178
Validation loss: 2.171481822126655

Epoch: 6| Step: 8
Training loss: 1.8634512424468994
Validation loss: 2.1846288045247397

Epoch: 6| Step: 9
Training loss: 2.884115695953369
Validation loss: 2.181554227746943

Epoch: 6| Step: 10
Training loss: 1.8978396654129028
Validation loss: 2.1908293385659494

Epoch: 6| Step: 11
Training loss: 2.1711885929107666
Validation loss: 2.185920279513123

Epoch: 6| Step: 12
Training loss: 2.291870355606079
Validation loss: 2.1783411028564617

Epoch: 6| Step: 13
Training loss: 2.3388051986694336
Validation loss: 2.1683921506327968

Epoch: 73| Step: 0
Training loss: 2.91558837890625
Validation loss: 2.161120730061685

Epoch: 6| Step: 1
Training loss: 2.377558946609497
Validation loss: 2.167178056573355

Epoch: 6| Step: 2
Training loss: 3.3982768058776855
Validation loss: 2.173102947973436

Epoch: 6| Step: 3
Training loss: 2.6524481773376465
Validation loss: 2.178194224193532

Epoch: 6| Step: 4
Training loss: 2.6768577098846436
Validation loss: 2.188814560572306

Epoch: 6| Step: 5
Training loss: 1.3605983257293701
Validation loss: 2.1985160843018563

Epoch: 6| Step: 6
Training loss: 1.7798619270324707
Validation loss: 2.20170182822853

Epoch: 6| Step: 7
Training loss: 2.933619499206543
Validation loss: 2.19412185043417

Epoch: 6| Step: 8
Training loss: 2.5162510871887207
Validation loss: 2.1809910779358237

Epoch: 6| Step: 9
Training loss: 2.139256000518799
Validation loss: 2.179784705561976

Epoch: 6| Step: 10
Training loss: 2.895895004272461
Validation loss: 2.175149566383772

Epoch: 6| Step: 11
Training loss: 1.941494107246399
Validation loss: 2.172301430856028

Epoch: 6| Step: 12
Training loss: 2.490278720855713
Validation loss: 2.1706350618793118

Epoch: 6| Step: 13
Training loss: 3.294959545135498
Validation loss: 2.1633653948383946

Epoch: 74| Step: 0
Training loss: 2.321439743041992
Validation loss: 2.1669672945494294

Epoch: 6| Step: 1
Training loss: 2.625819683074951
Validation loss: 2.1673643794111026

Epoch: 6| Step: 2
Training loss: 2.9086833000183105
Validation loss: 2.187213061958231

Epoch: 6| Step: 3
Training loss: 1.9929659366607666
Validation loss: 2.28475615286058

Epoch: 6| Step: 4
Training loss: 3.3655643463134766
Validation loss: 2.3264232297097482

Epoch: 6| Step: 5
Training loss: 2.4141886234283447
Validation loss: 2.3484489148662937

Epoch: 6| Step: 6
Training loss: 2.3387579917907715
Validation loss: 2.3557815961940314

Epoch: 6| Step: 7
Training loss: 2.058103084564209
Validation loss: 2.34514344123102

Epoch: 6| Step: 8
Training loss: 2.2150650024414062
Validation loss: 2.303014143820732

Epoch: 6| Step: 9
Training loss: 2.3677597045898438
Validation loss: 2.2489589645016577

Epoch: 6| Step: 10
Training loss: 2.3525612354278564
Validation loss: 2.2346806577456895

Epoch: 6| Step: 11
Training loss: 2.9209203720092773
Validation loss: 2.2214295889741633

Epoch: 6| Step: 12
Training loss: 2.622974157333374
Validation loss: 2.2237800500726186

Epoch: 6| Step: 13
Training loss: 3.216416358947754
Validation loss: 2.2309379731455157

Epoch: 75| Step: 0
Training loss: 3.0051465034484863
Validation loss: 2.224820529260943

Epoch: 6| Step: 1
Training loss: 2.613586902618408
Validation loss: 2.22509797926872

Epoch: 6| Step: 2
Training loss: 2.329008102416992
Validation loss: 2.2184607444270963

Epoch: 6| Step: 3
Training loss: 2.487652063369751
Validation loss: 2.1972438750728482

Epoch: 6| Step: 4
Training loss: 2.5066468715667725
Validation loss: 2.174050377261254

Epoch: 6| Step: 5
Training loss: 2.802809715270996
Validation loss: 2.1880053192056637

Epoch: 6| Step: 6
Training loss: 2.6842966079711914
Validation loss: 2.211325422410042

Epoch: 6| Step: 7
Training loss: 2.294240951538086
Validation loss: 2.2436748960966706

Epoch: 6| Step: 8
Training loss: 2.005852699279785
Validation loss: 2.2617881682611283

Epoch: 6| Step: 9
Training loss: 2.6193959712982178
Validation loss: 2.322657505671183

Epoch: 6| Step: 10
Training loss: 2.909493923187256
Validation loss: 2.3342404109175487

Epoch: 6| Step: 11
Training loss: 2.431366205215454
Validation loss: 2.3196601406220467

Epoch: 6| Step: 12
Training loss: 2.483593463897705
Validation loss: 2.2652519902875348

Epoch: 6| Step: 13
Training loss: 1.6389248371124268
Validation loss: 2.2302981525339107

Epoch: 76| Step: 0
Training loss: 1.6871999502182007
Validation loss: 2.197364273891654

Epoch: 6| Step: 1
Training loss: 2.9620790481567383
Validation loss: 2.1920308477135113

Epoch: 6| Step: 2
Training loss: 2.299985408782959
Validation loss: 2.1804882518706785

Epoch: 6| Step: 3
Training loss: 2.394473075866699
Validation loss: 2.186248262723287

Epoch: 6| Step: 4
Training loss: 2.589686155319214
Validation loss: 2.1805656494632846

Epoch: 6| Step: 5
Training loss: 2.4942591190338135
Validation loss: 2.186212078217537

Epoch: 6| Step: 6
Training loss: 3.211895704269409
Validation loss: 2.1804770500429216

Epoch: 6| Step: 7
Training loss: 2.0429630279541016
Validation loss: 2.176266957354802

Epoch: 6| Step: 8
Training loss: 2.5412404537200928
Validation loss: 2.155933262199484

Epoch: 6| Step: 9
Training loss: 3.0418858528137207
Validation loss: 2.1486898929842058

Epoch: 6| Step: 10
Training loss: 2.3342089653015137
Validation loss: 2.1377255403867332

Epoch: 6| Step: 11
Training loss: 2.608922243118286
Validation loss: 2.1505950009951027

Epoch: 6| Step: 12
Training loss: 2.4158921241760254
Validation loss: 2.1516205315948813

Epoch: 6| Step: 13
Training loss: 1.795961856842041
Validation loss: 2.1530580982085197

Epoch: 77| Step: 0
Training loss: 2.578523635864258
Validation loss: 2.1710063257525043

Epoch: 6| Step: 1
Training loss: 2.2569265365600586
Validation loss: 2.1886320088499334

Epoch: 6| Step: 2
Training loss: 2.228060245513916
Validation loss: 2.1843211804666827

Epoch: 6| Step: 3
Training loss: 2.2586231231689453
Validation loss: 2.190046318115727

Epoch: 6| Step: 4
Training loss: 2.636737823486328
Validation loss: 2.1697901089986167

Epoch: 6| Step: 5
Training loss: 2.4705677032470703
Validation loss: 2.1706141041171167

Epoch: 6| Step: 6
Training loss: 2.825484275817871
Validation loss: 2.16053424086622

Epoch: 6| Step: 7
Training loss: 2.923332691192627
Validation loss: 2.163371193793512

Epoch: 6| Step: 8
Training loss: 2.281853199005127
Validation loss: 2.169635744505031

Epoch: 6| Step: 9
Training loss: 2.612658977508545
Validation loss: 2.166931903490456

Epoch: 6| Step: 10
Training loss: 3.3067989349365234
Validation loss: 2.166363743043715

Epoch: 6| Step: 11
Training loss: 1.37186598777771
Validation loss: 2.164175682170417

Epoch: 6| Step: 12
Training loss: 2.4754374027252197
Validation loss: 2.169367295439525

Epoch: 6| Step: 13
Training loss: 2.455131769180298
Validation loss: 2.182790474225116

Epoch: 78| Step: 0
Training loss: 2.275515079498291
Validation loss: 2.1962907673210226

Epoch: 6| Step: 1
Training loss: 2.579151153564453
Validation loss: 2.211271821811635

Epoch: 6| Step: 2
Training loss: 3.4279022216796875
Validation loss: 2.218556482304809

Epoch: 6| Step: 3
Training loss: 2.288878917694092
Validation loss: 2.2314428180776615

Epoch: 6| Step: 4
Training loss: 2.6384406089782715
Validation loss: 2.225518725251639

Epoch: 6| Step: 5
Training loss: 2.2743117809295654
Validation loss: 2.2339805531245407

Epoch: 6| Step: 6
Training loss: 1.932600736618042
Validation loss: 2.217583220492127

Epoch: 6| Step: 7
Training loss: 2.47442364692688
Validation loss: 2.202504088801722

Epoch: 6| Step: 8
Training loss: 2.150268316268921
Validation loss: 2.1961302039443806

Epoch: 6| Step: 9
Training loss: 2.6184096336364746
Validation loss: 2.1758194456818285

Epoch: 6| Step: 10
Training loss: 2.4625906944274902
Validation loss: 2.162117829886816

Epoch: 6| Step: 11
Training loss: 2.7635107040405273
Validation loss: 2.1568707009797454

Epoch: 6| Step: 12
Training loss: 2.239755868911743
Validation loss: 2.1507667982450096

Epoch: 6| Step: 13
Training loss: 2.3934075832366943
Validation loss: 2.1479809040664346

Epoch: 79| Step: 0
Training loss: 2.2918715476989746
Validation loss: 2.1364099005217194

Epoch: 6| Step: 1
Training loss: 2.3607420921325684
Validation loss: 2.1455843217911257

Epoch: 6| Step: 2
Training loss: 2.4870615005493164
Validation loss: 2.1406406138532903

Epoch: 6| Step: 3
Training loss: 2.5550904273986816
Validation loss: 2.139772222888085

Epoch: 6| Step: 4
Training loss: 2.6223983764648438
Validation loss: 2.138929982339182

Epoch: 6| Step: 5
Training loss: 2.600013494491577
Validation loss: 2.1391608407420497

Epoch: 6| Step: 6
Training loss: 2.399470329284668
Validation loss: 2.136659824719993

Epoch: 6| Step: 7
Training loss: 2.587223529815674
Validation loss: 2.133711443152479

Epoch: 6| Step: 8
Training loss: 2.730461835861206
Validation loss: 2.1302195364429104

Epoch: 6| Step: 9
Training loss: 2.3146414756774902
Validation loss: 2.113967385343326

Epoch: 6| Step: 10
Training loss: 3.0700855255126953
Validation loss: 2.1179249953198176

Epoch: 6| Step: 11
Training loss: 2.0124473571777344
Validation loss: 2.1138706053456953

Epoch: 6| Step: 12
Training loss: 2.149405002593994
Validation loss: 2.1187682074885212

Epoch: 6| Step: 13
Training loss: 2.8408327102661133
Validation loss: 2.1297618291711293

Epoch: 80| Step: 0
Training loss: 2.55584454536438
Validation loss: 2.1475051449191187

Epoch: 6| Step: 1
Training loss: 2.3306775093078613
Validation loss: 2.1598567373009137

Epoch: 6| Step: 2
Training loss: 2.9738283157348633
Validation loss: 2.1878906514054988

Epoch: 6| Step: 3
Training loss: 2.243208408355713
Validation loss: 2.2329888138719785

Epoch: 6| Step: 4
Training loss: 2.2360239028930664
Validation loss: 2.2857028386926137

Epoch: 6| Step: 5
Training loss: 2.0459976196289062
Validation loss: 2.267391868816909

Epoch: 6| Step: 6
Training loss: 2.4945144653320312
Validation loss: 2.2911368698202152

Epoch: 6| Step: 7
Training loss: 2.295572280883789
Validation loss: 2.2531608740488687

Epoch: 6| Step: 8
Training loss: 2.408540725708008
Validation loss: 2.172580603630312

Epoch: 6| Step: 9
Training loss: 1.5771491527557373
Validation loss: 2.139213785048454

Epoch: 6| Step: 10
Training loss: 2.2803428173065186
Validation loss: 2.1166531411550378

Epoch: 6| Step: 11
Training loss: 3.166827440261841
Validation loss: 2.100099935326525

Epoch: 6| Step: 12
Training loss: 3.4453954696655273
Validation loss: 2.0994005869793635

Epoch: 6| Step: 13
Training loss: 2.5596368312835693
Validation loss: 2.1156781270939815

Epoch: 81| Step: 0
Training loss: 2.7185683250427246
Validation loss: 2.13190112831772

Epoch: 6| Step: 1
Training loss: 2.4673144817352295
Validation loss: 2.1346030017381072

Epoch: 6| Step: 2
Training loss: 2.71067476272583
Validation loss: 2.1323251339697067

Epoch: 6| Step: 3
Training loss: 2.324655771255493
Validation loss: 2.129554274261639

Epoch: 6| Step: 4
Training loss: 1.941200852394104
Validation loss: 2.1407338009085706

Epoch: 6| Step: 5
Training loss: 2.563218832015991
Validation loss: 2.1438663005828857

Epoch: 6| Step: 6
Training loss: 2.02394962310791
Validation loss: 2.1516467294385357

Epoch: 6| Step: 7
Training loss: 3.0065691471099854
Validation loss: 2.181409881960961

Epoch: 6| Step: 8
Training loss: 2.8751611709594727
Validation loss: 2.220915525190292

Epoch: 6| Step: 9
Training loss: 2.5500478744506836
Validation loss: 2.2374855036376626

Epoch: 6| Step: 10
Training loss: 3.335883378982544
Validation loss: 2.223710608738725

Epoch: 6| Step: 11
Training loss: 2.8900203704833984
Validation loss: 2.201645533243815

Epoch: 6| Step: 12
Training loss: 1.7763731479644775
Validation loss: 2.185046121638308

Epoch: 6| Step: 13
Training loss: 1.9577383995056152
Validation loss: 2.1842034427068566

Epoch: 82| Step: 0
Training loss: 2.7423713207244873
Validation loss: 2.2163446949374292

Epoch: 6| Step: 1
Training loss: 2.2488317489624023
Validation loss: 2.2424490015993834

Epoch: 6| Step: 2
Training loss: 1.2461326122283936
Validation loss: 2.281256277074096

Epoch: 6| Step: 3
Training loss: 2.643589973449707
Validation loss: 2.3186966962711786

Epoch: 6| Step: 4
Training loss: 2.3312389850616455
Validation loss: 2.3146413756955053

Epoch: 6| Step: 5
Training loss: 2.2952468395233154
Validation loss: 2.3140810535800074

Epoch: 6| Step: 6
Training loss: 3.094710350036621
Validation loss: 2.245461566473848

Epoch: 6| Step: 7
Training loss: 2.755340814590454
Validation loss: 2.1682359762089227

Epoch: 6| Step: 8
Training loss: 2.2905490398406982
Validation loss: 2.134339476144442

Epoch: 6| Step: 9
Training loss: 2.514883041381836
Validation loss: 2.1127738080998903

Epoch: 6| Step: 10
Training loss: 3.198800563812256
Validation loss: 2.099237923981041

Epoch: 6| Step: 11
Training loss: 2.7119171619415283
Validation loss: 2.0922514007937525

Epoch: 6| Step: 12
Training loss: 2.570378303527832
Validation loss: 2.096176980644144

Epoch: 6| Step: 13
Training loss: 1.9460999965667725
Validation loss: 2.1045739368725846

Epoch: 83| Step: 0
Training loss: 2.157895088195801
Validation loss: 2.1021455718624975

Epoch: 6| Step: 1
Training loss: 3.197861909866333
Validation loss: 2.1008575731708157

Epoch: 6| Step: 2
Training loss: 1.7472072839736938
Validation loss: 2.098456140487425

Epoch: 6| Step: 3
Training loss: 2.20147705078125
Validation loss: 2.098755545513604

Epoch: 6| Step: 4
Training loss: 3.0691661834716797
Validation loss: 2.1116114457448325

Epoch: 6| Step: 5
Training loss: 2.5185155868530273
Validation loss: 2.1324084933086107

Epoch: 6| Step: 6
Training loss: 2.6083085536956787
Validation loss: 2.1514402871490805

Epoch: 6| Step: 7
Training loss: 3.0819714069366455
Validation loss: 2.1695731250188683

Epoch: 6| Step: 8
Training loss: 2.5139060020446777
Validation loss: 2.20359278750676

Epoch: 6| Step: 9
Training loss: 2.767885446548462
Validation loss: 2.2383659193592687

Epoch: 6| Step: 10
Training loss: 1.7800921201705933
Validation loss: 2.2131879406590618

Epoch: 6| Step: 11
Training loss: 1.9190119504928589
Validation loss: 2.2158890180690314

Epoch: 6| Step: 12
Training loss: 2.3117949962615967
Validation loss: 2.1767319966388006

Epoch: 6| Step: 13
Training loss: 2.8777570724487305
Validation loss: 2.154729789303195

Epoch: 84| Step: 0
Training loss: 2.3748691082000732
Validation loss: 2.121939471972886

Epoch: 6| Step: 1
Training loss: 2.011223793029785
Validation loss: 2.108591630894651

Epoch: 6| Step: 2
Training loss: 2.378732442855835
Validation loss: 2.097740750158987

Epoch: 6| Step: 3
Training loss: 1.9896396398544312
Validation loss: 2.0993766618031326

Epoch: 6| Step: 4
Training loss: 2.5432257652282715
Validation loss: 2.0937231227915776

Epoch: 6| Step: 5
Training loss: 3.213486909866333
Validation loss: 2.089544552628712

Epoch: 6| Step: 6
Training loss: 3.08054256439209
Validation loss: 2.099230932933028

Epoch: 6| Step: 7
Training loss: 2.305961847305298
Validation loss: 2.1128170105718795

Epoch: 6| Step: 8
Training loss: 1.9261969327926636
Validation loss: 2.1293116692573792

Epoch: 6| Step: 9
Training loss: 2.565164804458618
Validation loss: 2.143433286297706

Epoch: 6| Step: 10
Training loss: 2.645782709121704
Validation loss: 2.1445425710370465

Epoch: 6| Step: 11
Training loss: 2.729153633117676
Validation loss: 2.129439594925091

Epoch: 6| Step: 12
Training loss: 2.626898765563965
Validation loss: 2.117141253204756

Epoch: 6| Step: 13
Training loss: 1.845841884613037
Validation loss: 2.1058656272067817

Epoch: 85| Step: 0
Training loss: 2.9643068313598633
Validation loss: 2.1003336534705213

Epoch: 6| Step: 1
Training loss: 2.588313341140747
Validation loss: 2.1220123819125596

Epoch: 6| Step: 2
Training loss: 2.740731716156006
Validation loss: 2.1406723017333658

Epoch: 6| Step: 3
Training loss: 2.8366494178771973
Validation loss: 2.1683247089385986

Epoch: 6| Step: 4
Training loss: 3.205233573913574
Validation loss: 2.186331033706665

Epoch: 6| Step: 5
Training loss: 2.278902053833008
Validation loss: 2.2244572806101974

Epoch: 6| Step: 6
Training loss: 2.3101258277893066
Validation loss: 2.2643596818370204

Epoch: 6| Step: 7
Training loss: 2.233905792236328
Validation loss: 2.318466491596673

Epoch: 6| Step: 8
Training loss: 2.099769353866577
Validation loss: 2.326241272752003

Epoch: 6| Step: 9
Training loss: 2.3777706623077393
Validation loss: 2.295889933904012

Epoch: 6| Step: 10
Training loss: 2.4444448947906494
Validation loss: 2.274732087248115

Epoch: 6| Step: 11
Training loss: 1.1290658712387085
Validation loss: 2.2221665343930646

Epoch: 6| Step: 12
Training loss: 2.620237112045288
Validation loss: 2.167616598067745

Epoch: 6| Step: 13
Training loss: 2.6337645053863525
Validation loss: 2.1119978722705635

Epoch: 86| Step: 0
Training loss: 2.6618294715881348
Validation loss: 2.099244971429148

Epoch: 6| Step: 1
Training loss: 1.9233019351959229
Validation loss: 2.1090849535439604

Epoch: 6| Step: 2
Training loss: 2.1608543395996094
Validation loss: 2.1212107942950342

Epoch: 6| Step: 3
Training loss: 3.37361216545105
Validation loss: 2.1485911261650825

Epoch: 6| Step: 4
Training loss: 1.837860107421875
Validation loss: 2.1575148451712822

Epoch: 6| Step: 5
Training loss: 2.575687885284424
Validation loss: 2.1676694552103677

Epoch: 6| Step: 6
Training loss: 2.5153422355651855
Validation loss: 2.1641030132129626

Epoch: 6| Step: 7
Training loss: 2.6725716590881348
Validation loss: 2.1569413549156597

Epoch: 6| Step: 8
Training loss: 3.0929312705993652
Validation loss: 2.1465812908705844

Epoch: 6| Step: 9
Training loss: 2.1597516536712646
Validation loss: 2.1385944786892144

Epoch: 6| Step: 10
Training loss: 2.970353603363037
Validation loss: 2.1278882718855336

Epoch: 6| Step: 11
Training loss: 2.296156883239746
Validation loss: 2.114333345044044

Epoch: 6| Step: 12
Training loss: 3.039059638977051
Validation loss: 2.104935548638785

Epoch: 6| Step: 13
Training loss: 1.5580319166183472
Validation loss: 2.0985615817449426

Epoch: 87| Step: 0
Training loss: 2.916738986968994
Validation loss: 2.093386603939918

Epoch: 6| Step: 1
Training loss: 2.3720531463623047
Validation loss: 2.0931787029389413

Epoch: 6| Step: 2
Training loss: 2.2641384601593018
Validation loss: 2.1003660078971618

Epoch: 6| Step: 3
Training loss: 2.66465163230896
Validation loss: 2.114817953878833

Epoch: 6| Step: 4
Training loss: 2.3691587448120117
Validation loss: 2.126428187534373

Epoch: 6| Step: 5
Training loss: 2.394756317138672
Validation loss: 2.134452854433367

Epoch: 6| Step: 6
Training loss: 2.07271671295166
Validation loss: 2.146166661734222

Epoch: 6| Step: 7
Training loss: 2.3365533351898193
Validation loss: 2.138597564030719

Epoch: 6| Step: 8
Training loss: 1.7426660060882568
Validation loss: 2.136528553501252

Epoch: 6| Step: 9
Training loss: 2.956529378890991
Validation loss: 2.145759205664358

Epoch: 6| Step: 10
Training loss: 2.846942901611328
Validation loss: 2.164406407263971

Epoch: 6| Step: 11
Training loss: 2.816782236099243
Validation loss: 2.1579473557010775

Epoch: 6| Step: 12
Training loss: 2.308300256729126
Validation loss: 2.1633089050169914

Epoch: 6| Step: 13
Training loss: 1.8060040473937988
Validation loss: 2.151692005895799

Epoch: 88| Step: 0
Training loss: 2.3990819454193115
Validation loss: 2.152029022093742

Epoch: 6| Step: 1
Training loss: 3.019636631011963
Validation loss: 2.13713098854147

Epoch: 6| Step: 2
Training loss: 1.914527416229248
Validation loss: 2.1390072812316236

Epoch: 6| Step: 3
Training loss: 2.139070987701416
Validation loss: 2.1476530298109977

Epoch: 6| Step: 4
Training loss: 2.3563432693481445
Validation loss: 2.1626390231552945

Epoch: 6| Step: 5
Training loss: 2.407681941986084
Validation loss: 2.1687571335864324

Epoch: 6| Step: 6
Training loss: 2.442412853240967
Validation loss: 2.1753839139015443

Epoch: 6| Step: 7
Training loss: 3.1221375465393066
Validation loss: 2.181790218558363

Epoch: 6| Step: 8
Training loss: 2.4215946197509766
Validation loss: 2.1921012017034713

Epoch: 6| Step: 9
Training loss: 1.9086918830871582
Validation loss: 2.1872234677755706

Epoch: 6| Step: 10
Training loss: 2.6193222999572754
Validation loss: 2.1754026271963633

Epoch: 6| Step: 11
Training loss: 2.9960622787475586
Validation loss: 2.154748665389194

Epoch: 6| Step: 12
Training loss: 1.6264314651489258
Validation loss: 2.1321998065517795

Epoch: 6| Step: 13
Training loss: 3.1344528198242188
Validation loss: 2.1158437792972853

Epoch: 89| Step: 0
Training loss: 2.2002081871032715
Validation loss: 2.091031369342599

Epoch: 6| Step: 1
Training loss: 2.9638748168945312
Validation loss: 2.0861133913840018

Epoch: 6| Step: 2
Training loss: 2.7792110443115234
Validation loss: 2.07973111573086

Epoch: 6| Step: 3
Training loss: 2.7153310775756836
Validation loss: 2.0732864385010092

Epoch: 6| Step: 4
Training loss: 2.7519876956939697
Validation loss: 2.0802198776634793

Epoch: 6| Step: 5
Training loss: 1.8808443546295166
Validation loss: 2.0780236310856317

Epoch: 6| Step: 6
Training loss: 2.2204666137695312
Validation loss: 2.0780673309039046

Epoch: 6| Step: 7
Training loss: 2.1433029174804688
Validation loss: 2.0817615921779344

Epoch: 6| Step: 8
Training loss: 1.9102290868759155
Validation loss: 2.082043563165972

Epoch: 6| Step: 9
Training loss: 1.9904561042785645
Validation loss: 2.0887717021408903

Epoch: 6| Step: 10
Training loss: 2.988204002380371
Validation loss: 2.104659944452265

Epoch: 6| Step: 11
Training loss: 2.4075918197631836
Validation loss: 2.1111554817486833

Epoch: 6| Step: 12
Training loss: 2.5703792572021484
Validation loss: 2.1248665317412345

Epoch: 6| Step: 13
Training loss: 2.7099997997283936
Validation loss: 2.1329798903516544

Epoch: 90| Step: 0
Training loss: 2.8097798824310303
Validation loss: 2.1291383748413413

Epoch: 6| Step: 1
Training loss: 2.1553456783294678
Validation loss: 2.124676018632868

Epoch: 6| Step: 2
Training loss: 2.7556424140930176
Validation loss: 2.1089083635678856

Epoch: 6| Step: 3
Training loss: 2.5547375679016113
Validation loss: 2.1207100781061317

Epoch: 6| Step: 4
Training loss: 2.4050655364990234
Validation loss: 2.1103050375497467

Epoch: 6| Step: 5
Training loss: 1.7945623397827148
Validation loss: 2.1211422668990267

Epoch: 6| Step: 6
Training loss: 2.523876667022705
Validation loss: 2.1036428072119273

Epoch: 6| Step: 7
Training loss: 2.7196528911590576
Validation loss: 2.1221983227678525

Epoch: 6| Step: 8
Training loss: 1.8531709909439087
Validation loss: 2.1254896297249743

Epoch: 6| Step: 9
Training loss: 2.0553247928619385
Validation loss: 2.120904814812445

Epoch: 6| Step: 10
Training loss: 2.3001441955566406
Validation loss: 2.112981996228618

Epoch: 6| Step: 11
Training loss: 2.3620195388793945
Validation loss: 2.1083043083067863

Epoch: 6| Step: 12
Training loss: 2.879594564437866
Validation loss: 2.1074068046385244

Epoch: 6| Step: 13
Training loss: 2.5619115829467773
Validation loss: 2.0870213713697208

Epoch: 91| Step: 0
Training loss: 2.9419286251068115
Validation loss: 2.100148025379386

Epoch: 6| Step: 1
Training loss: 2.118823766708374
Validation loss: 2.096466370808181

Epoch: 6| Step: 2
Training loss: 2.1620631217956543
Validation loss: 2.104703390470115

Epoch: 6| Step: 3
Training loss: 1.9896613359451294
Validation loss: 2.1119111558442474

Epoch: 6| Step: 4
Training loss: 3.0727176666259766
Validation loss: 2.12404094588372

Epoch: 6| Step: 5
Training loss: 2.840341806411743
Validation loss: 2.1284568053419872

Epoch: 6| Step: 6
Training loss: 2.0315160751342773
Validation loss: 2.127317605480071

Epoch: 6| Step: 7
Training loss: 2.5715279579162598
Validation loss: 2.145295607146396

Epoch: 6| Step: 8
Training loss: 2.625420093536377
Validation loss: 2.1546019841265935

Epoch: 6| Step: 9
Training loss: 2.4246718883514404
Validation loss: 2.2139885989568566

Epoch: 6| Step: 10
Training loss: 2.6984448432922363
Validation loss: 2.264571112971152

Epoch: 6| Step: 11
Training loss: 2.0710549354553223
Validation loss: 2.3232454663963726

Epoch: 6| Step: 12
Training loss: 1.9601283073425293
Validation loss: 2.2802772111790155

Epoch: 6| Step: 13
Training loss: 2.404348850250244
Validation loss: 2.2249259077092653

Epoch: 92| Step: 0
Training loss: 2.2383008003234863
Validation loss: 2.1642482588368077

Epoch: 6| Step: 1
Training loss: 2.396120548248291
Validation loss: 2.138499706022201

Epoch: 6| Step: 2
Training loss: 1.7326346635818481
Validation loss: 2.102632963529197

Epoch: 6| Step: 3
Training loss: 2.249941825866699
Validation loss: 2.085986816754905

Epoch: 6| Step: 4
Training loss: 2.7326314449310303
Validation loss: 2.081399119028481

Epoch: 6| Step: 5
Training loss: 2.267263174057007
Validation loss: 2.0906506340990783

Epoch: 6| Step: 6
Training loss: 3.1282429695129395
Validation loss: 2.091813207954489

Epoch: 6| Step: 7
Training loss: 2.6769354343414307
Validation loss: 2.0950391779663744

Epoch: 6| Step: 8
Training loss: 2.0926144123077393
Validation loss: 2.105549704643988

Epoch: 6| Step: 9
Training loss: 2.9138779640197754
Validation loss: 2.112428121669318

Epoch: 6| Step: 10
Training loss: 1.7965829372406006
Validation loss: 2.109156670108918

Epoch: 6| Step: 11
Training loss: 2.9147300720214844
Validation loss: 2.107874949773153

Epoch: 6| Step: 12
Training loss: 2.2244553565979004
Validation loss: 2.110053303421185

Epoch: 6| Step: 13
Training loss: 1.6710270643234253
Validation loss: 2.102316858947918

Epoch: 93| Step: 0
Training loss: 1.7826948165893555
Validation loss: 2.106239855930369

Epoch: 6| Step: 1
Training loss: 2.3829305171966553
Validation loss: 2.108353345624862

Epoch: 6| Step: 2
Training loss: 2.6512482166290283
Validation loss: 2.1213853846314135

Epoch: 6| Step: 3
Training loss: 2.7664084434509277
Validation loss: 2.129258314768473

Epoch: 6| Step: 4
Training loss: 2.824491500854492
Validation loss: 2.1303036648740052

Epoch: 6| Step: 5
Training loss: 2.3673787117004395
Validation loss: 2.1342765849123717

Epoch: 6| Step: 6
Training loss: 2.4457573890686035
Validation loss: 2.1546609863158195

Epoch: 6| Step: 7
Training loss: 1.861857295036316
Validation loss: 2.155661747019778

Epoch: 6| Step: 8
Training loss: 2.4875645637512207
Validation loss: 2.14510779483344

Epoch: 6| Step: 9
Training loss: 1.9502828121185303
Validation loss: 2.1289077164024435

Epoch: 6| Step: 10
Training loss: 3.23551344871521
Validation loss: 2.117944227751865

Epoch: 6| Step: 11
Training loss: 1.6784436702728271
Validation loss: 2.1054999443792526

Epoch: 6| Step: 12
Training loss: 2.4496657848358154
Validation loss: 2.0852044641330676

Epoch: 6| Step: 13
Training loss: 2.6216177940368652
Validation loss: 2.0844726613772813

Epoch: 94| Step: 0
Training loss: 2.2601304054260254
Validation loss: 2.085535009702047

Epoch: 6| Step: 1
Training loss: 1.8090360164642334
Validation loss: 2.0823181662508237

Epoch: 6| Step: 2
Training loss: 2.632577419281006
Validation loss: 2.0812144497389435

Epoch: 6| Step: 3
Training loss: 2.060868263244629
Validation loss: 2.0847249313067366

Epoch: 6| Step: 4
Training loss: 1.8456554412841797
Validation loss: 2.102022176147789

Epoch: 6| Step: 5
Training loss: 2.669278144836426
Validation loss: 2.1100237510537587

Epoch: 6| Step: 6
Training loss: 2.4314932823181152
Validation loss: 2.1534984778332453

Epoch: 6| Step: 7
Training loss: 2.5938096046447754
Validation loss: 2.160801120983657

Epoch: 6| Step: 8
Training loss: 2.853403091430664
Validation loss: 2.1583514495562484

Epoch: 6| Step: 9
Training loss: 3.0673584938049316
Validation loss: 2.184411661599272

Epoch: 6| Step: 10
Training loss: 2.472975254058838
Validation loss: 2.2204979081307687

Epoch: 6| Step: 11
Training loss: 2.014763832092285
Validation loss: 2.248780256958418

Epoch: 6| Step: 12
Training loss: 2.6674532890319824
Validation loss: 2.244114250265142

Epoch: 6| Step: 13
Training loss: 2.291013717651367
Validation loss: 2.2047615871634534

Epoch: 95| Step: 0
Training loss: 2.050835132598877
Validation loss: 2.146862599157518

Epoch: 6| Step: 1
Training loss: 2.9803850650787354
Validation loss: 2.097981132486815

Epoch: 6| Step: 2
Training loss: 2.589482545852661
Validation loss: 2.0692993415299283

Epoch: 6| Step: 3
Training loss: 2.3025729656219482
Validation loss: 2.053847271908996

Epoch: 6| Step: 4
Training loss: 2.7353930473327637
Validation loss: 2.0546936347920406

Epoch: 6| Step: 5
Training loss: 2.3626015186309814
Validation loss: 2.048572390310226

Epoch: 6| Step: 6
Training loss: 1.7212638854980469
Validation loss: 2.0473341659833024

Epoch: 6| Step: 7
Training loss: 2.323230743408203
Validation loss: 2.0554883351889988

Epoch: 6| Step: 8
Training loss: 2.580350399017334
Validation loss: 2.062424900711224

Epoch: 6| Step: 9
Training loss: 2.459721326828003
Validation loss: 2.0647016930323776

Epoch: 6| Step: 10
Training loss: 2.1021299362182617
Validation loss: 2.057267737644975

Epoch: 6| Step: 11
Training loss: 2.310779571533203
Validation loss: 2.0617085015901955

Epoch: 6| Step: 12
Training loss: 2.2970519065856934
Validation loss: 2.0731788758308656

Epoch: 6| Step: 13
Training loss: 2.851846933364868
Validation loss: 2.0948893100984636

Epoch: 96| Step: 0
Training loss: 2.5839433670043945
Validation loss: 2.132252623957972

Epoch: 6| Step: 1
Training loss: 1.7470672130584717
Validation loss: 2.1247763428636777

Epoch: 6| Step: 2
Training loss: 2.9300131797790527
Validation loss: 2.118646307658124

Epoch: 6| Step: 3
Training loss: 3.1693687438964844
Validation loss: 2.1051546963312293

Epoch: 6| Step: 4
Training loss: 3.0647835731506348
Validation loss: 2.081180851946595

Epoch: 6| Step: 5
Training loss: 2.5184102058410645
Validation loss: 2.066495942813094

Epoch: 6| Step: 6
Training loss: 2.70634388923645
Validation loss: 2.0735566744240383

Epoch: 6| Step: 7
Training loss: 2.596644401550293
Validation loss: 2.0772523854368474

Epoch: 6| Step: 8
Training loss: 2.236873149871826
Validation loss: 2.0692204198529645

Epoch: 6| Step: 9
Training loss: 1.333299160003662
Validation loss: 2.068515593005765

Epoch: 6| Step: 10
Training loss: 2.082840919494629
Validation loss: 2.047533227551368

Epoch: 6| Step: 11
Training loss: 1.7067062854766846
Validation loss: 2.0514640872196486

Epoch: 6| Step: 12
Training loss: 2.3843986988067627
Validation loss: 2.067202332199261

Epoch: 6| Step: 13
Training loss: 2.219963550567627
Validation loss: 2.0626227214772213

Epoch: 97| Step: 0
Training loss: 2.3960039615631104
Validation loss: 2.074681420480051

Epoch: 6| Step: 1
Training loss: 2.7569994926452637
Validation loss: 2.0797139367749615

Epoch: 6| Step: 2
Training loss: 1.8017854690551758
Validation loss: 2.0894500414530435

Epoch: 6| Step: 3
Training loss: 2.5757272243499756
Validation loss: 2.0998633048867665

Epoch: 6| Step: 4
Training loss: 2.6365885734558105
Validation loss: 2.119316916311941

Epoch: 6| Step: 5
Training loss: 1.8755760192871094
Validation loss: 2.1589901011477233

Epoch: 6| Step: 6
Training loss: 2.3390650749206543
Validation loss: 2.173377094730254

Epoch: 6| Step: 7
Training loss: 2.4753150939941406
Validation loss: 2.176838799189496

Epoch: 6| Step: 8
Training loss: 3.530730962753296
Validation loss: 2.1696038015427126

Epoch: 6| Step: 9
Training loss: 2.6155881881713867
Validation loss: 2.1316142646215295

Epoch: 6| Step: 10
Training loss: 2.3379855155944824
Validation loss: 2.098856686263956

Epoch: 6| Step: 11
Training loss: 1.7063343524932861
Validation loss: 2.0735719293676396

Epoch: 6| Step: 12
Training loss: 1.7198750972747803
Validation loss: 2.064763198616684

Epoch: 6| Step: 13
Training loss: 2.678079128265381
Validation loss: 2.046231190363566

Epoch: 98| Step: 0
Training loss: 2.653681516647339
Validation loss: 2.041754332921838

Epoch: 6| Step: 1
Training loss: 2.0987391471862793
Validation loss: 2.053023426763473

Epoch: 6| Step: 2
Training loss: 2.1989328861236572
Validation loss: 2.053129200012453

Epoch: 6| Step: 3
Training loss: 2.7890563011169434
Validation loss: 2.056423330819735

Epoch: 6| Step: 4
Training loss: 2.4988110065460205
Validation loss: 2.0634510337665515

Epoch: 6| Step: 5
Training loss: 2.2609565258026123
Validation loss: 2.074753321627135

Epoch: 6| Step: 6
Training loss: 3.138303756713867
Validation loss: 2.09368823933345

Epoch: 6| Step: 7
Training loss: 2.335102081298828
Validation loss: 2.0937908669953704

Epoch: 6| Step: 8
Training loss: 2.5174832344055176
Validation loss: 2.0998965860694967

Epoch: 6| Step: 9
Training loss: 2.2495594024658203
Validation loss: 2.0769179969705562

Epoch: 6| Step: 10
Training loss: 2.2295193672180176
Validation loss: 2.0737030941952943

Epoch: 6| Step: 11
Training loss: 1.8748931884765625
Validation loss: 2.074149142029465

Epoch: 6| Step: 12
Training loss: 1.5440125465393066
Validation loss: 2.0614136777898318

Epoch: 6| Step: 13
Training loss: 2.821929454803467
Validation loss: 2.055840999849381

Epoch: 99| Step: 0
Training loss: 2.1854639053344727
Validation loss: 2.05441019868338

Epoch: 6| Step: 1
Training loss: 2.623770236968994
Validation loss: 2.0566527330747215

Epoch: 6| Step: 2
Training loss: 2.8520681858062744
Validation loss: 2.06143166301071

Epoch: 6| Step: 3
Training loss: 1.7674356698989868
Validation loss: 2.0726529885363836

Epoch: 6| Step: 4
Training loss: 2.707671642303467
Validation loss: 2.068206597399968

Epoch: 6| Step: 5
Training loss: 2.535337209701538
Validation loss: 2.0672693688382386

Epoch: 6| Step: 6
Training loss: 2.553680419921875
Validation loss: 2.081852969302926

Epoch: 6| Step: 7
Training loss: 2.0297648906707764
Validation loss: 2.0848884044154996

Epoch: 6| Step: 8
Training loss: 2.6411209106445312
Validation loss: 2.0810144511602258

Epoch: 6| Step: 9
Training loss: 2.073953151702881
Validation loss: 2.0899111891305573

Epoch: 6| Step: 10
Training loss: 2.113016128540039
Validation loss: 2.102614488652957

Epoch: 6| Step: 11
Training loss: 2.312790632247925
Validation loss: 2.1047141116152526

Epoch: 6| Step: 12
Training loss: 2.3152008056640625
Validation loss: 2.103964246729369

Epoch: 6| Step: 13
Training loss: 1.9286068677902222
Validation loss: 2.1069397080329155

Epoch: 100| Step: 0
Training loss: 2.3113677501678467
Validation loss: 2.1214388109022573

Epoch: 6| Step: 1
Training loss: 2.3235812187194824
Validation loss: 2.1428760790055796

Epoch: 6| Step: 2
Training loss: 2.9007325172424316
Validation loss: 2.1590630828693347

Epoch: 6| Step: 3
Training loss: 1.9885272979736328
Validation loss: 2.170134652045465

Epoch: 6| Step: 4
Training loss: 2.6244049072265625
Validation loss: 2.1924177856855493

Epoch: 6| Step: 5
Training loss: 2.5054874420166016
Validation loss: 2.206860714061286

Epoch: 6| Step: 6
Training loss: 1.936280608177185
Validation loss: 2.181699788698586

Epoch: 6| Step: 7
Training loss: 2.1977357864379883
Validation loss: 2.1890797986779162

Epoch: 6| Step: 8
Training loss: 2.550905227661133
Validation loss: 2.182196386398808

Epoch: 6| Step: 9
Training loss: 2.368990421295166
Validation loss: 2.172245625526674

Epoch: 6| Step: 10
Training loss: 2.9051053524017334
Validation loss: 2.1661350906536145

Epoch: 6| Step: 11
Training loss: 2.2729034423828125
Validation loss: 2.1442145109176636

Epoch: 6| Step: 12
Training loss: 1.9132040739059448
Validation loss: 2.13749615094995

Epoch: 6| Step: 13
Training loss: 2.149096965789795
Validation loss: 2.12245435355812

Epoch: 101| Step: 0
Training loss: 2.558929920196533
Validation loss: 2.0942899219451414

Epoch: 6| Step: 1
Training loss: 2.063890218734741
Validation loss: 2.10219100598366

Epoch: 6| Step: 2
Training loss: 1.7038609981536865
Validation loss: 2.1328574252384964

Epoch: 6| Step: 3
Training loss: 2.4698402881622314
Validation loss: 2.1657901912607174

Epoch: 6| Step: 4
Training loss: 2.498690605163574
Validation loss: 2.1917380645710933

Epoch: 6| Step: 5
Training loss: 2.362464427947998
Validation loss: 2.240122566940964

Epoch: 6| Step: 6
Training loss: 2.537501573562622
Validation loss: 2.2545382745804323

Epoch: 6| Step: 7
Training loss: 2.985928535461426
Validation loss: 2.2367034548072406

Epoch: 6| Step: 8
Training loss: 2.6484267711639404
Validation loss: 2.1574622790018716

Epoch: 6| Step: 9
Training loss: 2.128502368927002
Validation loss: 2.05154094132044

Epoch: 6| Step: 10
Training loss: 1.6420953273773193
Validation loss: 2.0246364173068794

Epoch: 6| Step: 11
Training loss: 2.4980828762054443
Validation loss: 2.010691047996603

Epoch: 6| Step: 12
Training loss: 2.772123336791992
Validation loss: 2.032161469100624

Epoch: 6| Step: 13
Training loss: 2.9054312705993652
Validation loss: 2.0715110750608545

Epoch: 102| Step: 0
Training loss: 2.5027856826782227
Validation loss: 2.0979704510781074

Epoch: 6| Step: 1
Training loss: 2.3343758583068848
Validation loss: 2.1114346032501548

Epoch: 6| Step: 2
Training loss: 2.5338821411132812
Validation loss: 2.1292757962339666

Epoch: 6| Step: 3
Training loss: 2.3384432792663574
Validation loss: 2.133880933125814

Epoch: 6| Step: 4
Training loss: 2.9257965087890625
Validation loss: 2.134940296091059

Epoch: 6| Step: 5
Training loss: 2.3813812732696533
Validation loss: 2.13027100921959

Epoch: 6| Step: 6
Training loss: 2.161400318145752
Validation loss: 2.1136104278667

Epoch: 6| Step: 7
Training loss: 2.351555824279785
Validation loss: 2.1091417779204664

Epoch: 6| Step: 8
Training loss: 2.0422239303588867
Validation loss: 2.0978595595205984

Epoch: 6| Step: 9
Training loss: 2.7997536659240723
Validation loss: 2.1180474578693347

Epoch: 6| Step: 10
Training loss: 2.8880419731140137
Validation loss: 2.1196346359868206

Epoch: 6| Step: 11
Training loss: 2.736220359802246
Validation loss: 2.093655568297191

Epoch: 6| Step: 12
Training loss: 1.822824478149414
Validation loss: 2.0917761454018216

Epoch: 6| Step: 13
Training loss: 2.7836666107177734
Validation loss: 2.0801059712645826

Epoch: 103| Step: 0
Training loss: 2.668621063232422
Validation loss: 2.087215429993086

Epoch: 6| Step: 1
Training loss: 2.753814697265625
Validation loss: 2.099594951957785

Epoch: 6| Step: 2
Training loss: 2.023620128631592
Validation loss: 2.0890377657387846

Epoch: 6| Step: 3
Training loss: 1.9289324283599854
Validation loss: 2.083362676764047

Epoch: 6| Step: 4
Training loss: 2.1699137687683105
Validation loss: 2.076780919105776

Epoch: 6| Step: 5
Training loss: 2.483515739440918
Validation loss: 2.076456569856213

Epoch: 6| Step: 6
Training loss: 2.5205931663513184
Validation loss: 2.08602879124303

Epoch: 6| Step: 7
Training loss: 2.918337821960449
Validation loss: 2.1329447454021824

Epoch: 6| Step: 8
Training loss: 2.413544178009033
Validation loss: 2.1564895388900593

Epoch: 6| Step: 9
Training loss: 2.63491153717041
Validation loss: 2.18438381789833

Epoch: 6| Step: 10
Training loss: 2.615746259689331
Validation loss: 2.1735854712865685

Epoch: 6| Step: 11
Training loss: 2.248401641845703
Validation loss: 2.148239435688142

Epoch: 6| Step: 12
Training loss: 1.2913326025009155
Validation loss: 2.1070984845520346

Epoch: 6| Step: 13
Training loss: 2.4910125732421875
Validation loss: 2.0722131113852225

Epoch: 104| Step: 0
Training loss: 1.5904927253723145
Validation loss: 2.0766893971350884

Epoch: 6| Step: 1
Training loss: 2.1900196075439453
Validation loss: 2.0940789432935816

Epoch: 6| Step: 2
Training loss: 2.361319065093994
Validation loss: 2.079360015930668

Epoch: 6| Step: 3
Training loss: 3.134488582611084
Validation loss: 2.073249924567438

Epoch: 6| Step: 4
Training loss: 2.539900302886963
Validation loss: 2.0630677541097007

Epoch: 6| Step: 5
Training loss: 2.932413339614868
Validation loss: 2.0671032282613937

Epoch: 6| Step: 6
Training loss: 2.7802958488464355
Validation loss: 2.0415074389467955

Epoch: 6| Step: 7
Training loss: 1.9760441780090332
Validation loss: 2.02880383435116

Epoch: 6| Step: 8
Training loss: 2.177093505859375
Validation loss: 2.0266088183208177

Epoch: 6| Step: 9
Training loss: 2.0154666900634766
Validation loss: 2.0290381370052213

Epoch: 6| Step: 10
Training loss: 2.9051523208618164
Validation loss: 2.0255792756234445

Epoch: 6| Step: 11
Training loss: 2.267702579498291
Validation loss: 2.0128808354818695

Epoch: 6| Step: 12
Training loss: 2.3258681297302246
Validation loss: 2.0097372249890397

Epoch: 6| Step: 13
Training loss: 0.9379479885101318
Validation loss: 2.0095780972511537

Epoch: 105| Step: 0
Training loss: 2.3342647552490234
Validation loss: 2.0134742131797214

Epoch: 6| Step: 1
Training loss: 2.2859954833984375
Validation loss: 2.0151089340127926

Epoch: 6| Step: 2
Training loss: 2.4539780616760254
Validation loss: 2.020174676372159

Epoch: 6| Step: 3
Training loss: 1.7399358749389648
Validation loss: 2.020098823373036

Epoch: 6| Step: 4
Training loss: 3.0339107513427734
Validation loss: 2.0344305910089964

Epoch: 6| Step: 5
Training loss: 2.1126198768615723
Validation loss: 2.050804030510687

Epoch: 6| Step: 6
Training loss: 1.9259288311004639
Validation loss: 2.0617818576033398

Epoch: 6| Step: 7
Training loss: 2.33544659614563
Validation loss: 2.119695157133123

Epoch: 6| Step: 8
Training loss: 2.4682905673980713
Validation loss: 2.157558539862274

Epoch: 6| Step: 9
Training loss: 2.839592456817627
Validation loss: 2.1608171411739883

Epoch: 6| Step: 10
Training loss: 2.8235127925872803
Validation loss: 2.169269461785593

Epoch: 6| Step: 11
Training loss: 2.476041316986084
Validation loss: 2.1698682641470306

Epoch: 6| Step: 12
Training loss: 1.856684923171997
Validation loss: 2.144216952785369

Epoch: 6| Step: 13
Training loss: 2.4625422954559326
Validation loss: 2.1050619822676464

Epoch: 106| Step: 0
Training loss: 2.3984460830688477
Validation loss: 2.083043280468192

Epoch: 6| Step: 1
Training loss: 1.7349275350570679
Validation loss: 2.0282625562401226

Epoch: 6| Step: 2
Training loss: 2.3745923042297363
Validation loss: 2.0291360603865756

Epoch: 6| Step: 3
Training loss: 2.325174331665039
Validation loss: 2.025012759752171

Epoch: 6| Step: 4
Training loss: 2.0086145401000977
Validation loss: 2.0360026974831857

Epoch: 6| Step: 5
Training loss: 2.112574577331543
Validation loss: 2.022100015353131

Epoch: 6| Step: 6
Training loss: 2.59860897064209
Validation loss: 2.004343425073931

Epoch: 6| Step: 7
Training loss: 2.0779881477355957
Validation loss: 2.0189927265208256

Epoch: 6| Step: 8
Training loss: 2.6681649684906006
Validation loss: 2.030519208600444

Epoch: 6| Step: 9
Training loss: 1.6546275615692139
Validation loss: 2.028942285045501

Epoch: 6| Step: 10
Training loss: 2.632274866104126
Validation loss: 2.0409682361028527

Epoch: 6| Step: 11
Training loss: 2.6584107875823975
Validation loss: 2.0345691045125327

Epoch: 6| Step: 12
Training loss: 2.7173428535461426
Validation loss: 2.054187067093388

Epoch: 6| Step: 13
Training loss: 2.4833874702453613
Validation loss: 2.058783613225465

Epoch: 107| Step: 0
Training loss: 2.8857550621032715
Validation loss: 2.053113583595522

Epoch: 6| Step: 1
Training loss: 2.0289742946624756
Validation loss: 2.047242590176162

Epoch: 6| Step: 2
Training loss: 2.0775201320648193
Validation loss: 2.048329681478521

Epoch: 6| Step: 3
Training loss: 2.3851165771484375
Validation loss: 2.054131894983271

Epoch: 6| Step: 4
Training loss: 2.252861261367798
Validation loss: 2.059994441206737

Epoch: 6| Step: 5
Training loss: 2.7514829635620117
Validation loss: 2.077424218577723

Epoch: 6| Step: 6
Training loss: 2.667152166366577
Validation loss: 2.072567219375282

Epoch: 6| Step: 7
Training loss: 1.879015564918518
Validation loss: 2.063498200908784

Epoch: 6| Step: 8
Training loss: 2.0116686820983887
Validation loss: 2.0520162326033398

Epoch: 6| Step: 9
Training loss: 2.3134140968322754
Validation loss: 2.0425628064781107

Epoch: 6| Step: 10
Training loss: 2.620561122894287
Validation loss: 2.035672874860866

Epoch: 6| Step: 11
Training loss: 1.9067811965942383
Validation loss: 2.026156658767372

Epoch: 6| Step: 12
Training loss: 2.312981128692627
Validation loss: 2.0357743693936254

Epoch: 6| Step: 13
Training loss: 1.6324331760406494
Validation loss: 2.0247043948019705

Epoch: 108| Step: 0
Training loss: 2.3625783920288086
Validation loss: 2.0296661212880123

Epoch: 6| Step: 1
Training loss: 1.9984420537948608
Validation loss: 2.0519838179311445

Epoch: 6| Step: 2
Training loss: 2.980374813079834
Validation loss: 2.0569093791387414

Epoch: 6| Step: 3
Training loss: 2.6951165199279785
Validation loss: 2.0575783278352473

Epoch: 6| Step: 4
Training loss: 2.252720594406128
Validation loss: 2.0601475777164584

Epoch: 6| Step: 5
Training loss: 2.1858904361724854
Validation loss: 2.048193952088715

Epoch: 6| Step: 6
Training loss: 2.402587890625
Validation loss: 2.0229872772770543

Epoch: 6| Step: 7
Training loss: 2.012417793273926
Validation loss: 2.0210794761616695

Epoch: 6| Step: 8
Training loss: 2.564753293991089
Validation loss: 2.0260194181114115

Epoch: 6| Step: 9
Training loss: 1.9332778453826904
Validation loss: 2.0276736649133826

Epoch: 6| Step: 10
Training loss: 1.751132845878601
Validation loss: 2.0333553462900142

Epoch: 6| Step: 11
Training loss: 1.98514986038208
Validation loss: 2.035834013774831

Epoch: 6| Step: 12
Training loss: 2.473877429962158
Validation loss: 2.0522840638314523

Epoch: 6| Step: 13
Training loss: 2.8178951740264893
Validation loss: 2.041020800990443

Epoch: 109| Step: 0
Training loss: 2.3547990322113037
Validation loss: 2.04041709438447

Epoch: 6| Step: 1
Training loss: 2.143282651901245
Validation loss: 2.0231196598340104

Epoch: 6| Step: 2
Training loss: 3.4236817359924316
Validation loss: 2.0292661907852336

Epoch: 6| Step: 3
Training loss: 2.078146457672119
Validation loss: 2.037058448278776

Epoch: 6| Step: 4
Training loss: 2.6429784297943115
Validation loss: 2.0409520415849585

Epoch: 6| Step: 5
Training loss: 1.9385647773742676
Validation loss: 2.0462296624337473

Epoch: 6| Step: 6
Training loss: 2.4206485748291016
Validation loss: 2.0426959324908514

Epoch: 6| Step: 7
Training loss: 2.556680679321289
Validation loss: 2.033718419331376

Epoch: 6| Step: 8
Training loss: 1.4339990615844727
Validation loss: 2.0369702898046023

Epoch: 6| Step: 9
Training loss: 3.0842556953430176
Validation loss: 2.041995156195856

Epoch: 6| Step: 10
Training loss: 2.1133761405944824
Validation loss: 2.031784144780969

Epoch: 6| Step: 11
Training loss: 1.5280182361602783
Validation loss: 2.0443590558985227

Epoch: 6| Step: 12
Training loss: 1.8903489112854004
Validation loss: 2.0508201763194096

Epoch: 6| Step: 13
Training loss: 2.353156089782715
Validation loss: 2.0594580506765716

Epoch: 110| Step: 0
Training loss: 1.7865657806396484
Validation loss: 2.073047927630845

Epoch: 6| Step: 1
Training loss: 2.1423468589782715
Validation loss: 2.0777131818955943

Epoch: 6| Step: 2
Training loss: 2.595365524291992
Validation loss: 2.077813486899099

Epoch: 6| Step: 3
Training loss: 2.2173874378204346
Validation loss: 2.0738211472829184

Epoch: 6| Step: 4
Training loss: 2.0948729515075684
Validation loss: 2.082320551718435

Epoch: 6| Step: 5
Training loss: 2.5526623725891113
Validation loss: 2.0709243820559595

Epoch: 6| Step: 6
Training loss: 2.4525365829467773
Validation loss: 2.0493688711556057

Epoch: 6| Step: 7
Training loss: 2.767404556274414
Validation loss: 2.0370028262497275

Epoch: 6| Step: 8
Training loss: 2.0140888690948486
Validation loss: 2.0339253897308023

Epoch: 6| Step: 9
Training loss: 1.9344961643218994
Validation loss: 2.0297850690862185

Epoch: 6| Step: 10
Training loss: 2.438389301300049
Validation loss: 2.0197856721057685

Epoch: 6| Step: 11
Training loss: 2.0914902687072754
Validation loss: 2.035972761851485

Epoch: 6| Step: 12
Training loss: 2.3735036849975586
Validation loss: 2.0232445501512095

Epoch: 6| Step: 13
Training loss: 2.872892379760742
Validation loss: 2.0221187299297703

Epoch: 111| Step: 0
Training loss: 2.1199862957000732
Validation loss: 2.026852910236646

Epoch: 6| Step: 1
Training loss: 2.337599754333496
Validation loss: 2.018601353450488

Epoch: 6| Step: 2
Training loss: 2.289856433868408
Validation loss: 2.0203390582915275

Epoch: 6| Step: 3
Training loss: 1.9152594804763794
Validation loss: 2.0203070845655215

Epoch: 6| Step: 4
Training loss: 1.5635533332824707
Validation loss: 2.0300150302148636

Epoch: 6| Step: 5
Training loss: 2.769130229949951
Validation loss: 2.038551233148062

Epoch: 6| Step: 6
Training loss: 2.3063719272613525
Validation loss: 2.0352168826646704

Epoch: 6| Step: 7
Training loss: 2.4625468254089355
Validation loss: 2.0549064938740065

Epoch: 6| Step: 8
Training loss: 2.410537004470825
Validation loss: 2.0541233157598846

Epoch: 6| Step: 9
Training loss: 2.9875636100769043
Validation loss: 2.069704322404759

Epoch: 6| Step: 10
Training loss: 2.5411436557769775
Validation loss: 2.083813798043036

Epoch: 6| Step: 11
Training loss: 2.0117928981781006
Validation loss: 2.097323251026933

Epoch: 6| Step: 12
Training loss: 2.13065767288208
Validation loss: 2.094526852330854

Epoch: 6| Step: 13
Training loss: 2.0308783054351807
Validation loss: 2.0982219096153014

Epoch: 112| Step: 0
Training loss: 2.7897801399230957
Validation loss: 2.100546585616245

Epoch: 6| Step: 1
Training loss: 2.79613995552063
Validation loss: 2.1081084999986874

Epoch: 6| Step: 2
Training loss: 2.1102333068847656
Validation loss: 2.0990518600709978

Epoch: 6| Step: 3
Training loss: 2.7809267044067383
Validation loss: 2.084499816740713

Epoch: 6| Step: 4
Training loss: 1.9402766227722168
Validation loss: 2.053414547315208

Epoch: 6| Step: 5
Training loss: 2.2211523056030273
Validation loss: 2.0167301944507066

Epoch: 6| Step: 6
Training loss: 1.8486864566802979
Validation loss: 2.001236927124762

Epoch: 6| Step: 7
Training loss: 1.6040843725204468
Validation loss: 1.9914054075876872

Epoch: 6| Step: 8
Training loss: 2.9226808547973633
Validation loss: 1.9863735809121081

Epoch: 6| Step: 9
Training loss: 2.2831978797912598
Validation loss: 1.9950552717331917

Epoch: 6| Step: 10
Training loss: 2.1742053031921387
Validation loss: 2.000407131769324

Epoch: 6| Step: 11
Training loss: 2.2478721141815186
Validation loss: 2.0092547016759075

Epoch: 6| Step: 12
Training loss: 1.832289218902588
Validation loss: 2.017174465681917

Epoch: 6| Step: 13
Training loss: 2.560943365097046
Validation loss: 2.015945433288492

Epoch: 113| Step: 0
Training loss: 1.6904464960098267
Validation loss: 2.0276281602921022

Epoch: 6| Step: 1
Training loss: 2.2594034671783447
Validation loss: 2.0524525104030484

Epoch: 6| Step: 2
Training loss: 1.6656339168548584
Validation loss: 2.076238875748009

Epoch: 6| Step: 3
Training loss: 1.7409486770629883
Validation loss: 2.1028562489376275

Epoch: 6| Step: 4
Training loss: 1.8708447217941284
Validation loss: 2.1326008432654926

Epoch: 6| Step: 5
Training loss: 3.107150077819824
Validation loss: 2.171632556505101

Epoch: 6| Step: 6
Training loss: 2.555518627166748
Validation loss: 2.157623214106406

Epoch: 6| Step: 7
Training loss: 2.5223069190979004
Validation loss: 2.179073105576218

Epoch: 6| Step: 8
Training loss: 2.067836284637451
Validation loss: 2.168704696880874

Epoch: 6| Step: 9
Training loss: 2.112051486968994
Validation loss: 2.1715682988525717

Epoch: 6| Step: 10
Training loss: 2.7606759071350098
Validation loss: 2.175420302216725

Epoch: 6| Step: 11
Training loss: 2.601745128631592
Validation loss: 2.154119424922492

Epoch: 6| Step: 12
Training loss: 2.763720989227295
Validation loss: 2.1282212785495225

Epoch: 6| Step: 13
Training loss: 2.1205673217773438
Validation loss: 2.1008968327635076

Epoch: 114| Step: 0
Training loss: 2.545964241027832
Validation loss: 2.069314269609349

Epoch: 6| Step: 1
Training loss: 2.4111104011535645
Validation loss: 2.0484877555601058

Epoch: 6| Step: 2
Training loss: 2.474332809448242
Validation loss: 2.0507539985000447

Epoch: 6| Step: 3
Training loss: 1.85704505443573
Validation loss: 2.0606825274805867

Epoch: 6| Step: 4
Training loss: 1.788548231124878
Validation loss: 2.0495345310498307

Epoch: 6| Step: 5
Training loss: 2.5960750579833984
Validation loss: 2.0392135343244

Epoch: 6| Step: 6
Training loss: 2.1752476692199707
Validation loss: 2.028173380000617

Epoch: 6| Step: 7
Training loss: 3.0764880180358887
Validation loss: 2.0000715460828555

Epoch: 6| Step: 8
Training loss: 2.134434223175049
Validation loss: 1.9760386174724949

Epoch: 6| Step: 9
Training loss: 2.6529831886291504
Validation loss: 2.003148768537788

Epoch: 6| Step: 10
Training loss: 2.614292621612549
Validation loss: 2.016942083194692

Epoch: 6| Step: 11
Training loss: 2.07983660697937
Validation loss: 2.0540591273256528

Epoch: 6| Step: 12
Training loss: 1.9180359840393066
Validation loss: 2.094705391955632

Epoch: 6| Step: 13
Training loss: 1.730277180671692
Validation loss: 2.1152125943091606

Epoch: 115| Step: 0
Training loss: 1.8681060075759888
Validation loss: 2.1473652880678893

Epoch: 6| Step: 1
Training loss: 2.0949504375457764
Validation loss: 2.148773095941031

Epoch: 6| Step: 2
Training loss: 1.5735938549041748
Validation loss: 2.168306951881737

Epoch: 6| Step: 3
Training loss: 2.7710041999816895
Validation loss: 2.15163520843752

Epoch: 6| Step: 4
Training loss: 2.7059736251831055
Validation loss: 2.1264949998547955

Epoch: 6| Step: 5
Training loss: 1.9896118640899658
Validation loss: 2.0717023290613645

Epoch: 6| Step: 6
Training loss: 1.8935269117355347
Validation loss: 2.044338300663938

Epoch: 6| Step: 7
Training loss: 2.498650550842285
Validation loss: 2.013468709043277

Epoch: 6| Step: 8
Training loss: 2.45033597946167
Validation loss: 2.0007114666764454

Epoch: 6| Step: 9
Training loss: 2.495742082595825
Validation loss: 1.9967718419208322

Epoch: 6| Step: 10
Training loss: 2.3008203506469727
Validation loss: 1.9925254557722358

Epoch: 6| Step: 11
Training loss: 2.4255521297454834
Validation loss: 1.9896344856549335

Epoch: 6| Step: 12
Training loss: 2.5522446632385254
Validation loss: 1.9810558877965456

Epoch: 6| Step: 13
Training loss: 2.8040246963500977
Validation loss: 1.9968586198745235

Epoch: 116| Step: 0
Training loss: 2.1805930137634277
Validation loss: 1.9955376117460188

Epoch: 6| Step: 1
Training loss: 1.7851903438568115
Validation loss: 1.9992526705547045

Epoch: 6| Step: 2
Training loss: 2.4944729804992676
Validation loss: 2.0186569280521844

Epoch: 6| Step: 3
Training loss: 2.415462017059326
Validation loss: 2.0247962872187295

Epoch: 6| Step: 4
Training loss: 2.080631732940674
Validation loss: 2.0256738688356135

Epoch: 6| Step: 5
Training loss: 1.8160182237625122
Validation loss: 2.0290951421183925

Epoch: 6| Step: 6
Training loss: 2.5457677841186523
Validation loss: 2.0721411538380448

Epoch: 6| Step: 7
Training loss: 2.5013270378112793
Validation loss: 2.1172034202083463

Epoch: 6| Step: 8
Training loss: 2.363180637359619
Validation loss: 2.1534308848842496

Epoch: 6| Step: 9
Training loss: 2.361135959625244
Validation loss: 2.192031047677481

Epoch: 6| Step: 10
Training loss: 3.2150721549987793
Validation loss: 2.172825123674126

Epoch: 6| Step: 11
Training loss: 2.0198378562927246
Validation loss: 2.1701418686938543

Epoch: 6| Step: 12
Training loss: 1.8649182319641113
Validation loss: 2.15361258035065

Epoch: 6| Step: 13
Training loss: 3.0445170402526855
Validation loss: 2.1199943814226376

Epoch: 117| Step: 0
Training loss: 2.6613759994506836
Validation loss: 2.095307760341193

Epoch: 6| Step: 1
Training loss: 2.7287778854370117
Validation loss: 2.0643149819425357

Epoch: 6| Step: 2
Training loss: 1.1802361011505127
Validation loss: 2.056028204579507

Epoch: 6| Step: 3
Training loss: 2.6358578205108643
Validation loss: 2.0546477866429154

Epoch: 6| Step: 4
Training loss: 2.3723936080932617
Validation loss: 2.070426643535655

Epoch: 6| Step: 5
Training loss: 1.8583447933197021
Validation loss: 2.0498890364041893

Epoch: 6| Step: 6
Training loss: 2.1208086013793945
Validation loss: 2.063823602532828

Epoch: 6| Step: 7
Training loss: 1.579026699066162
Validation loss: 2.0748476366842947

Epoch: 6| Step: 8
Training loss: 2.8091115951538086
Validation loss: 2.078205636752549

Epoch: 6| Step: 9
Training loss: 2.5212411880493164
Validation loss: 2.0859315369718816

Epoch: 6| Step: 10
Training loss: 2.054033041000366
Validation loss: 2.092680943909512

Epoch: 6| Step: 11
Training loss: 3.1029105186462402
Validation loss: 2.071147982792188

Epoch: 6| Step: 12
Training loss: 2.142678737640381
Validation loss: 2.060333331425985

Epoch: 6| Step: 13
Training loss: 1.581352949142456
Validation loss: 2.03976184321988

Epoch: 118| Step: 0
Training loss: 1.8135212659835815
Validation loss: 2.0329164407586537

Epoch: 6| Step: 1
Training loss: 2.145536422729492
Validation loss: 2.0469841162363687

Epoch: 6| Step: 2
Training loss: 1.9945905208587646
Validation loss: 2.0513169586017566

Epoch: 6| Step: 3
Training loss: 2.0185446739196777
Validation loss: 2.0705781828972603

Epoch: 6| Step: 4
Training loss: 1.8433713912963867
Validation loss: 2.0723270985387985

Epoch: 6| Step: 5
Training loss: 2.369504928588867
Validation loss: 2.0661103058886785

Epoch: 6| Step: 6
Training loss: 3.5211501121520996
Validation loss: 2.0685210945785686

Epoch: 6| Step: 7
Training loss: 1.7458596229553223
Validation loss: 2.0463948224180486

Epoch: 6| Step: 8
Training loss: 2.044942617416382
Validation loss: 2.0428646815720426

Epoch: 6| Step: 9
Training loss: 2.4637107849121094
Validation loss: 2.0430661452713834

Epoch: 6| Step: 10
Training loss: 2.268613338470459
Validation loss: 2.049090705892091

Epoch: 6| Step: 11
Training loss: 2.079829216003418
Validation loss: 2.034180994956724

Epoch: 6| Step: 12
Training loss: 1.9511826038360596
Validation loss: 2.0297061627910984

Epoch: 6| Step: 13
Training loss: 3.5529367923736572
Validation loss: 2.036778588448801

Epoch: 119| Step: 0
Training loss: 2.369614839553833
Validation loss: 2.0702654046397053

Epoch: 6| Step: 1
Training loss: 2.6247811317443848
Validation loss: 2.105140145106982

Epoch: 6| Step: 2
Training loss: 1.9316550493240356
Validation loss: 2.064021033625449

Epoch: 6| Step: 3
Training loss: 1.9097700119018555
Validation loss: 2.0245145572129117

Epoch: 6| Step: 4
Training loss: 1.9602477550506592
Validation loss: 2.0208754808672014

Epoch: 6| Step: 5
Training loss: 1.5836219787597656
Validation loss: 1.9904621903614332

Epoch: 6| Step: 6
Training loss: 2.036656379699707
Validation loss: 1.9861663503031577

Epoch: 6| Step: 7
Training loss: 2.3386361598968506
Validation loss: 1.9792444680326728

Epoch: 6| Step: 8
Training loss: 2.1994104385375977
Validation loss: 1.9925550235215055

Epoch: 6| Step: 9
Training loss: 2.1225626468658447
Validation loss: 2.0092555258863714

Epoch: 6| Step: 10
Training loss: 1.8426998853683472
Validation loss: 2.0338076494073354

Epoch: 6| Step: 11
Training loss: 3.010326862335205
Validation loss: 2.059276356491991

Epoch: 6| Step: 12
Training loss: 3.1362991333007812
Validation loss: 2.075095858625186

Epoch: 6| Step: 13
Training loss: 1.901216745376587
Validation loss: 2.068070355282035

Epoch: 120| Step: 0
Training loss: 2.323180675506592
Validation loss: 2.101733043629636

Epoch: 6| Step: 1
Training loss: 1.8273876905441284
Validation loss: 2.097841698636291

Epoch: 6| Step: 2
Training loss: 1.5804338455200195
Validation loss: 2.0823162242930424

Epoch: 6| Step: 3
Training loss: 2.588127613067627
Validation loss: 2.0624513831189883

Epoch: 6| Step: 4
Training loss: 1.9811760187149048
Validation loss: 2.027145565197032

Epoch: 6| Step: 5
Training loss: 1.282815933227539
Validation loss: 2.0372407692734913

Epoch: 6| Step: 6
Training loss: 2.583193302154541
Validation loss: 2.042962604953397

Epoch: 6| Step: 7
Training loss: 2.0797982215881348
Validation loss: 2.0705464116988646

Epoch: 6| Step: 8
Training loss: 3.4347732067108154
Validation loss: 2.079677692023657

Epoch: 6| Step: 9
Training loss: 1.6243791580200195
Validation loss: 2.091683946630006

Epoch: 6| Step: 10
Training loss: 2.7125630378723145
Validation loss: 2.091739252049436

Epoch: 6| Step: 11
Training loss: 2.6216189861297607
Validation loss: 2.106146397129182

Epoch: 6| Step: 12
Training loss: 2.0770535469055176
Validation loss: 2.1008391034218574

Epoch: 6| Step: 13
Training loss: 1.4139994382858276
Validation loss: 2.0792904156510548

Epoch: 121| Step: 0
Training loss: 1.9545313119888306
Validation loss: 2.0277524507173927

Epoch: 6| Step: 1
Training loss: 2.767237901687622
Validation loss: 2.0195701840103313

Epoch: 6| Step: 2
Training loss: 2.0325980186462402
Validation loss: 2.0341190779080955

Epoch: 6| Step: 3
Training loss: 2.3328962326049805
Validation loss: 2.043052247775498

Epoch: 6| Step: 4
Training loss: 2.494985580444336
Validation loss: 2.0676158935792985

Epoch: 6| Step: 5
Training loss: 2.9864635467529297
Validation loss: 2.0697327557430474

Epoch: 6| Step: 6
Training loss: 2.0823559761047363
Validation loss: 2.0733539007043325

Epoch: 6| Step: 7
Training loss: 1.7327780723571777
Validation loss: 2.0586573359786824

Epoch: 6| Step: 8
Training loss: 1.837416648864746
Validation loss: 2.0237017728949107

Epoch: 6| Step: 9
Training loss: 2.3296236991882324
Validation loss: 2.0153766011679046

Epoch: 6| Step: 10
Training loss: 2.3289313316345215
Validation loss: 1.9940992222037366

Epoch: 6| Step: 11
Training loss: 2.2001166343688965
Validation loss: 1.9902444206258303

Epoch: 6| Step: 12
Training loss: 2.6052188873291016
Validation loss: 2.000588200425589

Epoch: 6| Step: 13
Training loss: 0.9064391255378723
Validation loss: 2.02188697425268

Epoch: 122| Step: 0
Training loss: 1.7996165752410889
Validation loss: 2.0561151837789886

Epoch: 6| Step: 1
Training loss: 2.144923686981201
Validation loss: 2.1059846416596444

Epoch: 6| Step: 2
Training loss: 2.022080898284912
Validation loss: 2.165225730147413

Epoch: 6| Step: 3
Training loss: 2.3653564453125
Validation loss: 2.2179432248556488

Epoch: 6| Step: 4
Training loss: 2.4114737510681152
Validation loss: 2.254546719212686

Epoch: 6| Step: 5
Training loss: 3.030435562133789
Validation loss: 2.268150803863361

Epoch: 6| Step: 6
Training loss: 2.2435836791992188
Validation loss: 2.2432945697538313

Epoch: 6| Step: 7
Training loss: 2.223961591720581
Validation loss: 2.2297364998889226

Epoch: 6| Step: 8
Training loss: 1.8776081800460815
Validation loss: 2.2054879306465067

Epoch: 6| Step: 9
Training loss: 2.387394666671753
Validation loss: 2.193294962247213

Epoch: 6| Step: 10
Training loss: 2.6799798011779785
Validation loss: 2.170666161403861

Epoch: 6| Step: 11
Training loss: 2.3058242797851562
Validation loss: 2.1584409180507866

Epoch: 6| Step: 12
Training loss: 1.772918701171875
Validation loss: 2.1104429793614212

Epoch: 6| Step: 13
Training loss: 1.6463731527328491
Validation loss: 2.06946119441781

Epoch: 123| Step: 0
Training loss: 2.3463263511657715
Validation loss: 2.028790254746714

Epoch: 6| Step: 1
Training loss: 2.5069687366485596
Validation loss: 2.015972806561378

Epoch: 6| Step: 2
Training loss: 2.5071370601654053
Validation loss: 2.012158418214449

Epoch: 6| Step: 3
Training loss: 2.1104016304016113
Validation loss: 2.0126325814954695

Epoch: 6| Step: 4
Training loss: 1.8261302709579468
Validation loss: 2.0207886695861816

Epoch: 6| Step: 5
Training loss: 2.636610984802246
Validation loss: 2.021783249352568

Epoch: 6| Step: 6
Training loss: 1.957931399345398
Validation loss: 2.021209087423099

Epoch: 6| Step: 7
Training loss: 1.3551990985870361
Validation loss: 2.0197884921104676

Epoch: 6| Step: 8
Training loss: 2.407855749130249
Validation loss: 2.0114274050599787

Epoch: 6| Step: 9
Training loss: 1.9051014184951782
Validation loss: 2.0030258009510655

Epoch: 6| Step: 10
Training loss: 1.911750316619873
Validation loss: 2.008606861996394

Epoch: 6| Step: 11
Training loss: 2.33951997756958
Validation loss: 2.0083579376179683

Epoch: 6| Step: 12
Training loss: 1.8307737112045288
Validation loss: 2.0060213599153744

Epoch: 6| Step: 13
Training loss: 2.2861578464508057
Validation loss: 2.010132138447095

Epoch: 124| Step: 0
Training loss: 1.7412817478179932
Validation loss: 2.038585933305884

Epoch: 6| Step: 1
Training loss: 1.7446048259735107
Validation loss: 2.063051562155447

Epoch: 6| Step: 2
Training loss: 2.35842227935791
Validation loss: 2.0745117997610443

Epoch: 6| Step: 3
Training loss: 2.240654945373535
Validation loss: 2.0753576627341648

Epoch: 6| Step: 4
Training loss: 2.795494556427002
Validation loss: 2.0840563492108415

Epoch: 6| Step: 5
Training loss: 2.7797141075134277
Validation loss: 2.0796799416183145

Epoch: 6| Step: 6
Training loss: 2.2826266288757324
Validation loss: 2.0681038851379068

Epoch: 6| Step: 7
Training loss: 2.391188144683838
Validation loss: 2.0622036123788483

Epoch: 6| Step: 8
Training loss: 2.553392171859741
Validation loss: 2.0686138086421515

Epoch: 6| Step: 9
Training loss: 1.8510125875473022
Validation loss: 2.062265332027148

Epoch: 6| Step: 10
Training loss: 1.7228541374206543
Validation loss: 2.0574986703934206

Epoch: 6| Step: 11
Training loss: 1.656851053237915
Validation loss: 2.0622995309932257

Epoch: 6| Step: 12
Training loss: 1.5773670673370361
Validation loss: 2.063710146052863

Epoch: 6| Step: 13
Training loss: 2.978569746017456
Validation loss: 2.03822523291393

Epoch: 125| Step: 0
Training loss: 1.6394963264465332
Validation loss: 2.059377677979008

Epoch: 6| Step: 1
Training loss: 2.2187557220458984
Validation loss: 2.064633524546059

Epoch: 6| Step: 2
Training loss: 2.0128891468048096
Validation loss: 2.05399945218076

Epoch: 6| Step: 3
Training loss: 2.253554105758667
Validation loss: 2.0469808437490977

Epoch: 6| Step: 4
Training loss: 2.668290138244629
Validation loss: 2.034002375859086

Epoch: 6| Step: 5
Training loss: 2.5484771728515625
Validation loss: 2.0315748440322055

Epoch: 6| Step: 6
Training loss: 1.8091272115707397
Validation loss: 2.0476827044640817

Epoch: 6| Step: 7
Training loss: 1.9271174669265747
Validation loss: 2.0538778817781838

Epoch: 6| Step: 8
Training loss: 1.872793436050415
Validation loss: 2.044366849366055

Epoch: 6| Step: 9
Training loss: 1.6309295892715454
Validation loss: 2.0538238440790484

Epoch: 6| Step: 10
Training loss: 1.9674065113067627
Validation loss: 2.0833525811472247

Epoch: 6| Step: 11
Training loss: 2.91829776763916
Validation loss: 2.091334624957013

Epoch: 6| Step: 12
Training loss: 1.5896811485290527
Validation loss: 2.091103248698737

Epoch: 6| Step: 13
Training loss: 3.0088212490081787
Validation loss: 2.082813601340017

Testing loss: 2.2329338947931925

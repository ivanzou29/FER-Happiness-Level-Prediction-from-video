Epoch: 1| Step: 0
Training loss: 5.0025715827941895
Validation loss: 5.251740255663472

Epoch: 6| Step: 1
Training loss: 5.700790882110596
Validation loss: 5.221755361044279

Epoch: 6| Step: 2
Training loss: 5.251513481140137
Validation loss: 5.190817945746965

Epoch: 6| Step: 3
Training loss: 3.837606430053711
Validation loss: 5.15624721588627

Epoch: 6| Step: 4
Training loss: 5.103047847747803
Validation loss: 5.117985740784676

Epoch: 6| Step: 5
Training loss: 4.432611465454102
Validation loss: 5.075575374787854

Epoch: 6| Step: 6
Training loss: 5.242006301879883
Validation loss: 5.029099982271912

Epoch: 6| Step: 7
Training loss: 4.527895450592041
Validation loss: 4.9784338602455716

Epoch: 6| Step: 8
Training loss: 5.287594795227051
Validation loss: 4.925357341766357

Epoch: 6| Step: 9
Training loss: 5.418251037597656
Validation loss: 4.868518024362544

Epoch: 6| Step: 10
Training loss: 4.965006351470947
Validation loss: 4.807612214037167

Epoch: 6| Step: 11
Training loss: 4.108670234680176
Validation loss: 4.743902391003024

Epoch: 6| Step: 12
Training loss: 3.7213075160980225
Validation loss: 4.679295652656145

Epoch: 6| Step: 13
Training loss: 4.006709575653076
Validation loss: 4.614190229805567

Epoch: 2| Step: 0
Training loss: 4.173226833343506
Validation loss: 4.545558060369184

Epoch: 6| Step: 1
Training loss: 3.729426622390747
Validation loss: 4.477824047047605

Epoch: 6| Step: 2
Training loss: 5.106527328491211
Validation loss: 4.410250576593542

Epoch: 6| Step: 3
Training loss: 5.438259124755859
Validation loss: 4.34257302745696

Epoch: 6| Step: 4
Training loss: 2.692453384399414
Validation loss: 4.273067812765798

Epoch: 6| Step: 5
Training loss: 3.9331417083740234
Validation loss: 4.207370419656077

Epoch: 6| Step: 6
Training loss: 4.828771591186523
Validation loss: 4.139209203822638

Epoch: 6| Step: 7
Training loss: 3.668659210205078
Validation loss: 4.068786615966468

Epoch: 6| Step: 8
Training loss: 3.3390846252441406
Validation loss: 3.998859561899657

Epoch: 6| Step: 9
Training loss: 3.7206172943115234
Validation loss: 3.9360461619592484

Epoch: 6| Step: 10
Training loss: 2.8106255531311035
Validation loss: 3.879370027972806

Epoch: 6| Step: 11
Training loss: 3.615548849105835
Validation loss: 3.826224491160403

Epoch: 6| Step: 12
Training loss: 4.029709339141846
Validation loss: 3.774113514090097

Epoch: 6| Step: 13
Training loss: 4.580511093139648
Validation loss: 3.7233822627734114

Epoch: 3| Step: 0
Training loss: 3.968721866607666
Validation loss: 3.6746461365812566

Epoch: 6| Step: 1
Training loss: 3.3699800968170166
Validation loss: 3.6439845177435104

Epoch: 6| Step: 2
Training loss: 3.9425251483917236
Validation loss: 3.606352078017368

Epoch: 6| Step: 3
Training loss: 3.852586507797241
Validation loss: 3.5680918103905133

Epoch: 6| Step: 4
Training loss: 4.357351303100586
Validation loss: 3.53708896329326

Epoch: 6| Step: 5
Training loss: 4.145689964294434
Validation loss: 3.504005814111361

Epoch: 6| Step: 6
Training loss: 3.2532098293304443
Validation loss: 3.4704216295673

Epoch: 6| Step: 7
Training loss: 2.7212789058685303
Validation loss: 3.4438077301107426

Epoch: 6| Step: 8
Training loss: 2.3790595531463623
Validation loss: 3.4078512678864183

Epoch: 6| Step: 9
Training loss: 2.5852487087249756
Validation loss: 3.3744761072179323

Epoch: 6| Step: 10
Training loss: 3.0453038215637207
Validation loss: 3.3528524778222524

Epoch: 6| Step: 11
Training loss: 3.119755744934082
Validation loss: 3.3223510967787875

Epoch: 6| Step: 12
Training loss: 3.6311724185943604
Validation loss: 3.299410145769837

Epoch: 6| Step: 13
Training loss: 3.384589195251465
Validation loss: 3.2755117006199335

Epoch: 4| Step: 0
Training loss: 3.4477360248565674
Validation loss: 3.249244938614548

Epoch: 6| Step: 1
Training loss: 3.222599983215332
Validation loss: 3.231077506978025

Epoch: 6| Step: 2
Training loss: 3.134428024291992
Validation loss: 3.2140025528528358

Epoch: 6| Step: 3
Training loss: 3.7016379833221436
Validation loss: 3.1890028574133433

Epoch: 6| Step: 4
Training loss: 2.7722251415252686
Validation loss: 3.1670209156569613

Epoch: 6| Step: 5
Training loss: 2.338524341583252
Validation loss: 3.1479376592943744

Epoch: 6| Step: 6
Training loss: 2.632479190826416
Validation loss: 3.132488466078235

Epoch: 6| Step: 7
Training loss: 3.1386799812316895
Validation loss: 3.118596469202349

Epoch: 6| Step: 8
Training loss: 3.6071219444274902
Validation loss: 3.0962191063870668

Epoch: 6| Step: 9
Training loss: 3.546966075897217
Validation loss: 3.0935939922127673

Epoch: 6| Step: 10
Training loss: 2.537562847137451
Validation loss: 3.072340037233086

Epoch: 6| Step: 11
Training loss: 2.8749213218688965
Validation loss: 3.0607114658560803

Epoch: 6| Step: 12
Training loss: 3.426744222640991
Validation loss: 3.0432024232802855

Epoch: 6| Step: 13
Training loss: 4.02585506439209
Validation loss: 3.031482873424407

Epoch: 5| Step: 0
Training loss: 4.309014797210693
Validation loss: 3.018692913875785

Epoch: 6| Step: 1
Training loss: 3.5724973678588867
Validation loss: 2.9997372755440335

Epoch: 6| Step: 2
Training loss: 1.8694149255752563
Validation loss: 2.9879091990891324

Epoch: 6| Step: 3
Training loss: 2.5102827548980713
Validation loss: 2.9758888444592877

Epoch: 6| Step: 4
Training loss: 3.28646183013916
Validation loss: 2.9747781240811912

Epoch: 6| Step: 5
Training loss: 3.277101516723633
Validation loss: 2.957580256205733

Epoch: 6| Step: 6
Training loss: 3.5872340202331543
Validation loss: 2.9459949308826077

Epoch: 6| Step: 7
Training loss: 3.1395421028137207
Validation loss: 2.9371617558181926

Epoch: 6| Step: 8
Training loss: 2.6414954662323
Validation loss: 2.9284570934951946

Epoch: 6| Step: 9
Training loss: 2.378997802734375
Validation loss: 2.9154705796190488

Epoch: 6| Step: 10
Training loss: 2.7007532119750977
Validation loss: 2.910455298680131

Epoch: 6| Step: 11
Training loss: 3.01419734954834
Validation loss: 2.896672171931113

Epoch: 6| Step: 12
Training loss: 2.601527452468872
Validation loss: 2.886471927806895

Epoch: 6| Step: 13
Training loss: 3.8917760848999023
Validation loss: 2.8801008347542054

Epoch: 6| Step: 0
Training loss: 3.619291067123413
Validation loss: 2.8664241939462642

Epoch: 6| Step: 1
Training loss: 1.6705708503723145
Validation loss: 2.8563555414958666

Epoch: 6| Step: 2
Training loss: 3.2837038040161133
Validation loss: 2.8708131082596315

Epoch: 6| Step: 3
Training loss: 3.7216389179229736
Validation loss: 2.8353215827736804

Epoch: 6| Step: 4
Training loss: 2.6731605529785156
Validation loss: 2.8299753307014384

Epoch: 6| Step: 5
Training loss: 3.2258553504943848
Validation loss: 2.8301121137475453

Epoch: 6| Step: 6
Training loss: 2.489759922027588
Validation loss: 2.8274010150663313

Epoch: 6| Step: 7
Training loss: 2.551717519760132
Validation loss: 2.823152190895491

Epoch: 6| Step: 8
Training loss: 3.1041157245635986
Validation loss: 2.806628780980264

Epoch: 6| Step: 9
Training loss: 3.8724489212036133
Validation loss: 2.7965413626804145

Epoch: 6| Step: 10
Training loss: 2.866145610809326
Validation loss: 2.7822236912224882

Epoch: 6| Step: 11
Training loss: 2.5716981887817383
Validation loss: 2.7805208647122948

Epoch: 6| Step: 12
Training loss: 2.2485742568969727
Validation loss: 2.7985762447439213

Epoch: 6| Step: 13
Training loss: 3.655829429626465
Validation loss: 2.797245571690221

Epoch: 7| Step: 0
Training loss: 2.5381155014038086
Validation loss: 2.7699636977205992

Epoch: 6| Step: 1
Training loss: 3.1786890029907227
Validation loss: 2.7526426033307145

Epoch: 6| Step: 2
Training loss: 2.720590114593506
Validation loss: 2.742955187315582

Epoch: 6| Step: 3
Training loss: 2.40710186958313
Validation loss: 2.7391905066787556

Epoch: 6| Step: 4
Training loss: 2.724245548248291
Validation loss: 2.7306308079791326

Epoch: 6| Step: 5
Training loss: 2.1892776489257812
Validation loss: 2.7300958120694725

Epoch: 6| Step: 6
Training loss: 4.015699863433838
Validation loss: 2.7410464184258574

Epoch: 6| Step: 7
Training loss: 3.220588207244873
Validation loss: 2.7232210661775325

Epoch: 6| Step: 8
Training loss: 2.0083084106445312
Validation loss: 2.719645684765231

Epoch: 6| Step: 9
Training loss: 2.9554593563079834
Validation loss: 2.717916511720227

Epoch: 6| Step: 10
Training loss: 3.1448185443878174
Validation loss: 2.7127928708189275

Epoch: 6| Step: 11
Training loss: 2.813983917236328
Validation loss: 2.7054620917125414

Epoch: 6| Step: 12
Training loss: 3.6246814727783203
Validation loss: 2.7011737080030542

Epoch: 6| Step: 13
Training loss: 2.7888550758361816
Validation loss: 2.700480373956824

Epoch: 8| Step: 0
Training loss: 2.5771565437316895
Validation loss: 2.6985207552550943

Epoch: 6| Step: 1
Training loss: 2.2788586616516113
Validation loss: 2.6835473224680912

Epoch: 6| Step: 2
Training loss: 3.622133255004883
Validation loss: 2.6778542610906784

Epoch: 6| Step: 3
Training loss: 3.0488109588623047
Validation loss: 2.6782749237552768

Epoch: 6| Step: 4
Training loss: 2.5710701942443848
Validation loss: 2.6714130063210764

Epoch: 6| Step: 5
Training loss: 2.4852676391601562
Validation loss: 2.688128363701605

Epoch: 6| Step: 6
Training loss: 3.01955509185791
Validation loss: 2.692288403869957

Epoch: 6| Step: 7
Training loss: 2.76615309715271
Validation loss: 2.679480238627362

Epoch: 6| Step: 8
Training loss: 3.083451271057129
Validation loss: 2.6542514985607517

Epoch: 6| Step: 9
Training loss: 3.405055284500122
Validation loss: 2.6450213514348513

Epoch: 6| Step: 10
Training loss: 2.316615343093872
Validation loss: 2.6751128986317623

Epoch: 6| Step: 11
Training loss: 3.3685197830200195
Validation loss: 2.7020275156985045

Epoch: 6| Step: 12
Training loss: 2.734470844268799
Validation loss: 2.710766356478455

Epoch: 6| Step: 13
Training loss: 2.5146965980529785
Validation loss: 2.7082457952601935

Epoch: 9| Step: 0
Training loss: 1.7502657175064087
Validation loss: 2.6880215162871988

Epoch: 6| Step: 1
Training loss: 2.1422970294952393
Validation loss: 2.6652310202198644

Epoch: 6| Step: 2
Training loss: 3.122666597366333
Validation loss: 2.651436416051721

Epoch: 6| Step: 3
Training loss: 2.0948147773742676
Validation loss: 2.6477901781758955

Epoch: 6| Step: 4
Training loss: 2.6451854705810547
Validation loss: 2.6335934362103863

Epoch: 6| Step: 5
Training loss: 4.1307291984558105
Validation loss: 2.7153709934603785

Epoch: 6| Step: 6
Training loss: 2.2856719493865967
Validation loss: 2.6358120390163955

Epoch: 6| Step: 7
Training loss: 2.8989908695220947
Validation loss: 2.6214036992801133

Epoch: 6| Step: 8
Training loss: 2.9561800956726074
Validation loss: 2.6447017551750265

Epoch: 6| Step: 9
Training loss: 3.474973678588867
Validation loss: 2.673691911082114

Epoch: 6| Step: 10
Training loss: 3.4811954498291016
Validation loss: 2.644528453068067

Epoch: 6| Step: 11
Training loss: 3.1050214767456055
Validation loss: 2.6275557087313746

Epoch: 6| Step: 12
Training loss: 2.601421594619751
Validation loss: 2.621567256989018

Epoch: 6| Step: 13
Training loss: 3.309358835220337
Validation loss: 2.6176337862527497

Epoch: 10| Step: 0
Training loss: 3.0246829986572266
Validation loss: 2.6123804071898102

Epoch: 6| Step: 1
Training loss: 1.696602702140808
Validation loss: 2.607930129574191

Epoch: 6| Step: 2
Training loss: 2.555894613265991
Validation loss: 2.6203792607912453

Epoch: 6| Step: 3
Training loss: 2.621575355529785
Validation loss: 2.6107302404219106

Epoch: 6| Step: 4
Training loss: 3.1580357551574707
Validation loss: 2.6059747588249946

Epoch: 6| Step: 5
Training loss: 2.9674019813537598
Validation loss: 2.605392332999937

Epoch: 6| Step: 6
Training loss: 3.43643856048584
Validation loss: 2.5952696056776148

Epoch: 6| Step: 7
Training loss: 2.6861519813537598
Validation loss: 2.5914855849358345

Epoch: 6| Step: 8
Training loss: 3.0015363693237305
Validation loss: 2.5866030236726165

Epoch: 6| Step: 9
Training loss: 3.6817290782928467
Validation loss: 2.5869153263748332

Epoch: 6| Step: 10
Training loss: 3.1330690383911133
Validation loss: 2.576778211901265

Epoch: 6| Step: 11
Training loss: 1.8345670700073242
Validation loss: 2.574848372449157

Epoch: 6| Step: 12
Training loss: 2.335629940032959
Validation loss: 2.57927454158824

Epoch: 6| Step: 13
Training loss: 3.3114655017852783
Validation loss: 2.5819527077418503

Epoch: 11| Step: 0
Training loss: 3.346043348312378
Validation loss: 2.5658389278637466

Epoch: 6| Step: 1
Training loss: 3.228071689605713
Validation loss: 2.5669616473618375

Epoch: 6| Step: 2
Training loss: 3.107595682144165
Validation loss: 2.5683713856563775

Epoch: 6| Step: 3
Training loss: 1.987361192703247
Validation loss: 2.5804547673912457

Epoch: 6| Step: 4
Training loss: 2.4018778800964355
Validation loss: 2.595688412266393

Epoch: 6| Step: 5
Training loss: 3.4528374671936035
Validation loss: 2.576750070818009

Epoch: 6| Step: 6
Training loss: 2.1713716983795166
Validation loss: 2.5737263438522175

Epoch: 6| Step: 7
Training loss: 2.0240180492401123
Validation loss: 2.6149013350086827

Epoch: 6| Step: 8
Training loss: 3.0843448638916016
Validation loss: 2.6424842393526466

Epoch: 6| Step: 9
Training loss: 2.7698659896850586
Validation loss: 2.6653335786634877

Epoch: 6| Step: 10
Training loss: 2.5687150955200195
Validation loss: 2.6668372487509124

Epoch: 6| Step: 11
Training loss: 2.6915745735168457
Validation loss: 2.6533596823292394

Epoch: 6| Step: 12
Training loss: 3.0749166011810303
Validation loss: 2.6404266049785

Epoch: 6| Step: 13
Training loss: 3.609830617904663
Validation loss: 2.6278051099469586

Epoch: 12| Step: 0
Training loss: 1.6036921739578247
Validation loss: 2.602961262067159

Epoch: 6| Step: 1
Training loss: 3.2334930896759033
Validation loss: 2.600912883717527

Epoch: 6| Step: 2
Training loss: 2.58258318901062
Validation loss: 2.6008581422990367

Epoch: 6| Step: 3
Training loss: 2.9759035110473633
Validation loss: 2.6092966923149685

Epoch: 6| Step: 4
Training loss: 2.6252946853637695
Validation loss: 2.615801772763652

Epoch: 6| Step: 5
Training loss: 2.126051664352417
Validation loss: 2.616043670203096

Epoch: 6| Step: 6
Training loss: 2.612983226776123
Validation loss: 2.6219371416235484

Epoch: 6| Step: 7
Training loss: 2.9212701320648193
Validation loss: 2.5975340002326557

Epoch: 6| Step: 8
Training loss: 3.4755730628967285
Validation loss: 2.590246054434007

Epoch: 6| Step: 9
Training loss: 3.239251136779785
Validation loss: 2.595027398037654

Epoch: 6| Step: 10
Training loss: 2.8907084465026855
Validation loss: 2.5448828743350123

Epoch: 6| Step: 11
Training loss: 3.0794801712036133
Validation loss: 2.524476969113914

Epoch: 6| Step: 12
Training loss: 2.87432861328125
Validation loss: 2.5199237561994985

Epoch: 6| Step: 13
Training loss: 2.637768030166626
Validation loss: 2.532678101652412

Epoch: 13| Step: 0
Training loss: 3.1801812648773193
Validation loss: 2.5281063433616393

Epoch: 6| Step: 1
Training loss: 3.1726322174072266
Validation loss: 2.5386367408178185

Epoch: 6| Step: 2
Training loss: 2.7053732872009277
Validation loss: 2.528212244792651

Epoch: 6| Step: 3
Training loss: 2.1098885536193848
Validation loss: 2.52310356017082

Epoch: 6| Step: 4
Training loss: 3.487104654312134
Validation loss: 2.528152975984799

Epoch: 6| Step: 5
Training loss: 2.445918083190918
Validation loss: 2.525366888251356

Epoch: 6| Step: 6
Training loss: 2.929631471633911
Validation loss: 2.523333598208684

Epoch: 6| Step: 7
Training loss: 2.082674503326416
Validation loss: 2.531184168272121

Epoch: 6| Step: 8
Training loss: 3.145719289779663
Validation loss: 2.528155331970543

Epoch: 6| Step: 9
Training loss: 3.1283626556396484
Validation loss: 2.515110178660321

Epoch: 6| Step: 10
Training loss: 2.7345542907714844
Validation loss: 2.507362563122985

Epoch: 6| Step: 11
Training loss: 1.8886544704437256
Validation loss: 2.5033431565889748

Epoch: 6| Step: 12
Training loss: 2.540057897567749
Validation loss: 2.505553719817951

Epoch: 6| Step: 13
Training loss: 3.154109001159668
Validation loss: 2.5266431736689743

Epoch: 14| Step: 0
Training loss: 3.077039957046509
Validation loss: 2.550991973569316

Epoch: 6| Step: 1
Training loss: 2.54008412361145
Validation loss: 2.532191276550293

Epoch: 6| Step: 2
Training loss: 1.9026991128921509
Validation loss: 2.505286432081653

Epoch: 6| Step: 3
Training loss: 2.375913619995117
Validation loss: 2.4985060871288343

Epoch: 6| Step: 4
Training loss: 3.261960983276367
Validation loss: 2.5072727075187107

Epoch: 6| Step: 5
Training loss: 2.7293097972869873
Validation loss: 2.5097415485689716

Epoch: 6| Step: 6
Training loss: 2.961310386657715
Validation loss: 2.5156945336249565

Epoch: 6| Step: 7
Training loss: 2.2250728607177734
Validation loss: 2.527172757733253

Epoch: 6| Step: 8
Training loss: 3.0369973182678223
Validation loss: 2.547379811604818

Epoch: 6| Step: 9
Training loss: 2.605135917663574
Validation loss: 2.518827735736806

Epoch: 6| Step: 10
Training loss: 2.9447689056396484
Validation loss: 2.4985486820179927

Epoch: 6| Step: 11
Training loss: 2.8604581356048584
Validation loss: 2.4868004783507316

Epoch: 6| Step: 12
Training loss: 3.2129743099212646
Validation loss: 2.4851422181693454

Epoch: 6| Step: 13
Training loss: 2.7451133728027344
Validation loss: 2.495968308500064

Epoch: 15| Step: 0
Training loss: 3.0506865978240967
Validation loss: 2.5142276825443393

Epoch: 6| Step: 1
Training loss: 2.970008134841919
Validation loss: 2.510254806087863

Epoch: 6| Step: 2
Training loss: 2.567652702331543
Validation loss: 2.4892505163787515

Epoch: 6| Step: 3
Training loss: 2.4760477542877197
Validation loss: 2.48018919011598

Epoch: 6| Step: 4
Training loss: 2.095951795578003
Validation loss: 2.4799776871999106

Epoch: 6| Step: 5
Training loss: 2.718113660812378
Validation loss: 2.477978314122846

Epoch: 6| Step: 6
Training loss: 2.53920316696167
Validation loss: 2.4801651764941472

Epoch: 6| Step: 7
Training loss: 2.955845355987549
Validation loss: 2.475343627314414

Epoch: 6| Step: 8
Training loss: 2.863649368286133
Validation loss: 2.470522621626495

Epoch: 6| Step: 9
Training loss: 2.887300968170166
Validation loss: 2.4727839833946637

Epoch: 6| Step: 10
Training loss: 2.7624869346618652
Validation loss: 2.467241875586971

Epoch: 6| Step: 11
Training loss: 2.631807804107666
Validation loss: 2.4719008450867026

Epoch: 6| Step: 12
Training loss: 2.7250773906707764
Validation loss: 2.466704301936652

Epoch: 6| Step: 13
Training loss: 2.8848400115966797
Validation loss: 2.471355499759797

Epoch: 16| Step: 0
Training loss: 2.992299795150757
Validation loss: 2.4811605202254428

Epoch: 6| Step: 1
Training loss: 2.296069860458374
Validation loss: 2.5019069487048733

Epoch: 6| Step: 2
Training loss: 2.433835983276367
Validation loss: 2.4801434906580115

Epoch: 6| Step: 3
Training loss: 2.444653272628784
Validation loss: 2.4667063938674105

Epoch: 6| Step: 4
Training loss: 3.036799907684326
Validation loss: 2.4612587164807063

Epoch: 6| Step: 5
Training loss: 2.0806937217712402
Validation loss: 2.4680977124039845

Epoch: 6| Step: 6
Training loss: 3.033059597015381
Validation loss: 2.4633036377609416

Epoch: 6| Step: 7
Training loss: 2.6333322525024414
Validation loss: 2.4701174228422103

Epoch: 6| Step: 8
Training loss: 2.3335120677948
Validation loss: 2.4682557223945536

Epoch: 6| Step: 9
Training loss: 3.362290382385254
Validation loss: 2.4604952258448445

Epoch: 6| Step: 10
Training loss: 2.61395001411438
Validation loss: 2.463791001227594

Epoch: 6| Step: 11
Training loss: 2.9882700443267822
Validation loss: 2.47323029528382

Epoch: 6| Step: 12
Training loss: 2.7255001068115234
Validation loss: 2.498059167656847

Epoch: 6| Step: 13
Training loss: 3.406371593475342
Validation loss: 2.5201525406170915

Epoch: 17| Step: 0
Training loss: 1.9817980527877808
Validation loss: 2.521655023738902

Epoch: 6| Step: 1
Training loss: 2.2858026027679443
Validation loss: 2.4751376157165854

Epoch: 6| Step: 2
Training loss: 2.77097225189209
Validation loss: 2.469910934407224

Epoch: 6| Step: 3
Training loss: 2.682075262069702
Validation loss: 2.47067436095207

Epoch: 6| Step: 4
Training loss: 1.9024510383605957
Validation loss: 2.4655262013917327

Epoch: 6| Step: 5
Training loss: 4.066546440124512
Validation loss: 2.4739076219579226

Epoch: 6| Step: 6
Training loss: 3.1401004791259766
Validation loss: 2.4674582994112404

Epoch: 6| Step: 7
Training loss: 2.3439743518829346
Validation loss: 2.4619701267570577

Epoch: 6| Step: 8
Training loss: 3.041813850402832
Validation loss: 2.4574745496114097

Epoch: 6| Step: 9
Training loss: 2.913220167160034
Validation loss: 2.4538358001298803

Epoch: 6| Step: 10
Training loss: 2.719820976257324
Validation loss: 2.4510276086868776

Epoch: 6| Step: 11
Training loss: 2.5587329864501953
Validation loss: 2.451443628598285

Epoch: 6| Step: 12
Training loss: 2.008881092071533
Validation loss: 2.450548300179102

Epoch: 6| Step: 13
Training loss: 3.946279287338257
Validation loss: 2.4446712822042485

Epoch: 18| Step: 0
Training loss: 2.2229628562927246
Validation loss: 2.451164207150859

Epoch: 6| Step: 1
Training loss: 2.689448595046997
Validation loss: 2.4785566970866215

Epoch: 6| Step: 2
Training loss: 2.3240976333618164
Validation loss: 2.55701804930164

Epoch: 6| Step: 3
Training loss: 3.4471335411071777
Validation loss: 2.5726803066909953

Epoch: 6| Step: 4
Training loss: 3.461285352706909
Validation loss: 2.5400193788672007

Epoch: 6| Step: 5
Training loss: 2.523935317993164
Validation loss: 2.530316701499365

Epoch: 6| Step: 6
Training loss: 2.8891916275024414
Validation loss: 2.498632087502428

Epoch: 6| Step: 7
Training loss: 2.2800912857055664
Validation loss: 2.4715809565718456

Epoch: 6| Step: 8
Training loss: 2.8031508922576904
Validation loss: 2.4688292857139342

Epoch: 6| Step: 9
Training loss: 2.6385345458984375
Validation loss: 2.4881767739531813

Epoch: 6| Step: 10
Training loss: 2.733154535293579
Validation loss: 2.5353060896678636

Epoch: 6| Step: 11
Training loss: 3.09647274017334
Validation loss: 2.5667995842554236

Epoch: 6| Step: 12
Training loss: 3.124354362487793
Validation loss: 2.580905311851091

Epoch: 6| Step: 13
Training loss: 1.9386643171310425
Validation loss: 2.5331798304793653

Epoch: 19| Step: 0
Training loss: 2.909679412841797
Validation loss: 2.475289306332988

Epoch: 6| Step: 1
Training loss: 2.501051664352417
Validation loss: 2.4582764038475613

Epoch: 6| Step: 2
Training loss: 2.3763484954833984
Validation loss: 2.4437789506809686

Epoch: 6| Step: 3
Training loss: 3.0142593383789062
Validation loss: 2.444297985364032

Epoch: 6| Step: 4
Training loss: 3.407383918762207
Validation loss: 2.4492406204182613

Epoch: 6| Step: 5
Training loss: 2.853346347808838
Validation loss: 2.449951587184783

Epoch: 6| Step: 6
Training loss: 2.0067200660705566
Validation loss: 2.470759335384574

Epoch: 6| Step: 7
Training loss: 2.6658411026000977
Validation loss: 2.484805496790076

Epoch: 6| Step: 8
Training loss: 2.7306969165802
Validation loss: 2.5221606967269734

Epoch: 6| Step: 9
Training loss: 2.6682941913604736
Validation loss: 2.5090696222038678

Epoch: 6| Step: 10
Training loss: 2.861461639404297
Validation loss: 2.4778684441761305

Epoch: 6| Step: 11
Training loss: 2.7755284309387207
Validation loss: 2.4567309502632386

Epoch: 6| Step: 12
Training loss: 2.461721897125244
Validation loss: 2.439062803022323

Epoch: 6| Step: 13
Training loss: 2.3291075229644775
Validation loss: 2.4372520062231247

Epoch: 20| Step: 0
Training loss: 2.8871941566467285
Validation loss: 2.444778896147205

Epoch: 6| Step: 1
Training loss: 3.0921621322631836
Validation loss: 2.437842943335092

Epoch: 6| Step: 2
Training loss: 2.7371935844421387
Validation loss: 2.4391637489359868

Epoch: 6| Step: 3
Training loss: 2.5198981761932373
Validation loss: 2.4393507049929712

Epoch: 6| Step: 4
Training loss: 1.8492792844772339
Validation loss: 2.4416090801197994

Epoch: 6| Step: 5
Training loss: 2.9059371948242188
Validation loss: 2.44012612681235

Epoch: 6| Step: 6
Training loss: 2.601813316345215
Validation loss: 2.4375189247951714

Epoch: 6| Step: 7
Training loss: 2.497554302215576
Validation loss: 2.4436568393502185

Epoch: 6| Step: 8
Training loss: 1.991800308227539
Validation loss: 2.451339849861719

Epoch: 6| Step: 9
Training loss: 2.492673397064209
Validation loss: 2.449673160429924

Epoch: 6| Step: 10
Training loss: 3.3787779808044434
Validation loss: 2.439852512010964

Epoch: 6| Step: 11
Training loss: 3.239835739135742
Validation loss: 2.4371955215290027

Epoch: 6| Step: 12
Training loss: 2.885496139526367
Validation loss: 2.432510337521953

Epoch: 6| Step: 13
Training loss: 2.331364870071411
Validation loss: 2.427986847457065

Epoch: 21| Step: 0
Training loss: 2.5529417991638184
Validation loss: 2.4294926197298112

Epoch: 6| Step: 1
Training loss: 3.2136600017547607
Validation loss: 2.4580323337226786

Epoch: 6| Step: 2
Training loss: 2.3017919063568115
Validation loss: 2.4827019604303504

Epoch: 6| Step: 3
Training loss: 2.4736104011535645
Validation loss: 2.4764620309234946

Epoch: 6| Step: 4
Training loss: 2.779621124267578
Validation loss: 2.4446431923938055

Epoch: 6| Step: 5
Training loss: 3.16859769821167
Validation loss: 2.429841367147302

Epoch: 6| Step: 6
Training loss: 3.50685977935791
Validation loss: 2.4254555702209473

Epoch: 6| Step: 7
Training loss: 2.9608845710754395
Validation loss: 2.4230345038957495

Epoch: 6| Step: 8
Training loss: 2.345519781112671
Validation loss: 2.427975795602286

Epoch: 6| Step: 9
Training loss: 1.8654645681381226
Validation loss: 2.433212080309468

Epoch: 6| Step: 10
Training loss: 2.9211995601654053
Validation loss: 2.4422372746211227

Epoch: 6| Step: 11
Training loss: 2.3477487564086914
Validation loss: 2.443073131704843

Epoch: 6| Step: 12
Training loss: 2.663649559020996
Validation loss: 2.4475409600042526

Epoch: 6| Step: 13
Training loss: 2.334531545639038
Validation loss: 2.4355452752882436

Epoch: 22| Step: 0
Training loss: 2.561185359954834
Validation loss: 2.4488106619927192

Epoch: 6| Step: 1
Training loss: 2.5287859439849854
Validation loss: 2.4904483595202045

Epoch: 6| Step: 2
Training loss: 2.61853289604187
Validation loss: 2.449605903317851

Epoch: 6| Step: 3
Training loss: 3.1531081199645996
Validation loss: 2.4467629130168627

Epoch: 6| Step: 4
Training loss: 2.97686767578125
Validation loss: 2.462138993765718

Epoch: 6| Step: 5
Training loss: 2.457371234893799
Validation loss: 2.4674824719787924

Epoch: 6| Step: 6
Training loss: 3.323624610900879
Validation loss: 2.496697820642943

Epoch: 6| Step: 7
Training loss: 3.2919921875
Validation loss: 2.4785535412449993

Epoch: 6| Step: 8
Training loss: 2.8260369300842285
Validation loss: 2.444789550637686

Epoch: 6| Step: 9
Training loss: 2.4517717361450195
Validation loss: 2.4356314546318463

Epoch: 6| Step: 10
Training loss: 2.7319366931915283
Validation loss: 2.450532131297614

Epoch: 6| Step: 11
Training loss: 2.076633930206299
Validation loss: 2.4567378464565484

Epoch: 6| Step: 12
Training loss: 2.1171131134033203
Validation loss: 2.4406774428582962

Epoch: 6| Step: 13
Training loss: 2.420806407928467
Validation loss: 2.43682893501815

Epoch: 23| Step: 0
Training loss: 2.7005200386047363
Validation loss: 2.43526969417449

Epoch: 6| Step: 1
Training loss: 2.317798614501953
Validation loss: 2.4330598731194772

Epoch: 6| Step: 2
Training loss: 2.555969476699829
Validation loss: 2.446865999570457

Epoch: 6| Step: 3
Training loss: 3.271549701690674
Validation loss: 2.4394000473842827

Epoch: 6| Step: 4
Training loss: 2.461653709411621
Validation loss: 2.4298504244896675

Epoch: 6| Step: 5
Training loss: 2.3647632598876953
Validation loss: 2.4285296086342103

Epoch: 6| Step: 6
Training loss: 2.3684089183807373
Validation loss: 2.427221408454321

Epoch: 6| Step: 7
Training loss: 2.0274720191955566
Validation loss: 2.417672784097733

Epoch: 6| Step: 8
Training loss: 3.482269287109375
Validation loss: 2.413903428662208

Epoch: 6| Step: 9
Training loss: 2.9053122997283936
Validation loss: 2.40504216635099

Epoch: 6| Step: 10
Training loss: 2.8128461837768555
Validation loss: 2.4025205642946306

Epoch: 6| Step: 11
Training loss: 2.361332893371582
Validation loss: 2.405856758035639

Epoch: 6| Step: 12
Training loss: 3.4032177925109863
Validation loss: 2.4039810549828315

Epoch: 6| Step: 13
Training loss: 1.6796565055847168
Validation loss: 2.399406845851611

Epoch: 24| Step: 0
Training loss: 2.7545485496520996
Validation loss: 2.3929150514705206

Epoch: 6| Step: 1
Training loss: 2.917387008666992
Validation loss: 2.394238495057629

Epoch: 6| Step: 2
Training loss: 1.82072114944458
Validation loss: 2.3951494309210006

Epoch: 6| Step: 3
Training loss: 3.0219621658325195
Validation loss: 2.3950024894488755

Epoch: 6| Step: 4
Training loss: 3.6426029205322266
Validation loss: 2.3931979671601327

Epoch: 6| Step: 5
Training loss: 2.3867530822753906
Validation loss: 2.3953034057412097

Epoch: 6| Step: 6
Training loss: 2.5049445629119873
Validation loss: 2.3968997309284825

Epoch: 6| Step: 7
Training loss: 2.597407817840576
Validation loss: 2.4091563916975454

Epoch: 6| Step: 8
Training loss: 2.8485593795776367
Validation loss: 2.4202930747821765

Epoch: 6| Step: 9
Training loss: 3.7655019760131836
Validation loss: 2.42971485148194

Epoch: 6| Step: 10
Training loss: 2.050605535507202
Validation loss: 2.3963763021653697

Epoch: 6| Step: 11
Training loss: 2.106456756591797
Validation loss: 2.390050624006538

Epoch: 6| Step: 12
Training loss: 2.162616729736328
Validation loss: 2.392758128463581

Epoch: 6| Step: 13
Training loss: 2.8397393226623535
Validation loss: 2.3966484018551406

Epoch: 25| Step: 0
Training loss: 1.8332664966583252
Validation loss: 2.4042723025045087

Epoch: 6| Step: 1
Training loss: 3.0169854164123535
Validation loss: 2.388781798783169

Epoch: 6| Step: 2
Training loss: 1.40963876247406
Validation loss: 2.3924604769675963

Epoch: 6| Step: 3
Training loss: 3.066995143890381
Validation loss: 2.4022657743064304

Epoch: 6| Step: 4
Training loss: 3.2245194911956787
Validation loss: 2.426851587910806

Epoch: 6| Step: 5
Training loss: 2.890109062194824
Validation loss: 2.4506666403944775

Epoch: 6| Step: 6
Training loss: 3.8204917907714844
Validation loss: 2.4668153203943723

Epoch: 6| Step: 7
Training loss: 2.267465114593506
Validation loss: 2.438247606318484

Epoch: 6| Step: 8
Training loss: 2.255194664001465
Validation loss: 2.42200235397585

Epoch: 6| Step: 9
Training loss: 2.164414405822754
Validation loss: 2.4081073140585296

Epoch: 6| Step: 10
Training loss: 3.07733154296875
Validation loss: 2.3937604222246396

Epoch: 6| Step: 11
Training loss: 2.5906081199645996
Validation loss: 2.3913477543861634

Epoch: 6| Step: 12
Training loss: 2.8390839099884033
Validation loss: 2.3988896069988126

Epoch: 6| Step: 13
Training loss: 2.7775120735168457
Validation loss: 2.3922496226526078

Epoch: 26| Step: 0
Training loss: 2.0711112022399902
Validation loss: 2.3913213309421333

Epoch: 6| Step: 1
Training loss: 2.0067152976989746
Validation loss: 2.3865498804276988

Epoch: 6| Step: 2
Training loss: 3.107438564300537
Validation loss: 2.3855953011461484

Epoch: 6| Step: 3
Training loss: 3.1292989253997803
Validation loss: 2.382630660969724

Epoch: 6| Step: 4
Training loss: 2.2048580646514893
Validation loss: 2.3792428201244724

Epoch: 6| Step: 5
Training loss: 3.0768237113952637
Validation loss: 2.379604406254266

Epoch: 6| Step: 6
Training loss: 2.2971701622009277
Validation loss: 2.3808305853156635

Epoch: 6| Step: 7
Training loss: 2.989798069000244
Validation loss: 2.379805657171434

Epoch: 6| Step: 8
Training loss: 3.298257350921631
Validation loss: 2.373423550718574

Epoch: 6| Step: 9
Training loss: 2.306856632232666
Validation loss: 2.3819764608977945

Epoch: 6| Step: 10
Training loss: 2.911027669906616
Validation loss: 2.3785911349840063

Epoch: 6| Step: 11
Training loss: 2.8712363243103027
Validation loss: 2.3932174277561966

Epoch: 6| Step: 12
Training loss: 2.25419545173645
Validation loss: 2.3970915809754403

Epoch: 6| Step: 13
Training loss: 2.37172532081604
Validation loss: 2.3975361982981362

Epoch: 27| Step: 0
Training loss: 3.1266698837280273
Validation loss: 2.434060130068051

Epoch: 6| Step: 1
Training loss: 2.3517487049102783
Validation loss: 2.431905064531552

Epoch: 6| Step: 2
Training loss: 2.4100615978240967
Validation loss: 2.437488548217281

Epoch: 6| Step: 3
Training loss: 2.7343568801879883
Validation loss: 2.418918619873703

Epoch: 6| Step: 4
Training loss: 2.676931619644165
Validation loss: 2.425710303809053

Epoch: 6| Step: 5
Training loss: 2.333051919937134
Validation loss: 2.4331206506298435

Epoch: 6| Step: 6
Training loss: 1.9921801090240479
Validation loss: 2.410995880762736

Epoch: 6| Step: 7
Training loss: 2.962451934814453
Validation loss: 2.3907955256841515

Epoch: 6| Step: 8
Training loss: 3.14621639251709
Validation loss: 2.37539743608044

Epoch: 6| Step: 9
Training loss: 2.5193722248077393
Validation loss: 2.3721786391350532

Epoch: 6| Step: 10
Training loss: 2.113600492477417
Validation loss: 2.377154509226481

Epoch: 6| Step: 11
Training loss: 2.881876230239868
Validation loss: 2.385322186254686

Epoch: 6| Step: 12
Training loss: 2.580232620239258
Validation loss: 2.3839482850925897

Epoch: 6| Step: 13
Training loss: 3.4745113849639893
Validation loss: 2.391424250859086

Epoch: 28| Step: 0
Training loss: 2.0520355701446533
Validation loss: 2.380405500370969

Epoch: 6| Step: 1
Training loss: 2.956653594970703
Validation loss: 2.3866823693757415

Epoch: 6| Step: 2
Training loss: 2.5808768272399902
Validation loss: 2.4025779693357405

Epoch: 6| Step: 3
Training loss: 2.512758731842041
Validation loss: 2.4094042521651073

Epoch: 6| Step: 4
Training loss: 2.3074371814727783
Validation loss: 2.501008810535554

Epoch: 6| Step: 5
Training loss: 3.4245214462280273
Validation loss: 2.5370583970059633

Epoch: 6| Step: 6
Training loss: 1.4964118003845215
Validation loss: 2.5161501464023384

Epoch: 6| Step: 7
Training loss: 2.9150712490081787
Validation loss: 2.524501992810157

Epoch: 6| Step: 8
Training loss: 3.082103729248047
Validation loss: 2.4823094721763366

Epoch: 6| Step: 9
Training loss: 2.567595958709717
Validation loss: 2.4060234023678686

Epoch: 6| Step: 10
Training loss: 3.0599398612976074
Validation loss: 2.3716188118021977

Epoch: 6| Step: 11
Training loss: 2.6496846675872803
Validation loss: 2.3707412853035876

Epoch: 6| Step: 12
Training loss: 3.1017894744873047
Validation loss: 2.3723849532424763

Epoch: 6| Step: 13
Training loss: 2.2255828380584717
Validation loss: 2.3875022857419905

Epoch: 29| Step: 0
Training loss: 2.29360294342041
Validation loss: 2.385480729482507

Epoch: 6| Step: 1
Training loss: 2.321354389190674
Validation loss: 2.3700012160885717

Epoch: 6| Step: 2
Training loss: 3.0978193283081055
Validation loss: 2.361519149554673

Epoch: 6| Step: 3
Training loss: 1.968422532081604
Validation loss: 2.3558929274159093

Epoch: 6| Step: 4
Training loss: 2.7504334449768066
Validation loss: 2.366851217003279

Epoch: 6| Step: 5
Training loss: 2.5480539798736572
Validation loss: 2.387119754668205

Epoch: 6| Step: 6
Training loss: 2.771937370300293
Validation loss: 2.422995626285512

Epoch: 6| Step: 7
Training loss: 3.564512252807617
Validation loss: 2.4458761471574024

Epoch: 6| Step: 8
Training loss: 2.5008792877197266
Validation loss: 2.445932070414225

Epoch: 6| Step: 9
Training loss: 2.624981164932251
Validation loss: 2.404553057045065

Epoch: 6| Step: 10
Training loss: 2.6489686965942383
Validation loss: 2.3791293790263515

Epoch: 6| Step: 11
Training loss: 2.6290831565856934
Validation loss: 2.3659901157502206

Epoch: 6| Step: 12
Training loss: 2.6979076862335205
Validation loss: 2.3567557539991153

Epoch: 6| Step: 13
Training loss: 2.686952829360962
Validation loss: 2.3552602439798336

Epoch: 30| Step: 0
Training loss: 2.2890052795410156
Validation loss: 2.358891669140067

Epoch: 6| Step: 1
Training loss: 2.516575574874878
Validation loss: 2.372022444202054

Epoch: 6| Step: 2
Training loss: 2.4978413581848145
Validation loss: 2.36808301812859

Epoch: 6| Step: 3
Training loss: 2.452228546142578
Validation loss: 2.364348162886917

Epoch: 6| Step: 4
Training loss: 1.9093446731567383
Validation loss: 2.3531825004085416

Epoch: 6| Step: 5
Training loss: 3.2361745834350586
Validation loss: 2.3482543242874967

Epoch: 6| Step: 6
Training loss: 2.8606531620025635
Validation loss: 2.350674613829582

Epoch: 6| Step: 7
Training loss: 2.1471643447875977
Validation loss: 2.353181104506216

Epoch: 6| Step: 8
Training loss: 3.23679780960083
Validation loss: 2.3684119896222184

Epoch: 6| Step: 9
Training loss: 2.9904563426971436
Validation loss: 2.4002420466433287

Epoch: 6| Step: 10
Training loss: 2.6102099418640137
Validation loss: 2.386772758217268

Epoch: 6| Step: 11
Training loss: 3.0900235176086426
Validation loss: 2.384437802017376

Epoch: 6| Step: 12
Training loss: 1.7910685539245605
Validation loss: 2.381462207404516

Epoch: 6| Step: 13
Training loss: 3.7422688007354736
Validation loss: 2.3710305062673425

Epoch: 31| Step: 0
Training loss: 2.4337477684020996
Validation loss: 2.363389256179974

Epoch: 6| Step: 1
Training loss: 2.5767059326171875
Validation loss: 2.3647815847909577

Epoch: 6| Step: 2
Training loss: 2.6868295669555664
Validation loss: 2.355573215792256

Epoch: 6| Step: 3
Training loss: 2.6231141090393066
Validation loss: 2.370390833065074

Epoch: 6| Step: 4
Training loss: 1.9615355730056763
Validation loss: 2.363291355871385

Epoch: 6| Step: 5
Training loss: 2.3044192790985107
Validation loss: 2.3632377604002595

Epoch: 6| Step: 6
Training loss: 2.513279438018799
Validation loss: 2.358755883350167

Epoch: 6| Step: 7
Training loss: 2.9931468963623047
Validation loss: 2.3527800831743466

Epoch: 6| Step: 8
Training loss: 3.1518917083740234
Validation loss: 2.35178445487894

Epoch: 6| Step: 9
Training loss: 2.5994343757629395
Validation loss: 2.34601967693657

Epoch: 6| Step: 10
Training loss: 2.5069847106933594
Validation loss: 2.3440507406829507

Epoch: 6| Step: 11
Training loss: 3.13844633102417
Validation loss: 2.3458156278056483

Epoch: 6| Step: 12
Training loss: 2.6835062503814697
Validation loss: 2.344743033891083

Epoch: 6| Step: 13
Training loss: 2.3953654766082764
Validation loss: 2.3463306709002425

Epoch: 32| Step: 0
Training loss: 1.8269920349121094
Validation loss: 2.358069266042402

Epoch: 6| Step: 1
Training loss: 2.6061153411865234
Validation loss: 2.3609863532486783

Epoch: 6| Step: 2
Training loss: 2.4862656593322754
Validation loss: 2.3585514689004548

Epoch: 6| Step: 3
Training loss: 2.872972249984741
Validation loss: 2.3510564937386462

Epoch: 6| Step: 4
Training loss: 2.8985347747802734
Validation loss: 2.3481271779665382

Epoch: 6| Step: 5
Training loss: 2.122683525085449
Validation loss: 2.337767178012479

Epoch: 6| Step: 6
Training loss: 2.1581225395202637
Validation loss: 2.337538106467134

Epoch: 6| Step: 7
Training loss: 3.1084656715393066
Validation loss: 2.355869616231611

Epoch: 6| Step: 8
Training loss: 2.866071939468384
Validation loss: 2.368383494756555

Epoch: 6| Step: 9
Training loss: 2.331573009490967
Validation loss: 2.4077803037499868

Epoch: 6| Step: 10
Training loss: 3.0376248359680176
Validation loss: 2.4118914578550603

Epoch: 6| Step: 11
Training loss: 2.29665470123291
Validation loss: 2.3585253466841993

Epoch: 6| Step: 12
Training loss: 3.0294089317321777
Validation loss: 2.340666950389903

Epoch: 6| Step: 13
Training loss: 3.2724642753601074
Validation loss: 2.333592535347067

Epoch: 33| Step: 0
Training loss: 2.5822107791900635
Validation loss: 2.339867345748409

Epoch: 6| Step: 1
Training loss: 2.8365087509155273
Validation loss: 2.352586130942068

Epoch: 6| Step: 2
Training loss: 3.0135750770568848
Validation loss: 2.357907320863457

Epoch: 6| Step: 3
Training loss: 2.19993257522583
Validation loss: 2.354482860975368

Epoch: 6| Step: 4
Training loss: 2.8964285850524902
Validation loss: 2.350277405913158

Epoch: 6| Step: 5
Training loss: 2.4794082641601562
Validation loss: 2.3428389000636276

Epoch: 6| Step: 6
Training loss: 2.8213372230529785
Validation loss: 2.339344068240094

Epoch: 6| Step: 7
Training loss: 2.7322630882263184
Validation loss: 2.33138342313869

Epoch: 6| Step: 8
Training loss: 2.0754075050354004
Validation loss: 2.327304945197157

Epoch: 6| Step: 9
Training loss: 3.0414810180664062
Validation loss: 2.32779606439734

Epoch: 6| Step: 10
Training loss: 2.3180577754974365
Validation loss: 2.334269590275262

Epoch: 6| Step: 11
Training loss: 2.6024351119995117
Validation loss: 2.3709713874324674

Epoch: 6| Step: 12
Training loss: 2.2086546421051025
Validation loss: 2.391716223891063

Epoch: 6| Step: 13
Training loss: 3.382481098175049
Validation loss: 2.3850292723665953

Epoch: 34| Step: 0
Training loss: 3.086073875427246
Validation loss: 2.3420123438681326

Epoch: 6| Step: 1
Training loss: 3.28134822845459
Validation loss: 2.3359498336750972

Epoch: 6| Step: 2
Training loss: 2.567296028137207
Validation loss: 2.3251058798964306

Epoch: 6| Step: 3
Training loss: 2.549543619155884
Validation loss: 2.3219919204711914

Epoch: 6| Step: 4
Training loss: 2.4734435081481934
Validation loss: 2.3175680970632904

Epoch: 6| Step: 5
Training loss: 2.7855658531188965
Validation loss: 2.3195245035233034

Epoch: 6| Step: 6
Training loss: 2.5817675590515137
Validation loss: 2.3269189634630756

Epoch: 6| Step: 7
Training loss: 2.0112533569335938
Validation loss: 2.327543374030821

Epoch: 6| Step: 8
Training loss: 2.822737216949463
Validation loss: 2.3221030158381306

Epoch: 6| Step: 9
Training loss: 2.26625919342041
Validation loss: 2.325213811730826

Epoch: 6| Step: 10
Training loss: 2.204906940460205
Validation loss: 2.328584655638664

Epoch: 6| Step: 11
Training loss: 2.464668035507202
Validation loss: 2.3371935403475197

Epoch: 6| Step: 12
Training loss: 2.300074338912964
Validation loss: 2.3403136704557683

Epoch: 6| Step: 13
Training loss: 3.2780070304870605
Validation loss: 2.34319410785552

Epoch: 35| Step: 0
Training loss: 2.68713116645813
Validation loss: 2.3402077613338346

Epoch: 6| Step: 1
Training loss: 2.979668617248535
Validation loss: 2.331832847287578

Epoch: 6| Step: 2
Training loss: 3.1151838302612305
Validation loss: 2.308573225493072

Epoch: 6| Step: 3
Training loss: 2.5656654834747314
Validation loss: 2.3099826561507357

Epoch: 6| Step: 4
Training loss: 2.4648590087890625
Validation loss: 2.319964590892997

Epoch: 6| Step: 5
Training loss: 2.5440926551818848
Validation loss: 2.322286508416617

Epoch: 6| Step: 6
Training loss: 2.7316441535949707
Validation loss: 2.335083443631408

Epoch: 6| Step: 7
Training loss: 2.8728108406066895
Validation loss: 2.3506828379887406

Epoch: 6| Step: 8
Training loss: 2.6368892192840576
Validation loss: 2.3462482395992486

Epoch: 6| Step: 9
Training loss: 2.1896133422851562
Validation loss: 2.3318059546973116

Epoch: 6| Step: 10
Training loss: 2.484694480895996
Validation loss: 2.326469434204922

Epoch: 6| Step: 11
Training loss: 2.5555503368377686
Validation loss: 2.3301677473129763

Epoch: 6| Step: 12
Training loss: 2.455533742904663
Validation loss: 2.326256194422322

Epoch: 6| Step: 13
Training loss: 2.3325881958007812
Validation loss: 2.3403593519682526

Epoch: 36| Step: 0
Training loss: 2.0005691051483154
Validation loss: 2.373599503629951

Epoch: 6| Step: 1
Training loss: 2.777493476867676
Validation loss: 2.4018635954908145

Epoch: 6| Step: 2
Training loss: 2.289900302886963
Validation loss: 2.409336361833798

Epoch: 6| Step: 3
Training loss: 2.2627639770507812
Validation loss: 2.3969108032923874

Epoch: 6| Step: 4
Training loss: 2.63228702545166
Validation loss: 2.363060415432017

Epoch: 6| Step: 5
Training loss: 2.1454274654388428
Validation loss: 2.3595766482814664

Epoch: 6| Step: 6
Training loss: 2.372215986251831
Validation loss: 2.3554815887123026

Epoch: 6| Step: 7
Training loss: 2.7329320907592773
Validation loss: 2.3387082392169583

Epoch: 6| Step: 8
Training loss: 3.5150232315063477
Validation loss: 2.341275612513224

Epoch: 6| Step: 9
Training loss: 3.041008472442627
Validation loss: 2.316275332563667

Epoch: 6| Step: 10
Training loss: 2.7486047744750977
Validation loss: 2.305177539907476

Epoch: 6| Step: 11
Training loss: 3.245182514190674
Validation loss: 2.3029985863675355

Epoch: 6| Step: 12
Training loss: 2.05645751953125
Validation loss: 2.301279088502289

Epoch: 6| Step: 13
Training loss: 2.6249749660491943
Validation loss: 2.3021123101634364

Epoch: 37| Step: 0
Training loss: 2.489288330078125
Validation loss: 2.302480113121771

Epoch: 6| Step: 1
Training loss: 2.5841009616851807
Validation loss: 2.2996060438053583

Epoch: 6| Step: 2
Training loss: 2.4337716102600098
Validation loss: 2.300669011249337

Epoch: 6| Step: 3
Training loss: 2.4464926719665527
Validation loss: 2.2964777408107633

Epoch: 6| Step: 4
Training loss: 3.041950225830078
Validation loss: 2.2959204361002934

Epoch: 6| Step: 5
Training loss: 2.1421265602111816
Validation loss: 2.3081523859372703

Epoch: 6| Step: 6
Training loss: 2.9098963737487793
Validation loss: 2.3130051653872252

Epoch: 6| Step: 7
Training loss: 3.0218348503112793
Validation loss: 2.332575164815431

Epoch: 6| Step: 8
Training loss: 2.4660472869873047
Validation loss: 2.3344876202203895

Epoch: 6| Step: 9
Training loss: 2.5111663341522217
Validation loss: 2.3567691003122637

Epoch: 6| Step: 10
Training loss: 3.0759050846099854
Validation loss: 2.3303524140388734

Epoch: 6| Step: 11
Training loss: 3.301845073699951
Validation loss: 2.3178144808738463

Epoch: 6| Step: 12
Training loss: 1.7085561752319336
Validation loss: 2.30226364187015

Epoch: 6| Step: 13
Training loss: 1.8973650932312012
Validation loss: 2.293566357704901

Epoch: 38| Step: 0
Training loss: 3.339031219482422
Validation loss: 2.2878683395283197

Epoch: 6| Step: 1
Training loss: 2.7831082344055176
Validation loss: 2.3048065862348004

Epoch: 6| Step: 2
Training loss: 2.027179718017578
Validation loss: 2.323953397812382

Epoch: 6| Step: 3
Training loss: 2.3570103645324707
Validation loss: 2.3404342077111684

Epoch: 6| Step: 4
Training loss: 3.634242057800293
Validation loss: 2.37064705869203

Epoch: 6| Step: 5
Training loss: 2.8513824939727783
Validation loss: 2.3321652591869397

Epoch: 6| Step: 6
Training loss: 2.504432201385498
Validation loss: 2.3306290616271315

Epoch: 6| Step: 7
Training loss: 2.1446216106414795
Validation loss: 2.319288199947726

Epoch: 6| Step: 8
Training loss: 2.2713568210601807
Validation loss: 2.312999015213341

Epoch: 6| Step: 9
Training loss: 2.6365742683410645
Validation loss: 2.3078939376338834

Epoch: 6| Step: 10
Training loss: 2.1483845710754395
Validation loss: 2.298699766077021

Epoch: 6| Step: 11
Training loss: 2.6169886589050293
Validation loss: 2.291618629168439

Epoch: 6| Step: 12
Training loss: 2.7675063610076904
Validation loss: 2.2992540969643542

Epoch: 6| Step: 13
Training loss: 2.5881805419921875
Validation loss: 2.3112848856115855

Epoch: 39| Step: 0
Training loss: 2.647632598876953
Validation loss: 2.3274565589043403

Epoch: 6| Step: 1
Training loss: 2.7779674530029297
Validation loss: 2.35278558474715

Epoch: 6| Step: 2
Training loss: 2.770703077316284
Validation loss: 2.369098324929514

Epoch: 6| Step: 3
Training loss: 2.224522590637207
Validation loss: 2.350145291256648

Epoch: 6| Step: 4
Training loss: 2.4766955375671387
Validation loss: 2.3350967540535876

Epoch: 6| Step: 5
Training loss: 2.0454821586608887
Validation loss: 2.3154587514938845

Epoch: 6| Step: 6
Training loss: 2.7365212440490723
Validation loss: 2.289393389096824

Epoch: 6| Step: 7
Training loss: 3.2079262733459473
Validation loss: 2.287792323738016

Epoch: 6| Step: 8
Training loss: 3.547562599182129
Validation loss: 2.2799814747225855

Epoch: 6| Step: 9
Training loss: 2.324157953262329
Validation loss: 2.3037416268420476

Epoch: 6| Step: 10
Training loss: 2.1541028022766113
Validation loss: 2.3154643222849858

Epoch: 6| Step: 11
Training loss: 2.4954631328582764
Validation loss: 2.322679014616115

Epoch: 6| Step: 12
Training loss: 2.136111259460449
Validation loss: 2.324502293781568

Epoch: 6| Step: 13
Training loss: 3.373945951461792
Validation loss: 2.327214580710216

Epoch: 40| Step: 0
Training loss: 2.776515007019043
Validation loss: 2.3345788499360443

Epoch: 6| Step: 1
Training loss: 2.2187588214874268
Validation loss: 2.3237115952276413

Epoch: 6| Step: 2
Training loss: 3.018909215927124
Validation loss: 2.3473972582047984

Epoch: 6| Step: 3
Training loss: 2.376065731048584
Validation loss: 2.3501886860016854

Epoch: 6| Step: 4
Training loss: 2.0577542781829834
Validation loss: 2.3407737670406217

Epoch: 6| Step: 5
Training loss: 3.3101673126220703
Validation loss: 2.336530880261493

Epoch: 6| Step: 6
Training loss: 2.4918107986450195
Validation loss: 2.341087956582346

Epoch: 6| Step: 7
Training loss: 2.3741252422332764
Validation loss: 2.3504494338907223

Epoch: 6| Step: 8
Training loss: 2.365004539489746
Validation loss: 2.3417425181276057

Epoch: 6| Step: 9
Training loss: 2.36297869682312
Validation loss: 2.350830662635065

Epoch: 6| Step: 10
Training loss: 2.318594217300415
Validation loss: 2.355320881771785

Epoch: 6| Step: 11
Training loss: 2.966872453689575
Validation loss: 2.3646349624920915

Epoch: 6| Step: 12
Training loss: 2.97343111038208
Validation loss: 2.3560354517352198

Epoch: 6| Step: 13
Training loss: 3.348477602005005
Validation loss: 2.353368874519102

Epoch: 41| Step: 0
Training loss: 2.030557155609131
Validation loss: 2.3468184983858498

Epoch: 6| Step: 1
Training loss: 3.325148105621338
Validation loss: 2.3417216065109416

Epoch: 6| Step: 2
Training loss: 2.374507188796997
Validation loss: 2.3464296274287726

Epoch: 6| Step: 3
Training loss: 2.582554340362549
Validation loss: 2.351682832164149

Epoch: 6| Step: 4
Training loss: 2.5437512397766113
Validation loss: 2.3432137735428347

Epoch: 6| Step: 5
Training loss: 2.6719748973846436
Validation loss: 2.342372622541202

Epoch: 6| Step: 6
Training loss: 1.7943804264068604
Validation loss: 2.328819736357658

Epoch: 6| Step: 7
Training loss: 2.695265531539917
Validation loss: 2.33156769762757

Epoch: 6| Step: 8
Training loss: 3.3900578022003174
Validation loss: 2.3316848765137377

Epoch: 6| Step: 9
Training loss: 2.754018545150757
Validation loss: 2.3330785228360083

Epoch: 6| Step: 10
Training loss: 2.947685718536377
Validation loss: 2.3208564366063764

Epoch: 6| Step: 11
Training loss: 2.0393898487091064
Validation loss: 2.3159926373471498

Epoch: 6| Step: 12
Training loss: 2.211683988571167
Validation loss: 2.3052326145992486

Epoch: 6| Step: 13
Training loss: 3.395453929901123
Validation loss: 2.313166956747732

Epoch: 42| Step: 0
Training loss: 2.8266658782958984
Validation loss: 2.3139036009388585

Epoch: 6| Step: 1
Training loss: 2.3042097091674805
Validation loss: 2.318450356042513

Epoch: 6| Step: 2
Training loss: 2.5860204696655273
Validation loss: 2.324173797843277

Epoch: 6| Step: 3
Training loss: 2.797055244445801
Validation loss: 2.3333049615224204

Epoch: 6| Step: 4
Training loss: 2.5981240272521973
Validation loss: 2.3385451173269622

Epoch: 6| Step: 5
Training loss: 3.072695732116699
Validation loss: 2.341189205005605

Epoch: 6| Step: 6
Training loss: 1.9362472295761108
Validation loss: 2.336459511069841

Epoch: 6| Step: 7
Training loss: 3.213833808898926
Validation loss: 2.3608843818787606

Epoch: 6| Step: 8
Training loss: 2.0597496032714844
Validation loss: 2.3687366721450642

Epoch: 6| Step: 9
Training loss: 2.8795018196105957
Validation loss: 2.3576301092742593

Epoch: 6| Step: 10
Training loss: 2.348726272583008
Validation loss: 2.338997651171941

Epoch: 6| Step: 11
Training loss: 1.794944405555725
Validation loss: 2.3159394571858067

Epoch: 6| Step: 12
Training loss: 3.3279762268066406
Validation loss: 2.3076097824240245

Epoch: 6| Step: 13
Training loss: 2.343205451965332
Validation loss: 2.310142734999298

Epoch: 43| Step: 0
Training loss: 3.225538969039917
Validation loss: 2.3121453818454536

Epoch: 6| Step: 1
Training loss: 1.5428448915481567
Validation loss: 2.31942412161058

Epoch: 6| Step: 2
Training loss: 2.831676959991455
Validation loss: 2.3188594310514388

Epoch: 6| Step: 3
Training loss: 3.011897563934326
Validation loss: 2.349565839254728

Epoch: 6| Step: 4
Training loss: 2.501028060913086
Validation loss: 2.332139774035382

Epoch: 6| Step: 5
Training loss: 2.351081132888794
Validation loss: 2.3263469588372017

Epoch: 6| Step: 6
Training loss: 1.7377324104309082
Validation loss: 2.328166748887749

Epoch: 6| Step: 7
Training loss: 2.9666683673858643
Validation loss: 2.3183876237561627

Epoch: 6| Step: 8
Training loss: 3.1510467529296875
Validation loss: 2.3302223067129813

Epoch: 6| Step: 9
Training loss: 2.3078384399414062
Validation loss: 2.31987605556365

Epoch: 6| Step: 10
Training loss: 2.3560867309570312
Validation loss: 2.32624916620152

Epoch: 6| Step: 11
Training loss: 2.826119899749756
Validation loss: 2.3352923598340762

Epoch: 6| Step: 12
Training loss: 2.8777847290039062
Validation loss: 2.3444828064210954

Epoch: 6| Step: 13
Training loss: 2.541231870651245
Validation loss: 2.3370534322595082

Epoch: 44| Step: 0
Training loss: 2.877920627593994
Validation loss: 2.3195188071138118

Epoch: 6| Step: 1
Training loss: 3.2822775840759277
Validation loss: 2.3101532689986692

Epoch: 6| Step: 2
Training loss: 2.6128149032592773
Validation loss: 2.305305027192639

Epoch: 6| Step: 3
Training loss: 2.5774426460266113
Validation loss: 2.2955044777162614

Epoch: 6| Step: 4
Training loss: 2.3719451427459717
Validation loss: 2.2911953131357827

Epoch: 6| Step: 5
Training loss: 2.2506942749023438
Validation loss: 2.2879929952724005

Epoch: 6| Step: 6
Training loss: 2.642101764678955
Validation loss: 2.2921692607223347

Epoch: 6| Step: 7
Training loss: 1.8541333675384521
Validation loss: 2.28678749838183

Epoch: 6| Step: 8
Training loss: 2.810671091079712
Validation loss: 2.285981403884067

Epoch: 6| Step: 9
Training loss: 2.907134771347046
Validation loss: 2.289708399003552

Epoch: 6| Step: 10
Training loss: 2.6640589237213135
Validation loss: 2.2979812622070312

Epoch: 6| Step: 11
Training loss: 2.6985654830932617
Validation loss: 2.3292103095721175

Epoch: 6| Step: 12
Training loss: 2.744313955307007
Validation loss: 2.3499986587032193

Epoch: 6| Step: 13
Training loss: 1.4765697717666626
Validation loss: 2.377876615011564

Epoch: 45| Step: 0
Training loss: 1.979941964149475
Validation loss: 2.379267813057028

Epoch: 6| Step: 1
Training loss: 3.8216447830200195
Validation loss: 2.3709021434989026

Epoch: 6| Step: 2
Training loss: 2.698284864425659
Validation loss: 2.3537519901029524

Epoch: 6| Step: 3
Training loss: 2.872629165649414
Validation loss: 2.342659463164627

Epoch: 6| Step: 4
Training loss: 2.154052257537842
Validation loss: 2.3511200874082503

Epoch: 6| Step: 5
Training loss: 2.2516229152679443
Validation loss: 2.356183933955367

Epoch: 6| Step: 6
Training loss: 2.892615556716919
Validation loss: 2.3579688584932716

Epoch: 6| Step: 7
Training loss: 3.150498390197754
Validation loss: 2.326608991110197

Epoch: 6| Step: 8
Training loss: 2.4209518432617188
Validation loss: 2.3086911580895864

Epoch: 6| Step: 9
Training loss: 1.9595401287078857
Validation loss: 2.291289037273776

Epoch: 6| Step: 10
Training loss: 2.5709853172302246
Validation loss: 2.283270003975079

Epoch: 6| Step: 11
Training loss: 2.073556661605835
Validation loss: 2.2828796486700735

Epoch: 6| Step: 12
Training loss: 2.7093353271484375
Validation loss: 2.2867425487887476

Epoch: 6| Step: 13
Training loss: 2.636453151702881
Validation loss: 2.2896276084325646

Epoch: 46| Step: 0
Training loss: 2.0506134033203125
Validation loss: 2.2953406559523715

Epoch: 6| Step: 1
Training loss: 2.2857918739318848
Validation loss: 2.322530197840865

Epoch: 6| Step: 2
Training loss: 2.562588691711426
Validation loss: 2.345516930344284

Epoch: 6| Step: 3
Training loss: 2.6583001613616943
Validation loss: 2.3793243567148843

Epoch: 6| Step: 4
Training loss: 2.436081886291504
Validation loss: 2.3807758669699393

Epoch: 6| Step: 5
Training loss: 2.4563345909118652
Validation loss: 2.376309843473537

Epoch: 6| Step: 6
Training loss: 2.1843512058258057
Validation loss: 2.370306594397432

Epoch: 6| Step: 7
Training loss: 2.950035572052002
Validation loss: 2.341526000730453

Epoch: 6| Step: 8
Training loss: 1.8880773782730103
Validation loss: 2.3215843964648504

Epoch: 6| Step: 9
Training loss: 2.8262672424316406
Validation loss: 2.2884865986403597

Epoch: 6| Step: 10
Training loss: 3.04443097114563
Validation loss: 2.282701405145789

Epoch: 6| Step: 11
Training loss: 2.844175338745117
Validation loss: 2.2813484335458405

Epoch: 6| Step: 12
Training loss: 2.7612009048461914
Validation loss: 2.2914094258380193

Epoch: 6| Step: 13
Training loss: 3.6037609577178955
Validation loss: 2.291650228602912

Epoch: 47| Step: 0
Training loss: 2.6894707679748535
Validation loss: 2.301127872159404

Epoch: 6| Step: 1
Training loss: 2.5872879028320312
Validation loss: 2.305392378120012

Epoch: 6| Step: 2
Training loss: 2.782283306121826
Validation loss: 2.3030343658180645

Epoch: 6| Step: 3
Training loss: 2.6705002784729004
Validation loss: 2.3035296368342575

Epoch: 6| Step: 4
Training loss: 2.619774341583252
Validation loss: 2.285710350159676

Epoch: 6| Step: 5
Training loss: 2.3431432247161865
Validation loss: 2.2807818843472387

Epoch: 6| Step: 6
Training loss: 2.910205841064453
Validation loss: 2.2766695355856292

Epoch: 6| Step: 7
Training loss: 2.6725687980651855
Validation loss: 2.278995447261359

Epoch: 6| Step: 8
Training loss: 2.9119372367858887
Validation loss: 2.283454184891075

Epoch: 6| Step: 9
Training loss: 2.441880702972412
Validation loss: 2.288371930840195

Epoch: 6| Step: 10
Training loss: 2.4356777667999268
Validation loss: 2.2999148112471386

Epoch: 6| Step: 11
Training loss: 1.7614192962646484
Validation loss: 2.3129474501455984

Epoch: 6| Step: 12
Training loss: 2.0870413780212402
Validation loss: 2.3311549514852543

Epoch: 6| Step: 13
Training loss: 3.7748172283172607
Validation loss: 2.3385478860588482

Epoch: 48| Step: 0
Training loss: 2.5196404457092285
Validation loss: 2.3239906167471283

Epoch: 6| Step: 1
Training loss: 2.1148900985717773
Validation loss: 2.3079591540880102

Epoch: 6| Step: 2
Training loss: 2.5320661067962646
Validation loss: 2.287954012552897

Epoch: 6| Step: 3
Training loss: 2.8969039916992188
Validation loss: 2.2717958983554634

Epoch: 6| Step: 4
Training loss: 2.3082058429718018
Validation loss: 2.2743180631309428

Epoch: 6| Step: 5
Training loss: 2.814075231552124
Validation loss: 2.275367162560904

Epoch: 6| Step: 6
Training loss: 3.013159990310669
Validation loss: 2.2827154872237996

Epoch: 6| Step: 7
Training loss: 2.790134906768799
Validation loss: 2.27579160915908

Epoch: 6| Step: 8
Training loss: 2.221672534942627
Validation loss: 2.2741898900719097

Epoch: 6| Step: 9
Training loss: 3.208070755004883
Validation loss: 2.276754371581539

Epoch: 6| Step: 10
Training loss: 1.5348782539367676
Validation loss: 2.274096409479777

Epoch: 6| Step: 11
Training loss: 2.535940408706665
Validation loss: 2.2739458237924883

Epoch: 6| Step: 12
Training loss: 2.6960349082946777
Validation loss: 2.2800236927565707

Epoch: 6| Step: 13
Training loss: 2.692206382751465
Validation loss: 2.2858979163631314

Epoch: 49| Step: 0
Training loss: 2.453542947769165
Validation loss: 2.2711905971650155

Epoch: 6| Step: 1
Training loss: 2.9411442279815674
Validation loss: 2.2657520027570826

Epoch: 6| Step: 2
Training loss: 2.022289991378784
Validation loss: 2.256366632317984

Epoch: 6| Step: 3
Training loss: 2.8841984272003174
Validation loss: 2.257517509562995

Epoch: 6| Step: 4
Training loss: 2.1225104331970215
Validation loss: 2.2578692846400763

Epoch: 6| Step: 5
Training loss: 2.4314799308776855
Validation loss: 2.2445442458634735

Epoch: 6| Step: 6
Training loss: 2.7639212608337402
Validation loss: 2.249687592188517

Epoch: 6| Step: 7
Training loss: 2.1158976554870605
Validation loss: 2.253583469698506

Epoch: 6| Step: 8
Training loss: 1.5600450038909912
Validation loss: 2.247976154409429

Epoch: 6| Step: 9
Training loss: 3.158766031265259
Validation loss: 2.24964016355494

Epoch: 6| Step: 10
Training loss: 2.8768858909606934
Validation loss: 2.2474889191248084

Epoch: 6| Step: 11
Training loss: 2.5030572414398193
Validation loss: 2.255688746770223

Epoch: 6| Step: 12
Training loss: 2.7086377143859863
Validation loss: 2.25713772414833

Epoch: 6| Step: 13
Training loss: 3.473374366760254
Validation loss: 2.250850785163141

Epoch: 50| Step: 0
Training loss: 2.3034143447875977
Validation loss: 2.2522316978823755

Epoch: 6| Step: 1
Training loss: 2.4386260509490967
Validation loss: 2.257993580192648

Epoch: 6| Step: 2
Training loss: 2.7414097785949707
Validation loss: 2.272334098815918

Epoch: 6| Step: 3
Training loss: 2.0544629096984863
Validation loss: 2.2827212554152294

Epoch: 6| Step: 4
Training loss: 2.846421241760254
Validation loss: 2.281264615315263

Epoch: 6| Step: 5
Training loss: 2.820833206176758
Validation loss: 2.2691535488251717

Epoch: 6| Step: 6
Training loss: 2.9190335273742676
Validation loss: 2.2542168863358034

Epoch: 6| Step: 7
Training loss: 2.7058682441711426
Validation loss: 2.2514912543758268

Epoch: 6| Step: 8
Training loss: 2.627340793609619
Validation loss: 2.251908584307599

Epoch: 6| Step: 9
Training loss: 2.7547597885131836
Validation loss: 2.2509982880725654

Epoch: 6| Step: 10
Training loss: 2.482205390930176
Validation loss: 2.2425796447261686

Epoch: 6| Step: 11
Training loss: 2.323960304260254
Validation loss: 2.2417135777011996

Epoch: 6| Step: 12
Training loss: 2.317397117614746
Validation loss: 2.244116780578449

Epoch: 6| Step: 13
Training loss: 1.9536364078521729
Validation loss: 2.2453334382785264

Testing loss: 2.4669047488106624

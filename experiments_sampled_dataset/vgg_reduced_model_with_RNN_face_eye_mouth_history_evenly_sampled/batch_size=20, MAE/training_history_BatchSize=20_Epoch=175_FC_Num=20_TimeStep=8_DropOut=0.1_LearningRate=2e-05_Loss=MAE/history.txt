Epoch: 1| Step: 0
Training loss: 5.0884528160095215
Validation loss: 5.105058121424849

Epoch: 5| Step: 1
Training loss: 5.204684257507324
Validation loss: 5.085018829632831

Epoch: 5| Step: 2
Training loss: 5.003073215484619
Validation loss: 5.062592168008128

Epoch: 5| Step: 3
Training loss: 4.599699974060059
Validation loss: 5.036774845533474

Epoch: 5| Step: 4
Training loss: 3.5435783863067627
Validation loss: 5.007863680521647

Epoch: 5| Step: 5
Training loss: 5.489270210266113
Validation loss: 4.975402965340563

Epoch: 5| Step: 6
Training loss: 4.904052734375
Validation loss: 4.939151594715733

Epoch: 5| Step: 7
Training loss: 4.76958703994751
Validation loss: 4.897658819793373

Epoch: 5| Step: 8
Training loss: 4.603618621826172
Validation loss: 4.851372380410472

Epoch: 5| Step: 9
Training loss: 5.576344966888428
Validation loss: 4.798738305286695

Epoch: 5| Step: 10
Training loss: 3.245772361755371
Validation loss: 4.740426227610598

Epoch: 2| Step: 0
Training loss: 4.064418315887451
Validation loss: 4.675259251748362

Epoch: 5| Step: 1
Training loss: 4.6194939613342285
Validation loss: 4.604747531234577

Epoch: 5| Step: 2
Training loss: 4.278511047363281
Validation loss: 4.5267636647788425

Epoch: 5| Step: 3
Training loss: 4.050548553466797
Validation loss: 4.441637259657665

Epoch: 5| Step: 4
Training loss: 3.493025302886963
Validation loss: 4.352420012156169

Epoch: 5| Step: 5
Training loss: 4.1681365966796875
Validation loss: 4.260825387893185

Epoch: 5| Step: 6
Training loss: 3.324263095855713
Validation loss: 4.16948668162028

Epoch: 5| Step: 7
Training loss: 3.043766736984253
Validation loss: 4.082183909672563

Epoch: 5| Step: 8
Training loss: 4.884081840515137
Validation loss: 3.9968977000123713

Epoch: 5| Step: 9
Training loss: 3.7054696083068848
Validation loss: 3.9211224022731987

Epoch: 5| Step: 10
Training loss: 5.391716003417969
Validation loss: 3.853148542424684

Epoch: 3| Step: 0
Training loss: 3.688833236694336
Validation loss: 3.790739351703275

Epoch: 5| Step: 1
Training loss: 2.9927010536193848
Validation loss: 3.732881687020743

Epoch: 5| Step: 2
Training loss: 4.058790683746338
Validation loss: 3.688615337494881

Epoch: 5| Step: 3
Training loss: 3.556938886642456
Validation loss: 3.6434719870167394

Epoch: 5| Step: 4
Training loss: 4.185297966003418
Validation loss: 3.607721677390478

Epoch: 5| Step: 5
Training loss: 2.9978556632995605
Validation loss: 3.5759700805910173

Epoch: 5| Step: 6
Training loss: 3.4516310691833496
Validation loss: 3.5516837668675247

Epoch: 5| Step: 7
Training loss: 3.979008436203003
Validation loss: 3.5272203671034945

Epoch: 5| Step: 8
Training loss: 3.0349955558776855
Validation loss: 3.5045803080322924

Epoch: 5| Step: 9
Training loss: 3.041962146759033
Validation loss: 3.4846856824813353

Epoch: 5| Step: 10
Training loss: 3.6765947341918945
Validation loss: 3.462749873438189

Epoch: 4| Step: 0
Training loss: 3.1267170906066895
Validation loss: 3.4386506413900726

Epoch: 5| Step: 1
Training loss: 2.801003932952881
Validation loss: 3.414225004052603

Epoch: 5| Step: 2
Training loss: 2.8086729049682617
Validation loss: 3.387561090530888

Epoch: 5| Step: 3
Training loss: 3.496311664581299
Validation loss: 3.372697014962473

Epoch: 5| Step: 4
Training loss: 3.1822266578674316
Validation loss: 3.344895201344644

Epoch: 5| Step: 5
Training loss: 3.6406280994415283
Validation loss: 3.3217770925132175

Epoch: 5| Step: 6
Training loss: 2.8376171588897705
Validation loss: 3.3020167325132634

Epoch: 5| Step: 7
Training loss: 3.703933000564575
Validation loss: 3.285405517906271

Epoch: 5| Step: 8
Training loss: 3.0898990631103516
Validation loss: 3.268244033218712

Epoch: 5| Step: 9
Training loss: 3.925374984741211
Validation loss: 3.254477300951558

Epoch: 5| Step: 10
Training loss: 3.701978921890259
Validation loss: 3.2377473667103756

Epoch: 5| Step: 0
Training loss: 2.4454855918884277
Validation loss: 3.226629449475196

Epoch: 5| Step: 1
Training loss: 4.140758514404297
Validation loss: 3.2130570437318537

Epoch: 5| Step: 2
Training loss: 3.3288559913635254
Validation loss: 3.2002367563145135

Epoch: 5| Step: 3
Training loss: 2.875084400177002
Validation loss: 3.1886270866599133

Epoch: 5| Step: 4
Training loss: 3.7337074279785156
Validation loss: 3.177480359231272

Epoch: 5| Step: 5
Training loss: 2.4886016845703125
Validation loss: 3.1672336773205827

Epoch: 5| Step: 6
Training loss: 3.5068740844726562
Validation loss: 3.157016113240232

Epoch: 5| Step: 7
Training loss: 3.38407826423645
Validation loss: 3.1441956848226567

Epoch: 5| Step: 8
Training loss: 2.6827335357666016
Validation loss: 3.132520370585944

Epoch: 5| Step: 9
Training loss: 3.056020736694336
Validation loss: 3.1192025189758628

Epoch: 5| Step: 10
Training loss: 3.2800445556640625
Validation loss: 3.107630375892885

Epoch: 6| Step: 0
Training loss: 2.651284694671631
Validation loss: 3.0970233307089856

Epoch: 5| Step: 1
Training loss: 3.6742405891418457
Validation loss: 3.086030331991052

Epoch: 5| Step: 2
Training loss: 3.192340135574341
Validation loss: 3.099913397142964

Epoch: 5| Step: 3
Training loss: 3.442300796508789
Validation loss: 3.060973810893233

Epoch: 5| Step: 4
Training loss: 2.44726300239563
Validation loss: 3.056284640424995

Epoch: 5| Step: 5
Training loss: 3.840092897415161
Validation loss: 3.052457868411977

Epoch: 5| Step: 6
Training loss: 3.4846129417419434
Validation loss: 3.0478982053777224

Epoch: 5| Step: 7
Training loss: 3.6061713695526123
Validation loss: 3.0342591475414973

Epoch: 5| Step: 8
Training loss: 2.986110210418701
Validation loss: 3.020972959456905

Epoch: 5| Step: 9
Training loss: 2.1291420459747314
Validation loss: 3.0084374079140286

Epoch: 5| Step: 10
Training loss: 2.591435194015503
Validation loss: 2.9954457949566584

Epoch: 7| Step: 0
Training loss: 3.62949800491333
Validation loss: 3.006313593156876

Epoch: 5| Step: 1
Training loss: 3.1142771244049072
Validation loss: 2.984284372739894

Epoch: 5| Step: 2
Training loss: 2.6070001125335693
Validation loss: 2.9546747669096916

Epoch: 5| Step: 3
Training loss: 3.1481549739837646
Validation loss: 2.9298214399686424

Epoch: 5| Step: 4
Training loss: 3.1250832080841064
Validation loss: 2.9133273273385982

Epoch: 5| Step: 5
Training loss: 2.420811414718628
Validation loss: 2.9164644877115884

Epoch: 5| Step: 6
Training loss: 2.8337936401367188
Validation loss: 2.9229265387340257

Epoch: 5| Step: 7
Training loss: 3.0276780128479004
Validation loss: 2.9193117798015638

Epoch: 5| Step: 8
Training loss: 2.6555533409118652
Validation loss: 2.9146531140932472

Epoch: 5| Step: 9
Training loss: 3.0470938682556152
Validation loss: 2.8936297714069323

Epoch: 5| Step: 10
Training loss: 3.7150824069976807
Validation loss: 2.8865504393013577

Epoch: 8| Step: 0
Training loss: 3.0632338523864746
Validation loss: 2.876140932882986

Epoch: 5| Step: 1
Training loss: 3.4794883728027344
Validation loss: 2.867191381351922

Epoch: 5| Step: 2
Training loss: 2.7836010456085205
Validation loss: 2.8548118914327314

Epoch: 5| Step: 3
Training loss: 2.8863983154296875
Validation loss: 2.8454046146844023

Epoch: 5| Step: 4
Training loss: 3.5693581104278564
Validation loss: 2.838959627254035

Epoch: 5| Step: 5
Training loss: 3.066467046737671
Validation loss: 2.8364285294727614

Epoch: 5| Step: 6
Training loss: 2.365823268890381
Validation loss: 2.8271569436596287

Epoch: 5| Step: 7
Training loss: 3.1716277599334717
Validation loss: 2.8191606767715944

Epoch: 5| Step: 8
Training loss: 3.0990965366363525
Validation loss: 2.811721658193937

Epoch: 5| Step: 9
Training loss: 2.8421590328216553
Validation loss: 2.8101685585514193

Epoch: 5| Step: 10
Training loss: 2.0494327545166016
Validation loss: 2.8078208149120374

Epoch: 9| Step: 0
Training loss: 2.7315096855163574
Validation loss: 2.816952520801175

Epoch: 5| Step: 1
Training loss: 3.9215214252471924
Validation loss: 2.799418359674433

Epoch: 5| Step: 2
Training loss: 3.4326140880584717
Validation loss: 2.790100307874782

Epoch: 5| Step: 3
Training loss: 2.658867359161377
Validation loss: 2.7721492475078953

Epoch: 5| Step: 4
Training loss: 3.3696296215057373
Validation loss: 2.7657964357765774

Epoch: 5| Step: 5
Training loss: 3.277362823486328
Validation loss: 2.755306372078516

Epoch: 5| Step: 6
Training loss: 2.786996364593506
Validation loss: 2.7496498425801597

Epoch: 5| Step: 7
Training loss: 2.656639814376831
Validation loss: 2.746431173816804

Epoch: 5| Step: 8
Training loss: 2.865269422531128
Validation loss: 2.7471529847832135

Epoch: 5| Step: 9
Training loss: 2.1258578300476074
Validation loss: 2.7392602633404475

Epoch: 5| Step: 10
Training loss: 2.0684924125671387
Validation loss: 2.726854852450791

Epoch: 10| Step: 0
Training loss: 3.972352981567383
Validation loss: 2.7231698061830256

Epoch: 5| Step: 1
Training loss: 2.5343008041381836
Validation loss: 2.717435098463489

Epoch: 5| Step: 2
Training loss: 2.5641231536865234
Validation loss: 2.710156450989426

Epoch: 5| Step: 3
Training loss: 2.84245228767395
Validation loss: 2.7059419052575224

Epoch: 5| Step: 4
Training loss: 2.9488227367401123
Validation loss: 2.7009984857292584

Epoch: 5| Step: 5
Training loss: 3.262709140777588
Validation loss: 2.70292575897709

Epoch: 5| Step: 6
Training loss: 2.6261849403381348
Validation loss: 2.7030854968614477

Epoch: 5| Step: 7
Training loss: 3.0118651390075684
Validation loss: 2.6946317457383677

Epoch: 5| Step: 8
Training loss: 2.3490653038024902
Validation loss: 2.6909369140542965

Epoch: 5| Step: 9
Training loss: 2.6397266387939453
Validation loss: 2.6982218014296664

Epoch: 5| Step: 10
Training loss: 2.856261968612671
Validation loss: 2.7033588450442076

Epoch: 11| Step: 0
Training loss: 2.1438956260681152
Validation loss: 2.6875572281499065

Epoch: 5| Step: 1
Training loss: 2.50669527053833
Validation loss: 2.6883435377510647

Epoch: 5| Step: 2
Training loss: 3.4000563621520996
Validation loss: 2.6831753446209814

Epoch: 5| Step: 3
Training loss: 2.430482864379883
Validation loss: 2.6776803693463727

Epoch: 5| Step: 4
Training loss: 2.784273147583008
Validation loss: 2.673618180777437

Epoch: 5| Step: 5
Training loss: 3.2411792278289795
Validation loss: 2.675637088796144

Epoch: 5| Step: 6
Training loss: 2.6309595108032227
Validation loss: 2.6759296489018265

Epoch: 5| Step: 7
Training loss: 3.2450225353240967
Validation loss: 2.667620961384107

Epoch: 5| Step: 8
Training loss: 2.485650062561035
Validation loss: 2.6646594565401793

Epoch: 5| Step: 9
Training loss: 3.659703493118286
Validation loss: 2.6602746799427974

Epoch: 5| Step: 10
Training loss: 2.881898880004883
Validation loss: 2.672109483390726

Epoch: 12| Step: 0
Training loss: 2.9585483074188232
Validation loss: 2.6664409611814763

Epoch: 5| Step: 1
Training loss: 3.460257053375244
Validation loss: 2.6624981626387565

Epoch: 5| Step: 2
Training loss: 2.3161654472351074
Validation loss: 2.6609651555297194

Epoch: 5| Step: 3
Training loss: 2.464700698852539
Validation loss: 2.662820795530914

Epoch: 5| Step: 4
Training loss: 2.723440170288086
Validation loss: 2.6587644930808776

Epoch: 5| Step: 5
Training loss: 3.92620587348938
Validation loss: 2.655523282225414

Epoch: 5| Step: 6
Training loss: 2.9597761631011963
Validation loss: 2.6517206417616976

Epoch: 5| Step: 7
Training loss: 2.5295660495758057
Validation loss: 2.650260575356022

Epoch: 5| Step: 8
Training loss: 2.5809574127197266
Validation loss: 2.6481442759113927

Epoch: 5| Step: 9
Training loss: 2.788705587387085
Validation loss: 2.642561569008776

Epoch: 5| Step: 10
Training loss: 2.465165853500366
Validation loss: 2.6354421236181773

Epoch: 13| Step: 0
Training loss: 3.0565693378448486
Validation loss: 2.626189657436904

Epoch: 5| Step: 1
Training loss: 2.1469573974609375
Validation loss: 2.624168813869517

Epoch: 5| Step: 2
Training loss: 3.0818724632263184
Validation loss: 2.6251926793847034

Epoch: 5| Step: 3
Training loss: 2.877835512161255
Validation loss: 2.660954452330066

Epoch: 5| Step: 4
Training loss: 3.2432193756103516
Validation loss: 2.6942997696579143

Epoch: 5| Step: 5
Training loss: 2.6142020225524902
Validation loss: 2.69125686922381

Epoch: 5| Step: 6
Training loss: 3.0546822547912598
Validation loss: 2.688289862807079

Epoch: 5| Step: 7
Training loss: 3.5742359161376953
Validation loss: 2.685598942541307

Epoch: 5| Step: 8
Training loss: 2.437819719314575
Validation loss: 2.698402771385767

Epoch: 5| Step: 9
Training loss: 2.37337064743042
Validation loss: 2.6975820243999524

Epoch: 5| Step: 10
Training loss: 2.9346179962158203
Validation loss: 2.6940267419302337

Epoch: 14| Step: 0
Training loss: 3.043668746948242
Validation loss: 2.6784348513490412

Epoch: 5| Step: 1
Training loss: 2.982130527496338
Validation loss: 2.6783379995694725

Epoch: 5| Step: 2
Training loss: 3.124911069869995
Validation loss: 2.6732151431422078

Epoch: 5| Step: 3
Training loss: 3.6021556854248047
Validation loss: 2.671502282542567

Epoch: 5| Step: 4
Training loss: 2.5835251808166504
Validation loss: 2.672647773578603

Epoch: 5| Step: 5
Training loss: 1.9559532403945923
Validation loss: 2.6703033293447187

Epoch: 5| Step: 6
Training loss: 2.693509578704834
Validation loss: 2.6673836015885874

Epoch: 5| Step: 7
Training loss: 3.431208372116089
Validation loss: 2.6638230687828472

Epoch: 5| Step: 8
Training loss: 2.4045281410217285
Validation loss: 2.664172039237074

Epoch: 5| Step: 9
Training loss: 2.683411121368408
Validation loss: 2.661568041770689

Epoch: 5| Step: 10
Training loss: 2.796992540359497
Validation loss: 2.6574181946375037

Epoch: 15| Step: 0
Training loss: 2.804961919784546
Validation loss: 2.6548346191324215

Epoch: 5| Step: 1
Training loss: 3.143754243850708
Validation loss: 2.65076631628057

Epoch: 5| Step: 2
Training loss: 2.7610998153686523
Validation loss: 2.646752765101771

Epoch: 5| Step: 3
Training loss: 2.671408176422119
Validation loss: 2.6441076083849837

Epoch: 5| Step: 4
Training loss: 3.2510910034179688
Validation loss: 2.641941637121221

Epoch: 5| Step: 5
Training loss: 2.6288740634918213
Validation loss: 2.6403372082658993

Epoch: 5| Step: 6
Training loss: 3.199289083480835
Validation loss: 2.635871700061265

Epoch: 5| Step: 7
Training loss: 2.7750084400177
Validation loss: 2.636881753962527

Epoch: 5| Step: 8
Training loss: 2.504983425140381
Validation loss: 2.6308548629924817

Epoch: 5| Step: 9
Training loss: 3.005176067352295
Validation loss: 2.6309776075424685

Epoch: 5| Step: 10
Training loss: 2.272916078567505
Validation loss: 2.6308626615872948

Epoch: 16| Step: 0
Training loss: 2.316037893295288
Validation loss: 2.6353231399290022

Epoch: 5| Step: 1
Training loss: 3.1036219596862793
Validation loss: 2.6326848050599456

Epoch: 5| Step: 2
Training loss: 3.0272889137268066
Validation loss: 2.634118010920863

Epoch: 5| Step: 3
Training loss: 3.3672142028808594
Validation loss: 2.633340779171195

Epoch: 5| Step: 4
Training loss: 2.7693238258361816
Validation loss: 2.620891301862655

Epoch: 5| Step: 5
Training loss: 2.2310590744018555
Validation loss: 2.6176990078341578

Epoch: 5| Step: 6
Training loss: 2.6426830291748047
Validation loss: 2.601996306450136

Epoch: 5| Step: 7
Training loss: 2.5232200622558594
Validation loss: 2.5694898354109896

Epoch: 5| Step: 8
Training loss: 2.969247341156006
Validation loss: 2.6037078801021782

Epoch: 5| Step: 9
Training loss: 3.2500147819519043
Validation loss: 2.588343774118731

Epoch: 5| Step: 10
Training loss: 2.593442916870117
Validation loss: 2.585226753706573

Epoch: 17| Step: 0
Training loss: 2.5338690280914307
Validation loss: 2.581303342696159

Epoch: 5| Step: 1
Training loss: 3.1058106422424316
Validation loss: 2.575814800877725

Epoch: 5| Step: 2
Training loss: 2.3120200634002686
Validation loss: 2.565487141250282

Epoch: 5| Step: 3
Training loss: 2.8017630577087402
Validation loss: 2.568828921164236

Epoch: 5| Step: 4
Training loss: 2.8346686363220215
Validation loss: 2.582700639642695

Epoch: 5| Step: 5
Training loss: 3.269437313079834
Validation loss: 2.5785211081145913

Epoch: 5| Step: 6
Training loss: 2.258491277694702
Validation loss: 2.560505854186191

Epoch: 5| Step: 7
Training loss: 3.3489270210266113
Validation loss: 2.5512524650942896

Epoch: 5| Step: 8
Training loss: 2.6755285263061523
Validation loss: 2.550042152404785

Epoch: 5| Step: 9
Training loss: 2.7664380073547363
Validation loss: 2.5501740030063096

Epoch: 5| Step: 10
Training loss: 2.610569953918457
Validation loss: 2.5626151895010345

Epoch: 18| Step: 0
Training loss: 2.947533369064331
Validation loss: 2.593699247606339

Epoch: 5| Step: 1
Training loss: 2.9430761337280273
Validation loss: 2.640109928705359

Epoch: 5| Step: 2
Training loss: 2.8421034812927246
Validation loss: 2.6032243133873068

Epoch: 5| Step: 3
Training loss: 2.539252758026123
Validation loss: 2.5616611972931893

Epoch: 5| Step: 4
Training loss: 2.6293680667877197
Validation loss: 2.5430877593255814

Epoch: 5| Step: 5
Training loss: 2.4027953147888184
Validation loss: 2.5409454402103218

Epoch: 5| Step: 6
Training loss: 2.686005115509033
Validation loss: 2.5494067438187136

Epoch: 5| Step: 7
Training loss: 2.5675406455993652
Validation loss: 2.5482677170025405

Epoch: 5| Step: 8
Training loss: 3.4161362648010254
Validation loss: 2.540742779290804

Epoch: 5| Step: 9
Training loss: 2.522548198699951
Validation loss: 2.5324206954689434

Epoch: 5| Step: 10
Training loss: 2.9363346099853516
Validation loss: 2.5289511603693806

Epoch: 19| Step: 0
Training loss: 3.047043800354004
Validation loss: 2.5687946299070954

Epoch: 5| Step: 1
Training loss: 2.6277542114257812
Validation loss: 2.597768388768678

Epoch: 5| Step: 2
Training loss: 2.2841567993164062
Validation loss: 2.6134216247066373

Epoch: 5| Step: 3
Training loss: 3.5297329425811768
Validation loss: 2.58505646387736

Epoch: 5| Step: 4
Training loss: 2.617431163787842
Validation loss: 2.5268415609995523

Epoch: 5| Step: 5
Training loss: 2.50434947013855
Validation loss: 2.515658114546089

Epoch: 5| Step: 6
Training loss: 2.300652503967285
Validation loss: 2.5224241825842086

Epoch: 5| Step: 7
Training loss: 2.9571144580841064
Validation loss: 2.525046858736264

Epoch: 5| Step: 8
Training loss: 2.998986005783081
Validation loss: 2.5256810931749243

Epoch: 5| Step: 9
Training loss: 3.2246830463409424
Validation loss: 2.516694853382726

Epoch: 5| Step: 10
Training loss: 2.035435914993286
Validation loss: 2.525228205547538

Epoch: 20| Step: 0
Training loss: 2.44181489944458
Validation loss: 2.5748641875482376

Epoch: 5| Step: 1
Training loss: 3.232041835784912
Validation loss: 2.6016722648374495

Epoch: 5| Step: 2
Training loss: 2.09446382522583
Validation loss: 2.6090730415877474

Epoch: 5| Step: 3
Training loss: 3.1581151485443115
Validation loss: 2.6301505693825344

Epoch: 5| Step: 4
Training loss: 2.5954880714416504
Validation loss: 2.594803361482518

Epoch: 5| Step: 5
Training loss: 2.2656025886535645
Validation loss: 2.5408857458381244

Epoch: 5| Step: 6
Training loss: 2.4323158264160156
Validation loss: 2.517899318407941

Epoch: 5| Step: 7
Training loss: 3.187624931335449
Validation loss: 2.539431675787895

Epoch: 5| Step: 8
Training loss: 3.0082859992980957
Validation loss: 2.5489435170286443

Epoch: 5| Step: 9
Training loss: 3.693463087081909
Validation loss: 2.55774155739815

Epoch: 5| Step: 10
Training loss: 2.0926120281219482
Validation loss: 2.500594667209092

Epoch: 21| Step: 0
Training loss: 2.53385853767395
Validation loss: 2.5127876291992846

Epoch: 5| Step: 1
Training loss: 2.2031733989715576
Validation loss: 2.5503958015031714

Epoch: 5| Step: 2
Training loss: 2.6990411281585693
Validation loss: 2.5677590062541347

Epoch: 5| Step: 3
Training loss: 2.138089656829834
Validation loss: 2.5607208846717753

Epoch: 5| Step: 4
Training loss: 2.924499988555908
Validation loss: 2.5284156235315467

Epoch: 5| Step: 5
Training loss: 3.490431547164917
Validation loss: 2.509655062870313

Epoch: 5| Step: 6
Training loss: 3.0784919261932373
Validation loss: 2.497472316988053

Epoch: 5| Step: 7
Training loss: 2.0059733390808105
Validation loss: 2.4889692311645835

Epoch: 5| Step: 8
Training loss: 3.8328468799591064
Validation loss: 2.4942558914102535

Epoch: 5| Step: 9
Training loss: 2.624546527862549
Validation loss: 2.501686365373673

Epoch: 5| Step: 10
Training loss: 2.4474732875823975
Validation loss: 2.505840088731499

Epoch: 22| Step: 0
Training loss: 2.4058876037597656
Validation loss: 2.527726919420304

Epoch: 5| Step: 1
Training loss: 3.209388256072998
Validation loss: 2.54992708083122

Epoch: 5| Step: 2
Training loss: 2.6849584579467773
Validation loss: 2.4970421303984938

Epoch: 5| Step: 3
Training loss: 2.402592182159424
Validation loss: 2.4834199233721663

Epoch: 5| Step: 4
Training loss: 2.8768110275268555
Validation loss: 2.483844795534688

Epoch: 5| Step: 5
Training loss: 2.2555198669433594
Validation loss: 2.486599752979894

Epoch: 5| Step: 6
Training loss: 2.629793405532837
Validation loss: 2.506649407007361

Epoch: 5| Step: 7
Training loss: 3.2191929817199707
Validation loss: 2.510785654026975

Epoch: 5| Step: 8
Training loss: 2.5681583881378174
Validation loss: 2.497385383934103

Epoch: 5| Step: 9
Training loss: 2.223397731781006
Validation loss: 2.4913091531363865

Epoch: 5| Step: 10
Training loss: 3.514051675796509
Validation loss: 2.4855780191318964

Epoch: 23| Step: 0
Training loss: 3.4806294441223145
Validation loss: 2.4808914123042936

Epoch: 5| Step: 1
Training loss: 2.3102896213531494
Validation loss: 2.476705197365053

Epoch: 5| Step: 2
Training loss: 2.172577381134033
Validation loss: 2.4783392337060746

Epoch: 5| Step: 3
Training loss: 2.207418918609619
Validation loss: 2.47712266829706

Epoch: 5| Step: 4
Training loss: 2.9052066802978516
Validation loss: 2.4745139101500153

Epoch: 5| Step: 5
Training loss: 3.034839630126953
Validation loss: 2.4735421185852378

Epoch: 5| Step: 6
Training loss: 2.635540723800659
Validation loss: 2.4715086106331117

Epoch: 5| Step: 7
Training loss: 2.314936637878418
Validation loss: 2.4735825215616534

Epoch: 5| Step: 8
Training loss: 2.827932834625244
Validation loss: 2.469053045395882

Epoch: 5| Step: 9
Training loss: 2.826920747756958
Validation loss: 2.468987064976846

Epoch: 5| Step: 10
Training loss: 3.007521629333496
Validation loss: 2.4791807128537084

Epoch: 24| Step: 0
Training loss: 2.483335256576538
Validation loss: 2.489189955496019

Epoch: 5| Step: 1
Training loss: 2.585294246673584
Validation loss: 2.5051036355316

Epoch: 5| Step: 2
Training loss: 2.6529102325439453
Validation loss: 2.529831204363095

Epoch: 5| Step: 3
Training loss: 3.0553195476531982
Validation loss: 2.517007121475794

Epoch: 5| Step: 4
Training loss: 2.532273769378662
Validation loss: 2.484006461276803

Epoch: 5| Step: 5
Training loss: 2.41395902633667
Validation loss: 2.466829123035554

Epoch: 5| Step: 6
Training loss: 2.310138463973999
Validation loss: 2.4661975342740297

Epoch: 5| Step: 7
Training loss: 2.991410732269287
Validation loss: 2.473646561304728

Epoch: 5| Step: 8
Training loss: 2.114431142807007
Validation loss: 2.475766574182818

Epoch: 5| Step: 9
Training loss: 3.3093574047088623
Validation loss: 2.487277157845036

Epoch: 5| Step: 10
Training loss: 3.2992920875549316
Validation loss: 2.471108482730004

Epoch: 25| Step: 0
Training loss: 3.1539852619171143
Validation loss: 2.4558898761708248

Epoch: 5| Step: 1
Training loss: 2.501194477081299
Validation loss: 2.4537591549658004

Epoch: 5| Step: 2
Training loss: 2.9091975688934326
Validation loss: 2.4589124571892524

Epoch: 5| Step: 3
Training loss: 2.245307207107544
Validation loss: 2.4604652748313

Epoch: 5| Step: 4
Training loss: 2.581206798553467
Validation loss: 2.4663399650204565

Epoch: 5| Step: 5
Training loss: 2.391636610031128
Validation loss: 2.4586970011393228

Epoch: 5| Step: 6
Training loss: 2.4182467460632324
Validation loss: 2.461724667138951

Epoch: 5| Step: 7
Training loss: 2.235031843185425
Validation loss: 2.458606125206076

Epoch: 5| Step: 8
Training loss: 2.8564865589141846
Validation loss: 2.4449526007457445

Epoch: 5| Step: 9
Training loss: 3.371671199798584
Validation loss: 2.433533599299769

Epoch: 5| Step: 10
Training loss: 2.8123459815979004
Validation loss: 2.4309835690324024

Epoch: 26| Step: 0
Training loss: 2.1986865997314453
Validation loss: 2.434394483925194

Epoch: 5| Step: 1
Training loss: 2.901146411895752
Validation loss: 2.4402914508696525

Epoch: 5| Step: 2
Training loss: 2.8440322875976562
Validation loss: 2.436590228029477

Epoch: 5| Step: 3
Training loss: 2.9482431411743164
Validation loss: 2.4384469268142537

Epoch: 5| Step: 4
Training loss: 3.1809935569763184
Validation loss: 2.4375895300219135

Epoch: 5| Step: 5
Training loss: 2.2634897232055664
Validation loss: 2.435776364418768

Epoch: 5| Step: 6
Training loss: 2.1116280555725098
Validation loss: 2.4381753629253757

Epoch: 5| Step: 7
Training loss: 2.781163215637207
Validation loss: 2.432561311670529

Epoch: 5| Step: 8
Training loss: 2.9111690521240234
Validation loss: 2.4403583977812078

Epoch: 5| Step: 9
Training loss: 2.4808108806610107
Validation loss: 2.431114646696275

Epoch: 5| Step: 10
Training loss: 2.706801176071167
Validation loss: 2.432224340336297

Epoch: 27| Step: 0
Training loss: 3.34302020072937
Validation loss: 2.4339220216197353

Epoch: 5| Step: 1
Training loss: 2.7447476387023926
Validation loss: 2.428486293362033

Epoch: 5| Step: 2
Training loss: 2.470372200012207
Validation loss: 2.435116083391251

Epoch: 5| Step: 3
Training loss: 2.4928171634674072
Validation loss: 2.425003379903814

Epoch: 5| Step: 4
Training loss: 3.2234694957733154
Validation loss: 2.4237001788231636

Epoch: 5| Step: 5
Training loss: 2.675102710723877
Validation loss: 2.4154033865979923

Epoch: 5| Step: 6
Training loss: 2.738438129425049
Validation loss: 2.4179908588368404

Epoch: 5| Step: 7
Training loss: 3.1285223960876465
Validation loss: 2.4163450041124896

Epoch: 5| Step: 8
Training loss: 1.9533519744873047
Validation loss: 2.4164569044625885

Epoch: 5| Step: 9
Training loss: 1.883805274963379
Validation loss: 2.419462884626081

Epoch: 5| Step: 10
Training loss: 2.6113994121551514
Validation loss: 2.4378917396709485

Epoch: 28| Step: 0
Training loss: 2.8441927433013916
Validation loss: 2.4556711873700543

Epoch: 5| Step: 1
Training loss: 2.486293315887451
Validation loss: 2.4734862645467124

Epoch: 5| Step: 2
Training loss: 2.359205961227417
Validation loss: 2.492752141849969

Epoch: 5| Step: 3
Training loss: 2.2419207096099854
Validation loss: 2.4578059642545638

Epoch: 5| Step: 4
Training loss: 2.1894309520721436
Validation loss: 2.4302482143525155

Epoch: 5| Step: 5
Training loss: 3.0738883018493652
Validation loss: 2.413695150806058

Epoch: 5| Step: 6
Training loss: 2.7982466220855713
Validation loss: 2.4313178062438965

Epoch: 5| Step: 7
Training loss: 3.2219130992889404
Validation loss: 2.4388203646547053

Epoch: 5| Step: 8
Training loss: 3.0467047691345215
Validation loss: 2.436628210929132

Epoch: 5| Step: 9
Training loss: 2.49841046333313
Validation loss: 2.4189629529112127

Epoch: 5| Step: 10
Training loss: 2.6451621055603027
Validation loss: 2.4090784595858667

Epoch: 29| Step: 0
Training loss: 2.877537965774536
Validation loss: 2.4044727048566266

Epoch: 5| Step: 1
Training loss: 1.8075599670410156
Validation loss: 2.408725248870029

Epoch: 5| Step: 2
Training loss: 2.970024585723877
Validation loss: 2.4120507753023537

Epoch: 5| Step: 3
Training loss: 2.412111282348633
Validation loss: 2.4090666719662246

Epoch: 5| Step: 4
Training loss: 3.0913310050964355
Validation loss: 2.4218301926889727

Epoch: 5| Step: 5
Training loss: 2.455777645111084
Validation loss: 2.4257503760758268

Epoch: 5| Step: 6
Training loss: 2.9112191200256348
Validation loss: 2.42891235761745

Epoch: 5| Step: 7
Training loss: 2.2102713584899902
Validation loss: 2.4319078588998444

Epoch: 5| Step: 8
Training loss: 2.7536566257476807
Validation loss: 2.450574803095992

Epoch: 5| Step: 9
Training loss: 3.0868237018585205
Validation loss: 2.4335443794086413

Epoch: 5| Step: 10
Training loss: 2.53933048248291
Validation loss: 2.420194233617475

Epoch: 30| Step: 0
Training loss: 3.240467071533203
Validation loss: 2.4150506552829536

Epoch: 5| Step: 1
Training loss: 2.5956130027770996
Validation loss: 2.4124385515848794

Epoch: 5| Step: 2
Training loss: 2.163515567779541
Validation loss: 2.409427929950017

Epoch: 5| Step: 3
Training loss: 2.214982509613037
Validation loss: 2.405125787181239

Epoch: 5| Step: 4
Training loss: 3.059868335723877
Validation loss: 2.4119242134914605

Epoch: 5| Step: 5
Training loss: 2.7871477603912354
Validation loss: 2.4191332478677072

Epoch: 5| Step: 6
Training loss: 3.2285561561584473
Validation loss: 2.457089485660676

Epoch: 5| Step: 7
Training loss: 2.328723430633545
Validation loss: 2.4788970793447187

Epoch: 5| Step: 8
Training loss: 2.208786725997925
Validation loss: 2.470971635592881

Epoch: 5| Step: 9
Training loss: 2.8021395206451416
Validation loss: 2.4592668382070397

Epoch: 5| Step: 10
Training loss: 2.6280815601348877
Validation loss: 2.4550344226180867

Epoch: 31| Step: 0
Training loss: 3.1194252967834473
Validation loss: 2.4048510905235045

Epoch: 5| Step: 1
Training loss: 3.3005528450012207
Validation loss: 2.4124976665742937

Epoch: 5| Step: 2
Training loss: 2.5643582344055176
Validation loss: 2.417461964391893

Epoch: 5| Step: 3
Training loss: 2.6789443492889404
Validation loss: 2.4279674483883764

Epoch: 5| Step: 4
Training loss: 2.7488248348236084
Validation loss: 2.4436238658043647

Epoch: 5| Step: 5
Training loss: 2.19584321975708
Validation loss: 2.4359164237976074

Epoch: 5| Step: 6
Training loss: 3.313469648361206
Validation loss: 2.429405097038515

Epoch: 5| Step: 7
Training loss: 2.1241910457611084
Validation loss: 2.412667461620864

Epoch: 5| Step: 8
Training loss: 2.199089288711548
Validation loss: 2.419689078484812

Epoch: 5| Step: 9
Training loss: 2.9635820388793945
Validation loss: 2.4343956516635035

Epoch: 5| Step: 10
Training loss: 2.069737434387207
Validation loss: 2.4476890333237185

Epoch: 32| Step: 0
Training loss: 2.53849720954895
Validation loss: 2.438085348375382

Epoch: 5| Step: 1
Training loss: 1.9826072454452515
Validation loss: 2.4333526293436685

Epoch: 5| Step: 2
Training loss: 2.898298740386963
Validation loss: 2.438278926316128

Epoch: 5| Step: 3
Training loss: 2.696789026260376
Validation loss: 2.431776028807445

Epoch: 5| Step: 4
Training loss: 2.900263786315918
Validation loss: 2.4444704901787544

Epoch: 5| Step: 5
Training loss: 3.1025516986846924
Validation loss: 2.417037712630405

Epoch: 5| Step: 6
Training loss: 3.1976749897003174
Validation loss: 2.4002045482717533

Epoch: 5| Step: 7
Training loss: 1.931239366531372
Validation loss: 2.39017669616207

Epoch: 5| Step: 8
Training loss: 2.911100149154663
Validation loss: 2.3833133200163483

Epoch: 5| Step: 9
Training loss: 2.493741512298584
Validation loss: 2.3872248280432915

Epoch: 5| Step: 10
Training loss: 2.378572463989258
Validation loss: 2.399084257823165

Epoch: 33| Step: 0
Training loss: 2.4501819610595703
Validation loss: 2.409919966933548

Epoch: 5| Step: 1
Training loss: 2.1204705238342285
Validation loss: 2.4149985749234437

Epoch: 5| Step: 2
Training loss: 2.7518529891967773
Validation loss: 2.4192541055781867

Epoch: 5| Step: 3
Training loss: 3.1950185298919678
Validation loss: 2.4159771011721705

Epoch: 5| Step: 4
Training loss: 2.2236876487731934
Validation loss: 2.4164483752301944

Epoch: 5| Step: 5
Training loss: 2.556588888168335
Validation loss: 2.4136432242649857

Epoch: 5| Step: 6
Training loss: 2.753690004348755
Validation loss: 2.410089895289431

Epoch: 5| Step: 7
Training loss: 2.7765145301818848
Validation loss: 2.4149384190959315

Epoch: 5| Step: 8
Training loss: 2.7168376445770264
Validation loss: 2.417093533341603

Epoch: 5| Step: 9
Training loss: 3.122081756591797
Validation loss: 2.432075103123983

Epoch: 5| Step: 10
Training loss: 2.526590347290039
Validation loss: 2.4330197636799147

Epoch: 34| Step: 0
Training loss: 2.8242030143737793
Validation loss: 2.403063702326949

Epoch: 5| Step: 1
Training loss: 2.2973508834838867
Validation loss: 2.4042450689500376

Epoch: 5| Step: 2
Training loss: 2.865726947784424
Validation loss: 2.4102530325612714

Epoch: 5| Step: 3
Training loss: 2.8378117084503174
Validation loss: 2.4131233563987156

Epoch: 5| Step: 4
Training loss: 1.8780107498168945
Validation loss: 2.408167157121884

Epoch: 5| Step: 5
Training loss: 2.4980781078338623
Validation loss: 2.4020177805295555

Epoch: 5| Step: 6
Training loss: 2.2890970706939697
Validation loss: 2.3844563012482016

Epoch: 5| Step: 7
Training loss: 2.038072347640991
Validation loss: 2.377394753117715

Epoch: 5| Step: 8
Training loss: 3.275055408477783
Validation loss: 2.37999249786459

Epoch: 5| Step: 9
Training loss: 2.9830799102783203
Validation loss: 2.3892415287674114

Epoch: 5| Step: 10
Training loss: 3.289715051651001
Validation loss: 2.3793821334838867

Epoch: 35| Step: 0
Training loss: 2.590205430984497
Validation loss: 2.371363260412729

Epoch: 5| Step: 1
Training loss: 3.293476104736328
Validation loss: 2.3725450936184136

Epoch: 5| Step: 2
Training loss: 2.554504871368408
Validation loss: 2.3707571106572307

Epoch: 5| Step: 3
Training loss: 2.6353578567504883
Validation loss: 2.3667696150400306

Epoch: 5| Step: 4
Training loss: 1.9158127307891846
Validation loss: 2.3703456694079983

Epoch: 5| Step: 5
Training loss: 2.3015520572662354
Validation loss: 2.3695661867818525

Epoch: 5| Step: 6
Training loss: 2.8447718620300293
Validation loss: 2.3695336567458285

Epoch: 5| Step: 7
Training loss: 3.20025897026062
Validation loss: 2.3704114011538926

Epoch: 5| Step: 8
Training loss: 2.6389341354370117
Validation loss: 2.3687954666793987

Epoch: 5| Step: 9
Training loss: 2.501070499420166
Validation loss: 2.3725194725939023

Epoch: 5| Step: 10
Training loss: 2.348695993423462
Validation loss: 2.362141780955817

Epoch: 36| Step: 0
Training loss: 2.8056437969207764
Validation loss: 2.3624531594655847

Epoch: 5| Step: 1
Training loss: 2.0498805046081543
Validation loss: 2.360384033572289

Epoch: 5| Step: 2
Training loss: 2.6771559715270996
Validation loss: 2.3607225994909964

Epoch: 5| Step: 3
Training loss: 2.9518585205078125
Validation loss: 2.3661074587093887

Epoch: 5| Step: 4
Training loss: 2.060727596282959
Validation loss: 2.363140265146891

Epoch: 5| Step: 5
Training loss: 2.9318528175354004
Validation loss: 2.3715789907722065

Epoch: 5| Step: 6
Training loss: 2.9428977966308594
Validation loss: 2.381601213127054

Epoch: 5| Step: 7
Training loss: 2.3576157093048096
Validation loss: 2.3788522930555445

Epoch: 5| Step: 8
Training loss: 2.485926866531372
Validation loss: 2.3784946216050016

Epoch: 5| Step: 9
Training loss: 2.8670759201049805
Validation loss: 2.3989828094359367

Epoch: 5| Step: 10
Training loss: 2.598005533218384
Validation loss: 2.414027216613934

Epoch: 37| Step: 0
Training loss: 3.220040798187256
Validation loss: 2.4249247940637733

Epoch: 5| Step: 1
Training loss: 2.578721284866333
Validation loss: 2.4726983193428285

Epoch: 5| Step: 2
Training loss: 2.504826307296753
Validation loss: 2.4375326095088834

Epoch: 5| Step: 3
Training loss: 2.869321584701538
Validation loss: 2.40410433020643

Epoch: 5| Step: 4
Training loss: 2.0521934032440186
Validation loss: 2.3814666476300967

Epoch: 5| Step: 5
Training loss: 3.1182265281677246
Validation loss: 2.378261258525233

Epoch: 5| Step: 6
Training loss: 2.407013416290283
Validation loss: 2.3734676414920437

Epoch: 5| Step: 7
Training loss: 2.902541160583496
Validation loss: 2.373073272807624

Epoch: 5| Step: 8
Training loss: 1.9315998554229736
Validation loss: 2.3669161873479045

Epoch: 5| Step: 9
Training loss: 2.795173168182373
Validation loss: 2.3642950852711997

Epoch: 5| Step: 10
Training loss: 2.4982612133026123
Validation loss: 2.3557004954225276

Epoch: 38| Step: 0
Training loss: 3.047978162765503
Validation loss: 2.3759806258704073

Epoch: 5| Step: 1
Training loss: 2.8427295684814453
Validation loss: 2.393289425039804

Epoch: 5| Step: 2
Training loss: 2.007202625274658
Validation loss: 2.408087040788384

Epoch: 5| Step: 3
Training loss: 2.6652214527130127
Validation loss: 2.4171055388706986

Epoch: 5| Step: 4
Training loss: 2.359591245651245
Validation loss: 2.417263751388878

Epoch: 5| Step: 5
Training loss: 2.0624728202819824
Validation loss: 2.4229826824639433

Epoch: 5| Step: 6
Training loss: 2.801891326904297
Validation loss: 2.415686221532924

Epoch: 5| Step: 7
Training loss: 2.5108444690704346
Validation loss: 2.43136288017355

Epoch: 5| Step: 8
Training loss: 3.1615982055664062
Validation loss: 2.4499481929245817

Epoch: 5| Step: 9
Training loss: 2.554687738418579
Validation loss: 2.4537053159488145

Epoch: 5| Step: 10
Training loss: 3.068373680114746
Validation loss: 2.426107511725477

Epoch: 39| Step: 0
Training loss: 2.7475998401641846
Validation loss: 2.392580404076525

Epoch: 5| Step: 1
Training loss: 2.759321689605713
Validation loss: 2.3538310758529173

Epoch: 5| Step: 2
Training loss: 2.911686897277832
Validation loss: 2.3392688151328795

Epoch: 5| Step: 3
Training loss: 2.8096699714660645
Validation loss: 2.3370957989846506

Epoch: 5| Step: 4
Training loss: 2.075132131576538
Validation loss: 2.349894303147511

Epoch: 5| Step: 5
Training loss: 2.5890917778015137
Validation loss: 2.3629896320322508

Epoch: 5| Step: 6
Training loss: 2.1333937644958496
Validation loss: 2.3791031734917754

Epoch: 5| Step: 7
Training loss: 2.299234390258789
Validation loss: 2.4014928340911865

Epoch: 5| Step: 8
Training loss: 3.094860553741455
Validation loss: 2.3658009216349614

Epoch: 5| Step: 9
Training loss: 2.668734073638916
Validation loss: 2.35320637559378

Epoch: 5| Step: 10
Training loss: 2.9403507709503174
Validation loss: 2.351967329620033

Epoch: 40| Step: 0
Training loss: 2.3601560592651367
Validation loss: 2.3628172925723496

Epoch: 5| Step: 1
Training loss: 2.4498884677886963
Validation loss: 2.389083352140201

Epoch: 5| Step: 2
Training loss: 2.1733317375183105
Validation loss: 2.3805249326972553

Epoch: 5| Step: 3
Training loss: 3.1489853858947754
Validation loss: 2.4090432428544566

Epoch: 5| Step: 4
Training loss: 3.5566933155059814
Validation loss: 2.4170134887900403

Epoch: 5| Step: 5
Training loss: 1.6303802728652954
Validation loss: 2.3950290013385076

Epoch: 5| Step: 6
Training loss: 2.9550840854644775
Validation loss: 2.4019812153231714

Epoch: 5| Step: 7
Training loss: 2.7921528816223145
Validation loss: 2.4064791715273293

Epoch: 5| Step: 8
Training loss: 2.181912899017334
Validation loss: 2.381066414617723

Epoch: 5| Step: 9
Training loss: 2.4251835346221924
Validation loss: 2.3517636150442143

Epoch: 5| Step: 10
Training loss: 3.1210274696350098
Validation loss: 2.3473430410508187

Epoch: 41| Step: 0
Training loss: 3.205857038497925
Validation loss: 2.3474896979588333

Epoch: 5| Step: 1
Training loss: 2.5389275550842285
Validation loss: 2.3394491134151334

Epoch: 5| Step: 2
Training loss: 3.057168960571289
Validation loss: 2.335290506321897

Epoch: 5| Step: 3
Training loss: 2.3399524688720703
Validation loss: 2.331487230075303

Epoch: 5| Step: 4
Training loss: 2.5685393810272217
Validation loss: 2.339313050752045

Epoch: 5| Step: 5
Training loss: 2.423034191131592
Validation loss: 2.3396920798927225

Epoch: 5| Step: 6
Training loss: 2.4884097576141357
Validation loss: 2.360723751847462

Epoch: 5| Step: 7
Training loss: 2.4944329261779785
Validation loss: 2.3620877855567524

Epoch: 5| Step: 8
Training loss: 2.351555347442627
Validation loss: 2.35966492724675

Epoch: 5| Step: 9
Training loss: 3.003678798675537
Validation loss: 2.3609912280113465

Epoch: 5| Step: 10
Training loss: 2.1162283420562744
Validation loss: 2.3478656507307485

Epoch: 42| Step: 0
Training loss: 3.435518264770508
Validation loss: 2.349513582004014

Epoch: 5| Step: 1
Training loss: 2.4024112224578857
Validation loss: 2.352406117223924

Epoch: 5| Step: 2
Training loss: 2.568845272064209
Validation loss: 2.3489132491491174

Epoch: 5| Step: 3
Training loss: 2.026243209838867
Validation loss: 2.3377846569143315

Epoch: 5| Step: 4
Training loss: 2.8045315742492676
Validation loss: 2.3452286592093845

Epoch: 5| Step: 5
Training loss: 2.6164634227752686
Validation loss: 2.3538397986401796

Epoch: 5| Step: 6
Training loss: 2.9074132442474365
Validation loss: 2.350658570566485

Epoch: 5| Step: 7
Training loss: 2.0779526233673096
Validation loss: 2.3504658052998204

Epoch: 5| Step: 8
Training loss: 2.69950270652771
Validation loss: 2.3674643026885165

Epoch: 5| Step: 9
Training loss: 2.2903084754943848
Validation loss: 2.3792917420787196

Epoch: 5| Step: 10
Training loss: 2.756434440612793
Validation loss: 2.4119094212849936

Epoch: 43| Step: 0
Training loss: 2.5685977935791016
Validation loss: 2.3904085313120196

Epoch: 5| Step: 1
Training loss: 2.7588653564453125
Validation loss: 2.3566147153095534

Epoch: 5| Step: 2
Training loss: 1.7407420873641968
Validation loss: 2.348347548515566

Epoch: 5| Step: 3
Training loss: 2.897510528564453
Validation loss: 2.3412684779013357

Epoch: 5| Step: 4
Training loss: 2.617234468460083
Validation loss: 2.3372700624568488

Epoch: 5| Step: 5
Training loss: 3.217108964920044
Validation loss: 2.3451635606827272

Epoch: 5| Step: 6
Training loss: 2.4160449504852295
Validation loss: 2.3490726922148015

Epoch: 5| Step: 7
Training loss: 2.477898359298706
Validation loss: 2.352938018819337

Epoch: 5| Step: 8
Training loss: 2.3287596702575684
Validation loss: 2.3577774263197377

Epoch: 5| Step: 9
Training loss: 2.7413675785064697
Validation loss: 2.4330424903541483

Epoch: 5| Step: 10
Training loss: 3.0368411540985107
Validation loss: 2.438339912763206

Epoch: 44| Step: 0
Training loss: 1.9718215465545654
Validation loss: 2.418063240666543

Epoch: 5| Step: 1
Training loss: 2.8404881954193115
Validation loss: 2.402295281810145

Epoch: 5| Step: 2
Training loss: 2.035393714904785
Validation loss: 2.361848287684943

Epoch: 5| Step: 3
Training loss: 2.3719518184661865
Validation loss: 2.3323542789746354

Epoch: 5| Step: 4
Training loss: 2.3905844688415527
Validation loss: 2.323077400525411

Epoch: 5| Step: 5
Training loss: 2.8397879600524902
Validation loss: 2.3096362877917547

Epoch: 5| Step: 6
Training loss: 3.421626329421997
Validation loss: 2.315339990841445

Epoch: 5| Step: 7
Training loss: 2.9926185607910156
Validation loss: 2.320468125804778

Epoch: 5| Step: 8
Training loss: 2.328791618347168
Validation loss: 2.3293178132785264

Epoch: 5| Step: 9
Training loss: 2.6783339977264404
Validation loss: 2.3323772415038078

Epoch: 5| Step: 10
Training loss: 2.8708934783935547
Validation loss: 2.3449337097906295

Epoch: 45| Step: 0
Training loss: 2.760613203048706
Validation loss: 2.3361576398213706

Epoch: 5| Step: 1
Training loss: 1.8837333917617798
Validation loss: 2.3219930459094305

Epoch: 5| Step: 2
Training loss: 2.6463284492492676
Validation loss: 2.3164647727884273

Epoch: 5| Step: 3
Training loss: 3.114110231399536
Validation loss: 2.3148543091230493

Epoch: 5| Step: 4
Training loss: 3.200817108154297
Validation loss: 2.317333221435547

Epoch: 5| Step: 5
Training loss: 2.972475528717041
Validation loss: 2.321874374984413

Epoch: 5| Step: 6
Training loss: 2.035727024078369
Validation loss: 2.314406696186271

Epoch: 5| Step: 7
Training loss: 2.698476552963257
Validation loss: 2.329481283823649

Epoch: 5| Step: 8
Training loss: 2.945838689804077
Validation loss: 2.3167555255274617

Epoch: 5| Step: 9
Training loss: 2.151454210281372
Validation loss: 2.309691693193169

Epoch: 5| Step: 10
Training loss: 1.9855095148086548
Validation loss: 2.3062773366128244

Epoch: 46| Step: 0
Training loss: 2.2379767894744873
Validation loss: 2.3015480887505317

Epoch: 5| Step: 1
Training loss: 2.5843944549560547
Validation loss: 2.307258609802492

Epoch: 5| Step: 2
Training loss: 2.5157856941223145
Validation loss: 2.306281902456796

Epoch: 5| Step: 3
Training loss: 2.833286762237549
Validation loss: 2.3129510366788475

Epoch: 5| Step: 4
Training loss: 2.0061144828796387
Validation loss: 2.3161711436445995

Epoch: 5| Step: 5
Training loss: 3.189074993133545
Validation loss: 2.340593768704322

Epoch: 5| Step: 6
Training loss: 2.688084125518799
Validation loss: 2.3640108262338946

Epoch: 5| Step: 7
Training loss: 1.7430598735809326
Validation loss: 2.36291596710041

Epoch: 5| Step: 8
Training loss: 3.6521716117858887
Validation loss: 2.3638544159550823

Epoch: 5| Step: 9
Training loss: 2.566117525100708
Validation loss: 2.3381457200614353

Epoch: 5| Step: 10
Training loss: 2.366318941116333
Validation loss: 2.319872501075909

Epoch: 47| Step: 0
Training loss: 2.5768046379089355
Validation loss: 2.3140207849523073

Epoch: 5| Step: 1
Training loss: 2.487232208251953
Validation loss: 2.311214680312782

Epoch: 5| Step: 2
Training loss: 2.801970958709717
Validation loss: 2.3129028094712125

Epoch: 5| Step: 3
Training loss: 2.2804112434387207
Validation loss: 2.3159871896107993

Epoch: 5| Step: 4
Training loss: 2.5253958702087402
Validation loss: 2.3182795381033294

Epoch: 5| Step: 5
Training loss: 2.729599952697754
Validation loss: 2.3202972360836562

Epoch: 5| Step: 6
Training loss: 2.66194224357605
Validation loss: 2.324428476313109

Epoch: 5| Step: 7
Training loss: 2.726728916168213
Validation loss: 2.3181493897591867

Epoch: 5| Step: 8
Training loss: 2.740238904953003
Validation loss: 2.324390667741017

Epoch: 5| Step: 9
Training loss: 2.5562500953674316
Validation loss: 2.341884905292142

Epoch: 5| Step: 10
Training loss: 2.2891571521759033
Validation loss: 2.36607252910573

Epoch: 48| Step: 0
Training loss: 3.3442676067352295
Validation loss: 2.3918094378645702

Epoch: 5| Step: 1
Training loss: 2.0748448371887207
Validation loss: 2.417527083427675

Epoch: 5| Step: 2
Training loss: 3.0453996658325195
Validation loss: 2.4052915496210896

Epoch: 5| Step: 3
Training loss: 2.2381396293640137
Validation loss: 2.417302757181147

Epoch: 5| Step: 4
Training loss: 2.3357136249542236
Validation loss: 2.407907421870898

Epoch: 5| Step: 5
Training loss: 3.0806479454040527
Validation loss: 2.40675437578591

Epoch: 5| Step: 6
Training loss: 2.6799235343933105
Validation loss: 2.401575432028822

Epoch: 5| Step: 7
Training loss: 2.6096062660217285
Validation loss: 2.36154757776568

Epoch: 5| Step: 8
Training loss: 1.9136154651641846
Validation loss: 2.3154252113834506

Epoch: 5| Step: 9
Training loss: 2.2657430171966553
Validation loss: 2.297513856682726

Epoch: 5| Step: 10
Training loss: 2.829094171524048
Validation loss: 2.286932945251465

Epoch: 49| Step: 0
Training loss: 2.7670278549194336
Validation loss: 2.301133715978233

Epoch: 5| Step: 1
Training loss: 2.7305431365966797
Validation loss: 2.319728805172828

Epoch: 5| Step: 2
Training loss: 2.6747329235076904
Validation loss: 2.3418078627637637

Epoch: 5| Step: 3
Training loss: 2.384864091873169
Validation loss: 2.348456423769715

Epoch: 5| Step: 4
Training loss: 2.198586940765381
Validation loss: 2.3183560807217836

Epoch: 5| Step: 5
Training loss: 3.0011420249938965
Validation loss: 2.30511305152729

Epoch: 5| Step: 6
Training loss: 2.8213844299316406
Validation loss: 2.2856088363996117

Epoch: 5| Step: 7
Training loss: 2.506762981414795
Validation loss: 2.27894583312414

Epoch: 5| Step: 8
Training loss: 2.9441545009613037
Validation loss: 2.274589571901547

Epoch: 5| Step: 9
Training loss: 1.8902089595794678
Validation loss: 2.279295567543276

Epoch: 5| Step: 10
Training loss: 2.8711819648742676
Validation loss: 2.300080627523443

Epoch: 50| Step: 0
Training loss: 3.044699192047119
Validation loss: 2.3213392534563617

Epoch: 5| Step: 1
Training loss: 2.8217594623565674
Validation loss: 2.359692568420082

Epoch: 5| Step: 2
Training loss: 1.9601218700408936
Validation loss: 2.35517813569756

Epoch: 5| Step: 3
Training loss: 2.667309284210205
Validation loss: 2.3273947905468684

Epoch: 5| Step: 4
Training loss: 2.555823802947998
Validation loss: 2.321755342586066

Epoch: 5| Step: 5
Training loss: 2.2100415229797363
Validation loss: 2.3000791970119683

Epoch: 5| Step: 6
Training loss: 3.033134937286377
Validation loss: 2.294648426835255

Epoch: 5| Step: 7
Training loss: 1.993753433227539
Validation loss: 2.2866364576483287

Epoch: 5| Step: 8
Training loss: 2.388327121734619
Validation loss: 2.2856752295647897

Epoch: 5| Step: 9
Training loss: 2.845653772354126
Validation loss: 2.28966676035235

Epoch: 5| Step: 10
Training loss: 2.8594279289245605
Validation loss: 2.3014058169498237

Epoch: 51| Step: 0
Training loss: 2.9903430938720703
Validation loss: 2.290348375997236

Epoch: 5| Step: 1
Training loss: 2.166106939315796
Validation loss: 2.2996864588029924

Epoch: 5| Step: 2
Training loss: 2.2986021041870117
Validation loss: 2.297441618416899

Epoch: 5| Step: 3
Training loss: 2.6471009254455566
Validation loss: 2.308420950366605

Epoch: 5| Step: 4
Training loss: 2.4777791500091553
Validation loss: 2.2985600117714173

Epoch: 5| Step: 5
Training loss: 2.8772921562194824
Validation loss: 2.293452226987449

Epoch: 5| Step: 6
Training loss: 2.559548854827881
Validation loss: 2.2993600560772802

Epoch: 5| Step: 7
Training loss: 2.313892126083374
Validation loss: 2.277414216790148

Epoch: 5| Step: 8
Training loss: 2.588918447494507
Validation loss: 2.273066818073232

Epoch: 5| Step: 9
Training loss: 2.544429302215576
Validation loss: 2.2654001148798133

Epoch: 5| Step: 10
Training loss: 2.747264862060547
Validation loss: 2.261853999989007

Epoch: 52| Step: 0
Training loss: 2.9396915435791016
Validation loss: 2.2638876617595716

Epoch: 5| Step: 1
Training loss: 2.501176357269287
Validation loss: 2.2603076991214546

Epoch: 5| Step: 2
Training loss: 2.4716010093688965
Validation loss: 2.26105837668142

Epoch: 5| Step: 3
Training loss: 1.9567699432373047
Validation loss: 2.2599207893494637

Epoch: 5| Step: 4
Training loss: 2.3843770027160645
Validation loss: 2.2601460487611833

Epoch: 5| Step: 5
Training loss: 2.7017712593078613
Validation loss: 2.263438199156074

Epoch: 5| Step: 6
Training loss: 2.69183611869812
Validation loss: 2.2617481882854173

Epoch: 5| Step: 7
Training loss: 3.0773251056671143
Validation loss: 2.26292004892903

Epoch: 5| Step: 8
Training loss: 2.542232036590576
Validation loss: 2.2669901078747166

Epoch: 5| Step: 9
Training loss: 2.399735927581787
Validation loss: 2.2701185057240147

Epoch: 5| Step: 10
Training loss: 2.454657554626465
Validation loss: 2.303035864266016

Epoch: 53| Step: 0
Training loss: 2.8489670753479004
Validation loss: 2.316734875402143

Epoch: 5| Step: 1
Training loss: 2.1656322479248047
Validation loss: 2.313228968651064

Epoch: 5| Step: 2
Training loss: 2.4992594718933105
Validation loss: 2.3033117427620837

Epoch: 5| Step: 3
Training loss: 2.7974369525909424
Validation loss: 2.313026782005064

Epoch: 5| Step: 4
Training loss: 2.4900031089782715
Validation loss: 2.2890581213017946

Epoch: 5| Step: 5
Training loss: 2.8282790184020996
Validation loss: 2.269526930265529

Epoch: 5| Step: 6
Training loss: 2.628039836883545
Validation loss: 2.2682770426555345

Epoch: 5| Step: 7
Training loss: 2.2860794067382812
Validation loss: 2.270128119376398

Epoch: 5| Step: 8
Training loss: 3.1651852130889893
Validation loss: 2.272782761563537

Epoch: 5| Step: 9
Training loss: 2.3798985481262207
Validation loss: 2.275293852693291

Epoch: 5| Step: 10
Training loss: 1.9149118661880493
Validation loss: 2.2736697863507014

Epoch: 54| Step: 0
Training loss: 3.2473232746124268
Validation loss: 2.2685583253060617

Epoch: 5| Step: 1
Training loss: 3.0130016803741455
Validation loss: 2.2721178249646257

Epoch: 5| Step: 2
Training loss: 2.0820326805114746
Validation loss: 2.2753671651245444

Epoch: 5| Step: 3
Training loss: 2.457659959793091
Validation loss: 2.2718727588653564

Epoch: 5| Step: 4
Training loss: 2.3434624671936035
Validation loss: 2.2765878195403726

Epoch: 5| Step: 5
Training loss: 2.1748812198638916
Validation loss: 2.2743707113368536

Epoch: 5| Step: 6
Training loss: 2.8046388626098633
Validation loss: 2.2769380833512995

Epoch: 5| Step: 7
Training loss: 1.855078101158142
Validation loss: 2.2938378344299974

Epoch: 5| Step: 8
Training loss: 3.0169098377227783
Validation loss: 2.2849516791682087

Epoch: 5| Step: 9
Training loss: 2.0821330547332764
Validation loss: 2.284711669850093

Epoch: 5| Step: 10
Training loss: 2.9090516567230225
Validation loss: 2.2617434250411166

Epoch: 55| Step: 0
Training loss: 2.7625808715820312
Validation loss: 2.270695327430643

Epoch: 5| Step: 1
Training loss: 3.1870174407958984
Validation loss: 2.2643516294417845

Epoch: 5| Step: 2
Training loss: 2.1355013847351074
Validation loss: 2.2480540403755764

Epoch: 5| Step: 3
Training loss: 3.025651216506958
Validation loss: 2.2549897060599378

Epoch: 5| Step: 4
Training loss: 2.1237454414367676
Validation loss: 2.2454884590641147

Epoch: 5| Step: 5
Training loss: 2.6850924491882324
Validation loss: 2.2511073722634265

Epoch: 5| Step: 6
Training loss: 2.355100631713867
Validation loss: 2.260044515773814

Epoch: 5| Step: 7
Training loss: 2.3590874671936035
Validation loss: 2.2981128384990077

Epoch: 5| Step: 8
Training loss: 2.780247926712036
Validation loss: 2.3219568960128294

Epoch: 5| Step: 9
Training loss: 2.1564743518829346
Validation loss: 2.325141724719796

Epoch: 5| Step: 10
Training loss: 2.4020657539367676
Validation loss: 2.33226611280954

Epoch: 56| Step: 0
Training loss: 3.126534938812256
Validation loss: 2.3192192739056003

Epoch: 5| Step: 1
Training loss: 2.207836627960205
Validation loss: 2.2925981372915287

Epoch: 5| Step: 2
Training loss: 2.18131422996521
Validation loss: 2.2731543228190434

Epoch: 5| Step: 3
Training loss: 2.348604202270508
Validation loss: 2.2564903843787407

Epoch: 5| Step: 4
Training loss: 2.796483278274536
Validation loss: 2.2539608478546143

Epoch: 5| Step: 5
Training loss: 2.4923789501190186
Validation loss: 2.2609645628160044

Epoch: 5| Step: 6
Training loss: 3.0149307250976562
Validation loss: 2.2640253241344164

Epoch: 5| Step: 7
Training loss: 1.9640519618988037
Validation loss: 2.24880305669641

Epoch: 5| Step: 8
Training loss: 2.676901340484619
Validation loss: 2.2469630574667327

Epoch: 5| Step: 9
Training loss: 2.4316954612731934
Validation loss: 2.251537604998517

Epoch: 5| Step: 10
Training loss: 2.619732141494751
Validation loss: 2.264458058982767

Epoch: 57| Step: 0
Training loss: 2.740643262863159
Validation loss: 2.244359177927817

Epoch: 5| Step: 1
Training loss: 2.3710851669311523
Validation loss: 2.242758071550759

Epoch: 5| Step: 2
Training loss: 2.365360975265503
Validation loss: 2.250960216727308

Epoch: 5| Step: 3
Training loss: 2.1318469047546387
Validation loss: 2.253072851447649

Epoch: 5| Step: 4
Training loss: 2.274049758911133
Validation loss: 2.255353932739586

Epoch: 5| Step: 5
Training loss: 2.916884660720825
Validation loss: 2.2637240168868855

Epoch: 5| Step: 6
Training loss: 2.598834991455078
Validation loss: 2.258257201922837

Epoch: 5| Step: 7
Training loss: 2.5322437286376953
Validation loss: 2.255375587812034

Epoch: 5| Step: 8
Training loss: 2.5769503116607666
Validation loss: 2.2467005099019697

Epoch: 5| Step: 9
Training loss: 2.251938581466675
Validation loss: 2.2608388521338023

Epoch: 5| Step: 10
Training loss: 3.058657169342041
Validation loss: 2.2704131962150655

Epoch: 58| Step: 0
Training loss: 2.5353426933288574
Validation loss: 2.2654272869069088

Epoch: 5| Step: 1
Training loss: 2.793112277984619
Validation loss: 2.2675262535772016

Epoch: 5| Step: 2
Training loss: 2.260457992553711
Validation loss: 2.271357403006605

Epoch: 5| Step: 3
Training loss: 1.6980865001678467
Validation loss: 2.281650827777001

Epoch: 5| Step: 4
Training loss: 2.860085964202881
Validation loss: 2.292407812610749

Epoch: 5| Step: 5
Training loss: 2.9378128051757812
Validation loss: 2.291298540689612

Epoch: 5| Step: 6
Training loss: 1.990424394607544
Validation loss: 2.277359520235369

Epoch: 5| Step: 7
Training loss: 2.4616878032684326
Validation loss: 2.2716747842809206

Epoch: 5| Step: 8
Training loss: 3.032320022583008
Validation loss: 2.2502492858517553

Epoch: 5| Step: 9
Training loss: 1.7670997381210327
Validation loss: 2.2255131493332567

Epoch: 5| Step: 10
Training loss: 3.3933918476104736
Validation loss: 2.2249818053296817

Epoch: 59| Step: 0
Training loss: 2.606879711151123
Validation loss: 2.2259529406024563

Epoch: 5| Step: 1
Training loss: 2.6184322834014893
Validation loss: 2.2291349070046538

Epoch: 5| Step: 2
Training loss: 2.1788671016693115
Validation loss: 2.2361226158757366

Epoch: 5| Step: 3
Training loss: 2.487433671951294
Validation loss: 2.2416463128982054

Epoch: 5| Step: 4
Training loss: 3.0262303352355957
Validation loss: 2.2414629138926023

Epoch: 5| Step: 5
Training loss: 2.5617847442626953
Validation loss: 2.234856643984395

Epoch: 5| Step: 6
Training loss: 3.4607901573181152
Validation loss: 2.2358696781178957

Epoch: 5| Step: 7
Training loss: 2.4936037063598633
Validation loss: 2.2308125111364547

Epoch: 5| Step: 8
Training loss: 2.306720495223999
Validation loss: 2.213447581055344

Epoch: 5| Step: 9
Training loss: 2.353949546813965
Validation loss: 2.2220293603917605

Epoch: 5| Step: 10
Training loss: 1.6728681325912476
Validation loss: 2.249581460029848

Epoch: 60| Step: 0
Training loss: 2.0305914878845215
Validation loss: 2.276226007810203

Epoch: 5| Step: 1
Training loss: 2.3146462440490723
Validation loss: 2.3644917011260986

Epoch: 5| Step: 2
Training loss: 2.710303783416748
Validation loss: 2.39513643839026

Epoch: 5| Step: 3
Training loss: 2.2675585746765137
Validation loss: 2.4165075068832724

Epoch: 5| Step: 4
Training loss: 3.1384215354919434
Validation loss: 2.3630040435380835

Epoch: 5| Step: 5
Training loss: 2.0609283447265625
Validation loss: 2.311066665957051

Epoch: 5| Step: 6
Training loss: 2.8152191638946533
Validation loss: 2.263923088709513

Epoch: 5| Step: 7
Training loss: 2.3478541374206543
Validation loss: 2.233676802727484

Epoch: 5| Step: 8
Training loss: 3.1660702228546143
Validation loss: 2.2125795477180072

Epoch: 5| Step: 9
Training loss: 2.3812296390533447
Validation loss: 2.209450706358879

Epoch: 5| Step: 10
Training loss: 2.726506471633911
Validation loss: 2.2116994037423083

Epoch: 61| Step: 0
Training loss: 2.000051736831665
Validation loss: 2.2253273533236597

Epoch: 5| Step: 1
Training loss: 3.170987367630005
Validation loss: 2.22813928768199

Epoch: 5| Step: 2
Training loss: 2.7913546562194824
Validation loss: 2.232664108276367

Epoch: 5| Step: 3
Training loss: 2.6269209384918213
Validation loss: 2.2249625203430012

Epoch: 5| Step: 4
Training loss: 2.2976107597351074
Validation loss: 2.228545811868483

Epoch: 5| Step: 5
Training loss: 2.3318309783935547
Validation loss: 2.217417058124337

Epoch: 5| Step: 6
Training loss: 2.639054775238037
Validation loss: 2.210836451540711

Epoch: 5| Step: 7
Training loss: 2.497910499572754
Validation loss: 2.2095524598193426

Epoch: 5| Step: 8
Training loss: 2.419912338256836
Validation loss: 2.21203290518894

Epoch: 5| Step: 9
Training loss: 2.392390727996826
Validation loss: 2.2282770205569524

Epoch: 5| Step: 10
Training loss: 2.6413395404815674
Validation loss: 2.240446823899464

Epoch: 62| Step: 0
Training loss: 2.569859743118286
Validation loss: 2.258448603332684

Epoch: 5| Step: 1
Training loss: 2.4516005516052246
Validation loss: 2.267402059288435

Epoch: 5| Step: 2
Training loss: 2.329773426055908
Validation loss: 2.300827536531674

Epoch: 5| Step: 3
Training loss: 2.569840908050537
Validation loss: 2.332276651936193

Epoch: 5| Step: 4
Training loss: 3.1094186305999756
Validation loss: 2.3468590269806566

Epoch: 5| Step: 5
Training loss: 2.4937949180603027
Validation loss: 2.328764664229526

Epoch: 5| Step: 6
Training loss: 2.4286961555480957
Validation loss: 2.296692307277392

Epoch: 5| Step: 7
Training loss: 2.553807497024536
Validation loss: 2.264664985800302

Epoch: 5| Step: 8
Training loss: 2.7207884788513184
Validation loss: 2.240711578758814

Epoch: 5| Step: 9
Training loss: 2.1742420196533203
Validation loss: 2.2232283571714997

Epoch: 5| Step: 10
Training loss: 2.3075098991394043
Validation loss: 2.230357503378263

Epoch: 63| Step: 0
Training loss: 2.1947224140167236
Validation loss: 2.228559491454914

Epoch: 5| Step: 1
Training loss: 3.719789981842041
Validation loss: 2.2103509005679878

Epoch: 5| Step: 2
Training loss: 2.085390090942383
Validation loss: 2.200650517658521

Epoch: 5| Step: 3
Training loss: 2.3056063652038574
Validation loss: 2.204988646250899

Epoch: 5| Step: 4
Training loss: 2.387235164642334
Validation loss: 2.203599529881631

Epoch: 5| Step: 5
Training loss: 2.107851028442383
Validation loss: 2.1983041378759567

Epoch: 5| Step: 6
Training loss: 3.142582654953003
Validation loss: 2.207659018936978

Epoch: 5| Step: 7
Training loss: 2.2569289207458496
Validation loss: 2.211217559793944

Epoch: 5| Step: 8
Training loss: 2.222747325897217
Validation loss: 2.2222944228879866

Epoch: 5| Step: 9
Training loss: 2.612149477005005
Validation loss: 2.2523135113459762

Epoch: 5| Step: 10
Training loss: 2.697162628173828
Validation loss: 2.2630602723808697

Epoch: 64| Step: 0
Training loss: 2.2115375995635986
Validation loss: 2.236104585791147

Epoch: 5| Step: 1
Training loss: 2.2712039947509766
Validation loss: 2.2197990455935077

Epoch: 5| Step: 2
Training loss: 3.4657559394836426
Validation loss: 2.2091649360554193

Epoch: 5| Step: 3
Training loss: 2.582353115081787
Validation loss: 2.2005840065658733

Epoch: 5| Step: 4
Training loss: 2.0532095432281494
Validation loss: 2.196464951320361

Epoch: 5| Step: 5
Training loss: 1.7241284847259521
Validation loss: 2.1937185564348773

Epoch: 5| Step: 6
Training loss: 2.9573888778686523
Validation loss: 2.19173014292153

Epoch: 5| Step: 7
Training loss: 2.7345757484436035
Validation loss: 2.1964712501854025

Epoch: 5| Step: 8
Training loss: 2.5456607341766357
Validation loss: 2.196672595957274

Epoch: 5| Step: 9
Training loss: 2.5844035148620605
Validation loss: 2.1925551814417683

Epoch: 5| Step: 10
Training loss: 2.341642379760742
Validation loss: 2.1961111714763026

Epoch: 65| Step: 0
Training loss: 2.8540897369384766
Validation loss: 2.208456003537742

Epoch: 5| Step: 1
Training loss: 2.0780749320983887
Validation loss: 2.2128304153360348

Epoch: 5| Step: 2
Training loss: 2.0219833850860596
Validation loss: 2.2048660939739597

Epoch: 5| Step: 3
Training loss: 2.637016773223877
Validation loss: 2.2051011285474225

Epoch: 5| Step: 4
Training loss: 2.436169147491455
Validation loss: 2.201274009161098

Epoch: 5| Step: 5
Training loss: 3.1184334754943848
Validation loss: 2.205849511649019

Epoch: 5| Step: 6
Training loss: 3.0060946941375732
Validation loss: 2.1949833516151673

Epoch: 5| Step: 7
Training loss: 2.106555700302124
Validation loss: 2.2188924512555523

Epoch: 5| Step: 8
Training loss: 1.7801830768585205
Validation loss: 2.216787815093994

Epoch: 5| Step: 9
Training loss: 2.3237500190734863
Validation loss: 2.216203648556945

Epoch: 5| Step: 10
Training loss: 3.0477449893951416
Validation loss: 2.2332461828826577

Epoch: 66| Step: 0
Training loss: 2.727989912033081
Validation loss: 2.2467514725141626

Epoch: 5| Step: 1
Training loss: 2.9569172859191895
Validation loss: 2.2206783551041798

Epoch: 5| Step: 2
Training loss: 2.2863597869873047
Validation loss: 2.2108936950724614

Epoch: 5| Step: 3
Training loss: 1.8837772607803345
Validation loss: 2.1971484102228636

Epoch: 5| Step: 4
Training loss: 2.7059707641601562
Validation loss: 2.189883141107457

Epoch: 5| Step: 5
Training loss: 2.409996509552002
Validation loss: 2.194682046931277

Epoch: 5| Step: 6
Training loss: 2.732889175415039
Validation loss: 2.187522183182419

Epoch: 5| Step: 7
Training loss: 2.8434131145477295
Validation loss: 2.1824512968781176

Epoch: 5| Step: 8
Training loss: 2.294252872467041
Validation loss: 2.18255345026652

Epoch: 5| Step: 9
Training loss: 2.6492340564727783
Validation loss: 2.1780365461944253

Epoch: 5| Step: 10
Training loss: 1.8303673267364502
Validation loss: 2.174236641135267

Epoch: 67| Step: 0
Training loss: 2.048771381378174
Validation loss: 2.183711777451218

Epoch: 5| Step: 1
Training loss: 2.692129611968994
Validation loss: 2.1904579490743656

Epoch: 5| Step: 2
Training loss: 2.7038159370422363
Validation loss: 2.218327486386863

Epoch: 5| Step: 3
Training loss: 2.3245537281036377
Validation loss: 2.237247964387299

Epoch: 5| Step: 4
Training loss: 2.751953125
Validation loss: 2.2311625506288264

Epoch: 5| Step: 5
Training loss: 2.3891282081604004
Validation loss: 2.223758420636577

Epoch: 5| Step: 6
Training loss: 2.5239250659942627
Validation loss: 2.2129429847963396

Epoch: 5| Step: 7
Training loss: 2.609327554702759
Validation loss: 2.2146592422198226

Epoch: 5| Step: 8
Training loss: 2.4506638050079346
Validation loss: 2.2043298700804352

Epoch: 5| Step: 9
Training loss: 2.3207452297210693
Validation loss: 2.198030651256602

Epoch: 5| Step: 10
Training loss: 2.5293354988098145
Validation loss: 2.1877237622455885

Epoch: 68| Step: 0
Training loss: 2.878629446029663
Validation loss: 2.1740324753586964

Epoch: 5| Step: 1
Training loss: 2.5843091011047363
Validation loss: 2.1788770780768445

Epoch: 5| Step: 2
Training loss: 2.5519354343414307
Validation loss: 2.186210117032451

Epoch: 5| Step: 3
Training loss: 3.420309543609619
Validation loss: 2.1955491112124537

Epoch: 5| Step: 4
Training loss: 1.3025962114334106
Validation loss: 2.194041285463559

Epoch: 5| Step: 5
Training loss: 1.9574801921844482
Validation loss: 2.1886398817903254

Epoch: 5| Step: 6
Training loss: 2.575155019760132
Validation loss: 2.189308184449391

Epoch: 5| Step: 7
Training loss: 2.3487164974212646
Validation loss: 2.1883805669764036

Epoch: 5| Step: 8
Training loss: 2.2202439308166504
Validation loss: 2.1831101832851285

Epoch: 5| Step: 9
Training loss: 3.0904078483581543
Validation loss: 2.200073616479033

Epoch: 5| Step: 10
Training loss: 2.0950374603271484
Validation loss: 2.230098230864412

Epoch: 69| Step: 0
Training loss: 3.0226082801818848
Validation loss: 2.2362794440279723

Epoch: 5| Step: 1
Training loss: 2.9619083404541016
Validation loss: 2.222442191134217

Epoch: 5| Step: 2
Training loss: 2.363966703414917
Validation loss: 2.2211912011587494

Epoch: 5| Step: 3
Training loss: 2.059110164642334
Validation loss: 2.1818094407358477

Epoch: 5| Step: 4
Training loss: 2.634028673171997
Validation loss: 2.1667576797546877

Epoch: 5| Step: 5
Training loss: 2.068563461303711
Validation loss: 2.163284103075663

Epoch: 5| Step: 6
Training loss: 2.2601943016052246
Validation loss: 2.174133212335648

Epoch: 5| Step: 7
Training loss: 2.341029644012451
Validation loss: 2.1743278811054845

Epoch: 5| Step: 8
Training loss: 2.4539968967437744
Validation loss: 2.17572723409181

Epoch: 5| Step: 9
Training loss: 2.249199628829956
Validation loss: 2.1750568292474233

Epoch: 5| Step: 10
Training loss: 2.8500726222991943
Validation loss: 2.199490593325707

Epoch: 70| Step: 0
Training loss: 2.667728900909424
Validation loss: 2.2052751894920104

Epoch: 5| Step: 1
Training loss: 2.263000965118408
Validation loss: 2.216551253872533

Epoch: 5| Step: 2
Training loss: 2.6627211570739746
Validation loss: 2.2225002037581576

Epoch: 5| Step: 3
Training loss: 2.2005858421325684
Validation loss: 2.2312177945208806

Epoch: 5| Step: 4
Training loss: 2.0510478019714355
Validation loss: 2.2011883643365677

Epoch: 5| Step: 5
Training loss: 2.047612428665161
Validation loss: 2.2164733538063626

Epoch: 5| Step: 6
Training loss: 2.937483310699463
Validation loss: 2.180468042691549

Epoch: 5| Step: 7
Training loss: 2.1488823890686035
Validation loss: 2.177608346426359

Epoch: 5| Step: 8
Training loss: 2.568206787109375
Validation loss: 2.1477666606185255

Epoch: 5| Step: 9
Training loss: 3.258021116256714
Validation loss: 2.1583104928334556

Epoch: 5| Step: 10
Training loss: 2.1181979179382324
Validation loss: 2.164094644208108

Epoch: 71| Step: 0
Training loss: 2.6148171424865723
Validation loss: 2.159592677188176

Epoch: 5| Step: 1
Training loss: 2.868230104446411
Validation loss: 2.158758312143305

Epoch: 5| Step: 2
Training loss: 2.6494834423065186
Validation loss: 2.170055051003733

Epoch: 5| Step: 3
Training loss: 2.3348031044006348
Validation loss: 2.162232668169083

Epoch: 5| Step: 4
Training loss: 2.568659543991089
Validation loss: 2.158121883228261

Epoch: 5| Step: 5
Training loss: 2.2193973064422607
Validation loss: 2.1591942002696376

Epoch: 5| Step: 6
Training loss: 2.7857799530029297
Validation loss: 2.175465952965521

Epoch: 5| Step: 7
Training loss: 2.4533257484436035
Validation loss: 2.163023776905511

Epoch: 5| Step: 8
Training loss: 2.177001714706421
Validation loss: 2.189185078426074

Epoch: 5| Step: 9
Training loss: 1.8942291736602783
Validation loss: 2.206784612389021

Epoch: 5| Step: 10
Training loss: 2.6042306423187256
Validation loss: 2.2078536928340955

Epoch: 72| Step: 0
Training loss: 2.762993812561035
Validation loss: 2.1945451126303723

Epoch: 5| Step: 1
Training loss: 2.817683696746826
Validation loss: 2.179924790577222

Epoch: 5| Step: 2
Training loss: 1.9170631170272827
Validation loss: 2.1567950992174048

Epoch: 5| Step: 3
Training loss: 2.532583713531494
Validation loss: 2.1494555678418887

Epoch: 5| Step: 4
Training loss: 2.139706611633301
Validation loss: 2.1577133247929234

Epoch: 5| Step: 5
Training loss: 2.2754220962524414
Validation loss: 2.1572818961194766

Epoch: 5| Step: 6
Training loss: 2.3550448417663574
Validation loss: 2.1520681791408087

Epoch: 5| Step: 7
Training loss: 2.3657383918762207
Validation loss: 2.1531166466333533

Epoch: 5| Step: 8
Training loss: 2.3284177780151367
Validation loss: 2.161383967245779

Epoch: 5| Step: 9
Training loss: 2.12786602973938
Validation loss: 2.1706242228067048

Epoch: 5| Step: 10
Training loss: 3.351839542388916
Validation loss: 2.172405994066628

Epoch: 73| Step: 0
Training loss: 3.3076038360595703
Validation loss: 2.1768352357290124

Epoch: 5| Step: 1
Training loss: 2.4862167835235596
Validation loss: 2.1633918298188077

Epoch: 5| Step: 2
Training loss: 2.275813579559326
Validation loss: 2.1590124176394556

Epoch: 5| Step: 3
Training loss: 2.406615734100342
Validation loss: 2.143772263680735

Epoch: 5| Step: 4
Training loss: 1.9336143732070923
Validation loss: 2.1636916155456216

Epoch: 5| Step: 5
Training loss: 2.981092929840088
Validation loss: 2.163281081825174

Epoch: 5| Step: 6
Training loss: 1.835884690284729
Validation loss: 2.165666300763366

Epoch: 5| Step: 7
Training loss: 2.0822322368621826
Validation loss: 2.1653185634202856

Epoch: 5| Step: 8
Training loss: 2.6571412086486816
Validation loss: 2.1642411267885597

Epoch: 5| Step: 9
Training loss: 2.6704185009002686
Validation loss: 2.1814140683861187

Epoch: 5| Step: 10
Training loss: 2.172358751296997
Validation loss: 2.170555232673563

Epoch: 74| Step: 0
Training loss: 2.8136324882507324
Validation loss: 2.173082044047694

Epoch: 5| Step: 1
Training loss: 2.6668026447296143
Validation loss: 2.1594008040684525

Epoch: 5| Step: 2
Training loss: 2.269320249557495
Validation loss: 2.149062233586465

Epoch: 5| Step: 3
Training loss: 2.5147976875305176
Validation loss: 2.141832105575069

Epoch: 5| Step: 4
Training loss: 1.949466347694397
Validation loss: 2.1286654497987483

Epoch: 5| Step: 5
Training loss: 1.8949085474014282
Validation loss: 2.1348761384205153

Epoch: 5| Step: 6
Training loss: 2.0770325660705566
Validation loss: 2.1508425538257887

Epoch: 5| Step: 7
Training loss: 2.3171679973602295
Validation loss: 2.158531714511174

Epoch: 5| Step: 8
Training loss: 2.6161277294158936
Validation loss: 2.146627410765617

Epoch: 5| Step: 9
Training loss: 2.7638261318206787
Validation loss: 2.1629094180240425

Epoch: 5| Step: 10
Training loss: 2.986159324645996
Validation loss: 2.169207630618926

Epoch: 75| Step: 0
Training loss: 2.774263858795166
Validation loss: 2.162402026114925

Epoch: 5| Step: 1
Training loss: 2.3514647483825684
Validation loss: 2.1647424531239334

Epoch: 5| Step: 2
Training loss: 2.0752227306365967
Validation loss: 2.1492537195964525

Epoch: 5| Step: 3
Training loss: 2.3827145099639893
Validation loss: 2.1491355716541247

Epoch: 5| Step: 4
Training loss: 2.3072052001953125
Validation loss: 2.139156171070632

Epoch: 5| Step: 5
Training loss: 2.4007456302642822
Validation loss: 2.159505342924467

Epoch: 5| Step: 6
Training loss: 2.5532655715942383
Validation loss: 2.161629529409511

Epoch: 5| Step: 7
Training loss: 2.1265645027160645
Validation loss: 2.178382319788779

Epoch: 5| Step: 8
Training loss: 2.825382947921753
Validation loss: 2.180975683273808

Epoch: 5| Step: 9
Training loss: 2.4898548126220703
Validation loss: 2.1702421595973354

Epoch: 5| Step: 10
Training loss: 2.3604888916015625
Validation loss: 2.167870811236802

Epoch: 76| Step: 0
Training loss: 2.268967866897583
Validation loss: 2.16600457314522

Epoch: 5| Step: 1
Training loss: 2.6761422157287598
Validation loss: 2.1711444098462342

Epoch: 5| Step: 2
Training loss: 2.515340566635132
Validation loss: 2.158280764856646

Epoch: 5| Step: 3
Training loss: 2.4924428462982178
Validation loss: 2.1744979837889313

Epoch: 5| Step: 4
Training loss: 2.1312947273254395
Validation loss: 2.1845861814355336

Epoch: 5| Step: 5
Training loss: 2.832448959350586
Validation loss: 2.196609092015092

Epoch: 5| Step: 6
Training loss: 2.338911533355713
Validation loss: 2.194326139265491

Epoch: 5| Step: 7
Training loss: 1.868385672569275
Validation loss: 2.1544041992515646

Epoch: 5| Step: 8
Training loss: 2.011446714401245
Validation loss: 2.1413576295298915

Epoch: 5| Step: 9
Training loss: 3.122737407684326
Validation loss: 2.133381801266824

Epoch: 5| Step: 10
Training loss: 2.3462023735046387
Validation loss: 2.1309643022475706

Epoch: 77| Step: 0
Training loss: 3.108813762664795
Validation loss: 2.1305226100388395

Epoch: 5| Step: 1
Training loss: 1.9022928476333618
Validation loss: 2.120348012575539

Epoch: 5| Step: 2
Training loss: 2.180715799331665
Validation loss: 2.1169099961557696

Epoch: 5| Step: 3
Training loss: 2.8046653270721436
Validation loss: 2.116959430838144

Epoch: 5| Step: 4
Training loss: 2.299531936645508
Validation loss: 2.1218782778709167

Epoch: 5| Step: 5
Training loss: 2.5294971466064453
Validation loss: 2.1334651183056574

Epoch: 5| Step: 6
Training loss: 2.408665180206299
Validation loss: 2.14524330118651

Epoch: 5| Step: 7
Training loss: 1.996373176574707
Validation loss: 2.162199668986823

Epoch: 5| Step: 8
Training loss: 2.9019670486450195
Validation loss: 2.1609730669247207

Epoch: 5| Step: 9
Training loss: 2.487934112548828
Validation loss: 2.168488016692541

Epoch: 5| Step: 10
Training loss: 1.7180895805358887
Validation loss: 2.167077128605176

Epoch: 78| Step: 0
Training loss: 2.200199604034424
Validation loss: 2.1724305101620254

Epoch: 5| Step: 1
Training loss: 2.8423190116882324
Validation loss: 2.197038996604181

Epoch: 5| Step: 2
Training loss: 2.5365450382232666
Validation loss: 2.1620896272761847

Epoch: 5| Step: 3
Training loss: 2.1295526027679443
Validation loss: 2.1369144314078876

Epoch: 5| Step: 4
Training loss: 2.454683303833008
Validation loss: 2.1243313435585267

Epoch: 5| Step: 5
Training loss: 2.0404887199401855
Validation loss: 2.1183840203028854

Epoch: 5| Step: 6
Training loss: 2.3818001747131348
Validation loss: 2.1132581374978505

Epoch: 5| Step: 7
Training loss: 1.8752151727676392
Validation loss: 2.1160129526610016

Epoch: 5| Step: 8
Training loss: 2.410142421722412
Validation loss: 2.130685193564302

Epoch: 5| Step: 9
Training loss: 3.1467366218566895
Validation loss: 2.130549256519605

Epoch: 5| Step: 10
Training loss: 2.4879767894744873
Validation loss: 2.1343062821254937

Epoch: 79| Step: 0
Training loss: 2.235666275024414
Validation loss: 2.1267023214729885

Epoch: 5| Step: 1
Training loss: 2.549781084060669
Validation loss: 2.121798102573682

Epoch: 5| Step: 2
Training loss: 2.9460997581481934
Validation loss: 2.1257302427804596

Epoch: 5| Step: 3
Training loss: 2.292935848236084
Validation loss: 2.1288270719589724

Epoch: 5| Step: 4
Training loss: 2.4487252235412598
Validation loss: 2.116587431200089

Epoch: 5| Step: 5
Training loss: 2.041477680206299
Validation loss: 2.128699751310451

Epoch: 5| Step: 6
Training loss: 2.438746929168701
Validation loss: 2.1398119106087634

Epoch: 5| Step: 7
Training loss: 2.61417555809021
Validation loss: 2.1413450599998556

Epoch: 5| Step: 8
Training loss: 2.4928677082061768
Validation loss: 2.1658989870420067

Epoch: 5| Step: 9
Training loss: 2.385791063308716
Validation loss: 2.1499716440836587

Epoch: 5| Step: 10
Training loss: 1.8037447929382324
Validation loss: 2.1321784347616215

Epoch: 80| Step: 0
Training loss: 2.082801342010498
Validation loss: 2.1237635381760134

Epoch: 5| Step: 1
Training loss: 2.6679179668426514
Validation loss: 2.133057104643955

Epoch: 5| Step: 2
Training loss: 2.408111810684204
Validation loss: 2.1261862747130857

Epoch: 5| Step: 3
Training loss: 2.9658284187316895
Validation loss: 2.127699493080057

Epoch: 5| Step: 4
Training loss: 2.2597222328186035
Validation loss: 2.1211909965802263

Epoch: 5| Step: 5
Training loss: 2.816887617111206
Validation loss: 2.1145153250745548

Epoch: 5| Step: 6
Training loss: 1.304900884628296
Validation loss: 2.124645469009235

Epoch: 5| Step: 7
Training loss: 2.569312334060669
Validation loss: 2.114854710076445

Epoch: 5| Step: 8
Training loss: 2.704951763153076
Validation loss: 2.1226977148363666

Epoch: 5| Step: 9
Training loss: 2.1684999465942383
Validation loss: 2.167698907595809

Epoch: 5| Step: 10
Training loss: 2.378593921661377
Validation loss: 2.238471797717515

Epoch: 81| Step: 0
Training loss: 2.749650478363037
Validation loss: 2.245577966013262

Epoch: 5| Step: 1
Training loss: 2.6222002506256104
Validation loss: 2.2148525048327703

Epoch: 5| Step: 2
Training loss: 2.499737024307251
Validation loss: 2.195971001860916

Epoch: 5| Step: 3
Training loss: 2.781496524810791
Validation loss: 2.177214945516279

Epoch: 5| Step: 4
Training loss: 1.989654779434204
Validation loss: 2.106285748943206

Epoch: 5| Step: 5
Training loss: 1.991931676864624
Validation loss: 2.0930814794314805

Epoch: 5| Step: 6
Training loss: 2.209958553314209
Validation loss: 2.1083328339361374

Epoch: 5| Step: 7
Training loss: 2.5637083053588867
Validation loss: 2.130240571114325

Epoch: 5| Step: 8
Training loss: 2.316114902496338
Validation loss: 2.1182403718271563

Epoch: 5| Step: 9
Training loss: 2.2074809074401855
Validation loss: 2.1146874376522597

Epoch: 5| Step: 10
Training loss: 2.8055460453033447
Validation loss: 2.113065027421521

Epoch: 82| Step: 0
Training loss: 1.683172583580017
Validation loss: 2.1043532458684777

Epoch: 5| Step: 1
Training loss: 2.403332233428955
Validation loss: 2.101140881097445

Epoch: 5| Step: 2
Training loss: 2.6308891773223877
Validation loss: 2.1125680118478756

Epoch: 5| Step: 3
Training loss: 2.241626501083374
Validation loss: 2.1740952499451174

Epoch: 5| Step: 4
Training loss: 2.2660775184631348
Validation loss: 2.2415286853749263

Epoch: 5| Step: 5
Training loss: 2.656856060028076
Validation loss: 2.233156088859804

Epoch: 5| Step: 6
Training loss: 2.341270923614502
Validation loss: 2.17485245068868

Epoch: 5| Step: 7
Training loss: 2.1088099479675293
Validation loss: 2.1223773853753203

Epoch: 5| Step: 8
Training loss: 2.526474952697754
Validation loss: 2.0960996458607335

Epoch: 5| Step: 9
Training loss: 2.824403762817383
Validation loss: 2.0995154009070447

Epoch: 5| Step: 10
Training loss: 3.0728321075439453
Validation loss: 2.103373073762463

Epoch: 83| Step: 0
Training loss: 2.538949489593506
Validation loss: 2.1080013680201706

Epoch: 5| Step: 1
Training loss: 2.62577486038208
Validation loss: 2.1099632991257535

Epoch: 5| Step: 2
Training loss: 2.337618112564087
Validation loss: 2.1005081668976815

Epoch: 5| Step: 3
Training loss: 3.097012996673584
Validation loss: 2.097246200807633

Epoch: 5| Step: 4
Training loss: 2.7204604148864746
Validation loss: 2.101341391122469

Epoch: 5| Step: 5
Training loss: 1.7730712890625
Validation loss: 2.1020089580166723

Epoch: 5| Step: 6
Training loss: 1.9937055110931396
Validation loss: 2.126980441872792

Epoch: 5| Step: 7
Training loss: 1.9336860179901123
Validation loss: 2.1445947436876196

Epoch: 5| Step: 8
Training loss: 2.3551535606384277
Validation loss: 2.185227260794691

Epoch: 5| Step: 9
Training loss: 2.89483904838562
Validation loss: 2.226091374633133

Epoch: 5| Step: 10
Training loss: 2.2016401290893555
Validation loss: 2.2353181787716445

Epoch: 84| Step: 0
Training loss: 2.2835099697113037
Validation loss: 2.22953942642417

Epoch: 5| Step: 1
Training loss: 2.833888530731201
Validation loss: 2.193142903748379

Epoch: 5| Step: 2
Training loss: 2.5361735820770264
Validation loss: 2.1542562618050525

Epoch: 5| Step: 3
Training loss: 1.479142665863037
Validation loss: 2.127305840933195

Epoch: 5| Step: 4
Training loss: 1.9752413034439087
Validation loss: 2.1067120977627334

Epoch: 5| Step: 5
Training loss: 2.74534273147583
Validation loss: 2.1004822920727473

Epoch: 5| Step: 6
Training loss: 2.320709228515625
Validation loss: 2.097088472817534

Epoch: 5| Step: 7
Training loss: 2.671325206756592
Validation loss: 2.1133862105748986

Epoch: 5| Step: 8
Training loss: 2.4889512062072754
Validation loss: 2.1047660996837

Epoch: 5| Step: 9
Training loss: 2.3398597240448
Validation loss: 2.0973718627806632

Epoch: 5| Step: 10
Training loss: 2.731787919998169
Validation loss: 2.0968732474952616

Epoch: 85| Step: 0
Training loss: 2.227735757827759
Validation loss: 2.091794870233023

Epoch: 5| Step: 1
Training loss: 2.6378533840179443
Validation loss: 2.0875473381370626

Epoch: 5| Step: 2
Training loss: 2.2537217140197754
Validation loss: 2.103565150691617

Epoch: 5| Step: 3
Training loss: 2.5226364135742188
Validation loss: 2.1066237572700746

Epoch: 5| Step: 4
Training loss: 2.6743218898773193
Validation loss: 2.1074514594129337

Epoch: 5| Step: 5
Training loss: 2.222853183746338
Validation loss: 2.0985422493309103

Epoch: 5| Step: 6
Training loss: 2.2583935260772705
Validation loss: 2.090976622796828

Epoch: 5| Step: 7
Training loss: 1.9052737951278687
Validation loss: 2.1021143518468386

Epoch: 5| Step: 8
Training loss: 2.2870326042175293
Validation loss: 2.078498314785701

Epoch: 5| Step: 9
Training loss: 2.0345468521118164
Validation loss: 2.086489797920309

Epoch: 5| Step: 10
Training loss: 3.012502670288086
Validation loss: 2.0820212441106

Epoch: 86| Step: 0
Training loss: 2.7462260723114014
Validation loss: 2.0820522487804456

Epoch: 5| Step: 1
Training loss: 2.3830342292785645
Validation loss: 2.0855916405236847

Epoch: 5| Step: 2
Training loss: 2.238755464553833
Validation loss: 2.095338906011274

Epoch: 5| Step: 3
Training loss: 1.9678713083267212
Validation loss: 2.091610203507126

Epoch: 5| Step: 4
Training loss: 2.5383522510528564
Validation loss: 2.0879041815316803

Epoch: 5| Step: 5
Training loss: 2.771684408187866
Validation loss: 2.0866355101267495

Epoch: 5| Step: 6
Training loss: 1.7903320789337158
Validation loss: 2.104493344983747

Epoch: 5| Step: 7
Training loss: 2.183009624481201
Validation loss: 2.1164390156345982

Epoch: 5| Step: 8
Training loss: 2.424492835998535
Validation loss: 2.100679994911276

Epoch: 5| Step: 9
Training loss: 2.396366596221924
Validation loss: 2.1070397182177474

Epoch: 5| Step: 10
Training loss: 2.568124294281006
Validation loss: 2.108965089244227

Epoch: 87| Step: 0
Training loss: 2.637383460998535
Validation loss: 2.124504489283408

Epoch: 5| Step: 1
Training loss: 2.6113572120666504
Validation loss: 2.082434287635229

Epoch: 5| Step: 2
Training loss: 2.5608346462249756
Validation loss: 2.1160798354815413

Epoch: 5| Step: 3
Training loss: 2.6630263328552246
Validation loss: 2.105423870907035

Epoch: 5| Step: 4
Training loss: 2.2249839305877686
Validation loss: 2.1000778418715282

Epoch: 5| Step: 5
Training loss: 2.194169282913208
Validation loss: 2.1160868880569295

Epoch: 5| Step: 6
Training loss: 2.255929946899414
Validation loss: 2.093175088205645

Epoch: 5| Step: 7
Training loss: 2.3103508949279785
Validation loss: 2.1045942870519494

Epoch: 5| Step: 8
Training loss: 2.269073963165283
Validation loss: 2.1177490603539253

Epoch: 5| Step: 9
Training loss: 2.336388349533081
Validation loss: 2.0982314514857467

Epoch: 5| Step: 10
Training loss: 1.926604151725769
Validation loss: 2.08774297980852

Epoch: 88| Step: 0
Training loss: 2.0077896118164062
Validation loss: 2.0861104175608647

Epoch: 5| Step: 1
Training loss: 2.578834056854248
Validation loss: 2.079363787046043

Epoch: 5| Step: 2
Training loss: 2.580289363861084
Validation loss: 2.0832552397122948

Epoch: 5| Step: 3
Training loss: 2.5548295974731445
Validation loss: 2.0837145748958794

Epoch: 5| Step: 4
Training loss: 1.7245442867279053
Validation loss: 2.083225093862062

Epoch: 5| Step: 5
Training loss: 2.4685351848602295
Validation loss: 2.099314958818497

Epoch: 5| Step: 6
Training loss: 2.590986728668213
Validation loss: 2.103284446142053

Epoch: 5| Step: 7
Training loss: 2.8009371757507324
Validation loss: 2.083726220233466

Epoch: 5| Step: 8
Training loss: 2.308375835418701
Validation loss: 2.0753963672986595

Epoch: 5| Step: 9
Training loss: 2.517449140548706
Validation loss: 2.0725637789695495

Epoch: 5| Step: 10
Training loss: 1.9073354005813599
Validation loss: 2.065656676087328

Epoch: 89| Step: 0
Training loss: 1.9875869750976562
Validation loss: 2.0778178835427887

Epoch: 5| Step: 1
Training loss: 2.8383285999298096
Validation loss: 2.0764167924081125

Epoch: 5| Step: 2
Training loss: 3.0908889770507812
Validation loss: 2.0656776453859065

Epoch: 5| Step: 3
Training loss: 3.0168583393096924
Validation loss: 2.0857468420459377

Epoch: 5| Step: 4
Training loss: 2.183042526245117
Validation loss: 2.086754870671098

Epoch: 5| Step: 5
Training loss: 2.3520777225494385
Validation loss: 2.0881041865195

Epoch: 5| Step: 6
Training loss: 2.133206367492676
Validation loss: 2.0908155056738083

Epoch: 5| Step: 7
Training loss: 2.0747978687286377
Validation loss: 2.0941207472996046

Epoch: 5| Step: 8
Training loss: 2.1909186840057373
Validation loss: 2.093126630270353

Epoch: 5| Step: 9
Training loss: 2.1022191047668457
Validation loss: 2.088884820220291

Epoch: 5| Step: 10
Training loss: 1.9183565378189087
Validation loss: 2.101074810950987

Epoch: 90| Step: 0
Training loss: 2.4865074157714844
Validation loss: 2.1263149066637923

Epoch: 5| Step: 1
Training loss: 2.674373149871826
Validation loss: 2.1330441941497145

Epoch: 5| Step: 2
Training loss: 2.0807077884674072
Validation loss: 2.1498355891114924

Epoch: 5| Step: 3
Training loss: 2.264469623565674
Validation loss: 2.1218509520253828

Epoch: 5| Step: 4
Training loss: 2.3651013374328613
Validation loss: 2.103563775298416

Epoch: 5| Step: 5
Training loss: 2.574504852294922
Validation loss: 2.0868301391601562

Epoch: 5| Step: 6
Training loss: 2.3930506706237793
Validation loss: 2.0751798409287647

Epoch: 5| Step: 7
Training loss: 2.8392996788024902
Validation loss: 2.072498160023843

Epoch: 5| Step: 8
Training loss: 2.7252488136291504
Validation loss: 2.0699195938725627

Epoch: 5| Step: 9
Training loss: 2.0250768661499023
Validation loss: 2.0612674784916702

Epoch: 5| Step: 10
Training loss: 1.3240243196487427
Validation loss: 2.0717072268967986

Epoch: 91| Step: 0
Training loss: 2.306184768676758
Validation loss: 2.0758951607570855

Epoch: 5| Step: 1
Training loss: 2.7276527881622314
Validation loss: 2.0639313869578864

Epoch: 5| Step: 2
Training loss: 2.4144320487976074
Validation loss: 2.0605169188591743

Epoch: 5| Step: 3
Training loss: 3.016350269317627
Validation loss: 2.065116782342234

Epoch: 5| Step: 4
Training loss: 2.381208896636963
Validation loss: 2.06646317051303

Epoch: 5| Step: 5
Training loss: 2.0966362953186035
Validation loss: 2.0632320937289985

Epoch: 5| Step: 6
Training loss: 2.3191306591033936
Validation loss: 2.060895748035882

Epoch: 5| Step: 7
Training loss: 2.6180553436279297
Validation loss: 2.0645302700740036

Epoch: 5| Step: 8
Training loss: 2.419055223464966
Validation loss: 2.068079927916168

Epoch: 5| Step: 9
Training loss: 1.7158210277557373
Validation loss: 2.0947047318181684

Epoch: 5| Step: 10
Training loss: 1.8577089309692383
Validation loss: 2.095698182300855

Epoch: 92| Step: 0
Training loss: 2.2077860832214355
Validation loss: 2.0856010580575592

Epoch: 5| Step: 1
Training loss: 2.8661441802978516
Validation loss: 2.090142385933989

Epoch: 5| Step: 2
Training loss: 2.5080339908599854
Validation loss: 2.068543600779708

Epoch: 5| Step: 3
Training loss: 2.67114520072937
Validation loss: 2.0678745110829673

Epoch: 5| Step: 4
Training loss: 2.6687049865722656
Validation loss: 2.052686322119928

Epoch: 5| Step: 5
Training loss: 2.067291021347046
Validation loss: 2.0574831654948573

Epoch: 5| Step: 6
Training loss: 1.5522037744522095
Validation loss: 2.047365232180524

Epoch: 5| Step: 7
Training loss: 2.445798397064209
Validation loss: 2.0466788943095873

Epoch: 5| Step: 8
Training loss: 2.3144164085388184
Validation loss: 2.058549188798474

Epoch: 5| Step: 9
Training loss: 2.532597064971924
Validation loss: 2.0571584265719176

Epoch: 5| Step: 10
Training loss: 1.8121989965438843
Validation loss: 2.069052791082731

Epoch: 93| Step: 0
Training loss: 2.25887393951416
Validation loss: 2.0653332177028862

Epoch: 5| Step: 1
Training loss: 3.1592202186584473
Validation loss: 2.0609378558333202

Epoch: 5| Step: 2
Training loss: 2.3978192806243896
Validation loss: 2.070608297983805

Epoch: 5| Step: 3
Training loss: 2.290544271469116
Validation loss: 2.0625700014893726

Epoch: 5| Step: 4
Training loss: 2.2060132026672363
Validation loss: 2.0490287042433217

Epoch: 5| Step: 5
Training loss: 2.7378854751586914
Validation loss: 2.0625523674872612

Epoch: 5| Step: 6
Training loss: 2.626919984817505
Validation loss: 2.04571726886175

Epoch: 5| Step: 7
Training loss: 1.6996228694915771
Validation loss: 2.0522204470890824

Epoch: 5| Step: 8
Training loss: 2.1487908363342285
Validation loss: 2.068110248093964

Epoch: 5| Step: 9
Training loss: 2.1741809844970703
Validation loss: 2.082521221970999

Epoch: 5| Step: 10
Training loss: 1.942460060119629
Validation loss: 2.1084123811414166

Epoch: 94| Step: 0
Training loss: 2.2869811058044434
Validation loss: 2.1199694064355667

Epoch: 5| Step: 1
Training loss: 2.2330498695373535
Validation loss: 2.107800619576567

Epoch: 5| Step: 2
Training loss: 2.4661781787872314
Validation loss: 2.092513874012937

Epoch: 5| Step: 3
Training loss: 2.816190719604492
Validation loss: 2.077075012268559

Epoch: 5| Step: 4
Training loss: 1.7454516887664795
Validation loss: 2.056571865594515

Epoch: 5| Step: 5
Training loss: 2.475100040435791
Validation loss: 2.0732821264574604

Epoch: 5| Step: 6
Training loss: 2.708085060119629
Validation loss: 2.0554403592181463

Epoch: 5| Step: 7
Training loss: 1.8485958576202393
Validation loss: 2.051195688145135

Epoch: 5| Step: 8
Training loss: 2.152104377746582
Validation loss: 2.0569518919914

Epoch: 5| Step: 9
Training loss: 2.6225943565368652
Validation loss: 2.0726610614407446

Epoch: 5| Step: 10
Training loss: 2.1801040172576904
Validation loss: 2.0498362279707387

Epoch: 95| Step: 0
Training loss: 1.9058706760406494
Validation loss: 2.0706857673583494

Epoch: 5| Step: 1
Training loss: 1.8354743719100952
Validation loss: 2.0744774469765286

Epoch: 5| Step: 2
Training loss: 3.13403058052063
Validation loss: 2.0478000051231793

Epoch: 5| Step: 3
Training loss: 2.67925763130188
Validation loss: 2.034623446003083

Epoch: 5| Step: 4
Training loss: 2.0144059658050537
Validation loss: 2.0409297353477887

Epoch: 5| Step: 5
Training loss: 2.8372738361358643
Validation loss: 2.0459317571373394

Epoch: 5| Step: 6
Training loss: 2.106750965118408
Validation loss: 2.048890568876779

Epoch: 5| Step: 7
Training loss: 2.03938627243042
Validation loss: 2.0488247012579315

Epoch: 5| Step: 8
Training loss: 2.6894233226776123
Validation loss: 2.0647554935947543

Epoch: 5| Step: 9
Training loss: 2.1988556385040283
Validation loss: 2.0555106273261448

Epoch: 5| Step: 10
Training loss: 2.1458194255828857
Validation loss: 2.069957612663187

Epoch: 96| Step: 0
Training loss: 2.380695104598999
Validation loss: 2.076198790663032

Epoch: 5| Step: 1
Training loss: 2.0379621982574463
Validation loss: 2.065561689356322

Epoch: 5| Step: 2
Training loss: 2.3777871131896973
Validation loss: 2.040415420327135

Epoch: 5| Step: 3
Training loss: 2.1130287647247314
Validation loss: 2.041892833607171

Epoch: 5| Step: 4
Training loss: 1.8252846002578735
Validation loss: 2.042219977225027

Epoch: 5| Step: 5
Training loss: 2.0775065422058105
Validation loss: 2.0414229823696997

Epoch: 5| Step: 6
Training loss: 2.200345039367676
Validation loss: 2.0277281807314966

Epoch: 5| Step: 7
Training loss: 2.88978910446167
Validation loss: 2.0250850467271704

Epoch: 5| Step: 8
Training loss: 2.5695176124572754
Validation loss: 2.0385737342219197

Epoch: 5| Step: 9
Training loss: 2.3709452152252197
Validation loss: 2.0351306930665047

Epoch: 5| Step: 10
Training loss: 2.5452234745025635
Validation loss: 2.0337576302148963

Epoch: 97| Step: 0
Training loss: 2.404932737350464
Validation loss: 2.039535258405952

Epoch: 5| Step: 1
Training loss: 2.4337716102600098
Validation loss: 2.0700637704582623

Epoch: 5| Step: 2
Training loss: 2.2010085582733154
Validation loss: 2.051796385037002

Epoch: 5| Step: 3
Training loss: 2.5581424236297607
Validation loss: 2.048718455017254

Epoch: 5| Step: 4
Training loss: 1.763310194015503
Validation loss: 2.0415153080417263

Epoch: 5| Step: 5
Training loss: 2.4345345497131348
Validation loss: 2.0384879663426387

Epoch: 5| Step: 6
Training loss: 2.657665252685547
Validation loss: 2.065664965619323

Epoch: 5| Step: 7
Training loss: 1.9586474895477295
Validation loss: 2.0689689766976143

Epoch: 5| Step: 8
Training loss: 2.607848644256592
Validation loss: 2.049922266314107

Epoch: 5| Step: 9
Training loss: 2.1531808376312256
Validation loss: 2.041986514163274

Epoch: 5| Step: 10
Training loss: 2.157581090927124
Validation loss: 2.0381444846430132

Epoch: 98| Step: 0
Training loss: 2.1262295246124268
Validation loss: 2.053420320633919

Epoch: 5| Step: 1
Training loss: 2.3054232597351074
Validation loss: 2.048470292040097

Epoch: 5| Step: 2
Training loss: 1.818140983581543
Validation loss: 2.0413147762257564

Epoch: 5| Step: 3
Training loss: 2.386584758758545
Validation loss: 2.0248559751818256

Epoch: 5| Step: 4
Training loss: 2.4667351245880127
Validation loss: 2.0121729809750795

Epoch: 5| Step: 5
Training loss: 2.3262219429016113
Validation loss: 2.0025884400131884

Epoch: 5| Step: 6
Training loss: 2.4250450134277344
Validation loss: 2.021054797275092

Epoch: 5| Step: 7
Training loss: 2.6110925674438477
Validation loss: 2.027568345428795

Epoch: 5| Step: 8
Training loss: 2.1734321117401123
Validation loss: 2.0012120046923236

Epoch: 5| Step: 9
Training loss: 2.1469056606292725
Validation loss: 2.0133231480916343

Epoch: 5| Step: 10
Training loss: 2.6667349338531494
Validation loss: 2.0181730037094443

Epoch: 99| Step: 0
Training loss: 2.6894216537475586
Validation loss: 2.019635710664975

Epoch: 5| Step: 1
Training loss: 2.3285017013549805
Validation loss: 2.0398273801290863

Epoch: 5| Step: 2
Training loss: 1.857743263244629
Validation loss: 2.0553895299152662

Epoch: 5| Step: 3
Training loss: 2.4224090576171875
Validation loss: 2.0633806592674664

Epoch: 5| Step: 4
Training loss: 1.366432547569275
Validation loss: 2.0850182194863596

Epoch: 5| Step: 5
Training loss: 1.9950659275054932
Validation loss: 2.0731644335613457

Epoch: 5| Step: 6
Training loss: 2.7128710746765137
Validation loss: 2.0580576440339446

Epoch: 5| Step: 7
Training loss: 2.497750759124756
Validation loss: 2.040088261327436

Epoch: 5| Step: 8
Training loss: 2.254868268966675
Validation loss: 2.053099286171698

Epoch: 5| Step: 9
Training loss: 2.575554609298706
Validation loss: 2.0372211933135986

Epoch: 5| Step: 10
Training loss: 2.5134072303771973
Validation loss: 2.049839424830611

Epoch: 100| Step: 0
Training loss: 2.3768513202667236
Validation loss: 2.0362240242701706

Epoch: 5| Step: 1
Training loss: 2.729940891265869
Validation loss: 2.025157672102733

Epoch: 5| Step: 2
Training loss: 1.6420587301254272
Validation loss: 2.0258842206770376

Epoch: 5| Step: 3
Training loss: 2.4616682529449463
Validation loss: 2.040866546733405

Epoch: 5| Step: 4
Training loss: 2.176159143447876
Validation loss: 2.063463598169306

Epoch: 5| Step: 5
Training loss: 2.6274943351745605
Validation loss: 2.0543378271082395

Epoch: 5| Step: 6
Training loss: 1.9144887924194336
Validation loss: 2.0608897875714045

Epoch: 5| Step: 7
Training loss: 2.0315051078796387
Validation loss: 2.061548344550594

Epoch: 5| Step: 8
Training loss: 1.8175382614135742
Validation loss: 2.062916250639064

Epoch: 5| Step: 9
Training loss: 2.668257236480713
Validation loss: 2.124144691292958

Epoch: 5| Step: 10
Training loss: 3.0336527824401855
Validation loss: 2.1289882608639297

Epoch: 101| Step: 0
Training loss: 2.4482882022857666
Validation loss: 2.134559739020563

Epoch: 5| Step: 1
Training loss: 2.4864754676818848
Validation loss: 2.142959648563016

Epoch: 5| Step: 2
Training loss: 2.3195502758026123
Validation loss: 2.1393757609910864

Epoch: 5| Step: 3
Training loss: 2.3360729217529297
Validation loss: 2.11310080559023

Epoch: 5| Step: 4
Training loss: 1.6884667873382568
Validation loss: 2.089798342797064

Epoch: 5| Step: 5
Training loss: 2.39036226272583
Validation loss: 2.073548568192349

Epoch: 5| Step: 6
Training loss: 2.4660444259643555
Validation loss: 2.0736624861276276

Epoch: 5| Step: 7
Training loss: 2.6886019706726074
Validation loss: 2.071387155081636

Epoch: 5| Step: 8
Training loss: 2.0648887157440186
Validation loss: 2.072943946366669

Epoch: 5| Step: 9
Training loss: 1.904421091079712
Validation loss: 2.0612711316795758

Epoch: 5| Step: 10
Training loss: 2.3862264156341553
Validation loss: 2.057399131918466

Epoch: 102| Step: 0
Training loss: 2.614777088165283
Validation loss: 2.047598933660856

Epoch: 5| Step: 1
Training loss: 2.290841579437256
Validation loss: 2.0254424041317356

Epoch: 5| Step: 2
Training loss: 2.1157491207122803
Validation loss: 2.0116818733112787

Epoch: 5| Step: 3
Training loss: 2.372223377227783
Validation loss: 2.011736959539434

Epoch: 5| Step: 4
Training loss: 2.4621694087982178
Validation loss: 2.003736510071703

Epoch: 5| Step: 5
Training loss: 1.9866069555282593
Validation loss: 2.0002328913698912

Epoch: 5| Step: 6
Training loss: 2.478402614593506
Validation loss: 1.9995131441341933

Epoch: 5| Step: 7
Training loss: 1.9823650121688843
Validation loss: 1.9976993248026857

Epoch: 5| Step: 8
Training loss: 1.9923986196517944
Validation loss: 2.002705665044887

Epoch: 5| Step: 9
Training loss: 2.1823554039001465
Validation loss: 2.0015168164366033

Epoch: 5| Step: 10
Training loss: 2.4846389293670654
Validation loss: 2.0175243731467956

Epoch: 103| Step: 0
Training loss: 2.5767223834991455
Validation loss: 2.00586409466241

Epoch: 5| Step: 1
Training loss: 2.3370792865753174
Validation loss: 2.0143391663028347

Epoch: 5| Step: 2
Training loss: 2.5080466270446777
Validation loss: 2.0157566608921176

Epoch: 5| Step: 3
Training loss: 2.0039350986480713
Validation loss: 2.0465384478210122

Epoch: 5| Step: 4
Training loss: 2.7462546825408936
Validation loss: 2.0827969504940893

Epoch: 5| Step: 5
Training loss: 2.1945574283599854
Validation loss: 2.095082547075005

Epoch: 5| Step: 6
Training loss: 1.9317264556884766
Validation loss: 2.0585076450019755

Epoch: 5| Step: 7
Training loss: 1.4253251552581787
Validation loss: 2.0332615285791378

Epoch: 5| Step: 8
Training loss: 1.935518503189087
Validation loss: 2.0104707864023026

Epoch: 5| Step: 9
Training loss: 2.525146484375
Validation loss: 2.0126638591930432

Epoch: 5| Step: 10
Training loss: 2.8985061645507812
Validation loss: 2.005482163480533

Epoch: 104| Step: 0
Training loss: 2.0428059101104736
Validation loss: 2.002863178970993

Epoch: 5| Step: 1
Training loss: 2.2176952362060547
Validation loss: 2.01639485743738

Epoch: 5| Step: 2
Training loss: 2.7097246646881104
Validation loss: 2.031607602232246

Epoch: 5| Step: 3
Training loss: 1.5932127237319946
Validation loss: 2.0418411659938034

Epoch: 5| Step: 4
Training loss: 2.6508090496063232
Validation loss: 2.025816507236932

Epoch: 5| Step: 5
Training loss: 2.483264446258545
Validation loss: 2.0214434798045824

Epoch: 5| Step: 6
Training loss: 2.097644329071045
Validation loss: 2.016273615180805

Epoch: 5| Step: 7
Training loss: 2.495079517364502
Validation loss: 2.024878232709823

Epoch: 5| Step: 8
Training loss: 2.606968402862549
Validation loss: 2.034759185647452

Epoch: 5| Step: 9
Training loss: 2.100736141204834
Validation loss: 2.052783340536138

Epoch: 5| Step: 10
Training loss: 2.044027805328369
Validation loss: 2.0479430690888436

Epoch: 105| Step: 0
Training loss: 2.0571274757385254
Validation loss: 2.070236109918164

Epoch: 5| Step: 1
Training loss: 2.8373918533325195
Validation loss: 2.081508077600951

Epoch: 5| Step: 2
Training loss: 1.9506601095199585
Validation loss: 2.129772640043689

Epoch: 5| Step: 3
Training loss: 2.456650972366333
Validation loss: 2.1524458931338404

Epoch: 5| Step: 4
Training loss: 2.7679543495178223
Validation loss: 2.16288237930626

Epoch: 5| Step: 5
Training loss: 1.861920952796936
Validation loss: 2.140915237447267

Epoch: 5| Step: 6
Training loss: 2.384218215942383
Validation loss: 2.1107104337343605

Epoch: 5| Step: 7
Training loss: 2.037501335144043
Validation loss: 2.07171001613781

Epoch: 5| Step: 8
Training loss: 1.8124748468399048
Validation loss: 2.0614942863423336

Epoch: 5| Step: 9
Training loss: 2.4007649421691895
Validation loss: 2.0692249626241703

Epoch: 5| Step: 10
Training loss: 2.330618381500244
Validation loss: 2.0613513082586308

Epoch: 106| Step: 0
Training loss: 2.378145217895508
Validation loss: 2.047968485022104

Epoch: 5| Step: 1
Training loss: 2.3365070819854736
Validation loss: 2.0430973537506594

Epoch: 5| Step: 2
Training loss: 2.1599040031433105
Validation loss: 2.046263930618122

Epoch: 5| Step: 3
Training loss: 2.2362494468688965
Validation loss: 2.0418816151157504

Epoch: 5| Step: 4
Training loss: 1.7189013957977295
Validation loss: 2.0492379101373817

Epoch: 5| Step: 5
Training loss: 2.268282651901245
Validation loss: 2.0633568507368847

Epoch: 5| Step: 6
Training loss: 2.591989755630493
Validation loss: 2.0634675769395727

Epoch: 5| Step: 7
Training loss: 2.238964557647705
Validation loss: 2.0609622193920996

Epoch: 5| Step: 8
Training loss: 2.5497398376464844
Validation loss: 2.049747790059736

Epoch: 5| Step: 9
Training loss: 2.586660385131836
Validation loss: 2.0378291965812765

Epoch: 5| Step: 10
Training loss: 1.9291415214538574
Validation loss: 2.0224757168882634

Epoch: 107| Step: 0
Training loss: 2.3413336277008057
Validation loss: 2.00927052702955

Epoch: 5| Step: 1
Training loss: 2.2901980876922607
Validation loss: 2.0084669654087355

Epoch: 5| Step: 2
Training loss: 2.2345633506774902
Validation loss: 2.010665092416989

Epoch: 5| Step: 3
Training loss: 2.155388355255127
Validation loss: 2.030825989220732

Epoch: 5| Step: 4
Training loss: 1.803227424621582
Validation loss: 2.0567304370223836

Epoch: 5| Step: 5
Training loss: 1.936377763748169
Validation loss: 2.0664831976736746

Epoch: 5| Step: 6
Training loss: 2.0665061473846436
Validation loss: 2.09337430871943

Epoch: 5| Step: 7
Training loss: 2.7637462615966797
Validation loss: 2.1423263293440624

Epoch: 5| Step: 8
Training loss: 2.595186710357666
Validation loss: 2.2212277996924614

Epoch: 5| Step: 9
Training loss: 2.431755542755127
Validation loss: 2.1961795565902547

Epoch: 5| Step: 10
Training loss: 2.2730021476745605
Validation loss: 2.1952211703023603

Epoch: 108| Step: 0
Training loss: 2.4009201526641846
Validation loss: 2.1016294033296647

Epoch: 5| Step: 1
Training loss: 2.703014612197876
Validation loss: 2.0392523914255123

Epoch: 5| Step: 2
Training loss: 2.3136146068573
Validation loss: 1.9921706671355872

Epoch: 5| Step: 3
Training loss: 1.5203914642333984
Validation loss: 2.034165733604021

Epoch: 5| Step: 4
Training loss: 2.5443294048309326
Validation loss: 2.1123041350354432

Epoch: 5| Step: 5
Training loss: 2.2950072288513184
Validation loss: 2.143784622992239

Epoch: 5| Step: 6
Training loss: 1.832374930381775
Validation loss: 2.1330203933100544

Epoch: 5| Step: 7
Training loss: 2.375382423400879
Validation loss: 2.113685264382311

Epoch: 5| Step: 8
Training loss: 2.4103012084960938
Validation loss: 2.0521587069316576

Epoch: 5| Step: 9
Training loss: 2.716338872909546
Validation loss: 1.9823220237608878

Epoch: 5| Step: 10
Training loss: 2.5675182342529297
Validation loss: 1.9848621532481203

Epoch: 109| Step: 0
Training loss: 2.4723334312438965
Validation loss: 2.003184503124606

Epoch: 5| Step: 1
Training loss: 2.494101047515869
Validation loss: 2.027654278662897

Epoch: 5| Step: 2
Training loss: 2.1510376930236816
Validation loss: 2.0513762017732025

Epoch: 5| Step: 3
Training loss: 2.699755907058716
Validation loss: 2.0536686451204362

Epoch: 5| Step: 4
Training loss: 2.870985269546509
Validation loss: 2.061786349101733

Epoch: 5| Step: 5
Training loss: 2.0990705490112305
Validation loss: 2.088784137079793

Epoch: 5| Step: 6
Training loss: 1.6484464406967163
Validation loss: 2.077165860001759

Epoch: 5| Step: 7
Training loss: 2.378274917602539
Validation loss: 2.1060766507220525

Epoch: 5| Step: 8
Training loss: 2.4237396717071533
Validation loss: 2.1172556120862245

Epoch: 5| Step: 9
Training loss: 1.9792616367340088
Validation loss: 2.1304890596738426

Epoch: 5| Step: 10
Training loss: 1.9608291387557983
Validation loss: 2.084046315121394

Epoch: 110| Step: 0
Training loss: 2.052579164505005
Validation loss: 2.0727267342229045

Epoch: 5| Step: 1
Training loss: 2.59435772895813
Validation loss: 2.047727213110975

Epoch: 5| Step: 2
Training loss: 2.649564027786255
Validation loss: 2.044483446305798

Epoch: 5| Step: 3
Training loss: 2.6162092685699463
Validation loss: 2.058421065730433

Epoch: 5| Step: 4
Training loss: 1.3839203119277954
Validation loss: 2.061206453589983

Epoch: 5| Step: 5
Training loss: 2.1690564155578613
Validation loss: 2.0552704667532318

Epoch: 5| Step: 6
Training loss: 3.113246440887451
Validation loss: 2.050472544085595

Epoch: 5| Step: 7
Training loss: 2.3925087451934814
Validation loss: 2.049726939970447

Epoch: 5| Step: 8
Training loss: 1.7544448375701904
Validation loss: 2.0327541981973956

Epoch: 5| Step: 9
Training loss: 2.224857807159424
Validation loss: 2.0328724691944737

Epoch: 5| Step: 10
Training loss: 1.9011319875717163
Validation loss: 2.0159959459817536

Epoch: 111| Step: 0
Training loss: 2.49552845954895
Validation loss: 2.0046232797766246

Epoch: 5| Step: 1
Training loss: 2.3980727195739746
Validation loss: 1.9893145471490838

Epoch: 5| Step: 2
Training loss: 1.9891523122787476
Validation loss: 1.9747593172134892

Epoch: 5| Step: 3
Training loss: 2.225119113922119
Validation loss: 1.9667737740342335

Epoch: 5| Step: 4
Training loss: 2.6167585849761963
Validation loss: 1.9724952097861999

Epoch: 5| Step: 5
Training loss: 2.045743227005005
Validation loss: 1.9901978405573035

Epoch: 5| Step: 6
Training loss: 2.407200574874878
Validation loss: 1.9922607816675657

Epoch: 5| Step: 7
Training loss: 1.7642250061035156
Validation loss: 2.0091340080384286

Epoch: 5| Step: 8
Training loss: 2.0357441902160645
Validation loss: 2.01352338124347

Epoch: 5| Step: 9
Training loss: 2.316072940826416
Validation loss: 2.0286614625684676

Epoch: 5| Step: 10
Training loss: 2.2089366912841797
Validation loss: 2.035877840493315

Epoch: 112| Step: 0
Training loss: 2.000000238418579
Validation loss: 2.0554355523919545

Epoch: 5| Step: 1
Training loss: 1.9261897802352905
Validation loss: 2.10923514058513

Epoch: 5| Step: 2
Training loss: 1.9126087427139282
Validation loss: 2.133946141889018

Epoch: 5| Step: 3
Training loss: 2.416105270385742
Validation loss: 2.1129265164816253

Epoch: 5| Step: 4
Training loss: 2.750943660736084
Validation loss: 2.082104099694119

Epoch: 5| Step: 5
Training loss: 2.5622973442077637
Validation loss: 2.042051604999009

Epoch: 5| Step: 6
Training loss: 1.809920310974121
Validation loss: 2.0125845016971713

Epoch: 5| Step: 7
Training loss: 2.1031830310821533
Validation loss: 1.9886514743169148

Epoch: 5| Step: 8
Training loss: 1.9539167881011963
Validation loss: 1.99043579511745

Epoch: 5| Step: 9
Training loss: 2.636209726333618
Validation loss: 1.9846290542233376

Epoch: 5| Step: 10
Training loss: 2.597708225250244
Validation loss: 1.988306330096337

Epoch: 113| Step: 0
Training loss: 2.250917673110962
Validation loss: 1.9962297370356898

Epoch: 5| Step: 1
Training loss: 1.660448431968689
Validation loss: 2.002028255052464

Epoch: 5| Step: 2
Training loss: 2.376054048538208
Validation loss: 2.002245495396276

Epoch: 5| Step: 3
Training loss: 1.9836256504058838
Validation loss: 2.0092473517182055

Epoch: 5| Step: 4
Training loss: 2.2670347690582275
Validation loss: 2.0102020540545062

Epoch: 5| Step: 5
Training loss: 2.700904130935669
Validation loss: 2.0146930884289485

Epoch: 5| Step: 6
Training loss: 2.655627489089966
Validation loss: 2.0101518912981917

Epoch: 5| Step: 7
Training loss: 1.8625662326812744
Validation loss: 2.030844775579309

Epoch: 5| Step: 8
Training loss: 2.0575881004333496
Validation loss: 2.0363718078982447

Epoch: 5| Step: 9
Training loss: 2.0029070377349854
Validation loss: 2.0479124553741945

Epoch: 5| Step: 10
Training loss: 2.2268240451812744
Validation loss: 2.071554821024659

Epoch: 114| Step: 0
Training loss: 1.8828121423721313
Validation loss: 2.1044433732186594

Epoch: 5| Step: 1
Training loss: 2.4373176097869873
Validation loss: 2.1104375316250708

Epoch: 5| Step: 2
Training loss: 2.588374614715576
Validation loss: 2.1076038165759017

Epoch: 5| Step: 3
Training loss: 2.4456913471221924
Validation loss: 2.122684171122889

Epoch: 5| Step: 4
Training loss: 1.8895533084869385
Validation loss: 2.134805356302569

Epoch: 5| Step: 5
Training loss: 2.50504732131958
Validation loss: 2.0876129622100503

Epoch: 5| Step: 6
Training loss: 1.8724753856658936
Validation loss: 2.0821707094869306

Epoch: 5| Step: 7
Training loss: 2.354349136352539
Validation loss: 2.072307686651907

Epoch: 5| Step: 8
Training loss: 2.618929624557495
Validation loss: 2.091356191583859

Epoch: 5| Step: 9
Training loss: 1.9238975048065186
Validation loss: 2.1047947996406147

Epoch: 5| Step: 10
Training loss: 1.933135747909546
Validation loss: 2.083118865566869

Epoch: 115| Step: 0
Training loss: 2.2981419563293457
Validation loss: 2.0713963380423923

Epoch: 5| Step: 1
Training loss: 2.0995099544525146
Validation loss: 2.0697863858233214

Epoch: 5| Step: 2
Training loss: 2.329617977142334
Validation loss: 2.0875002799495572

Epoch: 5| Step: 3
Training loss: 1.660077691078186
Validation loss: 2.099956532960297

Epoch: 5| Step: 4
Training loss: 1.9788026809692383
Validation loss: 2.124301069526262

Epoch: 5| Step: 5
Training loss: 2.5569612979888916
Validation loss: 2.112665340464602

Epoch: 5| Step: 6
Training loss: 2.520113706588745
Validation loss: 2.0901894300214705

Epoch: 5| Step: 7
Training loss: 1.8473212718963623
Validation loss: 2.033163353961001

Epoch: 5| Step: 8
Training loss: 2.148055076599121
Validation loss: 2.036501180741095

Epoch: 5| Step: 9
Training loss: 1.6415274143218994
Validation loss: 2.0029212992678405

Epoch: 5| Step: 10
Training loss: 2.921030044555664
Validation loss: 2.002209342936034

Epoch: 116| Step: 0
Training loss: 2.2794029712677
Validation loss: 1.9830033304870769

Epoch: 5| Step: 1
Training loss: 2.0907702445983887
Validation loss: 1.9699159822156351

Epoch: 5| Step: 2
Training loss: 1.9420692920684814
Validation loss: 1.9943555657581618

Epoch: 5| Step: 3
Training loss: 1.4278484582901
Validation loss: 2.016293333422753

Epoch: 5| Step: 4
Training loss: 2.2303543090820312
Validation loss: 2.0447658723400486

Epoch: 5| Step: 5
Training loss: 2.7400407791137695
Validation loss: 2.1049853742763562

Epoch: 5| Step: 6
Training loss: 2.481915235519409
Validation loss: 2.1313192998209307

Epoch: 5| Step: 7
Training loss: 2.3059897422790527
Validation loss: 2.16665328702619

Epoch: 5| Step: 8
Training loss: 1.954332709312439
Validation loss: 2.179096096305437

Epoch: 5| Step: 9
Training loss: 1.9421154260635376
Validation loss: 2.122401516924622

Epoch: 5| Step: 10
Training loss: 2.614691734313965
Validation loss: 2.0800254293667373

Epoch: 117| Step: 0
Training loss: 1.8746192455291748
Validation loss: 2.0447510134789253

Epoch: 5| Step: 1
Training loss: 2.5868608951568604
Validation loss: 2.0373468809230353

Epoch: 5| Step: 2
Training loss: 2.8320910930633545
Validation loss: 2.0399922606765584

Epoch: 5| Step: 3
Training loss: 2.239069700241089
Validation loss: 2.04880363710465

Epoch: 5| Step: 4
Training loss: 1.1567847728729248
Validation loss: 2.058566972773562

Epoch: 5| Step: 5
Training loss: 1.8803291320800781
Validation loss: 2.040519457991405

Epoch: 5| Step: 6
Training loss: 2.6706671714782715
Validation loss: 2.0356023491069837

Epoch: 5| Step: 7
Training loss: 2.3009300231933594
Validation loss: 2.0301366826539398

Epoch: 5| Step: 8
Training loss: 2.4084243774414062
Validation loss: 2.0169153239137385

Epoch: 5| Step: 9
Training loss: 2.4029784202575684
Validation loss: 2.0327608675085087

Epoch: 5| Step: 10
Training loss: 1.812031865119934
Validation loss: 2.0471187394152404

Epoch: 118| Step: 0
Training loss: 2.089890956878662
Validation loss: 2.066361742634927

Epoch: 5| Step: 1
Training loss: 2.401273250579834
Validation loss: 2.055792277859103

Epoch: 5| Step: 2
Training loss: 2.11623215675354
Validation loss: 2.049756016782535

Epoch: 5| Step: 3
Training loss: 2.2085015773773193
Validation loss: 2.0180767954036756

Epoch: 5| Step: 4
Training loss: 1.9525638818740845
Validation loss: 2.062338788022277

Epoch: 5| Step: 5
Training loss: 1.8824878931045532
Validation loss: 2.1099620480691232

Epoch: 5| Step: 6
Training loss: 2.136806011199951
Validation loss: 2.12844616751517

Epoch: 5| Step: 7
Training loss: 2.2734274864196777
Validation loss: 2.1373804743571947

Epoch: 5| Step: 8
Training loss: 2.5823893547058105
Validation loss: 2.1567591364665697

Epoch: 5| Step: 9
Training loss: 2.3087222576141357
Validation loss: 2.15664715279815

Epoch: 5| Step: 10
Training loss: 2.496044874191284
Validation loss: 2.100610448468116

Epoch: 119| Step: 0
Training loss: 2.171468734741211
Validation loss: 2.0476158870163785

Epoch: 5| Step: 1
Training loss: 1.9660170078277588
Validation loss: 2.0313187081326722

Epoch: 5| Step: 2
Training loss: 1.8767979145050049
Validation loss: 2.0041748375021

Epoch: 5| Step: 3
Training loss: 2.4122707843780518
Validation loss: 2.0106108675720873

Epoch: 5| Step: 4
Training loss: 2.163290023803711
Validation loss: 2.002075308112688

Epoch: 5| Step: 5
Training loss: 2.235795259475708
Validation loss: 2.013201249543057

Epoch: 5| Step: 6
Training loss: 2.5060153007507324
Validation loss: 2.0010973958558935

Epoch: 5| Step: 7
Training loss: 2.3565664291381836
Validation loss: 2.0119723684044293

Epoch: 5| Step: 8
Training loss: 1.9299097061157227
Validation loss: 1.9952381118651359

Epoch: 5| Step: 9
Training loss: 2.2581496238708496
Validation loss: 2.0094600826181392

Epoch: 5| Step: 10
Training loss: 1.640578269958496
Validation loss: 2.0218610443094724

Epoch: 120| Step: 0
Training loss: 2.408616304397583
Validation loss: 2.056816408711095

Epoch: 5| Step: 1
Training loss: 1.735822319984436
Validation loss: 2.057705153701126

Epoch: 5| Step: 2
Training loss: 2.9821925163269043
Validation loss: 2.059133255353538

Epoch: 5| Step: 3
Training loss: 2.425734758377075
Validation loss: 2.0606593752420075

Epoch: 5| Step: 4
Training loss: 1.8990243673324585
Validation loss: 2.0768584564167965

Epoch: 5| Step: 5
Training loss: 1.9990205764770508
Validation loss: 2.0767617302556194

Epoch: 5| Step: 6
Training loss: 2.2194342613220215
Validation loss: 2.0798184922946397

Epoch: 5| Step: 7
Training loss: 1.946422815322876
Validation loss: 2.1166229017319216

Epoch: 5| Step: 8
Training loss: 2.2063496112823486
Validation loss: 2.095352557397658

Epoch: 5| Step: 9
Training loss: 1.750335693359375
Validation loss: 2.0740267307527605

Epoch: 5| Step: 10
Training loss: 1.7154675722122192
Validation loss: 2.0672337483334284

Epoch: 121| Step: 0
Training loss: 1.7382051944732666
Validation loss: 2.079024701990107

Epoch: 5| Step: 1
Training loss: 2.347247362136841
Validation loss: 2.0713563939576507

Epoch: 5| Step: 2
Training loss: 2.5004453659057617
Validation loss: 2.0631253360420145

Epoch: 5| Step: 3
Training loss: 2.4013688564300537
Validation loss: 2.0578161003769084

Epoch: 5| Step: 4
Training loss: 1.8021376132965088
Validation loss: 2.0815933160884406

Epoch: 5| Step: 5
Training loss: 2.5139377117156982
Validation loss: 2.095379016732657

Epoch: 5| Step: 6
Training loss: 2.0553760528564453
Validation loss: 2.099188768735496

Epoch: 5| Step: 7
Training loss: 1.4182647466659546
Validation loss: 2.137103360186341

Epoch: 5| Step: 8
Training loss: 2.3045876026153564
Validation loss: 2.162547480675482

Epoch: 5| Step: 9
Training loss: 2.7558224201202393
Validation loss: 2.12577635242093

Epoch: 5| Step: 10
Training loss: 1.2053402662277222
Validation loss: 2.0991088151931763

Epoch: 122| Step: 0
Training loss: 1.6381587982177734
Validation loss: 2.0749163678897324

Epoch: 5| Step: 1
Training loss: 1.9553951025009155
Validation loss: 2.067070162424477

Epoch: 5| Step: 2
Training loss: 2.4273979663848877
Validation loss: 2.060549195094775

Epoch: 5| Step: 3
Training loss: 2.562267780303955
Validation loss: 2.0984340572869904

Epoch: 5| Step: 4
Training loss: 2.0877156257629395
Validation loss: 2.099924046506164

Epoch: 5| Step: 5
Training loss: 1.6837871074676514
Validation loss: 2.1004074081297843

Epoch: 5| Step: 6
Training loss: 1.9919605255126953
Validation loss: 2.0615765638248895

Epoch: 5| Step: 7
Training loss: 2.235308885574341
Validation loss: 2.0920432280468684

Epoch: 5| Step: 8
Training loss: 2.1455371379852295
Validation loss: 2.085571958172706

Epoch: 5| Step: 9
Training loss: 2.165322780609131
Validation loss: 2.0982851495024977

Epoch: 5| Step: 10
Training loss: 2.0497090816497803
Validation loss: 2.110374096901186

Epoch: 123| Step: 0
Training loss: 2.233875036239624
Validation loss: 2.112660202928769

Epoch: 5| Step: 1
Training loss: 2.271117687225342
Validation loss: 2.1405077518955355

Epoch: 5| Step: 2
Training loss: 2.133976936340332
Validation loss: 2.106342387455766

Epoch: 5| Step: 3
Training loss: 1.4578320980072021
Validation loss: 2.0979703267415366

Epoch: 5| Step: 4
Training loss: 2.0362565517425537
Validation loss: 2.0783661334745345

Epoch: 5| Step: 5
Training loss: 2.117887496948242
Validation loss: 2.0587133489629275

Epoch: 5| Step: 6
Training loss: 2.446213960647583
Validation loss: 2.0510257392801265

Epoch: 5| Step: 7
Training loss: 1.7451503276824951
Validation loss: 2.0466203689575195

Epoch: 5| Step: 8
Training loss: 1.6835931539535522
Validation loss: 2.042947766601398

Epoch: 5| Step: 9
Training loss: 2.3488576412200928
Validation loss: 2.043826960748242

Epoch: 5| Step: 10
Training loss: 2.1988420486450195
Validation loss: 2.0383752135820288

Epoch: 124| Step: 0
Training loss: 2.305234909057617
Validation loss: 2.0609051681333974

Epoch: 5| Step: 1
Training loss: 1.8662773370742798
Validation loss: 2.0583413852158414

Epoch: 5| Step: 2
Training loss: 2.563591241836548
Validation loss: 2.0949622097835747

Epoch: 5| Step: 3
Training loss: 2.0695855617523193
Validation loss: 2.145356712802764

Epoch: 5| Step: 4
Training loss: 2.6290879249572754
Validation loss: 2.1712176107591197

Epoch: 5| Step: 5
Training loss: 1.227850079536438
Validation loss: 2.0761508980105

Epoch: 5| Step: 6
Training loss: 2.272243022918701
Validation loss: 2.051726592484341

Epoch: 5| Step: 7
Training loss: 1.8106979131698608
Validation loss: 2.029921475277152

Epoch: 5| Step: 8
Training loss: 1.4064915180206299
Validation loss: 2.023479361687937

Epoch: 5| Step: 9
Training loss: 2.2668204307556152
Validation loss: 2.016083081563314

Epoch: 5| Step: 10
Training loss: 2.515194892883301
Validation loss: 2.0195820844301613

Epoch: 125| Step: 0
Training loss: 1.8328263759613037
Validation loss: 2.0444714997404363

Epoch: 5| Step: 1
Training loss: 1.4790451526641846
Validation loss: 2.060653466050343

Epoch: 5| Step: 2
Training loss: 2.5245985984802246
Validation loss: 2.0435616764971005

Epoch: 5| Step: 3
Training loss: 2.4127659797668457
Validation loss: 2.0485845637577835

Epoch: 5| Step: 4
Training loss: 1.5150645971298218
Validation loss: 2.0806185686460106

Epoch: 5| Step: 5
Training loss: 1.9191240072250366
Validation loss: 2.100811002075031

Epoch: 5| Step: 6
Training loss: 1.6482019424438477
Validation loss: 2.1245848594173307

Epoch: 5| Step: 7
Training loss: 2.709096908569336
Validation loss: 2.1249737073016424

Epoch: 5| Step: 8
Training loss: 2.2640907764434814
Validation loss: 2.1591649773300334

Epoch: 5| Step: 9
Training loss: 2.2629013061523438
Validation loss: 2.1417425614531322

Epoch: 5| Step: 10
Training loss: 1.9865330457687378
Validation loss: 2.149848497042092

Epoch: 126| Step: 0
Training loss: 2.1681106090545654
Validation loss: 2.11467787911815

Epoch: 5| Step: 1
Training loss: 3.1067445278167725
Validation loss: 2.0838113536116896

Epoch: 5| Step: 2
Training loss: 2.250706195831299
Validation loss: 2.033768928179177

Epoch: 5| Step: 3
Training loss: 1.7888991832733154
Validation loss: 1.9857848690402122

Epoch: 5| Step: 4
Training loss: 1.8533880710601807
Validation loss: 1.9909433062358568

Epoch: 5| Step: 5
Training loss: 1.6904751062393188
Validation loss: 2.0218177303191154

Epoch: 5| Step: 6
Training loss: 2.1088767051696777
Validation loss: 2.0566638387659544

Epoch: 5| Step: 7
Training loss: 1.7410728931427002
Validation loss: 2.0661612761917936

Epoch: 5| Step: 8
Training loss: 2.2467288970947266
Validation loss: 2.076836311688987

Epoch: 5| Step: 9
Training loss: 1.6845697164535522
Validation loss: 2.0864787127382014

Epoch: 5| Step: 10
Training loss: 2.4631192684173584
Validation loss: 2.0655707364441245

Epoch: 127| Step: 0
Training loss: 2.124495029449463
Validation loss: 2.1168266957806003

Epoch: 5| Step: 1
Training loss: 2.3333098888397217
Validation loss: 2.1974843984009116

Epoch: 5| Step: 2
Training loss: 1.8584377765655518
Validation loss: 2.2593545952150897

Epoch: 5| Step: 3
Training loss: 2.2352447509765625
Validation loss: 2.323725369668776

Epoch: 5| Step: 4
Training loss: 2.084784984588623
Validation loss: 2.299301093624484

Epoch: 5| Step: 5
Training loss: 1.935302972793579
Validation loss: 2.1557312614174298

Epoch: 5| Step: 6
Training loss: 1.6245445013046265
Validation loss: 2.0944448940215574

Epoch: 5| Step: 7
Training loss: 1.88385009765625
Validation loss: 2.0169723726088002

Epoch: 5| Step: 8
Training loss: 2.169268846511841
Validation loss: 1.9866168165719638

Epoch: 5| Step: 9
Training loss: 2.3816170692443848
Validation loss: 2.0054052722069526

Epoch: 5| Step: 10
Training loss: 2.591150999069214
Validation loss: 2.001656557924004

Epoch: 128| Step: 0
Training loss: 1.4475092887878418
Validation loss: 2.026954286841936

Epoch: 5| Step: 1
Training loss: 2.1555368900299072
Validation loss: 2.0311269388403943

Epoch: 5| Step: 2
Training loss: 3.1022305488586426
Validation loss: 2.0462153855190484

Epoch: 5| Step: 3
Training loss: 2.180264949798584
Validation loss: 2.0392474064262966

Epoch: 5| Step: 4
Training loss: 2.1069679260253906
Validation loss: 2.038898601326891

Epoch: 5| Step: 5
Training loss: 1.8586584329605103
Validation loss: 2.0435113522314254

Epoch: 5| Step: 6
Training loss: 1.7992759943008423
Validation loss: 2.024685723807222

Epoch: 5| Step: 7
Training loss: 2.5464224815368652
Validation loss: 2.051285966750114

Epoch: 5| Step: 8
Training loss: 1.8969497680664062
Validation loss: 2.109476570160158

Epoch: 5| Step: 9
Training loss: 2.128225326538086
Validation loss: 2.1714110617996543

Epoch: 5| Step: 10
Training loss: 2.357311964035034
Validation loss: 2.2503404155854256

Epoch: 129| Step: 0
Training loss: 1.8805444240570068
Validation loss: 2.2329026460647583

Epoch: 5| Step: 1
Training loss: 2.054685115814209
Validation loss: 2.2032831202271166

Epoch: 5| Step: 2
Training loss: 1.280132532119751
Validation loss: 2.1832970201328235

Epoch: 5| Step: 3
Training loss: 1.851210355758667
Validation loss: 2.1480658003078994

Epoch: 5| Step: 4
Training loss: 1.8409656286239624
Validation loss: 2.1374515359119703

Epoch: 5| Step: 5
Training loss: 2.245858907699585
Validation loss: 2.1317226399657545

Epoch: 5| Step: 6
Training loss: 1.9138072729110718
Validation loss: 2.106452134347731

Epoch: 5| Step: 7
Training loss: 2.6577882766723633
Validation loss: 2.1350601603907924

Epoch: 5| Step: 8
Training loss: 2.1722750663757324
Validation loss: 2.1440712867244596

Epoch: 5| Step: 9
Training loss: 2.6625638008117676
Validation loss: 2.126755909253192

Epoch: 5| Step: 10
Training loss: 2.3022379875183105
Validation loss: 2.070748334289879

Epoch: 130| Step: 0
Training loss: 2.5721817016601562
Validation loss: 2.0415398318280458

Epoch: 5| Step: 1
Training loss: 1.5117788314819336
Validation loss: 2.030960626499627

Epoch: 5| Step: 2
Training loss: 2.2096946239471436
Validation loss: 2.0256113057495444

Epoch: 5| Step: 3
Training loss: 1.6810137033462524
Validation loss: 2.0572045490305912

Epoch: 5| Step: 4
Training loss: 1.4702811241149902
Validation loss: 2.0763328024136123

Epoch: 5| Step: 5
Training loss: 1.994051218032837
Validation loss: 2.107132565590643

Epoch: 5| Step: 6
Training loss: 2.690587043762207
Validation loss: 2.165467873696358

Epoch: 5| Step: 7
Training loss: 2.910166025161743
Validation loss: 2.19103487589026

Epoch: 5| Step: 8
Training loss: 1.797353744506836
Validation loss: 2.2344755588039273

Epoch: 5| Step: 9
Training loss: 2.2672152519226074
Validation loss: 2.203479587390859

Epoch: 5| Step: 10
Training loss: 1.6213520765304565
Validation loss: 2.133617088358889

Epoch: 131| Step: 0
Training loss: 1.1928369998931885
Validation loss: 2.104848175920466

Epoch: 5| Step: 1
Training loss: 2.4507439136505127
Validation loss: 2.1195402760659494

Epoch: 5| Step: 2
Training loss: 2.1721785068511963
Validation loss: 2.103393239359702

Epoch: 5| Step: 3
Training loss: 1.515315294265747
Validation loss: 2.095663409079275

Epoch: 5| Step: 4
Training loss: 1.8925584554672241
Validation loss: 2.059187258443525

Epoch: 5| Step: 5
Training loss: 2.405590295791626
Validation loss: 2.0567547992993425

Epoch: 5| Step: 6
Training loss: 2.224210262298584
Validation loss: 2.0638374141467515

Epoch: 5| Step: 7
Training loss: 2.036565065383911
Validation loss: 2.0604674277767057

Epoch: 5| Step: 8
Training loss: 2.1817736625671387
Validation loss: 2.089223557902921

Epoch: 5| Step: 9
Training loss: 2.5271153450012207
Validation loss: 2.138702800196986

Epoch: 5| Step: 10
Training loss: 1.4759366512298584
Validation loss: 2.133151661965155

Epoch: 132| Step: 0
Training loss: 2.315822124481201
Validation loss: 2.074388124609506

Epoch: 5| Step: 1
Training loss: 1.4981589317321777
Validation loss: 2.0259478938195015

Epoch: 5| Step: 2
Training loss: 2.1725401878356934
Validation loss: 2.0089499668408464

Epoch: 5| Step: 3
Training loss: 1.4540094137191772
Validation loss: 2.0048492313713155

Epoch: 5| Step: 4
Training loss: 2.1301822662353516
Validation loss: 2.0220021970810427

Epoch: 5| Step: 5
Training loss: 2.259904384613037
Validation loss: 2.053878822634297

Epoch: 5| Step: 6
Training loss: 1.9776999950408936
Validation loss: 2.056936546038556

Epoch: 5| Step: 7
Training loss: 1.7244268655776978
Validation loss: 2.0557877530333815

Epoch: 5| Step: 8
Training loss: 1.8575176000595093
Validation loss: 2.0794910512944704

Epoch: 5| Step: 9
Training loss: 2.2190234661102295
Validation loss: 2.2238127954544558

Epoch: 5| Step: 10
Training loss: 2.3286776542663574
Validation loss: 2.374664786041424

Epoch: 133| Step: 0
Training loss: 2.3145394325256348
Validation loss: 2.40117278663061

Epoch: 5| Step: 1
Training loss: 2.6923439502716064
Validation loss: 2.273951748365997

Epoch: 5| Step: 2
Training loss: 1.8736114501953125
Validation loss: 2.153099088258641

Epoch: 5| Step: 3
Training loss: 1.705512285232544
Validation loss: 2.041120562502133

Epoch: 5| Step: 4
Training loss: 1.5834983587265015
Validation loss: 2.007308066532176

Epoch: 5| Step: 5
Training loss: 1.946417212486267
Validation loss: 1.9857752400059854

Epoch: 5| Step: 6
Training loss: 1.5040977001190186
Validation loss: 2.0125398738409883

Epoch: 5| Step: 7
Training loss: 1.2133795022964478
Validation loss: 2.0111508702719085

Epoch: 5| Step: 8
Training loss: 3.329197645187378
Validation loss: 2.0035918143487748

Epoch: 5| Step: 9
Training loss: 1.7869640588760376
Validation loss: 2.033332975961829

Epoch: 5| Step: 10
Training loss: 2.541365623474121
Validation loss: 2.046533764049571

Epoch: 134| Step: 0
Training loss: 1.703573226928711
Validation loss: 2.044996073169093

Epoch: 5| Step: 1
Training loss: 1.7080589532852173
Validation loss: 2.0631042782978346

Epoch: 5| Step: 2
Training loss: 2.102613925933838
Validation loss: 2.0855929864350187

Epoch: 5| Step: 3
Training loss: 1.9576174020767212
Validation loss: 2.1929379765705397

Epoch: 5| Step: 4
Training loss: 2.5189337730407715
Validation loss: 2.222051571774226

Epoch: 5| Step: 5
Training loss: 1.8043582439422607
Validation loss: 2.2141680845650296

Epoch: 5| Step: 6
Training loss: 1.587691068649292
Validation loss: 2.1456957735041136

Epoch: 5| Step: 7
Training loss: 2.0672709941864014
Validation loss: 2.0714992989775953

Epoch: 5| Step: 8
Training loss: 1.758853554725647
Validation loss: 2.045076202320796

Epoch: 5| Step: 9
Training loss: 2.433729887008667
Validation loss: 2.0426464311538206

Epoch: 5| Step: 10
Training loss: 1.9778083562850952
Validation loss: 2.071019898178757

Epoch: 135| Step: 0
Training loss: 1.8669525384902954
Validation loss: 2.0440763247910367

Epoch: 5| Step: 1
Training loss: 2.2059454917907715
Validation loss: 2.03814084042785

Epoch: 5| Step: 2
Training loss: 1.9942362308502197
Validation loss: 2.0557732466728456

Epoch: 5| Step: 3
Training loss: 1.706994652748108
Validation loss: 2.028815092579011

Epoch: 5| Step: 4
Training loss: 2.303497314453125
Validation loss: 2.0892892806760726

Epoch: 5| Step: 5
Training loss: 1.9299780130386353
Validation loss: 2.1122082433392926

Epoch: 5| Step: 6
Training loss: 1.9356663227081299
Validation loss: 2.149647076924642

Epoch: 5| Step: 7
Training loss: 1.9102122783660889
Validation loss: 2.1450629849587717

Epoch: 5| Step: 8
Training loss: 2.3138909339904785
Validation loss: 2.133878950149782

Epoch: 5| Step: 9
Training loss: 2.196305513381958
Validation loss: 2.105839990800427

Epoch: 5| Step: 10
Training loss: 0.8737167119979858
Validation loss: 2.0913506566837268

Epoch: 136| Step: 0
Training loss: 1.4118447303771973
Validation loss: 2.060880502065023

Epoch: 5| Step: 1
Training loss: 2.1736559867858887
Validation loss: 2.0427001240432903

Epoch: 5| Step: 2
Training loss: 2.296269655227661
Validation loss: 2.019694787199779

Epoch: 5| Step: 3
Training loss: 1.8168036937713623
Validation loss: 2.0158348019405077

Epoch: 5| Step: 4
Training loss: 1.5204503536224365
Validation loss: 2.0110399235961256

Epoch: 5| Step: 5
Training loss: 1.922374963760376
Validation loss: 2.0088541866630636

Epoch: 5| Step: 6
Training loss: 2.5145835876464844
Validation loss: 2.022971048150011

Epoch: 5| Step: 7
Training loss: 1.7823455333709717
Validation loss: 2.02828448946758

Epoch: 5| Step: 8
Training loss: 2.011229991912842
Validation loss: 2.0324246562937254

Epoch: 5| Step: 9
Training loss: 1.8850290775299072
Validation loss: 2.071041145632344

Epoch: 5| Step: 10
Training loss: 1.8367444276809692
Validation loss: 2.0833839934359313

Epoch: 137| Step: 0
Training loss: 1.5699656009674072
Validation loss: 2.0738448276314685

Epoch: 5| Step: 1
Training loss: 1.8809702396392822
Validation loss: 2.0823659871214177

Epoch: 5| Step: 2
Training loss: 2.0308098793029785
Validation loss: 2.082966766049785

Epoch: 5| Step: 3
Training loss: 2.16968035697937
Validation loss: 2.0796641201101322

Epoch: 5| Step: 4
Training loss: 1.1306886672973633
Validation loss: 2.0411980305948565

Epoch: 5| Step: 5
Training loss: 2.328186273574829
Validation loss: 2.057337967298364

Epoch: 5| Step: 6
Training loss: 1.5970258712768555
Validation loss: 2.0636084900107434

Epoch: 5| Step: 7
Training loss: 2.467956066131592
Validation loss: 2.1038518977421585

Epoch: 5| Step: 8
Training loss: 2.25425386428833
Validation loss: 2.086743249688097

Epoch: 5| Step: 9
Training loss: 1.427898645401001
Validation loss: 2.033555664041991

Epoch: 5| Step: 10
Training loss: 2.2346432209014893
Validation loss: 2.0182385675368772

Epoch: 138| Step: 0
Training loss: 1.8195457458496094
Validation loss: 2.0444648265838623

Epoch: 5| Step: 1
Training loss: 1.2761512994766235
Validation loss: 2.0820097461823495

Epoch: 5| Step: 2
Training loss: 2.1981780529022217
Validation loss: 2.098755032785477

Epoch: 5| Step: 3
Training loss: 1.6025593280792236
Validation loss: 2.202146186623522

Epoch: 5| Step: 4
Training loss: 2.4053196907043457
Validation loss: 2.2458466252973004

Epoch: 5| Step: 5
Training loss: 2.1856930255889893
Validation loss: 2.3031758903175272

Epoch: 5| Step: 6
Training loss: 1.8608726263046265
Validation loss: 2.2800859687148884

Epoch: 5| Step: 7
Training loss: 1.9641939401626587
Validation loss: 2.2174072457898046

Epoch: 5| Step: 8
Training loss: 1.7769677639007568
Validation loss: 2.1205113600659113

Epoch: 5| Step: 9
Training loss: 1.7614450454711914
Validation loss: 2.0602893021798905

Epoch: 5| Step: 10
Training loss: 2.0609354972839355
Validation loss: 2.030679569449476

Epoch: 139| Step: 0
Training loss: 1.7894508838653564
Validation loss: 1.9928206782187186

Epoch: 5| Step: 1
Training loss: 1.5287584066390991
Validation loss: 1.968378274671493

Epoch: 5| Step: 2
Training loss: 2.8643901348114014
Validation loss: 1.9455787776618876

Epoch: 5| Step: 3
Training loss: 1.3996392488479614
Validation loss: 1.9638774933353547

Epoch: 5| Step: 4
Training loss: 2.248497486114502
Validation loss: 1.9547308221940072

Epoch: 5| Step: 5
Training loss: 1.1134110689163208
Validation loss: 1.9612001449831071

Epoch: 5| Step: 6
Training loss: 2.4518418312072754
Validation loss: 1.9699464818482757

Epoch: 5| Step: 7
Training loss: 2.161172389984131
Validation loss: 2.0166328850612847

Epoch: 5| Step: 8
Training loss: 2.1694464683532715
Validation loss: 2.051498187485562

Epoch: 5| Step: 9
Training loss: 1.7135937213897705
Validation loss: 2.100197627980222

Epoch: 5| Step: 10
Training loss: 1.4316813945770264
Validation loss: 2.1178968260365147

Epoch: 140| Step: 0
Training loss: 2.0764992237091064
Validation loss: 2.1448756981921453

Epoch: 5| Step: 1
Training loss: 1.899397850036621
Validation loss: 2.1595816689152874

Epoch: 5| Step: 2
Training loss: 1.5900700092315674
Validation loss: 2.1352323229594896

Epoch: 5| Step: 3
Training loss: 1.5100657939910889
Validation loss: 2.1004049649802585

Epoch: 5| Step: 4
Training loss: 1.6084131002426147
Validation loss: 2.0588866767062934

Epoch: 5| Step: 5
Training loss: 1.8849385976791382
Validation loss: 2.0517421050738265

Epoch: 5| Step: 6
Training loss: 1.6142889261245728
Validation loss: 2.04192461762377

Epoch: 5| Step: 7
Training loss: 1.9132732152938843
Validation loss: 2.053767490130599

Epoch: 5| Step: 8
Training loss: 2.2092835903167725
Validation loss: 2.067239833134477

Epoch: 5| Step: 9
Training loss: 2.565207004547119
Validation loss: 2.0595816309734056

Epoch: 5| Step: 10
Training loss: 1.5410141944885254
Validation loss: 2.0583677727689027

Epoch: 141| Step: 0
Training loss: 2.43902325630188
Validation loss: 2.0497947457016155

Epoch: 5| Step: 1
Training loss: 1.125256061553955
Validation loss: 2.023584181262601

Epoch: 5| Step: 2
Training loss: 1.681998610496521
Validation loss: 2.0143679572689916

Epoch: 5| Step: 3
Training loss: 1.6650474071502686
Validation loss: 2.046358582794025

Epoch: 5| Step: 4
Training loss: 1.7196853160858154
Validation loss: 2.0611255399642454

Epoch: 5| Step: 5
Training loss: 2.081109046936035
Validation loss: 2.108961405292634

Epoch: 5| Step: 6
Training loss: 2.043792724609375
Validation loss: 2.135742179809078

Epoch: 5| Step: 7
Training loss: 2.7244229316711426
Validation loss: 2.277434620805966

Epoch: 5| Step: 8
Training loss: 1.1569184064865112
Validation loss: 2.2517116326157764

Epoch: 5| Step: 9
Training loss: 2.1020655632019043
Validation loss: 2.124290144571694

Epoch: 5| Step: 10
Training loss: 1.6552494764328003
Validation loss: 2.0519035887974564

Epoch: 142| Step: 0
Training loss: 1.852121114730835
Validation loss: 2.0159297322714202

Epoch: 5| Step: 1
Training loss: 1.7617372274398804
Validation loss: 1.9996168651888448

Epoch: 5| Step: 2
Training loss: 2.032160758972168
Validation loss: 2.027869875713061

Epoch: 5| Step: 3
Training loss: 1.5028042793273926
Validation loss: 2.0050813946672665

Epoch: 5| Step: 4
Training loss: 1.9373201131820679
Validation loss: 1.9890915873230144

Epoch: 5| Step: 5
Training loss: 1.5528390407562256
Validation loss: 2.0094659084914834

Epoch: 5| Step: 6
Training loss: 2.196942090988159
Validation loss: 2.022478292065282

Epoch: 5| Step: 7
Training loss: 1.7033240795135498
Validation loss: 2.035304969356906

Epoch: 5| Step: 8
Training loss: 1.9039462804794312
Validation loss: 2.056799824519824

Epoch: 5| Step: 9
Training loss: 1.4762732982635498
Validation loss: 2.106092183820663

Epoch: 5| Step: 10
Training loss: 2.264089822769165
Validation loss: 2.1713594031590286

Epoch: 143| Step: 0
Training loss: 1.6330931186676025
Validation loss: 2.2021079345416

Epoch: 5| Step: 1
Training loss: 1.9329204559326172
Validation loss: 2.1757468228699057

Epoch: 5| Step: 2
Training loss: 2.090797185897827
Validation loss: 2.103292981783549

Epoch: 5| Step: 3
Training loss: 1.4661270380020142
Validation loss: 2.048325446344191

Epoch: 5| Step: 4
Training loss: 1.927369475364685
Validation loss: 1.991336771236953

Epoch: 5| Step: 5
Training loss: 2.020966053009033
Validation loss: 1.9724676955130793

Epoch: 5| Step: 6
Training loss: 1.3321843147277832
Validation loss: 1.9738532343218405

Epoch: 5| Step: 7
Training loss: 2.1546576023101807
Validation loss: 1.9848644092518797

Epoch: 5| Step: 8
Training loss: 1.6001594066619873
Validation loss: 1.971883112384427

Epoch: 5| Step: 9
Training loss: 2.4121310710906982
Validation loss: 1.9589739896917855

Epoch: 5| Step: 10
Training loss: 2.5037307739257812
Validation loss: 1.9730787097766835

Epoch: 144| Step: 0
Training loss: 2.0344412326812744
Validation loss: 2.005010058802943

Epoch: 5| Step: 1
Training loss: 1.7051481008529663
Validation loss: 2.033418027303552

Epoch: 5| Step: 2
Training loss: 1.588052749633789
Validation loss: 2.083505779184321

Epoch: 5| Step: 3
Training loss: 1.6882997751235962
Validation loss: 2.1316562544914985

Epoch: 5| Step: 4
Training loss: 1.7222248315811157
Validation loss: 2.1579873228585846

Epoch: 5| Step: 5
Training loss: 2.069521903991699
Validation loss: 2.127470462552963

Epoch: 5| Step: 6
Training loss: 1.9048048257827759
Validation loss: 2.108335292467507

Epoch: 5| Step: 7
Training loss: 1.7312209606170654
Validation loss: 2.096776577734178

Epoch: 5| Step: 8
Training loss: 2.586246967315674
Validation loss: 2.0602380639763287

Epoch: 5| Step: 9
Training loss: 1.8450276851654053
Validation loss: 2.035383680815338

Epoch: 5| Step: 10
Training loss: 1.286516547203064
Validation loss: 2.0219787218237437

Epoch: 145| Step: 0
Training loss: 1.4245792627334595
Validation loss: 2.0313474452623757

Epoch: 5| Step: 1
Training loss: 1.9052196741104126
Validation loss: 2.0237579589248984

Epoch: 5| Step: 2
Training loss: 1.719580888748169
Validation loss: 2.0670056471260647

Epoch: 5| Step: 3
Training loss: 1.9395949840545654
Validation loss: 2.0731224629186813

Epoch: 5| Step: 4
Training loss: 2.1190028190612793
Validation loss: 2.0907121576288694

Epoch: 5| Step: 5
Training loss: 1.379325270652771
Validation loss: 2.0671422032899756

Epoch: 5| Step: 6
Training loss: 2.304894208908081
Validation loss: 2.047106942822856

Epoch: 5| Step: 7
Training loss: 1.8556251525878906
Validation loss: 2.020475790064822

Epoch: 5| Step: 8
Training loss: 1.5640144348144531
Validation loss: 2.0026640430573495

Epoch: 5| Step: 9
Training loss: 2.459897994995117
Validation loss: 2.0189661261855916

Epoch: 5| Step: 10
Training loss: 1.477602243423462
Validation loss: 2.0350071140514907

Epoch: 146| Step: 0
Training loss: 1.7323358058929443
Validation loss: 2.0204527352445867

Epoch: 5| Step: 1
Training loss: 1.6361805200576782
Validation loss: 2.002239942550659

Epoch: 5| Step: 2
Training loss: 1.7413709163665771
Validation loss: 2.0049571298783824

Epoch: 5| Step: 3
Training loss: 2.604670763015747
Validation loss: 2.0290418812023696

Epoch: 5| Step: 4
Training loss: 1.6549237966537476
Validation loss: 2.0800738949929514

Epoch: 5| Step: 5
Training loss: 2.155531406402588
Validation loss: 2.129098617902366

Epoch: 5| Step: 6
Training loss: 1.2966011762619019
Validation loss: 2.138200121541177

Epoch: 5| Step: 7
Training loss: 0.9921805262565613
Validation loss: 2.10095275345669

Epoch: 5| Step: 8
Training loss: 2.054745674133301
Validation loss: 2.016138445946478

Epoch: 5| Step: 9
Training loss: 2.1365091800689697
Validation loss: 1.9717559583725468

Epoch: 5| Step: 10
Training loss: 2.340116500854492
Validation loss: 1.962376025415236

Epoch: 147| Step: 0
Training loss: 1.661616325378418
Validation loss: 1.9786653877586446

Epoch: 5| Step: 1
Training loss: 1.9783055782318115
Validation loss: 1.9967473835073493

Epoch: 5| Step: 2
Training loss: 2.059929609298706
Validation loss: 1.9954036615228141

Epoch: 5| Step: 3
Training loss: 2.478677272796631
Validation loss: 1.9962479786206317

Epoch: 5| Step: 4
Training loss: 1.2788723707199097
Validation loss: 2.0139485918065554

Epoch: 5| Step: 5
Training loss: 1.9148975610733032
Validation loss: 2.040842531829752

Epoch: 5| Step: 6
Training loss: 1.485445499420166
Validation loss: 2.075031729154689

Epoch: 5| Step: 7
Training loss: 1.8134387731552124
Validation loss: 2.1577673394192933

Epoch: 5| Step: 8
Training loss: 2.3189711570739746
Validation loss: 2.2587829969262563

Epoch: 5| Step: 9
Training loss: 1.9043395519256592
Validation loss: 2.2525635073261876

Epoch: 5| Step: 10
Training loss: 1.47843599319458
Validation loss: 2.186586623550743

Epoch: 148| Step: 0
Training loss: 2.040503978729248
Validation loss: 2.0771074730862855

Epoch: 5| Step: 1
Training loss: 1.8723701238632202
Validation loss: 2.0287358248105614

Epoch: 5| Step: 2
Training loss: 2.007547616958618
Validation loss: 2.0058495895836943

Epoch: 5| Step: 3
Training loss: 2.1358351707458496
Validation loss: 1.9879173168572046

Epoch: 5| Step: 4
Training loss: 1.1799952983856201
Validation loss: 1.9676281341942408

Epoch: 5| Step: 5
Training loss: 2.041139602661133
Validation loss: 1.9473260487279584

Epoch: 5| Step: 6
Training loss: 1.3061121702194214
Validation loss: 1.9474622870004306

Epoch: 5| Step: 7
Training loss: 2.5957281589508057
Validation loss: 1.947727713533627

Epoch: 5| Step: 8
Training loss: 1.956621527671814
Validation loss: 1.963971762246983

Epoch: 5| Step: 9
Training loss: 1.819004774093628
Validation loss: 1.997527632662045

Epoch: 5| Step: 10
Training loss: 1.1649494171142578
Validation loss: 2.0107953625340618

Epoch: 149| Step: 0
Training loss: 1.409926176071167
Validation loss: 2.0544675537334975

Epoch: 5| Step: 1
Training loss: 1.9253604412078857
Validation loss: 2.0639253688114945

Epoch: 5| Step: 2
Training loss: 1.7602287530899048
Validation loss: 2.1260126970147573

Epoch: 5| Step: 3
Training loss: 2.632068157196045
Validation loss: 2.161025019102199

Epoch: 5| Step: 4
Training loss: 1.3438633680343628
Validation loss: 2.161623404872033

Epoch: 5| Step: 5
Training loss: 1.7999454736709595
Validation loss: 2.1430094895824308

Epoch: 5| Step: 6
Training loss: 1.66156005859375
Validation loss: 2.1115880755967993

Epoch: 5| Step: 7
Training loss: 1.8337398767471313
Validation loss: 2.0697199324125886

Epoch: 5| Step: 8
Training loss: 1.4287225008010864
Validation loss: 2.0506510939649356

Epoch: 5| Step: 9
Training loss: 2.036571502685547
Validation loss: 2.044164608883601

Epoch: 5| Step: 10
Training loss: 1.7933664321899414
Validation loss: 2.046277884514101

Epoch: 150| Step: 0
Training loss: 2.640761613845825
Validation loss: 2.039046379827684

Epoch: 5| Step: 1
Training loss: 1.9633983373641968
Validation loss: 2.028227001108149

Epoch: 5| Step: 2
Training loss: 2.0312137603759766
Validation loss: 2.098238912961816

Epoch: 5| Step: 3
Training loss: 1.6313879489898682
Validation loss: 2.0991602713061916

Epoch: 5| Step: 4
Training loss: 1.2816861867904663
Validation loss: 2.057722765912292

Epoch: 5| Step: 5
Training loss: 1.282791256904602
Validation loss: 2.0076807788623277

Epoch: 5| Step: 6
Training loss: 1.525979995727539
Validation loss: 2.012492226016137

Epoch: 5| Step: 7
Training loss: 2.5834813117980957
Validation loss: 2.005853654235922

Epoch: 5| Step: 8
Training loss: 1.2878835201263428
Validation loss: 2.023643878198439

Epoch: 5| Step: 9
Training loss: 1.7638556957244873
Validation loss: 2.0201408786158406

Epoch: 5| Step: 10
Training loss: 1.2488266229629517
Validation loss: 2.046216330220622

Epoch: 151| Step: 0
Training loss: 1.6538432836532593
Validation loss: 2.0526181267153834

Epoch: 5| Step: 1
Training loss: 1.8035881519317627
Validation loss: 2.0476480145608225

Epoch: 5| Step: 2
Training loss: 1.6756350994110107
Validation loss: 2.0449423661795993

Epoch: 5| Step: 3
Training loss: 1.4898618459701538
Validation loss: 2.0476798908684843

Epoch: 5| Step: 4
Training loss: 2.101147174835205
Validation loss: 2.0975618234244724

Epoch: 5| Step: 5
Training loss: 1.6089601516723633
Validation loss: 2.108263813039308

Epoch: 5| Step: 6
Training loss: 1.6426448822021484
Validation loss: 2.097134347884886

Epoch: 5| Step: 7
Training loss: 2.0519354343414307
Validation loss: 2.0805479608556277

Epoch: 5| Step: 8
Training loss: 1.7416203022003174
Validation loss: 2.035928018631474

Epoch: 5| Step: 9
Training loss: 1.5196828842163086
Validation loss: 2.00584912812838

Epoch: 5| Step: 10
Training loss: 1.6112251281738281
Validation loss: 1.989271463886384

Epoch: 152| Step: 0
Training loss: 1.5111663341522217
Validation loss: 1.9476104077472483

Epoch: 5| Step: 1
Training loss: 1.427587866783142
Validation loss: 1.9658124523778115

Epoch: 5| Step: 2
Training loss: 1.8979402780532837
Validation loss: 1.9881012234636533

Epoch: 5| Step: 3
Training loss: 1.2571513652801514
Validation loss: 1.9921830110652472

Epoch: 5| Step: 4
Training loss: 1.684100866317749
Validation loss: 2.058315719327619

Epoch: 5| Step: 5
Training loss: 2.373563051223755
Validation loss: 2.12287595323337

Epoch: 5| Step: 6
Training loss: 2.0450692176818848
Validation loss: 2.1695798135572866

Epoch: 5| Step: 7
Training loss: 1.6178888082504272
Validation loss: 2.1907625993092856

Epoch: 5| Step: 8
Training loss: 1.9289634227752686
Validation loss: 2.175987769198674

Epoch: 5| Step: 9
Training loss: 1.530771255493164
Validation loss: 2.0951960599550636

Epoch: 5| Step: 10
Training loss: 1.9653187990188599
Validation loss: 2.0533771027800856

Epoch: 153| Step: 0
Training loss: 1.6223056316375732
Validation loss: 2.0372908884479153

Epoch: 5| Step: 1
Training loss: 1.6699886322021484
Validation loss: 2.0184402747820784

Epoch: 5| Step: 2
Training loss: 2.0020253658294678
Validation loss: 2.0101923186291932

Epoch: 5| Step: 3
Training loss: 1.9877229928970337
Validation loss: 2.0024024401941607

Epoch: 5| Step: 4
Training loss: 2.015915870666504
Validation loss: 1.9854520918220602

Epoch: 5| Step: 5
Training loss: 1.71175217628479
Validation loss: 1.9774830520793956

Epoch: 5| Step: 6
Training loss: 1.536934494972229
Validation loss: 1.9993557724901425

Epoch: 5| Step: 7
Training loss: 1.0801126956939697
Validation loss: 1.9930467977318713

Epoch: 5| Step: 8
Training loss: 1.6165435314178467
Validation loss: 2.002314098419682

Epoch: 5| Step: 9
Training loss: 1.524067997932434
Validation loss: 2.091228186443288

Epoch: 5| Step: 10
Training loss: 2.497718572616577
Validation loss: 2.2117724598094983

Epoch: 154| Step: 0
Training loss: 1.732740044593811
Validation loss: 2.226107992151732

Epoch: 5| Step: 1
Training loss: 1.2949621677398682
Validation loss: 2.1217891862315517

Epoch: 5| Step: 2
Training loss: 1.9232900142669678
Validation loss: 2.045440076499857

Epoch: 5| Step: 3
Training loss: 1.1378304958343506
Validation loss: 2.0089298243163736

Epoch: 5| Step: 4
Training loss: 1.5379817485809326
Validation loss: 1.9582945557050808

Epoch: 5| Step: 5
Training loss: 2.2480411529541016
Validation loss: 1.9456861403680616

Epoch: 5| Step: 6
Training loss: 2.0743675231933594
Validation loss: 1.9528719327783073

Epoch: 5| Step: 7
Training loss: 1.4322117567062378
Validation loss: 1.9577569782093007

Epoch: 5| Step: 8
Training loss: 1.919867753982544
Validation loss: 1.9708437329979354

Epoch: 5| Step: 9
Training loss: 2.218529462814331
Validation loss: 1.981376019857263

Epoch: 5| Step: 10
Training loss: 1.705276370048523
Validation loss: 2.011166040615369

Epoch: 155| Step: 0
Training loss: 1.6076749563217163
Validation loss: 2.041365126127838

Epoch: 5| Step: 1
Training loss: 1.3026138544082642
Validation loss: 2.0877519871598933

Epoch: 5| Step: 2
Training loss: 1.584135890007019
Validation loss: 2.1200875261778473

Epoch: 5| Step: 3
Training loss: 1.979697585105896
Validation loss: 2.118415671010171

Epoch: 5| Step: 4
Training loss: 1.7753570079803467
Validation loss: 2.138300031744024

Epoch: 5| Step: 5
Training loss: 2.1195547580718994
Validation loss: 2.1395976620335735

Epoch: 5| Step: 6
Training loss: 1.3645696640014648
Validation loss: 2.079892728918342

Epoch: 5| Step: 7
Training loss: 1.7756364345550537
Validation loss: 2.0735506729413102

Epoch: 5| Step: 8
Training loss: 1.422050952911377
Validation loss: 2.021803420077088

Epoch: 5| Step: 9
Training loss: 2.0045995712280273
Validation loss: 2.0089953125164075

Epoch: 5| Step: 10
Training loss: 1.5987476110458374
Validation loss: 1.9766412678585257

Epoch: 156| Step: 0
Training loss: 1.3927313089370728
Validation loss: 2.0351844731197564

Epoch: 5| Step: 1
Training loss: 2.2133946418762207
Validation loss: 2.101247674675398

Epoch: 5| Step: 2
Training loss: 1.6487592458724976
Validation loss: 2.1472658034293883

Epoch: 5| Step: 3
Training loss: 1.9813776016235352
Validation loss: 2.1844773113086657

Epoch: 5| Step: 4
Training loss: 1.7001672983169556
Validation loss: 2.185348951688377

Epoch: 5| Step: 5
Training loss: 1.4162466526031494
Validation loss: 2.071663714224292

Epoch: 5| Step: 6
Training loss: 1.848527193069458
Validation loss: 2.0312983784624326

Epoch: 5| Step: 7
Training loss: 1.1967798471450806
Validation loss: 2.015995422999064

Epoch: 5| Step: 8
Training loss: 1.601231575012207
Validation loss: 2.01839384981381

Epoch: 5| Step: 9
Training loss: 1.9742351770401
Validation loss: 1.9895319938659668

Epoch: 5| Step: 10
Training loss: 1.239980936050415
Validation loss: 1.9873305264339651

Epoch: 157| Step: 0
Training loss: 1.9133014678955078
Validation loss: 1.979263000590827

Epoch: 5| Step: 1
Training loss: 1.5291064977645874
Validation loss: 1.9929937752344276

Epoch: 5| Step: 2
Training loss: 1.7100164890289307
Validation loss: 2.039072803271714

Epoch: 5| Step: 3
Training loss: 1.4178026914596558
Validation loss: 2.0541380246480307

Epoch: 5| Step: 4
Training loss: 1.4446545839309692
Validation loss: 2.0700944290366223

Epoch: 5| Step: 5
Training loss: 2.3857288360595703
Validation loss: 2.059462898521013

Epoch: 5| Step: 6
Training loss: 1.568008303642273
Validation loss: 2.0175033512935845

Epoch: 5| Step: 7
Training loss: 1.4468567371368408
Validation loss: 1.99471406526463

Epoch: 5| Step: 8
Training loss: 2.055900812149048
Validation loss: 1.980025000469659

Epoch: 5| Step: 9
Training loss: 1.396449089050293
Validation loss: 2.0159105793122323

Epoch: 5| Step: 10
Training loss: 1.5063329935073853
Validation loss: 2.0064229785755114

Epoch: 158| Step: 0
Training loss: 1.3037049770355225
Validation loss: 2.020233077387656

Epoch: 5| Step: 1
Training loss: 1.7143869400024414
Validation loss: 2.066987940060195

Epoch: 5| Step: 2
Training loss: 1.7231553792953491
Validation loss: 2.0557582250205417

Epoch: 5| Step: 3
Training loss: 1.6096044778823853
Validation loss: 2.0641141001896193

Epoch: 5| Step: 4
Training loss: 1.6162490844726562
Validation loss: 2.0416184984227663

Epoch: 5| Step: 5
Training loss: 1.2899112701416016
Validation loss: 2.045886984435461

Epoch: 5| Step: 6
Training loss: 2.048983335494995
Validation loss: 2.0139273969076013

Epoch: 5| Step: 7
Training loss: 2.0247139930725098
Validation loss: 2.002469879324718

Epoch: 5| Step: 8
Training loss: 0.9136699438095093
Validation loss: 1.98027451063997

Epoch: 5| Step: 9
Training loss: 1.8819042444229126
Validation loss: 1.9917806361311226

Epoch: 5| Step: 10
Training loss: 2.009655714035034
Validation loss: 2.00249449412028

Epoch: 159| Step: 0
Training loss: 2.0764565467834473
Validation loss: 1.9863106563527098

Epoch: 5| Step: 1
Training loss: 1.4272273778915405
Validation loss: 2.0205605106969036

Epoch: 5| Step: 2
Training loss: 1.4794261455535889
Validation loss: 2.0280904462260585

Epoch: 5| Step: 3
Training loss: 1.1633309125900269
Validation loss: 2.062450644790485

Epoch: 5| Step: 4
Training loss: 1.6055828332901
Validation loss: 2.048224200484573

Epoch: 5| Step: 5
Training loss: 1.7597192525863647
Validation loss: 2.0374700856465164

Epoch: 5| Step: 6
Training loss: 1.6544615030288696
Validation loss: 1.9900807693440428

Epoch: 5| Step: 7
Training loss: 1.4724701642990112
Validation loss: 1.9689038979109896

Epoch: 5| Step: 8
Training loss: 2.2409300804138184
Validation loss: 1.995013020371878

Epoch: 5| Step: 9
Training loss: 1.326896071434021
Validation loss: 2.0100739284228255

Epoch: 5| Step: 10
Training loss: 1.4492828845977783
Validation loss: 2.0549644065159622

Epoch: 160| Step: 0
Training loss: 1.6354917287826538
Validation loss: 2.114703732152139

Epoch: 5| Step: 1
Training loss: 2.2226948738098145
Validation loss: 2.1508390634290633

Epoch: 5| Step: 2
Training loss: 1.7505298852920532
Validation loss: 2.0823002374300392

Epoch: 5| Step: 3
Training loss: 0.8860222101211548
Validation loss: 1.992123447438722

Epoch: 5| Step: 4
Training loss: 1.542973279953003
Validation loss: 1.969168475879136

Epoch: 5| Step: 5
Training loss: 1.5680943727493286
Validation loss: 1.9896974332870976

Epoch: 5| Step: 6
Training loss: 1.6002858877182007
Validation loss: 1.9678456244930145

Epoch: 5| Step: 7
Training loss: 1.7988481521606445
Validation loss: 1.9556080833558114

Epoch: 5| Step: 8
Training loss: 1.2775853872299194
Validation loss: 1.9673746580718665

Epoch: 5| Step: 9
Training loss: 1.858302354812622
Validation loss: 1.9512452874132382

Epoch: 5| Step: 10
Training loss: 1.8236149549484253
Validation loss: 1.964541809533232

Epoch: 161| Step: 0
Training loss: 1.8092625141143799
Validation loss: 2.014597818415652

Epoch: 5| Step: 1
Training loss: 1.537499189376831
Validation loss: 2.05473602971723

Epoch: 5| Step: 2
Training loss: 1.410840392112732
Validation loss: 2.096664390256328

Epoch: 5| Step: 3
Training loss: 1.8473647832870483
Validation loss: 2.135748983711325

Epoch: 5| Step: 4
Training loss: 2.1314682960510254
Validation loss: 2.097246157225742

Epoch: 5| Step: 5
Training loss: 1.2825038433074951
Validation loss: 2.073996102938088

Epoch: 5| Step: 6
Training loss: 1.5528013706207275
Validation loss: 2.03669479585463

Epoch: 5| Step: 7
Training loss: 1.5438244342803955
Validation loss: 2.000148929575438

Epoch: 5| Step: 8
Training loss: 1.2112773656845093
Validation loss: 2.002490933223437

Epoch: 5| Step: 9
Training loss: 1.764582633972168
Validation loss: 2.000207629255069

Epoch: 5| Step: 10
Training loss: 1.6970648765563965
Validation loss: 2.004639071802939

Epoch: 162| Step: 0
Training loss: 1.4681332111358643
Validation loss: 2.0139554815907634

Epoch: 5| Step: 1
Training loss: 1.3546327352523804
Validation loss: 2.033093633190278

Epoch: 5| Step: 2
Training loss: 1.5523968935012817
Validation loss: 2.041515942542784

Epoch: 5| Step: 3
Training loss: 1.437396764755249
Validation loss: 2.03809049565305

Epoch: 5| Step: 4
Training loss: 1.3156845569610596
Validation loss: 2.017583689381999

Epoch: 5| Step: 5
Training loss: 1.249528408050537
Validation loss: 2.0354612014626943

Epoch: 5| Step: 6
Training loss: 2.233426809310913
Validation loss: 2.012977860307181

Epoch: 5| Step: 7
Training loss: 1.841737985610962
Validation loss: 2.0128054843154004

Epoch: 5| Step: 8
Training loss: 1.222612738609314
Validation loss: 1.9876036054344588

Epoch: 5| Step: 9
Training loss: 1.6568483114242554
Validation loss: 1.9606519078695646

Epoch: 5| Step: 10
Training loss: 1.562413215637207
Validation loss: 1.9555994208141039

Epoch: 163| Step: 0
Training loss: 1.8910973072052002
Validation loss: 1.9405669371287029

Epoch: 5| Step: 1
Training loss: 1.4558377265930176
Validation loss: 1.9668818712234497

Epoch: 5| Step: 2
Training loss: 1.165778398513794
Validation loss: 1.9694277548020886

Epoch: 5| Step: 3
Training loss: 1.5372037887573242
Validation loss: 1.9594461815331572

Epoch: 5| Step: 4
Training loss: 1.360306978225708
Validation loss: 2.0285656041996454

Epoch: 5| Step: 5
Training loss: 2.0245890617370605
Validation loss: 2.0429349214799943

Epoch: 5| Step: 6
Training loss: 1.1890456676483154
Validation loss: 2.0276851256688437

Epoch: 5| Step: 7
Training loss: 1.8373031616210938
Validation loss: 2.0130445623910553

Epoch: 5| Step: 8
Training loss: 1.6086552143096924
Validation loss: 2.007344138237738

Epoch: 5| Step: 9
Training loss: 1.0642818212509155
Validation loss: 1.9976976161362023

Epoch: 5| Step: 10
Training loss: 1.8918344974517822
Validation loss: 1.9621233568396619

Epoch: 164| Step: 0
Training loss: 1.1736085414886475
Validation loss: 2.0003824362190823

Epoch: 5| Step: 1
Training loss: 1.0676876306533813
Validation loss: 2.0176612407930437

Epoch: 5| Step: 2
Training loss: 1.9136028289794922
Validation loss: 2.0738018199961674

Epoch: 5| Step: 3
Training loss: 1.3980648517608643
Validation loss: 2.056894095995093

Epoch: 5| Step: 4
Training loss: 1.134581208229065
Validation loss: 2.01321251674365

Epoch: 5| Step: 5
Training loss: 2.450972080230713
Validation loss: 1.976304510588287

Epoch: 5| Step: 6
Training loss: 2.1622159481048584
Validation loss: 1.9492439634056502

Epoch: 5| Step: 7
Training loss: 0.9003137350082397
Validation loss: 1.9172575640422043

Epoch: 5| Step: 8
Training loss: 1.3951655626296997
Validation loss: 1.924013622345463

Epoch: 5| Step: 9
Training loss: 2.0456180572509766
Validation loss: 1.9463630184050529

Epoch: 5| Step: 10
Training loss: 1.8375279903411865
Validation loss: 1.978173876321444

Epoch: 165| Step: 0
Training loss: 1.830092191696167
Validation loss: 2.0170362700698194

Epoch: 5| Step: 1
Training loss: 1.562947392463684
Validation loss: 2.0605022804711455

Epoch: 5| Step: 2
Training loss: 1.210139513015747
Validation loss: 2.070889455015941

Epoch: 5| Step: 3
Training loss: 1.5086872577667236
Validation loss: 2.0634848058864637

Epoch: 5| Step: 4
Training loss: 1.1990325450897217
Validation loss: 2.07893806247301

Epoch: 5| Step: 5
Training loss: 1.623030662536621
Validation loss: 2.0408632165642193

Epoch: 5| Step: 6
Training loss: 1.9894558191299438
Validation loss: 2.0466845061189387

Epoch: 5| Step: 7
Training loss: 1.317167043685913
Validation loss: 2.022237903328352

Epoch: 5| Step: 8
Training loss: 1.169266700744629
Validation loss: 1.9741184890911143

Epoch: 5| Step: 9
Training loss: 1.7681143283843994
Validation loss: 1.9771880411332654

Epoch: 5| Step: 10
Training loss: 1.4439479112625122
Validation loss: 1.9930708985174856

Epoch: 166| Step: 0
Training loss: 1.0763475894927979
Validation loss: 2.0045747782594416

Epoch: 5| Step: 1
Training loss: 1.2749297618865967
Validation loss: 2.028534097056235

Epoch: 5| Step: 2
Training loss: 1.6267449855804443
Validation loss: 2.0422735086051365

Epoch: 5| Step: 3
Training loss: 2.180251359939575
Validation loss: 2.045902024033249

Epoch: 5| Step: 4
Training loss: 2.008558511734009
Validation loss: 1.9767094760812738

Epoch: 5| Step: 5
Training loss: 1.192054033279419
Validation loss: 1.969322072562351

Epoch: 5| Step: 6
Training loss: 1.065561056137085
Validation loss: 1.969245764517015

Epoch: 5| Step: 7
Training loss: 1.5369192361831665
Validation loss: 1.9752476638363254

Epoch: 5| Step: 8
Training loss: 1.5709128379821777
Validation loss: 1.9833317174706409

Epoch: 5| Step: 9
Training loss: 0.9520511627197266
Validation loss: 2.06875886712023

Epoch: 5| Step: 10
Training loss: 1.9271132946014404
Validation loss: 2.192519992910406

Epoch: 167| Step: 0
Training loss: 1.9785465002059937
Validation loss: 2.188242035527383

Epoch: 5| Step: 1
Training loss: 1.290794014930725
Validation loss: 2.0438358014629734

Epoch: 5| Step: 2
Training loss: 1.4025461673736572
Validation loss: 1.9709184477406163

Epoch: 5| Step: 3
Training loss: 1.2652885913848877
Validation loss: 1.94706142076882

Epoch: 5| Step: 4
Training loss: 1.7012161016464233
Validation loss: 1.9594142770254483

Epoch: 5| Step: 5
Training loss: 1.1703075170516968
Validation loss: 1.9490575944223711

Epoch: 5| Step: 6
Training loss: 1.6134521961212158
Validation loss: 1.9367218761033909

Epoch: 5| Step: 7
Training loss: 1.5966665744781494
Validation loss: 1.9524760643641155

Epoch: 5| Step: 8
Training loss: 1.5622155666351318
Validation loss: 1.9662352274822932

Epoch: 5| Step: 9
Training loss: 2.0465924739837646
Validation loss: 1.9864544868469238

Epoch: 5| Step: 10
Training loss: 1.375229835510254
Validation loss: 2.0031464548521143

Epoch: 168| Step: 0
Training loss: 1.430838942527771
Validation loss: 2.046118337620971

Epoch: 5| Step: 1
Training loss: 1.4468448162078857
Validation loss: 2.0615272880882345

Epoch: 5| Step: 2
Training loss: 1.7553911209106445
Validation loss: 2.081282777170981

Epoch: 5| Step: 3
Training loss: 1.64870285987854
Validation loss: 2.0346250098238707

Epoch: 5| Step: 4
Training loss: 1.470323920249939
Validation loss: 2.0335398297156058

Epoch: 5| Step: 5
Training loss: 1.3316411972045898
Validation loss: 2.0301033271256315

Epoch: 5| Step: 6
Training loss: 1.169321894645691
Validation loss: 2.0482550346723167

Epoch: 5| Step: 7
Training loss: 1.6890592575073242
Validation loss: 2.028731103866331

Epoch: 5| Step: 8
Training loss: 2.3951213359832764
Validation loss: 2.004202296656947

Epoch: 5| Step: 9
Training loss: 0.8039703369140625
Validation loss: 1.9989280457137732

Epoch: 5| Step: 10
Training loss: 1.5256428718566895
Validation loss: 1.997955870884721

Epoch: 169| Step: 0
Training loss: 1.8018295764923096
Validation loss: 1.9754547226813532

Epoch: 5| Step: 1
Training loss: 1.4007318019866943
Validation loss: 1.9829506207537908

Epoch: 5| Step: 2
Training loss: 1.6840606927871704
Validation loss: 2.003540381308525

Epoch: 5| Step: 3
Training loss: 1.4081236124038696
Validation loss: 2.0210087145528486

Epoch: 5| Step: 4
Training loss: 1.6691265106201172
Validation loss: 2.0234013962489303

Epoch: 5| Step: 5
Training loss: 1.4463245868682861
Validation loss: 2.053191167052074

Epoch: 5| Step: 6
Training loss: 1.2973692417144775
Validation loss: 2.0622821392551547

Epoch: 5| Step: 7
Training loss: 1.5579580068588257
Validation loss: 2.1517891268576346

Epoch: 5| Step: 8
Training loss: 1.0152102708816528
Validation loss: 2.2050925890604653

Epoch: 5| Step: 9
Training loss: 1.575918436050415
Validation loss: 2.1890282297647126

Epoch: 5| Step: 10
Training loss: 1.8950220346450806
Validation loss: 2.027847323366391

Epoch: 170| Step: 0
Training loss: 1.4872678518295288
Validation loss: 1.9505974682428504

Epoch: 5| Step: 1
Training loss: 1.556794285774231
Validation loss: 1.908606759963497

Epoch: 5| Step: 2
Training loss: 2.4899468421936035
Validation loss: 1.9171217667159213

Epoch: 5| Step: 3
Training loss: 1.5849729776382446
Validation loss: 1.9254000891921341

Epoch: 5| Step: 4
Training loss: 1.5866340398788452
Validation loss: 1.9236086747979606

Epoch: 5| Step: 5
Training loss: 1.7595422267913818
Validation loss: 1.9333886946401289

Epoch: 5| Step: 6
Training loss: 1.3739410638809204
Validation loss: 1.9516449769337971

Epoch: 5| Step: 7
Training loss: 1.2011029720306396
Validation loss: 1.9697909790982482

Epoch: 5| Step: 8
Training loss: 1.36129629611969
Validation loss: 2.0546069709203576

Epoch: 5| Step: 9
Training loss: 1.7064244747161865
Validation loss: 2.1077968343611686

Epoch: 5| Step: 10
Training loss: 1.2608695030212402
Validation loss: 2.1743521946732716

Epoch: 171| Step: 0
Training loss: 2.0062410831451416
Validation loss: 2.190483357316704

Epoch: 5| Step: 1
Training loss: 1.6219762563705444
Validation loss: 2.1724239869784285

Epoch: 5| Step: 2
Training loss: 1.2265368700027466
Validation loss: 2.149977796821184

Epoch: 5| Step: 3
Training loss: 1.1383615732192993
Validation loss: 2.1089788021579867

Epoch: 5| Step: 4
Training loss: 1.6326286792755127
Validation loss: 2.044356507639731

Epoch: 5| Step: 5
Training loss: 1.7289035320281982
Validation loss: 2.0130233995376097

Epoch: 5| Step: 6
Training loss: 0.9213754534721375
Validation loss: 2.0049636005073466

Epoch: 5| Step: 7
Training loss: 1.2570546865463257
Validation loss: 1.993307605866463

Epoch: 5| Step: 8
Training loss: 1.4344184398651123
Validation loss: 2.020729554596768

Epoch: 5| Step: 9
Training loss: 1.349953293800354
Validation loss: 2.0368788678158998

Epoch: 5| Step: 10
Training loss: 1.9345810413360596
Validation loss: 2.046257239516063

Epoch: 172| Step: 0
Training loss: 1.4651280641555786
Validation loss: 2.040134906768799

Epoch: 5| Step: 1
Training loss: 1.0392053127288818
Validation loss: 2.049370499067409

Epoch: 5| Step: 2
Training loss: 1.0378482341766357
Validation loss: 2.027080100069764

Epoch: 5| Step: 3
Training loss: 1.7301762104034424
Validation loss: 2.0319086300429476

Epoch: 5| Step: 4
Training loss: 1.6333599090576172
Validation loss: 2.038672016512963

Epoch: 5| Step: 5
Training loss: 1.5098302364349365
Validation loss: 2.0359913162005845

Epoch: 5| Step: 6
Training loss: 2.0332329273223877
Validation loss: 2.0394447721460813

Epoch: 5| Step: 7
Training loss: 1.3742526769638062
Validation loss: 2.0244738081450104

Epoch: 5| Step: 8
Training loss: 1.226517915725708
Validation loss: 1.9694581749618694

Epoch: 5| Step: 9
Training loss: 1.3656318187713623
Validation loss: 1.985430448285995

Epoch: 5| Step: 10
Training loss: 1.0992213487625122
Validation loss: 2.0057326644979496

Epoch: 173| Step: 0
Training loss: 1.4901856184005737
Validation loss: 2.0039920486429685

Epoch: 5| Step: 1
Training loss: 0.9547300338745117
Validation loss: 1.973517103861737

Epoch: 5| Step: 2
Training loss: 1.445000410079956
Validation loss: 1.9816681428622174

Epoch: 5| Step: 3
Training loss: 1.5323565006256104
Validation loss: 2.0064767752924273

Epoch: 5| Step: 4
Training loss: 1.2695567607879639
Validation loss: 2.0149476553804133

Epoch: 5| Step: 5
Training loss: 1.5062259435653687
Validation loss: 2.018055091622055

Epoch: 5| Step: 6
Training loss: 1.7208236455917358
Validation loss: 2.0463552910794496

Epoch: 5| Step: 7
Training loss: 1.5039637088775635
Validation loss: 2.0870171464899534

Epoch: 5| Step: 8
Training loss: 1.3884203433990479
Validation loss: 2.080960032760456

Epoch: 5| Step: 9
Training loss: 1.0665611028671265
Validation loss: 2.0664150227782545

Epoch: 5| Step: 10
Training loss: 1.6939297914505005
Validation loss: 2.0724919201225362

Epoch: 174| Step: 0
Training loss: 1.2956135272979736
Validation loss: 2.058103787001743

Epoch: 5| Step: 1
Training loss: 1.2743278741836548
Validation loss: 2.042602568544367

Epoch: 5| Step: 2
Training loss: 1.2969954013824463
Validation loss: 2.000385276732906

Epoch: 5| Step: 3
Training loss: 1.1744840145111084
Validation loss: 1.968885407652906

Epoch: 5| Step: 4
Training loss: 1.888396978378296
Validation loss: 1.9508622589931692

Epoch: 5| Step: 5
Training loss: 1.4508088827133179
Validation loss: 1.9499625928940312

Epoch: 5| Step: 6
Training loss: 1.6406478881835938
Validation loss: 1.9605391512634933

Epoch: 5| Step: 7
Training loss: 1.1210440397262573
Validation loss: 1.9635184772552983

Epoch: 5| Step: 8
Training loss: 1.8591959476470947
Validation loss: 1.9877607155871648

Epoch: 5| Step: 9
Training loss: 1.528428316116333
Validation loss: 2.0390481615579255

Epoch: 5| Step: 10
Training loss: 0.7298840880393982
Validation loss: 2.1042601728952057

Epoch: 175| Step: 0
Training loss: 1.8614795207977295
Validation loss: 2.1346728647908857

Epoch: 5| Step: 1
Training loss: 1.346720814704895
Validation loss: 2.0818941798261417

Epoch: 5| Step: 2
Training loss: 1.2291052341461182
Validation loss: 2.0609268514058923

Epoch: 5| Step: 3
Training loss: 1.0024281740188599
Validation loss: 1.998672495606125

Epoch: 5| Step: 4
Training loss: 1.3220672607421875
Validation loss: 1.967802655312323

Epoch: 5| Step: 5
Training loss: 1.7809594869613647
Validation loss: 1.9829166884063392

Epoch: 5| Step: 6
Training loss: 1.0424866676330566
Validation loss: 1.9732137969745103

Epoch: 5| Step: 7
Training loss: 1.3651567697525024
Validation loss: 1.9880404997897405

Epoch: 5| Step: 8
Training loss: 1.4001338481903076
Validation loss: 1.9918004517914147

Epoch: 5| Step: 9
Training loss: 1.4166629314422607
Validation loss: 2.0077337154778103

Epoch: 5| Step: 10
Training loss: 1.4521900415420532
Validation loss: 2.005755102762612

Testing loss: 2.305069777700636

Epoch: 1| Step: 0
Training loss: 5.8139142990112305
Validation loss: 5.221063998437697

Epoch: 5| Step: 1
Training loss: 4.120880126953125
Validation loss: 5.203396294706611

Epoch: 5| Step: 2
Training loss: 4.274011135101318
Validation loss: 5.186515656850672

Epoch: 5| Step: 3
Training loss: 5.9428205490112305
Validation loss: 5.168413603177634

Epoch: 5| Step: 4
Training loss: 4.501070022583008
Validation loss: 5.147109621314592

Epoch: 5| Step: 5
Training loss: 5.983473300933838
Validation loss: 5.124007532673497

Epoch: 5| Step: 6
Training loss: 3.9495246410369873
Validation loss: 5.0981262114740185

Epoch: 5| Step: 7
Training loss: 5.257871150970459
Validation loss: 5.068909732244348

Epoch: 5| Step: 8
Training loss: 4.718064308166504
Validation loss: 5.036026093267625

Epoch: 5| Step: 9
Training loss: 4.842446327209473
Validation loss: 5.000920203424269

Epoch: 5| Step: 10
Training loss: 4.508472442626953
Validation loss: 4.96209043072116

Epoch: 2| Step: 0
Training loss: 5.348258018493652
Validation loss: 4.920355145649244

Epoch: 5| Step: 1
Training loss: 3.2602171897888184
Validation loss: 4.876852168831774

Epoch: 5| Step: 2
Training loss: 4.34873628616333
Validation loss: 4.829921948012485

Epoch: 5| Step: 3
Training loss: 4.614752292633057
Validation loss: 4.781830341585221

Epoch: 5| Step: 4
Training loss: 4.355748176574707
Validation loss: 4.731543715282153

Epoch: 5| Step: 5
Training loss: 5.253375053405762
Validation loss: 4.680669633291101

Epoch: 5| Step: 6
Training loss: 4.818282127380371
Validation loss: 4.629444363296673

Epoch: 5| Step: 7
Training loss: 4.954444885253906
Validation loss: 4.579404369477303

Epoch: 5| Step: 8
Training loss: 3.7679660320281982
Validation loss: 4.528730530892649

Epoch: 5| Step: 9
Training loss: 3.540494203567505
Validation loss: 4.476870116367135

Epoch: 5| Step: 10
Training loss: 5.003197193145752
Validation loss: 4.429021907109086

Epoch: 3| Step: 0
Training loss: 4.708922386169434
Validation loss: 4.381930925512827

Epoch: 5| Step: 1
Training loss: 3.4482314586639404
Validation loss: 4.337817007495511

Epoch: 5| Step: 2
Training loss: 4.622291088104248
Validation loss: 4.2924132757289435

Epoch: 5| Step: 3
Training loss: 5.045650482177734
Validation loss: 4.243734339232086

Epoch: 5| Step: 4
Training loss: 5.460615158081055
Validation loss: 4.191818791051065

Epoch: 5| Step: 5
Training loss: 3.087578296661377
Validation loss: 4.130154799389583

Epoch: 5| Step: 6
Training loss: 3.7439544200897217
Validation loss: 4.070517780960247

Epoch: 5| Step: 7
Training loss: 2.988827705383301
Validation loss: 4.018626300237512

Epoch: 5| Step: 8
Training loss: 2.835245370864868
Validation loss: 3.9726933894618863

Epoch: 5| Step: 9
Training loss: 4.132956504821777
Validation loss: 3.9359080099290416

Epoch: 5| Step: 10
Training loss: 3.5665972232818604
Validation loss: 3.8956267141526744

Epoch: 4| Step: 0
Training loss: 3.5148704051971436
Validation loss: 3.868928873410789

Epoch: 5| Step: 1
Training loss: 4.012935638427734
Validation loss: 3.8397786694188274

Epoch: 5| Step: 2
Training loss: 3.4931111335754395
Validation loss: 3.8116333792286534

Epoch: 5| Step: 3
Training loss: 3.1926093101501465
Validation loss: 3.782128462227442

Epoch: 5| Step: 4
Training loss: 3.7249016761779785
Validation loss: 3.7579646213080293

Epoch: 5| Step: 5
Training loss: 4.346980094909668
Validation loss: 3.730573743902227

Epoch: 5| Step: 6
Training loss: 3.49131441116333
Validation loss: 3.7036221822102866

Epoch: 5| Step: 7
Training loss: 3.7269434928894043
Validation loss: 3.677211679438109

Epoch: 5| Step: 8
Training loss: 3.2184154987335205
Validation loss: 3.6559998963468816

Epoch: 5| Step: 9
Training loss: 2.6981449127197266
Validation loss: 3.637144442527525

Epoch: 5| Step: 10
Training loss: 4.689603805541992
Validation loss: 3.623540632186397

Epoch: 5| Step: 0
Training loss: 3.511960506439209
Validation loss: 3.609668183070357

Epoch: 5| Step: 1
Training loss: 2.874624013900757
Validation loss: 3.600522179757395

Epoch: 5| Step: 2
Training loss: 4.163722991943359
Validation loss: 3.584609454677951

Epoch: 5| Step: 3
Training loss: 3.519406795501709
Validation loss: 3.573316820206181

Epoch: 5| Step: 4
Training loss: 3.555448055267334
Validation loss: 3.5671735886604554

Epoch: 5| Step: 5
Training loss: 4.268919467926025
Validation loss: 3.5571971760001233

Epoch: 5| Step: 6
Training loss: 3.9803004264831543
Validation loss: 3.5490235641438472

Epoch: 5| Step: 7
Training loss: 3.855428695678711
Validation loss: 3.53544718475752

Epoch: 5| Step: 8
Training loss: 3.1478402614593506
Validation loss: 3.5262075393430647

Epoch: 5| Step: 9
Training loss: 2.979546070098877
Validation loss: 3.517357487832346

Epoch: 5| Step: 10
Training loss: 2.3026416301727295
Validation loss: 3.510939536556121

Epoch: 6| Step: 0
Training loss: 4.080327033996582
Validation loss: 3.5072696798591205

Epoch: 5| Step: 1
Training loss: 4.196303844451904
Validation loss: 3.499778591176515

Epoch: 5| Step: 2
Training loss: 2.1007485389709473
Validation loss: 3.489864149401265

Epoch: 5| Step: 3
Training loss: 2.837486743927002
Validation loss: 3.480467504070651

Epoch: 5| Step: 4
Training loss: 2.3955581188201904
Validation loss: 3.4731323103750906

Epoch: 5| Step: 5
Training loss: 4.668920993804932
Validation loss: 3.4682292348595074

Epoch: 5| Step: 6
Training loss: 4.36175012588501
Validation loss: 3.462715438617173

Epoch: 5| Step: 7
Training loss: 3.846393585205078
Validation loss: 3.453420580074351

Epoch: 5| Step: 8
Training loss: 2.9654219150543213
Validation loss: 3.4451034248516126

Epoch: 5| Step: 9
Training loss: 3.1051673889160156
Validation loss: 3.4354606546381468

Epoch: 5| Step: 10
Training loss: 2.8667190074920654
Validation loss: 3.4266469196606706

Epoch: 7| Step: 0
Training loss: 3.872812271118164
Validation loss: 3.419054933773574

Epoch: 5| Step: 1
Training loss: 4.186051368713379
Validation loss: 3.4126303529226654

Epoch: 5| Step: 2
Training loss: 3.103700637817383
Validation loss: 3.401960639543431

Epoch: 5| Step: 3
Training loss: 3.0376522541046143
Validation loss: 3.390538351510161

Epoch: 5| Step: 4
Training loss: 2.9887938499450684
Validation loss: 3.3796532820629817

Epoch: 5| Step: 5
Training loss: 3.258225917816162
Validation loss: 3.3691195775103826

Epoch: 5| Step: 6
Training loss: 3.1449227333068848
Validation loss: 3.362327383410546

Epoch: 5| Step: 7
Training loss: 3.2148051261901855
Validation loss: 3.3545730370347218

Epoch: 5| Step: 8
Training loss: 2.6710457801818848
Validation loss: 3.3628756974333074

Epoch: 5| Step: 9
Training loss: 2.9772486686706543
Validation loss: 3.3413928452358452

Epoch: 5| Step: 10
Training loss: 4.38565731048584
Validation loss: 3.3388819386882167

Epoch: 8| Step: 0
Training loss: 3.75903058052063
Validation loss: 3.3396628364439933

Epoch: 5| Step: 1
Training loss: 3.184424877166748
Validation loss: 3.339792077259351

Epoch: 5| Step: 2
Training loss: 2.8882534503936768
Validation loss: 3.355131026237242

Epoch: 5| Step: 3
Training loss: 3.675459384918213
Validation loss: 3.3325021292573664

Epoch: 5| Step: 4
Training loss: 2.5618467330932617
Validation loss: 3.3255782435017247

Epoch: 5| Step: 5
Training loss: 2.8274643421173096
Validation loss: 3.322854741927116

Epoch: 5| Step: 6
Training loss: 2.4423775672912598
Validation loss: 3.319030028517528

Epoch: 5| Step: 7
Training loss: 3.7462494373321533
Validation loss: 3.316254682438348

Epoch: 5| Step: 8
Training loss: 3.9346306324005127
Validation loss: 3.3095494137015393

Epoch: 5| Step: 9
Training loss: 3.465773344039917
Validation loss: 3.3032851372995684

Epoch: 5| Step: 10
Training loss: 3.759437084197998
Validation loss: 3.295559649826378

Epoch: 9| Step: 0
Training loss: 3.4600143432617188
Validation loss: 3.289193571254771

Epoch: 5| Step: 1
Training loss: 3.86186146736145
Validation loss: 3.280429752924109

Epoch: 5| Step: 2
Training loss: 3.3600242137908936
Validation loss: 3.275558740861954

Epoch: 5| Step: 3
Training loss: 3.1245462894439697
Validation loss: 3.2693260433853313

Epoch: 5| Step: 4
Training loss: 4.028589248657227
Validation loss: 3.264376360882995

Epoch: 5| Step: 5
Training loss: 3.2385029792785645
Validation loss: 3.258322254303963

Epoch: 5| Step: 6
Training loss: 3.4376606941223145
Validation loss: 3.2516783719421714

Epoch: 5| Step: 7
Training loss: 3.1901392936706543
Validation loss: 3.2442411145856305

Epoch: 5| Step: 8
Training loss: 2.8382632732391357
Validation loss: 3.2398958513813634

Epoch: 5| Step: 9
Training loss: 2.842909336090088
Validation loss: 3.233528034661406

Epoch: 5| Step: 10
Training loss: 2.14371395111084
Validation loss: 3.224402727619294

Epoch: 10| Step: 0
Training loss: 2.8313040733337402
Validation loss: 3.2217890370276665

Epoch: 5| Step: 1
Training loss: 2.8637471199035645
Validation loss: 3.2156363302661526

Epoch: 5| Step: 2
Training loss: 3.774702548980713
Validation loss: 3.216052829578359

Epoch: 5| Step: 3
Training loss: 3.244260311126709
Validation loss: 3.2027207574536725

Epoch: 5| Step: 4
Training loss: 3.055236339569092
Validation loss: 3.1975865569165958

Epoch: 5| Step: 5
Training loss: 2.6794028282165527
Validation loss: 3.195281526093842

Epoch: 5| Step: 6
Training loss: 3.0819625854492188
Validation loss: 3.192694253818963

Epoch: 5| Step: 7
Training loss: 3.451448440551758
Validation loss: 3.1882387540673696

Epoch: 5| Step: 8
Training loss: 3.6097030639648438
Validation loss: 3.190677155730545

Epoch: 5| Step: 9
Training loss: 3.5141208171844482
Validation loss: 3.1724235524413404

Epoch: 5| Step: 10
Training loss: 3.0470681190490723
Validation loss: 3.1676859240378104

Epoch: 11| Step: 0
Training loss: 3.815905809402466
Validation loss: 3.164952285828129

Epoch: 5| Step: 1
Training loss: 2.789036989212036
Validation loss: 3.151406354801629

Epoch: 5| Step: 2
Training loss: 3.5731441974639893
Validation loss: 3.148174465343516

Epoch: 5| Step: 3
Training loss: 3.3047447204589844
Validation loss: 3.14216544038506

Epoch: 5| Step: 4
Training loss: 3.314830780029297
Validation loss: 3.141675779896398

Epoch: 5| Step: 5
Training loss: 2.628863573074341
Validation loss: 3.1376766620143766

Epoch: 5| Step: 6
Training loss: 2.963212013244629
Validation loss: 3.1333227080683552

Epoch: 5| Step: 7
Training loss: 3.11322283744812
Validation loss: 3.1300994785883094

Epoch: 5| Step: 8
Training loss: 2.8790836334228516
Validation loss: 3.1248042660374797

Epoch: 5| Step: 9
Training loss: 3.3022067546844482
Validation loss: 3.120201331312938

Epoch: 5| Step: 10
Training loss: 3.0278866291046143
Validation loss: 3.1183773958554832

Epoch: 12| Step: 0
Training loss: 3.0363306999206543
Validation loss: 3.1232628104507283

Epoch: 5| Step: 1
Training loss: 2.7361361980438232
Validation loss: 3.1262007733826995

Epoch: 5| Step: 2
Training loss: 2.925126314163208
Validation loss: 3.130723138009348

Epoch: 5| Step: 3
Training loss: 3.04109525680542
Validation loss: 3.13716213164791

Epoch: 5| Step: 4
Training loss: 2.6406168937683105
Validation loss: 3.094609950178413

Epoch: 5| Step: 5
Training loss: 3.7969906330108643
Validation loss: 3.089517706183977

Epoch: 5| Step: 6
Training loss: 3.2737488746643066
Validation loss: 3.097470129689863

Epoch: 5| Step: 7
Training loss: 3.321183681488037
Validation loss: 3.10339262664959

Epoch: 5| Step: 8
Training loss: 2.934340715408325
Validation loss: 3.0774950135138726

Epoch: 5| Step: 9
Training loss: 3.0742130279541016
Validation loss: 3.0738046451281478

Epoch: 5| Step: 10
Training loss: 3.830495834350586
Validation loss: 3.0977408988501436

Epoch: 13| Step: 0
Training loss: 2.980358839035034
Validation loss: 3.1242303540629726

Epoch: 5| Step: 1
Training loss: 4.480813503265381
Validation loss: 3.1513748374036563

Epoch: 5| Step: 2
Training loss: 2.5172414779663086
Validation loss: 3.123583539839714

Epoch: 5| Step: 3
Training loss: 3.075920343399048
Validation loss: 3.1173815265778573

Epoch: 5| Step: 4
Training loss: 3.18748140335083
Validation loss: 3.1180848101133942

Epoch: 5| Step: 5
Training loss: 3.097468852996826
Validation loss: 3.1178607402309293

Epoch: 5| Step: 6
Training loss: 2.703810214996338
Validation loss: 3.1100921733405

Epoch: 5| Step: 7
Training loss: 2.9612584114074707
Validation loss: 3.0973584267400924

Epoch: 5| Step: 8
Training loss: 3.2248375415802
Validation loss: 3.090498588418448

Epoch: 5| Step: 9
Training loss: 3.1467292308807373
Validation loss: 3.0869301929268786

Epoch: 5| Step: 10
Training loss: 3.106984853744507
Validation loss: 3.0823654128659155

Epoch: 14| Step: 0
Training loss: 2.9641222953796387
Validation loss: 3.0782056905890025

Epoch: 5| Step: 1
Training loss: 2.812730312347412
Validation loss: 3.07368363359923

Epoch: 5| Step: 2
Training loss: 3.5854480266571045
Validation loss: 3.071841275820168

Epoch: 5| Step: 3
Training loss: 2.993560314178467
Validation loss: 3.06859435830065

Epoch: 5| Step: 4
Training loss: 3.483398914337158
Validation loss: 3.0641782463237806

Epoch: 5| Step: 5
Training loss: 3.18194842338562
Validation loss: 3.0838028205338346

Epoch: 5| Step: 6
Training loss: 2.567105293273926
Validation loss: 3.0707155965989634

Epoch: 5| Step: 7
Training loss: 2.985344409942627
Validation loss: 3.038431162475258

Epoch: 5| Step: 8
Training loss: 2.594916582107544
Validation loss: 3.0091540736536824

Epoch: 5| Step: 9
Training loss: 3.9994664192199707
Validation loss: 3.0207171773397796

Epoch: 5| Step: 10
Training loss: 2.8981404304504395
Validation loss: 3.05759790379514

Epoch: 15| Step: 0
Training loss: 2.6263272762298584
Validation loss: 3.08575076441611

Epoch: 5| Step: 1
Training loss: 2.0592119693756104
Validation loss: 3.1059632685876664

Epoch: 5| Step: 2
Training loss: 2.3499104976654053
Validation loss: 3.0966538921479256

Epoch: 5| Step: 3
Training loss: 3.9440741539001465
Validation loss: 3.074145617023591

Epoch: 5| Step: 4
Training loss: 2.7172012329101562
Validation loss: 3.0622864589896253

Epoch: 5| Step: 5
Training loss: 3.391469955444336
Validation loss: 3.0617766405946467

Epoch: 5| Step: 6
Training loss: 3.01267409324646
Validation loss: 3.063445762921405

Epoch: 5| Step: 7
Training loss: 4.469082832336426
Validation loss: 3.065982121293263

Epoch: 5| Step: 8
Training loss: 3.0523438453674316
Validation loss: 3.0550318533374416

Epoch: 5| Step: 9
Training loss: 3.947575330734253
Validation loss: 3.0384693402116016

Epoch: 5| Step: 10
Training loss: 2.5723061561584473
Validation loss: 3.0249571800231934

Epoch: 16| Step: 0
Training loss: 3.023603916168213
Validation loss: 3.0194291837753786

Epoch: 5| Step: 1
Training loss: 3.4559905529022217
Validation loss: 3.0144938345878356

Epoch: 5| Step: 2
Training loss: 2.465212106704712
Validation loss: 3.0118252795229674

Epoch: 5| Step: 3
Training loss: 3.5942459106445312
Validation loss: 3.0068277671772945

Epoch: 5| Step: 4
Training loss: 2.751406192779541
Validation loss: 2.9989405267982074

Epoch: 5| Step: 5
Training loss: 2.7571046352386475
Validation loss: 2.9943054594019407

Epoch: 5| Step: 6
Training loss: 3.408889055252075
Validation loss: 2.9916102963109172

Epoch: 5| Step: 7
Training loss: 2.893491268157959
Validation loss: 2.9898748526009182

Epoch: 5| Step: 8
Training loss: 4.344792366027832
Validation loss: 2.989540192388719

Epoch: 5| Step: 9
Training loss: 2.7408008575439453
Validation loss: 2.9909770540011826

Epoch: 5| Step: 10
Training loss: 2.1673665046691895
Validation loss: 2.984898723581786

Epoch: 17| Step: 0
Training loss: 3.614091396331787
Validation loss: 2.985209088171682

Epoch: 5| Step: 1
Training loss: 3.054912805557251
Validation loss: 2.984171816097793

Epoch: 5| Step: 2
Training loss: 2.5848588943481445
Validation loss: 2.9839536348978677

Epoch: 5| Step: 3
Training loss: 3.060082197189331
Validation loss: 2.9792425555567585

Epoch: 5| Step: 4
Training loss: 3.2813591957092285
Validation loss: 2.9768662503970567

Epoch: 5| Step: 5
Training loss: 3.99040150642395
Validation loss: 2.9795537815299085

Epoch: 5| Step: 6
Training loss: 2.736743211746216
Validation loss: 2.9824508338846187

Epoch: 5| Step: 7
Training loss: 3.1894049644470215
Validation loss: 2.992460027817757

Epoch: 5| Step: 8
Training loss: 2.667908191680908
Validation loss: 2.972728672847953

Epoch: 5| Step: 9
Training loss: 2.505711317062378
Validation loss: 2.9694710188014533

Epoch: 5| Step: 10
Training loss: 2.865638494491577
Validation loss: 2.9695016414888444

Epoch: 18| Step: 0
Training loss: 2.9349799156188965
Validation loss: 2.9699196712945097

Epoch: 5| Step: 1
Training loss: 2.8962621688842773
Validation loss: 2.9680962998379945

Epoch: 5| Step: 2
Training loss: 2.8028392791748047
Validation loss: 2.965750430219917

Epoch: 5| Step: 3
Training loss: 2.8677361011505127
Validation loss: 2.961537504708895

Epoch: 5| Step: 4
Training loss: 2.465827465057373
Validation loss: 2.960618431850146

Epoch: 5| Step: 5
Training loss: 3.657949924468994
Validation loss: 2.9576293114692933

Epoch: 5| Step: 6
Training loss: 3.5965607166290283
Validation loss: 2.95721556550713

Epoch: 5| Step: 7
Training loss: 2.9160823822021484
Validation loss: 2.95490934002784

Epoch: 5| Step: 8
Training loss: 2.940929889678955
Validation loss: 2.9540149088828795

Epoch: 5| Step: 9
Training loss: 3.4895527362823486
Validation loss: 2.9521151665718324

Epoch: 5| Step: 10
Training loss: 2.8626914024353027
Validation loss: 2.9502867293614212

Epoch: 19| Step: 0
Training loss: 2.856963634490967
Validation loss: 2.9493904447042816

Epoch: 5| Step: 1
Training loss: 3.9005463123321533
Validation loss: 2.9488063114945606

Epoch: 5| Step: 2
Training loss: 3.489164352416992
Validation loss: 2.947345959242954

Epoch: 5| Step: 3
Training loss: 3.6360440254211426
Validation loss: 2.945541130599155

Epoch: 5| Step: 4
Training loss: 2.3842074871063232
Validation loss: 2.94425545712953

Epoch: 5| Step: 5
Training loss: 2.877054452896118
Validation loss: 2.942762026222803

Epoch: 5| Step: 6
Training loss: 2.5726120471954346
Validation loss: 2.940601364258797

Epoch: 5| Step: 7
Training loss: 2.9048562049865723
Validation loss: 2.9386101076679845

Epoch: 5| Step: 8
Training loss: 2.502089023590088
Validation loss: 2.936177945906116

Epoch: 5| Step: 9
Training loss: 2.9031014442443848
Validation loss: 2.935689623637866

Epoch: 5| Step: 10
Training loss: 3.3554277420043945
Validation loss: 2.9328016465710056

Epoch: 20| Step: 0
Training loss: 3.1001462936401367
Validation loss: 2.931450364410236

Epoch: 5| Step: 1
Training loss: 3.21980357170105
Validation loss: 2.930215599716351

Epoch: 5| Step: 2
Training loss: 2.7355995178222656
Validation loss: 2.9293344456662416

Epoch: 5| Step: 3
Training loss: 3.355384111404419
Validation loss: 2.9283583036033054

Epoch: 5| Step: 4
Training loss: 3.26212739944458
Validation loss: 2.9273537307657223

Epoch: 5| Step: 5
Training loss: 3.6885573863983154
Validation loss: 2.9261527881827405

Epoch: 5| Step: 6
Training loss: 2.8950746059417725
Validation loss: 2.92531418800354

Epoch: 5| Step: 7
Training loss: 2.8470096588134766
Validation loss: 2.9233488754559587

Epoch: 5| Step: 8
Training loss: 2.92820143699646
Validation loss: 2.921807537796677

Epoch: 5| Step: 9
Training loss: 2.37575101852417
Validation loss: 2.9209069154595815

Epoch: 5| Step: 10
Training loss: 2.7745609283447266
Validation loss: 2.9203091180452736

Epoch: 21| Step: 0
Training loss: 3.1578011512756348
Validation loss: 2.919113051506781

Epoch: 5| Step: 1
Training loss: 3.221349000930786
Validation loss: 2.9187562696395384

Epoch: 5| Step: 2
Training loss: 3.5218093395233154
Validation loss: 2.9172525585338636

Epoch: 5| Step: 3
Training loss: 2.520444393157959
Validation loss: 2.9163281327934674

Epoch: 5| Step: 4
Training loss: 3.324036121368408
Validation loss: 2.9152872741863294

Epoch: 5| Step: 5
Training loss: 2.873805284500122
Validation loss: 2.9141124320286576

Epoch: 5| Step: 6
Training loss: 3.137101411819458
Validation loss: 2.9130933694942023

Epoch: 5| Step: 7
Training loss: 2.8336892127990723
Validation loss: 2.9130395099680912

Epoch: 5| Step: 8
Training loss: 3.291220188140869
Validation loss: 2.9113527472301195

Epoch: 5| Step: 9
Training loss: 2.562051296234131
Validation loss: 2.9102447212383313

Epoch: 5| Step: 10
Training loss: 2.629770040512085
Validation loss: 2.909403398472776

Epoch: 22| Step: 0
Training loss: 2.7134246826171875
Validation loss: 2.909228955545733

Epoch: 5| Step: 1
Training loss: 3.113152503967285
Validation loss: 2.9072301977424213

Epoch: 5| Step: 2
Training loss: 3.435476779937744
Validation loss: 2.906383045258061

Epoch: 5| Step: 3
Training loss: 2.576348066329956
Validation loss: 2.90614761588394

Epoch: 5| Step: 4
Training loss: 2.7978713512420654
Validation loss: 2.904731547960671

Epoch: 5| Step: 5
Training loss: 3.551335573196411
Validation loss: 2.9037612638165875

Epoch: 5| Step: 6
Training loss: 2.4814865589141846
Validation loss: 2.903349766167261

Epoch: 5| Step: 7
Training loss: 2.6114373207092285
Validation loss: 2.90211231734163

Epoch: 5| Step: 8
Training loss: 3.725177049636841
Validation loss: 2.9013482063047347

Epoch: 5| Step: 9
Training loss: 2.8498897552490234
Validation loss: 2.900541813142838

Epoch: 5| Step: 10
Training loss: 3.2358591556549072
Validation loss: 2.8990922768910727

Epoch: 23| Step: 0
Training loss: 2.160020589828491
Validation loss: 2.898433087974466

Epoch: 5| Step: 1
Training loss: 3.5011322498321533
Validation loss: 2.8974156482245332

Epoch: 5| Step: 2
Training loss: 3.5910232067108154
Validation loss: 2.89682327547381

Epoch: 5| Step: 3
Training loss: 2.863306999206543
Validation loss: 2.895842006129603

Epoch: 5| Step: 4
Training loss: 3.1990504264831543
Validation loss: 2.894889639269921

Epoch: 5| Step: 5
Training loss: 2.7773497104644775
Validation loss: 2.8940504468897337

Epoch: 5| Step: 6
Training loss: 2.744811773300171
Validation loss: 2.892889727828323

Epoch: 5| Step: 7
Training loss: 3.6613762378692627
Validation loss: 2.8923236759760047

Epoch: 5| Step: 8
Training loss: 2.670602321624756
Validation loss: 2.891098609534643

Epoch: 5| Step: 9
Training loss: 3.3550972938537598
Validation loss: 2.8902387619018555

Epoch: 5| Step: 10
Training loss: 2.3585569858551025
Validation loss: 2.8888576517822924

Epoch: 24| Step: 0
Training loss: 2.983643054962158
Validation loss: 2.888351276356687

Epoch: 5| Step: 1
Training loss: 3.2082111835479736
Validation loss: 2.8872229950402373

Epoch: 5| Step: 2
Training loss: 2.986937999725342
Validation loss: 2.8867372851217947

Epoch: 5| Step: 3
Training loss: 2.2946434020996094
Validation loss: 2.8855806627581195

Epoch: 5| Step: 4
Training loss: 3.5187981128692627
Validation loss: 2.8845986832854567

Epoch: 5| Step: 5
Training loss: 2.7732880115509033
Validation loss: 2.8840354155468684

Epoch: 5| Step: 6
Training loss: 3.0472943782806396
Validation loss: 2.882937164716823

Epoch: 5| Step: 7
Training loss: 3.362506151199341
Validation loss: 2.881575858721169

Epoch: 5| Step: 8
Training loss: 2.80664324760437
Validation loss: 2.8811363789343063

Epoch: 5| Step: 9
Training loss: 2.789949655532837
Validation loss: 2.8795872042256017

Epoch: 5| Step: 10
Training loss: 3.1546905040740967
Validation loss: 2.8787489603924494

Epoch: 25| Step: 0
Training loss: 2.905625820159912
Validation loss: 2.8781223604756017

Epoch: 5| Step: 1
Training loss: 3.2976059913635254
Validation loss: 2.876971826758436

Epoch: 5| Step: 2
Training loss: 2.6730570793151855
Validation loss: 2.875853212930823

Epoch: 5| Step: 3
Training loss: 2.9238104820251465
Validation loss: 2.874654136678224

Epoch: 5| Step: 4
Training loss: 2.7324557304382324
Validation loss: 2.8735070484940723

Epoch: 5| Step: 5
Training loss: 3.3963077068328857
Validation loss: 2.872510133251067

Epoch: 5| Step: 6
Training loss: 3.464257001876831
Validation loss: 2.8713161458251295

Epoch: 5| Step: 7
Training loss: 3.3177733421325684
Validation loss: 2.8702856648352837

Epoch: 5| Step: 8
Training loss: 3.2199578285217285
Validation loss: 2.86931187106717

Epoch: 5| Step: 9
Training loss: 2.335218906402588
Validation loss: 2.868306031791113

Epoch: 5| Step: 10
Training loss: 2.486727476119995
Validation loss: 2.8677304431956303

Epoch: 26| Step: 0
Training loss: 3.1564910411834717
Validation loss: 2.8688841430089806

Epoch: 5| Step: 1
Training loss: 3.696986436843872
Validation loss: 2.863535188859509

Epoch: 5| Step: 2
Training loss: 3.582599639892578
Validation loss: 2.8615144529650287

Epoch: 5| Step: 3
Training loss: 3.7054734230041504
Validation loss: 2.8594612947074314

Epoch: 5| Step: 4
Training loss: 3.04852032661438
Validation loss: 2.856602120143111

Epoch: 5| Step: 5
Training loss: 2.9595656394958496
Validation loss: 2.8529185274595856

Epoch: 5| Step: 6
Training loss: 2.6808512210845947
Validation loss: 2.8490089831813687

Epoch: 5| Step: 7
Training loss: 2.550537347793579
Validation loss: 2.8421080394457747

Epoch: 5| Step: 8
Training loss: 2.7389261722564697
Validation loss: 2.8333364250839397

Epoch: 5| Step: 9
Training loss: 2.263888120651245
Validation loss: 2.835104975649106

Epoch: 5| Step: 10
Training loss: 2.1668500900268555
Validation loss: 2.832830398313461

Epoch: 27| Step: 0
Training loss: 2.817889451980591
Validation loss: 2.830853413510066

Epoch: 5| Step: 1
Training loss: 1.6036880016326904
Validation loss: 2.8303797219389226

Epoch: 5| Step: 2
Training loss: 2.9962146282196045
Validation loss: 2.829861097438361

Epoch: 5| Step: 3
Training loss: 2.7935328483581543
Validation loss: 2.8269662267418316

Epoch: 5| Step: 4
Training loss: 3.632030487060547
Validation loss: 2.8251629696097424

Epoch: 5| Step: 5
Training loss: 3.5858891010284424
Validation loss: 2.8243120998464604

Epoch: 5| Step: 6
Training loss: 3.417182207107544
Validation loss: 2.8240062780277704

Epoch: 5| Step: 7
Training loss: 3.2816720008850098
Validation loss: 2.822452396474859

Epoch: 5| Step: 8
Training loss: 2.989130735397339
Validation loss: 2.8217034186086347

Epoch: 5| Step: 9
Training loss: 2.751451015472412
Validation loss: 2.820815752911311

Epoch: 5| Step: 10
Training loss: 2.536938428878784
Validation loss: 2.8195931834559285

Epoch: 28| Step: 0
Training loss: 3.3231608867645264
Validation loss: 2.8187817655583864

Epoch: 5| Step: 1
Training loss: 3.1179256439208984
Validation loss: 2.8171817846195673

Epoch: 5| Step: 2
Training loss: 3.1975302696228027
Validation loss: 2.816471169071813

Epoch: 5| Step: 3
Training loss: 2.156334400177002
Validation loss: 2.815144790116177

Epoch: 5| Step: 4
Training loss: 3.4848830699920654
Validation loss: 2.8141514255154516

Epoch: 5| Step: 5
Training loss: 2.7910423278808594
Validation loss: 2.8138601600482898

Epoch: 5| Step: 6
Training loss: 2.680476427078247
Validation loss: 2.8415562747627177

Epoch: 5| Step: 7
Training loss: 2.7793490886688232
Validation loss: 2.8115129317006757

Epoch: 5| Step: 8
Training loss: 2.8624749183654785
Validation loss: 2.8104666791936403

Epoch: 5| Step: 9
Training loss: 3.2260985374450684
Validation loss: 2.8105340926877913

Epoch: 5| Step: 10
Training loss: 2.765007495880127
Validation loss: 2.8101306358973184

Epoch: 29| Step: 0
Training loss: 2.923671245574951
Validation loss: 2.8118736590108564

Epoch: 5| Step: 1
Training loss: 3.222590684890747
Validation loss: 2.814356198874853

Epoch: 5| Step: 2
Training loss: 3.563429355621338
Validation loss: 2.823847237453666

Epoch: 5| Step: 3
Training loss: 2.603687286376953
Validation loss: 2.8178920617667575

Epoch: 5| Step: 4
Training loss: 2.4017527103424072
Validation loss: 2.8074632306252756

Epoch: 5| Step: 5
Training loss: 3.331347703933716
Validation loss: 2.8090409463451755

Epoch: 5| Step: 6
Training loss: 2.8251559734344482
Validation loss: 2.8141471852538404

Epoch: 5| Step: 7
Training loss: 3.3395373821258545
Validation loss: 2.8104730806043072

Epoch: 5| Step: 8
Training loss: 2.404510498046875
Validation loss: 2.8103181700552664

Epoch: 5| Step: 9
Training loss: 2.937623977661133
Validation loss: 2.8053686746986966

Epoch: 5| Step: 10
Training loss: 2.825533628463745
Validation loss: 2.8042168822339786

Epoch: 30| Step: 0
Training loss: 2.789985179901123
Validation loss: 2.802810489490468

Epoch: 5| Step: 1
Training loss: 3.235543727874756
Validation loss: 2.8021946645552114

Epoch: 5| Step: 2
Training loss: 2.6870574951171875
Validation loss: 2.8007492839649157

Epoch: 5| Step: 3
Training loss: 2.8560802936553955
Validation loss: 2.7996929512229016

Epoch: 5| Step: 4
Training loss: 3.454847812652588
Validation loss: 2.7986818436653382

Epoch: 5| Step: 5
Training loss: 2.648179531097412
Validation loss: 2.797784149005849

Epoch: 5| Step: 6
Training loss: 3.1441121101379395
Validation loss: 2.7972517833914807

Epoch: 5| Step: 7
Training loss: 2.824885606765747
Validation loss: 2.795839286619617

Epoch: 5| Step: 8
Training loss: 3.380324125289917
Validation loss: 2.794685168932843

Epoch: 5| Step: 9
Training loss: 2.421325445175171
Validation loss: 2.79423870835253

Epoch: 5| Step: 10
Training loss: 2.817580461502075
Validation loss: 2.793820624710411

Epoch: 31| Step: 0
Training loss: 3.188140392303467
Validation loss: 2.7930976806148404

Epoch: 5| Step: 1
Training loss: 2.8537380695343018
Validation loss: 2.7922845809690413

Epoch: 5| Step: 2
Training loss: 3.4821395874023438
Validation loss: 2.791567458901354

Epoch: 5| Step: 3
Training loss: 2.6102852821350098
Validation loss: 2.7912315296870407

Epoch: 5| Step: 4
Training loss: 2.4057202339172363
Validation loss: 2.7903994821733042

Epoch: 5| Step: 5
Training loss: 3.0280113220214844
Validation loss: 2.7899297514269428

Epoch: 5| Step: 6
Training loss: 2.640371799468994
Validation loss: 2.7893311413385535

Epoch: 5| Step: 7
Training loss: 2.8996968269348145
Validation loss: 2.7888666788736978

Epoch: 5| Step: 8
Training loss: 2.859738826751709
Validation loss: 2.7878201033479426

Epoch: 5| Step: 9
Training loss: 2.868320941925049
Validation loss: 2.7875292403723604

Epoch: 5| Step: 10
Training loss: 3.4622416496276855
Validation loss: 2.7872997637717956

Epoch: 32| Step: 0
Training loss: 2.561617612838745
Validation loss: 2.786383846754669

Epoch: 5| Step: 1
Training loss: 2.7527687549591064
Validation loss: 2.786954733633226

Epoch: 5| Step: 2
Training loss: 3.100032329559326
Validation loss: 2.7861554186831237

Epoch: 5| Step: 3
Training loss: 2.2307217121124268
Validation loss: 2.7861897919767644

Epoch: 5| Step: 4
Training loss: 3.4530701637268066
Validation loss: 2.787319080803984

Epoch: 5| Step: 5
Training loss: 3.0684399604797363
Validation loss: 2.785772758145486

Epoch: 5| Step: 6
Training loss: 3.174609661102295
Validation loss: 2.785150476681289

Epoch: 5| Step: 7
Training loss: 2.735316753387451
Validation loss: 2.7834956005055416

Epoch: 5| Step: 8
Training loss: 2.5087432861328125
Validation loss: 2.7828811291725404

Epoch: 5| Step: 9
Training loss: 3.2899696826934814
Validation loss: 2.781041463216146

Epoch: 5| Step: 10
Training loss: 3.3745291233062744
Validation loss: 2.7803840534661406

Epoch: 33| Step: 0
Training loss: 2.9281113147735596
Validation loss: 2.779840084814256

Epoch: 5| Step: 1
Training loss: 3.0852763652801514
Validation loss: 2.7793941907985236

Epoch: 5| Step: 2
Training loss: 2.7266318798065186
Validation loss: 2.7787747126753612

Epoch: 5| Step: 3
Training loss: 2.4455313682556152
Validation loss: 2.7782069739475044

Epoch: 5| Step: 4
Training loss: 3.270477294921875
Validation loss: 2.777586862605105

Epoch: 5| Step: 5
Training loss: 3.082019090652466
Validation loss: 2.7775657407699095

Epoch: 5| Step: 6
Training loss: 3.217743396759033
Validation loss: 2.776743232562978

Epoch: 5| Step: 7
Training loss: 2.708993673324585
Validation loss: 2.7762411537990777

Epoch: 5| Step: 8
Training loss: 3.085087299346924
Validation loss: 2.7746662324474705

Epoch: 5| Step: 9
Training loss: 2.609480619430542
Validation loss: 2.7748103526330765

Epoch: 5| Step: 10
Training loss: 2.9757003784179688
Validation loss: 2.7744500431963193

Epoch: 34| Step: 0
Training loss: 3.079662561416626
Validation loss: 2.773302391011228

Epoch: 5| Step: 1
Training loss: 2.5290615558624268
Validation loss: 2.773281733194987

Epoch: 5| Step: 2
Training loss: 3.7215194702148438
Validation loss: 2.771531512660365

Epoch: 5| Step: 3
Training loss: 2.4539482593536377
Validation loss: 2.7702943483988443

Epoch: 5| Step: 4
Training loss: 2.7048678398132324
Validation loss: 2.770872780071792

Epoch: 5| Step: 5
Training loss: 3.2064075469970703
Validation loss: 2.770240640127531

Epoch: 5| Step: 6
Training loss: 3.177490234375
Validation loss: 2.7706761924169396

Epoch: 5| Step: 7
Training loss: 3.0686681270599365
Validation loss: 2.7696440527516026

Epoch: 5| Step: 8
Training loss: 2.862471580505371
Validation loss: 2.7690160633415304

Epoch: 5| Step: 9
Training loss: 2.679694652557373
Validation loss: 2.76714672324478

Epoch: 5| Step: 10
Training loss: 2.5351998805999756
Validation loss: 2.766966732599402

Epoch: 35| Step: 0
Training loss: 2.861553907394409
Validation loss: 2.7671005700224187

Epoch: 5| Step: 1
Training loss: 2.9442977905273438
Validation loss: 2.766094661528064

Epoch: 5| Step: 2
Training loss: 3.2272472381591797
Validation loss: 2.76552757652857

Epoch: 5| Step: 3
Training loss: 3.7073822021484375
Validation loss: 2.764449888660062

Epoch: 5| Step: 4
Training loss: 3.13703989982605
Validation loss: 2.76457239479147

Epoch: 5| Step: 5
Training loss: 2.3667750358581543
Validation loss: 2.76357667164136

Epoch: 5| Step: 6
Training loss: 2.7371788024902344
Validation loss: 2.7628176032855944

Epoch: 5| Step: 7
Training loss: 2.2758681774139404
Validation loss: 2.7625693275082495

Epoch: 5| Step: 8
Training loss: 2.5630440711975098
Validation loss: 2.761943068555606

Epoch: 5| Step: 9
Training loss: 2.958399534225464
Validation loss: 2.761603165698308

Epoch: 5| Step: 10
Training loss: 3.3118350505828857
Validation loss: 2.760438370448287

Epoch: 36| Step: 0
Training loss: 3.082000732421875
Validation loss: 2.759935325191867

Epoch: 5| Step: 1
Training loss: 3.0975825786590576
Validation loss: 2.759805230684178

Epoch: 5| Step: 2
Training loss: 2.7630527019500732
Validation loss: 2.758847826270647

Epoch: 5| Step: 3
Training loss: 1.8298546075820923
Validation loss: 2.758135800720543

Epoch: 5| Step: 4
Training loss: 3.382634401321411
Validation loss: 2.757903968134234

Epoch: 5| Step: 5
Training loss: 2.778701066970825
Validation loss: 2.7572851616849183

Epoch: 5| Step: 6
Training loss: 2.450416088104248
Validation loss: 2.756805694231423

Epoch: 5| Step: 7
Training loss: 3.4379076957702637
Validation loss: 2.7561030182787167

Epoch: 5| Step: 8
Training loss: 3.692267656326294
Validation loss: 2.7551659896809566

Epoch: 5| Step: 9
Training loss: 3.280736207962036
Validation loss: 2.7547010401243806

Epoch: 5| Step: 10
Training loss: 2.055352210998535
Validation loss: 2.752758026123047

Epoch: 37| Step: 0
Training loss: 2.6413745880126953
Validation loss: 2.753742810218565

Epoch: 5| Step: 1
Training loss: 3.076707363128662
Validation loss: 2.751886142197476

Epoch: 5| Step: 2
Training loss: 2.986921548843384
Validation loss: 2.7513663025312525

Epoch: 5| Step: 3
Training loss: 3.2811827659606934
Validation loss: 2.751445129353513

Epoch: 5| Step: 4
Training loss: 2.9381604194641113
Validation loss: 2.749573384561846

Epoch: 5| Step: 5
Training loss: 2.210902452468872
Validation loss: 2.7498441306493615

Epoch: 5| Step: 6
Training loss: 3.8324954509735107
Validation loss: 2.7501089854906966

Epoch: 5| Step: 7
Training loss: 3.1625587940216064
Validation loss: 2.7480183314251643

Epoch: 5| Step: 8
Training loss: 2.567131757736206
Validation loss: 2.7640106319099345

Epoch: 5| Step: 9
Training loss: 2.4065585136413574
Validation loss: 2.747787155130858

Epoch: 5| Step: 10
Training loss: 2.8170199394226074
Validation loss: 2.748028152732439

Epoch: 38| Step: 0
Training loss: 2.072761058807373
Validation loss: 2.7488017082214355

Epoch: 5| Step: 1
Training loss: 3.1162431240081787
Validation loss: 2.749624526628884

Epoch: 5| Step: 2
Training loss: 2.7352752685546875
Validation loss: 2.7522963708446873

Epoch: 5| Step: 3
Training loss: 3.6964328289031982
Validation loss: 2.7549706915373444

Epoch: 5| Step: 4
Training loss: 2.431389570236206
Validation loss: 2.751673972734841

Epoch: 5| Step: 5
Training loss: 3.2849769592285156
Validation loss: 2.7458978622190413

Epoch: 5| Step: 6
Training loss: 3.474980115890503
Validation loss: 2.7445602250355545

Epoch: 5| Step: 7
Training loss: 3.155696153640747
Validation loss: 2.743796912572717

Epoch: 5| Step: 8
Training loss: 2.3507065773010254
Validation loss: 2.7428059347214235

Epoch: 5| Step: 9
Training loss: 2.247887372970581
Validation loss: 2.745696588229108

Epoch: 5| Step: 10
Training loss: 3.4296376705169678
Validation loss: 2.7481020496737574

Epoch: 39| Step: 0
Training loss: 3.0752511024475098
Validation loss: 2.746394823956233

Epoch: 5| Step: 1
Training loss: 2.486100673675537
Validation loss: 2.741035238389046

Epoch: 5| Step: 2
Training loss: 2.714226722717285
Validation loss: 2.7404227666957404

Epoch: 5| Step: 3
Training loss: 3.385209560394287
Validation loss: 2.74043869972229

Epoch: 5| Step: 4
Training loss: 2.4296460151672363
Validation loss: 2.7369703887611307

Epoch: 5| Step: 5
Training loss: 2.6134417057037354
Validation loss: 2.7369541224612983

Epoch: 5| Step: 6
Training loss: 3.301297426223755
Validation loss: 2.7360530284143265

Epoch: 5| Step: 7
Training loss: 3.646700620651245
Validation loss: 2.736266702734014

Epoch: 5| Step: 8
Training loss: 2.8527207374572754
Validation loss: 2.735224013687462

Epoch: 5| Step: 9
Training loss: 2.711068630218506
Validation loss: 2.735985461101737

Epoch: 5| Step: 10
Training loss: 2.568424940109253
Validation loss: 2.736024530985022

Epoch: 40| Step: 0
Training loss: 3.048086643218994
Validation loss: 2.734346289788523

Epoch: 5| Step: 1
Training loss: 2.9332199096679688
Validation loss: 2.735350319134292

Epoch: 5| Step: 2
Training loss: 2.9515862464904785
Validation loss: 2.7319549796401814

Epoch: 5| Step: 3
Training loss: 3.0523324012756348
Validation loss: 2.732614960721744

Epoch: 5| Step: 4
Training loss: 2.7163596153259277
Validation loss: 2.732627225178544

Epoch: 5| Step: 5
Training loss: 2.514425754547119
Validation loss: 2.7316229522869153

Epoch: 5| Step: 6
Training loss: 3.600433349609375
Validation loss: 2.7317848154293594

Epoch: 5| Step: 7
Training loss: 1.959558129310608
Validation loss: 2.731627297657792

Epoch: 5| Step: 8
Training loss: 3.6969573497772217
Validation loss: 2.728980505338279

Epoch: 5| Step: 9
Training loss: 2.8844552040100098
Validation loss: 2.7292872218675512

Epoch: 5| Step: 10
Training loss: 2.319215774536133
Validation loss: 2.726592427940779

Epoch: 41| Step: 0
Training loss: 3.4544830322265625
Validation loss: 2.7247335731342273

Epoch: 5| Step: 1
Training loss: 2.547386884689331
Validation loss: 2.7247448198256956

Epoch: 5| Step: 2
Training loss: 3.3477470874786377
Validation loss: 2.724166308679888

Epoch: 5| Step: 3
Training loss: 2.801851987838745
Validation loss: 2.725044855507471

Epoch: 5| Step: 4
Training loss: 2.934708833694458
Validation loss: 2.724019653053694

Epoch: 5| Step: 5
Training loss: 2.7293663024902344
Validation loss: 2.719656252091931

Epoch: 5| Step: 6
Training loss: 3.2445406913757324
Validation loss: 2.7200689008159022

Epoch: 5| Step: 7
Training loss: 3.063141345977783
Validation loss: 2.7206629758240073

Epoch: 5| Step: 8
Training loss: 3.447827100753784
Validation loss: 2.717796620502267

Epoch: 5| Step: 9
Training loss: 2.196350336074829
Validation loss: 2.7180827304881108

Epoch: 5| Step: 10
Training loss: 1.7856173515319824
Validation loss: 2.71661312349381

Epoch: 42| Step: 0
Training loss: 2.703333616256714
Validation loss: 2.720484813054403

Epoch: 5| Step: 1
Training loss: 1.8333711624145508
Validation loss: 2.721099099805278

Epoch: 5| Step: 2
Training loss: 3.012174129486084
Validation loss: 2.721329435225456

Epoch: 5| Step: 3
Training loss: 3.4896881580352783
Validation loss: 2.724055351749543

Epoch: 5| Step: 4
Training loss: 3.498260021209717
Validation loss: 2.724529968794956

Epoch: 5| Step: 5
Training loss: 2.570183753967285
Validation loss: 2.7177448375250703

Epoch: 5| Step: 6
Training loss: 3.257474422454834
Validation loss: 2.718931887739448

Epoch: 5| Step: 7
Training loss: 3.122600555419922
Validation loss: 2.712049743180634

Epoch: 5| Step: 8
Training loss: 2.8926796913146973
Validation loss: 2.712706458184027

Epoch: 5| Step: 9
Training loss: 2.3087613582611084
Validation loss: 2.71293048192096

Epoch: 5| Step: 10
Training loss: 3.005113363265991
Validation loss: 2.714055190804184

Epoch: 43| Step: 0
Training loss: 2.524341106414795
Validation loss: 2.7108187265293573

Epoch: 5| Step: 1
Training loss: 2.5464940071105957
Validation loss: 2.706802393800469

Epoch: 5| Step: 2
Training loss: 3.629915952682495
Validation loss: 2.7078231073194936

Epoch: 5| Step: 3
Training loss: 2.777097702026367
Validation loss: 2.709416861175209

Epoch: 5| Step: 4
Training loss: 2.7951414585113525
Validation loss: 2.70819842174489

Epoch: 5| Step: 5
Training loss: 2.9306836128234863
Validation loss: 2.7056476864763486

Epoch: 5| Step: 6
Training loss: 2.85705828666687
Validation loss: 2.703810409833026

Epoch: 5| Step: 7
Training loss: 2.49204158782959
Validation loss: 2.7084102015341482

Epoch: 5| Step: 8
Training loss: 3.063793897628784
Validation loss: 2.7028420612376225

Epoch: 5| Step: 9
Training loss: 3.0301809310913086
Validation loss: 2.71030661623965

Epoch: 5| Step: 10
Training loss: 2.9215879440307617
Validation loss: 2.7336528301239014

Epoch: 44| Step: 0
Training loss: 2.8849780559539795
Validation loss: 2.7683839208336285

Epoch: 5| Step: 1
Training loss: 2.399566411972046
Validation loss: 2.7534416183348625

Epoch: 5| Step: 2
Training loss: 2.8004062175750732
Validation loss: 2.7604666704772622

Epoch: 5| Step: 3
Training loss: 3.383854389190674
Validation loss: 2.7196809066239225

Epoch: 5| Step: 4
Training loss: 3.6566455364227295
Validation loss: 2.7056625914830033

Epoch: 5| Step: 5
Training loss: 2.531902551651001
Validation loss: 2.745775450942337

Epoch: 5| Step: 6
Training loss: 2.860244035720825
Validation loss: 2.735392988369029

Epoch: 5| Step: 7
Training loss: 3.2934889793395996
Validation loss: 2.7153093097030476

Epoch: 5| Step: 8
Training loss: 3.395035982131958
Validation loss: 2.714655173722134

Epoch: 5| Step: 9
Training loss: 2.082428455352783
Validation loss: 2.704727703525174

Epoch: 5| Step: 10
Training loss: 2.3025646209716797
Validation loss: 2.7041604826527257

Epoch: 45| Step: 0
Training loss: 2.251788854598999
Validation loss: 2.7478324392790436

Epoch: 5| Step: 1
Training loss: 2.5115625858306885
Validation loss: 2.738926177383751

Epoch: 5| Step: 2
Training loss: 2.077204942703247
Validation loss: 2.7355387441573606

Epoch: 5| Step: 3
Training loss: 2.8520166873931885
Validation loss: 2.7406076103128414

Epoch: 5| Step: 4
Training loss: 3.123485565185547
Validation loss: 2.7405287450359714

Epoch: 5| Step: 5
Training loss: 3.0198168754577637
Validation loss: 2.7242930960911576

Epoch: 5| Step: 6
Training loss: 2.7976176738739014
Validation loss: 2.7201667677971626

Epoch: 5| Step: 7
Training loss: 3.2401649951934814
Validation loss: 2.7222425732561337

Epoch: 5| Step: 8
Training loss: 3.2802517414093018
Validation loss: 2.7237227603953373

Epoch: 5| Step: 9
Training loss: 3.1060426235198975
Validation loss: 2.7040410298173145

Epoch: 5| Step: 10
Training loss: 3.3375418186187744
Validation loss: 2.6981935629280667

Epoch: 46| Step: 0
Training loss: 3.4078986644744873
Validation loss: 2.69303648958924

Epoch: 5| Step: 1
Training loss: 2.7065765857696533
Validation loss: 2.6950028609204035

Epoch: 5| Step: 2
Training loss: 2.8831963539123535
Validation loss: 2.7040284423417944

Epoch: 5| Step: 3
Training loss: 2.355365037918091
Validation loss: 2.7077053080322924

Epoch: 5| Step: 4
Training loss: 2.7974693775177
Validation loss: 2.7389692721828336

Epoch: 5| Step: 5
Training loss: 2.6904854774475098
Validation loss: 2.7363882449365433

Epoch: 5| Step: 6
Training loss: 2.3662123680114746
Validation loss: 2.7235504068354124

Epoch: 5| Step: 7
Training loss: 3.1885032653808594
Validation loss: 2.710144337787423

Epoch: 5| Step: 8
Training loss: 3.2059006690979004
Validation loss: 2.7114243712476505

Epoch: 5| Step: 9
Training loss: 2.7136738300323486
Validation loss: 2.721767787010439

Epoch: 5| Step: 10
Training loss: 3.187173366546631
Validation loss: 2.7177225005242134

Epoch: 47| Step: 0
Training loss: 2.2878003120422363
Validation loss: 2.7057030406049503

Epoch: 5| Step: 1
Training loss: 3.049478530883789
Validation loss: 2.703941781033752

Epoch: 5| Step: 2
Training loss: 2.7596051692962646
Validation loss: 2.6909746495626305

Epoch: 5| Step: 3
Training loss: 2.808879852294922
Validation loss: 2.686253191322409

Epoch: 5| Step: 4
Training loss: 2.4765686988830566
Validation loss: 2.684412858819449

Epoch: 5| Step: 5
Training loss: 3.103588819503784
Validation loss: 2.6820375816796416

Epoch: 5| Step: 6
Training loss: 2.951178550720215
Validation loss: 2.682869795830019

Epoch: 5| Step: 7
Training loss: 2.846184253692627
Validation loss: 2.6856370049138225

Epoch: 5| Step: 8
Training loss: 2.9241812229156494
Validation loss: 2.682045882748019

Epoch: 5| Step: 9
Training loss: 2.5792109966278076
Validation loss: 2.698792319143972

Epoch: 5| Step: 10
Training loss: 3.4952120780944824
Validation loss: 2.7000079283150296

Epoch: 48| Step: 0
Training loss: 2.8165805339813232
Validation loss: 2.684487460761942

Epoch: 5| Step: 1
Training loss: 3.074331760406494
Validation loss: 2.682973564312022

Epoch: 5| Step: 2
Training loss: 3.267916202545166
Validation loss: 2.67771892393789

Epoch: 5| Step: 3
Training loss: 2.716496706008911
Validation loss: 2.6752890412525465

Epoch: 5| Step: 4
Training loss: 3.524707317352295
Validation loss: 2.673837656615883

Epoch: 5| Step: 5
Training loss: 2.7832233905792236
Validation loss: 2.6791453669148106

Epoch: 5| Step: 6
Training loss: 2.2462480068206787
Validation loss: 2.688217329722579

Epoch: 5| Step: 7
Training loss: 2.961869955062866
Validation loss: 2.694036065891225

Epoch: 5| Step: 8
Training loss: 2.460463047027588
Validation loss: 2.698712812956943

Epoch: 5| Step: 9
Training loss: 2.320014238357544
Validation loss: 2.6874132643463793

Epoch: 5| Step: 10
Training loss: 3.04750394821167
Validation loss: 2.686667046239299

Epoch: 49| Step: 0
Training loss: 2.604651927947998
Validation loss: 2.6792328639697005

Epoch: 5| Step: 1
Training loss: 2.905691146850586
Validation loss: 2.6770409243081206

Epoch: 5| Step: 2
Training loss: 3.4319252967834473
Validation loss: 2.683222222071822

Epoch: 5| Step: 3
Training loss: 2.475520133972168
Validation loss: 2.693363389661235

Epoch: 5| Step: 4
Training loss: 2.023221254348755
Validation loss: 2.6936895411501647

Epoch: 5| Step: 5
Training loss: 3.1830334663391113
Validation loss: 2.718872780440956

Epoch: 5| Step: 6
Training loss: 3.009394884109497
Validation loss: 2.719992827343684

Epoch: 5| Step: 7
Training loss: 2.9307615756988525
Validation loss: 2.713615796899283

Epoch: 5| Step: 8
Training loss: 2.964022159576416
Validation loss: 2.7049190408440045

Epoch: 5| Step: 9
Training loss: 2.512003183364868
Validation loss: 2.694908433063056

Epoch: 5| Step: 10
Training loss: 3.239293336868286
Validation loss: 2.690051222360262

Epoch: 50| Step: 0
Training loss: 2.922264814376831
Validation loss: 2.684022395841537

Epoch: 5| Step: 1
Training loss: 3.2929482460021973
Validation loss: 2.679718371360533

Epoch: 5| Step: 2
Training loss: 2.436584949493408
Validation loss: 2.674572903622863

Epoch: 5| Step: 3
Training loss: 2.236635684967041
Validation loss: 2.6782465673262075

Epoch: 5| Step: 4
Training loss: 2.2477848529815674
Validation loss: 2.6868302488839753

Epoch: 5| Step: 5
Training loss: 2.884270429611206
Validation loss: 2.6891006577399468

Epoch: 5| Step: 6
Training loss: 3.029139995574951
Validation loss: 2.6902115883365756

Epoch: 5| Step: 7
Training loss: 3.4346988201141357
Validation loss: 2.6880572457467355

Epoch: 5| Step: 8
Training loss: 2.162916421890259
Validation loss: 2.6924548302927325

Epoch: 5| Step: 9
Training loss: 3.456454038619995
Validation loss: 2.7078661687912478

Epoch: 5| Step: 10
Training loss: 2.9253170490264893
Validation loss: 2.7151617029661774

Epoch: 51| Step: 0
Training loss: 2.8498482704162598
Validation loss: 2.7307349712617937

Epoch: 5| Step: 1
Training loss: 3.1579387187957764
Validation loss: 2.737728149660172

Epoch: 5| Step: 2
Training loss: 2.3941025733947754
Validation loss: 2.741864045461019

Epoch: 5| Step: 3
Training loss: 3.2250449657440186
Validation loss: 2.745983639071065

Epoch: 5| Step: 4
Training loss: 2.7236721515655518
Validation loss: 2.7487932353891353

Epoch: 5| Step: 5
Training loss: 2.47300124168396
Validation loss: 2.7493860337042038

Epoch: 5| Step: 6
Training loss: 2.781508684158325
Validation loss: 2.748497373314314

Epoch: 5| Step: 7
Training loss: 3.1919569969177246
Validation loss: 2.745180171023133

Epoch: 5| Step: 8
Training loss: 2.7978262901306152
Validation loss: 2.730925488215621

Epoch: 5| Step: 9
Training loss: 2.6693170070648193
Validation loss: 2.724579018931235

Epoch: 5| Step: 10
Training loss: 3.1318466663360596
Validation loss: 2.718134428865166

Epoch: 52| Step: 0
Training loss: 2.963522434234619
Validation loss: 2.7152180594782673

Epoch: 5| Step: 1
Training loss: 2.9580159187316895
Validation loss: 2.710753784384779

Epoch: 5| Step: 2
Training loss: 2.1787800788879395
Validation loss: 2.713189181461129

Epoch: 5| Step: 3
Training loss: 1.8445221185684204
Validation loss: 2.7114083510573193

Epoch: 5| Step: 4
Training loss: 2.949822187423706
Validation loss: 2.7068004556881484

Epoch: 5| Step: 5
Training loss: 3.2281603813171387
Validation loss: 2.710086948128157

Epoch: 5| Step: 6
Training loss: 3.499941349029541
Validation loss: 2.7063479449159358

Epoch: 5| Step: 7
Training loss: 2.746838331222534
Validation loss: 2.7041742929848294

Epoch: 5| Step: 8
Training loss: 2.235161781311035
Validation loss: 2.7037542327757804

Epoch: 5| Step: 9
Training loss: 2.837224245071411
Validation loss: 2.699689906130555

Epoch: 5| Step: 10
Training loss: 3.903654098510742
Validation loss: 2.7033017322581303

Epoch: 53| Step: 0
Training loss: 2.8747267723083496
Validation loss: 2.703333047128493

Epoch: 5| Step: 1
Training loss: 3.3524158000946045
Validation loss: 2.7024331785017446

Epoch: 5| Step: 2
Training loss: 3.0599210262298584
Validation loss: 2.700151352472203

Epoch: 5| Step: 3
Training loss: 2.2485568523406982
Validation loss: 2.7007107452679704

Epoch: 5| Step: 4
Training loss: 2.789397954940796
Validation loss: 2.7034215568214335

Epoch: 5| Step: 5
Training loss: 3.2163569927215576
Validation loss: 2.695687258115379

Epoch: 5| Step: 6
Training loss: 2.899064302444458
Validation loss: 2.6964088229722876

Epoch: 5| Step: 7
Training loss: 2.793151617050171
Validation loss: 2.6974609103254092

Epoch: 5| Step: 8
Training loss: 3.0179152488708496
Validation loss: 2.696060570337439

Epoch: 5| Step: 9
Training loss: 2.2213172912597656
Validation loss: 2.696604169825072

Epoch: 5| Step: 10
Training loss: 2.5732529163360596
Validation loss: 2.6942705723547165

Epoch: 54| Step: 0
Training loss: 3.478959321975708
Validation loss: 2.6921727734227336

Epoch: 5| Step: 1
Training loss: 2.6687467098236084
Validation loss: 2.6899231787650817

Epoch: 5| Step: 2
Training loss: 2.065160036087036
Validation loss: 2.6866471152151785

Epoch: 5| Step: 3
Training loss: 2.3815901279449463
Validation loss: 2.68849515914917

Epoch: 5| Step: 4
Training loss: 2.9877052307128906
Validation loss: 2.6881790238042034

Epoch: 5| Step: 5
Training loss: 3.578075885772705
Validation loss: 2.687557869060065

Epoch: 5| Step: 6
Training loss: 2.763373613357544
Validation loss: 2.683407393834924

Epoch: 5| Step: 7
Training loss: 2.0967049598693848
Validation loss: 2.6824548757204445

Epoch: 5| Step: 8
Training loss: 3.254237651824951
Validation loss: 2.6848781416493077

Epoch: 5| Step: 9
Training loss: 3.4658846855163574
Validation loss: 2.6791088734903643

Epoch: 5| Step: 10
Training loss: 2.239241361618042
Validation loss: 2.693052554643282

Epoch: 55| Step: 0
Training loss: 2.5532355308532715
Validation loss: 2.7008160750071206

Epoch: 5| Step: 1
Training loss: 2.5454905033111572
Validation loss: 2.6982516216975387

Epoch: 5| Step: 2
Training loss: 3.0980160236358643
Validation loss: 2.692987472780289

Epoch: 5| Step: 3
Training loss: 2.768253803253174
Validation loss: 2.6939869209002425

Epoch: 5| Step: 4
Training loss: 2.971816301345825
Validation loss: 2.6937028284995788

Epoch: 5| Step: 5
Training loss: 2.774756908416748
Validation loss: 2.6906728334324335

Epoch: 5| Step: 6
Training loss: 2.944425582885742
Validation loss: 2.677783009826496

Epoch: 5| Step: 7
Training loss: 2.7411818504333496
Validation loss: 2.674480087013655

Epoch: 5| Step: 8
Training loss: 2.5707802772521973
Validation loss: 2.668832284148021

Epoch: 5| Step: 9
Training loss: 3.383775234222412
Validation loss: 2.6679804632740636

Epoch: 5| Step: 10
Training loss: 2.605562210083008
Validation loss: 2.669677439556327

Epoch: 56| Step: 0
Training loss: 2.377758741378784
Validation loss: 2.6694471861726496

Epoch: 5| Step: 1
Training loss: 2.999774932861328
Validation loss: 2.6664863555662093

Epoch: 5| Step: 2
Training loss: 2.753142833709717
Validation loss: 2.6645902561885055

Epoch: 5| Step: 3
Training loss: 2.747525453567505
Validation loss: 2.663726370821717

Epoch: 5| Step: 4
Training loss: 3.21722412109375
Validation loss: 2.6627791363705873

Epoch: 5| Step: 5
Training loss: 3.1057794094085693
Validation loss: 2.6564205333750737

Epoch: 5| Step: 6
Training loss: 2.3041911125183105
Validation loss: 2.6564138243275304

Epoch: 5| Step: 7
Training loss: 3.471601963043213
Validation loss: 2.6534906638565885

Epoch: 5| Step: 8
Training loss: 2.471430540084839
Validation loss: 2.6406806720200406

Epoch: 5| Step: 9
Training loss: 2.8180935382843018
Validation loss: 2.6496720057661816

Epoch: 5| Step: 10
Training loss: 2.521479606628418
Validation loss: 2.6465792476489978

Epoch: 57| Step: 0
Training loss: 3.2529449462890625
Validation loss: 2.641303229075606

Epoch: 5| Step: 1
Training loss: 2.474274158477783
Validation loss: 2.6347635663965696

Epoch: 5| Step: 2
Training loss: 2.9277210235595703
Validation loss: 2.635568698247274

Epoch: 5| Step: 3
Training loss: 2.843604326248169
Validation loss: 2.646179594019408

Epoch: 5| Step: 4
Training loss: 3.5780029296875
Validation loss: 2.6416008651897473

Epoch: 5| Step: 5
Training loss: 3.0360267162323
Validation loss: 2.627432850099379

Epoch: 5| Step: 6
Training loss: 2.465853452682495
Validation loss: 2.6252154073407574

Epoch: 5| Step: 7
Training loss: 3.106306791305542
Validation loss: 2.6272411372071955

Epoch: 5| Step: 8
Training loss: 2.224128007888794
Validation loss: 2.6287750249267905

Epoch: 5| Step: 9
Training loss: 2.4591801166534424
Validation loss: 2.630042709330077

Epoch: 5| Step: 10
Training loss: 2.1171810626983643
Validation loss: 2.6362833002562165

Epoch: 58| Step: 0
Training loss: 2.889738082885742
Validation loss: 2.6440826462161158

Epoch: 5| Step: 1
Training loss: 2.365302562713623
Validation loss: 2.6522829404441257

Epoch: 5| Step: 2
Training loss: 2.4443271160125732
Validation loss: 2.666312445876419

Epoch: 5| Step: 3
Training loss: 3.538351058959961
Validation loss: 2.680436001029066

Epoch: 5| Step: 4
Training loss: 3.0709726810455322
Validation loss: 2.673130494292064

Epoch: 5| Step: 5
Training loss: 2.8346056938171387
Validation loss: 2.6514208675712667

Epoch: 5| Step: 6
Training loss: 2.4547107219696045
Validation loss: 2.629760273041264

Epoch: 5| Step: 7
Training loss: 2.9077322483062744
Validation loss: 2.630033778887923

Epoch: 5| Step: 8
Training loss: 2.7578835487365723
Validation loss: 2.662712553496002

Epoch: 5| Step: 9
Training loss: 2.8628368377685547
Validation loss: 2.693844420935518

Epoch: 5| Step: 10
Training loss: 2.6834657192230225
Validation loss: 2.6334367541856665

Epoch: 59| Step: 0
Training loss: 3.397784471511841
Validation loss: 2.6126739466062157

Epoch: 5| Step: 1
Training loss: 3.3596668243408203
Validation loss: 2.603455399954191

Epoch: 5| Step: 2
Training loss: 2.9978280067443848
Validation loss: 2.6065322968267624

Epoch: 5| Step: 3
Training loss: 3.0584778785705566
Validation loss: 2.6161691142666723

Epoch: 5| Step: 4
Training loss: 2.7456214427948
Validation loss: 2.637369178956555

Epoch: 5| Step: 5
Training loss: 2.4981093406677246
Validation loss: 2.655550105597383

Epoch: 5| Step: 6
Training loss: 2.327181339263916
Validation loss: 2.662424574616135

Epoch: 5| Step: 7
Training loss: 2.579451084136963
Validation loss: 2.6585540797120784

Epoch: 5| Step: 8
Training loss: 2.4966273307800293
Validation loss: 2.647175537642612

Epoch: 5| Step: 9
Training loss: 2.281648874282837
Validation loss: 2.6435218344452562

Epoch: 5| Step: 10
Training loss: 2.8939261436462402
Validation loss: 2.6375413504979943

Epoch: 60| Step: 0
Training loss: 2.5124382972717285
Validation loss: 2.6304681352389756

Epoch: 5| Step: 1
Training loss: 3.0529205799102783
Validation loss: 2.62591746289243

Epoch: 5| Step: 2
Training loss: 3.316720962524414
Validation loss: 2.623937573484195

Epoch: 5| Step: 3
Training loss: 2.3904035091400146
Validation loss: 2.613312541797597

Epoch: 5| Step: 4
Training loss: 2.312605619430542
Validation loss: 2.613373043716595

Epoch: 5| Step: 5
Training loss: 2.5500783920288086
Validation loss: 2.588213136119227

Epoch: 5| Step: 6
Training loss: 3.072892665863037
Validation loss: 2.5892113665098786

Epoch: 5| Step: 7
Training loss: 2.746173620223999
Validation loss: 2.5917967673270934

Epoch: 5| Step: 8
Training loss: 3.222647190093994
Validation loss: 2.6030300919727614

Epoch: 5| Step: 9
Training loss: 2.850062608718872
Validation loss: 2.6086937125011156

Epoch: 5| Step: 10
Training loss: 2.5156242847442627
Validation loss: 2.605475982030233

Epoch: 61| Step: 0
Training loss: 2.516310214996338
Validation loss: 2.6045410351086686

Epoch: 5| Step: 1
Training loss: 2.9764790534973145
Validation loss: 2.5945990470147904

Epoch: 5| Step: 2
Training loss: 3.718759536743164
Validation loss: 2.5857433119127826

Epoch: 5| Step: 3
Training loss: 2.719208002090454
Validation loss: 2.5894614086356214

Epoch: 5| Step: 4
Training loss: 2.9886600971221924
Validation loss: 2.5967813973785727

Epoch: 5| Step: 5
Training loss: 2.9541923999786377
Validation loss: 2.602235553085163

Epoch: 5| Step: 6
Training loss: 2.3568031787872314
Validation loss: 2.608349682182394

Epoch: 5| Step: 7
Training loss: 2.36958909034729
Validation loss: 2.602094973287275

Epoch: 5| Step: 8
Training loss: 3.0235795974731445
Validation loss: 2.5906281548161663

Epoch: 5| Step: 9
Training loss: 2.92000150680542
Validation loss: 2.5818440273243892

Epoch: 5| Step: 10
Training loss: 1.7732328176498413
Validation loss: 2.5751938935249084

Epoch: 62| Step: 0
Training loss: 2.487720489501953
Validation loss: 2.5664866675612745

Epoch: 5| Step: 1
Training loss: 2.482140064239502
Validation loss: 2.548079921353248

Epoch: 5| Step: 2
Training loss: 3.0542681217193604
Validation loss: 2.5546029767682477

Epoch: 5| Step: 3
Training loss: 2.4210896492004395
Validation loss: 2.561146956618114

Epoch: 5| Step: 4
Training loss: 2.285205125808716
Validation loss: 2.5662363575350855

Epoch: 5| Step: 5
Training loss: 3.1816704273223877
Validation loss: 2.5656449820405696

Epoch: 5| Step: 6
Training loss: 2.502094268798828
Validation loss: 2.551210303460398

Epoch: 5| Step: 7
Training loss: 2.892411708831787
Validation loss: 2.5389358074434343

Epoch: 5| Step: 8
Training loss: 2.7786805629730225
Validation loss: 2.534286486205234

Epoch: 5| Step: 9
Training loss: 3.0055460929870605
Validation loss: 2.530459991065405

Epoch: 5| Step: 10
Training loss: 2.858445167541504
Validation loss: 2.526023277672388

Epoch: 63| Step: 0
Training loss: 2.5188851356506348
Validation loss: 2.522724997612738

Epoch: 5| Step: 1
Training loss: 3.511246919631958
Validation loss: 2.5234017961768695

Epoch: 5| Step: 2
Training loss: 2.551353693008423
Validation loss: 2.5203564987387708

Epoch: 5| Step: 3
Training loss: 2.539541721343994
Validation loss: 2.520052053595102

Epoch: 5| Step: 4
Training loss: 2.5170798301696777
Validation loss: 2.5200497565730924

Epoch: 5| Step: 5
Training loss: 3.337484836578369
Validation loss: 2.5200141399137435

Epoch: 5| Step: 6
Training loss: 2.825571060180664
Validation loss: 2.521361720177435

Epoch: 5| Step: 7
Training loss: 2.4890384674072266
Validation loss: 2.5213114087299635

Epoch: 5| Step: 8
Training loss: 3.251614809036255
Validation loss: 2.5246872671188845

Epoch: 5| Step: 9
Training loss: 1.9117225408554077
Validation loss: 2.5274517049071608

Epoch: 5| Step: 10
Training loss: 2.3409810066223145
Validation loss: 2.539258446744693

Epoch: 64| Step: 0
Training loss: 2.8251500129699707
Validation loss: 2.5269573939743863

Epoch: 5| Step: 1
Training loss: 3.2196974754333496
Validation loss: 2.529097810868294

Epoch: 5| Step: 2
Training loss: 2.478318452835083
Validation loss: 2.523804885084911

Epoch: 5| Step: 3
Training loss: 2.8873062133789062
Validation loss: 2.524948727700018

Epoch: 5| Step: 4
Training loss: 2.9778084754943848
Validation loss: 2.5238757338575137

Epoch: 5| Step: 5
Training loss: 2.69289493560791
Validation loss: 2.5243335693113265

Epoch: 5| Step: 6
Training loss: 2.9832911491394043
Validation loss: 2.5188058294275755

Epoch: 5| Step: 7
Training loss: 2.7158031463623047
Validation loss: 2.519595943471437

Epoch: 5| Step: 8
Training loss: 2.163179397583008
Validation loss: 2.516995224901425

Epoch: 5| Step: 9
Training loss: 2.5286803245544434
Validation loss: 2.5192963564267723

Epoch: 5| Step: 10
Training loss: 2.231718063354492
Validation loss: 2.524330162232922

Epoch: 65| Step: 0
Training loss: 2.9044151306152344
Validation loss: 2.529642517848681

Epoch: 5| Step: 1
Training loss: 2.308716058731079
Validation loss: 2.533049650089715

Epoch: 5| Step: 2
Training loss: 2.775735378265381
Validation loss: 2.555272279247161

Epoch: 5| Step: 3
Training loss: 2.2730536460876465
Validation loss: 2.5760374453759964

Epoch: 5| Step: 4
Training loss: 2.8760008811950684
Validation loss: 2.5610938764387563

Epoch: 5| Step: 5
Training loss: 2.1356921195983887
Validation loss: 2.5270143349965415

Epoch: 5| Step: 6
Training loss: 2.714249849319458
Validation loss: 2.5168406066074165

Epoch: 5| Step: 7
Training loss: 3.0904321670532227
Validation loss: 2.5082740014599216

Epoch: 5| Step: 8
Training loss: 2.8230652809143066
Validation loss: 2.518675988720309

Epoch: 5| Step: 9
Training loss: 3.2805705070495605
Validation loss: 2.531862728057369

Epoch: 5| Step: 10
Training loss: 2.729837417602539
Validation loss: 2.514776914350448

Epoch: 66| Step: 0
Training loss: 2.5701241493225098
Validation loss: 2.5093646664773264

Epoch: 5| Step: 1
Training loss: 2.4924933910369873
Validation loss: 2.5095277268399476

Epoch: 5| Step: 2
Training loss: 2.6239678859710693
Validation loss: 2.510283944427326

Epoch: 5| Step: 3
Training loss: 2.7821590900421143
Validation loss: 2.5145208348510084

Epoch: 5| Step: 4
Training loss: 2.8481719493865967
Validation loss: 2.512413510712244

Epoch: 5| Step: 5
Training loss: 2.319239854812622
Validation loss: 2.5176800630425893

Epoch: 5| Step: 6
Training loss: 2.7797293663024902
Validation loss: 2.5083148966553392

Epoch: 5| Step: 7
Training loss: 2.420287609100342
Validation loss: 2.5066501555904264

Epoch: 5| Step: 8
Training loss: 2.39024019241333
Validation loss: 2.5104747895271546

Epoch: 5| Step: 9
Training loss: 3.5824859142303467
Validation loss: 2.510374992124496

Epoch: 5| Step: 10
Training loss: 3.006087064743042
Validation loss: 2.512003734547605

Epoch: 67| Step: 0
Training loss: 2.5798916816711426
Validation loss: 2.510721068228445

Epoch: 5| Step: 1
Training loss: 2.506152629852295
Validation loss: 2.512218544560094

Epoch: 5| Step: 2
Training loss: 2.4433295726776123
Validation loss: 2.5135584031381915

Epoch: 5| Step: 3
Training loss: 2.884615659713745
Validation loss: 2.518169282585062

Epoch: 5| Step: 4
Training loss: 2.9193294048309326
Validation loss: 2.5134003034202

Epoch: 5| Step: 5
Training loss: 3.389117479324341
Validation loss: 2.514698020873531

Epoch: 5| Step: 6
Training loss: 2.278902530670166
Validation loss: 2.5150301482087825

Epoch: 5| Step: 7
Training loss: 2.7135324478149414
Validation loss: 2.511666605549474

Epoch: 5| Step: 8
Training loss: 2.339421033859253
Validation loss: 2.515067864489812

Epoch: 5| Step: 9
Training loss: 3.1014914512634277
Validation loss: 2.5166262554866012

Epoch: 5| Step: 10
Training loss: 2.4999144077301025
Validation loss: 2.5183099264739663

Epoch: 68| Step: 0
Training loss: 2.555440902709961
Validation loss: 2.51843031760185

Epoch: 5| Step: 1
Training loss: 2.2634313106536865
Validation loss: 2.5313208180089153

Epoch: 5| Step: 2
Training loss: 3.3077664375305176
Validation loss: 2.5289265160919516

Epoch: 5| Step: 3
Training loss: 2.661663055419922
Validation loss: 2.5297543541077645

Epoch: 5| Step: 4
Training loss: 2.7429604530334473
Validation loss: 2.5206368559150287

Epoch: 5| Step: 5
Training loss: 2.663977861404419
Validation loss: 2.508036249427385

Epoch: 5| Step: 6
Training loss: 2.5117695331573486
Validation loss: 2.5040523082979265

Epoch: 5| Step: 7
Training loss: 2.4757843017578125
Validation loss: 2.502249017838509

Epoch: 5| Step: 8
Training loss: 2.9741947650909424
Validation loss: 2.4948059871632564

Epoch: 5| Step: 9
Training loss: 2.8337202072143555
Validation loss: 2.4923455279360534

Epoch: 5| Step: 10
Training loss: 2.643904209136963
Validation loss: 2.49062745289136

Epoch: 69| Step: 0
Training loss: 2.694943428039551
Validation loss: 2.4875947249832975

Epoch: 5| Step: 1
Training loss: 2.901928186416626
Validation loss: 2.4862835471348097

Epoch: 5| Step: 2
Training loss: 2.4364168643951416
Validation loss: 2.4855490756291214

Epoch: 5| Step: 3
Training loss: 3.0149190425872803
Validation loss: 2.4885648988908335

Epoch: 5| Step: 4
Training loss: 2.913243055343628
Validation loss: 2.4852086138981644

Epoch: 5| Step: 5
Training loss: 2.3564453125
Validation loss: 2.4818973515623357

Epoch: 5| Step: 6
Training loss: 2.5674397945404053
Validation loss: 2.4858858764812513

Epoch: 5| Step: 7
Training loss: 2.6635780334472656
Validation loss: 2.481491434958673

Epoch: 5| Step: 8
Training loss: 2.420973539352417
Validation loss: 2.4783592352303128

Epoch: 5| Step: 9
Training loss: 2.87532639503479
Validation loss: 2.4798329491769113

Epoch: 5| Step: 10
Training loss: 2.7978811264038086
Validation loss: 2.4789044011023735

Epoch: 70| Step: 0
Training loss: 2.508030891418457
Validation loss: 2.477077248275921

Epoch: 5| Step: 1
Training loss: 3.2799224853515625
Validation loss: 2.4825008633316203

Epoch: 5| Step: 2
Training loss: 2.663759469985962
Validation loss: 2.4777479710117465

Epoch: 5| Step: 3
Training loss: 2.4235641956329346
Validation loss: 2.4789065058513353

Epoch: 5| Step: 4
Training loss: 2.418344020843506
Validation loss: 2.48679099031674

Epoch: 5| Step: 5
Training loss: 3.141867160797119
Validation loss: 2.4881039921955397

Epoch: 5| Step: 6
Training loss: 2.1102750301361084
Validation loss: 2.4816959737449564

Epoch: 5| Step: 7
Training loss: 2.5376391410827637
Validation loss: 2.478616186367568

Epoch: 5| Step: 8
Training loss: 2.831760883331299
Validation loss: 2.476930997704947

Epoch: 5| Step: 9
Training loss: 2.939025402069092
Validation loss: 2.4782304533066286

Epoch: 5| Step: 10
Training loss: 2.65748929977417
Validation loss: 2.475823921542014

Epoch: 71| Step: 0
Training loss: 3.230485200881958
Validation loss: 2.478206298684561

Epoch: 5| Step: 1
Training loss: 2.1331114768981934
Validation loss: 2.4768226146698

Epoch: 5| Step: 2
Training loss: 2.6875109672546387
Validation loss: 2.4755019577600623

Epoch: 5| Step: 3
Training loss: 2.7496142387390137
Validation loss: 2.4773940117128435

Epoch: 5| Step: 4
Training loss: 2.7647292613983154
Validation loss: 2.4730297288587018

Epoch: 5| Step: 5
Training loss: 3.042649269104004
Validation loss: 2.471552969307028

Epoch: 5| Step: 6
Training loss: 2.2261104583740234
Validation loss: 2.4705527418403217

Epoch: 5| Step: 7
Training loss: 2.9619343280792236
Validation loss: 2.4726657713613203

Epoch: 5| Step: 8
Training loss: 2.2300219535827637
Validation loss: 2.470645107248778

Epoch: 5| Step: 9
Training loss: 2.6355881690979004
Validation loss: 2.470878254982733

Epoch: 5| Step: 10
Training loss: 2.9282703399658203
Validation loss: 2.470181785604005

Epoch: 72| Step: 0
Training loss: 2.2960641384124756
Validation loss: 2.471533352328885

Epoch: 5| Step: 1
Training loss: 2.2673611640930176
Validation loss: 2.473944423019245

Epoch: 5| Step: 2
Training loss: 2.356896162033081
Validation loss: 2.4770767996388097

Epoch: 5| Step: 3
Training loss: 3.0995848178863525
Validation loss: 2.476305607826479

Epoch: 5| Step: 4
Training loss: 3.2880637645721436
Validation loss: 2.4942743086045787

Epoch: 5| Step: 5
Training loss: 2.3441505432128906
Validation loss: 2.497419859773369

Epoch: 5| Step: 6
Training loss: 3.1522724628448486
Validation loss: 2.4904540764388217

Epoch: 5| Step: 7
Training loss: 2.8405826091766357
Validation loss: 2.470758302237398

Epoch: 5| Step: 8
Training loss: 2.7281744480133057
Validation loss: 2.465971880061652

Epoch: 5| Step: 9
Training loss: 3.0992558002471924
Validation loss: 2.4645975148806007

Epoch: 5| Step: 10
Training loss: 1.8539568185806274
Validation loss: 2.4622016568337717

Epoch: 73| Step: 0
Training loss: 2.6053953170776367
Validation loss: 2.467606275312362

Epoch: 5| Step: 1
Training loss: 2.89542818069458
Validation loss: 2.4702255315678094

Epoch: 5| Step: 2
Training loss: 3.165309190750122
Validation loss: 2.4739210785076184

Epoch: 5| Step: 3
Training loss: 2.823620557785034
Validation loss: 2.468629111525833

Epoch: 5| Step: 4
Training loss: 3.4446778297424316
Validation loss: 2.4635376058599

Epoch: 5| Step: 5
Training loss: 2.212388038635254
Validation loss: 2.462305391988447

Epoch: 5| Step: 6
Training loss: 2.603287696838379
Validation loss: 2.4618920818451913

Epoch: 5| Step: 7
Training loss: 2.0233731269836426
Validation loss: 2.4610145425283783

Epoch: 5| Step: 8
Training loss: 2.434338092803955
Validation loss: 2.460433506196545

Epoch: 5| Step: 9
Training loss: 2.145230531692505
Validation loss: 2.463564403595463

Epoch: 5| Step: 10
Training loss: 3.171990156173706
Validation loss: 2.4664259110727618

Epoch: 74| Step: 0
Training loss: 2.0708110332489014
Validation loss: 2.479349918262933

Epoch: 5| Step: 1
Training loss: 3.5061612129211426
Validation loss: 2.5120294350449757

Epoch: 5| Step: 2
Training loss: 2.747648000717163
Validation loss: 2.534977677047894

Epoch: 5| Step: 3
Training loss: 2.399733543395996
Validation loss: 2.518359891829952

Epoch: 5| Step: 4
Training loss: 2.5024447441101074
Validation loss: 2.5011780851630756

Epoch: 5| Step: 5
Training loss: 2.277411937713623
Validation loss: 2.4795647026390157

Epoch: 5| Step: 6
Training loss: 3.3843624591827393
Validation loss: 2.466771384721161

Epoch: 5| Step: 7
Training loss: 2.5765044689178467
Validation loss: 2.4634793727628645

Epoch: 5| Step: 8
Training loss: 2.19718861579895
Validation loss: 2.453395520487139

Epoch: 5| Step: 9
Training loss: 3.349891185760498
Validation loss: 2.453556073609219

Epoch: 5| Step: 10
Training loss: 2.4114792346954346
Validation loss: 2.454130695712182

Epoch: 75| Step: 0
Training loss: 2.532214403152466
Validation loss: 2.4529397744004444

Epoch: 5| Step: 1
Training loss: 2.903557538986206
Validation loss: 2.4546107297302573

Epoch: 5| Step: 2
Training loss: 2.7046356201171875
Validation loss: 2.458865245183309

Epoch: 5| Step: 3
Training loss: 3.228835344314575
Validation loss: 2.4565553280615036

Epoch: 5| Step: 4
Training loss: 2.5768983364105225
Validation loss: 2.456977172564435

Epoch: 5| Step: 5
Training loss: 2.969071865081787
Validation loss: 2.4549607307680192

Epoch: 5| Step: 6
Training loss: 2.6711957454681396
Validation loss: 2.454024120043683

Epoch: 5| Step: 7
Training loss: 2.561694860458374
Validation loss: 2.4514628892303794

Epoch: 5| Step: 8
Training loss: 2.0536158084869385
Validation loss: 2.451113252229588

Epoch: 5| Step: 9
Training loss: 2.203144073486328
Validation loss: 2.4540481823746876

Epoch: 5| Step: 10
Training loss: 3.1378121376037598
Validation loss: 2.4549752871195474

Epoch: 76| Step: 0
Training loss: 3.717681407928467
Validation loss: 2.4636300533048567

Epoch: 5| Step: 1
Training loss: 2.685168504714966
Validation loss: 2.4662361132201327

Epoch: 5| Step: 2
Training loss: 2.8791568279266357
Validation loss: 2.473148574111282

Epoch: 5| Step: 3
Training loss: 2.6535770893096924
Validation loss: 2.466137542519518

Epoch: 5| Step: 4
Training loss: 2.8408210277557373
Validation loss: 2.4673192731795774

Epoch: 5| Step: 5
Training loss: 2.473898410797119
Validation loss: 2.46108784855053

Epoch: 5| Step: 6
Training loss: 2.8740150928497314
Validation loss: 2.45447140355264

Epoch: 5| Step: 7
Training loss: 2.4657015800476074
Validation loss: 2.4487244852127565

Epoch: 5| Step: 8
Training loss: 2.789482593536377
Validation loss: 2.446069158533568

Epoch: 5| Step: 9
Training loss: 1.970489263534546
Validation loss: 2.444236527207077

Epoch: 5| Step: 10
Training loss: 1.845292329788208
Validation loss: 2.4427613340398318

Epoch: 77| Step: 0
Training loss: 2.9229466915130615
Validation loss: 2.4503265350095687

Epoch: 5| Step: 1
Training loss: 2.3754043579101562
Validation loss: 2.4585396423134753

Epoch: 5| Step: 2
Training loss: 2.746458053588867
Validation loss: 2.4644672896272395

Epoch: 5| Step: 3
Training loss: 2.4947381019592285
Validation loss: 2.459189473941762

Epoch: 5| Step: 4
Training loss: 2.7659270763397217
Validation loss: 2.4508512814839682

Epoch: 5| Step: 5
Training loss: 2.668870449066162
Validation loss: 2.457009618000318

Epoch: 5| Step: 6
Training loss: 2.3389840126037598
Validation loss: 2.4617437931799118

Epoch: 5| Step: 7
Training loss: 2.384840726852417
Validation loss: 2.4648913824430077

Epoch: 5| Step: 8
Training loss: 2.6167123317718506
Validation loss: 2.4689416834103164

Epoch: 5| Step: 9
Training loss: 2.556762218475342
Validation loss: 2.4606014605491393

Epoch: 5| Step: 10
Training loss: 3.642153024673462
Validation loss: 2.4528541590577815

Epoch: 78| Step: 0
Training loss: 2.5554442405700684
Validation loss: 2.4495976253222396

Epoch: 5| Step: 1
Training loss: 2.4538567066192627
Validation loss: 2.447484298418927

Epoch: 5| Step: 2
Training loss: 2.4645137786865234
Validation loss: 2.442503342064478

Epoch: 5| Step: 3
Training loss: 3.303917646408081
Validation loss: 2.4391254763449393

Epoch: 5| Step: 4
Training loss: 2.2173547744750977
Validation loss: 2.43971303842401

Epoch: 5| Step: 5
Training loss: 3.256293535232544
Validation loss: 2.437704655431932

Epoch: 5| Step: 6
Training loss: 2.9567198753356934
Validation loss: 2.4380557203805573

Epoch: 5| Step: 7
Training loss: 2.598888635635376
Validation loss: 2.4388486518654773

Epoch: 5| Step: 8
Training loss: 2.47996187210083
Validation loss: 2.4403982521385275

Epoch: 5| Step: 9
Training loss: 2.8008389472961426
Validation loss: 2.441522095793037

Epoch: 5| Step: 10
Training loss: 2.1043105125427246
Validation loss: 2.446884121946109

Epoch: 79| Step: 0
Training loss: 2.554867744445801
Validation loss: 2.4460109126183296

Epoch: 5| Step: 1
Training loss: 2.462953567504883
Validation loss: 2.4466487002629105

Epoch: 5| Step: 2
Training loss: 2.385209560394287
Validation loss: 2.4504288293982066

Epoch: 5| Step: 3
Training loss: 3.0072312355041504
Validation loss: 2.4640760729389806

Epoch: 5| Step: 4
Training loss: 2.4285573959350586
Validation loss: 2.4530738656238844

Epoch: 5| Step: 5
Training loss: 2.839122772216797
Validation loss: 2.45058613182396

Epoch: 5| Step: 6
Training loss: 2.4547181129455566
Validation loss: 2.4589016668258177

Epoch: 5| Step: 7
Training loss: 3.1708407402038574
Validation loss: 2.452701812149376

Epoch: 5| Step: 8
Training loss: 2.300022602081299
Validation loss: 2.467424543954993

Epoch: 5| Step: 9
Training loss: 3.1579935550689697
Validation loss: 2.465815936365435

Epoch: 5| Step: 10
Training loss: 2.533487319946289
Validation loss: 2.44070343817434

Epoch: 80| Step: 0
Training loss: 2.382678508758545
Validation loss: 2.4604288326796664

Epoch: 5| Step: 1
Training loss: 2.7346982955932617
Validation loss: 2.491988351268153

Epoch: 5| Step: 2
Training loss: 1.9180752038955688
Validation loss: 2.4739097292705248

Epoch: 5| Step: 3
Training loss: 2.5786561965942383
Validation loss: 2.4738151514402

Epoch: 5| Step: 4
Training loss: 2.699556827545166
Validation loss: 2.4302344834932716

Epoch: 5| Step: 5
Training loss: 2.603607177734375
Validation loss: 2.429263743021155

Epoch: 5| Step: 6
Training loss: 2.8652923107147217
Validation loss: 2.430996405181064

Epoch: 5| Step: 7
Training loss: 3.2834174633026123
Validation loss: 2.4337470659645657

Epoch: 5| Step: 8
Training loss: 2.3881113529205322
Validation loss: 2.4370310768004386

Epoch: 5| Step: 9
Training loss: 2.762493133544922
Validation loss: 2.4365348764645156

Epoch: 5| Step: 10
Training loss: 3.0745296478271484
Validation loss: 2.4355805535470285

Epoch: 81| Step: 0
Training loss: 2.6643450260162354
Validation loss: 2.4287013520476637

Epoch: 5| Step: 1
Training loss: 2.836730480194092
Validation loss: 2.4297753918555474

Epoch: 5| Step: 2
Training loss: 2.4864590167999268
Validation loss: 2.42520659969699

Epoch: 5| Step: 3
Training loss: 2.8855490684509277
Validation loss: 2.4273071032698437

Epoch: 5| Step: 4
Training loss: 3.25964617729187
Validation loss: 2.43363219435497

Epoch: 5| Step: 5
Training loss: 2.298459768295288
Validation loss: 2.43166370032936

Epoch: 5| Step: 6
Training loss: 2.496778964996338
Validation loss: 2.4311391717644146

Epoch: 5| Step: 7
Training loss: 2.558102607727051
Validation loss: 2.4341708383252545

Epoch: 5| Step: 8
Training loss: 2.111485242843628
Validation loss: 2.437626836120441

Epoch: 5| Step: 9
Training loss: 2.9879612922668457
Validation loss: 2.447458754303635

Epoch: 5| Step: 10
Training loss: 2.6463184356689453
Validation loss: 2.460600140274212

Epoch: 82| Step: 0
Training loss: 2.6678543090820312
Validation loss: 2.4627274672190347

Epoch: 5| Step: 1
Training loss: 2.6647584438323975
Validation loss: 2.459892944623065

Epoch: 5| Step: 2
Training loss: 2.1242847442626953
Validation loss: 2.437108075746926

Epoch: 5| Step: 3
Training loss: 2.982766628265381
Validation loss: 2.426835254956317

Epoch: 5| Step: 4
Training loss: 2.448687791824341
Validation loss: 2.4155715678327825

Epoch: 5| Step: 5
Training loss: 2.901768445968628
Validation loss: 2.412548339495095

Epoch: 5| Step: 6
Training loss: 2.9254937171936035
Validation loss: 2.4139002600023822

Epoch: 5| Step: 7
Training loss: 3.280425548553467
Validation loss: 2.421561207822574

Epoch: 5| Step: 8
Training loss: 2.176703691482544
Validation loss: 2.4202684792139197

Epoch: 5| Step: 9
Training loss: 2.1608059406280518
Validation loss: 2.418289489643548

Epoch: 5| Step: 10
Training loss: 3.000976800918579
Validation loss: 2.422512513335033

Epoch: 83| Step: 0
Training loss: 2.3240435123443604
Validation loss: 2.4241684918762534

Epoch: 5| Step: 1
Training loss: 2.4025399684906006
Validation loss: 2.428587726367417

Epoch: 5| Step: 2
Training loss: 3.037423610687256
Validation loss: 2.4301622042091946

Epoch: 5| Step: 3
Training loss: 2.001289129257202
Validation loss: 2.4263859948804303

Epoch: 5| Step: 4
Training loss: 3.0299410820007324
Validation loss: 2.4384227542467016

Epoch: 5| Step: 5
Training loss: 2.430649757385254
Validation loss: 2.43813060175988

Epoch: 5| Step: 6
Training loss: 2.7583231925964355
Validation loss: 2.437491511785856

Epoch: 5| Step: 7
Training loss: 2.855088949203491
Validation loss: 2.432625339877221

Epoch: 5| Step: 8
Training loss: 2.624530792236328
Validation loss: 2.432220379511515

Epoch: 5| Step: 9
Training loss: 2.9163994789123535
Validation loss: 2.4266034428791334

Epoch: 5| Step: 10
Training loss: 2.8776988983154297
Validation loss: 2.4261024587897846

Epoch: 84| Step: 0
Training loss: 2.718848705291748
Validation loss: 2.4172643922990367

Epoch: 5| Step: 1
Training loss: 2.339761972427368
Validation loss: 2.4170259814108572

Epoch: 5| Step: 2
Training loss: 2.7379679679870605
Validation loss: 2.4171937319540207

Epoch: 5| Step: 3
Training loss: 2.191779613494873
Validation loss: 2.4177909512673654

Epoch: 5| Step: 4
Training loss: 2.854147434234619
Validation loss: 2.415163460598197

Epoch: 5| Step: 5
Training loss: 2.89855694770813
Validation loss: 2.4168575040755735

Epoch: 5| Step: 6
Training loss: 2.7411928176879883
Validation loss: 2.41265102612075

Epoch: 5| Step: 7
Training loss: 2.1842129230499268
Validation loss: 2.4065408040118474

Epoch: 5| Step: 8
Training loss: 2.7345306873321533
Validation loss: 2.4035622971032256

Epoch: 5| Step: 9
Training loss: 2.6907272338867188
Validation loss: 2.4054243128786803

Epoch: 5| Step: 10
Training loss: 3.047956705093384
Validation loss: 2.401390293593048

Epoch: 85| Step: 0
Training loss: 2.545806407928467
Validation loss: 2.3997943350063857

Epoch: 5| Step: 1
Training loss: 3.2810802459716797
Validation loss: 2.398362740393608

Epoch: 5| Step: 2
Training loss: 2.5306015014648438
Validation loss: 2.397184310420867

Epoch: 5| Step: 3
Training loss: 1.9445202350616455
Validation loss: 2.395951614584974

Epoch: 5| Step: 4
Training loss: 2.7820355892181396
Validation loss: 2.3984981172828266

Epoch: 5| Step: 5
Training loss: 3.168670892715454
Validation loss: 2.3964029537734164

Epoch: 5| Step: 6
Training loss: 2.415724039077759
Validation loss: 2.397880461908156

Epoch: 5| Step: 7
Training loss: 2.1541199684143066
Validation loss: 2.3966928246200725

Epoch: 5| Step: 8
Training loss: 2.186824321746826
Validation loss: 2.3965441052631666

Epoch: 5| Step: 9
Training loss: 3.5818068981170654
Validation loss: 2.400803194251112

Epoch: 5| Step: 10
Training loss: 2.426131248474121
Validation loss: 2.4029741851232385

Epoch: 86| Step: 0
Training loss: 3.360380172729492
Validation loss: 2.418226424083915

Epoch: 5| Step: 1
Training loss: 2.588287353515625
Validation loss: 2.4248768462929675

Epoch: 5| Step: 2
Training loss: 2.4207851886749268
Validation loss: 2.4402788082758584

Epoch: 5| Step: 3
Training loss: 2.1133503913879395
Validation loss: 2.456380162187802

Epoch: 5| Step: 4
Training loss: 2.21125864982605
Validation loss: 2.4436175105392293

Epoch: 5| Step: 5
Training loss: 2.752925395965576
Validation loss: 2.450920940727316

Epoch: 5| Step: 6
Training loss: 2.595737934112549
Validation loss: 2.4465150294765348

Epoch: 5| Step: 7
Training loss: 3.093132495880127
Validation loss: 2.4325722571342223

Epoch: 5| Step: 8
Training loss: 2.950601577758789
Validation loss: 2.415904034850418

Epoch: 5| Step: 9
Training loss: 2.4208786487579346
Validation loss: 2.4080534840142853

Epoch: 5| Step: 10
Training loss: 2.545254707336426
Validation loss: 2.3967292206261748

Epoch: 87| Step: 0
Training loss: 2.288247585296631
Validation loss: 2.390514531443196

Epoch: 5| Step: 1
Training loss: 2.1736490726470947
Validation loss: 2.3927276647219093

Epoch: 5| Step: 2
Training loss: 3.4668705463409424
Validation loss: 2.3923010723565215

Epoch: 5| Step: 3
Training loss: 2.8530948162078857
Validation loss: 2.390808295178157

Epoch: 5| Step: 4
Training loss: 2.476900577545166
Validation loss: 2.3905062111475135

Epoch: 5| Step: 5
Training loss: 2.4624714851379395
Validation loss: 2.392046933533043

Epoch: 5| Step: 6
Training loss: 2.325754165649414
Validation loss: 2.3870093335387526

Epoch: 5| Step: 7
Training loss: 2.319526433944702
Validation loss: 2.3905451451578448

Epoch: 5| Step: 8
Training loss: 2.6426308155059814
Validation loss: 2.3943424455581175

Epoch: 5| Step: 9
Training loss: 2.8906683921813965
Validation loss: 2.3891983288590626

Epoch: 5| Step: 10
Training loss: 3.177631139755249
Validation loss: 2.3921536835291053

Epoch: 88| Step: 0
Training loss: 2.3194644451141357
Validation loss: 2.3891520448910293

Epoch: 5| Step: 1
Training loss: 2.446030378341675
Validation loss: 2.393174768776022

Epoch: 5| Step: 2
Training loss: 2.64693546295166
Validation loss: 2.3937541925778953

Epoch: 5| Step: 3
Training loss: 2.599309206008911
Validation loss: 2.4002780222123667

Epoch: 5| Step: 4
Training loss: 2.9308981895446777
Validation loss: 2.3998383373342533

Epoch: 5| Step: 5
Training loss: 2.3765978813171387
Validation loss: 2.399761753697549

Epoch: 5| Step: 6
Training loss: 2.5561153888702393
Validation loss: 2.3985063722056728

Epoch: 5| Step: 7
Training loss: 2.8707802295684814
Validation loss: 2.405777326194189

Epoch: 5| Step: 8
Training loss: 2.904003620147705
Validation loss: 2.4077126800373034

Epoch: 5| Step: 9
Training loss: 2.506288766860962
Validation loss: 2.4049859662209787

Epoch: 5| Step: 10
Training loss: 2.7838878631591797
Validation loss: 2.392082118218945

Epoch: 89| Step: 0
Training loss: 2.951838731765747
Validation loss: 2.3866224442758868

Epoch: 5| Step: 1
Training loss: 1.615471601486206
Validation loss: 2.389545025364045

Epoch: 5| Step: 2
Training loss: 2.9539670944213867
Validation loss: 2.386332445247199

Epoch: 5| Step: 3
Training loss: 3.2159969806671143
Validation loss: 2.3855403161817983

Epoch: 5| Step: 4
Training loss: 2.1621837615966797
Validation loss: 2.3770177518167803

Epoch: 5| Step: 5
Training loss: 2.8334364891052246
Validation loss: 2.381315879924323

Epoch: 5| Step: 6
Training loss: 2.122678279876709
Validation loss: 2.3765004014456146

Epoch: 5| Step: 7
Training loss: 2.5591678619384766
Validation loss: 2.3744128468216106

Epoch: 5| Step: 8
Training loss: 2.6136527061462402
Validation loss: 2.37369107174617

Epoch: 5| Step: 9
Training loss: 2.826178789138794
Validation loss: 2.372265008188063

Epoch: 5| Step: 10
Training loss: 3.0495853424072266
Validation loss: 2.3708314536720194

Epoch: 90| Step: 0
Training loss: 2.378331184387207
Validation loss: 2.3714105595824537

Epoch: 5| Step: 1
Training loss: 3.196434497833252
Validation loss: 2.3709844619997087

Epoch: 5| Step: 2
Training loss: 3.3853440284729004
Validation loss: 2.3749870613057125

Epoch: 5| Step: 3
Training loss: 1.8152844905853271
Validation loss: 2.3700189128998788

Epoch: 5| Step: 4
Training loss: 2.150270938873291
Validation loss: 2.3656003141915924

Epoch: 5| Step: 5
Training loss: 2.641587734222412
Validation loss: 2.370852824180357

Epoch: 5| Step: 6
Training loss: 2.403912305831909
Validation loss: 2.3729396302212953

Epoch: 5| Step: 7
Training loss: 3.1321418285369873
Validation loss: 2.377473077466411

Epoch: 5| Step: 8
Training loss: 2.989966630935669
Validation loss: 2.3951508857870616

Epoch: 5| Step: 9
Training loss: 2.4316306114196777
Validation loss: 2.408622231534732

Epoch: 5| Step: 10
Training loss: 2.2994449138641357
Validation loss: 2.426084667123774

Epoch: 91| Step: 0
Training loss: 3.2870841026306152
Validation loss: 2.445158379052275

Epoch: 5| Step: 1
Training loss: 2.9232470989227295
Validation loss: 2.4371592460140103

Epoch: 5| Step: 2
Training loss: 1.9104363918304443
Validation loss: 2.409804081404081

Epoch: 5| Step: 3
Training loss: 3.0603232383728027
Validation loss: 2.388134951232582

Epoch: 5| Step: 4
Training loss: 2.917956829071045
Validation loss: 2.3653674433308263

Epoch: 5| Step: 5
Training loss: 2.5570249557495117
Validation loss: 2.3557234169334493

Epoch: 5| Step: 6
Training loss: 2.0601108074188232
Validation loss: 2.3582272785966114

Epoch: 5| Step: 7
Training loss: 2.6817479133605957
Validation loss: 2.361371865836523

Epoch: 5| Step: 8
Training loss: 2.946807384490967
Validation loss: 2.356920244873211

Epoch: 5| Step: 9
Training loss: 1.4423601627349854
Validation loss: 2.3564022997374177

Epoch: 5| Step: 10
Training loss: 3.3256688117980957
Validation loss: 2.352789810908738

Epoch: 92| Step: 0
Training loss: 2.5269899368286133
Validation loss: 2.352068039678758

Epoch: 5| Step: 1
Training loss: 1.7988369464874268
Validation loss: 2.352353508754443

Epoch: 5| Step: 2
Training loss: 2.1368534564971924
Validation loss: 2.3657604161129204

Epoch: 5| Step: 3
Training loss: 3.013766050338745
Validation loss: 2.376110480677697

Epoch: 5| Step: 4
Training loss: 2.8021514415740967
Validation loss: 2.3731243738564114

Epoch: 5| Step: 5
Training loss: 3.595698118209839
Validation loss: 2.3676279001338507

Epoch: 5| Step: 6
Training loss: 2.310173273086548
Validation loss: 2.364994846364503

Epoch: 5| Step: 7
Training loss: 2.819535732269287
Validation loss: 2.35926825769486

Epoch: 5| Step: 8
Training loss: 3.0783915519714355
Validation loss: 2.354643311551822

Epoch: 5| Step: 9
Training loss: 1.6864521503448486
Validation loss: 2.3502872618295814

Epoch: 5| Step: 10
Training loss: 2.9960098266601562
Validation loss: 2.3498876581909838

Epoch: 93| Step: 0
Training loss: 3.3817219734191895
Validation loss: 2.3476653560515373

Epoch: 5| Step: 1
Training loss: 3.2648041248321533
Validation loss: 2.3439294292080786

Epoch: 5| Step: 2
Training loss: 2.17877197265625
Validation loss: 2.343057373518585

Epoch: 5| Step: 3
Training loss: 2.392906427383423
Validation loss: 2.3508288039956042

Epoch: 5| Step: 4
Training loss: 2.2267446517944336
Validation loss: 2.3415188507367204

Epoch: 5| Step: 5
Training loss: 1.8155405521392822
Validation loss: 2.3438143640436153

Epoch: 5| Step: 6
Training loss: 3.002140522003174
Validation loss: 2.3431729347475114

Epoch: 5| Step: 7
Training loss: 2.722991704940796
Validation loss: 2.339639763678274

Epoch: 5| Step: 8
Training loss: 2.952322006225586
Validation loss: 2.341288766553325

Epoch: 5| Step: 9
Training loss: 2.6146252155303955
Validation loss: 2.3415662421975085

Epoch: 5| Step: 10
Training loss: 2.0176985263824463
Validation loss: 2.34262571027202

Epoch: 94| Step: 0
Training loss: 3.3010735511779785
Validation loss: 2.3415173458796676

Epoch: 5| Step: 1
Training loss: 2.183671236038208
Validation loss: 2.340213606434484

Epoch: 5| Step: 2
Training loss: 2.0100064277648926
Validation loss: 2.340047405612084

Epoch: 5| Step: 3
Training loss: 2.6136372089385986
Validation loss: 2.3449743076037337

Epoch: 5| Step: 4
Training loss: 2.05725359916687
Validation loss: 2.34421379335465

Epoch: 5| Step: 5
Training loss: 2.9355721473693848
Validation loss: 2.36474044604968

Epoch: 5| Step: 6
Training loss: 2.181959629058838
Validation loss: 2.388079140775947

Epoch: 5| Step: 7
Training loss: 2.995318651199341
Validation loss: 2.3899036658707487

Epoch: 5| Step: 8
Training loss: 2.7565388679504395
Validation loss: 2.3777343534654185

Epoch: 5| Step: 9
Training loss: 2.8516030311584473
Validation loss: 2.3568998306028304

Epoch: 5| Step: 10
Training loss: 2.934243679046631
Validation loss: 2.3427610576793714

Epoch: 95| Step: 0
Training loss: 2.3124356269836426
Validation loss: 2.338754371930194

Epoch: 5| Step: 1
Training loss: 2.7976150512695312
Validation loss: 2.339312130405057

Epoch: 5| Step: 2
Training loss: 2.8831489086151123
Validation loss: 2.3414926170020975

Epoch: 5| Step: 3
Training loss: 2.5080080032348633
Validation loss: 2.347004355922822

Epoch: 5| Step: 4
Training loss: 2.682650566101074
Validation loss: 2.3484754254741054

Epoch: 5| Step: 5
Training loss: 2.473707675933838
Validation loss: 2.3437449803916355

Epoch: 5| Step: 6
Training loss: 2.6840362548828125
Validation loss: 2.342576567844678

Epoch: 5| Step: 7
Training loss: 2.4341228008270264
Validation loss: 2.3440307365950717

Epoch: 5| Step: 8
Training loss: 2.2329211235046387
Validation loss: 2.34056689405954

Epoch: 5| Step: 9
Training loss: 2.843013048171997
Validation loss: 2.344114085679413

Epoch: 5| Step: 10
Training loss: 2.8110711574554443
Validation loss: 2.3437872407256917

Epoch: 96| Step: 0
Training loss: 3.0887811183929443
Validation loss: 2.3461687385395007

Epoch: 5| Step: 1
Training loss: 2.152160406112671
Validation loss: 2.3567858280674105

Epoch: 5| Step: 2
Training loss: 2.751934766769409
Validation loss: 2.3514451801135974

Epoch: 5| Step: 3
Training loss: 2.963369846343994
Validation loss: 2.358155217221988

Epoch: 5| Step: 4
Training loss: 3.381633758544922
Validation loss: 2.363867505904167

Epoch: 5| Step: 5
Training loss: 1.8775240182876587
Validation loss: 2.354863812846522

Epoch: 5| Step: 6
Training loss: 2.1445298194885254
Validation loss: 2.3444619153135564

Epoch: 5| Step: 7
Training loss: 2.3128209114074707
Validation loss: 2.3432398560226604

Epoch: 5| Step: 8
Training loss: 2.7371761798858643
Validation loss: 2.342514281631798

Epoch: 5| Step: 9
Training loss: 2.412008047103882
Validation loss: 2.336971575214017

Epoch: 5| Step: 10
Training loss: 2.7094640731811523
Validation loss: 2.340030139492404

Epoch: 97| Step: 0
Training loss: 3.2982494831085205
Validation loss: 2.347534361705985

Epoch: 5| Step: 1
Training loss: 2.377791404724121
Validation loss: 2.3568630167233047

Epoch: 5| Step: 2
Training loss: 2.9042105674743652
Validation loss: 2.390001166251398

Epoch: 5| Step: 3
Training loss: 2.6494059562683105
Validation loss: 2.4016585683309906

Epoch: 5| Step: 4
Training loss: 3.4427649974823
Validation loss: 2.384192823081888

Epoch: 5| Step: 5
Training loss: 2.7312052249908447
Validation loss: 2.355147715537779

Epoch: 5| Step: 6
Training loss: 2.3749148845672607
Validation loss: 2.3359825072749967

Epoch: 5| Step: 7
Training loss: 2.3385961055755615
Validation loss: 2.3269109790043165

Epoch: 5| Step: 8
Training loss: 2.2585270404815674
Validation loss: 2.3383938343294206

Epoch: 5| Step: 9
Training loss: 1.9007787704467773
Validation loss: 2.343663771947225

Epoch: 5| Step: 10
Training loss: 2.4122982025146484
Validation loss: 2.3606228751520955

Epoch: 98| Step: 0
Training loss: 2.6112143993377686
Validation loss: 2.363604922448435

Epoch: 5| Step: 1
Training loss: 3.1798017024993896
Validation loss: 2.379431447675151

Epoch: 5| Step: 2
Training loss: 2.268251419067383
Validation loss: 2.4179443825957594

Epoch: 5| Step: 3
Training loss: 2.9415717124938965
Validation loss: 2.4728086122902493

Epoch: 5| Step: 4
Training loss: 2.533107042312622
Validation loss: 2.4558681749528453

Epoch: 5| Step: 5
Training loss: 2.7942962646484375
Validation loss: 2.3957196871439614

Epoch: 5| Step: 6
Training loss: 2.100844144821167
Validation loss: 2.3770342437169885

Epoch: 5| Step: 7
Training loss: 2.8141722679138184
Validation loss: 2.3715096827476256

Epoch: 5| Step: 8
Training loss: 2.9726216793060303
Validation loss: 2.3903742374912387

Epoch: 5| Step: 9
Training loss: 2.241858720779419
Validation loss: 2.402115265528361

Epoch: 5| Step: 10
Training loss: 2.4388248920440674
Validation loss: 2.3949264275130404

Epoch: 99| Step: 0
Training loss: 2.394519090652466
Validation loss: 2.381884031398322

Epoch: 5| Step: 1
Training loss: 2.7153570652008057
Validation loss: 2.384036453821326

Epoch: 5| Step: 2
Training loss: 2.380593776702881
Validation loss: 2.4298368089942524

Epoch: 5| Step: 3
Training loss: 3.1701345443725586
Validation loss: 2.4251430906275266

Epoch: 5| Step: 4
Training loss: 1.8734047412872314
Validation loss: 2.395667673439108

Epoch: 5| Step: 5
Training loss: 2.3607261180877686
Validation loss: 2.323605743787622

Epoch: 5| Step: 6
Training loss: 2.654130458831787
Validation loss: 2.3149030721315773

Epoch: 5| Step: 7
Training loss: 2.6837613582611084
Validation loss: 2.315867263783691

Epoch: 5| Step: 8
Training loss: 2.6301140785217285
Validation loss: 2.3134026232586113

Epoch: 5| Step: 9
Training loss: 3.0866024494171143
Validation loss: 2.3134854711512083

Epoch: 5| Step: 10
Training loss: 2.6892831325531006
Validation loss: 2.312092997694528

Epoch: 100| Step: 0
Training loss: 2.8241829872131348
Validation loss: 2.3113438083279516

Epoch: 5| Step: 1
Training loss: 2.5224831104278564
Validation loss: 2.3207382591821815

Epoch: 5| Step: 2
Training loss: 2.107924699783325
Validation loss: 2.334579690810173

Epoch: 5| Step: 3
Training loss: 2.4674837589263916
Validation loss: 2.35464318080615

Epoch: 5| Step: 4
Training loss: 2.499940872192383
Validation loss: 2.4165691432132514

Epoch: 5| Step: 5
Training loss: 3.027705669403076
Validation loss: 2.4164034371734946

Epoch: 5| Step: 6
Training loss: 2.272153377532959
Validation loss: 2.376155653307515

Epoch: 5| Step: 7
Training loss: 2.5063209533691406
Validation loss: 2.341516092259397

Epoch: 5| Step: 8
Training loss: 2.9511122703552246
Validation loss: 2.3302299694348405

Epoch: 5| Step: 9
Training loss: 2.424163818359375
Validation loss: 2.312812664175546

Epoch: 5| Step: 10
Training loss: 2.98707914352417
Validation loss: 2.310871101194812

Epoch: 101| Step: 0
Training loss: 2.4486122131347656
Validation loss: 2.3090216882767214

Epoch: 5| Step: 1
Training loss: 3.016782283782959
Validation loss: 2.3104449408028715

Epoch: 5| Step: 2
Training loss: 2.6050872802734375
Validation loss: 2.314294740717898

Epoch: 5| Step: 3
Training loss: 2.2577216625213623
Validation loss: 2.3087947753167923

Epoch: 5| Step: 4
Training loss: 2.3749988079071045
Validation loss: 2.317165244010187

Epoch: 5| Step: 5
Training loss: 2.899141311645508
Validation loss: 2.3140596907625914

Epoch: 5| Step: 6
Training loss: 2.423395872116089
Validation loss: 2.3105293909708657

Epoch: 5| Step: 7
Training loss: 2.1831374168395996
Validation loss: 2.3155054943535918

Epoch: 5| Step: 8
Training loss: 3.118427038192749
Validation loss: 2.3166878351601223

Epoch: 5| Step: 9
Training loss: 2.1890227794647217
Validation loss: 2.3110706780546453

Epoch: 5| Step: 10
Training loss: 2.8307018280029297
Validation loss: 2.313072774999885

Epoch: 102| Step: 0
Training loss: 2.539118528366089
Validation loss: 2.3293389966410976

Epoch: 5| Step: 1
Training loss: 2.845712661743164
Validation loss: 2.327884071616716

Epoch: 5| Step: 2
Training loss: 2.643359899520874
Validation loss: 2.343630488200854

Epoch: 5| Step: 3
Training loss: 2.5657401084899902
Validation loss: 2.333764265942317

Epoch: 5| Step: 4
Training loss: 2.398237705230713
Validation loss: 2.3379925425334642

Epoch: 5| Step: 5
Training loss: 2.4583182334899902
Validation loss: 2.327719206451088

Epoch: 5| Step: 6
Training loss: 2.8144917488098145
Validation loss: 2.31655982489227

Epoch: 5| Step: 7
Training loss: 2.200028896331787
Validation loss: 2.314240014681252

Epoch: 5| Step: 8
Training loss: 2.257483959197998
Validation loss: 2.3114337780142344

Epoch: 5| Step: 9
Training loss: 2.6501097679138184
Validation loss: 2.309481205478791

Epoch: 5| Step: 10
Training loss: 2.9263672828674316
Validation loss: 2.311011104173558

Epoch: 103| Step: 0
Training loss: 2.866882801055908
Validation loss: 2.3172835457709526

Epoch: 5| Step: 1
Training loss: 2.9637482166290283
Validation loss: 2.3207094284795944

Epoch: 5| Step: 2
Training loss: 2.4150397777557373
Validation loss: 2.3271602020468762

Epoch: 5| Step: 3
Training loss: 2.3903698921203613
Validation loss: 2.336815362335533

Epoch: 5| Step: 4
Training loss: 2.7917513847351074
Validation loss: 2.3285355901205413

Epoch: 5| Step: 5
Training loss: 2.637012481689453
Validation loss: 2.3290696554286505

Epoch: 5| Step: 6
Training loss: 2.2382102012634277
Validation loss: 2.325542929351971

Epoch: 5| Step: 7
Training loss: 2.685387134552002
Validation loss: 2.329572569939398

Epoch: 5| Step: 8
Training loss: 2.5135064125061035
Validation loss: 2.3252766721992084

Epoch: 5| Step: 9
Training loss: 3.0825774669647217
Validation loss: 2.333806589085569

Epoch: 5| Step: 10
Training loss: 1.7475069761276245
Validation loss: 2.324298648424046

Epoch: 104| Step: 0
Training loss: 2.4256420135498047
Validation loss: 2.331704283273348

Epoch: 5| Step: 1
Training loss: 2.884782075881958
Validation loss: 2.335740085571043

Epoch: 5| Step: 2
Training loss: 2.3865909576416016
Validation loss: 2.3285877114983013

Epoch: 5| Step: 3
Training loss: 1.9164097309112549
Validation loss: 2.3235904209075438

Epoch: 5| Step: 4
Training loss: 2.821155548095703
Validation loss: 2.3172345751075336

Epoch: 5| Step: 5
Training loss: 2.5027287006378174
Validation loss: 2.310076995562482

Epoch: 5| Step: 6
Training loss: 2.621790647506714
Validation loss: 2.3166806595299834

Epoch: 5| Step: 7
Training loss: 2.7143075466156006
Validation loss: 2.324558691311908

Epoch: 5| Step: 8
Training loss: 1.8026024103164673
Validation loss: 2.3210497569012385

Epoch: 5| Step: 9
Training loss: 3.195620536804199
Validation loss: 2.3212131787371892

Epoch: 5| Step: 10
Training loss: 2.9336891174316406
Validation loss: 2.3191529038131877

Epoch: 105| Step: 0
Training loss: 2.366527557373047
Validation loss: 2.30894112330611

Epoch: 5| Step: 1
Training loss: 2.277580499649048
Validation loss: 2.2937165485915316

Epoch: 5| Step: 2
Training loss: 2.550760269165039
Validation loss: 2.2857983727608957

Epoch: 5| Step: 3
Training loss: 2.452950954437256
Validation loss: 2.2871545412207164

Epoch: 5| Step: 4
Training loss: 2.5704963207244873
Validation loss: 2.284939089129048

Epoch: 5| Step: 5
Training loss: 2.846022844314575
Validation loss: 2.286083698272705

Epoch: 5| Step: 6
Training loss: 2.3612818717956543
Validation loss: 2.290346037956976

Epoch: 5| Step: 7
Training loss: 2.520155668258667
Validation loss: 2.2912565456923617

Epoch: 5| Step: 8
Training loss: 3.4702041149139404
Validation loss: 2.299552535498014

Epoch: 5| Step: 9
Training loss: 2.0167524814605713
Validation loss: 2.309204593781502

Epoch: 5| Step: 10
Training loss: 2.6794955730438232
Validation loss: 2.305284156594225

Epoch: 106| Step: 0
Training loss: 2.324150562286377
Validation loss: 2.31037147327136

Epoch: 5| Step: 1
Training loss: 3.2753264904022217
Validation loss: 2.329956787888722

Epoch: 5| Step: 2
Training loss: 2.8438146114349365
Validation loss: 2.3330121911982054

Epoch: 5| Step: 3
Training loss: 2.370391607284546
Validation loss: 2.3012169573896673

Epoch: 5| Step: 4
Training loss: 2.17295503616333
Validation loss: 2.285297429689797

Epoch: 5| Step: 5
Training loss: 2.946542739868164
Validation loss: 2.2914784082802395

Epoch: 5| Step: 6
Training loss: 2.1794397830963135
Validation loss: 2.2868408695344002

Epoch: 5| Step: 7
Training loss: 3.060616970062256
Validation loss: 2.2903690184316328

Epoch: 5| Step: 8
Training loss: 2.3115711212158203
Validation loss: 2.291605777637933

Epoch: 5| Step: 9
Training loss: 2.6207661628723145
Validation loss: 2.2950229388411327

Epoch: 5| Step: 10
Training loss: 1.9458167552947998
Validation loss: 2.295124025755031

Epoch: 107| Step: 0
Training loss: 2.5147998332977295
Validation loss: 2.2964596594533613

Epoch: 5| Step: 1
Training loss: 3.070478677749634
Validation loss: 2.2967351534033336

Epoch: 5| Step: 2
Training loss: 2.9141807556152344
Validation loss: 2.297580257538826

Epoch: 5| Step: 3
Training loss: 3.2371392250061035
Validation loss: 2.2922347361041653

Epoch: 5| Step: 4
Training loss: 2.304170608520508
Validation loss: 2.291981209990799

Epoch: 5| Step: 5
Training loss: 2.113295078277588
Validation loss: 2.2928244503595496

Epoch: 5| Step: 6
Training loss: 1.8913081884384155
Validation loss: 2.2956232409323416

Epoch: 5| Step: 7
Training loss: 2.553257703781128
Validation loss: 2.304795144706644

Epoch: 5| Step: 8
Training loss: 2.6642072200775146
Validation loss: 2.3297330051340084

Epoch: 5| Step: 9
Training loss: 2.5616893768310547
Validation loss: 2.3446295261383057

Epoch: 5| Step: 10
Training loss: 2.2737131118774414
Validation loss: 2.352748924686063

Epoch: 108| Step: 0
Training loss: 2.2366185188293457
Validation loss: 2.3345264850124234

Epoch: 5| Step: 1
Training loss: 3.040341854095459
Validation loss: 2.3237231649378294

Epoch: 5| Step: 2
Training loss: 2.617295503616333
Validation loss: 2.310049985044746

Epoch: 5| Step: 3
Training loss: 2.643545627593994
Validation loss: 2.299090193163964

Epoch: 5| Step: 4
Training loss: 2.155738115310669
Validation loss: 2.303021525823942

Epoch: 5| Step: 5
Training loss: 2.6840834617614746
Validation loss: 2.300955810854512

Epoch: 5| Step: 6
Training loss: 2.2540109157562256
Validation loss: 2.3009966368316324

Epoch: 5| Step: 7
Training loss: 3.3387084007263184
Validation loss: 2.297396465014386

Epoch: 5| Step: 8
Training loss: 1.986639380455017
Validation loss: 2.2949559714204524

Epoch: 5| Step: 9
Training loss: 2.5462727546691895
Validation loss: 2.291281154078822

Epoch: 5| Step: 10
Training loss: 2.5850026607513428
Validation loss: 2.2916184830409225

Epoch: 109| Step: 0
Training loss: 2.476104259490967
Validation loss: 2.2910984331561672

Epoch: 5| Step: 1
Training loss: 1.943864107131958
Validation loss: 2.2919702606816448

Epoch: 5| Step: 2
Training loss: 2.9768078327178955
Validation loss: 2.2930592080598236

Epoch: 5| Step: 3
Training loss: 2.411606550216675
Validation loss: 2.2979667058555027

Epoch: 5| Step: 4
Training loss: 2.5980966091156006
Validation loss: 2.318137373975528

Epoch: 5| Step: 5
Training loss: 2.329158067703247
Validation loss: 2.337613195501348

Epoch: 5| Step: 6
Training loss: 2.9855761528015137
Validation loss: 2.3275819619496665

Epoch: 5| Step: 7
Training loss: 2.463592529296875
Validation loss: 2.30801216248543

Epoch: 5| Step: 8
Training loss: 2.7453277111053467
Validation loss: 2.285763386757143

Epoch: 5| Step: 9
Training loss: 2.637892723083496
Validation loss: 2.278066699222852

Epoch: 5| Step: 10
Training loss: 2.4278576374053955
Validation loss: 2.2885870138804116

Epoch: 110| Step: 0
Training loss: 2.3999595642089844
Validation loss: 2.355911836829237

Epoch: 5| Step: 1
Training loss: 2.838347911834717
Validation loss: 2.436921488854193

Epoch: 5| Step: 2
Training loss: 2.22743821144104
Validation loss: 2.3232380959295456

Epoch: 5| Step: 3
Training loss: 1.990715742111206
Validation loss: 2.2881870756867113

Epoch: 5| Step: 4
Training loss: 2.571387767791748
Validation loss: 2.278158428848431

Epoch: 5| Step: 5
Training loss: 3.0754075050354004
Validation loss: 2.2821394089729554

Epoch: 5| Step: 6
Training loss: 2.8503689765930176
Validation loss: 2.2976471839412564

Epoch: 5| Step: 7
Training loss: 1.8041530847549438
Validation loss: 2.2919442012745845

Epoch: 5| Step: 8
Training loss: 2.2250189781188965
Validation loss: 2.3381524906363538

Epoch: 5| Step: 9
Training loss: 3.4446609020233154
Validation loss: 2.3870629290098786

Epoch: 5| Step: 10
Training loss: 2.7823424339294434
Validation loss: 2.3760965742090696

Epoch: 111| Step: 0
Training loss: 1.9670253992080688
Validation loss: 2.3586166289544876

Epoch: 5| Step: 1
Training loss: 2.008772134780884
Validation loss: 2.305815068624353

Epoch: 5| Step: 2
Training loss: 2.4256083965301514
Validation loss: 2.2803989636000765

Epoch: 5| Step: 3
Training loss: 2.665372371673584
Validation loss: 2.2857226402528825

Epoch: 5| Step: 4
Training loss: 2.7912192344665527
Validation loss: 2.324007595739057

Epoch: 5| Step: 5
Training loss: 3.0995030403137207
Validation loss: 2.3263918456210884

Epoch: 5| Step: 6
Training loss: 2.687734603881836
Validation loss: 2.314811719361172

Epoch: 5| Step: 7
Training loss: 3.0067386627197266
Validation loss: 2.352335924743324

Epoch: 5| Step: 8
Training loss: 2.619649887084961
Validation loss: 2.3515514814725487

Epoch: 5| Step: 9
Training loss: 2.6953749656677246
Validation loss: 2.354141846779854

Epoch: 5| Step: 10
Training loss: 2.071005344390869
Validation loss: 2.3350811209729923

Epoch: 112| Step: 0
Training loss: 2.4732418060302734
Validation loss: 2.3176893162470993

Epoch: 5| Step: 1
Training loss: 2.5636675357818604
Validation loss: 2.3198633078605897

Epoch: 5| Step: 2
Training loss: 3.0516555309295654
Validation loss: 2.327899553442514

Epoch: 5| Step: 3
Training loss: 3.2074203491210938
Validation loss: 2.3615977712856826

Epoch: 5| Step: 4
Training loss: 2.4077112674713135
Validation loss: 2.362115942021852

Epoch: 5| Step: 5
Training loss: 2.430471181869507
Validation loss: 2.348311685746716

Epoch: 5| Step: 6
Training loss: 2.340702772140503
Validation loss: 2.3181600211769022

Epoch: 5| Step: 7
Training loss: 2.402782917022705
Validation loss: 2.282457358093672

Epoch: 5| Step: 8
Training loss: 2.1295132637023926
Validation loss: 2.2670056461006083

Epoch: 5| Step: 9
Training loss: 2.533250093460083
Validation loss: 2.262125586950651

Epoch: 5| Step: 10
Training loss: 2.482936143875122
Validation loss: 2.2566285312816663

Epoch: 113| Step: 0
Training loss: 1.8237502574920654
Validation loss: 2.252761860047617

Epoch: 5| Step: 1
Training loss: 2.2708683013916016
Validation loss: 2.2473496314018004

Epoch: 5| Step: 2
Training loss: 2.3714547157287598
Validation loss: 2.2494814216449694

Epoch: 5| Step: 3
Training loss: 2.5359976291656494
Validation loss: 2.2517501487526843

Epoch: 5| Step: 4
Training loss: 3.232682466506958
Validation loss: 2.255098009622225

Epoch: 5| Step: 5
Training loss: 2.5712764263153076
Validation loss: 2.2502388851616972

Epoch: 5| Step: 6
Training loss: 2.89906644821167
Validation loss: 2.252850076203705

Epoch: 5| Step: 7
Training loss: 2.4557244777679443
Validation loss: 2.2493513604646087

Epoch: 5| Step: 8
Training loss: 2.888387441635132
Validation loss: 2.253174768981113

Epoch: 5| Step: 9
Training loss: 2.178915500640869
Validation loss: 2.24790205237686

Epoch: 5| Step: 10
Training loss: 2.4660823345184326
Validation loss: 2.244341386261807

Epoch: 114| Step: 0
Training loss: 2.825511932373047
Validation loss: 2.240415293683288

Epoch: 5| Step: 1
Training loss: 2.0607221126556396
Validation loss: 2.239186509963005

Epoch: 5| Step: 2
Training loss: 2.4784483909606934
Validation loss: 2.235207526914535

Epoch: 5| Step: 3
Training loss: 2.207190990447998
Validation loss: 2.2370733573872554

Epoch: 5| Step: 4
Training loss: 2.5941033363342285
Validation loss: 2.2454025104481685

Epoch: 5| Step: 5
Training loss: 3.048571825027466
Validation loss: 2.2374872879315446

Epoch: 5| Step: 6
Training loss: 2.5283801555633545
Validation loss: 2.2349127736142886

Epoch: 5| Step: 7
Training loss: 2.203824281692505
Validation loss: 2.2292523922458773

Epoch: 5| Step: 8
Training loss: 2.243602752685547
Validation loss: 2.230787346439977

Epoch: 5| Step: 9
Training loss: 2.604796886444092
Validation loss: 2.229115216962753

Epoch: 5| Step: 10
Training loss: 2.855630397796631
Validation loss: 2.2355407668698217

Epoch: 115| Step: 0
Training loss: 2.5993285179138184
Validation loss: 2.2359896449632544

Epoch: 5| Step: 1
Training loss: 3.249073028564453
Validation loss: 2.229828731988066

Epoch: 5| Step: 2
Training loss: 2.9528520107269287
Validation loss: 2.237773741445234

Epoch: 5| Step: 3
Training loss: 2.0568127632141113
Validation loss: 2.233860736252159

Epoch: 5| Step: 4
Training loss: 2.4649693965911865
Validation loss: 2.2322164427849556

Epoch: 5| Step: 5
Training loss: 1.8105392456054688
Validation loss: 2.2321905025871853

Epoch: 5| Step: 6
Training loss: 2.5808119773864746
Validation loss: 2.223067911722327

Epoch: 5| Step: 7
Training loss: 2.109147548675537
Validation loss: 2.2279112723565873

Epoch: 5| Step: 8
Training loss: 2.625321865081787
Validation loss: 2.2292371437113774

Epoch: 5| Step: 9
Training loss: 2.4704413414001465
Validation loss: 2.2262199207018782

Epoch: 5| Step: 10
Training loss: 2.595405101776123
Validation loss: 2.2240264300377137

Epoch: 116| Step: 0
Training loss: 2.5164237022399902
Validation loss: 2.2243567846154653

Epoch: 5| Step: 1
Training loss: 2.2186062335968018
Validation loss: 2.221110889988561

Epoch: 5| Step: 2
Training loss: 2.1910507678985596
Validation loss: 2.2218445859929568

Epoch: 5| Step: 3
Training loss: 2.030036211013794
Validation loss: 2.2311698775137625

Epoch: 5| Step: 4
Training loss: 2.5054268836975098
Validation loss: 2.249949014315041

Epoch: 5| Step: 5
Training loss: 3.8604836463928223
Validation loss: 2.2520165904875724

Epoch: 5| Step: 6
Training loss: 2.302794933319092
Validation loss: 2.2372507305555445

Epoch: 5| Step: 7
Training loss: 2.091456890106201
Validation loss: 2.2356566511174685

Epoch: 5| Step: 8
Training loss: 2.8919363021850586
Validation loss: 2.2364493364928872

Epoch: 5| Step: 9
Training loss: 2.5218913555145264
Validation loss: 2.2317924243147655

Epoch: 5| Step: 10
Training loss: 2.316680669784546
Validation loss: 2.2240792500075472

Epoch: 117| Step: 0
Training loss: 2.27030873298645
Validation loss: 2.2259979042955624

Epoch: 5| Step: 1
Training loss: 2.5607540607452393
Validation loss: 2.2201606406960437

Epoch: 5| Step: 2
Training loss: 2.5780863761901855
Validation loss: 2.2235213582233717

Epoch: 5| Step: 3
Training loss: 2.199932813644409
Validation loss: 2.2315223729738625

Epoch: 5| Step: 4
Training loss: 2.0645344257354736
Validation loss: 2.2306317667807303

Epoch: 5| Step: 5
Training loss: 2.2520625591278076
Validation loss: 2.232406193210233

Epoch: 5| Step: 6
Training loss: 2.650892734527588
Validation loss: 2.232212861378988

Epoch: 5| Step: 7
Training loss: 2.759138822555542
Validation loss: 2.2354251659044655

Epoch: 5| Step: 8
Training loss: 3.1549253463745117
Validation loss: 2.2345662732278146

Epoch: 5| Step: 9
Training loss: 2.619163990020752
Validation loss: 2.231442718095677

Epoch: 5| Step: 10
Training loss: 2.2427515983581543
Validation loss: 2.2305458950740036

Epoch: 118| Step: 0
Training loss: 1.9386497735977173
Validation loss: 2.2287409690118607

Epoch: 5| Step: 1
Training loss: 2.516605854034424
Validation loss: 2.2297426551900883

Epoch: 5| Step: 2
Training loss: 2.4867238998413086
Validation loss: 2.2276732998509563

Epoch: 5| Step: 3
Training loss: 2.78676176071167
Validation loss: 2.23850929608909

Epoch: 5| Step: 4
Training loss: 2.965068817138672
Validation loss: 2.2443408966064453

Epoch: 5| Step: 5
Training loss: 2.172814130783081
Validation loss: 2.2389923910940848

Epoch: 5| Step: 6
Training loss: 3.0152931213378906
Validation loss: 2.243218121990081

Epoch: 5| Step: 7
Training loss: 2.3859121799468994
Validation loss: 2.246939887282669

Epoch: 5| Step: 8
Training loss: 2.4419941902160645
Validation loss: 2.274481427284979

Epoch: 5| Step: 9
Training loss: 2.426034450531006
Validation loss: 2.2945915114495063

Epoch: 5| Step: 10
Training loss: 2.297628879547119
Validation loss: 2.3051903017105593

Epoch: 119| Step: 0
Training loss: 2.8771700859069824
Validation loss: 2.295897176188807

Epoch: 5| Step: 1
Training loss: 2.4486281871795654
Validation loss: 2.303589767025363

Epoch: 5| Step: 2
Training loss: 2.722048282623291
Validation loss: 2.3089636013072026

Epoch: 5| Step: 3
Training loss: 3.213228940963745
Validation loss: 2.3039145790120608

Epoch: 5| Step: 4
Training loss: 2.2748425006866455
Validation loss: 2.29886531829834

Epoch: 5| Step: 5
Training loss: 2.0271358489990234
Validation loss: 2.297683741456719

Epoch: 5| Step: 6
Training loss: 1.978027582168579
Validation loss: 2.3133486855414604

Epoch: 5| Step: 7
Training loss: 2.756267786026001
Validation loss: 2.3254988629330873

Epoch: 5| Step: 8
Training loss: 2.883040189743042
Validation loss: 2.357348680496216

Epoch: 5| Step: 9
Training loss: 2.356677293777466
Validation loss: 2.361909668932679

Epoch: 5| Step: 10
Training loss: 2.2629904747009277
Validation loss: 2.3756732120308826

Epoch: 120| Step: 0
Training loss: 2.0426526069641113
Validation loss: 2.3731060694622736

Epoch: 5| Step: 1
Training loss: 2.7231040000915527
Validation loss: 2.35626381956121

Epoch: 5| Step: 2
Training loss: 3.0181667804718018
Validation loss: 2.3348961876284693

Epoch: 5| Step: 3
Training loss: 2.960395336151123
Validation loss: 2.3224335485889065

Epoch: 5| Step: 4
Training loss: 2.064358949661255
Validation loss: 2.304449758222026

Epoch: 5| Step: 5
Training loss: 2.3111374378204346
Validation loss: 2.2890047514310448

Epoch: 5| Step: 6
Training loss: 2.413691520690918
Validation loss: 2.2693644877403014

Epoch: 5| Step: 7
Training loss: 2.050595283508301
Validation loss: 2.285678058542231

Epoch: 5| Step: 8
Training loss: 2.277331829071045
Validation loss: 2.3572385054762646

Epoch: 5| Step: 9
Training loss: 3.0037176609039307
Validation loss: 2.3601946676931074

Epoch: 5| Step: 10
Training loss: 3.1423470973968506
Validation loss: 2.3490111084394556

Epoch: 121| Step: 0
Training loss: 2.6372694969177246
Validation loss: 2.3028784823674027

Epoch: 5| Step: 1
Training loss: 2.2979187965393066
Validation loss: 2.2289591643118087

Epoch: 5| Step: 2
Training loss: 2.6707520484924316
Validation loss: 2.207241008358617

Epoch: 5| Step: 3
Training loss: 2.226118564605713
Validation loss: 2.1992710405780422

Epoch: 5| Step: 4
Training loss: 2.088250160217285
Validation loss: 2.186980055224511

Epoch: 5| Step: 5
Training loss: 2.9720168113708496
Validation loss: 2.1865605461981987

Epoch: 5| Step: 6
Training loss: 2.5519344806671143
Validation loss: 2.1803156970649638

Epoch: 5| Step: 7
Training loss: 2.559715986251831
Validation loss: 2.178967736100638

Epoch: 5| Step: 8
Training loss: 2.2239766120910645
Validation loss: 2.19362287880272

Epoch: 5| Step: 9
Training loss: 2.752861738204956
Validation loss: 2.203429860453452

Epoch: 5| Step: 10
Training loss: 2.3830020427703857
Validation loss: 2.212423170766523

Epoch: 122| Step: 0
Training loss: 3.1531364917755127
Validation loss: 2.2168975594223186

Epoch: 5| Step: 1
Training loss: 2.507349729537964
Validation loss: 2.2097738994065153

Epoch: 5| Step: 2
Training loss: 2.6894726753234863
Validation loss: 2.2004496615420104

Epoch: 5| Step: 3
Training loss: 2.9312901496887207
Validation loss: 2.198601345862112

Epoch: 5| Step: 4
Training loss: 2.6835238933563232
Validation loss: 2.183433460932906

Epoch: 5| Step: 5
Training loss: 1.7758901119232178
Validation loss: 2.17541152943847

Epoch: 5| Step: 6
Training loss: 2.7961392402648926
Validation loss: 2.175472476149118

Epoch: 5| Step: 7
Training loss: 2.1434788703918457
Validation loss: 2.168500787468367

Epoch: 5| Step: 8
Training loss: 2.63547945022583
Validation loss: 2.161708765132453

Epoch: 5| Step: 9
Training loss: 1.6367727518081665
Validation loss: 2.1670733087806293

Epoch: 5| Step: 10
Training loss: 2.217097759246826
Validation loss: 2.164527520056694

Epoch: 123| Step: 0
Training loss: 2.572174072265625
Validation loss: 2.164903917620259

Epoch: 5| Step: 1
Training loss: 2.1899123191833496
Validation loss: 2.175050586782476

Epoch: 5| Step: 2
Training loss: 2.8309152126312256
Validation loss: 2.19497952922698

Epoch: 5| Step: 3
Training loss: 2.1426644325256348
Validation loss: 2.185376746680147

Epoch: 5| Step: 4
Training loss: 2.506885528564453
Validation loss: 2.1900887412409626

Epoch: 5| Step: 5
Training loss: 2.5502562522888184
Validation loss: 2.1636276475844847

Epoch: 5| Step: 6
Training loss: 2.039877414703369
Validation loss: 2.1677916613958215

Epoch: 5| Step: 7
Training loss: 2.7921061515808105
Validation loss: 2.1573359171549478

Epoch: 5| Step: 8
Training loss: 2.637772798538208
Validation loss: 2.1557845248970935

Epoch: 5| Step: 9
Training loss: 2.625028133392334
Validation loss: 2.1623917395068752

Epoch: 5| Step: 10
Training loss: 1.9958828687667847
Validation loss: 2.1657455928864016

Epoch: 124| Step: 0
Training loss: 2.498595714569092
Validation loss: 2.159677502929523

Epoch: 5| Step: 1
Training loss: 2.2298033237457275
Validation loss: 2.176738747986414

Epoch: 5| Step: 2
Training loss: 2.16874623298645
Validation loss: 2.188970778578071

Epoch: 5| Step: 3
Training loss: 2.634889602661133
Validation loss: 2.193420510138235

Epoch: 5| Step: 4
Training loss: 1.833936095237732
Validation loss: 2.1980223463427637

Epoch: 5| Step: 5
Training loss: 2.5219955444335938
Validation loss: 2.2040713576860327

Epoch: 5| Step: 6
Training loss: 2.8807904720306396
Validation loss: 2.2019920041484218

Epoch: 5| Step: 7
Training loss: 2.2341344356536865
Validation loss: 2.19685620902687

Epoch: 5| Step: 8
Training loss: 2.8243489265441895
Validation loss: 2.1884559867202595

Epoch: 5| Step: 9
Training loss: 2.468334674835205
Validation loss: 2.179660609973374

Epoch: 5| Step: 10
Training loss: 2.6282379627227783
Validation loss: 2.1650368244417253

Epoch: 125| Step: 0
Training loss: 2.7672691345214844
Validation loss: 2.1657547027834

Epoch: 5| Step: 1
Training loss: 2.009551525115967
Validation loss: 2.160034418106079

Epoch: 5| Step: 2
Training loss: 2.0630242824554443
Validation loss: 2.1634128529538392

Epoch: 5| Step: 3
Training loss: 2.5827736854553223
Validation loss: 2.157557225996448

Epoch: 5| Step: 4
Training loss: 2.7537548542022705
Validation loss: 2.169914545551423

Epoch: 5| Step: 5
Training loss: 2.688819646835327
Validation loss: 2.1787805557250977

Epoch: 5| Step: 6
Training loss: 2.233839511871338
Validation loss: 2.1931663149146625

Epoch: 5| Step: 7
Training loss: 2.314175844192505
Validation loss: 2.177584402022823

Epoch: 5| Step: 8
Training loss: 2.865926742553711
Validation loss: 2.173168264409547

Epoch: 5| Step: 9
Training loss: 2.388122081756592
Validation loss: 2.1593091500702726

Epoch: 5| Step: 10
Training loss: 2.413478374481201
Validation loss: 2.1535495301728607

Epoch: 126| Step: 0
Training loss: 2.2939534187316895
Validation loss: 2.147687860714492

Epoch: 5| Step: 1
Training loss: 2.4446401596069336
Validation loss: 2.1467146565837245

Epoch: 5| Step: 2
Training loss: 2.6755404472351074
Validation loss: 2.142114172699631

Epoch: 5| Step: 3
Training loss: 2.704166889190674
Validation loss: 2.1366491394658245

Epoch: 5| Step: 4
Training loss: 2.03452730178833
Validation loss: 2.1408417686339347

Epoch: 5| Step: 5
Training loss: 2.070343255996704
Validation loss: 2.1375409313427505

Epoch: 5| Step: 6
Training loss: 2.721775531768799
Validation loss: 2.1353469484595844

Epoch: 5| Step: 7
Training loss: 2.6611359119415283
Validation loss: 2.137064787649339

Epoch: 5| Step: 8
Training loss: 2.435971260070801
Validation loss: 2.1484114457202215

Epoch: 5| Step: 9
Training loss: 2.1882731914520264
Validation loss: 2.1605534168981735

Epoch: 5| Step: 10
Training loss: 2.630913257598877
Validation loss: 2.169484432025622

Epoch: 127| Step: 0
Training loss: 2.5511488914489746
Validation loss: 2.180782848788846

Epoch: 5| Step: 1
Training loss: 2.265758991241455
Validation loss: 2.194731407268073

Epoch: 5| Step: 2
Training loss: 2.7403976917266846
Validation loss: 2.2023933984900035

Epoch: 5| Step: 3
Training loss: 2.129549503326416
Validation loss: 2.21054599618399

Epoch: 5| Step: 4
Training loss: 2.266364574432373
Validation loss: 2.1991457246964976

Epoch: 5| Step: 5
Training loss: 2.3645811080932617
Validation loss: 2.189492512774724

Epoch: 5| Step: 6
Training loss: 2.901948928833008
Validation loss: 2.1839817006100892

Epoch: 5| Step: 7
Training loss: 2.078754186630249
Validation loss: 2.1849651746852423

Epoch: 5| Step: 8
Training loss: 2.4617018699645996
Validation loss: 2.1809139302981797

Epoch: 5| Step: 9
Training loss: 2.416646957397461
Validation loss: 2.1783859524675595

Epoch: 5| Step: 10
Training loss: 2.799055337905884
Validation loss: 2.165916091652327

Epoch: 128| Step: 0
Training loss: 2.6631505489349365
Validation loss: 2.169919813832929

Epoch: 5| Step: 1
Training loss: 2.503985643386841
Validation loss: 2.1661365314196517

Epoch: 5| Step: 2
Training loss: 2.347360134124756
Validation loss: 2.171990798365685

Epoch: 5| Step: 3
Training loss: 3.095862865447998
Validation loss: 2.167309966138614

Epoch: 5| Step: 4
Training loss: 2.3134899139404297
Validation loss: 2.1583549745621218

Epoch: 5| Step: 5
Training loss: 1.8380224704742432
Validation loss: 2.1654525674799436

Epoch: 5| Step: 6
Training loss: 2.4383809566497803
Validation loss: 2.17176708867473

Epoch: 5| Step: 7
Training loss: 2.4608726501464844
Validation loss: 2.169018245512439

Epoch: 5| Step: 8
Training loss: 2.1946053504943848
Validation loss: 2.183254039415749

Epoch: 5| Step: 9
Training loss: 2.3486123085021973
Validation loss: 2.1891547608119186

Epoch: 5| Step: 10
Training loss: 2.637897253036499
Validation loss: 2.2157085557137766

Epoch: 129| Step: 0
Training loss: 2.6863603591918945
Validation loss: 2.2453926096680346

Epoch: 5| Step: 1
Training loss: 1.9295704364776611
Validation loss: 2.263301564801124

Epoch: 5| Step: 2
Training loss: 2.2924952507019043
Validation loss: 2.2673852674422728

Epoch: 5| Step: 3
Training loss: 2.48417592048645
Validation loss: 2.256133915275656

Epoch: 5| Step: 4
Training loss: 2.5300228595733643
Validation loss: 2.262254971329884

Epoch: 5| Step: 5
Training loss: 2.8457884788513184
Validation loss: 2.228326738521617

Epoch: 5| Step: 6
Training loss: 2.6591174602508545
Validation loss: 2.1802154689706783

Epoch: 5| Step: 7
Training loss: 2.5461912155151367
Validation loss: 2.1634486580407746

Epoch: 5| Step: 8
Training loss: 2.359339714050293
Validation loss: 2.1663750320352535

Epoch: 5| Step: 9
Training loss: 2.1040022373199463
Validation loss: 2.1551708047107985

Epoch: 5| Step: 10
Training loss: 2.335367441177368
Validation loss: 2.165308112739235

Epoch: 130| Step: 0
Training loss: 2.333594799041748
Validation loss: 2.17011276624536

Epoch: 5| Step: 1
Training loss: 2.6196353435516357
Validation loss: 2.1709218537935646

Epoch: 5| Step: 2
Training loss: 2.406048536300659
Validation loss: 2.1793526552056752

Epoch: 5| Step: 3
Training loss: 1.5036519765853882
Validation loss: 2.1745313072717316

Epoch: 5| Step: 4
Training loss: 2.588606357574463
Validation loss: 2.1598300626201015

Epoch: 5| Step: 5
Training loss: 2.8611385822296143
Validation loss: 2.1371146094414497

Epoch: 5| Step: 6
Training loss: 2.605581283569336
Validation loss: 2.140146069629218

Epoch: 5| Step: 7
Training loss: 2.5517566204071045
Validation loss: 2.1404930929983816

Epoch: 5| Step: 8
Training loss: 2.6669204235076904
Validation loss: 2.1341382918819303

Epoch: 5| Step: 9
Training loss: 2.5495827198028564
Validation loss: 2.132028418202554

Epoch: 5| Step: 10
Training loss: 2.1767029762268066
Validation loss: 2.119855410309248

Epoch: 131| Step: 0
Training loss: 2.0418083667755127
Validation loss: 2.1210256545774397

Epoch: 5| Step: 1
Training loss: 1.696110725402832
Validation loss: 2.145347895160798

Epoch: 5| Step: 2
Training loss: 2.3821380138397217
Validation loss: 2.165684707703129

Epoch: 5| Step: 3
Training loss: 2.7099783420562744
Validation loss: 2.206176834721719

Epoch: 5| Step: 4
Training loss: 2.5155835151672363
Validation loss: 2.227438565223448

Epoch: 5| Step: 5
Training loss: 2.622472047805786
Validation loss: 2.209243328340592

Epoch: 5| Step: 6
Training loss: 3.051286220550537
Validation loss: 2.1869366348430677

Epoch: 5| Step: 7
Training loss: 1.8293540477752686
Validation loss: 2.1694445917683263

Epoch: 5| Step: 8
Training loss: 2.2193286418914795
Validation loss: 2.1290963683077084

Epoch: 5| Step: 9
Training loss: 3.0410382747650146
Validation loss: 2.1141938291570193

Epoch: 5| Step: 10
Training loss: 2.6791560649871826
Validation loss: 2.116281137671522

Epoch: 132| Step: 0
Training loss: 2.2083828449249268
Validation loss: 2.120216813138736

Epoch: 5| Step: 1
Training loss: 2.396286964416504
Validation loss: 2.11888607599402

Epoch: 5| Step: 2
Training loss: 2.382148265838623
Validation loss: 2.12545084568762

Epoch: 5| Step: 3
Training loss: 3.2272753715515137
Validation loss: 2.1198745568593345

Epoch: 5| Step: 4
Training loss: 2.316181182861328
Validation loss: 2.1207220913261495

Epoch: 5| Step: 5
Training loss: 3.3661811351776123
Validation loss: 2.121599530660978

Epoch: 5| Step: 6
Training loss: 2.131774425506592
Validation loss: 2.1194420860659693

Epoch: 5| Step: 7
Training loss: 1.9642126560211182
Validation loss: 2.123502354468069

Epoch: 5| Step: 8
Training loss: 2.512864589691162
Validation loss: 2.12502690797211

Epoch: 5| Step: 9
Training loss: 2.2510533332824707
Validation loss: 2.1316038575223697

Epoch: 5| Step: 10
Training loss: 1.8943506479263306
Validation loss: 2.149074908225767

Epoch: 133| Step: 0
Training loss: 2.536402702331543
Validation loss: 2.1482451833704466

Epoch: 5| Step: 1
Training loss: 2.6606690883636475
Validation loss: 2.1694500433501376

Epoch: 5| Step: 2
Training loss: 1.951481580734253
Validation loss: 2.1709778616505284

Epoch: 5| Step: 3
Training loss: 2.372699022293091
Validation loss: 2.1745856756805093

Epoch: 5| Step: 4
Training loss: 2.3299355506896973
Validation loss: 2.1795131878186296

Epoch: 5| Step: 5
Training loss: 2.7721176147460938
Validation loss: 2.168092671261039

Epoch: 5| Step: 6
Training loss: 2.155970335006714
Validation loss: 2.1435942431931854

Epoch: 5| Step: 7
Training loss: 2.554330825805664
Validation loss: 2.13059800927357

Epoch: 5| Step: 8
Training loss: 2.5720572471618652
Validation loss: 2.1206967689657725

Epoch: 5| Step: 9
Training loss: 2.236924409866333
Validation loss: 2.120660025586364

Epoch: 5| Step: 10
Training loss: 2.3884387016296387
Validation loss: 2.1208354914060203

Epoch: 134| Step: 0
Training loss: 1.725783348083496
Validation loss: 2.119054695611359

Epoch: 5| Step: 1
Training loss: 2.5070016384124756
Validation loss: 2.1161853908210673

Epoch: 5| Step: 2
Training loss: 2.219080924987793
Validation loss: 2.116366683795888

Epoch: 5| Step: 3
Training loss: 2.736966133117676
Validation loss: 2.132479639463527

Epoch: 5| Step: 4
Training loss: 2.764554738998413
Validation loss: 2.140247796171455

Epoch: 5| Step: 5
Training loss: 2.4393017292022705
Validation loss: 2.151174350451398

Epoch: 5| Step: 6
Training loss: 1.92153799533844
Validation loss: 2.1544840874210482

Epoch: 5| Step: 7
Training loss: 2.9697866439819336
Validation loss: 2.145698775527298

Epoch: 5| Step: 8
Training loss: 2.270771026611328
Validation loss: 2.1314464307600454

Epoch: 5| Step: 9
Training loss: 2.5046937465667725
Validation loss: 2.123839234793058

Epoch: 5| Step: 10
Training loss: 2.3904922008514404
Validation loss: 2.1284068451132825

Epoch: 135| Step: 0
Training loss: 2.3804993629455566
Validation loss: 2.1312106065852667

Epoch: 5| Step: 1
Training loss: 1.996787667274475
Validation loss: 2.131898928714055

Epoch: 5| Step: 2
Training loss: 2.344576358795166
Validation loss: 2.1323262696625083

Epoch: 5| Step: 3
Training loss: 2.71256685256958
Validation loss: 2.13454274464679

Epoch: 5| Step: 4
Training loss: 2.948316812515259
Validation loss: 2.136469376984463

Epoch: 5| Step: 5
Training loss: 2.7725255489349365
Validation loss: 2.1485029753818305

Epoch: 5| Step: 6
Training loss: 1.9082849025726318
Validation loss: 2.159262009846267

Epoch: 5| Step: 7
Training loss: 2.947401523590088
Validation loss: 2.1827811438550233

Epoch: 5| Step: 8
Training loss: 2.51479434967041
Validation loss: 2.1935730800833753

Epoch: 5| Step: 9
Training loss: 2.123746395111084
Validation loss: 2.1982426630553378

Epoch: 5| Step: 10
Training loss: 1.8620609045028687
Validation loss: 2.17376906641068

Epoch: 136| Step: 0
Training loss: 2.5155985355377197
Validation loss: 2.1528480283675657

Epoch: 5| Step: 1
Training loss: 1.9055591821670532
Validation loss: 2.1393464585786224

Epoch: 5| Step: 2
Training loss: 3.1257758140563965
Validation loss: 2.1222161887794413

Epoch: 5| Step: 3
Training loss: 2.783235549926758
Validation loss: 2.1258173681074575

Epoch: 5| Step: 4
Training loss: 1.8255935907363892
Validation loss: 2.1208292438137915

Epoch: 5| Step: 5
Training loss: 2.208427667617798
Validation loss: 2.1203153620484056

Epoch: 5| Step: 6
Training loss: 1.9177204370498657
Validation loss: 2.1182811414041827

Epoch: 5| Step: 7
Training loss: 2.140110969543457
Validation loss: 2.122341412369923

Epoch: 5| Step: 8
Training loss: 2.8239753246307373
Validation loss: 2.14365549754071

Epoch: 5| Step: 9
Training loss: 2.471966505050659
Validation loss: 2.174769224659089

Epoch: 5| Step: 10
Training loss: 2.6437196731567383
Validation loss: 2.1968254184210174

Epoch: 137| Step: 0
Training loss: 2.3420143127441406
Validation loss: 2.2139956874232136

Epoch: 5| Step: 1
Training loss: 2.6171700954437256
Validation loss: 2.213276376006424

Epoch: 5| Step: 2
Training loss: 2.708463668823242
Validation loss: 2.2037198799912647

Epoch: 5| Step: 3
Training loss: 2.1133508682250977
Validation loss: 2.1692985667977283

Epoch: 5| Step: 4
Training loss: 2.3714823722839355
Validation loss: 2.140577103502007

Epoch: 5| Step: 5
Training loss: 2.5385091304779053
Validation loss: 2.111919090312014

Epoch: 5| Step: 6
Training loss: 1.9128614664077759
Validation loss: 2.1085808277130127

Epoch: 5| Step: 7
Training loss: 2.4732043743133545
Validation loss: 2.106576491427678

Epoch: 5| Step: 8
Training loss: 2.5960216522216797
Validation loss: 2.1110870684346845

Epoch: 5| Step: 9
Training loss: 2.6914451122283936
Validation loss: 2.1165182257211335

Epoch: 5| Step: 10
Training loss: 2.229273796081543
Validation loss: 2.1155999360545987

Epoch: 138| Step: 0
Training loss: 2.1643848419189453
Validation loss: 2.1214326479101695

Epoch: 5| Step: 1
Training loss: 2.6193695068359375
Validation loss: 2.145116654775476

Epoch: 5| Step: 2
Training loss: 2.9915614128112793
Validation loss: 2.156679448261056

Epoch: 5| Step: 3
Training loss: 2.6810951232910156
Validation loss: 2.158387614834693

Epoch: 5| Step: 4
Training loss: 2.20750093460083
Validation loss: 2.1517321653263544

Epoch: 5| Step: 5
Training loss: 2.406200408935547
Validation loss: 2.1570326717950965

Epoch: 5| Step: 6
Training loss: 2.0018324851989746
Validation loss: 2.1493342871307046

Epoch: 5| Step: 7
Training loss: 2.483177900314331
Validation loss: 2.141857049798453

Epoch: 5| Step: 8
Training loss: 1.897487998008728
Validation loss: 2.148469924926758

Epoch: 5| Step: 9
Training loss: 2.4094345569610596
Validation loss: 2.137447464850641

Epoch: 5| Step: 10
Training loss: 2.4353315830230713
Validation loss: 2.125435065197688

Epoch: 139| Step: 0
Training loss: 2.3352198600769043
Validation loss: 2.125184323198052

Epoch: 5| Step: 1
Training loss: 2.3638339042663574
Validation loss: 2.1243061211801346

Epoch: 5| Step: 2
Training loss: 2.3243656158447266
Validation loss: 2.1277419854235906

Epoch: 5| Step: 3
Training loss: 2.3124661445617676
Validation loss: 2.129734226452407

Epoch: 5| Step: 4
Training loss: 2.5069727897644043
Validation loss: 2.124466441010916

Epoch: 5| Step: 5
Training loss: 2.8034307956695557
Validation loss: 2.1290157341188

Epoch: 5| Step: 6
Training loss: 1.7595179080963135
Validation loss: 2.1281032434073825

Epoch: 5| Step: 7
Training loss: 2.6604363918304443
Validation loss: 2.1383270140617125

Epoch: 5| Step: 8
Training loss: 2.0284790992736816
Validation loss: 2.1282677829906507

Epoch: 5| Step: 9
Training loss: 2.6134681701660156
Validation loss: 2.1361310917844056

Epoch: 5| Step: 10
Training loss: 2.4489521980285645
Validation loss: 2.139916976292928

Epoch: 140| Step: 0
Training loss: 1.9374240636825562
Validation loss: 2.129051018786687

Epoch: 5| Step: 1
Training loss: 1.9628067016601562
Validation loss: 2.1266666817408737

Epoch: 5| Step: 2
Training loss: 1.8184388875961304
Validation loss: 2.123713319019605

Epoch: 5| Step: 3
Training loss: 2.7896339893341064
Validation loss: 2.133187376042848

Epoch: 5| Step: 4
Training loss: 2.378465175628662
Validation loss: 2.1363407616974204

Epoch: 5| Step: 5
Training loss: 2.6881656646728516
Validation loss: 2.136664016272432

Epoch: 5| Step: 6
Training loss: 2.54998779296875
Validation loss: 2.133996537936631

Epoch: 5| Step: 7
Training loss: 3.050168514251709
Validation loss: 2.1377945587199223

Epoch: 5| Step: 8
Training loss: 2.0671210289001465
Validation loss: 2.12938255135731

Epoch: 5| Step: 9
Training loss: 2.611272096633911
Validation loss: 2.1236125551244265

Epoch: 5| Step: 10
Training loss: 2.2026803493499756
Validation loss: 2.105303955334489

Epoch: 141| Step: 0
Training loss: 2.6936635971069336
Validation loss: 2.0980228711200017

Epoch: 5| Step: 1
Training loss: 2.6572506427764893
Validation loss: 2.104739284002653

Epoch: 5| Step: 2
Training loss: 3.123319149017334
Validation loss: 2.1204110050714142

Epoch: 5| Step: 3
Training loss: 1.8207346200942993
Validation loss: 2.116065799549062

Epoch: 5| Step: 4
Training loss: 1.8873567581176758
Validation loss: 2.1139083370085685

Epoch: 5| Step: 5
Training loss: 3.1353423595428467
Validation loss: 2.1123558731489283

Epoch: 5| Step: 6
Training loss: 1.8913075923919678
Validation loss: 2.1010905004316762

Epoch: 5| Step: 7
Training loss: 1.9367021322250366
Validation loss: 2.090104100524738

Epoch: 5| Step: 8
Training loss: 2.1192240715026855
Validation loss: 2.097440210721826

Epoch: 5| Step: 9
Training loss: 2.805861711502075
Validation loss: 2.101267194235197

Epoch: 5| Step: 10
Training loss: 2.347566604614258
Validation loss: 2.1098701389887

Epoch: 142| Step: 0
Training loss: 2.1346352100372314
Validation loss: 2.1277759357165267

Epoch: 5| Step: 1
Training loss: 1.9797760248184204
Validation loss: 2.121346572394012

Epoch: 5| Step: 2
Training loss: 2.0857534408569336
Validation loss: 2.130245993214269

Epoch: 5| Step: 3
Training loss: 2.4974148273468018
Validation loss: 2.120617724234058

Epoch: 5| Step: 4
Training loss: 2.296610116958618
Validation loss: 2.1296709583651636

Epoch: 5| Step: 5
Training loss: 2.4738316535949707
Validation loss: 2.1278726413685787

Epoch: 5| Step: 6
Training loss: 2.9679393768310547
Validation loss: 2.125453297809888

Epoch: 5| Step: 7
Training loss: 1.9938106536865234
Validation loss: 2.1245731205068608

Epoch: 5| Step: 8
Training loss: 2.512681245803833
Validation loss: 2.121006190135915

Epoch: 5| Step: 9
Training loss: 2.8642754554748535
Validation loss: 2.1153068196388984

Epoch: 5| Step: 10
Training loss: 2.4164538383483887
Validation loss: 2.1088409372555312

Epoch: 143| Step: 0
Training loss: 2.6996562480926514
Validation loss: 2.10347669611695

Epoch: 5| Step: 1
Training loss: 2.5006542205810547
Validation loss: 2.112847505077239

Epoch: 5| Step: 2
Training loss: 2.462244749069214
Validation loss: 2.10896453549785

Epoch: 5| Step: 3
Training loss: 1.8148365020751953
Validation loss: 2.123782386061966

Epoch: 5| Step: 4
Training loss: 1.83297598361969
Validation loss: 2.1528668583080335

Epoch: 5| Step: 5
Training loss: 2.5874133110046387
Validation loss: 2.1592652438789286

Epoch: 5| Step: 6
Training loss: 2.415788173675537
Validation loss: 2.1764962621914443

Epoch: 5| Step: 7
Training loss: 1.8917415142059326
Validation loss: 2.144350899163113

Epoch: 5| Step: 8
Training loss: 2.9510233402252197
Validation loss: 2.1115127558349283

Epoch: 5| Step: 9
Training loss: 2.615647315979004
Validation loss: 2.094018518283803

Epoch: 5| Step: 10
Training loss: 2.463510036468506
Validation loss: 2.0912279839156778

Epoch: 144| Step: 0
Training loss: 2.57720685005188
Validation loss: 2.0875330048222698

Epoch: 5| Step: 1
Training loss: 2.6055755615234375
Validation loss: 2.086816704401406

Epoch: 5| Step: 2
Training loss: 2.061102867126465
Validation loss: 2.0856107640010055

Epoch: 5| Step: 3
Training loss: 1.9745371341705322
Validation loss: 2.0833946466445923

Epoch: 5| Step: 4
Training loss: 2.127908229827881
Validation loss: 2.0759758872370564

Epoch: 5| Step: 5
Training loss: 2.366107225418091
Validation loss: 2.109072287877401

Epoch: 5| Step: 6
Training loss: 2.466548204421997
Validation loss: 2.1366346215689056

Epoch: 5| Step: 7
Training loss: 3.456467390060425
Validation loss: 2.143343548620901

Epoch: 5| Step: 8
Training loss: 1.6878798007965088
Validation loss: 2.148166548821234

Epoch: 5| Step: 9
Training loss: 2.453756809234619
Validation loss: 2.1279558302253805

Epoch: 5| Step: 10
Training loss: 2.6602540016174316
Validation loss: 2.1127956246816986

Epoch: 145| Step: 0
Training loss: 2.4767284393310547
Validation loss: 2.0789021407404253

Epoch: 5| Step: 1
Training loss: 2.303612470626831
Validation loss: 2.07696125840628

Epoch: 5| Step: 2
Training loss: 2.1208951473236084
Validation loss: 2.0765658552928636

Epoch: 5| Step: 3
Training loss: 1.7468522787094116
Validation loss: 2.082687649675595

Epoch: 5| Step: 4
Training loss: 2.238034725189209
Validation loss: 2.085289360374533

Epoch: 5| Step: 5
Training loss: 2.546104669570923
Validation loss: 2.090022789534702

Epoch: 5| Step: 6
Training loss: 3.0794060230255127
Validation loss: 2.096977674832908

Epoch: 5| Step: 7
Training loss: 2.276146650314331
Validation loss: 2.107846221616191

Epoch: 5| Step: 8
Training loss: 3.00461745262146
Validation loss: 2.112116770077777

Epoch: 5| Step: 9
Training loss: 2.2108054161071777
Validation loss: 2.1021127316259567

Epoch: 5| Step: 10
Training loss: 1.979566216468811
Validation loss: 2.0872690972461494

Epoch: 146| Step: 0
Training loss: 2.8984110355377197
Validation loss: 2.0832358675618328

Epoch: 5| Step: 1
Training loss: 1.9423465728759766
Validation loss: 2.089609924183097

Epoch: 5| Step: 2
Training loss: 2.432046413421631
Validation loss: 2.0879443153258292

Epoch: 5| Step: 3
Training loss: 2.112642288208008
Validation loss: 2.0939125296890095

Epoch: 5| Step: 4
Training loss: 2.1611571311950684
Validation loss: 2.0825177097833283

Epoch: 5| Step: 5
Training loss: 2.0168190002441406
Validation loss: 2.0738118463946926

Epoch: 5| Step: 6
Training loss: 2.5800247192382812
Validation loss: 2.0689405548957085

Epoch: 5| Step: 7
Training loss: 2.523282766342163
Validation loss: 2.0679073820831957

Epoch: 5| Step: 8
Training loss: 2.6418488025665283
Validation loss: 2.0735005460759646

Epoch: 5| Step: 9
Training loss: 2.4701061248779297
Validation loss: 2.073966387779482

Epoch: 5| Step: 10
Training loss: 2.1505746841430664
Validation loss: 2.0704761564090686

Epoch: 147| Step: 0
Training loss: 2.9488391876220703
Validation loss: 2.0765830662942704

Epoch: 5| Step: 1
Training loss: 2.6285178661346436
Validation loss: 2.075106595152168

Epoch: 5| Step: 2
Training loss: 1.7963101863861084
Validation loss: 2.075957800752373

Epoch: 5| Step: 3
Training loss: 2.1917567253112793
Validation loss: 2.0841533599361295

Epoch: 5| Step: 4
Training loss: 2.510451555252075
Validation loss: 2.0773114799171366

Epoch: 5| Step: 5
Training loss: 2.5228023529052734
Validation loss: 2.0828768771181823

Epoch: 5| Step: 6
Training loss: 1.8115984201431274
Validation loss: 2.0823363155447026

Epoch: 5| Step: 7
Training loss: 2.644733428955078
Validation loss: 2.072526079352184

Epoch: 5| Step: 8
Training loss: 2.3714022636413574
Validation loss: 2.087812751852056

Epoch: 5| Step: 9
Training loss: 2.360386371612549
Validation loss: 2.0862696452807357

Epoch: 5| Step: 10
Training loss: 1.8830461502075195
Validation loss: 2.1002279097034084

Epoch: 148| Step: 0
Training loss: 2.1494791507720947
Validation loss: 2.0948398779797297

Epoch: 5| Step: 1
Training loss: 2.687314033508301
Validation loss: 2.092102644264057

Epoch: 5| Step: 2
Training loss: 1.3799604177474976
Validation loss: 2.0937990475726385

Epoch: 5| Step: 3
Training loss: 2.3630003929138184
Validation loss: 2.127347033510926

Epoch: 5| Step: 4
Training loss: 2.791217803955078
Validation loss: 2.121982829545134

Epoch: 5| Step: 5
Training loss: 2.6387362480163574
Validation loss: 2.1100418388202624

Epoch: 5| Step: 6
Training loss: 2.2579898834228516
Validation loss: 2.1037937415543424

Epoch: 5| Step: 7
Training loss: 2.337211847305298
Validation loss: 2.0925726480381464

Epoch: 5| Step: 8
Training loss: 2.55310320854187
Validation loss: 2.0858677407746673

Epoch: 5| Step: 9
Training loss: 2.4282538890838623
Validation loss: 2.0706673975913756

Epoch: 5| Step: 10
Training loss: 2.1073951721191406
Validation loss: 2.068115511248189

Epoch: 149| Step: 0
Training loss: 2.7059311866760254
Validation loss: 2.065898287680841

Epoch: 5| Step: 1
Training loss: 2.062662363052368
Validation loss: 2.0542666783896824

Epoch: 5| Step: 2
Training loss: 2.0679197311401367
Validation loss: 2.060690818294402

Epoch: 5| Step: 3
Training loss: 2.039742946624756
Validation loss: 2.060662748993084

Epoch: 5| Step: 4
Training loss: 2.4504528045654297
Validation loss: 2.0540307234692317

Epoch: 5| Step: 5
Training loss: 2.1143360137939453
Validation loss: 2.0649622563392884

Epoch: 5| Step: 6
Training loss: 2.679668426513672
Validation loss: 2.0709362799121487

Epoch: 5| Step: 7
Training loss: 2.8675034046173096
Validation loss: 2.0909580569113455

Epoch: 5| Step: 8
Training loss: 2.139395236968994
Validation loss: 2.087570216066094

Epoch: 5| Step: 9
Training loss: 2.4819107055664062
Validation loss: 2.098497612501985

Epoch: 5| Step: 10
Training loss: 2.0886902809143066
Validation loss: 2.077712787094937

Epoch: 150| Step: 0
Training loss: 2.94956636428833
Validation loss: 2.0695356527964273

Epoch: 5| Step: 1
Training loss: 1.7717254161834717
Validation loss: 2.0709078965648526

Epoch: 5| Step: 2
Training loss: 2.5076582431793213
Validation loss: 2.0585678213386127

Epoch: 5| Step: 3
Training loss: 1.6857845783233643
Validation loss: 2.0646638049874255

Epoch: 5| Step: 4
Training loss: 2.5995934009552
Validation loss: 2.059902375744235

Epoch: 5| Step: 5
Training loss: 2.3056538105010986
Validation loss: 2.0596328422587407

Epoch: 5| Step: 6
Training loss: 2.4184746742248535
Validation loss: 2.061152106972151

Epoch: 5| Step: 7
Training loss: 2.5461182594299316
Validation loss: 2.073163696514663

Epoch: 5| Step: 8
Training loss: 2.3432822227478027
Validation loss: 2.0736758939681517

Epoch: 5| Step: 9
Training loss: 2.7696826457977295
Validation loss: 2.0845402620171987

Epoch: 5| Step: 10
Training loss: 1.613661766052246
Validation loss: 2.0896375230563584

Testing loss: 2.343172974056668

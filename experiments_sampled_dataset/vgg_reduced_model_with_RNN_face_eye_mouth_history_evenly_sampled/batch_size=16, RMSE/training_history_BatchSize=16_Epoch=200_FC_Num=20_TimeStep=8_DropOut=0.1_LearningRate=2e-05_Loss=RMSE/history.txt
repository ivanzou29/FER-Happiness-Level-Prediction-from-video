Epoch: 1| Step: 0
Training loss: 6.170246150765394
Validation loss: 5.772788725563815

Epoch: 6| Step: 1
Training loss: 5.775067296812639
Validation loss: 5.755111715648805

Epoch: 6| Step: 2
Training loss: 5.295248946694641
Validation loss: 5.736756831004794

Epoch: 6| Step: 3
Training loss: 5.444595442946475
Validation loss: 5.716007953540987

Epoch: 6| Step: 4
Training loss: 5.85856081582872
Validation loss: 5.692595995209175

Epoch: 6| Step: 5
Training loss: 6.071822402105882
Validation loss: 5.6651541243324885

Epoch: 6| Step: 6
Training loss: 5.458572945419331
Validation loss: 5.634450998246967

Epoch: 6| Step: 7
Training loss: 5.65354576590416
Validation loss: 5.599695901407413

Epoch: 6| Step: 8
Training loss: 7.171992016338409
Validation loss: 5.560081539658327

Epoch: 6| Step: 9
Training loss: 3.6364250351317313
Validation loss: 5.517748545487595

Epoch: 6| Step: 10
Training loss: 6.1881513156452685
Validation loss: 5.471671215551987

Epoch: 6| Step: 11
Training loss: 5.451457033588589
Validation loss: 5.421837369939486

Epoch: 6| Step: 12
Training loss: 5.2856412610050505
Validation loss: 5.36758811167163

Epoch: 6| Step: 13
Training loss: 3.8836230654272885
Validation loss: 5.310212242912455

Epoch: 2| Step: 0
Training loss: 4.017979269442287
Validation loss: 5.249128360143212

Epoch: 6| Step: 1
Training loss: 4.7633227180877
Validation loss: 5.186645006414592

Epoch: 6| Step: 2
Training loss: 5.389901375230979
Validation loss: 5.121274633096179

Epoch: 6| Step: 3
Training loss: 5.430699447629054
Validation loss: 5.052641917908325

Epoch: 6| Step: 4
Training loss: 5.561571365163329
Validation loss: 4.9841342481744935

Epoch: 6| Step: 5
Training loss: 5.331082683768538
Validation loss: 4.9136379515097985

Epoch: 6| Step: 6
Training loss: 4.061113795952026
Validation loss: 4.848137850273449

Epoch: 6| Step: 7
Training loss: 5.030737051079058
Validation loss: 4.785209693836262

Epoch: 6| Step: 8
Training loss: 5.314725521891688
Validation loss: 4.724630152741304

Epoch: 6| Step: 9
Training loss: 5.676209362080293
Validation loss: 4.669353539302339

Epoch: 6| Step: 10
Training loss: 4.51297415569605
Validation loss: 4.612010498110916

Epoch: 6| Step: 11
Training loss: 4.364125934143901
Validation loss: 4.5564643471282436

Epoch: 6| Step: 12
Training loss: 3.5392142088585885
Validation loss: 4.507940353423331

Epoch: 6| Step: 13
Training loss: 5.415563246016655
Validation loss: 4.46914203925426

Epoch: 3| Step: 0
Training loss: 5.848395963985553
Validation loss: 4.441379457967039

Epoch: 6| Step: 1
Training loss: 4.061785356313369
Validation loss: 4.406729993274103

Epoch: 6| Step: 2
Training loss: 4.296235303945215
Validation loss: 4.37875356138406

Epoch: 6| Step: 3
Training loss: 4.335853039717934
Validation loss: 4.360513630246959

Epoch: 6| Step: 4
Training loss: 4.210311928109653
Validation loss: 4.340161582173495

Epoch: 6| Step: 5
Training loss: 3.954139790311319
Validation loss: 4.310227382472891

Epoch: 6| Step: 6
Training loss: 4.272297653129809
Validation loss: 4.2832421851192946

Epoch: 6| Step: 7
Training loss: 3.644023569610782
Validation loss: 4.260516255808322

Epoch: 6| Step: 8
Training loss: 4.154220659267954
Validation loss: 4.239975402913149

Epoch: 6| Step: 9
Training loss: 3.86017677534831
Validation loss: 4.221417798980268

Epoch: 6| Step: 10
Training loss: 3.980710846760917
Validation loss: 4.199375895721764

Epoch: 6| Step: 11
Training loss: 4.944089717260824
Validation loss: 4.177625386764499

Epoch: 6| Step: 12
Training loss: 4.6026487768392075
Validation loss: 4.162212454992263

Epoch: 6| Step: 13
Training loss: 5.495956668621864
Validation loss: 4.1465761200828695

Epoch: 4| Step: 0
Training loss: 4.649439394392501
Validation loss: 4.133252469503011

Epoch: 6| Step: 1
Training loss: 4.195386378030266
Validation loss: 4.116917970607704

Epoch: 6| Step: 2
Training loss: 4.665524456476409
Validation loss: 4.097680913742105

Epoch: 6| Step: 3
Training loss: 4.777302627199338
Validation loss: 4.07973036149923

Epoch: 6| Step: 4
Training loss: 3.4143636121146237
Validation loss: 4.063729278638501

Epoch: 6| Step: 5
Training loss: 4.717203455238109
Validation loss: 4.046033274983533

Epoch: 6| Step: 6
Training loss: 4.224657909380823
Validation loss: 4.031196027252772

Epoch: 6| Step: 7
Training loss: 4.140331099534134
Validation loss: 4.018697429033658

Epoch: 6| Step: 8
Training loss: 3.8798047702870297
Validation loss: 4.007036211764081

Epoch: 6| Step: 9
Training loss: 4.0869316802562325
Validation loss: 3.9951901781827512

Epoch: 6| Step: 10
Training loss: 3.1445251843885598
Validation loss: 3.9810886793331455

Epoch: 6| Step: 11
Training loss: 3.8765127859191346
Validation loss: 3.973730768962734

Epoch: 6| Step: 12
Training loss: 4.1538946605360705
Validation loss: 3.964049445898563

Epoch: 6| Step: 13
Training loss: 4.507567718047959
Validation loss: 3.951959372879358

Epoch: 5| Step: 0
Training loss: 3.4436206772575066
Validation loss: 3.944372935361599

Epoch: 6| Step: 1
Training loss: 5.276391497578413
Validation loss: 3.940443845815642

Epoch: 6| Step: 2
Training loss: 3.9231404850271154
Validation loss: 3.9252757537623784

Epoch: 6| Step: 3
Training loss: 3.522164869861063
Validation loss: 3.918202511227893

Epoch: 6| Step: 4
Training loss: 3.505048652779246
Validation loss: 3.9097340341004383

Epoch: 6| Step: 5
Training loss: 4.0585733069895396
Validation loss: 3.9022360456747576

Epoch: 6| Step: 6
Training loss: 4.5021888919419615
Validation loss: 3.892043802753507

Epoch: 6| Step: 7
Training loss: 3.6651397618060595
Validation loss: 3.8813261353608715

Epoch: 6| Step: 8
Training loss: 4.458607079964084
Validation loss: 3.87188031982834

Epoch: 6| Step: 9
Training loss: 3.742954884254413
Validation loss: 3.866982471595004

Epoch: 6| Step: 10
Training loss: 3.5313820603244626
Validation loss: 3.851357404619725

Epoch: 6| Step: 11
Training loss: 3.8234525854064887
Validation loss: 3.8380120982184445

Epoch: 6| Step: 12
Training loss: 4.775643046753993
Validation loss: 3.8290338297543

Epoch: 6| Step: 13
Training loss: 3.9824898841667173
Validation loss: 3.8168466348145405

Epoch: 6| Step: 0
Training loss: 4.296680792841294
Validation loss: 3.806619577802083

Epoch: 6| Step: 1
Training loss: 4.265390242939155
Validation loss: 3.7947246715246283

Epoch: 6| Step: 2
Training loss: 4.0702632966975685
Validation loss: 3.783145811334058

Epoch: 6| Step: 3
Training loss: 3.660094497794843
Validation loss: 3.7724350459016462

Epoch: 6| Step: 4
Training loss: 3.3760145569799356
Validation loss: 3.762097082292032

Epoch: 6| Step: 5
Training loss: 4.561489045477553
Validation loss: 3.7522098169270075

Epoch: 6| Step: 6
Training loss: 2.7697027285154094
Validation loss: 3.7431210157980543

Epoch: 6| Step: 7
Training loss: 3.1114344845089703
Validation loss: 3.7372849907994783

Epoch: 6| Step: 8
Training loss: 3.9655943812998666
Validation loss: 3.7297614681867493

Epoch: 6| Step: 9
Training loss: 4.510963436167104
Validation loss: 3.7223953437818915

Epoch: 6| Step: 10
Training loss: 3.242319274430535
Validation loss: 3.7100172631305695

Epoch: 6| Step: 11
Training loss: 3.726900161384424
Validation loss: 3.7025056522429796

Epoch: 6| Step: 12
Training loss: 3.7401246057244157
Validation loss: 3.6949066339519105

Epoch: 6| Step: 13
Training loss: 5.730478069607282
Validation loss: 3.6914134928175466

Epoch: 7| Step: 0
Training loss: 3.2058989631976
Validation loss: 3.6831773965374652

Epoch: 6| Step: 1
Training loss: 4.228816476739717
Validation loss: 3.673255119601749

Epoch: 6| Step: 2
Training loss: 4.458198093727995
Validation loss: 3.6640619598534507

Epoch: 6| Step: 3
Training loss: 4.39307571381315
Validation loss: 3.656082058311632

Epoch: 6| Step: 4
Training loss: 3.695501149303953
Validation loss: 3.6499308250436258

Epoch: 6| Step: 5
Training loss: 3.295402306408731
Validation loss: 3.6424641838882907

Epoch: 6| Step: 6
Training loss: 3.595996718306524
Validation loss: 3.634675493584342

Epoch: 6| Step: 7
Training loss: 3.7225530075695414
Validation loss: 3.628715877938774

Epoch: 6| Step: 8
Training loss: 4.22754932665796
Validation loss: 3.620677965499492

Epoch: 6| Step: 9
Training loss: 3.608058408581592
Validation loss: 3.6108237465885686

Epoch: 6| Step: 10
Training loss: 3.6599975681817987
Validation loss: 3.604640175374829

Epoch: 6| Step: 11
Training loss: 3.6426992115011627
Validation loss: 3.5985161177936034

Epoch: 6| Step: 12
Training loss: 4.012492698591488
Validation loss: 3.5916522194769493

Epoch: 6| Step: 13
Training loss: 3.4366401290485618
Validation loss: 3.58009997125786

Epoch: 8| Step: 0
Training loss: 4.137739910995485
Validation loss: 3.5807265729997324

Epoch: 6| Step: 1
Training loss: 3.4500604209584735
Validation loss: 3.5753401864276535

Epoch: 6| Step: 2
Training loss: 2.704699900933127
Validation loss: 3.5714082432826117

Epoch: 6| Step: 3
Training loss: 3.773257549410017
Validation loss: 3.5680401305184946

Epoch: 6| Step: 4
Training loss: 3.6699387660102243
Validation loss: 3.566492586216598

Epoch: 6| Step: 5
Training loss: 4.6918018183826256
Validation loss: 3.5665342886180387

Epoch: 6| Step: 6
Training loss: 3.4818286572345634
Validation loss: 3.554295269605102

Epoch: 6| Step: 7
Training loss: 3.9297168647169505
Validation loss: 3.55177568120932

Epoch: 6| Step: 8
Training loss: 3.8160483935344525
Validation loss: 3.5480873139755147

Epoch: 6| Step: 9
Training loss: 4.353830838675495
Validation loss: 3.543150444159723

Epoch: 6| Step: 10
Training loss: 3.642946608163397
Validation loss: 3.534151504732877

Epoch: 6| Step: 11
Training loss: 3.8135827199932932
Validation loss: 3.5273807739775656

Epoch: 6| Step: 12
Training loss: 3.7725611709681117
Validation loss: 3.521759517676323

Epoch: 6| Step: 13
Training loss: 2.154843189349242
Validation loss: 3.518603121990031

Epoch: 9| Step: 0
Training loss: 3.059073574970854
Validation loss: 3.518317289402449

Epoch: 6| Step: 1
Training loss: 4.274680784922809
Validation loss: 3.515519361652124

Epoch: 6| Step: 2
Training loss: 3.939507714320276
Validation loss: 3.51112136396257

Epoch: 6| Step: 3
Training loss: 3.543765953068424
Validation loss: 3.5039387205731596

Epoch: 6| Step: 4
Training loss: 3.3915893308428298
Validation loss: 3.5025017743465803

Epoch: 6| Step: 5
Training loss: 3.2683707197723506
Validation loss: 3.498915932177849

Epoch: 6| Step: 6
Training loss: 2.7339673419612023
Validation loss: 3.496458960723874

Epoch: 6| Step: 7
Training loss: 3.7564400369363855
Validation loss: 3.4919097996260486

Epoch: 6| Step: 8
Training loss: 3.489489987771259
Validation loss: 3.4876634091754712

Epoch: 6| Step: 9
Training loss: 4.131675866780894
Validation loss: 3.481966857867687

Epoch: 6| Step: 10
Training loss: 4.082549636268097
Validation loss: 3.4766255344318946

Epoch: 6| Step: 11
Training loss: 4.02173621494712
Validation loss: 3.4732381159554735

Epoch: 6| Step: 12
Training loss: 2.96988423414058
Validation loss: 3.46503483150965

Epoch: 6| Step: 13
Training loss: 5.3122499294888685
Validation loss: 3.460357102537998

Epoch: 10| Step: 0
Training loss: 3.464766956325563
Validation loss: 3.4553726714323596

Epoch: 6| Step: 1
Training loss: 4.2576274866626775
Validation loss: 3.4529163707939525

Epoch: 6| Step: 2
Training loss: 4.407099432266509
Validation loss: 3.449738219145393

Epoch: 6| Step: 3
Training loss: 3.72850411494753
Validation loss: 3.4488215329646064

Epoch: 6| Step: 4
Training loss: 4.424722031305615
Validation loss: 3.4450922526743075

Epoch: 6| Step: 5
Training loss: 3.0677474860826957
Validation loss: 3.4414191717022775

Epoch: 6| Step: 6
Training loss: 3.8176534873419383
Validation loss: 3.4396438655218837

Epoch: 6| Step: 7
Training loss: 3.8715837247462086
Validation loss: 3.436258330811133

Epoch: 6| Step: 8
Training loss: 3.596549627069075
Validation loss: 3.434017427172415

Epoch: 6| Step: 9
Training loss: 2.4490608517309154
Validation loss: 3.4302921995900806

Epoch: 6| Step: 10
Training loss: 2.771036833745118
Validation loss: 3.428435722712222

Epoch: 6| Step: 11
Training loss: 3.783668438246927
Validation loss: 3.4303600018124314

Epoch: 6| Step: 12
Training loss: 3.4293006904772887
Validation loss: 3.424529280058602

Epoch: 6| Step: 13
Training loss: 3.6330256573889064
Validation loss: 3.4221093241831047

Epoch: 11| Step: 0
Training loss: 3.122507550953515
Validation loss: 3.421124865922771

Epoch: 6| Step: 1
Training loss: 4.089293405545831
Validation loss: 3.4188886920835766

Epoch: 6| Step: 2
Training loss: 3.156842619904144
Validation loss: 3.4164819124713386

Epoch: 6| Step: 3
Training loss: 3.2604036341072566
Validation loss: 3.4145675371611484

Epoch: 6| Step: 4
Training loss: 3.3228393992147187
Validation loss: 3.4123739732611424

Epoch: 6| Step: 5
Training loss: 3.896886482619535
Validation loss: 3.4097860601680305

Epoch: 6| Step: 6
Training loss: 3.825127623180456
Validation loss: 3.4079737121958167

Epoch: 6| Step: 7
Training loss: 3.19138518271619
Validation loss: 3.404812289152601

Epoch: 6| Step: 8
Training loss: 3.8997039878209927
Validation loss: 3.404238848201417

Epoch: 6| Step: 9
Training loss: 4.017877206903648
Validation loss: 3.4019570597655675

Epoch: 6| Step: 10
Training loss: 4.342080872754674
Validation loss: 3.4005543359452277

Epoch: 6| Step: 11
Training loss: 3.6111616196319747
Validation loss: 3.3981378132968145

Epoch: 6| Step: 12
Training loss: 4.02597505120375
Validation loss: 3.3949538153302528

Epoch: 6| Step: 13
Training loss: 1.7478189500934895
Validation loss: 3.394266133928889

Epoch: 12| Step: 0
Training loss: 3.539219732770699
Validation loss: 3.3931519076768377

Epoch: 6| Step: 1
Training loss: 3.3897439541515566
Validation loss: 3.396178837139403

Epoch: 6| Step: 2
Training loss: 3.3411942456435515
Validation loss: 3.389172434116989

Epoch: 6| Step: 3
Training loss: 4.378008870470898
Validation loss: 3.393627748727616

Epoch: 6| Step: 4
Training loss: 2.950715712025779
Validation loss: 3.399015104048325

Epoch: 6| Step: 5
Training loss: 3.8526839969964954
Validation loss: 3.3992266213921507

Epoch: 6| Step: 6
Training loss: 3.8960335147990577
Validation loss: 3.390698906142313

Epoch: 6| Step: 7
Training loss: 4.0513323534981645
Validation loss: 3.3869949778677304

Epoch: 6| Step: 8
Training loss: 3.602103169998668
Validation loss: 3.3845979448672043

Epoch: 6| Step: 9
Training loss: 3.7015629946707143
Validation loss: 3.3821024766862124

Epoch: 6| Step: 10
Training loss: 3.7886835596998005
Validation loss: 3.380813074059402

Epoch: 6| Step: 11
Training loss: 3.115241109906513
Validation loss: 3.3791310661581755

Epoch: 6| Step: 12
Training loss: 3.481444627673495
Validation loss: 3.3790143535998305

Epoch: 6| Step: 13
Training loss: 3.0813847819904963
Validation loss: 3.3774759158181014

Epoch: 13| Step: 0
Training loss: 3.73561490595403
Validation loss: 3.3765088335347726

Epoch: 6| Step: 1
Training loss: 4.030486751722495
Validation loss: 3.3762203199518184

Epoch: 6| Step: 2
Training loss: 4.252215088680297
Validation loss: 3.372218542664681

Epoch: 6| Step: 3
Training loss: 3.2530084037590123
Validation loss: 3.3712302772652905

Epoch: 6| Step: 4
Training loss: 4.006664446313362
Validation loss: 3.370962489043995

Epoch: 6| Step: 5
Training loss: 3.7098318922509423
Validation loss: 3.36840329365577

Epoch: 6| Step: 6
Training loss: 3.3219270258908815
Validation loss: 3.367254709112117

Epoch: 6| Step: 7
Training loss: 3.374776762184438
Validation loss: 3.3655950884907075

Epoch: 6| Step: 8
Training loss: 3.5215959494248996
Validation loss: 3.364865656580889

Epoch: 6| Step: 9
Training loss: 3.6753889435975005
Validation loss: 3.3628085981261875

Epoch: 6| Step: 10
Training loss: 3.6661221504485195
Validation loss: 3.3614149374681443

Epoch: 6| Step: 11
Training loss: 3.410158766912731
Validation loss: 3.359755270693885

Epoch: 6| Step: 12
Training loss: 2.926572888666992
Validation loss: 3.3593010854447414

Epoch: 6| Step: 13
Training loss: 3.1037643649828515
Validation loss: 3.3571568600149093

Epoch: 14| Step: 0
Training loss: 4.310452182353659
Validation loss: 3.355914279278245

Epoch: 6| Step: 1
Training loss: 4.140346071457151
Validation loss: 3.355346089151191

Epoch: 6| Step: 2
Training loss: 3.384117244862763
Validation loss: 3.3543588247855576

Epoch: 6| Step: 3
Training loss: 2.2475173817133243
Validation loss: 3.3530926221395165

Epoch: 6| Step: 4
Training loss: 2.8724398616806877
Validation loss: 3.353542817624554

Epoch: 6| Step: 5
Training loss: 4.785598672149484
Validation loss: 3.3538179532868293

Epoch: 6| Step: 6
Training loss: 3.365424807801073
Validation loss: 3.351274312698244

Epoch: 6| Step: 7
Training loss: 3.1662618646049263
Validation loss: 3.350189569540226

Epoch: 6| Step: 8
Training loss: 3.8710016110692065
Validation loss: 3.3483483191723304

Epoch: 6| Step: 9
Training loss: 3.6817292139540645
Validation loss: 3.3480951220611677

Epoch: 6| Step: 10
Training loss: 3.705252635749223
Validation loss: 3.3467893437283447

Epoch: 6| Step: 11
Training loss: 3.2007041156497893
Validation loss: 3.3453912564429356

Epoch: 6| Step: 12
Training loss: 3.725217180513398
Validation loss: 3.344041538879104

Epoch: 6| Step: 13
Training loss: 2.5163122150671784
Validation loss: 3.3436637440242776

Epoch: 15| Step: 0
Training loss: 3.8775368046935323
Validation loss: 3.342563874574718

Epoch: 6| Step: 1
Training loss: 4.297825489688865
Validation loss: 3.3416551297984727

Epoch: 6| Step: 2
Training loss: 3.224607896255718
Validation loss: 3.340511074729127

Epoch: 6| Step: 3
Training loss: 3.7926697853823432
Validation loss: 3.3393092921131164

Epoch: 6| Step: 4
Training loss: 3.304755604720808
Validation loss: 3.3406333639982826

Epoch: 6| Step: 5
Training loss: 3.505865631522969
Validation loss: 3.3473465209842064

Epoch: 6| Step: 6
Training loss: 3.282469895072999
Validation loss: 3.3373734299052167

Epoch: 6| Step: 7
Training loss: 3.4289466561756887
Validation loss: 3.340109882307088

Epoch: 6| Step: 8
Training loss: 3.563318794251754
Validation loss: 3.347996486035429

Epoch: 6| Step: 9
Training loss: 3.37941650771508
Validation loss: 3.3560809695103853

Epoch: 6| Step: 10
Training loss: 4.150971926203417
Validation loss: 3.3544621664486916

Epoch: 6| Step: 11
Training loss: 3.4080881181348115
Validation loss: 3.3451879669949074

Epoch: 6| Step: 12
Training loss: 3.682837172932573
Validation loss: 3.3436038767662426

Epoch: 6| Step: 13
Training loss: 2.510552356477163
Validation loss: 3.345607029904309

Epoch: 16| Step: 0
Training loss: 3.9470348866411484
Validation loss: 3.3507025415930993

Epoch: 6| Step: 1
Training loss: 3.425013944500012
Validation loss: 3.3466420233660434

Epoch: 6| Step: 2
Training loss: 3.486268217710082
Validation loss: 3.344141584055518

Epoch: 6| Step: 3
Training loss: 3.1509463629483974
Validation loss: 3.3373571279493266

Epoch: 6| Step: 4
Training loss: 3.6615071119348537
Validation loss: 3.336707914756642

Epoch: 6| Step: 5
Training loss: 4.123808255138263
Validation loss: 3.3328002462396475

Epoch: 6| Step: 6
Training loss: 3.065749954647937
Validation loss: 3.3308612583350823

Epoch: 6| Step: 7
Training loss: 2.8165862175401983
Validation loss: 3.329449492665331

Epoch: 6| Step: 8
Training loss: 3.6666347039881453
Validation loss: 3.330736847685948

Epoch: 6| Step: 9
Training loss: 4.358779046126245
Validation loss: 3.327512783473669

Epoch: 6| Step: 10
Training loss: 3.5029575930638543
Validation loss: 3.3258311344781335

Epoch: 6| Step: 11
Training loss: 3.5713145646563484
Validation loss: 3.3234272617055667

Epoch: 6| Step: 12
Training loss: 3.2928559452524007
Validation loss: 3.322598828690995

Epoch: 6| Step: 13
Training loss: 3.697644391132539
Validation loss: 3.321286099246695

Epoch: 17| Step: 0
Training loss: 3.567055984592448
Validation loss: 3.320793840624308

Epoch: 6| Step: 1
Training loss: 3.599577317325372
Validation loss: 3.3201430966751952

Epoch: 6| Step: 2
Training loss: 3.4890554152133624
Validation loss: 3.3186060945382208

Epoch: 6| Step: 3
Training loss: 3.3162626093837884
Validation loss: 3.318075302356621

Epoch: 6| Step: 4
Training loss: 4.296800647439093
Validation loss: 3.3164641655242484

Epoch: 6| Step: 5
Training loss: 3.0724673874864754
Validation loss: 3.315547896050958

Epoch: 6| Step: 6
Training loss: 3.428026496633463
Validation loss: 3.31447944928205

Epoch: 6| Step: 7
Training loss: 3.138426661333563
Validation loss: 3.3186133529896256

Epoch: 6| Step: 8
Training loss: 3.596294795749449
Validation loss: 3.3204054991349086

Epoch: 6| Step: 9
Training loss: 4.133821706735327
Validation loss: 3.3116070702246794

Epoch: 6| Step: 10
Training loss: 3.5987377549996964
Validation loss: 3.3101668504387156

Epoch: 6| Step: 11
Training loss: 3.909283612080374
Validation loss: 3.3097604655633317

Epoch: 6| Step: 12
Training loss: 3.31853841620805
Validation loss: 3.3097275601238008

Epoch: 6| Step: 13
Training loss: 2.7155142635335396
Validation loss: 3.3120111982058082

Epoch: 18| Step: 0
Training loss: 3.757113893408667
Validation loss: 3.312579046642428

Epoch: 6| Step: 1
Training loss: 4.04510412169411
Validation loss: 3.309686580070706

Epoch: 6| Step: 2
Training loss: 3.1350091477158735
Validation loss: 3.307223314440178

Epoch: 6| Step: 3
Training loss: 3.376411001751206
Validation loss: 3.305228929131106

Epoch: 6| Step: 4
Training loss: 3.222511242002731
Validation loss: 3.303638852067632

Epoch: 6| Step: 5
Training loss: 3.1048511202835436
Validation loss: 3.302308749002781

Epoch: 6| Step: 6
Training loss: 3.0804030610447155
Validation loss: 3.301425229434305

Epoch: 6| Step: 7
Training loss: 3.610908479792651
Validation loss: 3.300808144659034

Epoch: 6| Step: 8
Training loss: 3.59943000731553
Validation loss: 3.301031641143922

Epoch: 6| Step: 9
Training loss: 3.675177464774049
Validation loss: 3.2992258800306686

Epoch: 6| Step: 10
Training loss: 4.0168445679666425
Validation loss: 3.2981883799311675

Epoch: 6| Step: 11
Training loss: 3.812059470818578
Validation loss: 3.298750262748072

Epoch: 6| Step: 12
Training loss: 3.5226296050552377
Validation loss: 3.2963348834259905

Epoch: 6| Step: 13
Training loss: 3.5129093786854857
Validation loss: 3.29595886170219

Epoch: 19| Step: 0
Training loss: 3.570758549646426
Validation loss: 3.2962672129164

Epoch: 6| Step: 1
Training loss: 3.4206267566700794
Validation loss: 3.294055419841296

Epoch: 6| Step: 2
Training loss: 4.050678836774929
Validation loss: 3.293297793543806

Epoch: 6| Step: 3
Training loss: 3.495504898689907
Validation loss: 3.2922646423431714

Epoch: 6| Step: 4
Training loss: 3.5405526297732584
Validation loss: 3.2951480549494656

Epoch: 6| Step: 5
Training loss: 3.6364234615953372
Validation loss: 3.2978636489783644

Epoch: 6| Step: 6
Training loss: 3.6014389711987405
Validation loss: 3.34335971809064

Epoch: 6| Step: 7
Training loss: 3.3363940174495057
Validation loss: 3.31272443497459

Epoch: 6| Step: 8
Training loss: 3.3217368268535363
Validation loss: 3.2882987586826413

Epoch: 6| Step: 9
Training loss: 3.987873172338818
Validation loss: 3.2875308535646464

Epoch: 6| Step: 10
Training loss: 3.9444356718249365
Validation loss: 3.285167309399056

Epoch: 6| Step: 11
Training loss: 2.4073040127626757
Validation loss: 3.2877524673364187

Epoch: 6| Step: 12
Training loss: 3.1811404645907326
Validation loss: 3.2857792391274296

Epoch: 6| Step: 13
Training loss: 3.9459387556022545
Validation loss: 3.2867828396666017

Epoch: 20| Step: 0
Training loss: 2.7534163668337466
Validation loss: 3.2861364978749568

Epoch: 6| Step: 1
Training loss: 3.219794539062904
Validation loss: 3.2872215533937394

Epoch: 6| Step: 2
Training loss: 3.382366476983111
Validation loss: 3.2845578179440524

Epoch: 6| Step: 3
Training loss: 3.763833913992048
Validation loss: 3.282849938918101

Epoch: 6| Step: 4
Training loss: 3.75975078629377
Validation loss: 3.283204708533445

Epoch: 6| Step: 5
Training loss: 2.6456566899323666
Validation loss: 3.2799208863914546

Epoch: 6| Step: 6
Training loss: 3.732533531574908
Validation loss: 3.2784259274777825

Epoch: 6| Step: 7
Training loss: 3.685424996742738
Validation loss: 3.2771164319787234

Epoch: 6| Step: 8
Training loss: 3.8865687049005833
Validation loss: 3.274883212884988

Epoch: 6| Step: 9
Training loss: 3.7830745614125862
Validation loss: 3.274501909323842

Epoch: 6| Step: 10
Training loss: 4.0723342414033326
Validation loss: 3.272318778940087

Epoch: 6| Step: 11
Training loss: 3.9685409370686164
Validation loss: 3.271497547831468

Epoch: 6| Step: 12
Training loss: 2.6825702608973505
Validation loss: 3.2705210159161586

Epoch: 6| Step: 13
Training loss: 3.6767703923672577
Validation loss: 3.2709328410495413

Epoch: 21| Step: 0
Training loss: 3.4465302198937158
Validation loss: 3.273184142686343

Epoch: 6| Step: 1
Training loss: 4.1863162516889805
Validation loss: 3.2927406685631913

Epoch: 6| Step: 2
Training loss: 3.9410647044858127
Validation loss: 3.291723532527533

Epoch: 6| Step: 3
Training loss: 4.158651246256205
Validation loss: 3.2688310737698822

Epoch: 6| Step: 4
Training loss: 4.030330346059024
Validation loss: 3.2676035731823134

Epoch: 6| Step: 5
Training loss: 3.2704790899474045
Validation loss: 3.267652819826366

Epoch: 6| Step: 6
Training loss: 3.5840878838084183
Validation loss: 3.273947790694248

Epoch: 6| Step: 7
Training loss: 3.4451252373906747
Validation loss: 3.269109524217817

Epoch: 6| Step: 8
Training loss: 3.8793399257189805
Validation loss: 3.2639086106948954

Epoch: 6| Step: 9
Training loss: 3.008213244407405
Validation loss: 3.263713820862039

Epoch: 6| Step: 10
Training loss: 3.299723185864614
Validation loss: 3.2625093642173373

Epoch: 6| Step: 11
Training loss: 2.2957784928858485
Validation loss: 3.259152193393937

Epoch: 6| Step: 12
Training loss: 2.4455783687272565
Validation loss: 3.2583563239499704

Epoch: 6| Step: 13
Training loss: 3.6870991117163014
Validation loss: 3.2585651091715526

Epoch: 22| Step: 0
Training loss: 3.421057198401735
Validation loss: 3.2584017599416564

Epoch: 6| Step: 1
Training loss: 3.474934468920739
Validation loss: 3.25755199044826

Epoch: 6| Step: 2
Training loss: 2.776202475713403
Validation loss: 3.256351294638719

Epoch: 6| Step: 3
Training loss: 4.164111511056551
Validation loss: 3.256057214059856

Epoch: 6| Step: 4
Training loss: 3.1668342412144224
Validation loss: 3.255724436678261

Epoch: 6| Step: 5
Training loss: 3.8474293288494232
Validation loss: 3.2528749454692787

Epoch: 6| Step: 6
Training loss: 3.621255321663372
Validation loss: 3.251985237031767

Epoch: 6| Step: 7
Training loss: 3.3050802371720884
Validation loss: 3.2508926695960882

Epoch: 6| Step: 8
Training loss: 2.7599128311323926
Validation loss: 3.2512593783398964

Epoch: 6| Step: 9
Training loss: 4.101135812999252
Validation loss: 3.2538279478281003

Epoch: 6| Step: 10
Training loss: 3.555668219794672
Validation loss: 3.250795412185915

Epoch: 6| Step: 11
Training loss: 3.634184416270033
Validation loss: 3.2513147681162113

Epoch: 6| Step: 12
Training loss: 3.2523906424901394
Validation loss: 3.250723288978567

Epoch: 6| Step: 13
Training loss: 3.7553193829430427
Validation loss: 3.2553700821919276

Epoch: 23| Step: 0
Training loss: 3.0333458645181004
Validation loss: 3.257854208112784

Epoch: 6| Step: 1
Training loss: 3.5897393444930463
Validation loss: 3.291949232898449

Epoch: 6| Step: 2
Training loss: 3.350177896814976
Validation loss: 3.3841747879302693

Epoch: 6| Step: 3
Training loss: 4.092443359579603
Validation loss: 3.544996686436595

Epoch: 6| Step: 4
Training loss: 3.7976415040986087
Validation loss: 3.311859157097932

Epoch: 6| Step: 5
Training loss: 3.532612630529349
Validation loss: 3.243736249641774

Epoch: 6| Step: 6
Training loss: 3.0454786964815
Validation loss: 3.323067952930786

Epoch: 6| Step: 7
Training loss: 3.744188000971785
Validation loss: 3.3946958952457433

Epoch: 6| Step: 8
Training loss: 2.981822415145863
Validation loss: 3.398426054851688

Epoch: 6| Step: 9
Training loss: 4.458639164117319
Validation loss: 3.3932749516652434

Epoch: 6| Step: 10
Training loss: 4.18094657401416
Validation loss: 3.34364632112352

Epoch: 6| Step: 11
Training loss: 3.134105234907424
Validation loss: 3.259790434237547

Epoch: 6| Step: 12
Training loss: 3.0232832521821047
Validation loss: 3.2488879075143866

Epoch: 6| Step: 13
Training loss: 3.891528411522335
Validation loss: 3.25237665763563

Epoch: 24| Step: 0
Training loss: 3.738303699645235
Validation loss: 3.2490707982788014

Epoch: 6| Step: 1
Training loss: 3.5702590771440788
Validation loss: 3.256309770395302

Epoch: 6| Step: 2
Training loss: 3.094854600784987
Validation loss: 3.260950456757015

Epoch: 6| Step: 3
Training loss: 2.9743031815693497
Validation loss: 3.2731952519351606

Epoch: 6| Step: 4
Training loss: 3.4969023211709866
Validation loss: 3.3053966862740185

Epoch: 6| Step: 5
Training loss: 3.63595792717927
Validation loss: 3.295854528841829

Epoch: 6| Step: 6
Training loss: 3.7810678989919118
Validation loss: 3.261593981395079

Epoch: 6| Step: 7
Training loss: 4.001363998548114
Validation loss: 3.250211298646953

Epoch: 6| Step: 8
Training loss: 3.091642490371266
Validation loss: 3.2362617520872567

Epoch: 6| Step: 9
Training loss: 3.871072686346894
Validation loss: 3.234275782662818

Epoch: 6| Step: 10
Training loss: 3.2812261671381493
Validation loss: 3.2348580709312116

Epoch: 6| Step: 11
Training loss: 3.199690601173386
Validation loss: 3.2306759935420226

Epoch: 6| Step: 12
Training loss: 3.861199939782397
Validation loss: 3.2260826640160865

Epoch: 6| Step: 13
Training loss: 3.10891996341627
Validation loss: 3.21698741957551

Epoch: 25| Step: 0
Training loss: 3.0158395641466678
Validation loss: 3.2117729931375876

Epoch: 6| Step: 1
Training loss: 2.7711608138679247
Validation loss: 3.2032393776486203

Epoch: 6| Step: 2
Training loss: 2.2807649070539426
Validation loss: 3.1974771484927467

Epoch: 6| Step: 3
Training loss: 3.515761987348485
Validation loss: 3.193314391987769

Epoch: 6| Step: 4
Training loss: 3.600591632123582
Validation loss: 3.2018692592474194

Epoch: 6| Step: 5
Training loss: 2.7759575368838103
Validation loss: 3.2173306993455415

Epoch: 6| Step: 6
Training loss: 3.8418328691279013
Validation loss: 3.1952970650659633

Epoch: 6| Step: 7
Training loss: 3.772517564048858
Validation loss: 3.1944833271310946

Epoch: 6| Step: 8
Training loss: 3.9355457845454445
Validation loss: 3.189507694429508

Epoch: 6| Step: 9
Training loss: 3.9998087837291028
Validation loss: 3.1844949022271285

Epoch: 6| Step: 10
Training loss: 3.249189715979171
Validation loss: 3.182940123972776

Epoch: 6| Step: 11
Training loss: 3.8858003537679675
Validation loss: 3.1799895895975028

Epoch: 6| Step: 12
Training loss: 3.7398698673965964
Validation loss: 3.180674858152645

Epoch: 6| Step: 13
Training loss: 3.4219457889010654
Validation loss: 3.180841677520699

Epoch: 26| Step: 0
Training loss: 3.156114027663645
Validation loss: 3.1790332611426697

Epoch: 6| Step: 1
Training loss: 2.9686614977042534
Validation loss: 3.178904348833274

Epoch: 6| Step: 2
Training loss: 2.9153670185596314
Validation loss: 3.1783233293849444

Epoch: 6| Step: 3
Training loss: 2.905093055061978
Validation loss: 3.177654976793905

Epoch: 6| Step: 4
Training loss: 3.620317789534062
Validation loss: 3.17679535226145

Epoch: 6| Step: 5
Training loss: 3.619030872045695
Validation loss: 3.1755755171853384

Epoch: 6| Step: 6
Training loss: 3.952866738066637
Validation loss: 3.1739770274985863

Epoch: 6| Step: 7
Training loss: 4.145920616777427
Validation loss: 3.1718588997907964

Epoch: 6| Step: 8
Training loss: 3.8542959792979983
Validation loss: 3.1711116146739524

Epoch: 6| Step: 9
Training loss: 2.8457537288525723
Validation loss: 3.170400378060219

Epoch: 6| Step: 10
Training loss: 3.322319016876968
Validation loss: 3.1675259015962642

Epoch: 6| Step: 11
Training loss: 3.505471720820699
Validation loss: 3.16628571265987

Epoch: 6| Step: 12
Training loss: 3.655130329026308
Validation loss: 3.1655706512347086

Epoch: 6| Step: 13
Training loss: 3.2859348821334824
Validation loss: 3.165813197183765

Epoch: 27| Step: 0
Training loss: 3.8190903532253113
Validation loss: 3.166352635072012

Epoch: 6| Step: 1
Training loss: 2.8603470614402107
Validation loss: 3.1640942716655474

Epoch: 6| Step: 2
Training loss: 2.9179112276793457
Validation loss: 3.163209954197758

Epoch: 6| Step: 3
Training loss: 3.7887793366326226
Validation loss: 3.160611961235472

Epoch: 6| Step: 4
Training loss: 3.2043410063209627
Validation loss: 3.160310182825673

Epoch: 6| Step: 5
Training loss: 3.1029052902456393
Validation loss: 3.163184308042647

Epoch: 6| Step: 6
Training loss: 4.274788763170242
Validation loss: 3.159366585818983

Epoch: 6| Step: 7
Training loss: 3.447976533460309
Validation loss: 3.157736123086418

Epoch: 6| Step: 8
Training loss: 3.5981118972961177
Validation loss: 3.1594137675344585

Epoch: 6| Step: 9
Training loss: 3.7579430698226166
Validation loss: 3.158696926443113

Epoch: 6| Step: 10
Training loss: 2.5415682122177947
Validation loss: 3.158188762035125

Epoch: 6| Step: 11
Training loss: 3.330861287582276
Validation loss: 3.1524657252422803

Epoch: 6| Step: 12
Training loss: 4.056918726550115
Validation loss: 3.1518403297703115

Epoch: 6| Step: 13
Training loss: 2.0865180532815724
Validation loss: 3.1528765132433545

Epoch: 28| Step: 0
Training loss: 2.1293450979378754
Validation loss: 3.151013893459975

Epoch: 6| Step: 1
Training loss: 4.114306837360714
Validation loss: 3.1501790322821424

Epoch: 6| Step: 2
Training loss: 3.3271326725851926
Validation loss: 3.147092178755523

Epoch: 6| Step: 3
Training loss: 4.226793096037348
Validation loss: 3.1472901592938607

Epoch: 6| Step: 4
Training loss: 2.2454179095304156
Validation loss: 3.1459118399263395

Epoch: 6| Step: 5
Training loss: 3.3998100227686208
Validation loss: 3.1474592400592276

Epoch: 6| Step: 6
Training loss: 3.6585399271011596
Validation loss: 3.1473826343536104

Epoch: 6| Step: 7
Training loss: 3.002563335122325
Validation loss: 3.1371126222504095

Epoch: 6| Step: 8
Training loss: 3.9925348239017726
Validation loss: 3.139537101456546

Epoch: 6| Step: 9
Training loss: 3.5391231304305637
Validation loss: 3.1338382048671787

Epoch: 6| Step: 10
Training loss: 3.7770337014041813
Validation loss: 3.138116087361568

Epoch: 6| Step: 11
Training loss: 3.0260345581461436
Validation loss: 3.1288287508687276

Epoch: 6| Step: 12
Training loss: 3.603439801666713
Validation loss: 3.1315562508914723

Epoch: 6| Step: 13
Training loss: 2.484446110697358
Validation loss: 3.130695221544818

Epoch: 29| Step: 0
Training loss: 3.440054204646811
Validation loss: 3.127710129527047

Epoch: 6| Step: 1
Training loss: 3.4277619189598463
Validation loss: 3.127787633425345

Epoch: 6| Step: 2
Training loss: 3.361377917402486
Validation loss: 3.1270426368649016

Epoch: 6| Step: 3
Training loss: 2.8343045496149952
Validation loss: 3.1297153182037962

Epoch: 6| Step: 4
Training loss: 2.9586818830016597
Validation loss: 3.1283537393624106

Epoch: 6| Step: 5
Training loss: 3.128215203904373
Validation loss: 3.128718354758472

Epoch: 6| Step: 6
Training loss: 3.486031861098903
Validation loss: 3.129483802482926

Epoch: 6| Step: 7
Training loss: 3.993805141396647
Validation loss: 3.1248202011723496

Epoch: 6| Step: 8
Training loss: 3.323608485864358
Validation loss: 3.1230386473284457

Epoch: 6| Step: 9
Training loss: 3.7200899675216292
Validation loss: 3.121334714036625

Epoch: 6| Step: 10
Training loss: 3.443353558825916
Validation loss: 3.1212935290770147

Epoch: 6| Step: 11
Training loss: 3.362230941047914
Validation loss: 3.1181831201913663

Epoch: 6| Step: 12
Training loss: 3.530766276759003
Validation loss: 3.119918463758782

Epoch: 6| Step: 13
Training loss: 3.4379021149304503
Validation loss: 3.1206730064325257

Epoch: 30| Step: 0
Training loss: 3.810706295117923
Validation loss: 3.118381063356982

Epoch: 6| Step: 1
Training loss: 3.6612713884983745
Validation loss: 3.1168505523308534

Epoch: 6| Step: 2
Training loss: 3.39953735794253
Validation loss: 3.1167703129835083

Epoch: 6| Step: 3
Training loss: 3.435346310189856
Validation loss: 3.1164305520904416

Epoch: 6| Step: 4
Training loss: 3.7072594334243547
Validation loss: 3.113804581182877

Epoch: 6| Step: 5
Training loss: 3.1963718588898153
Validation loss: 3.1129263231930695

Epoch: 6| Step: 6
Training loss: 3.275999973599873
Validation loss: 3.1123456125046816

Epoch: 6| Step: 7
Training loss: 3.5686021710724916
Validation loss: 3.11041687698341

Epoch: 6| Step: 8
Training loss: 2.186352019855019
Validation loss: 3.109476705458336

Epoch: 6| Step: 9
Training loss: 3.106036056427176
Validation loss: 3.1098688567902495

Epoch: 6| Step: 10
Training loss: 2.578788348313593
Validation loss: 3.115485704536753

Epoch: 6| Step: 11
Training loss: 3.9806637702164287
Validation loss: 3.120261769694679

Epoch: 6| Step: 12
Training loss: 3.6432380957922357
Validation loss: 3.1077833549550786

Epoch: 6| Step: 13
Training loss: 3.3671689486601055
Validation loss: 3.1124391455857556

Epoch: 31| Step: 0
Training loss: 3.3163130784173607
Validation loss: 3.105447841869802

Epoch: 6| Step: 1
Training loss: 2.9173810538108946
Validation loss: 3.1037946154484306

Epoch: 6| Step: 2
Training loss: 3.7375067822449046
Validation loss: 3.104518625694764

Epoch: 6| Step: 3
Training loss: 3.3422436039350023
Validation loss: 3.1039104225144256

Epoch: 6| Step: 4
Training loss: 3.213372043149581
Validation loss: 3.103745491331459

Epoch: 6| Step: 5
Training loss: 3.000160054069886
Validation loss: 3.1032161067971122

Epoch: 6| Step: 6
Training loss: 2.7095626290899224
Validation loss: 3.103133346265007

Epoch: 6| Step: 7
Training loss: 3.669078005111471
Validation loss: 3.102179832453111

Epoch: 6| Step: 8
Training loss: 3.683287072790213
Validation loss: 3.101553711956602

Epoch: 6| Step: 9
Training loss: 3.231274579247969
Validation loss: 3.0992939285530636

Epoch: 6| Step: 10
Training loss: 3.9643941690148994
Validation loss: 3.0987752682324596

Epoch: 6| Step: 11
Training loss: 3.142984291700093
Validation loss: 3.0985682077076073

Epoch: 6| Step: 12
Training loss: 3.781222508858613
Validation loss: 3.097768610449732

Epoch: 6| Step: 13
Training loss: 3.2312393099757437
Validation loss: 3.0964914031698454

Epoch: 32| Step: 0
Training loss: 3.457254969367717
Validation loss: 3.095421183194879

Epoch: 6| Step: 1
Training loss: 2.2476738455371734
Validation loss: 3.0959067054681135

Epoch: 6| Step: 2
Training loss: 3.8972672596186557
Validation loss: 3.095312972291675

Epoch: 6| Step: 3
Training loss: 4.750010841758803
Validation loss: 3.0935454538691936

Epoch: 6| Step: 4
Training loss: 3.5125531827219634
Validation loss: 3.092825320397982

Epoch: 6| Step: 5
Training loss: 3.1491228971512535
Validation loss: 3.092903879985911

Epoch: 6| Step: 6
Training loss: 3.0849864411635726
Validation loss: 3.0932817201035467

Epoch: 6| Step: 7
Training loss: 3.2640762672115717
Validation loss: 3.09265895584104

Epoch: 6| Step: 8
Training loss: 2.471361734414179
Validation loss: 3.0899339353186273

Epoch: 6| Step: 9
Training loss: 3.2077894534227513
Validation loss: 3.095907960828428

Epoch: 6| Step: 10
Training loss: 3.49437424980569
Validation loss: 3.091207832677884

Epoch: 6| Step: 11
Training loss: 3.3400850264639654
Validation loss: 3.0940780540296657

Epoch: 6| Step: 12
Training loss: 3.4090803793542253
Validation loss: 3.0933734668392856

Epoch: 6| Step: 13
Training loss: 2.923826657220323
Validation loss: 3.095125623609282

Epoch: 33| Step: 0
Training loss: 2.7955723291522925
Validation loss: 3.0926624108770957

Epoch: 6| Step: 1
Training loss: 3.831823438634427
Validation loss: 3.093685092702973

Epoch: 6| Step: 2
Training loss: 3.0970333670917856
Validation loss: 3.091226762974756

Epoch: 6| Step: 3
Training loss: 4.0182553000186445
Validation loss: 3.0932740961748517

Epoch: 6| Step: 4
Training loss: 2.955276593529393
Validation loss: 3.09309955619305

Epoch: 6| Step: 5
Training loss: 3.6101097250877086
Validation loss: 3.092118498565977

Epoch: 6| Step: 6
Training loss: 3.110072992087691
Validation loss: 3.093278196141885

Epoch: 6| Step: 7
Training loss: 3.280289427618068
Validation loss: 3.091836109653121

Epoch: 6| Step: 8
Training loss: 3.1365405800503736
Validation loss: 3.087280340153989

Epoch: 6| Step: 9
Training loss: 3.0279677406316043
Validation loss: 3.0873429258653347

Epoch: 6| Step: 10
Training loss: 3.8979208074425715
Validation loss: 3.086560680208121

Epoch: 6| Step: 11
Training loss: 3.270944743273693
Validation loss: 3.08451792624821

Epoch: 6| Step: 12
Training loss: 2.885079337736517
Validation loss: 3.0847074483747194

Epoch: 6| Step: 13
Training loss: 3.975692683871498
Validation loss: 3.083016517035934

Epoch: 34| Step: 0
Training loss: 3.9831490818542514
Validation loss: 3.0799385068444334

Epoch: 6| Step: 1
Training loss: 3.4311021394260934
Validation loss: 3.077066895418683

Epoch: 6| Step: 2
Training loss: 3.4941509965259714
Validation loss: 3.07461564844808

Epoch: 6| Step: 3
Training loss: 3.831260673669879
Validation loss: 3.0749135571246655

Epoch: 6| Step: 4
Training loss: 3.0996110672155903
Validation loss: 3.0730799466666197

Epoch: 6| Step: 5
Training loss: 3.166656259887728
Validation loss: 3.074078171874155

Epoch: 6| Step: 6
Training loss: 3.1608366923302214
Validation loss: 3.0713303554057165

Epoch: 6| Step: 7
Training loss: 3.591563745662905
Validation loss: 3.0718310263184465

Epoch: 6| Step: 8
Training loss: 2.3525682497411085
Validation loss: 3.0711877251694117

Epoch: 6| Step: 9
Training loss: 3.2987633671115706
Validation loss: 3.070597992117465

Epoch: 6| Step: 10
Training loss: 3.6041205666006446
Validation loss: 3.069288248943855

Epoch: 6| Step: 11
Training loss: 3.5123606806917667
Validation loss: 3.069322302966819

Epoch: 6| Step: 12
Training loss: 3.1234771069548275
Validation loss: 3.068291803333718

Epoch: 6| Step: 13
Training loss: 2.42899668400343
Validation loss: 3.068879271296683

Epoch: 35| Step: 0
Training loss: 3.826739874171642
Validation loss: 3.069084161617281

Epoch: 6| Step: 1
Training loss: 3.3648638219613343
Validation loss: 3.066501169255516

Epoch: 6| Step: 2
Training loss: 3.0238822195942943
Validation loss: 3.069483718456244

Epoch: 6| Step: 3
Training loss: 2.757546820013958
Validation loss: 3.0731547724163515

Epoch: 6| Step: 4
Training loss: 3.295309119767707
Validation loss: 3.079468468024181

Epoch: 6| Step: 5
Training loss: 2.643791331876695
Validation loss: 3.0916474092789983

Epoch: 6| Step: 6
Training loss: 3.482140752770763
Validation loss: 3.0900014253161645

Epoch: 6| Step: 7
Training loss: 3.7489517018241063
Validation loss: 3.0741338353162413

Epoch: 6| Step: 8
Training loss: 3.8301812079766537
Validation loss: 3.068040127267303

Epoch: 6| Step: 9
Training loss: 3.302711563437632
Validation loss: 3.0680354963933456

Epoch: 6| Step: 10
Training loss: 3.334083059556624
Validation loss: 3.0652419876766883

Epoch: 6| Step: 11
Training loss: 3.956059508837472
Validation loss: 3.0642376277711874

Epoch: 6| Step: 12
Training loss: 2.646917486469078
Validation loss: 3.067776640982143

Epoch: 6| Step: 13
Training loss: 3.032278299427786
Validation loss: 3.0717125801599368

Epoch: 36| Step: 0
Training loss: 2.6686624767236586
Validation loss: 3.059736892597276

Epoch: 6| Step: 1
Training loss: 3.1984209456245996
Validation loss: 3.061348238738752

Epoch: 6| Step: 2
Training loss: 3.604033509957081
Validation loss: 3.0641567209985787

Epoch: 6| Step: 3
Training loss: 3.032302830914819
Validation loss: 3.067708837530459

Epoch: 6| Step: 4
Training loss: 4.057511303710235
Validation loss: 3.0779956877706742

Epoch: 6| Step: 5
Training loss: 3.0607911616770576
Validation loss: 3.0837896508807505

Epoch: 6| Step: 6
Training loss: 3.1974638246543376
Validation loss: 3.0859343165667705

Epoch: 6| Step: 7
Training loss: 3.1877948400230585
Validation loss: 3.082333906364099

Epoch: 6| Step: 8
Training loss: 3.0451608384222255
Validation loss: 3.060370668717727

Epoch: 6| Step: 9
Training loss: 3.0277732495226424
Validation loss: 3.051233627696785

Epoch: 6| Step: 10
Training loss: 3.678981834829226
Validation loss: 3.0500169480214514

Epoch: 6| Step: 11
Training loss: 3.22668767136707
Validation loss: 3.049261223415972

Epoch: 6| Step: 12
Training loss: 3.987109633324548
Validation loss: 3.049325389818121

Epoch: 6| Step: 13
Training loss: 3.4795073991475656
Validation loss: 3.050905632865851

Epoch: 37| Step: 0
Training loss: 3.3585930912348156
Validation loss: 3.0504239793438055

Epoch: 6| Step: 1
Training loss: 2.9734105520385374
Validation loss: 3.051875290077557

Epoch: 6| Step: 2
Training loss: 2.9114445030325204
Validation loss: 3.050631251237444

Epoch: 6| Step: 3
Training loss: 2.9978113137812175
Validation loss: 3.0489711983447276

Epoch: 6| Step: 4
Training loss: 3.5085360114358926
Validation loss: 3.047978605871673

Epoch: 6| Step: 5
Training loss: 3.7375057615918306
Validation loss: 3.0498386320868285

Epoch: 6| Step: 6
Training loss: 3.130099446481028
Validation loss: 3.048417086962936

Epoch: 6| Step: 7
Training loss: 2.7088963632512666
Validation loss: 3.0455214269110855

Epoch: 6| Step: 8
Training loss: 2.8869463633413024
Validation loss: 3.0439144874644892

Epoch: 6| Step: 9
Training loss: 3.4005995614478537
Validation loss: 3.0438812988272783

Epoch: 6| Step: 10
Training loss: 3.8422804093267953
Validation loss: 3.043628620079845

Epoch: 6| Step: 11
Training loss: 3.527241279744303
Validation loss: 3.0443064255977976

Epoch: 6| Step: 12
Training loss: 3.7442868263383113
Validation loss: 3.0438451232553354

Epoch: 6| Step: 13
Training loss: 3.6954051483992156
Validation loss: 3.044077758903758

Epoch: 38| Step: 0
Training loss: 3.496328062591474
Validation loss: 3.044598563090515

Epoch: 6| Step: 1
Training loss: 3.0151222877311534
Validation loss: 3.0449662667894715

Epoch: 6| Step: 2
Training loss: 2.944152441680545
Validation loss: 3.042195804391488

Epoch: 6| Step: 3
Training loss: 3.028651746162991
Validation loss: 3.0420148624621994

Epoch: 6| Step: 4
Training loss: 2.9047294658812026
Validation loss: 3.042767868310245

Epoch: 6| Step: 5
Training loss: 3.351176917742301
Validation loss: 3.041026606174299

Epoch: 6| Step: 6
Training loss: 3.6512382915117123
Validation loss: 3.041259686703475

Epoch: 6| Step: 7
Training loss: 3.1536095782757227
Validation loss: 3.0395970154838876

Epoch: 6| Step: 8
Training loss: 3.3803799452907546
Validation loss: 3.038497439103464

Epoch: 6| Step: 9
Training loss: 3.838252159736725
Validation loss: 3.036753900539586

Epoch: 6| Step: 10
Training loss: 3.2201774913134096
Validation loss: 3.0362293772925715

Epoch: 6| Step: 11
Training loss: 3.0129422128366596
Validation loss: 3.0361067278944818

Epoch: 6| Step: 12
Training loss: 3.9826157220094407
Validation loss: 3.0339748025684794

Epoch: 6| Step: 13
Training loss: 3.1326942326394387
Validation loss: 3.033531822851181

Epoch: 39| Step: 0
Training loss: 2.9781243339289656
Validation loss: 3.0330331258165395

Epoch: 6| Step: 1
Training loss: 2.6983491018336028
Validation loss: 3.03251834370637

Epoch: 6| Step: 2
Training loss: 3.0135016204743548
Validation loss: 3.035330174222585

Epoch: 6| Step: 3
Training loss: 3.208480303151773
Validation loss: 3.0310507631596986

Epoch: 6| Step: 4
Training loss: 2.9734376539509944
Validation loss: 3.032032168707202

Epoch: 6| Step: 5
Training loss: 3.265220215455601
Validation loss: 3.0337639048933167

Epoch: 6| Step: 6
Training loss: 4.3481780610459735
Validation loss: 3.0382670391607713

Epoch: 6| Step: 7
Training loss: 3.3231338541609934
Validation loss: 3.0294972621210223

Epoch: 6| Step: 8
Training loss: 3.6294094759231696
Validation loss: 3.0255602681251403

Epoch: 6| Step: 9
Training loss: 3.4282270724071577
Validation loss: 3.0258933757754725

Epoch: 6| Step: 10
Training loss: 3.1058092998219244
Validation loss: 3.027204272457372

Epoch: 6| Step: 11
Training loss: 3.557202466048348
Validation loss: 3.0244949593143144

Epoch: 6| Step: 12
Training loss: 3.389545602867868
Validation loss: 3.0256246761976286

Epoch: 6| Step: 13
Training loss: 2.832307704717851
Validation loss: 3.0241041157395605

Epoch: 40| Step: 0
Training loss: 3.9298615009470725
Validation loss: 3.023940613646025

Epoch: 6| Step: 1
Training loss: 3.4838972234534165
Validation loss: 3.022788848575284

Epoch: 6| Step: 2
Training loss: 1.6845553101687039
Validation loss: 3.0222061302422993

Epoch: 6| Step: 3
Training loss: 3.1675722600798153
Validation loss: 3.0201272231867247

Epoch: 6| Step: 4
Training loss: 3.026192447409742
Validation loss: 3.0204113672272244

Epoch: 6| Step: 5
Training loss: 4.458906095321423
Validation loss: 3.019667956859512

Epoch: 6| Step: 6
Training loss: 3.224943405955933
Validation loss: 3.02054949068007

Epoch: 6| Step: 7
Training loss: 3.6817018863023185
Validation loss: 3.020367927612009

Epoch: 6| Step: 8
Training loss: 3.4852646859356713
Validation loss: 3.022504332615218

Epoch: 6| Step: 9
Training loss: 2.5148937987093256
Validation loss: 3.020027004374653

Epoch: 6| Step: 10
Training loss: 3.110478651004455
Validation loss: 3.0256797525106953

Epoch: 6| Step: 11
Training loss: 3.731419882429376
Validation loss: 3.023961813193639

Epoch: 6| Step: 12
Training loss: 2.391580826905822
Validation loss: 3.0190397551146693

Epoch: 6| Step: 13
Training loss: 3.2750562066067057
Validation loss: 3.0163474017057355

Epoch: 41| Step: 0
Training loss: 3.036244947068842
Validation loss: 3.0151088170173836

Epoch: 6| Step: 1
Training loss: 2.402038852602943
Validation loss: 3.0103306730188417

Epoch: 6| Step: 2
Training loss: 3.7469737239554664
Validation loss: 3.0114378439226512

Epoch: 6| Step: 3
Training loss: 2.130719062248471
Validation loss: 3.0089851061482236

Epoch: 6| Step: 4
Training loss: 3.3537756808474604
Validation loss: 3.0073065458061685

Epoch: 6| Step: 5
Training loss: 4.434107383518606
Validation loss: 3.0078474008948217

Epoch: 6| Step: 6
Training loss: 4.043238357179041
Validation loss: 3.0073198315322625

Epoch: 6| Step: 7
Training loss: 3.5519700614939063
Validation loss: 3.008177936985289

Epoch: 6| Step: 8
Training loss: 1.9025663836974547
Validation loss: 3.0066756504785825

Epoch: 6| Step: 9
Training loss: 3.032320443142093
Validation loss: 3.0059653367853496

Epoch: 6| Step: 10
Training loss: 3.58738423336801
Validation loss: 3.0065036115731583

Epoch: 6| Step: 11
Training loss: 3.5045621294241798
Validation loss: 3.004490649771991

Epoch: 6| Step: 12
Training loss: 3.1772877778525217
Validation loss: 3.0029422857188854

Epoch: 6| Step: 13
Training loss: 3.011925834928115
Validation loss: 3.003084368659932

Epoch: 42| Step: 0
Training loss: 3.234005377100318
Validation loss: 3.0010826471411405

Epoch: 6| Step: 1
Training loss: 3.691680090702585
Validation loss: 3.0037234675429723

Epoch: 6| Step: 2
Training loss: 3.3806684011451016
Validation loss: 3.0031605294993433

Epoch: 6| Step: 3
Training loss: 2.6828735806727506
Validation loss: 3.0011464375847887

Epoch: 6| Step: 4
Training loss: 3.61507015999105
Validation loss: 2.999398143789031

Epoch: 6| Step: 5
Training loss: 2.676454334111043
Validation loss: 2.9983443610397424

Epoch: 6| Step: 6
Training loss: 3.4910561864741076
Validation loss: 2.9986715947677647

Epoch: 6| Step: 7
Training loss: 2.8036488599558327
Validation loss: 2.9957151696669753

Epoch: 6| Step: 8
Training loss: 3.5016393909242374
Validation loss: 2.995758205988586

Epoch: 6| Step: 9
Training loss: 3.399102788239074
Validation loss: 2.9975845113457193

Epoch: 6| Step: 10
Training loss: 2.9868915592683662
Validation loss: 2.996224512686261

Epoch: 6| Step: 11
Training loss: 3.833787614745447
Validation loss: 2.9950016958555894

Epoch: 6| Step: 12
Training loss: 3.1373290421920896
Validation loss: 2.993882105856644

Epoch: 6| Step: 13
Training loss: 3.1761181642088787
Validation loss: 2.9941303606268574

Epoch: 43| Step: 0
Training loss: 2.8861864809734836
Validation loss: 2.9948730942765414

Epoch: 6| Step: 1
Training loss: 3.4048282560932805
Validation loss: 2.993708357229235

Epoch: 6| Step: 2
Training loss: 3.2647303765797715
Validation loss: 2.992355165057521

Epoch: 6| Step: 3
Training loss: 2.266154260312118
Validation loss: 2.992368332969649

Epoch: 6| Step: 4
Training loss: 2.6259684819685547
Validation loss: 2.9937685892077432

Epoch: 6| Step: 5
Training loss: 3.024806932325473
Validation loss: 2.991181447710195

Epoch: 6| Step: 6
Training loss: 3.070229876234013
Validation loss: 2.993984256029326

Epoch: 6| Step: 7
Training loss: 3.139934558793909
Validation loss: 3.007344841102391

Epoch: 6| Step: 8
Training loss: 3.7940488744713665
Validation loss: 3.0276219051048177

Epoch: 6| Step: 9
Training loss: 3.6728085893441222
Validation loss: 3.021548204768403

Epoch: 6| Step: 10
Training loss: 2.854108118352407
Validation loss: 2.9859392400147002

Epoch: 6| Step: 11
Training loss: 3.879123831463634
Validation loss: 2.9869672764285617

Epoch: 6| Step: 12
Training loss: 3.5419582620893912
Validation loss: 2.9878905050844917

Epoch: 6| Step: 13
Training loss: 4.428414082698432
Validation loss: 2.9860265038890645

Epoch: 44| Step: 0
Training loss: 3.2252011140318286
Validation loss: 2.99049356781485

Epoch: 6| Step: 1
Training loss: 3.3200561424472124
Validation loss: 3.000350268572048

Epoch: 6| Step: 2
Training loss: 3.082715943531122
Validation loss: 3.006346647881738

Epoch: 6| Step: 3
Training loss: 4.277363816305671
Validation loss: 3.0066434210055646

Epoch: 6| Step: 4
Training loss: 3.1683955910425534
Validation loss: 2.992264398270889

Epoch: 6| Step: 5
Training loss: 3.4062001635904133
Validation loss: 2.9882805619656905

Epoch: 6| Step: 6
Training loss: 3.101735453743953
Validation loss: 2.986528326410027

Epoch: 6| Step: 7
Training loss: 3.5513907435079193
Validation loss: 2.985574304462148

Epoch: 6| Step: 8
Training loss: 2.702475938434039
Validation loss: 2.9817114795853725

Epoch: 6| Step: 9
Training loss: 3.2784656154964
Validation loss: 2.97844852667129

Epoch: 6| Step: 10
Training loss: 2.3448475620200404
Validation loss: 2.9781044824025993

Epoch: 6| Step: 11
Training loss: 3.299309721913552
Validation loss: 2.9788798314532854

Epoch: 6| Step: 12
Training loss: 2.89529503592151
Validation loss: 2.9759106341993107

Epoch: 6| Step: 13
Training loss: 4.189137608311189
Validation loss: 2.9781546089520177

Epoch: 45| Step: 0
Training loss: 3.507629933523392
Validation loss: 2.977394069450986

Epoch: 6| Step: 1
Training loss: 2.9784543090512496
Validation loss: 2.9789431920054215

Epoch: 6| Step: 2
Training loss: 3.73222129358667
Validation loss: 2.9786461041814385

Epoch: 6| Step: 3
Training loss: 3.171944453038101
Validation loss: 2.9852306700777724

Epoch: 6| Step: 4
Training loss: 2.8221677136184313
Validation loss: 2.982080556121062

Epoch: 6| Step: 5
Training loss: 3.725010834748557
Validation loss: 2.979240348301174

Epoch: 6| Step: 6
Training loss: 3.3965062759860314
Validation loss: 2.975546666553919

Epoch: 6| Step: 7
Training loss: 2.901720023616852
Validation loss: 2.97172077200119

Epoch: 6| Step: 8
Training loss: 2.9446564204209245
Validation loss: 2.971440323443191

Epoch: 6| Step: 9
Training loss: 3.0961433169882584
Validation loss: 2.971755247898464

Epoch: 6| Step: 10
Training loss: 2.9394894619847065
Validation loss: 2.9713255387379895

Epoch: 6| Step: 11
Training loss: 3.7823180904881966
Validation loss: 2.970584097739491

Epoch: 6| Step: 12
Training loss: 3.3433438348525977
Validation loss: 2.9695182596374425

Epoch: 6| Step: 13
Training loss: 3.043813879451117
Validation loss: 2.9686961973849213

Epoch: 46| Step: 0
Training loss: 3.2758110381650027
Validation loss: 2.967413801381318

Epoch: 6| Step: 1
Training loss: 3.027754193446459
Validation loss: 2.9672178234040474

Epoch: 6| Step: 2
Training loss: 2.726881123373504
Validation loss: 2.9675042371594103

Epoch: 6| Step: 3
Training loss: 2.5260034512385263
Validation loss: 2.96756957786637

Epoch: 6| Step: 4
Training loss: 2.7732335835026576
Validation loss: 2.9663205718380943

Epoch: 6| Step: 5
Training loss: 3.2130553600434744
Validation loss: 2.965476880532129

Epoch: 6| Step: 6
Training loss: 4.130741400871522
Validation loss: 2.970644293729217

Epoch: 6| Step: 7
Training loss: 3.9892899660194416
Validation loss: 2.969864122952046

Epoch: 6| Step: 8
Training loss: 2.9104038210025345
Validation loss: 2.9628757993395456

Epoch: 6| Step: 9
Training loss: 3.1699357141179734
Validation loss: 2.9624792808997893

Epoch: 6| Step: 10
Training loss: 2.867616543384519
Validation loss: 2.958610066479857

Epoch: 6| Step: 11
Training loss: 3.778587854397576
Validation loss: 2.96001975592251

Epoch: 6| Step: 12
Training loss: 3.112651229922521
Validation loss: 2.959689350500337

Epoch: 6| Step: 13
Training loss: 3.789718225115034
Validation loss: 2.9596189530177104

Epoch: 47| Step: 0
Training loss: 3.2387315094279012
Validation loss: 2.9586019698769466

Epoch: 6| Step: 1
Training loss: 3.4708064401476935
Validation loss: 2.9574333796698076

Epoch: 6| Step: 2
Training loss: 3.574867864981193
Validation loss: 2.9565668275506436

Epoch: 6| Step: 3
Training loss: 2.879279558882437
Validation loss: 2.9530491799381453

Epoch: 6| Step: 4
Training loss: 3.1796713038976336
Validation loss: 2.9554953961852637

Epoch: 6| Step: 5
Training loss: 2.1965948411875567
Validation loss: 2.960649287201681

Epoch: 6| Step: 6
Training loss: 3.140216250414171
Validation loss: 2.9751279633651837

Epoch: 6| Step: 7
Training loss: 2.569896536796543
Validation loss: 2.9535808846155867

Epoch: 6| Step: 8
Training loss: 2.799710163374303
Validation loss: 2.95171041096159

Epoch: 6| Step: 9
Training loss: 3.7491269684845125
Validation loss: 2.952042103688764

Epoch: 6| Step: 10
Training loss: 2.712923844795259
Validation loss: 2.950574933773325

Epoch: 6| Step: 11
Training loss: 3.6181207033923575
Validation loss: 2.9520161210303097

Epoch: 6| Step: 12
Training loss: 3.9437835450113568
Validation loss: 2.949692720768711

Epoch: 6| Step: 13
Training loss: 4.176327732964678
Validation loss: 2.9478256354732317

Epoch: 48| Step: 0
Training loss: 2.6175163404069446
Validation loss: 2.9482216469452567

Epoch: 6| Step: 1
Training loss: 3.5422641175426453
Validation loss: 2.9477657971021562

Epoch: 6| Step: 2
Training loss: 2.7865387426640376
Validation loss: 2.949642857991815

Epoch: 6| Step: 3
Training loss: 2.7961851153564243
Validation loss: 2.9499900139162882

Epoch: 6| Step: 4
Training loss: 3.0352797317424893
Validation loss: 2.9556374879017313

Epoch: 6| Step: 5
Training loss: 3.414674612380374
Validation loss: 2.9531311818127772

Epoch: 6| Step: 6
Training loss: 3.3764594419357548
Validation loss: 2.954303807198717

Epoch: 6| Step: 7
Training loss: 3.3096063318709747
Validation loss: 2.955293887541832

Epoch: 6| Step: 8
Training loss: 4.382426035768815
Validation loss: 2.949325787110117

Epoch: 6| Step: 9
Training loss: 2.855186855528105
Validation loss: 2.9463760224086526

Epoch: 6| Step: 10
Training loss: 3.766750942964082
Validation loss: 2.9463883891084928

Epoch: 6| Step: 11
Training loss: 3.2626755824287463
Validation loss: 2.947051709535874

Epoch: 6| Step: 12
Training loss: 2.8820505349932506
Validation loss: 2.946063984683925

Epoch: 6| Step: 13
Training loss: 2.519429333421991
Validation loss: 2.9512659504182444

Epoch: 49| Step: 0
Training loss: 3.6007049929886237
Validation loss: 2.953226996085918

Epoch: 6| Step: 1
Training loss: 3.4036984467568625
Validation loss: 2.9573642045138357

Epoch: 6| Step: 2
Training loss: 3.5698458243451814
Validation loss: 2.9431964863953444

Epoch: 6| Step: 3
Training loss: 3.423587457671012
Validation loss: 2.941240683698528

Epoch: 6| Step: 4
Training loss: 3.5680731296828334
Validation loss: 2.9644747508538676

Epoch: 6| Step: 5
Training loss: 3.203309291097054
Validation loss: 3.012106752758671

Epoch: 6| Step: 6
Training loss: 3.3072127845948907
Validation loss: 3.0048361546180455

Epoch: 6| Step: 7
Training loss: 2.3838106659969456
Validation loss: 2.9644844935335555

Epoch: 6| Step: 8
Training loss: 2.9617633724649743
Validation loss: 2.940884102013476

Epoch: 6| Step: 9
Training loss: 3.18777075720835
Validation loss: 2.941444358267781

Epoch: 6| Step: 10
Training loss: 2.6298912436332085
Validation loss: 2.9385974595127635

Epoch: 6| Step: 11
Training loss: 3.2578455788376095
Validation loss: 2.939741570588055

Epoch: 6| Step: 12
Training loss: 3.4926716513697973
Validation loss: 2.9481984158245127

Epoch: 6| Step: 13
Training loss: 3.212446837226546
Validation loss: 2.9490778394368524

Epoch: 50| Step: 0
Training loss: 3.2350671000058915
Validation loss: 2.952078183242928

Epoch: 6| Step: 1
Training loss: 3.154654222211802
Validation loss: 2.947705838366496

Epoch: 6| Step: 2
Training loss: 2.753501310404654
Validation loss: 2.9489607472860704

Epoch: 6| Step: 3
Training loss: 3.7692057572797015
Validation loss: 2.9495902904431133

Epoch: 6| Step: 4
Training loss: 3.681054381789861
Validation loss: 2.94915532977912

Epoch: 6| Step: 5
Training loss: 3.1660525245092916
Validation loss: 2.947303918454511

Epoch: 6| Step: 6
Training loss: 2.972347930278003
Validation loss: 2.9473443520239027

Epoch: 6| Step: 7
Training loss: 3.085431252976754
Validation loss: 2.9404528021399026

Epoch: 6| Step: 8
Training loss: 3.6276934910073737
Validation loss: 2.942528373725113

Epoch: 6| Step: 9
Training loss: 2.8062173820248177
Validation loss: 2.93983441769778

Epoch: 6| Step: 10
Training loss: 3.709554121260998
Validation loss: 2.9392458044207572

Epoch: 6| Step: 11
Training loss: 2.9089095549516766
Validation loss: 2.93522709185776

Epoch: 6| Step: 12
Training loss: 2.9797469598751594
Validation loss: 2.935258969203159

Epoch: 6| Step: 13
Training loss: 2.931023946476196
Validation loss: 2.9311131199002878

Epoch: 51| Step: 0
Training loss: 2.582303077597653
Validation loss: 2.927932767233206

Epoch: 6| Step: 1
Training loss: 3.4352719108538503
Validation loss: 2.925154251337938

Epoch: 6| Step: 2
Training loss: 3.503280192467282
Validation loss: 2.9274453383159047

Epoch: 6| Step: 3
Training loss: 2.2695056171906605
Validation loss: 2.923181497588014

Epoch: 6| Step: 4
Training loss: 2.453227630217804
Validation loss: 2.923193803676331

Epoch: 6| Step: 5
Training loss: 3.105131848372363
Validation loss: 2.922556897315711

Epoch: 6| Step: 6
Training loss: 3.6878165416309097
Validation loss: 2.923932916777791

Epoch: 6| Step: 7
Training loss: 3.546030624644684
Validation loss: 2.9227474406756895

Epoch: 6| Step: 8
Training loss: 2.6076915130068694
Validation loss: 2.920662050469482

Epoch: 6| Step: 9
Training loss: 3.4604028844281403
Validation loss: 2.9222412178187103

Epoch: 6| Step: 10
Training loss: 3.151572510112852
Validation loss: 2.9207628849322447

Epoch: 6| Step: 11
Training loss: 2.984193007427404
Validation loss: 2.9236372767728858

Epoch: 6| Step: 12
Training loss: 3.79619350518176
Validation loss: 2.924194520784171

Epoch: 6| Step: 13
Training loss: 4.369947731526034
Validation loss: 2.9218355274544767

Epoch: 52| Step: 0
Training loss: 3.212724990981298
Validation loss: 2.9233486058434925

Epoch: 6| Step: 1
Training loss: 3.3567159375460975
Validation loss: 2.9236178690142807

Epoch: 6| Step: 2
Training loss: 3.1038256634981365
Validation loss: 2.922537248171042

Epoch: 6| Step: 3
Training loss: 3.606385681667523
Validation loss: 2.928206760304574

Epoch: 6| Step: 4
Training loss: 3.2561841961733644
Validation loss: 2.9217704511644773

Epoch: 6| Step: 5
Training loss: 1.9769281946165678
Validation loss: 2.916872903824809

Epoch: 6| Step: 6
Training loss: 3.2244162452575296
Validation loss: 2.9323445376216375

Epoch: 6| Step: 7
Training loss: 2.9284061325423796
Validation loss: 2.9138294111244045

Epoch: 6| Step: 8
Training loss: 2.8425612427783
Validation loss: 2.9155677436231127

Epoch: 6| Step: 9
Training loss: 3.5638809204673207
Validation loss: 2.918738755672651

Epoch: 6| Step: 10
Training loss: 3.114612404848167
Validation loss: 2.918746478014572

Epoch: 6| Step: 11
Training loss: 4.266762606217667
Validation loss: 2.916004223167953

Epoch: 6| Step: 12
Training loss: 2.7334116955510908
Validation loss: 2.9190385909841883

Epoch: 6| Step: 13
Training loss: 3.360852151848467
Validation loss: 2.923534272039053

Epoch: 53| Step: 0
Training loss: 3.860794732894182
Validation loss: 2.9325613276237674

Epoch: 6| Step: 1
Training loss: 3.270525454145835
Validation loss: 2.940217337655588

Epoch: 6| Step: 2
Training loss: 3.214485676918511
Validation loss: 2.938352474670322

Epoch: 6| Step: 3
Training loss: 3.3556834675098086
Validation loss: 2.9350456347111797

Epoch: 6| Step: 4
Training loss: 2.5205673098423786
Validation loss: 2.9208487826964467

Epoch: 6| Step: 5
Training loss: 2.8544225276417285
Validation loss: 2.919294540834306

Epoch: 6| Step: 6
Training loss: 2.964292693663373
Validation loss: 2.9084040324467453

Epoch: 6| Step: 7
Training loss: 3.53710667880212
Validation loss: 2.907253537144576

Epoch: 6| Step: 8
Training loss: 3.222748874142404
Validation loss: 2.909209855185705

Epoch: 6| Step: 9
Training loss: 3.0495305935209625
Validation loss: 2.908887853581699

Epoch: 6| Step: 10
Training loss: 3.0849975699688756
Validation loss: 2.907354412513632

Epoch: 6| Step: 11
Training loss: 3.0727434702195455
Validation loss: 2.907928160472272

Epoch: 6| Step: 12
Training loss: 3.305556307097135
Validation loss: 2.9085936150044516

Epoch: 6| Step: 13
Training loss: 3.5683087291579008
Validation loss: 2.9085292851578957

Epoch: 54| Step: 0
Training loss: 3.177143100530727
Validation loss: 2.907965647789165

Epoch: 6| Step: 1
Training loss: 3.0353912694562797
Validation loss: 2.910710821161984

Epoch: 6| Step: 2
Training loss: 3.1964591284385024
Validation loss: 2.912906580992907

Epoch: 6| Step: 3
Training loss: 3.032341200278627
Validation loss: 2.9116019285956933

Epoch: 6| Step: 4
Training loss: 3.032079524162374
Validation loss: 2.90855493353294

Epoch: 6| Step: 5
Training loss: 3.2761558589632513
Validation loss: 2.9091678817104056

Epoch: 6| Step: 6
Training loss: 2.604227070425768
Validation loss: 2.9085036339123853

Epoch: 6| Step: 7
Training loss: 3.4236434477610924
Validation loss: 2.9067710861838982

Epoch: 6| Step: 8
Training loss: 3.433989796022908
Validation loss: 2.9073874595949474

Epoch: 6| Step: 9
Training loss: 3.433495703502726
Validation loss: 2.912602387709909

Epoch: 6| Step: 10
Training loss: 3.0429859870941516
Validation loss: 2.9149559934372435

Epoch: 6| Step: 11
Training loss: 3.0472881330488444
Validation loss: 2.920690018405879

Epoch: 6| Step: 12
Training loss: 3.0867483112036767
Validation loss: 2.930538802306465

Epoch: 6| Step: 13
Training loss: 4.248473622602386
Validation loss: 2.9413393946129185

Epoch: 55| Step: 0
Training loss: 3.0705195155738743
Validation loss: 2.915139375611234

Epoch: 6| Step: 1
Training loss: 2.429595062361879
Validation loss: 2.9116012039511308

Epoch: 6| Step: 2
Training loss: 3.208000231868897
Validation loss: 2.9079908629878415

Epoch: 6| Step: 3
Training loss: 2.6794419245618313
Validation loss: 2.9091853987108687

Epoch: 6| Step: 4
Training loss: 3.033673605505175
Validation loss: 2.912205784504001

Epoch: 6| Step: 5
Training loss: 3.175016904770911
Validation loss: 2.9098746756568032

Epoch: 6| Step: 6
Training loss: 2.5889287054177137
Validation loss: 2.9048025597953058

Epoch: 6| Step: 7
Training loss: 3.9598331755360823
Validation loss: 2.899812501850976

Epoch: 6| Step: 8
Training loss: 3.717862832808535
Validation loss: 2.898721448250794

Epoch: 6| Step: 9
Training loss: 2.667131999264653
Validation loss: 2.9039057718973282

Epoch: 6| Step: 10
Training loss: 3.311137512992151
Validation loss: 2.904046743269753

Epoch: 6| Step: 11
Training loss: 3.2906556971205885
Validation loss: 2.907577305057573

Epoch: 6| Step: 12
Training loss: 3.4508369425761116
Validation loss: 2.9037643301753864

Epoch: 6| Step: 13
Training loss: 4.121838832230916
Validation loss: 2.8982773850945622

Epoch: 56| Step: 0
Training loss: 2.922045697616648
Validation loss: 2.8942543887017935

Epoch: 6| Step: 1
Training loss: 2.604913243406826
Validation loss: 2.889508098057554

Epoch: 6| Step: 2
Training loss: 3.9794327545408525
Validation loss: 2.8887218681320936

Epoch: 6| Step: 3
Training loss: 2.7962754954990605
Validation loss: 2.8885956908742267

Epoch: 6| Step: 4
Training loss: 3.6140212502374647
Validation loss: 2.8884608474478384

Epoch: 6| Step: 5
Training loss: 3.6923511728758807
Validation loss: 2.8856099701717914

Epoch: 6| Step: 6
Training loss: 2.629213493724132
Validation loss: 2.886507663144006

Epoch: 6| Step: 7
Training loss: 2.5122041366753245
Validation loss: 2.8833328854756526

Epoch: 6| Step: 8
Training loss: 3.426246115658996
Validation loss: 2.8848227293928397

Epoch: 6| Step: 9
Training loss: 2.4816845414863162
Validation loss: 2.8861722423714298

Epoch: 6| Step: 10
Training loss: 2.626801009187814
Validation loss: 2.8845145283935

Epoch: 6| Step: 11
Training loss: 4.035521380881503
Validation loss: 2.8850072209859

Epoch: 6| Step: 12
Training loss: 3.0725054104713125
Validation loss: 2.8858841474086225

Epoch: 6| Step: 13
Training loss: 3.739170841571497
Validation loss: 2.882918086378918

Epoch: 57| Step: 0
Training loss: 2.8439927888287895
Validation loss: 2.8829018574844834

Epoch: 6| Step: 1
Training loss: 3.1040011026832746
Validation loss: 2.878866513337655

Epoch: 6| Step: 2
Training loss: 2.9823644766190007
Validation loss: 2.8865397730009095

Epoch: 6| Step: 3
Training loss: 2.9376469027533494
Validation loss: 2.8869688423686988

Epoch: 6| Step: 4
Training loss: 3.4333795994971434
Validation loss: 2.884212833757785

Epoch: 6| Step: 5
Training loss: 3.309937835755001
Validation loss: 2.890317546321236

Epoch: 6| Step: 6
Training loss: 3.134890659013441
Validation loss: 2.8771759508970147

Epoch: 6| Step: 7
Training loss: 3.582307727922287
Validation loss: 2.868853941973208

Epoch: 6| Step: 8
Training loss: 3.3691150078283347
Validation loss: 2.8594043231915016

Epoch: 6| Step: 9
Training loss: 3.1361971334758234
Validation loss: 2.858661584354443

Epoch: 6| Step: 10
Training loss: 2.706127461749243
Validation loss: 2.864049944995477

Epoch: 6| Step: 11
Training loss: 3.193818953454671
Validation loss: 2.863987120094172

Epoch: 6| Step: 12
Training loss: 2.9551213696852305
Validation loss: 2.864338825489095

Epoch: 6| Step: 13
Training loss: 3.835578979395123
Validation loss: 2.8661496912734474

Epoch: 58| Step: 0
Training loss: 3.5684745615474314
Validation loss: 2.869045963704231

Epoch: 6| Step: 1
Training loss: 3.67178110854858
Validation loss: 2.866787346758199

Epoch: 6| Step: 2
Training loss: 3.2407467501952953
Validation loss: 2.867982043287957

Epoch: 6| Step: 3
Training loss: 3.1006969714370993
Validation loss: 2.867652576601895

Epoch: 6| Step: 4
Training loss: 2.9976412719320553
Validation loss: 2.865900391475643

Epoch: 6| Step: 5
Training loss: 3.4207402268136087
Validation loss: 2.8655707793900502

Epoch: 6| Step: 6
Training loss: 3.4714961822985457
Validation loss: 2.8628723159616567

Epoch: 6| Step: 7
Training loss: 2.6803969620175514
Validation loss: 2.8631569780095596

Epoch: 6| Step: 8
Training loss: 3.009019961066831
Validation loss: 2.8625629817197225

Epoch: 6| Step: 9
Training loss: 2.4217182723985755
Validation loss: 2.860583318968364

Epoch: 6| Step: 10
Training loss: 2.651641707847358
Validation loss: 2.8607236542950827

Epoch: 6| Step: 11
Training loss: 3.77493300725848
Validation loss: 2.8606322095757992

Epoch: 6| Step: 12
Training loss: 2.8223318547592293
Validation loss: 2.8596280306138904

Epoch: 6| Step: 13
Training loss: 3.4132256431529195
Validation loss: 2.857756310446236

Epoch: 59| Step: 0
Training loss: 3.0059723216271754
Validation loss: 2.8557257786514536

Epoch: 6| Step: 1
Training loss: 3.080031680650151
Validation loss: 2.8544360678076215

Epoch: 6| Step: 2
Training loss: 3.4905500904265585
Validation loss: 2.8531932268120404

Epoch: 6| Step: 3
Training loss: 3.4474275977794155
Validation loss: 2.849835238455381

Epoch: 6| Step: 4
Training loss: 3.449022160043863
Validation loss: 2.850591959461779

Epoch: 6| Step: 5
Training loss: 3.6065052067782397
Validation loss: 2.846498712661042

Epoch: 6| Step: 6
Training loss: 3.310859561883165
Validation loss: 2.846343482236828

Epoch: 6| Step: 7
Training loss: 3.2097168618702216
Validation loss: 2.8450296005126976

Epoch: 6| Step: 8
Training loss: 3.5213207983361756
Validation loss: 2.8426323855541176

Epoch: 6| Step: 9
Training loss: 2.882486133657885
Validation loss: 2.844007201645312

Epoch: 6| Step: 10
Training loss: 3.181876813670361
Validation loss: 2.8421392322177716

Epoch: 6| Step: 11
Training loss: 2.6051550451841137
Validation loss: 2.8453029785602415

Epoch: 6| Step: 12
Training loss: 2.240497018306002
Validation loss: 2.8557193343649594

Epoch: 6| Step: 13
Training loss: 2.452642016639585
Validation loss: 2.885058423904477

Epoch: 60| Step: 0
Training loss: 3.660734637798663
Validation loss: 2.942314735085385

Epoch: 6| Step: 1
Training loss: 2.6899301054712477
Validation loss: 2.8848928033219705

Epoch: 6| Step: 2
Training loss: 2.8110169633253776
Validation loss: 2.8445103785038897

Epoch: 6| Step: 3
Training loss: 2.917102526840565
Validation loss: 2.842642192302372

Epoch: 6| Step: 4
Training loss: 3.2021024234742903
Validation loss: 2.840901120321031

Epoch: 6| Step: 5
Training loss: 3.6361066423982367
Validation loss: 2.8441101237444943

Epoch: 6| Step: 6
Training loss: 3.674904210924777
Validation loss: 2.8436312916078554

Epoch: 6| Step: 7
Training loss: 2.9682819198219685
Validation loss: 2.8403761108951437

Epoch: 6| Step: 8
Training loss: 3.1249063096306946
Validation loss: 2.8413440930720744

Epoch: 6| Step: 9
Training loss: 2.8535497142470967
Validation loss: 2.836593061748082

Epoch: 6| Step: 10
Training loss: 2.875975899299584
Validation loss: 2.838745469928555

Epoch: 6| Step: 11
Training loss: 3.312917719000309
Validation loss: 2.835716991778028

Epoch: 6| Step: 12
Training loss: 2.961552296929144
Validation loss: 2.8361767869864214

Epoch: 6| Step: 13
Training loss: 3.168827189872831
Validation loss: 2.8370912806564252

Epoch: 61| Step: 0
Training loss: 2.9157806322238073
Validation loss: 2.837474293041452

Epoch: 6| Step: 1
Training loss: 3.059932021339645
Validation loss: 2.8408206788864367

Epoch: 6| Step: 2
Training loss: 3.1711076080628455
Validation loss: 2.8431843913302144

Epoch: 6| Step: 3
Training loss: 2.921963777060408
Validation loss: 2.8452940198239802

Epoch: 6| Step: 4
Training loss: 3.376564793407221
Validation loss: 2.8457736361031682

Epoch: 6| Step: 5
Training loss: 3.210608992281427
Validation loss: 2.8512644406475403

Epoch: 6| Step: 6
Training loss: 2.6935043683212263
Validation loss: 2.8534038537224378

Epoch: 6| Step: 7
Training loss: 2.7362932724404256
Validation loss: 2.8598754649259486

Epoch: 6| Step: 8
Training loss: 3.133810973454875
Validation loss: 2.8644661464194856

Epoch: 6| Step: 9
Training loss: 3.834107624857667
Validation loss: 2.859848094498441

Epoch: 6| Step: 10
Training loss: 3.2152975881436334
Validation loss: 2.8529882022848705

Epoch: 6| Step: 11
Training loss: 3.6598310621284575
Validation loss: 2.848215652927733

Epoch: 6| Step: 12
Training loss: 3.0605122869176693
Validation loss: 2.841134229242904

Epoch: 6| Step: 13
Training loss: 3.097515858262472
Validation loss: 2.841601826442236

Epoch: 62| Step: 0
Training loss: 3.1658800720904408
Validation loss: 2.842995395763279

Epoch: 6| Step: 1
Training loss: 3.0187870505134438
Validation loss: 2.8422330867088

Epoch: 6| Step: 2
Training loss: 3.5150794390062385
Validation loss: 2.8406053817109296

Epoch: 6| Step: 3
Training loss: 3.6357542535221485
Validation loss: 2.838901792186983

Epoch: 6| Step: 4
Training loss: 3.2879364188142444
Validation loss: 2.839752421933469

Epoch: 6| Step: 5
Training loss: 3.0880729915807636
Validation loss: 2.8348594285834015

Epoch: 6| Step: 6
Training loss: 2.707864036003597
Validation loss: 2.8339265999088976

Epoch: 6| Step: 7
Training loss: 2.739483056692944
Validation loss: 2.8299359320741257

Epoch: 6| Step: 8
Training loss: 3.1332687844736262
Validation loss: 2.8300006814840835

Epoch: 6| Step: 9
Training loss: 3.2284490136522894
Validation loss: 2.8296092101302537

Epoch: 6| Step: 10
Training loss: 2.945709120862317
Validation loss: 2.828207381576088

Epoch: 6| Step: 11
Training loss: 3.4727839477540665
Validation loss: 2.8383745234398825

Epoch: 6| Step: 12
Training loss: 2.8089056146937903
Validation loss: 2.8392035078603053

Epoch: 6| Step: 13
Training loss: 3.2035511756787765
Validation loss: 2.8443788637474574

Epoch: 63| Step: 0
Training loss: 2.611512821715999
Validation loss: 2.8233243783072517

Epoch: 6| Step: 1
Training loss: 3.0311043301217806
Validation loss: 2.8164791402509413

Epoch: 6| Step: 2
Training loss: 3.3530174956281944
Validation loss: 2.818802077635388

Epoch: 6| Step: 3
Training loss: 2.7724184555018434
Validation loss: 2.8196011079176837

Epoch: 6| Step: 4
Training loss: 3.361913671035314
Validation loss: 2.81963765654794

Epoch: 6| Step: 5
Training loss: 2.8851738746119207
Validation loss: 2.8204368146001775

Epoch: 6| Step: 6
Training loss: 3.3329022287700742
Validation loss: 2.8243261038968077

Epoch: 6| Step: 7
Training loss: 3.1054027238460566
Validation loss: 2.822092234139323

Epoch: 6| Step: 8
Training loss: 3.2487508867592094
Validation loss: 2.8235111477139716

Epoch: 6| Step: 9
Training loss: 3.0107784559828943
Validation loss: 2.82308793491539

Epoch: 6| Step: 10
Training loss: 3.2557162852386665
Validation loss: 2.827988502926414

Epoch: 6| Step: 11
Training loss: 2.7601978911069565
Validation loss: 2.8413594215827542

Epoch: 6| Step: 12
Training loss: 3.7580339203948623
Validation loss: 2.8550439277885573

Epoch: 6| Step: 13
Training loss: 3.2986951386752796
Validation loss: 2.825341657516018

Epoch: 64| Step: 0
Training loss: 2.9595605874605115
Validation loss: 2.817500730577524

Epoch: 6| Step: 1
Training loss: 2.6832735406441324
Validation loss: 2.817023145164693

Epoch: 6| Step: 2
Training loss: 3.4251329772083388
Validation loss: 2.814694749410925

Epoch: 6| Step: 3
Training loss: 3.9992671533641073
Validation loss: 2.8096119133946336

Epoch: 6| Step: 4
Training loss: 3.40901058217983
Validation loss: 2.820045274688004

Epoch: 6| Step: 5
Training loss: 2.535294680349986
Validation loss: 2.877027421820147

Epoch: 6| Step: 6
Training loss: 1.9502184794372277
Validation loss: 2.9986932294329294

Epoch: 6| Step: 7
Training loss: 3.8809408362660167
Validation loss: 3.086794335574961

Epoch: 6| Step: 8
Training loss: 3.514425522374793
Validation loss: 3.025426270470301

Epoch: 6| Step: 9
Training loss: 3.7394506206003966
Validation loss: 2.9099177498796576

Epoch: 6| Step: 10
Training loss: 3.246142812787315
Validation loss: 2.808319087951038

Epoch: 6| Step: 11
Training loss: 2.4557521868881156
Validation loss: 2.812121093457196

Epoch: 6| Step: 12
Training loss: 2.8665122086660717
Validation loss: 2.8167650268102484

Epoch: 6| Step: 13
Training loss: 3.071399292536999
Validation loss: 2.822369734154658

Epoch: 65| Step: 0
Training loss: 2.8153881926030784
Validation loss: 2.829885713029237

Epoch: 6| Step: 1
Training loss: 3.0924053930738618
Validation loss: 2.8536508673604617

Epoch: 6| Step: 2
Training loss: 2.8650762700286103
Validation loss: 2.8860962498620917

Epoch: 6| Step: 3
Training loss: 3.8082612034306007
Validation loss: 2.9806073542718936

Epoch: 6| Step: 4
Training loss: 3.944425758950356
Validation loss: 2.8371144808983915

Epoch: 6| Step: 5
Training loss: 2.674409612229738
Validation loss: 2.815367132593473

Epoch: 6| Step: 6
Training loss: 2.9096228603573624
Validation loss: 2.8090149775532423

Epoch: 6| Step: 7
Training loss: 3.2561209332738996
Validation loss: 2.806384927103274

Epoch: 6| Step: 8
Training loss: 3.714560320417941
Validation loss: 2.8069178149873273

Epoch: 6| Step: 9
Training loss: 2.9232814076363605
Validation loss: 2.8131088700708418

Epoch: 6| Step: 10
Training loss: 2.5938602102373762
Validation loss: 2.840283430702132

Epoch: 6| Step: 11
Training loss: 3.4908306720353472
Validation loss: 2.8695072293441437

Epoch: 6| Step: 12
Training loss: 2.797821566702209
Validation loss: 2.873819269245944

Epoch: 6| Step: 13
Training loss: 2.9483896871226754
Validation loss: 2.8525037505812256

Epoch: 66| Step: 0
Training loss: 2.670487249827128
Validation loss: 2.8247570033882368

Epoch: 6| Step: 1
Training loss: 3.1408062782920547
Validation loss: 2.8028602521808246

Epoch: 6| Step: 2
Training loss: 3.555460885562468
Validation loss: 2.8006384291638593

Epoch: 6| Step: 3
Training loss: 3.2332248577785356
Validation loss: 2.7995630105301075

Epoch: 6| Step: 4
Training loss: 3.6216795611045924
Validation loss: 2.799159594661046

Epoch: 6| Step: 5
Training loss: 2.530017505630423
Validation loss: 2.802440993918287

Epoch: 6| Step: 6
Training loss: 3.1583498777360486
Validation loss: 2.799377866926406

Epoch: 6| Step: 7
Training loss: 3.1743479141669053
Validation loss: 2.8034262116392386

Epoch: 6| Step: 8
Training loss: 2.609490854580947
Validation loss: 2.8039564727665875

Epoch: 6| Step: 9
Training loss: 3.121937280898554
Validation loss: 2.802695911908603

Epoch: 6| Step: 10
Training loss: 3.310226523954604
Validation loss: 2.805004734344374

Epoch: 6| Step: 11
Training loss: 3.282253148369344
Validation loss: 2.8072311179016047

Epoch: 6| Step: 12
Training loss: 3.1435463446903116
Validation loss: 2.804555670872672

Epoch: 6| Step: 13
Training loss: 2.593921885482645
Validation loss: 2.8021293566519363

Epoch: 67| Step: 0
Training loss: 3.4801247166614684
Validation loss: 2.7985237217727077

Epoch: 6| Step: 1
Training loss: 4.015019590109114
Validation loss: 2.8004907428851737

Epoch: 6| Step: 2
Training loss: 3.2419017252161537
Validation loss: 2.795671443403818

Epoch: 6| Step: 3
Training loss: 2.740568286130602
Validation loss: 2.793451206867497

Epoch: 6| Step: 4
Training loss: 3.44056828284918
Validation loss: 2.795114346755293

Epoch: 6| Step: 5
Training loss: 2.4735650536330036
Validation loss: 2.792894510455238

Epoch: 6| Step: 6
Training loss: 3.479324854987416
Validation loss: 2.796933035409671

Epoch: 6| Step: 7
Training loss: 2.7972376071574607
Validation loss: 2.7975038101258463

Epoch: 6| Step: 8
Training loss: 2.7314096120465927
Validation loss: 2.8029815323600853

Epoch: 6| Step: 9
Training loss: 2.655654840550841
Validation loss: 2.8079347007377864

Epoch: 6| Step: 10
Training loss: 2.8560122534972545
Validation loss: 2.8021179022034532

Epoch: 6| Step: 11
Training loss: 2.607900328439987
Validation loss: 2.8044255779018243

Epoch: 6| Step: 12
Training loss: 3.279273173352777
Validation loss: 2.8049281733400666

Epoch: 6| Step: 13
Training loss: 3.406333012400524
Validation loss: 2.811495163231508

Epoch: 68| Step: 0
Training loss: 2.886084872842362
Validation loss: 2.821312424490348

Epoch: 6| Step: 1
Training loss: 3.920249328469209
Validation loss: 2.8162711816850146

Epoch: 6| Step: 2
Training loss: 2.2159905265302635
Validation loss: 2.795826659368754

Epoch: 6| Step: 3
Training loss: 3.1286500594926214
Validation loss: 2.7895795375588586

Epoch: 6| Step: 4
Training loss: 2.9697909487738268
Validation loss: 2.789569026866193

Epoch: 6| Step: 5
Training loss: 2.9677340726761634
Validation loss: 2.7916852096091165

Epoch: 6| Step: 6
Training loss: 3.5647043468832695
Validation loss: 2.7952422206982193

Epoch: 6| Step: 7
Training loss: 3.1255608627552127
Validation loss: 2.795543088387544

Epoch: 6| Step: 8
Training loss: 2.6284814182413463
Validation loss: 2.7947186766742376

Epoch: 6| Step: 9
Training loss: 2.8185380335784926
Validation loss: 2.79488681750502

Epoch: 6| Step: 10
Training loss: 2.734540138707537
Validation loss: 2.794934229405936

Epoch: 6| Step: 11
Training loss: 3.4219081650611702
Validation loss: 2.792330370195658

Epoch: 6| Step: 12
Training loss: 3.435471092025882
Validation loss: 2.789184942648824

Epoch: 6| Step: 13
Training loss: 3.8175486921141686
Validation loss: 2.7914971410502156

Epoch: 69| Step: 0
Training loss: 2.8156068384260093
Validation loss: 2.785909620112439

Epoch: 6| Step: 1
Training loss: 3.73485022677358
Validation loss: 2.78266808920247

Epoch: 6| Step: 2
Training loss: 2.6158161271246287
Validation loss: 2.782648122041078

Epoch: 6| Step: 3
Training loss: 3.5066966977543843
Validation loss: 2.7836065647395674

Epoch: 6| Step: 4
Training loss: 3.06404386258913
Validation loss: 2.78192209085134

Epoch: 6| Step: 5
Training loss: 2.8644599101902966
Validation loss: 2.78117942374853

Epoch: 6| Step: 6
Training loss: 3.4168583575803373
Validation loss: 2.7836910862198523

Epoch: 6| Step: 7
Training loss: 3.069091864848286
Validation loss: 2.7779842891290936

Epoch: 6| Step: 8
Training loss: 2.9189297933156366
Validation loss: 2.777233085517729

Epoch: 6| Step: 9
Training loss: 3.069120452336817
Validation loss: 2.77894215397765

Epoch: 6| Step: 10
Training loss: 3.281372939940183
Validation loss: 2.7765705987127536

Epoch: 6| Step: 11
Training loss: 3.1812618772513575
Validation loss: 2.777747355947407

Epoch: 6| Step: 12
Training loss: 2.855196040916958
Validation loss: 2.786623404283284

Epoch: 6| Step: 13
Training loss: 2.490671491966774
Validation loss: 2.794915937637812

Epoch: 70| Step: 0
Training loss: 2.674284177888585
Validation loss: 2.8188201407352875

Epoch: 6| Step: 1
Training loss: 2.9663841055846483
Validation loss: 2.815798323277073

Epoch: 6| Step: 2
Training loss: 3.679581237684092
Validation loss: 2.818684296765018

Epoch: 6| Step: 3
Training loss: 2.861691063266115
Validation loss: 2.798186472202265

Epoch: 6| Step: 4
Training loss: 2.889367237183781
Validation loss: 2.7850844766872402

Epoch: 6| Step: 5
Training loss: 2.883457349487923
Validation loss: 2.7726689486016802

Epoch: 6| Step: 6
Training loss: 3.4281308197556286
Validation loss: 2.773965502285277

Epoch: 6| Step: 7
Training loss: 2.849556998908226
Validation loss: 2.7738452048428104

Epoch: 6| Step: 8
Training loss: 2.882400441894582
Validation loss: 2.771730667346258

Epoch: 6| Step: 9
Training loss: 2.815860436150934
Validation loss: 2.7722571091519956

Epoch: 6| Step: 10
Training loss: 3.235090683339673
Validation loss: 2.771967998272267

Epoch: 6| Step: 11
Training loss: 3.565837701977714
Validation loss: 2.7722468194562113

Epoch: 6| Step: 12
Training loss: 3.777533099410073
Validation loss: 2.7693533123215044

Epoch: 6| Step: 13
Training loss: 1.4460560303750367
Validation loss: 2.771590167806681

Epoch: 71| Step: 0
Training loss: 3.1193910234300004
Validation loss: 2.7714661624902073

Epoch: 6| Step: 1
Training loss: 3.0372454940859184
Validation loss: 2.768505602441976

Epoch: 6| Step: 2
Training loss: 3.3955587888483674
Validation loss: 2.7703815128529534

Epoch: 6| Step: 3
Training loss: 3.32537269511274
Validation loss: 2.767814937629108

Epoch: 6| Step: 4
Training loss: 2.7473664244513576
Validation loss: 2.7719947705549144

Epoch: 6| Step: 5
Training loss: 2.93457989937078
Validation loss: 2.7714807582228262

Epoch: 6| Step: 6
Training loss: 3.7405838048669504
Validation loss: 2.7721178986395314

Epoch: 6| Step: 7
Training loss: 3.038403437132295
Validation loss: 2.771360084660318

Epoch: 6| Step: 8
Training loss: 3.154065575401162
Validation loss: 2.7720441304639776

Epoch: 6| Step: 9
Training loss: 2.856399739219783
Validation loss: 2.769655108692444

Epoch: 6| Step: 10
Training loss: 2.902418174148824
Validation loss: 2.7693141828644223

Epoch: 6| Step: 11
Training loss: 2.9099153764618215
Validation loss: 2.7696636215815094

Epoch: 6| Step: 12
Training loss: 3.139707061077884
Validation loss: 2.766636909841768

Epoch: 6| Step: 13
Training loss: 2.64543391138936
Validation loss: 2.766247723193694

Epoch: 72| Step: 0
Training loss: 2.9893476191691764
Validation loss: 2.7659047405150208

Epoch: 6| Step: 1
Training loss: 2.665019559806342
Validation loss: 2.765014547694604

Epoch: 6| Step: 2
Training loss: 2.8035655208174823
Validation loss: 2.764677517272467

Epoch: 6| Step: 3
Training loss: 3.2144957640301492
Validation loss: 2.7623675924766102

Epoch: 6| Step: 4
Training loss: 2.7444504315220457
Validation loss: 2.7612145686813148

Epoch: 6| Step: 5
Training loss: 3.3775050438674548
Validation loss: 2.7616383191763396

Epoch: 6| Step: 6
Training loss: 2.911628422463211
Validation loss: 2.760779576622404

Epoch: 6| Step: 7
Training loss: 3.1027954110927367
Validation loss: 2.757976932498682

Epoch: 6| Step: 8
Training loss: 2.669523308451615
Validation loss: 2.7558751175908425

Epoch: 6| Step: 9
Training loss: 3.7339788889193666
Validation loss: 2.759306295703212

Epoch: 6| Step: 10
Training loss: 3.0685896759728735
Validation loss: 2.761689529534722

Epoch: 6| Step: 11
Training loss: 3.43034200136632
Validation loss: 2.7572865628820997

Epoch: 6| Step: 12
Training loss: 3.100875816911922
Validation loss: 2.755044813452525

Epoch: 6| Step: 13
Training loss: 3.2727631304443046
Validation loss: 2.757026178898258

Epoch: 73| Step: 0
Training loss: 2.645109262787638
Validation loss: 2.7552369519822775

Epoch: 6| Step: 1
Training loss: 3.0579011603100557
Validation loss: 2.7545501917290016

Epoch: 6| Step: 2
Training loss: 2.388654108363506
Validation loss: 2.7557929794034304

Epoch: 6| Step: 3
Training loss: 2.9302309066349586
Validation loss: 2.757226182997043

Epoch: 6| Step: 4
Training loss: 3.3777739816220462
Validation loss: 2.7552871415240245

Epoch: 6| Step: 5
Training loss: 3.043457148757117
Validation loss: 2.7556674640711187

Epoch: 6| Step: 6
Training loss: 3.2977202719461203
Validation loss: 2.7533117491167505

Epoch: 6| Step: 7
Training loss: 2.9093658807695597
Validation loss: 2.7566643737589205

Epoch: 6| Step: 8
Training loss: 2.7529546297424536
Validation loss: 2.755930799667992

Epoch: 6| Step: 9
Training loss: 2.9908031318892023
Validation loss: 2.753849772934049

Epoch: 6| Step: 10
Training loss: 3.049339354209588
Validation loss: 2.7512098561442153

Epoch: 6| Step: 11
Training loss: 3.350062178988381
Validation loss: 2.7514922762695577

Epoch: 6| Step: 12
Training loss: 3.7716252178679706
Validation loss: 2.7499637219151345

Epoch: 6| Step: 13
Training loss: 3.5735105386132666
Validation loss: 2.7483457267579947

Epoch: 74| Step: 0
Training loss: 3.6106610009483764
Validation loss: 2.746992211474914

Epoch: 6| Step: 1
Training loss: 2.7108726823516913
Validation loss: 2.746888891814915

Epoch: 6| Step: 2
Training loss: 3.0572533331595593
Validation loss: 2.745593993711876

Epoch: 6| Step: 3
Training loss: 3.1952226621501314
Validation loss: 2.7451488845163805

Epoch: 6| Step: 4
Training loss: 2.938103796406286
Validation loss: 2.745098494671182

Epoch: 6| Step: 5
Training loss: 2.7323132398604213
Validation loss: 2.7432813570596633

Epoch: 6| Step: 6
Training loss: 3.333719199417
Validation loss: 2.743125818329763

Epoch: 6| Step: 7
Training loss: 3.3407439507770302
Validation loss: 2.7427471571298865

Epoch: 6| Step: 8
Training loss: 2.88955388199682
Validation loss: 2.7411728119768983

Epoch: 6| Step: 9
Training loss: 2.991523370171088
Validation loss: 2.743008615253015

Epoch: 6| Step: 10
Training loss: 3.106287204204937
Validation loss: 2.739227757203031

Epoch: 6| Step: 11
Training loss: 3.382758794274534
Validation loss: 2.7387900181548934

Epoch: 6| Step: 12
Training loss: 2.480332642673266
Validation loss: 2.7399006285551115

Epoch: 6| Step: 13
Training loss: 3.0891391804448807
Validation loss: 2.739194278017885

Epoch: 75| Step: 0
Training loss: 3.368290200640738
Validation loss: 2.739630872604921

Epoch: 6| Step: 1
Training loss: 3.4212060561892863
Validation loss: 2.739603891665566

Epoch: 6| Step: 2
Training loss: 2.751510378740892
Validation loss: 2.741417802660028

Epoch: 6| Step: 3
Training loss: 2.70260959187775
Validation loss: 2.7517788811767163

Epoch: 6| Step: 4
Training loss: 3.026428162956641
Validation loss: 2.7555460040314728

Epoch: 6| Step: 5
Training loss: 3.2031891607045893
Validation loss: 2.7698362849447804

Epoch: 6| Step: 6
Training loss: 2.2056882840225667
Validation loss: 2.776718330224214

Epoch: 6| Step: 7
Training loss: 3.391106074787609
Validation loss: 2.7804266132631392

Epoch: 6| Step: 8
Training loss: 3.422988780015259
Validation loss: 2.7754831379997373

Epoch: 6| Step: 9
Training loss: 3.2999782330344205
Validation loss: 2.7429900959124653

Epoch: 6| Step: 10
Training loss: 3.177257762352267
Validation loss: 2.733052364237832

Epoch: 6| Step: 11
Training loss: 2.9911776200499194
Validation loss: 2.73080320277373

Epoch: 6| Step: 12
Training loss: 2.3691710165893825
Validation loss: 2.732154935478022

Epoch: 6| Step: 13
Training loss: 3.65733895020906
Validation loss: 2.7336769593569947

Epoch: 76| Step: 0
Training loss: 3.2405238283820736
Validation loss: 2.73236580016038

Epoch: 6| Step: 1
Training loss: 3.2747782508382284
Validation loss: 2.7365163848555447

Epoch: 6| Step: 2
Training loss: 2.8895938168101516
Validation loss: 2.737693424520917

Epoch: 6| Step: 3
Training loss: 3.3305343002250534
Validation loss: 2.7392183776067673

Epoch: 6| Step: 4
Training loss: 2.960524880035231
Validation loss: 2.738940260135108

Epoch: 6| Step: 5
Training loss: 2.9066803316209255
Validation loss: 2.7421237809723222

Epoch: 6| Step: 6
Training loss: 2.5352455911044114
Validation loss: 2.739336659058555

Epoch: 6| Step: 7
Training loss: 2.2762468421574766
Validation loss: 2.7390220950268107

Epoch: 6| Step: 8
Training loss: 3.244495719202574
Validation loss: 2.7370075534902445

Epoch: 6| Step: 9
Training loss: 3.5031958022436345
Validation loss: 2.7344127384048518

Epoch: 6| Step: 10
Training loss: 3.585477911725123
Validation loss: 2.732560765187723

Epoch: 6| Step: 11
Training loss: 2.3048571508003923
Validation loss: 2.7317501190775655

Epoch: 6| Step: 12
Training loss: 3.124931029512329
Validation loss: 2.7303787078815045

Epoch: 6| Step: 13
Training loss: 3.8010321570573926
Validation loss: 2.7284719805280795

Epoch: 77| Step: 0
Training loss: 2.7043389036777223
Validation loss: 2.7280562796483756

Epoch: 6| Step: 1
Training loss: 3.346211605770876
Validation loss: 2.7263731165104486

Epoch: 6| Step: 2
Training loss: 3.4156359226509365
Validation loss: 2.7240635366038735

Epoch: 6| Step: 3
Training loss: 2.6666800876120758
Validation loss: 2.725859586476081

Epoch: 6| Step: 4
Training loss: 2.787434933848145
Validation loss: 2.7258454245577046

Epoch: 6| Step: 5
Training loss: 3.328321567515241
Validation loss: 2.7252456527219193

Epoch: 6| Step: 6
Training loss: 2.612949321747366
Validation loss: 2.725862465311029

Epoch: 6| Step: 7
Training loss: 3.248758812628576
Validation loss: 2.7253875565580197

Epoch: 6| Step: 8
Training loss: 3.188352882124585
Validation loss: 2.7247412869697416

Epoch: 6| Step: 9
Training loss: 3.456605425972397
Validation loss: 2.727269392813232

Epoch: 6| Step: 10
Training loss: 2.3972144969443914
Validation loss: 2.7222915221272266

Epoch: 6| Step: 11
Training loss: 3.1044117022242297
Validation loss: 2.7233123508998154

Epoch: 6| Step: 12
Training loss: 3.070050798673062
Validation loss: 2.7219171818432417

Epoch: 6| Step: 13
Training loss: 3.417137315169687
Validation loss: 2.722194305532374

Epoch: 78| Step: 0
Training loss: 2.905221408410169
Validation loss: 2.7206908575414994

Epoch: 6| Step: 1
Training loss: 3.0676240677370283
Validation loss: 2.7196117747242106

Epoch: 6| Step: 2
Training loss: 2.962566481218548
Validation loss: 2.7227909211780643

Epoch: 6| Step: 3
Training loss: 3.2325438692519213
Validation loss: 2.7216257876609724

Epoch: 6| Step: 4
Training loss: 2.794257446901668
Validation loss: 2.7189536644853165

Epoch: 6| Step: 5
Training loss: 2.597672954304844
Validation loss: 2.726592711830656

Epoch: 6| Step: 6
Training loss: 2.905515352323138
Validation loss: 2.731243455244061

Epoch: 6| Step: 7
Training loss: 3.2226399739172322
Validation loss: 2.732553527100303

Epoch: 6| Step: 8
Training loss: 3.305026278254255
Validation loss: 2.73034483475138

Epoch: 6| Step: 9
Training loss: 3.639014284789878
Validation loss: 2.738917047304181

Epoch: 6| Step: 10
Training loss: 2.673943772751253
Validation loss: 2.721636672835589

Epoch: 6| Step: 11
Training loss: 2.827539457118376
Validation loss: 2.71928147856739

Epoch: 6| Step: 12
Training loss: 3.6865535750009846
Validation loss: 2.71626483100203

Epoch: 6| Step: 13
Training loss: 2.3568548422886866
Validation loss: 2.7131328072012395

Epoch: 79| Step: 0
Training loss: 3.4445326564800656
Validation loss: 2.712670774808105

Epoch: 6| Step: 1
Training loss: 3.1153432031833357
Validation loss: 2.7123999038312716

Epoch: 6| Step: 2
Training loss: 3.506911673985494
Validation loss: 2.716327471016049

Epoch: 6| Step: 3
Training loss: 2.9527416485019224
Validation loss: 2.7132038675152415

Epoch: 6| Step: 4
Training loss: 2.137507263528183
Validation loss: 2.7130995200410664

Epoch: 6| Step: 5
Training loss: 3.3435512020160436
Validation loss: 2.7140759469250915

Epoch: 6| Step: 6
Training loss: 3.149594380739144
Validation loss: 2.7150189450383944

Epoch: 6| Step: 7
Training loss: 2.346410436657632
Validation loss: 2.7114362038986393

Epoch: 6| Step: 8
Training loss: 3.1161566155461933
Validation loss: 2.7116674225635817

Epoch: 6| Step: 9
Training loss: 2.7821981978250943
Validation loss: 2.7103115540161684

Epoch: 6| Step: 10
Training loss: 2.914427051803356
Validation loss: 2.709632470762328

Epoch: 6| Step: 11
Training loss: 3.202689387776341
Validation loss: 2.7105801311234847

Epoch: 6| Step: 12
Training loss: 3.408046983350238
Validation loss: 2.7098103328658745

Epoch: 6| Step: 13
Training loss: 2.7214780253038815
Validation loss: 2.720010335427276

Epoch: 80| Step: 0
Training loss: 3.207434160801126
Validation loss: 2.7259046336403387

Epoch: 6| Step: 1
Training loss: 2.672837647250953
Validation loss: 2.742102595815908

Epoch: 6| Step: 2
Training loss: 3.52758112348443
Validation loss: 2.772870253182063

Epoch: 6| Step: 3
Training loss: 2.942868946834811
Validation loss: 2.779173170197941

Epoch: 6| Step: 4
Training loss: 2.637571115122605
Validation loss: 2.794235720267288

Epoch: 6| Step: 5
Training loss: 3.0681635589570058
Validation loss: 2.801012784275794

Epoch: 6| Step: 6
Training loss: 2.720698130014932
Validation loss: 2.811046758128038

Epoch: 6| Step: 7
Training loss: 2.5470170993597887
Validation loss: 2.826815387890023

Epoch: 6| Step: 8
Training loss: 3.270236760705459
Validation loss: 2.8544459956650443

Epoch: 6| Step: 9
Training loss: 3.4964205285478993
Validation loss: 2.8550739096429627

Epoch: 6| Step: 10
Training loss: 3.387773712473829
Validation loss: 2.800487602058526

Epoch: 6| Step: 11
Training loss: 2.8871098770886454
Validation loss: 2.77356957793149

Epoch: 6| Step: 12
Training loss: 3.1124817935763476
Validation loss: 2.7553968898708496

Epoch: 6| Step: 13
Training loss: 3.6964459283920115
Validation loss: 2.7402379572386826

Epoch: 81| Step: 0
Training loss: 2.7645643538796385
Validation loss: 2.7051942269607974

Epoch: 6| Step: 1
Training loss: 2.6357345891887514
Validation loss: 2.702021825915801

Epoch: 6| Step: 2
Training loss: 3.3287398795626877
Validation loss: 2.7035276525743948

Epoch: 6| Step: 3
Training loss: 3.089150757365752
Validation loss: 2.7075394459995503

Epoch: 6| Step: 4
Training loss: 2.7995192319383477
Validation loss: 2.7187731681405176

Epoch: 6| Step: 5
Training loss: 3.2762005418219404
Validation loss: 2.728391196339884

Epoch: 6| Step: 6
Training loss: 3.1610512049726407
Validation loss: 2.7290560159716

Epoch: 6| Step: 7
Training loss: 3.003141983095352
Validation loss: 2.7256756084156986

Epoch: 6| Step: 8
Training loss: 2.988141783408047
Validation loss: 2.7200262553400383

Epoch: 6| Step: 9
Training loss: 3.162891764744839
Validation loss: 2.7147131116976304

Epoch: 6| Step: 10
Training loss: 2.847872585875439
Validation loss: 2.710854851339756

Epoch: 6| Step: 11
Training loss: 3.3360385566788566
Validation loss: 2.706631023399714

Epoch: 6| Step: 12
Training loss: 3.020263898826506
Validation loss: 2.703284487529262

Epoch: 6| Step: 13
Training loss: 3.433055431884063
Validation loss: 2.702144182390568

Epoch: 82| Step: 0
Training loss: 2.385087524983319
Validation loss: 2.702302379722123

Epoch: 6| Step: 1
Training loss: 3.0453794281182853
Validation loss: 2.701508310234354

Epoch: 6| Step: 2
Training loss: 4.003864567237299
Validation loss: 2.7003111924299588

Epoch: 6| Step: 3
Training loss: 2.8893286195102483
Validation loss: 2.700644129559186

Epoch: 6| Step: 4
Training loss: 2.7222359769391495
Validation loss: 2.7013699435928498

Epoch: 6| Step: 5
Training loss: 2.7146431608675066
Validation loss: 2.701928054813833

Epoch: 6| Step: 6
Training loss: 3.1378206856663287
Validation loss: 2.6995107679989365

Epoch: 6| Step: 7
Training loss: 3.4419701381631476
Validation loss: 2.699115164999238

Epoch: 6| Step: 8
Training loss: 3.1034323487293305
Validation loss: 2.6977047374741607

Epoch: 6| Step: 9
Training loss: 2.970019902120883
Validation loss: 2.697902686217116

Epoch: 6| Step: 10
Training loss: 3.9804102899902363
Validation loss: 2.696412574729231

Epoch: 6| Step: 11
Training loss: 2.4203366223605243
Validation loss: 2.696283595375275

Epoch: 6| Step: 12
Training loss: 2.3765008099516667
Validation loss: 2.6952540170900443

Epoch: 6| Step: 13
Training loss: 2.791481571798771
Validation loss: 2.693397781372012

Epoch: 83| Step: 0
Training loss: 2.9280016308243177
Validation loss: 2.6936303019371794

Epoch: 6| Step: 1
Training loss: 3.8271670018607544
Validation loss: 2.6933524180102353

Epoch: 6| Step: 2
Training loss: 3.3614684213621495
Validation loss: 2.6931932666075693

Epoch: 6| Step: 3
Training loss: 2.5919372474585844
Validation loss: 2.693584962478257

Epoch: 6| Step: 4
Training loss: 2.1832790523314056
Validation loss: 2.691073191205433

Epoch: 6| Step: 5
Training loss: 2.9619686374484986
Validation loss: 2.6902348138874985

Epoch: 6| Step: 6
Training loss: 3.208805165130559
Validation loss: 2.6888628349461774

Epoch: 6| Step: 7
Training loss: 2.8481131816298197
Validation loss: 2.6924712193182048

Epoch: 6| Step: 8
Training loss: 3.208600532667609
Validation loss: 2.6919409253258912

Epoch: 6| Step: 9
Training loss: 3.223033832046994
Validation loss: 2.689105263929054

Epoch: 6| Step: 10
Training loss: 2.953466647937389
Validation loss: 2.689717186399324

Epoch: 6| Step: 11
Training loss: 2.525213980342041
Validation loss: 2.6899413457315138

Epoch: 6| Step: 12
Training loss: 3.129329581322498
Validation loss: 2.6901384476989905

Epoch: 6| Step: 13
Training loss: 3.4494084168903036
Validation loss: 2.6884686521791608

Epoch: 84| Step: 0
Training loss: 3.791075999326051
Validation loss: 2.6905136420169984

Epoch: 6| Step: 1
Training loss: 3.0091475261148863
Validation loss: 2.6881666889957723

Epoch: 6| Step: 2
Training loss: 3.0303009749896854
Validation loss: 2.6860510059951186

Epoch: 6| Step: 3
Training loss: 2.92346017850136
Validation loss: 2.685991791713082

Epoch: 6| Step: 4
Training loss: 3.4346437552124276
Validation loss: 2.684966526063826

Epoch: 6| Step: 5
Training loss: 2.9817103128524254
Validation loss: 2.684459629162353

Epoch: 6| Step: 6
Training loss: 2.6833007297154206
Validation loss: 2.6840913508906747

Epoch: 6| Step: 7
Training loss: 3.4554278259820865
Validation loss: 2.6803853613237005

Epoch: 6| Step: 8
Training loss: 3.251583887164616
Validation loss: 2.6831605503722935

Epoch: 6| Step: 9
Training loss: 2.6899204443756903
Validation loss: 2.6804052266085407

Epoch: 6| Step: 10
Training loss: 2.7728863235598045
Validation loss: 2.6803672194173758

Epoch: 6| Step: 11
Training loss: 2.686705893363485
Validation loss: 2.681481953468612

Epoch: 6| Step: 12
Training loss: 2.6587830078486485
Validation loss: 2.6788742748187673

Epoch: 6| Step: 13
Training loss: 2.6716183544731846
Validation loss: 2.681008133478955

Epoch: 85| Step: 0
Training loss: 2.687120366451799
Validation loss: 2.683175444449774

Epoch: 6| Step: 1
Training loss: 3.5727405263710956
Validation loss: 2.685689214519645

Epoch: 6| Step: 2
Training loss: 2.6802694950965686
Validation loss: 2.6829585847877735

Epoch: 6| Step: 3
Training loss: 3.330775678407816
Validation loss: 2.6892693788864497

Epoch: 6| Step: 4
Training loss: 3.374204329901246
Validation loss: 2.6865357798646152

Epoch: 6| Step: 5
Training loss: 2.422937762976853
Validation loss: 2.682020558173309

Epoch: 6| Step: 6
Training loss: 2.4232919793183836
Validation loss: 2.6768697011240787

Epoch: 6| Step: 7
Training loss: 3.2973946256770432
Validation loss: 2.6814579946568604

Epoch: 6| Step: 8
Training loss: 3.2266335837452074
Validation loss: 2.6795314137174673

Epoch: 6| Step: 9
Training loss: 2.7036649131701807
Validation loss: 2.686846066639561

Epoch: 6| Step: 10
Training loss: 3.4816059690753876
Validation loss: 2.7004439153847932

Epoch: 6| Step: 11
Training loss: 2.8674469292542875
Validation loss: 2.686191976947069

Epoch: 6| Step: 12
Training loss: 2.9583353570922233
Validation loss: 2.67677783112699

Epoch: 6| Step: 13
Training loss: 3.0809356714830005
Validation loss: 2.6733412548673363

Epoch: 86| Step: 0
Training loss: 2.539569886203012
Validation loss: 2.6707923901251776

Epoch: 6| Step: 1
Training loss: 2.8654806685214127
Validation loss: 2.6715377540089023

Epoch: 6| Step: 2
Training loss: 3.2158625716077753
Validation loss: 2.6696801786489703

Epoch: 6| Step: 3
Training loss: 3.5444497038331537
Validation loss: 2.6725550564903613

Epoch: 6| Step: 4
Training loss: 2.5957973626728035
Validation loss: 2.673285099206405

Epoch: 6| Step: 5
Training loss: 2.885235354973467
Validation loss: 2.676302615059439

Epoch: 6| Step: 6
Training loss: 3.2296715782596177
Validation loss: 2.6748750782639625

Epoch: 6| Step: 7
Training loss: 2.43850110354466
Validation loss: 2.6783998145322974

Epoch: 6| Step: 8
Training loss: 3.480750281253566
Validation loss: 2.6765041696024006

Epoch: 6| Step: 9
Training loss: 3.321698211530344
Validation loss: 2.675321324151482

Epoch: 6| Step: 10
Training loss: 2.8137854287735706
Validation loss: 2.6733994135426506

Epoch: 6| Step: 11
Training loss: 3.4625766566635727
Validation loss: 2.6749615891613936

Epoch: 6| Step: 12
Training loss: 2.4821032808134227
Validation loss: 2.6750055000433406

Epoch: 6| Step: 13
Training loss: 3.0828290131673324
Validation loss: 2.6734839009378475

Epoch: 87| Step: 0
Training loss: 2.528698989723261
Validation loss: 2.678818785423127

Epoch: 6| Step: 1
Training loss: 2.794787858886248
Validation loss: 2.6765322281782056

Epoch: 6| Step: 2
Training loss: 2.724938551183687
Validation loss: 2.6783630308581023

Epoch: 6| Step: 3
Training loss: 3.8388531514068633
Validation loss: 2.679559317174561

Epoch: 6| Step: 4
Training loss: 3.303330018638182
Validation loss: 2.677732238100898

Epoch: 6| Step: 5
Training loss: 2.594562449234652
Validation loss: 2.676017303509812

Epoch: 6| Step: 6
Training loss: 3.1179976784805747
Validation loss: 2.6728638394723663

Epoch: 6| Step: 7
Training loss: 2.8169568670381118
Validation loss: 2.6702669480310157

Epoch: 6| Step: 8
Training loss: 2.3023531196126577
Validation loss: 2.6704080792512066

Epoch: 6| Step: 9
Training loss: 3.484884866265732
Validation loss: 2.6688511505865757

Epoch: 6| Step: 10
Training loss: 2.7489813738733457
Validation loss: 2.6675098125176833

Epoch: 6| Step: 11
Training loss: 3.2640857628138993
Validation loss: 2.6662000342709273

Epoch: 6| Step: 12
Training loss: 2.66441329481115
Validation loss: 2.66624908741243

Epoch: 6| Step: 13
Training loss: 3.9680342141414515
Validation loss: 2.6644504269370852

Epoch: 88| Step: 0
Training loss: 2.706688356907695
Validation loss: 2.665947531310918

Epoch: 6| Step: 1
Training loss: 3.122987175252174
Validation loss: 2.6656933829799407

Epoch: 6| Step: 2
Training loss: 3.2595968139895013
Validation loss: 2.6660839708058255

Epoch: 6| Step: 3
Training loss: 2.8211056764234885
Validation loss: 2.664307372270506

Epoch: 6| Step: 4
Training loss: 3.0488642532192385
Validation loss: 2.667364578736124

Epoch: 6| Step: 5
Training loss: 3.041150785271076
Validation loss: 2.6694014937113146

Epoch: 6| Step: 6
Training loss: 3.2954868087863387
Validation loss: 2.685730323111828

Epoch: 6| Step: 7
Training loss: 2.683605921466439
Validation loss: 2.715366984327501

Epoch: 6| Step: 8
Training loss: 2.795772740199702
Validation loss: 2.7359306914211423

Epoch: 6| Step: 9
Training loss: 2.784682823488018
Validation loss: 2.6852691369548216

Epoch: 6| Step: 10
Training loss: 3.054274743333296
Validation loss: 2.6649285761325348

Epoch: 6| Step: 11
Training loss: 2.943070020925647
Validation loss: 2.657349838340653

Epoch: 6| Step: 12
Training loss: 3.5173197412563226
Validation loss: 2.656402875927364

Epoch: 6| Step: 13
Training loss: 3.129677438172246
Validation loss: 2.657043456269925

Epoch: 89| Step: 0
Training loss: 2.5438440928759496
Validation loss: 2.6600291489339356

Epoch: 6| Step: 1
Training loss: 3.24325036701073
Validation loss: 2.660505231823961

Epoch: 6| Step: 2
Training loss: 3.3952267969078895
Validation loss: 2.6606169497400693

Epoch: 6| Step: 3
Training loss: 2.9855264569414985
Validation loss: 2.6612720092362094

Epoch: 6| Step: 4
Training loss: 2.9140528456295334
Validation loss: 2.6618707918135205

Epoch: 6| Step: 5
Training loss: 2.963241767569985
Validation loss: 2.6675303138000515

Epoch: 6| Step: 6
Training loss: 3.1086429520150416
Validation loss: 2.672247253787775

Epoch: 6| Step: 7
Training loss: 3.3620492625987244
Validation loss: 2.670162743944055

Epoch: 6| Step: 8
Training loss: 2.8901696748654255
Validation loss: 2.6678667864107486

Epoch: 6| Step: 9
Training loss: 2.7656675863489366
Validation loss: 2.662851111788548

Epoch: 6| Step: 10
Training loss: 2.698419786670112
Validation loss: 2.6594887461975985

Epoch: 6| Step: 11
Training loss: 3.058181208708511
Validation loss: 2.6590063383329094

Epoch: 6| Step: 12
Training loss: 3.3636283030870286
Validation loss: 2.6564864283232787

Epoch: 6| Step: 13
Training loss: 2.58935063473349
Validation loss: 2.6539571286995325

Epoch: 90| Step: 0
Training loss: 2.900897604049424
Validation loss: 2.6506901093774653

Epoch: 6| Step: 1
Training loss: 2.8627322973087597
Validation loss: 2.6545057405254244

Epoch: 6| Step: 2
Training loss: 3.413305133116976
Validation loss: 2.656380176189469

Epoch: 6| Step: 3
Training loss: 3.167056929649685
Validation loss: 2.6551798103280415

Epoch: 6| Step: 4
Training loss: 2.765616379201783
Validation loss: 2.656535573974453

Epoch: 6| Step: 5
Training loss: 3.809717382326821
Validation loss: 2.653276247573074

Epoch: 6| Step: 6
Training loss: 2.651767493951908
Validation loss: 2.6613916800094852

Epoch: 6| Step: 7
Training loss: 3.025085157976994
Validation loss: 2.6544333198642236

Epoch: 6| Step: 8
Training loss: 2.477828222834218
Validation loss: 2.651544768609238

Epoch: 6| Step: 9
Training loss: 3.6974353459130334
Validation loss: 2.657699135620364

Epoch: 6| Step: 10
Training loss: 2.740754277345757
Validation loss: 2.6535709950240256

Epoch: 6| Step: 11
Training loss: 2.4585363875328836
Validation loss: 2.648121667261301

Epoch: 6| Step: 12
Training loss: 2.7862861556338143
Validation loss: 2.6502710059495413

Epoch: 6| Step: 13
Training loss: 2.9411619371167306
Validation loss: 2.649811333944162

Epoch: 91| Step: 0
Training loss: 3.8895193466919555
Validation loss: 2.6506438603645757

Epoch: 6| Step: 1
Training loss: 2.1325096502231067
Validation loss: 2.6495868080184404

Epoch: 6| Step: 2
Training loss: 2.949950272739068
Validation loss: 2.6540113982218374

Epoch: 6| Step: 3
Training loss: 3.0584843056927635
Validation loss: 2.6515928098820325

Epoch: 6| Step: 4
Training loss: 3.4241055399592017
Validation loss: 2.6469640185082164

Epoch: 6| Step: 5
Training loss: 2.895199676637613
Validation loss: 2.646769685353901

Epoch: 6| Step: 6
Training loss: 3.1105132966682523
Validation loss: 2.6470380928748676

Epoch: 6| Step: 7
Training loss: 3.0664409271028883
Validation loss: 2.6482864033319093

Epoch: 6| Step: 8
Training loss: 2.8707633937811265
Validation loss: 2.6509983958471417

Epoch: 6| Step: 9
Training loss: 3.5591682782649956
Validation loss: 2.653038849028514

Epoch: 6| Step: 10
Training loss: 2.6437726644351627
Validation loss: 2.6553885600626925

Epoch: 6| Step: 11
Training loss: 2.532217900341914
Validation loss: 2.661570690143261

Epoch: 6| Step: 12
Training loss: 2.734695764528484
Validation loss: 2.673332770890721

Epoch: 6| Step: 13
Training loss: 2.63786469600299
Validation loss: 2.678283555147996

Epoch: 92| Step: 0
Training loss: 3.626211029863743
Validation loss: 2.67622791332035

Epoch: 6| Step: 1
Training loss: 2.742254881289093
Validation loss: 2.659586072560872

Epoch: 6| Step: 2
Training loss: 2.569551210218632
Validation loss: 2.646648602740044

Epoch: 6| Step: 3
Training loss: 2.7753304911896794
Validation loss: 2.6425436638669204

Epoch: 6| Step: 4
Training loss: 3.1380200566756473
Validation loss: 2.6433874724890245

Epoch: 6| Step: 5
Training loss: 2.456868031331041
Validation loss: 2.644701770879488

Epoch: 6| Step: 6
Training loss: 3.807521133013542
Validation loss: 2.6445506362324434

Epoch: 6| Step: 7
Training loss: 2.918900061595134
Validation loss: 2.6485398873171557

Epoch: 6| Step: 8
Training loss: 2.923902654621664
Validation loss: 2.6452493647312223

Epoch: 6| Step: 9
Training loss: 2.9464217710210097
Validation loss: 2.6506541017774836

Epoch: 6| Step: 10
Training loss: 3.0773083528147165
Validation loss: 2.652410841718569

Epoch: 6| Step: 11
Training loss: 3.2362783240548194
Validation loss: 2.651198557395203

Epoch: 6| Step: 12
Training loss: 2.7365130104049795
Validation loss: 2.6516101209245013

Epoch: 6| Step: 13
Training loss: 2.7005634850150586
Validation loss: 2.6497911028834484

Epoch: 93| Step: 0
Training loss: 2.774540899291856
Validation loss: 2.645294365810632

Epoch: 6| Step: 1
Training loss: 2.2704282160107194
Validation loss: 2.644168717367407

Epoch: 6| Step: 2
Training loss: 3.35787802765866
Validation loss: 2.643445227268037

Epoch: 6| Step: 3
Training loss: 2.8601223328576584
Validation loss: 2.642151192209897

Epoch: 6| Step: 4
Training loss: 3.179406155972947
Validation loss: 2.641748156354446

Epoch: 6| Step: 5
Training loss: 3.014658876925559
Validation loss: 2.6401726925799576

Epoch: 6| Step: 6
Training loss: 3.0728407683401433
Validation loss: 2.6398333659989324

Epoch: 6| Step: 7
Training loss: 2.759895640210828
Validation loss: 2.641352596525954

Epoch: 6| Step: 8
Training loss: 2.9431649633040795
Validation loss: 2.641806387539038

Epoch: 6| Step: 9
Training loss: 3.2967885082886026
Validation loss: 2.640938824847504

Epoch: 6| Step: 10
Training loss: 3.8404042639126312
Validation loss: 2.644476421421155

Epoch: 6| Step: 11
Training loss: 2.895592622335104
Validation loss: 2.644047944687047

Epoch: 6| Step: 12
Training loss: 2.6051034285208363
Validation loss: 2.64414018738147

Epoch: 6| Step: 13
Training loss: 2.5403355620092087
Validation loss: 2.6434156090929304

Epoch: 94| Step: 0
Training loss: 2.9381683887815147
Validation loss: 2.6499259527428762

Epoch: 6| Step: 1
Training loss: 3.0410206426580313
Validation loss: 2.647820042132893

Epoch: 6| Step: 2
Training loss: 2.7558281435099103
Validation loss: 2.6524486773204194

Epoch: 6| Step: 3
Training loss: 3.2891431060119856
Validation loss: 2.6588389182792715

Epoch: 6| Step: 4
Training loss: 3.2466491150691654
Validation loss: 2.664960798830113

Epoch: 6| Step: 5
Training loss: 3.157288031270246
Validation loss: 2.6499305732255687

Epoch: 6| Step: 6
Training loss: 3.013913474346463
Validation loss: 2.64360774503634

Epoch: 6| Step: 7
Training loss: 3.0556247375825243
Validation loss: 2.6379948115186265

Epoch: 6| Step: 8
Training loss: 2.703955814818806
Validation loss: 2.6333327966172635

Epoch: 6| Step: 9
Training loss: 3.1716870783117135
Validation loss: 2.634271878334303

Epoch: 6| Step: 10
Training loss: 2.6044343327610604
Validation loss: 2.6297947367846173

Epoch: 6| Step: 11
Training loss: 3.0152593993892913
Validation loss: 2.6304102380421384

Epoch: 6| Step: 12
Training loss: 2.7595332237492762
Validation loss: 2.6316529681315073

Epoch: 6| Step: 13
Training loss: 2.9252128914850375
Validation loss: 2.629617420260753

Epoch: 95| Step: 0
Training loss: 3.6492200253222458
Validation loss: 2.634645647809034

Epoch: 6| Step: 1
Training loss: 3.023415104588406
Validation loss: 2.631358708822259

Epoch: 6| Step: 2
Training loss: 3.338030906483881
Validation loss: 2.6313736715412555

Epoch: 6| Step: 3
Training loss: 2.3499222925202843
Validation loss: 2.62985244303976

Epoch: 6| Step: 4
Training loss: 2.9538105163961452
Validation loss: 2.6318694878100093

Epoch: 6| Step: 5
Training loss: 2.534205465608293
Validation loss: 2.630643737941567

Epoch: 6| Step: 6
Training loss: 2.933516703278351
Validation loss: 2.631929147334443

Epoch: 6| Step: 7
Training loss: 3.2451556855224695
Validation loss: 2.631314382315188

Epoch: 6| Step: 8
Training loss: 2.9120944743603525
Validation loss: 2.6310568184238763

Epoch: 6| Step: 9
Training loss: 3.000769198670802
Validation loss: 2.63132602105494

Epoch: 6| Step: 10
Training loss: 2.8660312576772276
Validation loss: 2.6349936393872264

Epoch: 6| Step: 11
Training loss: 2.6863581871091284
Validation loss: 2.634935805896263

Epoch: 6| Step: 12
Training loss: 2.9329947153350115
Validation loss: 2.633083155905448

Epoch: 6| Step: 13
Training loss: 3.199912791255891
Validation loss: 2.632055992384411

Epoch: 96| Step: 0
Training loss: 3.412164435463736
Validation loss: 2.641618747832319

Epoch: 6| Step: 1
Training loss: 3.3887197177818638
Validation loss: 2.6423819101747457

Epoch: 6| Step: 2
Training loss: 3.2211913692421743
Validation loss: 2.6460043819819896

Epoch: 6| Step: 3
Training loss: 2.7312090174483106
Validation loss: 2.6510662304407706

Epoch: 6| Step: 4
Training loss: 1.9815469850162282
Validation loss: 2.660317841337306

Epoch: 6| Step: 5
Training loss: 3.545961237093646
Validation loss: 2.6688279630850484

Epoch: 6| Step: 6
Training loss: 2.741904915271879
Validation loss: 2.6397408334445394

Epoch: 6| Step: 7
Training loss: 3.16319040595275
Validation loss: 2.6250941488685027

Epoch: 6| Step: 8
Training loss: 2.841917348713531
Validation loss: 2.626215056335438

Epoch: 6| Step: 9
Training loss: 3.172634390892131
Validation loss: 2.619955053163894

Epoch: 6| Step: 10
Training loss: 3.172815343216248
Validation loss: 2.623798335563952

Epoch: 6| Step: 11
Training loss: 2.5705221258790574
Validation loss: 2.6246634051212845

Epoch: 6| Step: 12
Training loss: 2.6033053588666673
Validation loss: 2.6220268008346985

Epoch: 6| Step: 13
Training loss: 2.9350315842236903
Validation loss: 2.6245786599247705

Epoch: 97| Step: 0
Training loss: 3.1691798894785066
Validation loss: 2.6244176227388363

Epoch: 6| Step: 1
Training loss: 2.88158756686615
Validation loss: 2.6255692325281563

Epoch: 6| Step: 2
Training loss: 2.9746419162185087
Validation loss: 2.626376903785807

Epoch: 6| Step: 3
Training loss: 2.7910511609879087
Validation loss: 2.6233737691681682

Epoch: 6| Step: 4
Training loss: 2.7159508512934925
Validation loss: 2.6234330062526254

Epoch: 6| Step: 5
Training loss: 3.4675792746087004
Validation loss: 2.623089403158896

Epoch: 6| Step: 6
Training loss: 3.1293877887102624
Validation loss: 2.6223154868412193

Epoch: 6| Step: 7
Training loss: 2.97415760840179
Validation loss: 2.6244262760769836

Epoch: 6| Step: 8
Training loss: 2.8597471901724494
Validation loss: 2.6218675409843764

Epoch: 6| Step: 9
Training loss: 3.029812186686937
Validation loss: 2.6273363335191156

Epoch: 6| Step: 10
Training loss: 2.872334281052074
Validation loss: 2.6344587932984607

Epoch: 6| Step: 11
Training loss: 2.955290146991539
Validation loss: 2.637893418258143

Epoch: 6| Step: 12
Training loss: 3.2473303760986814
Validation loss: 2.6422505815219677

Epoch: 6| Step: 13
Training loss: 2.228394300282546
Validation loss: 2.6437388375432893

Epoch: 98| Step: 0
Training loss: 3.0668857855536222
Validation loss: 2.6455465375632095

Epoch: 6| Step: 1
Training loss: 3.426439698293621
Validation loss: 2.651882304685433

Epoch: 6| Step: 2
Training loss: 2.4821267181373634
Validation loss: 2.631490635607138

Epoch: 6| Step: 3
Training loss: 2.938479483423274
Validation loss: 2.6223538612664448

Epoch: 6| Step: 4
Training loss: 2.7137775770110184
Validation loss: 2.619002722723061

Epoch: 6| Step: 5
Training loss: 3.350900437913966
Validation loss: 2.6195021675101726

Epoch: 6| Step: 6
Training loss: 3.3785568144818345
Validation loss: 2.6202571522797697

Epoch: 6| Step: 7
Training loss: 3.301476327518056
Validation loss: 2.6177739046752726

Epoch: 6| Step: 8
Training loss: 2.641370859842683
Validation loss: 2.6176266263415346

Epoch: 6| Step: 9
Training loss: 2.9275265467846583
Validation loss: 2.6189125504175745

Epoch: 6| Step: 10
Training loss: 3.2236092932628098
Validation loss: 2.6166718809040455

Epoch: 6| Step: 11
Training loss: 3.0973047968677068
Validation loss: 2.614562792751054

Epoch: 6| Step: 12
Training loss: 2.0873655081727955
Validation loss: 2.6174034993141833

Epoch: 6| Step: 13
Training loss: 2.623544380232831
Validation loss: 2.6155896729429453

Epoch: 99| Step: 0
Training loss: 3.027395097850826
Validation loss: 2.6145795370996954

Epoch: 6| Step: 1
Training loss: 2.887366195154656
Validation loss: 2.621814265455776

Epoch: 6| Step: 2
Training loss: 2.896652124219883
Validation loss: 2.626662950812427

Epoch: 6| Step: 3
Training loss: 2.4186395506567275
Validation loss: 2.629831280522209

Epoch: 6| Step: 4
Training loss: 3.8542900409369203
Validation loss: 2.6408006328501172

Epoch: 6| Step: 5
Training loss: 2.7887323435212044
Validation loss: 2.6367920232547712

Epoch: 6| Step: 6
Training loss: 2.343091439866347
Validation loss: 2.6311887993185388

Epoch: 6| Step: 7
Training loss: 3.074430310580993
Validation loss: 2.621902864453727

Epoch: 6| Step: 8
Training loss: 2.9715766700842443
Validation loss: 2.6133231346319783

Epoch: 6| Step: 9
Training loss: 2.578956701044637
Validation loss: 2.6110108344366982

Epoch: 6| Step: 10
Training loss: 3.6767299291356386
Validation loss: 2.6121767557594238

Epoch: 6| Step: 11
Training loss: 1.9764766749871319
Validation loss: 2.611570479395973

Epoch: 6| Step: 12
Training loss: 3.4120891114077034
Validation loss: 2.611116994641559

Epoch: 6| Step: 13
Training loss: 3.3712992688384698
Validation loss: 2.6116169962936953

Epoch: 100| Step: 0
Training loss: 2.8418116408551093
Validation loss: 2.612866055917845

Epoch: 6| Step: 1
Training loss: 2.6372558046805565
Validation loss: 2.611226343269539

Epoch: 6| Step: 2
Training loss: 3.2842993236646865
Validation loss: 2.610678440760878

Epoch: 6| Step: 3
Training loss: 3.2408390044397075
Validation loss: 2.6096749725548603

Epoch: 6| Step: 4
Training loss: 2.5463074595916284
Validation loss: 2.607331664997605

Epoch: 6| Step: 5
Training loss: 2.8070412962362408
Validation loss: 2.609122092301331

Epoch: 6| Step: 6
Training loss: 3.555250588557229
Validation loss: 2.610799144245831

Epoch: 6| Step: 7
Training loss: 2.775818138832994
Validation loss: 2.615290470454931

Epoch: 6| Step: 8
Training loss: 2.914956127117947
Validation loss: 2.6115362236134656

Epoch: 6| Step: 9
Training loss: 2.9989666748637536
Validation loss: 2.610593843558725

Epoch: 6| Step: 10
Training loss: 2.862524580537629
Validation loss: 2.60944250485063

Epoch: 6| Step: 11
Training loss: 3.3227099570591827
Validation loss: 2.6064555731589674

Epoch: 6| Step: 12
Training loss: 2.7925699015912633
Validation loss: 2.609393505796897

Epoch: 6| Step: 13
Training loss: 2.7887731236380153
Validation loss: 2.610265335158945

Epoch: 101| Step: 0
Training loss: 2.7512894988355168
Validation loss: 2.6119721522502

Epoch: 6| Step: 1
Training loss: 2.711972349678577
Validation loss: 2.6127894104699503

Epoch: 6| Step: 2
Training loss: 3.6585053880648464
Validation loss: 2.6140566288279032

Epoch: 6| Step: 3
Training loss: 3.1294024165746537
Validation loss: 2.615857110694979

Epoch: 6| Step: 4
Training loss: 2.9224400917354516
Validation loss: 2.618401756218448

Epoch: 6| Step: 5
Training loss: 2.3696805711353393
Validation loss: 2.6385464002559353

Epoch: 6| Step: 6
Training loss: 3.2503415441778722
Validation loss: 2.658443119331746

Epoch: 6| Step: 7
Training loss: 2.6963342471483336
Validation loss: 2.695846814255023

Epoch: 6| Step: 8
Training loss: 3.1604565075798323
Validation loss: 2.7947076908741817

Epoch: 6| Step: 9
Training loss: 2.888100341216111
Validation loss: 2.760408124641549

Epoch: 6| Step: 10
Training loss: 2.8871009583957616
Validation loss: 2.6705689552327225

Epoch: 6| Step: 11
Training loss: 2.917633568485531
Validation loss: 2.6034086211105136

Epoch: 6| Step: 12
Training loss: 3.3287490474572072
Validation loss: 2.6009136576909344

Epoch: 6| Step: 13
Training loss: 2.9253133035435983
Validation loss: 2.6256730564038553

Epoch: 102| Step: 0
Training loss: 3.273060779243799
Validation loss: 2.706668857854642

Epoch: 6| Step: 1
Training loss: 2.652055188129808
Validation loss: 2.759012198332678

Epoch: 6| Step: 2
Training loss: 2.7157406869001885
Validation loss: 2.7215142356607234

Epoch: 6| Step: 3
Training loss: 3.525562263178089
Validation loss: 2.7062529330022316

Epoch: 6| Step: 4
Training loss: 2.496970630098737
Validation loss: 2.696577764181622

Epoch: 6| Step: 5
Training loss: 3.3634930587159344
Validation loss: 2.699948138533082

Epoch: 6| Step: 6
Training loss: 3.2243448168559516
Validation loss: 2.69505247978466

Epoch: 6| Step: 7
Training loss: 3.1130086092586318
Validation loss: 2.694881841023657

Epoch: 6| Step: 8
Training loss: 3.641178596548163
Validation loss: 2.6873603797888017

Epoch: 6| Step: 9
Training loss: 2.7026963086954727
Validation loss: 2.6840145874152133

Epoch: 6| Step: 10
Training loss: 2.873249018317878
Validation loss: 2.685285584567092

Epoch: 6| Step: 11
Training loss: 2.5579305706608397
Validation loss: 2.698533989122071

Epoch: 6| Step: 12
Training loss: 3.0875945845104633
Validation loss: 2.730466804595831

Epoch: 6| Step: 13
Training loss: 3.3237678768943986
Validation loss: 2.7060146171541803

Epoch: 103| Step: 0
Training loss: 2.942013944996058
Validation loss: 2.6640373375303086

Epoch: 6| Step: 1
Training loss: 3.5057443073909016
Validation loss: 2.607357236073185

Epoch: 6| Step: 2
Training loss: 3.2564587604869866
Validation loss: 2.605425271030002

Epoch: 6| Step: 3
Training loss: 3.0312556630504446
Validation loss: 2.6067231484642686

Epoch: 6| Step: 4
Training loss: 2.9206649804312512
Validation loss: 2.611587144765928

Epoch: 6| Step: 5
Training loss: 3.1068592725105155
Validation loss: 2.6525424811117726

Epoch: 6| Step: 6
Training loss: 2.5917856522340856
Validation loss: 2.6677994232118047

Epoch: 6| Step: 7
Training loss: 3.82640666153679
Validation loss: 2.7093739942152872

Epoch: 6| Step: 8
Training loss: 2.907570815663086
Validation loss: 2.669877281338363

Epoch: 6| Step: 9
Training loss: 3.032594048534688
Validation loss: 2.635107343183682

Epoch: 6| Step: 10
Training loss: 2.837205503532464
Validation loss: 2.635790486647344

Epoch: 6| Step: 11
Training loss: 2.889239334793125
Validation loss: 2.6334510034579846

Epoch: 6| Step: 12
Training loss: 2.034336151830321
Validation loss: 2.636716774317261

Epoch: 6| Step: 13
Training loss: 2.3568250000069924
Validation loss: 2.648769117720143

Epoch: 104| Step: 0
Training loss: 2.7850103781487934
Validation loss: 2.6515897140891656

Epoch: 6| Step: 1
Training loss: 2.1918243172613168
Validation loss: 2.6548531485344475

Epoch: 6| Step: 2
Training loss: 3.3953046016539807
Validation loss: 2.680950367447844

Epoch: 6| Step: 3
Training loss: 3.559984490221506
Validation loss: 2.700668457342905

Epoch: 6| Step: 4
Training loss: 3.02210957544738
Validation loss: 2.683188651606005

Epoch: 6| Step: 5
Training loss: 3.2643871242729685
Validation loss: 2.6597862037469815

Epoch: 6| Step: 6
Training loss: 3.081433991361041
Validation loss: 2.6493994503468667

Epoch: 6| Step: 7
Training loss: 2.457555863452745
Validation loss: 2.6316908751925006

Epoch: 6| Step: 8
Training loss: 2.706916399304246
Validation loss: 2.613243032728699

Epoch: 6| Step: 9
Training loss: 3.492751517608621
Validation loss: 2.6142939673935803

Epoch: 6| Step: 10
Training loss: 3.375566646902873
Validation loss: 2.6013758255281694

Epoch: 6| Step: 11
Training loss: 2.986404607578078
Validation loss: 2.5907727453647142

Epoch: 6| Step: 12
Training loss: 2.221131457083528
Validation loss: 2.5924903063227442

Epoch: 6| Step: 13
Training loss: 2.7603604484927375
Validation loss: 2.595895657834495

Epoch: 105| Step: 0
Training loss: 2.232682660907915
Validation loss: 2.6027544257220026

Epoch: 6| Step: 1
Training loss: 3.130863639462161
Validation loss: 2.6317254151190426

Epoch: 6| Step: 2
Training loss: 3.1517065602532477
Validation loss: 2.645438918627897

Epoch: 6| Step: 3
Training loss: 3.3916816998522714
Validation loss: 2.6902098800163987

Epoch: 6| Step: 4
Training loss: 2.811484768208371
Validation loss: 2.6810035938081986

Epoch: 6| Step: 5
Training loss: 3.0843449213325673
Validation loss: 2.663756348171625

Epoch: 6| Step: 6
Training loss: 2.094006650766561
Validation loss: 2.62216249289664

Epoch: 6| Step: 7
Training loss: 2.776572505351143
Validation loss: 2.5974822132951623

Epoch: 6| Step: 8
Training loss: 3.0160537968297407
Validation loss: 2.5941442216476593

Epoch: 6| Step: 9
Training loss: 3.0751272407816024
Validation loss: 2.591938163349126

Epoch: 6| Step: 10
Training loss: 2.737804420301648
Validation loss: 2.5917513070636566

Epoch: 6| Step: 11
Training loss: 3.4631756505478974
Validation loss: 2.5923121776869187

Epoch: 6| Step: 12
Training loss: 3.1794958408338707
Validation loss: 2.591207868839627

Epoch: 6| Step: 13
Training loss: 3.3098675326206175
Validation loss: 2.5922265568335834

Epoch: 106| Step: 0
Training loss: 3.0324695139686755
Validation loss: 2.5898022221224117

Epoch: 6| Step: 1
Training loss: 2.9047215862439053
Validation loss: 2.5899248700836597

Epoch: 6| Step: 2
Training loss: 2.590655399599703
Validation loss: 2.586795757772779

Epoch: 6| Step: 3
Training loss: 2.8596804929873993
Validation loss: 2.5852684437079168

Epoch: 6| Step: 4
Training loss: 3.0195328291756867
Validation loss: 2.582842799787785

Epoch: 6| Step: 5
Training loss: 3.099248758165474
Validation loss: 2.5807026893538363

Epoch: 6| Step: 6
Training loss: 2.516471578181718
Validation loss: 2.581747965526973

Epoch: 6| Step: 7
Training loss: 3.602241104639187
Validation loss: 2.583023203183956

Epoch: 6| Step: 8
Training loss: 2.798472028441543
Validation loss: 2.583025467066727

Epoch: 6| Step: 9
Training loss: 3.2901448623163776
Validation loss: 2.583892942927654

Epoch: 6| Step: 10
Training loss: 2.893000436022634
Validation loss: 2.5888503932030225

Epoch: 6| Step: 11
Training loss: 2.9000185538389
Validation loss: 2.5937576293833104

Epoch: 6| Step: 12
Training loss: 3.2622324282076245
Validation loss: 2.5990550080163137

Epoch: 6| Step: 13
Training loss: 1.9286218086605431
Validation loss: 2.606570760840805

Epoch: 107| Step: 0
Training loss: 3.448344377955109
Validation loss: 2.606591623396449

Epoch: 6| Step: 1
Training loss: 2.6548053178693447
Validation loss: 2.6159896065823274

Epoch: 6| Step: 2
Training loss: 2.9639741730587956
Validation loss: 2.6094805292271186

Epoch: 6| Step: 3
Training loss: 3.243414589183236
Validation loss: 2.637746098952666

Epoch: 6| Step: 4
Training loss: 3.341100338125359
Validation loss: 2.6499212074523286

Epoch: 6| Step: 5
Training loss: 2.2653301178653225
Validation loss: 2.62018456920733

Epoch: 6| Step: 6
Training loss: 3.083557137967109
Validation loss: 2.590703879279113

Epoch: 6| Step: 7
Training loss: 2.6188982937506857
Validation loss: 2.579912568167594

Epoch: 6| Step: 8
Training loss: 2.4927270000400195
Validation loss: 2.577010918398637

Epoch: 6| Step: 9
Training loss: 2.5068566708214477
Validation loss: 2.578178725890056

Epoch: 6| Step: 10
Training loss: 3.2229844174767353
Validation loss: 2.5753615677739394

Epoch: 6| Step: 11
Training loss: 2.729336352326935
Validation loss: 2.576512186945976

Epoch: 6| Step: 12
Training loss: 3.082217985619522
Validation loss: 2.5751216285446983

Epoch: 6| Step: 13
Training loss: 3.6257110424514654
Validation loss: 2.5806809886885977

Epoch: 108| Step: 0
Training loss: 3.062530206025303
Validation loss: 2.5797568160258475

Epoch: 6| Step: 1
Training loss: 3.4263365761953426
Validation loss: 2.5791019064258127

Epoch: 6| Step: 2
Training loss: 3.162646318149346
Validation loss: 2.5801581754844243

Epoch: 6| Step: 3
Training loss: 2.7435914708526754
Validation loss: 2.5886759065925924

Epoch: 6| Step: 4
Training loss: 2.8938392707781224
Validation loss: 2.5973230829711067

Epoch: 6| Step: 5
Training loss: 2.167231143877748
Validation loss: 2.6040934956433195

Epoch: 6| Step: 6
Training loss: 2.5418914528758156
Validation loss: 2.6007643767547624

Epoch: 6| Step: 7
Training loss: 3.0004195873569603
Validation loss: 2.640065530438803

Epoch: 6| Step: 8
Training loss: 3.1771277919639505
Validation loss: 2.6616595723379204

Epoch: 6| Step: 9
Training loss: 3.197588345617263
Validation loss: 2.660752768931332

Epoch: 6| Step: 10
Training loss: 2.9694641860418285
Validation loss: 2.68114219827636

Epoch: 6| Step: 11
Training loss: 3.596280608459623
Validation loss: 2.666009886165

Epoch: 6| Step: 12
Training loss: 2.4838308062221124
Validation loss: 2.6286904575988514

Epoch: 6| Step: 13
Training loss: 2.668874115034397
Validation loss: 2.5886299905749626

Epoch: 109| Step: 0
Training loss: 2.9066645829021205
Validation loss: 2.5827880870132156

Epoch: 6| Step: 1
Training loss: 2.7080241882731513
Validation loss: 2.581412331185556

Epoch: 6| Step: 2
Training loss: 2.99101756358498
Validation loss: 2.5855322575044286

Epoch: 6| Step: 3
Training loss: 2.196619913802871
Validation loss: 2.5826228071362323

Epoch: 6| Step: 4
Training loss: 1.8595889954078673
Validation loss: 2.5798347916506805

Epoch: 6| Step: 5
Training loss: 3.4008965376090883
Validation loss: 2.5708039299538625

Epoch: 6| Step: 6
Training loss: 2.8091997068580716
Validation loss: 2.5700601802441927

Epoch: 6| Step: 7
Training loss: 3.4284411275036213
Validation loss: 2.570578017242577

Epoch: 6| Step: 8
Training loss: 3.0656031243246638
Validation loss: 2.5702786458816775

Epoch: 6| Step: 9
Training loss: 3.024170621559442
Validation loss: 2.5726937468995605

Epoch: 6| Step: 10
Training loss: 3.0008076534286525
Validation loss: 2.5751825957844265

Epoch: 6| Step: 11
Training loss: 3.260518439088818
Validation loss: 2.573198260642049

Epoch: 6| Step: 12
Training loss: 3.1515237907033384
Validation loss: 2.5739998628295497

Epoch: 6| Step: 13
Training loss: 2.9801148370723354
Validation loss: 2.5847460624849927

Epoch: 110| Step: 0
Training loss: 2.911255494469207
Validation loss: 2.583432995173471

Epoch: 6| Step: 1
Training loss: 3.3100015106658716
Validation loss: 2.581152884595537

Epoch: 6| Step: 2
Training loss: 2.847489633382225
Validation loss: 2.581854738154425

Epoch: 6| Step: 3
Training loss: 3.236413875127556
Validation loss: 2.5869727173443717

Epoch: 6| Step: 4
Training loss: 3.069150903933882
Validation loss: 2.5810834857204625

Epoch: 6| Step: 5
Training loss: 2.566225553564292
Validation loss: 2.5856176270890057

Epoch: 6| Step: 6
Training loss: 2.6317017699741747
Validation loss: 2.586354602385945

Epoch: 6| Step: 7
Training loss: 3.153005310751002
Validation loss: 2.585884229756181

Epoch: 6| Step: 8
Training loss: 2.4438284858919683
Validation loss: 2.5810058448086672

Epoch: 6| Step: 9
Training loss: 2.2537977169357384
Validation loss: 2.5795588578357815

Epoch: 6| Step: 10
Training loss: 3.3944455608970303
Validation loss: 2.5759439509752693

Epoch: 6| Step: 11
Training loss: 2.958749088211467
Validation loss: 2.579684366452229

Epoch: 6| Step: 12
Training loss: 3.1849902875383886
Validation loss: 2.5697277371511804

Epoch: 6| Step: 13
Training loss: 2.786407660383044
Validation loss: 2.5713585492498536

Epoch: 111| Step: 0
Training loss: 2.94123586538593
Validation loss: 2.5696904933123204

Epoch: 6| Step: 1
Training loss: 3.1825791315997223
Validation loss: 2.5684402758142317

Epoch: 6| Step: 2
Training loss: 2.5537643397858036
Validation loss: 2.566772378881491

Epoch: 6| Step: 3
Training loss: 2.7489387892162878
Validation loss: 2.5672800883895586

Epoch: 6| Step: 4
Training loss: 2.5156938057724463
Validation loss: 2.570117811284052

Epoch: 6| Step: 5
Training loss: 2.6685378443704257
Validation loss: 2.5685466324874

Epoch: 6| Step: 6
Training loss: 2.0998648554594763
Validation loss: 2.5637073253829095

Epoch: 6| Step: 7
Training loss: 3.3314592020215237
Validation loss: 2.5661079244710465

Epoch: 6| Step: 8
Training loss: 3.215353349457138
Validation loss: 2.57085015936337

Epoch: 6| Step: 9
Training loss: 3.229666853692474
Validation loss: 2.569192552030766

Epoch: 6| Step: 10
Training loss: 3.1271525789371273
Validation loss: 2.567473006256497

Epoch: 6| Step: 11
Training loss: 3.4369273315596702
Validation loss: 2.5715040331384014

Epoch: 6| Step: 12
Training loss: 2.39127495600614
Validation loss: 2.5779753794778655

Epoch: 6| Step: 13
Training loss: 3.449281650889867
Validation loss: 2.57743925966475

Epoch: 112| Step: 0
Training loss: 2.384038690969906
Validation loss: 2.596066646302304

Epoch: 6| Step: 1
Training loss: 3.52525996338543
Validation loss: 2.6035197764251725

Epoch: 6| Step: 2
Training loss: 2.5820881293375746
Validation loss: 2.625244363640984

Epoch: 6| Step: 3
Training loss: 3.8056183289287584
Validation loss: 2.6340619383264174

Epoch: 6| Step: 4
Training loss: 2.9467407325423487
Validation loss: 2.5952540579813363

Epoch: 6| Step: 5
Training loss: 2.6069756172424174
Validation loss: 2.57499963597296

Epoch: 6| Step: 6
Training loss: 3.228536893078768
Validation loss: 2.5695101266093228

Epoch: 6| Step: 7
Training loss: 2.4785416934639235
Validation loss: 2.566592134731451

Epoch: 6| Step: 8
Training loss: 2.9131633608865664
Validation loss: 2.5618524901325377

Epoch: 6| Step: 9
Training loss: 2.7300756168722384
Validation loss: 2.5612668067150373

Epoch: 6| Step: 10
Training loss: 2.75896898611293
Validation loss: 2.561638325806866

Epoch: 6| Step: 11
Training loss: 3.1514213564448874
Validation loss: 2.562911605092021

Epoch: 6| Step: 12
Training loss: 2.4565614575649644
Validation loss: 2.5636259277081463

Epoch: 6| Step: 13
Training loss: 3.1491754391189604
Validation loss: 2.5602036418147858

Epoch: 113| Step: 0
Training loss: 3.3332585962182253
Validation loss: 2.5626718186891844

Epoch: 6| Step: 1
Training loss: 2.5301837323687497
Validation loss: 2.5631323303583047

Epoch: 6| Step: 2
Training loss: 3.017990054559721
Validation loss: 2.5636839253848374

Epoch: 6| Step: 3
Training loss: 3.0864651385843933
Validation loss: 2.5669755845918965

Epoch: 6| Step: 4
Training loss: 2.937129869882943
Validation loss: 2.568812698514455

Epoch: 6| Step: 5
Training loss: 2.194231846593802
Validation loss: 2.569520617568659

Epoch: 6| Step: 6
Training loss: 2.9308002444096792
Validation loss: 2.570531643314075

Epoch: 6| Step: 7
Training loss: 2.303438320095075
Validation loss: 2.5674979718140762

Epoch: 6| Step: 8
Training loss: 2.8769605834615644
Validation loss: 2.565123992605106

Epoch: 6| Step: 9
Training loss: 2.9112713821431764
Validation loss: 2.567727030931228

Epoch: 6| Step: 10
Training loss: 3.130938723505179
Validation loss: 2.574815555987256

Epoch: 6| Step: 11
Training loss: 3.3641430042093923
Validation loss: 2.5783187987275737

Epoch: 6| Step: 12
Training loss: 3.205348289954168
Validation loss: 2.589455485076052

Epoch: 6| Step: 13
Training loss: 3.0925449374857354
Validation loss: 2.590738230418868

Epoch: 114| Step: 0
Training loss: 2.668926731663276
Validation loss: 2.600867343144305

Epoch: 6| Step: 1
Training loss: 2.9228540096151634
Validation loss: 2.6152711721857815

Epoch: 6| Step: 2
Training loss: 2.8609094723589568
Validation loss: 2.6310273375152597

Epoch: 6| Step: 3
Training loss: 3.1616405118250213
Validation loss: 2.617068874981003

Epoch: 6| Step: 4
Training loss: 3.06759095844444
Validation loss: 2.587573734941151

Epoch: 6| Step: 5
Training loss: 2.417116474077195
Validation loss: 2.5760561430926936

Epoch: 6| Step: 6
Training loss: 3.466747083709342
Validation loss: 2.5671117398567347

Epoch: 6| Step: 7
Training loss: 3.3696322671176726
Validation loss: 2.567819477905616

Epoch: 6| Step: 8
Training loss: 3.029827767440586
Validation loss: 2.5738778798395696

Epoch: 6| Step: 9
Training loss: 2.6030065978967185
Validation loss: 2.579350749580046

Epoch: 6| Step: 10
Training loss: 2.356018305955114
Validation loss: 2.5888029785204876

Epoch: 6| Step: 11
Training loss: 2.5653098913015215
Validation loss: 2.596094001654297

Epoch: 6| Step: 12
Training loss: 3.189955868550275
Validation loss: 2.5967414665745823

Epoch: 6| Step: 13
Training loss: 3.178604578913803
Validation loss: 2.600218387019063

Epoch: 115| Step: 0
Training loss: 2.507323410494636
Validation loss: 2.578006848822071

Epoch: 6| Step: 1
Training loss: 3.068211426254907
Validation loss: 2.5722493706338803

Epoch: 6| Step: 2
Training loss: 3.0261483274868026
Validation loss: 2.5714006461702343

Epoch: 6| Step: 3
Training loss: 2.6916975907319887
Validation loss: 2.56820770767855

Epoch: 6| Step: 4
Training loss: 2.6589921148957245
Validation loss: 2.5673449395039154

Epoch: 6| Step: 5
Training loss: 3.2449986048432393
Validation loss: 2.5651893939287866

Epoch: 6| Step: 6
Training loss: 2.6876390997795934
Validation loss: 2.5635247171296354

Epoch: 6| Step: 7
Training loss: 2.911302338289797
Validation loss: 2.5588381935675173

Epoch: 6| Step: 8
Training loss: 3.1162946373611247
Validation loss: 2.561312575686717

Epoch: 6| Step: 9
Training loss: 2.8481207156154342
Validation loss: 2.558264107860011

Epoch: 6| Step: 10
Training loss: 3.148093988123074
Validation loss: 2.5590665351483706

Epoch: 6| Step: 11
Training loss: 2.953130974334648
Validation loss: 2.555596928739394

Epoch: 6| Step: 12
Training loss: 3.590724434401247
Validation loss: 2.5553126801195223

Epoch: 6| Step: 13
Training loss: 1.959998646949768
Validation loss: 2.554471170373443

Epoch: 116| Step: 0
Training loss: 3.073001838839943
Validation loss: 2.5549309847719046

Epoch: 6| Step: 1
Training loss: 2.80524526166401
Validation loss: 2.553857198838853

Epoch: 6| Step: 2
Training loss: 3.2519355292316754
Validation loss: 2.5587649873858997

Epoch: 6| Step: 3
Training loss: 2.7916882547331445
Validation loss: 2.559686368136015

Epoch: 6| Step: 4
Training loss: 3.4752473208010484
Validation loss: 2.5674097250741315

Epoch: 6| Step: 5
Training loss: 2.5370105139755452
Validation loss: 2.579480070831384

Epoch: 6| Step: 6
Training loss: 2.8418056002872927
Validation loss: 2.5899037480544447

Epoch: 6| Step: 7
Training loss: 3.0843174025579274
Validation loss: 2.603192403034175

Epoch: 6| Step: 8
Training loss: 2.2620805045861023
Validation loss: 2.6200378676747014

Epoch: 6| Step: 9
Training loss: 3.0526573674208266
Validation loss: 2.626111940164587

Epoch: 6| Step: 10
Training loss: 3.0412011160412344
Validation loss: 2.5946193008779317

Epoch: 6| Step: 11
Training loss: 2.8057031519205267
Validation loss: 2.5663642489659844

Epoch: 6| Step: 12
Training loss: 2.6715673079867397
Validation loss: 2.551938903485374

Epoch: 6| Step: 13
Training loss: 2.9873485830074276
Validation loss: 2.5523331969152756

Epoch: 117| Step: 0
Training loss: 2.845186646143202
Validation loss: 2.5561863493534727

Epoch: 6| Step: 1
Training loss: 2.9543244686046077
Validation loss: 2.556492201433508

Epoch: 6| Step: 2
Training loss: 2.6428504008498046
Validation loss: 2.5602340734328957

Epoch: 6| Step: 3
Training loss: 2.422580936052216
Validation loss: 2.5622713810872195

Epoch: 6| Step: 4
Training loss: 2.865031999073115
Validation loss: 2.5645188913132464

Epoch: 6| Step: 5
Training loss: 2.4887121956222984
Validation loss: 2.5617400872671223

Epoch: 6| Step: 6
Training loss: 3.319261954667943
Validation loss: 2.5635065082416304

Epoch: 6| Step: 7
Training loss: 3.100851520367782
Validation loss: 2.562881529362874

Epoch: 6| Step: 8
Training loss: 3.2006218782784766
Validation loss: 2.5624219600900506

Epoch: 6| Step: 9
Training loss: 3.127805137476973
Validation loss: 2.5606984764677323

Epoch: 6| Step: 10
Training loss: 2.7655803439743076
Validation loss: 2.559328715529566

Epoch: 6| Step: 11
Training loss: 2.7923874470184753
Validation loss: 2.560537675882147

Epoch: 6| Step: 12
Training loss: 3.228315491424345
Validation loss: 2.559870858062734

Epoch: 6| Step: 13
Training loss: 3.41725383148213
Validation loss: 2.561911980937254

Epoch: 118| Step: 0
Training loss: 2.477983614531032
Validation loss: 2.561352181847404

Epoch: 6| Step: 1
Training loss: 3.391780533312441
Validation loss: 2.5592523369266633

Epoch: 6| Step: 2
Training loss: 3.3776664090898247
Validation loss: 2.566115935748347

Epoch: 6| Step: 3
Training loss: 2.271207364510921
Validation loss: 2.5704339091582975

Epoch: 6| Step: 4
Training loss: 3.181693828927088
Validation loss: 2.5727501799934482

Epoch: 6| Step: 5
Training loss: 2.8206504555890555
Validation loss: 2.5793807435997773

Epoch: 6| Step: 6
Training loss: 3.053401588554366
Validation loss: 2.6028004323797775

Epoch: 6| Step: 7
Training loss: 3.2128998266395588
Validation loss: 2.592189915145533

Epoch: 6| Step: 8
Training loss: 3.228840538701971
Validation loss: 2.592037069710374

Epoch: 6| Step: 9
Training loss: 2.491371523452713
Validation loss: 2.5782915853934933

Epoch: 6| Step: 10
Training loss: 2.6658879176143597
Validation loss: 2.5751167852303936

Epoch: 6| Step: 11
Training loss: 2.5868184873754085
Validation loss: 2.5619009104485566

Epoch: 6| Step: 12
Training loss: 2.872786457595626
Validation loss: 2.5613282869060408

Epoch: 6| Step: 13
Training loss: 2.8730716042974653
Validation loss: 2.5594280036046424

Epoch: 119| Step: 0
Training loss: 2.980257879274466
Validation loss: 2.5644399721347706

Epoch: 6| Step: 1
Training loss: 2.5604072026408145
Validation loss: 2.5799771891907737

Epoch: 6| Step: 2
Training loss: 3.170671958342495
Validation loss: 2.592936486452787

Epoch: 6| Step: 3
Training loss: 2.5266793511239207
Validation loss: 2.6238385886791833

Epoch: 6| Step: 4
Training loss: 3.423411960410782
Validation loss: 2.68350425396999

Epoch: 6| Step: 5
Training loss: 3.572344379172215
Validation loss: 2.6579928717817674

Epoch: 6| Step: 6
Training loss: 3.543438830834218
Validation loss: 2.6145117883552023

Epoch: 6| Step: 7
Training loss: 2.74967460441145
Validation loss: 2.574049476867734

Epoch: 6| Step: 8
Training loss: 2.703642867210455
Validation loss: 2.5531499833960782

Epoch: 6| Step: 9
Training loss: 2.5057124676832188
Validation loss: 2.5465334274368456

Epoch: 6| Step: 10
Training loss: 2.740796293257942
Validation loss: 2.5432000063020395

Epoch: 6| Step: 11
Training loss: 2.770608754246999
Validation loss: 2.5420187325197605

Epoch: 6| Step: 12
Training loss: 2.6687816537592384
Validation loss: 2.5439693863356543

Epoch: 6| Step: 13
Training loss: 2.60595588762681
Validation loss: 2.5464521457545577

Epoch: 120| Step: 0
Training loss: 2.4680601677260854
Validation loss: 2.548153818228969

Epoch: 6| Step: 1
Training loss: 3.122675527093676
Validation loss: 2.5473634930950615

Epoch: 6| Step: 2
Training loss: 2.4908552764542033
Validation loss: 2.542929582877179

Epoch: 6| Step: 3
Training loss: 2.960601223895848
Validation loss: 2.543352509718766

Epoch: 6| Step: 4
Training loss: 2.5323229740853685
Validation loss: 2.5424656246848234

Epoch: 6| Step: 5
Training loss: 3.298478446211166
Validation loss: 2.54413022442826

Epoch: 6| Step: 6
Training loss: 3.368996260494015
Validation loss: 2.5446375416346956

Epoch: 6| Step: 7
Training loss: 2.441377538893678
Validation loss: 2.5437334218809897

Epoch: 6| Step: 8
Training loss: 3.239898829823276
Validation loss: 2.546380073517377

Epoch: 6| Step: 9
Training loss: 2.9343520808518213
Validation loss: 2.539358018159085

Epoch: 6| Step: 10
Training loss: 3.0552387959604395
Validation loss: 2.5413902356983793

Epoch: 6| Step: 11
Training loss: 3.1461667185246163
Validation loss: 2.5398386274046074

Epoch: 6| Step: 12
Training loss: 2.5830810074990853
Validation loss: 2.5485374138110033

Epoch: 6| Step: 13
Training loss: 3.2614115861636956
Validation loss: 2.5455870153240583

Epoch: 121| Step: 0
Training loss: 3.0104805346691053
Validation loss: 2.543793923978692

Epoch: 6| Step: 1
Training loss: 3.3649770468939377
Validation loss: 2.5486628517368737

Epoch: 6| Step: 2
Training loss: 2.8865633085158247
Validation loss: 2.562054407018374

Epoch: 6| Step: 3
Training loss: 2.7455006985973496
Validation loss: 2.5808420133113126

Epoch: 6| Step: 4
Training loss: 3.2572114605000833
Validation loss: 2.6177769288163937

Epoch: 6| Step: 5
Training loss: 2.7302100150403827
Validation loss: 2.662189925800202

Epoch: 6| Step: 6
Training loss: 2.2424001559651785
Validation loss: 2.6730781273408106

Epoch: 6| Step: 7
Training loss: 3.000502703192049
Validation loss: 2.6909092758143425

Epoch: 6| Step: 8
Training loss: 2.7699509747845648
Validation loss: 2.6952539067544996

Epoch: 6| Step: 9
Training loss: 2.7975952244046702
Validation loss: 2.6698322519498343

Epoch: 6| Step: 10
Training loss: 2.6606528201002977
Validation loss: 2.658879419968788

Epoch: 6| Step: 11
Training loss: 3.2911852874942564
Validation loss: 2.6342085731823097

Epoch: 6| Step: 12
Training loss: 3.083872739330807
Validation loss: 2.623474770827791

Epoch: 6| Step: 13
Training loss: 3.0681702417739674
Validation loss: 2.6166747201696734

Epoch: 122| Step: 0
Training loss: 2.9149685593965686
Validation loss: 2.607143985666251

Epoch: 6| Step: 1
Training loss: 2.458553649168023
Validation loss: 2.6048110259894446

Epoch: 6| Step: 2
Training loss: 3.1400795837813518
Validation loss: 2.6041661422893037

Epoch: 6| Step: 3
Training loss: 2.8926806589482568
Validation loss: 2.5881330979333046

Epoch: 6| Step: 4
Training loss: 2.85104791884224
Validation loss: 2.5822366542765973

Epoch: 6| Step: 5
Training loss: 2.8293145981449075
Validation loss: 2.5713555044146092

Epoch: 6| Step: 6
Training loss: 2.9820624374105975
Validation loss: 2.559653062547727

Epoch: 6| Step: 7
Training loss: 3.018044563464984
Validation loss: 2.557770568290265

Epoch: 6| Step: 8
Training loss: 3.0862123777399275
Validation loss: 2.5583998569621595

Epoch: 6| Step: 9
Training loss: 3.101186119322684
Validation loss: 2.5493987163326115

Epoch: 6| Step: 10
Training loss: 2.79693944419907
Validation loss: 2.546731191491065

Epoch: 6| Step: 11
Training loss: 2.8425560425507013
Validation loss: 2.5404660761408926

Epoch: 6| Step: 12
Training loss: 2.8080857285674834
Validation loss: 2.5365698142760977

Epoch: 6| Step: 13
Training loss: 3.319337661352156
Validation loss: 2.5340451084062017

Epoch: 123| Step: 0
Training loss: 2.412307743456337
Validation loss: 2.5347851670734247

Epoch: 6| Step: 1
Training loss: 2.807453969440522
Validation loss: 2.533043842932776

Epoch: 6| Step: 2
Training loss: 3.321076428662781
Validation loss: 2.5346638850958363

Epoch: 6| Step: 3
Training loss: 2.801589657619434
Validation loss: 2.530048305396144

Epoch: 6| Step: 4
Training loss: 2.765543101331944
Validation loss: 2.530275440570283

Epoch: 6| Step: 5
Training loss: 2.9877922904706904
Validation loss: 2.5297937968432516

Epoch: 6| Step: 6
Training loss: 2.6408514343328315
Validation loss: 2.5284319400074398

Epoch: 6| Step: 7
Training loss: 3.3461316619798747
Validation loss: 2.5315003387127595

Epoch: 6| Step: 8
Training loss: 2.907310702458525
Validation loss: 2.533027715401589

Epoch: 6| Step: 9
Training loss: 3.2843154393683682
Validation loss: 2.5342842489713604

Epoch: 6| Step: 10
Training loss: 3.13076753535177
Validation loss: 2.535830808210314

Epoch: 6| Step: 11
Training loss: 2.542126306548765
Validation loss: 2.5337522963637054

Epoch: 6| Step: 12
Training loss: 2.70588692923254
Validation loss: 2.539181671753565

Epoch: 6| Step: 13
Training loss: 2.861257464997839
Validation loss: 2.542293840032395

Epoch: 124| Step: 0
Training loss: 3.5121305604010824
Validation loss: 2.548665810018597

Epoch: 6| Step: 1
Training loss: 2.828287699534885
Validation loss: 2.53881308063351

Epoch: 6| Step: 2
Training loss: 3.2317123879130887
Validation loss: 2.546142305640148

Epoch: 6| Step: 3
Training loss: 2.7116307847975034
Validation loss: 2.549181673395643

Epoch: 6| Step: 4
Training loss: 3.284371771116693
Validation loss: 2.554636318746147

Epoch: 6| Step: 5
Training loss: 2.7358947890001675
Validation loss: 2.553639516873058

Epoch: 6| Step: 6
Training loss: 3.3061599517270497
Validation loss: 2.5713760265742915

Epoch: 6| Step: 7
Training loss: 2.2956458715024386
Validation loss: 2.5899837769525162

Epoch: 6| Step: 8
Training loss: 2.835149145761284
Validation loss: 2.585560634032519

Epoch: 6| Step: 9
Training loss: 2.939170849195691
Validation loss: 2.5811623022461943

Epoch: 6| Step: 10
Training loss: 2.299719158939077
Validation loss: 2.547309178664922

Epoch: 6| Step: 11
Training loss: 2.552578770302902
Validation loss: 2.541371323485373

Epoch: 6| Step: 12
Training loss: 3.1880999729749857
Validation loss: 2.5420429728495932

Epoch: 6| Step: 13
Training loss: 2.363925612697693
Validation loss: 2.5374842077628954

Epoch: 125| Step: 0
Training loss: 2.788397958268428
Validation loss: 2.5451426696532886

Epoch: 6| Step: 1
Training loss: 2.852553939911135
Validation loss: 2.548506088069718

Epoch: 6| Step: 2
Training loss: 3.616506821459161
Validation loss: 2.549947743581123

Epoch: 6| Step: 3
Training loss: 2.7405305295906053
Validation loss: 2.5590081402940084

Epoch: 6| Step: 4
Training loss: 2.934927442911291
Validation loss: 2.555405279254336

Epoch: 6| Step: 5
Training loss: 2.844125576160228
Validation loss: 2.559694625859777

Epoch: 6| Step: 6
Training loss: 2.889930105453123
Validation loss: 2.5409050810280975

Epoch: 6| Step: 7
Training loss: 3.1000082508100415
Validation loss: 2.5343666087547105

Epoch: 6| Step: 8
Training loss: 3.560185667832637
Validation loss: 2.532311776267375

Epoch: 6| Step: 9
Training loss: 2.3777203289511406
Validation loss: 2.5341055494453366

Epoch: 6| Step: 10
Training loss: 2.837352221165558
Validation loss: 2.527565272823735

Epoch: 6| Step: 11
Training loss: 2.7703824261975156
Validation loss: 2.53168705071717

Epoch: 6| Step: 12
Training loss: 2.1606125858337566
Validation loss: 2.528837154139321

Epoch: 6| Step: 13
Training loss: 2.622846628414964
Validation loss: 2.5288848299332165

Epoch: 126| Step: 0
Training loss: 2.9346487939276775
Validation loss: 2.5286065521607504

Epoch: 6| Step: 1
Training loss: 2.488117973204644
Validation loss: 2.5280691883607087

Epoch: 6| Step: 2
Training loss: 3.305626413421127
Validation loss: 2.533949602070028

Epoch: 6| Step: 3
Training loss: 2.807252184154365
Validation loss: 2.5270922650567402

Epoch: 6| Step: 4
Training loss: 2.8712918790587163
Validation loss: 2.526714397160545

Epoch: 6| Step: 5
Training loss: 2.8337531807187086
Validation loss: 2.5292486586167646

Epoch: 6| Step: 6
Training loss: 3.177337452882483
Validation loss: 2.529692717333625

Epoch: 6| Step: 7
Training loss: 2.520983562774517
Validation loss: 2.5355319589383365

Epoch: 6| Step: 8
Training loss: 3.0610640915869003
Validation loss: 2.5380104024441126

Epoch: 6| Step: 9
Training loss: 2.9387438149591976
Validation loss: 2.5407796001344023

Epoch: 6| Step: 10
Training loss: 3.2566753204865857
Validation loss: 2.549221659606482

Epoch: 6| Step: 11
Training loss: 2.4494894507812286
Validation loss: 2.5550150846112243

Epoch: 6| Step: 12
Training loss: 2.8234115861168987
Validation loss: 2.5765887125831646

Epoch: 6| Step: 13
Training loss: 2.5408547047731442
Validation loss: 2.5921550739086405

Epoch: 127| Step: 0
Training loss: 3.258671855333425
Validation loss: 2.610650750670486

Epoch: 6| Step: 1
Training loss: 3.010138546109209
Validation loss: 2.575446336809509

Epoch: 6| Step: 2
Training loss: 2.225372853274275
Validation loss: 2.545706683757765

Epoch: 6| Step: 3
Training loss: 3.3725797664407966
Validation loss: 2.5314553158671536

Epoch: 6| Step: 4
Training loss: 3.083399866434473
Validation loss: 2.5261817811233067

Epoch: 6| Step: 5
Training loss: 2.5043941028495684
Validation loss: 2.5301283074442393

Epoch: 6| Step: 6
Training loss: 2.894868447538203
Validation loss: 2.5271836522837403

Epoch: 6| Step: 7
Training loss: 3.0175713301400036
Validation loss: 2.5324794640038606

Epoch: 6| Step: 8
Training loss: 2.554586985999276
Validation loss: 2.539950664156751

Epoch: 6| Step: 9
Training loss: 3.0470214808539104
Validation loss: 2.536656030225632

Epoch: 6| Step: 10
Training loss: 2.811239850342689
Validation loss: 2.5325993484012046

Epoch: 6| Step: 11
Training loss: 3.2813755556335478
Validation loss: 2.5291525423880983

Epoch: 6| Step: 12
Training loss: 2.6329349296923654
Validation loss: 2.5286619465686515

Epoch: 6| Step: 13
Training loss: 2.1727260321885287
Validation loss: 2.5289524518050586

Epoch: 128| Step: 0
Training loss: 2.9292454093525335
Validation loss: 2.528165701701734

Epoch: 6| Step: 1
Training loss: 2.9656050236213063
Validation loss: 2.5295060243333527

Epoch: 6| Step: 2
Training loss: 2.8552710259861387
Validation loss: 2.536556512784186

Epoch: 6| Step: 3
Training loss: 3.416736943212241
Validation loss: 2.5395213520425917

Epoch: 6| Step: 4
Training loss: 2.6865308921652296
Validation loss: 2.548348605528687

Epoch: 6| Step: 5
Training loss: 2.6008571092308626
Validation loss: 2.571729058048682

Epoch: 6| Step: 6
Training loss: 2.792354147986007
Validation loss: 2.585914980758683

Epoch: 6| Step: 7
Training loss: 2.721083067512408
Validation loss: 2.6033707256761174

Epoch: 6| Step: 8
Training loss: 3.5147128808085424
Validation loss: 2.6051906818527106

Epoch: 6| Step: 9
Training loss: 3.1706907570289764
Validation loss: 2.609598932352646

Epoch: 6| Step: 10
Training loss: 2.3649335649445975
Validation loss: 2.6042350933970493

Epoch: 6| Step: 11
Training loss: 2.458461521060907
Validation loss: 2.6133372490602196

Epoch: 6| Step: 12
Training loss: 2.8350021459177768
Validation loss: 2.591343046192307

Epoch: 6| Step: 13
Training loss: 3.0686229298809287
Validation loss: 2.5664048585007495

Epoch: 129| Step: 0
Training loss: 2.8314371682925055
Validation loss: 2.557206583876852

Epoch: 6| Step: 1
Training loss: 2.66387813523073
Validation loss: 2.552664791030976

Epoch: 6| Step: 2
Training loss: 2.8094505732707105
Validation loss: 2.559917088626925

Epoch: 6| Step: 3
Training loss: 2.606818860221654
Validation loss: 2.5617182149588396

Epoch: 6| Step: 4
Training loss: 2.3741036781675566
Validation loss: 2.5620825782943015

Epoch: 6| Step: 5
Training loss: 2.5696050255803504
Validation loss: 2.567789977822266

Epoch: 6| Step: 6
Training loss: 2.4861363339225
Validation loss: 2.579751500436486

Epoch: 6| Step: 7
Training loss: 3.1805376795488955
Validation loss: 2.5773057378598296

Epoch: 6| Step: 8
Training loss: 3.4114322263383605
Validation loss: 2.5673363109571175

Epoch: 6| Step: 9
Training loss: 3.4398335252445698
Validation loss: 2.5681378771291623

Epoch: 6| Step: 10
Training loss: 2.5677408728301305
Validation loss: 2.56239405267183

Epoch: 6| Step: 11
Training loss: 3.17626634109419
Validation loss: 2.560392253742693

Epoch: 6| Step: 12
Training loss: 3.047398918296771
Validation loss: 2.564593832544414

Epoch: 6| Step: 13
Training loss: 3.027009337628533
Validation loss: 2.572183379383919

Epoch: 130| Step: 0
Training loss: 3.1938034262173027
Validation loss: 2.5659620037546356

Epoch: 6| Step: 1
Training loss: 2.321453700086807
Validation loss: 2.5674809084314054

Epoch: 6| Step: 2
Training loss: 3.252548099194421
Validation loss: 2.552668468774046

Epoch: 6| Step: 3
Training loss: 2.5148570150148175
Validation loss: 2.5472123882425204

Epoch: 6| Step: 4
Training loss: 3.2397908004286315
Validation loss: 2.545867328011685

Epoch: 6| Step: 5
Training loss: 3.0839024267730077
Validation loss: 2.547130884658038

Epoch: 6| Step: 6
Training loss: 3.0747836029080022
Validation loss: 2.5565561870519655

Epoch: 6| Step: 7
Training loss: 3.059609119158219
Validation loss: 2.559647922547645

Epoch: 6| Step: 8
Training loss: 2.6867192043471557
Validation loss: 2.573461079343912

Epoch: 6| Step: 9
Training loss: 2.7442615631713405
Validation loss: 2.5794673712641147

Epoch: 6| Step: 10
Training loss: 2.505973545723345
Validation loss: 2.5782379095834664

Epoch: 6| Step: 11
Training loss: 3.350181597442121
Validation loss: 2.5813462921315793

Epoch: 6| Step: 12
Training loss: 2.0785080406138055
Validation loss: 2.58002850069961

Epoch: 6| Step: 13
Training loss: 2.777968796414045
Validation loss: 2.583549904916472

Epoch: 131| Step: 0
Training loss: 2.8630828996888518
Validation loss: 2.5969776659695807

Epoch: 6| Step: 1
Training loss: 3.0200771218361924
Validation loss: 2.584651248298914

Epoch: 6| Step: 2
Training loss: 3.0061452232707513
Validation loss: 2.583725197598082

Epoch: 6| Step: 3
Training loss: 3.189579005189663
Validation loss: 2.5522579629773463

Epoch: 6| Step: 4
Training loss: 2.5110129971124167
Validation loss: 2.5500350132194636

Epoch: 6| Step: 5
Training loss: 2.8949816066440937
Validation loss: 2.5494893442001674

Epoch: 6| Step: 6
Training loss: 2.7187873574682877
Validation loss: 2.607539238016148

Epoch: 6| Step: 7
Training loss: 3.170428166881019
Validation loss: 2.635182081121911

Epoch: 6| Step: 8
Training loss: 3.0542366495133395
Validation loss: 2.6681831345916245

Epoch: 6| Step: 9
Training loss: 3.542665243555065
Validation loss: 2.6579500513861016

Epoch: 6| Step: 10
Training loss: 2.8454631633556025
Validation loss: 2.611615919445765

Epoch: 6| Step: 11
Training loss: 3.1712823323491284
Validation loss: 2.602254727974048

Epoch: 6| Step: 12
Training loss: 2.8325459190208755
Validation loss: 2.602775987658134

Epoch: 6| Step: 13
Training loss: 1.613249549831373
Validation loss: 2.6334533992180447

Epoch: 132| Step: 0
Training loss: 2.7470622410064056
Validation loss: 2.6656844543913523

Epoch: 6| Step: 1
Training loss: 3.2260123977754
Validation loss: 2.7125021479188747

Epoch: 6| Step: 2
Training loss: 3.5317004431321086
Validation loss: 2.775258104426489

Epoch: 6| Step: 3
Training loss: 2.78895419041382
Validation loss: 2.752930412005301

Epoch: 6| Step: 4
Training loss: 2.5760912647606684
Validation loss: 2.7336063140147226

Epoch: 6| Step: 5
Training loss: 3.4574863974744288
Validation loss: 2.724049300411249

Epoch: 6| Step: 6
Training loss: 2.441229876441657
Validation loss: 2.7034117853404718

Epoch: 6| Step: 7
Training loss: 2.669287784181442
Validation loss: 2.705334257245547

Epoch: 6| Step: 8
Training loss: 2.698625645707716
Validation loss: 2.716451002668108

Epoch: 6| Step: 9
Training loss: 2.388988657896088
Validation loss: 2.722729716049309

Epoch: 6| Step: 10
Training loss: 2.7409550432194605
Validation loss: 2.7280091366666452

Epoch: 6| Step: 11
Training loss: 3.494175287611049
Validation loss: 2.7003080205257657

Epoch: 6| Step: 12
Training loss: 3.5656649267182843
Validation loss: 2.6810138210850964

Epoch: 6| Step: 13
Training loss: 3.0797406134435654
Validation loss: 2.648433936854739

Epoch: 133| Step: 0
Training loss: 3.007366989631819
Validation loss: 2.6415714733419415

Epoch: 6| Step: 1
Training loss: 2.7804072099667096
Validation loss: 2.65161421155422

Epoch: 6| Step: 2
Training loss: 2.8924902594162476
Validation loss: 2.6514763458303814

Epoch: 6| Step: 3
Training loss: 3.3342192108455735
Validation loss: 2.653157881417009

Epoch: 6| Step: 4
Training loss: 2.8894339092173165
Validation loss: 2.644602785830166

Epoch: 6| Step: 5
Training loss: 2.8246020720425316
Validation loss: 2.651949057165836

Epoch: 6| Step: 6
Training loss: 2.872331458875099
Validation loss: 2.651002621840136

Epoch: 6| Step: 7
Training loss: 2.628148143906808
Validation loss: 2.6465642777083764

Epoch: 6| Step: 8
Training loss: 3.032280186472296
Validation loss: 2.634852451633757

Epoch: 6| Step: 9
Training loss: 3.4803095480389237
Validation loss: 2.6310016544793156

Epoch: 6| Step: 10
Training loss: 3.0387806896572815
Validation loss: 2.627168194552184

Epoch: 6| Step: 11
Training loss: 3.1197202713597902
Validation loss: 2.6225036102979855

Epoch: 6| Step: 12
Training loss: 2.4661499044887583
Validation loss: 2.6216782180078653

Epoch: 6| Step: 13
Training loss: 2.1900380261050567
Validation loss: 2.6184239970427257

Epoch: 134| Step: 0
Training loss: 3.4541047376381364
Validation loss: 2.622976547702272

Epoch: 6| Step: 1
Training loss: 1.9277328908618907
Validation loss: 2.6296035609019603

Epoch: 6| Step: 2
Training loss: 3.374153030931979
Validation loss: 2.6379480707320955

Epoch: 6| Step: 3
Training loss: 2.8430468350154396
Validation loss: 2.6549535870363896

Epoch: 6| Step: 4
Training loss: 2.5155234938335527
Validation loss: 2.650251965299197

Epoch: 6| Step: 5
Training loss: 3.301168962991261
Validation loss: 2.6532593319580053

Epoch: 6| Step: 6
Training loss: 2.8638765515089357
Validation loss: 2.6341314334062904

Epoch: 6| Step: 7
Training loss: 2.971672466769489
Validation loss: 2.6152264445887203

Epoch: 6| Step: 8
Training loss: 3.0414309649831446
Validation loss: 2.6000400542494053

Epoch: 6| Step: 9
Training loss: 3.2895153757766056
Validation loss: 2.5934852220818247

Epoch: 6| Step: 10
Training loss: 2.252658968279079
Validation loss: 2.5908913058466863

Epoch: 6| Step: 11
Training loss: 2.8837518583042887
Validation loss: 2.5906328669420833

Epoch: 6| Step: 12
Training loss: 2.9119865652549954
Validation loss: 2.5928023675799654

Epoch: 6| Step: 13
Training loss: 2.877385186656478
Validation loss: 2.594609466693937

Epoch: 135| Step: 0
Training loss: 2.5129235968612553
Validation loss: 2.5936746577699057

Epoch: 6| Step: 1
Training loss: 2.51956474814214
Validation loss: 2.6012542452312895

Epoch: 6| Step: 2
Training loss: 2.7389229581707255
Validation loss: 2.6154575292669904

Epoch: 6| Step: 3
Training loss: 2.947212072129848
Validation loss: 2.6394933673882606

Epoch: 6| Step: 4
Training loss: 3.4649118719962684
Validation loss: 2.6589415375590195

Epoch: 6| Step: 5
Training loss: 3.1788744437594594
Validation loss: 2.663949744338231

Epoch: 6| Step: 6
Training loss: 3.357783166642756
Validation loss: 2.6801971665513835

Epoch: 6| Step: 7
Training loss: 3.1110680346307236
Validation loss: 2.7113560193373343

Epoch: 6| Step: 8
Training loss: 2.7663273323650692
Validation loss: 2.6997735285149185

Epoch: 6| Step: 9
Training loss: 2.390251579291328
Validation loss: 2.6864968688743467

Epoch: 6| Step: 10
Training loss: 2.700139155157444
Validation loss: 2.652656229041334

Epoch: 6| Step: 11
Training loss: 2.6527761744504903
Validation loss: 2.629585585352984

Epoch: 6| Step: 12
Training loss: 2.5628769760105614
Validation loss: 2.6097233302575766

Epoch: 6| Step: 13
Training loss: 3.846841452829582
Validation loss: 2.596333938049005

Epoch: 136| Step: 0
Training loss: 3.149597105872637
Validation loss: 2.5905346274107632

Epoch: 6| Step: 1
Training loss: 2.977596233616025
Validation loss: 2.5899777092946246

Epoch: 6| Step: 2
Training loss: 2.1282740783010454
Validation loss: 2.5906541507592507

Epoch: 6| Step: 3
Training loss: 2.9085951336599414
Validation loss: 2.5824990633955385

Epoch: 6| Step: 4
Training loss: 3.041673338024888
Validation loss: 2.585473965611974

Epoch: 6| Step: 5
Training loss: 3.0510821126955343
Validation loss: 2.581327831562118

Epoch: 6| Step: 6
Training loss: 2.859439869963011
Validation loss: 2.5862816536767808

Epoch: 6| Step: 7
Training loss: 2.8688818475348103
Validation loss: 2.5848710494703915

Epoch: 6| Step: 8
Training loss: 2.777765040898262
Validation loss: 2.5794156868190643

Epoch: 6| Step: 9
Training loss: 3.3556995245874455
Validation loss: 2.5857478097832405

Epoch: 6| Step: 10
Training loss: 2.69262083244683
Validation loss: 2.6016726975680062

Epoch: 6| Step: 11
Training loss: 3.0304734326626623
Validation loss: 2.622627111646211

Epoch: 6| Step: 12
Training loss: 2.4905535564442474
Validation loss: 2.6325654548415467

Epoch: 6| Step: 13
Training loss: 3.360835835627513
Validation loss: 2.6302876810885527

Epoch: 137| Step: 0
Training loss: 3.055475079796403
Validation loss: 2.609217019444782

Epoch: 6| Step: 1
Training loss: 2.7964972555282444
Validation loss: 2.587526142800139

Epoch: 6| Step: 2
Training loss: 3.1934146234708485
Validation loss: 2.5698143056415685

Epoch: 6| Step: 3
Training loss: 2.9765955004527287
Validation loss: 2.574529179309275

Epoch: 6| Step: 4
Training loss: 2.6929066410986735
Validation loss: 2.5641238362129615

Epoch: 6| Step: 5
Training loss: 2.7062198804628483
Validation loss: 2.5608056842629883

Epoch: 6| Step: 6
Training loss: 3.293221569198314
Validation loss: 2.564557677732329

Epoch: 6| Step: 7
Training loss: 2.316023436333861
Validation loss: 2.5598566311028246

Epoch: 6| Step: 8
Training loss: 2.0938552146779825
Validation loss: 2.565775420915001

Epoch: 6| Step: 9
Training loss: 2.448963498773053
Validation loss: 2.5805725293167314

Epoch: 6| Step: 10
Training loss: 3.0610940002344726
Validation loss: 2.602926799005963

Epoch: 6| Step: 11
Training loss: 2.9781970244561546
Validation loss: 2.624899016985442

Epoch: 6| Step: 12
Training loss: 3.441066071305851
Validation loss: 2.6179540944943955

Epoch: 6| Step: 13
Training loss: 3.14683230540644
Validation loss: 2.590238397905526

Epoch: 138| Step: 0
Training loss: 3.0304473128793536
Validation loss: 2.571883338642878

Epoch: 6| Step: 1
Training loss: 3.0182941858739465
Validation loss: 2.5624384588799742

Epoch: 6| Step: 2
Training loss: 3.240566206778553
Validation loss: 2.5660407812115387

Epoch: 6| Step: 3
Training loss: 2.4802977495498637
Validation loss: 2.562065045578809

Epoch: 6| Step: 4
Training loss: 3.0811405804687535
Validation loss: 2.563958676022576

Epoch: 6| Step: 5
Training loss: 2.59267890599799
Validation loss: 2.563138872647128

Epoch: 6| Step: 6
Training loss: 2.963333810892771
Validation loss: 2.5636251797037457

Epoch: 6| Step: 7
Training loss: 2.9058502906834747
Validation loss: 2.5621329654220015

Epoch: 6| Step: 8
Training loss: 3.301782219251333
Validation loss: 2.566404290113545

Epoch: 6| Step: 9
Training loss: 2.808821922233646
Validation loss: 2.5670935783825795

Epoch: 6| Step: 10
Training loss: 2.6037216620109565
Validation loss: 2.566452693976282

Epoch: 6| Step: 11
Training loss: 2.7014761951223076
Validation loss: 2.5669016946478633

Epoch: 6| Step: 12
Training loss: 2.640522068884229
Validation loss: 2.5608554087676176

Epoch: 6| Step: 13
Training loss: 2.6296590602921257
Validation loss: 2.561080116049302

Epoch: 139| Step: 0
Training loss: 2.983984317472427
Validation loss: 2.570483286839121

Epoch: 6| Step: 1
Training loss: 2.821423678885386
Validation loss: 2.5807855714008805

Epoch: 6| Step: 2
Training loss: 3.2986156334204653
Validation loss: 2.579432798467506

Epoch: 6| Step: 3
Training loss: 2.9984492426448996
Validation loss: 2.567660503112117

Epoch: 6| Step: 4
Training loss: 2.9727778366026443
Validation loss: 2.5644540067204065

Epoch: 6| Step: 5
Training loss: 3.0894529768301897
Validation loss: 2.56702678048179

Epoch: 6| Step: 6
Training loss: 2.980278999017928
Validation loss: 2.557873579088516

Epoch: 6| Step: 7
Training loss: 2.9090990437588964
Validation loss: 2.5549047605442197

Epoch: 6| Step: 8
Training loss: 2.4478462912541485
Validation loss: 2.5573761508366863

Epoch: 6| Step: 9
Training loss: 2.863771154691036
Validation loss: 2.5503006602846523

Epoch: 6| Step: 10
Training loss: 2.1828553072009362
Validation loss: 2.5552106024893484

Epoch: 6| Step: 11
Training loss: 2.3676327327591724
Validation loss: 2.5563690260130887

Epoch: 6| Step: 12
Training loss: 3.1972787492094716
Validation loss: 2.559850984762452

Epoch: 6| Step: 13
Training loss: 2.5914187698130826
Validation loss: 2.5711452864133464

Epoch: 140| Step: 0
Training loss: 2.6802296437615825
Validation loss: 2.5779318738728065

Epoch: 6| Step: 1
Training loss: 3.078713917429805
Validation loss: 2.5936734044526855

Epoch: 6| Step: 2
Training loss: 2.5780169840061604
Validation loss: 2.6039623949884736

Epoch: 6| Step: 3
Training loss: 2.6810516355181324
Validation loss: 2.6057089614787428

Epoch: 6| Step: 4
Training loss: 3.0453124123148076
Validation loss: 2.630481610173114

Epoch: 6| Step: 5
Training loss: 2.780356274286245
Validation loss: 2.6570294427938976

Epoch: 6| Step: 6
Training loss: 2.7432325496523564
Validation loss: 2.673650165669128

Epoch: 6| Step: 7
Training loss: 2.7134064526590085
Validation loss: 2.6896410991436377

Epoch: 6| Step: 8
Training loss: 2.4653963899211084
Validation loss: 2.6964501342328826

Epoch: 6| Step: 9
Training loss: 2.78063014977652
Validation loss: 2.653797891640543

Epoch: 6| Step: 10
Training loss: 2.94750700531597
Validation loss: 2.6133451184977448

Epoch: 6| Step: 11
Training loss: 3.2980759589225284
Validation loss: 2.5776077422819936

Epoch: 6| Step: 12
Training loss: 3.4880465335629185
Validation loss: 2.5554106600337305

Epoch: 6| Step: 13
Training loss: 2.946500098611185
Validation loss: 2.5503986010512967

Epoch: 141| Step: 0
Training loss: 3.148876074447288
Validation loss: 2.547960285804846

Epoch: 6| Step: 1
Training loss: 3.4098170090963036
Validation loss: 2.55568534061694

Epoch: 6| Step: 2
Training loss: 2.7486143956339584
Validation loss: 2.550332685729912

Epoch: 6| Step: 3
Training loss: 2.248493113834015
Validation loss: 2.5463419123002837

Epoch: 6| Step: 4
Training loss: 2.603737045453645
Validation loss: 2.548880809867592

Epoch: 6| Step: 5
Training loss: 2.815857303363449
Validation loss: 2.5479716915419606

Epoch: 6| Step: 6
Training loss: 3.2000890719414943
Validation loss: 2.543346526360716

Epoch: 6| Step: 7
Training loss: 3.3170737593382116
Validation loss: 2.5643511652863453

Epoch: 6| Step: 8
Training loss: 2.418012134859164
Validation loss: 2.6038195930458463

Epoch: 6| Step: 9
Training loss: 2.306088813586741
Validation loss: 2.6557527466355335

Epoch: 6| Step: 10
Training loss: 2.9495036208652246
Validation loss: 2.6897740760101767

Epoch: 6| Step: 11
Training loss: 3.1847228873772937
Validation loss: 2.7140186391258667

Epoch: 6| Step: 12
Training loss: 2.712988701318427
Validation loss: 2.6874891539896395

Epoch: 6| Step: 13
Training loss: 3.049324185901305
Validation loss: 2.6643309253029965

Epoch: 142| Step: 0
Training loss: 3.0346220471812106
Validation loss: 2.628663270305131

Epoch: 6| Step: 1
Training loss: 2.7560389708267587
Validation loss: 2.6199749465960367

Epoch: 6| Step: 2
Training loss: 2.444385486912891
Validation loss: 2.589342390895268

Epoch: 6| Step: 3
Training loss: 3.104480821385296
Validation loss: 2.5564392893478542

Epoch: 6| Step: 4
Training loss: 2.8574071898118882
Validation loss: 2.542243955168242

Epoch: 6| Step: 5
Training loss: 2.930065079835312
Validation loss: 2.538763574715893

Epoch: 6| Step: 6
Training loss: 2.919414279667777
Validation loss: 2.544208158354471

Epoch: 6| Step: 7
Training loss: 2.90084401706685
Validation loss: 2.544530932170881

Epoch: 6| Step: 8
Training loss: 2.871701379951055
Validation loss: 2.5431496099826356

Epoch: 6| Step: 9
Training loss: 2.64980125761622
Validation loss: 2.541641978267173

Epoch: 6| Step: 10
Training loss: 3.090165035939152
Validation loss: 2.5409476855103947

Epoch: 6| Step: 11
Training loss: 3.4041811758702583
Validation loss: 2.5418416438264355

Epoch: 6| Step: 12
Training loss: 2.7005497125585376
Validation loss: 2.548052057478026

Epoch: 6| Step: 13
Training loss: 2.7638450972657074
Validation loss: 2.5561260533219055

Epoch: 143| Step: 0
Training loss: 2.449187015406947
Validation loss: 2.5544190142943846

Epoch: 6| Step: 1
Training loss: 3.0270389526502326
Validation loss: 2.571290887055927

Epoch: 6| Step: 2
Training loss: 3.3082923796943127
Validation loss: 2.5803817959445574

Epoch: 6| Step: 3
Training loss: 2.7166679565645055
Validation loss: 2.583152249274898

Epoch: 6| Step: 4
Training loss: 3.2924384327014957
Validation loss: 2.5924278860035836

Epoch: 6| Step: 5
Training loss: 2.9303348894088734
Validation loss: 2.6058567628561726

Epoch: 6| Step: 6
Training loss: 2.692039559185603
Validation loss: 2.6025532895911616

Epoch: 6| Step: 7
Training loss: 2.8276913979641347
Validation loss: 2.6177132291678546

Epoch: 6| Step: 8
Training loss: 2.4895160192436605
Validation loss: 2.624301263657351

Epoch: 6| Step: 9
Training loss: 2.8696152389951246
Validation loss: 2.637707578158374

Epoch: 6| Step: 10
Training loss: 3.2394970129028433
Validation loss: 2.6476628827819324

Epoch: 6| Step: 11
Training loss: 2.9150038929515776
Validation loss: 2.5857210652710667

Epoch: 6| Step: 12
Training loss: 2.2473202748303533
Validation loss: 2.555458134499952

Epoch: 6| Step: 13
Training loss: 3.2789033763873814
Validation loss: 2.53313814164409

Epoch: 144| Step: 0
Training loss: 2.940664656368662
Validation loss: 2.5284870938901554

Epoch: 6| Step: 1
Training loss: 3.275353064688486
Validation loss: 2.527258961128531

Epoch: 6| Step: 2
Training loss: 2.535409406297915
Validation loss: 2.526365098121117

Epoch: 6| Step: 3
Training loss: 2.8057395215879337
Validation loss: 2.527839666385222

Epoch: 6| Step: 4
Training loss: 2.560149906209956
Validation loss: 2.5334848807373858

Epoch: 6| Step: 5
Training loss: 3.1671276928110745
Validation loss: 2.5326555057552853

Epoch: 6| Step: 6
Training loss: 3.0995881453077323
Validation loss: 2.5393185631404114

Epoch: 6| Step: 7
Training loss: 3.0059732734065636
Validation loss: 2.548245628204201

Epoch: 6| Step: 8
Training loss: 2.4577156411575625
Validation loss: 2.544293527030357

Epoch: 6| Step: 9
Training loss: 2.8202604252031045
Validation loss: 2.54652133068653

Epoch: 6| Step: 10
Training loss: 3.2838698328034237
Validation loss: 2.545276964138569

Epoch: 6| Step: 11
Training loss: 2.7827766600351223
Validation loss: 2.553174428310361

Epoch: 6| Step: 12
Training loss: 2.236865702820541
Validation loss: 2.553250137278573

Epoch: 6| Step: 13
Training loss: 2.348036647959033
Validation loss: 2.5634914164076177

Epoch: 145| Step: 0
Training loss: 2.9839112885202854
Validation loss: 2.5718222981755416

Epoch: 6| Step: 1
Training loss: 2.6683201034888255
Validation loss: 2.5812332364521513

Epoch: 6| Step: 2
Training loss: 3.1199868236165704
Validation loss: 2.579507378962845

Epoch: 6| Step: 3
Training loss: 3.008976064762395
Validation loss: 2.574496326633852

Epoch: 6| Step: 4
Training loss: 2.5063801415346783
Validation loss: 2.562014884259385

Epoch: 6| Step: 5
Training loss: 2.6510419164831096
Validation loss: 2.5640886915962007

Epoch: 6| Step: 6
Training loss: 3.0548804336849793
Validation loss: 2.554225763873375

Epoch: 6| Step: 7
Training loss: 3.079950865811893
Validation loss: 2.537693122777985

Epoch: 6| Step: 8
Training loss: 2.1531449227041137
Validation loss: 2.5366843350024104

Epoch: 6| Step: 9
Training loss: 2.797628034946341
Validation loss: 2.5299576225413567

Epoch: 6| Step: 10
Training loss: 2.6853096486132744
Validation loss: 2.5225943977956278

Epoch: 6| Step: 11
Training loss: 2.885494153098973
Validation loss: 2.5185557318667393

Epoch: 6| Step: 12
Training loss: 2.8180050696604213
Validation loss: 2.5149116388724564

Epoch: 6| Step: 13
Training loss: 3.4226002381876115
Validation loss: 2.519678288695777

Epoch: 146| Step: 0
Training loss: 2.3196036029759406
Validation loss: 2.521697591749188

Epoch: 6| Step: 1
Training loss: 3.1243270149369207
Validation loss: 2.516381500232762

Epoch: 6| Step: 2
Training loss: 2.4984013214259297
Validation loss: 2.5159240220868915

Epoch: 6| Step: 3
Training loss: 2.4371611530803587
Validation loss: 2.5140049259718604

Epoch: 6| Step: 4
Training loss: 3.3018724793383543
Validation loss: 2.5212503195299116

Epoch: 6| Step: 5
Training loss: 2.967589784081842
Validation loss: 2.5264833315176207

Epoch: 6| Step: 6
Training loss: 2.9616224961072
Validation loss: 2.528208886415865

Epoch: 6| Step: 7
Training loss: 3.2061089735637824
Validation loss: 2.534153870687327

Epoch: 6| Step: 8
Training loss: 3.300450849079553
Validation loss: 2.5260729394636035

Epoch: 6| Step: 9
Training loss: 2.7507011213389845
Validation loss: 2.5252542313523056

Epoch: 6| Step: 10
Training loss: 2.5897989168501128
Validation loss: 2.5261574641991635

Epoch: 6| Step: 11
Training loss: 1.9700164959792956
Validation loss: 2.5244030723142927

Epoch: 6| Step: 12
Training loss: 2.9673336465127345
Validation loss: 2.5117262519269303

Epoch: 6| Step: 13
Training loss: 2.5772827622250536
Validation loss: 2.5087867019935937

Epoch: 147| Step: 0
Training loss: 2.78980610655698
Validation loss: 2.5045779387633917

Epoch: 6| Step: 1
Training loss: 1.9683686144339982
Validation loss: 2.5049305952193346

Epoch: 6| Step: 2
Training loss: 2.6932783771642756
Validation loss: 2.503832235345105

Epoch: 6| Step: 3
Training loss: 2.5277169142255453
Validation loss: 2.5124229556883537

Epoch: 6| Step: 4
Training loss: 3.2852331780714983
Validation loss: 2.5291412251143255

Epoch: 6| Step: 5
Training loss: 3.0262855698497395
Validation loss: 2.5400942469491037

Epoch: 6| Step: 6
Training loss: 2.606134195293514
Validation loss: 2.551724689757608

Epoch: 6| Step: 7
Training loss: 3.210754834847574
Validation loss: 2.5561532017560578

Epoch: 6| Step: 8
Training loss: 2.915089607759022
Validation loss: 2.5413803831671657

Epoch: 6| Step: 9
Training loss: 2.4858397957946616
Validation loss: 2.539647140400581

Epoch: 6| Step: 10
Training loss: 2.6652872173422013
Validation loss: 2.5145484344512674

Epoch: 6| Step: 11
Training loss: 3.1180620615852823
Validation loss: 2.5186495907777258

Epoch: 6| Step: 12
Training loss: 2.89147112035866
Validation loss: 2.507612830032197

Epoch: 6| Step: 13
Training loss: 2.9356186706331795
Validation loss: 2.5030417406416654

Epoch: 148| Step: 0
Training loss: 2.792402815668763
Validation loss: 2.5006428579090003

Epoch: 6| Step: 1
Training loss: 1.846337458683347
Validation loss: 2.498884712139487

Epoch: 6| Step: 2
Training loss: 2.643100999753407
Validation loss: 2.4965433537488715

Epoch: 6| Step: 3
Training loss: 2.9454128744086834
Validation loss: 2.4975237980988916

Epoch: 6| Step: 4
Training loss: 3.3124216898172665
Validation loss: 2.4958373217968806

Epoch: 6| Step: 5
Training loss: 3.401709794240384
Validation loss: 2.4906682115951506

Epoch: 6| Step: 6
Training loss: 2.1611197945693554
Validation loss: 2.4995871059357384

Epoch: 6| Step: 7
Training loss: 3.101448883358619
Validation loss: 2.499043260055799

Epoch: 6| Step: 8
Training loss: 2.7274292713089143
Validation loss: 2.501444280447228

Epoch: 6| Step: 9
Training loss: 2.6871359822569256
Validation loss: 2.515688619775402

Epoch: 6| Step: 10
Training loss: 2.7388679430944993
Validation loss: 2.518907239480197

Epoch: 6| Step: 11
Training loss: 2.4320149451474204
Validation loss: 2.530029044947014

Epoch: 6| Step: 12
Training loss: 3.5174607294923006
Validation loss: 2.5427758008961843

Epoch: 6| Step: 13
Training loss: 2.216246791262813
Validation loss: 2.5096414095990935

Epoch: 149| Step: 0
Training loss: 2.4723875071485355
Validation loss: 2.495795024773879

Epoch: 6| Step: 1
Training loss: 2.6696271754650396
Validation loss: 2.486232806683791

Epoch: 6| Step: 2
Training loss: 2.355291812254699
Validation loss: 2.4839892798361154

Epoch: 6| Step: 3
Training loss: 3.188522773162454
Validation loss: 2.4829748161097718

Epoch: 6| Step: 4
Training loss: 2.808454358882354
Validation loss: 2.485316488151184

Epoch: 6| Step: 5
Training loss: 3.0809270043372803
Validation loss: 2.481386873154432

Epoch: 6| Step: 6
Training loss: 2.588539312861784
Validation loss: 2.4827080689424212

Epoch: 6| Step: 7
Training loss: 2.8157907519734002
Validation loss: 2.48527524681747

Epoch: 6| Step: 8
Training loss: 2.9006511943664295
Validation loss: 2.483090322970058

Epoch: 6| Step: 9
Training loss: 3.0836336831560476
Validation loss: 2.4902158638935514

Epoch: 6| Step: 10
Training loss: 2.70511390855168
Validation loss: 2.4900600644350255

Epoch: 6| Step: 11
Training loss: 2.718408672786392
Validation loss: 2.4916822705912214

Epoch: 6| Step: 12
Training loss: 3.123021987520051
Validation loss: 2.495721961136

Epoch: 6| Step: 13
Training loss: 2.4917892089955465
Validation loss: 2.499520731690927

Epoch: 150| Step: 0
Training loss: 2.693405848068287
Validation loss: 2.4938987010540616

Epoch: 6| Step: 1
Training loss: 2.8720975405671267
Validation loss: 2.4953324717857046

Epoch: 6| Step: 2
Training loss: 2.6087536100500595
Validation loss: 2.4944446880843123

Epoch: 6| Step: 3
Training loss: 3.4353034197245647
Validation loss: 2.4978179248169328

Epoch: 6| Step: 4
Training loss: 2.4424173687454434
Validation loss: 2.50712103171524

Epoch: 6| Step: 5
Training loss: 2.2928851500677845
Validation loss: 2.5078632322898247

Epoch: 6| Step: 6
Training loss: 2.4891045614114247
Validation loss: 2.5050715053351773

Epoch: 6| Step: 7
Training loss: 2.6590922688359933
Validation loss: 2.491871537091854

Epoch: 6| Step: 8
Training loss: 2.966363690653432
Validation loss: 2.4889502482180266

Epoch: 6| Step: 9
Training loss: 2.766085139213088
Validation loss: 2.485104390383085

Epoch: 6| Step: 10
Training loss: 2.8367426126117823
Validation loss: 2.488619362679954

Epoch: 6| Step: 11
Training loss: 3.107001237194354
Validation loss: 2.491245658217935

Epoch: 6| Step: 12
Training loss: 2.5879178949034913
Validation loss: 2.4915232347089216

Epoch: 6| Step: 13
Training loss: 3.2370246685550583
Validation loss: 2.48704845785287

Epoch: 151| Step: 0
Training loss: 2.689832163399929
Validation loss: 2.4885802704023647

Epoch: 6| Step: 1
Training loss: 2.7931828343431686
Validation loss: 2.4940469117309387

Epoch: 6| Step: 2
Training loss: 3.102678765887953
Validation loss: 2.4898560471972733

Epoch: 6| Step: 3
Training loss: 2.797151264206985
Validation loss: 2.492164021505912

Epoch: 6| Step: 4
Training loss: 2.590167041237625
Validation loss: 2.4910296944940873

Epoch: 6| Step: 5
Training loss: 2.080877789730444
Validation loss: 2.495139219292223

Epoch: 6| Step: 6
Training loss: 2.7112724690893475
Validation loss: 2.4901490586665993

Epoch: 6| Step: 7
Training loss: 2.5285715130859066
Validation loss: 2.4933182763255974

Epoch: 6| Step: 8
Training loss: 2.6536957071925316
Validation loss: 2.4968838806253153

Epoch: 6| Step: 9
Training loss: 3.2408365031630915
Validation loss: 2.5091647093055203

Epoch: 6| Step: 10
Training loss: 2.675022791827781
Validation loss: 2.504673932528344

Epoch: 6| Step: 11
Training loss: 3.0259220452009026
Validation loss: 2.507054511302131

Epoch: 6| Step: 12
Training loss: 3.1758928079941007
Validation loss: 2.498305941429394

Epoch: 6| Step: 13
Training loss: 2.5975625386938304
Validation loss: 2.5003152935597734

Epoch: 152| Step: 0
Training loss: 2.6497266664432297
Validation loss: 2.4983437024966966

Epoch: 6| Step: 1
Training loss: 2.9136096919581993
Validation loss: 2.4974512849436654

Epoch: 6| Step: 2
Training loss: 2.870739475113018
Validation loss: 2.4933338052471603

Epoch: 6| Step: 3
Training loss: 2.02467432111127
Validation loss: 2.496327018554658

Epoch: 6| Step: 4
Training loss: 2.548025982794638
Validation loss: 2.490515205815611

Epoch: 6| Step: 5
Training loss: 2.6792363712939102
Validation loss: 2.4860995784634183

Epoch: 6| Step: 6
Training loss: 3.5089848859477253
Validation loss: 2.4859794935059254

Epoch: 6| Step: 7
Training loss: 2.6540591910343974
Validation loss: 2.492634551115559

Epoch: 6| Step: 8
Training loss: 2.8618615183832414
Validation loss: 2.4984456696112813

Epoch: 6| Step: 9
Training loss: 3.149101698414646
Validation loss: 2.508267661601013

Epoch: 6| Step: 10
Training loss: 2.9785801414631523
Validation loss: 2.5045806318059123

Epoch: 6| Step: 11
Training loss: 2.472341219101484
Validation loss: 2.499100117081099

Epoch: 6| Step: 12
Training loss: 2.323343782260044
Validation loss: 2.4935217290966514

Epoch: 6| Step: 13
Training loss: 3.059469163515135
Validation loss: 2.4952251571426483

Epoch: 153| Step: 0
Training loss: 2.350890206080957
Validation loss: 2.489260590088927

Epoch: 6| Step: 1
Training loss: 2.5881400049360055
Validation loss: 2.483819122463228

Epoch: 6| Step: 2
Training loss: 2.905965811668125
Validation loss: 2.4806195592506537

Epoch: 6| Step: 3
Training loss: 3.279979641548802
Validation loss: 2.4755965396351254

Epoch: 6| Step: 4
Training loss: 2.5763048624992644
Validation loss: 2.478501895064345

Epoch: 6| Step: 5
Training loss: 3.091214306444427
Validation loss: 2.4823635608859287

Epoch: 6| Step: 6
Training loss: 2.543085379789945
Validation loss: 2.480147853096561

Epoch: 6| Step: 7
Training loss: 3.1131449326139573
Validation loss: 2.487876333311831

Epoch: 6| Step: 8
Training loss: 2.71054232572643
Validation loss: 2.4967387749933856

Epoch: 6| Step: 9
Training loss: 3.2415038341691953
Validation loss: 2.514012283411748

Epoch: 6| Step: 10
Training loss: 1.960550520810486
Validation loss: 2.5250645389957156

Epoch: 6| Step: 11
Training loss: 3.0841181160143383
Validation loss: 2.5251461365268897

Epoch: 6| Step: 12
Training loss: 2.779266164238706
Validation loss: 2.528402585691153

Epoch: 6| Step: 13
Training loss: 2.236802816193572
Validation loss: 2.537373176524007

Epoch: 154| Step: 0
Training loss: 2.5902187714547553
Validation loss: 2.534499084057793

Epoch: 6| Step: 1
Training loss: 2.3616348458647534
Validation loss: 2.523791200346143

Epoch: 6| Step: 2
Training loss: 2.8594682188420752
Validation loss: 2.5095933069377505

Epoch: 6| Step: 3
Training loss: 2.7163204869667052
Validation loss: 2.5058406028299864

Epoch: 6| Step: 4
Training loss: 2.893649441844184
Validation loss: 2.5098198780560783

Epoch: 6| Step: 5
Training loss: 3.2941757650818273
Validation loss: 2.506603342141464

Epoch: 6| Step: 6
Training loss: 3.1744401453179196
Validation loss: 2.50913476380554

Epoch: 6| Step: 7
Training loss: 2.7438297407862864
Validation loss: 2.5180368613556765

Epoch: 6| Step: 8
Training loss: 2.557655873102569
Validation loss: 2.511736751524996

Epoch: 6| Step: 9
Training loss: 2.224237024026507
Validation loss: 2.5132239349190972

Epoch: 6| Step: 10
Training loss: 2.3029961020684726
Validation loss: 2.505795566031379

Epoch: 6| Step: 11
Training loss: 3.3274730348246777
Validation loss: 2.502353563257106

Epoch: 6| Step: 12
Training loss: 2.397191820753667
Validation loss: 2.501470389750758

Epoch: 6| Step: 13
Training loss: 3.3694805648249995
Validation loss: 2.500018681692238

Epoch: 155| Step: 0
Training loss: 2.5591669503446486
Validation loss: 2.491286371605223

Epoch: 6| Step: 1
Training loss: 2.5679805113566894
Validation loss: 2.4844212877611906

Epoch: 6| Step: 2
Training loss: 2.778660543892034
Validation loss: 2.478972650679564

Epoch: 6| Step: 3
Training loss: 2.6284327994687713
Validation loss: 2.4832725418632324

Epoch: 6| Step: 4
Training loss: 2.9367941150295613
Validation loss: 2.488443121166385

Epoch: 6| Step: 5
Training loss: 2.081639809517925
Validation loss: 2.499450305269452

Epoch: 6| Step: 6
Training loss: 2.260055325598926
Validation loss: 2.4984500171686665

Epoch: 6| Step: 7
Training loss: 2.8930129626524654
Validation loss: 2.5036197892866787

Epoch: 6| Step: 8
Training loss: 3.142120764283077
Validation loss: 2.5089394058327343

Epoch: 6| Step: 9
Training loss: 3.2489977538340873
Validation loss: 2.5104300927039858

Epoch: 6| Step: 10
Training loss: 2.2832073019102084
Validation loss: 2.5062505927085303

Epoch: 6| Step: 11
Training loss: 3.050372498075302
Validation loss: 2.5203565500726044

Epoch: 6| Step: 12
Training loss: 2.7117972206273095
Validation loss: 2.5240004229820454

Epoch: 6| Step: 13
Training loss: 3.9004642210288725
Validation loss: 2.5180938168865135

Epoch: 156| Step: 0
Training loss: 2.939938446230967
Validation loss: 2.499539163630862

Epoch: 6| Step: 1
Training loss: 2.7549003808760535
Validation loss: 2.4946089556127062

Epoch: 6| Step: 2
Training loss: 2.203897110934614
Validation loss: 2.495417378723663

Epoch: 6| Step: 3
Training loss: 2.968191797079577
Validation loss: 2.498683966575203

Epoch: 6| Step: 4
Training loss: 3.5652392380147204
Validation loss: 2.4952295822304587

Epoch: 6| Step: 5
Training loss: 3.0788446350049923
Validation loss: 2.489330746611349

Epoch: 6| Step: 6
Training loss: 3.2283298187420537
Validation loss: 2.5046785978314725

Epoch: 6| Step: 7
Training loss: 1.088584363668862
Validation loss: 2.514850367510574

Epoch: 6| Step: 8
Training loss: 2.713344066451805
Validation loss: 2.5229528935760874

Epoch: 6| Step: 9
Training loss: 3.236064966952443
Validation loss: 2.5213528722079728

Epoch: 6| Step: 10
Training loss: 2.7246741283747973
Validation loss: 2.51042939420597

Epoch: 6| Step: 11
Training loss: 2.3667100481525756
Validation loss: 2.5096055418480683

Epoch: 6| Step: 12
Training loss: 2.6209012684308246
Validation loss: 2.5021345859760706

Epoch: 6| Step: 13
Training loss: 2.4736316558919524
Validation loss: 2.4903372434296216

Epoch: 157| Step: 0
Training loss: 3.2917977962852483
Validation loss: 2.4913094323506266

Epoch: 6| Step: 1
Training loss: 3.1909440825370816
Validation loss: 2.4902548676978067

Epoch: 6| Step: 2
Training loss: 2.087775973962737
Validation loss: 2.499611368018226

Epoch: 6| Step: 3
Training loss: 2.8620882762674285
Validation loss: 2.493829708227214

Epoch: 6| Step: 4
Training loss: 2.1971116482507553
Validation loss: 2.4966734843083858

Epoch: 6| Step: 5
Training loss: 2.6837173277985373
Validation loss: 2.4957713449895422

Epoch: 6| Step: 6
Training loss: 2.957761807820941
Validation loss: 2.5079153312838187

Epoch: 6| Step: 7
Training loss: 2.6160151802462477
Validation loss: 2.5027370462207266

Epoch: 6| Step: 8
Training loss: 2.906384536746549
Validation loss: 2.503100340136683

Epoch: 6| Step: 9
Training loss: 2.51103625954928
Validation loss: 2.50819650058154

Epoch: 6| Step: 10
Training loss: 2.8804998451501707
Validation loss: 2.5143053795723396

Epoch: 6| Step: 11
Training loss: 2.652486491119472
Validation loss: 2.520788156919363

Epoch: 6| Step: 12
Training loss: 2.9163199264050186
Validation loss: 2.5052221904708496

Epoch: 6| Step: 13
Training loss: 2.516158053390107
Validation loss: 2.505200236095944

Epoch: 158| Step: 0
Training loss: 2.484185649445682
Validation loss: 2.493799610967012

Epoch: 6| Step: 1
Training loss: 2.95793317497244
Validation loss: 2.493671276038226

Epoch: 6| Step: 2
Training loss: 2.1433039698905594
Validation loss: 2.4814630780362132

Epoch: 6| Step: 3
Training loss: 2.8649005136309587
Validation loss: 2.482083141187509

Epoch: 6| Step: 4
Training loss: 2.3845578664748093
Validation loss: 2.479892056539382

Epoch: 6| Step: 5
Training loss: 2.995988070577546
Validation loss: 2.479530566973175

Epoch: 6| Step: 6
Training loss: 2.613245577490318
Validation loss: 2.4820508321309243

Epoch: 6| Step: 7
Training loss: 3.0708543139240896
Validation loss: 2.475767339801187

Epoch: 6| Step: 8
Training loss: 2.932440438344567
Validation loss: 2.4861349861769346

Epoch: 6| Step: 9
Training loss: 2.77698155011761
Validation loss: 2.49222619418233

Epoch: 6| Step: 10
Training loss: 2.3901414569188226
Validation loss: 2.499706538214334

Epoch: 6| Step: 11
Training loss: 3.2623993489023917
Validation loss: 2.5448752088361104

Epoch: 6| Step: 12
Training loss: 2.7584241669633944
Validation loss: 2.5819134672246067

Epoch: 6| Step: 13
Training loss: 3.2298047491530144
Validation loss: 2.6049294937377128

Epoch: 159| Step: 0
Training loss: 2.351276063424779
Validation loss: 2.56989669640705

Epoch: 6| Step: 1
Training loss: 2.541162367856405
Validation loss: 2.5338468722369862

Epoch: 6| Step: 2
Training loss: 3.3220534842800156
Validation loss: 2.5015498843224053

Epoch: 6| Step: 3
Training loss: 2.4484253591541982
Validation loss: 2.483543325411255

Epoch: 6| Step: 4
Training loss: 2.878846290823205
Validation loss: 2.4764243063065132

Epoch: 6| Step: 5
Training loss: 2.775399559054938
Validation loss: 2.478689014488677

Epoch: 6| Step: 6
Training loss: 2.522021672971747
Validation loss: 2.4785154988098874

Epoch: 6| Step: 7
Training loss: 3.052396651884314
Validation loss: 2.4849042988392225

Epoch: 6| Step: 8
Training loss: 2.5096107761690396
Validation loss: 2.4857449290426907

Epoch: 6| Step: 9
Training loss: 2.838386928062056
Validation loss: 2.489331420134042

Epoch: 6| Step: 10
Training loss: 2.592897573829159
Validation loss: 2.4939898069019657

Epoch: 6| Step: 11
Training loss: 3.0229908221399198
Validation loss: 2.5099434334862476

Epoch: 6| Step: 12
Training loss: 3.275888331250881
Validation loss: 2.50280982824686

Epoch: 6| Step: 13
Training loss: 2.594472853345987
Validation loss: 2.502247081463762

Epoch: 160| Step: 0
Training loss: 2.305571822858461
Validation loss: 2.506347618915641

Epoch: 6| Step: 1
Training loss: 2.8704610737332548
Validation loss: 2.499452530997921

Epoch: 6| Step: 2
Training loss: 3.340806467606872
Validation loss: 2.5071722064339057

Epoch: 6| Step: 3
Training loss: 2.430095086454731
Validation loss: 2.497652589419359

Epoch: 6| Step: 4
Training loss: 2.510214537612534
Validation loss: 2.50405417139083

Epoch: 6| Step: 5
Training loss: 1.7496947976048922
Validation loss: 2.514429328883331

Epoch: 6| Step: 6
Training loss: 3.3687512512770894
Validation loss: 2.5174873641370583

Epoch: 6| Step: 7
Training loss: 3.0714463331653135
Validation loss: 2.5076648606213494

Epoch: 6| Step: 8
Training loss: 2.8420802653479327
Validation loss: 2.5005679172549873

Epoch: 6| Step: 9
Training loss: 2.5679152420174516
Validation loss: 2.498795475526601

Epoch: 6| Step: 10
Training loss: 2.651561144062476
Validation loss: 2.5022931062428424

Epoch: 6| Step: 11
Training loss: 3.0920497961932423
Validation loss: 2.4917132592180877

Epoch: 6| Step: 12
Training loss: 2.7066799007367845
Validation loss: 2.4785235046288876

Epoch: 6| Step: 13
Training loss: 2.560346768705225
Validation loss: 2.473838422078844

Epoch: 161| Step: 0
Training loss: 2.831877353038328
Validation loss: 2.466574577895165

Epoch: 6| Step: 1
Training loss: 3.00588903976387
Validation loss: 2.465625880213021

Epoch: 6| Step: 2
Training loss: 2.7711402512749745
Validation loss: 2.4614406005678338

Epoch: 6| Step: 3
Training loss: 2.51112864252833
Validation loss: 2.4635778588928736

Epoch: 6| Step: 4
Training loss: 3.451026520531269
Validation loss: 2.462208134563731

Epoch: 6| Step: 5
Training loss: 2.236474817787069
Validation loss: 2.466969870207949

Epoch: 6| Step: 6
Training loss: 1.9957922541691953
Validation loss: 2.4709323465278685

Epoch: 6| Step: 7
Training loss: 2.5818773186257005
Validation loss: 2.4750882539979266

Epoch: 6| Step: 8
Training loss: 3.179281222844734
Validation loss: 2.481531970458675

Epoch: 6| Step: 9
Training loss: 2.780407895962772
Validation loss: 2.484786227840623

Epoch: 6| Step: 10
Training loss: 2.584747962839431
Validation loss: 2.489202276549061

Epoch: 6| Step: 11
Training loss: 2.6180340209219186
Validation loss: 2.495338049908632

Epoch: 6| Step: 12
Training loss: 2.894025627175667
Validation loss: 2.500927086665361

Epoch: 6| Step: 13
Training loss: 2.680969376539597
Validation loss: 2.4964709517745987

Epoch: 162| Step: 0
Training loss: 2.8815475210957406
Validation loss: 2.512023060699358

Epoch: 6| Step: 1
Training loss: 2.617935756929663
Validation loss: 2.524495864354912

Epoch: 6| Step: 2
Training loss: 3.692800042753699
Validation loss: 2.5207247868755767

Epoch: 6| Step: 3
Training loss: 2.5397268511865003
Validation loss: 2.5188047378631127

Epoch: 6| Step: 4
Training loss: 2.5244612846163546
Validation loss: 2.491977446846886

Epoch: 6| Step: 5
Training loss: 2.746816526448074
Validation loss: 2.482700858296603

Epoch: 6| Step: 6
Training loss: 2.735537908404018
Validation loss: 2.479077956909127

Epoch: 6| Step: 7
Training loss: 2.9535150825369825
Validation loss: 2.472310847284335

Epoch: 6| Step: 8
Training loss: 2.9668843078788716
Validation loss: 2.4731956455676722

Epoch: 6| Step: 9
Training loss: 2.6584515592141207
Validation loss: 2.470298234885506

Epoch: 6| Step: 10
Training loss: 2.78976816178054
Validation loss: 2.4746885493807715

Epoch: 6| Step: 11
Training loss: 2.2694064449063807
Validation loss: 2.4765923062432313

Epoch: 6| Step: 12
Training loss: 2.221601990399621
Validation loss: 2.478192042093693

Epoch: 6| Step: 13
Training loss: 2.510546373575296
Validation loss: 2.4791969682829333

Epoch: 163| Step: 0
Training loss: 3.1525651931979586
Validation loss: 2.4729464871763245

Epoch: 6| Step: 1
Training loss: 2.9230905544102166
Validation loss: 2.4848522941057523

Epoch: 6| Step: 2
Training loss: 3.1441384775081187
Validation loss: 2.4791145108959247

Epoch: 6| Step: 3
Training loss: 2.8223925076906404
Validation loss: 2.4837221162422796

Epoch: 6| Step: 4
Training loss: 2.302677221778014
Validation loss: 2.484041288188631

Epoch: 6| Step: 5
Training loss: 2.4333421506678414
Validation loss: 2.4856034554937008

Epoch: 6| Step: 6
Training loss: 3.63741845632699
Validation loss: 2.4842646434933853

Epoch: 6| Step: 7
Training loss: 2.2894471141591137
Validation loss: 2.4932508036166285

Epoch: 6| Step: 8
Training loss: 2.3398162381651573
Validation loss: 2.508087416078315

Epoch: 6| Step: 9
Training loss: 2.2175812127725334
Validation loss: 2.508472652066436

Epoch: 6| Step: 10
Training loss: 2.3651628050304097
Validation loss: 2.5193454534972863

Epoch: 6| Step: 11
Training loss: 3.2193607380513516
Validation loss: 2.5248300152993557

Epoch: 6| Step: 12
Training loss: 2.769156835559434
Validation loss: 2.5008544753693567

Epoch: 6| Step: 13
Training loss: 1.974072361641123
Validation loss: 2.4985937234364055

Epoch: 164| Step: 0
Training loss: 3.010682636554261
Validation loss: 2.490464914819119

Epoch: 6| Step: 1
Training loss: 2.614527383841472
Validation loss: 2.4938555652127175

Epoch: 6| Step: 2
Training loss: 2.8594853947908985
Validation loss: 2.506539438583336

Epoch: 6| Step: 3
Training loss: 2.8066998331491844
Validation loss: 2.5067653228607223

Epoch: 6| Step: 4
Training loss: 2.84355414418267
Validation loss: 2.503670194905016

Epoch: 6| Step: 5
Training loss: 2.662054921948588
Validation loss: 2.505542046164506

Epoch: 6| Step: 6
Training loss: 2.187616617636481
Validation loss: 2.4975695013890813

Epoch: 6| Step: 7
Training loss: 2.5013167728202568
Validation loss: 2.4845814726479554

Epoch: 6| Step: 8
Training loss: 3.0788954337026135
Validation loss: 2.472001901462911

Epoch: 6| Step: 9
Training loss: 2.447639308935348
Validation loss: 2.475276307789188

Epoch: 6| Step: 10
Training loss: 3.355666841959081
Validation loss: 2.4799220998118923

Epoch: 6| Step: 11
Training loss: 2.3599783018698774
Validation loss: 2.478159569547008

Epoch: 6| Step: 12
Training loss: 2.3591751115010675
Validation loss: 2.4723120967979986

Epoch: 6| Step: 13
Training loss: 3.0821686340577688
Validation loss: 2.471509978201112

Epoch: 165| Step: 0
Training loss: 2.590434701738248
Validation loss: 2.469437848767143

Epoch: 6| Step: 1
Training loss: 3.158937122401478
Validation loss: 2.4622155030950004

Epoch: 6| Step: 2
Training loss: 3.090390163097284
Validation loss: 2.474538036533197

Epoch: 6| Step: 3
Training loss: 3.279794715411948
Validation loss: 2.4938419639351537

Epoch: 6| Step: 4
Training loss: 3.025274620516461
Validation loss: 2.5022773281263686

Epoch: 6| Step: 5
Training loss: 2.711460030186743
Validation loss: 2.5167965304645006

Epoch: 6| Step: 6
Training loss: 2.4323342188204067
Validation loss: 2.5328085307056094

Epoch: 6| Step: 7
Training loss: 2.8651068931464447
Validation loss: 2.535969870961729

Epoch: 6| Step: 8
Training loss: 2.5737898936799564
Validation loss: 2.5414073179016596

Epoch: 6| Step: 9
Training loss: 2.2226192424354836
Validation loss: 2.5232014168625754

Epoch: 6| Step: 10
Training loss: 2.838027562239444
Validation loss: 2.5017399116962746

Epoch: 6| Step: 11
Training loss: 2.062339198462013
Validation loss: 2.493858708782149

Epoch: 6| Step: 12
Training loss: 2.2969474521550173
Validation loss: 2.4861783683684764

Epoch: 6| Step: 13
Training loss: 3.086465447570313
Validation loss: 2.485237617281324

Epoch: 166| Step: 0
Training loss: 3.2972984584388834
Validation loss: 2.472488388032099

Epoch: 6| Step: 1
Training loss: 2.0071470590265275
Validation loss: 2.473595747917708

Epoch: 6| Step: 2
Training loss: 2.5951533370386652
Validation loss: 2.470437060791162

Epoch: 6| Step: 3
Training loss: 2.652394627123876
Validation loss: 2.465368082981784

Epoch: 6| Step: 4
Training loss: 2.8425406095611714
Validation loss: 2.4685550839649335

Epoch: 6| Step: 5
Training loss: 2.483428486872122
Validation loss: 2.4698143385033835

Epoch: 6| Step: 6
Training loss: 2.8376609252467717
Validation loss: 2.4713562811154968

Epoch: 6| Step: 7
Training loss: 3.026954990159446
Validation loss: 2.4669997653306392

Epoch: 6| Step: 8
Training loss: 2.1761838037194376
Validation loss: 2.467742465101809

Epoch: 6| Step: 9
Training loss: 2.999092600604715
Validation loss: 2.466363683202772

Epoch: 6| Step: 10
Training loss: 2.789065235467846
Validation loss: 2.4711360322724225

Epoch: 6| Step: 11
Training loss: 2.6096192205662643
Validation loss: 2.482739488650857

Epoch: 6| Step: 12
Training loss: 2.9275390885607777
Validation loss: 2.5058721795669823

Epoch: 6| Step: 13
Training loss: 2.7421310723305288
Validation loss: 2.5243813639672967

Epoch: 167| Step: 0
Training loss: 2.5833050100763266
Validation loss: 2.5222747296858286

Epoch: 6| Step: 1
Training loss: 2.9697581938363937
Validation loss: 2.5083540786977103

Epoch: 6| Step: 2
Training loss: 2.4815567634111733
Validation loss: 2.5028230980830313

Epoch: 6| Step: 3
Training loss: 2.8706140649455616
Validation loss: 2.4915634589352

Epoch: 6| Step: 4
Training loss: 3.0058649272508013
Validation loss: 2.4888465870046597

Epoch: 6| Step: 5
Training loss: 2.8116000961023655
Validation loss: 2.477840707191089

Epoch: 6| Step: 6
Training loss: 2.998417118832178
Validation loss: 2.4677636930898075

Epoch: 6| Step: 7
Training loss: 2.5834143738699606
Validation loss: 2.468437097111953

Epoch: 6| Step: 8
Training loss: 2.3299414186112397
Validation loss: 2.4668892384458623

Epoch: 6| Step: 9
Training loss: 2.658329407819602
Validation loss: 2.4709279111277778

Epoch: 6| Step: 10
Training loss: 2.4920107022418443
Validation loss: 2.4778360125671086

Epoch: 6| Step: 11
Training loss: 2.6747378354529103
Validation loss: 2.49074242289929

Epoch: 6| Step: 12
Training loss: 2.8027923896979123
Validation loss: 2.4892900970167315

Epoch: 6| Step: 13
Training loss: 2.7409426045133767
Validation loss: 2.4964544380469373

Epoch: 168| Step: 0
Training loss: 2.8481598920193743
Validation loss: 2.490656898551492

Epoch: 6| Step: 1
Training loss: 2.221611005119674
Validation loss: 2.4802995211430043

Epoch: 6| Step: 2
Training loss: 2.071723773667393
Validation loss: 2.4869440269639913

Epoch: 6| Step: 3
Training loss: 2.3765530778546378
Validation loss: 2.484451076080329

Epoch: 6| Step: 4
Training loss: 1.7965055666422418
Validation loss: 2.4871764859148424

Epoch: 6| Step: 5
Training loss: 2.764253696623755
Validation loss: 2.482802804949796

Epoch: 6| Step: 6
Training loss: 2.8730038886676286
Validation loss: 2.4824355171613433

Epoch: 6| Step: 7
Training loss: 3.1093906421363773
Validation loss: 2.4773999246884624

Epoch: 6| Step: 8
Training loss: 3.072195160209877
Validation loss: 2.4709439418192396

Epoch: 6| Step: 9
Training loss: 2.180576594332894
Validation loss: 2.476492760229075

Epoch: 6| Step: 10
Training loss: 3.0474733987700264
Validation loss: 2.467597560558665

Epoch: 6| Step: 11
Training loss: 2.9126228273875663
Validation loss: 2.466752158401512

Epoch: 6| Step: 12
Training loss: 3.2522830280295687
Validation loss: 2.4762050983717185

Epoch: 6| Step: 13
Training loss: 3.251478299137293
Validation loss: 2.4767638606263245

Epoch: 169| Step: 0
Training loss: 2.5176368392833393
Validation loss: 2.4848080004465323

Epoch: 6| Step: 1
Training loss: 2.4608074819944656
Validation loss: 2.4854067842812606

Epoch: 6| Step: 2
Training loss: 3.0041437141998393
Validation loss: 2.4829794168701236

Epoch: 6| Step: 3
Training loss: 2.3564529007790687
Validation loss: 2.4762770928031617

Epoch: 6| Step: 4
Training loss: 2.5217424965311257
Validation loss: 2.48500885800161

Epoch: 6| Step: 5
Training loss: 2.514057310678687
Validation loss: 2.48217477263529

Epoch: 6| Step: 6
Training loss: 3.049317149031635
Validation loss: 2.494643445085861

Epoch: 6| Step: 7
Training loss: 2.9327916497702176
Validation loss: 2.494124189379484

Epoch: 6| Step: 8
Training loss: 2.3350716201286783
Validation loss: 2.491908464611503

Epoch: 6| Step: 9
Training loss: 2.7751743090071903
Validation loss: 2.490485688183068

Epoch: 6| Step: 10
Training loss: 2.1082077752924904
Validation loss: 2.4852459707388896

Epoch: 6| Step: 11
Training loss: 3.121425109766587
Validation loss: 2.475905934636816

Epoch: 6| Step: 12
Training loss: 3.25920035167724
Validation loss: 2.475513983032712

Epoch: 6| Step: 13
Training loss: 2.6547790044559263
Validation loss: 2.4815595423882857

Epoch: 170| Step: 0
Training loss: 3.020930865236989
Validation loss: 2.48273477902422

Epoch: 6| Step: 1
Training loss: 2.96780942773916
Validation loss: 2.4790154818850296

Epoch: 6| Step: 2
Training loss: 2.371962562522911
Validation loss: 2.483021055635066

Epoch: 6| Step: 3
Training loss: 2.7424891813998493
Validation loss: 2.4816755211010344

Epoch: 6| Step: 4
Training loss: 2.7439015131589573
Validation loss: 2.496739903441604

Epoch: 6| Step: 5
Training loss: 2.5614813664101304
Validation loss: 2.498492873603396

Epoch: 6| Step: 6
Training loss: 1.8809767515133957
Validation loss: 2.499248076745886

Epoch: 6| Step: 7
Training loss: 2.9602346265438917
Validation loss: 2.495777187660545

Epoch: 6| Step: 8
Training loss: 2.5922575184090118
Validation loss: 2.4949454318978463

Epoch: 6| Step: 9
Training loss: 2.5020442234342117
Validation loss: 2.4922410767166907

Epoch: 6| Step: 10
Training loss: 2.595776788658001
Validation loss: 2.502373572525901

Epoch: 6| Step: 11
Training loss: 3.0022814340946247
Validation loss: 2.4888150425017725

Epoch: 6| Step: 12
Training loss: 2.618041670605317
Validation loss: 2.4877464701236716

Epoch: 6| Step: 13
Training loss: 3.243658481255013
Validation loss: 2.4977622928353362

Epoch: 171| Step: 0
Training loss: 2.973246812744793
Validation loss: 2.507455257987092

Epoch: 6| Step: 1
Training loss: 2.9514685127637628
Validation loss: 2.508343461713646

Epoch: 6| Step: 2
Training loss: 2.194134053143532
Validation loss: 2.5254872711575045

Epoch: 6| Step: 3
Training loss: 2.6786253133530895
Validation loss: 2.521244229828231

Epoch: 6| Step: 4
Training loss: 2.921090576627221
Validation loss: 2.5387103698745577

Epoch: 6| Step: 5
Training loss: 2.4444248916344895
Validation loss: 2.538882261683328

Epoch: 6| Step: 6
Training loss: 2.909787557915895
Validation loss: 2.529063486796019

Epoch: 6| Step: 7
Training loss: 3.0211213774758217
Validation loss: 2.517737474172376

Epoch: 6| Step: 8
Training loss: 2.2245221342776325
Validation loss: 2.502891156742937

Epoch: 6| Step: 9
Training loss: 2.646218329426158
Validation loss: 2.497154614184539

Epoch: 6| Step: 10
Training loss: 2.794053172616565
Validation loss: 2.4971124554848827

Epoch: 6| Step: 11
Training loss: 2.749839604641937
Validation loss: 2.4973668988321687

Epoch: 6| Step: 12
Training loss: 2.4622526015501194
Validation loss: 2.4909989947912448

Epoch: 6| Step: 13
Training loss: 3.0567797741889464
Validation loss: 2.4956722116370345

Epoch: 172| Step: 0
Training loss: 2.405941039227837
Validation loss: 2.4999487922920443

Epoch: 6| Step: 1
Training loss: 2.7394635617916654
Validation loss: 2.5110148348374546

Epoch: 6| Step: 2
Training loss: 3.1727161513598423
Validation loss: 2.509839877340599

Epoch: 6| Step: 3
Training loss: 2.6141554848412865
Validation loss: 2.519861832610671

Epoch: 6| Step: 4
Training loss: 2.3698685071176064
Validation loss: 2.5360454425328474

Epoch: 6| Step: 5
Training loss: 2.267090211827587
Validation loss: 2.5542631498995143

Epoch: 6| Step: 6
Training loss: 2.7095577895510905
Validation loss: 2.5539445818229796

Epoch: 6| Step: 7
Training loss: 3.231749422595189
Validation loss: 2.5491022250751505

Epoch: 6| Step: 8
Training loss: 2.922252773404019
Validation loss: 2.5230423060898906

Epoch: 6| Step: 9
Training loss: 2.795264350386865
Validation loss: 2.505845326321834

Epoch: 6| Step: 10
Training loss: 1.9340897357481925
Validation loss: 2.4741216382637705

Epoch: 6| Step: 11
Training loss: 2.487228865679098
Validation loss: 2.460795451392867

Epoch: 6| Step: 12
Training loss: 3.147169288428777
Validation loss: 2.453054687963782

Epoch: 6| Step: 13
Training loss: 3.1574858710304365
Validation loss: 2.4563129424394616

Epoch: 173| Step: 0
Training loss: 2.818404802047489
Validation loss: 2.459990729172688

Epoch: 6| Step: 1
Training loss: 2.742999095308862
Validation loss: 2.4580695287263468

Epoch: 6| Step: 2
Training loss: 2.3383319536976765
Validation loss: 2.4597292198161185

Epoch: 6| Step: 3
Training loss: 2.6171650899810146
Validation loss: 2.4626018520804505

Epoch: 6| Step: 4
Training loss: 2.7146895330524
Validation loss: 2.451501602163862

Epoch: 6| Step: 5
Training loss: 2.4699889345732022
Validation loss: 2.455253066270436

Epoch: 6| Step: 6
Training loss: 2.704666139373966
Validation loss: 2.4603475837323776

Epoch: 6| Step: 7
Training loss: 3.3648041612339092
Validation loss: 2.46010450363453

Epoch: 6| Step: 8
Training loss: 2.5893868205808688
Validation loss: 2.4716148187575926

Epoch: 6| Step: 9
Training loss: 3.037416458550882
Validation loss: 2.501763333114228

Epoch: 6| Step: 10
Training loss: 2.605783880984648
Validation loss: 2.510655470709405

Epoch: 6| Step: 11
Training loss: 3.0274576276464753
Validation loss: 2.523737989683122

Epoch: 6| Step: 12
Training loss: 2.257397465030142
Validation loss: 2.523640699531801

Epoch: 6| Step: 13
Training loss: 2.877307629157866
Validation loss: 2.526572866673045

Epoch: 174| Step: 0
Training loss: 2.814280476195265
Validation loss: 2.53210994991262

Epoch: 6| Step: 1
Training loss: 3.172136568455959
Validation loss: 2.5448570307107827

Epoch: 6| Step: 2
Training loss: 2.507066467170942
Validation loss: 2.5186223638088516

Epoch: 6| Step: 3
Training loss: 2.5965721679096156
Validation loss: 2.5090920595086743

Epoch: 6| Step: 4
Training loss: 2.2452654298057415
Validation loss: 2.4924963680859684

Epoch: 6| Step: 5
Training loss: 2.469885649408649
Validation loss: 2.492683420194377

Epoch: 6| Step: 6
Training loss: 2.313013896031246
Validation loss: 2.494464519276922

Epoch: 6| Step: 7
Training loss: 2.6062709597103844
Validation loss: 2.486914691154778

Epoch: 6| Step: 8
Training loss: 3.037001982627023
Validation loss: 2.48403134029377

Epoch: 6| Step: 9
Training loss: 2.5311808635547717
Validation loss: 2.469113519545054

Epoch: 6| Step: 10
Training loss: 2.6118854621263248
Validation loss: 2.466991629628353

Epoch: 6| Step: 11
Training loss: 2.8700545525765926
Validation loss: 2.4727429308648334

Epoch: 6| Step: 12
Training loss: 2.740640230908584
Validation loss: 2.4734729975142145

Epoch: 6| Step: 13
Training loss: 3.441651351808429
Validation loss: 2.4768743971088623

Epoch: 175| Step: 0
Training loss: 3.108256076401495
Validation loss: 2.4874778714419317

Epoch: 6| Step: 1
Training loss: 2.123508827746217
Validation loss: 2.488583452570509

Epoch: 6| Step: 2
Training loss: 2.678966992552592
Validation loss: 2.5064340265228036

Epoch: 6| Step: 3
Training loss: 2.256133303173696
Validation loss: 2.495831466940623

Epoch: 6| Step: 4
Training loss: 2.8299881504201663
Validation loss: 2.495513072528211

Epoch: 6| Step: 5
Training loss: 2.915370616875363
Validation loss: 2.4933366512973816

Epoch: 6| Step: 6
Training loss: 2.8153289978038525
Validation loss: 2.4927104296272917

Epoch: 6| Step: 7
Training loss: 2.280166786852817
Validation loss: 2.4916458510798587

Epoch: 6| Step: 8
Training loss: 3.131825888664889
Validation loss: 2.4806334087150446

Epoch: 6| Step: 9
Training loss: 2.379568825140198
Validation loss: 2.484872966342032

Epoch: 6| Step: 10
Training loss: 2.518547303631576
Validation loss: 2.502613293274712

Epoch: 6| Step: 11
Training loss: 2.6130294336353965
Validation loss: 2.503748869098532

Epoch: 6| Step: 12
Training loss: 3.061568137881407
Validation loss: 2.5004826192672804

Epoch: 6| Step: 13
Training loss: 2.5905145895406663
Validation loss: 2.505017650395431

Epoch: 176| Step: 0
Training loss: 3.0424094316579597
Validation loss: 2.49986046945193

Epoch: 6| Step: 1
Training loss: 2.859730682764007
Validation loss: 2.5161035401409957

Epoch: 6| Step: 2
Training loss: 2.7913291119760038
Validation loss: 2.5007243276152247

Epoch: 6| Step: 3
Training loss: 3.077934917641822
Validation loss: 2.4938296270157294

Epoch: 6| Step: 4
Training loss: 2.1935000285350137
Validation loss: 2.4797155938116

Epoch: 6| Step: 5
Training loss: 2.977467316801249
Validation loss: 2.484725332037981

Epoch: 6| Step: 6
Training loss: 2.5808222677231543
Validation loss: 2.4746736462345784

Epoch: 6| Step: 7
Training loss: 2.7092527931750223
Validation loss: 2.490560851406658

Epoch: 6| Step: 8
Training loss: 2.7628963824740307
Validation loss: 2.487165360046184

Epoch: 6| Step: 9
Training loss: 2.531302180753148
Validation loss: 2.49056057966017

Epoch: 6| Step: 10
Training loss: 2.338063373095111
Validation loss: 2.4937405939937647

Epoch: 6| Step: 11
Training loss: 2.5492445331547415
Validation loss: 2.4982063042269194

Epoch: 6| Step: 12
Training loss: 2.220260750518499
Validation loss: 2.4930554709292294

Epoch: 6| Step: 13
Training loss: 2.573398210402774
Validation loss: 2.4888444269853265

Epoch: 177| Step: 0
Training loss: 2.019774195448131
Validation loss: 2.4898618430007526

Epoch: 6| Step: 1
Training loss: 2.5714500252267727
Validation loss: 2.503666748276807

Epoch: 6| Step: 2
Training loss: 2.674352556826711
Validation loss: 2.501534511961483

Epoch: 6| Step: 3
Training loss: 2.626188372506703
Validation loss: 2.5026799583672568

Epoch: 6| Step: 4
Training loss: 3.1656154259615064
Validation loss: 2.492744220317463

Epoch: 6| Step: 5
Training loss: 2.9420544643378665
Validation loss: 2.487613936154557

Epoch: 6| Step: 6
Training loss: 2.7696565887540836
Validation loss: 2.486045374409578

Epoch: 6| Step: 7
Training loss: 1.8971075729250335
Validation loss: 2.478126275175861

Epoch: 6| Step: 8
Training loss: 2.4529343063198015
Validation loss: 2.485843760099506

Epoch: 6| Step: 9
Training loss: 2.781268859113553
Validation loss: 2.4897932994762626

Epoch: 6| Step: 10
Training loss: 2.5234127456820854
Validation loss: 2.483885722249028

Epoch: 6| Step: 11
Training loss: 3.0595774816385846
Validation loss: 2.482791405471137

Epoch: 6| Step: 12
Training loss: 3.090014736300887
Validation loss: 2.487098545034836

Epoch: 6| Step: 13
Training loss: 2.477872580219514
Validation loss: 2.5051889978866377

Epoch: 178| Step: 0
Training loss: 2.982408125070629
Validation loss: 2.506849814956682

Epoch: 6| Step: 1
Training loss: 2.9589460213067147
Validation loss: 2.5261069638323277

Epoch: 6| Step: 2
Training loss: 2.4740898231230126
Validation loss: 2.5474525882323458

Epoch: 6| Step: 3
Training loss: 2.307442779988783
Validation loss: 2.5352986097945065

Epoch: 6| Step: 4
Training loss: 2.797045398827886
Validation loss: 2.5353942776541816

Epoch: 6| Step: 5
Training loss: 3.035672923215003
Validation loss: 2.533686986108756

Epoch: 6| Step: 6
Training loss: 2.3482316973287793
Validation loss: 2.5262987262090983

Epoch: 6| Step: 7
Training loss: 2.6675358388936834
Validation loss: 2.5211549986070856

Epoch: 6| Step: 8
Training loss: 2.211829985984876
Validation loss: 2.509267680313751

Epoch: 6| Step: 9
Training loss: 2.6622838322100577
Validation loss: 2.492772933199477

Epoch: 6| Step: 10
Training loss: 3.021466226159537
Validation loss: 2.482313286505552

Epoch: 6| Step: 11
Training loss: 3.1956733182779056
Validation loss: 2.4832322793492985

Epoch: 6| Step: 12
Training loss: 2.4231487247229513
Validation loss: 2.4729810009405813

Epoch: 6| Step: 13
Training loss: 1.8060428434593927
Validation loss: 2.479817501917515

Epoch: 179| Step: 0
Training loss: 3.1463808673714366
Validation loss: 2.484535102228416

Epoch: 6| Step: 1
Training loss: 3.328945864603089
Validation loss: 2.495854082017361

Epoch: 6| Step: 2
Training loss: 1.835030709199395
Validation loss: 2.5135396707548785

Epoch: 6| Step: 3
Training loss: 2.4705794111195756
Validation loss: 2.533620563160663

Epoch: 6| Step: 4
Training loss: 2.8018505656810513
Validation loss: 2.5426023836307197

Epoch: 6| Step: 5
Training loss: 2.2765575903719792
Validation loss: 2.5585501107185427

Epoch: 6| Step: 6
Training loss: 2.9954464050194987
Validation loss: 2.5667926470417863

Epoch: 6| Step: 7
Training loss: 2.934074839879792
Validation loss: 2.5788189970371196

Epoch: 6| Step: 8
Training loss: 2.64758881330788
Validation loss: 2.605444068580855

Epoch: 6| Step: 9
Training loss: 2.7746811348487324
Validation loss: 2.551103643876794

Epoch: 6| Step: 10
Training loss: 2.1768256100295935
Validation loss: 2.486055427678226

Epoch: 6| Step: 11
Training loss: 2.3631757223985868
Validation loss: 2.4622807995837914

Epoch: 6| Step: 12
Training loss: 2.978437498975383
Validation loss: 2.452856972633651

Epoch: 6| Step: 13
Training loss: 2.5706253557044967
Validation loss: 2.457311498874078

Epoch: 180| Step: 0
Training loss: 2.9753497198657795
Validation loss: 2.4699462364223916

Epoch: 6| Step: 1
Training loss: 2.5870318445369853
Validation loss: 2.4831436953910124

Epoch: 6| Step: 2
Training loss: 2.4408355778347057
Validation loss: 2.485799366537062

Epoch: 6| Step: 3
Training loss: 2.0394588392199924
Validation loss: 2.486187386337099

Epoch: 6| Step: 4
Training loss: 3.167768855438221
Validation loss: 2.49499385601095

Epoch: 6| Step: 5
Training loss: 2.862526079750362
Validation loss: 2.517468914420062

Epoch: 6| Step: 6
Training loss: 3.2499652273812116
Validation loss: 2.5065091436303235

Epoch: 6| Step: 7
Training loss: 2.840679816964553
Validation loss: 2.4982253995444474

Epoch: 6| Step: 8
Training loss: 2.438271816031293
Validation loss: 2.486348263235969

Epoch: 6| Step: 9
Training loss: 2.8384264068067555
Validation loss: 2.472243742806703

Epoch: 6| Step: 10
Training loss: 3.0674062859755287
Validation loss: 2.463108329450609

Epoch: 6| Step: 11
Training loss: 2.9210610301411166
Validation loss: 2.457396306957396

Epoch: 6| Step: 12
Training loss: 3.00870538314747
Validation loss: 2.4594000078087355

Epoch: 6| Step: 13
Training loss: 2.5198357449845137
Validation loss: 2.46252366121188

Epoch: 181| Step: 0
Training loss: 3.3170179829562834
Validation loss: 2.475565301625846

Epoch: 6| Step: 1
Training loss: 2.814913582974486
Validation loss: 2.4882806132833446

Epoch: 6| Step: 2
Training loss: 2.4770429367822517
Validation loss: 2.536259433009086

Epoch: 6| Step: 3
Training loss: 2.6090552768020165
Validation loss: 2.579552064011167

Epoch: 6| Step: 4
Training loss: 2.8878386393877884
Validation loss: 2.626097991041638

Epoch: 6| Step: 5
Training loss: 2.576512427736999
Validation loss: 2.6292543094145646

Epoch: 6| Step: 6
Training loss: 3.1468586713865405
Validation loss: 2.5941829841374937

Epoch: 6| Step: 7
Training loss: 1.942081817427756
Validation loss: 2.570258874000577

Epoch: 6| Step: 8
Training loss: 2.4809541002153765
Validation loss: 2.5567352215502286

Epoch: 6| Step: 9
Training loss: 2.959107006792577
Validation loss: 2.5455004301131936

Epoch: 6| Step: 10
Training loss: 3.2489880673702176
Validation loss: 2.522735007225512

Epoch: 6| Step: 11
Training loss: 2.183719310664948
Validation loss: 2.516077868997386

Epoch: 6| Step: 12
Training loss: 2.2212138934788555
Validation loss: 2.493607286782994

Epoch: 6| Step: 13
Training loss: 2.609348593938012
Validation loss: 2.4733107177193077

Epoch: 182| Step: 0
Training loss: 2.8534265565646675
Validation loss: 2.462210222160317

Epoch: 6| Step: 1
Training loss: 2.846347670389192
Validation loss: 2.464722245617803

Epoch: 6| Step: 2
Training loss: 3.1020685739125526
Validation loss: 2.452508341381906

Epoch: 6| Step: 3
Training loss: 1.6436874145660965
Validation loss: 2.4489839421684034

Epoch: 6| Step: 4
Training loss: 2.252390756953437
Validation loss: 2.448217072879932

Epoch: 6| Step: 5
Training loss: 2.5021261710746248
Validation loss: 2.4494494964228197

Epoch: 6| Step: 6
Training loss: 2.5222083721070034
Validation loss: 2.4624372879603884

Epoch: 6| Step: 7
Training loss: 2.7825495223499317
Validation loss: 2.473399679754448

Epoch: 6| Step: 8
Training loss: 2.7737769550459523
Validation loss: 2.4963888113047106

Epoch: 6| Step: 9
Training loss: 3.06336900487587
Validation loss: 2.5133235602200936

Epoch: 6| Step: 10
Training loss: 2.6160521820416807
Validation loss: 2.5367281028411193

Epoch: 6| Step: 11
Training loss: 2.613179249048693
Validation loss: 2.541419378469241

Epoch: 6| Step: 12
Training loss: 2.6981770648753187
Validation loss: 2.5346246806696597

Epoch: 6| Step: 13
Training loss: 3.0089576820042327
Validation loss: 2.539486193017817

Epoch: 183| Step: 0
Training loss: 2.7012914712485916
Validation loss: 2.5448605595639995

Epoch: 6| Step: 1
Training loss: 2.8784136201859645
Validation loss: 2.5420990456274413

Epoch: 6| Step: 2
Training loss: 2.426980820047597
Validation loss: 2.531700500319665

Epoch: 6| Step: 3
Training loss: 2.730807260205106
Validation loss: 2.5154008612004146

Epoch: 6| Step: 4
Training loss: 2.9097413452135092
Validation loss: 2.505360919988183

Epoch: 6| Step: 5
Training loss: 1.875208207332337
Validation loss: 2.4883407421073334

Epoch: 6| Step: 6
Training loss: 2.393845925264157
Validation loss: 2.4854685442476834

Epoch: 6| Step: 7
Training loss: 2.6737724838987886
Validation loss: 2.4755889706804313

Epoch: 6| Step: 8
Training loss: 2.6404542585621207
Validation loss: 2.485828009066355

Epoch: 6| Step: 9
Training loss: 2.696393401639273
Validation loss: 2.4906043232575854

Epoch: 6| Step: 10
Training loss: 2.700705443166113
Validation loss: 2.4817314981983047

Epoch: 6| Step: 11
Training loss: 3.0817080329469224
Validation loss: 2.4807654403204675

Epoch: 6| Step: 12
Training loss: 2.574808472852884
Validation loss: 2.490927228257261

Epoch: 6| Step: 13
Training loss: 2.795322861287087
Validation loss: 2.4944564258754727

Epoch: 184| Step: 0
Training loss: 2.6399966062177174
Validation loss: 2.505869333431271

Epoch: 6| Step: 1
Training loss: 2.6657539434875153
Validation loss: 2.537945492681049

Epoch: 6| Step: 2
Training loss: 2.5955328271943428
Validation loss: 2.5800592479658153

Epoch: 6| Step: 3
Training loss: 2.8355342693918386
Validation loss: 2.5941020826934595

Epoch: 6| Step: 4
Training loss: 2.9381070422881734
Validation loss: 2.611039590921533

Epoch: 6| Step: 5
Training loss: 2.6843887772833837
Validation loss: 2.624457934245092

Epoch: 6| Step: 6
Training loss: 2.276278055050619
Validation loss: 2.6089960070742655

Epoch: 6| Step: 7
Training loss: 2.421870668468909
Validation loss: 2.5789137154946298

Epoch: 6| Step: 8
Training loss: 2.485104052017345
Validation loss: 2.5576813885408205

Epoch: 6| Step: 9
Training loss: 2.7799008987107494
Validation loss: 2.558144265723132

Epoch: 6| Step: 10
Training loss: 3.164606193642401
Validation loss: 2.544955388484073

Epoch: 6| Step: 11
Training loss: 2.4540380248813336
Validation loss: 2.52338936773568

Epoch: 6| Step: 12
Training loss: 2.75285052902555
Validation loss: 2.4989895352615052

Epoch: 6| Step: 13
Training loss: 2.64806271041995
Validation loss: 2.485622450635131

Epoch: 185| Step: 0
Training loss: 2.770358673588741
Validation loss: 2.4843109167998256

Epoch: 6| Step: 1
Training loss: 2.809327094801518
Validation loss: 2.4821752570279902

Epoch: 6| Step: 2
Training loss: 2.645128822146152
Validation loss: 2.475514375524829

Epoch: 6| Step: 3
Training loss: 1.7476967231794418
Validation loss: 2.471863498435152

Epoch: 6| Step: 4
Training loss: 2.6073048311745244
Validation loss: 2.4773166799904924

Epoch: 6| Step: 5
Training loss: 2.737356075875554
Validation loss: 2.468634444211878

Epoch: 6| Step: 6
Training loss: 3.0215254067592094
Validation loss: 2.484537165905897

Epoch: 6| Step: 7
Training loss: 2.3681550015934905
Validation loss: 2.4766352303711647

Epoch: 6| Step: 8
Training loss: 2.7831585796873846
Validation loss: 2.48173701443799

Epoch: 6| Step: 9
Training loss: 3.218733556020268
Validation loss: 2.48288705621282

Epoch: 6| Step: 10
Training loss: 2.810901357048375
Validation loss: 2.4885186824398975

Epoch: 6| Step: 11
Training loss: 2.4549746009771405
Validation loss: 2.4938861968561747

Epoch: 6| Step: 12
Training loss: 2.7504315471164764
Validation loss: 2.496690748216243

Epoch: 6| Step: 13
Training loss: 2.2228172525510335
Validation loss: 2.5031684363216162

Epoch: 186| Step: 0
Training loss: 2.739903381760356
Validation loss: 2.5001952187396035

Epoch: 6| Step: 1
Training loss: 2.790159805670711
Validation loss: 2.4932820256903865

Epoch: 6| Step: 2
Training loss: 2.455993724392506
Validation loss: 2.490967102925883

Epoch: 6| Step: 3
Training loss: 2.5488988406138477
Validation loss: 2.489665959194002

Epoch: 6| Step: 4
Training loss: 2.9767095574524416
Validation loss: 2.5074109091433465

Epoch: 6| Step: 5
Training loss: 2.593895597852503
Validation loss: 2.5084225023853643

Epoch: 6| Step: 6
Training loss: 2.46756300578121
Validation loss: 2.5346679500385347

Epoch: 6| Step: 7
Training loss: 2.5476897639012406
Validation loss: 2.5287896051830816

Epoch: 6| Step: 8
Training loss: 2.8224439518434385
Validation loss: 2.5137630372525144

Epoch: 6| Step: 9
Training loss: 2.534192670667609
Validation loss: 2.49415177936944

Epoch: 6| Step: 10
Training loss: 2.712249877929685
Validation loss: 2.4672317665192462

Epoch: 6| Step: 11
Training loss: 2.932283680409769
Validation loss: 2.459898079607368

Epoch: 6| Step: 12
Training loss: 2.4365437906214047
Validation loss: 2.4574383896340186

Epoch: 6| Step: 13
Training loss: 2.708080886421664
Validation loss: 2.4481983959132134

Epoch: 187| Step: 0
Training loss: 2.3888254810267093
Validation loss: 2.4437732330460795

Epoch: 6| Step: 1
Training loss: 2.6525948901375953
Validation loss: 2.4436818779548304

Epoch: 6| Step: 2
Training loss: 2.809322002789364
Validation loss: 2.448628830499871

Epoch: 6| Step: 3
Training loss: 2.9599846184820606
Validation loss: 2.4497033103841637

Epoch: 6| Step: 4
Training loss: 2.2301769165886074
Validation loss: 2.4633986061188446

Epoch: 6| Step: 5
Training loss: 2.2070321142144325
Validation loss: 2.4647779815913347

Epoch: 6| Step: 6
Training loss: 2.7352186372948197
Validation loss: 2.4893611672045663

Epoch: 6| Step: 7
Training loss: 2.6358017068469666
Validation loss: 2.5042495572815717

Epoch: 6| Step: 8
Training loss: 3.125923935680482
Validation loss: 2.519790803679162

Epoch: 6| Step: 9
Training loss: 2.5177434684964903
Validation loss: 2.5447686133586895

Epoch: 6| Step: 10
Training loss: 3.2876040017970283
Validation loss: 2.5503944043706466

Epoch: 6| Step: 11
Training loss: 2.739369653604744
Validation loss: 2.5399165355555886

Epoch: 6| Step: 12
Training loss: 1.8973648745907923
Validation loss: 2.535768674443499

Epoch: 6| Step: 13
Training loss: 2.843789068938785
Validation loss: 2.548737888029891

Epoch: 188| Step: 0
Training loss: 2.7945088015308692
Validation loss: 2.543157545397649

Epoch: 6| Step: 1
Training loss: 2.1865655265461137
Validation loss: 2.548210573456051

Epoch: 6| Step: 2
Training loss: 2.8946192183621533
Validation loss: 2.5412012212324235

Epoch: 6| Step: 3
Training loss: 2.5099622597459748
Validation loss: 2.5291580241268625

Epoch: 6| Step: 4
Training loss: 3.114557901914402
Validation loss: 2.5075928635922664

Epoch: 6| Step: 5
Training loss: 2.628908154513844
Validation loss: 2.490306701961005

Epoch: 6| Step: 6
Training loss: 2.175737199046329
Validation loss: 2.4732956196957536

Epoch: 6| Step: 7
Training loss: 2.6481665188503647
Validation loss: 2.4902705516113675

Epoch: 6| Step: 8
Training loss: 2.5489686189128804
Validation loss: 2.4903634732985123

Epoch: 6| Step: 9
Training loss: 2.9766156850153322
Validation loss: 2.502790448833276

Epoch: 6| Step: 10
Training loss: 2.711371922932701
Validation loss: 2.5097742012151594

Epoch: 6| Step: 11
Training loss: 1.8996162679564046
Validation loss: 2.5108141881585726

Epoch: 6| Step: 12
Training loss: 2.8899512253179633
Validation loss: 2.5210565390060466

Epoch: 6| Step: 13
Training loss: 3.2597153044348515
Validation loss: 2.5262825931649187

Epoch: 189| Step: 0
Training loss: 2.994806244303289
Validation loss: 2.540635297265234

Epoch: 6| Step: 1
Training loss: 3.4861939476543213
Validation loss: 2.537339892272342

Epoch: 6| Step: 2
Training loss: 2.0199952763794915
Validation loss: 2.519745080193327

Epoch: 6| Step: 3
Training loss: 2.6248087132103217
Validation loss: 2.5046594114864416

Epoch: 6| Step: 4
Training loss: 2.239281662133132
Validation loss: 2.5051936335773357

Epoch: 6| Step: 5
Training loss: 2.563292590124641
Validation loss: 2.505030209044347

Epoch: 6| Step: 6
Training loss: 3.394083816662283
Validation loss: 2.4898501556241084

Epoch: 6| Step: 7
Training loss: 2.6803050761449705
Validation loss: 2.494410922485139

Epoch: 6| Step: 8
Training loss: 2.460771827570716
Validation loss: 2.494723041077045

Epoch: 6| Step: 9
Training loss: 2.4283584934136435
Validation loss: 2.5021011116522125

Epoch: 6| Step: 10
Training loss: 1.9591333364974883
Validation loss: 2.504164475813399

Epoch: 6| Step: 11
Training loss: 2.6534835770815604
Validation loss: 2.5020914753163765

Epoch: 6| Step: 12
Training loss: 2.6269636529761153
Validation loss: 2.499922430209149

Epoch: 6| Step: 13
Training loss: 2.7575326404732086
Validation loss: 2.500491483635093

Epoch: 190| Step: 0
Training loss: 2.851503470868054
Validation loss: 2.5035729768707142

Epoch: 6| Step: 1
Training loss: 2.560919111184647
Validation loss: 2.4973633069750654

Epoch: 6| Step: 2
Training loss: 2.899832418137933
Validation loss: 2.4941309651030767

Epoch: 6| Step: 3
Training loss: 2.699757854764985
Validation loss: 2.489410030514826

Epoch: 6| Step: 4
Training loss: 2.567355417342667
Validation loss: 2.489138596739049

Epoch: 6| Step: 5
Training loss: 2.3856249611434555
Validation loss: 2.4920678873977695

Epoch: 6| Step: 6
Training loss: 2.2907073585099305
Validation loss: 2.4928242183825047

Epoch: 6| Step: 7
Training loss: 2.736274016210871
Validation loss: 2.498518913194585

Epoch: 6| Step: 8
Training loss: 3.122705151039263
Validation loss: 2.4897132215282243

Epoch: 6| Step: 9
Training loss: 2.4297342081272086
Validation loss: 2.4935940696852055

Epoch: 6| Step: 10
Training loss: 2.858156705539073
Validation loss: 2.4990025550680746

Epoch: 6| Step: 11
Training loss: 1.7076971994133014
Validation loss: 2.5070761467609035

Epoch: 6| Step: 12
Training loss: 2.7498528701264235
Validation loss: 2.516691208811906

Epoch: 6| Step: 13
Training loss: 2.603625513434869
Validation loss: 2.5344714335989207

Epoch: 191| Step: 0
Training loss: 3.0057142196615896
Validation loss: 2.565074208927237

Epoch: 6| Step: 1
Training loss: 1.5745771203680212
Validation loss: 2.6080407178582687

Epoch: 6| Step: 2
Training loss: 3.0529416453847444
Validation loss: 2.618274793093895

Epoch: 6| Step: 3
Training loss: 2.4058485810981787
Validation loss: 2.6137371752287253

Epoch: 6| Step: 4
Training loss: 2.825976741357595
Validation loss: 2.5864455985411263

Epoch: 6| Step: 5
Training loss: 1.5729649854232075
Validation loss: 2.5597514897552354

Epoch: 6| Step: 6
Training loss: 2.376681987058564
Validation loss: 2.546592943900142

Epoch: 6| Step: 7
Training loss: 2.4087909312153215
Validation loss: 2.5353309369800603

Epoch: 6| Step: 8
Training loss: 3.175433037730731
Validation loss: 2.5212481252460455

Epoch: 6| Step: 9
Training loss: 2.809594836772745
Validation loss: 2.5314067496811803

Epoch: 6| Step: 10
Training loss: 3.0865413026776047
Validation loss: 2.520255103254066

Epoch: 6| Step: 11
Training loss: 2.870425690239773
Validation loss: 2.5143756366521024

Epoch: 6| Step: 12
Training loss: 2.719822639124371
Validation loss: 2.5047018402872094

Epoch: 6| Step: 13
Training loss: 2.090828750785751
Validation loss: 2.5064670041941612

Epoch: 192| Step: 0
Training loss: 2.073832962740402
Validation loss: 2.5011513940137697

Epoch: 6| Step: 1
Training loss: 2.540762933456622
Validation loss: 2.504939810236257

Epoch: 6| Step: 2
Training loss: 2.716257202166977
Validation loss: 2.504103317211562

Epoch: 6| Step: 3
Training loss: 2.486315850814165
Validation loss: 2.512405253973633

Epoch: 6| Step: 4
Training loss: 2.0283152580657413
Validation loss: 2.5133164873905303

Epoch: 6| Step: 5
Training loss: 2.8668568599801483
Validation loss: 2.5141736725914368

Epoch: 6| Step: 6
Training loss: 2.6673979253285625
Validation loss: 2.504308782653529

Epoch: 6| Step: 7
Training loss: 2.4054709820738567
Validation loss: 2.509880416783923

Epoch: 6| Step: 8
Training loss: 2.4630420223837675
Validation loss: 2.5014095906040037

Epoch: 6| Step: 9
Training loss: 2.6239254886701286
Validation loss: 2.491506608958449

Epoch: 6| Step: 10
Training loss: 3.1618892034606363
Validation loss: 2.4871154826827486

Epoch: 6| Step: 11
Training loss: 2.5869560886308824
Validation loss: 2.5245016126199427

Epoch: 6| Step: 12
Training loss: 3.028128048606889
Validation loss: 2.5479741022752025

Epoch: 6| Step: 13
Training loss: 2.956521800411936
Validation loss: 2.5963451717643546

Epoch: 193| Step: 0
Training loss: 2.69722187026083
Validation loss: 2.597420361035549

Epoch: 6| Step: 1
Training loss: 2.8559443003461547
Validation loss: 2.5746159493028946

Epoch: 6| Step: 2
Training loss: 2.405027103548925
Validation loss: 2.517447749141457

Epoch: 6| Step: 3
Training loss: 1.8575867460385875
Validation loss: 2.4873263329312953

Epoch: 6| Step: 4
Training loss: 3.017879927767646
Validation loss: 2.486661524761417

Epoch: 6| Step: 5
Training loss: 2.4861202227983727
Validation loss: 2.4884326263063645

Epoch: 6| Step: 6
Training loss: 2.468823395615862
Validation loss: 2.4936298109989403

Epoch: 6| Step: 7
Training loss: 2.3766534972216107
Validation loss: 2.5128558690940417

Epoch: 6| Step: 8
Training loss: 3.060417245579598
Validation loss: 2.514537002510885

Epoch: 6| Step: 9
Training loss: 2.602987546371465
Validation loss: 2.5136121073524795

Epoch: 6| Step: 10
Training loss: 2.2638106875444617
Validation loss: 2.520903569497897

Epoch: 6| Step: 11
Training loss: 3.0915780197829044
Validation loss: 2.511206804354202

Epoch: 6| Step: 12
Training loss: 3.0717577456878593
Validation loss: 2.5200845217589642

Epoch: 6| Step: 13
Training loss: 2.1375149598072194
Validation loss: 2.5090014652637387

Epoch: 194| Step: 0
Training loss: 2.491878956795664
Validation loss: 2.5010649156276354

Epoch: 6| Step: 1
Training loss: 1.825503434022564
Validation loss: 2.503527595147679

Epoch: 6| Step: 2
Training loss: 3.1762400691009645
Validation loss: 2.492404982388572

Epoch: 6| Step: 3
Training loss: 2.3678644301424208
Validation loss: 2.509871951245375

Epoch: 6| Step: 4
Training loss: 2.31340813203459
Validation loss: 2.5146760051811925

Epoch: 6| Step: 5
Training loss: 2.4200437451972254
Validation loss: 2.5285009092946122

Epoch: 6| Step: 6
Training loss: 2.6957217583028217
Validation loss: 2.5512695458203103

Epoch: 6| Step: 7
Training loss: 3.0399121663304527
Validation loss: 2.575288558531658

Epoch: 6| Step: 8
Training loss: 2.8316513753848214
Validation loss: 2.5800279303454894

Epoch: 6| Step: 9
Training loss: 2.195162404858377
Validation loss: 2.542846295666386

Epoch: 6| Step: 10
Training loss: 2.4174780524913446
Validation loss: 2.5223156455619433

Epoch: 6| Step: 11
Training loss: 2.684610721066153
Validation loss: 2.505068219259126

Epoch: 6| Step: 12
Training loss: 2.764300185313883
Validation loss: 2.5066423364109562

Epoch: 6| Step: 13
Training loss: 2.5193692413998625
Validation loss: 2.4983877992798162

Epoch: 195| Step: 0
Training loss: 3.0720418083405674
Validation loss: 2.5014466109872826

Epoch: 6| Step: 1
Training loss: 2.7154283950628084
Validation loss: 2.5033429058429317

Epoch: 6| Step: 2
Training loss: 2.020876880714012
Validation loss: 2.519674588236127

Epoch: 6| Step: 3
Training loss: 2.9918799179989803
Validation loss: 2.533711763488624

Epoch: 6| Step: 4
Training loss: 2.6641852834379414
Validation loss: 2.5560320630933546

Epoch: 6| Step: 5
Training loss: 2.9298334924561953
Validation loss: 2.5688631242861457

Epoch: 6| Step: 6
Training loss: 1.9551025754941185
Validation loss: 2.583281651144314

Epoch: 6| Step: 7
Training loss: 2.5127615417695126
Validation loss: 2.5806952260144813

Epoch: 6| Step: 8
Training loss: 2.6226230940826634
Validation loss: 2.5632858091950794

Epoch: 6| Step: 9
Training loss: 2.1744331278847877
Validation loss: 2.536355436586657

Epoch: 6| Step: 10
Training loss: 2.6857361438986436
Validation loss: 2.524950019721896

Epoch: 6| Step: 11
Training loss: 2.5158321698103854
Validation loss: 2.508587360965713

Epoch: 6| Step: 12
Training loss: 2.331922263405433
Validation loss: 2.491754436178303

Epoch: 6| Step: 13
Training loss: 1.617767303760971
Validation loss: 2.492150108562751

Epoch: 196| Step: 0
Training loss: 2.5094322606604678
Validation loss: 2.4933892903024173

Epoch: 6| Step: 1
Training loss: 2.092603568883195
Validation loss: 2.4859940751918836

Epoch: 6| Step: 2
Training loss: 2.5974504661793594
Validation loss: 2.4998263350023575

Epoch: 6| Step: 3
Training loss: 2.278608281870641
Validation loss: 2.5005097043951614

Epoch: 6| Step: 4
Training loss: 2.7763622067863345
Validation loss: 2.5068717088660684

Epoch: 6| Step: 5
Training loss: 2.7217881338840377
Validation loss: 2.522131221070059

Epoch: 6| Step: 6
Training loss: 2.7162966127078256
Validation loss: 2.5375609162601696

Epoch: 6| Step: 7
Training loss: 2.809125104574272
Validation loss: 2.553191370421501

Epoch: 6| Step: 8
Training loss: 2.0690091619053645
Validation loss: 2.576089858589522

Epoch: 6| Step: 9
Training loss: 2.3821758482857143
Validation loss: 2.56680376235135

Epoch: 6| Step: 10
Training loss: 2.6912288081304387
Validation loss: 2.5806721404867528

Epoch: 6| Step: 11
Training loss: 2.7518801764084966
Validation loss: 2.568228203084486

Epoch: 6| Step: 12
Training loss: 2.3520136302241648
Validation loss: 2.5657663044901877

Epoch: 6| Step: 13
Training loss: 2.394188313406803
Validation loss: 2.5507242072352208

Epoch: 197| Step: 0
Training loss: 2.596950716275939
Validation loss: 2.544633219090007

Epoch: 6| Step: 1
Training loss: 2.2102632067229377
Validation loss: 2.54997157777328

Epoch: 6| Step: 2
Training loss: 2.2448654929734584
Validation loss: 2.5710229446303003

Epoch: 6| Step: 3
Training loss: 2.7235241769467238
Validation loss: 2.5775573483406022

Epoch: 6| Step: 4
Training loss: 2.2908865785799977
Validation loss: 2.5671120594240286

Epoch: 6| Step: 5
Training loss: 1.9390841130136462
Validation loss: 2.5596892225371746

Epoch: 6| Step: 6
Training loss: 2.635013825961183
Validation loss: 2.5536597908846304

Epoch: 6| Step: 7
Training loss: 3.028451945603333
Validation loss: 2.5568000844441547

Epoch: 6| Step: 8
Training loss: 2.7165816857399765
Validation loss: 2.5408287529792815

Epoch: 6| Step: 9
Training loss: 2.2083997416557795
Validation loss: 2.549237698767186

Epoch: 6| Step: 10
Training loss: 2.761142697613079
Validation loss: 2.5427443275492316

Epoch: 6| Step: 11
Training loss: 3.037231207363447
Validation loss: 2.539220066782059

Epoch: 6| Step: 12
Training loss: 2.6647228865557087
Validation loss: 2.5495005842204166

Epoch: 6| Step: 13
Training loss: 1.4379025600511108
Validation loss: 2.5566821420258985

Epoch: 198| Step: 0
Training loss: 2.210663148282836
Validation loss: 2.558972498641832

Epoch: 6| Step: 1
Training loss: 2.515478947350625
Validation loss: 2.573835555501631

Epoch: 6| Step: 2
Training loss: 2.515159042517048
Validation loss: 2.5435245973088336

Epoch: 6| Step: 3
Training loss: 2.553643436159745
Validation loss: 2.507001163218011

Epoch: 6| Step: 4
Training loss: 2.6210245273960466
Validation loss: 2.4936620466235

Epoch: 6| Step: 5
Training loss: 2.9793687943045226
Validation loss: 2.479649198387125

Epoch: 6| Step: 6
Training loss: 2.328941458071566
Validation loss: 2.4802689542654828

Epoch: 6| Step: 7
Training loss: 2.6014637971316965
Validation loss: 2.4781909258892902

Epoch: 6| Step: 8
Training loss: 2.0565591272507033
Validation loss: 2.4761416402845677

Epoch: 6| Step: 9
Training loss: 2.6356771488652657
Validation loss: 2.4760281471663577

Epoch: 6| Step: 10
Training loss: 2.854872530414798
Validation loss: 2.4813237097959115

Epoch: 6| Step: 11
Training loss: 2.326666970658305
Validation loss: 2.511901795723931

Epoch: 6| Step: 12
Training loss: 2.6317390041866284
Validation loss: 2.529283424779101

Epoch: 6| Step: 13
Training loss: 1.9731259834835682
Validation loss: 2.5604777526523987

Epoch: 199| Step: 0
Training loss: 2.5779413331239347
Validation loss: 2.581641883602713

Epoch: 6| Step: 1
Training loss: 2.3097948283777154
Validation loss: 2.556684221669835

Epoch: 6| Step: 2
Training loss: 2.4316899428670933
Validation loss: 2.536514428833991

Epoch: 6| Step: 3
Training loss: 2.4601372280457547
Validation loss: 2.534864004225435

Epoch: 6| Step: 4
Training loss: 1.9669790022620897
Validation loss: 2.527298391396384

Epoch: 6| Step: 5
Training loss: 2.5717715496333295
Validation loss: 2.531074282857975

Epoch: 6| Step: 6
Training loss: 1.9490514771390004
Validation loss: 2.5338334370627362

Epoch: 6| Step: 7
Training loss: 2.7345648999348264
Validation loss: 2.544019674713675

Epoch: 6| Step: 8
Training loss: 2.7433352771750603
Validation loss: 2.5493195372423703

Epoch: 6| Step: 9
Training loss: 2.5422013350979555
Validation loss: 2.566613102505671

Epoch: 6| Step: 10
Training loss: 2.705983673452449
Validation loss: 2.581330624292851

Epoch: 6| Step: 11
Training loss: 2.8242878041458472
Validation loss: 2.6142049202720608

Epoch: 6| Step: 12
Training loss: 2.225980983889397
Validation loss: 2.633861629826538

Epoch: 6| Step: 13
Training loss: 2.9883712455811136
Validation loss: 2.6458832246202393

Epoch: 200| Step: 0
Training loss: 1.5485580228060387
Validation loss: 2.6408816850735173

Epoch: 6| Step: 1
Training loss: 2.1397607548395827
Validation loss: 2.6011371200112228

Epoch: 6| Step: 2
Training loss: 2.6885459106679823
Validation loss: 2.5645407237408966

Epoch: 6| Step: 3
Training loss: 2.240083458231257
Validation loss: 2.5486394066543956

Epoch: 6| Step: 4
Training loss: 2.5839692430348693
Validation loss: 2.522057783954674

Epoch: 6| Step: 5
Training loss: 2.620888714793986
Validation loss: 2.5257286663548717

Epoch: 6| Step: 6
Training loss: 3.2178953943599486
Validation loss: 2.515183732312513

Epoch: 6| Step: 7
Training loss: 2.4102405304601553
Validation loss: 2.51336285108855

Epoch: 6| Step: 8
Training loss: 2.477131197643135
Validation loss: 2.533566088689231

Epoch: 6| Step: 9
Training loss: 2.665747593411838
Validation loss: 2.539196222537145

Epoch: 6| Step: 10
Training loss: 2.266247578286646
Validation loss: 2.538046337455844

Epoch: 6| Step: 11
Training loss: 3.1430134208021263
Validation loss: 2.5428263689583566

Epoch: 6| Step: 12
Training loss: 1.7969855233077932
Validation loss: 2.545952099315923

Epoch: 6| Step: 13
Training loss: 2.546871068284446
Validation loss: 2.5835030910462233

Testing loss: 2.650927327206612

Epoch: 1| Step: 0
Training loss: 4.578469276428223
Validation loss: 5.146904627482097

Epoch: 6| Step: 1
Training loss: 4.81166410446167
Validation loss: 5.124221463357249

Epoch: 6| Step: 2
Training loss: 4.521579742431641
Validation loss: 5.103360114559051

Epoch: 6| Step: 3
Training loss: 4.5414719581604
Validation loss: 5.080167539658085

Epoch: 6| Step: 4
Training loss: 5.038161277770996
Validation loss: 5.054920652861236

Epoch: 6| Step: 5
Training loss: 4.748271465301514
Validation loss: 5.026962295655282

Epoch: 6| Step: 6
Training loss: 3.8123106956481934
Validation loss: 4.994381163709907

Epoch: 6| Step: 7
Training loss: 4.799808502197266
Validation loss: 4.958422117335822

Epoch: 6| Step: 8
Training loss: 5.245171546936035
Validation loss: 4.918710113853536

Epoch: 6| Step: 9
Training loss: 5.52078914642334
Validation loss: 4.874564155455558

Epoch: 6| Step: 10
Training loss: 4.403167247772217
Validation loss: 4.824305185707667

Epoch: 6| Step: 11
Training loss: 5.969710350036621
Validation loss: 4.769962823519143

Epoch: 6| Step: 12
Training loss: 4.011739253997803
Validation loss: 4.711380015137375

Epoch: 6| Step: 13
Training loss: 4.2939133644104
Validation loss: 4.65007165170485

Epoch: 2| Step: 0
Training loss: 4.014837265014648
Validation loss: 4.586393223013929

Epoch: 6| Step: 1
Training loss: 3.269247531890869
Validation loss: 4.519432380635251

Epoch: 6| Step: 2
Training loss: 4.211935043334961
Validation loss: 4.450914306025351

Epoch: 6| Step: 3
Training loss: 4.402912139892578
Validation loss: 4.384572813587804

Epoch: 6| Step: 4
Training loss: 4.564108371734619
Validation loss: 4.317916941899125

Epoch: 6| Step: 5
Training loss: 5.088912010192871
Validation loss: 4.2480843810624975

Epoch: 6| Step: 6
Training loss: 4.331345081329346
Validation loss: 4.176492624385382

Epoch: 6| Step: 7
Training loss: 3.6463654041290283
Validation loss: 4.102710708495109

Epoch: 6| Step: 8
Training loss: 4.310375690460205
Validation loss: 4.032647830183788

Epoch: 6| Step: 9
Training loss: 5.231235504150391
Validation loss: 3.964678966870872

Epoch: 6| Step: 10
Training loss: 3.602231025695801
Validation loss: 3.9055273814867904

Epoch: 6| Step: 11
Training loss: 3.5651683807373047
Validation loss: 3.850038041350662

Epoch: 6| Step: 12
Training loss: 2.2944228649139404
Validation loss: 3.7994467776308776

Epoch: 6| Step: 13
Training loss: 3.2894296646118164
Validation loss: 3.7534972006274807

Epoch: 3| Step: 0
Training loss: 2.2461187839508057
Validation loss: 3.709262063426356

Epoch: 6| Step: 1
Training loss: 3.9870445728302
Validation loss: 3.670408074573804

Epoch: 6| Step: 2
Training loss: 4.268606662750244
Validation loss: 3.6334619906640824

Epoch: 6| Step: 3
Training loss: 2.9269142150878906
Validation loss: 3.5952700389328824

Epoch: 6| Step: 4
Training loss: 3.420865535736084
Validation loss: 3.5642466083649667

Epoch: 6| Step: 5
Training loss: 3.7673532962799072
Validation loss: 3.5370595250078427

Epoch: 6| Step: 6
Training loss: 4.382449150085449
Validation loss: 3.508363864755118

Epoch: 6| Step: 7
Training loss: 3.3708157539367676
Validation loss: 3.4763176107919342

Epoch: 6| Step: 8
Training loss: 2.982619285583496
Validation loss: 3.4403146466901227

Epoch: 6| Step: 9
Training loss: 3.042769432067871
Validation loss: 3.409893899835566

Epoch: 6| Step: 10
Training loss: 2.452843427658081
Validation loss: 3.3855129313725296

Epoch: 6| Step: 11
Training loss: 4.2735419273376465
Validation loss: 3.3625890413920083

Epoch: 6| Step: 12
Training loss: 3.6140568256378174
Validation loss: 3.3381302843811693

Epoch: 6| Step: 13
Training loss: 3.070061683654785
Validation loss: 3.321943562517884

Epoch: 4| Step: 0
Training loss: 2.4712841510772705
Validation loss: 3.312362604243781

Epoch: 6| Step: 1
Training loss: 3.561225652694702
Validation loss: 3.303674772221555

Epoch: 6| Step: 2
Training loss: 3.179426431655884
Validation loss: 3.2863650475778887

Epoch: 6| Step: 3
Training loss: 3.0613057613372803
Validation loss: 3.268523277774934

Epoch: 6| Step: 4
Training loss: 3.92901349067688
Validation loss: 3.250067026384415

Epoch: 6| Step: 5
Training loss: 3.0649032592773438
Validation loss: 3.2341224147427465

Epoch: 6| Step: 6
Training loss: 3.421978712081909
Validation loss: 3.222030060265654

Epoch: 6| Step: 7
Training loss: 3.6026113033294678
Validation loss: 3.2110824046596402

Epoch: 6| Step: 8
Training loss: 2.4755380153656006
Validation loss: 3.2015895561505388

Epoch: 6| Step: 9
Training loss: 3.341404438018799
Validation loss: 3.1903137391613376

Epoch: 6| Step: 10
Training loss: 3.1169662475585938
Validation loss: 3.1847896217018046

Epoch: 6| Step: 11
Training loss: 3.282731056213379
Validation loss: 3.1735148788780294

Epoch: 6| Step: 12
Training loss: 3.182830333709717
Validation loss: 3.1779188058709584

Epoch: 6| Step: 13
Training loss: 3.438429355621338
Validation loss: 3.1522127069452757

Epoch: 5| Step: 0
Training loss: 3.041494846343994
Validation loss: 3.14486345424447

Epoch: 6| Step: 1
Training loss: 3.5068976879119873
Validation loss: 3.140164493232645

Epoch: 6| Step: 2
Training loss: 4.182385444641113
Validation loss: 3.131983639091574

Epoch: 6| Step: 3
Training loss: 2.9880123138427734
Validation loss: 3.127451155775337

Epoch: 6| Step: 4
Training loss: 3.3716132640838623
Validation loss: 3.1143597761789956

Epoch: 6| Step: 5
Training loss: 2.707799196243286
Validation loss: 3.1076622496369066

Epoch: 6| Step: 6
Training loss: 3.22312331199646
Validation loss: 3.1003712787423083

Epoch: 6| Step: 7
Training loss: 3.7082200050354004
Validation loss: 3.093057914446759

Epoch: 6| Step: 8
Training loss: 2.8933281898498535
Validation loss: 3.084527700178085

Epoch: 6| Step: 9
Training loss: 2.899959087371826
Validation loss: 3.0762345944681475

Epoch: 6| Step: 10
Training loss: 2.4193029403686523
Validation loss: 3.07138035374303

Epoch: 6| Step: 11
Training loss: 3.354902982711792
Validation loss: 3.065224680849301

Epoch: 6| Step: 12
Training loss: 2.3560216426849365
Validation loss: 3.062776627079133

Epoch: 6| Step: 13
Training loss: 3.2475366592407227
Validation loss: 3.0593262539115003

Epoch: 6| Step: 0
Training loss: 2.4318082332611084
Validation loss: 3.0510707029732327

Epoch: 6| Step: 1
Training loss: 3.649953842163086
Validation loss: 3.051259327960271

Epoch: 6| Step: 2
Training loss: 2.68508243560791
Validation loss: 3.0463292880724837

Epoch: 6| Step: 3
Training loss: 2.563478708267212
Validation loss: 3.0475076244723414

Epoch: 6| Step: 4
Training loss: 3.7916526794433594
Validation loss: 3.032917848197363

Epoch: 6| Step: 5
Training loss: 3.2845778465270996
Validation loss: 3.027531580258441

Epoch: 6| Step: 6
Training loss: 3.728361129760742
Validation loss: 3.0253686469088317

Epoch: 6| Step: 7
Training loss: 1.9857065677642822
Validation loss: 3.024401100732947

Epoch: 6| Step: 8
Training loss: 3.4633688926696777
Validation loss: 3.030700691284672

Epoch: 6| Step: 9
Training loss: 3.086299419403076
Validation loss: 3.0239670994461223

Epoch: 6| Step: 10
Training loss: 2.7324156761169434
Validation loss: 3.008025371900169

Epoch: 6| Step: 11
Training loss: 3.6231884956359863
Validation loss: 3.007817506790161

Epoch: 6| Step: 12
Training loss: 3.470421314239502
Validation loss: 3.0145119338907223

Epoch: 6| Step: 13
Training loss: 2.4499762058258057
Validation loss: 3.021463568492602

Epoch: 7| Step: 0
Training loss: 2.958765983581543
Validation loss: 3.0171652327301683

Epoch: 6| Step: 1
Training loss: 3.44551420211792
Validation loss: 3.0034658344843055

Epoch: 6| Step: 2
Training loss: 2.1348700523376465
Validation loss: 2.989765321054766

Epoch: 6| Step: 3
Training loss: 3.1090354919433594
Validation loss: 2.983199627168717

Epoch: 6| Step: 4
Training loss: 2.48762845993042
Validation loss: 2.9802249375210015

Epoch: 6| Step: 5
Training loss: 3.5425987243652344
Validation loss: 2.9765195513284333

Epoch: 6| Step: 6
Training loss: 2.7561287879943848
Validation loss: 2.9665662139974613

Epoch: 6| Step: 7
Training loss: 3.150634527206421
Validation loss: 2.961508886788481

Epoch: 6| Step: 8
Training loss: 4.247426986694336
Validation loss: 2.959373463866531

Epoch: 6| Step: 9
Training loss: 3.4899110794067383
Validation loss: 2.9516031665186726

Epoch: 6| Step: 10
Training loss: 3.3999264240264893
Validation loss: 2.943771605850548

Epoch: 6| Step: 11
Training loss: 2.350390672683716
Validation loss: 2.937004179082891

Epoch: 6| Step: 12
Training loss: 2.8161120414733887
Validation loss: 2.9335952933116625

Epoch: 6| Step: 13
Training loss: 2.4912233352661133
Validation loss: 2.9326763691440707

Epoch: 8| Step: 0
Training loss: 3.0405571460723877
Validation loss: 2.9323515597210137

Epoch: 6| Step: 1
Training loss: 2.7465105056762695
Validation loss: 2.9223596408802974

Epoch: 6| Step: 2
Training loss: 3.1753804683685303
Validation loss: 2.912094985285113

Epoch: 6| Step: 3
Training loss: 3.713432788848877
Validation loss: 2.917481001987252

Epoch: 6| Step: 4
Training loss: 3.7989401817321777
Validation loss: 2.962412290675666

Epoch: 6| Step: 5
Training loss: 2.993340253829956
Validation loss: 2.993424174606159

Epoch: 6| Step: 6
Training loss: 2.9640884399414062
Validation loss: 2.9858256514354418

Epoch: 6| Step: 7
Training loss: 2.3103013038635254
Validation loss: 2.9776741817433345

Epoch: 6| Step: 8
Training loss: 3.242863655090332
Validation loss: 2.9703149154622066

Epoch: 6| Step: 9
Training loss: 2.774768829345703
Validation loss: 2.972889495152299

Epoch: 6| Step: 10
Training loss: 2.9860849380493164
Validation loss: 2.9669821390541653

Epoch: 6| Step: 11
Training loss: 3.548072576522827
Validation loss: 2.962582411304597

Epoch: 6| Step: 12
Training loss: 2.380535364151001
Validation loss: 2.957411978834419

Epoch: 6| Step: 13
Training loss: 2.7242908477783203
Validation loss: 2.9535575246298187

Epoch: 9| Step: 0
Training loss: 2.968836545944214
Validation loss: 2.9416131101628786

Epoch: 6| Step: 1
Training loss: 2.8382885456085205
Validation loss: 2.9265130848012944

Epoch: 6| Step: 2
Training loss: 3.623197078704834
Validation loss: 2.930313000115015

Epoch: 6| Step: 3
Training loss: 2.930257558822632
Validation loss: 2.9178376454179005

Epoch: 6| Step: 4
Training loss: 2.7396082878112793
Validation loss: 2.9103627922714397

Epoch: 6| Step: 5
Training loss: 2.761435031890869
Validation loss: 2.908449939502183

Epoch: 6| Step: 6
Training loss: 2.8355627059936523
Validation loss: 2.907575635499852

Epoch: 6| Step: 7
Training loss: 3.132673501968384
Validation loss: 2.9020624417130665

Epoch: 6| Step: 8
Training loss: 2.869121551513672
Validation loss: 2.8951991783675326

Epoch: 6| Step: 9
Training loss: 3.421771287918091
Validation loss: 2.890475885842436

Epoch: 6| Step: 10
Training loss: 3.28538179397583
Validation loss: 2.8886496046537995

Epoch: 6| Step: 11
Training loss: 3.446537733078003
Validation loss: 2.886393521421699

Epoch: 6| Step: 12
Training loss: 2.588467597961426
Validation loss: 2.8848845061435493

Epoch: 6| Step: 13
Training loss: 2.38669490814209
Validation loss: 2.8802955817150813

Epoch: 10| Step: 0
Training loss: 2.957427978515625
Validation loss: 2.877429844230734

Epoch: 6| Step: 1
Training loss: 2.56640887260437
Validation loss: 2.874804586492559

Epoch: 6| Step: 2
Training loss: 3.181403875350952
Validation loss: 2.873607358624858

Epoch: 6| Step: 3
Training loss: 3.4055087566375732
Validation loss: 2.872669514789376

Epoch: 6| Step: 4
Training loss: 3.0754828453063965
Validation loss: 2.8637878228259344

Epoch: 6| Step: 5
Training loss: 3.7917871475219727
Validation loss: 2.8557237476430912

Epoch: 6| Step: 6
Training loss: 2.076198101043701
Validation loss: 2.847121589927263

Epoch: 6| Step: 7
Training loss: 3.6226184368133545
Validation loss: 2.848438670558314

Epoch: 6| Step: 8
Training loss: 2.8869078159332275
Validation loss: 2.8487532472097747

Epoch: 6| Step: 9
Training loss: 2.8419013023376465
Validation loss: 2.847781494099607

Epoch: 6| Step: 10
Training loss: 3.2663817405700684
Validation loss: 2.842666946431642

Epoch: 6| Step: 11
Training loss: 2.1219875812530518
Validation loss: 2.843021221058343

Epoch: 6| Step: 12
Training loss: 2.784334182739258
Validation loss: 2.8428626009213027

Epoch: 6| Step: 13
Training loss: 3.0275721549987793
Validation loss: 2.84074039869411

Epoch: 11| Step: 0
Training loss: 3.8437576293945312
Validation loss: 2.8364887955368205

Epoch: 6| Step: 1
Training loss: 3.2890307903289795
Validation loss: 2.8312880403252056

Epoch: 6| Step: 2
Training loss: 2.694011688232422
Validation loss: 2.82768343340966

Epoch: 6| Step: 3
Training loss: 2.4242796897888184
Validation loss: 2.826899564394387

Epoch: 6| Step: 4
Training loss: 2.3522145748138428
Validation loss: 2.8266322587126043

Epoch: 6| Step: 5
Training loss: 2.146324872970581
Validation loss: 2.8319526974872877

Epoch: 6| Step: 6
Training loss: 2.3148560523986816
Validation loss: 2.8264108575800413

Epoch: 6| Step: 7
Training loss: 2.8948283195495605
Validation loss: 2.8218294625641196

Epoch: 6| Step: 8
Training loss: 3.2295989990234375
Validation loss: 2.8138730115787958

Epoch: 6| Step: 9
Training loss: 3.1476635932922363
Validation loss: 2.8159431872829312

Epoch: 6| Step: 10
Training loss: 2.8314316272735596
Validation loss: 2.8198788242955364

Epoch: 6| Step: 11
Training loss: 3.3978424072265625
Validation loss: 2.811279550675423

Epoch: 6| Step: 12
Training loss: 3.702399253845215
Validation loss: 2.801896707985991

Epoch: 6| Step: 13
Training loss: 3.076481819152832
Validation loss: 2.803300170488255

Epoch: 12| Step: 0
Training loss: 3.3371760845184326
Validation loss: 2.8082857516504105

Epoch: 6| Step: 1
Training loss: 3.006160259246826
Validation loss: 2.807475341263638

Epoch: 6| Step: 2
Training loss: 2.739471912384033
Validation loss: 2.8039193871200725

Epoch: 6| Step: 3
Training loss: 2.01949143409729
Validation loss: 2.799801513712893

Epoch: 6| Step: 4
Training loss: 3.651388645172119
Validation loss: 2.7979793830584456

Epoch: 6| Step: 5
Training loss: 2.6031737327575684
Validation loss: 2.7951778134992047

Epoch: 6| Step: 6
Training loss: 2.8786139488220215
Validation loss: 2.7891014622103785

Epoch: 6| Step: 7
Training loss: 2.885815382003784
Validation loss: 2.783097344060098

Epoch: 6| Step: 8
Training loss: 3.723557472229004
Validation loss: 2.771628318294402

Epoch: 6| Step: 9
Training loss: 2.577927350997925
Validation loss: 2.7417248167017454

Epoch: 6| Step: 10
Training loss: 2.514233112335205
Validation loss: 2.750365746918545

Epoch: 6| Step: 11
Training loss: 3.230583667755127
Validation loss: 2.7569162896884385

Epoch: 6| Step: 12
Training loss: 3.1151795387268066
Validation loss: 2.7491072762397026

Epoch: 6| Step: 13
Training loss: 2.478987693786621
Validation loss: 2.742195695959112

Epoch: 13| Step: 0
Training loss: 2.7559762001037598
Validation loss: 2.7435484470859652

Epoch: 6| Step: 1
Training loss: 2.5784807205200195
Validation loss: 2.7442867755889893

Epoch: 6| Step: 2
Training loss: 3.6267001628875732
Validation loss: 2.7430090955508653

Epoch: 6| Step: 3
Training loss: 2.9270222187042236
Validation loss: 2.732589024369435

Epoch: 6| Step: 4
Training loss: 3.435563325881958
Validation loss: 2.727515694915607

Epoch: 6| Step: 5
Training loss: 2.5742785930633545
Validation loss: 2.7220190212290776

Epoch: 6| Step: 6
Training loss: 3.188706874847412
Validation loss: 2.718233608430432

Epoch: 6| Step: 7
Training loss: 3.1284635066986084
Validation loss: 2.717166177688106

Epoch: 6| Step: 8
Training loss: 2.7161855697631836
Validation loss: 2.7157221173727386

Epoch: 6| Step: 9
Training loss: 2.94197940826416
Validation loss: 2.7140135457438808

Epoch: 6| Step: 10
Training loss: 2.696112632751465
Validation loss: 2.713396087769539

Epoch: 6| Step: 11
Training loss: 3.16959810256958
Validation loss: 2.7116504664062173

Epoch: 6| Step: 12
Training loss: 2.559171676635742
Validation loss: 2.7121456617950113

Epoch: 6| Step: 13
Training loss: 1.4281724691390991
Validation loss: 2.710441163791123

Epoch: 14| Step: 0
Training loss: 3.027346611022949
Validation loss: 2.716546868765226

Epoch: 6| Step: 1
Training loss: 2.912233352661133
Validation loss: 2.707512224874189

Epoch: 6| Step: 2
Training loss: 3.461209297180176
Validation loss: 2.7048983625186387

Epoch: 6| Step: 3
Training loss: 2.834383010864258
Validation loss: 2.7073634286080637

Epoch: 6| Step: 4
Training loss: 2.412506580352783
Validation loss: 2.705889432660995

Epoch: 6| Step: 5
Training loss: 2.151357650756836
Validation loss: 2.707328540022655

Epoch: 6| Step: 6
Training loss: 3.0546185970306396
Validation loss: 2.707347095653575

Epoch: 6| Step: 7
Training loss: 2.2045412063598633
Validation loss: 2.7074169471699703

Epoch: 6| Step: 8
Training loss: 3.145548105239868
Validation loss: 2.7102275484351703

Epoch: 6| Step: 9
Training loss: 3.933222770690918
Validation loss: 2.7029872273886077

Epoch: 6| Step: 10
Training loss: 2.50215220451355
Validation loss: 2.7003797023527083

Epoch: 6| Step: 11
Training loss: 2.775580406188965
Validation loss: 2.698064773313461

Epoch: 6| Step: 12
Training loss: 2.8441991806030273
Validation loss: 2.702558784074681

Epoch: 6| Step: 13
Training loss: 3.043888807296753
Validation loss: 2.7095247776277605

Epoch: 15| Step: 0
Training loss: 2.5578901767730713
Validation loss: 2.7126459203740603

Epoch: 6| Step: 1
Training loss: 3.326667070388794
Validation loss: 2.71014412244161

Epoch: 6| Step: 2
Training loss: 2.5765485763549805
Validation loss: 2.705475353425549

Epoch: 6| Step: 3
Training loss: 3.319406509399414
Validation loss: 2.7027759244365077

Epoch: 6| Step: 4
Training loss: 2.7459869384765625
Validation loss: 2.6997169576665407

Epoch: 6| Step: 5
Training loss: 3.1205384731292725
Validation loss: 2.699047565460205

Epoch: 6| Step: 6
Training loss: 2.5289881229400635
Validation loss: 2.7007964605926187

Epoch: 6| Step: 7
Training loss: 2.1268649101257324
Validation loss: 2.6966008524740896

Epoch: 6| Step: 8
Training loss: 3.3774235248565674
Validation loss: 2.695383897391699

Epoch: 6| Step: 9
Training loss: 2.9118800163269043
Validation loss: 2.69894362777792

Epoch: 6| Step: 10
Training loss: 3.360330581665039
Validation loss: 2.693971231419553

Epoch: 6| Step: 11
Training loss: 2.6566665172576904
Validation loss: 2.6931170673780542

Epoch: 6| Step: 12
Training loss: 2.858126163482666
Validation loss: 2.692678528447305

Epoch: 6| Step: 13
Training loss: 2.517260789871216
Validation loss: 2.685480679235151

Epoch: 16| Step: 0
Training loss: 3.3668770790100098
Validation loss: 2.686861317644837

Epoch: 6| Step: 1
Training loss: 3.4175925254821777
Validation loss: 2.6914815723255114

Epoch: 6| Step: 2
Training loss: 2.8551528453826904
Validation loss: 2.6982640861183085

Epoch: 6| Step: 3
Training loss: 2.8318638801574707
Validation loss: 2.6842491960012786

Epoch: 6| Step: 4
Training loss: 3.4683713912963867
Validation loss: 2.684531250307637

Epoch: 6| Step: 5
Training loss: 2.883293628692627
Validation loss: 2.682140470832907

Epoch: 6| Step: 6
Training loss: 1.496549367904663
Validation loss: 2.6842077188594367

Epoch: 6| Step: 7
Training loss: 3.6597084999084473
Validation loss: 2.6858265297387236

Epoch: 6| Step: 8
Training loss: 2.940110206604004
Validation loss: 2.6782764721942205

Epoch: 6| Step: 9
Training loss: 2.7421064376831055
Validation loss: 2.6817502975463867

Epoch: 6| Step: 10
Training loss: 2.550922393798828
Validation loss: 2.682666701655234

Epoch: 6| Step: 11
Training loss: 1.1652066707611084
Validation loss: 2.698379878074892

Epoch: 6| Step: 12
Training loss: 3.6202127933502197
Validation loss: 2.7022812956122944

Epoch: 6| Step: 13
Training loss: 3.2840235233306885
Validation loss: 2.6898098325216644

Epoch: 17| Step: 0
Training loss: 2.3800764083862305
Validation loss: 2.681092280213551

Epoch: 6| Step: 1
Training loss: 4.010580062866211
Validation loss: 2.67717678828906

Epoch: 6| Step: 2
Training loss: 3.880949020385742
Validation loss: 2.679657613077471

Epoch: 6| Step: 3
Training loss: 2.375828981399536
Validation loss: 2.6795503811169694

Epoch: 6| Step: 4
Training loss: 1.8024578094482422
Validation loss: 2.6938411676755516

Epoch: 6| Step: 5
Training loss: 3.83748197555542
Validation loss: 2.7174860380029164

Epoch: 6| Step: 6
Training loss: 3.3161516189575195
Validation loss: 2.6819459289632817

Epoch: 6| Step: 7
Training loss: 2.2345528602600098
Validation loss: 2.6702290247845393

Epoch: 6| Step: 8
Training loss: 2.1102542877197266
Validation loss: 2.6712835501599055

Epoch: 6| Step: 9
Training loss: 3.457951307296753
Validation loss: 2.6894176877954954

Epoch: 6| Step: 10
Training loss: 2.4994893074035645
Validation loss: 2.7081132063301663

Epoch: 6| Step: 11
Training loss: 2.217329502105713
Validation loss: 2.7152619413150254

Epoch: 6| Step: 12
Training loss: 3.3446664810180664
Validation loss: 2.6843849894821004

Epoch: 6| Step: 13
Training loss: 2.2122414112091064
Validation loss: 2.6613173664257093

Epoch: 18| Step: 0
Training loss: 3.081665277481079
Validation loss: 2.6676935508687007

Epoch: 6| Step: 1
Training loss: 3.4975688457489014
Validation loss: 2.6792789864283737

Epoch: 6| Step: 2
Training loss: 2.773357391357422
Validation loss: 2.6858899670262493

Epoch: 6| Step: 3
Training loss: 2.8448646068573
Validation loss: 2.8158209118791806

Epoch: 6| Step: 4
Training loss: 3.154177665710449
Validation loss: 2.787599901999197

Epoch: 6| Step: 5
Training loss: 2.898975372314453
Validation loss: 2.849368756817233

Epoch: 6| Step: 6
Training loss: 3.208312749862671
Validation loss: 2.8420500140036307

Epoch: 6| Step: 7
Training loss: 2.919065237045288
Validation loss: 2.8034359896054832

Epoch: 6| Step: 8
Training loss: 3.2944180965423584
Validation loss: 2.7716296872785016

Epoch: 6| Step: 9
Training loss: 3.4537055492401123
Validation loss: 2.7736556388998546

Epoch: 6| Step: 10
Training loss: 1.6612626314163208
Validation loss: 2.79722096074012

Epoch: 6| Step: 11
Training loss: 3.0018858909606934
Validation loss: 2.8130136459104476

Epoch: 6| Step: 12
Training loss: 2.1461079120635986
Validation loss: 2.8249727679837133

Epoch: 6| Step: 13
Training loss: 2.9158899784088135
Validation loss: 2.856901438005509

Epoch: 19| Step: 0
Training loss: 2.2987213134765625
Validation loss: 2.8615100332485732

Epoch: 6| Step: 1
Training loss: 3.508391857147217
Validation loss: 2.8594944810354583

Epoch: 6| Step: 2
Training loss: 3.6807689666748047
Validation loss: 2.8009897316655805

Epoch: 6| Step: 3
Training loss: 3.3511335849761963
Validation loss: 2.760709501081897

Epoch: 6| Step: 4
Training loss: 2.8062567710876465
Validation loss: 2.737592053669755

Epoch: 6| Step: 5
Training loss: 2.3420002460479736
Validation loss: 2.7327718273285897

Epoch: 6| Step: 6
Training loss: 2.9575135707855225
Validation loss: 2.733417510986328

Epoch: 6| Step: 7
Training loss: 2.847874402999878
Validation loss: 2.7690780111538467

Epoch: 6| Step: 8
Training loss: 2.9576148986816406
Validation loss: 2.810742216725503

Epoch: 6| Step: 9
Training loss: 2.848637104034424
Validation loss: 2.8595467485407347

Epoch: 6| Step: 10
Training loss: 3.1498031616210938
Validation loss: 2.760884536209927

Epoch: 6| Step: 11
Training loss: 2.8176512718200684
Validation loss: 2.722018135491238

Epoch: 6| Step: 12
Training loss: 2.5071444511413574
Validation loss: 2.7211657595890824

Epoch: 6| Step: 13
Training loss: 3.0044755935668945
Validation loss: 2.747063527825058

Epoch: 20| Step: 0
Training loss: 2.9211864471435547
Validation loss: 2.7656388949322444

Epoch: 6| Step: 1
Training loss: 2.8863937854766846
Validation loss: 2.7924836143370597

Epoch: 6| Step: 2
Training loss: 3.1078433990478516
Validation loss: 2.7834637113796767

Epoch: 6| Step: 3
Training loss: 3.0911366939544678
Validation loss: 2.736819359564012

Epoch: 6| Step: 4
Training loss: 2.5532522201538086
Validation loss: 2.7360876042355775

Epoch: 6| Step: 5
Training loss: 2.319542169570923
Validation loss: 2.7451016338922645

Epoch: 6| Step: 6
Training loss: 3.058223247528076
Validation loss: 2.7520669352623726

Epoch: 6| Step: 7
Training loss: 3.159571647644043
Validation loss: 2.7409783896579536

Epoch: 6| Step: 8
Training loss: 3.384873390197754
Validation loss: 2.709352385613226

Epoch: 6| Step: 9
Training loss: 2.936476945877075
Validation loss: 2.6995979739773657

Epoch: 6| Step: 10
Training loss: 2.6029982566833496
Validation loss: 2.7097602839111

Epoch: 6| Step: 11
Training loss: 2.822161912918091
Validation loss: 2.7150523406203075

Epoch: 6| Step: 12
Training loss: 3.2453622817993164
Validation loss: 2.718422810236613

Epoch: 6| Step: 13
Training loss: 2.074998617172241
Validation loss: 2.7155262577918267

Epoch: 21| Step: 0
Training loss: 3.0427746772766113
Validation loss: 2.712854544321696

Epoch: 6| Step: 1
Training loss: 2.8708577156066895
Validation loss: 2.71036894090714

Epoch: 6| Step: 2
Training loss: 2.752248764038086
Validation loss: 2.711738173679639

Epoch: 6| Step: 3
Training loss: 3.66538143157959
Validation loss: 2.7021313072532736

Epoch: 6| Step: 4
Training loss: 3.714780807495117
Validation loss: 2.698558812500328

Epoch: 6| Step: 5
Training loss: 2.3828277587890625
Validation loss: 2.6984470787868706

Epoch: 6| Step: 6
Training loss: 2.841247797012329
Validation loss: 2.696483237769014

Epoch: 6| Step: 7
Training loss: 2.9752378463745117
Validation loss: 2.696928990784512

Epoch: 6| Step: 8
Training loss: 2.5976216793060303
Validation loss: 2.6982400391691472

Epoch: 6| Step: 9
Training loss: 2.3621697425842285
Validation loss: 2.6989902219464703

Epoch: 6| Step: 10
Training loss: 3.116056203842163
Validation loss: 2.697220148578767

Epoch: 6| Step: 11
Training loss: 2.5668797492980957
Validation loss: 2.691354513168335

Epoch: 6| Step: 12
Training loss: 2.9773130416870117
Validation loss: 2.688864851510653

Epoch: 6| Step: 13
Training loss: 1.664271593093872
Validation loss: 2.6887144529691307

Epoch: 22| Step: 0
Training loss: 3.064657211303711
Validation loss: 2.686319847260752

Epoch: 6| Step: 1
Training loss: 2.9781360626220703
Validation loss: 2.6886275660607124

Epoch: 6| Step: 2
Training loss: 2.0841825008392334
Validation loss: 2.6854810894176526

Epoch: 6| Step: 3
Training loss: 3.127786159515381
Validation loss: 2.6842161045279553

Epoch: 6| Step: 4
Training loss: 2.9022367000579834
Validation loss: 2.6813711453509588

Epoch: 6| Step: 5
Training loss: 2.7036335468292236
Validation loss: 2.679147910046321

Epoch: 6| Step: 6
Training loss: 2.907684803009033
Validation loss: 2.67724351216388

Epoch: 6| Step: 7
Training loss: 2.580270767211914
Validation loss: 2.6767446789690243

Epoch: 6| Step: 8
Training loss: 2.081761360168457
Validation loss: 2.679056118893367

Epoch: 6| Step: 9
Training loss: 3.207836627960205
Validation loss: 2.6757954628236833

Epoch: 6| Step: 10
Training loss: 3.3984153270721436
Validation loss: 2.6799337761376494

Epoch: 6| Step: 11
Training loss: 3.0001401901245117
Validation loss: 2.682763171452348

Epoch: 6| Step: 12
Training loss: 2.8506462574005127
Validation loss: 2.6780505744359826

Epoch: 6| Step: 13
Training loss: 3.1402504444122314
Validation loss: 2.679202959101687

Epoch: 23| Step: 0
Training loss: 2.580143928527832
Validation loss: 2.6727157485100532

Epoch: 6| Step: 1
Training loss: 2.008465528488159
Validation loss: 2.6759528729223434

Epoch: 6| Step: 2
Training loss: 3.32785964012146
Validation loss: 2.6745446343575754

Epoch: 6| Step: 3
Training loss: 2.9918577671051025
Validation loss: 2.669057979378649

Epoch: 6| Step: 4
Training loss: 3.057366371154785
Validation loss: 2.6664541536761868

Epoch: 6| Step: 5
Training loss: 3.077314615249634
Validation loss: 2.6692805520949827

Epoch: 6| Step: 6
Training loss: 3.1418097019195557
Validation loss: 2.6658500676514

Epoch: 6| Step: 7
Training loss: 2.772134780883789
Validation loss: 2.6655551541236138

Epoch: 6| Step: 8
Training loss: 2.1302525997161865
Validation loss: 2.666082412965836

Epoch: 6| Step: 9
Training loss: 2.5458521842956543
Validation loss: 2.65835004468118

Epoch: 6| Step: 10
Training loss: 2.9342334270477295
Validation loss: 2.6578574718967563

Epoch: 6| Step: 11
Training loss: 3.983651876449585
Validation loss: 2.6600115709407355

Epoch: 6| Step: 12
Training loss: 2.3505687713623047
Validation loss: 2.658773301750101

Epoch: 6| Step: 13
Training loss: 2.7479918003082275
Validation loss: 2.6561427808577016

Epoch: 24| Step: 0
Training loss: 1.9181848764419556
Validation loss: 2.6557427811366257

Epoch: 6| Step: 1
Training loss: 3.3438327312469482
Validation loss: 2.6540927758780857

Epoch: 6| Step: 2
Training loss: 2.5507688522338867
Validation loss: 2.654941225564608

Epoch: 6| Step: 3
Training loss: 2.617621421813965
Validation loss: 2.6535235297295356

Epoch: 6| Step: 4
Training loss: 2.9426023960113525
Validation loss: 2.651402476013348

Epoch: 6| Step: 5
Training loss: 2.479485034942627
Validation loss: 2.649141703882525

Epoch: 6| Step: 6
Training loss: 3.340986728668213
Validation loss: 2.648658396095358

Epoch: 6| Step: 7
Training loss: 2.4904966354370117
Validation loss: 2.6522623633825653

Epoch: 6| Step: 8
Training loss: 3.272552251815796
Validation loss: 2.6570601924773185

Epoch: 6| Step: 9
Training loss: 2.509294033050537
Validation loss: 2.647106880782753

Epoch: 6| Step: 10
Training loss: 2.597407817840576
Validation loss: 2.650278352922009

Epoch: 6| Step: 11
Training loss: 3.883460521697998
Validation loss: 2.6499145236066592

Epoch: 6| Step: 12
Training loss: 2.8192834854125977
Validation loss: 2.642410252683906

Epoch: 6| Step: 13
Training loss: 2.690460681915283
Validation loss: 2.6438145945149083

Epoch: 25| Step: 0
Training loss: 2.759294271469116
Validation loss: 2.6432149461520615

Epoch: 6| Step: 1
Training loss: 2.3816168308258057
Validation loss: 2.6474597864253546

Epoch: 6| Step: 2
Training loss: 3.0506558418273926
Validation loss: 2.64754605549638

Epoch: 6| Step: 3
Training loss: 2.612374782562256
Validation loss: 2.64581730545208

Epoch: 6| Step: 4
Training loss: 2.163768768310547
Validation loss: 2.6505623197042816

Epoch: 6| Step: 5
Training loss: 2.606916666030884
Validation loss: 2.64657239760122

Epoch: 6| Step: 6
Training loss: 2.5570240020751953
Validation loss: 2.647186433115313

Epoch: 6| Step: 7
Training loss: 3.787893295288086
Validation loss: 2.6445253484992572

Epoch: 6| Step: 8
Training loss: 2.526702880859375
Validation loss: 2.637903723665463

Epoch: 6| Step: 9
Training loss: 3.100673198699951
Validation loss: 2.634688602980747

Epoch: 6| Step: 10
Training loss: 2.585805892944336
Validation loss: 2.640391319028793

Epoch: 6| Step: 11
Training loss: 2.879544734954834
Validation loss: 2.6332759626450075

Epoch: 6| Step: 12
Training loss: 3.4467837810516357
Validation loss: 2.632547660540509

Epoch: 6| Step: 13
Training loss: 3.1833629608154297
Validation loss: 2.6328315222135155

Epoch: 26| Step: 0
Training loss: 3.0678915977478027
Validation loss: 2.635490184189171

Epoch: 6| Step: 1
Training loss: 1.8435256481170654
Validation loss: 2.6305006011839835

Epoch: 6| Step: 2
Training loss: 2.905276298522949
Validation loss: 2.631248071629514

Epoch: 6| Step: 3
Training loss: 2.854921340942383
Validation loss: 2.6264095357669297

Epoch: 6| Step: 4
Training loss: 2.5635013580322266
Validation loss: 2.6409851581819597

Epoch: 6| Step: 5
Training loss: 2.866000175476074
Validation loss: 2.6625246950375137

Epoch: 6| Step: 6
Training loss: 2.331242561340332
Validation loss: 2.690831079277941

Epoch: 6| Step: 7
Training loss: 3.416020393371582
Validation loss: 2.730359915764101

Epoch: 6| Step: 8
Training loss: 3.2363126277923584
Validation loss: 2.7279006717025593

Epoch: 6| Step: 9
Training loss: 2.7660837173461914
Validation loss: 2.683403553501252

Epoch: 6| Step: 10
Training loss: 2.680542230606079
Validation loss: 2.6523559196020967

Epoch: 6| Step: 11
Training loss: 2.6073179244995117
Validation loss: 2.6269727394145024

Epoch: 6| Step: 12
Training loss: 3.291548728942871
Validation loss: 2.6310025004930395

Epoch: 6| Step: 13
Training loss: 3.2408716678619385
Validation loss: 2.661988450634864

Epoch: 27| Step: 0
Training loss: 2.2482523918151855
Validation loss: 2.721611484404533

Epoch: 6| Step: 1
Training loss: 2.4910006523132324
Validation loss: 2.74996341172085

Epoch: 6| Step: 2
Training loss: 3.3083384037017822
Validation loss: 2.710228307272798

Epoch: 6| Step: 3
Training loss: 3.1034421920776367
Validation loss: 2.6776250587996615

Epoch: 6| Step: 4
Training loss: 2.397942304611206
Validation loss: 2.665330128003192

Epoch: 6| Step: 5
Training loss: 3.298460006713867
Validation loss: 2.642405576603387

Epoch: 6| Step: 6
Training loss: 2.465151071548462
Validation loss: 2.6300026575724282

Epoch: 6| Step: 7
Training loss: 2.7807397842407227
Validation loss: 2.6209072861620175

Epoch: 6| Step: 8
Training loss: 3.214031219482422
Validation loss: 2.624211172903738

Epoch: 6| Step: 9
Training loss: 2.816021680831909
Validation loss: 2.638173662206178

Epoch: 6| Step: 10
Training loss: 2.555976390838623
Validation loss: 2.6581730458044235

Epoch: 6| Step: 11
Training loss: 2.9186339378356934
Validation loss: 2.68312333988887

Epoch: 6| Step: 12
Training loss: 3.0495736598968506
Validation loss: 2.6868474534762803

Epoch: 6| Step: 13
Training loss: 3.2877888679504395
Validation loss: 2.654512923250916

Epoch: 28| Step: 0
Training loss: 2.810321807861328
Validation loss: 2.639802799429945

Epoch: 6| Step: 1
Training loss: 2.81547212600708
Validation loss: 2.6318355708993892

Epoch: 6| Step: 2
Training loss: 2.5908749103546143
Validation loss: 2.6184798902080906

Epoch: 6| Step: 3
Training loss: 2.8525092601776123
Validation loss: 2.61703299706982

Epoch: 6| Step: 4
Training loss: 2.311286449432373
Validation loss: 2.618591667503439

Epoch: 6| Step: 5
Training loss: 3.1220037937164307
Validation loss: 2.619546628767444

Epoch: 6| Step: 6
Training loss: 3.4839272499084473
Validation loss: 2.62191576855157

Epoch: 6| Step: 7
Training loss: 3.074564218521118
Validation loss: 2.6220987625019525

Epoch: 6| Step: 8
Training loss: 2.8773112297058105
Validation loss: 2.623010435412007

Epoch: 6| Step: 9
Training loss: 3.220841646194458
Validation loss: 2.6334500543532835

Epoch: 6| Step: 10
Training loss: 2.65651273727417
Validation loss: 2.617602509836997

Epoch: 6| Step: 11
Training loss: 2.2895474433898926
Validation loss: 2.617972716208427

Epoch: 6| Step: 12
Training loss: 2.9017486572265625
Validation loss: 2.6177559565472346

Epoch: 6| Step: 13
Training loss: 1.98550546169281
Validation loss: 2.6143350985742386

Epoch: 29| Step: 0
Training loss: 2.3850274085998535
Validation loss: 2.612221189724502

Epoch: 6| Step: 1
Training loss: 2.5349624156951904
Validation loss: 2.6183916573883383

Epoch: 6| Step: 2
Training loss: 2.133406400680542
Validation loss: 2.632830563411918

Epoch: 6| Step: 3
Training loss: 3.0949037075042725
Validation loss: 2.6433247443168395

Epoch: 6| Step: 4
Training loss: 2.738264560699463
Validation loss: 2.646808498649187

Epoch: 6| Step: 5
Training loss: 3.1706788539886475
Validation loss: 2.644326074149019

Epoch: 6| Step: 6
Training loss: 2.603945255279541
Validation loss: 2.6485910210558163

Epoch: 6| Step: 7
Training loss: 3.1276659965515137
Validation loss: 2.669266418744159

Epoch: 6| Step: 8
Training loss: 3.55401611328125
Validation loss: 2.659191834029331

Epoch: 6| Step: 9
Training loss: 3.032571792602539
Validation loss: 2.6239244963533137

Epoch: 6| Step: 10
Training loss: 2.591533660888672
Validation loss: 2.610568787461968

Epoch: 6| Step: 11
Training loss: 2.876049041748047
Validation loss: 2.601742652154738

Epoch: 6| Step: 12
Training loss: 2.667236566543579
Validation loss: 2.603459850434334

Epoch: 6| Step: 13
Training loss: 2.513336658477783
Validation loss: 2.6107194603130384

Epoch: 30| Step: 0
Training loss: 2.759227752685547
Validation loss: 2.6189979301985873

Epoch: 6| Step: 1
Training loss: 2.9446911811828613
Validation loss: 2.6318894637528287

Epoch: 6| Step: 2
Training loss: 2.184471607208252
Validation loss: 2.642920001860588

Epoch: 6| Step: 3
Training loss: 3.037620782852173
Validation loss: 2.662508023682461

Epoch: 6| Step: 4
Training loss: 3.4788155555725098
Validation loss: 2.6510972105046755

Epoch: 6| Step: 5
Training loss: 2.058112859725952
Validation loss: 2.644514665808729

Epoch: 6| Step: 6
Training loss: 2.7286853790283203
Validation loss: 2.6100359821832306

Epoch: 6| Step: 7
Training loss: 2.8121800422668457
Validation loss: 2.5919597712896203

Epoch: 6| Step: 8
Training loss: 3.337512731552124
Validation loss: 2.5957469530003046

Epoch: 6| Step: 9
Training loss: 2.2506494522094727
Validation loss: 2.6295425558602936

Epoch: 6| Step: 10
Training loss: 3.0469512939453125
Validation loss: 2.6449666177072833

Epoch: 6| Step: 11
Training loss: 3.426363945007324
Validation loss: 2.660589959031792

Epoch: 6| Step: 12
Training loss: 2.155003309249878
Validation loss: 2.6246103343143257

Epoch: 6| Step: 13
Training loss: 3.4746170043945312
Validation loss: 2.5967251844303583

Epoch: 31| Step: 0
Training loss: 3.507502794265747
Validation loss: 2.5824548711058912

Epoch: 6| Step: 1
Training loss: 3.183474063873291
Validation loss: 2.583847920099894

Epoch: 6| Step: 2
Training loss: 2.8467421531677246
Validation loss: 2.5804847389139156

Epoch: 6| Step: 3
Training loss: 2.180548667907715
Validation loss: 2.5822607855642996

Epoch: 6| Step: 4
Training loss: 2.977571487426758
Validation loss: 2.5812794636654597

Epoch: 6| Step: 5
Training loss: 2.728353500366211
Validation loss: 2.5849888042737077

Epoch: 6| Step: 6
Training loss: 2.6670942306518555
Validation loss: 2.5780737015508834

Epoch: 6| Step: 7
Training loss: 1.4771337509155273
Validation loss: 2.5776215240519535

Epoch: 6| Step: 8
Training loss: 2.599616050720215
Validation loss: 2.583512636923021

Epoch: 6| Step: 9
Training loss: 3.130190849304199
Validation loss: 2.586815413608346

Epoch: 6| Step: 10
Training loss: 2.8594517707824707
Validation loss: 2.584316184443812

Epoch: 6| Step: 11
Training loss: 2.918497085571289
Validation loss: 2.573767840221364

Epoch: 6| Step: 12
Training loss: 3.060690402984619
Validation loss: 2.5751516536999772

Epoch: 6| Step: 13
Training loss: 2.533508062362671
Validation loss: 2.572456932836963

Epoch: 32| Step: 0
Training loss: 2.800168514251709
Validation loss: 2.575099250321747

Epoch: 6| Step: 1
Training loss: 2.547420024871826
Validation loss: 2.579303523545624

Epoch: 6| Step: 2
Training loss: 2.96681547164917
Validation loss: 2.5783966561799407

Epoch: 6| Step: 3
Training loss: 3.334775686264038
Validation loss: 2.5750204311904086

Epoch: 6| Step: 4
Training loss: 2.6996941566467285
Validation loss: 2.575105982442056

Epoch: 6| Step: 5
Training loss: 3.11909818649292
Validation loss: 2.5731158461622012

Epoch: 6| Step: 6
Training loss: 2.886075019836426
Validation loss: 2.5711587500828568

Epoch: 6| Step: 7
Training loss: 2.6768226623535156
Validation loss: 2.570194887858565

Epoch: 6| Step: 8
Training loss: 2.322953701019287
Validation loss: 2.5684116168688704

Epoch: 6| Step: 9
Training loss: 3.1703076362609863
Validation loss: 2.567550746343469

Epoch: 6| Step: 10
Training loss: 2.301840305328369
Validation loss: 2.5728195251957064

Epoch: 6| Step: 11
Training loss: 2.3408024311065674
Validation loss: 2.571214258029897

Epoch: 6| Step: 12
Training loss: 2.825040578842163
Validation loss: 2.5611318952293805

Epoch: 6| Step: 13
Training loss: 2.62304425239563
Validation loss: 2.5631864070892334

Epoch: 33| Step: 0
Training loss: 2.6780314445495605
Validation loss: 2.5587237932348765

Epoch: 6| Step: 1
Training loss: 2.9797377586364746
Validation loss: 2.560970421760313

Epoch: 6| Step: 2
Training loss: 3.2137627601623535
Validation loss: 2.557149458956975

Epoch: 6| Step: 3
Training loss: 2.271139621734619
Validation loss: 2.564482335121401

Epoch: 6| Step: 4
Training loss: 2.0225934982299805
Validation loss: 2.56498638532495

Epoch: 6| Step: 5
Training loss: 2.8043031692504883
Validation loss: 2.5632801260999454

Epoch: 6| Step: 6
Training loss: 3.2317585945129395
Validation loss: 2.5614609949050413

Epoch: 6| Step: 7
Training loss: 2.1720376014709473
Validation loss: 2.5633481087223178

Epoch: 6| Step: 8
Training loss: 1.9796394109725952
Validation loss: 2.560855939824094

Epoch: 6| Step: 9
Training loss: 3.2662196159362793
Validation loss: 2.5576213559796734

Epoch: 6| Step: 10
Training loss: 3.0878617763519287
Validation loss: 2.5568332338845856

Epoch: 6| Step: 11
Training loss: 2.3113536834716797
Validation loss: 2.5546767275820494

Epoch: 6| Step: 12
Training loss: 3.7489676475524902
Validation loss: 2.5531292294943206

Epoch: 6| Step: 13
Training loss: 2.6631271839141846
Validation loss: 2.554489284433344

Epoch: 34| Step: 0
Training loss: 2.2395739555358887
Validation loss: 2.552076170521398

Epoch: 6| Step: 1
Training loss: 2.1559691429138184
Validation loss: 2.5559758524740896

Epoch: 6| Step: 2
Training loss: 3.722262382507324
Validation loss: 2.558335465769614

Epoch: 6| Step: 3
Training loss: 3.107052803039551
Validation loss: 2.5586981696467244

Epoch: 6| Step: 4
Training loss: 3.0721237659454346
Validation loss: 2.5687956066541773

Epoch: 6| Step: 5
Training loss: 2.9723198413848877
Validation loss: 2.5833488356682563

Epoch: 6| Step: 6
Training loss: 2.792182445526123
Validation loss: 2.5755890005378315

Epoch: 6| Step: 7
Training loss: 2.6161692142486572
Validation loss: 2.5593544488312094

Epoch: 6| Step: 8
Training loss: 2.7528889179229736
Validation loss: 2.5520836281520065

Epoch: 6| Step: 9
Training loss: 2.617446184158325
Validation loss: 2.5478092316658265

Epoch: 6| Step: 10
Training loss: 2.632950782775879
Validation loss: 2.547610359807168

Epoch: 6| Step: 11
Training loss: 2.5688867568969727
Validation loss: 2.541751295007685

Epoch: 6| Step: 12
Training loss: 2.524646282196045
Validation loss: 2.542232387809343

Epoch: 6| Step: 13
Training loss: 2.4844582080841064
Validation loss: 2.541501634864397

Epoch: 35| Step: 0
Training loss: 3.0345458984375
Validation loss: 2.541119329390987

Epoch: 6| Step: 1
Training loss: 3.282618999481201
Validation loss: 2.5444994664961293

Epoch: 6| Step: 2
Training loss: 2.067373514175415
Validation loss: 2.5409052551433606

Epoch: 6| Step: 3
Training loss: 3.074017286300659
Validation loss: 2.5503156108240925

Epoch: 6| Step: 4
Training loss: 2.490171432495117
Validation loss: 2.5655487327165503

Epoch: 6| Step: 5
Training loss: 1.8960261344909668
Validation loss: 2.573880551963724

Epoch: 6| Step: 6
Training loss: 2.5990562438964844
Validation loss: 2.559171197234943

Epoch: 6| Step: 7
Training loss: 3.845653533935547
Validation loss: 2.553159067707677

Epoch: 6| Step: 8
Training loss: 2.668485164642334
Validation loss: 2.5442718280259

Epoch: 6| Step: 9
Training loss: 3.162191390991211
Validation loss: 2.5406240083838023

Epoch: 6| Step: 10
Training loss: 2.4412155151367188
Validation loss: 2.535318200306226

Epoch: 6| Step: 11
Training loss: 2.471956729888916
Validation loss: 2.5347670714060464

Epoch: 6| Step: 12
Training loss: 3.0175390243530273
Validation loss: 2.5522061342834146

Epoch: 6| Step: 13
Training loss: 2.2605698108673096
Validation loss: 2.5658268595254548

Epoch: 36| Step: 0
Training loss: 2.225200653076172
Validation loss: 2.579749479088732

Epoch: 6| Step: 1
Training loss: 3.0675301551818848
Validation loss: 2.5626778474418064

Epoch: 6| Step: 2
Training loss: 2.668900489807129
Validation loss: 2.538381968775103

Epoch: 6| Step: 3
Training loss: 2.0132176876068115
Validation loss: 2.5319236837407595

Epoch: 6| Step: 4
Training loss: 3.067305564880371
Validation loss: 2.530234972635905

Epoch: 6| Step: 5
Training loss: 2.3586714267730713
Validation loss: 2.5345767082706576

Epoch: 6| Step: 6
Training loss: 2.014885425567627
Validation loss: 2.531544536672613

Epoch: 6| Step: 7
Training loss: 3.6239678859710693
Validation loss: 2.5295805520908807

Epoch: 6| Step: 8
Training loss: 2.6226351261138916
Validation loss: 2.5297812492616716

Epoch: 6| Step: 9
Training loss: 2.8731577396392822
Validation loss: 2.5360224631524857

Epoch: 6| Step: 10
Training loss: 3.4534106254577637
Validation loss: 2.5368375188560894

Epoch: 6| Step: 11
Training loss: 2.6844513416290283
Validation loss: 2.5375046576223066

Epoch: 6| Step: 12
Training loss: 3.2513585090637207
Validation loss: 2.539455424072922

Epoch: 6| Step: 13
Training loss: 2.0830729007720947
Validation loss: 2.53889863721786

Epoch: 37| Step: 0
Training loss: 2.981264591217041
Validation loss: 2.5353225354225404

Epoch: 6| Step: 1
Training loss: 2.5243422985076904
Validation loss: 2.527557114119171

Epoch: 6| Step: 2
Training loss: 2.6201248168945312
Validation loss: 2.5376949361575547

Epoch: 6| Step: 3
Training loss: 2.1859097480773926
Validation loss: 2.5654630327737458

Epoch: 6| Step: 4
Training loss: 2.5209314823150635
Validation loss: 2.5684609028600875

Epoch: 6| Step: 5
Training loss: 2.693286418914795
Validation loss: 2.571054776509603

Epoch: 6| Step: 6
Training loss: 3.181398391723633
Validation loss: 2.5745783467446604

Epoch: 6| Step: 7
Training loss: 2.2945876121520996
Validation loss: 2.568696447598037

Epoch: 6| Step: 8
Training loss: 3.257685899734497
Validation loss: 2.5698893095857356

Epoch: 6| Step: 9
Training loss: 3.188735008239746
Validation loss: 2.560603495567076

Epoch: 6| Step: 10
Training loss: 3.0689120292663574
Validation loss: 2.5452930004365983

Epoch: 6| Step: 11
Training loss: 2.699629068374634
Validation loss: 2.5440869408269084

Epoch: 6| Step: 12
Training loss: 2.8753445148468018
Validation loss: 2.536777719374626

Epoch: 6| Step: 13
Training loss: 1.695889949798584
Validation loss: 2.5250621380344516

Epoch: 38| Step: 0
Training loss: 2.836324691772461
Validation loss: 2.5174948118066274

Epoch: 6| Step: 1
Training loss: 3.177319049835205
Validation loss: 2.5209209662611767

Epoch: 6| Step: 2
Training loss: 3.0467238426208496
Validation loss: 2.5290345761083786

Epoch: 6| Step: 3
Training loss: 2.290926933288574
Validation loss: 2.529901119970506

Epoch: 6| Step: 4
Training loss: 3.53812837600708
Validation loss: 2.531488351924445

Epoch: 6| Step: 5
Training loss: 2.856397867202759
Validation loss: 2.531867363119638

Epoch: 6| Step: 6
Training loss: 3.1432530879974365
Validation loss: 2.529810062018774

Epoch: 6| Step: 7
Training loss: 2.459628105163574
Validation loss: 2.524181837676674

Epoch: 6| Step: 8
Training loss: 2.9144911766052246
Validation loss: 2.520011204545216

Epoch: 6| Step: 9
Training loss: 2.187100887298584
Validation loss: 2.513220166647306

Epoch: 6| Step: 10
Training loss: 2.536137580871582
Validation loss: 2.512664633412515

Epoch: 6| Step: 11
Training loss: 1.843586802482605
Validation loss: 2.5046501915941954

Epoch: 6| Step: 12
Training loss: 2.6951770782470703
Validation loss: 2.5083072698244484

Epoch: 6| Step: 13
Training loss: 2.6079931259155273
Validation loss: 2.511358384163149

Epoch: 39| Step: 0
Training loss: 2.583329677581787
Validation loss: 2.508476298342469

Epoch: 6| Step: 1
Training loss: 2.5571577548980713
Validation loss: 2.504590562594834

Epoch: 6| Step: 2
Training loss: 2.3406779766082764
Validation loss: 2.4971089529734787

Epoch: 6| Step: 3
Training loss: 2.0915894508361816
Validation loss: 2.5000770809829875

Epoch: 6| Step: 4
Training loss: 2.4484355449676514
Validation loss: 2.4894776139208066

Epoch: 6| Step: 5
Training loss: 2.8729920387268066
Validation loss: 2.4838970938036518

Epoch: 6| Step: 6
Training loss: 3.1488943099975586
Validation loss: 2.485376916905885

Epoch: 6| Step: 7
Training loss: 3.4315385818481445
Validation loss: 2.4935596014863703

Epoch: 6| Step: 8
Training loss: 2.743074417114258
Validation loss: 2.4894322913180114

Epoch: 6| Step: 9
Training loss: 3.0693166255950928
Validation loss: 2.490392941300587

Epoch: 6| Step: 10
Training loss: 3.3461036682128906
Validation loss: 2.491580911861953

Epoch: 6| Step: 11
Training loss: 3.0232973098754883
Validation loss: 2.4921579002052225

Epoch: 6| Step: 12
Training loss: 1.751575231552124
Validation loss: 2.4910832835781958

Epoch: 6| Step: 13
Training loss: 2.224066734313965
Validation loss: 2.4842313412697083

Epoch: 40| Step: 0
Training loss: 3.129136562347412
Validation loss: 2.474816437690489

Epoch: 6| Step: 1
Training loss: 2.1388792991638184
Validation loss: 2.4765144983927407

Epoch: 6| Step: 2
Training loss: 2.2790157794952393
Validation loss: 2.4793279029989757

Epoch: 6| Step: 3
Training loss: 3.2144246101379395
Validation loss: 2.483217829017229

Epoch: 6| Step: 4
Training loss: 3.269257068634033
Validation loss: 2.473599395444316

Epoch: 6| Step: 5
Training loss: 2.5582621097564697
Validation loss: 2.4698519065815914

Epoch: 6| Step: 6
Training loss: 2.355633020401001
Validation loss: 2.465476876945906

Epoch: 6| Step: 7
Training loss: 2.898688316345215
Validation loss: 2.4694903922337357

Epoch: 6| Step: 8
Training loss: 2.595547676086426
Validation loss: 2.470962598759641

Epoch: 6| Step: 9
Training loss: 2.3731322288513184
Validation loss: 2.4738232166536394

Epoch: 6| Step: 10
Training loss: 2.6519904136657715
Validation loss: 2.4716337496234524

Epoch: 6| Step: 11
Training loss: 2.951237678527832
Validation loss: 2.471685436464125

Epoch: 6| Step: 12
Training loss: 2.9140377044677734
Validation loss: 2.475759408807242

Epoch: 6| Step: 13
Training loss: 1.9732680320739746
Validation loss: 2.478734129218645

Epoch: 41| Step: 0
Training loss: 2.7612133026123047
Validation loss: 2.5027670424471617

Epoch: 6| Step: 1
Training loss: 3.0157103538513184
Validation loss: 2.5036539954523884

Epoch: 6| Step: 2
Training loss: 2.934788227081299
Validation loss: 2.4884169896443686

Epoch: 6| Step: 3
Training loss: 2.8559250831604004
Validation loss: 2.4896037552946355

Epoch: 6| Step: 4
Training loss: 2.370422840118408
Validation loss: 2.475614563111336

Epoch: 6| Step: 5
Training loss: 3.587451934814453
Validation loss: 2.4811527344488327

Epoch: 6| Step: 6
Training loss: 3.1599111557006836
Validation loss: 2.4778761427889586

Epoch: 6| Step: 7
Training loss: 2.187346935272217
Validation loss: 2.4724521329326015

Epoch: 6| Step: 8
Training loss: 1.573440670967102
Validation loss: 2.468531398363011

Epoch: 6| Step: 9
Training loss: 2.831228733062744
Validation loss: 2.4670271206927556

Epoch: 6| Step: 10
Training loss: 2.9510226249694824
Validation loss: 2.4565953439281834

Epoch: 6| Step: 11
Training loss: 2.4806711673736572
Validation loss: 2.4603532462991695

Epoch: 6| Step: 12
Training loss: 2.094730854034424
Validation loss: 2.459687932845085

Epoch: 6| Step: 13
Training loss: 3.1790971755981445
Validation loss: 2.4627828649295274

Epoch: 42| Step: 0
Training loss: 2.95438814163208
Validation loss: 2.4627521935329644

Epoch: 6| Step: 1
Training loss: 2.6813132762908936
Validation loss: 2.4626200737491732

Epoch: 6| Step: 2
Training loss: 2.7636351585388184
Validation loss: 2.4572590627977924

Epoch: 6| Step: 3
Training loss: 2.824490547180176
Validation loss: 2.4732683576563352

Epoch: 6| Step: 4
Training loss: 3.656909942626953
Validation loss: 2.487450615052254

Epoch: 6| Step: 5
Training loss: 2.2134029865264893
Validation loss: 2.5160915774683796

Epoch: 6| Step: 6
Training loss: 2.059704542160034
Validation loss: 2.5217584128020913

Epoch: 6| Step: 7
Training loss: 3.126124382019043
Validation loss: 2.5292888661866546

Epoch: 6| Step: 8
Training loss: 2.4228925704956055
Validation loss: 2.488682900705645

Epoch: 6| Step: 9
Training loss: 2.555471420288086
Validation loss: 2.4695941145702074

Epoch: 6| Step: 10
Training loss: 2.9388694763183594
Validation loss: 2.4618803762620494

Epoch: 6| Step: 11
Training loss: 2.2417008876800537
Validation loss: 2.4585899973428376

Epoch: 6| Step: 12
Training loss: 2.3546085357666016
Validation loss: 2.4610526433555027

Epoch: 6| Step: 13
Training loss: 2.9774770736694336
Validation loss: 2.4603949464777464

Epoch: 43| Step: 0
Training loss: 2.77834415435791
Validation loss: 2.46286375804614

Epoch: 6| Step: 1
Training loss: 3.1028292179107666
Validation loss: 2.4689606184600503

Epoch: 6| Step: 2
Training loss: 2.7923343181610107
Validation loss: 2.4780617272982033

Epoch: 6| Step: 3
Training loss: 2.6310341358184814
Validation loss: 2.4811605099708802

Epoch: 6| Step: 4
Training loss: 2.2518222332000732
Validation loss: 2.5097583904061267

Epoch: 6| Step: 5
Training loss: 2.4762539863586426
Validation loss: 2.503637795807213

Epoch: 6| Step: 6
Training loss: 3.1019811630249023
Validation loss: 2.482666333516439

Epoch: 6| Step: 7
Training loss: 1.6942954063415527
Validation loss: 2.472267594388736

Epoch: 6| Step: 8
Training loss: 2.869652032852173
Validation loss: 2.467512199955602

Epoch: 6| Step: 9
Training loss: 2.678924560546875
Validation loss: 2.4637031657721407

Epoch: 6| Step: 10
Training loss: 2.740208625793457
Validation loss: 2.467240374575379

Epoch: 6| Step: 11
Training loss: 2.5589847564697266
Validation loss: 2.4635094827221287

Epoch: 6| Step: 12
Training loss: 3.504451274871826
Validation loss: 2.471052053154156

Epoch: 6| Step: 13
Training loss: 2.682558298110962
Validation loss: 2.472020940114093

Epoch: 44| Step: 0
Training loss: 2.592273712158203
Validation loss: 2.483355245282573

Epoch: 6| Step: 1
Training loss: 2.9085960388183594
Validation loss: 2.5063652966612127

Epoch: 6| Step: 2
Training loss: 3.556400775909424
Validation loss: 2.49848755457068

Epoch: 6| Step: 3
Training loss: 2.780022144317627
Validation loss: 2.4843115755306777

Epoch: 6| Step: 4
Training loss: 2.407529830932617
Validation loss: 2.4665281977704776

Epoch: 6| Step: 5
Training loss: 1.9150340557098389
Validation loss: 2.450467819808632

Epoch: 6| Step: 6
Training loss: 2.080414295196533
Validation loss: 2.4504582138471704

Epoch: 6| Step: 7
Training loss: 3.8345999717712402
Validation loss: 2.447506950747582

Epoch: 6| Step: 8
Training loss: 1.9523550271987915
Validation loss: 2.4457557983295892

Epoch: 6| Step: 9
Training loss: 2.555807590484619
Validation loss: 2.4417873121077016

Epoch: 6| Step: 10
Training loss: 2.677396059036255
Validation loss: 2.45282216482265

Epoch: 6| Step: 11
Training loss: 3.2249412536621094
Validation loss: 2.459890414309758

Epoch: 6| Step: 12
Training loss: 2.3455958366394043
Validation loss: 2.4488659648485083

Epoch: 6| Step: 13
Training loss: 2.7743680477142334
Validation loss: 2.4534247664995092

Epoch: 45| Step: 0
Training loss: 2.5980420112609863
Validation loss: 2.4766394758737214

Epoch: 6| Step: 1
Training loss: 3.277566432952881
Validation loss: 2.514766687987953

Epoch: 6| Step: 2
Training loss: 3.147029399871826
Validation loss: 2.5802630506536013

Epoch: 6| Step: 3
Training loss: 2.832273244857788
Validation loss: 2.6087884108225503

Epoch: 6| Step: 4
Training loss: 2.927896499633789
Validation loss: 2.602132043530864

Epoch: 6| Step: 5
Training loss: 2.5896852016448975
Validation loss: 2.5601969431805354

Epoch: 6| Step: 6
Training loss: 1.9054028987884521
Validation loss: 2.507100262949544

Epoch: 6| Step: 7
Training loss: 2.7050795555114746
Validation loss: 2.489508431444886

Epoch: 6| Step: 8
Training loss: 2.5306265354156494
Validation loss: 2.4771335970970894

Epoch: 6| Step: 9
Training loss: 2.6017632484436035
Validation loss: 2.48122218860093

Epoch: 6| Step: 10
Training loss: 2.4940261840820312
Validation loss: 2.4975738422845

Epoch: 6| Step: 11
Training loss: 3.0047645568847656
Validation loss: 2.511900594157557

Epoch: 6| Step: 12
Training loss: 2.2586095333099365
Validation loss: 2.5113494626937376

Epoch: 6| Step: 13
Training loss: 3.2244434356689453
Validation loss: 2.4926694875122397

Epoch: 46| Step: 0
Training loss: 2.748962640762329
Validation loss: 2.4631033456453713

Epoch: 6| Step: 1
Training loss: 2.501305103302002
Validation loss: 2.4760180134927072

Epoch: 6| Step: 2
Training loss: 2.96854829788208
Validation loss: 2.4868548736777356

Epoch: 6| Step: 3
Training loss: 2.4567158222198486
Validation loss: 2.4692538630577827

Epoch: 6| Step: 4
Training loss: 2.945275068283081
Validation loss: 2.4603876401019353

Epoch: 6| Step: 5
Training loss: 2.889749765396118
Validation loss: 2.452295390508508

Epoch: 6| Step: 6
Training loss: 2.9078965187072754
Validation loss: 2.4466703707172024

Epoch: 6| Step: 7
Training loss: 2.7369565963745117
Validation loss: 2.4425558018428024

Epoch: 6| Step: 8
Training loss: 2.448031187057495
Validation loss: 2.440230251640402

Epoch: 6| Step: 9
Training loss: 2.7454681396484375
Validation loss: 2.4387188778128674

Epoch: 6| Step: 10
Training loss: 2.6497130393981934
Validation loss: 2.4338755351240917

Epoch: 6| Step: 11
Training loss: 2.150446653366089
Validation loss: 2.433571133562314

Epoch: 6| Step: 12
Training loss: 2.0276968479156494
Validation loss: 2.4384003531548286

Epoch: 6| Step: 13
Training loss: 3.9408798217773438
Validation loss: 2.4381198626692577

Epoch: 47| Step: 0
Training loss: 3.0425777435302734
Validation loss: 2.436760838313769

Epoch: 6| Step: 1
Training loss: 2.900394916534424
Validation loss: 2.4385404356064333

Epoch: 6| Step: 2
Training loss: 2.368807077407837
Validation loss: 2.4426411300577144

Epoch: 6| Step: 3
Training loss: 2.567774772644043
Validation loss: 2.4635664660443544

Epoch: 6| Step: 4
Training loss: 2.807356119155884
Validation loss: 2.4993861541953137

Epoch: 6| Step: 5
Training loss: 2.7393882274627686
Validation loss: 2.4682570375421995

Epoch: 6| Step: 6
Training loss: 2.727003812789917
Validation loss: 2.4499076938116424

Epoch: 6| Step: 7
Training loss: 2.472649574279785
Validation loss: 2.430307662615212

Epoch: 6| Step: 8
Training loss: 2.8540377616882324
Validation loss: 2.427446401247414

Epoch: 6| Step: 9
Training loss: 2.1548666954040527
Validation loss: 2.4295439822699434

Epoch: 6| Step: 10
Training loss: 2.562300205230713
Validation loss: 2.4272325731092885

Epoch: 6| Step: 11
Training loss: 2.9940383434295654
Validation loss: 2.4192158022234516

Epoch: 6| Step: 12
Training loss: 2.5611016750335693
Validation loss: 2.4169234024581088

Epoch: 6| Step: 13
Training loss: 2.6051340103149414
Validation loss: 2.4200885424049954

Epoch: 48| Step: 0
Training loss: 2.5835957527160645
Validation loss: 2.420563272250596

Epoch: 6| Step: 1
Training loss: 3.0952630043029785
Validation loss: 2.4402547805540022

Epoch: 6| Step: 2
Training loss: 2.4785232543945312
Validation loss: 2.437240485222109

Epoch: 6| Step: 3
Training loss: 2.7113380432128906
Validation loss: 2.441443602244059

Epoch: 6| Step: 4
Training loss: 3.0602619647979736
Validation loss: 2.440529315702377

Epoch: 6| Step: 5
Training loss: 2.1470229625701904
Validation loss: 2.4575115814003894

Epoch: 6| Step: 6
Training loss: 2.555166006088257
Validation loss: 2.475568594471101

Epoch: 6| Step: 7
Training loss: 2.414583683013916
Validation loss: 2.473031759262085

Epoch: 6| Step: 8
Training loss: 2.9935598373413086
Validation loss: 2.4766109989535425

Epoch: 6| Step: 9
Training loss: 2.273844003677368
Validation loss: 2.4498924875772126

Epoch: 6| Step: 10
Training loss: 3.0645267963409424
Validation loss: 2.4246803150382092

Epoch: 6| Step: 11
Training loss: 2.3487420082092285
Validation loss: 2.4192740814660185

Epoch: 6| Step: 12
Training loss: 2.7910497188568115
Validation loss: 2.4155540491945002

Epoch: 6| Step: 13
Training loss: 2.9132542610168457
Validation loss: 2.4079666599150626

Epoch: 49| Step: 0
Training loss: 2.371890068054199
Validation loss: 2.4112738640077653

Epoch: 6| Step: 1
Training loss: 2.244109869003296
Validation loss: 2.4088531783832017

Epoch: 6| Step: 2
Training loss: 3.88871431350708
Validation loss: 2.4138610414279404

Epoch: 6| Step: 3
Training loss: 2.6032824516296387
Validation loss: 2.4120616784659763

Epoch: 6| Step: 4
Training loss: 2.428114891052246
Validation loss: 2.413863184631512

Epoch: 6| Step: 5
Training loss: 2.5546960830688477
Validation loss: 2.407776545452815

Epoch: 6| Step: 6
Training loss: 3.1443276405334473
Validation loss: 2.410170873006185

Epoch: 6| Step: 7
Training loss: 3.006793260574341
Validation loss: 2.4205947486303185

Epoch: 6| Step: 8
Training loss: 2.2390217781066895
Validation loss: 2.408245801925659

Epoch: 6| Step: 9
Training loss: 2.0063657760620117
Validation loss: 2.411635239919027

Epoch: 6| Step: 10
Training loss: 2.7249579429626465
Validation loss: 2.4123109694450133

Epoch: 6| Step: 11
Training loss: 2.529118299484253
Validation loss: 2.4112604484763196

Epoch: 6| Step: 12
Training loss: 2.826807737350464
Validation loss: 2.4102147343338176

Epoch: 6| Step: 13
Training loss: 2.4989356994628906
Validation loss: 2.4119760605596725

Epoch: 50| Step: 0
Training loss: 2.2007718086242676
Validation loss: 2.4266493705011185

Epoch: 6| Step: 1
Training loss: 2.8743042945861816
Validation loss: 2.4615541427366194

Epoch: 6| Step: 2
Training loss: 2.043212413787842
Validation loss: 2.481002840944516

Epoch: 6| Step: 3
Training loss: 3.1273698806762695
Validation loss: 2.4637565535883748

Epoch: 6| Step: 4
Training loss: 2.635706663131714
Validation loss: 2.414912898053405

Epoch: 6| Step: 5
Training loss: 2.9789609909057617
Validation loss: 2.403387710612307

Epoch: 6| Step: 6
Training loss: 2.166717529296875
Validation loss: 2.413584734803887

Epoch: 6| Step: 7
Training loss: 3.2034404277801514
Validation loss: 2.434648116429647

Epoch: 6| Step: 8
Training loss: 2.4291832447052
Validation loss: 2.4686761107496036

Epoch: 6| Step: 9
Training loss: 2.1159286499023438
Validation loss: 2.468234057067543

Epoch: 6| Step: 10
Training loss: 2.886654853820801
Validation loss: 2.4511077788568314

Epoch: 6| Step: 11
Training loss: 3.2995805740356445
Validation loss: 2.4313122764710458

Epoch: 6| Step: 12
Training loss: 2.7253246307373047
Validation loss: 2.4247827555543635

Epoch: 6| Step: 13
Training loss: 2.8512322902679443
Validation loss: 2.4197292225335234

Epoch: 51| Step: 0
Training loss: 2.449727773666382
Validation loss: 2.4199474985881517

Epoch: 6| Step: 1
Training loss: 2.0269625186920166
Validation loss: 2.423550005882017

Epoch: 6| Step: 2
Training loss: 3.1144237518310547
Validation loss: 2.433661876186248

Epoch: 6| Step: 3
Training loss: 2.8244009017944336
Validation loss: 2.435187703819685

Epoch: 6| Step: 4
Training loss: 2.833301544189453
Validation loss: 2.4257383526012464

Epoch: 6| Step: 5
Training loss: 3.134492874145508
Validation loss: 2.4171349028105378

Epoch: 6| Step: 6
Training loss: 2.855551242828369
Validation loss: 2.4072343918585006

Epoch: 6| Step: 7
Training loss: 3.345362424850464
Validation loss: 2.4041539263981644

Epoch: 6| Step: 8
Training loss: 2.780482292175293
Validation loss: 2.4001225809897146

Epoch: 6| Step: 9
Training loss: 1.927314281463623
Validation loss: 2.4036381898387784

Epoch: 6| Step: 10
Training loss: 2.724797010421753
Validation loss: 2.4050438173355593

Epoch: 6| Step: 11
Training loss: 2.3251709938049316
Validation loss: 2.3975828719395462

Epoch: 6| Step: 12
Training loss: 2.3758440017700195
Validation loss: 2.396658651290401

Epoch: 6| Step: 13
Training loss: 2.432492733001709
Validation loss: 2.399035511478301

Epoch: 52| Step: 0
Training loss: 2.776639223098755
Validation loss: 2.4280838633096344

Epoch: 6| Step: 1
Training loss: 2.636385202407837
Validation loss: 2.4480842134003997

Epoch: 6| Step: 2
Training loss: 2.276623249053955
Validation loss: 2.4718021808132047

Epoch: 6| Step: 3
Training loss: 2.1315455436706543
Validation loss: 2.4485375983740694

Epoch: 6| Step: 4
Training loss: 2.2967381477355957
Validation loss: 2.435892694739885

Epoch: 6| Step: 5
Training loss: 3.4725069999694824
Validation loss: 2.4031364097390124

Epoch: 6| Step: 6
Training loss: 2.5676751136779785
Validation loss: 2.391305290242677

Epoch: 6| Step: 7
Training loss: 3.0945181846618652
Validation loss: 2.386354415647445

Epoch: 6| Step: 8
Training loss: 2.6131606101989746
Validation loss: 2.3932265312440935

Epoch: 6| Step: 9
Training loss: 2.9715335369110107
Validation loss: 2.402913813949913

Epoch: 6| Step: 10
Training loss: 2.2947943210601807
Validation loss: 2.4070193280455885

Epoch: 6| Step: 11
Training loss: 2.5085911750793457
Validation loss: 2.4130087026985745

Epoch: 6| Step: 12
Training loss: 2.9315860271453857
Validation loss: 2.4129248152496996

Epoch: 6| Step: 13
Training loss: 2.926168203353882
Validation loss: 2.408674970749886

Epoch: 53| Step: 0
Training loss: 2.0642518997192383
Validation loss: 2.4057331803024455

Epoch: 6| Step: 1
Training loss: 2.5151758193969727
Validation loss: 2.4013915882315686

Epoch: 6| Step: 2
Training loss: 3.3062243461608887
Validation loss: 2.3940621370910318

Epoch: 6| Step: 3
Training loss: 2.787447214126587
Validation loss: 2.3945419301268873

Epoch: 6| Step: 4
Training loss: 3.2693512439727783
Validation loss: 2.390046706763647

Epoch: 6| Step: 5
Training loss: 2.211299180984497
Validation loss: 2.3855915018307265

Epoch: 6| Step: 6
Training loss: 2.9365267753601074
Validation loss: 2.381927749162079

Epoch: 6| Step: 7
Training loss: 2.3025197982788086
Validation loss: 2.3802063362572783

Epoch: 6| Step: 8
Training loss: 2.6545333862304688
Validation loss: 2.38015047196419

Epoch: 6| Step: 9
Training loss: 2.7870664596557617
Validation loss: 2.389963380752071

Epoch: 6| Step: 10
Training loss: 2.384915351867676
Validation loss: 2.3890767738383305

Epoch: 6| Step: 11
Training loss: 2.439249038696289
Validation loss: 2.4111627301862164

Epoch: 6| Step: 12
Training loss: 2.345351457595825
Validation loss: 2.4161361148280482

Epoch: 6| Step: 13
Training loss: 3.3028504848480225
Validation loss: 2.432588529843156

Epoch: 54| Step: 0
Training loss: 2.4261226654052734
Validation loss: 2.449053579761136

Epoch: 6| Step: 1
Training loss: 3.017353057861328
Validation loss: 2.443311719484227

Epoch: 6| Step: 2
Training loss: 2.5237503051757812
Validation loss: 2.42646118902391

Epoch: 6| Step: 3
Training loss: 3.2317755222320557
Validation loss: 2.405875700776295

Epoch: 6| Step: 4
Training loss: 2.2133991718292236
Validation loss: 2.3955760950683267

Epoch: 6| Step: 5
Training loss: 3.1894407272338867
Validation loss: 2.3902972590538765

Epoch: 6| Step: 6
Training loss: 2.4256606101989746
Validation loss: 2.3866034887170278

Epoch: 6| Step: 7
Training loss: 2.6447510719299316
Validation loss: 2.386935726288826

Epoch: 6| Step: 8
Training loss: 2.153289794921875
Validation loss: 2.38096373568299

Epoch: 6| Step: 9
Training loss: 3.495751142501831
Validation loss: 2.380705520670901

Epoch: 6| Step: 10
Training loss: 2.63921856880188
Validation loss: 2.38095514235958

Epoch: 6| Step: 11
Training loss: 2.5022289752960205
Validation loss: 2.3812630330362627

Epoch: 6| Step: 12
Training loss: 2.237917900085449
Validation loss: 2.3867355418461624

Epoch: 6| Step: 13
Training loss: 2.1728098392486572
Validation loss: 2.4038640222241803

Epoch: 55| Step: 0
Training loss: 3.0380589962005615
Validation loss: 2.405040139793068

Epoch: 6| Step: 1
Training loss: 2.5641098022460938
Validation loss: 2.4106236811607116

Epoch: 6| Step: 2
Training loss: 2.4374942779541016
Validation loss: 2.4158701025029665

Epoch: 6| Step: 3
Training loss: 3.806387186050415
Validation loss: 2.4209737944346603

Epoch: 6| Step: 4
Training loss: 2.7268195152282715
Validation loss: 2.4123728095844226

Epoch: 6| Step: 5
Training loss: 2.104659080505371
Validation loss: 2.3996292724404285

Epoch: 6| Step: 6
Training loss: 2.676581859588623
Validation loss: 2.37355266463372

Epoch: 6| Step: 7
Training loss: 2.9033803939819336
Validation loss: 2.3714474965167303

Epoch: 6| Step: 8
Training loss: 2.2034618854522705
Validation loss: 2.3702923764464674

Epoch: 6| Step: 9
Training loss: 2.435220718383789
Validation loss: 2.3641166430647655

Epoch: 6| Step: 10
Training loss: 2.0054070949554443
Validation loss: 2.3751594917748564

Epoch: 6| Step: 11
Training loss: 2.583036422729492
Validation loss: 2.369939816895352

Epoch: 6| Step: 12
Training loss: 2.318228244781494
Validation loss: 2.369086137381933

Epoch: 6| Step: 13
Training loss: 3.7404305934906006
Validation loss: 2.3703991264425297

Epoch: 56| Step: 0
Training loss: 3.1098389625549316
Validation loss: 2.371037160196612

Epoch: 6| Step: 1
Training loss: 2.0851333141326904
Validation loss: 2.36876662828589

Epoch: 6| Step: 2
Training loss: 2.7323503494262695
Validation loss: 2.376936676681683

Epoch: 6| Step: 3
Training loss: 2.223001480102539
Validation loss: 2.386345253195814

Epoch: 6| Step: 4
Training loss: 2.7358593940734863
Validation loss: 2.388318505338443

Epoch: 6| Step: 5
Training loss: 2.7410807609558105
Validation loss: 2.3914577589240125

Epoch: 6| Step: 6
Training loss: 3.4427547454833984
Validation loss: 2.396584928676646

Epoch: 6| Step: 7
Training loss: 3.3512487411499023
Validation loss: 2.408414553570491

Epoch: 6| Step: 8
Training loss: 1.7485593557357788
Validation loss: 2.4098400864549863

Epoch: 6| Step: 9
Training loss: 2.7416303157806396
Validation loss: 2.398117096193375

Epoch: 6| Step: 10
Training loss: 2.599583148956299
Validation loss: 2.4165129559014433

Epoch: 6| Step: 11
Training loss: 2.0864992141723633
Validation loss: 2.4300576230531097

Epoch: 6| Step: 12
Training loss: 2.171344757080078
Validation loss: 2.3939183911969586

Epoch: 6| Step: 13
Training loss: 3.340846061706543
Validation loss: 2.38244157068191

Epoch: 57| Step: 0
Training loss: 2.365370512008667
Validation loss: 2.369776584768808

Epoch: 6| Step: 1
Training loss: 2.855950117111206
Validation loss: 2.369993132929648

Epoch: 6| Step: 2
Training loss: 2.7133917808532715
Validation loss: 2.3636107137126308

Epoch: 6| Step: 3
Training loss: 2.8732757568359375
Validation loss: 2.363550745030885

Epoch: 6| Step: 4
Training loss: 3.014342784881592
Validation loss: 2.361542610711949

Epoch: 6| Step: 5
Training loss: 2.4213056564331055
Validation loss: 2.3599146514810543

Epoch: 6| Step: 6
Training loss: 3.145007371902466
Validation loss: 2.365819984866727

Epoch: 6| Step: 7
Training loss: 2.8992812633514404
Validation loss: 2.3752515085281862

Epoch: 6| Step: 8
Training loss: 2.4257335662841797
Validation loss: 2.367987576351371

Epoch: 6| Step: 9
Training loss: 2.658457040786743
Validation loss: 2.3756585608246508

Epoch: 6| Step: 10
Training loss: 2.4259591102600098
Validation loss: 2.3733580881549465

Epoch: 6| Step: 11
Training loss: 2.3460941314697266
Validation loss: 2.371309265013664

Epoch: 6| Step: 12
Training loss: 2.6527748107910156
Validation loss: 2.3707338712548696

Epoch: 6| Step: 13
Training loss: 1.4577442407608032
Validation loss: 2.3737277394981793

Epoch: 58| Step: 0
Training loss: 2.6809134483337402
Validation loss: 2.3738432468906527

Epoch: 6| Step: 1
Training loss: 2.0453717708587646
Validation loss: 2.3942651300020117

Epoch: 6| Step: 2
Training loss: 3.1298604011535645
Validation loss: 2.3894149770018873

Epoch: 6| Step: 3
Training loss: 1.8602195978164673
Validation loss: 2.3939286496049617

Epoch: 6| Step: 4
Training loss: 2.5320568084716797
Validation loss: 2.3873293476720012

Epoch: 6| Step: 5
Training loss: 2.7640368938446045
Validation loss: 2.391624007173764

Epoch: 6| Step: 6
Training loss: 2.657200813293457
Validation loss: 2.3767327467600503

Epoch: 6| Step: 7
Training loss: 2.4390735626220703
Validation loss: 2.3626492126013643

Epoch: 6| Step: 8
Training loss: 3.0653891563415527
Validation loss: 2.35851764422591

Epoch: 6| Step: 9
Training loss: 3.047574043273926
Validation loss: 2.3510161292168403

Epoch: 6| Step: 10
Training loss: 2.0404675006866455
Validation loss: 2.3453586063077374

Epoch: 6| Step: 11
Training loss: 3.0074527263641357
Validation loss: 2.3421601992781445

Epoch: 6| Step: 12
Training loss: 2.9421675205230713
Validation loss: 2.347419156823107

Epoch: 6| Step: 13
Training loss: 2.2156295776367188
Validation loss: 2.35181531342127

Epoch: 59| Step: 0
Training loss: 3.1830129623413086
Validation loss: 2.3690483403462235

Epoch: 6| Step: 1
Training loss: 2.5401883125305176
Validation loss: 2.375722962041055

Epoch: 6| Step: 2
Training loss: 2.6418092250823975
Validation loss: 2.3855138542831584

Epoch: 6| Step: 3
Training loss: 3.2478713989257812
Validation loss: 2.3945397689778316

Epoch: 6| Step: 4
Training loss: 2.4693803787231445
Validation loss: 2.3798080234117407

Epoch: 6| Step: 5
Training loss: 2.7459189891815186
Validation loss: 2.372822884590395

Epoch: 6| Step: 6
Training loss: 2.013885736465454
Validation loss: 2.3589887542109333

Epoch: 6| Step: 7
Training loss: 2.909905433654785
Validation loss: 2.350524502415811

Epoch: 6| Step: 8
Training loss: 2.681933879852295
Validation loss: 2.3466980559851534

Epoch: 6| Step: 9
Training loss: 2.428650379180908
Validation loss: 2.3398618031573553

Epoch: 6| Step: 10
Training loss: 2.074310302734375
Validation loss: 2.346136813522667

Epoch: 6| Step: 11
Training loss: 2.436187267303467
Validation loss: 2.3490675546789683

Epoch: 6| Step: 12
Training loss: 2.9036550521850586
Validation loss: 2.352410767668037

Epoch: 6| Step: 13
Training loss: 2.457482099533081
Validation loss: 2.3679578150472333

Epoch: 60| Step: 0
Training loss: 3.679381847381592
Validation loss: 2.4443569926805395

Epoch: 6| Step: 1
Training loss: 2.8496150970458984
Validation loss: 2.526861088250273

Epoch: 6| Step: 2
Training loss: 2.9205827713012695
Validation loss: 2.527754409338838

Epoch: 6| Step: 3
Training loss: 2.3844120502471924
Validation loss: 2.454548115371376

Epoch: 6| Step: 4
Training loss: 2.7651920318603516
Validation loss: 2.414835678633823

Epoch: 6| Step: 5
Training loss: 2.6552982330322266
Validation loss: 2.3797249819642756

Epoch: 6| Step: 6
Training loss: 3.018030881881714
Validation loss: 2.356142241467712

Epoch: 6| Step: 7
Training loss: 3.367729663848877
Validation loss: 2.340419464213874

Epoch: 6| Step: 8
Training loss: 2.3037919998168945
Validation loss: 2.3343302819036666

Epoch: 6| Step: 9
Training loss: 1.8929455280303955
Validation loss: 2.332057450407295

Epoch: 6| Step: 10
Training loss: 1.9414111375808716
Validation loss: 2.3393631058354534

Epoch: 6| Step: 11
Training loss: 2.216836929321289
Validation loss: 2.3464563610733196

Epoch: 6| Step: 12
Training loss: 2.7187259197235107
Validation loss: 2.349938864349037

Epoch: 6| Step: 13
Training loss: 2.1618802547454834
Validation loss: 2.354677782263807

Epoch: 61| Step: 0
Training loss: 2.576939344406128
Validation loss: 2.363282729220647

Epoch: 6| Step: 1
Training loss: 2.06872820854187
Validation loss: 2.3749708103877243

Epoch: 6| Step: 2
Training loss: 3.3127601146698
Validation loss: 2.369755509079144

Epoch: 6| Step: 3
Training loss: 2.9320287704467773
Validation loss: 2.3658112659249255

Epoch: 6| Step: 4
Training loss: 2.3298354148864746
Validation loss: 2.3595397087835495

Epoch: 6| Step: 5
Training loss: 2.364858627319336
Validation loss: 2.35855092540864

Epoch: 6| Step: 6
Training loss: 2.409928798675537
Validation loss: 2.3530874290773944

Epoch: 6| Step: 7
Training loss: 2.5983786582946777
Validation loss: 2.346750651636431

Epoch: 6| Step: 8
Training loss: 3.114114284515381
Validation loss: 2.329725150139101

Epoch: 6| Step: 9
Training loss: 2.8751797676086426
Validation loss: 2.3278324860398487

Epoch: 6| Step: 10
Training loss: 3.0438625812530518
Validation loss: 2.326727910708356

Epoch: 6| Step: 11
Training loss: 2.172515869140625
Validation loss: 2.3283480444262104

Epoch: 6| Step: 12
Training loss: 2.506402015686035
Validation loss: 2.345618345404184

Epoch: 6| Step: 13
Training loss: 2.161677122116089
Validation loss: 2.355238737598542

Epoch: 62| Step: 0
Training loss: 2.2285022735595703
Validation loss: 2.364292408830376

Epoch: 6| Step: 1
Training loss: 2.567732334136963
Validation loss: 2.361336451704784

Epoch: 6| Step: 2
Training loss: 3.152292013168335
Validation loss: 2.3676481657130743

Epoch: 6| Step: 3
Training loss: 2.8091721534729004
Validation loss: 2.3526237882593626

Epoch: 6| Step: 4
Training loss: 3.109464645385742
Validation loss: 2.341319194404028

Epoch: 6| Step: 5
Training loss: 2.8774237632751465
Validation loss: 2.345247337895055

Epoch: 6| Step: 6
Training loss: 2.592855215072632
Validation loss: 2.3408748334453953

Epoch: 6| Step: 7
Training loss: 2.5183286666870117
Validation loss: 2.3326092253449144

Epoch: 6| Step: 8
Training loss: 2.420283794403076
Validation loss: 2.330505335202781

Epoch: 6| Step: 9
Training loss: 2.3247718811035156
Validation loss: 2.3295208587441394

Epoch: 6| Step: 10
Training loss: 2.3966574668884277
Validation loss: 2.3274118464480162

Epoch: 6| Step: 11
Training loss: 2.382032871246338
Validation loss: 2.332869199014479

Epoch: 6| Step: 12
Training loss: 2.3397088050842285
Validation loss: 2.3285160013424453

Epoch: 6| Step: 13
Training loss: 2.8399922847747803
Validation loss: 2.33107990090565

Epoch: 63| Step: 0
Training loss: 2.0681991577148438
Validation loss: 2.3319430351257324

Epoch: 6| Step: 1
Training loss: 2.409423589706421
Validation loss: 2.3414216054383146

Epoch: 6| Step: 2
Training loss: 2.397373676300049
Validation loss: 2.35078163044427

Epoch: 6| Step: 3
Training loss: 3.0021491050720215
Validation loss: 2.361924858503444

Epoch: 6| Step: 4
Training loss: 2.516724109649658
Validation loss: 2.365419364744617

Epoch: 6| Step: 5
Training loss: 2.007617950439453
Validation loss: 2.34812528856339

Epoch: 6| Step: 6
Training loss: 2.1943938732147217
Validation loss: 2.3498576046318136

Epoch: 6| Step: 7
Training loss: 2.951019763946533
Validation loss: 2.3522047176155993

Epoch: 6| Step: 8
Training loss: 2.7062811851501465
Validation loss: 2.3835492595549552

Epoch: 6| Step: 9
Training loss: 2.951241970062256
Validation loss: 2.4329309258409726

Epoch: 6| Step: 10
Training loss: 2.5223093032836914
Validation loss: 2.403767913900396

Epoch: 6| Step: 11
Training loss: 3.1112537384033203
Validation loss: 2.3841063643014557

Epoch: 6| Step: 12
Training loss: 2.963371515274048
Validation loss: 2.3545505128880984

Epoch: 6| Step: 13
Training loss: 3.2526865005493164
Validation loss: 2.330820360491353

Epoch: 64| Step: 0
Training loss: 2.3513214588165283
Validation loss: 2.333324627209735

Epoch: 6| Step: 1
Training loss: 2.3516056537628174
Validation loss: 2.327310092987553

Epoch: 6| Step: 2
Training loss: 2.486199140548706
Validation loss: 2.3300039768218994

Epoch: 6| Step: 3
Training loss: 2.39898681640625
Validation loss: 2.3343449228553363

Epoch: 6| Step: 4
Training loss: 2.858184814453125
Validation loss: 2.3419485886891684

Epoch: 6| Step: 5
Training loss: 3.518557071685791
Validation loss: 2.3355455065286286

Epoch: 6| Step: 6
Training loss: 2.6449108123779297
Validation loss: 2.3286603984012397

Epoch: 6| Step: 7
Training loss: 2.1364357471466064
Validation loss: 2.3222141394051175

Epoch: 6| Step: 8
Training loss: 2.180220603942871
Validation loss: 2.3242455502992034

Epoch: 6| Step: 9
Training loss: 2.365399122238159
Validation loss: 2.3178259275292836

Epoch: 6| Step: 10
Training loss: 2.748807907104492
Validation loss: 2.323836124071511

Epoch: 6| Step: 11
Training loss: 2.7519359588623047
Validation loss: 2.322153068357898

Epoch: 6| Step: 12
Training loss: 2.674668073654175
Validation loss: 2.32168746507296

Epoch: 6| Step: 13
Training loss: 3.040631055831909
Validation loss: 2.3222897257856143

Epoch: 65| Step: 0
Training loss: 2.7889041900634766
Validation loss: 2.318751952981436

Epoch: 6| Step: 1
Training loss: 2.9930949211120605
Validation loss: 2.319232071599653

Epoch: 6| Step: 2
Training loss: 3.346207618713379
Validation loss: 2.3212912505672825

Epoch: 6| Step: 3
Training loss: 1.843827247619629
Validation loss: 2.320955188043656

Epoch: 6| Step: 4
Training loss: 2.279392719268799
Validation loss: 2.3254996204888947

Epoch: 6| Step: 5
Training loss: 2.8363633155822754
Validation loss: 2.3227986712609567

Epoch: 6| Step: 6
Training loss: 3.053349256515503
Validation loss: 2.319254477818807

Epoch: 6| Step: 7
Training loss: 2.2019200325012207
Validation loss: 2.31628647927315

Epoch: 6| Step: 8
Training loss: 3.4421777725219727
Validation loss: 2.3222756719076507

Epoch: 6| Step: 9
Training loss: 2.395596981048584
Validation loss: 2.3138797872809955

Epoch: 6| Step: 10
Training loss: 2.5826146602630615
Validation loss: 2.308481590722197

Epoch: 6| Step: 11
Training loss: 1.617692470550537
Validation loss: 2.3042404497823408

Epoch: 6| Step: 12
Training loss: 2.452962636947632
Validation loss: 2.3102771312959733

Epoch: 6| Step: 13
Training loss: 2.2582406997680664
Validation loss: 2.306066273361124

Epoch: 66| Step: 0
Training loss: 2.6954431533813477
Validation loss: 2.30437941961391

Epoch: 6| Step: 1
Training loss: 2.2141311168670654
Validation loss: 2.3052888608747915

Epoch: 6| Step: 2
Training loss: 2.444638729095459
Validation loss: 2.3056345037234727

Epoch: 6| Step: 3
Training loss: 2.915780544281006
Validation loss: 2.314762674352174

Epoch: 6| Step: 4
Training loss: 2.828634738922119
Validation loss: 2.303609609603882

Epoch: 6| Step: 5
Training loss: 2.968376636505127
Validation loss: 2.3121134722104637

Epoch: 6| Step: 6
Training loss: 2.4906322956085205
Validation loss: 2.310152430688181

Epoch: 6| Step: 7
Training loss: 1.8226737976074219
Validation loss: 2.3117774840324157

Epoch: 6| Step: 8
Training loss: 2.623650550842285
Validation loss: 2.320427658737347

Epoch: 6| Step: 9
Training loss: 2.35032320022583
Validation loss: 2.318942639135545

Epoch: 6| Step: 10
Training loss: 2.925614356994629
Validation loss: 2.3104437551190777

Epoch: 6| Step: 11
Training loss: 3.2180557250976562
Validation loss: 2.308043331228277

Epoch: 6| Step: 12
Training loss: 2.717214584350586
Validation loss: 2.3022546640006443

Epoch: 6| Step: 13
Training loss: 1.5383775234222412
Validation loss: 2.30112620066571

Epoch: 67| Step: 0
Training loss: 2.7875640392303467
Validation loss: 2.2947083391169065

Epoch: 6| Step: 1
Training loss: 2.4257192611694336
Validation loss: 2.2900045610243276

Epoch: 6| Step: 2
Training loss: 2.805290699005127
Validation loss: 2.2931968576164654

Epoch: 6| Step: 3
Training loss: 2.589869737625122
Validation loss: 2.2854176669992428

Epoch: 6| Step: 4
Training loss: 1.6461528539657593
Validation loss: 2.2816971117450344

Epoch: 6| Step: 5
Training loss: 3.078770875930786
Validation loss: 2.2854797429935907

Epoch: 6| Step: 6
Training loss: 2.7671589851379395
Validation loss: 2.2840766368373746

Epoch: 6| Step: 7
Training loss: 2.272010326385498
Validation loss: 2.2970107960444626

Epoch: 6| Step: 8
Training loss: 2.6200265884399414
Validation loss: 2.2980869585467922

Epoch: 6| Step: 9
Training loss: 2.6538915634155273
Validation loss: 2.295452123047203

Epoch: 6| Step: 10
Training loss: 2.265233039855957
Validation loss: 2.2842201238037436

Epoch: 6| Step: 11
Training loss: 3.0269875526428223
Validation loss: 2.2899353901545205

Epoch: 6| Step: 12
Training loss: 2.30299711227417
Validation loss: 2.295981717366044

Epoch: 6| Step: 13
Training loss: 2.8979668617248535
Validation loss: 2.319039370424004

Epoch: 68| Step: 0
Training loss: 2.2577309608459473
Validation loss: 2.332731508439587

Epoch: 6| Step: 1
Training loss: 2.669268846511841
Validation loss: 2.3564424950589418

Epoch: 6| Step: 2
Training loss: 2.912729024887085
Validation loss: 2.3567839514824653

Epoch: 6| Step: 3
Training loss: 2.461636543273926
Validation loss: 2.3557997493333716

Epoch: 6| Step: 4
Training loss: 2.596949577331543
Validation loss: 2.3265119560303225

Epoch: 6| Step: 5
Training loss: 2.1615049839019775
Validation loss: 2.302065000739149

Epoch: 6| Step: 6
Training loss: 2.5194642543792725
Validation loss: 2.280559150121545

Epoch: 6| Step: 7
Training loss: 3.0746660232543945
Validation loss: 2.276650946627381

Epoch: 6| Step: 8
Training loss: 2.289477586746216
Validation loss: 2.2723264796759493

Epoch: 6| Step: 9
Training loss: 2.3622612953186035
Validation loss: 2.2670692602793374

Epoch: 6| Step: 10
Training loss: 2.921518325805664
Validation loss: 2.2757837810823993

Epoch: 6| Step: 11
Training loss: 2.151996612548828
Validation loss: 2.2761391542291127

Epoch: 6| Step: 12
Training loss: 3.1380085945129395
Validation loss: 2.27950277379764

Epoch: 6| Step: 13
Training loss: 2.8893144130706787
Validation loss: 2.2733665845727407

Epoch: 69| Step: 0
Training loss: 2.378997802734375
Validation loss: 2.2747025951262443

Epoch: 6| Step: 1
Training loss: 2.27254056930542
Validation loss: 2.2733509156011764

Epoch: 6| Step: 2
Training loss: 2.2335150241851807
Validation loss: 2.2712538037248837

Epoch: 6| Step: 3
Training loss: 2.6194825172424316
Validation loss: 2.2839531808771114

Epoch: 6| Step: 4
Training loss: 1.6078591346740723
Validation loss: 2.2978527263928483

Epoch: 6| Step: 5
Training loss: 2.7657530307769775
Validation loss: 2.2910274190287434

Epoch: 6| Step: 6
Training loss: 2.5003578662872314
Validation loss: 2.3026354851261264

Epoch: 6| Step: 7
Training loss: 2.211137294769287
Validation loss: 2.291040866605697

Epoch: 6| Step: 8
Training loss: 2.6169514656066895
Validation loss: 2.2930089645488287

Epoch: 6| Step: 9
Training loss: 3.201986312866211
Validation loss: 2.3010598921006724

Epoch: 6| Step: 10
Training loss: 2.62960147857666
Validation loss: 2.292499162817514

Epoch: 6| Step: 11
Training loss: 3.4489870071411133
Validation loss: 2.28736005034498

Epoch: 6| Step: 12
Training loss: 2.7584028244018555
Validation loss: 2.2885233971380416

Epoch: 6| Step: 13
Training loss: 2.885117292404175
Validation loss: 2.2864585717519126

Epoch: 70| Step: 0
Training loss: 2.202836751937866
Validation loss: 2.2767659592372116

Epoch: 6| Step: 1
Training loss: 2.9387664794921875
Validation loss: 2.273371095298439

Epoch: 6| Step: 2
Training loss: 2.5621604919433594
Validation loss: 2.275090488054419

Epoch: 6| Step: 3
Training loss: 2.9958698749542236
Validation loss: 2.268166272870956

Epoch: 6| Step: 4
Training loss: 1.9097023010253906
Validation loss: 2.2699939486800984

Epoch: 6| Step: 5
Training loss: 2.520620346069336
Validation loss: 2.2696231449804

Epoch: 6| Step: 6
Training loss: 3.40885591506958
Validation loss: 2.2686459505429832

Epoch: 6| Step: 7
Training loss: 1.9698123931884766
Validation loss: 2.2692803618728474

Epoch: 6| Step: 8
Training loss: 2.6092758178710938
Validation loss: 2.2670313119888306

Epoch: 6| Step: 9
Training loss: 1.916025161743164
Validation loss: 2.2629054438683296

Epoch: 6| Step: 10
Training loss: 2.605701208114624
Validation loss: 2.265810058962914

Epoch: 6| Step: 11
Training loss: 2.8441200256347656
Validation loss: 2.261713594518682

Epoch: 6| Step: 12
Training loss: 2.4568123817443848
Validation loss: 2.26131784531378

Epoch: 6| Step: 13
Training loss: 2.9281537532806396
Validation loss: 2.265461573036768

Epoch: 71| Step: 0
Training loss: 2.6284735202789307
Validation loss: 2.2688339474380657

Epoch: 6| Step: 1
Training loss: 2.889836311340332
Validation loss: 2.274940252304077

Epoch: 6| Step: 2
Training loss: 2.021177291870117
Validation loss: 2.276521505848054

Epoch: 6| Step: 3
Training loss: 2.361614942550659
Validation loss: 2.2818302544214393

Epoch: 6| Step: 4
Training loss: 3.0230183601379395
Validation loss: 2.276122672583467

Epoch: 6| Step: 5
Training loss: 2.5936293601989746
Validation loss: 2.2731764585741105

Epoch: 6| Step: 6
Training loss: 2.1730823516845703
Validation loss: 2.2746554779750046

Epoch: 6| Step: 7
Training loss: 3.0644114017486572
Validation loss: 2.2786339829044957

Epoch: 6| Step: 8
Training loss: 2.4783315658569336
Validation loss: 2.2832052451308056

Epoch: 6| Step: 9
Training loss: 2.542325019836426
Validation loss: 2.2732030755730084

Epoch: 6| Step: 10
Training loss: 3.065962314605713
Validation loss: 2.271379678480087

Epoch: 6| Step: 11
Training loss: 1.9299057722091675
Validation loss: 2.2611972798583326

Epoch: 6| Step: 12
Training loss: 1.866185188293457
Validation loss: 2.2603027282222623

Epoch: 6| Step: 13
Training loss: 3.094693660736084
Validation loss: 2.2630584983415503

Epoch: 72| Step: 0
Training loss: 2.3893280029296875
Validation loss: 2.2646236342768513

Epoch: 6| Step: 1
Training loss: 2.141679525375366
Validation loss: 2.2510471536267187

Epoch: 6| Step: 2
Training loss: 3.316007137298584
Validation loss: 2.2509660131187847

Epoch: 6| Step: 3
Training loss: 2.5193686485290527
Validation loss: 2.243306703464959

Epoch: 6| Step: 4
Training loss: 2.8754496574401855
Validation loss: 2.2517191722828853

Epoch: 6| Step: 5
Training loss: 2.4502058029174805
Validation loss: 2.237482581087338

Epoch: 6| Step: 6
Training loss: 3.167050361633301
Validation loss: 2.233626655353013

Epoch: 6| Step: 7
Training loss: 2.6651320457458496
Validation loss: 2.2309407470046834

Epoch: 6| Step: 8
Training loss: 1.7355587482452393
Validation loss: 2.225462405912338

Epoch: 6| Step: 9
Training loss: 2.234424591064453
Validation loss: 2.226183306786322

Epoch: 6| Step: 10
Training loss: 2.862732410430908
Validation loss: 2.2303132472499723

Epoch: 6| Step: 11
Training loss: 3.0568485260009766
Validation loss: 2.2330656743818715

Epoch: 6| Step: 12
Training loss: 1.326291799545288
Validation loss: 2.24066323618735

Epoch: 6| Step: 13
Training loss: 2.6063756942749023
Validation loss: 2.2520755285857827

Epoch: 73| Step: 0
Training loss: 2.8034591674804688
Validation loss: 2.263067304447133

Epoch: 6| Step: 1
Training loss: 1.8816055059432983
Validation loss: 2.2682397160478818

Epoch: 6| Step: 2
Training loss: 3.2259721755981445
Validation loss: 2.2751389857261413

Epoch: 6| Step: 3
Training loss: 2.6674818992614746
Validation loss: 2.267254903752317

Epoch: 6| Step: 4
Training loss: 2.687255382537842
Validation loss: 2.264673791905885

Epoch: 6| Step: 5
Training loss: 2.9714605808258057
Validation loss: 2.251276910945933

Epoch: 6| Step: 6
Training loss: 2.3704471588134766
Validation loss: 2.24380769524523

Epoch: 6| Step: 7
Training loss: 2.9605367183685303
Validation loss: 2.236805708177628

Epoch: 6| Step: 8
Training loss: 1.6927986145019531
Validation loss: 2.2279117979029173

Epoch: 6| Step: 9
Training loss: 2.448664665222168
Validation loss: 2.220084996633632

Epoch: 6| Step: 10
Training loss: 1.9526207447052002
Validation loss: 2.224429484336607

Epoch: 6| Step: 11
Training loss: 2.1984643936157227
Validation loss: 2.2298230663422616

Epoch: 6| Step: 12
Training loss: 2.501838445663452
Validation loss: 2.263722285147636

Epoch: 6| Step: 13
Training loss: 3.46136736869812
Validation loss: 2.2738350411897064

Epoch: 74| Step: 0
Training loss: 3.1021664142608643
Validation loss: 2.277227353024226

Epoch: 6| Step: 1
Training loss: 2.632269859313965
Validation loss: 2.2431989664672525

Epoch: 6| Step: 2
Training loss: 2.3224496841430664
Validation loss: 2.2400667936571184

Epoch: 6| Step: 3
Training loss: 2.3208250999450684
Validation loss: 2.2163002260269655

Epoch: 6| Step: 4
Training loss: 2.7873659133911133
Validation loss: 2.2029707765066497

Epoch: 6| Step: 5
Training loss: 2.4322285652160645
Validation loss: 2.2017631159033826

Epoch: 6| Step: 6
Training loss: 2.933476686477661
Validation loss: 2.2035502285085697

Epoch: 6| Step: 7
Training loss: 2.0541281700134277
Validation loss: 2.2080836270445134

Epoch: 6| Step: 8
Training loss: 1.8579148054122925
Validation loss: 2.2037083102810766

Epoch: 6| Step: 9
Training loss: 2.610046148300171
Validation loss: 2.1969510919304303

Epoch: 6| Step: 10
Training loss: 2.620875835418701
Validation loss: 2.196401365341679

Epoch: 6| Step: 11
Training loss: 2.555112838745117
Validation loss: 2.207167926655021

Epoch: 6| Step: 12
Training loss: 2.5029077529907227
Validation loss: 2.2150388712524087

Epoch: 6| Step: 13
Training loss: 2.551786184310913
Validation loss: 2.265083505261329

Epoch: 75| Step: 0
Training loss: 2.2738301753997803
Validation loss: 2.3103519844752487

Epoch: 6| Step: 1
Training loss: 2.077609062194824
Validation loss: 2.3843954711832027

Epoch: 6| Step: 2
Training loss: 3.3397786617279053
Validation loss: 2.4812599100092405

Epoch: 6| Step: 3
Training loss: 2.1516880989074707
Validation loss: 2.474920670191447

Epoch: 6| Step: 4
Training loss: 2.0760390758514404
Validation loss: 2.4854296920120076

Epoch: 6| Step: 5
Training loss: 3.3321242332458496
Validation loss: 2.440193601833877

Epoch: 6| Step: 6
Training loss: 2.2989449501037598
Validation loss: 2.3603724356620543

Epoch: 6| Step: 7
Training loss: 2.8260297775268555
Validation loss: 2.2961547605453

Epoch: 6| Step: 8
Training loss: 1.84369957447052
Validation loss: 2.247744414114183

Epoch: 6| Step: 9
Training loss: 2.5519232749938965
Validation loss: 2.2176863198639243

Epoch: 6| Step: 10
Training loss: 2.5480546951293945
Validation loss: 2.2064261641553653

Epoch: 6| Step: 11
Training loss: 2.318794012069702
Validation loss: 2.220485261691514

Epoch: 6| Step: 12
Training loss: 3.5281734466552734
Validation loss: 2.221760501143753

Epoch: 6| Step: 13
Training loss: 2.331193685531616
Validation loss: 2.2289364337921143

Epoch: 76| Step: 0
Training loss: 2.045999765396118
Validation loss: 2.2435367850847143

Epoch: 6| Step: 1
Training loss: 2.543492078781128
Validation loss: 2.2186064233062086

Epoch: 6| Step: 2
Training loss: 2.7231087684631348
Validation loss: 2.2143494800854753

Epoch: 6| Step: 3
Training loss: 2.387083053588867
Validation loss: 2.2117254041856333

Epoch: 6| Step: 4
Training loss: 2.9241671562194824
Validation loss: 2.2143945693969727

Epoch: 6| Step: 5
Training loss: 2.896648406982422
Validation loss: 2.214274275687433

Epoch: 6| Step: 6
Training loss: 3.348137140274048
Validation loss: 2.2122199227732997

Epoch: 6| Step: 7
Training loss: 1.6154346466064453
Validation loss: 2.2078230201557116

Epoch: 6| Step: 8
Training loss: 2.5719313621520996
Validation loss: 2.2117179491186656

Epoch: 6| Step: 9
Training loss: 2.162245750427246
Validation loss: 2.223107873752553

Epoch: 6| Step: 10
Training loss: 2.3477697372436523
Validation loss: 2.230947991853119

Epoch: 6| Step: 11
Training loss: 2.6821670532226562
Validation loss: 2.254841127703267

Epoch: 6| Step: 12
Training loss: 2.2275137901306152
Validation loss: 2.2432255642388457

Epoch: 6| Step: 13
Training loss: 2.8431928157806396
Validation loss: 2.2175212842161938

Epoch: 77| Step: 0
Training loss: 2.2020509243011475
Validation loss: 2.2047968987495667

Epoch: 6| Step: 1
Training loss: 2.7464003562927246
Validation loss: 2.194096211464174

Epoch: 6| Step: 2
Training loss: 2.638606071472168
Validation loss: 2.1831628596910866

Epoch: 6| Step: 3
Training loss: 2.9524550437927246
Validation loss: 2.1874695977857037

Epoch: 6| Step: 4
Training loss: 1.6662347316741943
Validation loss: 2.1938910279222714

Epoch: 6| Step: 5
Training loss: 2.258119583129883
Validation loss: 2.1866160643998014

Epoch: 6| Step: 6
Training loss: 3.0207746028900146
Validation loss: 2.1856273399886263

Epoch: 6| Step: 7
Training loss: 2.752136707305908
Validation loss: 2.1877377366506927

Epoch: 6| Step: 8
Training loss: 2.543088436126709
Validation loss: 2.186328952030469

Epoch: 6| Step: 9
Training loss: 2.242321014404297
Validation loss: 2.184554138491231

Epoch: 6| Step: 10
Training loss: 2.282858371734619
Validation loss: 2.191543427846765

Epoch: 6| Step: 11
Training loss: 2.741121768951416
Validation loss: 2.1955895039343063

Epoch: 6| Step: 12
Training loss: 3.1219024658203125
Validation loss: 2.20237672457131

Epoch: 6| Step: 13
Training loss: 1.5187335014343262
Validation loss: 2.231039665078604

Epoch: 78| Step: 0
Training loss: 2.4651193618774414
Validation loss: 2.2434457271329817

Epoch: 6| Step: 1
Training loss: 3.112370014190674
Validation loss: 2.276397899914813

Epoch: 6| Step: 2
Training loss: 2.0855586528778076
Validation loss: 2.27534588690727

Epoch: 6| Step: 3
Training loss: 2.2007298469543457
Validation loss: 2.2415337203651347

Epoch: 6| Step: 4
Training loss: 2.5302700996398926
Validation loss: 2.242633717034453

Epoch: 6| Step: 5
Training loss: 2.4526491165161133
Validation loss: 2.223586026058402

Epoch: 6| Step: 6
Training loss: 2.8309502601623535
Validation loss: 2.224112146644182

Epoch: 6| Step: 7
Training loss: 2.2753567695617676
Validation loss: 2.2225636410456833

Epoch: 6| Step: 8
Training loss: 2.3574395179748535
Validation loss: 2.2094513421417563

Epoch: 6| Step: 9
Training loss: 2.688876152038574
Validation loss: 2.2149391123043594

Epoch: 6| Step: 10
Training loss: 2.296264171600342
Validation loss: 2.2158183897695234

Epoch: 6| Step: 11
Training loss: 2.7627956867218018
Validation loss: 2.2077526930839784

Epoch: 6| Step: 12
Training loss: 1.9879744052886963
Validation loss: 2.2254276839635705

Epoch: 6| Step: 13
Training loss: 3.5150134563446045
Validation loss: 2.250457530380577

Epoch: 79| Step: 0
Training loss: 1.7235300540924072
Validation loss: 2.2484104658967707

Epoch: 6| Step: 1
Training loss: 2.617361068725586
Validation loss: 2.260465142547443

Epoch: 6| Step: 2
Training loss: 2.185091495513916
Validation loss: 2.2734376871457664

Epoch: 6| Step: 3
Training loss: 2.429133653640747
Validation loss: 2.3124959904660463

Epoch: 6| Step: 4
Training loss: 2.5294792652130127
Validation loss: 2.313211118021319

Epoch: 6| Step: 5
Training loss: 2.526549816131592
Validation loss: 2.2715681957942184

Epoch: 6| Step: 6
Training loss: 2.9457170963287354
Validation loss: 2.2689813875382945

Epoch: 6| Step: 7
Training loss: 2.239569902420044
Validation loss: 2.251415237303703

Epoch: 6| Step: 8
Training loss: 2.8046929836273193
Validation loss: 2.2345001671903875

Epoch: 6| Step: 9
Training loss: 2.1405277252197266
Validation loss: 2.2345217658627416

Epoch: 6| Step: 10
Training loss: 2.5832791328430176
Validation loss: 2.221862462259108

Epoch: 6| Step: 11
Training loss: 2.918513536453247
Validation loss: 2.2253558379347607

Epoch: 6| Step: 12
Training loss: 2.7920427322387695
Validation loss: 2.20886504778298

Epoch: 6| Step: 13
Training loss: 2.5108511447906494
Validation loss: 2.195491883062547

Epoch: 80| Step: 0
Training loss: 2.7477622032165527
Validation loss: 2.1746707347131546

Epoch: 6| Step: 1
Training loss: 2.747910976409912
Validation loss: 2.17266781099381

Epoch: 6| Step: 2
Training loss: 2.26668643951416
Validation loss: 2.1894633257260887

Epoch: 6| Step: 3
Training loss: 2.2862019538879395
Validation loss: 2.197709460412302

Epoch: 6| Step: 4
Training loss: 2.1256332397460938
Validation loss: 2.2048331588827152

Epoch: 6| Step: 5
Training loss: 2.4710965156555176
Validation loss: 2.193808824785294

Epoch: 6| Step: 6
Training loss: 2.475548267364502
Validation loss: 2.1759436656070013

Epoch: 6| Step: 7
Training loss: 2.3248744010925293
Validation loss: 2.157460751072053

Epoch: 6| Step: 8
Training loss: 2.01108980178833
Validation loss: 2.143254710781959

Epoch: 6| Step: 9
Training loss: 2.523134231567383
Validation loss: 2.140078242107104

Epoch: 6| Step: 10
Training loss: 2.3273820877075195
Validation loss: 2.1456465003310994

Epoch: 6| Step: 11
Training loss: 2.7415616512298584
Validation loss: 2.1623345933934695

Epoch: 6| Step: 12
Training loss: 3.0032553672790527
Validation loss: 2.206779815817392

Epoch: 6| Step: 13
Training loss: 3.0214641094207764
Validation loss: 2.250199261532035

Epoch: 81| Step: 0
Training loss: 2.5268964767456055
Validation loss: 2.2669681554199546

Epoch: 6| Step: 1
Training loss: 2.5387179851531982
Validation loss: 2.2022684517727105

Epoch: 6| Step: 2
Training loss: 2.0764126777648926
Validation loss: 2.1432180494390507

Epoch: 6| Step: 3
Training loss: 2.7304348945617676
Validation loss: 2.1198359612495667

Epoch: 6| Step: 4
Training loss: 2.807086229324341
Validation loss: 2.1394774811242216

Epoch: 6| Step: 5
Training loss: 3.3855347633361816
Validation loss: 2.1513381850334907

Epoch: 6| Step: 6
Training loss: 2.184736728668213
Validation loss: 2.162483843423987

Epoch: 6| Step: 7
Training loss: 2.6488113403320312
Validation loss: 2.16144262718898

Epoch: 6| Step: 8
Training loss: 2.5531928539276123
Validation loss: 2.1626669924746276

Epoch: 6| Step: 9
Training loss: 2.3618884086608887
Validation loss: 2.15016258147455

Epoch: 6| Step: 10
Training loss: 2.2606542110443115
Validation loss: 2.1389632327582246

Epoch: 6| Step: 11
Training loss: 2.3081207275390625
Validation loss: 2.139376084009806

Epoch: 6| Step: 12
Training loss: 2.961552381515503
Validation loss: 2.1472336169212096

Epoch: 6| Step: 13
Training loss: 2.17289400100708
Validation loss: 2.160075585047404

Epoch: 82| Step: 0
Training loss: 2.5762135982513428
Validation loss: 2.167544658466052

Epoch: 6| Step: 1
Training loss: 2.8907036781311035
Validation loss: 2.1761100574206282

Epoch: 6| Step: 2
Training loss: 2.529205799102783
Validation loss: 2.183683151839882

Epoch: 6| Step: 3
Training loss: 2.790325403213501
Validation loss: 2.2038989246532483

Epoch: 6| Step: 4
Training loss: 2.2186708450317383
Validation loss: 2.1784689759695404

Epoch: 6| Step: 5
Training loss: 2.304274082183838
Validation loss: 2.183463958001906

Epoch: 6| Step: 6
Training loss: 2.841381072998047
Validation loss: 2.162959780744327

Epoch: 6| Step: 7
Training loss: 2.1236796379089355
Validation loss: 2.1608822627734114

Epoch: 6| Step: 8
Training loss: 2.4096455574035645
Validation loss: 2.1353253959327616

Epoch: 6| Step: 9
Training loss: 1.7357845306396484
Validation loss: 2.1304409888482865

Epoch: 6| Step: 10
Training loss: 2.151594638824463
Validation loss: 2.1208160308099564

Epoch: 6| Step: 11
Training loss: 3.1487717628479004
Validation loss: 2.1177965223148303

Epoch: 6| Step: 12
Training loss: 2.446213960647583
Validation loss: 2.1237354765656176

Epoch: 6| Step: 13
Training loss: 2.0722453594207764
Validation loss: 2.148566461378528

Epoch: 83| Step: 0
Training loss: 2.67134165763855
Validation loss: 2.1606872389393468

Epoch: 6| Step: 1
Training loss: 2.2236595153808594
Validation loss: 2.1573616048341155

Epoch: 6| Step: 2
Training loss: 2.893807888031006
Validation loss: 2.157142327677819

Epoch: 6| Step: 3
Training loss: 1.9478271007537842
Validation loss: 2.1429557492656093

Epoch: 6| Step: 4
Training loss: 2.3667330741882324
Validation loss: 2.139257300284601

Epoch: 6| Step: 5
Training loss: 2.2119290828704834
Validation loss: 2.1354956460255448

Epoch: 6| Step: 6
Training loss: 2.562427520751953
Validation loss: 2.133587206563642

Epoch: 6| Step: 7
Training loss: 2.7065300941467285
Validation loss: 2.119972477677048

Epoch: 6| Step: 8
Training loss: 2.804987668991089
Validation loss: 2.11910419823021

Epoch: 6| Step: 9
Training loss: 1.8426809310913086
Validation loss: 2.122929378222394

Epoch: 6| Step: 10
Training loss: 2.627195119857788
Validation loss: 2.1689534238589707

Epoch: 6| Step: 11
Training loss: 2.4491958618164062
Validation loss: 2.2019158512033443

Epoch: 6| Step: 12
Training loss: 2.486865758895874
Validation loss: 2.226231457084738

Epoch: 6| Step: 13
Training loss: 2.639286994934082
Validation loss: 2.230468121908044

Epoch: 84| Step: 0
Training loss: 2.4206342697143555
Validation loss: 2.2390259619682067

Epoch: 6| Step: 1
Training loss: 2.576559066772461
Validation loss: 2.1772927186822377

Epoch: 6| Step: 2
Training loss: 1.9743726253509521
Validation loss: 2.1561473646471576

Epoch: 6| Step: 3
Training loss: 3.0031495094299316
Validation loss: 2.132602268649686

Epoch: 6| Step: 4
Training loss: 3.2350308895111084
Validation loss: 2.1248797344905075

Epoch: 6| Step: 5
Training loss: 1.5776621103286743
Validation loss: 2.138617556582215

Epoch: 6| Step: 6
Training loss: 2.9613561630249023
Validation loss: 2.1266260916186916

Epoch: 6| Step: 7
Training loss: 2.3761792182922363
Validation loss: 2.13552116194079

Epoch: 6| Step: 8
Training loss: 2.6124467849731445
Validation loss: 2.142126444847353

Epoch: 6| Step: 9
Training loss: 2.7033629417419434
Validation loss: 2.1321636861370457

Epoch: 6| Step: 10
Training loss: 2.232252359390259
Validation loss: 2.143635457561862

Epoch: 6| Step: 11
Training loss: 2.7401957511901855
Validation loss: 2.1401774908906672

Epoch: 6| Step: 12
Training loss: 1.9076323509216309
Validation loss: 2.1255961746297856

Epoch: 6| Step: 13
Training loss: 1.505776047706604
Validation loss: 2.1247695620341966

Epoch: 85| Step: 0
Training loss: 2.070712089538574
Validation loss: 2.1217430842820035

Epoch: 6| Step: 1
Training loss: 2.28262996673584
Validation loss: 2.134753114433699

Epoch: 6| Step: 2
Training loss: 1.892533540725708
Validation loss: 2.1600672045061664

Epoch: 6| Step: 3
Training loss: 2.6204869747161865
Validation loss: 2.21401713227713

Epoch: 6| Step: 4
Training loss: 2.802013397216797
Validation loss: 2.258244232464862

Epoch: 6| Step: 5
Training loss: 2.7905867099761963
Validation loss: 2.3151035590838362

Epoch: 6| Step: 6
Training loss: 2.2411115169525146
Validation loss: 2.364469328234273

Epoch: 6| Step: 7
Training loss: 2.190110206604004
Validation loss: 2.3019926163458053

Epoch: 6| Step: 8
Training loss: 2.4538793563842773
Validation loss: 2.212577514750983

Epoch: 6| Step: 9
Training loss: 2.378430128097534
Validation loss: 2.1790047037986016

Epoch: 6| Step: 10
Training loss: 3.039707660675049
Validation loss: 2.1211839158047914

Epoch: 6| Step: 11
Training loss: 2.831821918487549
Validation loss: 2.1158313802493516

Epoch: 6| Step: 12
Training loss: 2.2780871391296387
Validation loss: 2.1096828855494016

Epoch: 6| Step: 13
Training loss: 2.651909589767456
Validation loss: 2.1212318097391436

Epoch: 86| Step: 0
Training loss: 2.729635715484619
Validation loss: 2.129948426318425

Epoch: 6| Step: 1
Training loss: 2.0118117332458496
Validation loss: 2.1199949044053272

Epoch: 6| Step: 2
Training loss: 2.3605880737304688
Validation loss: 2.1308929202377156

Epoch: 6| Step: 3
Training loss: 2.2812447547912598
Validation loss: 2.1345091583908244

Epoch: 6| Step: 4
Training loss: 2.8021347522735596
Validation loss: 2.1341022599127983

Epoch: 6| Step: 5
Training loss: 2.936201810836792
Validation loss: 2.142608996360533

Epoch: 6| Step: 6
Training loss: 2.6463541984558105
Validation loss: 2.1926640592595583

Epoch: 6| Step: 7
Training loss: 2.5820250511169434
Validation loss: 2.272062901527651

Epoch: 6| Step: 8
Training loss: 2.390900135040283
Validation loss: 2.3102693737194104

Epoch: 6| Step: 9
Training loss: 2.157794713973999
Validation loss: 2.325963642007561

Epoch: 6| Step: 10
Training loss: 1.7022252082824707
Validation loss: 2.221456391837007

Epoch: 6| Step: 11
Training loss: 2.359065532684326
Validation loss: 2.1718664938403713

Epoch: 6| Step: 12
Training loss: 3.018404006958008
Validation loss: 2.1286482477700837

Epoch: 6| Step: 13
Training loss: 2.585975170135498
Validation loss: 2.1082648807956326

Epoch: 87| Step: 0
Training loss: 3.1065313816070557
Validation loss: 2.0994154842950965

Epoch: 6| Step: 1
Training loss: 2.5234010219573975
Validation loss: 2.106644788096028

Epoch: 6| Step: 2
Training loss: 1.7211134433746338
Validation loss: 2.114257111344286

Epoch: 6| Step: 3
Training loss: 2.460300922393799
Validation loss: 2.1055078275742067

Epoch: 6| Step: 4
Training loss: 1.6946498155593872
Validation loss: 2.0988561722540084

Epoch: 6| Step: 5
Training loss: 2.3758625984191895
Validation loss: 2.106950895760649

Epoch: 6| Step: 6
Training loss: 2.454726457595825
Validation loss: 2.1054945697066603

Epoch: 6| Step: 7
Training loss: 2.5251784324645996
Validation loss: 2.141210748303321

Epoch: 6| Step: 8
Training loss: 2.9798617362976074
Validation loss: 2.1548021685692573

Epoch: 6| Step: 9
Training loss: 2.4036874771118164
Validation loss: 2.1540047481495845

Epoch: 6| Step: 10
Training loss: 2.612222194671631
Validation loss: 2.1759512501378215

Epoch: 6| Step: 11
Training loss: 2.3082284927368164
Validation loss: 2.2007109990683933

Epoch: 6| Step: 12
Training loss: 2.7989845275878906
Validation loss: 2.215779789032475

Epoch: 6| Step: 13
Training loss: 1.7794718742370605
Validation loss: 2.2079611080949024

Epoch: 88| Step: 0
Training loss: 2.4141669273376465
Validation loss: 2.1898913768029984

Epoch: 6| Step: 1
Training loss: 2.2405712604522705
Validation loss: 2.186626934236096

Epoch: 6| Step: 2
Training loss: 1.5103503465652466
Validation loss: 2.1800238522150184

Epoch: 6| Step: 3
Training loss: 2.493630886077881
Validation loss: 2.1690214474995932

Epoch: 6| Step: 4
Training loss: 2.121469497680664
Validation loss: 2.1676635255095777

Epoch: 6| Step: 5
Training loss: 2.668489933013916
Validation loss: 2.185523768906952

Epoch: 6| Step: 6
Training loss: 2.6176931858062744
Validation loss: 2.1991506904684086

Epoch: 6| Step: 7
Training loss: 2.0181684494018555
Validation loss: 2.250382747701419

Epoch: 6| Step: 8
Training loss: 3.0382018089294434
Validation loss: 2.283081828906972

Epoch: 6| Step: 9
Training loss: 2.7476890087127686
Validation loss: 2.3352545499801636

Epoch: 6| Step: 10
Training loss: 1.6504478454589844
Validation loss: 2.3464683819842596

Epoch: 6| Step: 11
Training loss: 2.839632034301758
Validation loss: 2.287351187839303

Epoch: 6| Step: 12
Training loss: 2.798804759979248
Validation loss: 2.205471811756011

Epoch: 6| Step: 13
Training loss: 3.7067713737487793
Validation loss: 2.1629879038820983

Epoch: 89| Step: 0
Training loss: 2.3304855823516846
Validation loss: 2.1317083694601573

Epoch: 6| Step: 1
Training loss: 2.133678913116455
Validation loss: 2.1331197959120556

Epoch: 6| Step: 2
Training loss: 2.0191540718078613
Validation loss: 2.111386433724434

Epoch: 6| Step: 3
Training loss: 2.5315237045288086
Validation loss: 2.0972405082436016

Epoch: 6| Step: 4
Training loss: 2.458259344100952
Validation loss: 2.0963455964160222

Epoch: 6| Step: 5
Training loss: 2.102409839630127
Validation loss: 2.073396090538271

Epoch: 6| Step: 6
Training loss: 2.9439010620117188
Validation loss: 2.083947843120944

Epoch: 6| Step: 7
Training loss: 2.497084617614746
Validation loss: 2.088935511086577

Epoch: 6| Step: 8
Training loss: 2.49139142036438
Validation loss: 2.0933541405585503

Epoch: 6| Step: 9
Training loss: 2.691039562225342
Validation loss: 2.1041649362092376

Epoch: 6| Step: 10
Training loss: 2.342437744140625
Validation loss: 2.136831268187492

Epoch: 6| Step: 11
Training loss: 2.626333713531494
Validation loss: 2.1382321055217455

Epoch: 6| Step: 12
Training loss: 2.4892539978027344
Validation loss: 2.137100769627479

Epoch: 6| Step: 13
Training loss: 2.842153787612915
Validation loss: 2.1215520405000254

Epoch: 90| Step: 0
Training loss: 3.0956509113311768
Validation loss: 2.130571226919851

Epoch: 6| Step: 1
Training loss: 1.8460674285888672
Validation loss: 2.130092515740343

Epoch: 6| Step: 2
Training loss: 2.8243236541748047
Validation loss: 2.1258701303953766

Epoch: 6| Step: 3
Training loss: 2.2146852016448975
Validation loss: 2.1312436288402927

Epoch: 6| Step: 4
Training loss: 2.3365511894226074
Validation loss: 2.099859869608315

Epoch: 6| Step: 5
Training loss: 2.0289011001586914
Validation loss: 2.0869644482930503

Epoch: 6| Step: 6
Training loss: 2.8832948207855225
Validation loss: 2.076140624220653

Epoch: 6| Step: 7
Training loss: 2.734558582305908
Validation loss: 2.076724535675459

Epoch: 6| Step: 8
Training loss: 2.163076400756836
Validation loss: 2.073490627350346

Epoch: 6| Step: 9
Training loss: 2.320446729660034
Validation loss: 2.0894554840621127

Epoch: 6| Step: 10
Training loss: 2.6815176010131836
Validation loss: 2.087002715756816

Epoch: 6| Step: 11
Training loss: 2.9202818870544434
Validation loss: 2.078440773871637

Epoch: 6| Step: 12
Training loss: 2.1593706607818604
Validation loss: 2.0817705098018853

Epoch: 6| Step: 13
Training loss: 1.2781832218170166
Validation loss: 2.091892050158593

Epoch: 91| Step: 0
Training loss: 2.131366729736328
Validation loss: 2.131057828985235

Epoch: 6| Step: 1
Training loss: 2.9864370822906494
Validation loss: 2.155595125690583

Epoch: 6| Step: 2
Training loss: 3.140998363494873
Validation loss: 2.1567834884889665

Epoch: 6| Step: 3
Training loss: 2.1462907791137695
Validation loss: 2.1385004674234698

Epoch: 6| Step: 4
Training loss: 2.056488513946533
Validation loss: 2.1779988440134193

Epoch: 6| Step: 5
Training loss: 2.8782799243927
Validation loss: 2.2017688879402737

Epoch: 6| Step: 6
Training loss: 2.4862985610961914
Validation loss: 2.1733115873029156

Epoch: 6| Step: 7
Training loss: 2.4470489025115967
Validation loss: 2.1220597041550504

Epoch: 6| Step: 8
Training loss: 2.5738487243652344
Validation loss: 2.0838315358725925

Epoch: 6| Step: 9
Training loss: 2.7451906204223633
Validation loss: 2.0923342858591387

Epoch: 6| Step: 10
Training loss: 1.7841914892196655
Validation loss: 2.09130536612644

Epoch: 6| Step: 11
Training loss: 1.9418953657150269
Validation loss: 2.097777320492652

Epoch: 6| Step: 12
Training loss: 2.012794017791748
Validation loss: 2.110605174495328

Epoch: 6| Step: 13
Training loss: 2.665140151977539
Validation loss: 2.1191598497411257

Epoch: 92| Step: 0
Training loss: 2.2540206909179688
Validation loss: 2.104975027422751

Epoch: 6| Step: 1
Training loss: 2.294325351715088
Validation loss: 2.08413605536184

Epoch: 6| Step: 2
Training loss: 2.40685772895813
Validation loss: 2.081543528905479

Epoch: 6| Step: 3
Training loss: 2.6359429359436035
Validation loss: 2.089326371428787

Epoch: 6| Step: 4
Training loss: 3.061204195022583
Validation loss: 2.1299353876421527

Epoch: 6| Step: 5
Training loss: 2.168043613433838
Validation loss: 2.142244460762188

Epoch: 6| Step: 6
Training loss: 1.9634710550308228
Validation loss: 2.180179726692938

Epoch: 6| Step: 7
Training loss: 2.523606300354004
Validation loss: 2.1491513623986194

Epoch: 6| Step: 8
Training loss: 2.7654812335968018
Validation loss: 2.1436833848235426

Epoch: 6| Step: 9
Training loss: 2.5406711101531982
Validation loss: 2.130275108480966

Epoch: 6| Step: 10
Training loss: 2.2412571907043457
Validation loss: 2.1087287600322435

Epoch: 6| Step: 11
Training loss: 2.4586181640625
Validation loss: 2.1023153976727555

Epoch: 6| Step: 12
Training loss: 2.039975881576538
Validation loss: 2.097917841326806

Epoch: 6| Step: 13
Training loss: 2.9554758071899414
Validation loss: 2.0776120924180552

Epoch: 93| Step: 0
Training loss: 2.5789852142333984
Validation loss: 2.078737606284439

Epoch: 6| Step: 1
Training loss: 1.8999441862106323
Validation loss: 2.0835347983144943

Epoch: 6| Step: 2
Training loss: 2.7651009559631348
Validation loss: 2.0924928675415697

Epoch: 6| Step: 3
Training loss: 2.1453535556793213
Validation loss: 2.0835408267154487

Epoch: 6| Step: 4
Training loss: 2.9861998558044434
Validation loss: 2.0725826704373924

Epoch: 6| Step: 5
Training loss: 2.1260526180267334
Validation loss: 2.0741895937150523

Epoch: 6| Step: 6
Training loss: 2.940276622772217
Validation loss: 2.0855810167968913

Epoch: 6| Step: 7
Training loss: 2.138289213180542
Validation loss: 2.0880611865751204

Epoch: 6| Step: 8
Training loss: 2.1287941932678223
Validation loss: 2.108357153913026

Epoch: 6| Step: 9
Training loss: 1.6029515266418457
Validation loss: 2.1732114784179197

Epoch: 6| Step: 10
Training loss: 2.3515775203704834
Validation loss: 2.2040379560121925

Epoch: 6| Step: 11
Training loss: 3.056983470916748
Validation loss: 2.27131579255545

Epoch: 6| Step: 12
Training loss: 2.7448158264160156
Validation loss: 2.2733715682901363

Epoch: 6| Step: 13
Training loss: 2.2489919662475586
Validation loss: 2.25523498494138

Epoch: 94| Step: 0
Training loss: 2.4193530082702637
Validation loss: 2.1553822896813832

Epoch: 6| Step: 1
Training loss: 2.1214756965637207
Validation loss: 2.088940143585205

Epoch: 6| Step: 2
Training loss: 2.458927869796753
Validation loss: 2.0704970872530373

Epoch: 6| Step: 3
Training loss: 2.9259419441223145
Validation loss: 2.111625466295468

Epoch: 6| Step: 4
Training loss: 2.5301504135131836
Validation loss: 2.1448003015210553

Epoch: 6| Step: 5
Training loss: 2.2869081497192383
Validation loss: 2.2050214826419787

Epoch: 6| Step: 6
Training loss: 3.0144803524017334
Validation loss: 2.2468089595917733

Epoch: 6| Step: 7
Training loss: 2.4346389770507812
Validation loss: 2.2088136083336285

Epoch: 6| Step: 8
Training loss: 2.464996099472046
Validation loss: 2.170464490049629

Epoch: 6| Step: 9
Training loss: 2.4657232761383057
Validation loss: 2.1081408300707416

Epoch: 6| Step: 10
Training loss: 1.8336124420166016
Validation loss: 2.077295741727275

Epoch: 6| Step: 11
Training loss: 2.1658787727355957
Validation loss: 2.076225039779499

Epoch: 6| Step: 12
Training loss: 2.5654826164245605
Validation loss: 2.099971766112953

Epoch: 6| Step: 13
Training loss: 3.1095848083496094
Validation loss: 2.178631559495003

Epoch: 95| Step: 0
Training loss: 2.4827070236206055
Validation loss: 2.280593087596278

Epoch: 6| Step: 1
Training loss: 2.56386137008667
Validation loss: 2.3166457888900593

Epoch: 6| Step: 2
Training loss: 1.9146575927734375
Validation loss: 2.263772392785677

Epoch: 6| Step: 3
Training loss: 2.1604771614074707
Validation loss: 2.173832831844207

Epoch: 6| Step: 4
Training loss: 2.4348442554473877
Validation loss: 2.1145608681504444

Epoch: 6| Step: 5
Training loss: 2.530311107635498
Validation loss: 2.07142702610262

Epoch: 6| Step: 6
Training loss: 2.187073230743408
Validation loss: 2.0639635260387132

Epoch: 6| Step: 7
Training loss: 2.7018775939941406
Validation loss: 2.055717202924913

Epoch: 6| Step: 8
Training loss: 3.4579293727874756
Validation loss: 2.0575671195983887

Epoch: 6| Step: 9
Training loss: 2.7785518169403076
Validation loss: 2.071035559459399

Epoch: 6| Step: 10
Training loss: 1.9636528491973877
Validation loss: 2.069522782038617

Epoch: 6| Step: 11
Training loss: 2.5696401596069336
Validation loss: 2.059565605655793

Epoch: 6| Step: 12
Training loss: 2.5266294479370117
Validation loss: 2.0645898208823255

Epoch: 6| Step: 13
Training loss: 2.1363391876220703
Validation loss: 2.061003242769549

Epoch: 96| Step: 0
Training loss: 1.8137239217758179
Validation loss: 2.0722546013452674

Epoch: 6| Step: 1
Training loss: 1.970877766609192
Validation loss: 2.0687133932626374

Epoch: 6| Step: 2
Training loss: 3.017282485961914
Validation loss: 2.0824959585743565

Epoch: 6| Step: 3
Training loss: 1.6400172710418701
Validation loss: 2.0846629014579197

Epoch: 6| Step: 4
Training loss: 2.7239322662353516
Validation loss: 2.0987774120864047

Epoch: 6| Step: 5
Training loss: 2.1180944442749023
Validation loss: 2.0884013534874044

Epoch: 6| Step: 6
Training loss: 2.6754961013793945
Validation loss: 2.10098272497936

Epoch: 6| Step: 7
Training loss: 1.9222239255905151
Validation loss: 2.100094182516939

Epoch: 6| Step: 8
Training loss: 2.4026951789855957
Validation loss: 2.1038433364642564

Epoch: 6| Step: 9
Training loss: 2.138232946395874
Validation loss: 2.141769322015906

Epoch: 6| Step: 10
Training loss: 2.700932502746582
Validation loss: 2.1616485593139485

Epoch: 6| Step: 11
Training loss: 3.1206374168395996
Validation loss: 2.161276662221519

Epoch: 6| Step: 12
Training loss: 2.7984771728515625
Validation loss: 2.1777061159892748

Epoch: 6| Step: 13
Training loss: 2.4962925910949707
Validation loss: 2.1463022078237226

Epoch: 97| Step: 0
Training loss: 1.1604658365249634
Validation loss: 2.114711946056735

Epoch: 6| Step: 1
Training loss: 2.263972759246826
Validation loss: 2.1123928382832515

Epoch: 6| Step: 2
Training loss: 2.343825101852417
Validation loss: 2.0874499005656086

Epoch: 6| Step: 3
Training loss: 2.671872615814209
Validation loss: 2.0729158886017336

Epoch: 6| Step: 4
Training loss: 2.632046937942505
Validation loss: 2.075710658104189

Epoch: 6| Step: 5
Training loss: 2.973766803741455
Validation loss: 2.082199514553111

Epoch: 6| Step: 6
Training loss: 2.832064628601074
Validation loss: 2.085367338631743

Epoch: 6| Step: 7
Training loss: 2.9480135440826416
Validation loss: 2.084427173419665

Epoch: 6| Step: 8
Training loss: 2.333465099334717
Validation loss: 2.0994080497372534

Epoch: 6| Step: 9
Training loss: 2.8945069313049316
Validation loss: 2.1266405531155166

Epoch: 6| Step: 10
Training loss: 2.1197080612182617
Validation loss: 2.133106585471861

Epoch: 6| Step: 11
Training loss: 1.5259655714035034
Validation loss: 2.1103674698901433

Epoch: 6| Step: 12
Training loss: 2.226374626159668
Validation loss: 2.093928207633316

Epoch: 6| Step: 13
Training loss: 2.8829281330108643
Validation loss: 2.100443458044401

Epoch: 98| Step: 0
Training loss: 2.327892780303955
Validation loss: 2.1261839559001308

Epoch: 6| Step: 1
Training loss: 2.5992302894592285
Validation loss: 2.178020768268134

Epoch: 6| Step: 2
Training loss: 2.5143160820007324
Validation loss: 2.2073341646502094

Epoch: 6| Step: 3
Training loss: 2.059083938598633
Validation loss: 2.277163613227106

Epoch: 6| Step: 4
Training loss: 2.3215126991271973
Validation loss: 2.2789557633861417

Epoch: 6| Step: 5
Training loss: 1.9195431470870972
Validation loss: 2.2549166884473575

Epoch: 6| Step: 6
Training loss: 3.0055649280548096
Validation loss: 2.216420755591444

Epoch: 6| Step: 7
Training loss: 2.784237861633301
Validation loss: 2.1503551390863236

Epoch: 6| Step: 8
Training loss: 2.72171688079834
Validation loss: 2.081899150725334

Epoch: 6| Step: 9
Training loss: 2.0158886909484863
Validation loss: 2.0598105435730307

Epoch: 6| Step: 10
Training loss: 2.190697431564331
Validation loss: 2.054734713287764

Epoch: 6| Step: 11
Training loss: 2.7554128170013428
Validation loss: 2.0670260831873906

Epoch: 6| Step: 12
Training loss: 1.9311552047729492
Validation loss: 2.108836570093709

Epoch: 6| Step: 13
Training loss: 2.871016502380371
Validation loss: 2.1406748935740483

Epoch: 99| Step: 0
Training loss: 2.090027332305908
Validation loss: 2.1585700999024096

Epoch: 6| Step: 1
Training loss: 1.9519894123077393
Validation loss: 2.136803816723567

Epoch: 6| Step: 2
Training loss: 2.6431961059570312
Validation loss: 2.112024389287477

Epoch: 6| Step: 3
Training loss: 2.081958293914795
Validation loss: 2.112659623545985

Epoch: 6| Step: 4
Training loss: 2.890361785888672
Validation loss: 2.112816867008004

Epoch: 6| Step: 5
Training loss: 3.044020175933838
Validation loss: 2.1256678604310557

Epoch: 6| Step: 6
Training loss: 2.24038028717041
Validation loss: 2.1094980034776913

Epoch: 6| Step: 7
Training loss: 2.684445381164551
Validation loss: 2.097467055884741

Epoch: 6| Step: 8
Training loss: 2.5194997787475586
Validation loss: 2.106559202235232

Epoch: 6| Step: 9
Training loss: 1.745908260345459
Validation loss: 2.0976926793334303

Epoch: 6| Step: 10
Training loss: 2.2135462760925293
Validation loss: 2.1011628130430817

Epoch: 6| Step: 11
Training loss: 3.055570602416992
Validation loss: 2.1151944693698677

Epoch: 6| Step: 12
Training loss: 1.9622199535369873
Validation loss: 2.1450626439945673

Epoch: 6| Step: 13
Training loss: 3.409069061279297
Validation loss: 2.192052836059242

Epoch: 100| Step: 0
Training loss: 3.037130117416382
Validation loss: 2.2124791145324707

Epoch: 6| Step: 1
Training loss: 2.319715976715088
Validation loss: 2.238000290368193

Epoch: 6| Step: 2
Training loss: 1.9889984130859375
Validation loss: 2.2350288360349593

Epoch: 6| Step: 3
Training loss: 1.3939533233642578
Validation loss: 2.220120617138442

Epoch: 6| Step: 4
Training loss: 3.240678548812866
Validation loss: 2.172350424592213

Epoch: 6| Step: 5
Training loss: 2.3508424758911133
Validation loss: 2.118863010919222

Epoch: 6| Step: 6
Training loss: 1.9642877578735352
Validation loss: 2.081752154134935

Epoch: 6| Step: 7
Training loss: 2.2856500148773193
Validation loss: 2.0642202079937024

Epoch: 6| Step: 8
Training loss: 1.7632098197937012
Validation loss: 2.0618326407606884

Epoch: 6| Step: 9
Training loss: 2.789926052093506
Validation loss: 2.0596642391656035

Epoch: 6| Step: 10
Training loss: 2.828969955444336
Validation loss: 2.0667984793263097

Epoch: 6| Step: 11
Training loss: 2.300503730773926
Validation loss: 2.057538309404927

Epoch: 6| Step: 12
Training loss: 2.324397563934326
Validation loss: 2.064331954525363

Epoch: 6| Step: 13
Training loss: 2.9622106552124023
Validation loss: 2.04987003982708

Epoch: 101| Step: 0
Training loss: 2.7539048194885254
Validation loss: 2.0533467172294535

Epoch: 6| Step: 1
Training loss: 2.4114999771118164
Validation loss: 2.0603916016958093

Epoch: 6| Step: 2
Training loss: 2.5454137325286865
Validation loss: 2.0676741382127166

Epoch: 6| Step: 3
Training loss: 2.8245015144348145
Validation loss: 2.053980165912259

Epoch: 6| Step: 4
Training loss: 2.641996383666992
Validation loss: 2.083360013141427

Epoch: 6| Step: 5
Training loss: 2.203864336013794
Validation loss: 2.0898795127868652

Epoch: 6| Step: 6
Training loss: 1.9063537120819092
Validation loss: 2.128768536352342

Epoch: 6| Step: 7
Training loss: 2.2555320262908936
Validation loss: 2.113970355321002

Epoch: 6| Step: 8
Training loss: 2.3227691650390625
Validation loss: 2.103772051872746

Epoch: 6| Step: 9
Training loss: 2.118147373199463
Validation loss: 2.0757649175582396

Epoch: 6| Step: 10
Training loss: 2.685763359069824
Validation loss: 2.0574072740411244

Epoch: 6| Step: 11
Training loss: 1.3854825496673584
Validation loss: 2.052155602362848

Epoch: 6| Step: 12
Training loss: 2.5085861682891846
Validation loss: 2.0435958216267247

Epoch: 6| Step: 13
Training loss: 2.8233554363250732
Validation loss: 2.043187382400677

Epoch: 102| Step: 0
Training loss: 2.1871609687805176
Validation loss: 2.0500294188017487

Epoch: 6| Step: 1
Training loss: 1.9475568532943726
Validation loss: 2.0668330090020293

Epoch: 6| Step: 2
Training loss: 2.522491216659546
Validation loss: 2.075545977520686

Epoch: 6| Step: 3
Training loss: 2.2707481384277344
Validation loss: 2.0943234518010128

Epoch: 6| Step: 4
Training loss: 2.4258124828338623
Validation loss: 2.104018634365451

Epoch: 6| Step: 5
Training loss: 1.8086447715759277
Validation loss: 2.0965638352978613

Epoch: 6| Step: 6
Training loss: 2.1108641624450684
Validation loss: 2.1128295967655797

Epoch: 6| Step: 7
Training loss: 2.2676405906677246
Validation loss: 2.1423714853102163

Epoch: 6| Step: 8
Training loss: 2.3917384147644043
Validation loss: 2.159796537891511

Epoch: 6| Step: 9
Training loss: 2.963805675506592
Validation loss: 2.1811138122312483

Epoch: 6| Step: 10
Training loss: 2.708773136138916
Validation loss: 2.2111804972412767

Epoch: 6| Step: 11
Training loss: 2.677541732788086
Validation loss: 2.199611576654578

Epoch: 6| Step: 12
Training loss: 2.2124104499816895
Validation loss: 2.1648504029038134

Epoch: 6| Step: 13
Training loss: 3.0113580226898193
Validation loss: 2.123557621432889

Epoch: 103| Step: 0
Training loss: 1.8753833770751953
Validation loss: 2.109901207749562

Epoch: 6| Step: 1
Training loss: 3.1003284454345703
Validation loss: 2.102076288192503

Epoch: 6| Step: 2
Training loss: 1.7124524116516113
Validation loss: 2.1123751645447104

Epoch: 6| Step: 3
Training loss: 2.3445801734924316
Validation loss: 2.103496238749514

Epoch: 6| Step: 4
Training loss: 2.187920570373535
Validation loss: 2.0907004135911182

Epoch: 6| Step: 5
Training loss: 2.370605945587158
Validation loss: 2.093586244890767

Epoch: 6| Step: 6
Training loss: 2.3767104148864746
Validation loss: 2.0807769042189403

Epoch: 6| Step: 7
Training loss: 1.9861735105514526
Validation loss: 2.08818022794621

Epoch: 6| Step: 8
Training loss: 2.66172194480896
Validation loss: 2.090050708863043

Epoch: 6| Step: 9
Training loss: 2.7738537788391113
Validation loss: 2.0992297126400854

Epoch: 6| Step: 10
Training loss: 1.9098188877105713
Validation loss: 2.111063334249681

Epoch: 6| Step: 11
Training loss: 2.76820707321167
Validation loss: 2.138935186529672

Epoch: 6| Step: 12
Training loss: 2.356252670288086
Validation loss: 2.1558920401398853

Epoch: 6| Step: 13
Training loss: 2.5910181999206543
Validation loss: 2.133163323966406

Epoch: 104| Step: 0
Training loss: 2.2215352058410645
Validation loss: 2.1573957935456307

Epoch: 6| Step: 1
Training loss: 2.479039192199707
Validation loss: 2.1615608315314017

Epoch: 6| Step: 2
Training loss: 2.7887966632843018
Validation loss: 2.1692008715803905

Epoch: 6| Step: 3
Training loss: 1.7820278406143188
Validation loss: 2.1205199200619935

Epoch: 6| Step: 4
Training loss: 2.615170478820801
Validation loss: 2.093313214599445

Epoch: 6| Step: 5
Training loss: 3.073864459991455
Validation loss: 2.0796378902209702

Epoch: 6| Step: 6
Training loss: 2.2552151679992676
Validation loss: 2.056835674470471

Epoch: 6| Step: 7
Training loss: 1.7907339334487915
Validation loss: 2.0430613140906058

Epoch: 6| Step: 8
Training loss: 2.379645824432373
Validation loss: 2.0390558140252226

Epoch: 6| Step: 9
Training loss: 1.9406368732452393
Validation loss: 2.035772544081493

Epoch: 6| Step: 10
Training loss: 2.396956205368042
Validation loss: 2.022048770740468

Epoch: 6| Step: 11
Training loss: 2.4818179607391357
Validation loss: 2.0321065187454224

Epoch: 6| Step: 12
Training loss: 2.7426531314849854
Validation loss: 2.0342632339846705

Epoch: 6| Step: 13
Training loss: 2.327950954437256
Validation loss: 2.0474839133601033

Epoch: 105| Step: 0
Training loss: 3.1707441806793213
Validation loss: 2.0474365424084406

Epoch: 6| Step: 1
Training loss: 2.100550651550293
Validation loss: 2.0462808762827227

Epoch: 6| Step: 2
Training loss: 2.030607223510742
Validation loss: 2.0540830473746023

Epoch: 6| Step: 3
Training loss: 2.2569761276245117
Validation loss: 2.064725716908773

Epoch: 6| Step: 4
Training loss: 2.459880828857422
Validation loss: 2.0691457692012993

Epoch: 6| Step: 5
Training loss: 1.9698519706726074
Validation loss: 2.0798725030755483

Epoch: 6| Step: 6
Training loss: 2.2705328464508057
Validation loss: 2.0800526924030756

Epoch: 6| Step: 7
Training loss: 2.303389072418213
Validation loss: 2.087895724081224

Epoch: 6| Step: 8
Training loss: 2.6621479988098145
Validation loss: 2.079185055148217

Epoch: 6| Step: 9
Training loss: 2.3849387168884277
Validation loss: 2.0819114151821343

Epoch: 6| Step: 10
Training loss: 2.849113702774048
Validation loss: 2.0928177167010564

Epoch: 6| Step: 11
Training loss: 2.3946926593780518
Validation loss: 2.0966353236988025

Epoch: 6| Step: 12
Training loss: 1.81111741065979
Validation loss: 2.096977206968492

Epoch: 6| Step: 13
Training loss: 2.103078842163086
Validation loss: 2.080434588975804

Epoch: 106| Step: 0
Training loss: 2.5154733657836914
Validation loss: 2.064269176093481

Epoch: 6| Step: 1
Training loss: 2.2325868606567383
Validation loss: 2.0501284701849825

Epoch: 6| Step: 2
Training loss: 1.6778526306152344
Validation loss: 2.057215431685089

Epoch: 6| Step: 3
Training loss: 2.604463577270508
Validation loss: 2.0566500668884604

Epoch: 6| Step: 4
Training loss: 2.1329007148742676
Validation loss: 2.057864737767045

Epoch: 6| Step: 5
Training loss: 2.7882542610168457
Validation loss: 2.064889680954718

Epoch: 6| Step: 6
Training loss: 2.794147491455078
Validation loss: 2.092657486597697

Epoch: 6| Step: 7
Training loss: 2.4697189331054688
Validation loss: 2.1040837226375455

Epoch: 6| Step: 8
Training loss: 2.3230621814727783
Validation loss: 2.1331992072443806

Epoch: 6| Step: 9
Training loss: 1.7485086917877197
Validation loss: 2.1583748350861254

Epoch: 6| Step: 10
Training loss: 2.2909646034240723
Validation loss: 2.154410141770558

Epoch: 6| Step: 11
Training loss: 2.014937400817871
Validation loss: 2.1227244638627574

Epoch: 6| Step: 12
Training loss: 2.728332042694092
Validation loss: 2.1244432439086256

Epoch: 6| Step: 13
Training loss: 2.6549415588378906
Validation loss: 2.0713186699856996

Epoch: 107| Step: 0
Training loss: 2.0379080772399902
Validation loss: 2.0415038190862185

Epoch: 6| Step: 1
Training loss: 2.0873641967773438
Validation loss: 2.0390971424759075

Epoch: 6| Step: 2
Training loss: 2.060603618621826
Validation loss: 2.0366161702781596

Epoch: 6| Step: 3
Training loss: 2.5798721313476562
Validation loss: 2.026743717091058

Epoch: 6| Step: 4
Training loss: 2.875126600265503
Validation loss: 2.0220574999368317

Epoch: 6| Step: 5
Training loss: 1.7993152141571045
Validation loss: 2.012083779099167

Epoch: 6| Step: 6
Training loss: 2.620701789855957
Validation loss: 2.024994065684657

Epoch: 6| Step: 7
Training loss: 2.4046378135681152
Validation loss: 2.0448832191446775

Epoch: 6| Step: 8
Training loss: 1.8734283447265625
Validation loss: 2.0500496613082064

Epoch: 6| Step: 9
Training loss: 2.9112963676452637
Validation loss: 2.0448548434883036

Epoch: 6| Step: 10
Training loss: 3.0004007816314697
Validation loss: 2.0463962516477032

Epoch: 6| Step: 11
Training loss: 1.7559276819229126
Validation loss: 2.051763855000978

Epoch: 6| Step: 12
Training loss: 2.3232975006103516
Validation loss: 2.071292431123795

Epoch: 6| Step: 13
Training loss: 2.4275143146514893
Validation loss: 2.0940121707095893

Epoch: 108| Step: 0
Training loss: 2.5836563110351562
Validation loss: 2.1013262964064077

Epoch: 6| Step: 1
Training loss: 2.2857751846313477
Validation loss: 2.1192436282352736

Epoch: 6| Step: 2
Training loss: 2.9298014640808105
Validation loss: 2.109723767926616

Epoch: 6| Step: 3
Training loss: 2.3576416969299316
Validation loss: 2.07532912172297

Epoch: 6| Step: 4
Training loss: 2.5583701133728027
Validation loss: 2.055259732789891

Epoch: 6| Step: 5
Training loss: 1.8988951444625854
Validation loss: 2.0502514890445176

Epoch: 6| Step: 6
Training loss: 2.2667157649993896
Validation loss: 2.0504151672445317

Epoch: 6| Step: 7
Training loss: 2.6217122077941895
Validation loss: 2.062672886797177

Epoch: 6| Step: 8
Training loss: 2.2156131267547607
Validation loss: 2.04628667523784

Epoch: 6| Step: 9
Training loss: 2.327563762664795
Validation loss: 2.053624645356209

Epoch: 6| Step: 10
Training loss: 2.2350680828094482
Validation loss: 2.0429237837432535

Epoch: 6| Step: 11
Training loss: 2.1778464317321777
Validation loss: 2.0351942072632494

Epoch: 6| Step: 12
Training loss: 1.7752466201782227
Validation loss: 2.0396570928635134

Epoch: 6| Step: 13
Training loss: 2.1966664791107178
Validation loss: 2.0394290672835482

Epoch: 109| Step: 0
Training loss: 2.3128819465637207
Validation loss: 2.0523455078883837

Epoch: 6| Step: 1
Training loss: 2.1856024265289307
Validation loss: 2.0675503976883425

Epoch: 6| Step: 2
Training loss: 2.271939277648926
Validation loss: 2.0635718043132494

Epoch: 6| Step: 3
Training loss: 2.8195624351501465
Validation loss: 2.0680347091408184

Epoch: 6| Step: 4
Training loss: 2.031482219696045
Validation loss: 2.054945752184878

Epoch: 6| Step: 5
Training loss: 2.713627338409424
Validation loss: 2.0671802771988737

Epoch: 6| Step: 6
Training loss: 2.213667392730713
Validation loss: 2.0666884042883433

Epoch: 6| Step: 7
Training loss: 2.0621986389160156
Validation loss: 2.0689667476120817

Epoch: 6| Step: 8
Training loss: 2.2911136150360107
Validation loss: 2.0756636434985745

Epoch: 6| Step: 9
Training loss: 2.4091639518737793
Validation loss: 2.0775622167894916

Epoch: 6| Step: 10
Training loss: 2.316941738128662
Validation loss: 2.08157729974357

Epoch: 6| Step: 11
Training loss: 2.5555338859558105
Validation loss: 2.07300930125739

Epoch: 6| Step: 12
Training loss: 1.8854175806045532
Validation loss: 2.0877404725679787

Epoch: 6| Step: 13
Training loss: 1.8462262153625488
Validation loss: 2.096858443752412

Epoch: 110| Step: 0
Training loss: 2.2751388549804688
Validation loss: 2.095364655217817

Epoch: 6| Step: 1
Training loss: 1.877575397491455
Validation loss: 2.101163124525419

Epoch: 6| Step: 2
Training loss: 2.477649211883545
Validation loss: 2.087408847706292

Epoch: 6| Step: 3
Training loss: 1.838667869567871
Validation loss: 2.079797887033032

Epoch: 6| Step: 4
Training loss: 2.9736194610595703
Validation loss: 2.0750470545984085

Epoch: 6| Step: 5
Training loss: 2.146561622619629
Validation loss: 2.067608255212025

Epoch: 6| Step: 6
Training loss: 1.8810365200042725
Validation loss: 2.0651604539604596

Epoch: 6| Step: 7
Training loss: 2.5840163230895996
Validation loss: 2.0595924251823017

Epoch: 6| Step: 8
Training loss: 2.2848281860351562
Validation loss: 2.075117234260805

Epoch: 6| Step: 9
Training loss: 1.9900031089782715
Validation loss: 2.070023126499627

Epoch: 6| Step: 10
Training loss: 2.6277811527252197
Validation loss: 2.0686507853128577

Epoch: 6| Step: 11
Training loss: 2.3355724811553955
Validation loss: 2.0657103471858527

Epoch: 6| Step: 12
Training loss: 2.3992724418640137
Validation loss: 2.06789267575869

Epoch: 6| Step: 13
Training loss: 2.4409666061401367
Validation loss: 2.0728421326606505

Epoch: 111| Step: 0
Training loss: 2.0895111560821533
Validation loss: 2.075004075163154

Epoch: 6| Step: 1
Training loss: 1.514155626296997
Validation loss: 2.0641234767052437

Epoch: 6| Step: 2
Training loss: 2.5245542526245117
Validation loss: 2.065427900642477

Epoch: 6| Step: 3
Training loss: 2.369028091430664
Validation loss: 2.056584117233112

Epoch: 6| Step: 4
Training loss: 2.0526633262634277
Validation loss: 2.0756624437147573

Epoch: 6| Step: 5
Training loss: 2.845353603363037
Validation loss: 2.0632324270022813

Epoch: 6| Step: 6
Training loss: 1.7049314975738525
Validation loss: 2.068151589362852

Epoch: 6| Step: 7
Training loss: 1.9372830390930176
Validation loss: 2.0641682378707396

Epoch: 6| Step: 8
Training loss: 2.806645393371582
Validation loss: 2.0564822778906873

Epoch: 6| Step: 9
Training loss: 2.544520616531372
Validation loss: 2.068448010311332

Epoch: 6| Step: 10
Training loss: 2.684797525405884
Validation loss: 2.080825627491038

Epoch: 6| Step: 11
Training loss: 1.8577158451080322
Validation loss: 2.0686341844579226

Epoch: 6| Step: 12
Training loss: 2.286405086517334
Validation loss: 2.0641187160245833

Epoch: 6| Step: 13
Training loss: 3.163506507873535
Validation loss: 2.074088622164983

Epoch: 112| Step: 0
Training loss: 2.2952964305877686
Validation loss: 2.0921755272855043

Epoch: 6| Step: 1
Training loss: 2.4773426055908203
Validation loss: 2.118825574075022

Epoch: 6| Step: 2
Training loss: 2.1056690216064453
Validation loss: 2.138226483457832

Epoch: 6| Step: 3
Training loss: 2.5484213829040527
Validation loss: 2.106230725524246

Epoch: 6| Step: 4
Training loss: 2.202517032623291
Validation loss: 2.0709114946344847

Epoch: 6| Step: 5
Training loss: 2.5680577754974365
Validation loss: 2.0588490475890455

Epoch: 6| Step: 6
Training loss: 2.3425958156585693
Validation loss: 2.0495011473214753

Epoch: 6| Step: 7
Training loss: 2.194443702697754
Validation loss: 2.0472075734087216

Epoch: 6| Step: 8
Training loss: 1.8083992004394531
Validation loss: 2.0573750644601803

Epoch: 6| Step: 9
Training loss: 2.89163875579834
Validation loss: 2.0633382130694646

Epoch: 6| Step: 10
Training loss: 2.276031017303467
Validation loss: 2.063184566395257

Epoch: 6| Step: 11
Training loss: 2.1372179985046387
Validation loss: 2.0631524734599616

Epoch: 6| Step: 12
Training loss: 2.4663314819335938
Validation loss: 2.0722026145586403

Epoch: 6| Step: 13
Training loss: 2.1623599529266357
Validation loss: 2.0551585382030857

Epoch: 113| Step: 0
Training loss: 2.1920008659362793
Validation loss: 2.0441205578465618

Epoch: 6| Step: 1
Training loss: 1.6999351978302002
Validation loss: 2.045182346015848

Epoch: 6| Step: 2
Training loss: 2.3252506256103516
Validation loss: 2.0644644639825307

Epoch: 6| Step: 3
Training loss: 2.7151992321014404
Validation loss: 2.087462153486026

Epoch: 6| Step: 4
Training loss: 2.6860122680664062
Validation loss: 2.097100434764739

Epoch: 6| Step: 5
Training loss: 2.048231840133667
Validation loss: 2.133305054838939

Epoch: 6| Step: 6
Training loss: 1.5183712244033813
Validation loss: 2.210329665932604

Epoch: 6| Step: 7
Training loss: 1.9502114057540894
Validation loss: 2.180718888518631

Epoch: 6| Step: 8
Training loss: 1.8850514888763428
Validation loss: 2.124360543425365

Epoch: 6| Step: 9
Training loss: 2.463258743286133
Validation loss: 2.075027673475204

Epoch: 6| Step: 10
Training loss: 2.876476764678955
Validation loss: 2.0653092374083815

Epoch: 6| Step: 11
Training loss: 2.5672967433929443
Validation loss: 2.054884679855839

Epoch: 6| Step: 12
Training loss: 2.2065250873565674
Validation loss: 2.075212813192798

Epoch: 6| Step: 13
Training loss: 3.52836012840271
Validation loss: 2.0865258914168163

Epoch: 114| Step: 0
Training loss: 2.3713064193725586
Validation loss: 2.1165417458421443

Epoch: 6| Step: 1
Training loss: 2.3466906547546387
Validation loss: 2.1208415390342794

Epoch: 6| Step: 2
Training loss: 2.960524082183838
Validation loss: 2.1451827300492154

Epoch: 6| Step: 3
Training loss: 1.987823486328125
Validation loss: 2.147875875555059

Epoch: 6| Step: 4
Training loss: 2.2000484466552734
Validation loss: 2.1270752350489297

Epoch: 6| Step: 5
Training loss: 2.5395705699920654
Validation loss: 2.0717281064679547

Epoch: 6| Step: 6
Training loss: 2.82362961769104
Validation loss: 2.056431281951166

Epoch: 6| Step: 7
Training loss: 2.0936667919158936
Validation loss: 2.0436904045843307

Epoch: 6| Step: 8
Training loss: 1.8110036849975586
Validation loss: 2.04058269018768

Epoch: 6| Step: 9
Training loss: 1.982717752456665
Validation loss: 2.0628949647308676

Epoch: 6| Step: 10
Training loss: 2.1394567489624023
Validation loss: 2.1058451411544636

Epoch: 6| Step: 11
Training loss: 2.546722412109375
Validation loss: 2.1131480355416574

Epoch: 6| Step: 12
Training loss: 2.1084935665130615
Validation loss: 2.110663552438059

Epoch: 6| Step: 13
Training loss: 2.1609132289886475
Validation loss: 2.101505717923564

Epoch: 115| Step: 0
Training loss: 1.9208972454071045
Validation loss: 2.078252054029895

Epoch: 6| Step: 1
Training loss: 2.356071710586548
Validation loss: 2.064290941402476

Epoch: 6| Step: 2
Training loss: 1.4355621337890625
Validation loss: 2.0547648963107856

Epoch: 6| Step: 3
Training loss: 2.700392246246338
Validation loss: 2.045867801994406

Epoch: 6| Step: 4
Training loss: 2.5920701026916504
Validation loss: 2.028105797306184

Epoch: 6| Step: 5
Training loss: 2.356843948364258
Validation loss: 2.0180351247069654

Epoch: 6| Step: 6
Training loss: 2.1137728691101074
Validation loss: 2.033927362452271

Epoch: 6| Step: 7
Training loss: 2.4133503437042236
Validation loss: 2.052358858046993

Epoch: 6| Step: 8
Training loss: 2.4712095260620117
Validation loss: 2.056253512700399

Epoch: 6| Step: 9
Training loss: 1.5779519081115723
Validation loss: 2.0699323684938493

Epoch: 6| Step: 10
Training loss: 3.4219226837158203
Validation loss: 2.0669500545788835

Epoch: 6| Step: 11
Training loss: 1.9694921970367432
Validation loss: 2.0756724290950324

Epoch: 6| Step: 12
Training loss: 1.604649305343628
Validation loss: 2.0795470847878406

Epoch: 6| Step: 13
Training loss: 2.8682260513305664
Validation loss: 2.1010228997917584

Epoch: 116| Step: 0
Training loss: 2.2244043350219727
Validation loss: 2.093900126795615

Epoch: 6| Step: 1
Training loss: 1.9057217836380005
Validation loss: 2.0851026555543304

Epoch: 6| Step: 2
Training loss: 1.8464125394821167
Validation loss: 2.0764116651268414

Epoch: 6| Step: 3
Training loss: 2.7472400665283203
Validation loss: 2.0903537363134403

Epoch: 6| Step: 4
Training loss: 1.3133878707885742
Validation loss: 2.1168689779056016

Epoch: 6| Step: 5
Training loss: 2.7653069496154785
Validation loss: 2.112438437759235

Epoch: 6| Step: 6
Training loss: 2.2928214073181152
Validation loss: 2.1029296857054516

Epoch: 6| Step: 7
Training loss: 2.5555498600006104
Validation loss: 2.0878553980140278

Epoch: 6| Step: 8
Training loss: 1.8791617155075073
Validation loss: 2.0757989703968005

Epoch: 6| Step: 9
Training loss: 2.009023427963257
Validation loss: 2.0745649363404963

Epoch: 6| Step: 10
Training loss: 2.42043399810791
Validation loss: 2.0486322372190413

Epoch: 6| Step: 11
Training loss: 2.6848316192626953
Validation loss: 2.0277443009038127

Epoch: 6| Step: 12
Training loss: 2.3543031215667725
Validation loss: 2.0233337186997935

Epoch: 6| Step: 13
Training loss: 2.9539177417755127
Validation loss: 2.030390297212908

Epoch: 117| Step: 0
Training loss: 2.6929445266723633
Validation loss: 2.0655923992074947

Epoch: 6| Step: 1
Training loss: 2.5852322578430176
Validation loss: 2.0931444270636446

Epoch: 6| Step: 2
Training loss: 2.1979336738586426
Validation loss: 2.0810258311610066

Epoch: 6| Step: 3
Training loss: 2.692228317260742
Validation loss: 2.0510720335027224

Epoch: 6| Step: 4
Training loss: 2.6492393016815186
Validation loss: 2.041591205904561

Epoch: 6| Step: 5
Training loss: 2.6394519805908203
Validation loss: 2.027706548731814

Epoch: 6| Step: 6
Training loss: 2.034423351287842
Validation loss: 2.0122385268570273

Epoch: 6| Step: 7
Training loss: 1.5300161838531494
Validation loss: 2.011917188603391

Epoch: 6| Step: 8
Training loss: 2.587404727935791
Validation loss: 2.022000415350801

Epoch: 6| Step: 9
Training loss: 2.4525437355041504
Validation loss: 2.0505096668838174

Epoch: 6| Step: 10
Training loss: 2.7490007877349854
Validation loss: 2.0592256117892522

Epoch: 6| Step: 11
Training loss: 1.5317938327789307
Validation loss: 2.0521448017448507

Epoch: 6| Step: 12
Training loss: 1.3238469362258911
Validation loss: 2.0615552189529582

Epoch: 6| Step: 13
Training loss: 1.9107908010482788
Validation loss: 2.0593366238378708

Epoch: 118| Step: 0
Training loss: 2.395543336868286
Validation loss: 2.072646781962405

Epoch: 6| Step: 1
Training loss: 2.4414258003234863
Validation loss: 2.0402965404654063

Epoch: 6| Step: 2
Training loss: 2.4485602378845215
Validation loss: 2.050650491509386

Epoch: 6| Step: 3
Training loss: 2.279869794845581
Validation loss: 2.0404556656396515

Epoch: 6| Step: 4
Training loss: 2.5017778873443604
Validation loss: 2.0306184676385697

Epoch: 6| Step: 5
Training loss: 2.092391014099121
Validation loss: 2.0202002307420135

Epoch: 6| Step: 6
Training loss: 2.098601818084717
Validation loss: 2.022399894652828

Epoch: 6| Step: 7
Training loss: 2.368460178375244
Validation loss: 2.0064431364818285

Epoch: 6| Step: 8
Training loss: 2.4759082794189453
Validation loss: 2.011596538687265

Epoch: 6| Step: 9
Training loss: 2.207026481628418
Validation loss: 2.0367614248747468

Epoch: 6| Step: 10
Training loss: 1.3885619640350342
Validation loss: 2.071080647489076

Epoch: 6| Step: 11
Training loss: 2.250497341156006
Validation loss: 2.106711890107842

Epoch: 6| Step: 12
Training loss: 2.074941873550415
Validation loss: 2.13321449295167

Epoch: 6| Step: 13
Training loss: 2.7497177124023438
Validation loss: 2.102500802727156

Epoch: 119| Step: 0
Training loss: 1.040934443473816
Validation loss: 2.0854578377098165

Epoch: 6| Step: 1
Training loss: 1.8888131380081177
Validation loss: 2.0600506259549047

Epoch: 6| Step: 2
Training loss: 2.1247715950012207
Validation loss: 2.045108308074295

Epoch: 6| Step: 3
Training loss: 2.508390426635742
Validation loss: 2.049316862578033

Epoch: 6| Step: 4
Training loss: 1.7390449047088623
Validation loss: 2.0734154870433192

Epoch: 6| Step: 5
Training loss: 2.5405335426330566
Validation loss: 2.0845432948040705

Epoch: 6| Step: 6
Training loss: 1.3842346668243408
Validation loss: 2.102892114270118

Epoch: 6| Step: 7
Training loss: 2.4391884803771973
Validation loss: 2.1175252083809144

Epoch: 6| Step: 8
Training loss: 2.5415472984313965
Validation loss: 2.1121570282084967

Epoch: 6| Step: 9
Training loss: 2.455026865005493
Validation loss: 2.1089806364428614

Epoch: 6| Step: 10
Training loss: 2.5052895545959473
Validation loss: 2.10592629191696

Epoch: 6| Step: 11
Training loss: 2.9240546226501465
Validation loss: 2.0871155274811612

Epoch: 6| Step: 12
Training loss: 2.7829408645629883
Validation loss: 2.0542548446245092

Epoch: 6| Step: 13
Training loss: 2.5135409832000732
Validation loss: 2.0617255267276557

Epoch: 120| Step: 0
Training loss: 2.083937406539917
Validation loss: 2.039851342478106

Epoch: 6| Step: 1
Training loss: 2.2380213737487793
Validation loss: 2.0077676721798476

Epoch: 6| Step: 2
Training loss: 2.678098201751709
Validation loss: 2.0046425250268753

Epoch: 6| Step: 3
Training loss: 2.839949607849121
Validation loss: 2.0022901591434272

Epoch: 6| Step: 4
Training loss: 1.7377307415008545
Validation loss: 2.0075371573048253

Epoch: 6| Step: 5
Training loss: 2.0584332942962646
Validation loss: 1.9919622841701712

Epoch: 6| Step: 6
Training loss: 2.4021942615509033
Validation loss: 2.0062061355959986

Epoch: 6| Step: 7
Training loss: 1.9351330995559692
Validation loss: 2.023503067672894

Epoch: 6| Step: 8
Training loss: 1.8455326557159424
Validation loss: 2.037215514849591

Epoch: 6| Step: 9
Training loss: 2.11356782913208
Validation loss: 2.0463446827344995

Epoch: 6| Step: 10
Training loss: 2.492072105407715
Validation loss: 2.053534333423902

Epoch: 6| Step: 11
Training loss: 2.364293098449707
Validation loss: 2.063852826754252

Epoch: 6| Step: 12
Training loss: 1.9678120613098145
Validation loss: 2.0577534706361833

Epoch: 6| Step: 13
Training loss: 2.4501216411590576
Validation loss: 2.065663283871066

Epoch: 121| Step: 0
Training loss: 1.9663280248641968
Validation loss: 2.078300873438517

Epoch: 6| Step: 1
Training loss: 1.9826359748840332
Validation loss: 2.0483720174399753

Epoch: 6| Step: 2
Training loss: 2.9444174766540527
Validation loss: 2.0377395793955815

Epoch: 6| Step: 3
Training loss: 1.6007428169250488
Validation loss: 2.031968362869755

Epoch: 6| Step: 4
Training loss: 2.1453490257263184
Validation loss: 2.037129065041901

Epoch: 6| Step: 5
Training loss: 1.811984896659851
Validation loss: 2.0398118226758895

Epoch: 6| Step: 6
Training loss: 2.4104089736938477
Validation loss: 2.0375207201127084

Epoch: 6| Step: 7
Training loss: 2.2983345985412598
Validation loss: 2.040934193518854

Epoch: 6| Step: 8
Training loss: 2.185828924179077
Validation loss: 2.036828033385738

Epoch: 6| Step: 9
Training loss: 2.2885313034057617
Validation loss: 2.038942954873526

Epoch: 6| Step: 10
Training loss: 2.5830886363983154
Validation loss: 2.0321310540681243

Epoch: 6| Step: 11
Training loss: 2.0539350509643555
Validation loss: 2.041355208684039

Epoch: 6| Step: 12
Training loss: 2.4889543056488037
Validation loss: 2.048421925114047

Epoch: 6| Step: 13
Training loss: 1.5934845209121704
Validation loss: 2.05122894881874

Epoch: 122| Step: 0
Training loss: 2.4115188121795654
Validation loss: 2.0544711364212858

Epoch: 6| Step: 1
Training loss: 2.356034994125366
Validation loss: 2.0722660018551733

Epoch: 6| Step: 2
Training loss: 1.7592236995697021
Validation loss: 2.052733113688807

Epoch: 6| Step: 3
Training loss: 1.7433300018310547
Validation loss: 2.068730495309317

Epoch: 6| Step: 4
Training loss: 2.541675090789795
Validation loss: 2.0788695863498154

Epoch: 6| Step: 5
Training loss: 1.9613219499588013
Validation loss: 2.0508479213201873

Epoch: 6| Step: 6
Training loss: 1.9868831634521484
Validation loss: 2.0775923587942637

Epoch: 6| Step: 7
Training loss: 2.101520538330078
Validation loss: 2.0969145515913605

Epoch: 6| Step: 8
Training loss: 2.1950201988220215
Validation loss: 2.0759410601790234

Epoch: 6| Step: 9
Training loss: 1.3956466913223267
Validation loss: 2.0782721606633996

Epoch: 6| Step: 10
Training loss: 2.0460121631622314
Validation loss: 2.0812438457242903

Epoch: 6| Step: 11
Training loss: 3.1984124183654785
Validation loss: 2.0914302820800454

Epoch: 6| Step: 12
Training loss: 2.1774041652679443
Validation loss: 2.076592560737364

Epoch: 6| Step: 13
Training loss: 2.817181348800659
Validation loss: 2.0518509841734365

Epoch: 123| Step: 0
Training loss: 2.2010974884033203
Validation loss: 2.04031769434611

Epoch: 6| Step: 1
Training loss: 1.7729406356811523
Validation loss: 2.033113473205156

Epoch: 6| Step: 2
Training loss: 2.2252211570739746
Validation loss: 2.0272962841936337

Epoch: 6| Step: 3
Training loss: 1.6209867000579834
Validation loss: 2.0161977070634083

Epoch: 6| Step: 4
Training loss: 2.4739222526550293
Validation loss: 1.9968032234458513

Epoch: 6| Step: 5
Training loss: 2.1520233154296875
Validation loss: 1.998219350332855

Epoch: 6| Step: 6
Training loss: 2.265752077102661
Validation loss: 1.9830114841461182

Epoch: 6| Step: 7
Training loss: 2.3210091590881348
Validation loss: 1.964500383664203

Epoch: 6| Step: 8
Training loss: 2.4651551246643066
Validation loss: 1.957828588383172

Epoch: 6| Step: 9
Training loss: 2.6956660747528076
Validation loss: 1.9602065599092873

Epoch: 6| Step: 10
Training loss: 2.815910816192627
Validation loss: 1.959714155043325

Epoch: 6| Step: 11
Training loss: 2.124288558959961
Validation loss: 1.969678883911461

Epoch: 6| Step: 12
Training loss: 1.8983891010284424
Validation loss: 1.9728514173979401

Epoch: 6| Step: 13
Training loss: 2.1249685287475586
Validation loss: 1.9811966021855671

Epoch: 124| Step: 0
Training loss: 2.7346413135528564
Validation loss: 2.0066974701419955

Epoch: 6| Step: 1
Training loss: 2.1293559074401855
Validation loss: 2.0162516101714103

Epoch: 6| Step: 2
Training loss: 2.589071035385132
Validation loss: 2.044077201556134

Epoch: 6| Step: 3
Training loss: 1.8282390832901
Validation loss: 2.0732185981606923

Epoch: 6| Step: 4
Training loss: 2.2987425327301025
Validation loss: 2.094565189012917

Epoch: 6| Step: 5
Training loss: 1.872667670249939
Validation loss: 2.104440901869087

Epoch: 6| Step: 6
Training loss: 1.3059897422790527
Validation loss: 2.111016427316973

Epoch: 6| Step: 7
Training loss: 1.5884108543395996
Validation loss: 2.1408457217677945

Epoch: 6| Step: 8
Training loss: 2.539813280105591
Validation loss: 2.15801751741799

Epoch: 6| Step: 9
Training loss: 3.319655418395996
Validation loss: 2.19108683447684

Epoch: 6| Step: 10
Training loss: 1.9928048849105835
Validation loss: 2.1938589055051088

Epoch: 6| Step: 11
Training loss: 2.5350093841552734
Validation loss: 2.167701635309445

Epoch: 6| Step: 12
Training loss: 1.7999811172485352
Validation loss: 2.1440383695786998

Epoch: 6| Step: 13
Training loss: 2.645780324935913
Validation loss: 2.1076930607518842

Epoch: 125| Step: 0
Training loss: 1.874189853668213
Validation loss: 2.080040793265066

Epoch: 6| Step: 1
Training loss: 1.716882586479187
Validation loss: 2.092246732404155

Epoch: 6| Step: 2
Training loss: 1.6791086196899414
Validation loss: 2.0776946980466127

Epoch: 6| Step: 3
Training loss: 2.767557382583618
Validation loss: 2.0964520874843804

Epoch: 6| Step: 4
Training loss: 2.4169554710388184
Validation loss: 2.066307642126596

Epoch: 6| Step: 5
Training loss: 2.321715831756592
Validation loss: 2.045663749018023

Epoch: 6| Step: 6
Training loss: 2.387441635131836
Validation loss: 2.0286965190723376

Epoch: 6| Step: 7
Training loss: 2.539440631866455
Validation loss: 2.0391210996976463

Epoch: 6| Step: 8
Training loss: 2.2043166160583496
Validation loss: 2.0247637738463697

Epoch: 6| Step: 9
Training loss: 2.582892417907715
Validation loss: 2.0221139231035785

Epoch: 6| Step: 10
Training loss: 1.9604073762893677
Validation loss: 2.0268967600278955

Epoch: 6| Step: 11
Training loss: 1.6851986646652222
Validation loss: 2.018561391420262

Epoch: 6| Step: 12
Training loss: 2.237579822540283
Validation loss: 2.0134750963539205

Epoch: 6| Step: 13
Training loss: 1.793520212173462
Validation loss: 1.9934044038095782

Epoch: 126| Step: 0
Training loss: 1.8805068731307983
Validation loss: 1.9798999512067406

Epoch: 6| Step: 1
Training loss: 1.7680065631866455
Validation loss: 1.987865440307125

Epoch: 6| Step: 2
Training loss: 2.111706256866455
Validation loss: 2.016659435405526

Epoch: 6| Step: 3
Training loss: 3.14406681060791
Validation loss: 2.0161070913396855

Epoch: 6| Step: 4
Training loss: 2.7204973697662354
Validation loss: 2.0141659193141486

Epoch: 6| Step: 5
Training loss: 1.9708125591278076
Validation loss: 2.014255451899703

Epoch: 6| Step: 6
Training loss: 2.360743522644043
Validation loss: 2.0065618996979087

Epoch: 6| Step: 7
Training loss: 1.6670482158660889
Validation loss: 2.011074031552961

Epoch: 6| Step: 8
Training loss: 1.8190205097198486
Validation loss: 2.0241175018331057

Epoch: 6| Step: 9
Training loss: 2.3981575965881348
Validation loss: 2.0328720449119486

Epoch: 6| Step: 10
Training loss: 2.1000003814697266
Validation loss: 2.050766021974625

Epoch: 6| Step: 11
Training loss: 2.340719223022461
Validation loss: 2.055405757760489

Epoch: 6| Step: 12
Training loss: 2.1733875274658203
Validation loss: 2.0699739686904417

Epoch: 6| Step: 13
Training loss: 1.3666194677352905
Validation loss: 2.0651317757944905

Epoch: 127| Step: 0
Training loss: 2.3566062450408936
Validation loss: 2.0915603201876403

Epoch: 6| Step: 1
Training loss: 1.8494969606399536
Validation loss: 2.0981075661156767

Epoch: 6| Step: 2
Training loss: 2.1233508586883545
Validation loss: 2.0715206438495266

Epoch: 6| Step: 3
Training loss: 2.251746654510498
Validation loss: 2.07746373966176

Epoch: 6| Step: 4
Training loss: 1.6248571872711182
Validation loss: 2.0810963287148425

Epoch: 6| Step: 5
Training loss: 1.8450758457183838
Validation loss: 2.0620398021513417

Epoch: 6| Step: 6
Training loss: 2.6149349212646484
Validation loss: 2.0471543547927693

Epoch: 6| Step: 7
Training loss: 2.802307605743408
Validation loss: 2.0414827331419914

Epoch: 6| Step: 8
Training loss: 1.6507455110549927
Validation loss: 2.0381011680890153

Epoch: 6| Step: 9
Training loss: 1.914260745048523
Validation loss: 2.035502322258488

Epoch: 6| Step: 10
Training loss: 2.461487054824829
Validation loss: 2.0468476459544194

Epoch: 6| Step: 11
Training loss: 1.9405871629714966
Validation loss: 2.030554750914215

Epoch: 6| Step: 12
Training loss: 2.5620312690734863
Validation loss: 2.0491248074398247

Epoch: 6| Step: 13
Training loss: 1.6393930912017822
Validation loss: 2.0412372978784705

Epoch: 128| Step: 0
Training loss: 1.6149027347564697
Validation loss: 2.0450118228953373

Epoch: 6| Step: 1
Training loss: 2.238304853439331
Validation loss: 2.049209487053656

Epoch: 6| Step: 2
Training loss: 2.5192208290100098
Validation loss: 2.0544818537209624

Epoch: 6| Step: 3
Training loss: 1.9358073472976685
Validation loss: 2.0530007129074423

Epoch: 6| Step: 4
Training loss: 1.4175390005111694
Validation loss: 2.0472999541990218

Epoch: 6| Step: 5
Training loss: 2.106565237045288
Validation loss: 2.0447428226470947

Epoch: 6| Step: 6
Training loss: 2.298729658126831
Validation loss: 2.0554212203589817

Epoch: 6| Step: 7
Training loss: 1.8254482746124268
Validation loss: 2.049776554107666

Epoch: 6| Step: 8
Training loss: 2.0297720432281494
Validation loss: 2.0554653418961393

Epoch: 6| Step: 9
Training loss: 2.548358917236328
Validation loss: 2.0573155521064677

Epoch: 6| Step: 10
Training loss: 2.325270175933838
Validation loss: 2.0332103929212018

Epoch: 6| Step: 11
Training loss: 2.2376511096954346
Validation loss: 2.0356996379872805

Epoch: 6| Step: 12
Training loss: 2.3332643508911133
Validation loss: 2.0357887334721063

Epoch: 6| Step: 13
Training loss: 2.1229159832000732
Validation loss: 2.0626343719420897

Epoch: 129| Step: 0
Training loss: 2.343585968017578
Validation loss: 2.1256207778889644

Epoch: 6| Step: 1
Training loss: 1.789005160331726
Validation loss: 2.156166840625066

Epoch: 6| Step: 2
Training loss: 2.457587480545044
Validation loss: 2.150409975359517

Epoch: 6| Step: 3
Training loss: 1.5488324165344238
Validation loss: 2.0830880929065008

Epoch: 6| Step: 4
Training loss: 2.0519068241119385
Validation loss: 2.042022169277232

Epoch: 6| Step: 5
Training loss: 2.19199538230896
Validation loss: 2.0185288075477845

Epoch: 6| Step: 6
Training loss: 2.2167890071868896
Validation loss: 2.0223408552908126

Epoch: 6| Step: 7
Training loss: 1.9018135070800781
Validation loss: 2.03780298976488

Epoch: 6| Step: 8
Training loss: 2.544173240661621
Validation loss: 2.0562186087331464

Epoch: 6| Step: 9
Training loss: 1.9668961763381958
Validation loss: 2.0766197122553343

Epoch: 6| Step: 10
Training loss: 2.2757349014282227
Validation loss: 2.0588287589370564

Epoch: 6| Step: 11
Training loss: 2.25136137008667
Validation loss: 2.0589878918022237

Epoch: 6| Step: 12
Training loss: 2.4887218475341797
Validation loss: 2.0319099977452266

Epoch: 6| Step: 13
Training loss: 2.6824378967285156
Validation loss: 2.0293848283829226

Epoch: 130| Step: 0
Training loss: 2.524798631668091
Validation loss: 2.013170175654914

Epoch: 6| Step: 1
Training loss: 1.6423819065093994
Validation loss: 2.024498474213385

Epoch: 6| Step: 2
Training loss: 1.7494792938232422
Validation loss: 2.0122529768174693

Epoch: 6| Step: 3
Training loss: 1.8556711673736572
Validation loss: 2.015034128260869

Epoch: 6| Step: 4
Training loss: 1.637308955192566
Validation loss: 2.0178453024997505

Epoch: 6| Step: 5
Training loss: 1.6684459447860718
Validation loss: 2.0197351107033352

Epoch: 6| Step: 6
Training loss: 2.157461404800415
Validation loss: 2.025517850793818

Epoch: 6| Step: 7
Training loss: 2.0898585319519043
Validation loss: 2.0232363400920743

Epoch: 6| Step: 8
Training loss: 2.4860057830810547
Validation loss: 2.016332498160742

Epoch: 6| Step: 9
Training loss: 2.47067928314209
Validation loss: 2.0067702852269655

Epoch: 6| Step: 10
Training loss: 2.484429359436035
Validation loss: 2.0091802189427037

Epoch: 6| Step: 11
Training loss: 2.5992207527160645
Validation loss: 2.000949728873468

Epoch: 6| Step: 12
Training loss: 1.907499074935913
Validation loss: 2.013292145985429

Epoch: 6| Step: 13
Training loss: 2.6452596187591553
Validation loss: 2.035727059969338

Epoch: 131| Step: 0
Training loss: 2.291426420211792
Validation loss: 2.045122314524907

Epoch: 6| Step: 1
Training loss: 2.9996135234832764
Validation loss: 2.044509458285506

Epoch: 6| Step: 2
Training loss: 1.8801459074020386
Validation loss: 2.0347477082283265

Epoch: 6| Step: 3
Training loss: 3.1303720474243164
Validation loss: 2.0386250403619584

Epoch: 6| Step: 4
Training loss: 1.9562716484069824
Validation loss: 2.031432463276771

Epoch: 6| Step: 5
Training loss: 1.8934640884399414
Validation loss: 2.0322124573492233

Epoch: 6| Step: 6
Training loss: 2.1453299522399902
Validation loss: 2.012548041600053

Epoch: 6| Step: 7
Training loss: 1.37791907787323
Validation loss: 2.015657763327322

Epoch: 6| Step: 8
Training loss: 2.145505666732788
Validation loss: 1.9980747263918641

Epoch: 6| Step: 9
Training loss: 1.9221100807189941
Validation loss: 1.9982728496674569

Epoch: 6| Step: 10
Training loss: 1.4324941635131836
Validation loss: 1.9870945997135614

Epoch: 6| Step: 11
Training loss: 2.5239155292510986
Validation loss: 2.006202477280812

Epoch: 6| Step: 12
Training loss: 2.0375852584838867
Validation loss: 2.0080858328009166

Epoch: 6| Step: 13
Training loss: 1.7479101419448853
Validation loss: 2.0125521511159916

Epoch: 132| Step: 0
Training loss: 1.9640960693359375
Validation loss: 2.0308635029741513

Epoch: 6| Step: 1
Training loss: 2.2500226497650146
Validation loss: 2.0357805926312684

Epoch: 6| Step: 2
Training loss: 1.703200340270996
Validation loss: 2.06999570579939

Epoch: 6| Step: 3
Training loss: 2.4585719108581543
Validation loss: 2.1071930611005394

Epoch: 6| Step: 4
Training loss: 2.082353115081787
Validation loss: 2.123481160850935

Epoch: 6| Step: 5
Training loss: 1.8957788944244385
Validation loss: 2.1748464338241087

Epoch: 6| Step: 6
Training loss: 1.4163976907730103
Validation loss: 2.262135636421942

Epoch: 6| Step: 7
Training loss: 1.8394464254379272
Validation loss: 2.180353054436304

Epoch: 6| Step: 8
Training loss: 2.545485019683838
Validation loss: 2.093892502528365

Epoch: 6| Step: 9
Training loss: 2.56788969039917
Validation loss: 2.078311789420343

Epoch: 6| Step: 10
Training loss: 1.5608503818511963
Validation loss: 2.1027774733881794

Epoch: 6| Step: 11
Training loss: 2.5535929203033447
Validation loss: 2.123394380333603

Epoch: 6| Step: 12
Training loss: 2.7116565704345703
Validation loss: 2.1151066595508206

Epoch: 6| Step: 13
Training loss: 2.6785669326782227
Validation loss: 2.0915863449855516

Epoch: 133| Step: 0
Training loss: 2.4210033416748047
Validation loss: 2.074040841030818

Epoch: 6| Step: 1
Training loss: 1.1430721282958984
Validation loss: 2.067055162563119

Epoch: 6| Step: 2
Training loss: 1.6978298425674438
Validation loss: 2.037343543062928

Epoch: 6| Step: 3
Training loss: 2.032618284225464
Validation loss: 2.0146287077216694

Epoch: 6| Step: 4
Training loss: 2.9353537559509277
Validation loss: 2.00493259327386

Epoch: 6| Step: 5
Training loss: 2.034489870071411
Validation loss: 1.9971070879249162

Epoch: 6| Step: 6
Training loss: 1.992368221282959
Validation loss: 1.994247523687219

Epoch: 6| Step: 7
Training loss: 2.217751979827881
Validation loss: 1.990845416181831

Epoch: 6| Step: 8
Training loss: 2.5187020301818848
Validation loss: 1.9824386860734673

Epoch: 6| Step: 9
Training loss: 2.206042528152466
Validation loss: 1.9880898690992785

Epoch: 6| Step: 10
Training loss: 2.444969892501831
Validation loss: 1.997551243792298

Epoch: 6| Step: 11
Training loss: 1.740017294883728
Validation loss: 2.001138664061023

Epoch: 6| Step: 12
Training loss: 2.3688430786132812
Validation loss: 2.037695336085494

Epoch: 6| Step: 13
Training loss: 1.9590191841125488
Validation loss: 2.071995791568551

Epoch: 134| Step: 0
Training loss: 2.5170681476593018
Validation loss: 2.108610422380509

Epoch: 6| Step: 1
Training loss: 2.122202157974243
Validation loss: 2.150919593790526

Epoch: 6| Step: 2
Training loss: 2.911724805831909
Validation loss: 2.1557600344381025

Epoch: 6| Step: 3
Training loss: 1.8088927268981934
Validation loss: 2.149360469592515

Epoch: 6| Step: 4
Training loss: 1.6656155586242676
Validation loss: 2.087943030941871

Epoch: 6| Step: 5
Training loss: 1.7473042011260986
Validation loss: 2.0891830357172156

Epoch: 6| Step: 6
Training loss: 1.5240004062652588
Validation loss: 2.088598334661094

Epoch: 6| Step: 7
Training loss: 3.021587610244751
Validation loss: 2.128549846269751

Epoch: 6| Step: 8
Training loss: 2.4191267490386963
Validation loss: 2.1740765315230175

Epoch: 6| Step: 9
Training loss: 2.4856252670288086
Validation loss: 2.221040689817039

Epoch: 6| Step: 10
Training loss: 2.4930386543273926
Validation loss: 2.1938173629904307

Epoch: 6| Step: 11
Training loss: 2.0529346466064453
Validation loss: 2.1453189926762737

Epoch: 6| Step: 12
Training loss: 1.5612530708312988
Validation loss: 2.120446184630035

Epoch: 6| Step: 13
Training loss: 2.1229617595672607
Validation loss: 2.0810091674968763

Epoch: 135| Step: 0
Training loss: 2.1006476879119873
Validation loss: 2.0991028419104953

Epoch: 6| Step: 1
Training loss: 2.249013900756836
Validation loss: 2.1300173497969106

Epoch: 6| Step: 2
Training loss: 2.252371311187744
Validation loss: 2.1457594697193434

Epoch: 6| Step: 3
Training loss: 2.147891044616699
Validation loss: 2.1741718835728143

Epoch: 6| Step: 4
Training loss: 2.152970314025879
Validation loss: 2.1932450443185787

Epoch: 6| Step: 5
Training loss: 2.3494818210601807
Validation loss: 2.2119654981038903

Epoch: 6| Step: 6
Training loss: 1.8323177099227905
Validation loss: 2.1978991826375327

Epoch: 6| Step: 7
Training loss: 2.5058510303497314
Validation loss: 2.1673334260140695

Epoch: 6| Step: 8
Training loss: 1.7591352462768555
Validation loss: 2.138519375554977

Epoch: 6| Step: 9
Training loss: 2.043715000152588
Validation loss: 2.120686977140365

Epoch: 6| Step: 10
Training loss: 2.6050310134887695
Validation loss: 2.089023003014185

Epoch: 6| Step: 11
Training loss: 1.9282509088516235
Validation loss: 2.030968196930424

Epoch: 6| Step: 12
Training loss: 1.8178054094314575
Validation loss: 2.068659738827777

Epoch: 6| Step: 13
Training loss: 1.935313105583191
Validation loss: 2.187622643286182

Epoch: 136| Step: 0
Training loss: 2.5290398597717285
Validation loss: 2.381503317945747

Epoch: 6| Step: 1
Training loss: 2.826813220977783
Validation loss: 2.4033974293739564

Epoch: 6| Step: 2
Training loss: 2.5733909606933594
Validation loss: 2.2583609422047934

Epoch: 6| Step: 3
Training loss: 2.289675235748291
Validation loss: 2.1081925822842504

Epoch: 6| Step: 4
Training loss: 1.1949383020401
Validation loss: 2.0602937641964165

Epoch: 6| Step: 5
Training loss: 2.194760322570801
Validation loss: 2.0207399860505135

Epoch: 6| Step: 6
Training loss: 2.279566526412964
Validation loss: 2.0160163089793217

Epoch: 6| Step: 7
Training loss: 2.1506776809692383
Validation loss: 2.034556568309825

Epoch: 6| Step: 8
Training loss: 2.3929789066314697
Validation loss: 2.123297296544557

Epoch: 6| Step: 9
Training loss: 2.022477149963379
Validation loss: 2.1698823231522755

Epoch: 6| Step: 10
Training loss: 2.3891561031341553
Validation loss: 2.1405003788650676

Epoch: 6| Step: 11
Training loss: 1.8175407648086548
Validation loss: 2.094242062619937

Epoch: 6| Step: 12
Training loss: 2.066401720046997
Validation loss: 2.042772821200791

Epoch: 6| Step: 13
Training loss: 2.2162084579467773
Validation loss: 2.0267338368200485

Epoch: 137| Step: 0
Training loss: 1.3959579467773438
Validation loss: 2.0005013455626783

Epoch: 6| Step: 1
Training loss: 2.4780893325805664
Validation loss: 2.0031943833956154

Epoch: 6| Step: 2
Training loss: 2.5220847129821777
Validation loss: 1.9877479589113625

Epoch: 6| Step: 3
Training loss: 2.0010986328125
Validation loss: 1.9849864949462235

Epoch: 6| Step: 4
Training loss: 2.8020403385162354
Validation loss: 2.0044323167493268

Epoch: 6| Step: 5
Training loss: 2.5211305618286133
Validation loss: 1.9878012185455651

Epoch: 6| Step: 6
Training loss: 1.6742489337921143
Validation loss: 2.010391817297987

Epoch: 6| Step: 7
Training loss: 1.4539692401885986
Validation loss: 2.005561077466575

Epoch: 6| Step: 8
Training loss: 2.445535659790039
Validation loss: 2.008670065992622

Epoch: 6| Step: 9
Training loss: 2.2798519134521484
Validation loss: 2.0064234631035918

Epoch: 6| Step: 10
Training loss: 1.5394420623779297
Validation loss: 2.022790567849272

Epoch: 6| Step: 11
Training loss: 2.2397665977478027
Validation loss: 2.030970725961911

Epoch: 6| Step: 12
Training loss: 1.6913503408432007
Validation loss: 2.0344248458903325

Epoch: 6| Step: 13
Training loss: 1.7556837797164917
Validation loss: 2.0420237843708327

Epoch: 138| Step: 0
Training loss: 1.580976963043213
Validation loss: 2.0493595151491064

Epoch: 6| Step: 1
Training loss: 2.8202786445617676
Validation loss: 2.0522039654434368

Epoch: 6| Step: 2
Training loss: 1.8534510135650635
Validation loss: 2.068212624519102

Epoch: 6| Step: 3
Training loss: 2.3788695335388184
Validation loss: 2.057853767948766

Epoch: 6| Step: 4
Training loss: 1.646146297454834
Validation loss: 2.0635498057129564

Epoch: 6| Step: 5
Training loss: 1.8869802951812744
Validation loss: 2.0547863334737797

Epoch: 6| Step: 6
Training loss: 1.3823275566101074
Validation loss: 2.0629080085344214

Epoch: 6| Step: 7
Training loss: 2.360323905944824
Validation loss: 2.0671222697022142

Epoch: 6| Step: 8
Training loss: 2.1633729934692383
Validation loss: 2.067899832161524

Epoch: 6| Step: 9
Training loss: 1.3583565950393677
Validation loss: 2.0593537181936283

Epoch: 6| Step: 10
Training loss: 2.0806822776794434
Validation loss: 2.054317897365939

Epoch: 6| Step: 11
Training loss: 2.366180419921875
Validation loss: 2.054270636650824

Epoch: 6| Step: 12
Training loss: 2.8244378566741943
Validation loss: 2.046834848260367

Epoch: 6| Step: 13
Training loss: 1.3679804801940918
Validation loss: 2.0663289229075112

Epoch: 139| Step: 0
Training loss: 2.2246155738830566
Validation loss: 2.0905214163564865

Epoch: 6| Step: 1
Training loss: 2.2789878845214844
Validation loss: 2.074320149678056

Epoch: 6| Step: 2
Training loss: 1.7809348106384277
Validation loss: 2.0533507152270247

Epoch: 6| Step: 3
Training loss: 1.2283132076263428
Validation loss: 2.034181280802655

Epoch: 6| Step: 4
Training loss: 2.096282958984375
Validation loss: 2.0082892628126245

Epoch: 6| Step: 5
Training loss: 1.5532243251800537
Validation loss: 1.9970260281716623

Epoch: 6| Step: 6
Training loss: 2.140864372253418
Validation loss: 2.0108829288072485

Epoch: 6| Step: 7
Training loss: 2.0950188636779785
Validation loss: 2.0067338866572224

Epoch: 6| Step: 8
Training loss: 1.6267991065979004
Validation loss: 2.0159203724194596

Epoch: 6| Step: 9
Training loss: 1.9191184043884277
Validation loss: 2.0212087810680432

Epoch: 6| Step: 10
Training loss: 2.7111735343933105
Validation loss: 2.047145930669641

Epoch: 6| Step: 11
Training loss: 2.7096219062805176
Validation loss: 2.0518028377204813

Epoch: 6| Step: 12
Training loss: 2.21913480758667
Validation loss: 2.065878539957026

Epoch: 6| Step: 13
Training loss: 2.315646171569824
Validation loss: 2.074754221464998

Epoch: 140| Step: 0
Training loss: 2.2695422172546387
Validation loss: 2.062372374278243

Epoch: 6| Step: 1
Training loss: 1.8490049839019775
Validation loss: 2.0580250998978973

Epoch: 6| Step: 2
Training loss: 2.176607131958008
Validation loss: 2.0756827054485196

Epoch: 6| Step: 3
Training loss: 1.178743600845337
Validation loss: 2.0904997279567104

Epoch: 6| Step: 4
Training loss: 3.122588634490967
Validation loss: 2.070514021381255

Epoch: 6| Step: 5
Training loss: 1.9651899337768555
Validation loss: 2.0559881220581713

Epoch: 6| Step: 6
Training loss: 2.0194320678710938
Validation loss: 2.0356212354475454

Epoch: 6| Step: 7
Training loss: 1.813962459564209
Validation loss: 2.0260326247061453

Epoch: 6| Step: 8
Training loss: 1.1088533401489258
Validation loss: 2.0489580041618756

Epoch: 6| Step: 9
Training loss: 1.6907049417495728
Validation loss: 2.043733481437929

Epoch: 6| Step: 10
Training loss: 2.4187817573547363
Validation loss: 2.05299727378353

Epoch: 6| Step: 11
Training loss: 1.7530779838562012
Validation loss: 2.056502688315607

Epoch: 6| Step: 12
Training loss: 2.1464219093322754
Validation loss: 2.061959917827319

Epoch: 6| Step: 13
Training loss: 2.5337865352630615
Validation loss: 2.048725402483376

Epoch: 141| Step: 0
Training loss: 1.4089255332946777
Validation loss: 2.059103035157727

Epoch: 6| Step: 1
Training loss: 1.946960687637329
Validation loss: 2.044592647142308

Epoch: 6| Step: 2
Training loss: 1.9158958196640015
Validation loss: 2.0327227192540325

Epoch: 6| Step: 3
Training loss: 2.491438388824463
Validation loss: 2.017506778881114

Epoch: 6| Step: 4
Training loss: 1.8426892757415771
Validation loss: 2.0176769200191704

Epoch: 6| Step: 5
Training loss: 2.344993829727173
Validation loss: 2.014524413693336

Epoch: 6| Step: 6
Training loss: 2.0504565238952637
Validation loss: 2.014095475596766

Epoch: 6| Step: 7
Training loss: 1.5926339626312256
Validation loss: 2.0234917081812376

Epoch: 6| Step: 8
Training loss: 1.8922629356384277
Validation loss: 2.0259362920638053

Epoch: 6| Step: 9
Training loss: 1.9151248931884766
Validation loss: 2.0516477912984867

Epoch: 6| Step: 10
Training loss: 2.30755615234375
Validation loss: 2.0444999766606156

Epoch: 6| Step: 11
Training loss: 2.136242389678955
Validation loss: 2.0417296463443386

Epoch: 6| Step: 12
Training loss: 2.5391716957092285
Validation loss: 2.030332024379443

Epoch: 6| Step: 13
Training loss: 1.30530846118927
Validation loss: 2.024648897109493

Epoch: 142| Step: 0
Training loss: 2.2409703731536865
Validation loss: 2.0242876852712324

Epoch: 6| Step: 1
Training loss: 1.9055182933807373
Validation loss: 2.0455846837771836

Epoch: 6| Step: 2
Training loss: 1.3222908973693848
Validation loss: 2.03605911039537

Epoch: 6| Step: 3
Training loss: 3.1834256649017334
Validation loss: 2.0362825188585507

Epoch: 6| Step: 4
Training loss: 2.2700023651123047
Validation loss: 2.042182812126734

Epoch: 6| Step: 5
Training loss: 1.9134726524353027
Validation loss: 2.0357922725780035

Epoch: 6| Step: 6
Training loss: 2.8010237216949463
Validation loss: 2.043902699665357

Epoch: 6| Step: 7
Training loss: 2.30918550491333
Validation loss: 2.0408819760045698

Epoch: 6| Step: 8
Training loss: 1.5560230016708374
Validation loss: 2.0341417251094693

Epoch: 6| Step: 9
Training loss: 1.929917812347412
Validation loss: 2.0466423060304377

Epoch: 6| Step: 10
Training loss: 1.4713349342346191
Validation loss: 2.034186315792863

Epoch: 6| Step: 11
Training loss: 1.295595645904541
Validation loss: 2.027451815143708

Epoch: 6| Step: 12
Training loss: 1.4928808212280273
Validation loss: 2.036347168748097

Epoch: 6| Step: 13
Training loss: 1.5573612451553345
Validation loss: 2.028780086066133

Epoch: 143| Step: 0
Training loss: 2.506990432739258
Validation loss: 2.0305795015827304

Epoch: 6| Step: 1
Training loss: 1.7303917407989502
Validation loss: 2.0191621806031916

Epoch: 6| Step: 2
Training loss: 1.720671534538269
Validation loss: 2.012107491493225

Epoch: 6| Step: 3
Training loss: 2.0140798091888428
Validation loss: 1.9922338711318148

Epoch: 6| Step: 4
Training loss: 2.1673483848571777
Validation loss: 1.98288393533358

Epoch: 6| Step: 5
Training loss: 1.8582189083099365
Validation loss: 1.9976107497369089

Epoch: 6| Step: 6
Training loss: 1.5431900024414062
Validation loss: 1.9877767896139493

Epoch: 6| Step: 7
Training loss: 1.5903335809707642
Validation loss: 2.01019129830022

Epoch: 6| Step: 8
Training loss: 2.005356550216675
Validation loss: 2.0095535209102016

Epoch: 6| Step: 9
Training loss: 1.6364948749542236
Validation loss: 2.028581591062648

Epoch: 6| Step: 10
Training loss: 2.362124443054199
Validation loss: 2.0561377950893935

Epoch: 6| Step: 11
Training loss: 2.529017448425293
Validation loss: 2.0644705782654467

Epoch: 6| Step: 12
Training loss: 1.6514750719070435
Validation loss: 2.0786398559488277

Epoch: 6| Step: 13
Training loss: 1.704021692276001
Validation loss: 2.087437182344416

Epoch: 144| Step: 0
Training loss: 2.2141900062561035
Validation loss: 2.1126595850913756

Epoch: 6| Step: 1
Training loss: 2.3379271030426025
Validation loss: 2.125481720893614

Epoch: 6| Step: 2
Training loss: 1.5257327556610107
Validation loss: 2.126650841005387

Epoch: 6| Step: 3
Training loss: 1.9230085611343384
Validation loss: 2.113149207125428

Epoch: 6| Step: 4
Training loss: 1.6721259355545044
Validation loss: 2.112383952704809

Epoch: 6| Step: 5
Training loss: 2.348361015319824
Validation loss: 2.1339691685092066

Epoch: 6| Step: 6
Training loss: 2.804661273956299
Validation loss: 2.1215919551029

Epoch: 6| Step: 7
Training loss: 1.4550762176513672
Validation loss: 2.111100883894069

Epoch: 6| Step: 8
Training loss: 2.329616069793701
Validation loss: 2.069124939621136

Epoch: 6| Step: 9
Training loss: 2.13167667388916
Validation loss: 2.0647703152830883

Epoch: 6| Step: 10
Training loss: 1.4944943189620972
Validation loss: 2.0411549460503364

Epoch: 6| Step: 11
Training loss: 1.956860065460205
Validation loss: 2.0726530410910167

Epoch: 6| Step: 12
Training loss: 1.5208895206451416
Validation loss: 2.0576817168984363

Epoch: 6| Step: 13
Training loss: 1.7009812593460083
Validation loss: 2.0628113669733845

Epoch: 145| Step: 0
Training loss: 2.0665102005004883
Validation loss: 2.0612538322325675

Epoch: 6| Step: 1
Training loss: 1.8847771883010864
Validation loss: 2.0493584268836567

Epoch: 6| Step: 2
Training loss: 1.6790746450424194
Validation loss: 2.0577385015385126

Epoch: 6| Step: 3
Training loss: 2.116766929626465
Validation loss: 2.0505608640691286

Epoch: 6| Step: 4
Training loss: 2.3328857421875
Validation loss: 2.075325809499269

Epoch: 6| Step: 5
Training loss: 2.2311861515045166
Validation loss: 2.0833470539380143

Epoch: 6| Step: 6
Training loss: 1.5778852701187134
Validation loss: 2.09024513793248

Epoch: 6| Step: 7
Training loss: 1.9896693229675293
Validation loss: 2.0890373824745097

Epoch: 6| Step: 8
Training loss: 2.10648775100708
Validation loss: 2.1111371722272647

Epoch: 6| Step: 9
Training loss: 2.224559783935547
Validation loss: 2.1027905146280923

Epoch: 6| Step: 10
Training loss: 2.1719794273376465
Validation loss: 2.090833931840876

Epoch: 6| Step: 11
Training loss: 1.3388644456863403
Validation loss: 2.0794942866089525

Epoch: 6| Step: 12
Training loss: 1.6801396608352661
Validation loss: 2.070971970917076

Epoch: 6| Step: 13
Training loss: 2.2957849502563477
Validation loss: 2.04405959703589

Epoch: 146| Step: 0
Training loss: 1.7253438234329224
Validation loss: 2.050495078486781

Epoch: 6| Step: 1
Training loss: 1.9540319442749023
Validation loss: 2.032997250556946

Epoch: 6| Step: 2
Training loss: 2.1552929878234863
Validation loss: 2.0450369888736355

Epoch: 6| Step: 3
Training loss: 2.0133965015411377
Validation loss: 2.0447151276373092

Epoch: 6| Step: 4
Training loss: 1.861289381980896
Validation loss: 2.0445831591083157

Epoch: 6| Step: 5
Training loss: 1.6774249076843262
Validation loss: 2.042469614295549

Epoch: 6| Step: 6
Training loss: 1.4433716535568237
Validation loss: 2.034353477980501

Epoch: 6| Step: 7
Training loss: 1.434973955154419
Validation loss: 2.036831554546151

Epoch: 6| Step: 8
Training loss: 1.9488309621810913
Validation loss: 2.0271479711737683

Epoch: 6| Step: 9
Training loss: 2.7015373706817627
Validation loss: 2.0300633984227336

Epoch: 6| Step: 10
Training loss: 1.995308518409729
Validation loss: 2.044362098939957

Epoch: 6| Step: 11
Training loss: 2.041369915008545
Validation loss: 2.0325161128915767

Epoch: 6| Step: 12
Training loss: 2.082606792449951
Validation loss: 2.013584480490736

Epoch: 6| Step: 13
Training loss: 2.0395419597625732
Validation loss: 2.025756928228563

Epoch: 147| Step: 0
Training loss: 2.1027321815490723
Validation loss: 2.0145361244037585

Epoch: 6| Step: 1
Training loss: 2.0764102935791016
Validation loss: 2.011885284095682

Epoch: 6| Step: 2
Training loss: 1.872734785079956
Validation loss: 2.018286494798558

Epoch: 6| Step: 3
Training loss: 1.8024626970291138
Validation loss: 2.05038450097525

Epoch: 6| Step: 4
Training loss: 1.802321434020996
Validation loss: 2.0418247727937597

Epoch: 6| Step: 5
Training loss: 2.197349786758423
Validation loss: 2.0328995566214285

Epoch: 6| Step: 6
Training loss: 1.8662647008895874
Validation loss: 2.0392302415704213

Epoch: 6| Step: 7
Training loss: 1.6580207347869873
Validation loss: 2.0229274457500828

Epoch: 6| Step: 8
Training loss: 1.3244836330413818
Validation loss: 2.020913149720879

Epoch: 6| Step: 9
Training loss: 2.042661666870117
Validation loss: 2.029753881116067

Epoch: 6| Step: 10
Training loss: 2.4048123359680176
Validation loss: 2.0502979447764735

Epoch: 6| Step: 11
Training loss: 1.5783710479736328
Validation loss: 2.0592474437529042

Epoch: 6| Step: 12
Training loss: 1.8202351331710815
Validation loss: 2.061973346176968

Epoch: 6| Step: 13
Training loss: 2.3360114097595215
Validation loss: 2.0934552377270115

Epoch: 148| Step: 0
Training loss: 1.8825708627700806
Validation loss: 2.103067498053274

Epoch: 6| Step: 1
Training loss: 2.5808117389678955
Validation loss: 2.09473680937162

Epoch: 6| Step: 2
Training loss: 1.8509836196899414
Validation loss: 2.0880220602917414

Epoch: 6| Step: 3
Training loss: 2.395660400390625
Validation loss: 2.1039102615848666

Epoch: 6| Step: 4
Training loss: 1.7675402164459229
Validation loss: 2.0916413325135426

Epoch: 6| Step: 5
Training loss: 1.9187431335449219
Validation loss: 2.0729184637787523

Epoch: 6| Step: 6
Training loss: 2.0880491733551025
Validation loss: 2.0539076251368367

Epoch: 6| Step: 7
Training loss: 1.3232252597808838
Validation loss: 2.0616198252606135

Epoch: 6| Step: 8
Training loss: 1.895593285560608
Validation loss: 2.0761416368587042

Epoch: 6| Step: 9
Training loss: 1.562307357788086
Validation loss: 2.0984750191370645

Epoch: 6| Step: 10
Training loss: 2.335768461227417
Validation loss: 2.123858449279621

Epoch: 6| Step: 11
Training loss: 1.3126814365386963
Validation loss: 2.125967735885292

Epoch: 6| Step: 12
Training loss: 1.9241628646850586
Validation loss: 2.1094846289644957

Epoch: 6| Step: 13
Training loss: 1.3659588098526
Validation loss: 2.1114877705932944

Epoch: 149| Step: 0
Training loss: 1.6823923587799072
Validation loss: 2.096021213839131

Epoch: 6| Step: 1
Training loss: 1.5794918537139893
Validation loss: 2.1046219974435787

Epoch: 6| Step: 2
Training loss: 2.2576630115509033
Validation loss: 2.1262585604062645

Epoch: 6| Step: 3
Training loss: 1.8009412288665771
Validation loss: 2.1326874020279094

Epoch: 6| Step: 4
Training loss: 1.749446988105774
Validation loss: 2.1293876273657686

Epoch: 6| Step: 5
Training loss: 1.9131667613983154
Validation loss: 2.121864180411062

Epoch: 6| Step: 6
Training loss: 1.7810454368591309
Validation loss: 2.0933263096758115

Epoch: 6| Step: 7
Training loss: 2.481112480163574
Validation loss: 2.084504065975066

Epoch: 6| Step: 8
Training loss: 1.4155545234680176
Validation loss: 2.038428862889608

Epoch: 6| Step: 9
Training loss: 1.8957360982894897
Validation loss: 2.041920003070626

Epoch: 6| Step: 10
Training loss: 2.1654248237609863
Validation loss: 2.0221404106386247

Epoch: 6| Step: 11
Training loss: 2.221656560897827
Validation loss: 2.0226646802758657

Epoch: 6| Step: 12
Training loss: 1.9484946727752686
Validation loss: 2.0026215519956363

Epoch: 6| Step: 13
Training loss: 1.3360190391540527
Validation loss: 2.0140953525420158

Epoch: 150| Step: 0
Training loss: 1.8285164833068848
Validation loss: 2.0190882682800293

Epoch: 6| Step: 1
Training loss: 2.2030818462371826
Validation loss: 2.0077723392876248

Epoch: 6| Step: 2
Training loss: 2.448260545730591
Validation loss: 2.033958776022798

Epoch: 6| Step: 3
Training loss: 1.663414716720581
Validation loss: 2.0313813186460927

Epoch: 6| Step: 4
Training loss: 2.5996909141540527
Validation loss: 2.0668700010545793

Epoch: 6| Step: 5
Training loss: 2.060368299484253
Validation loss: 2.0855876322715514

Epoch: 6| Step: 6
Training loss: 1.6116465330123901
Validation loss: 2.096343640358217

Epoch: 6| Step: 7
Training loss: 1.003234624862671
Validation loss: 2.077958127503754

Epoch: 6| Step: 8
Training loss: 2.0841777324676514
Validation loss: 2.0527705313057028

Epoch: 6| Step: 9
Training loss: 2.0007450580596924
Validation loss: 2.0711825611770793

Epoch: 6| Step: 10
Training loss: 1.84405517578125
Validation loss: 2.0779169092896166

Epoch: 6| Step: 11
Training loss: 1.5061169862747192
Validation loss: 2.067170609710037

Epoch: 6| Step: 12
Training loss: 1.605922818183899
Validation loss: 2.0741606284213323

Epoch: 6| Step: 13
Training loss: 1.42452073097229
Validation loss: 2.095413992481847

Epoch: 151| Step: 0
Training loss: 2.40564227104187
Validation loss: 2.1060075606069257

Epoch: 6| Step: 1
Training loss: 2.0421507358551025
Validation loss: 2.124275046010171

Epoch: 6| Step: 2
Training loss: 1.3721873760223389
Validation loss: 2.1528769975067465

Epoch: 6| Step: 3
Training loss: 1.593005895614624
Validation loss: 2.1196143614348544

Epoch: 6| Step: 4
Training loss: 2.367612838745117
Validation loss: 2.102567349710772

Epoch: 6| Step: 5
Training loss: 1.7442554235458374
Validation loss: 2.1073784776913222

Epoch: 6| Step: 6
Training loss: 1.6324889659881592
Validation loss: 2.102279923295462

Epoch: 6| Step: 7
Training loss: 1.8698019981384277
Validation loss: 2.0999349291606615

Epoch: 6| Step: 8
Training loss: 1.7633306980133057
Validation loss: 2.116948696874803

Epoch: 6| Step: 9
Training loss: 1.4616827964782715
Validation loss: 2.1115169576419297

Epoch: 6| Step: 10
Training loss: 1.7725791931152344
Validation loss: 2.084391200414268

Epoch: 6| Step: 11
Training loss: 2.345522165298462
Validation loss: 2.0629036247089343

Epoch: 6| Step: 12
Training loss: 2.1724507808685303
Validation loss: 2.037872278562156

Epoch: 6| Step: 13
Training loss: 1.957155466079712
Validation loss: 2.0095018199695054

Epoch: 152| Step: 0
Training loss: 1.9234883785247803
Validation loss: 2.032475212568878

Epoch: 6| Step: 1
Training loss: 1.1018102169036865
Validation loss: 2.0298359265891452

Epoch: 6| Step: 2
Training loss: 2.1543660163879395
Validation loss: 2.045762050536371

Epoch: 6| Step: 3
Training loss: 1.7110350131988525
Validation loss: 2.0492597139009865

Epoch: 6| Step: 4
Training loss: 1.3779044151306152
Validation loss: 2.0635024873159264

Epoch: 6| Step: 5
Training loss: 1.6739238500595093
Validation loss: 2.0421598098611318

Epoch: 6| Step: 6
Training loss: 1.6403865814208984
Validation loss: 2.0877443885290496

Epoch: 6| Step: 7
Training loss: 1.5852090120315552
Validation loss: 2.1078651951205347

Epoch: 6| Step: 8
Training loss: 2.443075656890869
Validation loss: 2.166523947510668

Epoch: 6| Step: 9
Training loss: 1.9720957279205322
Validation loss: 2.168030395302721

Epoch: 6| Step: 10
Training loss: 2.8404054641723633
Validation loss: 2.179294499017859

Epoch: 6| Step: 11
Training loss: 2.0216407775878906
Validation loss: 2.1450493694633566

Epoch: 6| Step: 12
Training loss: 1.6941944360733032
Validation loss: 2.1380048951795025

Epoch: 6| Step: 13
Training loss: 1.359185814857483
Validation loss: 2.107628365998627

Epoch: 153| Step: 0
Training loss: 2.288112163543701
Validation loss: 2.070770982773073

Epoch: 6| Step: 1
Training loss: 2.1186485290527344
Validation loss: 2.0749651308982604

Epoch: 6| Step: 2
Training loss: 2.0740151405334473
Validation loss: 2.067299276269892

Epoch: 6| Step: 3
Training loss: 1.7633476257324219
Validation loss: 2.0583598818830264

Epoch: 6| Step: 4
Training loss: 1.801223874092102
Validation loss: 2.0705801774096746

Epoch: 6| Step: 5
Training loss: 1.6816887855529785
Validation loss: 2.0780975510997157

Epoch: 6| Step: 6
Training loss: 1.8388137817382812
Validation loss: 2.083716315607871

Epoch: 6| Step: 7
Training loss: 1.5665533542633057
Validation loss: 2.073047367475366

Epoch: 6| Step: 8
Training loss: 1.8753174543380737
Validation loss: 2.079458046984929

Epoch: 6| Step: 9
Training loss: 1.6044284105300903
Validation loss: 2.0547428092648907

Epoch: 6| Step: 10
Training loss: 1.815774917602539
Validation loss: 2.0392031797798733

Epoch: 6| Step: 11
Training loss: 1.188507080078125
Validation loss: 2.0456721782684326

Epoch: 6| Step: 12
Training loss: 2.110412836074829
Validation loss: 2.0736286268439343

Epoch: 6| Step: 13
Training loss: 2.1534793376922607
Validation loss: 2.086194566501084

Epoch: 154| Step: 0
Training loss: 1.821149468421936
Validation loss: 2.0827897223093177

Epoch: 6| Step: 1
Training loss: 1.6863876581192017
Validation loss: 2.0661516343393633

Epoch: 6| Step: 2
Training loss: 1.4860525131225586
Validation loss: 2.0722974410621067

Epoch: 6| Step: 3
Training loss: 1.7395555973052979
Validation loss: 2.0830617848262993

Epoch: 6| Step: 4
Training loss: 1.457594394683838
Validation loss: 2.0903112426880868

Epoch: 6| Step: 5
Training loss: 1.5184576511383057
Validation loss: 2.1170393138803463

Epoch: 6| Step: 6
Training loss: 2.516287326812744
Validation loss: 2.118774644790157

Epoch: 6| Step: 7
Training loss: 2.297579765319824
Validation loss: 2.11992076904543

Epoch: 6| Step: 8
Training loss: 2.0443403720855713
Validation loss: 2.103619872882802

Epoch: 6| Step: 9
Training loss: 1.5102405548095703
Validation loss: 2.098860889352778

Epoch: 6| Step: 10
Training loss: 1.525099515914917
Validation loss: 2.0957128411980084

Epoch: 6| Step: 11
Training loss: 1.8160170316696167
Validation loss: 2.0853468730885494

Epoch: 6| Step: 12
Training loss: 2.1433584690093994
Validation loss: 2.082328772032133

Epoch: 6| Step: 13
Training loss: 1.2672570943832397
Validation loss: 2.0678758672488633

Epoch: 155| Step: 0
Training loss: 1.571099042892456
Validation loss: 2.0644368869002148

Epoch: 6| Step: 1
Training loss: 1.6830739974975586
Validation loss: 2.062977453713776

Epoch: 6| Step: 2
Training loss: 1.369771957397461
Validation loss: 2.0642354014099284

Epoch: 6| Step: 3
Training loss: 1.828603744506836
Validation loss: 2.0486632559889104

Epoch: 6| Step: 4
Training loss: 2.087695837020874
Validation loss: 2.060380757495921

Epoch: 6| Step: 5
Training loss: 1.5115940570831299
Validation loss: 2.0796002085490892

Epoch: 6| Step: 6
Training loss: 2.2636616230010986
Validation loss: 2.069715999787854

Epoch: 6| Step: 7
Training loss: 2.366973638534546
Validation loss: 2.050333140998758

Epoch: 6| Step: 8
Training loss: 1.3355005979537964
Validation loss: 2.0175284800990934

Epoch: 6| Step: 9
Training loss: 2.081972122192383
Validation loss: 2.001388870259767

Epoch: 6| Step: 10
Training loss: 1.9557151794433594
Validation loss: 2.0059600260949906

Epoch: 6| Step: 11
Training loss: 1.5420604944229126
Validation loss: 2.004555515063706

Epoch: 6| Step: 12
Training loss: 1.6917595863342285
Validation loss: 2.0147163124494654

Epoch: 6| Step: 13
Training loss: 1.7765214443206787
Validation loss: 2.0136285546005412

Epoch: 156| Step: 0
Training loss: 1.4407360553741455
Validation loss: 2.043809179336794

Epoch: 6| Step: 1
Training loss: 1.3814595937728882
Validation loss: 2.0505668552972938

Epoch: 6| Step: 2
Training loss: 1.7291516065597534
Validation loss: 2.0625517188861804

Epoch: 6| Step: 3
Training loss: 2.037522315979004
Validation loss: 2.0963626164262013

Epoch: 6| Step: 4
Training loss: 1.9884605407714844
Validation loss: 2.089636969309981

Epoch: 6| Step: 5
Training loss: 1.2988574504852295
Validation loss: 2.0858755547513246

Epoch: 6| Step: 6
Training loss: 1.9438890218734741
Validation loss: 2.0988859348399664

Epoch: 6| Step: 7
Training loss: 1.5827871561050415
Validation loss: 2.1120059285112607

Epoch: 6| Step: 8
Training loss: 1.9486993551254272
Validation loss: 2.1223608306659165

Epoch: 6| Step: 9
Training loss: 2.201301336288452
Validation loss: 2.115650882003128

Epoch: 6| Step: 10
Training loss: 1.964749813079834
Validation loss: 2.080036458148751

Epoch: 6| Step: 11
Training loss: 1.849941372871399
Validation loss: 2.059371256059216

Epoch: 6| Step: 12
Training loss: 1.2175878286361694
Validation loss: 2.036747838861199

Epoch: 6| Step: 13
Training loss: 2.2769885063171387
Validation loss: 2.015856671076949

Epoch: 157| Step: 0
Training loss: 1.8015618324279785
Validation loss: 2.0187550693429928

Epoch: 6| Step: 1
Training loss: 1.901831030845642
Validation loss: 2.0442042760951544

Epoch: 6| Step: 2
Training loss: 2.551110029220581
Validation loss: 2.0481064550338255

Epoch: 6| Step: 3
Training loss: 1.316843032836914
Validation loss: 2.046131921070878

Epoch: 6| Step: 4
Training loss: 1.7087663412094116
Validation loss: 2.0563157399495444

Epoch: 6| Step: 5
Training loss: 1.9319486618041992
Validation loss: 2.0324268469246487

Epoch: 6| Step: 6
Training loss: 0.9856294393539429
Validation loss: 2.0363118763892882

Epoch: 6| Step: 7
Training loss: 2.049239158630371
Validation loss: 2.0371183118512555

Epoch: 6| Step: 8
Training loss: 1.801924228668213
Validation loss: 2.060322320589455

Epoch: 6| Step: 9
Training loss: 1.8928964138031006
Validation loss: 2.074568445964526

Epoch: 6| Step: 10
Training loss: 1.3991105556488037
Validation loss: 2.0903845833193873

Epoch: 6| Step: 11
Training loss: 1.8702527284622192
Validation loss: 2.0954807304566905

Epoch: 6| Step: 12
Training loss: 1.3754520416259766
Validation loss: 2.109061728241623

Epoch: 6| Step: 13
Training loss: 1.8351857662200928
Validation loss: 2.1360299535976943

Epoch: 158| Step: 0
Training loss: 1.5507594347000122
Validation loss: 2.128211418787638

Epoch: 6| Step: 1
Training loss: 1.312752366065979
Validation loss: 2.1262799488600863

Epoch: 6| Step: 2
Training loss: 2.0509495735168457
Validation loss: 2.114904080667803

Epoch: 6| Step: 3
Training loss: 1.6212480068206787
Validation loss: 2.108611335036575

Epoch: 6| Step: 4
Training loss: 1.495269775390625
Validation loss: 2.1196033493165047

Epoch: 6| Step: 5
Training loss: 1.4920837879180908
Validation loss: 2.1173593818500476

Epoch: 6| Step: 6
Training loss: 1.3783574104309082
Validation loss: 2.0984611536866877

Epoch: 6| Step: 7
Training loss: 1.5389058589935303
Validation loss: 2.087616769216394

Epoch: 6| Step: 8
Training loss: 1.8062455654144287
Validation loss: 2.0697211860328593

Epoch: 6| Step: 9
Training loss: 2.1605191230773926
Validation loss: 2.087025071984978

Epoch: 6| Step: 10
Training loss: 2.118302345275879
Validation loss: 2.1009312778390865

Epoch: 6| Step: 11
Training loss: 2.083394765853882
Validation loss: 2.128760018656331

Epoch: 6| Step: 12
Training loss: 2.6672215461730957
Validation loss: 2.154669318147885

Epoch: 6| Step: 13
Training loss: 2.153722047805786
Validation loss: 2.143342851310648

Epoch: 159| Step: 0
Training loss: 1.9988789558410645
Validation loss: 2.084878747181226

Epoch: 6| Step: 1
Training loss: 2.549546718597412
Validation loss: 2.062035419607675

Epoch: 6| Step: 2
Training loss: 2.0975522994995117
Validation loss: 2.049054952077968

Epoch: 6| Step: 3
Training loss: 1.9928172826766968
Validation loss: 2.0641186878245366

Epoch: 6| Step: 4
Training loss: 1.9191319942474365
Validation loss: 2.0844802138625935

Epoch: 6| Step: 5
Training loss: 1.94285249710083
Validation loss: 2.0508466843635804

Epoch: 6| Step: 6
Training loss: 1.502835988998413
Validation loss: 2.0276268682172223

Epoch: 6| Step: 7
Training loss: 1.9915342330932617
Validation loss: 2.0264500046289093

Epoch: 6| Step: 8
Training loss: 1.6460226774215698
Validation loss: 2.050914449076499

Epoch: 6| Step: 9
Training loss: 2.015148162841797
Validation loss: 2.0772964967194425

Epoch: 6| Step: 10
Training loss: 1.616745948791504
Validation loss: 2.1149309758217103

Epoch: 6| Step: 11
Training loss: 0.9309810996055603
Validation loss: 2.1457251412894136

Epoch: 6| Step: 12
Training loss: 1.4826202392578125
Validation loss: 2.144579529762268

Epoch: 6| Step: 13
Training loss: 1.4866701364517212
Validation loss: 2.137423592229043

Epoch: 160| Step: 0
Training loss: 1.72784423828125
Validation loss: 2.144102560576572

Epoch: 6| Step: 1
Training loss: 1.1663224697113037
Validation loss: 2.1573997056612404

Epoch: 6| Step: 2
Training loss: 2.2579827308654785
Validation loss: 2.175232959050004

Epoch: 6| Step: 3
Training loss: 1.6671925783157349
Validation loss: 2.1686385780252437

Epoch: 6| Step: 4
Training loss: 1.9891273975372314
Validation loss: 2.2097857049716416

Epoch: 6| Step: 5
Training loss: 2.4067416191101074
Validation loss: 2.192904272387105

Epoch: 6| Step: 6
Training loss: 1.1152960062026978
Validation loss: 2.1836183276227725

Epoch: 6| Step: 7
Training loss: 1.7694671154022217
Validation loss: 2.166951153867988

Epoch: 6| Step: 8
Training loss: 1.4711463451385498
Validation loss: 2.155939520046275

Epoch: 6| Step: 9
Training loss: 1.6851139068603516
Validation loss: 2.1460535205820555

Epoch: 6| Step: 10
Training loss: 1.1653996706008911
Validation loss: 2.155238611723787

Epoch: 6| Step: 11
Training loss: 2.4175148010253906
Validation loss: 2.143916198002395

Epoch: 6| Step: 12
Training loss: 1.7055516242980957
Validation loss: 2.098240990792551

Epoch: 6| Step: 13
Training loss: 2.026820182800293
Validation loss: 2.08216070231571

Epoch: 161| Step: 0
Training loss: 1.6845530271530151
Validation loss: 2.069998052812392

Epoch: 6| Step: 1
Training loss: 1.614957571029663
Validation loss: 2.060641606648763

Epoch: 6| Step: 2
Training loss: 1.5689257383346558
Validation loss: 2.086813703660042

Epoch: 6| Step: 3
Training loss: 1.8008688688278198
Validation loss: 2.070884535389562

Epoch: 6| Step: 4
Training loss: 2.309380054473877
Validation loss: 2.03267494581079

Epoch: 6| Step: 5
Training loss: 1.705552339553833
Validation loss: 2.0004666364321144

Epoch: 6| Step: 6
Training loss: 1.1402461528778076
Validation loss: 2.0043539718915055

Epoch: 6| Step: 7
Training loss: 1.1483699083328247
Validation loss: 2.017762892989702

Epoch: 6| Step: 8
Training loss: 1.6998493671417236
Validation loss: 2.031138002231557

Epoch: 6| Step: 9
Training loss: 2.249406337738037
Validation loss: 2.0397331201902

Epoch: 6| Step: 10
Training loss: 1.6109070777893066
Validation loss: 2.0313085074065835

Epoch: 6| Step: 11
Training loss: 1.9423015117645264
Validation loss: 2.0415373707330353

Epoch: 6| Step: 12
Training loss: 1.6350603103637695
Validation loss: 2.0608581522459626

Epoch: 6| Step: 13
Training loss: 2.1492576599121094
Validation loss: 2.0449517362861225

Epoch: 162| Step: 0
Training loss: 1.642340898513794
Validation loss: 2.0506347405013217

Epoch: 6| Step: 1
Training loss: 2.0069851875305176
Validation loss: 2.042657088207942

Epoch: 6| Step: 2
Training loss: 2.0932559967041016
Validation loss: 2.04791235667403

Epoch: 6| Step: 3
Training loss: 1.4988048076629639
Validation loss: 2.0298833116408317

Epoch: 6| Step: 4
Training loss: 1.2646878957748413
Validation loss: 2.055764434158161

Epoch: 6| Step: 5
Training loss: 1.322413444519043
Validation loss: 2.0745674038446076

Epoch: 6| Step: 6
Training loss: 1.8622323274612427
Validation loss: 2.083378594408753

Epoch: 6| Step: 7
Training loss: 0.9893550872802734
Validation loss: 2.100419221385833

Epoch: 6| Step: 8
Training loss: 1.7961804866790771
Validation loss: 2.0982435928877963

Epoch: 6| Step: 9
Training loss: 1.9883527755737305
Validation loss: 2.1125583302590156

Epoch: 6| Step: 10
Training loss: 1.8659636974334717
Validation loss: 2.1024364630381265

Epoch: 6| Step: 11
Training loss: 2.0614233016967773
Validation loss: 2.118974006304177

Epoch: 6| Step: 12
Training loss: 1.2711905241012573
Validation loss: 2.1251688029176448

Epoch: 6| Step: 13
Training loss: 1.2690130472183228
Validation loss: 2.1165785712580525

Epoch: 163| Step: 0
Training loss: 1.69805908203125
Validation loss: 2.118137990274737

Epoch: 6| Step: 1
Training loss: 1.7867518663406372
Validation loss: 2.1141400965311195

Epoch: 6| Step: 2
Training loss: 0.862852931022644
Validation loss: 2.0924047680311304

Epoch: 6| Step: 3
Training loss: 1.6671833992004395
Validation loss: 2.1026657166019564

Epoch: 6| Step: 4
Training loss: 1.3754479885101318
Validation loss: 2.111107595505253

Epoch: 6| Step: 5
Training loss: 1.610926866531372
Validation loss: 2.096436964568271

Epoch: 6| Step: 6
Training loss: 1.3520479202270508
Validation loss: 2.0684532247563845

Epoch: 6| Step: 7
Training loss: 1.737749695777893
Validation loss: 2.047177004557784

Epoch: 6| Step: 8
Training loss: 2.330850839614868
Validation loss: 2.0265155889654674

Epoch: 6| Step: 9
Training loss: 1.991258144378662
Validation loss: 2.039960415132584

Epoch: 6| Step: 10
Training loss: 1.4644256830215454
Validation loss: 2.010595903601698

Epoch: 6| Step: 11
Training loss: 1.8591777086257935
Validation loss: 2.021501474483039

Epoch: 6| Step: 12
Training loss: 1.6256506443023682
Validation loss: 2.0202629514919814

Epoch: 6| Step: 13
Training loss: 1.6881651878356934
Validation loss: 2.0413010979211457

Epoch: 164| Step: 0
Training loss: 1.8379123210906982
Validation loss: 2.0694651936972015

Epoch: 6| Step: 1
Training loss: 1.6025856733322144
Validation loss: 2.0813903475320465

Epoch: 6| Step: 2
Training loss: 1.1402654647827148
Validation loss: 2.1343211076592885

Epoch: 6| Step: 3
Training loss: 1.7356566190719604
Validation loss: 2.1389300156665105

Epoch: 6| Step: 4
Training loss: 2.1359236240386963
Validation loss: 2.193015285717544

Epoch: 6| Step: 5
Training loss: 1.3709418773651123
Validation loss: 2.1468085704311246

Epoch: 6| Step: 6
Training loss: 2.131380081176758
Validation loss: 2.1512628729625414

Epoch: 6| Step: 7
Training loss: 1.7557837963104248
Validation loss: 2.156578843311597

Epoch: 6| Step: 8
Training loss: 2.075704574584961
Validation loss: 2.1195178865104594

Epoch: 6| Step: 9
Training loss: 1.6516621112823486
Validation loss: 2.1069688540633007

Epoch: 6| Step: 10
Training loss: 1.385835886001587
Validation loss: 2.0711773774957143

Epoch: 6| Step: 11
Training loss: 1.3147499561309814
Validation loss: 2.0638858836184264

Epoch: 6| Step: 12
Training loss: 1.5527911186218262
Validation loss: 2.0415277250351442

Epoch: 6| Step: 13
Training loss: 0.8142313361167908
Validation loss: 2.038428851353225

Epoch: 165| Step: 0
Training loss: 2.320019245147705
Validation loss: 2.0322177102488856

Epoch: 6| Step: 1
Training loss: 1.5682647228240967
Validation loss: 2.0363677265823528

Epoch: 6| Step: 2
Training loss: 1.2744605541229248
Validation loss: 2.0367027136587326

Epoch: 6| Step: 3
Training loss: 1.94143807888031
Validation loss: 2.027953491416029

Epoch: 6| Step: 4
Training loss: 1.735025405883789
Validation loss: 2.0343069312393025

Epoch: 6| Step: 5
Training loss: 0.8426005244255066
Validation loss: 2.0360997684540285

Epoch: 6| Step: 6
Training loss: 1.463407039642334
Validation loss: 2.029727367944615

Epoch: 6| Step: 7
Training loss: 1.2181508541107178
Validation loss: 2.0187942917628954

Epoch: 6| Step: 8
Training loss: 1.617660403251648
Validation loss: 2.035908769535762

Epoch: 6| Step: 9
Training loss: 1.4581677913665771
Validation loss: 2.0156664438145135

Epoch: 6| Step: 10
Training loss: 1.8729695081710815
Validation loss: 2.0099945722087735

Epoch: 6| Step: 11
Training loss: 1.3336814641952515
Validation loss: 2.0122660513847106

Epoch: 6| Step: 12
Training loss: 1.9042654037475586
Validation loss: 2.016116455037107

Epoch: 6| Step: 13
Training loss: 2.198727607727051
Validation loss: 2.0390401706900647

Epoch: 166| Step: 0
Training loss: 1.2447816133499146
Validation loss: 2.060818136379283

Epoch: 6| Step: 1
Training loss: 1.4866809844970703
Validation loss: 2.0770392853726625

Epoch: 6| Step: 2
Training loss: 1.4193506240844727
Validation loss: 2.109817356191656

Epoch: 6| Step: 3
Training loss: 1.9790904521942139
Validation loss: 2.1306055156133508

Epoch: 6| Step: 4
Training loss: 1.2573736906051636
Validation loss: 2.129046678543091

Epoch: 6| Step: 5
Training loss: 1.6660031080245972
Validation loss: 2.1331374235050653

Epoch: 6| Step: 6
Training loss: 1.4905149936676025
Validation loss: 2.130546001977818

Epoch: 6| Step: 7
Training loss: 1.6556930541992188
Validation loss: 2.129712781598491

Epoch: 6| Step: 8
Training loss: 2.0261921882629395
Validation loss: 2.1373532356754428

Epoch: 6| Step: 9
Training loss: 1.7619050741195679
Validation loss: 2.1401583276769167

Epoch: 6| Step: 10
Training loss: 1.3957586288452148
Validation loss: 2.1516512311914915

Epoch: 6| Step: 11
Training loss: 1.83456552028656
Validation loss: 2.153998887667092

Epoch: 6| Step: 12
Training loss: 1.5453007221221924
Validation loss: 2.1409614727061284

Epoch: 6| Step: 13
Training loss: 1.3321691751480103
Validation loss: 2.126224522949547

Epoch: 167| Step: 0
Training loss: 1.2165024280548096
Validation loss: 2.1019423277147355

Epoch: 6| Step: 1
Training loss: 2.0402116775512695
Validation loss: 2.0824512204816266

Epoch: 6| Step: 2
Training loss: 1.3434089422225952
Validation loss: 2.0689213416909658

Epoch: 6| Step: 3
Training loss: 1.6793150901794434
Validation loss: 2.071121177365703

Epoch: 6| Step: 4
Training loss: 1.489323616027832
Validation loss: 2.064975138633482

Epoch: 6| Step: 5
Training loss: 2.0217838287353516
Validation loss: 2.04537130812163

Epoch: 6| Step: 6
Training loss: 2.3353519439697266
Validation loss: 2.0274715782493673

Epoch: 6| Step: 7
Training loss: 1.0781121253967285
Validation loss: 1.9968399027342438

Epoch: 6| Step: 8
Training loss: 1.498821496963501
Validation loss: 1.9742081703678254

Epoch: 6| Step: 9
Training loss: 1.4638537168502808
Validation loss: 1.980317984857867

Epoch: 6| Step: 10
Training loss: 1.4827055931091309
Validation loss: 1.9809673396489953

Epoch: 6| Step: 11
Training loss: 1.4624104499816895
Validation loss: 1.993445873260498

Epoch: 6| Step: 12
Training loss: 1.5842911005020142
Validation loss: 1.9782328797924904

Epoch: 6| Step: 13
Training loss: 1.2587904930114746
Validation loss: 2.012808067824251

Epoch: 168| Step: 0
Training loss: 2.0345516204833984
Validation loss: 2.0253199300458355

Epoch: 6| Step: 1
Training loss: 1.7113184928894043
Validation loss: 2.047946923522539

Epoch: 6| Step: 2
Training loss: 1.396887183189392
Validation loss: 2.0986607972011773

Epoch: 6| Step: 3
Training loss: 1.5674933195114136
Validation loss: 2.094172336721933

Epoch: 6| Step: 4
Training loss: 1.4954931735992432
Validation loss: 2.1310977999882033

Epoch: 6| Step: 5
Training loss: 1.6335746049880981
Validation loss: 2.1337110791155087

Epoch: 6| Step: 6
Training loss: 1.2290836572647095
Validation loss: 2.105045590349423

Epoch: 6| Step: 7
Training loss: 1.8824217319488525
Validation loss: 2.107121923918365

Epoch: 6| Step: 8
Training loss: 1.8115538358688354
Validation loss: 2.065835186230239

Epoch: 6| Step: 9
Training loss: 0.9545440077781677
Validation loss: 2.0511526523097867

Epoch: 6| Step: 10
Training loss: 1.6505656242370605
Validation loss: 2.0242284292815835

Epoch: 6| Step: 11
Training loss: 0.8242666721343994
Validation loss: 2.053709478788478

Epoch: 6| Step: 12
Training loss: 1.9261882305145264
Validation loss: 2.0530616878181376

Epoch: 6| Step: 13
Training loss: 1.1709675788879395
Validation loss: 2.038352366416685

Epoch: 169| Step: 0
Training loss: 1.3777246475219727
Validation loss: 2.0413745205889464

Epoch: 6| Step: 1
Training loss: 1.6905804872512817
Validation loss: 2.0276752620614986

Epoch: 6| Step: 2
Training loss: 0.8764267563819885
Validation loss: 2.027577346371066

Epoch: 6| Step: 3
Training loss: 1.3381519317626953
Validation loss: 2.0590262310479277

Epoch: 6| Step: 4
Training loss: 1.5517600774765015
Validation loss: 2.0734272028810237

Epoch: 6| Step: 5
Training loss: 1.3756282329559326
Validation loss: 2.044365744436941

Epoch: 6| Step: 6
Training loss: 1.778874158859253
Validation loss: 2.0327385048712454

Epoch: 6| Step: 7
Training loss: 1.2493183612823486
Validation loss: 2.0364264019073977

Epoch: 6| Step: 8
Training loss: 1.97426438331604
Validation loss: 2.0428750848257415

Epoch: 6| Step: 9
Training loss: 2.045010566711426
Validation loss: 2.045880684288599

Epoch: 6| Step: 10
Training loss: 1.8374731540679932
Validation loss: 2.0572054898867043

Epoch: 6| Step: 11
Training loss: 1.631887435913086
Validation loss: 2.046767391184325

Epoch: 6| Step: 12
Training loss: 1.6009975671768188
Validation loss: 2.0489921492914998

Epoch: 6| Step: 13
Training loss: 0.9764435887336731
Validation loss: 2.0520143508911133

Epoch: 170| Step: 0
Training loss: 1.2639094591140747
Validation loss: 2.046182845228462

Epoch: 6| Step: 1
Training loss: 1.7024229764938354
Validation loss: 2.072064333064582

Epoch: 6| Step: 2
Training loss: 1.4820667505264282
Validation loss: 2.071411250739969

Epoch: 6| Step: 3
Training loss: 1.0324695110321045
Validation loss: 2.07983047475097

Epoch: 6| Step: 4
Training loss: 1.6601521968841553
Validation loss: 2.092799545616232

Epoch: 6| Step: 5
Training loss: 1.5314444303512573
Validation loss: 2.09961792730516

Epoch: 6| Step: 6
Training loss: 0.9411510229110718
Validation loss: 2.0950416108613372

Epoch: 6| Step: 7
Training loss: 1.1513421535491943
Validation loss: 2.1236088404091458

Epoch: 6| Step: 8
Training loss: 2.3995614051818848
Validation loss: 2.074568771546887

Epoch: 6| Step: 9
Training loss: 1.6505012512207031
Validation loss: 2.0746730425024547

Epoch: 6| Step: 10
Training loss: 0.8977999687194824
Validation loss: 2.0818986918336604

Epoch: 6| Step: 11
Training loss: 2.276409387588501
Validation loss: 2.076744969173144

Epoch: 6| Step: 12
Training loss: 1.6205302476882935
Validation loss: 2.0549396340565016

Epoch: 6| Step: 13
Training loss: 1.5205175876617432
Validation loss: 2.057743767256378

Epoch: 171| Step: 0
Training loss: 1.125791311264038
Validation loss: 2.0362567286337576

Epoch: 6| Step: 1
Training loss: 1.431487798690796
Validation loss: 2.002685623784219

Epoch: 6| Step: 2
Training loss: 1.6734429597854614
Validation loss: 2.0165489694123626

Epoch: 6| Step: 3
Training loss: 1.6420726776123047
Validation loss: 2.0150592711664017

Epoch: 6| Step: 4
Training loss: 1.6582553386688232
Validation loss: 2.0240320954271542

Epoch: 6| Step: 5
Training loss: 0.8617757558822632
Validation loss: 2.036541264544251

Epoch: 6| Step: 6
Training loss: 2.032458782196045
Validation loss: 2.0270201493335027

Epoch: 6| Step: 7
Training loss: 1.0411840677261353
Validation loss: 2.0428183258220716

Epoch: 6| Step: 8
Training loss: 1.9141032695770264
Validation loss: 2.0228606244569183

Epoch: 6| Step: 9
Training loss: 1.1915950775146484
Validation loss: 2.045396195944919

Epoch: 6| Step: 10
Training loss: 1.3864874839782715
Validation loss: 2.025118563764839

Epoch: 6| Step: 11
Training loss: 1.5268175601959229
Validation loss: 2.0261099569259153

Epoch: 6| Step: 12
Training loss: 1.9296321868896484
Validation loss: 2.008928288695633

Epoch: 6| Step: 13
Training loss: 0.7901369333267212
Validation loss: 2.002918417735766

Epoch: 172| Step: 0
Training loss: 1.197293758392334
Validation loss: 2.0060975833605696

Epoch: 6| Step: 1
Training loss: 1.198512315750122
Validation loss: 2.010078271230062

Epoch: 6| Step: 2
Training loss: 1.815089225769043
Validation loss: 2.0080024042437152

Epoch: 6| Step: 3
Training loss: 1.1494994163513184
Validation loss: 2.028263972651574

Epoch: 6| Step: 4
Training loss: 1.5582945346832275
Validation loss: 2.0389791022064867

Epoch: 6| Step: 5
Training loss: 1.55607271194458
Validation loss: 2.06317667166392

Epoch: 6| Step: 6
Training loss: 1.5153601169586182
Validation loss: 2.0749995336737683

Epoch: 6| Step: 7
Training loss: 1.5701594352722168
Validation loss: 2.068115520220931

Epoch: 6| Step: 8
Training loss: 1.6993353366851807
Validation loss: 2.0919062040185414

Epoch: 6| Step: 9
Training loss: 1.6680643558502197
Validation loss: 2.096695516699104

Epoch: 6| Step: 10
Training loss: 1.269205093383789
Validation loss: 2.0645995191348496

Epoch: 6| Step: 11
Training loss: 1.4806654453277588
Validation loss: 2.0604478210531254

Epoch: 6| Step: 12
Training loss: 1.3197112083435059
Validation loss: 2.0420233459882837

Epoch: 6| Step: 13
Training loss: 1.7031117677688599
Validation loss: 2.034443119520782

Epoch: 173| Step: 0
Training loss: 2.140169620513916
Validation loss: 2.018456523136426

Epoch: 6| Step: 1
Training loss: 1.861039400100708
Validation loss: 2.01760757353998

Epoch: 6| Step: 2
Training loss: 1.4022033214569092
Validation loss: 2.0087978339964345

Epoch: 6| Step: 3
Training loss: 1.6707996129989624
Validation loss: 1.9931040310090589

Epoch: 6| Step: 4
Training loss: 1.6614441871643066
Validation loss: 1.9891371752626152

Epoch: 6| Step: 5
Training loss: 0.7651234865188599
Validation loss: 1.9846364913448211

Epoch: 6| Step: 6
Training loss: 1.1102625131607056
Validation loss: 2.0078366661584504

Epoch: 6| Step: 7
Training loss: 1.6423671245574951
Validation loss: 2.0212015464741695

Epoch: 6| Step: 8
Training loss: 1.4542527198791504
Validation loss: 2.0043718635395007

Epoch: 6| Step: 9
Training loss: 1.3152732849121094
Validation loss: 2.038739488970849

Epoch: 6| Step: 10
Training loss: 1.3950213193893433
Validation loss: 2.028613034115043

Epoch: 6| Step: 11
Training loss: 1.211435079574585
Validation loss: 2.062978772706883

Epoch: 6| Step: 12
Training loss: 1.3400611877441406
Validation loss: 2.0617563570699384

Epoch: 6| Step: 13
Training loss: 1.1561137437820435
Validation loss: 2.0548048327046056

Epoch: 174| Step: 0
Training loss: 1.7493497133255005
Validation loss: 2.0504491611193587

Epoch: 6| Step: 1
Training loss: 1.22647225856781
Validation loss: 2.035366068604172

Epoch: 6| Step: 2
Training loss: 1.3683416843414307
Validation loss: 2.053381742969636

Epoch: 6| Step: 3
Training loss: 1.7983489036560059
Validation loss: 2.0940645971605854

Epoch: 6| Step: 4
Training loss: 1.2214668989181519
Validation loss: 2.1136895354076097

Epoch: 6| Step: 5
Training loss: 2.0382022857666016
Validation loss: 2.097450242247633

Epoch: 6| Step: 6
Training loss: 1.620798945426941
Validation loss: 2.059954604794902

Epoch: 6| Step: 7
Training loss: 0.8856979608535767
Validation loss: 2.050101423776278

Epoch: 6| Step: 8
Training loss: 1.3634520769119263
Validation loss: 2.0540501507379676

Epoch: 6| Step: 9
Training loss: 1.408118724822998
Validation loss: 2.0913861490065053

Epoch: 6| Step: 10
Training loss: 1.7246506214141846
Validation loss: 2.100504829037574

Epoch: 6| Step: 11
Training loss: 1.39006769657135
Validation loss: 2.1073290122452604

Epoch: 6| Step: 12
Training loss: 1.8426296710968018
Validation loss: 2.082368386689053

Epoch: 6| Step: 13
Training loss: 0.927344560623169
Validation loss: 2.092422116187311

Epoch: 175| Step: 0
Training loss: 1.6604516506195068
Validation loss: 2.081243835469728

Epoch: 6| Step: 1
Training loss: 0.9460094571113586
Validation loss: 2.056147357468964

Epoch: 6| Step: 2
Training loss: 1.5360207557678223
Validation loss: 2.0738567690695486

Epoch: 6| Step: 3
Training loss: 1.1520977020263672
Validation loss: 2.054266809135355

Epoch: 6| Step: 4
Training loss: 1.3513598442077637
Validation loss: 2.04308133996943

Epoch: 6| Step: 5
Training loss: 1.5789878368377686
Validation loss: 2.0551979746869815

Epoch: 6| Step: 6
Training loss: 1.1750489473342896
Validation loss: 2.036738699482333

Epoch: 6| Step: 7
Training loss: 1.443183422088623
Validation loss: 2.0381361848564556

Epoch: 6| Step: 8
Training loss: 1.070190668106079
Validation loss: 2.0439058785797446

Epoch: 6| Step: 9
Training loss: 1.6365063190460205
Validation loss: 2.0523657785948886

Epoch: 6| Step: 10
Training loss: 1.114673137664795
Validation loss: 2.0630122461626605

Epoch: 6| Step: 11
Training loss: 1.8694515228271484
Validation loss: 2.071241239065765

Epoch: 6| Step: 12
Training loss: 1.5863754749298096
Validation loss: 2.073665354841499

Epoch: 6| Step: 13
Training loss: 1.7078081369400024
Validation loss: 2.079734138263169

Testing loss: 2.264888111750285

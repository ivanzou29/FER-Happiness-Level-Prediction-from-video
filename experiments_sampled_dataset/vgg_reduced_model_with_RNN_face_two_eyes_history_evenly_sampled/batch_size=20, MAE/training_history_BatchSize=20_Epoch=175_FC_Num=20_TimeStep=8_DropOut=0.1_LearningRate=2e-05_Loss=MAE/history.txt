Epoch: 1| Step: 0
Training loss: 6.3194756507873535
Validation loss: 5.146451314290364

Epoch: 5| Step: 1
Training loss: 3.954503297805786
Validation loss: 5.127562774124966

Epoch: 5| Step: 2
Training loss: 4.461330890655518
Validation loss: 5.110389007035122

Epoch: 5| Step: 3
Training loss: 6.163359642028809
Validation loss: 5.091942684624785

Epoch: 5| Step: 4
Training loss: 4.407760143280029
Validation loss: 5.070994643754856

Epoch: 5| Step: 5
Training loss: 4.8885297775268555
Validation loss: 5.047022101699665

Epoch: 5| Step: 6
Training loss: 4.764345645904541
Validation loss: 5.019254238374772

Epoch: 5| Step: 7
Training loss: 4.649903297424316
Validation loss: 4.98687429325555

Epoch: 5| Step: 8
Training loss: 3.756439208984375
Validation loss: 4.951327693077825

Epoch: 5| Step: 9
Training loss: 5.001275539398193
Validation loss: 4.911138503782211

Epoch: 5| Step: 10
Training loss: 4.783664703369141
Validation loss: 4.866078710043302

Epoch: 2| Step: 0
Training loss: 5.046938419342041
Validation loss: 4.816750854574224

Epoch: 5| Step: 1
Training loss: 4.544917106628418
Validation loss: 4.762986593349005

Epoch: 5| Step: 2
Training loss: 3.5792243480682373
Validation loss: 4.706062511731219

Epoch: 5| Step: 3
Training loss: 6.117598533630371
Validation loss: 4.648140292013845

Epoch: 5| Step: 4
Training loss: 5.147504806518555
Validation loss: 4.587380429749848

Epoch: 5| Step: 5
Training loss: 4.294812202453613
Validation loss: 4.527585275711552

Epoch: 5| Step: 6
Training loss: 4.0875749588012695
Validation loss: 4.470339662285261

Epoch: 5| Step: 7
Training loss: 3.990044355392456
Validation loss: 4.413859885226014

Epoch: 5| Step: 8
Training loss: 3.1542809009552
Validation loss: 4.364731855289911

Epoch: 5| Step: 9
Training loss: 4.101921558380127
Validation loss: 4.316904206429759

Epoch: 5| Step: 10
Training loss: 3.4755196571350098
Validation loss: 4.274739019332394

Epoch: 3| Step: 0
Training loss: 4.7618279457092285
Validation loss: 4.235322567724412

Epoch: 5| Step: 1
Training loss: 3.7218289375305176
Validation loss: 4.193643418691492

Epoch: 5| Step: 2
Training loss: 3.4056396484375
Validation loss: 4.151819757235947

Epoch: 5| Step: 3
Training loss: 4.4918413162231445
Validation loss: 4.106851203467256

Epoch: 5| Step: 4
Training loss: 4.3684821128845215
Validation loss: 4.054778052914527

Epoch: 5| Step: 5
Training loss: 3.1991171836853027
Validation loss: 4.012615783240205

Epoch: 5| Step: 6
Training loss: 3.636051893234253
Validation loss: 3.974550985520886

Epoch: 5| Step: 7
Training loss: 3.241320848464966
Validation loss: 3.941527333310855

Epoch: 5| Step: 8
Training loss: 5.265931129455566
Validation loss: 3.9069421393896944

Epoch: 5| Step: 9
Training loss: 2.9043736457824707
Validation loss: 3.8742945424972044

Epoch: 5| Step: 10
Training loss: 3.5381886959075928
Validation loss: 3.8438445727030435

Epoch: 4| Step: 0
Training loss: 4.436295509338379
Validation loss: 3.8126151023372525

Epoch: 5| Step: 1
Training loss: 3.2622687816619873
Validation loss: 3.7762049962115545

Epoch: 5| Step: 2
Training loss: 3.6242527961730957
Validation loss: 3.7384708696796047

Epoch: 5| Step: 3
Training loss: 2.9176573753356934
Validation loss: 3.7093230268006683

Epoch: 5| Step: 4
Training loss: 4.164315223693848
Validation loss: 3.685933179752801

Epoch: 5| Step: 5
Training loss: 4.223924160003662
Validation loss: 3.6680723826090493

Epoch: 5| Step: 6
Training loss: 3.779376983642578
Validation loss: 3.6554500954125517

Epoch: 5| Step: 7
Training loss: 3.8569653034210205
Validation loss: 3.6331807618500083

Epoch: 5| Step: 8
Training loss: 3.678250789642334
Validation loss: 3.612569103958786

Epoch: 5| Step: 9
Training loss: 2.6264355182647705
Validation loss: 3.596766592353903

Epoch: 5| Step: 10
Training loss: 2.697908401489258
Validation loss: 3.583746866513324

Epoch: 5| Step: 0
Training loss: 4.099425315856934
Validation loss: 3.565986966574064

Epoch: 5| Step: 1
Training loss: 3.234265089035034
Validation loss: 3.5488898882301907

Epoch: 5| Step: 2
Training loss: 2.328031063079834
Validation loss: 3.532169859896424

Epoch: 5| Step: 3
Training loss: 3.7165024280548096
Validation loss: 3.5170202383431057

Epoch: 5| Step: 4
Training loss: 3.0614824295043945
Validation loss: 3.4951593414429696

Epoch: 5| Step: 5
Training loss: 2.343611240386963
Validation loss: 3.474593993156187

Epoch: 5| Step: 6
Training loss: 4.150888442993164
Validation loss: 3.4533536254718737

Epoch: 5| Step: 7
Training loss: 3.329040050506592
Validation loss: 3.4330542907919934

Epoch: 5| Step: 8
Training loss: 4.196972370147705
Validation loss: 3.4244742496039278

Epoch: 5| Step: 9
Training loss: 3.7472267150878906
Validation loss: 3.3961802144204416

Epoch: 5| Step: 10
Training loss: 3.370087146759033
Validation loss: 3.3805884340757966

Epoch: 6| Step: 0
Training loss: 3.6563162803649902
Validation loss: 3.365029550367786

Epoch: 5| Step: 1
Training loss: 3.279855728149414
Validation loss: 3.349153800677228

Epoch: 5| Step: 2
Training loss: 1.6702568531036377
Validation loss: 3.3339868463495725

Epoch: 5| Step: 3
Training loss: 3.2045159339904785
Validation loss: 3.3150559215135473

Epoch: 5| Step: 4
Training loss: 3.332054853439331
Validation loss: 3.2999164212134575

Epoch: 5| Step: 5
Training loss: 3.1725106239318848
Validation loss: 3.280116496547576

Epoch: 5| Step: 6
Training loss: 2.820491075515747
Validation loss: 3.2654977921516664

Epoch: 5| Step: 7
Training loss: 3.3855273723602295
Validation loss: 3.2616507263593775

Epoch: 5| Step: 8
Training loss: 3.5801281929016113
Validation loss: 3.2549231590763217

Epoch: 5| Step: 9
Training loss: 4.220374584197998
Validation loss: 3.2381990545539447

Epoch: 5| Step: 10
Training loss: 3.6822774410247803
Validation loss: 3.236771568175285

Epoch: 7| Step: 0
Training loss: 3.190851926803589
Validation loss: 3.217217594064692

Epoch: 5| Step: 1
Training loss: 3.4484519958496094
Validation loss: 3.2382387832928727

Epoch: 5| Step: 2
Training loss: 2.88430118560791
Validation loss: 3.1967949072519937

Epoch: 5| Step: 3
Training loss: 3.0361714363098145
Validation loss: 3.1956964308215725

Epoch: 5| Step: 4
Training loss: 3.7990097999572754
Validation loss: 3.1935889592734714

Epoch: 5| Step: 5
Training loss: 3.4679782390594482
Validation loss: 3.1841036888860885

Epoch: 5| Step: 6
Training loss: 3.223336696624756
Validation loss: 3.170459670405234

Epoch: 5| Step: 7
Training loss: 2.9236412048339844
Validation loss: 3.160092963967272

Epoch: 5| Step: 8
Training loss: 3.158357620239258
Validation loss: 3.1591518873809488

Epoch: 5| Step: 9
Training loss: 2.9366488456726074
Validation loss: 3.148092275024742

Epoch: 5| Step: 10
Training loss: 2.9760875701904297
Validation loss: 3.1361996871168896

Epoch: 8| Step: 0
Training loss: 3.6128928661346436
Validation loss: 3.1308999189766507

Epoch: 5| Step: 1
Training loss: 3.5238430500030518
Validation loss: 3.1186313167695077

Epoch: 5| Step: 2
Training loss: 2.7262449264526367
Validation loss: 3.1065533981528333

Epoch: 5| Step: 3
Training loss: 3.2667248249053955
Validation loss: 3.101269947585239

Epoch: 5| Step: 4
Training loss: 3.0713889598846436
Validation loss: 3.0945358635276876

Epoch: 5| Step: 5
Training loss: 3.559603214263916
Validation loss: 3.1174408082039125

Epoch: 5| Step: 6
Training loss: 2.14499831199646
Validation loss: 3.0822297116761566

Epoch: 5| Step: 7
Training loss: 3.3822739124298096
Validation loss: 3.0730580232476674

Epoch: 5| Step: 8
Training loss: 2.9015865325927734
Validation loss: 3.069768836421351

Epoch: 5| Step: 9
Training loss: 3.1115901470184326
Validation loss: 3.077661639900618

Epoch: 5| Step: 10
Training loss: 3.124905824661255
Validation loss: 3.079795152910294

Epoch: 9| Step: 0
Training loss: 2.9300811290740967
Validation loss: 3.0727400190086773

Epoch: 5| Step: 1
Training loss: 2.9232869148254395
Validation loss: 3.0561615138925533

Epoch: 5| Step: 2
Training loss: 3.3407769203186035
Validation loss: 3.048048967956215

Epoch: 5| Step: 3
Training loss: 3.6223816871643066
Validation loss: 3.038481673886699

Epoch: 5| Step: 4
Training loss: 2.570213794708252
Validation loss: 3.036703545560119

Epoch: 5| Step: 5
Training loss: 3.685965061187744
Validation loss: 3.0302202470840944

Epoch: 5| Step: 6
Training loss: 2.4602012634277344
Validation loss: 3.0222282922396095

Epoch: 5| Step: 7
Training loss: 2.7945804595947266
Validation loss: 3.0271479032372914

Epoch: 5| Step: 8
Training loss: 3.3040072917938232
Validation loss: 3.0265368441099763

Epoch: 5| Step: 9
Training loss: 3.177056312561035
Validation loss: 3.0116073290506997

Epoch: 5| Step: 10
Training loss: 3.1990137100219727
Validation loss: 3.0406286998461654

Epoch: 10| Step: 0
Training loss: 3.814429521560669
Validation loss: 3.029990837138186

Epoch: 5| Step: 1
Training loss: 3.2481722831726074
Validation loss: 2.9974043369293213

Epoch: 5| Step: 2
Training loss: 2.6121795177459717
Validation loss: 2.993033101481776

Epoch: 5| Step: 3
Training loss: 2.8935348987579346
Validation loss: 3.005164174623387

Epoch: 5| Step: 4
Training loss: 3.0975403785705566
Validation loss: 3.016460887847408

Epoch: 5| Step: 5
Training loss: 3.2612853050231934
Validation loss: 3.0162935410776446

Epoch: 5| Step: 6
Training loss: 2.9149324893951416
Validation loss: 3.005153327859858

Epoch: 5| Step: 7
Training loss: 2.3724513053894043
Validation loss: 2.983204633958878

Epoch: 5| Step: 8
Training loss: 2.82346773147583
Validation loss: 2.9704201272738877

Epoch: 5| Step: 9
Training loss: 3.1653075218200684
Validation loss: 2.9602453554830244

Epoch: 5| Step: 10
Training loss: 3.585625410079956
Validation loss: 3.0123282606883715

Epoch: 11| Step: 0
Training loss: 2.657252550125122
Validation loss: 3.014490750528151

Epoch: 5| Step: 1
Training loss: 2.8378729820251465
Validation loss: 3.025509439488893

Epoch: 5| Step: 2
Training loss: 2.8062403202056885
Validation loss: 3.0113991639947377

Epoch: 5| Step: 3
Training loss: 3.160076141357422
Validation loss: 2.9375340784749677

Epoch: 5| Step: 4
Training loss: 3.6251187324523926
Validation loss: 2.921617784807759

Epoch: 5| Step: 5
Training loss: 2.6760146617889404
Validation loss: 2.920231214133642

Epoch: 5| Step: 6
Training loss: 3.152017116546631
Validation loss: 2.938573545025241

Epoch: 5| Step: 7
Training loss: 2.8571441173553467
Validation loss: 3.000504280931206

Epoch: 5| Step: 8
Training loss: 2.6590237617492676
Validation loss: 2.9270924880940425

Epoch: 5| Step: 9
Training loss: 3.374018430709839
Validation loss: 2.9117223703733055

Epoch: 5| Step: 10
Training loss: 3.6701245307922363
Validation loss: 2.9150285951552855

Epoch: 12| Step: 0
Training loss: 3.3045074939727783
Validation loss: 2.9236948567052043

Epoch: 5| Step: 1
Training loss: 3.499343156814575
Validation loss: 2.9270079058985554

Epoch: 5| Step: 2
Training loss: 2.758960247039795
Validation loss: 2.925244628742177

Epoch: 5| Step: 3
Training loss: 2.2167868614196777
Validation loss: 2.917288036756618

Epoch: 5| Step: 4
Training loss: 2.5489659309387207
Validation loss: 2.9097703092841694

Epoch: 5| Step: 5
Training loss: 3.0959994792938232
Validation loss: 2.9044676980664654

Epoch: 5| Step: 6
Training loss: 2.8867950439453125
Validation loss: 2.9010821183522544

Epoch: 5| Step: 7
Training loss: 3.2938125133514404
Validation loss: 2.8938483730439217

Epoch: 5| Step: 8
Training loss: 2.850883960723877
Validation loss: 2.889665085782287

Epoch: 5| Step: 9
Training loss: 3.213557004928589
Validation loss: 2.8811293750680904

Epoch: 5| Step: 10
Training loss: 3.4245030879974365
Validation loss: 2.8810288367732877

Epoch: 13| Step: 0
Training loss: 2.6761906147003174
Validation loss: 2.8761899138009674

Epoch: 5| Step: 1
Training loss: 2.46840763092041
Validation loss: 2.8693584524175173

Epoch: 5| Step: 2
Training loss: 3.441424608230591
Validation loss: 2.8629793479878414

Epoch: 5| Step: 3
Training loss: 3.5374042987823486
Validation loss: 2.858209953513197

Epoch: 5| Step: 4
Training loss: 3.0777316093444824
Validation loss: 2.8543981198341615

Epoch: 5| Step: 5
Training loss: 2.885901689529419
Validation loss: 2.8495486449169856

Epoch: 5| Step: 6
Training loss: 2.731388807296753
Validation loss: 2.850613832473755

Epoch: 5| Step: 7
Training loss: 2.7711892127990723
Validation loss: 2.8457751863746235

Epoch: 5| Step: 8
Training loss: 3.502899169921875
Validation loss: 2.8379808087502756

Epoch: 5| Step: 9
Training loss: 2.3353588581085205
Validation loss: 2.8362682045146985

Epoch: 5| Step: 10
Training loss: 3.2565646171569824
Validation loss: 2.829036020463513

Epoch: 14| Step: 0
Training loss: 3.2576053142547607
Validation loss: 2.8285109586613153

Epoch: 5| Step: 1
Training loss: 3.0456948280334473
Validation loss: 2.8311915038734354

Epoch: 5| Step: 2
Training loss: 2.1760635375976562
Validation loss: 2.8245652567955757

Epoch: 5| Step: 3
Training loss: 2.571371078491211
Validation loss: 2.818286462496686

Epoch: 5| Step: 4
Training loss: 3.8730480670928955
Validation loss: 2.814323571420485

Epoch: 5| Step: 5
Training loss: 2.535477876663208
Validation loss: 2.813465687536424

Epoch: 5| Step: 6
Training loss: 3.186952829360962
Validation loss: 2.8112455747460805

Epoch: 5| Step: 7
Training loss: 2.998485565185547
Validation loss: 2.8104306600427114

Epoch: 5| Step: 8
Training loss: 2.8229587078094482
Validation loss: 2.805860334827054

Epoch: 5| Step: 9
Training loss: 2.810746192932129
Validation loss: 2.80702204345375

Epoch: 5| Step: 10
Training loss: 3.099634885787964
Validation loss: 2.800457964661301

Epoch: 15| Step: 0
Training loss: 2.4752376079559326
Validation loss: 2.8025956512779318

Epoch: 5| Step: 1
Training loss: 3.0019686222076416
Validation loss: 2.794883710081859

Epoch: 5| Step: 2
Training loss: 2.7912704944610596
Validation loss: 2.796886182600452

Epoch: 5| Step: 3
Training loss: 2.7749741077423096
Validation loss: 2.7912851943764636

Epoch: 5| Step: 4
Training loss: 2.4014599323272705
Validation loss: 2.7864523036505586

Epoch: 5| Step: 5
Training loss: 3.5703845024108887
Validation loss: 2.789843177282682

Epoch: 5| Step: 6
Training loss: 2.316772699356079
Validation loss: 2.794919378014021

Epoch: 5| Step: 7
Training loss: 3.597728729248047
Validation loss: 2.79950419164473

Epoch: 5| Step: 8
Training loss: 2.9259121417999268
Validation loss: 2.8042667911898707

Epoch: 5| Step: 9
Training loss: 3.345427989959717
Validation loss: 2.7953611112410024

Epoch: 5| Step: 10
Training loss: 3.010127544403076
Validation loss: 2.776247816701089

Epoch: 16| Step: 0
Training loss: 3.3150150775909424
Validation loss: 2.766717000674176

Epoch: 5| Step: 1
Training loss: 2.1769070625305176
Validation loss: 2.7768265944655224

Epoch: 5| Step: 2
Training loss: 3.1336991786956787
Validation loss: 2.7810133939148276

Epoch: 5| Step: 3
Training loss: 2.381779670715332
Validation loss: 2.7822021745866343

Epoch: 5| Step: 4
Training loss: 3.390781879425049
Validation loss: 2.7765081672258276

Epoch: 5| Step: 5
Training loss: 2.4292562007904053
Validation loss: 2.7677172076317573

Epoch: 5| Step: 6
Training loss: 2.967440366744995
Validation loss: 2.7667394453479397

Epoch: 5| Step: 7
Training loss: 3.365628719329834
Validation loss: 2.757310159744755

Epoch: 5| Step: 8
Training loss: 3.7340264320373535
Validation loss: 2.756399905809792

Epoch: 5| Step: 9
Training loss: 2.158226490020752
Validation loss: 2.7496494144521733

Epoch: 5| Step: 10
Training loss: 2.9963717460632324
Validation loss: 2.751959270046603

Epoch: 17| Step: 0
Training loss: 2.1484949588775635
Validation loss: 2.7612434048806467

Epoch: 5| Step: 1
Training loss: 3.032749652862549
Validation loss: 2.7630656483352825

Epoch: 5| Step: 2
Training loss: 2.7034354209899902
Validation loss: 2.7592501255773727

Epoch: 5| Step: 3
Training loss: 3.175163745880127
Validation loss: 2.7531321369191653

Epoch: 5| Step: 4
Training loss: 2.887315273284912
Validation loss: 2.7439831200466362

Epoch: 5| Step: 5
Training loss: 2.6220853328704834
Validation loss: 2.7422503668774842

Epoch: 5| Step: 6
Training loss: 3.521456480026245
Validation loss: 2.73622126220375

Epoch: 5| Step: 7
Training loss: 3.5202701091766357
Validation loss: 2.7363090438227498

Epoch: 5| Step: 8
Training loss: 3.0364205837249756
Validation loss: 2.7380779238157373

Epoch: 5| Step: 9
Training loss: 2.474259614944458
Validation loss: 2.7321937750744563

Epoch: 5| Step: 10
Training loss: 2.764817714691162
Validation loss: 2.733168194370885

Epoch: 18| Step: 0
Training loss: 2.542421817779541
Validation loss: 2.7337506150686615

Epoch: 5| Step: 1
Training loss: 3.0183730125427246
Validation loss: 2.730194701943346

Epoch: 5| Step: 2
Training loss: 3.148303985595703
Validation loss: 2.726569611539123

Epoch: 5| Step: 3
Training loss: 3.1236162185668945
Validation loss: 2.727676683856595

Epoch: 5| Step: 4
Training loss: 1.4555058479309082
Validation loss: 2.7249657441211004

Epoch: 5| Step: 5
Training loss: 2.5546340942382812
Validation loss: 2.7212827744022494

Epoch: 5| Step: 6
Training loss: 3.4097843170166016
Validation loss: 2.7156953119462535

Epoch: 5| Step: 7
Training loss: 3.2394745349884033
Validation loss: 2.7152166930578088

Epoch: 5| Step: 8
Training loss: 3.1231796741485596
Validation loss: 2.7096492654533795

Epoch: 5| Step: 9
Training loss: 3.409132480621338
Validation loss: 2.7064434430932485

Epoch: 5| Step: 10
Training loss: 2.6507368087768555
Validation loss: 2.7049060047313733

Epoch: 19| Step: 0
Training loss: 3.277437925338745
Validation loss: 2.70314303264823

Epoch: 5| Step: 1
Training loss: 2.25051212310791
Validation loss: 2.6989990818885063

Epoch: 5| Step: 2
Training loss: 3.0540759563446045
Validation loss: 2.7123699931688208

Epoch: 5| Step: 3
Training loss: 2.557878255844116
Validation loss: 2.705939982527046

Epoch: 5| Step: 4
Training loss: 2.5020575523376465
Validation loss: 2.6969240711581324

Epoch: 5| Step: 5
Training loss: 2.69706654548645
Validation loss: 2.702193675502654

Epoch: 5| Step: 6
Training loss: 3.3854033946990967
Validation loss: 2.725982694215672

Epoch: 5| Step: 7
Training loss: 3.2261385917663574
Validation loss: 2.730583111445109

Epoch: 5| Step: 8
Training loss: 2.2290892601013184
Validation loss: 2.698358956203666

Epoch: 5| Step: 9
Training loss: 3.5544726848602295
Validation loss: 2.7087303207766626

Epoch: 5| Step: 10
Training loss: 2.898653745651245
Validation loss: 2.719500959560435

Epoch: 20| Step: 0
Training loss: 2.4384350776672363
Validation loss: 2.7195270779312297

Epoch: 5| Step: 1
Training loss: 3.6027512550354004
Validation loss: 2.7310696596740396

Epoch: 5| Step: 2
Training loss: 2.4612796306610107
Validation loss: 2.7230316336436937

Epoch: 5| Step: 3
Training loss: 2.4989700317382812
Validation loss: 2.724214517942039

Epoch: 5| Step: 4
Training loss: 2.609309196472168
Validation loss: 2.7203811266089

Epoch: 5| Step: 5
Training loss: 3.246210813522339
Validation loss: 2.7015493249380462

Epoch: 5| Step: 6
Training loss: 2.6614432334899902
Validation loss: 2.6941874257979856

Epoch: 5| Step: 7
Training loss: 3.4585986137390137
Validation loss: 2.6898557216890397

Epoch: 5| Step: 8
Training loss: 2.5843067169189453
Validation loss: 2.6897983166479293

Epoch: 5| Step: 9
Training loss: 2.9778060913085938
Validation loss: 2.6918543102920696

Epoch: 5| Step: 10
Training loss: 3.028524398803711
Validation loss: 2.6940443182504303

Epoch: 21| Step: 0
Training loss: 2.8242669105529785
Validation loss: 2.693938201473605

Epoch: 5| Step: 1
Training loss: 2.857424259185791
Validation loss: 2.6992337139703895

Epoch: 5| Step: 2
Training loss: 2.721431016921997
Validation loss: 2.6892777873623754

Epoch: 5| Step: 3
Training loss: 3.4871840476989746
Validation loss: 2.689558849539808

Epoch: 5| Step: 4
Training loss: 2.1409382820129395
Validation loss: 2.6862577276845134

Epoch: 5| Step: 5
Training loss: 2.8639814853668213
Validation loss: 2.6775920596174014

Epoch: 5| Step: 6
Training loss: 3.743473529815674
Validation loss: 2.6797452485689552

Epoch: 5| Step: 7
Training loss: 3.3529372215270996
Validation loss: 2.681113499467091

Epoch: 5| Step: 8
Training loss: 1.9326680898666382
Validation loss: 2.6848771290112565

Epoch: 5| Step: 9
Training loss: 3.120029926300049
Validation loss: 2.684298904993201

Epoch: 5| Step: 10
Training loss: 2.277791976928711
Validation loss: 2.678070604160268

Epoch: 22| Step: 0
Training loss: 2.597850799560547
Validation loss: 2.675513962263702

Epoch: 5| Step: 1
Training loss: 2.598576307296753
Validation loss: 2.6755823730140604

Epoch: 5| Step: 2
Training loss: 2.5209691524505615
Validation loss: 2.6716326693052888

Epoch: 5| Step: 3
Training loss: 2.414365291595459
Validation loss: 2.6733708279107207

Epoch: 5| Step: 4
Training loss: 2.481128215789795
Validation loss: 2.6726321289616246

Epoch: 5| Step: 5
Training loss: 2.6364498138427734
Validation loss: 2.666822669326618

Epoch: 5| Step: 6
Training loss: 3.650131940841675
Validation loss: 2.6681695061345256

Epoch: 5| Step: 7
Training loss: 2.4265449047088623
Validation loss: 2.6673185479256416

Epoch: 5| Step: 8
Training loss: 3.25495982170105
Validation loss: 2.666430955292076

Epoch: 5| Step: 9
Training loss: 3.665210723876953
Validation loss: 2.6630770801216044

Epoch: 5| Step: 10
Training loss: 3.043675184249878
Validation loss: 2.667597986036731

Epoch: 23| Step: 0
Training loss: 3.4627387523651123
Validation loss: 2.678200844795473

Epoch: 5| Step: 1
Training loss: 3.101942777633667
Validation loss: 2.682041727086549

Epoch: 5| Step: 2
Training loss: 2.1011300086975098
Validation loss: 2.6864381195396505

Epoch: 5| Step: 3
Training loss: 2.4825034141540527
Validation loss: 2.6924506515584965

Epoch: 5| Step: 4
Training loss: 3.066084384918213
Validation loss: 2.6645656401111233

Epoch: 5| Step: 5
Training loss: 2.2156970500946045
Validation loss: 2.665992964980423

Epoch: 5| Step: 6
Training loss: 3.5084502696990967
Validation loss: 2.7007426420847573

Epoch: 5| Step: 7
Training loss: 2.7952897548675537
Validation loss: 2.708047569438975

Epoch: 5| Step: 8
Training loss: 3.659008741378784
Validation loss: 2.67137489780303

Epoch: 5| Step: 9
Training loss: 2.573251962661743
Validation loss: 2.667765548152308

Epoch: 5| Step: 10
Training loss: 2.3338241577148438
Validation loss: 2.663454863332933

Epoch: 24| Step: 0
Training loss: 3.5367953777313232
Validation loss: 2.666275783251691

Epoch: 5| Step: 1
Training loss: 2.764866828918457
Validation loss: 2.663870919135309

Epoch: 5| Step: 2
Training loss: 2.1939690113067627
Validation loss: 2.6592729322371946

Epoch: 5| Step: 3
Training loss: 2.6829144954681396
Validation loss: 2.657431689641809

Epoch: 5| Step: 4
Training loss: 2.672353982925415
Validation loss: 2.671898562421081

Epoch: 5| Step: 5
Training loss: 2.9634459018707275
Validation loss: 2.694090699636808

Epoch: 5| Step: 6
Training loss: 2.6012942790985107
Validation loss: 2.677682033149145

Epoch: 5| Step: 7
Training loss: 3.372637987136841
Validation loss: 2.6614173971196657

Epoch: 5| Step: 8
Training loss: 2.2335152626037598
Validation loss: 2.6557054570926133

Epoch: 5| Step: 9
Training loss: 3.0091378688812256
Validation loss: 2.659219636712023

Epoch: 5| Step: 10
Training loss: 3.239863395690918
Validation loss: 2.6634482158127653

Epoch: 25| Step: 0
Training loss: 2.118772506713867
Validation loss: 2.658096851841096

Epoch: 5| Step: 1
Training loss: 2.9136500358581543
Validation loss: 2.649883485609485

Epoch: 5| Step: 2
Training loss: 3.0979342460632324
Validation loss: 2.654763256349871

Epoch: 5| Step: 3
Training loss: 2.6625027656555176
Validation loss: 2.65241914667109

Epoch: 5| Step: 4
Training loss: 3.1039390563964844
Validation loss: 2.6492070613368863

Epoch: 5| Step: 5
Training loss: 2.5727896690368652
Validation loss: 2.652071478546307

Epoch: 5| Step: 6
Training loss: 2.5879974365234375
Validation loss: 2.655008428840227

Epoch: 5| Step: 7
Training loss: 3.183675765991211
Validation loss: 2.6482525076917423

Epoch: 5| Step: 8
Training loss: 3.1033828258514404
Validation loss: 2.6478485676550094

Epoch: 5| Step: 9
Training loss: 2.659132480621338
Validation loss: 2.6441070597658873

Epoch: 5| Step: 10
Training loss: 3.162363290786743
Validation loss: 2.6420640253251597

Epoch: 26| Step: 0
Training loss: 2.5894200801849365
Validation loss: 2.6630209466462493

Epoch: 5| Step: 1
Training loss: 2.608518123626709
Validation loss: 2.7444210744673208

Epoch: 5| Step: 2
Training loss: 3.3384921550750732
Validation loss: 2.734064612337338

Epoch: 5| Step: 3
Training loss: 2.4363224506378174
Validation loss: 2.6557026832334456

Epoch: 5| Step: 4
Training loss: 3.332211971282959
Validation loss: 2.6400310172829577

Epoch: 5| Step: 5
Training loss: 3.194312334060669
Validation loss: 2.643059079365064

Epoch: 5| Step: 6
Training loss: 3.0283918380737305
Validation loss: 2.6692711973703034

Epoch: 5| Step: 7
Training loss: 2.130305767059326
Validation loss: 2.6657223111839703

Epoch: 5| Step: 8
Training loss: 2.7458481788635254
Validation loss: 2.657838216391943

Epoch: 5| Step: 9
Training loss: 3.123089551925659
Validation loss: 2.6558411275186846

Epoch: 5| Step: 10
Training loss: 2.638855457305908
Validation loss: 2.6527670532144527

Epoch: 27| Step: 0
Training loss: 2.3332574367523193
Validation loss: 2.6462284006098264

Epoch: 5| Step: 1
Training loss: 2.7547574043273926
Validation loss: 2.644895540770664

Epoch: 5| Step: 2
Training loss: 2.6359524726867676
Validation loss: 2.645273708528088

Epoch: 5| Step: 3
Training loss: 2.7390003204345703
Validation loss: 2.638091451378279

Epoch: 5| Step: 4
Training loss: 3.345287799835205
Validation loss: 2.6401425228323987

Epoch: 5| Step: 5
Training loss: 2.7710328102111816
Validation loss: 2.6396725587947394

Epoch: 5| Step: 6
Training loss: 2.322598934173584
Validation loss: 2.638306548518519

Epoch: 5| Step: 7
Training loss: 3.047996997833252
Validation loss: 2.6399741326608965

Epoch: 5| Step: 8
Training loss: 3.1265807151794434
Validation loss: 2.655066236372917

Epoch: 5| Step: 9
Training loss: 3.3766441345214844
Validation loss: 2.6832466586943595

Epoch: 5| Step: 10
Training loss: 2.3909337520599365
Validation loss: 2.696345485666747

Epoch: 28| Step: 0
Training loss: 2.7790303230285645
Validation loss: 2.683592047742618

Epoch: 5| Step: 1
Training loss: 3.106088399887085
Validation loss: 2.659654922382806

Epoch: 5| Step: 2
Training loss: 2.9327712059020996
Validation loss: 2.6308639741713002

Epoch: 5| Step: 3
Training loss: 3.5960545539855957
Validation loss: 2.6297539716125815

Epoch: 5| Step: 4
Training loss: 2.5610461235046387
Validation loss: 2.628342120878158

Epoch: 5| Step: 5
Training loss: 3.1019413471221924
Validation loss: 2.6280772429640575

Epoch: 5| Step: 6
Training loss: 2.579322338104248
Validation loss: 2.6364275383692917

Epoch: 5| Step: 7
Training loss: 2.5626416206359863
Validation loss: 2.640291057607179

Epoch: 5| Step: 8
Training loss: 2.3630175590515137
Validation loss: 2.6509843334074943

Epoch: 5| Step: 9
Training loss: 2.5923750400543213
Validation loss: 2.6295393436185774

Epoch: 5| Step: 10
Training loss: 2.7689030170440674
Validation loss: 2.6208262110269196

Epoch: 29| Step: 0
Training loss: 2.981287717819214
Validation loss: 2.621838508113738

Epoch: 5| Step: 1
Training loss: 2.747709274291992
Validation loss: 2.6313977010788454

Epoch: 5| Step: 2
Training loss: 2.212566375732422
Validation loss: 2.651216627449118

Epoch: 5| Step: 3
Training loss: 2.7293200492858887
Validation loss: 2.6826872723076933

Epoch: 5| Step: 4
Training loss: 2.6815571784973145
Validation loss: 2.67345138519041

Epoch: 5| Step: 5
Training loss: 2.5484261512756348
Validation loss: 2.655467182077387

Epoch: 5| Step: 6
Training loss: 3.1296496391296387
Validation loss: 2.6342882058953725

Epoch: 5| Step: 7
Training loss: 3.136249542236328
Validation loss: 2.6243309154305408

Epoch: 5| Step: 8
Training loss: 4.033660888671875
Validation loss: 2.6098164358446674

Epoch: 5| Step: 9
Training loss: 2.638953685760498
Validation loss: 2.6156231716114986

Epoch: 5| Step: 10
Training loss: 1.7831430435180664
Validation loss: 2.6306469466096614

Epoch: 30| Step: 0
Training loss: 2.3541629314422607
Validation loss: 2.6349288853265906

Epoch: 5| Step: 1
Training loss: 2.5843565464019775
Validation loss: 2.6504959111572592

Epoch: 5| Step: 2
Training loss: 2.356053352355957
Validation loss: 2.625564588013516

Epoch: 5| Step: 3
Training loss: 3.053588628768921
Validation loss: 2.610383636207991

Epoch: 5| Step: 4
Training loss: 3.0318799018859863
Validation loss: 2.609317297576576

Epoch: 5| Step: 5
Training loss: 2.482410430908203
Validation loss: 2.6084586164002777

Epoch: 5| Step: 6
Training loss: 3.215341567993164
Validation loss: 2.610162955458446

Epoch: 5| Step: 7
Training loss: 2.8935437202453613
Validation loss: 2.6115597422404955

Epoch: 5| Step: 8
Training loss: 3.305962085723877
Validation loss: 2.6057397063060472

Epoch: 5| Step: 9
Training loss: 2.871351718902588
Validation loss: 2.6058580388305006

Epoch: 5| Step: 10
Training loss: 2.309156894683838
Validation loss: 2.6079474597848873

Epoch: 31| Step: 0
Training loss: 2.417292356491089
Validation loss: 2.6093755819464244

Epoch: 5| Step: 1
Training loss: 2.8645005226135254
Validation loss: 2.610357476818946

Epoch: 5| Step: 2
Training loss: 2.826388120651245
Validation loss: 2.6092244912219305

Epoch: 5| Step: 3
Training loss: 2.7829673290252686
Validation loss: 2.6047290653310795

Epoch: 5| Step: 4
Training loss: 2.510651111602783
Validation loss: 2.601905345916748

Epoch: 5| Step: 5
Training loss: 2.502800226211548
Validation loss: 2.5997006585521083

Epoch: 5| Step: 6
Training loss: 2.7367169857025146
Validation loss: 2.6001031860228507

Epoch: 5| Step: 7
Training loss: 3.115893840789795
Validation loss: 2.6078751266643567

Epoch: 5| Step: 8
Training loss: 3.5917892456054688
Validation loss: 2.613874520024946

Epoch: 5| Step: 9
Training loss: 2.733276844024658
Validation loss: 2.624575689274778

Epoch: 5| Step: 10
Training loss: 2.424105405807495
Validation loss: 2.6097387395879275

Epoch: 32| Step: 0
Training loss: 2.6784706115722656
Validation loss: 2.5959253054793163

Epoch: 5| Step: 1
Training loss: 3.8815701007843018
Validation loss: 2.5939862805028118

Epoch: 5| Step: 2
Training loss: 2.5174012184143066
Validation loss: 2.6074616627026628

Epoch: 5| Step: 3
Training loss: 2.9620800018310547
Validation loss: 2.626261749575215

Epoch: 5| Step: 4
Training loss: 3.34436297416687
Validation loss: 2.6260782826331353

Epoch: 5| Step: 5
Training loss: 2.5079071521759033
Validation loss: 2.620289812805832

Epoch: 5| Step: 6
Training loss: 2.311006546020508
Validation loss: 2.603216230228383

Epoch: 5| Step: 7
Training loss: 2.4855494499206543
Validation loss: 2.5932046316003285

Epoch: 5| Step: 8
Training loss: 2.744497776031494
Validation loss: 2.590314588239116

Epoch: 5| Step: 9
Training loss: 2.589017152786255
Validation loss: 2.6060361810909805

Epoch: 5| Step: 10
Training loss: 2.4903712272644043
Validation loss: 2.6211517241693314

Epoch: 33| Step: 0
Training loss: 3.0469021797180176
Validation loss: 2.6266873626298803

Epoch: 5| Step: 1
Training loss: 3.2477760314941406
Validation loss: 2.613830530515281

Epoch: 5| Step: 2
Training loss: 3.6614105701446533
Validation loss: 2.597098717125513

Epoch: 5| Step: 3
Training loss: 2.7364628314971924
Validation loss: 2.5899745597634265

Epoch: 5| Step: 4
Training loss: 2.5909645557403564
Validation loss: 2.5861409428299114

Epoch: 5| Step: 5
Training loss: 2.715989589691162
Validation loss: 2.580532127811063

Epoch: 5| Step: 6
Training loss: 2.2115330696105957
Validation loss: 2.5841257674719698

Epoch: 5| Step: 7
Training loss: 2.9509360790252686
Validation loss: 2.5907254295964397

Epoch: 5| Step: 8
Training loss: 2.367769718170166
Validation loss: 2.592650790368357

Epoch: 5| Step: 9
Training loss: 2.6091396808624268
Validation loss: 2.595082413765692

Epoch: 5| Step: 10
Training loss: 2.2686073780059814
Validation loss: 2.602999505176339

Epoch: 34| Step: 0
Training loss: 2.7199127674102783
Validation loss: 2.5871507634398756

Epoch: 5| Step: 1
Training loss: 2.819894313812256
Validation loss: 2.5810942778023342

Epoch: 5| Step: 2
Training loss: 2.288357734680176
Validation loss: 2.5766950038171585

Epoch: 5| Step: 3
Training loss: 2.851851224899292
Validation loss: 2.5756132884692122

Epoch: 5| Step: 4
Training loss: 2.930756092071533
Validation loss: 2.5721904282928794

Epoch: 5| Step: 5
Training loss: 3.2569737434387207
Validation loss: 2.5760005238235637

Epoch: 5| Step: 6
Training loss: 2.715471029281616
Validation loss: 2.5723432417838805

Epoch: 5| Step: 7
Training loss: 2.9887142181396484
Validation loss: 2.569970305247973

Epoch: 5| Step: 8
Training loss: 2.1977531909942627
Validation loss: 2.576529282395558

Epoch: 5| Step: 9
Training loss: 2.8334481716156006
Validation loss: 2.5730489864144275

Epoch: 5| Step: 10
Training loss: 2.625892162322998
Validation loss: 2.5726517861889255

Epoch: 35| Step: 0
Training loss: 3.055380344390869
Validation loss: 2.565128272579562

Epoch: 5| Step: 1
Training loss: 2.518104076385498
Validation loss: 2.565707483599263

Epoch: 5| Step: 2
Training loss: 2.4070725440979004
Validation loss: 2.5638495568306214

Epoch: 5| Step: 3
Training loss: 2.7666678428649902
Validation loss: 2.5622986798645346

Epoch: 5| Step: 4
Training loss: 2.4094326496124268
Validation loss: 2.5643418040326846

Epoch: 5| Step: 5
Training loss: 2.913212299346924
Validation loss: 2.5607153190079557

Epoch: 5| Step: 6
Training loss: 3.128899335861206
Validation loss: 2.5615725414727324

Epoch: 5| Step: 7
Training loss: 2.5971152782440186
Validation loss: 2.5567286219648135

Epoch: 5| Step: 8
Training loss: 3.0305633544921875
Validation loss: 2.5545698852949243

Epoch: 5| Step: 9
Training loss: 3.026639223098755
Validation loss: 2.561681474408796

Epoch: 5| Step: 10
Training loss: 2.1874241828918457
Validation loss: 2.566483256637409

Epoch: 36| Step: 0
Training loss: 2.5788674354553223
Validation loss: 2.5781504005514164

Epoch: 5| Step: 1
Training loss: 2.722437620162964
Validation loss: 2.578997835036247

Epoch: 5| Step: 2
Training loss: 3.0214991569519043
Validation loss: 2.5758767538173224

Epoch: 5| Step: 3
Training loss: 2.7393741607666016
Validation loss: 2.5778308555644047

Epoch: 5| Step: 4
Training loss: 2.8557238578796387
Validation loss: 2.5679139373123006

Epoch: 5| Step: 5
Training loss: 1.926430344581604
Validation loss: 2.5688642994050057

Epoch: 5| Step: 6
Training loss: 3.1270053386688232
Validation loss: 2.5562372348641835

Epoch: 5| Step: 7
Training loss: 2.574455738067627
Validation loss: 2.5495160369462866

Epoch: 5| Step: 8
Training loss: 2.9085471630096436
Validation loss: 2.5527948230825444

Epoch: 5| Step: 9
Training loss: 2.7635178565979004
Validation loss: 2.561751042642901

Epoch: 5| Step: 10
Training loss: 2.9408817291259766
Validation loss: 2.5634100821710404

Epoch: 37| Step: 0
Training loss: 3.179180145263672
Validation loss: 2.57744510199434

Epoch: 5| Step: 1
Training loss: 3.3646533489227295
Validation loss: 2.597777371765465

Epoch: 5| Step: 2
Training loss: 2.6361541748046875
Validation loss: 2.587673253910516

Epoch: 5| Step: 3
Training loss: 2.406392812728882
Validation loss: 2.571400175812424

Epoch: 5| Step: 4
Training loss: 2.7526888847351074
Validation loss: 2.5587007794328915

Epoch: 5| Step: 5
Training loss: 2.5650742053985596
Validation loss: 2.5472721181890017

Epoch: 5| Step: 6
Training loss: 3.103768825531006
Validation loss: 2.540850475270261

Epoch: 5| Step: 7
Training loss: 2.5072083473205566
Validation loss: 2.5494966224957536

Epoch: 5| Step: 8
Training loss: 2.164189100265503
Validation loss: 2.5641347208330707

Epoch: 5| Step: 9
Training loss: 2.7324306964874268
Validation loss: 2.5692398676308255

Epoch: 5| Step: 10
Training loss: 2.764286518096924
Validation loss: 2.5638674920605076

Epoch: 38| Step: 0
Training loss: 2.3699207305908203
Validation loss: 2.5707266407628215

Epoch: 5| Step: 1
Training loss: 2.9795780181884766
Validation loss: 2.5568767183570453

Epoch: 5| Step: 2
Training loss: 3.0784194469451904
Validation loss: 2.545078414742665

Epoch: 5| Step: 3
Training loss: 2.3763599395751953
Validation loss: 2.5308042187844553

Epoch: 5| Step: 4
Training loss: 3.3720526695251465
Validation loss: 2.5360226964437835

Epoch: 5| Step: 5
Training loss: 2.7200794219970703
Validation loss: 2.5236691018586517

Epoch: 5| Step: 6
Training loss: 2.3265299797058105
Validation loss: 2.526219375671879

Epoch: 5| Step: 7
Training loss: 2.7010457515716553
Validation loss: 2.5187990742345012

Epoch: 5| Step: 8
Training loss: 2.666435718536377
Validation loss: 2.51924971354905

Epoch: 5| Step: 9
Training loss: 2.357387065887451
Validation loss: 2.5186143857176586

Epoch: 5| Step: 10
Training loss: 3.0200624465942383
Validation loss: 2.5247924404759563

Epoch: 39| Step: 0
Training loss: 2.9372830390930176
Validation loss: 2.522446047875189

Epoch: 5| Step: 1
Training loss: 2.878540515899658
Validation loss: 2.5184756555864887

Epoch: 5| Step: 2
Training loss: 2.9619762897491455
Validation loss: 2.522163898714127

Epoch: 5| Step: 3
Training loss: 2.1377112865448
Validation loss: 2.5155945157492035

Epoch: 5| Step: 4
Training loss: 2.1374456882476807
Validation loss: 2.5140329560925885

Epoch: 5| Step: 5
Training loss: 2.1121222972869873
Validation loss: 2.5220129976990404

Epoch: 5| Step: 6
Training loss: 3.106679916381836
Validation loss: 2.5296055142597487

Epoch: 5| Step: 7
Training loss: 2.738523483276367
Validation loss: 2.5248641557590936

Epoch: 5| Step: 8
Training loss: 2.668332815170288
Validation loss: 2.529299907786872

Epoch: 5| Step: 9
Training loss: 3.1786856651306152
Validation loss: 2.5247643839928413

Epoch: 5| Step: 10
Training loss: 3.0557777881622314
Validation loss: 2.5180532137552896

Epoch: 40| Step: 0
Training loss: 2.3027451038360596
Validation loss: 2.5135142264827603

Epoch: 5| Step: 1
Training loss: 2.5456390380859375
Validation loss: 2.51117701684275

Epoch: 5| Step: 2
Training loss: 3.2029075622558594
Validation loss: 2.5067056276464976

Epoch: 5| Step: 3
Training loss: 3.228536605834961
Validation loss: 2.5010773751043502

Epoch: 5| Step: 4
Training loss: 2.2796683311462402
Validation loss: 2.502634168953024

Epoch: 5| Step: 5
Training loss: 2.2551660537719727
Validation loss: 2.5022022698515203

Epoch: 5| Step: 6
Training loss: 2.9672739505767822
Validation loss: 2.4991405189678235

Epoch: 5| Step: 7
Training loss: 2.868455410003662
Validation loss: 2.5024533579426427

Epoch: 5| Step: 8
Training loss: 2.5837039947509766
Validation loss: 2.511425592566049

Epoch: 5| Step: 9
Training loss: 2.778982400894165
Validation loss: 2.523559299848413

Epoch: 5| Step: 10
Training loss: 2.717303514480591
Validation loss: 2.5234837685861895

Epoch: 41| Step: 0
Training loss: 3.0477185249328613
Validation loss: 2.5275066949987925

Epoch: 5| Step: 1
Training loss: 2.616753101348877
Validation loss: 2.4986089096274426

Epoch: 5| Step: 2
Training loss: 2.4681146144866943
Validation loss: 2.495355731697493

Epoch: 5| Step: 3
Training loss: 2.604722023010254
Validation loss: 2.4914931481884373

Epoch: 5| Step: 4
Training loss: 2.6272664070129395
Validation loss: 2.49686312419112

Epoch: 5| Step: 5
Training loss: 3.3581326007843018
Validation loss: 2.4982288678487143

Epoch: 5| Step: 6
Training loss: 2.8509140014648438
Validation loss: 2.499239626751151

Epoch: 5| Step: 7
Training loss: 3.2386131286621094
Validation loss: 2.4946502511219313

Epoch: 5| Step: 8
Training loss: 1.970770239830017
Validation loss: 2.493671614636657

Epoch: 5| Step: 9
Training loss: 2.858591318130493
Validation loss: 2.489043597252138

Epoch: 5| Step: 10
Training loss: 2.0211429595947266
Validation loss: 2.494502595675889

Epoch: 42| Step: 0
Training loss: 2.7972350120544434
Validation loss: 2.498357465190272

Epoch: 5| Step: 1
Training loss: 2.235605239868164
Validation loss: 2.520313234739406

Epoch: 5| Step: 2
Training loss: 3.494699478149414
Validation loss: 2.52954577630566

Epoch: 5| Step: 3
Training loss: 2.1542439460754395
Validation loss: 2.5742652288047214

Epoch: 5| Step: 4
Training loss: 2.8835506439208984
Validation loss: 2.5202341643712853

Epoch: 5| Step: 5
Training loss: 2.9030890464782715
Validation loss: 2.4966492191437752

Epoch: 5| Step: 6
Training loss: 2.109780788421631
Validation loss: 2.487266312363327

Epoch: 5| Step: 7
Training loss: 2.2962563037872314
Validation loss: 2.486910799498199

Epoch: 5| Step: 8
Training loss: 3.1912026405334473
Validation loss: 2.499732848136656

Epoch: 5| Step: 9
Training loss: 2.573340892791748
Validation loss: 2.5171591107563307

Epoch: 5| Step: 10
Training loss: 3.162172317504883
Validation loss: 2.521876455635153

Epoch: 43| Step: 0
Training loss: 2.9398059844970703
Validation loss: 2.5195270276838735

Epoch: 5| Step: 1
Training loss: 3.177891254425049
Validation loss: 2.5044946657714022

Epoch: 5| Step: 2
Training loss: 2.333909273147583
Validation loss: 2.5024729018570273

Epoch: 5| Step: 3
Training loss: 2.3377413749694824
Validation loss: 2.4889188197351273

Epoch: 5| Step: 4
Training loss: 3.106902837753296
Validation loss: 2.4922457305333947

Epoch: 5| Step: 5
Training loss: 2.8414080142974854
Validation loss: 2.5086233333874772

Epoch: 5| Step: 6
Training loss: 2.2364039421081543
Validation loss: 2.559367379834575

Epoch: 5| Step: 7
Training loss: 2.577320098876953
Validation loss: 2.5902176185320784

Epoch: 5| Step: 8
Training loss: 3.3885085582733154
Validation loss: 2.63297930840523

Epoch: 5| Step: 9
Training loss: 2.3836350440979004
Validation loss: 2.5759297929784304

Epoch: 5| Step: 10
Training loss: 2.669910192489624
Validation loss: 2.530888477961222

Epoch: 44| Step: 0
Training loss: 2.9556870460510254
Validation loss: 2.4892626475262385

Epoch: 5| Step: 1
Training loss: 3.387627124786377
Validation loss: 2.489513220325593

Epoch: 5| Step: 2
Training loss: 2.729332447052002
Validation loss: 2.5134085916703746

Epoch: 5| Step: 3
Training loss: 2.1833183765411377
Validation loss: 2.5486042576451458

Epoch: 5| Step: 4
Training loss: 2.4644789695739746
Validation loss: 2.5308791642547934

Epoch: 5| Step: 5
Training loss: 1.8205989599227905
Validation loss: 2.501227207081292

Epoch: 5| Step: 6
Training loss: 2.949073076248169
Validation loss: 2.483756270459903

Epoch: 5| Step: 7
Training loss: 2.718667507171631
Validation loss: 2.475777587582988

Epoch: 5| Step: 8
Training loss: 2.7496438026428223
Validation loss: 2.4784968027504544

Epoch: 5| Step: 9
Training loss: 2.9840023517608643
Validation loss: 2.4842547114177416

Epoch: 5| Step: 10
Training loss: 2.8602733612060547
Validation loss: 2.486852781746977

Epoch: 45| Step: 0
Training loss: 2.0794053077697754
Validation loss: 2.489597523084251

Epoch: 5| Step: 1
Training loss: 2.9836583137512207
Validation loss: 2.499436222096925

Epoch: 5| Step: 2
Training loss: 2.6124367713928223
Validation loss: 2.4961437384287515

Epoch: 5| Step: 3
Training loss: 2.862946033477783
Validation loss: 2.4937195957347913

Epoch: 5| Step: 4
Training loss: 2.280086040496826
Validation loss: 2.478935351935766

Epoch: 5| Step: 5
Training loss: 2.478745460510254
Validation loss: 2.4730131754311184

Epoch: 5| Step: 6
Training loss: 2.423755645751953
Validation loss: 2.4730766921915035

Epoch: 5| Step: 7
Training loss: 2.8967394828796387
Validation loss: 2.467338392811437

Epoch: 5| Step: 8
Training loss: 2.7371861934661865
Validation loss: 2.469339265618273

Epoch: 5| Step: 9
Training loss: 3.329517364501953
Validation loss: 2.47235361478662

Epoch: 5| Step: 10
Training loss: 3.1328372955322266
Validation loss: 2.4713993226328204

Epoch: 46| Step: 0
Training loss: 2.9969398975372314
Validation loss: 2.473122130158127

Epoch: 5| Step: 1
Training loss: 3.1285240650177
Validation loss: 2.47111020036923

Epoch: 5| Step: 2
Training loss: 2.28572940826416
Validation loss: 2.4671552975972495

Epoch: 5| Step: 3
Training loss: 2.0935559272766113
Validation loss: 2.463207569173587

Epoch: 5| Step: 4
Training loss: 2.8166823387145996
Validation loss: 2.474909492718276

Epoch: 5| Step: 5
Training loss: 2.7038190364837646
Validation loss: 2.486232926768641

Epoch: 5| Step: 6
Training loss: 2.851963758468628
Validation loss: 2.5151725507551626

Epoch: 5| Step: 7
Training loss: 3.030503511428833
Validation loss: 2.516015934687789

Epoch: 5| Step: 8
Training loss: 2.4319992065429688
Validation loss: 2.500649718828099

Epoch: 5| Step: 9
Training loss: 2.562448024749756
Validation loss: 2.486673714012228

Epoch: 5| Step: 10
Training loss: 2.7440505027770996
Validation loss: 2.47999817837951

Epoch: 47| Step: 0
Training loss: 3.1828858852386475
Validation loss: 2.4623739539936023

Epoch: 5| Step: 1
Training loss: 2.261990547180176
Validation loss: 2.4645121046291885

Epoch: 5| Step: 2
Training loss: 2.9912772178649902
Validation loss: 2.467200699672904

Epoch: 5| Step: 3
Training loss: 2.6697146892547607
Validation loss: 2.4763819248445573

Epoch: 5| Step: 4
Training loss: 2.5131773948669434
Validation loss: 2.484494411817161

Epoch: 5| Step: 5
Training loss: 2.912010431289673
Validation loss: 2.494739845234861

Epoch: 5| Step: 6
Training loss: 2.945258855819702
Validation loss: 2.492922076614954

Epoch: 5| Step: 7
Training loss: 2.577667474746704
Validation loss: 2.4889446458508893

Epoch: 5| Step: 8
Training loss: 2.6625189781188965
Validation loss: 2.476731131153722

Epoch: 5| Step: 9
Training loss: 2.126357078552246
Validation loss: 2.4689218664682038

Epoch: 5| Step: 10
Training loss: 2.921205759048462
Validation loss: 2.4601028529546594

Epoch: 48| Step: 0
Training loss: 3.119717836380005
Validation loss: 2.4513127419256393

Epoch: 5| Step: 1
Training loss: 2.705451488494873
Validation loss: 2.449685404377599

Epoch: 5| Step: 2
Training loss: 2.1669576168060303
Validation loss: 2.4600918677545365

Epoch: 5| Step: 3
Training loss: 2.4710991382598877
Validation loss: 2.472650638190649

Epoch: 5| Step: 4
Training loss: 2.3820903301239014
Validation loss: 2.4828514463158062

Epoch: 5| Step: 5
Training loss: 2.855825901031494
Validation loss: 2.491076256639214

Epoch: 5| Step: 6
Training loss: 2.913062334060669
Validation loss: 2.489949716034756

Epoch: 5| Step: 7
Training loss: 3.4237327575683594
Validation loss: 2.483180146063528

Epoch: 5| Step: 8
Training loss: 2.3856420516967773
Validation loss: 2.47844636055731

Epoch: 5| Step: 9
Training loss: 2.703950881958008
Validation loss: 2.47420403521548

Epoch: 5| Step: 10
Training loss: 2.246614456176758
Validation loss: 2.4748522235501196

Epoch: 49| Step: 0
Training loss: 2.563290596008301
Validation loss: 2.4643904265537055

Epoch: 5| Step: 1
Training loss: 3.089616298675537
Validation loss: 2.453881002241565

Epoch: 5| Step: 2
Training loss: 2.7193241119384766
Validation loss: 2.4531768111772436

Epoch: 5| Step: 3
Training loss: 2.3237833976745605
Validation loss: 2.453252079666302

Epoch: 5| Step: 4
Training loss: 2.7950055599212646
Validation loss: 2.455049955716697

Epoch: 5| Step: 5
Training loss: 2.6336894035339355
Validation loss: 2.460409538720244

Epoch: 5| Step: 6
Training loss: 3.0623691082000732
Validation loss: 2.4751887475290606

Epoch: 5| Step: 7
Training loss: 2.4089133739471436
Validation loss: 2.4717412097479707

Epoch: 5| Step: 8
Training loss: 2.948772430419922
Validation loss: 2.4625925556305917

Epoch: 5| Step: 9
Training loss: 2.4484333992004395
Validation loss: 2.447362164015411

Epoch: 5| Step: 10
Training loss: 2.464841842651367
Validation loss: 2.447071957331832

Epoch: 50| Step: 0
Training loss: 1.7896696329116821
Validation loss: 2.452178152658606

Epoch: 5| Step: 1
Training loss: 3.0270206928253174
Validation loss: 2.470544327971756

Epoch: 5| Step: 2
Training loss: 2.7091610431671143
Validation loss: 2.4942346721567135

Epoch: 5| Step: 3
Training loss: 2.2296295166015625
Validation loss: 2.4948071561833864

Epoch: 5| Step: 4
Training loss: 2.527556896209717
Validation loss: 2.484479276082849

Epoch: 5| Step: 5
Training loss: 2.8458433151245117
Validation loss: 2.4609801846165813

Epoch: 5| Step: 6
Training loss: 3.476187229156494
Validation loss: 2.4459014861814437

Epoch: 5| Step: 7
Training loss: 2.93591570854187
Validation loss: 2.4422972151028213

Epoch: 5| Step: 8
Training loss: 2.36200213432312
Validation loss: 2.443141601418936

Epoch: 5| Step: 9
Training loss: 2.854311943054199
Validation loss: 2.447340226942493

Epoch: 5| Step: 10
Training loss: 2.7097036838531494
Validation loss: 2.445971373588808

Epoch: 51| Step: 0
Training loss: 2.016475200653076
Validation loss: 2.446306500383603

Epoch: 5| Step: 1
Training loss: 2.759342670440674
Validation loss: 2.440339598604428

Epoch: 5| Step: 2
Training loss: 2.3123645782470703
Validation loss: 2.4392751929580525

Epoch: 5| Step: 3
Training loss: 2.4451956748962402
Validation loss: 2.436289515546573

Epoch: 5| Step: 4
Training loss: 3.0835702419281006
Validation loss: 2.433245058982603

Epoch: 5| Step: 5
Training loss: 3.2510154247283936
Validation loss: 2.438011448870423

Epoch: 5| Step: 6
Training loss: 2.4757676124572754
Validation loss: 2.4404410687826013

Epoch: 5| Step: 7
Training loss: 2.504730463027954
Validation loss: 2.4413694002295054

Epoch: 5| Step: 8
Training loss: 3.236799716949463
Validation loss: 2.4453042245680288

Epoch: 5| Step: 9
Training loss: 2.837214946746826
Validation loss: 2.450208358867194

Epoch: 5| Step: 10
Training loss: 2.3076748847961426
Validation loss: 2.448307591099893

Epoch: 52| Step: 0
Training loss: 2.6168277263641357
Validation loss: 2.469422278865691

Epoch: 5| Step: 1
Training loss: 2.9123828411102295
Validation loss: 2.466228049288514

Epoch: 5| Step: 2
Training loss: 2.7411751747131348
Validation loss: 2.4729907307573544

Epoch: 5| Step: 3
Training loss: 2.4574687480926514
Validation loss: 2.472885260017969

Epoch: 5| Step: 4
Training loss: 2.9828054904937744
Validation loss: 2.4617660712170344

Epoch: 5| Step: 5
Training loss: 3.250922679901123
Validation loss: 2.448298224838831

Epoch: 5| Step: 6
Training loss: 2.7276694774627686
Validation loss: 2.4377641652220037

Epoch: 5| Step: 7
Training loss: 2.498180866241455
Validation loss: 2.427200286619125

Epoch: 5| Step: 8
Training loss: 2.1585874557495117
Validation loss: 2.4259752765778573

Epoch: 5| Step: 9
Training loss: 2.684196949005127
Validation loss: 2.4247043389146046

Epoch: 5| Step: 10
Training loss: 2.306892156600952
Validation loss: 2.4261878792957594

Epoch: 53| Step: 0
Training loss: 3.089015483856201
Validation loss: 2.427248017762297

Epoch: 5| Step: 1
Training loss: 2.5012707710266113
Validation loss: 2.4246529763744724

Epoch: 5| Step: 2
Training loss: 2.315096616744995
Validation loss: 2.425554854895479

Epoch: 5| Step: 3
Training loss: 2.8689918518066406
Validation loss: 2.424267074113251

Epoch: 5| Step: 4
Training loss: 2.5754215717315674
Validation loss: 2.4244892469016452

Epoch: 5| Step: 5
Training loss: 3.04929518699646
Validation loss: 2.4233647187550864

Epoch: 5| Step: 6
Training loss: 2.6797935962677
Validation loss: 2.4223937091007026

Epoch: 5| Step: 7
Training loss: 2.2589516639709473
Validation loss: 2.422822206251083

Epoch: 5| Step: 8
Training loss: 2.6539711952209473
Validation loss: 2.421092105168168

Epoch: 5| Step: 9
Training loss: 2.3766369819641113
Validation loss: 2.421806363649266

Epoch: 5| Step: 10
Training loss: 2.824476957321167
Validation loss: 2.421864177591057

Epoch: 54| Step: 0
Training loss: 2.5163280963897705
Validation loss: 2.4232955081488496

Epoch: 5| Step: 1
Training loss: 2.430276870727539
Validation loss: 2.4259282658177037

Epoch: 5| Step: 2
Training loss: 2.5728647708892822
Validation loss: 2.4296908993874826

Epoch: 5| Step: 3
Training loss: 3.266326904296875
Validation loss: 2.4220420083692

Epoch: 5| Step: 4
Training loss: 2.4316418170928955
Validation loss: 2.422202924246429

Epoch: 5| Step: 5
Training loss: 2.5638742446899414
Validation loss: 2.4185931503131823

Epoch: 5| Step: 6
Training loss: 2.269624710083008
Validation loss: 2.423358166089622

Epoch: 5| Step: 7
Training loss: 2.979038953781128
Validation loss: 2.425743979792441

Epoch: 5| Step: 8
Training loss: 2.7351231575012207
Validation loss: 2.4227473120535574

Epoch: 5| Step: 9
Training loss: 2.677875518798828
Validation loss: 2.4269887721666725

Epoch: 5| Step: 10
Training loss: 2.694340944290161
Validation loss: 2.429845722772742

Epoch: 55| Step: 0
Training loss: 2.6174702644348145
Validation loss: 2.424799244890931

Epoch: 5| Step: 1
Training loss: 2.3761420249938965
Validation loss: 2.4158103440397527

Epoch: 5| Step: 2
Training loss: 2.2887187004089355
Validation loss: 2.4145985982751332

Epoch: 5| Step: 3
Training loss: 2.4157371520996094
Validation loss: 2.4156963953407864

Epoch: 5| Step: 4
Training loss: 2.811067819595337
Validation loss: 2.4121573817345405

Epoch: 5| Step: 5
Training loss: 2.667980194091797
Validation loss: 2.4143939120795137

Epoch: 5| Step: 6
Training loss: 3.2293639183044434
Validation loss: 2.4165896113200853

Epoch: 5| Step: 7
Training loss: 2.9135804176330566
Validation loss: 2.424650687043385

Epoch: 5| Step: 8
Training loss: 2.565054416656494
Validation loss: 2.427951858889672

Epoch: 5| Step: 9
Training loss: 2.372096538543701
Validation loss: 2.4287228199743454

Epoch: 5| Step: 10
Training loss: 2.8654494285583496
Validation loss: 2.4213463414099907

Epoch: 56| Step: 0
Training loss: 3.133526563644409
Validation loss: 2.416057430287843

Epoch: 5| Step: 1
Training loss: 3.025367021560669
Validation loss: 2.4157717356117825

Epoch: 5| Step: 2
Training loss: 2.2038373947143555
Validation loss: 2.4161976357941986

Epoch: 5| Step: 3
Training loss: 2.6520845890045166
Validation loss: 2.4195152098132717

Epoch: 5| Step: 4
Training loss: 3.0897302627563477
Validation loss: 2.41237207125592

Epoch: 5| Step: 5
Training loss: 2.2770726680755615
Validation loss: 2.4071504223731255

Epoch: 5| Step: 6
Training loss: 2.4444797039031982
Validation loss: 2.410745323345225

Epoch: 5| Step: 7
Training loss: 2.8024086952209473
Validation loss: 2.4050641521330802

Epoch: 5| Step: 8
Training loss: 2.1430087089538574
Validation loss: 2.411408203904347

Epoch: 5| Step: 9
Training loss: 2.526461124420166
Validation loss: 2.409759536866219

Epoch: 5| Step: 10
Training loss: 2.7613749504089355
Validation loss: 2.412243896915067

Epoch: 57| Step: 0
Training loss: 2.950334310531616
Validation loss: 2.4038084912043747

Epoch: 5| Step: 1
Training loss: 2.650970220565796
Validation loss: 2.400539413575203

Epoch: 5| Step: 2
Training loss: 3.262328624725342
Validation loss: 2.401172807139735

Epoch: 5| Step: 3
Training loss: 3.1730499267578125
Validation loss: 2.3998200713947253

Epoch: 5| Step: 4
Training loss: 1.8872333765029907
Validation loss: 2.3999832163574877

Epoch: 5| Step: 5
Training loss: 2.1474411487579346
Validation loss: 2.399907353103802

Epoch: 5| Step: 6
Training loss: 2.9932339191436768
Validation loss: 2.400576817092075

Epoch: 5| Step: 7
Training loss: 1.8156116008758545
Validation loss: 2.4056022218478623

Epoch: 5| Step: 8
Training loss: 2.5853843688964844
Validation loss: 2.41392062299995

Epoch: 5| Step: 9
Training loss: 2.3740952014923096
Validation loss: 2.4170735318173646

Epoch: 5| Step: 10
Training loss: 3.2595009803771973
Validation loss: 2.4205512615942184

Epoch: 58| Step: 0
Training loss: 2.8059797286987305
Validation loss: 2.4165005606989705

Epoch: 5| Step: 1
Training loss: 2.792325496673584
Validation loss: 2.4048192783068587

Epoch: 5| Step: 2
Training loss: 2.1759707927703857
Validation loss: 2.4012377979934856

Epoch: 5| Step: 3
Training loss: 2.512662172317505
Validation loss: 2.400497467287125

Epoch: 5| Step: 4
Training loss: 2.5501606464385986
Validation loss: 2.3953061719094553

Epoch: 5| Step: 5
Training loss: 2.526211977005005
Validation loss: 2.392214749449043

Epoch: 5| Step: 6
Training loss: 2.8969435691833496
Validation loss: 2.3960574288522043

Epoch: 5| Step: 7
Training loss: 3.1006131172180176
Validation loss: 2.400169223867437

Epoch: 5| Step: 8
Training loss: 2.472228765487671
Validation loss: 2.414311908906506

Epoch: 5| Step: 9
Training loss: 2.719207763671875
Validation loss: 2.4140610105247906

Epoch: 5| Step: 10
Training loss: 2.3554718494415283
Validation loss: 2.4213180259991716

Epoch: 59| Step: 0
Training loss: 2.8339672088623047
Validation loss: 2.404576086228894

Epoch: 5| Step: 1
Training loss: 1.8983955383300781
Validation loss: 2.4023911030061784

Epoch: 5| Step: 2
Training loss: 2.6171720027923584
Validation loss: 2.4069834960404264

Epoch: 5| Step: 3
Training loss: 2.340003252029419
Validation loss: 2.41221555074056

Epoch: 5| Step: 4
Training loss: 3.186488389968872
Validation loss: 2.405556155789283

Epoch: 5| Step: 5
Training loss: 2.789257049560547
Validation loss: 2.4005743483061432

Epoch: 5| Step: 6
Training loss: 2.3700592517852783
Validation loss: 2.3882755233395483

Epoch: 5| Step: 7
Training loss: 2.6085610389709473
Validation loss: 2.390067582489342

Epoch: 5| Step: 8
Training loss: 2.3265039920806885
Validation loss: 2.391459875209357

Epoch: 5| Step: 9
Training loss: 3.0124363899230957
Validation loss: 2.39555545519757

Epoch: 5| Step: 10
Training loss: 3.031172752380371
Validation loss: 2.399050335730276

Epoch: 60| Step: 0
Training loss: 2.6492950916290283
Validation loss: 2.407063497010098

Epoch: 5| Step: 1
Training loss: 2.328213930130005
Validation loss: 2.4026904413777013

Epoch: 5| Step: 2
Training loss: 2.0264382362365723
Validation loss: 2.3996831191483365

Epoch: 5| Step: 3
Training loss: 3.366415023803711
Validation loss: 2.4043305920016382

Epoch: 5| Step: 4
Training loss: 2.4851040840148926
Validation loss: 2.400256295357981

Epoch: 5| Step: 5
Training loss: 2.20849609375
Validation loss: 2.3971267259249123

Epoch: 5| Step: 6
Training loss: 2.539963483810425
Validation loss: 2.401206836905531

Epoch: 5| Step: 7
Training loss: 2.7184829711914062
Validation loss: 2.3946429837134575

Epoch: 5| Step: 8
Training loss: 2.8748393058776855
Validation loss: 2.3877551119814635

Epoch: 5| Step: 9
Training loss: 3.2145791053771973
Validation loss: 2.3824713460860716

Epoch: 5| Step: 10
Training loss: 2.534522294998169
Validation loss: 2.3804075025743052

Epoch: 61| Step: 0
Training loss: 2.577213764190674
Validation loss: 2.3968754942699144

Epoch: 5| Step: 1
Training loss: 3.0540964603424072
Validation loss: 2.4396512482755925

Epoch: 5| Step: 2
Training loss: 2.356630325317383
Validation loss: 2.482194121165942

Epoch: 5| Step: 3
Training loss: 2.756246566772461
Validation loss: 2.478975448557126

Epoch: 5| Step: 4
Training loss: 3.100602388381958
Validation loss: 2.4505823401994604

Epoch: 5| Step: 5
Training loss: 3.009810447692871
Validation loss: 2.412466073548922

Epoch: 5| Step: 6
Training loss: 2.19846773147583
Validation loss: 2.377835019942253

Epoch: 5| Step: 7
Training loss: 2.509551763534546
Validation loss: 2.3785860307755007

Epoch: 5| Step: 8
Training loss: 2.697538137435913
Validation loss: 2.3766509973874657

Epoch: 5| Step: 9
Training loss: 2.3168272972106934
Validation loss: 2.380671006377025

Epoch: 5| Step: 10
Training loss: 2.5991134643554688
Validation loss: 2.383978841125324

Epoch: 62| Step: 0
Training loss: 2.5778026580810547
Validation loss: 2.3871318909429733

Epoch: 5| Step: 1
Training loss: 2.138216733932495
Validation loss: 2.384244615031827

Epoch: 5| Step: 2
Training loss: 3.6628212928771973
Validation loss: 2.391026217450378

Epoch: 5| Step: 3
Training loss: 2.7882940769195557
Validation loss: 2.380928712506448

Epoch: 5| Step: 4
Training loss: 2.541928768157959
Validation loss: 2.374010709024245

Epoch: 5| Step: 5
Training loss: 2.6691770553588867
Validation loss: 2.3723102487543577

Epoch: 5| Step: 6
Training loss: 2.451024293899536
Validation loss: 2.391194802458568

Epoch: 5| Step: 7
Training loss: 2.287515640258789
Validation loss: 2.4322048643583893

Epoch: 5| Step: 8
Training loss: 2.8306987285614014
Validation loss: 2.4729587980495986

Epoch: 5| Step: 9
Training loss: 3.001229763031006
Validation loss: 2.479193131128947

Epoch: 5| Step: 10
Training loss: 2.069732904434204
Validation loss: 2.474334342505342

Epoch: 63| Step: 0
Training loss: 2.894326686859131
Validation loss: 2.4697437132558515

Epoch: 5| Step: 1
Training loss: 2.0297324657440186
Validation loss: 2.45491353670756

Epoch: 5| Step: 2
Training loss: 2.1179308891296387
Validation loss: 2.4138215434166694

Epoch: 5| Step: 3
Training loss: 2.853348970413208
Validation loss: 2.393941176834927

Epoch: 5| Step: 4
Training loss: 2.757201671600342
Validation loss: 2.3836120918232906

Epoch: 5| Step: 5
Training loss: 2.6420745849609375
Validation loss: 2.3830513749071347

Epoch: 5| Step: 6
Training loss: 2.7798824310302734
Validation loss: 2.3962345123291016

Epoch: 5| Step: 7
Training loss: 3.093440294265747
Validation loss: 2.4131119789615756

Epoch: 5| Step: 8
Training loss: 2.84757137298584
Validation loss: 2.379340761451311

Epoch: 5| Step: 9
Training loss: 2.4487812519073486
Validation loss: 2.3751754760742188

Epoch: 5| Step: 10
Training loss: 2.6727848052978516
Validation loss: 2.3651126046334543

Epoch: 64| Step: 0
Training loss: 2.7387051582336426
Validation loss: 2.378235432409471

Epoch: 5| Step: 1
Training loss: 2.423952102661133
Validation loss: 2.376952350780528

Epoch: 5| Step: 2
Training loss: 2.9349918365478516
Validation loss: 2.3675742149353027

Epoch: 5| Step: 3
Training loss: 2.9740350246429443
Validation loss: 2.3659787998404553

Epoch: 5| Step: 4
Training loss: 2.8524577617645264
Validation loss: 2.364228110159597

Epoch: 5| Step: 5
Training loss: 2.053541660308838
Validation loss: 2.36324546926765

Epoch: 5| Step: 6
Training loss: 2.899423122406006
Validation loss: 2.378510252121956

Epoch: 5| Step: 7
Training loss: 2.5473990440368652
Validation loss: 2.3875459906875447

Epoch: 5| Step: 8
Training loss: 3.0147995948791504
Validation loss: 2.3999983213281118

Epoch: 5| Step: 9
Training loss: 2.433563709259033
Validation loss: 2.3899423973534697

Epoch: 5| Step: 10
Training loss: 1.805285096168518
Validation loss: 2.390940377789159

Epoch: 65| Step: 0
Training loss: 3.3499789237976074
Validation loss: 2.392736251636218

Epoch: 5| Step: 1
Training loss: 2.456613063812256
Validation loss: 2.381954582788611

Epoch: 5| Step: 2
Training loss: 2.51092267036438
Validation loss: 2.3685842252546743

Epoch: 5| Step: 3
Training loss: 2.4268734455108643
Validation loss: 2.3657282014046945

Epoch: 5| Step: 4
Training loss: 1.9634475708007812
Validation loss: 2.358379708823337

Epoch: 5| Step: 5
Training loss: 2.4285264015197754
Validation loss: 2.3660991896865187

Epoch: 5| Step: 6
Training loss: 2.905353546142578
Validation loss: 2.369468976092595

Epoch: 5| Step: 7
Training loss: 2.964041233062744
Validation loss: 2.3700386990783033

Epoch: 5| Step: 8
Training loss: 3.0681846141815186
Validation loss: 2.392920140297182

Epoch: 5| Step: 9
Training loss: 2.2115163803100586
Validation loss: 2.4043431846044396

Epoch: 5| Step: 10
Training loss: 2.514514446258545
Validation loss: 2.395737450609925

Epoch: 66| Step: 0
Training loss: 2.7264328002929688
Validation loss: 2.3838880344103743

Epoch: 5| Step: 1
Training loss: 2.914252519607544
Validation loss: 2.3605258234085573

Epoch: 5| Step: 2
Training loss: 2.2667481899261475
Validation loss: 2.356810356981011

Epoch: 5| Step: 3
Training loss: 2.1595866680145264
Validation loss: 2.351552371055849

Epoch: 5| Step: 4
Training loss: 3.481560468673706
Validation loss: 2.3520511978416034

Epoch: 5| Step: 5
Training loss: 2.708522319793701
Validation loss: 2.351307738211847

Epoch: 5| Step: 6
Training loss: 2.7525346279144287
Validation loss: 2.3546662022990565

Epoch: 5| Step: 7
Training loss: 1.9967238903045654
Validation loss: 2.3509376971952376

Epoch: 5| Step: 8
Training loss: 3.009366512298584
Validation loss: 2.350600734833748

Epoch: 5| Step: 9
Training loss: 2.061588764190674
Validation loss: 2.3489563362572783

Epoch: 5| Step: 10
Training loss: 2.719360828399658
Validation loss: 2.347521217920447

Epoch: 67| Step: 0
Training loss: 2.7133681774139404
Validation loss: 2.3522154156879713

Epoch: 5| Step: 1
Training loss: 2.48514986038208
Validation loss: 2.3642898169896935

Epoch: 5| Step: 2
Training loss: 1.899468183517456
Validation loss: 2.3851735899525304

Epoch: 5| Step: 3
Training loss: 2.6907153129577637
Validation loss: 2.4000529576373357

Epoch: 5| Step: 4
Training loss: 2.056427001953125
Validation loss: 2.4059951395116825

Epoch: 5| Step: 5
Training loss: 2.8041281700134277
Validation loss: 2.4104290700727895

Epoch: 5| Step: 6
Training loss: 3.0870912075042725
Validation loss: 2.395927949618268

Epoch: 5| Step: 7
Training loss: 3.068784475326538
Validation loss: 2.3782740357101604

Epoch: 5| Step: 8
Training loss: 2.200413227081299
Validation loss: 2.351974356559015

Epoch: 5| Step: 9
Training loss: 2.9450979232788086
Validation loss: 2.3394896650827057

Epoch: 5| Step: 10
Training loss: 2.7442097663879395
Validation loss: 2.3348305789373254

Epoch: 68| Step: 0
Training loss: 2.5141427516937256
Validation loss: 2.3376867104602117

Epoch: 5| Step: 1
Training loss: 2.096436023712158
Validation loss: 2.3406007059158815

Epoch: 5| Step: 2
Training loss: 2.506304979324341
Validation loss: 2.3432039009627474

Epoch: 5| Step: 3
Training loss: 2.5824570655822754
Validation loss: 2.339483248290195

Epoch: 5| Step: 4
Training loss: 2.731123924255371
Validation loss: 2.3390731196249686

Epoch: 5| Step: 5
Training loss: 2.9349682331085205
Validation loss: 2.3391627598834295

Epoch: 5| Step: 6
Training loss: 2.7402567863464355
Validation loss: 2.3382204373677573

Epoch: 5| Step: 7
Training loss: 2.689087390899658
Validation loss: 2.3396600651484665

Epoch: 5| Step: 8
Training loss: 2.512579917907715
Validation loss: 2.3420209525733866

Epoch: 5| Step: 9
Training loss: 2.272325038909912
Validation loss: 2.341625207213945

Epoch: 5| Step: 10
Training loss: 3.27336049079895
Validation loss: 2.3408829140406784

Epoch: 69| Step: 0
Training loss: 2.289046049118042
Validation loss: 2.3417379753563994

Epoch: 5| Step: 1
Training loss: 2.864558458328247
Validation loss: 2.3423113028208413

Epoch: 5| Step: 2
Training loss: 2.3624799251556396
Validation loss: 2.3472115788408505

Epoch: 5| Step: 3
Training loss: 2.87015700340271
Validation loss: 2.3510470621047483

Epoch: 5| Step: 4
Training loss: 3.1274685859680176
Validation loss: 2.3550001241827525

Epoch: 5| Step: 5
Training loss: 2.5329129695892334
Validation loss: 2.3758959078019664

Epoch: 5| Step: 6
Training loss: 2.1494956016540527
Validation loss: 2.381977476099486

Epoch: 5| Step: 7
Training loss: 2.8724582195281982
Validation loss: 2.4074565287559264

Epoch: 5| Step: 8
Training loss: 2.429030656814575
Validation loss: 2.405782017656552

Epoch: 5| Step: 9
Training loss: 3.290417432785034
Validation loss: 2.376336254099364

Epoch: 5| Step: 10
Training loss: 1.6425033807754517
Validation loss: 2.362063575816411

Epoch: 70| Step: 0
Training loss: 2.100102186203003
Validation loss: 2.356364691129295

Epoch: 5| Step: 1
Training loss: 2.2445931434631348
Validation loss: 2.360749654872443

Epoch: 5| Step: 2
Training loss: 2.907026767730713
Validation loss: 2.3657812533840055

Epoch: 5| Step: 3
Training loss: 2.8015224933624268
Validation loss: 2.3664824949797763

Epoch: 5| Step: 4
Training loss: 2.4497900009155273
Validation loss: 2.3588688540202316

Epoch: 5| Step: 5
Training loss: 2.8608193397521973
Validation loss: 2.341191412300192

Epoch: 5| Step: 6
Training loss: 2.5610294342041016
Validation loss: 2.3281249666726715

Epoch: 5| Step: 7
Training loss: 2.423901081085205
Validation loss: 2.32562768074774

Epoch: 5| Step: 8
Training loss: 2.2241179943084717
Validation loss: 2.326659558921732

Epoch: 5| Step: 9
Training loss: 3.0797979831695557
Validation loss: 2.325809201886577

Epoch: 5| Step: 10
Training loss: 2.9457643032073975
Validation loss: 2.3235762324384464

Epoch: 71| Step: 0
Training loss: 2.150994300842285
Validation loss: 2.3195970930078977

Epoch: 5| Step: 1
Training loss: 2.210230588912964
Validation loss: 2.3213577603781097

Epoch: 5| Step: 2
Training loss: 2.5927977561950684
Validation loss: 2.322179663565851

Epoch: 5| Step: 3
Training loss: 2.5774378776550293
Validation loss: 2.3238178606956237

Epoch: 5| Step: 4
Training loss: 2.25911283493042
Validation loss: 2.329712621627315

Epoch: 5| Step: 5
Training loss: 2.0131373405456543
Validation loss: 2.338497179810719

Epoch: 5| Step: 6
Training loss: 2.7413318157196045
Validation loss: 2.372836269358153

Epoch: 5| Step: 7
Training loss: 2.6870808601379395
Validation loss: 2.384722932692497

Epoch: 5| Step: 8
Training loss: 3.9858810901641846
Validation loss: 2.3889456564380276

Epoch: 5| Step: 9
Training loss: 2.7922282218933105
Validation loss: 2.3832450118116153

Epoch: 5| Step: 10
Training loss: 2.554912567138672
Validation loss: 2.3474142500149306

Epoch: 72| Step: 0
Training loss: 2.6542606353759766
Validation loss: 2.3232886457955964

Epoch: 5| Step: 1
Training loss: 2.3144829273223877
Validation loss: 2.3160975594674387

Epoch: 5| Step: 2
Training loss: 2.5787580013275146
Validation loss: 2.3203310505036385

Epoch: 5| Step: 3
Training loss: 1.8103090524673462
Validation loss: 2.3362387008564447

Epoch: 5| Step: 4
Training loss: 2.9113640785217285
Validation loss: 2.330452411405502

Epoch: 5| Step: 5
Training loss: 2.884247303009033
Validation loss: 2.3370468975395284

Epoch: 5| Step: 6
Training loss: 2.7223668098449707
Validation loss: 2.368234829236102

Epoch: 5| Step: 7
Training loss: 2.6184916496276855
Validation loss: 2.4010836129547446

Epoch: 5| Step: 8
Training loss: 2.2185795307159424
Validation loss: 2.388121804883403

Epoch: 5| Step: 9
Training loss: 2.78538179397583
Validation loss: 2.376736076929236

Epoch: 5| Step: 10
Training loss: 3.1579830646514893
Validation loss: 2.380747320831463

Epoch: 73| Step: 0
Training loss: 3.3026599884033203
Validation loss: 2.3739021721706597

Epoch: 5| Step: 1
Training loss: 2.654651165008545
Validation loss: 2.3774341716561267

Epoch: 5| Step: 2
Training loss: 2.8798351287841797
Validation loss: 2.3910737165840725

Epoch: 5| Step: 3
Training loss: 2.3785839080810547
Validation loss: 2.4019531793491815

Epoch: 5| Step: 4
Training loss: 1.3934575319290161
Validation loss: 2.425228741861159

Epoch: 5| Step: 5
Training loss: 2.9639267921447754
Validation loss: 2.4400640764544086

Epoch: 5| Step: 6
Training loss: 1.8719689846038818
Validation loss: 2.437941410208261

Epoch: 5| Step: 7
Training loss: 3.625230073928833
Validation loss: 2.4076865385937434

Epoch: 5| Step: 8
Training loss: 2.746540069580078
Validation loss: 2.365823556018132

Epoch: 5| Step: 9
Training loss: 2.4461541175842285
Validation loss: 2.3401505536930536

Epoch: 5| Step: 10
Training loss: 2.5017995834350586
Validation loss: 2.362363438452444

Epoch: 74| Step: 0
Training loss: 2.787355899810791
Validation loss: 2.3970756556398127

Epoch: 5| Step: 1
Training loss: 2.50419282913208
Validation loss: 2.45578271086498

Epoch: 5| Step: 2
Training loss: 2.34985613822937
Validation loss: 2.4843268599561465

Epoch: 5| Step: 3
Training loss: 3.0388801097869873
Validation loss: 2.4726337591807046

Epoch: 5| Step: 4
Training loss: 2.765826463699341
Validation loss: 2.4635532030495266

Epoch: 5| Step: 5
Training loss: 2.8413257598876953
Validation loss: 2.4617130346195673

Epoch: 5| Step: 6
Training loss: 2.8396854400634766
Validation loss: 2.4426533534962642

Epoch: 5| Step: 7
Training loss: 3.0205795764923096
Validation loss: 2.433422306532501

Epoch: 5| Step: 8
Training loss: 2.1628456115722656
Validation loss: 2.408774216969808

Epoch: 5| Step: 9
Training loss: 2.2496731281280518
Validation loss: 2.402509812385805

Epoch: 5| Step: 10
Training loss: 2.7634713649749756
Validation loss: 2.4205667716200634

Epoch: 75| Step: 0
Training loss: 2.792259454727173
Validation loss: 2.414005890969307

Epoch: 5| Step: 1
Training loss: 2.266993761062622
Validation loss: 2.4263223217379664

Epoch: 5| Step: 2
Training loss: 2.9686872959136963
Validation loss: 2.4183211377871934

Epoch: 5| Step: 3
Training loss: 3.248147487640381
Validation loss: 2.4241842198115524

Epoch: 5| Step: 4
Training loss: 3.1356961727142334
Validation loss: 2.417673682653776

Epoch: 5| Step: 5
Training loss: 2.1882243156433105
Validation loss: 2.4090470267880346

Epoch: 5| Step: 6
Training loss: 2.8068604469299316
Validation loss: 2.4038120136466077

Epoch: 5| Step: 7
Training loss: 2.6683905124664307
Validation loss: 2.403518476793843

Epoch: 5| Step: 8
Training loss: 2.647653102874756
Validation loss: 2.396881580352783

Epoch: 5| Step: 9
Training loss: 2.218646764755249
Validation loss: 2.4055480790394608

Epoch: 5| Step: 10
Training loss: 1.8369089365005493
Validation loss: 2.4128844071460027

Epoch: 76| Step: 0
Training loss: 3.3688151836395264
Validation loss: 2.4224654423293246

Epoch: 5| Step: 1
Training loss: 3.188594102859497
Validation loss: 2.436426398574665

Epoch: 5| Step: 2
Training loss: 2.4496982097625732
Validation loss: 2.445749600728353

Epoch: 5| Step: 3
Training loss: 1.8795363903045654
Validation loss: 2.4496075901933896

Epoch: 5| Step: 4
Training loss: 2.828529119491577
Validation loss: 2.438864569510183

Epoch: 5| Step: 5
Training loss: 2.539524555206299
Validation loss: 2.4408301948219218

Epoch: 5| Step: 6
Training loss: 3.0059638023376465
Validation loss: 2.43799154989181

Epoch: 5| Step: 7
Training loss: 2.941312313079834
Validation loss: 2.4393892800936134

Epoch: 5| Step: 8
Training loss: 2.2222962379455566
Validation loss: 2.435202189671096

Epoch: 5| Step: 9
Training loss: 2.2664616107940674
Validation loss: 2.428284009297689

Epoch: 5| Step: 10
Training loss: 2.531916856765747
Validation loss: 2.4209307803902576

Epoch: 77| Step: 0
Training loss: 3.0677731037139893
Validation loss: 2.424756103946317

Epoch: 5| Step: 1
Training loss: 2.41524076461792
Validation loss: 2.4165824100535405

Epoch: 5| Step: 2
Training loss: 2.396118640899658
Validation loss: 2.4177844883293234

Epoch: 5| Step: 3
Training loss: 2.802053451538086
Validation loss: 2.4207540737685336

Epoch: 5| Step: 4
Training loss: 2.8600196838378906
Validation loss: 2.4166392023845384

Epoch: 5| Step: 5
Training loss: 2.4945385456085205
Validation loss: 2.41297080439906

Epoch: 5| Step: 6
Training loss: 2.808893918991089
Validation loss: 2.41028747763685

Epoch: 5| Step: 7
Training loss: 2.2382967472076416
Validation loss: 2.4099201925339235

Epoch: 5| Step: 8
Training loss: 2.370680332183838
Validation loss: 2.408953043722337

Epoch: 5| Step: 9
Training loss: 3.2266716957092285
Validation loss: 2.412275847568307

Epoch: 5| Step: 10
Training loss: 2.255450963973999
Validation loss: 2.4119053963691957

Epoch: 78| Step: 0
Training loss: 2.12414813041687
Validation loss: 2.415154534001504

Epoch: 5| Step: 1
Training loss: 2.838296413421631
Validation loss: 2.4193746300153833

Epoch: 5| Step: 2
Training loss: 3.2576231956481934
Validation loss: 2.4230849563434558

Epoch: 5| Step: 3
Training loss: 2.4531431198120117
Validation loss: 2.418303815267419

Epoch: 5| Step: 4
Training loss: 2.907299757003784
Validation loss: 2.4150819009350193

Epoch: 5| Step: 5
Training loss: 2.348761796951294
Validation loss: 2.418621547760502

Epoch: 5| Step: 6
Training loss: 1.780310034751892
Validation loss: 2.4271473192399546

Epoch: 5| Step: 7
Training loss: 2.902247905731201
Validation loss: 2.444397582802721

Epoch: 5| Step: 8
Training loss: 2.897176742553711
Validation loss: 2.439924127312117

Epoch: 5| Step: 9
Training loss: 2.53454852104187
Validation loss: 2.4301396390443206

Epoch: 5| Step: 10
Training loss: 2.8601036071777344
Validation loss: 2.415164896236953

Epoch: 79| Step: 0
Training loss: 2.095085382461548
Validation loss: 2.402132631630026

Epoch: 5| Step: 1
Training loss: 2.411123514175415
Validation loss: 2.3959680680305726

Epoch: 5| Step: 2
Training loss: 2.4166455268859863
Validation loss: 2.380274147115728

Epoch: 5| Step: 3
Training loss: 3.199143886566162
Validation loss: 2.38340243985576

Epoch: 5| Step: 4
Training loss: 2.9917333126068115
Validation loss: 2.3822189966837564

Epoch: 5| Step: 5
Training loss: 2.4262619018554688
Validation loss: 2.375471294567149

Epoch: 5| Step: 6
Training loss: 3.4359183311462402
Validation loss: 2.374877893796531

Epoch: 5| Step: 7
Training loss: 2.906461715698242
Validation loss: 2.37472806438323

Epoch: 5| Step: 8
Training loss: 2.6972062587738037
Validation loss: 2.3744497376103557

Epoch: 5| Step: 9
Training loss: 2.2695775032043457
Validation loss: 2.3716786189745833

Epoch: 5| Step: 10
Training loss: 1.9596689939498901
Validation loss: 2.370910208712342

Epoch: 80| Step: 0
Training loss: 2.8857197761535645
Validation loss: 2.3696994089311167

Epoch: 5| Step: 1
Training loss: 2.2156691551208496
Validation loss: 2.3707223476902133

Epoch: 5| Step: 2
Training loss: 3.0817291736602783
Validation loss: 2.3753351319220757

Epoch: 5| Step: 3
Training loss: 2.3154897689819336
Validation loss: 2.3789071600924254

Epoch: 5| Step: 4
Training loss: 2.43967866897583
Validation loss: 2.375312732112023

Epoch: 5| Step: 5
Training loss: 2.9699788093566895
Validation loss: 2.3810975756696475

Epoch: 5| Step: 6
Training loss: 2.355929136276245
Validation loss: 2.389156328734531

Epoch: 5| Step: 7
Training loss: 2.2467880249023438
Validation loss: 2.394984568319013

Epoch: 5| Step: 8
Training loss: 2.710171937942505
Validation loss: 2.4010584841492357

Epoch: 5| Step: 9
Training loss: 2.9023146629333496
Validation loss: 2.4176244504990114

Epoch: 5| Step: 10
Training loss: 2.720442295074463
Validation loss: 2.4127026245158207

Epoch: 81| Step: 0
Training loss: 3.0996696949005127
Validation loss: 2.416065855692792

Epoch: 5| Step: 1
Training loss: 2.6011128425598145
Validation loss: 2.4143449055251254

Epoch: 5| Step: 2
Training loss: 1.783477783203125
Validation loss: 2.4295625071371756

Epoch: 5| Step: 3
Training loss: 2.4823741912841797
Validation loss: 2.4426716245630735

Epoch: 5| Step: 4
Training loss: 2.524209499359131
Validation loss: 2.470488072723471

Epoch: 5| Step: 5
Training loss: 2.6363525390625
Validation loss: 2.4446554722324496

Epoch: 5| Step: 6
Training loss: 2.3938987255096436
Validation loss: 2.373567863177228

Epoch: 5| Step: 7
Training loss: 2.9064831733703613
Validation loss: 2.3307843746677523

Epoch: 5| Step: 8
Training loss: 3.4398226737976074
Validation loss: 2.2961949353576987

Epoch: 5| Step: 9
Training loss: 2.09669828414917
Validation loss: 2.2948440877340173

Epoch: 5| Step: 10
Training loss: 2.646470546722412
Validation loss: 2.360997889631538

Epoch: 82| Step: 0
Training loss: 2.313199520111084
Validation loss: 2.354919156720561

Epoch: 5| Step: 1
Training loss: 2.0085508823394775
Validation loss: 2.3369638355829383

Epoch: 5| Step: 2
Training loss: 2.224010944366455
Validation loss: 2.319348824921475

Epoch: 5| Step: 3
Training loss: 2.8318381309509277
Validation loss: 2.312965544321204

Epoch: 5| Step: 4
Training loss: 2.8524186611175537
Validation loss: 2.3249272197805424

Epoch: 5| Step: 5
Training loss: 2.8326594829559326
Validation loss: 2.3273014714640956

Epoch: 5| Step: 6
Training loss: 2.717650890350342
Validation loss: 2.3211548841127785

Epoch: 5| Step: 7
Training loss: 3.232653856277466
Validation loss: 2.297948166888247

Epoch: 5| Step: 8
Training loss: 2.109339952468872
Validation loss: 2.279573840479697

Epoch: 5| Step: 9
Training loss: 2.292954683303833
Validation loss: 2.270275656894971

Epoch: 5| Step: 10
Training loss: 2.9019196033477783
Validation loss: 2.2624915517786497

Epoch: 83| Step: 0
Training loss: 2.8542003631591797
Validation loss: 2.262072117097916

Epoch: 5| Step: 1
Training loss: 2.317538022994995
Validation loss: 2.2664713577557634

Epoch: 5| Step: 2
Training loss: 2.1140620708465576
Validation loss: 2.262094384880476

Epoch: 5| Step: 3
Training loss: 2.569277286529541
Validation loss: 2.2594954147133777

Epoch: 5| Step: 4
Training loss: 2.1773288249969482
Validation loss: 2.272400116407743

Epoch: 5| Step: 5
Training loss: 2.3068861961364746
Validation loss: 2.286568849317489

Epoch: 5| Step: 6
Training loss: 2.6348118782043457
Validation loss: 2.2948847432290354

Epoch: 5| Step: 7
Training loss: 2.8285934925079346
Validation loss: 2.2867352065219673

Epoch: 5| Step: 8
Training loss: 2.8498494625091553
Validation loss: 2.278474365511248

Epoch: 5| Step: 9
Training loss: 2.4980156421661377
Validation loss: 2.2869700154950543

Epoch: 5| Step: 10
Training loss: 3.0450408458709717
Validation loss: 2.271241529013521

Epoch: 84| Step: 0
Training loss: 1.607086420059204
Validation loss: 2.2724367598051667

Epoch: 5| Step: 1
Training loss: 2.395775556564331
Validation loss: 2.2719701233730523

Epoch: 5| Step: 2
Training loss: 2.9365341663360596
Validation loss: 2.2760808006409676

Epoch: 5| Step: 3
Training loss: 2.267587661743164
Validation loss: 2.2814393684428227

Epoch: 5| Step: 4
Training loss: 2.698676347732544
Validation loss: 2.298827587917287

Epoch: 5| Step: 5
Training loss: 2.20088267326355
Validation loss: 2.295166092534219

Epoch: 5| Step: 6
Training loss: 3.1951375007629395
Validation loss: 2.298162314199632

Epoch: 5| Step: 7
Training loss: 2.0732688903808594
Validation loss: 2.3090008125510266

Epoch: 5| Step: 8
Training loss: 2.9563703536987305
Validation loss: 2.3130888451812086

Epoch: 5| Step: 9
Training loss: 2.65995192527771
Validation loss: 2.3081432978312173

Epoch: 5| Step: 10
Training loss: 3.001478672027588
Validation loss: 2.2727534130055416

Epoch: 85| Step: 0
Training loss: 2.5177409648895264
Validation loss: 2.253976050243583

Epoch: 5| Step: 1
Training loss: 2.09238862991333
Validation loss: 2.249594029559884

Epoch: 5| Step: 2
Training loss: 2.4749250411987305
Validation loss: 2.248856641912973

Epoch: 5| Step: 3
Training loss: 2.3764302730560303
Validation loss: 2.256478553177208

Epoch: 5| Step: 4
Training loss: 2.639902353286743
Validation loss: 2.2604668063502156

Epoch: 5| Step: 5
Training loss: 2.803004741668701
Validation loss: 2.258468833020938

Epoch: 5| Step: 6
Training loss: 2.8933866024017334
Validation loss: 2.265839768994239

Epoch: 5| Step: 7
Training loss: 2.5439090728759766
Validation loss: 2.2588082475046956

Epoch: 5| Step: 8
Training loss: 2.2376327514648438
Validation loss: 2.2528494160662413

Epoch: 5| Step: 9
Training loss: 2.937136173248291
Validation loss: 2.2467502291484545

Epoch: 5| Step: 10
Training loss: 2.6391468048095703
Validation loss: 2.2521571215762886

Epoch: 86| Step: 0
Training loss: 3.0845048427581787
Validation loss: 2.2660871116063928

Epoch: 5| Step: 1
Training loss: 3.2404239177703857
Validation loss: 2.2656957103360083

Epoch: 5| Step: 2
Training loss: 2.455026149749756
Validation loss: 2.2759433484846547

Epoch: 5| Step: 3
Training loss: 2.4871268272399902
Validation loss: 2.286496990470476

Epoch: 5| Step: 4
Training loss: 2.7939114570617676
Validation loss: 2.2759148215734832

Epoch: 5| Step: 5
Training loss: 2.483596086502075
Validation loss: 2.2751294195011096

Epoch: 5| Step: 6
Training loss: 2.2095251083374023
Validation loss: 2.2589266043837353

Epoch: 5| Step: 7
Training loss: 2.4537341594696045
Validation loss: 2.2705001536236016

Epoch: 5| Step: 8
Training loss: 1.9377357959747314
Validation loss: 2.2816779690404094

Epoch: 5| Step: 9
Training loss: 2.2478690147399902
Validation loss: 2.2903616018192743

Epoch: 5| Step: 10
Training loss: 2.448707103729248
Validation loss: 2.28294216304697

Epoch: 87| Step: 0
Training loss: 2.7672882080078125
Validation loss: 2.271937785610076

Epoch: 5| Step: 1
Training loss: 2.8514621257781982
Validation loss: 2.250099746129846

Epoch: 5| Step: 2
Training loss: 2.248687982559204
Validation loss: 2.244074793272121

Epoch: 5| Step: 3
Training loss: 2.0284223556518555
Validation loss: 2.238454664907148

Epoch: 5| Step: 4
Training loss: 2.508427381515503
Validation loss: 2.239193426665439

Epoch: 5| Step: 5
Training loss: 2.430433750152588
Validation loss: 2.2343510197054957

Epoch: 5| Step: 6
Training loss: 2.4854278564453125
Validation loss: 2.2359748860841155

Epoch: 5| Step: 7
Training loss: 2.5534842014312744
Validation loss: 2.2361161260194677

Epoch: 5| Step: 8
Training loss: 2.7415735721588135
Validation loss: 2.239333716771936

Epoch: 5| Step: 9
Training loss: 2.5384185314178467
Validation loss: 2.238597737845554

Epoch: 5| Step: 10
Training loss: 2.7072744369506836
Validation loss: 2.243974231904553

Epoch: 88| Step: 0
Training loss: 2.4925777912139893
Validation loss: 2.2419549316488285

Epoch: 5| Step: 1
Training loss: 2.4133944511413574
Validation loss: 2.2626775003248647

Epoch: 5| Step: 2
Training loss: 3.006110668182373
Validation loss: 2.275828980630444

Epoch: 5| Step: 3
Training loss: 2.448357582092285
Validation loss: 2.3004153518266577

Epoch: 5| Step: 4
Training loss: 2.3416852951049805
Validation loss: 2.3115858416403494

Epoch: 5| Step: 5
Training loss: 2.65038800239563
Validation loss: 2.3364310341496624

Epoch: 5| Step: 6
Training loss: 2.900111198425293
Validation loss: 2.298183274525468

Epoch: 5| Step: 7
Training loss: 2.155226230621338
Validation loss: 2.259480817343599

Epoch: 5| Step: 8
Training loss: 2.511531114578247
Validation loss: 2.245501679758872

Epoch: 5| Step: 9
Training loss: 2.262899398803711
Validation loss: 2.2540209754820792

Epoch: 5| Step: 10
Training loss: 2.6746160984039307
Validation loss: 2.249506142831618

Epoch: 89| Step: 0
Training loss: 2.1370832920074463
Validation loss: 2.250647493588027

Epoch: 5| Step: 1
Training loss: 2.4145398139953613
Validation loss: 2.2469580506765716

Epoch: 5| Step: 2
Training loss: 2.98593807220459
Validation loss: 2.240712119686988

Epoch: 5| Step: 3
Training loss: 2.7835581302642822
Validation loss: 2.235700261208319

Epoch: 5| Step: 4
Training loss: 2.2143750190734863
Validation loss: 2.236587833332759

Epoch: 5| Step: 5
Training loss: 2.5776467323303223
Validation loss: 2.241086185619395

Epoch: 5| Step: 6
Training loss: 3.140087127685547
Validation loss: 2.2539010637549945

Epoch: 5| Step: 7
Training loss: 2.5758979320526123
Validation loss: 2.260983774738927

Epoch: 5| Step: 8
Training loss: 2.4991981983184814
Validation loss: 2.27271205635481

Epoch: 5| Step: 9
Training loss: 1.9639942646026611
Validation loss: 2.2801337242126465

Epoch: 5| Step: 10
Training loss: 2.528027057647705
Validation loss: 2.27935815242029

Epoch: 90| Step: 0
Training loss: 2.843212127685547
Validation loss: 2.278834301938293

Epoch: 5| Step: 1
Training loss: 2.297602653503418
Validation loss: 2.2774559400414907

Epoch: 5| Step: 2
Training loss: 2.428504705429077
Validation loss: 2.2639043792601554

Epoch: 5| Step: 3
Training loss: 2.288270950317383
Validation loss: 2.27845827738444

Epoch: 5| Step: 4
Training loss: 2.643179416656494
Validation loss: 2.2729279200236

Epoch: 5| Step: 5
Training loss: 2.59382700920105
Validation loss: 2.2798392388128463

Epoch: 5| Step: 6
Training loss: 2.1803066730499268
Validation loss: 2.2760743864120974

Epoch: 5| Step: 7
Training loss: 3.2487456798553467
Validation loss: 2.273459449891121

Epoch: 5| Step: 8
Training loss: 2.0691287517547607
Validation loss: 2.2588187520222

Epoch: 5| Step: 9
Training loss: 2.9873061180114746
Validation loss: 2.267762645598381

Epoch: 5| Step: 10
Training loss: 1.8931472301483154
Validation loss: 2.273725655771071

Epoch: 91| Step: 0
Training loss: 2.545438289642334
Validation loss: 2.2614730250450874

Epoch: 5| Step: 1
Training loss: 2.0291953086853027
Validation loss: 2.2632573496910835

Epoch: 5| Step: 2
Training loss: 2.8863863945007324
Validation loss: 2.2597440237640054

Epoch: 5| Step: 3
Training loss: 2.7175896167755127
Validation loss: 2.2602079529916086

Epoch: 5| Step: 4
Training loss: 2.0098958015441895
Validation loss: 2.2509974920621483

Epoch: 5| Step: 5
Training loss: 2.619264841079712
Validation loss: 2.2437696123635895

Epoch: 5| Step: 6
Training loss: 2.357347011566162
Validation loss: 2.237830629912756

Epoch: 5| Step: 7
Training loss: 2.221397876739502
Validation loss: 2.2328320882653676

Epoch: 5| Step: 8
Training loss: 2.6213135719299316
Validation loss: 2.2365330085959485

Epoch: 5| Step: 9
Training loss: 3.011989116668701
Validation loss: 2.237359539155037

Epoch: 5| Step: 10
Training loss: 2.536832332611084
Validation loss: 2.2377773856603973

Epoch: 92| Step: 0
Training loss: 2.696807384490967
Validation loss: 2.2356222162964525

Epoch: 5| Step: 1
Training loss: 2.61322283744812
Validation loss: 2.2474595962032193

Epoch: 5| Step: 2
Training loss: 1.6392933130264282
Validation loss: 2.253796108307377

Epoch: 5| Step: 3
Training loss: 1.9695956707000732
Validation loss: 2.262088780762047

Epoch: 5| Step: 4
Training loss: 3.236785888671875
Validation loss: 2.2617913984483287

Epoch: 5| Step: 5
Training loss: 2.9749975204467773
Validation loss: 2.254548616306756

Epoch: 5| Step: 6
Training loss: 2.676558256149292
Validation loss: 2.2390026482202674

Epoch: 5| Step: 7
Training loss: 2.60139799118042
Validation loss: 2.2530332278179865

Epoch: 5| Step: 8
Training loss: 2.427107334136963
Validation loss: 2.258722292479648

Epoch: 5| Step: 9
Training loss: 2.479614734649658
Validation loss: 2.251219541795792

Epoch: 5| Step: 10
Training loss: 2.003848075866699
Validation loss: 2.2606674137935845

Epoch: 93| Step: 0
Training loss: 1.9104950428009033
Validation loss: 2.2496495169977986

Epoch: 5| Step: 1
Training loss: 2.489905595779419
Validation loss: 2.241354728257784

Epoch: 5| Step: 2
Training loss: 2.2992312908172607
Validation loss: 2.232806873577897

Epoch: 5| Step: 3
Training loss: 2.6462390422821045
Validation loss: 2.2185490977379585

Epoch: 5| Step: 4
Training loss: 2.4137015342712402
Validation loss: 2.2119322592212307

Epoch: 5| Step: 5
Training loss: 2.5120151042938232
Validation loss: 2.2154522685594458

Epoch: 5| Step: 6
Training loss: 2.678443431854248
Validation loss: 2.2176148814539753

Epoch: 5| Step: 7
Training loss: 3.0800774097442627
Validation loss: 2.2068641544670187

Epoch: 5| Step: 8
Training loss: 2.27773118019104
Validation loss: 2.21483043701418

Epoch: 5| Step: 9
Training loss: 2.175389528274536
Validation loss: 2.219959694852111

Epoch: 5| Step: 10
Training loss: 3.042365074157715
Validation loss: 2.218569644035832

Epoch: 94| Step: 0
Training loss: 2.239572286605835
Validation loss: 2.234711766242981

Epoch: 5| Step: 1
Training loss: 2.6473805904388428
Validation loss: 2.2548876141989105

Epoch: 5| Step: 2
Training loss: 2.392204999923706
Validation loss: 2.25525160502362

Epoch: 5| Step: 3
Training loss: 2.2239623069763184
Validation loss: 2.2540392260397635

Epoch: 5| Step: 4
Training loss: 2.108372211456299
Validation loss: 2.2473428736450853

Epoch: 5| Step: 5
Training loss: 2.447080373764038
Validation loss: 2.244695932634415

Epoch: 5| Step: 6
Training loss: 3.1256871223449707
Validation loss: 2.2412188335131575

Epoch: 5| Step: 7
Training loss: 2.834754467010498
Validation loss: 2.230425379609549

Epoch: 5| Step: 8
Training loss: 3.08801531791687
Validation loss: 2.2352191427702546

Epoch: 5| Step: 9
Training loss: 1.8958797454833984
Validation loss: 2.234553478097403

Epoch: 5| Step: 10
Training loss: 2.329244375228882
Validation loss: 2.2485113092648086

Epoch: 95| Step: 0
Training loss: 1.948830246925354
Validation loss: 2.248166863636304

Epoch: 5| Step: 1
Training loss: 2.3333077430725098
Validation loss: 2.250917156537374

Epoch: 5| Step: 2
Training loss: 2.267284870147705
Validation loss: 2.2597273242089058

Epoch: 5| Step: 3
Training loss: 2.843985080718994
Validation loss: 2.254620147007768

Epoch: 5| Step: 4
Training loss: 2.6276354789733887
Validation loss: 2.2565059713138047

Epoch: 5| Step: 5
Training loss: 2.2871346473693848
Validation loss: 2.2483249069542013

Epoch: 5| Step: 6
Training loss: 2.820047378540039
Validation loss: 2.252686539003926

Epoch: 5| Step: 7
Training loss: 2.824481248855591
Validation loss: 2.235606731907014

Epoch: 5| Step: 8
Training loss: 2.1434667110443115
Validation loss: 2.2327431196807535

Epoch: 5| Step: 9
Training loss: 2.9625442028045654
Validation loss: 2.22768279685769

Epoch: 5| Step: 10
Training loss: 2.0647411346435547
Validation loss: 2.210023690295476

Epoch: 96| Step: 0
Training loss: 2.510314464569092
Validation loss: 2.211602576317326

Epoch: 5| Step: 1
Training loss: 2.593787908554077
Validation loss: 2.2054255136879544

Epoch: 5| Step: 2
Training loss: 2.01942777633667
Validation loss: 2.2127854516429286

Epoch: 5| Step: 3
Training loss: 2.466425895690918
Validation loss: 2.201764806624382

Epoch: 5| Step: 4
Training loss: 2.5739245414733887
Validation loss: 2.203016141409515

Epoch: 5| Step: 5
Training loss: 3.03902006149292
Validation loss: 2.2135473130851664

Epoch: 5| Step: 6
Training loss: 2.4738898277282715
Validation loss: 2.2292636068918372

Epoch: 5| Step: 7
Training loss: 2.1838667392730713
Validation loss: 2.2695309808177333

Epoch: 5| Step: 8
Training loss: 2.8288395404815674
Validation loss: 2.2686611836956394

Epoch: 5| Step: 9
Training loss: 2.4193954467773438
Validation loss: 2.254879015748219

Epoch: 5| Step: 10
Training loss: 2.1227457523345947
Validation loss: 2.242537445919488

Epoch: 97| Step: 0
Training loss: 2.59621000289917
Validation loss: 2.226345282728954

Epoch: 5| Step: 1
Training loss: 2.7046213150024414
Validation loss: 2.2223363307214554

Epoch: 5| Step: 2
Training loss: 2.402148723602295
Validation loss: 2.226995865503947

Epoch: 5| Step: 3
Training loss: 2.3231570720672607
Validation loss: 2.2257253354595554

Epoch: 5| Step: 4
Training loss: 2.6897997856140137
Validation loss: 2.2290446399360575

Epoch: 5| Step: 5
Training loss: 2.494105815887451
Validation loss: 2.219493417329686

Epoch: 5| Step: 6
Training loss: 2.937418222427368
Validation loss: 2.2184795025856263

Epoch: 5| Step: 7
Training loss: 2.36016583442688
Validation loss: 2.2210439328224427

Epoch: 5| Step: 8
Training loss: 2.0639655590057373
Validation loss: 2.205991252776115

Epoch: 5| Step: 9
Training loss: 2.3102657794952393
Validation loss: 2.2017214785340014

Epoch: 5| Step: 10
Training loss: 2.168012857437134
Validation loss: 2.196950989384805

Epoch: 98| Step: 0
Training loss: 3.0358242988586426
Validation loss: 2.1941946834646244

Epoch: 5| Step: 1
Training loss: 2.9495110511779785
Validation loss: 2.2002681173304075

Epoch: 5| Step: 2
Training loss: 1.900552749633789
Validation loss: 2.1889204184214273

Epoch: 5| Step: 3
Training loss: 2.124872922897339
Validation loss: 2.1931442406869706

Epoch: 5| Step: 4
Training loss: 2.694533348083496
Validation loss: 2.197865706618114

Epoch: 5| Step: 5
Training loss: 2.9283084869384766
Validation loss: 2.198985812484577

Epoch: 5| Step: 6
Training loss: 2.685180187225342
Validation loss: 2.200737217421173

Epoch: 5| Step: 7
Training loss: 1.7855993509292603
Validation loss: 2.2034479520654164

Epoch: 5| Step: 8
Training loss: 2.537548780441284
Validation loss: 2.2179278225027104

Epoch: 5| Step: 9
Training loss: 2.367112398147583
Validation loss: 2.2584193547566733

Epoch: 5| Step: 10
Training loss: 1.9659782648086548
Validation loss: 2.2754814163331063

Epoch: 99| Step: 0
Training loss: 2.31845760345459
Validation loss: 2.295023969424668

Epoch: 5| Step: 1
Training loss: 2.540194034576416
Validation loss: 2.325214578259376

Epoch: 5| Step: 2
Training loss: 2.277271270751953
Validation loss: 2.3203514801558627

Epoch: 5| Step: 3
Training loss: 2.203735589981079
Validation loss: 2.2813345155408307

Epoch: 5| Step: 4
Training loss: 2.0691983699798584
Validation loss: 2.2396590658413467

Epoch: 5| Step: 5
Training loss: 2.374009609222412
Validation loss: 2.2114182838829617

Epoch: 5| Step: 6
Training loss: 2.7603700160980225
Validation loss: 2.2007929278958227

Epoch: 5| Step: 7
Training loss: 2.897138833999634
Validation loss: 2.2075372536977134

Epoch: 5| Step: 8
Training loss: 2.5044009685516357
Validation loss: 2.208991696757655

Epoch: 5| Step: 9
Training loss: 2.635456085205078
Validation loss: 2.2025937572602303

Epoch: 5| Step: 10
Training loss: 2.638458728790283
Validation loss: 2.2102331576808805

Epoch: 100| Step: 0
Training loss: 2.3080475330352783
Validation loss: 2.2266557216644287

Epoch: 5| Step: 1
Training loss: 2.5130248069763184
Validation loss: 2.214985483436174

Epoch: 5| Step: 2
Training loss: 2.339559316635132
Validation loss: 2.2041622400283813

Epoch: 5| Step: 3
Training loss: 2.181246280670166
Validation loss: 2.199586364530748

Epoch: 5| Step: 4
Training loss: 2.8647756576538086
Validation loss: 2.1986790216097267

Epoch: 5| Step: 5
Training loss: 3.059499740600586
Validation loss: 2.2096072217469573

Epoch: 5| Step: 6
Training loss: 2.595449686050415
Validation loss: 2.2315692158155542

Epoch: 5| Step: 7
Training loss: 1.8969110250473022
Validation loss: 2.2489167618495163

Epoch: 5| Step: 8
Training loss: 2.3869614601135254
Validation loss: 2.249272725915396

Epoch: 5| Step: 9
Training loss: 2.348292350769043
Validation loss: 2.2425566052877777

Epoch: 5| Step: 10
Training loss: 2.79579758644104
Validation loss: 2.2226554578350437

Epoch: 101| Step: 0
Training loss: 2.495054244995117
Validation loss: 2.192256699326218

Epoch: 5| Step: 1
Training loss: 2.372692584991455
Validation loss: 2.190067232296031

Epoch: 5| Step: 2
Training loss: 2.118283748626709
Validation loss: 2.2047431802236908

Epoch: 5| Step: 3
Training loss: 2.416922092437744
Validation loss: 2.2130107700183825

Epoch: 5| Step: 4
Training loss: 2.078612804412842
Validation loss: 2.2209010585661857

Epoch: 5| Step: 5
Training loss: 2.3353190422058105
Validation loss: 2.242538090675108

Epoch: 5| Step: 6
Training loss: 2.224553108215332
Validation loss: 2.2440668639316352

Epoch: 5| Step: 7
Training loss: 3.288515567779541
Validation loss: 2.2348387164454304

Epoch: 5| Step: 8
Training loss: 2.4486923217773438
Validation loss: 2.2369252917587117

Epoch: 5| Step: 9
Training loss: 2.867356061935425
Validation loss: 2.247007376404219

Epoch: 5| Step: 10
Training loss: 2.2971134185791016
Validation loss: 2.226476510365804

Epoch: 102| Step: 0
Training loss: 2.1976568698883057
Validation loss: 2.214144915662786

Epoch: 5| Step: 1
Training loss: 3.0202407836914062
Validation loss: 2.2048683730504846

Epoch: 5| Step: 2
Training loss: 2.19805908203125
Validation loss: 2.217319166788491

Epoch: 5| Step: 3
Training loss: 1.9961395263671875
Validation loss: 2.2324435198178856

Epoch: 5| Step: 4
Training loss: 2.7312417030334473
Validation loss: 2.2562504788880706

Epoch: 5| Step: 5
Training loss: 2.9459803104400635
Validation loss: 2.272131166150493

Epoch: 5| Step: 6
Training loss: 2.1381313800811768
Validation loss: 2.2579925444818314

Epoch: 5| Step: 7
Training loss: 2.229578971862793
Validation loss: 2.2557402836379183

Epoch: 5| Step: 8
Training loss: 2.383390426635742
Validation loss: 2.2201023755535

Epoch: 5| Step: 9
Training loss: 2.485790491104126
Validation loss: 2.2184159473706315

Epoch: 5| Step: 10
Training loss: 2.5103890895843506
Validation loss: 2.2077733880730084

Epoch: 103| Step: 0
Training loss: 1.953871726989746
Validation loss: 2.19600078880146

Epoch: 5| Step: 1
Training loss: 2.3089210987091064
Validation loss: 2.2001758454948344

Epoch: 5| Step: 2
Training loss: 2.5603134632110596
Validation loss: 2.1839136103148102

Epoch: 5| Step: 3
Training loss: 2.54964017868042
Validation loss: 2.1940862158293366

Epoch: 5| Step: 4
Training loss: 2.1130318641662598
Validation loss: 2.197980583354991

Epoch: 5| Step: 5
Training loss: 2.896700382232666
Validation loss: 2.218453102214362

Epoch: 5| Step: 6
Training loss: 2.533632755279541
Validation loss: 2.208586032672595

Epoch: 5| Step: 7
Training loss: 2.626469135284424
Validation loss: 2.2112773746572514

Epoch: 5| Step: 8
Training loss: 2.559894561767578
Validation loss: 2.216584833719397

Epoch: 5| Step: 9
Training loss: 2.382164716720581
Validation loss: 2.2023855614405807

Epoch: 5| Step: 10
Training loss: 2.1873388290405273
Validation loss: 2.1848289376945904

Epoch: 104| Step: 0
Training loss: 2.4600796699523926
Validation loss: 2.1835104444975495

Epoch: 5| Step: 1
Training loss: 2.5290274620056152
Validation loss: 2.175694529728223

Epoch: 5| Step: 2
Training loss: 2.132490634918213
Validation loss: 2.174839227430282

Epoch: 5| Step: 3
Training loss: 2.28696346282959
Validation loss: 2.1760703735454108

Epoch: 5| Step: 4
Training loss: 2.9500997066497803
Validation loss: 2.1935965643134168

Epoch: 5| Step: 5
Training loss: 2.6726183891296387
Validation loss: 2.2174509520171792

Epoch: 5| Step: 6
Training loss: 1.8457014560699463
Validation loss: 2.2162744524658367

Epoch: 5| Step: 7
Training loss: 2.7721471786499023
Validation loss: 2.214490721302648

Epoch: 5| Step: 8
Training loss: 2.2946619987487793
Validation loss: 2.2011801632501746

Epoch: 5| Step: 9
Training loss: 2.3699193000793457
Validation loss: 2.1910427847216205

Epoch: 5| Step: 10
Training loss: 2.486189126968384
Validation loss: 2.1950271155244563

Epoch: 105| Step: 0
Training loss: 2.3429646492004395
Validation loss: 2.1869608189470027

Epoch: 5| Step: 1
Training loss: 1.8706157207489014
Validation loss: 2.201385216046405

Epoch: 5| Step: 2
Training loss: 2.063676595687866
Validation loss: 2.218408420521726

Epoch: 5| Step: 3
Training loss: 1.837424874305725
Validation loss: 2.216613336275983

Epoch: 5| Step: 4
Training loss: 2.8969979286193848
Validation loss: 2.2842645414413942

Epoch: 5| Step: 5
Training loss: 2.6303961277008057
Validation loss: 2.297863555210893

Epoch: 5| Step: 6
Training loss: 2.4128575325012207
Validation loss: 2.3023558739692933

Epoch: 5| Step: 7
Training loss: 2.550121784210205
Validation loss: 2.278904971256051

Epoch: 5| Step: 8
Training loss: 1.8896573781967163
Validation loss: 2.232972019462175

Epoch: 5| Step: 9
Training loss: 2.9008119106292725
Validation loss: 2.206440999943723

Epoch: 5| Step: 10
Training loss: 3.290696144104004
Validation loss: 2.1843761808128765

Epoch: 106| Step: 0
Training loss: 1.9382610321044922
Validation loss: 2.1788940942415627

Epoch: 5| Step: 1
Training loss: 2.021411895751953
Validation loss: 2.159939960766864

Epoch: 5| Step: 2
Training loss: 3.1256446838378906
Validation loss: 2.1709130451243412

Epoch: 5| Step: 3
Training loss: 2.736429214477539
Validation loss: 2.1718372157824937

Epoch: 5| Step: 4
Training loss: 2.8599350452423096
Validation loss: 2.184155043735299

Epoch: 5| Step: 5
Training loss: 1.644357442855835
Validation loss: 2.1880593299865723

Epoch: 5| Step: 6
Training loss: 2.4819202423095703
Validation loss: 2.1801918116948937

Epoch: 5| Step: 7
Training loss: 2.5071401596069336
Validation loss: 2.174935522899833

Epoch: 5| Step: 8
Training loss: 2.466939687728882
Validation loss: 2.161062131645859

Epoch: 5| Step: 9
Training loss: 2.156944751739502
Validation loss: 2.1614519921682214

Epoch: 5| Step: 10
Training loss: 2.687955617904663
Validation loss: 2.1596000066367527

Epoch: 107| Step: 0
Training loss: 2.1729767322540283
Validation loss: 2.1633456612146027

Epoch: 5| Step: 1
Training loss: 2.228494644165039
Validation loss: 2.179339357601699

Epoch: 5| Step: 2
Training loss: 2.6423943042755127
Validation loss: 2.2084013710739794

Epoch: 5| Step: 3
Training loss: 2.498563289642334
Validation loss: 2.2320775396080426

Epoch: 5| Step: 4
Training loss: 2.1743340492248535
Validation loss: 2.2138639547491588

Epoch: 5| Step: 5
Training loss: 2.108732223510742
Validation loss: 2.207058588663737

Epoch: 5| Step: 6
Training loss: 2.7374489307403564
Validation loss: 2.196071993920111

Epoch: 5| Step: 7
Training loss: 2.5533008575439453
Validation loss: 2.218851166386758

Epoch: 5| Step: 8
Training loss: 2.5286571979522705
Validation loss: 2.221663769855294

Epoch: 5| Step: 9
Training loss: 2.6474270820617676
Validation loss: 2.232780618052329

Epoch: 5| Step: 10
Training loss: 2.314897298812866
Validation loss: 2.2370491848197034

Epoch: 108| Step: 0
Training loss: 2.8105528354644775
Validation loss: 2.2485578380605227

Epoch: 5| Step: 1
Training loss: 2.2260122299194336
Validation loss: 2.245212588258969

Epoch: 5| Step: 2
Training loss: 2.603236675262451
Validation loss: 2.2007850267553843

Epoch: 5| Step: 3
Training loss: 3.3740501403808594
Validation loss: 2.183438390813848

Epoch: 5| Step: 4
Training loss: 2.05291485786438
Validation loss: 2.1656901631304013

Epoch: 5| Step: 5
Training loss: 2.2329368591308594
Validation loss: 2.1690776630114486

Epoch: 5| Step: 6
Training loss: 2.359809160232544
Validation loss: 2.184945983271445

Epoch: 5| Step: 7
Training loss: 1.9985233545303345
Validation loss: 2.198350296225599

Epoch: 5| Step: 8
Training loss: 2.444329023361206
Validation loss: 2.1986528519661195

Epoch: 5| Step: 9
Training loss: 2.1747467517852783
Validation loss: 2.2145927131816907

Epoch: 5| Step: 10
Training loss: 2.139094352722168
Validation loss: 2.195814432636384

Epoch: 109| Step: 0
Training loss: 3.0470452308654785
Validation loss: 2.1674416116488877

Epoch: 5| Step: 1
Training loss: 2.4704174995422363
Validation loss: 2.169329345867198

Epoch: 5| Step: 2
Training loss: 2.203960657119751
Validation loss: 2.1606796300539406

Epoch: 5| Step: 3
Training loss: 2.6332545280456543
Validation loss: 2.171361997563352

Epoch: 5| Step: 4
Training loss: 2.5181095600128174
Validation loss: 2.184538636156308

Epoch: 5| Step: 5
Training loss: 2.31693434715271
Validation loss: 2.189926447406892

Epoch: 5| Step: 6
Training loss: 1.9305826425552368
Validation loss: 2.1873769939586682

Epoch: 5| Step: 7
Training loss: 2.6326534748077393
Validation loss: 2.1632119750463836

Epoch: 5| Step: 8
Training loss: 2.0841572284698486
Validation loss: 2.152427460557671

Epoch: 5| Step: 9
Training loss: 2.4121456146240234
Validation loss: 2.144435872313797

Epoch: 5| Step: 10
Training loss: 1.9339377880096436
Validation loss: 2.135888015070269

Epoch: 110| Step: 0
Training loss: 2.5774083137512207
Validation loss: 2.150610617412034

Epoch: 5| Step: 1
Training loss: 2.4202919006347656
Validation loss: 2.1595098408319617

Epoch: 5| Step: 2
Training loss: 2.761751651763916
Validation loss: 2.1641743490772862

Epoch: 5| Step: 3
Training loss: 2.720198392868042
Validation loss: 2.1726365986690728

Epoch: 5| Step: 4
Training loss: 2.0946130752563477
Validation loss: 2.1795503734260477

Epoch: 5| Step: 5
Training loss: 2.7489068508148193
Validation loss: 2.168813979753884

Epoch: 5| Step: 6
Training loss: 1.9356180429458618
Validation loss: 2.1603466926082486

Epoch: 5| Step: 7
Training loss: 2.410322904586792
Validation loss: 2.188023437735855

Epoch: 5| Step: 8
Training loss: 2.3017516136169434
Validation loss: 2.2100766756201304

Epoch: 5| Step: 9
Training loss: 2.108259439468384
Validation loss: 2.238704965960595

Epoch: 5| Step: 10
Training loss: 2.178558111190796
Validation loss: 2.237339342794111

Epoch: 111| Step: 0
Training loss: 2.21844220161438
Validation loss: 2.250135455080258

Epoch: 5| Step: 1
Training loss: 1.9736769199371338
Validation loss: 2.2119463771902104

Epoch: 5| Step: 2
Training loss: 2.249937057495117
Validation loss: 2.186919007250058

Epoch: 5| Step: 3
Training loss: 2.2988507747650146
Validation loss: 2.1599241123404553

Epoch: 5| Step: 4
Training loss: 2.5331923961639404
Validation loss: 2.1426650119084183

Epoch: 5| Step: 5
Training loss: 2.633873462677002
Validation loss: 2.155987487044386

Epoch: 5| Step: 6
Training loss: 2.9909842014312744
Validation loss: 2.202324100719985

Epoch: 5| Step: 7
Training loss: 2.16621470451355
Validation loss: 2.200020008189704

Epoch: 5| Step: 8
Training loss: 2.238825798034668
Validation loss: 2.1720634942413657

Epoch: 5| Step: 9
Training loss: 2.324129581451416
Validation loss: 2.1406886910879486

Epoch: 5| Step: 10
Training loss: 2.752974510192871
Validation loss: 2.1360285384680635

Epoch: 112| Step: 0
Training loss: 2.1306376457214355
Validation loss: 2.1353837315754225

Epoch: 5| Step: 1
Training loss: 2.7809910774230957
Validation loss: 2.156213116902177

Epoch: 5| Step: 2
Training loss: 2.595170497894287
Validation loss: 2.151221126638433

Epoch: 5| Step: 3
Training loss: 2.0517115592956543
Validation loss: 2.1543534494215444

Epoch: 5| Step: 4
Training loss: 1.989267110824585
Validation loss: 2.1589429916874057

Epoch: 5| Step: 5
Training loss: 2.3099594116210938
Validation loss: 2.1699292300849833

Epoch: 5| Step: 6
Training loss: 2.0397911071777344
Validation loss: 2.1827845906698577

Epoch: 5| Step: 7
Training loss: 2.4178824424743652
Validation loss: 2.173727730269073

Epoch: 5| Step: 8
Training loss: 2.604980945587158
Validation loss: 2.1636497794940905

Epoch: 5| Step: 9
Training loss: 2.624082088470459
Validation loss: 2.1559208093150968

Epoch: 5| Step: 10
Training loss: 2.394709587097168
Validation loss: 2.175544833624235

Epoch: 113| Step: 0
Training loss: 2.0875356197357178
Validation loss: 2.279658181692964

Epoch: 5| Step: 1
Training loss: 2.580390453338623
Validation loss: 2.425418274376982

Epoch: 5| Step: 2
Training loss: 2.460984706878662
Validation loss: 2.431258240053731

Epoch: 5| Step: 3
Training loss: 2.4115192890167236
Validation loss: 2.30049471188617

Epoch: 5| Step: 4
Training loss: 2.2343685626983643
Validation loss: 2.1813177447165213

Epoch: 5| Step: 5
Training loss: 2.4184460639953613
Validation loss: 2.1456830770738664

Epoch: 5| Step: 6
Training loss: 2.447701930999756
Validation loss: 2.1297758651036087

Epoch: 5| Step: 7
Training loss: 2.314284324645996
Validation loss: 2.1447078617670203

Epoch: 5| Step: 8
Training loss: 2.8427517414093018
Validation loss: 2.1909617275320072

Epoch: 5| Step: 9
Training loss: 2.550938129425049
Validation loss: 2.2479469096788796

Epoch: 5| Step: 10
Training loss: 2.176997423171997
Validation loss: 2.2662057158767537

Epoch: 114| Step: 0
Training loss: 2.88313627243042
Validation loss: 2.227375856009863

Epoch: 5| Step: 1
Training loss: 2.5530495643615723
Validation loss: 2.133539817666495

Epoch: 5| Step: 2
Training loss: 2.238010883331299
Validation loss: 2.0995393696651665

Epoch: 5| Step: 3
Training loss: 1.9774143695831299
Validation loss: 2.1022019232473066

Epoch: 5| Step: 4
Training loss: 2.6628119945526123
Validation loss: 2.1295354109938427

Epoch: 5| Step: 5
Training loss: 1.8968673944473267
Validation loss: 2.161513772062076

Epoch: 5| Step: 6
Training loss: 1.9423713684082031
Validation loss: 2.172762683642808

Epoch: 5| Step: 7
Training loss: 2.757763385772705
Validation loss: 2.1962639157490065

Epoch: 5| Step: 8
Training loss: 2.1442651748657227
Validation loss: 2.20655148003691

Epoch: 5| Step: 9
Training loss: 3.055361747741699
Validation loss: 2.2092741535555933

Epoch: 5| Step: 10
Training loss: 2.0500221252441406
Validation loss: 2.1730546951293945

Epoch: 115| Step: 0
Training loss: 2.7583377361297607
Validation loss: 2.177470673796951

Epoch: 5| Step: 1
Training loss: 2.0841946601867676
Validation loss: 2.234139152752456

Epoch: 5| Step: 2
Training loss: 2.6418774127960205
Validation loss: 2.2969046497857697

Epoch: 5| Step: 3
Training loss: 2.507636547088623
Validation loss: 2.174235982279624

Epoch: 5| Step: 4
Training loss: 2.0818400382995605
Validation loss: 2.1059567184858423

Epoch: 5| Step: 5
Training loss: 1.3315565586090088
Validation loss: 2.094449461147349

Epoch: 5| Step: 6
Training loss: 2.6634252071380615
Validation loss: 2.0660163779412546

Epoch: 5| Step: 7
Training loss: 2.7323250770568848
Validation loss: 2.080812128641272

Epoch: 5| Step: 8
Training loss: 2.6618635654449463
Validation loss: 2.0802087937631915

Epoch: 5| Step: 9
Training loss: 2.5356497764587402
Validation loss: 2.0917583665540143

Epoch: 5| Step: 10
Training loss: 2.3494160175323486
Validation loss: 2.096426128059305

Epoch: 116| Step: 0
Training loss: 2.915614604949951
Validation loss: 2.09040311075026

Epoch: 5| Step: 1
Training loss: 2.3415961265563965
Validation loss: 2.0864367049227477

Epoch: 5| Step: 2
Training loss: 2.55532169342041
Validation loss: 2.0852864147514425

Epoch: 5| Step: 3
Training loss: 2.340066432952881
Validation loss: 2.0884338245596936

Epoch: 5| Step: 4
Training loss: 1.9937829971313477
Validation loss: 2.1005372411461285

Epoch: 5| Step: 5
Training loss: 2.151268482208252
Validation loss: 2.124181185999224

Epoch: 5| Step: 6
Training loss: 2.4319958686828613
Validation loss: 2.1289434740620274

Epoch: 5| Step: 7
Training loss: 1.9114511013031006
Validation loss: 2.1593177856937533

Epoch: 5| Step: 8
Training loss: 2.3694040775299072
Validation loss: 2.2083017877353135

Epoch: 5| Step: 9
Training loss: 2.806519031524658
Validation loss: 2.2504901104075934

Epoch: 5| Step: 10
Training loss: 2.206179618835449
Validation loss: 2.2294794103150726

Epoch: 117| Step: 0
Training loss: 2.80220365524292
Validation loss: 2.2057428411258164

Epoch: 5| Step: 1
Training loss: 1.889974594116211
Validation loss: 2.1927985837382655

Epoch: 5| Step: 2
Training loss: 2.172874927520752
Validation loss: 2.1725392085249706

Epoch: 5| Step: 3
Training loss: 2.514664888381958
Validation loss: 2.173077001366564

Epoch: 5| Step: 4
Training loss: 2.182408571243286
Validation loss: 2.1432317303073023

Epoch: 5| Step: 5
Training loss: 2.376058340072632
Validation loss: 2.1224799130552556

Epoch: 5| Step: 6
Training loss: 1.633270263671875
Validation loss: 2.0871618088855537

Epoch: 5| Step: 7
Training loss: 2.9199423789978027
Validation loss: 2.0862885341849378

Epoch: 5| Step: 8
Training loss: 2.695645809173584
Validation loss: 2.075926573045792

Epoch: 5| Step: 9
Training loss: 2.071354389190674
Validation loss: 2.0941407270328973

Epoch: 5| Step: 10
Training loss: 2.4886229038238525
Validation loss: 2.1267849886289207

Epoch: 118| Step: 0
Training loss: 2.1040117740631104
Validation loss: 2.167327159194536

Epoch: 5| Step: 1
Training loss: 2.4838287830352783
Validation loss: 2.172545873990623

Epoch: 5| Step: 2
Training loss: 2.325719118118286
Validation loss: 2.2138702113141298

Epoch: 5| Step: 3
Training loss: 2.2227110862731934
Validation loss: 2.2332319110952397

Epoch: 5| Step: 4
Training loss: 2.863941192626953
Validation loss: 2.2510775801956013

Epoch: 5| Step: 5
Training loss: 3.200592041015625
Validation loss: 2.209717330112252

Epoch: 5| Step: 6
Training loss: 2.194972515106201
Validation loss: 2.170864369279595

Epoch: 5| Step: 7
Training loss: 1.5305675268173218
Validation loss: 2.1770672695611113

Epoch: 5| Step: 8
Training loss: 1.8944118022918701
Validation loss: 2.181314549138469

Epoch: 5| Step: 9
Training loss: 2.558692455291748
Validation loss: 2.1847687254669848

Epoch: 5| Step: 10
Training loss: 2.501357316970825
Validation loss: 2.171413085793936

Epoch: 119| Step: 0
Training loss: 1.9461543560028076
Validation loss: 2.1557631236250683

Epoch: 5| Step: 1
Training loss: 2.533663272857666
Validation loss: 2.1352193035105222

Epoch: 5| Step: 2
Training loss: 2.6088411808013916
Validation loss: 2.1321384419677076

Epoch: 5| Step: 3
Training loss: 2.2588438987731934
Validation loss: 2.1463138211158013

Epoch: 5| Step: 4
Training loss: 2.8432109355926514
Validation loss: 2.1399803648712816

Epoch: 5| Step: 5
Training loss: 1.4592989683151245
Validation loss: 2.1732327297169673

Epoch: 5| Step: 6
Training loss: 2.425731658935547
Validation loss: 2.1907050135315105

Epoch: 5| Step: 7
Training loss: 1.9134715795516968
Validation loss: 2.2041703911237818

Epoch: 5| Step: 8
Training loss: 2.2932772636413574
Validation loss: 2.1920213571158786

Epoch: 5| Step: 9
Training loss: 2.9140098094940186
Validation loss: 2.1565779229646087

Epoch: 5| Step: 10
Training loss: 2.070399045944214
Validation loss: 2.141709464852528

Epoch: 120| Step: 0
Training loss: 2.577531099319458
Validation loss: 2.127287692921136

Epoch: 5| Step: 1
Training loss: 2.573901891708374
Validation loss: 2.1238989932562715

Epoch: 5| Step: 2
Training loss: 2.2198140621185303
Validation loss: 2.146136970930202

Epoch: 5| Step: 3
Training loss: 2.644606828689575
Validation loss: 2.154856030659009

Epoch: 5| Step: 4
Training loss: 2.4316537380218506
Validation loss: 2.1569164747832925

Epoch: 5| Step: 5
Training loss: 2.420689344406128
Validation loss: 2.1603679785164456

Epoch: 5| Step: 6
Training loss: 2.0735201835632324
Validation loss: 2.151203052971953

Epoch: 5| Step: 7
Training loss: 1.787583589553833
Validation loss: 2.1581259594168714

Epoch: 5| Step: 8
Training loss: 2.1500911712646484
Validation loss: 2.1759204877320157

Epoch: 5| Step: 9
Training loss: 2.2588534355163574
Validation loss: 2.1703918621104252

Epoch: 5| Step: 10
Training loss: 2.1993939876556396
Validation loss: 2.179985348896314

Epoch: 121| Step: 0
Training loss: 2.4659430980682373
Validation loss: 2.2000145194351033

Epoch: 5| Step: 1
Training loss: 1.9619977474212646
Validation loss: 2.215816190165858

Epoch: 5| Step: 2
Training loss: 2.3514516353607178
Validation loss: 2.1952867328479724

Epoch: 5| Step: 3
Training loss: 1.7967125177383423
Validation loss: 2.184060199286348

Epoch: 5| Step: 4
Training loss: 2.2056515216827393
Validation loss: 2.208645589890019

Epoch: 5| Step: 5
Training loss: 2.126570224761963
Validation loss: 2.248952179826716

Epoch: 5| Step: 6
Training loss: 2.1178696155548096
Validation loss: 2.2598655428937686

Epoch: 5| Step: 7
Training loss: 2.1857762336730957
Validation loss: 2.2375395195458525

Epoch: 5| Step: 8
Training loss: 2.740705728530884
Validation loss: 2.1937252424096547

Epoch: 5| Step: 9
Training loss: 2.689589738845825
Validation loss: 2.1561333697329284

Epoch: 5| Step: 10
Training loss: 2.5921616554260254
Validation loss: 2.120014700838315

Epoch: 122| Step: 0
Training loss: 1.5590866804122925
Validation loss: 2.126233252145911

Epoch: 5| Step: 1
Training loss: 2.637204647064209
Validation loss: 2.111683448155721

Epoch: 5| Step: 2
Training loss: 2.590120315551758
Validation loss: 2.112738658023137

Epoch: 5| Step: 3
Training loss: 2.2336032390594482
Validation loss: 2.1103618170625422

Epoch: 5| Step: 4
Training loss: 1.5062578916549683
Validation loss: 2.132942471452939

Epoch: 5| Step: 5
Training loss: 2.8447494506835938
Validation loss: 2.1422313362039547

Epoch: 5| Step: 6
Training loss: 1.9877628087997437
Validation loss: 2.1319294898740706

Epoch: 5| Step: 7
Training loss: 2.7779452800750732
Validation loss: 2.138775593491011

Epoch: 5| Step: 8
Training loss: 1.7531089782714844
Validation loss: 2.143488391753166

Epoch: 5| Step: 9
Training loss: 1.7433083057403564
Validation loss: 2.1543023663182415

Epoch: 5| Step: 10
Training loss: 3.3040707111358643
Validation loss: 2.1512104695843113

Epoch: 123| Step: 0
Training loss: 2.58105206489563
Validation loss: 2.184298328174058

Epoch: 5| Step: 1
Training loss: 2.324406623840332
Validation loss: 2.190028131649058

Epoch: 5| Step: 2
Training loss: 2.173945903778076
Validation loss: 2.214014612218385

Epoch: 5| Step: 3
Training loss: 2.260833740234375
Validation loss: 2.1825207792302614

Epoch: 5| Step: 4
Training loss: 2.4013986587524414
Validation loss: 2.1600973631746028

Epoch: 5| Step: 5
Training loss: 2.041090488433838
Validation loss: 2.145014334750432

Epoch: 5| Step: 6
Training loss: 1.9846031665802002
Validation loss: 2.1411366180707048

Epoch: 5| Step: 7
Training loss: 2.296574831008911
Validation loss: 2.1375141002798594

Epoch: 5| Step: 8
Training loss: 2.229870557785034
Validation loss: 2.12450272421683

Epoch: 5| Step: 9
Training loss: 1.9038984775543213
Validation loss: 2.1342484130654285

Epoch: 5| Step: 10
Training loss: 2.405738353729248
Validation loss: 2.1398657957712808

Epoch: 124| Step: 0
Training loss: 2.22502064704895
Validation loss: 2.1482980225675847

Epoch: 5| Step: 1
Training loss: 1.6293312311172485
Validation loss: 2.1622610809982463

Epoch: 5| Step: 2
Training loss: 2.118375062942505
Validation loss: 2.1603182579881404

Epoch: 5| Step: 3
Training loss: 2.050905704498291
Validation loss: 2.1702443220282115

Epoch: 5| Step: 4
Training loss: 1.3922759294509888
Validation loss: 2.1448400225690616

Epoch: 5| Step: 5
Training loss: 2.5907297134399414
Validation loss: 2.1432648512624923

Epoch: 5| Step: 6
Training loss: 1.9766113758087158
Validation loss: 2.1527098122463433

Epoch: 5| Step: 7
Training loss: 2.9386038780212402
Validation loss: 2.1514761704270557

Epoch: 5| Step: 8
Training loss: 2.723820447921753
Validation loss: 2.180602467188271

Epoch: 5| Step: 9
Training loss: 2.6802239418029785
Validation loss: 2.1944459048650597

Epoch: 5| Step: 10
Training loss: 2.1418063640594482
Validation loss: 2.1996017040744906

Epoch: 125| Step: 0
Training loss: 1.9996516704559326
Validation loss: 2.1891470903991372

Epoch: 5| Step: 1
Training loss: 2.6424853801727295
Validation loss: 2.1823554846548263

Epoch: 5| Step: 2
Training loss: 2.4265329837799072
Validation loss: 2.1578931757198867

Epoch: 5| Step: 3
Training loss: 1.8127238750457764
Validation loss: 2.1541787962759695

Epoch: 5| Step: 4
Training loss: 2.536956787109375
Validation loss: 2.1604470668300504

Epoch: 5| Step: 5
Training loss: 2.2102138996124268
Validation loss: 2.182136902245142

Epoch: 5| Step: 6
Training loss: 1.9705194234848022
Validation loss: 2.181062493273007

Epoch: 5| Step: 7
Training loss: 2.4924213886260986
Validation loss: 2.1570444030146443

Epoch: 5| Step: 8
Training loss: 2.345297336578369
Validation loss: 2.158168226160029

Epoch: 5| Step: 9
Training loss: 1.793003797531128
Validation loss: 2.114172427884994

Epoch: 5| Step: 10
Training loss: 1.9054954051971436
Validation loss: 2.113746594357234

Epoch: 126| Step: 0
Training loss: 2.0991387367248535
Validation loss: 2.1122628488848285

Epoch: 5| Step: 1
Training loss: 1.9358789920806885
Validation loss: 2.1203234323891262

Epoch: 5| Step: 2
Training loss: 1.9907058477401733
Validation loss: 2.1028176482005785

Epoch: 5| Step: 3
Training loss: 2.3601460456848145
Validation loss: 2.1221904741820468

Epoch: 5| Step: 4
Training loss: 2.8449995517730713
Validation loss: 2.1413177008269937

Epoch: 5| Step: 5
Training loss: 2.551090955734253
Validation loss: 2.184275040062525

Epoch: 5| Step: 6
Training loss: 2.01710844039917
Validation loss: 2.1828528501654185

Epoch: 5| Step: 7
Training loss: 2.4057984352111816
Validation loss: 2.2050337047987085

Epoch: 5| Step: 8
Training loss: 1.4977633953094482
Validation loss: 2.1846251692823184

Epoch: 5| Step: 9
Training loss: 2.6769957542419434
Validation loss: 2.184396154137068

Epoch: 5| Step: 10
Training loss: 1.7758195400238037
Validation loss: 2.142150899415375

Epoch: 127| Step: 0
Training loss: 2.6729910373687744
Validation loss: 2.1271771974461053

Epoch: 5| Step: 1
Training loss: 2.3905885219573975
Validation loss: 2.130008912855579

Epoch: 5| Step: 2
Training loss: 1.777008056640625
Validation loss: 2.145575337512519

Epoch: 5| Step: 3
Training loss: 2.2420058250427246
Validation loss: 2.146719010927344

Epoch: 5| Step: 4
Training loss: 2.5686161518096924
Validation loss: 2.1288487065222954

Epoch: 5| Step: 5
Training loss: 1.4827803373336792
Validation loss: 2.143150962809081

Epoch: 5| Step: 6
Training loss: 1.7341238260269165
Validation loss: 2.146802028020223

Epoch: 5| Step: 7
Training loss: 2.598137378692627
Validation loss: 2.1626438004996187

Epoch: 5| Step: 8
Training loss: 1.6305646896362305
Validation loss: 2.156614067733929

Epoch: 5| Step: 9
Training loss: 2.9543514251708984
Validation loss: 2.159313776159799

Epoch: 5| Step: 10
Training loss: 1.6910382509231567
Validation loss: 2.1407240103649836

Epoch: 128| Step: 0
Training loss: 2.1563022136688232
Validation loss: 2.137622037241536

Epoch: 5| Step: 1
Training loss: 2.0087404251098633
Validation loss: 2.1529528094876196

Epoch: 5| Step: 2
Training loss: 2.0933589935302734
Validation loss: 2.1443059828973587

Epoch: 5| Step: 3
Training loss: 2.059504985809326
Validation loss: 2.131658205422022

Epoch: 5| Step: 4
Training loss: 2.233524799346924
Validation loss: 2.12596829219531

Epoch: 5| Step: 5
Training loss: 1.892480492591858
Validation loss: 2.129005370601531

Epoch: 5| Step: 6
Training loss: 2.125845432281494
Validation loss: 2.152184186443206

Epoch: 5| Step: 7
Training loss: 2.2275593280792236
Validation loss: 2.172052344968242

Epoch: 5| Step: 8
Training loss: 1.8663766384124756
Validation loss: 2.193964478790119

Epoch: 5| Step: 9
Training loss: 2.7335383892059326
Validation loss: 2.248308494526853

Epoch: 5| Step: 10
Training loss: 2.830653667449951
Validation loss: 2.220240972375357

Epoch: 129| Step: 0
Training loss: 2.213768482208252
Validation loss: 2.157210651264396

Epoch: 5| Step: 1
Training loss: 2.2310965061187744
Validation loss: 2.1038050343913417

Epoch: 5| Step: 2
Training loss: 1.898834466934204
Validation loss: 2.0716449035111295

Epoch: 5| Step: 3
Training loss: 2.943692445755005
Validation loss: 2.0702253182729087

Epoch: 5| Step: 4
Training loss: 2.1086461544036865
Validation loss: 2.0704062651562434

Epoch: 5| Step: 5
Training loss: 2.005153179168701
Validation loss: 2.0620327982851254

Epoch: 5| Step: 6
Training loss: 2.796513795852661
Validation loss: 2.047311746945945

Epoch: 5| Step: 7
Training loss: 1.7680248022079468
Validation loss: 2.038441429856003

Epoch: 5| Step: 8
Training loss: 2.509031295776367
Validation loss: 2.030706987586073

Epoch: 5| Step: 9
Training loss: 2.55289888381958
Validation loss: 2.0480162533380653

Epoch: 5| Step: 10
Training loss: 1.742879867553711
Validation loss: 2.056669368538805

Epoch: 130| Step: 0
Training loss: 1.8911606073379517
Validation loss: 2.0981302812535274

Epoch: 5| Step: 1
Training loss: 2.431628942489624
Validation loss: 2.1934229379059165

Epoch: 5| Step: 2
Training loss: 2.396202802658081
Validation loss: 2.2638276802596224

Epoch: 5| Step: 3
Training loss: 2.507262945175171
Validation loss: 2.343488403545913

Epoch: 5| Step: 4
Training loss: 2.088548183441162
Validation loss: 2.3815043536565637

Epoch: 5| Step: 5
Training loss: 2.08634352684021
Validation loss: 2.3742621098795245

Epoch: 5| Step: 6
Training loss: 1.550280213356018
Validation loss: 2.2992588884087017

Epoch: 5| Step: 7
Training loss: 2.333110809326172
Validation loss: 2.2140025656710387

Epoch: 5| Step: 8
Training loss: 2.256517171859741
Validation loss: 2.1205438952292166

Epoch: 5| Step: 9
Training loss: 1.834024429321289
Validation loss: 2.0713148219611055

Epoch: 5| Step: 10
Training loss: 3.1910765171051025
Validation loss: 2.0648227917250765

Epoch: 131| Step: 0
Training loss: 2.2081704139709473
Validation loss: 2.061597607469046

Epoch: 5| Step: 1
Training loss: 1.764823317527771
Validation loss: 2.0680433319460962

Epoch: 5| Step: 2
Training loss: 1.7693195343017578
Validation loss: 2.0698212679996284

Epoch: 5| Step: 3
Training loss: 2.2845489978790283
Validation loss: 2.070659363141624

Epoch: 5| Step: 4
Training loss: 1.4676835536956787
Validation loss: 2.0889482908351447

Epoch: 5| Step: 5
Training loss: 2.8940563201904297
Validation loss: 2.0909951015185286

Epoch: 5| Step: 6
Training loss: 2.512488842010498
Validation loss: 2.100414014631702

Epoch: 5| Step: 7
Training loss: 2.4172160625457764
Validation loss: 2.1200858572477936

Epoch: 5| Step: 8
Training loss: 2.0701050758361816
Validation loss: 2.0994908014933267

Epoch: 5| Step: 9
Training loss: 2.5479207038879395
Validation loss: 2.096156117736652

Epoch: 5| Step: 10
Training loss: 2.26556134223938
Validation loss: 2.103231309562601

Epoch: 132| Step: 0
Training loss: 1.901418924331665
Validation loss: 2.1071613565568

Epoch: 5| Step: 1
Training loss: 2.50475811958313
Validation loss: 2.155611817554761

Epoch: 5| Step: 2
Training loss: 2.5832643508911133
Validation loss: 2.168528954188029

Epoch: 5| Step: 3
Training loss: 2.03538179397583
Validation loss: 2.1926634516767276

Epoch: 5| Step: 4
Training loss: 1.8969529867172241
Validation loss: 2.193402990218132

Epoch: 5| Step: 5
Training loss: 1.6279847621917725
Validation loss: 2.1966855218333583

Epoch: 5| Step: 6
Training loss: 1.992506980895996
Validation loss: 2.1706966405273764

Epoch: 5| Step: 7
Training loss: 2.5452685356140137
Validation loss: 2.170941011880034

Epoch: 5| Step: 8
Training loss: 2.533905029296875
Validation loss: 2.15086095563827

Epoch: 5| Step: 9
Training loss: 1.7782169580459595
Validation loss: 2.165431095707801

Epoch: 5| Step: 10
Training loss: 2.269113779067993
Validation loss: 2.191317037869525

Epoch: 133| Step: 0
Training loss: 2.097712755203247
Validation loss: 2.2206796676881853

Epoch: 5| Step: 1
Training loss: 1.7525012493133545
Validation loss: 2.259466681429135

Epoch: 5| Step: 2
Training loss: 2.165872097015381
Validation loss: 2.2385811959543536

Epoch: 5| Step: 3
Training loss: 2.1129393577575684
Validation loss: 2.2264344410229753

Epoch: 5| Step: 4
Training loss: 1.8839530944824219
Validation loss: 2.17390916937141

Epoch: 5| Step: 5
Training loss: 1.8367782831192017
Validation loss: 2.1652627657818537

Epoch: 5| Step: 6
Training loss: 2.453324317932129
Validation loss: 2.1576358861820673

Epoch: 5| Step: 7
Training loss: 2.063715696334839
Validation loss: 2.150700074370189

Epoch: 5| Step: 8
Training loss: 2.3639118671417236
Validation loss: 2.1749107837677

Epoch: 5| Step: 9
Training loss: 3.090217113494873
Validation loss: 2.144845131904848

Epoch: 5| Step: 10
Training loss: 2.334678888320923
Validation loss: 2.101838242623114

Epoch: 134| Step: 0
Training loss: 2.09047794342041
Validation loss: 2.079989112833495

Epoch: 5| Step: 1
Training loss: 3.0062096118927
Validation loss: 2.083701797710952

Epoch: 5| Step: 2
Training loss: 2.037688732147217
Validation loss: 2.08534098440601

Epoch: 5| Step: 3
Training loss: 1.9227405786514282
Validation loss: 2.1119408235755017

Epoch: 5| Step: 4
Training loss: 1.68655264377594
Validation loss: 2.1556478187602055

Epoch: 5| Step: 5
Training loss: 2.716770648956299
Validation loss: 2.194665052557504

Epoch: 5| Step: 6
Training loss: 2.068985939025879
Validation loss: 2.207192341486613

Epoch: 5| Step: 7
Training loss: 1.635627031326294
Validation loss: 2.1965396070993073

Epoch: 5| Step: 8
Training loss: 2.146653890609741
Validation loss: 2.1696514544948453

Epoch: 5| Step: 9
Training loss: 2.1832504272460938
Validation loss: 2.1408655028189383

Epoch: 5| Step: 10
Training loss: 2.2674665451049805
Validation loss: 2.1099483607917704

Epoch: 135| Step: 0
Training loss: 2.1457462310791016
Validation loss: 2.109675366391418

Epoch: 5| Step: 1
Training loss: 1.9752624034881592
Validation loss: 2.1138017600582493

Epoch: 5| Step: 2
Training loss: 2.394381284713745
Validation loss: 2.1253213779900664

Epoch: 5| Step: 3
Training loss: 2.018057346343994
Validation loss: 2.10942772383331

Epoch: 5| Step: 4
Training loss: 2.5662872791290283
Validation loss: 2.1159105082993865

Epoch: 5| Step: 5
Training loss: 2.2236907482147217
Validation loss: 2.0954214603670183

Epoch: 5| Step: 6
Training loss: 1.9515974521636963
Validation loss: 2.07411927048878

Epoch: 5| Step: 7
Training loss: 2.113983631134033
Validation loss: 2.0582852978860178

Epoch: 5| Step: 8
Training loss: 1.8244987726211548
Validation loss: 2.077744076328893

Epoch: 5| Step: 9
Training loss: 2.2055869102478027
Validation loss: 2.0747595653739026

Epoch: 5| Step: 10
Training loss: 2.1668004989624023
Validation loss: 2.0905180592690744

Epoch: 136| Step: 0
Training loss: 2.4995617866516113
Validation loss: 2.135590046964666

Epoch: 5| Step: 1
Training loss: 2.1057639122009277
Validation loss: 2.16590856223978

Epoch: 5| Step: 2
Training loss: 2.2233829498291016
Validation loss: 2.2156282727436354

Epoch: 5| Step: 3
Training loss: 2.262692928314209
Validation loss: 2.289098572987382

Epoch: 5| Step: 4
Training loss: 1.9332563877105713
Validation loss: 2.2980339732221378

Epoch: 5| Step: 5
Training loss: 2.3895363807678223
Validation loss: 2.2800350881391958

Epoch: 5| Step: 6
Training loss: 1.845056176185608
Validation loss: 2.2087943733379407

Epoch: 5| Step: 7
Training loss: 2.4920601844787598
Validation loss: 2.100505731439078

Epoch: 5| Step: 8
Training loss: 1.3023704290390015
Validation loss: 2.0682728713558567

Epoch: 5| Step: 9
Training loss: 2.0422470569610596
Validation loss: 2.0500709651618876

Epoch: 5| Step: 10
Training loss: 2.1599619388580322
Validation loss: 2.038485660347887

Epoch: 137| Step: 0
Training loss: 1.8570287227630615
Validation loss: 2.0375909779661443

Epoch: 5| Step: 1
Training loss: 1.8101304769515991
Validation loss: 2.0275683787561234

Epoch: 5| Step: 2
Training loss: 2.6217148303985596
Validation loss: 2.0253022691254974

Epoch: 5| Step: 3
Training loss: 2.1177589893341064
Validation loss: 2.0301199151623632

Epoch: 5| Step: 4
Training loss: 2.642533540725708
Validation loss: 2.053331231558195

Epoch: 5| Step: 5
Training loss: 2.228705644607544
Validation loss: 2.0623919476744947

Epoch: 5| Step: 6
Training loss: 2.998908519744873
Validation loss: 2.06215984846956

Epoch: 5| Step: 7
Training loss: 2.302042245864868
Validation loss: 2.073722449682092

Epoch: 5| Step: 8
Training loss: 1.6240530014038086
Validation loss: 2.064484903889318

Epoch: 5| Step: 9
Training loss: 2.0247464179992676
Validation loss: 2.085333593430058

Epoch: 5| Step: 10
Training loss: 1.757886528968811
Validation loss: 2.092416658196398

Epoch: 138| Step: 0
Training loss: 1.759982705116272
Validation loss: 2.110665862278272

Epoch: 5| Step: 1
Training loss: 2.0144948959350586
Validation loss: 2.147453941324706

Epoch: 5| Step: 2
Training loss: 1.9892566204071045
Validation loss: 2.166946800806189

Epoch: 5| Step: 3
Training loss: 2.08799409866333
Validation loss: 2.1833685521156556

Epoch: 5| Step: 4
Training loss: 2.05753755569458
Validation loss: 2.1646384346869683

Epoch: 5| Step: 5
Training loss: 3.159290075302124
Validation loss: 2.147862457459973

Epoch: 5| Step: 6
Training loss: 1.3920260667800903
Validation loss: 2.1303337773969098

Epoch: 5| Step: 7
Training loss: 1.8826935291290283
Validation loss: 2.1364793059646443

Epoch: 5| Step: 8
Training loss: 2.4799656867980957
Validation loss: 2.1415318442929174

Epoch: 5| Step: 9
Training loss: 2.541074275970459
Validation loss: 2.1469836978502173

Epoch: 5| Step: 10
Training loss: 1.362007737159729
Validation loss: 2.1262013373836393

Epoch: 139| Step: 0
Training loss: 2.381716728210449
Validation loss: 2.1219873864163636

Epoch: 5| Step: 1
Training loss: 1.9388405084609985
Validation loss: 2.136246622249644

Epoch: 5| Step: 2
Training loss: 2.7250406742095947
Validation loss: 2.12037326956308

Epoch: 5| Step: 3
Training loss: 1.8222519159317017
Validation loss: 2.123177454035769

Epoch: 5| Step: 4
Training loss: 1.9034782648086548
Validation loss: 2.1064762274424234

Epoch: 5| Step: 5
Training loss: 1.197090744972229
Validation loss: 2.122692938773863

Epoch: 5| Step: 6
Training loss: 2.0504138469696045
Validation loss: 2.1117141400614092

Epoch: 5| Step: 7
Training loss: 2.6946167945861816
Validation loss: 2.099157989666026

Epoch: 5| Step: 8
Training loss: 1.8851711750030518
Validation loss: 2.0755463210485314

Epoch: 5| Step: 9
Training loss: 1.5651572942733765
Validation loss: 2.069212841731246

Epoch: 5| Step: 10
Training loss: 2.3442647457122803
Validation loss: 2.0850419511077223

Epoch: 140| Step: 0
Training loss: 1.2851886749267578
Validation loss: 2.1005860118455786

Epoch: 5| Step: 1
Training loss: 2.0616488456726074
Validation loss: 2.126860728827856

Epoch: 5| Step: 2
Training loss: 1.8468536138534546
Validation loss: 2.130374848201711

Epoch: 5| Step: 3
Training loss: 1.945055603981018
Validation loss: 2.1508021611039356

Epoch: 5| Step: 4
Training loss: 2.135469913482666
Validation loss: 2.1450855834509737

Epoch: 5| Step: 5
Training loss: 2.2542927265167236
Validation loss: 2.140072799498035

Epoch: 5| Step: 6
Training loss: 1.9223960638046265
Validation loss: 2.136406683152722

Epoch: 5| Step: 7
Training loss: 2.2002265453338623
Validation loss: 2.1109556177610993

Epoch: 5| Step: 8
Training loss: 2.547703266143799
Validation loss: 2.073014493911497

Epoch: 5| Step: 9
Training loss: 1.993532419204712
Validation loss: 2.065850709074287

Epoch: 5| Step: 10
Training loss: 2.2141995429992676
Validation loss: 2.063096220775317

Epoch: 141| Step: 0
Training loss: 2.345214366912842
Validation loss: 2.0771762453099734

Epoch: 5| Step: 1
Training loss: 2.311933994293213
Validation loss: 2.0778919073843185

Epoch: 5| Step: 2
Training loss: 1.7860805988311768
Validation loss: 2.084661845237978

Epoch: 5| Step: 3
Training loss: 1.5763354301452637
Validation loss: 2.0940774051092004

Epoch: 5| Step: 4
Training loss: 1.2139942646026611
Validation loss: 2.121871340659357

Epoch: 5| Step: 5
Training loss: 2.3557167053222656
Validation loss: 2.133860970056185

Epoch: 5| Step: 6
Training loss: 1.642202377319336
Validation loss: 2.1235783792311147

Epoch: 5| Step: 7
Training loss: 1.9576107263565063
Validation loss: 2.13367094532136

Epoch: 5| Step: 8
Training loss: 2.1849334239959717
Validation loss: 2.1510735993744223

Epoch: 5| Step: 9
Training loss: 2.356612205505371
Validation loss: 2.178036246248471

Epoch: 5| Step: 10
Training loss: 2.584409475326538
Validation loss: 2.170552335759645

Epoch: 142| Step: 0
Training loss: 2.3088619709014893
Validation loss: 2.139347021297742

Epoch: 5| Step: 1
Training loss: 1.3708629608154297
Validation loss: 2.1176611185073853

Epoch: 5| Step: 2
Training loss: 2.5052177906036377
Validation loss: 2.0821473752298663

Epoch: 5| Step: 3
Training loss: 2.6308541297912598
Validation loss: 2.067799024684455

Epoch: 5| Step: 4
Training loss: 1.638471245765686
Validation loss: 2.0671098885997647

Epoch: 5| Step: 5
Training loss: 1.694239854812622
Validation loss: 2.044778012460278

Epoch: 5| Step: 6
Training loss: 1.672597885131836
Validation loss: 2.073945250562442

Epoch: 5| Step: 7
Training loss: 1.59000825881958
Validation loss: 2.1091013441803637

Epoch: 5| Step: 8
Training loss: 2.051150321960449
Validation loss: 2.133149557216193

Epoch: 5| Step: 9
Training loss: 1.9655014276504517
Validation loss: 2.140252485070177

Epoch: 5| Step: 10
Training loss: 2.8128244876861572
Validation loss: 2.1549500111610658

Epoch: 143| Step: 0
Training loss: 1.8049821853637695
Validation loss: 2.158269287437521

Epoch: 5| Step: 1
Training loss: 1.5370744466781616
Validation loss: 2.135937161343072

Epoch: 5| Step: 2
Training loss: 2.4579691886901855
Validation loss: 2.1343674300819315

Epoch: 5| Step: 3
Training loss: 0.7312825918197632
Validation loss: 2.119665474020025

Epoch: 5| Step: 4
Training loss: 2.5068485736846924
Validation loss: 2.097979453302199

Epoch: 5| Step: 5
Training loss: 2.2819666862487793
Validation loss: 2.0884101031928934

Epoch: 5| Step: 6
Training loss: 1.910291314125061
Validation loss: 2.089698745358375

Epoch: 5| Step: 7
Training loss: 2.9699127674102783
Validation loss: 2.0777553512204077

Epoch: 5| Step: 8
Training loss: 2.1085009574890137
Validation loss: 2.0828464326038154

Epoch: 5| Step: 9
Training loss: 1.5141651630401611
Validation loss: 2.1014690527351956

Epoch: 5| Step: 10
Training loss: 2.239658832550049
Validation loss: 2.113650532178981

Epoch: 144| Step: 0
Training loss: 1.5775150060653687
Validation loss: 2.130184215884055

Epoch: 5| Step: 1
Training loss: 2.752981185913086
Validation loss: 2.1345419922182636

Epoch: 5| Step: 2
Training loss: 2.254152297973633
Validation loss: 2.1176047914771625

Epoch: 5| Step: 3
Training loss: 1.684970498085022
Validation loss: 2.1334017604909916

Epoch: 5| Step: 4
Training loss: 2.174546957015991
Validation loss: 2.13441784920231

Epoch: 5| Step: 5
Training loss: 2.088867425918579
Validation loss: 2.1278780737230854

Epoch: 5| Step: 6
Training loss: 1.975046157836914
Validation loss: 2.1461171655244726

Epoch: 5| Step: 7
Training loss: 1.9729316234588623
Validation loss: 2.1439122307685112

Epoch: 5| Step: 8
Training loss: 1.4411373138427734
Validation loss: 2.1676017058792936

Epoch: 5| Step: 9
Training loss: 2.1756742000579834
Validation loss: 2.1907448332796813

Epoch: 5| Step: 10
Training loss: 1.8929853439331055
Validation loss: 2.1744724012190297

Epoch: 145| Step: 0
Training loss: 1.5810178518295288
Validation loss: 2.1121670071796705

Epoch: 5| Step: 1
Training loss: 1.6056301593780518
Validation loss: 2.085340722914665

Epoch: 5| Step: 2
Training loss: 1.9983842372894287
Validation loss: 2.048573447811988

Epoch: 5| Step: 3
Training loss: 1.6931089162826538
Validation loss: 2.06768693975223

Epoch: 5| Step: 4
Training loss: 2.618861675262451
Validation loss: 2.10419528074162

Epoch: 5| Step: 5
Training loss: 2.467888355255127
Validation loss: 2.1164228557258524

Epoch: 5| Step: 6
Training loss: 1.6761138439178467
Validation loss: 2.129319493488599

Epoch: 5| Step: 7
Training loss: 1.8539108037948608
Validation loss: 2.1256131356762302

Epoch: 5| Step: 8
Training loss: 1.5743443965911865
Validation loss: 2.1207768763265302

Epoch: 5| Step: 9
Training loss: 2.504596710205078
Validation loss: 2.108729318905902

Epoch: 5| Step: 10
Training loss: 2.692506790161133
Validation loss: 2.114206317932375

Epoch: 146| Step: 0
Training loss: 1.7582813501358032
Validation loss: 2.1275101169463126

Epoch: 5| Step: 1
Training loss: 1.7891952991485596
Validation loss: 2.1086653445356633

Epoch: 5| Step: 2
Training loss: 2.1552059650421143
Validation loss: 2.079068876081897

Epoch: 5| Step: 3
Training loss: 2.109769344329834
Validation loss: 2.0723173105588524

Epoch: 5| Step: 4
Training loss: 1.738265037536621
Validation loss: 2.0596234234430457

Epoch: 5| Step: 5
Training loss: 2.5514509677886963
Validation loss: 2.0592897527961322

Epoch: 5| Step: 6
Training loss: 1.9875072240829468
Validation loss: 2.0672337150061004

Epoch: 5| Step: 7
Training loss: 1.6151206493377686
Validation loss: 2.0677441602112143

Epoch: 5| Step: 8
Training loss: 2.2488155364990234
Validation loss: 2.0749456190293833

Epoch: 5| Step: 9
Training loss: 1.803818941116333
Validation loss: 2.082949589657527

Epoch: 5| Step: 10
Training loss: 2.222062587738037
Validation loss: 2.0821756675679195

Epoch: 147| Step: 0
Training loss: 1.5169349908828735
Validation loss: 2.1174468968504216

Epoch: 5| Step: 1
Training loss: 2.249152660369873
Validation loss: 2.146160469260267

Epoch: 5| Step: 2
Training loss: 1.6818406581878662
Validation loss: 2.1746781923437632

Epoch: 5| Step: 3
Training loss: 1.9467334747314453
Validation loss: 2.206262344955116

Epoch: 5| Step: 4
Training loss: 2.164463996887207
Validation loss: 2.1993704354891213

Epoch: 5| Step: 5
Training loss: 2.0823988914489746
Validation loss: 2.1741003477445213

Epoch: 5| Step: 6
Training loss: 1.924896478652954
Validation loss: 2.1960104973085466

Epoch: 5| Step: 7
Training loss: 1.819837212562561
Validation loss: 2.167830333914808

Epoch: 5| Step: 8
Training loss: 2.9175972938537598
Validation loss: 2.1419341743633313

Epoch: 5| Step: 9
Training loss: 1.9929536581039429
Validation loss: 2.0996097595460954

Epoch: 5| Step: 10
Training loss: 1.4182013273239136
Validation loss: 2.090299343550077

Epoch: 148| Step: 0
Training loss: 2.091226100921631
Validation loss: 2.0699386827407347

Epoch: 5| Step: 1
Training loss: 1.7216966152191162
Validation loss: 2.0511988132230696

Epoch: 5| Step: 2
Training loss: 2.2814152240753174
Validation loss: 2.047977093727358

Epoch: 5| Step: 3
Training loss: 2.1737499237060547
Validation loss: 2.083073935201091

Epoch: 5| Step: 4
Training loss: 2.283318042755127
Validation loss: 2.0756842577329246

Epoch: 5| Step: 5
Training loss: 1.7523272037506104
Validation loss: 2.0929913995086507

Epoch: 5| Step: 6
Training loss: 1.6720492839813232
Validation loss: 2.142160979650354

Epoch: 5| Step: 7
Training loss: 1.591354489326477
Validation loss: 2.1901050613772486

Epoch: 5| Step: 8
Training loss: 1.692823052406311
Validation loss: 2.2144299296922583

Epoch: 5| Step: 9
Training loss: 2.2636466026306152
Validation loss: 2.1956969102223716

Epoch: 5| Step: 10
Training loss: 2.3081552982330322
Validation loss: 2.160553237443329

Epoch: 149| Step: 0
Training loss: 1.8559291362762451
Validation loss: 2.12178849276676

Epoch: 5| Step: 1
Training loss: 1.816407561302185
Validation loss: 2.0893313474552606

Epoch: 5| Step: 2
Training loss: 2.078608989715576
Validation loss: 2.10169396861907

Epoch: 5| Step: 3
Training loss: 1.2587448358535767
Validation loss: 2.0922471015684065

Epoch: 5| Step: 4
Training loss: 1.8819167613983154
Validation loss: 2.0791065949265675

Epoch: 5| Step: 5
Training loss: 1.724449872970581
Validation loss: 2.082730052291706

Epoch: 5| Step: 6
Training loss: 2.25787353515625
Validation loss: 2.0565275889570995

Epoch: 5| Step: 7
Training loss: 2.5392208099365234
Validation loss: 2.05659887354861

Epoch: 5| Step: 8
Training loss: 1.4651126861572266
Validation loss: 2.0593987023958595

Epoch: 5| Step: 9
Training loss: 1.8191512823104858
Validation loss: 2.0587018971802085

Epoch: 5| Step: 10
Training loss: 2.730743885040283
Validation loss: 2.0818301541830904

Epoch: 150| Step: 0
Training loss: 1.027963399887085
Validation loss: 2.1044544994190173

Epoch: 5| Step: 1
Training loss: 1.901921272277832
Validation loss: 2.105464750720609

Epoch: 5| Step: 2
Training loss: 2.1679680347442627
Validation loss: 2.114560547695365

Epoch: 5| Step: 3
Training loss: 2.311594009399414
Validation loss: 2.125415207237326

Epoch: 5| Step: 4
Training loss: 1.5766500234603882
Validation loss: 2.1435428204074984

Epoch: 5| Step: 5
Training loss: 1.5234237909317017
Validation loss: 2.1583640101135417

Epoch: 5| Step: 6
Training loss: 2.833998680114746
Validation loss: 2.1589908676762737

Epoch: 5| Step: 7
Training loss: 1.7210050821304321
Validation loss: 2.169509018621137

Epoch: 5| Step: 8
Training loss: 2.5067005157470703
Validation loss: 2.164355952252624

Epoch: 5| Step: 9
Training loss: 1.8218358755111694
Validation loss: 2.1473022737810687

Epoch: 5| Step: 10
Training loss: 1.717893123626709
Validation loss: 2.1186445887370775

Epoch: 151| Step: 0
Training loss: 1.9748420715332031
Validation loss: 2.111331606423983

Epoch: 5| Step: 1
Training loss: 2.238344669342041
Validation loss: 2.0953202914166194

Epoch: 5| Step: 2
Training loss: 1.6715383529663086
Validation loss: 2.0867602107345418

Epoch: 5| Step: 3
Training loss: 1.5021378993988037
Validation loss: 2.097754855309763

Epoch: 5| Step: 4
Training loss: 1.7224063873291016
Validation loss: 2.1250407465042604

Epoch: 5| Step: 5
Training loss: 1.6993110179901123
Validation loss: 2.135700366830313

Epoch: 5| Step: 6
Training loss: 2.392174243927002
Validation loss: 2.1768986717347176

Epoch: 5| Step: 7
Training loss: 2.2626495361328125
Validation loss: 2.177663290372459

Epoch: 5| Step: 8
Training loss: 1.7244081497192383
Validation loss: 2.125397971881333

Epoch: 5| Step: 9
Training loss: 2.1698555946350098
Validation loss: 2.0947938529394006

Epoch: 5| Step: 10
Training loss: 1.6750760078430176
Validation loss: 2.0951041893292497

Epoch: 152| Step: 0
Training loss: 2.0414741039276123
Validation loss: 2.090489614394403

Epoch: 5| Step: 1
Training loss: 2.1032402515411377
Validation loss: 2.105368862869919

Epoch: 5| Step: 2
Training loss: 2.006131410598755
Validation loss: 2.1049971913778656

Epoch: 5| Step: 3
Training loss: 2.1428425312042236
Validation loss: 2.1107898732667327

Epoch: 5| Step: 4
Training loss: 1.5332096815109253
Validation loss: 2.1273254322749313

Epoch: 5| Step: 5
Training loss: 1.862342119216919
Validation loss: 2.108755456504001

Epoch: 5| Step: 6
Training loss: 1.9862041473388672
Validation loss: 2.0954151153564453

Epoch: 5| Step: 7
Training loss: 1.8787368535995483
Validation loss: 2.1014903053160636

Epoch: 5| Step: 8
Training loss: 1.7200380563735962
Validation loss: 2.0912997620080107

Epoch: 5| Step: 9
Training loss: 1.2679603099822998
Validation loss: 2.133119821548462

Epoch: 5| Step: 10
Training loss: 2.435366153717041
Validation loss: 2.1012957916464856

Epoch: 153| Step: 0
Training loss: 2.8940787315368652
Validation loss: 2.06733516980243

Epoch: 5| Step: 1
Training loss: 1.7940151691436768
Validation loss: 2.0545971598676456

Epoch: 5| Step: 2
Training loss: 1.8315207958221436
Validation loss: 2.0441413656357796

Epoch: 5| Step: 3
Training loss: 1.5633281469345093
Validation loss: 2.0328716744658766

Epoch: 5| Step: 4
Training loss: 2.1120569705963135
Validation loss: 2.031783634616483

Epoch: 5| Step: 5
Training loss: 1.9878753423690796
Validation loss: 2.0523495981770177

Epoch: 5| Step: 6
Training loss: 1.760992407798767
Validation loss: 2.0493566028533445

Epoch: 5| Step: 7
Training loss: 1.6230220794677734
Validation loss: 2.068679066114528

Epoch: 5| Step: 8
Training loss: 1.8723522424697876
Validation loss: 2.093970896095358

Epoch: 5| Step: 9
Training loss: 1.5881893634796143
Validation loss: 2.119887780117732

Epoch: 5| Step: 10
Training loss: 1.717714548110962
Validation loss: 2.146819553067607

Epoch: 154| Step: 0
Training loss: 2.039778232574463
Validation loss: 2.1629401201842935

Epoch: 5| Step: 1
Training loss: 1.9991931915283203
Validation loss: 2.179491212291102

Epoch: 5| Step: 2
Training loss: 2.112107038497925
Validation loss: 2.1615331172943115

Epoch: 5| Step: 3
Training loss: 2.407639265060425
Validation loss: 2.1022773083820137

Epoch: 5| Step: 4
Training loss: 1.9616937637329102
Validation loss: 2.0684367661835044

Epoch: 5| Step: 5
Training loss: 1.0134788751602173
Validation loss: 2.0458721806926112

Epoch: 5| Step: 6
Training loss: 1.8060716390609741
Validation loss: 2.042201521576092

Epoch: 5| Step: 7
Training loss: 1.8746238946914673
Validation loss: 2.0521621319555465

Epoch: 5| Step: 8
Training loss: 2.140542507171631
Validation loss: 2.0331684620149675

Epoch: 5| Step: 9
Training loss: 1.4940820932388306
Validation loss: 2.0544036549906575

Epoch: 5| Step: 10
Training loss: 2.0207595825195312
Validation loss: 2.0423125938702653

Epoch: 155| Step: 0
Training loss: 2.2440428733825684
Validation loss: 2.0614412664085306

Epoch: 5| Step: 1
Training loss: 1.5065151453018188
Validation loss: 2.0854084491729736

Epoch: 5| Step: 2
Training loss: 1.9242885112762451
Validation loss: 2.113424952312182

Epoch: 5| Step: 3
Training loss: 2.160592555999756
Validation loss: 2.1253567485399145

Epoch: 5| Step: 4
Training loss: 1.3927810192108154
Validation loss: 2.1211176585125666

Epoch: 5| Step: 5
Training loss: 2.165809154510498
Validation loss: 2.1138418592432493

Epoch: 5| Step: 6
Training loss: 2.011579990386963
Validation loss: 2.0932000888291227

Epoch: 5| Step: 7
Training loss: 1.6722862720489502
Validation loss: 2.1124965606197232

Epoch: 5| Step: 8
Training loss: 1.1550273895263672
Validation loss: 2.0927395525799004

Epoch: 5| Step: 9
Training loss: 2.2837932109832764
Validation loss: 2.0806940165899133

Epoch: 5| Step: 10
Training loss: 1.797208547592163
Validation loss: 2.056563392762215

Epoch: 156| Step: 0
Training loss: 2.427797794342041
Validation loss: 2.0510430976908696

Epoch: 5| Step: 1
Training loss: 1.8562742471694946
Validation loss: 2.0671136020332255

Epoch: 5| Step: 2
Training loss: 1.8282161951065063
Validation loss: 2.046962006117708

Epoch: 5| Step: 3
Training loss: 1.6851756572723389
Validation loss: 2.04238417584409

Epoch: 5| Step: 4
Training loss: 1.5395612716674805
Validation loss: 2.0367293768031622

Epoch: 5| Step: 5
Training loss: 2.246424913406372
Validation loss: 2.046373403200539

Epoch: 5| Step: 6
Training loss: 1.7261642217636108
Validation loss: 2.085243135370234

Epoch: 5| Step: 7
Training loss: 2.0475101470947266
Validation loss: 2.130539660812706

Epoch: 5| Step: 8
Training loss: 1.7020304203033447
Validation loss: 2.1460660093574115

Epoch: 5| Step: 9
Training loss: 1.6637680530548096
Validation loss: 2.1507528110217025

Epoch: 5| Step: 10
Training loss: 1.5609335899353027
Validation loss: 2.130386729394236

Epoch: 157| Step: 0
Training loss: 2.0975852012634277
Validation loss: 2.160118656773721

Epoch: 5| Step: 1
Training loss: 1.8409087657928467
Validation loss: 2.1427044612105175

Epoch: 5| Step: 2
Training loss: 2.122832775115967
Validation loss: 2.1463176511949107

Epoch: 5| Step: 3
Training loss: 1.646152138710022
Validation loss: 2.1399063987116658

Epoch: 5| Step: 4
Training loss: 1.5982496738433838
Validation loss: 2.100921118131248

Epoch: 5| Step: 5
Training loss: 2.0007755756378174
Validation loss: 2.0726573800527923

Epoch: 5| Step: 6
Training loss: 1.9534075260162354
Validation loss: 2.0526895151343396

Epoch: 5| Step: 7
Training loss: 1.3235822916030884
Validation loss: 2.0244592748662478

Epoch: 5| Step: 8
Training loss: 2.072953462600708
Validation loss: 2.0124348312295894

Epoch: 5| Step: 9
Training loss: 1.9306312799453735
Validation loss: 2.015223765885958

Epoch: 5| Step: 10
Training loss: 2.0397956371307373
Validation loss: 2.028784616019136

Epoch: 158| Step: 0
Training loss: 1.537170171737671
Validation loss: 2.0329446895148164

Epoch: 5| Step: 1
Training loss: 1.1037652492523193
Validation loss: 2.059807477458831

Epoch: 5| Step: 2
Training loss: 1.6437278985977173
Validation loss: 2.113308442536221

Epoch: 5| Step: 3
Training loss: 2.0243422985076904
Validation loss: 2.178093150097837

Epoch: 5| Step: 4
Training loss: 1.735515832901001
Validation loss: 2.2471893474619877

Epoch: 5| Step: 5
Training loss: 2.1303141117095947
Validation loss: 2.2400752139347855

Epoch: 5| Step: 6
Training loss: 1.5533539056777954
Validation loss: 2.188722777110274

Epoch: 5| Step: 7
Training loss: 2.4888861179351807
Validation loss: 2.108900161199672

Epoch: 5| Step: 8
Training loss: 2.3001139163970947
Validation loss: 2.069780262567664

Epoch: 5| Step: 9
Training loss: 1.441811442375183
Validation loss: 2.050991091676938

Epoch: 5| Step: 10
Training loss: 2.520899534225464
Validation loss: 2.0239970171323387

Epoch: 159| Step: 0
Training loss: 1.870221495628357
Validation loss: 2.0076802289614113

Epoch: 5| Step: 1
Training loss: 1.6394096612930298
Validation loss: 2.0268605383493568

Epoch: 5| Step: 2
Training loss: 1.8027832508087158
Validation loss: 2.0588607454812653

Epoch: 5| Step: 3
Training loss: 1.527143955230713
Validation loss: 2.0644236662054576

Epoch: 5| Step: 4
Training loss: 2.6095001697540283
Validation loss: 2.088685626624733

Epoch: 5| Step: 5
Training loss: 1.9838371276855469
Validation loss: 2.1233455698977233

Epoch: 5| Step: 6
Training loss: 2.057884454727173
Validation loss: 2.163663357816717

Epoch: 5| Step: 7
Training loss: 2.1075899600982666
Validation loss: 2.1671885867272653

Epoch: 5| Step: 8
Training loss: 1.2830806970596313
Validation loss: 2.1718919533555225

Epoch: 5| Step: 9
Training loss: 1.9831053018569946
Validation loss: 2.1618198681903142

Epoch: 5| Step: 10
Training loss: 1.9488357305526733
Validation loss: 2.175654880462154

Epoch: 160| Step: 0
Training loss: 1.2441415786743164
Validation loss: 2.1320195531332367

Epoch: 5| Step: 1
Training loss: 1.9389095306396484
Validation loss: 2.075916935038823

Epoch: 5| Step: 2
Training loss: 1.3520033359527588
Validation loss: 2.0449201958153838

Epoch: 5| Step: 3
Training loss: 1.2497260570526123
Validation loss: 1.990715770311253

Epoch: 5| Step: 4
Training loss: 2.2859771251678467
Validation loss: 1.9995844120620399

Epoch: 5| Step: 5
Training loss: 1.9717315435409546
Validation loss: 1.9954544190437562

Epoch: 5| Step: 6
Training loss: 2.4316303730010986
Validation loss: 2.0189468822171612

Epoch: 5| Step: 7
Training loss: 1.8681083917617798
Validation loss: 2.0619401649762223

Epoch: 5| Step: 8
Training loss: 1.9406242370605469
Validation loss: 2.08770598647415

Epoch: 5| Step: 9
Training loss: 1.8914358615875244
Validation loss: 2.1388223632689445

Epoch: 5| Step: 10
Training loss: 2.0475544929504395
Validation loss: 2.1940229528693744

Epoch: 161| Step: 0
Training loss: 1.4371956586837769
Validation loss: 2.270980170978013

Epoch: 5| Step: 1
Training loss: 2.703244924545288
Validation loss: 2.2711034769652994

Epoch: 5| Step: 2
Training loss: 1.964027762413025
Validation loss: 2.1311915510444233

Epoch: 5| Step: 3
Training loss: 1.1170058250427246
Validation loss: 2.0774797790793964

Epoch: 5| Step: 4
Training loss: 1.9219646453857422
Validation loss: 2.0593926111857095

Epoch: 5| Step: 5
Training loss: 1.4689059257507324
Validation loss: 2.0519065177568825

Epoch: 5| Step: 6
Training loss: 1.552293062210083
Validation loss: 2.044908023649646

Epoch: 5| Step: 7
Training loss: 1.7982699871063232
Validation loss: 2.0460381456600722

Epoch: 5| Step: 8
Training loss: 2.053737163543701
Validation loss: 2.041674913898591

Epoch: 5| Step: 9
Training loss: 1.987443208694458
Validation loss: 2.0411195101276522

Epoch: 5| Step: 10
Training loss: 2.044318675994873
Validation loss: 2.055783937054296

Epoch: 162| Step: 0
Training loss: 1.554292917251587
Validation loss: 2.0494746546591482

Epoch: 5| Step: 1
Training loss: 2.2582647800445557
Validation loss: 2.062263640024329

Epoch: 5| Step: 2
Training loss: 1.5150009393692017
Validation loss: 2.1152073773004676

Epoch: 5| Step: 3
Training loss: 1.6460838317871094
Validation loss: 2.107953558685959

Epoch: 5| Step: 4
Training loss: 1.9215675592422485
Validation loss: 2.154261435231855

Epoch: 5| Step: 5
Training loss: 1.5115312337875366
Validation loss: 2.1452860896305372

Epoch: 5| Step: 6
Training loss: 1.7020142078399658
Validation loss: 2.099586186870452

Epoch: 5| Step: 7
Training loss: 1.453980565071106
Validation loss: 2.0892551381100892

Epoch: 5| Step: 8
Training loss: 2.35111927986145
Validation loss: 2.0844413798342467

Epoch: 5| Step: 9
Training loss: 2.026475667953491
Validation loss: 2.080622378215995

Epoch: 5| Step: 10
Training loss: 1.6661180257797241
Validation loss: 2.0788988092894196

Epoch: 163| Step: 0
Training loss: 1.6416925191879272
Validation loss: 2.081339842529707

Epoch: 5| Step: 1
Training loss: 2.068246364593506
Validation loss: 2.0995395952655422

Epoch: 5| Step: 2
Training loss: 1.9222805500030518
Validation loss: 2.10358016978028

Epoch: 5| Step: 3
Training loss: 1.7197999954223633
Validation loss: 2.1173797730476625

Epoch: 5| Step: 4
Training loss: 2.1878387928009033
Validation loss: 2.107853889465332

Epoch: 5| Step: 5
Training loss: 1.875594139099121
Validation loss: 2.105913890305386

Epoch: 5| Step: 6
Training loss: 1.656280517578125
Validation loss: 2.1044380331552155

Epoch: 5| Step: 7
Training loss: 1.7284390926361084
Validation loss: 2.054415979693013

Epoch: 5| Step: 8
Training loss: 1.2634456157684326
Validation loss: 2.0040767218476985

Epoch: 5| Step: 9
Training loss: 1.688117265701294
Validation loss: 2.0024424932336293

Epoch: 5| Step: 10
Training loss: 1.8317590951919556
Validation loss: 2.000980584852157

Epoch: 164| Step: 0
Training loss: 1.8609619140625
Validation loss: 2.014550938401171

Epoch: 5| Step: 1
Training loss: 2.008640766143799
Validation loss: 2.0372560639535227

Epoch: 5| Step: 2
Training loss: 1.6669635772705078
Validation loss: 2.12061470811085

Epoch: 5| Step: 3
Training loss: 2.1042287349700928
Validation loss: 2.2238876742701374

Epoch: 5| Step: 4
Training loss: 1.7130533456802368
Validation loss: 2.31765010920904

Epoch: 5| Step: 5
Training loss: 1.3377444744110107
Validation loss: 2.289439252627793

Epoch: 5| Step: 6
Training loss: 1.1914070844650269
Validation loss: 2.15642116403067

Epoch: 5| Step: 7
Training loss: 2.015873432159424
Validation loss: 2.078838412479688

Epoch: 5| Step: 8
Training loss: 1.9788089990615845
Validation loss: 2.0306661128997803

Epoch: 5| Step: 9
Training loss: 1.412411093711853
Validation loss: 2.024686313444568

Epoch: 5| Step: 10
Training loss: 2.260505437850952
Validation loss: 2.0050015654615176

Epoch: 165| Step: 0
Training loss: 1.5758956670761108
Validation loss: 1.9928457403695712

Epoch: 5| Step: 1
Training loss: 1.9358253479003906
Validation loss: 1.9975601319343812

Epoch: 5| Step: 2
Training loss: 2.0995256900787354
Validation loss: 2.0046565558320735

Epoch: 5| Step: 3
Training loss: 1.4569002389907837
Validation loss: 2.023309523059476

Epoch: 5| Step: 4
Training loss: 2.358835458755493
Validation loss: 2.0388334361455773

Epoch: 5| Step: 5
Training loss: 2.0587644577026367
Validation loss: 2.066826546063987

Epoch: 5| Step: 6
Training loss: 1.6323966979980469
Validation loss: 2.0731489222536803

Epoch: 5| Step: 7
Training loss: 1.8549621105194092
Validation loss: 2.0744870529379895

Epoch: 5| Step: 8
Training loss: 1.538789987564087
Validation loss: 2.083512536941036

Epoch: 5| Step: 9
Training loss: 1.8454649448394775
Validation loss: 2.046046792819936

Epoch: 5| Step: 10
Training loss: 1.3127106428146362
Validation loss: 2.033227279622068

Epoch: 166| Step: 0
Training loss: 2.061624526977539
Validation loss: 2.019725896978891

Epoch: 5| Step: 1
Training loss: 1.5038353204727173
Validation loss: 2.0246245245779715

Epoch: 5| Step: 2
Training loss: 1.8926407098770142
Validation loss: 2.030445119386078

Epoch: 5| Step: 3
Training loss: 1.978541612625122
Validation loss: 2.052369672765014

Epoch: 5| Step: 4
Training loss: 1.875369668006897
Validation loss: 2.0469655913691365

Epoch: 5| Step: 5
Training loss: 1.4111156463623047
Validation loss: 2.0292407786974342

Epoch: 5| Step: 6
Training loss: 1.5974458456039429
Validation loss: 2.029299251494869

Epoch: 5| Step: 7
Training loss: 1.487708330154419
Validation loss: 2.0118711891994683

Epoch: 5| Step: 8
Training loss: 1.6015971899032593
Validation loss: 1.999273250179906

Epoch: 5| Step: 9
Training loss: 1.7523458003997803
Validation loss: 2.0217008231788554

Epoch: 5| Step: 10
Training loss: 2.124378204345703
Validation loss: 2.0588808969784806

Epoch: 167| Step: 0
Training loss: 1.8339507579803467
Validation loss: 2.11074072827575

Epoch: 5| Step: 1
Training loss: 1.5738306045532227
Validation loss: 2.1563596405008787

Epoch: 5| Step: 2
Training loss: 1.4480664730072021
Validation loss: 2.2133487783452517

Epoch: 5| Step: 3
Training loss: 1.146251916885376
Validation loss: 2.2204526701281146

Epoch: 5| Step: 4
Training loss: 2.746201992034912
Validation loss: 2.2923380149308072

Epoch: 5| Step: 5
Training loss: 1.864139199256897
Validation loss: 2.2314683570656726

Epoch: 5| Step: 6
Training loss: 2.1209428310394287
Validation loss: 2.143391970665224

Epoch: 5| Step: 7
Training loss: 1.4290400743484497
Validation loss: 2.0693961881822154

Epoch: 5| Step: 8
Training loss: 1.8419491052627563
Validation loss: 2.001181241004698

Epoch: 5| Step: 9
Training loss: 1.6335748434066772
Validation loss: 1.9680156143762733

Epoch: 5| Step: 10
Training loss: 2.145484209060669
Validation loss: 1.9575078128486552

Epoch: 168| Step: 0
Training loss: 1.4575601816177368
Validation loss: 1.9527117026749479

Epoch: 5| Step: 1
Training loss: 1.928066611289978
Validation loss: 1.9723170444529543

Epoch: 5| Step: 2
Training loss: 2.354804515838623
Validation loss: 1.9746760142746793

Epoch: 5| Step: 3
Training loss: 1.7994091510772705
Validation loss: 2.0262922343387397

Epoch: 5| Step: 4
Training loss: 1.3918880224227905
Validation loss: 2.0875560827152704

Epoch: 5| Step: 5
Training loss: 1.9209697246551514
Validation loss: 2.1465603100356234

Epoch: 5| Step: 6
Training loss: 1.8667361736297607
Validation loss: 2.143786591868247

Epoch: 5| Step: 7
Training loss: 1.6008796691894531
Validation loss: 2.1331577634298675

Epoch: 5| Step: 8
Training loss: 1.7676236629486084
Validation loss: 2.0718322056595997

Epoch: 5| Step: 9
Training loss: 1.9569370746612549
Validation loss: 2.037798830257949

Epoch: 5| Step: 10
Training loss: 1.3524969816207886
Validation loss: 1.9778006794632121

Epoch: 169| Step: 0
Training loss: 1.5466156005859375
Validation loss: 1.9580060230788363

Epoch: 5| Step: 1
Training loss: 1.9250237941741943
Validation loss: 1.9473414292899511

Epoch: 5| Step: 2
Training loss: 1.4566000699996948
Validation loss: 1.956024383985868

Epoch: 5| Step: 3
Training loss: 1.7156174182891846
Validation loss: 1.977371000474499

Epoch: 5| Step: 4
Training loss: 1.6153064966201782
Validation loss: 2.0058177812125093

Epoch: 5| Step: 5
Training loss: 1.3557913303375244
Validation loss: 2.031720822857272

Epoch: 5| Step: 6
Training loss: 2.376842737197876
Validation loss: 2.071812178498955

Epoch: 5| Step: 7
Training loss: 1.6279296875
Validation loss: 2.088757807208646

Epoch: 5| Step: 8
Training loss: 1.798048734664917
Validation loss: 2.082635623152538

Epoch: 5| Step: 9
Training loss: 1.564341425895691
Validation loss: 2.121668879703809

Epoch: 5| Step: 10
Training loss: 2.251629114151001
Validation loss: 2.1511351395678777

Epoch: 170| Step: 0
Training loss: 2.054816484451294
Validation loss: 2.175253665575417

Epoch: 5| Step: 1
Training loss: 1.4986932277679443
Validation loss: 2.134532872066703

Epoch: 5| Step: 2
Training loss: 2.2646970748901367
Validation loss: 2.1278308950444704

Epoch: 5| Step: 3
Training loss: 1.805301308631897
Validation loss: 2.14398742106653

Epoch: 5| Step: 4
Training loss: 1.8222450017929077
Validation loss: 2.124901415199362

Epoch: 5| Step: 5
Training loss: 1.4553468227386475
Validation loss: 2.0835104039920274

Epoch: 5| Step: 6
Training loss: 1.3051104545593262
Validation loss: 2.048311771885041

Epoch: 5| Step: 7
Training loss: 1.641088843345642
Validation loss: 2.009508104734523

Epoch: 5| Step: 8
Training loss: 1.5728720426559448
Validation loss: 2.000187932804067

Epoch: 5| Step: 9
Training loss: 1.7157264947891235
Validation loss: 1.9841429969315887

Epoch: 5| Step: 10
Training loss: 1.654002070426941
Validation loss: 1.9824099899620138

Epoch: 171| Step: 0
Training loss: 2.2295918464660645
Validation loss: 1.9872117170723536

Epoch: 5| Step: 1
Training loss: 2.0976722240448
Validation loss: 1.9996302076565322

Epoch: 5| Step: 2
Training loss: 1.784705400466919
Validation loss: 1.9908123593176565

Epoch: 5| Step: 3
Training loss: 1.695261001586914
Validation loss: 2.011902368196877

Epoch: 5| Step: 4
Training loss: 1.4777761697769165
Validation loss: 2.058569315941103

Epoch: 5| Step: 5
Training loss: 1.4018666744232178
Validation loss: 2.0877242780500844

Epoch: 5| Step: 6
Training loss: 1.336327075958252
Validation loss: 2.123937932393884

Epoch: 5| Step: 7
Training loss: 1.6610686779022217
Validation loss: 2.1747040146140644

Epoch: 5| Step: 8
Training loss: 1.733565092086792
Validation loss: 2.1747380161798127

Epoch: 5| Step: 9
Training loss: 2.0473999977111816
Validation loss: 2.1560787552146503

Epoch: 5| Step: 10
Training loss: 0.9877458214759827
Validation loss: 2.0880086498875774

Epoch: 172| Step: 0
Training loss: 1.3812185525894165
Validation loss: 2.040361847928775

Epoch: 5| Step: 1
Training loss: 1.7581768035888672
Validation loss: 2.020607466338783

Epoch: 5| Step: 2
Training loss: 1.9256786108016968
Validation loss: 2.008486842596403

Epoch: 5| Step: 3
Training loss: 1.6853992938995361
Validation loss: 1.9829076451639975

Epoch: 5| Step: 4
Training loss: 1.3210543394088745
Validation loss: 2.0054611711091894

Epoch: 5| Step: 5
Training loss: 1.7715787887573242
Validation loss: 1.9935283455797421

Epoch: 5| Step: 6
Training loss: 2.068119525909424
Validation loss: 1.9918756228621288

Epoch: 5| Step: 7
Training loss: 2.331392288208008
Validation loss: 1.9935023220636512

Epoch: 5| Step: 8
Training loss: 1.1318382024765015
Validation loss: 2.005543217864088

Epoch: 5| Step: 9
Training loss: 1.9267812967300415
Validation loss: 2.031698921675323

Epoch: 5| Step: 10
Training loss: 0.7016890048980713
Validation loss: 2.0562005632667133

Epoch: 173| Step: 0
Training loss: 1.367393970489502
Validation loss: 2.164039332379577

Epoch: 5| Step: 1
Training loss: 1.9400840997695923
Validation loss: 2.2103319911546606

Epoch: 5| Step: 2
Training loss: 1.839769721031189
Validation loss: 2.1408538408176874

Epoch: 5| Step: 3
Training loss: 1.5293983221054077
Validation loss: 2.0630596299325266

Epoch: 5| Step: 4
Training loss: 1.473502516746521
Validation loss: 2.0044079865178754

Epoch: 5| Step: 5
Training loss: 1.7755324840545654
Validation loss: 1.9851704630800473

Epoch: 5| Step: 6
Training loss: 1.8372166156768799
Validation loss: 2.0032551596241612

Epoch: 5| Step: 7
Training loss: 1.5312410593032837
Validation loss: 2.002936012001448

Epoch: 5| Step: 8
Training loss: 1.3695666790008545
Validation loss: 1.9923419260209607

Epoch: 5| Step: 9
Training loss: 1.6994144916534424
Validation loss: 1.9734314282735188

Epoch: 5| Step: 10
Training loss: 1.8313459157943726
Validation loss: 1.973750921987718

Epoch: 174| Step: 0
Training loss: 1.673246145248413
Validation loss: 2.0173248462779547

Epoch: 5| Step: 1
Training loss: 1.2940661907196045
Validation loss: 2.0297781344382995

Epoch: 5| Step: 2
Training loss: 1.9216371774673462
Validation loss: 2.0661897890029417

Epoch: 5| Step: 3
Training loss: 1.4678159952163696
Validation loss: 2.0511025485172065

Epoch: 5| Step: 4
Training loss: 1.3246814012527466
Validation loss: 2.0173091529518046

Epoch: 5| Step: 5
Training loss: 2.0086638927459717
Validation loss: 1.9695211302849553

Epoch: 5| Step: 6
Training loss: 1.4705190658569336
Validation loss: 1.9852634758077643

Epoch: 5| Step: 7
Training loss: 1.8958269357681274
Validation loss: 1.9578048003617154

Epoch: 5| Step: 8
Training loss: 1.3648654222488403
Validation loss: 1.96888727911057

Epoch: 5| Step: 9
Training loss: 1.824742078781128
Validation loss: 2.011754972960359

Epoch: 5| Step: 10
Training loss: 1.697569727897644
Validation loss: 2.053826355165051

Epoch: 175| Step: 0
Training loss: 1.6089286804199219
Validation loss: 2.0838391678307646

Epoch: 5| Step: 1
Training loss: 1.941440224647522
Validation loss: 2.057315957161688

Epoch: 5| Step: 2
Training loss: 1.557988166809082
Validation loss: 2.048080223862843

Epoch: 5| Step: 3
Training loss: 1.6836448907852173
Validation loss: 1.9989754435836629

Epoch: 5| Step: 4
Training loss: 1.3596783876419067
Validation loss: 1.979812386215374

Epoch: 5| Step: 5
Training loss: 1.6287338733673096
Validation loss: 1.9743830196319088

Epoch: 5| Step: 6
Training loss: 2.2724862098693848
Validation loss: 1.997133267823086

Epoch: 5| Step: 7
Training loss: 0.9569263458251953
Validation loss: 2.0301321834646244

Epoch: 5| Step: 8
Training loss: 1.8757121562957764
Validation loss: 2.061445084951257

Epoch: 5| Step: 9
Training loss: 1.3139688968658447
Validation loss: 2.0914065196949947

Epoch: 5| Step: 10
Training loss: 1.6715385913848877
Validation loss: 2.1278945938233407

Testing loss: 2.3461767435073853
